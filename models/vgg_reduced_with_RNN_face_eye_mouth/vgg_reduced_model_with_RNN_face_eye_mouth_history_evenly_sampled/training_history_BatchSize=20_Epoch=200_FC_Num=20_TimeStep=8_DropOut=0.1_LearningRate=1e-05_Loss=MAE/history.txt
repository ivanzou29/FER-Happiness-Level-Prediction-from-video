Epoch: 1| Step: 0
Training loss: 5.545695781707764
Validation loss: 5.150915489401869

Epoch: 5| Step: 1
Training loss: 5.51571798324585
Validation loss: 5.143588071228356

Epoch: 5| Step: 2
Training loss: 5.225088596343994
Validation loss: 5.136482659206595

Epoch: 5| Step: 3
Training loss: 3.754328966140747
Validation loss: 5.129236139276976

Epoch: 5| Step: 4
Training loss: 4.617422580718994
Validation loss: 5.1215675159167215

Epoch: 5| Step: 5
Training loss: 6.159590721130371
Validation loss: 5.114062129810292

Epoch: 5| Step: 6
Training loss: 4.434762001037598
Validation loss: 5.10550795831988

Epoch: 5| Step: 7
Training loss: 3.9655814170837402
Validation loss: 5.096707933692522

Epoch: 5| Step: 8
Training loss: 3.8447933197021484
Validation loss: 5.087205579203944

Epoch: 5| Step: 9
Training loss: 5.061608791351318
Validation loss: 5.077058140949537

Epoch: 5| Step: 10
Training loss: 6.018308162689209
Validation loss: 5.065894926747968

Epoch: 2| Step: 0
Training loss: 4.936661720275879
Validation loss: 5.054384857095698

Epoch: 5| Step: 1
Training loss: 5.595300674438477
Validation loss: 5.041953250926028

Epoch: 5| Step: 2
Training loss: 5.477599143981934
Validation loss: 5.027432708330052

Epoch: 5| Step: 3
Training loss: 5.074874401092529
Validation loss: 5.012878551278063

Epoch: 5| Step: 4
Training loss: 5.128472805023193
Validation loss: 4.9966816030522825

Epoch: 5| Step: 5
Training loss: 4.7946391105651855
Validation loss: 4.98001604695474

Epoch: 5| Step: 6
Training loss: 4.622066497802734
Validation loss: 4.9618394349211

Epoch: 5| Step: 7
Training loss: 4.1294660568237305
Validation loss: 4.943360328674316

Epoch: 5| Step: 8
Training loss: 4.925950527191162
Validation loss: 4.922865416413995

Epoch: 5| Step: 9
Training loss: 3.214756727218628
Validation loss: 4.901109110924505

Epoch: 5| Step: 10
Training loss: 4.540056228637695
Validation loss: 4.880089493208034

Epoch: 3| Step: 0
Training loss: 4.072463512420654
Validation loss: 4.856423998391756

Epoch: 5| Step: 1
Training loss: 4.1593427658081055
Validation loss: 4.831774157862509

Epoch: 5| Step: 2
Training loss: 6.085808753967285
Validation loss: 4.804614497769263

Epoch: 5| Step: 3
Training loss: 3.9739251136779785
Validation loss: 4.778567298766105

Epoch: 5| Step: 4
Training loss: 6.404422760009766
Validation loss: 4.750192913957822

Epoch: 5| Step: 5
Training loss: 3.9644775390625
Validation loss: 4.720566959791286

Epoch: 5| Step: 6
Training loss: 4.129583835601807
Validation loss: 4.690781418995191

Epoch: 5| Step: 7
Training loss: 3.7507567405700684
Validation loss: 4.659343729736984

Epoch: 5| Step: 8
Training loss: 4.687579154968262
Validation loss: 4.62523215816867

Epoch: 5| Step: 9
Training loss: 4.389684677124023
Validation loss: 4.591394255238194

Epoch: 5| Step: 10
Training loss: 3.7700963020324707
Validation loss: 4.5550063810040875

Epoch: 4| Step: 0
Training loss: 4.202622413635254
Validation loss: 4.516948094931982

Epoch: 5| Step: 1
Training loss: 4.376864433288574
Validation loss: 4.47702076614544

Epoch: 5| Step: 2
Training loss: 4.821991920471191
Validation loss: 4.437944927523213

Epoch: 5| Step: 3
Training loss: 4.566040992736816
Validation loss: 4.402012471229799

Epoch: 5| Step: 4
Training loss: 4.628711700439453
Validation loss: 4.362885685377224

Epoch: 5| Step: 5
Training loss: 3.7442946434020996
Validation loss: 4.327951492801789

Epoch: 5| Step: 6
Training loss: 2.777834415435791
Validation loss: 4.291938812501969

Epoch: 5| Step: 7
Training loss: 3.9495861530303955
Validation loss: 4.253878311444354

Epoch: 5| Step: 8
Training loss: 4.500669479370117
Validation loss: 4.218583565886303

Epoch: 5| Step: 9
Training loss: 4.016578197479248
Validation loss: 4.183345569077359

Epoch: 5| Step: 10
Training loss: 3.9424591064453125
Validation loss: 4.146846991713329

Epoch: 5| Step: 0
Training loss: 2.702667236328125
Validation loss: 4.115409871583344

Epoch: 5| Step: 1
Training loss: 4.122037410736084
Validation loss: 4.085067492659374

Epoch: 5| Step: 2
Training loss: 2.996001720428467
Validation loss: 4.055100199996784

Epoch: 5| Step: 3
Training loss: 3.787754774093628
Validation loss: 4.024997259980889

Epoch: 5| Step: 4
Training loss: 4.758061408996582
Validation loss: 3.9962022432716946

Epoch: 5| Step: 5
Training loss: 3.607731580734253
Validation loss: 3.966455336539976

Epoch: 5| Step: 6
Training loss: 4.593204498291016
Validation loss: 3.936075302862352

Epoch: 5| Step: 7
Training loss: 3.760685682296753
Validation loss: 3.904430781641314

Epoch: 5| Step: 8
Training loss: 4.428780555725098
Validation loss: 3.872613799187445

Epoch: 5| Step: 9
Training loss: 3.6812691688537598
Validation loss: 3.84480563030448

Epoch: 5| Step: 10
Training loss: 3.420289993286133
Validation loss: 3.8135371515827794

Epoch: 6| Step: 0
Training loss: 3.366621732711792
Validation loss: 3.7845800358762025

Epoch: 5| Step: 1
Training loss: 4.108789443969727
Validation loss: 3.756169242243613

Epoch: 5| Step: 2
Training loss: 4.353273391723633
Validation loss: 3.7303369429803666

Epoch: 5| Step: 3
Training loss: 2.51025390625
Validation loss: 3.70555478783064

Epoch: 5| Step: 4
Training loss: 3.281244993209839
Validation loss: 3.682212319425357

Epoch: 5| Step: 5
Training loss: 3.9965357780456543
Validation loss: 3.659663954088765

Epoch: 5| Step: 6
Training loss: 2.3949599266052246
Validation loss: 3.639216797326201

Epoch: 5| Step: 7
Training loss: 4.490067005157471
Validation loss: 3.6195695707874913

Epoch: 5| Step: 8
Training loss: 2.778268337249756
Validation loss: 3.601629267456711

Epoch: 5| Step: 9
Training loss: 4.392383098602295
Validation loss: 3.589617775332543

Epoch: 5| Step: 10
Training loss: 3.6432571411132812
Validation loss: 3.5757095095931843

Epoch: 7| Step: 0
Training loss: 4.594398021697998
Validation loss: 3.5596913471016833

Epoch: 5| Step: 1
Training loss: 2.7572638988494873
Validation loss: 3.54849148309359

Epoch: 5| Step: 2
Training loss: 4.077939033508301
Validation loss: 3.5380967304270756

Epoch: 5| Step: 3
Training loss: 3.544419765472412
Validation loss: 3.526440994713896

Epoch: 5| Step: 4
Training loss: 3.1103663444519043
Validation loss: 3.515611530632101

Epoch: 5| Step: 5
Training loss: 3.2944252490997314
Validation loss: 3.502613893119238

Epoch: 5| Step: 6
Training loss: 3.0106050968170166
Validation loss: 3.4953769099327827

Epoch: 5| Step: 7
Training loss: 3.517408847808838
Validation loss: 3.484202118330104

Epoch: 5| Step: 8
Training loss: 3.2229652404785156
Validation loss: 3.476736173834852

Epoch: 5| Step: 9
Training loss: 2.885937213897705
Validation loss: 3.466207524781586

Epoch: 5| Step: 10
Training loss: 3.910447120666504
Validation loss: 3.456773888680243

Epoch: 8| Step: 0
Training loss: 2.6919496059417725
Validation loss: 3.450409527747862

Epoch: 5| Step: 1
Training loss: 3.8138928413391113
Validation loss: 3.4395034338838313

Epoch: 5| Step: 2
Training loss: 3.305274486541748
Validation loss: 3.429659938299528

Epoch: 5| Step: 3
Training loss: 4.393566608428955
Validation loss: 3.42189436317772

Epoch: 5| Step: 4
Training loss: 3.0035476684570312
Validation loss: 3.414590512552569

Epoch: 5| Step: 5
Training loss: 4.24261999130249
Validation loss: 3.403210340007659

Epoch: 5| Step: 6
Training loss: 3.0614211559295654
Validation loss: 3.3967855463745775

Epoch: 5| Step: 7
Training loss: 3.2535691261291504
Validation loss: 3.3861370753216486

Epoch: 5| Step: 8
Training loss: 3.5379886627197266
Validation loss: 3.3770337181706584

Epoch: 5| Step: 9
Training loss: 2.894228458404541
Validation loss: 3.368684866095102

Epoch: 5| Step: 10
Training loss: 2.5790066719055176
Validation loss: 3.3614555430668656

Epoch: 9| Step: 0
Training loss: 2.5223031044006348
Validation loss: 3.354225827801612

Epoch: 5| Step: 1
Training loss: 2.8333964347839355
Validation loss: 3.3439336720333306

Epoch: 5| Step: 2
Training loss: 3.1790175437927246
Validation loss: 3.334832924668507

Epoch: 5| Step: 3
Training loss: 2.9084668159484863
Validation loss: 3.3289475056432907

Epoch: 5| Step: 4
Training loss: 4.076544761657715
Validation loss: 3.322301323695849

Epoch: 5| Step: 5
Training loss: 3.9531750679016113
Validation loss: 3.313787260363179

Epoch: 5| Step: 6
Training loss: 3.2206711769104004
Validation loss: 3.3068359564709406

Epoch: 5| Step: 7
Training loss: 3.0814049243927
Validation loss: 3.299637953440348

Epoch: 5| Step: 8
Training loss: 2.906656265258789
Validation loss: 3.292097822312386

Epoch: 5| Step: 9
Training loss: 3.2832489013671875
Validation loss: 3.2841585143919914

Epoch: 5| Step: 10
Training loss: 4.241954803466797
Validation loss: 3.2772098459223264

Epoch: 10| Step: 0
Training loss: 3.8629136085510254
Validation loss: 3.2730921032608196

Epoch: 5| Step: 1
Training loss: 3.6127681732177734
Validation loss: 3.2644212220304754

Epoch: 5| Step: 2
Training loss: 2.880823850631714
Validation loss: 3.2547360799645864

Epoch: 5| Step: 3
Training loss: 3.2456459999084473
Validation loss: 3.251447487902898

Epoch: 5| Step: 4
Training loss: 3.3368136882781982
Validation loss: 3.244006267157934

Epoch: 5| Step: 5
Training loss: 2.8848557472229004
Validation loss: 3.2388456124131397

Epoch: 5| Step: 6
Training loss: 3.6609127521514893
Validation loss: 3.236110738528672

Epoch: 5| Step: 7
Training loss: 2.6384482383728027
Validation loss: 3.2271231656433432

Epoch: 5| Step: 8
Training loss: 2.946010112762451
Validation loss: 3.21446745370024

Epoch: 5| Step: 9
Training loss: 3.157810926437378
Validation loss: 3.2078767668816353

Epoch: 5| Step: 10
Training loss: 3.2421531677246094
Validation loss: 3.2016235038798344

Epoch: 11| Step: 0
Training loss: 3.556675434112549
Validation loss: 3.1984552080913256

Epoch: 5| Step: 1
Training loss: 2.297616481781006
Validation loss: 3.1884069365839802

Epoch: 5| Step: 2
Training loss: 3.0544376373291016
Validation loss: 3.190135827628515

Epoch: 5| Step: 3
Training loss: 2.7375149726867676
Validation loss: 3.1808080442490114

Epoch: 5| Step: 4
Training loss: 3.3207473754882812
Validation loss: 3.1792064815439205

Epoch: 5| Step: 5
Training loss: 4.279486656188965
Validation loss: 3.1709776488683556

Epoch: 5| Step: 6
Training loss: 3.35760498046875
Validation loss: 3.1613899969285533

Epoch: 5| Step: 7
Training loss: 2.8941738605499268
Validation loss: 3.161793880565192

Epoch: 5| Step: 8
Training loss: 3.3657004833221436
Validation loss: 3.161549580994473

Epoch: 5| Step: 9
Training loss: 2.899932861328125
Validation loss: 3.154099602853098

Epoch: 5| Step: 10
Training loss: 3.1506893634796143
Validation loss: 3.1423353866864274

Epoch: 12| Step: 0
Training loss: 3.4111945629119873
Validation loss: 3.148795143250496

Epoch: 5| Step: 1
Training loss: 2.744431495666504
Validation loss: 3.152670242453134

Epoch: 5| Step: 2
Training loss: 3.285137891769409
Validation loss: 3.156435684491229

Epoch: 5| Step: 3
Training loss: 3.11458683013916
Validation loss: 3.135855064597181

Epoch: 5| Step: 4
Training loss: 3.201784133911133
Validation loss: 3.1206574542548067

Epoch: 5| Step: 5
Training loss: 3.553081512451172
Validation loss: 3.1173828058345343

Epoch: 5| Step: 6
Training loss: 3.1002700328826904
Validation loss: 3.1613678470734627

Epoch: 5| Step: 7
Training loss: 3.238210678100586
Validation loss: 3.1469850360706286

Epoch: 5| Step: 8
Training loss: 3.150684356689453
Validation loss: 3.103199371727564

Epoch: 5| Step: 9
Training loss: 2.997875928878784
Validation loss: 3.1021652606225785

Epoch: 5| Step: 10
Training loss: 2.816636085510254
Validation loss: 3.1109874991960424

Epoch: 13| Step: 0
Training loss: 3.4300358295440674
Validation loss: 3.118863049373832

Epoch: 5| Step: 1
Training loss: 3.456148147583008
Validation loss: 3.1128256064589306

Epoch: 5| Step: 2
Training loss: 2.375088691711426
Validation loss: 3.0982509274636545

Epoch: 5| Step: 3
Training loss: 2.932748794555664
Validation loss: 3.0841061043482956

Epoch: 5| Step: 4
Training loss: 2.7265119552612305
Validation loss: 3.0768671702313166

Epoch: 5| Step: 5
Training loss: 3.549175262451172
Validation loss: 3.0745176833163024

Epoch: 5| Step: 6
Training loss: 2.739494562149048
Validation loss: 3.0757383172230055

Epoch: 5| Step: 7
Training loss: 3.501239776611328
Validation loss: 3.0941064101393505

Epoch: 5| Step: 8
Training loss: 2.97031307220459
Validation loss: 3.078035557141868

Epoch: 5| Step: 9
Training loss: 3.3464112281799316
Validation loss: 3.0611739773904123

Epoch: 5| Step: 10
Training loss: 3.253951072692871
Validation loss: 3.0546457870032198

Epoch: 14| Step: 0
Training loss: 2.7839596271514893
Validation loss: 3.0473180406837055

Epoch: 5| Step: 1
Training loss: 3.033857583999634
Validation loss: 3.054312998248685

Epoch: 5| Step: 2
Training loss: 3.095175266265869
Validation loss: 3.050027396089287

Epoch: 5| Step: 3
Training loss: 3.4778456687927246
Validation loss: 3.0441682466896633

Epoch: 5| Step: 4
Training loss: 3.3883297443389893
Validation loss: 3.0357673655274096

Epoch: 5| Step: 5
Training loss: 2.881347179412842
Validation loss: 3.031041419634255

Epoch: 5| Step: 6
Training loss: 3.3039097785949707
Validation loss: 3.0275727907816568

Epoch: 5| Step: 7
Training loss: 3.1979000568389893
Validation loss: 3.0299052679410545

Epoch: 5| Step: 8
Training loss: 2.829967975616455
Validation loss: 3.0203920205434165

Epoch: 5| Step: 9
Training loss: 3.1239404678344727
Validation loss: 3.0157431223059215

Epoch: 5| Step: 10
Training loss: 2.7404003143310547
Validation loss: 3.0115915831699165

Epoch: 15| Step: 0
Training loss: 2.761354446411133
Validation loss: 3.0091147422790527

Epoch: 5| Step: 1
Training loss: 3.0700435638427734
Validation loss: 3.0062320027300107

Epoch: 5| Step: 2
Training loss: 2.244641065597534
Validation loss: 3.001308577035063

Epoch: 5| Step: 3
Training loss: 2.483874797821045
Validation loss: 3.001623087031867

Epoch: 5| Step: 4
Training loss: 3.4235756397247314
Validation loss: 2.994204910852576

Epoch: 5| Step: 5
Training loss: 2.9786484241485596
Validation loss: 2.994948487127981

Epoch: 5| Step: 6
Training loss: 3.7294418811798096
Validation loss: 2.9916614999053297

Epoch: 5| Step: 7
Training loss: 3.8524787425994873
Validation loss: 2.989640969102101

Epoch: 5| Step: 8
Training loss: 2.5557987689971924
Validation loss: 2.9810948320614394

Epoch: 5| Step: 9
Training loss: 3.103062629699707
Validation loss: 2.9791768391927085

Epoch: 5| Step: 10
Training loss: 3.4462106227874756
Validation loss: 2.977202025792932

Epoch: 16| Step: 0
Training loss: 3.2648777961730957
Validation loss: 2.9724217076455393

Epoch: 5| Step: 1
Training loss: 2.782470703125
Validation loss: 2.969358177595241

Epoch: 5| Step: 2
Training loss: 3.369009017944336
Validation loss: 2.9650330133335565

Epoch: 5| Step: 3
Training loss: 3.1200289726257324
Validation loss: 2.9629292898280646

Epoch: 5| Step: 4
Training loss: 3.1671528816223145
Validation loss: 2.9625054559400006

Epoch: 5| Step: 5
Training loss: 3.306558132171631
Validation loss: 2.9570506593232513

Epoch: 5| Step: 6
Training loss: 3.0964481830596924
Validation loss: 2.9675141919043755

Epoch: 5| Step: 7
Training loss: 2.722708225250244
Validation loss: 2.9598504804795787

Epoch: 5| Step: 8
Training loss: 2.769948959350586
Validation loss: 2.957612734968944

Epoch: 5| Step: 9
Training loss: 3.285454273223877
Validation loss: 2.945282656659362

Epoch: 5| Step: 10
Training loss: 2.408594846725464
Validation loss: 2.9439976240998957

Epoch: 17| Step: 0
Training loss: 2.067164897918701
Validation loss: 2.956019079813393

Epoch: 5| Step: 1
Training loss: 3.4474644660949707
Validation loss: 2.9672294355207876

Epoch: 5| Step: 2
Training loss: 3.7036585807800293
Validation loss: 2.968954558013588

Epoch: 5| Step: 3
Training loss: 3.0655999183654785
Validation loss: 2.9439919917814192

Epoch: 5| Step: 4
Training loss: 2.5578699111938477
Validation loss: 2.930761624408025

Epoch: 5| Step: 5
Training loss: 2.692810535430908
Validation loss: 2.9451104492269535

Epoch: 5| Step: 6
Training loss: 1.9449913501739502
Validation loss: 2.9413787344450593

Epoch: 5| Step: 7
Training loss: 3.922443389892578
Validation loss: 2.9576447497132006

Epoch: 5| Step: 8
Training loss: 3.0803558826446533
Validation loss: 2.9416640676477903

Epoch: 5| Step: 9
Training loss: 3.683140277862549
Validation loss: 2.9270306095000236

Epoch: 5| Step: 10
Training loss: 3.083737850189209
Validation loss: 2.9217976421438236

Epoch: 18| Step: 0
Training loss: 3.9887757301330566
Validation loss: 2.922030843714232

Epoch: 5| Step: 1
Training loss: 3.5597457885742188
Validation loss: 2.9320534377969723

Epoch: 5| Step: 2
Training loss: 3.1184916496276855
Validation loss: 2.9414022430296867

Epoch: 5| Step: 3
Training loss: 3.1106114387512207
Validation loss: 2.934164090823102

Epoch: 5| Step: 4
Training loss: 2.989704132080078
Validation loss: 2.9190105699723765

Epoch: 5| Step: 5
Training loss: 2.147338628768921
Validation loss: 2.910507845622237

Epoch: 5| Step: 6
Training loss: 3.1396422386169434
Validation loss: 2.9090032295514177

Epoch: 5| Step: 7
Training loss: 2.505514144897461
Validation loss: 2.9079069757974274

Epoch: 5| Step: 8
Training loss: 3.009819984436035
Validation loss: 2.909843234605687

Epoch: 5| Step: 9
Training loss: 2.322373867034912
Validation loss: 2.9067130857898342

Epoch: 5| Step: 10
Training loss: 3.2386302947998047
Validation loss: 2.912916068107851

Epoch: 19| Step: 0
Training loss: 3.257723569869995
Validation loss: 2.9022823815704673

Epoch: 5| Step: 1
Training loss: 3.988595962524414
Validation loss: 2.8959022491208968

Epoch: 5| Step: 2
Training loss: 2.5019798278808594
Validation loss: 2.892077584420481

Epoch: 5| Step: 3
Training loss: 3.448514223098755
Validation loss: 2.8931744637027865

Epoch: 5| Step: 4
Training loss: 2.932748556137085
Validation loss: 2.8905812283997894

Epoch: 5| Step: 5
Training loss: 3.0688259601593018
Validation loss: 2.887123179692094

Epoch: 5| Step: 6
Training loss: 2.4434149265289307
Validation loss: 2.889026495718187

Epoch: 5| Step: 7
Training loss: 3.2698941230773926
Validation loss: 2.8883916485694145

Epoch: 5| Step: 8
Training loss: 2.3398020267486572
Validation loss: 2.885023283702071

Epoch: 5| Step: 9
Training loss: 2.312011241912842
Validation loss: 2.8863410488251717

Epoch: 5| Step: 10
Training loss: 3.323748826980591
Validation loss: 2.8837855990215013

Epoch: 20| Step: 0
Training loss: 3.8383560180664062
Validation loss: 2.881742077489053

Epoch: 5| Step: 1
Training loss: 2.4619526863098145
Validation loss: 2.879481730922576

Epoch: 5| Step: 2
Training loss: 2.516902446746826
Validation loss: 2.8729805254167124

Epoch: 5| Step: 3
Training loss: 3.2974636554718018
Validation loss: 2.8701217046347995

Epoch: 5| Step: 4
Training loss: 2.8683278560638428
Validation loss: 2.8698159545980473

Epoch: 5| Step: 5
Training loss: 2.3775742053985596
Validation loss: 2.8686510029659478

Epoch: 5| Step: 6
Training loss: 2.824892282485962
Validation loss: 2.8675905350715882

Epoch: 5| Step: 7
Training loss: 3.612622022628784
Validation loss: 2.8665262319708384

Epoch: 5| Step: 8
Training loss: 2.888258934020996
Validation loss: 2.866865760536604

Epoch: 5| Step: 9
Training loss: 2.9497451782226562
Validation loss: 2.8615146093471076

Epoch: 5| Step: 10
Training loss: 3.0854074954986572
Validation loss: 2.8627977268670195

Epoch: 21| Step: 0
Training loss: 2.800178050994873
Validation loss: 2.8628930430258475

Epoch: 5| Step: 1
Training loss: 3.616422653198242
Validation loss: 2.861003819332328

Epoch: 5| Step: 2
Training loss: 3.8485207557678223
Validation loss: 2.8613289299831597

Epoch: 5| Step: 3
Training loss: 2.1315205097198486
Validation loss: 2.8558264342687463

Epoch: 5| Step: 4
Training loss: 2.607027530670166
Validation loss: 2.8547297421322075

Epoch: 5| Step: 5
Training loss: 2.9273550510406494
Validation loss: 2.8531549463989916

Epoch: 5| Step: 6
Training loss: 3.042848587036133
Validation loss: 2.8520186562691965

Epoch: 5| Step: 7
Training loss: 2.8345279693603516
Validation loss: 2.8507585961331605

Epoch: 5| Step: 8
Training loss: 3.129310131072998
Validation loss: 2.8497692179936234

Epoch: 5| Step: 9
Training loss: 2.2189900875091553
Validation loss: 2.8486498658375075

Epoch: 5| Step: 10
Training loss: 3.5299437046051025
Validation loss: 2.8475652151210333

Epoch: 22| Step: 0
Training loss: 2.3735055923461914
Validation loss: 2.8502226542401057

Epoch: 5| Step: 1
Training loss: 2.6985116004943848
Validation loss: 2.847877287095593

Epoch: 5| Step: 2
Training loss: 3.0028939247131348
Validation loss: 2.847856557497414

Epoch: 5| Step: 3
Training loss: 2.690376043319702
Validation loss: 2.8467277224345873

Epoch: 5| Step: 4
Training loss: 3.2713189125061035
Validation loss: 2.8437645717333724

Epoch: 5| Step: 5
Training loss: 3.5925076007843018
Validation loss: 2.8432175241490847

Epoch: 5| Step: 6
Training loss: 2.7664308547973633
Validation loss: 2.8414552827035227

Epoch: 5| Step: 7
Training loss: 3.1405880451202393
Validation loss: 2.8417335658945064

Epoch: 5| Step: 8
Training loss: 3.3221752643585205
Validation loss: 2.841857325646185

Epoch: 5| Step: 9
Training loss: 2.9505324363708496
Validation loss: 2.8374666885663102

Epoch: 5| Step: 10
Training loss: 2.657728672027588
Validation loss: 2.835524328293339

Epoch: 23| Step: 0
Training loss: 2.272033452987671
Validation loss: 2.837577909551641

Epoch: 5| Step: 1
Training loss: 2.9969871044158936
Validation loss: 2.8340376987252185

Epoch: 5| Step: 2
Training loss: 3.1459553241729736
Validation loss: 2.834212695398638

Epoch: 5| Step: 3
Training loss: 2.853416681289673
Validation loss: 2.8353431891369563

Epoch: 5| Step: 4
Training loss: 3.051631450653076
Validation loss: 2.8313591710982786

Epoch: 5| Step: 5
Training loss: 2.580974578857422
Validation loss: 2.8333808427215903

Epoch: 5| Step: 6
Training loss: 3.235797166824341
Validation loss: 2.8318422045758975

Epoch: 5| Step: 7
Training loss: 3.0123651027679443
Validation loss: 2.8328015817108976

Epoch: 5| Step: 8
Training loss: 2.9643781185150146
Validation loss: 2.8303510706911803

Epoch: 5| Step: 9
Training loss: 3.186028242111206
Validation loss: 2.830205807121851

Epoch: 5| Step: 10
Training loss: 3.1442105770111084
Validation loss: 2.8269104983216975

Epoch: 24| Step: 0
Training loss: 2.723442792892456
Validation loss: 2.82587246741018

Epoch: 5| Step: 1
Training loss: 3.52960467338562
Validation loss: 2.824742083908409

Epoch: 5| Step: 2
Training loss: 3.536046266555786
Validation loss: 2.8254348078081684

Epoch: 5| Step: 3
Training loss: 2.1994309425354004
Validation loss: 2.8260320796761462

Epoch: 5| Step: 4
Training loss: 2.0063111782073975
Validation loss: 2.824706713358561

Epoch: 5| Step: 5
Training loss: 2.7267165184020996
Validation loss: 2.825371455120784

Epoch: 5| Step: 6
Training loss: 3.659991502761841
Validation loss: 2.822336527609056

Epoch: 5| Step: 7
Training loss: 2.572237253189087
Validation loss: 2.8222235402753277

Epoch: 5| Step: 8
Training loss: 2.679279327392578
Validation loss: 2.8188302593846477

Epoch: 5| Step: 9
Training loss: 3.094494342803955
Validation loss: 2.819757607675368

Epoch: 5| Step: 10
Training loss: 3.748307704925537
Validation loss: 2.8186373992632796

Epoch: 25| Step: 0
Training loss: 3.6783714294433594
Validation loss: 2.818100647259784

Epoch: 5| Step: 1
Training loss: 2.5948143005371094
Validation loss: 2.816910518113003

Epoch: 5| Step: 2
Training loss: 2.716387987136841
Validation loss: 2.8113431417813866

Epoch: 5| Step: 3
Training loss: 1.842665433883667
Validation loss: 2.8139216874235418

Epoch: 5| Step: 4
Training loss: 2.9713339805603027
Validation loss: 2.8110620488402662

Epoch: 5| Step: 5
Training loss: 3.5737102031707764
Validation loss: 2.810350230945054

Epoch: 5| Step: 6
Training loss: 2.992133617401123
Validation loss: 2.809787086261216

Epoch: 5| Step: 7
Training loss: 2.896310329437256
Validation loss: 2.807274474892565

Epoch: 5| Step: 8
Training loss: 3.121523857116699
Validation loss: 2.809768333229967

Epoch: 5| Step: 9
Training loss: 2.6891613006591797
Validation loss: 2.8145870649686424

Epoch: 5| Step: 10
Training loss: 3.22231125831604
Validation loss: 2.8135371797828266

Epoch: 26| Step: 0
Training loss: 3.0050837993621826
Validation loss: 2.811997667435677

Epoch: 5| Step: 1
Training loss: 2.8670432567596436
Validation loss: 2.8096722402880268

Epoch: 5| Step: 2
Training loss: 2.8700530529022217
Validation loss: 2.806793943528206

Epoch: 5| Step: 3
Training loss: 2.4442687034606934
Validation loss: 2.802828155538087

Epoch: 5| Step: 4
Training loss: 2.7526848316192627
Validation loss: 2.811011178519136

Epoch: 5| Step: 5
Training loss: 2.7893967628479004
Validation loss: 2.802877031346803

Epoch: 5| Step: 6
Training loss: 2.173558235168457
Validation loss: 2.8007587232897357

Epoch: 5| Step: 7
Training loss: 3.6601409912109375
Validation loss: 2.799486667879166

Epoch: 5| Step: 8
Training loss: 3.0003695487976074
Validation loss: 2.8015345655461794

Epoch: 5| Step: 9
Training loss: 3.4509902000427246
Validation loss: 2.7979307969411216

Epoch: 5| Step: 10
Training loss: 3.234018325805664
Validation loss: 2.79640031373629

Epoch: 27| Step: 0
Training loss: 3.4601123332977295
Validation loss: 2.7942713255523355

Epoch: 5| Step: 1
Training loss: 2.5775647163391113
Validation loss: 2.795900708885603

Epoch: 5| Step: 2
Training loss: 2.491123676300049
Validation loss: 2.794215263858918

Epoch: 5| Step: 3
Training loss: 3.227067232131958
Validation loss: 2.8001824373840005

Epoch: 5| Step: 4
Training loss: 2.6322860717773438
Validation loss: 2.795691172281901

Epoch: 5| Step: 5
Training loss: 2.906527519226074
Validation loss: 2.796089756873346

Epoch: 5| Step: 6
Training loss: 2.5372414588928223
Validation loss: 2.793033710090063

Epoch: 5| Step: 7
Training loss: 3.0214414596557617
Validation loss: 2.789622578569638

Epoch: 5| Step: 8
Training loss: 3.807142972946167
Validation loss: 2.7957111225333264

Epoch: 5| Step: 9
Training loss: 2.2096171379089355
Validation loss: 2.7949498930285053

Epoch: 5| Step: 10
Training loss: 3.3137314319610596
Validation loss: 2.7943956595595165

Epoch: 28| Step: 0
Training loss: 3.1502866744995117
Validation loss: 2.7910100772816646

Epoch: 5| Step: 1
Training loss: 3.5404210090637207
Validation loss: 2.7959312803001812

Epoch: 5| Step: 2
Training loss: 3.8516552448272705
Validation loss: 2.7909097697145198

Epoch: 5| Step: 3
Training loss: 3.1126487255096436
Validation loss: 2.790074776577693

Epoch: 5| Step: 4
Training loss: 2.7706196308135986
Validation loss: 2.783445676167806

Epoch: 5| Step: 5
Training loss: 2.3847081661224365
Validation loss: 2.78496955543436

Epoch: 5| Step: 6
Training loss: 2.8524582386016846
Validation loss: 2.7816616771041707

Epoch: 5| Step: 7
Training loss: 2.6685900688171387
Validation loss: 2.7832413745182816

Epoch: 5| Step: 8
Training loss: 2.7480380535125732
Validation loss: 2.785521863609232

Epoch: 5| Step: 9
Training loss: 2.474965810775757
Validation loss: 2.7803913982965613

Epoch: 5| Step: 10
Training loss: 2.464581251144409
Validation loss: 2.7836601657252156

Epoch: 29| Step: 0
Training loss: 2.4796969890594482
Validation loss: 2.7793048325405327

Epoch: 5| Step: 1
Training loss: 3.1298272609710693
Validation loss: 2.7783565931422736

Epoch: 5| Step: 2
Training loss: 2.5934059619903564
Validation loss: 2.7760289099908646

Epoch: 5| Step: 3
Training loss: 2.983119487762451
Validation loss: 2.7765344035240913

Epoch: 5| Step: 4
Training loss: 2.717404842376709
Validation loss: 2.7783455028328845

Epoch: 5| Step: 5
Training loss: 4.041974067687988
Validation loss: 2.785555439610635

Epoch: 5| Step: 6
Training loss: 2.9170334339141846
Validation loss: 2.774541788203742

Epoch: 5| Step: 7
Training loss: 1.9378740787506104
Validation loss: 2.773746516114922

Epoch: 5| Step: 8
Training loss: 3.028942346572876
Validation loss: 2.770610906744516

Epoch: 5| Step: 9
Training loss: 2.4457645416259766
Validation loss: 2.770695824776926

Epoch: 5| Step: 10
Training loss: 3.8568384647369385
Validation loss: 2.7765727478970765

Epoch: 30| Step: 0
Training loss: 2.8794946670532227
Validation loss: 2.7710368607633855

Epoch: 5| Step: 1
Training loss: 3.251340866088867
Validation loss: 2.772858981163271

Epoch: 5| Step: 2
Training loss: 2.660839557647705
Validation loss: 2.777027004508562

Epoch: 5| Step: 3
Training loss: 2.512025833129883
Validation loss: 2.774225006821335

Epoch: 5| Step: 4
Training loss: 2.7590084075927734
Validation loss: 2.772817955222181

Epoch: 5| Step: 5
Training loss: 2.592449903488159
Validation loss: 2.7755112468555407

Epoch: 5| Step: 6
Training loss: 2.1816723346710205
Validation loss: 2.771417276833647

Epoch: 5| Step: 7
Training loss: 3.3396878242492676
Validation loss: 2.7731343879494617

Epoch: 5| Step: 8
Training loss: 2.6786510944366455
Validation loss: 2.7715329662446053

Epoch: 5| Step: 9
Training loss: 3.433964490890503
Validation loss: 2.769468222894976

Epoch: 5| Step: 10
Training loss: 3.794729709625244
Validation loss: 2.76692787806193

Epoch: 31| Step: 0
Training loss: 2.3764259815216064
Validation loss: 2.7695527973995415

Epoch: 5| Step: 1
Training loss: 3.2867207527160645
Validation loss: 2.773066243817729

Epoch: 5| Step: 2
Training loss: 3.171555280685425
Validation loss: 2.7712023796573764

Epoch: 5| Step: 3
Training loss: 3.1577200889587402
Validation loss: 2.766614111520911

Epoch: 5| Step: 4
Training loss: 2.4644737243652344
Validation loss: 2.769568920135498

Epoch: 5| Step: 5
Training loss: 2.0264556407928467
Validation loss: 2.7643854797527356

Epoch: 5| Step: 6
Training loss: 3.3161277770996094
Validation loss: 2.7682445177467923

Epoch: 5| Step: 7
Training loss: 2.753173351287842
Validation loss: 2.7680981877029582

Epoch: 5| Step: 8
Training loss: 2.2789337635040283
Validation loss: 2.7662275555313274

Epoch: 5| Step: 9
Training loss: 3.6262142658233643
Validation loss: 2.768667264651227

Epoch: 5| Step: 10
Training loss: 3.5926244258880615
Validation loss: 2.768610172374274

Epoch: 32| Step: 0
Training loss: 2.80794095993042
Validation loss: 2.767460374421971

Epoch: 5| Step: 1
Training loss: 2.720125436782837
Validation loss: 2.76753980113614

Epoch: 5| Step: 2
Training loss: 3.2869460582733154
Validation loss: 2.765540774150561

Epoch: 5| Step: 3
Training loss: 3.041893482208252
Validation loss: 2.7620998967078423

Epoch: 5| Step: 4
Training loss: 2.7016005516052246
Validation loss: 2.76680189307018

Epoch: 5| Step: 5
Training loss: 2.748715877532959
Validation loss: 2.767034407584898

Epoch: 5| Step: 6
Training loss: 3.746980667114258
Validation loss: 2.7668976271024315

Epoch: 5| Step: 7
Training loss: 2.6081509590148926
Validation loss: 2.765789383201189

Epoch: 5| Step: 8
Training loss: 3.1198456287384033
Validation loss: 2.758381243674986

Epoch: 5| Step: 9
Training loss: 2.8828728199005127
Validation loss: 2.7611251954109437

Epoch: 5| Step: 10
Training loss: 2.076389789581299
Validation loss: 2.7626859193207114

Epoch: 33| Step: 0
Training loss: 2.494812250137329
Validation loss: 2.791187876014299

Epoch: 5| Step: 1
Training loss: 3.976346492767334
Validation loss: 2.801261176345169

Epoch: 5| Step: 2
Training loss: 2.3532066345214844
Validation loss: 2.8007040433986212

Epoch: 5| Step: 3
Training loss: 3.3156394958496094
Validation loss: 2.7708327770233154

Epoch: 5| Step: 4
Training loss: 2.5782599449157715
Validation loss: 2.7563493046709286

Epoch: 5| Step: 5
Training loss: 3.5742695331573486
Validation loss: 2.761195598110076

Epoch: 5| Step: 6
Training loss: 2.3792929649353027
Validation loss: 2.7696671511537287

Epoch: 5| Step: 7
Training loss: 3.4331278800964355
Validation loss: 2.760537624359131

Epoch: 5| Step: 8
Training loss: 2.5757744312286377
Validation loss: 2.758512700757673

Epoch: 5| Step: 9
Training loss: 2.0333399772644043
Validation loss: 2.7531278543574835

Epoch: 5| Step: 10
Training loss: 3.288543462753296
Validation loss: 2.7595811582380727

Epoch: 34| Step: 0
Training loss: 2.8642375469207764
Validation loss: 2.7590040160763647

Epoch: 5| Step: 1
Training loss: 3.4608540534973145
Validation loss: 2.7641309922741306

Epoch: 5| Step: 2
Training loss: 2.894608497619629
Validation loss: 2.762173327066565

Epoch: 5| Step: 3
Training loss: 3.079848527908325
Validation loss: 2.757716335276122

Epoch: 5| Step: 4
Training loss: 3.152621269226074
Validation loss: 2.7629040851387927

Epoch: 5| Step: 5
Training loss: 2.9290707111358643
Validation loss: 2.7565628072266937

Epoch: 5| Step: 6
Training loss: 3.0302534103393555
Validation loss: 2.7526542858410905

Epoch: 5| Step: 7
Training loss: 2.4046998023986816
Validation loss: 2.751576590281661

Epoch: 5| Step: 8
Training loss: 2.1283178329467773
Validation loss: 2.7489661708954842

Epoch: 5| Step: 9
Training loss: 2.7283976078033447
Validation loss: 2.746257589709374

Epoch: 5| Step: 10
Training loss: 3.193904161453247
Validation loss: 2.755401237036592

Epoch: 35| Step: 0
Training loss: 3.356682538986206
Validation loss: 2.760272805408765

Epoch: 5| Step: 1
Training loss: 2.8122220039367676
Validation loss: 2.7536930089355796

Epoch: 5| Step: 2
Training loss: 2.722352981567383
Validation loss: 2.750703024607833

Epoch: 5| Step: 3
Training loss: 2.1854372024536133
Validation loss: 2.7535694645297144

Epoch: 5| Step: 4
Training loss: 2.887145519256592
Validation loss: 2.7486879517955165

Epoch: 5| Step: 5
Training loss: 2.6644978523254395
Validation loss: 2.7485097274985364

Epoch: 5| Step: 6
Training loss: 3.3735058307647705
Validation loss: 2.746888881088585

Epoch: 5| Step: 7
Training loss: 3.2834649085998535
Validation loss: 2.746428846031107

Epoch: 5| Step: 8
Training loss: 2.571164608001709
Validation loss: 2.7463411772122948

Epoch: 5| Step: 9
Training loss: 2.938291549682617
Validation loss: 2.7469399770100913

Epoch: 5| Step: 10
Training loss: 2.9398488998413086
Validation loss: 2.7483375251934095

Epoch: 36| Step: 0
Training loss: 3.254703998565674
Validation loss: 2.754121372776647

Epoch: 5| Step: 1
Training loss: 2.4400899410247803
Validation loss: 2.7540976206461587

Epoch: 5| Step: 2
Training loss: 2.3940091133117676
Validation loss: 2.747548859606507

Epoch: 5| Step: 3
Training loss: 3.1579477787017822
Validation loss: 2.7482835938853603

Epoch: 5| Step: 4
Training loss: 2.5414364337921143
Validation loss: 2.7574396620514574

Epoch: 5| Step: 5
Training loss: 2.71348237991333
Validation loss: 2.758303939655263

Epoch: 5| Step: 6
Training loss: 2.6963000297546387
Validation loss: 2.759669288512199

Epoch: 5| Step: 7
Training loss: 3.167684555053711
Validation loss: 2.7433883336282547

Epoch: 5| Step: 8
Training loss: 3.70111346244812
Validation loss: 2.7478303217118785

Epoch: 5| Step: 9
Training loss: 2.762397527694702
Validation loss: 2.7489106270574752

Epoch: 5| Step: 10
Training loss: 2.8034136295318604
Validation loss: 2.7414449107262397

Epoch: 37| Step: 0
Training loss: 2.9612953662872314
Validation loss: 2.7446267297191005

Epoch: 5| Step: 1
Training loss: 3.7370903491973877
Validation loss: 2.744283365946944

Epoch: 5| Step: 2
Training loss: 2.582610607147217
Validation loss: 2.7384381319886897

Epoch: 5| Step: 3
Training loss: 3.011890411376953
Validation loss: 2.745008965974213

Epoch: 5| Step: 4
Training loss: 2.70995831489563
Validation loss: 2.740647546706661

Epoch: 5| Step: 5
Training loss: 3.3439571857452393
Validation loss: 2.7383157271210865

Epoch: 5| Step: 6
Training loss: 2.0351903438568115
Validation loss: 2.7540444045938473

Epoch: 5| Step: 7
Training loss: 2.8704850673675537
Validation loss: 2.780349426372077

Epoch: 5| Step: 8
Training loss: 2.9294159412384033
Validation loss: 2.7679078143130065

Epoch: 5| Step: 9
Training loss: 2.9739043712615967
Validation loss: 2.7387752763686644

Epoch: 5| Step: 10
Training loss: 2.5960617065429688
Validation loss: 2.7311358144206386

Epoch: 38| Step: 0
Training loss: 2.988767147064209
Validation loss: 2.7281010125273015

Epoch: 5| Step: 1
Training loss: 3.3248677253723145
Validation loss: 2.7293828738633024

Epoch: 5| Step: 2
Training loss: 2.8439974784851074
Validation loss: 2.733345398338892

Epoch: 5| Step: 3
Training loss: 2.3122735023498535
Validation loss: 2.7342149749878915

Epoch: 5| Step: 4
Training loss: 2.6629514694213867
Validation loss: 2.732760114054526

Epoch: 5| Step: 5
Training loss: 2.6577515602111816
Validation loss: 2.7370493745291107

Epoch: 5| Step: 6
Training loss: 2.8073201179504395
Validation loss: 2.7297917822355866

Epoch: 5| Step: 7
Training loss: 4.020639896392822
Validation loss: 2.7288089183069046

Epoch: 5| Step: 8
Training loss: 2.987475872039795
Validation loss: 2.733541762957009

Epoch: 5| Step: 9
Training loss: 2.5655951499938965
Validation loss: 2.7264978680559384

Epoch: 5| Step: 10
Training loss: 2.407484292984009
Validation loss: 2.722570016819944

Epoch: 39| Step: 0
Training loss: 3.5765254497528076
Validation loss: 2.7290061186718684

Epoch: 5| Step: 1
Training loss: 1.7870705127716064
Validation loss: 2.725281297519643

Epoch: 5| Step: 2
Training loss: 2.3086726665496826
Validation loss: 2.7274260854208343

Epoch: 5| Step: 3
Training loss: 2.6694717407226562
Validation loss: 2.7259688223561933

Epoch: 5| Step: 4
Training loss: 2.920706272125244
Validation loss: 2.725919641474242

Epoch: 5| Step: 5
Training loss: 2.899535894393921
Validation loss: 2.723036627615652

Epoch: 5| Step: 6
Training loss: 2.911881446838379
Validation loss: 2.723806063334147

Epoch: 5| Step: 7
Training loss: 3.2709083557128906
Validation loss: 2.722766512183733

Epoch: 5| Step: 8
Training loss: 3.156938076019287
Validation loss: 2.722072693609422

Epoch: 5| Step: 9
Training loss: 3.036708116531372
Validation loss: 2.724593339427825

Epoch: 5| Step: 10
Training loss: 3.0647172927856445
Validation loss: 2.7688086135413057

Epoch: 40| Step: 0
Training loss: 2.728541851043701
Validation loss: 2.73308971107647

Epoch: 5| Step: 1
Training loss: 2.8990442752838135
Validation loss: 2.734856726020895

Epoch: 5| Step: 2
Training loss: 3.457118272781372
Validation loss: 2.7307014798605316

Epoch: 5| Step: 3
Training loss: 3.163604259490967
Validation loss: 2.7329820356061383

Epoch: 5| Step: 4
Training loss: 2.1949689388275146
Validation loss: 2.732070576760077

Epoch: 5| Step: 5
Training loss: 2.380223035812378
Validation loss: 2.731677370686685

Epoch: 5| Step: 6
Training loss: 2.7492246627807617
Validation loss: 2.72406038161247

Epoch: 5| Step: 7
Training loss: 3.1547255516052246
Validation loss: 2.730111368240849

Epoch: 5| Step: 8
Training loss: 2.5567660331726074
Validation loss: 2.7308005440619683

Epoch: 5| Step: 9
Training loss: 3.111177682876587
Validation loss: 2.7397470499879573

Epoch: 5| Step: 10
Training loss: 3.155294179916382
Validation loss: 2.735232207082933

Epoch: 41| Step: 0
Training loss: 2.466637372970581
Validation loss: 2.7362849661099014

Epoch: 5| Step: 1
Training loss: 2.5106940269470215
Validation loss: 2.734816117953229

Epoch: 5| Step: 2
Training loss: 2.3487296104431152
Validation loss: 2.733381435435305

Epoch: 5| Step: 3
Training loss: 3.3023695945739746
Validation loss: 2.734776555850942

Epoch: 5| Step: 4
Training loss: 2.4698405265808105
Validation loss: 2.731056110833281

Epoch: 5| Step: 5
Training loss: 2.5454506874084473
Validation loss: 2.7282443610570764

Epoch: 5| Step: 6
Training loss: 2.930852174758911
Validation loss: 2.723389671694848

Epoch: 5| Step: 7
Training loss: 3.283080577850342
Validation loss: 2.722858544318907

Epoch: 5| Step: 8
Training loss: 2.7832207679748535
Validation loss: 2.7249226134310485

Epoch: 5| Step: 9
Training loss: 3.5681347846984863
Validation loss: 2.7238666754896923

Epoch: 5| Step: 10
Training loss: 3.285703182220459
Validation loss: 2.735528579322241

Epoch: 42| Step: 0
Training loss: 3.1955387592315674
Validation loss: 2.728472048236478

Epoch: 5| Step: 1
Training loss: 2.6643643379211426
Validation loss: 2.727848573397565

Epoch: 5| Step: 2
Training loss: 2.837141752243042
Validation loss: 2.729802836654007

Epoch: 5| Step: 3
Training loss: 2.860755443572998
Validation loss: 2.7208558923454693

Epoch: 5| Step: 4
Training loss: 2.5951530933380127
Validation loss: 2.726955665055142

Epoch: 5| Step: 5
Training loss: 2.4391720294952393
Validation loss: 2.7255765109933834

Epoch: 5| Step: 6
Training loss: 3.45772123336792
Validation loss: 2.7291978020821848

Epoch: 5| Step: 7
Training loss: 3.384469985961914
Validation loss: 2.7249802466361754

Epoch: 5| Step: 8
Training loss: 3.0658812522888184
Validation loss: 2.721742953023603

Epoch: 5| Step: 9
Training loss: 1.835052251815796
Validation loss: 2.7267954246972197

Epoch: 5| Step: 10
Training loss: 3.170703172683716
Validation loss: 2.7245835155569096

Epoch: 43| Step: 0
Training loss: 3.0100255012512207
Validation loss: 2.7191351562417965

Epoch: 5| Step: 1
Training loss: 3.114192247390747
Validation loss: 2.722537571384061

Epoch: 5| Step: 2
Training loss: 2.7795963287353516
Validation loss: 2.732791375088435

Epoch: 5| Step: 3
Training loss: 2.782543182373047
Validation loss: 2.735113349012149

Epoch: 5| Step: 4
Training loss: 2.8318355083465576
Validation loss: 2.7314759531328754

Epoch: 5| Step: 5
Training loss: 3.611894130706787
Validation loss: 2.720019753261279

Epoch: 5| Step: 6
Training loss: 2.37597918510437
Validation loss: 2.720327536265055

Epoch: 5| Step: 7
Training loss: 2.646306037902832
Validation loss: 2.717017078912386

Epoch: 5| Step: 8
Training loss: 2.700852870941162
Validation loss: 2.7156329975333264

Epoch: 5| Step: 9
Training loss: 3.044830799102783
Validation loss: 2.714462875038065

Epoch: 5| Step: 10
Training loss: 2.4461774826049805
Validation loss: 2.7148943793389106

Epoch: 44| Step: 0
Training loss: 2.9363455772399902
Validation loss: 2.7125738564357964

Epoch: 5| Step: 1
Training loss: 2.3909878730773926
Validation loss: 2.716549068368891

Epoch: 5| Step: 2
Training loss: 1.8715522289276123
Validation loss: 2.716944956010388

Epoch: 5| Step: 3
Training loss: 3.7672431468963623
Validation loss: 2.7135446148533977

Epoch: 5| Step: 4
Training loss: 3.0486927032470703
Validation loss: 2.7195266241668374

Epoch: 5| Step: 5
Training loss: 3.021425247192383
Validation loss: 2.7194307568252727

Epoch: 5| Step: 6
Training loss: 2.3228697776794434
Validation loss: 2.716189376769527

Epoch: 5| Step: 7
Training loss: 2.7805328369140625
Validation loss: 2.7118715624655447

Epoch: 5| Step: 8
Training loss: 3.615741014480591
Validation loss: 2.711544167610907

Epoch: 5| Step: 9
Training loss: 2.84224796295166
Validation loss: 2.7122576313634075

Epoch: 5| Step: 10
Training loss: 2.6801013946533203
Validation loss: 2.708858561772172

Epoch: 45| Step: 0
Training loss: 2.968604564666748
Validation loss: 2.7131136976262575

Epoch: 5| Step: 1
Training loss: 2.9197287559509277
Validation loss: 2.7103316963359876

Epoch: 5| Step: 2
Training loss: 2.4584381580352783
Validation loss: 2.7064844664706977

Epoch: 5| Step: 3
Training loss: 2.5050415992736816
Validation loss: 2.707357875762447

Epoch: 5| Step: 4
Training loss: 2.9780774116516113
Validation loss: 2.702888173441733

Epoch: 5| Step: 5
Training loss: 3.6033103466033936
Validation loss: 2.7082137087339997

Epoch: 5| Step: 6
Training loss: 2.952863931655884
Validation loss: 2.7043912180008425

Epoch: 5| Step: 7
Training loss: 2.309497356414795
Validation loss: 2.7051664552380963

Epoch: 5| Step: 8
Training loss: 3.103379726409912
Validation loss: 2.7068190497736775

Epoch: 5| Step: 9
Training loss: 2.3968005180358887
Validation loss: 2.7036204953347482

Epoch: 5| Step: 10
Training loss: 3.088395357131958
Validation loss: 2.7061602761668544

Epoch: 46| Step: 0
Training loss: 2.921015977859497
Validation loss: 2.699339912783715

Epoch: 5| Step: 1
Training loss: 3.4808030128479004
Validation loss: 2.6984494168271302

Epoch: 5| Step: 2
Training loss: 2.3117499351501465
Validation loss: 2.6993322141708864

Epoch: 5| Step: 3
Training loss: 3.178309679031372
Validation loss: 2.699437654146584

Epoch: 5| Step: 4
Training loss: 2.6186699867248535
Validation loss: 2.700973044159592

Epoch: 5| Step: 5
Training loss: 2.9528918266296387
Validation loss: 2.7054821316913893

Epoch: 5| Step: 6
Training loss: 2.2550389766693115
Validation loss: 2.7039253891155286

Epoch: 5| Step: 7
Training loss: 3.2136268615722656
Validation loss: 2.7003260581724104

Epoch: 5| Step: 8
Training loss: 3.091752529144287
Validation loss: 2.700933092383928

Epoch: 5| Step: 9
Training loss: 2.5385243892669678
Validation loss: 2.7055366116185344

Epoch: 5| Step: 10
Training loss: 2.6109066009521484
Validation loss: 2.70516267899544

Epoch: 47| Step: 0
Training loss: 2.7584261894226074
Validation loss: 2.704734650991296

Epoch: 5| Step: 1
Training loss: 3.829272747039795
Validation loss: 2.705645448418074

Epoch: 5| Step: 2
Training loss: 3.087268352508545
Validation loss: 2.7188613773674093

Epoch: 5| Step: 3
Training loss: 3.399354934692383
Validation loss: 2.7261650126467467

Epoch: 5| Step: 4
Training loss: 2.0245423316955566
Validation loss: 2.7079726290959183

Epoch: 5| Step: 5
Training loss: 2.7926602363586426
Validation loss: 2.6992275612328642

Epoch: 5| Step: 6
Training loss: 2.555692195892334
Validation loss: 2.69095354951838

Epoch: 5| Step: 7
Training loss: 3.0816872119903564
Validation loss: 2.6972139394411476

Epoch: 5| Step: 8
Training loss: 2.6316375732421875
Validation loss: 2.708455744610038

Epoch: 5| Step: 9
Training loss: 2.253871202468872
Validation loss: 2.728421588097849

Epoch: 5| Step: 10
Training loss: 2.8391308784484863
Validation loss: 2.7382252088157077

Epoch: 48| Step: 0
Training loss: 3.672532320022583
Validation loss: 2.725811705794386

Epoch: 5| Step: 1
Training loss: 2.39408802986145
Validation loss: 2.7109497618931595

Epoch: 5| Step: 2
Training loss: 2.4951419830322266
Validation loss: 2.7104924750584427

Epoch: 5| Step: 3
Training loss: 2.237769842147827
Validation loss: 2.699479123597504

Epoch: 5| Step: 4
Training loss: 3.4825167655944824
Validation loss: 2.693587423652731

Epoch: 5| Step: 5
Training loss: 3.061405897140503
Validation loss: 2.691356115443732

Epoch: 5| Step: 6
Training loss: 3.1391818523406982
Validation loss: 2.6967919975198726

Epoch: 5| Step: 7
Training loss: 2.610935688018799
Validation loss: 2.708361807689872

Epoch: 5| Step: 8
Training loss: 3.2714500427246094
Validation loss: 2.716282859925301

Epoch: 5| Step: 9
Training loss: 2.3533875942230225
Validation loss: 2.6952742658635622

Epoch: 5| Step: 10
Training loss: 2.4186413288116455
Validation loss: 2.6863130241312008

Epoch: 49| Step: 0
Training loss: 2.424229145050049
Validation loss: 2.688446042358234

Epoch: 5| Step: 1
Training loss: 2.6736323833465576
Validation loss: 2.6909356322339786

Epoch: 5| Step: 2
Training loss: 2.668102741241455
Validation loss: 2.700474113546392

Epoch: 5| Step: 3
Training loss: 2.867968797683716
Validation loss: 2.7103093157532396

Epoch: 5| Step: 4
Training loss: 3.287405014038086
Validation loss: 2.7225661995590373

Epoch: 5| Step: 5
Training loss: 2.941450834274292
Validation loss: 2.7479024048774474

Epoch: 5| Step: 6
Training loss: 3.15889310836792
Validation loss: 2.724129028217767

Epoch: 5| Step: 7
Training loss: 2.198326587677002
Validation loss: 2.698297349355554

Epoch: 5| Step: 8
Training loss: 3.1045193672180176
Validation loss: 2.692934049073086

Epoch: 5| Step: 9
Training loss: 2.6878809928894043
Validation loss: 2.6954612424296718

Epoch: 5| Step: 10
Training loss: 3.3244857788085938
Validation loss: 2.6977063635344147

Epoch: 50| Step: 0
Training loss: 2.47465181350708
Validation loss: 2.704199580736058

Epoch: 5| Step: 1
Training loss: 2.711594343185425
Validation loss: 2.713266134262085

Epoch: 5| Step: 2
Training loss: 2.587599277496338
Validation loss: 2.73794888424617

Epoch: 5| Step: 3
Training loss: 3.4408748149871826
Validation loss: 2.7468870967947026

Epoch: 5| Step: 4
Training loss: 2.6292777061462402
Validation loss: 2.7105654644709762

Epoch: 5| Step: 5
Training loss: 2.8423705101013184
Validation loss: 2.6862593338053715

Epoch: 5| Step: 6
Training loss: 3.417491912841797
Validation loss: 2.682337007214946

Epoch: 5| Step: 7
Training loss: 2.8456921577453613
Validation loss: 2.6894564346600602

Epoch: 5| Step: 8
Training loss: 2.9944560527801514
Validation loss: 2.696071242773405

Epoch: 5| Step: 9
Training loss: 2.6959097385406494
Validation loss: 2.704436881567842

Epoch: 5| Step: 10
Training loss: 2.6259605884552
Validation loss: 2.7054313075157905

Epoch: 51| Step: 0
Training loss: 2.619837760925293
Validation loss: 2.707287703790972

Epoch: 5| Step: 1
Training loss: 2.7633278369903564
Validation loss: 2.693883749746507

Epoch: 5| Step: 2
Training loss: 2.943061351776123
Validation loss: 2.687243256517636

Epoch: 5| Step: 3
Training loss: 2.8055806159973145
Validation loss: 2.6789851906479045

Epoch: 5| Step: 4
Training loss: 2.6650214195251465
Validation loss: 2.6744541686068297

Epoch: 5| Step: 5
Training loss: 2.514514446258545
Validation loss: 2.668992265578239

Epoch: 5| Step: 6
Training loss: 3.0668139457702637
Validation loss: 2.6752845394995903

Epoch: 5| Step: 7
Training loss: 2.6649982929229736
Validation loss: 2.6743138990094586

Epoch: 5| Step: 8
Training loss: 3.14835524559021
Validation loss: 2.6709344438327256

Epoch: 5| Step: 9
Training loss: 3.166935443878174
Validation loss: 2.67160738155406

Epoch: 5| Step: 10
Training loss: 2.746295928955078
Validation loss: 2.6711817377357074

Epoch: 52| Step: 0
Training loss: 2.589562177658081
Validation loss: 2.672363337650094

Epoch: 5| Step: 1
Training loss: 2.137183427810669
Validation loss: 2.670059150265109

Epoch: 5| Step: 2
Training loss: 2.819096088409424
Validation loss: 2.6708656869908816

Epoch: 5| Step: 3
Training loss: 2.3818016052246094
Validation loss: 2.6755736873995875

Epoch: 5| Step: 4
Training loss: 3.0128097534179688
Validation loss: 2.672786546009843

Epoch: 5| Step: 5
Training loss: 2.7702293395996094
Validation loss: 2.6716437391055528

Epoch: 5| Step: 6
Training loss: 3.284090042114258
Validation loss: 2.6783812199869463

Epoch: 5| Step: 7
Training loss: 3.2162864208221436
Validation loss: 2.672107065877607

Epoch: 5| Step: 8
Training loss: 2.680452346801758
Validation loss: 2.6655149485475276

Epoch: 5| Step: 9
Training loss: 3.473893642425537
Validation loss: 2.668330192565918

Epoch: 5| Step: 10
Training loss: 2.6009576320648193
Validation loss: 2.6668252534763788

Epoch: 53| Step: 0
Training loss: 2.7718281745910645
Validation loss: 2.665158571735505

Epoch: 5| Step: 1
Training loss: 2.8327689170837402
Validation loss: 2.668383544491183

Epoch: 5| Step: 2
Training loss: 3.241107940673828
Validation loss: 2.670779561483732

Epoch: 5| Step: 3
Training loss: 2.2017807960510254
Validation loss: 2.6694559230599353

Epoch: 5| Step: 4
Training loss: 3.3623061180114746
Validation loss: 2.6756793324665358

Epoch: 5| Step: 5
Training loss: 2.1491189002990723
Validation loss: 2.669095221386161

Epoch: 5| Step: 6
Training loss: 2.430131435394287
Validation loss: 2.675208391681794

Epoch: 5| Step: 7
Training loss: 2.90447735786438
Validation loss: 2.6718995212226786

Epoch: 5| Step: 8
Training loss: 3.378629207611084
Validation loss: 2.6723330507996264

Epoch: 5| Step: 9
Training loss: 2.4270858764648438
Validation loss: 2.6686783682915474

Epoch: 5| Step: 10
Training loss: 3.321683168411255
Validation loss: 2.6662366672228743

Epoch: 54| Step: 0
Training loss: 2.1912665367126465
Validation loss: 2.6609417751271236

Epoch: 5| Step: 1
Training loss: 2.5872645378112793
Validation loss: 2.6608462000405915

Epoch: 5| Step: 2
Training loss: 2.599855422973633
Validation loss: 2.6695302763292865

Epoch: 5| Step: 3
Training loss: 2.6405134201049805
Validation loss: 2.6765152177503033

Epoch: 5| Step: 4
Training loss: 3.164018392562866
Validation loss: 2.677950818051574

Epoch: 5| Step: 5
Training loss: 3.3103766441345215
Validation loss: 2.678394353517922

Epoch: 5| Step: 6
Training loss: 2.4392693042755127
Validation loss: 2.6728346655445714

Epoch: 5| Step: 7
Training loss: 3.0734660625457764
Validation loss: 2.6726034328501713

Epoch: 5| Step: 8
Training loss: 2.95414137840271
Validation loss: 2.6694791765623194

Epoch: 5| Step: 9
Training loss: 3.094755172729492
Validation loss: 2.6668513513380483

Epoch: 5| Step: 10
Training loss: 2.887042999267578
Validation loss: 2.6670769337684876

Epoch: 55| Step: 0
Training loss: 2.643979787826538
Validation loss: 2.6626445349826606

Epoch: 5| Step: 1
Training loss: 3.6514194011688232
Validation loss: 2.663358344826647

Epoch: 5| Step: 2
Training loss: 2.2817039489746094
Validation loss: 2.6657229674759733

Epoch: 5| Step: 3
Training loss: 2.8655762672424316
Validation loss: 2.6680192742296445

Epoch: 5| Step: 4
Training loss: 3.3335964679718018
Validation loss: 2.6682443464955976

Epoch: 5| Step: 5
Training loss: 1.9966869354248047
Validation loss: 2.6696582225061234

Epoch: 5| Step: 6
Training loss: 2.3501782417297363
Validation loss: 2.6661611705697994

Epoch: 5| Step: 7
Training loss: 3.0583789348602295
Validation loss: 2.667427747480331

Epoch: 5| Step: 8
Training loss: 3.3791019916534424
Validation loss: 2.6658342064067884

Epoch: 5| Step: 9
Training loss: 2.308711290359497
Validation loss: 2.6633637720538723

Epoch: 5| Step: 10
Training loss: 3.0577316284179688
Validation loss: 2.6607682038378972

Epoch: 56| Step: 0
Training loss: 3.574260711669922
Validation loss: 2.658539100359845

Epoch: 5| Step: 1
Training loss: 2.9432342052459717
Validation loss: 2.6588128535978255

Epoch: 5| Step: 2
Training loss: 2.851154327392578
Validation loss: 2.661477704201975

Epoch: 5| Step: 3
Training loss: 2.970216751098633
Validation loss: 2.66240632149481

Epoch: 5| Step: 4
Training loss: 2.6335041522979736
Validation loss: 2.6685312025008665

Epoch: 5| Step: 5
Training loss: 2.5503132343292236
Validation loss: 2.6656490936074206

Epoch: 5| Step: 6
Training loss: 2.8987832069396973
Validation loss: 2.671393514961325

Epoch: 5| Step: 7
Training loss: 3.0878214836120605
Validation loss: 2.6725342812076693

Epoch: 5| Step: 8
Training loss: 2.436394214630127
Validation loss: 2.6667422402289604

Epoch: 5| Step: 9
Training loss: 2.617513656616211
Validation loss: 2.66505935371563

Epoch: 5| Step: 10
Training loss: 2.159580707550049
Validation loss: 2.6603970245648454

Epoch: 57| Step: 0
Training loss: 1.9807239770889282
Validation loss: 2.6565092866138746

Epoch: 5| Step: 1
Training loss: 2.7098464965820312
Validation loss: 2.660682039876138

Epoch: 5| Step: 2
Training loss: 3.4421916007995605
Validation loss: 2.653178926437132

Epoch: 5| Step: 3
Training loss: 2.3835299015045166
Validation loss: 2.652925637460524

Epoch: 5| Step: 4
Training loss: 3.1463561058044434
Validation loss: 2.652456934734057

Epoch: 5| Step: 5
Training loss: 2.3470449447631836
Validation loss: 2.6563152292723298

Epoch: 5| Step: 6
Training loss: 2.702364206314087
Validation loss: 2.6552360570558937

Epoch: 5| Step: 7
Training loss: 2.7722465991973877
Validation loss: 2.659135239098662

Epoch: 5| Step: 8
Training loss: 3.5057952404022217
Validation loss: 2.660837827190276

Epoch: 5| Step: 9
Training loss: 2.5266966819763184
Validation loss: 2.663875270915288

Epoch: 5| Step: 10
Training loss: 3.307743787765503
Validation loss: 2.6785262682104625

Epoch: 58| Step: 0
Training loss: 3.2575745582580566
Validation loss: 2.6575784555045505

Epoch: 5| Step: 1
Training loss: 2.8418216705322266
Validation loss: 2.6513457452097247

Epoch: 5| Step: 2
Training loss: 2.727294683456421
Validation loss: 2.6480986456717215

Epoch: 5| Step: 3
Training loss: 2.9137320518493652
Validation loss: 2.6505003360009964

Epoch: 5| Step: 4
Training loss: 2.6090190410614014
Validation loss: 2.650577760511829

Epoch: 5| Step: 5
Training loss: 2.8060781955718994
Validation loss: 2.652051225785286

Epoch: 5| Step: 6
Training loss: 2.2892065048217773
Validation loss: 2.6527227637588338

Epoch: 5| Step: 7
Training loss: 2.673675537109375
Validation loss: 2.660116252078805

Epoch: 5| Step: 8
Training loss: 2.4489293098449707
Validation loss: 2.655745288377167

Epoch: 5| Step: 9
Training loss: 3.158565044403076
Validation loss: 2.6581958647697204

Epoch: 5| Step: 10
Training loss: 3.1059343814849854
Validation loss: 2.6512654417304584

Epoch: 59| Step: 0
Training loss: 2.2391037940979004
Validation loss: 2.6487543454734226

Epoch: 5| Step: 1
Training loss: 3.1676840782165527
Validation loss: 2.6470807829210834

Epoch: 5| Step: 2
Training loss: 2.917222023010254
Validation loss: 2.651295513235113

Epoch: 5| Step: 3
Training loss: 2.8327910900115967
Validation loss: 2.6502664422476165

Epoch: 5| Step: 4
Training loss: 2.2941737174987793
Validation loss: 2.6504408954292216

Epoch: 5| Step: 5
Training loss: 2.5609593391418457
Validation loss: 2.647795174711494

Epoch: 5| Step: 6
Training loss: 3.1745104789733887
Validation loss: 2.650019463672433

Epoch: 5| Step: 7
Training loss: 2.611462354660034
Validation loss: 2.6491789997264905

Epoch: 5| Step: 8
Training loss: 3.042112350463867
Validation loss: 2.6481294862685667

Epoch: 5| Step: 9
Training loss: 3.0298476219177246
Validation loss: 2.652866591689407

Epoch: 5| Step: 10
Training loss: 2.7666914463043213
Validation loss: 2.6545783781236216

Epoch: 60| Step: 0
Training loss: 2.174332618713379
Validation loss: 2.657222055619763

Epoch: 5| Step: 1
Training loss: 2.8512322902679443
Validation loss: 2.6552427763579995

Epoch: 5| Step: 2
Training loss: 3.1982979774475098
Validation loss: 2.6503416953548307

Epoch: 5| Step: 3
Training loss: 2.6244099140167236
Validation loss: 2.6522890213997132

Epoch: 5| Step: 4
Training loss: 2.358745574951172
Validation loss: 2.6585090134733464

Epoch: 5| Step: 5
Training loss: 2.292375087738037
Validation loss: 2.647324736400317

Epoch: 5| Step: 6
Training loss: 3.277836322784424
Validation loss: 2.6439198755448863

Epoch: 5| Step: 7
Training loss: 2.946892261505127
Validation loss: 2.6445141428260395

Epoch: 5| Step: 8
Training loss: 2.843487501144409
Validation loss: 2.6477271151799027

Epoch: 5| Step: 9
Training loss: 3.206244707107544
Validation loss: 2.658647278303741

Epoch: 5| Step: 10
Training loss: 2.8812592029571533
Validation loss: 2.6570059740415184

Epoch: 61| Step: 0
Training loss: 2.4090375900268555
Validation loss: 2.6619496089155956

Epoch: 5| Step: 1
Training loss: 3.4197402000427246
Validation loss: 2.6609268675568285

Epoch: 5| Step: 2
Training loss: 2.8208084106445312
Validation loss: 2.649781729585381

Epoch: 5| Step: 3
Training loss: 2.688049077987671
Validation loss: 2.6383295930841917

Epoch: 5| Step: 4
Training loss: 3.1121132373809814
Validation loss: 2.641132426518266

Epoch: 5| Step: 5
Training loss: 3.0541744232177734
Validation loss: 2.637116568062895

Epoch: 5| Step: 6
Training loss: 2.255875587463379
Validation loss: 2.6341411682867233

Epoch: 5| Step: 7
Training loss: 2.536482095718384
Validation loss: 2.638698888081376

Epoch: 5| Step: 8
Training loss: 2.7465994358062744
Validation loss: 2.6360133309518137

Epoch: 5| Step: 9
Training loss: 2.90177583694458
Validation loss: 2.638590076918243

Epoch: 5| Step: 10
Training loss: 2.740457534790039
Validation loss: 2.63796692509805

Epoch: 62| Step: 0
Training loss: 2.5322842597961426
Validation loss: 2.637556832323792

Epoch: 5| Step: 1
Training loss: 2.2183895111083984
Validation loss: 2.6346416486206876

Epoch: 5| Step: 2
Training loss: 3.0571742057800293
Validation loss: 2.6356231525380123

Epoch: 5| Step: 3
Training loss: 3.32539701461792
Validation loss: 2.63735024134318

Epoch: 5| Step: 4
Training loss: 3.534829616546631
Validation loss: 2.636844327372889

Epoch: 5| Step: 5
Training loss: 3.264450788497925
Validation loss: 2.6332147685430383

Epoch: 5| Step: 6
Training loss: 1.9732933044433594
Validation loss: 2.6341719447925525

Epoch: 5| Step: 7
Training loss: 2.056957721710205
Validation loss: 2.636889401302543

Epoch: 5| Step: 8
Training loss: 3.059865951538086
Validation loss: 2.63088850052126

Epoch: 5| Step: 9
Training loss: 2.799036741256714
Validation loss: 2.630744077826059

Epoch: 5| Step: 10
Training loss: 2.7103967666625977
Validation loss: 2.6304553554904078

Epoch: 63| Step: 0
Training loss: 2.791639804840088
Validation loss: 2.62931736053959

Epoch: 5| Step: 1
Training loss: 2.7149946689605713
Validation loss: 2.6271125193565124

Epoch: 5| Step: 2
Training loss: 2.876807689666748
Validation loss: 2.6288508779259137

Epoch: 5| Step: 3
Training loss: 2.2867014408111572
Validation loss: 2.6288792164094987

Epoch: 5| Step: 4
Training loss: 1.9768003225326538
Validation loss: 2.6264542405323317

Epoch: 5| Step: 5
Training loss: 3.267023801803589
Validation loss: 2.626034526414769

Epoch: 5| Step: 6
Training loss: 3.236271619796753
Validation loss: 2.622027802210982

Epoch: 5| Step: 7
Training loss: 2.8732376098632812
Validation loss: 2.6236031439996537

Epoch: 5| Step: 8
Training loss: 2.9585280418395996
Validation loss: 2.6226126763128463

Epoch: 5| Step: 9
Training loss: 3.064746141433716
Validation loss: 2.622401665615779

Epoch: 5| Step: 10
Training loss: 2.3961381912231445
Validation loss: 2.6210484607245332

Epoch: 64| Step: 0
Training loss: 2.964567184448242
Validation loss: 2.6220518209600963

Epoch: 5| Step: 1
Training loss: 3.226868152618408
Validation loss: 2.6184293198329147

Epoch: 5| Step: 2
Training loss: 2.3581135272979736
Validation loss: 2.621770202472646

Epoch: 5| Step: 3
Training loss: 2.811734676361084
Validation loss: 2.622502406438192

Epoch: 5| Step: 4
Training loss: 2.6243133544921875
Validation loss: 2.6224305706639446

Epoch: 5| Step: 5
Training loss: 2.266281843185425
Validation loss: 2.623989215461157

Epoch: 5| Step: 6
Training loss: 3.1318461894989014
Validation loss: 2.6233796099180817

Epoch: 5| Step: 7
Training loss: 2.893336057662964
Validation loss: 2.6218889913251324

Epoch: 5| Step: 8
Training loss: 2.613623857498169
Validation loss: 2.6216189579297136

Epoch: 5| Step: 9
Training loss: 2.4325671195983887
Validation loss: 2.6197940457251763

Epoch: 5| Step: 10
Training loss: 3.2204833030700684
Validation loss: 2.6181412973711566

Epoch: 65| Step: 0
Training loss: 2.5373339653015137
Validation loss: 2.6191495990240448

Epoch: 5| Step: 1
Training loss: 3.517163038253784
Validation loss: 2.620312116479361

Epoch: 5| Step: 2
Training loss: 2.4975833892822266
Validation loss: 2.620956177352577

Epoch: 5| Step: 3
Training loss: 2.6481876373291016
Validation loss: 2.6177294279939387

Epoch: 5| Step: 4
Training loss: 2.9072489738464355
Validation loss: 2.6182293686815488

Epoch: 5| Step: 5
Training loss: 3.062481641769409
Validation loss: 2.6194536045033443

Epoch: 5| Step: 6
Training loss: 2.2970645427703857
Validation loss: 2.6213488014795447

Epoch: 5| Step: 7
Training loss: 2.9539291858673096
Validation loss: 2.6222263407963577

Epoch: 5| Step: 8
Training loss: 2.9082529544830322
Validation loss: 2.6181755732464533

Epoch: 5| Step: 9
Training loss: 2.099778413772583
Validation loss: 2.6210656499349945

Epoch: 5| Step: 10
Training loss: 3.0742411613464355
Validation loss: 2.6243663782714517

Epoch: 66| Step: 0
Training loss: 2.825438976287842
Validation loss: 2.6185660618607716

Epoch: 5| Step: 1
Training loss: 2.351391315460205
Validation loss: 2.624054789543152

Epoch: 5| Step: 2
Training loss: 2.316074848175049
Validation loss: 2.6180853843688965

Epoch: 5| Step: 3
Training loss: 3.6225037574768066
Validation loss: 2.6161298187830115

Epoch: 5| Step: 4
Training loss: 3.2975621223449707
Validation loss: 2.6181352881975073

Epoch: 5| Step: 5
Training loss: 2.500487804412842
Validation loss: 2.619076536547753

Epoch: 5| Step: 6
Training loss: 2.637813091278076
Validation loss: 2.614842907074959

Epoch: 5| Step: 7
Training loss: 2.5239462852478027
Validation loss: 2.6142931112679104

Epoch: 5| Step: 8
Training loss: 2.808737277984619
Validation loss: 2.6085054387328444

Epoch: 5| Step: 9
Training loss: 2.679234743118286
Validation loss: 2.614637062113772

Epoch: 5| Step: 10
Training loss: 2.859621286392212
Validation loss: 2.613561204684678

Epoch: 67| Step: 0
Training loss: 3.0471408367156982
Validation loss: 2.610878339377783

Epoch: 5| Step: 1
Training loss: 3.135148286819458
Validation loss: 2.6077768853915635

Epoch: 5| Step: 2
Training loss: 2.306344985961914
Validation loss: 2.6127258808382097

Epoch: 5| Step: 3
Training loss: 3.7704708576202393
Validation loss: 2.609943918002549

Epoch: 5| Step: 4
Training loss: 2.607536792755127
Validation loss: 2.6102322993739957

Epoch: 5| Step: 5
Training loss: 2.692349433898926
Validation loss: 2.6131734130203084

Epoch: 5| Step: 6
Training loss: 2.9456589221954346
Validation loss: 2.6084337542133946

Epoch: 5| Step: 7
Training loss: 2.4930598735809326
Validation loss: 2.608352912369595

Epoch: 5| Step: 8
Training loss: 2.431525230407715
Validation loss: 2.6047008986114175

Epoch: 5| Step: 9
Training loss: 2.510258436203003
Validation loss: 2.60309918977881

Epoch: 5| Step: 10
Training loss: 2.3636746406555176
Validation loss: 2.603847716444282

Epoch: 68| Step: 0
Training loss: 2.423241138458252
Validation loss: 2.606744776489914

Epoch: 5| Step: 1
Training loss: 2.7497544288635254
Validation loss: 2.605315633999404

Epoch: 5| Step: 2
Training loss: 2.235506057739258
Validation loss: 2.60823501822769

Epoch: 5| Step: 3
Training loss: 3.0675454139709473
Validation loss: 2.6120835632406254

Epoch: 5| Step: 4
Training loss: 3.439638137817383
Validation loss: 2.6080435193994993

Epoch: 5| Step: 5
Training loss: 2.517916202545166
Validation loss: 2.610120834842805

Epoch: 5| Step: 6
Training loss: 2.2512974739074707
Validation loss: 2.6153587551527124

Epoch: 5| Step: 7
Training loss: 3.177802562713623
Validation loss: 2.6100806010666715

Epoch: 5| Step: 8
Training loss: 2.9668872356414795
Validation loss: 2.606105935189032

Epoch: 5| Step: 9
Training loss: 2.97780442237854
Validation loss: 2.603199666546237

Epoch: 5| Step: 10
Training loss: 2.4965732097625732
Validation loss: 2.598281278405138

Epoch: 69| Step: 0
Training loss: 2.3662071228027344
Validation loss: 2.606396528982347

Epoch: 5| Step: 1
Training loss: 3.4937586784362793
Validation loss: 2.6095405214576313

Epoch: 5| Step: 2
Training loss: 2.984691858291626
Validation loss: 2.6086311750514533

Epoch: 5| Step: 3
Training loss: 2.8966140747070312
Validation loss: 2.6086278884641585

Epoch: 5| Step: 4
Training loss: 2.637718677520752
Validation loss: 2.620317123269522

Epoch: 5| Step: 5
Training loss: 2.9099490642547607
Validation loss: 2.6114810384729856

Epoch: 5| Step: 6
Training loss: 2.483057975769043
Validation loss: 2.598792940057734

Epoch: 5| Step: 7
Training loss: 2.4170925617218018
Validation loss: 2.599164001403316

Epoch: 5| Step: 8
Training loss: 2.792050361633301
Validation loss: 2.6008267684649398

Epoch: 5| Step: 9
Training loss: 2.6661572456359863
Validation loss: 2.5995454454934723

Epoch: 5| Step: 10
Training loss: 2.7445623874664307
Validation loss: 2.6003514438547115

Epoch: 70| Step: 0
Training loss: 2.472412109375
Validation loss: 2.606713843602006

Epoch: 5| Step: 1
Training loss: 2.8944804668426514
Validation loss: 2.609054532102359

Epoch: 5| Step: 2
Training loss: 3.055651903152466
Validation loss: 2.599980585036739

Epoch: 5| Step: 3
Training loss: 3.1522083282470703
Validation loss: 2.596940948117164

Epoch: 5| Step: 4
Training loss: 3.0119361877441406
Validation loss: 2.594385495749853

Epoch: 5| Step: 5
Training loss: 2.7135512828826904
Validation loss: 2.595328915503717

Epoch: 5| Step: 6
Training loss: 2.646174669265747
Validation loss: 2.59172986912471

Epoch: 5| Step: 7
Training loss: 2.632352113723755
Validation loss: 2.5893677588432067

Epoch: 5| Step: 8
Training loss: 2.9944040775299072
Validation loss: 2.5937224306086057

Epoch: 5| Step: 9
Training loss: 1.8478116989135742
Validation loss: 2.5947447053847776

Epoch: 5| Step: 10
Training loss: 2.84417724609375
Validation loss: 2.59034296261367

Epoch: 71| Step: 0
Training loss: 2.531515121459961
Validation loss: 2.5923958042616486

Epoch: 5| Step: 1
Training loss: 2.3037047386169434
Validation loss: 2.591896362202142

Epoch: 5| Step: 2
Training loss: 2.942143440246582
Validation loss: 2.5898483901895504

Epoch: 5| Step: 3
Training loss: 2.5359065532684326
Validation loss: 2.590254522139026

Epoch: 5| Step: 4
Training loss: 2.3097660541534424
Validation loss: 2.5915294795907955

Epoch: 5| Step: 5
Training loss: 2.197552442550659
Validation loss: 2.5916699901703866

Epoch: 5| Step: 6
Training loss: 2.9431300163269043
Validation loss: 2.587195575878184

Epoch: 5| Step: 7
Training loss: 3.5538811683654785
Validation loss: 2.5903754285586778

Epoch: 5| Step: 8
Training loss: 2.3895814418792725
Validation loss: 2.5927556945431616

Epoch: 5| Step: 9
Training loss: 3.268031597137451
Validation loss: 2.5902217383025796

Epoch: 5| Step: 10
Training loss: 3.2266101837158203
Validation loss: 2.5873271675520044

Epoch: 72| Step: 0
Training loss: 2.3845746517181396
Validation loss: 2.598920294033584

Epoch: 5| Step: 1
Training loss: 2.8187003135681152
Validation loss: 2.617659822587044

Epoch: 5| Step: 2
Training loss: 2.6136507987976074
Validation loss: 2.64458550689041

Epoch: 5| Step: 3
Training loss: 2.881594181060791
Validation loss: 2.6525942023082445

Epoch: 5| Step: 4
Training loss: 2.6842072010040283
Validation loss: 2.633286006989018

Epoch: 5| Step: 5
Training loss: 2.9493863582611084
Validation loss: 2.588454164484496

Epoch: 5| Step: 6
Training loss: 3.0804624557495117
Validation loss: 2.58625037952136

Epoch: 5| Step: 7
Training loss: 2.4670798778533936
Validation loss: 2.607700576064407

Epoch: 5| Step: 8
Training loss: 2.4397459030151367
Validation loss: 2.628498056883453

Epoch: 5| Step: 9
Training loss: 2.4822516441345215
Validation loss: 2.6351133443975963

Epoch: 5| Step: 10
Training loss: 3.799562454223633
Validation loss: 2.6370976945405364

Epoch: 73| Step: 0
Training loss: 3.260068416595459
Validation loss: 2.63355932440809

Epoch: 5| Step: 1
Training loss: 2.5107884407043457
Validation loss: 2.6178826337219565

Epoch: 5| Step: 2
Training loss: 2.7669360637664795
Validation loss: 2.598606901784097

Epoch: 5| Step: 3
Training loss: 2.852060556411743
Validation loss: 2.5844871254377466

Epoch: 5| Step: 4
Training loss: 3.2272884845733643
Validation loss: 2.584856330707509

Epoch: 5| Step: 5
Training loss: 2.3778235912323
Validation loss: 2.5916301383767077

Epoch: 5| Step: 6
Training loss: 2.4389255046844482
Validation loss: 2.59566907728872

Epoch: 5| Step: 7
Training loss: 2.8631439208984375
Validation loss: 2.6024339737430697

Epoch: 5| Step: 8
Training loss: 2.7916221618652344
Validation loss: 2.599868061721966

Epoch: 5| Step: 9
Training loss: 2.5643534660339355
Validation loss: 2.597098755580123

Epoch: 5| Step: 10
Training loss: 2.6622111797332764
Validation loss: 2.5957392466965543

Epoch: 74| Step: 0
Training loss: 2.5829713344573975
Validation loss: 2.5931590475061888

Epoch: 5| Step: 1
Training loss: 2.6708905696868896
Validation loss: 2.593342468302737

Epoch: 5| Step: 2
Training loss: 2.6295363903045654
Validation loss: 2.591574538138605

Epoch: 5| Step: 3
Training loss: 3.562946319580078
Validation loss: 2.5891153735499226

Epoch: 5| Step: 4
Training loss: 2.655684232711792
Validation loss: 2.581997174088673

Epoch: 5| Step: 5
Training loss: 2.5328478813171387
Validation loss: 2.5801955217956216

Epoch: 5| Step: 6
Training loss: 2.445312976837158
Validation loss: 2.5843409594669136

Epoch: 5| Step: 7
Training loss: 2.5797183513641357
Validation loss: 2.602646097060173

Epoch: 5| Step: 8
Training loss: 3.1133906841278076
Validation loss: 2.6057874874402116

Epoch: 5| Step: 9
Training loss: 2.4951672554016113
Validation loss: 2.6020199791077645

Epoch: 5| Step: 10
Training loss: 3.18060302734375
Validation loss: 2.5994122207805677

Epoch: 75| Step: 0
Training loss: 2.7164158821105957
Validation loss: 2.584314920568979

Epoch: 5| Step: 1
Training loss: 3.1171674728393555
Validation loss: 2.574269192193144

Epoch: 5| Step: 2
Training loss: 2.5162806510925293
Validation loss: 2.576247843362952

Epoch: 5| Step: 3
Training loss: 2.1446661949157715
Validation loss: 2.5763135033269084

Epoch: 5| Step: 4
Training loss: 2.8701062202453613
Validation loss: 2.5843055914807063

Epoch: 5| Step: 5
Training loss: 2.7209248542785645
Validation loss: 2.586875936036469

Epoch: 5| Step: 6
Training loss: 2.7675552368164062
Validation loss: 2.597099757963611

Epoch: 5| Step: 7
Training loss: 2.7398903369903564
Validation loss: 2.5915779811079784

Epoch: 5| Step: 8
Training loss: 2.5606911182403564
Validation loss: 2.587182714093116

Epoch: 5| Step: 9
Training loss: 3.1081337928771973
Validation loss: 2.581111925904469

Epoch: 5| Step: 10
Training loss: 3.042236089706421
Validation loss: 2.579407480455214

Epoch: 76| Step: 0
Training loss: 2.7758779525756836
Validation loss: 2.5848572920727473

Epoch: 5| Step: 1
Training loss: 2.92999005317688
Validation loss: 2.580678624491538

Epoch: 5| Step: 2
Training loss: 3.3288376331329346
Validation loss: 2.587336968350154

Epoch: 5| Step: 3
Training loss: 2.1304450035095215
Validation loss: 2.5876259111589

Epoch: 5| Step: 4
Training loss: 2.190328359603882
Validation loss: 2.5908439851576284

Epoch: 5| Step: 5
Training loss: 2.2539000511169434
Validation loss: 2.5779517440385717

Epoch: 5| Step: 6
Training loss: 2.9252383708953857
Validation loss: 2.5823457830695697

Epoch: 5| Step: 7
Training loss: 2.795405864715576
Validation loss: 2.5867042464594685

Epoch: 5| Step: 8
Training loss: 2.9267961978912354
Validation loss: 2.5887584199187574

Epoch: 5| Step: 9
Training loss: 2.556077718734741
Validation loss: 2.5916699465884956

Epoch: 5| Step: 10
Training loss: 3.4809536933898926
Validation loss: 2.5926152480545865

Epoch: 77| Step: 0
Training loss: 2.7682111263275146
Validation loss: 2.600023602926603

Epoch: 5| Step: 1
Training loss: 3.2333290576934814
Validation loss: 2.5953921707727576

Epoch: 5| Step: 2
Training loss: 2.82844877243042
Validation loss: 2.583528523804039

Epoch: 5| Step: 3
Training loss: 2.368342876434326
Validation loss: 2.57581340882086

Epoch: 5| Step: 4
Training loss: 3.063788890838623
Validation loss: 2.590223358523461

Epoch: 5| Step: 5
Training loss: 2.3394134044647217
Validation loss: 2.5969710760219122

Epoch: 5| Step: 6
Training loss: 3.093426465988159
Validation loss: 2.6233773795507287

Epoch: 5| Step: 7
Training loss: 2.4834797382354736
Validation loss: 2.6143666416086178

Epoch: 5| Step: 8
Training loss: 2.4269537925720215
Validation loss: 2.6589405587924424

Epoch: 5| Step: 9
Training loss: 2.458430767059326
Validation loss: 2.6815922901194584

Epoch: 5| Step: 10
Training loss: 3.3691246509552
Validation loss: 2.6723695775514007

Epoch: 78| Step: 0
Training loss: 3.257948637008667
Validation loss: 2.630927803695843

Epoch: 5| Step: 1
Training loss: 3.273426055908203
Validation loss: 2.595588267490428

Epoch: 5| Step: 2
Training loss: 2.4135098457336426
Validation loss: 2.603567572050197

Epoch: 5| Step: 3
Training loss: 2.5612454414367676
Validation loss: 2.627517987323064

Epoch: 5| Step: 4
Training loss: 2.8924357891082764
Validation loss: 2.6462071890472085

Epoch: 5| Step: 5
Training loss: 2.8075292110443115
Validation loss: 2.652339776357015

Epoch: 5| Step: 6
Training loss: 2.484044075012207
Validation loss: 2.6333729528611705

Epoch: 5| Step: 7
Training loss: 3.113515615463257
Validation loss: 2.630230660079628

Epoch: 5| Step: 8
Training loss: 2.879636764526367
Validation loss: 2.610143553826117

Epoch: 5| Step: 9
Training loss: 2.052403211593628
Validation loss: 2.604903369821528

Epoch: 5| Step: 10
Training loss: 2.7197718620300293
Validation loss: 2.5966392563235376

Epoch: 79| Step: 0
Training loss: 2.9257776737213135
Validation loss: 2.5805892687971874

Epoch: 5| Step: 1
Training loss: 2.9701666831970215
Validation loss: 2.5795432418905277

Epoch: 5| Step: 2
Training loss: 2.7248799800872803
Validation loss: 2.584793575348393

Epoch: 5| Step: 3
Training loss: 2.288853168487549
Validation loss: 2.5870849983666533

Epoch: 5| Step: 4
Training loss: 3.1764798164367676
Validation loss: 2.6113588886876262

Epoch: 5| Step: 5
Training loss: 2.2544045448303223
Validation loss: 2.61087296342337

Epoch: 5| Step: 6
Training loss: 2.6462631225585938
Validation loss: 2.588991524070822

Epoch: 5| Step: 7
Training loss: 2.4721598625183105
Validation loss: 2.574117358012866

Epoch: 5| Step: 8
Training loss: 3.0273962020874023
Validation loss: 2.570151498240809

Epoch: 5| Step: 9
Training loss: 2.09861421585083
Validation loss: 2.560946541447793

Epoch: 5| Step: 10
Training loss: 3.7061381340026855
Validation loss: 2.5555538387708765

Epoch: 80| Step: 0
Training loss: 2.4808526039123535
Validation loss: 2.555932068055676

Epoch: 5| Step: 1
Training loss: 3.0161728858947754
Validation loss: 2.558140552172097

Epoch: 5| Step: 2
Training loss: 2.946443557739258
Validation loss: 2.565899961738176

Epoch: 5| Step: 3
Training loss: 2.530946731567383
Validation loss: 2.565456828763408

Epoch: 5| Step: 4
Training loss: 2.8897347450256348
Validation loss: 2.5722934648554814

Epoch: 5| Step: 5
Training loss: 2.8021793365478516
Validation loss: 2.5825143296231508

Epoch: 5| Step: 6
Training loss: 2.778599262237549
Validation loss: 2.5772231009698685

Epoch: 5| Step: 7
Training loss: 2.4364070892333984
Validation loss: 2.583039468334567

Epoch: 5| Step: 8
Training loss: 2.224820852279663
Validation loss: 2.5770500731724564

Epoch: 5| Step: 9
Training loss: 2.897918462753296
Validation loss: 2.5765944398859495

Epoch: 5| Step: 10
Training loss: 3.158759832382202
Validation loss: 2.5660378599679596

Epoch: 81| Step: 0
Training loss: 2.8746705055236816
Validation loss: 2.553842877828947

Epoch: 5| Step: 1
Training loss: 2.756721019744873
Validation loss: 2.5448998148723314

Epoch: 5| Step: 2
Training loss: 2.9314651489257812
Validation loss: 2.544653900208012

Epoch: 5| Step: 3
Training loss: 2.577631950378418
Validation loss: 2.5437810369717178

Epoch: 5| Step: 4
Training loss: 2.673368215560913
Validation loss: 2.543323911646361

Epoch: 5| Step: 5
Training loss: 2.860569953918457
Validation loss: 2.5445351087918846

Epoch: 5| Step: 6
Training loss: 3.215001344680786
Validation loss: 2.5449251205690446

Epoch: 5| Step: 7
Training loss: 3.233177661895752
Validation loss: 2.542659833867063

Epoch: 5| Step: 8
Training loss: 2.1472811698913574
Validation loss: 2.546298119329637

Epoch: 5| Step: 9
Training loss: 2.290421724319458
Validation loss: 2.5415931209441154

Epoch: 5| Step: 10
Training loss: 2.2482213973999023
Validation loss: 2.5427792508115052

Epoch: 82| Step: 0
Training loss: 2.3505382537841797
Validation loss: 2.5426681144263155

Epoch: 5| Step: 1
Training loss: 3.4394583702087402
Validation loss: 2.542830813315607

Epoch: 5| Step: 2
Training loss: 2.84123158454895
Validation loss: 2.5412323474884033

Epoch: 5| Step: 3
Training loss: 2.7923550605773926
Validation loss: 2.5449107718724076

Epoch: 5| Step: 4
Training loss: 3.01414155960083
Validation loss: 2.549063256991807

Epoch: 5| Step: 5
Training loss: 1.9250843524932861
Validation loss: 2.547891209202428

Epoch: 5| Step: 6
Training loss: 3.0966684818267822
Validation loss: 2.5516131821499077

Epoch: 5| Step: 7
Training loss: 2.7977561950683594
Validation loss: 2.544410567129812

Epoch: 5| Step: 8
Training loss: 2.5357553958892822
Validation loss: 2.5412097541234826

Epoch: 5| Step: 9
Training loss: 2.527968168258667
Validation loss: 2.5392199254805043

Epoch: 5| Step: 10
Training loss: 2.584096908569336
Validation loss: 2.534497058519753

Epoch: 83| Step: 0
Training loss: 3.060750961303711
Validation loss: 2.5379144965961413

Epoch: 5| Step: 1
Training loss: 2.879517078399658
Validation loss: 2.5366575435925554

Epoch: 5| Step: 2
Training loss: 2.973698377609253
Validation loss: 2.54141463259215

Epoch: 5| Step: 3
Training loss: 2.8779168128967285
Validation loss: 2.541465741331859

Epoch: 5| Step: 4
Training loss: 3.0690183639526367
Validation loss: 2.540273307472147

Epoch: 5| Step: 5
Training loss: 2.778932571411133
Validation loss: 2.5428500995841077

Epoch: 5| Step: 6
Training loss: 2.5662474632263184
Validation loss: 2.5480004972027195

Epoch: 5| Step: 7
Training loss: 2.6081089973449707
Validation loss: 2.5497160239886214

Epoch: 5| Step: 8
Training loss: 2.434234857559204
Validation loss: 2.558522352608301

Epoch: 5| Step: 9
Training loss: 2.587806224822998
Validation loss: 2.5504794736062326

Epoch: 5| Step: 10
Training loss: 1.8886792659759521
Validation loss: 2.552405382997246

Epoch: 84| Step: 0
Training loss: 2.5899765491485596
Validation loss: 2.540861427143056

Epoch: 5| Step: 1
Training loss: 2.590916872024536
Validation loss: 2.5435697135104927

Epoch: 5| Step: 2
Training loss: 2.8306031227111816
Validation loss: 2.5435106087756414

Epoch: 5| Step: 3
Training loss: 2.755403995513916
Validation loss: 2.5407687669159262

Epoch: 5| Step: 4
Training loss: 2.6269354820251465
Validation loss: 2.539072113652383

Epoch: 5| Step: 5
Training loss: 2.4610848426818848
Validation loss: 2.540289468662713

Epoch: 5| Step: 6
Training loss: 3.2738442420959473
Validation loss: 2.5427055871614845

Epoch: 5| Step: 7
Training loss: 3.2172484397888184
Validation loss: 2.5395525040165072

Epoch: 5| Step: 8
Training loss: 2.6256914138793945
Validation loss: 2.545826656844026

Epoch: 5| Step: 9
Training loss: 3.078644037246704
Validation loss: 2.5488076620204474

Epoch: 5| Step: 10
Training loss: 1.6277713775634766
Validation loss: 2.543636429694391

Epoch: 85| Step: 0
Training loss: 3.2722530364990234
Validation loss: 2.5340801823523735

Epoch: 5| Step: 1
Training loss: 2.4220356941223145
Validation loss: 2.5384321212768555

Epoch: 5| Step: 2
Training loss: 2.1792099475860596
Validation loss: 2.5382393457556285

Epoch: 5| Step: 3
Training loss: 2.96571683883667
Validation loss: 2.5411951567537043

Epoch: 5| Step: 4
Training loss: 2.238769054412842
Validation loss: 2.5390071356168358

Epoch: 5| Step: 5
Training loss: 2.4139981269836426
Validation loss: 2.5526360593816286

Epoch: 5| Step: 6
Training loss: 3.012387752532959
Validation loss: 2.5873246590296426

Epoch: 5| Step: 7
Training loss: 2.377565622329712
Validation loss: 2.616107266436341

Epoch: 5| Step: 8
Training loss: 2.6641621589660645
Validation loss: 2.6351495353124474

Epoch: 5| Step: 9
Training loss: 3.4277710914611816
Validation loss: 2.634212293932515

Epoch: 5| Step: 10
Training loss: 3.07416033744812
Validation loss: 2.587071203416394

Epoch: 86| Step: 0
Training loss: 2.7513535022735596
Validation loss: 2.5505527065646265

Epoch: 5| Step: 1
Training loss: 2.9754080772399902
Validation loss: 2.532697352029944

Epoch: 5| Step: 2
Training loss: 2.88905668258667
Validation loss: 2.532270303336523

Epoch: 5| Step: 3
Training loss: 2.4963042736053467
Validation loss: 2.540881541467482

Epoch: 5| Step: 4
Training loss: 3.5915184020996094
Validation loss: 2.5522677026769167

Epoch: 5| Step: 5
Training loss: 2.794233798980713
Validation loss: 2.5613014621119343

Epoch: 5| Step: 6
Training loss: 2.643141031265259
Validation loss: 2.568749076576643

Epoch: 5| Step: 7
Training loss: 2.6657283306121826
Validation loss: 2.5861340927821335

Epoch: 5| Step: 8
Training loss: 2.6882002353668213
Validation loss: 2.572061105441022

Epoch: 5| Step: 9
Training loss: 2.4084300994873047
Validation loss: 2.5737234853929087

Epoch: 5| Step: 10
Training loss: 1.8780790567398071
Validation loss: 2.5533516227558093

Epoch: 87| Step: 0
Training loss: 2.264674663543701
Validation loss: 2.5487489136316444

Epoch: 5| Step: 1
Training loss: 3.2199578285217285
Validation loss: 2.5401988849844983

Epoch: 5| Step: 2
Training loss: 2.8851161003112793
Validation loss: 2.540345581628943

Epoch: 5| Step: 3
Training loss: 2.888024091720581
Validation loss: 2.539901646234656

Epoch: 5| Step: 4
Training loss: 2.4577133655548096
Validation loss: 2.5418129044194377

Epoch: 5| Step: 5
Training loss: 2.9867920875549316
Validation loss: 2.538883093864687

Epoch: 5| Step: 6
Training loss: 4.104756832122803
Validation loss: 2.5405182479530253

Epoch: 5| Step: 7
Training loss: 2.344996929168701
Validation loss: 2.5427841012195875

Epoch: 5| Step: 8
Training loss: 1.9368728399276733
Validation loss: 2.533199497448501

Epoch: 5| Step: 9
Training loss: 1.9937565326690674
Validation loss: 2.532100413435249

Epoch: 5| Step: 10
Training loss: 2.7741072177886963
Validation loss: 2.530241120246149

Epoch: 88| Step: 0
Training loss: 3.031144618988037
Validation loss: 2.5326634837735083

Epoch: 5| Step: 1
Training loss: 3.3171744346618652
Validation loss: 2.532771095152824

Epoch: 5| Step: 2
Training loss: 2.650014638900757
Validation loss: 2.5363338788350425

Epoch: 5| Step: 3
Training loss: 2.804810047149658
Validation loss: 2.536530343435144

Epoch: 5| Step: 4
Training loss: 2.9942195415496826
Validation loss: 2.538057937416979

Epoch: 5| Step: 5
Training loss: 2.6438605785369873
Validation loss: 2.5443041452797512

Epoch: 5| Step: 6
Training loss: 2.852520704269409
Validation loss: 2.536786761335147

Epoch: 5| Step: 7
Training loss: 2.7402074337005615
Validation loss: 2.5277978245930006

Epoch: 5| Step: 8
Training loss: 2.609964370727539
Validation loss: 2.523139007629887

Epoch: 5| Step: 9
Training loss: 2.277147054672241
Validation loss: 2.5202387020152104

Epoch: 5| Step: 10
Training loss: 1.8086568117141724
Validation loss: 2.51956844329834

Epoch: 89| Step: 0
Training loss: 3.1123175621032715
Validation loss: 2.5184622246731996

Epoch: 5| Step: 1
Training loss: 2.2921149730682373
Validation loss: 2.522006034851074

Epoch: 5| Step: 2
Training loss: 2.4950246810913086
Validation loss: 2.5186290740966797

Epoch: 5| Step: 3
Training loss: 2.3068320751190186
Validation loss: 2.522392516495079

Epoch: 5| Step: 4
Training loss: 2.5231375694274902
Validation loss: 2.5232015271340646

Epoch: 5| Step: 5
Training loss: 2.597430467605591
Validation loss: 2.5214198071469545

Epoch: 5| Step: 6
Training loss: 2.5248303413391113
Validation loss: 2.5258831490752516

Epoch: 5| Step: 7
Training loss: 2.5798354148864746
Validation loss: 2.523541106972643

Epoch: 5| Step: 8
Training loss: 3.0730020999908447
Validation loss: 2.524863648158248

Epoch: 5| Step: 9
Training loss: 3.037813663482666
Validation loss: 2.51839255902075

Epoch: 5| Step: 10
Training loss: 3.2639222145080566
Validation loss: 2.51524676815156

Epoch: 90| Step: 0
Training loss: 2.9267592430114746
Validation loss: 2.5134610386304956

Epoch: 5| Step: 1
Training loss: 3.058767557144165
Validation loss: 2.5175177615175963

Epoch: 5| Step: 2
Training loss: 2.194671630859375
Validation loss: 2.5124493491265083

Epoch: 5| Step: 3
Training loss: 2.692314386367798
Validation loss: 2.5135502866519395

Epoch: 5| Step: 4
Training loss: 2.4032340049743652
Validation loss: 2.5130620823111585

Epoch: 5| Step: 5
Training loss: 2.1680870056152344
Validation loss: 2.5119027937612226

Epoch: 5| Step: 6
Training loss: 3.2555530071258545
Validation loss: 2.513955498254427

Epoch: 5| Step: 7
Training loss: 2.4396166801452637
Validation loss: 2.5112610196554535

Epoch: 5| Step: 8
Training loss: 2.861419200897217
Validation loss: 2.512433200754145

Epoch: 5| Step: 9
Training loss: 3.282418727874756
Validation loss: 2.511890926668721

Epoch: 5| Step: 10
Training loss: 2.420363664627075
Validation loss: 2.5134535656180432

Epoch: 91| Step: 0
Training loss: 2.6265041828155518
Validation loss: 2.5086896163161083

Epoch: 5| Step: 1
Training loss: 3.171143054962158
Validation loss: 2.513483665322745

Epoch: 5| Step: 2
Training loss: 2.248145341873169
Validation loss: 2.506822334822788

Epoch: 5| Step: 3
Training loss: 2.2890849113464355
Validation loss: 2.510528395252843

Epoch: 5| Step: 4
Training loss: 2.224581480026245
Validation loss: 2.508409925686416

Epoch: 5| Step: 5
Training loss: 2.7385940551757812
Validation loss: 2.50863621311803

Epoch: 5| Step: 6
Training loss: 3.5409462451934814
Validation loss: 2.5072897454743743

Epoch: 5| Step: 7
Training loss: 2.7145416736602783
Validation loss: 2.506843348985077

Epoch: 5| Step: 8
Training loss: 2.314146041870117
Validation loss: 2.510845430435673

Epoch: 5| Step: 9
Training loss: 3.268968105316162
Validation loss: 2.513396291322606

Epoch: 5| Step: 10
Training loss: 2.4719862937927246
Validation loss: 2.5149074010951544

Epoch: 92| Step: 0
Training loss: 2.1340370178222656
Validation loss: 2.512545616396012

Epoch: 5| Step: 1
Training loss: 2.5555591583251953
Validation loss: 2.519373889892332

Epoch: 5| Step: 2
Training loss: 3.3906264305114746
Validation loss: 2.5231125790585756

Epoch: 5| Step: 3
Training loss: 3.1739325523376465
Validation loss: 2.5221785089021087

Epoch: 5| Step: 4
Training loss: 2.7339329719543457
Validation loss: 2.5300738632038073

Epoch: 5| Step: 5
Training loss: 2.4049580097198486
Validation loss: 2.5194527667055846

Epoch: 5| Step: 6
Training loss: 2.682651996612549
Validation loss: 2.5084931517160065

Epoch: 5| Step: 7
Training loss: 2.6757025718688965
Validation loss: 2.5069375371420257

Epoch: 5| Step: 8
Training loss: 2.341672420501709
Validation loss: 2.5039095609418807

Epoch: 5| Step: 9
Training loss: 2.942291736602783
Validation loss: 2.506926787796841

Epoch: 5| Step: 10
Training loss: 2.6584575176239014
Validation loss: 2.50772346732437

Epoch: 93| Step: 0
Training loss: 2.970304489135742
Validation loss: 2.507735113943777

Epoch: 5| Step: 1
Training loss: 2.6558170318603516
Validation loss: 2.5123158834313832

Epoch: 5| Step: 2
Training loss: 2.4431042671203613
Validation loss: 2.5108595714774182

Epoch: 5| Step: 3
Training loss: 2.860527515411377
Validation loss: 2.512218616342032

Epoch: 5| Step: 4
Training loss: 2.77366304397583
Validation loss: 2.5098452439872165

Epoch: 5| Step: 5
Training loss: 3.211498260498047
Validation loss: 2.512808948434809

Epoch: 5| Step: 6
Training loss: 2.3991217613220215
Validation loss: 2.5123617110713834

Epoch: 5| Step: 7
Training loss: 3.242147922515869
Validation loss: 2.513820804575438

Epoch: 5| Step: 8
Training loss: 2.145815372467041
Validation loss: 2.5096064664984263

Epoch: 5| Step: 9
Training loss: 3.1116816997528076
Validation loss: 2.51213821031714

Epoch: 5| Step: 10
Training loss: 1.70621919631958
Validation loss: 2.509378151227069

Epoch: 94| Step: 0
Training loss: 3.004114866256714
Validation loss: 2.5098807555372997

Epoch: 5| Step: 1
Training loss: 2.5805399417877197
Validation loss: 2.507842758650421

Epoch: 5| Step: 2
Training loss: 2.081662654876709
Validation loss: 2.507802924802226

Epoch: 5| Step: 3
Training loss: 3.085493564605713
Validation loss: 2.5017223127426638

Epoch: 5| Step: 4
Training loss: 2.4259610176086426
Validation loss: 2.506794286030595

Epoch: 5| Step: 5
Training loss: 1.634351372718811
Validation loss: 2.5023543091230493

Epoch: 5| Step: 6
Training loss: 2.943458080291748
Validation loss: 2.4999236342727498

Epoch: 5| Step: 7
Training loss: 3.0880963802337646
Validation loss: 2.5055873060739167

Epoch: 5| Step: 8
Training loss: 2.301187038421631
Validation loss: 2.5024362917869323

Epoch: 5| Step: 9
Training loss: 3.90032696723938
Validation loss: 2.5067266815452167

Epoch: 5| Step: 10
Training loss: 2.565642833709717
Validation loss: 2.5010576850624493

Epoch: 95| Step: 0
Training loss: 2.2951366901397705
Validation loss: 2.5015144719872424

Epoch: 5| Step: 1
Training loss: 3.083656072616577
Validation loss: 2.503836424120011

Epoch: 5| Step: 2
Training loss: 2.24460768699646
Validation loss: 2.500653064379128

Epoch: 5| Step: 3
Training loss: 2.6569042205810547
Validation loss: 2.5040133589057514

Epoch: 5| Step: 4
Training loss: 2.9301249980926514
Validation loss: 2.5009355057952223

Epoch: 5| Step: 5
Training loss: 2.288970470428467
Validation loss: 2.4992868669571413

Epoch: 5| Step: 6
Training loss: 2.533977508544922
Validation loss: 2.5036959584041307

Epoch: 5| Step: 7
Training loss: 2.901912212371826
Validation loss: 2.501500229681692

Epoch: 5| Step: 8
Training loss: 3.443373203277588
Validation loss: 2.5034019767597155

Epoch: 5| Step: 9
Training loss: 2.5857632160186768
Validation loss: 2.5036032020404773

Epoch: 5| Step: 10
Training loss: 2.5815420150756836
Validation loss: 2.5053669816704205

Epoch: 96| Step: 0
Training loss: 2.781765937805176
Validation loss: 2.5065006697049705

Epoch: 5| Step: 1
Training loss: 2.826460361480713
Validation loss: 2.5009884372834237

Epoch: 5| Step: 2
Training loss: 3.532027006149292
Validation loss: 2.4983695937741186

Epoch: 5| Step: 3
Training loss: 2.3191497325897217
Validation loss: 2.4954093630595873

Epoch: 5| Step: 4
Training loss: 3.5015182495117188
Validation loss: 2.4979343670670704

Epoch: 5| Step: 5
Training loss: 2.934051036834717
Validation loss: 2.498420374367827

Epoch: 5| Step: 6
Training loss: 2.6575169563293457
Validation loss: 2.492466139537032

Epoch: 5| Step: 7
Training loss: 2.4551594257354736
Validation loss: 2.497567610074115

Epoch: 5| Step: 8
Training loss: 2.0353505611419678
Validation loss: 2.495820822254304

Epoch: 5| Step: 9
Training loss: 2.0162758827209473
Validation loss: 2.4940616443593013

Epoch: 5| Step: 10
Training loss: 2.398942470550537
Validation loss: 2.493395595140355

Epoch: 97| Step: 0
Training loss: 2.4194424152374268
Validation loss: 2.5006871377268145

Epoch: 5| Step: 1
Training loss: 2.804161548614502
Validation loss: 2.495464724879111

Epoch: 5| Step: 2
Training loss: 2.936811923980713
Validation loss: 2.5003758168989614

Epoch: 5| Step: 3
Training loss: 3.087172031402588
Validation loss: 2.5037507190499255

Epoch: 5| Step: 4
Training loss: 2.6155447959899902
Validation loss: 2.504440992109237

Epoch: 5| Step: 5
Training loss: 3.0040392875671387
Validation loss: 2.510496649690854

Epoch: 5| Step: 6
Training loss: 2.601372718811035
Validation loss: 2.516997068159042

Epoch: 5| Step: 7
Training loss: 1.7298576831817627
Validation loss: 2.509835599571146

Epoch: 5| Step: 8
Training loss: 2.3931593894958496
Validation loss: 2.5135883438971733

Epoch: 5| Step: 9
Training loss: 2.9388794898986816
Validation loss: 2.5046865735002743

Epoch: 5| Step: 10
Training loss: 3.1280858516693115
Validation loss: 2.5018398684840046

Epoch: 98| Step: 0
Training loss: 2.345045328140259
Validation loss: 2.500010508362965

Epoch: 5| Step: 1
Training loss: 2.694232940673828
Validation loss: 2.499622839753346

Epoch: 5| Step: 2
Training loss: 3.1021599769592285
Validation loss: 2.5016631080258276

Epoch: 5| Step: 3
Training loss: 2.1990675926208496
Validation loss: 2.4931369058547483

Epoch: 5| Step: 4
Training loss: 2.4870765209198
Validation loss: 2.495848589046027

Epoch: 5| Step: 5
Training loss: 2.232975721359253
Validation loss: 2.495460366690031

Epoch: 5| Step: 6
Training loss: 2.8938660621643066
Validation loss: 2.4971514158351447

Epoch: 5| Step: 7
Training loss: 2.9558701515197754
Validation loss: 2.500310044134817

Epoch: 5| Step: 8
Training loss: 2.817941188812256
Validation loss: 2.5064354224871566

Epoch: 5| Step: 9
Training loss: 3.480644941329956
Validation loss: 2.509910214331842

Epoch: 5| Step: 10
Training loss: 2.128833532333374
Validation loss: 2.5082331370281916

Epoch: 99| Step: 0
Training loss: 2.764423370361328
Validation loss: 2.4992133994256296

Epoch: 5| Step: 1
Training loss: 2.450831651687622
Validation loss: 2.4904415043451453

Epoch: 5| Step: 2
Training loss: 2.786257028579712
Validation loss: 2.492511567249093

Epoch: 5| Step: 3
Training loss: 2.9985203742980957
Validation loss: 2.492844263712565

Epoch: 5| Step: 4
Training loss: 2.874305009841919
Validation loss: 2.4997137900321715

Epoch: 5| Step: 5
Training loss: 2.4858813285827637
Validation loss: 2.505908884027953

Epoch: 5| Step: 6
Training loss: 2.2272074222564697
Validation loss: 2.5051461137751097

Epoch: 5| Step: 7
Training loss: 2.6295852661132812
Validation loss: 2.496607131855462

Epoch: 5| Step: 8
Training loss: 2.1604909896850586
Validation loss: 2.496904186023179

Epoch: 5| Step: 9
Training loss: 3.40788197517395
Validation loss: 2.487600406010946

Epoch: 5| Step: 10
Training loss: 2.7016685009002686
Validation loss: 2.4879086068881455

Epoch: 100| Step: 0
Training loss: 2.219301700592041
Validation loss: 2.4847193738465667

Epoch: 5| Step: 1
Training loss: 2.5722880363464355
Validation loss: 2.4893883248811126

Epoch: 5| Step: 2
Training loss: 2.6021695137023926
Validation loss: 2.4847018129082135

Epoch: 5| Step: 3
Training loss: 2.9453487396240234
Validation loss: 2.485506664040268

Epoch: 5| Step: 4
Training loss: 2.954469680786133
Validation loss: 2.4886336198417087

Epoch: 5| Step: 5
Training loss: 2.600109577178955
Validation loss: 2.489456599758517

Epoch: 5| Step: 6
Training loss: 2.998286008834839
Validation loss: 2.4848316715609644

Epoch: 5| Step: 7
Training loss: 2.0509166717529297
Validation loss: 2.4803147315979004

Epoch: 5| Step: 8
Training loss: 2.374868154525757
Validation loss: 2.4828158373473794

Epoch: 5| Step: 9
Training loss: 2.939199924468994
Validation loss: 2.4807011747872956

Epoch: 5| Step: 10
Training loss: 3.2706592082977295
Validation loss: 2.483753204345703

Epoch: 101| Step: 0
Training loss: 2.4271817207336426
Validation loss: 2.483698634691136

Epoch: 5| Step: 1
Training loss: 2.6734344959259033
Validation loss: 2.4846941271135883

Epoch: 5| Step: 2
Training loss: 2.9665186405181885
Validation loss: 2.487626729472991

Epoch: 5| Step: 3
Training loss: 2.459096670150757
Validation loss: 2.4845701481706355

Epoch: 5| Step: 4
Training loss: 2.2072792053222656
Validation loss: 2.4800268578272995

Epoch: 5| Step: 5
Training loss: 2.5048277378082275
Validation loss: 2.489916214378931

Epoch: 5| Step: 6
Training loss: 2.922661304473877
Validation loss: 2.4939162013351277

Epoch: 5| Step: 7
Training loss: 2.3625330924987793
Validation loss: 2.4878890975829093

Epoch: 5| Step: 8
Training loss: 2.7549450397491455
Validation loss: 2.4862563097348778

Epoch: 5| Step: 9
Training loss: 3.3057682514190674
Validation loss: 2.4845530679149013

Epoch: 5| Step: 10
Training loss: 2.8264429569244385
Validation loss: 2.483526486222462

Epoch: 102| Step: 0
Training loss: 3.84977650642395
Validation loss: 2.4812490171001804

Epoch: 5| Step: 1
Training loss: 3.1376705169677734
Validation loss: 2.4762391300611597

Epoch: 5| Step: 2
Training loss: 2.497959613800049
Validation loss: 2.4759062182518745

Epoch: 5| Step: 3
Training loss: 2.6209490299224854
Validation loss: 2.4819626808166504

Epoch: 5| Step: 4
Training loss: 2.709545612335205
Validation loss: 2.482041089765487

Epoch: 5| Step: 5
Training loss: 2.3904662132263184
Validation loss: 2.4904984043490503

Epoch: 5| Step: 6
Training loss: 2.7901251316070557
Validation loss: 2.48966637990808

Epoch: 5| Step: 7
Training loss: 2.187162399291992
Validation loss: 2.4875255848771785

Epoch: 5| Step: 8
Training loss: 2.0045533180236816
Validation loss: 2.4896228262173232

Epoch: 5| Step: 9
Training loss: 2.6453745365142822
Validation loss: 2.488695098507789

Epoch: 5| Step: 10
Training loss: 2.4376583099365234
Validation loss: 2.494572167755455

Epoch: 103| Step: 0
Training loss: 2.173417806625366
Validation loss: 2.5055828350846485

Epoch: 5| Step: 1
Training loss: 3.0404272079467773
Validation loss: 2.513703771816787

Epoch: 5| Step: 2
Training loss: 2.0003952980041504
Validation loss: 2.5186126385965655

Epoch: 5| Step: 3
Training loss: 2.2909512519836426
Validation loss: 2.5354213253144295

Epoch: 5| Step: 4
Training loss: 2.7393157482147217
Validation loss: 2.5381976545497937

Epoch: 5| Step: 5
Training loss: 3.3032009601593018
Validation loss: 2.5384138758464525

Epoch: 5| Step: 6
Training loss: 2.288381576538086
Validation loss: 2.5220802137928624

Epoch: 5| Step: 7
Training loss: 3.3454761505126953
Validation loss: 2.498148633587745

Epoch: 5| Step: 8
Training loss: 2.767866849899292
Validation loss: 2.4818611452656407

Epoch: 5| Step: 9
Training loss: 2.6402878761291504
Validation loss: 2.5072318482142624

Epoch: 5| Step: 10
Training loss: 2.9626212120056152
Validation loss: 2.5227377953067904

Epoch: 104| Step: 0
Training loss: 2.3682761192321777
Validation loss: 2.5259689643818843

Epoch: 5| Step: 1
Training loss: 2.814178705215454
Validation loss: 2.5292145872628815

Epoch: 5| Step: 2
Training loss: 3.48394775390625
Validation loss: 2.529425423632386

Epoch: 5| Step: 3
Training loss: 2.08683443069458
Validation loss: 2.499474697215583

Epoch: 5| Step: 4
Training loss: 2.517940044403076
Validation loss: 2.4880196766186784

Epoch: 5| Step: 5
Training loss: 2.816866159439087
Validation loss: 2.4743143666175103

Epoch: 5| Step: 6
Training loss: 2.4520816802978516
Validation loss: 2.469584450926832

Epoch: 5| Step: 7
Training loss: 3.5024209022521973
Validation loss: 2.4709254310977076

Epoch: 5| Step: 8
Training loss: 2.579681634902954
Validation loss: 2.472465540773125

Epoch: 5| Step: 9
Training loss: 2.167863130569458
Validation loss: 2.4681333316269742

Epoch: 5| Step: 10
Training loss: 2.7115297317504883
Validation loss: 2.4698966036560717

Epoch: 105| Step: 0
Training loss: 2.721662998199463
Validation loss: 2.467596730878276

Epoch: 5| Step: 1
Training loss: 2.2579102516174316
Validation loss: 2.4680592526671705

Epoch: 5| Step: 2
Training loss: 2.478846549987793
Validation loss: 2.4633195425874446

Epoch: 5| Step: 3
Training loss: 2.414795398712158
Validation loss: 2.4653151471127748

Epoch: 5| Step: 4
Training loss: 2.6728763580322266
Validation loss: 2.4621799889431206

Epoch: 5| Step: 5
Training loss: 2.442821979522705
Validation loss: 2.466083259992702

Epoch: 5| Step: 6
Training loss: 3.6408820152282715
Validation loss: 2.466963114277009

Epoch: 5| Step: 7
Training loss: 2.382065534591675
Validation loss: 2.4660297619399203

Epoch: 5| Step: 8
Training loss: 2.8798985481262207
Validation loss: 2.4665329494783954

Epoch: 5| Step: 9
Training loss: 3.0538949966430664
Validation loss: 2.467377039694017

Epoch: 5| Step: 10
Training loss: 2.3581011295318604
Validation loss: 2.467036524126607

Epoch: 106| Step: 0
Training loss: 2.720757007598877
Validation loss: 2.466788935404952

Epoch: 5| Step: 1
Training loss: 2.8285248279571533
Validation loss: 2.4691594980096303

Epoch: 5| Step: 2
Training loss: 2.317531108856201
Validation loss: 2.4726939816628732

Epoch: 5| Step: 3
Training loss: 2.5612921714782715
Validation loss: 2.478396946384061

Epoch: 5| Step: 4
Training loss: 2.747753620147705
Validation loss: 2.4895126345337077

Epoch: 5| Step: 5
Training loss: 3.3148574829101562
Validation loss: 2.4880476561925744

Epoch: 5| Step: 6
Training loss: 2.5445339679718018
Validation loss: 2.483563338556597

Epoch: 5| Step: 7
Training loss: 2.4863522052764893
Validation loss: 2.476317116009292

Epoch: 5| Step: 8
Training loss: 3.051964521408081
Validation loss: 2.4685901775155017

Epoch: 5| Step: 9
Training loss: 2.40809965133667
Validation loss: 2.4656266935410036

Epoch: 5| Step: 10
Training loss: 2.3956668376922607
Validation loss: 2.463530150792932

Epoch: 107| Step: 0
Training loss: 2.638746738433838
Validation loss: 2.465972444062592

Epoch: 5| Step: 1
Training loss: 3.0320591926574707
Validation loss: 2.4620960348395893

Epoch: 5| Step: 2
Training loss: 2.3260598182678223
Validation loss: 2.4678147454415598

Epoch: 5| Step: 3
Training loss: 2.984189987182617
Validation loss: 2.465832902539161

Epoch: 5| Step: 4
Training loss: 2.530914545059204
Validation loss: 2.469269847357145

Epoch: 5| Step: 5
Training loss: 3.176151990890503
Validation loss: 2.488386138792961

Epoch: 5| Step: 6
Training loss: 2.586857795715332
Validation loss: 2.5072747174129693

Epoch: 5| Step: 7
Training loss: 2.2485127449035645
Validation loss: 2.513668188484766

Epoch: 5| Step: 8
Training loss: 2.6577961444854736
Validation loss: 2.5126176264978226

Epoch: 5| Step: 9
Training loss: 2.5847084522247314
Validation loss: 2.5232189419449016

Epoch: 5| Step: 10
Training loss: 2.6881327629089355
Validation loss: 2.518305934885497

Epoch: 108| Step: 0
Training loss: 2.343860149383545
Validation loss: 2.5055658432745163

Epoch: 5| Step: 1
Training loss: 2.0661699771881104
Validation loss: 2.474999717486802

Epoch: 5| Step: 2
Training loss: 2.3833584785461426
Validation loss: 2.4718818972187657

Epoch: 5| Step: 3
Training loss: 2.4039266109466553
Validation loss: 2.479204668793627

Epoch: 5| Step: 4
Training loss: 3.01596736907959
Validation loss: 2.4800915807806034

Epoch: 5| Step: 5
Training loss: 3.188230514526367
Validation loss: 2.4698577645004436

Epoch: 5| Step: 6
Training loss: 2.7873313426971436
Validation loss: 2.477753661012137

Epoch: 5| Step: 7
Training loss: 2.8251900672912598
Validation loss: 2.481279698751306

Epoch: 5| Step: 8
Training loss: 2.7851200103759766
Validation loss: 2.484863858069143

Epoch: 5| Step: 9
Training loss: 2.754812479019165
Validation loss: 2.484255406164354

Epoch: 5| Step: 10
Training loss: 2.767760753631592
Validation loss: 2.4821065754018803

Epoch: 109| Step: 0
Training loss: 2.329101324081421
Validation loss: 2.477718501962641

Epoch: 5| Step: 1
Training loss: 2.4656338691711426
Validation loss: 2.4662204788577173

Epoch: 5| Step: 2
Training loss: 3.5402674674987793
Validation loss: 2.4607472727375646

Epoch: 5| Step: 3
Training loss: 2.946432113647461
Validation loss: 2.4581868417801394

Epoch: 5| Step: 4
Training loss: 2.5090088844299316
Validation loss: 2.4566413664048716

Epoch: 5| Step: 5
Training loss: 2.8998959064483643
Validation loss: 2.459824662054739

Epoch: 5| Step: 6
Training loss: 2.8798155784606934
Validation loss: 2.4613062591962915

Epoch: 5| Step: 7
Training loss: 2.2831990718841553
Validation loss: 2.466049127681281

Epoch: 5| Step: 8
Training loss: 2.053586006164551
Validation loss: 2.4596545696258545

Epoch: 5| Step: 9
Training loss: 2.821739912033081
Validation loss: 2.4627106317909817

Epoch: 5| Step: 10
Training loss: 2.6361708641052246
Validation loss: 2.4619727673069125

Epoch: 110| Step: 0
Training loss: 2.6028172969818115
Validation loss: 2.4571900547191663

Epoch: 5| Step: 1
Training loss: 2.521284580230713
Validation loss: 2.459395916231217

Epoch: 5| Step: 2
Training loss: 1.7714128494262695
Validation loss: 2.4601356265365437

Epoch: 5| Step: 3
Training loss: 2.6270580291748047
Validation loss: 2.463444232940674

Epoch: 5| Step: 4
Training loss: 3.2624449729919434
Validation loss: 2.460619244524228

Epoch: 5| Step: 5
Training loss: 2.758901834487915
Validation loss: 2.4633639627887356

Epoch: 5| Step: 6
Training loss: 2.893826723098755
Validation loss: 2.465000770425284

Epoch: 5| Step: 7
Training loss: 2.7044355869293213
Validation loss: 2.4636386235555015

Epoch: 5| Step: 8
Training loss: 2.891279458999634
Validation loss: 2.4627623173498336

Epoch: 5| Step: 9
Training loss: 2.7708182334899902
Validation loss: 2.470966072492702

Epoch: 5| Step: 10
Training loss: 2.4501705169677734
Validation loss: 2.467508218621695

Epoch: 111| Step: 0
Training loss: 2.107215404510498
Validation loss: 2.4654931047911286

Epoch: 5| Step: 1
Training loss: 3.1788418292999268
Validation loss: 2.4670929754934003

Epoch: 5| Step: 2
Training loss: 2.176109790802002
Validation loss: 2.461933248786516

Epoch: 5| Step: 3
Training loss: 2.9127399921417236
Validation loss: 2.4732142468934417

Epoch: 5| Step: 4
Training loss: 2.9651923179626465
Validation loss: 2.486579418182373

Epoch: 5| Step: 5
Training loss: 2.3653457164764404
Validation loss: 2.4900229131021807

Epoch: 5| Step: 6
Training loss: 2.3646903038024902
Validation loss: 2.492141128868185

Epoch: 5| Step: 7
Training loss: 2.8585898876190186
Validation loss: 2.4776197735981276

Epoch: 5| Step: 8
Training loss: 2.4220213890075684
Validation loss: 2.464280351515739

Epoch: 5| Step: 9
Training loss: 3.4123306274414062
Validation loss: 2.4623146441675003

Epoch: 5| Step: 10
Training loss: 2.474292278289795
Validation loss: 2.4692834397797943

Epoch: 112| Step: 0
Training loss: 2.778278350830078
Validation loss: 2.4775940936098815

Epoch: 5| Step: 1
Training loss: 2.5104618072509766
Validation loss: 2.4806037769522717

Epoch: 5| Step: 2
Training loss: 2.5466408729553223
Validation loss: 2.4843118062583347

Epoch: 5| Step: 3
Training loss: 2.6784486770629883
Validation loss: 2.4738326175238496

Epoch: 5| Step: 4
Training loss: 2.4498584270477295
Validation loss: 2.469247966684321

Epoch: 5| Step: 5
Training loss: 3.8232052326202393
Validation loss: 2.4691177619400846

Epoch: 5| Step: 6
Training loss: 2.0562071800231934
Validation loss: 2.466528505407354

Epoch: 5| Step: 7
Training loss: 2.5178208351135254
Validation loss: 2.4677622600268294

Epoch: 5| Step: 8
Training loss: 3.0315990447998047
Validation loss: 2.4696062969905075

Epoch: 5| Step: 9
Training loss: 2.808244228363037
Validation loss: 2.4643245409893733

Epoch: 5| Step: 10
Training loss: 1.8920326232910156
Validation loss: 2.462436942644017

Epoch: 113| Step: 0
Training loss: 2.589576005935669
Validation loss: 2.4681117816637923

Epoch: 5| Step: 1
Training loss: 2.2077744007110596
Validation loss: 2.4695723056793213

Epoch: 5| Step: 2
Training loss: 2.719639539718628
Validation loss: 2.45942287547614

Epoch: 5| Step: 3
Training loss: 2.774644136428833
Validation loss: 2.460929129713325

Epoch: 5| Step: 4
Training loss: 2.447878122329712
Validation loss: 2.471382905078191

Epoch: 5| Step: 5
Training loss: 2.76358962059021
Validation loss: 2.4825542819115425

Epoch: 5| Step: 6
Training loss: 2.5558812618255615
Validation loss: 2.4847914967485654

Epoch: 5| Step: 7
Training loss: 2.5591726303100586
Validation loss: 2.488873338186613

Epoch: 5| Step: 8
Training loss: 3.0191922187805176
Validation loss: 2.4763123758377565

Epoch: 5| Step: 9
Training loss: 2.9132473468780518
Validation loss: 2.471887655155633

Epoch: 5| Step: 10
Training loss: 2.6840438842773438
Validation loss: 2.4627361348880235

Epoch: 114| Step: 0
Training loss: 3.0704398155212402
Validation loss: 2.4602803850686676

Epoch: 5| Step: 1
Training loss: 2.259181499481201
Validation loss: 2.453503631776379

Epoch: 5| Step: 2
Training loss: 3.0094895362854004
Validation loss: 2.454575066925377

Epoch: 5| Step: 3
Training loss: 2.412282705307007
Validation loss: 2.4522376880850842

Epoch: 5| Step: 4
Training loss: 2.700270175933838
Validation loss: 2.4562708972602763

Epoch: 5| Step: 5
Training loss: 2.8564095497131348
Validation loss: 2.4584164145172283

Epoch: 5| Step: 6
Training loss: 3.1833605766296387
Validation loss: 2.4546973577109714

Epoch: 5| Step: 7
Training loss: 2.047272205352783
Validation loss: 2.4586361454379175

Epoch: 5| Step: 8
Training loss: 2.0064926147460938
Validation loss: 2.4568828485345326

Epoch: 5| Step: 9
Training loss: 3.2182655334472656
Validation loss: 2.455100820910546

Epoch: 5| Step: 10
Training loss: 2.3378000259399414
Validation loss: 2.4562606657705

Epoch: 115| Step: 0
Training loss: 2.2022690773010254
Validation loss: 2.4539511075583835

Epoch: 5| Step: 1
Training loss: 2.3163394927978516
Validation loss: 2.4625265752115557

Epoch: 5| Step: 2
Training loss: 2.685995578765869
Validation loss: 2.4579901900342715

Epoch: 5| Step: 3
Training loss: 3.027055501937866
Validation loss: 2.468056146816541

Epoch: 5| Step: 4
Training loss: 2.5369396209716797
Validation loss: 2.463897543568765

Epoch: 5| Step: 5
Training loss: 3.1395232677459717
Validation loss: 2.4643174166320474

Epoch: 5| Step: 6
Training loss: 2.308523416519165
Validation loss: 2.4541223356800694

Epoch: 5| Step: 7
Training loss: 3.1292009353637695
Validation loss: 2.4567558996139036

Epoch: 5| Step: 8
Training loss: 2.454625368118286
Validation loss: 2.4603738361789333

Epoch: 5| Step: 9
Training loss: 2.7106773853302
Validation loss: 2.4652260144551597

Epoch: 5| Step: 10
Training loss: 2.609971761703491
Validation loss: 2.4696932761899886

Epoch: 116| Step: 0
Training loss: 2.101229667663574
Validation loss: 2.4636648175536946

Epoch: 5| Step: 1
Training loss: 3.412846803665161
Validation loss: 2.4618911025344685

Epoch: 5| Step: 2
Training loss: 2.604918956756592
Validation loss: 2.4636860560345393

Epoch: 5| Step: 3
Training loss: 2.418297529220581
Validation loss: 2.4638496983435845

Epoch: 5| Step: 4
Training loss: 2.6713740825653076
Validation loss: 2.467619493443479

Epoch: 5| Step: 5
Training loss: 2.1374740600585938
Validation loss: 2.4738732717370473

Epoch: 5| Step: 6
Training loss: 3.0176162719726562
Validation loss: 2.474944435140138

Epoch: 5| Step: 7
Training loss: 2.910984516143799
Validation loss: 2.4703046480814614

Epoch: 5| Step: 8
Training loss: 2.1974802017211914
Validation loss: 2.4541048798509824

Epoch: 5| Step: 9
Training loss: 3.072706460952759
Validation loss: 2.460721251785114

Epoch: 5| Step: 10
Training loss: 2.453592300415039
Validation loss: 2.4569906457777946

Epoch: 117| Step: 0
Training loss: 2.973085403442383
Validation loss: 2.4572070875475482

Epoch: 5| Step: 1
Training loss: 2.3582327365875244
Validation loss: 2.4557530315973426

Epoch: 5| Step: 2
Training loss: 2.444317579269409
Validation loss: 2.462635032592281

Epoch: 5| Step: 3
Training loss: 2.459165573120117
Validation loss: 2.465550737996255

Epoch: 5| Step: 4
Training loss: 2.423022747039795
Validation loss: 2.4725437215579453

Epoch: 5| Step: 5
Training loss: 2.8374273777008057
Validation loss: 2.453523261572725

Epoch: 5| Step: 6
Training loss: 2.5638179779052734
Validation loss: 2.4448169149378294

Epoch: 5| Step: 7
Training loss: 2.4972622394561768
Validation loss: 2.442016678471719

Epoch: 5| Step: 8
Training loss: 3.0304386615753174
Validation loss: 2.44495657438873

Epoch: 5| Step: 9
Training loss: 2.4966959953308105
Validation loss: 2.442794438331358

Epoch: 5| Step: 10
Training loss: 3.1012516021728516
Validation loss: 2.454534251202819

Epoch: 118| Step: 0
Training loss: 3.3196587562561035
Validation loss: 2.4538402941919144

Epoch: 5| Step: 1
Training loss: 2.231783866882324
Validation loss: 2.4743329530121176

Epoch: 5| Step: 2
Training loss: 2.377647876739502
Validation loss: 2.4831300961074008

Epoch: 5| Step: 3
Training loss: 2.3823676109313965
Validation loss: 2.4749942569322485

Epoch: 5| Step: 4
Training loss: 2.6956942081451416
Validation loss: 2.466336314396192

Epoch: 5| Step: 5
Training loss: 2.631070375442505
Validation loss: 2.4607864964392876

Epoch: 5| Step: 6
Training loss: 2.414760112762451
Validation loss: 2.454060850604888

Epoch: 5| Step: 7
Training loss: 3.0130839347839355
Validation loss: 2.446097345762355

Epoch: 5| Step: 8
Training loss: 2.6523444652557373
Validation loss: 2.4481772376644995

Epoch: 5| Step: 9
Training loss: 2.755601406097412
Validation loss: 2.444972122869184

Epoch: 5| Step: 10
Training loss: 2.5292491912841797
Validation loss: 2.451161671710271

Epoch: 119| Step: 0
Training loss: 2.111501693725586
Validation loss: 2.448655536097865

Epoch: 5| Step: 1
Training loss: 2.9506473541259766
Validation loss: 2.4470288202326786

Epoch: 5| Step: 2
Training loss: 2.3358092308044434
Validation loss: 2.454002477789438

Epoch: 5| Step: 3
Training loss: 2.851976156234741
Validation loss: 2.450521356316023

Epoch: 5| Step: 4
Training loss: 2.158282518386841
Validation loss: 2.452008911358413

Epoch: 5| Step: 5
Training loss: 3.1031222343444824
Validation loss: 2.458097163067069

Epoch: 5| Step: 6
Training loss: 2.550534725189209
Validation loss: 2.4568381181327243

Epoch: 5| Step: 7
Training loss: 2.694629669189453
Validation loss: 2.4618646406358287

Epoch: 5| Step: 8
Training loss: 2.7686312198638916
Validation loss: 2.4704923757942776

Epoch: 5| Step: 9
Training loss: 2.402114152908325
Validation loss: 2.469227803650723

Epoch: 5| Step: 10
Training loss: 3.1852128505706787
Validation loss: 2.4700360093065488

Epoch: 120| Step: 0
Training loss: 2.493391275405884
Validation loss: 2.4613454059887956

Epoch: 5| Step: 1
Training loss: 2.90580415725708
Validation loss: 2.466551632009527

Epoch: 5| Step: 2
Training loss: 2.3175854682922363
Validation loss: 2.4594905350797918

Epoch: 5| Step: 3
Training loss: 2.6027228832244873
Validation loss: 2.4600245491150887

Epoch: 5| Step: 4
Training loss: 2.986407518386841
Validation loss: 2.466306806892477

Epoch: 5| Step: 5
Training loss: 2.396543025970459
Validation loss: 2.466541354374219

Epoch: 5| Step: 6
Training loss: 2.7572474479675293
Validation loss: 2.4683062004786667

Epoch: 5| Step: 7
Training loss: 2.8431553840637207
Validation loss: 2.4700114611656434

Epoch: 5| Step: 8
Training loss: 2.9200170040130615
Validation loss: 2.4697086952065908

Epoch: 5| Step: 9
Training loss: 2.5403637886047363
Validation loss: 2.4689276167141494

Epoch: 5| Step: 10
Training loss: 2.177729368209839
Validation loss: 2.469061402864354

Epoch: 121| Step: 0
Training loss: 2.109442949295044
Validation loss: 2.4692452082069973

Epoch: 5| Step: 1
Training loss: 2.868793249130249
Validation loss: 2.4676522798435663

Epoch: 5| Step: 2
Training loss: 2.7277519702911377
Validation loss: 2.461434174609441

Epoch: 5| Step: 3
Training loss: 3.4233317375183105
Validation loss: 2.463638949137862

Epoch: 5| Step: 4
Training loss: 2.688232898712158
Validation loss: 2.4581128704932427

Epoch: 5| Step: 5
Training loss: 3.057584285736084
Validation loss: 2.4632048940145843

Epoch: 5| Step: 6
Training loss: 1.8532657623291016
Validation loss: 2.4456474140126216

Epoch: 5| Step: 7
Training loss: 2.2680439949035645
Validation loss: 2.4464608905135945

Epoch: 5| Step: 8
Training loss: 2.3253612518310547
Validation loss: 2.445836185127176

Epoch: 5| Step: 9
Training loss: 3.1011440753936768
Validation loss: 2.4431201463104575

Epoch: 5| Step: 10
Training loss: 2.4836952686309814
Validation loss: 2.44176588648109

Epoch: 122| Step: 0
Training loss: 2.1868274211883545
Validation loss: 2.4480913557032102

Epoch: 5| Step: 1
Training loss: 2.802121639251709
Validation loss: 2.4484521727408133

Epoch: 5| Step: 2
Training loss: 2.3757128715515137
Validation loss: 2.448279124434276

Epoch: 5| Step: 3
Training loss: 2.3120250701904297
Validation loss: 2.454652786254883

Epoch: 5| Step: 4
Training loss: 2.4832985401153564
Validation loss: 2.468593951194517

Epoch: 5| Step: 5
Training loss: 2.7524166107177734
Validation loss: 2.4727827554107993

Epoch: 5| Step: 6
Training loss: 3.301368236541748
Validation loss: 2.4733760959358624

Epoch: 5| Step: 7
Training loss: 2.3798232078552246
Validation loss: 2.4820307685482885

Epoch: 5| Step: 8
Training loss: 2.5357205867767334
Validation loss: 2.4801572215172554

Epoch: 5| Step: 9
Training loss: 2.5138840675354004
Validation loss: 2.476020231041857

Epoch: 5| Step: 10
Training loss: 3.5603508949279785
Validation loss: 2.467951011914079

Epoch: 123| Step: 0
Training loss: 2.2016117572784424
Validation loss: 2.469143100964126

Epoch: 5| Step: 1
Training loss: 2.289839029312134
Validation loss: 2.4489954671552105

Epoch: 5| Step: 2
Training loss: 2.1742727756500244
Validation loss: 2.4535587218499955

Epoch: 5| Step: 3
Training loss: 2.864750385284424
Validation loss: 2.450499370533933

Epoch: 5| Step: 4
Training loss: 3.199711322784424
Validation loss: 2.4497402560326362

Epoch: 5| Step: 5
Training loss: 3.0088019371032715
Validation loss: 2.4624458692407094

Epoch: 5| Step: 6
Training loss: 2.2214982509613037
Validation loss: 2.4487218267174176

Epoch: 5| Step: 7
Training loss: 2.3272271156311035
Validation loss: 2.448773968604303

Epoch: 5| Step: 8
Training loss: 2.5253310203552246
Validation loss: 2.445174394115325

Epoch: 5| Step: 9
Training loss: 3.3184943199157715
Validation loss: 2.4440077453531246

Epoch: 5| Step: 10
Training loss: 2.6478097438812256
Validation loss: 2.4477226503433718

Epoch: 124| Step: 0
Training loss: 2.392080307006836
Validation loss: 2.438345847591277

Epoch: 5| Step: 1
Training loss: 2.9231579303741455
Validation loss: 2.4416262616393385

Epoch: 5| Step: 2
Training loss: 2.888554811477661
Validation loss: 2.4485536006189164

Epoch: 5| Step: 3
Training loss: 2.434288740158081
Validation loss: 2.446126171337661

Epoch: 5| Step: 4
Training loss: 2.2970967292785645
Validation loss: 2.439196337935745

Epoch: 5| Step: 5
Training loss: 2.4877116680145264
Validation loss: 2.436233571780625

Epoch: 5| Step: 6
Training loss: 2.145730495452881
Validation loss: 2.4416464426184215

Epoch: 5| Step: 7
Training loss: 3.314793825149536
Validation loss: 2.446480046036423

Epoch: 5| Step: 8
Training loss: 2.3023455142974854
Validation loss: 2.4582376018647225

Epoch: 5| Step: 9
Training loss: 2.9331512451171875
Validation loss: 2.472135059295162

Epoch: 5| Step: 10
Training loss: 2.7364110946655273
Validation loss: 2.4602036322316816

Epoch: 125| Step: 0
Training loss: 2.425360918045044
Validation loss: 2.4591403212598575

Epoch: 5| Step: 1
Training loss: 1.7751350402832031
Validation loss: 2.446766273949736

Epoch: 5| Step: 2
Training loss: 3.4000258445739746
Validation loss: 2.4437496867231143

Epoch: 5| Step: 3
Training loss: 2.158461332321167
Validation loss: 2.4454216546909784

Epoch: 5| Step: 4
Training loss: 2.9763894081115723
Validation loss: 2.459400253911172

Epoch: 5| Step: 5
Training loss: 2.7613325119018555
Validation loss: 2.464788662490024

Epoch: 5| Step: 6
Training loss: 2.36883282661438
Validation loss: 2.46164039129852

Epoch: 5| Step: 7
Training loss: 2.3486037254333496
Validation loss: 2.4474318617133686

Epoch: 5| Step: 8
Training loss: 2.9765326976776123
Validation loss: 2.444023114378734

Epoch: 5| Step: 9
Training loss: 3.0297882556915283
Validation loss: 2.4505310161139375

Epoch: 5| Step: 10
Training loss: 2.7775967121124268
Validation loss: 2.4392226280704623

Epoch: 126| Step: 0
Training loss: 2.452258586883545
Validation loss: 2.448373184409193

Epoch: 5| Step: 1
Training loss: 2.7834067344665527
Validation loss: 2.4370941756874003

Epoch: 5| Step: 2
Training loss: 2.6865735054016113
Validation loss: 2.4341806019506147

Epoch: 5| Step: 3
Training loss: 2.619560718536377
Validation loss: 2.4327695318447646

Epoch: 5| Step: 4
Training loss: 2.3849844932556152
Validation loss: 2.4254736913147794

Epoch: 5| Step: 5
Training loss: 2.811297655105591
Validation loss: 2.432922153062718

Epoch: 5| Step: 6
Training loss: 2.150993585586548
Validation loss: 2.435167545913368

Epoch: 5| Step: 7
Training loss: 2.7161946296691895
Validation loss: 2.4313689329290904

Epoch: 5| Step: 8
Training loss: 2.448671579360962
Validation loss: 2.4422889063435216

Epoch: 5| Step: 9
Training loss: 2.7933297157287598
Validation loss: 2.444127885244226

Epoch: 5| Step: 10
Training loss: 3.054201126098633
Validation loss: 2.437391860510713

Epoch: 127| Step: 0
Training loss: 2.2229785919189453
Validation loss: 2.4339314788900395

Epoch: 5| Step: 1
Training loss: 2.8113834857940674
Validation loss: 2.4348357800514466

Epoch: 5| Step: 2
Training loss: 2.987365484237671
Validation loss: 2.4294072376784457

Epoch: 5| Step: 3
Training loss: 2.50844144821167
Validation loss: 2.4293796323960826

Epoch: 5| Step: 4
Training loss: 2.25010347366333
Validation loss: 2.432465504574519

Epoch: 5| Step: 5
Training loss: 2.9985594749450684
Validation loss: 2.4409461944333968

Epoch: 5| Step: 6
Training loss: 2.1274380683898926
Validation loss: 2.4450025430289646

Epoch: 5| Step: 7
Training loss: 2.370175838470459
Validation loss: 2.457890031158283

Epoch: 5| Step: 8
Training loss: 2.7277743816375732
Validation loss: 2.466237655249975

Epoch: 5| Step: 9
Training loss: 3.066199541091919
Validation loss: 2.4518405083687074

Epoch: 5| Step: 10
Training loss: 2.594866991043091
Validation loss: 2.4510619307077057

Epoch: 128| Step: 0
Training loss: 1.9615602493286133
Validation loss: 2.45081304734753

Epoch: 5| Step: 1
Training loss: 2.3989224433898926
Validation loss: 2.4638136048470773

Epoch: 5| Step: 2
Training loss: 2.2377421855926514
Validation loss: 2.461956613807268

Epoch: 5| Step: 3
Training loss: 2.367810010910034
Validation loss: 2.461944264750327

Epoch: 5| Step: 4
Training loss: 3.4687023162841797
Validation loss: 2.458889469023674

Epoch: 5| Step: 5
Training loss: 2.675889253616333
Validation loss: 2.4662111651512886

Epoch: 5| Step: 6
Training loss: 2.0520429611206055
Validation loss: 2.4730156929262224

Epoch: 5| Step: 7
Training loss: 2.468660831451416
Validation loss: 2.4504785691538165

Epoch: 5| Step: 8
Training loss: 3.149515151977539
Validation loss: 2.4491522696710404

Epoch: 5| Step: 9
Training loss: 2.79193115234375
Validation loss: 2.428398509179392

Epoch: 5| Step: 10
Training loss: 3.2148563861846924
Validation loss: 2.4226337504643265

Epoch: 129| Step: 0
Training loss: 2.3497719764709473
Validation loss: 2.4321552963667017

Epoch: 5| Step: 1
Training loss: 2.9188263416290283
Validation loss: 2.4261264852298203

Epoch: 5| Step: 2
Training loss: 2.926795244216919
Validation loss: 2.431944998361731

Epoch: 5| Step: 3
Training loss: 3.1640093326568604
Validation loss: 2.424609097101355

Epoch: 5| Step: 4
Training loss: 2.9882476329803467
Validation loss: 2.431073183654457

Epoch: 5| Step: 5
Training loss: 2.5329012870788574
Validation loss: 2.419036590924827

Epoch: 5| Step: 6
Training loss: 2.4382379055023193
Validation loss: 2.4126080543764177

Epoch: 5| Step: 7
Training loss: 1.9594539403915405
Validation loss: 2.4277329034702753

Epoch: 5| Step: 8
Training loss: 1.8075075149536133
Validation loss: 2.4246162188950406

Epoch: 5| Step: 9
Training loss: 2.8515877723693848
Validation loss: 2.438034693400065

Epoch: 5| Step: 10
Training loss: 2.942126989364624
Validation loss: 2.4629943011909403

Epoch: 130| Step: 0
Training loss: 2.1010026931762695
Validation loss: 2.48264209429423

Epoch: 5| Step: 1
Training loss: 2.254544496536255
Validation loss: 2.4836605364276516

Epoch: 5| Step: 2
Training loss: 2.5960750579833984
Validation loss: 2.4676710585112214

Epoch: 5| Step: 3
Training loss: 2.4192686080932617
Validation loss: 2.4656284111802296

Epoch: 5| Step: 4
Training loss: 2.3645331859588623
Validation loss: 2.437690181116904

Epoch: 5| Step: 5
Training loss: 2.2489430904388428
Validation loss: 2.422720830927613

Epoch: 5| Step: 6
Training loss: 2.6588473320007324
Validation loss: 2.427339579469414

Epoch: 5| Step: 7
Training loss: 3.134732961654663
Validation loss: 2.41277588054698

Epoch: 5| Step: 8
Training loss: 2.903294563293457
Validation loss: 2.4177004829529793

Epoch: 5| Step: 9
Training loss: 3.846966505050659
Validation loss: 2.4180040667133946

Epoch: 5| Step: 10
Training loss: 2.2301244735717773
Validation loss: 2.4180501353356147

Epoch: 131| Step: 0
Training loss: 2.485515832901001
Validation loss: 2.4181227632748183

Epoch: 5| Step: 1
Training loss: 2.197138547897339
Validation loss: 2.4196616295845277

Epoch: 5| Step: 2
Training loss: 2.7148079872131348
Validation loss: 2.4107109526152253

Epoch: 5| Step: 3
Training loss: 2.908849000930786
Validation loss: 2.4149777017613894

Epoch: 5| Step: 4
Training loss: 2.7403457164764404
Validation loss: 2.4265089496489494

Epoch: 5| Step: 5
Training loss: 1.9296079874038696
Validation loss: 2.4364106834575696

Epoch: 5| Step: 6
Training loss: 2.96236515045166
Validation loss: 2.434288005675039

Epoch: 5| Step: 7
Training loss: 2.391618251800537
Validation loss: 2.4572502977104596

Epoch: 5| Step: 8
Training loss: 2.3783037662506104
Validation loss: 2.4386937208073114

Epoch: 5| Step: 9
Training loss: 2.584336042404175
Validation loss: 2.4293590463617796

Epoch: 5| Step: 10
Training loss: 3.5333750247955322
Validation loss: 2.4235443056270642

Epoch: 132| Step: 0
Training loss: 2.5032222270965576
Validation loss: 2.422166278285365

Epoch: 5| Step: 1
Training loss: 2.714833974838257
Validation loss: 2.405408210651849

Epoch: 5| Step: 2
Training loss: 2.461988687515259
Validation loss: 2.4017721581202682

Epoch: 5| Step: 3
Training loss: 3.2103047370910645
Validation loss: 2.407191898233147

Epoch: 5| Step: 4
Training loss: 2.326688766479492
Validation loss: 2.4064596750402965

Epoch: 5| Step: 5
Training loss: 1.8365914821624756
Validation loss: 2.410250558648058

Epoch: 5| Step: 6
Training loss: 2.6914472579956055
Validation loss: 2.4030473104087253

Epoch: 5| Step: 7
Training loss: 2.84555721282959
Validation loss: 2.400598572146508

Epoch: 5| Step: 8
Training loss: 2.979867696762085
Validation loss: 2.4051534642455397

Epoch: 5| Step: 9
Training loss: 2.502357006072998
Validation loss: 2.4191974798838296

Epoch: 5| Step: 10
Training loss: 2.593360185623169
Validation loss: 2.4305005278638614

Epoch: 133| Step: 0
Training loss: 2.442513942718506
Validation loss: 2.43639273540948

Epoch: 5| Step: 1
Training loss: 2.922407627105713
Validation loss: 2.438138246536255

Epoch: 5| Step: 2
Training loss: 2.565096616744995
Validation loss: 2.4414640190780803

Epoch: 5| Step: 3
Training loss: 2.587538480758667
Validation loss: 2.441588214648667

Epoch: 5| Step: 4
Training loss: 2.7504165172576904
Validation loss: 2.4365042973590154

Epoch: 5| Step: 5
Training loss: 2.7628543376922607
Validation loss: 2.432188267348915

Epoch: 5| Step: 6
Training loss: 3.080817699432373
Validation loss: 2.4329980265709663

Epoch: 5| Step: 7
Training loss: 2.62568998336792
Validation loss: 2.4389996913171585

Epoch: 5| Step: 8
Training loss: 2.5804765224456787
Validation loss: 2.4343394951153825

Epoch: 5| Step: 9
Training loss: 2.036233425140381
Validation loss: 2.4313976995406614

Epoch: 5| Step: 10
Training loss: 2.2287437915802
Validation loss: 2.420643601366269

Epoch: 134| Step: 0
Training loss: 2.4972033500671387
Validation loss: 2.417493509989913

Epoch: 5| Step: 1
Training loss: 2.3565332889556885
Validation loss: 2.4129102922254995

Epoch: 5| Step: 2
Training loss: 2.3831887245178223
Validation loss: 2.4196501957472933

Epoch: 5| Step: 3
Training loss: 2.558565616607666
Validation loss: 2.450104216093658

Epoch: 5| Step: 4
Training loss: 2.5422866344451904
Validation loss: 2.47105666898912

Epoch: 5| Step: 5
Training loss: 3.2914366722106934
Validation loss: 2.48688308782475

Epoch: 5| Step: 6
Training loss: 2.5540549755096436
Validation loss: 2.449465646538683

Epoch: 5| Step: 7
Training loss: 2.197917938232422
Validation loss: 2.41284950061511

Epoch: 5| Step: 8
Training loss: 2.694746732711792
Validation loss: 2.402791189891036

Epoch: 5| Step: 9
Training loss: 2.7068898677825928
Validation loss: 2.391419110759612

Epoch: 5| Step: 10
Training loss: 3.0278806686401367
Validation loss: 2.395286862568189

Epoch: 135| Step: 0
Training loss: 2.2414841651916504
Validation loss: 2.3973472041468464

Epoch: 5| Step: 1
Training loss: 3.0524532794952393
Validation loss: 2.405261142279512

Epoch: 5| Step: 2
Training loss: 2.6511125564575195
Validation loss: 2.4111644632072857

Epoch: 5| Step: 3
Training loss: 1.8406286239624023
Validation loss: 2.4198876606520785

Epoch: 5| Step: 4
Training loss: 2.2640061378479004
Validation loss: 2.4102086751691756

Epoch: 5| Step: 5
Training loss: 3.069650173187256
Validation loss: 2.4024371229192263

Epoch: 5| Step: 6
Training loss: 2.9900901317596436
Validation loss: 2.3939977012654787

Epoch: 5| Step: 7
Training loss: 2.2797889709472656
Validation loss: 2.3840404633552796

Epoch: 5| Step: 8
Training loss: 2.7479426860809326
Validation loss: 2.399597203859719

Epoch: 5| Step: 9
Training loss: 2.864311933517456
Validation loss: 2.4035691676601285

Epoch: 5| Step: 10
Training loss: 2.687628746032715
Validation loss: 2.4154713205111924

Epoch: 136| Step: 0
Training loss: 3.301048755645752
Validation loss: 2.407577440302859

Epoch: 5| Step: 1
Training loss: 2.2290573120117188
Validation loss: 2.4196602170185377

Epoch: 5| Step: 2
Training loss: 2.8813326358795166
Validation loss: 2.4198201907578336

Epoch: 5| Step: 3
Training loss: 3.2659049034118652
Validation loss: 2.4132125685291905

Epoch: 5| Step: 4
Training loss: 1.8917505741119385
Validation loss: 2.4070505557521695

Epoch: 5| Step: 5
Training loss: 2.5013811588287354
Validation loss: 2.4087633445698726

Epoch: 5| Step: 6
Training loss: 2.0280203819274902
Validation loss: 2.4066639805352814

Epoch: 5| Step: 7
Training loss: 2.977356433868408
Validation loss: 2.4136993705585437

Epoch: 5| Step: 8
Training loss: 2.3912839889526367
Validation loss: 2.4091828894871536

Epoch: 5| Step: 9
Training loss: 2.168844223022461
Validation loss: 2.41120211796094

Epoch: 5| Step: 10
Training loss: 2.946450710296631
Validation loss: 2.4140387735059186

Epoch: 137| Step: 0
Training loss: 2.624788522720337
Validation loss: 2.4145609255759948

Epoch: 5| Step: 1
Training loss: 2.6883835792541504
Validation loss: 2.4073535447479575

Epoch: 5| Step: 2
Training loss: 2.5184803009033203
Validation loss: 2.394245027213968

Epoch: 5| Step: 3
Training loss: 2.9507100582122803
Validation loss: 2.3991065563694125

Epoch: 5| Step: 4
Training loss: 2.0785908699035645
Validation loss: 2.399217579954414

Epoch: 5| Step: 5
Training loss: 2.814274311065674
Validation loss: 2.4054145633533435

Epoch: 5| Step: 6
Training loss: 2.295417308807373
Validation loss: 2.423522844109484

Epoch: 5| Step: 7
Training loss: 2.3683502674102783
Validation loss: 2.461114227130849

Epoch: 5| Step: 8
Training loss: 2.035552501678467
Validation loss: 2.4671474605478267

Epoch: 5| Step: 9
Training loss: 2.8171887397766113
Validation loss: 2.4500318060639086

Epoch: 5| Step: 10
Training loss: 3.5027098655700684
Validation loss: 2.419091873271491

Epoch: 138| Step: 0
Training loss: 2.115783929824829
Validation loss: 2.4210618516450286

Epoch: 5| Step: 1
Training loss: 3.1683781147003174
Validation loss: 2.4030302955258276

Epoch: 5| Step: 2
Training loss: 3.4250030517578125
Validation loss: 2.4016140045658236

Epoch: 5| Step: 3
Training loss: 2.506532907485962
Validation loss: 2.421294653287498

Epoch: 5| Step: 4
Training loss: 2.3761096000671387
Validation loss: 2.4132298013215423

Epoch: 5| Step: 5
Training loss: 2.2844700813293457
Validation loss: 2.404821195910054

Epoch: 5| Step: 6
Training loss: 2.2803328037261963
Validation loss: 2.393820821598012

Epoch: 5| Step: 7
Training loss: 2.441436767578125
Validation loss: 2.4008043094347884

Epoch: 5| Step: 8
Training loss: 2.807379961013794
Validation loss: 2.3885793121912147

Epoch: 5| Step: 9
Training loss: 2.3637986183166504
Validation loss: 2.4176540720847344

Epoch: 5| Step: 10
Training loss: 2.892200469970703
Validation loss: 2.4229200988687496

Epoch: 139| Step: 0
Training loss: 2.572899580001831
Validation loss: 2.448594626560006

Epoch: 5| Step: 1
Training loss: 2.8520312309265137
Validation loss: 2.4191131796888126

Epoch: 5| Step: 2
Training loss: 2.32896089553833
Validation loss: 2.398888134187268

Epoch: 5| Step: 3
Training loss: 2.375762701034546
Validation loss: 2.406157229536323

Epoch: 5| Step: 4
Training loss: 1.964644193649292
Validation loss: 2.4011486884086364

Epoch: 5| Step: 5
Training loss: 2.734637498855591
Validation loss: 2.4049303429101103

Epoch: 5| Step: 6
Training loss: 2.4832699298858643
Validation loss: 2.4056374872884443

Epoch: 5| Step: 7
Training loss: 2.3913731575012207
Validation loss: 2.4061793665732107

Epoch: 5| Step: 8
Training loss: 2.6928303241729736
Validation loss: 2.406019849161948

Epoch: 5| Step: 9
Training loss: 2.9748129844665527
Validation loss: 2.4094239434888287

Epoch: 5| Step: 10
Training loss: 3.214660406112671
Validation loss: 2.408474822198191

Epoch: 140| Step: 0
Training loss: 2.814751148223877
Validation loss: 2.4172153985628517

Epoch: 5| Step: 1
Training loss: 2.645860195159912
Validation loss: 2.3985637926286265

Epoch: 5| Step: 2
Training loss: 2.1186940670013428
Validation loss: 2.3974099005422285

Epoch: 5| Step: 3
Training loss: 3.1526246070861816
Validation loss: 2.403163876584781

Epoch: 5| Step: 4
Training loss: 2.922698497772217
Validation loss: 2.399422040549658

Epoch: 5| Step: 5
Training loss: 2.66808819770813
Validation loss: 2.390862866114545

Epoch: 5| Step: 6
Training loss: 2.428654193878174
Validation loss: 2.410263579378846

Epoch: 5| Step: 7
Training loss: 2.1569600105285645
Validation loss: 2.4226542031893166

Epoch: 5| Step: 8
Training loss: 2.446761131286621
Validation loss: 2.4077920016422065

Epoch: 5| Step: 9
Training loss: 2.306640625
Validation loss: 2.43571134280133

Epoch: 5| Step: 10
Training loss: 2.865029811859131
Validation loss: 2.4397545476113596

Epoch: 141| Step: 0
Training loss: 1.8609825372695923
Validation loss: 2.4456152095589587

Epoch: 5| Step: 1
Training loss: 2.313288450241089
Validation loss: 2.421860905103786

Epoch: 5| Step: 2
Training loss: 3.0290133953094482
Validation loss: 2.432500990488196

Epoch: 5| Step: 3
Training loss: 2.4945859909057617
Validation loss: 2.4160486857096353

Epoch: 5| Step: 4
Training loss: 2.765123128890991
Validation loss: 2.4119229521802676

Epoch: 5| Step: 5
Training loss: 2.242326259613037
Validation loss: 2.4218797991352696

Epoch: 5| Step: 6
Training loss: 1.8637406826019287
Validation loss: 2.3957185732421054

Epoch: 5| Step: 7
Training loss: 2.9268321990966797
Validation loss: 2.4135267657618367

Epoch: 5| Step: 8
Training loss: 2.140596866607666
Validation loss: 2.407115959352063

Epoch: 5| Step: 9
Training loss: 3.7775630950927734
Validation loss: 2.4092209057141374

Epoch: 5| Step: 10
Training loss: 3.0650794506073
Validation loss: 2.4026974196075113

Epoch: 142| Step: 0
Training loss: 3.0900466442108154
Validation loss: 2.4066188309782293

Epoch: 5| Step: 1
Training loss: 2.3280959129333496
Validation loss: 2.396836201349894

Epoch: 5| Step: 2
Training loss: 2.464099407196045
Validation loss: 2.396671651512064

Epoch: 5| Step: 3
Training loss: 2.1856179237365723
Validation loss: 2.3993964733616

Epoch: 5| Step: 4
Training loss: 2.25138783454895
Validation loss: 2.4136120119402484

Epoch: 5| Step: 5
Training loss: 3.12396240234375
Validation loss: 2.4075466048332954

Epoch: 5| Step: 6
Training loss: 2.119072437286377
Validation loss: 2.390676088230584

Epoch: 5| Step: 7
Training loss: 2.629409074783325
Validation loss: 2.394361598517305

Epoch: 5| Step: 8
Training loss: 2.602191686630249
Validation loss: 2.396843251361642

Epoch: 5| Step: 9
Training loss: 2.4098098278045654
Validation loss: 2.396672491104372

Epoch: 5| Step: 10
Training loss: 3.237234592437744
Validation loss: 2.394373929628762

Epoch: 143| Step: 0
Training loss: 2.2434089183807373
Validation loss: 2.3908352210957515

Epoch: 5| Step: 1
Training loss: 2.2438998222351074
Validation loss: 2.38509298652731

Epoch: 5| Step: 2
Training loss: 3.0563042163848877
Validation loss: 2.3869242693788264

Epoch: 5| Step: 3
Training loss: 1.9697644710540771
Validation loss: 2.3839253687089488

Epoch: 5| Step: 4
Training loss: 1.8950666189193726
Validation loss: 2.3873889420622136

Epoch: 5| Step: 5
Training loss: 2.406831979751587
Validation loss: 2.400277417193177

Epoch: 5| Step: 6
Training loss: 3.164931535720825
Validation loss: 2.414625601101947

Epoch: 5| Step: 7
Training loss: 3.125560998916626
Validation loss: 2.4099298087499474

Epoch: 5| Step: 8
Training loss: 3.06608510017395
Validation loss: 2.3991142421640377

Epoch: 5| Step: 9
Training loss: 2.670945644378662
Validation loss: 2.3928724386358775

Epoch: 5| Step: 10
Training loss: 2.5268054008483887
Validation loss: 2.3740815142149567

Epoch: 144| Step: 0
Training loss: 3.2092010974884033
Validation loss: 2.3746789475922943

Epoch: 5| Step: 1
Training loss: 1.6800289154052734
Validation loss: 2.381372082617975

Epoch: 5| Step: 2
Training loss: 2.3111729621887207
Validation loss: 2.376932749184229

Epoch: 5| Step: 3
Training loss: 2.280428886413574
Validation loss: 2.3709591768121205

Epoch: 5| Step: 4
Training loss: 3.2821707725524902
Validation loss: 2.36627362620446

Epoch: 5| Step: 5
Training loss: 2.630884885787964
Validation loss: 2.3658863857228267

Epoch: 5| Step: 6
Training loss: 2.4205641746520996
Validation loss: 2.375347992425324

Epoch: 5| Step: 7
Training loss: 2.564741849899292
Validation loss: 2.374259518038842

Epoch: 5| Step: 8
Training loss: 3.1429221630096436
Validation loss: 2.3715224650598343

Epoch: 5| Step: 9
Training loss: 2.680546283721924
Validation loss: 2.3685913906302503

Epoch: 5| Step: 10
Training loss: 2.109626293182373
Validation loss: 2.3778456077780774

Epoch: 145| Step: 0
Training loss: 2.8886828422546387
Validation loss: 2.3805390814299225

Epoch: 5| Step: 1
Training loss: 2.108063220977783
Validation loss: 2.383570481372136

Epoch: 5| Step: 2
Training loss: 3.075159788131714
Validation loss: 2.3992381788069204

Epoch: 5| Step: 3
Training loss: 2.242934226989746
Validation loss: 2.3998828049628966

Epoch: 5| Step: 4
Training loss: 2.5204882621765137
Validation loss: 2.3933515138523553

Epoch: 5| Step: 5
Training loss: 2.583705425262451
Validation loss: 2.399347036115585

Epoch: 5| Step: 6
Training loss: 2.98643159866333
Validation loss: 2.3869318321187007

Epoch: 5| Step: 7
Training loss: 2.7842395305633545
Validation loss: 2.380004200884091

Epoch: 5| Step: 8
Training loss: 1.9678300619125366
Validation loss: 2.3779789940003426

Epoch: 5| Step: 9
Training loss: 2.2431766986846924
Validation loss: 2.385421424783686

Epoch: 5| Step: 10
Training loss: 3.0957250595092773
Validation loss: 2.389388422812185

Epoch: 146| Step: 0
Training loss: 2.34540057182312
Validation loss: 2.389512597873647

Epoch: 5| Step: 1
Training loss: 2.295867681503296
Validation loss: 2.379293462281586

Epoch: 5| Step: 2
Training loss: 3.1062710285186768
Validation loss: 2.394583002213509

Epoch: 5| Step: 3
Training loss: 2.9752042293548584
Validation loss: 2.4206871448024625

Epoch: 5| Step: 4
Training loss: 2.523505926132202
Validation loss: 2.4338459686566423

Epoch: 5| Step: 5
Training loss: 2.9274837970733643
Validation loss: 2.4313039754026677

Epoch: 5| Step: 6
Training loss: 2.553515911102295
Validation loss: 2.4251200973346667

Epoch: 5| Step: 7
Training loss: 2.8789381980895996
Validation loss: 2.4037285799621255

Epoch: 5| Step: 8
Training loss: 2.1756505966186523
Validation loss: 2.394632298459289

Epoch: 5| Step: 9
Training loss: 2.143542766571045
Validation loss: 2.384894855560795

Epoch: 5| Step: 10
Training loss: 2.375223159790039
Validation loss: 2.385687981882403

Epoch: 147| Step: 0
Training loss: 2.1201114654541016
Validation loss: 2.3881458723416893

Epoch: 5| Step: 1
Training loss: 2.2697997093200684
Validation loss: 2.3832178590118245

Epoch: 5| Step: 2
Training loss: 2.9944345951080322
Validation loss: 2.3788425948030207

Epoch: 5| Step: 3
Training loss: 2.6426589488983154
Validation loss: 2.383919264680596

Epoch: 5| Step: 4
Training loss: 2.4701457023620605
Validation loss: 2.376127484024212

Epoch: 5| Step: 5
Training loss: 2.5048279762268066
Validation loss: 2.3725329522163636

Epoch: 5| Step: 6
Training loss: 2.4858181476593018
Validation loss: 2.3727019986798688

Epoch: 5| Step: 7
Training loss: 2.7018704414367676
Validation loss: 2.3967196915739324

Epoch: 5| Step: 8
Training loss: 2.786045789718628
Validation loss: 2.403610837075018

Epoch: 5| Step: 9
Training loss: 2.5223584175109863
Validation loss: 2.4009430459750596

Epoch: 5| Step: 10
Training loss: 2.846794605255127
Validation loss: 2.3955314415757374

Epoch: 148| Step: 0
Training loss: 2.0091090202331543
Validation loss: 2.39318097278636

Epoch: 5| Step: 1
Training loss: 2.470371961593628
Validation loss: 2.3889554341634116

Epoch: 5| Step: 2
Training loss: 2.435181140899658
Validation loss: 2.3734681016655377

Epoch: 5| Step: 3
Training loss: 2.9588379859924316
Validation loss: 2.3970926769318117

Epoch: 5| Step: 4
Training loss: 2.545315980911255
Validation loss: 2.408270638476136

Epoch: 5| Step: 5
Training loss: 2.1191532611846924
Validation loss: 2.410158827740659

Epoch: 5| Step: 6
Training loss: 2.5075817108154297
Validation loss: 2.417817856675835

Epoch: 5| Step: 7
Training loss: 2.9851431846618652
Validation loss: 2.4250672196829193

Epoch: 5| Step: 8
Training loss: 3.198286771774292
Validation loss: 2.4211902297953123

Epoch: 5| Step: 9
Training loss: 2.5588953495025635
Validation loss: 2.428295648226174

Epoch: 5| Step: 10
Training loss: 2.6254568099975586
Validation loss: 2.421634688172289

Epoch: 149| Step: 0
Training loss: 2.426698684692383
Validation loss: 2.3988335363326536

Epoch: 5| Step: 1
Training loss: 2.6030020713806152
Validation loss: 2.393696674736597

Epoch: 5| Step: 2
Training loss: 2.7808547019958496
Validation loss: 2.403396062953498

Epoch: 5| Step: 3
Training loss: 2.952972888946533
Validation loss: 2.415613989676199

Epoch: 5| Step: 4
Training loss: 2.7722156047821045
Validation loss: 2.433040726569391

Epoch: 5| Step: 5
Training loss: 2.5728211402893066
Validation loss: 2.4392724549898537

Epoch: 5| Step: 6
Training loss: 2.5568528175354004
Validation loss: 2.4385277404580066

Epoch: 5| Step: 7
Training loss: 2.1943647861480713
Validation loss: 2.4650789486464633

Epoch: 5| Step: 8
Training loss: 2.438417911529541
Validation loss: 2.440975789100893

Epoch: 5| Step: 9
Training loss: 2.4420790672302246
Validation loss: 2.3994245452265583

Epoch: 5| Step: 10
Training loss: 2.5750157833099365
Validation loss: 2.3783314561331146

Epoch: 150| Step: 0
Training loss: 1.9491676092147827
Validation loss: 2.362871749426729

Epoch: 5| Step: 1
Training loss: 2.578758716583252
Validation loss: 2.358053135615523

Epoch: 5| Step: 2
Training loss: 2.3205578327178955
Validation loss: 2.3515941686527704

Epoch: 5| Step: 3
Training loss: 2.103145122528076
Validation loss: 2.3465160118636263

Epoch: 5| Step: 4
Training loss: 2.298100233078003
Validation loss: 2.3316067931472615

Epoch: 5| Step: 5
Training loss: 2.4316163063049316
Validation loss: 2.345775632448094

Epoch: 5| Step: 6
Training loss: 2.6826529502868652
Validation loss: 2.346950682260657

Epoch: 5| Step: 7
Training loss: 2.7635841369628906
Validation loss: 2.3511369228363037

Epoch: 5| Step: 8
Training loss: 2.446319580078125
Validation loss: 2.367632204486478

Epoch: 5| Step: 9
Training loss: 3.4007506370544434
Validation loss: 2.381695952466739

Epoch: 5| Step: 10
Training loss: 3.3446240425109863
Validation loss: 2.3783228166641726

Epoch: 151| Step: 0
Training loss: 2.6566364765167236
Validation loss: 2.382092575873098

Epoch: 5| Step: 1
Training loss: 2.9827053546905518
Validation loss: 2.3789634473862185

Epoch: 5| Step: 2
Training loss: 1.9689191579818726
Validation loss: 2.3741290261668544

Epoch: 5| Step: 3
Training loss: 2.1357452869415283
Validation loss: 2.373919556217809

Epoch: 5| Step: 4
Training loss: 3.269958972930908
Validation loss: 2.37826867001031

Epoch: 5| Step: 5
Training loss: 2.2337841987609863
Validation loss: 2.3834578478208153

Epoch: 5| Step: 6
Training loss: 2.6741268634796143
Validation loss: 2.380759375069731

Epoch: 5| Step: 7
Training loss: 2.125035524368286
Validation loss: 2.3798893369654173

Epoch: 5| Step: 8
Training loss: 2.7075648307800293
Validation loss: 2.3760298913524998

Epoch: 5| Step: 9
Training loss: 2.859053373336792
Validation loss: 2.3737657608524447

Epoch: 5| Step: 10
Training loss: 2.577927350997925
Validation loss: 2.368319660104731

Epoch: 152| Step: 0
Training loss: 2.72804856300354
Validation loss: 2.385246007673202

Epoch: 5| Step: 1
Training loss: 2.9175198078155518
Validation loss: 2.3911880498291342

Epoch: 5| Step: 2
Training loss: 3.2976768016815186
Validation loss: 2.4092265303416918

Epoch: 5| Step: 3
Training loss: 2.5044753551483154
Validation loss: 2.3858353681461786

Epoch: 5| Step: 4
Training loss: 2.067103862762451
Validation loss: 2.412451554370183

Epoch: 5| Step: 5
Training loss: 2.1385021209716797
Validation loss: 2.396722760251773

Epoch: 5| Step: 6
Training loss: 1.6535323858261108
Validation loss: 2.384504264400851

Epoch: 5| Step: 7
Training loss: 3.2617745399475098
Validation loss: 2.366893965710876

Epoch: 5| Step: 8
Training loss: 2.4626736640930176
Validation loss: 2.356769989895564

Epoch: 5| Step: 9
Training loss: 2.798628807067871
Validation loss: 2.364509113373295

Epoch: 5| Step: 10
Training loss: 2.145692825317383
Validation loss: 2.3700793263732747

Epoch: 153| Step: 0
Training loss: 2.479689121246338
Validation loss: 2.369054050855739

Epoch: 5| Step: 1
Training loss: 2.2192485332489014
Validation loss: 2.3771082765312603

Epoch: 5| Step: 2
Training loss: 2.8372786045074463
Validation loss: 2.374618504637031

Epoch: 5| Step: 3
Training loss: 2.4341132640838623
Validation loss: 2.3922711059611332

Epoch: 5| Step: 4
Training loss: 2.61387300491333
Validation loss: 2.3798696148780083

Epoch: 5| Step: 5
Training loss: 3.206167697906494
Validation loss: 2.3856744202234412

Epoch: 5| Step: 6
Training loss: 2.3315727710723877
Validation loss: 2.376277780020109

Epoch: 5| Step: 7
Training loss: 2.5850729942321777
Validation loss: 2.369700526678434

Epoch: 5| Step: 8
Training loss: 2.658782720565796
Validation loss: 2.369577992346979

Epoch: 5| Step: 9
Training loss: 2.290588617324829
Validation loss: 2.3776739361465618

Epoch: 5| Step: 10
Training loss: 2.5383946895599365
Validation loss: 2.4125296403002996

Epoch: 154| Step: 0
Training loss: 4.070032119750977
Validation loss: 2.440063909817767

Epoch: 5| Step: 1
Training loss: 2.575993776321411
Validation loss: 2.4252617307888564

Epoch: 5| Step: 2
Training loss: 2.213371753692627
Validation loss: 2.4446528675735637

Epoch: 5| Step: 3
Training loss: 2.8906872272491455
Validation loss: 2.4312182934053483

Epoch: 5| Step: 4
Training loss: 2.233642339706421
Validation loss: 2.4068202792957263

Epoch: 5| Step: 5
Training loss: 3.2704269886016846
Validation loss: 2.4253612949002172

Epoch: 5| Step: 6
Training loss: 2.4297118186950684
Validation loss: 2.404532855556857

Epoch: 5| Step: 7
Training loss: 2.2964179515838623
Validation loss: 2.392748607102261

Epoch: 5| Step: 8
Training loss: 2.307114362716675
Validation loss: 2.3725878192532446

Epoch: 5| Step: 9
Training loss: 2.0070135593414307
Validation loss: 2.370306944334379

Epoch: 5| Step: 10
Training loss: 1.7735276222229004
Validation loss: 2.3600400929809897

Epoch: 155| Step: 0
Training loss: 1.8914740085601807
Validation loss: 2.36361833541624

Epoch: 5| Step: 1
Training loss: 2.52940034866333
Validation loss: 2.3626342845219437

Epoch: 5| Step: 2
Training loss: 2.836622714996338
Validation loss: 2.367495334276589

Epoch: 5| Step: 3
Training loss: 1.866098403930664
Validation loss: 2.3713729048287995

Epoch: 5| Step: 4
Training loss: 2.2105274200439453
Validation loss: 2.3845968656642462

Epoch: 5| Step: 5
Training loss: 2.752403736114502
Validation loss: 2.379207452138265

Epoch: 5| Step: 6
Training loss: 3.038548707962036
Validation loss: 2.368256384326566

Epoch: 5| Step: 7
Training loss: 2.5515570640563965
Validation loss: 2.368497017891176

Epoch: 5| Step: 8
Training loss: 2.362704038619995
Validation loss: 2.3695366408235286

Epoch: 5| Step: 9
Training loss: 3.0267493724823
Validation loss: 2.372831316404445

Epoch: 5| Step: 10
Training loss: 3.089566946029663
Validation loss: 2.365644734392884

Epoch: 156| Step: 0
Training loss: 3.101107597351074
Validation loss: 2.3691295449451735

Epoch: 5| Step: 1
Training loss: 3.0610861778259277
Validation loss: 2.3609094619750977

Epoch: 5| Step: 2
Training loss: 2.3508996963500977
Validation loss: 2.362592151088099

Epoch: 5| Step: 3
Training loss: 2.6764280796051025
Validation loss: 2.361720988827367

Epoch: 5| Step: 4
Training loss: 3.0553016662597656
Validation loss: 2.3774937660463396

Epoch: 5| Step: 5
Training loss: 2.2245185375213623
Validation loss: 2.3551344640793337

Epoch: 5| Step: 6
Training loss: 2.1141459941864014
Validation loss: 2.3432675100141958

Epoch: 5| Step: 7
Training loss: 2.562340021133423
Validation loss: 2.354501188442271

Epoch: 5| Step: 8
Training loss: 2.5826826095581055
Validation loss: 2.3573096003583682

Epoch: 5| Step: 9
Training loss: 2.3932747840881348
Validation loss: 2.360892959820327

Epoch: 5| Step: 10
Training loss: 1.7575457096099854
Validation loss: 2.3488307255570606

Epoch: 157| Step: 0
Training loss: 2.8951356410980225
Validation loss: 2.3536024401264806

Epoch: 5| Step: 1
Training loss: 1.6259772777557373
Validation loss: 2.348209270866968

Epoch: 5| Step: 2
Training loss: 1.9349912405014038
Validation loss: 2.352986381899926

Epoch: 5| Step: 3
Training loss: 3.493572950363159
Validation loss: 2.3751607056586974

Epoch: 5| Step: 4
Training loss: 2.7822184562683105
Validation loss: 2.345544692008726

Epoch: 5| Step: 5
Training loss: 2.103182077407837
Validation loss: 2.3343582614775626

Epoch: 5| Step: 6
Training loss: 3.208566665649414
Validation loss: 2.325527221925797

Epoch: 5| Step: 7
Training loss: 2.6721911430358887
Validation loss: 2.3386208652168192

Epoch: 5| Step: 8
Training loss: 2.4199764728546143
Validation loss: 2.3448195636913343

Epoch: 5| Step: 9
Training loss: 2.963315963745117
Validation loss: 2.338349211600519

Epoch: 5| Step: 10
Training loss: 1.9573395252227783
Validation loss: 2.3438235021406606

Epoch: 158| Step: 0
Training loss: 2.7618215084075928
Validation loss: 2.340949232860278

Epoch: 5| Step: 1
Training loss: 2.2459049224853516
Validation loss: 2.3378014820878223

Epoch: 5| Step: 2
Training loss: 2.3352458477020264
Validation loss: 2.3328048541981685

Epoch: 5| Step: 3
Training loss: 2.3590519428253174
Validation loss: 2.3568920076534314

Epoch: 5| Step: 4
Training loss: 2.6606428623199463
Validation loss: 2.3682275510603383

Epoch: 5| Step: 5
Training loss: 2.8571553230285645
Validation loss: 2.389332281645908

Epoch: 5| Step: 6
Training loss: 2.7448291778564453
Validation loss: 2.3949201286479993

Epoch: 5| Step: 7
Training loss: 3.263887882232666
Validation loss: 2.391474759706887

Epoch: 5| Step: 8
Training loss: 2.7674622535705566
Validation loss: 2.367227351793679

Epoch: 5| Step: 9
Training loss: 1.920917272567749
Validation loss: 2.343043924659811

Epoch: 5| Step: 10
Training loss: 1.8413112163543701
Validation loss: 2.350560957385648

Epoch: 159| Step: 0
Training loss: 2.600902557373047
Validation loss: 2.35782096462865

Epoch: 5| Step: 1
Training loss: 1.5996251106262207
Validation loss: 2.358430180498349

Epoch: 5| Step: 2
Training loss: 3.2817161083221436
Validation loss: 2.3565524829331266

Epoch: 5| Step: 3
Training loss: 2.914177656173706
Validation loss: 2.337031102949573

Epoch: 5| Step: 4
Training loss: 2.5482590198516846
Validation loss: 2.33956915076061

Epoch: 5| Step: 5
Training loss: 3.4103596210479736
Validation loss: 2.3397005706705074

Epoch: 5| Step: 6
Training loss: 2.3795409202575684
Validation loss: 2.3337775789281374

Epoch: 5| Step: 7
Training loss: 2.1842236518859863
Validation loss: 2.3501261588065856

Epoch: 5| Step: 8
Training loss: 2.04118013381958
Validation loss: 2.361143700538143

Epoch: 5| Step: 9
Training loss: 3.028886079788208
Validation loss: 2.3777529578055105

Epoch: 5| Step: 10
Training loss: 1.9534515142440796
Validation loss: 2.39736375885625

Epoch: 160| Step: 0
Training loss: 2.346623659133911
Validation loss: 2.4088043807655253

Epoch: 5| Step: 1
Training loss: 2.0985379219055176
Validation loss: 2.379987552601804

Epoch: 5| Step: 2
Training loss: 2.5140349864959717
Validation loss: 2.388271993206393

Epoch: 5| Step: 3
Training loss: 2.9219470024108887
Validation loss: 2.396854818508189

Epoch: 5| Step: 4
Training loss: 2.832282543182373
Validation loss: 2.3604408002668813

Epoch: 5| Step: 5
Training loss: 2.3731532096862793
Validation loss: 2.3345233009707544

Epoch: 5| Step: 6
Training loss: 2.6539745330810547
Validation loss: 2.3299677320705947

Epoch: 5| Step: 7
Training loss: 2.48580002784729
Validation loss: 2.3309394031442623

Epoch: 5| Step: 8
Training loss: 2.5344653129577637
Validation loss: 2.3320759496381207

Epoch: 5| Step: 9
Training loss: 2.5624709129333496
Validation loss: 2.333043849596413

Epoch: 5| Step: 10
Training loss: 2.6326756477355957
Validation loss: 2.3522397856558523

Epoch: 161| Step: 0
Training loss: 2.665635347366333
Validation loss: 2.3519632688132663

Epoch: 5| Step: 1
Training loss: 1.9557844400405884
Validation loss: 2.348965003926267

Epoch: 5| Step: 2
Training loss: 2.5875439643859863
Validation loss: 2.34372051044177

Epoch: 5| Step: 3
Training loss: 3.043267250061035
Validation loss: 2.3506497388244956

Epoch: 5| Step: 4
Training loss: 2.6028850078582764
Validation loss: 2.3542631697911087

Epoch: 5| Step: 5
Training loss: 2.306411027908325
Validation loss: 2.359903279171195

Epoch: 5| Step: 6
Training loss: 2.7622523307800293
Validation loss: 2.3856496580185427

Epoch: 5| Step: 7
Training loss: 2.636843204498291
Validation loss: 2.401843870839765

Epoch: 5| Step: 8
Training loss: 2.6998958587646484
Validation loss: 2.3768818301539265

Epoch: 5| Step: 9
Training loss: 2.5512173175811768
Validation loss: 2.3895515857204312

Epoch: 5| Step: 10
Training loss: 2.054570436477661
Validation loss: 2.398233655960329

Epoch: 162| Step: 0
Training loss: 2.223144054412842
Validation loss: 2.3797200725924585

Epoch: 5| Step: 1
Training loss: 2.7762746810913086
Validation loss: 2.3857484248376664

Epoch: 5| Step: 2
Training loss: 3.098799228668213
Validation loss: 2.4063886109218804

Epoch: 5| Step: 3
Training loss: 1.9012947082519531
Validation loss: 2.4044788857941986

Epoch: 5| Step: 4
Training loss: 2.3374152183532715
Validation loss: 2.3825478887045257

Epoch: 5| Step: 5
Training loss: 2.584139585494995
Validation loss: 2.390739289663171

Epoch: 5| Step: 6
Training loss: 2.341203451156616
Validation loss: 2.376165595105899

Epoch: 5| Step: 7
Training loss: 2.867316722869873
Validation loss: 2.358870839559904

Epoch: 5| Step: 8
Training loss: 2.5359420776367188
Validation loss: 2.370352807865348

Epoch: 5| Step: 9
Training loss: 2.7637276649475098
Validation loss: 2.3631458333743516

Epoch: 5| Step: 10
Training loss: 2.334641695022583
Validation loss: 2.354200904087354

Epoch: 163| Step: 0
Training loss: 2.5302112102508545
Validation loss: 2.359322747876567

Epoch: 5| Step: 1
Training loss: 2.8059887886047363
Validation loss: 2.346263908570813

Epoch: 5| Step: 2
Training loss: 2.7551684379577637
Validation loss: 2.3520936094304568

Epoch: 5| Step: 3
Training loss: 2.5294463634490967
Validation loss: 2.344500490414199

Epoch: 5| Step: 4
Training loss: 2.384521245956421
Validation loss: 2.340174569878527

Epoch: 5| Step: 5
Training loss: 3.199319839477539
Validation loss: 2.36296784621413

Epoch: 5| Step: 6
Training loss: 2.676043748855591
Validation loss: 2.372306785275859

Epoch: 5| Step: 7
Training loss: 2.075324535369873
Validation loss: 2.3842852730904855

Epoch: 5| Step: 8
Training loss: 2.053840160369873
Validation loss: 2.3720510441769838

Epoch: 5| Step: 9
Training loss: 2.201115608215332
Validation loss: 2.3684050985561904

Epoch: 5| Step: 10
Training loss: 2.6013998985290527
Validation loss: 2.3572158505839687

Epoch: 164| Step: 0
Training loss: 2.612959384918213
Validation loss: 2.363532212472731

Epoch: 5| Step: 1
Training loss: 2.7799246311187744
Validation loss: 2.34189244239561

Epoch: 5| Step: 2
Training loss: 2.2525057792663574
Validation loss: 2.3534850125671714

Epoch: 5| Step: 3
Training loss: 2.948204517364502
Validation loss: 2.3460975077844437

Epoch: 5| Step: 4
Training loss: 2.902798891067505
Validation loss: 2.348622670737646

Epoch: 5| Step: 5
Training loss: 2.2072970867156982
Validation loss: 2.3540349621926584

Epoch: 5| Step: 6
Training loss: 2.0853257179260254
Validation loss: 2.34439101014086

Epoch: 5| Step: 7
Training loss: 2.5942740440368652
Validation loss: 2.3483645582711823

Epoch: 5| Step: 8
Training loss: 2.2214980125427246
Validation loss: 2.3434010013457267

Epoch: 5| Step: 9
Training loss: 2.7534093856811523
Validation loss: 2.337266778433195

Epoch: 5| Step: 10
Training loss: 2.2932560443878174
Validation loss: 2.3410775174376783

Epoch: 165| Step: 0
Training loss: 2.4695961475372314
Validation loss: 2.3453677367138606

Epoch: 5| Step: 1
Training loss: 2.689329147338867
Validation loss: 2.3420505049408122

Epoch: 5| Step: 2
Training loss: 2.330404758453369
Validation loss: 2.350068930656679

Epoch: 5| Step: 3
Training loss: 2.626302719116211
Validation loss: 2.3392749217248734

Epoch: 5| Step: 4
Training loss: 2.319068431854248
Validation loss: 2.334782572202785

Epoch: 5| Step: 5
Training loss: 2.842771530151367
Validation loss: 2.3382514728012906

Epoch: 5| Step: 6
Training loss: 2.5451197624206543
Validation loss: 2.32094068168312

Epoch: 5| Step: 7
Training loss: 1.8676412105560303
Validation loss: 2.327151281859285

Epoch: 5| Step: 8
Training loss: 1.9131996631622314
Validation loss: 2.3178961302644465

Epoch: 5| Step: 9
Training loss: 3.2376370429992676
Validation loss: 2.325227506699101

Epoch: 5| Step: 10
Training loss: 2.775012969970703
Validation loss: 2.320469364043205

Epoch: 166| Step: 0
Training loss: 2.427276134490967
Validation loss: 2.3323182995601366

Epoch: 5| Step: 1
Training loss: 2.145658016204834
Validation loss: 2.3419385456269786

Epoch: 5| Step: 2
Training loss: 2.984516143798828
Validation loss: 2.3298338485020462

Epoch: 5| Step: 3
Training loss: 1.9793884754180908
Validation loss: 2.3545553222779305

Epoch: 5| Step: 4
Training loss: 2.5085997581481934
Validation loss: 2.3645003175222747

Epoch: 5| Step: 5
Training loss: 2.658116102218628
Validation loss: 2.3881392350760837

Epoch: 5| Step: 6
Training loss: 2.7052104473114014
Validation loss: 2.398270035302767

Epoch: 5| Step: 7
Training loss: 2.8254215717315674
Validation loss: 2.384148126007408

Epoch: 5| Step: 8
Training loss: 2.3370070457458496
Validation loss: 2.3675717410220893

Epoch: 5| Step: 9
Training loss: 2.783052682876587
Validation loss: 2.3536803773654404

Epoch: 5| Step: 10
Training loss: 2.348626136779785
Validation loss: 2.370979639791673

Epoch: 167| Step: 0
Training loss: 2.058751106262207
Validation loss: 2.357884557016434

Epoch: 5| Step: 1
Training loss: 2.255659580230713
Validation loss: 2.3515797481741956

Epoch: 5| Step: 2
Training loss: 2.107950448989868
Validation loss: 2.3445516042811896

Epoch: 5| Step: 3
Training loss: 2.191044569015503
Validation loss: 2.3554147776737007

Epoch: 5| Step: 4
Training loss: 2.6247334480285645
Validation loss: 2.3577871245722615

Epoch: 5| Step: 5
Training loss: 3.046454906463623
Validation loss: 2.353390501391503

Epoch: 5| Step: 6
Training loss: 2.532005548477173
Validation loss: 2.3653880191105667

Epoch: 5| Step: 7
Training loss: 2.784180164337158
Validation loss: 2.364022139580019

Epoch: 5| Step: 8
Training loss: 2.9293971061706543
Validation loss: 2.3785462789638068

Epoch: 5| Step: 9
Training loss: 2.0878829956054688
Validation loss: 2.3519234759833223

Epoch: 5| Step: 10
Training loss: 2.9760217666625977
Validation loss: 2.3218572831922963

Epoch: 168| Step: 0
Training loss: 2.3858325481414795
Validation loss: 2.3179487874430995

Epoch: 5| Step: 1
Training loss: 2.6425817012786865
Validation loss: 2.3255320415701917

Epoch: 5| Step: 2
Training loss: 2.509500741958618
Validation loss: 2.328462275125647

Epoch: 5| Step: 3
Training loss: 2.681157350540161
Validation loss: 2.3248136325549056

Epoch: 5| Step: 4
Training loss: 2.7994918823242188
Validation loss: 2.3309924564053937

Epoch: 5| Step: 5
Training loss: 2.660771131515503
Validation loss: 2.330576204484509

Epoch: 5| Step: 6
Training loss: 1.8602606058120728
Validation loss: 2.346911217576714

Epoch: 5| Step: 7
Training loss: 2.413041114807129
Validation loss: 2.3581268069564656

Epoch: 5| Step: 8
Training loss: 2.133435010910034
Validation loss: 2.3643232109726116

Epoch: 5| Step: 9
Training loss: 2.4245095252990723
Validation loss: 2.3625163160344607

Epoch: 5| Step: 10
Training loss: 3.032707452774048
Validation loss: 2.365359931863764

Epoch: 169| Step: 0
Training loss: 2.8285415172576904
Validation loss: 2.342631783536685

Epoch: 5| Step: 1
Training loss: 2.5490260124206543
Validation loss: 2.341185144198838

Epoch: 5| Step: 2
Training loss: 2.21163010597229
Validation loss: 2.3288385150253132

Epoch: 5| Step: 3
Training loss: 2.1491355895996094
Validation loss: 2.3386033734967633

Epoch: 5| Step: 4
Training loss: 3.086205005645752
Validation loss: 2.3271841438867713

Epoch: 5| Step: 5
Training loss: 2.3674731254577637
Validation loss: 2.3245393101887037

Epoch: 5| Step: 6
Training loss: 2.5539355278015137
Validation loss: 2.3242428379674114

Epoch: 5| Step: 7
Training loss: 3.3022027015686035
Validation loss: 2.3249382383079937

Epoch: 5| Step: 8
Training loss: 1.8271362781524658
Validation loss: 2.325901259658157

Epoch: 5| Step: 9
Training loss: 2.661137104034424
Validation loss: 2.316990262718611

Epoch: 5| Step: 10
Training loss: 1.902348279953003
Validation loss: 2.3217528097091185

Epoch: 170| Step: 0
Training loss: 2.9733543395996094
Validation loss: 2.310094997447024

Epoch: 5| Step: 1
Training loss: 1.7613203525543213
Validation loss: 2.318138776286956

Epoch: 5| Step: 2
Training loss: 2.526702404022217
Validation loss: 2.3291783076460644

Epoch: 5| Step: 3
Training loss: 2.5129330158233643
Validation loss: 2.3480830205384122

Epoch: 5| Step: 4
Training loss: 1.3481497764587402
Validation loss: 2.3652271404061267

Epoch: 5| Step: 5
Training loss: 2.4357264041900635
Validation loss: 2.3656456675580753

Epoch: 5| Step: 6
Training loss: 2.9579761028289795
Validation loss: 2.347173237031506

Epoch: 5| Step: 7
Training loss: 2.302255153656006
Validation loss: 2.32047224813892

Epoch: 5| Step: 8
Training loss: 2.724412441253662
Validation loss: 2.331833936834848

Epoch: 5| Step: 9
Training loss: 2.7650821208953857
Validation loss: 2.3316924520718154

Epoch: 5| Step: 10
Training loss: 3.310521364212036
Validation loss: 2.331100117775702

Epoch: 171| Step: 0
Training loss: 2.3931713104248047
Validation loss: 2.333044111087758

Epoch: 5| Step: 1
Training loss: 2.689851760864258
Validation loss: 2.334988855546521

Epoch: 5| Step: 2
Training loss: 2.266465187072754
Validation loss: 2.345500474335045

Epoch: 5| Step: 3
Training loss: 2.0775914192199707
Validation loss: 2.33946136249009

Epoch: 5| Step: 4
Training loss: 2.2959938049316406
Validation loss: 2.3524816446406867

Epoch: 5| Step: 5
Training loss: 2.7517619132995605
Validation loss: 2.3529815058554373

Epoch: 5| Step: 6
Training loss: 3.0710220336914062
Validation loss: 2.3503679280639975

Epoch: 5| Step: 7
Training loss: 1.8205211162567139
Validation loss: 2.35413682845331

Epoch: 5| Step: 8
Training loss: 2.186190366744995
Validation loss: 2.351483339904457

Epoch: 5| Step: 9
Training loss: 3.0235791206359863
Validation loss: 2.336264233435354

Epoch: 5| Step: 10
Training loss: 2.8973262310028076
Validation loss: 2.32401204365556

Epoch: 172| Step: 0
Training loss: 2.287379741668701
Validation loss: 2.319278924695907

Epoch: 5| Step: 1
Training loss: 2.8919529914855957
Validation loss: 2.3204147610613095

Epoch: 5| Step: 2
Training loss: 2.1947875022888184
Validation loss: 2.3045884665622505

Epoch: 5| Step: 3
Training loss: 2.6490478515625
Validation loss: 2.30921051579137

Epoch: 5| Step: 4
Training loss: 1.853167176246643
Validation loss: 2.3208929851490963

Epoch: 5| Step: 5
Training loss: 2.750537633895874
Validation loss: 2.3202249311631724

Epoch: 5| Step: 6
Training loss: 2.954359531402588
Validation loss: 2.3273652804795133

Epoch: 5| Step: 7
Training loss: 2.5856120586395264
Validation loss: 2.3129579097993913

Epoch: 5| Step: 8
Training loss: 2.336596727371216
Validation loss: 2.2971296079697145

Epoch: 5| Step: 9
Training loss: 2.365544557571411
Validation loss: 2.3117303130447224

Epoch: 5| Step: 10
Training loss: 2.610560417175293
Validation loss: 2.3088278001354587

Epoch: 173| Step: 0
Training loss: 2.4961323738098145
Validation loss: 2.3108385429587415

Epoch: 5| Step: 1
Training loss: 2.3624932765960693
Validation loss: 2.3164231264463035

Epoch: 5| Step: 2
Training loss: 2.0986781120300293
Validation loss: 2.3107852781972578

Epoch: 5| Step: 3
Training loss: 2.223641872406006
Validation loss: 2.335145873408164

Epoch: 5| Step: 4
Training loss: 2.6497230529785156
Validation loss: 2.354709553462203

Epoch: 5| Step: 5
Training loss: 2.7438347339630127
Validation loss: 2.3452520088482927

Epoch: 5| Step: 6
Training loss: 2.2043323516845703
Validation loss: 2.350577510813231

Epoch: 5| Step: 7
Training loss: 2.665971040725708
Validation loss: 2.3236733764730473

Epoch: 5| Step: 8
Training loss: 2.978908061981201
Validation loss: 2.3202504816875664

Epoch: 5| Step: 9
Training loss: 2.6252379417419434
Validation loss: 2.2982550615905435

Epoch: 5| Step: 10
Training loss: 2.384305953979492
Validation loss: 2.285704694768434

Epoch: 174| Step: 0
Training loss: 2.9998269081115723
Validation loss: 2.2990484340216524

Epoch: 5| Step: 1
Training loss: 2.5009214878082275
Validation loss: 2.304442277518652

Epoch: 5| Step: 2
Training loss: 2.240859270095825
Validation loss: 2.3048033355384745

Epoch: 5| Step: 3
Training loss: 2.533453941345215
Validation loss: 2.3109596583151046

Epoch: 5| Step: 4
Training loss: 2.7800421714782715
Validation loss: 2.305842397033527

Epoch: 5| Step: 5
Training loss: 2.7750606536865234
Validation loss: 2.318553847651328

Epoch: 5| Step: 6
Training loss: 2.481076717376709
Validation loss: 2.320277339668684

Epoch: 5| Step: 7
Training loss: 2.093855142593384
Validation loss: 2.3134415482962005

Epoch: 5| Step: 8
Training loss: 2.24113392829895
Validation loss: 2.3216290166301112

Epoch: 5| Step: 9
Training loss: 1.9424110651016235
Validation loss: 2.3460954184173257

Epoch: 5| Step: 10
Training loss: 2.873896360397339
Validation loss: 2.3576096591129097

Epoch: 175| Step: 0
Training loss: 1.372003436088562
Validation loss: 2.3532216728374524

Epoch: 5| Step: 1
Training loss: 2.558774948120117
Validation loss: 2.334344438327256

Epoch: 5| Step: 2
Training loss: 2.531703472137451
Validation loss: 2.3331468643680697

Epoch: 5| Step: 3
Training loss: 2.1276397705078125
Validation loss: 2.2980646830733105

Epoch: 5| Step: 4
Training loss: 3.2287211418151855
Validation loss: 2.2936909608943488

Epoch: 5| Step: 5
Training loss: 2.465022087097168
Validation loss: 2.295933743958832

Epoch: 5| Step: 6
Training loss: 2.530151844024658
Validation loss: 2.3076839934113207

Epoch: 5| Step: 7
Training loss: 2.1687655448913574
Validation loss: 2.3150611462131625

Epoch: 5| Step: 8
Training loss: 2.954956531524658
Validation loss: 2.3168174707761375

Epoch: 5| Step: 9
Training loss: 3.2981674671173096
Validation loss: 2.3216665803745227

Epoch: 5| Step: 10
Training loss: 2.039947032928467
Validation loss: 2.309451415974607

Epoch: 176| Step: 0
Training loss: 2.9494011402130127
Validation loss: 2.303362479773901

Epoch: 5| Step: 1
Training loss: 2.2842025756835938
Validation loss: 2.30573114784815

Epoch: 5| Step: 2
Training loss: 2.2440361976623535
Validation loss: 2.329817548874886

Epoch: 5| Step: 3
Training loss: 2.184401273727417
Validation loss: 2.3238805827274116

Epoch: 5| Step: 4
Training loss: 2.30244779586792
Validation loss: 2.360232981302405

Epoch: 5| Step: 5
Training loss: 2.3167808055877686
Validation loss: 2.4032667375379995

Epoch: 5| Step: 6
Training loss: 3.0391616821289062
Validation loss: 2.4252735337903424

Epoch: 5| Step: 7
Training loss: 2.482515811920166
Validation loss: 2.425287728668541

Epoch: 5| Step: 8
Training loss: 3.4002342224121094
Validation loss: 2.3902044526992308

Epoch: 5| Step: 9
Training loss: 1.9829822778701782
Validation loss: 2.3555180641912643

Epoch: 5| Step: 10
Training loss: 2.3067455291748047
Validation loss: 2.338082239192019

Epoch: 177| Step: 0
Training loss: 2.5598552227020264
Validation loss: 2.322642182791105

Epoch: 5| Step: 1
Training loss: 2.5560994148254395
Validation loss: 2.322624688507408

Epoch: 5| Step: 2
Training loss: 1.9163062572479248
Validation loss: 2.3181325825311805

Epoch: 5| Step: 3
Training loss: 2.4181950092315674
Validation loss: 2.3346542337889313

Epoch: 5| Step: 4
Training loss: 2.714083433151245
Validation loss: 2.321251710255941

Epoch: 5| Step: 5
Training loss: 2.67360520362854
Validation loss: 2.311905821164449

Epoch: 5| Step: 6
Training loss: 2.2469887733459473
Validation loss: 2.2845943820091987

Epoch: 5| Step: 7
Training loss: 3.217557191848755
Validation loss: 2.277088754920549

Epoch: 5| Step: 8
Training loss: 2.4317526817321777
Validation loss: 2.262845162422426

Epoch: 5| Step: 9
Training loss: 2.3679583072662354
Validation loss: 2.2665417245639268

Epoch: 5| Step: 10
Training loss: 2.7018415927886963
Validation loss: 2.2705001574690624

Epoch: 178| Step: 0
Training loss: 2.7079029083251953
Validation loss: 2.3056243183792278

Epoch: 5| Step: 1
Training loss: 2.974175214767456
Validation loss: 2.3222453081479637

Epoch: 5| Step: 2
Training loss: 2.5457284450531006
Validation loss: 2.336618879789947

Epoch: 5| Step: 3
Training loss: 2.723672389984131
Validation loss: 2.3628879362537014

Epoch: 5| Step: 4
Training loss: 1.9081172943115234
Validation loss: 2.3922079276013117

Epoch: 5| Step: 5
Training loss: 2.448829174041748
Validation loss: 2.39401836036354

Epoch: 5| Step: 6
Training loss: 2.45562744140625
Validation loss: 2.369987387810984

Epoch: 5| Step: 7
Training loss: 2.49438738822937
Validation loss: 2.3275475630196194

Epoch: 5| Step: 8
Training loss: 2.3737523555755615
Validation loss: 2.3163561615892636

Epoch: 5| Step: 9
Training loss: 2.384490966796875
Validation loss: 2.294495831253708

Epoch: 5| Step: 10
Training loss: 2.3823516368865967
Validation loss: 2.295177362298453

Epoch: 179| Step: 0
Training loss: 2.38209867477417
Validation loss: 2.296998113714239

Epoch: 5| Step: 1
Training loss: 2.6868958473205566
Validation loss: 2.3045313794125795

Epoch: 5| Step: 2
Training loss: 2.68104887008667
Validation loss: 2.3163430178037254

Epoch: 5| Step: 3
Training loss: 2.9277310371398926
Validation loss: 2.333891668627339

Epoch: 5| Step: 4
Training loss: 2.4323692321777344
Validation loss: 2.329737091577181

Epoch: 5| Step: 5
Training loss: 2.4400432109832764
Validation loss: 2.3233292769360285

Epoch: 5| Step: 6
Training loss: 2.534276008605957
Validation loss: 2.327261370997275

Epoch: 5| Step: 7
Training loss: 2.1295313835144043
Validation loss: 2.315075615400909

Epoch: 5| Step: 8
Training loss: 2.8156180381774902
Validation loss: 2.304300482555102

Epoch: 5| Step: 9
Training loss: 1.938001036643982
Validation loss: 2.310015916824341

Epoch: 5| Step: 10
Training loss: 2.3002564907073975
Validation loss: 2.303927157514839

Epoch: 180| Step: 0
Training loss: 2.1828370094299316
Validation loss: 2.302517407683916

Epoch: 5| Step: 1
Training loss: 3.018608570098877
Validation loss: 2.344439447567027

Epoch: 5| Step: 2
Training loss: 2.5956430435180664
Validation loss: 2.3684822385029127

Epoch: 5| Step: 3
Training loss: 2.1690351963043213
Validation loss: 2.3945470061353458

Epoch: 5| Step: 4
Training loss: 1.7582260370254517
Validation loss: 2.3711780630132204

Epoch: 5| Step: 5
Training loss: 2.698699951171875
Validation loss: 2.3394620034002487

Epoch: 5| Step: 6
Training loss: 2.99055552482605
Validation loss: 2.3368856714617823

Epoch: 5| Step: 7
Training loss: 1.885223627090454
Validation loss: 2.3219428190621

Epoch: 5| Step: 8
Training loss: 2.5896453857421875
Validation loss: 2.307232085094657

Epoch: 5| Step: 9
Training loss: 2.8284575939178467
Validation loss: 2.314486521546559

Epoch: 5| Step: 10
Training loss: 2.2801475524902344
Validation loss: 2.315348612364902

Epoch: 181| Step: 0
Training loss: 2.4248738288879395
Validation loss: 2.306728289973351

Epoch: 5| Step: 1
Training loss: 2.637132406234741
Validation loss: 2.297210924087032

Epoch: 5| Step: 2
Training loss: 2.6431682109832764
Validation loss: 2.2875768574335242

Epoch: 5| Step: 3
Training loss: 2.405184268951416
Validation loss: 2.2849578498512186

Epoch: 5| Step: 4
Training loss: 2.1264901161193848
Validation loss: 2.2826640631562922

Epoch: 5| Step: 5
Training loss: 2.3361504077911377
Validation loss: 2.2964431778077157

Epoch: 5| Step: 6
Training loss: 3.2548515796661377
Validation loss: 2.3294451287997666

Epoch: 5| Step: 7
Training loss: 2.137667179107666
Validation loss: 2.351846843637446

Epoch: 5| Step: 8
Training loss: 2.42248272895813
Validation loss: 2.413247521205615

Epoch: 5| Step: 9
Training loss: 2.6865077018737793
Validation loss: 2.4269249644330753

Epoch: 5| Step: 10
Training loss: 1.9283958673477173
Validation loss: 2.3987450471488376

Epoch: 182| Step: 0
Training loss: 2.953248977661133
Validation loss: 2.3951355488069597

Epoch: 5| Step: 1
Training loss: 1.977343201637268
Validation loss: 2.3627201203377015

Epoch: 5| Step: 2
Training loss: 2.515026569366455
Validation loss: 2.3241555536946943

Epoch: 5| Step: 3
Training loss: 2.1610450744628906
Validation loss: 2.3187756820391585

Epoch: 5| Step: 4
Training loss: 2.294492244720459
Validation loss: 2.309955771251391

Epoch: 5| Step: 5
Training loss: 2.8039040565490723
Validation loss: 2.3024764881339124

Epoch: 5| Step: 6
Training loss: 2.626760482788086
Validation loss: 2.317507441325854

Epoch: 5| Step: 7
Training loss: 2.3397021293640137
Validation loss: 2.339284250813146

Epoch: 5| Step: 8
Training loss: 2.4116506576538086
Validation loss: 2.3583354385950233

Epoch: 5| Step: 9
Training loss: 2.7261240482330322
Validation loss: 2.363162845693609

Epoch: 5| Step: 10
Training loss: 2.5637588500976562
Validation loss: 2.351628262509582

Epoch: 183| Step: 0
Training loss: 2.431426525115967
Validation loss: 2.3307962135602067

Epoch: 5| Step: 1
Training loss: 1.8461354970932007
Validation loss: 2.3110539374812955

Epoch: 5| Step: 2
Training loss: 2.13474178314209
Validation loss: 2.31387355250697

Epoch: 5| Step: 3
Training loss: 2.9554011821746826
Validation loss: 2.320480751734908

Epoch: 5| Step: 4
Training loss: 3.0842697620391846
Validation loss: 2.341975388988372

Epoch: 5| Step: 5
Training loss: 1.967921495437622
Validation loss: 2.3754288124781784

Epoch: 5| Step: 6
Training loss: 3.1059696674346924
Validation loss: 2.378888645479756

Epoch: 5| Step: 7
Training loss: 2.4259331226348877
Validation loss: 2.37930792121477

Epoch: 5| Step: 8
Training loss: 2.3117144107818604
Validation loss: 2.332312568541496

Epoch: 5| Step: 9
Training loss: 2.6217875480651855
Validation loss: 2.306141266258814

Epoch: 5| Step: 10
Training loss: 2.368062734603882
Validation loss: 2.2850398004695935

Epoch: 184| Step: 0
Training loss: 2.9433414936065674
Validation loss: 2.2733566517470987

Epoch: 5| Step: 1
Training loss: 2.739027738571167
Validation loss: 2.2689011173863567

Epoch: 5| Step: 2
Training loss: 1.6461416482925415
Validation loss: 2.2775360871386785

Epoch: 5| Step: 3
Training loss: 2.2913992404937744
Validation loss: 2.2839866197237404

Epoch: 5| Step: 4
Training loss: 2.367365598678589
Validation loss: 2.26399383237285

Epoch: 5| Step: 5
Training loss: 2.5842788219451904
Validation loss: 2.2610654548932145

Epoch: 5| Step: 6
Training loss: 2.769291639328003
Validation loss: 2.26378878214026

Epoch: 5| Step: 7
Training loss: 2.0620293617248535
Validation loss: 2.2770224437918714

Epoch: 5| Step: 8
Training loss: 2.3360331058502197
Validation loss: 2.2776884289198023

Epoch: 5| Step: 9
Training loss: 3.003368854522705
Validation loss: 2.2792793332889514

Epoch: 5| Step: 10
Training loss: 2.3116371631622314
Validation loss: 2.277079825760216

Epoch: 185| Step: 0
Training loss: 1.9037504196166992
Validation loss: 2.3005657478045394

Epoch: 5| Step: 1
Training loss: 2.0829238891601562
Validation loss: 2.331078452448691

Epoch: 5| Step: 2
Training loss: 3.0180721282958984
Validation loss: 2.332586160270117

Epoch: 5| Step: 3
Training loss: 2.3393051624298096
Validation loss: 2.3434652025981615

Epoch: 5| Step: 4
Training loss: 2.4684982299804688
Validation loss: 2.2910099824269614

Epoch: 5| Step: 5
Training loss: 2.5957398414611816
Validation loss: 2.2874478550367456

Epoch: 5| Step: 6
Training loss: 2.10971736907959
Validation loss: 2.2752053148003033

Epoch: 5| Step: 7
Training loss: 3.1499412059783936
Validation loss: 2.2721119978094615

Epoch: 5| Step: 8
Training loss: 2.3404645919799805
Validation loss: 2.280844878124934

Epoch: 5| Step: 9
Training loss: 2.2257790565490723
Validation loss: 2.2816889388586885

Epoch: 5| Step: 10
Training loss: 2.8790974617004395
Validation loss: 2.294221629378616

Epoch: 186| Step: 0
Training loss: 2.691676378250122
Validation loss: 2.3135442451764177

Epoch: 5| Step: 1
Training loss: 2.371756076812744
Validation loss: 2.2944707767937773

Epoch: 5| Step: 2
Training loss: 2.6204590797424316
Validation loss: 2.2803621151114024

Epoch: 5| Step: 3
Training loss: 2.5918588638305664
Validation loss: 2.2865383086665982

Epoch: 5| Step: 4
Training loss: 2.4504363536834717
Validation loss: 2.278158039175054

Epoch: 5| Step: 5
Training loss: 2.414738893508911
Validation loss: 2.2828448228938605

Epoch: 5| Step: 6
Training loss: 2.199237108230591
Validation loss: 2.291553771624001

Epoch: 5| Step: 7
Training loss: 2.345308780670166
Validation loss: 2.283733105146757

Epoch: 5| Step: 8
Training loss: 2.7297260761260986
Validation loss: 2.299046493345691

Epoch: 5| Step: 9
Training loss: 2.238525867462158
Validation loss: 2.3133222287701023

Epoch: 5| Step: 10
Training loss: 2.22183895111084
Validation loss: 2.325128537352367

Epoch: 187| Step: 0
Training loss: 2.793487071990967
Validation loss: 2.3156530062357583

Epoch: 5| Step: 1
Training loss: 2.8433585166931152
Validation loss: 2.3058623344667497

Epoch: 5| Step: 2
Training loss: 1.7412059307098389
Validation loss: 2.283654461624802

Epoch: 5| Step: 3
Training loss: 2.1385438442230225
Validation loss: 2.304410675520538

Epoch: 5| Step: 4
Training loss: 2.3315820693969727
Validation loss: 2.3266106395311255

Epoch: 5| Step: 5
Training loss: 2.222144603729248
Validation loss: 2.30840116418818

Epoch: 5| Step: 6
Training loss: 2.669844150543213
Validation loss: 2.3086998565222627

Epoch: 5| Step: 7
Training loss: 2.5723867416381836
Validation loss: 2.3028295681040776

Epoch: 5| Step: 8
Training loss: 2.197087049484253
Validation loss: 2.3062126303231842

Epoch: 5| Step: 9
Training loss: 3.082287073135376
Validation loss: 2.30732121006135

Epoch: 5| Step: 10
Training loss: 2.247852087020874
Validation loss: 2.3068972492730744

Epoch: 188| Step: 0
Training loss: 2.5056300163269043
Validation loss: 2.2795557181040444

Epoch: 5| Step: 1
Training loss: 2.7682788372039795
Validation loss: 2.269355573961812

Epoch: 5| Step: 2
Training loss: 2.2916064262390137
Validation loss: 2.2634410627426638

Epoch: 5| Step: 3
Training loss: 2.565884590148926
Validation loss: 2.269720790206745

Epoch: 5| Step: 4
Training loss: 3.178527593612671
Validation loss: 2.2642802192318823

Epoch: 5| Step: 5
Training loss: 1.6802313327789307
Validation loss: 2.256949209397839

Epoch: 5| Step: 6
Training loss: 2.444709539413452
Validation loss: 2.2635745527923747

Epoch: 5| Step: 7
Training loss: 2.690390110015869
Validation loss: 2.2557936406904653

Epoch: 5| Step: 8
Training loss: 1.9447927474975586
Validation loss: 2.2416377016293105

Epoch: 5| Step: 9
Training loss: 2.401608943939209
Validation loss: 2.259605905061127

Epoch: 5| Step: 10
Training loss: 2.0850675106048584
Validation loss: 2.2725911909534084

Epoch: 189| Step: 0
Training loss: 2.7599549293518066
Validation loss: 2.3098979919187483

Epoch: 5| Step: 1
Training loss: 2.522120237350464
Validation loss: 2.3339224630786526

Epoch: 5| Step: 2
Training loss: 2.6812286376953125
Validation loss: 2.3616498029360207

Epoch: 5| Step: 3
Training loss: 2.1224238872528076
Validation loss: 2.3671766301637054

Epoch: 5| Step: 4
Training loss: 2.163684368133545
Validation loss: 2.345899594727383

Epoch: 5| Step: 5
Training loss: 3.2347426414489746
Validation loss: 2.3253332261116273

Epoch: 5| Step: 6
Training loss: 2.9021942615509033
Validation loss: 2.3160452919621624

Epoch: 5| Step: 7
Training loss: 2.292412757873535
Validation loss: 2.3287189698988393

Epoch: 5| Step: 8
Training loss: 2.6096272468566895
Validation loss: 2.3101280735385035

Epoch: 5| Step: 9
Training loss: 1.8947780132293701
Validation loss: 2.3347952135147585

Epoch: 5| Step: 10
Training loss: 1.7930386066436768
Validation loss: 2.318220515404978

Epoch: 190| Step: 0
Training loss: 2.3486571311950684
Validation loss: 2.2993993015699488

Epoch: 5| Step: 1
Training loss: 2.6230926513671875
Validation loss: 2.2760611093172463

Epoch: 5| Step: 2
Training loss: 2.4530162811279297
Validation loss: 2.255316670222949

Epoch: 5| Step: 3
Training loss: 2.443809986114502
Validation loss: 2.254543378788938

Epoch: 5| Step: 4
Training loss: 2.395350217819214
Validation loss: 2.25664302995128

Epoch: 5| Step: 5
Training loss: 2.598200559616089
Validation loss: 2.243688967920119

Epoch: 5| Step: 6
Training loss: 2.4438915252685547
Validation loss: 2.246332633879877

Epoch: 5| Step: 7
Training loss: 2.9211840629577637
Validation loss: 2.254274375977055

Epoch: 5| Step: 8
Training loss: 1.9199416637420654
Validation loss: 2.253279297582565

Epoch: 5| Step: 9
Training loss: 2.673823595046997
Validation loss: 2.268169090312014

Epoch: 5| Step: 10
Training loss: 2.034945249557495
Validation loss: 2.2722258811355918

Epoch: 191| Step: 0
Training loss: 2.47127366065979
Validation loss: 2.280385378868349

Epoch: 5| Step: 1
Training loss: 2.4318437576293945
Validation loss: 2.270794524941393

Epoch: 5| Step: 2
Training loss: 2.4336447715759277
Validation loss: 2.2863369564856253

Epoch: 5| Step: 3
Training loss: 2.348794460296631
Validation loss: 2.2781966629848687

Epoch: 5| Step: 4
Training loss: 1.8511521816253662
Validation loss: 2.2778847986652004

Epoch: 5| Step: 5
Training loss: 2.8990561962127686
Validation loss: 2.2985282841549126

Epoch: 5| Step: 6
Training loss: 2.8167529106140137
Validation loss: 2.2826399700615996

Epoch: 5| Step: 7
Training loss: 2.257589340209961
Validation loss: 2.284837202359271

Epoch: 5| Step: 8
Training loss: 2.6764278411865234
Validation loss: 2.3002987343777894

Epoch: 5| Step: 9
Training loss: 2.396735429763794
Validation loss: 2.2890893797720633

Epoch: 5| Step: 10
Training loss: 1.923113465309143
Validation loss: 2.2818090364497197

Epoch: 192| Step: 0
Training loss: 2.367408275604248
Validation loss: 2.2859629610533356

Epoch: 5| Step: 1
Training loss: 2.5348689556121826
Validation loss: 2.26825043975666

Epoch: 5| Step: 2
Training loss: 2.07737398147583
Validation loss: 2.2879472496689006

Epoch: 5| Step: 3
Training loss: 2.292644500732422
Validation loss: 2.279172525610975

Epoch: 5| Step: 4
Training loss: 2.3230462074279785
Validation loss: 2.2911544474222327

Epoch: 5| Step: 5
Training loss: 1.813065767288208
Validation loss: 2.277488382913733

Epoch: 5| Step: 6
Training loss: 2.1758999824523926
Validation loss: 2.2821090682860343

Epoch: 5| Step: 7
Training loss: 2.44087553024292
Validation loss: 2.3165136921790337

Epoch: 5| Step: 8
Training loss: 2.6210312843322754
Validation loss: 2.333897864946755

Epoch: 5| Step: 9
Training loss: 2.836003303527832
Validation loss: 2.3670860464854906

Epoch: 5| Step: 10
Training loss: 3.3585944175720215
Validation loss: 2.350928627034669

Epoch: 193| Step: 0
Training loss: 2.2176315784454346
Validation loss: 2.329051310016263

Epoch: 5| Step: 1
Training loss: 2.6223034858703613
Validation loss: 2.308003138470393

Epoch: 5| Step: 2
Training loss: 2.3218636512756348
Validation loss: 2.288885372941212

Epoch: 5| Step: 3
Training loss: 2.249530076980591
Validation loss: 2.2950025220071115

Epoch: 5| Step: 4
Training loss: 2.2615082263946533
Validation loss: 2.298842112223307

Epoch: 5| Step: 5
Training loss: 2.1050198078155518
Validation loss: 2.2660311537404216

Epoch: 5| Step: 6
Training loss: 2.649327278137207
Validation loss: 2.296643817296592

Epoch: 5| Step: 7
Training loss: 1.8912996053695679
Validation loss: 2.2703286909287974

Epoch: 5| Step: 8
Training loss: 2.777817964553833
Validation loss: 2.256549225058607

Epoch: 5| Step: 9
Training loss: 2.9285120964050293
Validation loss: 2.2641181561254684

Epoch: 5| Step: 10
Training loss: 2.3237037658691406
Validation loss: 2.2568919351024013

Epoch: 194| Step: 0
Training loss: 2.093479633331299
Validation loss: 2.266116157654793

Epoch: 5| Step: 1
Training loss: 2.049281597137451
Validation loss: 2.253159979338287

Epoch: 5| Step: 2
Training loss: 2.3153011798858643
Validation loss: 2.25295599045292

Epoch: 5| Step: 3
Training loss: 2.0037829875946045
Validation loss: 2.2426718819525933

Epoch: 5| Step: 4
Training loss: 2.509620189666748
Validation loss: 2.245275710218696

Epoch: 5| Step: 5
Training loss: 2.627622127532959
Validation loss: 2.241187636570264

Epoch: 5| Step: 6
Training loss: 2.466064929962158
Validation loss: 2.2500861049980245

Epoch: 5| Step: 7
Training loss: 3.0639848709106445
Validation loss: 2.2529836213716896

Epoch: 5| Step: 8
Training loss: 2.5711159706115723
Validation loss: 2.2836260411047165

Epoch: 5| Step: 9
Training loss: 2.1715316772460938
Validation loss: 2.294478880461826

Epoch: 5| Step: 10
Training loss: 2.7169628143310547
Validation loss: 2.307477679303897

Epoch: 195| Step: 0
Training loss: 2.267652750015259
Validation loss: 2.3181321197940457

Epoch: 5| Step: 1
Training loss: 2.456521511077881
Validation loss: 2.2772715988979546

Epoch: 5| Step: 2
Training loss: 2.663710117340088
Validation loss: 2.2602067467986897

Epoch: 5| Step: 3
Training loss: 2.0870556831359863
Validation loss: 2.2459927951135943

Epoch: 5| Step: 4
Training loss: 2.5148184299468994
Validation loss: 2.2396996303271224

Epoch: 5| Step: 5
Training loss: 2.01762318611145
Validation loss: 2.247774895801339

Epoch: 5| Step: 6
Training loss: 2.6213507652282715
Validation loss: 2.2544152326481317

Epoch: 5| Step: 7
Training loss: 2.4192118644714355
Validation loss: 2.253114915663196

Epoch: 5| Step: 8
Training loss: 2.341460704803467
Validation loss: 2.2343737925252607

Epoch: 5| Step: 9
Training loss: 2.056107521057129
Validation loss: 2.2641474405924478

Epoch: 5| Step: 10
Training loss: 3.110671043395996
Validation loss: 2.2651606272625666

Epoch: 196| Step: 0
Training loss: 2.0978875160217285
Validation loss: 2.2795541927378666

Epoch: 5| Step: 1
Training loss: 2.347719192504883
Validation loss: 2.28405418703633

Epoch: 5| Step: 2
Training loss: 2.889806032180786
Validation loss: 2.2943758785083728

Epoch: 5| Step: 3
Training loss: 2.345020294189453
Validation loss: 2.290830434009593

Epoch: 5| Step: 4
Training loss: 3.1913681030273438
Validation loss: 2.2919592677905993

Epoch: 5| Step: 5
Training loss: 2.2792725563049316
Validation loss: 2.284150508142287

Epoch: 5| Step: 6
Training loss: 1.8958003520965576
Validation loss: 2.2536256877324914

Epoch: 5| Step: 7
Training loss: 1.9593675136566162
Validation loss: 2.2534928911475727

Epoch: 5| Step: 8
Training loss: 1.6169925928115845
Validation loss: 2.2516523484260804

Epoch: 5| Step: 9
Training loss: 3.0048067569732666
Validation loss: 2.2515181290206088

Epoch: 5| Step: 10
Training loss: 2.8156239986419678
Validation loss: 2.261125615848008

Epoch: 197| Step: 0
Training loss: 1.3855253458023071
Validation loss: 2.260105238165907

Epoch: 5| Step: 1
Training loss: 2.4947195053100586
Validation loss: 2.266181566381967

Epoch: 5| Step: 2
Training loss: 2.8467423915863037
Validation loss: 2.2933850903664865

Epoch: 5| Step: 3
Training loss: 2.2488338947296143
Validation loss: 2.310004472732544

Epoch: 5| Step: 4
Training loss: 2.6953749656677246
Validation loss: 2.2835812748119397

Epoch: 5| Step: 5
Training loss: 2.3650481700897217
Validation loss: 2.2795258491270003

Epoch: 5| Step: 6
Training loss: 3.003281831741333
Validation loss: 2.277848546222974

Epoch: 5| Step: 7
Training loss: 2.5667829513549805
Validation loss: 2.2567213812182025

Epoch: 5| Step: 8
Training loss: 2.469475269317627
Validation loss: 2.267211724353093

Epoch: 5| Step: 9
Training loss: 2.567640542984009
Validation loss: 2.253336468050557

Epoch: 5| Step: 10
Training loss: 1.6695960760116577
Validation loss: 2.25586962187162

Epoch: 198| Step: 0
Training loss: 2.522921323776245
Validation loss: 2.2468279177142727

Epoch: 5| Step: 1
Training loss: 1.8262157440185547
Validation loss: 2.248986836402647

Epoch: 5| Step: 2
Training loss: 2.2068283557891846
Validation loss: 2.244802963349127

Epoch: 5| Step: 3
Training loss: 2.197401762008667
Validation loss: 2.250045830203641

Epoch: 5| Step: 4
Training loss: 2.2217330932617188
Validation loss: 2.2431287919321368

Epoch: 5| Step: 5
Training loss: 2.9290339946746826
Validation loss: 2.2387375831604004

Epoch: 5| Step: 6
Training loss: 2.65700626373291
Validation loss: 2.2459621249988513

Epoch: 5| Step: 7
Training loss: 1.9612306356430054
Validation loss: 2.249007022509011

Epoch: 5| Step: 8
Training loss: 2.7783095836639404
Validation loss: 2.288428327088715

Epoch: 5| Step: 9
Training loss: 1.890122413635254
Validation loss: 2.3237704064256404

Epoch: 5| Step: 10
Training loss: 3.2502288818359375
Validation loss: 2.305590875687138

Epoch: 199| Step: 0
Training loss: 2.0366737842559814
Validation loss: 2.2809823712994977

Epoch: 5| Step: 1
Training loss: 2.5457236766815186
Validation loss: 2.2555253377524753

Epoch: 5| Step: 2
Training loss: 1.8717867136001587
Validation loss: 2.2521337206645677

Epoch: 5| Step: 3
Training loss: 2.3501577377319336
Validation loss: 2.2616538360554683

Epoch: 5| Step: 4
Training loss: 2.3448891639709473
Validation loss: 2.234803961169335

Epoch: 5| Step: 5
Training loss: 2.019491672515869
Validation loss: 2.244635307660667

Epoch: 5| Step: 6
Training loss: 2.88348126411438
Validation loss: 2.2361568302236576

Epoch: 5| Step: 7
Training loss: 2.3856089115142822
Validation loss: 2.232493769737982

Epoch: 5| Step: 8
Training loss: 2.2334771156311035
Validation loss: 2.2279485182095597

Epoch: 5| Step: 9
Training loss: 3.031979560852051
Validation loss: 2.2447917025576354

Epoch: 5| Step: 10
Training loss: 2.454519510269165
Validation loss: 2.2592581625907653

Epoch: 200| Step: 0
Training loss: 2.1514315605163574
Validation loss: 2.274704425565658

Epoch: 5| Step: 1
Training loss: 2.302919864654541
Validation loss: 2.3271482349723898

Epoch: 5| Step: 2
Training loss: 2.561023235321045
Validation loss: 2.307914167322138

Epoch: 5| Step: 3
Training loss: 3.131030559539795
Validation loss: 2.2834792701146935

Epoch: 5| Step: 4
Training loss: 2.593982219696045
Validation loss: 2.259619851266184

Epoch: 5| Step: 5
Training loss: 2.0825037956237793
Validation loss: 2.238751603711036

Epoch: 5| Step: 6
Training loss: 2.122506618499756
Validation loss: 2.2349551775122203

Epoch: 5| Step: 7
Training loss: 2.2120256423950195
Validation loss: 2.245937265375609

Epoch: 5| Step: 8
Training loss: 2.2809102535247803
Validation loss: 2.2511774288710726

Epoch: 5| Step: 9
Training loss: 2.9179465770721436
Validation loss: 2.246964525150996

Epoch: 5| Step: 10
Training loss: 2.0432419776916504
Validation loss: 2.254995935706682

Testing loss: 2.4406218263838024

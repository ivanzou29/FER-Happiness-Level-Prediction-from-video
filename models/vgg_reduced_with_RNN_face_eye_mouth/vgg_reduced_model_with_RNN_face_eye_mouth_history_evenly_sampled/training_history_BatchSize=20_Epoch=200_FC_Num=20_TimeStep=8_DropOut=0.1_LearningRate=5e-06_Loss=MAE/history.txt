Epoch: 1| Step: 0
Training loss: 4.8783345222473145
Validation loss: 5.112529646965765

Epoch: 5| Step: 1
Training loss: 4.845025539398193
Validation loss: 5.107220029318205

Epoch: 5| Step: 2
Training loss: 5.279698848724365
Validation loss: 5.10213569928241

Epoch: 5| Step: 3
Training loss: 4.246450424194336
Validation loss: 5.097383037690194

Epoch: 5| Step: 4
Training loss: 5.5620222091674805
Validation loss: 5.092200863745905

Epoch: 5| Step: 5
Training loss: 5.514190196990967
Validation loss: 5.087007891747259

Epoch: 5| Step: 6
Training loss: 3.9327826499938965
Validation loss: 5.081813637928296

Epoch: 5| Step: 7
Training loss: 3.259615421295166
Validation loss: 5.076852629261632

Epoch: 5| Step: 8
Training loss: 6.081667900085449
Validation loss: 5.072160854134508

Epoch: 5| Step: 9
Training loss: 4.941800117492676
Validation loss: 5.066883830613987

Epoch: 5| Step: 10
Training loss: 5.229084014892578
Validation loss: 5.061206791990546

Epoch: 2| Step: 0
Training loss: 4.586822986602783
Validation loss: 5.055745150453301

Epoch: 5| Step: 1
Training loss: 4.200235366821289
Validation loss: 5.050033036098685

Epoch: 5| Step: 2
Training loss: 4.24212121963501
Validation loss: 5.043968257083688

Epoch: 5| Step: 3
Training loss: 4.88670015335083
Validation loss: 5.037619852250622

Epoch: 5| Step: 4
Training loss: 4.866607666015625
Validation loss: 5.031346074996456

Epoch: 5| Step: 5
Training loss: 5.504123210906982
Validation loss: 5.0245660710078415

Epoch: 5| Step: 6
Training loss: 5.601953029632568
Validation loss: 5.01780875011157

Epoch: 5| Step: 7
Training loss: 4.9861249923706055
Validation loss: 5.010089207721013

Epoch: 5| Step: 8
Training loss: 4.373543739318848
Validation loss: 5.002338235096265

Epoch: 5| Step: 9
Training loss: 4.896308422088623
Validation loss: 4.994474631483837

Epoch: 5| Step: 10
Training loss: 4.882845401763916
Validation loss: 4.985628317761165

Epoch: 3| Step: 0
Training loss: 4.020650863647461
Validation loss: 4.9772096141692135

Epoch: 5| Step: 1
Training loss: 5.22445011138916
Validation loss: 4.968005954578358

Epoch: 5| Step: 2
Training loss: 4.886699676513672
Validation loss: 4.958870764701597

Epoch: 5| Step: 3
Training loss: 5.174469947814941
Validation loss: 4.948826574510144

Epoch: 5| Step: 4
Training loss: 4.404909133911133
Validation loss: 4.93801232307188

Epoch: 5| Step: 5
Training loss: 4.297207832336426
Validation loss: 4.926965908337665

Epoch: 5| Step: 6
Training loss: 4.27561616897583
Validation loss: 4.9155848462094545

Epoch: 5| Step: 7
Training loss: 4.3820695877075195
Validation loss: 4.903983664768998

Epoch: 5| Step: 8
Training loss: 4.71466064453125
Validation loss: 4.891199322157009

Epoch: 5| Step: 9
Training loss: 5.124101638793945
Validation loss: 4.878599089960898

Epoch: 5| Step: 10
Training loss: 5.51956844329834
Validation loss: 4.864981461596745

Epoch: 4| Step: 0
Training loss: 5.292401313781738
Validation loss: 4.850886324400543

Epoch: 5| Step: 1
Training loss: 4.2533111572265625
Validation loss: 4.835902214050293

Epoch: 5| Step: 2
Training loss: 5.140402793884277
Validation loss: 4.820909402703726

Epoch: 5| Step: 3
Training loss: 5.008328437805176
Validation loss: 4.804840805710003

Epoch: 5| Step: 4
Training loss: 4.8577117919921875
Validation loss: 4.787585361029512

Epoch: 5| Step: 5
Training loss: 3.512714385986328
Validation loss: 4.769919190355527

Epoch: 5| Step: 6
Training loss: 4.575521469116211
Validation loss: 4.752869523981566

Epoch: 5| Step: 7
Training loss: 4.880727291107178
Validation loss: 4.734365606820711

Epoch: 5| Step: 8
Training loss: 4.982687950134277
Validation loss: 4.716129825961206

Epoch: 5| Step: 9
Training loss: 4.15432071685791
Validation loss: 4.697464348167501

Epoch: 5| Step: 10
Training loss: 3.2924933433532715
Validation loss: 4.676842635677707

Epoch: 5| Step: 0
Training loss: 5.629008769989014
Validation loss: 4.6559119327093965

Epoch: 5| Step: 1
Training loss: 3.2059714794158936
Validation loss: 4.635085367387341

Epoch: 5| Step: 2
Training loss: 3.0854668617248535
Validation loss: 4.612618451477379

Epoch: 5| Step: 3
Training loss: 5.497947692871094
Validation loss: 4.590837945220291

Epoch: 5| Step: 4
Training loss: 4.327203273773193
Validation loss: 4.567641842749811

Epoch: 5| Step: 5
Training loss: 4.034905433654785
Validation loss: 4.544319886033253

Epoch: 5| Step: 6
Training loss: 4.084188938140869
Validation loss: 4.521410890804824

Epoch: 5| Step: 7
Training loss: 3.6048240661621094
Validation loss: 4.495954162331038

Epoch: 5| Step: 8
Training loss: 4.120362281799316
Validation loss: 4.470813074419575

Epoch: 5| Step: 9
Training loss: 4.946571350097656
Validation loss: 4.446360813674106

Epoch: 5| Step: 10
Training loss: 5.18871545791626
Validation loss: 4.4228130719994985

Epoch: 6| Step: 0
Training loss: 4.514612674713135
Validation loss: 4.3977180680921

Epoch: 5| Step: 1
Training loss: 4.345551490783691
Validation loss: 4.373655621723462

Epoch: 5| Step: 2
Training loss: 4.301049709320068
Validation loss: 4.350084704737509

Epoch: 5| Step: 3
Training loss: 3.800291061401367
Validation loss: 4.326219658697805

Epoch: 5| Step: 4
Training loss: 4.325504302978516
Validation loss: 4.303001501226938

Epoch: 5| Step: 5
Training loss: 3.6717171669006348
Validation loss: 4.2796101570129395

Epoch: 5| Step: 6
Training loss: 3.355431079864502
Validation loss: 4.257949793210593

Epoch: 5| Step: 7
Training loss: 4.647148132324219
Validation loss: 4.235449047498806

Epoch: 5| Step: 8
Training loss: 3.680981397628784
Validation loss: 4.213555248834753

Epoch: 5| Step: 9
Training loss: 4.193316459655762
Validation loss: 4.193399811303744

Epoch: 5| Step: 10
Training loss: 4.12553071975708
Validation loss: 4.174344155096239

Epoch: 7| Step: 0
Training loss: 3.804276943206787
Validation loss: 4.152195181897891

Epoch: 5| Step: 1
Training loss: 3.294419765472412
Validation loss: 4.13142514997913

Epoch: 5| Step: 2
Training loss: 4.634607791900635
Validation loss: 4.112386272799585

Epoch: 5| Step: 3
Training loss: 3.5019073486328125
Validation loss: 4.0917899300975185

Epoch: 5| Step: 4
Training loss: 4.018693447113037
Validation loss: 4.07386540853849

Epoch: 5| Step: 5
Training loss: 4.174992084503174
Validation loss: 4.054370218707669

Epoch: 5| Step: 6
Training loss: 3.772956371307373
Validation loss: 4.035150607426961

Epoch: 5| Step: 7
Training loss: 4.273219108581543
Validation loss: 4.018460119924238

Epoch: 5| Step: 8
Training loss: 4.724453926086426
Validation loss: 3.99904796384996

Epoch: 5| Step: 9
Training loss: 3.136409282684326
Validation loss: 3.983438725112587

Epoch: 5| Step: 10
Training loss: 3.4165313243865967
Validation loss: 3.9663562261930077

Epoch: 8| Step: 0
Training loss: 2.626798629760742
Validation loss: 3.9521827287571405

Epoch: 5| Step: 1
Training loss: 4.356389045715332
Validation loss: 3.936353688598961

Epoch: 5| Step: 2
Training loss: 3.7274861335754395
Validation loss: 3.9224654218201995

Epoch: 5| Step: 3
Training loss: 4.061071872711182
Validation loss: 3.9057912006173083

Epoch: 5| Step: 4
Training loss: 4.109371185302734
Validation loss: 3.893015733329199

Epoch: 5| Step: 5
Training loss: 3.972712993621826
Validation loss: 3.878538998224402

Epoch: 5| Step: 6
Training loss: 3.09352445602417
Validation loss: 3.8653349594403337

Epoch: 5| Step: 7
Training loss: 4.498904705047607
Validation loss: 3.8525429720519693

Epoch: 5| Step: 8
Training loss: 4.153456211090088
Validation loss: 3.8396221258307017

Epoch: 5| Step: 9
Training loss: 3.2299041748046875
Validation loss: 3.826639618924869

Epoch: 5| Step: 10
Training loss: 3.3460311889648438
Validation loss: 3.815806617018997

Epoch: 9| Step: 0
Training loss: 4.309082984924316
Validation loss: 3.8031300472956833

Epoch: 5| Step: 1
Training loss: 2.4561095237731934
Validation loss: 3.7908007867874636

Epoch: 5| Step: 2
Training loss: 4.803287029266357
Validation loss: 3.779310918623401

Epoch: 5| Step: 3
Training loss: 2.9736006259918213
Validation loss: 3.7663516177926013

Epoch: 5| Step: 4
Training loss: 3.4296817779541016
Validation loss: 3.75510379832278

Epoch: 5| Step: 5
Training loss: 4.168871879577637
Validation loss: 3.740516811288813

Epoch: 5| Step: 6
Training loss: 3.730517864227295
Validation loss: 3.728141236048873

Epoch: 5| Step: 7
Training loss: 3.4580039978027344
Validation loss: 3.7163490736356346

Epoch: 5| Step: 8
Training loss: 2.990041971206665
Validation loss: 3.7061538388652187

Epoch: 5| Step: 9
Training loss: 4.159796237945557
Validation loss: 3.6949898555714595

Epoch: 5| Step: 10
Training loss: 3.5072624683380127
Validation loss: 3.682779630025228

Epoch: 10| Step: 0
Training loss: 3.6755454540252686
Validation loss: 3.674260380447552

Epoch: 5| Step: 1
Training loss: 3.5614326000213623
Validation loss: 3.663466666334419

Epoch: 5| Step: 2
Training loss: 3.85526704788208
Validation loss: 3.6531694678850073

Epoch: 5| Step: 3
Training loss: 3.823914051055908
Validation loss: 3.64094663435413

Epoch: 5| Step: 4
Training loss: 3.3048133850097656
Validation loss: 3.6312557702423423

Epoch: 5| Step: 5
Training loss: 3.353193998336792
Validation loss: 3.621788958067535

Epoch: 5| Step: 6
Training loss: 3.878446578979492
Validation loss: 3.611221108385312

Epoch: 5| Step: 7
Training loss: 3.2437210083007812
Validation loss: 3.6022786017387145

Epoch: 5| Step: 8
Training loss: 4.284518241882324
Validation loss: 3.591325595814695

Epoch: 5| Step: 9
Training loss: 2.4877843856811523
Validation loss: 3.5798793095414356

Epoch: 5| Step: 10
Training loss: 3.4361252784729004
Validation loss: 3.5706276175796345

Epoch: 11| Step: 0
Training loss: 3.6491711139678955
Validation loss: 3.56157947099337

Epoch: 5| Step: 1
Training loss: 3.7739551067352295
Validation loss: 3.549986498330229

Epoch: 5| Step: 2
Training loss: 3.1767523288726807
Validation loss: 3.5413135456782516

Epoch: 5| Step: 3
Training loss: 3.1871581077575684
Validation loss: 3.5312599956348376

Epoch: 5| Step: 4
Training loss: 3.1348564624786377
Validation loss: 3.5227016633556736

Epoch: 5| Step: 5
Training loss: 3.6467959880828857
Validation loss: 3.5134701472456737

Epoch: 5| Step: 6
Training loss: 3.3641128540039062
Validation loss: 3.5040971848272506

Epoch: 5| Step: 7
Training loss: 3.6144795417785645
Validation loss: 3.497230432366812

Epoch: 5| Step: 8
Training loss: 3.6396031379699707
Validation loss: 3.488764980787872

Epoch: 5| Step: 9
Training loss: 4.36699104309082
Validation loss: 3.477392255619008

Epoch: 5| Step: 10
Training loss: 2.166120767593384
Validation loss: 3.468037610412926

Epoch: 12| Step: 0
Training loss: 3.8815674781799316
Validation loss: 3.4592443794332524

Epoch: 5| Step: 1
Training loss: 3.304563045501709
Validation loss: 3.451423539910265

Epoch: 5| Step: 2
Training loss: 4.226485729217529
Validation loss: 3.441650503425188

Epoch: 5| Step: 3
Training loss: 2.7615628242492676
Validation loss: 3.430542492097424

Epoch: 5| Step: 4
Training loss: 2.8412208557128906
Validation loss: 3.4213170697612147

Epoch: 5| Step: 5
Training loss: 2.8422329425811768
Validation loss: 3.41355134594825

Epoch: 5| Step: 6
Training loss: 3.4452476501464844
Validation loss: 3.3996959886243268

Epoch: 5| Step: 7
Training loss: 3.647151231765747
Validation loss: 3.389864447296307

Epoch: 5| Step: 8
Training loss: 3.638808012008667
Validation loss: 3.3831548793341524

Epoch: 5| Step: 9
Training loss: 3.2223708629608154
Validation loss: 3.376630493389663

Epoch: 5| Step: 10
Training loss: 3.1034324169158936
Validation loss: 3.3657775925051783

Epoch: 13| Step: 0
Training loss: 3.372526168823242
Validation loss: 3.3570405180736254

Epoch: 5| Step: 1
Training loss: 4.027582168579102
Validation loss: 3.3492888840295936

Epoch: 5| Step: 2
Training loss: 3.541332960128784
Validation loss: 3.343682601887693

Epoch: 5| Step: 3
Training loss: 2.34855580329895
Validation loss: 3.3368610412843767

Epoch: 5| Step: 4
Training loss: 3.676213502883911
Validation loss: 3.3293164494217082

Epoch: 5| Step: 5
Training loss: 3.2214126586914062
Validation loss: 3.325841267903646

Epoch: 5| Step: 6
Training loss: 2.7720208168029785
Validation loss: 3.316702724784933

Epoch: 5| Step: 7
Training loss: 3.153355836868286
Validation loss: 3.311458110809326

Epoch: 5| Step: 8
Training loss: 3.3821609020233154
Validation loss: 3.3071042568452897

Epoch: 5| Step: 9
Training loss: 4.240455150604248
Validation loss: 3.301344892030121

Epoch: 5| Step: 10
Training loss: 2.2722911834716797
Validation loss: 3.2971132160514913

Epoch: 14| Step: 0
Training loss: 2.6703314781188965
Validation loss: 3.2939084473476616

Epoch: 5| Step: 1
Training loss: 3.3583884239196777
Validation loss: 3.2853277421766713

Epoch: 5| Step: 2
Training loss: 3.9192111492156982
Validation loss: 3.2855140393780125

Epoch: 5| Step: 3
Training loss: 3.2250118255615234
Validation loss: 3.2777610953136156

Epoch: 5| Step: 4
Training loss: 4.102313995361328
Validation loss: 3.2690857712940504

Epoch: 5| Step: 5
Training loss: 3.599111557006836
Validation loss: 3.2655606705655336

Epoch: 5| Step: 6
Training loss: 3.066106081008911
Validation loss: 3.2650878198685183

Epoch: 5| Step: 7
Training loss: 2.7807388305664062
Validation loss: 3.2577904603814565

Epoch: 5| Step: 8
Training loss: 2.8932080268859863
Validation loss: 3.2530872360352547

Epoch: 5| Step: 9
Training loss: 2.981367588043213
Validation loss: 3.247967653377082

Epoch: 5| Step: 10
Training loss: 3.069911003112793
Validation loss: 3.2425566257969027

Epoch: 15| Step: 0
Training loss: 3.2239983081817627
Validation loss: 3.239712320348268

Epoch: 5| Step: 1
Training loss: 3.013115406036377
Validation loss: 3.2349840466694166

Epoch: 5| Step: 2
Training loss: 2.2075400352478027
Validation loss: 3.2329807614767425

Epoch: 5| Step: 3
Training loss: 2.4264540672302246
Validation loss: 3.2255169704396236

Epoch: 5| Step: 4
Training loss: 3.7091269493103027
Validation loss: 3.222136697461528

Epoch: 5| Step: 5
Training loss: 2.757689952850342
Validation loss: 3.219526078111382

Epoch: 5| Step: 6
Training loss: 3.3324286937713623
Validation loss: 3.2140359365811912

Epoch: 5| Step: 7
Training loss: 3.4513461589813232
Validation loss: 3.2068145275115967

Epoch: 5| Step: 8
Training loss: 3.2312889099121094
Validation loss: 3.2058592996289654

Epoch: 5| Step: 9
Training loss: 3.676118850708008
Validation loss: 3.196516759933964

Epoch: 5| Step: 10
Training loss: 4.468915939331055
Validation loss: 3.1952264719111945

Epoch: 16| Step: 0
Training loss: 3.5973925590515137
Validation loss: 3.187895608204667

Epoch: 5| Step: 1
Training loss: 2.8525846004486084
Validation loss: 3.185482276383267

Epoch: 5| Step: 2
Training loss: 3.760960102081299
Validation loss: 3.1844383849892566

Epoch: 5| Step: 3
Training loss: 2.1989054679870605
Validation loss: 3.178666586517006

Epoch: 5| Step: 4
Training loss: 3.72381591796875
Validation loss: 3.173536164786226

Epoch: 5| Step: 5
Training loss: 2.3670244216918945
Validation loss: 3.1726651806985178

Epoch: 5| Step: 6
Training loss: 2.782196044921875
Validation loss: 3.169631042788106

Epoch: 5| Step: 7
Training loss: 4.206380844116211
Validation loss: 3.166183364006781

Epoch: 5| Step: 8
Training loss: 3.45708966255188
Validation loss: 3.158814158490909

Epoch: 5| Step: 9
Training loss: 2.559741497039795
Validation loss: 3.1560322648735455

Epoch: 5| Step: 10
Training loss: 3.454213857650757
Validation loss: 3.1496431212271414

Epoch: 17| Step: 0
Training loss: 3.042417049407959
Validation loss: 3.143836977661297

Epoch: 5| Step: 1
Training loss: 2.9820022583007812
Validation loss: 3.139499718143094

Epoch: 5| Step: 2
Training loss: 3.813908100128174
Validation loss: 3.1364958619558685

Epoch: 5| Step: 3
Training loss: 3.2530646324157715
Validation loss: 3.1311996136942217

Epoch: 5| Step: 4
Training loss: 3.656244993209839
Validation loss: 3.128915843143258

Epoch: 5| Step: 5
Training loss: 3.1774587631225586
Validation loss: 3.122455771251391

Epoch: 5| Step: 6
Training loss: 3.0484132766723633
Validation loss: 3.1201672554016113

Epoch: 5| Step: 7
Training loss: 2.397787570953369
Validation loss: 3.119894555819932

Epoch: 5| Step: 8
Training loss: 3.068047046661377
Validation loss: 3.1155317778228433

Epoch: 5| Step: 9
Training loss: 2.8062846660614014
Validation loss: 3.11408144427884

Epoch: 5| Step: 10
Training loss: 3.400642156600952
Validation loss: 3.1099519627068632

Epoch: 18| Step: 0
Training loss: 2.6507887840270996
Validation loss: 3.1077582913060344

Epoch: 5| Step: 1
Training loss: 2.689307451248169
Validation loss: 3.1009616262169293

Epoch: 5| Step: 2
Training loss: 3.232513427734375
Validation loss: 3.0976142985846407

Epoch: 5| Step: 3
Training loss: 3.056973934173584
Validation loss: 3.095364511653941

Epoch: 5| Step: 4
Training loss: 3.7475948333740234
Validation loss: 3.0950157334727626

Epoch: 5| Step: 5
Training loss: 2.1461634635925293
Validation loss: 3.095355890130484

Epoch: 5| Step: 6
Training loss: 3.7310776710510254
Validation loss: 3.092640261496267

Epoch: 5| Step: 7
Training loss: 2.9720888137817383
Validation loss: 3.093743965189944

Epoch: 5| Step: 8
Training loss: 4.267233848571777
Validation loss: 3.088534503854731

Epoch: 5| Step: 9
Training loss: 3.2081847190856934
Validation loss: 3.0820431324743454

Epoch: 5| Step: 10
Training loss: 2.574038505554199
Validation loss: 3.081845701381724

Epoch: 19| Step: 0
Training loss: 3.4617831707000732
Validation loss: 3.0739798648383028

Epoch: 5| Step: 1
Training loss: 3.4852118492126465
Validation loss: 3.0714424220464562

Epoch: 5| Step: 2
Training loss: 2.7031266689300537
Validation loss: 3.0657500400338122

Epoch: 5| Step: 3
Training loss: 3.917092800140381
Validation loss: 3.0662373471003708

Epoch: 5| Step: 4
Training loss: 2.562094211578369
Validation loss: 3.0644769284033004

Epoch: 5| Step: 5
Training loss: 3.2483162879943848
Validation loss: 3.0580776404309016

Epoch: 5| Step: 6
Training loss: 3.7248332500457764
Validation loss: 3.0572844987274497

Epoch: 5| Step: 7
Training loss: 3.0792317390441895
Validation loss: 3.0536421063125774

Epoch: 5| Step: 8
Training loss: 2.8685784339904785
Validation loss: 3.0551390647888184

Epoch: 5| Step: 9
Training loss: 2.727508783340454
Validation loss: 3.054367631994268

Epoch: 5| Step: 10
Training loss: 2.2265501022338867
Validation loss: 3.051238198434153

Epoch: 20| Step: 0
Training loss: 3.6038830280303955
Validation loss: 3.0447773369409705

Epoch: 5| Step: 1
Training loss: 2.6941440105438232
Validation loss: 3.0468714160303914

Epoch: 5| Step: 2
Training loss: 3.8751437664031982
Validation loss: 3.041534105936686

Epoch: 5| Step: 3
Training loss: 3.0771520137786865
Validation loss: 3.043074354048698

Epoch: 5| Step: 4
Training loss: 3.060062885284424
Validation loss: 3.041430793782716

Epoch: 5| Step: 5
Training loss: 2.4111874103546143
Validation loss: 3.037067603039485

Epoch: 5| Step: 6
Training loss: 3.170874834060669
Validation loss: 3.035718689682663

Epoch: 5| Step: 7
Training loss: 3.1395347118377686
Validation loss: 3.0392713085297616

Epoch: 5| Step: 8
Training loss: 2.7803452014923096
Validation loss: 3.0386913745634017

Epoch: 5| Step: 9
Training loss: 3.3085696697235107
Validation loss: 3.035229459885628

Epoch: 5| Step: 10
Training loss: 2.8106985092163086
Validation loss: 3.0327562234734975

Epoch: 21| Step: 0
Training loss: 3.3210747241973877
Validation loss: 3.0301549203934206

Epoch: 5| Step: 1
Training loss: 3.5951857566833496
Validation loss: 3.0257396262179137

Epoch: 5| Step: 2
Training loss: 3.1267151832580566
Validation loss: 3.019894282023112

Epoch: 5| Step: 3
Training loss: 3.2245121002197266
Validation loss: 3.0207829731766895

Epoch: 5| Step: 4
Training loss: 3.4485697746276855
Validation loss: 3.015699607069774

Epoch: 5| Step: 5
Training loss: 2.892434597015381
Validation loss: 3.0124446551005044

Epoch: 5| Step: 6
Training loss: 2.7284646034240723
Validation loss: 3.0121563378200737

Epoch: 5| Step: 7
Training loss: 3.713425397872925
Validation loss: 3.0075821594525407

Epoch: 5| Step: 8
Training loss: 2.0822033882141113
Validation loss: 3.0043852072890087

Epoch: 5| Step: 9
Training loss: 3.3462040424346924
Validation loss: 3.0020622079090407

Epoch: 5| Step: 10
Training loss: 2.17535400390625
Validation loss: 3.0019639333089194

Epoch: 22| Step: 0
Training loss: 2.9775829315185547
Validation loss: 3.001180156584709

Epoch: 5| Step: 1
Training loss: 3.0631699562072754
Validation loss: 3.0030012335828555

Epoch: 5| Step: 2
Training loss: 2.6752495765686035
Validation loss: 3.0001433510934152

Epoch: 5| Step: 3
Training loss: 4.062054634094238
Validation loss: 2.9980219025765695

Epoch: 5| Step: 4
Training loss: 2.988621234893799
Validation loss: 2.9928640960365214

Epoch: 5| Step: 5
Training loss: 2.401505470275879
Validation loss: 2.9890504293544318

Epoch: 5| Step: 6
Training loss: 2.857168674468994
Validation loss: 2.9910744928544566

Epoch: 5| Step: 7
Training loss: 2.8823342323303223
Validation loss: 2.989983694527739

Epoch: 5| Step: 8
Training loss: 3.3689770698547363
Validation loss: 2.9850013153527373

Epoch: 5| Step: 9
Training loss: 2.5605309009552
Validation loss: 2.981676068357242

Epoch: 5| Step: 10
Training loss: 3.916604518890381
Validation loss: 2.978669704929475

Epoch: 23| Step: 0
Training loss: 2.400886058807373
Validation loss: 2.980048030935308

Epoch: 5| Step: 1
Training loss: 2.517124652862549
Validation loss: 2.9778106776616906

Epoch: 5| Step: 2
Training loss: 3.1656641960144043
Validation loss: 2.9785376005275275

Epoch: 5| Step: 3
Training loss: 2.5437285900115967
Validation loss: 2.9764982551656742

Epoch: 5| Step: 4
Training loss: 2.80987548828125
Validation loss: 2.9766113681177937

Epoch: 5| Step: 5
Training loss: 3.6857192516326904
Validation loss: 2.9878891104011127

Epoch: 5| Step: 6
Training loss: 3.1900689601898193
Validation loss: 2.974484982029084

Epoch: 5| Step: 7
Training loss: 3.3647499084472656
Validation loss: 2.9735627251286663

Epoch: 5| Step: 8
Training loss: 2.596409797668457
Validation loss: 2.971352715646067

Epoch: 5| Step: 9
Training loss: 3.6653778553009033
Validation loss: 2.971078395843506

Epoch: 5| Step: 10
Training loss: 3.6541976928710938
Validation loss: 2.9662699904493106

Epoch: 24| Step: 0
Training loss: 2.1422314643859863
Validation loss: 2.969994642401254

Epoch: 5| Step: 1
Training loss: 2.342203378677368
Validation loss: 2.9640640725371656

Epoch: 5| Step: 2
Training loss: 3.5417122840881348
Validation loss: 2.966625275150422

Epoch: 5| Step: 3
Training loss: 3.0278987884521484
Validation loss: 2.9662172281613914

Epoch: 5| Step: 4
Training loss: 3.5743954181671143
Validation loss: 2.962563330127347

Epoch: 5| Step: 5
Training loss: 3.264486312866211
Validation loss: 2.960028625303699

Epoch: 5| Step: 6
Training loss: 2.6657259464263916
Validation loss: 2.9610256123286423

Epoch: 5| Step: 7
Training loss: 3.796804904937744
Validation loss: 2.9596472350499963

Epoch: 5| Step: 8
Training loss: 2.9530720710754395
Validation loss: 2.956405572993781

Epoch: 5| Step: 9
Training loss: 2.842578411102295
Validation loss: 2.952947252540178

Epoch: 5| Step: 10
Training loss: 3.295936346054077
Validation loss: 2.9545316901258243

Epoch: 25| Step: 0
Training loss: 3.0238535404205322
Validation loss: 2.96410793899208

Epoch: 5| Step: 1
Training loss: 3.3860716819763184
Validation loss: 2.9550862748135804

Epoch: 5| Step: 2
Training loss: 3.1112513542175293
Validation loss: 2.949538187314105

Epoch: 5| Step: 3
Training loss: 2.758330821990967
Validation loss: 2.9489813517498713

Epoch: 5| Step: 4
Training loss: 3.6355843544006348
Validation loss: 2.9473464053164244

Epoch: 5| Step: 5
Training loss: 2.9593300819396973
Validation loss: 2.9485529366359917

Epoch: 5| Step: 6
Training loss: 2.93131685256958
Validation loss: 2.9448632347968315

Epoch: 5| Step: 7
Training loss: 2.7753942012786865
Validation loss: 2.943489561798752

Epoch: 5| Step: 8
Training loss: 2.714752674102783
Validation loss: 2.939794599369008

Epoch: 5| Step: 9
Training loss: 2.9860446453094482
Validation loss: 2.9435144342401975

Epoch: 5| Step: 10
Training loss: 3.009809732437134
Validation loss: 2.937699815278412

Epoch: 26| Step: 0
Training loss: 3.079047679901123
Validation loss: 2.9367116369226927

Epoch: 5| Step: 1
Training loss: 3.6997809410095215
Validation loss: 2.9366058790555565

Epoch: 5| Step: 2
Training loss: 2.9090428352355957
Validation loss: 2.9364045768655758

Epoch: 5| Step: 3
Training loss: 2.828058958053589
Validation loss: 2.932137143227362

Epoch: 5| Step: 4
Training loss: 2.5187649726867676
Validation loss: 2.9290568828582764

Epoch: 5| Step: 5
Training loss: 3.1749038696289062
Validation loss: 2.931387183486774

Epoch: 5| Step: 6
Training loss: 2.0199732780456543
Validation loss: 2.9285091430910173

Epoch: 5| Step: 7
Training loss: 3.4720447063446045
Validation loss: 2.927037400584067

Epoch: 5| Step: 8
Training loss: 3.106780767440796
Validation loss: 2.9250155059240197

Epoch: 5| Step: 9
Training loss: 2.3226542472839355
Validation loss: 2.9286911410670124

Epoch: 5| Step: 10
Training loss: 4.247670650482178
Validation loss: 2.9298690339570403

Epoch: 27| Step: 0
Training loss: 2.8707149028778076
Validation loss: 2.9265936292627805

Epoch: 5| Step: 1
Training loss: 2.8006815910339355
Validation loss: 2.9244938460729455

Epoch: 5| Step: 2
Training loss: 3.2632956504821777
Validation loss: 2.920364059427733

Epoch: 5| Step: 3
Training loss: 2.704763889312744
Validation loss: 2.9205563555481615

Epoch: 5| Step: 4
Training loss: 3.1666035652160645
Validation loss: 2.920958431818152

Epoch: 5| Step: 5
Training loss: 2.6875462532043457
Validation loss: 2.9196393643656084

Epoch: 5| Step: 6
Training loss: 2.7686362266540527
Validation loss: 2.919807234118062

Epoch: 5| Step: 7
Training loss: 3.7052550315856934
Validation loss: 2.917074206054852

Epoch: 5| Step: 8
Training loss: 3.208721876144409
Validation loss: 2.9135330543723157

Epoch: 5| Step: 9
Training loss: 2.938650131225586
Validation loss: 2.9147638787505445

Epoch: 5| Step: 10
Training loss: 2.988288164138794
Validation loss: 2.913049926039993

Epoch: 28| Step: 0
Training loss: 2.650385618209839
Validation loss: 2.9157967157261346

Epoch: 5| Step: 1
Training loss: 2.557555913925171
Validation loss: 2.9157382621560046

Epoch: 5| Step: 2
Training loss: 3.1795873641967773
Validation loss: 2.914107068892448

Epoch: 5| Step: 3
Training loss: 3.0989603996276855
Validation loss: 2.9152970416571504

Epoch: 5| Step: 4
Training loss: 3.5213470458984375
Validation loss: 2.9137250249103834

Epoch: 5| Step: 5
Training loss: 2.3177356719970703
Validation loss: 2.9118381520753265

Epoch: 5| Step: 6
Training loss: 3.2052338123321533
Validation loss: 2.9091224619137344

Epoch: 5| Step: 7
Training loss: 3.387538433074951
Validation loss: 2.909152356527185

Epoch: 5| Step: 8
Training loss: 2.906062602996826
Validation loss: 2.9070012082335768

Epoch: 5| Step: 9
Training loss: 2.867840051651001
Validation loss: 2.9055621572720107

Epoch: 5| Step: 10
Training loss: 3.4255049228668213
Validation loss: 2.9056554789184244

Epoch: 29| Step: 0
Training loss: 3.319939136505127
Validation loss: 2.904335939756004

Epoch: 5| Step: 1
Training loss: 2.7076048851013184
Validation loss: 2.903408412010439

Epoch: 5| Step: 2
Training loss: 3.277540683746338
Validation loss: 2.9020438809548654

Epoch: 5| Step: 3
Training loss: 3.2576842308044434
Validation loss: 2.903028885523478

Epoch: 5| Step: 4
Training loss: 2.8908920288085938
Validation loss: 2.9024303754170737

Epoch: 5| Step: 5
Training loss: 2.2156033515930176
Validation loss: 2.901847416354764

Epoch: 5| Step: 6
Training loss: 3.5407047271728516
Validation loss: 2.9038204839152675

Epoch: 5| Step: 7
Training loss: 3.306666135787964
Validation loss: 2.9024012498958136

Epoch: 5| Step: 8
Training loss: 2.6719815731048584
Validation loss: 2.897463898504934

Epoch: 5| Step: 9
Training loss: 3.0733561515808105
Validation loss: 2.8961590336215113

Epoch: 5| Step: 10
Training loss: 2.6457576751708984
Validation loss: 2.8950498283550306

Epoch: 30| Step: 0
Training loss: 3.119645595550537
Validation loss: 2.8955126680353636

Epoch: 5| Step: 1
Training loss: 2.8654868602752686
Validation loss: 2.893552695551226

Epoch: 5| Step: 2
Training loss: 1.9879354238510132
Validation loss: 2.893696984937114

Epoch: 5| Step: 3
Training loss: 2.9593777656555176
Validation loss: 2.8926360196964715

Epoch: 5| Step: 4
Training loss: 3.540363311767578
Validation loss: 2.89142442775029

Epoch: 5| Step: 5
Training loss: 3.1993985176086426
Validation loss: 2.8895451484187955

Epoch: 5| Step: 6
Training loss: 3.1833393573760986
Validation loss: 2.8903710713950534

Epoch: 5| Step: 7
Training loss: 3.150118589401245
Validation loss: 2.8909649541301112

Epoch: 5| Step: 8
Training loss: 4.210184097290039
Validation loss: 2.887831962236794

Epoch: 5| Step: 9
Training loss: 2.014037609100342
Validation loss: 2.8885746232924925

Epoch: 5| Step: 10
Training loss: 2.590920925140381
Validation loss: 2.8867167836876324

Epoch: 31| Step: 0
Training loss: 2.407580614089966
Validation loss: 2.8870491135504937

Epoch: 5| Step: 1
Training loss: 1.8688157796859741
Validation loss: 2.8879179851983183

Epoch: 5| Step: 2
Training loss: 2.6711723804473877
Validation loss: 2.886117048161004

Epoch: 5| Step: 3
Training loss: 3.809487819671631
Validation loss: 2.8820400571310394

Epoch: 5| Step: 4
Training loss: 3.014831781387329
Validation loss: 2.881888376769199

Epoch: 5| Step: 5
Training loss: 2.4923977851867676
Validation loss: 2.8820644604262484

Epoch: 5| Step: 6
Training loss: 3.947493076324463
Validation loss: 2.880074034455002

Epoch: 5| Step: 7
Training loss: 2.954253673553467
Validation loss: 2.877614600684053

Epoch: 5| Step: 8
Training loss: 3.272785186767578
Validation loss: 2.878047086859262

Epoch: 5| Step: 9
Training loss: 2.714736223220825
Validation loss: 2.876115004221598

Epoch: 5| Step: 10
Training loss: 3.7798941135406494
Validation loss: 2.878473348515008

Epoch: 32| Step: 0
Training loss: 2.816741466522217
Validation loss: 2.8764408993464645

Epoch: 5| Step: 1
Training loss: 3.4761784076690674
Validation loss: 2.8774073329023135

Epoch: 5| Step: 2
Training loss: 2.8054018020629883
Validation loss: 2.8750649421445784

Epoch: 5| Step: 3
Training loss: 2.9722108840942383
Validation loss: 2.8725967253408125

Epoch: 5| Step: 4
Training loss: 3.1812262535095215
Validation loss: 2.8770280550884944

Epoch: 5| Step: 5
Training loss: 2.655561923980713
Validation loss: 2.873950563451295

Epoch: 5| Step: 6
Training loss: 3.1228957176208496
Validation loss: 2.872446398581228

Epoch: 5| Step: 7
Training loss: 2.602699041366577
Validation loss: 2.87335870599234

Epoch: 5| Step: 8
Training loss: 3.2238121032714844
Validation loss: 2.8719969359777306

Epoch: 5| Step: 9
Training loss: 2.784146785736084
Validation loss: 2.8708572233876875

Epoch: 5| Step: 10
Training loss: 3.110823631286621
Validation loss: 2.873073088225498

Epoch: 33| Step: 0
Training loss: 4.418738842010498
Validation loss: 2.869988267139722

Epoch: 5| Step: 1
Training loss: 2.6138482093811035
Validation loss: 2.867535501398066

Epoch: 5| Step: 2
Training loss: 2.8581669330596924
Validation loss: 2.868986378433884

Epoch: 5| Step: 3
Training loss: 3.3323605060577393
Validation loss: 2.866241214095905

Epoch: 5| Step: 4
Training loss: 2.7751646041870117
Validation loss: 2.869283009600896

Epoch: 5| Step: 5
Training loss: 3.291179656982422
Validation loss: 2.8657938844414166

Epoch: 5| Step: 6
Training loss: 2.201007604598999
Validation loss: 2.8647671284214145

Epoch: 5| Step: 7
Training loss: 2.072924852371216
Validation loss: 2.868036493178337

Epoch: 5| Step: 8
Training loss: 3.329202651977539
Validation loss: 2.8633936374418196

Epoch: 5| Step: 9
Training loss: 2.642979145050049
Validation loss: 2.859784754373694

Epoch: 5| Step: 10
Training loss: 3.187314748764038
Validation loss: 2.860712412864931

Epoch: 34| Step: 0
Training loss: 3.289977550506592
Validation loss: 2.8597879281608005

Epoch: 5| Step: 1
Training loss: 3.0257492065429688
Validation loss: 2.8591800453842326

Epoch: 5| Step: 2
Training loss: 2.8652420043945312
Validation loss: 2.858043832163657

Epoch: 5| Step: 3
Training loss: 3.1920125484466553
Validation loss: 2.8557310206915743

Epoch: 5| Step: 4
Training loss: 3.043627977371216
Validation loss: 2.8598766326904297

Epoch: 5| Step: 5
Training loss: 2.94986891746521
Validation loss: 2.8582729883091424

Epoch: 5| Step: 6
Training loss: 2.680856227874756
Validation loss: 2.8587984526029198

Epoch: 5| Step: 7
Training loss: 3.0707106590270996
Validation loss: 2.8570534516406316

Epoch: 5| Step: 8
Training loss: 2.6597514152526855
Validation loss: 2.8550870444184993

Epoch: 5| Step: 9
Training loss: 2.310655117034912
Validation loss: 2.8598960317591184

Epoch: 5| Step: 10
Training loss: 3.650554895401001
Validation loss: 2.8590212637378323

Epoch: 35| Step: 0
Training loss: 3.81740140914917
Validation loss: 2.8578202057910222

Epoch: 5| Step: 1
Training loss: 3.0014331340789795
Validation loss: 2.8559994389933925

Epoch: 5| Step: 2
Training loss: 3.1086935997009277
Validation loss: 2.8554501661690335

Epoch: 5| Step: 3
Training loss: 3.5316338539123535
Validation loss: 2.852341518607191

Epoch: 5| Step: 4
Training loss: 2.8913509845733643
Validation loss: 2.853206854994579

Epoch: 5| Step: 5
Training loss: 2.401211738586426
Validation loss: 2.84907187954072

Epoch: 5| Step: 6
Training loss: 2.5184578895568848
Validation loss: 2.85069937603448

Epoch: 5| Step: 7
Training loss: 2.4703147411346436
Validation loss: 2.8508424887093167

Epoch: 5| Step: 8
Training loss: 3.2191097736358643
Validation loss: 2.851771790494201

Epoch: 5| Step: 9
Training loss: 2.8428120613098145
Validation loss: 2.850419375204271

Epoch: 5| Step: 10
Training loss: 2.7576961517333984
Validation loss: 2.846728147998933

Epoch: 36| Step: 0
Training loss: 3.4294662475585938
Validation loss: 2.8481013364689325

Epoch: 5| Step: 1
Training loss: 3.1034255027770996
Validation loss: 2.849074371399418

Epoch: 5| Step: 2
Training loss: 2.7249820232391357
Validation loss: 2.850686942377398

Epoch: 5| Step: 3
Training loss: 3.398326873779297
Validation loss: 2.8430973534942954

Epoch: 5| Step: 4
Training loss: 3.3430514335632324
Validation loss: 2.8444672989588913

Epoch: 5| Step: 5
Training loss: 2.6439108848571777
Validation loss: 2.8434639387233283

Epoch: 5| Step: 6
Training loss: 2.8050131797790527
Validation loss: 2.8454348989712295

Epoch: 5| Step: 7
Training loss: 2.558291435241699
Validation loss: 2.8457137128358245

Epoch: 5| Step: 8
Training loss: 2.9105381965637207
Validation loss: 2.8471064670111543

Epoch: 5| Step: 9
Training loss: 3.2727394104003906
Validation loss: 2.84789853454918

Epoch: 5| Step: 10
Training loss: 2.267073392868042
Validation loss: 2.8439889518163537

Epoch: 37| Step: 0
Training loss: 3.106044054031372
Validation loss: 2.845852046884516

Epoch: 5| Step: 1
Training loss: 2.9315617084503174
Validation loss: 2.8435835889590684

Epoch: 5| Step: 2
Training loss: 3.5235114097595215
Validation loss: 2.844108227760561

Epoch: 5| Step: 3
Training loss: 2.673792839050293
Validation loss: 2.8442591313392884

Epoch: 5| Step: 4
Training loss: 3.326162815093994
Validation loss: 2.8473340798449773

Epoch: 5| Step: 5
Training loss: 3.473245143890381
Validation loss: 2.847434987304031

Epoch: 5| Step: 6
Training loss: 2.288437604904175
Validation loss: 2.8403371687858336

Epoch: 5| Step: 7
Training loss: 3.085317373275757
Validation loss: 2.846919805772843

Epoch: 5| Step: 8
Training loss: 3.1838736534118652
Validation loss: 2.8458023686562814

Epoch: 5| Step: 9
Training loss: 2.365428924560547
Validation loss: 2.8459467067513415

Epoch: 5| Step: 10
Training loss: 2.516815662384033
Validation loss: 2.8454941011244252

Epoch: 38| Step: 0
Training loss: 2.148538827896118
Validation loss: 2.8432714605844147

Epoch: 5| Step: 1
Training loss: 1.6490154266357422
Validation loss: 2.8477200872154644

Epoch: 5| Step: 2
Training loss: 3.021122455596924
Validation loss: 2.849163432275095

Epoch: 5| Step: 3
Training loss: 3.502411365509033
Validation loss: 2.846601963043213

Epoch: 5| Step: 4
Training loss: 3.152003765106201
Validation loss: 2.843977261615056

Epoch: 5| Step: 5
Training loss: 2.9049770832061768
Validation loss: 2.84228511523175

Epoch: 5| Step: 6
Training loss: 3.2770819664001465
Validation loss: 2.841751031978156

Epoch: 5| Step: 7
Training loss: 3.3159396648406982
Validation loss: 2.8409719185162614

Epoch: 5| Step: 8
Training loss: 3.066941499710083
Validation loss: 2.8386569869133735

Epoch: 5| Step: 9
Training loss: 3.0276169776916504
Validation loss: 2.8393876860218663

Epoch: 5| Step: 10
Training loss: 3.5426769256591797
Validation loss: 2.838695728650657

Epoch: 39| Step: 0
Training loss: 3.415571928024292
Validation loss: 2.840987866924655

Epoch: 5| Step: 1
Training loss: 3.3012046813964844
Validation loss: 2.836469857923446

Epoch: 5| Step: 2
Training loss: 2.0937106609344482
Validation loss: 2.8351819387046238

Epoch: 5| Step: 3
Training loss: 2.530116558074951
Validation loss: 2.8394246588471117

Epoch: 5| Step: 4
Training loss: 3.134404182434082
Validation loss: 2.8320384845938733

Epoch: 5| Step: 5
Training loss: 2.6343417167663574
Validation loss: 2.8341903840341875

Epoch: 5| Step: 6
Training loss: 3.269503116607666
Validation loss: 2.836392071939284

Epoch: 5| Step: 7
Training loss: 3.2281031608581543
Validation loss: 2.832625996681952

Epoch: 5| Step: 8
Training loss: 3.305926561355591
Validation loss: 2.8353134432146625

Epoch: 5| Step: 9
Training loss: 2.9508256912231445
Validation loss: 2.835187227495255

Epoch: 5| Step: 10
Training loss: 2.5579707622528076
Validation loss: 2.832960956840105

Epoch: 40| Step: 0
Training loss: 2.9857547283172607
Validation loss: 2.832455465870519

Epoch: 5| Step: 1
Training loss: 3.0723025798797607
Validation loss: 2.833042057611609

Epoch: 5| Step: 2
Training loss: 2.1250224113464355
Validation loss: 2.8304505220023533

Epoch: 5| Step: 3
Training loss: 3.4925589561462402
Validation loss: 2.8328506280017156

Epoch: 5| Step: 4
Training loss: 2.4893887042999268
Validation loss: 2.8309263593407086

Epoch: 5| Step: 5
Training loss: 2.876427412033081
Validation loss: 2.8283068979940107

Epoch: 5| Step: 6
Training loss: 2.729203462600708
Validation loss: 2.8312997894902385

Epoch: 5| Step: 7
Training loss: 3.659104824066162
Validation loss: 2.831348534553282

Epoch: 5| Step: 8
Training loss: 3.663693904876709
Validation loss: 2.829716508106519

Epoch: 5| Step: 9
Training loss: 2.93137788772583
Validation loss: 2.8322457549392537

Epoch: 5| Step: 10
Training loss: 2.325929880142212
Validation loss: 2.8281204367196686

Epoch: 41| Step: 0
Training loss: 2.3271453380584717
Validation loss: 2.829150835673014

Epoch: 5| Step: 1
Training loss: 2.6376900672912598
Validation loss: 2.8298583645974436

Epoch: 5| Step: 2
Training loss: 2.818601131439209
Validation loss: 2.8281904138544554

Epoch: 5| Step: 3
Training loss: 2.743940591812134
Validation loss: 2.8279762242429998

Epoch: 5| Step: 4
Training loss: 3.437303066253662
Validation loss: 2.8250851528618925

Epoch: 5| Step: 5
Training loss: 3.0745790004730225
Validation loss: 2.8272272515040573

Epoch: 5| Step: 6
Training loss: 2.620196580886841
Validation loss: 2.8263872105588197

Epoch: 5| Step: 7
Training loss: 3.5547587871551514
Validation loss: 2.8261793069942023

Epoch: 5| Step: 8
Training loss: 3.1264984607696533
Validation loss: 2.825860777208882

Epoch: 5| Step: 9
Training loss: 3.338198184967041
Validation loss: 2.825596358186455

Epoch: 5| Step: 10
Training loss: 2.6682658195495605
Validation loss: 2.8246656156355336

Epoch: 42| Step: 0
Training loss: 3.146300792694092
Validation loss: 2.8248761033499115

Epoch: 5| Step: 1
Training loss: 2.807553768157959
Validation loss: 2.8233733305367092

Epoch: 5| Step: 2
Training loss: 2.854189395904541
Validation loss: 2.824780515445176

Epoch: 5| Step: 3
Training loss: 3.8254802227020264
Validation loss: 2.8252235125469904

Epoch: 5| Step: 4
Training loss: 2.7943296432495117
Validation loss: 2.826220025298416

Epoch: 5| Step: 5
Training loss: 2.698253631591797
Validation loss: 2.823023555099323

Epoch: 5| Step: 6
Training loss: 3.1294236183166504
Validation loss: 2.827160535320159

Epoch: 5| Step: 7
Training loss: 2.959268569946289
Validation loss: 2.8215074795548634

Epoch: 5| Step: 8
Training loss: 2.217759847640991
Validation loss: 2.8220757771563787

Epoch: 5| Step: 9
Training loss: 3.1381704807281494
Validation loss: 2.8210655668730378

Epoch: 5| Step: 10
Training loss: 2.7748472690582275
Validation loss: 2.821823227790094

Epoch: 43| Step: 0
Training loss: 3.139549970626831
Validation loss: 2.8195914735076246

Epoch: 5| Step: 1
Training loss: 2.9906535148620605
Validation loss: 2.82038103893239

Epoch: 5| Step: 2
Training loss: 1.7518084049224854
Validation loss: 2.8218010779350036

Epoch: 5| Step: 3
Training loss: 3.4537930488586426
Validation loss: 2.822173221136934

Epoch: 5| Step: 4
Training loss: 4.01498556137085
Validation loss: 2.821250674545124

Epoch: 5| Step: 5
Training loss: 2.720341205596924
Validation loss: 2.819696198227585

Epoch: 5| Step: 6
Training loss: 2.727971076965332
Validation loss: 2.8223702651198193

Epoch: 5| Step: 7
Training loss: 2.984417676925659
Validation loss: 2.819161381772769

Epoch: 5| Step: 8
Training loss: 2.352304697036743
Validation loss: 2.8203086083935154

Epoch: 5| Step: 9
Training loss: 3.3250725269317627
Validation loss: 2.8189779379034556

Epoch: 5| Step: 10
Training loss: 2.8525524139404297
Validation loss: 2.818114706265029

Epoch: 44| Step: 0
Training loss: 2.9927918910980225
Validation loss: 2.8171878168659825

Epoch: 5| Step: 1
Training loss: 3.027337074279785
Validation loss: 2.818984206004809

Epoch: 5| Step: 2
Training loss: 3.4532268047332764
Validation loss: 2.8205659722769134

Epoch: 5| Step: 3
Training loss: 2.38698410987854
Validation loss: 2.8153609255308747

Epoch: 5| Step: 4
Training loss: 3.327191114425659
Validation loss: 2.8180889583403066

Epoch: 5| Step: 5
Training loss: 1.98771071434021
Validation loss: 2.821527114478491

Epoch: 5| Step: 6
Training loss: 2.7162203788757324
Validation loss: 2.8175650565854964

Epoch: 5| Step: 7
Training loss: 3.4710097312927246
Validation loss: 2.816049457878195

Epoch: 5| Step: 8
Training loss: 2.6606783866882324
Validation loss: 2.815700348987374

Epoch: 5| Step: 9
Training loss: 3.294419050216675
Validation loss: 2.8154076145541285

Epoch: 5| Step: 10
Training loss: 3.031266689300537
Validation loss: 2.8162099340910554

Epoch: 45| Step: 0
Training loss: 2.7066006660461426
Validation loss: 2.828089975541638

Epoch: 5| Step: 1
Training loss: 2.908184766769409
Validation loss: 2.8209053342060377

Epoch: 5| Step: 2
Training loss: 3.199435234069824
Validation loss: 2.8123647371927896

Epoch: 5| Step: 3
Training loss: 2.5229897499084473
Validation loss: 2.8123816956755934

Epoch: 5| Step: 4
Training loss: 2.703665018081665
Validation loss: 2.8132718763043805

Epoch: 5| Step: 5
Training loss: 2.6860296726226807
Validation loss: 2.8174022987324703

Epoch: 5| Step: 6
Training loss: 3.255038022994995
Validation loss: 2.818123648243566

Epoch: 5| Step: 7
Training loss: 3.09525990486145
Validation loss: 2.821334415866483

Epoch: 5| Step: 8
Training loss: 2.559854507446289
Validation loss: 2.8188690036855717

Epoch: 5| Step: 9
Training loss: 3.2736868858337402
Validation loss: 2.8204213085994927

Epoch: 5| Step: 10
Training loss: 3.529306411743164
Validation loss: 2.818296470949727

Epoch: 46| Step: 0
Training loss: 2.5910191535949707
Validation loss: 2.8191880641445035

Epoch: 5| Step: 1
Training loss: 2.669569969177246
Validation loss: 2.8174232359855407

Epoch: 5| Step: 2
Training loss: 3.4539546966552734
Validation loss: 2.8148663864340833

Epoch: 5| Step: 3
Training loss: 2.739490032196045
Validation loss: 2.8149551717183923

Epoch: 5| Step: 4
Training loss: 3.7868144512176514
Validation loss: 2.8121462278468634

Epoch: 5| Step: 5
Training loss: 3.1585757732391357
Validation loss: 2.8168313400719756

Epoch: 5| Step: 6
Training loss: 2.7396531105041504
Validation loss: 2.8149560138743412

Epoch: 5| Step: 7
Training loss: 3.188934803009033
Validation loss: 2.8164485987796577

Epoch: 5| Step: 8
Training loss: 2.7430033683776855
Validation loss: 2.8138060057035057

Epoch: 5| Step: 9
Training loss: 2.3319666385650635
Validation loss: 2.8169466654459634

Epoch: 5| Step: 10
Training loss: 2.863372325897217
Validation loss: 2.813977533771146

Epoch: 47| Step: 0
Training loss: 2.5069613456726074
Validation loss: 2.8111594184752433

Epoch: 5| Step: 1
Training loss: 2.7383742332458496
Validation loss: 2.809862152222664

Epoch: 5| Step: 2
Training loss: 3.0812554359436035
Validation loss: 2.8116990084289224

Epoch: 5| Step: 3
Training loss: 2.6841959953308105
Validation loss: 2.810460816147507

Epoch: 5| Step: 4
Training loss: 3.6697754859924316
Validation loss: 2.807306589618806

Epoch: 5| Step: 5
Training loss: 3.2801754474639893
Validation loss: 2.805631083826865

Epoch: 5| Step: 6
Training loss: 2.848872661590576
Validation loss: 2.8056409410251084

Epoch: 5| Step: 7
Training loss: 3.3720192909240723
Validation loss: 2.807513654872935

Epoch: 5| Step: 8
Training loss: 3.113860845565796
Validation loss: 2.805866390146235

Epoch: 5| Step: 9
Training loss: 2.7053418159484863
Validation loss: 2.8073398810560986

Epoch: 5| Step: 10
Training loss: 2.11342453956604
Validation loss: 2.805029917788762

Epoch: 48| Step: 0
Training loss: 3.025587558746338
Validation loss: 2.8036008983530025

Epoch: 5| Step: 1
Training loss: 3.566439390182495
Validation loss: 2.8032340541962655

Epoch: 5| Step: 2
Training loss: 2.8170981407165527
Validation loss: 2.8053250748624086

Epoch: 5| Step: 3
Training loss: 3.716442584991455
Validation loss: 2.8015709615522817

Epoch: 5| Step: 4
Training loss: 2.301809310913086
Validation loss: 2.802695010298042

Epoch: 5| Step: 5
Training loss: 3.6537017822265625
Validation loss: 2.8030328391700663

Epoch: 5| Step: 6
Training loss: 2.849186420440674
Validation loss: 2.8023119357324417

Epoch: 5| Step: 7
Training loss: 2.640599489212036
Validation loss: 2.801378042467179

Epoch: 5| Step: 8
Training loss: 2.6132214069366455
Validation loss: 2.80298460683515

Epoch: 5| Step: 9
Training loss: 2.429964065551758
Validation loss: 2.8022362775700067

Epoch: 5| Step: 10
Training loss: 2.543241500854492
Validation loss: 2.80605548684315

Epoch: 49| Step: 0
Training loss: 3.1690516471862793
Validation loss: 2.802781838242726

Epoch: 5| Step: 1
Training loss: 2.301586389541626
Validation loss: 2.8059360647714264

Epoch: 5| Step: 2
Training loss: 2.947267532348633
Validation loss: 2.806686550058344

Epoch: 5| Step: 3
Training loss: 3.049713611602783
Validation loss: 2.803613590937789

Epoch: 5| Step: 4
Training loss: 3.0113046169281006
Validation loss: 2.8014360961093696

Epoch: 5| Step: 5
Training loss: 3.247741222381592
Validation loss: 2.7998462441147014

Epoch: 5| Step: 6
Training loss: 2.971322536468506
Validation loss: 2.7976700618702877

Epoch: 5| Step: 7
Training loss: 3.0321900844573975
Validation loss: 2.7973105933076594

Epoch: 5| Step: 8
Training loss: 2.4772942066192627
Validation loss: 2.7964033772868495

Epoch: 5| Step: 9
Training loss: 2.670610189437866
Validation loss: 2.799549792402534

Epoch: 5| Step: 10
Training loss: 3.3986594676971436
Validation loss: 2.798677921295166

Epoch: 50| Step: 0
Training loss: 2.7238733768463135
Validation loss: 2.7995482413999495

Epoch: 5| Step: 1
Training loss: 3.0477936267852783
Validation loss: 2.7987487341768

Epoch: 5| Step: 2
Training loss: 2.1966052055358887
Validation loss: 2.79861403793417

Epoch: 5| Step: 3
Training loss: 2.9412827491760254
Validation loss: 2.798993315747989

Epoch: 5| Step: 4
Training loss: 2.554180145263672
Validation loss: 2.797983818156745

Epoch: 5| Step: 5
Training loss: 2.8954877853393555
Validation loss: 2.7974611610494633

Epoch: 5| Step: 6
Training loss: 3.4549148082733154
Validation loss: 2.7980830541221042

Epoch: 5| Step: 7
Training loss: 3.6334495544433594
Validation loss: 2.800264230338476

Epoch: 5| Step: 8
Training loss: 2.5614144802093506
Validation loss: 2.7975557747707573

Epoch: 5| Step: 9
Training loss: 3.639047145843506
Validation loss: 2.797716648347916

Epoch: 5| Step: 10
Training loss: 2.4574341773986816
Validation loss: 2.7959437049845213

Epoch: 51| Step: 0
Training loss: 2.8144466876983643
Validation loss: 2.7940593611809517

Epoch: 5| Step: 1
Training loss: 2.5044124126434326
Validation loss: 2.7964397604747484

Epoch: 5| Step: 2
Training loss: 3.432659864425659
Validation loss: 2.7951246538469867

Epoch: 5| Step: 3
Training loss: 2.283252000808716
Validation loss: 2.7946272511636057

Epoch: 5| Step: 4
Training loss: 3.152954578399658
Validation loss: 2.7967268882259244

Epoch: 5| Step: 5
Training loss: 3.3292853832244873
Validation loss: 2.7959019907059206

Epoch: 5| Step: 6
Training loss: 2.651996374130249
Validation loss: 2.7942994025445755

Epoch: 5| Step: 7
Training loss: 2.745588541030884
Validation loss: 2.7936991696716635

Epoch: 5| Step: 8
Training loss: 3.5915534496307373
Validation loss: 2.7942041607313257

Epoch: 5| Step: 9
Training loss: 3.5026180744171143
Validation loss: 2.7938505987967215

Epoch: 5| Step: 10
Training loss: 1.9947552680969238
Validation loss: 2.793821983439948

Epoch: 52| Step: 0
Training loss: 3.567169189453125
Validation loss: 2.7918551327079855

Epoch: 5| Step: 1
Training loss: 2.634347915649414
Validation loss: 2.7934306795879076

Epoch: 5| Step: 2
Training loss: 3.3564674854278564
Validation loss: 2.7930979395425446

Epoch: 5| Step: 3
Training loss: 2.5093495845794678
Validation loss: 2.7944435919484785

Epoch: 5| Step: 4
Training loss: 3.009930372238159
Validation loss: 2.7939430718780844

Epoch: 5| Step: 5
Training loss: 2.9933693408966064
Validation loss: 2.796865383783976

Epoch: 5| Step: 6
Training loss: 2.5932230949401855
Validation loss: 2.794314694660966

Epoch: 5| Step: 7
Training loss: 3.6066672801971436
Validation loss: 2.7941087010086223

Epoch: 5| Step: 8
Training loss: 2.1542716026306152
Validation loss: 2.7952412969322613

Epoch: 5| Step: 9
Training loss: 3.570809841156006
Validation loss: 2.7911427303027083

Epoch: 5| Step: 10
Training loss: 1.9934463500976562
Validation loss: 2.791229437756282

Epoch: 53| Step: 0
Training loss: 2.2082390785217285
Validation loss: 2.791557163320562

Epoch: 5| Step: 1
Training loss: 3.162095308303833
Validation loss: 2.79076059146594

Epoch: 5| Step: 2
Training loss: 3.0441489219665527
Validation loss: 2.7923397351336736

Epoch: 5| Step: 3
Training loss: 2.7379508018493652
Validation loss: 2.7915480495781027

Epoch: 5| Step: 4
Training loss: 2.4267420768737793
Validation loss: 2.7894904254585184

Epoch: 5| Step: 5
Training loss: 3.353125810623169
Validation loss: 2.788586001242361

Epoch: 5| Step: 6
Training loss: 3.1087136268615723
Validation loss: 2.789559705283052

Epoch: 5| Step: 7
Training loss: 2.4869253635406494
Validation loss: 2.7894007621272916

Epoch: 5| Step: 8
Training loss: 2.898043394088745
Validation loss: 2.789145326101652

Epoch: 5| Step: 9
Training loss: 3.092639446258545
Validation loss: 2.7895614947042158

Epoch: 5| Step: 10
Training loss: 3.707087755203247
Validation loss: 2.788001496304748

Epoch: 54| Step: 0
Training loss: 2.78749418258667
Validation loss: 2.7877501723586873

Epoch: 5| Step: 1
Training loss: 2.6129584312438965
Validation loss: 2.7894855647958736

Epoch: 5| Step: 2
Training loss: 2.379537343978882
Validation loss: 2.786875214627994

Epoch: 5| Step: 3
Training loss: 3.3930487632751465
Validation loss: 2.7879974329343407

Epoch: 5| Step: 4
Training loss: 3.210538387298584
Validation loss: 2.787028028118995

Epoch: 5| Step: 5
Training loss: 3.2885546684265137
Validation loss: 2.789361817862398

Epoch: 5| Step: 6
Training loss: 2.7745766639709473
Validation loss: 2.785744062034033

Epoch: 5| Step: 7
Training loss: 3.13227915763855
Validation loss: 2.7908705895946873

Epoch: 5| Step: 8
Training loss: 3.6429524421691895
Validation loss: 2.7884781616990284

Epoch: 5| Step: 9
Training loss: 2.440455913543701
Validation loss: 2.7895361069710023

Epoch: 5| Step: 10
Training loss: 2.3117520809173584
Validation loss: 2.7885088407865135

Epoch: 55| Step: 0
Training loss: 2.9035580158233643
Validation loss: 2.7903517958938435

Epoch: 5| Step: 1
Training loss: 3.055536985397339
Validation loss: 2.7888421473964566

Epoch: 5| Step: 2
Training loss: 2.320491313934326
Validation loss: 2.792619594963648

Epoch: 5| Step: 3
Training loss: 3.3487625122070312
Validation loss: 2.791580820596346

Epoch: 5| Step: 4
Training loss: 3.3166613578796387
Validation loss: 2.789059067285189

Epoch: 5| Step: 5
Training loss: 2.9912679195404053
Validation loss: 2.7897306283315024

Epoch: 5| Step: 6
Training loss: 3.0693118572235107
Validation loss: 2.7859554572771956

Epoch: 5| Step: 7
Training loss: 2.5097434520721436
Validation loss: 2.7853891182971258

Epoch: 5| Step: 8
Training loss: 2.884896993637085
Validation loss: 2.784123833461474

Epoch: 5| Step: 9
Training loss: 2.0956709384918213
Validation loss: 2.782382111395559

Epoch: 5| Step: 10
Training loss: 3.718290328979492
Validation loss: 2.78593397396867

Epoch: 56| Step: 0
Training loss: 2.568605899810791
Validation loss: 2.7850460903618925

Epoch: 5| Step: 1
Training loss: 2.9289183616638184
Validation loss: 2.785129685555735

Epoch: 5| Step: 2
Training loss: 3.102182388305664
Validation loss: 2.7881286964621594

Epoch: 5| Step: 3
Training loss: 2.899956464767456
Validation loss: 2.7882608111186693

Epoch: 5| Step: 4
Training loss: 2.5739495754241943
Validation loss: 2.7857299209922872

Epoch: 5| Step: 5
Training loss: 3.4885458946228027
Validation loss: 2.784742970620432

Epoch: 5| Step: 6
Training loss: 2.2062785625457764
Validation loss: 2.7825104933913036

Epoch: 5| Step: 7
Training loss: 2.5902035236358643
Validation loss: 2.786484090230798

Epoch: 5| Step: 8
Training loss: 3.310469150543213
Validation loss: 2.785748025422455

Epoch: 5| Step: 9
Training loss: 3.310891628265381
Validation loss: 2.7851598032059206

Epoch: 5| Step: 10
Training loss: 3.089944839477539
Validation loss: 2.784706133668141

Epoch: 57| Step: 0
Training loss: 2.1569952964782715
Validation loss: 2.782695395972139

Epoch: 5| Step: 1
Training loss: 2.576941967010498
Validation loss: 2.7809703273157917

Epoch: 5| Step: 2
Training loss: 3.10428524017334
Validation loss: 2.783033586317493

Epoch: 5| Step: 3
Training loss: 3.3645730018615723
Validation loss: 2.7805307885651946

Epoch: 5| Step: 4
Training loss: 2.784289836883545
Validation loss: 2.7842809923233522

Epoch: 5| Step: 5
Training loss: 3.3265233039855957
Validation loss: 2.7875673899086575

Epoch: 5| Step: 6
Training loss: 3.391577959060669
Validation loss: 2.786253562537573

Epoch: 5| Step: 7
Training loss: 3.3690457344055176
Validation loss: 2.7846786206768406

Epoch: 5| Step: 8
Training loss: 2.2294955253601074
Validation loss: 2.788044939758957

Epoch: 5| Step: 9
Training loss: 2.8576817512512207
Validation loss: 2.787639666629094

Epoch: 5| Step: 10
Training loss: 2.8314270973205566
Validation loss: 2.7808806614209245

Epoch: 58| Step: 0
Training loss: 3.339712619781494
Validation loss: 2.777012143083798

Epoch: 5| Step: 1
Training loss: 2.6783149242401123
Validation loss: 2.7785761484535794

Epoch: 5| Step: 2
Training loss: 2.574892044067383
Validation loss: 2.776248619120608

Epoch: 5| Step: 3
Training loss: 2.8546760082244873
Validation loss: 2.776044081616145

Epoch: 5| Step: 4
Training loss: 3.5983588695526123
Validation loss: 2.777882565734207

Epoch: 5| Step: 5
Training loss: 2.7335057258605957
Validation loss: 2.776361785909181

Epoch: 5| Step: 6
Training loss: 3.052905797958374
Validation loss: 2.7740028007056123

Epoch: 5| Step: 7
Training loss: 2.9834234714508057
Validation loss: 2.775772363908829

Epoch: 5| Step: 8
Training loss: 2.224940061569214
Validation loss: 2.774805586825135

Epoch: 5| Step: 9
Training loss: 2.9642086029052734
Validation loss: 2.7707329898752193

Epoch: 5| Step: 10
Training loss: 2.9580774307250977
Validation loss: 2.771480083465576

Epoch: 59| Step: 0
Training loss: 2.8208069801330566
Validation loss: 2.770260795470207

Epoch: 5| Step: 1
Training loss: 2.943958282470703
Validation loss: 2.7680538982473393

Epoch: 5| Step: 2
Training loss: 2.7906618118286133
Validation loss: 2.769902488236786

Epoch: 5| Step: 3
Training loss: 4.224469184875488
Validation loss: 2.7708240708997174

Epoch: 5| Step: 4
Training loss: 3.305985689163208
Validation loss: 2.768307711488457

Epoch: 5| Step: 5
Training loss: 2.5189170837402344
Validation loss: 2.769003245138353

Epoch: 5| Step: 6
Training loss: 2.884566307067871
Validation loss: 2.7676012618567354

Epoch: 5| Step: 7
Training loss: 2.2004971504211426
Validation loss: 2.76645730644144

Epoch: 5| Step: 8
Training loss: 2.5684878826141357
Validation loss: 2.7720581869925223

Epoch: 5| Step: 9
Training loss: 2.8518564701080322
Validation loss: 2.772033219696373

Epoch: 5| Step: 10
Training loss: 2.7934775352478027
Validation loss: 2.7681054812605663

Epoch: 60| Step: 0
Training loss: 3.435767412185669
Validation loss: 2.7698935513855307

Epoch: 5| Step: 1
Training loss: 2.681962728500366
Validation loss: 2.768634816651703

Epoch: 5| Step: 2
Training loss: 2.5741374492645264
Validation loss: 2.769664825931672

Epoch: 5| Step: 3
Training loss: 2.059583902359009
Validation loss: 2.7710206072817565

Epoch: 5| Step: 4
Training loss: 3.6563384532928467
Validation loss: 2.7671561497513966

Epoch: 5| Step: 5
Training loss: 2.3891706466674805
Validation loss: 2.7640268289914696

Epoch: 5| Step: 6
Training loss: 3.610874891281128
Validation loss: 2.7687997228355816

Epoch: 5| Step: 7
Training loss: 2.215188503265381
Validation loss: 2.765114753477035

Epoch: 5| Step: 8
Training loss: 2.3814635276794434
Validation loss: 2.7662914260741203

Epoch: 5| Step: 9
Training loss: 3.414165496826172
Validation loss: 2.766442806490006

Epoch: 5| Step: 10
Training loss: 3.542137622833252
Validation loss: 2.761389522142308

Epoch: 61| Step: 0
Training loss: 2.8150620460510254
Validation loss: 2.762966812297862

Epoch: 5| Step: 1
Training loss: 3.1281142234802246
Validation loss: 2.761564426524665

Epoch: 5| Step: 2
Training loss: 3.005293369293213
Validation loss: 2.765982304849932

Epoch: 5| Step: 3
Training loss: 3.256676197052002
Validation loss: 2.7632384300231934

Epoch: 5| Step: 4
Training loss: 3.1591501235961914
Validation loss: 2.7603504478290515

Epoch: 5| Step: 5
Training loss: 2.663325786590576
Validation loss: 2.7631532120448288

Epoch: 5| Step: 6
Training loss: 1.9563190937042236
Validation loss: 2.758939404641428

Epoch: 5| Step: 7
Training loss: 3.397728681564331
Validation loss: 2.760990817059753

Epoch: 5| Step: 8
Training loss: 2.6669583320617676
Validation loss: 2.7586585347370436

Epoch: 5| Step: 9
Training loss: 2.736353635787964
Validation loss: 2.7616209060915056

Epoch: 5| Step: 10
Training loss: 3.093165636062622
Validation loss: 2.759543949557889

Epoch: 62| Step: 0
Training loss: 2.871154308319092
Validation loss: 2.755979730236915

Epoch: 5| Step: 1
Training loss: 3.3396172523498535
Validation loss: 2.752287441684354

Epoch: 5| Step: 2
Training loss: 3.0038812160491943
Validation loss: 2.752975425412578

Epoch: 5| Step: 3
Training loss: 2.8000736236572266
Validation loss: 2.7573433973455943

Epoch: 5| Step: 4
Training loss: 3.490205764770508
Validation loss: 2.7520430229043447

Epoch: 5| Step: 5
Training loss: 2.7956345081329346
Validation loss: 2.7489430571115143

Epoch: 5| Step: 6
Training loss: 2.1274991035461426
Validation loss: 2.7548379000797065

Epoch: 5| Step: 7
Training loss: 2.46760892868042
Validation loss: 2.7527562264473207

Epoch: 5| Step: 8
Training loss: 3.3774242401123047
Validation loss: 2.7501988590404554

Epoch: 5| Step: 9
Training loss: 2.6843388080596924
Validation loss: 2.7466188194931194

Epoch: 5| Step: 10
Training loss: 2.79634428024292
Validation loss: 2.7474203853196997

Epoch: 63| Step: 0
Training loss: 3.312894344329834
Validation loss: 2.74699907149038

Epoch: 5| Step: 1
Training loss: 2.92022442817688
Validation loss: 2.745119256357993

Epoch: 5| Step: 2
Training loss: 3.20739483833313
Validation loss: 2.743171294530233

Epoch: 5| Step: 3
Training loss: 2.457484483718872
Validation loss: 2.7453526604560112

Epoch: 5| Step: 4
Training loss: 2.3097195625305176
Validation loss: 2.74493573557946

Epoch: 5| Step: 5
Training loss: 3.214249849319458
Validation loss: 2.744505930972356

Epoch: 5| Step: 6
Training loss: 2.8284473419189453
Validation loss: 2.7407315495193645

Epoch: 5| Step: 7
Training loss: 3.5646355152130127
Validation loss: 2.742653908268098

Epoch: 5| Step: 8
Training loss: 2.8447699546813965
Validation loss: 2.7458681111694663

Epoch: 5| Step: 9
Training loss: 2.4618465900421143
Validation loss: 2.740617395729147

Epoch: 5| Step: 10
Training loss: 2.5627801418304443
Validation loss: 2.746870479276103

Epoch: 64| Step: 0
Training loss: 3.2549636363983154
Validation loss: 2.7466028890302105

Epoch: 5| Step: 1
Training loss: 2.833858013153076
Validation loss: 2.745903476592033

Epoch: 5| Step: 2
Training loss: 2.918919801712036
Validation loss: 2.7450125037982898

Epoch: 5| Step: 3
Training loss: 2.4309005737304688
Validation loss: 2.7432924291139007

Epoch: 5| Step: 4
Training loss: 2.7578837871551514
Validation loss: 2.743592321231801

Epoch: 5| Step: 5
Training loss: 2.723602056503296
Validation loss: 2.746906908609534

Epoch: 5| Step: 6
Training loss: 3.3959975242614746
Validation loss: 2.742164816907657

Epoch: 5| Step: 7
Training loss: 2.7512879371643066
Validation loss: 2.7406463059045936

Epoch: 5| Step: 8
Training loss: 3.3869471549987793
Validation loss: 2.7385652398550384

Epoch: 5| Step: 9
Training loss: 2.969804048538208
Validation loss: 2.740605454291067

Epoch: 5| Step: 10
Training loss: 2.1541879177093506
Validation loss: 2.73948166190937

Epoch: 65| Step: 0
Training loss: 3.7109737396240234
Validation loss: 2.744903015834029

Epoch: 5| Step: 1
Training loss: 3.709444522857666
Validation loss: 2.7422857669091996

Epoch: 5| Step: 2
Training loss: 2.6521010398864746
Validation loss: 2.738282003710347

Epoch: 5| Step: 3
Training loss: 2.1200928688049316
Validation loss: 2.7394789649594213

Epoch: 5| Step: 4
Training loss: 2.412515163421631
Validation loss: 2.7379471076432096

Epoch: 5| Step: 5
Training loss: 3.0007424354553223
Validation loss: 2.7402765930339856

Epoch: 5| Step: 6
Training loss: 2.6777729988098145
Validation loss: 2.739998289333877

Epoch: 5| Step: 7
Training loss: 2.923858642578125
Validation loss: 2.7396609808809016

Epoch: 5| Step: 8
Training loss: 3.620211362838745
Validation loss: 2.737767337470926

Epoch: 5| Step: 9
Training loss: 2.9216856956481934
Validation loss: 2.7373192438515286

Epoch: 5| Step: 10
Training loss: 1.7704328298568726
Validation loss: 2.738535027350149

Epoch: 66| Step: 0
Training loss: 2.1100668907165527
Validation loss: 2.738034253479332

Epoch: 5| Step: 1
Training loss: 2.532163143157959
Validation loss: 2.7393908295580136

Epoch: 5| Step: 2
Training loss: 3.2191569805145264
Validation loss: 2.737082432675105

Epoch: 5| Step: 3
Training loss: 2.748046636581421
Validation loss: 2.733444659940658

Epoch: 5| Step: 4
Training loss: 2.878530979156494
Validation loss: 2.7368824174327235

Epoch: 5| Step: 5
Training loss: 3.1753406524658203
Validation loss: 2.736433070193055

Epoch: 5| Step: 6
Training loss: 2.4691314697265625
Validation loss: 2.736183081903765

Epoch: 5| Step: 7
Training loss: 3.410202741622925
Validation loss: 2.7349871563655075

Epoch: 5| Step: 8
Training loss: 3.8142266273498535
Validation loss: 2.7379286853216027

Epoch: 5| Step: 9
Training loss: 2.387206554412842
Validation loss: 2.739333901354062

Epoch: 5| Step: 10
Training loss: 2.9276938438415527
Validation loss: 2.732677044407014

Epoch: 67| Step: 0
Training loss: 3.685572862625122
Validation loss: 2.7337069767777638

Epoch: 5| Step: 1
Training loss: 2.0249762535095215
Validation loss: 2.7308164129975023

Epoch: 5| Step: 2
Training loss: 3.108649492263794
Validation loss: 2.7327424633887505

Epoch: 5| Step: 3
Training loss: 2.8990375995635986
Validation loss: 2.7342232247834564

Epoch: 5| Step: 4
Training loss: 2.4692542552948
Validation loss: 2.7338526812932824

Epoch: 5| Step: 5
Training loss: 2.8270065784454346
Validation loss: 2.7375758950428297

Epoch: 5| Step: 6
Training loss: 3.0481419563293457
Validation loss: 2.7376041207262265

Epoch: 5| Step: 7
Training loss: 3.45013165473938
Validation loss: 2.7386681828447568

Epoch: 5| Step: 8
Training loss: 2.9389936923980713
Validation loss: 2.7370304548612205

Epoch: 5| Step: 9
Training loss: 2.5059757232666016
Validation loss: 2.7354967132691415

Epoch: 5| Step: 10
Training loss: 2.6479320526123047
Validation loss: 2.7346329560843845

Epoch: 68| Step: 0
Training loss: 3.026610851287842
Validation loss: 2.733697416961834

Epoch: 5| Step: 1
Training loss: 2.958564043045044
Validation loss: 2.7344677525181926

Epoch: 5| Step: 2
Training loss: 1.8842090368270874
Validation loss: 2.732000973916823

Epoch: 5| Step: 3
Training loss: 3.231914520263672
Validation loss: 2.735089002117034

Epoch: 5| Step: 4
Training loss: 3.5957024097442627
Validation loss: 2.7295708656311035

Epoch: 5| Step: 5
Training loss: 2.6497011184692383
Validation loss: 2.728183282318936

Epoch: 5| Step: 6
Training loss: 4.147139072418213
Validation loss: 2.7297597803095335

Epoch: 5| Step: 7
Training loss: 2.5962321758270264
Validation loss: 2.733832428532262

Epoch: 5| Step: 8
Training loss: 2.014216899871826
Validation loss: 2.73136378360051

Epoch: 5| Step: 9
Training loss: 2.9151768684387207
Validation loss: 2.735696831057149

Epoch: 5| Step: 10
Training loss: 2.5430309772491455
Validation loss: 2.731102584510721

Epoch: 69| Step: 0
Training loss: 3.256892442703247
Validation loss: 2.731209893380442

Epoch: 5| Step: 1
Training loss: 3.161588430404663
Validation loss: 2.732005188542028

Epoch: 5| Step: 2
Training loss: 2.97090482711792
Validation loss: 2.7286517876450733

Epoch: 5| Step: 3
Training loss: 3.0301337242126465
Validation loss: 2.727554980144706

Epoch: 5| Step: 4
Training loss: 2.959519624710083
Validation loss: 2.7292220131043465

Epoch: 5| Step: 5
Training loss: 3.1648783683776855
Validation loss: 2.7294319342541438

Epoch: 5| Step: 6
Training loss: 2.4574813842773438
Validation loss: 2.7278388289995092

Epoch: 5| Step: 7
Training loss: 2.8852016925811768
Validation loss: 2.73001052230917

Epoch: 5| Step: 8
Training loss: 2.7948145866394043
Validation loss: 2.727325770162767

Epoch: 5| Step: 9
Training loss: 2.7787411212921143
Validation loss: 2.7284825309630363

Epoch: 5| Step: 10
Training loss: 2.0122432708740234
Validation loss: 2.727168147281934

Epoch: 70| Step: 0
Training loss: 3.8348591327667236
Validation loss: 2.7300124142759588

Epoch: 5| Step: 1
Training loss: 2.1445767879486084
Validation loss: 2.7300443316018708

Epoch: 5| Step: 2
Training loss: 3.3609299659729004
Validation loss: 2.7264431907284643

Epoch: 5| Step: 3
Training loss: 2.8046581745147705
Validation loss: 2.7262075690812964

Epoch: 5| Step: 4
Training loss: 2.774827480316162
Validation loss: 2.7226045388047413

Epoch: 5| Step: 5
Training loss: 2.4964282512664795
Validation loss: 2.7228790252439437

Epoch: 5| Step: 6
Training loss: 2.8748910427093506
Validation loss: 2.7276335300937777

Epoch: 5| Step: 7
Training loss: 2.8870365619659424
Validation loss: 2.729149277492236

Epoch: 5| Step: 8
Training loss: 3.051607131958008
Validation loss: 2.724553105651691

Epoch: 5| Step: 9
Training loss: 3.1875624656677246
Validation loss: 2.728226743718629

Epoch: 5| Step: 10
Training loss: 2.0350489616394043
Validation loss: 2.7266450466648227

Epoch: 71| Step: 0
Training loss: 2.8940253257751465
Validation loss: 2.728023836689611

Epoch: 5| Step: 1
Training loss: 2.756568193435669
Validation loss: 2.7257142656592914

Epoch: 5| Step: 2
Training loss: 2.720217704772949
Validation loss: 2.724319681044548

Epoch: 5| Step: 3
Training loss: 3.587702512741089
Validation loss: 2.7257538277615785

Epoch: 5| Step: 4
Training loss: 2.5817837715148926
Validation loss: 2.7240188096159246

Epoch: 5| Step: 5
Training loss: 2.5426838397979736
Validation loss: 2.72410253811908

Epoch: 5| Step: 6
Training loss: 3.093092203140259
Validation loss: 2.7255219157024095

Epoch: 5| Step: 7
Training loss: 2.4687912464141846
Validation loss: 2.7260591612067273

Epoch: 5| Step: 8
Training loss: 3.0038185119628906
Validation loss: 2.7259829223796888

Epoch: 5| Step: 9
Training loss: 2.807218313217163
Validation loss: 2.7234232938417824

Epoch: 5| Step: 10
Training loss: 3.185415029525757
Validation loss: 2.722052005029494

Epoch: 72| Step: 0
Training loss: 3.1569199562072754
Validation loss: 2.72078675095753

Epoch: 5| Step: 1
Training loss: 2.915665864944458
Validation loss: 2.7217223131528465

Epoch: 5| Step: 2
Training loss: 2.7539427280426025
Validation loss: 2.72150436780786

Epoch: 5| Step: 3
Training loss: 2.926574230194092
Validation loss: 2.7273838417504424

Epoch: 5| Step: 4
Training loss: 3.752558469772339
Validation loss: 2.726322120235812

Epoch: 5| Step: 5
Training loss: 2.7479445934295654
Validation loss: 2.7220690711852042

Epoch: 5| Step: 6
Training loss: 2.755110263824463
Validation loss: 2.719882534396264

Epoch: 5| Step: 7
Training loss: 2.9620754718780518
Validation loss: 2.720471271904566

Epoch: 5| Step: 8
Training loss: 2.5074849128723145
Validation loss: 2.7216242949167886

Epoch: 5| Step: 9
Training loss: 2.163588285446167
Validation loss: 2.7240954470890824

Epoch: 5| Step: 10
Training loss: 2.899780750274658
Validation loss: 2.7271144851561515

Epoch: 73| Step: 0
Training loss: 2.2904796600341797
Validation loss: 2.7291977046638407

Epoch: 5| Step: 1
Training loss: 3.142707586288452
Validation loss: 2.731478409100604

Epoch: 5| Step: 2
Training loss: 3.053670883178711
Validation loss: 2.730849476270778

Epoch: 5| Step: 3
Training loss: 2.8545219898223877
Validation loss: 2.7245977386351554

Epoch: 5| Step: 4
Training loss: 2.826022148132324
Validation loss: 2.7242634475872083

Epoch: 5| Step: 5
Training loss: 3.045626163482666
Validation loss: 2.725410202498077

Epoch: 5| Step: 6
Training loss: 3.862269878387451
Validation loss: 2.720390860752393

Epoch: 5| Step: 7
Training loss: 2.7595884799957275
Validation loss: 2.7186812303399526

Epoch: 5| Step: 8
Training loss: 2.720716953277588
Validation loss: 2.721700863171649

Epoch: 5| Step: 9
Training loss: 2.3036065101623535
Validation loss: 2.7205008409356557

Epoch: 5| Step: 10
Training loss: 2.6292457580566406
Validation loss: 2.722690987330611

Epoch: 74| Step: 0
Training loss: 2.6131751537323
Validation loss: 2.7246827130676596

Epoch: 5| Step: 1
Training loss: 2.3023617267608643
Validation loss: 2.7259977350952806

Epoch: 5| Step: 2
Training loss: 3.0002212524414062
Validation loss: 2.724613025624265

Epoch: 5| Step: 3
Training loss: 3.1017823219299316
Validation loss: 2.7239417901603122

Epoch: 5| Step: 4
Training loss: 3.631885051727295
Validation loss: 2.7247345293721845

Epoch: 5| Step: 5
Training loss: 2.9894962310791016
Validation loss: 2.719001488019061

Epoch: 5| Step: 6
Training loss: 2.335927963256836
Validation loss: 2.722691512876941

Epoch: 5| Step: 7
Training loss: 3.207944393157959
Validation loss: 2.723942813052926

Epoch: 5| Step: 8
Training loss: 2.3163485527038574
Validation loss: 2.7173358701890513

Epoch: 5| Step: 9
Training loss: 2.958266019821167
Validation loss: 2.720711244049893

Epoch: 5| Step: 10
Training loss: 3.12744140625
Validation loss: 2.71621266488106

Epoch: 75| Step: 0
Training loss: 2.825876474380493
Validation loss: 2.714745072908299

Epoch: 5| Step: 1
Training loss: 2.9324593544006348
Validation loss: 2.713014782115977

Epoch: 5| Step: 2
Training loss: 2.895721673965454
Validation loss: 2.7152365356363277

Epoch: 5| Step: 3
Training loss: 2.6674511432647705
Validation loss: 2.716617673955938

Epoch: 5| Step: 4
Training loss: 3.278566837310791
Validation loss: 2.716532232940838

Epoch: 5| Step: 5
Training loss: 2.786594867706299
Validation loss: 2.7160717210462018

Epoch: 5| Step: 6
Training loss: 2.5895497798919678
Validation loss: 2.7139526644060687

Epoch: 5| Step: 7
Training loss: 2.8922019004821777
Validation loss: 2.7129507628820275

Epoch: 5| Step: 8
Training loss: 2.5357234477996826
Validation loss: 2.716512818490305

Epoch: 5| Step: 9
Training loss: 2.817730665206909
Validation loss: 2.7136106157815583

Epoch: 5| Step: 10
Training loss: 3.325350522994995
Validation loss: 2.7143161912118234

Epoch: 76| Step: 0
Training loss: 2.8730173110961914
Validation loss: 2.71052199281672

Epoch: 5| Step: 1
Training loss: 2.647014617919922
Validation loss: 2.7158266857106197

Epoch: 5| Step: 2
Training loss: 2.4312126636505127
Validation loss: 2.7135571920743553

Epoch: 5| Step: 3
Training loss: 2.9265880584716797
Validation loss: 2.709717007093532

Epoch: 5| Step: 4
Training loss: 2.7744622230529785
Validation loss: 2.7108374334150747

Epoch: 5| Step: 5
Training loss: 2.92045521736145
Validation loss: 2.7104536666665027

Epoch: 5| Step: 6
Training loss: 2.8314380645751953
Validation loss: 2.708910221694618

Epoch: 5| Step: 7
Training loss: 3.270249605178833
Validation loss: 2.712133994666479

Epoch: 5| Step: 8
Training loss: 2.255793809890747
Validation loss: 2.7079981680839293

Epoch: 5| Step: 9
Training loss: 3.075888156890869
Validation loss: 2.712198293337258

Epoch: 5| Step: 10
Training loss: 3.55062198638916
Validation loss: 2.708577209903348

Epoch: 77| Step: 0
Training loss: 2.660426616668701
Validation loss: 2.7146710298394643

Epoch: 5| Step: 1
Training loss: 3.1159706115722656
Validation loss: 2.7080620642631286

Epoch: 5| Step: 2
Training loss: 2.6562659740448
Validation loss: 2.710578577492827

Epoch: 5| Step: 3
Training loss: 2.8218579292297363
Validation loss: 2.7095155151941444

Epoch: 5| Step: 4
Training loss: 2.805773973464966
Validation loss: 2.7119460259714434

Epoch: 5| Step: 5
Training loss: 3.2897377014160156
Validation loss: 2.7088030333160074

Epoch: 5| Step: 6
Training loss: 2.5465140342712402
Validation loss: 2.7108106792614026

Epoch: 5| Step: 7
Training loss: 3.374309539794922
Validation loss: 2.710898865935623

Epoch: 5| Step: 8
Training loss: 3.3046698570251465
Validation loss: 2.7095752710937173

Epoch: 5| Step: 9
Training loss: 2.1069774627685547
Validation loss: 2.7075524022502284

Epoch: 5| Step: 10
Training loss: 2.755070209503174
Validation loss: 2.7094838773050616

Epoch: 78| Step: 0
Training loss: 2.9577457904815674
Validation loss: 2.708086836722589

Epoch: 5| Step: 1
Training loss: 2.0574281215667725
Validation loss: 2.7100899886059504

Epoch: 5| Step: 2
Training loss: 3.6590819358825684
Validation loss: 2.704873877186929

Epoch: 5| Step: 3
Training loss: 3.024973154067993
Validation loss: 2.709617442982171

Epoch: 5| Step: 4
Training loss: 3.5693130493164062
Validation loss: 2.7099919293516423

Epoch: 5| Step: 5
Training loss: 1.79373300075531
Validation loss: 2.709535580809398

Epoch: 5| Step: 6
Training loss: 2.5129287242889404
Validation loss: 2.7079856190630185

Epoch: 5| Step: 7
Training loss: 3.313286304473877
Validation loss: 2.706358166151149

Epoch: 5| Step: 8
Training loss: 3.4173102378845215
Validation loss: 2.7032796798213834

Epoch: 5| Step: 9
Training loss: 2.470947265625
Validation loss: 2.705096460157825

Epoch: 5| Step: 10
Training loss: 2.602384090423584
Validation loss: 2.703394853940574

Epoch: 79| Step: 0
Training loss: 3.1406311988830566
Validation loss: 2.704383647570046

Epoch: 5| Step: 1
Training loss: 3.0805461406707764
Validation loss: 2.70122004068026

Epoch: 5| Step: 2
Training loss: 2.5611095428466797
Validation loss: 2.7018801678893385

Epoch: 5| Step: 3
Training loss: 3.665149688720703
Validation loss: 2.700040868533555

Epoch: 5| Step: 4
Training loss: 2.4763858318328857
Validation loss: 2.7001293166991203

Epoch: 5| Step: 5
Training loss: 2.931494951248169
Validation loss: 2.701645951117239

Epoch: 5| Step: 6
Training loss: 2.1810734272003174
Validation loss: 2.7030282533296974

Epoch: 5| Step: 7
Training loss: 2.3222556114196777
Validation loss: 2.7044816350424163

Epoch: 5| Step: 8
Training loss: 3.224517822265625
Validation loss: 2.705050322317308

Epoch: 5| Step: 9
Training loss: 3.1873791217803955
Validation loss: 2.702902634938558

Epoch: 5| Step: 10
Training loss: 2.584773540496826
Validation loss: 2.7019232421792965

Epoch: 80| Step: 0
Training loss: 2.646174192428589
Validation loss: 2.701653754839333

Epoch: 5| Step: 1
Training loss: 2.742753505706787
Validation loss: 2.704436461130778

Epoch: 5| Step: 2
Training loss: 3.4793999195098877
Validation loss: 2.70403701771972

Epoch: 5| Step: 3
Training loss: 3.139641523361206
Validation loss: 2.6982944216779483

Epoch: 5| Step: 4
Training loss: 2.534862756729126
Validation loss: 2.7037995502512944

Epoch: 5| Step: 5
Training loss: 3.206392765045166
Validation loss: 2.704122063934162

Epoch: 5| Step: 6
Training loss: 2.415337562561035
Validation loss: 2.7016284235062136

Epoch: 5| Step: 7
Training loss: 2.8328795433044434
Validation loss: 2.7049360505996214

Epoch: 5| Step: 8
Training loss: 2.7059905529022217
Validation loss: 2.7037764415946057

Epoch: 5| Step: 9
Training loss: 2.9454684257507324
Validation loss: 2.703717303532426

Epoch: 5| Step: 10
Training loss: 2.738037109375
Validation loss: 2.699706931268015

Epoch: 81| Step: 0
Training loss: 2.3496551513671875
Validation loss: 2.6968432959689888

Epoch: 5| Step: 1
Training loss: 2.552762269973755
Validation loss: 2.695984261010283

Epoch: 5| Step: 2
Training loss: 3.3569769859313965
Validation loss: 2.6980364604662825

Epoch: 5| Step: 3
Training loss: 2.725353479385376
Validation loss: 2.701027447177518

Epoch: 5| Step: 4
Training loss: 3.6289610862731934
Validation loss: 2.6991068522135415

Epoch: 5| Step: 5
Training loss: 2.725107192993164
Validation loss: 2.701765437279978

Epoch: 5| Step: 6
Training loss: 2.653891086578369
Validation loss: 2.701865014209542

Epoch: 5| Step: 7
Training loss: 3.2365829944610596
Validation loss: 2.7020638911954817

Epoch: 5| Step: 8
Training loss: 2.2641713619232178
Validation loss: 2.6979501221769597

Epoch: 5| Step: 9
Training loss: 2.7015364170074463
Validation loss: 2.6995916289667927

Epoch: 5| Step: 10
Training loss: 3.2770938873291016
Validation loss: 2.702710146545082

Epoch: 82| Step: 0
Training loss: 3.101811408996582
Validation loss: 2.703441640382172

Epoch: 5| Step: 1
Training loss: 2.235759973526001
Validation loss: 2.6983052453687115

Epoch: 5| Step: 2
Training loss: 2.6431593894958496
Validation loss: 2.703439356178366

Epoch: 5| Step: 3
Training loss: 2.6849188804626465
Validation loss: 2.7018641271898822

Epoch: 5| Step: 4
Training loss: 3.349149227142334
Validation loss: 2.7046240504069994

Epoch: 5| Step: 5
Training loss: 2.621859550476074
Validation loss: 2.7017541264974945

Epoch: 5| Step: 6
Training loss: 3.8135547637939453
Validation loss: 2.697758241366315

Epoch: 5| Step: 7
Training loss: 2.9782519340515137
Validation loss: 2.699869178956555

Epoch: 5| Step: 8
Training loss: 2.7906270027160645
Validation loss: 2.694728846191078

Epoch: 5| Step: 9
Training loss: 2.9816977977752686
Validation loss: 2.6943671318792526

Epoch: 5| Step: 10
Training loss: 2.057537794113159
Validation loss: 2.6943721848149456

Epoch: 83| Step: 0
Training loss: 3.427783966064453
Validation loss: 2.694648378638811

Epoch: 5| Step: 1
Training loss: 2.9358668327331543
Validation loss: 2.694629297461561

Epoch: 5| Step: 2
Training loss: 2.9977493286132812
Validation loss: 2.692947969641737

Epoch: 5| Step: 3
Training loss: 1.9958164691925049
Validation loss: 2.6955747835097776

Epoch: 5| Step: 4
Training loss: 3.266941785812378
Validation loss: 2.6931239251167542

Epoch: 5| Step: 5
Training loss: 2.5243563652038574
Validation loss: 2.6919747655109694

Epoch: 5| Step: 6
Training loss: 2.846456289291382
Validation loss: 2.696133969932474

Epoch: 5| Step: 7
Training loss: 2.5045506954193115
Validation loss: 2.6971097915403304

Epoch: 5| Step: 8
Training loss: 3.0628788471221924
Validation loss: 2.695744470883441

Epoch: 5| Step: 9
Training loss: 2.7628235816955566
Validation loss: 2.694811356964932

Epoch: 5| Step: 10
Training loss: 3.040250778198242
Validation loss: 2.6949105647302445

Epoch: 84| Step: 0
Training loss: 2.6375064849853516
Validation loss: 2.6928215334492345

Epoch: 5| Step: 1
Training loss: 3.5949020385742188
Validation loss: 2.695467431058166

Epoch: 5| Step: 2
Training loss: 2.8285629749298096
Validation loss: 2.6947058810982654

Epoch: 5| Step: 3
Training loss: 2.9640259742736816
Validation loss: 2.694590558287918

Epoch: 5| Step: 4
Training loss: 3.0651566982269287
Validation loss: 2.6935612129908737

Epoch: 5| Step: 5
Training loss: 1.8213768005371094
Validation loss: 2.694637031965358

Epoch: 5| Step: 6
Training loss: 2.7467575073242188
Validation loss: 2.692321446634108

Epoch: 5| Step: 7
Training loss: 2.513988733291626
Validation loss: 2.691245085449629

Epoch: 5| Step: 8
Training loss: 3.1513261795043945
Validation loss: 2.692751548623526

Epoch: 5| Step: 9
Training loss: 3.1718859672546387
Validation loss: 2.6921558790309454

Epoch: 5| Step: 10
Training loss: 2.7855217456817627
Validation loss: 2.689882557879212

Epoch: 85| Step: 0
Training loss: 2.2753167152404785
Validation loss: 2.6904863567762476

Epoch: 5| Step: 1
Training loss: 2.6842360496520996
Validation loss: 2.6939330562468498

Epoch: 5| Step: 2
Training loss: 1.8843708038330078
Validation loss: 2.6922755523394515

Epoch: 5| Step: 3
Training loss: 3.3612136840820312
Validation loss: 2.6896727802932903

Epoch: 5| Step: 4
Training loss: 3.2769532203674316
Validation loss: 2.691725233549713

Epoch: 5| Step: 5
Training loss: 2.9348649978637695
Validation loss: 2.6916690975107174

Epoch: 5| Step: 6
Training loss: 3.1320157051086426
Validation loss: 2.6906803936086674

Epoch: 5| Step: 7
Training loss: 2.7290289402008057
Validation loss: 2.695052972403906

Epoch: 5| Step: 8
Training loss: 3.1602959632873535
Validation loss: 2.6941613176817536

Epoch: 5| Step: 9
Training loss: 2.960435390472412
Validation loss: 2.696837091958651

Epoch: 5| Step: 10
Training loss: 2.851165294647217
Validation loss: 2.691947147410403

Epoch: 86| Step: 0
Training loss: 2.318878650665283
Validation loss: 2.6928805766567105

Epoch: 5| Step: 1
Training loss: 3.9213128089904785
Validation loss: 2.6973739772714596

Epoch: 5| Step: 2
Training loss: 2.7217109203338623
Validation loss: 2.6964743880815405

Epoch: 5| Step: 3
Training loss: 2.0640950202941895
Validation loss: 2.691745719602031

Epoch: 5| Step: 4
Training loss: 2.6164841651916504
Validation loss: 2.6890435103447206

Epoch: 5| Step: 5
Training loss: 2.6927382946014404
Validation loss: 2.685202613953621

Epoch: 5| Step: 6
Training loss: 2.470838785171509
Validation loss: 2.687256015757079

Epoch: 5| Step: 7
Training loss: 2.961545467376709
Validation loss: 2.688375906277728

Epoch: 5| Step: 8
Training loss: 3.51899790763855
Validation loss: 2.691880461990192

Epoch: 5| Step: 9
Training loss: 3.132753610610962
Validation loss: 2.687504906808176

Epoch: 5| Step: 10
Training loss: 2.9188320636749268
Validation loss: 2.691967987245129

Epoch: 87| Step: 0
Training loss: 2.795703887939453
Validation loss: 2.6876842155251452

Epoch: 5| Step: 1
Training loss: 3.1143782138824463
Validation loss: 2.6904581208382883

Epoch: 5| Step: 2
Training loss: 2.621943235397339
Validation loss: 2.6852557274603073

Epoch: 5| Step: 3
Training loss: 2.40446400642395
Validation loss: 2.687197264804635

Epoch: 5| Step: 4
Training loss: 3.0238959789276123
Validation loss: 2.6887220362181306

Epoch: 5| Step: 5
Training loss: 2.730581760406494
Validation loss: 2.690784513309438

Epoch: 5| Step: 6
Training loss: 3.0641531944274902
Validation loss: 2.688980988276902

Epoch: 5| Step: 7
Training loss: 3.0509066581726074
Validation loss: 2.694564596299202

Epoch: 5| Step: 8
Training loss: 2.2589313983917236
Validation loss: 2.693328760003531

Epoch: 5| Step: 9
Training loss: 3.179643154144287
Validation loss: 2.6931696348292853

Epoch: 5| Step: 10
Training loss: 3.0199131965637207
Validation loss: 2.6959418378850466

Epoch: 88| Step: 0
Training loss: 2.256976366043091
Validation loss: 2.6959007068346907

Epoch: 5| Step: 1
Training loss: 3.0282065868377686
Validation loss: 2.6912080421242663

Epoch: 5| Step: 2
Training loss: 2.7644412517547607
Validation loss: 2.690203970478427

Epoch: 5| Step: 3
Training loss: 2.577484130859375
Validation loss: 2.689756683124009

Epoch: 5| Step: 4
Training loss: 3.461442470550537
Validation loss: 2.688845334514495

Epoch: 5| Step: 5
Training loss: 2.9202098846435547
Validation loss: 2.691908954292215

Epoch: 5| Step: 6
Training loss: 2.5760562419891357
Validation loss: 2.6936261576990925

Epoch: 5| Step: 7
Training loss: 3.6025071144104004
Validation loss: 2.691434693592851

Epoch: 5| Step: 8
Training loss: 2.5042285919189453
Validation loss: 2.6913218677684827

Epoch: 5| Step: 9
Training loss: 2.347806930541992
Validation loss: 2.689892335604596

Epoch: 5| Step: 10
Training loss: 3.1765451431274414
Validation loss: 2.692489121549873

Epoch: 89| Step: 0
Training loss: 2.738335132598877
Validation loss: 2.6856805406590945

Epoch: 5| Step: 1
Training loss: 2.1834425926208496
Validation loss: 2.6849320421936693

Epoch: 5| Step: 2
Training loss: 3.122526168823242
Validation loss: 2.6853196902941634

Epoch: 5| Step: 3
Training loss: 2.6917738914489746
Validation loss: 2.686972774485106

Epoch: 5| Step: 4
Training loss: 3.5097293853759766
Validation loss: 2.6864258473919285

Epoch: 5| Step: 5
Training loss: 2.8816802501678467
Validation loss: 2.684454530797979

Epoch: 5| Step: 6
Training loss: 2.446506977081299
Validation loss: 2.6847331780259327

Epoch: 5| Step: 7
Training loss: 2.9184672832489014
Validation loss: 2.685392423342633

Epoch: 5| Step: 8
Training loss: 2.3911399841308594
Validation loss: 2.681275913792272

Epoch: 5| Step: 9
Training loss: 3.1288492679595947
Validation loss: 2.686143152175411

Epoch: 5| Step: 10
Training loss: 3.204921007156372
Validation loss: 2.6841124232097338

Epoch: 90| Step: 0
Training loss: 2.5704023838043213
Validation loss: 2.6836051684553905

Epoch: 5| Step: 1
Training loss: 2.790139675140381
Validation loss: 2.6856585164223947

Epoch: 5| Step: 2
Training loss: 3.363149642944336
Validation loss: 2.682875999840357

Epoch: 5| Step: 3
Training loss: 3.4906532764434814
Validation loss: 2.6783253326210925

Epoch: 5| Step: 4
Training loss: 2.800288677215576
Validation loss: 2.681260734476069

Epoch: 5| Step: 5
Training loss: 1.9213001728057861
Validation loss: 2.6858600160127044

Epoch: 5| Step: 6
Training loss: 2.8285703659057617
Validation loss: 2.6819901799642913

Epoch: 5| Step: 7
Training loss: 2.6621620655059814
Validation loss: 2.6874592945139897

Epoch: 5| Step: 8
Training loss: 2.4357876777648926
Validation loss: 2.6901881233338387

Epoch: 5| Step: 9
Training loss: 3.0186455249786377
Validation loss: 2.695833167722148

Epoch: 5| Step: 10
Training loss: 3.454922914505005
Validation loss: 2.696809499494491

Epoch: 91| Step: 0
Training loss: 3.337578535079956
Validation loss: 2.6906040355723393

Epoch: 5| Step: 1
Training loss: 2.113661527633667
Validation loss: 2.688125974388533

Epoch: 5| Step: 2
Training loss: 3.116034746170044
Validation loss: 2.6870256905914633

Epoch: 5| Step: 3
Training loss: 2.452988386154175
Validation loss: 2.680562852531351

Epoch: 5| Step: 4
Training loss: 2.4658451080322266
Validation loss: 2.678392538460352

Epoch: 5| Step: 5
Training loss: 3.333531618118286
Validation loss: 2.6792556316621843

Epoch: 5| Step: 6
Training loss: 2.8622214794158936
Validation loss: 2.681777387536982

Epoch: 5| Step: 7
Training loss: 2.5660104751586914
Validation loss: 2.6775295657496296

Epoch: 5| Step: 8
Training loss: 2.5180225372314453
Validation loss: 2.6730518417973674

Epoch: 5| Step: 9
Training loss: 3.471327304840088
Validation loss: 2.6786873084242626

Epoch: 5| Step: 10
Training loss: 2.9004287719726562
Validation loss: 2.674149749099567

Epoch: 92| Step: 0
Training loss: 3.396848201751709
Validation loss: 2.676566472617529

Epoch: 5| Step: 1
Training loss: 2.869426727294922
Validation loss: 2.6783580498028825

Epoch: 5| Step: 2
Training loss: 2.0633444786071777
Validation loss: 2.6731325605864167

Epoch: 5| Step: 3
Training loss: 3.2522621154785156
Validation loss: 2.673445640071746

Epoch: 5| Step: 4
Training loss: 2.513289451599121
Validation loss: 2.6741052571163384

Epoch: 5| Step: 5
Training loss: 2.9431071281433105
Validation loss: 2.6802103519439697

Epoch: 5| Step: 6
Training loss: 3.1273021697998047
Validation loss: 2.682008079303208

Epoch: 5| Step: 7
Training loss: 3.1930885314941406
Validation loss: 2.6792178846174672

Epoch: 5| Step: 8
Training loss: 2.8622615337371826
Validation loss: 2.6753367557320544

Epoch: 5| Step: 9
Training loss: 2.626011371612549
Validation loss: 2.678559357120145

Epoch: 5| Step: 10
Training loss: 2.15775465965271
Validation loss: 2.6786786715189614

Epoch: 93| Step: 0
Training loss: 3.458995819091797
Validation loss: 2.6747822043716267

Epoch: 5| Step: 1
Training loss: 2.5137033462524414
Validation loss: 2.677474899958539

Epoch: 5| Step: 2
Training loss: 2.584829807281494
Validation loss: 2.6792634712752474

Epoch: 5| Step: 3
Training loss: 2.6473984718322754
Validation loss: 2.6780984504248506

Epoch: 5| Step: 4
Training loss: 2.7966952323913574
Validation loss: 2.6784257811884724

Epoch: 5| Step: 5
Training loss: 2.5946362018585205
Validation loss: 2.6764294203891548

Epoch: 5| Step: 6
Training loss: 3.0938687324523926
Validation loss: 2.6772828691749164

Epoch: 5| Step: 7
Training loss: 2.455010414123535
Validation loss: 2.6804217074507024

Epoch: 5| Step: 8
Training loss: 2.9321537017822266
Validation loss: 2.6803869457655054

Epoch: 5| Step: 9
Training loss: 2.763650417327881
Validation loss: 2.678921545705488

Epoch: 5| Step: 10
Training loss: 3.3246822357177734
Validation loss: 2.677026125692552

Epoch: 94| Step: 0
Training loss: 2.8499648571014404
Validation loss: 2.6766316736898115

Epoch: 5| Step: 1
Training loss: 3.146676540374756
Validation loss: 2.673347701308548

Epoch: 5| Step: 2
Training loss: 2.9588358402252197
Validation loss: 2.6754349047137844

Epoch: 5| Step: 3
Training loss: 2.6401736736297607
Validation loss: 2.6734599913320234

Epoch: 5| Step: 4
Training loss: 2.3634848594665527
Validation loss: 2.6714259706517702

Epoch: 5| Step: 5
Training loss: 2.90730357170105
Validation loss: 2.672221670868576

Epoch: 5| Step: 6
Training loss: 2.590503215789795
Validation loss: 2.670202265503586

Epoch: 5| Step: 7
Training loss: 2.9229190349578857
Validation loss: 2.669231894195721

Epoch: 5| Step: 8
Training loss: 2.9854793548583984
Validation loss: 2.6713539759318032

Epoch: 5| Step: 9
Training loss: 2.5581722259521484
Validation loss: 2.672532340531708

Epoch: 5| Step: 10
Training loss: 3.1843864917755127
Validation loss: 2.671581042710171

Epoch: 95| Step: 0
Training loss: 2.579136610031128
Validation loss: 2.6716542346503145

Epoch: 5| Step: 1
Training loss: 2.792973279953003
Validation loss: 2.669330055995654

Epoch: 5| Step: 2
Training loss: 2.460536479949951
Validation loss: 2.669487178966563

Epoch: 5| Step: 3
Training loss: 2.499203681945801
Validation loss: 2.673677800804056

Epoch: 5| Step: 4
Training loss: 2.779451608657837
Validation loss: 2.6717367684969338

Epoch: 5| Step: 5
Training loss: 3.3696205615997314
Validation loss: 2.6746970530479186

Epoch: 5| Step: 6
Training loss: 2.8420448303222656
Validation loss: 2.6756605666170836

Epoch: 5| Step: 7
Training loss: 2.9630353450775146
Validation loss: 2.670581058789325

Epoch: 5| Step: 8
Training loss: 3.0451037883758545
Validation loss: 2.674539714731196

Epoch: 5| Step: 9
Training loss: 3.0382072925567627
Validation loss: 2.6768840154012046

Epoch: 5| Step: 10
Training loss: 2.6438400745391846
Validation loss: 2.670920205372636

Epoch: 96| Step: 0
Training loss: 2.870058298110962
Validation loss: 2.678960451515772

Epoch: 5| Step: 1
Training loss: 2.391690492630005
Validation loss: 2.668538442222021

Epoch: 5| Step: 2
Training loss: 3.099003314971924
Validation loss: 2.6696945903121785

Epoch: 5| Step: 3
Training loss: 2.3428404331207275
Validation loss: 2.6701321345503612

Epoch: 5| Step: 4
Training loss: 3.9045093059539795
Validation loss: 2.669227223242483

Epoch: 5| Step: 5
Training loss: 2.51607084274292
Validation loss: 2.6658133024810464

Epoch: 5| Step: 6
Training loss: 3.180037021636963
Validation loss: 2.6669156794906943

Epoch: 5| Step: 7
Training loss: 2.8609280586242676
Validation loss: 2.6667990069235525

Epoch: 5| Step: 8
Training loss: 2.5732924938201904
Validation loss: 2.6661341549247823

Epoch: 5| Step: 9
Training loss: 2.7245230674743652
Validation loss: 2.6635881982823855

Epoch: 5| Step: 10
Training loss: 2.5409717559814453
Validation loss: 2.667311465868386

Epoch: 97| Step: 0
Training loss: 2.7943716049194336
Validation loss: 2.6685716977683445

Epoch: 5| Step: 1
Training loss: 3.652869462966919
Validation loss: 2.669055595192858

Epoch: 5| Step: 2
Training loss: 2.7106690406799316
Validation loss: 2.669605747345955

Epoch: 5| Step: 3
Training loss: 3.5843958854675293
Validation loss: 2.6678670939578804

Epoch: 5| Step: 4
Training loss: 2.0452933311462402
Validation loss: 2.668473025803925

Epoch: 5| Step: 5
Training loss: 2.8371613025665283
Validation loss: 2.668986881932905

Epoch: 5| Step: 6
Training loss: 2.7767136096954346
Validation loss: 2.6657757989821897

Epoch: 5| Step: 7
Training loss: 2.9258639812469482
Validation loss: 2.6648235808136644

Epoch: 5| Step: 8
Training loss: 3.097898006439209
Validation loss: 2.6721037562175463

Epoch: 5| Step: 9
Training loss: 2.6751670837402344
Validation loss: 2.668845735570436

Epoch: 5| Step: 10
Training loss: 1.7465180158615112
Validation loss: 2.6668404533017065

Epoch: 98| Step: 0
Training loss: 2.8299996852874756
Validation loss: 2.6613386651521087

Epoch: 5| Step: 1
Training loss: 2.292147397994995
Validation loss: 2.666187422249907

Epoch: 5| Step: 2
Training loss: 2.625685214996338
Validation loss: 2.6653797882859425

Epoch: 5| Step: 3
Training loss: 2.9029641151428223
Validation loss: 2.6650556390003493

Epoch: 5| Step: 4
Training loss: 2.430351972579956
Validation loss: 2.666626223953821

Epoch: 5| Step: 5
Training loss: 3.2770028114318848
Validation loss: 2.6692457096551054

Epoch: 5| Step: 6
Training loss: 2.4928574562072754
Validation loss: 2.6646495378145607

Epoch: 5| Step: 7
Training loss: 2.858710289001465
Validation loss: 2.66399287152034

Epoch: 5| Step: 8
Training loss: 3.2167563438415527
Validation loss: 2.6625043781854774

Epoch: 5| Step: 9
Training loss: 2.5495998859405518
Validation loss: 2.6597245431715444

Epoch: 5| Step: 10
Training loss: 3.633770227432251
Validation loss: 2.66114822254386

Epoch: 99| Step: 0
Training loss: 2.5074989795684814
Validation loss: 2.659820992459533

Epoch: 5| Step: 1
Training loss: 2.8745200634002686
Validation loss: 2.6621261924825688

Epoch: 5| Step: 2
Training loss: 2.40944504737854
Validation loss: 2.6579303920909925

Epoch: 5| Step: 3
Training loss: 2.3745193481445312
Validation loss: 2.660009584119243

Epoch: 5| Step: 4
Training loss: 2.33939790725708
Validation loss: 2.660447443685224

Epoch: 5| Step: 5
Training loss: 3.6381068229675293
Validation loss: 2.6627066827589467

Epoch: 5| Step: 6
Training loss: 2.7653331756591797
Validation loss: 2.663705310513896

Epoch: 5| Step: 7
Training loss: 2.4888901710510254
Validation loss: 2.6654864331727386

Epoch: 5| Step: 8
Training loss: 3.538135051727295
Validation loss: 2.6674268066242175

Epoch: 5| Step: 9
Training loss: 3.2003681659698486
Validation loss: 2.6640307928926203

Epoch: 5| Step: 10
Training loss: 2.8166799545288086
Validation loss: 2.664499985274448

Epoch: 100| Step: 0
Training loss: 2.7991909980773926
Validation loss: 2.6585018532250517

Epoch: 5| Step: 1
Training loss: 3.3357303142547607
Validation loss: 2.6629005632092877

Epoch: 5| Step: 2
Training loss: 2.751352548599243
Validation loss: 2.66620228111103

Epoch: 5| Step: 3
Training loss: 2.8768162727355957
Validation loss: 2.667399106487151

Epoch: 5| Step: 4
Training loss: 2.956122875213623
Validation loss: 2.660925908755231

Epoch: 5| Step: 5
Training loss: 2.834327220916748
Validation loss: 2.6628065365616993

Epoch: 5| Step: 6
Training loss: 2.564880847930908
Validation loss: 2.6632432835076445

Epoch: 5| Step: 7
Training loss: 2.7055068016052246
Validation loss: 2.66344343462298

Epoch: 5| Step: 8
Training loss: 2.944645881652832
Validation loss: 2.660871560855578

Epoch: 5| Step: 9
Training loss: 2.4681475162506104
Validation loss: 2.6646276084325646

Epoch: 5| Step: 10
Training loss: 2.6890792846679688
Validation loss: 2.664730061766922

Epoch: 101| Step: 0
Training loss: 2.5139317512512207
Validation loss: 2.6624028016162176

Epoch: 5| Step: 1
Training loss: 2.3814749717712402
Validation loss: 2.6621499471766974

Epoch: 5| Step: 2
Training loss: 2.590216875076294
Validation loss: 2.661213626143753

Epoch: 5| Step: 3
Training loss: 2.7464916706085205
Validation loss: 2.6631868475226947

Epoch: 5| Step: 4
Training loss: 3.568809986114502
Validation loss: 2.6615185019790486

Epoch: 5| Step: 5
Training loss: 2.744541883468628
Validation loss: 2.6628851685472714

Epoch: 5| Step: 6
Training loss: 3.0967018604278564
Validation loss: 2.6644889872561217

Epoch: 5| Step: 7
Training loss: 3.6416893005371094
Validation loss: 2.659373355168168

Epoch: 5| Step: 8
Training loss: 2.8805994987487793
Validation loss: 2.665115130844937

Epoch: 5| Step: 9
Training loss: 1.8583217859268188
Validation loss: 2.6589740322482203

Epoch: 5| Step: 10
Training loss: 2.8921029567718506
Validation loss: 2.6565122348006054

Epoch: 102| Step: 0
Training loss: 3.221256971359253
Validation loss: 2.663133846816196

Epoch: 5| Step: 1
Training loss: 2.6369805335998535
Validation loss: 2.664936496365455

Epoch: 5| Step: 2
Training loss: 2.8263537883758545
Validation loss: 2.6668457497832594

Epoch: 5| Step: 3
Training loss: 2.7694270610809326
Validation loss: 2.664908445009621

Epoch: 5| Step: 4
Training loss: 2.6756210327148438
Validation loss: 2.6657537491090837

Epoch: 5| Step: 5
Training loss: 2.3810458183288574
Validation loss: 2.6630671357595794

Epoch: 5| Step: 6
Training loss: 2.8345019817352295
Validation loss: 2.6657698974814465

Epoch: 5| Step: 7
Training loss: 3.518172025680542
Validation loss: 2.6620332835822977

Epoch: 5| Step: 8
Training loss: 2.5495128631591797
Validation loss: 2.658253305701799

Epoch: 5| Step: 9
Training loss: 2.167694330215454
Validation loss: 2.6546166276419036

Epoch: 5| Step: 10
Training loss: 3.4522249698638916
Validation loss: 2.65511506090882

Epoch: 103| Step: 0
Training loss: 2.8500490188598633
Validation loss: 2.652492884666689

Epoch: 5| Step: 1
Training loss: 3.443307399749756
Validation loss: 2.6547041862241683

Epoch: 5| Step: 2
Training loss: 3.130128860473633
Validation loss: 2.65680278501203

Epoch: 5| Step: 3
Training loss: 1.7464544773101807
Validation loss: 2.654791021859774

Epoch: 5| Step: 4
Training loss: 2.7833809852600098
Validation loss: 2.6537667551348285

Epoch: 5| Step: 5
Training loss: 2.8865389823913574
Validation loss: 2.6516528437214513

Epoch: 5| Step: 6
Training loss: 2.590902328491211
Validation loss: 2.6528277499701387

Epoch: 5| Step: 7
Training loss: 2.829590082168579
Validation loss: 2.6537538138769006

Epoch: 5| Step: 8
Training loss: 3.171952962875366
Validation loss: 2.6532195306593374

Epoch: 5| Step: 9
Training loss: 2.6842010021209717
Validation loss: 2.6563036159802507

Epoch: 5| Step: 10
Training loss: 2.7534468173980713
Validation loss: 2.6574632070397817

Epoch: 104| Step: 0
Training loss: 2.9730687141418457
Validation loss: 2.661921703687278

Epoch: 5| Step: 1
Training loss: 3.3455498218536377
Validation loss: 2.656103659701604

Epoch: 5| Step: 2
Training loss: 3.4838180541992188
Validation loss: 2.65706725787091

Epoch: 5| Step: 3
Training loss: 2.5362131595611572
Validation loss: 2.6608801144425587

Epoch: 5| Step: 4
Training loss: 2.2320210933685303
Validation loss: 2.659910314826555

Epoch: 5| Step: 5
Training loss: 2.6825430393218994
Validation loss: 2.6581564052130586

Epoch: 5| Step: 6
Training loss: 3.441361904144287
Validation loss: 2.658708844133603

Epoch: 5| Step: 7
Training loss: 2.4550719261169434
Validation loss: 2.660258044478714

Epoch: 5| Step: 8
Training loss: 2.5700926780700684
Validation loss: 2.6592857504403717

Epoch: 5| Step: 9
Training loss: 3.2203097343444824
Validation loss: 2.6602877545100387

Epoch: 5| Step: 10
Training loss: 1.823451280593872
Validation loss: 2.6607681038559123

Epoch: 105| Step: 0
Training loss: 2.9038805961608887
Validation loss: 2.6576189276992634

Epoch: 5| Step: 1
Training loss: 3.259845018386841
Validation loss: 2.6541246547493884

Epoch: 5| Step: 2
Training loss: 2.4980902671813965
Validation loss: 2.6546373316036758

Epoch: 5| Step: 3
Training loss: 1.8950607776641846
Validation loss: 2.65290508731719

Epoch: 5| Step: 4
Training loss: 2.438946008682251
Validation loss: 2.6573351736991637

Epoch: 5| Step: 5
Training loss: 3.201446533203125
Validation loss: 2.659654458363851

Epoch: 5| Step: 6
Training loss: 3.1182169914245605
Validation loss: 2.659381305017779

Epoch: 5| Step: 7
Training loss: 3.0271763801574707
Validation loss: 2.657079806891821

Epoch: 5| Step: 8
Training loss: 3.3062381744384766
Validation loss: 2.650838534037272

Epoch: 5| Step: 9
Training loss: 2.653899908065796
Validation loss: 2.6497467358907065

Epoch: 5| Step: 10
Training loss: 2.5505330562591553
Validation loss: 2.6531803402849423

Epoch: 106| Step: 0
Training loss: 2.896514892578125
Validation loss: 2.649884241883473

Epoch: 5| Step: 1
Training loss: 2.755539655685425
Validation loss: 2.6521849632263184

Epoch: 5| Step: 2
Training loss: 3.55295991897583
Validation loss: 2.6531474385210263

Epoch: 5| Step: 3
Training loss: 2.948251247406006
Validation loss: 2.651376647333945

Epoch: 5| Step: 4
Training loss: 2.8663330078125
Validation loss: 2.6530738466529438

Epoch: 5| Step: 5
Training loss: 1.9591522216796875
Validation loss: 2.6543402748723186

Epoch: 5| Step: 6
Training loss: 3.2190043926239014
Validation loss: 2.654663416647142

Epoch: 5| Step: 7
Training loss: 2.3723983764648438
Validation loss: 2.6516953847741567

Epoch: 5| Step: 8
Training loss: 2.485107898712158
Validation loss: 2.6583757400512695

Epoch: 5| Step: 9
Training loss: 2.5622763633728027
Validation loss: 2.6642661479211625

Epoch: 5| Step: 10
Training loss: 3.2461211681365967
Validation loss: 2.6631139606557865

Epoch: 107| Step: 0
Training loss: 2.694319009780884
Validation loss: 2.6650203940688924

Epoch: 5| Step: 1
Training loss: 3.3642971515655518
Validation loss: 2.6731128308080856

Epoch: 5| Step: 2
Training loss: 2.694631576538086
Validation loss: 2.6582668776153238

Epoch: 5| Step: 3
Training loss: 2.2827136516571045
Validation loss: 2.650361253369239

Epoch: 5| Step: 4
Training loss: 2.4536428451538086
Validation loss: 2.6510924946877266

Epoch: 5| Step: 5
Training loss: 3.113694667816162
Validation loss: 2.6459859007148334

Epoch: 5| Step: 6
Training loss: 2.70587420463562
Validation loss: 2.6447578091775217

Epoch: 5| Step: 7
Training loss: 2.8924896717071533
Validation loss: 2.6458339844980547

Epoch: 5| Step: 8
Training loss: 3.3501548767089844
Validation loss: 2.6463542599831857

Epoch: 5| Step: 9
Training loss: 2.8648664951324463
Validation loss: 2.641787903283232

Epoch: 5| Step: 10
Training loss: 2.2425856590270996
Validation loss: 2.6453862831156743

Epoch: 108| Step: 0
Training loss: 2.4015629291534424
Validation loss: 2.645434292413855

Epoch: 5| Step: 1
Training loss: 2.4004454612731934
Validation loss: 2.6458359431195

Epoch: 5| Step: 2
Training loss: 2.8705294132232666
Validation loss: 2.647684194708383

Epoch: 5| Step: 3
Training loss: 2.553544521331787
Validation loss: 2.6437179042446997

Epoch: 5| Step: 4
Training loss: 3.1357052326202393
Validation loss: 2.648275134383991

Epoch: 5| Step: 5
Training loss: 2.780282735824585
Validation loss: 2.646064730100734

Epoch: 5| Step: 6
Training loss: 3.890063762664795
Validation loss: 2.644271563458186

Epoch: 5| Step: 7
Training loss: 3.016846179962158
Validation loss: 2.645640227102464

Epoch: 5| Step: 8
Training loss: 2.71952748298645
Validation loss: 2.6502167691466627

Epoch: 5| Step: 9
Training loss: 2.6202263832092285
Validation loss: 2.6501997363182808

Epoch: 5| Step: 10
Training loss: 2.3310816287994385
Validation loss: 2.6471145153045654

Epoch: 109| Step: 0
Training loss: 2.9808435440063477
Validation loss: 2.6462675961115028

Epoch: 5| Step: 1
Training loss: 2.407458782196045
Validation loss: 2.6448994785226803

Epoch: 5| Step: 2
Training loss: 2.5917470455169678
Validation loss: 2.6392790707208778

Epoch: 5| Step: 3
Training loss: 3.4019527435302734
Validation loss: 2.643411023642427

Epoch: 5| Step: 4
Training loss: 2.7213692665100098
Validation loss: 2.6437023403824016

Epoch: 5| Step: 5
Training loss: 3.6418368816375732
Validation loss: 2.6369993840494463

Epoch: 5| Step: 6
Training loss: 2.1573007106781006
Validation loss: 2.641195991987823

Epoch: 5| Step: 7
Training loss: 2.6140294075012207
Validation loss: 2.6403211932028494

Epoch: 5| Step: 8
Training loss: 2.4822983741760254
Validation loss: 2.6436403848791636

Epoch: 5| Step: 9
Training loss: 2.874185800552368
Validation loss: 2.641535423135245

Epoch: 5| Step: 10
Training loss: 2.8291964530944824
Validation loss: 2.643726048930999

Epoch: 110| Step: 0
Training loss: 2.511620283126831
Validation loss: 2.640608156881025

Epoch: 5| Step: 1
Training loss: 2.6878790855407715
Validation loss: 2.642270959833617

Epoch: 5| Step: 2
Training loss: 2.3550305366516113
Validation loss: 2.640480372213548

Epoch: 5| Step: 3
Training loss: 2.608822822570801
Validation loss: 2.640059724930794

Epoch: 5| Step: 4
Training loss: 3.3902359008789062
Validation loss: 2.639702514935565

Epoch: 5| Step: 5
Training loss: 2.9058332443237305
Validation loss: 2.640343148221252

Epoch: 5| Step: 6
Training loss: 3.3593878746032715
Validation loss: 2.642145461933587

Epoch: 5| Step: 7
Training loss: 2.5890491008758545
Validation loss: 2.64762419526295

Epoch: 5| Step: 8
Training loss: 2.738783359527588
Validation loss: 2.6453224741002566

Epoch: 5| Step: 9
Training loss: 2.7890734672546387
Validation loss: 2.644393026187856

Epoch: 5| Step: 10
Training loss: 2.716067314147949
Validation loss: 2.648120721181234

Epoch: 111| Step: 0
Training loss: 2.2710721492767334
Validation loss: 2.646813131147815

Epoch: 5| Step: 1
Training loss: 3.1546199321746826
Validation loss: 2.6457494971572713

Epoch: 5| Step: 2
Training loss: 3.3274893760681152
Validation loss: 2.6513448325536584

Epoch: 5| Step: 3
Training loss: 2.2282962799072266
Validation loss: 2.64315208824732

Epoch: 5| Step: 4
Training loss: 2.7310807704925537
Validation loss: 2.6407175858815513

Epoch: 5| Step: 5
Training loss: 2.79972505569458
Validation loss: 2.638405810120285

Epoch: 5| Step: 6
Training loss: 2.846090793609619
Validation loss: 2.636232004370741

Epoch: 5| Step: 7
Training loss: 2.483255386352539
Validation loss: 2.636776731860253

Epoch: 5| Step: 8
Training loss: 2.517303943634033
Validation loss: 2.6355038304482736

Epoch: 5| Step: 9
Training loss: 2.6552822589874268
Validation loss: 2.634592589511666

Epoch: 5| Step: 10
Training loss: 3.8964929580688477
Validation loss: 2.633343711976082

Epoch: 112| Step: 0
Training loss: 3.336498975753784
Validation loss: 2.6357303973167174

Epoch: 5| Step: 1
Training loss: 2.5119576454162598
Validation loss: 2.6336343749876945

Epoch: 5| Step: 2
Training loss: 2.255066394805908
Validation loss: 2.6331133329740135

Epoch: 5| Step: 3
Training loss: 2.7227706909179688
Validation loss: 2.6350831985473633

Epoch: 5| Step: 4
Training loss: 3.6448187828063965
Validation loss: 2.6368504878013366

Epoch: 5| Step: 5
Training loss: 3.3479537963867188
Validation loss: 2.6339337595047487

Epoch: 5| Step: 6
Training loss: 2.474611759185791
Validation loss: 2.6315595872940554

Epoch: 5| Step: 7
Training loss: 2.6229310035705566
Validation loss: 2.629753510157267

Epoch: 5| Step: 8
Training loss: 2.1500000953674316
Validation loss: 2.6316212223422144

Epoch: 5| Step: 9
Training loss: 3.082882881164551
Validation loss: 2.6335694584795224

Epoch: 5| Step: 10
Training loss: 2.3919010162353516
Validation loss: 2.6354578694989605

Epoch: 113| Step: 0
Training loss: 3.0076913833618164
Validation loss: 2.6353567005485616

Epoch: 5| Step: 1
Training loss: 2.8071911334991455
Validation loss: 2.6393175817305043

Epoch: 5| Step: 2
Training loss: 3.3958420753479004
Validation loss: 2.642902135848999

Epoch: 5| Step: 3
Training loss: 2.131103277206421
Validation loss: 2.63813954271296

Epoch: 5| Step: 4
Training loss: 3.043168783187866
Validation loss: 2.637415357815322

Epoch: 5| Step: 5
Training loss: 2.3930106163024902
Validation loss: 2.6357543596657376

Epoch: 5| Step: 6
Training loss: 3.0338921546936035
Validation loss: 2.6362755375523723

Epoch: 5| Step: 7
Training loss: 2.682837724685669
Validation loss: 2.636655358858006

Epoch: 5| Step: 8
Training loss: 2.921034097671509
Validation loss: 2.632331548198577

Epoch: 5| Step: 9
Training loss: 2.493818759918213
Validation loss: 2.633729273273099

Epoch: 5| Step: 10
Training loss: 2.7152392864227295
Validation loss: 2.6319409160203833

Epoch: 114| Step: 0
Training loss: 2.058579206466675
Validation loss: 2.6319635888581634

Epoch: 5| Step: 1
Training loss: 3.1298575401306152
Validation loss: 2.6301107534798245

Epoch: 5| Step: 2
Training loss: 3.012338638305664
Validation loss: 2.6264082693284556

Epoch: 5| Step: 3
Training loss: 2.4728875160217285
Validation loss: 2.6309419011556976

Epoch: 5| Step: 4
Training loss: 3.46380877494812
Validation loss: 2.633145962992022

Epoch: 5| Step: 5
Training loss: 2.636869430541992
Validation loss: 2.6267435781417356

Epoch: 5| Step: 6
Training loss: 2.3879313468933105
Validation loss: 2.6258555253346763

Epoch: 5| Step: 7
Training loss: 2.999282121658325
Validation loss: 2.628079701495427

Epoch: 5| Step: 8
Training loss: 2.8771045207977295
Validation loss: 2.6258194344018095

Epoch: 5| Step: 9
Training loss: 2.6696574687957764
Validation loss: 2.626625866018316

Epoch: 5| Step: 10
Training loss: 2.8693413734436035
Validation loss: 2.6224583938557613

Epoch: 115| Step: 0
Training loss: 3.5930614471435547
Validation loss: 2.627385354811145

Epoch: 5| Step: 1
Training loss: 2.9551711082458496
Validation loss: 2.6286059912814888

Epoch: 5| Step: 2
Training loss: 2.9688329696655273
Validation loss: 2.6265846529314594

Epoch: 5| Step: 3
Training loss: 2.3413543701171875
Validation loss: 2.6226253099338983

Epoch: 5| Step: 4
Training loss: 2.3654165267944336
Validation loss: 2.6226822791561

Epoch: 5| Step: 5
Training loss: 2.723686933517456
Validation loss: 2.6208135876604306

Epoch: 5| Step: 6
Training loss: 3.181184768676758
Validation loss: 2.624015462013983

Epoch: 5| Step: 7
Training loss: 2.9389753341674805
Validation loss: 2.6219715200444704

Epoch: 5| Step: 8
Training loss: 2.8051953315734863
Validation loss: 2.6234801738492903

Epoch: 5| Step: 9
Training loss: 1.8730354309082031
Validation loss: 2.621047768541562

Epoch: 5| Step: 10
Training loss: 2.8283333778381348
Validation loss: 2.6252059295613277

Epoch: 116| Step: 0
Training loss: 3.0647616386413574
Validation loss: 2.6234537914235103

Epoch: 5| Step: 1
Training loss: 2.5649125576019287
Validation loss: 2.623442226840604

Epoch: 5| Step: 2
Training loss: 2.810716152191162
Validation loss: 2.6241990212471253

Epoch: 5| Step: 3
Training loss: 3.3065788745880127
Validation loss: 2.622166374678253

Epoch: 5| Step: 4
Training loss: 2.6555323600769043
Validation loss: 2.620922088623047

Epoch: 5| Step: 5
Training loss: 2.474299907684326
Validation loss: 2.6222859710775395

Epoch: 5| Step: 6
Training loss: 2.058406352996826
Validation loss: 2.619935820179601

Epoch: 5| Step: 7
Training loss: 3.0193161964416504
Validation loss: 2.620809203834944

Epoch: 5| Step: 8
Training loss: 2.7947208881378174
Validation loss: 2.622260978144984

Epoch: 5| Step: 9
Training loss: 3.331808567047119
Validation loss: 2.623951058233938

Epoch: 5| Step: 10
Training loss: 2.404466390609741
Validation loss: 2.6245650591388827

Epoch: 117| Step: 0
Training loss: 2.01484751701355
Validation loss: 2.6259063674557592

Epoch: 5| Step: 1
Training loss: 2.989217519760132
Validation loss: 2.623383811725083

Epoch: 5| Step: 2
Training loss: 2.742060899734497
Validation loss: 2.625092398735785

Epoch: 5| Step: 3
Training loss: 2.7166004180908203
Validation loss: 2.6279423288119736

Epoch: 5| Step: 4
Training loss: 3.0354721546173096
Validation loss: 2.6244805294980287

Epoch: 5| Step: 5
Training loss: 2.259308099746704
Validation loss: 2.6268036237327

Epoch: 5| Step: 6
Training loss: 2.9227166175842285
Validation loss: 2.625150480578023

Epoch: 5| Step: 7
Training loss: 3.065772533416748
Validation loss: 2.631642415959348

Epoch: 5| Step: 8
Training loss: 3.0484023094177246
Validation loss: 2.6486305754671813

Epoch: 5| Step: 9
Training loss: 3.041926860809326
Validation loss: 2.6463736231609056

Epoch: 5| Step: 10
Training loss: 2.6818416118621826
Validation loss: 2.6358310227753012

Epoch: 118| Step: 0
Training loss: 2.376235008239746
Validation loss: 2.648974857022685

Epoch: 5| Step: 1
Training loss: 3.376833438873291
Validation loss: 2.650662145306987

Epoch: 5| Step: 2
Training loss: 2.7997918128967285
Validation loss: 2.6379505741980767

Epoch: 5| Step: 3
Training loss: 3.1219332218170166
Validation loss: 2.6311039206802205

Epoch: 5| Step: 4
Training loss: 2.9373176097869873
Validation loss: 2.634967760373187

Epoch: 5| Step: 5
Training loss: 2.2909131050109863
Validation loss: 2.627679829956383

Epoch: 5| Step: 6
Training loss: 2.7326159477233887
Validation loss: 2.6336036318091938

Epoch: 5| Step: 7
Training loss: 3.028916597366333
Validation loss: 2.6286594816433486

Epoch: 5| Step: 8
Training loss: 2.149573564529419
Validation loss: 2.6349537705862396

Epoch: 5| Step: 9
Training loss: 2.5221381187438965
Validation loss: 2.629968309915194

Epoch: 5| Step: 10
Training loss: 3.206871747970581
Validation loss: 2.63625083943849

Epoch: 119| Step: 0
Training loss: 3.0929553508758545
Validation loss: 2.6323934370471584

Epoch: 5| Step: 1
Training loss: 3.0720038414001465
Validation loss: 2.627751294002738

Epoch: 5| Step: 2
Training loss: 3.0829665660858154
Validation loss: 2.6269477823729157

Epoch: 5| Step: 3
Training loss: 2.8903400897979736
Validation loss: 2.633214307087724

Epoch: 5| Step: 4
Training loss: 2.036978244781494
Validation loss: 2.6301887471188783

Epoch: 5| Step: 5
Training loss: 2.7183690071105957
Validation loss: 2.628770742365109

Epoch: 5| Step: 6
Training loss: 2.984941005706787
Validation loss: 2.6338472725242696

Epoch: 5| Step: 7
Training loss: 3.4208648204803467
Validation loss: 2.6385247681730535

Epoch: 5| Step: 8
Training loss: 2.323610782623291
Validation loss: 2.633630073198708

Epoch: 5| Step: 9
Training loss: 2.223038911819458
Validation loss: 2.6296542331736577

Epoch: 5| Step: 10
Training loss: 2.567288875579834
Validation loss: 2.627970093040056

Epoch: 120| Step: 0
Training loss: 3.001736879348755
Validation loss: 2.6245853900909424

Epoch: 5| Step: 1
Training loss: 2.656731605529785
Validation loss: 2.6226089821066907

Epoch: 5| Step: 2
Training loss: 2.2439353466033936
Validation loss: 2.619181671450215

Epoch: 5| Step: 3
Training loss: 2.7493855953216553
Validation loss: 2.6196002473113356

Epoch: 5| Step: 4
Training loss: 2.982020854949951
Validation loss: 2.6139397262245097

Epoch: 5| Step: 5
Training loss: 3.2066447734832764
Validation loss: 2.617954595114595

Epoch: 5| Step: 6
Training loss: 2.9952683448791504
Validation loss: 2.617740010702482

Epoch: 5| Step: 7
Training loss: 2.845104217529297
Validation loss: 2.6146935416806127

Epoch: 5| Step: 8
Training loss: 2.2125866413116455
Validation loss: 2.6172960701809136

Epoch: 5| Step: 9
Training loss: 2.845451831817627
Validation loss: 2.6178215165292062

Epoch: 5| Step: 10
Training loss: 2.716407299041748
Validation loss: 2.621401822695168

Epoch: 121| Step: 0
Training loss: 2.6345152854919434
Validation loss: 2.6235151444711993

Epoch: 5| Step: 1
Training loss: 3.2959563732147217
Validation loss: 2.61871926758879

Epoch: 5| Step: 2
Training loss: 3.697864532470703
Validation loss: 2.618887916687996

Epoch: 5| Step: 3
Training loss: 2.6838536262512207
Validation loss: 2.6249491040424635

Epoch: 5| Step: 4
Training loss: 2.009739398956299
Validation loss: 2.6285943805530505

Epoch: 5| Step: 5
Training loss: 2.734569549560547
Validation loss: 2.6326826541654524

Epoch: 5| Step: 6
Training loss: 2.6350436210632324
Validation loss: 2.636530719777589

Epoch: 5| Step: 7
Training loss: 2.8775031566619873
Validation loss: 2.627876753448158

Epoch: 5| Step: 8
Training loss: 2.6285240650177
Validation loss: 2.6227628595085553

Epoch: 5| Step: 9
Training loss: 2.3619539737701416
Validation loss: 2.613054665186072

Epoch: 5| Step: 10
Training loss: 2.9803991317749023
Validation loss: 2.610537382864183

Epoch: 122| Step: 0
Training loss: 3.2137210369110107
Validation loss: 2.6124477104474138

Epoch: 5| Step: 1
Training loss: 2.9326982498168945
Validation loss: 2.613056605862033

Epoch: 5| Step: 2
Training loss: 2.956861972808838
Validation loss: 2.61877929523427

Epoch: 5| Step: 3
Training loss: 3.6304965019226074
Validation loss: 2.6198908462319324

Epoch: 5| Step: 4
Training loss: 2.4937198162078857
Validation loss: 2.6215509701800603

Epoch: 5| Step: 5
Training loss: 2.6606814861297607
Validation loss: 2.626388931787142

Epoch: 5| Step: 6
Training loss: 2.475095272064209
Validation loss: 2.6410964099309777

Epoch: 5| Step: 7
Training loss: 3.533298969268799
Validation loss: 2.6174664830648773

Epoch: 5| Step: 8
Training loss: 1.7621595859527588
Validation loss: 2.612041775898267

Epoch: 5| Step: 9
Training loss: 2.3265271186828613
Validation loss: 2.608007918121994

Epoch: 5| Step: 10
Training loss: 2.434084177017212
Validation loss: 2.6127364302194245

Epoch: 123| Step: 0
Training loss: 3.1763389110565186
Validation loss: 2.6094265881405083

Epoch: 5| Step: 1
Training loss: 3.039482831954956
Validation loss: 2.611687660217285

Epoch: 5| Step: 2
Training loss: 2.745244264602661
Validation loss: 2.6150733040225123

Epoch: 5| Step: 3
Training loss: 2.7191033363342285
Validation loss: 2.6139805778380363

Epoch: 5| Step: 4
Training loss: 2.4699547290802
Validation loss: 2.6138851719517864

Epoch: 5| Step: 5
Training loss: 3.225874423980713
Validation loss: 2.620993052759478

Epoch: 5| Step: 6
Training loss: 2.092385768890381
Validation loss: 2.619210981553601

Epoch: 5| Step: 7
Training loss: 2.563171863555908
Validation loss: 2.6160363715182067

Epoch: 5| Step: 8
Training loss: 2.98745059967041
Validation loss: 2.616453298958399

Epoch: 5| Step: 9
Training loss: 2.6840879917144775
Validation loss: 2.608283042907715

Epoch: 5| Step: 10
Training loss: 2.723679304122925
Validation loss: 2.6111187088874077

Epoch: 124| Step: 0
Training loss: 3.0960114002227783
Validation loss: 2.6072039886187484

Epoch: 5| Step: 1
Training loss: 2.0246243476867676
Validation loss: 2.6047851462518015

Epoch: 5| Step: 2
Training loss: 2.175943374633789
Validation loss: 2.611487465520059

Epoch: 5| Step: 3
Training loss: 2.5896146297454834
Validation loss: 2.6098040714058826

Epoch: 5| Step: 4
Training loss: 2.764317512512207
Validation loss: 2.605392494509297

Epoch: 5| Step: 5
Training loss: 2.714043378829956
Validation loss: 2.6056243168410433

Epoch: 5| Step: 6
Training loss: 2.2546887397766113
Validation loss: 2.6064851873664447

Epoch: 5| Step: 7
Training loss: 3.0788304805755615
Validation loss: 2.606910485093312

Epoch: 5| Step: 8
Training loss: 3.8398385047912598
Validation loss: 2.608660254427182

Epoch: 5| Step: 9
Training loss: 2.34716796875
Validation loss: 2.6076010862986245

Epoch: 5| Step: 10
Training loss: 3.6261072158813477
Validation loss: 2.6076663373619

Epoch: 125| Step: 0
Training loss: 3.0987870693206787
Validation loss: 2.6106562998987015

Epoch: 5| Step: 1
Training loss: 3.0367014408111572
Validation loss: 2.6138297614230903

Epoch: 5| Step: 2
Training loss: 2.307450532913208
Validation loss: 2.602841151657925

Epoch: 5| Step: 3
Training loss: 2.120832920074463
Validation loss: 2.6060759764845653

Epoch: 5| Step: 4
Training loss: 2.099677801132202
Validation loss: 2.602944102338565

Epoch: 5| Step: 5
Training loss: 2.816370964050293
Validation loss: 2.6061206581772014

Epoch: 5| Step: 6
Training loss: 2.683798313140869
Validation loss: 2.6037941414822816

Epoch: 5| Step: 7
Training loss: 2.624124526977539
Validation loss: 2.6064300562745784

Epoch: 5| Step: 8
Training loss: 3.0457913875579834
Validation loss: 2.606198828707459

Epoch: 5| Step: 9
Training loss: 3.5889251232147217
Validation loss: 2.606076145684847

Epoch: 5| Step: 10
Training loss: 3.0535919666290283
Validation loss: 2.6033801801743044

Epoch: 126| Step: 0
Training loss: 3.052438974380493
Validation loss: 2.6028082601485716

Epoch: 5| Step: 1
Training loss: 3.3114418983459473
Validation loss: 2.603851946451331

Epoch: 5| Step: 2
Training loss: 2.901329755783081
Validation loss: 2.599862895986085

Epoch: 5| Step: 3
Training loss: 2.620011568069458
Validation loss: 2.6062936141926754

Epoch: 5| Step: 4
Training loss: 2.460771083831787
Validation loss: 2.606519058186521

Epoch: 5| Step: 5
Training loss: 3.138136386871338
Validation loss: 2.6114278890753306

Epoch: 5| Step: 6
Training loss: 2.3764896392822266
Validation loss: 2.6134043790960826

Epoch: 5| Step: 7
Training loss: 2.6664414405822754
Validation loss: 2.6188678074908514

Epoch: 5| Step: 8
Training loss: 2.5044970512390137
Validation loss: 2.6138762248459684

Epoch: 5| Step: 9
Training loss: 2.9519340991973877
Validation loss: 2.614781807827693

Epoch: 5| Step: 10
Training loss: 2.338191270828247
Validation loss: 2.6099415261258363

Epoch: 127| Step: 0
Training loss: 2.7180371284484863
Validation loss: 2.6073988099252023

Epoch: 5| Step: 1
Training loss: 2.9644641876220703
Validation loss: 2.601666506900582

Epoch: 5| Step: 2
Training loss: 3.086348056793213
Validation loss: 2.603511018137778

Epoch: 5| Step: 3
Training loss: 2.4279704093933105
Validation loss: 2.6032937162665912

Epoch: 5| Step: 4
Training loss: 2.7754905223846436
Validation loss: 2.602178999172744

Epoch: 5| Step: 5
Training loss: 2.8718695640563965
Validation loss: 2.600790277604134

Epoch: 5| Step: 6
Training loss: 1.842694640159607
Validation loss: 2.602232038333852

Epoch: 5| Step: 7
Training loss: 2.1541171073913574
Validation loss: 2.6018143110377814

Epoch: 5| Step: 8
Training loss: 3.4724559783935547
Validation loss: 2.602751560108636

Epoch: 5| Step: 9
Training loss: 3.014983654022217
Validation loss: 2.6040804821957826

Epoch: 5| Step: 10
Training loss: 3.0156829357147217
Validation loss: 2.5979776100445817

Epoch: 128| Step: 0
Training loss: 2.680129051208496
Validation loss: 2.5985915455766904

Epoch: 5| Step: 1
Training loss: 2.6896843910217285
Validation loss: 2.596934421088106

Epoch: 5| Step: 2
Training loss: 2.7803962230682373
Validation loss: 2.6002015682958786

Epoch: 5| Step: 3
Training loss: 3.1608166694641113
Validation loss: 2.5975512484068513

Epoch: 5| Step: 4
Training loss: 2.9613566398620605
Validation loss: 2.598345661676058

Epoch: 5| Step: 5
Training loss: 3.1958515644073486
Validation loss: 2.5968603011100524

Epoch: 5| Step: 6
Training loss: 2.6218490600585938
Validation loss: 2.601000009044524

Epoch: 5| Step: 7
Training loss: 2.6490540504455566
Validation loss: 2.6033222598414265

Epoch: 5| Step: 8
Training loss: 2.499359607696533
Validation loss: 2.5994354576192875

Epoch: 5| Step: 9
Training loss: 2.7719874382019043
Validation loss: 2.602931163644278

Epoch: 5| Step: 10
Training loss: 2.231438159942627
Validation loss: 2.60197131095394

Epoch: 129| Step: 0
Training loss: 2.4253971576690674
Validation loss: 2.601058775378812

Epoch: 5| Step: 1
Training loss: 2.7404613494873047
Validation loss: 2.6001574634223856

Epoch: 5| Step: 2
Training loss: 2.0788490772247314
Validation loss: 2.6034506418371715

Epoch: 5| Step: 3
Training loss: 3.3182830810546875
Validation loss: 2.5992657317910144

Epoch: 5| Step: 4
Training loss: 2.7778005599975586
Validation loss: 2.6003358005195536

Epoch: 5| Step: 5
Training loss: 2.9863481521606445
Validation loss: 2.6009774002977597

Epoch: 5| Step: 6
Training loss: 2.795030117034912
Validation loss: 2.5973282475625314

Epoch: 5| Step: 7
Training loss: 2.5528037548065186
Validation loss: 2.5979469617207847

Epoch: 5| Step: 8
Training loss: 2.6997427940368652
Validation loss: 2.602298664790328

Epoch: 5| Step: 9
Training loss: 2.6725056171417236
Validation loss: 2.5995826362281718

Epoch: 5| Step: 10
Training loss: 3.322939157485962
Validation loss: 2.60234930182016

Epoch: 130| Step: 0
Training loss: 2.8215746879577637
Validation loss: 2.598567249954388

Epoch: 5| Step: 1
Training loss: 2.7504448890686035
Validation loss: 2.599461354235167

Epoch: 5| Step: 2
Training loss: 3.0071887969970703
Validation loss: 2.597517959533199

Epoch: 5| Step: 3
Training loss: 2.3409292697906494
Validation loss: 2.594705681647024

Epoch: 5| Step: 4
Training loss: 2.888685464859009
Validation loss: 2.5974232253207954

Epoch: 5| Step: 5
Training loss: 2.867154121398926
Validation loss: 2.59664713310939

Epoch: 5| Step: 6
Training loss: 3.0764498710632324
Validation loss: 2.5968674690492692

Epoch: 5| Step: 7
Training loss: 2.629708766937256
Validation loss: 2.5953125338400564

Epoch: 5| Step: 8
Training loss: 3.1762707233428955
Validation loss: 2.5975148190734205

Epoch: 5| Step: 9
Training loss: 2.360372543334961
Validation loss: 2.593225961090416

Epoch: 5| Step: 10
Training loss: 2.323530673980713
Validation loss: 2.59777489272497

Epoch: 131| Step: 0
Training loss: 2.1428067684173584
Validation loss: 2.595776081085205

Epoch: 5| Step: 1
Training loss: 3.7694594860076904
Validation loss: 2.5983259139522428

Epoch: 5| Step: 2
Training loss: 2.1578688621520996
Validation loss: 2.6030866305033364

Epoch: 5| Step: 3
Training loss: 3.792768955230713
Validation loss: 2.599131025293822

Epoch: 5| Step: 4
Training loss: 1.9266160726547241
Validation loss: 2.599725120811052

Epoch: 5| Step: 5
Training loss: 3.15057373046875
Validation loss: 2.602437024475426

Epoch: 5| Step: 6
Training loss: 3.094454288482666
Validation loss: 2.599235111667264

Epoch: 5| Step: 7
Training loss: 3.11824107170105
Validation loss: 2.594320948405932

Epoch: 5| Step: 8
Training loss: 2.199826717376709
Validation loss: 2.60021811403254

Epoch: 5| Step: 9
Training loss: 2.62073016166687
Validation loss: 2.5988945473906813

Epoch: 5| Step: 10
Training loss: 2.2199599742889404
Validation loss: 2.597797862945064

Epoch: 132| Step: 0
Training loss: 3.219062089920044
Validation loss: 2.593682878760881

Epoch: 5| Step: 1
Training loss: 2.638575792312622
Validation loss: 2.592865054325391

Epoch: 5| Step: 2
Training loss: 2.4363865852355957
Validation loss: 2.5939000550136773

Epoch: 5| Step: 3
Training loss: 3.258751630783081
Validation loss: 2.5943441519173245

Epoch: 5| Step: 4
Training loss: 2.981510639190674
Validation loss: 2.589333652168192

Epoch: 5| Step: 5
Training loss: 2.292482376098633
Validation loss: 2.5866781460341586

Epoch: 5| Step: 6
Training loss: 2.944553852081299
Validation loss: 2.5913837827661985

Epoch: 5| Step: 7
Training loss: 2.8711299896240234
Validation loss: 2.5917394366315616

Epoch: 5| Step: 8
Training loss: 3.1205132007598877
Validation loss: 2.5910910714057183

Epoch: 5| Step: 9
Training loss: 2.0305628776550293
Validation loss: 2.5899586190459547

Epoch: 5| Step: 10
Training loss: 2.4072158336639404
Validation loss: 2.590631887476931

Epoch: 133| Step: 0
Training loss: 3.471426486968994
Validation loss: 2.586038138276787

Epoch: 5| Step: 1
Training loss: 2.625194549560547
Validation loss: 2.5886581841335503

Epoch: 5| Step: 2
Training loss: 2.929783821105957
Validation loss: 2.58905977331182

Epoch: 5| Step: 3
Training loss: 2.6473987102508545
Validation loss: 2.590203041671425

Epoch: 5| Step: 4
Training loss: 3.3663032054901123
Validation loss: 2.592544327500046

Epoch: 5| Step: 5
Training loss: 2.6455066204071045
Validation loss: 2.5919044556156283

Epoch: 5| Step: 6
Training loss: 2.8064210414886475
Validation loss: 2.5980604746008433

Epoch: 5| Step: 7
Training loss: 2.1716761589050293
Validation loss: 2.5992811136348273

Epoch: 5| Step: 8
Training loss: 2.16609787940979
Validation loss: 2.5974085971873295

Epoch: 5| Step: 9
Training loss: 2.629591226577759
Validation loss: 2.5969487210755706

Epoch: 5| Step: 10
Training loss: 2.8408541679382324
Validation loss: 2.5966363953005884

Epoch: 134| Step: 0
Training loss: 2.959700107574463
Validation loss: 2.595863239739531

Epoch: 5| Step: 1
Training loss: 3.0359623432159424
Validation loss: 2.591357684904529

Epoch: 5| Step: 2
Training loss: 2.7371795177459717
Validation loss: 2.5878621070615706

Epoch: 5| Step: 3
Training loss: 2.610116481781006
Validation loss: 2.586821543273105

Epoch: 5| Step: 4
Training loss: 3.1641321182250977
Validation loss: 2.586855314111197

Epoch: 5| Step: 5
Training loss: 3.0498290061950684
Validation loss: 2.5857815691219863

Epoch: 5| Step: 6
Training loss: 2.9798388481140137
Validation loss: 2.5869359585546676

Epoch: 5| Step: 7
Training loss: 2.70680570602417
Validation loss: 2.584041505731562

Epoch: 5| Step: 8
Training loss: 1.8444578647613525
Validation loss: 2.5819220747998965

Epoch: 5| Step: 9
Training loss: 2.6428415775299072
Validation loss: 2.587284166325805

Epoch: 5| Step: 10
Training loss: 2.47249436378479
Validation loss: 2.5847181453499743

Epoch: 135| Step: 0
Training loss: 2.6281986236572266
Validation loss: 2.584783072112709

Epoch: 5| Step: 1
Training loss: 2.643002986907959
Validation loss: 2.587299980143065

Epoch: 5| Step: 2
Training loss: 2.3879847526550293
Validation loss: 2.587214608346262

Epoch: 5| Step: 3
Training loss: 1.8153034448623657
Validation loss: 2.587963342666626

Epoch: 5| Step: 4
Training loss: 2.187615394592285
Validation loss: 2.584646912031276

Epoch: 5| Step: 5
Training loss: 2.8422422409057617
Validation loss: 2.586006679842549

Epoch: 5| Step: 6
Training loss: 2.677140712738037
Validation loss: 2.5871840061679965

Epoch: 5| Step: 7
Training loss: 2.988100051879883
Validation loss: 2.5847666801944857

Epoch: 5| Step: 8
Training loss: 2.9765429496765137
Validation loss: 2.5856396921219362

Epoch: 5| Step: 9
Training loss: 3.732363224029541
Validation loss: 2.587126112753345

Epoch: 5| Step: 10
Training loss: 3.4379477500915527
Validation loss: 2.587372795228035

Epoch: 136| Step: 0
Training loss: 2.7197415828704834
Validation loss: 2.582754868333058

Epoch: 5| Step: 1
Training loss: 2.3647849559783936
Validation loss: 2.587038004270164

Epoch: 5| Step: 2
Training loss: 2.0450921058654785
Validation loss: 2.585326753636842

Epoch: 5| Step: 3
Training loss: 3.2098472118377686
Validation loss: 2.5851854842196227

Epoch: 5| Step: 4
Training loss: 3.160924196243286
Validation loss: 2.583961007415607

Epoch: 5| Step: 5
Training loss: 3.134204387664795
Validation loss: 2.5880401057581746

Epoch: 5| Step: 6
Training loss: 3.3069915771484375
Validation loss: 2.5879468969119492

Epoch: 5| Step: 7
Training loss: 2.0632617473602295
Validation loss: 2.587794739712951

Epoch: 5| Step: 8
Training loss: 2.909965991973877
Validation loss: 2.589277567402009

Epoch: 5| Step: 9
Training loss: 2.7429096698760986
Validation loss: 2.592723177325341

Epoch: 5| Step: 10
Training loss: 2.515925645828247
Validation loss: 2.5903346179634013

Epoch: 137| Step: 0
Training loss: 3.2699267864227295
Validation loss: 2.5922545925263436

Epoch: 5| Step: 1
Training loss: 2.237708568572998
Validation loss: 2.5840742408588366

Epoch: 5| Step: 2
Training loss: 2.4159092903137207
Validation loss: 2.5852950439658215

Epoch: 5| Step: 3
Training loss: 2.255239725112915
Validation loss: 2.5819613138834634

Epoch: 5| Step: 4
Training loss: 3.5796401500701904
Validation loss: 2.5827119170978503

Epoch: 5| Step: 5
Training loss: 3.0583267211914062
Validation loss: 2.5806513319733324

Epoch: 5| Step: 6
Training loss: 2.492143154144287
Validation loss: 2.5809040018307265

Epoch: 5| Step: 7
Training loss: 2.474094867706299
Validation loss: 2.580947191484513

Epoch: 5| Step: 8
Training loss: 2.7254624366760254
Validation loss: 2.581707613442534

Epoch: 5| Step: 9
Training loss: 3.088589668273926
Validation loss: 2.5813965566696657

Epoch: 5| Step: 10
Training loss: 2.4856417179107666
Validation loss: 2.5810814724173596

Epoch: 138| Step: 0
Training loss: 2.814729690551758
Validation loss: 2.5816438300635225

Epoch: 5| Step: 1
Training loss: 3.036304473876953
Validation loss: 2.579535879114623

Epoch: 5| Step: 2
Training loss: 3.03515887260437
Validation loss: 2.5803609663440334

Epoch: 5| Step: 3
Training loss: 2.6446025371551514
Validation loss: 2.5811172095678185

Epoch: 5| Step: 4
Training loss: 2.466620922088623
Validation loss: 2.5767909378133793

Epoch: 5| Step: 5
Training loss: 2.3787574768066406
Validation loss: 2.5768548006652505

Epoch: 5| Step: 6
Training loss: 2.1480472087860107
Validation loss: 2.5752423271056144

Epoch: 5| Step: 7
Training loss: 2.3084423542022705
Validation loss: 2.581839474298621

Epoch: 5| Step: 8
Training loss: 3.473358631134033
Validation loss: 2.5838397369589856

Epoch: 5| Step: 9
Training loss: 3.1883816719055176
Validation loss: 2.5880225755835093

Epoch: 5| Step: 10
Training loss: 2.613539218902588
Validation loss: 2.5884429434294343

Epoch: 139| Step: 0
Training loss: 2.887871742248535
Validation loss: 2.5927199317562963

Epoch: 5| Step: 1
Training loss: 2.972071647644043
Validation loss: 2.591421106810211

Epoch: 5| Step: 2
Training loss: 2.444240093231201
Validation loss: 2.59634078702619

Epoch: 5| Step: 3
Training loss: 2.6843135356903076
Validation loss: 2.596776318806474

Epoch: 5| Step: 4
Training loss: 2.1339364051818848
Validation loss: 2.5874971010351695

Epoch: 5| Step: 5
Training loss: 3.6441714763641357
Validation loss: 2.589154217832832

Epoch: 5| Step: 6
Training loss: 2.370129108428955
Validation loss: 2.5832977012921403

Epoch: 5| Step: 7
Training loss: 2.565218448638916
Validation loss: 2.585227740708218

Epoch: 5| Step: 8
Training loss: 2.6586880683898926
Validation loss: 2.5763846443545435

Epoch: 5| Step: 9
Training loss: 3.1134533882141113
Validation loss: 2.579909991192561

Epoch: 5| Step: 10
Training loss: 2.6018457412719727
Validation loss: 2.5768053839283604

Epoch: 140| Step: 0
Training loss: 3.18904709815979
Validation loss: 2.5809671032813286

Epoch: 5| Step: 1
Training loss: 2.4803519248962402
Validation loss: 2.5835451182498725

Epoch: 5| Step: 2
Training loss: 2.6091527938842773
Validation loss: 2.585384281732703

Epoch: 5| Step: 3
Training loss: 2.861299991607666
Validation loss: 2.58380824776106

Epoch: 5| Step: 4
Training loss: 3.0979721546173096
Validation loss: 2.582110651077763

Epoch: 5| Step: 5
Training loss: 2.1865227222442627
Validation loss: 2.581020170642484

Epoch: 5| Step: 6
Training loss: 2.4109201431274414
Validation loss: 2.5754049106310775

Epoch: 5| Step: 7
Training loss: 3.4924159049987793
Validation loss: 2.574864302912066

Epoch: 5| Step: 8
Training loss: 3.036468029022217
Validation loss: 2.57350678084999

Epoch: 5| Step: 9
Training loss: 1.8588440418243408
Validation loss: 2.5724905742112028

Epoch: 5| Step: 10
Training loss: 2.941232919692993
Validation loss: 2.5734475299876225

Epoch: 141| Step: 0
Training loss: 2.305239200592041
Validation loss: 2.5739749708483295

Epoch: 5| Step: 1
Training loss: 2.733948230743408
Validation loss: 2.5687346355889433

Epoch: 5| Step: 2
Training loss: 2.6158511638641357
Validation loss: 2.5730666345165623

Epoch: 5| Step: 3
Training loss: 2.8136699199676514
Validation loss: 2.5736441612243652

Epoch: 5| Step: 4
Training loss: 3.2722315788269043
Validation loss: 2.573547335081203

Epoch: 5| Step: 5
Training loss: 3.1665616035461426
Validation loss: 2.579001562569731

Epoch: 5| Step: 6
Training loss: 3.529193878173828
Validation loss: 2.5746242794939267

Epoch: 5| Step: 7
Training loss: 2.5250539779663086
Validation loss: 2.5805228551228843

Epoch: 5| Step: 8
Training loss: 2.3285439014434814
Validation loss: 2.5770628811210714

Epoch: 5| Step: 9
Training loss: 2.116316556930542
Validation loss: 2.578348554590697

Epoch: 5| Step: 10
Training loss: 2.6862432956695557
Validation loss: 2.5731931194182365

Epoch: 142| Step: 0
Training loss: 3.426098585128784
Validation loss: 2.5738413872257357

Epoch: 5| Step: 1
Training loss: 3.4569315910339355
Validation loss: 2.5716210462713756

Epoch: 5| Step: 2
Training loss: 2.397771120071411
Validation loss: 2.5723910588090138

Epoch: 5| Step: 3
Training loss: 2.2132577896118164
Validation loss: 2.567847923565936

Epoch: 5| Step: 4
Training loss: 2.568687915802002
Validation loss: 2.5717722703051824

Epoch: 5| Step: 5
Training loss: 1.9937622547149658
Validation loss: 2.5664583893232447

Epoch: 5| Step: 6
Training loss: 2.6176929473876953
Validation loss: 2.5693835468702417

Epoch: 5| Step: 7
Training loss: 2.457908868789673
Validation loss: 2.5724725697630193

Epoch: 5| Step: 8
Training loss: 2.7701783180236816
Validation loss: 2.5728816011900544

Epoch: 5| Step: 9
Training loss: 3.557959794998169
Validation loss: 2.5701848999146493

Epoch: 5| Step: 10
Training loss: 2.616558790206909
Validation loss: 2.571606389937862

Epoch: 143| Step: 0
Training loss: 3.161092758178711
Validation loss: 2.5766678881901566

Epoch: 5| Step: 1
Training loss: 2.8635120391845703
Validation loss: 2.5798663349561792

Epoch: 5| Step: 2
Training loss: 2.9505527019500732
Validation loss: 2.5730339404075377

Epoch: 5| Step: 3
Training loss: 2.121034860610962
Validation loss: 2.5728851492686937

Epoch: 5| Step: 4
Training loss: 2.3033995628356934
Validation loss: 2.565672561686526

Epoch: 5| Step: 5
Training loss: 2.7572615146636963
Validation loss: 2.566544343066472

Epoch: 5| Step: 6
Training loss: 2.9260759353637695
Validation loss: 2.561567501355243

Epoch: 5| Step: 7
Training loss: 2.859264373779297
Validation loss: 2.5660739509008264

Epoch: 5| Step: 8
Training loss: 2.3142638206481934
Validation loss: 2.5683695090714322

Epoch: 5| Step: 9
Training loss: 3.1519694328308105
Validation loss: 2.5710024295314664

Epoch: 5| Step: 10
Training loss: 2.6829514503479004
Validation loss: 2.5718138166653213

Epoch: 144| Step: 0
Training loss: 2.245985746383667
Validation loss: 2.571947779706729

Epoch: 5| Step: 1
Training loss: 2.7823293209075928
Validation loss: 2.570371658571305

Epoch: 5| Step: 2
Training loss: 1.9551531076431274
Validation loss: 2.5737903348861204

Epoch: 5| Step: 3
Training loss: 3.030059337615967
Validation loss: 2.5739830514436126

Epoch: 5| Step: 4
Training loss: 2.832291841506958
Validation loss: 2.5702912346009286

Epoch: 5| Step: 5
Training loss: 3.3323402404785156
Validation loss: 2.577195941760976

Epoch: 5| Step: 6
Training loss: 2.306906223297119
Validation loss: 2.5723998162054245

Epoch: 5| Step: 7
Training loss: 2.743410110473633
Validation loss: 2.5668192217426915

Epoch: 5| Step: 8
Training loss: 2.384092330932617
Validation loss: 2.568771823760002

Epoch: 5| Step: 9
Training loss: 3.589468002319336
Validation loss: 2.5786854579884517

Epoch: 5| Step: 10
Training loss: 2.864473819732666
Validation loss: 2.5666623012993925

Epoch: 145| Step: 0
Training loss: 3.396149158477783
Validation loss: 2.5651020567904235

Epoch: 5| Step: 1
Training loss: 2.4805500507354736
Validation loss: 2.5716009088741836

Epoch: 5| Step: 2
Training loss: 2.328263282775879
Validation loss: 2.5721843601554952

Epoch: 5| Step: 3
Training loss: 3.3423874378204346
Validation loss: 2.572902712770688

Epoch: 5| Step: 4
Training loss: 2.58805251121521
Validation loss: 2.577786022616971

Epoch: 5| Step: 5
Training loss: 2.8663601875305176
Validation loss: 2.572218520666963

Epoch: 5| Step: 6
Training loss: 2.370628833770752
Validation loss: 2.570144881484329

Epoch: 5| Step: 7
Training loss: 2.3957645893096924
Validation loss: 2.5728566979849212

Epoch: 5| Step: 8
Training loss: 1.9916722774505615
Validation loss: 2.5699475785737396

Epoch: 5| Step: 9
Training loss: 3.675873279571533
Validation loss: 2.563959326795352

Epoch: 5| Step: 10
Training loss: 2.5659162998199463
Validation loss: 2.5663901759732153

Epoch: 146| Step: 0
Training loss: 2.5719172954559326
Validation loss: 2.5576054742259364

Epoch: 5| Step: 1
Training loss: 3.599666118621826
Validation loss: 2.562328600114392

Epoch: 5| Step: 2
Training loss: 2.975487470626831
Validation loss: 2.5595721019211637

Epoch: 5| Step: 3
Training loss: 2.4270567893981934
Validation loss: 2.5583717387209655

Epoch: 5| Step: 4
Training loss: 3.52851939201355
Validation loss: 2.5604804895257436

Epoch: 5| Step: 5
Training loss: 2.889878988265991
Validation loss: 2.5595997020762455

Epoch: 5| Step: 6
Training loss: 2.496612548828125
Validation loss: 2.5580012567581667

Epoch: 5| Step: 7
Training loss: 3.082449436187744
Validation loss: 2.5566339979889574

Epoch: 5| Step: 8
Training loss: 1.9725935459136963
Validation loss: 2.560180169279857

Epoch: 5| Step: 9
Training loss: 2.078212022781372
Validation loss: 2.5580288005131546

Epoch: 5| Step: 10
Training loss: 2.3047893047332764
Validation loss: 2.5591243595205326

Epoch: 147| Step: 0
Training loss: 3.1599910259246826
Validation loss: 2.559891257234799

Epoch: 5| Step: 1
Training loss: 2.6708693504333496
Validation loss: 2.562299046465146

Epoch: 5| Step: 2
Training loss: 3.1140458583831787
Validation loss: 2.567171742839198

Epoch: 5| Step: 3
Training loss: 2.398526668548584
Validation loss: 2.5699937317961004

Epoch: 5| Step: 4
Training loss: 2.939138889312744
Validation loss: 2.584268944237822

Epoch: 5| Step: 5
Training loss: 2.826641798019409
Validation loss: 2.5851919087030555

Epoch: 5| Step: 6
Training loss: 2.726331949234009
Validation loss: 2.5867463286205004

Epoch: 5| Step: 7
Training loss: 2.332437515258789
Validation loss: 2.588552792867025

Epoch: 5| Step: 8
Training loss: 2.532221555709839
Validation loss: 2.5784079387623775

Epoch: 5| Step: 9
Training loss: 2.484813690185547
Validation loss: 2.573710831262732

Epoch: 5| Step: 10
Training loss: 2.9201855659484863
Validation loss: 2.5778755731480096

Epoch: 148| Step: 0
Training loss: 3.1177573204040527
Validation loss: 2.5961249515574467

Epoch: 5| Step: 1
Training loss: 2.5704023838043213
Validation loss: 2.5965523027604624

Epoch: 5| Step: 2
Training loss: 2.9770140647888184
Validation loss: 2.600546078015399

Epoch: 5| Step: 3
Training loss: 3.1396923065185547
Validation loss: 2.6023078374965216

Epoch: 5| Step: 4
Training loss: 2.6384880542755127
Validation loss: 2.596532006417551

Epoch: 5| Step: 5
Training loss: 2.883780002593994
Validation loss: 2.590969019038703

Epoch: 5| Step: 6
Training loss: 2.3747951984405518
Validation loss: 2.5727120291802192

Epoch: 5| Step: 7
Training loss: 2.5486230850219727
Validation loss: 2.5633436095330024

Epoch: 5| Step: 8
Training loss: 2.5962188243865967
Validation loss: 2.5653260113090597

Epoch: 5| Step: 9
Training loss: 2.505159616470337
Validation loss: 2.5621460714647846

Epoch: 5| Step: 10
Training loss: 2.7351458072662354
Validation loss: 2.567684124874812

Epoch: 149| Step: 0
Training loss: 2.828677177429199
Validation loss: 2.572539391056184

Epoch: 5| Step: 1
Training loss: 2.6731350421905518
Validation loss: 2.5744053189472487

Epoch: 5| Step: 2
Training loss: 2.3637399673461914
Validation loss: 2.5737479297063683

Epoch: 5| Step: 3
Training loss: 3.055860996246338
Validation loss: 2.565378719760526

Epoch: 5| Step: 4
Training loss: 3.516357898712158
Validation loss: 2.5600445142356296

Epoch: 5| Step: 5
Training loss: 2.3301281929016113
Validation loss: 2.561951473195066

Epoch: 5| Step: 6
Training loss: 3.1257987022399902
Validation loss: 2.5603904749757502

Epoch: 5| Step: 7
Training loss: 2.4281044006347656
Validation loss: 2.5548125364447154

Epoch: 5| Step: 8
Training loss: 2.7252490520477295
Validation loss: 2.5548398827993744

Epoch: 5| Step: 9
Training loss: 2.6793274879455566
Validation loss: 2.555244109963858

Epoch: 5| Step: 10
Training loss: 2.1954851150512695
Validation loss: 2.561832853542861

Epoch: 150| Step: 0
Training loss: 3.150304079055786
Validation loss: 2.5564529588145595

Epoch: 5| Step: 1
Training loss: 2.7129578590393066
Validation loss: 2.555378516515096

Epoch: 5| Step: 2
Training loss: 3.1218433380126953
Validation loss: 2.5549883893741074

Epoch: 5| Step: 3
Training loss: 2.1885933876037598
Validation loss: 2.5653200610991447

Epoch: 5| Step: 4
Training loss: 2.1527180671691895
Validation loss: 2.5683554962117183

Epoch: 5| Step: 5
Training loss: 2.1541576385498047
Validation loss: 2.5720848729533534

Epoch: 5| Step: 6
Training loss: 3.188066005706787
Validation loss: 2.5750717168213217

Epoch: 5| Step: 7
Training loss: 2.406148910522461
Validation loss: 2.5782028423842562

Epoch: 5| Step: 8
Training loss: 3.0027387142181396
Validation loss: 2.582850651074481

Epoch: 5| Step: 9
Training loss: 2.7778096199035645
Validation loss: 2.574764674709689

Epoch: 5| Step: 10
Training loss: 3.2558038234710693
Validation loss: 2.579251999496132

Epoch: 151| Step: 0
Training loss: 2.9433422088623047
Validation loss: 2.5689082376418577

Epoch: 5| Step: 1
Training loss: 2.4712424278259277
Validation loss: 2.5648715983154955

Epoch: 5| Step: 2
Training loss: 2.659635543823242
Validation loss: 2.5596160299034527

Epoch: 5| Step: 3
Training loss: 2.3522400856018066
Validation loss: 2.5611712599313385

Epoch: 5| Step: 4
Training loss: 2.9058170318603516
Validation loss: 2.5521495470436673

Epoch: 5| Step: 5
Training loss: 2.974449872970581
Validation loss: 2.5496547478501514

Epoch: 5| Step: 6
Training loss: 2.6917967796325684
Validation loss: 2.5486891295320246

Epoch: 5| Step: 7
Training loss: 2.7321720123291016
Validation loss: 2.5490474265108825

Epoch: 5| Step: 8
Training loss: 2.3376271724700928
Validation loss: 2.550662702129733

Epoch: 5| Step: 9
Training loss: 2.5975260734558105
Validation loss: 2.5515638397585962

Epoch: 5| Step: 10
Training loss: 3.4008119106292725
Validation loss: 2.546727811136553

Epoch: 152| Step: 0
Training loss: 2.7917916774749756
Validation loss: 2.5539004136157293

Epoch: 5| Step: 1
Training loss: 1.9992892742156982
Validation loss: 2.552422128697877

Epoch: 5| Step: 2
Training loss: 3.3989555835723877
Validation loss: 2.5532789922529653

Epoch: 5| Step: 3
Training loss: 2.591489791870117
Validation loss: 2.5520347190159622

Epoch: 5| Step: 4
Training loss: 2.541870594024658
Validation loss: 2.5517386133952806

Epoch: 5| Step: 5
Training loss: 2.527418375015259
Validation loss: 2.5521165658068914

Epoch: 5| Step: 6
Training loss: 2.269620418548584
Validation loss: 2.550034793474341

Epoch: 5| Step: 7
Training loss: 2.5331015586853027
Validation loss: 2.5547756866742204

Epoch: 5| Step: 8
Training loss: 3.4472603797912598
Validation loss: 2.5561144505777667

Epoch: 5| Step: 9
Training loss: 2.6594278812408447
Validation loss: 2.5536778716630835

Epoch: 5| Step: 10
Training loss: 3.224008798599243
Validation loss: 2.557593153369042

Epoch: 153| Step: 0
Training loss: 2.5625295639038086
Validation loss: 2.5599492852405836

Epoch: 5| Step: 1
Training loss: 2.426438331604004
Validation loss: 2.5609723726908364

Epoch: 5| Step: 2
Training loss: 2.8187355995178223
Validation loss: 2.5576485459522535

Epoch: 5| Step: 3
Training loss: 2.8241398334503174
Validation loss: 2.5579870336799213

Epoch: 5| Step: 4
Training loss: 2.3502070903778076
Validation loss: 2.5542142314295613

Epoch: 5| Step: 5
Training loss: 3.161956310272217
Validation loss: 2.5523313066010833

Epoch: 5| Step: 6
Training loss: 2.551243305206299
Validation loss: 2.549758126658778

Epoch: 5| Step: 7
Training loss: 2.650273084640503
Validation loss: 2.5496712628231255

Epoch: 5| Step: 8
Training loss: 3.007270336151123
Validation loss: 2.545056353333176

Epoch: 5| Step: 9
Training loss: 3.0810599327087402
Validation loss: 2.539861612422492

Epoch: 5| Step: 10
Training loss: 2.4070446491241455
Validation loss: 2.5426690706642727

Epoch: 154| Step: 0
Training loss: 2.342496156692505
Validation loss: 2.542192207869663

Epoch: 5| Step: 1
Training loss: 2.3579816818237305
Validation loss: 2.542320925702331

Epoch: 5| Step: 2
Training loss: 2.7864794731140137
Validation loss: 2.5438110623308408

Epoch: 5| Step: 3
Training loss: 3.112401247024536
Validation loss: 2.543170339317732

Epoch: 5| Step: 4
Training loss: 2.6821162700653076
Validation loss: 2.546625265511133

Epoch: 5| Step: 5
Training loss: 2.920375108718872
Validation loss: 2.543079396729828

Epoch: 5| Step: 6
Training loss: 2.3261020183563232
Validation loss: 2.5444327733849965

Epoch: 5| Step: 7
Training loss: 2.9667985439300537
Validation loss: 2.5431023361862346

Epoch: 5| Step: 8
Training loss: 2.4513440132141113
Validation loss: 2.5401339505308416

Epoch: 5| Step: 9
Training loss: 3.582268238067627
Validation loss: 2.5386748852268344

Epoch: 5| Step: 10
Training loss: 2.2912180423736572
Validation loss: 2.5408307506192114

Epoch: 155| Step: 0
Training loss: 2.8686327934265137
Validation loss: 2.5409514532294324

Epoch: 5| Step: 1
Training loss: 3.2901434898376465
Validation loss: 2.541317073247766

Epoch: 5| Step: 2
Training loss: 3.9369444847106934
Validation loss: 2.5402729921443488

Epoch: 5| Step: 3
Training loss: 2.376976490020752
Validation loss: 2.542222633156725

Epoch: 5| Step: 4
Training loss: 2.5430972576141357
Validation loss: 2.54328070404709

Epoch: 5| Step: 5
Training loss: 2.1512999534606934
Validation loss: 2.5412833357370026

Epoch: 5| Step: 6
Training loss: 2.657649517059326
Validation loss: 2.5405550797780356

Epoch: 5| Step: 7
Training loss: 2.349407196044922
Validation loss: 2.5380038830541793

Epoch: 5| Step: 8
Training loss: 2.743910551071167
Validation loss: 2.537268364301292

Epoch: 5| Step: 9
Training loss: 2.261108875274658
Validation loss: 2.5439720717809533

Epoch: 5| Step: 10
Training loss: 2.670950412750244
Validation loss: 2.54480456536816

Epoch: 156| Step: 0
Training loss: 2.1480562686920166
Validation loss: 2.545943747284592

Epoch: 5| Step: 1
Training loss: 3.323918104171753
Validation loss: 2.5417503874789

Epoch: 5| Step: 2
Training loss: 2.331392765045166
Validation loss: 2.5379469189592587

Epoch: 5| Step: 3
Training loss: 2.5316436290740967
Validation loss: 2.550567776926102

Epoch: 5| Step: 4
Training loss: 2.2306666374206543
Validation loss: 2.542076087767078

Epoch: 5| Step: 5
Training loss: 2.999558925628662
Validation loss: 2.5435228681051605

Epoch: 5| Step: 6
Training loss: 2.855321168899536
Validation loss: 2.5469167386331866

Epoch: 5| Step: 7
Training loss: 3.0867562294006348
Validation loss: 2.5491138581306703

Epoch: 5| Step: 8
Training loss: 2.8840999603271484
Validation loss: 2.5439735484379593

Epoch: 5| Step: 9
Training loss: 2.665174961090088
Validation loss: 2.5503923457155944

Epoch: 5| Step: 10
Training loss: 2.795152425765991
Validation loss: 2.5508639094650105

Epoch: 157| Step: 0
Training loss: 3.0789802074432373
Validation loss: 2.545828519328948

Epoch: 5| Step: 1
Training loss: 2.802339553833008
Validation loss: 2.5455495131913053

Epoch: 5| Step: 2
Training loss: 2.8485922813415527
Validation loss: 2.5478452892713648

Epoch: 5| Step: 3
Training loss: 1.8742568492889404
Validation loss: 2.5477885200131323

Epoch: 5| Step: 4
Training loss: 2.3785784244537354
Validation loss: 2.5399714208418325

Epoch: 5| Step: 5
Training loss: 3.5629241466522217
Validation loss: 2.5445311902671732

Epoch: 5| Step: 6
Training loss: 2.9946963787078857
Validation loss: 2.5430134419471986

Epoch: 5| Step: 7
Training loss: 2.7183520793914795
Validation loss: 2.5393958873646234

Epoch: 5| Step: 8
Training loss: 2.675349712371826
Validation loss: 2.5442047042231404

Epoch: 5| Step: 9
Training loss: 2.6824328899383545
Validation loss: 2.5461691323147027

Epoch: 5| Step: 10
Training loss: 2.068686008453369
Validation loss: 2.5416118509025982

Epoch: 158| Step: 0
Training loss: 2.1979565620422363
Validation loss: 2.5438352682257213

Epoch: 5| Step: 1
Training loss: 3.0961999893188477
Validation loss: 2.5333195117212113

Epoch: 5| Step: 2
Training loss: 3.163586139678955
Validation loss: 2.5392126216683337

Epoch: 5| Step: 3
Training loss: 3.0762875080108643
Validation loss: 2.540309206131966

Epoch: 5| Step: 4
Training loss: 2.564893960952759
Validation loss: 2.5377377284470426

Epoch: 5| Step: 5
Training loss: 2.4472362995147705
Validation loss: 2.5403281757908482

Epoch: 5| Step: 6
Training loss: 2.4715685844421387
Validation loss: 2.544770038256081

Epoch: 5| Step: 7
Training loss: 2.792644500732422
Validation loss: 2.5437281183017197

Epoch: 5| Step: 8
Training loss: 2.3445277214050293
Validation loss: 2.5370769346913984

Epoch: 5| Step: 9
Training loss: 2.601907730102539
Validation loss: 2.550913426183885

Epoch: 5| Step: 10
Training loss: 3.101438522338867
Validation loss: 2.5427017417005313

Epoch: 159| Step: 0
Training loss: 2.2074382305145264
Validation loss: 2.5467792416131623

Epoch: 5| Step: 1
Training loss: 2.9650139808654785
Validation loss: 2.539568437043057

Epoch: 5| Step: 2
Training loss: 2.455026149749756
Validation loss: 2.539221832829137

Epoch: 5| Step: 3
Training loss: 2.7599246501922607
Validation loss: 2.5391750515148206

Epoch: 5| Step: 4
Training loss: 2.086251735687256
Validation loss: 2.5405909220377603

Epoch: 5| Step: 5
Training loss: 3.3283557891845703
Validation loss: 2.546857431370725

Epoch: 5| Step: 6
Training loss: 2.6962714195251465
Validation loss: 2.5467447465465916

Epoch: 5| Step: 7
Training loss: 3.0003409385681152
Validation loss: 2.5439052684332735

Epoch: 5| Step: 8
Training loss: 2.7729554176330566
Validation loss: 2.5524938260355303

Epoch: 5| Step: 9
Training loss: 2.530102252960205
Validation loss: 2.5527493953704834

Epoch: 5| Step: 10
Training loss: 3.0125269889831543
Validation loss: 2.5493622467082035

Epoch: 160| Step: 0
Training loss: 2.631981611251831
Validation loss: 2.5442051195329234

Epoch: 5| Step: 1
Training loss: 2.644376277923584
Validation loss: 2.537766759113599

Epoch: 5| Step: 2
Training loss: 2.9863383769989014
Validation loss: 2.5400884843641713

Epoch: 5| Step: 3
Training loss: 3.0625596046447754
Validation loss: 2.534866384280625

Epoch: 5| Step: 4
Training loss: 2.5945029258728027
Validation loss: 2.5385188646213983

Epoch: 5| Step: 5
Training loss: 2.2287096977233887
Validation loss: 2.5355756590443272

Epoch: 5| Step: 6
Training loss: 2.7129976749420166
Validation loss: 2.5374684795256583

Epoch: 5| Step: 7
Training loss: 2.905545473098755
Validation loss: 2.532379524682158

Epoch: 5| Step: 8
Training loss: 2.4599990844726562
Validation loss: 2.534398727519538

Epoch: 5| Step: 9
Training loss: 2.1801018714904785
Validation loss: 2.5258949597676597

Epoch: 5| Step: 10
Training loss: 3.4550533294677734
Validation loss: 2.530046396358039

Epoch: 161| Step: 0
Training loss: 2.7790980339050293
Validation loss: 2.5292533700184157

Epoch: 5| Step: 1
Training loss: 3.131523847579956
Validation loss: 2.529238203520416

Epoch: 5| Step: 2
Training loss: 3.4162399768829346
Validation loss: 2.5268488801935667

Epoch: 5| Step: 3
Training loss: 2.114323139190674
Validation loss: 2.5291292436661257

Epoch: 5| Step: 4
Training loss: 2.482698917388916
Validation loss: 2.5232738474363923

Epoch: 5| Step: 5
Training loss: 2.8111374378204346
Validation loss: 2.5236329878530195

Epoch: 5| Step: 6
Training loss: 2.991562604904175
Validation loss: 2.5260248555931994

Epoch: 5| Step: 7
Training loss: 2.4367003440856934
Validation loss: 2.5268816999209824

Epoch: 5| Step: 8
Training loss: 2.629817485809326
Validation loss: 2.5254365423674225

Epoch: 5| Step: 9
Training loss: 2.7743992805480957
Validation loss: 2.5233054468708653

Epoch: 5| Step: 10
Training loss: 2.071624279022217
Validation loss: 2.5260816645878617

Epoch: 162| Step: 0
Training loss: 3.86702036857605
Validation loss: 2.523575254665908

Epoch: 5| Step: 1
Training loss: 2.5735812187194824
Validation loss: 2.523870680921821

Epoch: 5| Step: 2
Training loss: 2.446138858795166
Validation loss: 2.529154867254278

Epoch: 5| Step: 3
Training loss: 2.110673427581787
Validation loss: 2.52234637096364

Epoch: 5| Step: 4
Training loss: 3.36710786819458
Validation loss: 2.5266978407418854

Epoch: 5| Step: 5
Training loss: 2.046825647354126
Validation loss: 2.5315864060514714

Epoch: 5| Step: 6
Training loss: 3.0518431663513184
Validation loss: 2.5308913261659685

Epoch: 5| Step: 7
Training loss: 2.6526424884796143
Validation loss: 2.531592215261152

Epoch: 5| Step: 8
Training loss: 2.764627695083618
Validation loss: 2.529983030852451

Epoch: 5| Step: 9
Training loss: 2.225994825363159
Validation loss: 2.531049197719943

Epoch: 5| Step: 10
Training loss: 2.6215360164642334
Validation loss: 2.532634581288984

Epoch: 163| Step: 0
Training loss: 2.657761335372925
Validation loss: 2.5352264245351157

Epoch: 5| Step: 1
Training loss: 2.9621307849884033
Validation loss: 2.5337473320704635

Epoch: 5| Step: 2
Training loss: 1.9316699504852295
Validation loss: 2.52961237712573

Epoch: 5| Step: 3
Training loss: 2.4718284606933594
Validation loss: 2.5342300758566907

Epoch: 5| Step: 4
Training loss: 2.8393421173095703
Validation loss: 2.536563252889982

Epoch: 5| Step: 5
Training loss: 2.143402576446533
Validation loss: 2.5321041922415457

Epoch: 5| Step: 6
Training loss: 2.8797836303710938
Validation loss: 2.53299218352123

Epoch: 5| Step: 7
Training loss: 2.9440195560455322
Validation loss: 2.534730483126897

Epoch: 5| Step: 8
Training loss: 3.118898868560791
Validation loss: 2.5293031046467442

Epoch: 5| Step: 9
Training loss: 2.9405057430267334
Validation loss: 2.5365340312321982

Epoch: 5| Step: 10
Training loss: 2.8136088848114014
Validation loss: 2.5368864203012116

Epoch: 164| Step: 0
Training loss: 3.0365676879882812
Validation loss: 2.5345595805875716

Epoch: 5| Step: 1
Training loss: 2.6176345348358154
Validation loss: 2.535175943887362

Epoch: 5| Step: 2
Training loss: 2.4498703479766846
Validation loss: 2.53083441334386

Epoch: 5| Step: 3
Training loss: 2.5146048069000244
Validation loss: 2.5332795727637505

Epoch: 5| Step: 4
Training loss: 2.6442439556121826
Validation loss: 2.534942765389719

Epoch: 5| Step: 5
Training loss: 2.3703219890594482
Validation loss: 2.533796984662292

Epoch: 5| Step: 6
Training loss: 2.7328009605407715
Validation loss: 2.5278766539789017

Epoch: 5| Step: 7
Training loss: 2.1303763389587402
Validation loss: 2.533854338430589

Epoch: 5| Step: 8
Training loss: 3.037416458129883
Validation loss: 2.5358749871612876

Epoch: 5| Step: 9
Training loss: 2.9990406036376953
Validation loss: 2.5348655305882937

Epoch: 5| Step: 10
Training loss: 3.2484853267669678
Validation loss: 2.5332396312426497

Epoch: 165| Step: 0
Training loss: 2.6372172832489014
Validation loss: 2.5290456151449554

Epoch: 5| Step: 1
Training loss: 3.168757677078247
Validation loss: 2.527597144085874

Epoch: 5| Step: 2
Training loss: 2.499499559402466
Validation loss: 2.5278808339949577

Epoch: 5| Step: 3
Training loss: 2.369258165359497
Validation loss: 2.525619404290312

Epoch: 5| Step: 4
Training loss: 2.0989387035369873
Validation loss: 2.5343978071725495

Epoch: 5| Step: 5
Training loss: 2.5795998573303223
Validation loss: 2.532979021790207

Epoch: 5| Step: 6
Training loss: 2.385079860687256
Validation loss: 2.5273841939946657

Epoch: 5| Step: 7
Training loss: 4.002645969390869
Validation loss: 2.5297646932704474

Epoch: 5| Step: 8
Training loss: 2.911611795425415
Validation loss: 2.5229796671098277

Epoch: 5| Step: 9
Training loss: 2.7087221145629883
Validation loss: 2.527526286340529

Epoch: 5| Step: 10
Training loss: 2.267282724380493
Validation loss: 2.5250190919445408

Epoch: 166| Step: 0
Training loss: 3.006673574447632
Validation loss: 2.525242987499442

Epoch: 5| Step: 1
Training loss: 2.8894684314727783
Validation loss: 2.5209354969762985

Epoch: 5| Step: 2
Training loss: 2.438558578491211
Validation loss: 2.523006013644639

Epoch: 5| Step: 3
Training loss: 2.81939697265625
Validation loss: 2.52243560360324

Epoch: 5| Step: 4
Training loss: 2.85679292678833
Validation loss: 2.5201327082931355

Epoch: 5| Step: 5
Training loss: 2.6879990100860596
Validation loss: 2.5226078751266643

Epoch: 5| Step: 6
Training loss: 2.767747402191162
Validation loss: 2.5234252868160123

Epoch: 5| Step: 7
Training loss: 2.064485788345337
Validation loss: 2.5257446432626374

Epoch: 5| Step: 8
Training loss: 3.1852307319641113
Validation loss: 2.5209799120503087

Epoch: 5| Step: 9
Training loss: 2.583698272705078
Validation loss: 2.521181793623073

Epoch: 5| Step: 10
Training loss: 2.311135768890381
Validation loss: 2.5241092328102357

Epoch: 167| Step: 0
Training loss: 2.606900453567505
Validation loss: 2.523035944149058

Epoch: 5| Step: 1
Training loss: 2.5080883502960205
Validation loss: 2.5285841854669715

Epoch: 5| Step: 2
Training loss: 2.491016387939453
Validation loss: 2.5314265348578013

Epoch: 5| Step: 3
Training loss: 2.4012045860290527
Validation loss: 2.5272643014948857

Epoch: 5| Step: 4
Training loss: 3.037074565887451
Validation loss: 2.534370555672594

Epoch: 5| Step: 5
Training loss: 3.1450867652893066
Validation loss: 2.5383487081014984

Epoch: 5| Step: 6
Training loss: 3.3756561279296875
Validation loss: 2.5330393211815947

Epoch: 5| Step: 7
Training loss: 2.583592414855957
Validation loss: 2.533862477989607

Epoch: 5| Step: 8
Training loss: 2.7089927196502686
Validation loss: 2.5236144758039907

Epoch: 5| Step: 9
Training loss: 2.1025795936584473
Validation loss: 2.5272724782266924

Epoch: 5| Step: 10
Training loss: 2.720862627029419
Validation loss: 2.5279967067062215

Epoch: 168| Step: 0
Training loss: 2.226919651031494
Validation loss: 2.5264080750044955

Epoch: 5| Step: 1
Training loss: 3.2886433601379395
Validation loss: 2.530409807799965

Epoch: 5| Step: 2
Training loss: 2.515326738357544
Validation loss: 2.531338635311332

Epoch: 5| Step: 3
Training loss: 2.1189675331115723
Validation loss: 2.5295116081032702

Epoch: 5| Step: 4
Training loss: 2.832751989364624
Validation loss: 2.533223126524238

Epoch: 5| Step: 5
Training loss: 2.080214023590088
Validation loss: 2.5354614565449376

Epoch: 5| Step: 6
Training loss: 2.9253714084625244
Validation loss: 2.529290210816168

Epoch: 5| Step: 7
Training loss: 2.5708115100860596
Validation loss: 2.5321813193700646

Epoch: 5| Step: 8
Training loss: 2.6943507194519043
Validation loss: 2.5303718684822

Epoch: 5| Step: 9
Training loss: 3.020143508911133
Validation loss: 2.5269919339046685

Epoch: 5| Step: 10
Training loss: 3.4895551204681396
Validation loss: 2.5247771483595653

Epoch: 169| Step: 0
Training loss: 3.4226958751678467
Validation loss: 2.523337669270013

Epoch: 5| Step: 1
Training loss: 2.909736156463623
Validation loss: 2.5261604837192

Epoch: 5| Step: 2
Training loss: 2.9620471000671387
Validation loss: 2.525438929116854

Epoch: 5| Step: 3
Training loss: 1.951873540878296
Validation loss: 2.5262889656969296

Epoch: 5| Step: 4
Training loss: 2.2255027294158936
Validation loss: 2.527093630965038

Epoch: 5| Step: 5
Training loss: 2.809770107269287
Validation loss: 2.528695857653054

Epoch: 5| Step: 6
Training loss: 2.380465030670166
Validation loss: 2.5225480294996694

Epoch: 5| Step: 7
Training loss: 2.7270662784576416
Validation loss: 2.5223781524165982

Epoch: 5| Step: 8
Training loss: 2.8378090858459473
Validation loss: 2.516320595177271

Epoch: 5| Step: 9
Training loss: 3.1108477115631104
Validation loss: 2.5153595734668035

Epoch: 5| Step: 10
Training loss: 2.2369813919067383
Validation loss: 2.5166228560991186

Epoch: 170| Step: 0
Training loss: 2.69413685798645
Validation loss: 2.513888615433888

Epoch: 5| Step: 1
Training loss: 2.6175036430358887
Validation loss: 2.520602763340037

Epoch: 5| Step: 2
Training loss: 2.906276226043701
Validation loss: 2.518402696937643

Epoch: 5| Step: 3
Training loss: 2.4795985221862793
Validation loss: 2.5166593238871586

Epoch: 5| Step: 4
Training loss: 2.43194842338562
Validation loss: 2.5147846770542923

Epoch: 5| Step: 5
Training loss: 2.8511476516723633
Validation loss: 2.5195979226020073

Epoch: 5| Step: 6
Training loss: 2.4319636821746826
Validation loss: 2.523206103232599

Epoch: 5| Step: 7
Training loss: 2.7751002311706543
Validation loss: 2.517646648550546

Epoch: 5| Step: 8
Training loss: 2.881321430206299
Validation loss: 2.5205879877972346

Epoch: 5| Step: 9
Training loss: 2.4138379096984863
Validation loss: 2.5249997518395864

Epoch: 5| Step: 10
Training loss: 3.2422120571136475
Validation loss: 2.5293637860205864

Epoch: 171| Step: 0
Training loss: 2.81815767288208
Validation loss: 2.524955008619575

Epoch: 5| Step: 1
Training loss: 3.109989881515503
Validation loss: 2.5261856125247095

Epoch: 5| Step: 2
Training loss: 2.194052219390869
Validation loss: 2.5252233910304245

Epoch: 5| Step: 3
Training loss: 2.8903069496154785
Validation loss: 2.522363301246397

Epoch: 5| Step: 4
Training loss: 2.4309287071228027
Validation loss: 2.5200239509664555

Epoch: 5| Step: 5
Training loss: 3.229022264480591
Validation loss: 2.5220784115534958

Epoch: 5| Step: 6
Training loss: 2.360851764678955
Validation loss: 2.5278610747347594

Epoch: 5| Step: 7
Training loss: 2.9995250701904297
Validation loss: 2.5323289055978098

Epoch: 5| Step: 8
Training loss: 2.6695399284362793
Validation loss: 2.534515634659798

Epoch: 5| Step: 9
Training loss: 2.412553548812866
Validation loss: 2.538094074495377

Epoch: 5| Step: 10
Training loss: 2.5294008255004883
Validation loss: 2.54122571791372

Epoch: 172| Step: 0
Training loss: 1.8225380182266235
Validation loss: 2.537924825504262

Epoch: 5| Step: 1
Training loss: 1.706215262413025
Validation loss: 2.5345707657516643

Epoch: 5| Step: 2
Training loss: 3.264279842376709
Validation loss: 2.5314861343752955

Epoch: 5| Step: 3
Training loss: 2.347681760787964
Validation loss: 2.528462145918159

Epoch: 5| Step: 4
Training loss: 2.8621456623077393
Validation loss: 2.5294242225667483

Epoch: 5| Step: 5
Training loss: 2.7848193645477295
Validation loss: 2.530832803377541

Epoch: 5| Step: 6
Training loss: 2.4287197589874268
Validation loss: 2.5323181357435

Epoch: 5| Step: 7
Training loss: 3.320596218109131
Validation loss: 2.529237101154943

Epoch: 5| Step: 8
Training loss: 3.38273286819458
Validation loss: 2.5343154835444626

Epoch: 5| Step: 9
Training loss: 2.6422712802886963
Validation loss: 2.537098084726641

Epoch: 5| Step: 10
Training loss: 3.11757230758667
Validation loss: 2.529835216460689

Epoch: 173| Step: 0
Training loss: 3.419304370880127
Validation loss: 2.5254041174406647

Epoch: 5| Step: 1
Training loss: 2.122274398803711
Validation loss: 2.5230654747255388

Epoch: 5| Step: 2
Training loss: 2.4615731239318848
Validation loss: 2.5175416110664286

Epoch: 5| Step: 3
Training loss: 2.892179012298584
Validation loss: 2.5164056619008384

Epoch: 5| Step: 4
Training loss: 3.0509932041168213
Validation loss: 2.50837234527834

Epoch: 5| Step: 5
Training loss: 2.9696013927459717
Validation loss: 2.50649772408188

Epoch: 5| Step: 6
Training loss: 2.895399332046509
Validation loss: 2.508016533749078

Epoch: 5| Step: 7
Training loss: 3.131197690963745
Validation loss: 2.5042151020419214

Epoch: 5| Step: 8
Training loss: 2.6713662147521973
Validation loss: 2.5057307366401917

Epoch: 5| Step: 9
Training loss: 1.933664321899414
Validation loss: 2.5044611525791947

Epoch: 5| Step: 10
Training loss: 1.9953590631484985
Validation loss: 2.5057637101860455

Epoch: 174| Step: 0
Training loss: 2.0374178886413574
Validation loss: 2.506285067527525

Epoch: 5| Step: 1
Training loss: 3.212134599685669
Validation loss: 2.5044502391610095

Epoch: 5| Step: 2
Training loss: 2.712118148803711
Validation loss: 2.5024867314164356

Epoch: 5| Step: 3
Training loss: 4.027097225189209
Validation loss: 2.5054395493640693

Epoch: 5| Step: 4
Training loss: 2.835705280303955
Validation loss: 2.505554727328721

Epoch: 5| Step: 5
Training loss: 2.453636646270752
Validation loss: 2.5040991921578684

Epoch: 5| Step: 6
Training loss: 2.9407410621643066
Validation loss: 2.5038804623388473

Epoch: 5| Step: 7
Training loss: 2.03403639793396
Validation loss: 2.506230031290362

Epoch: 5| Step: 8
Training loss: 2.5680580139160156
Validation loss: 2.5058268347094135

Epoch: 5| Step: 9
Training loss: 1.8875548839569092
Validation loss: 2.505238756056755

Epoch: 5| Step: 10
Training loss: 2.9471287727355957
Validation loss: 2.506943723206879

Epoch: 175| Step: 0
Training loss: 2.6661524772644043
Validation loss: 2.5120054675686743

Epoch: 5| Step: 1
Training loss: 1.9035125970840454
Validation loss: 2.5088731422219226

Epoch: 5| Step: 2
Training loss: 1.8935253620147705
Validation loss: 2.5158132506955053

Epoch: 5| Step: 3
Training loss: 3.359973907470703
Validation loss: 2.51807156942224

Epoch: 5| Step: 4
Training loss: 3.0471065044403076
Validation loss: 2.521102938600766

Epoch: 5| Step: 5
Training loss: 2.63738751411438
Validation loss: 2.516978790683131

Epoch: 5| Step: 6
Training loss: 2.7775990962982178
Validation loss: 2.510062092094011

Epoch: 5| Step: 7
Training loss: 2.737419605255127
Validation loss: 2.507090409596761

Epoch: 5| Step: 8
Training loss: 2.3741867542266846
Validation loss: 2.5061034617885465

Epoch: 5| Step: 9
Training loss: 3.0630364418029785
Validation loss: 2.5042883503821587

Epoch: 5| Step: 10
Training loss: 3.2429046630859375
Validation loss: 2.50587313149565

Epoch: 176| Step: 0
Training loss: 2.263528823852539
Validation loss: 2.5055459878777944

Epoch: 5| Step: 1
Training loss: 2.4690260887145996
Validation loss: 2.507064470680811

Epoch: 5| Step: 2
Training loss: 2.4515440464019775
Validation loss: 2.5106414184775403

Epoch: 5| Step: 3
Training loss: 2.773834705352783
Validation loss: 2.5082028758141304

Epoch: 5| Step: 4
Training loss: 2.8847734928131104
Validation loss: 2.5070111828465618

Epoch: 5| Step: 5
Training loss: 2.859780788421631
Validation loss: 2.5090286065173406

Epoch: 5| Step: 6
Training loss: 2.6711983680725098
Validation loss: 2.5112302200768584

Epoch: 5| Step: 7
Training loss: 2.3677849769592285
Validation loss: 2.513229993081862

Epoch: 5| Step: 8
Training loss: 2.882488489151001
Validation loss: 2.517779081098495

Epoch: 5| Step: 9
Training loss: 3.4515380859375
Validation loss: 2.519308843920308

Epoch: 5| Step: 10
Training loss: 2.51646089553833
Validation loss: 2.5169575086203952

Epoch: 177| Step: 0
Training loss: 2.6985504627227783
Validation loss: 2.5149880224658596

Epoch: 5| Step: 1
Training loss: 2.4790873527526855
Validation loss: 2.5094218894999516

Epoch: 5| Step: 2
Training loss: 2.4541447162628174
Validation loss: 2.515923753861458

Epoch: 5| Step: 3
Training loss: 2.1299471855163574
Validation loss: 2.5137703239276843

Epoch: 5| Step: 4
Training loss: 2.7239718437194824
Validation loss: 2.5162940871331

Epoch: 5| Step: 5
Training loss: 2.521301031112671
Validation loss: 2.517288472062798

Epoch: 5| Step: 6
Training loss: 2.4948155879974365
Validation loss: 2.5178796232387586

Epoch: 5| Step: 7
Training loss: 2.9379048347473145
Validation loss: 2.513803005218506

Epoch: 5| Step: 8
Training loss: 2.6192965507507324
Validation loss: 2.5251302180751676

Epoch: 5| Step: 9
Training loss: 3.703364849090576
Validation loss: 2.5192576275076917

Epoch: 5| Step: 10
Training loss: 2.791069269180298
Validation loss: 2.521138442459927

Epoch: 178| Step: 0
Training loss: 3.4967703819274902
Validation loss: 2.519065903079125

Epoch: 5| Step: 1
Training loss: 3.223097324371338
Validation loss: 2.513436148243566

Epoch: 5| Step: 2
Training loss: 2.9212653636932373
Validation loss: 2.50924878222968

Epoch: 5| Step: 3
Training loss: 2.1865994930267334
Validation loss: 2.505509399598645

Epoch: 5| Step: 4
Training loss: 2.355416774749756
Validation loss: 2.5145773067269275

Epoch: 5| Step: 5
Training loss: 3.190833330154419
Validation loss: 2.513707157104246

Epoch: 5| Step: 6
Training loss: 1.880361557006836
Validation loss: 2.5108500244796916

Epoch: 5| Step: 7
Training loss: 2.6846728324890137
Validation loss: 2.5084233489087833

Epoch: 5| Step: 8
Training loss: 2.959348678588867
Validation loss: 2.508915311546736

Epoch: 5| Step: 9
Training loss: 2.6321396827697754
Validation loss: 2.5128276322477605

Epoch: 5| Step: 10
Training loss: 1.827073097229004
Validation loss: 2.512527904202861

Epoch: 179| Step: 0
Training loss: 2.132728099822998
Validation loss: 2.5199616288626068

Epoch: 5| Step: 1
Training loss: 2.8774688243865967
Validation loss: 2.5264355469775457

Epoch: 5| Step: 2
Training loss: 2.6527392864227295
Validation loss: 2.521115072311894

Epoch: 5| Step: 3
Training loss: 2.6719577312469482
Validation loss: 2.516719454078264

Epoch: 5| Step: 4
Training loss: 2.7698516845703125
Validation loss: 2.515578700650123

Epoch: 5| Step: 5
Training loss: 3.351971387863159
Validation loss: 2.515309695274599

Epoch: 5| Step: 6
Training loss: 2.5137133598327637
Validation loss: 2.5171781098970802

Epoch: 5| Step: 7
Training loss: 1.9149253368377686
Validation loss: 2.5074666905146774

Epoch: 5| Step: 8
Training loss: 3.378495693206787
Validation loss: 2.5087541431509037

Epoch: 5| Step: 9
Training loss: 2.786935806274414
Validation loss: 2.51041216106825

Epoch: 5| Step: 10
Training loss: 2.415463447570801
Validation loss: 2.5113264694008777

Epoch: 180| Step: 0
Training loss: 3.031494617462158
Validation loss: 2.50610758924997

Epoch: 5| Step: 1
Training loss: 2.8264870643615723
Validation loss: 2.505672918852939

Epoch: 5| Step: 2
Training loss: 2.547752857208252
Validation loss: 2.50468619408146

Epoch: 5| Step: 3
Training loss: 2.3174870014190674
Validation loss: 2.5022903129618657

Epoch: 5| Step: 4
Training loss: 2.806602954864502
Validation loss: 2.5086325650574057

Epoch: 5| Step: 5
Training loss: 2.7975945472717285
Validation loss: 2.5046287505857405

Epoch: 5| Step: 6
Training loss: 2.9524292945861816
Validation loss: 2.5025937864857335

Epoch: 5| Step: 7
Training loss: 3.0718746185302734
Validation loss: 2.498180156112999

Epoch: 5| Step: 8
Training loss: 1.9670169353485107
Validation loss: 2.5053596752946095

Epoch: 5| Step: 9
Training loss: 2.35721492767334
Validation loss: 2.5026490252505065

Epoch: 5| Step: 10
Training loss: 2.811927080154419
Validation loss: 2.50403836465651

Epoch: 181| Step: 0
Training loss: 2.4238810539245605
Validation loss: 2.5037362549894597

Epoch: 5| Step: 1
Training loss: 3.186871290206909
Validation loss: 2.5020468542652745

Epoch: 5| Step: 2
Training loss: 2.169511079788208
Validation loss: 2.5059480141567927

Epoch: 5| Step: 3
Training loss: 3.18699312210083
Validation loss: 2.5033075860751572

Epoch: 5| Step: 4
Training loss: 2.6908257007598877
Validation loss: 2.504394356922437

Epoch: 5| Step: 5
Training loss: 2.6668193340301514
Validation loss: 2.509370016795333

Epoch: 5| Step: 6
Training loss: 2.2803008556365967
Validation loss: 2.507376645200996

Epoch: 5| Step: 7
Training loss: 2.845798969268799
Validation loss: 2.509838673376268

Epoch: 5| Step: 8
Training loss: 2.630901336669922
Validation loss: 2.515243140600061

Epoch: 5| Step: 9
Training loss: 2.787497043609619
Validation loss: 2.5117780085532897

Epoch: 5| Step: 10
Training loss: 2.5548040866851807
Validation loss: 2.516684445001746

Epoch: 182| Step: 0
Training loss: 2.8861851692199707
Validation loss: 2.517218633364606

Epoch: 5| Step: 1
Training loss: 3.2190868854522705
Validation loss: 2.5190720968349005

Epoch: 5| Step: 2
Training loss: 2.6750411987304688
Validation loss: 2.518218658303702

Epoch: 5| Step: 3
Training loss: 3.080430269241333
Validation loss: 2.5160347389918503

Epoch: 5| Step: 4
Training loss: 2.7187955379486084
Validation loss: 2.5169812530599613

Epoch: 5| Step: 5
Training loss: 2.5795722007751465
Validation loss: 2.5098724211415937

Epoch: 5| Step: 6
Training loss: 2.8279991149902344
Validation loss: 2.510522743707062

Epoch: 5| Step: 7
Training loss: 2.554183006286621
Validation loss: 2.510383811048282

Epoch: 5| Step: 8
Training loss: 2.6043829917907715
Validation loss: 2.507860455461728

Epoch: 5| Step: 9
Training loss: 1.76027512550354
Validation loss: 2.508412168871972

Epoch: 5| Step: 10
Training loss: 2.5503575801849365
Validation loss: 2.504093218875188

Epoch: 183| Step: 0
Training loss: 2.532197952270508
Validation loss: 2.502107979148947

Epoch: 5| Step: 1
Training loss: 2.6876220703125
Validation loss: 2.507347760661956

Epoch: 5| Step: 2
Training loss: 3.0351080894470215
Validation loss: 2.502601654298844

Epoch: 5| Step: 3
Training loss: 2.741994857788086
Validation loss: 2.5010854915906022

Epoch: 5| Step: 4
Training loss: 2.980696439743042
Validation loss: 2.503258697448238

Epoch: 5| Step: 5
Training loss: 2.6110007762908936
Validation loss: 2.502006556398125

Epoch: 5| Step: 6
Training loss: 2.620720624923706
Validation loss: 2.5028254165444324

Epoch: 5| Step: 7
Training loss: 1.9248911142349243
Validation loss: 2.5002705397144442

Epoch: 5| Step: 8
Training loss: 2.593977212905884
Validation loss: 2.502267542705741

Epoch: 5| Step: 9
Training loss: 2.7270405292510986
Validation loss: 2.5032163768686275

Epoch: 5| Step: 10
Training loss: 2.9794018268585205
Validation loss: 2.5027504967105005

Epoch: 184| Step: 0
Training loss: 2.429359197616577
Validation loss: 2.509442283261207

Epoch: 5| Step: 1
Training loss: 2.642066478729248
Validation loss: 2.510529077181252

Epoch: 5| Step: 2
Training loss: 2.5250320434570312
Validation loss: 2.521511834154847

Epoch: 5| Step: 3
Training loss: 2.7671141624450684
Validation loss: 2.5293059707969747

Epoch: 5| Step: 4
Training loss: 3.1000068187713623
Validation loss: 2.523164769654633

Epoch: 5| Step: 5
Training loss: 3.0093255043029785
Validation loss: 2.5249211839450303

Epoch: 5| Step: 6
Training loss: 2.525463581085205
Validation loss: 2.5240712268378145

Epoch: 5| Step: 7
Training loss: 2.4156653881073
Validation loss: 2.5241511137254777

Epoch: 5| Step: 8
Training loss: 2.03403902053833
Validation loss: 2.532005533095329

Epoch: 5| Step: 9
Training loss: 3.481529951095581
Validation loss: 2.533076799044045

Epoch: 5| Step: 10
Training loss: 2.5053534507751465
Validation loss: 2.5170831372660976

Epoch: 185| Step: 0
Training loss: 2.4168753623962402
Validation loss: 2.521116497696087

Epoch: 5| Step: 1
Training loss: 2.582819700241089
Validation loss: 2.5339847354478735

Epoch: 5| Step: 2
Training loss: 2.6232643127441406
Validation loss: 2.552952920236895

Epoch: 5| Step: 3
Training loss: 2.3007616996765137
Validation loss: 2.5710005657647246

Epoch: 5| Step: 4
Training loss: 2.8650119304656982
Validation loss: 2.5352783600489297

Epoch: 5| Step: 5
Training loss: 3.368396759033203
Validation loss: 2.511999814741073

Epoch: 5| Step: 6
Training loss: 2.09639310836792
Validation loss: 2.5017845681918565

Epoch: 5| Step: 7
Training loss: 2.830195903778076
Validation loss: 2.504799891543645

Epoch: 5| Step: 8
Training loss: 2.591172456741333
Validation loss: 2.516775010734476

Epoch: 5| Step: 9
Training loss: 2.537712574005127
Validation loss: 2.5282589927796395

Epoch: 5| Step: 10
Training loss: 3.4799184799194336
Validation loss: 2.5284595540774766

Epoch: 186| Step: 0
Training loss: 2.428340435028076
Validation loss: 2.5283103527561313

Epoch: 5| Step: 1
Training loss: 3.310004472732544
Validation loss: 2.515925886810467

Epoch: 5| Step: 2
Training loss: 2.9486706256866455
Validation loss: 2.5157615036092777

Epoch: 5| Step: 3
Training loss: 2.6832213401794434
Validation loss: 2.510461179159021

Epoch: 5| Step: 4
Training loss: 2.2906014919281006
Validation loss: 2.509205328520908

Epoch: 5| Step: 5
Training loss: 2.3594071865081787
Validation loss: 2.5006708996270293

Epoch: 5| Step: 6
Training loss: 2.334869861602783
Validation loss: 2.498262523322977

Epoch: 5| Step: 7
Training loss: 2.414638042449951
Validation loss: 2.4953463667182514

Epoch: 5| Step: 8
Training loss: 2.157494068145752
Validation loss: 2.4922237883331957

Epoch: 5| Step: 9
Training loss: 3.4187164306640625
Validation loss: 2.4912292213850122

Epoch: 5| Step: 10
Training loss: 3.2704014778137207
Validation loss: 2.490342911853585

Epoch: 187| Step: 0
Training loss: 2.816924571990967
Validation loss: 2.4935256409388717

Epoch: 5| Step: 1
Training loss: 2.2576775550842285
Validation loss: 2.493283100025628

Epoch: 5| Step: 2
Training loss: 2.5205166339874268
Validation loss: 2.494620289853824

Epoch: 5| Step: 3
Training loss: 3.1209843158721924
Validation loss: 2.4954573159576743

Epoch: 5| Step: 4
Training loss: 2.8100297451019287
Validation loss: 2.4944371818214335

Epoch: 5| Step: 5
Training loss: 2.4799587726593018
Validation loss: 2.4947977322404102

Epoch: 5| Step: 6
Training loss: 2.266281843185425
Validation loss: 2.4959569746448147

Epoch: 5| Step: 7
Training loss: 2.7731058597564697
Validation loss: 2.492262706961683

Epoch: 5| Step: 8
Training loss: 2.8640952110290527
Validation loss: 2.500874807757716

Epoch: 5| Step: 9
Training loss: 3.042267322540283
Validation loss: 2.5038587508663053

Epoch: 5| Step: 10
Training loss: 2.466918468475342
Validation loss: 2.50172015928453

Epoch: 188| Step: 0
Training loss: 3.3831863403320312
Validation loss: 2.503150463104248

Epoch: 5| Step: 1
Training loss: 2.9604828357696533
Validation loss: 2.4982426294716458

Epoch: 5| Step: 2
Training loss: 3.4521446228027344
Validation loss: 2.4965070780887397

Epoch: 5| Step: 3
Training loss: 2.5303738117218018
Validation loss: 2.5034187532240346

Epoch: 5| Step: 4
Training loss: 2.226346969604492
Validation loss: 2.495008437864242

Epoch: 5| Step: 5
Training loss: 2.3814592361450195
Validation loss: 2.498336689446562

Epoch: 5| Step: 6
Training loss: 2.0465312004089355
Validation loss: 2.5021628564403904

Epoch: 5| Step: 7
Training loss: 2.1589226722717285
Validation loss: 2.5025069585410495

Epoch: 5| Step: 8
Training loss: 2.9147958755493164
Validation loss: 2.5018441959093978

Epoch: 5| Step: 9
Training loss: 2.934418201446533
Validation loss: 2.505813693487516

Epoch: 5| Step: 10
Training loss: 2.299886465072632
Validation loss: 2.5072758761785363

Epoch: 189| Step: 0
Training loss: 3.2910377979278564
Validation loss: 2.503407637278239

Epoch: 5| Step: 1
Training loss: 3.0130815505981445
Validation loss: 2.5057444995449436

Epoch: 5| Step: 2
Training loss: 2.4437851905822754
Validation loss: 2.5097171619374263

Epoch: 5| Step: 3
Training loss: 2.3952457904815674
Validation loss: 2.504203822023125

Epoch: 5| Step: 4
Training loss: 2.204860210418701
Validation loss: 2.5027658554815475

Epoch: 5| Step: 5
Training loss: 3.05839467048645
Validation loss: 2.5035725434621177

Epoch: 5| Step: 6
Training loss: 2.8127689361572266
Validation loss: 2.504966743530766

Epoch: 5| Step: 7
Training loss: 2.1087355613708496
Validation loss: 2.509854042401878

Epoch: 5| Step: 8
Training loss: 2.294222593307495
Validation loss: 2.5129951379632436

Epoch: 5| Step: 9
Training loss: 2.38966703414917
Validation loss: 2.5084285979629843

Epoch: 5| Step: 10
Training loss: 3.4460740089416504
Validation loss: 2.50511275311952

Epoch: 190| Step: 0
Training loss: 2.452003002166748
Validation loss: 2.5102523142291653

Epoch: 5| Step: 1
Training loss: 2.8519644737243652
Validation loss: 2.5043805811994817

Epoch: 5| Step: 2
Training loss: 2.915184736251831
Validation loss: 2.49876320233909

Epoch: 5| Step: 3
Training loss: 2.4027860164642334
Validation loss: 2.5026783430448143

Epoch: 5| Step: 4
Training loss: 2.8410837650299072
Validation loss: 2.5110660394032798

Epoch: 5| Step: 5
Training loss: 2.4158685207366943
Validation loss: 2.507434142533169

Epoch: 5| Step: 6
Training loss: 3.3623785972595215
Validation loss: 2.5062452362429712

Epoch: 5| Step: 7
Training loss: 2.0689404010772705
Validation loss: 2.5019939586680424

Epoch: 5| Step: 8
Training loss: 2.817661762237549
Validation loss: 2.500449590785529

Epoch: 5| Step: 9
Training loss: 2.7317402362823486
Validation loss: 2.498553476025981

Epoch: 5| Step: 10
Training loss: 2.581164836883545
Validation loss: 2.4975048367695143

Epoch: 191| Step: 0
Training loss: 2.128628969192505
Validation loss: 2.4921111573455152

Epoch: 5| Step: 1
Training loss: 3.7044663429260254
Validation loss: 2.489334685828096

Epoch: 5| Step: 2
Training loss: 2.635694980621338
Validation loss: 2.4893989870625157

Epoch: 5| Step: 3
Training loss: 2.984548568725586
Validation loss: 2.4895558946876117

Epoch: 5| Step: 4
Training loss: 2.4147214889526367
Validation loss: 2.4869982068256666

Epoch: 5| Step: 5
Training loss: 2.5294885635375977
Validation loss: 2.488123232318509

Epoch: 5| Step: 6
Training loss: 2.0564613342285156
Validation loss: 2.483694758466495

Epoch: 5| Step: 7
Training loss: 2.2777395248413086
Validation loss: 2.483041804323914

Epoch: 5| Step: 8
Training loss: 3.1332969665527344
Validation loss: 2.491292094671598

Epoch: 5| Step: 9
Training loss: 2.998709201812744
Validation loss: 2.490075793317569

Epoch: 5| Step: 10
Training loss: 2.4232325553894043
Validation loss: 2.4894773165384927

Epoch: 192| Step: 0
Training loss: 2.022068977355957
Validation loss: 2.496099820701025

Epoch: 5| Step: 1
Training loss: 3.5507168769836426
Validation loss: 2.4925134643431632

Epoch: 5| Step: 2
Training loss: 3.264694929122925
Validation loss: 2.490921525545018

Epoch: 5| Step: 3
Training loss: 2.969521999359131
Validation loss: 2.4957034510950886

Epoch: 5| Step: 4
Training loss: 1.9263532161712646
Validation loss: 2.493736490126579

Epoch: 5| Step: 5
Training loss: 2.8363871574401855
Validation loss: 2.4955837521501767

Epoch: 5| Step: 6
Training loss: 2.627072811126709
Validation loss: 2.493126864074379

Epoch: 5| Step: 7
Training loss: 2.3442234992980957
Validation loss: 2.4903138555506223

Epoch: 5| Step: 8
Training loss: 2.1537375450134277
Validation loss: 2.491482303988549

Epoch: 5| Step: 9
Training loss: 2.6167831420898438
Validation loss: 2.4973436811918854

Epoch: 5| Step: 10
Training loss: 3.071774959564209
Validation loss: 2.4992322793570896

Epoch: 193| Step: 0
Training loss: 2.824126720428467
Validation loss: 2.499158743889101

Epoch: 5| Step: 1
Training loss: 2.7651684284210205
Validation loss: 2.4968974615937922

Epoch: 5| Step: 2
Training loss: 2.1946301460266113
Validation loss: 2.499751147403512

Epoch: 5| Step: 3
Training loss: 2.7982311248779297
Validation loss: 2.496492526864493

Epoch: 5| Step: 4
Training loss: 1.6540911197662354
Validation loss: 2.4988584390250583

Epoch: 5| Step: 5
Training loss: 2.6470463275909424
Validation loss: 2.501176054759692

Epoch: 5| Step: 6
Training loss: 3.1387364864349365
Validation loss: 2.5012785209122526

Epoch: 5| Step: 7
Training loss: 2.5450963973999023
Validation loss: 2.5091932230098273

Epoch: 5| Step: 8
Training loss: 3.1418099403381348
Validation loss: 2.507666728829825

Epoch: 5| Step: 9
Training loss: 2.6212401390075684
Validation loss: 2.5058169236747165

Epoch: 5| Step: 10
Training loss: 3.029054880142212
Validation loss: 2.515854030527094

Epoch: 194| Step: 0
Training loss: 2.5478081703186035
Validation loss: 2.506938383143435

Epoch: 5| Step: 1
Training loss: 3.0812108516693115
Validation loss: 2.5016527791177072

Epoch: 5| Step: 2
Training loss: 2.5085463523864746
Validation loss: 2.4934313886909076

Epoch: 5| Step: 3
Training loss: 2.650869131088257
Validation loss: 2.499576214821108

Epoch: 5| Step: 4
Training loss: 1.8430455923080444
Validation loss: 2.4977416479459373

Epoch: 5| Step: 5
Training loss: 2.3588764667510986
Validation loss: 2.4979123659031366

Epoch: 5| Step: 6
Training loss: 3.317328929901123
Validation loss: 2.496467249367827

Epoch: 5| Step: 7
Training loss: 3.2915987968444824
Validation loss: 2.4962886482156734

Epoch: 5| Step: 8
Training loss: 2.6705851554870605
Validation loss: 2.4914556216168147

Epoch: 5| Step: 9
Training loss: 2.31998610496521
Validation loss: 2.4958853452436385

Epoch: 5| Step: 10
Training loss: 2.7004811763763428
Validation loss: 2.4990996109542025

Epoch: 195| Step: 0
Training loss: 2.6702938079833984
Validation loss: 2.498230731615456

Epoch: 5| Step: 1
Training loss: 2.907933473587036
Validation loss: 2.4901737166989233

Epoch: 5| Step: 2
Training loss: 2.9511733055114746
Validation loss: 2.491616943831085

Epoch: 5| Step: 3
Training loss: 2.395139694213867
Validation loss: 2.4925946984239804

Epoch: 5| Step: 4
Training loss: 2.9429526329040527
Validation loss: 2.489555166613671

Epoch: 5| Step: 5
Training loss: 2.625734806060791
Validation loss: 2.488678898862613

Epoch: 5| Step: 6
Training loss: 2.865788459777832
Validation loss: 2.494140548090781

Epoch: 5| Step: 7
Training loss: 2.573213815689087
Validation loss: 2.4948032440677768

Epoch: 5| Step: 8
Training loss: 2.548185110092163
Validation loss: 2.5024915997700026

Epoch: 5| Step: 9
Training loss: 2.284609079360962
Validation loss: 2.4974256023283927

Epoch: 5| Step: 10
Training loss: 2.4301626682281494
Validation loss: 2.5001542234933503

Epoch: 196| Step: 0
Training loss: 2.010073184967041
Validation loss: 2.503362532584898

Epoch: 5| Step: 1
Training loss: 2.73901104927063
Validation loss: 2.5071036277278775

Epoch: 5| Step: 2
Training loss: 2.5901355743408203
Validation loss: 2.511685694417646

Epoch: 5| Step: 3
Training loss: 2.3723886013031006
Validation loss: 2.5023621025905816

Epoch: 5| Step: 4
Training loss: 2.7030749320983887
Validation loss: 2.505170960580149

Epoch: 5| Step: 5
Training loss: 1.8294360637664795
Validation loss: 2.5051501720182356

Epoch: 5| Step: 6
Training loss: 3.007944107055664
Validation loss: 2.5081469166663384

Epoch: 5| Step: 7
Training loss: 2.7892773151397705
Validation loss: 2.511507882866808

Epoch: 5| Step: 8
Training loss: 3.1632957458496094
Validation loss: 2.5084135814379622

Epoch: 5| Step: 9
Training loss: 2.967693328857422
Validation loss: 2.5069570502927228

Epoch: 5| Step: 10
Training loss: 3.2905116081237793
Validation loss: 2.5089632516266196

Epoch: 197| Step: 0
Training loss: 2.2255101203918457
Validation loss: 2.5036190504668863

Epoch: 5| Step: 1
Training loss: 2.459179401397705
Validation loss: 2.499208419553695

Epoch: 5| Step: 2
Training loss: 1.975197196006775
Validation loss: 2.4982736341414915

Epoch: 5| Step: 3
Training loss: 2.5772736072540283
Validation loss: 2.4935423199848463

Epoch: 5| Step: 4
Training loss: 3.0726542472839355
Validation loss: 2.4908640487219698

Epoch: 5| Step: 5
Training loss: 3.265977144241333
Validation loss: 2.4959010244697653

Epoch: 5| Step: 6
Training loss: 2.753248929977417
Validation loss: 2.4909924768632457

Epoch: 5| Step: 7
Training loss: 2.406346082687378
Validation loss: 2.485339505698091

Epoch: 5| Step: 8
Training loss: 2.758242130279541
Validation loss: 2.4845232502106698

Epoch: 5| Step: 9
Training loss: 3.2864558696746826
Validation loss: 2.4824864095257175

Epoch: 5| Step: 10
Training loss: 2.4733052253723145
Validation loss: 2.4819754246742494

Epoch: 198| Step: 0
Training loss: 2.690325975418091
Validation loss: 2.4821811927262174

Epoch: 5| Step: 1
Training loss: 3.222980499267578
Validation loss: 2.4900701033171786

Epoch: 5| Step: 2
Training loss: 3.004756450653076
Validation loss: 2.4876598927282516

Epoch: 5| Step: 3
Training loss: 2.175778865814209
Validation loss: 2.4951561086921283

Epoch: 5| Step: 4
Training loss: 2.626282215118408
Validation loss: 2.4861638289625927

Epoch: 5| Step: 5
Training loss: 2.96024751663208
Validation loss: 2.4859104169312345

Epoch: 5| Step: 6
Training loss: 2.6985714435577393
Validation loss: 2.4867115687298518

Epoch: 5| Step: 7
Training loss: 2.871530771255493
Validation loss: 2.4835762336689937

Epoch: 5| Step: 8
Training loss: 2.0456478595733643
Validation loss: 2.483946233667353

Epoch: 5| Step: 9
Training loss: 2.3720791339874268
Validation loss: 2.482000127915413

Epoch: 5| Step: 10
Training loss: 2.577791213989258
Validation loss: 2.478187544371492

Epoch: 199| Step: 0
Training loss: 2.843783140182495
Validation loss: 2.481093192613253

Epoch: 5| Step: 1
Training loss: 2.1312930583953857
Validation loss: 2.4813167664312545

Epoch: 5| Step: 2
Training loss: 2.902834415435791
Validation loss: 2.489060776208037

Epoch: 5| Step: 3
Training loss: 3.0955810546875
Validation loss: 2.4890575152571484

Epoch: 5| Step: 4
Training loss: 2.184483051300049
Validation loss: 2.488861007075156

Epoch: 5| Step: 5
Training loss: 3.029404878616333
Validation loss: 2.4935608653612036

Epoch: 5| Step: 6
Training loss: 3.12440824508667
Validation loss: 2.4924060631823797

Epoch: 5| Step: 7
Training loss: 2.780195713043213
Validation loss: 2.4980138424904115

Epoch: 5| Step: 8
Training loss: 1.8689581155776978
Validation loss: 2.501531395860898

Epoch: 5| Step: 9
Training loss: 2.7404673099517822
Validation loss: 2.5010625085523053

Epoch: 5| Step: 10
Training loss: 2.5715677738189697
Validation loss: 2.4957601895896335

Epoch: 200| Step: 0
Training loss: 2.52956223487854
Validation loss: 2.5091619209576677

Epoch: 5| Step: 1
Training loss: 2.18347430229187
Validation loss: 2.4952005365843415

Epoch: 5| Step: 2
Training loss: 2.744654417037964
Validation loss: 2.4968029299089984

Epoch: 5| Step: 3
Training loss: 2.290696382522583
Validation loss: 2.496520780747937

Epoch: 5| Step: 4
Training loss: 2.6587626934051514
Validation loss: 2.4992630661174817

Epoch: 5| Step: 5
Training loss: 1.805686354637146
Validation loss: 2.502845279632076

Epoch: 5| Step: 6
Training loss: 3.364239454269409
Validation loss: 2.5044505775615735

Epoch: 5| Step: 7
Training loss: 3.1909899711608887
Validation loss: 2.5089464315804104

Epoch: 5| Step: 8
Training loss: 3.233240842819214
Validation loss: 2.5017445395069737

Epoch: 5| Step: 9
Training loss: 2.4081192016601562
Validation loss: 2.5061465130057385

Epoch: 5| Step: 10
Training loss: 2.9211268424987793
Validation loss: 2.498600052249047

Testing loss: 2.6232424312167697

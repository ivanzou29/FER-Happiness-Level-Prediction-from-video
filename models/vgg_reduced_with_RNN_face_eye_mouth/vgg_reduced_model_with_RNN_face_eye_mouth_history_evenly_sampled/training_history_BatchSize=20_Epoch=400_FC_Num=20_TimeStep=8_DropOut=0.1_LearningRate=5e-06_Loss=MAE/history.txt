Epoch: 1| Step: 0
Training loss: 4.907534599304199
Validation loss: 5.243682651109593

Epoch: 5| Step: 1
Training loss: 4.92900276184082
Validation loss: 5.2393233442819245

Epoch: 5| Step: 2
Training loss: 4.692010402679443
Validation loss: 5.23497187706732

Epoch: 5| Step: 3
Training loss: 4.022729396820068
Validation loss: 5.23028015834029

Epoch: 5| Step: 4
Training loss: 4.794393062591553
Validation loss: 5.225374478165821

Epoch: 5| Step: 5
Training loss: 5.2611894607543945
Validation loss: 5.220599415481732

Epoch: 5| Step: 6
Training loss: 4.23896598815918
Validation loss: 5.21535567314394

Epoch: 5| Step: 7
Training loss: 5.836958885192871
Validation loss: 5.210187927369149

Epoch: 5| Step: 8
Training loss: 6.197864532470703
Validation loss: 5.204381096747614

Epoch: 5| Step: 9
Training loss: 4.601799964904785
Validation loss: 5.198539867196032

Epoch: 5| Step: 10
Training loss: 5.782186031341553
Validation loss: 5.192661782746674

Epoch: 2| Step: 0
Training loss: 5.387719631195068
Validation loss: 5.186556739191855

Epoch: 5| Step: 1
Training loss: 4.572073936462402
Validation loss: 5.17935505220967

Epoch: 5| Step: 2
Training loss: 5.6555681228637695
Validation loss: 5.1725011128251275

Epoch: 5| Step: 3
Training loss: 4.812716484069824
Validation loss: 5.165311931281962

Epoch: 5| Step: 4
Training loss: 4.7229132652282715
Validation loss: 5.157752390830748

Epoch: 5| Step: 5
Training loss: 5.955506324768066
Validation loss: 5.149996167869978

Epoch: 5| Step: 6
Training loss: 4.640512943267822
Validation loss: 5.141638694270965

Epoch: 5| Step: 7
Training loss: 4.378308296203613
Validation loss: 5.133333370249758

Epoch: 5| Step: 8
Training loss: 4.896047115325928
Validation loss: 5.123921814785208

Epoch: 5| Step: 9
Training loss: 3.4216036796569824
Validation loss: 5.114396566985755

Epoch: 5| Step: 10
Training loss: 6.085579872131348
Validation loss: 5.104493325756442

Epoch: 3| Step: 0
Training loss: 4.777285099029541
Validation loss: 5.094557428872713

Epoch: 5| Step: 1
Training loss: 4.188611030578613
Validation loss: 5.083137548098001

Epoch: 5| Step: 2
Training loss: 6.154388427734375
Validation loss: 5.071883750218217

Epoch: 5| Step: 3
Training loss: 5.834874153137207
Validation loss: 5.059464372614379

Epoch: 5| Step: 4
Training loss: 4.596749305725098
Validation loss: 5.046776417763002

Epoch: 5| Step: 5
Training loss: 4.967025279998779
Validation loss: 5.0335207600747385

Epoch: 5| Step: 6
Training loss: 5.2397661209106445
Validation loss: 5.019287499048376

Epoch: 5| Step: 7
Training loss: 4.07167911529541
Validation loss: 5.0052843862964265

Epoch: 5| Step: 8
Training loss: 4.336878776550293
Validation loss: 4.989774462997272

Epoch: 5| Step: 9
Training loss: 3.878812313079834
Validation loss: 4.973224947529454

Epoch: 5| Step: 10
Training loss: 5.0672125816345215
Validation loss: 4.95727260651127

Epoch: 4| Step: 0
Training loss: 4.619053840637207
Validation loss: 4.940431738412508

Epoch: 5| Step: 1
Training loss: 3.4467482566833496
Validation loss: 4.922380488405945

Epoch: 5| Step: 2
Training loss: 4.685896396636963
Validation loss: 4.905027235707929

Epoch: 5| Step: 3
Training loss: 3.728217363357544
Validation loss: 4.884770178025769

Epoch: 5| Step: 4
Training loss: 5.171841621398926
Validation loss: 4.8654637182912515

Epoch: 5| Step: 5
Training loss: 4.7989301681518555
Validation loss: 4.84443724283608

Epoch: 5| Step: 6
Training loss: 5.254205226898193
Validation loss: 4.821900567700786

Epoch: 5| Step: 7
Training loss: 4.547407150268555
Validation loss: 4.800088600445819

Epoch: 5| Step: 8
Training loss: 4.77295446395874
Validation loss: 4.776142361343548

Epoch: 5| Step: 9
Training loss: 4.466029167175293
Validation loss: 4.753233253314931

Epoch: 5| Step: 10
Training loss: 5.533505439758301
Validation loss: 4.729954301670033

Epoch: 5| Step: 0
Training loss: 4.491865634918213
Validation loss: 4.703896978850006

Epoch: 5| Step: 1
Training loss: 5.220106601715088
Validation loss: 4.679263899403233

Epoch: 5| Step: 2
Training loss: 4.09202766418457
Validation loss: 4.655117829640706

Epoch: 5| Step: 3
Training loss: 4.507670879364014
Validation loss: 4.62920190441993

Epoch: 5| Step: 4
Training loss: 3.2963709831237793
Validation loss: 4.602846863449261

Epoch: 5| Step: 5
Training loss: 5.88070011138916
Validation loss: 4.576350119806105

Epoch: 5| Step: 6
Training loss: 4.258128643035889
Validation loss: 4.551377670739287

Epoch: 5| Step: 7
Training loss: 4.609160423278809
Validation loss: 4.52339231839744

Epoch: 5| Step: 8
Training loss: 4.273369789123535
Validation loss: 4.498384944854244

Epoch: 5| Step: 9
Training loss: 3.806506395339966
Validation loss: 4.471172076399609

Epoch: 5| Step: 10
Training loss: 3.371105909347534
Validation loss: 4.444221881128127

Epoch: 6| Step: 0
Training loss: 3.295091152191162
Validation loss: 4.417441316830215

Epoch: 5| Step: 1
Training loss: 3.6972038745880127
Validation loss: 4.395249546215099

Epoch: 5| Step: 2
Training loss: 4.943222999572754
Validation loss: 4.369091792773175

Epoch: 5| Step: 3
Training loss: 4.345310211181641
Validation loss: 4.343436307804559

Epoch: 5| Step: 4
Training loss: 5.19729471206665
Validation loss: 4.318550063717749

Epoch: 5| Step: 5
Training loss: 4.610353946685791
Validation loss: 4.294093219182825

Epoch: 5| Step: 6
Training loss: 2.919891119003296
Validation loss: 4.268603335144699

Epoch: 5| Step: 7
Training loss: 3.947728395462036
Validation loss: 4.2436726785475205

Epoch: 5| Step: 8
Training loss: 4.653458595275879
Validation loss: 4.217697205082063

Epoch: 5| Step: 9
Training loss: 4.229367256164551
Validation loss: 4.1899684552223455

Epoch: 5| Step: 10
Training loss: 3.0305051803588867
Validation loss: 4.161407844994658

Epoch: 7| Step: 0
Training loss: 4.419591426849365
Validation loss: 4.132775314392582

Epoch: 5| Step: 1
Training loss: 3.5047736167907715
Validation loss: 4.106120899159421

Epoch: 5| Step: 2
Training loss: 3.4737792015075684
Validation loss: 4.079740544801117

Epoch: 5| Step: 3
Training loss: 3.7452495098114014
Validation loss: 4.054009283742597

Epoch: 5| Step: 4
Training loss: 3.833662748336792
Validation loss: 4.029365575441751

Epoch: 5| Step: 5
Training loss: 3.77258563041687
Validation loss: 4.005021044003066

Epoch: 5| Step: 6
Training loss: 4.6753106117248535
Validation loss: 3.9814324378967285

Epoch: 5| Step: 7
Training loss: 3.140738010406494
Validation loss: 3.9592187584087415

Epoch: 5| Step: 8
Training loss: 2.9913954734802246
Validation loss: 3.9389149040304203

Epoch: 5| Step: 9
Training loss: 4.646241664886475
Validation loss: 3.9189521779296217

Epoch: 5| Step: 10
Training loss: 4.215216636657715
Validation loss: 3.8988944791978404

Epoch: 8| Step: 0
Training loss: 3.9335217475891113
Validation loss: 3.881514641546434

Epoch: 5| Step: 1
Training loss: 3.9324772357940674
Validation loss: 3.8636817675764843

Epoch: 5| Step: 2
Training loss: 4.096987247467041
Validation loss: 3.8459373520266626

Epoch: 5| Step: 3
Training loss: 3.9313342571258545
Validation loss: 3.8298336023925454

Epoch: 5| Step: 4
Training loss: 3.8498237133026123
Validation loss: 3.812867308175692

Epoch: 5| Step: 5
Training loss: 2.940852165222168
Validation loss: 3.799500403865691

Epoch: 5| Step: 6
Training loss: 3.106820583343506
Validation loss: 3.7857437800335627

Epoch: 5| Step: 7
Training loss: 3.6405417919158936
Validation loss: 3.772081600722446

Epoch: 5| Step: 8
Training loss: 3.709158420562744
Validation loss: 3.7613136922159502

Epoch: 5| Step: 9
Training loss: 4.027064323425293
Validation loss: 3.748825744916034

Epoch: 5| Step: 10
Training loss: 3.3328588008880615
Validation loss: 3.7385654500735703

Epoch: 9| Step: 0
Training loss: 4.0398054122924805
Validation loss: 3.727137686103903

Epoch: 5| Step: 1
Training loss: 3.4733119010925293
Validation loss: 3.7164254855084162

Epoch: 5| Step: 2
Training loss: 4.3345818519592285
Validation loss: 3.706094552111882

Epoch: 5| Step: 3
Training loss: 4.223462104797363
Validation loss: 3.6961604190129105

Epoch: 5| Step: 4
Training loss: 2.709211826324463
Validation loss: 3.6874362396937546

Epoch: 5| Step: 5
Training loss: 3.5214271545410156
Validation loss: 3.676480024091659

Epoch: 5| Step: 6
Training loss: 4.667388916015625
Validation loss: 3.6676965221281974

Epoch: 5| Step: 7
Training loss: 2.671895980834961
Validation loss: 3.6580140872668196

Epoch: 5| Step: 8
Training loss: 3.1737351417541504
Validation loss: 3.6491462799810592

Epoch: 5| Step: 9
Training loss: 2.8892669677734375
Validation loss: 3.6415048953025573

Epoch: 5| Step: 10
Training loss: 3.812962293624878
Validation loss: 3.6335026064226703

Epoch: 10| Step: 0
Training loss: 3.5860488414764404
Validation loss: 3.6259133482492096

Epoch: 5| Step: 1
Training loss: 2.3820736408233643
Validation loss: 3.6194385764419392

Epoch: 5| Step: 2
Training loss: 4.055424690246582
Validation loss: 3.611563774847215

Epoch: 5| Step: 3
Training loss: 3.7000956535339355
Validation loss: 3.6050505945759435

Epoch: 5| Step: 4
Training loss: 3.2926712036132812
Validation loss: 3.5995148381879254

Epoch: 5| Step: 5
Training loss: 3.7871246337890625
Validation loss: 3.5924173580702914

Epoch: 5| Step: 6
Training loss: 3.775047779083252
Validation loss: 3.585409031119398

Epoch: 5| Step: 7
Training loss: 4.093340873718262
Validation loss: 3.577703352897398

Epoch: 5| Step: 8
Training loss: 3.2570838928222656
Validation loss: 3.5713245535409577

Epoch: 5| Step: 9
Training loss: 2.953354597091675
Validation loss: 3.563764210670225

Epoch: 5| Step: 10
Training loss: 3.77359676361084
Validation loss: 3.559572345467024

Epoch: 11| Step: 0
Training loss: 4.056440353393555
Validation loss: 3.551263327239662

Epoch: 5| Step: 1
Training loss: 2.632650375366211
Validation loss: 3.5471181843870427

Epoch: 5| Step: 2
Training loss: 3.7852134704589844
Validation loss: 3.5410027734694944

Epoch: 5| Step: 3
Training loss: 3.2394137382507324
Validation loss: 3.5339849174663587

Epoch: 5| Step: 4
Training loss: 3.642796039581299
Validation loss: 3.5268948949793333

Epoch: 5| Step: 5
Training loss: 3.685314178466797
Validation loss: 3.524555488299298

Epoch: 5| Step: 6
Training loss: 3.5425057411193848
Validation loss: 3.516958313603555

Epoch: 5| Step: 7
Training loss: 3.782299041748047
Validation loss: 3.510455669895295

Epoch: 5| Step: 8
Training loss: 3.322592258453369
Validation loss: 3.5050883934062016

Epoch: 5| Step: 9
Training loss: 2.693472146987915
Validation loss: 3.50027290467293

Epoch: 5| Step: 10
Training loss: 3.63216233253479
Validation loss: 3.492036232384302

Epoch: 12| Step: 0
Training loss: 4.123085975646973
Validation loss: 3.489613902184271

Epoch: 5| Step: 1
Training loss: 3.7208473682403564
Validation loss: 3.4832792384650118

Epoch: 5| Step: 2
Training loss: 3.162177085876465
Validation loss: 3.477959297036612

Epoch: 5| Step: 3
Training loss: 3.112741470336914
Validation loss: 3.474081854666433

Epoch: 5| Step: 4
Training loss: 2.6308083534240723
Validation loss: 3.4672956902493715

Epoch: 5| Step: 5
Training loss: 3.7874789237976074
Validation loss: 3.4618734082868023

Epoch: 5| Step: 6
Training loss: 2.9309685230255127
Validation loss: 3.459837511021604

Epoch: 5| Step: 7
Training loss: 4.180834770202637
Validation loss: 3.451896116297732

Epoch: 5| Step: 8
Training loss: 3.2154898643493652
Validation loss: 3.4458979509210073

Epoch: 5| Step: 9
Training loss: 2.8442070484161377
Validation loss: 3.4435260167685886

Epoch: 5| Step: 10
Training loss: 3.8010177612304688
Validation loss: 3.4390652282263643

Epoch: 13| Step: 0
Training loss: 3.4799225330352783
Validation loss: 3.431743485953218

Epoch: 5| Step: 1
Training loss: 3.0378472805023193
Validation loss: 3.4294013259231404

Epoch: 5| Step: 2
Training loss: 3.170656204223633
Validation loss: 3.423872496492119

Epoch: 5| Step: 3
Training loss: 3.2091479301452637
Validation loss: 3.4183926941246114

Epoch: 5| Step: 4
Training loss: 3.550102710723877
Validation loss: 3.4132533919426704

Epoch: 5| Step: 5
Training loss: 3.339315414428711
Validation loss: 3.41019408933578

Epoch: 5| Step: 6
Training loss: 3.1716814041137695
Validation loss: 3.4038865412435224

Epoch: 5| Step: 7
Training loss: 3.782599687576294
Validation loss: 3.398286029856692

Epoch: 5| Step: 8
Training loss: 2.83205246925354
Validation loss: 3.3930963059907318

Epoch: 5| Step: 9
Training loss: 3.567157745361328
Validation loss: 3.388484631815264

Epoch: 5| Step: 10
Training loss: 3.8795740604400635
Validation loss: 3.386550175246372

Epoch: 14| Step: 0
Training loss: 3.6533164978027344
Validation loss: 3.3783757507160144

Epoch: 5| Step: 1
Training loss: 3.202789783477783
Validation loss: 3.373888010619789

Epoch: 5| Step: 2
Training loss: 3.4076457023620605
Validation loss: 3.3673477429215626

Epoch: 5| Step: 3
Training loss: 2.821281909942627
Validation loss: 3.361170253446025

Epoch: 5| Step: 4
Training loss: 3.9617552757263184
Validation loss: 3.3571307274603073

Epoch: 5| Step: 5
Training loss: 2.4561431407928467
Validation loss: 3.3510429730979343

Epoch: 5| Step: 6
Training loss: 3.2290847301483154
Validation loss: 3.342061878532492

Epoch: 5| Step: 7
Training loss: 2.9389309883117676
Validation loss: 3.3378557235963884

Epoch: 5| Step: 8
Training loss: 3.474313259124756
Validation loss: 3.333080509657501

Epoch: 5| Step: 9
Training loss: 3.8111720085144043
Validation loss: 3.3263399524073445

Epoch: 5| Step: 10
Training loss: 3.4914019107818604
Validation loss: 3.3208972818108013

Epoch: 15| Step: 0
Training loss: 3.1848621368408203
Validation loss: 3.3125112236187024

Epoch: 5| Step: 1
Training loss: 3.036400318145752
Validation loss: 3.306168661322645

Epoch: 5| Step: 2
Training loss: 2.9802968502044678
Validation loss: 3.298752430946596

Epoch: 5| Step: 3
Training loss: 2.535087823867798
Validation loss: 3.289907868190478

Epoch: 5| Step: 4
Training loss: 4.108885765075684
Validation loss: 3.2835585455740652

Epoch: 5| Step: 5
Training loss: 2.5800089836120605
Validation loss: 3.2817986960052163

Epoch: 5| Step: 6
Training loss: 4.053851127624512
Validation loss: 3.2762440866039646

Epoch: 5| Step: 7
Training loss: 3.7990214824676514
Validation loss: 3.26806055602207

Epoch: 5| Step: 8
Training loss: 3.193660020828247
Validation loss: 3.2645966340136785

Epoch: 5| Step: 9
Training loss: 3.4943320751190186
Validation loss: 3.2513867193652737

Epoch: 5| Step: 10
Training loss: 2.721651077270508
Validation loss: 3.256581526930614

Epoch: 16| Step: 0
Training loss: 3.383877992630005
Validation loss: 3.258638838286041

Epoch: 5| Step: 1
Training loss: 3.098616123199463
Validation loss: 3.2464539389456473

Epoch: 5| Step: 2
Training loss: 3.4357597827911377
Validation loss: 3.231067503652265

Epoch: 5| Step: 3
Training loss: 3.387225389480591
Validation loss: 3.2229671503907893

Epoch: 5| Step: 4
Training loss: 3.116213321685791
Validation loss: 3.217784473972936

Epoch: 5| Step: 5
Training loss: 2.637526750564575
Validation loss: 3.214779048837641

Epoch: 5| Step: 6
Training loss: 3.3511803150177
Validation loss: 3.218199840155981

Epoch: 5| Step: 7
Training loss: 1.93759286403656
Validation loss: 3.2147821149518414

Epoch: 5| Step: 8
Training loss: 3.246562957763672
Validation loss: 3.200262649084932

Epoch: 5| Step: 9
Training loss: 4.168463706970215
Validation loss: 3.187010085710915

Epoch: 5| Step: 10
Training loss: 3.5887951850891113
Validation loss: 3.1845825820840816

Epoch: 17| Step: 0
Training loss: 2.9171319007873535
Validation loss: 3.1801870612687964

Epoch: 5| Step: 1
Training loss: 2.6438851356506348
Validation loss: 3.1758287234972884

Epoch: 5| Step: 2
Training loss: 1.5720750093460083
Validation loss: 3.1687444025470364

Epoch: 5| Step: 3
Training loss: 3.272935152053833
Validation loss: 3.1638601159536712

Epoch: 5| Step: 4
Training loss: 4.363150596618652
Validation loss: 3.1605501713291293

Epoch: 5| Step: 5
Training loss: 3.092756986618042
Validation loss: 3.155391567496843

Epoch: 5| Step: 6
Training loss: 3.6884350776672363
Validation loss: 3.1481354082784345

Epoch: 5| Step: 7
Training loss: 3.5453319549560547
Validation loss: 3.1440827949072725

Epoch: 5| Step: 8
Training loss: 3.0694282054901123
Validation loss: 3.1364181938991753

Epoch: 5| Step: 9
Training loss: 3.5116171836853027
Validation loss: 3.1355398649810464

Epoch: 5| Step: 10
Training loss: 3.109455108642578
Validation loss: 3.1349454643905803

Epoch: 18| Step: 0
Training loss: 3.451411008834839
Validation loss: 3.1434487014688473

Epoch: 5| Step: 1
Training loss: 2.7363271713256836
Validation loss: 3.1423386578918784

Epoch: 5| Step: 2
Training loss: 2.9437646865844727
Validation loss: 3.12698680867431

Epoch: 5| Step: 3
Training loss: 3.060157299041748
Validation loss: 3.1089342460837415

Epoch: 5| Step: 4
Training loss: 3.273923397064209
Validation loss: 3.111091116423248

Epoch: 5| Step: 5
Training loss: 2.755150079727173
Validation loss: 3.1158049721871652

Epoch: 5| Step: 6
Training loss: 3.206740617752075
Validation loss: 3.111527978733022

Epoch: 5| Step: 7
Training loss: 3.268228054046631
Validation loss: 3.1010240893210135

Epoch: 5| Step: 8
Training loss: 3.2316627502441406
Validation loss: 3.091078676203246

Epoch: 5| Step: 9
Training loss: 3.320349931716919
Validation loss: 3.0864242866475093

Epoch: 5| Step: 10
Training loss: 3.2540855407714844
Validation loss: 3.082225686760359

Epoch: 19| Step: 0
Training loss: 3.6538875102996826
Validation loss: 3.0803310384032545

Epoch: 5| Step: 1
Training loss: 3.149832248687744
Validation loss: 3.0793992037414224

Epoch: 5| Step: 2
Training loss: 2.7618818283081055
Validation loss: 3.074004157896965

Epoch: 5| Step: 3
Training loss: 3.008347988128662
Validation loss: 3.075204674915601

Epoch: 5| Step: 4
Training loss: 3.418149471282959
Validation loss: 3.068378104958483

Epoch: 5| Step: 5
Training loss: 3.286163330078125
Validation loss: 3.065515738661571

Epoch: 5| Step: 6
Training loss: 3.236754894256592
Validation loss: 3.066904201302477

Epoch: 5| Step: 7
Training loss: 2.5087780952453613
Validation loss: 3.059330294209142

Epoch: 5| Step: 8
Training loss: 3.3717377185821533
Validation loss: 3.059823864249773

Epoch: 5| Step: 9
Training loss: 2.9746956825256348
Validation loss: 3.059174940150271

Epoch: 5| Step: 10
Training loss: 2.754761219024658
Validation loss: 3.0516261310987574

Epoch: 20| Step: 0
Training loss: 2.336695432662964
Validation loss: 3.04922886817686

Epoch: 5| Step: 1
Training loss: 2.926016330718994
Validation loss: 3.044891634295064

Epoch: 5| Step: 2
Training loss: 2.4697251319885254
Validation loss: 3.0421596868063814

Epoch: 5| Step: 3
Training loss: 4.033019542694092
Validation loss: 3.03710739843307

Epoch: 5| Step: 4
Training loss: 3.0750184059143066
Validation loss: 3.0333743761944514

Epoch: 5| Step: 5
Training loss: 2.9856467247009277
Validation loss: 3.0291912171148483

Epoch: 5| Step: 6
Training loss: 3.9027607440948486
Validation loss: 3.030155433121548

Epoch: 5| Step: 7
Training loss: 3.201288938522339
Validation loss: 3.02426040557123

Epoch: 5| Step: 8
Training loss: 2.2088875770568848
Validation loss: 3.021082957585653

Epoch: 5| Step: 9
Training loss: 3.3272500038146973
Validation loss: 3.0273324135811097

Epoch: 5| Step: 10
Training loss: 3.486708641052246
Validation loss: 3.020946387321718

Epoch: 21| Step: 0
Training loss: 2.726992130279541
Validation loss: 3.011249675545641

Epoch: 5| Step: 1
Training loss: 3.0176174640655518
Validation loss: 3.0103480277522916

Epoch: 5| Step: 2
Training loss: 2.8761279582977295
Validation loss: 3.006600777308146

Epoch: 5| Step: 3
Training loss: 3.023624897003174
Validation loss: 3.001524740649808

Epoch: 5| Step: 4
Training loss: 2.7759857177734375
Validation loss: 3.0055409503239456

Epoch: 5| Step: 5
Training loss: 3.31795072555542
Validation loss: 2.9995499887774066

Epoch: 5| Step: 6
Training loss: 3.2083003520965576
Validation loss: 2.9985851856970016

Epoch: 5| Step: 7
Training loss: 3.521559238433838
Validation loss: 2.991783485617689

Epoch: 5| Step: 8
Training loss: 3.5483238697052
Validation loss: 2.9925905119988228

Epoch: 5| Step: 9
Training loss: 2.988452434539795
Validation loss: 2.9888487733820432

Epoch: 5| Step: 10
Training loss: 2.5914549827575684
Validation loss: 2.9881609409086165

Epoch: 22| Step: 0
Training loss: 3.384370803833008
Validation loss: 2.9876788354689077

Epoch: 5| Step: 1
Training loss: 2.970390796661377
Validation loss: 2.981306870778402

Epoch: 5| Step: 2
Training loss: 3.5543811321258545
Validation loss: 2.9812155385171213

Epoch: 5| Step: 3
Training loss: 3.2790398597717285
Validation loss: 2.9816351167617308

Epoch: 5| Step: 4
Training loss: 2.3859946727752686
Validation loss: 2.9811269467876804

Epoch: 5| Step: 5
Training loss: 2.8120484352111816
Validation loss: 2.9772215761164182

Epoch: 5| Step: 6
Training loss: 3.2108142375946045
Validation loss: 2.9706346886132353

Epoch: 5| Step: 7
Training loss: 2.5773215293884277
Validation loss: 2.966671553991174

Epoch: 5| Step: 8
Training loss: 3.2116591930389404
Validation loss: 2.9659291774995866

Epoch: 5| Step: 9
Training loss: 2.9037604331970215
Validation loss: 2.964501586011661

Epoch: 5| Step: 10
Training loss: 3.2376182079315186
Validation loss: 2.9618892592768513

Epoch: 23| Step: 0
Training loss: 3.175955295562744
Validation loss: 2.9609256252165763

Epoch: 5| Step: 1
Training loss: 3.3345000743865967
Validation loss: 2.9590378961255475

Epoch: 5| Step: 2
Training loss: 2.491987705230713
Validation loss: 2.9576997936412854

Epoch: 5| Step: 3
Training loss: 3.1285908222198486
Validation loss: 2.953393684920444

Epoch: 5| Step: 4
Training loss: 3.472463607788086
Validation loss: 2.9509531579991823

Epoch: 5| Step: 5
Training loss: 2.170815944671631
Validation loss: 2.95136736541666

Epoch: 5| Step: 6
Training loss: 2.5754425525665283
Validation loss: 2.95183313533824

Epoch: 5| Step: 7
Training loss: 3.1551403999328613
Validation loss: 2.9504575242278395

Epoch: 5| Step: 8
Training loss: 3.2004637718200684
Validation loss: 2.9507094506294496

Epoch: 5| Step: 9
Training loss: 3.305103302001953
Validation loss: 2.9391712322030017

Epoch: 5| Step: 10
Training loss: 3.3293099403381348
Validation loss: 2.9399280753186954

Epoch: 24| Step: 0
Training loss: 3.2819817066192627
Validation loss: 2.9428000680861937

Epoch: 5| Step: 1
Training loss: 2.8750338554382324
Validation loss: 2.942756393904327

Epoch: 5| Step: 2
Training loss: 3.5879828929901123
Validation loss: 2.941425318359047

Epoch: 5| Step: 3
Training loss: 2.4977059364318848
Validation loss: 2.9340581611920427

Epoch: 5| Step: 4
Training loss: 3.1303820610046387
Validation loss: 2.932258221410936

Epoch: 5| Step: 5
Training loss: 2.87925386428833
Validation loss: 2.9279896956618114

Epoch: 5| Step: 6
Training loss: 2.908015727996826
Validation loss: 2.929811200787944

Epoch: 5| Step: 7
Training loss: 2.7657885551452637
Validation loss: 2.923257904668008

Epoch: 5| Step: 8
Training loss: 2.859379529953003
Validation loss: 2.92661101330993

Epoch: 5| Step: 9
Training loss: 3.8380470275878906
Validation loss: 2.9225766351146083

Epoch: 5| Step: 10
Training loss: 2.445965051651001
Validation loss: 2.92511421890669

Epoch: 25| Step: 0
Training loss: 3.2084598541259766
Validation loss: 2.9224929886479534

Epoch: 5| Step: 1
Training loss: 3.045219898223877
Validation loss: 2.9153068398916595

Epoch: 5| Step: 2
Training loss: 1.9519363641738892
Validation loss: 2.9126122664379817

Epoch: 5| Step: 3
Training loss: 3.398282289505005
Validation loss: 2.9094215311029905

Epoch: 5| Step: 4
Training loss: 2.6322388648986816
Validation loss: 2.9110581362119285

Epoch: 5| Step: 5
Training loss: 2.423335552215576
Validation loss: 2.9077455869285007

Epoch: 5| Step: 6
Training loss: 3.1785099506378174
Validation loss: 2.903971384930354

Epoch: 5| Step: 7
Training loss: 3.8782124519348145
Validation loss: 2.907861271212178

Epoch: 5| Step: 8
Training loss: 3.220956325531006
Validation loss: 2.9041391547008226

Epoch: 5| Step: 9
Training loss: 2.684370279312134
Validation loss: 2.9009503164599018

Epoch: 5| Step: 10
Training loss: 3.4352996349334717
Validation loss: 2.896424385809129

Epoch: 26| Step: 0
Training loss: 3.7736873626708984
Validation loss: 2.8972127514500774

Epoch: 5| Step: 1
Training loss: 2.952589750289917
Validation loss: 2.8913113635073424

Epoch: 5| Step: 2
Training loss: 3.0444672107696533
Validation loss: 2.8873666076249975

Epoch: 5| Step: 3
Training loss: 3.5074119567871094
Validation loss: 2.8855353734826528

Epoch: 5| Step: 4
Training loss: 3.1099677085876465
Validation loss: 2.8860542722927627

Epoch: 5| Step: 5
Training loss: 3.0256412029266357
Validation loss: 2.8816634224307154

Epoch: 5| Step: 6
Training loss: 2.0703155994415283
Validation loss: 2.879904721372871

Epoch: 5| Step: 7
Training loss: 2.910529851913452
Validation loss: 2.886298635954498

Epoch: 5| Step: 8
Training loss: 3.0457217693328857
Validation loss: 2.880140007183116

Epoch: 5| Step: 9
Training loss: 2.2600250244140625
Validation loss: 2.8785619992081837

Epoch: 5| Step: 10
Training loss: 3.092153310775757
Validation loss: 2.8783211246613534

Epoch: 27| Step: 0
Training loss: 2.9410400390625
Validation loss: 2.871676206588745

Epoch: 5| Step: 1
Training loss: 2.428020477294922
Validation loss: 2.870280942609233

Epoch: 5| Step: 2
Training loss: 1.4743356704711914
Validation loss: 2.868227579260385

Epoch: 5| Step: 3
Training loss: 3.595116138458252
Validation loss: 2.8677863690160934

Epoch: 5| Step: 4
Training loss: 4.000789642333984
Validation loss: 2.8674731254577637

Epoch: 5| Step: 5
Training loss: 3.391570568084717
Validation loss: 2.863401864164619

Epoch: 5| Step: 6
Training loss: 3.2756659984588623
Validation loss: 2.8619521612762124

Epoch: 5| Step: 7
Training loss: 3.2906126976013184
Validation loss: 2.861016422189692

Epoch: 5| Step: 8
Training loss: 3.1588077545166016
Validation loss: 2.857265818503595

Epoch: 5| Step: 9
Training loss: 2.5552849769592285
Validation loss: 2.858424655852779

Epoch: 5| Step: 10
Training loss: 2.4296658039093018
Validation loss: 2.855769754737936

Epoch: 28| Step: 0
Training loss: 2.13427734375
Validation loss: 2.8534654468618412

Epoch: 5| Step: 1
Training loss: 3.1712889671325684
Validation loss: 2.853864928727509

Epoch: 5| Step: 2
Training loss: 3.4200363159179688
Validation loss: 2.847915272558889

Epoch: 5| Step: 3
Training loss: 3.379887342453003
Validation loss: 2.8455573563934653

Epoch: 5| Step: 4
Training loss: 3.258167266845703
Validation loss: 2.845498049131004

Epoch: 5| Step: 5
Training loss: 2.3238108158111572
Validation loss: 2.8393605601403022

Epoch: 5| Step: 6
Training loss: 2.626910924911499
Validation loss: 2.839408210528794

Epoch: 5| Step: 7
Training loss: 3.439706325531006
Validation loss: 2.8402545836664017

Epoch: 5| Step: 8
Training loss: 2.8928914070129395
Validation loss: 2.8367067460090882

Epoch: 5| Step: 9
Training loss: 3.3670945167541504
Validation loss: 2.838289823583377

Epoch: 5| Step: 10
Training loss: 2.4247660636901855
Validation loss: 2.840821753266037

Epoch: 29| Step: 0
Training loss: 3.379237413406372
Validation loss: 2.8379831365359727

Epoch: 5| Step: 1
Training loss: 2.757777452468872
Validation loss: 2.834227195350073

Epoch: 5| Step: 2
Training loss: 2.943215847015381
Validation loss: 2.835792238994311

Epoch: 5| Step: 3
Training loss: 3.0747523307800293
Validation loss: 2.8385418691942768

Epoch: 5| Step: 4
Training loss: 3.283778429031372
Validation loss: 2.8343420003050115

Epoch: 5| Step: 5
Training loss: 2.6754345893859863
Validation loss: 2.8336000057958786

Epoch: 5| Step: 6
Training loss: 2.729663848876953
Validation loss: 2.830763570723995

Epoch: 5| Step: 7
Training loss: 2.4355735778808594
Validation loss: 2.82983749399903

Epoch: 5| Step: 8
Training loss: 3.3095335960388184
Validation loss: 2.8251502308794247

Epoch: 5| Step: 9
Training loss: 2.937346935272217
Validation loss: 2.8262809579090407

Epoch: 5| Step: 10
Training loss: 2.8595426082611084
Validation loss: 2.827192465464274

Epoch: 30| Step: 0
Training loss: 3.045070171356201
Validation loss: 2.822866162946147

Epoch: 5| Step: 1
Training loss: 2.382943630218506
Validation loss: 2.811942390216294

Epoch: 5| Step: 2
Training loss: 1.9413608312606812
Validation loss: 2.8115140315025084

Epoch: 5| Step: 3
Training loss: 3.7169933319091797
Validation loss: 2.8166568509994017

Epoch: 5| Step: 4
Training loss: 3.6540005207061768
Validation loss: 2.8197302279933805

Epoch: 5| Step: 5
Training loss: 3.145205497741699
Validation loss: 2.818894435000676

Epoch: 5| Step: 6
Training loss: 3.9678006172180176
Validation loss: 2.8117425493014756

Epoch: 5| Step: 7
Training loss: 2.9336276054382324
Validation loss: 2.806066277206585

Epoch: 5| Step: 8
Training loss: 1.9111322164535522
Validation loss: 2.8020303736450853

Epoch: 5| Step: 9
Training loss: 2.870206832885742
Validation loss: 2.8006183126921296

Epoch: 5| Step: 10
Training loss: 2.7234058380126953
Validation loss: 2.8023554637867916

Epoch: 31| Step: 0
Training loss: 3.1733360290527344
Validation loss: 2.805318914433961

Epoch: 5| Step: 1
Training loss: 2.118267774581909
Validation loss: 2.8010821650105138

Epoch: 5| Step: 2
Training loss: 2.890676498413086
Validation loss: 2.7987040960660545

Epoch: 5| Step: 3
Training loss: 2.801694393157959
Validation loss: 2.8084646988940496

Epoch: 5| Step: 4
Training loss: 2.9186532497406006
Validation loss: 2.7977314828544535

Epoch: 5| Step: 5
Training loss: 2.35793137550354
Validation loss: 2.792977873997022

Epoch: 5| Step: 6
Training loss: 2.5879805088043213
Validation loss: 2.793692873370263

Epoch: 5| Step: 7
Training loss: 2.6909947395324707
Validation loss: 2.7901221782930437

Epoch: 5| Step: 8
Training loss: 3.5199532508850098
Validation loss: 2.795272893803094

Epoch: 5| Step: 9
Training loss: 3.457181930541992
Validation loss: 2.8008950115532003

Epoch: 5| Step: 10
Training loss: 3.7887370586395264
Validation loss: 2.8015655163795716

Epoch: 32| Step: 0
Training loss: 2.770826816558838
Validation loss: 2.7902040532840195

Epoch: 5| Step: 1
Training loss: 2.8723807334899902
Validation loss: 2.7911608321692354

Epoch: 5| Step: 2
Training loss: 2.754474401473999
Validation loss: 2.7830099854418027

Epoch: 5| Step: 3
Training loss: 3.174668788909912
Validation loss: 2.782592870855844

Epoch: 5| Step: 4
Training loss: 2.1220574378967285
Validation loss: 2.782711200816657

Epoch: 5| Step: 5
Training loss: 2.760625123977661
Validation loss: 2.7788870180806806

Epoch: 5| Step: 6
Training loss: 3.039543390274048
Validation loss: 2.779387994479108

Epoch: 5| Step: 7
Training loss: 3.4485397338867188
Validation loss: 2.7791452664200977

Epoch: 5| Step: 8
Training loss: 2.6230695247650146
Validation loss: 2.7797119796917005

Epoch: 5| Step: 9
Training loss: 3.196640729904175
Validation loss: 2.7854669658086633

Epoch: 5| Step: 10
Training loss: 3.319295644760132
Validation loss: 2.780209805375786

Epoch: 33| Step: 0
Training loss: 2.406435012817383
Validation loss: 2.78435028752973

Epoch: 5| Step: 1
Training loss: 3.007338762283325
Validation loss: 2.7744370660474225

Epoch: 5| Step: 2
Training loss: 3.3721587657928467
Validation loss: 2.7710184563872633

Epoch: 5| Step: 3
Training loss: 2.7148194313049316
Validation loss: 2.770413285942488

Epoch: 5| Step: 4
Training loss: 2.897444248199463
Validation loss: 2.7675715723345355

Epoch: 5| Step: 5
Training loss: 3.0912094116210938
Validation loss: 2.7683563155512654

Epoch: 5| Step: 6
Training loss: 3.3879992961883545
Validation loss: 2.7683346399696926

Epoch: 5| Step: 7
Training loss: 3.0630314350128174
Validation loss: 2.765844762966197

Epoch: 5| Step: 8
Training loss: 3.0914013385772705
Validation loss: 2.7647589714296403

Epoch: 5| Step: 9
Training loss: 2.4475302696228027
Validation loss: 2.7710469948348178

Epoch: 5| Step: 10
Training loss: 2.426389694213867
Validation loss: 2.769807625842351

Epoch: 34| Step: 0
Training loss: 2.9717724323272705
Validation loss: 2.7745858110407347

Epoch: 5| Step: 1
Training loss: 3.0230023860931396
Validation loss: 2.768760158169654

Epoch: 5| Step: 2
Training loss: 2.676790714263916
Validation loss: 2.762362062290151

Epoch: 5| Step: 3
Training loss: 2.4710590839385986
Validation loss: 2.7652852355792956

Epoch: 5| Step: 4
Training loss: 2.6203083992004395
Validation loss: 2.7612767783544396

Epoch: 5| Step: 5
Training loss: 2.7874534130096436
Validation loss: 2.7551795385217153

Epoch: 5| Step: 6
Training loss: 3.174556255340576
Validation loss: 2.7573550926741732

Epoch: 5| Step: 7
Training loss: 3.1202845573425293
Validation loss: 2.7560282420086604

Epoch: 5| Step: 8
Training loss: 3.2408065795898438
Validation loss: 2.7546031423794326

Epoch: 5| Step: 9
Training loss: 2.6600852012634277
Validation loss: 2.7538353140636156

Epoch: 5| Step: 10
Training loss: 3.2010135650634766
Validation loss: 2.7545927980894684

Epoch: 35| Step: 0
Training loss: 3.0648694038391113
Validation loss: 2.7505432354506625

Epoch: 5| Step: 1
Training loss: 2.8705151081085205
Validation loss: 2.7479073744948193

Epoch: 5| Step: 2
Training loss: 2.7686376571655273
Validation loss: 2.7467268641277025

Epoch: 5| Step: 3
Training loss: 3.3911609649658203
Validation loss: 2.7454688856678624

Epoch: 5| Step: 4
Training loss: 2.2490832805633545
Validation loss: 2.746200135959092

Epoch: 5| Step: 5
Training loss: 2.673583507537842
Validation loss: 2.745437478506437

Epoch: 5| Step: 6
Training loss: 3.204411745071411
Validation loss: 2.744888928628737

Epoch: 5| Step: 7
Training loss: 3.1692023277282715
Validation loss: 2.7435310938025035

Epoch: 5| Step: 8
Training loss: 2.677957534790039
Validation loss: 2.7397197497788297

Epoch: 5| Step: 9
Training loss: 3.4646034240722656
Validation loss: 2.7412585468702417

Epoch: 5| Step: 10
Training loss: 2.1780428886413574
Validation loss: 2.736600970709196

Epoch: 36| Step: 0
Training loss: 2.7827820777893066
Validation loss: 2.737776610159105

Epoch: 5| Step: 1
Training loss: 2.5368947982788086
Validation loss: 2.7360546229988016

Epoch: 5| Step: 2
Training loss: 3.284198760986328
Validation loss: 2.7351464379218315

Epoch: 5| Step: 3
Training loss: 3.71120023727417
Validation loss: 2.734850432283135

Epoch: 5| Step: 4
Training loss: 2.7161967754364014
Validation loss: 2.7360076417205152

Epoch: 5| Step: 5
Training loss: 2.598423957824707
Validation loss: 2.739099387199648

Epoch: 5| Step: 6
Training loss: 4.15798282623291
Validation loss: 2.743403104043776

Epoch: 5| Step: 7
Training loss: 2.99733304977417
Validation loss: 2.735211364684566

Epoch: 5| Step: 8
Training loss: 2.2024314403533936
Validation loss: 2.7292098870841404

Epoch: 5| Step: 9
Training loss: 2.7741520404815674
Validation loss: 2.728635431617819

Epoch: 5| Step: 10
Training loss: 1.790400505065918
Validation loss: 2.7312957086870746

Epoch: 37| Step: 0
Training loss: 2.2441158294677734
Validation loss: 2.7373863420178814

Epoch: 5| Step: 1
Training loss: 3.245837688446045
Validation loss: 2.7421523601778093

Epoch: 5| Step: 2
Training loss: 3.5594496726989746
Validation loss: 2.749316002732964

Epoch: 5| Step: 3
Training loss: 3.138779401779175
Validation loss: 2.7522340025953067

Epoch: 5| Step: 4
Training loss: 2.3980650901794434
Validation loss: 2.738492896479945

Epoch: 5| Step: 5
Training loss: 2.705082654953003
Validation loss: 2.7340386426577004

Epoch: 5| Step: 6
Training loss: 2.0729172229766846
Validation loss: 2.7237745869544243

Epoch: 5| Step: 7
Training loss: 3.6195735931396484
Validation loss: 2.72051001364185

Epoch: 5| Step: 8
Training loss: 3.243619203567505
Validation loss: 2.7232214148326586

Epoch: 5| Step: 9
Training loss: 2.3688571453094482
Validation loss: 2.723804966095955

Epoch: 5| Step: 10
Training loss: 3.169914722442627
Validation loss: 2.7297059976926414

Epoch: 38| Step: 0
Training loss: 2.4218907356262207
Validation loss: 2.7229506200359714

Epoch: 5| Step: 1
Training loss: 2.07503080368042
Validation loss: 2.7166550338909192

Epoch: 5| Step: 2
Training loss: 3.7645084857940674
Validation loss: 2.715084206673407

Epoch: 5| Step: 3
Training loss: 3.165519952774048
Validation loss: 2.7184775132004932

Epoch: 5| Step: 4
Training loss: 3.0950515270233154
Validation loss: 2.7127435617549445

Epoch: 5| Step: 5
Training loss: 2.8113150596618652
Validation loss: 2.7155018288602113

Epoch: 5| Step: 6
Training loss: 2.625232696533203
Validation loss: 2.7128597664576706

Epoch: 5| Step: 7
Training loss: 2.719508647918701
Validation loss: 2.7159426391765638

Epoch: 5| Step: 8
Training loss: 3.2924251556396484
Validation loss: 2.7143394716324343

Epoch: 5| Step: 9
Training loss: 2.880446195602417
Validation loss: 2.712406850630237

Epoch: 5| Step: 10
Training loss: 2.710618734359741
Validation loss: 2.721309297828264

Epoch: 39| Step: 0
Training loss: 2.950059413909912
Validation loss: 2.717897076760569

Epoch: 5| Step: 1
Training loss: 2.6887407302856445
Validation loss: 2.714248785408594

Epoch: 5| Step: 2
Training loss: 2.8212461471557617
Validation loss: 2.709157500215756

Epoch: 5| Step: 3
Training loss: 3.080104351043701
Validation loss: 2.7110151321657243

Epoch: 5| Step: 4
Training loss: 2.6511006355285645
Validation loss: 2.707466135742844

Epoch: 5| Step: 5
Training loss: 2.7992615699768066
Validation loss: 2.7086077300451135

Epoch: 5| Step: 6
Training loss: 3.713709592819214
Validation loss: 2.7095633065828713

Epoch: 5| Step: 7
Training loss: 2.3545491695404053
Validation loss: 2.708807817069433

Epoch: 5| Step: 8
Training loss: 2.7532811164855957
Validation loss: 2.709526700358237

Epoch: 5| Step: 9
Training loss: 2.8728463649749756
Validation loss: 2.711221689819008

Epoch: 5| Step: 10
Training loss: 2.8682005405426025
Validation loss: 2.7098526621377594

Epoch: 40| Step: 0
Training loss: 3.148751735687256
Validation loss: 2.7111369743142077

Epoch: 5| Step: 1
Training loss: 3.1268725395202637
Validation loss: 2.7085884027583624

Epoch: 5| Step: 2
Training loss: 2.4491422176361084
Validation loss: 2.7070104845108522

Epoch: 5| Step: 3
Training loss: 3.25044322013855
Validation loss: 2.706265221359909

Epoch: 5| Step: 4
Training loss: 3.0470657348632812
Validation loss: 2.7080324388319448

Epoch: 5| Step: 5
Training loss: 2.915470600128174
Validation loss: 2.7094524675799954

Epoch: 5| Step: 6
Training loss: 3.031724452972412
Validation loss: 2.706168190125496

Epoch: 5| Step: 7
Training loss: 2.565430164337158
Validation loss: 2.704745864355436

Epoch: 5| Step: 8
Training loss: 2.3927371501922607
Validation loss: 2.702529994390344

Epoch: 5| Step: 9
Training loss: 2.756411075592041
Validation loss: 2.701009726011625

Epoch: 5| Step: 10
Training loss: 2.828197479248047
Validation loss: 2.702047896641557

Epoch: 41| Step: 0
Training loss: 2.834041118621826
Validation loss: 2.705727074735908

Epoch: 5| Step: 1
Training loss: 3.067350149154663
Validation loss: 2.702077532327303

Epoch: 5| Step: 2
Training loss: 2.237989902496338
Validation loss: 2.7091335199212514

Epoch: 5| Step: 3
Training loss: 3.57362699508667
Validation loss: 2.707321115719375

Epoch: 5| Step: 4
Training loss: 3.033812999725342
Validation loss: 2.7138598298513763

Epoch: 5| Step: 5
Training loss: 2.434938430786133
Validation loss: 2.714541603160161

Epoch: 5| Step: 6
Training loss: 3.293565273284912
Validation loss: 2.699667397365775

Epoch: 5| Step: 7
Training loss: 2.7824490070343018
Validation loss: 2.6947846938205022

Epoch: 5| Step: 8
Training loss: 2.683885335922241
Validation loss: 2.697092197274649

Epoch: 5| Step: 9
Training loss: 2.6443779468536377
Validation loss: 2.6996547560538016

Epoch: 5| Step: 10
Training loss: 2.9024693965911865
Validation loss: 2.7032388307714976

Epoch: 42| Step: 0
Training loss: 3.20088267326355
Validation loss: 2.704740242291522

Epoch: 5| Step: 1
Training loss: 2.222435235977173
Validation loss: 2.696803741557624

Epoch: 5| Step: 2
Training loss: 3.168381690979004
Validation loss: 2.69464688147268

Epoch: 5| Step: 3
Training loss: 3.2840919494628906
Validation loss: 2.6939860748988327

Epoch: 5| Step: 4
Training loss: 2.600698232650757
Validation loss: 2.698418895403544

Epoch: 5| Step: 5
Training loss: 1.842726469039917
Validation loss: 2.7186699580120783

Epoch: 5| Step: 6
Training loss: 3.4014525413513184
Validation loss: 2.714315301628523

Epoch: 5| Step: 7
Training loss: 2.967668294906616
Validation loss: 2.7013798964920865

Epoch: 5| Step: 8
Training loss: 3.07954740524292
Validation loss: 2.692786964037085

Epoch: 5| Step: 9
Training loss: 2.899420738220215
Validation loss: 2.688495659059094

Epoch: 5| Step: 10
Training loss: 2.735590696334839
Validation loss: 2.6884746833514144

Epoch: 43| Step: 0
Training loss: 3.685918092727661
Validation loss: 2.6901158978862147

Epoch: 5| Step: 1
Training loss: 2.9190266132354736
Validation loss: 2.6864902921902236

Epoch: 5| Step: 2
Training loss: 2.1619114875793457
Validation loss: 2.689570326958933

Epoch: 5| Step: 3
Training loss: 3.3653697967529297
Validation loss: 2.68130559690537

Epoch: 5| Step: 4
Training loss: 3.3082737922668457
Validation loss: 2.687412533708798

Epoch: 5| Step: 5
Training loss: 3.164377212524414
Validation loss: 2.686492463593842

Epoch: 5| Step: 6
Training loss: 2.843092441558838
Validation loss: 2.682784616306264

Epoch: 5| Step: 7
Training loss: 1.619584321975708
Validation loss: 2.6869853491424234

Epoch: 5| Step: 8
Training loss: 2.1344876289367676
Validation loss: 2.683343502783006

Epoch: 5| Step: 9
Training loss: 3.5410239696502686
Validation loss: 2.68355223953083

Epoch: 5| Step: 10
Training loss: 2.5445191860198975
Validation loss: 2.6817476108510006

Epoch: 44| Step: 0
Training loss: 2.888094425201416
Validation loss: 2.680719970374979

Epoch: 5| Step: 1
Training loss: 2.8298919200897217
Validation loss: 2.6808842561578237

Epoch: 5| Step: 2
Training loss: 2.5112509727478027
Validation loss: 2.680238749391289

Epoch: 5| Step: 3
Training loss: 3.2789509296417236
Validation loss: 2.6834915658479095

Epoch: 5| Step: 4
Training loss: 2.703249454498291
Validation loss: 2.6884238489212526

Epoch: 5| Step: 5
Training loss: 3.0996594429016113
Validation loss: 2.7024238007042998

Epoch: 5| Step: 6
Training loss: 2.713557481765747
Validation loss: 2.7086315360120548

Epoch: 5| Step: 7
Training loss: 2.603537082672119
Validation loss: 2.6966866165079098

Epoch: 5| Step: 8
Training loss: 3.389315366744995
Validation loss: 2.6813354312732653

Epoch: 5| Step: 9
Training loss: 2.5637364387512207
Validation loss: 2.6834154000846286

Epoch: 5| Step: 10
Training loss: 2.7659130096435547
Validation loss: 2.693931770581071

Epoch: 45| Step: 0
Training loss: 2.9366021156311035
Validation loss: 2.708338406778151

Epoch: 5| Step: 1
Training loss: 3.136255979537964
Validation loss: 2.717620634263562

Epoch: 5| Step: 2
Training loss: 2.4146957397460938
Validation loss: 2.7165135388733237

Epoch: 5| Step: 3
Training loss: 2.4092469215393066
Validation loss: 2.7199646042239283

Epoch: 5| Step: 4
Training loss: 2.668452739715576
Validation loss: 2.71169102063743

Epoch: 5| Step: 5
Training loss: 3.287382125854492
Validation loss: 2.7029108719159196

Epoch: 5| Step: 6
Training loss: 3.4240241050720215
Validation loss: 2.6953805646588727

Epoch: 5| Step: 7
Training loss: 2.828077554702759
Validation loss: 2.692729437223045

Epoch: 5| Step: 8
Training loss: 2.8541269302368164
Validation loss: 2.6899662633096018

Epoch: 5| Step: 9
Training loss: 2.416527509689331
Validation loss: 2.6906804551360426

Epoch: 5| Step: 10
Training loss: 3.204838752746582
Validation loss: 2.695307183009322

Epoch: 46| Step: 0
Training loss: 3.188520669937134
Validation loss: 2.696208115546934

Epoch: 5| Step: 1
Training loss: 3.0085151195526123
Validation loss: 2.6912418001441547

Epoch: 5| Step: 2
Training loss: 2.8382456302642822
Validation loss: 2.685082512517129

Epoch: 5| Step: 3
Training loss: 2.832742929458618
Validation loss: 2.6784142473692536

Epoch: 5| Step: 4
Training loss: 2.762957811355591
Validation loss: 2.677723641036659

Epoch: 5| Step: 5
Training loss: 2.5083773136138916
Validation loss: 2.675662512420326

Epoch: 5| Step: 6
Training loss: 2.8604419231414795
Validation loss: 2.6814856426690215

Epoch: 5| Step: 7
Training loss: 2.5017499923706055
Validation loss: 2.6860645945354173

Epoch: 5| Step: 8
Training loss: 3.1351571083068848
Validation loss: 2.6896226611188663

Epoch: 5| Step: 9
Training loss: 2.686924695968628
Validation loss: 2.6835363295770462

Epoch: 5| Step: 10
Training loss: 3.0627875328063965
Validation loss: 2.67854223456434

Epoch: 47| Step: 0
Training loss: 2.483859062194824
Validation loss: 2.6768979564789803

Epoch: 5| Step: 1
Training loss: 2.58565354347229
Validation loss: 2.677326222901703

Epoch: 5| Step: 2
Training loss: 2.8916354179382324
Validation loss: 2.674985783074492

Epoch: 5| Step: 3
Training loss: 3.917964220046997
Validation loss: 2.6736401357958393

Epoch: 5| Step: 4
Training loss: 3.2567081451416016
Validation loss: 2.673621205873387

Epoch: 5| Step: 5
Training loss: 3.3271477222442627
Validation loss: 2.677244996511808

Epoch: 5| Step: 6
Training loss: 2.4784538745880127
Validation loss: 2.676331943081271

Epoch: 5| Step: 7
Training loss: 3.329617977142334
Validation loss: 2.67416428237833

Epoch: 5| Step: 8
Training loss: 2.5531387329101562
Validation loss: 2.667399880706623

Epoch: 5| Step: 9
Training loss: 1.8654515743255615
Validation loss: 2.6682431185117332

Epoch: 5| Step: 10
Training loss: 2.528088092803955
Validation loss: 2.665840825726909

Epoch: 48| Step: 0
Training loss: 2.9320380687713623
Validation loss: 2.6670842273260957

Epoch: 5| Step: 1
Training loss: 3.118091583251953
Validation loss: 2.6686819804612028

Epoch: 5| Step: 2
Training loss: 2.362499952316284
Validation loss: 2.6671770388080227

Epoch: 5| Step: 3
Training loss: 3.011051893234253
Validation loss: 2.668941654184813

Epoch: 5| Step: 4
Training loss: 3.1400883197784424
Validation loss: 2.6680713699709986

Epoch: 5| Step: 5
Training loss: 3.0841596126556396
Validation loss: 2.6679454106156544

Epoch: 5| Step: 6
Training loss: 2.4253597259521484
Validation loss: 2.661952937802961

Epoch: 5| Step: 7
Training loss: 2.2877094745635986
Validation loss: 2.662302883722449

Epoch: 5| Step: 8
Training loss: 2.800898313522339
Validation loss: 2.6623436661176783

Epoch: 5| Step: 9
Training loss: 3.0946543216705322
Validation loss: 2.667997657611806

Epoch: 5| Step: 10
Training loss: 2.9718687534332275
Validation loss: 2.6612500426589802

Epoch: 49| Step: 0
Training loss: 2.273606777191162
Validation loss: 2.6626447836558023

Epoch: 5| Step: 1
Training loss: 2.374838352203369
Validation loss: 2.6629821074906217

Epoch: 5| Step: 2
Training loss: 2.253021717071533
Validation loss: 2.6606889565785727

Epoch: 5| Step: 3
Training loss: 3.307305097579956
Validation loss: 2.6681286699028424

Epoch: 5| Step: 4
Training loss: 2.6086232662200928
Validation loss: 2.6687929040642193

Epoch: 5| Step: 5
Training loss: 2.8563284873962402
Validation loss: 2.6630464446160103

Epoch: 5| Step: 6
Training loss: 2.967528820037842
Validation loss: 2.6600280654045845

Epoch: 5| Step: 7
Training loss: 2.7382407188415527
Validation loss: 2.658477790894047

Epoch: 5| Step: 8
Training loss: 3.6957848072052
Validation loss: 2.6597461520984607

Epoch: 5| Step: 9
Training loss: 3.2410900592803955
Validation loss: 2.665457171778525

Epoch: 5| Step: 10
Training loss: 2.8930559158325195
Validation loss: 2.664214208561887

Epoch: 50| Step: 0
Training loss: 3.0417354106903076
Validation loss: 2.664277640722131

Epoch: 5| Step: 1
Training loss: 1.8189754486083984
Validation loss: 2.6637434600501932

Epoch: 5| Step: 2
Training loss: 2.969325542449951
Validation loss: 2.6627893627330823

Epoch: 5| Step: 3
Training loss: 2.602074384689331
Validation loss: 2.6655689080556235

Epoch: 5| Step: 4
Training loss: 3.554412364959717
Validation loss: 2.6666464113420054

Epoch: 5| Step: 5
Training loss: 3.416332721710205
Validation loss: 2.6611764379726943

Epoch: 5| Step: 6
Training loss: 2.442934036254883
Validation loss: 2.662412356304866

Epoch: 5| Step: 7
Training loss: 3.2609336376190186
Validation loss: 2.660741288174865

Epoch: 5| Step: 8
Training loss: 2.1261277198791504
Validation loss: 2.654599861432147

Epoch: 5| Step: 9
Training loss: 2.5302393436431885
Validation loss: 2.6562560501919

Epoch: 5| Step: 10
Training loss: 3.5466842651367188
Validation loss: 2.654375309585243

Epoch: 51| Step: 0
Training loss: 3.127150058746338
Validation loss: 2.653611829203944

Epoch: 5| Step: 1
Training loss: 2.850203037261963
Validation loss: 2.659757129607662

Epoch: 5| Step: 2
Training loss: 2.4281868934631348
Validation loss: 2.6552668463799263

Epoch: 5| Step: 3
Training loss: 2.658320903778076
Validation loss: 2.6571432211065806

Epoch: 5| Step: 4
Training loss: 2.4467380046844482
Validation loss: 2.6538394035831576

Epoch: 5| Step: 5
Training loss: 2.247918128967285
Validation loss: 2.6570523990097867

Epoch: 5| Step: 6
Training loss: 3.1441116333007812
Validation loss: 2.653654839402886

Epoch: 5| Step: 7
Training loss: 3.184530735015869
Validation loss: 2.6520264379439817

Epoch: 5| Step: 8
Training loss: 2.5838911533355713
Validation loss: 2.650215600126533

Epoch: 5| Step: 9
Training loss: 2.704758882522583
Validation loss: 2.64948461773575

Epoch: 5| Step: 10
Training loss: 3.894826889038086
Validation loss: 2.6594078668984036

Epoch: 52| Step: 0
Training loss: 2.600895881652832
Validation loss: 2.651702073312575

Epoch: 5| Step: 1
Training loss: 2.087045431137085
Validation loss: 2.6514702919990785

Epoch: 5| Step: 2
Training loss: 2.984239101409912
Validation loss: 2.6508251518331547

Epoch: 5| Step: 3
Training loss: 2.576138973236084
Validation loss: 2.6477715661448817

Epoch: 5| Step: 4
Training loss: 2.4243438243865967
Validation loss: 2.6517998454391316

Epoch: 5| Step: 5
Training loss: 3.0854530334472656
Validation loss: 2.6480015298371673

Epoch: 5| Step: 6
Training loss: 3.094871759414673
Validation loss: 2.647469684641848

Epoch: 5| Step: 7
Training loss: 3.0807597637176514
Validation loss: 2.650018548452726

Epoch: 5| Step: 8
Training loss: 3.0434749126434326
Validation loss: 2.647295387842322

Epoch: 5| Step: 9
Training loss: 3.247107982635498
Validation loss: 2.6483482391603532

Epoch: 5| Step: 10
Training loss: 2.8338589668273926
Validation loss: 2.6476633112917662

Epoch: 53| Step: 0
Training loss: 3.064290761947632
Validation loss: 2.6481446348210818

Epoch: 5| Step: 1
Training loss: 2.088531255722046
Validation loss: 2.6502767942285024

Epoch: 5| Step: 2
Training loss: 3.0123138427734375
Validation loss: 2.641890154089979

Epoch: 5| Step: 3
Training loss: 3.1558773517608643
Validation loss: 2.6468065631005073

Epoch: 5| Step: 4
Training loss: 3.339670181274414
Validation loss: 2.645821553404613

Epoch: 5| Step: 5
Training loss: 3.096822738647461
Validation loss: 2.6396730535773822

Epoch: 5| Step: 6
Training loss: 3.154869794845581
Validation loss: 2.6447259431244223

Epoch: 5| Step: 7
Training loss: 2.7542662620544434
Validation loss: 2.6451719063584522

Epoch: 5| Step: 8
Training loss: 2.7682318687438965
Validation loss: 2.6389515015386764

Epoch: 5| Step: 9
Training loss: 2.3817410469055176
Validation loss: 2.639086425945323

Epoch: 5| Step: 10
Training loss: 2.168180465698242
Validation loss: 2.640600340340727

Epoch: 54| Step: 0
Training loss: 2.969369888305664
Validation loss: 2.6435352525403424

Epoch: 5| Step: 1
Training loss: 3.238048553466797
Validation loss: 2.6390903329336517

Epoch: 5| Step: 2
Training loss: 2.2874224185943604
Validation loss: 2.6390829137576524

Epoch: 5| Step: 3
Training loss: 2.5317203998565674
Validation loss: 2.6413635361579155

Epoch: 5| Step: 4
Training loss: 3.0451831817626953
Validation loss: 2.64250167979989

Epoch: 5| Step: 5
Training loss: 1.9447768926620483
Validation loss: 2.6356579308868735

Epoch: 5| Step: 6
Training loss: 2.227344036102295
Validation loss: 2.6383272883712605

Epoch: 5| Step: 7
Training loss: 2.7197813987731934
Validation loss: 2.6329807799349547

Epoch: 5| Step: 8
Training loss: 3.5275321006774902
Validation loss: 2.6364681797642864

Epoch: 5| Step: 9
Training loss: 2.9364192485809326
Validation loss: 2.6389747742683656

Epoch: 5| Step: 10
Training loss: 3.6834540367126465
Validation loss: 2.6386722595460954

Epoch: 55| Step: 0
Training loss: 2.973412036895752
Validation loss: 2.637183371410575

Epoch: 5| Step: 1
Training loss: 3.3839259147644043
Validation loss: 2.634154899145967

Epoch: 5| Step: 2
Training loss: 3.287118911743164
Validation loss: 2.635416587193807

Epoch: 5| Step: 3
Training loss: 2.882272243499756
Validation loss: 2.627229198332756

Epoch: 5| Step: 4
Training loss: 2.6407716274261475
Validation loss: 2.6308992780664915

Epoch: 5| Step: 5
Training loss: 1.987191915512085
Validation loss: 2.630918759171681

Epoch: 5| Step: 6
Training loss: 2.8632683753967285
Validation loss: 2.635312282910911

Epoch: 5| Step: 7
Training loss: 2.8415145874023438
Validation loss: 2.6328812517145628

Epoch: 5| Step: 8
Training loss: 2.3421199321746826
Validation loss: 2.6343570832283265

Epoch: 5| Step: 9
Training loss: 2.822852611541748
Validation loss: 2.6321758352300173

Epoch: 5| Step: 10
Training loss: 2.858721971511841
Validation loss: 2.6285222550874114

Epoch: 56| Step: 0
Training loss: 2.3468854427337646
Validation loss: 2.6295990584999003

Epoch: 5| Step: 1
Training loss: 2.9922146797180176
Validation loss: 2.6344077330763622

Epoch: 5| Step: 2
Training loss: 2.6111249923706055
Validation loss: 2.63225136264678

Epoch: 5| Step: 3
Training loss: 2.9277987480163574
Validation loss: 2.6235138549599597

Epoch: 5| Step: 4
Training loss: 2.5930161476135254
Validation loss: 2.6229105867365354

Epoch: 5| Step: 5
Training loss: 2.755234956741333
Validation loss: 2.622165372294764

Epoch: 5| Step: 6
Training loss: 2.9921860694885254
Validation loss: 2.6222577684669086

Epoch: 5| Step: 7
Training loss: 2.3033244609832764
Validation loss: 2.628714123079854

Epoch: 5| Step: 8
Training loss: 3.390565872192383
Validation loss: 2.6217142689612603

Epoch: 5| Step: 9
Training loss: 3.070652484893799
Validation loss: 2.627247364290299

Epoch: 5| Step: 10
Training loss: 2.8855743408203125
Validation loss: 2.6287400850685696

Epoch: 57| Step: 0
Training loss: 2.8033134937286377
Validation loss: 2.630624596790601

Epoch: 5| Step: 1
Training loss: 2.7844364643096924
Validation loss: 2.6289485244340796

Epoch: 5| Step: 2
Training loss: 2.755730152130127
Validation loss: 2.6240224761347615

Epoch: 5| Step: 3
Training loss: 3.3330788612365723
Validation loss: 2.620269367771764

Epoch: 5| Step: 4
Training loss: 2.840350389480591
Validation loss: 2.6208043739359868

Epoch: 5| Step: 5
Training loss: 2.473778009414673
Validation loss: 2.6212238060530795

Epoch: 5| Step: 6
Training loss: 1.9743196964263916
Validation loss: 2.6199926381470053

Epoch: 5| Step: 7
Training loss: 2.9265565872192383
Validation loss: 2.623176041469779

Epoch: 5| Step: 8
Training loss: 3.010593891143799
Validation loss: 2.616459761896441

Epoch: 5| Step: 9
Training loss: 2.8966219425201416
Validation loss: 2.6128278445172053

Epoch: 5| Step: 10
Training loss: 3.048321008682251
Validation loss: 2.61490313981169

Epoch: 58| Step: 0
Training loss: 2.9448485374450684
Validation loss: 2.6270678222820325

Epoch: 5| Step: 1
Training loss: 2.2369813919067383
Validation loss: 2.646345316722829

Epoch: 5| Step: 2
Training loss: 2.769012928009033
Validation loss: 2.639540787666075

Epoch: 5| Step: 3
Training loss: 2.6138083934783936
Validation loss: 2.6307060949264036

Epoch: 5| Step: 4
Training loss: 2.514846086502075
Validation loss: 2.6376878343602663

Epoch: 5| Step: 5
Training loss: 3.086909770965576
Validation loss: 2.622157117371918

Epoch: 5| Step: 6
Training loss: 3.413240432739258
Validation loss: 2.6168925813449326

Epoch: 5| Step: 7
Training loss: 3.149289846420288
Validation loss: 2.6181935238581833

Epoch: 5| Step: 8
Training loss: 2.5297675132751465
Validation loss: 2.616939357531968

Epoch: 5| Step: 9
Training loss: 2.434798002243042
Validation loss: 2.6275455926054265

Epoch: 5| Step: 10
Training loss: 3.2263200283050537
Validation loss: 2.6296765214653424

Epoch: 59| Step: 0
Training loss: 2.8493475914001465
Validation loss: 2.6233976092389835

Epoch: 5| Step: 1
Training loss: 2.557331085205078
Validation loss: 2.621630125148322

Epoch: 5| Step: 2
Training loss: 3.297882080078125
Validation loss: 2.6190192673795964

Epoch: 5| Step: 3
Training loss: 3.098707914352417
Validation loss: 2.6098731051209154

Epoch: 5| Step: 4
Training loss: 2.7217178344726562
Validation loss: 2.608560513424617

Epoch: 5| Step: 5
Training loss: 2.2491514682769775
Validation loss: 2.6118357796822824

Epoch: 5| Step: 6
Training loss: 2.857334613800049
Validation loss: 2.606300818022861

Epoch: 5| Step: 7
Training loss: 2.499194622039795
Validation loss: 2.6069386543766147

Epoch: 5| Step: 8
Training loss: 2.66764760017395
Validation loss: 2.607564546728647

Epoch: 5| Step: 9
Training loss: 3.1760783195495605
Validation loss: 2.6110549767812095

Epoch: 5| Step: 10
Training loss: 2.7639617919921875
Validation loss: 2.609267703948482

Epoch: 60| Step: 0
Training loss: 3.3996498584747314
Validation loss: 2.6084442984673286

Epoch: 5| Step: 1
Training loss: 2.4938106536865234
Validation loss: 2.606857492077735

Epoch: 5| Step: 2
Training loss: 1.9563013315200806
Validation loss: 2.6088953223279727

Epoch: 5| Step: 3
Training loss: 2.5462005138397217
Validation loss: 2.606537829163254

Epoch: 5| Step: 4
Training loss: 2.9984617233276367
Validation loss: 2.6039899138994116

Epoch: 5| Step: 5
Training loss: 3.0392589569091797
Validation loss: 2.61120972582089

Epoch: 5| Step: 6
Training loss: 3.374028444290161
Validation loss: 2.6173134209007345

Epoch: 5| Step: 7
Training loss: 2.5125787258148193
Validation loss: 2.6259787979946343

Epoch: 5| Step: 8
Training loss: 2.677180051803589
Validation loss: 2.609163022810413

Epoch: 5| Step: 9
Training loss: 2.561464786529541
Validation loss: 2.6026778554403656

Epoch: 5| Step: 10
Training loss: 3.2308902740478516
Validation loss: 2.6017475397356096

Epoch: 61| Step: 0
Training loss: 2.4553842544555664
Validation loss: 2.5994186119366716

Epoch: 5| Step: 1
Training loss: 3.08762788772583
Validation loss: 2.5916012461467455

Epoch: 5| Step: 2
Training loss: 2.59869384765625
Validation loss: 2.5986069005022765

Epoch: 5| Step: 3
Training loss: 2.3078229427337646
Validation loss: 2.598112895924558

Epoch: 5| Step: 4
Training loss: 2.992457866668701
Validation loss: 2.598100059775896

Epoch: 5| Step: 5
Training loss: 2.701972484588623
Validation loss: 2.5959469938790924

Epoch: 5| Step: 6
Training loss: 2.8091039657592773
Validation loss: 2.597478587140319

Epoch: 5| Step: 7
Training loss: 3.406651258468628
Validation loss: 2.595831422395604

Epoch: 5| Step: 8
Training loss: 2.9712812900543213
Validation loss: 2.591982477454729

Epoch: 5| Step: 9
Training loss: 2.302873134613037
Validation loss: 2.5952630325030257

Epoch: 5| Step: 10
Training loss: 3.004751682281494
Validation loss: 2.5910198098869732

Epoch: 62| Step: 0
Training loss: 4.037203788757324
Validation loss: 2.598420989128851

Epoch: 5| Step: 1
Training loss: 2.616548538208008
Validation loss: 2.602799912934662

Epoch: 5| Step: 2
Training loss: 2.939239978790283
Validation loss: 2.60581850236462

Epoch: 5| Step: 3
Training loss: 2.706514835357666
Validation loss: 2.605726985521214

Epoch: 5| Step: 4
Training loss: 2.7286229133605957
Validation loss: 2.602384828752087

Epoch: 5| Step: 5
Training loss: 2.6915993690490723
Validation loss: 2.595569743905016

Epoch: 5| Step: 6
Training loss: 2.5350823402404785
Validation loss: 2.592288145454981

Epoch: 5| Step: 7
Training loss: 2.1762771606445312
Validation loss: 2.587120053588703

Epoch: 5| Step: 8
Training loss: 2.774726152420044
Validation loss: 2.5905595364109164

Epoch: 5| Step: 9
Training loss: 2.654391288757324
Validation loss: 2.592416017286239

Epoch: 5| Step: 10
Training loss: 2.726332426071167
Validation loss: 2.5971857296523226

Epoch: 63| Step: 0
Training loss: 3.1741487979888916
Validation loss: 2.5990789885162027

Epoch: 5| Step: 1
Training loss: 2.4698519706726074
Validation loss: 2.5991805497036187

Epoch: 5| Step: 2
Training loss: 2.505143165588379
Validation loss: 2.593892179509645

Epoch: 5| Step: 3
Training loss: 2.205596446990967
Validation loss: 2.588641917833718

Epoch: 5| Step: 4
Training loss: 2.869939088821411
Validation loss: 2.5859906699067805

Epoch: 5| Step: 5
Training loss: 2.9682493209838867
Validation loss: 2.587071164961784

Epoch: 5| Step: 6
Training loss: 2.8723366260528564
Validation loss: 2.5862951740141837

Epoch: 5| Step: 7
Training loss: 2.2082390785217285
Validation loss: 2.584324115066118

Epoch: 5| Step: 8
Training loss: 2.6401054859161377
Validation loss: 2.587263717446276

Epoch: 5| Step: 9
Training loss: 3.140848159790039
Validation loss: 2.5879944396275345

Epoch: 5| Step: 10
Training loss: 3.6608521938323975
Validation loss: 2.5845956597276913

Epoch: 64| Step: 0
Training loss: 2.2787225246429443
Validation loss: 2.589209989834857

Epoch: 5| Step: 1
Training loss: 3.0591492652893066
Validation loss: 2.5976098250317317

Epoch: 5| Step: 2
Training loss: 2.4205822944641113
Validation loss: 2.6005330457482287

Epoch: 5| Step: 3
Training loss: 2.8520734310150146
Validation loss: 2.5866195873547624

Epoch: 5| Step: 4
Training loss: 3.391709804534912
Validation loss: 2.5861704477699856

Epoch: 5| Step: 5
Training loss: 2.2334487438201904
Validation loss: 2.5837515400302027

Epoch: 5| Step: 6
Training loss: 2.6289005279541016
Validation loss: 2.57900062940454

Epoch: 5| Step: 7
Training loss: 3.2656302452087402
Validation loss: 2.579174851858488

Epoch: 5| Step: 8
Training loss: 2.852639675140381
Validation loss: 2.5791499794170423

Epoch: 5| Step: 9
Training loss: 2.9613890647888184
Validation loss: 2.578563479967015

Epoch: 5| Step: 10
Training loss: 2.524847984313965
Validation loss: 2.5760427803121586

Epoch: 65| Step: 0
Training loss: 2.8767199516296387
Validation loss: 2.576348222712035

Epoch: 5| Step: 1
Training loss: 3.1268365383148193
Validation loss: 2.5800546317972164

Epoch: 5| Step: 2
Training loss: 2.3736791610717773
Validation loss: 2.5782363978765344

Epoch: 5| Step: 3
Training loss: 2.6816651821136475
Validation loss: 2.5747168474299933

Epoch: 5| Step: 4
Training loss: 2.529202938079834
Validation loss: 2.5823959894077753

Epoch: 5| Step: 5
Training loss: 2.1101584434509277
Validation loss: 2.569460840635402

Epoch: 5| Step: 6
Training loss: 3.1159780025482178
Validation loss: 2.572701836145052

Epoch: 5| Step: 7
Training loss: 2.5492148399353027
Validation loss: 2.577105633674129

Epoch: 5| Step: 8
Training loss: 3.3580639362335205
Validation loss: 2.571427504221598

Epoch: 5| Step: 9
Training loss: 2.8662590980529785
Validation loss: 2.5731587448427753

Epoch: 5| Step: 10
Training loss: 2.811065912246704
Validation loss: 2.5760815963950208

Epoch: 66| Step: 0
Training loss: 3.11480712890625
Validation loss: 2.5843749277053343

Epoch: 5| Step: 1
Training loss: 3.00868558883667
Validation loss: 2.5920877636119886

Epoch: 5| Step: 2
Training loss: 2.1592764854431152
Validation loss: 2.57511313756307

Epoch: 5| Step: 3
Training loss: 2.529898166656494
Validation loss: 2.5752675046202955

Epoch: 5| Step: 4
Training loss: 3.3101730346679688
Validation loss: 2.571337056416337

Epoch: 5| Step: 5
Training loss: 2.9047584533691406
Validation loss: 2.571872562490484

Epoch: 5| Step: 6
Training loss: 2.261913299560547
Validation loss: 2.568279477857774

Epoch: 5| Step: 7
Training loss: 3.269882917404175
Validation loss: 2.5688668169001097

Epoch: 5| Step: 8
Training loss: 2.354654312133789
Validation loss: 2.573195775349935

Epoch: 5| Step: 9
Training loss: 2.5875401496887207
Validation loss: 2.570358673731486

Epoch: 5| Step: 10
Training loss: 2.9509899616241455
Validation loss: 2.572078638179328

Epoch: 67| Step: 0
Training loss: 3.1140408515930176
Validation loss: 2.5691335739627963

Epoch: 5| Step: 1
Training loss: 2.872424602508545
Validation loss: 2.571405287711851

Epoch: 5| Step: 2
Training loss: 2.277998208999634
Validation loss: 2.569512718467302

Epoch: 5| Step: 3
Training loss: 3.1663899421691895
Validation loss: 2.5717404221975677

Epoch: 5| Step: 4
Training loss: 2.295875072479248
Validation loss: 2.5745649107040895

Epoch: 5| Step: 5
Training loss: 2.772995710372925
Validation loss: 2.5864123144457416

Epoch: 5| Step: 6
Training loss: 3.304180860519409
Validation loss: 2.5884012124871694

Epoch: 5| Step: 7
Training loss: 2.5769832134246826
Validation loss: 2.5885145587305867

Epoch: 5| Step: 8
Training loss: 3.1654601097106934
Validation loss: 2.5807433333448184

Epoch: 5| Step: 9
Training loss: 2.060828685760498
Validation loss: 2.5735692465177147

Epoch: 5| Step: 10
Training loss: 2.7860140800476074
Validation loss: 2.569639593042353

Epoch: 68| Step: 0
Training loss: 3.5920474529266357
Validation loss: 2.5682887287550074

Epoch: 5| Step: 1
Training loss: 2.819542407989502
Validation loss: 2.566607367607855

Epoch: 5| Step: 2
Training loss: 2.4407098293304443
Validation loss: 2.570452179960025

Epoch: 5| Step: 3
Training loss: 2.176088333129883
Validation loss: 2.5694736819113455

Epoch: 5| Step: 4
Training loss: 3.094728708267212
Validation loss: 2.570136013851371

Epoch: 5| Step: 5
Training loss: 2.549074411392212
Validation loss: 2.565475256212296

Epoch: 5| Step: 6
Training loss: 2.4113755226135254
Validation loss: 2.5633480984677552

Epoch: 5| Step: 7
Training loss: 3.316254138946533
Validation loss: 2.5632813489565285

Epoch: 5| Step: 8
Training loss: 2.303269624710083
Validation loss: 2.562027559485487

Epoch: 5| Step: 9
Training loss: 2.4395673274993896
Validation loss: 2.562440597882835

Epoch: 5| Step: 10
Training loss: 3.234448194503784
Validation loss: 2.5655601024627686

Epoch: 69| Step: 0
Training loss: 2.6111080646514893
Validation loss: 2.5728398420477427

Epoch: 5| Step: 1
Training loss: 3.1048800945281982
Validation loss: 2.568879258248114

Epoch: 5| Step: 2
Training loss: 2.757969379425049
Validation loss: 2.5710258817160003

Epoch: 5| Step: 3
Training loss: 2.8233642578125
Validation loss: 2.570248867875786

Epoch: 5| Step: 4
Training loss: 2.85884690284729
Validation loss: 2.5737024609760573

Epoch: 5| Step: 5
Training loss: 3.166626453399658
Validation loss: 2.570444599274666

Epoch: 5| Step: 6
Training loss: 2.3403234481811523
Validation loss: 2.5755397504375828

Epoch: 5| Step: 7
Training loss: 2.175769090652466
Validation loss: 2.5630554101800405

Epoch: 5| Step: 8
Training loss: 2.3479456901550293
Validation loss: 2.556924266199912

Epoch: 5| Step: 9
Training loss: 2.6651577949523926
Validation loss: 2.556327394259873

Epoch: 5| Step: 10
Training loss: 3.4916794300079346
Validation loss: 2.5591088571856098

Epoch: 70| Step: 0
Training loss: 2.585773229598999
Validation loss: 2.568126896376251

Epoch: 5| Step: 1
Training loss: 2.709099769592285
Validation loss: 2.5723727198057276

Epoch: 5| Step: 2
Training loss: 2.277139902114868
Validation loss: 2.5767898098115

Epoch: 5| Step: 3
Training loss: 3.220531463623047
Validation loss: 2.574355028008902

Epoch: 5| Step: 4
Training loss: 2.952073812484741
Validation loss: 2.57820217071041

Epoch: 5| Step: 5
Training loss: 3.1304616928100586
Validation loss: 2.5787384304949033

Epoch: 5| Step: 6
Training loss: 2.440072536468506
Validation loss: 2.576205643274451

Epoch: 5| Step: 7
Training loss: 3.039292573928833
Validation loss: 2.5735647088737896

Epoch: 5| Step: 8
Training loss: 3.3066916465759277
Validation loss: 2.564212642690187

Epoch: 5| Step: 9
Training loss: 2.6753714084625244
Validation loss: 2.557171521648284

Epoch: 5| Step: 10
Training loss: 1.953637957572937
Validation loss: 2.5523058393950104

Epoch: 71| Step: 0
Training loss: 2.9851553440093994
Validation loss: 2.5672032986917803

Epoch: 5| Step: 1
Training loss: 2.7838187217712402
Validation loss: 2.5759129088412047

Epoch: 5| Step: 2
Training loss: 2.5747880935668945
Validation loss: 2.582622789567517

Epoch: 5| Step: 3
Training loss: 2.868276596069336
Validation loss: 2.6003681049552014

Epoch: 5| Step: 4
Training loss: 2.625676155090332
Validation loss: 2.5885998331090456

Epoch: 5| Step: 5
Training loss: 3.0155880451202393
Validation loss: 2.5668483831549205

Epoch: 5| Step: 6
Training loss: 2.556391954421997
Validation loss: 2.545686896129321

Epoch: 5| Step: 7
Training loss: 2.577786684036255
Validation loss: 2.5447289687331005

Epoch: 5| Step: 8
Training loss: 2.9085891246795654
Validation loss: 2.5487412457825034

Epoch: 5| Step: 9
Training loss: 2.4420130252838135
Validation loss: 2.554868162319224

Epoch: 5| Step: 10
Training loss: 3.075221538543701
Validation loss: 2.5575112681235037

Epoch: 72| Step: 0
Training loss: 2.8074448108673096
Validation loss: 2.5589999075858825

Epoch: 5| Step: 1
Training loss: 3.0430657863616943
Validation loss: 2.555602929925406

Epoch: 5| Step: 2
Training loss: 2.734449863433838
Validation loss: 2.557791079244306

Epoch: 5| Step: 3
Training loss: 2.246544361114502
Validation loss: 2.5585411569123626

Epoch: 5| Step: 4
Training loss: 3.0053772926330566
Validation loss: 2.555936531354022

Epoch: 5| Step: 5
Training loss: 2.4093992710113525
Validation loss: 2.5584542520584597

Epoch: 5| Step: 6
Training loss: 2.7413623332977295
Validation loss: 2.5469039896483063

Epoch: 5| Step: 7
Training loss: 2.6030967235565186
Validation loss: 2.5407469682796027

Epoch: 5| Step: 8
Training loss: 3.2012572288513184
Validation loss: 2.5411445133147703

Epoch: 5| Step: 9
Training loss: 2.7800753116607666
Validation loss: 2.5415025757205103

Epoch: 5| Step: 10
Training loss: 2.592327356338501
Validation loss: 2.5373095517517417

Epoch: 73| Step: 0
Training loss: 2.863703489303589
Validation loss: 2.533493862357191

Epoch: 5| Step: 1
Training loss: 2.2226600646972656
Validation loss: 2.5374376902016262

Epoch: 5| Step: 2
Training loss: 2.499147891998291
Validation loss: 2.5398500042576946

Epoch: 5| Step: 3
Training loss: 2.3275363445281982
Validation loss: 2.5400134773664576

Epoch: 5| Step: 4
Training loss: 3.091646671295166
Validation loss: 2.5370585354425574

Epoch: 5| Step: 5
Training loss: 3.08984375
Validation loss: 2.5413546639104045

Epoch: 5| Step: 6
Training loss: 2.554511785507202
Validation loss: 2.538290762132214

Epoch: 5| Step: 7
Training loss: 2.8528120517730713
Validation loss: 2.538846592749319

Epoch: 5| Step: 8
Training loss: 2.7812559604644775
Validation loss: 2.5288173870373796

Epoch: 5| Step: 9
Training loss: 2.971677303314209
Validation loss: 2.5272252636571086

Epoch: 5| Step: 10
Training loss: 2.808397054672241
Validation loss: 2.527389705822032

Epoch: 74| Step: 0
Training loss: 1.803222417831421
Validation loss: 2.5263859687312955

Epoch: 5| Step: 1
Training loss: 3.56669545173645
Validation loss: 2.524715679948048

Epoch: 5| Step: 2
Training loss: 2.7075233459472656
Validation loss: 2.5270686252142793

Epoch: 5| Step: 3
Training loss: 2.771246910095215
Validation loss: 2.525417681663267

Epoch: 5| Step: 4
Training loss: 2.2327041625976562
Validation loss: 2.523305336634318

Epoch: 5| Step: 5
Training loss: 2.9783263206481934
Validation loss: 2.521788722725325

Epoch: 5| Step: 6
Training loss: 2.752537488937378
Validation loss: 2.5217492042049283

Epoch: 5| Step: 7
Training loss: 3.3194003105163574
Validation loss: 2.5218744252317693

Epoch: 5| Step: 8
Training loss: 2.2828333377838135
Validation loss: 2.5235545583950576

Epoch: 5| Step: 9
Training loss: 2.5914275646209717
Validation loss: 2.5196929285603185

Epoch: 5| Step: 10
Training loss: 2.9834423065185547
Validation loss: 2.5182297819404194

Epoch: 75| Step: 0
Training loss: 2.727935314178467
Validation loss: 2.5177471740271455

Epoch: 5| Step: 1
Training loss: 2.4372332096099854
Validation loss: 2.516521192366077

Epoch: 5| Step: 2
Training loss: 3.176517963409424
Validation loss: 2.520061530092711

Epoch: 5| Step: 3
Training loss: 2.5201547145843506
Validation loss: 2.5162926130397345

Epoch: 5| Step: 4
Training loss: 3.238173246383667
Validation loss: 2.52040119324961

Epoch: 5| Step: 5
Training loss: 2.1560733318328857
Validation loss: 2.5180326020845802

Epoch: 5| Step: 6
Training loss: 2.4238269329071045
Validation loss: 2.5158035473157

Epoch: 5| Step: 7
Training loss: 2.869957447052002
Validation loss: 2.5160666332449964

Epoch: 5| Step: 8
Training loss: 2.5014712810516357
Validation loss: 2.5170501509020404

Epoch: 5| Step: 9
Training loss: 3.35786509513855
Validation loss: 2.5137155055999756

Epoch: 5| Step: 10
Training loss: 2.4574697017669678
Validation loss: 2.519443522217453

Epoch: 76| Step: 0
Training loss: 2.221832275390625
Validation loss: 2.5193796183473323

Epoch: 5| Step: 1
Training loss: 2.8386924266815186
Validation loss: 2.5128227536396315

Epoch: 5| Step: 2
Training loss: 2.906449317932129
Validation loss: 2.51481730194502

Epoch: 5| Step: 3
Training loss: 2.5936086177825928
Validation loss: 2.515207226558398

Epoch: 5| Step: 4
Training loss: 3.121394634246826
Validation loss: 2.5197376743439706

Epoch: 5| Step: 5
Training loss: 3.2913870811462402
Validation loss: 2.516035005610476

Epoch: 5| Step: 6
Training loss: 2.178034543991089
Validation loss: 2.5224040887689076

Epoch: 5| Step: 7
Training loss: 2.5089335441589355
Validation loss: 2.5194426480159966

Epoch: 5| Step: 8
Training loss: 2.7939655780792236
Validation loss: 2.5208656762235906

Epoch: 5| Step: 9
Training loss: 2.4318270683288574
Validation loss: 2.5278052104416715

Epoch: 5| Step: 10
Training loss: 3.096198558807373
Validation loss: 2.51652132567539

Epoch: 77| Step: 0
Training loss: 3.2458653450012207
Validation loss: 2.5165561809334704

Epoch: 5| Step: 1
Training loss: 2.660820484161377
Validation loss: 2.512626132657451

Epoch: 5| Step: 2
Training loss: 2.525041103363037
Validation loss: 2.513737755437051

Epoch: 5| Step: 3
Training loss: 3.0142533779144287
Validation loss: 2.519577908259566

Epoch: 5| Step: 4
Training loss: 2.8875174522399902
Validation loss: 2.5181678315644622

Epoch: 5| Step: 5
Training loss: 1.9874460697174072
Validation loss: 2.518914871318366

Epoch: 5| Step: 6
Training loss: 2.561985492706299
Validation loss: 2.5176905560237106

Epoch: 5| Step: 7
Training loss: 2.8621113300323486
Validation loss: 2.5131508611863658

Epoch: 5| Step: 8
Training loss: 2.448057174682617
Validation loss: 2.512924937791722

Epoch: 5| Step: 9
Training loss: 2.906928300857544
Validation loss: 2.5086181009969404

Epoch: 5| Step: 10
Training loss: 2.8540258407592773
Validation loss: 2.5115886298559045

Epoch: 78| Step: 0
Training loss: 2.899576187133789
Validation loss: 2.5180292462789886

Epoch: 5| Step: 1
Training loss: 2.8479528427124023
Validation loss: 2.514706059168744

Epoch: 5| Step: 2
Training loss: 2.31467342376709
Validation loss: 2.5224216035617295

Epoch: 5| Step: 3
Training loss: 2.399075984954834
Validation loss: 2.5197973738434496

Epoch: 5| Step: 4
Training loss: 3.649543046951294
Validation loss: 2.510675089333647

Epoch: 5| Step: 5
Training loss: 2.6125991344451904
Validation loss: 2.5167795022328696

Epoch: 5| Step: 6
Training loss: 2.858990430831909
Validation loss: 2.5214572004092637

Epoch: 5| Step: 7
Training loss: 2.620208740234375
Validation loss: 2.5257328274429485

Epoch: 5| Step: 8
Training loss: 2.7191853523254395
Validation loss: 2.5178053917423373

Epoch: 5| Step: 9
Training loss: 2.174940586090088
Validation loss: 2.527353786653088

Epoch: 5| Step: 10
Training loss: 2.733588933944702
Validation loss: 2.5260538131959978

Epoch: 79| Step: 0
Training loss: 2.0216686725616455
Validation loss: 2.5173886258115052

Epoch: 5| Step: 1
Training loss: 2.613067865371704
Validation loss: 2.514063791562152

Epoch: 5| Step: 2
Training loss: 2.9197559356689453
Validation loss: 2.5160162474519465

Epoch: 5| Step: 3
Training loss: 2.8054606914520264
Validation loss: 2.5128858576538744

Epoch: 5| Step: 4
Training loss: 3.3204474449157715
Validation loss: 2.513345049273583

Epoch: 5| Step: 5
Training loss: 2.7588746547698975
Validation loss: 2.5110246340433755

Epoch: 5| Step: 6
Training loss: 2.660796642303467
Validation loss: 2.5083254204001477

Epoch: 5| Step: 7
Training loss: 2.5842232704162598
Validation loss: 2.51088729212361

Epoch: 5| Step: 8
Training loss: 2.650935649871826
Validation loss: 2.5092653536027476

Epoch: 5| Step: 9
Training loss: 2.5543980598449707
Validation loss: 2.503491838773092

Epoch: 5| Step: 10
Training loss: 2.9171791076660156
Validation loss: 2.504569443323279

Epoch: 80| Step: 0
Training loss: 2.3748221397399902
Validation loss: 2.5109935063187794

Epoch: 5| Step: 1
Training loss: 2.2961108684539795
Validation loss: 2.5120709044958955

Epoch: 5| Step: 2
Training loss: 2.7476983070373535
Validation loss: 2.521706978480021

Epoch: 5| Step: 3
Training loss: 2.70078706741333
Validation loss: 2.5144619890438613

Epoch: 5| Step: 4
Training loss: 2.6821770668029785
Validation loss: 2.513262277008385

Epoch: 5| Step: 5
Training loss: 2.918560743331909
Validation loss: 2.50771693260439

Epoch: 5| Step: 6
Training loss: 1.966758131980896
Validation loss: 2.506956010736445

Epoch: 5| Step: 7
Training loss: 3.5583877563476562
Validation loss: 2.5070494785103747

Epoch: 5| Step: 8
Training loss: 3.0821995735168457
Validation loss: 2.506877709460515

Epoch: 5| Step: 9
Training loss: 2.6248867511749268
Validation loss: 2.5038608812516734

Epoch: 5| Step: 10
Training loss: 2.85599422454834
Validation loss: 2.502697188367126

Epoch: 81| Step: 0
Training loss: 1.9905287027359009
Validation loss: 2.502217420967676

Epoch: 5| Step: 1
Training loss: 3.089556932449341
Validation loss: 2.510591378775976

Epoch: 5| Step: 2
Training loss: 2.670734167098999
Validation loss: 2.519323436162805

Epoch: 5| Step: 3
Training loss: 2.879150390625
Validation loss: 2.51764319532661

Epoch: 5| Step: 4
Training loss: 2.8789496421813965
Validation loss: 2.538395825252738

Epoch: 5| Step: 5
Training loss: 2.7504048347473145
Validation loss: 2.523584586317821

Epoch: 5| Step: 6
Training loss: 2.635554075241089
Validation loss: 2.5168498280227825

Epoch: 5| Step: 7
Training loss: 2.5806386470794678
Validation loss: 2.5099656992061163

Epoch: 5| Step: 8
Training loss: 2.7238831520080566
Validation loss: 2.500725548754456

Epoch: 5| Step: 9
Training loss: 2.3217806816101074
Validation loss: 2.5027513119482223

Epoch: 5| Step: 10
Training loss: 3.408328056335449
Validation loss: 2.494877689628191

Epoch: 82| Step: 0
Training loss: 3.127554416656494
Validation loss: 2.4985104042996644

Epoch: 5| Step: 1
Training loss: 2.4994750022888184
Validation loss: 2.4999002410519506

Epoch: 5| Step: 2
Training loss: 2.9493134021759033
Validation loss: 2.4982076383406118

Epoch: 5| Step: 3
Training loss: 3.5834898948669434
Validation loss: 2.5014691058025567

Epoch: 5| Step: 4
Training loss: 3.09083890914917
Validation loss: 2.501175365140361

Epoch: 5| Step: 5
Training loss: 2.3855605125427246
Validation loss: 2.497139161632907

Epoch: 5| Step: 6
Training loss: 2.2036101818084717
Validation loss: 2.494854234880017

Epoch: 5| Step: 7
Training loss: 2.615767002105713
Validation loss: 2.500881907760456

Epoch: 5| Step: 8
Training loss: 2.4505510330200195
Validation loss: 2.4963581536405828

Epoch: 5| Step: 9
Training loss: 2.555622100830078
Validation loss: 2.5083516438802085

Epoch: 5| Step: 10
Training loss: 2.166437864303589
Validation loss: 2.507633222046719

Epoch: 83| Step: 0
Training loss: 2.885881185531616
Validation loss: 2.5183732945431947

Epoch: 5| Step: 1
Training loss: 1.8815702199935913
Validation loss: 2.5166541684058403

Epoch: 5| Step: 2
Training loss: 3.1109893321990967
Validation loss: 2.508152548984815

Epoch: 5| Step: 3
Training loss: 2.744338274002075
Validation loss: 2.5030083630674627

Epoch: 5| Step: 4
Training loss: 2.74153733253479
Validation loss: 2.498426596323649

Epoch: 5| Step: 5
Training loss: 2.470930576324463
Validation loss: 2.4979692043796664

Epoch: 5| Step: 6
Training loss: 3.2697834968566895
Validation loss: 2.4981476863225303

Epoch: 5| Step: 7
Training loss: 2.1580214500427246
Validation loss: 2.4937593860010945

Epoch: 5| Step: 8
Training loss: 2.6696059703826904
Validation loss: 2.494963671571465

Epoch: 5| Step: 9
Training loss: 2.8679885864257812
Validation loss: 2.4935130496178903

Epoch: 5| Step: 10
Training loss: 2.9404008388519287
Validation loss: 2.489598981795772

Epoch: 84| Step: 0
Training loss: 2.7824432849884033
Validation loss: 2.491906632659256

Epoch: 5| Step: 1
Training loss: 2.5431065559387207
Validation loss: 2.489842014928018

Epoch: 5| Step: 2
Training loss: 2.7661805152893066
Validation loss: 2.4874508278344267

Epoch: 5| Step: 3
Training loss: 1.9751918315887451
Validation loss: 2.493369866442937

Epoch: 5| Step: 4
Training loss: 2.5560238361358643
Validation loss: 2.494973792824694

Epoch: 5| Step: 5
Training loss: 3.17031192779541
Validation loss: 2.5053979914675475

Epoch: 5| Step: 6
Training loss: 2.8923845291137695
Validation loss: 2.513887128522319

Epoch: 5| Step: 7
Training loss: 2.463385581970215
Validation loss: 2.514724164880732

Epoch: 5| Step: 8
Training loss: 2.569275379180908
Validation loss: 2.504707162098218

Epoch: 5| Step: 9
Training loss: 3.607661008834839
Validation loss: 2.5058871597372074

Epoch: 5| Step: 10
Training loss: 2.2798173427581787
Validation loss: 2.50195481956646

Epoch: 85| Step: 0
Training loss: 2.6040146350860596
Validation loss: 2.503294414089572

Epoch: 5| Step: 1
Training loss: 3.094269037246704
Validation loss: 2.5001412130171254

Epoch: 5| Step: 2
Training loss: 3.0302062034606934
Validation loss: 2.493963505632134

Epoch: 5| Step: 3
Training loss: 2.467478036880493
Validation loss: 2.4891687259879163

Epoch: 5| Step: 4
Training loss: 2.2227344512939453
Validation loss: 2.493483735669044

Epoch: 5| Step: 5
Training loss: 2.491417407989502
Validation loss: 2.497738278040322

Epoch: 5| Step: 6
Training loss: 2.906863212585449
Validation loss: 2.506708509178572

Epoch: 5| Step: 7
Training loss: 3.092844247817993
Validation loss: 2.507309644452987

Epoch: 5| Step: 8
Training loss: 3.3570914268493652
Validation loss: 2.5104990620766916

Epoch: 5| Step: 9
Training loss: 2.2430315017700195
Validation loss: 2.5077709023670485

Epoch: 5| Step: 10
Training loss: 2.217740297317505
Validation loss: 2.505271837275515

Epoch: 86| Step: 0
Training loss: 2.179246664047241
Validation loss: 2.500002799495574

Epoch: 5| Step: 1
Training loss: 2.7328670024871826
Validation loss: 2.4998201939367477

Epoch: 5| Step: 2
Training loss: 2.258126974105835
Validation loss: 2.4905434013694845

Epoch: 5| Step: 3
Training loss: 2.786146402359009
Validation loss: 2.484633976413358

Epoch: 5| Step: 4
Training loss: 3.201418399810791
Validation loss: 2.4892049527937368

Epoch: 5| Step: 5
Training loss: 2.792489767074585
Validation loss: 2.489930147765785

Epoch: 5| Step: 6
Training loss: 3.0637664794921875
Validation loss: 2.4849052583017657

Epoch: 5| Step: 7
Training loss: 1.6083450317382812
Validation loss: 2.4905050826329056

Epoch: 5| Step: 8
Training loss: 3.3132851123809814
Validation loss: 2.498970518830002

Epoch: 5| Step: 9
Training loss: 3.0359864234924316
Validation loss: 2.493620689197253

Epoch: 5| Step: 10
Training loss: 2.759275436401367
Validation loss: 2.49922602663758

Epoch: 87| Step: 0
Training loss: 2.978116750717163
Validation loss: 2.4956355889638266

Epoch: 5| Step: 1
Training loss: 3.151581048965454
Validation loss: 2.4966423306413876

Epoch: 5| Step: 2
Training loss: 2.618762731552124
Validation loss: 2.4976239588952835

Epoch: 5| Step: 3
Training loss: 2.5031464099884033
Validation loss: 2.4943915695272465

Epoch: 5| Step: 4
Training loss: 2.679633617401123
Validation loss: 2.4896016941275647

Epoch: 5| Step: 5
Training loss: 3.0039756298065186
Validation loss: 2.487978345604353

Epoch: 5| Step: 6
Training loss: 2.1067044734954834
Validation loss: 2.4818745223424767

Epoch: 5| Step: 7
Training loss: 2.9430959224700928
Validation loss: 2.487913323986915

Epoch: 5| Step: 8
Training loss: 2.3415470123291016
Validation loss: 2.4982698348260697

Epoch: 5| Step: 9
Training loss: 2.5837812423706055
Validation loss: 2.4991256421612156

Epoch: 5| Step: 10
Training loss: 2.8225371837615967
Validation loss: 2.4968283073876494

Epoch: 88| Step: 0
Training loss: 2.3682029247283936
Validation loss: 2.4888598534368698

Epoch: 5| Step: 1
Training loss: 2.7781529426574707
Validation loss: 2.4872071127737723

Epoch: 5| Step: 2
Training loss: 3.081897258758545
Validation loss: 2.489947900977186

Epoch: 5| Step: 3
Training loss: 2.3950588703155518
Validation loss: 2.508574644724528

Epoch: 5| Step: 4
Training loss: 2.4752161502838135
Validation loss: 2.506954426406532

Epoch: 5| Step: 5
Training loss: 2.5386765003204346
Validation loss: 2.5100979317900953

Epoch: 5| Step: 6
Training loss: 2.52455472946167
Validation loss: 2.4997636707880164

Epoch: 5| Step: 7
Training loss: 2.860373020172119
Validation loss: 2.4892087495455177

Epoch: 5| Step: 8
Training loss: 2.9456048011779785
Validation loss: 2.4849565388053976

Epoch: 5| Step: 9
Training loss: 2.943901777267456
Validation loss: 2.4853024816000335

Epoch: 5| Step: 10
Training loss: 2.792255163192749
Validation loss: 2.48174447654396

Epoch: 89| Step: 0
Training loss: 2.929117441177368
Validation loss: 2.4846462793247674

Epoch: 5| Step: 1
Training loss: 2.358212947845459
Validation loss: 2.4924183327664613

Epoch: 5| Step: 2
Training loss: 2.843257427215576
Validation loss: 2.4871075025168796

Epoch: 5| Step: 3
Training loss: 2.2700068950653076
Validation loss: 2.4912363431786977

Epoch: 5| Step: 4
Training loss: 2.689387559890747
Validation loss: 2.4964186350504556

Epoch: 5| Step: 5
Training loss: 2.2756268978118896
Validation loss: 2.498982360286097

Epoch: 5| Step: 6
Training loss: 2.8200910091400146
Validation loss: 2.5006501495197253

Epoch: 5| Step: 7
Training loss: 2.8488173484802246
Validation loss: 2.499567408715525

Epoch: 5| Step: 8
Training loss: 2.391296863555908
Validation loss: 2.5072180942822526

Epoch: 5| Step: 9
Training loss: 2.877310037612915
Validation loss: 2.4871982964136268

Epoch: 5| Step: 10
Training loss: 3.508979082107544
Validation loss: 2.4836440086364746

Epoch: 90| Step: 0
Training loss: 2.7876672744750977
Validation loss: 2.4787017299282934

Epoch: 5| Step: 1
Training loss: 2.8181042671203613
Validation loss: 2.48125518778319

Epoch: 5| Step: 2
Training loss: 2.3866868019104004
Validation loss: 2.4803320246358074

Epoch: 5| Step: 3
Training loss: 1.8391062021255493
Validation loss: 2.4841326282870386

Epoch: 5| Step: 4
Training loss: 3.6771209239959717
Validation loss: 2.490543270623812

Epoch: 5| Step: 5
Training loss: 2.901284694671631
Validation loss: 2.4866826047179518

Epoch: 5| Step: 6
Training loss: 2.5317842960357666
Validation loss: 2.4856956440915345

Epoch: 5| Step: 7
Training loss: 3.2878379821777344
Validation loss: 2.484831066541774

Epoch: 5| Step: 8
Training loss: 3.057217597961426
Validation loss: 2.4842024285306215

Epoch: 5| Step: 9
Training loss: 2.0919394493103027
Validation loss: 2.4821025094678326

Epoch: 5| Step: 10
Training loss: 2.2126083374023438
Validation loss: 2.482302286291635

Epoch: 91| Step: 0
Training loss: 2.4578495025634766
Validation loss: 2.4910624257979856

Epoch: 5| Step: 1
Training loss: 2.802260160446167
Validation loss: 2.497951694714126

Epoch: 5| Step: 2
Training loss: 3.2952704429626465
Validation loss: 2.515291457535118

Epoch: 5| Step: 3
Training loss: 2.8465118408203125
Validation loss: 2.515907738798408

Epoch: 5| Step: 4
Training loss: 2.148881435394287
Validation loss: 2.5151400040554743

Epoch: 5| Step: 5
Training loss: 2.9638264179229736
Validation loss: 2.500733883150162

Epoch: 5| Step: 6
Training loss: 2.7846462726593018
Validation loss: 2.491333556431596

Epoch: 5| Step: 7
Training loss: 2.2125024795532227
Validation loss: 2.485199587319487

Epoch: 5| Step: 8
Training loss: 2.9550206661224365
Validation loss: 2.484449780115517

Epoch: 5| Step: 9
Training loss: 2.4179344177246094
Validation loss: 2.4784038271955264

Epoch: 5| Step: 10
Training loss: 2.7573628425598145
Validation loss: 2.478019122154482

Epoch: 92| Step: 0
Training loss: 2.133201837539673
Validation loss: 2.477703271373626

Epoch: 5| Step: 1
Training loss: 2.622025728225708
Validation loss: 2.4777344170437066

Epoch: 5| Step: 2
Training loss: 2.5074243545532227
Validation loss: 2.4811642990317395

Epoch: 5| Step: 3
Training loss: 2.5203707218170166
Validation loss: 2.479912761718996

Epoch: 5| Step: 4
Training loss: 3.339824676513672
Validation loss: 2.479545283061202

Epoch: 5| Step: 5
Training loss: 2.4599361419677734
Validation loss: 2.4776154487363753

Epoch: 5| Step: 6
Training loss: 3.322598695755005
Validation loss: 2.487756267670662

Epoch: 5| Step: 7
Training loss: 3.2929909229278564
Validation loss: 2.48229052687204

Epoch: 5| Step: 8
Training loss: 2.6498939990997314
Validation loss: 2.4849454895142586

Epoch: 5| Step: 9
Training loss: 2.202934980392456
Validation loss: 2.4842898525217527

Epoch: 5| Step: 10
Training loss: 2.4993255138397217
Validation loss: 2.488700323207404

Epoch: 93| Step: 0
Training loss: 2.8383474349975586
Validation loss: 2.4876389862388693

Epoch: 5| Step: 1
Training loss: 3.38283109664917
Validation loss: 2.476879681310346

Epoch: 5| Step: 2
Training loss: 2.3005611896514893
Validation loss: 2.4736548726276686

Epoch: 5| Step: 3
Training loss: 2.962808132171631
Validation loss: 2.476666388973113

Epoch: 5| Step: 4
Training loss: 2.5383002758026123
Validation loss: 2.472507563970422

Epoch: 5| Step: 5
Training loss: 2.3646368980407715
Validation loss: 2.4803039719981532

Epoch: 5| Step: 6
Training loss: 2.715366840362549
Validation loss: 2.4753075850907194

Epoch: 5| Step: 7
Training loss: 2.7459797859191895
Validation loss: 2.473905832536759

Epoch: 5| Step: 8
Training loss: 2.4840540885925293
Validation loss: 2.4777263569575485

Epoch: 5| Step: 9
Training loss: 2.3566088676452637
Validation loss: 2.477713164462838

Epoch: 5| Step: 10
Training loss: 2.866842031478882
Validation loss: 2.4769258858055196

Epoch: 94| Step: 0
Training loss: 2.588038206100464
Validation loss: 2.4869476108140844

Epoch: 5| Step: 1
Training loss: 2.4917194843292236
Validation loss: 2.4897401050854753

Epoch: 5| Step: 2
Training loss: 3.398366928100586
Validation loss: 2.479612540173274

Epoch: 5| Step: 3
Training loss: 2.8407702445983887
Validation loss: 2.4840244554704234

Epoch: 5| Step: 4
Training loss: 2.515894651412964
Validation loss: 2.4775534291421213

Epoch: 5| Step: 5
Training loss: 2.7216906547546387
Validation loss: 2.4728294739159207

Epoch: 5| Step: 6
Training loss: 2.473426342010498
Validation loss: 2.4748293507483696

Epoch: 5| Step: 7
Training loss: 2.7359619140625
Validation loss: 2.476704200108846

Epoch: 5| Step: 8
Training loss: 2.349153995513916
Validation loss: 2.4849702747919227

Epoch: 5| Step: 9
Training loss: 2.7005515098571777
Validation loss: 2.48290245250989

Epoch: 5| Step: 10
Training loss: 2.757486343383789
Validation loss: 2.4840519351343953

Epoch: 95| Step: 0
Training loss: 2.2017104625701904
Validation loss: 2.4792563479433776

Epoch: 5| Step: 1
Training loss: 3.2501564025878906
Validation loss: 2.4768227479791127

Epoch: 5| Step: 2
Training loss: 2.529059410095215
Validation loss: 2.4791421377530662

Epoch: 5| Step: 3
Training loss: 3.3980679512023926
Validation loss: 2.476748048618276

Epoch: 5| Step: 4
Training loss: 2.1788158416748047
Validation loss: 2.475792605389831

Epoch: 5| Step: 5
Training loss: 3.2292587757110596
Validation loss: 2.475350736289896

Epoch: 5| Step: 6
Training loss: 2.562622308731079
Validation loss: 2.4849869717833815

Epoch: 5| Step: 7
Training loss: 2.4510083198547363
Validation loss: 2.479331744614468

Epoch: 5| Step: 8
Training loss: 2.075385093688965
Validation loss: 2.4784973564968316

Epoch: 5| Step: 9
Training loss: 2.6417269706726074
Validation loss: 2.4816770733043714

Epoch: 5| Step: 10
Training loss: 3.053439140319824
Validation loss: 2.485099166952154

Epoch: 96| Step: 0
Training loss: 2.1749470233917236
Validation loss: 2.476796570644584

Epoch: 5| Step: 1
Training loss: 2.8914542198181152
Validation loss: 2.4894659109013055

Epoch: 5| Step: 2
Training loss: 2.7537121772766113
Validation loss: 2.4868295141445693

Epoch: 5| Step: 3
Training loss: 3.1507465839385986
Validation loss: 2.490976372072774

Epoch: 5| Step: 4
Training loss: 2.5476577281951904
Validation loss: 2.4933273356447936

Epoch: 5| Step: 5
Training loss: 3.0915098190307617
Validation loss: 2.4861550587479786

Epoch: 5| Step: 6
Training loss: 2.4569296836853027
Validation loss: 2.4950108630682832

Epoch: 5| Step: 7
Training loss: 3.021965503692627
Validation loss: 2.483984931822746

Epoch: 5| Step: 8
Training loss: 2.0306522846221924
Validation loss: 2.479813814163208

Epoch: 5| Step: 9
Training loss: 2.8678741455078125
Validation loss: 2.474752608165946

Epoch: 5| Step: 10
Training loss: 2.5312957763671875
Validation loss: 2.4697195201791744

Epoch: 97| Step: 0
Training loss: 3.6364803314208984
Validation loss: 2.469651909284694

Epoch: 5| Step: 1
Training loss: 3.2028422355651855
Validation loss: 2.4710988921503865

Epoch: 5| Step: 2
Training loss: 2.593221664428711
Validation loss: 2.4739123108566448

Epoch: 5| Step: 3
Training loss: 2.423327684402466
Validation loss: 2.4698635083372875

Epoch: 5| Step: 4
Training loss: 2.3902363777160645
Validation loss: 2.468248913365026

Epoch: 5| Step: 5
Training loss: 2.8413593769073486
Validation loss: 2.4712357213420253

Epoch: 5| Step: 6
Training loss: 2.087271213531494
Validation loss: 2.464392585139121

Epoch: 5| Step: 7
Training loss: 3.5759453773498535
Validation loss: 2.467424882355557

Epoch: 5| Step: 8
Training loss: 2.1953518390655518
Validation loss: 2.468218216332056

Epoch: 5| Step: 9
Training loss: 2.318676233291626
Validation loss: 2.4637893297339

Epoch: 5| Step: 10
Training loss: 2.2000834941864014
Validation loss: 2.4660477458789782

Epoch: 98| Step: 0
Training loss: 2.0384700298309326
Validation loss: 2.4605933132991997

Epoch: 5| Step: 1
Training loss: 3.0586228370666504
Validation loss: 2.471857088868336

Epoch: 5| Step: 2
Training loss: 2.6485347747802734
Validation loss: 2.475626584022276

Epoch: 5| Step: 3
Training loss: 2.0127978324890137
Validation loss: 2.4745330836183284

Epoch: 5| Step: 4
Training loss: 2.444671392440796
Validation loss: 2.475790416040728

Epoch: 5| Step: 5
Training loss: 3.136295795440674
Validation loss: 2.468767519920103

Epoch: 5| Step: 6
Training loss: 3.147073745727539
Validation loss: 2.4664594947650866

Epoch: 5| Step: 7
Training loss: 2.300305128097534
Validation loss: 2.4658122293410765

Epoch: 5| Step: 8
Training loss: 2.3896260261535645
Validation loss: 2.464815537134806

Epoch: 5| Step: 9
Training loss: 3.4669718742370605
Validation loss: 2.4723039057946976

Epoch: 5| Step: 10
Training loss: 2.8987767696380615
Validation loss: 2.4684414581585954

Epoch: 99| Step: 0
Training loss: 2.8466479778289795
Validation loss: 2.471215307071645

Epoch: 5| Step: 1
Training loss: 2.304969310760498
Validation loss: 2.4708264489327707

Epoch: 5| Step: 2
Training loss: 3.2079532146453857
Validation loss: 2.472828244650236

Epoch: 5| Step: 3
Training loss: 2.3504996299743652
Validation loss: 2.474145091989989

Epoch: 5| Step: 4
Training loss: 2.62186861038208
Validation loss: 2.4663212222437703

Epoch: 5| Step: 5
Training loss: 2.326810121536255
Validation loss: 2.4685845093060563

Epoch: 5| Step: 6
Training loss: 2.2573680877685547
Validation loss: 2.467624407942577

Epoch: 5| Step: 7
Training loss: 2.550710439682007
Validation loss: 2.4634223279132637

Epoch: 5| Step: 8
Training loss: 2.9325225353240967
Validation loss: 2.466287787242602

Epoch: 5| Step: 9
Training loss: 2.696509599685669
Validation loss: 2.46823836142017

Epoch: 5| Step: 10
Training loss: 3.41858172416687
Validation loss: 2.465573054487987

Epoch: 100| Step: 0
Training loss: 3.0337443351745605
Validation loss: 2.4631642654377925

Epoch: 5| Step: 1
Training loss: 1.8975484371185303
Validation loss: 2.4641550766524447

Epoch: 5| Step: 2
Training loss: 3.4459495544433594
Validation loss: 2.4662556417526735

Epoch: 5| Step: 3
Training loss: 2.6770129203796387
Validation loss: 2.467143153631559

Epoch: 5| Step: 4
Training loss: 2.822526454925537
Validation loss: 2.467253062032884

Epoch: 5| Step: 5
Training loss: 2.835205554962158
Validation loss: 2.4721047929538194

Epoch: 5| Step: 6
Training loss: 2.870262861251831
Validation loss: 2.4722050953936834

Epoch: 5| Step: 7
Training loss: 2.4905247688293457
Validation loss: 2.4706349552318616

Epoch: 5| Step: 8
Training loss: 2.065103769302368
Validation loss: 2.4693983703531246

Epoch: 5| Step: 9
Training loss: 2.5868358612060547
Validation loss: 2.468629465308241

Epoch: 5| Step: 10
Training loss: 2.7435109615325928
Validation loss: 2.4667515934154554

Epoch: 101| Step: 0
Training loss: 3.2416183948516846
Validation loss: 2.4711750835500736

Epoch: 5| Step: 1
Training loss: 2.822915554046631
Validation loss: 2.474640661670316

Epoch: 5| Step: 2
Training loss: 2.6872243881225586
Validation loss: 2.4701427823753765

Epoch: 5| Step: 3
Training loss: 3.0159478187561035
Validation loss: 2.4707497089139876

Epoch: 5| Step: 4
Training loss: 2.39404034614563
Validation loss: 2.469369411468506

Epoch: 5| Step: 5
Training loss: 2.5841078758239746
Validation loss: 2.463658450752176

Epoch: 5| Step: 6
Training loss: 2.384434223175049
Validation loss: 2.4605260766962522

Epoch: 5| Step: 7
Training loss: 2.5308563709259033
Validation loss: 2.4600332552386868

Epoch: 5| Step: 8
Training loss: 2.518155336380005
Validation loss: 2.459423013912734

Epoch: 5| Step: 9
Training loss: 2.6359784603118896
Validation loss: 2.457575151997228

Epoch: 5| Step: 10
Training loss: 2.5896639823913574
Validation loss: 2.4627093422797417

Epoch: 102| Step: 0
Training loss: 2.7746574878692627
Validation loss: 2.454553550289523

Epoch: 5| Step: 1
Training loss: 2.77953839302063
Validation loss: 2.456940130520892

Epoch: 5| Step: 2
Training loss: 2.9151406288146973
Validation loss: 2.4555366129003544

Epoch: 5| Step: 3
Training loss: 2.5998566150665283
Validation loss: 2.455543141211233

Epoch: 5| Step: 4
Training loss: 2.8242077827453613
Validation loss: 2.4648457137487267

Epoch: 5| Step: 5
Training loss: 2.9312312602996826
Validation loss: 2.479087188679685

Epoch: 5| Step: 6
Training loss: 2.787970542907715
Validation loss: 2.4879093682894142

Epoch: 5| Step: 7
Training loss: 2.6558003425598145
Validation loss: 2.4860473935322096

Epoch: 5| Step: 8
Training loss: 2.127352714538574
Validation loss: 2.464083471605855

Epoch: 5| Step: 9
Training loss: 2.4555938243865967
Validation loss: 2.454237530308385

Epoch: 5| Step: 10
Training loss: 2.697525978088379
Validation loss: 2.4616137243086293

Epoch: 103| Step: 0
Training loss: 3.1683061122894287
Validation loss: 2.465235076924806

Epoch: 5| Step: 1
Training loss: 2.912827730178833
Validation loss: 2.469610962816464

Epoch: 5| Step: 2
Training loss: 2.9452757835388184
Validation loss: 2.475525732963316

Epoch: 5| Step: 3
Training loss: 2.2683682441711426
Validation loss: 2.484510821680869

Epoch: 5| Step: 4
Training loss: 2.182960033416748
Validation loss: 2.4827372053618073

Epoch: 5| Step: 5
Training loss: 2.724956512451172
Validation loss: 2.4823233927449873

Epoch: 5| Step: 6
Training loss: 2.9087722301483154
Validation loss: 2.4777323225493073

Epoch: 5| Step: 7
Training loss: 1.5569323301315308
Validation loss: 2.473131336191649

Epoch: 5| Step: 8
Training loss: 2.907644510269165
Validation loss: 2.471740407328452

Epoch: 5| Step: 9
Training loss: 2.644908905029297
Validation loss: 2.4675549025176675

Epoch: 5| Step: 10
Training loss: 3.5557191371917725
Validation loss: 2.4622851520456295

Epoch: 104| Step: 0
Training loss: 2.376737117767334
Validation loss: 2.458273608197448

Epoch: 5| Step: 1
Training loss: 2.3049066066741943
Validation loss: 2.455152539796727

Epoch: 5| Step: 2
Training loss: 2.9791500568389893
Validation loss: 2.4549722850963636

Epoch: 5| Step: 3
Training loss: 2.6521382331848145
Validation loss: 2.4640647416473715

Epoch: 5| Step: 4
Training loss: 2.5691542625427246
Validation loss: 2.474433583597983

Epoch: 5| Step: 5
Training loss: 2.3458781242370605
Validation loss: 2.474866613265007

Epoch: 5| Step: 6
Training loss: 2.3263678550720215
Validation loss: 2.4658685115075882

Epoch: 5| Step: 7
Training loss: 2.4223532676696777
Validation loss: 2.4680819331958728

Epoch: 5| Step: 8
Training loss: 3.0121731758117676
Validation loss: 2.462798862047093

Epoch: 5| Step: 9
Training loss: 3.210834503173828
Validation loss: 2.4576277989213184

Epoch: 5| Step: 10
Training loss: 3.2984519004821777
Validation loss: 2.455598908085977

Epoch: 105| Step: 0
Training loss: 2.5756611824035645
Validation loss: 2.450272460137644

Epoch: 5| Step: 1
Training loss: 2.8078792095184326
Validation loss: 2.4492494470329693

Epoch: 5| Step: 2
Training loss: 2.162187337875366
Validation loss: 2.4532746935403473

Epoch: 5| Step: 3
Training loss: 3.21052622795105
Validation loss: 2.45565975353282

Epoch: 5| Step: 4
Training loss: 2.367736577987671
Validation loss: 2.456063906351725

Epoch: 5| Step: 5
Training loss: 2.157627582550049
Validation loss: 2.4535379717426915

Epoch: 5| Step: 6
Training loss: 2.6194167137145996
Validation loss: 2.457643044892178

Epoch: 5| Step: 7
Training loss: 2.449552059173584
Validation loss: 2.4552239038610972

Epoch: 5| Step: 8
Training loss: 2.8241050243377686
Validation loss: 2.454624158079906

Epoch: 5| Step: 9
Training loss: 3.432027816772461
Validation loss: 2.458079937965639

Epoch: 5| Step: 10
Training loss: 2.844472885131836
Validation loss: 2.4620210034872896

Epoch: 106| Step: 0
Training loss: 3.367412567138672
Validation loss: 2.461938783686648

Epoch: 5| Step: 1
Training loss: 3.0991756916046143
Validation loss: 2.460954009845693

Epoch: 5| Step: 2
Training loss: 2.3500561714172363
Validation loss: 2.4654138318953978

Epoch: 5| Step: 3
Training loss: 2.942434787750244
Validation loss: 2.4646105920114825

Epoch: 5| Step: 4
Training loss: 2.0337793827056885
Validation loss: 2.474604193882276

Epoch: 5| Step: 5
Training loss: 2.4999470710754395
Validation loss: 2.4624246140962005

Epoch: 5| Step: 6
Training loss: 2.884438991546631
Validation loss: 2.463063447706161

Epoch: 5| Step: 7
Training loss: 2.857038974761963
Validation loss: 2.4639237055214505

Epoch: 5| Step: 8
Training loss: 1.993657112121582
Validation loss: 2.4557465814775035

Epoch: 5| Step: 9
Training loss: 2.455997943878174
Validation loss: 2.4539954457231747

Epoch: 5| Step: 10
Training loss: 2.8552069664001465
Validation loss: 2.465789633412515

Epoch: 107| Step: 0
Training loss: 2.7619261741638184
Validation loss: 2.464356791588568

Epoch: 5| Step: 1
Training loss: 2.2255797386169434
Validation loss: 2.4629145335125666

Epoch: 5| Step: 2
Training loss: 3.0881295204162598
Validation loss: 2.455731430361348

Epoch: 5| Step: 3
Training loss: 2.8852760791778564
Validation loss: 2.4565941108170377

Epoch: 5| Step: 4
Training loss: 1.9098087549209595
Validation loss: 2.453565284770022

Epoch: 5| Step: 5
Training loss: 2.718674898147583
Validation loss: 2.4518518396603164

Epoch: 5| Step: 6
Training loss: 2.0828444957733154
Validation loss: 2.4509954606333086

Epoch: 5| Step: 7
Training loss: 2.307382583618164
Validation loss: 2.4537218463036323

Epoch: 5| Step: 8
Training loss: 2.675811290740967
Validation loss: 2.4528539616574525

Epoch: 5| Step: 9
Training loss: 3.481260299682617
Validation loss: 2.4557235779300814

Epoch: 5| Step: 10
Training loss: 3.3742916584014893
Validation loss: 2.4539290243579495

Epoch: 108| Step: 0
Training loss: 2.8420917987823486
Validation loss: 2.4535743908215593

Epoch: 5| Step: 1
Training loss: 2.7705187797546387
Validation loss: 2.4513394242973736

Epoch: 5| Step: 2
Training loss: 2.5461010932922363
Validation loss: 2.4537626671534714

Epoch: 5| Step: 3
Training loss: 3.240701675415039
Validation loss: 2.4467734521435154

Epoch: 5| Step: 4
Training loss: 2.4573473930358887
Validation loss: 2.4505097250784598

Epoch: 5| Step: 5
Training loss: 2.5171403884887695
Validation loss: 2.4420794299853745

Epoch: 5| Step: 6
Training loss: 2.285860538482666
Validation loss: 2.446156701733989

Epoch: 5| Step: 7
Training loss: 2.741548538208008
Validation loss: 2.450127442677816

Epoch: 5| Step: 8
Training loss: 2.282496690750122
Validation loss: 2.448511803021995

Epoch: 5| Step: 9
Training loss: 3.105041980743408
Validation loss: 2.442287673232376

Epoch: 5| Step: 10
Training loss: 2.445220947265625
Validation loss: 2.4464001783760647

Epoch: 109| Step: 0
Training loss: 2.4686360359191895
Validation loss: 2.4466208283619215

Epoch: 5| Step: 1
Training loss: 2.8271210193634033
Validation loss: 2.452602263419859

Epoch: 5| Step: 2
Training loss: 2.8817741870880127
Validation loss: 2.448029387381769

Epoch: 5| Step: 3
Training loss: 2.671867609024048
Validation loss: 2.4500625287332842

Epoch: 5| Step: 4
Training loss: 1.9865391254425049
Validation loss: 2.453759665130287

Epoch: 5| Step: 5
Training loss: 2.889875888824463
Validation loss: 2.4568088029020574

Epoch: 5| Step: 6
Training loss: 2.7485079765319824
Validation loss: 2.4587275661447996

Epoch: 5| Step: 7
Training loss: 2.4019851684570312
Validation loss: 2.461030189708997

Epoch: 5| Step: 8
Training loss: 2.7835476398468018
Validation loss: 2.4572032907957673

Epoch: 5| Step: 9
Training loss: 2.410644769668579
Validation loss: 2.457286573225452

Epoch: 5| Step: 10
Training loss: 3.2676775455474854
Validation loss: 2.459331597051313

Epoch: 110| Step: 0
Training loss: 2.8831892013549805
Validation loss: 2.453507479800973

Epoch: 5| Step: 1
Training loss: 2.379051923751831
Validation loss: 2.4501182699716217

Epoch: 5| Step: 2
Training loss: 1.990427017211914
Validation loss: 2.4501201798838954

Epoch: 5| Step: 3
Training loss: 3.1080822944641113
Validation loss: 2.4485439241573377

Epoch: 5| Step: 4
Training loss: 2.9352545738220215
Validation loss: 2.447228036901002

Epoch: 5| Step: 5
Training loss: 2.948272228240967
Validation loss: 2.447705479078395

Epoch: 5| Step: 6
Training loss: 2.808809995651245
Validation loss: 2.4459263996411393

Epoch: 5| Step: 7
Training loss: 2.4663567543029785
Validation loss: 2.4472066458835395

Epoch: 5| Step: 8
Training loss: 2.7454893589019775
Validation loss: 2.4512801426713184

Epoch: 5| Step: 9
Training loss: 2.364351272583008
Validation loss: 2.450983432031447

Epoch: 5| Step: 10
Training loss: 2.5695388317108154
Validation loss: 2.4516518282633957

Epoch: 111| Step: 0
Training loss: 2.1607742309570312
Validation loss: 2.4636102363627446

Epoch: 5| Step: 1
Training loss: 3.0115110874176025
Validation loss: 2.456386027797576

Epoch: 5| Step: 2
Training loss: 3.281564712524414
Validation loss: 2.457712875899448

Epoch: 5| Step: 3
Training loss: 2.294255256652832
Validation loss: 2.4677433813771894

Epoch: 5| Step: 4
Training loss: 2.552745819091797
Validation loss: 2.469309268459197

Epoch: 5| Step: 5
Training loss: 2.684439182281494
Validation loss: 2.4672036722142208

Epoch: 5| Step: 6
Training loss: 2.2237401008605957
Validation loss: 2.457930329025433

Epoch: 5| Step: 7
Training loss: 2.6201298236846924
Validation loss: 2.4515398548495386

Epoch: 5| Step: 8
Training loss: 2.9198336601257324
Validation loss: 2.4514982572165867

Epoch: 5| Step: 9
Training loss: 2.679316997528076
Validation loss: 2.446552981612503

Epoch: 5| Step: 10
Training loss: 2.861234664916992
Validation loss: 2.4529374466147473

Epoch: 112| Step: 0
Training loss: 2.3663089275360107
Validation loss: 2.4503569295329433

Epoch: 5| Step: 1
Training loss: 2.078549861907959
Validation loss: 2.4479809371373986

Epoch: 5| Step: 2
Training loss: 3.3381404876708984
Validation loss: 2.4513554906332367

Epoch: 5| Step: 3
Training loss: 2.5424437522888184
Validation loss: 2.4513316372389435

Epoch: 5| Step: 4
Training loss: 2.188873767852783
Validation loss: 2.448741202713341

Epoch: 5| Step: 5
Training loss: 3.012329339981079
Validation loss: 2.4455069752149683

Epoch: 5| Step: 6
Training loss: 2.6284801959991455
Validation loss: 2.447175359213224

Epoch: 5| Step: 7
Training loss: 2.546255350112915
Validation loss: 2.443959779636834

Epoch: 5| Step: 8
Training loss: 2.5908896923065186
Validation loss: 2.442115686273062

Epoch: 5| Step: 9
Training loss: 2.7307896614074707
Validation loss: 2.4386049726957917

Epoch: 5| Step: 10
Training loss: 3.421677350997925
Validation loss: 2.452094293409778

Epoch: 113| Step: 0
Training loss: 3.098811626434326
Validation loss: 2.4549310117639522

Epoch: 5| Step: 1
Training loss: 2.313335418701172
Validation loss: 2.489581838730843

Epoch: 5| Step: 2
Training loss: 2.2350921630859375
Validation loss: 2.4822139509262575

Epoch: 5| Step: 3
Training loss: 2.727336883544922
Validation loss: 2.4989913253374

Epoch: 5| Step: 4
Training loss: 2.786494493484497
Validation loss: 2.4978208657233947

Epoch: 5| Step: 5
Training loss: 2.149899959564209
Validation loss: 2.4801191001810055

Epoch: 5| Step: 6
Training loss: 2.3995070457458496
Validation loss: 2.4891475836435952

Epoch: 5| Step: 7
Training loss: 3.0072007179260254
Validation loss: 2.4624463614597114

Epoch: 5| Step: 8
Training loss: 2.9516570568084717
Validation loss: 2.4497910289354223

Epoch: 5| Step: 9
Training loss: 2.963534355163574
Validation loss: 2.434161441300505

Epoch: 5| Step: 10
Training loss: 2.781980514526367
Validation loss: 2.43175793463184

Epoch: 114| Step: 0
Training loss: 2.2887377738952637
Validation loss: 2.432401636595367

Epoch: 5| Step: 1
Training loss: 2.769136667251587
Validation loss: 2.431537292336905

Epoch: 5| Step: 2
Training loss: 2.659090042114258
Validation loss: 2.4309065239403838

Epoch: 5| Step: 3
Training loss: 2.2699575424194336
Validation loss: 2.4294116112493698

Epoch: 5| Step: 4
Training loss: 1.8761332035064697
Validation loss: 2.430605132092712

Epoch: 5| Step: 5
Training loss: 2.801848888397217
Validation loss: 2.426209072912893

Epoch: 5| Step: 6
Training loss: 2.898448944091797
Validation loss: 2.4328814552676294

Epoch: 5| Step: 7
Training loss: 3.0246965885162354
Validation loss: 2.434498312652752

Epoch: 5| Step: 8
Training loss: 2.7693123817443848
Validation loss: 2.4336779630312355

Epoch: 5| Step: 9
Training loss: 2.876861810684204
Validation loss: 2.4408852464409283

Epoch: 5| Step: 10
Training loss: 3.1027958393096924
Validation loss: 2.452021583434074

Epoch: 115| Step: 0
Training loss: 2.294844627380371
Validation loss: 2.4532707198973625

Epoch: 5| Step: 1
Training loss: 1.8043334484100342
Validation loss: 2.462887574267644

Epoch: 5| Step: 2
Training loss: 3.22533917427063
Validation loss: 2.4562025531645744

Epoch: 5| Step: 3
Training loss: 2.242588996887207
Validation loss: 2.4534435861854145

Epoch: 5| Step: 4
Training loss: 3.2195448875427246
Validation loss: 2.4584430609979937

Epoch: 5| Step: 5
Training loss: 2.9124817848205566
Validation loss: 2.45798728030215

Epoch: 5| Step: 6
Training loss: 2.7010416984558105
Validation loss: 2.4572232333562707

Epoch: 5| Step: 7
Training loss: 2.9611144065856934
Validation loss: 2.4592909787290838

Epoch: 5| Step: 8
Training loss: 2.423649549484253
Validation loss: 2.457284786367929

Epoch: 5| Step: 9
Training loss: 2.979954957962036
Validation loss: 2.4573275735301356

Epoch: 5| Step: 10
Training loss: 2.4541821479797363
Validation loss: 2.452293498541719

Epoch: 116| Step: 0
Training loss: 2.230376720428467
Validation loss: 2.4490044040064656

Epoch: 5| Step: 1
Training loss: 3.2674241065979004
Validation loss: 2.4454520312688683

Epoch: 5| Step: 2
Training loss: 3.034548759460449
Validation loss: 2.4442078349410847

Epoch: 5| Step: 3
Training loss: 2.4919686317443848
Validation loss: 2.4328487124494327

Epoch: 5| Step: 4
Training loss: 2.8660941123962402
Validation loss: 2.4363420547977572

Epoch: 5| Step: 5
Training loss: 2.270056962966919
Validation loss: 2.437956974070559

Epoch: 5| Step: 6
Training loss: 2.6434741020202637
Validation loss: 2.431255520031016

Epoch: 5| Step: 7
Training loss: 3.182098865509033
Validation loss: 2.4292841214005665

Epoch: 5| Step: 8
Training loss: 2.3630764484405518
Validation loss: 2.4327758537825717

Epoch: 5| Step: 9
Training loss: 2.554652690887451
Validation loss: 2.4403054175838346

Epoch: 5| Step: 10
Training loss: 2.2509632110595703
Validation loss: 2.439798552502868

Epoch: 117| Step: 0
Training loss: 2.725679874420166
Validation loss: 2.4360903668147262

Epoch: 5| Step: 1
Training loss: 3.1895508766174316
Validation loss: 2.4325697216936337

Epoch: 5| Step: 2
Training loss: 2.2594802379608154
Validation loss: 2.430044709995229

Epoch: 5| Step: 3
Training loss: 2.893124580383301
Validation loss: 2.4260164050645727

Epoch: 5| Step: 4
Training loss: 2.641388416290283
Validation loss: 2.4300239868061517

Epoch: 5| Step: 5
Training loss: 2.2161078453063965
Validation loss: 2.4310183063630135

Epoch: 5| Step: 6
Training loss: 3.2492592334747314
Validation loss: 2.423456600917283

Epoch: 5| Step: 7
Training loss: 2.461578845977783
Validation loss: 2.4258488942218084

Epoch: 5| Step: 8
Training loss: 3.1046841144561768
Validation loss: 2.4263036276704524

Epoch: 5| Step: 9
Training loss: 2.131481647491455
Validation loss: 2.432772272376604

Epoch: 5| Step: 10
Training loss: 2.2979977130889893
Validation loss: 2.430621438128974

Epoch: 118| Step: 0
Training loss: 1.9002888202667236
Validation loss: 2.4408349760117067

Epoch: 5| Step: 1
Training loss: 3.025763988494873
Validation loss: 2.438591598182596

Epoch: 5| Step: 2
Training loss: 3.3416543006896973
Validation loss: 2.4479462139068113

Epoch: 5| Step: 3
Training loss: 2.703003406524658
Validation loss: 2.437765957206808

Epoch: 5| Step: 4
Training loss: 2.511898994445801
Validation loss: 2.43838474827428

Epoch: 5| Step: 5
Training loss: 2.9790232181549072
Validation loss: 2.4401641648302794

Epoch: 5| Step: 6
Training loss: 2.4357385635375977
Validation loss: 2.442087286262102

Epoch: 5| Step: 7
Training loss: 2.8066844940185547
Validation loss: 2.4433367713805167

Epoch: 5| Step: 8
Training loss: 2.6594760417938232
Validation loss: 2.4450385621798936

Epoch: 5| Step: 9
Training loss: 2.0942740440368652
Validation loss: 2.460407571126056

Epoch: 5| Step: 10
Training loss: 2.6827046871185303
Validation loss: 2.4642252306784354

Epoch: 119| Step: 0
Training loss: 2.363996744155884
Validation loss: 2.4678425993970645

Epoch: 5| Step: 1
Training loss: 2.376621723175049
Validation loss: 2.4631336171139955

Epoch: 5| Step: 2
Training loss: 2.4273266792297363
Validation loss: 2.471471566025929

Epoch: 5| Step: 3
Training loss: 2.31080961227417
Validation loss: 2.4792792207451275

Epoch: 5| Step: 4
Training loss: 2.719343662261963
Validation loss: 2.4759171060336533

Epoch: 5| Step: 5
Training loss: 2.983525276184082
Validation loss: 2.4864407303512737

Epoch: 5| Step: 6
Training loss: 2.2090678215026855
Validation loss: 2.4845094796149962

Epoch: 5| Step: 7
Training loss: 2.4496054649353027
Validation loss: 2.481076753267678

Epoch: 5| Step: 8
Training loss: 2.6435608863830566
Validation loss: 2.4620543397882932

Epoch: 5| Step: 9
Training loss: 2.801776885986328
Validation loss: 2.4677174552794425

Epoch: 5| Step: 10
Training loss: 4.183976173400879
Validation loss: 2.4704592279208604

Epoch: 120| Step: 0
Training loss: 2.9083354473114014
Validation loss: 2.454308662363278

Epoch: 5| Step: 1
Training loss: 1.9553325176239014
Validation loss: 2.4415487217646774

Epoch: 5| Step: 2
Training loss: 2.645890712738037
Validation loss: 2.428612442426784

Epoch: 5| Step: 3
Training loss: 2.6176390647888184
Validation loss: 2.4276008221410934

Epoch: 5| Step: 4
Training loss: 2.2200331687927246
Validation loss: 2.4226501090552217

Epoch: 5| Step: 5
Training loss: 2.568803071975708
Validation loss: 2.4265084189753376

Epoch: 5| Step: 6
Training loss: 2.5431294441223145
Validation loss: 2.424635120617446

Epoch: 5| Step: 7
Training loss: 2.78898286819458
Validation loss: 2.42606093037513

Epoch: 5| Step: 8
Training loss: 3.1612815856933594
Validation loss: 2.4267621399253927

Epoch: 5| Step: 9
Training loss: 2.9474852085113525
Validation loss: 2.4296989133281093

Epoch: 5| Step: 10
Training loss: 2.8759665489196777
Validation loss: 2.4275635673153784

Epoch: 121| Step: 0
Training loss: 2.8332931995391846
Validation loss: 2.425948432696763

Epoch: 5| Step: 1
Training loss: 2.8261871337890625
Validation loss: 2.4254674552589335

Epoch: 5| Step: 2
Training loss: 1.911855936050415
Validation loss: 2.4327745386349258

Epoch: 5| Step: 3
Training loss: 3.1253745555877686
Validation loss: 2.43973881711242

Epoch: 5| Step: 4
Training loss: 2.79656982421875
Validation loss: 2.4426536585695002

Epoch: 5| Step: 5
Training loss: 3.127774238586426
Validation loss: 2.4380452248357956

Epoch: 5| Step: 6
Training loss: 2.277562379837036
Validation loss: 2.4300713821124007

Epoch: 5| Step: 7
Training loss: 2.2685012817382812
Validation loss: 2.4306895912334485

Epoch: 5| Step: 8
Training loss: 3.3960044384002686
Validation loss: 2.4226940806194017

Epoch: 5| Step: 9
Training loss: 2.1884372234344482
Validation loss: 2.430906008648616

Epoch: 5| Step: 10
Training loss: 2.353644609451294
Validation loss: 2.4246661919419483

Epoch: 122| Step: 0
Training loss: 2.3305773735046387
Validation loss: 2.424964527929983

Epoch: 5| Step: 1
Training loss: 3.0583975315093994
Validation loss: 2.4245112865201888

Epoch: 5| Step: 2
Training loss: 2.6787383556365967
Validation loss: 2.4268209242051646

Epoch: 5| Step: 3
Training loss: 2.4538607597351074
Validation loss: 2.4262799857765116

Epoch: 5| Step: 4
Training loss: 2.183034658432007
Validation loss: 2.4236792748974216

Epoch: 5| Step: 5
Training loss: 2.579392671585083
Validation loss: 2.4263355834509737

Epoch: 5| Step: 6
Training loss: 2.3301408290863037
Validation loss: 2.4244067028004634

Epoch: 5| Step: 7
Training loss: 2.540926694869995
Validation loss: 2.43336844700639

Epoch: 5| Step: 8
Training loss: 2.549882411956787
Validation loss: 2.4280949766917894

Epoch: 5| Step: 9
Training loss: 3.036574363708496
Validation loss: 2.4426950664930445

Epoch: 5| Step: 10
Training loss: 3.505364418029785
Validation loss: 2.4551770456375612

Epoch: 123| Step: 0
Training loss: 2.7551348209381104
Validation loss: 2.4601167748051305

Epoch: 5| Step: 1
Training loss: 2.6305959224700928
Validation loss: 2.4703585511894635

Epoch: 5| Step: 2
Training loss: 2.1726295948028564
Validation loss: 2.4701512526440363

Epoch: 5| Step: 3
Training loss: 2.906080961227417
Validation loss: 2.4677560073073193

Epoch: 5| Step: 4
Training loss: 2.6602842807769775
Validation loss: 2.451695183272003

Epoch: 5| Step: 5
Training loss: 2.43668270111084
Validation loss: 2.439428129503804

Epoch: 5| Step: 6
Training loss: 2.795816421508789
Validation loss: 2.428101021756408

Epoch: 5| Step: 7
Training loss: 2.4240009784698486
Validation loss: 2.422689445557133

Epoch: 5| Step: 8
Training loss: 2.689321517944336
Validation loss: 2.421880275972428

Epoch: 5| Step: 9
Training loss: 2.92346453666687
Validation loss: 2.4296913710973596

Epoch: 5| Step: 10
Training loss: 2.8145852088928223
Validation loss: 2.427439743472684

Epoch: 124| Step: 0
Training loss: 2.9660356044769287
Validation loss: 2.4254142571521062

Epoch: 5| Step: 1
Training loss: 3.16878080368042
Validation loss: 2.42739111890075

Epoch: 5| Step: 2
Training loss: 3.118077516555786
Validation loss: 2.4295376013684016

Epoch: 5| Step: 3
Training loss: 1.9256258010864258
Validation loss: 2.4294165667667182

Epoch: 5| Step: 4
Training loss: 3.4024555683135986
Validation loss: 2.426786186874554

Epoch: 5| Step: 5
Training loss: 2.5188426971435547
Validation loss: 2.4175450878758586

Epoch: 5| Step: 6
Training loss: 2.697876453399658
Validation loss: 2.413938678720946

Epoch: 5| Step: 7
Training loss: 2.2865734100341797
Validation loss: 2.4097039930282103

Epoch: 5| Step: 8
Training loss: 2.138287305831909
Validation loss: 2.420891736143379

Epoch: 5| Step: 9
Training loss: 2.424750566482544
Validation loss: 2.426637972554853

Epoch: 5| Step: 10
Training loss: 2.427649736404419
Validation loss: 2.4447516792563984

Epoch: 125| Step: 0
Training loss: 2.9761335849761963
Validation loss: 2.462566332150531

Epoch: 5| Step: 1
Training loss: 2.615387439727783
Validation loss: 2.4675356162491666

Epoch: 5| Step: 2
Training loss: 2.844879627227783
Validation loss: 2.4783199987103863

Epoch: 5| Step: 3
Training loss: 2.3303351402282715
Validation loss: 2.4875523223671863

Epoch: 5| Step: 4
Training loss: 2.462493419647217
Validation loss: 2.4740579000083347

Epoch: 5| Step: 5
Training loss: 2.7187917232513428
Validation loss: 2.4635024045103338

Epoch: 5| Step: 6
Training loss: 2.7185518741607666
Validation loss: 2.4544805890770367

Epoch: 5| Step: 7
Training loss: 2.0634398460388184
Validation loss: 2.4302495423183648

Epoch: 5| Step: 8
Training loss: 2.719226360321045
Validation loss: 2.418559323074997

Epoch: 5| Step: 9
Training loss: 2.4725966453552246
Validation loss: 2.414534153476838

Epoch: 5| Step: 10
Training loss: 3.6111292839050293
Validation loss: 2.411494134574808

Epoch: 126| Step: 0
Training loss: 3.0951340198516846
Validation loss: 2.411633570988973

Epoch: 5| Step: 1
Training loss: 2.713069438934326
Validation loss: 2.4190112621553483

Epoch: 5| Step: 2
Training loss: 2.5759103298187256
Validation loss: 2.4187883330929663

Epoch: 5| Step: 3
Training loss: 3.049299478530884
Validation loss: 2.418812785097348

Epoch: 5| Step: 4
Training loss: 2.5013298988342285
Validation loss: 2.4176186489802536

Epoch: 5| Step: 5
Training loss: 2.8306539058685303
Validation loss: 2.411770018198157

Epoch: 5| Step: 6
Training loss: 2.2517127990722656
Validation loss: 2.4169183623406196

Epoch: 5| Step: 7
Training loss: 2.8418772220611572
Validation loss: 2.4151587383721465

Epoch: 5| Step: 8
Training loss: 2.7274489402770996
Validation loss: 2.419190606763286

Epoch: 5| Step: 9
Training loss: 2.315783977508545
Validation loss: 2.4218484253011723

Epoch: 5| Step: 10
Training loss: 2.2746987342834473
Validation loss: 2.417483329772949

Epoch: 127| Step: 0
Training loss: 3.2518646717071533
Validation loss: 2.419954094835507

Epoch: 5| Step: 1
Training loss: 2.4450736045837402
Validation loss: 2.421650981390348

Epoch: 5| Step: 2
Training loss: 2.6681227684020996
Validation loss: 2.409545483127717

Epoch: 5| Step: 3
Training loss: 3.0280909538269043
Validation loss: 2.4130280158853017

Epoch: 5| Step: 4
Training loss: 2.1414921283721924
Validation loss: 2.4088124998154177

Epoch: 5| Step: 5
Training loss: 2.5190088748931885
Validation loss: 2.413587580444992

Epoch: 5| Step: 6
Training loss: 2.16733980178833
Validation loss: 2.4128486802501063

Epoch: 5| Step: 7
Training loss: 2.4459385871887207
Validation loss: 2.4212362894447903

Epoch: 5| Step: 8
Training loss: 2.887887954711914
Validation loss: 2.441036685820549

Epoch: 5| Step: 9
Training loss: 2.7842013835906982
Validation loss: 2.4384687869779524

Epoch: 5| Step: 10
Training loss: 2.7377426624298096
Validation loss: 2.430666431303947

Epoch: 128| Step: 0
Training loss: 2.591275453567505
Validation loss: 2.4193578535510647

Epoch: 5| Step: 1
Training loss: 2.9534173011779785
Validation loss: 2.4073367811018422

Epoch: 5| Step: 2
Training loss: 2.3329169750213623
Validation loss: 2.406159024084768

Epoch: 5| Step: 3
Training loss: 2.37573504447937
Validation loss: 2.408411059328305

Epoch: 5| Step: 4
Training loss: 2.8551783561706543
Validation loss: 2.4059207798332296

Epoch: 5| Step: 5
Training loss: 2.693443775177002
Validation loss: 2.4100013599600842

Epoch: 5| Step: 6
Training loss: 2.930283784866333
Validation loss: 2.4025614005263134

Epoch: 5| Step: 7
Training loss: 1.946069359779358
Validation loss: 2.405837905022406

Epoch: 5| Step: 8
Training loss: 2.8529281616210938
Validation loss: 2.409106813451295

Epoch: 5| Step: 9
Training loss: 3.605832576751709
Validation loss: 2.4056462216120895

Epoch: 5| Step: 10
Training loss: 1.8487486839294434
Validation loss: 2.4062973094242874

Epoch: 129| Step: 0
Training loss: 2.6866021156311035
Validation loss: 2.413550179491761

Epoch: 5| Step: 1
Training loss: 1.9924322366714478
Validation loss: 2.4129400919842463

Epoch: 5| Step: 2
Training loss: 2.956263303756714
Validation loss: 2.41185410048372

Epoch: 5| Step: 3
Training loss: 2.722761631011963
Validation loss: 2.4114868589626846

Epoch: 5| Step: 4
Training loss: 2.347498655319214
Validation loss: 2.413667960833478

Epoch: 5| Step: 5
Training loss: 2.8173232078552246
Validation loss: 2.4119841411549556

Epoch: 5| Step: 6
Training loss: 3.0115914344787598
Validation loss: 2.407786300105433

Epoch: 5| Step: 7
Training loss: 2.6682639122009277
Validation loss: 2.4060614852495092

Epoch: 5| Step: 8
Training loss: 2.8200933933258057
Validation loss: 2.4043677314635246

Epoch: 5| Step: 9
Training loss: 2.6793220043182373
Validation loss: 2.4103998035512944

Epoch: 5| Step: 10
Training loss: 2.3694870471954346
Validation loss: 2.4107639251216764

Epoch: 130| Step: 0
Training loss: 2.777528762817383
Validation loss: 2.4132806178062194

Epoch: 5| Step: 1
Training loss: 2.186239004135132
Validation loss: 2.415058128295406

Epoch: 5| Step: 2
Training loss: 2.7988688945770264
Validation loss: 2.4189091856761644

Epoch: 5| Step: 3
Training loss: 2.0818772315979004
Validation loss: 2.4266680261140228

Epoch: 5| Step: 4
Training loss: 2.382789134979248
Validation loss: 2.437438222669786

Epoch: 5| Step: 5
Training loss: 2.2734150886535645
Validation loss: 2.4433884697575725

Epoch: 5| Step: 6
Training loss: 2.9011425971984863
Validation loss: 2.44915295416309

Epoch: 5| Step: 7
Training loss: 3.04742169380188
Validation loss: 2.43991082970814

Epoch: 5| Step: 8
Training loss: 3.3075873851776123
Validation loss: 2.431817695658694

Epoch: 5| Step: 9
Training loss: 2.423703670501709
Validation loss: 2.415932096460814

Epoch: 5| Step: 10
Training loss: 2.883625030517578
Validation loss: 2.4178898590867237

Epoch: 131| Step: 0
Training loss: 2.512312889099121
Validation loss: 2.408233563105265

Epoch: 5| Step: 1
Training loss: 2.522129774093628
Validation loss: 2.4055100256396877

Epoch: 5| Step: 2
Training loss: 3.355686664581299
Validation loss: 2.413010738229239

Epoch: 5| Step: 3
Training loss: 2.3832364082336426
Validation loss: 2.414451611939297

Epoch: 5| Step: 4
Training loss: 2.411860942840576
Validation loss: 2.413457061654778

Epoch: 5| Step: 5
Training loss: 2.854541778564453
Validation loss: 2.4118112492304977

Epoch: 5| Step: 6
Training loss: 2.5384671688079834
Validation loss: 2.417111445498723

Epoch: 5| Step: 7
Training loss: 2.7627787590026855
Validation loss: 2.4219178025440504

Epoch: 5| Step: 8
Training loss: 2.587641477584839
Validation loss: 2.4225041045937488

Epoch: 5| Step: 9
Training loss: 2.209939956665039
Validation loss: 2.4167980635037987

Epoch: 5| Step: 10
Training loss: 3.0888681411743164
Validation loss: 2.4111230706655853

Epoch: 132| Step: 0
Training loss: 2.5934760570526123
Validation loss: 2.415509121392363

Epoch: 5| Step: 1
Training loss: 2.625704050064087
Validation loss: 2.404225949318178

Epoch: 5| Step: 2
Training loss: 2.5347042083740234
Validation loss: 2.4086287149818997

Epoch: 5| Step: 3
Training loss: 3.038428783416748
Validation loss: 2.4049833615620932

Epoch: 5| Step: 4
Training loss: 2.3846335411071777
Validation loss: 2.415215548648629

Epoch: 5| Step: 5
Training loss: 2.8578054904937744
Validation loss: 2.4268727764006583

Epoch: 5| Step: 6
Training loss: 2.4370405673980713
Validation loss: 2.429593381061349

Epoch: 5| Step: 7
Training loss: 3.152444839477539
Validation loss: 2.4247990039087113

Epoch: 5| Step: 8
Training loss: 2.6215660572052
Validation loss: 2.423600117365519

Epoch: 5| Step: 9
Training loss: 1.7616040706634521
Validation loss: 2.416311994675667

Epoch: 5| Step: 10
Training loss: 3.0765790939331055
Validation loss: 2.408831465628839

Epoch: 133| Step: 0
Training loss: 3.2042670249938965
Validation loss: 2.4155287306795836

Epoch: 5| Step: 1
Training loss: 2.576510190963745
Validation loss: 2.4046800213475383

Epoch: 5| Step: 2
Training loss: 2.4097046852111816
Validation loss: 2.4050343780107397

Epoch: 5| Step: 3
Training loss: 3.048300266265869
Validation loss: 2.407032817922613

Epoch: 5| Step: 4
Training loss: 2.4203524589538574
Validation loss: 2.4024170752494567

Epoch: 5| Step: 5
Training loss: 2.8158254623413086
Validation loss: 2.4005215065453642

Epoch: 5| Step: 6
Training loss: 2.284451484680176
Validation loss: 2.400766629044728

Epoch: 5| Step: 7
Training loss: 2.6723408699035645
Validation loss: 2.400799956372989

Epoch: 5| Step: 8
Training loss: 2.4414219856262207
Validation loss: 2.4039018179780696

Epoch: 5| Step: 9
Training loss: 2.6305618286132812
Validation loss: 2.405280866930562

Epoch: 5| Step: 10
Training loss: 2.373788356781006
Validation loss: 2.402466297149658

Epoch: 134| Step: 0
Training loss: 2.0072028636932373
Validation loss: 2.4045855665719635

Epoch: 5| Step: 1
Training loss: 3.2038185596466064
Validation loss: 2.4100966889371156

Epoch: 5| Step: 2
Training loss: 1.8100343942642212
Validation loss: 2.416838881789997

Epoch: 5| Step: 3
Training loss: 2.794332981109619
Validation loss: 2.418237196501865

Epoch: 5| Step: 4
Training loss: 2.3944144248962402
Validation loss: 2.4127886141500166

Epoch: 5| Step: 5
Training loss: 2.606663227081299
Validation loss: 2.4119180863903416

Epoch: 5| Step: 6
Training loss: 2.8604936599731445
Validation loss: 2.413840519484653

Epoch: 5| Step: 7
Training loss: 2.5950076580047607
Validation loss: 2.411305337823847

Epoch: 5| Step: 8
Training loss: 3.2883620262145996
Validation loss: 2.411916832770071

Epoch: 5| Step: 9
Training loss: 2.696056604385376
Validation loss: 2.4109237860607844

Epoch: 5| Step: 10
Training loss: 2.6214680671691895
Validation loss: 2.412517906517111

Epoch: 135| Step: 0
Training loss: 2.8607075214385986
Validation loss: 2.4206001604757

Epoch: 5| Step: 1
Training loss: 2.672527551651001
Validation loss: 2.4209146448360976

Epoch: 5| Step: 2
Training loss: 2.0292108058929443
Validation loss: 2.426992731709634

Epoch: 5| Step: 3
Training loss: 3.3524863719940186
Validation loss: 2.421741682996032

Epoch: 5| Step: 4
Training loss: 2.7838330268859863
Validation loss: 2.413539678819718

Epoch: 5| Step: 5
Training loss: 2.181354284286499
Validation loss: 2.419892453378247

Epoch: 5| Step: 6
Training loss: 2.8247718811035156
Validation loss: 2.432195371197116

Epoch: 5| Step: 7
Training loss: 2.7516493797302246
Validation loss: 2.4501799127107025

Epoch: 5| Step: 8
Training loss: 2.6563544273376465
Validation loss: 2.4518692006347

Epoch: 5| Step: 9
Training loss: 2.6422054767608643
Validation loss: 2.4285125578603437

Epoch: 5| Step: 10
Training loss: 2.1434268951416016
Validation loss: 2.4048710279567267

Epoch: 136| Step: 0
Training loss: 2.4727623462677
Validation loss: 2.398820302819693

Epoch: 5| Step: 1
Training loss: 2.3921923637390137
Validation loss: 2.399306220393027

Epoch: 5| Step: 2
Training loss: 3.048572063446045
Validation loss: 2.396562158420522

Epoch: 5| Step: 3
Training loss: 2.4859213829040527
Validation loss: 2.3989778667367916

Epoch: 5| Step: 4
Training loss: 2.0446314811706543
Validation loss: 2.398101943795399

Epoch: 5| Step: 5
Training loss: 2.51080584526062
Validation loss: 2.395406130821474

Epoch: 5| Step: 6
Training loss: 3.6795337200164795
Validation loss: 2.3970871715135473

Epoch: 5| Step: 7
Training loss: 2.5815494060516357
Validation loss: 2.4006821160675376

Epoch: 5| Step: 8
Training loss: 3.018742322921753
Validation loss: 2.4014310964974026

Epoch: 5| Step: 9
Training loss: 2.3972387313842773
Validation loss: 2.398272322070214

Epoch: 5| Step: 10
Training loss: 2.3781259059906006
Validation loss: 2.4010220804522113

Epoch: 137| Step: 0
Training loss: 3.0227174758911133
Validation loss: 2.3970187505086265

Epoch: 5| Step: 1
Training loss: 2.3012359142303467
Validation loss: 2.3997414035181843

Epoch: 5| Step: 2
Training loss: 2.805866003036499
Validation loss: 2.3975907192435315

Epoch: 5| Step: 3
Training loss: 2.6417267322540283
Validation loss: 2.402180963946927

Epoch: 5| Step: 4
Training loss: 2.878830671310425
Validation loss: 2.3998561725821546

Epoch: 5| Step: 5
Training loss: 2.981257200241089
Validation loss: 2.412377785610896

Epoch: 5| Step: 6
Training loss: 2.6804919242858887
Validation loss: 2.413428116870183

Epoch: 5| Step: 7
Training loss: 2.2542481422424316
Validation loss: 2.426720788401942

Epoch: 5| Step: 8
Training loss: 2.538179874420166
Validation loss: 2.4352473853736796

Epoch: 5| Step: 9
Training loss: 2.5377888679504395
Validation loss: 2.4398029106919483

Epoch: 5| Step: 10
Training loss: 2.362760543823242
Validation loss: 2.450524978740241

Epoch: 138| Step: 0
Training loss: 2.8851816654205322
Validation loss: 2.4492099285125732

Epoch: 5| Step: 1
Training loss: 2.3028643131256104
Validation loss: 2.437091412082795

Epoch: 5| Step: 2
Training loss: 2.6998817920684814
Validation loss: 2.426479088362827

Epoch: 5| Step: 3
Training loss: 2.600200891494751
Validation loss: 2.400727075915183

Epoch: 5| Step: 4
Training loss: 2.436194658279419
Validation loss: 2.4009057911493445

Epoch: 5| Step: 5
Training loss: 2.837686538696289
Validation loss: 2.3945065108678674

Epoch: 5| Step: 6
Training loss: 2.275158405303955
Validation loss: 2.3901815260610273

Epoch: 5| Step: 7
Training loss: 2.9003453254699707
Validation loss: 2.3961407241000923

Epoch: 5| Step: 8
Training loss: 3.067902088165283
Validation loss: 2.40396386320873

Epoch: 5| Step: 9
Training loss: 2.2982900142669678
Validation loss: 2.4088824846411265

Epoch: 5| Step: 10
Training loss: 2.87866473197937
Validation loss: 2.424149890099802

Epoch: 139| Step: 0
Training loss: 2.7754147052764893
Validation loss: 2.431286337555096

Epoch: 5| Step: 1
Training loss: 3.196615219116211
Validation loss: 2.4490662338913127

Epoch: 5| Step: 2
Training loss: 2.443683385848999
Validation loss: 2.446098994183284

Epoch: 5| Step: 3
Training loss: 2.763899803161621
Validation loss: 2.433677637448875

Epoch: 5| Step: 4
Training loss: 2.541376829147339
Validation loss: 2.4417141329857612

Epoch: 5| Step: 5
Training loss: 2.8457627296447754
Validation loss: 2.431186212006436

Epoch: 5| Step: 6
Training loss: 2.0761218070983887
Validation loss: 2.4120897041854037

Epoch: 5| Step: 7
Training loss: 2.0963172912597656
Validation loss: 2.4060671406407512

Epoch: 5| Step: 8
Training loss: 2.679185628890991
Validation loss: 2.4148373001365253

Epoch: 5| Step: 9
Training loss: 2.5856757164001465
Validation loss: 2.4133697017546623

Epoch: 5| Step: 10
Training loss: 3.136821746826172
Validation loss: 2.426644040692237

Epoch: 140| Step: 0
Training loss: 3.275684356689453
Validation loss: 2.422656361774732

Epoch: 5| Step: 1
Training loss: 2.269526481628418
Validation loss: 2.4283125413361417

Epoch: 5| Step: 2
Training loss: 2.09143328666687
Validation loss: 2.4223924888077604

Epoch: 5| Step: 3
Training loss: 2.6931939125061035
Validation loss: 2.415016366589454

Epoch: 5| Step: 4
Training loss: 2.4852981567382812
Validation loss: 2.413279228312995

Epoch: 5| Step: 5
Training loss: 2.2023425102233887
Validation loss: 2.4079909914283344

Epoch: 5| Step: 6
Training loss: 2.5315465927124023
Validation loss: 2.3915246430263726

Epoch: 5| Step: 7
Training loss: 2.7479350566864014
Validation loss: 2.3932181404482935

Epoch: 5| Step: 8
Training loss: 2.851097822189331
Validation loss: 2.3866643149365663

Epoch: 5| Step: 9
Training loss: 3.125779628753662
Validation loss: 2.3862576253952517

Epoch: 5| Step: 10
Training loss: 2.6965956687927246
Validation loss: 2.3908940079391643

Epoch: 141| Step: 0
Training loss: 2.4215221405029297
Validation loss: 2.3859498705915225

Epoch: 5| Step: 1
Training loss: 2.5608248710632324
Validation loss: 2.389142414574982

Epoch: 5| Step: 2
Training loss: 2.8599023818969727
Validation loss: 2.3849539641411073

Epoch: 5| Step: 3
Training loss: 2.498168468475342
Validation loss: 2.384777348528626

Epoch: 5| Step: 4
Training loss: 2.6813793182373047
Validation loss: 2.3872820587568384

Epoch: 5| Step: 5
Training loss: 3.333306074142456
Validation loss: 2.3848403243608374

Epoch: 5| Step: 6
Training loss: 2.7401938438415527
Validation loss: 2.3872557814403246

Epoch: 5| Step: 7
Training loss: 2.3188700675964355
Validation loss: 2.384789487367035

Epoch: 5| Step: 8
Training loss: 2.471559524536133
Validation loss: 2.3848105861294653

Epoch: 5| Step: 9
Training loss: 2.5788235664367676
Validation loss: 2.3945166962121123

Epoch: 5| Step: 10
Training loss: 2.5313475131988525
Validation loss: 2.396360440920758

Epoch: 142| Step: 0
Training loss: 2.652209758758545
Validation loss: 2.3976415870010213

Epoch: 5| Step: 1
Training loss: 2.7968146800994873
Validation loss: 2.3950925950081117

Epoch: 5| Step: 2
Training loss: 2.5382609367370605
Validation loss: 2.398099681382538

Epoch: 5| Step: 3
Training loss: 2.3293039798736572
Validation loss: 2.3925147005306777

Epoch: 5| Step: 4
Training loss: 2.2070469856262207
Validation loss: 2.3918358587449595

Epoch: 5| Step: 5
Training loss: 2.7781472206115723
Validation loss: 2.385784113278953

Epoch: 5| Step: 6
Training loss: 2.941701889038086
Validation loss: 2.3964603126689954

Epoch: 5| Step: 7
Training loss: 2.8497986793518066
Validation loss: 2.403356482905726

Epoch: 5| Step: 8
Training loss: 2.684760093688965
Validation loss: 2.4071307720676547

Epoch: 5| Step: 9
Training loss: 2.8167901039123535
Validation loss: 2.4194000151849564

Epoch: 5| Step: 10
Training loss: 2.1178810596466064
Validation loss: 2.4090399126852713

Epoch: 143| Step: 0
Training loss: 2.404722213745117
Validation loss: 2.403700490151682

Epoch: 5| Step: 1
Training loss: 2.300269603729248
Validation loss: 2.3995307901854157

Epoch: 5| Step: 2
Training loss: 2.712754964828491
Validation loss: 2.387719984977476

Epoch: 5| Step: 3
Training loss: 1.7528293132781982
Validation loss: 2.385538037105273

Epoch: 5| Step: 4
Training loss: 3.0687050819396973
Validation loss: 2.388140642514793

Epoch: 5| Step: 5
Training loss: 3.0612266063690186
Validation loss: 2.388603389904063

Epoch: 5| Step: 6
Training loss: 2.3582661151885986
Validation loss: 2.389000913148285

Epoch: 5| Step: 7
Training loss: 3.0446362495422363
Validation loss: 2.386568066894367

Epoch: 5| Step: 8
Training loss: 3.195828914642334
Validation loss: 2.3839921182201755

Epoch: 5| Step: 9
Training loss: 2.4430410861968994
Validation loss: 2.3858539776135514

Epoch: 5| Step: 10
Training loss: 2.4962658882141113
Validation loss: 2.385481278101603

Epoch: 144| Step: 0
Training loss: 3.0644354820251465
Validation loss: 2.381203833446708

Epoch: 5| Step: 1
Training loss: 2.6288247108459473
Validation loss: 2.3782115174878027

Epoch: 5| Step: 2
Training loss: 2.863576650619507
Validation loss: 2.3830723147238455

Epoch: 5| Step: 3
Training loss: 3.4884390830993652
Validation loss: 2.3869225927578506

Epoch: 5| Step: 4
Training loss: 2.2682223320007324
Validation loss: 2.383867117666429

Epoch: 5| Step: 5
Training loss: 2.2133936882019043
Validation loss: 2.3872721502857823

Epoch: 5| Step: 6
Training loss: 2.6377453804016113
Validation loss: 2.3932174482653217

Epoch: 5| Step: 7
Training loss: 2.9890379905700684
Validation loss: 2.4026796715233916

Epoch: 5| Step: 8
Training loss: 2.295196056365967
Validation loss: 2.3946630531741726

Epoch: 5| Step: 9
Training loss: 2.167783498764038
Validation loss: 2.3909327624946513

Epoch: 5| Step: 10
Training loss: 2.21303129196167
Validation loss: 2.3902184758135068

Epoch: 145| Step: 0
Training loss: 1.7337337732315063
Validation loss: 2.3849723082716747

Epoch: 5| Step: 1
Training loss: 2.52982759475708
Validation loss: 2.3797712595232072

Epoch: 5| Step: 2
Training loss: 2.318312168121338
Validation loss: 2.378612341419343

Epoch: 5| Step: 3
Training loss: 2.4801576137542725
Validation loss: 2.378675168560397

Epoch: 5| Step: 4
Training loss: 2.6756250858306885
Validation loss: 2.376658062781057

Epoch: 5| Step: 5
Training loss: 2.594730854034424
Validation loss: 2.3797723990614696

Epoch: 5| Step: 6
Training loss: 2.6270384788513184
Validation loss: 2.3797812333670993

Epoch: 5| Step: 7
Training loss: 2.951927661895752
Validation loss: 2.3790412000430528

Epoch: 5| Step: 8
Training loss: 2.7147464752197266
Validation loss: 2.3833246205442693

Epoch: 5| Step: 9
Training loss: 3.497953414916992
Validation loss: 2.380123105100406

Epoch: 5| Step: 10
Training loss: 2.699655055999756
Validation loss: 2.3848921791199715

Epoch: 146| Step: 0
Training loss: 2.8430607318878174
Validation loss: 2.3813591311054845

Epoch: 5| Step: 1
Training loss: 1.967983603477478
Validation loss: 2.3844613875112226

Epoch: 5| Step: 2
Training loss: 3.2091991901397705
Validation loss: 2.3854327419752717

Epoch: 5| Step: 3
Training loss: 2.1832430362701416
Validation loss: 2.3882063563152025

Epoch: 5| Step: 4
Training loss: 2.3830268383026123
Validation loss: 2.391558336955245

Epoch: 5| Step: 5
Training loss: 2.560181140899658
Validation loss: 2.3889164360620643

Epoch: 5| Step: 6
Training loss: 3.104874849319458
Validation loss: 2.3896093804349183

Epoch: 5| Step: 7
Training loss: 2.371917247772217
Validation loss: 2.386387607102753

Epoch: 5| Step: 8
Training loss: 3.287773847579956
Validation loss: 2.386304275963896

Epoch: 5| Step: 9
Training loss: 2.394139289855957
Validation loss: 2.3900176953243952

Epoch: 5| Step: 10
Training loss: 2.453784227371216
Validation loss: 2.3880746441502727

Epoch: 147| Step: 0
Training loss: 2.371945858001709
Validation loss: 2.387230944889848

Epoch: 5| Step: 1
Training loss: 2.741381883621216
Validation loss: 2.390786164550371

Epoch: 5| Step: 2
Training loss: 2.184079170227051
Validation loss: 2.3914704220269316

Epoch: 5| Step: 3
Training loss: 3.2170143127441406
Validation loss: 2.3947434194626345

Epoch: 5| Step: 4
Training loss: 2.839298725128174
Validation loss: 2.4014926571999826

Epoch: 5| Step: 5
Training loss: 2.7770140171051025
Validation loss: 2.403854331662578

Epoch: 5| Step: 6
Training loss: 2.3010926246643066
Validation loss: 2.4042879176396195

Epoch: 5| Step: 7
Training loss: 2.6529593467712402
Validation loss: 2.403853593334075

Epoch: 5| Step: 8
Training loss: 2.1228854656219482
Validation loss: 2.4019339135898057

Epoch: 5| Step: 9
Training loss: 2.9291341304779053
Validation loss: 2.389996054351971

Epoch: 5| Step: 10
Training loss: 2.7289209365844727
Validation loss: 2.3895171944813063

Epoch: 148| Step: 0
Training loss: 2.704908609390259
Validation loss: 2.3843947213183165

Epoch: 5| Step: 1
Training loss: 2.5074121952056885
Validation loss: 2.379735626200194

Epoch: 5| Step: 2
Training loss: 2.439619302749634
Validation loss: 2.3785817546229207

Epoch: 5| Step: 3
Training loss: 2.367906093597412
Validation loss: 2.3760791055617796

Epoch: 5| Step: 4
Training loss: 2.217153549194336
Validation loss: 2.374021607060586

Epoch: 5| Step: 5
Training loss: 2.8581337928771973
Validation loss: 2.3746558850811375

Epoch: 5| Step: 6
Training loss: 2.800131320953369
Validation loss: 2.3851596027292232

Epoch: 5| Step: 7
Training loss: 3.0371134281158447
Validation loss: 2.3857345093962965

Epoch: 5| Step: 8
Training loss: 2.8814010620117188
Validation loss: 2.3887010415395102

Epoch: 5| Step: 9
Training loss: 2.5649006366729736
Validation loss: 2.3901305813943186

Epoch: 5| Step: 10
Training loss: 2.424211025238037
Validation loss: 2.3981226567299134

Epoch: 149| Step: 0
Training loss: 2.486636161804199
Validation loss: 2.3893587743082354

Epoch: 5| Step: 1
Training loss: 2.4065744876861572
Validation loss: 2.3805508767404864

Epoch: 5| Step: 2
Training loss: 2.768629789352417
Validation loss: 2.3807155111784577

Epoch: 5| Step: 3
Training loss: 2.750967025756836
Validation loss: 2.3743855594306864

Epoch: 5| Step: 4
Training loss: 2.396735668182373
Validation loss: 2.378081921608217

Epoch: 5| Step: 5
Training loss: 2.7398922443389893
Validation loss: 2.3778398716321556

Epoch: 5| Step: 6
Training loss: 3.033235549926758
Validation loss: 2.376659590710876

Epoch: 5| Step: 7
Training loss: 2.6377243995666504
Validation loss: 2.369816028943626

Epoch: 5| Step: 8
Training loss: 2.477344036102295
Validation loss: 2.3722738168572866

Epoch: 5| Step: 9
Training loss: 2.05229115486145
Validation loss: 2.3745652693574146

Epoch: 5| Step: 10
Training loss: 3.110302209854126
Validation loss: 2.377582409048593

Epoch: 150| Step: 0
Training loss: 2.083988904953003
Validation loss: 2.379867866475095

Epoch: 5| Step: 1
Training loss: 2.4681835174560547
Validation loss: 2.381783116248346

Epoch: 5| Step: 2
Training loss: 2.964231491088867
Validation loss: 2.3857598689294632

Epoch: 5| Step: 3
Training loss: 2.837745428085327
Validation loss: 2.385510698441536

Epoch: 5| Step: 4
Training loss: 2.6770424842834473
Validation loss: 2.386411090050974

Epoch: 5| Step: 5
Training loss: 2.6337063312530518
Validation loss: 2.3888088246827484

Epoch: 5| Step: 6
Training loss: 2.3806374073028564
Validation loss: 2.389513164438227

Epoch: 5| Step: 7
Training loss: 2.944469928741455
Validation loss: 2.387226520046111

Epoch: 5| Step: 8
Training loss: 2.1795361042022705
Validation loss: 2.37946258565431

Epoch: 5| Step: 9
Training loss: 3.4895107746124268
Validation loss: 2.3906745115915933

Epoch: 5| Step: 10
Training loss: 2.0587987899780273
Validation loss: 2.3830134022620415

Epoch: 151| Step: 0
Training loss: 2.3896396160125732
Validation loss: 2.3791580751378048

Epoch: 5| Step: 1
Training loss: 2.2492504119873047
Validation loss: 2.3861578818290465

Epoch: 5| Step: 2
Training loss: 2.0674118995666504
Validation loss: 2.3991301367359776

Epoch: 5| Step: 3
Training loss: 2.8084969520568848
Validation loss: 2.3941271458902667

Epoch: 5| Step: 4
Training loss: 2.736151933670044
Validation loss: 2.3940197883113736

Epoch: 5| Step: 5
Training loss: 2.7761359214782715
Validation loss: 2.389913874287759

Epoch: 5| Step: 6
Training loss: 2.4952545166015625
Validation loss: 2.3876696299481135

Epoch: 5| Step: 7
Training loss: 3.500272274017334
Validation loss: 2.390019183517784

Epoch: 5| Step: 8
Training loss: 2.621568202972412
Validation loss: 2.3810853650493007

Epoch: 5| Step: 9
Training loss: 2.0685877799987793
Validation loss: 2.375893428761472

Epoch: 5| Step: 10
Training loss: 3.030088186264038
Validation loss: 2.3763182240147747

Epoch: 152| Step: 0
Training loss: 2.6640195846557617
Validation loss: 2.3727964765282086

Epoch: 5| Step: 1
Training loss: 2.6442818641662598
Validation loss: 2.3761671114993352

Epoch: 5| Step: 2
Training loss: 3.0848000049591064
Validation loss: 2.379207682865922

Epoch: 5| Step: 3
Training loss: 2.5327205657958984
Validation loss: 2.378031725524574

Epoch: 5| Step: 4
Training loss: 2.4314310550689697
Validation loss: 2.379774831956433

Epoch: 5| Step: 5
Training loss: 3.001370906829834
Validation loss: 2.3815292030252437

Epoch: 5| Step: 6
Training loss: 1.7344039678573608
Validation loss: 2.3812345022796304

Epoch: 5| Step: 7
Training loss: 2.476393938064575
Validation loss: 2.383213876396097

Epoch: 5| Step: 8
Training loss: 2.819204807281494
Validation loss: 2.3817265725904897

Epoch: 5| Step: 9
Training loss: 3.0635223388671875
Validation loss: 2.3822109596703642

Epoch: 5| Step: 10
Training loss: 2.181126832962036
Validation loss: 2.376779197364725

Epoch: 153| Step: 0
Training loss: 3.062351942062378
Validation loss: 2.3860596123562066

Epoch: 5| Step: 1
Training loss: 2.334705352783203
Validation loss: 2.382684502550351

Epoch: 5| Step: 2
Training loss: 1.8724052906036377
Validation loss: 2.3865104747074906

Epoch: 5| Step: 3
Training loss: 3.018289089202881
Validation loss: 2.385566193570373

Epoch: 5| Step: 4
Training loss: 2.723644256591797
Validation loss: 2.383663333872313

Epoch: 5| Step: 5
Training loss: 2.6329903602600098
Validation loss: 2.375312243738482

Epoch: 5| Step: 6
Training loss: 2.295992374420166
Validation loss: 2.376368584171418

Epoch: 5| Step: 7
Training loss: 2.187337636947632
Validation loss: 2.3665625433767996

Epoch: 5| Step: 8
Training loss: 2.752913236618042
Validation loss: 2.3639345451067855

Epoch: 5| Step: 9
Training loss: 2.5129432678222656
Validation loss: 2.3668009081194477

Epoch: 5| Step: 10
Training loss: 3.404318332672119
Validation loss: 2.3679356087920485

Epoch: 154| Step: 0
Training loss: 2.081193208694458
Validation loss: 2.3662859239885883

Epoch: 5| Step: 1
Training loss: 1.9722869396209717
Validation loss: 2.370915059120424

Epoch: 5| Step: 2
Training loss: 2.231905937194824
Validation loss: 2.371174503398198

Epoch: 5| Step: 3
Training loss: 2.4667880535125732
Validation loss: 2.371911738508491

Epoch: 5| Step: 4
Training loss: 3.2812225818634033
Validation loss: 2.3720772420206377

Epoch: 5| Step: 5
Training loss: 2.309126377105713
Validation loss: 2.3738723442118657

Epoch: 5| Step: 6
Training loss: 3.044232130050659
Validation loss: 2.3838396841479885

Epoch: 5| Step: 7
Training loss: 2.8088736534118652
Validation loss: 2.381820273655717

Epoch: 5| Step: 8
Training loss: 2.3551552295684814
Validation loss: 2.3834691765487834

Epoch: 5| Step: 9
Training loss: 3.424992322921753
Validation loss: 2.3888876617595716

Epoch: 5| Step: 10
Training loss: 2.6793503761291504
Validation loss: 2.383559655117732

Epoch: 155| Step: 0
Training loss: 2.6551976203918457
Validation loss: 2.3884933507570656

Epoch: 5| Step: 1
Training loss: 2.364563465118408
Validation loss: 2.387353712512601

Epoch: 5| Step: 2
Training loss: 2.725529432296753
Validation loss: 2.390384389508155

Epoch: 5| Step: 3
Training loss: 2.5213515758514404
Validation loss: 2.387494476892615

Epoch: 5| Step: 4
Training loss: 2.5993146896362305
Validation loss: 2.3843010907532065

Epoch: 5| Step: 5
Training loss: 2.935633897781372
Validation loss: 2.3904499776901735

Epoch: 5| Step: 6
Training loss: 2.488825798034668
Validation loss: 2.3899065576573855

Epoch: 5| Step: 7
Training loss: 3.4737706184387207
Validation loss: 2.3904918470690326

Epoch: 5| Step: 8
Training loss: 2.1795623302459717
Validation loss: 2.379206367718276

Epoch: 5| Step: 9
Training loss: 2.3210766315460205
Validation loss: 2.3762356106952955

Epoch: 5| Step: 10
Training loss: 2.377734899520874
Validation loss: 2.370659157794009

Epoch: 156| Step: 0
Training loss: 2.8132176399230957
Validation loss: 2.365976305418117

Epoch: 5| Step: 1
Training loss: 1.8185951709747314
Validation loss: 2.3656774643928773

Epoch: 5| Step: 2
Training loss: 2.486034631729126
Validation loss: 2.3726629031601774

Epoch: 5| Step: 3
Training loss: 3.0974178314208984
Validation loss: 2.3731261043138403

Epoch: 5| Step: 4
Training loss: 2.353165864944458
Validation loss: 2.37090088346953

Epoch: 5| Step: 5
Training loss: 2.5579841136932373
Validation loss: 2.3669984866214056

Epoch: 5| Step: 6
Training loss: 3.1652960777282715
Validation loss: 2.3702798222982757

Epoch: 5| Step: 7
Training loss: 2.9033827781677246
Validation loss: 2.364466864575622

Epoch: 5| Step: 8
Training loss: 2.4592697620391846
Validation loss: 2.369738281414073

Epoch: 5| Step: 9
Training loss: 2.4081356525421143
Validation loss: 2.364775670472012

Epoch: 5| Step: 10
Training loss: 2.5534114837646484
Validation loss: 2.3765534380430817

Epoch: 157| Step: 0
Training loss: 2.0846457481384277
Validation loss: 2.3723685151787213

Epoch: 5| Step: 1
Training loss: 2.5350937843322754
Validation loss: 2.377537512010144

Epoch: 5| Step: 2
Training loss: 2.9964101314544678
Validation loss: 2.3870072903171664

Epoch: 5| Step: 3
Training loss: 2.2632241249084473
Validation loss: 2.3932502833745812

Epoch: 5| Step: 4
Training loss: 2.485222339630127
Validation loss: 2.399033920739287

Epoch: 5| Step: 5
Training loss: 3.214560031890869
Validation loss: 2.3934244186647478

Epoch: 5| Step: 6
Training loss: 2.4234790802001953
Validation loss: 2.397163942296018

Epoch: 5| Step: 7
Training loss: 2.94195294380188
Validation loss: 2.403726718759024

Epoch: 5| Step: 8
Training loss: 2.8226523399353027
Validation loss: 2.368842940176687

Epoch: 5| Step: 9
Training loss: 2.168046236038208
Validation loss: 2.3666139725715882

Epoch: 5| Step: 10
Training loss: 2.8143842220306396
Validation loss: 2.357753207606654

Epoch: 158| Step: 0
Training loss: 2.9596264362335205
Validation loss: 2.3605135435699136

Epoch: 5| Step: 1
Training loss: 2.2988955974578857
Validation loss: 2.353754956235168

Epoch: 5| Step: 2
Training loss: 3.6304993629455566
Validation loss: 2.35472075657178

Epoch: 5| Step: 3
Training loss: 2.685540199279785
Validation loss: 2.352588692019063

Epoch: 5| Step: 4
Training loss: 2.078200578689575
Validation loss: 2.3518663990882134

Epoch: 5| Step: 5
Training loss: 2.3320963382720947
Validation loss: 2.3587375815196703

Epoch: 5| Step: 6
Training loss: 2.3775503635406494
Validation loss: 2.3548677659803823

Epoch: 5| Step: 7
Training loss: 2.837495803833008
Validation loss: 2.362157752436976

Epoch: 5| Step: 8
Training loss: 2.3364264965057373
Validation loss: 2.366041192444422

Epoch: 5| Step: 9
Training loss: 2.0239176750183105
Validation loss: 2.3737163825701644

Epoch: 5| Step: 10
Training loss: 3.1785786151885986
Validation loss: 2.3814529116435716

Epoch: 159| Step: 0
Training loss: 2.2493197917938232
Validation loss: 2.368481211764838

Epoch: 5| Step: 1
Training loss: 2.7555346488952637
Validation loss: 2.3885263704484507

Epoch: 5| Step: 2
Training loss: 3.0868513584136963
Validation loss: 2.3956074304478143

Epoch: 5| Step: 3
Training loss: 2.287257432937622
Validation loss: 2.3899209486540927

Epoch: 5| Step: 4
Training loss: 3.1407761573791504
Validation loss: 2.3788042273572696

Epoch: 5| Step: 5
Training loss: 2.115226984024048
Validation loss: 2.3797022834900887

Epoch: 5| Step: 6
Training loss: 2.2696902751922607
Validation loss: 2.364383976946595

Epoch: 5| Step: 7
Training loss: 2.5458693504333496
Validation loss: 2.364068523530037

Epoch: 5| Step: 8
Training loss: 2.893078088760376
Validation loss: 2.3602279642576813

Epoch: 5| Step: 9
Training loss: 2.4592673778533936
Validation loss: 2.3537914599141767

Epoch: 5| Step: 10
Training loss: 2.931391954421997
Validation loss: 2.3553092428433

Epoch: 160| Step: 0
Training loss: 2.3242263793945312
Validation loss: 2.356046002398255

Epoch: 5| Step: 1
Training loss: 3.0005106925964355
Validation loss: 2.3557456078067904

Epoch: 5| Step: 2
Training loss: 2.4947750568389893
Validation loss: 2.3511992141764653

Epoch: 5| Step: 3
Training loss: 3.0354769229888916
Validation loss: 2.355491675356383

Epoch: 5| Step: 4
Training loss: 2.4001924991607666
Validation loss: 2.3512048157312537

Epoch: 5| Step: 5
Training loss: 2.8349251747131348
Validation loss: 2.352620775981616

Epoch: 5| Step: 6
Training loss: 2.108009099960327
Validation loss: 2.352081847447221

Epoch: 5| Step: 7
Training loss: 2.7151482105255127
Validation loss: 2.358017119028235

Epoch: 5| Step: 8
Training loss: 2.728761672973633
Validation loss: 2.3559599973822154

Epoch: 5| Step: 9
Training loss: 2.5352911949157715
Validation loss: 2.3779455102900022

Epoch: 5| Step: 10
Training loss: 2.383380174636841
Validation loss: 2.3735333924652426

Epoch: 161| Step: 0
Training loss: 2.2162654399871826
Validation loss: 2.379768007545061

Epoch: 5| Step: 1
Training loss: 2.402064561843872
Validation loss: 2.3806576190456266

Epoch: 5| Step: 2
Training loss: 2.6199257373809814
Validation loss: 2.395850637907623

Epoch: 5| Step: 3
Training loss: 2.898400068283081
Validation loss: 2.3883108323620212

Epoch: 5| Step: 4
Training loss: 2.976835012435913
Validation loss: 2.3860349962788243

Epoch: 5| Step: 5
Training loss: 3.028862714767456
Validation loss: 2.3801865500788533

Epoch: 5| Step: 6
Training loss: 2.4373295307159424
Validation loss: 2.371515256102367

Epoch: 5| Step: 7
Training loss: 3.1724164485931396
Validation loss: 2.3645481294201267

Epoch: 5| Step: 8
Training loss: 2.5670878887176514
Validation loss: 2.3621594803307646

Epoch: 5| Step: 9
Training loss: 2.1570334434509277
Validation loss: 2.3571600144909275

Epoch: 5| Step: 10
Training loss: 2.009976625442505
Validation loss: 2.357386230140604

Epoch: 162| Step: 0
Training loss: 2.439563274383545
Validation loss: 2.3578709633119646

Epoch: 5| Step: 1
Training loss: 3.093339204788208
Validation loss: 2.3736141317634174

Epoch: 5| Step: 2
Training loss: 2.6301283836364746
Validation loss: 2.3699733570057857

Epoch: 5| Step: 3
Training loss: 2.8967220783233643
Validation loss: 2.3639279386048675

Epoch: 5| Step: 4
Training loss: 2.0779075622558594
Validation loss: 2.3559608177472184

Epoch: 5| Step: 5
Training loss: 2.227825403213501
Validation loss: 2.3581180854510237

Epoch: 5| Step: 6
Training loss: 2.7696497440338135
Validation loss: 2.352977993667767

Epoch: 5| Step: 7
Training loss: 2.458202838897705
Validation loss: 2.3546402274921374

Epoch: 5| Step: 8
Training loss: 2.7364706993103027
Validation loss: 2.3530186606991674

Epoch: 5| Step: 9
Training loss: 2.865680456161499
Validation loss: 2.361796707235357

Epoch: 5| Step: 10
Training loss: 2.423593759536743
Validation loss: 2.3643742402394614

Epoch: 163| Step: 0
Training loss: 2.6158392429351807
Validation loss: 2.36559288219739

Epoch: 5| Step: 1
Training loss: 2.143589496612549
Validation loss: 2.377723406719905

Epoch: 5| Step: 2
Training loss: 2.0819005966186523
Validation loss: 2.3860398107959377

Epoch: 5| Step: 3
Training loss: 3.0014941692352295
Validation loss: 2.395315959889402

Epoch: 5| Step: 4
Training loss: 2.2710652351379395
Validation loss: 2.4024945356512584

Epoch: 5| Step: 5
Training loss: 2.482616424560547
Validation loss: 2.386289859330782

Epoch: 5| Step: 6
Training loss: 2.56769061088562
Validation loss: 2.3796734502238612

Epoch: 5| Step: 7
Training loss: 2.4904112815856934
Validation loss: 2.3718558844699653

Epoch: 5| Step: 8
Training loss: 2.994168281555176
Validation loss: 2.3690018833324475

Epoch: 5| Step: 9
Training loss: 2.687680721282959
Validation loss: 2.3628955066844983

Epoch: 5| Step: 10
Training loss: 3.3400514125823975
Validation loss: 2.374685578448798

Epoch: 164| Step: 0
Training loss: 3.004960536956787
Validation loss: 2.3792987177448888

Epoch: 5| Step: 1
Training loss: 2.787980079650879
Validation loss: 2.366520904725598

Epoch: 5| Step: 2
Training loss: 2.325138568878174
Validation loss: 2.365496007345056

Epoch: 5| Step: 3
Training loss: 2.085557222366333
Validation loss: 2.368629749103259

Epoch: 5| Step: 4
Training loss: 3.462482452392578
Validation loss: 2.35852752706056

Epoch: 5| Step: 5
Training loss: 2.7695274353027344
Validation loss: 2.3562442871832077

Epoch: 5| Step: 6
Training loss: 2.435628652572632
Validation loss: 2.3500472243114183

Epoch: 5| Step: 7
Training loss: 2.9771103858947754
Validation loss: 2.353893087756249

Epoch: 5| Step: 8
Training loss: 2.4065167903900146
Validation loss: 2.3573504904265046

Epoch: 5| Step: 9
Training loss: 2.203439235687256
Validation loss: 2.3731817353156304

Epoch: 5| Step: 10
Training loss: 2.0911097526550293
Validation loss: 2.3979088234645065

Epoch: 165| Step: 0
Training loss: 2.6302192211151123
Validation loss: 2.4380832282445764

Epoch: 5| Step: 1
Training loss: 3.3205809593200684
Validation loss: 2.4432325670796056

Epoch: 5| Step: 2
Training loss: 1.9604631662368774
Validation loss: 2.41984647832891

Epoch: 5| Step: 3
Training loss: 2.932954788208008
Validation loss: 2.376160196078721

Epoch: 5| Step: 4
Training loss: 3.122758388519287
Validation loss: 2.3572563971242597

Epoch: 5| Step: 5
Training loss: 2.893078565597534
Validation loss: 2.350524804925406

Epoch: 5| Step: 6
Training loss: 2.526978015899658
Validation loss: 2.3470906108938236

Epoch: 5| Step: 7
Training loss: 2.040656328201294
Validation loss: 2.345394294749024

Epoch: 5| Step: 8
Training loss: 2.5720372200012207
Validation loss: 2.344749500674586

Epoch: 5| Step: 9
Training loss: 2.6184608936309814
Validation loss: 2.3460724815245597

Epoch: 5| Step: 10
Training loss: 2.0379538536071777
Validation loss: 2.3452411108119513

Epoch: 166| Step: 0
Training loss: 3.3601016998291016
Validation loss: 2.345303817461896

Epoch: 5| Step: 1
Training loss: 2.423804998397827
Validation loss: 2.3468114381195395

Epoch: 5| Step: 2
Training loss: 2.861171007156372
Validation loss: 2.3535350189414075

Epoch: 5| Step: 3
Training loss: 2.5967888832092285
Validation loss: 2.3517472308169127

Epoch: 5| Step: 4
Training loss: 2.135667324066162
Validation loss: 2.3555955989386446

Epoch: 5| Step: 5
Training loss: 2.403144359588623
Validation loss: 2.350535301751988

Epoch: 5| Step: 6
Training loss: 2.3246066570281982
Validation loss: 2.3588081406008814

Epoch: 5| Step: 7
Training loss: 2.6900296211242676
Validation loss: 2.3586241096578617

Epoch: 5| Step: 8
Training loss: 2.5439610481262207
Validation loss: 2.3607320836795274

Epoch: 5| Step: 9
Training loss: 2.754040241241455
Validation loss: 2.358356216902374

Epoch: 5| Step: 10
Training loss: 2.567901849746704
Validation loss: 2.3564246957020094

Epoch: 167| Step: 0
Training loss: 2.967233180999756
Validation loss: 2.3614370233269146

Epoch: 5| Step: 1
Training loss: 2.6279373168945312
Validation loss: 2.3641254325066843

Epoch: 5| Step: 2
Training loss: 2.3779959678649902
Validation loss: 2.3560758354843303

Epoch: 5| Step: 3
Training loss: 2.176713466644287
Validation loss: 2.3585994243621826

Epoch: 5| Step: 4
Training loss: 3.100421190261841
Validation loss: 2.359932940493348

Epoch: 5| Step: 5
Training loss: 3.2352466583251953
Validation loss: 2.350967158553421

Epoch: 5| Step: 6
Training loss: 1.8628566265106201
Validation loss: 2.3463109693219586

Epoch: 5| Step: 7
Training loss: 2.6183767318725586
Validation loss: 2.3464890295459377

Epoch: 5| Step: 8
Training loss: 2.4408695697784424
Validation loss: 2.3436915720662763

Epoch: 5| Step: 9
Training loss: 2.33659291267395
Validation loss: 2.344792858246834

Epoch: 5| Step: 10
Training loss: 2.8235654830932617
Validation loss: 2.3466133814986034

Epoch: 168| Step: 0
Training loss: 3.3096923828125
Validation loss: 2.3662372173801547

Epoch: 5| Step: 1
Training loss: 3.5417397022247314
Validation loss: 2.387228423549283

Epoch: 5| Step: 2
Training loss: 2.1433205604553223
Validation loss: 2.392768065134684

Epoch: 5| Step: 3
Training loss: 2.550891160964966
Validation loss: 2.4079102546938005

Epoch: 5| Step: 4
Training loss: 3.1044280529022217
Validation loss: 2.406218974821029

Epoch: 5| Step: 5
Training loss: 2.6090476512908936
Validation loss: 2.40801336175652

Epoch: 5| Step: 6
Training loss: 2.4427177906036377
Validation loss: 2.3993397733216644

Epoch: 5| Step: 7
Training loss: 2.737584352493286
Validation loss: 2.3877702861703853

Epoch: 5| Step: 8
Training loss: 2.168001413345337
Validation loss: 2.3677380469537552

Epoch: 5| Step: 9
Training loss: 2.2427685260772705
Validation loss: 2.3619775720821914

Epoch: 5| Step: 10
Training loss: 1.6332182884216309
Validation loss: 2.3473102277325046

Epoch: 169| Step: 0
Training loss: 3.03432559967041
Validation loss: 2.3452463790934575

Epoch: 5| Step: 1
Training loss: 2.4545726776123047
Validation loss: 2.3427207059757684

Epoch: 5| Step: 2
Training loss: 2.674093246459961
Validation loss: 2.346083774361559

Epoch: 5| Step: 3
Training loss: 3.0161077976226807
Validation loss: 2.341557377128191

Epoch: 5| Step: 4
Training loss: 2.083695888519287
Validation loss: 2.342011820885443

Epoch: 5| Step: 5
Training loss: 1.9604823589324951
Validation loss: 2.3402671660146406

Epoch: 5| Step: 6
Training loss: 2.4991984367370605
Validation loss: 2.3483997314207015

Epoch: 5| Step: 7
Training loss: 2.6541998386383057
Validation loss: 2.360739167018603

Epoch: 5| Step: 8
Training loss: 2.8839402198791504
Validation loss: 2.376313932480351

Epoch: 5| Step: 9
Training loss: 2.5129425525665283
Validation loss: 2.382696677279729

Epoch: 5| Step: 10
Training loss: 2.8159427642822266
Validation loss: 2.3751262644285798

Epoch: 170| Step: 0
Training loss: 2.2897772789001465
Validation loss: 2.37392234545882

Epoch: 5| Step: 1
Training loss: 2.8531453609466553
Validation loss: 2.370700890018094

Epoch: 5| Step: 2
Training loss: 2.23397159576416
Validation loss: 2.351345072510422

Epoch: 5| Step: 3
Training loss: 2.8625781536102295
Validation loss: 2.3420789651973273

Epoch: 5| Step: 4
Training loss: 2.7425522804260254
Validation loss: 2.3386974206534763

Epoch: 5| Step: 5
Training loss: 2.5414838790893555
Validation loss: 2.3408088171353905

Epoch: 5| Step: 6
Training loss: 2.8620827198028564
Validation loss: 2.3459558999666603

Epoch: 5| Step: 7
Training loss: 2.633237361907959
Validation loss: 2.342841576504451

Epoch: 5| Step: 8
Training loss: 2.1636013984680176
Validation loss: 2.3469808383654525

Epoch: 5| Step: 9
Training loss: 2.627943515777588
Validation loss: 2.348691583961569

Epoch: 5| Step: 10
Training loss: 2.781296491622925
Validation loss: 2.346233598647579

Epoch: 171| Step: 0
Training loss: 2.75877046585083
Validation loss: 2.356889236357904

Epoch: 5| Step: 1
Training loss: 2.524935483932495
Validation loss: 2.35563309731022

Epoch: 5| Step: 2
Training loss: 2.660681962966919
Validation loss: 2.35101584465273

Epoch: 5| Step: 3
Training loss: 2.2579333782196045
Validation loss: 2.35085153836076

Epoch: 5| Step: 4
Training loss: 2.1911263465881348
Validation loss: 2.354994476482432

Epoch: 5| Step: 5
Training loss: 2.9550137519836426
Validation loss: 2.351724898943337

Epoch: 5| Step: 6
Training loss: 2.656418800354004
Validation loss: 2.346184781802598

Epoch: 5| Step: 7
Training loss: 3.354954242706299
Validation loss: 2.354708484424058

Epoch: 5| Step: 8
Training loss: 2.2439615726470947
Validation loss: 2.346153655359822

Epoch: 5| Step: 9
Training loss: 2.367943286895752
Validation loss: 2.3511084061796947

Epoch: 5| Step: 10
Training loss: 2.585808515548706
Validation loss: 2.3590284650043776

Epoch: 172| Step: 0
Training loss: 2.0617117881774902
Validation loss: 2.364620936814175

Epoch: 5| Step: 1
Training loss: 3.0852904319763184
Validation loss: 2.3737155493869575

Epoch: 5| Step: 2
Training loss: 2.4449305534362793
Validation loss: 2.371997871706563

Epoch: 5| Step: 3
Training loss: 2.317918300628662
Validation loss: 2.3766275990393853

Epoch: 5| Step: 4
Training loss: 3.132819890975952
Validation loss: 2.368562759891633

Epoch: 5| Step: 5
Training loss: 2.8443856239318848
Validation loss: 2.3700203395658925

Epoch: 5| Step: 6
Training loss: 2.659292459487915
Validation loss: 2.371085710422967

Epoch: 5| Step: 7
Training loss: 2.5310018062591553
Validation loss: 2.353801183803107

Epoch: 5| Step: 8
Training loss: 2.361170530319214
Validation loss: 2.342569225577898

Epoch: 5| Step: 9
Training loss: 2.225956916809082
Validation loss: 2.339807638558008

Epoch: 5| Step: 10
Training loss: 2.9067189693450928
Validation loss: 2.33927139928264

Epoch: 173| Step: 0
Training loss: 2.717583417892456
Validation loss: 2.337944020507156

Epoch: 5| Step: 1
Training loss: 2.679218053817749
Validation loss: 2.3432674895050707

Epoch: 5| Step: 2
Training loss: 2.479732036590576
Validation loss: 2.3429641979996876

Epoch: 5| Step: 3
Training loss: 2.2481935024261475
Validation loss: 2.3401672199208248

Epoch: 5| Step: 4
Training loss: 2.3591268062591553
Validation loss: 2.343186857879803

Epoch: 5| Step: 5
Training loss: 2.879453182220459
Validation loss: 2.34015054087485

Epoch: 5| Step: 6
Training loss: 3.1091396808624268
Validation loss: 2.341874361038208

Epoch: 5| Step: 7
Training loss: 2.5564208030700684
Validation loss: 2.342085761408652

Epoch: 5| Step: 8
Training loss: 2.2349510192871094
Validation loss: 2.3514045694822907

Epoch: 5| Step: 9
Training loss: 2.6461563110351562
Validation loss: 2.360826857628361

Epoch: 5| Step: 10
Training loss: 2.578139305114746
Validation loss: 2.3757805003914783

Epoch: 174| Step: 0
Training loss: 2.4483909606933594
Validation loss: 2.3753740223505164

Epoch: 5| Step: 1
Training loss: 2.75219988822937
Validation loss: 2.3860469172077794

Epoch: 5| Step: 2
Training loss: 2.9157161712646484
Validation loss: 2.4096069605119768

Epoch: 5| Step: 3
Training loss: 2.653859853744507
Validation loss: 2.3898777756639706

Epoch: 5| Step: 4
Training loss: 2.6656112670898438
Validation loss: 2.387988805770874

Epoch: 5| Step: 5
Training loss: 2.2907328605651855
Validation loss: 2.3678631885077364

Epoch: 5| Step: 6
Training loss: 2.080352783203125
Validation loss: 2.3600878074604976

Epoch: 5| Step: 7
Training loss: 2.8213202953338623
Validation loss: 2.3554252732184624

Epoch: 5| Step: 8
Training loss: 3.076063394546509
Validation loss: 2.3545914567926878

Epoch: 5| Step: 9
Training loss: 2.5739970207214355
Validation loss: 2.3617866859641126

Epoch: 5| Step: 10
Training loss: 2.1255228519439697
Validation loss: 2.3679914833396993

Epoch: 175| Step: 0
Training loss: 3.447739839553833
Validation loss: 2.3680357369043494

Epoch: 5| Step: 1
Training loss: 2.3105196952819824
Validation loss: 2.3592806195700042

Epoch: 5| Step: 2
Training loss: 2.328174114227295
Validation loss: 2.3701057767355316

Epoch: 5| Step: 3
Training loss: 2.65761137008667
Validation loss: 2.3581662306221585

Epoch: 5| Step: 4
Training loss: 2.4591872692108154
Validation loss: 2.3635437155282624

Epoch: 5| Step: 5
Training loss: 3.3263847827911377
Validation loss: 2.349818930831007

Epoch: 5| Step: 6
Training loss: 2.1386654376983643
Validation loss: 2.3477188284679125

Epoch: 5| Step: 7
Training loss: 2.01702880859375
Validation loss: 2.3445192742091354

Epoch: 5| Step: 8
Training loss: 2.7371532917022705
Validation loss: 2.34291741283991

Epoch: 5| Step: 9
Training loss: 3.3025174140930176
Validation loss: 2.342820268805309

Epoch: 5| Step: 10
Training loss: 1.84273099899292
Validation loss: 2.34053554842549

Epoch: 176| Step: 0
Training loss: 2.5158791542053223
Validation loss: 2.340566788950274

Epoch: 5| Step: 1
Training loss: 2.0375545024871826
Validation loss: 2.3395766468458277

Epoch: 5| Step: 2
Training loss: 2.9131674766540527
Validation loss: 2.349696728491014

Epoch: 5| Step: 3
Training loss: 2.261009693145752
Validation loss: 2.3494861100309636

Epoch: 5| Step: 4
Training loss: 2.6276161670684814
Validation loss: 2.364697451232582

Epoch: 5| Step: 5
Training loss: 2.566037654876709
Validation loss: 2.3709842184538483

Epoch: 5| Step: 6
Training loss: 2.650461435317993
Validation loss: 2.3801786950839463

Epoch: 5| Step: 7
Training loss: 2.6924726963043213
Validation loss: 2.382055292847336

Epoch: 5| Step: 8
Training loss: 2.479039192199707
Validation loss: 2.3855854952207176

Epoch: 5| Step: 9
Training loss: 2.7321174144744873
Validation loss: 2.376870119443504

Epoch: 5| Step: 10
Training loss: 3.0129175186157227
Validation loss: 2.377997265067152

Epoch: 177| Step: 0
Training loss: 1.8946459293365479
Validation loss: 2.3773918074946248

Epoch: 5| Step: 1
Training loss: 2.100135326385498
Validation loss: 2.3536506852796

Epoch: 5| Step: 2
Training loss: 2.6700806617736816
Validation loss: 2.3489088191780993

Epoch: 5| Step: 3
Training loss: 3.042771577835083
Validation loss: 2.341136036380645

Epoch: 5| Step: 4
Training loss: 3.3234896659851074
Validation loss: 2.3402442239945933

Epoch: 5| Step: 5
Training loss: 2.449385643005371
Validation loss: 2.339517493401804

Epoch: 5| Step: 6
Training loss: 2.130152463912964
Validation loss: 2.3398639155972387

Epoch: 5| Step: 7
Training loss: 3.113328218460083
Validation loss: 2.3340245575033207

Epoch: 5| Step: 8
Training loss: 2.141864776611328
Validation loss: 2.3335460334695797

Epoch: 5| Step: 9
Training loss: 2.8885092735290527
Validation loss: 2.3359083308968493

Epoch: 5| Step: 10
Training loss: 2.562000036239624
Validation loss: 2.332761541489632

Epoch: 178| Step: 0
Training loss: 2.33412766456604
Validation loss: 2.3354008889967397

Epoch: 5| Step: 1
Training loss: 2.8281333446502686
Validation loss: 2.3402932715672318

Epoch: 5| Step: 2
Training loss: 2.7468931674957275
Validation loss: 2.335569215077226

Epoch: 5| Step: 3
Training loss: 2.366654872894287
Validation loss: 2.338089394313033

Epoch: 5| Step: 4
Training loss: 2.5892536640167236
Validation loss: 2.335745503825526

Epoch: 5| Step: 5
Training loss: 2.2244679927825928
Validation loss: 2.335315901746032

Epoch: 5| Step: 6
Training loss: 2.7358100414276123
Validation loss: 2.340147751633839

Epoch: 5| Step: 7
Training loss: 2.7102134227752686
Validation loss: 2.3429721965584704

Epoch: 5| Step: 8
Training loss: 2.6381728649139404
Validation loss: 2.34534962202913

Epoch: 5| Step: 9
Training loss: 2.7311511039733887
Validation loss: 2.3553987933743383

Epoch: 5| Step: 10
Training loss: 2.4218506813049316
Validation loss: 2.3471636900337796

Epoch: 179| Step: 0
Training loss: 2.448139190673828
Validation loss: 2.3571082058773247

Epoch: 5| Step: 1
Training loss: 2.7128288745880127
Validation loss: 2.3614514758509975

Epoch: 5| Step: 2
Training loss: 3.0843119621276855
Validation loss: 2.3659988270011

Epoch: 5| Step: 3
Training loss: 3.1469569206237793
Validation loss: 2.3535903448699624

Epoch: 5| Step: 4
Training loss: 2.120241165161133
Validation loss: 2.349593677828389

Epoch: 5| Step: 5
Training loss: 2.4367549419403076
Validation loss: 2.3318687587656

Epoch: 5| Step: 6
Training loss: 2.479627847671509
Validation loss: 2.3269051454400502

Epoch: 5| Step: 7
Training loss: 2.625843048095703
Validation loss: 2.328940155685589

Epoch: 5| Step: 8
Training loss: 2.1916422843933105
Validation loss: 2.3253943574044014

Epoch: 5| Step: 9
Training loss: 2.4713988304138184
Validation loss: 2.328072455621535

Epoch: 5| Step: 10
Training loss: 2.681976079940796
Validation loss: 2.3287550967226744

Epoch: 180| Step: 0
Training loss: 2.199974536895752
Validation loss: 2.329030503508865

Epoch: 5| Step: 1
Training loss: 2.4585018157958984
Validation loss: 2.3253874112200994

Epoch: 5| Step: 2
Training loss: 2.0653891563415527
Validation loss: 2.33117413520813

Epoch: 5| Step: 3
Training loss: 2.742443561553955
Validation loss: 2.3339686239919355

Epoch: 5| Step: 4
Training loss: 2.256423234939575
Validation loss: 2.3355112114260272

Epoch: 5| Step: 5
Training loss: 2.305851697921753
Validation loss: 2.3401149242155013

Epoch: 5| Step: 6
Training loss: 2.7157442569732666
Validation loss: 2.340835334152304

Epoch: 5| Step: 7
Training loss: 3.3993735313415527
Validation loss: 2.343830059933406

Epoch: 5| Step: 8
Training loss: 2.695899486541748
Validation loss: 2.342799666107342

Epoch: 5| Step: 9
Training loss: 2.66798734664917
Validation loss: 2.3392419815063477

Epoch: 5| Step: 10
Training loss: 2.991292953491211
Validation loss: 2.3363004974139634

Epoch: 181| Step: 0
Training loss: 3.123964786529541
Validation loss: 2.3433537149942048

Epoch: 5| Step: 1
Training loss: 2.242544174194336
Validation loss: 2.349147237757201

Epoch: 5| Step: 2
Training loss: 2.577998161315918
Validation loss: 2.346915042528542

Epoch: 5| Step: 3
Training loss: 2.555419445037842
Validation loss: 2.338132576275897

Epoch: 5| Step: 4
Training loss: 1.9934556484222412
Validation loss: 2.33261372709787

Epoch: 5| Step: 5
Training loss: 3.085737705230713
Validation loss: 2.332745175207815

Epoch: 5| Step: 6
Training loss: 2.486755132675171
Validation loss: 2.3384661161771385

Epoch: 5| Step: 7
Training loss: 2.0313708782196045
Validation loss: 2.3279296172562467

Epoch: 5| Step: 8
Training loss: 2.8555147647857666
Validation loss: 2.332159109013055

Epoch: 5| Step: 9
Training loss: 2.702202320098877
Validation loss: 2.336326358138874

Epoch: 5| Step: 10
Training loss: 2.6570088863372803
Validation loss: 2.3385757553961968

Epoch: 182| Step: 0
Training loss: 3.631639003753662
Validation loss: 2.338988575884091

Epoch: 5| Step: 1
Training loss: 2.8140196800231934
Validation loss: 2.337785469588413

Epoch: 5| Step: 2
Training loss: 2.494215726852417
Validation loss: 2.342124000672371

Epoch: 5| Step: 3
Training loss: 2.4800705909729004
Validation loss: 2.341627005607851

Epoch: 5| Step: 4
Training loss: 2.339402675628662
Validation loss: 2.3416760172895206

Epoch: 5| Step: 5
Training loss: 2.6803574562072754
Validation loss: 2.3438088791344756

Epoch: 5| Step: 6
Training loss: 1.9954684972763062
Validation loss: 2.3478751669647875

Epoch: 5| Step: 7
Training loss: 2.972440242767334
Validation loss: 2.3456028148692143

Epoch: 5| Step: 8
Training loss: 2.3334121704101562
Validation loss: 2.3508823969030894

Epoch: 5| Step: 9
Training loss: 2.4137320518493652
Validation loss: 2.3503604037787325

Epoch: 5| Step: 10
Training loss: 2.047471046447754
Validation loss: 2.345108219372329

Epoch: 183| Step: 0
Training loss: 2.127782106399536
Validation loss: 2.3466036704278763

Epoch: 5| Step: 1
Training loss: 2.3176674842834473
Validation loss: 2.3385033710028535

Epoch: 5| Step: 2
Training loss: 3.102689743041992
Validation loss: 2.3366877237955728

Epoch: 5| Step: 3
Training loss: 2.4595141410827637
Validation loss: 2.335886198987243

Epoch: 5| Step: 4
Training loss: 2.3781676292419434
Validation loss: 2.33941569635945

Epoch: 5| Step: 5
Training loss: 2.4336097240448
Validation loss: 2.328536202830653

Epoch: 5| Step: 6
Training loss: 2.7123193740844727
Validation loss: 2.325384621979088

Epoch: 5| Step: 7
Training loss: 2.717028856277466
Validation loss: 2.329229941932104

Epoch: 5| Step: 8
Training loss: 2.5761125087738037
Validation loss: 2.337507765780213

Epoch: 5| Step: 9
Training loss: 2.789124011993408
Validation loss: 2.33483971318891

Epoch: 5| Step: 10
Training loss: 2.6611170768737793
Validation loss: 2.328856555364465

Epoch: 184| Step: 0
Training loss: 2.477480411529541
Validation loss: 2.3383477862163256

Epoch: 5| Step: 1
Training loss: 2.8790950775146484
Validation loss: 2.327224777590844

Epoch: 5| Step: 2
Training loss: 2.43269681930542
Validation loss: 2.327554984759259

Epoch: 5| Step: 3
Training loss: 2.4007506370544434
Validation loss: 2.325708379027664

Epoch: 5| Step: 4
Training loss: 2.512713670730591
Validation loss: 2.327432217136506

Epoch: 5| Step: 5
Training loss: 1.939307451248169
Validation loss: 2.329730277420372

Epoch: 5| Step: 6
Training loss: 2.6530580520629883
Validation loss: 2.3295722507661387

Epoch: 5| Step: 7
Training loss: 2.375572681427002
Validation loss: 2.330994757272864

Epoch: 5| Step: 8
Training loss: 2.7298812866210938
Validation loss: 2.3278746374191774

Epoch: 5| Step: 9
Training loss: 2.9204373359680176
Validation loss: 2.3251666048521638

Epoch: 5| Step: 10
Training loss: 2.948157787322998
Validation loss: 2.3274860125716015

Epoch: 185| Step: 0
Training loss: 2.5010783672332764
Validation loss: 2.3280752910080778

Epoch: 5| Step: 1
Training loss: 2.4637274742126465
Validation loss: 2.325102839418637

Epoch: 5| Step: 2
Training loss: 2.807969570159912
Validation loss: 2.3282957102662776

Epoch: 5| Step: 3
Training loss: 2.8781402111053467
Validation loss: 2.3344790166424167

Epoch: 5| Step: 4
Training loss: 2.3682591915130615
Validation loss: 2.3348413949371665

Epoch: 5| Step: 5
Training loss: 2.7485127449035645
Validation loss: 2.3315186910731818

Epoch: 5| Step: 6
Training loss: 2.3193764686584473
Validation loss: 2.3410665809467273

Epoch: 5| Step: 7
Training loss: 2.3545851707458496
Validation loss: 2.3401229868653

Epoch: 5| Step: 8
Training loss: 2.452223539352417
Validation loss: 2.3513961735592095

Epoch: 5| Step: 9
Training loss: 2.704935312271118
Validation loss: 2.3601455303930465

Epoch: 5| Step: 10
Training loss: 2.592820644378662
Validation loss: 2.36101036174323

Epoch: 186| Step: 0
Training loss: 2.2638943195343018
Validation loss: 2.3648042781378633

Epoch: 5| Step: 1
Training loss: 2.4122719764709473
Validation loss: 2.3672636939633276

Epoch: 5| Step: 2
Training loss: 3.042466402053833
Validation loss: 2.363798605498447

Epoch: 5| Step: 3
Training loss: 2.2362546920776367
Validation loss: 2.351046533994777

Epoch: 5| Step: 4
Training loss: 2.92450213432312
Validation loss: 2.3463384874405397

Epoch: 5| Step: 5
Training loss: 3.1942105293273926
Validation loss: 2.346695674363003

Epoch: 5| Step: 6
Training loss: 2.0235562324523926
Validation loss: 2.3348204089749243

Epoch: 5| Step: 7
Training loss: 2.4767868518829346
Validation loss: 2.3276305890852407

Epoch: 5| Step: 8
Training loss: 2.2472705841064453
Validation loss: 2.324324941122404

Epoch: 5| Step: 9
Training loss: 2.780468463897705
Validation loss: 2.324542071229668

Epoch: 5| Step: 10
Training loss: 2.6550066471099854
Validation loss: 2.3213277478371896

Epoch: 187| Step: 0
Training loss: 2.5045437812805176
Validation loss: 2.3223592286468833

Epoch: 5| Step: 1
Training loss: 2.921692371368408
Validation loss: 2.3182657252075853

Epoch: 5| Step: 2
Training loss: 2.8184683322906494
Validation loss: 2.3280515824594805

Epoch: 5| Step: 3
Training loss: 2.614061117172241
Validation loss: 2.333361243688932

Epoch: 5| Step: 4
Training loss: 2.523699998855591
Validation loss: 2.342887709217687

Epoch: 5| Step: 5
Training loss: 2.3306658267974854
Validation loss: 2.35562821613845

Epoch: 5| Step: 6
Training loss: 2.630455255508423
Validation loss: 2.361299157142639

Epoch: 5| Step: 7
Training loss: 2.4045233726501465
Validation loss: 2.3776305901106967

Epoch: 5| Step: 8
Training loss: 2.0730948448181152
Validation loss: 2.383935974490258

Epoch: 5| Step: 9
Training loss: 3.1483421325683594
Validation loss: 2.3916783243097286

Epoch: 5| Step: 10
Training loss: 2.141911506652832
Validation loss: 2.372582609935473

Epoch: 188| Step: 0
Training loss: 2.1641461849212646
Validation loss: 2.3457834182247037

Epoch: 5| Step: 1
Training loss: 2.001737117767334
Validation loss: 2.3248846454005085

Epoch: 5| Step: 2
Training loss: 2.849691152572632
Validation loss: 2.327278264107243

Epoch: 5| Step: 3
Training loss: 2.535935640335083
Validation loss: 2.323463733478259

Epoch: 5| Step: 4
Training loss: 2.568575620651245
Validation loss: 2.322444608134608

Epoch: 5| Step: 5
Training loss: 2.7755067348480225
Validation loss: 2.328354520182456

Epoch: 5| Step: 6
Training loss: 2.610250473022461
Validation loss: 2.3217317263285318

Epoch: 5| Step: 7
Training loss: 2.593000888824463
Validation loss: 2.3273513342744563

Epoch: 5| Step: 8
Training loss: 2.7174792289733887
Validation loss: 2.320547439718759

Epoch: 5| Step: 9
Training loss: 2.8862709999084473
Validation loss: 2.3182455557648853

Epoch: 5| Step: 10
Training loss: 2.618413209915161
Validation loss: 2.315388474413144

Epoch: 189| Step: 0
Training loss: 2.366652250289917
Validation loss: 2.3226990289585565

Epoch: 5| Step: 1
Training loss: 2.512273073196411
Validation loss: 2.314170532329108

Epoch: 5| Step: 2
Training loss: 1.8530209064483643
Validation loss: 2.3300654324152137

Epoch: 5| Step: 3
Training loss: 2.77982759475708
Validation loss: 2.3438635000618557

Epoch: 5| Step: 4
Training loss: 2.6855709552764893
Validation loss: 2.350640684045771

Epoch: 5| Step: 5
Training loss: 2.942139148712158
Validation loss: 2.3474597136179605

Epoch: 5| Step: 6
Training loss: 2.2858142852783203
Validation loss: 2.34677053395138

Epoch: 5| Step: 7
Training loss: 2.2257111072540283
Validation loss: 2.3378213015935754

Epoch: 5| Step: 8
Training loss: 2.9751672744750977
Validation loss: 2.343444114090294

Epoch: 5| Step: 9
Training loss: 2.58884334564209
Validation loss: 2.333011761788399

Epoch: 5| Step: 10
Training loss: 3.0680577754974365
Validation loss: 2.3280772342476794

Epoch: 190| Step: 0
Training loss: 2.320894241333008
Validation loss: 2.3268082680240756

Epoch: 5| Step: 1
Training loss: 2.552973985671997
Validation loss: 2.3199192554719987

Epoch: 5| Step: 2
Training loss: 2.661395788192749
Validation loss: 2.3156495658300256

Epoch: 5| Step: 3
Training loss: 2.481238603591919
Validation loss: 2.3181928075769895

Epoch: 5| Step: 4
Training loss: 2.1744933128356934
Validation loss: 2.3123650499569472

Epoch: 5| Step: 5
Training loss: 2.21832013130188
Validation loss: 2.318071711447931

Epoch: 5| Step: 6
Training loss: 2.542269229888916
Validation loss: 2.321754196638702

Epoch: 5| Step: 7
Training loss: 2.7721705436706543
Validation loss: 2.314288781535241

Epoch: 5| Step: 8
Training loss: 2.825010299682617
Validation loss: 2.328154294721542

Epoch: 5| Step: 9
Training loss: 2.6734633445739746
Validation loss: 2.3320956845437326

Epoch: 5| Step: 10
Training loss: 2.9875214099884033
Validation loss: 2.3484729259244856

Epoch: 191| Step: 0
Training loss: 2.7479991912841797
Validation loss: 2.349090544126367

Epoch: 5| Step: 1
Training loss: 2.7442500591278076
Validation loss: 2.345760178822343

Epoch: 5| Step: 2
Training loss: 2.0918123722076416
Validation loss: 2.3346326966439523

Epoch: 5| Step: 3
Training loss: 3.0521438121795654
Validation loss: 2.3270337709816555

Epoch: 5| Step: 4
Training loss: 2.7714343070983887
Validation loss: 2.322966428213222

Epoch: 5| Step: 5
Training loss: 2.2161240577697754
Validation loss: 2.3249536996246665

Epoch: 5| Step: 6
Training loss: 2.129971742630005
Validation loss: 2.3244946772052395

Epoch: 5| Step: 7
Training loss: 2.787018299102783
Validation loss: 2.326506599303215

Epoch: 5| Step: 8
Training loss: 2.7832298278808594
Validation loss: 2.3057616756808375

Epoch: 5| Step: 9
Training loss: 2.3823630809783936
Validation loss: 2.306326381621822

Epoch: 5| Step: 10
Training loss: 2.4326868057250977
Validation loss: 2.305687430084393

Epoch: 192| Step: 0
Training loss: 2.361133098602295
Validation loss: 2.3052153894978185

Epoch: 5| Step: 1
Training loss: 2.10516619682312
Validation loss: 2.307810196312525

Epoch: 5| Step: 2
Training loss: 2.594336986541748
Validation loss: 2.314014765524095

Epoch: 5| Step: 3
Training loss: 2.357515811920166
Validation loss: 2.3150897000425603

Epoch: 5| Step: 4
Training loss: 2.477120876312256
Validation loss: 2.322312867769631

Epoch: 5| Step: 5
Training loss: 2.765735626220703
Validation loss: 2.321773423943468

Epoch: 5| Step: 6
Training loss: 2.688354253768921
Validation loss: 2.3278994585878108

Epoch: 5| Step: 7
Training loss: 2.703824996948242
Validation loss: 2.3215670752268966

Epoch: 5| Step: 8
Training loss: 2.8031115531921387
Validation loss: 2.322851783485823

Epoch: 5| Step: 9
Training loss: 2.555812358856201
Validation loss: 2.3226166976395475

Epoch: 5| Step: 10
Training loss: 2.953416109085083
Validation loss: 2.326556451859013

Epoch: 193| Step: 0
Training loss: 1.9783427715301514
Validation loss: 2.3314982729573406

Epoch: 5| Step: 1
Training loss: 2.887019634246826
Validation loss: 2.3282643415594615

Epoch: 5| Step: 2
Training loss: 2.751819610595703
Validation loss: 2.334850416388563

Epoch: 5| Step: 3
Training loss: 2.289914608001709
Validation loss: 2.3328126348474973

Epoch: 5| Step: 4
Training loss: 2.2472305297851562
Validation loss: 2.3195924194910194

Epoch: 5| Step: 5
Training loss: 2.621896266937256
Validation loss: 2.3214544685938026

Epoch: 5| Step: 6
Training loss: 2.6372838020324707
Validation loss: 2.320382179752473

Epoch: 5| Step: 7
Training loss: 2.700533628463745
Validation loss: 2.3173233565463813

Epoch: 5| Step: 8
Training loss: 2.589360475540161
Validation loss: 2.3239128358902468

Epoch: 5| Step: 9
Training loss: 2.6765077114105225
Validation loss: 2.317550959125642

Epoch: 5| Step: 10
Training loss: 2.6422622203826904
Validation loss: 2.3259046898093274

Epoch: 194| Step: 0
Training loss: 3.063943862915039
Validation loss: 2.3184144061098815

Epoch: 5| Step: 1
Training loss: 2.8400158882141113
Validation loss: 2.3393897766708047

Epoch: 5| Step: 2
Training loss: 1.8986036777496338
Validation loss: 2.3282422327226207

Epoch: 5| Step: 3
Training loss: 2.344350576400757
Validation loss: 2.3339438348688106

Epoch: 5| Step: 4
Training loss: 2.669522285461426
Validation loss: 2.344276705095845

Epoch: 5| Step: 5
Training loss: 2.330496311187744
Validation loss: 2.3374315359259166

Epoch: 5| Step: 6
Training loss: 3.0747485160827637
Validation loss: 2.333628164824619

Epoch: 5| Step: 7
Training loss: 2.0241358280181885
Validation loss: 2.328945211184922

Epoch: 5| Step: 8
Training loss: 2.383094549179077
Validation loss: 2.3373982111612954

Epoch: 5| Step: 9
Training loss: 2.384103298187256
Validation loss: 2.330832835166685

Epoch: 5| Step: 10
Training loss: 3.0630407333374023
Validation loss: 2.339487765424995

Epoch: 195| Step: 0
Training loss: 2.283677577972412
Validation loss: 2.3260360456282094

Epoch: 5| Step: 1
Training loss: 1.9329750537872314
Validation loss: 2.325553273641935

Epoch: 5| Step: 2
Training loss: 2.5787153244018555
Validation loss: 2.336639291496687

Epoch: 5| Step: 3
Training loss: 1.6479847431182861
Validation loss: 2.318426439839025

Epoch: 5| Step: 4
Training loss: 2.491562843322754
Validation loss: 2.3271286077396844

Epoch: 5| Step: 5
Training loss: 2.752549409866333
Validation loss: 2.3293466798720823

Epoch: 5| Step: 6
Training loss: 2.787152051925659
Validation loss: 2.331772509441581

Epoch: 5| Step: 7
Training loss: 2.619866371154785
Validation loss: 2.3331793226221555

Epoch: 5| Step: 8
Training loss: 3.191147565841675
Validation loss: 2.334708272769887

Epoch: 5| Step: 9
Training loss: 2.5790162086486816
Validation loss: 2.333620761030464

Epoch: 5| Step: 10
Training loss: 3.245699167251587
Validation loss: 2.3307963289240354

Epoch: 196| Step: 0
Training loss: 2.516787528991699
Validation loss: 2.329749620088967

Epoch: 5| Step: 1
Training loss: 2.386836528778076
Validation loss: 2.332122033642184

Epoch: 5| Step: 2
Training loss: 2.6648733615875244
Validation loss: 2.3400573025467577

Epoch: 5| Step: 3
Training loss: 2.413597583770752
Validation loss: 2.338483818115727

Epoch: 5| Step: 4
Training loss: 2.3439979553222656
Validation loss: 2.3321858734212895

Epoch: 5| Step: 5
Training loss: 2.752703905105591
Validation loss: 2.326826023799117

Epoch: 5| Step: 6
Training loss: 2.770596504211426
Validation loss: 2.3220999266511653

Epoch: 5| Step: 7
Training loss: 2.5328972339630127
Validation loss: 2.306108849022978

Epoch: 5| Step: 8
Training loss: 2.739163875579834
Validation loss: 2.3208474215640815

Epoch: 5| Step: 9
Training loss: 2.3730900287628174
Validation loss: 2.305099933378158

Epoch: 5| Step: 10
Training loss: 2.432666540145874
Validation loss: 2.30411861788842

Epoch: 197| Step: 0
Training loss: 2.5498006343841553
Validation loss: 2.2977893070508073

Epoch: 5| Step: 1
Training loss: 2.2424888610839844
Validation loss: 2.30157071416096

Epoch: 5| Step: 2
Training loss: 2.7406868934631348
Validation loss: 2.2954614905900854

Epoch: 5| Step: 3
Training loss: 3.2476372718811035
Validation loss: 2.2979108082350863

Epoch: 5| Step: 4
Training loss: 2.7373909950256348
Validation loss: 2.3068501564764206

Epoch: 5| Step: 5
Training loss: 2.246181011199951
Validation loss: 2.3107695528255996

Epoch: 5| Step: 6
Training loss: 2.1607351303100586
Validation loss: 2.311478663516301

Epoch: 5| Step: 7
Training loss: 2.311156749725342
Validation loss: 2.3101573451872794

Epoch: 5| Step: 8
Training loss: 2.1864094734191895
Validation loss: 2.316222885603546

Epoch: 5| Step: 9
Training loss: 3.0276126861572266
Validation loss: 2.315174074583156

Epoch: 5| Step: 10
Training loss: 2.6792428493499756
Validation loss: 2.305700320069508

Epoch: 198| Step: 0
Training loss: 2.5153517723083496
Validation loss: 2.3147663685583297

Epoch: 5| Step: 1
Training loss: 2.426953077316284
Validation loss: 2.312769141248477

Epoch: 5| Step: 2
Training loss: 2.315375804901123
Validation loss: 2.3151714468515046

Epoch: 5| Step: 3
Training loss: 2.370729923248291
Validation loss: 2.3122341145751295

Epoch: 5| Step: 4
Training loss: 2.6268370151519775
Validation loss: 2.313906741398637

Epoch: 5| Step: 5
Training loss: 2.4440793991088867
Validation loss: 2.303189459667411

Epoch: 5| Step: 6
Training loss: 2.4901726245880127
Validation loss: 2.3083119059121735

Epoch: 5| Step: 7
Training loss: 2.9273312091827393
Validation loss: 2.309944555323611

Epoch: 5| Step: 8
Training loss: 2.120673179626465
Validation loss: 2.3087790191814466

Epoch: 5| Step: 9
Training loss: 3.042640209197998
Validation loss: 2.315906819476876

Epoch: 5| Step: 10
Training loss: 2.709454298019409
Validation loss: 2.321161767487885

Epoch: 199| Step: 0
Training loss: 2.6195902824401855
Validation loss: 2.316777172908988

Epoch: 5| Step: 1
Training loss: 2.2615914344787598
Validation loss: 2.3163979412407003

Epoch: 5| Step: 2
Training loss: 2.470057964324951
Validation loss: 2.3209791055289646

Epoch: 5| Step: 3
Training loss: 2.5530810356140137
Validation loss: 2.3276771627446657

Epoch: 5| Step: 4
Training loss: 2.7647523880004883
Validation loss: 2.3289283552477436

Epoch: 5| Step: 5
Training loss: 1.9754232168197632
Validation loss: 2.327697564196843

Epoch: 5| Step: 6
Training loss: 2.665117025375366
Validation loss: 2.33889707954981

Epoch: 5| Step: 7
Training loss: 1.8769077062606812
Validation loss: 2.337745851086032

Epoch: 5| Step: 8
Training loss: 2.756218433380127
Validation loss: 2.3391924699147544

Epoch: 5| Step: 9
Training loss: 3.4618916511535645
Validation loss: 2.3246122457647838

Epoch: 5| Step: 10
Training loss: 2.6548609733581543
Validation loss: 2.315348140655025

Epoch: 200| Step: 0
Training loss: 2.8607802391052246
Validation loss: 2.3196911247827674

Epoch: 5| Step: 1
Training loss: 1.800591230392456
Validation loss: 2.312510457090152

Epoch: 5| Step: 2
Training loss: 3.564204454421997
Validation loss: 2.3186920817180345

Epoch: 5| Step: 3
Training loss: 3.014468193054199
Validation loss: 2.3123957085353073

Epoch: 5| Step: 4
Training loss: 3.0056252479553223
Validation loss: 2.316695285099809

Epoch: 5| Step: 5
Training loss: 2.3650383949279785
Validation loss: 2.322122161106397

Epoch: 5| Step: 6
Training loss: 1.7804012298583984
Validation loss: 2.3255172724364908

Epoch: 5| Step: 7
Training loss: 2.472806930541992
Validation loss: 2.317785277161547

Epoch: 5| Step: 8
Training loss: 2.080674648284912
Validation loss: 2.3133630265471754

Epoch: 5| Step: 9
Training loss: 2.430105209350586
Validation loss: 2.313454786936442

Epoch: 5| Step: 10
Training loss: 2.5332014560699463
Validation loss: 2.3194172305445515

Epoch: 201| Step: 0
Training loss: 1.4493553638458252
Validation loss: 2.315035230369978

Epoch: 5| Step: 1
Training loss: 2.6395678520202637
Validation loss: 2.319923585461032

Epoch: 5| Step: 2
Training loss: 2.394110679626465
Validation loss: 2.319037519475465

Epoch: 5| Step: 3
Training loss: 2.720590353012085
Validation loss: 2.3177211182091826

Epoch: 5| Step: 4
Training loss: 2.1935501098632812
Validation loss: 2.328091240698291

Epoch: 5| Step: 5
Training loss: 2.872694730758667
Validation loss: 2.317898360631799

Epoch: 5| Step: 6
Training loss: 3.1882729530334473
Validation loss: 2.310438458637525

Epoch: 5| Step: 7
Training loss: 2.6176981925964355
Validation loss: 2.299161239336896

Epoch: 5| Step: 8
Training loss: 2.240020275115967
Validation loss: 2.3119188175406507

Epoch: 5| Step: 9
Training loss: 2.569066286087036
Validation loss: 2.3229897304247786

Epoch: 5| Step: 10
Training loss: 3.1012122631073
Validation loss: 2.3444242528689805

Epoch: 202| Step: 0
Training loss: 2.096343517303467
Validation loss: 2.336044107714007

Epoch: 5| Step: 1
Training loss: 3.0962424278259277
Validation loss: 2.350889790442682

Epoch: 5| Step: 2
Training loss: 2.6536853313446045
Validation loss: 2.355152448018392

Epoch: 5| Step: 3
Training loss: 2.3770999908447266
Validation loss: 2.362638012055428

Epoch: 5| Step: 4
Training loss: 2.3059728145599365
Validation loss: 2.341276007313882

Epoch: 5| Step: 5
Training loss: 1.93340265750885
Validation loss: 2.322989099769182

Epoch: 5| Step: 6
Training loss: 3.135930299758911
Validation loss: 2.3105833889335714

Epoch: 5| Step: 7
Training loss: 2.37849497795105
Validation loss: 2.3029729832885084

Epoch: 5| Step: 8
Training loss: 2.5974860191345215
Validation loss: 2.312883471929899

Epoch: 5| Step: 9
Training loss: 2.5583436489105225
Validation loss: 2.3069053132046937

Epoch: 5| Step: 10
Training loss: 2.7695820331573486
Validation loss: 2.314812501271566

Epoch: 203| Step: 0
Training loss: 2.5588536262512207
Validation loss: 2.3184845908995597

Epoch: 5| Step: 1
Training loss: 2.1554527282714844
Validation loss: 2.33536462501813

Epoch: 5| Step: 2
Training loss: 3.2665886878967285
Validation loss: 2.344993947654642

Epoch: 5| Step: 3
Training loss: 2.1893794536590576
Validation loss: 2.347270163156653

Epoch: 5| Step: 4
Training loss: 3.3397574424743652
Validation loss: 2.3263992442879626

Epoch: 5| Step: 5
Training loss: 2.860968828201294
Validation loss: 2.3085507615920036

Epoch: 5| Step: 6
Training loss: 2.1726019382476807
Validation loss: 2.2783497020762455

Epoch: 5| Step: 7
Training loss: 2.3580594062805176
Validation loss: 2.2850716216589815

Epoch: 5| Step: 8
Training loss: 2.0667128562927246
Validation loss: 2.2864516114675872

Epoch: 5| Step: 9
Training loss: 2.3097195625305176
Validation loss: 2.2905654804680937

Epoch: 5| Step: 10
Training loss: 2.9838926792144775
Validation loss: 2.291732488139983

Epoch: 204| Step: 0
Training loss: 2.603966236114502
Validation loss: 2.2934608254381406

Epoch: 5| Step: 1
Training loss: 2.6980373859405518
Validation loss: 2.2999948916896695

Epoch: 5| Step: 2
Training loss: 2.543489933013916
Validation loss: 2.2982519852217806

Epoch: 5| Step: 3
Training loss: 2.39216685295105
Validation loss: 2.291126012802124

Epoch: 5| Step: 4
Training loss: 3.1841931343078613
Validation loss: 2.2930817655337754

Epoch: 5| Step: 5
Training loss: 2.9962549209594727
Validation loss: 2.2885146756326

Epoch: 5| Step: 6
Training loss: 2.602342128753662
Validation loss: 2.28904963052401

Epoch: 5| Step: 7
Training loss: 1.894272804260254
Validation loss: 2.298935377469627

Epoch: 5| Step: 8
Training loss: 1.9226707220077515
Validation loss: 2.307309291696036

Epoch: 5| Step: 9
Training loss: 2.883575916290283
Validation loss: 2.3249980711167857

Epoch: 5| Step: 10
Training loss: 2.328368663787842
Validation loss: 2.3268397174855715

Epoch: 205| Step: 0
Training loss: 2.383800506591797
Validation loss: 2.331693066063748

Epoch: 5| Step: 1
Training loss: 2.864905834197998
Validation loss: 2.3206907959394556

Epoch: 5| Step: 2
Training loss: 2.193131685256958
Validation loss: 2.3099716658233316

Epoch: 5| Step: 3
Training loss: 2.4907498359680176
Validation loss: 2.309765241479361

Epoch: 5| Step: 4
Training loss: 2.2434325218200684
Validation loss: 2.317505180194814

Epoch: 5| Step: 5
Training loss: 2.7366607189178467
Validation loss: 2.3235382444115094

Epoch: 5| Step: 6
Training loss: 2.2872042655944824
Validation loss: 2.3237522058589484

Epoch: 5| Step: 7
Training loss: 2.831002712249756
Validation loss: 2.328657275886946

Epoch: 5| Step: 8
Training loss: 2.446528673171997
Validation loss: 2.347062869738507

Epoch: 5| Step: 9
Training loss: 2.4788825511932373
Validation loss: 2.3440109042711157

Epoch: 5| Step: 10
Training loss: 3.3126556873321533
Validation loss: 2.3504422480060208

Epoch: 206| Step: 0
Training loss: 2.3850760459899902
Validation loss: 2.353503837380358

Epoch: 5| Step: 1
Training loss: 3.0157248973846436
Validation loss: 2.3548674250161774

Epoch: 5| Step: 2
Training loss: 2.5811119079589844
Validation loss: 2.3406630639106996

Epoch: 5| Step: 3
Training loss: 2.641313076019287
Validation loss: 2.3274578843065488

Epoch: 5| Step: 4
Training loss: 2.1429107189178467
Validation loss: 2.3230632620473064

Epoch: 5| Step: 5
Training loss: 2.347379207611084
Validation loss: 2.3144433677837415

Epoch: 5| Step: 6
Training loss: 2.3718419075012207
Validation loss: 2.311147571891867

Epoch: 5| Step: 7
Training loss: 2.7252743244171143
Validation loss: 2.3064441962908675

Epoch: 5| Step: 8
Training loss: 2.1619510650634766
Validation loss: 2.315459305240262

Epoch: 5| Step: 9
Training loss: 3.459853410720825
Validation loss: 2.3310864817711616

Epoch: 5| Step: 10
Training loss: 2.2733657360076904
Validation loss: 2.3324473801479546

Epoch: 207| Step: 0
Training loss: 2.783613443374634
Validation loss: 2.33250839735872

Epoch: 5| Step: 1
Training loss: 2.9171805381774902
Validation loss: 2.3333796070468042

Epoch: 5| Step: 2
Training loss: 2.869795322418213
Validation loss: 2.3355794029851116

Epoch: 5| Step: 3
Training loss: 2.384290933609009
Validation loss: 2.3205201472005537

Epoch: 5| Step: 4
Training loss: 2.2178890705108643
Validation loss: 2.3014709052219184

Epoch: 5| Step: 5
Training loss: 2.235738754272461
Validation loss: 2.2942536082319034

Epoch: 5| Step: 6
Training loss: 2.723719358444214
Validation loss: 2.2823947065620014

Epoch: 5| Step: 7
Training loss: 2.1149916648864746
Validation loss: 2.2828033970248316

Epoch: 5| Step: 8
Training loss: 2.4201838970184326
Validation loss: 2.277014998979466

Epoch: 5| Step: 9
Training loss: 2.4090685844421387
Validation loss: 2.27291779364309

Epoch: 5| Step: 10
Training loss: 3.1795899868011475
Validation loss: 2.273081015515071

Epoch: 208| Step: 0
Training loss: 2.909693479537964
Validation loss: 2.2750512938345633

Epoch: 5| Step: 1
Training loss: 2.181406021118164
Validation loss: 2.274750642879035

Epoch: 5| Step: 2
Training loss: 2.8516907691955566
Validation loss: 2.2728588081175283

Epoch: 5| Step: 3
Training loss: 2.9535751342773438
Validation loss: 2.2831542620094876

Epoch: 5| Step: 4
Training loss: 1.711764931678772
Validation loss: 2.2793629912919897

Epoch: 5| Step: 5
Training loss: 2.21319317817688
Validation loss: 2.2836917677233295

Epoch: 5| Step: 6
Training loss: 2.285411834716797
Validation loss: 2.2818583814046716

Epoch: 5| Step: 7
Training loss: 3.1974077224731445
Validation loss: 2.283996969140986

Epoch: 5| Step: 8
Training loss: 2.5654492378234863
Validation loss: 2.282854193000383

Epoch: 5| Step: 9
Training loss: 2.3480422496795654
Validation loss: 2.274517672036284

Epoch: 5| Step: 10
Training loss: 2.850419282913208
Validation loss: 2.28451011770515

Epoch: 209| Step: 0
Training loss: 2.490726947784424
Validation loss: 2.2883779848775556

Epoch: 5| Step: 1
Training loss: 2.3431053161621094
Validation loss: 2.290050570682813

Epoch: 5| Step: 2
Training loss: 2.32631516456604
Validation loss: 2.2984872633411038

Epoch: 5| Step: 3
Training loss: 2.53428053855896
Validation loss: 2.30010344905238

Epoch: 5| Step: 4
Training loss: 2.5856354236602783
Validation loss: 2.3150498110760926

Epoch: 5| Step: 5
Training loss: 2.295675039291382
Validation loss: 2.3219331849005913

Epoch: 5| Step: 6
Training loss: 2.595454454421997
Validation loss: 2.312093662959273

Epoch: 5| Step: 7
Training loss: 2.3709521293640137
Validation loss: 2.3099183074889647

Epoch: 5| Step: 8
Training loss: 2.502547025680542
Validation loss: 2.314802644073322

Epoch: 5| Step: 9
Training loss: 2.534425735473633
Validation loss: 2.3108734366714314

Epoch: 5| Step: 10
Training loss: 3.4592998027801514
Validation loss: 2.3085727102013043

Epoch: 210| Step: 0
Training loss: 2.5702853202819824
Validation loss: 2.3110424485257877

Epoch: 5| Step: 1
Training loss: 3.195112943649292
Validation loss: 2.3104695889257614

Epoch: 5| Step: 2
Training loss: 2.4627673625946045
Validation loss: 2.3047608816495506

Epoch: 5| Step: 3
Training loss: 2.4971694946289062
Validation loss: 2.2917258970199095

Epoch: 5| Step: 4
Training loss: 2.481076717376709
Validation loss: 2.3055882659009708

Epoch: 5| Step: 5
Training loss: 2.191462755203247
Validation loss: 2.302892505481679

Epoch: 5| Step: 6
Training loss: 2.63055682182312
Validation loss: 2.3061666270738006

Epoch: 5| Step: 7
Training loss: 2.3972878456115723
Validation loss: 2.3171630905520533

Epoch: 5| Step: 8
Training loss: 2.3297998905181885
Validation loss: 2.3069988655787643

Epoch: 5| Step: 9
Training loss: 2.0160624980926514
Validation loss: 2.294319764260323

Epoch: 5| Step: 10
Training loss: 3.258151054382324
Validation loss: 2.289234676668721

Epoch: 211| Step: 0
Training loss: 2.752845048904419
Validation loss: 2.290411105719946

Epoch: 5| Step: 1
Training loss: 2.892239809036255
Validation loss: 2.282651011661817

Epoch: 5| Step: 2
Training loss: 2.2882227897644043
Validation loss: 2.2794259261059504

Epoch: 5| Step: 3
Training loss: 2.029658079147339
Validation loss: 2.2774195235262633

Epoch: 5| Step: 4
Training loss: 2.982149362564087
Validation loss: 2.27658676075679

Epoch: 5| Step: 5
Training loss: 1.9455692768096924
Validation loss: 2.2769917326588787

Epoch: 5| Step: 6
Training loss: 2.6551642417907715
Validation loss: 2.272432760525775

Epoch: 5| Step: 7
Training loss: 2.7158994674682617
Validation loss: 2.279997082166774

Epoch: 5| Step: 8
Training loss: 2.5891432762145996
Validation loss: 2.2681102675776326

Epoch: 5| Step: 9
Training loss: 2.4429593086242676
Validation loss: 2.2754693915767055

Epoch: 5| Step: 10
Training loss: 2.5998098850250244
Validation loss: 2.2792224704578357

Epoch: 212| Step: 0
Training loss: 2.0019783973693848
Validation loss: 2.2897011721006004

Epoch: 5| Step: 1
Training loss: 3.2269129753112793
Validation loss: 2.316712305109988

Epoch: 5| Step: 2
Training loss: 2.1076228618621826
Validation loss: 2.3451245600177395

Epoch: 5| Step: 3
Training loss: 2.5358195304870605
Validation loss: 2.363604486629527

Epoch: 5| Step: 4
Training loss: 2.4071707725524902
Validation loss: 2.3676242520732265

Epoch: 5| Step: 5
Training loss: 2.713931083679199
Validation loss: 2.3652157911690335

Epoch: 5| Step: 6
Training loss: 2.4039392471313477
Validation loss: 2.3720986458563034

Epoch: 5| Step: 7
Training loss: 2.3402178287506104
Validation loss: 2.354348031423425

Epoch: 5| Step: 8
Training loss: 2.6934714317321777
Validation loss: 2.3378886945786013

Epoch: 5| Step: 9
Training loss: 2.9555351734161377
Validation loss: 2.3149113911454395

Epoch: 5| Step: 10
Training loss: 2.681072950363159
Validation loss: 2.2867956648590746

Epoch: 213| Step: 0
Training loss: 2.861689805984497
Validation loss: 2.291941696597684

Epoch: 5| Step: 1
Training loss: 2.4110264778137207
Validation loss: 2.2800169555089806

Epoch: 5| Step: 2
Training loss: 2.6245598793029785
Validation loss: 2.2930712469162478

Epoch: 5| Step: 3
Training loss: 2.54099178314209
Validation loss: 2.2961298599038074

Epoch: 5| Step: 4
Training loss: 2.390137195587158
Validation loss: 2.2988475215050483

Epoch: 5| Step: 5
Training loss: 2.618856906890869
Validation loss: 2.302515793872136

Epoch: 5| Step: 6
Training loss: 2.417623996734619
Validation loss: 2.3063753035760697

Epoch: 5| Step: 7
Training loss: 2.8932509422302246
Validation loss: 2.30608376379936

Epoch: 5| Step: 8
Training loss: 2.6422982215881348
Validation loss: 2.294732263011317

Epoch: 5| Step: 9
Training loss: 2.2166290283203125
Validation loss: 2.300571349359328

Epoch: 5| Step: 10
Training loss: 2.202993869781494
Validation loss: 2.307562397372338

Epoch: 214| Step: 0
Training loss: 2.664130449295044
Validation loss: 2.3286434181274904

Epoch: 5| Step: 1
Training loss: 2.108531951904297
Validation loss: 2.356705842479583

Epoch: 5| Step: 2
Training loss: 2.466846227645874
Validation loss: 2.3952302958375666

Epoch: 5| Step: 3
Training loss: 2.5773093700408936
Validation loss: 2.4075191815694175

Epoch: 5| Step: 4
Training loss: 2.9263226985931396
Validation loss: 2.3993019314222437

Epoch: 5| Step: 5
Training loss: 2.3830623626708984
Validation loss: 2.3887725030222247

Epoch: 5| Step: 6
Training loss: 2.850966691970825
Validation loss: 2.392357564741565

Epoch: 5| Step: 7
Training loss: 2.745147228240967
Validation loss: 2.3751930754671813

Epoch: 5| Step: 8
Training loss: 2.6159653663635254
Validation loss: 2.3371843138048725

Epoch: 5| Step: 9
Training loss: 2.1699635982513428
Validation loss: 2.3295958119053997

Epoch: 5| Step: 10
Training loss: 2.569666862487793
Validation loss: 2.3050093958454747

Epoch: 215| Step: 0
Training loss: 2.5830368995666504
Validation loss: 2.3043601487272527

Epoch: 5| Step: 1
Training loss: 2.3764312267303467
Validation loss: 2.291182178322987

Epoch: 5| Step: 2
Training loss: 2.799123764038086
Validation loss: 2.2963379480505504

Epoch: 5| Step: 3
Training loss: 2.576977014541626
Validation loss: 2.2832421615559566

Epoch: 5| Step: 4
Training loss: 2.429044246673584
Validation loss: 2.2845093204129125

Epoch: 5| Step: 5
Training loss: 2.404921293258667
Validation loss: 2.2816770333115772

Epoch: 5| Step: 6
Training loss: 3.295649290084839
Validation loss: 2.291633149628998

Epoch: 5| Step: 7
Training loss: 2.4385275840759277
Validation loss: 2.283374927377188

Epoch: 5| Step: 8
Training loss: 2.0415780544281006
Validation loss: 2.2830885969182497

Epoch: 5| Step: 9
Training loss: 2.2462029457092285
Validation loss: 2.2946819848911737

Epoch: 5| Step: 10
Training loss: 2.8554835319519043
Validation loss: 2.301860732416953

Epoch: 216| Step: 0
Training loss: 1.9661662578582764
Validation loss: 2.316683897408106

Epoch: 5| Step: 1
Training loss: 2.586876392364502
Validation loss: 2.3365306290247108

Epoch: 5| Step: 2
Training loss: 2.6461262702941895
Validation loss: 2.3381309201640468

Epoch: 5| Step: 3
Training loss: 2.792773723602295
Validation loss: 2.3328923281802925

Epoch: 5| Step: 4
Training loss: 3.1321136951446533
Validation loss: 2.325764094629595

Epoch: 5| Step: 5
Training loss: 2.6909422874450684
Validation loss: 2.3096971293931365

Epoch: 5| Step: 6
Training loss: 2.455165386199951
Validation loss: 2.2971014412500526

Epoch: 5| Step: 7
Training loss: 2.765500068664551
Validation loss: 2.2765985150491037

Epoch: 5| Step: 8
Training loss: 2.093458652496338
Validation loss: 2.2672480947227887

Epoch: 5| Step: 9
Training loss: 2.097907543182373
Validation loss: 2.26719093579118

Epoch: 5| Step: 10
Training loss: 2.7477147579193115
Validation loss: 2.2709050152891423

Epoch: 217| Step: 0
Training loss: 2.8407676219940186
Validation loss: 2.2670060460285475

Epoch: 5| Step: 1
Training loss: 2.4012017250061035
Validation loss: 2.273837243357012

Epoch: 5| Step: 2
Training loss: 2.5642871856689453
Validation loss: 2.2763486164872364

Epoch: 5| Step: 3
Training loss: 2.4305193424224854
Validation loss: 2.2716892432141047

Epoch: 5| Step: 4
Training loss: 2.0871224403381348
Validation loss: 2.2798822618299917

Epoch: 5| Step: 5
Training loss: 2.8309452533721924
Validation loss: 2.272286658645958

Epoch: 5| Step: 6
Training loss: 2.635540723800659
Validation loss: 2.2757058963980725

Epoch: 5| Step: 7
Training loss: 2.871948719024658
Validation loss: 2.2709018286838325

Epoch: 5| Step: 8
Training loss: 2.192486047744751
Validation loss: 2.2827297641384985

Epoch: 5| Step: 9
Training loss: 2.8040506839752197
Validation loss: 2.286746266067669

Epoch: 5| Step: 10
Training loss: 2.097588062286377
Validation loss: 2.294065220381624

Epoch: 218| Step: 0
Training loss: 2.6269774436950684
Validation loss: 2.3047113290397068

Epoch: 5| Step: 1
Training loss: 2.468296527862549
Validation loss: 2.301685017924155

Epoch: 5| Step: 2
Training loss: 2.454150676727295
Validation loss: 2.2973978314348447

Epoch: 5| Step: 3
Training loss: 2.6598432064056396
Validation loss: 2.2976361551592426

Epoch: 5| Step: 4
Training loss: 2.543076753616333
Validation loss: 2.297589158499113

Epoch: 5| Step: 5
Training loss: 2.6559853553771973
Validation loss: 2.2783360878626504

Epoch: 5| Step: 6
Training loss: 2.8615612983703613
Validation loss: 2.2668596980392293

Epoch: 5| Step: 7
Training loss: 2.46045184135437
Validation loss: 2.2739323492973083

Epoch: 5| Step: 8
Training loss: 2.156344175338745
Validation loss: 2.279384869401173

Epoch: 5| Step: 9
Training loss: 2.3157753944396973
Validation loss: 2.2754037585309757

Epoch: 5| Step: 10
Training loss: 2.757204294204712
Validation loss: 2.27381161720522

Epoch: 219| Step: 0
Training loss: 2.625756025314331
Validation loss: 2.278823757684359

Epoch: 5| Step: 1
Training loss: 2.623047351837158
Validation loss: 2.2814143626920638

Epoch: 5| Step: 2
Training loss: 1.859155297279358
Validation loss: 2.276619848384652

Epoch: 5| Step: 3
Training loss: 2.828406572341919
Validation loss: 2.2825821535561674

Epoch: 5| Step: 4
Training loss: 1.6604877710342407
Validation loss: 2.2796595301679385

Epoch: 5| Step: 5
Training loss: 2.416867971420288
Validation loss: 2.284684174804277

Epoch: 5| Step: 6
Training loss: 2.325105667114258
Validation loss: 2.2780888900961926

Epoch: 5| Step: 7
Training loss: 3.243746519088745
Validation loss: 2.2696166474332093

Epoch: 5| Step: 8
Training loss: 2.9759953022003174
Validation loss: 2.271522988555252

Epoch: 5| Step: 9
Training loss: 2.9165396690368652
Validation loss: 2.2700727011567805

Epoch: 5| Step: 10
Training loss: 2.221494674682617
Validation loss: 2.271384880106936

Epoch: 220| Step: 0
Training loss: 2.773049831390381
Validation loss: 2.27026258232773

Epoch: 5| Step: 1
Training loss: 2.265838146209717
Validation loss: 2.262937391957929

Epoch: 5| Step: 2
Training loss: 2.851024627685547
Validation loss: 2.268469061902774

Epoch: 5| Step: 3
Training loss: 2.0920262336730957
Validation loss: 2.2619550228118896

Epoch: 5| Step: 4
Training loss: 2.4472079277038574
Validation loss: 2.2687998074357227

Epoch: 5| Step: 5
Training loss: 3.0526983737945557
Validation loss: 2.2696877115516254

Epoch: 5| Step: 6
Training loss: 2.3908634185791016
Validation loss: 2.2737105584913686

Epoch: 5| Step: 7
Training loss: 1.8787758350372314
Validation loss: 2.2735591319299515

Epoch: 5| Step: 8
Training loss: 2.819657564163208
Validation loss: 2.2836321592330933

Epoch: 5| Step: 9
Training loss: 2.2649879455566406
Validation loss: 2.2810919438638995

Epoch: 5| Step: 10
Training loss: 2.9360427856445312
Validation loss: 2.282832581509826

Epoch: 221| Step: 0
Training loss: 2.8050055503845215
Validation loss: 2.2935169563498548

Epoch: 5| Step: 1
Training loss: 2.1902871131896973
Validation loss: 2.2898450718131116

Epoch: 5| Step: 2
Training loss: 2.412050247192383
Validation loss: 2.300886345166032

Epoch: 5| Step: 3
Training loss: 3.3271045684814453
Validation loss: 2.2950614677962435

Epoch: 5| Step: 4
Training loss: 2.6487040519714355
Validation loss: 2.2980747556173675

Epoch: 5| Step: 5
Training loss: 1.894413709640503
Validation loss: 2.295253660089226

Epoch: 5| Step: 6
Training loss: 2.664522886276245
Validation loss: 2.2979352038393737

Epoch: 5| Step: 7
Training loss: 2.5987091064453125
Validation loss: 2.2943708024999148

Epoch: 5| Step: 8
Training loss: 2.476259231567383
Validation loss: 2.2837995585574897

Epoch: 5| Step: 9
Training loss: 2.6322731971740723
Validation loss: 2.2731410085514026

Epoch: 5| Step: 10
Training loss: 1.9240002632141113
Validation loss: 2.266588516132806

Epoch: 222| Step: 0
Training loss: 2.554100751876831
Validation loss: 2.264035271060082

Epoch: 5| Step: 1
Training loss: 2.0665812492370605
Validation loss: 2.272810636028167

Epoch: 5| Step: 2
Training loss: 2.1030237674713135
Validation loss: 2.27239715924827

Epoch: 5| Step: 3
Training loss: 3.0501365661621094
Validation loss: 2.2665188850895053

Epoch: 5| Step: 4
Training loss: 2.7828314304351807
Validation loss: 2.2664959763967865

Epoch: 5| Step: 5
Training loss: 3.38324236869812
Validation loss: 2.269279591498836

Epoch: 5| Step: 6
Training loss: 1.7109256982803345
Validation loss: 2.27599262678495

Epoch: 5| Step: 7
Training loss: 2.8001532554626465
Validation loss: 2.278607794033584

Epoch: 5| Step: 8
Training loss: 3.002230167388916
Validation loss: 2.2974103368738645

Epoch: 5| Step: 9
Training loss: 2.046525478363037
Validation loss: 2.296346074791365

Epoch: 5| Step: 10
Training loss: 2.2246456146240234
Validation loss: 2.2968472998629332

Epoch: 223| Step: 0
Training loss: 2.325434446334839
Validation loss: 2.2825171050205024

Epoch: 5| Step: 1
Training loss: 2.5545525550842285
Validation loss: 2.2786823575214674

Epoch: 5| Step: 2
Training loss: 2.4302241802215576
Validation loss: 2.279322762643137

Epoch: 5| Step: 3
Training loss: 2.1519553661346436
Validation loss: 2.2766725837543444

Epoch: 5| Step: 4
Training loss: 2.778804063796997
Validation loss: 2.272432017069991

Epoch: 5| Step: 5
Training loss: 1.6621707677841187
Validation loss: 2.269167010502149

Epoch: 5| Step: 6
Training loss: 2.7894349098205566
Validation loss: 2.2624688148498535

Epoch: 5| Step: 7
Training loss: 2.613399028778076
Validation loss: 2.2506489599904707

Epoch: 5| Step: 8
Training loss: 2.7893056869506836
Validation loss: 2.255114263103854

Epoch: 5| Step: 9
Training loss: 2.5651190280914307
Validation loss: 2.2540272820380425

Epoch: 5| Step: 10
Training loss: 3.124699592590332
Validation loss: 2.258750823236281

Epoch: 224| Step: 0
Training loss: 2.2404322624206543
Validation loss: 2.2538595571312854

Epoch: 5| Step: 1
Training loss: 2.2534549236297607
Validation loss: 2.261906682804067

Epoch: 5| Step: 2
Training loss: 2.9649577140808105
Validation loss: 2.2558197821340253

Epoch: 5| Step: 3
Training loss: 2.3598780632019043
Validation loss: 2.261007611469556

Epoch: 5| Step: 4
Training loss: 2.272923231124878
Validation loss: 2.265175078504829

Epoch: 5| Step: 5
Training loss: 2.718357563018799
Validation loss: 2.2771051904206634

Epoch: 5| Step: 6
Training loss: 3.065944194793701
Validation loss: 2.2840113755195373

Epoch: 5| Step: 7
Training loss: 2.217315196990967
Validation loss: 2.27945388004344

Epoch: 5| Step: 8
Training loss: 2.731834888458252
Validation loss: 2.2739883186996623

Epoch: 5| Step: 9
Training loss: 2.7587907314300537
Validation loss: 2.2812232176462808

Epoch: 5| Step: 10
Training loss: 2.140345573425293
Validation loss: 2.2831860511533675

Epoch: 225| Step: 0
Training loss: 2.729538679122925
Validation loss: 2.274936096642607

Epoch: 5| Step: 1
Training loss: 2.598201036453247
Validation loss: 2.2780191770163913

Epoch: 5| Step: 2
Training loss: 1.766172170639038
Validation loss: 2.2739157189605055

Epoch: 5| Step: 3
Training loss: 2.1334567070007324
Validation loss: 2.2683713333581084

Epoch: 5| Step: 4
Training loss: 3.3963851928710938
Validation loss: 2.271900171874672

Epoch: 5| Step: 5
Training loss: 2.166647434234619
Validation loss: 2.272921682685934

Epoch: 5| Step: 6
Training loss: 2.4148776531219482
Validation loss: 2.271138265568723

Epoch: 5| Step: 7
Training loss: 2.666046619415283
Validation loss: 2.2648794651031494

Epoch: 5| Step: 8
Training loss: 2.9367599487304688
Validation loss: 2.2630929434171287

Epoch: 5| Step: 9
Training loss: 2.4203197956085205
Validation loss: 2.2643440000472532

Epoch: 5| Step: 10
Training loss: 2.3465938568115234
Validation loss: 2.2736976864517375

Epoch: 226| Step: 0
Training loss: 2.4106600284576416
Validation loss: 2.27480802741102

Epoch: 5| Step: 1
Training loss: 2.5576529502868652
Validation loss: 2.279761706629107

Epoch: 5| Step: 2
Training loss: 2.835390567779541
Validation loss: 2.2784982765874555

Epoch: 5| Step: 3
Training loss: 2.3476836681365967
Validation loss: 2.284941775824434

Epoch: 5| Step: 4
Training loss: 2.5042920112609863
Validation loss: 2.303390113256311

Epoch: 5| Step: 5
Training loss: 2.5617337226867676
Validation loss: 2.3102155090660177

Epoch: 5| Step: 6
Training loss: 2.633714199066162
Validation loss: 2.3183916255991948

Epoch: 5| Step: 7
Training loss: 2.8777365684509277
Validation loss: 2.3077207431998303

Epoch: 5| Step: 8
Training loss: 2.4790573120117188
Validation loss: 2.2962066768318095

Epoch: 5| Step: 9
Training loss: 2.5119385719299316
Validation loss: 2.2875134432187645

Epoch: 5| Step: 10
Training loss: 1.704315185546875
Validation loss: 2.290609064922538

Epoch: 227| Step: 0
Training loss: 2.706033945083618
Validation loss: 2.2816820208744337

Epoch: 5| Step: 1
Training loss: 1.906103491783142
Validation loss: 2.280392744207895

Epoch: 5| Step: 2
Training loss: 2.591097354888916
Validation loss: 2.286315215531216

Epoch: 5| Step: 3
Training loss: 2.6541409492492676
Validation loss: 2.2707663338671447

Epoch: 5| Step: 4
Training loss: 3.2519288063049316
Validation loss: 2.2781949402183614

Epoch: 5| Step: 5
Training loss: 2.821901559829712
Validation loss: 2.272134545028851

Epoch: 5| Step: 6
Training loss: 1.6808815002441406
Validation loss: 2.2605838724361953

Epoch: 5| Step: 7
Training loss: 2.599658489227295
Validation loss: 2.2573894428950485

Epoch: 5| Step: 8
Training loss: 2.1825666427612305
Validation loss: 2.271223702738362

Epoch: 5| Step: 9
Training loss: 2.2099239826202393
Validation loss: 2.263405453774237

Epoch: 5| Step: 10
Training loss: 2.9945268630981445
Validation loss: 2.2515805818701304

Epoch: 228| Step: 0
Training loss: 2.4612579345703125
Validation loss: 2.2633021890476184

Epoch: 5| Step: 1
Training loss: 2.6972880363464355
Validation loss: 2.2668956223354546

Epoch: 5| Step: 2
Training loss: 2.779555559158325
Validation loss: 2.2727753295693347

Epoch: 5| Step: 3
Training loss: 2.579834461212158
Validation loss: 2.2745312490770893

Epoch: 5| Step: 4
Training loss: 2.2994632720947266
Validation loss: 2.2917872603221605

Epoch: 5| Step: 5
Training loss: 2.305849552154541
Validation loss: 2.2905424769206713

Epoch: 5| Step: 6
Training loss: 2.256868600845337
Validation loss: 2.294565769933885

Epoch: 5| Step: 7
Training loss: 3.1175994873046875
Validation loss: 2.282237099063012

Epoch: 5| Step: 8
Training loss: 1.8994947671890259
Validation loss: 2.2822051586643344

Epoch: 5| Step: 9
Training loss: 2.6245951652526855
Validation loss: 2.2844331315768662

Epoch: 5| Step: 10
Training loss: 2.42533540725708
Validation loss: 2.267629128630443

Epoch: 229| Step: 0
Training loss: 2.3522210121154785
Validation loss: 2.271701389743436

Epoch: 5| Step: 1
Training loss: 2.319601058959961
Validation loss: 2.2632916114663564

Epoch: 5| Step: 2
Training loss: 2.200634241104126
Validation loss: 2.2630942380556496

Epoch: 5| Step: 3
Training loss: 3.143735885620117
Validation loss: 2.2638933979054934

Epoch: 5| Step: 4
Training loss: 2.6028389930725098
Validation loss: 2.2616847971434235

Epoch: 5| Step: 5
Training loss: 1.7422754764556885
Validation loss: 2.2640352582418792

Epoch: 5| Step: 6
Training loss: 2.4073596000671387
Validation loss: 2.2695831098864154

Epoch: 5| Step: 7
Training loss: 2.39827299118042
Validation loss: 2.279120413205957

Epoch: 5| Step: 8
Training loss: 2.54777193069458
Validation loss: 2.2920154422842045

Epoch: 5| Step: 9
Training loss: 2.5593953132629395
Validation loss: 2.299809927581459

Epoch: 5| Step: 10
Training loss: 3.421008348464966
Validation loss: 2.3057907858202533

Epoch: 230| Step: 0
Training loss: 2.7879574298858643
Validation loss: 2.3196047941843667

Epoch: 5| Step: 1
Training loss: 2.3531761169433594
Validation loss: 2.3270711834712694

Epoch: 5| Step: 2
Training loss: 2.9261603355407715
Validation loss: 2.3314756783105994

Epoch: 5| Step: 3
Training loss: 2.344587802886963
Validation loss: 2.328746913581766

Epoch: 5| Step: 4
Training loss: 2.206493377685547
Validation loss: 2.319915497174827

Epoch: 5| Step: 5
Training loss: 1.7050516605377197
Validation loss: 2.3024149735768638

Epoch: 5| Step: 6
Training loss: 2.312544345855713
Validation loss: 2.286425913533857

Epoch: 5| Step: 7
Training loss: 2.5583043098449707
Validation loss: 2.2736057889076973

Epoch: 5| Step: 8
Training loss: 2.2880136966705322
Validation loss: 2.2604663577131046

Epoch: 5| Step: 9
Training loss: 2.8572490215301514
Validation loss: 2.2478311677132883

Epoch: 5| Step: 10
Training loss: 3.2876501083374023
Validation loss: 2.2588347337579213

Epoch: 231| Step: 0
Training loss: 2.511321544647217
Validation loss: 2.264173376944757

Epoch: 5| Step: 1
Training loss: 2.0772345066070557
Validation loss: 2.28167417997955

Epoch: 5| Step: 2
Training loss: 2.6746153831481934
Validation loss: 2.289544265757325

Epoch: 5| Step: 3
Training loss: 2.803074359893799
Validation loss: 2.2921965840042278

Epoch: 5| Step: 4
Training loss: 2.257211685180664
Validation loss: 2.3132622447065128

Epoch: 5| Step: 5
Training loss: 3.0146288871765137
Validation loss: 2.3274838027133735

Epoch: 5| Step: 6
Training loss: 2.968273162841797
Validation loss: 2.3398346029302126

Epoch: 5| Step: 7
Training loss: 1.8531475067138672
Validation loss: 2.338648796081543

Epoch: 5| Step: 8
Training loss: 2.8520214557647705
Validation loss: 2.3324807126034974

Epoch: 5| Step: 9
Training loss: 2.7599000930786133
Validation loss: 2.319591329943749

Epoch: 5| Step: 10
Training loss: 1.7814769744873047
Validation loss: 2.3202543104848554

Epoch: 232| Step: 0
Training loss: 2.1260719299316406
Validation loss: 2.321313981086977

Epoch: 5| Step: 1
Training loss: 2.673647403717041
Validation loss: 2.3219049207625853

Epoch: 5| Step: 2
Training loss: 3.084512710571289
Validation loss: 2.3269842978446715

Epoch: 5| Step: 3
Training loss: 2.0945863723754883
Validation loss: 2.324871955379363

Epoch: 5| Step: 4
Training loss: 1.8921873569488525
Validation loss: 2.319812015820575

Epoch: 5| Step: 5
Training loss: 2.42124605178833
Validation loss: 2.326693775833294

Epoch: 5| Step: 6
Training loss: 2.1454596519470215
Validation loss: 2.3221278805886545

Epoch: 5| Step: 7
Training loss: 2.655081272125244
Validation loss: 2.319892639754921

Epoch: 5| Step: 8
Training loss: 3.3696835041046143
Validation loss: 2.321634995040073

Epoch: 5| Step: 9
Training loss: 2.5434374809265137
Validation loss: 2.310332521315544

Epoch: 5| Step: 10
Training loss: 2.700984001159668
Validation loss: 2.3033317186499156

Epoch: 233| Step: 0
Training loss: 2.949990749359131
Validation loss: 2.2990831354612946

Epoch: 5| Step: 1
Training loss: 1.7244274616241455
Validation loss: 2.2921359795396046

Epoch: 5| Step: 2
Training loss: 2.043358325958252
Validation loss: 2.295262006021315

Epoch: 5| Step: 3
Training loss: 2.2331995964050293
Validation loss: 2.289843628483434

Epoch: 5| Step: 4
Training loss: 2.2814605236053467
Validation loss: 2.3013711616557133

Epoch: 5| Step: 5
Training loss: 3.06596040725708
Validation loss: 2.311527693143455

Epoch: 5| Step: 6
Training loss: 1.9657161235809326
Validation loss: 2.3289485900632796

Epoch: 5| Step: 7
Training loss: 2.5034477710723877
Validation loss: 2.3157355759733464

Epoch: 5| Step: 8
Training loss: 3.0014195442199707
Validation loss: 2.300266150505312

Epoch: 5| Step: 9
Training loss: 2.3270225524902344
Validation loss: 2.291963641361524

Epoch: 5| Step: 10
Training loss: 3.625404119491577
Validation loss: 2.2871140536441597

Epoch: 234| Step: 0
Training loss: 2.520627498626709
Validation loss: 2.2770354542680966

Epoch: 5| Step: 1
Training loss: 2.8253026008605957
Validation loss: 2.265585781425558

Epoch: 5| Step: 2
Training loss: 3.1032841205596924
Validation loss: 2.261089837679299

Epoch: 5| Step: 3
Training loss: 2.133755922317505
Validation loss: 2.2568823470864245

Epoch: 5| Step: 4
Training loss: 1.7617788314819336
Validation loss: 2.2701017984779934

Epoch: 5| Step: 5
Training loss: 2.5662171840667725
Validation loss: 2.269547798300302

Epoch: 5| Step: 6
Training loss: 2.3538222312927246
Validation loss: 2.261356097395702

Epoch: 5| Step: 7
Training loss: 2.2784664630889893
Validation loss: 2.2542976794704312

Epoch: 5| Step: 8
Training loss: 2.5652732849121094
Validation loss: 2.258725966176679

Epoch: 5| Step: 9
Training loss: 2.8979012966156006
Validation loss: 2.259087106233002

Epoch: 5| Step: 10
Training loss: 2.2794911861419678
Validation loss: 2.2601752435007403

Epoch: 235| Step: 0
Training loss: 2.4595911502838135
Validation loss: 2.2720121722067557

Epoch: 5| Step: 1
Training loss: 2.2455954551696777
Validation loss: 2.2697099370341145

Epoch: 5| Step: 2
Training loss: 2.239316463470459
Validation loss: 2.2812218037984704

Epoch: 5| Step: 3
Training loss: 2.8022003173828125
Validation loss: 2.2768563378241753

Epoch: 5| Step: 4
Training loss: 2.8882532119750977
Validation loss: 2.2721415232586604

Epoch: 5| Step: 5
Training loss: 1.9221513271331787
Validation loss: 2.2713418006896973

Epoch: 5| Step: 6
Training loss: 3.087258815765381
Validation loss: 2.274066714830296

Epoch: 5| Step: 7
Training loss: 2.4493343830108643
Validation loss: 2.2727862378602386

Epoch: 5| Step: 8
Training loss: 2.4044787883758545
Validation loss: 2.2867876945003385

Epoch: 5| Step: 9
Training loss: 2.0138461589813232
Validation loss: 2.283751885096232

Epoch: 5| Step: 10
Training loss: 2.9302048683166504
Validation loss: 2.2984500828609673

Epoch: 236| Step: 0
Training loss: 2.4566991329193115
Validation loss: 2.297332522689655

Epoch: 5| Step: 1
Training loss: 1.9828910827636719
Validation loss: 2.2968950117788007

Epoch: 5| Step: 2
Training loss: 2.6964282989501953
Validation loss: 2.282710402242599

Epoch: 5| Step: 3
Training loss: 2.4672694206237793
Validation loss: 2.27606298077491

Epoch: 5| Step: 4
Training loss: 2.2414774894714355
Validation loss: 2.2695693969726562

Epoch: 5| Step: 5
Training loss: 2.6509721279144287
Validation loss: 2.269538276938982

Epoch: 5| Step: 6
Training loss: 2.7500853538513184
Validation loss: 2.2728502904215167

Epoch: 5| Step: 7
Training loss: 2.158658027648926
Validation loss: 2.275794265090778

Epoch: 5| Step: 8
Training loss: 2.444300413131714
Validation loss: 2.2799966078932568

Epoch: 5| Step: 9
Training loss: 2.864539384841919
Validation loss: 2.270642319033223

Epoch: 5| Step: 10
Training loss: 2.651421308517456
Validation loss: 2.281743913568476

Epoch: 237| Step: 0
Training loss: 2.614769697189331
Validation loss: 2.275529681995351

Epoch: 5| Step: 1
Training loss: 2.3950486183166504
Validation loss: 2.2782429854075112

Epoch: 5| Step: 2
Training loss: 2.263705253601074
Validation loss: 2.2776598468903573

Epoch: 5| Step: 3
Training loss: 1.8770726919174194
Validation loss: 2.2888759912983065

Epoch: 5| Step: 4
Training loss: 2.968902111053467
Validation loss: 2.2920054825403358

Epoch: 5| Step: 5
Training loss: 2.702810287475586
Validation loss: 2.2831212756454304

Epoch: 5| Step: 6
Training loss: 2.290039539337158
Validation loss: 2.285927582812566

Epoch: 5| Step: 7
Training loss: 2.5079212188720703
Validation loss: 2.2700253712233676

Epoch: 5| Step: 8
Training loss: 2.667867660522461
Validation loss: 2.260944635637345

Epoch: 5| Step: 9
Training loss: 2.294543504714966
Validation loss: 2.2666262606138825

Epoch: 5| Step: 10
Training loss: 2.745840311050415
Validation loss: 2.263953326850809

Epoch: 238| Step: 0
Training loss: 2.5909035205841064
Validation loss: 2.2632640843750327

Epoch: 5| Step: 1
Training loss: 2.3043739795684814
Validation loss: 2.2654197831307687

Epoch: 5| Step: 2
Training loss: 1.9092590808868408
Validation loss: 2.2537525956348707

Epoch: 5| Step: 3
Training loss: 2.2477736473083496
Validation loss: 2.2523686168014363

Epoch: 5| Step: 4
Training loss: 2.7125141620635986
Validation loss: 2.2485796302877445

Epoch: 5| Step: 5
Training loss: 2.771868944168091
Validation loss: 2.255043762986378

Epoch: 5| Step: 6
Training loss: 2.2812705039978027
Validation loss: 2.255749361489409

Epoch: 5| Step: 7
Training loss: 2.6617040634155273
Validation loss: 2.2747779815427718

Epoch: 5| Step: 8
Training loss: 2.7658767700195312
Validation loss: 2.261564741852463

Epoch: 5| Step: 9
Training loss: 2.572906970977783
Validation loss: 2.271470791550093

Epoch: 5| Step: 10
Training loss: 2.4666919708251953
Validation loss: 2.2603962985418176

Epoch: 239| Step: 0
Training loss: 2.7646021842956543
Validation loss: 2.2679662832649807

Epoch: 5| Step: 1
Training loss: 2.3128244876861572
Validation loss: 2.2665857704736854

Epoch: 5| Step: 2
Training loss: 2.7580931186676025
Validation loss: 2.262315127157396

Epoch: 5| Step: 3
Training loss: 2.285057544708252
Validation loss: 2.264972343239733

Epoch: 5| Step: 4
Training loss: 2.521304130554199
Validation loss: 2.256390351121144

Epoch: 5| Step: 5
Training loss: 2.5721282958984375
Validation loss: 2.261978890306206

Epoch: 5| Step: 6
Training loss: 2.181523323059082
Validation loss: 2.2549758188186155

Epoch: 5| Step: 7
Training loss: 2.371920585632324
Validation loss: 2.2580240439343195

Epoch: 5| Step: 8
Training loss: 2.249328136444092
Validation loss: 2.2633143240405666

Epoch: 5| Step: 9
Training loss: 2.599360466003418
Validation loss: 2.2675625867741083

Epoch: 5| Step: 10
Training loss: 2.5282182693481445
Validation loss: 2.267639348583837

Epoch: 240| Step: 0
Training loss: 2.3868348598480225
Validation loss: 2.2803156324612197

Epoch: 5| Step: 1
Training loss: 2.2087199687957764
Validation loss: 2.2895591848640033

Epoch: 5| Step: 2
Training loss: 2.07157564163208
Validation loss: 2.286241501890203

Epoch: 5| Step: 3
Training loss: 2.2842421531677246
Validation loss: 2.2822589887085782

Epoch: 5| Step: 4
Training loss: 2.4049620628356934
Validation loss: 2.2849616414757183

Epoch: 5| Step: 5
Training loss: 2.7201552391052246
Validation loss: 2.277956695966823

Epoch: 5| Step: 6
Training loss: 2.535956859588623
Validation loss: 2.280311615236344

Epoch: 5| Step: 7
Training loss: 3.0707578659057617
Validation loss: 2.2630881263363745

Epoch: 5| Step: 8
Training loss: 2.7875707149505615
Validation loss: 2.2546584426715808

Epoch: 5| Step: 9
Training loss: 2.442117214202881
Validation loss: 2.2535551171148978

Epoch: 5| Step: 10
Training loss: 2.450855255126953
Validation loss: 2.241540524267381

Epoch: 241| Step: 0
Training loss: 2.54823637008667
Validation loss: 2.2422983108028287

Epoch: 5| Step: 1
Training loss: 2.5053012371063232
Validation loss: 2.239875275601623

Epoch: 5| Step: 2
Training loss: 2.5210864543914795
Validation loss: 2.253020035323276

Epoch: 5| Step: 3
Training loss: 2.700981855392456
Validation loss: 2.254475560239566

Epoch: 5| Step: 4
Training loss: 2.388317823410034
Validation loss: 2.2616650263468423

Epoch: 5| Step: 5
Training loss: 1.8687689304351807
Validation loss: 2.2675134635740712

Epoch: 5| Step: 6
Training loss: 2.36102557182312
Validation loss: 2.2822289159221034

Epoch: 5| Step: 7
Training loss: 2.543853282928467
Validation loss: 2.2783244732887513

Epoch: 5| Step: 8
Training loss: 2.7119052410125732
Validation loss: 2.2702266400860203

Epoch: 5| Step: 9
Training loss: 2.4283623695373535
Validation loss: 2.267435896781183

Epoch: 5| Step: 10
Training loss: 2.8350298404693604
Validation loss: 2.2436477266332155

Epoch: 242| Step: 0
Training loss: 2.624800205230713
Validation loss: 2.2376317029358237

Epoch: 5| Step: 1
Training loss: 2.7987773418426514
Validation loss: 2.2450409012456096

Epoch: 5| Step: 2
Training loss: 2.4554100036621094
Validation loss: 2.248080770174662

Epoch: 5| Step: 3
Training loss: 3.08815336227417
Validation loss: 2.254396635998962

Epoch: 5| Step: 4
Training loss: 2.5490894317626953
Validation loss: 2.2459233050705283

Epoch: 5| Step: 5
Training loss: 2.550227403640747
Validation loss: 2.2573674289129113

Epoch: 5| Step: 6
Training loss: 2.4012789726257324
Validation loss: 2.2642910147225983

Epoch: 5| Step: 7
Training loss: 2.6758573055267334
Validation loss: 2.2784550625790834

Epoch: 5| Step: 8
Training loss: 1.994458794593811
Validation loss: 2.281035351496871

Epoch: 5| Step: 9
Training loss: 2.5054333209991455
Validation loss: 2.293567898452923

Epoch: 5| Step: 10
Training loss: 1.4585700035095215
Validation loss: 2.298499048397105

Epoch: 243| Step: 0
Training loss: 2.865666389465332
Validation loss: 2.28439022392355

Epoch: 5| Step: 1
Training loss: 2.603998899459839
Validation loss: 2.2712184613750828

Epoch: 5| Step: 2
Training loss: 2.0753486156463623
Validation loss: 2.253688648182859

Epoch: 5| Step: 3
Training loss: 2.1638102531433105
Validation loss: 2.2519056130481023

Epoch: 5| Step: 4
Training loss: 2.4026622772216797
Validation loss: 2.255362974700107

Epoch: 5| Step: 5
Training loss: 2.28328537940979
Validation loss: 2.270094028083227

Epoch: 5| Step: 6
Training loss: 2.694423198699951
Validation loss: 2.2630276961993148

Epoch: 5| Step: 7
Training loss: 2.6534342765808105
Validation loss: 2.2614753374489407

Epoch: 5| Step: 8
Training loss: 2.0405848026275635
Validation loss: 2.2559919767482306

Epoch: 5| Step: 9
Training loss: 2.854926824569702
Validation loss: 2.248402500665316

Epoch: 5| Step: 10
Training loss: 2.5824742317199707
Validation loss: 2.2506184577941895

Epoch: 244| Step: 0
Training loss: 2.4148433208465576
Validation loss: 2.250717680941346

Epoch: 5| Step: 1
Training loss: 2.55484938621521
Validation loss: 2.2530195149042274

Epoch: 5| Step: 2
Training loss: 2.0737571716308594
Validation loss: 2.245198849708803

Epoch: 5| Step: 3
Training loss: 2.3214926719665527
Validation loss: 2.25594832563913

Epoch: 5| Step: 4
Training loss: 2.0321953296661377
Validation loss: 2.251127045641663

Epoch: 5| Step: 5
Training loss: 2.3015811443328857
Validation loss: 2.2539720202005036

Epoch: 5| Step: 6
Training loss: 2.488354444503784
Validation loss: 2.2568082501811366

Epoch: 5| Step: 7
Training loss: 2.4037017822265625
Validation loss: 2.2631939918764177

Epoch: 5| Step: 8
Training loss: 3.152658462524414
Validation loss: 2.2621344084380777

Epoch: 5| Step: 9
Training loss: 2.7589809894561768
Validation loss: 2.271131013029365

Epoch: 5| Step: 10
Training loss: 2.730274200439453
Validation loss: 2.2754648321418354

Epoch: 245| Step: 0
Training loss: 2.6748321056365967
Validation loss: 2.283477626821046

Epoch: 5| Step: 1
Training loss: 2.0842528343200684
Validation loss: 2.2835595069393033

Epoch: 5| Step: 2
Training loss: 2.4968502521514893
Validation loss: 2.2632586853478545

Epoch: 5| Step: 3
Training loss: 1.9218038320541382
Validation loss: 2.2460746278044996

Epoch: 5| Step: 4
Training loss: 2.3924174308776855
Validation loss: 2.2395713047314714

Epoch: 5| Step: 5
Training loss: 3.3167223930358887
Validation loss: 2.232728394128943

Epoch: 5| Step: 6
Training loss: 2.3127081394195557
Validation loss: 2.232659019449706

Epoch: 5| Step: 7
Training loss: 2.3401355743408203
Validation loss: 2.252445552938728

Epoch: 5| Step: 8
Training loss: 2.798194169998169
Validation loss: 2.2507811131015902

Epoch: 5| Step: 9
Training loss: 2.1437668800354004
Validation loss: 2.2518873522358556

Epoch: 5| Step: 10
Training loss: 2.913546323776245
Validation loss: 2.2689207318008586

Epoch: 246| Step: 0
Training loss: 2.7079074382781982
Validation loss: 2.2605194942925566

Epoch: 5| Step: 1
Training loss: 2.4395856857299805
Validation loss: 2.2566484097511537

Epoch: 5| Step: 2
Training loss: 2.3521318435668945
Validation loss: 2.257813802329443

Epoch: 5| Step: 3
Training loss: 3.311030626296997
Validation loss: 2.257504347831972

Epoch: 5| Step: 4
Training loss: 1.8480770587921143
Validation loss: 2.2574732713801886

Epoch: 5| Step: 5
Training loss: 2.7374911308288574
Validation loss: 2.255163574731478

Epoch: 5| Step: 6
Training loss: 2.767195224761963
Validation loss: 2.24641768393978

Epoch: 5| Step: 7
Training loss: 2.0143163204193115
Validation loss: 2.2507263998831473

Epoch: 5| Step: 8
Training loss: 2.387300491333008
Validation loss: 2.2440057364843224

Epoch: 5| Step: 9
Training loss: 2.5267255306243896
Validation loss: 2.241884923750354

Epoch: 5| Step: 10
Training loss: 2.112363815307617
Validation loss: 2.245909102501408

Epoch: 247| Step: 0
Training loss: 2.2164969444274902
Validation loss: 2.23582019857181

Epoch: 5| Step: 1
Training loss: 2.6702544689178467
Validation loss: 2.234268493549798

Epoch: 5| Step: 2
Training loss: 2.5414130687713623
Validation loss: 2.2315662471196984

Epoch: 5| Step: 3
Training loss: 2.7675259113311768
Validation loss: 2.2452669579495668

Epoch: 5| Step: 4
Training loss: 3.0222296714782715
Validation loss: 2.251504869871242

Epoch: 5| Step: 5
Training loss: 2.4418108463287354
Validation loss: 2.250172484305597

Epoch: 5| Step: 6
Training loss: 2.6531975269317627
Validation loss: 2.2561982729101695

Epoch: 5| Step: 7
Training loss: 2.118365526199341
Validation loss: 2.266259149838519

Epoch: 5| Step: 8
Training loss: 2.5164549350738525
Validation loss: 2.2636220403896865

Epoch: 5| Step: 9
Training loss: 2.2213339805603027
Validation loss: 2.2669598492242957

Epoch: 5| Step: 10
Training loss: 1.9101043939590454
Validation loss: 2.280969089077365

Epoch: 248| Step: 0
Training loss: 2.436641216278076
Validation loss: 2.2802192472642466

Epoch: 5| Step: 1
Training loss: 2.202449321746826
Validation loss: 2.2601955193345264

Epoch: 5| Step: 2
Training loss: 2.3195533752441406
Validation loss: 2.26424841983344

Epoch: 5| Step: 3
Training loss: 2.6402711868286133
Validation loss: 2.2477297911079983

Epoch: 5| Step: 4
Training loss: 2.4715046882629395
Validation loss: 2.254194672389697

Epoch: 5| Step: 5
Training loss: 2.8759326934814453
Validation loss: 2.247095451560072

Epoch: 5| Step: 6
Training loss: 3.0042285919189453
Validation loss: 2.246630535330824

Epoch: 5| Step: 7
Training loss: 2.4467318058013916
Validation loss: 2.241996234463107

Epoch: 5| Step: 8
Training loss: 2.2571232318878174
Validation loss: 2.2334110224118797

Epoch: 5| Step: 9
Training loss: 2.1818814277648926
Validation loss: 2.2410739621808453

Epoch: 5| Step: 10
Training loss: 2.1581437587738037
Validation loss: 2.241428368835039

Epoch: 249| Step: 0
Training loss: 2.6645233631134033
Validation loss: 2.2474905854912213

Epoch: 5| Step: 1
Training loss: 2.324748992919922
Validation loss: 2.254465667150354

Epoch: 5| Step: 2
Training loss: 2.5918185710906982
Validation loss: 2.2494541265631236

Epoch: 5| Step: 3
Training loss: 2.2928428649902344
Validation loss: 2.264228456763811

Epoch: 5| Step: 4
Training loss: 2.088303565979004
Validation loss: 2.272384862745962

Epoch: 5| Step: 5
Training loss: 3.2423923015594482
Validation loss: 2.2838025093078613

Epoch: 5| Step: 6
Training loss: 2.5334885120391846
Validation loss: 2.2872056217603784

Epoch: 5| Step: 7
Training loss: 2.2924556732177734
Validation loss: 2.279445953266595

Epoch: 5| Step: 8
Training loss: 2.115525245666504
Validation loss: 2.2729315642387635

Epoch: 5| Step: 9
Training loss: 2.7708497047424316
Validation loss: 2.274786562047979

Epoch: 5| Step: 10
Training loss: 2.34112548828125
Validation loss: 2.2595318773741364

Epoch: 250| Step: 0
Training loss: 2.4029860496520996
Validation loss: 2.2588951844041065

Epoch: 5| Step: 1
Training loss: 3.153852939605713
Validation loss: 2.2594710011636057

Epoch: 5| Step: 2
Training loss: 2.7637012004852295
Validation loss: 2.2436497749820834

Epoch: 5| Step: 3
Training loss: 2.45090389251709
Validation loss: 2.235515620118828

Epoch: 5| Step: 4
Training loss: 2.1409573554992676
Validation loss: 2.2228856073912753

Epoch: 5| Step: 5
Training loss: 2.1653895378112793
Validation loss: 2.2261686132800196

Epoch: 5| Step: 6
Training loss: 3.008121967315674
Validation loss: 2.231499118189658

Epoch: 5| Step: 7
Training loss: 2.3661270141601562
Validation loss: 2.244822939236959

Epoch: 5| Step: 8
Training loss: 1.9041755199432373
Validation loss: 2.2542845818304245

Epoch: 5| Step: 9
Training loss: 2.514322280883789
Validation loss: 2.255963017863612

Epoch: 5| Step: 10
Training loss: 2.2776038646698
Validation loss: 2.2671422727646364

Epoch: 251| Step: 0
Training loss: 2.1599960327148438
Validation loss: 2.2824079349476802

Epoch: 5| Step: 1
Training loss: 1.739223837852478
Validation loss: 2.297419099397557

Epoch: 5| Step: 2
Training loss: 1.812007188796997
Validation loss: 2.3068525932168447

Epoch: 5| Step: 3
Training loss: 2.9754223823547363
Validation loss: 2.3139775030074583

Epoch: 5| Step: 4
Training loss: 2.518301486968994
Validation loss: 2.3163248851735103

Epoch: 5| Step: 5
Training loss: 3.108882188796997
Validation loss: 2.3035173095682615

Epoch: 5| Step: 6
Training loss: 2.7639834880828857
Validation loss: 2.2810739599248415

Epoch: 5| Step: 7
Training loss: 2.5326082706451416
Validation loss: 2.2727069213826168

Epoch: 5| Step: 8
Training loss: 2.2742505073547363
Validation loss: 2.263823993744389

Epoch: 5| Step: 9
Training loss: 2.5946669578552246
Validation loss: 2.249411356064581

Epoch: 5| Step: 10
Training loss: 2.7503161430358887
Validation loss: 2.241720204712242

Epoch: 252| Step: 0
Training loss: 2.9438223838806152
Validation loss: 2.228058376619893

Epoch: 5| Step: 1
Training loss: 2.235513687133789
Validation loss: 2.221574471842858

Epoch: 5| Step: 2
Training loss: 2.735957622528076
Validation loss: 2.220027249346497

Epoch: 5| Step: 3
Training loss: 2.398766279220581
Validation loss: 2.226502417236246

Epoch: 5| Step: 4
Training loss: 1.919751524925232
Validation loss: 2.2178700931610598

Epoch: 5| Step: 5
Training loss: 2.096712350845337
Validation loss: 2.214937207519367

Epoch: 5| Step: 6
Training loss: 2.566192388534546
Validation loss: 2.2287485740518056

Epoch: 5| Step: 7
Training loss: 2.8088932037353516
Validation loss: 2.229809404701315

Epoch: 5| Step: 8
Training loss: 2.518221139907837
Validation loss: 2.2364113407750286

Epoch: 5| Step: 9
Training loss: 2.434816360473633
Validation loss: 2.2428543824021534

Epoch: 5| Step: 10
Training loss: 2.6157491207122803
Validation loss: 2.2446007779849473

Epoch: 253| Step: 0
Training loss: 2.125800609588623
Validation loss: 2.2348802179418583

Epoch: 5| Step: 1
Training loss: 2.3327338695526123
Validation loss: 2.250131082791154

Epoch: 5| Step: 2
Training loss: 2.6060125827789307
Validation loss: 2.247512743037234

Epoch: 5| Step: 3
Training loss: 1.6234819889068604
Validation loss: 2.2452934736846597

Epoch: 5| Step: 4
Training loss: 2.2908830642700195
Validation loss: 2.2526278316333728

Epoch: 5| Step: 5
Training loss: 2.6630735397338867
Validation loss: 2.255689213352819

Epoch: 5| Step: 6
Training loss: 3.080976963043213
Validation loss: 2.2515237382663194

Epoch: 5| Step: 7
Training loss: 2.766356945037842
Validation loss: 2.2527199227322816

Epoch: 5| Step: 8
Training loss: 3.1482858657836914
Validation loss: 2.2478720731632684

Epoch: 5| Step: 9
Training loss: 2.028285026550293
Validation loss: 2.249784537540969

Epoch: 5| Step: 10
Training loss: 2.4236645698547363
Validation loss: 2.2477274402495353

Epoch: 254| Step: 0
Training loss: 2.6256260871887207
Validation loss: 2.241888411583439

Epoch: 5| Step: 1
Training loss: 3.2110023498535156
Validation loss: 2.231842351216142

Epoch: 5| Step: 2
Training loss: 3.089902877807617
Validation loss: 2.2169634398593696

Epoch: 5| Step: 3
Training loss: 2.637256145477295
Validation loss: 2.2184222744357203

Epoch: 5| Step: 4
Training loss: 2.188385486602783
Validation loss: 2.2235576619384108

Epoch: 5| Step: 5
Training loss: 2.0429978370666504
Validation loss: 2.215237941793216

Epoch: 5| Step: 6
Training loss: 2.762049674987793
Validation loss: 2.2277285232338855

Epoch: 5| Step: 7
Training loss: 2.0009312629699707
Validation loss: 2.2318639037429646

Epoch: 5| Step: 8
Training loss: 2.4100472927093506
Validation loss: 2.2245561217748993

Epoch: 5| Step: 9
Training loss: 2.1210837364196777
Validation loss: 2.2243340733230754

Epoch: 5| Step: 10
Training loss: 1.8520362377166748
Validation loss: 2.2304433135576147

Epoch: 255| Step: 0
Training loss: 1.9676473140716553
Validation loss: 2.2278050402159333

Epoch: 5| Step: 1
Training loss: 2.332315683364868
Validation loss: 2.2307238117341073

Epoch: 5| Step: 2
Training loss: 2.234056234359741
Validation loss: 2.2366855785410893

Epoch: 5| Step: 3
Training loss: 2.5699524879455566
Validation loss: 2.245353296238889

Epoch: 5| Step: 4
Training loss: 2.3490796089172363
Validation loss: 2.250985530114943

Epoch: 5| Step: 5
Training loss: 2.398834466934204
Validation loss: 2.249950655045048

Epoch: 5| Step: 6
Training loss: 3.1989831924438477
Validation loss: 2.253723846968784

Epoch: 5| Step: 7
Training loss: 2.5695385932922363
Validation loss: 2.2672891129729567

Epoch: 5| Step: 8
Training loss: 2.3270721435546875
Validation loss: 2.2614907269836753

Epoch: 5| Step: 9
Training loss: 2.4944515228271484
Validation loss: 2.253097608525266

Epoch: 5| Step: 10
Training loss: 2.5956075191497803
Validation loss: 2.244626557955178

Epoch: 256| Step: 0
Training loss: 2.8839962482452393
Validation loss: 2.2347627737188853

Epoch: 5| Step: 1
Training loss: 2.4073915481567383
Validation loss: 2.233441447698942

Epoch: 5| Step: 2
Training loss: 1.970106840133667
Validation loss: 2.239004922169511

Epoch: 5| Step: 3
Training loss: 2.6964123249053955
Validation loss: 2.2298340797424316

Epoch: 5| Step: 4
Training loss: 2.2812204360961914
Validation loss: 2.2272461127209406

Epoch: 5| Step: 5
Training loss: 2.2512450218200684
Validation loss: 2.2450966681203535

Epoch: 5| Step: 6
Training loss: 2.530219793319702
Validation loss: 2.2454661271905385

Epoch: 5| Step: 7
Training loss: 2.4620513916015625
Validation loss: 2.256854244457778

Epoch: 5| Step: 8
Training loss: 1.9100440740585327
Validation loss: 2.256645423109813

Epoch: 5| Step: 9
Training loss: 2.754891872406006
Validation loss: 2.261819685659101

Epoch: 5| Step: 10
Training loss: 3.0042808055877686
Validation loss: 2.2467254695071968

Epoch: 257| Step: 0
Training loss: 2.1774563789367676
Validation loss: 2.2394281766747914

Epoch: 5| Step: 1
Training loss: 2.5830845832824707
Validation loss: 2.2312349478403726

Epoch: 5| Step: 2
Training loss: 2.564453363418579
Validation loss: 2.2331406916341474

Epoch: 5| Step: 3
Training loss: 2.5685219764709473
Validation loss: 2.237328710094575

Epoch: 5| Step: 4
Training loss: 2.3245763778686523
Validation loss: 2.2445661662727274

Epoch: 5| Step: 5
Training loss: 1.9352178573608398
Validation loss: 2.2535325096499537

Epoch: 5| Step: 6
Training loss: 3.0336852073669434
Validation loss: 2.2405241535555933

Epoch: 5| Step: 7
Training loss: 2.1788172721862793
Validation loss: 2.2584805693677676

Epoch: 5| Step: 8
Training loss: 3.1408772468566895
Validation loss: 2.2603471920054448

Epoch: 5| Step: 9
Training loss: 2.2290120124816895
Validation loss: 2.25314672018892

Epoch: 5| Step: 10
Training loss: 2.266958475112915
Validation loss: 2.2491946617762246

Epoch: 258| Step: 0
Training loss: 2.013284206390381
Validation loss: 2.248757295711066

Epoch: 5| Step: 1
Training loss: 1.8258787393569946
Validation loss: 2.2559158135485906

Epoch: 5| Step: 2
Training loss: 2.3152339458465576
Validation loss: 2.2495140696084626

Epoch: 5| Step: 3
Training loss: 2.741302490234375
Validation loss: 2.254405847159765

Epoch: 5| Step: 4
Training loss: 2.5791401863098145
Validation loss: 2.2561864622177614

Epoch: 5| Step: 5
Training loss: 3.1147189140319824
Validation loss: 2.264191739020809

Epoch: 5| Step: 6
Training loss: 2.6696951389312744
Validation loss: 2.250742038091024

Epoch: 5| Step: 7
Training loss: 2.0336802005767822
Validation loss: 2.247452179590861

Epoch: 5| Step: 8
Training loss: 2.6129696369171143
Validation loss: 2.2436760420440347

Epoch: 5| Step: 9
Training loss: 2.380488634109497
Validation loss: 2.2437729271509315

Epoch: 5| Step: 10
Training loss: 2.639863967895508
Validation loss: 2.2466232545914187

Epoch: 259| Step: 0
Training loss: 2.3890464305877686
Validation loss: 2.2438774621614845

Epoch: 5| Step: 1
Training loss: 2.6098761558532715
Validation loss: 2.25348170598348

Epoch: 5| Step: 2
Training loss: 2.121410369873047
Validation loss: 2.249743357781441

Epoch: 5| Step: 3
Training loss: 2.5247890949249268
Validation loss: 2.2530037587688816

Epoch: 5| Step: 4
Training loss: 2.283820629119873
Validation loss: 2.245267665514382

Epoch: 5| Step: 5
Training loss: 2.479998826980591
Validation loss: 2.2540088622800765

Epoch: 5| Step: 6
Training loss: 2.9150941371917725
Validation loss: 2.2419402932608

Epoch: 5| Step: 7
Training loss: 2.5690066814422607
Validation loss: 2.240788416195941

Epoch: 5| Step: 8
Training loss: 2.319680690765381
Validation loss: 2.2379833523945143

Epoch: 5| Step: 9
Training loss: 2.7136166095733643
Validation loss: 2.236110661619453

Epoch: 5| Step: 10
Training loss: 1.9478449821472168
Validation loss: 2.24043563360809

Epoch: 260| Step: 0
Training loss: 2.1157591342926025
Validation loss: 2.2337139626984954

Epoch: 5| Step: 1
Training loss: 2.8820912837982178
Validation loss: 2.23048819521422

Epoch: 5| Step: 2
Training loss: 2.7601702213287354
Validation loss: 2.24029952479947

Epoch: 5| Step: 3
Training loss: 2.682637929916382
Validation loss: 2.2284463836300756

Epoch: 5| Step: 4
Training loss: 2.222691535949707
Validation loss: 2.2375324438976985

Epoch: 5| Step: 5
Training loss: 1.3943568468093872
Validation loss: 2.226555971689122

Epoch: 5| Step: 6
Training loss: 2.1475696563720703
Validation loss: 2.23372398396974

Epoch: 5| Step: 7
Training loss: 2.8381571769714355
Validation loss: 2.2400728887127292

Epoch: 5| Step: 8
Training loss: 2.369154930114746
Validation loss: 2.2229631177840696

Epoch: 5| Step: 9
Training loss: 2.5449748039245605
Validation loss: 2.226891774003224

Epoch: 5| Step: 10
Training loss: 3.042181968688965
Validation loss: 2.2293763545251664

Epoch: 261| Step: 0
Training loss: 2.37717866897583
Validation loss: 2.2472685408848587

Epoch: 5| Step: 1
Training loss: 2.290553331375122
Validation loss: 2.2385105997003536

Epoch: 5| Step: 2
Training loss: 2.201608419418335
Validation loss: 2.2364864631365706

Epoch: 5| Step: 3
Training loss: 2.7685928344726562
Validation loss: 2.2485722111117457

Epoch: 5| Step: 4
Training loss: 2.0131688117980957
Validation loss: 2.2510623726793515

Epoch: 5| Step: 5
Training loss: 2.040924549102783
Validation loss: 2.245567866550979

Epoch: 5| Step: 6
Training loss: 2.4942307472229004
Validation loss: 2.2474055392767793

Epoch: 5| Step: 7
Training loss: 2.714475154876709
Validation loss: 2.2398290864882933

Epoch: 5| Step: 8
Training loss: 2.785348892211914
Validation loss: 2.234802184566375

Epoch: 5| Step: 9
Training loss: 3.08478045463562
Validation loss: 2.2383730155165478

Epoch: 5| Step: 10
Training loss: 2.213778257369995
Validation loss: 2.237228101299655

Epoch: 262| Step: 0
Training loss: 2.312861442565918
Validation loss: 2.2212437429735736

Epoch: 5| Step: 1
Training loss: 2.7920780181884766
Validation loss: 2.2227749619432675

Epoch: 5| Step: 2
Training loss: 2.1579957008361816
Validation loss: 2.227310795937815

Epoch: 5| Step: 3
Training loss: 2.2215123176574707
Validation loss: 2.225138828318606

Epoch: 5| Step: 4
Training loss: 2.8157403469085693
Validation loss: 2.216603176568144

Epoch: 5| Step: 5
Training loss: 2.461360454559326
Validation loss: 2.213877862499606

Epoch: 5| Step: 6
Training loss: 2.40142822265625
Validation loss: 2.214735024718828

Epoch: 5| Step: 7
Training loss: 2.9823126792907715
Validation loss: 2.2282853587981193

Epoch: 5| Step: 8
Training loss: 2.746403217315674
Validation loss: 2.232797425280335

Epoch: 5| Step: 9
Training loss: 1.7761716842651367
Validation loss: 2.236452523098197

Epoch: 5| Step: 10
Training loss: 2.2007508277893066
Validation loss: 2.244667222422938

Epoch: 263| Step: 0
Training loss: 2.416746139526367
Validation loss: 2.2403022781495125

Epoch: 5| Step: 1
Training loss: 1.972402811050415
Validation loss: 2.2394798571063625

Epoch: 5| Step: 2
Training loss: 2.3426976203918457
Validation loss: 2.244648295064126

Epoch: 5| Step: 3
Training loss: 2.546708106994629
Validation loss: 2.2654037501222346

Epoch: 5| Step: 4
Training loss: 2.621868848800659
Validation loss: 2.2552658921928814

Epoch: 5| Step: 5
Training loss: 1.870483636856079
Validation loss: 2.2476062133748043

Epoch: 5| Step: 6
Training loss: 2.2546184062957764
Validation loss: 2.239452592788204

Epoch: 5| Step: 7
Training loss: 2.804288625717163
Validation loss: 2.222205695285592

Epoch: 5| Step: 8
Training loss: 2.3968191146850586
Validation loss: 2.231807577994562

Epoch: 5| Step: 9
Training loss: 2.782374858856201
Validation loss: 2.2170329324660765

Epoch: 5| Step: 10
Training loss: 2.9875423908233643
Validation loss: 2.227156610899074

Epoch: 264| Step: 0
Training loss: 2.1411423683166504
Validation loss: 2.228698390786366

Epoch: 5| Step: 1
Training loss: 2.653395175933838
Validation loss: 2.2477718886508735

Epoch: 5| Step: 2
Training loss: 2.790229082107544
Validation loss: 2.268417831390135

Epoch: 5| Step: 3
Training loss: 1.8956222534179688
Validation loss: 2.2479369717259563

Epoch: 5| Step: 4
Training loss: 3.2300305366516113
Validation loss: 2.2411196308751262

Epoch: 5| Step: 5
Training loss: 2.4394047260284424
Validation loss: 2.2283693398198774

Epoch: 5| Step: 6
Training loss: 2.4837069511413574
Validation loss: 2.225981127831244

Epoch: 5| Step: 7
Training loss: 1.8083664178848267
Validation loss: 2.2190097506328295

Epoch: 5| Step: 8
Training loss: 2.4156432151794434
Validation loss: 2.215895845044044

Epoch: 5| Step: 9
Training loss: 2.466733455657959
Validation loss: 2.223651014348512

Epoch: 5| Step: 10
Training loss: 2.5263521671295166
Validation loss: 2.217572817238428

Epoch: 265| Step: 0
Training loss: 2.308089017868042
Validation loss: 2.2228231404417302

Epoch: 5| Step: 1
Training loss: 1.867266058921814
Validation loss: 2.230384321622951

Epoch: 5| Step: 2
Training loss: 1.859449028968811
Validation loss: 2.2243149870185444

Epoch: 5| Step: 3
Training loss: 2.694906711578369
Validation loss: 2.2459092499107443

Epoch: 5| Step: 4
Training loss: 3.1377482414245605
Validation loss: 2.2358435738471245

Epoch: 5| Step: 5
Training loss: 2.4480552673339844
Validation loss: 2.2416706187750703

Epoch: 5| Step: 6
Training loss: 2.2879714965820312
Validation loss: 2.228070310367051

Epoch: 5| Step: 7
Training loss: 2.7646350860595703
Validation loss: 2.2289843956629434

Epoch: 5| Step: 8
Training loss: 2.3034191131591797
Validation loss: 2.224287686809417

Epoch: 5| Step: 9
Training loss: 2.847064256668091
Validation loss: 2.2180602217233307

Epoch: 5| Step: 10
Training loss: 2.3093929290771484
Validation loss: 2.2189119451789447

Epoch: 266| Step: 0
Training loss: 2.545382022857666
Validation loss: 2.2314461661923315

Epoch: 5| Step: 1
Training loss: 2.567939281463623
Validation loss: 2.235333031223666

Epoch: 5| Step: 2
Training loss: 2.1624279022216797
Validation loss: 2.2454501736548638

Epoch: 5| Step: 3
Training loss: 2.200528144836426
Validation loss: 2.24190919117261

Epoch: 5| Step: 4
Training loss: 2.6435298919677734
Validation loss: 2.261450618825933

Epoch: 5| Step: 5
Training loss: 2.2642102241516113
Validation loss: 2.253160454893625

Epoch: 5| Step: 6
Training loss: 2.243058919906616
Validation loss: 2.255802474996095

Epoch: 5| Step: 7
Training loss: 3.0185625553131104
Validation loss: 2.265790359948271

Epoch: 5| Step: 8
Training loss: 2.608124256134033
Validation loss: 2.2460690929043676

Epoch: 5| Step: 9
Training loss: 2.1019506454467773
Validation loss: 2.2532125134621896

Epoch: 5| Step: 10
Training loss: 2.359299659729004
Validation loss: 2.2352380060380503

Epoch: 267| Step: 0
Training loss: 2.1780307292938232
Validation loss: 2.228139829891984

Epoch: 5| Step: 1
Training loss: 2.0919957160949707
Validation loss: 2.2154225892918085

Epoch: 5| Step: 2
Training loss: 2.511444091796875
Validation loss: 2.221032877122202

Epoch: 5| Step: 3
Training loss: 2.6539127826690674
Validation loss: 2.227434615935049

Epoch: 5| Step: 4
Training loss: 2.5574429035186768
Validation loss: 2.2241007307524323

Epoch: 5| Step: 5
Training loss: 1.829154372215271
Validation loss: 2.2270862466545513

Epoch: 5| Step: 6
Training loss: 2.875664710998535
Validation loss: 2.2219430233842585

Epoch: 5| Step: 7
Training loss: 2.7476820945739746
Validation loss: 2.2254918159977084

Epoch: 5| Step: 8
Training loss: 1.9720211029052734
Validation loss: 2.2149135604981454

Epoch: 5| Step: 9
Training loss: 2.4632372856140137
Validation loss: 2.218496532850368

Epoch: 5| Step: 10
Training loss: 2.969709634780884
Validation loss: 2.207966891668176

Epoch: 268| Step: 0
Training loss: 2.5867321491241455
Validation loss: 2.2140558611962105

Epoch: 5| Step: 1
Training loss: 2.498995304107666
Validation loss: 2.2107090668011735

Epoch: 5| Step: 2
Training loss: 2.1902565956115723
Validation loss: 2.2102577942673878

Epoch: 5| Step: 3
Training loss: 3.362611770629883
Validation loss: 2.2232316847770446

Epoch: 5| Step: 4
Training loss: 2.0824637413024902
Validation loss: 2.2453757229671685

Epoch: 5| Step: 5
Training loss: 2.301347017288208
Validation loss: 2.244731474948186

Epoch: 5| Step: 6
Training loss: 2.197972059249878
Validation loss: 2.2522462132156535

Epoch: 5| Step: 7
Training loss: 2.4629600048065186
Validation loss: 2.2384758944152505

Epoch: 5| Step: 8
Training loss: 2.1020965576171875
Validation loss: 2.2593928255060667

Epoch: 5| Step: 9
Training loss: 2.5076591968536377
Validation loss: 2.2591648409443517

Epoch: 5| Step: 10
Training loss: 2.412907361984253
Validation loss: 2.269089057881345

Epoch: 269| Step: 0
Training loss: 2.4882285594940186
Validation loss: 2.2506781137117775

Epoch: 5| Step: 1
Training loss: 2.5240960121154785
Validation loss: 2.251587344754127

Epoch: 5| Step: 2
Training loss: 2.925166368484497
Validation loss: 2.2362283301609818

Epoch: 5| Step: 3
Training loss: 2.0963664054870605
Validation loss: 2.242564534628263

Epoch: 5| Step: 4
Training loss: 2.3812458515167236
Validation loss: 2.238417799754809

Epoch: 5| Step: 5
Training loss: 2.4164013862609863
Validation loss: 2.22689990843496

Epoch: 5| Step: 6
Training loss: 2.438767910003662
Validation loss: 2.220707913880707

Epoch: 5| Step: 7
Training loss: 2.2377939224243164
Validation loss: 2.2118170440837903

Epoch: 5| Step: 8
Training loss: 2.362157106399536
Validation loss: 2.208788464146276

Epoch: 5| Step: 9
Training loss: 2.586421251296997
Validation loss: 2.2172937572643323

Epoch: 5| Step: 10
Training loss: 2.3743762969970703
Validation loss: 2.217183527126107

Epoch: 270| Step: 0
Training loss: 2.5647735595703125
Validation loss: 2.2248559895382134

Epoch: 5| Step: 1
Training loss: 1.875880479812622
Validation loss: 2.222249468167623

Epoch: 5| Step: 2
Training loss: 2.4344255924224854
Validation loss: 2.2153698090584046

Epoch: 5| Step: 3
Training loss: 2.534083843231201
Validation loss: 2.209186415518484

Epoch: 5| Step: 4
Training loss: 2.486694574356079
Validation loss: 2.2004465685095838

Epoch: 5| Step: 5
Training loss: 2.5345282554626465
Validation loss: 2.2002304471949095

Epoch: 5| Step: 6
Training loss: 2.985750198364258
Validation loss: 2.19945599186805

Epoch: 5| Step: 7
Training loss: 2.541973114013672
Validation loss: 2.189947271859774

Epoch: 5| Step: 8
Training loss: 2.517524003982544
Validation loss: 2.1818337530218144

Epoch: 5| Step: 9
Training loss: 1.9788519144058228
Validation loss: 2.1869120187656854

Epoch: 5| Step: 10
Training loss: 2.704702615737915
Validation loss: 2.185638604625579

Epoch: 271| Step: 0
Training loss: 2.3605217933654785
Validation loss: 2.1804025762824604

Epoch: 5| Step: 1
Training loss: 2.4864282608032227
Validation loss: 2.177835782368978

Epoch: 5| Step: 2
Training loss: 2.3900654315948486
Validation loss: 2.173967771632697

Epoch: 5| Step: 3
Training loss: 2.5602176189422607
Validation loss: 2.179227711052023

Epoch: 5| Step: 4
Training loss: 2.2600159645080566
Validation loss: 2.179827877270278

Epoch: 5| Step: 5
Training loss: 2.6551759243011475
Validation loss: 2.179930340859198

Epoch: 5| Step: 6
Training loss: 2.460618495941162
Validation loss: 2.199038223553729

Epoch: 5| Step: 7
Training loss: 2.581643581390381
Validation loss: 2.1976383527119956

Epoch: 5| Step: 8
Training loss: 2.6467061042785645
Validation loss: 2.2046604707676876

Epoch: 5| Step: 9
Training loss: 2.1860320568084717
Validation loss: 2.225414337650422

Epoch: 5| Step: 10
Training loss: 2.292262554168701
Validation loss: 2.2188882494485505

Epoch: 272| Step: 0
Training loss: 1.992611289024353
Validation loss: 2.220309594626068

Epoch: 5| Step: 1
Training loss: 2.2578842639923096
Validation loss: 2.222675315795406

Epoch: 5| Step: 2
Training loss: 2.495529890060425
Validation loss: 2.2297430551180275

Epoch: 5| Step: 3
Training loss: 2.555671215057373
Validation loss: 2.2083998392986994

Epoch: 5| Step: 4
Training loss: 2.5908825397491455
Validation loss: 2.211562515586935

Epoch: 5| Step: 5
Training loss: 2.9226372241973877
Validation loss: 2.2003449522038943

Epoch: 5| Step: 6
Training loss: 1.9138147830963135
Validation loss: 2.2069312475060903

Epoch: 5| Step: 7
Training loss: 2.6871654987335205
Validation loss: 2.2038013217269734

Epoch: 5| Step: 8
Training loss: 2.3605575561523438
Validation loss: 2.2031285993514524

Epoch: 5| Step: 9
Training loss: 2.3114891052246094
Validation loss: 2.2095059028235813

Epoch: 5| Step: 10
Training loss: 3.0086495876312256
Validation loss: 2.204348397511308

Epoch: 273| Step: 0
Training loss: 2.2215518951416016
Validation loss: 2.207210574098813

Epoch: 5| Step: 1
Training loss: 2.105386257171631
Validation loss: 2.216642915561635

Epoch: 5| Step: 2
Training loss: 2.665792942047119
Validation loss: 2.231685605100406

Epoch: 5| Step: 3
Training loss: 2.386768341064453
Validation loss: 2.2315520599324215

Epoch: 5| Step: 4
Training loss: 2.3658804893493652
Validation loss: 2.239751926032446

Epoch: 5| Step: 5
Training loss: 3.1373372077941895
Validation loss: 2.265967022988104

Epoch: 5| Step: 6
Training loss: 3.041553020477295
Validation loss: 2.2915217581615654

Epoch: 5| Step: 7
Training loss: 2.541787624359131
Validation loss: 2.285964206982684

Epoch: 5| Step: 8
Training loss: 1.8528419733047485
Validation loss: 2.2710677398148404

Epoch: 5| Step: 9
Training loss: 2.2268173694610596
Validation loss: 2.2866518907649542

Epoch: 5| Step: 10
Training loss: 2.2929630279541016
Validation loss: 2.2902652986588015

Epoch: 274| Step: 0
Training loss: 2.5449941158294678
Validation loss: 2.2666010369536695

Epoch: 5| Step: 1
Training loss: 2.5792200565338135
Validation loss: 2.2555194157426075

Epoch: 5| Step: 2
Training loss: 2.45810866355896
Validation loss: 2.2525466949709

Epoch: 5| Step: 3
Training loss: 2.2354259490966797
Validation loss: 2.261945234831943

Epoch: 5| Step: 4
Training loss: 2.29988169670105
Validation loss: 2.2343541986198834

Epoch: 5| Step: 5
Training loss: 2.2728664875030518
Validation loss: 2.228336400883172

Epoch: 5| Step: 6
Training loss: 2.4765305519104004
Validation loss: 2.2273938117488736

Epoch: 5| Step: 7
Training loss: 2.3626840114593506
Validation loss: 2.215944490125102

Epoch: 5| Step: 8
Training loss: 2.7740976810455322
Validation loss: 2.208940677745368

Epoch: 5| Step: 9
Training loss: 2.488605260848999
Validation loss: 2.20907845035676

Epoch: 5| Step: 10
Training loss: 2.3880746364593506
Validation loss: 2.193104595266363

Epoch: 275| Step: 0
Training loss: 3.0636703968048096
Validation loss: 2.184925490169115

Epoch: 5| Step: 1
Training loss: 2.572779893875122
Validation loss: 2.1903540062647995

Epoch: 5| Step: 2
Training loss: 2.377748966217041
Validation loss: 2.202726161608132

Epoch: 5| Step: 3
Training loss: 2.0548713207244873
Validation loss: 2.210511253726098

Epoch: 5| Step: 4
Training loss: 1.925986886024475
Validation loss: 2.2165274671328965

Epoch: 5| Step: 5
Training loss: 1.9825502634048462
Validation loss: 2.233037423062068

Epoch: 5| Step: 6
Training loss: 2.6878693103790283
Validation loss: 2.245367357807775

Epoch: 5| Step: 7
Training loss: 2.6703314781188965
Validation loss: 2.245826457136421

Epoch: 5| Step: 8
Training loss: 2.008669376373291
Validation loss: 2.252781783380816

Epoch: 5| Step: 9
Training loss: 2.8702566623687744
Validation loss: 2.248500850892836

Epoch: 5| Step: 10
Training loss: 2.535799980163574
Validation loss: 2.2473887781943045

Epoch: 276| Step: 0
Training loss: 2.517591953277588
Validation loss: 2.2391871739459295

Epoch: 5| Step: 1
Training loss: 2.4674689769744873
Validation loss: 2.2184432757798063

Epoch: 5| Step: 2
Training loss: 2.799955368041992
Validation loss: 2.2001579025740265

Epoch: 5| Step: 3
Training loss: 1.5609347820281982
Validation loss: 2.1984162638264317

Epoch: 5| Step: 4
Training loss: 3.078878402709961
Validation loss: 2.2080906693653395

Epoch: 5| Step: 5
Training loss: 2.014730930328369
Validation loss: 2.197204837235071

Epoch: 5| Step: 6
Training loss: 2.1067986488342285
Validation loss: 2.2075336722917456

Epoch: 5| Step: 7
Training loss: 2.6572346687316895
Validation loss: 2.2053515424010572

Epoch: 5| Step: 8
Training loss: 2.1078784465789795
Validation loss: 2.1961212517112814

Epoch: 5| Step: 9
Training loss: 2.4454503059387207
Validation loss: 2.2089487955134404

Epoch: 5| Step: 10
Training loss: 3.121030807495117
Validation loss: 2.2188072691681566

Epoch: 277| Step: 0
Training loss: 2.1982688903808594
Validation loss: 2.2231486612750637

Epoch: 5| Step: 1
Training loss: 2.5765597820281982
Validation loss: 2.2200963971435383

Epoch: 5| Step: 2
Training loss: 2.02421236038208
Validation loss: 2.2382782249040503

Epoch: 5| Step: 3
Training loss: 3.1488699913024902
Validation loss: 2.247866428026589

Epoch: 5| Step: 4
Training loss: 2.505760669708252
Validation loss: 2.2578658596161874

Epoch: 5| Step: 5
Training loss: 2.27919340133667
Validation loss: 2.265596930698682

Epoch: 5| Step: 6
Training loss: 1.9275109767913818
Validation loss: 2.2584467062386135

Epoch: 5| Step: 7
Training loss: 1.8406215906143188
Validation loss: 2.248258429188882

Epoch: 5| Step: 8
Training loss: 2.3904175758361816
Validation loss: 2.246125882671725

Epoch: 5| Step: 9
Training loss: 2.88305401802063
Validation loss: 2.2332878830612346

Epoch: 5| Step: 10
Training loss: 2.8774383068084717
Validation loss: 2.232142568916403

Epoch: 278| Step: 0
Training loss: 2.8750176429748535
Validation loss: 2.2225311443369877

Epoch: 5| Step: 1
Training loss: 2.1660139560699463
Validation loss: 2.21167419802758

Epoch: 5| Step: 2
Training loss: 2.737227439880371
Validation loss: 2.2051478406434417

Epoch: 5| Step: 3
Training loss: 2.8239874839782715
Validation loss: 2.208550019930768

Epoch: 5| Step: 4
Training loss: 2.7815418243408203
Validation loss: 2.203773724135532

Epoch: 5| Step: 5
Training loss: 2.555394411087036
Validation loss: 2.2119988267139723

Epoch: 5| Step: 6
Training loss: 1.92181396484375
Validation loss: 2.2044042848771617

Epoch: 5| Step: 7
Training loss: 2.47113037109375
Validation loss: 2.1939954219325895

Epoch: 5| Step: 8
Training loss: 1.8665634393692017
Validation loss: 2.1938607077444754

Epoch: 5| Step: 9
Training loss: 2.5101637840270996
Validation loss: 2.2054447589382047

Epoch: 5| Step: 10
Training loss: 1.8047525882720947
Validation loss: 2.208441775332215

Epoch: 279| Step: 0
Training loss: 2.145381450653076
Validation loss: 2.2349122672952633

Epoch: 5| Step: 1
Training loss: 1.7850364446640015
Validation loss: 2.2429982757055633

Epoch: 5| Step: 2
Training loss: 2.7063891887664795
Validation loss: 2.2722412540066625

Epoch: 5| Step: 3
Training loss: 2.9129977226257324
Validation loss: 2.299289100913591

Epoch: 5| Step: 4
Training loss: 2.4930033683776855
Validation loss: 2.318076995111281

Epoch: 5| Step: 5
Training loss: 3.1411123275756836
Validation loss: 2.3493287204414286

Epoch: 5| Step: 6
Training loss: 2.181079387664795
Validation loss: 2.337703909925235

Epoch: 5| Step: 7
Training loss: 1.9279162883758545
Validation loss: 2.3338910084898754

Epoch: 5| Step: 8
Training loss: 3.12282133102417
Validation loss: 2.329800154573174

Epoch: 5| Step: 9
Training loss: 2.3683948516845703
Validation loss: 2.306864607718683

Epoch: 5| Step: 10
Training loss: 2.1775498390197754
Validation loss: 2.267056941986084

Epoch: 280| Step: 0
Training loss: 2.3832547664642334
Validation loss: 2.233902908140613

Epoch: 5| Step: 1
Training loss: 2.132584810256958
Validation loss: 2.2068950822276454

Epoch: 5| Step: 2
Training loss: 2.094815731048584
Validation loss: 2.1976967165547032

Epoch: 5| Step: 3
Training loss: 1.9714992046356201
Validation loss: 2.1908145143139746

Epoch: 5| Step: 4
Training loss: 2.565936326980591
Validation loss: 2.194589925068681

Epoch: 5| Step: 5
Training loss: 3.1350443363189697
Validation loss: 2.196306849038729

Epoch: 5| Step: 6
Training loss: 2.592520236968994
Validation loss: 2.2006768513751287

Epoch: 5| Step: 7
Training loss: 2.008603096008301
Validation loss: 2.2100605169932046

Epoch: 5| Step: 8
Training loss: 2.100335121154785
Validation loss: 2.2042680094319005

Epoch: 5| Step: 9
Training loss: 2.394120454788208
Validation loss: 2.2102539052245436

Epoch: 5| Step: 10
Training loss: 3.3472397327423096
Validation loss: 2.1988002766845045

Epoch: 281| Step: 0
Training loss: 2.847341537475586
Validation loss: 2.201365009430916

Epoch: 5| Step: 1
Training loss: 3.092939853668213
Validation loss: 2.2026431483607136

Epoch: 5| Step: 2
Training loss: 2.484182834625244
Validation loss: 2.1996166629175984

Epoch: 5| Step: 3
Training loss: 2.312673807144165
Validation loss: 2.204579953224428

Epoch: 5| Step: 4
Training loss: 1.8766591548919678
Validation loss: 2.1918234568770214

Epoch: 5| Step: 5
Training loss: 2.9663448333740234
Validation loss: 2.204700231552124

Epoch: 5| Step: 6
Training loss: 2.077453136444092
Validation loss: 2.2017534522600073

Epoch: 5| Step: 7
Training loss: 1.9885660409927368
Validation loss: 2.207274483096215

Epoch: 5| Step: 8
Training loss: 2.0870602130889893
Validation loss: 2.231419578675301

Epoch: 5| Step: 9
Training loss: 2.495948314666748
Validation loss: 2.2352590522458478

Epoch: 5| Step: 10
Training loss: 2.2809932231903076
Validation loss: 2.259498183445264

Epoch: 282| Step: 0
Training loss: 2.5976569652557373
Validation loss: 2.259582970732002

Epoch: 5| Step: 1
Training loss: 2.461292028427124
Validation loss: 2.2707801557356313

Epoch: 5| Step: 2
Training loss: 2.737823247909546
Validation loss: 2.260571613106676

Epoch: 5| Step: 3
Training loss: 2.682425022125244
Validation loss: 2.2375909769406883

Epoch: 5| Step: 4
Training loss: 2.300572156906128
Validation loss: 2.2249470141626175

Epoch: 5| Step: 5
Training loss: 1.467651128768921
Validation loss: 2.207996778590705

Epoch: 5| Step: 6
Training loss: 2.3541290760040283
Validation loss: 2.2052508195241294

Epoch: 5| Step: 7
Training loss: 2.661210298538208
Validation loss: 2.2140175937324442

Epoch: 5| Step: 8
Training loss: 2.2630600929260254
Validation loss: 2.2102743118040022

Epoch: 5| Step: 9
Training loss: 2.621025562286377
Validation loss: 2.2088968497450634

Epoch: 5| Step: 10
Training loss: 2.4994313716888428
Validation loss: 2.209984461466471

Epoch: 283| Step: 0
Training loss: 2.2040371894836426
Validation loss: 2.2146068157688266

Epoch: 5| Step: 1
Training loss: 2.553650140762329
Validation loss: 2.205095244992164

Epoch: 5| Step: 2
Training loss: 2.3234429359436035
Validation loss: 2.214472847600137

Epoch: 5| Step: 3
Training loss: 2.0680458545684814
Validation loss: 2.2378810708240797

Epoch: 5| Step: 4
Training loss: 2.447935104370117
Validation loss: 2.2444314943846835

Epoch: 5| Step: 5
Training loss: 2.6342074871063232
Validation loss: 2.253982359363187

Epoch: 5| Step: 6
Training loss: 2.8925769329071045
Validation loss: 2.2760008483804683

Epoch: 5| Step: 7
Training loss: 2.212721586227417
Validation loss: 2.2813450700493267

Epoch: 5| Step: 8
Training loss: 2.4621965885162354
Validation loss: 2.2777382455846316

Epoch: 5| Step: 9
Training loss: 2.3822615146636963
Validation loss: 2.2486041053648917

Epoch: 5| Step: 10
Training loss: 2.396512985229492
Validation loss: 2.2235460217281053

Epoch: 284| Step: 0
Training loss: 2.1639187335968018
Validation loss: 2.191351518836073

Epoch: 5| Step: 1
Training loss: 2.467454433441162
Validation loss: 2.2009429893186017

Epoch: 5| Step: 2
Training loss: 1.6102632284164429
Validation loss: 2.198749931909705

Epoch: 5| Step: 3
Training loss: 2.0941338539123535
Validation loss: 2.20856261509721

Epoch: 5| Step: 4
Training loss: 2.5839133262634277
Validation loss: 2.2101564138166365

Epoch: 5| Step: 5
Training loss: 2.660761594772339
Validation loss: 2.2226787946557485

Epoch: 5| Step: 6
Training loss: 3.02506685256958
Validation loss: 2.2119549166771675

Epoch: 5| Step: 7
Training loss: 2.2711215019226074
Validation loss: 2.207890458004449

Epoch: 5| Step: 8
Training loss: 2.676276683807373
Validation loss: 2.2100772806393203

Epoch: 5| Step: 9
Training loss: 2.5294971466064453
Validation loss: 2.212263978937621

Epoch: 5| Step: 10
Training loss: 2.4907000064849854
Validation loss: 2.2252788594973985

Epoch: 285| Step: 0
Training loss: 1.9112880229949951
Validation loss: 2.213135785953973

Epoch: 5| Step: 1
Training loss: 2.1793694496154785
Validation loss: 2.2081448788283975

Epoch: 5| Step: 2
Training loss: 2.0921807289123535
Validation loss: 2.2199309769497124

Epoch: 5| Step: 3
Training loss: 3.046586275100708
Validation loss: 2.22005828221639

Epoch: 5| Step: 4
Training loss: 2.677490711212158
Validation loss: 2.2193659351718042

Epoch: 5| Step: 5
Training loss: 3.0785632133483887
Validation loss: 2.203124564181092

Epoch: 5| Step: 6
Training loss: 2.3743293285369873
Validation loss: 2.2009988933481197

Epoch: 5| Step: 7
Training loss: 1.768713355064392
Validation loss: 2.2088238295688423

Epoch: 5| Step: 8
Training loss: 2.867720127105713
Validation loss: 2.203944972766343

Epoch: 5| Step: 9
Training loss: 2.641951560974121
Validation loss: 2.198862797470503

Epoch: 5| Step: 10
Training loss: 1.741783618927002
Validation loss: 2.192065743989842

Epoch: 286| Step: 0
Training loss: 2.6081013679504395
Validation loss: 2.189757549634544

Epoch: 5| Step: 1
Training loss: 2.3146674633026123
Validation loss: 2.1830220914656118

Epoch: 5| Step: 2
Training loss: 2.709804058074951
Validation loss: 2.193121667831175

Epoch: 5| Step: 3
Training loss: 2.429091215133667
Validation loss: 2.2101684462639595

Epoch: 5| Step: 4
Training loss: 2.637538194656372
Validation loss: 2.2259972787672475

Epoch: 5| Step: 5
Training loss: 2.2465968132019043
Validation loss: 2.2451534207149217

Epoch: 5| Step: 6
Training loss: 2.1770517826080322
Validation loss: 2.2486128973704513

Epoch: 5| Step: 7
Training loss: 2.8096837997436523
Validation loss: 2.2610320404011715

Epoch: 5| Step: 8
Training loss: 2.6537063121795654
Validation loss: 2.2648740635123303

Epoch: 5| Step: 9
Training loss: 2.133056640625
Validation loss: 2.2536307419500043

Epoch: 5| Step: 10
Training loss: 1.8144203424453735
Validation loss: 2.246465108727896

Epoch: 287| Step: 0
Training loss: 2.568018674850464
Validation loss: 2.214690878827085

Epoch: 5| Step: 1
Training loss: 2.4462177753448486
Validation loss: 2.1816788129909064

Epoch: 5| Step: 2
Training loss: 2.8421175479888916
Validation loss: 2.170301677078329

Epoch: 5| Step: 3
Training loss: 2.013913631439209
Validation loss: 2.184139015854046

Epoch: 5| Step: 4
Training loss: 2.700833797454834
Validation loss: 2.198954261759276

Epoch: 5| Step: 5
Training loss: 1.9136511087417603
Validation loss: 2.2170568922514557

Epoch: 5| Step: 6
Training loss: 2.2413153648376465
Validation loss: 2.217636796735948

Epoch: 5| Step: 7
Training loss: 2.7241621017456055
Validation loss: 2.2325688792813208

Epoch: 5| Step: 8
Training loss: 2.547769069671631
Validation loss: 2.241513590658865

Epoch: 5| Step: 9
Training loss: 2.8162190914154053
Validation loss: 2.2658999632763606

Epoch: 5| Step: 10
Training loss: 2.268130302429199
Validation loss: 2.2656727503704768

Epoch: 288| Step: 0
Training loss: 2.6351542472839355
Validation loss: 2.275189530464911

Epoch: 5| Step: 1
Training loss: 2.9785525798797607
Validation loss: 2.276811097257881

Epoch: 5| Step: 2
Training loss: 2.3670215606689453
Validation loss: 2.2794490219444357

Epoch: 5| Step: 3
Training loss: 2.200146198272705
Validation loss: 2.258716403797109

Epoch: 5| Step: 4
Training loss: 2.4745445251464844
Validation loss: 2.2469543385249313

Epoch: 5| Step: 5
Training loss: 2.2676913738250732
Validation loss: 2.237296809432327

Epoch: 5| Step: 6
Training loss: 3.181020498275757
Validation loss: 2.22681926655513

Epoch: 5| Step: 7
Training loss: 2.465589761734009
Validation loss: 2.225912929863058

Epoch: 5| Step: 8
Training loss: 2.3746256828308105
Validation loss: 2.2377359200549383

Epoch: 5| Step: 9
Training loss: 1.7915184497833252
Validation loss: 2.2359432584495953

Epoch: 5| Step: 10
Training loss: 2.1097309589385986
Validation loss: 2.2367405558145173

Epoch: 289| Step: 0
Training loss: 2.4424757957458496
Validation loss: 2.234178481563445

Epoch: 5| Step: 1
Training loss: 2.206130027770996
Validation loss: 2.2415226249284643

Epoch: 5| Step: 2
Training loss: 2.0679383277893066
Validation loss: 2.2602148825122463

Epoch: 5| Step: 3
Training loss: 2.3780770301818848
Validation loss: 2.248487731461884

Epoch: 5| Step: 4
Training loss: 2.734898090362549
Validation loss: 2.238408947503695

Epoch: 5| Step: 5
Training loss: 3.1092166900634766
Validation loss: 2.2289235258615143

Epoch: 5| Step: 6
Training loss: 2.037060022354126
Validation loss: 2.2387840555560206

Epoch: 5| Step: 7
Training loss: 3.2636215686798096
Validation loss: 2.2315398134211057

Epoch: 5| Step: 8
Training loss: 2.2462096214294434
Validation loss: 2.2362239283900105

Epoch: 5| Step: 9
Training loss: 2.4371066093444824
Validation loss: 2.2469236850738525

Epoch: 5| Step: 10
Training loss: 1.4885610342025757
Validation loss: 2.2329098357949206

Epoch: 290| Step: 0
Training loss: 2.2613189220428467
Validation loss: 2.2284168094717045

Epoch: 5| Step: 1
Training loss: 2.139467716217041
Validation loss: 2.2392472579915035

Epoch: 5| Step: 2
Training loss: 2.665846347808838
Validation loss: 2.2178915392967964

Epoch: 5| Step: 3
Training loss: 2.1866233348846436
Validation loss: 2.214296711388455

Epoch: 5| Step: 4
Training loss: 2.503053903579712
Validation loss: 2.2040096970014673

Epoch: 5| Step: 5
Training loss: 2.2236247062683105
Validation loss: 2.2107130481350805

Epoch: 5| Step: 6
Training loss: 2.5853452682495117
Validation loss: 2.203561273954248

Epoch: 5| Step: 7
Training loss: 2.248284101486206
Validation loss: 2.2009561228495773

Epoch: 5| Step: 8
Training loss: 2.824610948562622
Validation loss: 2.20641698504007

Epoch: 5| Step: 9
Training loss: 2.4950664043426514
Validation loss: 2.2009641688357116

Epoch: 5| Step: 10
Training loss: 2.2422094345092773
Validation loss: 2.2166691416053363

Epoch: 291| Step: 0
Training loss: 2.838473320007324
Validation loss: 2.219911811172321

Epoch: 5| Step: 1
Training loss: 2.039565086364746
Validation loss: 2.2189195643189135

Epoch: 5| Step: 2
Training loss: 2.3690102100372314
Validation loss: 2.2220834660273727

Epoch: 5| Step: 3
Training loss: 2.4595859050750732
Validation loss: 2.222537738020702

Epoch: 5| Step: 4
Training loss: 2.0354819297790527
Validation loss: 2.2404879831498667

Epoch: 5| Step: 5
Training loss: 2.5189402103424072
Validation loss: 2.261488776053152

Epoch: 5| Step: 6
Training loss: 2.0102734565734863
Validation loss: 2.2984888476710164

Epoch: 5| Step: 7
Training loss: 2.5017876625061035
Validation loss: 2.32099562562922

Epoch: 5| Step: 8
Training loss: 2.759917736053467
Validation loss: 2.333217070948693

Epoch: 5| Step: 9
Training loss: 2.4553027153015137
Validation loss: 2.3226089003265544

Epoch: 5| Step: 10
Training loss: 2.6495234966278076
Validation loss: 2.28467188214743

Epoch: 292| Step: 0
Training loss: 2.319831371307373
Validation loss: 2.247146339826686

Epoch: 5| Step: 1
Training loss: 3.036783456802368
Validation loss: 2.222751235449186

Epoch: 5| Step: 2
Training loss: 1.6220417022705078
Validation loss: 2.195824738471739

Epoch: 5| Step: 3
Training loss: 3.033506393432617
Validation loss: 2.18521894690811

Epoch: 5| Step: 4
Training loss: 2.6191837787628174
Validation loss: 2.186080709580452

Epoch: 5| Step: 5
Training loss: 2.0705840587615967
Validation loss: 2.1875471274058023

Epoch: 5| Step: 6
Training loss: 2.3064661026000977
Validation loss: 2.1887619059572936

Epoch: 5| Step: 7
Training loss: 2.8275275230407715
Validation loss: 2.194046425563033

Epoch: 5| Step: 8
Training loss: 1.9954942464828491
Validation loss: 2.18838099510439

Epoch: 5| Step: 9
Training loss: 2.2876856327056885
Validation loss: 2.183732273758099

Epoch: 5| Step: 10
Training loss: 2.534851312637329
Validation loss: 2.182147777208718

Epoch: 293| Step: 0
Training loss: 2.496187686920166
Validation loss: 2.175850909243348

Epoch: 5| Step: 1
Training loss: 2.2886435985565186
Validation loss: 2.17844513411163

Epoch: 5| Step: 2
Training loss: 2.3689334392547607
Validation loss: 2.1834475660836823

Epoch: 5| Step: 3
Training loss: 2.482670307159424
Validation loss: 2.1740525973740445

Epoch: 5| Step: 4
Training loss: 3.301351547241211
Validation loss: 2.183389312477522

Epoch: 5| Step: 5
Training loss: 2.1122632026672363
Validation loss: 2.185717898030435

Epoch: 5| Step: 6
Training loss: 1.9177331924438477
Validation loss: 2.186717633278139

Epoch: 5| Step: 7
Training loss: 2.7955944538116455
Validation loss: 2.2015293080319642

Epoch: 5| Step: 8
Training loss: 2.3926024436950684
Validation loss: 2.1811931902362454

Epoch: 5| Step: 9
Training loss: 2.837545394897461
Validation loss: 2.203932155844986

Epoch: 5| Step: 10
Training loss: 1.3189892768859863
Validation loss: 2.1906587577635244

Epoch: 294| Step: 0
Training loss: 2.381047248840332
Validation loss: 2.1949608172139814

Epoch: 5| Step: 1
Training loss: 1.9186556339263916
Validation loss: 2.207046194743085

Epoch: 5| Step: 2
Training loss: 2.167027711868286
Validation loss: 2.2055369269463325

Epoch: 5| Step: 3
Training loss: 2.2863574028015137
Validation loss: 2.2342541256258563

Epoch: 5| Step: 4
Training loss: 2.3256402015686035
Validation loss: 2.215320902486001

Epoch: 5| Step: 5
Training loss: 2.752901315689087
Validation loss: 2.222535812726585

Epoch: 5| Step: 6
Training loss: 2.403794050216675
Validation loss: 2.239126166989726

Epoch: 5| Step: 7
Training loss: 2.656337261199951
Validation loss: 2.2528833189318256

Epoch: 5| Step: 8
Training loss: 2.9531497955322266
Validation loss: 2.2549499465573217

Epoch: 5| Step: 9
Training loss: 2.1053526401519775
Validation loss: 2.2234100282833142

Epoch: 5| Step: 10
Training loss: 2.557056188583374
Validation loss: 2.2130562490032566

Epoch: 295| Step: 0
Training loss: 2.741309642791748
Validation loss: 2.207391195399787

Epoch: 5| Step: 1
Training loss: 1.9468028545379639
Validation loss: 2.2001013858343965

Epoch: 5| Step: 2
Training loss: 2.8237640857696533
Validation loss: 2.2047522529478996

Epoch: 5| Step: 3
Training loss: 2.6888363361358643
Validation loss: 2.2195903306366294

Epoch: 5| Step: 4
Training loss: 1.7379474639892578
Validation loss: 2.2260362486685477

Epoch: 5| Step: 5
Training loss: 1.7415611743927002
Validation loss: 2.2356877660238617

Epoch: 5| Step: 6
Training loss: 2.749885082244873
Validation loss: 2.2288690331161662

Epoch: 5| Step: 7
Training loss: 1.6678119897842407
Validation loss: 2.2183127941623813

Epoch: 5| Step: 8
Training loss: 2.9186644554138184
Validation loss: 2.234218830703407

Epoch: 5| Step: 9
Training loss: 2.666543483734131
Validation loss: 2.241852193750361

Epoch: 5| Step: 10
Training loss: 2.7181644439697266
Validation loss: 2.2210103568210395

Epoch: 296| Step: 0
Training loss: 2.4234042167663574
Validation loss: 2.2126456435008715

Epoch: 5| Step: 1
Training loss: 2.449598789215088
Validation loss: 2.204736166102912

Epoch: 5| Step: 2
Training loss: 2.689239025115967
Validation loss: 2.202090665858279

Epoch: 5| Step: 3
Training loss: 2.662034511566162
Validation loss: 2.1907059941240536

Epoch: 5| Step: 4
Training loss: 2.488905191421509
Validation loss: 2.1792092015666347

Epoch: 5| Step: 5
Training loss: 2.519218921661377
Validation loss: 2.1734106617589153

Epoch: 5| Step: 6
Training loss: 1.8582322597503662
Validation loss: 2.1593627186231714

Epoch: 5| Step: 7
Training loss: 2.7329440116882324
Validation loss: 2.152507176963232

Epoch: 5| Step: 8
Training loss: 2.340592861175537
Validation loss: 2.1649061185057445

Epoch: 5| Step: 9
Training loss: 1.9698899984359741
Validation loss: 2.174316760032408

Epoch: 5| Step: 10
Training loss: 2.021103858947754
Validation loss: 2.181324585791557

Epoch: 297| Step: 0
Training loss: 2.321437120437622
Validation loss: 2.1995438529599096

Epoch: 5| Step: 1
Training loss: 2.590362071990967
Validation loss: 2.204988310413976

Epoch: 5| Step: 2
Training loss: 1.8728597164154053
Validation loss: 2.2071366412665254

Epoch: 5| Step: 3
Training loss: 2.5175375938415527
Validation loss: 2.2027352727869505

Epoch: 5| Step: 4
Training loss: 2.499772548675537
Validation loss: 2.2157621486212618

Epoch: 5| Step: 5
Training loss: 2.708768129348755
Validation loss: 2.2277001719320975

Epoch: 5| Step: 6
Training loss: 2.252559185028076
Validation loss: 2.2232521528838785

Epoch: 5| Step: 7
Training loss: 2.379500150680542
Validation loss: 2.249005179251394

Epoch: 5| Step: 8
Training loss: 2.285579204559326
Validation loss: 2.2472714326714955

Epoch: 5| Step: 9
Training loss: 2.2821502685546875
Validation loss: 2.2430237390661754

Epoch: 5| Step: 10
Training loss: 2.653822898864746
Validation loss: 2.2206329376466813

Epoch: 298| Step: 0
Training loss: 1.8349065780639648
Validation loss: 2.2249930520211496

Epoch: 5| Step: 1
Training loss: 3.102374315261841
Validation loss: 2.205567923925256

Epoch: 5| Step: 2
Training loss: 2.646646499633789
Validation loss: 2.2077566654451433

Epoch: 5| Step: 3
Training loss: 1.4878289699554443
Validation loss: 2.208741572595412

Epoch: 5| Step: 4
Training loss: 2.224891185760498
Validation loss: 2.1990896758212837

Epoch: 5| Step: 5
Training loss: 3.1211352348327637
Validation loss: 2.2131870562030422

Epoch: 5| Step: 6
Training loss: 1.946352243423462
Validation loss: 2.2234728797789542

Epoch: 5| Step: 7
Training loss: 2.822727918624878
Validation loss: 2.2381435542978267

Epoch: 5| Step: 8
Training loss: 2.058638095855713
Validation loss: 2.2322481422014135

Epoch: 5| Step: 9
Training loss: 2.0188961029052734
Validation loss: 2.22716841390056

Epoch: 5| Step: 10
Training loss: 3.0167806148529053
Validation loss: 2.2128015307969946

Epoch: 299| Step: 0
Training loss: 2.5177042484283447
Validation loss: 2.2429995408622165

Epoch: 5| Step: 1
Training loss: 2.70527982711792
Validation loss: 2.2285033605431996

Epoch: 5| Step: 2
Training loss: 2.21085786819458
Validation loss: 2.213885573930638

Epoch: 5| Step: 3
Training loss: 2.937256336212158
Validation loss: 2.200394361249862

Epoch: 5| Step: 4
Training loss: 2.1183485984802246
Validation loss: 2.198136269405324

Epoch: 5| Step: 5
Training loss: 2.21244478225708
Validation loss: 2.1921913880173878

Epoch: 5| Step: 6
Training loss: 2.4219489097595215
Validation loss: 2.1839535236358643

Epoch: 5| Step: 7
Training loss: 2.2704999446868896
Validation loss: 2.1869003003643406

Epoch: 5| Step: 8
Training loss: 2.4873263835906982
Validation loss: 2.196015742517287

Epoch: 5| Step: 9
Training loss: 2.058964967727661
Validation loss: 2.200523678974439

Epoch: 5| Step: 10
Training loss: 2.408503532409668
Validation loss: 2.194526316017233

Epoch: 300| Step: 0
Training loss: 2.3086230754852295
Validation loss: 2.1903410060431368

Epoch: 5| Step: 1
Training loss: 2.3286757469177246
Validation loss: 2.1917139330217914

Epoch: 5| Step: 2
Training loss: 2.391976833343506
Validation loss: 2.189021938590593

Epoch: 5| Step: 3
Training loss: 2.491215229034424
Validation loss: 2.1828234541800713

Epoch: 5| Step: 4
Training loss: 2.355729579925537
Validation loss: 2.173412935708159

Epoch: 5| Step: 5
Training loss: 2.6363472938537598
Validation loss: 2.1702598705086658

Epoch: 5| Step: 6
Training loss: 2.593064069747925
Validation loss: 2.168377553263018

Epoch: 5| Step: 7
Training loss: 2.564819097518921
Validation loss: 2.177703049875075

Epoch: 5| Step: 8
Training loss: 2.394010066986084
Validation loss: 2.1702792465045886

Epoch: 5| Step: 9
Training loss: 2.0143961906433105
Validation loss: 2.166550810619067

Epoch: 5| Step: 10
Training loss: 2.0747475624084473
Validation loss: 2.1802093713514266

Epoch: 301| Step: 0
Training loss: 2.43104887008667
Validation loss: 2.184439371990901

Epoch: 5| Step: 1
Training loss: 2.4703001976013184
Validation loss: 2.1729580151137484

Epoch: 5| Step: 2
Training loss: 2.5225021839141846
Validation loss: 2.1760393317027757

Epoch: 5| Step: 3
Training loss: 2.2813658714294434
Validation loss: 2.1689363025849864

Epoch: 5| Step: 4
Training loss: 2.7614667415618896
Validation loss: 2.17180238744264

Epoch: 5| Step: 5
Training loss: 2.2148282527923584
Validation loss: 2.170031983365295

Epoch: 5| Step: 6
Training loss: 2.5723531246185303
Validation loss: 2.1842985358289493

Epoch: 5| Step: 7
Training loss: 2.5001213550567627
Validation loss: 2.1729065807916785

Epoch: 5| Step: 8
Training loss: 2.051067352294922
Validation loss: 2.1887616957387617

Epoch: 5| Step: 9
Training loss: 2.3229899406433105
Validation loss: 2.1859403246192524

Epoch: 5| Step: 10
Training loss: 2.0991005897521973
Validation loss: 2.198698689860682

Epoch: 302| Step: 0
Training loss: 2.3029239177703857
Validation loss: 2.1924598575920187

Epoch: 5| Step: 1
Training loss: 2.318270444869995
Validation loss: 2.189699890793011

Epoch: 5| Step: 2
Training loss: 2.553377151489258
Validation loss: 2.1982772414402296

Epoch: 5| Step: 3
Training loss: 2.3071372509002686
Validation loss: 2.2007263116939093

Epoch: 5| Step: 4
Training loss: 2.3403801918029785
Validation loss: 2.2203310176890385

Epoch: 5| Step: 5
Training loss: 1.8791437149047852
Validation loss: 2.2184085589583202

Epoch: 5| Step: 6
Training loss: 2.368924140930176
Validation loss: 2.221409184958345

Epoch: 5| Step: 7
Training loss: 2.858689785003662
Validation loss: 2.2180244743183093

Epoch: 5| Step: 8
Training loss: 2.3696587085723877
Validation loss: 2.208603371856033

Epoch: 5| Step: 9
Training loss: 2.5425007343292236
Validation loss: 2.1933492178558023

Epoch: 5| Step: 10
Training loss: 2.3303167819976807
Validation loss: 2.197566586156045

Epoch: 303| Step: 0
Training loss: 3.150843381881714
Validation loss: 2.1917412152854343

Epoch: 5| Step: 1
Training loss: 1.8126178979873657
Validation loss: 2.1720812628346104

Epoch: 5| Step: 2
Training loss: 2.4175407886505127
Validation loss: 2.1791809502468316

Epoch: 5| Step: 3
Training loss: 2.3936736583709717
Validation loss: 2.1793638493425105

Epoch: 5| Step: 4
Training loss: 2.3881916999816895
Validation loss: 2.1662424661779918

Epoch: 5| Step: 5
Training loss: 2.3132896423339844
Validation loss: 2.1823795354494484

Epoch: 5| Step: 6
Training loss: 2.7698707580566406
Validation loss: 2.1814695506967525

Epoch: 5| Step: 7
Training loss: 2.324958324432373
Validation loss: 2.184410971979941

Epoch: 5| Step: 8
Training loss: 1.51101815700531
Validation loss: 2.192337343769689

Epoch: 5| Step: 9
Training loss: 3.0376880168914795
Validation loss: 2.2160526039779826

Epoch: 5| Step: 10
Training loss: 1.830862283706665
Validation loss: 2.2329568452732538

Epoch: 304| Step: 0
Training loss: 2.5038514137268066
Validation loss: 2.232033819280645

Epoch: 5| Step: 1
Training loss: 2.157438278198242
Validation loss: 2.252063097492341

Epoch: 5| Step: 2
Training loss: 2.5228819847106934
Validation loss: 2.243099667692697

Epoch: 5| Step: 3
Training loss: 2.7407240867614746
Validation loss: 2.2181013220099994

Epoch: 5| Step: 4
Training loss: 1.830967664718628
Validation loss: 2.1949562077881186

Epoch: 5| Step: 5
Training loss: 2.3583810329437256
Validation loss: 2.1859011111720914

Epoch: 5| Step: 6
Training loss: 2.4840900897979736
Validation loss: 2.1890931078182754

Epoch: 5| Step: 7
Training loss: 2.470229148864746
Validation loss: 2.1861014058513026

Epoch: 5| Step: 8
Training loss: 2.6114249229431152
Validation loss: 2.2057620504850983

Epoch: 5| Step: 9
Training loss: 2.370067834854126
Validation loss: 2.217272666192824

Epoch: 5| Step: 10
Training loss: 2.1441187858581543
Validation loss: 2.224320876982904

Epoch: 305| Step: 0
Training loss: 2.346367359161377
Validation loss: 2.2183540213492607

Epoch: 5| Step: 1
Training loss: 1.8709838390350342
Validation loss: 2.220647563216507

Epoch: 5| Step: 2
Training loss: 2.922880172729492
Validation loss: 2.2205435640068463

Epoch: 5| Step: 3
Training loss: 1.6886346340179443
Validation loss: 2.221792867106776

Epoch: 5| Step: 4
Training loss: 2.5949008464813232
Validation loss: 2.205009288685296

Epoch: 5| Step: 5
Training loss: 2.20497465133667
Validation loss: 2.204800449391847

Epoch: 5| Step: 6
Training loss: 2.295383930206299
Validation loss: 2.1912771783849245

Epoch: 5| Step: 7
Training loss: 2.126142978668213
Validation loss: 2.1891127376146216

Epoch: 5| Step: 8
Training loss: 3.102571964263916
Validation loss: 2.1993804772694907

Epoch: 5| Step: 9
Training loss: 2.542264699935913
Validation loss: 2.179556210835775

Epoch: 5| Step: 10
Training loss: 2.348787546157837
Validation loss: 2.180238417399827

Epoch: 306| Step: 0
Training loss: 2.024326801300049
Validation loss: 2.1790257077063284

Epoch: 5| Step: 1
Training loss: 2.6121320724487305
Validation loss: 2.179817766271612

Epoch: 5| Step: 2
Training loss: 2.1570255756378174
Validation loss: 2.1774634750940467

Epoch: 5| Step: 3
Training loss: 1.9882862567901611
Validation loss: 2.1880285996262745

Epoch: 5| Step: 4
Training loss: 2.3644020557403564
Validation loss: 2.1892420502119165

Epoch: 5| Step: 5
Training loss: 2.8933911323547363
Validation loss: 2.1939943964763353

Epoch: 5| Step: 6
Training loss: 2.4788498878479004
Validation loss: 2.1964567707430933

Epoch: 5| Step: 7
Training loss: 2.0590262413024902
Validation loss: 2.203365292600406

Epoch: 5| Step: 8
Training loss: 2.939025402069092
Validation loss: 2.2127045482717533

Epoch: 5| Step: 9
Training loss: 2.4129862785339355
Validation loss: 2.2181841506752917

Epoch: 5| Step: 10
Training loss: 2.2853338718414307
Validation loss: 2.198516635484593

Epoch: 307| Step: 0
Training loss: 2.3604674339294434
Validation loss: 2.183253181877957

Epoch: 5| Step: 1
Training loss: 1.642742395401001
Validation loss: 2.169524740147334

Epoch: 5| Step: 2
Training loss: 2.16811203956604
Validation loss: 2.1741968636871665

Epoch: 5| Step: 3
Training loss: 3.359177350997925
Validation loss: 2.162688283510106

Epoch: 5| Step: 4
Training loss: 2.4751744270324707
Validation loss: 2.1625205470669653

Epoch: 5| Step: 5
Training loss: 1.857203483581543
Validation loss: 2.1689028893747637

Epoch: 5| Step: 6
Training loss: 1.904219627380371
Validation loss: 2.172822562597131

Epoch: 5| Step: 7
Training loss: 2.4138596057891846
Validation loss: 2.173945585886637

Epoch: 5| Step: 8
Training loss: 2.1123945713043213
Validation loss: 2.165840874436081

Epoch: 5| Step: 9
Training loss: 3.4256691932678223
Validation loss: 2.1785993806777464

Epoch: 5| Step: 10
Training loss: 2.4350826740264893
Validation loss: 2.182232254294939

Epoch: 308| Step: 0
Training loss: 2.521928071975708
Validation loss: 2.1839306790341615

Epoch: 5| Step: 1
Training loss: 2.6317343711853027
Validation loss: 2.1931841937444543

Epoch: 5| Step: 2
Training loss: 2.5011212825775146
Validation loss: 2.2114492539436585

Epoch: 5| Step: 3
Training loss: 2.870072364807129
Validation loss: 2.2497758506446757

Epoch: 5| Step: 4
Training loss: 1.712385892868042
Validation loss: 2.2515717168008127

Epoch: 5| Step: 5
Training loss: 1.96845281124115
Validation loss: 2.2455248268701697

Epoch: 5| Step: 6
Training loss: 2.4004406929016113
Validation loss: 2.2409815185813495

Epoch: 5| Step: 7
Training loss: 2.681908130645752
Validation loss: 2.2422531253548077

Epoch: 5| Step: 8
Training loss: 2.230433464050293
Validation loss: 2.2189463799999607

Epoch: 5| Step: 9
Training loss: 2.4278030395507812
Validation loss: 2.2138152225043184

Epoch: 5| Step: 10
Training loss: 2.1471614837646484
Validation loss: 2.1858245762445594

Epoch: 309| Step: 0
Training loss: 2.3584952354431152
Validation loss: 2.1963402686580533

Epoch: 5| Step: 1
Training loss: 2.3638975620269775
Validation loss: 2.190041229289065

Epoch: 5| Step: 2
Training loss: 2.157733917236328
Validation loss: 2.186202323564919

Epoch: 5| Step: 3
Training loss: 2.440487861633301
Validation loss: 2.1941106268154678

Epoch: 5| Step: 4
Training loss: 1.926131010055542
Validation loss: 2.1754034078249367

Epoch: 5| Step: 5
Training loss: 2.219338893890381
Validation loss: 2.181739386691842

Epoch: 5| Step: 6
Training loss: 1.9891242980957031
Validation loss: 2.179209058002759

Epoch: 5| Step: 7
Training loss: 2.3415043354034424
Validation loss: 2.1801856948483374

Epoch: 5| Step: 8
Training loss: 2.672335624694824
Validation loss: 2.1888968483094247

Epoch: 5| Step: 9
Training loss: 2.8785130977630615
Validation loss: 2.2048586055796635

Epoch: 5| Step: 10
Training loss: 2.743691921234131
Validation loss: 2.2160044485522854

Epoch: 310| Step: 0
Training loss: 2.560183048248291
Validation loss: 2.232339664172101

Epoch: 5| Step: 1
Training loss: 2.1729540824890137
Validation loss: 2.224749831743138

Epoch: 5| Step: 2
Training loss: 2.1726794242858887
Validation loss: 2.207322620576428

Epoch: 5| Step: 3
Training loss: 2.7958271503448486
Validation loss: 2.1909035200713785

Epoch: 5| Step: 4
Training loss: 1.9630380868911743
Validation loss: 2.179864739858976

Epoch: 5| Step: 5
Training loss: 2.615339756011963
Validation loss: 2.168096823077048

Epoch: 5| Step: 6
Training loss: 2.9653427600860596
Validation loss: 2.1627669616412093

Epoch: 5| Step: 7
Training loss: 2.593958616256714
Validation loss: 2.1656416616132184

Epoch: 5| Step: 8
Training loss: 1.763028860092163
Validation loss: 2.1719854057476087

Epoch: 5| Step: 9
Training loss: 2.5100250244140625
Validation loss: 2.179830256328788

Epoch: 5| Step: 10
Training loss: 1.8959404230117798
Validation loss: 2.1744281117634108

Epoch: 311| Step: 0
Training loss: 2.402517318725586
Validation loss: 2.187209736916327

Epoch: 5| Step: 1
Training loss: 2.724398136138916
Validation loss: 2.1900490483930035

Epoch: 5| Step: 2
Training loss: 1.8837693929672241
Validation loss: 2.2014188587024646

Epoch: 5| Step: 3
Training loss: 2.138920545578003
Validation loss: 2.20330741841306

Epoch: 5| Step: 4
Training loss: 2.471328020095825
Validation loss: 2.2080782382718978

Epoch: 5| Step: 5
Training loss: 2.495206356048584
Validation loss: 2.2310704108207458

Epoch: 5| Step: 6
Training loss: 3.1253485679626465
Validation loss: 2.2271676127628615

Epoch: 5| Step: 7
Training loss: 2.1814372539520264
Validation loss: 2.2133061424378426

Epoch: 5| Step: 8
Training loss: 2.553302526473999
Validation loss: 2.2029773022538874

Epoch: 5| Step: 9
Training loss: 1.9884799718856812
Validation loss: 2.181370327549596

Epoch: 5| Step: 10
Training loss: 1.9829483032226562
Validation loss: 2.1897422728999967

Epoch: 312| Step: 0
Training loss: 2.7353157997131348
Validation loss: 2.1791791198074177

Epoch: 5| Step: 1
Training loss: 2.4427006244659424
Validation loss: 2.1858583060644006

Epoch: 5| Step: 2
Training loss: 2.1553573608398438
Validation loss: 2.1778210132352767

Epoch: 5| Step: 3
Training loss: 2.9525020122528076
Validation loss: 2.1671822827349425

Epoch: 5| Step: 4
Training loss: 1.434382677078247
Validation loss: 2.1626644698522424

Epoch: 5| Step: 5
Training loss: 2.1369881629943848
Validation loss: 2.1726366345600416

Epoch: 5| Step: 6
Training loss: 2.172074556350708
Validation loss: 2.1682359146815475

Epoch: 5| Step: 7
Training loss: 2.329730987548828
Validation loss: 2.1851401252131306

Epoch: 5| Step: 8
Training loss: 2.2408392429351807
Validation loss: 2.186276533270395

Epoch: 5| Step: 9
Training loss: 2.8627028465270996
Validation loss: 2.1948287346029796

Epoch: 5| Step: 10
Training loss: 2.413055896759033
Validation loss: 2.219256857390045

Epoch: 313| Step: 0
Training loss: 1.9797894954681396
Validation loss: 2.2132491809065624

Epoch: 5| Step: 1
Training loss: 2.107433795928955
Validation loss: 2.21162816914179

Epoch: 5| Step: 2
Training loss: 2.4054794311523438
Validation loss: 2.2116121451059976

Epoch: 5| Step: 3
Training loss: 2.5969221591949463
Validation loss: 2.2302447083175823

Epoch: 5| Step: 4
Training loss: 2.8023688793182373
Validation loss: 2.226567567035716

Epoch: 5| Step: 5
Training loss: 2.240663766860962
Validation loss: 2.2066263101434194

Epoch: 5| Step: 6
Training loss: 1.937429428100586
Validation loss: 2.2284161403614986

Epoch: 5| Step: 7
Training loss: 2.998918294906616
Validation loss: 2.245864757927515

Epoch: 5| Step: 8
Training loss: 1.9654500484466553
Validation loss: 2.247578731147192

Epoch: 5| Step: 9
Training loss: 2.527742862701416
Validation loss: 2.2334157446379304

Epoch: 5| Step: 10
Training loss: 2.331998825073242
Validation loss: 2.223465391384658

Epoch: 314| Step: 0
Training loss: 2.0636894702911377
Validation loss: 2.202731381180466

Epoch: 5| Step: 1
Training loss: 2.5284130573272705
Validation loss: 2.18216900415318

Epoch: 5| Step: 2
Training loss: 2.2879786491394043
Validation loss: 2.179693898847026

Epoch: 5| Step: 3
Training loss: 2.6047778129577637
Validation loss: 2.158097195368941

Epoch: 5| Step: 4
Training loss: 1.9388080835342407
Validation loss: 2.1697207548285045

Epoch: 5| Step: 5
Training loss: 2.562692165374756
Validation loss: 2.1748703807912846

Epoch: 5| Step: 6
Training loss: 2.3067965507507324
Validation loss: 2.1627488943838302

Epoch: 5| Step: 7
Training loss: 2.11264967918396
Validation loss: 2.1822059667238625

Epoch: 5| Step: 8
Training loss: 2.1220593452453613
Validation loss: 2.194175402323405

Epoch: 5| Step: 9
Training loss: 2.64137601852417
Validation loss: 2.1959808872592066

Epoch: 5| Step: 10
Training loss: 2.8362717628479004
Validation loss: 2.204768660247967

Epoch: 315| Step: 0
Training loss: 2.5195045471191406
Validation loss: 2.207384555570541

Epoch: 5| Step: 1
Training loss: 2.562808036804199
Validation loss: 2.2156234274628344

Epoch: 5| Step: 2
Training loss: 2.3220391273498535
Validation loss: 2.223922844856016

Epoch: 5| Step: 3
Training loss: 2.4775874614715576
Validation loss: 2.2190846422667145

Epoch: 5| Step: 4
Training loss: 2.3064723014831543
Validation loss: 2.226777991940898

Epoch: 5| Step: 5
Training loss: 2.5870308876037598
Validation loss: 2.209421133482328

Epoch: 5| Step: 6
Training loss: 2.298321008682251
Validation loss: 2.2134437073943434

Epoch: 5| Step: 7
Training loss: 2.459932327270508
Validation loss: 2.218249906775772

Epoch: 5| Step: 8
Training loss: 2.2757303714752197
Validation loss: 2.2090117444274244

Epoch: 5| Step: 9
Training loss: 2.1448402404785156
Validation loss: 2.1998125878713464

Epoch: 5| Step: 10
Training loss: 1.8473610877990723
Validation loss: 2.1774052189242457

Epoch: 316| Step: 0
Training loss: 3.051504135131836
Validation loss: 2.1862260321135163

Epoch: 5| Step: 1
Training loss: 2.8249669075012207
Validation loss: 2.189843549523302

Epoch: 5| Step: 2
Training loss: 2.342404842376709
Validation loss: 2.163264436106528

Epoch: 5| Step: 3
Training loss: 2.128819704055786
Validation loss: 2.1646503799705097

Epoch: 5| Step: 4
Training loss: 2.3677265644073486
Validation loss: 2.1600214999209166

Epoch: 5| Step: 5
Training loss: 2.099726676940918
Validation loss: 2.1566337359848844

Epoch: 5| Step: 6
Training loss: 2.001753330230713
Validation loss: 2.145694613456726

Epoch: 5| Step: 7
Training loss: 2.575191020965576
Validation loss: 2.156620748581425

Epoch: 5| Step: 8
Training loss: 2.1189615726470947
Validation loss: 2.164192594507689

Epoch: 5| Step: 9
Training loss: 1.9170620441436768
Validation loss: 2.1591371502927554

Epoch: 5| Step: 10
Training loss: 2.487031936645508
Validation loss: 2.1746297574812368

Epoch: 317| Step: 0
Training loss: 3.206249952316284
Validation loss: 2.183351614141977

Epoch: 5| Step: 1
Training loss: 2.3713269233703613
Validation loss: 2.1937787327715146

Epoch: 5| Step: 2
Training loss: 2.2791733741760254
Validation loss: 2.183538511235227

Epoch: 5| Step: 3
Training loss: 2.1754825115203857
Validation loss: 2.186515279995498

Epoch: 5| Step: 4
Training loss: 2.7489068508148193
Validation loss: 2.191970024057614

Epoch: 5| Step: 5
Training loss: 2.738670825958252
Validation loss: 2.2170434485199633

Epoch: 5| Step: 6
Training loss: 1.8984298706054688
Validation loss: 2.2093489618711573

Epoch: 5| Step: 7
Training loss: 2.719513416290283
Validation loss: 2.215408750759658

Epoch: 5| Step: 8
Training loss: 2.144998073577881
Validation loss: 2.2101952132358345

Epoch: 5| Step: 9
Training loss: 1.5709831714630127
Validation loss: 2.2142279583920716

Epoch: 5| Step: 10
Training loss: 1.890998363494873
Validation loss: 2.2019446562695246

Epoch: 318| Step: 0
Training loss: 2.3319501876831055
Validation loss: 2.2190404322839554

Epoch: 5| Step: 1
Training loss: 2.474475145339966
Validation loss: 2.197725216547648

Epoch: 5| Step: 2
Training loss: 2.1329846382141113
Validation loss: 2.206420088327059

Epoch: 5| Step: 3
Training loss: 1.9811394214630127
Validation loss: 2.185382452062381

Epoch: 5| Step: 4
Training loss: 1.7165319919586182
Validation loss: 2.1908235908836446

Epoch: 5| Step: 5
Training loss: 2.2636990547180176
Validation loss: 2.195337544205368

Epoch: 5| Step: 6
Training loss: 2.455723285675049
Validation loss: 2.189656730621092

Epoch: 5| Step: 7
Training loss: 2.297210693359375
Validation loss: 2.1920019285653227

Epoch: 5| Step: 8
Training loss: 3.1450486183166504
Validation loss: 2.189114596254082

Epoch: 5| Step: 9
Training loss: 2.6302764415740967
Validation loss: 2.184485243212792

Epoch: 5| Step: 10
Training loss: 2.3415369987487793
Validation loss: 2.180171307697091

Epoch: 319| Step: 0
Training loss: 2.2080941200256348
Validation loss: 2.1734527669927126

Epoch: 5| Step: 1
Training loss: 2.7557170391082764
Validation loss: 2.186619404823549

Epoch: 5| Step: 2
Training loss: 2.13883900642395
Validation loss: 2.172716768839026

Epoch: 5| Step: 3
Training loss: 2.405595302581787
Validation loss: 2.186743929821958

Epoch: 5| Step: 4
Training loss: 2.052216053009033
Validation loss: 2.1943537753115416

Epoch: 5| Step: 5
Training loss: 2.027923107147217
Validation loss: 2.2061791804529007

Epoch: 5| Step: 6
Training loss: 2.5037014484405518
Validation loss: 2.2252784685422013

Epoch: 5| Step: 7
Training loss: 2.3388702869415283
Validation loss: 2.243058104668894

Epoch: 5| Step: 8
Training loss: 2.0825648307800293
Validation loss: 2.2386460663169943

Epoch: 5| Step: 9
Training loss: 3.3658337593078613
Validation loss: 2.269094387690226

Epoch: 5| Step: 10
Training loss: 2.0411500930786133
Validation loss: 2.255152310094526

Epoch: 320| Step: 0
Training loss: 2.641451835632324
Validation loss: 2.2259694581390708

Epoch: 5| Step: 1
Training loss: 2.1274406909942627
Validation loss: 2.2210593761936313

Epoch: 5| Step: 2
Training loss: 2.200038194656372
Validation loss: 2.225514160689487

Epoch: 5| Step: 3
Training loss: 2.3496804237365723
Validation loss: 2.2396301582295406

Epoch: 5| Step: 4
Training loss: 2.484065532684326
Validation loss: 2.2365170448057112

Epoch: 5| Step: 5
Training loss: 2.5419235229492188
Validation loss: 2.2193299801118913

Epoch: 5| Step: 6
Training loss: 2.8090202808380127
Validation loss: 2.189094037138006

Epoch: 5| Step: 7
Training loss: 1.8182153701782227
Validation loss: 2.195715868344871

Epoch: 5| Step: 8
Training loss: 2.6079580783843994
Validation loss: 2.1924388767570577

Epoch: 5| Step: 9
Training loss: 1.6040973663330078
Validation loss: 2.1856221639981834

Epoch: 5| Step: 10
Training loss: 2.7787699699401855
Validation loss: 2.1944491068522134

Epoch: 321| Step: 0
Training loss: 2.832430601119995
Validation loss: 2.1814304603043424

Epoch: 5| Step: 1
Training loss: 1.7933003902435303
Validation loss: 2.183409639584121

Epoch: 5| Step: 2
Training loss: 2.319833517074585
Validation loss: 2.19062606493632

Epoch: 5| Step: 3
Training loss: 1.9472379684448242
Validation loss: 2.1859248915026264

Epoch: 5| Step: 4
Training loss: 2.619009494781494
Validation loss: 2.2016168076504945

Epoch: 5| Step: 5
Training loss: 2.1916756629943848
Validation loss: 2.2172735685943277

Epoch: 5| Step: 6
Training loss: 2.6791210174560547
Validation loss: 2.1984073551752235

Epoch: 5| Step: 7
Training loss: 2.5321667194366455
Validation loss: 2.197249017735963

Epoch: 5| Step: 8
Training loss: 1.8624556064605713
Validation loss: 2.1988514110606205

Epoch: 5| Step: 9
Training loss: 2.6074137687683105
Validation loss: 2.1917386298538535

Epoch: 5| Step: 10
Training loss: 2.287165880203247
Validation loss: 2.1870672446425243

Epoch: 322| Step: 0
Training loss: 1.7929093837738037
Validation loss: 2.172929940685149

Epoch: 5| Step: 1
Training loss: 2.7052090167999268
Validation loss: 2.1718473844630743

Epoch: 5| Step: 2
Training loss: 1.8339112997055054
Validation loss: 2.169942059824544

Epoch: 5| Step: 3
Training loss: 2.259544849395752
Validation loss: 2.175165991629324

Epoch: 5| Step: 4
Training loss: 2.412128448486328
Validation loss: 2.159790740218214

Epoch: 5| Step: 5
Training loss: 2.9172520637512207
Validation loss: 2.1719353763006066

Epoch: 5| Step: 6
Training loss: 2.54129958152771
Validation loss: 2.176314030924151

Epoch: 5| Step: 7
Training loss: 2.2381246089935303
Validation loss: 2.169432414475308

Epoch: 5| Step: 8
Training loss: 2.1290011405944824
Validation loss: 2.1784512227581394

Epoch: 5| Step: 9
Training loss: 2.543999195098877
Validation loss: 2.1739160617192588

Epoch: 5| Step: 10
Training loss: 2.375359535217285
Validation loss: 2.1590566148040113

Epoch: 323| Step: 0
Training loss: 1.8159277439117432
Validation loss: 2.168801510205833

Epoch: 5| Step: 1
Training loss: 2.5370914936065674
Validation loss: 2.1873767324673232

Epoch: 5| Step: 2
Training loss: 2.694859743118286
Validation loss: 2.209094467983451

Epoch: 5| Step: 3
Training loss: 2.5763542652130127
Validation loss: 2.2218862771987915

Epoch: 5| Step: 4
Training loss: 2.5268795490264893
Validation loss: 2.228266757021668

Epoch: 5| Step: 5
Training loss: 2.9976634979248047
Validation loss: 2.241505742073059

Epoch: 5| Step: 6
Training loss: 2.3562419414520264
Validation loss: 2.232789429285193

Epoch: 5| Step: 7
Training loss: 2.172813892364502
Validation loss: 2.218799368027718

Epoch: 5| Step: 8
Training loss: 1.3870363235473633
Validation loss: 2.1901977600589877

Epoch: 5| Step: 9
Training loss: 1.9359899759292603
Validation loss: 2.201183860019971

Epoch: 5| Step: 10
Training loss: 2.787797212600708
Validation loss: 2.1993619934205086

Epoch: 324| Step: 0
Training loss: 2.5894761085510254
Validation loss: 2.1940805194198445

Epoch: 5| Step: 1
Training loss: 2.621609926223755
Validation loss: 2.1972180579298284

Epoch: 5| Step: 2
Training loss: 2.027294158935547
Validation loss: 2.1860503355662027

Epoch: 5| Step: 3
Training loss: 2.0571272373199463
Validation loss: 2.1837681031996206

Epoch: 5| Step: 4
Training loss: 2.867593288421631
Validation loss: 2.2033714581561346

Epoch: 5| Step: 5
Training loss: 1.783494234085083
Validation loss: 2.2047113192978727

Epoch: 5| Step: 6
Training loss: 2.1393051147460938
Validation loss: 2.23341889022499

Epoch: 5| Step: 7
Training loss: 2.377373456954956
Validation loss: 2.232945239672097

Epoch: 5| Step: 8
Training loss: 1.4937883615493774
Validation loss: 2.2249271485113327

Epoch: 5| Step: 9
Training loss: 2.6377320289611816
Validation loss: 2.1994650389558528

Epoch: 5| Step: 10
Training loss: 3.309575319290161
Validation loss: 2.1931492487589517

Epoch: 325| Step: 0
Training loss: 2.4415321350097656
Validation loss: 2.1805663339553343

Epoch: 5| Step: 1
Training loss: 1.853851079940796
Validation loss: 2.1605153955439085

Epoch: 5| Step: 2
Training loss: 2.2858078479766846
Validation loss: 2.146394001540317

Epoch: 5| Step: 3
Training loss: 2.3652915954589844
Validation loss: 2.1522472366209953

Epoch: 5| Step: 4
Training loss: 2.1758885383605957
Validation loss: 2.1471803867688743

Epoch: 5| Step: 5
Training loss: 2.4798243045806885
Validation loss: 2.150412182654104

Epoch: 5| Step: 6
Training loss: 2.537187099456787
Validation loss: 2.1551528540990685

Epoch: 5| Step: 7
Training loss: 2.6283047199249268
Validation loss: 2.166275703778831

Epoch: 5| Step: 8
Training loss: 2.224327802658081
Validation loss: 2.1780394084991945

Epoch: 5| Step: 9
Training loss: 2.451183795928955
Validation loss: 2.1883954066102222

Epoch: 5| Step: 10
Training loss: 2.4100193977355957
Validation loss: 2.182669472950761

Epoch: 326| Step: 0
Training loss: 1.920873999595642
Validation loss: 2.2102803645595426

Epoch: 5| Step: 1
Training loss: 2.2532572746276855
Validation loss: 2.2191791406241794

Epoch: 5| Step: 2
Training loss: 2.423701286315918
Validation loss: 2.243458121053634

Epoch: 5| Step: 3
Training loss: 2.629300355911255
Validation loss: 2.2504978359386487

Epoch: 5| Step: 4
Training loss: 2.751342296600342
Validation loss: 2.2242020317303237

Epoch: 5| Step: 5
Training loss: 3.0472419261932373
Validation loss: 2.219894421997891

Epoch: 5| Step: 6
Training loss: 1.799700379371643
Validation loss: 2.195233227104269

Epoch: 5| Step: 7
Training loss: 2.208665609359741
Validation loss: 2.1739082003152497

Epoch: 5| Step: 8
Training loss: 2.4931442737579346
Validation loss: 2.165962107719914

Epoch: 5| Step: 9
Training loss: 1.5485203266143799
Validation loss: 2.1501718157081195

Epoch: 5| Step: 10
Training loss: 2.6935012340545654
Validation loss: 2.149644381256514

Epoch: 327| Step: 0
Training loss: 2.210763931274414
Validation loss: 2.1587946081674225

Epoch: 5| Step: 1
Training loss: 2.181035280227661
Validation loss: 2.1514833460571947

Epoch: 5| Step: 2
Training loss: 2.882880687713623
Validation loss: 2.164172762183733

Epoch: 5| Step: 3
Training loss: 2.8521199226379395
Validation loss: 2.185682919717604

Epoch: 5| Step: 4
Training loss: 2.2897162437438965
Validation loss: 2.2058089215268373

Epoch: 5| Step: 5
Training loss: 1.9941108226776123
Validation loss: 2.209786404845535

Epoch: 5| Step: 6
Training loss: 2.062906503677368
Validation loss: 2.211823299366941

Epoch: 5| Step: 7
Training loss: 2.2652387619018555
Validation loss: 2.206932544708252

Epoch: 5| Step: 8
Training loss: 2.381849765777588
Validation loss: 2.20401902608974

Epoch: 5| Step: 9
Training loss: 2.014606475830078
Validation loss: 2.201464392805612

Epoch: 5| Step: 10
Training loss: 2.496845006942749
Validation loss: 2.2172418845597135

Epoch: 328| Step: 0
Training loss: 1.7084659337997437
Validation loss: 2.1922300887364212

Epoch: 5| Step: 1
Training loss: 1.5819025039672852
Validation loss: 2.1802308200508036

Epoch: 5| Step: 2
Training loss: 2.4161925315856934
Validation loss: 2.1703905443991385

Epoch: 5| Step: 3
Training loss: 2.832545757293701
Validation loss: 2.179064586598386

Epoch: 5| Step: 4
Training loss: 2.9416604042053223
Validation loss: 2.1862513711375575

Epoch: 5| Step: 5
Training loss: 2.9990243911743164
Validation loss: 2.1728877611057733

Epoch: 5| Step: 6
Training loss: 2.4302687644958496
Validation loss: 2.1724709182657223

Epoch: 5| Step: 7
Training loss: 2.1619410514831543
Validation loss: 2.150872399730067

Epoch: 5| Step: 8
Training loss: 2.615666151046753
Validation loss: 2.163057392643344

Epoch: 5| Step: 9
Training loss: 2.0202488899230957
Validation loss: 2.1804148817575104

Epoch: 5| Step: 10
Training loss: 2.0283823013305664
Validation loss: 2.181983901608375

Epoch: 329| Step: 0
Training loss: 1.9444376230239868
Validation loss: 2.2039457162221274

Epoch: 5| Step: 1
Training loss: 2.9826037883758545
Validation loss: 2.2306125010213544

Epoch: 5| Step: 2
Training loss: 2.548668146133423
Validation loss: 2.2591534212071407

Epoch: 5| Step: 3
Training loss: 2.641835927963257
Validation loss: 2.2674154902017243

Epoch: 5| Step: 4
Training loss: 2.2266058921813965
Validation loss: 2.2659252433366674

Epoch: 5| Step: 5
Training loss: 1.9511890411376953
Validation loss: 2.248359262302358

Epoch: 5| Step: 6
Training loss: 2.59295654296875
Validation loss: 2.208608990074486

Epoch: 5| Step: 7
Training loss: 2.730564594268799
Validation loss: 2.2005574780125774

Epoch: 5| Step: 8
Training loss: 2.158329963684082
Validation loss: 2.170302188524636

Epoch: 5| Step: 9
Training loss: 1.9676510095596313
Validation loss: 2.1618451687597458

Epoch: 5| Step: 10
Training loss: 1.954069972038269
Validation loss: 2.1629355979222122

Epoch: 330| Step: 0
Training loss: 2.067777156829834
Validation loss: 2.1694619271063034

Epoch: 5| Step: 1
Training loss: 2.620296001434326
Validation loss: 2.158224521144744

Epoch: 5| Step: 2
Training loss: 2.3058762550354004
Validation loss: 2.1540599176960606

Epoch: 5| Step: 3
Training loss: 2.854489803314209
Validation loss: 2.1534436800146617

Epoch: 5| Step: 4
Training loss: 2.696061372756958
Validation loss: 2.1391303231639247

Epoch: 5| Step: 5
Training loss: 2.2717132568359375
Validation loss: 2.1729064564551077

Epoch: 5| Step: 6
Training loss: 2.1043412685394287
Validation loss: 2.1558136145273843

Epoch: 5| Step: 7
Training loss: 2.258176565170288
Validation loss: 2.1705251842416744

Epoch: 5| Step: 8
Training loss: 1.9910789728164673
Validation loss: 2.174254032873338

Epoch: 5| Step: 9
Training loss: 2.2873454093933105
Validation loss: 2.1685043329833658

Epoch: 5| Step: 10
Training loss: 1.9652059078216553
Validation loss: 2.189948566498295

Epoch: 331| Step: 0
Training loss: 2.5655059814453125
Validation loss: 2.178458088187761

Epoch: 5| Step: 1
Training loss: 2.200136661529541
Validation loss: 2.201190435758201

Epoch: 5| Step: 2
Training loss: 2.8392605781555176
Validation loss: 2.2304547973858413

Epoch: 5| Step: 3
Training loss: 2.452855110168457
Validation loss: 2.241254283535865

Epoch: 5| Step: 4
Training loss: 1.8996320962905884
Validation loss: 2.245867383095526

Epoch: 5| Step: 5
Training loss: 2.894486665725708
Validation loss: 2.2214149146951656

Epoch: 5| Step: 6
Training loss: 2.206064224243164
Validation loss: 2.189674013404436

Epoch: 5| Step: 7
Training loss: 2.3294849395751953
Validation loss: 2.1669701068632063

Epoch: 5| Step: 8
Training loss: 2.307724714279175
Validation loss: 2.1830500197666947

Epoch: 5| Step: 9
Training loss: 2.1825428009033203
Validation loss: 2.185087078361101

Epoch: 5| Step: 10
Training loss: 1.6792179346084595
Validation loss: 2.194331351146903

Epoch: 332| Step: 0
Training loss: 2.078723669052124
Validation loss: 2.1908608328911567

Epoch: 5| Step: 1
Training loss: 1.3056795597076416
Validation loss: 2.204373913426553

Epoch: 5| Step: 2
Training loss: 2.7156338691711426
Validation loss: 2.2061653393571095

Epoch: 5| Step: 3
Training loss: 2.373713493347168
Validation loss: 2.2156506584536646

Epoch: 5| Step: 4
Training loss: 2.6491024494171143
Validation loss: 2.213232805652003

Epoch: 5| Step: 5
Training loss: 2.5418097972869873
Validation loss: 2.1846089516916583

Epoch: 5| Step: 6
Training loss: 2.5388102531433105
Validation loss: 2.184338867023427

Epoch: 5| Step: 7
Training loss: 1.9786722660064697
Validation loss: 2.1647389755454114

Epoch: 5| Step: 8
Training loss: 1.9332062005996704
Validation loss: 2.1571908061222365

Epoch: 5| Step: 9
Training loss: 3.0178799629211426
Validation loss: 2.1635980042078162

Epoch: 5| Step: 10
Training loss: 2.3899948596954346
Validation loss: 2.154191396569693

Epoch: 333| Step: 0
Training loss: 1.8223998546600342
Validation loss: 2.1734360623103317

Epoch: 5| Step: 1
Training loss: 2.5926740169525146
Validation loss: 2.170887780445878

Epoch: 5| Step: 2
Training loss: 1.445710301399231
Validation loss: 2.1948614684484338

Epoch: 5| Step: 3
Training loss: 2.265282154083252
Validation loss: 2.1914952467846613

Epoch: 5| Step: 4
Training loss: 2.3119845390319824
Validation loss: 2.1926766518623597

Epoch: 5| Step: 5
Training loss: 2.6605513095855713
Validation loss: 2.1815284785404

Epoch: 5| Step: 6
Training loss: 2.558502435684204
Validation loss: 2.1844251027671238

Epoch: 5| Step: 7
Training loss: 2.065840721130371
Validation loss: 2.1562878957358738

Epoch: 5| Step: 8
Training loss: 2.846315860748291
Validation loss: 2.1604657096247517

Epoch: 5| Step: 9
Training loss: 2.2461700439453125
Validation loss: 2.1536719696496123

Epoch: 5| Step: 10
Training loss: 2.927144765853882
Validation loss: 2.1455969425939743

Epoch: 334| Step: 0
Training loss: 1.987592101097107
Validation loss: 2.151928004398141

Epoch: 5| Step: 1
Training loss: 2.1845784187316895
Validation loss: 2.154312538844283

Epoch: 5| Step: 2
Training loss: 2.119508981704712
Validation loss: 2.1674694015133764

Epoch: 5| Step: 3
Training loss: 2.208773136138916
Validation loss: 2.1729678543665076

Epoch: 5| Step: 4
Training loss: 1.9098503589630127
Validation loss: 2.191599613876753

Epoch: 5| Step: 5
Training loss: 1.6589291095733643
Validation loss: 2.2020934832993375

Epoch: 5| Step: 6
Training loss: 2.41969895362854
Validation loss: 2.227012429186093

Epoch: 5| Step: 7
Training loss: 3.091616630554199
Validation loss: 2.2306579159152125

Epoch: 5| Step: 8
Training loss: 2.7872767448425293
Validation loss: 2.236156726396212

Epoch: 5| Step: 9
Training loss: 2.9560916423797607
Validation loss: 2.218975833667222

Epoch: 5| Step: 10
Training loss: 2.2782208919525146
Validation loss: 2.190444577124811

Epoch: 335| Step: 0
Training loss: 1.9183765649795532
Validation loss: 2.2039922488633024

Epoch: 5| Step: 1
Training loss: 2.3710379600524902
Validation loss: 2.195073002128191

Epoch: 5| Step: 2
Training loss: 1.7486377954483032
Validation loss: 2.1741854144680883

Epoch: 5| Step: 3
Training loss: 2.716522693634033
Validation loss: 2.182821335331086

Epoch: 5| Step: 4
Training loss: 2.1795244216918945
Validation loss: 2.1566587917266355

Epoch: 5| Step: 5
Training loss: 2.4459567070007324
Validation loss: 2.186413462444018

Epoch: 5| Step: 6
Training loss: 2.376469850540161
Validation loss: 2.1924710504470335

Epoch: 5| Step: 7
Training loss: 2.579866409301758
Validation loss: 2.1936119935845815

Epoch: 5| Step: 8
Training loss: 2.0912299156188965
Validation loss: 2.208508768389302

Epoch: 5| Step: 9
Training loss: 2.3211090564727783
Validation loss: 2.2221667997298704

Epoch: 5| Step: 10
Training loss: 2.7196807861328125
Validation loss: 2.2190874955987416

Epoch: 336| Step: 0
Training loss: 2.7237002849578857
Validation loss: 2.213928564902275

Epoch: 5| Step: 1
Training loss: 2.0501418113708496
Validation loss: 2.197811072872531

Epoch: 5| Step: 2
Training loss: 2.8508214950561523
Validation loss: 2.208666529706729

Epoch: 5| Step: 3
Training loss: 2.2942211627960205
Validation loss: 2.2052707210663827

Epoch: 5| Step: 4
Training loss: 2.4447097778320312
Validation loss: 2.195455362719874

Epoch: 5| Step: 5
Training loss: 1.6982237100601196
Validation loss: 2.19119401900999

Epoch: 5| Step: 6
Training loss: 2.135262966156006
Validation loss: 2.1816670253712642

Epoch: 5| Step: 7
Training loss: 2.492574691772461
Validation loss: 2.170990461944252

Epoch: 5| Step: 8
Training loss: 2.1989405155181885
Validation loss: 2.1719785787725963

Epoch: 5| Step: 9
Training loss: 2.2170569896698
Validation loss: 2.1700012709504817

Epoch: 5| Step: 10
Training loss: 2.266148805618286
Validation loss: 2.158285152527594

Epoch: 337| Step: 0
Training loss: 2.725350856781006
Validation loss: 2.1543847027645318

Epoch: 5| Step: 1
Training loss: 2.6634950637817383
Validation loss: 2.1820160881165536

Epoch: 5| Step: 2
Training loss: 2.1467127799987793
Validation loss: 2.178754042553645

Epoch: 5| Step: 3
Training loss: 2.6455893516540527
Validation loss: 2.1940109960494505

Epoch: 5| Step: 4
Training loss: 2.487614393234253
Validation loss: 2.2020465968757548

Epoch: 5| Step: 5
Training loss: 1.4996917247772217
Validation loss: 2.2322488266934633

Epoch: 5| Step: 6
Training loss: 2.4108285903930664
Validation loss: 2.2292853234916605

Epoch: 5| Step: 7
Training loss: 1.6271460056304932
Validation loss: 2.2375856778954946

Epoch: 5| Step: 8
Training loss: 2.8138480186462402
Validation loss: 2.2398461116257535

Epoch: 5| Step: 9
Training loss: 1.8235816955566406
Validation loss: 2.2328987339491486

Epoch: 5| Step: 10
Training loss: 2.549211025238037
Validation loss: 2.214216298954461

Epoch: 338| Step: 0
Training loss: 1.8446416854858398
Validation loss: 2.208689837045567

Epoch: 5| Step: 1
Training loss: 1.8409847021102905
Validation loss: 2.1936065817392

Epoch: 5| Step: 2
Training loss: 1.9180189371109009
Validation loss: 2.1694385044036375

Epoch: 5| Step: 3
Training loss: 2.3778610229492188
Validation loss: 2.157744223071683

Epoch: 5| Step: 4
Training loss: 2.778421640396118
Validation loss: 2.1697605553493706

Epoch: 5| Step: 5
Training loss: 2.485125780105591
Validation loss: 2.159541876085343

Epoch: 5| Step: 6
Training loss: 2.0155866146087646
Validation loss: 2.151377800972231

Epoch: 5| Step: 7
Training loss: 2.677100896835327
Validation loss: 2.150070418593704

Epoch: 5| Step: 8
Training loss: 2.6611697673797607
Validation loss: 2.1474692385683776

Epoch: 5| Step: 9
Training loss: 2.422539472579956
Validation loss: 2.1375809766912974

Epoch: 5| Step: 10
Training loss: 2.389427661895752
Validation loss: 2.1443983585603776

Epoch: 339| Step: 0
Training loss: 2.4486942291259766
Validation loss: 2.148150174848495

Epoch: 5| Step: 1
Training loss: 2.4515717029571533
Validation loss: 2.167320361701391

Epoch: 5| Step: 2
Training loss: 1.941489815711975
Validation loss: 2.181028542980071

Epoch: 5| Step: 3
Training loss: 2.4317283630371094
Validation loss: 2.1992223621696554

Epoch: 5| Step: 4
Training loss: 2.514533281326294
Validation loss: 2.195515012228361

Epoch: 5| Step: 5
Training loss: 2.238442897796631
Validation loss: 2.1961085847629014

Epoch: 5| Step: 6
Training loss: 2.9804158210754395
Validation loss: 2.2112646961724884

Epoch: 5| Step: 7
Training loss: 1.607322096824646
Validation loss: 2.197423988772977

Epoch: 5| Step: 8
Training loss: 1.999057412147522
Validation loss: 2.201276417701475

Epoch: 5| Step: 9
Training loss: 2.4147143363952637
Validation loss: 2.177983222469207

Epoch: 5| Step: 10
Training loss: 2.368934392929077
Validation loss: 2.1869800654790734

Epoch: 340| Step: 0
Training loss: 1.875982642173767
Validation loss: 2.1598708860335813

Epoch: 5| Step: 1
Training loss: 2.891570568084717
Validation loss: 2.1631722116983063

Epoch: 5| Step: 2
Training loss: 2.8858399391174316
Validation loss: 2.14859333474149

Epoch: 5| Step: 3
Training loss: 2.1892812252044678
Validation loss: 2.1457464618067585

Epoch: 5| Step: 4
Training loss: 2.4448885917663574
Validation loss: 2.157852643279619

Epoch: 5| Step: 5
Training loss: 2.0989162921905518
Validation loss: 2.16418198872638

Epoch: 5| Step: 6
Training loss: 2.5785012245178223
Validation loss: 2.1535459872215026

Epoch: 5| Step: 7
Training loss: 2.2822959423065186
Validation loss: 2.189277169524982

Epoch: 5| Step: 8
Training loss: 2.3863043785095215
Validation loss: 2.205863014344246

Epoch: 5| Step: 9
Training loss: 1.900783896446228
Validation loss: 2.2057682109135452

Epoch: 5| Step: 10
Training loss: 1.7297236919403076
Validation loss: 2.2118759104000625

Epoch: 341| Step: 0
Training loss: 2.4705376625061035
Validation loss: 2.207961551604732

Epoch: 5| Step: 1
Training loss: 2.0494978427886963
Validation loss: 2.198113049230268

Epoch: 5| Step: 2
Training loss: 2.882420063018799
Validation loss: 2.1853258302134853

Epoch: 5| Step: 3
Training loss: 1.6985172033309937
Validation loss: 2.1662462654934136

Epoch: 5| Step: 4
Training loss: 2.759972333908081
Validation loss: 2.156547522032133

Epoch: 5| Step: 5
Training loss: 2.0263609886169434
Validation loss: 2.1711559218745076

Epoch: 5| Step: 6
Training loss: 2.0646462440490723
Validation loss: 2.17455582721259

Epoch: 5| Step: 7
Training loss: 2.551374912261963
Validation loss: 2.1941642274138746

Epoch: 5| Step: 8
Training loss: 1.948366403579712
Validation loss: 2.2070466831166256

Epoch: 5| Step: 9
Training loss: 2.4726760387420654
Validation loss: 2.2161091886540896

Epoch: 5| Step: 10
Training loss: 2.3913049697875977
Validation loss: 2.2134842180436656

Epoch: 342| Step: 0
Training loss: 2.1101229190826416
Validation loss: 2.1963086589690177

Epoch: 5| Step: 1
Training loss: 2.0432653427124023
Validation loss: 2.1975614640020553

Epoch: 5| Step: 2
Training loss: 2.376720428466797
Validation loss: 2.18180727189587

Epoch: 5| Step: 3
Training loss: 2.517763614654541
Validation loss: 2.174723932819982

Epoch: 5| Step: 4
Training loss: 2.4176578521728516
Validation loss: 2.162569074220555

Epoch: 5| Step: 5
Training loss: 2.6185543537139893
Validation loss: 2.1722191828553394

Epoch: 5| Step: 6
Training loss: 2.569483757019043
Validation loss: 2.155235605855142

Epoch: 5| Step: 7
Training loss: 2.3082008361816406
Validation loss: 2.1652590741393385

Epoch: 5| Step: 8
Training loss: 1.9314115047454834
Validation loss: 2.1828790890273226

Epoch: 5| Step: 9
Training loss: 1.9761626720428467
Validation loss: 2.172006359664343

Epoch: 5| Step: 10
Training loss: 2.2391469478607178
Validation loss: 2.1733060921392133

Epoch: 343| Step: 0
Training loss: 1.9350112676620483
Validation loss: 2.1713916627309655

Epoch: 5| Step: 1
Training loss: 2.5379974842071533
Validation loss: 2.1550754295882357

Epoch: 5| Step: 2
Training loss: 1.8009116649627686
Validation loss: 2.163256352947604

Epoch: 5| Step: 3
Training loss: 2.314628839492798
Validation loss: 2.1579226678417576

Epoch: 5| Step: 4
Training loss: 2.155474901199341
Validation loss: 2.1687402289400817

Epoch: 5| Step: 5
Training loss: 2.8555853366851807
Validation loss: 2.1737998198437434

Epoch: 5| Step: 6
Training loss: 2.187777042388916
Validation loss: 2.1656042427145024

Epoch: 5| Step: 7
Training loss: 2.3276984691619873
Validation loss: 2.198302689418998

Epoch: 5| Step: 8
Training loss: 2.103053331375122
Validation loss: 2.187399705251058

Epoch: 5| Step: 9
Training loss: 2.602001667022705
Validation loss: 2.195889042269799

Epoch: 5| Step: 10
Training loss: 2.4197864532470703
Validation loss: 2.2048828319836686

Epoch: 344| Step: 0
Training loss: 2.4720242023468018
Validation loss: 2.1856509844462075

Epoch: 5| Step: 1
Training loss: 2.014230966567993
Validation loss: 2.1653383213986634

Epoch: 5| Step: 2
Training loss: 1.8958505392074585
Validation loss: 2.160210600463293

Epoch: 5| Step: 3
Training loss: 2.1976966857910156
Validation loss: 2.154121316889281

Epoch: 5| Step: 4
Training loss: 2.2061495780944824
Validation loss: 2.15042781829834

Epoch: 5| Step: 5
Training loss: 2.7584290504455566
Validation loss: 2.1477055831622054

Epoch: 5| Step: 6
Training loss: 2.2830305099487305
Validation loss: 2.163848255270271

Epoch: 5| Step: 7
Training loss: 2.173741102218628
Validation loss: 2.1543114646788566

Epoch: 5| Step: 8
Training loss: 2.4966845512390137
Validation loss: 2.184061875907324

Epoch: 5| Step: 9
Training loss: 2.3495571613311768
Validation loss: 2.1964278272403184

Epoch: 5| Step: 10
Training loss: 2.4175846576690674
Validation loss: 2.215412055292437

Epoch: 345| Step: 0
Training loss: 2.6487221717834473
Validation loss: 2.2111916542053223

Epoch: 5| Step: 1
Training loss: 1.8638683557510376
Validation loss: 2.192476543047095

Epoch: 5| Step: 2
Training loss: 2.951486587524414
Validation loss: 2.1871917273408625

Epoch: 5| Step: 3
Training loss: 2.1740963459014893
Validation loss: 2.165607101173811

Epoch: 5| Step: 4
Training loss: 2.445620059967041
Validation loss: 2.1765304073210685

Epoch: 5| Step: 5
Training loss: 2.110898494720459
Validation loss: 2.171923088771041

Epoch: 5| Step: 6
Training loss: 2.6764659881591797
Validation loss: 2.171317659398561

Epoch: 5| Step: 7
Training loss: 2.1017627716064453
Validation loss: 2.1759053353340394

Epoch: 5| Step: 8
Training loss: 2.3181865215301514
Validation loss: 2.158206469269209

Epoch: 5| Step: 9
Training loss: 1.7194675207138062
Validation loss: 2.1713965836391655

Epoch: 5| Step: 10
Training loss: 1.8751261234283447
Validation loss: 2.1616252417205484

Epoch: 346| Step: 0
Training loss: 2.4794788360595703
Validation loss: 2.164591637990808

Epoch: 5| Step: 1
Training loss: 2.340893507003784
Validation loss: 2.1737171052604594

Epoch: 5| Step: 2
Training loss: 2.320667266845703
Validation loss: 2.1780501475898166

Epoch: 5| Step: 3
Training loss: 1.7855541706085205
Validation loss: 2.19145050356465

Epoch: 5| Step: 4
Training loss: 2.183506727218628
Validation loss: 2.1925270531767156

Epoch: 5| Step: 5
Training loss: 1.9153474569320679
Validation loss: 2.18918671659244

Epoch: 5| Step: 6
Training loss: 2.2724337577819824
Validation loss: 2.196104811083886

Epoch: 5| Step: 7
Training loss: 2.2637197971343994
Validation loss: 2.180287140671925

Epoch: 5| Step: 8
Training loss: 2.3567376136779785
Validation loss: 2.1778538355263333

Epoch: 5| Step: 9
Training loss: 2.658958911895752
Validation loss: 2.1764584587466334

Epoch: 5| Step: 10
Training loss: 2.4885125160217285
Validation loss: 2.156428820343428

Epoch: 347| Step: 0
Training loss: 2.352510452270508
Validation loss: 2.150610048283813

Epoch: 5| Step: 1
Training loss: 2.5894603729248047
Validation loss: 2.1449633670109574

Epoch: 5| Step: 2
Training loss: 2.039036512374878
Validation loss: 2.153911575194328

Epoch: 5| Step: 3
Training loss: 2.5117640495300293
Validation loss: 2.174119326376146

Epoch: 5| Step: 4
Training loss: 2.6454029083251953
Validation loss: 2.152012360993252

Epoch: 5| Step: 5
Training loss: 2.3763434886932373
Validation loss: 2.1544545363354426

Epoch: 5| Step: 6
Training loss: 2.1316680908203125
Validation loss: 2.155857927055769

Epoch: 5| Step: 7
Training loss: 2.000908374786377
Validation loss: 2.1837915502568728

Epoch: 5| Step: 8
Training loss: 2.189066171646118
Validation loss: 2.1956939338355936

Epoch: 5| Step: 9
Training loss: 2.3357653617858887
Validation loss: 2.21820488027347

Epoch: 5| Step: 10
Training loss: 1.9866104125976562
Validation loss: 2.2309485327812935

Epoch: 348| Step: 0
Training loss: 2.411485195159912
Validation loss: 2.2043203384645524

Epoch: 5| Step: 1
Training loss: 2.148205280303955
Validation loss: 2.180251931631437

Epoch: 5| Step: 2
Training loss: 1.938145399093628
Validation loss: 2.1543113800787155

Epoch: 5| Step: 3
Training loss: 2.5248684883117676
Validation loss: 2.13523329201565

Epoch: 5| Step: 4
Training loss: 1.6123754978179932
Validation loss: 2.1419776383266655

Epoch: 5| Step: 5
Training loss: 2.261007785797119
Validation loss: 2.13642123950425

Epoch: 5| Step: 6
Training loss: 2.8590455055236816
Validation loss: 2.1631210875767533

Epoch: 5| Step: 7
Training loss: 2.3290581703186035
Validation loss: 2.1769013661210255

Epoch: 5| Step: 8
Training loss: 2.415961980819702
Validation loss: 2.1603281267227663

Epoch: 5| Step: 9
Training loss: 2.511317253112793
Validation loss: 2.1662853892131517

Epoch: 5| Step: 10
Training loss: 2.3124637603759766
Validation loss: 2.1576334507234636

Epoch: 349| Step: 0
Training loss: 2.2134029865264893
Validation loss: 2.1668045648964505

Epoch: 5| Step: 1
Training loss: 2.652980327606201
Validation loss: 2.166012292267174

Epoch: 5| Step: 2
Training loss: 2.4591116905212402
Validation loss: 2.1640210997673774

Epoch: 5| Step: 3
Training loss: 2.2169089317321777
Validation loss: 2.174969623165746

Epoch: 5| Step: 4
Training loss: 2.6877222061157227
Validation loss: 2.19926949983002

Epoch: 5| Step: 5
Training loss: 2.3044378757476807
Validation loss: 2.2110058428138815

Epoch: 5| Step: 6
Training loss: 2.3611388206481934
Validation loss: 2.2053982032242643

Epoch: 5| Step: 7
Training loss: 1.9439140558242798
Validation loss: 2.199648439243276

Epoch: 5| Step: 8
Training loss: 2.06355881690979
Validation loss: 2.191280822600088

Epoch: 5| Step: 9
Training loss: 1.9616200923919678
Validation loss: 2.1797789424978276

Epoch: 5| Step: 10
Training loss: 2.150583028793335
Validation loss: 2.1767450827424244

Epoch: 350| Step: 0
Training loss: 2.590555191040039
Validation loss: 2.1490092751800374

Epoch: 5| Step: 1
Training loss: 2.459259510040283
Validation loss: 2.1544274822358163

Epoch: 5| Step: 2
Training loss: 1.828464150428772
Validation loss: 2.155666391054789

Epoch: 5| Step: 3
Training loss: 1.9412431716918945
Validation loss: 2.1457693781903995

Epoch: 5| Step: 4
Training loss: 2.6487319469451904
Validation loss: 2.1503972545746834

Epoch: 5| Step: 5
Training loss: 2.25233793258667
Validation loss: 2.146350492713272

Epoch: 5| Step: 6
Training loss: 2.678290843963623
Validation loss: 2.154799412655574

Epoch: 5| Step: 7
Training loss: 2.217623233795166
Validation loss: 2.171930497692477

Epoch: 5| Step: 8
Training loss: 2.0405585765838623
Validation loss: 2.1712180299143635

Epoch: 5| Step: 9
Training loss: 1.9321949481964111
Validation loss: 2.1935078021018737

Epoch: 5| Step: 10
Training loss: 2.5573577880859375
Validation loss: 2.184999235214726

Epoch: 351| Step: 0
Training loss: 2.658491611480713
Validation loss: 2.1878865213804346

Epoch: 5| Step: 1
Training loss: 2.4095091819763184
Validation loss: 2.1813072607081425

Epoch: 5| Step: 2
Training loss: 2.199936866760254
Validation loss: 2.1712762912114463

Epoch: 5| Step: 3
Training loss: 2.1523995399475098
Validation loss: 2.172589804536553

Epoch: 5| Step: 4
Training loss: 1.643307089805603
Validation loss: 2.1523344055298836

Epoch: 5| Step: 5
Training loss: 2.4056501388549805
Validation loss: 2.1717668451288694

Epoch: 5| Step: 6
Training loss: 2.1211445331573486
Validation loss: 2.1683106576242754

Epoch: 5| Step: 7
Training loss: 2.016552448272705
Validation loss: 2.188636126056794

Epoch: 5| Step: 8
Training loss: 2.9395668506622314
Validation loss: 2.2017165332712154

Epoch: 5| Step: 9
Training loss: 2.061495542526245
Validation loss: 2.2400003299918225

Epoch: 5| Step: 10
Training loss: 2.4236841201782227
Validation loss: 2.224036988391671

Epoch: 352| Step: 0
Training loss: 1.4524822235107422
Validation loss: 2.2191672120043027

Epoch: 5| Step: 1
Training loss: 2.5872981548309326
Validation loss: 2.208377677907226

Epoch: 5| Step: 2
Training loss: 2.6357645988464355
Validation loss: 2.209314582168415

Epoch: 5| Step: 3
Training loss: 2.3105854988098145
Validation loss: 2.1845029861696306

Epoch: 5| Step: 4
Training loss: 2.519303321838379
Validation loss: 2.1868078683012273

Epoch: 5| Step: 5
Training loss: 2.205350160598755
Validation loss: 2.159284991602744

Epoch: 5| Step: 6
Training loss: 2.2122840881347656
Validation loss: 2.1501313640225317

Epoch: 5| Step: 7
Training loss: 2.8503644466400146
Validation loss: 2.132935001004127

Epoch: 5| Step: 8
Training loss: 1.6233609914779663
Validation loss: 2.1489131322471042

Epoch: 5| Step: 9
Training loss: 2.536294460296631
Validation loss: 2.140106708772721

Epoch: 5| Step: 10
Training loss: 2.2533202171325684
Validation loss: 2.1452600673962663

Epoch: 353| Step: 0
Training loss: 1.8302723169326782
Validation loss: 2.1401313274137435

Epoch: 5| Step: 1
Training loss: 2.5841479301452637
Validation loss: 2.156550781701201

Epoch: 5| Step: 2
Training loss: 1.42168128490448
Validation loss: 2.1720212762073805

Epoch: 5| Step: 3
Training loss: 1.7931404113769531
Validation loss: 2.173298410190049

Epoch: 5| Step: 4
Training loss: 2.433032989501953
Validation loss: 2.190294865638979

Epoch: 5| Step: 5
Training loss: 2.2403016090393066
Validation loss: 2.1868923928148005

Epoch: 5| Step: 6
Training loss: 2.8152995109558105
Validation loss: 2.2055131504612584

Epoch: 5| Step: 7
Training loss: 2.3848328590393066
Validation loss: 2.212141049805508

Epoch: 5| Step: 8
Training loss: 2.6294829845428467
Validation loss: 2.195768651141915

Epoch: 5| Step: 9
Training loss: 2.5798964500427246
Validation loss: 2.1739083079881567

Epoch: 5| Step: 10
Training loss: 2.210866928100586
Validation loss: 2.1527659021398073

Epoch: 354| Step: 0
Training loss: 2.0105643272399902
Validation loss: 2.155431842291227

Epoch: 5| Step: 1
Training loss: 2.473607063293457
Validation loss: 2.1551920367825415

Epoch: 5| Step: 2
Training loss: 2.4367847442626953
Validation loss: 2.1482758124669394

Epoch: 5| Step: 3
Training loss: 2.044985055923462
Validation loss: 2.1588634521730485

Epoch: 5| Step: 4
Training loss: 2.035609006881714
Validation loss: 2.161776586245465

Epoch: 5| Step: 5
Training loss: 2.4755215644836426
Validation loss: 2.16516561021087

Epoch: 5| Step: 6
Training loss: 2.408731460571289
Validation loss: 2.1760033176791285

Epoch: 5| Step: 7
Training loss: 1.6314404010772705
Validation loss: 2.1842870558461835

Epoch: 5| Step: 8
Training loss: 2.3683903217315674
Validation loss: 2.1865313258222354

Epoch: 5| Step: 9
Training loss: 2.589982509613037
Validation loss: 2.1870742574814828

Epoch: 5| Step: 10
Training loss: 2.374913454055786
Validation loss: 2.179055827920155

Epoch: 355| Step: 0
Training loss: 2.30405855178833
Validation loss: 2.1710973785769556

Epoch: 5| Step: 1
Training loss: 2.1615586280822754
Validation loss: 2.1622505316170315

Epoch: 5| Step: 2
Training loss: 2.2438507080078125
Validation loss: 2.1721575016616494

Epoch: 5| Step: 3
Training loss: 2.3754806518554688
Validation loss: 2.156418337616869

Epoch: 5| Step: 4
Training loss: 2.1250157356262207
Validation loss: 2.145484075751356

Epoch: 5| Step: 5
Training loss: 2.7343649864196777
Validation loss: 2.145692225425474

Epoch: 5| Step: 6
Training loss: 2.471179485321045
Validation loss: 2.150722775408017

Epoch: 5| Step: 7
Training loss: 2.3609938621520996
Validation loss: 2.1566239172412502

Epoch: 5| Step: 8
Training loss: 2.112197160720825
Validation loss: 2.171977109806512

Epoch: 5| Step: 9
Training loss: 2.2866199016571045
Validation loss: 2.193955011265252

Epoch: 5| Step: 10
Training loss: 1.5942986011505127
Validation loss: 2.1957907779242403

Epoch: 356| Step: 0
Training loss: 1.9491182565689087
Validation loss: 2.1851779863398564

Epoch: 5| Step: 1
Training loss: 2.547016143798828
Validation loss: 2.1726635604776363

Epoch: 5| Step: 2
Training loss: 2.3100388050079346
Validation loss: 2.1888267071016374

Epoch: 5| Step: 3
Training loss: 2.5554122924804688
Validation loss: 2.1685939296599357

Epoch: 5| Step: 4
Training loss: 2.7217459678649902
Validation loss: 2.1626180346294115

Epoch: 5| Step: 5
Training loss: 1.818524718284607
Validation loss: 2.160526471753274

Epoch: 5| Step: 6
Training loss: 2.2363295555114746
Validation loss: 2.1535467896410214

Epoch: 5| Step: 7
Training loss: 2.4380621910095215
Validation loss: 2.1590522925059

Epoch: 5| Step: 8
Training loss: 1.9293848276138306
Validation loss: 2.161483513411655

Epoch: 5| Step: 9
Training loss: 1.860303282737732
Validation loss: 2.1595921311327206

Epoch: 5| Step: 10
Training loss: 2.3480284214019775
Validation loss: 2.164356857217768

Epoch: 357| Step: 0
Training loss: 1.7946770191192627
Validation loss: 2.191720821524179

Epoch: 5| Step: 1
Training loss: 2.4265124797821045
Validation loss: 2.1831922633673555

Epoch: 5| Step: 2
Training loss: 2.0975613594055176
Validation loss: 2.188599604432301

Epoch: 5| Step: 3
Training loss: 2.4186503887176514
Validation loss: 2.1884013324655514

Epoch: 5| Step: 4
Training loss: 2.470189332962036
Validation loss: 2.173462977973364

Epoch: 5| Step: 5
Training loss: 1.905416488647461
Validation loss: 2.171419688450393

Epoch: 5| Step: 6
Training loss: 2.0952062606811523
Validation loss: 2.156317028948056

Epoch: 5| Step: 7
Training loss: 2.348783254623413
Validation loss: 2.17040652613486

Epoch: 5| Step: 8
Training loss: 2.579472303390503
Validation loss: 2.168217743596723

Epoch: 5| Step: 9
Training loss: 2.633523941040039
Validation loss: 2.1773989508228917

Epoch: 5| Step: 10
Training loss: 1.8885971307754517
Validation loss: 2.1771926008244997

Epoch: 358| Step: 0
Training loss: 2.69783616065979
Validation loss: 2.2062716125160136

Epoch: 5| Step: 1
Training loss: 2.4176416397094727
Validation loss: 2.184785440403928

Epoch: 5| Step: 2
Training loss: 1.6093400716781616
Validation loss: 2.2006696090903333

Epoch: 5| Step: 3
Training loss: 1.8578712940216064
Validation loss: 2.1989811594768236

Epoch: 5| Step: 4
Training loss: 2.4323692321777344
Validation loss: 2.1843890605434293

Epoch: 5| Step: 5
Training loss: 2.2769389152526855
Validation loss: 2.1670900314084944

Epoch: 5| Step: 6
Training loss: 2.3767924308776855
Validation loss: 2.162084358994679

Epoch: 5| Step: 7
Training loss: 2.3532156944274902
Validation loss: 2.162468571816721

Epoch: 5| Step: 8
Training loss: 2.1768832206726074
Validation loss: 2.1640138446643786

Epoch: 5| Step: 9
Training loss: 2.283499240875244
Validation loss: 2.151339897545435

Epoch: 5| Step: 10
Training loss: 2.2571218013763428
Validation loss: 2.1552148826660646

Epoch: 359| Step: 0
Training loss: 2.590914726257324
Validation loss: 2.1449564720994685

Epoch: 5| Step: 1
Training loss: 2.3866727352142334
Validation loss: 2.1375533842271373

Epoch: 5| Step: 2
Training loss: 2.2015247344970703
Validation loss: 2.1321229063054568

Epoch: 5| Step: 3
Training loss: 2.1341419219970703
Validation loss: 2.124537726884247

Epoch: 5| Step: 4
Training loss: 2.2729039192199707
Validation loss: 2.143608909781261

Epoch: 5| Step: 5
Training loss: 1.7504043579101562
Validation loss: 2.1651566720777944

Epoch: 5| Step: 6
Training loss: 1.9122402667999268
Validation loss: 2.169937874681206

Epoch: 5| Step: 7
Training loss: 2.5575737953186035
Validation loss: 2.168102602804861

Epoch: 5| Step: 8
Training loss: 2.558189868927002
Validation loss: 2.180535534376739

Epoch: 5| Step: 9
Training loss: 2.3518362045288086
Validation loss: 2.1853478929047943

Epoch: 5| Step: 10
Training loss: 1.9872136116027832
Validation loss: 2.178998834343367

Epoch: 360| Step: 0
Training loss: 2.1639404296875
Validation loss: 2.1885683613438762

Epoch: 5| Step: 1
Training loss: 2.3136136531829834
Validation loss: 2.173607871096621

Epoch: 5| Step: 2
Training loss: 2.1241354942321777
Validation loss: 2.1586564843372633

Epoch: 5| Step: 3
Training loss: 2.1549439430236816
Validation loss: 2.1705868218534734

Epoch: 5| Step: 4
Training loss: 1.9349950551986694
Validation loss: 2.1635105789348645

Epoch: 5| Step: 5
Training loss: 2.4694488048553467
Validation loss: 2.1891005064851496

Epoch: 5| Step: 6
Training loss: 2.531146764755249
Validation loss: 2.1874548747975338

Epoch: 5| Step: 7
Training loss: 2.406303644180298
Validation loss: 2.227854092915853

Epoch: 5| Step: 8
Training loss: 2.398381471633911
Validation loss: 2.2041752787046534

Epoch: 5| Step: 9
Training loss: 2.0343093872070312
Validation loss: 2.193449392113634

Epoch: 5| Step: 10
Training loss: 2.289928436279297
Validation loss: 2.176736911137899

Epoch: 361| Step: 0
Training loss: 2.537271022796631
Validation loss: 2.14607633057461

Epoch: 5| Step: 1
Training loss: 2.203202486038208
Validation loss: 2.1417936407109743

Epoch: 5| Step: 2
Training loss: 2.135202646255493
Validation loss: 2.139587447207461

Epoch: 5| Step: 3
Training loss: 2.1701674461364746
Validation loss: 2.1467790578001287

Epoch: 5| Step: 4
Training loss: 2.307393789291382
Validation loss: 2.1281277953937487

Epoch: 5| Step: 5
Training loss: 1.527136206626892
Validation loss: 2.141606466744536

Epoch: 5| Step: 6
Training loss: 2.232635498046875
Validation loss: 2.1469840631690076

Epoch: 5| Step: 7
Training loss: 2.1853435039520264
Validation loss: 2.170456850400535

Epoch: 5| Step: 8
Training loss: 2.4020984172821045
Validation loss: 2.19242339493126

Epoch: 5| Step: 9
Training loss: 2.7220165729522705
Validation loss: 2.2205490219977593

Epoch: 5| Step: 10
Training loss: 2.353489398956299
Validation loss: 2.21846705098306

Epoch: 362| Step: 0
Training loss: 3.120723247528076
Validation loss: 2.2038121607995804

Epoch: 5| Step: 1
Training loss: 2.469011068344116
Validation loss: 2.179800642433987

Epoch: 5| Step: 2
Training loss: 2.0423591136932373
Validation loss: 2.1641692679415465

Epoch: 5| Step: 3
Training loss: 2.6258385181427
Validation loss: 2.137370353103966

Epoch: 5| Step: 4
Training loss: 2.2394871711730957
Validation loss: 2.148703608461606

Epoch: 5| Step: 5
Training loss: 2.718614101409912
Validation loss: 2.133197687005484

Epoch: 5| Step: 6
Training loss: 2.1965153217315674
Validation loss: 2.1293446812578427

Epoch: 5| Step: 7
Training loss: 2.3015480041503906
Validation loss: 2.151142035761187

Epoch: 5| Step: 8
Training loss: 1.945840835571289
Validation loss: 2.1470232176524338

Epoch: 5| Step: 9
Training loss: 1.6972736120224
Validation loss: 2.133244811847646

Epoch: 5| Step: 10
Training loss: 1.3909980058670044
Validation loss: 2.14378006996647

Epoch: 363| Step: 0
Training loss: 1.9521373510360718
Validation loss: 2.1522278349886657

Epoch: 5| Step: 1
Training loss: 1.579211711883545
Validation loss: 2.172916917390721

Epoch: 5| Step: 2
Training loss: 2.0578866004943848
Validation loss: 2.20820769956035

Epoch: 5| Step: 3
Training loss: 2.137962818145752
Validation loss: 2.201757954012963

Epoch: 5| Step: 4
Training loss: 2.6703288555145264
Validation loss: 2.1921123202129076

Epoch: 5| Step: 5
Training loss: 2.455538272857666
Validation loss: 2.178211727449971

Epoch: 5| Step: 6
Training loss: 2.266817808151245
Validation loss: 2.182620512541904

Epoch: 5| Step: 7
Training loss: 2.5588138103485107
Validation loss: 2.177928409268779

Epoch: 5| Step: 8
Training loss: 1.7717094421386719
Validation loss: 2.181061521653206

Epoch: 5| Step: 9
Training loss: 2.941267490386963
Validation loss: 2.171911213987617

Epoch: 5| Step: 10
Training loss: 2.1995599269866943
Validation loss: 2.160029608716247

Epoch: 364| Step: 0
Training loss: 2.6026721000671387
Validation loss: 2.1442926006932415

Epoch: 5| Step: 1
Training loss: 1.457106113433838
Validation loss: 2.1406293274253927

Epoch: 5| Step: 2
Training loss: 1.9152562618255615
Validation loss: 2.11952930496585

Epoch: 5| Step: 3
Training loss: 1.8101673126220703
Validation loss: 2.140112841001121

Epoch: 5| Step: 4
Training loss: 2.514085054397583
Validation loss: 2.144217694959333

Epoch: 5| Step: 5
Training loss: 2.75974440574646
Validation loss: 2.16033955671454

Epoch: 5| Step: 6
Training loss: 1.9577713012695312
Validation loss: 2.174873698142267

Epoch: 5| Step: 7
Training loss: 2.4191958904266357
Validation loss: 2.1880671542177916

Epoch: 5| Step: 8
Training loss: 2.1240148544311523
Validation loss: 2.1905469586772304

Epoch: 5| Step: 9
Training loss: 2.5813870429992676
Validation loss: 2.1912412207613707

Epoch: 5| Step: 10
Training loss: 2.6791768074035645
Validation loss: 2.1962812151960147

Epoch: 365| Step: 0
Training loss: 2.599990129470825
Validation loss: 2.1770781137609996

Epoch: 5| Step: 1
Training loss: 2.3418641090393066
Validation loss: 2.1733581327622935

Epoch: 5| Step: 2
Training loss: 2.496295213699341
Validation loss: 2.185534300342683

Epoch: 5| Step: 3
Training loss: 2.0311026573181152
Validation loss: 2.1851157219179216

Epoch: 5| Step: 4
Training loss: 1.958255410194397
Validation loss: 2.154788860710718

Epoch: 5| Step: 5
Training loss: 2.8433518409729004
Validation loss: 2.1603351741708736

Epoch: 5| Step: 6
Training loss: 2.5298779010772705
Validation loss: 2.1540336788341565

Epoch: 5| Step: 7
Training loss: 2.2737843990325928
Validation loss: 2.1442245155252437

Epoch: 5| Step: 8
Training loss: 1.7269500494003296
Validation loss: 2.164912223815918

Epoch: 5| Step: 9
Training loss: 1.9046733379364014
Validation loss: 2.1896811518617856

Epoch: 5| Step: 10
Training loss: 2.1541082859039307
Validation loss: 2.1911260517694617

Epoch: 366| Step: 0
Training loss: 2.542346239089966
Validation loss: 2.2086936068791214

Epoch: 5| Step: 1
Training loss: 2.1956610679626465
Validation loss: 2.1962736652743433

Epoch: 5| Step: 2
Training loss: 2.0085530281066895
Validation loss: 2.166431289847179

Epoch: 5| Step: 3
Training loss: 2.452394962310791
Validation loss: 2.134594140514251

Epoch: 5| Step: 4
Training loss: 1.9714679718017578
Validation loss: 2.121132464819057

Epoch: 5| Step: 5
Training loss: 2.487921953201294
Validation loss: 2.114476211609379

Epoch: 5| Step: 6
Training loss: 2.082563877105713
Validation loss: 2.137993453651346

Epoch: 5| Step: 7
Training loss: 1.9084856510162354
Validation loss: 2.1372219721476235

Epoch: 5| Step: 8
Training loss: 1.780940055847168
Validation loss: 2.148388170426892

Epoch: 5| Step: 9
Training loss: 2.647958517074585
Validation loss: 2.1582496909685034

Epoch: 5| Step: 10
Training loss: 2.813530683517456
Validation loss: 2.176476127357893

Epoch: 367| Step: 0
Training loss: 2.074559450149536
Validation loss: 2.1714456260845227

Epoch: 5| Step: 1
Training loss: 2.1635329723358154
Validation loss: 2.1678240273588445

Epoch: 5| Step: 2
Training loss: 1.6890738010406494
Validation loss: 2.180828297010032

Epoch: 5| Step: 3
Training loss: 2.4397075176239014
Validation loss: 2.1670313727471138

Epoch: 5| Step: 4
Training loss: 2.525205612182617
Validation loss: 2.152235000364242

Epoch: 5| Step: 5
Training loss: 2.6962647438049316
Validation loss: 2.1622119629254906

Epoch: 5| Step: 6
Training loss: 2.0174989700317383
Validation loss: 2.163335472024897

Epoch: 5| Step: 7
Training loss: 2.266806125640869
Validation loss: 2.1585017801612936

Epoch: 5| Step: 8
Training loss: 2.2540125846862793
Validation loss: 2.166405282994752

Epoch: 5| Step: 9
Training loss: 1.6993625164031982
Validation loss: 2.1586727762735016

Epoch: 5| Step: 10
Training loss: 2.5920560359954834
Validation loss: 2.160112960364229

Epoch: 368| Step: 0
Training loss: 1.9719728231430054
Validation loss: 2.1705613623383226

Epoch: 5| Step: 1
Training loss: 1.8530819416046143
Validation loss: 2.168473946150913

Epoch: 5| Step: 2
Training loss: 2.6074721813201904
Validation loss: 2.1907136824823197

Epoch: 5| Step: 3
Training loss: 2.850891590118408
Validation loss: 2.1934081098084808

Epoch: 5| Step: 4
Training loss: 2.973104953765869
Validation loss: 2.184978964508221

Epoch: 5| Step: 5
Training loss: 2.38651442527771
Validation loss: 2.184430797894796

Epoch: 5| Step: 6
Training loss: 1.9422321319580078
Validation loss: 2.175394847828855

Epoch: 5| Step: 7
Training loss: 1.529422402381897
Validation loss: 2.1687307216787852

Epoch: 5| Step: 8
Training loss: 1.8626512289047241
Validation loss: 2.17275934321906

Epoch: 5| Step: 9
Training loss: 2.3836231231689453
Validation loss: 2.1730361702621623

Epoch: 5| Step: 10
Training loss: 2.102079391479492
Validation loss: 2.1776179267514135

Epoch: 369| Step: 0
Training loss: 2.2573397159576416
Validation loss: 2.184197659133583

Epoch: 5| Step: 1
Training loss: 2.177088499069214
Validation loss: 2.1763057554921796

Epoch: 5| Step: 2
Training loss: 2.598313093185425
Validation loss: 2.1714301186223186

Epoch: 5| Step: 3
Training loss: 2.204923152923584
Validation loss: 2.1580781295735347

Epoch: 5| Step: 4
Training loss: 2.0245232582092285
Validation loss: 2.1647035562863914

Epoch: 5| Step: 5
Training loss: 2.130788803100586
Validation loss: 2.1599707872636857

Epoch: 5| Step: 6
Training loss: 2.6229944229125977
Validation loss: 2.1580970479596044

Epoch: 5| Step: 7
Training loss: 2.364269495010376
Validation loss: 2.155974697041255

Epoch: 5| Step: 8
Training loss: 2.335986375808716
Validation loss: 2.166732521467311

Epoch: 5| Step: 9
Training loss: 2.3242592811584473
Validation loss: 2.1617097162431285

Epoch: 5| Step: 10
Training loss: 1.2600631713867188
Validation loss: 2.1744764953531246

Epoch: 370| Step: 0
Training loss: 2.7474374771118164
Validation loss: 2.1687289386667232

Epoch: 5| Step: 1
Training loss: 1.946021318435669
Validation loss: 2.1598288423271588

Epoch: 5| Step: 2
Training loss: 2.5337471961975098
Validation loss: 2.1760189353778796

Epoch: 5| Step: 3
Training loss: 1.6747757196426392
Validation loss: 2.1978223195639988

Epoch: 5| Step: 4
Training loss: 1.9757325649261475
Validation loss: 2.203201106799546

Epoch: 5| Step: 5
Training loss: 2.172593593597412
Validation loss: 2.20379977585167

Epoch: 5| Step: 6
Training loss: 2.8857338428497314
Validation loss: 2.1788925150389313

Epoch: 5| Step: 7
Training loss: 1.4042524099349976
Validation loss: 2.1806778933412287

Epoch: 5| Step: 8
Training loss: 2.447981357574463
Validation loss: 2.1891621953697613

Epoch: 5| Step: 9
Training loss: 2.3232245445251465
Validation loss: 2.198583246559225

Epoch: 5| Step: 10
Training loss: 2.277228593826294
Validation loss: 2.1871118340440976

Epoch: 371| Step: 0
Training loss: 1.995566725730896
Validation loss: 2.1852013000877957

Epoch: 5| Step: 1
Training loss: 3.0717413425445557
Validation loss: 2.1516398024815384

Epoch: 5| Step: 2
Training loss: 1.9615936279296875
Validation loss: 2.1258113127882763

Epoch: 5| Step: 3
Training loss: 2.123260021209717
Validation loss: 2.1072043090738277

Epoch: 5| Step: 4
Training loss: 1.6535860300064087
Validation loss: 2.1150666641932663

Epoch: 5| Step: 5
Training loss: 1.6901248693466187
Validation loss: 2.123162713102115

Epoch: 5| Step: 6
Training loss: 2.234038829803467
Validation loss: 2.1312114166957077

Epoch: 5| Step: 7
Training loss: 3.3304145336151123
Validation loss: 2.144624828010477

Epoch: 5| Step: 8
Training loss: 2.3061187267303467
Validation loss: 2.149312319294099

Epoch: 5| Step: 9
Training loss: 2.2903523445129395
Validation loss: 2.156472272770379

Epoch: 5| Step: 10
Training loss: 1.8935246467590332
Validation loss: 2.1519734795375536

Epoch: 372| Step: 0
Training loss: 1.9887974262237549
Validation loss: 2.152308443541168

Epoch: 5| Step: 1
Training loss: 3.194291591644287
Validation loss: 2.13796615985132

Epoch: 5| Step: 2
Training loss: 2.0555992126464844
Validation loss: 2.1457196461257113

Epoch: 5| Step: 3
Training loss: 2.5863735675811768
Validation loss: 2.14366336535382

Epoch: 5| Step: 4
Training loss: 1.9971168041229248
Validation loss: 2.1377973607791367

Epoch: 5| Step: 5
Training loss: 1.7430312633514404
Validation loss: 2.137627301677581

Epoch: 5| Step: 6
Training loss: 2.5975146293640137
Validation loss: 2.1308026006144862

Epoch: 5| Step: 7
Training loss: 2.246539831161499
Validation loss: 2.144153295024749

Epoch: 5| Step: 8
Training loss: 0.976601779460907
Validation loss: 2.1558649001582975

Epoch: 5| Step: 9
Training loss: 2.5234427452087402
Validation loss: 2.1608807271526707

Epoch: 5| Step: 10
Training loss: 2.3908920288085938
Validation loss: 2.173270753634873

Epoch: 373| Step: 0
Training loss: 2.1171836853027344
Validation loss: 2.1941097551776516

Epoch: 5| Step: 1
Training loss: 2.2288432121276855
Validation loss: 2.2268310362292874

Epoch: 5| Step: 2
Training loss: 2.595261573791504
Validation loss: 2.2159750000123055

Epoch: 5| Step: 3
Training loss: 1.6612564325332642
Validation loss: 2.2144066377352645

Epoch: 5| Step: 4
Training loss: 1.5086135864257812
Validation loss: 2.186630433605563

Epoch: 5| Step: 5
Training loss: 2.496896743774414
Validation loss: 2.1766003601012693

Epoch: 5| Step: 6
Training loss: 2.067091703414917
Validation loss: 2.1699736131134855

Epoch: 5| Step: 7
Training loss: 2.77746319770813
Validation loss: 2.1698036911667034

Epoch: 5| Step: 8
Training loss: 2.8571228981018066
Validation loss: 2.160088651923723

Epoch: 5| Step: 9
Training loss: 1.9577548503875732
Validation loss: 2.1660847663879395

Epoch: 5| Step: 10
Training loss: 2.1323466300964355
Validation loss: 2.15744972485368

Epoch: 374| Step: 0
Training loss: 1.9479033946990967
Validation loss: 2.175300731453844

Epoch: 5| Step: 1
Training loss: 1.9263088703155518
Validation loss: 2.1933553475205616

Epoch: 5| Step: 2
Training loss: 2.122786283493042
Validation loss: 2.2100014763493694

Epoch: 5| Step: 3
Training loss: 2.262457847595215
Validation loss: 2.210592785189229

Epoch: 5| Step: 4
Training loss: 2.2518458366394043
Validation loss: 2.182271162668864

Epoch: 5| Step: 5
Training loss: 1.6541703939437866
Validation loss: 2.1614646398892967

Epoch: 5| Step: 6
Training loss: 2.1420583724975586
Validation loss: 2.135091056105911

Epoch: 5| Step: 7
Training loss: 2.7164034843444824
Validation loss: 2.1344607542919856

Epoch: 5| Step: 8
Training loss: 2.1973133087158203
Validation loss: 2.1399948366226687

Epoch: 5| Step: 9
Training loss: 2.3594021797180176
Validation loss: 2.1351446823407243

Epoch: 5| Step: 10
Training loss: 3.0351762771606445
Validation loss: 2.1455519301916963

Epoch: 375| Step: 0
Training loss: 2.468486785888672
Validation loss: 2.1606828397320164

Epoch: 5| Step: 1
Training loss: 2.437267303466797
Validation loss: 2.166373537432763

Epoch: 5| Step: 2
Training loss: 2.1339094638824463
Validation loss: 2.1527751902098298

Epoch: 5| Step: 3
Training loss: 2.0526185035705566
Validation loss: 2.1657220650744695

Epoch: 5| Step: 4
Training loss: 1.9939266443252563
Validation loss: 2.177996086817916

Epoch: 5| Step: 5
Training loss: 2.000239133834839
Validation loss: 2.168196852489184

Epoch: 5| Step: 6
Training loss: 2.2940680980682373
Validation loss: 2.194712649109543

Epoch: 5| Step: 7
Training loss: 2.0892698764801025
Validation loss: 2.203828360444756

Epoch: 5| Step: 8
Training loss: 2.350137710571289
Validation loss: 2.2078928229629353

Epoch: 5| Step: 9
Training loss: 2.2924442291259766
Validation loss: 2.2075745854326474

Epoch: 5| Step: 10
Training loss: 2.110628128051758
Validation loss: 2.1867109011578303

Epoch: 376| Step: 0
Training loss: 2.2861275672912598
Validation loss: 2.1685955062989266

Epoch: 5| Step: 1
Training loss: 2.4407382011413574
Validation loss: 2.1476241516810592

Epoch: 5| Step: 2
Training loss: 2.1438145637512207
Validation loss: 2.1431248521292083

Epoch: 5| Step: 3
Training loss: 2.1142849922180176
Validation loss: 2.141387403652232

Epoch: 5| Step: 4
Training loss: 2.373338222503662
Validation loss: 2.126973003469488

Epoch: 5| Step: 5
Training loss: 3.1313376426696777
Validation loss: 2.114738984774518

Epoch: 5| Step: 6
Training loss: 1.6982682943344116
Validation loss: 2.105155175732028

Epoch: 5| Step: 7
Training loss: 2.2394261360168457
Validation loss: 2.1208995260218138

Epoch: 5| Step: 8
Training loss: 2.140012502670288
Validation loss: 2.1160945277060232

Epoch: 5| Step: 9
Training loss: 1.8231468200683594
Validation loss: 2.149601831231066

Epoch: 5| Step: 10
Training loss: 2.00824236869812
Validation loss: 2.154604010684516

Epoch: 377| Step: 0
Training loss: 2.9518277645111084
Validation loss: 2.1752872877223517

Epoch: 5| Step: 1
Training loss: 2.0548717975616455
Validation loss: 2.194500514256057

Epoch: 5| Step: 2
Training loss: 2.41654896736145
Validation loss: 2.2216903522450435

Epoch: 5| Step: 3
Training loss: 2.2300798892974854
Validation loss: 2.18633093372468

Epoch: 5| Step: 4
Training loss: 1.7096703052520752
Validation loss: 2.1779440833676245

Epoch: 5| Step: 5
Training loss: 2.0757741928100586
Validation loss: 2.153385813518237

Epoch: 5| Step: 6
Training loss: 2.4232404232025146
Validation loss: 2.152584522001205

Epoch: 5| Step: 7
Training loss: 2.158338785171509
Validation loss: 2.1374655385171213

Epoch: 5| Step: 8
Training loss: 2.2474193572998047
Validation loss: 2.1263712683031635

Epoch: 5| Step: 9
Training loss: 1.9273288249969482
Validation loss: 2.1209580590648036

Epoch: 5| Step: 10
Training loss: 2.24528169631958
Validation loss: 2.144141080558941

Epoch: 378| Step: 0
Training loss: 2.267043113708496
Validation loss: 2.168271877432382

Epoch: 5| Step: 1
Training loss: 2.392883777618408
Validation loss: 2.176419137626566

Epoch: 5| Step: 2
Training loss: 2.159403085708618
Validation loss: 2.2010871748770438

Epoch: 5| Step: 3
Training loss: 1.96492600440979
Validation loss: 2.2110879203324676

Epoch: 5| Step: 4
Training loss: 2.2266950607299805
Validation loss: 2.194121185169425

Epoch: 5| Step: 5
Training loss: 2.352646589279175
Validation loss: 2.172292588859476

Epoch: 5| Step: 6
Training loss: 2.362877607345581
Validation loss: 2.161179224650065

Epoch: 5| Step: 7
Training loss: 2.0751967430114746
Validation loss: 2.1654867638823805

Epoch: 5| Step: 8
Training loss: 2.0442073345184326
Validation loss: 2.1582129873255247

Epoch: 5| Step: 9
Training loss: 2.2171785831451416
Validation loss: 2.1645396601769233

Epoch: 5| Step: 10
Training loss: 2.0937185287475586
Validation loss: 2.172611521136376

Epoch: 379| Step: 0
Training loss: 3.013381242752075
Validation loss: 2.155824012653802

Epoch: 5| Step: 1
Training loss: 1.918440580368042
Validation loss: 2.147168367139755

Epoch: 5| Step: 2
Training loss: 2.2433950901031494
Validation loss: 2.1497614332424697

Epoch: 5| Step: 3
Training loss: 1.6250979900360107
Validation loss: 2.142882236870386

Epoch: 5| Step: 4
Training loss: 1.9814987182617188
Validation loss: 2.1522748060123895

Epoch: 5| Step: 5
Training loss: 2.6025309562683105
Validation loss: 2.1521895662430794

Epoch: 5| Step: 6
Training loss: 1.9665310382843018
Validation loss: 2.144875849446943

Epoch: 5| Step: 7
Training loss: 1.6068115234375
Validation loss: 2.1409270071214244

Epoch: 5| Step: 8
Training loss: 2.6416053771972656
Validation loss: 2.184950613206433

Epoch: 5| Step: 9
Training loss: 1.983809471130371
Validation loss: 2.193587609516677

Epoch: 5| Step: 10
Training loss: 2.682180404663086
Validation loss: 2.1730650830012497

Epoch: 380| Step: 0
Training loss: 2.2786922454833984
Validation loss: 2.1641054409806446

Epoch: 5| Step: 1
Training loss: 1.577785849571228
Validation loss: 2.1653734842936196

Epoch: 5| Step: 2
Training loss: 2.5422158241271973
Validation loss: 2.176450146141873

Epoch: 5| Step: 3
Training loss: 2.4756674766540527
Validation loss: 2.180495777437764

Epoch: 5| Step: 4
Training loss: 1.6533819437026978
Validation loss: 2.2001888162346295

Epoch: 5| Step: 5
Training loss: 2.0150370597839355
Validation loss: 2.2100113130384877

Epoch: 5| Step: 6
Training loss: 2.1560654640197754
Validation loss: 2.2318422204704693

Epoch: 5| Step: 7
Training loss: 2.8693318367004395
Validation loss: 2.21139927833311

Epoch: 5| Step: 8
Training loss: 2.3066420555114746
Validation loss: 2.196300250227733

Epoch: 5| Step: 9
Training loss: 2.7646217346191406
Validation loss: 2.1683432825150026

Epoch: 5| Step: 10
Training loss: 1.8486738204956055
Validation loss: 2.1604871621695896

Epoch: 381| Step: 0
Training loss: 2.4260313510894775
Validation loss: 2.1477353265208583

Epoch: 5| Step: 1
Training loss: 2.128469228744507
Validation loss: 2.1745118441120272

Epoch: 5| Step: 2
Training loss: 2.0370020866394043
Validation loss: 2.1772029322962605

Epoch: 5| Step: 3
Training loss: 1.9134546518325806
Validation loss: 2.157024439945016

Epoch: 5| Step: 4
Training loss: 2.8764193058013916
Validation loss: 2.153119728129397

Epoch: 5| Step: 5
Training loss: 2.1057486534118652
Validation loss: 2.1282633376377884

Epoch: 5| Step: 6
Training loss: 2.2024004459381104
Validation loss: 2.104125884271437

Epoch: 5| Step: 7
Training loss: 2.3621177673339844
Validation loss: 2.086411979890639

Epoch: 5| Step: 8
Training loss: 2.225067615509033
Validation loss: 2.089644394895082

Epoch: 5| Step: 9
Training loss: 2.2452383041381836
Validation loss: 2.0885375699689313

Epoch: 5| Step: 10
Training loss: 2.0634801387786865
Validation loss: 2.092607452023414

Epoch: 382| Step: 0
Training loss: 2.5136094093322754
Validation loss: 2.1048101943026305

Epoch: 5| Step: 1
Training loss: 2.182687282562256
Validation loss: 2.0986816831814346

Epoch: 5| Step: 2
Training loss: 2.4231748580932617
Validation loss: 2.1153164038094143

Epoch: 5| Step: 3
Training loss: 1.930762529373169
Validation loss: 2.1239640071827877

Epoch: 5| Step: 4
Training loss: 1.880832314491272
Validation loss: 2.136897148624543

Epoch: 5| Step: 5
Training loss: 2.1664071083068848
Validation loss: 2.1807975961316015

Epoch: 5| Step: 6
Training loss: 2.48419189453125
Validation loss: 2.1829015337010866

Epoch: 5| Step: 7
Training loss: 1.9075191020965576
Validation loss: 2.182687759399414

Epoch: 5| Step: 8
Training loss: 1.9491583108901978
Validation loss: 2.1630260970002864

Epoch: 5| Step: 9
Training loss: 2.8266913890838623
Validation loss: 2.157091822675479

Epoch: 5| Step: 10
Training loss: 2.116689682006836
Validation loss: 2.1345513456611225

Epoch: 383| Step: 0
Training loss: 2.371814250946045
Validation loss: 2.1307016213734946

Epoch: 5| Step: 1
Training loss: 2.010166645050049
Validation loss: 2.1257257717911915

Epoch: 5| Step: 2
Training loss: 2.122161626815796
Validation loss: 2.104673170274304

Epoch: 5| Step: 3
Training loss: 2.363553762435913
Validation loss: 2.1189014514287314

Epoch: 5| Step: 4
Training loss: 2.26405668258667
Validation loss: 2.117239752123433

Epoch: 5| Step: 5
Training loss: 2.651604175567627
Validation loss: 2.110101648556289

Epoch: 5| Step: 6
Training loss: 2.646981716156006
Validation loss: 2.131622361880477

Epoch: 5| Step: 7
Training loss: 1.2047381401062012
Validation loss: 2.123890592205909

Epoch: 5| Step: 8
Training loss: 1.8853687047958374
Validation loss: 2.1455078817182973

Epoch: 5| Step: 9
Training loss: 2.6765990257263184
Validation loss: 2.1714152956521637

Epoch: 5| Step: 10
Training loss: 2.077646255493164
Validation loss: 2.207020531418503

Epoch: 384| Step: 0
Training loss: 1.9009761810302734
Validation loss: 2.224377227085893

Epoch: 5| Step: 1
Training loss: 2.303109645843506
Validation loss: 2.200496389019874

Epoch: 5| Step: 2
Training loss: 1.8601993322372437
Validation loss: 2.19321810301914

Epoch: 5| Step: 3
Training loss: 2.8235630989074707
Validation loss: 2.174958498247208

Epoch: 5| Step: 4
Training loss: 3.0083162784576416
Validation loss: 2.170021095583516

Epoch: 5| Step: 5
Training loss: 2.248176336288452
Validation loss: 2.160825434551444

Epoch: 5| Step: 6
Training loss: 2.175938367843628
Validation loss: 2.150771946035406

Epoch: 5| Step: 7
Training loss: 2.3492114543914795
Validation loss: 2.1393853131160943

Epoch: 5| Step: 8
Training loss: 1.2624108791351318
Validation loss: 2.142330356823501

Epoch: 5| Step: 9
Training loss: 1.9962358474731445
Validation loss: 2.1456217714535293

Epoch: 5| Step: 10
Training loss: 2.436922311782837
Validation loss: 2.1492442392533824

Epoch: 385| Step: 0
Training loss: 1.9093101024627686
Validation loss: 2.1408393844481437

Epoch: 5| Step: 1
Training loss: 2.3689301013946533
Validation loss: 2.165731660781368

Epoch: 5| Step: 2
Training loss: 2.839402198791504
Validation loss: 2.1706565169877905

Epoch: 5| Step: 3
Training loss: 2.1501574516296387
Validation loss: 2.17069617009932

Epoch: 5| Step: 4
Training loss: 1.7127265930175781
Validation loss: 2.1817872549897883

Epoch: 5| Step: 5
Training loss: 2.1661734580993652
Validation loss: 2.198382023842104

Epoch: 5| Step: 6
Training loss: 2.186474323272705
Validation loss: 2.1956722556903796

Epoch: 5| Step: 7
Training loss: 2.298480749130249
Validation loss: 2.219914677322552

Epoch: 5| Step: 8
Training loss: 1.849657416343689
Validation loss: 2.1975804605791645

Epoch: 5| Step: 9
Training loss: 2.7445812225341797
Validation loss: 2.165769274516772

Epoch: 5| Step: 10
Training loss: 1.672958254814148
Validation loss: 2.151639005189301

Epoch: 386| Step: 0
Training loss: 2.0520195960998535
Validation loss: 2.136438549205821

Epoch: 5| Step: 1
Training loss: 1.8928887844085693
Validation loss: 2.1166718262498097

Epoch: 5| Step: 2
Training loss: 2.3402187824249268
Validation loss: 2.12466626269843

Epoch: 5| Step: 3
Training loss: 2.3223862648010254
Validation loss: 2.137200565748317

Epoch: 5| Step: 4
Training loss: 2.336683511734009
Validation loss: 2.1181422971910044

Epoch: 5| Step: 5
Training loss: 1.7861509323120117
Validation loss: 2.1394535341570453

Epoch: 5| Step: 6
Training loss: 2.3899378776550293
Validation loss: 2.1574040958958287

Epoch: 5| Step: 7
Training loss: 2.287722110748291
Validation loss: 2.1719684754648516

Epoch: 5| Step: 8
Training loss: 2.4799888134002686
Validation loss: 2.201520791617773

Epoch: 5| Step: 9
Training loss: 2.1969716548919678
Validation loss: 2.212638678089265

Epoch: 5| Step: 10
Training loss: 1.8077032566070557
Validation loss: 2.2336022482123425

Epoch: 387| Step: 0
Training loss: 1.9087011814117432
Validation loss: 2.183827063088776

Epoch: 5| Step: 1
Training loss: 2.348414659500122
Validation loss: 2.195827517458188

Epoch: 5| Step: 2
Training loss: 1.9993622303009033
Validation loss: 2.1906169819575485

Epoch: 5| Step: 3
Training loss: 1.9033539295196533
Validation loss: 2.1977323383413334

Epoch: 5| Step: 4
Training loss: 2.028156280517578
Validation loss: 2.178707309948501

Epoch: 5| Step: 5
Training loss: 2.6200480461120605
Validation loss: 2.1637692592477284

Epoch: 5| Step: 6
Training loss: 2.8727526664733887
Validation loss: 2.166179426254765

Epoch: 5| Step: 7
Training loss: 2.2933743000030518
Validation loss: 2.1430552262131886

Epoch: 5| Step: 8
Training loss: 2.1172471046447754
Validation loss: 2.1362632743773924

Epoch: 5| Step: 9
Training loss: 1.9302947521209717
Validation loss: 2.1121732265718522

Epoch: 5| Step: 10
Training loss: 1.8888837099075317
Validation loss: 2.1243762405969764

Epoch: 388| Step: 0
Training loss: 2.7823920249938965
Validation loss: 2.1248878843040875

Epoch: 5| Step: 1
Training loss: 2.038640260696411
Validation loss: 2.115697204425771

Epoch: 5| Step: 2
Training loss: 2.1697235107421875
Validation loss: 2.137615765294721

Epoch: 5| Step: 3
Training loss: 2.133841037750244
Validation loss: 2.1662050267701507

Epoch: 5| Step: 4
Training loss: 1.6255252361297607
Validation loss: 2.186712844397432

Epoch: 5| Step: 5
Training loss: 2.105189800262451
Validation loss: 2.2248977512441654

Epoch: 5| Step: 6
Training loss: 2.1010704040527344
Validation loss: 2.2009166427837905

Epoch: 5| Step: 7
Training loss: 2.3523130416870117
Validation loss: 2.2010621998899724

Epoch: 5| Step: 8
Training loss: 2.2898967266082764
Validation loss: 2.1737988354057394

Epoch: 5| Step: 9
Training loss: 2.371346950531006
Validation loss: 2.1534102424498527

Epoch: 5| Step: 10
Training loss: 2.131331205368042
Validation loss: 2.1442312066273024

Epoch: 389| Step: 0
Training loss: 1.7494208812713623
Validation loss: 2.1524786231338338

Epoch: 5| Step: 1
Training loss: 2.6752686500549316
Validation loss: 2.1284645167730187

Epoch: 5| Step: 2
Training loss: 1.674133062362671
Validation loss: 2.126045298832719

Epoch: 5| Step: 3
Training loss: 1.7783550024032593
Validation loss: 2.129271390617535

Epoch: 5| Step: 4
Training loss: 2.4821066856384277
Validation loss: 2.132884419092568

Epoch: 5| Step: 5
Training loss: 2.505182981491089
Validation loss: 2.1371866041614163

Epoch: 5| Step: 6
Training loss: 1.6334953308105469
Validation loss: 2.133496399848692

Epoch: 5| Step: 7
Training loss: 1.9728405475616455
Validation loss: 2.1544135937126736

Epoch: 5| Step: 8
Training loss: 2.114922523498535
Validation loss: 2.156835404775476

Epoch: 5| Step: 9
Training loss: 2.678628921508789
Validation loss: 2.146994626650246

Epoch: 5| Step: 10
Training loss: 2.696112871170044
Validation loss: 2.144816542184481

Epoch: 390| Step: 0
Training loss: 2.4094905853271484
Validation loss: 2.159225445921703

Epoch: 5| Step: 1
Training loss: 1.89650559425354
Validation loss: 2.1699392564835085

Epoch: 5| Step: 2
Training loss: 2.066305637359619
Validation loss: 2.155657258085025

Epoch: 5| Step: 3
Training loss: 2.214366912841797
Validation loss: 2.162845775645266

Epoch: 5| Step: 4
Training loss: 2.3673248291015625
Validation loss: 2.1498682063112975

Epoch: 5| Step: 5
Training loss: 2.4628701210021973
Validation loss: 2.155180426054103

Epoch: 5| Step: 6
Training loss: 2.4116756916046143
Validation loss: 2.156721786786151

Epoch: 5| Step: 7
Training loss: 2.344081401824951
Validation loss: 2.140565356900615

Epoch: 5| Step: 8
Training loss: 1.849635124206543
Validation loss: 2.1415695631375877

Epoch: 5| Step: 9
Training loss: 1.961681604385376
Validation loss: 2.1413088690850044

Epoch: 5| Step: 10
Training loss: 1.6840912103652954
Validation loss: 2.1472870483193347

Epoch: 391| Step: 0
Training loss: 2.5874438285827637
Validation loss: 2.1492851652124876

Epoch: 5| Step: 1
Training loss: 1.4499318599700928
Validation loss: 2.1486550172170005

Epoch: 5| Step: 2
Training loss: 1.8646934032440186
Validation loss: 2.183303463843561

Epoch: 5| Step: 3
Training loss: 2.5084800720214844
Validation loss: 2.1835560747372207

Epoch: 5| Step: 4
Training loss: 1.951025366783142
Validation loss: 2.1862376800147434

Epoch: 5| Step: 5
Training loss: 2.128693103790283
Validation loss: 2.18698751413694

Epoch: 5| Step: 6
Training loss: 2.1563174724578857
Validation loss: 2.180959806647352

Epoch: 5| Step: 7
Training loss: 2.6152729988098145
Validation loss: 2.1644570878756944

Epoch: 5| Step: 8
Training loss: 2.378342866897583
Validation loss: 2.1522854592210505

Epoch: 5| Step: 9
Training loss: 2.145181179046631
Validation loss: 2.151370347187083

Epoch: 5| Step: 10
Training loss: 1.9330543279647827
Validation loss: 2.13878720806491

Epoch: 392| Step: 0
Training loss: 2.38535475730896
Validation loss: 2.138374916968807

Epoch: 5| Step: 1
Training loss: 2.213240385055542
Validation loss: 2.127288579940796

Epoch: 5| Step: 2
Training loss: 2.3991453647613525
Validation loss: 2.129988334512198

Epoch: 5| Step: 3
Training loss: 2.178077459335327
Validation loss: 2.125975670353059

Epoch: 5| Step: 4
Training loss: 2.3065805435180664
Validation loss: 2.115197052237808

Epoch: 5| Step: 5
Training loss: 1.7045071125030518
Validation loss: 2.1248440127218924

Epoch: 5| Step: 6
Training loss: 2.1150879859924316
Validation loss: 2.130764380578072

Epoch: 5| Step: 7
Training loss: 1.8723621368408203
Validation loss: 2.1464580515379548

Epoch: 5| Step: 8
Training loss: 2.26456880569458
Validation loss: 2.1806815465291343

Epoch: 5| Step: 9
Training loss: 2.2622122764587402
Validation loss: 2.1874284128988943

Epoch: 5| Step: 10
Training loss: 2.104417324066162
Validation loss: 2.183672699877011

Epoch: 393| Step: 0
Training loss: 1.5556730031967163
Validation loss: 2.1946042250561457

Epoch: 5| Step: 1
Training loss: 1.9538986682891846
Validation loss: 2.17612559564652

Epoch: 5| Step: 2
Training loss: 1.829947829246521
Validation loss: 2.1773100591474965

Epoch: 5| Step: 3
Training loss: 2.365875005722046
Validation loss: 2.187645027714391

Epoch: 5| Step: 4
Training loss: 3.020660877227783
Validation loss: 2.1753827730814614

Epoch: 5| Step: 5
Training loss: 1.6937373876571655
Validation loss: 2.1617160535627797

Epoch: 5| Step: 6
Training loss: 1.9516394138336182
Validation loss: 2.1529117527828423

Epoch: 5| Step: 7
Training loss: 1.5961674451828003
Validation loss: 2.1380244057665587

Epoch: 5| Step: 8
Training loss: 2.270132541656494
Validation loss: 2.1393215194825204

Epoch: 5| Step: 9
Training loss: 2.331047534942627
Validation loss: 2.116599670020483

Epoch: 5| Step: 10
Training loss: 3.359184980392456
Validation loss: 2.1222594425242436

Epoch: 394| Step: 0
Training loss: 2.435100793838501
Validation loss: 2.140422769772109

Epoch: 5| Step: 1
Training loss: 2.276995897293091
Validation loss: 2.127572823596257

Epoch: 5| Step: 2
Training loss: 2.7484612464904785
Validation loss: 2.1310041258412022

Epoch: 5| Step: 3
Training loss: 1.925782561302185
Validation loss: 2.1486844247387302

Epoch: 5| Step: 4
Training loss: 1.7707229852676392
Validation loss: 2.155733013665804

Epoch: 5| Step: 5
Training loss: 2.278305768966675
Validation loss: 2.143476060641709

Epoch: 5| Step: 6
Training loss: 1.7548210620880127
Validation loss: 2.15606197234123

Epoch: 5| Step: 7
Training loss: 2.642594575881958
Validation loss: 2.14664440513939

Epoch: 5| Step: 8
Training loss: 1.8486629724502563
Validation loss: 2.153089092623803

Epoch: 5| Step: 9
Training loss: 2.416001558303833
Validation loss: 2.145238930179227

Epoch: 5| Step: 10
Training loss: 1.5146642923355103
Validation loss: 2.148431929208899

Epoch: 395| Step: 0
Training loss: 1.4534287452697754
Validation loss: 2.1406688356912262

Epoch: 5| Step: 1
Training loss: 2.046050548553467
Validation loss: 2.1536917250643492

Epoch: 5| Step: 2
Training loss: 2.123654842376709
Validation loss: 2.1622782496995825

Epoch: 5| Step: 3
Training loss: 2.339437246322632
Validation loss: 2.1765956365933983

Epoch: 5| Step: 4
Training loss: 2.2040982246398926
Validation loss: 2.1648476328901065

Epoch: 5| Step: 5
Training loss: 2.20194149017334
Validation loss: 2.157575531672406

Epoch: 5| Step: 6
Training loss: 2.6502184867858887
Validation loss: 2.161987704615439

Epoch: 5| Step: 7
Training loss: 2.3529767990112305
Validation loss: 2.138638909145068

Epoch: 5| Step: 8
Training loss: 1.6859047412872314
Validation loss: 2.1342709884848645

Epoch: 5| Step: 9
Training loss: 2.8741705417633057
Validation loss: 2.10528390894654

Epoch: 5| Step: 10
Training loss: 1.80543851852417
Validation loss: 2.111493928458101

Epoch: 396| Step: 0
Training loss: 2.5075652599334717
Validation loss: 2.0954774092602473

Epoch: 5| Step: 1
Training loss: 2.1540751457214355
Validation loss: 2.1117959099431194

Epoch: 5| Step: 2
Training loss: 1.451295256614685
Validation loss: 2.101215831695064

Epoch: 5| Step: 3
Training loss: 2.440887451171875
Validation loss: 2.108004463616238

Epoch: 5| Step: 4
Training loss: 2.234342575073242
Validation loss: 2.130920353756156

Epoch: 5| Step: 5
Training loss: 2.155395984649658
Validation loss: 2.133894233293431

Epoch: 5| Step: 6
Training loss: 2.0852837562561035
Validation loss: 2.1628627879645235

Epoch: 5| Step: 7
Training loss: 1.7156085968017578
Validation loss: 2.170017550068517

Epoch: 5| Step: 8
Training loss: 2.1115870475769043
Validation loss: 2.1645387347026537

Epoch: 5| Step: 9
Training loss: 2.145151138305664
Validation loss: 2.1871591691047914

Epoch: 5| Step: 10
Training loss: 2.839151382446289
Validation loss: 2.197626265146399

Epoch: 397| Step: 0
Training loss: 2.239980697631836
Validation loss: 2.192911450580884

Epoch: 5| Step: 1
Training loss: 1.9972349405288696
Validation loss: 2.183781639222176

Epoch: 5| Step: 2
Training loss: 1.9401237964630127
Validation loss: 2.1563066077488724

Epoch: 5| Step: 3
Training loss: 2.104919195175171
Validation loss: 2.1524135425526607

Epoch: 5| Step: 4
Training loss: 2.075894832611084
Validation loss: 2.1418819632581485

Epoch: 5| Step: 5
Training loss: 1.8113123178482056
Validation loss: 2.138512897235091

Epoch: 5| Step: 6
Training loss: 1.7892818450927734
Validation loss: 2.1247900455228743

Epoch: 5| Step: 7
Training loss: 2.385166645050049
Validation loss: 2.132927530555315

Epoch: 5| Step: 8
Training loss: 2.549980640411377
Validation loss: 2.136156141117055

Epoch: 5| Step: 9
Training loss: 2.2844743728637695
Validation loss: 2.1402835756219845

Epoch: 5| Step: 10
Training loss: 2.5958385467529297
Validation loss: 2.140532875573763

Epoch: 398| Step: 0
Training loss: 1.9621455669403076
Validation loss: 2.1326664699021207

Epoch: 5| Step: 1
Training loss: 2.174198627471924
Validation loss: 2.1223723401305494

Epoch: 5| Step: 2
Training loss: 2.5134634971618652
Validation loss: 2.111444780903478

Epoch: 5| Step: 3
Training loss: 1.9502365589141846
Validation loss: 2.106621288484143

Epoch: 5| Step: 4
Training loss: 2.5698399543762207
Validation loss: 2.1158480464771228

Epoch: 5| Step: 5
Training loss: 2.2170777320861816
Validation loss: 2.1150093129886094

Epoch: 5| Step: 6
Training loss: 1.7180849313735962
Validation loss: 2.131005120533769

Epoch: 5| Step: 7
Training loss: 1.488883376121521
Validation loss: 2.1357255571631977

Epoch: 5| Step: 8
Training loss: 2.7216243743896484
Validation loss: 2.1711623707125263

Epoch: 5| Step: 9
Training loss: 2.2517709732055664
Validation loss: 2.1726586831513273

Epoch: 5| Step: 10
Training loss: 2.1491353511810303
Validation loss: 2.169677306247014

Epoch: 399| Step: 0
Training loss: 2.3351125717163086
Validation loss: 2.177939402159824

Epoch: 5| Step: 1
Training loss: 2.016282558441162
Validation loss: 2.1604788021374772

Epoch: 5| Step: 2
Training loss: 1.8337433338165283
Validation loss: 2.1425281570803736

Epoch: 5| Step: 3
Training loss: 2.1509640216827393
Validation loss: 2.1504307690487114

Epoch: 5| Step: 4
Training loss: 1.1170713901519775
Validation loss: 2.1546853447473175

Epoch: 5| Step: 5
Training loss: 2.5644116401672363
Validation loss: 2.1644429263248237

Epoch: 5| Step: 6
Training loss: 2.295055866241455
Validation loss: 2.145825230947105

Epoch: 5| Step: 7
Training loss: 2.583564281463623
Validation loss: 2.1637157829858924

Epoch: 5| Step: 8
Training loss: 2.3801398277282715
Validation loss: 2.1627626880522697

Epoch: 5| Step: 9
Training loss: 1.7988035678863525
Validation loss: 2.1655053848861368

Epoch: 5| Step: 10
Training loss: 2.5471630096435547
Validation loss: 2.1569164747832925

Epoch: 400| Step: 0
Training loss: 2.635070562362671
Validation loss: 2.1690335991562053

Epoch: 5| Step: 1
Training loss: 2.2864558696746826
Validation loss: 2.1625859724578036

Epoch: 5| Step: 2
Training loss: 2.2786717414855957
Validation loss: 2.1550875017719884

Epoch: 5| Step: 3
Training loss: 1.3835113048553467
Validation loss: 2.1616872382420365

Epoch: 5| Step: 4
Training loss: 2.168881893157959
Validation loss: 2.144592450511071

Epoch: 5| Step: 5
Training loss: 2.5144829750061035
Validation loss: 2.154757620185934

Epoch: 5| Step: 6
Training loss: 1.5631208419799805
Validation loss: 2.1249874484154487

Epoch: 5| Step: 7
Training loss: 1.8896763324737549
Validation loss: 2.1289213383069603

Epoch: 5| Step: 8
Training loss: 2.0768256187438965
Validation loss: 2.124256815961612

Epoch: 5| Step: 9
Training loss: 2.6193883419036865
Validation loss: 2.1208741075249127

Epoch: 5| Step: 10
Training loss: 2.0255069732666016
Validation loss: 2.119752355801162

Testing loss: 2.2631932232115

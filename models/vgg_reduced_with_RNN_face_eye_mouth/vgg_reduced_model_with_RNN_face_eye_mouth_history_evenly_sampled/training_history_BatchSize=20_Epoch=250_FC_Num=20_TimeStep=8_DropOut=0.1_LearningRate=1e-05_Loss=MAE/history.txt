Epoch: 1| Step: 0
Training loss: 5.471351146697998
Validation loss: 5.1632745240324285

Epoch: 5| Step: 1
Training loss: 5.287245273590088
Validation loss: 5.154776906454435

Epoch: 5| Step: 2
Training loss: 4.094463348388672
Validation loss: 5.146323880841655

Epoch: 5| Step: 3
Training loss: 4.343711853027344
Validation loss: 5.136974329589515

Epoch: 5| Step: 4
Training loss: 4.8824028968811035
Validation loss: 5.127371193260275

Epoch: 5| Step: 5
Training loss: 6.335907936096191
Validation loss: 5.116922993813792

Epoch: 5| Step: 6
Training loss: 4.587671279907227
Validation loss: 5.105621727564001

Epoch: 5| Step: 7
Training loss: 5.052276611328125
Validation loss: 5.09339371547904

Epoch: 5| Step: 8
Training loss: 5.226010799407959
Validation loss: 5.079966883505544

Epoch: 5| Step: 9
Training loss: 4.1505889892578125
Validation loss: 5.066318942654517

Epoch: 5| Step: 10
Training loss: 4.500085830688477
Validation loss: 5.051652769888601

Epoch: 2| Step: 0
Training loss: 4.255250930786133
Validation loss: 5.035804943371844

Epoch: 5| Step: 1
Training loss: 5.383210182189941
Validation loss: 5.018984640798261

Epoch: 5| Step: 2
Training loss: 4.259310722351074
Validation loss: 5.000863541838943

Epoch: 5| Step: 3
Training loss: 4.199690341949463
Validation loss: 4.981597659408405

Epoch: 5| Step: 4
Training loss: 5.85274600982666
Validation loss: 4.96039500287784

Epoch: 5| Step: 5
Training loss: 4.859679222106934
Validation loss: 4.9389280606341615

Epoch: 5| Step: 6
Training loss: 3.9620425701141357
Validation loss: 4.914983385352678

Epoch: 5| Step: 7
Training loss: 5.747384548187256
Validation loss: 4.8908446065841185

Epoch: 5| Step: 8
Training loss: 4.519812107086182
Validation loss: 4.864301112390334

Epoch: 5| Step: 9
Training loss: 4.726310729980469
Validation loss: 4.835303480907153

Epoch: 5| Step: 10
Training loss: 4.119958400726318
Validation loss: 4.8056371647824525

Epoch: 3| Step: 0
Training loss: 3.6969635486602783
Validation loss: 4.774015195908085

Epoch: 5| Step: 1
Training loss: 4.627682685852051
Validation loss: 4.740922486910256

Epoch: 5| Step: 2
Training loss: 4.780661582946777
Validation loss: 4.706971727391725

Epoch: 5| Step: 3
Training loss: 3.9501748085021973
Validation loss: 4.671947715102985

Epoch: 5| Step: 4
Training loss: 4.501561164855957
Validation loss: 4.63414143490535

Epoch: 5| Step: 5
Training loss: 4.4113335609436035
Validation loss: 4.597331044494465

Epoch: 5| Step: 6
Training loss: 3.6821446418762207
Validation loss: 4.559774891022713

Epoch: 5| Step: 7
Training loss: 5.0604448318481445
Validation loss: 4.520355657864642

Epoch: 5| Step: 8
Training loss: 5.289404392242432
Validation loss: 4.480761938197638

Epoch: 5| Step: 9
Training loss: 4.367029666900635
Validation loss: 4.443453542647823

Epoch: 5| Step: 10
Training loss: 3.7237229347229004
Validation loss: 4.405397210069882

Epoch: 4| Step: 0
Training loss: 2.883635997772217
Validation loss: 4.369398268320227

Epoch: 5| Step: 1
Training loss: 5.0325751304626465
Validation loss: 4.337596401091544

Epoch: 5| Step: 2
Training loss: 4.752770900726318
Validation loss: 4.304528456862255

Epoch: 5| Step: 3
Training loss: 3.9474449157714844
Validation loss: 4.275940402861564

Epoch: 5| Step: 4
Training loss: 4.262541770935059
Validation loss: 4.250183028559531

Epoch: 5| Step: 5
Training loss: 4.605887413024902
Validation loss: 4.225373824437459

Epoch: 5| Step: 6
Training loss: 4.10659646987915
Validation loss: 4.2025327964495585

Epoch: 5| Step: 7
Training loss: 4.054725170135498
Validation loss: 4.179263937857844

Epoch: 5| Step: 8
Training loss: 4.523770332336426
Validation loss: 4.159450197732577

Epoch: 5| Step: 9
Training loss: 2.4219439029693604
Validation loss: 4.138660671890423

Epoch: 5| Step: 10
Training loss: 3.9740803241729736
Validation loss: 4.119385714172035

Epoch: 5| Step: 0
Training loss: 2.794809103012085
Validation loss: 4.0996964772542315

Epoch: 5| Step: 1
Training loss: 4.767993927001953
Validation loss: 4.080735926987023

Epoch: 5| Step: 2
Training loss: 3.9824929237365723
Validation loss: 4.061328065010809

Epoch: 5| Step: 3
Training loss: 2.9080464839935303
Validation loss: 4.043058338985648

Epoch: 5| Step: 4
Training loss: 3.1214473247528076
Validation loss: 4.024252958195184

Epoch: 5| Step: 5
Training loss: 3.626642942428589
Validation loss: 4.004999309457759

Epoch: 5| Step: 6
Training loss: 3.5354390144348145
Validation loss: 3.9861644929455173

Epoch: 5| Step: 7
Training loss: 4.839380264282227
Validation loss: 3.968378882254324

Epoch: 5| Step: 8
Training loss: 4.623913764953613
Validation loss: 3.94907687299995

Epoch: 5| Step: 9
Training loss: 4.800350666046143
Validation loss: 3.929622404036983

Epoch: 5| Step: 10
Training loss: 3.2051122188568115
Validation loss: 3.909369404597949

Epoch: 6| Step: 0
Training loss: 3.8182811737060547
Validation loss: 3.8880685837038103

Epoch: 5| Step: 1
Training loss: 2.603379726409912
Validation loss: 3.8686557175010763

Epoch: 5| Step: 2
Training loss: 3.759303569793701
Validation loss: 3.8477360510057017

Epoch: 5| Step: 3
Training loss: 3.1979362964630127
Validation loss: 3.8331766589995353

Epoch: 5| Step: 4
Training loss: 3.213230848312378
Validation loss: 3.8166535182665755

Epoch: 5| Step: 5
Training loss: 3.319916248321533
Validation loss: 3.7997782384195635

Epoch: 5| Step: 6
Training loss: 3.9090874195098877
Validation loss: 3.7842239718283377

Epoch: 5| Step: 7
Training loss: 4.480983734130859
Validation loss: 3.769771122163342

Epoch: 5| Step: 8
Training loss: 3.5718071460723877
Validation loss: 3.7560066535908687

Epoch: 5| Step: 9
Training loss: 4.706084251403809
Validation loss: 3.7391097494350967

Epoch: 5| Step: 10
Training loss: 3.986879587173462
Validation loss: 3.723945097256732

Epoch: 7| Step: 0
Training loss: 4.957482814788818
Validation loss: 3.7071921902318157

Epoch: 5| Step: 1
Training loss: 3.5262985229492188
Validation loss: 3.6912743558165846

Epoch: 5| Step: 2
Training loss: 2.984712839126587
Validation loss: 3.672418268777991

Epoch: 5| Step: 3
Training loss: 3.2228221893310547
Validation loss: 3.6573227913148942

Epoch: 5| Step: 4
Training loss: 2.8856470584869385
Validation loss: 3.643087471685102

Epoch: 5| Step: 5
Training loss: 3.8054168224334717
Validation loss: 3.6307613618912233

Epoch: 5| Step: 6
Training loss: 4.245590686798096
Validation loss: 3.6166605590492167

Epoch: 5| Step: 7
Training loss: 3.442169189453125
Validation loss: 3.6031639422139814

Epoch: 5| Step: 8
Training loss: 2.905902862548828
Validation loss: 3.5928719223186536

Epoch: 5| Step: 9
Training loss: 3.5853493213653564
Validation loss: 3.579971044294296

Epoch: 5| Step: 10
Training loss: 3.5016977787017822
Validation loss: 3.568059467500256

Epoch: 8| Step: 0
Training loss: 3.474971055984497
Validation loss: 3.556380287293465

Epoch: 5| Step: 1
Training loss: 3.0270447731018066
Validation loss: 3.5478425077212754

Epoch: 5| Step: 2
Training loss: 4.048579216003418
Validation loss: 3.5381307730110745

Epoch: 5| Step: 3
Training loss: 2.912280559539795
Validation loss: 3.531016283137824

Epoch: 5| Step: 4
Training loss: 4.356392860412598
Validation loss: 3.5211741180830103

Epoch: 5| Step: 5
Training loss: 2.9968206882476807
Validation loss: 3.5120644261760097

Epoch: 5| Step: 6
Training loss: 3.538073778152466
Validation loss: 3.505705289943244

Epoch: 5| Step: 7
Training loss: 2.945718765258789
Validation loss: 3.497449274986021

Epoch: 5| Step: 8
Training loss: 3.448241710662842
Validation loss: 3.4924801113784953

Epoch: 5| Step: 9
Training loss: 3.438619613647461
Validation loss: 3.4857786035024994

Epoch: 5| Step: 10
Training loss: 3.802182912826538
Validation loss: 3.479611832608459

Epoch: 9| Step: 0
Training loss: 3.5303444862365723
Validation loss: 3.473021863609232

Epoch: 5| Step: 1
Training loss: 2.684725761413574
Validation loss: 3.4660102423801216

Epoch: 5| Step: 2
Training loss: 3.9079926013946533
Validation loss: 3.457386996156426

Epoch: 5| Step: 3
Training loss: 3.736743450164795
Validation loss: 3.4522635680372997

Epoch: 5| Step: 4
Training loss: 2.8466711044311523
Validation loss: 3.443377640939528

Epoch: 5| Step: 5
Training loss: 3.612499237060547
Validation loss: 3.43640608684991

Epoch: 5| Step: 6
Training loss: 4.005192756652832
Validation loss: 3.427332426912041

Epoch: 5| Step: 7
Training loss: 3.2258098125457764
Validation loss: 3.420139620381017

Epoch: 5| Step: 8
Training loss: 3.809563159942627
Validation loss: 3.4115439845669653

Epoch: 5| Step: 9
Training loss: 3.1155080795288086
Validation loss: 3.402338158699774

Epoch: 5| Step: 10
Training loss: 2.611929416656494
Validation loss: 3.3918626193077333

Epoch: 10| Step: 0
Training loss: 3.225632429122925
Validation loss: 3.3783590511609147

Epoch: 5| Step: 1
Training loss: 3.1638541221618652
Validation loss: 3.3680727789478917

Epoch: 5| Step: 2
Training loss: 2.7972970008850098
Validation loss: 3.3595608793279177

Epoch: 5| Step: 3
Training loss: 4.48977518081665
Validation loss: 3.35329129106255

Epoch: 5| Step: 4
Training loss: 4.242819786071777
Validation loss: 3.345304168680663

Epoch: 5| Step: 5
Training loss: 2.779367208480835
Validation loss: 3.337833394286453

Epoch: 5| Step: 6
Training loss: 3.1032793521881104
Validation loss: 3.3327807098306637

Epoch: 5| Step: 7
Training loss: 3.5120036602020264
Validation loss: 3.320248270547518

Epoch: 5| Step: 8
Training loss: 3.167173147201538
Validation loss: 3.3158873434989684

Epoch: 5| Step: 9
Training loss: 2.5133473873138428
Validation loss: 3.3080079529875066

Epoch: 5| Step: 10
Training loss: 3.29518985748291
Validation loss: 3.2998626744875343

Epoch: 11| Step: 0
Training loss: 2.195579767227173
Validation loss: 3.2951910521394465

Epoch: 5| Step: 1
Training loss: 3.7707252502441406
Validation loss: 3.2861226194648334

Epoch: 5| Step: 2
Training loss: 3.8728528022766113
Validation loss: 3.28098032551427

Epoch: 5| Step: 3
Training loss: 3.5522475242614746
Validation loss: 3.2723620322442826

Epoch: 5| Step: 4
Training loss: 3.4871551990509033
Validation loss: 3.265987447513047

Epoch: 5| Step: 5
Training loss: 2.1863627433776855
Validation loss: 3.256754900819512

Epoch: 5| Step: 6
Training loss: 4.087498664855957
Validation loss: 3.2530799578594904

Epoch: 5| Step: 7
Training loss: 3.2738444805145264
Validation loss: 3.247766576787477

Epoch: 5| Step: 8
Training loss: 2.550168514251709
Validation loss: 3.2397099541079615

Epoch: 5| Step: 9
Training loss: 3.0302278995513916
Validation loss: 3.2355233059134534

Epoch: 5| Step: 10
Training loss: 3.739583969116211
Validation loss: 3.2301577957727576

Epoch: 12| Step: 0
Training loss: 3.0330023765563965
Validation loss: 3.224593254827684

Epoch: 5| Step: 1
Training loss: 2.517970323562622
Validation loss: 3.22362248615552

Epoch: 5| Step: 2
Training loss: 2.912947654724121
Validation loss: 3.2225481771653697

Epoch: 5| Step: 3
Training loss: 2.7356297969818115
Validation loss: 3.214792589987478

Epoch: 5| Step: 4
Training loss: 3.0701632499694824
Validation loss: 3.2113170572506484

Epoch: 5| Step: 5
Training loss: 3.2176222801208496
Validation loss: 3.212849417040425

Epoch: 5| Step: 6
Training loss: 3.2863452434539795
Validation loss: 3.2087883744188535

Epoch: 5| Step: 7
Training loss: 3.715888500213623
Validation loss: 3.202878805898851

Epoch: 5| Step: 8
Training loss: 3.1428420543670654
Validation loss: 3.194610144502373

Epoch: 5| Step: 9
Training loss: 3.7526252269744873
Validation loss: 3.1923751702872654

Epoch: 5| Step: 10
Training loss: 3.9472992420196533
Validation loss: 3.1905214683983916

Epoch: 13| Step: 0
Training loss: 2.472930908203125
Validation loss: 3.1838767682352374

Epoch: 5| Step: 1
Training loss: 3.2503039836883545
Validation loss: 3.179757992426554

Epoch: 5| Step: 2
Training loss: 3.2194511890411377
Validation loss: 3.1762675905740387

Epoch: 5| Step: 3
Training loss: 2.7190909385681152
Validation loss: 3.1741661640905563

Epoch: 5| Step: 4
Training loss: 3.8958027362823486
Validation loss: 3.1729284204462522

Epoch: 5| Step: 5
Training loss: 3.373366594314575
Validation loss: 3.1670184032891386

Epoch: 5| Step: 6
Training loss: 3.070204496383667
Validation loss: 3.1647921480158323

Epoch: 5| Step: 7
Training loss: 3.2958474159240723
Validation loss: 3.158344007307483

Epoch: 5| Step: 8
Training loss: 2.8991165161132812
Validation loss: 3.152772136913833

Epoch: 5| Step: 9
Training loss: 3.4291701316833496
Validation loss: 3.1520306243691394

Epoch: 5| Step: 10
Training loss: 3.3005733489990234
Validation loss: 3.147642599639072

Epoch: 14| Step: 0
Training loss: 2.4944050312042236
Validation loss: 3.1450717526097454

Epoch: 5| Step: 1
Training loss: 3.388073682785034
Validation loss: 3.14329973600244

Epoch: 5| Step: 2
Training loss: 3.938246965408325
Validation loss: 3.1409800539734545

Epoch: 5| Step: 3
Training loss: 2.696211338043213
Validation loss: 3.1360270951383855

Epoch: 5| Step: 4
Training loss: 4.148664951324463
Validation loss: 3.1339420169912358

Epoch: 5| Step: 5
Training loss: 2.9953904151916504
Validation loss: 3.1321448049237652

Epoch: 5| Step: 6
Training loss: 2.9793388843536377
Validation loss: 3.128485705262871

Epoch: 5| Step: 7
Training loss: 2.718125581741333
Validation loss: 3.1245992183685303

Epoch: 5| Step: 8
Training loss: 3.01137113571167
Validation loss: 3.1240992366626696

Epoch: 5| Step: 9
Training loss: 3.277759552001953
Validation loss: 3.120789620184129

Epoch: 5| Step: 10
Training loss: 2.988118886947632
Validation loss: 3.1163813708930888

Epoch: 15| Step: 0
Training loss: 2.630464553833008
Validation loss: 3.1147293352311656

Epoch: 5| Step: 1
Training loss: 2.928950548171997
Validation loss: 3.1190074028507357

Epoch: 5| Step: 2
Training loss: 2.9120829105377197
Validation loss: 3.1120843220782537

Epoch: 5| Step: 3
Training loss: 2.5752711296081543
Validation loss: 3.109450868380967

Epoch: 5| Step: 4
Training loss: 3.3963046073913574
Validation loss: 3.1056057765919673

Epoch: 5| Step: 5
Training loss: 3.388833999633789
Validation loss: 3.1028276771627445

Epoch: 5| Step: 6
Training loss: 3.373014450073242
Validation loss: 3.1040393947273173

Epoch: 5| Step: 7
Training loss: 3.22808575630188
Validation loss: 3.0986177895658757

Epoch: 5| Step: 8
Training loss: 3.950608730316162
Validation loss: 3.096527535428283

Epoch: 5| Step: 9
Training loss: 3.2429661750793457
Validation loss: 3.0937392352729716

Epoch: 5| Step: 10
Training loss: 2.7800145149230957
Validation loss: 3.091197277909966

Epoch: 16| Step: 0
Training loss: 3.1721959114074707
Validation loss: 3.0917781758052048

Epoch: 5| Step: 1
Training loss: 2.719407558441162
Validation loss: 3.0888320886960594

Epoch: 5| Step: 2
Training loss: 2.6718661785125732
Validation loss: 3.0891470755300214

Epoch: 5| Step: 3
Training loss: 2.6511263847351074
Validation loss: 3.0942080559269076

Epoch: 5| Step: 4
Training loss: 3.160949230194092
Validation loss: 3.0948572722814416

Epoch: 5| Step: 5
Training loss: 3.6554622650146484
Validation loss: 3.0851148174655054

Epoch: 5| Step: 6
Training loss: 3.225947856903076
Validation loss: 3.0800173410805325

Epoch: 5| Step: 7
Training loss: 3.5144927501678467
Validation loss: 3.0815377825049945

Epoch: 5| Step: 8
Training loss: 3.3452587127685547
Validation loss: 3.0866081509538876

Epoch: 5| Step: 9
Training loss: 2.9143242835998535
Validation loss: 3.088697733417634

Epoch: 5| Step: 10
Training loss: 3.3156604766845703
Validation loss: 3.08026764469762

Epoch: 17| Step: 0
Training loss: 3.494330883026123
Validation loss: 3.0773565205194617

Epoch: 5| Step: 1
Training loss: 2.6496291160583496
Validation loss: 3.0762458898687877

Epoch: 5| Step: 2
Training loss: 3.1389565467834473
Validation loss: 3.0807014357659126

Epoch: 5| Step: 3
Training loss: 3.415961503982544
Validation loss: 3.077432260718397

Epoch: 5| Step: 4
Training loss: 2.2041068077087402
Validation loss: 3.075871923918365

Epoch: 5| Step: 5
Training loss: 3.262302875518799
Validation loss: 3.070358842931768

Epoch: 5| Step: 6
Training loss: 3.481884717941284
Validation loss: 3.0674498517026185

Epoch: 5| Step: 7
Training loss: 3.2536327838897705
Validation loss: 3.065686712982834

Epoch: 5| Step: 8
Training loss: 3.566862106323242
Validation loss: 3.0656744100714244

Epoch: 5| Step: 9
Training loss: 2.707782030105591
Validation loss: 3.0642943664263655

Epoch: 5| Step: 10
Training loss: 3.04302978515625
Validation loss: 3.0643345668751705

Epoch: 18| Step: 0
Training loss: 3.382737398147583
Validation loss: 3.064870208822271

Epoch: 5| Step: 1
Training loss: 3.274549961090088
Validation loss: 3.059995728154336

Epoch: 5| Step: 2
Training loss: 2.747668743133545
Validation loss: 3.0537501124925512

Epoch: 5| Step: 3
Training loss: 4.09836483001709
Validation loss: 3.0530730908916843

Epoch: 5| Step: 4
Training loss: 2.961634397506714
Validation loss: 3.0530430424597954

Epoch: 5| Step: 5
Training loss: 2.852174997329712
Validation loss: 3.0568338081400883

Epoch: 5| Step: 6
Training loss: 3.0239288806915283
Validation loss: 3.0598433991914153

Epoch: 5| Step: 7
Training loss: 2.7698042392730713
Validation loss: 3.0609259656680528

Epoch: 5| Step: 8
Training loss: 3.7257602214813232
Validation loss: 3.0530054210334696

Epoch: 5| Step: 9
Training loss: 2.7157466411590576
Validation loss: 3.0468577774622108

Epoch: 5| Step: 10
Training loss: 2.499547004699707
Validation loss: 3.0437959137783257

Epoch: 19| Step: 0
Training loss: 3.3827285766601562
Validation loss: 3.043342690314016

Epoch: 5| Step: 1
Training loss: 3.0364131927490234
Validation loss: 3.0463493690695813

Epoch: 5| Step: 2
Training loss: 3.0615811347961426
Validation loss: 3.0509650348335184

Epoch: 5| Step: 3
Training loss: 3.4527831077575684
Validation loss: 3.0484832845708376

Epoch: 5| Step: 4
Training loss: 2.850217342376709
Validation loss: 3.0404908862165225

Epoch: 5| Step: 5
Training loss: 2.476245880126953
Validation loss: 3.0381041572939966

Epoch: 5| Step: 6
Training loss: 3.29559063911438
Validation loss: 3.0368737892438005

Epoch: 5| Step: 7
Training loss: 2.3466639518737793
Validation loss: 3.0380995658136185

Epoch: 5| Step: 8
Training loss: 3.5448360443115234
Validation loss: 3.042500767656552

Epoch: 5| Step: 9
Training loss: 3.3077392578125
Validation loss: 3.036629053854173

Epoch: 5| Step: 10
Training loss: 3.3236448764801025
Validation loss: 3.0353505995965775

Epoch: 20| Step: 0
Training loss: 3.2053380012512207
Validation loss: 3.0334051552639214

Epoch: 5| Step: 1
Training loss: 3.3494725227355957
Validation loss: 3.0322135058782433

Epoch: 5| Step: 2
Training loss: 2.5275983810424805
Validation loss: 3.029990908920124

Epoch: 5| Step: 3
Training loss: 2.488720655441284
Validation loss: 3.0277687477809128

Epoch: 5| Step: 4
Training loss: 3.456035614013672
Validation loss: 3.0289445077219317

Epoch: 5| Step: 5
Training loss: 3.7544217109680176
Validation loss: 3.0279153418797318

Epoch: 5| Step: 6
Training loss: 2.1998274326324463
Validation loss: 3.029421288480041

Epoch: 5| Step: 7
Training loss: 3.1012160778045654
Validation loss: 3.0248602897890153

Epoch: 5| Step: 8
Training loss: 2.768932580947876
Validation loss: 3.023498199319327

Epoch: 5| Step: 9
Training loss: 3.8343188762664795
Validation loss: 3.023940478601763

Epoch: 5| Step: 10
Training loss: 3.2791881561279297
Validation loss: 3.0233690661768757

Epoch: 21| Step: 0
Training loss: 2.9567763805389404
Validation loss: 3.0226716533783944

Epoch: 5| Step: 1
Training loss: 3.4597675800323486
Validation loss: 3.0210200099534887

Epoch: 5| Step: 2
Training loss: 2.586536407470703
Validation loss: 3.0207096786909204

Epoch: 5| Step: 3
Training loss: 2.8626487255096436
Validation loss: 3.0196134454460553

Epoch: 5| Step: 4
Training loss: 2.3628365993499756
Validation loss: 3.017981272871776

Epoch: 5| Step: 5
Training loss: 3.5443687438964844
Validation loss: 3.0189655724392144

Epoch: 5| Step: 6
Training loss: 3.4497313499450684
Validation loss: 3.0182052273904123

Epoch: 5| Step: 7
Training loss: 2.6523287296295166
Validation loss: 3.017195608026238

Epoch: 5| Step: 8
Training loss: 4.133365154266357
Validation loss: 3.016035695229807

Epoch: 5| Step: 9
Training loss: 2.1005258560180664
Validation loss: 3.0150549386137273

Epoch: 5| Step: 10
Training loss: 3.868164539337158
Validation loss: 3.015641273990754

Epoch: 22| Step: 0
Training loss: 3.069706439971924
Validation loss: 3.014716645722748

Epoch: 5| Step: 1
Training loss: 3.0005087852478027
Validation loss: 3.012799965438022

Epoch: 5| Step: 2
Training loss: 2.214989423751831
Validation loss: 3.013873043880668

Epoch: 5| Step: 3
Training loss: 2.603982448577881
Validation loss: 3.01124503022881

Epoch: 5| Step: 4
Training loss: 3.724370241165161
Validation loss: 3.010251819446523

Epoch: 5| Step: 5
Training loss: 2.7479805946350098
Validation loss: 3.0111211807497087

Epoch: 5| Step: 6
Training loss: 3.203458786010742
Validation loss: 3.009591894765054

Epoch: 5| Step: 7
Training loss: 3.607020616531372
Validation loss: 3.007466249568488

Epoch: 5| Step: 8
Training loss: 3.662166118621826
Validation loss: 3.009317082743491

Epoch: 5| Step: 9
Training loss: 3.410186767578125
Validation loss: 3.0105098370582826

Epoch: 5| Step: 10
Training loss: 2.4907026290893555
Validation loss: 3.009661779608778

Epoch: 23| Step: 0
Training loss: 3.1492362022399902
Validation loss: 3.007192765512774

Epoch: 5| Step: 1
Training loss: 3.783722400665283
Validation loss: 3.007794439151723

Epoch: 5| Step: 2
Training loss: 2.8619208335876465
Validation loss: 3.0047845096998316

Epoch: 5| Step: 3
Training loss: 3.738171339035034
Validation loss: 3.0026868517680834

Epoch: 5| Step: 4
Training loss: 3.5144429206848145
Validation loss: 3.002187887827555

Epoch: 5| Step: 5
Training loss: 2.9243063926696777
Validation loss: 2.999962229882517

Epoch: 5| Step: 6
Training loss: 2.2403955459594727
Validation loss: 2.9997218449910483

Epoch: 5| Step: 7
Training loss: 3.2640655040740967
Validation loss: 2.9973491238009546

Epoch: 5| Step: 8
Training loss: 2.482720136642456
Validation loss: 2.9946366740811254

Epoch: 5| Step: 9
Training loss: 3.605900287628174
Validation loss: 2.995097529503607

Epoch: 5| Step: 10
Training loss: 2.0235767364501953
Validation loss: 2.9922141387898433

Epoch: 24| Step: 0
Training loss: 3.059040069580078
Validation loss: 2.9886734177989345

Epoch: 5| Step: 1
Training loss: 3.0366625785827637
Validation loss: 2.988065460676788

Epoch: 5| Step: 2
Training loss: 3.4265246391296387
Validation loss: 2.9858212983736427

Epoch: 5| Step: 3
Training loss: 4.180718421936035
Validation loss: 2.98459037657707

Epoch: 5| Step: 4
Training loss: 2.5451934337615967
Validation loss: 2.978230768634427

Epoch: 5| Step: 5
Training loss: 3.120272397994995
Validation loss: 2.9782672671861548

Epoch: 5| Step: 6
Training loss: 3.1722259521484375
Validation loss: 2.976434928114696

Epoch: 5| Step: 7
Training loss: 2.782911777496338
Validation loss: 2.9763716446456088

Epoch: 5| Step: 8
Training loss: 2.711759567260742
Validation loss: 2.9756164499508437

Epoch: 5| Step: 9
Training loss: 3.42326021194458
Validation loss: 2.9715694304435485

Epoch: 5| Step: 10
Training loss: 1.9539395570755005
Validation loss: 2.971530695115366

Epoch: 25| Step: 0
Training loss: 2.618237257003784
Validation loss: 2.9725642076102634

Epoch: 5| Step: 1
Training loss: 2.709404706954956
Validation loss: 2.9687728446017028

Epoch: 5| Step: 2
Training loss: 3.1396124362945557
Validation loss: 2.9694845420058056

Epoch: 5| Step: 3
Training loss: 3.4913489818573
Validation loss: 2.969696321795064

Epoch: 5| Step: 4
Training loss: 2.6645145416259766
Validation loss: 2.9671951058090373

Epoch: 5| Step: 5
Training loss: 3.048313856124878
Validation loss: 2.9649631566898798

Epoch: 5| Step: 6
Training loss: 3.354121685028076
Validation loss: 2.959313928440053

Epoch: 5| Step: 7
Training loss: 2.6949076652526855
Validation loss: 2.9578737135856383

Epoch: 5| Step: 8
Training loss: 3.4132018089294434
Validation loss: 2.957059032173567

Epoch: 5| Step: 9
Training loss: 2.9419937133789062
Validation loss: 2.9542925050181728

Epoch: 5| Step: 10
Training loss: 3.4499576091766357
Validation loss: 2.9572142529231247

Epoch: 26| Step: 0
Training loss: 3.753286361694336
Validation loss: 2.955604722422938

Epoch: 5| Step: 1
Training loss: 3.3799712657928467
Validation loss: 2.952103932698568

Epoch: 5| Step: 2
Training loss: 2.437195301055908
Validation loss: 2.9498386921421176

Epoch: 5| Step: 3
Training loss: 2.907362937927246
Validation loss: 2.949929929548694

Epoch: 5| Step: 4
Training loss: 3.273951768875122
Validation loss: 2.9503037391170377

Epoch: 5| Step: 5
Training loss: 3.867154359817505
Validation loss: 2.947755008615473

Epoch: 5| Step: 6
Training loss: 3.024815559387207
Validation loss: 2.944478832265382

Epoch: 5| Step: 7
Training loss: 2.550915241241455
Validation loss: 2.9458761035755114

Epoch: 5| Step: 8
Training loss: 2.131242275238037
Validation loss: 2.944957971572876

Epoch: 5| Step: 9
Training loss: 3.1649587154388428
Validation loss: 2.944182560008059

Epoch: 5| Step: 10
Training loss: 2.83915376663208
Validation loss: 2.939792784311438

Epoch: 27| Step: 0
Training loss: 3.3375744819641113
Validation loss: 2.940653713800574

Epoch: 5| Step: 1
Training loss: 2.5710835456848145
Validation loss: 2.9412321108643726

Epoch: 5| Step: 2
Training loss: 3.5156033039093018
Validation loss: 2.9484613403197257

Epoch: 5| Step: 3
Training loss: 2.4891693592071533
Validation loss: 2.94811983518703

Epoch: 5| Step: 4
Training loss: 3.4295382499694824
Validation loss: 2.9369938809384584

Epoch: 5| Step: 5
Training loss: 3.0590624809265137
Validation loss: 2.936177525469052

Epoch: 5| Step: 6
Training loss: 2.460994243621826
Validation loss: 2.936018346458353

Epoch: 5| Step: 7
Training loss: 3.6244492530822754
Validation loss: 2.932439560531288

Epoch: 5| Step: 8
Training loss: 2.3860721588134766
Validation loss: 2.9345704611911567

Epoch: 5| Step: 9
Training loss: 2.824403762817383
Validation loss: 2.9370573618078746

Epoch: 5| Step: 10
Training loss: 3.7193093299865723
Validation loss: 2.9378958261141213

Epoch: 28| Step: 0
Training loss: 3.79426646232605
Validation loss: 2.9399251758411364

Epoch: 5| Step: 1
Training loss: 2.110870361328125
Validation loss: 2.9393027751676497

Epoch: 5| Step: 2
Training loss: 2.9281575679779053
Validation loss: 2.941816383792508

Epoch: 5| Step: 3
Training loss: 3.193885087966919
Validation loss: 2.9447684262388494

Epoch: 5| Step: 4
Training loss: 2.7034225463867188
Validation loss: 2.9368464869837605

Epoch: 5| Step: 5
Training loss: 3.5646719932556152
Validation loss: 2.9316161832501813

Epoch: 5| Step: 6
Training loss: 2.949906587600708
Validation loss: 2.931950428152597

Epoch: 5| Step: 7
Training loss: 2.817843198776245
Validation loss: 2.936108886554677

Epoch: 5| Step: 8
Training loss: 2.253408432006836
Validation loss: 2.933353570199782

Epoch: 5| Step: 9
Training loss: 3.593869686126709
Validation loss: 2.926270923306865

Epoch: 5| Step: 10
Training loss: 3.379896879196167
Validation loss: 2.922279888583768

Epoch: 29| Step: 0
Training loss: 2.444951057434082
Validation loss: 2.921237801992765

Epoch: 5| Step: 1
Training loss: 3.860710620880127
Validation loss: 2.920620649091659

Epoch: 5| Step: 2
Training loss: 3.0229830741882324
Validation loss: 2.921381570959604

Epoch: 5| Step: 3
Training loss: 3.096259593963623
Validation loss: 2.91707242432461

Epoch: 5| Step: 4
Training loss: 2.4487452507019043
Validation loss: 2.9183907457577285

Epoch: 5| Step: 5
Training loss: 2.097287654876709
Validation loss: 2.9151670907133367

Epoch: 5| Step: 6
Training loss: 2.7735300064086914
Validation loss: 2.9153665317002164

Epoch: 5| Step: 7
Training loss: 2.7852981090545654
Validation loss: 2.917431982614661

Epoch: 5| Step: 8
Training loss: 3.2351410388946533
Validation loss: 2.919423262278239

Epoch: 5| Step: 9
Training loss: 3.820718288421631
Validation loss: 2.9148243473422144

Epoch: 5| Step: 10
Training loss: 3.648538589477539
Validation loss: 2.9149054763137654

Epoch: 30| Step: 0
Training loss: 2.4638946056365967
Validation loss: 2.9199469038235244

Epoch: 5| Step: 1
Training loss: 3.807666063308716
Validation loss: 2.921273277651879

Epoch: 5| Step: 2
Training loss: 2.578437089920044
Validation loss: 2.9110169410705566

Epoch: 5| Step: 3
Training loss: 2.668079376220703
Validation loss: 2.916019852443408

Epoch: 5| Step: 4
Training loss: 3.267241954803467
Validation loss: 2.922704212127193

Epoch: 5| Step: 5
Training loss: 3.115403652191162
Validation loss: 2.9188293795431814

Epoch: 5| Step: 6
Training loss: 3.37404203414917
Validation loss: 2.910995057834092

Epoch: 5| Step: 7
Training loss: 3.140934467315674
Validation loss: 2.9125652646505706

Epoch: 5| Step: 8
Training loss: 3.2045352458953857
Validation loss: 2.9119989615614696

Epoch: 5| Step: 9
Training loss: 3.078298568725586
Validation loss: 2.92064864917468

Epoch: 5| Step: 10
Training loss: 2.310842275619507
Validation loss: 2.9238109357895388

Epoch: 31| Step: 0
Training loss: 3.432572603225708
Validation loss: 2.9143522401009836

Epoch: 5| Step: 1
Training loss: 3.3674473762512207
Validation loss: 2.908711407774238

Epoch: 5| Step: 2
Training loss: 3.2944438457489014
Validation loss: 2.908947948486574

Epoch: 5| Step: 3
Training loss: 3.0611894130706787
Validation loss: 2.9041266646436465

Epoch: 5| Step: 4
Training loss: 3.6545119285583496
Validation loss: 2.904002827982749

Epoch: 5| Step: 5
Training loss: 2.558596134185791
Validation loss: 2.9034695266395487

Epoch: 5| Step: 6
Training loss: 2.385153293609619
Validation loss: 2.9046904784376903

Epoch: 5| Step: 7
Training loss: 2.2693724632263184
Validation loss: 2.9040900840554187

Epoch: 5| Step: 8
Training loss: 2.748297691345215
Validation loss: 2.9045624425334315

Epoch: 5| Step: 9
Training loss: 3.5208678245544434
Validation loss: 2.9033863826464583

Epoch: 5| Step: 10
Training loss: 2.703993320465088
Validation loss: 2.901010197977866

Epoch: 32| Step: 0
Training loss: 2.9176204204559326
Validation loss: 2.900683743979341

Epoch: 5| Step: 1
Training loss: 3.840904951095581
Validation loss: 2.899554532061341

Epoch: 5| Step: 2
Training loss: 2.6042234897613525
Validation loss: 2.899142831884405

Epoch: 5| Step: 3
Training loss: 3.2380447387695312
Validation loss: 2.8966908275440173

Epoch: 5| Step: 4
Training loss: 3.1469616889953613
Validation loss: 2.8978515850600375

Epoch: 5| Step: 5
Training loss: 3.122525930404663
Validation loss: 2.9000852287456556

Epoch: 5| Step: 6
Training loss: 2.5992627143859863
Validation loss: 2.8982109818407285

Epoch: 5| Step: 7
Training loss: 2.7903878688812256
Validation loss: 2.896759751022503

Epoch: 5| Step: 8
Training loss: 2.9700541496276855
Validation loss: 2.9000656374039187

Epoch: 5| Step: 9
Training loss: 3.437218189239502
Validation loss: 2.9014356008139988

Epoch: 5| Step: 10
Training loss: 2.182680368423462
Validation loss: 2.9043839849451536

Epoch: 33| Step: 0
Training loss: 2.8904707431793213
Validation loss: 2.900350396351148

Epoch: 5| Step: 1
Training loss: 3.7244606018066406
Validation loss: 2.90834520452766

Epoch: 5| Step: 2
Training loss: 3.144899845123291
Validation loss: 2.9284476823704217

Epoch: 5| Step: 3
Training loss: 2.806704044342041
Validation loss: 2.920398278902936

Epoch: 5| Step: 4
Training loss: 3.2973148822784424
Validation loss: 2.894015032757995

Epoch: 5| Step: 5
Training loss: 2.7722301483154297
Validation loss: 2.8925582875487623

Epoch: 5| Step: 6
Training loss: 3.0525519847869873
Validation loss: 2.893289940331572

Epoch: 5| Step: 7
Training loss: 2.786848545074463
Validation loss: 2.9293791786316903

Epoch: 5| Step: 8
Training loss: 2.609520196914673
Validation loss: 2.890932003657023

Epoch: 5| Step: 9
Training loss: 2.701235055923462
Validation loss: 2.8896978209095616

Epoch: 5| Step: 10
Training loss: 3.21002197265625
Validation loss: 2.8942465500165055

Epoch: 34| Step: 0
Training loss: 2.6752986907958984
Validation loss: 2.895459903183804

Epoch: 5| Step: 1
Training loss: 2.5225324630737305
Validation loss: 2.8973919858214674

Epoch: 5| Step: 2
Training loss: 2.6162400245666504
Validation loss: 2.890351423653223

Epoch: 5| Step: 3
Training loss: 3.3121657371520996
Validation loss: 2.889599120745095

Epoch: 5| Step: 4
Training loss: 2.756486654281616
Validation loss: 2.891184265895556

Epoch: 5| Step: 5
Training loss: 3.5887207984924316
Validation loss: 2.8913057875889603

Epoch: 5| Step: 6
Training loss: 3.9030356407165527
Validation loss: 2.8948176163499073

Epoch: 5| Step: 7
Training loss: 2.147286891937256
Validation loss: 2.899128765188238

Epoch: 5| Step: 8
Training loss: 3.1134324073791504
Validation loss: 2.896434614735265

Epoch: 5| Step: 9
Training loss: 2.7361416816711426
Validation loss: 2.8950998424201884

Epoch: 5| Step: 10
Training loss: 3.633838653564453
Validation loss: 2.893642984410768

Epoch: 35| Step: 0
Training loss: 3.1864495277404785
Validation loss: 2.889057018423593

Epoch: 5| Step: 1
Training loss: 2.822050094604492
Validation loss: 2.8841230510383524

Epoch: 5| Step: 2
Training loss: 3.0661683082580566
Validation loss: 2.8833011017050794

Epoch: 5| Step: 3
Training loss: 2.6882636547088623
Validation loss: 2.8819852285487677

Epoch: 5| Step: 4
Training loss: 3.374523639678955
Validation loss: 2.886545099237914

Epoch: 5| Step: 5
Training loss: 3.1346192359924316
Validation loss: 2.8833083670626403

Epoch: 5| Step: 6
Training loss: 2.9920592308044434
Validation loss: 2.882229515301284

Epoch: 5| Step: 7
Training loss: 3.3246123790740967
Validation loss: 2.8775253834262973

Epoch: 5| Step: 8
Training loss: 2.0653128623962402
Validation loss: 2.8790038631808375

Epoch: 5| Step: 9
Training loss: 3.301546573638916
Validation loss: 2.879080549363167

Epoch: 5| Step: 10
Training loss: 2.8772668838500977
Validation loss: 2.8781236192231536

Epoch: 36| Step: 0
Training loss: 2.7526822090148926
Validation loss: 2.8778466896344255

Epoch: 5| Step: 1
Training loss: 3.378507614135742
Validation loss: 2.8771904412136284

Epoch: 5| Step: 2
Training loss: 3.1686112880706787
Validation loss: 2.875712448550809

Epoch: 5| Step: 3
Training loss: 2.778447389602661
Validation loss: 2.872293951690838

Epoch: 5| Step: 4
Training loss: 2.872049331665039
Validation loss: 2.8805585625351116

Epoch: 5| Step: 5
Training loss: 3.1257052421569824
Validation loss: 2.8757505852689027

Epoch: 5| Step: 6
Training loss: 3.2313950061798096
Validation loss: 2.8813385604530253

Epoch: 5| Step: 7
Training loss: 2.7891666889190674
Validation loss: 2.8768431576349403

Epoch: 5| Step: 8
Training loss: 2.8226895332336426
Validation loss: 2.878259458849507

Epoch: 5| Step: 9
Training loss: 3.521125316619873
Validation loss: 2.876070563511182

Epoch: 5| Step: 10
Training loss: 2.205514907836914
Validation loss: 2.8769198233081448

Epoch: 37| Step: 0
Training loss: 2.6096041202545166
Validation loss: 2.8789974463883268

Epoch: 5| Step: 1
Training loss: 2.7057015895843506
Validation loss: 2.8739306542181198

Epoch: 5| Step: 2
Training loss: 2.4635939598083496
Validation loss: 2.874923441999702

Epoch: 5| Step: 3
Training loss: 3.3426177501678467
Validation loss: 2.8721166041589554

Epoch: 5| Step: 4
Training loss: 3.201289415359497
Validation loss: 2.8672773248405865

Epoch: 5| Step: 5
Training loss: 3.328778028488159
Validation loss: 2.866175284949682

Epoch: 5| Step: 6
Training loss: 2.9024643898010254
Validation loss: 2.863736903795632

Epoch: 5| Step: 7
Training loss: 2.3123888969421387
Validation loss: 2.8657574884353147

Epoch: 5| Step: 8
Training loss: 3.698960542678833
Validation loss: 2.8658201079214773

Epoch: 5| Step: 9
Training loss: 3.0262768268585205
Validation loss: 2.8696275167567755

Epoch: 5| Step: 10
Training loss: 3.126396656036377
Validation loss: 2.869434287471156

Epoch: 38| Step: 0
Training loss: 2.2312276363372803
Validation loss: 2.867479649923181

Epoch: 5| Step: 1
Training loss: 3.505281448364258
Validation loss: 2.8675823186033513

Epoch: 5| Step: 2
Training loss: 2.2976315021514893
Validation loss: 2.8668569954492713

Epoch: 5| Step: 3
Training loss: 3.3921189308166504
Validation loss: 2.8718202473014913

Epoch: 5| Step: 4
Training loss: 3.501387119293213
Validation loss: 2.871977093399212

Epoch: 5| Step: 5
Training loss: 2.664741039276123
Validation loss: 2.871298764341621

Epoch: 5| Step: 6
Training loss: 2.8262133598327637
Validation loss: 2.86482580759192

Epoch: 5| Step: 7
Training loss: 3.731060028076172
Validation loss: 2.865884675774523

Epoch: 5| Step: 8
Training loss: 2.841533899307251
Validation loss: 2.8618956945275746

Epoch: 5| Step: 9
Training loss: 2.952549457550049
Validation loss: 2.860602730063982

Epoch: 5| Step: 10
Training loss: 2.6844029426574707
Validation loss: 2.860267134122951

Epoch: 39| Step: 0
Training loss: 2.3214423656463623
Validation loss: 2.860301048524918

Epoch: 5| Step: 1
Training loss: 2.6691958904266357
Validation loss: 2.8580885882018716

Epoch: 5| Step: 2
Training loss: 2.377183437347412
Validation loss: 2.859813815803938

Epoch: 5| Step: 3
Training loss: 2.875161647796631
Validation loss: 2.870801915404617

Epoch: 5| Step: 4
Training loss: 2.7852895259857178
Validation loss: 2.860369964312482

Epoch: 5| Step: 5
Training loss: 3.2131571769714355
Validation loss: 2.8655759929328837

Epoch: 5| Step: 6
Training loss: 3.3814849853515625
Validation loss: 2.8581919567559355

Epoch: 5| Step: 7
Training loss: 3.5473084449768066
Validation loss: 2.8609730146264516

Epoch: 5| Step: 8
Training loss: 3.105659246444702
Validation loss: 2.8582809125223467

Epoch: 5| Step: 9
Training loss: 3.078615665435791
Validation loss: 2.8536653698131604

Epoch: 5| Step: 10
Training loss: 3.320913076400757
Validation loss: 2.853916452777001

Epoch: 40| Step: 0
Training loss: 2.863386631011963
Validation loss: 2.8515758386222263

Epoch: 5| Step: 1
Training loss: 3.806896924972534
Validation loss: 2.853622467287125

Epoch: 5| Step: 2
Training loss: 2.392914056777954
Validation loss: 2.8540903496485885

Epoch: 5| Step: 3
Training loss: 3.375591278076172
Validation loss: 2.8550498767565657

Epoch: 5| Step: 4
Training loss: 2.868649959564209
Validation loss: 2.854991046331262

Epoch: 5| Step: 5
Training loss: 3.0228054523468018
Validation loss: 2.8570756886595037

Epoch: 5| Step: 6
Training loss: 2.6424946784973145
Validation loss: 2.8577432324809413

Epoch: 5| Step: 7
Training loss: 2.738800525665283
Validation loss: 2.8554065714600267

Epoch: 5| Step: 8
Training loss: 3.2131481170654297
Validation loss: 2.8637620505466255

Epoch: 5| Step: 9
Training loss: 2.7094340324401855
Validation loss: 2.8589377992896625

Epoch: 5| Step: 10
Training loss: 2.9266719818115234
Validation loss: 2.8580486107898015

Epoch: 41| Step: 0
Training loss: 2.960012197494507
Validation loss: 2.8559436054639917

Epoch: 5| Step: 1
Training loss: 3.335608959197998
Validation loss: 2.85025720186131

Epoch: 5| Step: 2
Training loss: 2.6894946098327637
Validation loss: 2.849055185112902

Epoch: 5| Step: 3
Training loss: 2.341963291168213
Validation loss: 2.8482243502011864

Epoch: 5| Step: 4
Training loss: 2.673083782196045
Validation loss: 2.847906087034492

Epoch: 5| Step: 5
Training loss: 3.4346115589141846
Validation loss: 2.8537005839809293

Epoch: 5| Step: 6
Training loss: 2.9569075107574463
Validation loss: 2.8606184297992336

Epoch: 5| Step: 7
Training loss: 2.9640450477600098
Validation loss: 2.854482607174945

Epoch: 5| Step: 8
Training loss: 3.5677177906036377
Validation loss: 2.8481884566686486

Epoch: 5| Step: 9
Training loss: 2.9874606132507324
Validation loss: 2.8508418554900796

Epoch: 5| Step: 10
Training loss: 2.5626885890960693
Validation loss: 2.856117074207593

Epoch: 42| Step: 0
Training loss: 2.81494402885437
Validation loss: 2.856666913596533

Epoch: 5| Step: 1
Training loss: 3.5220749378204346
Validation loss: 2.8681256514723583

Epoch: 5| Step: 2
Training loss: 3.550812244415283
Validation loss: 2.875548634477841

Epoch: 5| Step: 3
Training loss: 2.6664340496063232
Validation loss: 2.889945968504875

Epoch: 5| Step: 4
Training loss: 2.770742177963257
Validation loss: 2.881400715920233

Epoch: 5| Step: 5
Training loss: 2.424734592437744
Validation loss: 2.870235714861142

Epoch: 5| Step: 6
Training loss: 2.05401611328125
Validation loss: 2.8590337948132585

Epoch: 5| Step: 7
Training loss: 2.629960536956787
Validation loss: 2.8650979893181914

Epoch: 5| Step: 8
Training loss: 2.880962371826172
Validation loss: 2.863969518292335

Epoch: 5| Step: 9
Training loss: 3.6385321617126465
Validation loss: 2.852988891704108

Epoch: 5| Step: 10
Training loss: 3.6486504077911377
Validation loss: 2.854588875206568

Epoch: 43| Step: 0
Training loss: 3.3001160621643066
Validation loss: 2.870324352736114

Epoch: 5| Step: 1
Training loss: 2.513024091720581
Validation loss: 2.8576146992303992

Epoch: 5| Step: 2
Training loss: 3.250948429107666
Validation loss: 2.8523705620919504

Epoch: 5| Step: 3
Training loss: 2.829787492752075
Validation loss: 2.8484755100742465

Epoch: 5| Step: 4
Training loss: 2.9891884326934814
Validation loss: 2.8428855134594824

Epoch: 5| Step: 5
Training loss: 2.626302719116211
Validation loss: 2.8423329860933366

Epoch: 5| Step: 6
Training loss: 2.065269947052002
Validation loss: 2.8394483135592554

Epoch: 5| Step: 7
Training loss: 3.386807680130005
Validation loss: 2.847052376757386

Epoch: 5| Step: 8
Training loss: 3.456758975982666
Validation loss: 2.8512207103031937

Epoch: 5| Step: 9
Training loss: 3.187072515487671
Validation loss: 2.8499647366103305

Epoch: 5| Step: 10
Training loss: 2.835153341293335
Validation loss: 2.8487854055179063

Epoch: 44| Step: 0
Training loss: 2.969649076461792
Validation loss: 2.8427666694887224

Epoch: 5| Step: 1
Training loss: 2.7773258686065674
Validation loss: 2.838569007894044

Epoch: 5| Step: 2
Training loss: 2.9621214866638184
Validation loss: 2.8455662906810804

Epoch: 5| Step: 3
Training loss: 2.788154125213623
Validation loss: 2.851794071094964

Epoch: 5| Step: 4
Training loss: 2.884054660797119
Validation loss: 2.8492744045872844

Epoch: 5| Step: 5
Training loss: 2.8989481925964355
Validation loss: 2.8500679923642065

Epoch: 5| Step: 6
Training loss: 2.8901305198669434
Validation loss: 2.844090692458614

Epoch: 5| Step: 7
Training loss: 3.4347050189971924
Validation loss: 2.8442394297610045

Epoch: 5| Step: 8
Training loss: 2.881378650665283
Validation loss: 2.8413873846812914

Epoch: 5| Step: 9
Training loss: 3.1317927837371826
Validation loss: 2.839216552754884

Epoch: 5| Step: 10
Training loss: 2.7485475540161133
Validation loss: 2.8378663268140567

Epoch: 45| Step: 0
Training loss: 3.127077102661133
Validation loss: 2.839477631353563

Epoch: 5| Step: 1
Training loss: 2.2819950580596924
Validation loss: 2.840936588984664

Epoch: 5| Step: 2
Training loss: 2.5099639892578125
Validation loss: 2.8404758258532454

Epoch: 5| Step: 3
Training loss: 3.5223746299743652
Validation loss: 2.8406671503538727

Epoch: 5| Step: 4
Training loss: 2.793255567550659
Validation loss: 2.843740837548369

Epoch: 5| Step: 5
Training loss: 3.062311887741089
Validation loss: 2.843376657014252

Epoch: 5| Step: 6
Training loss: 2.898481845855713
Validation loss: 2.849395921153407

Epoch: 5| Step: 7
Training loss: 3.4632821083068848
Validation loss: 2.8526879536208285

Epoch: 5| Step: 8
Training loss: 3.1139512062072754
Validation loss: 2.850170179079938

Epoch: 5| Step: 9
Training loss: 2.025850534439087
Validation loss: 2.842109736575875

Epoch: 5| Step: 10
Training loss: 3.5787765979766846
Validation loss: 2.844822111950126

Epoch: 46| Step: 0
Training loss: 2.452547550201416
Validation loss: 2.841177978823262

Epoch: 5| Step: 1
Training loss: 2.6995186805725098
Validation loss: 2.8400193824562976

Epoch: 5| Step: 2
Training loss: 2.413386106491089
Validation loss: 2.840056255299558

Epoch: 5| Step: 3
Training loss: 3.4171440601348877
Validation loss: 2.8419657138086136

Epoch: 5| Step: 4
Training loss: 3.1677310466766357
Validation loss: 2.8429138224612

Epoch: 5| Step: 5
Training loss: 3.8552188873291016
Validation loss: 2.8392055111546672

Epoch: 5| Step: 6
Training loss: 3.1135051250457764
Validation loss: 2.838776283366706

Epoch: 5| Step: 7
Training loss: 2.76202130317688
Validation loss: 2.840746251485681

Epoch: 5| Step: 8
Training loss: 3.1486153602600098
Validation loss: 2.8367260015139015

Epoch: 5| Step: 9
Training loss: 2.2287421226501465
Validation loss: 2.8447401267226025

Epoch: 5| Step: 10
Training loss: 3.0789778232574463
Validation loss: 2.8398013422566075

Epoch: 47| Step: 0
Training loss: 2.914807081222534
Validation loss: 2.8377001413735012

Epoch: 5| Step: 1
Training loss: 3.114269733428955
Validation loss: 2.8364507126551803

Epoch: 5| Step: 2
Training loss: 2.9364123344421387
Validation loss: 2.837005497306906

Epoch: 5| Step: 3
Training loss: 2.653740406036377
Validation loss: 2.834331714978782

Epoch: 5| Step: 4
Training loss: 3.2816600799560547
Validation loss: 2.836088272833055

Epoch: 5| Step: 5
Training loss: 2.7254812717437744
Validation loss: 2.8318663079251527

Epoch: 5| Step: 6
Training loss: 3.2316291332244873
Validation loss: 2.829001616406184

Epoch: 5| Step: 7
Training loss: 3.16896390914917
Validation loss: 2.827260886469195

Epoch: 5| Step: 8
Training loss: 3.283524751663208
Validation loss: 2.8319921698621524

Epoch: 5| Step: 9
Training loss: 2.6334147453308105
Validation loss: 2.8310249749050347

Epoch: 5| Step: 10
Training loss: 2.260833501815796
Validation loss: 2.8315618730360463

Epoch: 48| Step: 0
Training loss: 3.184244155883789
Validation loss: 2.8330660609788794

Epoch: 5| Step: 1
Training loss: 3.6383280754089355
Validation loss: 2.841795136851649

Epoch: 5| Step: 2
Training loss: 2.905299663543701
Validation loss: 2.844555870179207

Epoch: 5| Step: 3
Training loss: 2.9500536918640137
Validation loss: 2.843045634608115

Epoch: 5| Step: 4
Training loss: 2.6142399311065674
Validation loss: 2.8407753590614564

Epoch: 5| Step: 5
Training loss: 2.4791626930236816
Validation loss: 2.8342267108219925

Epoch: 5| Step: 6
Training loss: 2.697150468826294
Validation loss: 2.829577366511027

Epoch: 5| Step: 7
Training loss: 3.022721767425537
Validation loss: 2.827838308067732

Epoch: 5| Step: 8
Training loss: 2.569847583770752
Validation loss: 2.825829105992471

Epoch: 5| Step: 9
Training loss: 3.0919220447540283
Validation loss: 2.827243084548622

Epoch: 5| Step: 10
Training loss: 3.1032848358154297
Validation loss: 2.825604187544956

Epoch: 49| Step: 0
Training loss: 2.2329442501068115
Validation loss: 2.8270536597057054

Epoch: 5| Step: 1
Training loss: 3.0094850063323975
Validation loss: 2.8234865639799382

Epoch: 5| Step: 2
Training loss: 3.1284756660461426
Validation loss: 2.8265300002149356

Epoch: 5| Step: 3
Training loss: 2.362480640411377
Validation loss: 2.821505187660135

Epoch: 5| Step: 4
Training loss: 2.750025510787964
Validation loss: 2.823038378069478

Epoch: 5| Step: 5
Training loss: 3.9326863288879395
Validation loss: 2.8251721064249673

Epoch: 5| Step: 6
Training loss: 2.517518997192383
Validation loss: 2.819925744046447

Epoch: 5| Step: 7
Training loss: 2.645329236984253
Validation loss: 2.8216327082726265

Epoch: 5| Step: 8
Training loss: 2.923475980758667
Validation loss: 2.820988480762769

Epoch: 5| Step: 9
Training loss: 3.0308032035827637
Validation loss: 2.8218691759212042

Epoch: 5| Step: 10
Training loss: 3.701117515563965
Validation loss: 2.822132518214564

Epoch: 50| Step: 0
Training loss: 2.6609764099121094
Validation loss: 2.822663430244692

Epoch: 5| Step: 1
Training loss: 2.8912389278411865
Validation loss: 2.820980241221766

Epoch: 5| Step: 2
Training loss: 2.988765239715576
Validation loss: 2.8214808228195354

Epoch: 5| Step: 3
Training loss: 2.563018321990967
Validation loss: 2.8258270320071968

Epoch: 5| Step: 4
Training loss: 2.8469767570495605
Validation loss: 2.8255692630685787

Epoch: 5| Step: 5
Training loss: 3.315899610519409
Validation loss: 2.8251337184700915

Epoch: 5| Step: 6
Training loss: 3.13905668258667
Validation loss: 2.828223100272558

Epoch: 5| Step: 7
Training loss: 3.0618672370910645
Validation loss: 2.825360575029927

Epoch: 5| Step: 8
Training loss: 2.7274959087371826
Validation loss: 2.82913193907789

Epoch: 5| Step: 9
Training loss: 2.9041671752929688
Validation loss: 2.8265422954354236

Epoch: 5| Step: 10
Training loss: 3.006431818008423
Validation loss: 2.827365070260981

Epoch: 51| Step: 0
Training loss: 3.0653796195983887
Validation loss: 2.825722122705111

Epoch: 5| Step: 1
Training loss: 2.43923020362854
Validation loss: 2.831300053545224

Epoch: 5| Step: 2
Training loss: 3.174882650375366
Validation loss: 2.8262804990173667

Epoch: 5| Step: 3
Training loss: 2.676391363143921
Validation loss: 2.8235340374772266

Epoch: 5| Step: 4
Training loss: 3.7276763916015625
Validation loss: 2.825023494740968

Epoch: 5| Step: 5
Training loss: 2.3286826610565186
Validation loss: 2.8231842133306686

Epoch: 5| Step: 6
Training loss: 2.9871532917022705
Validation loss: 2.8239309710841023

Epoch: 5| Step: 7
Training loss: 2.6691372394561768
Validation loss: 2.819558266670473

Epoch: 5| Step: 8
Training loss: 3.052056074142456
Validation loss: 2.81975826140373

Epoch: 5| Step: 9
Training loss: 2.6293530464172363
Validation loss: 2.8162252210801646

Epoch: 5| Step: 10
Training loss: 3.36570143699646
Validation loss: 2.8131997559660222

Epoch: 52| Step: 0
Training loss: 2.638789415359497
Validation loss: 2.8138943590143675

Epoch: 5| Step: 1
Training loss: 3.3119921684265137
Validation loss: 2.812973609534643

Epoch: 5| Step: 2
Training loss: 3.3428397178649902
Validation loss: 2.8140123377564135

Epoch: 5| Step: 3
Training loss: 3.0906810760498047
Validation loss: 2.8136910110391598

Epoch: 5| Step: 4
Training loss: 2.9633688926696777
Validation loss: 2.816863070252121

Epoch: 5| Step: 5
Training loss: 3.00423002243042
Validation loss: 2.8141134246703117

Epoch: 5| Step: 6
Training loss: 2.5653817653656006
Validation loss: 2.8120600279941352

Epoch: 5| Step: 7
Training loss: 3.4163360595703125
Validation loss: 2.808701730543567

Epoch: 5| Step: 8
Training loss: 2.2153139114379883
Validation loss: 2.8054183503632903

Epoch: 5| Step: 9
Training loss: 2.6571474075317383
Validation loss: 2.8062095616453435

Epoch: 5| Step: 10
Training loss: 2.769496440887451
Validation loss: 2.806829188459663

Epoch: 53| Step: 0
Training loss: 3.0880143642425537
Validation loss: 2.8113669016027965

Epoch: 5| Step: 1
Training loss: 2.496293544769287
Validation loss: 2.8131257103335474

Epoch: 5| Step: 2
Training loss: 2.3152873516082764
Validation loss: 2.8175554352421917

Epoch: 5| Step: 3
Training loss: 3.3141567707061768
Validation loss: 2.8224749616397324

Epoch: 5| Step: 4
Training loss: 3.4933199882507324
Validation loss: 2.829994922043175

Epoch: 5| Step: 5
Training loss: 2.4788496494293213
Validation loss: 2.8310004921369654

Epoch: 5| Step: 6
Training loss: 3.0474112033843994
Validation loss: 2.831413386970438

Epoch: 5| Step: 7
Training loss: 3.3118960857391357
Validation loss: 2.82193334128267

Epoch: 5| Step: 8
Training loss: 1.7727138996124268
Validation loss: 2.8196238369070072

Epoch: 5| Step: 9
Training loss: 2.887218475341797
Validation loss: 2.8157638016567437

Epoch: 5| Step: 10
Training loss: 3.9743854999542236
Validation loss: 2.8154308231928016

Epoch: 54| Step: 0
Training loss: 3.4890353679656982
Validation loss: 2.8129948185336207

Epoch: 5| Step: 1
Training loss: 2.6075901985168457
Validation loss: 2.8056989664672525

Epoch: 5| Step: 2
Training loss: 2.9289088249206543
Validation loss: 2.803892425311509

Epoch: 5| Step: 3
Training loss: 2.7099809646606445
Validation loss: 2.803987168496655

Epoch: 5| Step: 4
Training loss: 2.9156084060668945
Validation loss: 2.803432082617155

Epoch: 5| Step: 5
Training loss: 2.9831485748291016
Validation loss: 2.8028688276967695

Epoch: 5| Step: 6
Training loss: 2.9647176265716553
Validation loss: 2.8031802279974825

Epoch: 5| Step: 7
Training loss: 1.814522385597229
Validation loss: 2.803988395198699

Epoch: 5| Step: 8
Training loss: 3.087456464767456
Validation loss: 2.802980166609569

Epoch: 5| Step: 9
Training loss: 3.4375693798065186
Validation loss: 2.802054656449185

Epoch: 5| Step: 10
Training loss: 2.9922006130218506
Validation loss: 2.8012903659574446

Epoch: 55| Step: 0
Training loss: 2.926060199737549
Validation loss: 2.8010130108043714

Epoch: 5| Step: 1
Training loss: 2.815863847732544
Validation loss: 2.800637588706068

Epoch: 5| Step: 2
Training loss: 2.9378533363342285
Validation loss: 2.7995425654995825

Epoch: 5| Step: 3
Training loss: 2.9689862728118896
Validation loss: 2.797618789057578

Epoch: 5| Step: 4
Training loss: 3.1224522590637207
Validation loss: 2.7969998441716677

Epoch: 5| Step: 5
Training loss: 3.9010956287384033
Validation loss: 2.7963115451156453

Epoch: 5| Step: 6
Training loss: 2.4949746131896973
Validation loss: 2.796049782024917

Epoch: 5| Step: 7
Training loss: 2.8285491466522217
Validation loss: 2.7963385299969743

Epoch: 5| Step: 8
Training loss: 2.069498300552368
Validation loss: 2.7962856010724138

Epoch: 5| Step: 9
Training loss: 3.5703742504119873
Validation loss: 2.7979437356354087

Epoch: 5| Step: 10
Training loss: 2.109334707260132
Validation loss: 2.7970103961165234

Epoch: 56| Step: 0
Training loss: 3.0234789848327637
Validation loss: 2.7973151206970215

Epoch: 5| Step: 1
Training loss: 3.2909457683563232
Validation loss: 2.7926965246918383

Epoch: 5| Step: 2
Training loss: 2.9335169792175293
Validation loss: 2.796499767611104

Epoch: 5| Step: 3
Training loss: 2.8496785163879395
Validation loss: 2.7909773165179836

Epoch: 5| Step: 4
Training loss: 3.1802353858947754
Validation loss: 2.7922267631817888

Epoch: 5| Step: 5
Training loss: 2.844750165939331
Validation loss: 2.7936873948702248

Epoch: 5| Step: 6
Training loss: 2.4210660457611084
Validation loss: 2.793553329283191

Epoch: 5| Step: 7
Training loss: 2.9218382835388184
Validation loss: 2.792292748728106

Epoch: 5| Step: 8
Training loss: 3.1224818229675293
Validation loss: 2.789890104724515

Epoch: 5| Step: 9
Training loss: 2.4904532432556152
Validation loss: 2.788605341347315

Epoch: 5| Step: 10
Training loss: 2.8056716918945312
Validation loss: 2.7882842863759687

Epoch: 57| Step: 0
Training loss: 2.8040261268615723
Validation loss: 2.787531260521181

Epoch: 5| Step: 1
Training loss: 2.4028573036193848
Validation loss: 2.785571841783421

Epoch: 5| Step: 2
Training loss: 2.293473958969116
Validation loss: 2.7866894968094362

Epoch: 5| Step: 3
Training loss: 2.849954843521118
Validation loss: 2.7844977865936937

Epoch: 5| Step: 4
Training loss: 2.6685073375701904
Validation loss: 2.7858383399184032

Epoch: 5| Step: 5
Training loss: 2.6356394290924072
Validation loss: 2.7870123309473835

Epoch: 5| Step: 6
Training loss: 2.8670144081115723
Validation loss: 2.785713788001768

Epoch: 5| Step: 7
Training loss: 2.655236005783081
Validation loss: 2.789813003232402

Epoch: 5| Step: 8
Training loss: 3.329123020172119
Validation loss: 2.793212080514559

Epoch: 5| Step: 9
Training loss: 3.556107759475708
Validation loss: 2.78965732102753

Epoch: 5| Step: 10
Training loss: 3.874737501144409
Validation loss: 2.790659407133697

Epoch: 58| Step: 0
Training loss: 2.8368871212005615
Validation loss: 2.789356093252859

Epoch: 5| Step: 1
Training loss: 3.6972756385803223
Validation loss: 2.787343599463022

Epoch: 5| Step: 2
Training loss: 3.125150442123413
Validation loss: 2.787805034268287

Epoch: 5| Step: 3
Training loss: 2.8767313957214355
Validation loss: 2.787556089380736

Epoch: 5| Step: 4
Training loss: 2.210261821746826
Validation loss: 2.7863414825931674

Epoch: 5| Step: 5
Training loss: 3.557133197784424
Validation loss: 2.780368669058687

Epoch: 5| Step: 6
Training loss: 3.0262136459350586
Validation loss: 2.779761291319324

Epoch: 5| Step: 7
Training loss: 2.4799485206604004
Validation loss: 2.7804748473628873

Epoch: 5| Step: 8
Training loss: 2.728513240814209
Validation loss: 2.7786022258061234

Epoch: 5| Step: 9
Training loss: 2.5541610717773438
Validation loss: 2.7775280629434893

Epoch: 5| Step: 10
Training loss: 2.571489095687866
Validation loss: 2.775594408794116

Epoch: 59| Step: 0
Training loss: 2.7542524337768555
Validation loss: 2.776583369060229

Epoch: 5| Step: 1
Training loss: 3.568514585494995
Validation loss: 2.78082178485009

Epoch: 5| Step: 2
Training loss: 1.9532649517059326
Validation loss: 2.7787115766156103

Epoch: 5| Step: 3
Training loss: 3.4903111457824707
Validation loss: 2.777652702023906

Epoch: 5| Step: 4
Training loss: 2.672150135040283
Validation loss: 2.7763618884548062

Epoch: 5| Step: 5
Training loss: 3.062136173248291
Validation loss: 2.7786980264930317

Epoch: 5| Step: 6
Training loss: 2.2558491230010986
Validation loss: 2.777814216511224

Epoch: 5| Step: 7
Training loss: 3.352172374725342
Validation loss: 2.779492585889755

Epoch: 5| Step: 8
Training loss: 2.8460426330566406
Validation loss: 2.7806577913222776

Epoch: 5| Step: 9
Training loss: 2.7331626415252686
Validation loss: 2.782100080161966

Epoch: 5| Step: 10
Training loss: 3.023988723754883
Validation loss: 2.7831701411995837

Epoch: 60| Step: 0
Training loss: 3.0892090797424316
Validation loss: 2.783666585081367

Epoch: 5| Step: 1
Training loss: 2.6522459983825684
Validation loss: 2.7824546547346216

Epoch: 5| Step: 2
Training loss: 2.6109344959259033
Validation loss: 2.7771010398864746

Epoch: 5| Step: 3
Training loss: 2.789170503616333
Validation loss: 2.7762189424166115

Epoch: 5| Step: 4
Training loss: 2.830512523651123
Validation loss: 2.772231250680903

Epoch: 5| Step: 5
Training loss: 2.7488152980804443
Validation loss: 2.773122513166038

Epoch: 5| Step: 6
Training loss: 2.2970070838928223
Validation loss: 2.7708439801328923

Epoch: 5| Step: 7
Training loss: 3.025660991668701
Validation loss: 2.7711216224137174

Epoch: 5| Step: 8
Training loss: 3.09186053276062
Validation loss: 2.7723347627988426

Epoch: 5| Step: 9
Training loss: 3.4010238647460938
Validation loss: 2.7722666391762356

Epoch: 5| Step: 10
Training loss: 3.1615991592407227
Validation loss: 2.774758272273566

Epoch: 61| Step: 0
Training loss: 2.5577213764190674
Validation loss: 2.7715374410793348

Epoch: 5| Step: 1
Training loss: 2.707278251647949
Validation loss: 2.770040301866429

Epoch: 5| Step: 2
Training loss: 1.956361174583435
Validation loss: 2.7704436958477063

Epoch: 5| Step: 3
Training loss: 3.473051071166992
Validation loss: 2.771244610509565

Epoch: 5| Step: 4
Training loss: 3.0764007568359375
Validation loss: 2.7689288303416264

Epoch: 5| Step: 5
Training loss: 2.7778639793395996
Validation loss: 2.772329689354025

Epoch: 5| Step: 6
Training loss: 3.8513076305389404
Validation loss: 2.7690616807629986

Epoch: 5| Step: 7
Training loss: 2.8645527362823486
Validation loss: 2.7696641824578725

Epoch: 5| Step: 8
Training loss: 3.159496545791626
Validation loss: 2.769438100117509

Epoch: 5| Step: 9
Training loss: 2.1397716999053955
Validation loss: 2.7693116459795224

Epoch: 5| Step: 10
Training loss: 3.0400784015655518
Validation loss: 2.7720105904404835

Epoch: 62| Step: 0
Training loss: 2.4361939430236816
Validation loss: 2.768450388344385

Epoch: 5| Step: 1
Training loss: 3.60792875289917
Validation loss: 2.7675831804993334

Epoch: 5| Step: 2
Training loss: 2.335994243621826
Validation loss: 2.76315467075635

Epoch: 5| Step: 3
Training loss: 2.653137683868408
Validation loss: 2.763721619882891

Epoch: 5| Step: 4
Training loss: 2.8318753242492676
Validation loss: 2.7640766661654235

Epoch: 5| Step: 5
Training loss: 2.8082947731018066
Validation loss: 2.7659181651248725

Epoch: 5| Step: 6
Training loss: 2.7799715995788574
Validation loss: 2.7645843541750343

Epoch: 5| Step: 7
Training loss: 3.8250114917755127
Validation loss: 2.7642157257244153

Epoch: 5| Step: 8
Training loss: 2.8579583168029785
Validation loss: 2.76345799046178

Epoch: 5| Step: 9
Training loss: 2.863602876663208
Validation loss: 2.7677561775330575

Epoch: 5| Step: 10
Training loss: 2.4758594036102295
Validation loss: 2.7658382667008268

Epoch: 63| Step: 0
Training loss: 3.1039376258850098
Validation loss: 2.765158942950669

Epoch: 5| Step: 1
Training loss: 3.7611770629882812
Validation loss: 2.76823652175165

Epoch: 5| Step: 2
Training loss: 3.5592658519744873
Validation loss: 2.7694364183692524

Epoch: 5| Step: 3
Training loss: 2.460526466369629
Validation loss: 2.767262312673753

Epoch: 5| Step: 4
Training loss: 2.351762294769287
Validation loss: 2.7708024055727067

Epoch: 5| Step: 5
Training loss: 2.3018276691436768
Validation loss: 2.7611110082236667

Epoch: 5| Step: 6
Training loss: 2.823690891265869
Validation loss: 2.75534555988927

Epoch: 5| Step: 7
Training loss: 3.3666012287139893
Validation loss: 2.757542751168692

Epoch: 5| Step: 8
Training loss: 2.951874017715454
Validation loss: 2.7632107042497203

Epoch: 5| Step: 9
Training loss: 1.9858249425888062
Validation loss: 2.768995267088695

Epoch: 5| Step: 10
Training loss: 2.910839319229126
Validation loss: 2.7732009580058437

Epoch: 64| Step: 0
Training loss: 2.712576389312744
Validation loss: 2.769662362273021

Epoch: 5| Step: 1
Training loss: 3.2179183959960938
Validation loss: 2.7622793553977885

Epoch: 5| Step: 2
Training loss: 2.9931137561798096
Validation loss: 2.7646848309424614

Epoch: 5| Step: 3
Training loss: 3.1560604572296143
Validation loss: 2.7568742511092976

Epoch: 5| Step: 4
Training loss: 2.835266351699829
Validation loss: 2.7528552239941013

Epoch: 5| Step: 5
Training loss: 3.373228073120117
Validation loss: 2.752151202130061

Epoch: 5| Step: 6
Training loss: 2.76961612701416
Validation loss: 2.7507974076014694

Epoch: 5| Step: 7
Training loss: 2.220716953277588
Validation loss: 2.7507419919454925

Epoch: 5| Step: 8
Training loss: 2.844517946243286
Validation loss: 2.752818984370078

Epoch: 5| Step: 9
Training loss: 2.236298084259033
Validation loss: 2.7539404489660777

Epoch: 5| Step: 10
Training loss: 3.199706554412842
Validation loss: 2.752612688208139

Epoch: 65| Step: 0
Training loss: 3.1542887687683105
Validation loss: 2.7585021270218717

Epoch: 5| Step: 1
Training loss: 2.870331048965454
Validation loss: 2.756889133043187

Epoch: 5| Step: 2
Training loss: 2.8317246437072754
Validation loss: 2.7602726310812016

Epoch: 5| Step: 3
Training loss: 2.2642550468444824
Validation loss: 2.7589948484974522

Epoch: 5| Step: 4
Training loss: 2.1310954093933105
Validation loss: 2.758502701277374

Epoch: 5| Step: 5
Training loss: 3.5653679370880127
Validation loss: 2.7588119173562653

Epoch: 5| Step: 6
Training loss: 2.807832956314087
Validation loss: 2.758896889225129

Epoch: 5| Step: 7
Training loss: 3.2230377197265625
Validation loss: 2.7482887621848815

Epoch: 5| Step: 8
Training loss: 2.9850430488586426
Validation loss: 2.7469614346822104

Epoch: 5| Step: 9
Training loss: 2.5174343585968018
Validation loss: 2.745408591403756

Epoch: 5| Step: 10
Training loss: 3.237016439437866
Validation loss: 2.748089792907879

Epoch: 66| Step: 0
Training loss: 3.095663070678711
Validation loss: 2.747379828524846

Epoch: 5| Step: 1
Training loss: 2.786531448364258
Validation loss: 2.7469427354874147

Epoch: 5| Step: 2
Training loss: 2.7978837490081787
Validation loss: 2.748150153826642

Epoch: 5| Step: 3
Training loss: 3.2883338928222656
Validation loss: 2.748459441687471

Epoch: 5| Step: 4
Training loss: 2.940908193588257
Validation loss: 2.7515531765517367

Epoch: 5| Step: 5
Training loss: 3.4594597816467285
Validation loss: 2.7524553627096195

Epoch: 5| Step: 6
Training loss: 2.042630195617676
Validation loss: 2.7531563107685377

Epoch: 5| Step: 7
Training loss: 3.2509636878967285
Validation loss: 2.7508701047589703

Epoch: 5| Step: 8
Training loss: 2.3231241703033447
Validation loss: 2.74627592743084

Epoch: 5| Step: 9
Training loss: 3.3423352241516113
Validation loss: 2.7423255110299714

Epoch: 5| Step: 10
Training loss: 2.136242151260376
Validation loss: 2.739263508909492

Epoch: 67| Step: 0
Training loss: 2.6482436656951904
Validation loss: 2.7378037206588255

Epoch: 5| Step: 1
Training loss: 1.885339379310608
Validation loss: 2.7426750377942155

Epoch: 5| Step: 2
Training loss: 2.9279277324676514
Validation loss: 2.745008099463678

Epoch: 5| Step: 3
Training loss: 3.099430799484253
Validation loss: 2.7484707909245647

Epoch: 5| Step: 4
Training loss: 3.385446071624756
Validation loss: 2.7481738085387857

Epoch: 5| Step: 5
Training loss: 3.4220824241638184
Validation loss: 2.754425856374925

Epoch: 5| Step: 6
Training loss: 2.8671715259552
Validation loss: 2.7475037728586504

Epoch: 5| Step: 7
Training loss: 2.771549701690674
Validation loss: 2.7504319888289257

Epoch: 5| Step: 8
Training loss: 3.1868324279785156
Validation loss: 2.747702221716604

Epoch: 5| Step: 9
Training loss: 2.0323004722595215
Validation loss: 2.7482249198421353

Epoch: 5| Step: 10
Training loss: 3.2307071685791016
Validation loss: 2.740249313333983

Epoch: 68| Step: 0
Training loss: 3.763277530670166
Validation loss: 2.7397916804077806

Epoch: 5| Step: 1
Training loss: 3.323424816131592
Validation loss: 2.7370926872376473

Epoch: 5| Step: 2
Training loss: 2.522871255874634
Validation loss: 2.7349396956864225

Epoch: 5| Step: 3
Training loss: 2.943761110305786
Validation loss: 2.7338294213817966

Epoch: 5| Step: 4
Training loss: 2.346019744873047
Validation loss: 2.7371627105179654

Epoch: 5| Step: 5
Training loss: 3.709583282470703
Validation loss: 2.736784332542009

Epoch: 5| Step: 6
Training loss: 1.912449598312378
Validation loss: 2.7384951986292356

Epoch: 5| Step: 7
Training loss: 2.400161027908325
Validation loss: 2.7391629449782835

Epoch: 5| Step: 8
Training loss: 2.2708652019500732
Validation loss: 2.7367296321417696

Epoch: 5| Step: 9
Training loss: 3.351975202560425
Validation loss: 2.7312370756621003

Epoch: 5| Step: 10
Training loss: 2.8987088203430176
Validation loss: 2.734160356624152

Epoch: 69| Step: 0
Training loss: 2.774365186691284
Validation loss: 2.730177056404852

Epoch: 5| Step: 1
Training loss: 2.7918219566345215
Validation loss: 2.732257484107889

Epoch: 5| Step: 2
Training loss: 2.8531341552734375
Validation loss: 2.73015276334619

Epoch: 5| Step: 3
Training loss: 3.1116268634796143
Validation loss: 2.733966983774657

Epoch: 5| Step: 4
Training loss: 2.367687940597534
Validation loss: 2.7359200190472346

Epoch: 5| Step: 5
Training loss: 3.336745500564575
Validation loss: 2.738830033168998

Epoch: 5| Step: 6
Training loss: 2.7155556678771973
Validation loss: 2.7341298416096675

Epoch: 5| Step: 7
Training loss: 2.9388251304626465
Validation loss: 2.733472385714131

Epoch: 5| Step: 8
Training loss: 2.4219348430633545
Validation loss: 2.7373243711327993

Epoch: 5| Step: 9
Training loss: 3.3363070487976074
Validation loss: 2.734018084823444

Epoch: 5| Step: 10
Training loss: 2.6407523155212402
Validation loss: 2.7316536288107596

Epoch: 70| Step: 0
Training loss: 2.8944485187530518
Validation loss: 2.7299375123875116

Epoch: 5| Step: 1
Training loss: 2.6876907348632812
Validation loss: 2.731718078736336

Epoch: 5| Step: 2
Training loss: 3.705296277999878
Validation loss: 2.7313846388170795

Epoch: 5| Step: 3
Training loss: 2.532654047012329
Validation loss: 2.7313985311856834

Epoch: 5| Step: 4
Training loss: 2.5853700637817383
Validation loss: 2.725571342693862

Epoch: 5| Step: 5
Training loss: 2.6392064094543457
Validation loss: 2.7249604271304224

Epoch: 5| Step: 6
Training loss: 3.094589948654175
Validation loss: 2.724607539433305

Epoch: 5| Step: 7
Training loss: 2.9298079013824463
Validation loss: 2.7266883157914683

Epoch: 5| Step: 8
Training loss: 2.496659278869629
Validation loss: 2.7244758272683747

Epoch: 5| Step: 9
Training loss: 2.4764556884765625
Validation loss: 2.7260510972751084

Epoch: 5| Step: 10
Training loss: 3.3182733058929443
Validation loss: 2.7264410372703307

Epoch: 71| Step: 0
Training loss: 2.8879306316375732
Validation loss: 2.726396547850742

Epoch: 5| Step: 1
Training loss: 3.488966464996338
Validation loss: 2.7252747986906316

Epoch: 5| Step: 2
Training loss: 3.1092991828918457
Validation loss: 2.724259853363037

Epoch: 5| Step: 3
Training loss: 2.4574332237243652
Validation loss: 2.7237231731414795

Epoch: 5| Step: 4
Training loss: 2.709097385406494
Validation loss: 2.7282118976757093

Epoch: 5| Step: 5
Training loss: 2.770962953567505
Validation loss: 2.730424798944945

Epoch: 5| Step: 6
Training loss: 2.6956939697265625
Validation loss: 2.7307220453857095

Epoch: 5| Step: 7
Training loss: 3.1440582275390625
Validation loss: 2.7269166874629196

Epoch: 5| Step: 8
Training loss: 3.390697479248047
Validation loss: 2.7357546308989167

Epoch: 5| Step: 9
Training loss: 1.9963626861572266
Validation loss: 2.7385169049744964

Epoch: 5| Step: 10
Training loss: 2.6028785705566406
Validation loss: 2.7324238951488207

Epoch: 72| Step: 0
Training loss: 2.5441784858703613
Validation loss: 2.7293187264473207

Epoch: 5| Step: 1
Training loss: 3.2781271934509277
Validation loss: 2.725515027200022

Epoch: 5| Step: 2
Training loss: 2.766829013824463
Validation loss: 2.7200259162533666

Epoch: 5| Step: 3
Training loss: 2.694835662841797
Validation loss: 2.718109707678518

Epoch: 5| Step: 4
Training loss: 2.5783944129943848
Validation loss: 2.7177420175203713

Epoch: 5| Step: 5
Training loss: 3.4304816722869873
Validation loss: 2.7168323455318326

Epoch: 5| Step: 6
Training loss: 2.390002965927124
Validation loss: 2.717031822409681

Epoch: 5| Step: 7
Training loss: 2.6153178215026855
Validation loss: 2.71564648484671

Epoch: 5| Step: 8
Training loss: 2.9797427654266357
Validation loss: 2.7141313501583633

Epoch: 5| Step: 9
Training loss: 2.968122720718384
Validation loss: 2.717295359539729

Epoch: 5| Step: 10
Training loss: 3.004236936569214
Validation loss: 2.7175021376661075

Epoch: 73| Step: 0
Training loss: 3.6065239906311035
Validation loss: 2.7175338601553314

Epoch: 5| Step: 1
Training loss: 2.76090145111084
Validation loss: 2.720647488870928

Epoch: 5| Step: 2
Training loss: 2.931316375732422
Validation loss: 2.727665619183612

Epoch: 5| Step: 3
Training loss: 3.214531660079956
Validation loss: 2.730957461941627

Epoch: 5| Step: 4
Training loss: 3.2522616386413574
Validation loss: 2.734190228164837

Epoch: 5| Step: 5
Training loss: 1.7626972198486328
Validation loss: 2.7376340461033646

Epoch: 5| Step: 6
Training loss: 2.1137325763702393
Validation loss: 2.725466843574278

Epoch: 5| Step: 7
Training loss: 3.509192705154419
Validation loss: 2.719507817299135

Epoch: 5| Step: 8
Training loss: 3.2165603637695312
Validation loss: 2.7137113796767367

Epoch: 5| Step: 9
Training loss: 2.000107765197754
Validation loss: 2.7141169296797885

Epoch: 5| Step: 10
Training loss: 2.8744938373565674
Validation loss: 2.7100827642666396

Epoch: 74| Step: 0
Training loss: 2.36130952835083
Validation loss: 2.7082955555249284

Epoch: 5| Step: 1
Training loss: 3.460015058517456
Validation loss: 2.709842756230344

Epoch: 5| Step: 2
Training loss: 3.5399296283721924
Validation loss: 2.7111308395221667

Epoch: 5| Step: 3
Training loss: 2.6579527854919434
Validation loss: 2.710157448245633

Epoch: 5| Step: 4
Training loss: 2.6650309562683105
Validation loss: 2.7136149611524356

Epoch: 5| Step: 5
Training loss: 2.926309108734131
Validation loss: 2.713452331481441

Epoch: 5| Step: 6
Training loss: 1.9501011371612549
Validation loss: 2.717588065772928

Epoch: 5| Step: 7
Training loss: 2.5237317085266113
Validation loss: 2.7140263844561834

Epoch: 5| Step: 8
Training loss: 3.0141584873199463
Validation loss: 2.7112288936491935

Epoch: 5| Step: 9
Training loss: 2.8314359188079834
Validation loss: 2.714825725042692

Epoch: 5| Step: 10
Training loss: 3.271688222885132
Validation loss: 2.7119269601760374

Epoch: 75| Step: 0
Training loss: 2.836132049560547
Validation loss: 2.7087481842246106

Epoch: 5| Step: 1
Training loss: 3.2797980308532715
Validation loss: 2.706583217907977

Epoch: 5| Step: 2
Training loss: 2.86099910736084
Validation loss: 2.7061484244561966

Epoch: 5| Step: 3
Training loss: 3.320211410522461
Validation loss: 2.705298851895076

Epoch: 5| Step: 4
Training loss: 3.0765604972839355
Validation loss: 2.7061984949214484

Epoch: 5| Step: 5
Training loss: 2.190211057662964
Validation loss: 2.70568561297591

Epoch: 5| Step: 6
Training loss: 2.3755550384521484
Validation loss: 2.709557161536268

Epoch: 5| Step: 7
Training loss: 2.991636037826538
Validation loss: 2.704344223904353

Epoch: 5| Step: 8
Training loss: 2.4670488834381104
Validation loss: 2.703330421960482

Epoch: 5| Step: 9
Training loss: 3.0383052825927734
Validation loss: 2.703841758030717

Epoch: 5| Step: 10
Training loss: 2.6327662467956543
Validation loss: 2.70451521360746

Epoch: 76| Step: 0
Training loss: 2.882944345474243
Validation loss: 2.7071370181216987

Epoch: 5| Step: 1
Training loss: 3.0909547805786133
Validation loss: 2.7127217118458082

Epoch: 5| Step: 2
Training loss: 2.6616714000701904
Validation loss: 2.7078948559299594

Epoch: 5| Step: 3
Training loss: 3.2782206535339355
Validation loss: 2.7094877817297496

Epoch: 5| Step: 4
Training loss: 2.9200220108032227
Validation loss: 2.7114117658266457

Epoch: 5| Step: 5
Training loss: 2.6972246170043945
Validation loss: 2.7066567021031536

Epoch: 5| Step: 6
Training loss: 2.5097928047180176
Validation loss: 2.7067212366288707

Epoch: 5| Step: 7
Training loss: 2.563842296600342
Validation loss: 2.707396045807869

Epoch: 5| Step: 8
Training loss: 2.3771281242370605
Validation loss: 2.7062224008703746

Epoch: 5| Step: 9
Training loss: 2.5621490478515625
Validation loss: 2.7062203268851004

Epoch: 5| Step: 10
Training loss: 3.6647510528564453
Validation loss: 2.7035819381795902

Epoch: 77| Step: 0
Training loss: 3.3406269550323486
Validation loss: 2.7037045212202173

Epoch: 5| Step: 1
Training loss: 2.113095760345459
Validation loss: 2.7032371131322717

Epoch: 5| Step: 2
Training loss: 2.9480605125427246
Validation loss: 2.70373398001476

Epoch: 5| Step: 3
Training loss: 3.0267395973205566
Validation loss: 2.7764142251783803

Epoch: 5| Step: 4
Training loss: 3.2379543781280518
Validation loss: 2.7913519336331274

Epoch: 5| Step: 5
Training loss: 3.794271945953369
Validation loss: 2.7896720388884186

Epoch: 5| Step: 6
Training loss: 2.1353535652160645
Validation loss: 2.784270930033858

Epoch: 5| Step: 7
Training loss: 2.0603854656219482
Validation loss: 2.777707689551897

Epoch: 5| Step: 8
Training loss: 2.9028213024139404
Validation loss: 2.7702323698228404

Epoch: 5| Step: 9
Training loss: 3.5590622425079346
Validation loss: 2.7702791434462353

Epoch: 5| Step: 10
Training loss: 2.3267040252685547
Validation loss: 2.7674632815904516

Epoch: 78| Step: 0
Training loss: 2.806384563446045
Validation loss: 2.7629630309279247

Epoch: 5| Step: 1
Training loss: 3.0237393379211426
Validation loss: 2.723520427621821

Epoch: 5| Step: 2
Training loss: 2.0914251804351807
Validation loss: 2.7742982910525416

Epoch: 5| Step: 3
Training loss: 2.624889373779297
Validation loss: 2.80585810189606

Epoch: 5| Step: 4
Training loss: 3.695005416870117
Validation loss: 2.823750388237738

Epoch: 5| Step: 5
Training loss: 2.6914422512054443
Validation loss: 2.7463008229450514

Epoch: 5| Step: 6
Training loss: 2.2046821117401123
Validation loss: 2.710393503148069

Epoch: 5| Step: 7
Training loss: 3.4306728839874268
Validation loss: 2.7224875521916214

Epoch: 5| Step: 8
Training loss: 3.0921683311462402
Validation loss: 2.7246098620917207

Epoch: 5| Step: 9
Training loss: 2.6927437782287598
Validation loss: 2.7151676249760452

Epoch: 5| Step: 10
Training loss: 3.0744926929473877
Validation loss: 2.698949311369209

Epoch: 79| Step: 0
Training loss: 2.435098171234131
Validation loss: 2.703925810834413

Epoch: 5| Step: 1
Training loss: 3.6099274158477783
Validation loss: 2.7084022106662875

Epoch: 5| Step: 2
Training loss: 2.850883960723877
Validation loss: 2.7015241704961306

Epoch: 5| Step: 3
Training loss: 2.8570761680603027
Validation loss: 2.702745473513039

Epoch: 5| Step: 4
Training loss: 1.9279224872589111
Validation loss: 2.7067401280967136

Epoch: 5| Step: 5
Training loss: 3.379462480545044
Validation loss: 2.7119578981912262

Epoch: 5| Step: 6
Training loss: 3.145801305770874
Validation loss: 2.7142982893092658

Epoch: 5| Step: 7
Training loss: 2.3095316886901855
Validation loss: 2.7094891173865205

Epoch: 5| Step: 8
Training loss: 2.314512252807617
Validation loss: 2.717203576077697

Epoch: 5| Step: 9
Training loss: 3.3368000984191895
Validation loss: 2.717919662434568

Epoch: 5| Step: 10
Training loss: 2.935229778289795
Validation loss: 2.715754544863137

Epoch: 80| Step: 0
Training loss: 2.947579860687256
Validation loss: 2.7155838781787502

Epoch: 5| Step: 1
Training loss: 2.6470043659210205
Validation loss: 2.7127278979106615

Epoch: 5| Step: 2
Training loss: 2.818135976791382
Validation loss: 2.7131828825960875

Epoch: 5| Step: 3
Training loss: 2.2491304874420166
Validation loss: 2.710347383253036

Epoch: 5| Step: 4
Training loss: 3.1558005809783936
Validation loss: 2.7128601740765315

Epoch: 5| Step: 5
Training loss: 3.1724321842193604
Validation loss: 2.705072428590508

Epoch: 5| Step: 6
Training loss: 2.8988945484161377
Validation loss: 2.7052238038791123

Epoch: 5| Step: 7
Training loss: 2.8425960540771484
Validation loss: 2.696918833640314

Epoch: 5| Step: 8
Training loss: 2.442100763320923
Validation loss: 2.69441795092757

Epoch: 5| Step: 9
Training loss: 2.845241069793701
Validation loss: 2.693947607471097

Epoch: 5| Step: 10
Training loss: 3.0966625213623047
Validation loss: 2.6974664657346663

Epoch: 81| Step: 0
Training loss: 2.875178813934326
Validation loss: 2.7000863552093506

Epoch: 5| Step: 1
Training loss: 3.262944459915161
Validation loss: 2.699934854302355

Epoch: 5| Step: 2
Training loss: 2.887042284011841
Validation loss: 2.6913223728056876

Epoch: 5| Step: 3
Training loss: 2.710329532623291
Validation loss: 2.6860181593125865

Epoch: 5| Step: 4
Training loss: 3.311660051345825
Validation loss: 2.6841730225470757

Epoch: 5| Step: 5
Training loss: 2.8881888389587402
Validation loss: 2.6894363741720877

Epoch: 5| Step: 6
Training loss: 2.319018840789795
Validation loss: 2.6887267610078216

Epoch: 5| Step: 7
Training loss: 2.7451891899108887
Validation loss: 2.692002163138441

Epoch: 5| Step: 8
Training loss: 3.2393813133239746
Validation loss: 2.6960522256871706

Epoch: 5| Step: 9
Training loss: 2.48134446144104
Validation loss: 2.693319777006744

Epoch: 5| Step: 10
Training loss: 2.2441530227661133
Validation loss: 2.6955587992104153

Epoch: 82| Step: 0
Training loss: 3.1586194038391113
Validation loss: 2.6970802276365218

Epoch: 5| Step: 1
Training loss: 2.967672348022461
Validation loss: 2.696718333869852

Epoch: 5| Step: 2
Training loss: 3.483346462249756
Validation loss: 2.6967914591553392

Epoch: 5| Step: 3
Training loss: 2.4496712684631348
Validation loss: 2.695766028537545

Epoch: 5| Step: 4
Training loss: 2.3567776679992676
Validation loss: 2.6919910036107546

Epoch: 5| Step: 5
Training loss: 2.2238850593566895
Validation loss: 2.6885037934908302

Epoch: 5| Step: 6
Training loss: 2.7683463096618652
Validation loss: 2.685391249195222

Epoch: 5| Step: 7
Training loss: 2.6297454833984375
Validation loss: 2.6767815748850503

Epoch: 5| Step: 8
Training loss: 3.0541012287139893
Validation loss: 2.6767541567484536

Epoch: 5| Step: 9
Training loss: 2.6909375190734863
Validation loss: 2.677523466848558

Epoch: 5| Step: 10
Training loss: 3.131793260574341
Validation loss: 2.6760156949361167

Epoch: 83| Step: 0
Training loss: 2.541384220123291
Validation loss: 2.6775429838447162

Epoch: 5| Step: 1
Training loss: 2.4028263092041016
Validation loss: 2.6779972276379986

Epoch: 5| Step: 2
Training loss: 3.0843546390533447
Validation loss: 2.67909433252068

Epoch: 5| Step: 3
Training loss: 2.7646892070770264
Validation loss: 2.678857259852912

Epoch: 5| Step: 4
Training loss: 2.529301166534424
Validation loss: 2.68115782994096

Epoch: 5| Step: 5
Training loss: 2.551525592803955
Validation loss: 2.6775362465971257

Epoch: 5| Step: 6
Training loss: 2.9368529319763184
Validation loss: 2.678070696451331

Epoch: 5| Step: 7
Training loss: 2.6054821014404297
Validation loss: 2.6749503227972213

Epoch: 5| Step: 8
Training loss: 3.3402748107910156
Validation loss: 2.6748599570284606

Epoch: 5| Step: 9
Training loss: 3.3227505683898926
Validation loss: 2.6748005472203737

Epoch: 5| Step: 10
Training loss: 2.829942464828491
Validation loss: 2.6720667090467227

Epoch: 84| Step: 0
Training loss: 2.880702495574951
Validation loss: 2.672901922656644

Epoch: 5| Step: 1
Training loss: 3.3697123527526855
Validation loss: 2.6743301268546813

Epoch: 5| Step: 2
Training loss: 3.132941484451294
Validation loss: 2.6736142148253736

Epoch: 5| Step: 3
Training loss: 2.9465761184692383
Validation loss: 2.677705905770743

Epoch: 5| Step: 4
Training loss: 2.3097593784332275
Validation loss: 2.6790633304144746

Epoch: 5| Step: 5
Training loss: 1.7499618530273438
Validation loss: 2.6792569122006817

Epoch: 5| Step: 6
Training loss: 2.5597777366638184
Validation loss: 2.6816747803841867

Epoch: 5| Step: 7
Training loss: 3.2712960243225098
Validation loss: 2.6932576779396302

Epoch: 5| Step: 8
Training loss: 2.4011788368225098
Validation loss: 2.6913618682533182

Epoch: 5| Step: 9
Training loss: 3.440608263015747
Validation loss: 2.692596076637186

Epoch: 5| Step: 10
Training loss: 2.859689474105835
Validation loss: 2.6750460875931608

Epoch: 85| Step: 0
Training loss: 3.0137197971343994
Validation loss: 2.6746846732272895

Epoch: 5| Step: 1
Training loss: 3.064558267593384
Validation loss: 2.665434140031056

Epoch: 5| Step: 2
Training loss: 2.3573474884033203
Validation loss: 2.670754027623002

Epoch: 5| Step: 3
Training loss: 2.743880033493042
Validation loss: 2.670953978774368

Epoch: 5| Step: 4
Training loss: 2.43867826461792
Validation loss: 2.669920949525731

Epoch: 5| Step: 5
Training loss: 2.7292211055755615
Validation loss: 2.6697989279224026

Epoch: 5| Step: 6
Training loss: 2.2929139137268066
Validation loss: 2.6710278449519986

Epoch: 5| Step: 7
Training loss: 3.7801966667175293
Validation loss: 2.667742418986495

Epoch: 5| Step: 8
Training loss: 2.773322582244873
Validation loss: 2.6680742258666665

Epoch: 5| Step: 9
Training loss: 2.7041842937469482
Validation loss: 2.6674839963195143

Epoch: 5| Step: 10
Training loss: 2.999995708465576
Validation loss: 2.6676530914921917

Epoch: 86| Step: 0
Training loss: 2.9128053188323975
Validation loss: 2.673877998064923

Epoch: 5| Step: 1
Training loss: 3.187870979309082
Validation loss: 2.6731289714895268

Epoch: 5| Step: 2
Training loss: 2.03786039352417
Validation loss: 2.664710480679748

Epoch: 5| Step: 3
Training loss: 2.827249050140381
Validation loss: 2.660538024799798

Epoch: 5| Step: 4
Training loss: 2.746067523956299
Validation loss: 2.6637224228151384

Epoch: 5| Step: 5
Training loss: 2.5748283863067627
Validation loss: 2.664702659012169

Epoch: 5| Step: 6
Training loss: 2.613969326019287
Validation loss: 2.6623848868954565

Epoch: 5| Step: 7
Training loss: 2.72615647315979
Validation loss: 2.6615672290966077

Epoch: 5| Step: 8
Training loss: 3.330739974975586
Validation loss: 2.661930032955703

Epoch: 5| Step: 9
Training loss: 3.2322006225585938
Validation loss: 2.6656211883791032

Epoch: 5| Step: 10
Training loss: 2.59037446975708
Validation loss: 2.665273025471677

Epoch: 87| Step: 0
Training loss: 2.3379626274108887
Validation loss: 2.6661027349451536

Epoch: 5| Step: 1
Training loss: 3.6137213706970215
Validation loss: 2.6627506107412358

Epoch: 5| Step: 2
Training loss: 2.0299859046936035
Validation loss: 2.6634167137966362

Epoch: 5| Step: 3
Training loss: 2.5322606563568115
Validation loss: 2.6614404442489787

Epoch: 5| Step: 4
Training loss: 2.4465954303741455
Validation loss: 2.659235644084151

Epoch: 5| Step: 5
Training loss: 2.1466832160949707
Validation loss: 2.6597381919942875

Epoch: 5| Step: 6
Training loss: 3.2660107612609863
Validation loss: 2.6573828035785305

Epoch: 5| Step: 7
Training loss: 3.3027966022491455
Validation loss: 2.657339906179777

Epoch: 5| Step: 8
Training loss: 3.036280870437622
Validation loss: 2.6591142095545286

Epoch: 5| Step: 9
Training loss: 3.0895066261291504
Validation loss: 2.655676862244965

Epoch: 5| Step: 10
Training loss: 3.003316879272461
Validation loss: 2.6589193523571057

Epoch: 88| Step: 0
Training loss: 2.3743197917938232
Validation loss: 2.6583575215390933

Epoch: 5| Step: 1
Training loss: 3.1794185638427734
Validation loss: 2.659428619569348

Epoch: 5| Step: 2
Training loss: 1.6142288446426392
Validation loss: 2.6575975289908786

Epoch: 5| Step: 3
Training loss: 3.1370339393615723
Validation loss: 2.6566156238637944

Epoch: 5| Step: 4
Training loss: 2.900725841522217
Validation loss: 2.6576536804117183

Epoch: 5| Step: 5
Training loss: 2.4758658409118652
Validation loss: 2.6558410070275746

Epoch: 5| Step: 6
Training loss: 3.433596134185791
Validation loss: 2.656482991351876

Epoch: 5| Step: 7
Training loss: 2.8979105949401855
Validation loss: 2.6562236355197046

Epoch: 5| Step: 8
Training loss: 3.0881378650665283
Validation loss: 2.6580642910413843

Epoch: 5| Step: 9
Training loss: 2.333012104034424
Validation loss: 2.6583496883351314

Epoch: 5| Step: 10
Training loss: 3.406532049179077
Validation loss: 2.6560451574223016

Epoch: 89| Step: 0
Training loss: 3.086595058441162
Validation loss: 2.655286676140242

Epoch: 5| Step: 1
Training loss: 2.4729602336883545
Validation loss: 2.6540670651261524

Epoch: 5| Step: 2
Training loss: 2.9924864768981934
Validation loss: 2.65372157865955

Epoch: 5| Step: 3
Training loss: 2.555629014968872
Validation loss: 2.658309357140654

Epoch: 5| Step: 4
Training loss: 3.0834603309631348
Validation loss: 2.66021324998589

Epoch: 5| Step: 5
Training loss: 2.87109637260437
Validation loss: 2.663463738656813

Epoch: 5| Step: 6
Training loss: 2.5215935707092285
Validation loss: 2.657693188677552

Epoch: 5| Step: 7
Training loss: 2.973958969116211
Validation loss: 2.6575315665173274

Epoch: 5| Step: 8
Training loss: 2.1618056297302246
Validation loss: 2.6570438133772982

Epoch: 5| Step: 9
Training loss: 2.6323328018188477
Validation loss: 2.654174530377952

Epoch: 5| Step: 10
Training loss: 3.4678032398223877
Validation loss: 2.6679490791854037

Epoch: 90| Step: 0
Training loss: 2.905623197555542
Validation loss: 2.6519615316903717

Epoch: 5| Step: 1
Training loss: 2.724024534225464
Validation loss: 2.6511327502548054

Epoch: 5| Step: 2
Training loss: 2.883960723876953
Validation loss: 2.6522780054359028

Epoch: 5| Step: 3
Training loss: 2.611311912536621
Validation loss: 2.6506615864333285

Epoch: 5| Step: 4
Training loss: 3.33081316947937
Validation loss: 2.6748090174890335

Epoch: 5| Step: 5
Training loss: 3.0196032524108887
Validation loss: 2.6558064952973397

Epoch: 5| Step: 6
Training loss: 2.6052653789520264
Validation loss: 2.6484705735278387

Epoch: 5| Step: 7
Training loss: 2.4145970344543457
Validation loss: 2.648496889298962

Epoch: 5| Step: 8
Training loss: 2.837043285369873
Validation loss: 2.64796253173582

Epoch: 5| Step: 9
Training loss: 3.0993759632110596
Validation loss: 2.6466024614149526

Epoch: 5| Step: 10
Training loss: 2.2376813888549805
Validation loss: 2.6483708812344458

Epoch: 91| Step: 0
Training loss: 2.6169095039367676
Validation loss: 2.649537194159723

Epoch: 5| Step: 1
Training loss: 3.3890602588653564
Validation loss: 2.6494639176194386

Epoch: 5| Step: 2
Training loss: 2.2353312969207764
Validation loss: 2.647289106922765

Epoch: 5| Step: 3
Training loss: 2.525298595428467
Validation loss: 2.6484838865136586

Epoch: 5| Step: 4
Training loss: 2.812685489654541
Validation loss: 2.6476300224181144

Epoch: 5| Step: 5
Training loss: 2.878000020980835
Validation loss: 2.651764174943329

Epoch: 5| Step: 6
Training loss: 3.055973529815674
Validation loss: 2.650590660751507

Epoch: 5| Step: 7
Training loss: 2.8694026470184326
Validation loss: 2.653248207543486

Epoch: 5| Step: 8
Training loss: 2.009897232055664
Validation loss: 2.6477516492207847

Epoch: 5| Step: 9
Training loss: 2.6802139282226562
Validation loss: 2.647001163933867

Epoch: 5| Step: 10
Training loss: 3.748591661453247
Validation loss: 2.6453888108653407

Epoch: 92| Step: 0
Training loss: 2.1310575008392334
Validation loss: 2.645019269758655

Epoch: 5| Step: 1
Training loss: 2.2185328006744385
Validation loss: 2.645140709415559

Epoch: 5| Step: 2
Training loss: 3.188323974609375
Validation loss: 2.643258233224192

Epoch: 5| Step: 3
Training loss: 3.038506031036377
Validation loss: 2.644856422178207

Epoch: 5| Step: 4
Training loss: 3.196676731109619
Validation loss: 2.6427214453297276

Epoch: 5| Step: 5
Training loss: 2.6777729988098145
Validation loss: 2.642836865558419

Epoch: 5| Step: 6
Training loss: 2.9136803150177
Validation loss: 2.64138735494306

Epoch: 5| Step: 7
Training loss: 2.289672374725342
Validation loss: 2.643516222635905

Epoch: 5| Step: 8
Training loss: 3.3597915172576904
Validation loss: 2.6413120659448768

Epoch: 5| Step: 9
Training loss: 3.003282070159912
Validation loss: 2.6409572350081576

Epoch: 5| Step: 10
Training loss: 2.5574872493743896
Validation loss: 2.640986201583698

Epoch: 93| Step: 0
Training loss: 2.9650888442993164
Validation loss: 2.6414267042631745

Epoch: 5| Step: 1
Training loss: 2.5385243892669678
Validation loss: 2.6420694858797136

Epoch: 5| Step: 2
Training loss: 2.739811897277832
Validation loss: 2.638890343327676

Epoch: 5| Step: 3
Training loss: 2.68921160697937
Validation loss: 2.638214798383815

Epoch: 5| Step: 4
Training loss: 2.812499523162842
Validation loss: 2.641134859413229

Epoch: 5| Step: 5
Training loss: 2.1237080097198486
Validation loss: 2.6402035400431645

Epoch: 5| Step: 6
Training loss: 2.536597728729248
Validation loss: 2.6378564014229724

Epoch: 5| Step: 7
Training loss: 3.2693264484405518
Validation loss: 2.638102736524356

Epoch: 5| Step: 8
Training loss: 3.2345097064971924
Validation loss: 2.638137548200546

Epoch: 5| Step: 9
Training loss: 2.9616169929504395
Validation loss: 2.6388330177594255

Epoch: 5| Step: 10
Training loss: 2.6910414695739746
Validation loss: 2.6368570225213164

Epoch: 94| Step: 0
Training loss: 3.051449775695801
Validation loss: 2.6323023790954263

Epoch: 5| Step: 1
Training loss: 2.939763307571411
Validation loss: 2.6354185791425806

Epoch: 5| Step: 2
Training loss: 2.4965271949768066
Validation loss: 2.6360936549402054

Epoch: 5| Step: 3
Training loss: 2.348145008087158
Validation loss: 2.6351177307867233

Epoch: 5| Step: 4
Training loss: 2.9695842266082764
Validation loss: 2.6368935877277004

Epoch: 5| Step: 5
Training loss: 2.3164546489715576
Validation loss: 2.6383390067726054

Epoch: 5| Step: 6
Training loss: 2.756091594696045
Validation loss: 2.6369468422346216

Epoch: 5| Step: 7
Training loss: 3.5052719116210938
Validation loss: 2.634874438726774

Epoch: 5| Step: 8
Training loss: 2.650007963180542
Validation loss: 2.633807484821607

Epoch: 5| Step: 9
Training loss: 2.61775279045105
Validation loss: 2.6305221665290093

Epoch: 5| Step: 10
Training loss: 2.91798996925354
Validation loss: 2.631936773177116

Epoch: 95| Step: 0
Training loss: 3.086808681488037
Validation loss: 2.6321111058676117

Epoch: 5| Step: 1
Training loss: 2.9176888465881348
Validation loss: 2.6331827153441725

Epoch: 5| Step: 2
Training loss: 2.884174346923828
Validation loss: 2.6350873003723803

Epoch: 5| Step: 3
Training loss: 2.028784990310669
Validation loss: 2.6345215997388287

Epoch: 5| Step: 4
Training loss: 2.2504472732543945
Validation loss: 2.6338171446195213

Epoch: 5| Step: 5
Training loss: 2.4912774562835693
Validation loss: 2.630339648133965

Epoch: 5| Step: 6
Training loss: 2.7448198795318604
Validation loss: 2.6310543860158613

Epoch: 5| Step: 7
Training loss: 3.1861464977264404
Validation loss: 2.629757529945784

Epoch: 5| Step: 8
Training loss: 2.6665396690368652
Validation loss: 2.6287050067737536

Epoch: 5| Step: 9
Training loss: 3.1640045642852783
Validation loss: 2.6284802549628803

Epoch: 5| Step: 10
Training loss: 3.1754722595214844
Validation loss: 2.628045476892943

Epoch: 96| Step: 0
Training loss: 2.567354202270508
Validation loss: 2.6268322903622865

Epoch: 5| Step: 1
Training loss: 2.8949122428894043
Validation loss: 2.630623366243096

Epoch: 5| Step: 2
Training loss: 3.160919189453125
Validation loss: 2.6291693807930074

Epoch: 5| Step: 3
Training loss: 2.699470043182373
Validation loss: 2.6259744218600694

Epoch: 5| Step: 4
Training loss: 2.2612922191619873
Validation loss: 2.628298049331993

Epoch: 5| Step: 5
Training loss: 2.852391481399536
Validation loss: 2.6235382992734193

Epoch: 5| Step: 6
Training loss: 3.5641281604766846
Validation loss: 2.6253069190568823

Epoch: 5| Step: 7
Training loss: 2.3657212257385254
Validation loss: 2.627336140601866

Epoch: 5| Step: 8
Training loss: 3.2433876991271973
Validation loss: 2.6253920498714653

Epoch: 5| Step: 9
Training loss: 2.2134957313537598
Validation loss: 2.626326148227979

Epoch: 5| Step: 10
Training loss: 2.6652956008911133
Validation loss: 2.628202115335772

Epoch: 97| Step: 0
Training loss: 2.8631138801574707
Validation loss: 2.6285380394228044

Epoch: 5| Step: 1
Training loss: 3.7788829803466797
Validation loss: 2.625297320786343

Epoch: 5| Step: 2
Training loss: 2.638932704925537
Validation loss: 2.625045758421703

Epoch: 5| Step: 3
Training loss: 2.1752686500549316
Validation loss: 2.625048837354106

Epoch: 5| Step: 4
Training loss: 3.4961791038513184
Validation loss: 2.624888276541105

Epoch: 5| Step: 5
Training loss: 2.8890206813812256
Validation loss: 2.6252880891164145

Epoch: 5| Step: 6
Training loss: 2.7328734397888184
Validation loss: 2.6230381586218394

Epoch: 5| Step: 7
Training loss: 2.370558738708496
Validation loss: 2.6234476079222975

Epoch: 5| Step: 8
Training loss: 2.3506205081939697
Validation loss: 2.626499629789783

Epoch: 5| Step: 9
Training loss: 2.9411447048187256
Validation loss: 2.6236765461583293

Epoch: 5| Step: 10
Training loss: 2.090376377105713
Validation loss: 2.619597832361857

Epoch: 98| Step: 0
Training loss: 2.9815611839294434
Validation loss: 2.621882612987231

Epoch: 5| Step: 1
Training loss: 2.858478546142578
Validation loss: 2.619531185396256

Epoch: 5| Step: 2
Training loss: 2.6320464611053467
Validation loss: 2.6216202756410003

Epoch: 5| Step: 3
Training loss: 2.8276522159576416
Validation loss: 2.625302101976128

Epoch: 5| Step: 4
Training loss: 2.480841875076294
Validation loss: 2.620712075182187

Epoch: 5| Step: 5
Training loss: 2.3226964473724365
Validation loss: 2.6207336353999313

Epoch: 5| Step: 6
Training loss: 3.1080431938171387
Validation loss: 2.622051336432016

Epoch: 5| Step: 7
Training loss: 2.2267580032348633
Validation loss: 2.626198989088817

Epoch: 5| Step: 8
Training loss: 3.5071654319763184
Validation loss: 2.6207285952824417

Epoch: 5| Step: 9
Training loss: 2.464174747467041
Validation loss: 2.6229665074297177

Epoch: 5| Step: 10
Training loss: 3.069117307662964
Validation loss: 2.61833833366312

Epoch: 99| Step: 0
Training loss: 2.1873345375061035
Validation loss: 2.618940714866884

Epoch: 5| Step: 1
Training loss: 2.9111690521240234
Validation loss: 2.621431294307914

Epoch: 5| Step: 2
Training loss: 2.7031173706054688
Validation loss: 2.6261340059259886

Epoch: 5| Step: 3
Training loss: 2.409719467163086
Validation loss: 2.628204999431487

Epoch: 5| Step: 4
Training loss: 2.864840269088745
Validation loss: 2.6273635946294314

Epoch: 5| Step: 5
Training loss: 2.7168030738830566
Validation loss: 2.63132910061908

Epoch: 5| Step: 6
Training loss: 3.149787425994873
Validation loss: 2.629548272778911

Epoch: 5| Step: 7
Training loss: 3.5485527515411377
Validation loss: 2.627202962034492

Epoch: 5| Step: 8
Training loss: 2.523742198944092
Validation loss: 2.623010999412947

Epoch: 5| Step: 9
Training loss: 2.540867805480957
Validation loss: 2.619416565023443

Epoch: 5| Step: 10
Training loss: 2.993823766708374
Validation loss: 2.620387969478484

Epoch: 100| Step: 0
Training loss: 2.9605469703674316
Validation loss: 2.6165280803557365

Epoch: 5| Step: 1
Training loss: 2.0747039318084717
Validation loss: 2.61616006717887

Epoch: 5| Step: 2
Training loss: 3.385005235671997
Validation loss: 2.6148096387104323

Epoch: 5| Step: 3
Training loss: 2.616485595703125
Validation loss: 2.616909647500643

Epoch: 5| Step: 4
Training loss: 2.4657907485961914
Validation loss: 2.6160075792702298

Epoch: 5| Step: 5
Training loss: 2.56585955619812
Validation loss: 2.6123290831042874

Epoch: 5| Step: 6
Training loss: 2.903247117996216
Validation loss: 2.6110760165799047

Epoch: 5| Step: 7
Training loss: 2.5633978843688965
Validation loss: 2.6120519125333397

Epoch: 5| Step: 8
Training loss: 2.9447474479675293
Validation loss: 2.6103759170860372

Epoch: 5| Step: 9
Training loss: 2.8339130878448486
Validation loss: 2.6155619852004515

Epoch: 5| Step: 10
Training loss: 3.1379685401916504
Validation loss: 2.6193304779709026

Epoch: 101| Step: 0
Training loss: 2.589888334274292
Validation loss: 2.6293921906461

Epoch: 5| Step: 1
Training loss: 3.0944180488586426
Validation loss: 2.649717951333651

Epoch: 5| Step: 2
Training loss: 2.2079861164093018
Validation loss: 2.6343727701453754

Epoch: 5| Step: 3
Training loss: 2.3291454315185547
Validation loss: 2.6145489728578957

Epoch: 5| Step: 4
Training loss: 3.125838279724121
Validation loss: 2.6116152501875356

Epoch: 5| Step: 5
Training loss: 3.338214159011841
Validation loss: 2.6147959052875476

Epoch: 5| Step: 6
Training loss: 2.1618237495422363
Validation loss: 2.6171318241344985

Epoch: 5| Step: 7
Training loss: 2.8563947677612305
Validation loss: 2.617452431750554

Epoch: 5| Step: 8
Training loss: 2.7744204998016357
Validation loss: 2.618976382799046

Epoch: 5| Step: 9
Training loss: 3.0461299419403076
Validation loss: 2.616739526871712

Epoch: 5| Step: 10
Training loss: 2.949230432510376
Validation loss: 2.619282325108846

Epoch: 102| Step: 0
Training loss: 2.9532694816589355
Validation loss: 2.627965109322661

Epoch: 5| Step: 1
Training loss: 2.9142651557922363
Validation loss: 2.6608832523386967

Epoch: 5| Step: 2
Training loss: 3.046299695968628
Validation loss: 2.7559440033410185

Epoch: 5| Step: 3
Training loss: 2.7201266288757324
Validation loss: 2.6677548987891084

Epoch: 5| Step: 4
Training loss: 2.7488837242126465
Validation loss: 2.656975858954973

Epoch: 5| Step: 5
Training loss: 1.7183030843734741
Validation loss: 2.6311202920893186

Epoch: 5| Step: 6
Training loss: 2.884228467941284
Validation loss: 2.6057551240408294

Epoch: 5| Step: 7
Training loss: 2.2491962909698486
Validation loss: 2.608415711310602

Epoch: 5| Step: 8
Training loss: 2.6324667930603027
Validation loss: 2.6197062435970513

Epoch: 5| Step: 9
Training loss: 3.451493740081787
Validation loss: 2.6526002140455347

Epoch: 5| Step: 10
Training loss: 3.451305627822876
Validation loss: 2.669739943678661

Epoch: 103| Step: 0
Training loss: 3.015188217163086
Validation loss: 2.692271524860013

Epoch: 5| Step: 1
Training loss: 3.2716376781463623
Validation loss: 2.654200277020854

Epoch: 5| Step: 2
Training loss: 3.247418165206909
Validation loss: 2.6166808092465965

Epoch: 5| Step: 3
Training loss: 2.7521755695343018
Validation loss: 2.6051998830610708

Epoch: 5| Step: 4
Training loss: 2.971590757369995
Validation loss: 2.604141181515109

Epoch: 5| Step: 5
Training loss: 2.347973585128784
Validation loss: 2.6061156488234

Epoch: 5| Step: 6
Training loss: 2.3797709941864014
Validation loss: 2.6084479362733903

Epoch: 5| Step: 7
Training loss: 2.26932430267334
Validation loss: 2.634881747666226

Epoch: 5| Step: 8
Training loss: 2.8310465812683105
Validation loss: 2.6814070722108245

Epoch: 5| Step: 9
Training loss: 3.143881320953369
Validation loss: 2.682640924248644

Epoch: 5| Step: 10
Training loss: 2.2945408821105957
Validation loss: 2.6402652212368545

Epoch: 104| Step: 0
Training loss: 2.4532737731933594
Validation loss: 2.623598360246228

Epoch: 5| Step: 1
Training loss: 2.976597309112549
Validation loss: 2.6181012815044773

Epoch: 5| Step: 2
Training loss: 3.0765326023101807
Validation loss: 2.6082761672235306

Epoch: 5| Step: 3
Training loss: 2.482213258743286
Validation loss: 2.6042130121620755

Epoch: 5| Step: 4
Training loss: 2.3529114723205566
Validation loss: 2.6027769837328183

Epoch: 5| Step: 5
Training loss: 2.9765634536743164
Validation loss: 2.6040682638845136

Epoch: 5| Step: 6
Training loss: 2.8234121799468994
Validation loss: 2.6036793288364204

Epoch: 5| Step: 7
Training loss: 2.7359683513641357
Validation loss: 2.604000101807297

Epoch: 5| Step: 8
Training loss: 2.3594653606414795
Validation loss: 2.6046825083353187

Epoch: 5| Step: 9
Training loss: 2.3244028091430664
Validation loss: 2.605812403463548

Epoch: 5| Step: 10
Training loss: 4.015175819396973
Validation loss: 2.608783357886858

Epoch: 105| Step: 0
Training loss: 1.574520468711853
Validation loss: 2.6091394860257386

Epoch: 5| Step: 1
Training loss: 2.989777088165283
Validation loss: 2.6053489818367908

Epoch: 5| Step: 2
Training loss: 3.408144474029541
Validation loss: 2.6077170090008805

Epoch: 5| Step: 3
Training loss: 3.0527000427246094
Validation loss: 2.6039444451691

Epoch: 5| Step: 4
Training loss: 2.674257755279541
Validation loss: 2.604424351005144

Epoch: 5| Step: 5
Training loss: 3.266019821166992
Validation loss: 2.604326673733291

Epoch: 5| Step: 6
Training loss: 2.3954379558563232
Validation loss: 2.6022483917974655

Epoch: 5| Step: 7
Training loss: 3.122393846511841
Validation loss: 2.6017749514631046

Epoch: 5| Step: 8
Training loss: 2.250417947769165
Validation loss: 2.605006499957013

Epoch: 5| Step: 9
Training loss: 2.91572904586792
Validation loss: 2.6041967638077272

Epoch: 5| Step: 10
Training loss: 2.629945755004883
Validation loss: 2.6000285584439515

Epoch: 106| Step: 0
Training loss: 3.8277182579040527
Validation loss: 2.6022111651717976

Epoch: 5| Step: 1
Training loss: 3.2282347679138184
Validation loss: 2.6023133185602005

Epoch: 5| Step: 2
Training loss: 2.331637144088745
Validation loss: 2.5985508221451954

Epoch: 5| Step: 3
Training loss: 2.8082056045532227
Validation loss: 2.5959215010366132

Epoch: 5| Step: 4
Training loss: 2.826451063156128
Validation loss: 2.595409595838157

Epoch: 5| Step: 5
Training loss: 1.7827905416488647
Validation loss: 2.595371061755765

Epoch: 5| Step: 6
Training loss: 2.8385605812072754
Validation loss: 2.5944366660169376

Epoch: 5| Step: 7
Training loss: 2.1431069374084473
Validation loss: 2.594917487072688

Epoch: 5| Step: 8
Training loss: 3.0287322998046875
Validation loss: 2.594095519793931

Epoch: 5| Step: 9
Training loss: 2.8083760738372803
Validation loss: 2.5946062405904136

Epoch: 5| Step: 10
Training loss: 2.626772880554199
Validation loss: 2.594866024550571

Epoch: 107| Step: 0
Training loss: 3.0416054725646973
Validation loss: 2.5944769946477746

Epoch: 5| Step: 1
Training loss: 2.5918149948120117
Validation loss: 2.593671373141709

Epoch: 5| Step: 2
Training loss: 2.56091570854187
Validation loss: 2.590853932083294

Epoch: 5| Step: 3
Training loss: 3.1351616382598877
Validation loss: 2.590496363178376

Epoch: 5| Step: 4
Training loss: 2.9006950855255127
Validation loss: 2.5883472888700423

Epoch: 5| Step: 5
Training loss: 2.824012279510498
Validation loss: 2.591873586818736

Epoch: 5| Step: 6
Training loss: 1.886013388633728
Validation loss: 2.5908125215961086

Epoch: 5| Step: 7
Training loss: 3.4007492065429688
Validation loss: 2.592570948344405

Epoch: 5| Step: 8
Training loss: 2.630511522293091
Validation loss: 2.5899529226364626

Epoch: 5| Step: 9
Training loss: 2.325535535812378
Validation loss: 2.5861179495370514

Epoch: 5| Step: 10
Training loss: 2.9591455459594727
Validation loss: 2.584868264454667

Epoch: 108| Step: 0
Training loss: 2.3561458587646484
Validation loss: 2.58920854906882

Epoch: 5| Step: 1
Training loss: 2.9792659282684326
Validation loss: 2.586128806555143

Epoch: 5| Step: 2
Training loss: 3.327953815460205
Validation loss: 2.5887329911672943

Epoch: 5| Step: 3
Training loss: 3.329369068145752
Validation loss: 2.5905844883252214

Epoch: 5| Step: 4
Training loss: 2.328444004058838
Validation loss: 2.5920896555787776

Epoch: 5| Step: 5
Training loss: 2.7789132595062256
Validation loss: 2.5878977083390757

Epoch: 5| Step: 6
Training loss: 1.9790045022964478
Validation loss: 2.588224882720619

Epoch: 5| Step: 7
Training loss: 3.1926207542419434
Validation loss: 2.5832284419767317

Epoch: 5| Step: 8
Training loss: 3.1314687728881836
Validation loss: 2.5877426747352845

Epoch: 5| Step: 9
Training loss: 2.3169898986816406
Validation loss: 2.589852738124068

Epoch: 5| Step: 10
Training loss: 2.4216253757476807
Validation loss: 2.5909474921482865

Epoch: 109| Step: 0
Training loss: 2.757964849472046
Validation loss: 2.5889515825497207

Epoch: 5| Step: 1
Training loss: 2.844580888748169
Validation loss: 2.586944718514719

Epoch: 5| Step: 2
Training loss: 3.1014716625213623
Validation loss: 2.5873367606952624

Epoch: 5| Step: 3
Training loss: 2.716773271560669
Validation loss: 2.588760773340861

Epoch: 5| Step: 4
Training loss: 2.4459750652313232
Validation loss: 2.586018769971786

Epoch: 5| Step: 5
Training loss: 2.764843463897705
Validation loss: 2.5946514709021455

Epoch: 5| Step: 6
Training loss: 3.008903741836548
Validation loss: 2.5934926463711645

Epoch: 5| Step: 7
Training loss: 2.6001248359680176
Validation loss: 2.5918526316201813

Epoch: 5| Step: 8
Training loss: 2.754763603210449
Validation loss: 2.5931778569375314

Epoch: 5| Step: 9
Training loss: 2.3886520862579346
Validation loss: 2.59247265836244

Epoch: 5| Step: 10
Training loss: 2.8404102325439453
Validation loss: 2.589382856122909

Epoch: 110| Step: 0
Training loss: 3.458402633666992
Validation loss: 2.5877747971524476

Epoch: 5| Step: 1
Training loss: 2.503012180328369
Validation loss: 2.5837977752890637

Epoch: 5| Step: 2
Training loss: 3.0832648277282715
Validation loss: 2.579430995448943

Epoch: 5| Step: 3
Training loss: 2.902228832244873
Validation loss: 2.5800696880586687

Epoch: 5| Step: 4
Training loss: 2.7572567462921143
Validation loss: 2.5814411845258487

Epoch: 5| Step: 5
Training loss: 2.081592559814453
Validation loss: 2.5777154404629945

Epoch: 5| Step: 6
Training loss: 2.674245834350586
Validation loss: 2.579820886734993

Epoch: 5| Step: 7
Training loss: 1.9191901683807373
Validation loss: 2.5790015984607

Epoch: 5| Step: 8
Training loss: 2.6509523391723633
Validation loss: 2.577725436097832

Epoch: 5| Step: 9
Training loss: 3.0321717262268066
Validation loss: 2.5771542851642897

Epoch: 5| Step: 10
Training loss: 3.1868093013763428
Validation loss: 2.578819456920829

Epoch: 111| Step: 0
Training loss: 2.1417930126190186
Validation loss: 2.576570792864728

Epoch: 5| Step: 1
Training loss: 3.3094921112060547
Validation loss: 2.580640774901195

Epoch: 5| Step: 2
Training loss: 2.4528262615203857
Validation loss: 2.581616060708159

Epoch: 5| Step: 3
Training loss: 2.339597702026367
Validation loss: 2.587855008340651

Epoch: 5| Step: 4
Training loss: 2.6751151084899902
Validation loss: 2.591558112893053

Epoch: 5| Step: 5
Training loss: 2.2422614097595215
Validation loss: 2.6018539551765687

Epoch: 5| Step: 6
Training loss: 3.3179550170898438
Validation loss: 2.6044808395447268

Epoch: 5| Step: 7
Training loss: 2.7094922065734863
Validation loss: 2.587820573519635

Epoch: 5| Step: 8
Training loss: 3.3378169536590576
Validation loss: 2.5867207768142864

Epoch: 5| Step: 9
Training loss: 2.45237135887146
Validation loss: 2.577089771147697

Epoch: 5| Step: 10
Training loss: 3.221754550933838
Validation loss: 2.5756398477861957

Epoch: 112| Step: 0
Training loss: 2.989063262939453
Validation loss: 2.5779140277575423

Epoch: 5| Step: 1
Training loss: 2.639878749847412
Validation loss: 2.582894189383394

Epoch: 5| Step: 2
Training loss: 3.050142288208008
Validation loss: 2.589928901323708

Epoch: 5| Step: 3
Training loss: 2.4081058502197266
Validation loss: 2.588167472552228

Epoch: 5| Step: 4
Training loss: 2.9921886920928955
Validation loss: 2.584550972907774

Epoch: 5| Step: 5
Training loss: 2.0302698612213135
Validation loss: 2.5804851901146675

Epoch: 5| Step: 6
Training loss: 2.9235966205596924
Validation loss: 2.5809762503511164

Epoch: 5| Step: 7
Training loss: 3.229971408843994
Validation loss: 2.5749270915985107

Epoch: 5| Step: 8
Training loss: 2.029445171356201
Validation loss: 2.5726249781988

Epoch: 5| Step: 9
Training loss: 3.140735149383545
Validation loss: 2.571644083146126

Epoch: 5| Step: 10
Training loss: 2.7297794818878174
Validation loss: 2.573595549470635

Epoch: 113| Step: 0
Training loss: 3.3411126136779785
Validation loss: 2.5698908580246793

Epoch: 5| Step: 1
Training loss: 2.2322463989257812
Validation loss: 2.570539674451274

Epoch: 5| Step: 2
Training loss: 3.052929162979126
Validation loss: 2.5715647384684575

Epoch: 5| Step: 3
Training loss: 3.1179566383361816
Validation loss: 2.5718115298978743

Epoch: 5| Step: 4
Training loss: 2.57226300239563
Validation loss: 2.5710018065667923

Epoch: 5| Step: 5
Training loss: 2.028897523880005
Validation loss: 2.5752694837508665

Epoch: 5| Step: 6
Training loss: 3.0341744422912598
Validation loss: 2.573080816576558

Epoch: 5| Step: 7
Training loss: 2.9022328853607178
Validation loss: 2.5701635319699525

Epoch: 5| Step: 8
Training loss: 2.5593209266662598
Validation loss: 2.5707346700852916

Epoch: 5| Step: 9
Training loss: 3.097121000289917
Validation loss: 2.568585370176582

Epoch: 5| Step: 10
Training loss: 2.044674873352051
Validation loss: 2.5696628068083074

Epoch: 114| Step: 0
Training loss: 3.395292282104492
Validation loss: 2.568039548012518

Epoch: 5| Step: 1
Training loss: 2.4100356101989746
Validation loss: 2.5683778973035913

Epoch: 5| Step: 2
Training loss: 3.2660083770751953
Validation loss: 2.5680903952608825

Epoch: 5| Step: 3
Training loss: 2.4283387660980225
Validation loss: 2.5646819594085857

Epoch: 5| Step: 4
Training loss: 2.0935721397399902
Validation loss: 2.563446424340689

Epoch: 5| Step: 5
Training loss: 2.7023160457611084
Validation loss: 2.562199177280549

Epoch: 5| Step: 6
Training loss: 2.316587448120117
Validation loss: 2.5642483952224895

Epoch: 5| Step: 7
Training loss: 3.058539867401123
Validation loss: 2.5646367765242055

Epoch: 5| Step: 8
Training loss: 2.2380211353302
Validation loss: 2.564638119871898

Epoch: 5| Step: 9
Training loss: 3.279167652130127
Validation loss: 2.565047553790513

Epoch: 5| Step: 10
Training loss: 2.871549129486084
Validation loss: 2.56303769798689

Epoch: 115| Step: 0
Training loss: 2.5304977893829346
Validation loss: 2.5627532723129436

Epoch: 5| Step: 1
Training loss: 2.4822657108306885
Validation loss: 2.5612833602454073

Epoch: 5| Step: 2
Training loss: 2.2442965507507324
Validation loss: 2.560543824267644

Epoch: 5| Step: 3
Training loss: 3.1256773471832275
Validation loss: 2.561220392104118

Epoch: 5| Step: 4
Training loss: 2.3387513160705566
Validation loss: 2.561859799969581

Epoch: 5| Step: 5
Training loss: 3.3956539630889893
Validation loss: 2.5599135224537184

Epoch: 5| Step: 6
Training loss: 3.0075793266296387
Validation loss: 2.5604996271030878

Epoch: 5| Step: 7
Training loss: 2.683638334274292
Validation loss: 2.56072300223894

Epoch: 5| Step: 8
Training loss: 2.945413589477539
Validation loss: 2.5601732653956257

Epoch: 5| Step: 9
Training loss: 2.7544360160827637
Validation loss: 2.5601779107124574

Epoch: 5| Step: 10
Training loss: 2.44484806060791
Validation loss: 2.560218813598797

Epoch: 116| Step: 0
Training loss: 2.356362819671631
Validation loss: 2.5603740471665577

Epoch: 5| Step: 1
Training loss: 3.058380603790283
Validation loss: 2.561125901437575

Epoch: 5| Step: 2
Training loss: 2.1864659786224365
Validation loss: 2.5689349507772796

Epoch: 5| Step: 3
Training loss: 2.466665506362915
Validation loss: 2.563651136172715

Epoch: 5| Step: 4
Training loss: 2.7919528484344482
Validation loss: 2.5599613343515704

Epoch: 5| Step: 5
Training loss: 2.5837440490722656
Validation loss: 2.5580900125606085

Epoch: 5| Step: 6
Training loss: 2.9388229846954346
Validation loss: 2.5573852344225814

Epoch: 5| Step: 7
Training loss: 2.8892087936401367
Validation loss: 2.5592502445302983

Epoch: 5| Step: 8
Training loss: 3.019704580307007
Validation loss: 2.558152009082097

Epoch: 5| Step: 9
Training loss: 2.359583616256714
Validation loss: 2.5580186664417224

Epoch: 5| Step: 10
Training loss: 3.452873468399048
Validation loss: 2.5588838618288756

Epoch: 117| Step: 0
Training loss: 3.2949366569519043
Validation loss: 2.5610415935516357

Epoch: 5| Step: 1
Training loss: 2.611929178237915
Validation loss: 2.563139251483384

Epoch: 5| Step: 2
Training loss: 2.7585110664367676
Validation loss: 2.5652504915832193

Epoch: 5| Step: 3
Training loss: 2.1738903522491455
Validation loss: 2.562566613638273

Epoch: 5| Step: 4
Training loss: 3.2619552612304688
Validation loss: 2.5655266495161158

Epoch: 5| Step: 5
Training loss: 2.6247878074645996
Validation loss: 2.5644332952396844

Epoch: 5| Step: 6
Training loss: 2.349921703338623
Validation loss: 2.561034184630199

Epoch: 5| Step: 7
Training loss: 2.872069835662842
Validation loss: 2.5648189129367953

Epoch: 5| Step: 8
Training loss: 2.8370721340179443
Validation loss: 2.5603898238110285

Epoch: 5| Step: 9
Training loss: 2.6261775493621826
Validation loss: 2.561624788468884

Epoch: 5| Step: 10
Training loss: 2.613164186477661
Validation loss: 2.5579495840175177

Epoch: 118| Step: 0
Training loss: 2.678699016571045
Validation loss: 2.555268572222802

Epoch: 5| Step: 1
Training loss: 2.571458101272583
Validation loss: 2.552725181784681

Epoch: 5| Step: 2
Training loss: 2.760741710662842
Validation loss: 2.555078539797055

Epoch: 5| Step: 3
Training loss: 3.181036949157715
Validation loss: 2.5563995607437624

Epoch: 5| Step: 4
Training loss: 2.7037196159362793
Validation loss: 2.555286386961578

Epoch: 5| Step: 5
Training loss: 3.5713932514190674
Validation loss: 2.5542857775124173

Epoch: 5| Step: 6
Training loss: 2.142209768295288
Validation loss: 2.5530625389468287

Epoch: 5| Step: 7
Training loss: 2.200847625732422
Validation loss: 2.554072595411731

Epoch: 5| Step: 8
Training loss: 2.440629482269287
Validation loss: 2.553471544737457

Epoch: 5| Step: 9
Training loss: 2.355412244796753
Validation loss: 2.551229946074947

Epoch: 5| Step: 10
Training loss: 3.4474518299102783
Validation loss: 2.54940975353282

Epoch: 119| Step: 0
Training loss: 2.8676657676696777
Validation loss: 2.5527193059203444

Epoch: 5| Step: 1
Training loss: 2.405812978744507
Validation loss: 2.551153685456963

Epoch: 5| Step: 2
Training loss: 2.2973742485046387
Validation loss: 2.5503147314953547

Epoch: 5| Step: 3
Training loss: 3.242436647415161
Validation loss: 2.5495303984611266

Epoch: 5| Step: 4
Training loss: 2.462531566619873
Validation loss: 2.5481235314440984

Epoch: 5| Step: 5
Training loss: 2.7964820861816406
Validation loss: 2.5483432046828733

Epoch: 5| Step: 6
Training loss: 2.5092904567718506
Validation loss: 2.5485103130340576

Epoch: 5| Step: 7
Training loss: 3.192162275314331
Validation loss: 2.5473949678482546

Epoch: 5| Step: 8
Training loss: 2.6635513305664062
Validation loss: 2.5483389464757775

Epoch: 5| Step: 9
Training loss: 2.746978759765625
Validation loss: 2.5492127864591536

Epoch: 5| Step: 10
Training loss: 2.727330207824707
Validation loss: 2.547605506835445

Epoch: 120| Step: 0
Training loss: 3.085486888885498
Validation loss: 2.556281046200824

Epoch: 5| Step: 1
Training loss: 1.9219688177108765
Validation loss: 2.55683470285067

Epoch: 5| Step: 2
Training loss: 2.443429470062256
Validation loss: 2.5541899076072117

Epoch: 5| Step: 3
Training loss: 2.87676739692688
Validation loss: 2.553146150804335

Epoch: 5| Step: 4
Training loss: 2.7724735736846924
Validation loss: 2.5502520735545824

Epoch: 5| Step: 5
Training loss: 3.2533981800079346
Validation loss: 2.546043478032594

Epoch: 5| Step: 6
Training loss: 3.3801586627960205
Validation loss: 2.546526188491493

Epoch: 5| Step: 7
Training loss: 2.436655044555664
Validation loss: 2.5442980438150387

Epoch: 5| Step: 8
Training loss: 2.199734687805176
Validation loss: 2.5463193001285678

Epoch: 5| Step: 9
Training loss: 2.898127794265747
Validation loss: 2.544775573156213

Epoch: 5| Step: 10
Training loss: 2.623350143432617
Validation loss: 2.5442557155445056

Epoch: 121| Step: 0
Training loss: 2.8192758560180664
Validation loss: 2.544713427943568

Epoch: 5| Step: 1
Training loss: 2.0675597190856934
Validation loss: 2.5433490814701205

Epoch: 5| Step: 2
Training loss: 3.0351929664611816
Validation loss: 2.5422603699468795

Epoch: 5| Step: 3
Training loss: 2.6528990268707275
Validation loss: 2.544499866424068

Epoch: 5| Step: 4
Training loss: 2.978121280670166
Validation loss: 2.5464231480834303

Epoch: 5| Step: 5
Training loss: 2.62119722366333
Validation loss: 2.5442467428022817

Epoch: 5| Step: 6
Training loss: 2.71125864982605
Validation loss: 2.552192782842985

Epoch: 5| Step: 7
Training loss: 3.4800567626953125
Validation loss: 2.5462777794048352

Epoch: 5| Step: 8
Training loss: 2.526731491088867
Validation loss: 2.5391336102639475

Epoch: 5| Step: 9
Training loss: 2.347677707672119
Validation loss: 2.5398956447519283

Epoch: 5| Step: 10
Training loss: 2.588400363922119
Validation loss: 2.5391369583786174

Epoch: 122| Step: 0
Training loss: 2.960142135620117
Validation loss: 2.5403651575888357

Epoch: 5| Step: 1
Training loss: 2.6005542278289795
Validation loss: 2.540161286630938

Epoch: 5| Step: 2
Training loss: 2.5403544902801514
Validation loss: 2.538308597380115

Epoch: 5| Step: 3
Training loss: 3.7718605995178223
Validation loss: 2.538236028404646

Epoch: 5| Step: 4
Training loss: 2.1105997562408447
Validation loss: 2.5374262153461413

Epoch: 5| Step: 5
Training loss: 2.882554531097412
Validation loss: 2.5364580949147544

Epoch: 5| Step: 6
Training loss: 2.4115216732025146
Validation loss: 2.536857875444556

Epoch: 5| Step: 7
Training loss: 2.2702698707580566
Validation loss: 2.5383231537316435

Epoch: 5| Step: 8
Training loss: 2.986969232559204
Validation loss: 2.5350523635905278

Epoch: 5| Step: 9
Training loss: 2.888005256652832
Validation loss: 2.5358238912397817

Epoch: 5| Step: 10
Training loss: 2.3774094581604004
Validation loss: 2.537756900633535

Epoch: 123| Step: 0
Training loss: 2.428027391433716
Validation loss: 2.5371822080304547

Epoch: 5| Step: 1
Training loss: 2.6514129638671875
Validation loss: 2.535627975258776

Epoch: 5| Step: 2
Training loss: 2.2648847103118896
Validation loss: 2.536285972082487

Epoch: 5| Step: 3
Training loss: 2.884406566619873
Validation loss: 2.5380550763940297

Epoch: 5| Step: 4
Training loss: 2.5814530849456787
Validation loss: 2.538691048981041

Epoch: 5| Step: 5
Training loss: 3.1562132835388184
Validation loss: 2.5354903846658687

Epoch: 5| Step: 6
Training loss: 3.0045342445373535
Validation loss: 2.5378003094785955

Epoch: 5| Step: 7
Training loss: 3.0314605236053467
Validation loss: 2.539623729644283

Epoch: 5| Step: 8
Training loss: 2.483692169189453
Validation loss: 2.5395712878114436

Epoch: 5| Step: 9
Training loss: 2.4931135177612305
Validation loss: 2.5408395080156225

Epoch: 5| Step: 10
Training loss: 2.8410704135894775
Validation loss: 2.542550192084364

Epoch: 124| Step: 0
Training loss: 3.0892438888549805
Validation loss: 2.544550193253384

Epoch: 5| Step: 1
Training loss: 2.515071153640747
Validation loss: 2.542074631619197

Epoch: 5| Step: 2
Training loss: 2.995633125305176
Validation loss: 2.5370818081722466

Epoch: 5| Step: 3
Training loss: 2.884739637374878
Validation loss: 2.5358335536013366

Epoch: 5| Step: 4
Training loss: 2.110602617263794
Validation loss: 2.5330423821685133

Epoch: 5| Step: 5
Training loss: 2.0852723121643066
Validation loss: 2.5317471950284895

Epoch: 5| Step: 6
Training loss: 2.567814350128174
Validation loss: 2.5301800466352895

Epoch: 5| Step: 7
Training loss: 2.5641844272613525
Validation loss: 2.528964636146381

Epoch: 5| Step: 8
Training loss: 3.3246567249298096
Validation loss: 2.531280545778172

Epoch: 5| Step: 9
Training loss: 2.8739840984344482
Validation loss: 2.531455885979437

Epoch: 5| Step: 10
Training loss: 2.799149513244629
Validation loss: 2.5290203914847424

Epoch: 125| Step: 0
Training loss: 2.834059238433838
Validation loss: 2.5306264636337117

Epoch: 5| Step: 1
Training loss: 2.707730770111084
Validation loss: 2.531691712717856

Epoch: 5| Step: 2
Training loss: 2.6944432258605957
Validation loss: 2.5295029609434065

Epoch: 5| Step: 3
Training loss: 2.0919175148010254
Validation loss: 2.5309172932819655

Epoch: 5| Step: 4
Training loss: 2.7607948780059814
Validation loss: 2.5278008778889975

Epoch: 5| Step: 5
Training loss: 2.3882546424865723
Validation loss: 2.5316528838167907

Epoch: 5| Step: 6
Training loss: 2.916825532913208
Validation loss: 2.5282066483651437

Epoch: 5| Step: 7
Training loss: 3.6166298389434814
Validation loss: 2.5273406864494405

Epoch: 5| Step: 8
Training loss: 2.152430295944214
Validation loss: 2.529830132761309

Epoch: 5| Step: 9
Training loss: 2.5501716136932373
Validation loss: 2.5296678209817536

Epoch: 5| Step: 10
Training loss: 3.14595103263855
Validation loss: 2.525642397583172

Epoch: 126| Step: 0
Training loss: 3.212193012237549
Validation loss: 2.525879362578033

Epoch: 5| Step: 1
Training loss: 3.2199807167053223
Validation loss: 2.5284217429417435

Epoch: 5| Step: 2
Training loss: 3.000382900238037
Validation loss: 2.527736136990209

Epoch: 5| Step: 3
Training loss: 2.3781914710998535
Validation loss: 2.52490754794049

Epoch: 5| Step: 4
Training loss: 2.632772207260132
Validation loss: 2.5258596815088743

Epoch: 5| Step: 5
Training loss: 2.2619235515594482
Validation loss: 2.52691045371435

Epoch: 5| Step: 6
Training loss: 2.520033359527588
Validation loss: 2.527747456745435

Epoch: 5| Step: 7
Training loss: 2.9089016914367676
Validation loss: 2.5256249084267566

Epoch: 5| Step: 8
Training loss: 2.474229335784912
Validation loss: 2.526031409540484

Epoch: 5| Step: 9
Training loss: 2.676802396774292
Validation loss: 2.5283735772614837

Epoch: 5| Step: 10
Training loss: 2.3960540294647217
Validation loss: 2.5267319679260254

Epoch: 127| Step: 0
Training loss: 2.6097779273986816
Validation loss: 2.5288793117769304

Epoch: 5| Step: 1
Training loss: 2.374660015106201
Validation loss: 2.5322751229809177

Epoch: 5| Step: 2
Training loss: 2.954216480255127
Validation loss: 2.528689461369668

Epoch: 5| Step: 3
Training loss: 2.520092487335205
Validation loss: 2.528387984921855

Epoch: 5| Step: 4
Training loss: 1.9975227117538452
Validation loss: 2.5265192703534196

Epoch: 5| Step: 5
Training loss: 2.747832775115967
Validation loss: 2.525780934159474

Epoch: 5| Step: 6
Training loss: 2.9349732398986816
Validation loss: 2.5230258152049077

Epoch: 5| Step: 7
Training loss: 2.870434522628784
Validation loss: 2.5250705698485016

Epoch: 5| Step: 8
Training loss: 3.2670700550079346
Validation loss: 2.5255726793760895

Epoch: 5| Step: 9
Training loss: 2.9116251468658447
Validation loss: 2.524161487497309

Epoch: 5| Step: 10
Training loss: 2.5270602703094482
Validation loss: 2.52394937956205

Epoch: 128| Step: 0
Training loss: 3.1877243518829346
Validation loss: 2.5239520560028734

Epoch: 5| Step: 1
Training loss: 2.8454480171203613
Validation loss: 2.522106401381954

Epoch: 5| Step: 2
Training loss: 2.229861259460449
Validation loss: 2.5220785833174184

Epoch: 5| Step: 3
Training loss: 2.356480836868286
Validation loss: 2.5266351238373788

Epoch: 5| Step: 4
Training loss: 2.423182249069214
Validation loss: 2.5287957550376974

Epoch: 5| Step: 5
Training loss: 2.2864015102386475
Validation loss: 2.5318656300985687

Epoch: 5| Step: 6
Training loss: 2.402195453643799
Validation loss: 2.5274196337628108

Epoch: 5| Step: 7
Training loss: 3.234524965286255
Validation loss: 2.5272934026615594

Epoch: 5| Step: 8
Training loss: 2.0607187747955322
Validation loss: 2.5270299168043238

Epoch: 5| Step: 9
Training loss: 3.4391140937805176
Validation loss: 2.5257752685136694

Epoch: 5| Step: 10
Training loss: 3.3489151000976562
Validation loss: 2.52331050749748

Epoch: 129| Step: 0
Training loss: 2.4124348163604736
Validation loss: 2.524775940884826

Epoch: 5| Step: 1
Training loss: 2.7749226093292236
Validation loss: 2.525445081854379

Epoch: 5| Step: 2
Training loss: 2.1311697959899902
Validation loss: 2.5290845645371305

Epoch: 5| Step: 3
Training loss: 3.3063273429870605
Validation loss: 2.525603235408824

Epoch: 5| Step: 4
Training loss: 2.4810752868652344
Validation loss: 2.535992537775347

Epoch: 5| Step: 5
Training loss: 3.2240214347839355
Validation loss: 2.5254961418849167

Epoch: 5| Step: 6
Training loss: 3.332728862762451
Validation loss: 2.520015590934343

Epoch: 5| Step: 7
Training loss: 1.9473435878753662
Validation loss: 2.518278709021948

Epoch: 5| Step: 8
Training loss: 2.856816530227661
Validation loss: 2.5208251232741983

Epoch: 5| Step: 9
Training loss: 1.9968864917755127
Validation loss: 2.5211299004093295

Epoch: 5| Step: 10
Training loss: 3.3341732025146484
Validation loss: 2.5248781327278382

Epoch: 130| Step: 0
Training loss: 3.0516409873962402
Validation loss: 2.5194740859411096

Epoch: 5| Step: 1
Training loss: 2.263564348220825
Validation loss: 2.520135724416343

Epoch: 5| Step: 2
Training loss: 2.789738178253174
Validation loss: 2.520227680924118

Epoch: 5| Step: 3
Training loss: 3.0743143558502197
Validation loss: 2.5213276801570768

Epoch: 5| Step: 4
Training loss: 3.126055955886841
Validation loss: 2.520754788511543

Epoch: 5| Step: 5
Training loss: 2.6592001914978027
Validation loss: 2.5217297666816303

Epoch: 5| Step: 6
Training loss: 2.6828627586364746
Validation loss: 2.521236022313436

Epoch: 5| Step: 7
Training loss: 2.680755615234375
Validation loss: 2.5188769525097263

Epoch: 5| Step: 8
Training loss: 2.613102674484253
Validation loss: 2.5154292327101513

Epoch: 5| Step: 9
Training loss: 2.452807903289795
Validation loss: 2.5168516200075866

Epoch: 5| Step: 10
Training loss: 2.202646255493164
Validation loss: 2.51560462931151

Epoch: 131| Step: 0
Training loss: 2.3814878463745117
Validation loss: 2.5181956829563266

Epoch: 5| Step: 1
Training loss: 2.640049695968628
Validation loss: 2.5142217169525805

Epoch: 5| Step: 2
Training loss: 2.5979807376861572
Validation loss: 2.518756766473093

Epoch: 5| Step: 3
Training loss: 2.413439989089966
Validation loss: 2.5160325855337162

Epoch: 5| Step: 4
Training loss: 2.1740524768829346
Validation loss: 2.5189009071678243

Epoch: 5| Step: 5
Training loss: 2.9426798820495605
Validation loss: 2.5223361189647386

Epoch: 5| Step: 6
Training loss: 3.9553890228271484
Validation loss: 2.521475450966948

Epoch: 5| Step: 7
Training loss: 2.4358174800872803
Validation loss: 2.5184259107035976

Epoch: 5| Step: 8
Training loss: 2.1910030841827393
Validation loss: 2.5158675332223215

Epoch: 5| Step: 9
Training loss: 3.1674959659576416
Validation loss: 2.5135747771109305

Epoch: 5| Step: 10
Training loss: 2.766741991043091
Validation loss: 2.5138362274375012

Epoch: 132| Step: 0
Training loss: 2.5297956466674805
Validation loss: 2.5122229437674246

Epoch: 5| Step: 1
Training loss: 2.4504103660583496
Validation loss: 2.5101057021848616

Epoch: 5| Step: 2
Training loss: 2.1075801849365234
Validation loss: 2.5112918474340953

Epoch: 5| Step: 3
Training loss: 2.869814872741699
Validation loss: 2.510907898667038

Epoch: 5| Step: 4
Training loss: 2.370781421661377
Validation loss: 2.5111816262686126

Epoch: 5| Step: 5
Training loss: 3.2530288696289062
Validation loss: 2.506796303615775

Epoch: 5| Step: 6
Training loss: 2.1609883308410645
Validation loss: 2.508378036560551

Epoch: 5| Step: 7
Training loss: 3.7333812713623047
Validation loss: 2.5102892101451917

Epoch: 5| Step: 8
Training loss: 2.3616297245025635
Validation loss: 2.5079427585806897

Epoch: 5| Step: 9
Training loss: 2.9981329441070557
Validation loss: 2.5101393371499996

Epoch: 5| Step: 10
Training loss: 2.82071590423584
Validation loss: 2.5087441295705815

Epoch: 133| Step: 0
Training loss: 2.3518471717834473
Validation loss: 2.5100152774523665

Epoch: 5| Step: 1
Training loss: 2.271167755126953
Validation loss: 2.5104476969729186

Epoch: 5| Step: 2
Training loss: 2.2883832454681396
Validation loss: 2.512951015144266

Epoch: 5| Step: 3
Training loss: 3.6486849784851074
Validation loss: 2.5133160980798865

Epoch: 5| Step: 4
Training loss: 2.619468927383423
Validation loss: 2.5097393015379548

Epoch: 5| Step: 5
Training loss: 2.844480514526367
Validation loss: 2.510148976438789

Epoch: 5| Step: 6
Training loss: 2.6590747833251953
Validation loss: 2.5076627808232463

Epoch: 5| Step: 7
Training loss: 2.5493202209472656
Validation loss: 2.509710645162931

Epoch: 5| Step: 8
Training loss: 2.479536294937134
Validation loss: 2.5074568845892466

Epoch: 5| Step: 9
Training loss: 3.0213661193847656
Validation loss: 2.5068939526875815

Epoch: 5| Step: 10
Training loss: 2.919447183609009
Validation loss: 2.5141614047429894

Epoch: 134| Step: 0
Training loss: 3.0009007453918457
Validation loss: 2.511245996721329

Epoch: 5| Step: 1
Training loss: 2.061488389968872
Validation loss: 2.510314477387295

Epoch: 5| Step: 2
Training loss: 2.663057804107666
Validation loss: 2.5106005591730916

Epoch: 5| Step: 3
Training loss: 3.4690117835998535
Validation loss: 2.5091663124740764

Epoch: 5| Step: 4
Training loss: 3.000377655029297
Validation loss: 2.5061057536832747

Epoch: 5| Step: 5
Training loss: 2.521174907684326
Validation loss: 2.5049358567883893

Epoch: 5| Step: 6
Training loss: 2.5328190326690674
Validation loss: 2.5078367751131774

Epoch: 5| Step: 7
Training loss: 2.9625210762023926
Validation loss: 2.5074611761236705

Epoch: 5| Step: 8
Training loss: 2.534177780151367
Validation loss: 2.505423694528559

Epoch: 5| Step: 9
Training loss: 2.583307981491089
Validation loss: 2.505717597981935

Epoch: 5| Step: 10
Training loss: 2.2125041484832764
Validation loss: 2.5046511132230043

Epoch: 135| Step: 0
Training loss: 2.6716365814208984
Validation loss: 2.5033827750913558

Epoch: 5| Step: 1
Training loss: 2.4665300846099854
Validation loss: 2.5032122699163293

Epoch: 5| Step: 2
Training loss: 2.6305408477783203
Validation loss: 2.5049046316454486

Epoch: 5| Step: 3
Training loss: 2.2627532482147217
Validation loss: 2.5088986299371205

Epoch: 5| Step: 4
Training loss: 2.629953145980835
Validation loss: 2.5051068003459642

Epoch: 5| Step: 5
Training loss: 2.721289873123169
Validation loss: 2.504838707626507

Epoch: 5| Step: 6
Training loss: 3.361947536468506
Validation loss: 2.5054616133371987

Epoch: 5| Step: 7
Training loss: 2.9946084022521973
Validation loss: 2.5021582418872463

Epoch: 5| Step: 8
Training loss: 2.7698564529418945
Validation loss: 2.5032951062725437

Epoch: 5| Step: 9
Training loss: 2.5372536182403564
Validation loss: 2.5048290555195143

Epoch: 5| Step: 10
Training loss: 2.4863860607147217
Validation loss: 2.502451019902383

Epoch: 136| Step: 0
Training loss: 2.291645050048828
Validation loss: 2.5050494158139793

Epoch: 5| Step: 1
Training loss: 2.488217353820801
Validation loss: 2.503517440570298

Epoch: 5| Step: 2
Training loss: 2.9623730182647705
Validation loss: 2.5048335905997985

Epoch: 5| Step: 3
Training loss: 2.724630832672119
Validation loss: 2.5076951724226757

Epoch: 5| Step: 4
Training loss: 3.1992125511169434
Validation loss: 2.5048946308833298

Epoch: 5| Step: 5
Training loss: 3.284207582473755
Validation loss: 2.506283490888534

Epoch: 5| Step: 6
Training loss: 2.881303310394287
Validation loss: 2.5042727403743292

Epoch: 5| Step: 7
Training loss: 1.8338226079940796
Validation loss: 2.5049410045787854

Epoch: 5| Step: 8
Training loss: 2.7258431911468506
Validation loss: 2.5020747492390294

Epoch: 5| Step: 9
Training loss: 2.634734630584717
Validation loss: 2.504910410091441

Epoch: 5| Step: 10
Training loss: 2.462721824645996
Validation loss: 2.5054853885404524

Epoch: 137| Step: 0
Training loss: 2.4383413791656494
Validation loss: 2.504920341635263

Epoch: 5| Step: 1
Training loss: 2.6317107677459717
Validation loss: 2.5089009372136926

Epoch: 5| Step: 2
Training loss: 2.6358184814453125
Validation loss: 2.5090544685240714

Epoch: 5| Step: 3
Training loss: 2.764204978942871
Validation loss: 2.509181740463421

Epoch: 5| Step: 4
Training loss: 2.779468536376953
Validation loss: 2.504710087212183

Epoch: 5| Step: 5
Training loss: 3.283796787261963
Validation loss: 2.5090258352218138

Epoch: 5| Step: 6
Training loss: 2.9780824184417725
Validation loss: 2.5070277234559417

Epoch: 5| Step: 7
Training loss: 2.4060423374176025
Validation loss: 2.5037328389383133

Epoch: 5| Step: 8
Training loss: 2.1724660396575928
Validation loss: 2.5023258270755893

Epoch: 5| Step: 9
Training loss: 2.1393439769744873
Validation loss: 2.5012212825077835

Epoch: 5| Step: 10
Training loss: 3.407141923904419
Validation loss: 2.498611443786211

Epoch: 138| Step: 0
Training loss: 2.831146001815796
Validation loss: 2.5022254118355374

Epoch: 5| Step: 1
Training loss: 2.989793062210083
Validation loss: 2.501927747521349

Epoch: 5| Step: 2
Training loss: 2.836350440979004
Validation loss: 2.5037832003767773

Epoch: 5| Step: 3
Training loss: 2.6671996116638184
Validation loss: 2.509048915678455

Epoch: 5| Step: 4
Training loss: 2.559387683868408
Validation loss: 2.5064928685465167

Epoch: 5| Step: 5
Training loss: 2.3174338340759277
Validation loss: 2.5013064722861014

Epoch: 5| Step: 6
Training loss: 3.063079833984375
Validation loss: 2.4970057895106654

Epoch: 5| Step: 7
Training loss: 3.2396576404571533
Validation loss: 2.5000353474770822

Epoch: 5| Step: 8
Training loss: 2.467569351196289
Validation loss: 2.505203511125298

Epoch: 5| Step: 9
Training loss: 2.352640151977539
Validation loss: 2.5056884134969404

Epoch: 5| Step: 10
Training loss: 2.1918087005615234
Validation loss: 2.5025219122568765

Epoch: 139| Step: 0
Training loss: 2.7990682125091553
Validation loss: 2.498357826663602

Epoch: 5| Step: 1
Training loss: 2.9425458908081055
Validation loss: 2.49760445984461

Epoch: 5| Step: 2
Training loss: 2.8125529289245605
Validation loss: 2.493527389341785

Epoch: 5| Step: 3
Training loss: 2.4122366905212402
Validation loss: 2.4918769739007436

Epoch: 5| Step: 4
Training loss: 3.0758156776428223
Validation loss: 2.492010070431617

Epoch: 5| Step: 5
Training loss: 2.7296242713928223
Validation loss: 2.492373543400918

Epoch: 5| Step: 6
Training loss: 2.88874888420105
Validation loss: 2.4904856348550446

Epoch: 5| Step: 7
Training loss: 2.4912455081939697
Validation loss: 2.494542311596614

Epoch: 5| Step: 8
Training loss: 2.6414809226989746
Validation loss: 2.4992292158065306

Epoch: 5| Step: 9
Training loss: 1.8925492763519287
Validation loss: 2.499056951974028

Epoch: 5| Step: 10
Training loss: 2.857452630996704
Validation loss: 2.4948645971154653

Epoch: 140| Step: 0
Training loss: 2.108344316482544
Validation loss: 2.490423945970433

Epoch: 5| Step: 1
Training loss: 2.593752384185791
Validation loss: 2.49299446998104

Epoch: 5| Step: 2
Training loss: 2.8108253479003906
Validation loss: 2.4906798485786683

Epoch: 5| Step: 3
Training loss: 2.311145067214966
Validation loss: 2.4917271649965675

Epoch: 5| Step: 4
Training loss: 3.122453212738037
Validation loss: 2.4943681186245334

Epoch: 5| Step: 5
Training loss: 2.492732286453247
Validation loss: 2.4916448336775585

Epoch: 5| Step: 6
Training loss: 2.113049030303955
Validation loss: 2.4945924871711322

Epoch: 5| Step: 7
Training loss: 3.1130995750427246
Validation loss: 2.491924714016658

Epoch: 5| Step: 8
Training loss: 2.375659227371216
Validation loss: 2.490780125382126

Epoch: 5| Step: 9
Training loss: 3.2804369926452637
Validation loss: 2.4869435653891614

Epoch: 5| Step: 10
Training loss: 3.2578063011169434
Validation loss: 2.491983116313975

Epoch: 141| Step: 0
Training loss: 1.6706628799438477
Validation loss: 2.491770341832151

Epoch: 5| Step: 1
Training loss: 2.577115535736084
Validation loss: 2.489917206507857

Epoch: 5| Step: 2
Training loss: 2.5557658672332764
Validation loss: 2.4896779983274397

Epoch: 5| Step: 3
Training loss: 2.138664722442627
Validation loss: 2.4896854559580484

Epoch: 5| Step: 4
Training loss: 2.7672922611236572
Validation loss: 2.4945080254667547

Epoch: 5| Step: 5
Training loss: 2.9915270805358887
Validation loss: 2.4910621591793594

Epoch: 5| Step: 6
Training loss: 2.553361177444458
Validation loss: 2.4903347646036456

Epoch: 5| Step: 7
Training loss: 3.218201160430908
Validation loss: 2.490014491542693

Epoch: 5| Step: 8
Training loss: 2.9555606842041016
Validation loss: 2.4881152952871015

Epoch: 5| Step: 9
Training loss: 3.1509227752685547
Validation loss: 2.486835680982118

Epoch: 5| Step: 10
Training loss: 2.8868584632873535
Validation loss: 2.4898615216696136

Epoch: 142| Step: 0
Training loss: 2.6508712768554688
Validation loss: 2.4910703525748303

Epoch: 5| Step: 1
Training loss: 2.515291690826416
Validation loss: 2.4951888104920745

Epoch: 5| Step: 2
Training loss: 2.513765573501587
Validation loss: 2.4946787126602663

Epoch: 5| Step: 3
Training loss: 3.055891513824463
Validation loss: 2.4938986250149306

Epoch: 5| Step: 4
Training loss: 2.5314388275146484
Validation loss: 2.4933911446602113

Epoch: 5| Step: 5
Training loss: 2.47491455078125
Validation loss: 2.49079462276992

Epoch: 5| Step: 6
Training loss: 2.864461898803711
Validation loss: 2.4851329018992763

Epoch: 5| Step: 7
Training loss: 2.9876937866210938
Validation loss: 2.48463192806449

Epoch: 5| Step: 8
Training loss: 3.149205446243286
Validation loss: 2.4852784679782007

Epoch: 5| Step: 9
Training loss: 2.0414316654205322
Validation loss: 2.490231160194643

Epoch: 5| Step: 10
Training loss: 2.7136459350585938
Validation loss: 2.4866839096110356

Epoch: 143| Step: 0
Training loss: 2.51682710647583
Validation loss: 2.4900640595343804

Epoch: 5| Step: 1
Training loss: 2.5982415676116943
Validation loss: 2.4887902480299755

Epoch: 5| Step: 2
Training loss: 2.802041530609131
Validation loss: 2.4880843265082246

Epoch: 5| Step: 3
Training loss: 2.631356716156006
Validation loss: 2.4834700758739183

Epoch: 5| Step: 4
Training loss: 2.945624351501465
Validation loss: 2.4890022764923754

Epoch: 5| Step: 5
Training loss: 2.4272799491882324
Validation loss: 2.4888473403069282

Epoch: 5| Step: 6
Training loss: 2.8473336696624756
Validation loss: 2.4839733544216362

Epoch: 5| Step: 7
Training loss: 2.6604342460632324
Validation loss: 2.4873010676394225

Epoch: 5| Step: 8
Training loss: 2.6445631980895996
Validation loss: 2.4851406364030737

Epoch: 5| Step: 9
Training loss: 3.159627914428711
Validation loss: 2.4868692736471854

Epoch: 5| Step: 10
Training loss: 2.0863254070281982
Validation loss: 2.4824975921261694

Epoch: 144| Step: 0
Training loss: 3.2696661949157715
Validation loss: 2.4870751775721067

Epoch: 5| Step: 1
Training loss: 2.78300142288208
Validation loss: 2.4837489948477796

Epoch: 5| Step: 2
Training loss: 2.2865042686462402
Validation loss: 2.4832470852841615

Epoch: 5| Step: 3
Training loss: 2.4436962604522705
Validation loss: 2.487413934482041

Epoch: 5| Step: 4
Training loss: 2.465400218963623
Validation loss: 2.4911326336604294

Epoch: 5| Step: 5
Training loss: 2.568812370300293
Validation loss: 2.4951315797785276

Epoch: 5| Step: 6
Training loss: 3.3829174041748047
Validation loss: 2.496143517955657

Epoch: 5| Step: 7
Training loss: 2.5062873363494873
Validation loss: 2.493174663154028

Epoch: 5| Step: 8
Training loss: 2.410384178161621
Validation loss: 2.4858864994459253

Epoch: 5| Step: 9
Training loss: 2.730022430419922
Validation loss: 2.480545498991525

Epoch: 5| Step: 10
Training loss: 2.535971164703369
Validation loss: 2.4856568203177503

Epoch: 145| Step: 0
Training loss: 2.7082693576812744
Validation loss: 2.486031086214127

Epoch: 5| Step: 1
Training loss: 2.4830803871154785
Validation loss: 2.4910384788308093

Epoch: 5| Step: 2
Training loss: 2.356135368347168
Validation loss: 2.4879001725104546

Epoch: 5| Step: 3
Training loss: 2.5623562335968018
Validation loss: 2.4864159117462816

Epoch: 5| Step: 4
Training loss: 2.790104627609253
Validation loss: 2.481821793381886

Epoch: 5| Step: 5
Training loss: 2.942415237426758
Validation loss: 2.4843656760390087

Epoch: 5| Step: 6
Training loss: 2.785419464111328
Validation loss: 2.484430113146382

Epoch: 5| Step: 7
Training loss: 2.446002960205078
Validation loss: 2.4795632823821037

Epoch: 5| Step: 8
Training loss: 1.9136064052581787
Validation loss: 2.4783557691881732

Epoch: 5| Step: 9
Training loss: 3.295388698577881
Validation loss: 2.4864444527574765

Epoch: 5| Step: 10
Training loss: 3.1747241020202637
Validation loss: 2.491717287289199

Epoch: 146| Step: 0
Training loss: 2.5414586067199707
Validation loss: 2.494576936127037

Epoch: 5| Step: 1
Training loss: 2.861327648162842
Validation loss: 2.5062201253829466

Epoch: 5| Step: 2
Training loss: 3.199165105819702
Validation loss: 2.5315591648060787

Epoch: 5| Step: 3
Training loss: 2.706303596496582
Validation loss: 2.5402587639388217

Epoch: 5| Step: 4
Training loss: 2.2503597736358643
Validation loss: 2.5387978117953063

Epoch: 5| Step: 5
Training loss: 2.8917078971862793
Validation loss: 2.5210313412450973

Epoch: 5| Step: 6
Training loss: 2.9355955123901367
Validation loss: 2.497651812850788

Epoch: 5| Step: 7
Training loss: 3.2948906421661377
Validation loss: 2.4782841487597396

Epoch: 5| Step: 8
Training loss: 2.2533609867095947
Validation loss: 2.4750061611975394

Epoch: 5| Step: 9
Training loss: 2.2316596508026123
Validation loss: 2.493281582350372

Epoch: 5| Step: 10
Training loss: 2.31616473197937
Validation loss: 2.506767747222736

Epoch: 147| Step: 0
Training loss: 2.3409032821655273
Validation loss: 2.5213551264937206

Epoch: 5| Step: 1
Training loss: 2.4895589351654053
Validation loss: 2.536132945809313

Epoch: 5| Step: 2
Training loss: 3.0469703674316406
Validation loss: 2.53297011826628

Epoch: 5| Step: 3
Training loss: 3.0394036769866943
Validation loss: 2.519799458083286

Epoch: 5| Step: 4
Training loss: 2.3341898918151855
Validation loss: 2.5024917561520814

Epoch: 5| Step: 5
Training loss: 2.549931287765503
Validation loss: 2.501954363238427

Epoch: 5| Step: 6
Training loss: 2.6647591590881348
Validation loss: 2.487150248660836

Epoch: 5| Step: 7
Training loss: 2.79809308052063
Validation loss: 2.476547682157127

Epoch: 5| Step: 8
Training loss: 3.3187127113342285
Validation loss: 2.479198484010594

Epoch: 5| Step: 9
Training loss: 2.3493752479553223
Validation loss: 2.4842091811600553

Epoch: 5| Step: 10
Training loss: 2.546215772628784
Validation loss: 2.487728145814711

Epoch: 148| Step: 0
Training loss: 2.958350658416748
Validation loss: 2.4999091753395657

Epoch: 5| Step: 1
Training loss: 3.4403483867645264
Validation loss: 2.500163685890936

Epoch: 5| Step: 2
Training loss: 2.437835216522217
Validation loss: 2.507222851117452

Epoch: 5| Step: 3
Training loss: 2.716454029083252
Validation loss: 2.508196871767762

Epoch: 5| Step: 4
Training loss: 2.872292995452881
Validation loss: 2.5099870927872194

Epoch: 5| Step: 5
Training loss: 3.255484104156494
Validation loss: 2.4962981259951027

Epoch: 5| Step: 6
Training loss: 3.0529041290283203
Validation loss: 2.4836674582573677

Epoch: 5| Step: 7
Training loss: 2.0789825916290283
Validation loss: 2.4742879534280426

Epoch: 5| Step: 8
Training loss: 1.9625177383422852
Validation loss: 2.468431093359506

Epoch: 5| Step: 9
Training loss: 2.351250171661377
Validation loss: 2.469830451473113

Epoch: 5| Step: 10
Training loss: 2.2413599491119385
Validation loss: 2.472110935436782

Epoch: 149| Step: 0
Training loss: 2.826263904571533
Validation loss: 2.475259245082896

Epoch: 5| Step: 1
Training loss: 3.134272336959839
Validation loss: 2.4790014195185837

Epoch: 5| Step: 2
Training loss: 2.8104498386383057
Validation loss: 2.4868109738954933

Epoch: 5| Step: 3
Training loss: 2.3520419597625732
Validation loss: 2.4807035769185712

Epoch: 5| Step: 4
Training loss: 2.3341569900512695
Validation loss: 2.477532514961817

Epoch: 5| Step: 5
Training loss: 2.8220584392547607
Validation loss: 2.4712198267700853

Epoch: 5| Step: 6
Training loss: 3.0874085426330566
Validation loss: 2.4698950603444088

Epoch: 5| Step: 7
Training loss: 2.211104393005371
Validation loss: 2.480655357401858

Epoch: 5| Step: 8
Training loss: 2.8885674476623535
Validation loss: 2.5029310385386148

Epoch: 5| Step: 9
Training loss: 2.4464383125305176
Validation loss: 2.4806293518312517

Epoch: 5| Step: 10
Training loss: 2.5774333477020264
Validation loss: 2.473089833413401

Epoch: 150| Step: 0
Training loss: 2.2747490406036377
Validation loss: 2.467848131733556

Epoch: 5| Step: 1
Training loss: 2.7062315940856934
Validation loss: 2.46406659003227

Epoch: 5| Step: 2
Training loss: 2.408740520477295
Validation loss: 2.46489147217043

Epoch: 5| Step: 3
Training loss: 3.0520846843719482
Validation loss: 2.463983481930148

Epoch: 5| Step: 4
Training loss: 2.757326602935791
Validation loss: 2.4653683964924147

Epoch: 5| Step: 5
Training loss: 2.925370454788208
Validation loss: 2.466725786526998

Epoch: 5| Step: 6
Training loss: 3.1674861907958984
Validation loss: 2.468195458894135

Epoch: 5| Step: 7
Training loss: 2.692342758178711
Validation loss: 2.4720522511389946

Epoch: 5| Step: 8
Training loss: 2.07055926322937
Validation loss: 2.4712412024057038

Epoch: 5| Step: 9
Training loss: 2.803140163421631
Validation loss: 2.468627619486983

Epoch: 5| Step: 10
Training loss: 2.460216999053955
Validation loss: 2.4739079295947985

Epoch: 151| Step: 0
Training loss: 3.005211591720581
Validation loss: 2.482619644493185

Epoch: 5| Step: 1
Training loss: 2.7873692512512207
Validation loss: 2.4994602485369612

Epoch: 5| Step: 2
Training loss: 2.5420079231262207
Validation loss: 2.5010575043257846

Epoch: 5| Step: 3
Training loss: 2.8628838062286377
Validation loss: 2.507484684708298

Epoch: 5| Step: 4
Training loss: 2.1491029262542725
Validation loss: 2.524208750776065

Epoch: 5| Step: 5
Training loss: 2.7207603454589844
Validation loss: 2.522976562541018

Epoch: 5| Step: 6
Training loss: 2.2110092639923096
Validation loss: 2.4940690199534097

Epoch: 5| Step: 7
Training loss: 2.594534397125244
Validation loss: 2.473242864813856

Epoch: 5| Step: 8
Training loss: 3.2236053943634033
Validation loss: 2.4635346833095757

Epoch: 5| Step: 9
Training loss: 2.89323091506958
Validation loss: 2.4622607461867796

Epoch: 5| Step: 10
Training loss: 2.426715135574341
Validation loss: 2.4680428569034865

Epoch: 152| Step: 0
Training loss: 2.6006102561950684
Validation loss: 2.472620500031338

Epoch: 5| Step: 1
Training loss: 2.3553268909454346
Validation loss: 2.485933760161041

Epoch: 5| Step: 2
Training loss: 2.7879061698913574
Validation loss: 2.5109534212338027

Epoch: 5| Step: 3
Training loss: 2.9842846393585205
Validation loss: 2.498220923126385

Epoch: 5| Step: 4
Training loss: 2.44970703125
Validation loss: 2.4966823144625594

Epoch: 5| Step: 5
Training loss: 2.1332104206085205
Validation loss: 2.480406850896856

Epoch: 5| Step: 6
Training loss: 3.079638957977295
Validation loss: 2.473215990169074

Epoch: 5| Step: 7
Training loss: 3.5536327362060547
Validation loss: 2.4681084796946537

Epoch: 5| Step: 8
Training loss: 3.2589328289031982
Validation loss: 2.4666918990432576

Epoch: 5| Step: 9
Training loss: 2.2640445232391357
Validation loss: 2.4655811453378327

Epoch: 5| Step: 10
Training loss: 1.883387565612793
Validation loss: 2.4661250652805453

Epoch: 153| Step: 0
Training loss: 3.039503574371338
Validation loss: 2.4687165316715034

Epoch: 5| Step: 1
Training loss: 2.583181858062744
Validation loss: 2.475063708520705

Epoch: 5| Step: 2
Training loss: 2.4835433959960938
Validation loss: 2.479993156207505

Epoch: 5| Step: 3
Training loss: 2.9071848392486572
Validation loss: 2.4919595744020198

Epoch: 5| Step: 4
Training loss: 2.5569422245025635
Validation loss: 2.477404761058028

Epoch: 5| Step: 5
Training loss: 2.0482935905456543
Validation loss: 2.4649321699655182

Epoch: 5| Step: 6
Training loss: 2.622659683227539
Validation loss: 2.460213689393895

Epoch: 5| Step: 7
Training loss: 3.083861827850342
Validation loss: 2.463418463225006

Epoch: 5| Step: 8
Training loss: 2.5844743251800537
Validation loss: 2.4728835372514624

Epoch: 5| Step: 9
Training loss: 2.8571388721466064
Validation loss: 2.479730321514991

Epoch: 5| Step: 10
Training loss: 2.618955135345459
Validation loss: 2.492697536304433

Epoch: 154| Step: 0
Training loss: 2.8160247802734375
Validation loss: 2.502143536844561

Epoch: 5| Step: 1
Training loss: 1.9627434015274048
Validation loss: 2.506044964636526

Epoch: 5| Step: 2
Training loss: 3.305973768234253
Validation loss: 2.5185071806753836

Epoch: 5| Step: 3
Training loss: 2.9870378971099854
Validation loss: 2.5130613234735306

Epoch: 5| Step: 4
Training loss: 2.2184460163116455
Validation loss: 2.5099380875146515

Epoch: 5| Step: 5
Training loss: 2.0685086250305176
Validation loss: 2.498648617857246

Epoch: 5| Step: 6
Training loss: 3.1903817653656006
Validation loss: 2.4866884370003977

Epoch: 5| Step: 7
Training loss: 2.7416465282440186
Validation loss: 2.4707349756712556

Epoch: 5| Step: 8
Training loss: 3.111052989959717
Validation loss: 2.465280884055681

Epoch: 5| Step: 9
Training loss: 2.636646270751953
Validation loss: 2.460704157429357

Epoch: 5| Step: 10
Training loss: 2.451746940612793
Validation loss: 2.458722132508473

Epoch: 155| Step: 0
Training loss: 2.795346736907959
Validation loss: 2.458895270542432

Epoch: 5| Step: 1
Training loss: 2.808225154876709
Validation loss: 2.459960796499765

Epoch: 5| Step: 2
Training loss: 2.4537649154663086
Validation loss: 2.462336147985151

Epoch: 5| Step: 3
Training loss: 2.5830283164978027
Validation loss: 2.455915217758507

Epoch: 5| Step: 4
Training loss: 2.8802194595336914
Validation loss: 2.45726635379176

Epoch: 5| Step: 5
Training loss: 3.0485329627990723
Validation loss: 2.4616446315601306

Epoch: 5| Step: 6
Training loss: 1.9365745782852173
Validation loss: 2.464156194399762

Epoch: 5| Step: 7
Training loss: 2.8610198497772217
Validation loss: 2.465663650984405

Epoch: 5| Step: 8
Training loss: 2.464264392852783
Validation loss: 2.4611107405795845

Epoch: 5| Step: 9
Training loss: 2.4375648498535156
Validation loss: 2.4685689172437115

Epoch: 5| Step: 10
Training loss: 3.0673797130584717
Validation loss: 2.465694563363188

Epoch: 156| Step: 0
Training loss: 2.8550071716308594
Validation loss: 2.4712869095545944

Epoch: 5| Step: 1
Training loss: 2.427184820175171
Validation loss: 2.460974959916966

Epoch: 5| Step: 2
Training loss: 2.572181463241577
Validation loss: 2.458165091852988

Epoch: 5| Step: 3
Training loss: 2.8466193675994873
Validation loss: 2.4511200689500376

Epoch: 5| Step: 4
Training loss: 2.6678338050842285
Validation loss: 2.4516713721777803

Epoch: 5| Step: 5
Training loss: 2.3954403400421143
Validation loss: 2.4518683930878997

Epoch: 5| Step: 6
Training loss: 2.7566232681274414
Validation loss: 2.4560367676519577

Epoch: 5| Step: 7
Training loss: 2.7430760860443115
Validation loss: 2.455712290220363

Epoch: 5| Step: 8
Training loss: 2.6135411262512207
Validation loss: 2.4561687105445453

Epoch: 5| Step: 9
Training loss: 2.9328417778015137
Validation loss: 2.456554621778509

Epoch: 5| Step: 10
Training loss: 2.5051987171173096
Validation loss: 2.456826215149254

Epoch: 157| Step: 0
Training loss: 2.7137722969055176
Validation loss: 2.4559096610674294

Epoch: 5| Step: 1
Training loss: 2.5793728828430176
Validation loss: 2.4562488691781157

Epoch: 5| Step: 2
Training loss: 2.6874544620513916
Validation loss: 2.4585082351520495

Epoch: 5| Step: 3
Training loss: 2.791935682296753
Validation loss: 2.4549612409325055

Epoch: 5| Step: 4
Training loss: 2.7969467639923096
Validation loss: 2.4552791323713077

Epoch: 5| Step: 5
Training loss: 2.591326951980591
Validation loss: 2.452063678413309

Epoch: 5| Step: 6
Training loss: 2.880615711212158
Validation loss: 2.452153085380472

Epoch: 5| Step: 7
Training loss: 2.812541961669922
Validation loss: 2.453584219819756

Epoch: 5| Step: 8
Training loss: 2.4514782428741455
Validation loss: 2.449573255354358

Epoch: 5| Step: 9
Training loss: 2.4108357429504395
Validation loss: 2.4540790793716267

Epoch: 5| Step: 10
Training loss: 2.6557905673980713
Validation loss: 2.4655262244644987

Epoch: 158| Step: 0
Training loss: 2.7719566822052
Validation loss: 2.486649813190583

Epoch: 5| Step: 1
Training loss: 2.031574249267578
Validation loss: 2.476897857522452

Epoch: 5| Step: 2
Training loss: 2.7311911582946777
Validation loss: 2.490220373676669

Epoch: 5| Step: 3
Training loss: 2.597602128982544
Validation loss: 2.4868002399321525

Epoch: 5| Step: 4
Training loss: 3.133089542388916
Validation loss: 2.4839069561291764

Epoch: 5| Step: 5
Training loss: 2.566317081451416
Validation loss: 2.4712926880005868

Epoch: 5| Step: 6
Training loss: 2.4641454219818115
Validation loss: 2.4587701443702943

Epoch: 5| Step: 7
Training loss: 2.7887840270996094
Validation loss: 2.449279897956438

Epoch: 5| Step: 8
Training loss: 2.5101492404937744
Validation loss: 2.44626312358405

Epoch: 5| Step: 9
Training loss: 2.813854932785034
Validation loss: 2.4458451783785256

Epoch: 5| Step: 10
Training loss: 2.960451126098633
Validation loss: 2.45234574169241

Epoch: 159| Step: 0
Training loss: 2.3960013389587402
Validation loss: 2.46099030586981

Epoch: 5| Step: 1
Training loss: 2.6228880882263184
Validation loss: 2.468258765435988

Epoch: 5| Step: 2
Training loss: 3.159285068511963
Validation loss: 2.477582190626411

Epoch: 5| Step: 3
Training loss: 2.387159824371338
Validation loss: 2.490110795984986

Epoch: 5| Step: 4
Training loss: 2.791114330291748
Validation loss: 2.4970733888687624

Epoch: 5| Step: 5
Training loss: 2.8407328128814697
Validation loss: 2.5149162123280187

Epoch: 5| Step: 6
Training loss: 2.8852386474609375
Validation loss: 2.512889651842015

Epoch: 5| Step: 7
Training loss: 2.995347499847412
Validation loss: 2.4911187733373334

Epoch: 5| Step: 8
Training loss: 2.7511303424835205
Validation loss: 2.477801702355826

Epoch: 5| Step: 9
Training loss: 2.3401923179626465
Validation loss: 2.4579386429120134

Epoch: 5| Step: 10
Training loss: 2.123640775680542
Validation loss: 2.4463152090708413

Epoch: 160| Step: 0
Training loss: 2.614835262298584
Validation loss: 2.4472452632842527

Epoch: 5| Step: 1
Training loss: 2.9089038372039795
Validation loss: 2.446854063259658

Epoch: 5| Step: 2
Training loss: 3.1240570545196533
Validation loss: 2.4520286090912355

Epoch: 5| Step: 3
Training loss: 2.9183509349823
Validation loss: 2.45716796382781

Epoch: 5| Step: 4
Training loss: 1.4223732948303223
Validation loss: 2.455126667535433

Epoch: 5| Step: 5
Training loss: 2.926370143890381
Validation loss: 2.456082461982645

Epoch: 5| Step: 6
Training loss: 2.681375026702881
Validation loss: 2.4595900248455744

Epoch: 5| Step: 7
Training loss: 2.298651933670044
Validation loss: 2.4584337165278773

Epoch: 5| Step: 8
Training loss: 2.91835355758667
Validation loss: 2.462378171182448

Epoch: 5| Step: 9
Training loss: 2.5159618854522705
Validation loss: 2.4845950603485107

Epoch: 5| Step: 10
Training loss: 3.0144407749176025
Validation loss: 2.4632782102912985

Epoch: 161| Step: 0
Training loss: 2.46655011177063
Validation loss: 2.4516322792217298

Epoch: 5| Step: 1
Training loss: 2.3170700073242188
Validation loss: 2.4447001667432886

Epoch: 5| Step: 2
Training loss: 2.9201900959014893
Validation loss: 2.4464915465283137

Epoch: 5| Step: 3
Training loss: 2.5581769943237305
Validation loss: 2.4519491246951524

Epoch: 5| Step: 4
Training loss: 3.380232334136963
Validation loss: 2.467855727800759

Epoch: 5| Step: 5
Training loss: 2.5384738445281982
Validation loss: 2.4608544418888707

Epoch: 5| Step: 6
Training loss: 2.3951449394226074
Validation loss: 2.4601698460117465

Epoch: 5| Step: 7
Training loss: 2.7671284675598145
Validation loss: 2.450083483931839

Epoch: 5| Step: 8
Training loss: 2.287341833114624
Validation loss: 2.465781891217796

Epoch: 5| Step: 9
Training loss: 3.312819004058838
Validation loss: 2.4693840216564875

Epoch: 5| Step: 10
Training loss: 2.2757883071899414
Validation loss: 2.4711240107013333

Epoch: 162| Step: 0
Training loss: 3.0666110515594482
Validation loss: 2.465624455482729

Epoch: 5| Step: 1
Training loss: 3.267493486404419
Validation loss: 2.4565530643668225

Epoch: 5| Step: 2
Training loss: 1.913559913635254
Validation loss: 2.4549851507268925

Epoch: 5| Step: 3
Training loss: 2.3898284435272217
Validation loss: 2.44984418858764

Epoch: 5| Step: 4
Training loss: 2.742496967315674
Validation loss: 2.4516980930041243

Epoch: 5| Step: 5
Training loss: 2.493988037109375
Validation loss: 2.4510026336998068

Epoch: 5| Step: 6
Training loss: 2.7570383548736572
Validation loss: 2.454937140146891

Epoch: 5| Step: 7
Training loss: 2.5062403678894043
Validation loss: 2.45874350301681

Epoch: 5| Step: 8
Training loss: 2.8646469116210938
Validation loss: 2.4644739602201726

Epoch: 5| Step: 9
Training loss: 2.254711151123047
Validation loss: 2.4675954644398024

Epoch: 5| Step: 10
Training loss: 3.098680019378662
Validation loss: 2.479800775486936

Epoch: 163| Step: 0
Training loss: 3.005314588546753
Validation loss: 2.479922189507433

Epoch: 5| Step: 1
Training loss: 2.9474899768829346
Validation loss: 2.4769939530280327

Epoch: 5| Step: 2
Training loss: 2.940429210662842
Validation loss: 2.4699175486000637

Epoch: 5| Step: 3
Training loss: 3.2836754322052
Validation loss: 2.469784939160911

Epoch: 5| Step: 4
Training loss: 2.3865456581115723
Validation loss: 2.4624784761859524

Epoch: 5| Step: 5
Training loss: 2.000927448272705
Validation loss: 2.4632668700269473

Epoch: 5| Step: 6
Training loss: 2.1633121967315674
Validation loss: 2.4605283121908865

Epoch: 5| Step: 7
Training loss: 2.9695680141448975
Validation loss: 2.4639304068780716

Epoch: 5| Step: 8
Training loss: 2.8576266765594482
Validation loss: 2.457184078872845

Epoch: 5| Step: 9
Training loss: 2.7208943367004395
Validation loss: 2.4525807775476927

Epoch: 5| Step: 10
Training loss: 1.7874337434768677
Validation loss: 2.442964235941569

Epoch: 164| Step: 0
Training loss: 3.096294641494751
Validation loss: 2.4453838358643236

Epoch: 5| Step: 1
Training loss: 2.3771934509277344
Validation loss: 2.4429648589062434

Epoch: 5| Step: 2
Training loss: 2.6020309925079346
Validation loss: 2.437757202374038

Epoch: 5| Step: 3
Training loss: 2.862532138824463
Validation loss: 2.4421642570085424

Epoch: 5| Step: 4
Training loss: 2.790703296661377
Validation loss: 2.4391321751379196

Epoch: 5| Step: 5
Training loss: 2.691985845565796
Validation loss: 2.439337948317169

Epoch: 5| Step: 6
Training loss: 3.0793936252593994
Validation loss: 2.442957096202399

Epoch: 5| Step: 7
Training loss: 2.1972744464874268
Validation loss: 2.4363578186240247

Epoch: 5| Step: 8
Training loss: 2.446052074432373
Validation loss: 2.4413248005733696

Epoch: 5| Step: 9
Training loss: 2.0421793460845947
Validation loss: 2.441236524171727

Epoch: 5| Step: 10
Training loss: 3.0523691177368164
Validation loss: 2.440684744106826

Epoch: 165| Step: 0
Training loss: 2.279270648956299
Validation loss: 2.4491230313495924

Epoch: 5| Step: 1
Training loss: 2.5032296180725098
Validation loss: 2.4471438059242825

Epoch: 5| Step: 2
Training loss: 3.031583547592163
Validation loss: 2.446502277928014

Epoch: 5| Step: 3
Training loss: 2.4777541160583496
Validation loss: 2.449340343475342

Epoch: 5| Step: 4
Training loss: 2.711582899093628
Validation loss: 2.4475335177554878

Epoch: 5| Step: 5
Training loss: 3.091359853744507
Validation loss: 2.4416311915202806

Epoch: 5| Step: 6
Training loss: 2.523533344268799
Validation loss: 2.439936448169011

Epoch: 5| Step: 7
Training loss: 3.1190943717956543
Validation loss: 2.438436851706556

Epoch: 5| Step: 8
Training loss: 2.998582124710083
Validation loss: 2.4371762096240954

Epoch: 5| Step: 9
Training loss: 2.0869171619415283
Validation loss: 2.444585916816547

Epoch: 5| Step: 10
Training loss: 2.259995460510254
Validation loss: 2.4435657352529545

Epoch: 166| Step: 0
Training loss: 2.3536019325256348
Validation loss: 2.448676980951781

Epoch: 5| Step: 1
Training loss: 1.908612608909607
Validation loss: 2.462506999251663

Epoch: 5| Step: 2
Training loss: 3.0844407081604004
Validation loss: 2.4676687204709618

Epoch: 5| Step: 3
Training loss: 2.351067066192627
Validation loss: 2.47722444739393

Epoch: 5| Step: 4
Training loss: 2.4601199626922607
Validation loss: 2.486704877627793

Epoch: 5| Step: 5
Training loss: 2.686180830001831
Validation loss: 2.4899206648590746

Epoch: 5| Step: 6
Training loss: 3.072406768798828
Validation loss: 2.4771961230103687

Epoch: 5| Step: 7
Training loss: 2.569821834564209
Validation loss: 2.459502294499387

Epoch: 5| Step: 8
Training loss: 2.8399550914764404
Validation loss: 2.44802511635647

Epoch: 5| Step: 9
Training loss: 3.33111834526062
Validation loss: 2.4416108144226896

Epoch: 5| Step: 10
Training loss: 2.5213711261749268
Validation loss: 2.4378171249102523

Epoch: 167| Step: 0
Training loss: 3.0922112464904785
Validation loss: 2.437228843729983

Epoch: 5| Step: 1
Training loss: 2.581648588180542
Validation loss: 2.436825759949223

Epoch: 5| Step: 2
Training loss: 2.655259370803833
Validation loss: 2.4450505266907396

Epoch: 5| Step: 3
Training loss: 2.73960018157959
Validation loss: 2.4519346478164836

Epoch: 5| Step: 4
Training loss: 1.369265079498291
Validation loss: 2.4459173704988215

Epoch: 5| Step: 5
Training loss: 2.4438579082489014
Validation loss: 2.450754832195979

Epoch: 5| Step: 6
Training loss: 2.8365046977996826
Validation loss: 2.4443978827486754

Epoch: 5| Step: 7
Training loss: 3.136500120162964
Validation loss: 2.4430236944588284

Epoch: 5| Step: 8
Training loss: 2.5106801986694336
Validation loss: 2.4437215084670694

Epoch: 5| Step: 9
Training loss: 2.8473830223083496
Validation loss: 2.436707371024675

Epoch: 5| Step: 10
Training loss: 2.986386299133301
Validation loss: 2.434331529883928

Epoch: 168| Step: 0
Training loss: 2.096217632293701
Validation loss: 2.4282772438500517

Epoch: 5| Step: 1
Training loss: 2.843156337738037
Validation loss: 2.4294573440346667

Epoch: 5| Step: 2
Training loss: 3.2698378562927246
Validation loss: 2.4266072421945553

Epoch: 5| Step: 3
Training loss: 2.75386118888855
Validation loss: 2.4261367282559796

Epoch: 5| Step: 4
Training loss: 2.3516480922698975
Validation loss: 2.424116019279726

Epoch: 5| Step: 5
Training loss: 2.6389501094818115
Validation loss: 2.426753633765764

Epoch: 5| Step: 6
Training loss: 2.8089511394500732
Validation loss: 2.424968901500907

Epoch: 5| Step: 7
Training loss: 2.5657005310058594
Validation loss: 2.4232984819719867

Epoch: 5| Step: 8
Training loss: 3.112748622894287
Validation loss: 2.4243799409558697

Epoch: 5| Step: 9
Training loss: 1.9738438129425049
Validation loss: 2.4220032576591737

Epoch: 5| Step: 10
Training loss: 2.7518022060394287
Validation loss: 2.4216996700532976

Epoch: 169| Step: 0
Training loss: 2.5449700355529785
Validation loss: 2.4223502246282433

Epoch: 5| Step: 1
Training loss: 3.1526217460632324
Validation loss: 2.4237197881103842

Epoch: 5| Step: 2
Training loss: 2.5297577381134033
Validation loss: 2.4313340751073693

Epoch: 5| Step: 3
Training loss: 2.1580071449279785
Validation loss: 2.4377245492832635

Epoch: 5| Step: 4
Training loss: 2.4678797721862793
Validation loss: 2.430370138537499

Epoch: 5| Step: 5
Training loss: 2.2127482891082764
Validation loss: 2.4390667869198706

Epoch: 5| Step: 6
Training loss: 2.7086715698242188
Validation loss: 2.4323640715691353

Epoch: 5| Step: 7
Training loss: 3.0640201568603516
Validation loss: 2.4333733204872376

Epoch: 5| Step: 8
Training loss: 2.1870150566101074
Validation loss: 2.4375139923505884

Epoch: 5| Step: 9
Training loss: 2.9678432941436768
Validation loss: 2.4447091523037163

Epoch: 5| Step: 10
Training loss: 3.1442837715148926
Validation loss: 2.43755478243674

Epoch: 170| Step: 0
Training loss: 3.1665821075439453
Validation loss: 2.446971401091545

Epoch: 5| Step: 1
Training loss: 2.1749815940856934
Validation loss: 2.4491347548782185

Epoch: 5| Step: 2
Training loss: 2.795696258544922
Validation loss: 2.445849630140489

Epoch: 5| Step: 3
Training loss: 2.0645384788513184
Validation loss: 2.4464728524607997

Epoch: 5| Step: 4
Training loss: 2.9464001655578613
Validation loss: 2.4537162473124843

Epoch: 5| Step: 5
Training loss: 2.2183306217193604
Validation loss: 2.4522643730204594

Epoch: 5| Step: 6
Training loss: 2.8089606761932373
Validation loss: 2.464137875905601

Epoch: 5| Step: 7
Training loss: 2.7158637046813965
Validation loss: 2.472236571773406

Epoch: 5| Step: 8
Training loss: 2.7309913635253906
Validation loss: 2.480450886552052

Epoch: 5| Step: 9
Training loss: 3.0715363025665283
Validation loss: 2.4911109196242465

Epoch: 5| Step: 10
Training loss: 2.32340407371521
Validation loss: 2.4951385785174627

Epoch: 171| Step: 0
Training loss: 3.088672161102295
Validation loss: 2.503756925623904

Epoch: 5| Step: 1
Training loss: 3.2839272022247314
Validation loss: 2.502290884653727

Epoch: 5| Step: 2
Training loss: 1.8195245265960693
Validation loss: 2.4830409788316294

Epoch: 5| Step: 3
Training loss: 2.911508083343506
Validation loss: 2.4739919247165805

Epoch: 5| Step: 4
Training loss: 2.4555392265319824
Validation loss: 2.469850501706523

Epoch: 5| Step: 5
Training loss: 2.5833740234375
Validation loss: 2.4585964679718018

Epoch: 5| Step: 6
Training loss: 2.9739325046539307
Validation loss: 2.444413487629224

Epoch: 5| Step: 7
Training loss: 2.1144917011260986
Validation loss: 2.433751301098895

Epoch: 5| Step: 8
Training loss: 2.069303035736084
Validation loss: 2.4330666859944663

Epoch: 5| Step: 9
Training loss: 2.627129077911377
Validation loss: 2.4383371671040854

Epoch: 5| Step: 10
Training loss: 3.242603063583374
Validation loss: 2.434074383909984

Epoch: 172| Step: 0
Training loss: 3.0398693084716797
Validation loss: 2.4417690102772047

Epoch: 5| Step: 1
Training loss: 2.4642233848571777
Validation loss: 2.4306507725869455

Epoch: 5| Step: 2
Training loss: 3.2145698070526123
Validation loss: 2.435779092132404

Epoch: 5| Step: 3
Training loss: 2.6798038482666016
Validation loss: 2.4307675105269237

Epoch: 5| Step: 4
Training loss: 2.660949230194092
Validation loss: 2.4284852422693723

Epoch: 5| Step: 5
Training loss: 2.4853756427764893
Validation loss: 2.4313347288357314

Epoch: 5| Step: 6
Training loss: 2.6687355041503906
Validation loss: 2.42862872410846

Epoch: 5| Step: 7
Training loss: 1.7521578073501587
Validation loss: 2.4462764263153076

Epoch: 5| Step: 8
Training loss: 2.4431724548339844
Validation loss: 2.477257602958269

Epoch: 5| Step: 9
Training loss: 3.0103156566619873
Validation loss: 2.540879523882302

Epoch: 5| Step: 10
Training loss: 2.9358367919921875
Validation loss: 2.555693195712182

Epoch: 173| Step: 0
Training loss: 3.1897661685943604
Validation loss: 2.5517333553683375

Epoch: 5| Step: 1
Training loss: 2.168733835220337
Validation loss: 2.532346007644489

Epoch: 5| Step: 2
Training loss: 2.8090736865997314
Validation loss: 2.5300747707325923

Epoch: 5| Step: 3
Training loss: 2.542006731033325
Validation loss: 2.5211012389070246

Epoch: 5| Step: 4
Training loss: 3.0390775203704834
Validation loss: 2.4763668737103863

Epoch: 5| Step: 5
Training loss: 2.6409897804260254
Validation loss: 2.43571969514252

Epoch: 5| Step: 6
Training loss: 3.3020129203796387
Validation loss: 2.436593927362914

Epoch: 5| Step: 7
Training loss: 2.129196882247925
Validation loss: 2.446005816100746

Epoch: 5| Step: 8
Training loss: 2.5250039100646973
Validation loss: 2.46029298023511

Epoch: 5| Step: 9
Training loss: 2.563096523284912
Validation loss: 2.4723646230595087

Epoch: 5| Step: 10
Training loss: 2.115622043609619
Validation loss: 2.4936823268090524

Epoch: 174| Step: 0
Training loss: 1.998509168624878
Validation loss: 2.510881188095257

Epoch: 5| Step: 1
Training loss: 2.338198184967041
Validation loss: 2.5015829301649526

Epoch: 5| Step: 2
Training loss: 3.0824341773986816
Validation loss: 2.516126342999038

Epoch: 5| Step: 3
Training loss: 3.014141321182251
Validation loss: 2.5019272591478083

Epoch: 5| Step: 4
Training loss: 2.1648106575012207
Validation loss: 2.459699761482977

Epoch: 5| Step: 5
Training loss: 2.9576525688171387
Validation loss: 2.4256815064337944

Epoch: 5| Step: 6
Training loss: 3.0643062591552734
Validation loss: 2.4231714638330604

Epoch: 5| Step: 7
Training loss: 2.7136454582214355
Validation loss: 2.4146450027342765

Epoch: 5| Step: 8
Training loss: 2.72755765914917
Validation loss: 2.4179254219096196

Epoch: 5| Step: 9
Training loss: 2.565430164337158
Validation loss: 2.4141841088571856

Epoch: 5| Step: 10
Training loss: 2.5399980545043945
Validation loss: 2.4116674418090494

Epoch: 175| Step: 0
Training loss: 2.6082375049591064
Validation loss: 2.415337731761317

Epoch: 5| Step: 1
Training loss: 2.807922840118408
Validation loss: 2.415813466554047

Epoch: 5| Step: 2
Training loss: 3.2845892906188965
Validation loss: 2.420450113152945

Epoch: 5| Step: 3
Training loss: 2.513789415359497
Validation loss: 2.421279740589921

Epoch: 5| Step: 4
Training loss: 2.4163613319396973
Validation loss: 2.4452123641967773

Epoch: 5| Step: 5
Training loss: 2.853515148162842
Validation loss: 2.4219214172773462

Epoch: 5| Step: 6
Training loss: 2.3031973838806152
Validation loss: 2.416387334946663

Epoch: 5| Step: 7
Training loss: 2.7011802196502686
Validation loss: 2.414461240973524

Epoch: 5| Step: 8
Training loss: 2.6095266342163086
Validation loss: 2.4131502182252946

Epoch: 5| Step: 9
Training loss: 2.0784225463867188
Validation loss: 2.4110473535394155

Epoch: 5| Step: 10
Training loss: 2.9813754558563232
Validation loss: 2.415138549702142

Epoch: 176| Step: 0
Training loss: 2.7300071716308594
Validation loss: 2.4083583406222764

Epoch: 5| Step: 1
Training loss: 2.3626463413238525
Validation loss: 2.4110541933326313

Epoch: 5| Step: 2
Training loss: 2.6027770042419434
Validation loss: 2.414268360343031

Epoch: 5| Step: 3
Training loss: 2.180419445037842
Validation loss: 2.412755367576435

Epoch: 5| Step: 4
Training loss: 2.850313901901245
Validation loss: 2.410704069240119

Epoch: 5| Step: 5
Training loss: 3.1567482948303223
Validation loss: 2.4081072884221233

Epoch: 5| Step: 6
Training loss: 3.194664478302002
Validation loss: 2.4058001451594855

Epoch: 5| Step: 7
Training loss: 2.7428975105285645
Validation loss: 2.4046659725968555

Epoch: 5| Step: 8
Training loss: 2.171098470687866
Validation loss: 2.406101513934392

Epoch: 5| Step: 9
Training loss: 2.5673584938049316
Validation loss: 2.4026792433954056

Epoch: 5| Step: 10
Training loss: 2.391735315322876
Validation loss: 2.405407162122829

Epoch: 177| Step: 0
Training loss: 2.8376243114471436
Validation loss: 2.403931192172471

Epoch: 5| Step: 1
Training loss: 2.0182697772979736
Validation loss: 2.407449117270849

Epoch: 5| Step: 2
Training loss: 2.752537965774536
Validation loss: 2.4072699623723186

Epoch: 5| Step: 3
Training loss: 2.246211290359497
Validation loss: 2.4084849485787014

Epoch: 5| Step: 4
Training loss: 2.25787353515625
Validation loss: 2.4136753543730705

Epoch: 5| Step: 5
Training loss: 3.523314952850342
Validation loss: 2.4043149896847305

Epoch: 5| Step: 6
Training loss: 2.8846595287323
Validation loss: 2.403698108529532

Epoch: 5| Step: 7
Training loss: 2.411935329437256
Validation loss: 2.4039689212717037

Epoch: 5| Step: 8
Training loss: 2.8164219856262207
Validation loss: 2.406239245527534

Epoch: 5| Step: 9
Training loss: 2.5334372520446777
Validation loss: 2.406385765280775

Epoch: 5| Step: 10
Training loss: 2.734496831893921
Validation loss: 2.4054914264268774

Epoch: 178| Step: 0
Training loss: 3.1765055656433105
Validation loss: 2.40705717250865

Epoch: 5| Step: 1
Training loss: 2.237170696258545
Validation loss: 2.403757664465135

Epoch: 5| Step: 2
Training loss: 3.0167949199676514
Validation loss: 2.403439296189175

Epoch: 5| Step: 3
Training loss: 2.8215978145599365
Validation loss: 2.405924125384259

Epoch: 5| Step: 4
Training loss: 2.4038727283477783
Validation loss: 2.403639994641786

Epoch: 5| Step: 5
Training loss: 2.2412734031677246
Validation loss: 2.403534125256282

Epoch: 5| Step: 6
Training loss: 2.3713080883026123
Validation loss: 2.4048296918151197

Epoch: 5| Step: 7
Training loss: 2.481919527053833
Validation loss: 2.4082913680743148

Epoch: 5| Step: 8
Training loss: 2.5157551765441895
Validation loss: 2.4095391432444253

Epoch: 5| Step: 9
Training loss: 2.468374252319336
Validation loss: 2.4107974844594158

Epoch: 5| Step: 10
Training loss: 3.268453359603882
Validation loss: 2.4112520243531916

Epoch: 179| Step: 0
Training loss: 2.521282911300659
Validation loss: 2.4148256855626262

Epoch: 5| Step: 1
Training loss: 2.3196372985839844
Validation loss: 2.4126127330205773

Epoch: 5| Step: 2
Training loss: 2.850475311279297
Validation loss: 2.415718347795548

Epoch: 5| Step: 3
Training loss: 2.445751667022705
Validation loss: 2.4114024113583308

Epoch: 5| Step: 4
Training loss: 2.666865825653076
Validation loss: 2.414607083925637

Epoch: 5| Step: 5
Training loss: 2.8177952766418457
Validation loss: 2.4140706857045493

Epoch: 5| Step: 6
Training loss: 2.4179983139038086
Validation loss: 2.414356659817439

Epoch: 5| Step: 7
Training loss: 2.940816879272461
Validation loss: 2.417083167260693

Epoch: 5| Step: 8
Training loss: 2.454853057861328
Validation loss: 2.4172471133611535

Epoch: 5| Step: 9
Training loss: 2.2351479530334473
Validation loss: 2.4129364721236692

Epoch: 5| Step: 10
Training loss: 3.273630380630493
Validation loss: 2.414850163203414

Epoch: 180| Step: 0
Training loss: 1.6480661630630493
Validation loss: 2.4167593192028742

Epoch: 5| Step: 1
Training loss: 2.8463473320007324
Validation loss: 2.419771591822306

Epoch: 5| Step: 2
Training loss: 2.256441593170166
Validation loss: 2.4191098341377835

Epoch: 5| Step: 3
Training loss: 2.6164345741271973
Validation loss: 2.4204244870011524

Epoch: 5| Step: 4
Training loss: 2.989748477935791
Validation loss: 2.4264133591805734

Epoch: 5| Step: 5
Training loss: 2.7985329627990723
Validation loss: 2.43230446692436

Epoch: 5| Step: 6
Training loss: 2.7044341564178467
Validation loss: 2.4290801325151996

Epoch: 5| Step: 7
Training loss: 2.1785731315612793
Validation loss: 2.4341387953809512

Epoch: 5| Step: 8
Training loss: 3.067558526992798
Validation loss: 2.4284226740560224

Epoch: 5| Step: 9
Training loss: 3.067143201828003
Validation loss: 2.429442545419098

Epoch: 5| Step: 10
Training loss: 2.682818651199341
Validation loss: 2.430305473266109

Epoch: 181| Step: 0
Training loss: 2.668179750442505
Validation loss: 2.416962464650472

Epoch: 5| Step: 1
Training loss: 2.671761989593506
Validation loss: 2.4276865092656945

Epoch: 5| Step: 2
Training loss: 2.594473361968994
Validation loss: 2.429420680128118

Epoch: 5| Step: 3
Training loss: 2.9743895530700684
Validation loss: 2.420562244230701

Epoch: 5| Step: 4
Training loss: 2.8720667362213135
Validation loss: 2.4186294771009877

Epoch: 5| Step: 5
Training loss: 2.7186079025268555
Validation loss: 2.410169288676272

Epoch: 5| Step: 6
Training loss: 2.1451592445373535
Validation loss: 2.406213273284256

Epoch: 5| Step: 7
Training loss: 2.2720494270324707
Validation loss: 2.408618991092969

Epoch: 5| Step: 8
Training loss: 2.785468339920044
Validation loss: 2.40718017085906

Epoch: 5| Step: 9
Training loss: 2.5652787685394287
Validation loss: 2.4159184630199144

Epoch: 5| Step: 10
Training loss: 2.468559741973877
Validation loss: 2.418837737011653

Epoch: 182| Step: 0
Training loss: 2.613330364227295
Validation loss: 2.4234993355248564

Epoch: 5| Step: 1
Training loss: 2.7612719535827637
Validation loss: 2.4216132446001937

Epoch: 5| Step: 2
Training loss: 2.3788459300994873
Validation loss: 2.4384344111206713

Epoch: 5| Step: 3
Training loss: 2.60539174079895
Validation loss: 2.4258582771465345

Epoch: 5| Step: 4
Training loss: 2.3048095703125
Validation loss: 2.424064310648108

Epoch: 5| Step: 5
Training loss: 2.4969522953033447
Validation loss: 2.423409805502943

Epoch: 5| Step: 6
Training loss: 2.4869654178619385
Validation loss: 2.4136583112901255

Epoch: 5| Step: 7
Training loss: 2.808199882507324
Validation loss: 2.417715467432494

Epoch: 5| Step: 8
Training loss: 2.7362539768218994
Validation loss: 2.4201720042895247

Epoch: 5| Step: 9
Training loss: 2.638237237930298
Validation loss: 2.4125787699094383

Epoch: 5| Step: 10
Training loss: 2.9950060844421387
Validation loss: 2.416991418407809

Epoch: 183| Step: 0
Training loss: 2.6403539180755615
Validation loss: 2.4064031108733146

Epoch: 5| Step: 1
Training loss: 2.3195905685424805
Validation loss: 2.409550743718301

Epoch: 5| Step: 2
Training loss: 3.050018310546875
Validation loss: 2.4162268100246305

Epoch: 5| Step: 3
Training loss: 2.6666131019592285
Validation loss: 2.419999896839101

Epoch: 5| Step: 4
Training loss: 2.5710043907165527
Validation loss: 2.4249285010881323

Epoch: 5| Step: 5
Training loss: 2.8768057823181152
Validation loss: 2.432991276505173

Epoch: 5| Step: 6
Training loss: 2.732248067855835
Validation loss: 2.4202727015300463

Epoch: 5| Step: 7
Training loss: 2.2876477241516113
Validation loss: 2.4211433779808784

Epoch: 5| Step: 8
Training loss: 2.6066973209381104
Validation loss: 2.4125966769392773

Epoch: 5| Step: 9
Training loss: 2.2051587104797363
Validation loss: 2.4004665036355295

Epoch: 5| Step: 10
Training loss: 2.8774633407592773
Validation loss: 2.406174559747019

Epoch: 184| Step: 0
Training loss: 2.5792839527130127
Validation loss: 2.416479331190868

Epoch: 5| Step: 1
Training loss: 2.629648208618164
Validation loss: 2.4184638966796217

Epoch: 5| Step: 2
Training loss: 2.6886355876922607
Validation loss: 2.4254513043229298

Epoch: 5| Step: 3
Training loss: 2.6750717163085938
Validation loss: 2.4277726193910003

Epoch: 5| Step: 4
Training loss: 2.7830681800842285
Validation loss: 2.4405217375806583

Epoch: 5| Step: 5
Training loss: 2.55208420753479
Validation loss: 2.4402903408132572

Epoch: 5| Step: 6
Training loss: 2.339123487472534
Validation loss: 2.434652054181663

Epoch: 5| Step: 7
Training loss: 2.8097214698791504
Validation loss: 2.431578077295775

Epoch: 5| Step: 8
Training loss: 2.985929012298584
Validation loss: 2.411777880883986

Epoch: 5| Step: 9
Training loss: 2.9063711166381836
Validation loss: 2.3973151586389028

Epoch: 5| Step: 10
Training loss: 1.6739552021026611
Validation loss: 2.395120536127398

Epoch: 185| Step: 0
Training loss: 2.7095160484313965
Validation loss: 2.3911005220105572

Epoch: 5| Step: 1
Training loss: 2.548309087753296
Validation loss: 2.399343236800163

Epoch: 5| Step: 2
Training loss: 2.352344036102295
Validation loss: 2.405058814633277

Epoch: 5| Step: 3
Training loss: 2.971142530441284
Validation loss: 2.4098922437237156

Epoch: 5| Step: 4
Training loss: 2.5967955589294434
Validation loss: 2.4220849621680474

Epoch: 5| Step: 5
Training loss: 1.9550567865371704
Validation loss: 2.4224838326054234

Epoch: 5| Step: 6
Training loss: 2.388950824737549
Validation loss: 2.43337167462995

Epoch: 5| Step: 7
Training loss: 2.935457706451416
Validation loss: 2.429139444904943

Epoch: 5| Step: 8
Training loss: 2.854808807373047
Validation loss: 2.4228524969470118

Epoch: 5| Step: 9
Training loss: 3.0123684406280518
Validation loss: 2.420671196394069

Epoch: 5| Step: 10
Training loss: 2.491346836090088
Validation loss: 2.4102211972718597

Epoch: 186| Step: 0
Training loss: 2.8417115211486816
Validation loss: 2.40493602906504

Epoch: 5| Step: 1
Training loss: 2.515331268310547
Validation loss: 2.4000645734930552

Epoch: 5| Step: 2
Training loss: 2.2191615104675293
Validation loss: 2.3960138495250414

Epoch: 5| Step: 3
Training loss: 2.6065008640289307
Validation loss: 2.3961226004426197

Epoch: 5| Step: 4
Training loss: 1.8998119831085205
Validation loss: 2.404159181861467

Epoch: 5| Step: 5
Training loss: 2.4708733558654785
Validation loss: 2.4046722535164125

Epoch: 5| Step: 6
Training loss: 2.5920724868774414
Validation loss: 2.4034347072724374

Epoch: 5| Step: 7
Training loss: 3.4874515533447266
Validation loss: 2.4107503198808238

Epoch: 5| Step: 8
Training loss: 2.5830130577087402
Validation loss: 2.4068516121115735

Epoch: 5| Step: 9
Training loss: 2.664379596710205
Validation loss: 2.4151377280553183

Epoch: 5| Step: 10
Training loss: 2.8352346420288086
Validation loss: 2.412682153845346

Epoch: 187| Step: 0
Training loss: 2.679854393005371
Validation loss: 2.4250462414115987

Epoch: 5| Step: 1
Training loss: 2.283984661102295
Validation loss: 2.411643410241732

Epoch: 5| Step: 2
Training loss: 2.8297369480133057
Validation loss: 2.4101559628722486

Epoch: 5| Step: 3
Training loss: 2.63895845413208
Validation loss: 2.408100935720628

Epoch: 5| Step: 4
Training loss: 2.501574754714966
Validation loss: 2.404452259822558

Epoch: 5| Step: 5
Training loss: 2.141420364379883
Validation loss: 2.4086359034302416

Epoch: 5| Step: 6
Training loss: 3.0236401557922363
Validation loss: 2.4091856069462274

Epoch: 5| Step: 7
Training loss: 2.8234915733337402
Validation loss: 2.3983095512595227

Epoch: 5| Step: 8
Training loss: 2.808244228363037
Validation loss: 2.4004246393839517

Epoch: 5| Step: 9
Training loss: 2.523987293243408
Validation loss: 2.3993123423668647

Epoch: 5| Step: 10
Training loss: 2.3037571907043457
Validation loss: 2.3953560039561284

Epoch: 188| Step: 0
Training loss: 3.030244827270508
Validation loss: 2.3939446274952223

Epoch: 5| Step: 1
Training loss: 2.5993916988372803
Validation loss: 2.398929685674688

Epoch: 5| Step: 2
Training loss: 2.677079916000366
Validation loss: 2.4031175233984507

Epoch: 5| Step: 3
Training loss: 3.447216749191284
Validation loss: 2.3945874911482616

Epoch: 5| Step: 4
Training loss: 2.6070194244384766
Validation loss: 2.3989324287701677

Epoch: 5| Step: 5
Training loss: 2.0029938220977783
Validation loss: 2.393276814491518

Epoch: 5| Step: 6
Training loss: 2.4728646278381348
Validation loss: 2.400231335752754

Epoch: 5| Step: 7
Training loss: 2.3374438285827637
Validation loss: 2.3947817638356197

Epoch: 5| Step: 8
Training loss: 2.0608208179473877
Validation loss: 2.3987303651789182

Epoch: 5| Step: 9
Training loss: 2.8495733737945557
Validation loss: 2.399228726663897

Epoch: 5| Step: 10
Training loss: 2.4450557231903076
Validation loss: 2.4033094785546743

Epoch: 189| Step: 0
Training loss: 2.2345192432403564
Validation loss: 2.4100856934824297

Epoch: 5| Step: 1
Training loss: 2.686311721801758
Validation loss: 2.418043573697408

Epoch: 5| Step: 2
Training loss: 3.009079933166504
Validation loss: 2.4303366445725962

Epoch: 5| Step: 3
Training loss: 2.784799575805664
Validation loss: 2.426128231069093

Epoch: 5| Step: 4
Training loss: 2.124835968017578
Validation loss: 2.4204008681799776

Epoch: 5| Step: 5
Training loss: 2.3754560947418213
Validation loss: 2.4054291581594818

Epoch: 5| Step: 6
Training loss: 2.6611549854278564
Validation loss: 2.3950645897978093

Epoch: 5| Step: 7
Training loss: 2.848945379257202
Validation loss: 2.395686411088513

Epoch: 5| Step: 8
Training loss: 3.1863603591918945
Validation loss: 2.3995028670116136

Epoch: 5| Step: 9
Training loss: 2.406392812728882
Validation loss: 2.4097343362787718

Epoch: 5| Step: 10
Training loss: 2.233208179473877
Validation loss: 2.40423991346872

Epoch: 190| Step: 0
Training loss: 2.8343071937561035
Validation loss: 2.404958273774834

Epoch: 5| Step: 1
Training loss: 2.350438356399536
Validation loss: 2.4012026479167323

Epoch: 5| Step: 2
Training loss: 3.193136215209961
Validation loss: 2.402234362017724

Epoch: 5| Step: 3
Training loss: 2.4219002723693848
Validation loss: 2.3957434802927

Epoch: 5| Step: 4
Training loss: 1.7683162689208984
Validation loss: 2.394969058293168

Epoch: 5| Step: 5
Training loss: 2.4095232486724854
Validation loss: 2.4009499037137596

Epoch: 5| Step: 6
Training loss: 2.285385847091675
Validation loss: 2.3966560209951093

Epoch: 5| Step: 7
Training loss: 2.945432186126709
Validation loss: 2.394765600081413

Epoch: 5| Step: 8
Training loss: 2.4488887786865234
Validation loss: 2.405414419789468

Epoch: 5| Step: 9
Training loss: 2.801980495452881
Validation loss: 2.4067086814552225

Epoch: 5| Step: 10
Training loss: 3.300208330154419
Validation loss: 2.404775829725368

Epoch: 191| Step: 0
Training loss: 2.735170364379883
Validation loss: 2.4006751339922667

Epoch: 5| Step: 1
Training loss: 3.151089668273926
Validation loss: 2.4033304029895413

Epoch: 5| Step: 2
Training loss: 3.3692429065704346
Validation loss: 2.3950114045091855

Epoch: 5| Step: 3
Training loss: 1.7661346197128296
Validation loss: 2.3965483198883715

Epoch: 5| Step: 4
Training loss: 1.5097004175186157
Validation loss: 2.3957116270578034

Epoch: 5| Step: 5
Training loss: 2.4426023960113525
Validation loss: 2.3974551129084762

Epoch: 5| Step: 6
Training loss: 3.2987568378448486
Validation loss: 2.392085556061037

Epoch: 5| Step: 7
Training loss: 2.4993934631347656
Validation loss: 2.395243816478278

Epoch: 5| Step: 8
Training loss: 2.8491592407226562
Validation loss: 2.3894523574459936

Epoch: 5| Step: 9
Training loss: 2.270047426223755
Validation loss: 2.383289380740094

Epoch: 5| Step: 10
Training loss: 2.698141098022461
Validation loss: 2.382527046306159

Epoch: 192| Step: 0
Training loss: 2.120093822479248
Validation loss: 2.3880755798791045

Epoch: 5| Step: 1
Training loss: 2.3434576988220215
Validation loss: 2.3907029513389833

Epoch: 5| Step: 2
Training loss: 2.0500121116638184
Validation loss: 2.3902518236508934

Epoch: 5| Step: 3
Training loss: 2.2970008850097656
Validation loss: 2.3938184425395024

Epoch: 5| Step: 4
Training loss: 3.0930662155151367
Validation loss: 2.393134283763106

Epoch: 5| Step: 5
Training loss: 2.2835633754730225
Validation loss: 2.3920361867514988

Epoch: 5| Step: 6
Training loss: 2.866518497467041
Validation loss: 2.3916922897420902

Epoch: 5| Step: 7
Training loss: 3.0705313682556152
Validation loss: 2.4002151386712187

Epoch: 5| Step: 8
Training loss: 2.797288179397583
Validation loss: 2.4023549454186552

Epoch: 5| Step: 9
Training loss: 2.820638418197632
Validation loss: 2.395654978290681

Epoch: 5| Step: 10
Training loss: 2.7770020961761475
Validation loss: 2.394850900096278

Epoch: 193| Step: 0
Training loss: 2.643888235092163
Validation loss: 2.4061905132826937

Epoch: 5| Step: 1
Training loss: 2.9106078147888184
Validation loss: 2.3988089253825526

Epoch: 5| Step: 2
Training loss: 2.9171431064605713
Validation loss: 2.402299475926225

Epoch: 5| Step: 3
Training loss: 2.585763454437256
Validation loss: 2.401401445429812

Epoch: 5| Step: 4
Training loss: 3.214628219604492
Validation loss: 2.4004993438720703

Epoch: 5| Step: 5
Training loss: 1.9052718877792358
Validation loss: 2.4000130314980783

Epoch: 5| Step: 6
Training loss: 2.6112494468688965
Validation loss: 2.3965201788051154

Epoch: 5| Step: 7
Training loss: 2.545788526535034
Validation loss: 2.389593034662226

Epoch: 5| Step: 8
Training loss: 1.8740031719207764
Validation loss: 2.3868828717098443

Epoch: 5| Step: 9
Training loss: 2.491758108139038
Validation loss: 2.391235819426916

Epoch: 5| Step: 10
Training loss: 2.8268589973449707
Validation loss: 2.393374248217511

Epoch: 194| Step: 0
Training loss: 2.3584823608398438
Validation loss: 2.3895583306589434

Epoch: 5| Step: 1
Training loss: 2.756093978881836
Validation loss: 2.3893092601530013

Epoch: 5| Step: 2
Training loss: 2.511502504348755
Validation loss: 2.382687299482284

Epoch: 5| Step: 3
Training loss: 3.2974400520324707
Validation loss: 2.3870729938630135

Epoch: 5| Step: 4
Training loss: 2.726410150527954
Validation loss: 2.388816592513874

Epoch: 5| Step: 5
Training loss: 2.4963889122009277
Validation loss: 2.378526620967414

Epoch: 5| Step: 6
Training loss: 3.137214183807373
Validation loss: 2.3821492682221117

Epoch: 5| Step: 7
Training loss: 1.9384552240371704
Validation loss: 2.386289938803642

Epoch: 5| Step: 8
Training loss: 2.2787561416625977
Validation loss: 2.3868326371715916

Epoch: 5| Step: 9
Training loss: 2.6841282844543457
Validation loss: 2.3999146466614096

Epoch: 5| Step: 10
Training loss: 2.224912405014038
Validation loss: 2.3981136762967674

Epoch: 195| Step: 0
Training loss: 2.916846513748169
Validation loss: 2.3933772630588983

Epoch: 5| Step: 1
Training loss: 2.9910969734191895
Validation loss: 2.3907702199874388

Epoch: 5| Step: 2
Training loss: 2.5118448734283447
Validation loss: 2.397941179172967

Epoch: 5| Step: 3
Training loss: 1.9186527729034424
Validation loss: 2.397765339061778

Epoch: 5| Step: 4
Training loss: 1.71047043800354
Validation loss: 2.3956610797553934

Epoch: 5| Step: 5
Training loss: 3.1910085678100586
Validation loss: 2.3995226813900854

Epoch: 5| Step: 6
Training loss: 2.9673774242401123
Validation loss: 2.394756927285143

Epoch: 5| Step: 7
Training loss: 2.234402656555176
Validation loss: 2.405473880870368

Epoch: 5| Step: 8
Training loss: 2.1205592155456543
Validation loss: 2.393361665869272

Epoch: 5| Step: 9
Training loss: 2.9701342582702637
Validation loss: 2.4013666004262944

Epoch: 5| Step: 10
Training loss: 2.897941827774048
Validation loss: 2.3973665634791055

Epoch: 196| Step: 0
Training loss: 2.6513729095458984
Validation loss: 2.394024246482439

Epoch: 5| Step: 1
Training loss: 2.663966655731201
Validation loss: 2.4005565079309608

Epoch: 5| Step: 2
Training loss: 2.7228264808654785
Validation loss: 2.4034934530976

Epoch: 5| Step: 3
Training loss: 2.2664120197296143
Validation loss: 2.4036827856494534

Epoch: 5| Step: 4
Training loss: 2.842325448989868
Validation loss: 2.402688895502398

Epoch: 5| Step: 5
Training loss: 2.356337308883667
Validation loss: 2.3900703127666185

Epoch: 5| Step: 6
Training loss: 2.546674966812134
Validation loss: 2.3956695910423034

Epoch: 5| Step: 7
Training loss: 3.7148311138153076
Validation loss: 2.401636246711977

Epoch: 5| Step: 8
Training loss: 2.5630903244018555
Validation loss: 2.4089046319325766

Epoch: 5| Step: 9
Training loss: 2.3512349128723145
Validation loss: 2.400470159387076

Epoch: 5| Step: 10
Training loss: 1.6710245609283447
Validation loss: 2.3843713473248225

Epoch: 197| Step: 0
Training loss: 1.576719045639038
Validation loss: 2.3824852435819563

Epoch: 5| Step: 1
Training loss: 2.4274978637695312
Validation loss: 2.392863691494029

Epoch: 5| Step: 2
Training loss: 3.2998032569885254
Validation loss: 2.3948989939946

Epoch: 5| Step: 3
Training loss: 2.0189287662506104
Validation loss: 2.403422563306747

Epoch: 5| Step: 4
Training loss: 2.822364091873169
Validation loss: 2.4002195455694713

Epoch: 5| Step: 5
Training loss: 2.0471625328063965
Validation loss: 2.4022667100352626

Epoch: 5| Step: 6
Training loss: 2.984890937805176
Validation loss: 2.392328998093964

Epoch: 5| Step: 7
Training loss: 2.907505989074707
Validation loss: 2.3956046206976778

Epoch: 5| Step: 8
Training loss: 2.7680721282958984
Validation loss: 2.3941709559450866

Epoch: 5| Step: 9
Training loss: 2.846057176589966
Validation loss: 2.3932893378760225

Epoch: 5| Step: 10
Training loss: 2.6777842044830322
Validation loss: 2.394281848784416

Epoch: 198| Step: 0
Training loss: 2.252847671508789
Validation loss: 2.4044622785301617

Epoch: 5| Step: 1
Training loss: 2.750819683074951
Validation loss: 2.393859527444327

Epoch: 5| Step: 2
Training loss: 2.2231762409210205
Validation loss: 2.391850281787175

Epoch: 5| Step: 3
Training loss: 2.7769827842712402
Validation loss: 2.392663004577801

Epoch: 5| Step: 4
Training loss: 2.8635449409484863
Validation loss: 2.3819232602273264

Epoch: 5| Step: 5
Training loss: 2.5822489261627197
Validation loss: 2.3870952360091673

Epoch: 5| Step: 6
Training loss: 2.6065514087677
Validation loss: 2.3759572531587336

Epoch: 5| Step: 7
Training loss: 1.847313642501831
Validation loss: 2.3792946018198484

Epoch: 5| Step: 8
Training loss: 2.7598605155944824
Validation loss: 2.3793571482422533

Epoch: 5| Step: 9
Training loss: 2.467921018600464
Validation loss: 2.3746406929467314

Epoch: 5| Step: 10
Training loss: 3.368100166320801
Validation loss: 2.3756037219878166

Epoch: 199| Step: 0
Training loss: 2.7226881980895996
Validation loss: 2.381856728625554

Epoch: 5| Step: 1
Training loss: 2.2930984497070312
Validation loss: 2.388333853854928

Epoch: 5| Step: 2
Training loss: 2.290198564529419
Validation loss: 2.391402911114436

Epoch: 5| Step: 3
Training loss: 2.8900492191314697
Validation loss: 2.3915994154509677

Epoch: 5| Step: 4
Training loss: 2.2342140674591064
Validation loss: 2.391503700646021

Epoch: 5| Step: 5
Training loss: 2.8690218925476074
Validation loss: 2.3907202674496557

Epoch: 5| Step: 6
Training loss: 3.0213968753814697
Validation loss: 2.3887771944845877

Epoch: 5| Step: 7
Training loss: 2.502182722091675
Validation loss: 2.380708853403727

Epoch: 5| Step: 8
Training loss: 2.7724385261535645
Validation loss: 2.3891526576011413

Epoch: 5| Step: 9
Training loss: 2.008862018585205
Validation loss: 2.397400984200098

Epoch: 5| Step: 10
Training loss: 2.769075393676758
Validation loss: 2.408822577486756

Epoch: 200| Step: 0
Training loss: 2.402940034866333
Validation loss: 2.4074858850048435

Epoch: 5| Step: 1
Training loss: 2.8981940746307373
Validation loss: 2.4068513326747443

Epoch: 5| Step: 2
Training loss: 3.0719423294067383
Validation loss: 2.385627313326764

Epoch: 5| Step: 3
Training loss: 2.901834487915039
Validation loss: 2.3830337857687347

Epoch: 5| Step: 4
Training loss: 2.748171329498291
Validation loss: 2.3760030705441713

Epoch: 5| Step: 5
Training loss: 2.4266371726989746
Validation loss: 2.3666734797980196

Epoch: 5| Step: 6
Training loss: 2.460615873336792
Validation loss: 2.3682441044879217

Epoch: 5| Step: 7
Training loss: 2.7297630310058594
Validation loss: 2.3707267597157466

Epoch: 5| Step: 8
Training loss: 2.3010966777801514
Validation loss: 2.368618876703324

Epoch: 5| Step: 9
Training loss: 2.3045003414154053
Validation loss: 2.3714122003124607

Epoch: 5| Step: 10
Training loss: 1.994990587234497
Validation loss: 2.3722347110830326

Epoch: 201| Step: 0
Training loss: 2.028048038482666
Validation loss: 2.3723697726444533

Epoch: 5| Step: 1
Training loss: 3.11411190032959
Validation loss: 2.383433436834684

Epoch: 5| Step: 2
Training loss: 3.1324782371520996
Validation loss: 2.3924889179968063

Epoch: 5| Step: 3
Training loss: 2.6565799713134766
Validation loss: 2.3963848006340767

Epoch: 5| Step: 4
Training loss: 2.49971604347229
Validation loss: 2.397889309031989

Epoch: 5| Step: 5
Training loss: 2.4360525608062744
Validation loss: 2.4040466406012095

Epoch: 5| Step: 6
Training loss: 3.3807945251464844
Validation loss: 2.402365497363511

Epoch: 5| Step: 7
Training loss: 1.8804528713226318
Validation loss: 2.3844326798633864

Epoch: 5| Step: 8
Training loss: 2.046574115753174
Validation loss: 2.3790412872068343

Epoch: 5| Step: 9
Training loss: 2.4196128845214844
Validation loss: 2.3784864769187024

Epoch: 5| Step: 10
Training loss: 2.776505947113037
Validation loss: 2.371013856703235

Epoch: 202| Step: 0
Training loss: 2.64335560798645
Validation loss: 2.3739803196281515

Epoch: 5| Step: 1
Training loss: 1.9771060943603516
Validation loss: 2.3636407698354414

Epoch: 5| Step: 2
Training loss: 2.4698843955993652
Validation loss: 2.365684927150767

Epoch: 5| Step: 3
Training loss: 2.653116464614868
Validation loss: 2.358808737929149

Epoch: 5| Step: 4
Training loss: 2.4342570304870605
Validation loss: 2.36198498869455

Epoch: 5| Step: 5
Training loss: 2.3334641456604004
Validation loss: 2.3639241623622116

Epoch: 5| Step: 6
Training loss: 2.9030849933624268
Validation loss: 2.369897629625054

Epoch: 5| Step: 7
Training loss: 2.436763286590576
Validation loss: 2.371290749119174

Epoch: 5| Step: 8
Training loss: 2.2903096675872803
Validation loss: 2.3818121879331526

Epoch: 5| Step: 9
Training loss: 2.8221821784973145
Validation loss: 2.3885489843224965

Epoch: 5| Step: 10
Training loss: 3.443692684173584
Validation loss: 2.3946382768692507

Epoch: 203| Step: 0
Training loss: 2.720679521560669
Validation loss: 2.3947383729360436

Epoch: 5| Step: 1
Training loss: 2.7626168727874756
Validation loss: 2.394684048109157

Epoch: 5| Step: 2
Training loss: 2.277323007583618
Validation loss: 2.4033082198071223

Epoch: 5| Step: 3
Training loss: 2.0171761512756348
Validation loss: 2.408856999489569

Epoch: 5| Step: 4
Training loss: 2.5424506664276123
Validation loss: 2.4085239338618454

Epoch: 5| Step: 5
Training loss: 2.64003586769104
Validation loss: 2.413901041912776

Epoch: 5| Step: 6
Training loss: 2.652400016784668
Validation loss: 2.407494398855394

Epoch: 5| Step: 7
Training loss: 3.0444400310516357
Validation loss: 2.389325487998224

Epoch: 5| Step: 8
Training loss: 2.536745071411133
Validation loss: 2.3783933065270864

Epoch: 5| Step: 9
Training loss: 2.6125786304473877
Validation loss: 2.3706845596272457

Epoch: 5| Step: 10
Training loss: 2.4664885997772217
Validation loss: 2.372971910302357

Epoch: 204| Step: 0
Training loss: 2.4210398197174072
Validation loss: 2.366213931832262

Epoch: 5| Step: 1
Training loss: 3.4762916564941406
Validation loss: 2.3583305061504407

Epoch: 5| Step: 2
Training loss: 2.2894952297210693
Validation loss: 2.365309343543104

Epoch: 5| Step: 3
Training loss: 2.4788711071014404
Validation loss: 2.3651631403994817

Epoch: 5| Step: 4
Training loss: 2.755312919616699
Validation loss: 2.3679347474087953

Epoch: 5| Step: 5
Training loss: 2.3171374797821045
Validation loss: 2.3688244435095016

Epoch: 5| Step: 6
Training loss: 2.461547374725342
Validation loss: 2.375661297511029

Epoch: 5| Step: 7
Training loss: 2.8606109619140625
Validation loss: 2.3765320008800876

Epoch: 5| Step: 8
Training loss: 2.4755618572235107
Validation loss: 2.3812835601068314

Epoch: 5| Step: 9
Training loss: 2.2465028762817383
Validation loss: 2.370951114162322

Epoch: 5| Step: 10
Training loss: 2.5447518825531006
Validation loss: 2.3641702718632196

Epoch: 205| Step: 0
Training loss: 2.67207670211792
Validation loss: 2.3651283505142375

Epoch: 5| Step: 1
Training loss: 2.908802032470703
Validation loss: 2.3736527786459973

Epoch: 5| Step: 2
Training loss: 2.7468743324279785
Validation loss: 2.3683743041048766

Epoch: 5| Step: 3
Training loss: 2.63922119140625
Validation loss: 2.3756451478568454

Epoch: 5| Step: 4
Training loss: 2.4350006580352783
Validation loss: 2.3814313091257566

Epoch: 5| Step: 5
Training loss: 2.3723058700561523
Validation loss: 2.379049680566275

Epoch: 5| Step: 6
Training loss: 2.859708309173584
Validation loss: 2.382757532981134

Epoch: 5| Step: 7
Training loss: 2.459102153778076
Validation loss: 2.389193509214668

Epoch: 5| Step: 8
Training loss: 2.517241954803467
Validation loss: 2.393753533722252

Epoch: 5| Step: 9
Training loss: 2.337722063064575
Validation loss: 2.3873257021750174

Epoch: 5| Step: 10
Training loss: 2.2272281646728516
Validation loss: 2.394827497902737

Epoch: 206| Step: 0
Training loss: 2.919654369354248
Validation loss: 2.3957443673123597

Epoch: 5| Step: 1
Training loss: 2.4587912559509277
Validation loss: 2.4065023288931897

Epoch: 5| Step: 2
Training loss: 2.7485432624816895
Validation loss: 2.4112311678548015

Epoch: 5| Step: 3
Training loss: 2.409095048904419
Validation loss: 2.417623940334525

Epoch: 5| Step: 4
Training loss: 2.729175567626953
Validation loss: 2.4135846322582615

Epoch: 5| Step: 5
Training loss: 2.109980344772339
Validation loss: 2.403636868282031

Epoch: 5| Step: 6
Training loss: 2.3855061531066895
Validation loss: 2.387141504595357

Epoch: 5| Step: 7
Training loss: 2.4215075969696045
Validation loss: 2.3832772316471225

Epoch: 5| Step: 8
Training loss: 3.3191165924072266
Validation loss: 2.3723439760105585

Epoch: 5| Step: 9
Training loss: 2.1890413761138916
Validation loss: 2.3648563533700924

Epoch: 5| Step: 10
Training loss: 2.5608551502227783
Validation loss: 2.3710598445707753

Epoch: 207| Step: 0
Training loss: 2.6748297214508057
Validation loss: 2.3828432867603917

Epoch: 5| Step: 1
Training loss: 2.513484239578247
Validation loss: 2.3816661860353205

Epoch: 5| Step: 2
Training loss: 2.565774917602539
Validation loss: 2.401349590670678

Epoch: 5| Step: 3
Training loss: 2.5227935314178467
Validation loss: 2.4106346817426783

Epoch: 5| Step: 4
Training loss: 3.057701826095581
Validation loss: 2.4179114680136404

Epoch: 5| Step: 5
Training loss: 2.2017571926116943
Validation loss: 2.401702057930731

Epoch: 5| Step: 6
Training loss: 3.1760082244873047
Validation loss: 2.3941659773549726

Epoch: 5| Step: 7
Training loss: 2.7186989784240723
Validation loss: 2.383146329592633

Epoch: 5| Step: 8
Training loss: 2.0540611743927
Validation loss: 2.389472971680344

Epoch: 5| Step: 9
Training loss: 2.5986838340759277
Validation loss: 2.4071966243046585

Epoch: 5| Step: 10
Training loss: 2.07165789604187
Validation loss: 2.440096342435447

Epoch: 208| Step: 0
Training loss: 3.0656485557556152
Validation loss: 2.4333697698449575

Epoch: 5| Step: 1
Training loss: 3.1072463989257812
Validation loss: 2.4322353588637484

Epoch: 5| Step: 2
Training loss: 2.4444661140441895
Validation loss: 2.422244579561295

Epoch: 5| Step: 3
Training loss: 2.2537004947662354
Validation loss: 2.4096153064440657

Epoch: 5| Step: 4
Training loss: 2.18506121635437
Validation loss: 2.387080789894186

Epoch: 5| Step: 5
Training loss: 2.5195670127868652
Validation loss: 2.3763413480533067

Epoch: 5| Step: 6
Training loss: 2.2281222343444824
Validation loss: 2.37569546955888

Epoch: 5| Step: 7
Training loss: 2.7952170372009277
Validation loss: 2.3550405207500664

Epoch: 5| Step: 8
Training loss: 2.440568208694458
Validation loss: 2.3599236011505127

Epoch: 5| Step: 9
Training loss: 2.6430206298828125
Validation loss: 2.3491209783861713

Epoch: 5| Step: 10
Training loss: 2.691704273223877
Validation loss: 2.342017650604248

Epoch: 209| Step: 0
Training loss: 2.289863348007202
Validation loss: 2.3357837674438313

Epoch: 5| Step: 1
Training loss: 2.0470118522644043
Validation loss: 2.345152603682651

Epoch: 5| Step: 2
Training loss: 2.199007749557495
Validation loss: 2.3530422795203423

Epoch: 5| Step: 3
Training loss: 2.524740695953369
Validation loss: 2.375605139681088

Epoch: 5| Step: 4
Training loss: 2.779658079147339
Validation loss: 2.368858319456859

Epoch: 5| Step: 5
Training loss: 2.8918027877807617
Validation loss: 2.3748940139688473

Epoch: 5| Step: 6
Training loss: 1.9332067966461182
Validation loss: 2.375716893903671

Epoch: 5| Step: 7
Training loss: 3.0644638538360596
Validation loss: 2.3805636487981325

Epoch: 5| Step: 8
Training loss: 2.7739357948303223
Validation loss: 2.3961555624520905

Epoch: 5| Step: 9
Training loss: 3.0324816703796387
Validation loss: 2.398358970560053

Epoch: 5| Step: 10
Training loss: 2.770129442214966
Validation loss: 2.3838332442827124

Epoch: 210| Step: 0
Training loss: 2.530698299407959
Validation loss: 2.3778413675164662

Epoch: 5| Step: 1
Training loss: 2.6623051166534424
Validation loss: 2.3719008776449386

Epoch: 5| Step: 2
Training loss: 2.4379591941833496
Validation loss: 2.3709399674528386

Epoch: 5| Step: 3
Training loss: 2.062427043914795
Validation loss: 2.364360596543999

Epoch: 5| Step: 4
Training loss: 2.7666733264923096
Validation loss: 2.375588147870956

Epoch: 5| Step: 5
Training loss: 3.6175904273986816
Validation loss: 2.3767246136101345

Epoch: 5| Step: 6
Training loss: 2.2032129764556885
Validation loss: 2.3769916693369546

Epoch: 5| Step: 7
Training loss: 2.6139872074127197
Validation loss: 2.368901442455989

Epoch: 5| Step: 8
Training loss: 2.3169658184051514
Validation loss: 2.3580406506856284

Epoch: 5| Step: 9
Training loss: 2.681685447692871
Validation loss: 2.347389744174096

Epoch: 5| Step: 10
Training loss: 2.186339855194092
Validation loss: 2.354249744005101

Epoch: 211| Step: 0
Training loss: 3.0683326721191406
Validation loss: 2.354958285567581

Epoch: 5| Step: 1
Training loss: 2.8868703842163086
Validation loss: 2.352329005477249

Epoch: 5| Step: 2
Training loss: 2.9443094730377197
Validation loss: 2.3506163807325464

Epoch: 5| Step: 3
Training loss: 2.5581188201904297
Validation loss: 2.3466561430244037

Epoch: 5| Step: 4
Training loss: 2.8284242153167725
Validation loss: 2.3479895719917874

Epoch: 5| Step: 5
Training loss: 2.4049580097198486
Validation loss: 2.35754922897585

Epoch: 5| Step: 6
Training loss: 2.5045244693756104
Validation loss: 2.3614169000297465

Epoch: 5| Step: 7
Training loss: 2.567153215408325
Validation loss: 2.352151886109383

Epoch: 5| Step: 8
Training loss: 2.2770633697509766
Validation loss: 2.3563707285029913

Epoch: 5| Step: 9
Training loss: 2.283717632293701
Validation loss: 2.3521127341895975

Epoch: 5| Step: 10
Training loss: 1.7428934574127197
Validation loss: 2.348488325713783

Epoch: 212| Step: 0
Training loss: 2.963266372680664
Validation loss: 2.3450012822305

Epoch: 5| Step: 1
Training loss: 2.66972017288208
Validation loss: 2.345512584973407

Epoch: 5| Step: 2
Training loss: 2.07684326171875
Validation loss: 2.3582448549168085

Epoch: 5| Step: 3
Training loss: 2.4856553077697754
Validation loss: 2.369827954999862

Epoch: 5| Step: 4
Training loss: 3.0254340171813965
Validation loss: 2.3820707336548836

Epoch: 5| Step: 5
Training loss: 2.3849284648895264
Validation loss: 2.375033888765561

Epoch: 5| Step: 6
Training loss: 2.5304770469665527
Validation loss: 2.366584816286641

Epoch: 5| Step: 7
Training loss: 2.390716552734375
Validation loss: 2.3694049824950514

Epoch: 5| Step: 8
Training loss: 2.5900323390960693
Validation loss: 2.36010972146065

Epoch: 5| Step: 9
Training loss: 1.9170150756835938
Validation loss: 2.3658068641539542

Epoch: 5| Step: 10
Training loss: 3.16828989982605
Validation loss: 2.3813377349607405

Epoch: 213| Step: 0
Training loss: 2.1910057067871094
Validation loss: 2.3895194351032214

Epoch: 5| Step: 1
Training loss: 2.635087251663208
Validation loss: 2.393902809389176

Epoch: 5| Step: 2
Training loss: 2.2814221382141113
Validation loss: 2.3841287833388134

Epoch: 5| Step: 3
Training loss: 2.5593316555023193
Validation loss: 2.3673983568786294

Epoch: 5| Step: 4
Training loss: 2.538372278213501
Validation loss: 2.3600959649649997

Epoch: 5| Step: 5
Training loss: 2.1480746269226074
Validation loss: 2.3437586061416136

Epoch: 5| Step: 6
Training loss: 2.4734413623809814
Validation loss: 2.3438996397038943

Epoch: 5| Step: 7
Training loss: 3.190920352935791
Validation loss: 2.34820161455421

Epoch: 5| Step: 8
Training loss: 2.602217674255371
Validation loss: 2.348531330785444

Epoch: 5| Step: 9
Training loss: 2.511779308319092
Validation loss: 2.341216625705842

Epoch: 5| Step: 10
Training loss: 2.9986743927001953
Validation loss: 2.345358135879681

Epoch: 214| Step: 0
Training loss: 2.815695285797119
Validation loss: 2.3447509811770533

Epoch: 5| Step: 1
Training loss: 2.8430848121643066
Validation loss: 2.3467662488260577

Epoch: 5| Step: 2
Training loss: 3.0511679649353027
Validation loss: 2.3413283799284246

Epoch: 5| Step: 3
Training loss: 1.8139053583145142
Validation loss: 2.346257758396928

Epoch: 5| Step: 4
Training loss: 2.610809087753296
Validation loss: 2.3553642303712907

Epoch: 5| Step: 5
Training loss: 2.719588041305542
Validation loss: 2.361006239409088

Epoch: 5| Step: 6
Training loss: 2.5773048400878906
Validation loss: 2.362093863948699

Epoch: 5| Step: 7
Training loss: 1.8279399871826172
Validation loss: 2.3560549956496044

Epoch: 5| Step: 8
Training loss: 2.5932703018188477
Validation loss: 2.348794437223865

Epoch: 5| Step: 9
Training loss: 2.7945151329040527
Validation loss: 2.356832124853647

Epoch: 5| Step: 10
Training loss: 2.2551212310791016
Validation loss: 2.3561997387998845

Epoch: 215| Step: 0
Training loss: 2.162830352783203
Validation loss: 2.3491524586113552

Epoch: 5| Step: 1
Training loss: 1.926775336265564
Validation loss: 2.351088854574388

Epoch: 5| Step: 2
Training loss: 2.7873501777648926
Validation loss: 2.3483262549164476

Epoch: 5| Step: 3
Training loss: 2.8623745441436768
Validation loss: 2.346265173727466

Epoch: 5| Step: 4
Training loss: 2.497999668121338
Validation loss: 2.364783581867013

Epoch: 5| Step: 5
Training loss: 3.0988056659698486
Validation loss: 2.360230174115909

Epoch: 5| Step: 6
Training loss: 2.208458423614502
Validation loss: 2.354107197894845

Epoch: 5| Step: 7
Training loss: 2.6557908058166504
Validation loss: 2.3549237123099704

Epoch: 5| Step: 8
Training loss: 2.2387468814849854
Validation loss: 2.356454289087685

Epoch: 5| Step: 9
Training loss: 2.820977210998535
Validation loss: 2.359220499633461

Epoch: 5| Step: 10
Training loss: 2.733915090560913
Validation loss: 2.357599853187479

Epoch: 216| Step: 0
Training loss: 2.297881603240967
Validation loss: 2.3600318611309095

Epoch: 5| Step: 1
Training loss: 2.8744654655456543
Validation loss: 2.3518110808505805

Epoch: 5| Step: 2
Training loss: 3.137453317642212
Validation loss: 2.3556924763546196

Epoch: 5| Step: 3
Training loss: 2.84922194480896
Validation loss: 2.3501716506096626

Epoch: 5| Step: 4
Training loss: 2.3458001613616943
Validation loss: 2.3454671111158145

Epoch: 5| Step: 5
Training loss: 2.623126268386841
Validation loss: 2.3413204813516266

Epoch: 5| Step: 6
Training loss: 2.459791898727417
Validation loss: 2.3436688582102456

Epoch: 5| Step: 7
Training loss: 1.7091821432113647
Validation loss: 2.3379340146177556

Epoch: 5| Step: 8
Training loss: 2.1840877532958984
Validation loss: 2.34030907641175

Epoch: 5| Step: 9
Training loss: 2.702784299850464
Validation loss: 2.362818051409978

Epoch: 5| Step: 10
Training loss: 2.7805099487304688
Validation loss: 2.364171258864864

Epoch: 217| Step: 0
Training loss: 2.107912540435791
Validation loss: 2.3538422456351658

Epoch: 5| Step: 1
Training loss: 2.613661527633667
Validation loss: 2.362617210675311

Epoch: 5| Step: 2
Training loss: 2.282020092010498
Validation loss: 2.3569517494529806

Epoch: 5| Step: 3
Training loss: 2.6484954357147217
Validation loss: 2.3461803313224547

Epoch: 5| Step: 4
Training loss: 2.580711841583252
Validation loss: 2.3497329963150846

Epoch: 5| Step: 5
Training loss: 2.89754056930542
Validation loss: 2.3466741192725395

Epoch: 5| Step: 6
Training loss: 1.9193222522735596
Validation loss: 2.343921317849108

Epoch: 5| Step: 7
Training loss: 2.507847309112549
Validation loss: 2.3417036328264462

Epoch: 5| Step: 8
Training loss: 1.9499485492706299
Validation loss: 2.334091058341406

Epoch: 5| Step: 9
Training loss: 3.223897933959961
Validation loss: 2.3423892451870825

Epoch: 5| Step: 10
Training loss: 3.210184097290039
Validation loss: 2.3377264571446243

Epoch: 218| Step: 0
Training loss: 2.5153396129608154
Validation loss: 2.3328666353738434

Epoch: 5| Step: 1
Training loss: 2.893187999725342
Validation loss: 2.3343918400426067

Epoch: 5| Step: 2
Training loss: 2.726090908050537
Validation loss: 2.3401836772118845

Epoch: 5| Step: 3
Training loss: 2.851778507232666
Validation loss: 2.3491154896315707

Epoch: 5| Step: 4
Training loss: 2.2115774154663086
Validation loss: 2.3434734113754763

Epoch: 5| Step: 5
Training loss: 2.629035472869873
Validation loss: 2.3524231180067985

Epoch: 5| Step: 6
Training loss: 2.716865301132202
Validation loss: 2.3590960758988575

Epoch: 5| Step: 7
Training loss: 2.50734281539917
Validation loss: 2.362555206462901

Epoch: 5| Step: 8
Training loss: 2.681591510772705
Validation loss: 2.3585789357462237

Epoch: 5| Step: 9
Training loss: 2.441713809967041
Validation loss: 2.3553201485705633

Epoch: 5| Step: 10
Training loss: 1.5015254020690918
Validation loss: 2.3381289961517497

Epoch: 219| Step: 0
Training loss: 2.0516936779022217
Validation loss: 2.33706231527431

Epoch: 5| Step: 1
Training loss: 2.646251678466797
Validation loss: 2.334274632956392

Epoch: 5| Step: 2
Training loss: 3.1152451038360596
Validation loss: 2.337672354072653

Epoch: 5| Step: 3
Training loss: 2.4052557945251465
Validation loss: 2.3382369497770905

Epoch: 5| Step: 4
Training loss: 2.309807538986206
Validation loss: 2.337545935825635

Epoch: 5| Step: 5
Training loss: 2.373749256134033
Validation loss: 2.3426919316732757

Epoch: 5| Step: 6
Training loss: 2.4763574600219727
Validation loss: 2.3481752693012194

Epoch: 5| Step: 7
Training loss: 2.09588360786438
Validation loss: 2.3555371658776396

Epoch: 5| Step: 8
Training loss: 2.40303373336792
Validation loss: 2.344256470280309

Epoch: 5| Step: 9
Training loss: 2.4205453395843506
Validation loss: 2.345601047238996

Epoch: 5| Step: 10
Training loss: 3.581873655319214
Validation loss: 2.341042059724049

Epoch: 220| Step: 0
Training loss: 2.5493645668029785
Validation loss: 2.3344564719866683

Epoch: 5| Step: 1
Training loss: 3.029634952545166
Validation loss: 2.333023363544095

Epoch: 5| Step: 2
Training loss: 2.4161529541015625
Validation loss: 2.3431401150200957

Epoch: 5| Step: 3
Training loss: 2.7193644046783447
Validation loss: 2.352131894839707

Epoch: 5| Step: 4
Training loss: 2.4066081047058105
Validation loss: 2.3638581204158005

Epoch: 5| Step: 5
Training loss: 2.9662222862243652
Validation loss: 2.361945422746802

Epoch: 5| Step: 6
Training loss: 1.9792964458465576
Validation loss: 2.354765271627775

Epoch: 5| Step: 7
Training loss: 2.4174633026123047
Validation loss: 2.348347471606347

Epoch: 5| Step: 8
Training loss: 2.0387072563171387
Validation loss: 2.3345786704812

Epoch: 5| Step: 9
Training loss: 2.8523266315460205
Validation loss: 2.3237412039951613

Epoch: 5| Step: 10
Training loss: 2.4354445934295654
Validation loss: 2.3257471002558225

Epoch: 221| Step: 0
Training loss: 1.9742467403411865
Validation loss: 2.3328919128705095

Epoch: 5| Step: 1
Training loss: 2.291703224182129
Validation loss: 2.3342820982779227

Epoch: 5| Step: 2
Training loss: 2.07069730758667
Validation loss: 2.3320735974978377

Epoch: 5| Step: 3
Training loss: 2.9133529663085938
Validation loss: 2.349478775455106

Epoch: 5| Step: 4
Training loss: 2.6382949352264404
Validation loss: 2.3887623125506985

Epoch: 5| Step: 5
Training loss: 2.330872058868408
Validation loss: 2.3902069804488972

Epoch: 5| Step: 6
Training loss: 2.4015910625457764
Validation loss: 2.3965996209011284

Epoch: 5| Step: 7
Training loss: 2.3116440773010254
Validation loss: 2.3785350681633077

Epoch: 5| Step: 8
Training loss: 3.432952880859375
Validation loss: 2.3540458012652654

Epoch: 5| Step: 9
Training loss: 2.673705577850342
Validation loss: 2.327510674794515

Epoch: 5| Step: 10
Training loss: 2.799668550491333
Validation loss: 2.3336294030630462

Epoch: 222| Step: 0
Training loss: 2.4948713779449463
Validation loss: 2.345024178105016

Epoch: 5| Step: 1
Training loss: 3.3327438831329346
Validation loss: 2.347890289880896

Epoch: 5| Step: 2
Training loss: 2.6238436698913574
Validation loss: 2.3619745341680383

Epoch: 5| Step: 3
Training loss: 2.4245638847351074
Validation loss: 2.3520511017050794

Epoch: 5| Step: 4
Training loss: 2.189772605895996
Validation loss: 2.351650545673986

Epoch: 5| Step: 5
Training loss: 2.694122076034546
Validation loss: 2.342939717795259

Epoch: 5| Step: 6
Training loss: 2.2708053588867188
Validation loss: 2.3400020676274456

Epoch: 5| Step: 7
Training loss: 2.9900119304656982
Validation loss: 2.3508574834433933

Epoch: 5| Step: 8
Training loss: 1.6827590465545654
Validation loss: 2.3576422045307774

Epoch: 5| Step: 9
Training loss: 2.490187168121338
Validation loss: 2.3773930508603334

Epoch: 5| Step: 10
Training loss: 2.863577127456665
Validation loss: 2.390857381205405

Epoch: 223| Step: 0
Training loss: 2.949758291244507
Validation loss: 2.3602242572333223

Epoch: 5| Step: 1
Training loss: 2.2801918983459473
Validation loss: 2.3730837888615106

Epoch: 5| Step: 2
Training loss: 2.8283259868621826
Validation loss: 2.3598034407502864

Epoch: 5| Step: 3
Training loss: 1.939969778060913
Validation loss: 2.357985965667232

Epoch: 5| Step: 4
Training loss: 3.029149055480957
Validation loss: 2.349870668944492

Epoch: 5| Step: 5
Training loss: 2.3530311584472656
Validation loss: 2.3431544278257634

Epoch: 5| Step: 6
Training loss: 2.5789437294006348
Validation loss: 2.3479258962856826

Epoch: 5| Step: 7
Training loss: 2.1790413856506348
Validation loss: 2.3450416211158998

Epoch: 5| Step: 8
Training loss: 2.2021594047546387
Validation loss: 2.34595831747978

Epoch: 5| Step: 9
Training loss: 3.077587604522705
Validation loss: 2.3421944777170816

Epoch: 5| Step: 10
Training loss: 2.327255964279175
Validation loss: 2.3441116553480907

Epoch: 224| Step: 0
Training loss: 1.447096586227417
Validation loss: 2.33265455051135

Epoch: 5| Step: 1
Training loss: 1.761985421180725
Validation loss: 2.3228128135845227

Epoch: 5| Step: 2
Training loss: 3.3355095386505127
Validation loss: 2.3255635461499615

Epoch: 5| Step: 3
Training loss: 3.2491660118103027
Validation loss: 2.332130145001155

Epoch: 5| Step: 4
Training loss: 2.9593260288238525
Validation loss: 2.335587704053489

Epoch: 5| Step: 5
Training loss: 2.033129930496216
Validation loss: 2.3509142116833757

Epoch: 5| Step: 6
Training loss: 2.7155842781066895
Validation loss: 2.347880240409605

Epoch: 5| Step: 7
Training loss: 2.477442979812622
Validation loss: 2.3454886482607935

Epoch: 5| Step: 8
Training loss: 2.5384340286254883
Validation loss: 2.347512547687818

Epoch: 5| Step: 9
Training loss: 2.3958709239959717
Validation loss: 2.350636059238065

Epoch: 5| Step: 10
Training loss: 2.8411715030670166
Validation loss: 2.341825423702117

Epoch: 225| Step: 0
Training loss: 2.071448802947998
Validation loss: 2.335138941323885

Epoch: 5| Step: 1
Training loss: 2.518038749694824
Validation loss: 2.3305622249521236

Epoch: 5| Step: 2
Training loss: 2.3923630714416504
Validation loss: 2.3324583679117183

Epoch: 5| Step: 3
Training loss: 2.3042023181915283
Validation loss: 2.3316780290296

Epoch: 5| Step: 4
Training loss: 2.8520171642303467
Validation loss: 2.337781483127225

Epoch: 5| Step: 5
Training loss: 2.2701547145843506
Validation loss: 2.3324626286824546

Epoch: 5| Step: 6
Training loss: 2.45477294921875
Validation loss: 2.3437933203994588

Epoch: 5| Step: 7
Training loss: 2.914963483810425
Validation loss: 2.3434794051672823

Epoch: 5| Step: 8
Training loss: 2.409048557281494
Validation loss: 2.338847839704124

Epoch: 5| Step: 9
Training loss: 2.777477741241455
Validation loss: 2.34999260979314

Epoch: 5| Step: 10
Training loss: 2.7771971225738525
Validation loss: 2.3452315638142247

Epoch: 226| Step: 0
Training loss: 2.4892287254333496
Validation loss: 2.343000973424604

Epoch: 5| Step: 1
Training loss: 1.9369522333145142
Validation loss: 2.3360295987898305

Epoch: 5| Step: 2
Training loss: 2.543184757232666
Validation loss: 2.33870743807926

Epoch: 5| Step: 3
Training loss: 2.9888739585876465
Validation loss: 2.3482057945702666

Epoch: 5| Step: 4
Training loss: 2.4707443714141846
Validation loss: 2.336155976018598

Epoch: 5| Step: 5
Training loss: 2.4073429107666016
Validation loss: 2.3445972165753766

Epoch: 5| Step: 6
Training loss: 2.3271446228027344
Validation loss: 2.3355364440589823

Epoch: 5| Step: 7
Training loss: 2.249818801879883
Validation loss: 2.3286454831400225

Epoch: 5| Step: 8
Training loss: 3.0288548469543457
Validation loss: 2.3278959643456245

Epoch: 5| Step: 9
Training loss: 2.445517063140869
Validation loss: 2.324525322965396

Epoch: 5| Step: 10
Training loss: 2.609015464782715
Validation loss: 2.3263952706449773

Epoch: 227| Step: 0
Training loss: 2.611605405807495
Validation loss: 2.347230642072616

Epoch: 5| Step: 1
Training loss: 2.5283617973327637
Validation loss: 2.3393440554218907

Epoch: 5| Step: 2
Training loss: 2.5444252490997314
Validation loss: 2.3392471190421813

Epoch: 5| Step: 3
Training loss: 2.8518919944763184
Validation loss: 2.3299619587518836

Epoch: 5| Step: 4
Training loss: 2.149746894836426
Validation loss: 2.320378566300997

Epoch: 5| Step: 5
Training loss: 2.0810608863830566
Validation loss: 2.322041021880283

Epoch: 5| Step: 6
Training loss: 2.5155787467956543
Validation loss: 2.3377590487080235

Epoch: 5| Step: 7
Training loss: 1.9998571872711182
Validation loss: 2.3604904297859437

Epoch: 5| Step: 8
Training loss: 3.1316142082214355
Validation loss: 2.3847536169072634

Epoch: 5| Step: 9
Training loss: 2.6987318992614746
Validation loss: 2.407145448910293

Epoch: 5| Step: 10
Training loss: 2.7228047847747803
Validation loss: 2.4006767144767185

Epoch: 228| Step: 0
Training loss: 2.5431435108184814
Validation loss: 2.3858083140465522

Epoch: 5| Step: 1
Training loss: 2.560382127761841
Validation loss: 2.3659400209303825

Epoch: 5| Step: 2
Training loss: 3.340184450149536
Validation loss: 2.350966474061371

Epoch: 5| Step: 3
Training loss: 2.186192274093628
Validation loss: 2.33804617774102

Epoch: 5| Step: 4
Training loss: 1.66879403591156
Validation loss: 2.339835610440982

Epoch: 5| Step: 5
Training loss: 2.846299171447754
Validation loss: 2.330394906382407

Epoch: 5| Step: 6
Training loss: 2.134298801422119
Validation loss: 2.330297285510648

Epoch: 5| Step: 7
Training loss: 3.0737273693084717
Validation loss: 2.3319052880810154

Epoch: 5| Step: 8
Training loss: 2.4270856380462646
Validation loss: 2.3365618439130884

Epoch: 5| Step: 9
Training loss: 2.533139228820801
Validation loss: 2.3352161581798265

Epoch: 5| Step: 10
Training loss: 2.1262736320495605
Validation loss: 2.334793702248604

Epoch: 229| Step: 0
Training loss: 2.6145875453948975
Validation loss: 2.3298879092739475

Epoch: 5| Step: 1
Training loss: 2.200817346572876
Validation loss: 2.3219302008228917

Epoch: 5| Step: 2
Training loss: 2.3230082988739014
Validation loss: 2.326504558645269

Epoch: 5| Step: 3
Training loss: 2.3042571544647217
Validation loss: 2.328831518850019

Epoch: 5| Step: 4
Training loss: 2.1920228004455566
Validation loss: 2.327787260855398

Epoch: 5| Step: 5
Training loss: 2.520817279815674
Validation loss: 2.3349053628983034

Epoch: 5| Step: 6
Training loss: 2.768389940261841
Validation loss: 2.335823025754703

Epoch: 5| Step: 7
Training loss: 3.1847894191741943
Validation loss: 2.333017482552477

Epoch: 5| Step: 8
Training loss: 2.661116361618042
Validation loss: 2.354915570187312

Epoch: 5| Step: 9
Training loss: 2.005049467086792
Validation loss: 2.3701121678916355

Epoch: 5| Step: 10
Training loss: 2.6966731548309326
Validation loss: 2.3745789117710565

Epoch: 230| Step: 0
Training loss: 2.5749292373657227
Validation loss: 2.3748103264839417

Epoch: 5| Step: 1
Training loss: 2.201768159866333
Validation loss: 2.3572365494184595

Epoch: 5| Step: 2
Training loss: 2.7737042903900146
Validation loss: 2.3449233091005715

Epoch: 5| Step: 3
Training loss: 1.916766881942749
Validation loss: 2.3305418491363525

Epoch: 5| Step: 4
Training loss: 2.7448208332061768
Validation loss: 2.308820709105461

Epoch: 5| Step: 5
Training loss: 2.544102191925049
Validation loss: 2.3104894597043275

Epoch: 5| Step: 6
Training loss: 2.2632994651794434
Validation loss: 2.3149035438414542

Epoch: 5| Step: 7
Training loss: 2.6304359436035156
Validation loss: 2.3213915747980916

Epoch: 5| Step: 8
Training loss: 2.3539443016052246
Validation loss: 2.312426787550731

Epoch: 5| Step: 9
Training loss: 2.714362382888794
Validation loss: 2.3166867507401334

Epoch: 5| Step: 10
Training loss: 2.7930917739868164
Validation loss: 2.322752880793746

Epoch: 231| Step: 0
Training loss: 1.6890592575073242
Validation loss: 2.327556680607539

Epoch: 5| Step: 1
Training loss: 3.254223585128784
Validation loss: 2.3182094584229174

Epoch: 5| Step: 2
Training loss: 2.3793652057647705
Validation loss: 2.3183931919836227

Epoch: 5| Step: 3
Training loss: 2.1638426780700684
Validation loss: 2.309691382992652

Epoch: 5| Step: 4
Training loss: 2.3307313919067383
Validation loss: 2.3167699280605523

Epoch: 5| Step: 5
Training loss: 2.9636223316192627
Validation loss: 2.316726076987482

Epoch: 5| Step: 6
Training loss: 3.693192720413208
Validation loss: 2.3156957857070433

Epoch: 5| Step: 7
Training loss: 2.484227180480957
Validation loss: 2.307702636206022

Epoch: 5| Step: 8
Training loss: 2.103710651397705
Validation loss: 2.307004528660928

Epoch: 5| Step: 9
Training loss: 2.0305237770080566
Validation loss: 2.3148346383084535

Epoch: 5| Step: 10
Training loss: 2.309694528579712
Validation loss: 2.323065188623244

Epoch: 232| Step: 0
Training loss: 2.438025712966919
Validation loss: 2.34841428777223

Epoch: 5| Step: 1
Training loss: 2.6228623390197754
Validation loss: 2.390625907528785

Epoch: 5| Step: 2
Training loss: 2.707242965698242
Validation loss: 2.4148185753053233

Epoch: 5| Step: 3
Training loss: 2.764681816101074
Validation loss: 2.453199698079017

Epoch: 5| Step: 4
Training loss: 1.8170496225357056
Validation loss: 2.4192948443915254

Epoch: 5| Step: 5
Training loss: 2.9512240886688232
Validation loss: 2.398691782387354

Epoch: 5| Step: 6
Training loss: 3.233872175216675
Validation loss: 2.3814557547210367

Epoch: 5| Step: 7
Training loss: 2.2485742568969727
Validation loss: 2.3722751550776984

Epoch: 5| Step: 8
Training loss: 2.399275302886963
Validation loss: 2.359085952081988

Epoch: 5| Step: 9
Training loss: 2.4710631370544434
Validation loss: 2.375993687619445

Epoch: 5| Step: 10
Training loss: 2.325878381729126
Validation loss: 2.3716796213580715

Epoch: 233| Step: 0
Training loss: 2.1975016593933105
Validation loss: 2.368880884621733

Epoch: 5| Step: 1
Training loss: 1.996948003768921
Validation loss: 2.3469824649954356

Epoch: 5| Step: 2
Training loss: 2.5936713218688965
Validation loss: 2.3282311244677474

Epoch: 5| Step: 3
Training loss: 2.4419946670532227
Validation loss: 2.320249213967272

Epoch: 5| Step: 4
Training loss: 2.74461030960083
Validation loss: 2.352116282268237

Epoch: 5| Step: 5
Training loss: 2.6724255084991455
Validation loss: 2.3674882688829975

Epoch: 5| Step: 6
Training loss: 2.4133338928222656
Validation loss: 2.369610112200501

Epoch: 5| Step: 7
Training loss: 2.7318265438079834
Validation loss: 2.3482009057075746

Epoch: 5| Step: 8
Training loss: 2.5052449703216553
Validation loss: 2.3536665721606185

Epoch: 5| Step: 9
Training loss: 2.6746914386749268
Validation loss: 2.3384419846278366

Epoch: 5| Step: 10
Training loss: 2.6695661544799805
Validation loss: 2.3390525899907595

Epoch: 234| Step: 0
Training loss: 3.076979875564575
Validation loss: 2.3217324723479567

Epoch: 5| Step: 1
Training loss: 1.9071305990219116
Validation loss: 2.323360009859967

Epoch: 5| Step: 2
Training loss: 2.120997190475464
Validation loss: 2.3236980899687736

Epoch: 5| Step: 3
Training loss: 3.0428478717803955
Validation loss: 2.3232945190962924

Epoch: 5| Step: 4
Training loss: 2.349868059158325
Validation loss: 2.3174018398407967

Epoch: 5| Step: 5
Training loss: 2.317394256591797
Validation loss: 2.319785071957496

Epoch: 5| Step: 6
Training loss: 2.279782772064209
Validation loss: 2.31132814961095

Epoch: 5| Step: 7
Training loss: 3.011216640472412
Validation loss: 2.3109682247202885

Epoch: 5| Step: 8
Training loss: 1.8300790786743164
Validation loss: 2.3148028363463697

Epoch: 5| Step: 9
Training loss: 2.717998743057251
Validation loss: 2.2983852253165296

Epoch: 5| Step: 10
Training loss: 2.8520946502685547
Validation loss: 2.291090880670855

Epoch: 235| Step: 0
Training loss: 2.1083121299743652
Validation loss: 2.2903142026675645

Epoch: 5| Step: 1
Training loss: 2.553840398788452
Validation loss: 2.2943573267229143

Epoch: 5| Step: 2
Training loss: 2.679102897644043
Validation loss: 2.294293030615776

Epoch: 5| Step: 3
Training loss: 2.1632587909698486
Validation loss: 2.3059161683564544

Epoch: 5| Step: 4
Training loss: 2.38846755027771
Validation loss: 2.3098572813054568

Epoch: 5| Step: 5
Training loss: 2.380791187286377
Validation loss: 2.3261356302486953

Epoch: 5| Step: 6
Training loss: 2.7766757011413574
Validation loss: 2.320531122146114

Epoch: 5| Step: 7
Training loss: 2.0212440490722656
Validation loss: 2.324085904705909

Epoch: 5| Step: 8
Training loss: 2.255073308944702
Validation loss: 2.317271629969279

Epoch: 5| Step: 9
Training loss: 2.9958138465881348
Validation loss: 2.3310009843559674

Epoch: 5| Step: 10
Training loss: 3.183823585510254
Validation loss: 2.3268373371452413

Epoch: 236| Step: 0
Training loss: 2.210038661956787
Validation loss: 2.3324523971926783

Epoch: 5| Step: 1
Training loss: 2.6539955139160156
Validation loss: 2.3349980128708707

Epoch: 5| Step: 2
Training loss: 2.6363296508789062
Validation loss: 2.3359878563111827

Epoch: 5| Step: 3
Training loss: 2.530769109725952
Validation loss: 2.327005047951975

Epoch: 5| Step: 4
Training loss: 2.0212345123291016
Validation loss: 2.3254214538040983

Epoch: 5| Step: 5
Training loss: 3.291132688522339
Validation loss: 2.299693553678451

Epoch: 5| Step: 6
Training loss: 2.5712907314300537
Validation loss: 2.295283963603358

Epoch: 5| Step: 7
Training loss: 2.3721156120300293
Validation loss: 2.2915175960909937

Epoch: 5| Step: 8
Training loss: 2.0450196266174316
Validation loss: 2.288003308798677

Epoch: 5| Step: 9
Training loss: 2.190788507461548
Validation loss: 2.285518359112483

Epoch: 5| Step: 10
Training loss: 2.769230604171753
Validation loss: 2.2785807527521604

Epoch: 237| Step: 0
Training loss: 2.488464832305908
Validation loss: 2.294095480313865

Epoch: 5| Step: 1
Training loss: 2.5170083045959473
Validation loss: 2.2826550442685365

Epoch: 5| Step: 2
Training loss: 2.3501899242401123
Validation loss: 2.2882041956788752

Epoch: 5| Step: 3
Training loss: 2.527050733566284
Validation loss: 2.292429483065041

Epoch: 5| Step: 4
Training loss: 2.4763121604919434
Validation loss: 2.2943415564875447

Epoch: 5| Step: 5
Training loss: 1.9969463348388672
Validation loss: 2.2797771653821393

Epoch: 5| Step: 6
Training loss: 3.112908124923706
Validation loss: 2.2759555334685952

Epoch: 5| Step: 7
Training loss: 2.4376931190490723
Validation loss: 2.2829985772409747

Epoch: 5| Step: 8
Training loss: 3.287008762359619
Validation loss: 2.2811664048061577

Epoch: 5| Step: 9
Training loss: 2.1019089221954346
Validation loss: 2.2832557565422467

Epoch: 5| Step: 10
Training loss: 1.9254601001739502
Validation loss: 2.2995099431724957

Epoch: 238| Step: 0
Training loss: 2.9158999919891357
Validation loss: 2.3421263489671933

Epoch: 5| Step: 1
Training loss: 2.691025972366333
Validation loss: 2.3516958682767806

Epoch: 5| Step: 2
Training loss: 2.20735764503479
Validation loss: 2.3635302102693947

Epoch: 5| Step: 3
Training loss: 2.568833827972412
Validation loss: 2.362935366169099

Epoch: 5| Step: 4
Training loss: 1.9382282495498657
Validation loss: 2.3605009073852212

Epoch: 5| Step: 5
Training loss: 2.606153964996338
Validation loss: 2.358150633432532

Epoch: 5| Step: 6
Training loss: 2.42818546295166
Validation loss: 2.3167888579830045

Epoch: 5| Step: 7
Training loss: 2.472409248352051
Validation loss: 2.309312237206326

Epoch: 5| Step: 8
Training loss: 2.6964352130889893
Validation loss: 2.3102723065242974

Epoch: 5| Step: 9
Training loss: 2.562222480773926
Validation loss: 2.3129076752611386

Epoch: 5| Step: 10
Training loss: 2.188448190689087
Validation loss: 2.307618507774927

Epoch: 239| Step: 0
Training loss: 2.735476493835449
Validation loss: 2.311923970458328

Epoch: 5| Step: 1
Training loss: 2.103325843811035
Validation loss: 2.281562464211577

Epoch: 5| Step: 2
Training loss: 2.466602087020874
Validation loss: 2.2830275002346245

Epoch: 5| Step: 3
Training loss: 2.1743857860565186
Validation loss: 2.2750219965493805

Epoch: 5| Step: 4
Training loss: 3.1555211544036865
Validation loss: 2.270272280580254

Epoch: 5| Step: 5
Training loss: 2.778329849243164
Validation loss: 2.2795755965735323

Epoch: 5| Step: 6
Training loss: 2.1780011653900146
Validation loss: 2.2920107174945135

Epoch: 5| Step: 7
Training loss: 2.5076591968536377
Validation loss: 2.302684425025858

Epoch: 5| Step: 8
Training loss: 2.6053247451782227
Validation loss: 2.3257970579208864

Epoch: 5| Step: 9
Training loss: 2.4634463787078857
Validation loss: 2.3275179888612483

Epoch: 5| Step: 10
Training loss: 2.2888221740722656
Validation loss: 2.3286384638919624

Epoch: 240| Step: 0
Training loss: 1.963155746459961
Validation loss: 2.3164903130582584

Epoch: 5| Step: 1
Training loss: 1.743208646774292
Validation loss: 2.312449142497073

Epoch: 5| Step: 2
Training loss: 3.069836139678955
Validation loss: 2.2993934910784484

Epoch: 5| Step: 3
Training loss: 2.7004857063293457
Validation loss: 2.290299464297551

Epoch: 5| Step: 4
Training loss: 2.623037338256836
Validation loss: 2.299729903539022

Epoch: 5| Step: 5
Training loss: 2.221196413040161
Validation loss: 2.305731132466306

Epoch: 5| Step: 6
Training loss: 2.2361743450164795
Validation loss: 2.3105357475178216

Epoch: 5| Step: 7
Training loss: 3.4620959758758545
Validation loss: 2.317573972927627

Epoch: 5| Step: 8
Training loss: 2.284834623336792
Validation loss: 2.3073808262425084

Epoch: 5| Step: 9
Training loss: 2.507293224334717
Validation loss: 2.303285765391524

Epoch: 5| Step: 10
Training loss: 2.4270429611206055
Validation loss: 2.295245075738558

Epoch: 241| Step: 0
Training loss: 1.9254789352416992
Validation loss: 2.291866378117633

Epoch: 5| Step: 1
Training loss: 2.831759214401245
Validation loss: 2.294630942806121

Epoch: 5| Step: 2
Training loss: 2.0692028999328613
Validation loss: 2.3160544108319026

Epoch: 5| Step: 3
Training loss: 2.8129489421844482
Validation loss: 2.3290867395298456

Epoch: 5| Step: 4
Training loss: 2.738940477371216
Validation loss: 2.335464087865686

Epoch: 5| Step: 5
Training loss: 2.4576220512390137
Validation loss: 2.3358786490655716

Epoch: 5| Step: 6
Training loss: 1.8418083190917969
Validation loss: 2.341339229255594

Epoch: 5| Step: 7
Training loss: 2.832916736602783
Validation loss: 2.3317805977277857

Epoch: 5| Step: 8
Training loss: 2.784301280975342
Validation loss: 2.31732689949774

Epoch: 5| Step: 9
Training loss: 2.104212999343872
Validation loss: 2.3151808502853557

Epoch: 5| Step: 10
Training loss: 2.6912753582000732
Validation loss: 2.299049767114783

Epoch: 242| Step: 0
Training loss: 2.189448833465576
Validation loss: 2.2856651480479906

Epoch: 5| Step: 1
Training loss: 2.6881332397460938
Validation loss: 2.282566898612566

Epoch: 5| Step: 2
Training loss: 3.345792293548584
Validation loss: 2.2926864470205

Epoch: 5| Step: 3
Training loss: 2.3923535346984863
Validation loss: 2.295280982089299

Epoch: 5| Step: 4
Training loss: 2.114199638366699
Validation loss: 2.292791474250055

Epoch: 5| Step: 5
Training loss: 2.212817430496216
Validation loss: 2.2935853081364788

Epoch: 5| Step: 6
Training loss: 2.36987042427063
Validation loss: 2.298139169651975

Epoch: 5| Step: 7
Training loss: 2.9790172576904297
Validation loss: 2.3064995145285003

Epoch: 5| Step: 8
Training loss: 2.03112530708313
Validation loss: 2.309275065698931

Epoch: 5| Step: 9
Training loss: 2.2987518310546875
Validation loss: 2.3097710609436035

Epoch: 5| Step: 10
Training loss: 2.3730242252349854
Validation loss: 2.2950389718496673

Epoch: 243| Step: 0
Training loss: 1.9629275798797607
Validation loss: 2.2814238353442122

Epoch: 5| Step: 1
Training loss: 2.155900478363037
Validation loss: 2.2896958166553127

Epoch: 5| Step: 2
Training loss: 2.5510623455047607
Validation loss: 2.3043558789837744

Epoch: 5| Step: 3
Training loss: 2.6686432361602783
Validation loss: 2.300686817015371

Epoch: 5| Step: 4
Training loss: 2.024557113647461
Validation loss: 2.2963480257218882

Epoch: 5| Step: 5
Training loss: 2.0980396270751953
Validation loss: 2.3026569145981983

Epoch: 5| Step: 6
Training loss: 2.9168412685394287
Validation loss: 2.288386506419028

Epoch: 5| Step: 7
Training loss: 2.850217342376709
Validation loss: 2.2963989832068004

Epoch: 5| Step: 8
Training loss: 2.417253017425537
Validation loss: 2.3004461796052995

Epoch: 5| Step: 9
Training loss: 2.9294514656066895
Validation loss: 2.2933249473571777

Epoch: 5| Step: 10
Training loss: 2.271899938583374
Validation loss: 2.292228574393898

Epoch: 244| Step: 0
Training loss: 1.947317361831665
Validation loss: 2.280600832354638

Epoch: 5| Step: 1
Training loss: 3.4018425941467285
Validation loss: 2.29109053714301

Epoch: 5| Step: 2
Training loss: 2.2149319648742676
Validation loss: 2.276372822382117

Epoch: 5| Step: 3
Training loss: 2.09281063079834
Validation loss: 2.283013612993302

Epoch: 5| Step: 4
Training loss: 2.3320069313049316
Validation loss: 2.2972250446196525

Epoch: 5| Step: 5
Training loss: 2.256549835205078
Validation loss: 2.2972145388203282

Epoch: 5| Step: 6
Training loss: 2.3222081661224365
Validation loss: 2.3169417842741935

Epoch: 5| Step: 7
Training loss: 3.1322720050811768
Validation loss: 2.3097493263982956

Epoch: 5| Step: 8
Training loss: 1.8257604837417603
Validation loss: 2.3038100939925

Epoch: 5| Step: 9
Training loss: 2.752737045288086
Validation loss: 2.316062163281184

Epoch: 5| Step: 10
Training loss: 2.471592426300049
Validation loss: 2.3009764109888384

Epoch: 245| Step: 0
Training loss: 2.3348476886749268
Validation loss: 2.2995929871836016

Epoch: 5| Step: 1
Training loss: 2.4428117275238037
Validation loss: 2.308139747188937

Epoch: 5| Step: 2
Training loss: 2.1121537685394287
Validation loss: 2.301793490686724

Epoch: 5| Step: 3
Training loss: 2.7841084003448486
Validation loss: 2.311229367409983

Epoch: 5| Step: 4
Training loss: 2.5409786701202393
Validation loss: 2.296437591634771

Epoch: 5| Step: 5
Training loss: 2.0310418605804443
Validation loss: 2.2901964520895355

Epoch: 5| Step: 6
Training loss: 2.414843797683716
Validation loss: 2.2880440014664845

Epoch: 5| Step: 7
Training loss: 2.4032154083251953
Validation loss: 2.2942257363309144

Epoch: 5| Step: 8
Training loss: 2.617888927459717
Validation loss: 2.294426031010125

Epoch: 5| Step: 9
Training loss: 3.114988088607788
Validation loss: 2.2938206324013333

Epoch: 5| Step: 10
Training loss: 1.820752501487732
Validation loss: 2.2892431751374276

Epoch: 246| Step: 0
Training loss: 2.696648359298706
Validation loss: 2.3007599487099597

Epoch: 5| Step: 1
Training loss: 2.7139575481414795
Validation loss: 2.3093837512436735

Epoch: 5| Step: 2
Training loss: 2.840487480163574
Validation loss: 2.2979142537681003

Epoch: 5| Step: 3
Training loss: 2.409904956817627
Validation loss: 2.2834569279865553

Epoch: 5| Step: 4
Training loss: 2.136730432510376
Validation loss: 2.291483791925574

Epoch: 5| Step: 5
Training loss: 2.572587013244629
Validation loss: 2.289485862178187

Epoch: 5| Step: 6
Training loss: 2.2164437770843506
Validation loss: 2.287399673974642

Epoch: 5| Step: 7
Training loss: 1.705749750137329
Validation loss: 2.2946140509779736

Epoch: 5| Step: 8
Training loss: 2.6734161376953125
Validation loss: 2.2997990987634145

Epoch: 5| Step: 9
Training loss: 2.138591766357422
Validation loss: 2.290567005834272

Epoch: 5| Step: 10
Training loss: 2.687103748321533
Validation loss: 2.2856530117732223

Epoch: 247| Step: 0
Training loss: 2.151047945022583
Validation loss: 2.28909275608678

Epoch: 5| Step: 1
Training loss: 2.461944103240967
Validation loss: 2.2800067932375017

Epoch: 5| Step: 2
Training loss: 2.1230971813201904
Validation loss: 2.270431790300595

Epoch: 5| Step: 3
Training loss: 3.4274532794952393
Validation loss: 2.283174004606021

Epoch: 5| Step: 4
Training loss: 2.518726348876953
Validation loss: 2.2876426942886843

Epoch: 5| Step: 5
Training loss: 3.3855507373809814
Validation loss: 2.2987377002675045

Epoch: 5| Step: 6
Training loss: 1.7728172540664673
Validation loss: 2.283018130128102

Epoch: 5| Step: 7
Training loss: 2.0643105506896973
Validation loss: 2.2898650502645843

Epoch: 5| Step: 8
Training loss: 2.583879232406616
Validation loss: 2.286242518373715

Epoch: 5| Step: 9
Training loss: 2.0559897422790527
Validation loss: 2.2754380164607877

Epoch: 5| Step: 10
Training loss: 2.039015293121338
Validation loss: 2.3013620876496836

Epoch: 248| Step: 0
Training loss: 2.226133346557617
Validation loss: 2.3103925105064147

Epoch: 5| Step: 1
Training loss: 2.670152187347412
Validation loss: 2.3172515515358216

Epoch: 5| Step: 2
Training loss: 3.1051437854766846
Validation loss: 2.3018412179844354

Epoch: 5| Step: 3
Training loss: 2.9403109550476074
Validation loss: 2.27490141571209

Epoch: 5| Step: 4
Training loss: 2.4240822792053223
Validation loss: 2.261167468563203

Epoch: 5| Step: 5
Training loss: 2.967212200164795
Validation loss: 2.2614426023216656

Epoch: 5| Step: 6
Training loss: 2.2502174377441406
Validation loss: 2.2665452329061364

Epoch: 5| Step: 7
Training loss: 1.753641128540039
Validation loss: 2.2639113549263246

Epoch: 5| Step: 8
Training loss: 2.165255546569824
Validation loss: 2.257275573668941

Epoch: 5| Step: 9
Training loss: 2.1170530319213867
Validation loss: 2.2654655441161125

Epoch: 5| Step: 10
Training loss: 2.0181970596313477
Validation loss: 2.2649174018572737

Epoch: 249| Step: 0
Training loss: 2.911329507827759
Validation loss: 2.276362298637308

Epoch: 5| Step: 1
Training loss: 3.09399151802063
Validation loss: 2.2712363607139996

Epoch: 5| Step: 2
Training loss: 2.5284790992736816
Validation loss: 2.286819035007108

Epoch: 5| Step: 3
Training loss: 2.3192033767700195
Validation loss: 2.3086050300187964

Epoch: 5| Step: 4
Training loss: 1.6467596292495728
Validation loss: 2.2992915953359296

Epoch: 5| Step: 5
Training loss: 2.6601223945617676
Validation loss: 2.298133568097186

Epoch: 5| Step: 6
Training loss: 2.0050227642059326
Validation loss: 2.285796489766849

Epoch: 5| Step: 7
Training loss: 1.9676669836044312
Validation loss: 2.271889063619798

Epoch: 5| Step: 8
Training loss: 2.072794198989868
Validation loss: 2.2684476631943897

Epoch: 5| Step: 9
Training loss: 2.5649960041046143
Validation loss: 2.250245989009898

Epoch: 5| Step: 10
Training loss: 2.97692608833313
Validation loss: 2.2562445107326714

Epoch: 250| Step: 0
Training loss: 2.5730085372924805
Validation loss: 2.2585465215867564

Epoch: 5| Step: 1
Training loss: 2.4506988525390625
Validation loss: 2.2868250698171635

Epoch: 5| Step: 2
Training loss: 2.956559658050537
Validation loss: 2.276974385784518

Epoch: 5| Step: 3
Training loss: 1.8443752527236938
Validation loss: 2.2723959607462727

Epoch: 5| Step: 4
Training loss: 2.524101734161377
Validation loss: 2.268064101537069

Epoch: 5| Step: 5
Training loss: 2.108973979949951
Validation loss: 2.253756715405372

Epoch: 5| Step: 6
Training loss: 2.097046375274658
Validation loss: 2.2630920205065

Epoch: 5| Step: 7
Training loss: 2.8379645347595215
Validation loss: 2.272527906202501

Epoch: 5| Step: 8
Training loss: 2.6524417400360107
Validation loss: 2.2777674877515404

Epoch: 5| Step: 9
Training loss: 2.0837528705596924
Validation loss: 2.2944435970757597

Epoch: 5| Step: 10
Training loss: 2.5004453659057617
Validation loss: 2.306578697696809

Testing loss: 2.541013797124227

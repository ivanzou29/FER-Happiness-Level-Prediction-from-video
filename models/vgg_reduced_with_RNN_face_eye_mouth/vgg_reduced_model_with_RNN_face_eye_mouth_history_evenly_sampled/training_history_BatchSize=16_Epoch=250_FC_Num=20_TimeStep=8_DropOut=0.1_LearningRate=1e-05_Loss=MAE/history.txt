Epoch: 1| Step: 0
Training loss: 5.962535858154297
Validation loss: 5.205433455846643

Epoch: 6| Step: 1
Training loss: 5.227067947387695
Validation loss: 5.194728661608952

Epoch: 6| Step: 2
Training loss: 6.0879974365234375
Validation loss: 5.184422390435332

Epoch: 6| Step: 3
Training loss: 5.080760955810547
Validation loss: 5.174608276736352

Epoch: 6| Step: 4
Training loss: 4.143241882324219
Validation loss: 5.164467350129159

Epoch: 6| Step: 5
Training loss: 4.7527899742126465
Validation loss: 5.154711882273356

Epoch: 6| Step: 6
Training loss: 6.271759986877441
Validation loss: 5.144765043771395

Epoch: 6| Step: 7
Training loss: 4.858213424682617
Validation loss: 5.13353507749496

Epoch: 6| Step: 8
Training loss: 3.65347957611084
Validation loss: 5.122130760582545

Epoch: 6| Step: 9
Training loss: 4.591300010681152
Validation loss: 5.1096079734063915

Epoch: 6| Step: 10
Training loss: 5.570648670196533
Validation loss: 5.096379759491131

Epoch: 6| Step: 11
Training loss: 4.272946357727051
Validation loss: 5.08197013280725

Epoch: 6| Step: 12
Training loss: 3.9579520225524902
Validation loss: 5.065839372655397

Epoch: 6| Step: 13
Training loss: 4.368680000305176
Validation loss: 5.049191890224334

Epoch: 2| Step: 0
Training loss: 3.5707480907440186
Validation loss: 5.0304517848517305

Epoch: 6| Step: 1
Training loss: 5.514113426208496
Validation loss: 5.011064601200883

Epoch: 6| Step: 2
Training loss: 5.0250654220581055
Validation loss: 4.989015415150632

Epoch: 6| Step: 3
Training loss: 4.8331098556518555
Validation loss: 4.96569428905364

Epoch: 6| Step: 4
Training loss: 5.55352783203125
Validation loss: 4.94087064907115

Epoch: 6| Step: 5
Training loss: 4.0847272872924805
Validation loss: 4.912985027477306

Epoch: 6| Step: 6
Training loss: 4.166555404663086
Validation loss: 4.884162467013123

Epoch: 6| Step: 7
Training loss: 4.040485382080078
Validation loss: 4.853007208916448

Epoch: 6| Step: 8
Training loss: 5.533459663391113
Validation loss: 4.821066015510149

Epoch: 6| Step: 9
Training loss: 4.665114879608154
Validation loss: 4.786501597332698

Epoch: 6| Step: 10
Training loss: 4.515642166137695
Validation loss: 4.750106467995592

Epoch: 6| Step: 11
Training loss: 4.444725513458252
Validation loss: 4.712473392486572

Epoch: 6| Step: 12
Training loss: 4.231928825378418
Validation loss: 4.675330528648951

Epoch: 6| Step: 13
Training loss: 5.0269269943237305
Validation loss: 4.63638247213056

Epoch: 3| Step: 0
Training loss: 4.473528861999512
Validation loss: 4.597658080439413

Epoch: 6| Step: 1
Training loss: 4.974687576293945
Validation loss: 4.559297551390945

Epoch: 6| Step: 2
Training loss: 3.916638135910034
Validation loss: 4.521759140876032

Epoch: 6| Step: 3
Training loss: 3.818955898284912
Validation loss: 4.484623652632519

Epoch: 6| Step: 4
Training loss: 3.520258903503418
Validation loss: 4.44809264521445

Epoch: 6| Step: 5
Training loss: 3.8711328506469727
Validation loss: 4.412437244128156

Epoch: 6| Step: 6
Training loss: 3.698884963989258
Validation loss: 4.379519072912073

Epoch: 6| Step: 7
Training loss: 3.991345167160034
Validation loss: 4.348641577587332

Epoch: 6| Step: 8
Training loss: 3.4830446243286133
Validation loss: 4.31681562495488

Epoch: 6| Step: 9
Training loss: 3.6786584854125977
Validation loss: 4.2877830279770714

Epoch: 6| Step: 10
Training loss: 5.150505542755127
Validation loss: 4.260438539648569

Epoch: 6| Step: 11
Training loss: 4.145763397216797
Validation loss: 4.234310683383737

Epoch: 6| Step: 12
Training loss: 5.663888454437256
Validation loss: 4.211266461239066

Epoch: 6| Step: 13
Training loss: 4.060318470001221
Validation loss: 4.188201550514467

Epoch: 4| Step: 0
Training loss: 4.288547515869141
Validation loss: 4.166672942458942

Epoch: 6| Step: 1
Training loss: 3.9405202865600586
Validation loss: 4.143516032926498

Epoch: 6| Step: 2
Training loss: 4.919155597686768
Validation loss: 4.123034097815073

Epoch: 6| Step: 3
Training loss: 3.533169984817505
Validation loss: 4.102610152254822

Epoch: 6| Step: 4
Training loss: 3.5112087726593018
Validation loss: 4.0812017071631645

Epoch: 6| Step: 5
Training loss: 3.975374698638916
Validation loss: 4.05976471336939

Epoch: 6| Step: 6
Training loss: 2.69964599609375
Validation loss: 4.037313281848866

Epoch: 6| Step: 7
Training loss: 3.7935523986816406
Validation loss: 4.011427099986743

Epoch: 6| Step: 8
Training loss: 3.578861713409424
Validation loss: 3.987806327881352

Epoch: 6| Step: 9
Training loss: 4.193365573883057
Validation loss: 3.9701179791522283

Epoch: 6| Step: 10
Training loss: 4.684355735778809
Validation loss: 3.952244653496691

Epoch: 6| Step: 11
Training loss: 4.187984466552734
Validation loss: 3.9392775207437496

Epoch: 6| Step: 12
Training loss: 3.3644070625305176
Validation loss: 3.9257903406696935

Epoch: 6| Step: 13
Training loss: 3.4630110263824463
Validation loss: 3.9131055647327053

Epoch: 5| Step: 0
Training loss: 4.57624626159668
Validation loss: 3.901159414681055

Epoch: 6| Step: 1
Training loss: 3.716118097305298
Validation loss: 3.8858219295419674

Epoch: 6| Step: 2
Training loss: 2.661256790161133
Validation loss: 3.871959409406108

Epoch: 6| Step: 3
Training loss: 4.399418830871582
Validation loss: 3.8577537459711873

Epoch: 6| Step: 4
Training loss: 3.7030885219573975
Validation loss: 3.842080316235942

Epoch: 6| Step: 5
Training loss: 2.5133910179138184
Validation loss: 3.828596156130555

Epoch: 6| Step: 6
Training loss: 4.417107105255127
Validation loss: 3.8155665961644982

Epoch: 6| Step: 7
Training loss: 3.890864372253418
Validation loss: 3.8016468171150453

Epoch: 6| Step: 8
Training loss: 3.267540454864502
Validation loss: 3.791287232470769

Epoch: 6| Step: 9
Training loss: 3.2011890411376953
Validation loss: 3.778745574335898

Epoch: 6| Step: 10
Training loss: 3.706279993057251
Validation loss: 3.769308231210196

Epoch: 6| Step: 11
Training loss: 3.975637912750244
Validation loss: 3.7601411060620378

Epoch: 6| Step: 12
Training loss: 3.9361889362335205
Validation loss: 3.748075305774648

Epoch: 6| Step: 13
Training loss: 3.9546022415161133
Validation loss: 3.735109221550726

Epoch: 6| Step: 0
Training loss: 3.7744054794311523
Validation loss: 3.7236812704352924

Epoch: 6| Step: 1
Training loss: 3.0705411434173584
Validation loss: 3.714375949675037

Epoch: 6| Step: 2
Training loss: 4.381567478179932
Validation loss: 3.703992341154365

Epoch: 6| Step: 3
Training loss: 4.140254974365234
Validation loss: 3.691904678139635

Epoch: 6| Step: 4
Training loss: 1.6542266607284546
Validation loss: 3.6843191987724713

Epoch: 6| Step: 5
Training loss: 3.4965970516204834
Validation loss: 3.675364125159479

Epoch: 6| Step: 6
Training loss: 3.082831382751465
Validation loss: 3.6654588483995005

Epoch: 6| Step: 7
Training loss: 3.5333056449890137
Validation loss: 3.6539097498821955

Epoch: 6| Step: 8
Training loss: 4.214384078979492
Validation loss: 3.646990247952041

Epoch: 6| Step: 9
Training loss: 2.998528242111206
Validation loss: 3.6357968750820366

Epoch: 6| Step: 10
Training loss: 4.029674530029297
Validation loss: 3.6273873749599663

Epoch: 6| Step: 11
Training loss: 5.0421648025512695
Validation loss: 3.6184599015020553

Epoch: 6| Step: 12
Training loss: 2.3642640113830566
Validation loss: 3.6058792991022908

Epoch: 6| Step: 13
Training loss: 4.60784387588501
Validation loss: 3.594249104940763

Epoch: 7| Step: 0
Training loss: 3.9574573040008545
Validation loss: 3.5826195286166285

Epoch: 6| Step: 1
Training loss: 2.9870948791503906
Validation loss: 3.5748747805113434

Epoch: 6| Step: 2
Training loss: 3.731491804122925
Validation loss: 3.568065671510594

Epoch: 6| Step: 3
Training loss: 3.7679173946380615
Validation loss: 3.557782701266709

Epoch: 6| Step: 4
Training loss: 2.5957720279693604
Validation loss: 3.5466607180974816

Epoch: 6| Step: 5
Training loss: 4.0362162590026855
Validation loss: 3.5342669589545137

Epoch: 6| Step: 6
Training loss: 2.844290018081665
Validation loss: 3.5266081953561432

Epoch: 6| Step: 7
Training loss: 3.946127414703369
Validation loss: 3.518309329145698

Epoch: 6| Step: 8
Training loss: 3.4107460975646973
Validation loss: 3.507299946200463

Epoch: 6| Step: 9
Training loss: 3.4205074310302734
Validation loss: 3.5015727576389106

Epoch: 6| Step: 10
Training loss: 3.361809730529785
Validation loss: 3.498632771994478

Epoch: 6| Step: 11
Training loss: 3.420543909072876
Validation loss: 3.4895548179585445

Epoch: 6| Step: 12
Training loss: 3.8249683380126953
Validation loss: 3.4838497331065517

Epoch: 6| Step: 13
Training loss: 2.811367988586426
Validation loss: 3.4746499138493694

Epoch: 8| Step: 0
Training loss: 2.530754327774048
Validation loss: 3.4666140976772515

Epoch: 6| Step: 1
Training loss: 2.4949493408203125
Validation loss: 3.4584530553510113

Epoch: 6| Step: 2
Training loss: 3.752716064453125
Validation loss: 3.45430386963711

Epoch: 6| Step: 3
Training loss: 3.053598403930664
Validation loss: 3.4473196947446434

Epoch: 6| Step: 4
Training loss: 2.9320130348205566
Validation loss: 3.4413327504229803

Epoch: 6| Step: 5
Training loss: 3.724760055541992
Validation loss: 3.4339096469263874

Epoch: 6| Step: 6
Training loss: 2.9810709953308105
Validation loss: 3.427127189533685

Epoch: 6| Step: 7
Training loss: 4.833108901977539
Validation loss: 3.4204737012104323

Epoch: 6| Step: 8
Training loss: 3.5923948287963867
Validation loss: 3.41561371280301

Epoch: 6| Step: 9
Training loss: 3.414748191833496
Validation loss: 3.4097189057257866

Epoch: 6| Step: 10
Training loss: 2.738232135772705
Validation loss: 3.4062119094274377

Epoch: 6| Step: 11
Training loss: 3.962587833404541
Validation loss: 3.401366238952965

Epoch: 6| Step: 12
Training loss: 3.4337949752807617
Validation loss: 3.3977994995732463

Epoch: 6| Step: 13
Training loss: 4.086422920227051
Validation loss: 3.3877839631931757

Epoch: 9| Step: 0
Training loss: 3.0093750953674316
Validation loss: 3.381878601607456

Epoch: 6| Step: 1
Training loss: 2.9618804454803467
Validation loss: 3.3784041045814432

Epoch: 6| Step: 2
Training loss: 3.3109230995178223
Validation loss: 3.376265033598869

Epoch: 6| Step: 3
Training loss: 3.057004928588867
Validation loss: 3.374238101384973

Epoch: 6| Step: 4
Training loss: 4.564002513885498
Validation loss: 3.3688349185451383

Epoch: 6| Step: 5
Training loss: 4.0172929763793945
Validation loss: 3.3733889518245572

Epoch: 6| Step: 6
Training loss: 2.787184715270996
Validation loss: 3.35904025518766

Epoch: 6| Step: 7
Training loss: 3.846684455871582
Validation loss: 3.352357469579225

Epoch: 6| Step: 8
Training loss: 3.1000614166259766
Validation loss: 3.349220465588313

Epoch: 6| Step: 9
Training loss: 2.3640661239624023
Validation loss: 3.350824561170352

Epoch: 6| Step: 10
Training loss: 2.6785595417022705
Validation loss: 3.3491712847063617

Epoch: 6| Step: 11
Training loss: 4.27105188369751
Validation loss: 3.3419467146678636

Epoch: 6| Step: 12
Training loss: 3.099771738052368
Validation loss: 3.336424840393887

Epoch: 6| Step: 13
Training loss: 3.3635971546173096
Validation loss: 3.3301145748425554

Epoch: 10| Step: 0
Training loss: 2.927777051925659
Validation loss: 3.3252038545505975

Epoch: 6| Step: 1
Training loss: 3.6319332122802734
Validation loss: 3.324032291289299

Epoch: 6| Step: 2
Training loss: 3.3686165809631348
Validation loss: 3.3215213334688576

Epoch: 6| Step: 3
Training loss: 3.6303224563598633
Validation loss: 3.3180435472919094

Epoch: 6| Step: 4
Training loss: 4.573144912719727
Validation loss: 3.315064496891473

Epoch: 6| Step: 5
Training loss: 2.8831629753112793
Validation loss: 3.312632078765541

Epoch: 6| Step: 6
Training loss: 2.9853992462158203
Validation loss: 3.306681997032576

Epoch: 6| Step: 7
Training loss: 3.443675994873047
Validation loss: 3.3019899091412945

Epoch: 6| Step: 8
Training loss: 2.6093411445617676
Validation loss: 3.2997610543363836

Epoch: 6| Step: 9
Training loss: 3.0534934997558594
Validation loss: 3.303809837628436

Epoch: 6| Step: 10
Training loss: 3.3518083095550537
Validation loss: 3.3012687057577152

Epoch: 6| Step: 11
Training loss: 2.5896706581115723
Validation loss: 3.2919630389059744

Epoch: 6| Step: 12
Training loss: 3.353545665740967
Validation loss: 3.285551614658807

Epoch: 6| Step: 13
Training loss: 3.579038381576538
Validation loss: 3.283060701944495

Epoch: 11| Step: 0
Training loss: 3.535619020462036
Validation loss: 3.282031987303047

Epoch: 6| Step: 1
Training loss: 2.47588849067688
Validation loss: 3.2806262329060543

Epoch: 6| Step: 2
Training loss: 2.270205020904541
Validation loss: 3.2750499633050736

Epoch: 6| Step: 3
Training loss: 4.015201568603516
Validation loss: 3.271451803945726

Epoch: 6| Step: 4
Training loss: 3.0709357261657715
Validation loss: 3.2677743588724444

Epoch: 6| Step: 5
Training loss: 2.3581597805023193
Validation loss: 3.2648882327541227

Epoch: 6| Step: 6
Training loss: 3.5417490005493164
Validation loss: 3.2641913685747372

Epoch: 6| Step: 7
Training loss: 3.2393157482147217
Validation loss: 3.260414236335344

Epoch: 6| Step: 8
Training loss: 4.344821929931641
Validation loss: 3.259659272368236

Epoch: 6| Step: 9
Training loss: 3.5736052989959717
Validation loss: 3.2571995719786613

Epoch: 6| Step: 10
Training loss: 3.5470240116119385
Validation loss: 3.2531472175352034

Epoch: 6| Step: 11
Training loss: 2.7008724212646484
Validation loss: 3.24908519816655

Epoch: 6| Step: 12
Training loss: 3.4184553623199463
Validation loss: 3.2445833631741103

Epoch: 6| Step: 13
Training loss: 3.3264758586883545
Validation loss: 3.24431251710461

Epoch: 12| Step: 0
Training loss: 3.225633144378662
Validation loss: 3.2461749302443637

Epoch: 6| Step: 1
Training loss: 2.828334331512451
Validation loss: 3.2430180682930896

Epoch: 6| Step: 2
Training loss: 4.194891452789307
Validation loss: 3.2414976678868777

Epoch: 6| Step: 3
Training loss: 4.324872970581055
Validation loss: 3.2339487075805664

Epoch: 6| Step: 4
Training loss: 2.9009785652160645
Validation loss: 3.228716424716416

Epoch: 6| Step: 5
Training loss: 2.7010245323181152
Validation loss: 3.2285348933230162

Epoch: 6| Step: 6
Training loss: 3.3975489139556885
Validation loss: 3.2319784420792774

Epoch: 6| Step: 7
Training loss: 3.45223331451416
Validation loss: 3.2267766614114084

Epoch: 6| Step: 8
Training loss: 3.1184120178222656
Validation loss: 3.22151771924829

Epoch: 6| Step: 9
Training loss: 2.040088415145874
Validation loss: 3.218807153804328

Epoch: 6| Step: 10
Training loss: 3.1468217372894287
Validation loss: 3.2160855057418987

Epoch: 6| Step: 11
Training loss: 3.517179489135742
Validation loss: 3.218079190100393

Epoch: 6| Step: 12
Training loss: 3.3881325721740723
Validation loss: 3.2115199642796672

Epoch: 6| Step: 13
Training loss: 2.4932398796081543
Validation loss: 3.207591792588593

Epoch: 13| Step: 0
Training loss: 4.059499740600586
Validation loss: 3.205321717005904

Epoch: 6| Step: 1
Training loss: 2.810871124267578
Validation loss: 3.2026611553725375

Epoch: 6| Step: 2
Training loss: 3.5323076248168945
Validation loss: 3.2042268578724196

Epoch: 6| Step: 3
Training loss: 3.529973030090332
Validation loss: 3.202997769078901

Epoch: 6| Step: 4
Training loss: 3.3096094131469727
Validation loss: 3.2032802950951362

Epoch: 6| Step: 5
Training loss: 3.190913438796997
Validation loss: 3.199296348838396

Epoch: 6| Step: 6
Training loss: 4.064746856689453
Validation loss: 3.194062030443581

Epoch: 6| Step: 7
Training loss: 2.90397310256958
Validation loss: 3.1909779707590737

Epoch: 6| Step: 8
Training loss: 2.270076274871826
Validation loss: 3.187135681029289

Epoch: 6| Step: 9
Training loss: 3.962641716003418
Validation loss: 3.1844769062534457

Epoch: 6| Step: 10
Training loss: 2.761122226715088
Validation loss: 3.181331431993874

Epoch: 6| Step: 11
Training loss: 2.555305004119873
Validation loss: 3.179467116632769

Epoch: 6| Step: 12
Training loss: 2.2433390617370605
Validation loss: 3.1791105526749805

Epoch: 6| Step: 13
Training loss: 3.7196781635284424
Validation loss: 3.1816973814400296

Epoch: 14| Step: 0
Training loss: 2.3941397666931152
Validation loss: 3.1844984869803152

Epoch: 6| Step: 1
Training loss: 2.7868471145629883
Validation loss: 3.1798779374809674

Epoch: 6| Step: 2
Training loss: 2.917311191558838
Validation loss: 3.1716624152275825

Epoch: 6| Step: 3
Training loss: 2.369521379470825
Validation loss: 3.171520920209987

Epoch: 6| Step: 4
Training loss: 3.016433000564575
Validation loss: 3.1691383956581034

Epoch: 6| Step: 5
Training loss: 4.060388088226318
Validation loss: 3.1704302398107385

Epoch: 6| Step: 6
Training loss: 3.766622543334961
Validation loss: 3.164595204014932

Epoch: 6| Step: 7
Training loss: 3.491657257080078
Validation loss: 3.1624634009535595

Epoch: 6| Step: 8
Training loss: 3.977839946746826
Validation loss: 3.1616396775809665

Epoch: 6| Step: 9
Training loss: 2.739022970199585
Validation loss: 3.160563979097592

Epoch: 6| Step: 10
Training loss: 3.529513359069824
Validation loss: 3.1632631055770384

Epoch: 6| Step: 11
Training loss: 2.7852134704589844
Validation loss: 3.169593939217188

Epoch: 6| Step: 12
Training loss: 3.069671392440796
Validation loss: 3.165283356943438

Epoch: 6| Step: 13
Training loss: 3.7606239318847656
Validation loss: 3.155172863314229

Epoch: 15| Step: 0
Training loss: 3.3805019855499268
Validation loss: 3.149650542966781

Epoch: 6| Step: 1
Training loss: 3.4421427249908447
Validation loss: 3.144847241781091

Epoch: 6| Step: 2
Training loss: 2.2341036796569824
Validation loss: 3.1453912463239444

Epoch: 6| Step: 3
Training loss: 3.760913133621216
Validation loss: 3.1464537266762025

Epoch: 6| Step: 4
Training loss: 3.591435432434082
Validation loss: 3.142896521476007

Epoch: 6| Step: 5
Training loss: 3.301551342010498
Validation loss: 3.1369696227453088

Epoch: 6| Step: 6
Training loss: 2.64622163772583
Validation loss: 3.134078779528218

Epoch: 6| Step: 7
Training loss: 2.9840292930603027
Validation loss: 3.135819581247145

Epoch: 6| Step: 8
Training loss: 2.9206180572509766
Validation loss: 3.1333182088790403

Epoch: 6| Step: 9
Training loss: 2.730170965194702
Validation loss: 3.1287344809501403

Epoch: 6| Step: 10
Training loss: 3.972346544265747
Validation loss: 3.1270374533950642

Epoch: 6| Step: 11
Training loss: 3.5594754219055176
Validation loss: 3.125641861269551

Epoch: 6| Step: 12
Training loss: 2.378429889678955
Validation loss: 3.1242186920617216

Epoch: 6| Step: 13
Training loss: 3.266606092453003
Validation loss: 3.1193659638845794

Epoch: 16| Step: 0
Training loss: 3.1663994789123535
Validation loss: 3.118410820602089

Epoch: 6| Step: 1
Training loss: 2.8446784019470215
Validation loss: 3.118114479126469

Epoch: 6| Step: 2
Training loss: 3.0491814613342285
Validation loss: 3.118050700874739

Epoch: 6| Step: 3
Training loss: 3.030029535293579
Validation loss: 3.1150086259329193

Epoch: 6| Step: 4
Training loss: 3.0463967323303223
Validation loss: 3.1139378983487367

Epoch: 6| Step: 5
Training loss: 2.5292863845825195
Validation loss: 3.1172130287334485

Epoch: 6| Step: 6
Training loss: 3.1755566596984863
Validation loss: 3.134399749899423

Epoch: 6| Step: 7
Training loss: 3.907670021057129
Validation loss: 3.109425562684254

Epoch: 6| Step: 8
Training loss: 2.8473806381225586
Validation loss: 3.1068521186869633

Epoch: 6| Step: 9
Training loss: 3.2199625968933105
Validation loss: 3.1173197300203386

Epoch: 6| Step: 10
Training loss: 3.427945613861084
Validation loss: 3.1009271760140695

Epoch: 6| Step: 11
Training loss: 3.266310453414917
Validation loss: 3.0999800466722056

Epoch: 6| Step: 12
Training loss: 2.8061084747314453
Validation loss: 3.1033487063582226

Epoch: 6| Step: 13
Training loss: 3.947993278503418
Validation loss: 3.107090870539347

Epoch: 17| Step: 0
Training loss: 3.529979705810547
Validation loss: 3.108486213991719

Epoch: 6| Step: 1
Training loss: 2.631625175476074
Validation loss: 3.1058335586260726

Epoch: 6| Step: 2
Training loss: 2.7452220916748047
Validation loss: 3.10552204039789

Epoch: 6| Step: 3
Training loss: 3.4353275299072266
Validation loss: 3.0992347194302465

Epoch: 6| Step: 4
Training loss: 2.658522129058838
Validation loss: 3.0936230459520893

Epoch: 6| Step: 5
Training loss: 3.050121545791626
Validation loss: 3.0913824317275838

Epoch: 6| Step: 6
Training loss: 4.058820724487305
Validation loss: 3.089774880357968

Epoch: 6| Step: 7
Training loss: 3.282473087310791
Validation loss: 3.0884304943905083

Epoch: 6| Step: 8
Training loss: 2.7340917587280273
Validation loss: 3.084249750260384

Epoch: 6| Step: 9
Training loss: 3.7629876136779785
Validation loss: 3.0820083105435936

Epoch: 6| Step: 10
Training loss: 3.307857036590576
Validation loss: 3.0807148974428893

Epoch: 6| Step: 11
Training loss: 2.5292909145355225
Validation loss: 3.075877163999824

Epoch: 6| Step: 12
Training loss: 2.479763984680176
Validation loss: 3.0734945753569245

Epoch: 6| Step: 13
Training loss: 3.804447650909424
Validation loss: 3.0698316968897337

Epoch: 18| Step: 0
Training loss: 2.9050815105438232
Validation loss: 3.065664173454367

Epoch: 6| Step: 1
Training loss: 3.033259391784668
Validation loss: 3.064090023758591

Epoch: 6| Step: 2
Training loss: 3.9375667572021484
Validation loss: 3.06498376272058

Epoch: 6| Step: 3
Training loss: 3.363126754760742
Validation loss: 3.0610316414986887

Epoch: 6| Step: 4
Training loss: 3.599641799926758
Validation loss: 3.0621457997188775

Epoch: 6| Step: 5
Training loss: 2.9263978004455566
Validation loss: 3.056368886783559

Epoch: 6| Step: 6
Training loss: 3.899364709854126
Validation loss: 3.055415750831686

Epoch: 6| Step: 7
Training loss: 3.635087490081787
Validation loss: 3.0548553748797347

Epoch: 6| Step: 8
Training loss: 2.807569980621338
Validation loss: 3.0520106207939888

Epoch: 6| Step: 9
Training loss: 2.1283512115478516
Validation loss: 3.0487169809238885

Epoch: 6| Step: 10
Training loss: 3.621419906616211
Validation loss: 3.0488243795210317

Epoch: 6| Step: 11
Training loss: 2.4702529907226562
Validation loss: 3.0438376703569965

Epoch: 6| Step: 12
Training loss: 2.142444610595703
Validation loss: 3.0441671597060336

Epoch: 6| Step: 13
Training loss: 2.755581855773926
Validation loss: 3.0419743830157864

Epoch: 19| Step: 0
Training loss: 3.72387433052063
Validation loss: 3.0450399947422806

Epoch: 6| Step: 1
Training loss: 3.8011269569396973
Validation loss: 3.0348183160187094

Epoch: 6| Step: 2
Training loss: 2.3604416847229004
Validation loss: 3.036313792710663

Epoch: 6| Step: 3
Training loss: 3.49692440032959
Validation loss: 3.0364166459729596

Epoch: 6| Step: 4
Training loss: 3.164062738418579
Validation loss: 3.037086204815936

Epoch: 6| Step: 5
Training loss: 2.12925124168396
Validation loss: 3.030688774201178

Epoch: 6| Step: 6
Training loss: 2.578122854232788
Validation loss: 3.0278684862198366

Epoch: 6| Step: 7
Training loss: 2.736135721206665
Validation loss: 3.0277830452047367

Epoch: 6| Step: 8
Training loss: 2.9008450508117676
Validation loss: 3.0298205678180983

Epoch: 6| Step: 9
Training loss: 2.3450565338134766
Validation loss: 3.0302871427228375

Epoch: 6| Step: 10
Training loss: 3.534494400024414
Validation loss: 3.0278906770931777

Epoch: 6| Step: 11
Training loss: 3.196833372116089
Validation loss: 3.0269193213473082

Epoch: 6| Step: 12
Training loss: 3.959352493286133
Validation loss: 3.0301163452927784

Epoch: 6| Step: 13
Training loss: 3.466074228286743
Validation loss: 3.026634677763908

Epoch: 20| Step: 0
Training loss: 4.477910995483398
Validation loss: 3.019180426033594

Epoch: 6| Step: 1
Training loss: 3.6481781005859375
Validation loss: 3.019742445279193

Epoch: 6| Step: 2
Training loss: 2.7784512042999268
Validation loss: 3.015714171112225

Epoch: 6| Step: 3
Training loss: 3.3771848678588867
Validation loss: 3.0134793404609925

Epoch: 6| Step: 4
Training loss: 2.954484462738037
Validation loss: 3.011796102728895

Epoch: 6| Step: 5
Training loss: 2.690178394317627
Validation loss: 3.011639589904457

Epoch: 6| Step: 6
Training loss: 3.6748623847961426
Validation loss: 3.0134132831327376

Epoch: 6| Step: 7
Training loss: 2.054452419281006
Validation loss: 3.0118725786926928

Epoch: 6| Step: 8
Training loss: 3.1219964027404785
Validation loss: 3.008896789243144

Epoch: 6| Step: 9
Training loss: 3.1610560417175293
Validation loss: 3.0068007874232467

Epoch: 6| Step: 10
Training loss: 2.869847059249878
Validation loss: 3.001969196463144

Epoch: 6| Step: 11
Training loss: 2.773374557495117
Validation loss: 2.9999924244419223

Epoch: 6| Step: 12
Training loss: 2.7160181999206543
Validation loss: 3.0006741477597143

Epoch: 6| Step: 13
Training loss: 2.439201831817627
Validation loss: 3.0442237572003434

Epoch: 21| Step: 0
Training loss: 2.840015172958374
Validation loss: 3.0761910664137972

Epoch: 6| Step: 1
Training loss: 3.2549240589141846
Validation loss: 3.0812845691557853

Epoch: 6| Step: 2
Training loss: 3.374958038330078
Validation loss: 3.0649223019999843

Epoch: 6| Step: 3
Training loss: 2.956284999847412
Validation loss: 3.0608722702149422

Epoch: 6| Step: 4
Training loss: 3.154665946960449
Validation loss: 3.0204083355524207

Epoch: 6| Step: 5
Training loss: 3.444476366043091
Validation loss: 2.9957875923443864

Epoch: 6| Step: 6
Training loss: 2.9842641353607178
Validation loss: 3.000714140553628

Epoch: 6| Step: 7
Training loss: 3.8646080493927
Validation loss: 2.9978437782615743

Epoch: 6| Step: 8
Training loss: 3.05959153175354
Validation loss: 2.9969302274847545

Epoch: 6| Step: 9
Training loss: 2.6105823516845703
Validation loss: 2.9959485171943583

Epoch: 6| Step: 10
Training loss: 2.628279447555542
Validation loss: 2.995083834535332

Epoch: 6| Step: 11
Training loss: 2.536430597305298
Validation loss: 2.996816614622711

Epoch: 6| Step: 12
Training loss: 3.836052179336548
Validation loss: 2.996777211466143

Epoch: 6| Step: 13
Training loss: 2.1738369464874268
Validation loss: 2.9938051239136727

Epoch: 22| Step: 0
Training loss: 2.718693256378174
Validation loss: 2.996606147417458

Epoch: 6| Step: 1
Training loss: 2.9382991790771484
Validation loss: 3.00515152049321

Epoch: 6| Step: 2
Training loss: 2.913088321685791
Validation loss: 3.01870669088056

Epoch: 6| Step: 3
Training loss: 3.1743266582489014
Validation loss: 3.0015526279326408

Epoch: 6| Step: 4
Training loss: 3.8914194107055664
Validation loss: 2.9900841764224473

Epoch: 6| Step: 5
Training loss: 2.774995803833008
Validation loss: 2.989903726885396

Epoch: 6| Step: 6
Training loss: 2.8440098762512207
Validation loss: 2.9881255165223153

Epoch: 6| Step: 7
Training loss: 3.929440975189209
Validation loss: 2.9895482165839082

Epoch: 6| Step: 8
Training loss: 3.24749755859375
Validation loss: 2.9921135979314006

Epoch: 6| Step: 9
Training loss: 3.0766854286193848
Validation loss: 2.994829290656633

Epoch: 6| Step: 10
Training loss: 1.7925703525543213
Validation loss: 2.992034596781577

Epoch: 6| Step: 11
Training loss: 2.629432201385498
Validation loss: 2.9903816125726186

Epoch: 6| Step: 12
Training loss: 3.6178598403930664
Validation loss: 2.989285751055646

Epoch: 6| Step: 13
Training loss: 3.4492027759552
Validation loss: 2.9856942443437475

Epoch: 23| Step: 0
Training loss: 3.1011104583740234
Validation loss: 2.9797610672571326

Epoch: 6| Step: 1
Training loss: 4.034849166870117
Validation loss: 2.9789333522960706

Epoch: 6| Step: 2
Training loss: 2.582059383392334
Validation loss: 2.9768599156410462

Epoch: 6| Step: 3
Training loss: 3.228571891784668
Validation loss: 2.9742640013335855

Epoch: 6| Step: 4
Training loss: 2.763319253921509
Validation loss: 2.9731402704792638

Epoch: 6| Step: 5
Training loss: 2.8323354721069336
Validation loss: 2.971609120727867

Epoch: 6| Step: 6
Training loss: 3.474994421005249
Validation loss: 2.9703126363856818

Epoch: 6| Step: 7
Training loss: 2.181145668029785
Validation loss: 2.9728018699153775

Epoch: 6| Step: 8
Training loss: 2.860381841659546
Validation loss: 2.9749514569518385

Epoch: 6| Step: 9
Training loss: 2.811372756958008
Validation loss: 2.97611165303056

Epoch: 6| Step: 10
Training loss: 3.491912841796875
Validation loss: 2.9782798059525026

Epoch: 6| Step: 11
Training loss: 3.515549659729004
Validation loss: 2.967602760561051

Epoch: 6| Step: 12
Training loss: 2.6550300121307373
Validation loss: 2.963372766330678

Epoch: 6| Step: 13
Training loss: 3.1851468086242676
Validation loss: 2.9624568262407855

Epoch: 24| Step: 0
Training loss: 2.9709928035736084
Validation loss: 2.966188451295258

Epoch: 6| Step: 1
Training loss: 2.4820241928100586
Validation loss: 2.9665580564929592

Epoch: 6| Step: 2
Training loss: 2.8780860900878906
Validation loss: 2.9629748662312827

Epoch: 6| Step: 3
Training loss: 2.7627854347229004
Validation loss: 2.964582040745725

Epoch: 6| Step: 4
Training loss: 3.5021493434906006
Validation loss: 2.962785005569458

Epoch: 6| Step: 5
Training loss: 2.585341453552246
Validation loss: 2.964505203308598

Epoch: 6| Step: 6
Training loss: 2.7395386695861816
Validation loss: 2.969765373455581

Epoch: 6| Step: 7
Training loss: 2.9219322204589844
Validation loss: 2.9787866633425475

Epoch: 6| Step: 8
Training loss: 3.568030595779419
Validation loss: 2.9622011030873945

Epoch: 6| Step: 9
Training loss: 2.8439948558807373
Validation loss: 2.958353873222105

Epoch: 6| Step: 10
Training loss: 3.9908995628356934
Validation loss: 2.956340710322062

Epoch: 6| Step: 11
Training loss: 3.2722573280334473
Validation loss: 2.958391756139776

Epoch: 6| Step: 12
Training loss: 2.210383653640747
Validation loss: 2.972950845636347

Epoch: 6| Step: 13
Training loss: 4.565171241760254
Validation loss: 2.9746637600724415

Epoch: 25| Step: 0
Training loss: 3.872807741165161
Validation loss: 2.959224283054311

Epoch: 6| Step: 1
Training loss: 3.1267409324645996
Validation loss: 2.9599120437457995

Epoch: 6| Step: 2
Training loss: 3.033294916152954
Validation loss: 2.9611511615014847

Epoch: 6| Step: 3
Training loss: 2.417198896408081
Validation loss: 2.9628175022781535

Epoch: 6| Step: 4
Training loss: 3.735166072845459
Validation loss: 2.9629713104617212

Epoch: 6| Step: 5
Training loss: 2.4823875427246094
Validation loss: 2.9618263603538595

Epoch: 6| Step: 6
Training loss: 2.932386636734009
Validation loss: 2.9641679281829507

Epoch: 6| Step: 7
Training loss: 2.7666373252868652
Validation loss: 2.9614478823959187

Epoch: 6| Step: 8
Training loss: 2.5259313583374023
Validation loss: 2.9632647601507043

Epoch: 6| Step: 9
Training loss: 3.1420059204101562
Validation loss: 2.958733263836112

Epoch: 6| Step: 10
Training loss: 3.4723095893859863
Validation loss: 2.965714908415271

Epoch: 6| Step: 11
Training loss: 2.789090633392334
Validation loss: 2.965304354185699

Epoch: 6| Step: 12
Training loss: 2.8798484802246094
Validation loss: 2.9734633327812277

Epoch: 6| Step: 13
Training loss: 3.6096911430358887
Validation loss: 2.969274164527975

Epoch: 26| Step: 0
Training loss: 3.060910701751709
Validation loss: 2.9608477930868826

Epoch: 6| Step: 1
Training loss: 3.2256126403808594
Validation loss: 2.9533047060812674

Epoch: 6| Step: 2
Training loss: 3.136160373687744
Validation loss: 2.949832785514093

Epoch: 6| Step: 3
Training loss: 2.0883219242095947
Validation loss: 2.9491034220623713

Epoch: 6| Step: 4
Training loss: 2.5239405632019043
Validation loss: 2.9460624238496185

Epoch: 6| Step: 5
Training loss: 3.063537359237671
Validation loss: 2.9478880410553305

Epoch: 6| Step: 6
Training loss: 2.1165220737457275
Validation loss: 2.945491067824825

Epoch: 6| Step: 7
Training loss: 3.8917133808135986
Validation loss: 2.9460281300288376

Epoch: 6| Step: 8
Training loss: 3.6291372776031494
Validation loss: 2.9448107596366637

Epoch: 6| Step: 9
Training loss: 3.7437427043914795
Validation loss: 2.945235803563108

Epoch: 6| Step: 10
Training loss: 3.301582098007202
Validation loss: 2.943032923565116

Epoch: 6| Step: 11
Training loss: 3.4855735301971436
Validation loss: 2.94180388604441

Epoch: 6| Step: 12
Training loss: 2.268233060836792
Validation loss: 2.942325251076811

Epoch: 6| Step: 13
Training loss: 2.7337446212768555
Validation loss: 2.939001608920354

Epoch: 27| Step: 0
Training loss: 2.816375732421875
Validation loss: 2.9423878474902083

Epoch: 6| Step: 1
Training loss: 2.8140149116516113
Validation loss: 2.9507577162916943

Epoch: 6| Step: 2
Training loss: 2.1697278022766113
Validation loss: 2.9572133223215737

Epoch: 6| Step: 3
Training loss: 4.0368194580078125
Validation loss: 2.9575685993317635

Epoch: 6| Step: 4
Training loss: 3.0353245735168457
Validation loss: 2.9504688580830893

Epoch: 6| Step: 5
Training loss: 2.7965354919433594
Validation loss: 2.9439644710991972

Epoch: 6| Step: 6
Training loss: 3.161545991897583
Validation loss: 2.942229101734777

Epoch: 6| Step: 7
Training loss: 3.296839952468872
Validation loss: 2.9415489576196157

Epoch: 6| Step: 8
Training loss: 3.096580982208252
Validation loss: 2.9403665963039605

Epoch: 6| Step: 9
Training loss: 2.9021706581115723
Validation loss: 2.94056970842423

Epoch: 6| Step: 10
Training loss: 2.574418306350708
Validation loss: 2.9397909974539154

Epoch: 6| Step: 11
Training loss: 3.5483832359313965
Validation loss: 2.937147150757492

Epoch: 6| Step: 12
Training loss: 3.4703803062438965
Validation loss: 2.9351541919092976

Epoch: 6| Step: 13
Training loss: 2.2200212478637695
Validation loss: 2.934427563862134

Epoch: 28| Step: 0
Training loss: 2.605588436126709
Validation loss: 2.935107356758528

Epoch: 6| Step: 1
Training loss: 3.066627025604248
Validation loss: 2.93508570168608

Epoch: 6| Step: 2
Training loss: 2.8676466941833496
Validation loss: 2.9351909160614014

Epoch: 6| Step: 3
Training loss: 2.8887174129486084
Validation loss: 2.939088018991614

Epoch: 6| Step: 4
Training loss: 3.2646584510803223
Validation loss: 2.9453127717459076

Epoch: 6| Step: 5
Training loss: 3.5834622383117676
Validation loss: 2.9533106819275887

Epoch: 6| Step: 6
Training loss: 2.730567455291748
Validation loss: 2.9441109370159846

Epoch: 6| Step: 7
Training loss: 2.671708583831787
Validation loss: 2.9402712647632887

Epoch: 6| Step: 8
Training loss: 3.3137669563293457
Validation loss: 2.934474780995359

Epoch: 6| Step: 9
Training loss: 3.4879515171051025
Validation loss: 2.931631734294276

Epoch: 6| Step: 10
Training loss: 3.0988950729370117
Validation loss: 2.928619679584298

Epoch: 6| Step: 11
Training loss: 3.1626152992248535
Validation loss: 2.9274910111581125

Epoch: 6| Step: 12
Training loss: 2.5023903846740723
Validation loss: 2.9295623815187843

Epoch: 6| Step: 13
Training loss: 2.825289249420166
Validation loss: 2.926063947780158

Epoch: 29| Step: 0
Training loss: 2.570713520050049
Validation loss: 2.926413272016792

Epoch: 6| Step: 1
Training loss: 3.5082902908325195
Validation loss: 2.9275975381174395

Epoch: 6| Step: 2
Training loss: 3.4506049156188965
Validation loss: 2.925191146071239

Epoch: 6| Step: 3
Training loss: 2.2571797370910645
Validation loss: 2.925373866993894

Epoch: 6| Step: 4
Training loss: 3.495919704437256
Validation loss: 2.9218940350317184

Epoch: 6| Step: 5
Training loss: 3.091844081878662
Validation loss: 2.922545345880652

Epoch: 6| Step: 6
Training loss: 3.440901756286621
Validation loss: 2.9257558443213023

Epoch: 6| Step: 7
Training loss: 3.306858777999878
Validation loss: 2.9215185770424466

Epoch: 6| Step: 8
Training loss: 2.6119723320007324
Validation loss: 2.9222402803359495

Epoch: 6| Step: 9
Training loss: 3.4136738777160645
Validation loss: 2.917694450706564

Epoch: 6| Step: 10
Training loss: 2.468487501144409
Validation loss: 2.9135758748618503

Epoch: 6| Step: 11
Training loss: 2.3003392219543457
Validation loss: 2.9161049242942565

Epoch: 6| Step: 12
Training loss: 2.978510856628418
Validation loss: 2.9153411593488467

Epoch: 6| Step: 13
Training loss: 3.39682674407959
Validation loss: 2.923013089805521

Epoch: 30| Step: 0
Training loss: 3.24501895904541
Validation loss: 2.9236295428327335

Epoch: 6| Step: 1
Training loss: 3.403444766998291
Validation loss: 2.9242527613075833

Epoch: 6| Step: 2
Training loss: 3.0279526710510254
Validation loss: 2.923269271850586

Epoch: 6| Step: 3
Training loss: 2.4822256565093994
Validation loss: 2.9213097300580753

Epoch: 6| Step: 4
Training loss: 3.162743091583252
Validation loss: 2.916412563734157

Epoch: 6| Step: 5
Training loss: 3.269432306289673
Validation loss: 2.913022812976632

Epoch: 6| Step: 6
Training loss: 2.102074146270752
Validation loss: 2.914211421884516

Epoch: 6| Step: 7
Training loss: 2.686495065689087
Validation loss: 2.9111669550659838

Epoch: 6| Step: 8
Training loss: 3.428873062133789
Validation loss: 2.9128415764019056

Epoch: 6| Step: 9
Training loss: 2.037536859512329
Validation loss: 2.911730602223386

Epoch: 6| Step: 10
Training loss: 3.9059109687805176
Validation loss: 2.9116523547839095

Epoch: 6| Step: 11
Training loss: 2.434393882751465
Validation loss: 2.9079265491936797

Epoch: 6| Step: 12
Training loss: 3.673366069793701
Validation loss: 2.9100842270799863

Epoch: 6| Step: 13
Training loss: 3.133674144744873
Validation loss: 2.9079417515826482

Epoch: 31| Step: 0
Training loss: 3.286658763885498
Validation loss: 2.9072611255030476

Epoch: 6| Step: 1
Training loss: 2.9292776584625244
Validation loss: 2.906616328864969

Epoch: 6| Step: 2
Training loss: 3.4480478763580322
Validation loss: 2.9047000715809483

Epoch: 6| Step: 3
Training loss: 2.572293519973755
Validation loss: 2.904086600067795

Epoch: 6| Step: 4
Training loss: 2.8066353797912598
Validation loss: 2.9042166817572808

Epoch: 6| Step: 5
Training loss: 2.9580917358398438
Validation loss: 2.903353860301356

Epoch: 6| Step: 6
Training loss: 2.2930655479431152
Validation loss: 2.9024592291924263

Epoch: 6| Step: 7
Training loss: 2.983336925506592
Validation loss: 2.8993053385006484

Epoch: 6| Step: 8
Training loss: 3.1114354133605957
Validation loss: 2.9001232706090456

Epoch: 6| Step: 9
Training loss: 2.3090293407440186
Validation loss: 2.909455830051053

Epoch: 6| Step: 10
Training loss: 3.430375099182129
Validation loss: 2.9079125004429973

Epoch: 6| Step: 11
Training loss: 2.772934913635254
Validation loss: 2.9054853352167274

Epoch: 6| Step: 12
Training loss: 4.398563385009766
Validation loss: 2.8983452012462

Epoch: 6| Step: 13
Training loss: 2.1662445068359375
Validation loss: 2.8978823231112574

Epoch: 32| Step: 0
Training loss: 3.513031482696533
Validation loss: 2.895190779880811

Epoch: 6| Step: 1
Training loss: 3.6391186714172363
Validation loss: 2.8920690833881335

Epoch: 6| Step: 2
Training loss: 2.9923014640808105
Validation loss: 2.893026567274524

Epoch: 6| Step: 3
Training loss: 2.762519359588623
Validation loss: 2.89326572930941

Epoch: 6| Step: 4
Training loss: 2.9361572265625
Validation loss: 2.8902673080403316

Epoch: 6| Step: 5
Training loss: 2.194173574447632
Validation loss: 2.890009780083933

Epoch: 6| Step: 6
Training loss: 2.963129997253418
Validation loss: 2.8884409755788822

Epoch: 6| Step: 7
Training loss: 2.9615015983581543
Validation loss: 2.887588567631219

Epoch: 6| Step: 8
Training loss: 3.098987579345703
Validation loss: 2.8935525391691472

Epoch: 6| Step: 9
Training loss: 2.4765191078186035
Validation loss: 2.8915996654059297

Epoch: 6| Step: 10
Training loss: 2.7267210483551025
Validation loss: 2.8905838868951284

Epoch: 6| Step: 11
Training loss: 2.8214311599731445
Validation loss: 2.893545037956648

Epoch: 6| Step: 12
Training loss: 4.057647705078125
Validation loss: 2.8924313796463834

Epoch: 6| Step: 13
Training loss: 2.234361171722412
Validation loss: 2.896507781039002

Epoch: 33| Step: 0
Training loss: 2.694938898086548
Validation loss: 2.8980861940691547

Epoch: 6| Step: 1
Training loss: 3.0512681007385254
Validation loss: 2.898275552257415

Epoch: 6| Step: 2
Training loss: 2.7456750869750977
Validation loss: 2.894171581473402

Epoch: 6| Step: 3
Training loss: 4.115747928619385
Validation loss: 2.8855263802313034

Epoch: 6| Step: 4
Training loss: 2.493911027908325
Validation loss: 2.881876153330649

Epoch: 6| Step: 5
Training loss: 3.628138303756714
Validation loss: 2.881783098302862

Epoch: 6| Step: 6
Training loss: 3.5063886642456055
Validation loss: 2.8826885966844458

Epoch: 6| Step: 7
Training loss: 2.5228970050811768
Validation loss: 2.8805079280689196

Epoch: 6| Step: 8
Training loss: 2.3371996879577637
Validation loss: 2.882479401044948

Epoch: 6| Step: 9
Training loss: 3.2127065658569336
Validation loss: 2.8817352940959315

Epoch: 6| Step: 10
Training loss: 2.4492669105529785
Validation loss: 2.8817559724212973

Epoch: 6| Step: 11
Training loss: 2.576650619506836
Validation loss: 2.8801720424364974

Epoch: 6| Step: 12
Training loss: 3.740248203277588
Validation loss: 2.8774549294543523

Epoch: 6| Step: 13
Training loss: 2.1581220626831055
Validation loss: 2.8744450384570706

Epoch: 34| Step: 0
Training loss: 3.208012104034424
Validation loss: 2.8743298822833645

Epoch: 6| Step: 1
Training loss: 2.9469499588012695
Validation loss: 2.875202642974033

Epoch: 6| Step: 2
Training loss: 3.029590368270874
Validation loss: 2.8745078194525933

Epoch: 6| Step: 3
Training loss: 3.065673828125
Validation loss: 2.874359305186938

Epoch: 6| Step: 4
Training loss: 2.383737564086914
Validation loss: 2.872703770155548

Epoch: 6| Step: 5
Training loss: 3.72591495513916
Validation loss: 2.8761412917926745

Epoch: 6| Step: 6
Training loss: 3.835681200027466
Validation loss: 2.8817840981227096

Epoch: 6| Step: 7
Training loss: 3.125959873199463
Validation loss: 2.8856798551415883

Epoch: 6| Step: 8
Training loss: 2.934112548828125
Validation loss: 2.875871807016352

Epoch: 6| Step: 9
Training loss: 2.826949119567871
Validation loss: 2.871984202374694

Epoch: 6| Step: 10
Training loss: 2.3471426963806152
Validation loss: 2.8693250404891146

Epoch: 6| Step: 11
Training loss: 2.6460537910461426
Validation loss: 2.8697762156045563

Epoch: 6| Step: 12
Training loss: 2.812747001647949
Validation loss: 2.866111381079561

Epoch: 6| Step: 13
Training loss: 2.3050010204315186
Validation loss: 2.861933897900325

Epoch: 35| Step: 0
Training loss: 2.728534460067749
Validation loss: 2.864041979594897

Epoch: 6| Step: 1
Training loss: 2.3261122703552246
Validation loss: 2.859192963569395

Epoch: 6| Step: 2
Training loss: 2.509739875793457
Validation loss: 2.866853380715975

Epoch: 6| Step: 3
Training loss: 2.9869511127471924
Validation loss: 2.8729149218528502

Epoch: 6| Step: 4
Training loss: 2.731896162033081
Validation loss: 2.891606192434988

Epoch: 6| Step: 5
Training loss: 2.954925060272217
Validation loss: 2.887363910675049

Epoch: 6| Step: 6
Training loss: 3.6786983013153076
Validation loss: 2.8768998371657504

Epoch: 6| Step: 7
Training loss: 3.719284772872925
Validation loss: 2.8668146005240818

Epoch: 6| Step: 8
Training loss: 3.264069080352783
Validation loss: 2.8600045506672194

Epoch: 6| Step: 9
Training loss: 3.1751019954681396
Validation loss: 2.8545581192098637

Epoch: 6| Step: 10
Training loss: 3.156219720840454
Validation loss: 2.8563375319204023

Epoch: 6| Step: 11
Training loss: 2.5890841484069824
Validation loss: 2.8582368153397755

Epoch: 6| Step: 12
Training loss: 2.5782198905944824
Validation loss: 2.857337987551125

Epoch: 6| Step: 13
Training loss: 3.0789430141448975
Validation loss: 2.863250163293654

Epoch: 36| Step: 0
Training loss: 1.7156939506530762
Validation loss: 2.8630115678233485

Epoch: 6| Step: 1
Training loss: 2.93155574798584
Validation loss: 2.8619925847617527

Epoch: 6| Step: 2
Training loss: 3.100955009460449
Validation loss: 2.8648262690472346

Epoch: 6| Step: 3
Training loss: 3.353722095489502
Validation loss: 2.8640899376202653

Epoch: 6| Step: 4
Training loss: 3.490098714828491
Validation loss: 2.8624043259569394

Epoch: 6| Step: 5
Training loss: 3.7194697856903076
Validation loss: 2.8602624477878695

Epoch: 6| Step: 6
Training loss: 2.477022409439087
Validation loss: 2.858011050890851

Epoch: 6| Step: 7
Training loss: 2.527374505996704
Validation loss: 2.855192833049323

Epoch: 6| Step: 8
Training loss: 2.7574687004089355
Validation loss: 2.8536236670709427

Epoch: 6| Step: 9
Training loss: 2.513934373855591
Validation loss: 2.853756438019455

Epoch: 6| Step: 10
Training loss: 3.3533897399902344
Validation loss: 2.851855457469981

Epoch: 6| Step: 11
Training loss: 3.8913166522979736
Validation loss: 2.849526923189881

Epoch: 6| Step: 12
Training loss: 2.804191827774048
Validation loss: 2.846895538350587

Epoch: 6| Step: 13
Training loss: 2.488621950149536
Validation loss: 2.844928321018014

Epoch: 37| Step: 0
Training loss: 3.0852177143096924
Validation loss: 2.8438279833844913

Epoch: 6| Step: 1
Training loss: 4.215019702911377
Validation loss: 2.8418089266746276

Epoch: 6| Step: 2
Training loss: 3.29671311378479
Validation loss: 2.8437638590412755

Epoch: 6| Step: 3
Training loss: 2.812821865081787
Validation loss: 2.842415302030502

Epoch: 6| Step: 4
Training loss: 1.9736742973327637
Validation loss: 2.844632133360832

Epoch: 6| Step: 5
Training loss: 2.758805513381958
Validation loss: 2.842687940084806

Epoch: 6| Step: 6
Training loss: 3.6192123889923096
Validation loss: 2.8406540142592562

Epoch: 6| Step: 7
Training loss: 2.916131019592285
Validation loss: 2.8378455305612214

Epoch: 6| Step: 8
Training loss: 1.8884649276733398
Validation loss: 2.838197200529037

Epoch: 6| Step: 9
Training loss: 3.163783311843872
Validation loss: 2.8382684876841884

Epoch: 6| Step: 10
Training loss: 2.8639354705810547
Validation loss: 2.8373943759549047

Epoch: 6| Step: 11
Training loss: 3.475677013397217
Validation loss: 2.8366825170414423

Epoch: 6| Step: 12
Training loss: 2.5542874336242676
Validation loss: 2.837005361433952

Epoch: 6| Step: 13
Training loss: 2.2225425243377686
Validation loss: 2.8355578222582416

Epoch: 38| Step: 0
Training loss: 2.8814053535461426
Validation loss: 2.836646379963044

Epoch: 6| Step: 1
Training loss: 3.6928534507751465
Validation loss: 2.8379286181542183

Epoch: 6| Step: 2
Training loss: 2.843844175338745
Validation loss: 2.8349622782840522

Epoch: 6| Step: 3
Training loss: 2.930210590362549
Validation loss: 2.8397878523795836

Epoch: 6| Step: 4
Training loss: 4.131567001342773
Validation loss: 2.836780014858451

Epoch: 6| Step: 5
Training loss: 2.85182523727417
Validation loss: 2.837330613085019

Epoch: 6| Step: 6
Training loss: 2.114018440246582
Validation loss: 2.8403747209938626

Epoch: 6| Step: 7
Training loss: 3.725208282470703
Validation loss: 2.8430163911593858

Epoch: 6| Step: 8
Training loss: 2.3017988204956055
Validation loss: 2.8500597502595637

Epoch: 6| Step: 9
Training loss: 2.6852684020996094
Validation loss: 2.8481607026951288

Epoch: 6| Step: 10
Training loss: 2.8757762908935547
Validation loss: 2.850422713064378

Epoch: 6| Step: 11
Training loss: 2.4725871086120605
Validation loss: 2.858233992771436

Epoch: 6| Step: 12
Training loss: 3.467219591140747
Validation loss: 2.86995852890835

Epoch: 6| Step: 13
Training loss: 1.5304515361785889
Validation loss: 2.8481263114560034

Epoch: 39| Step: 0
Training loss: 3.060962677001953
Validation loss: 2.836073754936136

Epoch: 6| Step: 1
Training loss: 3.3177242279052734
Validation loss: 2.8330940584982596

Epoch: 6| Step: 2
Training loss: 3.065223217010498
Validation loss: 2.8306508243724866

Epoch: 6| Step: 3
Training loss: 3.010805130004883
Validation loss: 2.8302294720885572

Epoch: 6| Step: 4
Training loss: 3.504168748855591
Validation loss: 2.8313535028888333

Epoch: 6| Step: 5
Training loss: 3.046998977661133
Validation loss: 2.8296264986838064

Epoch: 6| Step: 6
Training loss: 2.6129090785980225
Validation loss: 2.833900438841953

Epoch: 6| Step: 7
Training loss: 3.546621561050415
Validation loss: 2.8312760501779537

Epoch: 6| Step: 8
Training loss: 2.399207592010498
Validation loss: 2.827598189794889

Epoch: 6| Step: 9
Training loss: 2.4929380416870117
Validation loss: 2.829080202246225

Epoch: 6| Step: 10
Training loss: 3.411917209625244
Validation loss: 2.8252988758907525

Epoch: 6| Step: 11
Training loss: 2.475397825241089
Validation loss: 2.825881778552968

Epoch: 6| Step: 12
Training loss: 2.7108211517333984
Validation loss: 2.8244917367094304

Epoch: 6| Step: 13
Training loss: 2.11942720413208
Validation loss: 2.8219617900028022

Epoch: 40| Step: 0
Training loss: 2.6369400024414062
Validation loss: 2.8233791858919206

Epoch: 6| Step: 1
Training loss: 3.3445749282836914
Validation loss: 2.826543661855882

Epoch: 6| Step: 2
Training loss: 3.2194199562072754
Validation loss: 2.82996553503057

Epoch: 6| Step: 3
Training loss: 2.980571985244751
Validation loss: 2.843878020522415

Epoch: 6| Step: 4
Training loss: 2.4821176528930664
Validation loss: 2.8885117320604223

Epoch: 6| Step: 5
Training loss: 2.720468044281006
Validation loss: 2.8410220479452484

Epoch: 6| Step: 6
Training loss: 2.876011371612549
Validation loss: 2.8293183849703882

Epoch: 6| Step: 7
Training loss: 3.140126943588257
Validation loss: 2.822433579352594

Epoch: 6| Step: 8
Training loss: 2.9844796657562256
Validation loss: 2.8251480338394

Epoch: 6| Step: 9
Training loss: 3.2149500846862793
Validation loss: 2.820322887871855

Epoch: 6| Step: 10
Training loss: 3.0440850257873535
Validation loss: 2.8223981934209026

Epoch: 6| Step: 11
Training loss: 3.087345600128174
Validation loss: 2.828113127780217

Epoch: 6| Step: 12
Training loss: 2.2861149311065674
Validation loss: 2.824537472058368

Epoch: 6| Step: 13
Training loss: 3.2111587524414062
Validation loss: 2.821028499193089

Epoch: 41| Step: 0
Training loss: 3.2900145053863525
Validation loss: 2.820276173212195

Epoch: 6| Step: 1
Training loss: 3.4652788639068604
Validation loss: 2.8190941067152124

Epoch: 6| Step: 2
Training loss: 2.20556378364563
Validation loss: 2.817701429449102

Epoch: 6| Step: 3
Training loss: 2.657125949859619
Validation loss: 2.815552996050927

Epoch: 6| Step: 4
Training loss: 3.2284908294677734
Validation loss: 2.816506826749412

Epoch: 6| Step: 5
Training loss: 3.4631216526031494
Validation loss: 2.8130522312656527

Epoch: 6| Step: 6
Training loss: 2.949251651763916
Validation loss: 2.8116064122928086

Epoch: 6| Step: 7
Training loss: 2.2752280235290527
Validation loss: 2.8100754548144597

Epoch: 6| Step: 8
Training loss: 2.1098647117614746
Validation loss: 2.806672219307192

Epoch: 6| Step: 9
Training loss: 3.980586290359497
Validation loss: 2.8048774811529342

Epoch: 6| Step: 10
Training loss: 3.151529312133789
Validation loss: 2.7984124896346882

Epoch: 6| Step: 11
Training loss: 2.4101240634918213
Validation loss: 2.8029697402831046

Epoch: 6| Step: 12
Training loss: 2.9942517280578613
Validation loss: 2.8010901225510465

Epoch: 6| Step: 13
Training loss: 2.5637166500091553
Validation loss: 2.8059761139654342

Epoch: 42| Step: 0
Training loss: 3.0217599868774414
Validation loss: 2.810371160507202

Epoch: 6| Step: 1
Training loss: 2.9690256118774414
Validation loss: 2.8107737879599295

Epoch: 6| Step: 2
Training loss: 3.572859764099121
Validation loss: 2.808829366519887

Epoch: 6| Step: 3
Training loss: 3.0193145275115967
Validation loss: 2.8094256949681107

Epoch: 6| Step: 4
Training loss: 3.344785690307617
Validation loss: 2.8060815206138034

Epoch: 6| Step: 5
Training loss: 3.181347370147705
Validation loss: 2.799452107439759

Epoch: 6| Step: 6
Training loss: 1.916466474533081
Validation loss: 2.8022925315364713

Epoch: 6| Step: 7
Training loss: 2.705883502960205
Validation loss: 2.7934779608121483

Epoch: 6| Step: 8
Training loss: 3.1452324390411377
Validation loss: 2.7957261659765757

Epoch: 6| Step: 9
Training loss: 2.9033451080322266
Validation loss: 2.7935756637204077

Epoch: 6| Step: 10
Training loss: 2.511636257171631
Validation loss: 2.7919660332382366

Epoch: 6| Step: 11
Training loss: 2.393775463104248
Validation loss: 2.7933156387780302

Epoch: 6| Step: 12
Training loss: 3.261913537979126
Validation loss: 2.793080550368114

Epoch: 6| Step: 13
Training loss: 2.9151601791381836
Validation loss: 2.7908157994670253

Epoch: 43| Step: 0
Training loss: 2.129535675048828
Validation loss: 2.7906589405511015

Epoch: 6| Step: 1
Training loss: 4.026113033294678
Validation loss: 2.7897604562902965

Epoch: 6| Step: 2
Training loss: 3.400916814804077
Validation loss: 2.7935686675451135

Epoch: 6| Step: 3
Training loss: 2.378061294555664
Validation loss: 2.795453515104068

Epoch: 6| Step: 4
Training loss: 3.3444008827209473
Validation loss: 2.8037543501905215

Epoch: 6| Step: 5
Training loss: 3.176112651824951
Validation loss: 2.8103743804398404

Epoch: 6| Step: 6
Training loss: 2.3295230865478516
Validation loss: 2.8203806748954197

Epoch: 6| Step: 7
Training loss: 2.9023966789245605
Validation loss: 2.8168560074221705

Epoch: 6| Step: 8
Training loss: 3.7321791648864746
Validation loss: 2.8246895292753815

Epoch: 6| Step: 9
Training loss: 2.3991472721099854
Validation loss: 2.81154784335885

Epoch: 6| Step: 10
Training loss: 3.0642340183258057
Validation loss: 2.8145748671664985

Epoch: 6| Step: 11
Training loss: 2.0272507667541504
Validation loss: 2.8092571919964207

Epoch: 6| Step: 12
Training loss: 2.917109966278076
Validation loss: 2.8062489135290987

Epoch: 6| Step: 13
Training loss: 2.9350550174713135
Validation loss: 2.8088225139084684

Epoch: 44| Step: 0
Training loss: 2.581393003463745
Validation loss: 2.806046344900644

Epoch: 6| Step: 1
Training loss: 3.0809593200683594
Validation loss: 2.800703771652714

Epoch: 6| Step: 2
Training loss: 2.5272529125213623
Validation loss: 2.7935733948984454

Epoch: 6| Step: 3
Training loss: 2.8948071002960205
Validation loss: 2.789154916681269

Epoch: 6| Step: 4
Training loss: 2.541193962097168
Validation loss: 2.7924143396398073

Epoch: 6| Step: 5
Training loss: 2.968414783477783
Validation loss: 2.79791122610851

Epoch: 6| Step: 6
Training loss: 3.2461495399475098
Validation loss: 2.7912551767082623

Epoch: 6| Step: 7
Training loss: 2.253340005874634
Validation loss: 2.78516193871857

Epoch: 6| Step: 8
Training loss: 3.6200273036956787
Validation loss: 2.785272726448633

Epoch: 6| Step: 9
Training loss: 2.773137092590332
Validation loss: 2.7813992936124086

Epoch: 6| Step: 10
Training loss: 2.453195333480835
Validation loss: 2.7777779204871065

Epoch: 6| Step: 11
Training loss: 3.2992043495178223
Validation loss: 2.7780984294029976

Epoch: 6| Step: 12
Training loss: 3.370231866836548
Validation loss: 2.7794051221621934

Epoch: 6| Step: 13
Training loss: 3.2209410667419434
Validation loss: 2.7824035870131625

Epoch: 45| Step: 0
Training loss: 3.000849962234497
Validation loss: 2.7817745260013047

Epoch: 6| Step: 1
Training loss: 3.693056583404541
Validation loss: 2.782690530182213

Epoch: 6| Step: 2
Training loss: 2.9055604934692383
Validation loss: 2.7857733439373713

Epoch: 6| Step: 3
Training loss: 3.145838737487793
Validation loss: 2.784444837160008

Epoch: 6| Step: 4
Training loss: 2.6802096366882324
Validation loss: 2.796101206092424

Epoch: 6| Step: 5
Training loss: 3.225308895111084
Validation loss: 2.7987102282944547

Epoch: 6| Step: 6
Training loss: 3.1473634243011475
Validation loss: 2.8040229492290045

Epoch: 6| Step: 7
Training loss: 3.5714263916015625
Validation loss: 2.8190646966298423

Epoch: 6| Step: 8
Training loss: 2.894594192504883
Validation loss: 2.8116865722081994

Epoch: 6| Step: 9
Training loss: 1.8721448183059692
Validation loss: 2.803150451311501

Epoch: 6| Step: 10
Training loss: 2.5662078857421875
Validation loss: 2.789904602112309

Epoch: 6| Step: 11
Training loss: 2.683953285217285
Validation loss: 2.7844049546026413

Epoch: 6| Step: 12
Training loss: 1.8327503204345703
Validation loss: 2.779577145012476

Epoch: 6| Step: 13
Training loss: 3.7544682025909424
Validation loss: 2.77501501575593

Epoch: 46| Step: 0
Training loss: 3.335381507873535
Validation loss: 2.773032006397042

Epoch: 6| Step: 1
Training loss: 2.4563097953796387
Validation loss: 2.772337387966853

Epoch: 6| Step: 2
Training loss: 2.942943572998047
Validation loss: 2.7719607019937165

Epoch: 6| Step: 3
Training loss: 2.8730039596557617
Validation loss: 2.7752107958639822

Epoch: 6| Step: 4
Training loss: 3.2321548461914062
Validation loss: 2.780984873412758

Epoch: 6| Step: 5
Training loss: 2.1988213062286377
Validation loss: 2.7790488389230545

Epoch: 6| Step: 6
Training loss: 3.122121810913086
Validation loss: 2.772069695175335

Epoch: 6| Step: 7
Training loss: 3.3422513008117676
Validation loss: 2.7708268627043693

Epoch: 6| Step: 8
Training loss: 2.7683568000793457
Validation loss: 2.7704698219094226

Epoch: 6| Step: 9
Training loss: 2.794658899307251
Validation loss: 2.771006638003934

Epoch: 6| Step: 10
Training loss: 3.0897209644317627
Validation loss: 2.7743393169936312

Epoch: 6| Step: 11
Training loss: 3.3046493530273438
Validation loss: 2.7743672632401988

Epoch: 6| Step: 12
Training loss: 2.4060912132263184
Validation loss: 2.7760067755176174

Epoch: 6| Step: 13
Training loss: 2.4313981533050537
Validation loss: 2.7753633863182476

Epoch: 47| Step: 0
Training loss: 2.423410415649414
Validation loss: 2.778228049637169

Epoch: 6| Step: 1
Training loss: 3.227950096130371
Validation loss: 2.774360695192891

Epoch: 6| Step: 2
Training loss: 2.4523675441741943
Validation loss: 2.7743595851364957

Epoch: 6| Step: 3
Training loss: 3.6922922134399414
Validation loss: 2.774130108535931

Epoch: 6| Step: 4
Training loss: 2.396097183227539
Validation loss: 2.7729276021321616

Epoch: 6| Step: 5
Training loss: 2.6462740898132324
Validation loss: 2.770361218401181

Epoch: 6| Step: 6
Training loss: 3.3009285926818848
Validation loss: 2.7714313691662205

Epoch: 6| Step: 7
Training loss: 4.018143177032471
Validation loss: 2.7667897080862396

Epoch: 6| Step: 8
Training loss: 3.394591808319092
Validation loss: 2.7660774364266345

Epoch: 6| Step: 9
Training loss: 2.5234663486480713
Validation loss: 2.7635531656203733

Epoch: 6| Step: 10
Training loss: 1.9779822826385498
Validation loss: 2.766690477248161

Epoch: 6| Step: 11
Training loss: 2.7785964012145996
Validation loss: 2.7655427148265224

Epoch: 6| Step: 12
Training loss: 2.9172754287719727
Validation loss: 2.7652623217592955

Epoch: 6| Step: 13
Training loss: 2.5844266414642334
Validation loss: 2.7683534955465667

Epoch: 48| Step: 0
Training loss: 3.1307246685028076
Validation loss: 2.7656344059974916

Epoch: 6| Step: 1
Training loss: 2.713390350341797
Validation loss: 2.768080328100471

Epoch: 6| Step: 2
Training loss: 2.321495771408081
Validation loss: 2.7681505859539075

Epoch: 6| Step: 3
Training loss: 2.6568422317504883
Validation loss: 2.764882315871536

Epoch: 6| Step: 4
Training loss: 2.5069098472595215
Validation loss: 2.764770810322095

Epoch: 6| Step: 5
Training loss: 3.6443490982055664
Validation loss: 2.7676044587166078

Epoch: 6| Step: 6
Training loss: 2.031782388687134
Validation loss: 2.7638228221606185

Epoch: 6| Step: 7
Training loss: 2.772757053375244
Validation loss: 2.7614437995418424

Epoch: 6| Step: 8
Training loss: 4.11793851852417
Validation loss: 2.760314843987906

Epoch: 6| Step: 9
Training loss: 2.4720993041992188
Validation loss: 2.759859605502057

Epoch: 6| Step: 10
Training loss: 3.483794689178467
Validation loss: 2.758485194175474

Epoch: 6| Step: 11
Training loss: 3.372814178466797
Validation loss: 2.7538631808373237

Epoch: 6| Step: 12
Training loss: 2.967834949493408
Validation loss: 2.760286484995196

Epoch: 6| Step: 13
Training loss: 1.6827483177185059
Validation loss: 2.7594841603309876

Epoch: 49| Step: 0
Training loss: 2.831981658935547
Validation loss: 2.7574039249009985

Epoch: 6| Step: 1
Training loss: 3.205864906311035
Validation loss: 2.7576578150513353

Epoch: 6| Step: 2
Training loss: 4.080216407775879
Validation loss: 2.7551387253627984

Epoch: 6| Step: 3
Training loss: 3.0482287406921387
Validation loss: 2.758771760489351

Epoch: 6| Step: 4
Training loss: 2.472827911376953
Validation loss: 2.7596959708839335

Epoch: 6| Step: 5
Training loss: 3.172858715057373
Validation loss: 2.7610718921948503

Epoch: 6| Step: 6
Training loss: 2.654484272003174
Validation loss: 2.759490395105013

Epoch: 6| Step: 7
Training loss: 3.2045297622680664
Validation loss: 2.7576707563092633

Epoch: 6| Step: 8
Training loss: 2.8374099731445312
Validation loss: 2.7590789025829685

Epoch: 6| Step: 9
Training loss: 1.7315590381622314
Validation loss: 2.7556971068023355

Epoch: 6| Step: 10
Training loss: 2.9741034507751465
Validation loss: 2.7523574803465154

Epoch: 6| Step: 11
Training loss: 2.457888603210449
Validation loss: 2.751509681824715

Epoch: 6| Step: 12
Training loss: 2.7145748138427734
Validation loss: 2.751673589470566

Epoch: 6| Step: 13
Training loss: 3.2321691513061523
Validation loss: 2.752879406816216

Epoch: 50| Step: 0
Training loss: 2.9218716621398926
Validation loss: 2.7502312608944472

Epoch: 6| Step: 1
Training loss: 2.27390456199646
Validation loss: 2.7517720576255553

Epoch: 6| Step: 2
Training loss: 3.3382911682128906
Validation loss: 2.7522286010044876

Epoch: 6| Step: 3
Training loss: 2.5623557567596436
Validation loss: 2.7558923485458537

Epoch: 6| Step: 4
Training loss: 3.0186545848846436
Validation loss: 2.757316686773813

Epoch: 6| Step: 5
Training loss: 2.1183571815490723
Validation loss: 2.7574817954853015

Epoch: 6| Step: 6
Training loss: 3.8643741607666016
Validation loss: 2.7554399736465944

Epoch: 6| Step: 7
Training loss: 3.6683571338653564
Validation loss: 2.7504821259488343

Epoch: 6| Step: 8
Training loss: 3.0671913623809814
Validation loss: 2.747561549627653

Epoch: 6| Step: 9
Training loss: 2.6192479133605957
Validation loss: 2.7470028477330364

Epoch: 6| Step: 10
Training loss: 2.7754321098327637
Validation loss: 2.749520342837098

Epoch: 6| Step: 11
Training loss: 2.6564106941223145
Validation loss: 2.7508343035174954

Epoch: 6| Step: 12
Training loss: 2.631934642791748
Validation loss: 2.751373811434674

Epoch: 6| Step: 13
Training loss: 2.654001235961914
Validation loss: 2.749978491055068

Epoch: 51| Step: 0
Training loss: 2.7479403018951416
Validation loss: 2.747237654142482

Epoch: 6| Step: 1
Training loss: 2.8304171562194824
Validation loss: 2.7457550084719093

Epoch: 6| Step: 2
Training loss: 2.3555760383605957
Validation loss: 2.7430314222971597

Epoch: 6| Step: 3
Training loss: 3.4012365341186523
Validation loss: 2.742625582602716

Epoch: 6| Step: 4
Training loss: 2.369218349456787
Validation loss: 2.7418736693679646

Epoch: 6| Step: 5
Training loss: 3.2851898670196533
Validation loss: 2.7411318235499884

Epoch: 6| Step: 6
Training loss: 3.237762928009033
Validation loss: 2.740066987211986

Epoch: 6| Step: 7
Training loss: 2.7374978065490723
Validation loss: 2.739905954689108

Epoch: 6| Step: 8
Training loss: 2.3851847648620605
Validation loss: 2.7387832774910876

Epoch: 6| Step: 9
Training loss: 2.5471880435943604
Validation loss: 2.7387641963138374

Epoch: 6| Step: 10
Training loss: 3.608797788619995
Validation loss: 2.7374804378837667

Epoch: 6| Step: 11
Training loss: 2.625176191329956
Validation loss: 2.736433352193525

Epoch: 6| Step: 12
Training loss: 3.3597469329833984
Validation loss: 2.736342235278058

Epoch: 6| Step: 13
Training loss: 2.428868293762207
Validation loss: 2.7354748966873332

Epoch: 52| Step: 0
Training loss: 3.445103645324707
Validation loss: 2.734749355623799

Epoch: 6| Step: 1
Training loss: 2.8597912788391113
Validation loss: 2.7339666761377805

Epoch: 6| Step: 2
Training loss: 3.0961546897888184
Validation loss: 2.7399181806912987

Epoch: 6| Step: 3
Training loss: 3.3170948028564453
Validation loss: 2.7438357824920327

Epoch: 6| Step: 4
Training loss: 3.711513042449951
Validation loss: 2.7493323715784217

Epoch: 6| Step: 5
Training loss: 2.869532585144043
Validation loss: 2.753442551500054

Epoch: 6| Step: 6
Training loss: 2.9504024982452393
Validation loss: 2.759005364551339

Epoch: 6| Step: 7
Training loss: 2.2425198554992676
Validation loss: 2.767345702776345

Epoch: 6| Step: 8
Training loss: 2.792466640472412
Validation loss: 2.7651085674121814

Epoch: 6| Step: 9
Training loss: 2.949812889099121
Validation loss: 2.748858695389122

Epoch: 6| Step: 10
Training loss: 2.4065370559692383
Validation loss: 2.744710035221551

Epoch: 6| Step: 11
Training loss: 2.550978660583496
Validation loss: 2.738213667305567

Epoch: 6| Step: 12
Training loss: 2.6785078048706055
Validation loss: 2.7348405238120788

Epoch: 6| Step: 13
Training loss: 1.8190345764160156
Validation loss: 2.7359475628022225

Epoch: 53| Step: 0
Training loss: 2.5472307205200195
Validation loss: 2.7357527222684634

Epoch: 6| Step: 1
Training loss: 3.2200703620910645
Validation loss: 2.7345638480237735

Epoch: 6| Step: 2
Training loss: 3.104410409927368
Validation loss: 2.733443160210886

Epoch: 6| Step: 3
Training loss: 2.344198226928711
Validation loss: 2.7331346055512786

Epoch: 6| Step: 4
Training loss: 3.5231518745422363
Validation loss: 2.732358440276115

Epoch: 6| Step: 5
Training loss: 3.0637009143829346
Validation loss: 2.7320905039387364

Epoch: 6| Step: 6
Training loss: 2.8673479557037354
Validation loss: 2.735768066939487

Epoch: 6| Step: 7
Training loss: 2.431743621826172
Validation loss: 2.731390727463589

Epoch: 6| Step: 8
Training loss: 2.7170372009277344
Validation loss: 2.732227320312172

Epoch: 6| Step: 9
Training loss: 3.051098108291626
Validation loss: 2.72920084768726

Epoch: 6| Step: 10
Training loss: 2.6221394538879395
Validation loss: 2.7315066681113294

Epoch: 6| Step: 11
Training loss: 3.170322895050049
Validation loss: 2.732848439165341

Epoch: 6| Step: 12
Training loss: 2.6527323722839355
Validation loss: 2.731951387979651

Epoch: 6| Step: 13
Training loss: 2.595736503601074
Validation loss: 2.730407207242904

Epoch: 54| Step: 0
Training loss: 2.418901205062866
Validation loss: 2.7307843162167456

Epoch: 6| Step: 1
Training loss: 4.098946571350098
Validation loss: 2.731261727630451

Epoch: 6| Step: 2
Training loss: 3.241492748260498
Validation loss: 2.730579872285166

Epoch: 6| Step: 3
Training loss: 3.450826644897461
Validation loss: 2.7295379484853437

Epoch: 6| Step: 4
Training loss: 2.104644298553467
Validation loss: 2.726832310358683

Epoch: 6| Step: 5
Training loss: 2.2862801551818848
Validation loss: 2.727568370039745

Epoch: 6| Step: 6
Training loss: 3.266204595565796
Validation loss: 2.7264696500634633

Epoch: 6| Step: 7
Training loss: 3.362626075744629
Validation loss: 2.7235590591225574

Epoch: 6| Step: 8
Training loss: 3.2044003009796143
Validation loss: 2.722408588214587

Epoch: 6| Step: 9
Training loss: 2.984769821166992
Validation loss: 2.725173963013516

Epoch: 6| Step: 10
Training loss: 1.956484079360962
Validation loss: 2.7265349049721994

Epoch: 6| Step: 11
Training loss: 2.354011058807373
Validation loss: 2.726392597280523

Epoch: 6| Step: 12
Training loss: 2.5843091011047363
Validation loss: 2.7310408725533435

Epoch: 6| Step: 13
Training loss: 2.501610279083252
Validation loss: 2.7345147850692912

Epoch: 55| Step: 0
Training loss: 3.128082275390625
Validation loss: 2.7239045686619257

Epoch: 6| Step: 1
Training loss: 4.289076805114746
Validation loss: 2.7238587410219255

Epoch: 6| Step: 2
Training loss: 2.6926486492156982
Validation loss: 2.715997988177884

Epoch: 6| Step: 3
Training loss: 2.861117362976074
Validation loss: 2.71061247651295

Epoch: 6| Step: 4
Training loss: 3.722592353820801
Validation loss: 2.709710277536864

Epoch: 6| Step: 5
Training loss: 2.555326223373413
Validation loss: 2.7059812904686056

Epoch: 6| Step: 6
Training loss: 2.147156238555908
Validation loss: 2.7033477367893344

Epoch: 6| Step: 7
Training loss: 2.5810084342956543
Validation loss: 2.704742529058969

Epoch: 6| Step: 8
Training loss: 3.0903730392456055
Validation loss: 2.7046374633748043

Epoch: 6| Step: 9
Training loss: 2.4625322818756104
Validation loss: 2.7046500572594265

Epoch: 6| Step: 10
Training loss: 2.273653507232666
Validation loss: 2.7045777356752785

Epoch: 6| Step: 11
Training loss: 3.150120258331299
Validation loss: 2.7121795300514466

Epoch: 6| Step: 12
Training loss: 2.2131175994873047
Validation loss: 2.7094663496940368

Epoch: 6| Step: 13
Training loss: 2.6556406021118164
Validation loss: 2.706614073886666

Epoch: 56| Step: 0
Training loss: 2.8818249702453613
Validation loss: 2.7082054256111063

Epoch: 6| Step: 1
Training loss: 2.251495599746704
Validation loss: 2.7056667035625828

Epoch: 6| Step: 2
Training loss: 3.6651744842529297
Validation loss: 2.7088423698179183

Epoch: 6| Step: 3
Training loss: 3.2051544189453125
Validation loss: 2.7094351501875025

Epoch: 6| Step: 4
Training loss: 2.4102277755737305
Validation loss: 2.7105473420953237

Epoch: 6| Step: 5
Training loss: 3.12455415725708
Validation loss: 2.7141600578061995

Epoch: 6| Step: 6
Training loss: 2.535250186920166
Validation loss: 2.7103985201927925

Epoch: 6| Step: 7
Training loss: 3.149841547012329
Validation loss: 2.7072546097540084

Epoch: 6| Step: 8
Training loss: 2.269855260848999
Validation loss: 2.705427436418431

Epoch: 6| Step: 9
Training loss: 3.115748405456543
Validation loss: 2.699818418871972

Epoch: 6| Step: 10
Training loss: 3.024110794067383
Validation loss: 2.6949187555620746

Epoch: 6| Step: 11
Training loss: 3.2255539894104004
Validation loss: 2.6956299325471282

Epoch: 6| Step: 12
Training loss: 2.2754452228546143
Validation loss: 2.695445881094984

Epoch: 6| Step: 13
Training loss: 2.3542087078094482
Validation loss: 2.6954090928518646

Epoch: 57| Step: 0
Training loss: 3.2174947261810303
Validation loss: 2.695931952486756

Epoch: 6| Step: 1
Training loss: 2.5884768962860107
Validation loss: 2.6937921867575696

Epoch: 6| Step: 2
Training loss: 3.222923755645752
Validation loss: 2.6950220754069667

Epoch: 6| Step: 3
Training loss: 2.4783103466033936
Validation loss: 2.6933282498390443

Epoch: 6| Step: 4
Training loss: 3.63794207572937
Validation loss: 2.69750916060581

Epoch: 6| Step: 5
Training loss: 2.226261615753174
Validation loss: 2.6974319386225876

Epoch: 6| Step: 6
Training loss: 3.3437840938568115
Validation loss: 2.699509222020385

Epoch: 6| Step: 7
Training loss: 2.393305778503418
Validation loss: 2.699592557004703

Epoch: 6| Step: 8
Training loss: 2.6292262077331543
Validation loss: 2.7015715196568477

Epoch: 6| Step: 9
Training loss: 2.982196569442749
Validation loss: 2.7072480417067006

Epoch: 6| Step: 10
Training loss: 3.566981315612793
Validation loss: 2.7037640310102895

Epoch: 6| Step: 11
Training loss: 2.0414085388183594
Validation loss: 2.703656583703974

Epoch: 6| Step: 12
Training loss: 2.655200958251953
Validation loss: 2.6990173478280344

Epoch: 6| Step: 13
Training loss: 2.6070303916931152
Validation loss: 2.696017496047481

Epoch: 58| Step: 0
Training loss: 3.624290943145752
Validation loss: 2.692633669863465

Epoch: 6| Step: 1
Training loss: 2.9895195960998535
Validation loss: 2.6910009845610587

Epoch: 6| Step: 2
Training loss: 2.657508373260498
Validation loss: 2.68922701702323

Epoch: 6| Step: 3
Training loss: 2.9408650398254395
Validation loss: 2.685119413560437

Epoch: 6| Step: 4
Training loss: 2.990060806274414
Validation loss: 2.685547428746377

Epoch: 6| Step: 5
Training loss: 3.0593342781066895
Validation loss: 2.685640670919931

Epoch: 6| Step: 6
Training loss: 1.9470415115356445
Validation loss: 2.686361507702899

Epoch: 6| Step: 7
Training loss: 2.297724723815918
Validation loss: 2.6869722027932443

Epoch: 6| Step: 8
Training loss: 3.6066789627075195
Validation loss: 2.6880848382108953

Epoch: 6| Step: 9
Training loss: 3.193927764892578
Validation loss: 2.6909770657939296

Epoch: 6| Step: 10
Training loss: 2.2421228885650635
Validation loss: 2.6869173767746135

Epoch: 6| Step: 11
Training loss: 3.0426902770996094
Validation loss: 2.6880869711599042

Epoch: 6| Step: 12
Training loss: 3.032750129699707
Validation loss: 2.6890032932322514

Epoch: 6| Step: 13
Training loss: 1.1339788436889648
Validation loss: 2.687455161925285

Epoch: 59| Step: 0
Training loss: 3.2312707901000977
Validation loss: 2.6912153895183275

Epoch: 6| Step: 1
Training loss: 3.0987658500671387
Validation loss: 2.6967480823557866

Epoch: 6| Step: 2
Training loss: 2.959321975708008
Validation loss: 2.6999142016133955

Epoch: 6| Step: 3
Training loss: 2.1321098804473877
Validation loss: 2.7006075023322977

Epoch: 6| Step: 4
Training loss: 3.6950340270996094
Validation loss: 2.700436294719737

Epoch: 6| Step: 5
Training loss: 2.6926016807556152
Validation loss: 2.7030648210997223

Epoch: 6| Step: 6
Training loss: 2.5928120613098145
Validation loss: 2.6982523241350727

Epoch: 6| Step: 7
Training loss: 3.3439557552337646
Validation loss: 2.695298833231772

Epoch: 6| Step: 8
Training loss: 2.5445680618286133
Validation loss: 2.6883387539976384

Epoch: 6| Step: 9
Training loss: 2.7967827320098877
Validation loss: 2.6879237185242357

Epoch: 6| Step: 10
Training loss: 3.061920642852783
Validation loss: 2.6852587730653825

Epoch: 6| Step: 11
Training loss: 2.789990186691284
Validation loss: 2.681204398473104

Epoch: 6| Step: 12
Training loss: 2.1354105472564697
Validation loss: 2.6836183071136475

Epoch: 6| Step: 13
Training loss: 2.1224076747894287
Validation loss: 2.683189179307671

Epoch: 60| Step: 0
Training loss: 4.353586196899414
Validation loss: 2.6850434400702037

Epoch: 6| Step: 1
Training loss: 3.5304253101348877
Validation loss: 2.689346203240015

Epoch: 6| Step: 2
Training loss: 3.3737263679504395
Validation loss: 2.6864875721675094

Epoch: 6| Step: 3
Training loss: 2.1219139099121094
Validation loss: 2.681035610937303

Epoch: 6| Step: 4
Training loss: 3.2361834049224854
Validation loss: 2.681245114213677

Epoch: 6| Step: 5
Training loss: 1.9912924766540527
Validation loss: 2.6757805578170286

Epoch: 6| Step: 6
Training loss: 2.0526585578918457
Validation loss: 2.6758046304025958

Epoch: 6| Step: 7
Training loss: 2.4623186588287354
Validation loss: 2.676577055326072

Epoch: 6| Step: 8
Training loss: 2.3305606842041016
Validation loss: 2.6736686614251908

Epoch: 6| Step: 9
Training loss: 3.3030266761779785
Validation loss: 2.6746994910701627

Epoch: 6| Step: 10
Training loss: 2.7139320373535156
Validation loss: 2.674188560055148

Epoch: 6| Step: 11
Training loss: 2.4851908683776855
Validation loss: 2.67347187893365

Epoch: 6| Step: 12
Training loss: 2.68576717376709
Validation loss: 2.672374766360047

Epoch: 6| Step: 13
Training loss: 2.7274577617645264
Validation loss: 2.6712087251806773

Epoch: 61| Step: 0
Training loss: 3.450458526611328
Validation loss: 2.672257520819223

Epoch: 6| Step: 1
Training loss: 2.6024837493896484
Validation loss: 2.674622784378708

Epoch: 6| Step: 2
Training loss: 2.1637165546417236
Validation loss: 2.6776291939520065

Epoch: 6| Step: 3
Training loss: 3.2965071201324463
Validation loss: 2.6779191211987565

Epoch: 6| Step: 4
Training loss: 3.5344748497009277
Validation loss: 2.677630191208214

Epoch: 6| Step: 5
Training loss: 2.6968183517456055
Validation loss: 2.6805933957458823

Epoch: 6| Step: 6
Training loss: 2.244622230529785
Validation loss: 2.677905328812138

Epoch: 6| Step: 7
Training loss: 2.5753750801086426
Validation loss: 2.6774691381762104

Epoch: 6| Step: 8
Training loss: 2.679084300994873
Validation loss: 2.677533313792239

Epoch: 6| Step: 9
Training loss: 3.325143337249756
Validation loss: 2.6799320636257047

Epoch: 6| Step: 10
Training loss: 3.042050361633301
Validation loss: 2.6768453890277493

Epoch: 6| Step: 11
Training loss: 2.6192374229431152
Validation loss: 2.6719805322667605

Epoch: 6| Step: 12
Training loss: 2.225876808166504
Validation loss: 2.669964869817098

Epoch: 6| Step: 13
Training loss: 2.968733787536621
Validation loss: 2.664798782717797

Epoch: 62| Step: 0
Training loss: 3.3770408630371094
Validation loss: 2.662946024248677

Epoch: 6| Step: 1
Training loss: 2.930375576019287
Validation loss: 2.6655728893895305

Epoch: 6| Step: 2
Training loss: 4.0748467445373535
Validation loss: 2.6678995470846854

Epoch: 6| Step: 3
Training loss: 1.9418612718582153
Validation loss: 2.6701280250344226

Epoch: 6| Step: 4
Training loss: 2.175671339035034
Validation loss: 2.672270905586981

Epoch: 6| Step: 5
Training loss: 2.6158201694488525
Validation loss: 2.679328180128528

Epoch: 6| Step: 6
Training loss: 2.107001304626465
Validation loss: 2.6648040689447874

Epoch: 6| Step: 7
Training loss: 2.7092831134796143
Validation loss: 2.6658605298688336

Epoch: 6| Step: 8
Training loss: 3.076984405517578
Validation loss: 2.661843487011489

Epoch: 6| Step: 9
Training loss: 2.282338857650757
Validation loss: 2.663830103412751

Epoch: 6| Step: 10
Training loss: 3.174834728240967
Validation loss: 2.662522344179051

Epoch: 6| Step: 11
Training loss: 2.7524800300598145
Validation loss: 2.6648264623457387

Epoch: 6| Step: 12
Training loss: 3.0669045448303223
Validation loss: 2.662945293611096

Epoch: 6| Step: 13
Training loss: 3.1265153884887695
Validation loss: 2.6661997213158557

Epoch: 63| Step: 0
Training loss: 2.5875308513641357
Validation loss: 2.667546187677691

Epoch: 6| Step: 1
Training loss: 2.52091646194458
Validation loss: 2.67190412552126

Epoch: 6| Step: 2
Training loss: 3.098477363586426
Validation loss: 2.6697799877453874

Epoch: 6| Step: 3
Training loss: 2.3970329761505127
Validation loss: 2.665703232570361

Epoch: 6| Step: 4
Training loss: 2.5101187229156494
Validation loss: 2.6638834194470475

Epoch: 6| Step: 5
Training loss: 3.4170103073120117
Validation loss: 2.6600921666750343

Epoch: 6| Step: 6
Training loss: 2.9441561698913574
Validation loss: 2.65921038709661

Epoch: 6| Step: 7
Training loss: 3.7456111907958984
Validation loss: 2.659046939624253

Epoch: 6| Step: 8
Training loss: 2.8461947441101074
Validation loss: 2.6572979522007767

Epoch: 6| Step: 9
Training loss: 2.9618358612060547
Validation loss: 2.657735668202882

Epoch: 6| Step: 10
Training loss: 2.317906141281128
Validation loss: 2.657607099061371

Epoch: 6| Step: 11
Training loss: 2.6852262020111084
Validation loss: 2.658587163494479

Epoch: 6| Step: 12
Training loss: 3.1296768188476562
Validation loss: 2.6592614881453978

Epoch: 6| Step: 13
Training loss: 1.5855950117111206
Validation loss: 2.6574496966536327

Epoch: 64| Step: 0
Training loss: 2.9140639305114746
Validation loss: 2.655866276833319

Epoch: 6| Step: 1
Training loss: 1.9081926345825195
Validation loss: 2.654543407501713

Epoch: 6| Step: 2
Training loss: 3.4170851707458496
Validation loss: 2.6529733955219226

Epoch: 6| Step: 3
Training loss: 2.8737006187438965
Validation loss: 2.655779054087977

Epoch: 6| Step: 4
Training loss: 2.710421562194824
Validation loss: 2.655750528458626

Epoch: 6| Step: 5
Training loss: 2.4194860458374023
Validation loss: 2.6609760792024675

Epoch: 6| Step: 6
Training loss: 2.9700794219970703
Validation loss: 2.659539138117144

Epoch: 6| Step: 7
Training loss: 2.7326226234436035
Validation loss: 2.66276647967677

Epoch: 6| Step: 8
Training loss: 2.6261157989501953
Validation loss: 2.6592699635413384

Epoch: 6| Step: 9
Training loss: 3.367882251739502
Validation loss: 2.654385036037814

Epoch: 6| Step: 10
Training loss: 3.216437339782715
Validation loss: 2.6525550760248655

Epoch: 6| Step: 11
Training loss: 2.6110987663269043
Validation loss: 2.6484275658925376

Epoch: 6| Step: 12
Training loss: 2.56264328956604
Validation loss: 2.647752987441196

Epoch: 6| Step: 13
Training loss: 2.8019375801086426
Validation loss: 2.647673899127591

Epoch: 65| Step: 0
Training loss: 2.922534942626953
Validation loss: 2.6466884151581795

Epoch: 6| Step: 1
Training loss: 2.2402713298797607
Validation loss: 2.646346584443123

Epoch: 6| Step: 2
Training loss: 2.000804901123047
Validation loss: 2.648222756642167

Epoch: 6| Step: 3
Training loss: 2.3361072540283203
Validation loss: 2.6494487075395483

Epoch: 6| Step: 4
Training loss: 3.4315826892852783
Validation loss: 2.652731127636407

Epoch: 6| Step: 5
Training loss: 3.019906997680664
Validation loss: 2.6500944757974274

Epoch: 6| Step: 6
Training loss: 3.0093488693237305
Validation loss: 2.651284130670691

Epoch: 6| Step: 7
Training loss: 1.835921049118042
Validation loss: 2.6469103367097917

Epoch: 6| Step: 8
Training loss: 2.5426764488220215
Validation loss: 2.64450438304614

Epoch: 6| Step: 9
Training loss: 3.634429454803467
Validation loss: 2.6430081936620895

Epoch: 6| Step: 10
Training loss: 2.899167776107788
Validation loss: 2.642909308915497

Epoch: 6| Step: 11
Training loss: 2.9427237510681152
Validation loss: 2.645510927323372

Epoch: 6| Step: 12
Training loss: 2.8756442070007324
Validation loss: 2.6508501498929915

Epoch: 6| Step: 13
Training loss: 3.9638376235961914
Validation loss: 2.6625244771280596

Epoch: 66| Step: 0
Training loss: 2.8328304290771484
Validation loss: 2.656738555559548

Epoch: 6| Step: 1
Training loss: 2.380445957183838
Validation loss: 2.655670340343188

Epoch: 6| Step: 2
Training loss: 2.259861469268799
Validation loss: 2.652670652635636

Epoch: 6| Step: 3
Training loss: 3.488949775695801
Validation loss: 2.6515814104387836

Epoch: 6| Step: 4
Training loss: 2.6430530548095703
Validation loss: 2.6456852548865863

Epoch: 6| Step: 5
Training loss: 2.7252769470214844
Validation loss: 2.6423427289532078

Epoch: 6| Step: 6
Training loss: 3.029383420944214
Validation loss: 2.641158526943576

Epoch: 6| Step: 7
Training loss: 2.774777889251709
Validation loss: 2.6403776702060493

Epoch: 6| Step: 8
Training loss: 2.561885118484497
Validation loss: 2.6407925159700456

Epoch: 6| Step: 9
Training loss: 3.74718976020813
Validation loss: 2.637526555727887

Epoch: 6| Step: 10
Training loss: 2.060696601867676
Validation loss: 2.6377852091225247

Epoch: 6| Step: 11
Training loss: 2.8343725204467773
Validation loss: 2.6356990016916746

Epoch: 6| Step: 12
Training loss: 2.8014590740203857
Validation loss: 2.63638973236084

Epoch: 6| Step: 13
Training loss: 2.884524345397949
Validation loss: 2.6349780585176203

Epoch: 67| Step: 0
Training loss: 2.377487897872925
Validation loss: 2.6360358525347967

Epoch: 6| Step: 1
Training loss: 2.924288511276245
Validation loss: 2.634201995788082

Epoch: 6| Step: 2
Training loss: 2.8028531074523926
Validation loss: 2.6349110859696583

Epoch: 6| Step: 3
Training loss: 3.562575340270996
Validation loss: 2.636563616414224

Epoch: 6| Step: 4
Training loss: 2.7010862827301025
Validation loss: 2.6383992318184144

Epoch: 6| Step: 5
Training loss: 2.5143065452575684
Validation loss: 2.639300566847606

Epoch: 6| Step: 6
Training loss: 2.404714584350586
Validation loss: 2.641106854202927

Epoch: 6| Step: 7
Training loss: 3.368222713470459
Validation loss: 2.6382460517268025

Epoch: 6| Step: 8
Training loss: 2.83475923538208
Validation loss: 2.6389103986883677

Epoch: 6| Step: 9
Training loss: 2.0246071815490723
Validation loss: 2.638358546841529

Epoch: 6| Step: 10
Training loss: 2.26082181930542
Validation loss: 2.637199019873014

Epoch: 6| Step: 11
Training loss: 2.5493996143341064
Validation loss: 2.6343557244987896

Epoch: 6| Step: 12
Training loss: 3.2219786643981934
Validation loss: 2.6334165244974117

Epoch: 6| Step: 13
Training loss: 3.9004926681518555
Validation loss: 2.6338912261429654

Epoch: 68| Step: 0
Training loss: 2.6898255348205566
Validation loss: 2.633069489591865

Epoch: 6| Step: 1
Training loss: 1.9990994930267334
Validation loss: 2.6302751418082946

Epoch: 6| Step: 2
Training loss: 2.7900357246398926
Validation loss: 2.6320398366579445

Epoch: 6| Step: 3
Training loss: 2.942406415939331
Validation loss: 2.631568201126591

Epoch: 6| Step: 4
Training loss: 3.461060047149658
Validation loss: 2.628233032841836

Epoch: 6| Step: 5
Training loss: 3.5592827796936035
Validation loss: 2.6285578794376825

Epoch: 6| Step: 6
Training loss: 1.9785354137420654
Validation loss: 2.6277031283224783

Epoch: 6| Step: 7
Training loss: 2.4816956520080566
Validation loss: 2.628793654903289

Epoch: 6| Step: 8
Training loss: 2.5250492095947266
Validation loss: 2.6282104240950717

Epoch: 6| Step: 9
Training loss: 2.439075231552124
Validation loss: 2.6303480158569994

Epoch: 6| Step: 10
Training loss: 2.458828926086426
Validation loss: 2.634612544890373

Epoch: 6| Step: 11
Training loss: 3.2262492179870605
Validation loss: 2.6380341386282318

Epoch: 6| Step: 12
Training loss: 3.632110118865967
Validation loss: 2.632375801763227

Epoch: 6| Step: 13
Training loss: 2.710206985473633
Validation loss: 2.6258829127075853

Epoch: 69| Step: 0
Training loss: 3.1315765380859375
Validation loss: 2.625411315630841

Epoch: 6| Step: 1
Training loss: 2.1087310314178467
Validation loss: 2.6281467048070764

Epoch: 6| Step: 2
Training loss: 3.470485210418701
Validation loss: 2.6288206372209775

Epoch: 6| Step: 3
Training loss: 2.944986343383789
Validation loss: 2.6315597488034155

Epoch: 6| Step: 4
Training loss: 2.571953535079956
Validation loss: 2.632704706602199

Epoch: 6| Step: 5
Training loss: 2.8260903358459473
Validation loss: 2.6308573228056713

Epoch: 6| Step: 6
Training loss: 3.427804470062256
Validation loss: 2.631210075911655

Epoch: 6| Step: 7
Training loss: 2.576643943786621
Validation loss: 2.6358289026444957

Epoch: 6| Step: 8
Training loss: 2.600449562072754
Validation loss: 2.6316788452927784

Epoch: 6| Step: 9
Training loss: 2.1786818504333496
Validation loss: 2.6303630593002483

Epoch: 6| Step: 10
Training loss: 3.4118120670318604
Validation loss: 2.6274979293987317

Epoch: 6| Step: 11
Training loss: 2.6681904792785645
Validation loss: 2.624537919157295

Epoch: 6| Step: 12
Training loss: 2.4073286056518555
Validation loss: 2.6222590964327575

Epoch: 6| Step: 13
Training loss: 2.3394594192504883
Validation loss: 2.6208812908459733

Epoch: 70| Step: 0
Training loss: 3.1077804565429688
Validation loss: 2.620869944172521

Epoch: 6| Step: 1
Training loss: 3.449946880340576
Validation loss: 2.625658953061668

Epoch: 6| Step: 2
Training loss: 2.5697309970855713
Validation loss: 2.6298462960027877

Epoch: 6| Step: 3
Training loss: 2.143702745437622
Validation loss: 2.6343893338275213

Epoch: 6| Step: 4
Training loss: 3.242760181427002
Validation loss: 2.630852263460877

Epoch: 6| Step: 5
Training loss: 2.544684648513794
Validation loss: 2.626702695764521

Epoch: 6| Step: 6
Training loss: 2.7758030891418457
Validation loss: 2.6210137900485786

Epoch: 6| Step: 7
Training loss: 1.0263065099716187
Validation loss: 2.618549044414233

Epoch: 6| Step: 8
Training loss: 3.1802361011505127
Validation loss: 2.62040953482351

Epoch: 6| Step: 9
Training loss: 3.519845485687256
Validation loss: 2.62171830156798

Epoch: 6| Step: 10
Training loss: 3.0299930572509766
Validation loss: 2.6260902086893716

Epoch: 6| Step: 11
Training loss: 3.263606548309326
Validation loss: 2.6278916174365627

Epoch: 6| Step: 12
Training loss: 2.6745738983154297
Validation loss: 2.627009553294028

Epoch: 6| Step: 13
Training loss: 2.0063071250915527
Validation loss: 2.630829623950425

Epoch: 71| Step: 0
Training loss: 3.095099687576294
Validation loss: 2.6264840556729223

Epoch: 6| Step: 1
Training loss: 3.099494457244873
Validation loss: 2.624883310769194

Epoch: 6| Step: 2
Training loss: 2.6931755542755127
Validation loss: 2.6217847434423303

Epoch: 6| Step: 3
Training loss: 3.465212821960449
Validation loss: 2.6155700017047185

Epoch: 6| Step: 4
Training loss: 2.221780300140381
Validation loss: 2.6147726556306243

Epoch: 6| Step: 5
Training loss: 2.9185543060302734
Validation loss: 2.6122899081117366

Epoch: 6| Step: 6
Training loss: 2.359833240509033
Validation loss: 2.6114524538798998

Epoch: 6| Step: 7
Training loss: 2.749026298522949
Validation loss: 2.61636507895685

Epoch: 6| Step: 8
Training loss: 3.2132482528686523
Validation loss: 2.61899394630104

Epoch: 6| Step: 9
Training loss: 2.1303176879882812
Validation loss: 2.6276056023054224

Epoch: 6| Step: 10
Training loss: 3.3463594913482666
Validation loss: 2.6390128674045688

Epoch: 6| Step: 11
Training loss: 2.3711252212524414
Validation loss: 2.6418752260105585

Epoch: 6| Step: 12
Training loss: 2.071336030960083
Validation loss: 2.6492501407541256

Epoch: 6| Step: 13
Training loss: 3.4144952297210693
Validation loss: 2.634051699792185

Epoch: 72| Step: 0
Training loss: 2.925980806350708
Validation loss: 2.6759909788767495

Epoch: 6| Step: 1
Training loss: 3.1830544471740723
Validation loss: 2.671673510664253

Epoch: 6| Step: 2
Training loss: 2.746962785720825
Validation loss: 2.6767452327154015

Epoch: 6| Step: 3
Training loss: 3.2947568893432617
Validation loss: 2.6704483083499375

Epoch: 6| Step: 4
Training loss: 2.3669166564941406
Validation loss: 2.644066684989519

Epoch: 6| Step: 5
Training loss: 3.5124576091766357
Validation loss: 2.629046642652122

Epoch: 6| Step: 6
Training loss: 1.8958876132965088
Validation loss: 2.6167852827297744

Epoch: 6| Step: 7
Training loss: 2.1807727813720703
Validation loss: 2.6160750927463656

Epoch: 6| Step: 8
Training loss: 2.4423582553863525
Validation loss: 2.6260984969395462

Epoch: 6| Step: 9
Training loss: 2.851731300354004
Validation loss: 2.646787484486898

Epoch: 6| Step: 10
Training loss: 3.3891124725341797
Validation loss: 2.648789921114522

Epoch: 6| Step: 11
Training loss: 2.4145073890686035
Validation loss: 2.621044389663204

Epoch: 6| Step: 12
Training loss: 2.892486572265625
Validation loss: 2.6205014926131054

Epoch: 6| Step: 13
Training loss: 2.872591257095337
Validation loss: 2.6172468482807116

Epoch: 73| Step: 0
Training loss: 2.59031343460083
Validation loss: 2.627853108990577

Epoch: 6| Step: 1
Training loss: 2.958101511001587
Validation loss: 2.6578430693636657

Epoch: 6| Step: 2
Training loss: 2.2835965156555176
Validation loss: 2.6926802127592024

Epoch: 6| Step: 3
Training loss: 2.8597655296325684
Validation loss: 2.724179244810535

Epoch: 6| Step: 4
Training loss: 3.6628522872924805
Validation loss: 2.713157764045141

Epoch: 6| Step: 5
Training loss: 3.0812442302703857
Validation loss: 2.7003087971800115

Epoch: 6| Step: 6
Training loss: 2.449535369873047
Validation loss: 2.6601268655510357

Epoch: 6| Step: 7
Training loss: 3.4454877376556396
Validation loss: 2.6225908879310853

Epoch: 6| Step: 8
Training loss: 2.778256416320801
Validation loss: 2.6069372341197026

Epoch: 6| Step: 9
Training loss: 2.5345613956451416
Validation loss: 2.607633839371384

Epoch: 6| Step: 10
Training loss: 3.212172746658325
Validation loss: 2.60529722193236

Epoch: 6| Step: 11
Training loss: 2.7908711433410645
Validation loss: 2.608502859710365

Epoch: 6| Step: 12
Training loss: 1.9456182718276978
Validation loss: 2.6106056449233845

Epoch: 6| Step: 13
Training loss: 2.121692657470703
Validation loss: 2.6180425715702835

Epoch: 74| Step: 0
Training loss: 2.840909481048584
Validation loss: 2.609117348988851

Epoch: 6| Step: 1
Training loss: 2.1255993843078613
Validation loss: 2.6108905243617233

Epoch: 6| Step: 2
Training loss: 2.923356294631958
Validation loss: 2.6112966537475586

Epoch: 6| Step: 3
Training loss: 2.3762047290802
Validation loss: 2.6158682607835337

Epoch: 6| Step: 4
Training loss: 2.4646010398864746
Validation loss: 2.617115318134267

Epoch: 6| Step: 5
Training loss: 3.2681524753570557
Validation loss: 2.611845642007807

Epoch: 6| Step: 6
Training loss: 3.28450083732605
Validation loss: 2.605685354560934

Epoch: 6| Step: 7
Training loss: 1.7020313739776611
Validation loss: 2.6039930210318616

Epoch: 6| Step: 8
Training loss: 3.4849798679351807
Validation loss: 2.600043099413636

Epoch: 6| Step: 9
Training loss: 3.0726265907287598
Validation loss: 2.5978568625706497

Epoch: 6| Step: 10
Training loss: 2.9504141807556152
Validation loss: 2.6013069434832503

Epoch: 6| Step: 11
Training loss: 3.322819471359253
Validation loss: 2.606308837090769

Epoch: 6| Step: 12
Training loss: 2.2704014778137207
Validation loss: 2.607608210655951

Epoch: 6| Step: 13
Training loss: 2.564833641052246
Validation loss: 2.6065338632111907

Epoch: 75| Step: 0
Training loss: 3.4698517322540283
Validation loss: 2.62367162909559

Epoch: 6| Step: 1
Training loss: 3.4721689224243164
Validation loss: 2.649037763636599

Epoch: 6| Step: 2
Training loss: 3.0451602935791016
Validation loss: 2.6616543518599642

Epoch: 6| Step: 3
Training loss: 1.620851755142212
Validation loss: 2.6595702171325684

Epoch: 6| Step: 4
Training loss: 3.0782628059387207
Validation loss: 2.635266496289161

Epoch: 6| Step: 5
Training loss: 2.628915309906006
Validation loss: 2.6171207940706642

Epoch: 6| Step: 6
Training loss: 2.492891788482666
Validation loss: 2.6096852646079114

Epoch: 6| Step: 7
Training loss: 2.8975908756256104
Validation loss: 2.6042364925466557

Epoch: 6| Step: 8
Training loss: 2.533376455307007
Validation loss: 2.5946939247910694

Epoch: 6| Step: 9
Training loss: 2.4262309074401855
Validation loss: 2.590857687816825

Epoch: 6| Step: 10
Training loss: 2.284944534301758
Validation loss: 2.598126165328487

Epoch: 6| Step: 11
Training loss: 2.149364709854126
Validation loss: 2.59934563021506

Epoch: 6| Step: 12
Training loss: 3.4447150230407715
Validation loss: 2.5985180408723894

Epoch: 6| Step: 13
Training loss: 3.520693063735962
Validation loss: 2.5993733406066895

Epoch: 76| Step: 0
Training loss: 3.729722261428833
Validation loss: 2.5990685032260035

Epoch: 6| Step: 1
Training loss: 2.726740837097168
Validation loss: 2.600149498190931

Epoch: 6| Step: 2
Training loss: 2.2659358978271484
Validation loss: 2.6038906881886144

Epoch: 6| Step: 3
Training loss: 3.7619409561157227
Validation loss: 2.6064622222736316

Epoch: 6| Step: 4
Training loss: 3.3906025886535645
Validation loss: 2.6200331718690935

Epoch: 6| Step: 5
Training loss: 2.9201526641845703
Validation loss: 2.5992513138760804

Epoch: 6| Step: 6
Training loss: 2.5323691368103027
Validation loss: 2.5915099510582547

Epoch: 6| Step: 7
Training loss: 2.512836217880249
Validation loss: 2.591532420086604

Epoch: 6| Step: 8
Training loss: 2.650050163269043
Validation loss: 2.5872156338025163

Epoch: 6| Step: 9
Training loss: 1.8556503057479858
Validation loss: 2.5858634774402907

Epoch: 6| Step: 10
Training loss: 2.6787221431732178
Validation loss: 2.584412838823052

Epoch: 6| Step: 11
Training loss: 2.6805405616760254
Validation loss: 2.5880214065633793

Epoch: 6| Step: 12
Training loss: 2.3495829105377197
Validation loss: 2.598257477565478

Epoch: 6| Step: 13
Training loss: 2.564608573913574
Validation loss: 2.60468553984037

Epoch: 77| Step: 0
Training loss: 2.912588596343994
Validation loss: 2.614029786920035

Epoch: 6| Step: 1
Training loss: 2.6845178604125977
Validation loss: 2.6195545452897266

Epoch: 6| Step: 2
Training loss: 2.9709274768829346
Validation loss: 2.6156026445409304

Epoch: 6| Step: 3
Training loss: 2.3535046577453613
Validation loss: 2.6113899959030973

Epoch: 6| Step: 4
Training loss: 2.161811590194702
Validation loss: 2.5994201270482873

Epoch: 6| Step: 5
Training loss: 2.695268392562866
Validation loss: 2.5959547437647337

Epoch: 6| Step: 6
Training loss: 3.584153175354004
Validation loss: 2.5933239485627864

Epoch: 6| Step: 7
Training loss: 2.4094338417053223
Validation loss: 2.5972381612306

Epoch: 6| Step: 8
Training loss: 3.0070576667785645
Validation loss: 2.6045158396485033

Epoch: 6| Step: 9
Training loss: 3.7583377361297607
Validation loss: 2.5940795970219437

Epoch: 6| Step: 10
Training loss: 2.6620960235595703
Validation loss: 2.5947546087285525

Epoch: 6| Step: 11
Training loss: 2.407545804977417
Validation loss: 2.591472694950719

Epoch: 6| Step: 12
Training loss: 2.5577034950256348
Validation loss: 2.5887769601678334

Epoch: 6| Step: 13
Training loss: 2.311894178390503
Validation loss: 2.586138379189276

Epoch: 78| Step: 0
Training loss: 1.7611839771270752
Validation loss: 2.589622671886157

Epoch: 6| Step: 1
Training loss: 2.6722402572631836
Validation loss: 2.5875484635753017

Epoch: 6| Step: 2
Training loss: 3.0150668621063232
Validation loss: 2.597498655319214

Epoch: 6| Step: 3
Training loss: 2.910254955291748
Validation loss: 2.603236681671553

Epoch: 6| Step: 4
Training loss: 2.084838628768921
Validation loss: 2.5943691730499268

Epoch: 6| Step: 5
Training loss: 2.731259346008301
Validation loss: 2.5852245976847987

Epoch: 6| Step: 6
Training loss: 2.789313554763794
Validation loss: 2.583956454389839

Epoch: 6| Step: 7
Training loss: 3.0803170204162598
Validation loss: 2.580632822487944

Epoch: 6| Step: 8
Training loss: 2.5767030715942383
Validation loss: 2.5827305214379424

Epoch: 6| Step: 9
Training loss: 2.1216671466827393
Validation loss: 2.580825246790404

Epoch: 6| Step: 10
Training loss: 2.1435489654541016
Validation loss: 2.5796804453736994

Epoch: 6| Step: 11
Training loss: 3.17543888092041
Validation loss: 2.5826100098189486

Epoch: 6| Step: 12
Training loss: 4.3595075607299805
Validation loss: 2.5902259247277373

Epoch: 6| Step: 13
Training loss: 3.2394940853118896
Validation loss: 2.5792413244965258

Epoch: 79| Step: 0
Training loss: 2.309305191040039
Validation loss: 2.5753638667445027

Epoch: 6| Step: 1
Training loss: 2.910662889480591
Validation loss: 2.5743963641505085

Epoch: 6| Step: 2
Training loss: 2.8545684814453125
Validation loss: 2.570991393058531

Epoch: 6| Step: 3
Training loss: 2.7816672325134277
Validation loss: 2.57074406326458

Epoch: 6| Step: 4
Training loss: 3.2321078777313232
Validation loss: 2.5701199885337584

Epoch: 6| Step: 5
Training loss: 3.3707993030548096
Validation loss: 2.573694449599071

Epoch: 6| Step: 6
Training loss: 3.029921293258667
Validation loss: 2.573815284236785

Epoch: 6| Step: 7
Training loss: 2.9602887630462646
Validation loss: 2.57389404953167

Epoch: 6| Step: 8
Training loss: 2.872837543487549
Validation loss: 2.5771360935703402

Epoch: 6| Step: 9
Training loss: 2.81286358833313
Validation loss: 2.568959733491303

Epoch: 6| Step: 10
Training loss: 2.2780919075012207
Validation loss: 2.5685140420031805

Epoch: 6| Step: 11
Training loss: 2.293696403503418
Validation loss: 2.568895116929085

Epoch: 6| Step: 12
Training loss: 2.0266361236572266
Validation loss: 2.5669280534149497

Epoch: 6| Step: 13
Training loss: 2.6542301177978516
Validation loss: 2.5690225965233258

Epoch: 80| Step: 0
Training loss: 3.387476921081543
Validation loss: 2.567202388599355

Epoch: 6| Step: 1
Training loss: 2.8367929458618164
Validation loss: 2.5655298438123477

Epoch: 6| Step: 2
Training loss: 2.6608967781066895
Validation loss: 2.56654622990598

Epoch: 6| Step: 3
Training loss: 2.5288963317871094
Validation loss: 2.5658434078257573

Epoch: 6| Step: 4
Training loss: 1.185138463973999
Validation loss: 2.5648153161489837

Epoch: 6| Step: 5
Training loss: 2.7663638591766357
Validation loss: 2.5646229354284142

Epoch: 6| Step: 6
Training loss: 3.0097763538360596
Validation loss: 2.5672617573891916

Epoch: 6| Step: 7
Training loss: 2.7385144233703613
Validation loss: 2.566179329349149

Epoch: 6| Step: 8
Training loss: 2.593742847442627
Validation loss: 2.57132811187416

Epoch: 6| Step: 9
Training loss: 2.7169578075408936
Validation loss: 2.56705306935054

Epoch: 6| Step: 10
Training loss: 2.897966146469116
Validation loss: 2.5670667797006588

Epoch: 6| Step: 11
Training loss: 3.089601993560791
Validation loss: 2.5691649401059715

Epoch: 6| Step: 12
Training loss: 3.407072067260742
Validation loss: 2.5631466168229298

Epoch: 6| Step: 13
Training loss: 2.3941378593444824
Validation loss: 2.56282534650577

Epoch: 81| Step: 0
Training loss: 2.511443614959717
Validation loss: 2.559924030816683

Epoch: 6| Step: 1
Training loss: 2.6083860397338867
Validation loss: 2.559299281848374

Epoch: 6| Step: 2
Training loss: 3.633453607559204
Validation loss: 2.5616501480020504

Epoch: 6| Step: 3
Training loss: 3.0087599754333496
Validation loss: 2.5588020509289158

Epoch: 6| Step: 4
Training loss: 1.7174856662750244
Validation loss: 2.559005691159156

Epoch: 6| Step: 5
Training loss: 3.4818058013916016
Validation loss: 2.557412819195819

Epoch: 6| Step: 6
Training loss: 2.300185203552246
Validation loss: 2.558585548913607

Epoch: 6| Step: 7
Training loss: 2.713604211807251
Validation loss: 2.5574938199853383

Epoch: 6| Step: 8
Training loss: 3.178476095199585
Validation loss: 2.5583975161275556

Epoch: 6| Step: 9
Training loss: 3.2167396545410156
Validation loss: 2.558741013209025

Epoch: 6| Step: 10
Training loss: 2.503329038619995
Validation loss: 2.559440520501906

Epoch: 6| Step: 11
Training loss: 2.357149124145508
Validation loss: 2.55923730070873

Epoch: 6| Step: 12
Training loss: 2.1323206424713135
Validation loss: 2.5687304773638324

Epoch: 6| Step: 13
Training loss: 2.9447898864746094
Validation loss: 2.5606161086790022

Epoch: 82| Step: 0
Training loss: 3.486539125442505
Validation loss: 2.5570488719530005

Epoch: 6| Step: 1
Training loss: 2.2234439849853516
Validation loss: 2.5569058413146646

Epoch: 6| Step: 2
Training loss: 2.57595157623291
Validation loss: 2.554041870178715

Epoch: 6| Step: 3
Training loss: 2.2556915283203125
Validation loss: 2.5559969256001134

Epoch: 6| Step: 4
Training loss: 3.261446237564087
Validation loss: 2.5569257300387145

Epoch: 6| Step: 5
Training loss: 2.4943172931671143
Validation loss: 2.557956354592436

Epoch: 6| Step: 6
Training loss: 2.081432342529297
Validation loss: 2.5575734928090084

Epoch: 6| Step: 7
Training loss: 3.266019105911255
Validation loss: 2.5530387791254188

Epoch: 6| Step: 8
Training loss: 2.4133923053741455
Validation loss: 2.55215694058326

Epoch: 6| Step: 9
Training loss: 3.492431640625
Validation loss: 2.5507840674410582

Epoch: 6| Step: 10
Training loss: 2.4587883949279785
Validation loss: 2.550432438491493

Epoch: 6| Step: 11
Training loss: 3.461541175842285
Validation loss: 2.5482581943594

Epoch: 6| Step: 12
Training loss: 2.6121609210968018
Validation loss: 2.5456759698929323

Epoch: 6| Step: 13
Training loss: 1.679738163948059
Validation loss: 2.5459925231113227

Epoch: 83| Step: 0
Training loss: 2.74951171875
Validation loss: 2.5436575976751183

Epoch: 6| Step: 1
Training loss: 2.3136963844299316
Validation loss: 2.5441083292807303

Epoch: 6| Step: 2
Training loss: 2.5073370933532715
Validation loss: 2.5437466508598736

Epoch: 6| Step: 3
Training loss: 3.8493897914886475
Validation loss: 2.54050959310224

Epoch: 6| Step: 4
Training loss: 2.8438780307769775
Validation loss: 2.5388635230320755

Epoch: 6| Step: 5
Training loss: 3.0453455448150635
Validation loss: 2.537205926833614

Epoch: 6| Step: 6
Training loss: 2.234194278717041
Validation loss: 2.535067001978556

Epoch: 6| Step: 7
Training loss: 2.0050759315490723
Validation loss: 2.534926563180903

Epoch: 6| Step: 8
Training loss: 2.803044319152832
Validation loss: 2.537799366058842

Epoch: 6| Step: 9
Training loss: 2.9652628898620605
Validation loss: 2.531477648724792

Epoch: 6| Step: 10
Training loss: 3.1428730487823486
Validation loss: 2.530899375997564

Epoch: 6| Step: 11
Training loss: 2.5799777507781982
Validation loss: 2.5303515106119137

Epoch: 6| Step: 12
Training loss: 2.309450626373291
Validation loss: 2.530149777730306

Epoch: 6| Step: 13
Training loss: 2.5591015815734863
Validation loss: 2.534104106246784

Epoch: 84| Step: 0
Training loss: 2.382749557495117
Validation loss: 2.5338759165938183

Epoch: 6| Step: 1
Training loss: 3.0874297618865967
Validation loss: 2.5335080085262174

Epoch: 6| Step: 2
Training loss: 2.5656328201293945
Validation loss: 2.5325065838393344

Epoch: 6| Step: 3
Training loss: 2.642340898513794
Validation loss: 2.5307021166688655

Epoch: 6| Step: 4
Training loss: 3.1210012435913086
Validation loss: 2.5309983812352663

Epoch: 6| Step: 5
Training loss: 2.1652379035949707
Validation loss: 2.5267305707418792

Epoch: 6| Step: 6
Training loss: 2.426443576812744
Validation loss: 2.5297227110914005

Epoch: 6| Step: 7
Training loss: 2.383575677871704
Validation loss: 2.531138571359778

Epoch: 6| Step: 8
Training loss: 3.1008691787719727
Validation loss: 2.531421535758562

Epoch: 6| Step: 9
Training loss: 2.938683271408081
Validation loss: 2.5301616602046515

Epoch: 6| Step: 10
Training loss: 2.699418067932129
Validation loss: 2.534456577352298

Epoch: 6| Step: 11
Training loss: 3.293639898300171
Validation loss: 2.533186461335869

Epoch: 6| Step: 12
Training loss: 2.185786247253418
Validation loss: 2.534228442817606

Epoch: 6| Step: 13
Training loss: 3.192213535308838
Validation loss: 2.5348611903446976

Epoch: 85| Step: 0
Training loss: 2.1034791469573975
Validation loss: 2.530889977690994

Epoch: 6| Step: 1
Training loss: 2.5042104721069336
Validation loss: 2.5295107185199694

Epoch: 6| Step: 2
Training loss: 1.8706159591674805
Validation loss: 2.5280146983362015

Epoch: 6| Step: 3
Training loss: 3.0900650024414062
Validation loss: 2.5229589426389305

Epoch: 6| Step: 4
Training loss: 3.359875202178955
Validation loss: 2.5230070083372054

Epoch: 6| Step: 5
Training loss: 2.6191534996032715
Validation loss: 2.5231255408256286

Epoch: 6| Step: 6
Training loss: 3.1827120780944824
Validation loss: 2.523310153715072

Epoch: 6| Step: 7
Training loss: 3.4742257595062256
Validation loss: 2.5248382450431905

Epoch: 6| Step: 8
Training loss: 2.752615451812744
Validation loss: 2.525413118382936

Epoch: 6| Step: 9
Training loss: 2.6126480102539062
Validation loss: 2.5238637488375426

Epoch: 6| Step: 10
Training loss: 2.5841071605682373
Validation loss: 2.5217339402885846

Epoch: 6| Step: 11
Training loss: 1.9147130250930786
Validation loss: 2.523065310652538

Epoch: 6| Step: 12
Training loss: 2.6494431495666504
Validation loss: 2.520992919962893

Epoch: 6| Step: 13
Training loss: 3.605727195739746
Validation loss: 2.522740771693568

Epoch: 86| Step: 0
Training loss: 2.6431736946105957
Validation loss: 2.529212431241107

Epoch: 6| Step: 1
Training loss: 2.74965763092041
Validation loss: 2.5280338692408737

Epoch: 6| Step: 2
Training loss: 3.8694403171539307
Validation loss: 2.528063230617072

Epoch: 6| Step: 3
Training loss: 2.36686372756958
Validation loss: 2.5276005396278958

Epoch: 6| Step: 4
Training loss: 2.5412347316741943
Validation loss: 2.5268336137135825

Epoch: 6| Step: 5
Training loss: 2.676914930343628
Validation loss: 2.528803330595775

Epoch: 6| Step: 6
Training loss: 2.5963287353515625
Validation loss: 2.5219315867270193

Epoch: 6| Step: 7
Training loss: 1.9535131454467773
Validation loss: 2.519591603227841

Epoch: 6| Step: 8
Training loss: 3.2818450927734375
Validation loss: 2.5150130294984385

Epoch: 6| Step: 9
Training loss: 3.2804200649261475
Validation loss: 2.5174646377563477

Epoch: 6| Step: 10
Training loss: 1.9700210094451904
Validation loss: 2.518812430802212

Epoch: 6| Step: 11
Training loss: 3.079749584197998
Validation loss: 2.5191595913261495

Epoch: 6| Step: 12
Training loss: 2.567707061767578
Validation loss: 2.5161467623966995

Epoch: 6| Step: 13
Training loss: 2.0568928718566895
Validation loss: 2.5180851592812488

Epoch: 87| Step: 0
Training loss: 2.88533878326416
Validation loss: 2.5152159685729654

Epoch: 6| Step: 1
Training loss: 2.562922477722168
Validation loss: 2.5158682279689337

Epoch: 6| Step: 2
Training loss: 2.7852861881256104
Validation loss: 2.51643455156716

Epoch: 6| Step: 3
Training loss: 2.3009285926818848
Validation loss: 2.518003620127196

Epoch: 6| Step: 4
Training loss: 3.284433364868164
Validation loss: 2.516346275165517

Epoch: 6| Step: 5
Training loss: 2.731231689453125
Validation loss: 2.51562927615258

Epoch: 6| Step: 6
Training loss: 2.48982572555542
Validation loss: 2.518229515321793

Epoch: 6| Step: 7
Training loss: 2.5840625762939453
Validation loss: 2.519893677003922

Epoch: 6| Step: 8
Training loss: 2.9581661224365234
Validation loss: 2.520790102661297

Epoch: 6| Step: 9
Training loss: 2.201012372970581
Validation loss: 2.521174725665841

Epoch: 6| Step: 10
Training loss: 2.22287917137146
Validation loss: 2.5198660781306605

Epoch: 6| Step: 11
Training loss: 2.421156644821167
Validation loss: 2.5280898412068686

Epoch: 6| Step: 12
Training loss: 3.5653915405273438
Validation loss: 2.5282381093630226

Epoch: 6| Step: 13
Training loss: 2.972473621368408
Validation loss: 2.5235970174112627

Epoch: 88| Step: 0
Training loss: 2.129701614379883
Validation loss: 2.5213947014142106

Epoch: 6| Step: 1
Training loss: 1.9115099906921387
Validation loss: 2.516362897811397

Epoch: 6| Step: 2
Training loss: 2.7336461544036865
Validation loss: 2.516390133929509

Epoch: 6| Step: 3
Training loss: 2.8895363807678223
Validation loss: 2.5169428445959605

Epoch: 6| Step: 4
Training loss: 3.317715644836426
Validation loss: 2.5180945704060216

Epoch: 6| Step: 5
Training loss: 3.0057737827301025
Validation loss: 2.516548800212081

Epoch: 6| Step: 6
Training loss: 2.9874143600463867
Validation loss: 2.5171205279647664

Epoch: 6| Step: 7
Training loss: 2.477747917175293
Validation loss: 2.517693001736877

Epoch: 6| Step: 8
Training loss: 2.3378689289093018
Validation loss: 2.5119953719518517

Epoch: 6| Step: 9
Training loss: 3.1444363594055176
Validation loss: 2.513001431701004

Epoch: 6| Step: 10
Training loss: 2.5240426063537598
Validation loss: 2.513955036799113

Epoch: 6| Step: 11
Training loss: 3.6060497760772705
Validation loss: 2.5132598928225938

Epoch: 6| Step: 12
Training loss: 2.465097427368164
Validation loss: 2.5197645207887054

Epoch: 6| Step: 13
Training loss: 1.970306396484375
Validation loss: 2.5184304457838818

Epoch: 89| Step: 0
Training loss: 3.5543150901794434
Validation loss: 2.51966493873186

Epoch: 6| Step: 1
Training loss: 2.5606279373168945
Validation loss: 2.5198837557146625

Epoch: 6| Step: 2
Training loss: 2.755096435546875
Validation loss: 2.520101531859367

Epoch: 6| Step: 3
Training loss: 2.519984245300293
Validation loss: 2.516421333436043

Epoch: 6| Step: 4
Training loss: 3.45746111869812
Validation loss: 2.515319847291516

Epoch: 6| Step: 5
Training loss: 2.708540678024292
Validation loss: 2.513413344660113

Epoch: 6| Step: 6
Training loss: 2.5214684009552
Validation loss: 2.5059697089656705

Epoch: 6| Step: 7
Training loss: 3.010767936706543
Validation loss: 2.5081153249227874

Epoch: 6| Step: 8
Training loss: 3.022838830947876
Validation loss: 2.509304872123144

Epoch: 6| Step: 9
Training loss: 3.219085693359375
Validation loss: 2.519156663648544

Epoch: 6| Step: 10
Training loss: 2.0017757415771484
Validation loss: 2.5222660982480614

Epoch: 6| Step: 11
Training loss: 2.570882797241211
Validation loss: 2.5398835469317693

Epoch: 6| Step: 12
Training loss: 1.678614616394043
Validation loss: 2.5464795404864895

Epoch: 6| Step: 13
Training loss: 2.057725191116333
Validation loss: 2.5435329252673733

Epoch: 90| Step: 0
Training loss: 3.356379747390747
Validation loss: 2.5273162664905673

Epoch: 6| Step: 1
Training loss: 2.9326975345611572
Validation loss: 2.5129304598736506

Epoch: 6| Step: 2
Training loss: 2.4581384658813477
Validation loss: 2.5047222106687483

Epoch: 6| Step: 3
Training loss: 2.413029432296753
Validation loss: 2.5076727918399278

Epoch: 6| Step: 4
Training loss: 2.602421760559082
Validation loss: 2.5128479465361564

Epoch: 6| Step: 5
Training loss: 2.671727180480957
Validation loss: 2.5178848581929363

Epoch: 6| Step: 6
Training loss: 2.533848285675049
Validation loss: 2.5227703509792203

Epoch: 6| Step: 7
Training loss: 2.345348834991455
Validation loss: 2.5218825852999123

Epoch: 6| Step: 8
Training loss: 3.4092907905578613
Validation loss: 2.5237818328283166

Epoch: 6| Step: 9
Training loss: 2.0637712478637695
Validation loss: 2.513997777815788

Epoch: 6| Step: 10
Training loss: 2.4565882682800293
Validation loss: 2.5116708458110852

Epoch: 6| Step: 11
Training loss: 2.7953529357910156
Validation loss: 2.506690543184998

Epoch: 6| Step: 12
Training loss: 3.2116732597351074
Validation loss: 2.5095103427927983

Epoch: 6| Step: 13
Training loss: 2.426300525665283
Validation loss: 2.5154467449393323

Epoch: 91| Step: 0
Training loss: 3.2339816093444824
Validation loss: 2.512626655640141

Epoch: 6| Step: 1
Training loss: 2.8453049659729004
Validation loss: 2.5166135859745804

Epoch: 6| Step: 2
Training loss: 2.4176361560821533
Validation loss: 2.5309827789183585

Epoch: 6| Step: 3
Training loss: 2.5014119148254395
Validation loss: 2.5382537047068277

Epoch: 6| Step: 4
Training loss: 2.4319560527801514
Validation loss: 2.5318891822650866

Epoch: 6| Step: 5
Training loss: 3.0350394248962402
Validation loss: 2.5264797031238513

Epoch: 6| Step: 6
Training loss: 2.53312349319458
Validation loss: 2.5164444574745755

Epoch: 6| Step: 7
Training loss: 3.4134469032287598
Validation loss: 2.506500872232581

Epoch: 6| Step: 8
Training loss: 2.26131534576416
Validation loss: 2.502840195932696

Epoch: 6| Step: 9
Training loss: 2.71885347366333
Validation loss: 2.500184525725662

Epoch: 6| Step: 10
Training loss: 1.876924991607666
Validation loss: 2.5053716167326896

Epoch: 6| Step: 11
Training loss: 2.9814579486846924
Validation loss: 2.5061852880703506

Epoch: 6| Step: 12
Training loss: 2.659060001373291
Validation loss: 2.5095926228389946

Epoch: 6| Step: 13
Training loss: 2.725531578063965
Validation loss: 2.5149966260438323

Epoch: 92| Step: 0
Training loss: 2.6558027267456055
Validation loss: 2.523368268884638

Epoch: 6| Step: 1
Training loss: 3.046359062194824
Validation loss: 2.5392697267634894

Epoch: 6| Step: 2
Training loss: 3.023183584213257
Validation loss: 2.527534975800463

Epoch: 6| Step: 3
Training loss: 3.1205272674560547
Validation loss: 2.5153761986763246

Epoch: 6| Step: 4
Training loss: 2.437216281890869
Validation loss: 2.5069076450922156

Epoch: 6| Step: 5
Training loss: 3.3911123275756836
Validation loss: 2.5008541922415457

Epoch: 6| Step: 6
Training loss: 2.6603260040283203
Validation loss: 2.497831190786054

Epoch: 6| Step: 7
Training loss: 2.33862042427063
Validation loss: 2.4959487145946873

Epoch: 6| Step: 8
Training loss: 2.4514622688293457
Validation loss: 2.4949076175689697

Epoch: 6| Step: 9
Training loss: 2.753763198852539
Validation loss: 2.5022518250250045

Epoch: 6| Step: 10
Training loss: 1.798081636428833
Validation loss: 2.5043382772835354

Epoch: 6| Step: 11
Training loss: 4.059914588928223
Validation loss: 2.50948767764594

Epoch: 6| Step: 12
Training loss: 1.9158263206481934
Validation loss: 2.5110518060704714

Epoch: 6| Step: 13
Training loss: 1.8387805223464966
Validation loss: 2.509088154762022

Epoch: 93| Step: 0
Training loss: 2.165334701538086
Validation loss: 2.5033204965693976

Epoch: 6| Step: 1
Training loss: 1.9896240234375
Validation loss: 2.4992456231065976

Epoch: 6| Step: 2
Training loss: 2.026472806930542
Validation loss: 2.4960612891822733

Epoch: 6| Step: 3
Training loss: 3.089026927947998
Validation loss: 2.4954351532843804

Epoch: 6| Step: 4
Training loss: 2.471898317337036
Validation loss: 2.4903485954448743

Epoch: 6| Step: 5
Training loss: 2.7811410427093506
Validation loss: 2.493171857249352

Epoch: 6| Step: 6
Training loss: 2.366203546524048
Validation loss: 2.4955233015039915

Epoch: 6| Step: 7
Training loss: 3.6911325454711914
Validation loss: 2.494821697153071

Epoch: 6| Step: 8
Training loss: 2.89174222946167
Validation loss: 2.4950337486882366

Epoch: 6| Step: 9
Training loss: 3.442795753479004
Validation loss: 2.4991637199155745

Epoch: 6| Step: 10
Training loss: 2.286224842071533
Validation loss: 2.502317274770429

Epoch: 6| Step: 11
Training loss: 2.687919855117798
Validation loss: 2.4969012045091197

Epoch: 6| Step: 12
Training loss: 2.637261390686035
Validation loss: 2.4938902342191307

Epoch: 6| Step: 13
Training loss: 3.4241857528686523
Validation loss: 2.494384857916063

Epoch: 94| Step: 0
Training loss: 2.9408302307128906
Validation loss: 2.4929536824585288

Epoch: 6| Step: 1
Training loss: 2.5445613861083984
Validation loss: 2.492562240169894

Epoch: 6| Step: 2
Training loss: 2.894319534301758
Validation loss: 2.4918994865109845

Epoch: 6| Step: 3
Training loss: 2.773136615753174
Validation loss: 2.4937611651676956

Epoch: 6| Step: 4
Training loss: 3.0686209201812744
Validation loss: 2.4937143966715825

Epoch: 6| Step: 5
Training loss: 2.6815645694732666
Validation loss: 2.494682178702406

Epoch: 6| Step: 6
Training loss: 2.3303897380828857
Validation loss: 2.5004070292236986

Epoch: 6| Step: 7
Training loss: 1.6323840618133545
Validation loss: 2.4990827498897428

Epoch: 6| Step: 8
Training loss: 1.7991514205932617
Validation loss: 2.498901113387077

Epoch: 6| Step: 9
Training loss: 2.4934325218200684
Validation loss: 2.4967404616776334

Epoch: 6| Step: 10
Training loss: 3.0263123512268066
Validation loss: 2.4954171565271195

Epoch: 6| Step: 11
Training loss: 3.1425986289978027
Validation loss: 2.5008388539796234

Epoch: 6| Step: 12
Training loss: 2.8118739128112793
Validation loss: 2.490052900006694

Epoch: 6| Step: 13
Training loss: 4.026094436645508
Validation loss: 2.494717126251549

Epoch: 95| Step: 0
Training loss: 2.970487117767334
Validation loss: 2.4882714671473347

Epoch: 6| Step: 1
Training loss: 1.7380011081695557
Validation loss: 2.489269087391515

Epoch: 6| Step: 2
Training loss: 2.452003002166748
Validation loss: 2.486068692258609

Epoch: 6| Step: 3
Training loss: 1.7353214025497437
Validation loss: 2.48810867340334

Epoch: 6| Step: 4
Training loss: 2.6223771572113037
Validation loss: 2.496720549880817

Epoch: 6| Step: 5
Training loss: 3.237065315246582
Validation loss: 2.49957872462529

Epoch: 6| Step: 6
Training loss: 2.7179548740386963
Validation loss: 2.5057576510214035

Epoch: 6| Step: 7
Training loss: 2.772256374359131
Validation loss: 2.52763283124534

Epoch: 6| Step: 8
Training loss: 2.5331976413726807
Validation loss: 2.5408931137413107

Epoch: 6| Step: 9
Training loss: 3.278740882873535
Validation loss: 2.5353672171151764

Epoch: 6| Step: 10
Training loss: 3.1563720703125
Validation loss: 2.527552940512216

Epoch: 6| Step: 11
Training loss: 2.2297134399414062
Validation loss: 2.5067589821354037

Epoch: 6| Step: 12
Training loss: 3.5323612689971924
Validation loss: 2.4856195090919413

Epoch: 6| Step: 13
Training loss: 2.693706512451172
Validation loss: 2.4842815245351484

Epoch: 96| Step: 0
Training loss: 1.6748003959655762
Validation loss: 2.4806456053128807

Epoch: 6| Step: 1
Training loss: 2.755403995513916
Validation loss: 2.479558583228819

Epoch: 6| Step: 2
Training loss: 2.9264163970947266
Validation loss: 2.4845880590459353

Epoch: 6| Step: 3
Training loss: 3.2537736892700195
Validation loss: 2.49133534713458

Epoch: 6| Step: 4
Training loss: 2.5529890060424805
Validation loss: 2.496033050680673

Epoch: 6| Step: 5
Training loss: 3.3434486389160156
Validation loss: 2.5263747092216247

Epoch: 6| Step: 6
Training loss: 2.685784101486206
Validation loss: 2.5624185633915726

Epoch: 6| Step: 7
Training loss: 2.830453634262085
Validation loss: 2.5804153642346783

Epoch: 6| Step: 8
Training loss: 3.169851303100586
Validation loss: 2.580387567961088

Epoch: 6| Step: 9
Training loss: 1.5193145275115967
Validation loss: 2.582796489038775

Epoch: 6| Step: 10
Training loss: 2.740666389465332
Validation loss: 2.5740063639097315

Epoch: 6| Step: 11
Training loss: 2.979857921600342
Validation loss: 2.5550683006163566

Epoch: 6| Step: 12
Training loss: 2.97637939453125
Validation loss: 2.5432998211153093

Epoch: 6| Step: 13
Training loss: 2.622249126434326
Validation loss: 2.534219272675053

Epoch: 97| Step: 0
Training loss: 2.0834860801696777
Validation loss: 2.536914199911138

Epoch: 6| Step: 1
Training loss: 2.734431743621826
Validation loss: 2.5339155197143555

Epoch: 6| Step: 2
Training loss: 2.397470474243164
Validation loss: 2.522237152181646

Epoch: 6| Step: 3
Training loss: 2.614798069000244
Validation loss: 2.5097452235478226

Epoch: 6| Step: 4
Training loss: 2.425886869430542
Validation loss: 2.498184916793659

Epoch: 6| Step: 5
Training loss: 3.4221930503845215
Validation loss: 2.488004284520303

Epoch: 6| Step: 6
Training loss: 2.703533411026001
Validation loss: 2.4834184005696285

Epoch: 6| Step: 7
Training loss: 3.298706293106079
Validation loss: 2.480878578719272

Epoch: 6| Step: 8
Training loss: 3.178054094314575
Validation loss: 2.4783919626666653

Epoch: 6| Step: 9
Training loss: 1.6676784753799438
Validation loss: 2.479354473852342

Epoch: 6| Step: 10
Training loss: 3.3115713596343994
Validation loss: 2.4793002733620266

Epoch: 6| Step: 11
Training loss: 2.621267795562744
Validation loss: 2.4777215603859193

Epoch: 6| Step: 12
Training loss: 2.929844379425049
Validation loss: 2.4755821369027577

Epoch: 6| Step: 13
Training loss: 2.185295581817627
Validation loss: 2.4813855514731458

Epoch: 98| Step: 0
Training loss: 2.4140167236328125
Validation loss: 2.4854661162181566

Epoch: 6| Step: 1
Training loss: 2.2097558975219727
Validation loss: 2.4876866340637207

Epoch: 6| Step: 2
Training loss: 2.457242012023926
Validation loss: 2.491709509203511

Epoch: 6| Step: 3
Training loss: 2.513063907623291
Validation loss: 2.499722166727948

Epoch: 6| Step: 4
Training loss: 2.943761110305786
Validation loss: 2.5083843200437483

Epoch: 6| Step: 5
Training loss: 3.0629615783691406
Validation loss: 2.520723778714416

Epoch: 6| Step: 6
Training loss: 2.8989248275756836
Validation loss: 2.5197095819698867

Epoch: 6| Step: 7
Training loss: 2.7385191917419434
Validation loss: 2.528653206363801

Epoch: 6| Step: 8
Training loss: 2.2119760513305664
Validation loss: 2.5010676896700295

Epoch: 6| Step: 9
Training loss: 3.133152484893799
Validation loss: 2.4847480507307154

Epoch: 6| Step: 10
Training loss: 2.9910037517547607
Validation loss: 2.4733536756166847

Epoch: 6| Step: 11
Training loss: 2.4503395557403564
Validation loss: 2.47379054305374

Epoch: 6| Step: 12
Training loss: 2.4539692401885986
Validation loss: 2.474996397572179

Epoch: 6| Step: 13
Training loss: 3.398597002029419
Validation loss: 2.475427596799789

Epoch: 99| Step: 0
Training loss: 3.1380882263183594
Validation loss: 2.4879605770111084

Epoch: 6| Step: 1
Training loss: 2.524324893951416
Validation loss: 2.5037818980473343

Epoch: 6| Step: 2
Training loss: 2.8493778705596924
Validation loss: 2.509481786399759

Epoch: 6| Step: 3
Training loss: 2.6335973739624023
Validation loss: 2.5027593797253025

Epoch: 6| Step: 4
Training loss: 2.5527710914611816
Validation loss: 2.4907131297613985

Epoch: 6| Step: 5
Training loss: 2.632582902908325
Validation loss: 2.4872014394370456

Epoch: 6| Step: 6
Training loss: 2.8202757835388184
Validation loss: 2.4923965674574657

Epoch: 6| Step: 7
Training loss: 2.966365337371826
Validation loss: 2.4955071403134252

Epoch: 6| Step: 8
Training loss: 2.390535831451416
Validation loss: 2.4910793894080707

Epoch: 6| Step: 9
Training loss: 2.5125088691711426
Validation loss: 2.4869160857251895

Epoch: 6| Step: 10
Training loss: 3.2501022815704346
Validation loss: 2.484681528101685

Epoch: 6| Step: 11
Training loss: 2.444875717163086
Validation loss: 2.4841869351684407

Epoch: 6| Step: 12
Training loss: 2.530503749847412
Validation loss: 2.487560431162516

Epoch: 6| Step: 13
Training loss: 2.0772018432617188
Validation loss: 2.496766495448287

Epoch: 100| Step: 0
Training loss: 3.1061649322509766
Validation loss: 2.516078369591826

Epoch: 6| Step: 1
Training loss: 2.0344667434692383
Validation loss: 2.5444080957802395

Epoch: 6| Step: 2
Training loss: 2.536590814590454
Validation loss: 2.5555912166513424

Epoch: 6| Step: 3
Training loss: 2.510472059249878
Validation loss: 2.5455623749763734

Epoch: 6| Step: 4
Training loss: 2.430492877960205
Validation loss: 2.5251524474031184

Epoch: 6| Step: 5
Training loss: 2.586305618286133
Validation loss: 2.515672286351522

Epoch: 6| Step: 6
Training loss: 2.414497137069702
Validation loss: 2.498617156859367

Epoch: 6| Step: 7
Training loss: 3.387650728225708
Validation loss: 2.4910102300746466

Epoch: 6| Step: 8
Training loss: 2.462754726409912
Validation loss: 2.4814506884544127

Epoch: 6| Step: 9
Training loss: 2.193228244781494
Validation loss: 2.4746437072753906

Epoch: 6| Step: 10
Training loss: 2.561061143875122
Validation loss: 2.473769764746389

Epoch: 6| Step: 11
Training loss: 3.186702251434326
Validation loss: 2.473763906827537

Epoch: 6| Step: 12
Training loss: 3.08852481842041
Validation loss: 2.4763388813182874

Epoch: 6| Step: 13
Training loss: 3.6396021842956543
Validation loss: 2.479514060481902

Epoch: 101| Step: 0
Training loss: 2.0367019176483154
Validation loss: 2.4839465541224324

Epoch: 6| Step: 1
Training loss: 2.432192325592041
Validation loss: 2.487002613723919

Epoch: 6| Step: 2
Training loss: 2.708400249481201
Validation loss: 2.492706908974596

Epoch: 6| Step: 3
Training loss: 3.4350006580352783
Validation loss: 2.4771120086792977

Epoch: 6| Step: 4
Training loss: 1.96052885055542
Validation loss: 2.4709292278494885

Epoch: 6| Step: 5
Training loss: 2.6699628829956055
Validation loss: 2.4676336267943024

Epoch: 6| Step: 6
Training loss: 2.877255916595459
Validation loss: 2.464876687654885

Epoch: 6| Step: 7
Training loss: 2.3763012886047363
Validation loss: 2.466061694647676

Epoch: 6| Step: 8
Training loss: 3.006180763244629
Validation loss: 2.466622534618583

Epoch: 6| Step: 9
Training loss: 2.7563042640686035
Validation loss: 2.462842318319505

Epoch: 6| Step: 10
Training loss: 2.4276602268218994
Validation loss: 2.4667487554652716

Epoch: 6| Step: 11
Training loss: 3.814561128616333
Validation loss: 2.4634659469768567

Epoch: 6| Step: 12
Training loss: 2.1347861289978027
Validation loss: 2.4585584978903494

Epoch: 6| Step: 13
Training loss: 2.998425006866455
Validation loss: 2.457943295919767

Epoch: 102| Step: 0
Training loss: 2.213590621948242
Validation loss: 2.457233290518484

Epoch: 6| Step: 1
Training loss: 2.9877758026123047
Validation loss: 2.4569049342986076

Epoch: 6| Step: 2
Training loss: 2.3125386238098145
Validation loss: 2.45681663995148

Epoch: 6| Step: 3
Training loss: 2.6986241340637207
Validation loss: 2.4584894846844416

Epoch: 6| Step: 4
Training loss: 2.146063804626465
Validation loss: 2.4563238851485716

Epoch: 6| Step: 5
Training loss: 2.8292770385742188
Validation loss: 2.462223429833689

Epoch: 6| Step: 6
Training loss: 2.8442506790161133
Validation loss: 2.468153376733103

Epoch: 6| Step: 7
Training loss: 2.9216678142547607
Validation loss: 2.473403056462606

Epoch: 6| Step: 8
Training loss: 3.246290922164917
Validation loss: 2.4833419476785967

Epoch: 6| Step: 9
Training loss: 2.8939640522003174
Validation loss: 2.480999392847861

Epoch: 6| Step: 10
Training loss: 2.326263904571533
Validation loss: 2.478281944028793

Epoch: 6| Step: 11
Training loss: 3.0219123363494873
Validation loss: 2.4748637471147763

Epoch: 6| Step: 12
Training loss: 2.3190793991088867
Validation loss: 2.4704450766245523

Epoch: 6| Step: 13
Training loss: 2.7466814517974854
Validation loss: 2.4699447898454565

Epoch: 103| Step: 0
Training loss: 2.6844701766967773
Validation loss: 2.468959303312404

Epoch: 6| Step: 1
Training loss: 3.1137161254882812
Validation loss: 2.467165575232557

Epoch: 6| Step: 2
Training loss: 3.166891098022461
Validation loss: 2.46191119891341

Epoch: 6| Step: 3
Training loss: 2.6993565559387207
Validation loss: 2.461777679381832

Epoch: 6| Step: 4
Training loss: 2.0604896545410156
Validation loss: 2.458664317284861

Epoch: 6| Step: 5
Training loss: 2.161435127258301
Validation loss: 2.454230644369638

Epoch: 6| Step: 6
Training loss: 1.9770760536193848
Validation loss: 2.4555771145769345

Epoch: 6| Step: 7
Training loss: 3.0749011039733887
Validation loss: 2.4530556791572162

Epoch: 6| Step: 8
Training loss: 3.01308274269104
Validation loss: 2.4500780925955823

Epoch: 6| Step: 9
Training loss: 2.488409996032715
Validation loss: 2.4542186362769014

Epoch: 6| Step: 10
Training loss: 2.0485334396362305
Validation loss: 2.4528456323890278

Epoch: 6| Step: 11
Training loss: 3.069883346557617
Validation loss: 2.456795471970753

Epoch: 6| Step: 12
Training loss: 2.6871116161346436
Validation loss: 2.454727554834017

Epoch: 6| Step: 13
Training loss: 3.458826780319214
Validation loss: 2.4595834670528287

Epoch: 104| Step: 0
Training loss: 2.880868434906006
Validation loss: 2.465700598173244

Epoch: 6| Step: 1
Training loss: 2.506253242492676
Validation loss: 2.477207765784315

Epoch: 6| Step: 2
Training loss: 3.693516731262207
Validation loss: 2.469301792883104

Epoch: 6| Step: 3
Training loss: 2.320000648498535
Validation loss: 2.4633229650476927

Epoch: 6| Step: 4
Training loss: 2.4270286560058594
Validation loss: 2.4619750233106714

Epoch: 6| Step: 5
Training loss: 2.1344988346099854
Validation loss: 2.4540391134959396

Epoch: 6| Step: 6
Training loss: 3.107050895690918
Validation loss: 2.4512230324488815

Epoch: 6| Step: 7
Training loss: 3.0107579231262207
Validation loss: 2.4544901027474353

Epoch: 6| Step: 8
Training loss: 1.9277012348175049
Validation loss: 2.4576735265793337

Epoch: 6| Step: 9
Training loss: 2.3669300079345703
Validation loss: 2.4675163069079

Epoch: 6| Step: 10
Training loss: 2.238307476043701
Validation loss: 2.4631444818230084

Epoch: 6| Step: 11
Training loss: 2.988821029663086
Validation loss: 2.469275327139003

Epoch: 6| Step: 12
Training loss: 2.5641894340515137
Validation loss: 2.4673738633432696

Epoch: 6| Step: 13
Training loss: 3.6322696208953857
Validation loss: 2.475555435303719

Epoch: 105| Step: 0
Training loss: 1.7501686811447144
Validation loss: 2.469861656106928

Epoch: 6| Step: 1
Training loss: 3.566333293914795
Validation loss: 2.475714022113431

Epoch: 6| Step: 2
Training loss: 2.408261299133301
Validation loss: 2.47754341812544

Epoch: 6| Step: 3
Training loss: 2.471712589263916
Validation loss: 2.4690494537353516

Epoch: 6| Step: 4
Training loss: 3.1376752853393555
Validation loss: 2.4536924234000583

Epoch: 6| Step: 5
Training loss: 2.6624755859375
Validation loss: 2.450396760817497

Epoch: 6| Step: 6
Training loss: 2.460515260696411
Validation loss: 2.4448065270659742

Epoch: 6| Step: 7
Training loss: 2.570010185241699
Validation loss: 2.4487320684617564

Epoch: 6| Step: 8
Training loss: 2.937957763671875
Validation loss: 2.4447684364934124

Epoch: 6| Step: 9
Training loss: 2.451420783996582
Validation loss: 2.4475398807115454

Epoch: 6| Step: 10
Training loss: 3.0639262199401855
Validation loss: 2.4450784370463383

Epoch: 6| Step: 11
Training loss: 2.1502416133880615
Validation loss: 2.4496522744496665

Epoch: 6| Step: 12
Training loss: 2.3728179931640625
Validation loss: 2.4456983509884087

Epoch: 6| Step: 13
Training loss: 3.6454687118530273
Validation loss: 2.441670371640113

Epoch: 106| Step: 0
Training loss: 2.530061721801758
Validation loss: 2.4456746103943034

Epoch: 6| Step: 1
Training loss: 2.3735389709472656
Validation loss: 2.4422710428955736

Epoch: 6| Step: 2
Training loss: 2.3800299167633057
Validation loss: 2.444257923351821

Epoch: 6| Step: 3
Training loss: 3.1120223999023438
Validation loss: 2.4427529509349535

Epoch: 6| Step: 4
Training loss: 2.301628828048706
Validation loss: 2.4393054362266295

Epoch: 6| Step: 5
Training loss: 2.8795764446258545
Validation loss: 2.443270775579637

Epoch: 6| Step: 6
Training loss: 2.5821480751037598
Validation loss: 2.4394695015363794

Epoch: 6| Step: 7
Training loss: 2.4577717781066895
Validation loss: 2.4494135969428608

Epoch: 6| Step: 8
Training loss: 2.564748764038086
Validation loss: 2.453116604076919

Epoch: 6| Step: 9
Training loss: 2.4198992252349854
Validation loss: 2.448960655479021

Epoch: 6| Step: 10
Training loss: 3.0925233364105225
Validation loss: 2.4474123318990073

Epoch: 6| Step: 11
Training loss: 2.7814321517944336
Validation loss: 2.455578011851157

Epoch: 6| Step: 12
Training loss: 2.4895708560943604
Validation loss: 2.4520801882590018

Epoch: 6| Step: 13
Training loss: 3.788274049758911
Validation loss: 2.461709109685754

Epoch: 107| Step: 0
Training loss: 2.41259765625
Validation loss: 2.4633746557338263

Epoch: 6| Step: 1
Training loss: 2.132080554962158
Validation loss: 2.4743739328076764

Epoch: 6| Step: 2
Training loss: 2.7240161895751953
Validation loss: 2.4706718344842233

Epoch: 6| Step: 3
Training loss: 2.5615053176879883
Validation loss: 2.474476957833895

Epoch: 6| Step: 4
Training loss: 2.703035831451416
Validation loss: 2.4654036414238716

Epoch: 6| Step: 5
Training loss: 3.1021902561187744
Validation loss: 2.4586169232604322

Epoch: 6| Step: 6
Training loss: 3.712099552154541
Validation loss: 2.45920999973051

Epoch: 6| Step: 7
Training loss: 2.741971969604492
Validation loss: 2.444652029263076

Epoch: 6| Step: 8
Training loss: 2.5987167358398438
Validation loss: 2.4512399024860834

Epoch: 6| Step: 9
Training loss: 2.7043490409851074
Validation loss: 2.446313799068492

Epoch: 6| Step: 10
Training loss: 1.7111854553222656
Validation loss: 2.453373373195689

Epoch: 6| Step: 11
Training loss: 2.9396495819091797
Validation loss: 2.4477523667837984

Epoch: 6| Step: 12
Training loss: 2.6931493282318115
Validation loss: 2.448447773533483

Epoch: 6| Step: 13
Training loss: 2.5130908489227295
Validation loss: 2.4467009370044996

Epoch: 108| Step: 0
Training loss: 2.2637553215026855
Validation loss: 2.4441094475407756

Epoch: 6| Step: 1
Training loss: 2.6515579223632812
Validation loss: 2.4427128453408518

Epoch: 6| Step: 2
Training loss: 2.585538864135742
Validation loss: 2.439831667048957

Epoch: 6| Step: 3
Training loss: 3.4700777530670166
Validation loss: 2.438923079480407

Epoch: 6| Step: 4
Training loss: 2.4971024990081787
Validation loss: 2.4365095297495523

Epoch: 6| Step: 5
Training loss: 2.3965492248535156
Validation loss: 2.4363772612746044

Epoch: 6| Step: 6
Training loss: 2.4740893840789795
Validation loss: 2.4413938881248556

Epoch: 6| Step: 7
Training loss: 2.149840831756592
Validation loss: 2.4433458056501163

Epoch: 6| Step: 8
Training loss: 2.9636459350585938
Validation loss: 2.4470001907758814

Epoch: 6| Step: 9
Training loss: 2.7066946029663086
Validation loss: 2.4414374264337684

Epoch: 6| Step: 10
Training loss: 2.7276692390441895
Validation loss: 2.4388289913054435

Epoch: 6| Step: 11
Training loss: 3.075803518295288
Validation loss: 2.440126188339726

Epoch: 6| Step: 12
Training loss: 2.5721030235290527
Validation loss: 2.44004165228977

Epoch: 6| Step: 13
Training loss: 2.7144575119018555
Validation loss: 2.450428598670549

Epoch: 109| Step: 0
Training loss: 1.9325776100158691
Validation loss: 2.4528864276024605

Epoch: 6| Step: 1
Training loss: 2.22135066986084
Validation loss: 2.4555027869439896

Epoch: 6| Step: 2
Training loss: 3.076925277709961
Validation loss: 2.47089796937922

Epoch: 6| Step: 3
Training loss: 2.8146517276763916
Validation loss: 2.465159587962653

Epoch: 6| Step: 4
Training loss: 2.7158496379852295
Validation loss: 2.4685151269358974

Epoch: 6| Step: 5
Training loss: 3.3653321266174316
Validation loss: 2.467569528087493

Epoch: 6| Step: 6
Training loss: 1.8359782695770264
Validation loss: 2.467507686666263

Epoch: 6| Step: 7
Training loss: 2.9547858238220215
Validation loss: 2.4644955947834957

Epoch: 6| Step: 8
Training loss: 2.8060460090637207
Validation loss: 2.462605130287909

Epoch: 6| Step: 9
Training loss: 2.4704160690307617
Validation loss: 2.46154454446608

Epoch: 6| Step: 10
Training loss: 2.699481964111328
Validation loss: 2.4542135577048025

Epoch: 6| Step: 11
Training loss: 2.749222755432129
Validation loss: 2.4500345260866228

Epoch: 6| Step: 12
Training loss: 2.858607530593872
Validation loss: 2.4471280856799056

Epoch: 6| Step: 13
Training loss: 2.563899517059326
Validation loss: 2.443604559026739

Epoch: 110| Step: 0
Training loss: 2.9364137649536133
Validation loss: 2.4436246246419926

Epoch: 6| Step: 1
Training loss: 2.6730756759643555
Validation loss: 2.4351737358236827

Epoch: 6| Step: 2
Training loss: 3.041195869445801
Validation loss: 2.4339664700210735

Epoch: 6| Step: 3
Training loss: 2.414947748184204
Validation loss: 2.4344904807306107

Epoch: 6| Step: 4
Training loss: 3.206239700317383
Validation loss: 2.436734614833709

Epoch: 6| Step: 5
Training loss: 2.7099223136901855
Validation loss: 2.4382518311982513

Epoch: 6| Step: 6
Training loss: 2.5230956077575684
Validation loss: 2.4394632436895884

Epoch: 6| Step: 7
Training loss: 2.1460585594177246
Validation loss: 2.4423513053565897

Epoch: 6| Step: 8
Training loss: 2.3872296810150146
Validation loss: 2.4512456078683176

Epoch: 6| Step: 9
Training loss: 3.0464024543762207
Validation loss: 2.4616083637360604

Epoch: 6| Step: 10
Training loss: 3.1692214012145996
Validation loss: 2.4586311719750844

Epoch: 6| Step: 11
Training loss: 2.2680282592773438
Validation loss: 2.455622708925637

Epoch: 6| Step: 12
Training loss: 2.4900286197662354
Validation loss: 2.4475768612277125

Epoch: 6| Step: 13
Training loss: 1.6763651371002197
Validation loss: 2.4494008992307927

Epoch: 111| Step: 0
Training loss: 3.0211663246154785
Validation loss: 2.455671646261728

Epoch: 6| Step: 1
Training loss: 1.9180235862731934
Validation loss: 2.4601318502938874

Epoch: 6| Step: 2
Training loss: 2.2767834663391113
Validation loss: 2.463693828992946

Epoch: 6| Step: 3
Training loss: 3.4422829151153564
Validation loss: 2.4630222743557346

Epoch: 6| Step: 4
Training loss: 2.174504280090332
Validation loss: 2.470920455071234

Epoch: 6| Step: 5
Training loss: 2.1578550338745117
Validation loss: 2.4753236616811445

Epoch: 6| Step: 6
Training loss: 3.2346746921539307
Validation loss: 2.4726326798879974

Epoch: 6| Step: 7
Training loss: 2.445126533508301
Validation loss: 2.472909253130677

Epoch: 6| Step: 8
Training loss: 2.328705310821533
Validation loss: 2.457204234215521

Epoch: 6| Step: 9
Training loss: 2.7623534202575684
Validation loss: 2.45464902283043

Epoch: 6| Step: 10
Training loss: 2.808688163757324
Validation loss: 2.4493559214376632

Epoch: 6| Step: 11
Training loss: 3.27905535697937
Validation loss: 2.433690171087942

Epoch: 6| Step: 12
Training loss: 2.519951820373535
Validation loss: 2.4357170058834936

Epoch: 6| Step: 13
Training loss: 2.971771240234375
Validation loss: 2.421583528159767

Epoch: 112| Step: 0
Training loss: 2.179854393005371
Validation loss: 2.4232410846217984

Epoch: 6| Step: 1
Training loss: 2.486527919769287
Validation loss: 2.4263795703969975

Epoch: 6| Step: 2
Training loss: 2.3921194076538086
Validation loss: 2.428167866122338

Epoch: 6| Step: 3
Training loss: 3.5480825901031494
Validation loss: 2.432167418541447

Epoch: 6| Step: 4
Training loss: 3.3214774131774902
Validation loss: 2.432678166256156

Epoch: 6| Step: 5
Training loss: 2.361462354660034
Validation loss: 2.430963949490619

Epoch: 6| Step: 6
Training loss: 2.6312050819396973
Validation loss: 2.4297093217090895

Epoch: 6| Step: 7
Training loss: 2.443767786026001
Validation loss: 2.4302582407510407

Epoch: 6| Step: 8
Training loss: 2.5333943367004395
Validation loss: 2.4286893452367475

Epoch: 6| Step: 9
Training loss: 3.2166547775268555
Validation loss: 2.4396141459864955

Epoch: 6| Step: 10
Training loss: 2.1608643531799316
Validation loss: 2.4466029341502855

Epoch: 6| Step: 11
Training loss: 2.645042896270752
Validation loss: 2.448970471659014

Epoch: 6| Step: 12
Training loss: 2.282984972000122
Validation loss: 2.455923670081682

Epoch: 6| Step: 13
Training loss: 3.154953956604004
Validation loss: 2.4441868823061705

Epoch: 113| Step: 0
Training loss: 1.7888939380645752
Validation loss: 2.425004700178741

Epoch: 6| Step: 1
Training loss: 2.6643505096435547
Validation loss: 2.4214326848265944

Epoch: 6| Step: 2
Training loss: 2.298996686935425
Validation loss: 2.418139560248262

Epoch: 6| Step: 3
Training loss: 2.0707767009735107
Validation loss: 2.4152236433439356

Epoch: 6| Step: 4
Training loss: 2.59213924407959
Validation loss: 2.416536804168455

Epoch: 6| Step: 5
Training loss: 2.9118731021881104
Validation loss: 2.42024754452449

Epoch: 6| Step: 6
Training loss: 3.0041592121124268
Validation loss: 2.4225370986487276

Epoch: 6| Step: 7
Training loss: 3.5798473358154297
Validation loss: 2.4234689961197557

Epoch: 6| Step: 8
Training loss: 3.1409003734588623
Validation loss: 2.4254944362948017

Epoch: 6| Step: 9
Training loss: 2.3790440559387207
Validation loss: 2.422306050536453

Epoch: 6| Step: 10
Training loss: 2.515598773956299
Validation loss: 2.416425651119601

Epoch: 6| Step: 11
Training loss: 2.611457347869873
Validation loss: 2.415140854415073

Epoch: 6| Step: 12
Training loss: 3.0797736644744873
Validation loss: 2.415279003881639

Epoch: 6| Step: 13
Training loss: 2.433535575866699
Validation loss: 2.4183022386284283

Epoch: 114| Step: 0
Training loss: 1.9298510551452637
Validation loss: 2.4211935176644275

Epoch: 6| Step: 1
Training loss: 2.25978422164917
Validation loss: 2.427211615347093

Epoch: 6| Step: 2
Training loss: 2.4896395206451416
Validation loss: 2.447357452043923

Epoch: 6| Step: 3
Training loss: 2.8991870880126953
Validation loss: 2.466042331469956

Epoch: 6| Step: 4
Training loss: 2.8188977241516113
Validation loss: 2.5016557375590005

Epoch: 6| Step: 5
Training loss: 2.506723642349243
Validation loss: 2.5234755700634373

Epoch: 6| Step: 6
Training loss: 2.4042930603027344
Validation loss: 2.5182817905179915

Epoch: 6| Step: 7
Training loss: 3.127242088317871
Validation loss: 2.485325736384238

Epoch: 6| Step: 8
Training loss: 2.9631776809692383
Validation loss: 2.4565702330681587

Epoch: 6| Step: 9
Training loss: 2.90102481842041
Validation loss: 2.433033497102799

Epoch: 6| Step: 10
Training loss: 3.28546404838562
Validation loss: 2.424352679201352

Epoch: 6| Step: 11
Training loss: 2.502798080444336
Validation loss: 2.413296468796269

Epoch: 6| Step: 12
Training loss: 2.306155204772949
Validation loss: 2.409766956042218

Epoch: 6| Step: 13
Training loss: 2.9945406913757324
Validation loss: 2.4083344321097098

Epoch: 115| Step: 0
Training loss: 2.502321243286133
Validation loss: 2.414241052442981

Epoch: 6| Step: 1
Training loss: 2.880146026611328
Validation loss: 2.416527825017129

Epoch: 6| Step: 2
Training loss: 2.045372486114502
Validation loss: 2.4209792178164244

Epoch: 6| Step: 3
Training loss: 2.7718987464904785
Validation loss: 2.4242632183977353

Epoch: 6| Step: 4
Training loss: 3.0075507164001465
Validation loss: 2.423029843197074

Epoch: 6| Step: 5
Training loss: 2.8987069129943848
Validation loss: 2.420485932339904

Epoch: 6| Step: 6
Training loss: 2.7527623176574707
Validation loss: 2.416472113260659

Epoch: 6| Step: 7
Training loss: 2.933969020843506
Validation loss: 2.4145825832120833

Epoch: 6| Step: 8
Training loss: 2.8634448051452637
Validation loss: 2.4122329194058656

Epoch: 6| Step: 9
Training loss: 2.6728882789611816
Validation loss: 2.4159171427449873

Epoch: 6| Step: 10
Training loss: 2.537830352783203
Validation loss: 2.4152518087817776

Epoch: 6| Step: 11
Training loss: 1.905259609222412
Validation loss: 2.4131350414727324

Epoch: 6| Step: 12
Training loss: 2.711243152618408
Validation loss: 2.415351329311248

Epoch: 6| Step: 13
Training loss: 2.7017364501953125
Validation loss: 2.424205190391951

Epoch: 116| Step: 0
Training loss: 2.7016618251800537
Validation loss: 2.4356593983147734

Epoch: 6| Step: 1
Training loss: 2.48624324798584
Validation loss: 2.4421152555814354

Epoch: 6| Step: 2
Training loss: 2.7322776317596436
Validation loss: 2.4571555737526185

Epoch: 6| Step: 3
Training loss: 2.173675775527954
Validation loss: 2.4471443622343

Epoch: 6| Step: 4
Training loss: 2.448965311050415
Validation loss: 2.4421823409295853

Epoch: 6| Step: 5
Training loss: 2.0330302715301514
Validation loss: 2.4397274422389206

Epoch: 6| Step: 6
Training loss: 3.5050647258758545
Validation loss: 2.430085612881568

Epoch: 6| Step: 7
Training loss: 2.697286605834961
Validation loss: 2.427868125259235

Epoch: 6| Step: 8
Training loss: 2.795034885406494
Validation loss: 2.429044485092163

Epoch: 6| Step: 9
Training loss: 2.559626340866089
Validation loss: 2.4281352335406887

Epoch: 6| Step: 10
Training loss: 3.3980355262756348
Validation loss: 2.429451245133595

Epoch: 6| Step: 11
Training loss: 2.7452547550201416
Validation loss: 2.4237961743467595

Epoch: 6| Step: 12
Training loss: 1.9469456672668457
Validation loss: 2.4238307014588387

Epoch: 6| Step: 13
Training loss: 3.127178907394409
Validation loss: 2.41565364919683

Epoch: 117| Step: 0
Training loss: 3.5934243202209473
Validation loss: 2.41716508711538

Epoch: 6| Step: 1
Training loss: 2.253933906555176
Validation loss: 2.4088403768436883

Epoch: 6| Step: 2
Training loss: 2.287602424621582
Validation loss: 2.409550525808847

Epoch: 6| Step: 3
Training loss: 3.232365369796753
Validation loss: 2.4063560937040593

Epoch: 6| Step: 4
Training loss: 3.288963556289673
Validation loss: 2.404320042620423

Epoch: 6| Step: 5
Training loss: 2.9226927757263184
Validation loss: 2.4033467820895615

Epoch: 6| Step: 6
Training loss: 1.9703861474990845
Validation loss: 2.402625378742013

Epoch: 6| Step: 7
Training loss: 2.203186511993408
Validation loss: 2.4029035337509645

Epoch: 6| Step: 8
Training loss: 3.1113059520721436
Validation loss: 2.401696938340382

Epoch: 6| Step: 9
Training loss: 2.8591413497924805
Validation loss: 2.40732672650327

Epoch: 6| Step: 10
Training loss: 2.878706455230713
Validation loss: 2.4145035564258532

Epoch: 6| Step: 11
Training loss: 1.6485430002212524
Validation loss: 2.4193483142442602

Epoch: 6| Step: 12
Training loss: 2.4580864906311035
Validation loss: 2.423354618010982

Epoch: 6| Step: 13
Training loss: 1.9266995191574097
Validation loss: 2.421863114962014

Epoch: 118| Step: 0
Training loss: 2.3001394271850586
Validation loss: 2.426641451415195

Epoch: 6| Step: 1
Training loss: 3.139782428741455
Validation loss: 2.426284736202609

Epoch: 6| Step: 2
Training loss: 2.633375406265259
Validation loss: 2.4264031123089533

Epoch: 6| Step: 3
Training loss: 2.6296064853668213
Validation loss: 2.423815301669541

Epoch: 6| Step: 4
Training loss: 2.469925880432129
Validation loss: 2.4218079454155377

Epoch: 6| Step: 5
Training loss: 2.844299077987671
Validation loss: 2.4152430052398355

Epoch: 6| Step: 6
Training loss: 2.40000319480896
Validation loss: 2.4061388354147635

Epoch: 6| Step: 7
Training loss: 2.16746187210083
Validation loss: 2.4049031170465613

Epoch: 6| Step: 8
Training loss: 2.8890538215637207
Validation loss: 2.40347727652519

Epoch: 6| Step: 9
Training loss: 2.2150707244873047
Validation loss: 2.4066700627726894

Epoch: 6| Step: 10
Training loss: 2.482943058013916
Validation loss: 2.4026655074088805

Epoch: 6| Step: 11
Training loss: 2.833404541015625
Validation loss: 2.4020447756654475

Epoch: 6| Step: 12
Training loss: 3.172288656234741
Validation loss: 2.4036118958586004

Epoch: 6| Step: 13
Training loss: 2.8912487030029297
Validation loss: 2.4001199712035475

Epoch: 119| Step: 0
Training loss: 2.250185966491699
Validation loss: 2.4019284735443773

Epoch: 6| Step: 1
Training loss: 3.03934907913208
Validation loss: 2.4065841064658215

Epoch: 6| Step: 2
Training loss: 2.4453933238983154
Validation loss: 2.4023042186614005

Epoch: 6| Step: 3
Training loss: 2.0546722412109375
Validation loss: 2.399676502391856

Epoch: 6| Step: 4
Training loss: 2.695500373840332
Validation loss: 2.404336593484366

Epoch: 6| Step: 5
Training loss: 3.431601047515869
Validation loss: 2.3994161621216805

Epoch: 6| Step: 6
Training loss: 2.8071908950805664
Validation loss: 2.39651390557648

Epoch: 6| Step: 7
Training loss: 2.360017776489258
Validation loss: 2.3962536627246487

Epoch: 6| Step: 8
Training loss: 2.423800468444824
Validation loss: 2.395735022842243

Epoch: 6| Step: 9
Training loss: 2.8811066150665283
Validation loss: 2.3957047821373068

Epoch: 6| Step: 10
Training loss: 2.448448419570923
Validation loss: 2.395720092199182

Epoch: 6| Step: 11
Training loss: 2.5729119777679443
Validation loss: 2.3994084558179303

Epoch: 6| Step: 12
Training loss: 2.753241539001465
Validation loss: 2.4052023836361465

Epoch: 6| Step: 13
Training loss: 2.896164894104004
Validation loss: 2.4171795742486113

Epoch: 120| Step: 0
Training loss: 1.9903851747512817
Validation loss: 2.4415994908220027

Epoch: 6| Step: 1
Training loss: 2.7369041442871094
Validation loss: 2.452269507992652

Epoch: 6| Step: 2
Training loss: 3.2332611083984375
Validation loss: 2.4887067835818053

Epoch: 6| Step: 3
Training loss: 2.63338041305542
Validation loss: 2.4991597744726364

Epoch: 6| Step: 4
Training loss: 3.352053165435791
Validation loss: 2.507688832539384

Epoch: 6| Step: 5
Training loss: 3.1678669452667236
Validation loss: 2.522175117205548

Epoch: 6| Step: 6
Training loss: 2.4814844131469727
Validation loss: 2.511924627006695

Epoch: 6| Step: 7
Training loss: 2.471794843673706
Validation loss: 2.4927658675819315

Epoch: 6| Step: 8
Training loss: 2.9151439666748047
Validation loss: 2.462232275675702

Epoch: 6| Step: 9
Training loss: 2.3653178215026855
Validation loss: 2.4435889900371595

Epoch: 6| Step: 10
Training loss: 2.2422404289245605
Validation loss: 2.428271726895404

Epoch: 6| Step: 11
Training loss: 2.5107946395874023
Validation loss: 2.413669570799797

Epoch: 6| Step: 12
Training loss: 2.6227967739105225
Validation loss: 2.4100381917850946

Epoch: 6| Step: 13
Training loss: 2.326965808868408
Validation loss: 2.410139931145535

Epoch: 121| Step: 0
Training loss: 2.6157515048980713
Validation loss: 2.4089030142753356

Epoch: 6| Step: 1
Training loss: 2.422133207321167
Validation loss: 2.4057519025700067

Epoch: 6| Step: 2
Training loss: 2.575793743133545
Validation loss: 2.4074151951779603

Epoch: 6| Step: 3
Training loss: 2.7604188919067383
Validation loss: 2.4065581752407934

Epoch: 6| Step: 4
Training loss: 2.169126510620117
Validation loss: 2.410734353526946

Epoch: 6| Step: 5
Training loss: 1.767096996307373
Validation loss: 2.4126692612965903

Epoch: 6| Step: 6
Training loss: 2.4672658443450928
Validation loss: 2.4219240450089976

Epoch: 6| Step: 7
Training loss: 3.002061605453491
Validation loss: 2.415660068552981

Epoch: 6| Step: 8
Training loss: 2.6521472930908203
Validation loss: 2.413256911821263

Epoch: 6| Step: 9
Training loss: 3.1186914443969727
Validation loss: 2.4126923520077943

Epoch: 6| Step: 10
Training loss: 3.043442726135254
Validation loss: 2.414533407457413

Epoch: 6| Step: 11
Training loss: 2.691953420639038
Validation loss: 2.410689446233934

Epoch: 6| Step: 12
Training loss: 3.1406607627868652
Validation loss: 2.4067467387004564

Epoch: 6| Step: 13
Training loss: 2.5933847427368164
Validation loss: 2.408020427150111

Epoch: 122| Step: 0
Training loss: 2.5812227725982666
Validation loss: 2.4057540073189685

Epoch: 6| Step: 1
Training loss: 2.5248451232910156
Validation loss: 2.4035722081379225

Epoch: 6| Step: 2
Training loss: 2.2493152618408203
Validation loss: 2.404586684319281

Epoch: 6| Step: 3
Training loss: 2.3035728931427
Validation loss: 2.4083558077453286

Epoch: 6| Step: 4
Training loss: 2.967421054840088
Validation loss: 2.400179309229697

Epoch: 6| Step: 5
Training loss: 2.856539487838745
Validation loss: 2.400867398067187

Epoch: 6| Step: 6
Training loss: 2.70033597946167
Validation loss: 2.403516674554476

Epoch: 6| Step: 7
Training loss: 2.1947293281555176
Validation loss: 2.4111257829973773

Epoch: 6| Step: 8
Training loss: 2.8504419326782227
Validation loss: 2.422047471487394

Epoch: 6| Step: 9
Training loss: 3.03751540184021
Validation loss: 2.4228048811676683

Epoch: 6| Step: 10
Training loss: 2.468661308288574
Validation loss: 2.4310490623597176

Epoch: 6| Step: 11
Training loss: 2.7931971549987793
Validation loss: 2.431470086497645

Epoch: 6| Step: 12
Training loss: 2.955209970474243
Validation loss: 2.435886021583311

Epoch: 6| Step: 13
Training loss: 2.099787950515747
Validation loss: 2.4310577684833157

Epoch: 123| Step: 0
Training loss: 2.845280647277832
Validation loss: 2.423437759440432

Epoch: 6| Step: 1
Training loss: 2.103506088256836
Validation loss: 2.412750200558734

Epoch: 6| Step: 2
Training loss: 2.05991792678833
Validation loss: 2.410033018358292

Epoch: 6| Step: 3
Training loss: 2.6477034091949463
Validation loss: 2.407120407268565

Epoch: 6| Step: 4
Training loss: 2.2908735275268555
Validation loss: 2.4047255464779433

Epoch: 6| Step: 5
Training loss: 2.1613426208496094
Validation loss: 2.4087828231114212

Epoch: 6| Step: 6
Training loss: 2.4198930263519287
Validation loss: 2.4085777164787374

Epoch: 6| Step: 7
Training loss: 2.7359378337860107
Validation loss: 2.4071993186909664

Epoch: 6| Step: 8
Training loss: 2.4336938858032227
Validation loss: 2.401893377304077

Epoch: 6| Step: 9
Training loss: 2.892549514770508
Validation loss: 2.4028654188238163

Epoch: 6| Step: 10
Training loss: 3.387478828430176
Validation loss: 2.400353988011678

Epoch: 6| Step: 11
Training loss: 2.705982208251953
Validation loss: 2.4045069371500323

Epoch: 6| Step: 12
Training loss: 3.0220632553100586
Validation loss: 2.3970963339651785

Epoch: 6| Step: 13
Training loss: 3.240487813949585
Validation loss: 2.394503003807478

Epoch: 124| Step: 0
Training loss: 2.7690160274505615
Validation loss: 2.3856408660129835

Epoch: 6| Step: 1
Training loss: 2.871030330657959
Validation loss: 2.379128151042487

Epoch: 6| Step: 2
Training loss: 2.884755849838257
Validation loss: 2.37694614677019

Epoch: 6| Step: 3
Training loss: 2.3373565673828125
Validation loss: 2.3830453400970786

Epoch: 6| Step: 4
Training loss: 2.513648271560669
Validation loss: 2.380732615788778

Epoch: 6| Step: 5
Training loss: 2.504124879837036
Validation loss: 2.383497615014353

Epoch: 6| Step: 6
Training loss: 2.8496201038360596
Validation loss: 2.3846171696980796

Epoch: 6| Step: 7
Training loss: 2.655632495880127
Validation loss: 2.385646366303967

Epoch: 6| Step: 8
Training loss: 2.008375644683838
Validation loss: 2.380197289169476

Epoch: 6| Step: 9
Training loss: 3.0668673515319824
Validation loss: 2.3808780177947013

Epoch: 6| Step: 10
Training loss: 3.295529842376709
Validation loss: 2.3778861620092906

Epoch: 6| Step: 11
Training loss: 1.9995609521865845
Validation loss: 2.376295543486072

Epoch: 6| Step: 12
Training loss: 2.7142298221588135
Validation loss: 2.384153460943571

Epoch: 6| Step: 13
Training loss: 2.1195802688598633
Validation loss: 2.3854059352669665

Epoch: 125| Step: 0
Training loss: 2.1619906425476074
Validation loss: 2.391779130504977

Epoch: 6| Step: 1
Training loss: 3.2333414554595947
Validation loss: 2.397871871148386

Epoch: 6| Step: 2
Training loss: 2.4740118980407715
Validation loss: 2.4158756015121297

Epoch: 6| Step: 3
Training loss: 1.689509630203247
Validation loss: 2.41899360123501

Epoch: 6| Step: 4
Training loss: 2.358762264251709
Validation loss: 2.414801623231621

Epoch: 6| Step: 5
Training loss: 2.515882968902588
Validation loss: 2.39913837627698

Epoch: 6| Step: 6
Training loss: 3.0951061248779297
Validation loss: 2.3903559074606946

Epoch: 6| Step: 7
Training loss: 2.8536200523376465
Validation loss: 2.3901218137433453

Epoch: 6| Step: 8
Training loss: 2.1035163402557373
Validation loss: 2.3865986254907425

Epoch: 6| Step: 9
Training loss: 2.6698222160339355
Validation loss: 2.3853988365460466

Epoch: 6| Step: 10
Training loss: 2.7873623371124268
Validation loss: 2.389611341620004

Epoch: 6| Step: 11
Training loss: 3.038205623626709
Validation loss: 2.3892694442502913

Epoch: 6| Step: 12
Training loss: 3.311676502227783
Validation loss: 2.394510821629596

Epoch: 6| Step: 13
Training loss: 2.373072385787964
Validation loss: 2.3982439605138635

Epoch: 126| Step: 0
Training loss: 1.4902219772338867
Validation loss: 2.3995387836169173

Epoch: 6| Step: 1
Training loss: 2.6976048946380615
Validation loss: 2.401794807885283

Epoch: 6| Step: 2
Training loss: 2.6986608505249023
Validation loss: 2.4202950462218253

Epoch: 6| Step: 3
Training loss: 3.2845258712768555
Validation loss: 2.4268742735667894

Epoch: 6| Step: 4
Training loss: 2.6146154403686523
Validation loss: 2.4091797567182973

Epoch: 6| Step: 5
Training loss: 2.5344395637512207
Validation loss: 2.398742104089388

Epoch: 6| Step: 6
Training loss: 2.1581878662109375
Validation loss: 2.3813625766384985

Epoch: 6| Step: 7
Training loss: 2.688624620437622
Validation loss: 2.381214593046455

Epoch: 6| Step: 8
Training loss: 2.665437698364258
Validation loss: 2.3821548031222437

Epoch: 6| Step: 9
Training loss: 2.777040481567383
Validation loss: 2.3787915962998585

Epoch: 6| Step: 10
Training loss: 2.780900001525879
Validation loss: 2.3853301232860935

Epoch: 6| Step: 11
Training loss: 2.6266815662384033
Validation loss: 2.3867379414137972

Epoch: 6| Step: 12
Training loss: 2.9727730751037598
Validation loss: 2.397017486633793

Epoch: 6| Step: 13
Training loss: 2.739638090133667
Validation loss: 2.3923679808134675

Epoch: 127| Step: 0
Training loss: 2.82973051071167
Validation loss: 2.39849074937964

Epoch: 6| Step: 1
Training loss: 2.3533315658569336
Validation loss: 2.4036571338612545

Epoch: 6| Step: 2
Training loss: 2.8459410667419434
Validation loss: 2.4151085833067536

Epoch: 6| Step: 3
Training loss: 2.448164224624634
Validation loss: 2.420486480959

Epoch: 6| Step: 4
Training loss: 3.0917701721191406
Validation loss: 2.415295785473239

Epoch: 6| Step: 5
Training loss: 2.6038382053375244
Validation loss: 2.4096615596484114

Epoch: 6| Step: 6
Training loss: 2.137342691421509
Validation loss: 2.4066825374480216

Epoch: 6| Step: 7
Training loss: 2.0333199501037598
Validation loss: 2.4066772473755704

Epoch: 6| Step: 8
Training loss: 3.080986738204956
Validation loss: 2.4083453839825046

Epoch: 6| Step: 9
Training loss: 2.9729104042053223
Validation loss: 2.4045138999979985

Epoch: 6| Step: 10
Training loss: 2.732523202896118
Validation loss: 2.4033357302347818

Epoch: 6| Step: 11
Training loss: 2.2121872901916504
Validation loss: 2.3962618176655104

Epoch: 6| Step: 12
Training loss: 2.4304614067077637
Validation loss: 2.395642367742395

Epoch: 6| Step: 13
Training loss: 3.0720720291137695
Validation loss: 2.395268104409659

Epoch: 128| Step: 0
Training loss: 3.1913468837738037
Validation loss: 2.400471382243659

Epoch: 6| Step: 1
Training loss: 2.6909592151641846
Validation loss: 2.3946994325166107

Epoch: 6| Step: 2
Training loss: 2.568441867828369
Validation loss: 2.3915835093426447

Epoch: 6| Step: 3
Training loss: 1.632385492324829
Validation loss: 2.396307178722915

Epoch: 6| Step: 4
Training loss: 2.579495906829834
Validation loss: 2.3940582172845

Epoch: 6| Step: 5
Training loss: 3.0585107803344727
Validation loss: 2.3976113668052097

Epoch: 6| Step: 6
Training loss: 2.56415057182312
Validation loss: 2.4043603430512133

Epoch: 6| Step: 7
Training loss: 1.847771167755127
Validation loss: 2.411501392241447

Epoch: 6| Step: 8
Training loss: 2.805488109588623
Validation loss: 2.402706097531062

Epoch: 6| Step: 9
Training loss: 2.3392186164855957
Validation loss: 2.398009697596232

Epoch: 6| Step: 10
Training loss: 3.03064227104187
Validation loss: 2.395710640056159

Epoch: 6| Step: 11
Training loss: 2.3709702491760254
Validation loss: 2.384131734089185

Epoch: 6| Step: 12
Training loss: 2.854323625564575
Validation loss: 2.386160332669494

Epoch: 6| Step: 13
Training loss: 3.337317943572998
Validation loss: 2.377863881408527

Epoch: 129| Step: 0
Training loss: 2.404980421066284
Validation loss: 2.3724451295791136

Epoch: 6| Step: 1
Training loss: 2.825255870819092
Validation loss: 2.375512966545679

Epoch: 6| Step: 2
Training loss: 2.849099636077881
Validation loss: 2.375089140348537

Epoch: 6| Step: 3
Training loss: 2.48862886428833
Validation loss: 2.37992496900661

Epoch: 6| Step: 4
Training loss: 2.989748477935791
Validation loss: 2.38091472143768

Epoch: 6| Step: 5
Training loss: 2.839900493621826
Validation loss: 2.3887616306222896

Epoch: 6| Step: 6
Training loss: 1.708490252494812
Validation loss: 2.3886032053219375

Epoch: 6| Step: 7
Training loss: 3.0432746410369873
Validation loss: 2.389000062019594

Epoch: 6| Step: 8
Training loss: 2.8421239852905273
Validation loss: 2.4094267506753244

Epoch: 6| Step: 9
Training loss: 2.3768608570098877
Validation loss: 2.4040045763856623

Epoch: 6| Step: 10
Training loss: 2.25240421295166
Validation loss: 2.4071845341754217

Epoch: 6| Step: 11
Training loss: 3.0506949424743652
Validation loss: 2.406545005818849

Epoch: 6| Step: 12
Training loss: 2.3546018600463867
Validation loss: 2.4107629970837663

Epoch: 6| Step: 13
Training loss: 2.528409004211426
Validation loss: 2.40752407299575

Epoch: 130| Step: 0
Training loss: 2.6398046016693115
Validation loss: 2.388594473561933

Epoch: 6| Step: 1
Training loss: 2.894089937210083
Validation loss: 2.378163609453427

Epoch: 6| Step: 2
Training loss: 2.769534111022949
Validation loss: 2.375433352685744

Epoch: 6| Step: 3
Training loss: 3.077678918838501
Validation loss: 2.373211517128893

Epoch: 6| Step: 4
Training loss: 1.9947038888931274
Validation loss: 2.368566402824976

Epoch: 6| Step: 5
Training loss: 2.406681537628174
Validation loss: 2.3671114239641415

Epoch: 6| Step: 6
Training loss: 2.7405779361724854
Validation loss: 2.3716256900500228

Epoch: 6| Step: 7
Training loss: 1.9800325632095337
Validation loss: 2.3759090105692544

Epoch: 6| Step: 8
Training loss: 2.967409610748291
Validation loss: 2.364225618300899

Epoch: 6| Step: 9
Training loss: 2.4828951358795166
Validation loss: 2.373862644677521

Epoch: 6| Step: 10
Training loss: 2.6070618629455566
Validation loss: 2.37861733026402

Epoch: 6| Step: 11
Training loss: 2.9915952682495117
Validation loss: 2.3791810299760554

Epoch: 6| Step: 12
Training loss: 2.725965976715088
Validation loss: 2.375978764667306

Epoch: 6| Step: 13
Training loss: 2.10142183303833
Validation loss: 2.3847887669840167

Epoch: 131| Step: 0
Training loss: 2.490935802459717
Validation loss: 2.3929250573599212

Epoch: 6| Step: 1
Training loss: 2.1653857231140137
Validation loss: 2.3887177334036878

Epoch: 6| Step: 2
Training loss: 2.2939209938049316
Validation loss: 2.3937934162796184

Epoch: 6| Step: 3
Training loss: 2.3485500812530518
Validation loss: 2.395553009484404

Epoch: 6| Step: 4
Training loss: 3.67393159866333
Validation loss: 2.39274017272457

Epoch: 6| Step: 5
Training loss: 3.099809169769287
Validation loss: 2.3882266680399575

Epoch: 6| Step: 6
Training loss: 2.370232343673706
Validation loss: 2.3884720058851343

Epoch: 6| Step: 7
Training loss: 2.1864094734191895
Validation loss: 2.3935437433181272

Epoch: 6| Step: 8
Training loss: 3.050414562225342
Validation loss: 2.4086746887494157

Epoch: 6| Step: 9
Training loss: 2.4802606105804443
Validation loss: 2.4132772876370336

Epoch: 6| Step: 10
Training loss: 2.392186164855957
Validation loss: 2.3836254355727986

Epoch: 6| Step: 11
Training loss: 2.9985013008117676
Validation loss: 2.3789139639946724

Epoch: 6| Step: 12
Training loss: 2.67787504196167
Validation loss: 2.3693883534400695

Epoch: 6| Step: 13
Training loss: 1.9553759098052979
Validation loss: 2.3641497550472135

Epoch: 132| Step: 0
Training loss: 2.325784683227539
Validation loss: 2.361952043348743

Epoch: 6| Step: 1
Training loss: 1.623782753944397
Validation loss: 2.3587353152613484

Epoch: 6| Step: 2
Training loss: 3.1691930294036865
Validation loss: 2.351511234878212

Epoch: 6| Step: 3
Training loss: 2.453737735748291
Validation loss: 2.35423122426515

Epoch: 6| Step: 4
Training loss: 2.355485677719116
Validation loss: 2.3488628992470364

Epoch: 6| Step: 5
Training loss: 2.8782296180725098
Validation loss: 2.358921217662032

Epoch: 6| Step: 6
Training loss: 2.385037899017334
Validation loss: 2.3572203036277526

Epoch: 6| Step: 7
Training loss: 2.8643598556518555
Validation loss: 2.3709238985533356

Epoch: 6| Step: 8
Training loss: 3.1544620990753174
Validation loss: 2.380157752703595

Epoch: 6| Step: 9
Training loss: 2.7529308795928955
Validation loss: 2.369076157128939

Epoch: 6| Step: 10
Training loss: 2.1751136779785156
Validation loss: 2.365374339524136

Epoch: 6| Step: 11
Training loss: 3.029921770095825
Validation loss: 2.37142167809189

Epoch: 6| Step: 12
Training loss: 2.904470920562744
Validation loss: 2.375038895555722

Epoch: 6| Step: 13
Training loss: 2.232091188430786
Validation loss: 2.3708827777575423

Epoch: 133| Step: 0
Training loss: 2.6925699710845947
Validation loss: 2.370564606881911

Epoch: 6| Step: 1
Training loss: 2.157681703567505
Validation loss: 2.3628993649636545

Epoch: 6| Step: 2
Training loss: 3.051647186279297
Validation loss: 2.3630895409532773

Epoch: 6| Step: 3
Training loss: 2.460042715072632
Validation loss: 2.3618010295334684

Epoch: 6| Step: 4
Training loss: 2.625361442565918
Validation loss: 2.364535206107683

Epoch: 6| Step: 5
Training loss: 3.1197381019592285
Validation loss: 2.3666872247572868

Epoch: 6| Step: 6
Training loss: 2.8636624813079834
Validation loss: 2.3674417644418697

Epoch: 6| Step: 7
Training loss: 2.418070077896118
Validation loss: 2.3631676858471287

Epoch: 6| Step: 8
Training loss: 3.1552834510803223
Validation loss: 2.3664673015635502

Epoch: 6| Step: 9
Training loss: 2.3922808170318604
Validation loss: 2.3633059276047574

Epoch: 6| Step: 10
Training loss: 2.4275169372558594
Validation loss: 2.360472591974402

Epoch: 6| Step: 11
Training loss: 1.912679672241211
Validation loss: 2.365075621553647

Epoch: 6| Step: 12
Training loss: 2.071521282196045
Validation loss: 2.366960040984615

Epoch: 6| Step: 13
Training loss: 3.4258861541748047
Validation loss: 2.3736298545714347

Epoch: 134| Step: 0
Training loss: 2.134214162826538
Validation loss: 2.381112716531241

Epoch: 6| Step: 1
Training loss: 3.2514140605926514
Validation loss: 2.380383899134974

Epoch: 6| Step: 2
Training loss: 2.522989511489868
Validation loss: 2.3942977049017466

Epoch: 6| Step: 3
Training loss: 2.617082357406616
Validation loss: 2.3986581807495444

Epoch: 6| Step: 4
Training loss: 2.673938751220703
Validation loss: 2.401665254305768

Epoch: 6| Step: 5
Training loss: 2.602900981903076
Validation loss: 2.415088935564923

Epoch: 6| Step: 6
Training loss: 2.2913012504577637
Validation loss: 2.4143668387525823

Epoch: 6| Step: 7
Training loss: 3.2522716522216797
Validation loss: 2.4237963102197133

Epoch: 6| Step: 8
Training loss: 2.904911518096924
Validation loss: 2.410145510909378

Epoch: 6| Step: 9
Training loss: 2.3208227157592773
Validation loss: 2.4148008233757428

Epoch: 6| Step: 10
Training loss: 1.8969802856445312
Validation loss: 2.4009553899047194

Epoch: 6| Step: 11
Training loss: 3.064793109893799
Validation loss: 2.393154472433111

Epoch: 6| Step: 12
Training loss: 2.0232887268066406
Validation loss: 2.3897663265146236

Epoch: 6| Step: 13
Training loss: 3.1099026203155518
Validation loss: 2.3900152508930494

Epoch: 135| Step: 0
Training loss: 2.132848024368286
Validation loss: 2.3822439024525304

Epoch: 6| Step: 1
Training loss: 2.413095474243164
Validation loss: 2.3830472705184773

Epoch: 6| Step: 2
Training loss: 2.631422996520996
Validation loss: 2.380684765436316

Epoch: 6| Step: 3
Training loss: 2.947432041168213
Validation loss: 2.3789088367134013

Epoch: 6| Step: 4
Training loss: 3.0739192962646484
Validation loss: 2.3723240795955864

Epoch: 6| Step: 5
Training loss: 2.750716209411621
Validation loss: 2.3686458885028796

Epoch: 6| Step: 6
Training loss: 3.249561309814453
Validation loss: 2.366889415248748

Epoch: 6| Step: 7
Training loss: 2.0433032512664795
Validation loss: 2.3582938717257593

Epoch: 6| Step: 8
Training loss: 2.3097405433654785
Validation loss: 2.354929188246368

Epoch: 6| Step: 9
Training loss: 2.4185924530029297
Validation loss: 2.3515594159403155

Epoch: 6| Step: 10
Training loss: 2.2341060638427734
Validation loss: 2.3524137863548855

Epoch: 6| Step: 11
Training loss: 2.2656259536743164
Validation loss: 2.351312768074774

Epoch: 6| Step: 12
Training loss: 3.158961534500122
Validation loss: 2.352450095197206

Epoch: 6| Step: 13
Training loss: 2.6133689880371094
Validation loss: 2.357421395599201

Epoch: 136| Step: 0
Training loss: 1.8774428367614746
Validation loss: 2.3551490460672686

Epoch: 6| Step: 1
Training loss: 3.38175106048584
Validation loss: 2.359547104886783

Epoch: 6| Step: 2
Training loss: 1.9211711883544922
Validation loss: 2.357283248696276

Epoch: 6| Step: 3
Training loss: 2.5726985931396484
Validation loss: 2.3589151931065384

Epoch: 6| Step: 4
Training loss: 2.5248160362243652
Validation loss: 2.361665218107162

Epoch: 6| Step: 5
Training loss: 2.9821770191192627
Validation loss: 2.3640244327565676

Epoch: 6| Step: 6
Training loss: 2.6203598976135254
Validation loss: 2.3579342903629428

Epoch: 6| Step: 7
Training loss: 2.7707810401916504
Validation loss: 2.366875494680097

Epoch: 6| Step: 8
Training loss: 2.7245564460754395
Validation loss: 2.366742208439817

Epoch: 6| Step: 9
Training loss: 2.7582015991210938
Validation loss: 2.371289950545116

Epoch: 6| Step: 10
Training loss: 1.9720361232757568
Validation loss: 2.370136441722993

Epoch: 6| Step: 11
Training loss: 2.2868003845214844
Validation loss: 2.3770627001280427

Epoch: 6| Step: 12
Training loss: 2.607318162918091
Validation loss: 2.374868526253649

Epoch: 6| Step: 13
Training loss: 3.7246525287628174
Validation loss: 2.3761430504501506

Epoch: 137| Step: 0
Training loss: 2.589843511581421
Validation loss: 2.3772246196705806

Epoch: 6| Step: 1
Training loss: 3.261080265045166
Validation loss: 2.3671074093029065

Epoch: 6| Step: 2
Training loss: 2.3863110542297363
Validation loss: 2.3618797358646186

Epoch: 6| Step: 3
Training loss: 2.4603841304779053
Validation loss: 2.3574009608196955

Epoch: 6| Step: 4
Training loss: 2.445042848587036
Validation loss: 2.3612156093761487

Epoch: 6| Step: 5
Training loss: 2.7851171493530273
Validation loss: 2.359968521261728

Epoch: 6| Step: 6
Training loss: 1.8332332372665405
Validation loss: 2.3645912575465378

Epoch: 6| Step: 7
Training loss: 2.735565662384033
Validation loss: 2.362789048943468

Epoch: 6| Step: 8
Training loss: 2.649904251098633
Validation loss: 2.362008569061115

Epoch: 6| Step: 9
Training loss: 2.7667794227600098
Validation loss: 2.3660936855500743

Epoch: 6| Step: 10
Training loss: 2.646181583404541
Validation loss: 2.3685633956745105

Epoch: 6| Step: 11
Training loss: 2.5028553009033203
Validation loss: 2.3698520070763043

Epoch: 6| Step: 12
Training loss: 2.707033157348633
Validation loss: 2.3766324520111084

Epoch: 6| Step: 13
Training loss: 2.221493721008301
Validation loss: 2.376836253750709

Epoch: 138| Step: 0
Training loss: 2.6473474502563477
Validation loss: 2.3625239608108357

Epoch: 6| Step: 1
Training loss: 2.6159827709198
Validation loss: 2.36979700544829

Epoch: 6| Step: 2
Training loss: 2.1732192039489746
Validation loss: 2.368621298061904

Epoch: 6| Step: 3
Training loss: 2.116870641708374
Validation loss: 2.368231991285919

Epoch: 6| Step: 4
Training loss: 2.6999194622039795
Validation loss: 2.3723363158523396

Epoch: 6| Step: 5
Training loss: 2.917128562927246
Validation loss: 2.3702615999406382

Epoch: 6| Step: 6
Training loss: 2.84956955909729
Validation loss: 2.374436204151441

Epoch: 6| Step: 7
Training loss: 2.562805414199829
Validation loss: 2.381051368610833

Epoch: 6| Step: 8
Training loss: 2.608051300048828
Validation loss: 2.3822916374411633

Epoch: 6| Step: 9
Training loss: 2.9179129600524902
Validation loss: 2.38472120864417

Epoch: 6| Step: 10
Training loss: 1.9257118701934814
Validation loss: 2.3741483457626833

Epoch: 6| Step: 11
Training loss: 2.9250705242156982
Validation loss: 2.3771883159555416

Epoch: 6| Step: 12
Training loss: 2.8078114986419678
Validation loss: 2.35689115524292

Epoch: 6| Step: 13
Training loss: 2.429645299911499
Validation loss: 2.3513665635098695

Epoch: 139| Step: 0
Training loss: 2.138197422027588
Validation loss: 2.343823232958394

Epoch: 6| Step: 1
Training loss: 2.5194523334503174
Validation loss: 2.343869306707895

Epoch: 6| Step: 2
Training loss: 2.5768048763275146
Validation loss: 2.3360551070141535

Epoch: 6| Step: 3
Training loss: 2.8357720375061035
Validation loss: 2.3399120094955608

Epoch: 6| Step: 4
Training loss: 2.811138153076172
Validation loss: 2.347977638244629

Epoch: 6| Step: 5
Training loss: 2.334000587463379
Validation loss: 2.3495994614016626

Epoch: 6| Step: 6
Training loss: 3.406492233276367
Validation loss: 2.349451567537041

Epoch: 6| Step: 7
Training loss: 1.8797721862792969
Validation loss: 2.3534472450133292

Epoch: 6| Step: 8
Training loss: 2.6815812587738037
Validation loss: 2.3524071196074128

Epoch: 6| Step: 9
Training loss: 1.8775783777236938
Validation loss: 2.35096594466958

Epoch: 6| Step: 10
Training loss: 3.1732075214385986
Validation loss: 2.3535845254057195

Epoch: 6| Step: 11
Training loss: 2.479135036468506
Validation loss: 2.3474866241537113

Epoch: 6| Step: 12
Training loss: 2.7718698978424072
Validation loss: 2.347280066500428

Epoch: 6| Step: 13
Training loss: 2.783909559249878
Validation loss: 2.3387209394926667

Epoch: 140| Step: 0
Training loss: 3.0576303005218506
Validation loss: 2.341824021390689

Epoch: 6| Step: 1
Training loss: 2.2213149070739746
Validation loss: 2.3451979339763684

Epoch: 6| Step: 2
Training loss: 2.491178512573242
Validation loss: 2.3629549446926323

Epoch: 6| Step: 3
Training loss: 2.550339698791504
Validation loss: 2.3648661887773903

Epoch: 6| Step: 4
Training loss: 2.090996503829956
Validation loss: 2.357343012286771

Epoch: 6| Step: 5
Training loss: 3.155120372772217
Validation loss: 2.342560306672127

Epoch: 6| Step: 6
Training loss: 1.8817155361175537
Validation loss: 2.338134878425188

Epoch: 6| Step: 7
Training loss: 2.1313579082489014
Validation loss: 2.3379922605329946

Epoch: 6| Step: 8
Training loss: 2.834057331085205
Validation loss: 2.3349654828348467

Epoch: 6| Step: 9
Training loss: 2.825824737548828
Validation loss: 2.3309111261880524

Epoch: 6| Step: 10
Training loss: 3.017643690109253
Validation loss: 2.325116329295661

Epoch: 6| Step: 11
Training loss: 3.1974124908447266
Validation loss: 2.324997148206157

Epoch: 6| Step: 12
Training loss: 2.5873289108276367
Validation loss: 2.3265795630793416

Epoch: 6| Step: 13
Training loss: 1.7456660270690918
Validation loss: 2.3265096961811023

Epoch: 141| Step: 0
Training loss: 1.8471332788467407
Validation loss: 2.333767380765689

Epoch: 6| Step: 1
Training loss: 2.826089382171631
Validation loss: 2.3443033361947663

Epoch: 6| Step: 2
Training loss: 2.8480963706970215
Validation loss: 2.3604510727749077

Epoch: 6| Step: 3
Training loss: 2.397993803024292
Validation loss: 2.37997071461011

Epoch: 6| Step: 4
Training loss: 2.4707117080688477
Validation loss: 2.405067482302266

Epoch: 6| Step: 5
Training loss: 3.041104555130005
Validation loss: 2.423927061019405

Epoch: 6| Step: 6
Training loss: 2.767487049102783
Validation loss: 2.4396076945848364

Epoch: 6| Step: 7
Training loss: 2.053070545196533
Validation loss: 2.4231839795266428

Epoch: 6| Step: 8
Training loss: 2.8951256275177
Validation loss: 2.4058911492747646

Epoch: 6| Step: 9
Training loss: 2.980698347091675
Validation loss: 2.37512755650346

Epoch: 6| Step: 10
Training loss: 2.743107795715332
Validation loss: 2.351021700007941

Epoch: 6| Step: 11
Training loss: 2.8643574714660645
Validation loss: 2.335375000071782

Epoch: 6| Step: 12
Training loss: 2.243603467941284
Validation loss: 2.3209200495032856

Epoch: 6| Step: 13
Training loss: 2.274834632873535
Validation loss: 2.318022979203091

Epoch: 142| Step: 0
Training loss: 2.256356716156006
Validation loss: 2.316615168766309

Epoch: 6| Step: 1
Training loss: 3.0714073181152344
Validation loss: 2.318115190793109

Epoch: 6| Step: 2
Training loss: 2.4283318519592285
Validation loss: 2.3233521856287473

Epoch: 6| Step: 3
Training loss: 3.1562108993530273
Validation loss: 2.320942022467172

Epoch: 6| Step: 4
Training loss: 2.567903518676758
Validation loss: 2.323667990264072

Epoch: 6| Step: 5
Training loss: 3.1601483821868896
Validation loss: 2.320698763734551

Epoch: 6| Step: 6
Training loss: 2.1945388317108154
Validation loss: 2.325445290534727

Epoch: 6| Step: 7
Training loss: 2.68289852142334
Validation loss: 2.3284720502873903

Epoch: 6| Step: 8
Training loss: 1.8369464874267578
Validation loss: 2.3206497366710375

Epoch: 6| Step: 9
Training loss: 2.9096484184265137
Validation loss: 2.3263637353015203

Epoch: 6| Step: 10
Training loss: 3.187121868133545
Validation loss: 2.3267959599853842

Epoch: 6| Step: 11
Training loss: 1.8193837404251099
Validation loss: 2.3186059741563696

Epoch: 6| Step: 12
Training loss: 2.3631062507629395
Validation loss: 2.320164185698314

Epoch: 6| Step: 13
Training loss: 2.67641019821167
Validation loss: 2.3307260390250915

Epoch: 143| Step: 0
Training loss: 2.4631898403167725
Validation loss: 2.33039596772963

Epoch: 6| Step: 1
Training loss: 3.8743667602539062
Validation loss: 2.346522590165497

Epoch: 6| Step: 2
Training loss: 2.1332311630249023
Validation loss: 2.354615642178443

Epoch: 6| Step: 3
Training loss: 2.3339061737060547
Validation loss: 2.3475381828123525

Epoch: 6| Step: 4
Training loss: 2.3601040840148926
Validation loss: 2.3446868568338375

Epoch: 6| Step: 5
Training loss: 1.445076823234558
Validation loss: 2.35046600526379

Epoch: 6| Step: 6
Training loss: 3.12158203125
Validation loss: 2.3518490637502363

Epoch: 6| Step: 7
Training loss: 3.2006852626800537
Validation loss: 2.359603158889278

Epoch: 6| Step: 8
Training loss: 2.02168869972229
Validation loss: 2.358584224536855

Epoch: 6| Step: 9
Training loss: 2.5804283618927
Validation loss: 2.3554587748742875

Epoch: 6| Step: 10
Training loss: 2.6908578872680664
Validation loss: 2.354925150512367

Epoch: 6| Step: 11
Training loss: 2.8565514087677
Validation loss: 2.3361198671402468

Epoch: 6| Step: 12
Training loss: 2.643622398376465
Validation loss: 2.322770385332005

Epoch: 6| Step: 13
Training loss: 2.5412042140960693
Validation loss: 2.312907244569512

Epoch: 144| Step: 0
Training loss: 2.688167095184326
Validation loss: 2.310037823133571

Epoch: 6| Step: 1
Training loss: 2.3351194858551025
Validation loss: 2.3056920561739194

Epoch: 6| Step: 2
Training loss: 2.553266763687134
Validation loss: 2.304365719518354

Epoch: 6| Step: 3
Training loss: 2.2418558597564697
Validation loss: 2.3057237286721506

Epoch: 6| Step: 4
Training loss: 2.317960262298584
Validation loss: 2.3031694004612584

Epoch: 6| Step: 5
Training loss: 2.131976842880249
Validation loss: 2.303823668469665

Epoch: 6| Step: 6
Training loss: 2.5414445400238037
Validation loss: 2.3051726766811904

Epoch: 6| Step: 7
Training loss: 2.723898410797119
Validation loss: 2.304063497051116

Epoch: 6| Step: 8
Training loss: 2.4296960830688477
Validation loss: 2.3073564524291665

Epoch: 6| Step: 9
Training loss: 2.7502334117889404
Validation loss: 2.3070577190768335

Epoch: 6| Step: 10
Training loss: 2.438913345336914
Validation loss: 2.307532597613591

Epoch: 6| Step: 11
Training loss: 3.043264627456665
Validation loss: 2.3142574602557766

Epoch: 6| Step: 12
Training loss: 2.9430532455444336
Validation loss: 2.311963083923504

Epoch: 6| Step: 13
Training loss: 3.17122745513916
Validation loss: 2.3187379093580347

Epoch: 145| Step: 0
Training loss: 3.574329376220703
Validation loss: 2.3235345322598695

Epoch: 6| Step: 1
Training loss: 2.201685905456543
Validation loss: 2.3274432407912387

Epoch: 6| Step: 2
Training loss: 2.446481704711914
Validation loss: 2.3435255955624323

Epoch: 6| Step: 3
Training loss: 2.723379373550415
Validation loss: 2.3405316132371143

Epoch: 6| Step: 4
Training loss: 2.839649200439453
Validation loss: 2.348985836070071

Epoch: 6| Step: 5
Training loss: 2.961369752883911
Validation loss: 2.352923934177686

Epoch: 6| Step: 6
Training loss: 2.76218843460083
Validation loss: 2.359461868962934

Epoch: 6| Step: 7
Training loss: 2.249105453491211
Validation loss: 2.356790219583819

Epoch: 6| Step: 8
Training loss: 2.2239761352539062
Validation loss: 2.3554808555110807

Epoch: 6| Step: 9
Training loss: 2.5487871170043945
Validation loss: 2.3417101419100197

Epoch: 6| Step: 10
Training loss: 2.3444619178771973
Validation loss: 2.3325250635864916

Epoch: 6| Step: 11
Training loss: 2.2769317626953125
Validation loss: 2.3338158592101066

Epoch: 6| Step: 12
Training loss: 2.2066407203674316
Validation loss: 2.3266961369463193

Epoch: 6| Step: 13
Training loss: 2.8650426864624023
Validation loss: 2.326420791687504

Epoch: 146| Step: 0
Training loss: 2.411545753479004
Validation loss: 2.3387341550601426

Epoch: 6| Step: 1
Training loss: 2.78532075881958
Validation loss: 2.334759350745909

Epoch: 6| Step: 2
Training loss: 2.258145809173584
Validation loss: 2.336780245586108

Epoch: 6| Step: 3
Training loss: 2.147831439971924
Validation loss: 2.3432684483066684

Epoch: 6| Step: 4
Training loss: 2.348684787750244
Validation loss: 2.35052934000569

Epoch: 6| Step: 5
Training loss: 2.7136354446411133
Validation loss: 2.3547673597130725

Epoch: 6| Step: 6
Training loss: 1.8216968774795532
Validation loss: 2.383584407068068

Epoch: 6| Step: 7
Training loss: 1.8394651412963867
Validation loss: 2.413889108165618

Epoch: 6| Step: 8
Training loss: 3.134549617767334
Validation loss: 2.402869329657606

Epoch: 6| Step: 9
Training loss: 3.205331802368164
Validation loss: 2.393543497208626

Epoch: 6| Step: 10
Training loss: 2.551636219024658
Validation loss: 2.3586015983294417

Epoch: 6| Step: 11
Training loss: 3.2829694747924805
Validation loss: 2.3405213740564164

Epoch: 6| Step: 12
Training loss: 2.411545753479004
Validation loss: 2.319023673252393

Epoch: 6| Step: 13
Training loss: 3.454129934310913
Validation loss: 2.3208843751620223

Epoch: 147| Step: 0
Training loss: 2.32755446434021
Validation loss: 2.3107847936691774

Epoch: 6| Step: 1
Training loss: 2.8640618324279785
Validation loss: 2.3113614564300864

Epoch: 6| Step: 2
Training loss: 2.7049155235290527
Validation loss: 2.3169039718566404

Epoch: 6| Step: 3
Training loss: 2.469224452972412
Validation loss: 2.316266523894443

Epoch: 6| Step: 4
Training loss: 2.480377674102783
Validation loss: 2.317586565530428

Epoch: 6| Step: 5
Training loss: 2.4635164737701416
Validation loss: 2.316324177608695

Epoch: 6| Step: 6
Training loss: 2.8794450759887695
Validation loss: 2.338456230778848

Epoch: 6| Step: 7
Training loss: 2.297123432159424
Validation loss: 2.346204532090054

Epoch: 6| Step: 8
Training loss: 2.699031114578247
Validation loss: 2.3705583157077914

Epoch: 6| Step: 9
Training loss: 2.6094868183135986
Validation loss: 2.337921714269987

Epoch: 6| Step: 10
Training loss: 2.795142650604248
Validation loss: 2.3310961672054824

Epoch: 6| Step: 11
Training loss: 2.2892906665802
Validation loss: 2.321983127183812

Epoch: 6| Step: 12
Training loss: 2.5877397060394287
Validation loss: 2.3373392243539133

Epoch: 6| Step: 13
Training loss: 2.912449836730957
Validation loss: 2.3428047600612847

Epoch: 148| Step: 0
Training loss: 2.110738754272461
Validation loss: 2.3538220159469114

Epoch: 6| Step: 1
Training loss: 3.069471836090088
Validation loss: 2.3504462293399278

Epoch: 6| Step: 2
Training loss: 2.479029655456543
Validation loss: 2.3514813043737925

Epoch: 6| Step: 3
Training loss: 2.8662991523742676
Validation loss: 2.348382483246506

Epoch: 6| Step: 4
Training loss: 2.9487295150756836
Validation loss: 2.348369626588719

Epoch: 6| Step: 5
Training loss: 2.4783482551574707
Validation loss: 2.353427920290219

Epoch: 6| Step: 6
Training loss: 2.0030136108398438
Validation loss: 2.346612568824522

Epoch: 6| Step: 7
Training loss: 2.5119316577911377
Validation loss: 2.342039296703954

Epoch: 6| Step: 8
Training loss: 2.482990264892578
Validation loss: 2.345665475373627

Epoch: 6| Step: 9
Training loss: 2.682051420211792
Validation loss: 2.344150330430718

Epoch: 6| Step: 10
Training loss: 2.4439539909362793
Validation loss: 2.3587338386043424

Epoch: 6| Step: 11
Training loss: 2.3444910049438477
Validation loss: 2.3606011713704755

Epoch: 6| Step: 12
Training loss: 2.8623104095458984
Validation loss: 2.36360917809189

Epoch: 6| Step: 13
Training loss: 2.7029669284820557
Validation loss: 2.3519849956676526

Epoch: 149| Step: 0
Training loss: 1.6263551712036133
Validation loss: 2.349844260882306

Epoch: 6| Step: 1
Training loss: 3.3354549407958984
Validation loss: 2.335491006092359

Epoch: 6| Step: 2
Training loss: 3.3085777759552
Validation loss: 2.3353151762357323

Epoch: 6| Step: 3
Training loss: 2.228994369506836
Validation loss: 2.334870781949771

Epoch: 6| Step: 4
Training loss: 3.089517831802368
Validation loss: 2.3358069466006373

Epoch: 6| Step: 5
Training loss: 2.3654162883758545
Validation loss: 2.327476232282577

Epoch: 6| Step: 6
Training loss: 1.9286338090896606
Validation loss: 2.333529228805214

Epoch: 6| Step: 7
Training loss: 2.3894405364990234
Validation loss: 2.3347433433737805

Epoch: 6| Step: 8
Training loss: 2.7557339668273926
Validation loss: 2.339423615445373

Epoch: 6| Step: 9
Training loss: 2.539637565612793
Validation loss: 2.337689212573472

Epoch: 6| Step: 10
Training loss: 2.9919967651367188
Validation loss: 2.3367276627530336

Epoch: 6| Step: 11
Training loss: 1.805555820465088
Validation loss: 2.3374108422187065

Epoch: 6| Step: 12
Training loss: 2.884333610534668
Validation loss: 2.329836195515048

Epoch: 6| Step: 13
Training loss: 2.373347282409668
Validation loss: 2.334265280795354

Epoch: 150| Step: 0
Training loss: 2.528993606567383
Validation loss: 2.3413223092274

Epoch: 6| Step: 1
Training loss: 2.1300954818725586
Validation loss: 2.341250733662677

Epoch: 6| Step: 2
Training loss: 3.3993654251098633
Validation loss: 2.34157314608174

Epoch: 6| Step: 3
Training loss: 2.794229745864868
Validation loss: 2.3325989143822783

Epoch: 6| Step: 4
Training loss: 2.5319736003875732
Validation loss: 2.3363067385970906

Epoch: 6| Step: 5
Training loss: 3.1608846187591553
Validation loss: 2.3448790273358746

Epoch: 6| Step: 6
Training loss: 1.7585158348083496
Validation loss: 2.339820414461115

Epoch: 6| Step: 7
Training loss: 2.706948757171631
Validation loss: 2.337067773265223

Epoch: 6| Step: 8
Training loss: 2.335998058319092
Validation loss: 2.339709025557323

Epoch: 6| Step: 9
Training loss: 2.3342955112457275
Validation loss: 2.3443455234650643

Epoch: 6| Step: 10
Training loss: 2.510340929031372
Validation loss: 2.345693326765491

Epoch: 6| Step: 11
Training loss: 2.597508430480957
Validation loss: 2.338936769834129

Epoch: 6| Step: 12
Training loss: 2.6401238441467285
Validation loss: 2.3380453484032744

Epoch: 6| Step: 13
Training loss: 1.8820538520812988
Validation loss: 2.328525371448968

Epoch: 151| Step: 0
Training loss: 2.785236120223999
Validation loss: 2.3260857097564207

Epoch: 6| Step: 1
Training loss: 3.133983850479126
Validation loss: 2.3342000617775867

Epoch: 6| Step: 2
Training loss: 2.8484010696411133
Validation loss: 2.3290389609593216

Epoch: 6| Step: 3
Training loss: 2.137202262878418
Validation loss: 2.332095971671484

Epoch: 6| Step: 4
Training loss: 2.5234053134918213
Validation loss: 2.332545562457013

Epoch: 6| Step: 5
Training loss: 2.7013940811157227
Validation loss: 2.3420773526673675

Epoch: 6| Step: 6
Training loss: 2.2620484828948975
Validation loss: 2.3483197483965146

Epoch: 6| Step: 7
Training loss: 1.815335988998413
Validation loss: 2.3580779542205152

Epoch: 6| Step: 8
Training loss: 2.327709674835205
Validation loss: 2.3639054452219317

Epoch: 6| Step: 9
Training loss: 2.6070492267608643
Validation loss: 2.3696265477006153

Epoch: 6| Step: 10
Training loss: 3.460784912109375
Validation loss: 2.3726612855029363

Epoch: 6| Step: 11
Training loss: 2.811793804168701
Validation loss: 2.36306930101046

Epoch: 6| Step: 12
Training loss: 1.6711838245391846
Validation loss: 2.3450591230905182

Epoch: 6| Step: 13
Training loss: 2.6168735027313232
Validation loss: 2.335509133595292

Epoch: 152| Step: 0
Training loss: 2.3403236865997314
Validation loss: 2.317276580359346

Epoch: 6| Step: 1
Training loss: 2.724107265472412
Validation loss: 2.3133236746634207

Epoch: 6| Step: 2
Training loss: 2.480898380279541
Validation loss: 2.3006575799757436

Epoch: 6| Step: 3
Training loss: 1.4123544692993164
Validation loss: 2.2918566093649915

Epoch: 6| Step: 4
Training loss: 1.608665108680725
Validation loss: 2.2864438885001728

Epoch: 6| Step: 5
Training loss: 2.5652341842651367
Validation loss: 2.275322380886283

Epoch: 6| Step: 6
Training loss: 2.8521406650543213
Validation loss: 2.2795355550704466

Epoch: 6| Step: 7
Training loss: 3.7697997093200684
Validation loss: 2.2851864676321707

Epoch: 6| Step: 8
Training loss: 3.1901028156280518
Validation loss: 2.284489602170965

Epoch: 6| Step: 9
Training loss: 2.701549768447876
Validation loss: 2.2856378529661443

Epoch: 6| Step: 10
Training loss: 2.831742286682129
Validation loss: 2.2885415015682096

Epoch: 6| Step: 11
Training loss: 2.3164141178131104
Validation loss: 2.28530385673687

Epoch: 6| Step: 12
Training loss: 2.891434669494629
Validation loss: 2.279592506347164

Epoch: 6| Step: 13
Training loss: 2.157531499862671
Validation loss: 2.2789240139786915

Epoch: 153| Step: 0
Training loss: 1.9730467796325684
Validation loss: 2.2714634403105705

Epoch: 6| Step: 1
Training loss: 2.145911931991577
Validation loss: 2.270094553629557

Epoch: 6| Step: 2
Training loss: 3.160414457321167
Validation loss: 2.271871910300306

Epoch: 6| Step: 3
Training loss: 2.12290096282959
Validation loss: 2.2751177921090076

Epoch: 6| Step: 4
Training loss: 2.9423022270202637
Validation loss: 2.2824142876491753

Epoch: 6| Step: 5
Training loss: 2.9691274166107178
Validation loss: 2.2897393524005847

Epoch: 6| Step: 6
Training loss: 2.9044268131256104
Validation loss: 2.3066441602604364

Epoch: 6| Step: 7
Training loss: 2.8316497802734375
Validation loss: 2.3023995763512066

Epoch: 6| Step: 8
Training loss: 2.0470993518829346
Validation loss: 2.309523126130463

Epoch: 6| Step: 9
Training loss: 3.05324649810791
Validation loss: 2.306066846334806

Epoch: 6| Step: 10
Training loss: 2.5985031127929688
Validation loss: 2.2973929989722466

Epoch: 6| Step: 11
Training loss: 2.491353988647461
Validation loss: 2.2918410249935683

Epoch: 6| Step: 12
Training loss: 1.8598387241363525
Validation loss: 2.2903448561186432

Epoch: 6| Step: 13
Training loss: 2.904247999191284
Validation loss: 2.2805955051093973

Epoch: 154| Step: 0
Training loss: 2.354660987854004
Validation loss: 2.2854275908521426

Epoch: 6| Step: 1
Training loss: 2.9992127418518066
Validation loss: 2.284251828347483

Epoch: 6| Step: 2
Training loss: 2.4592559337615967
Validation loss: 2.2886421372813563

Epoch: 6| Step: 3
Training loss: 2.0291638374328613
Validation loss: 2.280462787997338

Epoch: 6| Step: 4
Training loss: 2.6915359497070312
Validation loss: 2.282495762712212

Epoch: 6| Step: 5
Training loss: 3.096656322479248
Validation loss: 2.276390278211204

Epoch: 6| Step: 6
Training loss: 2.061584234237671
Validation loss: 2.27604418159813

Epoch: 6| Step: 7
Training loss: 2.8136026859283447
Validation loss: 2.275601056314284

Epoch: 6| Step: 8
Training loss: 2.8691892623901367
Validation loss: 2.2776818788179787

Epoch: 6| Step: 9
Training loss: 1.9325015544891357
Validation loss: 2.283895687390399

Epoch: 6| Step: 10
Training loss: 2.442450523376465
Validation loss: 2.2910616192766415

Epoch: 6| Step: 11
Training loss: 2.5634989738464355
Validation loss: 2.300777683975876

Epoch: 6| Step: 12
Training loss: 2.6446871757507324
Validation loss: 2.3233059760062926

Epoch: 6| Step: 13
Training loss: 2.90775990486145
Validation loss: 2.352766720197534

Epoch: 155| Step: 0
Training loss: 2.900017261505127
Validation loss: 2.357172135383852

Epoch: 6| Step: 1
Training loss: 2.45967960357666
Validation loss: 2.3768603006998696

Epoch: 6| Step: 2
Training loss: 3.0386881828308105
Validation loss: 2.391837309765559

Epoch: 6| Step: 3
Training loss: 2.2379562854766846
Validation loss: 2.4063238713049118

Epoch: 6| Step: 4
Training loss: 2.1038851737976074
Validation loss: 2.454476443670129

Epoch: 6| Step: 5
Training loss: 2.221609354019165
Validation loss: 2.519491540488376

Epoch: 6| Step: 6
Training loss: 2.219149112701416
Validation loss: 2.54192086701752

Epoch: 6| Step: 7
Training loss: 2.9707181453704834
Validation loss: 2.5046971203178487

Epoch: 6| Step: 8
Training loss: 2.5761632919311523
Validation loss: 2.4199254076967955

Epoch: 6| Step: 9
Training loss: 3.00225830078125
Validation loss: 2.354934976946923

Epoch: 6| Step: 10
Training loss: 2.5743322372436523
Validation loss: 2.323326777386409

Epoch: 6| Step: 11
Training loss: 2.934638023376465
Validation loss: 2.3107327338187926

Epoch: 6| Step: 12
Training loss: 2.4758450984954834
Validation loss: 2.2955022729853147

Epoch: 6| Step: 13
Training loss: 2.2271242141723633
Validation loss: 2.286725926142867

Epoch: 156| Step: 0
Training loss: 2.697303295135498
Validation loss: 2.2969882180613856

Epoch: 6| Step: 1
Training loss: 2.6307687759399414
Validation loss: 2.2973260136060816

Epoch: 6| Step: 2
Training loss: 3.4150681495666504
Validation loss: 2.307169602763268

Epoch: 6| Step: 3
Training loss: 2.2150540351867676
Validation loss: 2.2989553072119273

Epoch: 6| Step: 4
Training loss: 2.949113130569458
Validation loss: 2.2818088070038827

Epoch: 6| Step: 5
Training loss: 3.0301198959350586
Validation loss: 2.286724467431345

Epoch: 6| Step: 6
Training loss: 2.1276583671569824
Validation loss: 2.309935421072027

Epoch: 6| Step: 7
Training loss: 3.089540481567383
Validation loss: 2.3488522063019457

Epoch: 6| Step: 8
Training loss: 1.575704574584961
Validation loss: 2.3151449490618963

Epoch: 6| Step: 9
Training loss: 2.031259059906006
Validation loss: 2.299637166402673

Epoch: 6| Step: 10
Training loss: 2.897115707397461
Validation loss: 2.280195718170494

Epoch: 6| Step: 11
Training loss: 2.6233954429626465
Validation loss: 2.2759410745354107

Epoch: 6| Step: 12
Training loss: 2.641988515853882
Validation loss: 2.2660815715789795

Epoch: 6| Step: 13
Training loss: 1.9345003366470337
Validation loss: 2.2620448553433983

Epoch: 157| Step: 0
Training loss: 2.819143772125244
Validation loss: 2.2651759398880826

Epoch: 6| Step: 1
Training loss: 3.442821979522705
Validation loss: 2.2824545573162776

Epoch: 6| Step: 2
Training loss: 2.513019323348999
Validation loss: 2.293768967351606

Epoch: 6| Step: 3
Training loss: 1.5879747867584229
Validation loss: 2.3136999299449306

Epoch: 6| Step: 4
Training loss: 2.0479087829589844
Validation loss: 2.327229094761674

Epoch: 6| Step: 5
Training loss: 2.34689998626709
Validation loss: 2.324894901244871

Epoch: 6| Step: 6
Training loss: 2.713818073272705
Validation loss: 2.3151757870951006

Epoch: 6| Step: 7
Training loss: 2.9095141887664795
Validation loss: 2.322683411259805

Epoch: 6| Step: 8
Training loss: 2.7919809818267822
Validation loss: 2.319143100451398

Epoch: 6| Step: 9
Training loss: 2.828251361846924
Validation loss: 2.314912587083796

Epoch: 6| Step: 10
Training loss: 1.902844786643982
Validation loss: 2.329432850242943

Epoch: 6| Step: 11
Training loss: 2.3582448959350586
Validation loss: 2.3905765497556297

Epoch: 6| Step: 12
Training loss: 2.5425491333007812
Validation loss: 2.435559929058116

Epoch: 6| Step: 13
Training loss: 3.046210289001465
Validation loss: 2.425860056313135

Epoch: 158| Step: 0
Training loss: 2.04017972946167
Validation loss: 2.4780011356517835

Epoch: 6| Step: 1
Training loss: 2.239340305328369
Validation loss: 2.4622691587735246

Epoch: 6| Step: 2
Training loss: 2.1397175788879395
Validation loss: 2.4559285076715613

Epoch: 6| Step: 3
Training loss: 2.823198080062866
Validation loss: 2.4271007737805768

Epoch: 6| Step: 4
Training loss: 2.513462781906128
Validation loss: 2.396973092068908

Epoch: 6| Step: 5
Training loss: 1.9538533687591553
Validation loss: 2.3823741353968138

Epoch: 6| Step: 6
Training loss: 3.1144604682922363
Validation loss: 2.3599006834850518

Epoch: 6| Step: 7
Training loss: 2.3049750328063965
Validation loss: 2.325997829437256

Epoch: 6| Step: 8
Training loss: 2.820328950881958
Validation loss: 2.3054306660929034

Epoch: 6| Step: 9
Training loss: 2.8498964309692383
Validation loss: 2.28043725926389

Epoch: 6| Step: 10
Training loss: 2.8087823390960693
Validation loss: 2.272091075938235

Epoch: 6| Step: 11
Training loss: 3.4071905612945557
Validation loss: 2.2689770831856677

Epoch: 6| Step: 12
Training loss: 2.502342700958252
Validation loss: 2.270449096156705

Epoch: 6| Step: 13
Training loss: 2.5690855979919434
Validation loss: 2.270635145966725

Epoch: 159| Step: 0
Training loss: 3.2898080348968506
Validation loss: 2.276115540535219

Epoch: 6| Step: 1
Training loss: 3.1176798343658447
Validation loss: 2.2707459811241395

Epoch: 6| Step: 2
Training loss: 2.3468430042266846
Validation loss: 2.273753550744826

Epoch: 6| Step: 3
Training loss: 2.5564165115356445
Validation loss: 2.2687692680666522

Epoch: 6| Step: 4
Training loss: 1.887976884841919
Validation loss: 2.266947386085346

Epoch: 6| Step: 5
Training loss: 2.272994041442871
Validation loss: 2.267862210991562

Epoch: 6| Step: 6
Training loss: 3.141556739807129
Validation loss: 2.270260774961082

Epoch: 6| Step: 7
Training loss: 3.1503567695617676
Validation loss: 2.277242109339724

Epoch: 6| Step: 8
Training loss: 2.8639121055603027
Validation loss: 2.279468758131868

Epoch: 6| Step: 9
Training loss: 1.91119384765625
Validation loss: 2.2806898932303152

Epoch: 6| Step: 10
Training loss: 1.8559420108795166
Validation loss: 2.2941170482225317

Epoch: 6| Step: 11
Training loss: 2.5341577529907227
Validation loss: 2.311003370951581

Epoch: 6| Step: 12
Training loss: 2.63956880569458
Validation loss: 2.3015306816306165

Epoch: 6| Step: 13
Training loss: 1.6966191530227661
Validation loss: 2.30337433661184

Epoch: 160| Step: 0
Training loss: 2.2813472747802734
Validation loss: 2.305186745940998

Epoch: 6| Step: 1
Training loss: 2.9433465003967285
Validation loss: 2.3168969333812757

Epoch: 6| Step: 2
Training loss: 2.1525063514709473
Validation loss: 2.302901537187638

Epoch: 6| Step: 3
Training loss: 2.372913122177124
Validation loss: 2.3006169616535144

Epoch: 6| Step: 4
Training loss: 3.2957372665405273
Validation loss: 2.3052416181051605

Epoch: 6| Step: 5
Training loss: 3.3255693912506104
Validation loss: 2.292202216322704

Epoch: 6| Step: 6
Training loss: 2.0937891006469727
Validation loss: 2.2826137440178984

Epoch: 6| Step: 7
Training loss: 2.6838161945343018
Validation loss: 2.273144955276161

Epoch: 6| Step: 8
Training loss: 2.227768898010254
Validation loss: 2.2710509069504274

Epoch: 6| Step: 9
Training loss: 2.390833854675293
Validation loss: 2.276618713973671

Epoch: 6| Step: 10
Training loss: 2.1133790016174316
Validation loss: 2.2760821311704573

Epoch: 6| Step: 11
Training loss: 2.292614221572876
Validation loss: 2.28147933560033

Epoch: 6| Step: 12
Training loss: 3.004375457763672
Validation loss: 2.2813825684209026

Epoch: 6| Step: 13
Training loss: 2.1722970008850098
Validation loss: 2.280849920806064

Epoch: 161| Step: 0
Training loss: 2.805903911590576
Validation loss: 2.277330331904914

Epoch: 6| Step: 1
Training loss: 2.563998222351074
Validation loss: 2.2733262508146224

Epoch: 6| Step: 2
Training loss: 2.482551336288452
Validation loss: 2.2751550469347226

Epoch: 6| Step: 3
Training loss: 1.7656562328338623
Validation loss: 2.2773599880997852

Epoch: 6| Step: 4
Training loss: 2.6044790744781494
Validation loss: 2.2839610448447605

Epoch: 6| Step: 5
Training loss: 2.5713388919830322
Validation loss: 2.2926958043088197

Epoch: 6| Step: 6
Training loss: 2.4774107933044434
Validation loss: 2.2863290720088507

Epoch: 6| Step: 7
Training loss: 2.92885160446167
Validation loss: 2.289388795052805

Epoch: 6| Step: 8
Training loss: 2.8459291458129883
Validation loss: 2.2944737583078365

Epoch: 6| Step: 9
Training loss: 2.8801896572113037
Validation loss: 2.2882142451501664

Epoch: 6| Step: 10
Training loss: 2.580336809158325
Validation loss: 2.289245141449795

Epoch: 6| Step: 11
Training loss: 2.029168128967285
Validation loss: 2.281833770454571

Epoch: 6| Step: 12
Training loss: 2.2817015647888184
Validation loss: 2.292229406295284

Epoch: 6| Step: 13
Training loss: 2.6333813667297363
Validation loss: 2.2949804157339115

Epoch: 162| Step: 0
Training loss: 2.9645166397094727
Validation loss: 2.292403150630254

Epoch: 6| Step: 1
Training loss: 3.1723668575286865
Validation loss: 2.298581559170959

Epoch: 6| Step: 2
Training loss: 2.676647663116455
Validation loss: 2.2979880404728714

Epoch: 6| Step: 3
Training loss: 2.721900463104248
Validation loss: 2.294254941325034

Epoch: 6| Step: 4
Training loss: 1.9700080156326294
Validation loss: 2.28947603061635

Epoch: 6| Step: 5
Training loss: 2.140909433364868
Validation loss: 2.29108146057334

Epoch: 6| Step: 6
Training loss: 2.246082067489624
Validation loss: 2.2888769385635213

Epoch: 6| Step: 7
Training loss: 2.393861770629883
Validation loss: 2.2908943724888626

Epoch: 6| Step: 8
Training loss: 2.222705364227295
Validation loss: 2.291486091511224

Epoch: 6| Step: 9
Training loss: 2.246976852416992
Validation loss: 2.2931425417623212

Epoch: 6| Step: 10
Training loss: 2.9128148555755615
Validation loss: 2.299444911300495

Epoch: 6| Step: 11
Training loss: 2.454349994659424
Validation loss: 2.300851068189067

Epoch: 6| Step: 12
Training loss: 2.382582426071167
Validation loss: 2.2962556192951817

Epoch: 6| Step: 13
Training loss: 2.681439161300659
Validation loss: 2.289829420787032

Epoch: 163| Step: 0
Training loss: 3.2011146545410156
Validation loss: 2.2881381562961045

Epoch: 6| Step: 1
Training loss: 2.557647228240967
Validation loss: 2.288769254120447

Epoch: 6| Step: 2
Training loss: 2.658811569213867
Validation loss: 2.28873328111505

Epoch: 6| Step: 3
Training loss: 2.5420870780944824
Validation loss: 2.287859275776853

Epoch: 6| Step: 4
Training loss: 2.7513198852539062
Validation loss: 2.303774097914337

Epoch: 6| Step: 5
Training loss: 2.37125301361084
Validation loss: 2.3078069430525585

Epoch: 6| Step: 6
Training loss: 2.8488526344299316
Validation loss: 2.3059257127905406

Epoch: 6| Step: 7
Training loss: 2.4163687229156494
Validation loss: 2.3150671451322493

Epoch: 6| Step: 8
Training loss: 2.665435314178467
Validation loss: 2.3086606405114614

Epoch: 6| Step: 9
Training loss: 2.828436851501465
Validation loss: 2.2941866254293792

Epoch: 6| Step: 10
Training loss: 1.2309612035751343
Validation loss: 2.2733705325793196

Epoch: 6| Step: 11
Training loss: 2.8287107944488525
Validation loss: 2.255735335811492

Epoch: 6| Step: 12
Training loss: 2.382046699523926
Validation loss: 2.2716084475158365

Epoch: 6| Step: 13
Training loss: 1.8336639404296875
Validation loss: 2.317674493276945

Epoch: 164| Step: 0
Training loss: 2.492215156555176
Validation loss: 2.339685042699178

Epoch: 6| Step: 1
Training loss: 2.9949841499328613
Validation loss: 2.3550696501167874

Epoch: 6| Step: 2
Training loss: 2.0785553455352783
Validation loss: 2.3467655028066328

Epoch: 6| Step: 3
Training loss: 1.2847301959991455
Validation loss: 2.310312460827571

Epoch: 6| Step: 4
Training loss: 2.7075138092041016
Validation loss: 2.2865941114323114

Epoch: 6| Step: 5
Training loss: 2.6472103595733643
Validation loss: 2.2718451202556653

Epoch: 6| Step: 6
Training loss: 3.066152334213257
Validation loss: 2.257479421554073

Epoch: 6| Step: 7
Training loss: 2.707578659057617
Validation loss: 2.263138630056894

Epoch: 6| Step: 8
Training loss: 2.7375905513763428
Validation loss: 2.2628349488781345

Epoch: 6| Step: 9
Training loss: 2.2303080558776855
Validation loss: 2.271603279216315

Epoch: 6| Step: 10
Training loss: 2.9627280235290527
Validation loss: 2.2675260305404663

Epoch: 6| Step: 11
Training loss: 2.0048673152923584
Validation loss: 2.2667471772880963

Epoch: 6| Step: 12
Training loss: 2.6756668090820312
Validation loss: 2.278466219543129

Epoch: 6| Step: 13
Training loss: 2.9587783813476562
Validation loss: 2.2811316110754527

Epoch: 165| Step: 0
Training loss: 2.4486148357391357
Validation loss: 2.2611514727274575

Epoch: 6| Step: 1
Training loss: 1.5757019519805908
Validation loss: 2.2634966347807195

Epoch: 6| Step: 2
Training loss: 2.878446578979492
Validation loss: 2.2559896002533617

Epoch: 6| Step: 3
Training loss: 2.6488490104675293
Validation loss: 2.262298071256248

Epoch: 6| Step: 4
Training loss: 2.8087754249572754
Validation loss: 2.273312545591785

Epoch: 6| Step: 5
Training loss: 1.8881351947784424
Validation loss: 2.286293950132144

Epoch: 6| Step: 6
Training loss: 2.754301071166992
Validation loss: 2.292929267370573

Epoch: 6| Step: 7
Training loss: 2.2462310791015625
Validation loss: 2.3223179437780894

Epoch: 6| Step: 8
Training loss: 2.813643455505371
Validation loss: 2.3210839225399877

Epoch: 6| Step: 9
Training loss: 2.4296350479125977
Validation loss: 2.321219136638026

Epoch: 6| Step: 10
Training loss: 2.9151644706726074
Validation loss: 2.310881799267184

Epoch: 6| Step: 11
Training loss: 2.72206974029541
Validation loss: 2.29533790772961

Epoch: 6| Step: 12
Training loss: 2.8040199279785156
Validation loss: 2.2824455538103656

Epoch: 6| Step: 13
Training loss: 2.1581838130950928
Validation loss: 2.2748941118999193

Epoch: 166| Step: 0
Training loss: 2.3593854904174805
Validation loss: 2.2665770002590713

Epoch: 6| Step: 1
Training loss: 2.4909651279449463
Validation loss: 2.2663331108708538

Epoch: 6| Step: 2
Training loss: 3.187530279159546
Validation loss: 2.2634835909771662

Epoch: 6| Step: 3
Training loss: 1.97894287109375
Validation loss: 2.2651792649299867

Epoch: 6| Step: 4
Training loss: 2.7641994953155518
Validation loss: 2.2679546289546515

Epoch: 6| Step: 5
Training loss: 2.652404546737671
Validation loss: 2.2679279952920894

Epoch: 6| Step: 6
Training loss: 2.9754931926727295
Validation loss: 2.2670730083219466

Epoch: 6| Step: 7
Training loss: 1.3569421768188477
Validation loss: 2.2728485522731656

Epoch: 6| Step: 8
Training loss: 2.397258996963501
Validation loss: 2.266861466951268

Epoch: 6| Step: 9
Training loss: 2.7495622634887695
Validation loss: 2.2724719432092484

Epoch: 6| Step: 10
Training loss: 2.327281951904297
Validation loss: 2.2677599665939168

Epoch: 6| Step: 11
Training loss: 2.536219596862793
Validation loss: 2.2711200637202107

Epoch: 6| Step: 12
Training loss: 2.5922904014587402
Validation loss: 2.2600598950539865

Epoch: 6| Step: 13
Training loss: 2.7953031063079834
Validation loss: 2.262761523646693

Epoch: 167| Step: 0
Training loss: 2.62791109085083
Validation loss: 2.255705723198511

Epoch: 6| Step: 1
Training loss: 2.7128396034240723
Validation loss: 2.259092269405242

Epoch: 6| Step: 2
Training loss: 2.9424479007720947
Validation loss: 2.265980428264987

Epoch: 6| Step: 3
Training loss: 2.7221295833587646
Validation loss: 2.2707143970715102

Epoch: 6| Step: 4
Training loss: 2.429656505584717
Validation loss: 2.28009726924281

Epoch: 6| Step: 5
Training loss: 2.9564428329467773
Validation loss: 2.282461139463609

Epoch: 6| Step: 6
Training loss: 2.680351972579956
Validation loss: 2.2786688009897866

Epoch: 6| Step: 7
Training loss: 2.202788829803467
Validation loss: 2.278599408365065

Epoch: 6| Step: 8
Training loss: 2.6226577758789062
Validation loss: 2.295447826385498

Epoch: 6| Step: 9
Training loss: 1.960524082183838
Validation loss: 2.3035832169235393

Epoch: 6| Step: 10
Training loss: 2.819361686706543
Validation loss: 2.298730229818693

Epoch: 6| Step: 11
Training loss: 2.2530007362365723
Validation loss: 2.2914271329038884

Epoch: 6| Step: 12
Training loss: 1.9158929586410522
Validation loss: 2.280126187109178

Epoch: 6| Step: 13
Training loss: 1.9156864881515503
Validation loss: 2.2754429360871673

Epoch: 168| Step: 0
Training loss: 1.88543701171875
Validation loss: 2.2723840513537006

Epoch: 6| Step: 1
Training loss: 1.7511214017868042
Validation loss: 2.2676299695045716

Epoch: 6| Step: 2
Training loss: 3.2339324951171875
Validation loss: 2.26305216358554

Epoch: 6| Step: 3
Training loss: 2.939232110977173
Validation loss: 2.268517909511443

Epoch: 6| Step: 4
Training loss: 3.009908676147461
Validation loss: 2.2612188259760537

Epoch: 6| Step: 5
Training loss: 2.8084940910339355
Validation loss: 2.2581479908317648

Epoch: 6| Step: 6
Training loss: 2.54732608795166
Validation loss: 2.2663380971518894

Epoch: 6| Step: 7
Training loss: 2.4830574989318848
Validation loss: 2.2744024286987963

Epoch: 6| Step: 8
Training loss: 2.1337087154388428
Validation loss: 2.2782987522822555

Epoch: 6| Step: 9
Training loss: 2.210681676864624
Validation loss: 2.2780318516556934

Epoch: 6| Step: 10
Training loss: 3.506695032119751
Validation loss: 2.2907510906137447

Epoch: 6| Step: 11
Training loss: 1.7137972116470337
Validation loss: 2.2970311026419363

Epoch: 6| Step: 12
Training loss: 2.550110101699829
Validation loss: 2.2961875238726215

Epoch: 6| Step: 13
Training loss: 1.9157233238220215
Validation loss: 2.304703422771987

Epoch: 169| Step: 0
Training loss: 2.430521249771118
Validation loss: 2.304836875648909

Epoch: 6| Step: 1
Training loss: 2.3909261226654053
Validation loss: 2.2933063686534925

Epoch: 6| Step: 2
Training loss: 3.033104181289673
Validation loss: 2.2977404773876233

Epoch: 6| Step: 3
Training loss: 2.5440375804901123
Validation loss: 2.2895646428549163

Epoch: 6| Step: 4
Training loss: 2.8195972442626953
Validation loss: 2.2849366023976314

Epoch: 6| Step: 5
Training loss: 2.1752638816833496
Validation loss: 2.2942954365925123

Epoch: 6| Step: 6
Training loss: 2.987466335296631
Validation loss: 2.2853764026395735

Epoch: 6| Step: 7
Training loss: 2.1804909706115723
Validation loss: 2.286622316606583

Epoch: 6| Step: 8
Training loss: 1.858567476272583
Validation loss: 2.275237847399968

Epoch: 6| Step: 9
Training loss: 2.3978395462036133
Validation loss: 2.269065046823153

Epoch: 6| Step: 10
Training loss: 3.2355237007141113
Validation loss: 2.2824372988875195

Epoch: 6| Step: 11
Training loss: 2.4541969299316406
Validation loss: 2.2785721440469064

Epoch: 6| Step: 12
Training loss: 2.3088905811309814
Validation loss: 2.27739962198401

Epoch: 6| Step: 13
Training loss: 1.886221170425415
Validation loss: 2.2658610959206857

Epoch: 170| Step: 0
Training loss: 2.370483875274658
Validation loss: 2.252668960120088

Epoch: 6| Step: 1
Training loss: 2.251171350479126
Validation loss: 2.245696008846324

Epoch: 6| Step: 2
Training loss: 2.867502450942993
Validation loss: 2.2325735502345587

Epoch: 6| Step: 3
Training loss: 1.9091086387634277
Validation loss: 2.22706780382382

Epoch: 6| Step: 4
Training loss: 1.9300919771194458
Validation loss: 2.2373014470582366

Epoch: 6| Step: 5
Training loss: 2.349536657333374
Validation loss: 2.2348939244465162

Epoch: 6| Step: 6
Training loss: 2.5319466590881348
Validation loss: 2.246400502420241

Epoch: 6| Step: 7
Training loss: 2.829272985458374
Validation loss: 2.2467457709773893

Epoch: 6| Step: 8
Training loss: 2.8266446590423584
Validation loss: 2.2555478054990052

Epoch: 6| Step: 9
Training loss: 2.511373996734619
Validation loss: 2.264068442006265

Epoch: 6| Step: 10
Training loss: 2.999506950378418
Validation loss: 2.279692643432207

Epoch: 6| Step: 11
Training loss: 2.0136518478393555
Validation loss: 2.2899670882891585

Epoch: 6| Step: 12
Training loss: 2.576504945755005
Validation loss: 2.2817108502952

Epoch: 6| Step: 13
Training loss: 3.3635292053222656
Validation loss: 2.2816663890756588

Epoch: 171| Step: 0
Training loss: 1.5146204233169556
Validation loss: 2.2714030768281672

Epoch: 6| Step: 1
Training loss: 2.5547842979431152
Validation loss: 2.2670922792086037

Epoch: 6| Step: 2
Training loss: 2.0834498405456543
Validation loss: 2.2664198516517557

Epoch: 6| Step: 3
Training loss: 2.380054473876953
Validation loss: 2.2638145416013655

Epoch: 6| Step: 4
Training loss: 3.106067180633545
Validation loss: 2.2668012816418885

Epoch: 6| Step: 5
Training loss: 2.347311019897461
Validation loss: 2.2626087537375827

Epoch: 6| Step: 6
Training loss: 1.9838660955429077
Validation loss: 2.263612631828554

Epoch: 6| Step: 7
Training loss: 2.3990137577056885
Validation loss: 2.252282609221756

Epoch: 6| Step: 8
Training loss: 3.3658289909362793
Validation loss: 2.255015298884402

Epoch: 6| Step: 9
Training loss: 2.556053638458252
Validation loss: 2.257511495262064

Epoch: 6| Step: 10
Training loss: 2.540571689605713
Validation loss: 2.2587106996966946

Epoch: 6| Step: 11
Training loss: 2.7710752487182617
Validation loss: 2.268742968959193

Epoch: 6| Step: 12
Training loss: 2.746466636657715
Validation loss: 2.274516349197716

Epoch: 6| Step: 13
Training loss: 2.3125364780426025
Validation loss: 2.2751181664005404

Epoch: 172| Step: 0
Training loss: 2.1721863746643066
Validation loss: 2.274082724766065

Epoch: 6| Step: 1
Training loss: 2.8673248291015625
Validation loss: 2.271828956501458

Epoch: 6| Step: 2
Training loss: 2.3978471755981445
Validation loss: 2.272671720033051

Epoch: 6| Step: 3
Training loss: 3.0775043964385986
Validation loss: 2.272236101088985

Epoch: 6| Step: 4
Training loss: 2.127890110015869
Validation loss: 2.2702170674518873

Epoch: 6| Step: 5
Training loss: 3.2555251121520996
Validation loss: 2.2682646525803434

Epoch: 6| Step: 6
Training loss: 2.8142614364624023
Validation loss: 2.2640675472956833

Epoch: 6| Step: 7
Training loss: 2.403048038482666
Validation loss: 2.263807871008432

Epoch: 6| Step: 8
Training loss: 2.8296871185302734
Validation loss: 2.2612170583458355

Epoch: 6| Step: 9
Training loss: 1.8513264656066895
Validation loss: 2.2689742272899998

Epoch: 6| Step: 10
Training loss: 2.4921231269836426
Validation loss: 2.2803561738742295

Epoch: 6| Step: 11
Training loss: 1.9412897825241089
Validation loss: 2.2962611080497823

Epoch: 6| Step: 12
Training loss: 2.5833616256713867
Validation loss: 2.30519349087951

Epoch: 6| Step: 13
Training loss: 1.8265135288238525
Validation loss: 2.314218190408522

Epoch: 173| Step: 0
Training loss: 3.0830445289611816
Validation loss: 2.3025977406450497

Epoch: 6| Step: 1
Training loss: 2.025054454803467
Validation loss: 2.2701886982046147

Epoch: 6| Step: 2
Training loss: 2.0577187538146973
Validation loss: 2.249974617394068

Epoch: 6| Step: 3
Training loss: 2.544882297515869
Validation loss: 2.2436277251089773

Epoch: 6| Step: 4
Training loss: 2.947432041168213
Validation loss: 2.245681846013633

Epoch: 6| Step: 5
Training loss: 2.6433863639831543
Validation loss: 2.2454066045822634

Epoch: 6| Step: 6
Training loss: 2.7676568031311035
Validation loss: 2.2523040797120784

Epoch: 6| Step: 7
Training loss: 1.9122618436813354
Validation loss: 2.2479526817157702

Epoch: 6| Step: 8
Training loss: 2.5361056327819824
Validation loss: 2.250793154521655

Epoch: 6| Step: 9
Training loss: 2.0283312797546387
Validation loss: 2.255711169653041

Epoch: 6| Step: 10
Training loss: 2.2257070541381836
Validation loss: 2.254619090787826

Epoch: 6| Step: 11
Training loss: 2.197929859161377
Validation loss: 2.2628156267186648

Epoch: 6| Step: 12
Training loss: 3.361867904663086
Validation loss: 2.27319743299997

Epoch: 6| Step: 13
Training loss: 2.7034785747528076
Validation loss: 2.287954907263479

Epoch: 174| Step: 0
Training loss: 3.3356924057006836
Validation loss: 2.2859751639827603

Epoch: 6| Step: 1
Training loss: 2.7023072242736816
Validation loss: 2.2837475499799176

Epoch: 6| Step: 2
Training loss: 2.615023136138916
Validation loss: 2.2795269976380053

Epoch: 6| Step: 3
Training loss: 2.4980688095092773
Validation loss: 2.262662605572772

Epoch: 6| Step: 4
Training loss: 2.567296028137207
Validation loss: 2.267126608920354

Epoch: 6| Step: 5
Training loss: 2.677530288696289
Validation loss: 2.263317951592066

Epoch: 6| Step: 6
Training loss: 2.200411319732666
Validation loss: 2.2579939442296184

Epoch: 6| Step: 7
Training loss: 2.43894100189209
Validation loss: 2.262062677773096

Epoch: 6| Step: 8
Training loss: 2.0537965297698975
Validation loss: 2.264987161082606

Epoch: 6| Step: 9
Training loss: 2.3152410984039307
Validation loss: 2.2674860672284196

Epoch: 6| Step: 10
Training loss: 2.805699348449707
Validation loss: 2.2697885241559757

Epoch: 6| Step: 11
Training loss: 2.2479825019836426
Validation loss: 2.2687001612878617

Epoch: 6| Step: 12
Training loss: 2.211130380630493
Validation loss: 2.264544920254779

Epoch: 6| Step: 13
Training loss: 1.8250689506530762
Validation loss: 2.268147142984534

Epoch: 175| Step: 0
Training loss: 2.346804141998291
Validation loss: 2.2611077011272473

Epoch: 6| Step: 1
Training loss: 2.6106767654418945
Validation loss: 2.256623965437694

Epoch: 6| Step: 2
Training loss: 2.2481608390808105
Validation loss: 2.2501221446580786

Epoch: 6| Step: 3
Training loss: 2.723452568054199
Validation loss: 2.2474209826479674

Epoch: 6| Step: 4
Training loss: 2.196706771850586
Validation loss: 2.2436352788761096

Epoch: 6| Step: 5
Training loss: 2.996920108795166
Validation loss: 2.2389965570101173

Epoch: 6| Step: 6
Training loss: 1.9581921100616455
Validation loss: 2.241568875569169

Epoch: 6| Step: 7
Training loss: 2.74953293800354
Validation loss: 2.2325296530159573

Epoch: 6| Step: 8
Training loss: 1.9537112712860107
Validation loss: 2.2358510878778275

Epoch: 6| Step: 9
Training loss: 2.632113456726074
Validation loss: 2.2411929086972306

Epoch: 6| Step: 10
Training loss: 2.6632912158966064
Validation loss: 2.249216902640558

Epoch: 6| Step: 11
Training loss: 2.9192609786987305
Validation loss: 2.257053921299596

Epoch: 6| Step: 12
Training loss: 2.5035126209259033
Validation loss: 2.2558968323533253

Epoch: 6| Step: 13
Training loss: 2.1032447814941406
Validation loss: 2.265228820103471

Epoch: 176| Step: 0
Training loss: 3.30368709564209
Validation loss: 2.257735154962027

Epoch: 6| Step: 1
Training loss: 2.180001735687256
Validation loss: 2.2576426126623668

Epoch: 6| Step: 2
Training loss: 2.5272533893585205
Validation loss: 2.2663478543681483

Epoch: 6| Step: 3
Training loss: 1.7357838153839111
Validation loss: 2.278417606507578

Epoch: 6| Step: 4
Training loss: 2.409278154373169
Validation loss: 2.3024858390131304

Epoch: 6| Step: 5
Training loss: 2.2433907985687256
Validation loss: 2.3129606093129804

Epoch: 6| Step: 6
Training loss: 2.729829788208008
Validation loss: 2.3340723719648135

Epoch: 6| Step: 7
Training loss: 2.6713531017303467
Validation loss: 2.3285503207996325

Epoch: 6| Step: 8
Training loss: 2.0537679195404053
Validation loss: 2.318287803280738

Epoch: 6| Step: 9
Training loss: 2.33064603805542
Validation loss: 2.2933324331878335

Epoch: 6| Step: 10
Training loss: 2.8942315578460693
Validation loss: 2.2922884879573697

Epoch: 6| Step: 11
Training loss: 2.772340774536133
Validation loss: 2.2867133989129016

Epoch: 6| Step: 12
Training loss: 2.31117582321167
Validation loss: 2.2720657394778345

Epoch: 6| Step: 13
Training loss: 2.8763427734375
Validation loss: 2.268752009637894

Epoch: 177| Step: 0
Training loss: 2.1754817962646484
Validation loss: 2.2480492643130723

Epoch: 6| Step: 1
Training loss: 2.3327271938323975
Validation loss: 2.2500121670384563

Epoch: 6| Step: 2
Training loss: 2.2302968502044678
Validation loss: 2.2444733227452924

Epoch: 6| Step: 3
Training loss: 2.9425501823425293
Validation loss: 2.2568213119301745

Epoch: 6| Step: 4
Training loss: 2.3876707553863525
Validation loss: 2.2727705765795965

Epoch: 6| Step: 5
Training loss: 2.53891658782959
Validation loss: 2.278794991072788

Epoch: 6| Step: 6
Training loss: 2.594480276107788
Validation loss: 2.279656164107784

Epoch: 6| Step: 7
Training loss: 3.0591883659362793
Validation loss: 2.274776045994092

Epoch: 6| Step: 8
Training loss: 1.8265324831008911
Validation loss: 2.269976121123119

Epoch: 6| Step: 9
Training loss: 2.29484224319458
Validation loss: 2.2634254860621628

Epoch: 6| Step: 10
Training loss: 3.496518135070801
Validation loss: 2.2658215722730084

Epoch: 6| Step: 11
Training loss: 1.8289082050323486
Validation loss: 2.254339366830805

Epoch: 6| Step: 12
Training loss: 2.4308409690856934
Validation loss: 2.24236350546601

Epoch: 6| Step: 13
Training loss: 2.580430030822754
Validation loss: 2.241022197149133

Epoch: 178| Step: 0
Training loss: 2.354907512664795
Validation loss: 2.2289885474789526

Epoch: 6| Step: 1
Training loss: 2.1379504203796387
Validation loss: 2.221518797259177

Epoch: 6| Step: 2
Training loss: 2.2332186698913574
Validation loss: 2.21650541469615

Epoch: 6| Step: 3
Training loss: 2.6858482360839844
Validation loss: 2.21332484932356

Epoch: 6| Step: 4
Training loss: 3.4983162879943848
Validation loss: 2.2318294227764173

Epoch: 6| Step: 5
Training loss: 2.2709310054779053
Validation loss: 2.2306489098456597

Epoch: 6| Step: 6
Training loss: 3.007577896118164
Validation loss: 2.219412698540636

Epoch: 6| Step: 7
Training loss: 2.711745262145996
Validation loss: 2.2218844018956667

Epoch: 6| Step: 8
Training loss: 2.5514070987701416
Validation loss: 2.2361617959955686

Epoch: 6| Step: 9
Training loss: 1.909533977508545
Validation loss: 2.2395607348411315

Epoch: 6| Step: 10
Training loss: 2.46159291267395
Validation loss: 2.2321482396894887

Epoch: 6| Step: 11
Training loss: 1.765993595123291
Validation loss: 2.245731689596689

Epoch: 6| Step: 12
Training loss: 2.158965587615967
Validation loss: 2.2482589137169624

Epoch: 6| Step: 13
Training loss: 3.0783889293670654
Validation loss: 2.2508131124640025

Epoch: 179| Step: 0
Training loss: 2.38049578666687
Validation loss: 2.2414480691315024

Epoch: 6| Step: 1
Training loss: 2.351349115371704
Validation loss: 2.2389711667132635

Epoch: 6| Step: 2
Training loss: 2.2307329177856445
Validation loss: 2.2418137404226486

Epoch: 6| Step: 3
Training loss: 2.0072340965270996
Validation loss: 2.23807595878519

Epoch: 6| Step: 4
Training loss: 2.1971988677978516
Validation loss: 2.2446481104820006

Epoch: 6| Step: 5
Training loss: 3.4058005809783936
Validation loss: 2.2469941544276413

Epoch: 6| Step: 6
Training loss: 2.89335298538208
Validation loss: 2.2512437271815475

Epoch: 6| Step: 7
Training loss: 2.0116755962371826
Validation loss: 2.2600977138806413

Epoch: 6| Step: 8
Training loss: 2.1574244499206543
Validation loss: 2.260399146746564

Epoch: 6| Step: 9
Training loss: 2.7692811489105225
Validation loss: 2.264226551978819

Epoch: 6| Step: 10
Training loss: 2.7451047897338867
Validation loss: 2.266612688700358

Epoch: 6| Step: 11
Training loss: 2.002173900604248
Validation loss: 2.2603279749552407

Epoch: 6| Step: 12
Training loss: 2.6846814155578613
Validation loss: 2.265520240670891

Epoch: 6| Step: 13
Training loss: 2.632153272628784
Validation loss: 2.2579639547614643

Epoch: 180| Step: 0
Training loss: 1.830807089805603
Validation loss: 2.24987014391089

Epoch: 6| Step: 1
Training loss: 1.739362359046936
Validation loss: 2.243756012250018

Epoch: 6| Step: 2
Training loss: 2.729024648666382
Validation loss: 2.242068044600948

Epoch: 6| Step: 3
Training loss: 2.207900047302246
Validation loss: 2.2376089172978557

Epoch: 6| Step: 4
Training loss: 2.651625156402588
Validation loss: 2.2474362106733423

Epoch: 6| Step: 5
Training loss: 2.2113733291625977
Validation loss: 2.2485627076959096

Epoch: 6| Step: 6
Training loss: 2.789464235305786
Validation loss: 2.266525855628393

Epoch: 6| Step: 7
Training loss: 2.2380428314208984
Validation loss: 2.2766965537942867

Epoch: 6| Step: 8
Training loss: 2.8368234634399414
Validation loss: 2.277777082176619

Epoch: 6| Step: 9
Training loss: 2.686980724334717
Validation loss: 2.26633398379049

Epoch: 6| Step: 10
Training loss: 2.7838058471679688
Validation loss: 2.2373956006060363

Epoch: 6| Step: 11
Training loss: 2.5404052734375
Validation loss: 2.2191396387674476

Epoch: 6| Step: 12
Training loss: 2.984847068786621
Validation loss: 2.2177418162745814

Epoch: 6| Step: 13
Training loss: 1.9934194087982178
Validation loss: 2.209851466199403

Epoch: 181| Step: 0
Training loss: 2.5524253845214844
Validation loss: 2.2067220980121243

Epoch: 6| Step: 1
Training loss: 2.7450671195983887
Validation loss: 2.205571413040161

Epoch: 6| Step: 2
Training loss: 2.3508119583129883
Validation loss: 2.2078786050119708

Epoch: 6| Step: 3
Training loss: 1.9342087507247925
Validation loss: 2.2083941864710983

Epoch: 6| Step: 4
Training loss: 2.3101725578308105
Validation loss: 2.210891774905625

Epoch: 6| Step: 5
Training loss: 2.8269941806793213
Validation loss: 2.211129573083693

Epoch: 6| Step: 6
Training loss: 2.4781947135925293
Validation loss: 2.2145009169014553

Epoch: 6| Step: 7
Training loss: 3.055863857269287
Validation loss: 2.2205925526157504

Epoch: 6| Step: 8
Training loss: 3.110847234725952
Validation loss: 2.2245937829376548

Epoch: 6| Step: 9
Training loss: 2.255587100982666
Validation loss: 2.233482796658752

Epoch: 6| Step: 10
Training loss: 2.557131290435791
Validation loss: 2.2412227430651264

Epoch: 6| Step: 11
Training loss: 1.9284557104110718
Validation loss: 2.2405127351002028

Epoch: 6| Step: 12
Training loss: 1.8468503952026367
Validation loss: 2.2391099173535585

Epoch: 6| Step: 13
Training loss: 2.3993639945983887
Validation loss: 2.236824171517485

Epoch: 182| Step: 0
Training loss: 2.463893413543701
Validation loss: 2.231718058227211

Epoch: 6| Step: 1
Training loss: 2.0752954483032227
Validation loss: 2.2294795102970575

Epoch: 6| Step: 2
Training loss: 2.189577579498291
Validation loss: 2.220774996665216

Epoch: 6| Step: 3
Training loss: 2.1319947242736816
Validation loss: 2.218341927374563

Epoch: 6| Step: 4
Training loss: 2.8191733360290527
Validation loss: 2.2217001709886777

Epoch: 6| Step: 5
Training loss: 2.452188014984131
Validation loss: 2.227942371881136

Epoch: 6| Step: 6
Training loss: 2.0488126277923584
Validation loss: 2.2357709997443744

Epoch: 6| Step: 7
Training loss: 2.797201633453369
Validation loss: 2.233095354931329

Epoch: 6| Step: 8
Training loss: 3.0327682495117188
Validation loss: 2.2339736671857935

Epoch: 6| Step: 9
Training loss: 2.6207528114318848
Validation loss: 2.236251764400031

Epoch: 6| Step: 10
Training loss: 2.007187604904175
Validation loss: 2.2446712345205326

Epoch: 6| Step: 11
Training loss: 2.8357129096984863
Validation loss: 2.249158977180399

Epoch: 6| Step: 12
Training loss: 2.306913137435913
Validation loss: 2.2373801239075197

Epoch: 6| Step: 13
Training loss: 2.541867971420288
Validation loss: 2.2328260047461397

Epoch: 183| Step: 0
Training loss: 2.2846083641052246
Validation loss: 2.229157045323362

Epoch: 6| Step: 1
Training loss: 1.9284634590148926
Validation loss: 2.22839645929234

Epoch: 6| Step: 2
Training loss: 2.3986124992370605
Validation loss: 2.2332659408610356

Epoch: 6| Step: 3
Training loss: 2.0465734004974365
Validation loss: 2.243333831910164

Epoch: 6| Step: 4
Training loss: 2.2968103885650635
Validation loss: 2.2464502960123043

Epoch: 6| Step: 5
Training loss: 2.2713022232055664
Validation loss: 2.259249658994777

Epoch: 6| Step: 6
Training loss: 3.1853017807006836
Validation loss: 2.2627506897013676

Epoch: 6| Step: 7
Training loss: 2.5441999435424805
Validation loss: 2.264197023965979

Epoch: 6| Step: 8
Training loss: 2.794149398803711
Validation loss: 2.2447913000660558

Epoch: 6| Step: 9
Training loss: 1.941630244255066
Validation loss: 2.2258836684688443

Epoch: 6| Step: 10
Training loss: 2.583374261856079
Validation loss: 2.221738576889038

Epoch: 6| Step: 11
Training loss: 3.0360777378082275
Validation loss: 2.208983311089136

Epoch: 6| Step: 12
Training loss: 2.3550186157226562
Validation loss: 2.2109331213017946

Epoch: 6| Step: 13
Training loss: 2.6459853649139404
Validation loss: 2.206587735042777

Epoch: 184| Step: 0
Training loss: 2.8307271003723145
Validation loss: 2.212871325913296

Epoch: 6| Step: 1
Training loss: 1.4127135276794434
Validation loss: 2.222044673017276

Epoch: 6| Step: 2
Training loss: 2.0983264446258545
Validation loss: 2.2207047042026313

Epoch: 6| Step: 3
Training loss: 2.4873576164245605
Validation loss: 2.2202034573401175

Epoch: 6| Step: 4
Training loss: 2.633584976196289
Validation loss: 2.2224149909070743

Epoch: 6| Step: 5
Training loss: 2.16520094871521
Validation loss: 2.224440627200629

Epoch: 6| Step: 6
Training loss: 2.358872652053833
Validation loss: 2.2240776413230487

Epoch: 6| Step: 7
Training loss: 3.1574478149414062
Validation loss: 2.2330298551949124

Epoch: 6| Step: 8
Training loss: 2.4130167961120605
Validation loss: 2.2216421122192056

Epoch: 6| Step: 9
Training loss: 2.7766380310058594
Validation loss: 2.214024007961314

Epoch: 6| Step: 10
Training loss: 2.987238645553589
Validation loss: 2.2157524119141283

Epoch: 6| Step: 11
Training loss: 1.9464364051818848
Validation loss: 2.2031258972742225

Epoch: 6| Step: 12
Training loss: 2.6544222831726074
Validation loss: 2.2193065356182795

Epoch: 6| Step: 13
Training loss: 2.108670234680176
Validation loss: 2.2341397398261615

Epoch: 185| Step: 0
Training loss: 2.7872555255889893
Validation loss: 2.237668764206671

Epoch: 6| Step: 1
Training loss: 1.747178316116333
Validation loss: 2.2517874651057745

Epoch: 6| Step: 2
Training loss: 2.084268808364868
Validation loss: 2.262874759653563

Epoch: 6| Step: 3
Training loss: 2.816321849822998
Validation loss: 2.2683834119509627

Epoch: 6| Step: 4
Training loss: 1.6079251766204834
Validation loss: 2.2792339812042894

Epoch: 6| Step: 5
Training loss: 1.9777545928955078
Validation loss: 2.271608224479101

Epoch: 6| Step: 6
Training loss: 2.803691864013672
Validation loss: 2.2766385796249553

Epoch: 6| Step: 7
Training loss: 1.9371013641357422
Validation loss: 2.266034180118192

Epoch: 6| Step: 8
Training loss: 2.4484729766845703
Validation loss: 2.245437237524217

Epoch: 6| Step: 9
Training loss: 3.1838927268981934
Validation loss: 2.243572804235643

Epoch: 6| Step: 10
Training loss: 3.175443172454834
Validation loss: 2.238430651285315

Epoch: 6| Step: 11
Training loss: 2.809504985809326
Validation loss: 2.231743420324018

Epoch: 6| Step: 12
Training loss: 2.473451852798462
Validation loss: 2.2332557298803843

Epoch: 6| Step: 13
Training loss: 2.370561122894287
Validation loss: 2.234885646450904

Epoch: 186| Step: 0
Training loss: 2.8003883361816406
Validation loss: 2.2154615207384993

Epoch: 6| Step: 1
Training loss: 3.0748751163482666
Validation loss: 2.2103531488808255

Epoch: 6| Step: 2
Training loss: 1.6965677738189697
Validation loss: 2.2054837057667394

Epoch: 6| Step: 3
Training loss: 2.497150182723999
Validation loss: 2.199856622244722

Epoch: 6| Step: 4
Training loss: 2.086111068725586
Validation loss: 2.19775797987497

Epoch: 6| Step: 5
Training loss: 2.2731552124023438
Validation loss: 2.209971986791139

Epoch: 6| Step: 6
Training loss: 2.384840488433838
Validation loss: 2.204433274525468

Epoch: 6| Step: 7
Training loss: 2.3206963539123535
Validation loss: 2.2088757279098674

Epoch: 6| Step: 8
Training loss: 1.9097459316253662
Validation loss: 2.210772511779621

Epoch: 6| Step: 9
Training loss: 2.819812297821045
Validation loss: 2.1984601379722677

Epoch: 6| Step: 10
Training loss: 1.9330785274505615
Validation loss: 2.1969444341557

Epoch: 6| Step: 11
Training loss: 2.5932211875915527
Validation loss: 2.1920990610635407

Epoch: 6| Step: 12
Training loss: 2.5474138259887695
Validation loss: 2.1934495766957602

Epoch: 6| Step: 13
Training loss: 3.5455758571624756
Validation loss: 2.187484613028906

Epoch: 187| Step: 0
Training loss: 2.5964841842651367
Validation loss: 2.1948019817311275

Epoch: 6| Step: 1
Training loss: 2.442875862121582
Validation loss: 2.1954552101832565

Epoch: 6| Step: 2
Training loss: 2.5652785301208496
Validation loss: 2.200619287388299

Epoch: 6| Step: 3
Training loss: 2.203544855117798
Validation loss: 2.2041616004000426

Epoch: 6| Step: 4
Training loss: 2.9245800971984863
Validation loss: 2.201824565087595

Epoch: 6| Step: 5
Training loss: 2.286531925201416
Validation loss: 2.2046819963762836

Epoch: 6| Step: 6
Training loss: 2.8129422664642334
Validation loss: 2.205891857865036

Epoch: 6| Step: 7
Training loss: 2.0291380882263184
Validation loss: 2.2107300553270566

Epoch: 6| Step: 8
Training loss: 2.020063877105713
Validation loss: 2.2111662959539764

Epoch: 6| Step: 9
Training loss: 2.573406457901001
Validation loss: 2.2150235252995647

Epoch: 6| Step: 10
Training loss: 1.9486911296844482
Validation loss: 2.225699555489325

Epoch: 6| Step: 11
Training loss: 2.5934274196624756
Validation loss: 2.2364583451260804

Epoch: 6| Step: 12
Training loss: 2.7047061920166016
Validation loss: 2.23283044240808

Epoch: 6| Step: 13
Training loss: 2.0543830394744873
Validation loss: 2.2367406839965494

Epoch: 188| Step: 0
Training loss: 1.936478853225708
Validation loss: 2.232485029005235

Epoch: 6| Step: 1
Training loss: 2.250606060028076
Validation loss: 2.2338551910974647

Epoch: 6| Step: 2
Training loss: 1.6862660646438599
Validation loss: 2.2331619801059848

Epoch: 6| Step: 3
Training loss: 2.328397750854492
Validation loss: 2.2320441481887654

Epoch: 6| Step: 4
Training loss: 2.5783326625823975
Validation loss: 2.21734647340672

Epoch: 6| Step: 5
Training loss: 2.8124961853027344
Validation loss: 2.2027475116073445

Epoch: 6| Step: 6
Training loss: 2.293208360671997
Validation loss: 2.206099044892096

Epoch: 6| Step: 7
Training loss: 3.1813666820526123
Validation loss: 2.197018605406566

Epoch: 6| Step: 8
Training loss: 3.1966586112976074
Validation loss: 2.193946805051578

Epoch: 6| Step: 9
Training loss: 1.681398868560791
Validation loss: 2.196974008314071

Epoch: 6| Step: 10
Training loss: 2.494096040725708
Validation loss: 2.2006367765447146

Epoch: 6| Step: 11
Training loss: 2.1028833389282227
Validation loss: 2.199671718382066

Epoch: 6| Step: 12
Training loss: 2.7007884979248047
Validation loss: 2.2042407399864605

Epoch: 6| Step: 13
Training loss: 2.803499937057495
Validation loss: 2.201447917569068

Epoch: 189| Step: 0
Training loss: 1.9027855396270752
Validation loss: 2.2024632448791177

Epoch: 6| Step: 1
Training loss: 2.715339183807373
Validation loss: 2.1991043552275626

Epoch: 6| Step: 2
Training loss: 2.186185836791992
Validation loss: 2.2020541109064573

Epoch: 6| Step: 3
Training loss: 2.042015790939331
Validation loss: 2.205260774140717

Epoch: 6| Step: 4
Training loss: 2.332846164703369
Validation loss: 2.2175794109221427

Epoch: 6| Step: 5
Training loss: 2.4227824211120605
Validation loss: 2.221041440963745

Epoch: 6| Step: 6
Training loss: 2.759633779525757
Validation loss: 2.2136736480138635

Epoch: 6| Step: 7
Training loss: 2.5394253730773926
Validation loss: 2.2126411250842515

Epoch: 6| Step: 8
Training loss: 2.362905502319336
Validation loss: 2.209673232929681

Epoch: 6| Step: 9
Training loss: 1.6898798942565918
Validation loss: 2.2031340624696467

Epoch: 6| Step: 10
Training loss: 2.829338550567627
Validation loss: 2.182554201413226

Epoch: 6| Step: 11
Training loss: 2.812868118286133
Validation loss: 2.181072411998626

Epoch: 6| Step: 12
Training loss: 2.7048206329345703
Validation loss: 2.173374245243688

Epoch: 6| Step: 13
Training loss: 2.627009153366089
Validation loss: 2.1741351337843042

Epoch: 190| Step: 0
Training loss: 2.310945510864258
Validation loss: 2.1848827895297798

Epoch: 6| Step: 1
Training loss: 2.117863178253174
Validation loss: 2.1894749838818788

Epoch: 6| Step: 2
Training loss: 2.7743101119995117
Validation loss: 2.194095366744585

Epoch: 6| Step: 3
Training loss: 2.2453386783599854
Validation loss: 2.2060444278101765

Epoch: 6| Step: 4
Training loss: 2.8436198234558105
Validation loss: 2.2139451388389833

Epoch: 6| Step: 5
Training loss: 1.8530434370040894
Validation loss: 2.210296348858905

Epoch: 6| Step: 6
Training loss: 2.607569694519043
Validation loss: 2.212160974420527

Epoch: 6| Step: 7
Training loss: 2.8482906818389893
Validation loss: 2.214884432413245

Epoch: 6| Step: 8
Training loss: 1.847931146621704
Validation loss: 2.2072160846443585

Epoch: 6| Step: 9
Training loss: 2.432133913040161
Validation loss: 2.212531587128998

Epoch: 6| Step: 10
Training loss: 2.6129024028778076
Validation loss: 2.2159971062855055

Epoch: 6| Step: 11
Training loss: 2.2834420204162598
Validation loss: 2.230037749454539

Epoch: 6| Step: 12
Training loss: 2.5708224773406982
Validation loss: 2.234538988400531

Epoch: 6| Step: 13
Training loss: 2.747014045715332
Validation loss: 2.2337335899312007

Epoch: 191| Step: 0
Training loss: 2.2081470489501953
Validation loss: 2.2506755475075013

Epoch: 6| Step: 1
Training loss: 2.6558353900909424
Validation loss: 2.257752292899675

Epoch: 6| Step: 2
Training loss: 1.8440065383911133
Validation loss: 2.2493172358441096

Epoch: 6| Step: 3
Training loss: 2.269998073577881
Validation loss: 2.243293526352093

Epoch: 6| Step: 4
Training loss: 1.9400641918182373
Validation loss: 2.2430005124820176

Epoch: 6| Step: 5
Training loss: 2.9331905841827393
Validation loss: 2.235984235681513

Epoch: 6| Step: 6
Training loss: 2.806161880493164
Validation loss: 2.231502689341063

Epoch: 6| Step: 7
Training loss: 2.5638606548309326
Validation loss: 2.222425232651413

Epoch: 6| Step: 8
Training loss: 1.9355570077896118
Validation loss: 2.2084502558554373

Epoch: 6| Step: 9
Training loss: 2.5516738891601562
Validation loss: 2.210387101737402

Epoch: 6| Step: 10
Training loss: 2.7026925086975098
Validation loss: 2.2159378810595443

Epoch: 6| Step: 11
Training loss: 2.2013661861419678
Validation loss: 2.205675700659393

Epoch: 6| Step: 12
Training loss: 2.5849051475524902
Validation loss: 2.205036832440284

Epoch: 6| Step: 13
Training loss: 3.116581439971924
Validation loss: 2.19961517600603

Epoch: 192| Step: 0
Training loss: 1.8717278242111206
Validation loss: 2.1907284875069895

Epoch: 6| Step: 1
Training loss: 2.2611072063446045
Validation loss: 2.192202152744416

Epoch: 6| Step: 2
Training loss: 2.077047824859619
Validation loss: 2.1939079069322154

Epoch: 6| Step: 3
Training loss: 2.9727821350097656
Validation loss: 2.189492407665458

Epoch: 6| Step: 4
Training loss: 1.770509958267212
Validation loss: 2.1920007851815995

Epoch: 6| Step: 5
Training loss: 2.637099266052246
Validation loss: 2.1951354165231027

Epoch: 6| Step: 6
Training loss: 2.9781417846679688
Validation loss: 2.199798386584046

Epoch: 6| Step: 7
Training loss: 2.5337557792663574
Validation loss: 2.200025402089601

Epoch: 6| Step: 8
Training loss: 2.1467413902282715
Validation loss: 2.216969690015239

Epoch: 6| Step: 9
Training loss: 2.198007583618164
Validation loss: 2.218214396507509

Epoch: 6| Step: 10
Training loss: 2.2158029079437256
Validation loss: 2.2429220496967273

Epoch: 6| Step: 11
Training loss: 2.6964850425720215
Validation loss: 2.250287643042944

Epoch: 6| Step: 12
Training loss: 2.6194655895233154
Validation loss: 2.251592489980882

Epoch: 6| Step: 13
Training loss: 3.020373821258545
Validation loss: 2.2472292928285498

Epoch: 193| Step: 0
Training loss: 2.2744174003601074
Validation loss: 2.2497760070267545

Epoch: 6| Step: 1
Training loss: 3.1185550689697266
Validation loss: 2.2348926708262455

Epoch: 6| Step: 2
Training loss: 2.4826626777648926
Validation loss: 2.213109508637459

Epoch: 6| Step: 3
Training loss: 2.0377187728881836
Validation loss: 2.199665146489297

Epoch: 6| Step: 4
Training loss: 2.3642570972442627
Validation loss: 2.1844491189525974

Epoch: 6| Step: 5
Training loss: 2.1472318172454834
Validation loss: 2.18429986892208

Epoch: 6| Step: 6
Training loss: 2.282503128051758
Validation loss: 2.190857446321877

Epoch: 6| Step: 7
Training loss: 2.133823871612549
Validation loss: 2.1890044161068496

Epoch: 6| Step: 8
Training loss: 1.8882157802581787
Validation loss: 2.2005069127646824

Epoch: 6| Step: 9
Training loss: 2.5728421211242676
Validation loss: 2.2049986931585495

Epoch: 6| Step: 10
Training loss: 2.3464975357055664
Validation loss: 2.2053781299180883

Epoch: 6| Step: 11
Training loss: 2.954106330871582
Validation loss: 2.208272067449426

Epoch: 6| Step: 12
Training loss: 2.1405258178710938
Validation loss: 2.2071476546666955

Epoch: 6| Step: 13
Training loss: 3.515808582305908
Validation loss: 2.2055583615456857

Epoch: 194| Step: 0
Training loss: 2.7418670654296875
Validation loss: 2.1968798547662716

Epoch: 6| Step: 1
Training loss: 2.5407357215881348
Validation loss: 2.1912745121986634

Epoch: 6| Step: 2
Training loss: 2.439128875732422
Validation loss: 2.1868099768956504

Epoch: 6| Step: 3
Training loss: 2.2479491233825684
Validation loss: 2.1870142054814163

Epoch: 6| Step: 4
Training loss: 2.5959184169769287
Validation loss: 2.1857345206763155

Epoch: 6| Step: 5
Training loss: 1.9166254997253418
Validation loss: 2.1874428628593363

Epoch: 6| Step: 6
Training loss: 2.1162819862365723
Validation loss: 2.182566935016263

Epoch: 6| Step: 7
Training loss: 2.597278594970703
Validation loss: 2.1920880015178392

Epoch: 6| Step: 8
Training loss: 2.5536305904388428
Validation loss: 2.205593511622439

Epoch: 6| Step: 9
Training loss: 2.3328726291656494
Validation loss: 2.2047903896659933

Epoch: 6| Step: 10
Training loss: 2.180100917816162
Validation loss: 2.202391116849838

Epoch: 6| Step: 11
Training loss: 2.4748387336730957
Validation loss: 2.211136761532035

Epoch: 6| Step: 12
Training loss: 2.6447865962982178
Validation loss: 2.215009558585382

Epoch: 6| Step: 13
Training loss: 2.2503132820129395
Validation loss: 2.2131736124715498

Epoch: 195| Step: 0
Training loss: 2.5333123207092285
Validation loss: 2.2156869557596024

Epoch: 6| Step: 1
Training loss: 2.48097562789917
Validation loss: 2.2098164789138304

Epoch: 6| Step: 2
Training loss: 2.7591302394866943
Validation loss: 2.201457773485491

Epoch: 6| Step: 3
Training loss: 2.424569606781006
Validation loss: 2.1929972351238294

Epoch: 6| Step: 4
Training loss: 1.693061351776123
Validation loss: 2.1887971662705943

Epoch: 6| Step: 5
Training loss: 2.719816207885742
Validation loss: 2.188826209755354

Epoch: 6| Step: 6
Training loss: 2.662809371948242
Validation loss: 2.192870755349436

Epoch: 6| Step: 7
Training loss: 2.5547142028808594
Validation loss: 2.200120443938881

Epoch: 6| Step: 8
Training loss: 2.53020977973938
Validation loss: 2.206476334602602

Epoch: 6| Step: 9
Training loss: 2.0935897827148438
Validation loss: 2.2097512624597035

Epoch: 6| Step: 10
Training loss: 2.344339370727539
Validation loss: 2.2131558361873833

Epoch: 6| Step: 11
Training loss: 1.834782600402832
Validation loss: 2.209780280308057

Epoch: 6| Step: 12
Training loss: 2.715181589126587
Validation loss: 2.2072143298323437

Epoch: 6| Step: 13
Training loss: 2.2521722316741943
Validation loss: 2.20339274150069

Epoch: 196| Step: 0
Training loss: 2.7565784454345703
Validation loss: 2.1974880118523874

Epoch: 6| Step: 1
Training loss: 2.4290246963500977
Validation loss: 2.197616739939618

Epoch: 6| Step: 2
Training loss: 2.735836982727051
Validation loss: 2.1887290272661435

Epoch: 6| Step: 3
Training loss: 2.8515663146972656
Validation loss: 2.1778883036746772

Epoch: 6| Step: 4
Training loss: 2.644732713699341
Validation loss: 2.1691456815247894

Epoch: 6| Step: 5
Training loss: 2.83798885345459
Validation loss: 2.166504554851081

Epoch: 6| Step: 6
Training loss: 1.7808728218078613
Validation loss: 2.170756786100326

Epoch: 6| Step: 7
Training loss: 2.7995824813842773
Validation loss: 2.1760988696928947

Epoch: 6| Step: 8
Training loss: 2.2458581924438477
Validation loss: 2.1786277447977374

Epoch: 6| Step: 9
Training loss: 2.0991859436035156
Validation loss: 2.180876460126651

Epoch: 6| Step: 10
Training loss: 2.435030460357666
Validation loss: 2.1729029942584295

Epoch: 6| Step: 11
Training loss: 1.6454744338989258
Validation loss: 2.169469315518615

Epoch: 6| Step: 12
Training loss: 1.9570417404174805
Validation loss: 2.177179028910975

Epoch: 6| Step: 13
Training loss: 2.3262617588043213
Validation loss: 2.1904757868859077

Epoch: 197| Step: 0
Training loss: 2.2050018310546875
Validation loss: 2.205224916499148

Epoch: 6| Step: 1
Training loss: 1.9628397226333618
Validation loss: 2.2186186646902435

Epoch: 6| Step: 2
Training loss: 2.5444769859313965
Validation loss: 2.2374260733204503

Epoch: 6| Step: 3
Training loss: 2.174591302871704
Validation loss: 2.2491710544914327

Epoch: 6| Step: 4
Training loss: 2.500558376312256
Validation loss: 2.2718461867301696

Epoch: 6| Step: 5
Training loss: 2.4905591011047363
Validation loss: 2.2777784601334603

Epoch: 6| Step: 6
Training loss: 2.588773727416992
Validation loss: 2.2779448416925248

Epoch: 6| Step: 7
Training loss: 3.099104881286621
Validation loss: 2.254612582986073

Epoch: 6| Step: 8
Training loss: 2.389815330505371
Validation loss: 2.2294276798925092

Epoch: 6| Step: 9
Training loss: 2.6000912189483643
Validation loss: 2.2076502435950824

Epoch: 6| Step: 10
Training loss: 1.5538532733917236
Validation loss: 2.2068440555244364

Epoch: 6| Step: 11
Training loss: 3.338008165359497
Validation loss: 2.1979653373841317

Epoch: 6| Step: 12
Training loss: 2.083552598953247
Validation loss: 2.1853473776130268

Epoch: 6| Step: 13
Training loss: 1.8096858263015747
Validation loss: 2.183576883808259

Epoch: 198| Step: 0
Training loss: 2.6657397747039795
Validation loss: 2.1709557271772817

Epoch: 6| Step: 1
Training loss: 2.2109169960021973
Validation loss: 2.1703336213224675

Epoch: 6| Step: 2
Training loss: 2.063878059387207
Validation loss: 2.17761141766784

Epoch: 6| Step: 3
Training loss: 2.6728670597076416
Validation loss: 2.17091049942919

Epoch: 6| Step: 4
Training loss: 1.8944647312164307
Validation loss: 2.1557704761464107

Epoch: 6| Step: 5
Training loss: 2.5566864013671875
Validation loss: 2.144138902746221

Epoch: 6| Step: 6
Training loss: 2.474151611328125
Validation loss: 2.1462205456149195

Epoch: 6| Step: 7
Training loss: 2.545443058013916
Validation loss: 2.1535626842129614

Epoch: 6| Step: 8
Training loss: 2.2566423416137695
Validation loss: 2.138690061466668

Epoch: 6| Step: 9
Training loss: 2.2935538291931152
Validation loss: 2.1386283290001655

Epoch: 6| Step: 10
Training loss: 2.2453794479370117
Validation loss: 2.1368964718234156

Epoch: 6| Step: 11
Training loss: 2.6374363899230957
Validation loss: 2.1570791070179274

Epoch: 6| Step: 12
Training loss: 2.795269012451172
Validation loss: 2.155696686877999

Epoch: 6| Step: 13
Training loss: 2.089341163635254
Validation loss: 2.163029210541838

Epoch: 199| Step: 0
Training loss: 2.5656609535217285
Validation loss: 2.174934515389063

Epoch: 6| Step: 1
Training loss: 3.3064661026000977
Validation loss: 2.1946901454720447

Epoch: 6| Step: 2
Training loss: 2.890469789505005
Validation loss: 2.199694279701479

Epoch: 6| Step: 3
Training loss: 2.401470184326172
Validation loss: 2.2165873281417356

Epoch: 6| Step: 4
Training loss: 1.8073537349700928
Validation loss: 2.2228309954366376

Epoch: 6| Step: 5
Training loss: 2.0240960121154785
Validation loss: 2.218020575020903

Epoch: 6| Step: 6
Training loss: 2.6674435138702393
Validation loss: 2.2089041433026715

Epoch: 6| Step: 7
Training loss: 2.326594829559326
Validation loss: 2.208588141267018

Epoch: 6| Step: 8
Training loss: 2.6222991943359375
Validation loss: 2.2021103776911253

Epoch: 6| Step: 9
Training loss: 2.4562230110168457
Validation loss: 2.1974281341798845

Epoch: 6| Step: 10
Training loss: 2.298783779144287
Validation loss: 2.197480604212771

Epoch: 6| Step: 11
Training loss: 2.012033700942993
Validation loss: 2.2041504895815285

Epoch: 6| Step: 12
Training loss: 1.7500241994857788
Validation loss: 2.1956018235093806

Epoch: 6| Step: 13
Training loss: 2.3582825660705566
Validation loss: 2.2017521191668767

Epoch: 200| Step: 0
Training loss: 2.1610264778137207
Validation loss: 2.1870246856443343

Epoch: 6| Step: 1
Training loss: 3.2452683448791504
Validation loss: 2.1835148744685675

Epoch: 6| Step: 2
Training loss: 2.26820707321167
Validation loss: 2.1855568347438687

Epoch: 6| Step: 3
Training loss: 2.341277837753296
Validation loss: 2.1925502669426704

Epoch: 6| Step: 4
Training loss: 2.552722215652466
Validation loss: 2.2012208610452633

Epoch: 6| Step: 5
Training loss: 2.755635976791382
Validation loss: 2.2092699517485914

Epoch: 6| Step: 6
Training loss: 1.9494554996490479
Validation loss: 2.214034885488531

Epoch: 6| Step: 7
Training loss: 1.446821928024292
Validation loss: 2.2183576501825804

Epoch: 6| Step: 8
Training loss: 2.3203396797180176
Validation loss: 2.2187845706939697

Epoch: 6| Step: 9
Training loss: 2.6859569549560547
Validation loss: 2.1994278597575363

Epoch: 6| Step: 10
Training loss: 2.5952811241149902
Validation loss: 2.191820260017149

Epoch: 6| Step: 11
Training loss: 1.9193599224090576
Validation loss: 2.185439025202105

Epoch: 6| Step: 12
Training loss: 2.9382314682006836
Validation loss: 2.172792319328554

Epoch: 6| Step: 13
Training loss: 1.9810172319412231
Validation loss: 2.171117180137224

Epoch: 201| Step: 0
Training loss: 2.1900272369384766
Validation loss: 2.170881353398805

Epoch: 6| Step: 1
Training loss: 2.7906627655029297
Validation loss: 2.178157889714805

Epoch: 6| Step: 2
Training loss: 2.6198782920837402
Validation loss: 2.1930492437014015

Epoch: 6| Step: 3
Training loss: 1.9132682085037231
Validation loss: 2.194090379181729

Epoch: 6| Step: 4
Training loss: 2.1586148738861084
Validation loss: 2.2025880698234803

Epoch: 6| Step: 5
Training loss: 2.5793654918670654
Validation loss: 2.200447784957065

Epoch: 6| Step: 6
Training loss: 2.6329965591430664
Validation loss: 2.185156194112634

Epoch: 6| Step: 7
Training loss: 1.8806673288345337
Validation loss: 2.1833004079839236

Epoch: 6| Step: 8
Training loss: 2.821725845336914
Validation loss: 2.1755026617357807

Epoch: 6| Step: 9
Training loss: 1.9673919677734375
Validation loss: 2.179250406962569

Epoch: 6| Step: 10
Training loss: 2.574709892272949
Validation loss: 2.1793902535592355

Epoch: 6| Step: 11
Training loss: 2.252519369125366
Validation loss: 2.1797664998680033

Epoch: 6| Step: 12
Training loss: 2.281574249267578
Validation loss: 2.1929639206137708

Epoch: 6| Step: 13
Training loss: 2.8524720668792725
Validation loss: 2.197248185834577

Epoch: 202| Step: 0
Training loss: 2.458698272705078
Validation loss: 2.1992902909555743

Epoch: 6| Step: 1
Training loss: 2.079526424407959
Validation loss: 2.195421170162898

Epoch: 6| Step: 2
Training loss: 2.4427695274353027
Validation loss: 2.175908975703742

Epoch: 6| Step: 3
Training loss: 2.862703323364258
Validation loss: 2.167517349284182

Epoch: 6| Step: 4
Training loss: 2.5546600818634033
Validation loss: 2.175455170293008

Epoch: 6| Step: 5
Training loss: 1.8331183195114136
Validation loss: 2.183587425498552

Epoch: 6| Step: 6
Training loss: 1.9611788988113403
Validation loss: 2.1951499369836625

Epoch: 6| Step: 7
Training loss: 2.1776070594787598
Validation loss: 2.179844005133516

Epoch: 6| Step: 8
Training loss: 2.994964122772217
Validation loss: 2.1759612867909093

Epoch: 6| Step: 9
Training loss: 2.4631898403167725
Validation loss: 2.1657952672691754

Epoch: 6| Step: 10
Training loss: 2.092102527618408
Validation loss: 2.1781338876293552

Epoch: 6| Step: 11
Training loss: 2.5420098304748535
Validation loss: 2.179451245133595

Epoch: 6| Step: 12
Training loss: 2.796278715133667
Validation loss: 2.210577481536455

Epoch: 6| Step: 13
Training loss: 1.7150273323059082
Validation loss: 2.2083023389180503

Epoch: 203| Step: 0
Training loss: 2.34975004196167
Validation loss: 2.230229518746817

Epoch: 6| Step: 1
Training loss: 2.138829469680786
Validation loss: 2.227427456968574

Epoch: 6| Step: 2
Training loss: 2.8588268756866455
Validation loss: 2.2216495339588453

Epoch: 6| Step: 3
Training loss: 2.8781380653381348
Validation loss: 2.21214263028996

Epoch: 6| Step: 4
Training loss: 2.2406980991363525
Validation loss: 2.195768176868398

Epoch: 6| Step: 5
Training loss: 1.9590376615524292
Validation loss: 2.1904743025379796

Epoch: 6| Step: 6
Training loss: 1.968989610671997
Validation loss: 2.174201291094544

Epoch: 6| Step: 7
Training loss: 2.4993693828582764
Validation loss: 2.1787220316548503

Epoch: 6| Step: 8
Training loss: 3.1053380966186523
Validation loss: 2.181368238182478

Epoch: 6| Step: 9
Training loss: 1.8698782920837402
Validation loss: 2.1754167797744914

Epoch: 6| Step: 10
Training loss: 2.473268508911133
Validation loss: 2.174769424623059

Epoch: 6| Step: 11
Training loss: 1.7191619873046875
Validation loss: 2.1759570926748295

Epoch: 6| Step: 12
Training loss: 2.607473850250244
Validation loss: 2.1691711794945503

Epoch: 6| Step: 13
Training loss: 2.678422451019287
Validation loss: 2.1744036366862636

Epoch: 204| Step: 0
Training loss: 2.69389009475708
Validation loss: 2.1711378276989026

Epoch: 6| Step: 1
Training loss: 1.7419794797897339
Validation loss: 2.172899919171487

Epoch: 6| Step: 2
Training loss: 2.4010162353515625
Validation loss: 2.1720952321124334

Epoch: 6| Step: 3
Training loss: 2.465952157974243
Validation loss: 2.169882982007919

Epoch: 6| Step: 4
Training loss: 2.4624340534210205
Validation loss: 2.1831309923561673

Epoch: 6| Step: 5
Training loss: 2.2484612464904785
Validation loss: 2.1856368844227125

Epoch: 6| Step: 6
Training loss: 2.1575140953063965
Validation loss: 2.1996254690231813

Epoch: 6| Step: 7
Training loss: 2.1909284591674805
Validation loss: 2.196983619402814

Epoch: 6| Step: 8
Training loss: 2.4444398880004883
Validation loss: 2.1933774512301207

Epoch: 6| Step: 9
Training loss: 2.085477352142334
Validation loss: 2.18400566808639

Epoch: 6| Step: 10
Training loss: 2.3969931602478027
Validation loss: 2.1765848949391353

Epoch: 6| Step: 11
Training loss: 2.5873894691467285
Validation loss: 2.1768224854623117

Epoch: 6| Step: 12
Training loss: 2.6849751472473145
Validation loss: 2.167789166973483

Epoch: 6| Step: 13
Training loss: 2.3502209186553955
Validation loss: 2.161166388501403

Epoch: 205| Step: 0
Training loss: 2.727363109588623
Validation loss: 2.164466457982217

Epoch: 6| Step: 1
Training loss: 2.093743324279785
Validation loss: 2.1647336944457023

Epoch: 6| Step: 2
Training loss: 2.4455983638763428
Validation loss: 2.156542229396041

Epoch: 6| Step: 3
Training loss: 2.255744457244873
Validation loss: 2.162164285618772

Epoch: 6| Step: 4
Training loss: 2.335538864135742
Validation loss: 2.166100278977425

Epoch: 6| Step: 5
Training loss: 2.382932186126709
Validation loss: 2.167864643117433

Epoch: 6| Step: 6
Training loss: 2.2594072818756104
Validation loss: 2.175145718359178

Epoch: 6| Step: 7
Training loss: 3.0163259506225586
Validation loss: 2.1877168070885444

Epoch: 6| Step: 8
Training loss: 2.4239253997802734
Validation loss: 2.184740315201462

Epoch: 6| Step: 9
Training loss: 2.015986204147339
Validation loss: 2.191118881266604

Epoch: 6| Step: 10
Training loss: 1.607858419418335
Validation loss: 2.1856579190941265

Epoch: 6| Step: 11
Training loss: 2.587496757507324
Validation loss: 2.1852531292105235

Epoch: 6| Step: 12
Training loss: 1.9798150062561035
Validation loss: 2.1948376701724146

Epoch: 6| Step: 13
Training loss: 3.223987102508545
Validation loss: 2.2036361155971402

Epoch: 206| Step: 0
Training loss: 2.4771649837493896
Validation loss: 2.195780856634981

Epoch: 6| Step: 1
Training loss: 2.008108615875244
Validation loss: 2.185399193917551

Epoch: 6| Step: 2
Training loss: 3.1767477989196777
Validation loss: 2.1887009066920124

Epoch: 6| Step: 3
Training loss: 3.1887311935424805
Validation loss: 2.169346729914347

Epoch: 6| Step: 4
Training loss: 2.51243257522583
Validation loss: 2.167917909160737

Epoch: 6| Step: 5
Training loss: 2.4721827507019043
Validation loss: 2.170181912760581

Epoch: 6| Step: 6
Training loss: 2.7683393955230713
Validation loss: 2.1794506349871234

Epoch: 6| Step: 7
Training loss: 2.0700111389160156
Validation loss: 2.1730037478990454

Epoch: 6| Step: 8
Training loss: 1.599749207496643
Validation loss: 2.1639327926020466

Epoch: 6| Step: 9
Training loss: 1.9508415460586548
Validation loss: 2.1635236535021054

Epoch: 6| Step: 10
Training loss: 2.1912379264831543
Validation loss: 2.1587615141304592

Epoch: 6| Step: 11
Training loss: 1.6711909770965576
Validation loss: 2.159340448276971

Epoch: 6| Step: 12
Training loss: 2.4785735607147217
Validation loss: 2.165140382705196

Epoch: 6| Step: 13
Training loss: 1.8912252187728882
Validation loss: 2.1562120632458757

Epoch: 207| Step: 0
Training loss: 3.080862522125244
Validation loss: 2.1590345931309525

Epoch: 6| Step: 1
Training loss: 3.0032458305358887
Validation loss: 2.158517277368935

Epoch: 6| Step: 2
Training loss: 2.6707065105438232
Validation loss: 2.1545053515382993

Epoch: 6| Step: 3
Training loss: 2.004080057144165
Validation loss: 2.1529900873861005

Epoch: 6| Step: 4
Training loss: 1.9766628742218018
Validation loss: 2.1673540761393886

Epoch: 6| Step: 5
Training loss: 2.5183935165405273
Validation loss: 2.166365546564902

Epoch: 6| Step: 6
Training loss: 1.9355976581573486
Validation loss: 2.1737186190902547

Epoch: 6| Step: 7
Training loss: 2.053356409072876
Validation loss: 2.1850731629197315

Epoch: 6| Step: 8
Training loss: 2.2514970302581787
Validation loss: 2.181569658299928

Epoch: 6| Step: 9
Training loss: 2.373150587081909
Validation loss: 2.1680285904997136

Epoch: 6| Step: 10
Training loss: 2.442365884780884
Validation loss: 2.1652589639027915

Epoch: 6| Step: 11
Training loss: 1.7522962093353271
Validation loss: 2.1711195335593274

Epoch: 6| Step: 12
Training loss: 2.240475654602051
Validation loss: 2.1648399265863563

Epoch: 6| Step: 13
Training loss: 2.204819917678833
Validation loss: 2.170164951714136

Epoch: 208| Step: 0
Training loss: 2.0326528549194336
Validation loss: 2.181320910812706

Epoch: 6| Step: 1
Training loss: 1.944358468055725
Validation loss: 2.177802895986906

Epoch: 6| Step: 2
Training loss: 2.6054978370666504
Validation loss: 2.185590995255337

Epoch: 6| Step: 3
Training loss: 2.8786659240722656
Validation loss: 2.1904248652919645

Epoch: 6| Step: 4
Training loss: 2.4362335205078125
Validation loss: 2.182696306577293

Epoch: 6| Step: 5
Training loss: 2.630852699279785
Validation loss: 2.1882439608215005

Epoch: 6| Step: 6
Training loss: 2.1151282787323
Validation loss: 2.1674445316355717

Epoch: 6| Step: 7
Training loss: 2.951883316040039
Validation loss: 2.1685186688617994

Epoch: 6| Step: 8
Training loss: 1.6653409004211426
Validation loss: 2.15255400442308

Epoch: 6| Step: 9
Training loss: 2.705883741378784
Validation loss: 2.1500641351105063

Epoch: 6| Step: 10
Training loss: 2.4332785606384277
Validation loss: 2.159315170780305

Epoch: 6| Step: 11
Training loss: 1.9089715480804443
Validation loss: 2.153607883761006

Epoch: 6| Step: 12
Training loss: 2.393866539001465
Validation loss: 2.1537205378214517

Epoch: 6| Step: 13
Training loss: 1.4000805616378784
Validation loss: 2.151269502537225

Epoch: 209| Step: 0
Training loss: 2.5209360122680664
Validation loss: 2.167003165009201

Epoch: 6| Step: 1
Training loss: 2.1508231163024902
Validation loss: 2.193851742693173

Epoch: 6| Step: 2
Training loss: 2.738563060760498
Validation loss: 2.215552265926074

Epoch: 6| Step: 3
Training loss: 1.7715836763381958
Validation loss: 2.224298961700932

Epoch: 6| Step: 4
Training loss: 2.728822708129883
Validation loss: 2.2336483078618206

Epoch: 6| Step: 5
Training loss: 2.6115574836730957
Validation loss: 2.2259938909161474

Epoch: 6| Step: 6
Training loss: 2.5457260608673096
Validation loss: 2.195436526370305

Epoch: 6| Step: 7
Training loss: 2.9517855644226074
Validation loss: 2.190467934454641

Epoch: 6| Step: 8
Training loss: 2.1892964839935303
Validation loss: 2.1897444007217244

Epoch: 6| Step: 9
Training loss: 2.1727263927459717
Validation loss: 2.181954768396193

Epoch: 6| Step: 10
Training loss: 1.6149969100952148
Validation loss: 2.2015604767748105

Epoch: 6| Step: 11
Training loss: 2.194944381713867
Validation loss: 2.196638990474004

Epoch: 6| Step: 12
Training loss: 2.240347385406494
Validation loss: 2.190937549837174

Epoch: 6| Step: 13
Training loss: 2.2558846473693848
Validation loss: 2.1947286180270615

Epoch: 210| Step: 0
Training loss: 2.1616387367248535
Validation loss: 2.1997814447649064

Epoch: 6| Step: 1
Training loss: 1.8820865154266357
Validation loss: 2.187827879382718

Epoch: 6| Step: 2
Training loss: 2.5763967037200928
Validation loss: 2.1890056056361042

Epoch: 6| Step: 3
Training loss: 1.9231579303741455
Validation loss: 2.20321145108951

Epoch: 6| Step: 4
Training loss: 2.223052978515625
Validation loss: 2.2161372733372513

Epoch: 6| Step: 5
Training loss: 2.468705177307129
Validation loss: 2.2151729470940045

Epoch: 6| Step: 6
Training loss: 2.5255346298217773
Validation loss: 2.2080187233545447

Epoch: 6| Step: 7
Training loss: 2.225480556488037
Validation loss: 2.2039035212609077

Epoch: 6| Step: 8
Training loss: 2.2959797382354736
Validation loss: 2.1862041616952546

Epoch: 6| Step: 9
Training loss: 2.743180990219116
Validation loss: 2.1879988485766995

Epoch: 6| Step: 10
Training loss: 2.7446210384368896
Validation loss: 2.1769171504564184

Epoch: 6| Step: 11
Training loss: 2.049565315246582
Validation loss: 2.1613072464543004

Epoch: 6| Step: 12
Training loss: 2.995185136795044
Validation loss: 2.153470552095803

Epoch: 6| Step: 13
Training loss: 1.2971285581588745
Validation loss: 2.1410209799325592

Epoch: 211| Step: 0
Training loss: 1.7001118659973145
Validation loss: 2.148999987110015

Epoch: 6| Step: 1
Training loss: 2.4444453716278076
Validation loss: 2.1546768655059156

Epoch: 6| Step: 2
Training loss: 2.6520020961761475
Validation loss: 2.1767823773045696

Epoch: 6| Step: 3
Training loss: 2.8931832313537598
Validation loss: 2.2151802073242846

Epoch: 6| Step: 4
Training loss: 2.248673439025879
Validation loss: 2.2400752754621607

Epoch: 6| Step: 5
Training loss: 2.3424432277679443
Validation loss: 2.255072298870292

Epoch: 6| Step: 6
Training loss: 2.9351630210876465
Validation loss: 2.275306204313873

Epoch: 6| Step: 7
Training loss: 1.9488943815231323
Validation loss: 2.253456982233191

Epoch: 6| Step: 8
Training loss: 2.128946304321289
Validation loss: 2.2292626032265286

Epoch: 6| Step: 9
Training loss: 2.5722298622131348
Validation loss: 2.186789135779104

Epoch: 6| Step: 10
Training loss: 2.3555922508239746
Validation loss: 2.17179476317539

Epoch: 6| Step: 11
Training loss: 1.913264274597168
Validation loss: 2.1563890326407646

Epoch: 6| Step: 12
Training loss: 2.208868980407715
Validation loss: 2.149584536911339

Epoch: 6| Step: 13
Training loss: 2.6669914722442627
Validation loss: 2.1596567887131886

Epoch: 212| Step: 0
Training loss: 1.9467532634735107
Validation loss: 2.1732530414417224

Epoch: 6| Step: 1
Training loss: 1.850144386291504
Validation loss: 2.2027768755471833

Epoch: 6| Step: 2
Training loss: 3.122809648513794
Validation loss: 2.206347797506599

Epoch: 6| Step: 3
Training loss: 2.6169586181640625
Validation loss: 2.2114617952736477

Epoch: 6| Step: 4
Training loss: 3.001255750656128
Validation loss: 2.1855549427770797

Epoch: 6| Step: 5
Training loss: 2.3600072860717773
Validation loss: 2.151387027514878

Epoch: 6| Step: 6
Training loss: 2.450575351715088
Validation loss: 2.1425253216938307

Epoch: 6| Step: 7
Training loss: 1.934324860572815
Validation loss: 2.1370741680104244

Epoch: 6| Step: 8
Training loss: 2.3387789726257324
Validation loss: 2.122839771291261

Epoch: 6| Step: 9
Training loss: 2.1916558742523193
Validation loss: 2.1288865817490445

Epoch: 6| Step: 10
Training loss: 1.700000524520874
Validation loss: 2.121297174884427

Epoch: 6| Step: 11
Training loss: 2.672490358352661
Validation loss: 2.120331400184221

Epoch: 6| Step: 12
Training loss: 2.0542659759521484
Validation loss: 2.1303629823910293

Epoch: 6| Step: 13
Training loss: 2.5333125591278076
Validation loss: 2.1337256457216

Epoch: 213| Step: 0
Training loss: 1.8364698886871338
Validation loss: 2.142989984122656

Epoch: 6| Step: 1
Training loss: 1.8562626838684082
Validation loss: 2.161960094205795

Epoch: 6| Step: 2
Training loss: 1.8496906757354736
Validation loss: 2.1783544068695395

Epoch: 6| Step: 3
Training loss: 2.0289146900177
Validation loss: 2.2143394934233798

Epoch: 6| Step: 4
Training loss: 2.633356809616089
Validation loss: 2.239110359581568

Epoch: 6| Step: 5
Training loss: 1.886758804321289
Validation loss: 2.221918011224398

Epoch: 6| Step: 6
Training loss: 2.1803507804870605
Validation loss: 2.2184976531613256

Epoch: 6| Step: 7
Training loss: 2.7920985221862793
Validation loss: 2.212597547038909

Epoch: 6| Step: 8
Training loss: 2.280597686767578
Validation loss: 2.2035423453136156

Epoch: 6| Step: 9
Training loss: 2.9071459770202637
Validation loss: 2.2022199361555037

Epoch: 6| Step: 10
Training loss: 2.5491645336151123
Validation loss: 2.194630830518661

Epoch: 6| Step: 11
Training loss: 2.4257586002349854
Validation loss: 2.1968337451258013

Epoch: 6| Step: 12
Training loss: 2.756074905395508
Validation loss: 2.1993215237894366

Epoch: 6| Step: 13
Training loss: 2.594289779663086
Validation loss: 2.17792232190409

Epoch: 214| Step: 0
Training loss: 2.8638548851013184
Validation loss: 2.1800687607898506

Epoch: 6| Step: 1
Training loss: 2.213535785675049
Validation loss: 2.178949302242648

Epoch: 6| Step: 2
Training loss: 2.3582236766815186
Validation loss: 2.178545839043074

Epoch: 6| Step: 3
Training loss: 2.824613094329834
Validation loss: 2.1733213778465026

Epoch: 6| Step: 4
Training loss: 1.6863644123077393
Validation loss: 2.156574604331806

Epoch: 6| Step: 5
Training loss: 2.078327178955078
Validation loss: 2.160784024064259

Epoch: 6| Step: 6
Training loss: 2.066112756729126
Validation loss: 2.1622777702987834

Epoch: 6| Step: 7
Training loss: 2.706120014190674
Validation loss: 2.1632230102374987

Epoch: 6| Step: 8
Training loss: 1.4245102405548096
Validation loss: 2.164118298920252

Epoch: 6| Step: 9
Training loss: 2.4548556804656982
Validation loss: 2.1667394125333397

Epoch: 6| Step: 10
Training loss: 2.181885004043579
Validation loss: 2.1577445999268563

Epoch: 6| Step: 11
Training loss: 2.5745511054992676
Validation loss: 2.1539762430293585

Epoch: 6| Step: 12
Training loss: 2.652827262878418
Validation loss: 2.1489672173735914

Epoch: 6| Step: 13
Training loss: 2.1488122940063477
Validation loss: 2.1389119971183037

Epoch: 215| Step: 0
Training loss: 2.2287521362304688
Validation loss: 2.13719355803664

Epoch: 6| Step: 1
Training loss: 1.9387599229812622
Validation loss: 2.1351958731169343

Epoch: 6| Step: 2
Training loss: 2.021569013595581
Validation loss: 2.138917766591554

Epoch: 6| Step: 3
Training loss: 2.908461570739746
Validation loss: 2.1380111684081373

Epoch: 6| Step: 4
Training loss: 2.2582907676696777
Validation loss: 2.1335984994006414

Epoch: 6| Step: 5
Training loss: 1.9145095348358154
Validation loss: 2.132435360262471

Epoch: 6| Step: 6
Training loss: 2.6272478103637695
Validation loss: 2.1385676783900105

Epoch: 6| Step: 7
Training loss: 2.4313926696777344
Validation loss: 2.1482635492919595

Epoch: 6| Step: 8
Training loss: 2.711514472961426
Validation loss: 2.1656516469934934

Epoch: 6| Step: 9
Training loss: 2.374382972717285
Validation loss: 2.1917272972804245

Epoch: 6| Step: 10
Training loss: 1.8395261764526367
Validation loss: 2.193354123382158

Epoch: 6| Step: 11
Training loss: 1.9058735370635986
Validation loss: 2.1968093123487247

Epoch: 6| Step: 12
Training loss: 2.4872775077819824
Validation loss: 2.190944745976438

Epoch: 6| Step: 13
Training loss: 2.581603527069092
Validation loss: 2.1963381664727324

Epoch: 216| Step: 0
Training loss: 1.9453771114349365
Validation loss: 2.194983387506136

Epoch: 6| Step: 1
Training loss: 1.9228851795196533
Validation loss: 2.205390686629921

Epoch: 6| Step: 2
Training loss: 1.9866418838500977
Validation loss: 2.203894579282371

Epoch: 6| Step: 3
Training loss: 2.5801384449005127
Validation loss: 2.1970705704022477

Epoch: 6| Step: 4
Training loss: 2.209860324859619
Validation loss: 2.1939238937952186

Epoch: 6| Step: 5
Training loss: 2.123404026031494
Validation loss: 2.174800565165858

Epoch: 6| Step: 6
Training loss: 2.1912407875061035
Validation loss: 2.17928058614013

Epoch: 6| Step: 7
Training loss: 2.8474268913269043
Validation loss: 2.169298686007018

Epoch: 6| Step: 8
Training loss: 2.362677812576294
Validation loss: 2.1740130762900076

Epoch: 6| Step: 9
Training loss: 1.8390337228775024
Validation loss: 2.176957759805905

Epoch: 6| Step: 10
Training loss: 2.6222896575927734
Validation loss: 2.1946911427282516

Epoch: 6| Step: 11
Training loss: 2.6219241619110107
Validation loss: 2.177540276640205

Epoch: 6| Step: 12
Training loss: 2.330300807952881
Validation loss: 2.1689587280314457

Epoch: 6| Step: 13
Training loss: 2.766716957092285
Validation loss: 2.1580564898829304

Epoch: 217| Step: 0
Training loss: 2.876455068588257
Validation loss: 2.1557108509925103

Epoch: 6| Step: 1
Training loss: 1.4689743518829346
Validation loss: 2.151685727539883

Epoch: 6| Step: 2
Training loss: 2.679211139678955
Validation loss: 2.1485337018966675

Epoch: 6| Step: 3
Training loss: 2.0889642238616943
Validation loss: 2.1526262965253604

Epoch: 6| Step: 4
Training loss: 2.043149471282959
Validation loss: 2.1635437062991563

Epoch: 6| Step: 5
Training loss: 1.7925353050231934
Validation loss: 2.17375623923476

Epoch: 6| Step: 6
Training loss: 2.5257644653320312
Validation loss: 2.1696731018763717

Epoch: 6| Step: 7
Training loss: 1.918222188949585
Validation loss: 2.1816336570247525

Epoch: 6| Step: 8
Training loss: 3.0881996154785156
Validation loss: 2.176624257077453

Epoch: 6| Step: 9
Training loss: 2.685873508453369
Validation loss: 2.1672611595481954

Epoch: 6| Step: 10
Training loss: 2.2934200763702393
Validation loss: 2.1577248265666347

Epoch: 6| Step: 11
Training loss: 1.9210585355758667
Validation loss: 2.1573564365345943

Epoch: 6| Step: 12
Training loss: 2.58217716217041
Validation loss: 2.1579704464122815

Epoch: 6| Step: 13
Training loss: 1.63193941116333
Validation loss: 2.1565370841692855

Epoch: 218| Step: 0
Training loss: 1.4219403266906738
Validation loss: 2.1633767902210193

Epoch: 6| Step: 1
Training loss: 2.8711671829223633
Validation loss: 2.175464604490547

Epoch: 6| Step: 2
Training loss: 2.5024373531341553
Validation loss: 2.202469192525392

Epoch: 6| Step: 3
Training loss: 1.9081611633300781
Validation loss: 2.1897710702752553

Epoch: 6| Step: 4
Training loss: 2.2724575996398926
Validation loss: 2.1531772139251872

Epoch: 6| Step: 5
Training loss: 2.176219940185547
Validation loss: 2.1262291810845815

Epoch: 6| Step: 6
Training loss: 2.513887882232666
Validation loss: 2.133622984732351

Epoch: 6| Step: 7
Training loss: 1.8969793319702148
Validation loss: 2.1392768941899782

Epoch: 6| Step: 8
Training loss: 2.731240749359131
Validation loss: 2.1344967580610708

Epoch: 6| Step: 9
Training loss: 2.38735294342041
Validation loss: 2.144272235132033

Epoch: 6| Step: 10
Training loss: 2.273390054702759
Validation loss: 2.1499441618560464

Epoch: 6| Step: 11
Training loss: 2.2918052673339844
Validation loss: 2.1465631787494948

Epoch: 6| Step: 12
Training loss: 2.622238874435425
Validation loss: 2.166736302837249

Epoch: 6| Step: 13
Training loss: 2.215615749359131
Validation loss: 2.1722470355290238

Epoch: 219| Step: 0
Training loss: 2.682246208190918
Validation loss: 2.180228743501889

Epoch: 6| Step: 1
Training loss: 2.982696533203125
Validation loss: 2.183767621235181

Epoch: 6| Step: 2
Training loss: 2.1373326778411865
Validation loss: 2.1745712782747004

Epoch: 6| Step: 3
Training loss: 2.3238539695739746
Validation loss: 2.1839881122753186

Epoch: 6| Step: 4
Training loss: 2.3960719108581543
Validation loss: 2.171558177599343

Epoch: 6| Step: 5
Training loss: 2.5708770751953125
Validation loss: 2.1568445159542944

Epoch: 6| Step: 6
Training loss: 2.1060001850128174
Validation loss: 2.1273482614947903

Epoch: 6| Step: 7
Training loss: 3.3388538360595703
Validation loss: 2.132671647174384

Epoch: 6| Step: 8
Training loss: 1.8348337411880493
Validation loss: 2.1156639027339157

Epoch: 6| Step: 9
Training loss: 1.8338664770126343
Validation loss: 2.1097241165817424

Epoch: 6| Step: 10
Training loss: 1.5625691413879395
Validation loss: 2.13378942525515

Epoch: 6| Step: 11
Training loss: 2.1589555740356445
Validation loss: 2.14428949099715

Epoch: 6| Step: 12
Training loss: 1.889643907546997
Validation loss: 2.144704975107665

Epoch: 6| Step: 13
Training loss: 1.6820862293243408
Validation loss: 2.158697880724425

Epoch: 220| Step: 0
Training loss: 1.9101035594940186
Validation loss: 2.147375491357619

Epoch: 6| Step: 1
Training loss: 1.979019045829773
Validation loss: 2.143140971019704

Epoch: 6| Step: 2
Training loss: 2.5717263221740723
Validation loss: 2.13805406067961

Epoch: 6| Step: 3
Training loss: 2.390015125274658
Validation loss: 2.1356755020797893

Epoch: 6| Step: 4
Training loss: 1.9338953495025635
Validation loss: 2.1213569833386328

Epoch: 6| Step: 5
Training loss: 2.7007663249969482
Validation loss: 2.127874940954229

Epoch: 6| Step: 6
Training loss: 2.3184940814971924
Validation loss: 2.1282541854407198

Epoch: 6| Step: 7
Training loss: 2.5488874912261963
Validation loss: 2.1162073048212195

Epoch: 6| Step: 8
Training loss: 2.303631544113159
Validation loss: 2.122914433479309

Epoch: 6| Step: 9
Training loss: 1.945209264755249
Validation loss: 2.1136210733844387

Epoch: 6| Step: 10
Training loss: 2.556008815765381
Validation loss: 2.137675921122233

Epoch: 6| Step: 11
Training loss: 2.8821394443511963
Validation loss: 2.155532146012911

Epoch: 6| Step: 12
Training loss: 1.6876410245895386
Validation loss: 2.162828614634852

Epoch: 6| Step: 13
Training loss: 2.016456365585327
Validation loss: 2.1513544128787134

Epoch: 221| Step: 0
Training loss: 2.230935573577881
Validation loss: 2.149530303093695

Epoch: 6| Step: 1
Training loss: 2.3924994468688965
Validation loss: 2.1269772373219973

Epoch: 6| Step: 2
Training loss: 1.4039804935455322
Validation loss: 2.138109314826227

Epoch: 6| Step: 3
Training loss: 1.9561638832092285
Validation loss: 2.146481949795959

Epoch: 6| Step: 4
Training loss: 2.170400381088257
Validation loss: 2.1424164848942913

Epoch: 6| Step: 5
Training loss: 2.2393767833709717
Validation loss: 2.141903861876457

Epoch: 6| Step: 6
Training loss: 2.1449851989746094
Validation loss: 2.1486659716534358

Epoch: 6| Step: 7
Training loss: 3.0885062217712402
Validation loss: 2.134347149120864

Epoch: 6| Step: 8
Training loss: 3.056854724884033
Validation loss: 2.1495470513579664

Epoch: 6| Step: 9
Training loss: 1.8758422136306763
Validation loss: 2.1802844027037263

Epoch: 6| Step: 10
Training loss: 2.7671408653259277
Validation loss: 2.229611116070901

Epoch: 6| Step: 11
Training loss: 1.8775086402893066
Validation loss: 2.2377567368168987

Epoch: 6| Step: 12
Training loss: 2.727663040161133
Validation loss: 2.2035907545397357

Epoch: 6| Step: 13
Training loss: 1.9075876474380493
Validation loss: 2.1354498504310526

Epoch: 222| Step: 0
Training loss: 1.4655351638793945
Validation loss: 2.12740296189503

Epoch: 6| Step: 1
Training loss: 1.8950432538986206
Validation loss: 2.128576717069072

Epoch: 6| Step: 2
Training loss: 1.577926754951477
Validation loss: 2.115816652133901

Epoch: 6| Step: 3
Training loss: 2.2997183799743652
Validation loss: 2.1257258281912854

Epoch: 6| Step: 4
Training loss: 2.417764186859131
Validation loss: 2.1211821776564403

Epoch: 6| Step: 5
Training loss: 2.203554153442383
Validation loss: 2.118559124649212

Epoch: 6| Step: 6
Training loss: 1.7800102233886719
Validation loss: 2.1326697769985405

Epoch: 6| Step: 7
Training loss: 2.727006196975708
Validation loss: 2.154845404368575

Epoch: 6| Step: 8
Training loss: 2.3848936557769775
Validation loss: 2.1619242493824293

Epoch: 6| Step: 9
Training loss: 2.9751503467559814
Validation loss: 2.160497952533025

Epoch: 6| Step: 10
Training loss: 2.3685810565948486
Validation loss: 2.164449063680505

Epoch: 6| Step: 11
Training loss: 2.9869799613952637
Validation loss: 2.1523988323826946

Epoch: 6| Step: 12
Training loss: 2.1556758880615234
Validation loss: 2.1514999943394817

Epoch: 6| Step: 13
Training loss: 2.471806287765503
Validation loss: 2.1520679996859644

Epoch: 223| Step: 0
Training loss: 2.1095376014709473
Validation loss: 2.151777921184417

Epoch: 6| Step: 1
Training loss: 2.3306422233581543
Validation loss: 2.1574391626542613

Epoch: 6| Step: 2
Training loss: 2.512021541595459
Validation loss: 2.147342804939516

Epoch: 6| Step: 3
Training loss: 2.8605966567993164
Validation loss: 2.145799139494537

Epoch: 6| Step: 4
Training loss: 2.0429911613464355
Validation loss: 2.1480744987405758

Epoch: 6| Step: 5
Training loss: 2.0757699012756348
Validation loss: 2.1528747299666047

Epoch: 6| Step: 6
Training loss: 1.8479382991790771
Validation loss: 2.144817260003859

Epoch: 6| Step: 7
Training loss: 1.6172380447387695
Validation loss: 2.143075935302242

Epoch: 6| Step: 8
Training loss: 2.6957530975341797
Validation loss: 2.1352565391089326

Epoch: 6| Step: 9
Training loss: 2.609376907348633
Validation loss: 2.1221538512937483

Epoch: 6| Step: 10
Training loss: 2.030301094055176
Validation loss: 2.1113548330081406

Epoch: 6| Step: 11
Training loss: 2.640850305557251
Validation loss: 2.1124099608390563

Epoch: 6| Step: 12
Training loss: 2.491647720336914
Validation loss: 2.12449832629132

Epoch: 6| Step: 13
Training loss: 0.9446871280670166
Validation loss: 2.1204286954736196

Epoch: 224| Step: 0
Training loss: 2.3586270809173584
Validation loss: 2.11780499258349

Epoch: 6| Step: 1
Training loss: 1.9743657112121582
Validation loss: 2.122818926329254

Epoch: 6| Step: 2
Training loss: 2.353942394256592
Validation loss: 2.1191867461768528

Epoch: 6| Step: 3
Training loss: 3.0460262298583984
Validation loss: 2.1243497735710553

Epoch: 6| Step: 4
Training loss: 2.043578863143921
Validation loss: 2.141588559714697

Epoch: 6| Step: 5
Training loss: 1.8604447841644287
Validation loss: 2.164480040150304

Epoch: 6| Step: 6
Training loss: 2.371399164199829
Validation loss: 2.1759808396780365

Epoch: 6| Step: 7
Training loss: 2.111994743347168
Validation loss: 2.1953327963429112

Epoch: 6| Step: 8
Training loss: 2.3929386138916016
Validation loss: 2.1656194758671585

Epoch: 6| Step: 9
Training loss: 1.5637526512145996
Validation loss: 2.1546436073959514

Epoch: 6| Step: 10
Training loss: 2.234661817550659
Validation loss: 2.1587990125020347

Epoch: 6| Step: 11
Training loss: 2.743741512298584
Validation loss: 2.163190587874382

Epoch: 6| Step: 12
Training loss: 2.1540377140045166
Validation loss: 2.1535009517464587

Epoch: 6| Step: 13
Training loss: 1.9745763540267944
Validation loss: 2.147361756652914

Epoch: 225| Step: 0
Training loss: 2.638336181640625
Validation loss: 2.1329306710150933

Epoch: 6| Step: 1
Training loss: 1.838913917541504
Validation loss: 2.1379869561041556

Epoch: 6| Step: 2
Training loss: 1.690697431564331
Validation loss: 2.136881374543713

Epoch: 6| Step: 3
Training loss: 2.254807472229004
Validation loss: 2.1354586488457135

Epoch: 6| Step: 4
Training loss: 2.1894710063934326
Validation loss: 2.147305865441599

Epoch: 6| Step: 5
Training loss: 1.8802355527877808
Validation loss: 2.135402100060576

Epoch: 6| Step: 6
Training loss: 2.0550646781921387
Validation loss: 2.126915757374097

Epoch: 6| Step: 7
Training loss: 2.841144561767578
Validation loss: 2.1128968679776756

Epoch: 6| Step: 8
Training loss: 2.4012184143066406
Validation loss: 2.115726902920713

Epoch: 6| Step: 9
Training loss: 2.3101913928985596
Validation loss: 2.1131168873079362

Epoch: 6| Step: 10
Training loss: 2.3831796646118164
Validation loss: 2.0966683818447973

Epoch: 6| Step: 11
Training loss: 2.277970552444458
Validation loss: 2.1070402078731085

Epoch: 6| Step: 12
Training loss: 1.9779280424118042
Validation loss: 2.125247429775935

Epoch: 6| Step: 13
Training loss: 2.497960090637207
Validation loss: 2.1548522697981967

Epoch: 226| Step: 0
Training loss: 2.6691861152648926
Validation loss: 2.168387407897621

Epoch: 6| Step: 1
Training loss: 2.5186824798583984
Validation loss: 2.158260686423189

Epoch: 6| Step: 2
Training loss: 2.3546109199523926
Validation loss: 2.1730740019070205

Epoch: 6| Step: 3
Training loss: 1.974539041519165
Validation loss: 2.1430291373242616

Epoch: 6| Step: 4
Training loss: 2.6372008323669434
Validation loss: 2.1418487820574033

Epoch: 6| Step: 5
Training loss: 2.146235942840576
Validation loss: 2.1439391489951842

Epoch: 6| Step: 6
Training loss: 1.8200894594192505
Validation loss: 2.143781587641726

Epoch: 6| Step: 7
Training loss: 2.126349449157715
Validation loss: 2.1654841412780104

Epoch: 6| Step: 8
Training loss: 2.06671404838562
Validation loss: 2.151645039999357

Epoch: 6| Step: 9
Training loss: 2.3148481845855713
Validation loss: 2.1684068300390757

Epoch: 6| Step: 10
Training loss: 1.5868115425109863
Validation loss: 2.1800506973779328

Epoch: 6| Step: 11
Training loss: 2.403524160385132
Validation loss: 2.1957042371073077

Epoch: 6| Step: 12
Training loss: 2.284877300262451
Validation loss: 2.2063930726820424

Epoch: 6| Step: 13
Training loss: 2.0774831771850586
Validation loss: 2.2013117267239477

Epoch: 227| Step: 0
Training loss: 1.9909402132034302
Validation loss: 2.2293073567011024

Epoch: 6| Step: 1
Training loss: 2.873690366744995
Validation loss: 2.230813544283631

Epoch: 6| Step: 2
Training loss: 2.254704475402832
Validation loss: 2.2116666404149865

Epoch: 6| Step: 3
Training loss: 2.5202369689941406
Validation loss: 2.1673155958934496

Epoch: 6| Step: 4
Training loss: 1.856940746307373
Validation loss: 2.16016508430563

Epoch: 6| Step: 5
Training loss: 1.988577127456665
Validation loss: 2.132121343766489

Epoch: 6| Step: 6
Training loss: 2.419023036956787
Validation loss: 2.1214380918010587

Epoch: 6| Step: 7
Training loss: 2.529531478881836
Validation loss: 2.1229842606411187

Epoch: 6| Step: 8
Training loss: 2.0645649433135986
Validation loss: 2.1221384848317792

Epoch: 6| Step: 9
Training loss: 1.8104257583618164
Validation loss: 2.121649960035919

Epoch: 6| Step: 10
Training loss: 2.0458836555480957
Validation loss: 2.119399465540404

Epoch: 6| Step: 11
Training loss: 1.6307467222213745
Validation loss: 2.1141717049383346

Epoch: 6| Step: 12
Training loss: 2.0785446166992188
Validation loss: 2.1366023017514135

Epoch: 6| Step: 13
Training loss: 3.127141237258911
Validation loss: 2.1527049900383077

Epoch: 228| Step: 0
Training loss: 2.3468151092529297
Validation loss: 2.199135470133956

Epoch: 6| Step: 1
Training loss: 2.956360340118408
Validation loss: 2.1946636451187955

Epoch: 6| Step: 2
Training loss: 1.6225216388702393
Validation loss: 2.209528641034198

Epoch: 6| Step: 3
Training loss: 2.1887283325195312
Validation loss: 2.1750041977051766

Epoch: 6| Step: 4
Training loss: 1.7087059020996094
Validation loss: 2.16890529406968

Epoch: 6| Step: 5
Training loss: 1.5512675046920776
Validation loss: 2.1542396519773748

Epoch: 6| Step: 6
Training loss: 2.2048397064208984
Validation loss: 2.1446635479568155

Epoch: 6| Step: 7
Training loss: 1.2930042743682861
Validation loss: 2.1463952961788384

Epoch: 6| Step: 8
Training loss: 2.16379976272583
Validation loss: 2.1529304699231218

Epoch: 6| Step: 9
Training loss: 3.364741802215576
Validation loss: 2.1614081628860964

Epoch: 6| Step: 10
Training loss: 2.6313905715942383
Validation loss: 2.158393995736235

Epoch: 6| Step: 11
Training loss: 2.8918447494506836
Validation loss: 2.1381493178747033

Epoch: 6| Step: 12
Training loss: 1.9537619352340698
Validation loss: 2.1078430298836

Epoch: 6| Step: 13
Training loss: 1.9772334098815918
Validation loss: 2.1124833501795286

Epoch: 229| Step: 0
Training loss: 2.0989460945129395
Validation loss: 2.154580895618726

Epoch: 6| Step: 1
Training loss: 2.9619016647338867
Validation loss: 2.1932776794638684

Epoch: 6| Step: 2
Training loss: 2.266660213470459
Validation loss: 2.1850840583924325

Epoch: 6| Step: 3
Training loss: 2.5062594413757324
Validation loss: 2.2078190080581175

Epoch: 6| Step: 4
Training loss: 1.850339651107788
Validation loss: 2.190427928842524

Epoch: 6| Step: 5
Training loss: 2.034346580505371
Validation loss: 2.1845112180197113

Epoch: 6| Step: 6
Training loss: 1.9701950550079346
Validation loss: 2.171348524349992

Epoch: 6| Step: 7
Training loss: 2.6419689655303955
Validation loss: 2.1491711421679427

Epoch: 6| Step: 8
Training loss: 1.9953762292861938
Validation loss: 2.1292183578655286

Epoch: 6| Step: 9
Training loss: 1.9670008420944214
Validation loss: 2.1185670104078067

Epoch: 6| Step: 10
Training loss: 2.677762031555176
Validation loss: 2.1330547102036013

Epoch: 6| Step: 11
Training loss: 2.163219451904297
Validation loss: 2.14318153935094

Epoch: 6| Step: 12
Training loss: 1.7257590293884277
Validation loss: 2.1451361179351807

Epoch: 6| Step: 13
Training loss: 2.2408084869384766
Validation loss: 2.1470653190407702

Epoch: 230| Step: 0
Training loss: 1.9825618267059326
Validation loss: 2.159918151875978

Epoch: 6| Step: 1
Training loss: 1.867380976676941
Validation loss: 2.15274735676345

Epoch: 6| Step: 2
Training loss: 2.018240451812744
Validation loss: 2.184733666399474

Epoch: 6| Step: 3
Training loss: 2.954716682434082
Validation loss: 2.18619276631263

Epoch: 6| Step: 4
Training loss: 2.5396087169647217
Validation loss: 2.2026489037339405

Epoch: 6| Step: 5
Training loss: 2.5381102561950684
Validation loss: 2.213046186713762

Epoch: 6| Step: 6
Training loss: 2.1032888889312744
Validation loss: 2.198807462569206

Epoch: 6| Step: 7
Training loss: 1.943209171295166
Validation loss: 2.1656482194059636

Epoch: 6| Step: 8
Training loss: 2.261185646057129
Validation loss: 2.161417620156401

Epoch: 6| Step: 9
Training loss: 1.5496217012405396
Validation loss: 2.162209483884996

Epoch: 6| Step: 10
Training loss: 2.503303050994873
Validation loss: 2.13740429570598

Epoch: 6| Step: 11
Training loss: 1.4734718799591064
Validation loss: 2.1507773783899125

Epoch: 6| Step: 12
Training loss: 2.2962517738342285
Validation loss: 2.1291365597837713

Epoch: 6| Step: 13
Training loss: 2.8436193466186523
Validation loss: 2.133192353351142

Epoch: 231| Step: 0
Training loss: 1.3984224796295166
Validation loss: 2.130409707305252

Epoch: 6| Step: 1
Training loss: 1.966300368309021
Validation loss: 2.1371570902486003

Epoch: 6| Step: 2
Training loss: 2.361358165740967
Validation loss: 2.13635157256998

Epoch: 6| Step: 3
Training loss: 2.1238465309143066
Validation loss: 2.1328863892503964

Epoch: 6| Step: 4
Training loss: 2.1954925060272217
Validation loss: 2.129446844900808

Epoch: 6| Step: 5
Training loss: 2.7182908058166504
Validation loss: 2.1311121422757386

Epoch: 6| Step: 6
Training loss: 1.834154725074768
Validation loss: 2.1254795289808706

Epoch: 6| Step: 7
Training loss: 1.7907161712646484
Validation loss: 2.129466038878246

Epoch: 6| Step: 8
Training loss: 2.2046167850494385
Validation loss: 2.1489699886691187

Epoch: 6| Step: 9
Training loss: 2.5774502754211426
Validation loss: 2.1804876891515588

Epoch: 6| Step: 10
Training loss: 2.6099905967712402
Validation loss: 2.1893808508432038

Epoch: 6| Step: 11
Training loss: 2.8322510719299316
Validation loss: 2.203331780690019

Epoch: 6| Step: 12
Training loss: 1.6805720329284668
Validation loss: 2.194194973156016

Epoch: 6| Step: 13
Training loss: 2.4801948070526123
Validation loss: 2.1522399507543093

Epoch: 232| Step: 0
Training loss: 1.9220322370529175
Validation loss: 2.128439054694227

Epoch: 6| Step: 1
Training loss: 2.4644036293029785
Validation loss: 2.124680453731168

Epoch: 6| Step: 2
Training loss: 2.4478936195373535
Validation loss: 2.1174986567548526

Epoch: 6| Step: 3
Training loss: 1.9279544353485107
Validation loss: 2.132273439438112

Epoch: 6| Step: 4
Training loss: 2.3232359886169434
Validation loss: 2.14110557750989

Epoch: 6| Step: 5
Training loss: 2.1592841148376465
Validation loss: 2.1460959501163934

Epoch: 6| Step: 6
Training loss: 2.169185161590576
Validation loss: 2.15901699373799

Epoch: 6| Step: 7
Training loss: 2.367340564727783
Validation loss: 2.146628277276152

Epoch: 6| Step: 8
Training loss: 1.8088301420211792
Validation loss: 2.1371284915554907

Epoch: 6| Step: 9
Training loss: 2.57944917678833
Validation loss: 2.13122381189818

Epoch: 6| Step: 10
Training loss: 2.242405652999878
Validation loss: 2.1284075398598947

Epoch: 6| Step: 11
Training loss: 2.169694185256958
Validation loss: 2.163875586243086

Epoch: 6| Step: 12
Training loss: 1.9612572193145752
Validation loss: 2.214673880607851

Epoch: 6| Step: 13
Training loss: 2.2522125244140625
Validation loss: 2.20460105711414

Epoch: 233| Step: 0
Training loss: 1.7179688215255737
Validation loss: 2.186228059953259

Epoch: 6| Step: 1
Training loss: 2.378279685974121
Validation loss: 2.161092572314765

Epoch: 6| Step: 2
Training loss: 2.8054819107055664
Validation loss: 2.149788051523188

Epoch: 6| Step: 3
Training loss: 2.5593323707580566
Validation loss: 2.12277074398533

Epoch: 6| Step: 4
Training loss: 1.6065914630889893
Validation loss: 2.1144622205406107

Epoch: 6| Step: 5
Training loss: 1.784433126449585
Validation loss: 2.1061990555896553

Epoch: 6| Step: 6
Training loss: 2.6294617652893066
Validation loss: 2.100667081853395

Epoch: 6| Step: 7
Training loss: 2.4991745948791504
Validation loss: 2.099691734519056

Epoch: 6| Step: 8
Training loss: 1.9419797658920288
Validation loss: 2.0860176778608754

Epoch: 6| Step: 9
Training loss: 2.398507595062256
Validation loss: 2.1009322212588404

Epoch: 6| Step: 10
Training loss: 2.674406051635742
Validation loss: 2.0991563643178632

Epoch: 6| Step: 11
Training loss: 1.914049744606018
Validation loss: 2.1108384017021424

Epoch: 6| Step: 12
Training loss: 1.5976753234863281
Validation loss: 2.105355606284193

Epoch: 6| Step: 13
Training loss: 1.2796200513839722
Validation loss: 2.125454205338673

Epoch: 234| Step: 0
Training loss: 2.232207775115967
Validation loss: 2.15093816736693

Epoch: 6| Step: 1
Training loss: 2.7361855506896973
Validation loss: 2.185007326064571

Epoch: 6| Step: 2
Training loss: 1.6822681427001953
Validation loss: 2.209357502639935

Epoch: 6| Step: 3
Training loss: 2.1405081748962402
Validation loss: 2.1969351614675214

Epoch: 6| Step: 4
Training loss: 2.432586669921875
Validation loss: 2.225236633772491

Epoch: 6| Step: 5
Training loss: 1.934166431427002
Validation loss: 2.199159028709576

Epoch: 6| Step: 6
Training loss: 2.058492660522461
Validation loss: 2.1726413978043424

Epoch: 6| Step: 7
Training loss: 2.2042548656463623
Validation loss: 2.1617259517792733

Epoch: 6| Step: 8
Training loss: 2.4149742126464844
Validation loss: 2.1660503930942987

Epoch: 6| Step: 9
Training loss: 2.697605848312378
Validation loss: 2.164771669654436

Epoch: 6| Step: 10
Training loss: 1.7979868650436401
Validation loss: 2.1750549090805875

Epoch: 6| Step: 11
Training loss: 1.8372406959533691
Validation loss: 2.2008372763151764

Epoch: 6| Step: 12
Training loss: 1.9880516529083252
Validation loss: 2.207987141865556

Epoch: 6| Step: 13
Training loss: 2.1694135665893555
Validation loss: 2.2126193533661547

Epoch: 235| Step: 0
Training loss: 2.1817941665649414
Validation loss: 2.2012518605878277

Epoch: 6| Step: 1
Training loss: 2.2895419597625732
Validation loss: 2.1667264341026224

Epoch: 6| Step: 2
Training loss: 1.634475827217102
Validation loss: 2.152171705358772

Epoch: 6| Step: 3
Training loss: 2.2940945625305176
Validation loss: 2.1174979940537484

Epoch: 6| Step: 4
Training loss: 2.741318702697754
Validation loss: 2.0952622659744753

Epoch: 6| Step: 5
Training loss: 2.394925832748413
Validation loss: 2.085620285362326

Epoch: 6| Step: 6
Training loss: 2.6866915225982666
Validation loss: 2.0765061634843067

Epoch: 6| Step: 7
Training loss: 1.6462578773498535
Validation loss: 2.0769835646434496

Epoch: 6| Step: 8
Training loss: 1.8514142036437988
Validation loss: 2.0772790460176367

Epoch: 6| Step: 9
Training loss: 2.534620761871338
Validation loss: 2.0878509680430093

Epoch: 6| Step: 10
Training loss: 2.152310609817505
Validation loss: 2.104555691442182

Epoch: 6| Step: 11
Training loss: 2.3327856063842773
Validation loss: 2.1186616933473976

Epoch: 6| Step: 12
Training loss: 2.143615245819092
Validation loss: 2.1314050536001883

Epoch: 6| Step: 13
Training loss: 1.3476366996765137
Validation loss: 2.1326673107762493

Epoch: 236| Step: 0
Training loss: 2.513428211212158
Validation loss: 2.1296886192855013

Epoch: 6| Step: 1
Training loss: 2.4122817516326904
Validation loss: 2.1242361478908087

Epoch: 6| Step: 2
Training loss: 1.5655465126037598
Validation loss: 2.1338217078998523

Epoch: 6| Step: 3
Training loss: 1.9205697774887085
Validation loss: 2.116859543708063

Epoch: 6| Step: 4
Training loss: 2.2333250045776367
Validation loss: 2.129712825180382

Epoch: 6| Step: 5
Training loss: 2.293327808380127
Validation loss: 2.1404310323858775

Epoch: 6| Step: 6
Training loss: 2.0825202465057373
Validation loss: 2.1457580622806343

Epoch: 6| Step: 7
Training loss: 2.033383369445801
Validation loss: 2.157885818071263

Epoch: 6| Step: 8
Training loss: 2.1376237869262695
Validation loss: 2.1642943402772308

Epoch: 6| Step: 9
Training loss: 1.982654094696045
Validation loss: 2.173704934376542

Epoch: 6| Step: 10
Training loss: 2.2820334434509277
Validation loss: 2.171206102576307

Epoch: 6| Step: 11
Training loss: 1.911726713180542
Validation loss: 2.1578679161687053

Epoch: 6| Step: 12
Training loss: 2.617485761642456
Validation loss: 2.1689046634140836

Epoch: 6| Step: 13
Training loss: 2.126089334487915
Validation loss: 2.16874893506368

Epoch: 237| Step: 0
Training loss: 2.4586856365203857
Validation loss: 2.1801371638492872

Epoch: 6| Step: 1
Training loss: 2.1512153148651123
Validation loss: 2.1434278821432464

Epoch: 6| Step: 2
Training loss: 2.3639676570892334
Validation loss: 2.126499068352484

Epoch: 6| Step: 3
Training loss: 1.9397146701812744
Validation loss: 2.1140864561962824

Epoch: 6| Step: 4
Training loss: 2.2020552158355713
Validation loss: 2.095190663491526

Epoch: 6| Step: 5
Training loss: 1.720719575881958
Validation loss: 2.0875824574501283

Epoch: 6| Step: 6
Training loss: 2.860103130340576
Validation loss: 2.0848328272501626

Epoch: 6| Step: 7
Training loss: 2.2819366455078125
Validation loss: 2.089733108397453

Epoch: 6| Step: 8
Training loss: 1.8652734756469727
Validation loss: 2.0855063802452496

Epoch: 6| Step: 9
Training loss: 1.6977858543395996
Validation loss: 2.0984561750965733

Epoch: 6| Step: 10
Training loss: 1.9711703062057495
Validation loss: 2.093586193617954

Epoch: 6| Step: 11
Training loss: 1.929085612297058
Validation loss: 2.099352939154512

Epoch: 6| Step: 12
Training loss: 2.298490524291992
Validation loss: 2.112913126586586

Epoch: 6| Step: 13
Training loss: 2.1550827026367188
Validation loss: 2.133117047689294

Epoch: 238| Step: 0
Training loss: 2.2103636264801025
Validation loss: 2.1471524392404864

Epoch: 6| Step: 1
Training loss: 2.392454147338867
Validation loss: 2.157452732004145

Epoch: 6| Step: 2
Training loss: 2.0822629928588867
Validation loss: 2.1578942037397817

Epoch: 6| Step: 3
Training loss: 2.4114208221435547
Validation loss: 2.1562036878319195

Epoch: 6| Step: 4
Training loss: 1.8798165321350098
Validation loss: 2.1555686432828187

Epoch: 6| Step: 5
Training loss: 1.4033650159835815
Validation loss: 2.1431522882112892

Epoch: 6| Step: 6
Training loss: 1.9341936111450195
Validation loss: 2.144235751962149

Epoch: 6| Step: 7
Training loss: 1.9437501430511475
Validation loss: 2.1408255664251183

Epoch: 6| Step: 8
Training loss: 1.8344154357910156
Validation loss: 2.1424429647384153

Epoch: 6| Step: 9
Training loss: 2.055745840072632
Validation loss: 2.145064652607005

Epoch: 6| Step: 10
Training loss: 2.242892265319824
Validation loss: 2.1216987794445408

Epoch: 6| Step: 11
Training loss: 2.3540539741516113
Validation loss: 2.1207266648610434

Epoch: 6| Step: 12
Training loss: 2.6636669635772705
Validation loss: 2.120319063945483

Epoch: 6| Step: 13
Training loss: 2.803006172180176
Validation loss: 2.1029028354152555

Epoch: 239| Step: 0
Training loss: 2.2404725551605225
Validation loss: 2.1187156810555408

Epoch: 6| Step: 1
Training loss: 1.3024249076843262
Validation loss: 2.1338758096900037

Epoch: 6| Step: 2
Training loss: 2.0033371448516846
Validation loss: 2.134405025871851

Epoch: 6| Step: 3
Training loss: 1.8617655038833618
Validation loss: 2.15676143733404

Epoch: 6| Step: 4
Training loss: 2.578390121459961
Validation loss: 2.1566020160593014

Epoch: 6| Step: 5
Training loss: 1.802534818649292
Validation loss: 2.136934459850352

Epoch: 6| Step: 6
Training loss: 1.3792591094970703
Validation loss: 2.120082927006547

Epoch: 6| Step: 7
Training loss: 2.5276882648468018
Validation loss: 2.126915611246581

Epoch: 6| Step: 8
Training loss: 2.307438850402832
Validation loss: 2.1155684250657276

Epoch: 6| Step: 9
Training loss: 3.029757261276245
Validation loss: 2.1313938761270173

Epoch: 6| Step: 10
Training loss: 2.5372133255004883
Validation loss: 2.1248362961635796

Epoch: 6| Step: 11
Training loss: 2.015334129333496
Validation loss: 2.103675521830077

Epoch: 6| Step: 12
Training loss: 2.085564613342285
Validation loss: 2.10701157200721

Epoch: 6| Step: 13
Training loss: 2.029029369354248
Validation loss: 2.103406165235786

Epoch: 240| Step: 0
Training loss: 2.1805484294891357
Validation loss: 2.1169488840205695

Epoch: 6| Step: 1
Training loss: 2.5146169662475586
Validation loss: 2.134473713495398

Epoch: 6| Step: 2
Training loss: 2.1377203464508057
Validation loss: 2.130551153613675

Epoch: 6| Step: 3
Training loss: 1.2728991508483887
Validation loss: 2.1309578021367392

Epoch: 6| Step: 4
Training loss: 3.1124887466430664
Validation loss: 2.1146761166152133

Epoch: 6| Step: 5
Training loss: 1.8641453981399536
Validation loss: 2.102042075126402

Epoch: 6| Step: 6
Training loss: 2.063371181488037
Validation loss: 2.095179088654057

Epoch: 6| Step: 7
Training loss: 2.132089853286743
Validation loss: 2.0874383590554677

Epoch: 6| Step: 8
Training loss: 1.451157569885254
Validation loss: 2.0972323417663574

Epoch: 6| Step: 9
Training loss: 2.0937345027923584
Validation loss: 2.1144289983216153

Epoch: 6| Step: 10
Training loss: 2.2737538814544678
Validation loss: 2.1210635169859855

Epoch: 6| Step: 11
Training loss: 2.5139241218566895
Validation loss: 2.1363238301328433

Epoch: 6| Step: 12
Training loss: 2.1683993339538574
Validation loss: 2.1457733595243065

Epoch: 6| Step: 13
Training loss: 1.5386786460876465
Validation loss: 2.1653729510563675

Epoch: 241| Step: 0
Training loss: 1.7074713706970215
Validation loss: 2.179232105132072

Epoch: 6| Step: 1
Training loss: 2.208624839782715
Validation loss: 2.1959058341159614

Epoch: 6| Step: 2
Training loss: 1.8251820802688599
Validation loss: 2.1664596834490375

Epoch: 6| Step: 3
Training loss: 2.335127353668213
Validation loss: 2.139292986162247

Epoch: 6| Step: 4
Training loss: 2.2590436935424805
Validation loss: 2.111753220199257

Epoch: 6| Step: 5
Training loss: 2.4691977500915527
Validation loss: 2.105549504680018

Epoch: 6| Step: 6
Training loss: 2.643772602081299
Validation loss: 2.106298697892056

Epoch: 6| Step: 7
Training loss: 1.9390970468521118
Validation loss: 2.10143861462993

Epoch: 6| Step: 8
Training loss: 2.4710745811462402
Validation loss: 2.0978326207848004

Epoch: 6| Step: 9
Training loss: 1.644148826599121
Validation loss: 2.091432617556664

Epoch: 6| Step: 10
Training loss: 1.3817877769470215
Validation loss: 2.0954552760688205

Epoch: 6| Step: 11
Training loss: 1.8860794305801392
Validation loss: 2.100831799609687

Epoch: 6| Step: 12
Training loss: 1.8820955753326416
Validation loss: 2.099970498392659

Epoch: 6| Step: 13
Training loss: 3.3753602504730225
Validation loss: 2.1141483194084576

Epoch: 242| Step: 0
Training loss: 2.4314663410186768
Validation loss: 2.1043866783060055

Epoch: 6| Step: 1
Training loss: 1.7990562915802002
Validation loss: 2.0999300120979227

Epoch: 6| Step: 2
Training loss: 1.6390507221221924
Validation loss: 2.1030138795093825

Epoch: 6| Step: 3
Training loss: 2.202643394470215
Validation loss: 2.1094489943596626

Epoch: 6| Step: 4
Training loss: 1.7401129007339478
Validation loss: 2.1149642339316745

Epoch: 6| Step: 5
Training loss: 1.7852458953857422
Validation loss: 2.098405930303758

Epoch: 6| Step: 6
Training loss: 1.5561943054199219
Validation loss: 2.087510516566615

Epoch: 6| Step: 7
Training loss: 2.115919589996338
Validation loss: 2.0624715987072197

Epoch: 6| Step: 8
Training loss: 3.065427303314209
Validation loss: 2.0572882442064184

Epoch: 6| Step: 9
Training loss: 1.9678820371627808
Validation loss: 2.0665452364952333

Epoch: 6| Step: 10
Training loss: 2.0744760036468506
Validation loss: 2.058503927723054

Epoch: 6| Step: 11
Training loss: 2.4649062156677246
Validation loss: 2.064242239921324

Epoch: 6| Step: 12
Training loss: 2.2211754322052
Validation loss: 2.0632388784039404

Epoch: 6| Step: 13
Training loss: 2.5801947116851807
Validation loss: 2.0839207838940363

Epoch: 243| Step: 0
Training loss: 2.539191246032715
Validation loss: 2.0821136300281813

Epoch: 6| Step: 1
Training loss: 1.2994714975357056
Validation loss: 2.0949902444757442

Epoch: 6| Step: 2
Training loss: 2.47845458984375
Validation loss: 2.1013548322903213

Epoch: 6| Step: 3
Training loss: 2.3409626483917236
Validation loss: 2.090930741320374

Epoch: 6| Step: 4
Training loss: 1.8913683891296387
Validation loss: 2.089096923028269

Epoch: 6| Step: 5
Training loss: 2.4041635990142822
Validation loss: 2.088707003542172

Epoch: 6| Step: 6
Training loss: 1.6252119541168213
Validation loss: 2.0809755094589724

Epoch: 6| Step: 7
Training loss: 2.4616541862487793
Validation loss: 2.092364690637076

Epoch: 6| Step: 8
Training loss: 1.714084267616272
Validation loss: 2.093717400745679

Epoch: 6| Step: 9
Training loss: 2.347548007965088
Validation loss: 2.113319307245234

Epoch: 6| Step: 10
Training loss: 2.230295181274414
Validation loss: 2.108012128901738

Epoch: 6| Step: 11
Training loss: 1.5994895696640015
Validation loss: 2.101483582168497

Epoch: 6| Step: 12
Training loss: 1.9726970195770264
Validation loss: 2.103193977827667

Epoch: 6| Step: 13
Training loss: 2.31404185295105
Validation loss: 2.092696214234957

Epoch: 244| Step: 0
Training loss: 2.317875385284424
Validation loss: 2.101800080268614

Epoch: 6| Step: 1
Training loss: 2.4265036582946777
Validation loss: 2.086168817294541

Epoch: 6| Step: 2
Training loss: 2.0093834400177
Validation loss: 2.0846534800785843

Epoch: 6| Step: 3
Training loss: 2.230868339538574
Validation loss: 2.0860018525072324

Epoch: 6| Step: 4
Training loss: 2.8701298236846924
Validation loss: 2.0648270268594064

Epoch: 6| Step: 5
Training loss: 1.333128571510315
Validation loss: 2.089149921171127

Epoch: 6| Step: 6
Training loss: 1.6688929796218872
Validation loss: 2.0877885972299883

Epoch: 6| Step: 7
Training loss: 1.96231210231781
Validation loss: 2.101983230601075

Epoch: 6| Step: 8
Training loss: 2.066774845123291
Validation loss: 2.1147818847369124

Epoch: 6| Step: 9
Training loss: 1.8415818214416504
Validation loss: 2.1243420980309926

Epoch: 6| Step: 10
Training loss: 1.9835805892944336
Validation loss: 2.1162025851588093

Epoch: 6| Step: 11
Training loss: 2.4980595111846924
Validation loss: 2.1155796832935785

Epoch: 6| Step: 12
Training loss: 1.5070273876190186
Validation loss: 2.1147945529671124

Epoch: 6| Step: 13
Training loss: 2.456230640411377
Validation loss: 2.107922210488268

Epoch: 245| Step: 0
Training loss: 1.880178689956665
Validation loss: 2.1148047985569125

Epoch: 6| Step: 1
Training loss: 2.284527540206909
Validation loss: 2.109079644244204

Epoch: 6| Step: 2
Training loss: 2.3327865600585938
Validation loss: 2.087273424671542

Epoch: 6| Step: 3
Training loss: 1.8505613803863525
Validation loss: 2.094258867284303

Epoch: 6| Step: 4
Training loss: 1.8531343936920166
Validation loss: 2.08155539343434

Epoch: 6| Step: 5
Training loss: 2.5343852043151855
Validation loss: 2.089158811876851

Epoch: 6| Step: 6
Training loss: 1.4200770854949951
Validation loss: 2.0863250942640406

Epoch: 6| Step: 7
Training loss: 1.982980728149414
Validation loss: 2.079402913329422

Epoch: 6| Step: 8
Training loss: 2.5547564029693604
Validation loss: 2.0843915003602222

Epoch: 6| Step: 9
Training loss: 1.366716742515564
Validation loss: 2.0927981445866246

Epoch: 6| Step: 10
Training loss: 1.76485013961792
Validation loss: 2.10734071013748

Epoch: 6| Step: 11
Training loss: 2.2325260639190674
Validation loss: 2.1199762962197743

Epoch: 6| Step: 12
Training loss: 2.270298480987549
Validation loss: 2.13024728785279

Epoch: 6| Step: 13
Training loss: 3.432441234588623
Validation loss: 2.1483325804433515

Epoch: 246| Step: 0
Training loss: 1.9302771091461182
Validation loss: 2.114862765035322

Epoch: 6| Step: 1
Training loss: 1.885545015335083
Validation loss: 2.1138097163169616

Epoch: 6| Step: 2
Training loss: 2.3254523277282715
Validation loss: 2.1149750345496723

Epoch: 6| Step: 3
Training loss: 1.8202153444290161
Validation loss: 2.0946735617935017

Epoch: 6| Step: 4
Training loss: 1.9728448390960693
Validation loss: 2.0776077855017876

Epoch: 6| Step: 5
Training loss: 2.7536702156066895
Validation loss: 2.0823455395237094

Epoch: 6| Step: 6
Training loss: 2.1686344146728516
Validation loss: 2.0863953021264847

Epoch: 6| Step: 7
Training loss: 2.4492099285125732
Validation loss: 2.101873610609321

Epoch: 6| Step: 8
Training loss: 1.8519036769866943
Validation loss: 2.126114676075597

Epoch: 6| Step: 9
Training loss: 1.8658932447433472
Validation loss: 2.129519044712026

Epoch: 6| Step: 10
Training loss: 1.6734806299209595
Validation loss: 2.122792415721442

Epoch: 6| Step: 11
Training loss: 2.345977306365967
Validation loss: 2.0930426748850013

Epoch: 6| Step: 12
Training loss: 1.990653157234192
Validation loss: 2.0773193144029185

Epoch: 6| Step: 13
Training loss: 2.190673351287842
Validation loss: 2.0751260326754664

Epoch: 247| Step: 0
Training loss: 1.927809238433838
Validation loss: 2.090966340034239

Epoch: 6| Step: 1
Training loss: 1.640633225440979
Validation loss: 2.095041767243416

Epoch: 6| Step: 2
Training loss: 1.9012236595153809
Validation loss: 2.106081703657745

Epoch: 6| Step: 3
Training loss: 2.0889198780059814
Validation loss: 2.1121676442443684

Epoch: 6| Step: 4
Training loss: 2.301851749420166
Validation loss: 2.1112758062219106

Epoch: 6| Step: 5
Training loss: 2.3468170166015625
Validation loss: 2.1270029955012824

Epoch: 6| Step: 6
Training loss: 2.072772979736328
Validation loss: 2.122362721350885

Epoch: 6| Step: 7
Training loss: 1.0729525089263916
Validation loss: 2.1158186107553463

Epoch: 6| Step: 8
Training loss: 1.90980863571167
Validation loss: 2.114013341165358

Epoch: 6| Step: 9
Training loss: 2.263812303543091
Validation loss: 2.127638893742715

Epoch: 6| Step: 10
Training loss: 3.0224242210388184
Validation loss: 2.0995392619922595

Epoch: 6| Step: 11
Training loss: 2.047286033630371
Validation loss: 2.102390666161814

Epoch: 6| Step: 12
Training loss: 1.96993887424469
Validation loss: 2.097220754110685

Epoch: 6| Step: 13
Training loss: 2.6019303798675537
Validation loss: 2.081635275194722

Epoch: 248| Step: 0
Training loss: 1.947961449623108
Validation loss: 2.0839407444000244

Epoch: 6| Step: 1
Training loss: 1.8994146585464478
Validation loss: 2.0708080491712018

Epoch: 6| Step: 2
Training loss: 1.938709020614624
Validation loss: 2.0749244202849684

Epoch: 6| Step: 3
Training loss: 1.9158467054367065
Validation loss: 2.081040201648589

Epoch: 6| Step: 4
Training loss: 1.7649333477020264
Validation loss: 2.06593858298435

Epoch: 6| Step: 5
Training loss: 2.19111704826355
Validation loss: 2.065739800853114

Epoch: 6| Step: 6
Training loss: 2.2908411026000977
Validation loss: 2.0733765632875505

Epoch: 6| Step: 7
Training loss: 2.3699564933776855
Validation loss: 2.055871953246414

Epoch: 6| Step: 8
Training loss: 2.1810851097106934
Validation loss: 2.0467350329122236

Epoch: 6| Step: 9
Training loss: 2.072995662689209
Validation loss: 2.0392439852478685

Epoch: 6| Step: 10
Training loss: 1.7555642127990723
Validation loss: 2.049451835693852

Epoch: 6| Step: 11
Training loss: 2.577969551086426
Validation loss: 2.0569569423634517

Epoch: 6| Step: 12
Training loss: 1.7386448383331299
Validation loss: 2.072368812817399

Epoch: 6| Step: 13
Training loss: 2.410034418106079
Validation loss: 2.0766929349591656

Epoch: 249| Step: 0
Training loss: 2.1944637298583984
Validation loss: 2.0878912120737056

Epoch: 6| Step: 1
Training loss: 2.8318302631378174
Validation loss: 2.1036461783993627

Epoch: 6| Step: 2
Training loss: 2.4242773056030273
Validation loss: 2.0925468872952204

Epoch: 6| Step: 3
Training loss: 2.9875245094299316
Validation loss: 2.099656243478098

Epoch: 6| Step: 4
Training loss: 2.7166905403137207
Validation loss: 2.11092544627446

Epoch: 6| Step: 5
Training loss: 1.1206612586975098
Validation loss: 2.1135905269653565

Epoch: 6| Step: 6
Training loss: 1.6503710746765137
Validation loss: 2.10004250977629

Epoch: 6| Step: 7
Training loss: 2.2377493381500244
Validation loss: 2.0901863036617154

Epoch: 6| Step: 8
Training loss: 1.8118152618408203
Validation loss: 2.0859916504993232

Epoch: 6| Step: 9
Training loss: 1.6939538717269897
Validation loss: 2.0862020677135837

Epoch: 6| Step: 10
Training loss: 1.7635154724121094
Validation loss: 2.0648173696251324

Epoch: 6| Step: 11
Training loss: 1.8296070098876953
Validation loss: 2.070916586024787

Epoch: 6| Step: 12
Training loss: 2.001584768295288
Validation loss: 2.0698467300784205

Epoch: 6| Step: 13
Training loss: 0.7387363314628601
Validation loss: 2.062459591896303

Epoch: 250| Step: 0
Training loss: 1.651228904724121
Validation loss: 2.066793261035796

Epoch: 6| Step: 1
Training loss: 2.415102005004883
Validation loss: 2.0774501498027513

Epoch: 6| Step: 2
Training loss: 2.0563864707946777
Validation loss: 2.0675278543144144

Epoch: 6| Step: 3
Training loss: 2.295956611633301
Validation loss: 2.0771018330768873

Epoch: 6| Step: 4
Training loss: 1.8803620338439941
Validation loss: 2.0821177626168854

Epoch: 6| Step: 5
Training loss: 2.0967626571655273
Validation loss: 2.061125025954298

Epoch: 6| Step: 6
Training loss: 1.8046250343322754
Validation loss: 2.0491582885865243

Epoch: 6| Step: 7
Training loss: 2.078329086303711
Validation loss: 2.035633551177158

Epoch: 6| Step: 8
Training loss: 1.8292384147644043
Validation loss: 2.0533996474358345

Epoch: 6| Step: 9
Training loss: 2.3240771293640137
Validation loss: 2.0602563183794738

Epoch: 6| Step: 10
Training loss: 1.4560647010803223
Validation loss: 2.073384392646051

Epoch: 6| Step: 11
Training loss: 2.2470834255218506
Validation loss: 2.079775297513572

Epoch: 6| Step: 12
Training loss: 2.394747495651245
Validation loss: 2.0774821286560385

Epoch: 6| Step: 13
Training loss: 2.3097684383392334
Validation loss: 2.0846842835026402

Testing loss: 2.2516566276550294

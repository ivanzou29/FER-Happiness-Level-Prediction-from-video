Epoch: 1| Step: 0
Training loss: 5.458918164010366
Validation loss: 5.844530924775327

Epoch: 6| Step: 1
Training loss: 4.7090838123962975
Validation loss: 5.8251863812006786

Epoch: 6| Step: 2
Training loss: 5.699147799849663
Validation loss: 5.807125696582246

Epoch: 6| Step: 3
Training loss: 6.021982613489346
Validation loss: 5.788923243967473

Epoch: 6| Step: 4
Training loss: 6.216272521968676
Validation loss: 5.767816468973324

Epoch: 6| Step: 5
Training loss: 4.871467679989033
Validation loss: 5.744275502245942

Epoch: 6| Step: 6
Training loss: 5.12982880661773
Validation loss: 5.718522556228366

Epoch: 6| Step: 7
Training loss: 6.776570898837601
Validation loss: 5.688720480220618

Epoch: 6| Step: 8
Training loss: 5.981554287910354
Validation loss: 5.656199682847346

Epoch: 6| Step: 9
Training loss: 5.424236388015037
Validation loss: 5.619990947203588

Epoch: 6| Step: 10
Training loss: 5.215079107224021
Validation loss: 5.580249883874331

Epoch: 6| Step: 11
Training loss: 5.949371035374473
Validation loss: 5.536794664434073

Epoch: 6| Step: 12
Training loss: 5.443698550834122
Validation loss: 5.491648331988629

Epoch: 6| Step: 13
Training loss: 7.285580636516326
Validation loss: 5.443538913938847

Epoch: 2| Step: 0
Training loss: 5.727909278449549
Validation loss: 5.395358625821793

Epoch: 6| Step: 1
Training loss: 4.573705212753544
Validation loss: 5.34305627159391

Epoch: 6| Step: 2
Training loss: 5.415293455898857
Validation loss: 5.289296096017988

Epoch: 6| Step: 3
Training loss: 5.86301612127496
Validation loss: 5.234784602260292

Epoch: 6| Step: 4
Training loss: 5.24442813021247
Validation loss: 5.178711877075417

Epoch: 6| Step: 5
Training loss: 4.640484021033031
Validation loss: 5.123707139770881

Epoch: 6| Step: 6
Training loss: 5.027389846270313
Validation loss: 5.071132570145186

Epoch: 6| Step: 7
Training loss: 4.998105643949132
Validation loss: 5.019372286920325

Epoch: 6| Step: 8
Training loss: 5.3324377778290994
Validation loss: 4.967973772948274

Epoch: 6| Step: 9
Training loss: 5.188245604831626
Validation loss: 4.913758793316607

Epoch: 6| Step: 10
Training loss: 4.66056163268502
Validation loss: 4.859358919764197

Epoch: 6| Step: 11
Training loss: 4.438264378007118
Validation loss: 4.81067032346056

Epoch: 6| Step: 12
Training loss: 5.111265060391422
Validation loss: 4.767864443617257

Epoch: 6| Step: 13
Training loss: 5.395283065879112
Validation loss: 4.7260647701427665

Epoch: 3| Step: 0
Training loss: 5.516168065122804
Validation loss: 4.685668440915694

Epoch: 6| Step: 1
Training loss: 4.360647583211776
Validation loss: 4.640770239345891

Epoch: 6| Step: 2
Training loss: 4.3027955482487545
Validation loss: 4.589002564618529

Epoch: 6| Step: 3
Training loss: 3.9843788296550224
Validation loss: 4.557718079009261

Epoch: 6| Step: 4
Training loss: 4.350837030667965
Validation loss: 4.533760115013663

Epoch: 6| Step: 5
Training loss: 4.476575708286579
Validation loss: 4.513537539598345

Epoch: 6| Step: 6
Training loss: 4.72330684298183
Validation loss: 4.4898591212985

Epoch: 6| Step: 7
Training loss: 4.169627066203382
Validation loss: 4.46570088498956

Epoch: 6| Step: 8
Training loss: 4.629482468420745
Validation loss: 4.441005262305171

Epoch: 6| Step: 9
Training loss: 4.737183999667324
Validation loss: 4.418031348556222

Epoch: 6| Step: 10
Training loss: 3.7395104247966855
Validation loss: 4.390563263742097

Epoch: 6| Step: 11
Training loss: 4.524398359061082
Validation loss: 4.372121320325136

Epoch: 6| Step: 12
Training loss: 5.79876595884916
Validation loss: 4.357588195222654

Epoch: 6| Step: 13
Training loss: 4.6423994069250405
Validation loss: 4.328325815955532

Epoch: 4| Step: 0
Training loss: 3.1765976498109074
Validation loss: 4.307543311984365

Epoch: 6| Step: 1
Training loss: 4.035210135470313
Validation loss: 4.285587451037549

Epoch: 6| Step: 2
Training loss: 3.856803500672267
Validation loss: 4.286412482758489

Epoch: 6| Step: 3
Training loss: 5.072138711651081
Validation loss: 4.27897853607711

Epoch: 6| Step: 4
Training loss: 4.822263647351761
Validation loss: 4.244444548894232

Epoch: 6| Step: 5
Training loss: 4.361544817654765
Validation loss: 4.219055441825168

Epoch: 6| Step: 6
Training loss: 3.5323554815169347
Validation loss: 4.209038423877782

Epoch: 6| Step: 7
Training loss: 4.611188111690045
Validation loss: 4.200306246814285

Epoch: 6| Step: 8
Training loss: 4.232272255950263
Validation loss: 4.172857063492014

Epoch: 6| Step: 9
Training loss: 4.1470616244743095
Validation loss: 4.159886418346764

Epoch: 6| Step: 10
Training loss: 4.402527663985011
Validation loss: 4.144168161730724

Epoch: 6| Step: 11
Training loss: 4.752410678495712
Validation loss: 4.127689318951059

Epoch: 6| Step: 12
Training loss: 5.271601061891663
Validation loss: 4.111757230902329

Epoch: 6| Step: 13
Training loss: 3.48194656936695
Validation loss: 4.089504795923976

Epoch: 5| Step: 0
Training loss: 4.553360236904333
Validation loss: 4.068106403273759

Epoch: 6| Step: 1
Training loss: 4.280258398010026
Validation loss: 4.047563949134802

Epoch: 6| Step: 2
Training loss: 4.125595049854125
Validation loss: 4.040544067066653

Epoch: 6| Step: 3
Training loss: 4.330610202142464
Validation loss: 4.034173880479291

Epoch: 6| Step: 4
Training loss: 4.2930820439725235
Validation loss: 4.013701009788088

Epoch: 6| Step: 5
Training loss: 2.9594078440792266
Validation loss: 4.000823366885057

Epoch: 6| Step: 6
Training loss: 4.706074243027174
Validation loss: 3.9973519491763865

Epoch: 6| Step: 7
Training loss: 4.550086286795576
Validation loss: 3.98471286667419

Epoch: 6| Step: 8
Training loss: 3.8346417446980188
Validation loss: 3.966744373877454

Epoch: 6| Step: 9
Training loss: 4.467065880618631
Validation loss: 3.9482400149017858

Epoch: 6| Step: 10
Training loss: 4.595898184343253
Validation loss: 3.9326332116180542

Epoch: 6| Step: 11
Training loss: 3.417079683477168
Validation loss: 3.9168470171421763

Epoch: 6| Step: 12
Training loss: 4.308188453073099
Validation loss: 3.9017141791962016

Epoch: 6| Step: 13
Training loss: 1.7694652284006722
Validation loss: 3.891098491139299

Epoch: 6| Step: 0
Training loss: 3.1501720563129565
Validation loss: 3.8824114531825846

Epoch: 6| Step: 1
Training loss: 4.550762389311775
Validation loss: 3.8724409605382353

Epoch: 6| Step: 2
Training loss: 4.015019590109114
Validation loss: 3.8654744848550764

Epoch: 6| Step: 3
Training loss: 4.9286152539565515
Validation loss: 3.854340455522416

Epoch: 6| Step: 4
Training loss: 3.9913556152285903
Validation loss: 3.8476096309979386

Epoch: 6| Step: 5
Training loss: 3.3023350055146525
Validation loss: 3.8324220429294065

Epoch: 6| Step: 6
Training loss: 4.244203990984628
Validation loss: 3.820031451737008

Epoch: 6| Step: 7
Training loss: 3.4101941433229
Validation loss: 3.8077255241311927

Epoch: 6| Step: 8
Training loss: 3.634282427959379
Validation loss: 3.8026897091286993

Epoch: 6| Step: 9
Training loss: 4.438853527549438
Validation loss: 3.7960544707136687

Epoch: 6| Step: 10
Training loss: 5.163992322944117
Validation loss: 3.7835111908989627

Epoch: 6| Step: 11
Training loss: 3.4772218914701907
Validation loss: 3.7713066818784724

Epoch: 6| Step: 12
Training loss: 2.8915291686627578
Validation loss: 3.756320023465403

Epoch: 6| Step: 13
Training loss: 3.9102136872576527
Validation loss: 3.759583656223457

Epoch: 7| Step: 0
Training loss: 3.988462856839485
Validation loss: 3.7601785828428316

Epoch: 6| Step: 1
Training loss: 3.8352306204116546
Validation loss: 3.719450320334263

Epoch: 6| Step: 2
Training loss: 3.9337198186103857
Validation loss: 3.721126098895925

Epoch: 6| Step: 3
Training loss: 3.1770278342147082
Validation loss: 3.716344758531476

Epoch: 6| Step: 4
Training loss: 4.369293414851921
Validation loss: 3.7090948184178134

Epoch: 6| Step: 5
Training loss: 3.856783718969373
Validation loss: 3.6901704890785636

Epoch: 6| Step: 6
Training loss: 3.1370481553277463
Validation loss: 3.6667002557985398

Epoch: 6| Step: 7
Training loss: 3.3797449735780036
Validation loss: 3.6562839559417966

Epoch: 6| Step: 8
Training loss: 3.5155813256835797
Validation loss: 3.6449739254839884

Epoch: 6| Step: 9
Training loss: 3.9411389928514473
Validation loss: 3.631199155256712

Epoch: 6| Step: 10
Training loss: 4.484212520194547
Validation loss: 3.6208270973078136

Epoch: 6| Step: 11
Training loss: 4.235581342301312
Validation loss: 3.6070547023343944

Epoch: 6| Step: 12
Training loss: 3.773448114794853
Validation loss: 3.6013418239510986

Epoch: 6| Step: 13
Training loss: 4.273443748567186
Validation loss: 3.5842294757665263

Epoch: 8| Step: 0
Training loss: 4.231044233494391
Validation loss: 3.587423118855715

Epoch: 6| Step: 1
Training loss: 3.3884078832688425
Validation loss: 3.5626169806896133

Epoch: 6| Step: 2
Training loss: 3.7183812062732375
Validation loss: 3.5604836803234576

Epoch: 6| Step: 3
Training loss: 3.722260685306041
Validation loss: 3.5518192136168683

Epoch: 6| Step: 4
Training loss: 3.4127661297325957
Validation loss: 3.5352354102573713

Epoch: 6| Step: 5
Training loss: 3.5141093738189353
Validation loss: 3.5221470197931133

Epoch: 6| Step: 6
Training loss: 4.112340748958782
Validation loss: 3.505504308530667

Epoch: 6| Step: 7
Training loss: 3.597894815990817
Validation loss: 3.5074765705732407

Epoch: 6| Step: 8
Training loss: 3.679806587900107
Validation loss: 3.4886465363783543

Epoch: 6| Step: 9
Training loss: 3.7073730053954987
Validation loss: 3.488648912890536

Epoch: 6| Step: 10
Training loss: 3.127241780136007
Validation loss: 3.4854013086742412

Epoch: 6| Step: 11
Training loss: 3.9340104883361544
Validation loss: 3.477334663462421

Epoch: 6| Step: 12
Training loss: 4.570745672581003
Validation loss: 3.467033710307675

Epoch: 6| Step: 13
Training loss: 2.5750526385806527
Validation loss: 3.450443394961207

Epoch: 9| Step: 0
Training loss: 4.51144860189594
Validation loss: 3.434433462161217

Epoch: 6| Step: 1
Training loss: 3.488771137213154
Validation loss: 3.4194985850649537

Epoch: 6| Step: 2
Training loss: 2.8838259355181592
Validation loss: 3.408312076005181

Epoch: 6| Step: 3
Training loss: 2.8163574045207365
Validation loss: 3.405350094253336

Epoch: 6| Step: 4
Training loss: 3.035950938385
Validation loss: 3.391580901994446

Epoch: 6| Step: 5
Training loss: 3.2881600417195234
Validation loss: 3.3893786281637155

Epoch: 6| Step: 6
Training loss: 4.066485518709855
Validation loss: 3.3809074481422012

Epoch: 6| Step: 7
Training loss: 3.677343443981358
Validation loss: 3.3691419139952927

Epoch: 6| Step: 8
Training loss: 3.023296500769877
Validation loss: 3.371601683265087

Epoch: 6| Step: 9
Training loss: 4.19057385941833
Validation loss: 3.3745824266113957

Epoch: 6| Step: 10
Training loss: 3.300674490923293
Validation loss: 3.3603643915441737

Epoch: 6| Step: 11
Training loss: 4.041980508371324
Validation loss: 3.3484366548746434

Epoch: 6| Step: 12
Training loss: 4.057709671967971
Validation loss: 3.3342021676339644

Epoch: 6| Step: 13
Training loss: 3.564433459920069
Validation loss: 3.3265847144699157

Epoch: 10| Step: 0
Training loss: 4.207623311693369
Validation loss: 3.3188517529308745

Epoch: 6| Step: 1
Training loss: 3.470839961955017
Validation loss: 3.3104928317196105

Epoch: 6| Step: 2
Training loss: 3.8449352886970125
Validation loss: 3.303470358437026

Epoch: 6| Step: 3
Training loss: 3.9848147878838476
Validation loss: 3.317417140431958

Epoch: 6| Step: 4
Training loss: 4.240833494926375
Validation loss: 3.333178308686485

Epoch: 6| Step: 5
Training loss: 2.6629738471093147
Validation loss: 3.3077685446983285

Epoch: 6| Step: 6
Training loss: 3.7535512004067173
Validation loss: 3.2899810223207075

Epoch: 6| Step: 7
Training loss: 3.273233557564031
Validation loss: 3.2736495114767457

Epoch: 6| Step: 8
Training loss: 3.408287069166799
Validation loss: 3.2667467550007774

Epoch: 6| Step: 9
Training loss: 2.779683218272419
Validation loss: 3.261721063919725

Epoch: 6| Step: 10
Training loss: 3.443523608622373
Validation loss: 3.2695438128318584

Epoch: 6| Step: 11
Training loss: 3.122551836449393
Validation loss: 3.251691896888996

Epoch: 6| Step: 12
Training loss: 3.724006079876748
Validation loss: 3.2444563489138236

Epoch: 6| Step: 13
Training loss: 2.7211968822140085
Validation loss: 3.2540337596015148

Epoch: 11| Step: 0
Training loss: 3.535197524361953
Validation loss: 3.272019812170247

Epoch: 6| Step: 1
Training loss: 4.368830881748049
Validation loss: 3.2783566939831226

Epoch: 6| Step: 2
Training loss: 3.242737358070384
Validation loss: 3.2517416888291586

Epoch: 6| Step: 3
Training loss: 3.1600110901263747
Validation loss: 3.234148297066448

Epoch: 6| Step: 4
Training loss: 3.506283705170273
Validation loss: 3.2274177290048263

Epoch: 6| Step: 5
Training loss: 3.627133103580075
Validation loss: 3.2265347433372433

Epoch: 6| Step: 6
Training loss: 3.6101068192420414
Validation loss: 3.2295358378668078

Epoch: 6| Step: 7
Training loss: 3.5371368761009854
Validation loss: 3.21833565953545

Epoch: 6| Step: 8
Training loss: 3.6334918299643895
Validation loss: 3.2079590007855465

Epoch: 6| Step: 9
Training loss: 3.193557219836468
Validation loss: 3.200263229690284

Epoch: 6| Step: 10
Training loss: 3.2759005582386442
Validation loss: 3.1960528800886876

Epoch: 6| Step: 11
Training loss: 3.1604263322280475
Validation loss: 3.192418988860859

Epoch: 6| Step: 12
Training loss: 3.571873430066502
Validation loss: 3.191099501887934

Epoch: 6| Step: 13
Training loss: 2.8287797875886205
Validation loss: 3.1892409358837894

Epoch: 12| Step: 0
Training loss: 3.6022849196240543
Validation loss: 3.187721080701476

Epoch: 6| Step: 1
Training loss: 3.6801319542322832
Validation loss: 3.188162578554259

Epoch: 6| Step: 2
Training loss: 3.3940951963886397
Validation loss: 3.177625692476617

Epoch: 6| Step: 3
Training loss: 3.2824696045374
Validation loss: 3.182278028975559

Epoch: 6| Step: 4
Training loss: 3.540509397634579
Validation loss: 3.1892924375208906

Epoch: 6| Step: 5
Training loss: 3.189017888102418
Validation loss: 3.182366187697321

Epoch: 6| Step: 6
Training loss: 3.3526484368537206
Validation loss: 3.1734524355680733

Epoch: 6| Step: 7
Training loss: 3.1515389210353697
Validation loss: 3.1655503854577978

Epoch: 6| Step: 8
Training loss: 3.855283723080471
Validation loss: 3.160137107145497

Epoch: 6| Step: 9
Training loss: 3.89136936160304
Validation loss: 3.156442022112278

Epoch: 6| Step: 10
Training loss: 2.7402301986782147
Validation loss: 3.155177750259459

Epoch: 6| Step: 11
Training loss: 3.8080770132227175
Validation loss: 3.1479448125202927

Epoch: 6| Step: 12
Training loss: 3.1595342186597435
Validation loss: 3.1488669657836774

Epoch: 6| Step: 13
Training loss: 3.1270889929394694
Validation loss: 3.143050707812126

Epoch: 13| Step: 0
Training loss: 3.1757893579808862
Validation loss: 3.1437290039352983

Epoch: 6| Step: 1
Training loss: 3.314971289877178
Validation loss: 3.139895526717387

Epoch: 6| Step: 2
Training loss: 3.526543645846393
Validation loss: 3.1360644526219286

Epoch: 6| Step: 3
Training loss: 3.8077386610207804
Validation loss: 3.1348825499288875

Epoch: 6| Step: 4
Training loss: 3.2006711911587504
Validation loss: 3.1306593620880276

Epoch: 6| Step: 5
Training loss: 2.953692346398462
Validation loss: 3.1275055715836335

Epoch: 6| Step: 6
Training loss: 3.0868224601344707
Validation loss: 3.1215085564783585

Epoch: 6| Step: 7
Training loss: 3.2480979635564218
Validation loss: 3.121598947174165

Epoch: 6| Step: 8
Training loss: 4.019870280360157
Validation loss: 3.1164245337907603

Epoch: 6| Step: 9
Training loss: 3.905492724447881
Validation loss: 3.1103338349412937

Epoch: 6| Step: 10
Training loss: 2.8518029516558894
Validation loss: 3.1076878041956246

Epoch: 6| Step: 11
Training loss: 2.8906804259888905
Validation loss: 3.1090306469442637

Epoch: 6| Step: 12
Training loss: 3.8159396804904557
Validation loss: 3.1100818384884796

Epoch: 6| Step: 13
Training loss: 3.513758456322785
Validation loss: 3.1034484594570406

Epoch: 14| Step: 0
Training loss: 3.0080785063989235
Validation loss: 3.09548548678092

Epoch: 6| Step: 1
Training loss: 2.9001053692321124
Validation loss: 3.092631322045248

Epoch: 6| Step: 2
Training loss: 3.5741516440507124
Validation loss: 3.0909828148371092

Epoch: 6| Step: 3
Training loss: 3.9056625534842935
Validation loss: 3.0871057610819173

Epoch: 6| Step: 4
Training loss: 3.7092225940892845
Validation loss: 3.085221852709791

Epoch: 6| Step: 5
Training loss: 3.596339213537923
Validation loss: 3.0829618214086754

Epoch: 6| Step: 6
Training loss: 3.8647760444295076
Validation loss: 3.0801556904967846

Epoch: 6| Step: 7
Training loss: 3.4857407702884857
Validation loss: 3.0754822202259824

Epoch: 6| Step: 8
Training loss: 2.524586702268898
Validation loss: 3.0732453507132105

Epoch: 6| Step: 9
Training loss: 3.1163763458431437
Validation loss: 3.072961883184443

Epoch: 6| Step: 10
Training loss: 3.368385190443446
Validation loss: 3.064019186965198

Epoch: 6| Step: 11
Training loss: 3.2173857529163095
Validation loss: 3.060255499311142

Epoch: 6| Step: 12
Training loss: 3.406169645429068
Validation loss: 3.0603572773827223

Epoch: 6| Step: 13
Training loss: 2.820501685731868
Validation loss: 3.066771785366535

Epoch: 15| Step: 0
Training loss: 2.8483247953331223
Validation loss: 3.0557480281296376

Epoch: 6| Step: 1
Training loss: 3.431532239857815
Validation loss: 3.0614324931279113

Epoch: 6| Step: 2
Training loss: 3.168466926285308
Validation loss: 3.0733112836144705

Epoch: 6| Step: 3
Training loss: 2.8333326414518822
Validation loss: 3.0668725513972332

Epoch: 6| Step: 4
Training loss: 3.3195370475447286
Validation loss: 3.0608157544408106

Epoch: 6| Step: 5
Training loss: 3.3939001903705517
Validation loss: 3.054038180277416

Epoch: 6| Step: 6
Training loss: 3.4975157504378087
Validation loss: 3.0536754979459677

Epoch: 6| Step: 7
Training loss: 2.563661451665825
Validation loss: 3.052380321253814

Epoch: 6| Step: 8
Training loss: 3.2608759904881075
Validation loss: 3.052528914677745

Epoch: 6| Step: 9
Training loss: 3.786631891959539
Validation loss: 3.046457810365101

Epoch: 6| Step: 10
Training loss: 3.5668719051408484
Validation loss: 3.0421349578939263

Epoch: 6| Step: 11
Training loss: 2.796636645853491
Validation loss: 3.0435098302364745

Epoch: 6| Step: 12
Training loss: 3.9184910738883376
Validation loss: 3.044010372513357

Epoch: 6| Step: 13
Training loss: 4.458510826119086
Validation loss: 3.0394694109170612

Epoch: 16| Step: 0
Training loss: 3.4321174789631397
Validation loss: 3.033510069874648

Epoch: 6| Step: 1
Training loss: 4.020629614114745
Validation loss: 3.0300896684749494

Epoch: 6| Step: 2
Training loss: 3.621392263476073
Validation loss: 3.0267514634727557

Epoch: 6| Step: 3
Training loss: 3.49137332992074
Validation loss: 3.028864883041807

Epoch: 6| Step: 4
Training loss: 3.694194600413944
Validation loss: 3.031632356470556

Epoch: 6| Step: 5
Training loss: 2.6734234522456575
Validation loss: 3.037483170857413

Epoch: 6| Step: 6
Training loss: 2.8208717362359677
Validation loss: 3.0176141176228684

Epoch: 6| Step: 7
Training loss: 3.1461304951623243
Validation loss: 3.015789245352667

Epoch: 6| Step: 8
Training loss: 3.7455727191995636
Validation loss: 3.0167082627720934

Epoch: 6| Step: 9
Training loss: 2.687957369897762
Validation loss: 3.0177508561114017

Epoch: 6| Step: 10
Training loss: 3.0660930497892296
Validation loss: 3.0226032140165042

Epoch: 6| Step: 11
Training loss: 3.4943816185583056
Validation loss: 3.020007587030262

Epoch: 6| Step: 12
Training loss: 3.326269357790522
Validation loss: 3.0195194825680702

Epoch: 6| Step: 13
Training loss: 2.443959992501164
Validation loss: 3.0107370137943437

Epoch: 17| Step: 0
Training loss: 3.204357375350765
Validation loss: 3.0042942872406284

Epoch: 6| Step: 1
Training loss: 3.707340979223582
Validation loss: 3.0072831445426713

Epoch: 6| Step: 2
Training loss: 2.7859674799788157
Validation loss: 3.024661974325141

Epoch: 6| Step: 3
Training loss: 3.848033596066635
Validation loss: 3.019564370914491

Epoch: 6| Step: 4
Training loss: 3.1620208558197653
Validation loss: 3.0054965579546846

Epoch: 6| Step: 5
Training loss: 2.8329674166587404
Validation loss: 2.9982560038106887

Epoch: 6| Step: 6
Training loss: 3.2351857519105813
Validation loss: 2.9977690218672106

Epoch: 6| Step: 7
Training loss: 3.5910411410969356
Validation loss: 2.9970214578021186

Epoch: 6| Step: 8
Training loss: 3.3070639864830795
Validation loss: 2.99426101042164

Epoch: 6| Step: 9
Training loss: 3.250477682368273
Validation loss: 2.9965133573169855

Epoch: 6| Step: 10
Training loss: 2.2999188242975346
Validation loss: 2.9970977094116695

Epoch: 6| Step: 11
Training loss: 3.5811159385101274
Validation loss: 2.995628449900374

Epoch: 6| Step: 12
Training loss: 3.6149167540107663
Validation loss: 2.9902800090224213

Epoch: 6| Step: 13
Training loss: 3.4761471618123667
Validation loss: 2.9887686541924325

Epoch: 18| Step: 0
Training loss: 3.914049464525761
Validation loss: 2.99089603103525

Epoch: 6| Step: 1
Training loss: 2.4543737646866717
Validation loss: 2.986597849261627

Epoch: 6| Step: 2
Training loss: 3.8671342325876874
Validation loss: 2.9853212024536213

Epoch: 6| Step: 3
Training loss: 3.316202074287658
Validation loss: 2.9807033750477068

Epoch: 6| Step: 4
Training loss: 3.5113670503981878
Validation loss: 2.9825648976693193

Epoch: 6| Step: 5
Training loss: 2.696348660096049
Validation loss: 2.978073045598734

Epoch: 6| Step: 6
Training loss: 2.9362327601810536
Validation loss: 2.977202670707257

Epoch: 6| Step: 7
Training loss: 2.8389185850610406
Validation loss: 2.9787741170898188

Epoch: 6| Step: 8
Training loss: 2.483438087233861
Validation loss: 2.9768857941044238

Epoch: 6| Step: 9
Training loss: 3.377133295692517
Validation loss: 2.9815562914973293

Epoch: 6| Step: 10
Training loss: 3.7817509847369752
Validation loss: 3.0079383264439263

Epoch: 6| Step: 11
Training loss: 3.7595954202545547
Validation loss: 3.020369078563192

Epoch: 6| Step: 12
Training loss: 3.240458935388393
Validation loss: 2.9974027503351564

Epoch: 6| Step: 13
Training loss: 3.21406228636211
Validation loss: 2.98800409318444

Epoch: 19| Step: 0
Training loss: 3.5513477775757867
Validation loss: 2.981334896903529

Epoch: 6| Step: 1
Training loss: 3.7132389039703577
Validation loss: 2.9829560479935116

Epoch: 6| Step: 2
Training loss: 2.873138654476014
Validation loss: 2.9774336294432997

Epoch: 6| Step: 3
Training loss: 3.048027407515446
Validation loss: 2.975139837441827

Epoch: 6| Step: 4
Training loss: 3.414106495309705
Validation loss: 2.9726280508703176

Epoch: 6| Step: 5
Training loss: 3.153230941701559
Validation loss: 2.9714474420695955

Epoch: 6| Step: 6
Training loss: 3.5699765905045497
Validation loss: 2.9712922174626173

Epoch: 6| Step: 7
Training loss: 2.4022376556539826
Validation loss: 2.9740840502480284

Epoch: 6| Step: 8
Training loss: 3.261175309097547
Validation loss: 2.9767555831129964

Epoch: 6| Step: 9
Training loss: 3.3741691414360857
Validation loss: 2.9792310324979265

Epoch: 6| Step: 10
Training loss: 3.5599977506137557
Validation loss: 2.97719589219592

Epoch: 6| Step: 11
Training loss: 2.7205827170165944
Validation loss: 2.9716516335264545

Epoch: 6| Step: 12
Training loss: 3.494695458412063
Validation loss: 2.9816101423548504

Epoch: 6| Step: 13
Training loss: 3.5546045985353576
Validation loss: 2.9915867560153218

Epoch: 20| Step: 0
Training loss: 3.5541511550213025
Validation loss: 2.977810831184443

Epoch: 6| Step: 1
Training loss: 4.024952782774532
Validation loss: 2.961356135351336

Epoch: 6| Step: 2
Training loss: 3.3623351783488955
Validation loss: 2.9599148429670468

Epoch: 6| Step: 3
Training loss: 3.291952643868477
Validation loss: 2.962826204718681

Epoch: 6| Step: 4
Training loss: 3.123003359951297
Validation loss: 2.9667533079517487

Epoch: 6| Step: 5
Training loss: 3.268933240375107
Validation loss: 2.9632617662476513

Epoch: 6| Step: 6
Training loss: 3.6107518596130666
Validation loss: 2.962982296802253

Epoch: 6| Step: 7
Training loss: 2.9089562726292977
Validation loss: 2.95379145877469

Epoch: 6| Step: 8
Training loss: 2.467733439463645
Validation loss: 2.954152496395083

Epoch: 6| Step: 9
Training loss: 3.09711866295936
Validation loss: 2.9549248714213148

Epoch: 6| Step: 10
Training loss: 3.117394614927771
Validation loss: 2.9760383418118805

Epoch: 6| Step: 11
Training loss: 3.364440080331077
Validation loss: 2.9859464502846795

Epoch: 6| Step: 12
Training loss: 3.7493443233750363
Validation loss: 3.028070929048224

Epoch: 6| Step: 13
Training loss: 1.4665286533848325
Validation loss: 2.9472809245229605

Epoch: 21| Step: 0
Training loss: 3.8301386305617307
Validation loss: 2.9482641609178275

Epoch: 6| Step: 1
Training loss: 3.3498579194414106
Validation loss: 2.966268299848152

Epoch: 6| Step: 2
Training loss: 2.8494448807153407
Validation loss: 2.9780612064600684

Epoch: 6| Step: 3
Training loss: 4.017519018084289
Validation loss: 2.985724747554866

Epoch: 6| Step: 4
Training loss: 2.934453642647498
Validation loss: 2.971953354055751

Epoch: 6| Step: 5
Training loss: 3.2689629975865198
Validation loss: 2.958171461068286

Epoch: 6| Step: 6
Training loss: 2.493205565104523
Validation loss: 2.9468154986996806

Epoch: 6| Step: 7
Training loss: 3.678826038898156
Validation loss: 2.9388652736582097

Epoch: 6| Step: 8
Training loss: 2.720319798967538
Validation loss: 2.9411256234100587

Epoch: 6| Step: 9
Training loss: 3.395928379593152
Validation loss: 2.96095831771694

Epoch: 6| Step: 10
Training loss: 2.8359344734617116
Validation loss: 3.014438360613456

Epoch: 6| Step: 11
Training loss: 3.1430464941427867
Validation loss: 2.9534317432850967

Epoch: 6| Step: 12
Training loss: 3.1614914986108333
Validation loss: 2.9381790371207117

Epoch: 6| Step: 13
Training loss: 3.661850511684205
Validation loss: 2.9448485329026357

Epoch: 22| Step: 0
Training loss: 3.44491028113836
Validation loss: 2.958837427788396

Epoch: 6| Step: 1
Training loss: 2.92895124212117
Validation loss: 3.006341816236817

Epoch: 6| Step: 2
Training loss: 3.679597954764027
Validation loss: 3.01555698799886

Epoch: 6| Step: 3
Training loss: 2.508612769478614
Validation loss: 2.9455536939730673

Epoch: 6| Step: 4
Training loss: 3.051018660486721
Validation loss: 2.933855150556916

Epoch: 6| Step: 5
Training loss: 3.534436164144585
Validation loss: 2.931964069481215

Epoch: 6| Step: 6
Training loss: 3.812202066755948
Validation loss: 2.947252805314683

Epoch: 6| Step: 7
Training loss: 3.6898613820491675
Validation loss: 2.997702865902845

Epoch: 6| Step: 8
Training loss: 2.3846859430471246
Validation loss: 2.980420696651913

Epoch: 6| Step: 9
Training loss: 2.7568878266237866
Validation loss: 2.959566950297659

Epoch: 6| Step: 10
Training loss: 2.941324382316455
Validation loss: 2.9465703779797403

Epoch: 6| Step: 11
Training loss: 3.431506115747444
Validation loss: 2.938182427758803

Epoch: 6| Step: 12
Training loss: 3.484587658606495
Validation loss: 2.935812514962781

Epoch: 6| Step: 13
Training loss: 3.770192901573563
Validation loss: 2.928240062327267

Epoch: 23| Step: 0
Training loss: 3.5773690437110823
Validation loss: 2.927608188212535

Epoch: 6| Step: 1
Training loss: 3.1097536096147884
Validation loss: 2.9319961729149635

Epoch: 6| Step: 2
Training loss: 3.2920941586290913
Validation loss: 2.9355624809972243

Epoch: 6| Step: 3
Training loss: 2.7720262832386386
Validation loss: 2.937244377106389

Epoch: 6| Step: 4
Training loss: 3.887692736787956
Validation loss: 2.936172450680296

Epoch: 6| Step: 5
Training loss: 3.1469179182715235
Validation loss: 2.9351112970948785

Epoch: 6| Step: 6
Training loss: 2.876642504340489
Validation loss: 2.931257386634147

Epoch: 6| Step: 7
Training loss: 3.460981312769381
Validation loss: 2.9272761947919363

Epoch: 6| Step: 8
Training loss: 2.709206063922723
Validation loss: 2.9239788201400545

Epoch: 6| Step: 9
Training loss: 2.518698195310438
Validation loss: 2.9173223634478993

Epoch: 6| Step: 10
Training loss: 3.3550904386016613
Validation loss: 2.9177745847785017

Epoch: 6| Step: 11
Training loss: 3.1791796829403807
Validation loss: 2.911302718701226

Epoch: 6| Step: 12
Training loss: 3.502479764674326
Validation loss: 2.911399658593391

Epoch: 6| Step: 13
Training loss: 3.7572231663331976
Validation loss: 2.9249391587496807

Epoch: 24| Step: 0
Training loss: 2.8329450303819903
Validation loss: 2.9412502401113785

Epoch: 6| Step: 1
Training loss: 3.2273799424184024
Validation loss: 2.9355904563080344

Epoch: 6| Step: 2
Training loss: 2.336357359646657
Validation loss: 2.9132913419548343

Epoch: 6| Step: 3
Training loss: 2.7682355208995353
Validation loss: 2.906603241816764

Epoch: 6| Step: 4
Training loss: 2.7971229656413414
Validation loss: 2.905034176453512

Epoch: 6| Step: 5
Training loss: 3.1833290785782693
Validation loss: 2.9066702858250464

Epoch: 6| Step: 6
Training loss: 3.738869233163978
Validation loss: 2.9048415330503463

Epoch: 6| Step: 7
Training loss: 3.4395909279073833
Validation loss: 2.9023395331220767

Epoch: 6| Step: 8
Training loss: 3.7158878114762866
Validation loss: 2.903263986956299

Epoch: 6| Step: 9
Training loss: 3.6734309369624487
Validation loss: 2.900549566171067

Epoch: 6| Step: 10
Training loss: 3.1547038001838668
Validation loss: 2.8982007546613304

Epoch: 6| Step: 11
Training loss: 2.9153143517938833
Validation loss: 2.89548319513688

Epoch: 6| Step: 12
Training loss: 3.3488365772158004
Validation loss: 2.895486908472861

Epoch: 6| Step: 13
Training loss: 3.6111255710670944
Validation loss: 2.891248520127834

Epoch: 25| Step: 0
Training loss: 2.61030187534548
Validation loss: 2.894825331818189

Epoch: 6| Step: 1
Training loss: 2.7975540615445493
Validation loss: 2.8939512325782357

Epoch: 6| Step: 2
Training loss: 3.274767766974065
Validation loss: 2.9007949098206574

Epoch: 6| Step: 3
Training loss: 3.2153939834222385
Validation loss: 2.915493104692954

Epoch: 6| Step: 4
Training loss: 3.3478888449771977
Validation loss: 2.90312917313814

Epoch: 6| Step: 5
Training loss: 3.1027251786027628
Validation loss: 2.890248913121882

Epoch: 6| Step: 6
Training loss: 3.650871298275559
Validation loss: 2.884764456382318

Epoch: 6| Step: 7
Training loss: 3.2985182007314497
Validation loss: 2.8876784162051914

Epoch: 6| Step: 8
Training loss: 3.165039213157587
Validation loss: 2.8859706755654804

Epoch: 6| Step: 9
Training loss: 3.090721729026153
Validation loss: 2.891325318860089

Epoch: 6| Step: 10
Training loss: 3.25415243291408
Validation loss: 2.8804594407134108

Epoch: 6| Step: 11
Training loss: 3.439157814462477
Validation loss: 2.880291745296233

Epoch: 6| Step: 12
Training loss: 3.4189469861285535
Validation loss: 2.880023048605178

Epoch: 6| Step: 13
Training loss: 2.5625416822067257
Validation loss: 2.878835573518645

Epoch: 26| Step: 0
Training loss: 3.398681070653663
Validation loss: 2.879356877182218

Epoch: 6| Step: 1
Training loss: 3.0535062179039887
Validation loss: 2.8779512962840283

Epoch: 6| Step: 2
Training loss: 3.24191143285321
Validation loss: 2.8766288378795117

Epoch: 6| Step: 3
Training loss: 2.9029087014817025
Validation loss: 2.8928833409574475

Epoch: 6| Step: 4
Training loss: 2.774173950417986
Validation loss: 2.9048468574130024

Epoch: 6| Step: 5
Training loss: 2.882592169552744
Validation loss: 2.9239713553581343

Epoch: 6| Step: 6
Training loss: 3.1726169564022033
Validation loss: 2.9393865199827265

Epoch: 6| Step: 7
Training loss: 3.0336344670764115
Validation loss: 2.940287366993294

Epoch: 6| Step: 8
Training loss: 3.5711853162484135
Validation loss: 2.9432384423308555

Epoch: 6| Step: 9
Training loss: 2.648158415997015
Validation loss: 2.938706500423588

Epoch: 6| Step: 10
Training loss: 3.5110278102259014
Validation loss: 2.9334377371152778

Epoch: 6| Step: 11
Training loss: 3.7559304709209647
Validation loss: 2.93434569959591

Epoch: 6| Step: 12
Training loss: 3.18464023732071
Validation loss: 2.9313980685050645

Epoch: 6| Step: 13
Training loss: 4.016683357383482
Validation loss: 2.93082752779176

Epoch: 27| Step: 0
Training loss: 3.1967182378285126
Validation loss: 2.9319036949073967

Epoch: 6| Step: 1
Training loss: 2.9576684625520486
Validation loss: 2.926680098786921

Epoch: 6| Step: 2
Training loss: 3.5479084139489188
Validation loss: 2.926678352131742

Epoch: 6| Step: 3
Training loss: 3.569620077823409
Validation loss: 2.925277675605577

Epoch: 6| Step: 4
Training loss: 3.08224041788637
Validation loss: 2.9236105015207423

Epoch: 6| Step: 5
Training loss: 2.834595417843846
Validation loss: 2.9231039045435474

Epoch: 6| Step: 6
Training loss: 3.2111041171985155
Validation loss: 2.923311475398761

Epoch: 6| Step: 7
Training loss: 3.3627285564131117
Validation loss: 2.9214646553200083

Epoch: 6| Step: 8
Training loss: 3.043592357273861
Validation loss: 2.9189766439623637

Epoch: 6| Step: 9
Training loss: 3.0327580426931555
Validation loss: 2.921416226402063

Epoch: 6| Step: 10
Training loss: 3.05827663136929
Validation loss: 2.9188691263020257

Epoch: 6| Step: 11
Training loss: 3.172425771916549
Validation loss: 2.9195496884980807

Epoch: 6| Step: 12
Training loss: 3.1371917937981126
Validation loss: 2.9156022381147655

Epoch: 6| Step: 13
Training loss: 3.9893922818141414
Validation loss: 2.9178649011443847

Epoch: 28| Step: 0
Training loss: 3.549990726512563
Validation loss: 2.917364291658572

Epoch: 6| Step: 1
Training loss: 2.8845401538917885
Validation loss: 2.913402702284929

Epoch: 6| Step: 2
Training loss: 3.106948288962674
Validation loss: 2.9126359007139664

Epoch: 6| Step: 3
Training loss: 3.419296338154989
Validation loss: 2.912934207989972

Epoch: 6| Step: 4
Training loss: 3.1896957613521377
Validation loss: 2.9183404160319673

Epoch: 6| Step: 5
Training loss: 2.8378395448775295
Validation loss: 2.9225831146830132

Epoch: 6| Step: 6
Training loss: 2.8267680753955027
Validation loss: 2.923186261467573

Epoch: 6| Step: 7
Training loss: 2.96718499945384
Validation loss: 2.922574953359677

Epoch: 6| Step: 8
Training loss: 3.094532569012437
Validation loss: 2.9246959439196774

Epoch: 6| Step: 9
Training loss: 2.8433423431301437
Validation loss: 2.9117995640453174

Epoch: 6| Step: 10
Training loss: 3.6009633523717666
Validation loss: 2.906661293088415

Epoch: 6| Step: 11
Training loss: 3.678234680148026
Validation loss: 2.9110773796255014

Epoch: 6| Step: 12
Training loss: 3.5993017685136093
Validation loss: 2.914350965646105

Epoch: 6| Step: 13
Training loss: 3.0604731801015026
Validation loss: 2.9103980558078706

Epoch: 29| Step: 0
Training loss: 2.5748844935611936
Validation loss: 2.909201464240207

Epoch: 6| Step: 1
Training loss: 3.6529419108277423
Validation loss: 2.911038496901853

Epoch: 6| Step: 2
Training loss: 2.7855980992104206
Validation loss: 2.90993308278179

Epoch: 6| Step: 3
Training loss: 3.4384595052030056
Validation loss: 2.9077573032732933

Epoch: 6| Step: 4
Training loss: 3.197704063745874
Validation loss: 2.9032962505676996

Epoch: 6| Step: 5
Training loss: 3.3432330150773257
Validation loss: 2.9017273336039384

Epoch: 6| Step: 6
Training loss: 3.2130886028478853
Validation loss: 2.897295804683183

Epoch: 6| Step: 7
Training loss: 3.325427757798817
Validation loss: 2.900460156646934

Epoch: 6| Step: 8
Training loss: 2.285409662264369
Validation loss: 2.896949377424933

Epoch: 6| Step: 9
Training loss: 3.446963652009392
Validation loss: 2.895966664364948

Epoch: 6| Step: 10
Training loss: 3.6811655238874836
Validation loss: 2.89721334800787

Epoch: 6| Step: 11
Training loss: 2.739073112367134
Validation loss: 2.8989813434574807

Epoch: 6| Step: 12
Training loss: 2.9923025562286805
Validation loss: 2.897352347120134

Epoch: 6| Step: 13
Training loss: 4.061742154373328
Validation loss: 2.906598293760559

Epoch: 30| Step: 0
Training loss: 2.9100328662497628
Validation loss: 2.895589242915658

Epoch: 6| Step: 1
Training loss: 2.869788712755588
Validation loss: 2.8918082385544626

Epoch: 6| Step: 2
Training loss: 3.682784864461968
Validation loss: 2.894107844264515

Epoch: 6| Step: 3
Training loss: 2.3343704734257904
Validation loss: 2.8936879699603186

Epoch: 6| Step: 4
Training loss: 3.2142038183527015
Validation loss: 2.895299397647725

Epoch: 6| Step: 5
Training loss: 2.924344248502977
Validation loss: 2.898331569704659

Epoch: 6| Step: 6
Training loss: 3.649612662260708
Validation loss: 2.903217034628593

Epoch: 6| Step: 7
Training loss: 3.6887533353184843
Validation loss: 2.9033016881403366

Epoch: 6| Step: 8
Training loss: 2.597575480417238
Validation loss: 2.9019522334887076

Epoch: 6| Step: 9
Training loss: 3.3233371735617268
Validation loss: 2.9021466013128543

Epoch: 6| Step: 10
Training loss: 2.9728411944049213
Validation loss: 2.9034764585570625

Epoch: 6| Step: 11
Training loss: 3.414491953780123
Validation loss: 2.905195646774127

Epoch: 6| Step: 12
Training loss: 3.4616681914187657
Validation loss: 2.899811002465882

Epoch: 6| Step: 13
Training loss: 3.411444945952183
Validation loss: 2.8971662047094737

Epoch: 31| Step: 0
Training loss: 2.5649225832315468
Validation loss: 2.917046085998955

Epoch: 6| Step: 1
Training loss: 2.6354579595615277
Validation loss: 2.9448500302498584

Epoch: 6| Step: 2
Training loss: 3.364424206697893
Validation loss: 2.921738786402947

Epoch: 6| Step: 3
Training loss: 2.569227737590138
Validation loss: 2.8959301690157115

Epoch: 6| Step: 4
Training loss: 3.413882043464515
Validation loss: 2.8825180019338097

Epoch: 6| Step: 5
Training loss: 3.2480339559274043
Validation loss: 2.8819213382608284

Epoch: 6| Step: 6
Training loss: 3.275138322078559
Validation loss: 2.8851591991718055

Epoch: 6| Step: 7
Training loss: 2.9155747322418457
Validation loss: 2.888506521956953

Epoch: 6| Step: 8
Training loss: 2.966728405542202
Validation loss: 2.8901064721678673

Epoch: 6| Step: 9
Training loss: 3.7852531239619074
Validation loss: 2.8953049653471585

Epoch: 6| Step: 10
Training loss: 3.341652185372219
Validation loss: 2.887523917766913

Epoch: 6| Step: 11
Training loss: 3.8085709634735014
Validation loss: 2.8827510942473835

Epoch: 6| Step: 12
Training loss: 3.2542276029136223
Validation loss: 2.879715391527231

Epoch: 6| Step: 13
Training loss: 3.1992558627368877
Validation loss: 2.8789711482660074

Epoch: 32| Step: 0
Training loss: 3.2866944693214135
Validation loss: 2.882259622349599

Epoch: 6| Step: 1
Training loss: 3.4463145211053137
Validation loss: 2.8948736272971227

Epoch: 6| Step: 2
Training loss: 3.774575640340042
Validation loss: 2.9084872145990315

Epoch: 6| Step: 3
Training loss: 3.7674008056299546
Validation loss: 2.885228958382852

Epoch: 6| Step: 4
Training loss: 2.6789232059363783
Validation loss: 2.8727012279072386

Epoch: 6| Step: 5
Training loss: 2.53991478333987
Validation loss: 2.8815070726301024

Epoch: 6| Step: 6
Training loss: 2.984969953580655
Validation loss: 2.8958392110360505

Epoch: 6| Step: 7
Training loss: 3.4499839892914266
Validation loss: 2.9098209694223094

Epoch: 6| Step: 8
Training loss: 3.403995152560771
Validation loss: 2.91462602791156

Epoch: 6| Step: 9
Training loss: 2.9798096894293913
Validation loss: 2.90150632248645

Epoch: 6| Step: 10
Training loss: 3.0147175584783166
Validation loss: 2.8827376390540396

Epoch: 6| Step: 11
Training loss: 3.4358789956809344
Validation loss: 2.873294931960792

Epoch: 6| Step: 12
Training loss: 2.5406830765771375
Validation loss: 2.8665961565840954

Epoch: 6| Step: 13
Training loss: 3.275013255318177
Validation loss: 2.865738688561093

Epoch: 33| Step: 0
Training loss: 2.799877324141706
Validation loss: 2.866402937735815

Epoch: 6| Step: 1
Training loss: 3.1320423882491677
Validation loss: 2.8786720804765458

Epoch: 6| Step: 2
Training loss: 3.0339381303206037
Validation loss: 2.875691628961997

Epoch: 6| Step: 3
Training loss: 3.581420313861328
Validation loss: 2.8335367801691977

Epoch: 6| Step: 4
Training loss: 2.7371841387788507
Validation loss: 2.8165330698851574

Epoch: 6| Step: 5
Training loss: 2.881400902493208
Validation loss: 2.806633640984835

Epoch: 6| Step: 6
Training loss: 3.2345631125474634
Validation loss: 2.801193667580354

Epoch: 6| Step: 7
Training loss: 2.83469803051301
Validation loss: 2.8047463206965086

Epoch: 6| Step: 8
Training loss: 3.3563764097251916
Validation loss: 2.8038147225244887

Epoch: 6| Step: 9
Training loss: 2.516161559322999
Validation loss: 2.8048410014019725

Epoch: 6| Step: 10
Training loss: 3.48311438229796
Validation loss: 2.803496327516301

Epoch: 6| Step: 11
Training loss: 3.301970101870988
Validation loss: 2.80257986634066

Epoch: 6| Step: 12
Training loss: 3.389886309762502
Validation loss: 2.803863232426451

Epoch: 6| Step: 13
Training loss: 3.634945347937864
Validation loss: 2.8073091365818477

Epoch: 34| Step: 0
Training loss: 3.0696822046068992
Validation loss: 2.803778849069892

Epoch: 6| Step: 1
Training loss: 3.2350965791462563
Validation loss: 2.8069203348620815

Epoch: 6| Step: 2
Training loss: 2.615570570950265
Validation loss: 2.7995785275125855

Epoch: 6| Step: 3
Training loss: 3.8982816676488565
Validation loss: 2.7992273354878128

Epoch: 6| Step: 4
Training loss: 3.487930741593688
Validation loss: 2.800649676381248

Epoch: 6| Step: 5
Training loss: 2.3285571215739194
Validation loss: 2.798649514447278

Epoch: 6| Step: 6
Training loss: 2.5022259339398594
Validation loss: 2.8000289065103847

Epoch: 6| Step: 7
Training loss: 3.3957624057469302
Validation loss: 2.7991761927576952

Epoch: 6| Step: 8
Training loss: 3.4780539067518035
Validation loss: 2.7997960858724538

Epoch: 6| Step: 9
Training loss: 3.8420122141527258
Validation loss: 2.798482176822223

Epoch: 6| Step: 10
Training loss: 2.743349703913343
Validation loss: 2.796726696074109

Epoch: 6| Step: 11
Training loss: 2.6992772229895716
Validation loss: 2.79566791202092

Epoch: 6| Step: 12
Training loss: 2.830286706750259
Validation loss: 2.7922673951181824

Epoch: 6| Step: 13
Training loss: 2.918138042068668
Validation loss: 2.794030135061277

Epoch: 35| Step: 0
Training loss: 2.729027363568335
Validation loss: 2.7933209831132566

Epoch: 6| Step: 1
Training loss: 2.4673254997598386
Validation loss: 2.7953436660292525

Epoch: 6| Step: 2
Training loss: 3.125981901403204
Validation loss: 2.796768719007024

Epoch: 6| Step: 3
Training loss: 2.6445476582185763
Validation loss: 2.817535506696421

Epoch: 6| Step: 4
Training loss: 3.5094479696784053
Validation loss: 2.9348397131336896

Epoch: 6| Step: 5
Training loss: 3.7392602354147937
Validation loss: 2.9202746358846143

Epoch: 6| Step: 6
Training loss: 3.546427828794579
Validation loss: 2.793962270061264

Epoch: 6| Step: 7
Training loss: 3.487567072397699
Validation loss: 2.788914381808234

Epoch: 6| Step: 8
Training loss: 2.833302852990251
Validation loss: 2.7998382458644917

Epoch: 6| Step: 9
Training loss: 3.7624554096000282
Validation loss: 2.834121529925104

Epoch: 6| Step: 10
Training loss: 2.7446239982415865
Validation loss: 2.8006097026484364

Epoch: 6| Step: 11
Training loss: 2.3665798905631665
Validation loss: 2.790573923442107

Epoch: 6| Step: 12
Training loss: 3.373416387819676
Validation loss: 2.7894844950850493

Epoch: 6| Step: 13
Training loss: 3.316773303312852
Validation loss: 2.791951319422713

Epoch: 36| Step: 0
Training loss: 2.4189005644705404
Validation loss: 2.7909327004239204

Epoch: 6| Step: 1
Training loss: 3.423752639730494
Validation loss: 2.7875886596665334

Epoch: 6| Step: 2
Training loss: 3.072608923723377
Validation loss: 2.799259568244169

Epoch: 6| Step: 3
Training loss: 3.217986525569608
Validation loss: 2.804549119519784

Epoch: 6| Step: 4
Training loss: 3.3138681141608153
Validation loss: 2.8274752978154734

Epoch: 6| Step: 5
Training loss: 3.8257254417482245
Validation loss: 2.856546021144502

Epoch: 6| Step: 6
Training loss: 3.4877742044263718
Validation loss: 2.7937813874658812

Epoch: 6| Step: 7
Training loss: 2.619666721607885
Validation loss: 2.7790913377755766

Epoch: 6| Step: 8
Training loss: 2.6491849401595142
Validation loss: 2.778292906348981

Epoch: 6| Step: 9
Training loss: 2.8372967617837777
Validation loss: 2.779853678607907

Epoch: 6| Step: 10
Training loss: 3.264981000999799
Validation loss: 2.779973658871219

Epoch: 6| Step: 11
Training loss: 2.914343117300859
Validation loss: 2.7789146404877774

Epoch: 6| Step: 12
Training loss: 3.300603268130502
Validation loss: 2.781891885510375

Epoch: 6| Step: 13
Training loss: 2.733023260330949
Validation loss: 2.7813805206964792

Epoch: 37| Step: 0
Training loss: 2.4456184366451548
Validation loss: 2.784748854327507

Epoch: 6| Step: 1
Training loss: 3.3117199825096493
Validation loss: 2.7851739063159777

Epoch: 6| Step: 2
Training loss: 3.2915826778722352
Validation loss: 2.777424061995735

Epoch: 6| Step: 3
Training loss: 2.559203656135373
Validation loss: 2.776894876456701

Epoch: 6| Step: 4
Training loss: 3.12898671478933
Validation loss: 2.7741624720295435

Epoch: 6| Step: 5
Training loss: 2.428156428581692
Validation loss: 2.7723403676339986

Epoch: 6| Step: 6
Training loss: 2.5734024721783175
Validation loss: 2.7713080372804124

Epoch: 6| Step: 7
Training loss: 3.384216721938021
Validation loss: 2.76690372479862

Epoch: 6| Step: 8
Training loss: 3.4098603599796498
Validation loss: 2.7674266021570393

Epoch: 6| Step: 9
Training loss: 3.866716822485263
Validation loss: 2.766570167795854

Epoch: 6| Step: 10
Training loss: 3.101472252750703
Validation loss: 2.7702174316774886

Epoch: 6| Step: 11
Training loss: 3.145193392776716
Validation loss: 2.768734710761515

Epoch: 6| Step: 12
Training loss: 3.2683234496277627
Validation loss: 2.7769085352272374

Epoch: 6| Step: 13
Training loss: 2.9147876317969654
Validation loss: 2.7958727175204685

Epoch: 38| Step: 0
Training loss: 2.8788943245173626
Validation loss: 2.822396364422357

Epoch: 6| Step: 1
Training loss: 3.0443842170339295
Validation loss: 2.8754616090805203

Epoch: 6| Step: 2
Training loss: 3.3567089768602925
Validation loss: 2.8693905036651755

Epoch: 6| Step: 3
Training loss: 3.2618969531690087
Validation loss: 2.8122721726764097

Epoch: 6| Step: 4
Training loss: 2.8726337480740094
Validation loss: 2.7765026014996237

Epoch: 6| Step: 5
Training loss: 2.986023771994949
Validation loss: 2.764161445879135

Epoch: 6| Step: 6
Training loss: 2.8100000078737524
Validation loss: 2.7595545946682107

Epoch: 6| Step: 7
Training loss: 3.0620119912718144
Validation loss: 2.7609541465903282

Epoch: 6| Step: 8
Training loss: 3.110007983623696
Validation loss: 2.7623374201794437

Epoch: 6| Step: 9
Training loss: 2.783029222911361
Validation loss: 2.761740595989963

Epoch: 6| Step: 10
Training loss: 3.181931062652927
Validation loss: 2.7651650132158454

Epoch: 6| Step: 11
Training loss: 3.4566036326278002
Validation loss: 2.7673583932346877

Epoch: 6| Step: 12
Training loss: 3.2269931164110432
Validation loss: 2.769536714157786

Epoch: 6| Step: 13
Training loss: 3.5197963805259773
Validation loss: 2.7738561743772614

Epoch: 39| Step: 0
Training loss: 2.8470364538444257
Validation loss: 2.7669898821986805

Epoch: 6| Step: 1
Training loss: 2.8674865067820545
Validation loss: 2.764852406735733

Epoch: 6| Step: 2
Training loss: 3.113966422505735
Validation loss: 2.7626297502620782

Epoch: 6| Step: 3
Training loss: 2.9328690407068168
Validation loss: 2.7624080713378

Epoch: 6| Step: 4
Training loss: 3.4186586915258896
Validation loss: 2.758302070830373

Epoch: 6| Step: 5
Training loss: 3.07369754158545
Validation loss: 2.7553427657015477

Epoch: 6| Step: 6
Training loss: 3.02234245400844
Validation loss: 2.756324057539465

Epoch: 6| Step: 7
Training loss: 2.8866280630524357
Validation loss: 2.75514859863774

Epoch: 6| Step: 8
Training loss: 2.5490219334840596
Validation loss: 2.7529919402482483

Epoch: 6| Step: 9
Training loss: 3.588573675285101
Validation loss: 2.751328646103665

Epoch: 6| Step: 10
Training loss: 3.20543427380238
Validation loss: 2.747792044637413

Epoch: 6| Step: 11
Training loss: 3.052814660752479
Validation loss: 2.746946761643906

Epoch: 6| Step: 12
Training loss: 2.784131818056782
Validation loss: 2.7549101751575136

Epoch: 6| Step: 13
Training loss: 3.8639800362555
Validation loss: 2.768261783816604

Epoch: 40| Step: 0
Training loss: 2.9160480024784894
Validation loss: 2.7663735814038475

Epoch: 6| Step: 1
Training loss: 2.761810689688026
Validation loss: 2.7623284345975874

Epoch: 6| Step: 2
Training loss: 2.209517275533583
Validation loss: 2.7529081236019732

Epoch: 6| Step: 3
Training loss: 3.4861897075113126
Validation loss: 2.7454380275993757

Epoch: 6| Step: 4
Training loss: 3.434150589890876
Validation loss: 2.7432441284885885

Epoch: 6| Step: 5
Training loss: 2.7433698664566046
Validation loss: 2.745206155533022

Epoch: 6| Step: 6
Training loss: 2.85432864284104
Validation loss: 2.743019811846405

Epoch: 6| Step: 7
Training loss: 3.552751957376435
Validation loss: 2.747458791232562

Epoch: 6| Step: 8
Training loss: 2.8982919003503134
Validation loss: 2.748264089272837

Epoch: 6| Step: 9
Training loss: 2.9186778809093967
Validation loss: 2.7466230269677925

Epoch: 6| Step: 10
Training loss: 3.0206045831471373
Validation loss: 2.7469302893942373

Epoch: 6| Step: 11
Training loss: 3.2578619717654087
Validation loss: 2.74103474350407

Epoch: 6| Step: 12
Training loss: 3.481928355558481
Validation loss: 2.744152068279686

Epoch: 6| Step: 13
Training loss: 3.316247799236719
Validation loss: 2.7449575285623173

Epoch: 41| Step: 0
Training loss: 3.035914342362946
Validation loss: 2.741272689509738

Epoch: 6| Step: 1
Training loss: 2.821369850002897
Validation loss: 2.7402895553637334

Epoch: 6| Step: 2
Training loss: 2.621302043523666
Validation loss: 2.7401958429508673

Epoch: 6| Step: 3
Training loss: 3.108151142279666
Validation loss: 2.736309632586979

Epoch: 6| Step: 4
Training loss: 3.8004239297607887
Validation loss: 2.7392690590523086

Epoch: 6| Step: 5
Training loss: 3.792337351299271
Validation loss: 2.7370688945499877

Epoch: 6| Step: 6
Training loss: 3.158597167846129
Validation loss: 2.736689197627945

Epoch: 6| Step: 7
Training loss: 2.6684744091811776
Validation loss: 2.735745016989969

Epoch: 6| Step: 8
Training loss: 2.9194469461169192
Validation loss: 2.7391754287159347

Epoch: 6| Step: 9
Training loss: 3.618851412865811
Validation loss: 2.7464987661923237

Epoch: 6| Step: 10
Training loss: 2.9679117725965156
Validation loss: 2.75489515197177

Epoch: 6| Step: 11
Training loss: 2.8742897151430555
Validation loss: 2.769935986898805

Epoch: 6| Step: 12
Training loss: 1.7860109382542204
Validation loss: 2.7823253275103257

Epoch: 6| Step: 13
Training loss: 3.2189858359445633
Validation loss: 2.824366184010976

Epoch: 42| Step: 0
Training loss: 2.7540642442765897
Validation loss: 2.7717106482156293

Epoch: 6| Step: 1
Training loss: 3.2210942592430776
Validation loss: 2.743757250821073

Epoch: 6| Step: 2
Training loss: 3.5181495926689195
Validation loss: 2.7309983093933803

Epoch: 6| Step: 3
Training loss: 3.247247557427482
Validation loss: 2.7301260464223462

Epoch: 6| Step: 4
Training loss: 3.5148870753331107
Validation loss: 2.7305569780678227

Epoch: 6| Step: 5
Training loss: 3.3203096277561106
Validation loss: 2.7354907537827646

Epoch: 6| Step: 6
Training loss: 2.263224204479895
Validation loss: 2.7380689907605715

Epoch: 6| Step: 7
Training loss: 3.1721881279687802
Validation loss: 2.7420841021437545

Epoch: 6| Step: 8
Training loss: 2.9266631525086644
Validation loss: 2.7453615474531676

Epoch: 6| Step: 9
Training loss: 2.9232403018405257
Validation loss: 2.7439361681775543

Epoch: 6| Step: 10
Training loss: 2.236615851270845
Validation loss: 2.737798122164856

Epoch: 6| Step: 11
Training loss: 2.723508944862822
Validation loss: 2.733335417734948

Epoch: 6| Step: 12
Training loss: 3.5561339401319927
Validation loss: 2.7299386502686374

Epoch: 6| Step: 13
Training loss: 3.141916493433424
Validation loss: 2.7293165520042026

Epoch: 43| Step: 0
Training loss: 2.5044318970056865
Validation loss: 2.729370932100159

Epoch: 6| Step: 1
Training loss: 2.793413118878439
Validation loss: 2.7286566964162824

Epoch: 6| Step: 2
Training loss: 3.1117495570476317
Validation loss: 2.724925389265194

Epoch: 6| Step: 3
Training loss: 3.704583375803715
Validation loss: 2.7260777429523144

Epoch: 6| Step: 4
Training loss: 2.9593640175185603
Validation loss: 2.7265311877338507

Epoch: 6| Step: 5
Training loss: 2.900988173662214
Validation loss: 2.7244283923528303

Epoch: 6| Step: 6
Training loss: 3.0947786412863216
Validation loss: 2.7236397815638527

Epoch: 6| Step: 7
Training loss: 3.2603370892418226
Validation loss: 2.7280030273592355

Epoch: 6| Step: 8
Training loss: 3.3516561143868033
Validation loss: 2.7202544537645768

Epoch: 6| Step: 9
Training loss: 3.1183455761156553
Validation loss: 2.7261611262718355

Epoch: 6| Step: 10
Training loss: 2.86758378535872
Validation loss: 2.723598586268741

Epoch: 6| Step: 11
Training loss: 2.8270021180022504
Validation loss: 2.723384322282306

Epoch: 6| Step: 12
Training loss: 3.265983306356213
Validation loss: 2.721591847979732

Epoch: 6| Step: 13
Training loss: 2.621133681787618
Validation loss: 2.7200571721834605

Epoch: 44| Step: 0
Training loss: 3.029627887046884
Validation loss: 2.7183145446147132

Epoch: 6| Step: 1
Training loss: 2.83410181581696
Validation loss: 2.71930003775581

Epoch: 6| Step: 2
Training loss: 2.4132940062941834
Validation loss: 2.7169703334309654

Epoch: 6| Step: 3
Training loss: 2.7664676395511285
Validation loss: 2.7163961043899754

Epoch: 6| Step: 4
Training loss: 3.9725836313037686
Validation loss: 2.7157596828081316

Epoch: 6| Step: 5
Training loss: 3.023530549581553
Validation loss: 2.7220057219394636

Epoch: 6| Step: 6
Training loss: 3.341570848215339
Validation loss: 2.7249226844313243

Epoch: 6| Step: 7
Training loss: 2.4918993361762714
Validation loss: 2.750824471884909

Epoch: 6| Step: 8
Training loss: 4.002065840363023
Validation loss: 2.777995260776214

Epoch: 6| Step: 9
Training loss: 3.0649023170524914
Validation loss: 2.733520473668786

Epoch: 6| Step: 10
Training loss: 2.662833727485739
Validation loss: 2.7156499346013137

Epoch: 6| Step: 11
Training loss: 3.4438454062647725
Validation loss: 2.7104577061966806

Epoch: 6| Step: 12
Training loss: 1.9536284751463333
Validation loss: 2.7105746880943333

Epoch: 6| Step: 13
Training loss: 2.9560974339802417
Validation loss: 2.7136556846143836

Epoch: 45| Step: 0
Training loss: 3.450862505808741
Validation loss: 2.7184144867862425

Epoch: 6| Step: 1
Training loss: 2.7348917445598593
Validation loss: 2.719521260014514

Epoch: 6| Step: 2
Training loss: 3.1242925224551463
Validation loss: 2.7252905397700493

Epoch: 6| Step: 3
Training loss: 3.240881084450993
Validation loss: 2.720847561000711

Epoch: 6| Step: 4
Training loss: 3.7517802780982463
Validation loss: 2.71873058556855

Epoch: 6| Step: 5
Training loss: 2.709953723244902
Validation loss: 2.7132789519754774

Epoch: 6| Step: 6
Training loss: 3.3112515670064155
Validation loss: 2.709901169096847

Epoch: 6| Step: 7
Training loss: 2.974746270138699
Validation loss: 2.7073283436212816

Epoch: 6| Step: 8
Training loss: 2.8638257683763184
Validation loss: 2.7063192462187446

Epoch: 6| Step: 9
Training loss: 2.8441803994244483
Validation loss: 2.7051636074596597

Epoch: 6| Step: 10
Training loss: 3.0600728907661265
Validation loss: 2.7070703620691234

Epoch: 6| Step: 11
Training loss: 3.213255405298903
Validation loss: 2.7080251879702826

Epoch: 6| Step: 12
Training loss: 2.6650115081939427
Validation loss: 2.7110623997040797

Epoch: 6| Step: 13
Training loss: 2.1345214704129374
Validation loss: 2.7099906173451

Epoch: 46| Step: 0
Training loss: 3.13470751747157
Validation loss: 2.704474366594317

Epoch: 6| Step: 1
Training loss: 3.1652484696229903
Validation loss: 2.7030066021401007

Epoch: 6| Step: 2
Training loss: 2.425896732450178
Validation loss: 2.7051189427376126

Epoch: 6| Step: 3
Training loss: 2.769769182194039
Validation loss: 2.704746017069001

Epoch: 6| Step: 4
Training loss: 2.628023223176301
Validation loss: 2.703646641112227

Epoch: 6| Step: 5
Training loss: 2.6176972305623036
Validation loss: 2.704075657684432

Epoch: 6| Step: 6
Training loss: 3.490372632089566
Validation loss: 2.7037959022887184

Epoch: 6| Step: 7
Training loss: 3.1729773499130944
Validation loss: 2.7037023529613067

Epoch: 6| Step: 8
Training loss: 2.7875274793698783
Validation loss: 2.7049615574470383

Epoch: 6| Step: 9
Training loss: 3.443683818866612
Validation loss: 2.702741905074702

Epoch: 6| Step: 10
Training loss: 2.688012362587974
Validation loss: 2.700837207036896

Epoch: 6| Step: 11
Training loss: 3.079016386983986
Validation loss: 2.7004211605477337

Epoch: 6| Step: 12
Training loss: 3.778813609686634
Validation loss: 2.700177669224585

Epoch: 6| Step: 13
Training loss: 3.211801677163105
Validation loss: 2.703191214499777

Epoch: 47| Step: 0
Training loss: 2.8044958872677386
Validation loss: 2.6986522412192815

Epoch: 6| Step: 1
Training loss: 3.0479247803405136
Validation loss: 2.7022324776216498

Epoch: 6| Step: 2
Training loss: 3.44943965839514
Validation loss: 2.7080943441528937

Epoch: 6| Step: 3
Training loss: 2.261850936268336
Validation loss: 2.711645693146486

Epoch: 6| Step: 4
Training loss: 3.3880495758753097
Validation loss: 2.7247240124521577

Epoch: 6| Step: 5
Training loss: 2.9666046421839467
Validation loss: 2.723110326654064

Epoch: 6| Step: 6
Training loss: 3.379171266250835
Validation loss: 2.72565881668263

Epoch: 6| Step: 7
Training loss: 3.228427744994594
Validation loss: 2.725576747985596

Epoch: 6| Step: 8
Training loss: 3.4260565582969313
Validation loss: 2.7145486707451827

Epoch: 6| Step: 9
Training loss: 3.1277690063708916
Validation loss: 2.711049528810542

Epoch: 6| Step: 10
Training loss: 3.0460050465773056
Validation loss: 2.7097515062745567

Epoch: 6| Step: 11
Training loss: 2.2422180173704462
Validation loss: 2.7060243628974443

Epoch: 6| Step: 12
Training loss: 3.2879483109469625
Validation loss: 2.705684070060733

Epoch: 6| Step: 13
Training loss: 2.662853514793399
Validation loss: 2.7043925679045886

Epoch: 48| Step: 0
Training loss: 3.0207749108990054
Validation loss: 2.7004566517062045

Epoch: 6| Step: 1
Training loss: 3.6481185871726014
Validation loss: 2.7006898431709376

Epoch: 6| Step: 2
Training loss: 2.8767522364944433
Validation loss: 2.696922888586269

Epoch: 6| Step: 3
Training loss: 3.4547697957216577
Validation loss: 2.697492385502708

Epoch: 6| Step: 4
Training loss: 2.738446327272732
Validation loss: 2.6967493280282113

Epoch: 6| Step: 5
Training loss: 3.0286580438386883
Validation loss: 2.696680434190692

Epoch: 6| Step: 6
Training loss: 2.361570031937878
Validation loss: 2.694596894536793

Epoch: 6| Step: 7
Training loss: 2.740465106888828
Validation loss: 2.691462836254205

Epoch: 6| Step: 8
Training loss: 2.8803483723172523
Validation loss: 2.6906871017334884

Epoch: 6| Step: 9
Training loss: 3.188185075927861
Validation loss: 2.687149687035568

Epoch: 6| Step: 10
Training loss: 3.424664758021431
Validation loss: 2.7053428905634744

Epoch: 6| Step: 11
Training loss: 2.3162062560231274
Validation loss: 2.7697928667379226

Epoch: 6| Step: 12
Training loss: 3.481732379907366
Validation loss: 2.867536936315172

Epoch: 6| Step: 13
Training loss: 3.2320226693633076
Validation loss: 2.7139040897997875

Epoch: 49| Step: 0
Training loss: 2.5634265248092647
Validation loss: 2.6858840886390003

Epoch: 6| Step: 1
Training loss: 3.03921045565894
Validation loss: 2.693572837055943

Epoch: 6| Step: 2
Training loss: 3.4643368527622624
Validation loss: 2.6968168017851455

Epoch: 6| Step: 3
Training loss: 2.938538063549243
Validation loss: 2.6992016284175753

Epoch: 6| Step: 4
Training loss: 2.9640367537985512
Validation loss: 2.7029920786333927

Epoch: 6| Step: 5
Training loss: 2.7795633924117116
Validation loss: 2.7162745560131993

Epoch: 6| Step: 6
Training loss: 3.561495990820003
Validation loss: 2.728783286930356

Epoch: 6| Step: 7
Training loss: 2.348763761617107
Validation loss: 2.7262689326811604

Epoch: 6| Step: 8
Training loss: 3.246894820210649
Validation loss: 2.7111966094784266

Epoch: 6| Step: 9
Training loss: 2.4926433086615276
Validation loss: 2.703133763191149

Epoch: 6| Step: 10
Training loss: 3.2403479815451774
Validation loss: 2.6958666217137948

Epoch: 6| Step: 11
Training loss: 2.830692537849254
Validation loss: 2.6948893239377254

Epoch: 6| Step: 12
Training loss: 3.658136516887264
Validation loss: 2.6932210009146043

Epoch: 6| Step: 13
Training loss: 3.2935286619457416
Validation loss: 2.6866607338094197

Epoch: 50| Step: 0
Training loss: 3.0688821115433518
Validation loss: 2.6838712602360606

Epoch: 6| Step: 1
Training loss: 2.9636105672986948
Validation loss: 2.6821967243834828

Epoch: 6| Step: 2
Training loss: 3.1822163790939837
Validation loss: 2.6826057292431127

Epoch: 6| Step: 3
Training loss: 2.995179435946305
Validation loss: 2.6871967010981432

Epoch: 6| Step: 4
Training loss: 2.712049976057196
Validation loss: 2.688980414044985

Epoch: 6| Step: 5
Training loss: 2.7211366021951924
Validation loss: 2.694464222286726

Epoch: 6| Step: 6
Training loss: 2.8069997622738185
Validation loss: 2.6894057628916905

Epoch: 6| Step: 7
Training loss: 3.3688054634015514
Validation loss: 2.694444740381941

Epoch: 6| Step: 8
Training loss: 3.3615348082973973
Validation loss: 2.701148866718531

Epoch: 6| Step: 9
Training loss: 2.6750198506126783
Validation loss: 2.7130573322213736

Epoch: 6| Step: 10
Training loss: 3.253512025470004
Validation loss: 2.70287784734219

Epoch: 6| Step: 11
Training loss: 3.1668422215302887
Validation loss: 2.707642542076916

Epoch: 6| Step: 12
Training loss: 3.1635340873385465
Validation loss: 2.7013149447658518

Epoch: 6| Step: 13
Training loss: 2.308489027407461
Validation loss: 2.706650476314318

Epoch: 51| Step: 0
Training loss: 3.068607856892491
Validation loss: 2.697110923841838

Epoch: 6| Step: 1
Training loss: 2.65419393512446
Validation loss: 2.67722274387274

Epoch: 6| Step: 2
Training loss: 3.0809140035729836
Validation loss: 2.675081430490897

Epoch: 6| Step: 3
Training loss: 2.3645215019398718
Validation loss: 2.6773153971927295

Epoch: 6| Step: 4
Training loss: 2.942698322830503
Validation loss: 2.6815086501781145

Epoch: 6| Step: 5
Training loss: 2.933860147015132
Validation loss: 2.686715597506481

Epoch: 6| Step: 6
Training loss: 3.1552388299421343
Validation loss: 2.692446718443133

Epoch: 6| Step: 7
Training loss: 3.609815166105775
Validation loss: 2.688872137063143

Epoch: 6| Step: 8
Training loss: 3.1163581375841596
Validation loss: 2.690156046271585

Epoch: 6| Step: 9
Training loss: 3.1718359132059044
Validation loss: 2.691407671171857

Epoch: 6| Step: 10
Training loss: 3.1281642915161596
Validation loss: 2.6845289015637945

Epoch: 6| Step: 11
Training loss: 2.6928269577213753
Validation loss: 2.6823890605432124

Epoch: 6| Step: 12
Training loss: 2.940256975841461
Validation loss: 2.6815151598747726

Epoch: 6| Step: 13
Training loss: 3.656641621279308
Validation loss: 2.679648548882219

Epoch: 52| Step: 0
Training loss: 3.074460399393123
Validation loss: 2.6783346085911934

Epoch: 6| Step: 1
Training loss: 2.56242854321101
Validation loss: 2.677387839973339

Epoch: 6| Step: 2
Training loss: 2.8582901694668954
Validation loss: 2.6747951117057163

Epoch: 6| Step: 3
Training loss: 2.411272924777976
Validation loss: 2.671496202470798

Epoch: 6| Step: 4
Training loss: 3.067321096611308
Validation loss: 2.674837975841729

Epoch: 6| Step: 5
Training loss: 3.25878569705962
Validation loss: 2.667527879449199

Epoch: 6| Step: 6
Training loss: 3.0219786128177883
Validation loss: 2.6616737656284797

Epoch: 6| Step: 7
Training loss: 2.9570757561462457
Validation loss: 2.6641597179745995

Epoch: 6| Step: 8
Training loss: 2.306862118751448
Validation loss: 2.6687514983280543

Epoch: 6| Step: 9
Training loss: 3.201108418662546
Validation loss: 2.6689668362934826

Epoch: 6| Step: 10
Training loss: 3.713610519619776
Validation loss: 2.6776536867275578

Epoch: 6| Step: 11
Training loss: 3.2774755658272614
Validation loss: 2.6604346738166984

Epoch: 6| Step: 12
Training loss: 3.187320778988043
Validation loss: 2.661149197351403

Epoch: 6| Step: 13
Training loss: 3.0429071657660236
Validation loss: 2.6608881702773397

Epoch: 53| Step: 0
Training loss: 3.1388128565879234
Validation loss: 2.668032937084073

Epoch: 6| Step: 1
Training loss: 2.9553208034028033
Validation loss: 2.6648742076239675

Epoch: 6| Step: 2
Training loss: 2.6036705663525908
Validation loss: 2.663897743479702

Epoch: 6| Step: 3
Training loss: 2.710834072459656
Validation loss: 2.6671332488211226

Epoch: 6| Step: 4
Training loss: 3.007759391018546
Validation loss: 2.66692064470433

Epoch: 6| Step: 5
Training loss: 2.3980326854523994
Validation loss: 2.664845548174063

Epoch: 6| Step: 6
Training loss: 2.966638396339161
Validation loss: 2.6650244912866796

Epoch: 6| Step: 7
Training loss: 3.2269137655438436
Validation loss: 2.666788487446199

Epoch: 6| Step: 8
Training loss: 2.8498412907897923
Validation loss: 2.664420709346758

Epoch: 6| Step: 9
Training loss: 2.62471315542373
Validation loss: 2.6610910618857893

Epoch: 6| Step: 10
Training loss: 3.741750671757755
Validation loss: 2.6621607675124506

Epoch: 6| Step: 11
Training loss: 3.7176918640085455
Validation loss: 2.65906409372085

Epoch: 6| Step: 12
Training loss: 2.868515995762327
Validation loss: 2.6593342063962697

Epoch: 6| Step: 13
Training loss: 3.0613460312933314
Validation loss: 2.657075481281399

Epoch: 54| Step: 0
Training loss: 2.731147299784491
Validation loss: 2.6610231918963385

Epoch: 6| Step: 1
Training loss: 3.448293767093344
Validation loss: 2.6566576322206177

Epoch: 6| Step: 2
Training loss: 2.6201808834971017
Validation loss: 2.6530778390868646

Epoch: 6| Step: 3
Training loss: 2.982743861036639
Validation loss: 2.6554406281152905

Epoch: 6| Step: 4
Training loss: 3.258987178387488
Validation loss: 2.6509208655705554

Epoch: 6| Step: 5
Training loss: 3.3375487534751804
Validation loss: 2.6578820365654443

Epoch: 6| Step: 6
Training loss: 2.7304938100993685
Validation loss: 2.668071770481078

Epoch: 6| Step: 7
Training loss: 2.9635869152903176
Validation loss: 2.6752937262586762

Epoch: 6| Step: 8
Training loss: 3.0032224831850702
Validation loss: 2.691262803034646

Epoch: 6| Step: 9
Training loss: 3.2895820551638852
Validation loss: 2.68277566383909

Epoch: 6| Step: 10
Training loss: 2.861080640694378
Validation loss: 2.656507367882647

Epoch: 6| Step: 11
Training loss: 3.346661877369082
Validation loss: 2.6477454009553436

Epoch: 6| Step: 12
Training loss: 2.4540898071930455
Validation loss: 2.6488109604453642

Epoch: 6| Step: 13
Training loss: 2.7893173144113335
Validation loss: 2.6534982826807667

Epoch: 55| Step: 0
Training loss: 2.9091978812056976
Validation loss: 2.658171032561335

Epoch: 6| Step: 1
Training loss: 3.1460812367829125
Validation loss: 2.6600837947616514

Epoch: 6| Step: 2
Training loss: 2.714467501468441
Validation loss: 2.661689544209969

Epoch: 6| Step: 3
Training loss: 3.2553420545450895
Validation loss: 2.6587072039772086

Epoch: 6| Step: 4
Training loss: 2.942618597682306
Validation loss: 2.6604178846610878

Epoch: 6| Step: 5
Training loss: 3.0883272985436125
Validation loss: 2.6614893220797087

Epoch: 6| Step: 6
Training loss: 2.945496571098667
Validation loss: 2.6664295866646506

Epoch: 6| Step: 7
Training loss: 3.012204142244623
Validation loss: 2.66134284077343

Epoch: 6| Step: 8
Training loss: 2.767577694264919
Validation loss: 2.6577624334769867

Epoch: 6| Step: 9
Training loss: 3.1263534666661026
Validation loss: 2.654971443938698

Epoch: 6| Step: 10
Training loss: 3.1632514573472763
Validation loss: 2.6558960140075407

Epoch: 6| Step: 11
Training loss: 3.2388342740612686
Validation loss: 2.6492850332695865

Epoch: 6| Step: 12
Training loss: 2.99193012136568
Validation loss: 2.651556837744785

Epoch: 6| Step: 13
Training loss: 2.5667284991094927
Validation loss: 2.6486296646195027

Epoch: 56| Step: 0
Training loss: 2.980977786130143
Validation loss: 2.643030918958586

Epoch: 6| Step: 1
Training loss: 3.1578742654036485
Validation loss: 2.6465534460469313

Epoch: 6| Step: 2
Training loss: 2.891065739203873
Validation loss: 2.6450784090860107

Epoch: 6| Step: 3
Training loss: 3.2048750399838712
Validation loss: 2.6369289621578753

Epoch: 6| Step: 4
Training loss: 2.5273712015208805
Validation loss: 2.6442708957437677

Epoch: 6| Step: 5
Training loss: 3.337647904108418
Validation loss: 2.6463119898109078

Epoch: 6| Step: 6
Training loss: 3.1966488755426834
Validation loss: 2.6495264391759306

Epoch: 6| Step: 7
Training loss: 2.4704212374705765
Validation loss: 2.6497724399775473

Epoch: 6| Step: 8
Training loss: 2.415662710430909
Validation loss: 2.6539314405583703

Epoch: 6| Step: 9
Training loss: 3.919340249225216
Validation loss: 2.6564050077876864

Epoch: 6| Step: 10
Training loss: 3.0878864556126975
Validation loss: 2.647381233255657

Epoch: 6| Step: 11
Training loss: 2.7901046900095663
Validation loss: 2.643599351830137

Epoch: 6| Step: 12
Training loss: 2.564282518393198
Validation loss: 2.638021505174123

Epoch: 6| Step: 13
Training loss: 2.878623503383337
Validation loss: 2.639299333111285

Epoch: 57| Step: 0
Training loss: 3.4119208490687556
Validation loss: 2.6410176761325466

Epoch: 6| Step: 1
Training loss: 2.9904840864667936
Validation loss: 2.645155743286235

Epoch: 6| Step: 2
Training loss: 3.0051373363727367
Validation loss: 2.6497799453065953

Epoch: 6| Step: 3
Training loss: 3.399970048884694
Validation loss: 2.649313418826666

Epoch: 6| Step: 4
Training loss: 2.389133960760362
Validation loss: 2.6467182414660546

Epoch: 6| Step: 5
Training loss: 3.1268485133383437
Validation loss: 2.6405976608306707

Epoch: 6| Step: 6
Training loss: 2.1579158816201427
Validation loss: 2.636737621973607

Epoch: 6| Step: 7
Training loss: 3.1374141544589107
Validation loss: 2.6369466018215824

Epoch: 6| Step: 8
Training loss: 2.132501488670554
Validation loss: 2.6329212727970113

Epoch: 6| Step: 9
Training loss: 3.0459837563695875
Validation loss: 2.6374485098905986

Epoch: 6| Step: 10
Training loss: 3.23965568502573
Validation loss: 2.6571921311868976

Epoch: 6| Step: 11
Training loss: 2.9018020226994934
Validation loss: 2.698112155415182

Epoch: 6| Step: 12
Training loss: 3.476125213896293
Validation loss: 2.727514059845391

Epoch: 6| Step: 13
Training loss: 3.1663933351574767
Validation loss: 2.701840967008585

Epoch: 58| Step: 0
Training loss: 3.1175952921897654
Validation loss: 2.6764287962828712

Epoch: 6| Step: 1
Training loss: 2.4171109503650245
Validation loss: 2.647893791448141

Epoch: 6| Step: 2
Training loss: 3.212555489264963
Validation loss: 2.6268414846743737

Epoch: 6| Step: 3
Training loss: 3.535217352070364
Validation loss: 2.6328449979382893

Epoch: 6| Step: 4
Training loss: 3.1440004647135695
Validation loss: 2.6346796819416394

Epoch: 6| Step: 5
Training loss: 3.013975967422791
Validation loss: 2.6404607918002703

Epoch: 6| Step: 6
Training loss: 2.5595831668768216
Validation loss: 2.6473727977840773

Epoch: 6| Step: 7
Training loss: 3.087999644758767
Validation loss: 2.6514427313139093

Epoch: 6| Step: 8
Training loss: 2.4592184208390364
Validation loss: 2.6453839722184664

Epoch: 6| Step: 9
Training loss: 2.9372834470751905
Validation loss: 2.6433016216038996

Epoch: 6| Step: 10
Training loss: 3.3335167993282724
Validation loss: 2.641802542763873

Epoch: 6| Step: 11
Training loss: 2.2715903837177436
Validation loss: 2.6412411105890667

Epoch: 6| Step: 12
Training loss: 3.421832637437549
Validation loss: 2.63750103902512

Epoch: 6| Step: 13
Training loss: 3.0994826100417425
Validation loss: 2.642144919307562

Epoch: 59| Step: 0
Training loss: 2.6388102960316004
Validation loss: 2.635850609863861

Epoch: 6| Step: 1
Training loss: 2.8007412406159924
Validation loss: 2.6321223477144513

Epoch: 6| Step: 2
Training loss: 2.1410577712480756
Validation loss: 2.6374549096323956

Epoch: 6| Step: 3
Training loss: 3.3384280054003277
Validation loss: 2.65639268563069

Epoch: 6| Step: 4
Training loss: 2.976322514829378
Validation loss: 2.6805584961515887

Epoch: 6| Step: 5
Training loss: 2.3713821160367194
Validation loss: 2.690532231945414

Epoch: 6| Step: 6
Training loss: 3.4268010882223945
Validation loss: 2.7168515915562166

Epoch: 6| Step: 7
Training loss: 3.1442749677201953
Validation loss: 2.7164410970886794

Epoch: 6| Step: 8
Training loss: 3.308752567303234
Validation loss: 2.705233811937149

Epoch: 6| Step: 9
Training loss: 2.9940984216591047
Validation loss: 2.6912544679548196

Epoch: 6| Step: 10
Training loss: 3.418251941393271
Validation loss: 2.684049394821514

Epoch: 6| Step: 11
Training loss: 2.967420581503697
Validation loss: 2.6677930011281803

Epoch: 6| Step: 12
Training loss: 3.274051144423567
Validation loss: 2.6619815428124705

Epoch: 6| Step: 13
Training loss: 3.0517018744873345
Validation loss: 2.6483134966277824

Epoch: 60| Step: 0
Training loss: 3.6949906642522294
Validation loss: 2.6379833479742287

Epoch: 6| Step: 1
Training loss: 2.23090674914952
Validation loss: 2.624258955402268

Epoch: 6| Step: 2
Training loss: 3.309298137460216
Validation loss: 2.6254859089797513

Epoch: 6| Step: 3
Training loss: 2.6653632515122974
Validation loss: 2.619357797261696

Epoch: 6| Step: 4
Training loss: 2.700982275409299
Validation loss: 2.6195012084087397

Epoch: 6| Step: 5
Training loss: 2.607992754101195
Validation loss: 2.6243036736316294

Epoch: 6| Step: 6
Training loss: 3.4068185874174812
Validation loss: 2.625247148714753

Epoch: 6| Step: 7
Training loss: 3.154008276996867
Validation loss: 2.628309205150145

Epoch: 6| Step: 8
Training loss: 2.884322600544827
Validation loss: 2.6341226693704365

Epoch: 6| Step: 9
Training loss: 2.8031149363257373
Validation loss: 2.652275169982052

Epoch: 6| Step: 10
Training loss: 3.14662182423057
Validation loss: 2.6591161515050437

Epoch: 6| Step: 11
Training loss: 2.769907765694833
Validation loss: 2.67982085789533

Epoch: 6| Step: 12
Training loss: 3.0280837994145497
Validation loss: 2.7220083430244952

Epoch: 6| Step: 13
Training loss: 3.27678267321774
Validation loss: 2.7805423497675332

Epoch: 61| Step: 0
Training loss: 3.18367777520442
Validation loss: 2.7501667108279273

Epoch: 6| Step: 1
Training loss: 2.596920052515147
Validation loss: 2.7171338437684347

Epoch: 6| Step: 2
Training loss: 2.7080535059422557
Validation loss: 2.684877424311375

Epoch: 6| Step: 3
Training loss: 2.9259491122364825
Validation loss: 2.6491967123157596

Epoch: 6| Step: 4
Training loss: 3.3211651593600506
Validation loss: 2.635669714747996

Epoch: 6| Step: 5
Training loss: 3.3954604866650864
Validation loss: 2.628404569776391

Epoch: 6| Step: 6
Training loss: 2.8100415823361065
Validation loss: 2.631132881072317

Epoch: 6| Step: 7
Training loss: 3.5295335767332077
Validation loss: 2.637741643732885

Epoch: 6| Step: 8
Training loss: 2.792377371968497
Validation loss: 2.6370346904595463

Epoch: 6| Step: 9
Training loss: 2.671467889477483
Validation loss: 2.6306356181469965

Epoch: 6| Step: 10
Training loss: 3.3345925972465986
Validation loss: 2.6273135241350007

Epoch: 6| Step: 11
Training loss: 2.9030800215375043
Validation loss: 2.626493061592323

Epoch: 6| Step: 12
Training loss: 2.6499007692291943
Validation loss: 2.627862933634157

Epoch: 6| Step: 13
Training loss: 2.7748919044954743
Validation loss: 2.626761857469125

Epoch: 62| Step: 0
Training loss: 2.9150372631380543
Validation loss: 2.6238857693747386

Epoch: 6| Step: 1
Training loss: 3.085676660257869
Validation loss: 2.6219915494327015

Epoch: 6| Step: 2
Training loss: 3.3992781377683934
Validation loss: 2.6226332767858955

Epoch: 6| Step: 3
Training loss: 2.6807952459647777
Validation loss: 2.6209465427976997

Epoch: 6| Step: 4
Training loss: 2.694686684310278
Validation loss: 2.6175374800633353

Epoch: 6| Step: 5
Training loss: 3.028039707049474
Validation loss: 2.6187457169080153

Epoch: 6| Step: 6
Training loss: 3.0753523834033216
Validation loss: 2.617244647716404

Epoch: 6| Step: 7
Training loss: 2.665828175644587
Validation loss: 2.619259459607716

Epoch: 6| Step: 8
Training loss: 3.2123058215877034
Validation loss: 2.620958585084202

Epoch: 6| Step: 9
Training loss: 3.188795237951173
Validation loss: 2.6159090718192926

Epoch: 6| Step: 10
Training loss: 3.019127111790903
Validation loss: 2.61744912017964

Epoch: 6| Step: 11
Training loss: 2.747603499291491
Validation loss: 2.6085484815371025

Epoch: 6| Step: 12
Training loss: 3.0776546525561295
Validation loss: 2.6053706939970716

Epoch: 6| Step: 13
Training loss: 2.726764310784374
Validation loss: 2.6043146189017747

Epoch: 63| Step: 0
Training loss: 3.3786769193761192
Validation loss: 2.609523674394816

Epoch: 6| Step: 1
Training loss: 2.334586987451429
Validation loss: 2.6115064030667186

Epoch: 6| Step: 2
Training loss: 3.4173102044156196
Validation loss: 2.611923868081859

Epoch: 6| Step: 3
Training loss: 3.2767214087579664
Validation loss: 2.610043573722605

Epoch: 6| Step: 4
Training loss: 2.9114551487304157
Validation loss: 2.6123907179912993

Epoch: 6| Step: 5
Training loss: 2.9496358614391522
Validation loss: 2.6054443736069968

Epoch: 6| Step: 6
Training loss: 2.8930706503792707
Validation loss: 2.6033685671273763

Epoch: 6| Step: 7
Training loss: 2.803967481759271
Validation loss: 2.6035257416255595

Epoch: 6| Step: 8
Training loss: 3.074531122451813
Validation loss: 2.6089336003999684

Epoch: 6| Step: 9
Training loss: 3.2312854993629943
Validation loss: 2.607580920918444

Epoch: 6| Step: 10
Training loss: 2.6283716618999327
Validation loss: 2.6088136507961455

Epoch: 6| Step: 11
Training loss: 2.726683343433719
Validation loss: 2.6116860319209447

Epoch: 6| Step: 12
Training loss: 2.820492133762729
Validation loss: 2.6157535097553835

Epoch: 6| Step: 13
Training loss: 2.8987396987528027
Validation loss: 2.6236847054591057

Epoch: 64| Step: 0
Training loss: 2.9628896593810228
Validation loss: 2.6205739846426055

Epoch: 6| Step: 1
Training loss: 3.298990881401658
Validation loss: 2.630723613086954

Epoch: 6| Step: 2
Training loss: 3.0413276448590323
Validation loss: 2.6177423840637633

Epoch: 6| Step: 3
Training loss: 2.5477381454729504
Validation loss: 2.604410128790275

Epoch: 6| Step: 4
Training loss: 2.8071276747176155
Validation loss: 2.607725507653581

Epoch: 6| Step: 5
Training loss: 3.1669613215906463
Validation loss: 2.6014257196325703

Epoch: 6| Step: 6
Training loss: 3.4375650573122822
Validation loss: 2.597772196938375

Epoch: 6| Step: 7
Training loss: 3.2329900605237425
Validation loss: 2.5985853479239718

Epoch: 6| Step: 8
Training loss: 2.7204800067230748
Validation loss: 2.597010807786314

Epoch: 6| Step: 9
Training loss: 3.7003240494974303
Validation loss: 2.602163806835009

Epoch: 6| Step: 10
Training loss: 2.883316946993738
Validation loss: 2.6005754042754408

Epoch: 6| Step: 11
Training loss: 2.3557897953111993
Validation loss: 2.600366804123463

Epoch: 6| Step: 12
Training loss: 2.535161892689213
Validation loss: 2.6091457367175526

Epoch: 6| Step: 13
Training loss: 1.8518007020597345
Validation loss: 2.610397436244979

Epoch: 65| Step: 0
Training loss: 2.875521405249973
Validation loss: 2.6214506018453863

Epoch: 6| Step: 1
Training loss: 2.536797554993692
Validation loss: 2.613453700974991

Epoch: 6| Step: 2
Training loss: 2.702555866634448
Validation loss: 2.615248375205069

Epoch: 6| Step: 3
Training loss: 3.595503934681691
Validation loss: 2.6049668498249843

Epoch: 6| Step: 4
Training loss: 2.799843007182396
Validation loss: 2.603101932218729

Epoch: 6| Step: 5
Training loss: 3.08595696938083
Validation loss: 2.597766463268368

Epoch: 6| Step: 6
Training loss: 3.154480542011505
Validation loss: 2.5976430185273847

Epoch: 6| Step: 7
Training loss: 2.7516142702187625
Validation loss: 2.592015360087367

Epoch: 6| Step: 8
Training loss: 2.7693278207033796
Validation loss: 2.59396692904074

Epoch: 6| Step: 9
Training loss: 2.9576531465530134
Validation loss: 2.592271350964475

Epoch: 6| Step: 10
Training loss: 2.8093357512009978
Validation loss: 2.595432775899694

Epoch: 6| Step: 11
Training loss: 2.971677601512988
Validation loss: 2.5914229623813974

Epoch: 6| Step: 12
Training loss: 2.8995938115278466
Validation loss: 2.5992623750420223

Epoch: 6| Step: 13
Training loss: 3.50652563198267
Validation loss: 2.6005871101010682

Epoch: 66| Step: 0
Training loss: 2.992305743318858
Validation loss: 2.610219470878196

Epoch: 6| Step: 1
Training loss: 2.978555007424066
Validation loss: 2.6030801651793647

Epoch: 6| Step: 2
Training loss: 3.2487050924686947
Validation loss: 2.6098647492808658

Epoch: 6| Step: 3
Training loss: 2.2898129219366945
Validation loss: 2.6315606476925213

Epoch: 6| Step: 4
Training loss: 3.1851202365092135
Validation loss: 2.6384474019282877

Epoch: 6| Step: 5
Training loss: 3.1926736171177055
Validation loss: 2.640981762833544

Epoch: 6| Step: 6
Training loss: 3.0161152969805674
Validation loss: 2.6430198788233965

Epoch: 6| Step: 7
Training loss: 3.5283743703477186
Validation loss: 2.6455405828051997

Epoch: 6| Step: 8
Training loss: 2.752575621852398
Validation loss: 2.6144497035719323

Epoch: 6| Step: 9
Training loss: 3.128916155814922
Validation loss: 2.6016843142067967

Epoch: 6| Step: 10
Training loss: 2.6328209217514136
Validation loss: 2.582530295458735

Epoch: 6| Step: 11
Training loss: 3.219154406576949
Validation loss: 2.5879303192171577

Epoch: 6| Step: 12
Training loss: 2.147420524574261
Validation loss: 2.588222016879066

Epoch: 6| Step: 13
Training loss: 2.8427053356236387
Validation loss: 2.582593870232775

Epoch: 67| Step: 0
Training loss: 3.1955282795991296
Validation loss: 2.600918238091944

Epoch: 6| Step: 1
Training loss: 2.711010231119773
Validation loss: 2.5807365249411833

Epoch: 6| Step: 2
Training loss: 2.857849047444339
Validation loss: 2.5840912526458704

Epoch: 6| Step: 3
Training loss: 2.5789355304376667
Validation loss: 2.5961519267210647

Epoch: 6| Step: 4
Training loss: 2.555713599150932
Validation loss: 2.6131740387252935

Epoch: 6| Step: 5
Training loss: 3.259464275128636
Validation loss: 2.596089927229582

Epoch: 6| Step: 6
Training loss: 2.613206802470954
Validation loss: 2.5856841707749

Epoch: 6| Step: 7
Training loss: 3.627651560250837
Validation loss: 2.5843833955244335

Epoch: 6| Step: 8
Training loss: 2.8996572883940472
Validation loss: 2.587064561808785

Epoch: 6| Step: 9
Training loss: 2.796020803595167
Validation loss: 2.5878771306106123

Epoch: 6| Step: 10
Training loss: 2.696691188095633
Validation loss: 2.5931528874103362

Epoch: 6| Step: 11
Training loss: 2.952386026920112
Validation loss: 2.5914885972521784

Epoch: 6| Step: 12
Training loss: 3.395895382054903
Validation loss: 2.5944512619292377

Epoch: 6| Step: 13
Training loss: 2.9824818303465266
Validation loss: 2.5914737247638526

Epoch: 68| Step: 0
Training loss: 2.6739583955494055
Validation loss: 2.5936335836398747

Epoch: 6| Step: 1
Training loss: 3.0374329422193016
Validation loss: 2.5893317495578128

Epoch: 6| Step: 2
Training loss: 3.8688724430656656
Validation loss: 2.5865030974129097

Epoch: 6| Step: 3
Training loss: 2.798029485507617
Validation loss: 2.5904417649073213

Epoch: 6| Step: 4
Training loss: 2.7693229134219273
Validation loss: 2.5888730725832723

Epoch: 6| Step: 5
Training loss: 2.6155515197860333
Validation loss: 2.585436987272532

Epoch: 6| Step: 6
Training loss: 2.463878315565189
Validation loss: 2.5860565161132807

Epoch: 6| Step: 7
Training loss: 2.363267428775442
Validation loss: 2.581992389476564

Epoch: 6| Step: 8
Training loss: 2.8913490831917934
Validation loss: 2.578925094677049

Epoch: 6| Step: 9
Training loss: 2.679546919713534
Validation loss: 2.5895193145438857

Epoch: 6| Step: 10
Training loss: 3.09041130167973
Validation loss: 2.5968909659321286

Epoch: 6| Step: 11
Training loss: 2.5292735919838107
Validation loss: 2.6066433610664492

Epoch: 6| Step: 12
Training loss: 3.653790551507723
Validation loss: 2.6107835647847404

Epoch: 6| Step: 13
Training loss: 3.809011896116348
Validation loss: 2.594580248484007

Epoch: 69| Step: 0
Training loss: 2.720996410940902
Validation loss: 2.580102311805525

Epoch: 6| Step: 1
Training loss: 2.80224383909617
Validation loss: 2.5772644406557648

Epoch: 6| Step: 2
Training loss: 2.862993129582078
Validation loss: 2.5806555854166393

Epoch: 6| Step: 3
Training loss: 2.1781757540378863
Validation loss: 2.58375487988363

Epoch: 6| Step: 4
Training loss: 2.8716309751955658
Validation loss: 2.5828813815128817

Epoch: 6| Step: 5
Training loss: 3.066151524551386
Validation loss: 2.5803664620326554

Epoch: 6| Step: 6
Training loss: 2.6939511606820448
Validation loss: 2.578156918464529

Epoch: 6| Step: 7
Training loss: 2.688821246038076
Validation loss: 2.577762882360893

Epoch: 6| Step: 8
Training loss: 2.9101387177009634
Validation loss: 2.5805975211332526

Epoch: 6| Step: 9
Training loss: 3.5290889372629612
Validation loss: 2.5811340104554565

Epoch: 6| Step: 10
Training loss: 2.7148665831998438
Validation loss: 2.5806908113789935

Epoch: 6| Step: 11
Training loss: 3.750867361849326
Validation loss: 2.5879220515374746

Epoch: 6| Step: 12
Training loss: 2.9690447209280304
Validation loss: 2.5860221027844905

Epoch: 6| Step: 13
Training loss: 3.3034385683893186
Validation loss: 2.5808446853810105

Epoch: 70| Step: 0
Training loss: 2.6246304933152835
Validation loss: 2.5723584719488706

Epoch: 6| Step: 1
Training loss: 2.83980966413734
Validation loss: 2.573141738740833

Epoch: 6| Step: 2
Training loss: 3.136988114018004
Validation loss: 2.572204485465864

Epoch: 6| Step: 3
Training loss: 3.0793919158712493
Validation loss: 2.5732066468632797

Epoch: 6| Step: 4
Training loss: 2.469600193083483
Validation loss: 2.5783122551843785

Epoch: 6| Step: 5
Training loss: 3.3437290369250148
Validation loss: 2.5790046093762404

Epoch: 6| Step: 6
Training loss: 2.457364640413007
Validation loss: 2.5963080441351725

Epoch: 6| Step: 7
Training loss: 2.6448084631005018
Validation loss: 2.609872873778006

Epoch: 6| Step: 8
Training loss: 3.0678682571633593
Validation loss: 2.6092056898374216

Epoch: 6| Step: 9
Training loss: 3.306147692418915
Validation loss: 2.5706480886670273

Epoch: 6| Step: 10
Training loss: 2.6238989564467947
Validation loss: 2.5688222761739423

Epoch: 6| Step: 11
Training loss: 3.1740004460431215
Validation loss: 2.5771020892893852

Epoch: 6| Step: 12
Training loss: 3.190538342052854
Validation loss: 2.5813395745176444

Epoch: 6| Step: 13
Training loss: 2.9851482248485426
Validation loss: 2.588725426526663

Epoch: 71| Step: 0
Training loss: 2.7812244542695317
Validation loss: 2.599435827061646

Epoch: 6| Step: 1
Training loss: 2.445024369787295
Validation loss: 2.6005709750836683

Epoch: 6| Step: 2
Training loss: 3.1692698637251153
Validation loss: 2.6019725743048574

Epoch: 6| Step: 3
Training loss: 3.05639163822168
Validation loss: 2.5936435826385162

Epoch: 6| Step: 4
Training loss: 3.197439218141752
Validation loss: 2.599533853479919

Epoch: 6| Step: 5
Training loss: 3.190656856486182
Validation loss: 2.596063522313927

Epoch: 6| Step: 6
Training loss: 3.046809112618937
Validation loss: 2.58999810369551

Epoch: 6| Step: 7
Training loss: 3.01837538770099
Validation loss: 2.589971148685399

Epoch: 6| Step: 8
Training loss: 2.9951963112612776
Validation loss: 2.5833314163910117

Epoch: 6| Step: 9
Training loss: 2.660975930725445
Validation loss: 2.5786829717832838

Epoch: 6| Step: 10
Training loss: 2.5409752786117603
Validation loss: 2.576392064244931

Epoch: 6| Step: 11
Training loss: 3.25416269013133
Validation loss: 2.5653193241421897

Epoch: 6| Step: 12
Training loss: 3.0655390393943414
Validation loss: 2.5634139177253448

Epoch: 6| Step: 13
Training loss: 2.7773433207794587
Validation loss: 2.569347195464946

Epoch: 72| Step: 0
Training loss: 2.455476642029156
Validation loss: 2.5937659624747424

Epoch: 6| Step: 1
Training loss: 3.493838881998112
Validation loss: 2.6304552337416083

Epoch: 6| Step: 2
Training loss: 2.301075940778464
Validation loss: 2.6151912269983653

Epoch: 6| Step: 3
Training loss: 2.323427722861595
Validation loss: 2.6038611425201523

Epoch: 6| Step: 4
Training loss: 3.3460753724250174
Validation loss: 2.5933275181394353

Epoch: 6| Step: 5
Training loss: 2.84456731767636
Validation loss: 2.60092702727439

Epoch: 6| Step: 6
Training loss: 3.4668529925035556
Validation loss: 2.569704550101592

Epoch: 6| Step: 7
Training loss: 3.469167048496797
Validation loss: 2.5642137436904613

Epoch: 6| Step: 8
Training loss: 2.738893187491358
Validation loss: 2.5665724043709344

Epoch: 6| Step: 9
Training loss: 3.085045948891994
Validation loss: 2.5675677358955498

Epoch: 6| Step: 10
Training loss: 2.8879570273163653
Validation loss: 2.563463992664715

Epoch: 6| Step: 11
Training loss: 2.911551285808405
Validation loss: 2.565025994435124

Epoch: 6| Step: 12
Training loss: 2.612641442955432
Validation loss: 2.5688372069523147

Epoch: 6| Step: 13
Training loss: 2.6051849713981876
Validation loss: 2.5678169939533597

Epoch: 73| Step: 0
Training loss: 2.460017537930628
Validation loss: 2.5722214083789625

Epoch: 6| Step: 1
Training loss: 2.9772770542110685
Validation loss: 2.5729560491995276

Epoch: 6| Step: 2
Training loss: 2.9067308581843947
Validation loss: 2.5715312953726035

Epoch: 6| Step: 3
Training loss: 3.2649228742057166
Validation loss: 2.5723012150156186

Epoch: 6| Step: 4
Training loss: 2.891388003712524
Validation loss: 2.5777873653751717

Epoch: 6| Step: 5
Training loss: 3.58965512715146
Validation loss: 2.5727943974913194

Epoch: 6| Step: 6
Training loss: 3.1217303146116784
Validation loss: 2.5746554409293947

Epoch: 6| Step: 7
Training loss: 3.0425265067959004
Validation loss: 2.573302518832365

Epoch: 6| Step: 8
Training loss: 3.22713880957114
Validation loss: 2.5720074605571863

Epoch: 6| Step: 9
Training loss: 2.52207584077697
Validation loss: 2.5718602467845937

Epoch: 6| Step: 10
Training loss: 2.8413768549676406
Validation loss: 2.5730983063376636

Epoch: 6| Step: 11
Training loss: 2.6537205040146437
Validation loss: 2.5704794799971933

Epoch: 6| Step: 12
Training loss: 2.804954749845644
Validation loss: 2.5748572359652346

Epoch: 6| Step: 13
Training loss: 2.5028118532970374
Validation loss: 2.5805200950891027

Epoch: 74| Step: 0
Training loss: 3.4717876433636885
Validation loss: 2.5824408370538063

Epoch: 6| Step: 1
Training loss: 2.478961156026035
Validation loss: 2.5812526769707858

Epoch: 6| Step: 2
Training loss: 2.933397390648741
Validation loss: 2.5930105915501445

Epoch: 6| Step: 3
Training loss: 3.311731933221947
Validation loss: 2.6253820357143196

Epoch: 6| Step: 4
Training loss: 3.443920589590414
Validation loss: 2.6324404126176533

Epoch: 6| Step: 5
Training loss: 2.920422524491553
Validation loss: 2.62814755326865

Epoch: 6| Step: 6
Training loss: 3.345843363961037
Validation loss: 2.6325211710519176

Epoch: 6| Step: 7
Training loss: 3.1611646403730576
Validation loss: 2.659046692372254

Epoch: 6| Step: 8
Training loss: 2.8045750332440647
Validation loss: 2.6248138232797453

Epoch: 6| Step: 9
Training loss: 2.230149762380653
Validation loss: 2.5828897397685164

Epoch: 6| Step: 10
Training loss: 2.6051493710619424
Validation loss: 2.5601903048997046

Epoch: 6| Step: 11
Training loss: 2.5572743981756627
Validation loss: 2.5573429124001272

Epoch: 6| Step: 12
Training loss: 2.4132082517184688
Validation loss: 2.552799141712949

Epoch: 6| Step: 13
Training loss: 3.142081610869653
Validation loss: 2.5530020357115832

Epoch: 75| Step: 0
Training loss: 3.120992303185474
Validation loss: 2.5557047678283773

Epoch: 6| Step: 1
Training loss: 2.310726413539158
Validation loss: 2.5584610864005852

Epoch: 6| Step: 2
Training loss: 2.838441022202758
Validation loss: 2.5569833859938003

Epoch: 6| Step: 3
Training loss: 2.704728549445227
Validation loss: 2.5567084292425557

Epoch: 6| Step: 4
Training loss: 3.382680841927705
Validation loss: 2.559132934760031

Epoch: 6| Step: 5
Training loss: 2.723330968282257
Validation loss: 2.5601302944309596

Epoch: 6| Step: 6
Training loss: 2.859741187489495
Validation loss: 2.5668495454534352

Epoch: 6| Step: 7
Training loss: 3.476141400497811
Validation loss: 2.5757117976044315

Epoch: 6| Step: 8
Training loss: 3.2410557253718766
Validation loss: 2.5677520579165196

Epoch: 6| Step: 9
Training loss: 3.141765026977863
Validation loss: 2.586730505310726

Epoch: 6| Step: 10
Training loss: 3.227044833803057
Validation loss: 2.5989306893517288

Epoch: 6| Step: 11
Training loss: 2.7632406700375043
Validation loss: 2.6084165541431665

Epoch: 6| Step: 12
Training loss: 2.4714189419767494
Validation loss: 2.616961670953772

Epoch: 6| Step: 13
Training loss: 1.7220606044423392
Validation loss: 2.57901394143197

Testing loss: 2.7832674479348314

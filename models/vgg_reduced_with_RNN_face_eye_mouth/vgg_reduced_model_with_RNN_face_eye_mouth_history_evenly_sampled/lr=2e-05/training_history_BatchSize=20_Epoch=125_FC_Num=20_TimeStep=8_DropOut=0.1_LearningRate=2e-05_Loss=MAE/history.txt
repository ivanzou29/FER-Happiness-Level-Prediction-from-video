Epoch: 1| Step: 0
Training loss: 5.821118354797363
Validation loss: 5.171620994485835

Epoch: 5| Step: 1
Training loss: 3.7571423053741455
Validation loss: 5.156280122777467

Epoch: 5| Step: 2
Training loss: 3.6057639122009277
Validation loss: 5.142458244036603

Epoch: 5| Step: 3
Training loss: 5.320666313171387
Validation loss: 5.127105902600032

Epoch: 5| Step: 4
Training loss: 4.856647491455078
Validation loss: 5.109121886632776

Epoch: 5| Step: 5
Training loss: 6.1820759773254395
Validation loss: 5.087461128029772

Epoch: 5| Step: 6
Training loss: 5.552086353302002
Validation loss: 5.062971130494149

Epoch: 5| Step: 7
Training loss: 4.747187614440918
Validation loss: 5.034851863820066

Epoch: 5| Step: 8
Training loss: 4.296922206878662
Validation loss: 5.002860828112531

Epoch: 5| Step: 9
Training loss: 4.529594421386719
Validation loss: 4.967101702126124

Epoch: 5| Step: 10
Training loss: 4.907284736633301
Validation loss: 4.926955822975405

Epoch: 2| Step: 0
Training loss: 4.261237621307373
Validation loss: 4.882525156903011

Epoch: 5| Step: 1
Training loss: 4.783304214477539
Validation loss: 4.833036627820743

Epoch: 5| Step: 2
Training loss: 4.3899617195129395
Validation loss: 4.779497767007479

Epoch: 5| Step: 3
Training loss: 4.571591854095459
Validation loss: 4.723173895189839

Epoch: 5| Step: 4
Training loss: 4.86065673828125
Validation loss: 4.662351885149556

Epoch: 5| Step: 5
Training loss: 3.262019395828247
Validation loss: 4.601713759924776

Epoch: 5| Step: 6
Training loss: 3.9807045459747314
Validation loss: 4.537351877458634

Epoch: 5| Step: 7
Training loss: 4.583701133728027
Validation loss: 4.473213452164845

Epoch: 5| Step: 8
Training loss: 4.4087724685668945
Validation loss: 4.4081874509011545

Epoch: 5| Step: 9
Training loss: 4.988734245300293
Validation loss: 4.34487549976636

Epoch: 5| Step: 10
Training loss: 4.271915912628174
Validation loss: 4.28039421830126

Epoch: 3| Step: 0
Training loss: 4.262244701385498
Validation loss: 4.222894799324774

Epoch: 5| Step: 1
Training loss: 3.907768964767456
Validation loss: 4.161413233767274

Epoch: 5| Step: 2
Training loss: 4.6954522132873535
Validation loss: 4.101047515869141

Epoch: 5| Step: 3
Training loss: 3.954746723175049
Validation loss: 4.041184533026911

Epoch: 5| Step: 4
Training loss: 3.8734524250030518
Validation loss: 3.983602482785461

Epoch: 5| Step: 5
Training loss: 3.7818236351013184
Validation loss: 3.936141901118781

Epoch: 5| Step: 6
Training loss: 2.5430848598480225
Validation loss: 3.8917228739748717

Epoch: 5| Step: 7
Training loss: 2.833932399749756
Validation loss: 3.855421737958026

Epoch: 5| Step: 8
Training loss: 4.094555854797363
Validation loss: 3.8164329118626092

Epoch: 5| Step: 9
Training loss: 3.6097445487976074
Validation loss: 3.784080630989485

Epoch: 5| Step: 10
Training loss: 4.500422477722168
Validation loss: 3.754944121965798

Epoch: 4| Step: 0
Training loss: 4.0040974617004395
Validation loss: 3.7305807605866463

Epoch: 5| Step: 1
Training loss: 4.293202877044678
Validation loss: 3.7102621037472963

Epoch: 5| Step: 2
Training loss: 3.6507644653320312
Validation loss: 3.692332729216545

Epoch: 5| Step: 3
Training loss: 2.3829808235168457
Validation loss: 3.6746106916858303

Epoch: 5| Step: 4
Training loss: 3.9860477447509766
Validation loss: 3.663904854046401

Epoch: 5| Step: 5
Training loss: 4.007558345794678
Validation loss: 3.653921337537868

Epoch: 5| Step: 6
Training loss: 3.50299334526062
Validation loss: 3.64586018746899

Epoch: 5| Step: 7
Training loss: 2.7824203968048096
Validation loss: 3.62726564304803

Epoch: 5| Step: 8
Training loss: 3.8870041370391846
Validation loss: 3.611739602140201

Epoch: 5| Step: 9
Training loss: 3.3280093669891357
Validation loss: 3.5943736363482732

Epoch: 5| Step: 10
Training loss: 3.354271411895752
Validation loss: 3.5770113827079855

Epoch: 5| Step: 0
Training loss: 3.601872205734253
Validation loss: 3.562080898592549

Epoch: 5| Step: 1
Training loss: 3.1173758506774902
Validation loss: 3.5503411318666194

Epoch: 5| Step: 2
Training loss: 3.2243905067443848
Validation loss: 3.540205119758524

Epoch: 5| Step: 3
Training loss: 2.8990137577056885
Validation loss: 3.5288024717761624

Epoch: 5| Step: 4
Training loss: 3.8150787353515625
Validation loss: 3.5182979132539485

Epoch: 5| Step: 5
Training loss: 3.207362651824951
Validation loss: 3.5051806203780638

Epoch: 5| Step: 6
Training loss: 3.8909053802490234
Validation loss: 3.49478466023681

Epoch: 5| Step: 7
Training loss: 3.6624743938446045
Validation loss: 3.4839092095692954

Epoch: 5| Step: 8
Training loss: 2.768385410308838
Validation loss: 3.4695258191836778

Epoch: 5| Step: 9
Training loss: 3.9618308544158936
Validation loss: 3.4566552382643505

Epoch: 5| Step: 10
Training loss: 3.706266403198242
Validation loss: 3.447470667541668

Epoch: 6| Step: 0
Training loss: 2.872983455657959
Validation loss: 3.43858140771107

Epoch: 5| Step: 1
Training loss: 3.1227571964263916
Validation loss: 3.428940652519144

Epoch: 5| Step: 2
Training loss: 4.102269172668457
Validation loss: 3.417121107860278

Epoch: 5| Step: 3
Training loss: 4.397668361663818
Validation loss: 3.4078746559799358

Epoch: 5| Step: 4
Training loss: 2.8258583545684814
Validation loss: 3.3956139420950286

Epoch: 5| Step: 5
Training loss: 3.9673590660095215
Validation loss: 3.3891958267458024

Epoch: 5| Step: 6
Training loss: 2.7373464107513428
Validation loss: 3.3799488800828175

Epoch: 5| Step: 7
Training loss: 3.1548776626586914
Validation loss: 3.369177120988087

Epoch: 5| Step: 8
Training loss: 3.36950945854187
Validation loss: 3.356870687136086

Epoch: 5| Step: 9
Training loss: 3.2190558910369873
Validation loss: 3.3459451813851633

Epoch: 5| Step: 10
Training loss: 2.9346940517425537
Validation loss: 3.3368055769192275

Epoch: 7| Step: 0
Training loss: 3.4078075885772705
Validation loss: 3.3247958511434574

Epoch: 5| Step: 1
Training loss: 2.9647128582000732
Validation loss: 3.311985990052582

Epoch: 5| Step: 2
Training loss: 3.7912068367004395
Validation loss: 3.304112829187865

Epoch: 5| Step: 3
Training loss: 3.6499695777893066
Validation loss: 3.2981036170836417

Epoch: 5| Step: 4
Training loss: 2.7955479621887207
Validation loss: 3.281017482921641

Epoch: 5| Step: 5
Training loss: 3.355971574783325
Validation loss: 3.2716948140052056

Epoch: 5| Step: 6
Training loss: 2.4684576988220215
Validation loss: 3.2670529324521302

Epoch: 5| Step: 7
Training loss: 2.79487943649292
Validation loss: 3.2578762833790114

Epoch: 5| Step: 8
Training loss: 3.2016987800598145
Validation loss: 3.2451626536666707

Epoch: 5| Step: 9
Training loss: 3.969399929046631
Validation loss: 3.2379969243080384

Epoch: 5| Step: 10
Training loss: 3.4473702907562256
Validation loss: 3.2286478524566977

Epoch: 8| Step: 0
Training loss: 3.6103692054748535
Validation loss: 3.219737386190763

Epoch: 5| Step: 1
Training loss: 4.2362060546875
Validation loss: 3.2105104897611882

Epoch: 5| Step: 2
Training loss: 3.221850872039795
Validation loss: 3.209195654879334

Epoch: 5| Step: 3
Training loss: 2.730670213699341
Validation loss: 3.2002039750417075

Epoch: 5| Step: 4
Training loss: 2.9190616607666016
Validation loss: 3.1935548756712224

Epoch: 5| Step: 5
Training loss: 3.1509385108947754
Validation loss: 3.181602460081859

Epoch: 5| Step: 6
Training loss: 3.018474578857422
Validation loss: 3.1748300931786977

Epoch: 5| Step: 7
Training loss: 2.695472240447998
Validation loss: 3.177041471645396

Epoch: 5| Step: 8
Training loss: 3.2185072898864746
Validation loss: 3.1657568921325026

Epoch: 5| Step: 9
Training loss: 3.424689531326294
Validation loss: 3.1492396323911604

Epoch: 5| Step: 10
Training loss: 2.7338786125183105
Validation loss: 3.148792113027265

Epoch: 9| Step: 0
Training loss: 2.8122105598449707
Validation loss: 3.1443257126756894

Epoch: 5| Step: 1
Training loss: 3.1226046085357666
Validation loss: 3.148315452760266

Epoch: 5| Step: 2
Training loss: 3.6769185066223145
Validation loss: 3.1387717364936747

Epoch: 5| Step: 3
Training loss: 2.168875217437744
Validation loss: 3.1184983509843067

Epoch: 5| Step: 4
Training loss: 2.755397081375122
Validation loss: 3.126465235987017

Epoch: 5| Step: 5
Training loss: 3.80108380317688
Validation loss: 3.1247958598598355

Epoch: 5| Step: 6
Training loss: 3.0159080028533936
Validation loss: 3.1230671610883487

Epoch: 5| Step: 7
Training loss: 3.06107759475708
Validation loss: 3.1083345951572543

Epoch: 5| Step: 8
Training loss: 2.8077569007873535
Validation loss: 3.0940687451311337

Epoch: 5| Step: 9
Training loss: 3.6020705699920654
Validation loss: 3.0908950887700564

Epoch: 5| Step: 10
Training loss: 3.747180223464966
Validation loss: 3.087512121405653

Epoch: 10| Step: 0
Training loss: 2.6479852199554443
Validation loss: 3.0725512017485914

Epoch: 5| Step: 1
Training loss: 2.993075132369995
Validation loss: 3.06972901539136

Epoch: 5| Step: 2
Training loss: 2.8760972023010254
Validation loss: 3.0710012066748833

Epoch: 5| Step: 3
Training loss: 3.121105432510376
Validation loss: 3.065566237254809

Epoch: 5| Step: 4
Training loss: 3.2200558185577393
Validation loss: 3.058070041800058

Epoch: 5| Step: 5
Training loss: 3.42018461227417
Validation loss: 3.0412754038328766

Epoch: 5| Step: 6
Training loss: 3.0418710708618164
Validation loss: 3.03894385727503

Epoch: 5| Step: 7
Training loss: 2.901176929473877
Validation loss: 3.046375295167328

Epoch: 5| Step: 8
Training loss: 3.1125710010528564
Validation loss: 3.045129332491147

Epoch: 5| Step: 9
Training loss: 2.9567456245422363
Validation loss: 3.020656167819936

Epoch: 5| Step: 10
Training loss: 3.886185646057129
Validation loss: 3.021928951304446

Epoch: 11| Step: 0
Training loss: 3.260746717453003
Validation loss: 3.023872995889315

Epoch: 5| Step: 1
Training loss: 2.480450391769409
Validation loss: 3.020394707238802

Epoch: 5| Step: 2
Training loss: 3.436436414718628
Validation loss: 3.0222227778486026

Epoch: 5| Step: 3
Training loss: 3.224031925201416
Validation loss: 3.0184881840982745

Epoch: 5| Step: 4
Training loss: 2.998446464538574
Validation loss: 3.0042033554405294

Epoch: 5| Step: 5
Training loss: 2.670189142227173
Validation loss: 2.986141038197343

Epoch: 5| Step: 6
Training loss: 3.1662533283233643
Validation loss: 2.985504432391095

Epoch: 5| Step: 7
Training loss: 2.8336493968963623
Validation loss: 2.9826758164231495

Epoch: 5| Step: 8
Training loss: 3.3007988929748535
Validation loss: 2.970829768847394

Epoch: 5| Step: 9
Training loss: 2.520538806915283
Validation loss: 2.9631172431412565

Epoch: 5| Step: 10
Training loss: 3.87369441986084
Validation loss: 2.9710283869056293

Epoch: 12| Step: 0
Training loss: 2.6282131671905518
Validation loss: 2.978465408407232

Epoch: 5| Step: 1
Training loss: 3.2678470611572266
Validation loss: 2.9548168695101173

Epoch: 5| Step: 2
Training loss: 3.052501678466797
Validation loss: 2.9473357662077873

Epoch: 5| Step: 3
Training loss: 2.4417450428009033
Validation loss: 2.946124105043309

Epoch: 5| Step: 4
Training loss: 3.1987690925598145
Validation loss: 2.942800655159899

Epoch: 5| Step: 5
Training loss: 3.7254269123077393
Validation loss: 2.9455807747379428

Epoch: 5| Step: 6
Training loss: 2.5730807781219482
Validation loss: 2.9343674695619972

Epoch: 5| Step: 7
Training loss: 3.7260754108428955
Validation loss: 2.9317084230402464

Epoch: 5| Step: 8
Training loss: 3.037783145904541
Validation loss: 2.9298955907103834

Epoch: 5| Step: 9
Training loss: 2.333683729171753
Validation loss: 2.931507559232814

Epoch: 5| Step: 10
Training loss: 3.343600034713745
Validation loss: 2.927499358372022

Epoch: 13| Step: 0
Training loss: 2.8897018432617188
Validation loss: 2.9176611772147556

Epoch: 5| Step: 1
Training loss: 3.6843814849853516
Validation loss: 2.9186603946070515

Epoch: 5| Step: 2
Training loss: 3.4700546264648438
Validation loss: 2.9198110641971713

Epoch: 5| Step: 3
Training loss: 2.909365177154541
Validation loss: 2.915654218325051

Epoch: 5| Step: 4
Training loss: 2.657921552658081
Validation loss: 2.909459916494226

Epoch: 5| Step: 5
Training loss: 2.4602303504943848
Validation loss: 2.9078602457559235

Epoch: 5| Step: 6
Training loss: 2.6372146606445312
Validation loss: 2.901454843500609

Epoch: 5| Step: 7
Training loss: 2.7595901489257812
Validation loss: 2.8954479207274733

Epoch: 5| Step: 8
Training loss: 3.2294349670410156
Validation loss: 2.8923782481942126

Epoch: 5| Step: 9
Training loss: 3.244176149368286
Validation loss: 2.8905823102561374

Epoch: 5| Step: 10
Training loss: 3.0866692066192627
Validation loss: 2.8906233105608212

Epoch: 14| Step: 0
Training loss: 2.762586832046509
Validation loss: 2.8934138744108138

Epoch: 5| Step: 1
Training loss: 3.7209858894348145
Validation loss: 2.887342247911679

Epoch: 5| Step: 2
Training loss: 2.577536106109619
Validation loss: 2.880889413177326

Epoch: 5| Step: 3
Training loss: 3.367950439453125
Validation loss: 2.886060742921727

Epoch: 5| Step: 4
Training loss: 2.3395981788635254
Validation loss: 2.869583801556659

Epoch: 5| Step: 5
Training loss: 2.8058266639709473
Validation loss: 2.878397600625151

Epoch: 5| Step: 6
Training loss: 3.1739490032196045
Validation loss: 2.877364650849373

Epoch: 5| Step: 7
Training loss: 3.3891143798828125
Validation loss: 2.8694651255043606

Epoch: 5| Step: 8
Training loss: 2.8726813793182373
Validation loss: 2.872407305625177

Epoch: 5| Step: 9
Training loss: 3.2098071575164795
Validation loss: 2.858981176089215

Epoch: 5| Step: 10
Training loss: 2.508849620819092
Validation loss: 2.85302742322286

Epoch: 15| Step: 0
Training loss: 2.6404078006744385
Validation loss: 2.8471230306933

Epoch: 5| Step: 1
Training loss: 2.453303813934326
Validation loss: 2.8488889637813775

Epoch: 5| Step: 2
Training loss: 3.5487098693847656
Validation loss: 2.848684908241354

Epoch: 5| Step: 3
Training loss: 2.8690242767333984
Validation loss: 2.8493132283610683

Epoch: 5| Step: 4
Training loss: 3.013113021850586
Validation loss: 2.83750823749009

Epoch: 5| Step: 5
Training loss: 2.3684608936309814
Validation loss: 2.8306130952732538

Epoch: 5| Step: 6
Training loss: 3.1539840698242188
Validation loss: 2.8242319194219445

Epoch: 5| Step: 7
Training loss: 3.937788486480713
Validation loss: 2.8234672930932816

Epoch: 5| Step: 8
Training loss: 3.2180919647216797
Validation loss: 2.817570629940238

Epoch: 5| Step: 9
Training loss: 2.069150686264038
Validation loss: 2.817233952142859

Epoch: 5| Step: 10
Training loss: 3.227165699005127
Validation loss: 2.8217069000326176

Epoch: 16| Step: 0
Training loss: 2.7476069927215576
Validation loss: 2.818438429986277

Epoch: 5| Step: 1
Training loss: 2.9941749572753906
Validation loss: 2.81264502515075

Epoch: 5| Step: 2
Training loss: 4.131790637969971
Validation loss: 2.8064122840922368

Epoch: 5| Step: 3
Training loss: 2.3259530067443848
Validation loss: 2.7987158067764772

Epoch: 5| Step: 4
Training loss: 3.1100399494171143
Validation loss: 2.804855933753393

Epoch: 5| Step: 5
Training loss: 3.714906692504883
Validation loss: 2.8118577362388693

Epoch: 5| Step: 6
Training loss: 2.6343741416931152
Validation loss: 2.792788851645685

Epoch: 5| Step: 7
Training loss: 3.046226739883423
Validation loss: 2.7885952918760237

Epoch: 5| Step: 8
Training loss: 2.4929699897766113
Validation loss: 2.784334034048101

Epoch: 5| Step: 9
Training loss: 2.6210694313049316
Validation loss: 2.7821201816681893

Epoch: 5| Step: 10
Training loss: 2.29119873046875
Validation loss: 2.7813766387201126

Epoch: 17| Step: 0
Training loss: 2.847217082977295
Validation loss: 2.780698237880584

Epoch: 5| Step: 1
Training loss: 2.9770584106445312
Validation loss: 2.7771966918822257

Epoch: 5| Step: 2
Training loss: 3.0015311241149902
Validation loss: 2.767236525012601

Epoch: 5| Step: 3
Training loss: 2.398540496826172
Validation loss: 2.7733527998770438

Epoch: 5| Step: 4
Training loss: 3.0323262214660645
Validation loss: 2.778221030389109

Epoch: 5| Step: 5
Training loss: 3.4456658363342285
Validation loss: 2.7942987206161662

Epoch: 5| Step: 6
Training loss: 2.786668539047241
Validation loss: 2.7703424474244476

Epoch: 5| Step: 7
Training loss: 2.9478516578674316
Validation loss: 2.7588616289118284

Epoch: 5| Step: 8
Training loss: 2.3224778175354004
Validation loss: 2.7662454753793697

Epoch: 5| Step: 9
Training loss: 3.1387767791748047
Validation loss: 2.780893328369305

Epoch: 5| Step: 10
Training loss: 3.134051561355591
Validation loss: 2.7903811290699947

Epoch: 18| Step: 0
Training loss: 2.72744083404541
Validation loss: 2.781815769851849

Epoch: 5| Step: 1
Training loss: 2.9759867191314697
Validation loss: 2.7774547453849547

Epoch: 5| Step: 2
Training loss: 2.977125644683838
Validation loss: 2.7670321182538102

Epoch: 5| Step: 3
Training loss: 3.2945709228515625
Validation loss: 2.7597784842214277

Epoch: 5| Step: 4
Training loss: 3.3503894805908203
Validation loss: 2.8252577166403494

Epoch: 5| Step: 5
Training loss: 3.3075931072235107
Validation loss: 2.820463436906056

Epoch: 5| Step: 6
Training loss: 2.5431811809539795
Validation loss: 2.7849095970071773

Epoch: 5| Step: 7
Training loss: 3.0761382579803467
Validation loss: 2.766984419156146

Epoch: 5| Step: 8
Training loss: 3.114104986190796
Validation loss: 2.761991731582149

Epoch: 5| Step: 9
Training loss: 2.684784412384033
Validation loss: 2.746702799233057

Epoch: 5| Step: 10
Training loss: 1.847842812538147
Validation loss: 2.7426840900092997

Epoch: 19| Step: 0
Training loss: 3.4361259937286377
Validation loss: 2.7319885556415846

Epoch: 5| Step: 1
Training loss: 4.674952030181885
Validation loss: 2.731008924463744

Epoch: 5| Step: 2
Training loss: 2.36293888092041
Validation loss: 2.7170570819608626

Epoch: 5| Step: 3
Training loss: 2.3605237007141113
Validation loss: 2.724149420697202

Epoch: 5| Step: 4
Training loss: 2.728440523147583
Validation loss: 2.7193556985547467

Epoch: 5| Step: 5
Training loss: 3.3320701122283936
Validation loss: 2.720946381168981

Epoch: 5| Step: 6
Training loss: 2.329737663269043
Validation loss: 2.727493552751439

Epoch: 5| Step: 7
Training loss: 2.4180428981781006
Validation loss: 2.724382605603946

Epoch: 5| Step: 8
Training loss: 2.455457925796509
Validation loss: 2.7143162065936672

Epoch: 5| Step: 9
Training loss: 2.8609652519226074
Validation loss: 2.707620487418226

Epoch: 5| Step: 10
Training loss: 2.564687967300415
Validation loss: 2.701365360649683

Epoch: 20| Step: 0
Training loss: 3.5095412731170654
Validation loss: 2.732496007796257

Epoch: 5| Step: 1
Training loss: 2.6282737255096436
Validation loss: 2.7254452449019237

Epoch: 5| Step: 2
Training loss: 3.645534038543701
Validation loss: 2.7050621483915593

Epoch: 5| Step: 3
Training loss: 2.3611562252044678
Validation loss: 2.6956547075702297

Epoch: 5| Step: 4
Training loss: 2.7605979442596436
Validation loss: 2.705809288127448

Epoch: 5| Step: 5
Training loss: 2.4910781383514404
Validation loss: 2.703669012233775

Epoch: 5| Step: 6
Training loss: 2.572206974029541
Validation loss: 2.696530642048005

Epoch: 5| Step: 7
Training loss: 2.191988468170166
Validation loss: 2.704301618760632

Epoch: 5| Step: 8
Training loss: 3.765787124633789
Validation loss: 2.6930747621802875

Epoch: 5| Step: 9
Training loss: 2.7414629459381104
Validation loss: 2.686954734145954

Epoch: 5| Step: 10
Training loss: 2.7786362171173096
Validation loss: 2.6827738233791885

Epoch: 21| Step: 0
Training loss: 3.8226027488708496
Validation loss: 2.69357814070999

Epoch: 5| Step: 1
Training loss: 2.792527675628662
Validation loss: 2.686091402525543

Epoch: 5| Step: 2
Training loss: 1.9889885187149048
Validation loss: 2.6777721194810766

Epoch: 5| Step: 3
Training loss: 2.3771414756774902
Validation loss: 2.679636163096274

Epoch: 5| Step: 4
Training loss: 2.8287250995635986
Validation loss: 2.684957117162725

Epoch: 5| Step: 5
Training loss: 2.5264267921447754
Validation loss: 2.69318792768704

Epoch: 5| Step: 6
Training loss: 2.674907684326172
Validation loss: 2.6999451755195536

Epoch: 5| Step: 7
Training loss: 3.1048359870910645
Validation loss: 2.704754388460549

Epoch: 5| Step: 8
Training loss: 3.1652162075042725
Validation loss: 2.6899335333096084

Epoch: 5| Step: 9
Training loss: 3.720050096511841
Validation loss: 2.679511554779545

Epoch: 5| Step: 10
Training loss: 2.4086241722106934
Validation loss: 2.674660434005081

Epoch: 22| Step: 0
Training loss: 2.8709378242492676
Validation loss: 2.6855450932697584

Epoch: 5| Step: 1
Training loss: 2.5153396129608154
Validation loss: 2.698416994464013

Epoch: 5| Step: 2
Training loss: 2.325338363647461
Validation loss: 2.701466093781174

Epoch: 5| Step: 3
Training loss: 3.6107451915740967
Validation loss: 2.6848279045474146

Epoch: 5| Step: 4
Training loss: 3.1670327186584473
Validation loss: 2.669253603104622

Epoch: 5| Step: 5
Training loss: 3.308528184890747
Validation loss: 2.6715173464949413

Epoch: 5| Step: 6
Training loss: 3.0094470977783203
Validation loss: 2.676383661967452

Epoch: 5| Step: 7
Training loss: 2.4941277503967285
Validation loss: 2.668866859969272

Epoch: 5| Step: 8
Training loss: 2.308255672454834
Validation loss: 2.669588071043773

Epoch: 5| Step: 9
Training loss: 2.362279176712036
Validation loss: 2.666072173785138

Epoch: 5| Step: 10
Training loss: 3.461402416229248
Validation loss: 2.6568210022423857

Epoch: 23| Step: 0
Training loss: 2.6318366527557373
Validation loss: 2.6515826358590076

Epoch: 5| Step: 1
Training loss: 3.497471570968628
Validation loss: 2.6500043663927304

Epoch: 5| Step: 2
Training loss: 3.295062303543091
Validation loss: 2.6536940015772337

Epoch: 5| Step: 3
Training loss: 2.8519248962402344
Validation loss: 2.6437710946606052

Epoch: 5| Step: 4
Training loss: 2.7664954662323
Validation loss: 2.647530609561551

Epoch: 5| Step: 5
Training loss: 2.4833199977874756
Validation loss: 2.645891874067245

Epoch: 5| Step: 6
Training loss: 2.5873324871063232
Validation loss: 2.649685739189066

Epoch: 5| Step: 7
Training loss: 2.1182191371917725
Validation loss: 2.6430999643059185

Epoch: 5| Step: 8
Training loss: 3.426572799682617
Validation loss: 2.6482667922973633

Epoch: 5| Step: 9
Training loss: 2.856323003768921
Validation loss: 2.6695552205526702

Epoch: 5| Step: 10
Training loss: 2.397576332092285
Validation loss: 2.668405017545146

Epoch: 24| Step: 0
Training loss: 2.531430959701538
Validation loss: 2.6501808166503906

Epoch: 5| Step: 1
Training loss: 3.163057804107666
Validation loss: 2.6753891437284407

Epoch: 5| Step: 2
Training loss: 2.7839419841766357
Validation loss: 2.654649060259583

Epoch: 5| Step: 3
Training loss: 2.867687940597534
Validation loss: 2.6393748944805515

Epoch: 5| Step: 4
Training loss: 3.025189161300659
Validation loss: 2.652745654506068

Epoch: 5| Step: 5
Training loss: 2.9533982276916504
Validation loss: 2.6741806512237876

Epoch: 5| Step: 6
Training loss: 3.300503969192505
Validation loss: 2.658624343974616

Epoch: 5| Step: 7
Training loss: 2.7495102882385254
Validation loss: 2.6474792521487

Epoch: 5| Step: 8
Training loss: 2.4100255966186523
Validation loss: 2.638500393077891

Epoch: 5| Step: 9
Training loss: 2.5305142402648926
Validation loss: 2.641589246770387

Epoch: 5| Step: 10
Training loss: 2.5806751251220703
Validation loss: 2.640356094606461

Epoch: 25| Step: 0
Training loss: 2.809687376022339
Validation loss: 2.638391366568945

Epoch: 5| Step: 1
Training loss: 2.575460910797119
Validation loss: 2.6405303273149716

Epoch: 5| Step: 2
Training loss: 4.012375831604004
Validation loss: 2.640201719858313

Epoch: 5| Step: 3
Training loss: 2.435837507247925
Validation loss: 2.630799844700803

Epoch: 5| Step: 4
Training loss: 1.9240601062774658
Validation loss: 2.62891738389128

Epoch: 5| Step: 5
Training loss: 3.057966470718384
Validation loss: 2.635684072330434

Epoch: 5| Step: 6
Training loss: 2.5886991024017334
Validation loss: 2.645679391840453

Epoch: 5| Step: 7
Training loss: 3.37725830078125
Validation loss: 2.62718004052357

Epoch: 5| Step: 8
Training loss: 2.5214381217956543
Validation loss: 2.620155329345375

Epoch: 5| Step: 9
Training loss: 2.650611162185669
Validation loss: 2.620661384315901

Epoch: 5| Step: 10
Training loss: 2.6789238452911377
Validation loss: 2.6185115409153763

Epoch: 26| Step: 0
Training loss: 2.234046459197998
Validation loss: 2.6150391729929114

Epoch: 5| Step: 1
Training loss: 2.178715944290161
Validation loss: 2.6097677984545307

Epoch: 5| Step: 2
Training loss: 2.9769082069396973
Validation loss: 2.611218334526144

Epoch: 5| Step: 3
Training loss: 3.606187343597412
Validation loss: 2.6123780845313944

Epoch: 5| Step: 4
Training loss: 2.225318431854248
Validation loss: 2.6103313533208703

Epoch: 5| Step: 5
Training loss: 2.7715415954589844
Validation loss: 2.609646471597815

Epoch: 5| Step: 6
Training loss: 2.5814507007598877
Validation loss: 2.609798285268968

Epoch: 5| Step: 7
Training loss: 3.5362014770507812
Validation loss: 2.6258876990246516

Epoch: 5| Step: 8
Training loss: 3.067403793334961
Validation loss: 2.6236492818401707

Epoch: 5| Step: 9
Training loss: 2.8329031467437744
Validation loss: 2.6183519055766444

Epoch: 5| Step: 10
Training loss: 2.480222463607788
Validation loss: 2.6162949762036725

Epoch: 27| Step: 0
Training loss: 2.7559990882873535
Validation loss: 2.6004129661026822

Epoch: 5| Step: 1
Training loss: 3.321697950363159
Validation loss: 2.595225821259201

Epoch: 5| Step: 2
Training loss: 2.6330442428588867
Validation loss: 2.593533318529847

Epoch: 5| Step: 3
Training loss: 2.809755325317383
Validation loss: 2.5954527085827244

Epoch: 5| Step: 4
Training loss: 2.516620635986328
Validation loss: 2.5904595108442408

Epoch: 5| Step: 5
Training loss: 2.5362117290496826
Validation loss: 2.586957049626176

Epoch: 5| Step: 6
Training loss: 2.3011844158172607
Validation loss: 2.5836609871156755

Epoch: 5| Step: 7
Training loss: 2.881953239440918
Validation loss: 2.5848093853201917

Epoch: 5| Step: 8
Training loss: 3.326517105102539
Validation loss: 2.588372422802833

Epoch: 5| Step: 9
Training loss: 2.2834055423736572
Validation loss: 2.600991943831085

Epoch: 5| Step: 10
Training loss: 3.099740982055664
Validation loss: 2.5984406009797127

Epoch: 28| Step: 0
Training loss: 3.3303706645965576
Validation loss: 2.5681628463088826

Epoch: 5| Step: 1
Training loss: 2.981027126312256
Validation loss: 2.5609125065547165

Epoch: 5| Step: 2
Training loss: 3.2350215911865234
Validation loss: 2.5606265606418734

Epoch: 5| Step: 3
Training loss: 2.1308913230895996
Validation loss: 2.566824572060698

Epoch: 5| Step: 4
Training loss: 2.8823108673095703
Validation loss: 2.574192585483674

Epoch: 5| Step: 5
Training loss: 2.4032952785491943
Validation loss: 2.5624075987005748

Epoch: 5| Step: 6
Training loss: 2.0544590950012207
Validation loss: 2.554927049144622

Epoch: 5| Step: 7
Training loss: 3.158702850341797
Validation loss: 2.5682920050877396

Epoch: 5| Step: 8
Training loss: 2.5602805614471436
Validation loss: 2.5897019306818643

Epoch: 5| Step: 9
Training loss: 2.9680280685424805
Validation loss: 2.60772031353366

Epoch: 5| Step: 10
Training loss: 2.6857941150665283
Validation loss: 2.638165094519174

Epoch: 29| Step: 0
Training loss: 2.4048097133636475
Validation loss: 2.59095509077913

Epoch: 5| Step: 1
Training loss: 2.104762554168701
Validation loss: 2.55428211919723

Epoch: 5| Step: 2
Training loss: 2.388136625289917
Validation loss: 2.554197542129024

Epoch: 5| Step: 3
Training loss: 2.8542981147766113
Validation loss: 2.572682633194872

Epoch: 5| Step: 4
Training loss: 3.4085476398468018
Validation loss: 2.5790653433851016

Epoch: 5| Step: 5
Training loss: 3.000638484954834
Validation loss: 2.5736418257477465

Epoch: 5| Step: 6
Training loss: 2.779130697250366
Validation loss: 2.563682281842796

Epoch: 5| Step: 7
Training loss: 2.872424840927124
Validation loss: 2.555031648246191

Epoch: 5| Step: 8
Training loss: 2.5183420181274414
Validation loss: 2.5427967040769515

Epoch: 5| Step: 9
Training loss: 3.240422010421753
Validation loss: 2.5412872632344565

Epoch: 5| Step: 10
Training loss: 2.6868033409118652
Validation loss: 2.549258129571074

Epoch: 30| Step: 0
Training loss: 2.9963748455047607
Validation loss: 2.557344636609477

Epoch: 5| Step: 1
Training loss: 3.340911865234375
Validation loss: 2.581604734543831

Epoch: 5| Step: 2
Training loss: 2.82985258102417
Validation loss: 2.5495209796454317

Epoch: 5| Step: 3
Training loss: 2.825446605682373
Validation loss: 2.5481290432714645

Epoch: 5| Step: 4
Training loss: 2.963207721710205
Validation loss: 2.5556950902426117

Epoch: 5| Step: 5
Training loss: 2.581907272338867
Validation loss: 2.5595473371526247

Epoch: 5| Step: 6
Training loss: 2.7235846519470215
Validation loss: 2.5568321520282375

Epoch: 5| Step: 7
Training loss: 2.3853466510772705
Validation loss: 2.5454070875721593

Epoch: 5| Step: 8
Training loss: 2.5461418628692627
Validation loss: 2.5382893264934583

Epoch: 5| Step: 9
Training loss: 2.405550003051758
Validation loss: 2.5385380560351956

Epoch: 5| Step: 10
Training loss: 2.4028208255767822
Validation loss: 2.5361550905371226

Epoch: 31| Step: 0
Training loss: 2.2154343128204346
Validation loss: 2.5323558340790453

Epoch: 5| Step: 1
Training loss: 2.6250641345977783
Validation loss: 2.5257197041665354

Epoch: 5| Step: 2
Training loss: 2.2342493534088135
Validation loss: 2.5328616237127655

Epoch: 5| Step: 3
Training loss: 2.7533164024353027
Validation loss: 2.547208721919726

Epoch: 5| Step: 4
Training loss: 2.285369396209717
Validation loss: 2.5703893861462994

Epoch: 5| Step: 5
Training loss: 3.104471445083618
Validation loss: 2.5777127973494993

Epoch: 5| Step: 6
Training loss: 2.7519595623016357
Validation loss: 2.5707476779978764

Epoch: 5| Step: 7
Training loss: 2.276576519012451
Validation loss: 2.5208588569395003

Epoch: 5| Step: 8
Training loss: 3.2044677734375
Validation loss: 2.525059341102518

Epoch: 5| Step: 9
Training loss: 3.2613894939422607
Validation loss: 2.5416780864038775

Epoch: 5| Step: 10
Training loss: 3.4725165367126465
Validation loss: 2.556291377672585

Epoch: 32| Step: 0
Training loss: 2.221006393432617
Validation loss: 2.553994140317363

Epoch: 5| Step: 1
Training loss: 2.9169209003448486
Validation loss: 2.522372966171593

Epoch: 5| Step: 2
Training loss: 3.1622555255889893
Validation loss: 2.5142100190603607

Epoch: 5| Step: 3
Training loss: 2.5137157440185547
Validation loss: 2.514625087861092

Epoch: 5| Step: 4
Training loss: 2.9495372772216797
Validation loss: 2.5089644565377185

Epoch: 5| Step: 5
Training loss: 3.048335552215576
Validation loss: 2.50855649927611

Epoch: 5| Step: 6
Training loss: 2.805933952331543
Validation loss: 2.5067632018878894

Epoch: 5| Step: 7
Training loss: 2.5123977661132812
Validation loss: 2.499267852434548

Epoch: 5| Step: 8
Training loss: 2.70066499710083
Validation loss: 2.504656499431979

Epoch: 5| Step: 9
Training loss: 2.0839760303497314
Validation loss: 2.5104774390497515

Epoch: 5| Step: 10
Training loss: 2.873779296875
Validation loss: 2.5203297215123333

Epoch: 33| Step: 0
Training loss: 2.6326403617858887
Validation loss: 2.5281151020398704

Epoch: 5| Step: 1
Training loss: 2.194445848464966
Validation loss: 2.5310726088862263

Epoch: 5| Step: 2
Training loss: 3.143873691558838
Validation loss: 2.5076341244482223

Epoch: 5| Step: 3
Training loss: 2.2080087661743164
Validation loss: 2.489010149432767

Epoch: 5| Step: 4
Training loss: 2.777324676513672
Validation loss: 2.493047211759834

Epoch: 5| Step: 5
Training loss: 2.830397367477417
Validation loss: 2.497169570256305

Epoch: 5| Step: 6
Training loss: 2.708925485610962
Validation loss: 2.5104299258160334

Epoch: 5| Step: 7
Training loss: 2.5213510990142822
Validation loss: 2.5043204522901967

Epoch: 5| Step: 8
Training loss: 3.2126059532165527
Validation loss: 2.498488146771667

Epoch: 5| Step: 9
Training loss: 2.608229637145996
Validation loss: 2.494783498907602

Epoch: 5| Step: 10
Training loss: 2.7952287197113037
Validation loss: 2.497308287569272

Epoch: 34| Step: 0
Training loss: 3.1212406158447266
Validation loss: 2.483575164630849

Epoch: 5| Step: 1
Training loss: 2.590782642364502
Validation loss: 2.486273175926619

Epoch: 5| Step: 2
Training loss: 2.535525321960449
Validation loss: 2.4874239198623167

Epoch: 5| Step: 3
Training loss: 2.278665781021118
Validation loss: 2.492525731363604

Epoch: 5| Step: 4
Training loss: 2.9214701652526855
Validation loss: 2.5081294813463764

Epoch: 5| Step: 5
Training loss: 3.2424235343933105
Validation loss: 2.512492564416701

Epoch: 5| Step: 6
Training loss: 2.6765666007995605
Validation loss: 2.5196968714396157

Epoch: 5| Step: 7
Training loss: 2.775886058807373
Validation loss: 2.5037735918516755

Epoch: 5| Step: 8
Training loss: 2.642031192779541
Validation loss: 2.500903388505341

Epoch: 5| Step: 9
Training loss: 2.848158359527588
Validation loss: 2.515818372849495

Epoch: 5| Step: 10
Training loss: 1.8647198677062988
Validation loss: 2.5209352175394693

Epoch: 35| Step: 0
Training loss: 2.9640934467315674
Validation loss: 2.487307499813777

Epoch: 5| Step: 1
Training loss: 3.095083475112915
Validation loss: 2.485425833732851

Epoch: 5| Step: 2
Training loss: 2.826751708984375
Validation loss: 2.480504133368051

Epoch: 5| Step: 3
Training loss: 2.7747154235839844
Validation loss: 2.489103635152181

Epoch: 5| Step: 4
Training loss: 2.1586384773254395
Validation loss: 2.486273255399478

Epoch: 5| Step: 5
Training loss: 2.9002437591552734
Validation loss: 2.482710922918012

Epoch: 5| Step: 6
Training loss: 2.7906951904296875
Validation loss: 2.4915781508209887

Epoch: 5| Step: 7
Training loss: 2.232186794281006
Validation loss: 2.4803448774481334

Epoch: 5| Step: 8
Training loss: 2.6537270545959473
Validation loss: 2.4790062776175876

Epoch: 5| Step: 9
Training loss: 2.909162759780884
Validation loss: 2.4898258691192954

Epoch: 5| Step: 10
Training loss: 2.2136027812957764
Validation loss: 2.4985944019850863

Epoch: 36| Step: 0
Training loss: 2.64786958694458
Validation loss: 2.503293711652038

Epoch: 5| Step: 1
Training loss: 2.7142539024353027
Validation loss: 2.5096582981847946

Epoch: 5| Step: 2
Training loss: 3.452244520187378
Validation loss: 2.500620403597432

Epoch: 5| Step: 3
Training loss: 2.3226466178894043
Validation loss: 2.4889496167500815

Epoch: 5| Step: 4
Training loss: 3.3035411834716797
Validation loss: 2.4668698823580177

Epoch: 5| Step: 5
Training loss: 2.4034647941589355
Validation loss: 2.470587812444215

Epoch: 5| Step: 6
Training loss: 2.117354154586792
Validation loss: 2.485613167926829

Epoch: 5| Step: 7
Training loss: 1.7630579471588135
Validation loss: 2.5289316536277853

Epoch: 5| Step: 8
Training loss: 3.3165557384490967
Validation loss: 2.527508146019392

Epoch: 5| Step: 9
Training loss: 2.7983386516571045
Validation loss: 2.4800831169210453

Epoch: 5| Step: 10
Training loss: 2.7858729362487793
Validation loss: 2.4658288494233163

Epoch: 37| Step: 0
Training loss: 2.4582579135894775
Validation loss: 2.4681228771004626

Epoch: 5| Step: 1
Training loss: 2.737903594970703
Validation loss: 2.481811636237688

Epoch: 5| Step: 2
Training loss: 2.738940715789795
Validation loss: 2.477862047892745

Epoch: 5| Step: 3
Training loss: 2.1399593353271484
Validation loss: 2.4750808003128215

Epoch: 5| Step: 4
Training loss: 3.1901488304138184
Validation loss: 2.4635642036314933

Epoch: 5| Step: 5
Training loss: 2.5386452674865723
Validation loss: 2.4589609945974042

Epoch: 5| Step: 6
Training loss: 2.535411834716797
Validation loss: 2.458459578534608

Epoch: 5| Step: 7
Training loss: 3.0235860347747803
Validation loss: 2.459656230864986

Epoch: 5| Step: 8
Training loss: 2.4373791217803955
Validation loss: 2.462117200256676

Epoch: 5| Step: 9
Training loss: 2.9316909313201904
Validation loss: 2.4586605871877363

Epoch: 5| Step: 10
Training loss: 2.726496934890747
Validation loss: 2.458241190961612

Epoch: 38| Step: 0
Training loss: 3.0255050659179688
Validation loss: 2.461611916941981

Epoch: 5| Step: 1
Training loss: 2.622748374938965
Validation loss: 2.4621728774039977

Epoch: 5| Step: 2
Training loss: 2.1614222526550293
Validation loss: 2.455439434256605

Epoch: 5| Step: 3
Training loss: 2.1250033378601074
Validation loss: 2.4680359517374346

Epoch: 5| Step: 4
Training loss: 2.6307919025421143
Validation loss: 2.461092907895324

Epoch: 5| Step: 5
Training loss: 2.8806040287017822
Validation loss: 2.4616954839357765

Epoch: 5| Step: 6
Training loss: 3.124175548553467
Validation loss: 2.4570935054491927

Epoch: 5| Step: 7
Training loss: 2.3258631229400635
Validation loss: 2.4518575719607774

Epoch: 5| Step: 8
Training loss: 3.0242037773132324
Validation loss: 2.4525767423773326

Epoch: 5| Step: 9
Training loss: 2.8162455558776855
Validation loss: 2.4536410531690045

Epoch: 5| Step: 10
Training loss: 2.741696834564209
Validation loss: 2.451449999245264

Epoch: 39| Step: 0
Training loss: 2.5137972831726074
Validation loss: 2.45965584119161

Epoch: 5| Step: 1
Training loss: 2.403836250305176
Validation loss: 2.452317214781238

Epoch: 5| Step: 2
Training loss: 2.330608367919922
Validation loss: 2.4572827457099833

Epoch: 5| Step: 3
Training loss: 2.462568759918213
Validation loss: 2.4499497336726033

Epoch: 5| Step: 4
Training loss: 3.011626720428467
Validation loss: 2.457320003099339

Epoch: 5| Step: 5
Training loss: 2.4680933952331543
Validation loss: 2.4495607960608696

Epoch: 5| Step: 6
Training loss: 3.358654737472534
Validation loss: 2.4470420806638655

Epoch: 5| Step: 7
Training loss: 3.311488628387451
Validation loss: 2.4407240741996357

Epoch: 5| Step: 8
Training loss: 2.589092969894409
Validation loss: 2.443972820876747

Epoch: 5| Step: 9
Training loss: 3.0580239295959473
Validation loss: 2.4401408267277542

Epoch: 5| Step: 10
Training loss: 1.7157381772994995
Validation loss: 2.437700697170791

Epoch: 40| Step: 0
Training loss: 1.8479042053222656
Validation loss: 2.4355882188325286

Epoch: 5| Step: 1
Training loss: 2.9764063358306885
Validation loss: 2.435843506167012

Epoch: 5| Step: 2
Training loss: 2.4651801586151123
Validation loss: 2.430521224134712

Epoch: 5| Step: 3
Training loss: 3.2121777534484863
Validation loss: 2.4485273335569646

Epoch: 5| Step: 4
Training loss: 1.809927225112915
Validation loss: 2.464154792088334

Epoch: 5| Step: 5
Training loss: 2.164795398712158
Validation loss: 2.4697506837947394

Epoch: 5| Step: 6
Training loss: 3.214789628982544
Validation loss: 2.4978951408017065

Epoch: 5| Step: 7
Training loss: 2.9529194831848145
Validation loss: 2.494720382075156

Epoch: 5| Step: 8
Training loss: 3.058549404144287
Validation loss: 2.481869379679362

Epoch: 5| Step: 9
Training loss: 2.5501253604888916
Validation loss: 2.4612905825338056

Epoch: 5| Step: 10
Training loss: 3.2252864837646484
Validation loss: 2.4377521109837357

Epoch: 41| Step: 0
Training loss: 2.537755250930786
Validation loss: 2.431274647353798

Epoch: 5| Step: 1
Training loss: 3.2025704383850098
Validation loss: 2.4383698637767504

Epoch: 5| Step: 2
Training loss: 2.8486225605010986
Validation loss: 2.4506473259259294

Epoch: 5| Step: 3
Training loss: 2.504366874694824
Validation loss: 2.4609067158032487

Epoch: 5| Step: 4
Training loss: 2.411637306213379
Validation loss: 2.473383844539683

Epoch: 5| Step: 5
Training loss: 2.0677380561828613
Validation loss: 2.46133469253458

Epoch: 5| Step: 6
Training loss: 2.964275360107422
Validation loss: 2.462474233360701

Epoch: 5| Step: 7
Training loss: 2.4201107025146484
Validation loss: 2.4698871950949393

Epoch: 5| Step: 8
Training loss: 3.1602394580841064
Validation loss: 2.4640197625724216

Epoch: 5| Step: 9
Training loss: 2.899301528930664
Validation loss: 2.4418659158932265

Epoch: 5| Step: 10
Training loss: 2.442591428756714
Validation loss: 2.426201865237246

Epoch: 42| Step: 0
Training loss: 2.3727221488952637
Validation loss: 2.431265687429777

Epoch: 5| Step: 1
Training loss: 2.076247453689575
Validation loss: 2.445221888121738

Epoch: 5| Step: 2
Training loss: 2.3856682777404785
Validation loss: 2.4714726658277613

Epoch: 5| Step: 3
Training loss: 2.8868465423583984
Validation loss: 2.496082769927158

Epoch: 5| Step: 4
Training loss: 2.2556707859039307
Validation loss: 2.4570168782305974

Epoch: 5| Step: 5
Training loss: 2.9447717666625977
Validation loss: 2.433858320277224

Epoch: 5| Step: 6
Training loss: 2.9814977645874023
Validation loss: 2.4286334463345107

Epoch: 5| Step: 7
Training loss: 3.040499210357666
Validation loss: 2.420241743005732

Epoch: 5| Step: 8
Training loss: 2.9595398902893066
Validation loss: 2.4184535395714546

Epoch: 5| Step: 9
Training loss: 2.7111055850982666
Validation loss: 2.420473144900414

Epoch: 5| Step: 10
Training loss: 2.660022258758545
Validation loss: 2.4230282088761688

Epoch: 43| Step: 0
Training loss: 2.7035861015319824
Validation loss: 2.4288349484884613

Epoch: 5| Step: 1
Training loss: 2.9491329193115234
Validation loss: 2.4416075547536216

Epoch: 5| Step: 2
Training loss: 2.3582615852355957
Validation loss: 2.4629584871312624

Epoch: 5| Step: 3
Training loss: 3.0510239601135254
Validation loss: 2.4539610801204557

Epoch: 5| Step: 4
Training loss: 2.2687106132507324
Validation loss: 2.4351750830168366

Epoch: 5| Step: 5
Training loss: 2.738765239715576
Validation loss: 2.4375010023834887

Epoch: 5| Step: 6
Training loss: 2.5199320316314697
Validation loss: 2.4316185853814565

Epoch: 5| Step: 7
Training loss: 2.1637802124023438
Validation loss: 2.4363581339518228

Epoch: 5| Step: 8
Training loss: 2.420819044113159
Validation loss: 2.43895818597527

Epoch: 5| Step: 9
Training loss: 3.421853542327881
Validation loss: 2.4251748848986883

Epoch: 5| Step: 10
Training loss: 2.604825258255005
Validation loss: 2.417313334762409

Epoch: 44| Step: 0
Training loss: 2.7462198734283447
Validation loss: 2.4194833027419222

Epoch: 5| Step: 1
Training loss: 2.3864572048187256
Validation loss: 2.4290502507199525

Epoch: 5| Step: 2
Training loss: 2.9610788822174072
Validation loss: 2.438844637204242

Epoch: 5| Step: 3
Training loss: 2.7326836585998535
Validation loss: 2.4459399536091793

Epoch: 5| Step: 4
Training loss: 2.2088847160339355
Validation loss: 2.4255154568661927

Epoch: 5| Step: 5
Training loss: 2.7293553352355957
Validation loss: 2.4201045600316857

Epoch: 5| Step: 6
Training loss: 2.520606279373169
Validation loss: 2.4140601747779438

Epoch: 5| Step: 7
Training loss: 2.3190417289733887
Validation loss: 2.4060420784898984

Epoch: 5| Step: 8
Training loss: 3.332587718963623
Validation loss: 2.412906123745826

Epoch: 5| Step: 9
Training loss: 3.0917420387268066
Validation loss: 2.435785298706383

Epoch: 5| Step: 10
Training loss: 2.279782772064209
Validation loss: 2.4496806180605324

Epoch: 45| Step: 0
Training loss: 2.9595859050750732
Validation loss: 2.4719663281594553

Epoch: 5| Step: 1
Training loss: 2.674112319946289
Validation loss: 2.50010758830655

Epoch: 5| Step: 2
Training loss: 1.9725326299667358
Validation loss: 2.4899241591012604

Epoch: 5| Step: 3
Training loss: 2.170499324798584
Validation loss: 2.4513384347320883

Epoch: 5| Step: 4
Training loss: 2.6709823608398438
Validation loss: 2.4217227735827045

Epoch: 5| Step: 5
Training loss: 2.884324312210083
Validation loss: 2.410738750170636

Epoch: 5| Step: 6
Training loss: 3.065903902053833
Validation loss: 2.4132256623237365

Epoch: 5| Step: 7
Training loss: 2.647019147872925
Validation loss: 2.4287676708672636

Epoch: 5| Step: 8
Training loss: 2.600151538848877
Validation loss: 2.432500790524226

Epoch: 5| Step: 9
Training loss: 2.6663336753845215
Validation loss: 2.43367075920105

Epoch: 5| Step: 10
Training loss: 2.9037952423095703
Validation loss: 2.4228667443798435

Epoch: 46| Step: 0
Training loss: 2.791553258895874
Validation loss: 2.4170597368671047

Epoch: 5| Step: 1
Training loss: 3.1255903244018555
Validation loss: 2.421304092612318

Epoch: 5| Step: 2
Training loss: 2.1395504474639893
Validation loss: 2.4248670006311066

Epoch: 5| Step: 3
Training loss: 2.245068073272705
Validation loss: 2.4402274034356557

Epoch: 5| Step: 4
Training loss: 2.821089506149292
Validation loss: 2.463594187972366

Epoch: 5| Step: 5
Training loss: 2.5899410247802734
Validation loss: 2.446191672355898

Epoch: 5| Step: 6
Training loss: 2.554793119430542
Validation loss: 2.436880352676556

Epoch: 5| Step: 7
Training loss: 2.9732353687286377
Validation loss: 2.4068603387442966

Epoch: 5| Step: 8
Training loss: 2.6675758361816406
Validation loss: 2.398738007391653

Epoch: 5| Step: 9
Training loss: 2.365377187728882
Validation loss: 2.4042123261318413

Epoch: 5| Step: 10
Training loss: 2.999405860900879
Validation loss: 2.402907943212858

Epoch: 47| Step: 0
Training loss: 2.676741361618042
Validation loss: 2.4029971604706137

Epoch: 5| Step: 1
Training loss: 3.456362247467041
Validation loss: 2.4028268783323226

Epoch: 5| Step: 2
Training loss: 2.8180837631225586
Validation loss: 2.401194081511549

Epoch: 5| Step: 3
Training loss: 2.631094455718994
Validation loss: 2.3962918404609925

Epoch: 5| Step: 4
Training loss: 2.4978694915771484
Validation loss: 2.3968185135113296

Epoch: 5| Step: 5
Training loss: 2.652742385864258
Validation loss: 2.408908454320764

Epoch: 5| Step: 6
Training loss: 2.4500176906585693
Validation loss: 2.4512123471947125

Epoch: 5| Step: 7
Training loss: 2.3491837978363037
Validation loss: 2.5030434362349974

Epoch: 5| Step: 8
Training loss: 2.8279919624328613
Validation loss: 2.5171492176671184

Epoch: 5| Step: 9
Training loss: 2.4627723693847656
Validation loss: 2.475845170277421

Epoch: 5| Step: 10
Training loss: 2.420428514480591
Validation loss: 2.4157311095986316

Epoch: 48| Step: 0
Training loss: 3.165830612182617
Validation loss: 2.4015688383451073

Epoch: 5| Step: 1
Training loss: 2.409506320953369
Validation loss: 2.402846287655574

Epoch: 5| Step: 2
Training loss: 1.8676691055297852
Validation loss: 2.4236594912826375

Epoch: 5| Step: 3
Training loss: 2.7030961513519287
Validation loss: 2.4343388875325522

Epoch: 5| Step: 4
Training loss: 2.783365488052368
Validation loss: 2.432482962967247

Epoch: 5| Step: 5
Training loss: 2.5362110137939453
Validation loss: 2.4235763934350785

Epoch: 5| Step: 6
Training loss: 2.691307544708252
Validation loss: 2.4103884850778887

Epoch: 5| Step: 7
Training loss: 2.094966411590576
Validation loss: 2.415189068804505

Epoch: 5| Step: 8
Training loss: 2.893359422683716
Validation loss: 2.4292291569453415

Epoch: 5| Step: 9
Training loss: 3.3608546257019043
Validation loss: 2.4404467946739605

Epoch: 5| Step: 10
Training loss: 2.7384490966796875
Validation loss: 2.4281615800755

Epoch: 49| Step: 0
Training loss: 2.7874045372009277
Validation loss: 2.419936590297248

Epoch: 5| Step: 1
Training loss: 2.8109099864959717
Validation loss: 2.4050831615283923

Epoch: 5| Step: 2
Training loss: 1.920609474182129
Validation loss: 2.3950989143822783

Epoch: 5| Step: 3
Training loss: 2.382591962814331
Validation loss: 2.397001865089581

Epoch: 5| Step: 4
Training loss: 2.7238965034484863
Validation loss: 2.4008052349090576

Epoch: 5| Step: 5
Training loss: 3.0968804359436035
Validation loss: 2.405705664747505

Epoch: 5| Step: 6
Training loss: 2.9388160705566406
Validation loss: 2.4090282942659114

Epoch: 5| Step: 7
Training loss: 2.4908854961395264
Validation loss: 2.4007657086977394

Epoch: 5| Step: 8
Training loss: 2.299112319946289
Validation loss: 2.403222314773067

Epoch: 5| Step: 9
Training loss: 2.5524814128875732
Validation loss: 2.392548414968675

Epoch: 5| Step: 10
Training loss: 3.127408981323242
Validation loss: 2.3834114407980316

Epoch: 50| Step: 0
Training loss: 2.3830294609069824
Validation loss: 2.3915266272842244

Epoch: 5| Step: 1
Training loss: 2.5936992168426514
Validation loss: 2.397862870206115

Epoch: 5| Step: 2
Training loss: 2.9398746490478516
Validation loss: 2.406151968945739

Epoch: 5| Step: 3
Training loss: 2.748884916305542
Validation loss: 2.4256162592159805

Epoch: 5| Step: 4
Training loss: 2.69270658493042
Validation loss: 2.454311124740108

Epoch: 5| Step: 5
Training loss: 2.971548080444336
Validation loss: 2.446334851685391

Epoch: 5| Step: 6
Training loss: 2.8905608654022217
Validation loss: 2.440187907988025

Epoch: 5| Step: 7
Training loss: 3.098814010620117
Validation loss: 2.4378526492785384

Epoch: 5| Step: 8
Training loss: 2.461683750152588
Validation loss: 2.401887363003146

Epoch: 5| Step: 9
Training loss: 2.4483237266540527
Validation loss: 2.389320645281064

Epoch: 5| Step: 10
Training loss: 1.6036996841430664
Validation loss: 2.3850424648613058

Epoch: 51| Step: 0
Training loss: 2.578969717025757
Validation loss: 2.3818672639067455

Epoch: 5| Step: 1
Training loss: 2.9089274406433105
Validation loss: 2.3686778647925264

Epoch: 5| Step: 2
Training loss: 2.402207612991333
Validation loss: 2.374640921110748

Epoch: 5| Step: 3
Training loss: 2.6178512573242188
Validation loss: 2.3725888113821707

Epoch: 5| Step: 4
Training loss: 2.9857656955718994
Validation loss: 2.37626826378607

Epoch: 5| Step: 5
Training loss: 2.350475788116455
Validation loss: 2.381846107462401

Epoch: 5| Step: 6
Training loss: 2.506460428237915
Validation loss: 2.3807959941125687

Epoch: 5| Step: 7
Training loss: 2.741816282272339
Validation loss: 2.3954011573586413

Epoch: 5| Step: 8
Training loss: 2.0777859687805176
Validation loss: 2.4137362011017336

Epoch: 5| Step: 9
Training loss: 2.2584290504455566
Validation loss: 2.4406988415666806

Epoch: 5| Step: 10
Training loss: 3.7545971870422363
Validation loss: 2.4486352218094694

Epoch: 52| Step: 0
Training loss: 2.6118545532226562
Validation loss: 2.4346075211801836

Epoch: 5| Step: 1
Training loss: 2.2810516357421875
Validation loss: 2.403350519877608

Epoch: 5| Step: 2
Training loss: 2.8749728202819824
Validation loss: 2.383873365258658

Epoch: 5| Step: 3
Training loss: 2.5312116146087646
Validation loss: 2.376071071112028

Epoch: 5| Step: 4
Training loss: 2.678506374359131
Validation loss: 2.3786771143636396

Epoch: 5| Step: 5
Training loss: 2.689995765686035
Validation loss: 2.3763574425892164

Epoch: 5| Step: 6
Training loss: 2.5401623249053955
Validation loss: 2.3753061025373396

Epoch: 5| Step: 7
Training loss: 2.6721856594085693
Validation loss: 2.3793198831619753

Epoch: 5| Step: 8
Training loss: 2.7593235969543457
Validation loss: 2.3900806570565827

Epoch: 5| Step: 9
Training loss: 2.238190174102783
Validation loss: 2.37382338893029

Epoch: 5| Step: 10
Training loss: 3.1000404357910156
Validation loss: 2.3798985276170956

Epoch: 53| Step: 0
Training loss: 3.4147567749023438
Validation loss: 2.388803025727631

Epoch: 5| Step: 1
Training loss: 2.745703935623169
Validation loss: 2.3962541703254945

Epoch: 5| Step: 2
Training loss: 2.9227144718170166
Validation loss: 2.415832458003875

Epoch: 5| Step: 3
Training loss: 2.5220580101013184
Validation loss: 2.433855756636589

Epoch: 5| Step: 4
Training loss: 2.674501895904541
Validation loss: 2.473120020281884

Epoch: 5| Step: 5
Training loss: 2.6412365436553955
Validation loss: 2.4584899563943186

Epoch: 5| Step: 6
Training loss: 2.2090954780578613
Validation loss: 2.4246388776327974

Epoch: 5| Step: 7
Training loss: 3.22906756401062
Validation loss: 2.393618324751495

Epoch: 5| Step: 8
Training loss: 2.07710599899292
Validation loss: 2.3704788941209034

Epoch: 5| Step: 9
Training loss: 1.9516483545303345
Validation loss: 2.3578846864802863

Epoch: 5| Step: 10
Training loss: 2.572838306427002
Validation loss: 2.3553194410057476

Epoch: 54| Step: 0
Training loss: 2.1867966651916504
Validation loss: 2.3594547548601703

Epoch: 5| Step: 1
Training loss: 2.687279224395752
Validation loss: 2.356716881516159

Epoch: 5| Step: 2
Training loss: 2.3815853595733643
Validation loss: 2.3547798971976004

Epoch: 5| Step: 3
Training loss: 3.017188310623169
Validation loss: 2.35224393106276

Epoch: 5| Step: 4
Training loss: 2.3511433601379395
Validation loss: 2.360807846951228

Epoch: 5| Step: 5
Training loss: 2.4336647987365723
Validation loss: 2.3728500258538032

Epoch: 5| Step: 6
Training loss: 2.871422529220581
Validation loss: 2.433525352067845

Epoch: 5| Step: 7
Training loss: 2.925290584564209
Validation loss: 2.4834545325207453

Epoch: 5| Step: 8
Training loss: 2.8558387756347656
Validation loss: 2.4911856471851306

Epoch: 5| Step: 9
Training loss: 2.6987457275390625
Validation loss: 2.461891597317111

Epoch: 5| Step: 10
Training loss: 2.6920197010040283
Validation loss: 2.421756770021172

Epoch: 55| Step: 0
Training loss: 2.283658027648926
Validation loss: 2.363151278547061

Epoch: 5| Step: 1
Training loss: 2.8509678840637207
Validation loss: 2.3569888504602576

Epoch: 5| Step: 2
Training loss: 2.5500054359436035
Validation loss: 2.3863315684821016

Epoch: 5| Step: 3
Training loss: 2.978209972381592
Validation loss: 2.4015089696453464

Epoch: 5| Step: 4
Training loss: 2.716907024383545
Validation loss: 2.4007641320587485

Epoch: 5| Step: 5
Training loss: 2.6752943992614746
Validation loss: 2.3713066142092467

Epoch: 5| Step: 6
Training loss: 2.2217905521392822
Validation loss: 2.3661770013070877

Epoch: 5| Step: 7
Training loss: 2.4137914180755615
Validation loss: 2.3629191562693608

Epoch: 5| Step: 8
Training loss: 2.6391842365264893
Validation loss: 2.3713555823090258

Epoch: 5| Step: 9
Training loss: 2.928908348083496
Validation loss: 2.3995899333748767

Epoch: 5| Step: 10
Training loss: 2.5190229415893555
Validation loss: 2.400357738617928

Epoch: 56| Step: 0
Training loss: 2.7911839485168457
Validation loss: 2.4216026593280096

Epoch: 5| Step: 1
Training loss: 2.810622453689575
Validation loss: 2.4369683291322444

Epoch: 5| Step: 2
Training loss: 2.562948226928711
Validation loss: 2.441451664893858

Epoch: 5| Step: 3
Training loss: 2.4020683765411377
Validation loss: 2.4666457253117717

Epoch: 5| Step: 4
Training loss: 3.1028237342834473
Validation loss: 2.4524508522402857

Epoch: 5| Step: 5
Training loss: 2.3535876274108887
Validation loss: 2.397467751656809

Epoch: 5| Step: 6
Training loss: 2.791771650314331
Validation loss: 2.3678536850919008

Epoch: 5| Step: 7
Training loss: 2.889369487762451
Validation loss: 2.3587656046754573

Epoch: 5| Step: 8
Training loss: 2.3430335521698
Validation loss: 2.3606459812451432

Epoch: 5| Step: 9
Training loss: 2.3084311485290527
Validation loss: 2.3532089059070875

Epoch: 5| Step: 10
Training loss: 2.5998475551605225
Validation loss: 2.35743546998629

Epoch: 57| Step: 0
Training loss: 2.9204678535461426
Validation loss: 2.3762108305449128

Epoch: 5| Step: 1
Training loss: 2.6437182426452637
Validation loss: 2.3869145557444584

Epoch: 5| Step: 2
Training loss: 2.7217905521392822
Validation loss: 2.381007038136964

Epoch: 5| Step: 3
Training loss: 2.397533893585205
Validation loss: 2.391710594136228

Epoch: 5| Step: 4
Training loss: 2.3100719451904297
Validation loss: 2.401310936097176

Epoch: 5| Step: 5
Training loss: 3.168062925338745
Validation loss: 2.4275073697490077

Epoch: 5| Step: 6
Training loss: 2.4931082725524902
Validation loss: 2.437412513199673

Epoch: 5| Step: 7
Training loss: 2.8159966468811035
Validation loss: 2.4524292304951656

Epoch: 5| Step: 8
Training loss: 1.9138014316558838
Validation loss: 2.470708816282211

Epoch: 5| Step: 9
Training loss: 2.945328712463379
Validation loss: 2.439498178420528

Epoch: 5| Step: 10
Training loss: 2.6268367767333984
Validation loss: 2.4013512211461223

Epoch: 58| Step: 0
Training loss: 2.259920597076416
Validation loss: 2.3808977732094387

Epoch: 5| Step: 1
Training loss: 2.1011264324188232
Validation loss: 2.354395874084965

Epoch: 5| Step: 2
Training loss: 3.470630168914795
Validation loss: 2.3506820099328154

Epoch: 5| Step: 3
Training loss: 2.813021421432495
Validation loss: 2.3597053456050094

Epoch: 5| Step: 4
Training loss: 3.1569347381591797
Validation loss: 2.388050361346173

Epoch: 5| Step: 5
Training loss: 2.727875232696533
Validation loss: 2.4285954198529645

Epoch: 5| Step: 6
Training loss: 2.691262722015381
Validation loss: 2.387559178054974

Epoch: 5| Step: 7
Training loss: 2.3787708282470703
Validation loss: 2.380892445964198

Epoch: 5| Step: 8
Training loss: 2.2283289432525635
Validation loss: 2.370670995404643

Epoch: 5| Step: 9
Training loss: 2.685940980911255
Validation loss: 2.382778283088438

Epoch: 5| Step: 10
Training loss: 2.513441324234009
Validation loss: 2.388769167725758

Epoch: 59| Step: 0
Training loss: 2.454538583755493
Validation loss: 2.4213249170652

Epoch: 5| Step: 1
Training loss: 3.4088082313537598
Validation loss: 2.5877516295320246

Epoch: 5| Step: 2
Training loss: 2.8819079399108887
Validation loss: 2.7241624786007788

Epoch: 5| Step: 3
Training loss: 2.7061691284179688
Validation loss: 2.670873434312882

Epoch: 5| Step: 4
Training loss: 2.936782121658325
Validation loss: 2.678158842107301

Epoch: 5| Step: 5
Training loss: 3.3280601501464844
Validation loss: 2.567083489510321

Epoch: 5| Step: 6
Training loss: 2.498107433319092
Validation loss: 2.394173668276879

Epoch: 5| Step: 7
Training loss: 2.550990104675293
Validation loss: 2.3262210225546234

Epoch: 5| Step: 8
Training loss: 2.0719144344329834
Validation loss: 2.327957850630565

Epoch: 5| Step: 9
Training loss: 2.1890997886657715
Validation loss: 2.3431078823663856

Epoch: 5| Step: 10
Training loss: 2.887960433959961
Validation loss: 2.3706996312705417

Epoch: 60| Step: 0
Training loss: 2.0934057235717773
Validation loss: 2.4422337162879204

Epoch: 5| Step: 1
Training loss: 3.2935566902160645
Validation loss: 2.4022460060734905

Epoch: 5| Step: 2
Training loss: 2.7771077156066895
Validation loss: 2.366584557358937

Epoch: 5| Step: 3
Training loss: 2.848891496658325
Validation loss: 2.3451779221975677

Epoch: 5| Step: 4
Training loss: 2.660475254058838
Validation loss: 2.3405311466545187

Epoch: 5| Step: 5
Training loss: 2.2695584297180176
Validation loss: 2.3321127917176936

Epoch: 5| Step: 6
Training loss: 2.531740665435791
Validation loss: 2.3281083260813067

Epoch: 5| Step: 7
Training loss: 2.690830945968628
Validation loss: 2.3206771560894546

Epoch: 5| Step: 8
Training loss: 3.000326156616211
Validation loss: 2.317810002193656

Epoch: 5| Step: 9
Training loss: 2.015038251876831
Validation loss: 2.3705604794204875

Epoch: 5| Step: 10
Training loss: 2.5893311500549316
Validation loss: 2.4347952847839682

Epoch: 61| Step: 0
Training loss: 1.5561331510543823
Validation loss: 2.5301334883577082

Epoch: 5| Step: 1
Training loss: 3.3411953449249268
Validation loss: 2.6438546488361974

Epoch: 5| Step: 2
Training loss: 1.7984224557876587
Validation loss: 2.5628300892409457

Epoch: 5| Step: 3
Training loss: 2.464606761932373
Validation loss: 2.472225971119378

Epoch: 5| Step: 4
Training loss: 2.922241687774658
Validation loss: 2.38969684672612

Epoch: 5| Step: 5
Training loss: 2.4244372844696045
Validation loss: 2.3336242398908063

Epoch: 5| Step: 6
Training loss: 2.6095480918884277
Validation loss: 2.3213348747581564

Epoch: 5| Step: 7
Training loss: 2.5465872287750244
Validation loss: 2.337099234263102

Epoch: 5| Step: 8
Training loss: 2.985659122467041
Validation loss: 2.3665258166610554

Epoch: 5| Step: 9
Training loss: 3.3629627227783203
Validation loss: 2.3376447026447584

Epoch: 5| Step: 10
Training loss: 3.05932354927063
Validation loss: 2.331299202416533

Epoch: 62| Step: 0
Training loss: 2.2439136505126953
Validation loss: 2.3224195024018646

Epoch: 5| Step: 1
Training loss: 3.1056182384490967
Validation loss: 2.321277977317892

Epoch: 5| Step: 2
Training loss: 2.3198390007019043
Validation loss: 2.3354334267236854

Epoch: 5| Step: 3
Training loss: 2.6523823738098145
Validation loss: 2.353466440272588

Epoch: 5| Step: 4
Training loss: 2.290677547454834
Validation loss: 2.356298892728744

Epoch: 5| Step: 5
Training loss: 2.892050266265869
Validation loss: 2.354828452551237

Epoch: 5| Step: 6
Training loss: 2.9508728981018066
Validation loss: 2.3275957556181055

Epoch: 5| Step: 7
Training loss: 2.797610282897949
Validation loss: 2.328799666896943

Epoch: 5| Step: 8
Training loss: 2.893489122390747
Validation loss: 2.32487053255881

Epoch: 5| Step: 9
Training loss: 1.8353732824325562
Validation loss: 2.3196973698113554

Epoch: 5| Step: 10
Training loss: 2.594191551208496
Validation loss: 2.315602315369473

Epoch: 63| Step: 0
Training loss: 3.150289297103882
Validation loss: 2.3076944299923476

Epoch: 5| Step: 1
Training loss: 2.3509092330932617
Validation loss: 2.3139199749115975

Epoch: 5| Step: 2
Training loss: 2.979435443878174
Validation loss: 2.3100868373788814

Epoch: 5| Step: 3
Training loss: 3.062283992767334
Validation loss: 2.305110216140747

Epoch: 5| Step: 4
Training loss: 2.3297007083892822
Validation loss: 2.3102818791584303

Epoch: 5| Step: 5
Training loss: 2.6223201751708984
Validation loss: 2.307364945770592

Epoch: 5| Step: 6
Training loss: 2.547607183456421
Validation loss: 2.3083924939555507

Epoch: 5| Step: 7
Training loss: 2.091428279876709
Validation loss: 2.315968459652316

Epoch: 5| Step: 8
Training loss: 2.682933807373047
Validation loss: 2.3139942128171205

Epoch: 5| Step: 9
Training loss: 2.804527759552002
Validation loss: 2.3149301339221258

Epoch: 5| Step: 10
Training loss: 1.7850087881088257
Validation loss: 2.312408449829266

Epoch: 64| Step: 0
Training loss: 2.5034756660461426
Validation loss: 2.3140894059211976

Epoch: 5| Step: 1
Training loss: 2.373269557952881
Validation loss: 2.3181070076522006

Epoch: 5| Step: 2
Training loss: 2.678713321685791
Validation loss: 2.318422258541148

Epoch: 5| Step: 3
Training loss: 2.1532552242279053
Validation loss: 2.3307930987368346

Epoch: 5| Step: 4
Training loss: 3.098818778991699
Validation loss: 2.3343279361724854

Epoch: 5| Step: 5
Training loss: 2.521920680999756
Validation loss: 2.3279658056074575

Epoch: 5| Step: 6
Training loss: 2.7161402702331543
Validation loss: 2.3110351254863124

Epoch: 5| Step: 7
Training loss: 2.4673008918762207
Validation loss: 2.309194718637774

Epoch: 5| Step: 8
Training loss: 3.024449586868286
Validation loss: 2.3034864702532367

Epoch: 5| Step: 9
Training loss: 2.895427703857422
Validation loss: 2.308707168025355

Epoch: 5| Step: 10
Training loss: 1.866831660270691
Validation loss: 2.301954871864729

Epoch: 65| Step: 0
Training loss: 3.2440109252929688
Validation loss: 2.307141588580224

Epoch: 5| Step: 1
Training loss: 2.3937172889709473
Validation loss: 2.303244685613981

Epoch: 5| Step: 2
Training loss: 1.955814003944397
Validation loss: 2.3044875385940715

Epoch: 5| Step: 3
Training loss: 3.164116621017456
Validation loss: 2.3116368503980738

Epoch: 5| Step: 4
Training loss: 3.0148608684539795
Validation loss: 2.3298629881233297

Epoch: 5| Step: 5
Training loss: 2.339186429977417
Validation loss: 2.3338474381354546

Epoch: 5| Step: 6
Training loss: 2.3856842517852783
Validation loss: 2.3304690135422574

Epoch: 5| Step: 7
Training loss: 2.7638258934020996
Validation loss: 2.3380108623094458

Epoch: 5| Step: 8
Training loss: 2.6143202781677246
Validation loss: 2.332873700767435

Epoch: 5| Step: 9
Training loss: 2.3048272132873535
Validation loss: 2.3386442763831026

Epoch: 5| Step: 10
Training loss: 2.248642683029175
Validation loss: 2.354801021596437

Epoch: 66| Step: 0
Training loss: 3.065845012664795
Validation loss: 2.3545802126648607

Epoch: 5| Step: 1
Training loss: 2.119597911834717
Validation loss: 2.36651195761978

Epoch: 5| Step: 2
Training loss: 2.7229561805725098
Validation loss: 2.3637595894516155

Epoch: 5| Step: 3
Training loss: 1.9937429428100586
Validation loss: 2.3734219997159895

Epoch: 5| Step: 4
Training loss: 3.0545225143432617
Validation loss: 2.3592919098433627

Epoch: 5| Step: 5
Training loss: 1.9140465259552002
Validation loss: 2.350950192379695

Epoch: 5| Step: 6
Training loss: 2.787444829940796
Validation loss: 2.3308795062444543

Epoch: 5| Step: 7
Training loss: 2.2534806728363037
Validation loss: 2.3150687499712874

Epoch: 5| Step: 8
Training loss: 2.6548171043395996
Validation loss: 2.2995456444319857

Epoch: 5| Step: 9
Training loss: 3.079897403717041
Validation loss: 2.3009308179219565

Epoch: 5| Step: 10
Training loss: 2.803040027618408
Validation loss: 2.2930489663154847

Epoch: 67| Step: 0
Training loss: 2.6480979919433594
Validation loss: 2.2940200708245717

Epoch: 5| Step: 1
Training loss: 1.8721424341201782
Validation loss: 2.2989485110006025

Epoch: 5| Step: 2
Training loss: 2.4474997520446777
Validation loss: 2.314033241682155

Epoch: 5| Step: 3
Training loss: 2.507323741912842
Validation loss: 2.3153568185785764

Epoch: 5| Step: 4
Training loss: 2.6278648376464844
Validation loss: 2.3163048516037645

Epoch: 5| Step: 5
Training loss: 3.061055898666382
Validation loss: 2.3175879422054497

Epoch: 5| Step: 6
Training loss: 2.4569668769836426
Validation loss: 2.311204312950052

Epoch: 5| Step: 7
Training loss: 2.543109178543091
Validation loss: 2.3096548408590336

Epoch: 5| Step: 8
Training loss: 2.938936233520508
Validation loss: 2.2943824183556343

Epoch: 5| Step: 9
Training loss: 2.4180266857147217
Validation loss: 2.2884579961017897

Epoch: 5| Step: 10
Training loss: 3.0178277492523193
Validation loss: 2.2851900939018495

Epoch: 68| Step: 0
Training loss: 2.090031862258911
Validation loss: 2.2789269544745006

Epoch: 5| Step: 1
Training loss: 2.9665427207946777
Validation loss: 2.2750457563707904

Epoch: 5| Step: 2
Training loss: 2.986731767654419
Validation loss: 2.309691157392276

Epoch: 5| Step: 3
Training loss: 3.1887500286102295
Validation loss: 2.32242537313892

Epoch: 5| Step: 4
Training loss: 2.6551735401153564
Validation loss: 2.332430283228556

Epoch: 5| Step: 5
Training loss: 2.3023266792297363
Validation loss: 2.332805892472626

Epoch: 5| Step: 6
Training loss: 2.327481508255005
Validation loss: 2.313221521275018

Epoch: 5| Step: 7
Training loss: 2.029160737991333
Validation loss: 2.2921696683411956

Epoch: 5| Step: 8
Training loss: 2.5782814025878906
Validation loss: 2.2767686023507068

Epoch: 5| Step: 9
Training loss: 3.0579922199249268
Validation loss: 2.2728424354266097

Epoch: 5| Step: 10
Training loss: 2.2201993465423584
Validation loss: 2.273843196130568

Epoch: 69| Step: 0
Training loss: 2.549877643585205
Validation loss: 2.2700625696489887

Epoch: 5| Step: 1
Training loss: 2.4203906059265137
Validation loss: 2.2748109820068523

Epoch: 5| Step: 2
Training loss: 3.0593745708465576
Validation loss: 2.2832950212622203

Epoch: 5| Step: 3
Training loss: 2.715552806854248
Validation loss: 2.278568620322853

Epoch: 5| Step: 4
Training loss: 2.620990037918091
Validation loss: 2.276383343563285

Epoch: 5| Step: 5
Training loss: 2.549220561981201
Validation loss: 2.275065811731482

Epoch: 5| Step: 6
Training loss: 2.2963366508483887
Validation loss: 2.267866711462698

Epoch: 5| Step: 7
Training loss: 2.901745557785034
Validation loss: 2.2727781572649555

Epoch: 5| Step: 8
Training loss: 2.7846949100494385
Validation loss: 2.2703602724177863

Epoch: 5| Step: 9
Training loss: 2.659513473510742
Validation loss: 2.2826401674619285

Epoch: 5| Step: 10
Training loss: 1.6410013437271118
Validation loss: 2.280557588864398

Epoch: 70| Step: 0
Training loss: 2.5814595222473145
Validation loss: 2.294414858664236

Epoch: 5| Step: 1
Training loss: 2.3529062271118164
Validation loss: 2.309469525532056

Epoch: 5| Step: 2
Training loss: 2.671940326690674
Validation loss: 2.3189804784713255

Epoch: 5| Step: 3
Training loss: 2.0045289993286133
Validation loss: 2.322165363578386

Epoch: 5| Step: 4
Training loss: 3.189619302749634
Validation loss: 2.328294739928297

Epoch: 5| Step: 5
Training loss: 2.486268997192383
Validation loss: 2.305811966619184

Epoch: 5| Step: 6
Training loss: 2.2661566734313965
Validation loss: 2.31763469788336

Epoch: 5| Step: 7
Training loss: 2.5439839363098145
Validation loss: 2.3252104533615934

Epoch: 5| Step: 8
Training loss: 2.7453866004943848
Validation loss: 2.3373844597929265

Epoch: 5| Step: 9
Training loss: 2.2521250247955322
Validation loss: 2.34687896954116

Epoch: 5| Step: 10
Training loss: 3.3309528827667236
Validation loss: 2.390085921492628

Epoch: 71| Step: 0
Training loss: 2.060105800628662
Validation loss: 2.401251582689183

Epoch: 5| Step: 1
Training loss: 3.109030246734619
Validation loss: 2.3859517984492804

Epoch: 5| Step: 2
Training loss: 3.1807098388671875
Validation loss: 2.35712105484419

Epoch: 5| Step: 3
Training loss: 2.473130464553833
Validation loss: 2.3305617609331684

Epoch: 5| Step: 4
Training loss: 2.712934970855713
Validation loss: 2.297076604699576

Epoch: 5| Step: 5
Training loss: 2.387180805206299
Validation loss: 2.295438443460772

Epoch: 5| Step: 6
Training loss: 2.673569679260254
Validation loss: 2.2821062482813352

Epoch: 5| Step: 7
Training loss: 2.885289430618286
Validation loss: 2.2642936629633748

Epoch: 5| Step: 8
Training loss: 2.08229923248291
Validation loss: 2.2664720319932505

Epoch: 5| Step: 9
Training loss: 2.06807541847229
Validation loss: 2.2634975089821765

Epoch: 5| Step: 10
Training loss: 2.716373920440674
Validation loss: 2.263927913481189

Epoch: 72| Step: 0
Training loss: 2.577669620513916
Validation loss: 2.257930650505968

Epoch: 5| Step: 1
Training loss: 2.745429277420044
Validation loss: 2.26332345316487

Epoch: 5| Step: 2
Training loss: 2.952359676361084
Validation loss: 2.260081119434808

Epoch: 5| Step: 3
Training loss: 2.5641016960144043
Validation loss: 2.2685098789071523

Epoch: 5| Step: 4
Training loss: 2.090224027633667
Validation loss: 2.2595194667898197

Epoch: 5| Step: 5
Training loss: 2.9409873485565186
Validation loss: 2.2619317821277085

Epoch: 5| Step: 6
Training loss: 1.7453978061676025
Validation loss: 2.2774469467901413

Epoch: 5| Step: 7
Training loss: 2.3729195594787598
Validation loss: 2.2710858493722896

Epoch: 5| Step: 8
Training loss: 2.3572452068328857
Validation loss: 2.2767152991346133

Epoch: 5| Step: 9
Training loss: 2.957432270050049
Validation loss: 2.293516984549902

Epoch: 5| Step: 10
Training loss: 2.881500005722046
Validation loss: 2.288597826034792

Epoch: 73| Step: 0
Training loss: 2.506840467453003
Validation loss: 2.2698128364419423

Epoch: 5| Step: 1
Training loss: 2.4768805503845215
Validation loss: 2.2603167026273665

Epoch: 5| Step: 2
Training loss: 2.4735941886901855
Validation loss: 2.273010346197313

Epoch: 5| Step: 3
Training loss: 2.53125
Validation loss: 2.2761410769595893

Epoch: 5| Step: 4
Training loss: 2.771679639816284
Validation loss: 2.2903013152460896

Epoch: 5| Step: 5
Training loss: 2.366313934326172
Validation loss: 2.2789221091936995

Epoch: 5| Step: 6
Training loss: 2.4812774658203125
Validation loss: 2.2773424451069166

Epoch: 5| Step: 7
Training loss: 2.478147029876709
Validation loss: 2.2743610720480643

Epoch: 5| Step: 8
Training loss: 2.1896560192108154
Validation loss: 2.275804901635775

Epoch: 5| Step: 9
Training loss: 3.320322036743164
Validation loss: 2.264357941125029

Epoch: 5| Step: 10
Training loss: 2.419247627258301
Validation loss: 2.2570211105449225

Epoch: 74| Step: 0
Training loss: 2.3753223419189453
Validation loss: 2.2573219383916547

Epoch: 5| Step: 1
Training loss: 2.328563928604126
Validation loss: 2.2720363217015422

Epoch: 5| Step: 2
Training loss: 2.1966097354888916
Validation loss: 2.2720560002070602

Epoch: 5| Step: 3
Training loss: 3.1541903018951416
Validation loss: 2.266373657411145

Epoch: 5| Step: 4
Training loss: 2.206876277923584
Validation loss: 2.261033078675629

Epoch: 5| Step: 5
Training loss: 3.1928563117980957
Validation loss: 2.2600163105995423

Epoch: 5| Step: 6
Training loss: 2.5081169605255127
Validation loss: 2.2728860480811006

Epoch: 5| Step: 7
Training loss: 3.2348523139953613
Validation loss: 2.280798537756807

Epoch: 5| Step: 8
Training loss: 2.0969910621643066
Validation loss: 2.2788616995657645

Epoch: 5| Step: 9
Training loss: 2.206390142440796
Validation loss: 2.2857439441065632

Epoch: 5| Step: 10
Training loss: 2.4779090881347656
Validation loss: 2.3365938842937513

Epoch: 75| Step: 0
Training loss: 2.1654045581817627
Validation loss: 2.3344502884854554

Epoch: 5| Step: 1
Training loss: 2.3766422271728516
Validation loss: 2.316662352572205

Epoch: 5| Step: 2
Training loss: 2.2397618293762207
Validation loss: 2.2794956584130563

Epoch: 5| Step: 3
Training loss: 2.418175220489502
Validation loss: 2.270425328644373

Epoch: 5| Step: 4
Training loss: 2.8040385246276855
Validation loss: 2.2561884387846916

Epoch: 5| Step: 5
Training loss: 3.027750015258789
Validation loss: 2.2479761210821008

Epoch: 5| Step: 6
Training loss: 2.487924098968506
Validation loss: 2.2315534417347243

Epoch: 5| Step: 7
Training loss: 2.083768367767334
Validation loss: 2.2348114546909126

Epoch: 5| Step: 8
Training loss: 2.4976584911346436
Validation loss: 2.236199443058301

Epoch: 5| Step: 9
Training loss: 2.8918297290802
Validation loss: 2.2391194374330583

Epoch: 5| Step: 10
Training loss: 2.9673938751220703
Validation loss: 2.2358957285522134

Epoch: 76| Step: 0
Training loss: 3.190892457962036
Validation loss: 2.235964705867152

Epoch: 5| Step: 1
Training loss: 2.373171091079712
Validation loss: 2.2350731741997505

Epoch: 5| Step: 2
Training loss: 2.022881269454956
Validation loss: 2.2352392878583682

Epoch: 5| Step: 3
Training loss: 2.7006428241729736
Validation loss: 2.2352075346054567

Epoch: 5| Step: 4
Training loss: 2.808331251144409
Validation loss: 2.2403240665312736

Epoch: 5| Step: 5
Training loss: 2.380845069885254
Validation loss: 2.236601303982478

Epoch: 5| Step: 6
Training loss: 1.6583350896835327
Validation loss: 2.237224071256576

Epoch: 5| Step: 7
Training loss: 2.298027753829956
Validation loss: 2.232513935335221

Epoch: 5| Step: 8
Training loss: 3.029202699661255
Validation loss: 2.232887916667487

Epoch: 5| Step: 9
Training loss: 2.411170482635498
Validation loss: 2.2447640075478503

Epoch: 5| Step: 10
Training loss: 3.093498706817627
Validation loss: 2.2564802785073557

Epoch: 77| Step: 0
Training loss: 2.2194859981536865
Validation loss: 2.259352922439575

Epoch: 5| Step: 1
Training loss: 2.8183681964874268
Validation loss: 2.247769445501348

Epoch: 5| Step: 2
Training loss: 2.3667378425598145
Validation loss: 2.239824738553775

Epoch: 5| Step: 3
Training loss: 3.1156764030456543
Validation loss: 2.2495562286787134

Epoch: 5| Step: 4
Training loss: 2.1263277530670166
Validation loss: 2.255649043667701

Epoch: 5| Step: 5
Training loss: 2.5533738136291504
Validation loss: 2.298646483370053

Epoch: 5| Step: 6
Training loss: 1.7942945957183838
Validation loss: 2.3107604108830935

Epoch: 5| Step: 7
Training loss: 2.8604912757873535
Validation loss: 2.3127072395816928

Epoch: 5| Step: 8
Training loss: 3.1381969451904297
Validation loss: 2.2687531081579064

Epoch: 5| Step: 9
Training loss: 2.4812979698181152
Validation loss: 2.2340399449871433

Epoch: 5| Step: 10
Training loss: 2.410501480102539
Validation loss: 2.227109985966836

Epoch: 78| Step: 0
Training loss: 2.244455575942993
Validation loss: 2.228682453914355

Epoch: 5| Step: 1
Training loss: 2.4313418865203857
Validation loss: 2.2282208011996363

Epoch: 5| Step: 2
Training loss: 2.6762595176696777
Validation loss: 2.2297600392372376

Epoch: 5| Step: 3
Training loss: 2.801636219024658
Validation loss: 2.2347441065695977

Epoch: 5| Step: 4
Training loss: 2.532867670059204
Validation loss: 2.237302521223663

Epoch: 5| Step: 5
Training loss: 2.382516384124756
Validation loss: 2.240537956196775

Epoch: 5| Step: 6
Training loss: 1.9832274913787842
Validation loss: 2.2543438455109954

Epoch: 5| Step: 7
Training loss: 2.371432065963745
Validation loss: 2.253372415419548

Epoch: 5| Step: 8
Training loss: 2.9475083351135254
Validation loss: 2.2471903113908667

Epoch: 5| Step: 9
Training loss: 3.096362590789795
Validation loss: 2.2405242919921875

Epoch: 5| Step: 10
Training loss: 2.426865816116333
Validation loss: 2.2452690037347938

Epoch: 79| Step: 0
Training loss: 2.041379451751709
Validation loss: 2.267319853587817

Epoch: 5| Step: 1
Training loss: 2.5950348377227783
Validation loss: 2.2961295650851343

Epoch: 5| Step: 2
Training loss: 3.074845552444458
Validation loss: 2.27815164801895

Epoch: 5| Step: 3
Training loss: 2.281317710876465
Validation loss: 2.256609309104181

Epoch: 5| Step: 4
Training loss: 2.316748857498169
Validation loss: 2.2536398723561275

Epoch: 5| Step: 5
Training loss: 2.850338935852051
Validation loss: 2.2449784830052364

Epoch: 5| Step: 6
Training loss: 2.5706019401550293
Validation loss: 2.230314814916221

Epoch: 5| Step: 7
Training loss: 2.923614501953125
Validation loss: 2.2312034445424236

Epoch: 5| Step: 8
Training loss: 2.808234453201294
Validation loss: 2.239468284832534

Epoch: 5| Step: 9
Training loss: 1.727687120437622
Validation loss: 2.2574908246276197

Epoch: 5| Step: 10
Training loss: 2.8069565296173096
Validation loss: 2.2547142121099655

Epoch: 80| Step: 0
Training loss: 2.6253011226654053
Validation loss: 2.2748822691620036

Epoch: 5| Step: 1
Training loss: 2.271030902862549
Validation loss: 2.2597662197646273

Epoch: 5| Step: 2
Training loss: 2.0985987186431885
Validation loss: 2.247151187671128

Epoch: 5| Step: 3
Training loss: 2.2307865619659424
Validation loss: 2.2299339155997

Epoch: 5| Step: 4
Training loss: 2.1689646244049072
Validation loss: 2.2308143005576184

Epoch: 5| Step: 5
Training loss: 3.4492545127868652
Validation loss: 2.2326690355936685

Epoch: 5| Step: 6
Training loss: 2.70869779586792
Validation loss: 2.232219744754094

Epoch: 5| Step: 7
Training loss: 2.9067184925079346
Validation loss: 2.23162353679698

Epoch: 5| Step: 8
Training loss: 2.8773558139801025
Validation loss: 2.2282972912634573

Epoch: 5| Step: 9
Training loss: 2.0814507007598877
Validation loss: 2.2277944446891866

Epoch: 5| Step: 10
Training loss: 2.2512261867523193
Validation loss: 2.2368363975196757

Epoch: 81| Step: 0
Training loss: 2.2171988487243652
Validation loss: 2.246632178624471

Epoch: 5| Step: 1
Training loss: 2.6873526573181152
Validation loss: 2.241688616814152

Epoch: 5| Step: 2
Training loss: 3.308015823364258
Validation loss: 2.2601129188332507

Epoch: 5| Step: 3
Training loss: 1.5245389938354492
Validation loss: 2.2760773679261566

Epoch: 5| Step: 4
Training loss: 2.737515926361084
Validation loss: 2.286534352969098

Epoch: 5| Step: 5
Training loss: 2.402799129486084
Validation loss: 2.294990142186483

Epoch: 5| Step: 6
Training loss: 2.7483062744140625
Validation loss: 2.3256058000749156

Epoch: 5| Step: 7
Training loss: 2.522810697555542
Validation loss: 2.3371102040813816

Epoch: 5| Step: 8
Training loss: 2.8002266883850098
Validation loss: 2.36873387264949

Epoch: 5| Step: 9
Training loss: 2.015052318572998
Validation loss: 2.3875944947683685

Epoch: 5| Step: 10
Training loss: 3.148502826690674
Validation loss: 2.3734417448761644

Epoch: 82| Step: 0
Training loss: 1.7193858623504639
Validation loss: 2.3082507579557356

Epoch: 5| Step: 1
Training loss: 2.6617696285247803
Validation loss: 2.2653088390186267

Epoch: 5| Step: 2
Training loss: 3.0001285076141357
Validation loss: 2.2757224728984218

Epoch: 5| Step: 3
Training loss: 2.3711953163146973
Validation loss: 2.285736891531175

Epoch: 5| Step: 4
Training loss: 2.512317657470703
Validation loss: 2.302791997950564

Epoch: 5| Step: 5
Training loss: 2.9629547595977783
Validation loss: 2.3253890134954966

Epoch: 5| Step: 6
Training loss: 2.5246498584747314
Validation loss: 2.3258739543217484

Epoch: 5| Step: 7
Training loss: 2.05548095703125
Validation loss: 2.29578705628713

Epoch: 5| Step: 8
Training loss: 2.6060879230499268
Validation loss: 2.272404804024645

Epoch: 5| Step: 9
Training loss: 2.5643131732940674
Validation loss: 2.2274494504415863

Epoch: 5| Step: 10
Training loss: 3.230285882949829
Validation loss: 2.208173967176868

Epoch: 83| Step: 0
Training loss: 1.9440759420394897
Validation loss: 2.208925708647697

Epoch: 5| Step: 1
Training loss: 2.680591344833374
Validation loss: 2.254829575938563

Epoch: 5| Step: 2
Training loss: 2.4918053150177
Validation loss: 2.340548746047481

Epoch: 5| Step: 3
Training loss: 2.79380202293396
Validation loss: 2.4329038179048927

Epoch: 5| Step: 4
Training loss: 2.7268757820129395
Validation loss: 2.370283244758524

Epoch: 5| Step: 5
Training loss: 2.590808391571045
Validation loss: 2.334675024914485

Epoch: 5| Step: 6
Training loss: 2.2399048805236816
Validation loss: 2.2485602760827668

Epoch: 5| Step: 7
Training loss: 2.5978200435638428
Validation loss: 2.209535619264008

Epoch: 5| Step: 8
Training loss: 2.7912917137145996
Validation loss: 2.19882401856043

Epoch: 5| Step: 9
Training loss: 2.6181657314300537
Validation loss: 2.197078489488171

Epoch: 5| Step: 10
Training loss: 2.418578624725342
Validation loss: 2.202228815324845

Epoch: 84| Step: 0
Training loss: 2.36795711517334
Validation loss: 2.207460276542171

Epoch: 5| Step: 1
Training loss: 3.1221089363098145
Validation loss: 2.203836905058994

Epoch: 5| Step: 2
Training loss: 2.7284293174743652
Validation loss: 2.194719478648196

Epoch: 5| Step: 3
Training loss: 2.027315139770508
Validation loss: 2.191404906652307

Epoch: 5| Step: 4
Training loss: 2.8030686378479004
Validation loss: 2.186656782704015

Epoch: 5| Step: 5
Training loss: 2.10034441947937
Validation loss: 2.1909620710598525

Epoch: 5| Step: 6
Training loss: 2.1026244163513184
Validation loss: 2.199342619988226

Epoch: 5| Step: 7
Training loss: 2.6765482425689697
Validation loss: 2.218757819103938

Epoch: 5| Step: 8
Training loss: 2.2770802974700928
Validation loss: 2.252207686824183

Epoch: 5| Step: 9
Training loss: 3.042672634124756
Validation loss: 2.2807280607120965

Epoch: 5| Step: 10
Training loss: 2.223522901535034
Validation loss: 2.2660000221703642

Epoch: 85| Step: 0
Training loss: 2.358565330505371
Validation loss: 2.2586410199442217

Epoch: 5| Step: 1
Training loss: 2.0947108268737793
Validation loss: 2.263724127123433

Epoch: 5| Step: 2
Training loss: 1.7193225622177124
Validation loss: 2.2513568657700733

Epoch: 5| Step: 3
Training loss: 3.4631359577178955
Validation loss: 2.2500803034792662

Epoch: 5| Step: 4
Training loss: 2.227808713912964
Validation loss: 2.235254773529627

Epoch: 5| Step: 5
Training loss: 2.134580135345459
Validation loss: 2.2224421885705765

Epoch: 5| Step: 6
Training loss: 3.3684680461883545
Validation loss: 2.221528878775976

Epoch: 5| Step: 7
Training loss: 2.5742342472076416
Validation loss: 2.2001165215687086

Epoch: 5| Step: 8
Training loss: 2.0435144901275635
Validation loss: 2.2129348452373216

Epoch: 5| Step: 9
Training loss: 2.7893664836883545
Validation loss: 2.209331925197314

Epoch: 5| Step: 10
Training loss: 2.849343776702881
Validation loss: 2.2158190704161123

Epoch: 86| Step: 0
Training loss: 2.6299495697021484
Validation loss: 2.206047417015158

Epoch: 5| Step: 1
Training loss: 2.2643401622772217
Validation loss: 2.207530495940998

Epoch: 5| Step: 2
Training loss: 2.123396873474121
Validation loss: 2.1991196550348753

Epoch: 5| Step: 3
Training loss: 2.259089231491089
Validation loss: 2.196589628855387

Epoch: 5| Step: 4
Training loss: 2.205026388168335
Validation loss: 2.186135866308725

Epoch: 5| Step: 5
Training loss: 2.5185391902923584
Validation loss: 2.1869925401544057

Epoch: 5| Step: 6
Training loss: 2.374910354614258
Validation loss: 2.205302398691895

Epoch: 5| Step: 7
Training loss: 3.0089733600616455
Validation loss: 2.2233576364414667

Epoch: 5| Step: 8
Training loss: 2.528806209564209
Validation loss: 2.2306512145585913

Epoch: 5| Step: 9
Training loss: 3.321207046508789
Validation loss: 2.2197918033087127

Epoch: 5| Step: 10
Training loss: 2.2983458042144775
Validation loss: 2.219372680110316

Epoch: 87| Step: 0
Training loss: 2.531484365463257
Validation loss: 2.2253646389130624

Epoch: 5| Step: 1
Training loss: 2.3842272758483887
Validation loss: 2.219748773882466

Epoch: 5| Step: 2
Training loss: 2.6645991802215576
Validation loss: 2.224227930909844

Epoch: 5| Step: 3
Training loss: 2.501462936401367
Validation loss: 2.222383206890475

Epoch: 5| Step: 4
Training loss: 2.3194468021392822
Validation loss: 2.223531771731633

Epoch: 5| Step: 5
Training loss: 2.559014081954956
Validation loss: 2.2188848885156776

Epoch: 5| Step: 6
Training loss: 2.0700669288635254
Validation loss: 2.2029495905804377

Epoch: 5| Step: 7
Training loss: 2.5903897285461426
Validation loss: 2.2028755526388846

Epoch: 5| Step: 8
Training loss: 2.6483449935913086
Validation loss: 2.206491478027836

Epoch: 5| Step: 9
Training loss: 2.5421321392059326
Validation loss: 2.197224629822598

Epoch: 5| Step: 10
Training loss: 2.3952794075012207
Validation loss: 2.1993107693169707

Epoch: 88| Step: 0
Training loss: 2.0136680603027344
Validation loss: 2.194327646686185

Epoch: 5| Step: 1
Training loss: 2.2670984268188477
Validation loss: 2.190578427366031

Epoch: 5| Step: 2
Training loss: 2.0918385982513428
Validation loss: 2.1894769322487617

Epoch: 5| Step: 3
Training loss: 2.280668020248413
Validation loss: 2.1934626230629544

Epoch: 5| Step: 4
Training loss: 2.225569248199463
Validation loss: 2.2008407538936985

Epoch: 5| Step: 5
Training loss: 3.070425510406494
Validation loss: 2.208974556256366

Epoch: 5| Step: 6
Training loss: 2.5570900440216064
Validation loss: 2.2249719532587195

Epoch: 5| Step: 7
Training loss: 2.945044994354248
Validation loss: 2.220127074949203

Epoch: 5| Step: 8
Training loss: 2.9962475299835205
Validation loss: 2.2178976561433528

Epoch: 5| Step: 9
Training loss: 2.573716640472412
Validation loss: 2.2288517721237673

Epoch: 5| Step: 10
Training loss: 2.2424631118774414
Validation loss: 2.25701646138263

Epoch: 89| Step: 0
Training loss: 2.548478364944458
Validation loss: 2.2398319039293515

Epoch: 5| Step: 1
Training loss: 2.418191432952881
Validation loss: 2.1986541671137654

Epoch: 5| Step: 2
Training loss: 2.5490286350250244
Validation loss: 2.177010072174893

Epoch: 5| Step: 3
Training loss: 2.0270354747772217
Validation loss: 2.1837712372502973

Epoch: 5| Step: 4
Training loss: 2.2063262462615967
Validation loss: 2.1974047819773355

Epoch: 5| Step: 5
Training loss: 2.606790542602539
Validation loss: 2.235429161338396

Epoch: 5| Step: 6
Training loss: 2.5775067806243896
Validation loss: 2.270665325144286

Epoch: 5| Step: 7
Training loss: 2.853963613510132
Validation loss: 2.2894684037854596

Epoch: 5| Step: 8
Training loss: 2.8351035118103027
Validation loss: 2.316783074409731

Epoch: 5| Step: 9
Training loss: 3.060692548751831
Validation loss: 2.2926882928417576

Epoch: 5| Step: 10
Training loss: 1.9825001955032349
Validation loss: 2.252141137276926

Epoch: 90| Step: 0
Training loss: 3.272174119949341
Validation loss: 2.2284454402103218

Epoch: 5| Step: 1
Training loss: 2.3220345973968506
Validation loss: 2.2108446628816667

Epoch: 5| Step: 2
Training loss: 2.732945203781128
Validation loss: 2.19188246932081

Epoch: 5| Step: 3
Training loss: 2.295009136199951
Validation loss: 2.1809418086082704

Epoch: 5| Step: 4
Training loss: 2.9397900104522705
Validation loss: 2.1711555783466627

Epoch: 5| Step: 5
Training loss: 1.8874473571777344
Validation loss: 2.1698365570396505

Epoch: 5| Step: 6
Training loss: 3.0083956718444824
Validation loss: 2.1726076884936263

Epoch: 5| Step: 7
Training loss: 2.0455574989318848
Validation loss: 2.167850499512047

Epoch: 5| Step: 8
Training loss: 2.1998112201690674
Validation loss: 2.180466910844208

Epoch: 5| Step: 9
Training loss: 2.633249282836914
Validation loss: 2.1861572932171565

Epoch: 5| Step: 10
Training loss: 2.2863481044769287
Validation loss: 2.214805959373392

Epoch: 91| Step: 0
Training loss: 2.3547770977020264
Validation loss: 2.2147326700149046

Epoch: 5| Step: 1
Training loss: 2.664890766143799
Validation loss: 2.2200794514789375

Epoch: 5| Step: 2
Training loss: 2.201561689376831
Validation loss: 2.2501744583088863

Epoch: 5| Step: 3
Training loss: 2.0324058532714844
Validation loss: 2.229532221312164

Epoch: 5| Step: 4
Training loss: 1.723381757736206
Validation loss: 2.2293840351925103

Epoch: 5| Step: 5
Training loss: 2.441140651702881
Validation loss: 2.1913846872186147

Epoch: 5| Step: 6
Training loss: 3.1394588947296143
Validation loss: 2.1686894060463033

Epoch: 5| Step: 7
Training loss: 2.3445897102355957
Validation loss: 2.1545867445648357

Epoch: 5| Step: 8
Training loss: 3.2393527030944824
Validation loss: 2.1543705348045594

Epoch: 5| Step: 9
Training loss: 2.4843125343322754
Validation loss: 2.156993583966327

Epoch: 5| Step: 10
Training loss: 2.647937774658203
Validation loss: 2.1557091807806366

Epoch: 92| Step: 0
Training loss: 2.1786248683929443
Validation loss: 2.155370258515881

Epoch: 5| Step: 1
Training loss: 2.326742172241211
Validation loss: 2.156515554715228

Epoch: 5| Step: 2
Training loss: 2.8897392749786377
Validation loss: 2.1738988135450628

Epoch: 5| Step: 3
Training loss: 2.9045491218566895
Validation loss: 2.2277262210845947

Epoch: 5| Step: 4
Training loss: 2.1133475303649902
Validation loss: 2.251293400282501

Epoch: 5| Step: 5
Training loss: 2.7609124183654785
Validation loss: 2.2503910628698205

Epoch: 5| Step: 6
Training loss: 2.4768295288085938
Validation loss: 2.2426123106351463

Epoch: 5| Step: 7
Training loss: 2.3189408779144287
Validation loss: 2.1887074132119455

Epoch: 5| Step: 8
Training loss: 2.138162612915039
Validation loss: 2.169782487295007

Epoch: 5| Step: 9
Training loss: 2.5153088569641113
Validation loss: 2.1594043726562173

Epoch: 5| Step: 10
Training loss: 2.7110800743103027
Validation loss: 2.1504214425240793

Epoch: 93| Step: 0
Training loss: 2.123119831085205
Validation loss: 2.1494964681645876

Epoch: 5| Step: 1
Training loss: 2.6883087158203125
Validation loss: 2.150908142007807

Epoch: 5| Step: 2
Training loss: 2.749584674835205
Validation loss: 2.1557184765415807

Epoch: 5| Step: 3
Training loss: 2.293426990509033
Validation loss: 2.1511946314124653

Epoch: 5| Step: 4
Training loss: 2.872093677520752
Validation loss: 2.152540845255698

Epoch: 5| Step: 5
Training loss: 2.31630802154541
Validation loss: 2.1591688022818616

Epoch: 5| Step: 6
Training loss: 1.9080692529678345
Validation loss: 2.1793730002577587

Epoch: 5| Step: 7
Training loss: 2.2380852699279785
Validation loss: 2.1832522576855076

Epoch: 5| Step: 8
Training loss: 3.0275096893310547
Validation loss: 2.1873774836140294

Epoch: 5| Step: 9
Training loss: 2.6167542934417725
Validation loss: 2.1923468753855717

Epoch: 5| Step: 10
Training loss: 2.2815160751342773
Validation loss: 2.1779267044477564

Epoch: 94| Step: 0
Training loss: 2.6047720909118652
Validation loss: 2.1811963101868987

Epoch: 5| Step: 1
Training loss: 2.1181507110595703
Validation loss: 2.1858795842816754

Epoch: 5| Step: 2
Training loss: 2.6294972896575928
Validation loss: 2.205689776328302

Epoch: 5| Step: 3
Training loss: 2.251723527908325
Validation loss: 2.2338481923585296

Epoch: 5| Step: 4
Training loss: 2.6729118824005127
Validation loss: 2.257055426156649

Epoch: 5| Step: 5
Training loss: 2.7370073795318604
Validation loss: 2.271347224071462

Epoch: 5| Step: 6
Training loss: 2.63177227973938
Validation loss: 2.235523759677846

Epoch: 5| Step: 7
Training loss: 2.2267379760742188
Validation loss: 2.1949962531366656

Epoch: 5| Step: 8
Training loss: 2.3135921955108643
Validation loss: 2.176923398048647

Epoch: 5| Step: 9
Training loss: 2.515361785888672
Validation loss: 2.162039618338308

Epoch: 5| Step: 10
Training loss: 2.370225667953491
Validation loss: 2.146685328534854

Epoch: 95| Step: 0
Training loss: 2.6266398429870605
Validation loss: 2.1446413891289824

Epoch: 5| Step: 1
Training loss: 2.486400604248047
Validation loss: 2.1458928969598587

Epoch: 5| Step: 2
Training loss: 2.5970547199249268
Validation loss: 2.180737537722434

Epoch: 5| Step: 3
Training loss: 2.373098611831665
Validation loss: 2.1937577442456315

Epoch: 5| Step: 4
Training loss: 2.6026995182037354
Validation loss: 2.211361377469955

Epoch: 5| Step: 5
Training loss: 3.030656576156616
Validation loss: 2.221536510734148

Epoch: 5| Step: 6
Training loss: 2.187297821044922
Validation loss: 2.226316170025897

Epoch: 5| Step: 7
Training loss: 1.9087308645248413
Validation loss: 2.2154534093795286

Epoch: 5| Step: 8
Training loss: 2.6507678031921387
Validation loss: 2.19706464839238

Epoch: 5| Step: 9
Training loss: 2.9717159271240234
Validation loss: 2.173062709070021

Epoch: 5| Step: 10
Training loss: 2.143723964691162
Validation loss: 2.167244104928868

Epoch: 96| Step: 0
Training loss: 2.8656842708587646
Validation loss: 2.1475101529911

Epoch: 5| Step: 1
Training loss: 2.30303955078125
Validation loss: 2.140063161491066

Epoch: 5| Step: 2
Training loss: 2.1606879234313965
Validation loss: 2.1368288378561697

Epoch: 5| Step: 3
Training loss: 2.7382164001464844
Validation loss: 2.1432913785339682

Epoch: 5| Step: 4
Training loss: 3.069019317626953
Validation loss: 2.1543549670968005

Epoch: 5| Step: 5
Training loss: 2.5418813228607178
Validation loss: 2.2016257932109218

Epoch: 5| Step: 6
Training loss: 2.2603302001953125
Validation loss: 2.2334930768577

Epoch: 5| Step: 7
Training loss: 2.3004214763641357
Validation loss: 2.283206303914388

Epoch: 5| Step: 8
Training loss: 2.0022997856140137
Validation loss: 2.2974890983232887

Epoch: 5| Step: 9
Training loss: 2.403327465057373
Validation loss: 2.2626353130545667

Epoch: 5| Step: 10
Training loss: 2.5348894596099854
Validation loss: 2.270521033194757

Epoch: 97| Step: 0
Training loss: 1.8333591222763062
Validation loss: 2.237525277240302

Epoch: 5| Step: 1
Training loss: 2.414506435394287
Validation loss: 2.198593569058244

Epoch: 5| Step: 2
Training loss: 2.99434232711792
Validation loss: 2.1523457368214927

Epoch: 5| Step: 3
Training loss: 2.622044086456299
Validation loss: 2.1296786031415387

Epoch: 5| Step: 4
Training loss: 2.6912310123443604
Validation loss: 2.1268929704543083

Epoch: 5| Step: 5
Training loss: 2.7565293312072754
Validation loss: 2.141445623931064

Epoch: 5| Step: 6
Training loss: 2.1561434268951416
Validation loss: 2.1478183397682766

Epoch: 5| Step: 7
Training loss: 2.3437867164611816
Validation loss: 2.1634502667252735

Epoch: 5| Step: 8
Training loss: 2.4626269340515137
Validation loss: 2.1951011393659856

Epoch: 5| Step: 9
Training loss: 2.6002516746520996
Validation loss: 2.1910846284640733

Epoch: 5| Step: 10
Training loss: 2.5503108501434326
Validation loss: 2.1828726260892806

Epoch: 98| Step: 0
Training loss: 2.7006890773773193
Validation loss: 2.1620022840397333

Epoch: 5| Step: 1
Training loss: 1.9667673110961914
Validation loss: 2.1509422333009782

Epoch: 5| Step: 2
Training loss: 3.015996217727661
Validation loss: 2.145587944215344

Epoch: 5| Step: 3
Training loss: 2.8794045448303223
Validation loss: 2.1338092024608324

Epoch: 5| Step: 4
Training loss: 2.2559585571289062
Validation loss: 2.132538516034362

Epoch: 5| Step: 5
Training loss: 2.871082305908203
Validation loss: 2.128157454152261

Epoch: 5| Step: 6
Training loss: 2.2948436737060547
Validation loss: 2.131662973793604

Epoch: 5| Step: 7
Training loss: 2.4775657653808594
Validation loss: 2.1295006762268724

Epoch: 5| Step: 8
Training loss: 2.2502548694610596
Validation loss: 2.1548666979676936

Epoch: 5| Step: 9
Training loss: 2.330296277999878
Validation loss: 2.1639331130571264

Epoch: 5| Step: 10
Training loss: 1.9998799562454224
Validation loss: 2.1883502467986076

Epoch: 99| Step: 0
Training loss: 2.6143791675567627
Validation loss: 2.244903246561686

Epoch: 5| Step: 1
Training loss: 2.2018985748291016
Validation loss: 2.2658352236593924

Epoch: 5| Step: 2
Training loss: 2.2696568965911865
Validation loss: 2.2779489281356975

Epoch: 5| Step: 3
Training loss: 2.363182783126831
Validation loss: 2.275661381342078

Epoch: 5| Step: 4
Training loss: 2.89284348487854
Validation loss: 2.202893499405153

Epoch: 5| Step: 5
Training loss: 2.4789910316467285
Validation loss: 2.1494667888969503

Epoch: 5| Step: 6
Training loss: 2.085299015045166
Validation loss: 2.1550195114586943

Epoch: 5| Step: 7
Training loss: 2.622515916824341
Validation loss: 2.158361445191086

Epoch: 5| Step: 8
Training loss: 2.675208330154419
Validation loss: 2.169642162579362

Epoch: 5| Step: 9
Training loss: 2.448948383331299
Validation loss: 2.1749298393085437

Epoch: 5| Step: 10
Training loss: 2.4201955795288086
Validation loss: 2.17828784706772

Epoch: 100| Step: 0
Training loss: 2.170431613922119
Validation loss: 2.1717255576964347

Epoch: 5| Step: 1
Training loss: 2.5633232593536377
Validation loss: 2.1664098796024116

Epoch: 5| Step: 2
Training loss: 2.655855178833008
Validation loss: 2.1562793818853234

Epoch: 5| Step: 3
Training loss: 2.4385225772857666
Validation loss: 2.1411384356919156

Epoch: 5| Step: 4
Training loss: 2.202378749847412
Validation loss: 2.138074590313819

Epoch: 5| Step: 5
Training loss: 2.23231840133667
Validation loss: 2.132637554599393

Epoch: 5| Step: 6
Training loss: 2.212754964828491
Validation loss: 2.13411239142059

Epoch: 5| Step: 7
Training loss: 2.639691114425659
Validation loss: 2.130675685021185

Epoch: 5| Step: 8
Training loss: 2.5359864234924316
Validation loss: 2.127188669737949

Epoch: 5| Step: 9
Training loss: 2.6147780418395996
Validation loss: 2.1344029288138113

Epoch: 5| Step: 10
Training loss: 2.7155115604400635
Validation loss: 2.143306465559108

Epoch: 101| Step: 0
Training loss: 2.397066593170166
Validation loss: 2.142707370942639

Epoch: 5| Step: 1
Training loss: 2.3228344917297363
Validation loss: 2.145793076484434

Epoch: 5| Step: 2
Training loss: 3.040191650390625
Validation loss: 2.173996542089729

Epoch: 5| Step: 3
Training loss: 3.0243046283721924
Validation loss: 2.1954142637150262

Epoch: 5| Step: 4
Training loss: 2.7553439140319824
Validation loss: 2.16278741692984

Epoch: 5| Step: 5
Training loss: 1.9782441854476929
Validation loss: 2.13098842354231

Epoch: 5| Step: 6
Training loss: 2.475426197052002
Validation loss: 2.1258625689373223

Epoch: 5| Step: 7
Training loss: 2.063500165939331
Validation loss: 2.129535380230155

Epoch: 5| Step: 8
Training loss: 2.065779209136963
Validation loss: 2.1502583385795675

Epoch: 5| Step: 9
Training loss: 2.214815378189087
Validation loss: 2.173775706239926

Epoch: 5| Step: 10
Training loss: 2.6104094982147217
Validation loss: 2.205224096134145

Epoch: 102| Step: 0
Training loss: 2.4643847942352295
Validation loss: 2.2298346796343402

Epoch: 5| Step: 1
Training loss: 2.311066150665283
Validation loss: 2.2162739435831704

Epoch: 5| Step: 2
Training loss: 2.6505532264709473
Validation loss: 2.1941466305845525

Epoch: 5| Step: 3
Training loss: 2.2210986614227295
Validation loss: 2.1687756148717736

Epoch: 5| Step: 4
Training loss: 1.7228667736053467
Validation loss: 2.1399892043041926

Epoch: 5| Step: 5
Training loss: 3.128086566925049
Validation loss: 2.1306151600294214

Epoch: 5| Step: 6
Training loss: 3.062622547149658
Validation loss: 2.120608710473584

Epoch: 5| Step: 7
Training loss: 2.948720693588257
Validation loss: 2.1073729838094404

Epoch: 5| Step: 8
Training loss: 1.994133710861206
Validation loss: 2.09797231869031

Epoch: 5| Step: 9
Training loss: 2.6388843059539795
Validation loss: 2.0942449108246834

Epoch: 5| Step: 10
Training loss: 2.097561836242676
Validation loss: 2.098077804811539

Epoch: 103| Step: 0
Training loss: 3.0913474559783936
Validation loss: 2.109820012123354

Epoch: 5| Step: 1
Training loss: 2.232229232788086
Validation loss: 2.117183777593797

Epoch: 5| Step: 2
Training loss: 2.504260301589966
Validation loss: 2.162233979471268

Epoch: 5| Step: 3
Training loss: 2.7785582542419434
Validation loss: 2.1945242984320528

Epoch: 5| Step: 4
Training loss: 2.1899945735931396
Validation loss: 2.2533213246253228

Epoch: 5| Step: 5
Training loss: 2.6693029403686523
Validation loss: 2.2904713230748333

Epoch: 5| Step: 6
Training loss: 2.016181707382202
Validation loss: 2.273989592829058

Epoch: 5| Step: 7
Training loss: 2.2292144298553467
Validation loss: 2.23658521457385

Epoch: 5| Step: 8
Training loss: 2.107858180999756
Validation loss: 2.1880293123183714

Epoch: 5| Step: 9
Training loss: 2.7957684993743896
Validation loss: 2.1440334255977342

Epoch: 5| Step: 10
Training loss: 2.375730276107788
Validation loss: 2.1195810994794293

Epoch: 104| Step: 0
Training loss: 1.982619047164917
Validation loss: 2.10638968278003

Epoch: 5| Step: 1
Training loss: 2.540116786956787
Validation loss: 2.1057991494414625

Epoch: 5| Step: 2
Training loss: 3.044649839401245
Validation loss: 2.1222748807681504

Epoch: 5| Step: 3
Training loss: 2.4874978065490723
Validation loss: 2.1439667337684223

Epoch: 5| Step: 4
Training loss: 2.0984082221984863
Validation loss: 2.132311221091978

Epoch: 5| Step: 5
Training loss: 2.698798179626465
Validation loss: 2.1308172172115696

Epoch: 5| Step: 6
Training loss: 2.3106491565704346
Validation loss: 2.1288449918070147

Epoch: 5| Step: 7
Training loss: 2.392183780670166
Validation loss: 2.136204204251689

Epoch: 5| Step: 8
Training loss: 2.5230088233947754
Validation loss: 2.1390549546928814

Epoch: 5| Step: 9
Training loss: 1.9051506519317627
Validation loss: 2.1330745079184092

Epoch: 5| Step: 10
Training loss: 2.876128673553467
Validation loss: 2.129061543813316

Epoch: 105| Step: 0
Training loss: 2.593601942062378
Validation loss: 2.1760802268981934

Epoch: 5| Step: 1
Training loss: 2.2910022735595703
Validation loss: 2.2336757259984172

Epoch: 5| Step: 2
Training loss: 2.398484706878662
Validation loss: 2.239840538271012

Epoch: 5| Step: 3
Training loss: 3.084719181060791
Validation loss: 2.234925131643972

Epoch: 5| Step: 4
Training loss: 2.298006057739258
Validation loss: 2.1992813848680064

Epoch: 5| Step: 5
Training loss: 2.5117263793945312
Validation loss: 2.1761528471464753

Epoch: 5| Step: 6
Training loss: 2.705317974090576
Validation loss: 2.1720037614145586

Epoch: 5| Step: 7
Training loss: 1.7824302911758423
Validation loss: 2.148032795998358

Epoch: 5| Step: 8
Training loss: 2.2747247219085693
Validation loss: 2.1405565764314387

Epoch: 5| Step: 9
Training loss: 2.113788604736328
Validation loss: 2.139774191764093

Epoch: 5| Step: 10
Training loss: 2.9412243366241455
Validation loss: 2.145441484707658

Epoch: 106| Step: 0
Training loss: 2.7143666744232178
Validation loss: 2.1241853057697253

Epoch: 5| Step: 1
Training loss: 2.696303129196167
Validation loss: 2.1065806573437107

Epoch: 5| Step: 2
Training loss: 2.728684902191162
Validation loss: 2.087145241357947

Epoch: 5| Step: 3
Training loss: 1.6561603546142578
Validation loss: 2.0910073646935086

Epoch: 5| Step: 4
Training loss: 2.9369451999664307
Validation loss: 2.108409657273241

Epoch: 5| Step: 5
Training loss: 1.8021351099014282
Validation loss: 2.1038535589812906

Epoch: 5| Step: 6
Training loss: 2.3629708290100098
Validation loss: 2.1122612543003534

Epoch: 5| Step: 7
Training loss: 2.0565989017486572
Validation loss: 2.113091820029802

Epoch: 5| Step: 8
Training loss: 2.3363492488861084
Validation loss: 2.097526809220673

Epoch: 5| Step: 9
Training loss: 2.9175937175750732
Validation loss: 2.116347869237264

Epoch: 5| Step: 10
Training loss: 2.437077522277832
Validation loss: 2.1457864571643133

Epoch: 107| Step: 0
Training loss: 2.016662836074829
Validation loss: 2.2348205991970596

Epoch: 5| Step: 1
Training loss: 2.435699939727783
Validation loss: 2.3211043547558528

Epoch: 5| Step: 2
Training loss: 2.5020229816436768
Validation loss: 2.37351647756433

Epoch: 5| Step: 3
Training loss: 2.8142006397247314
Validation loss: 2.3774273933902865

Epoch: 5| Step: 4
Training loss: 1.8988736867904663
Validation loss: 2.2183648604218678

Epoch: 5| Step: 5
Training loss: 2.8064985275268555
Validation loss: 2.1049736481840893

Epoch: 5| Step: 6
Training loss: 2.924151659011841
Validation loss: 2.0750159781466246

Epoch: 5| Step: 7
Training loss: 2.2420525550842285
Validation loss: 2.120678635053737

Epoch: 5| Step: 8
Training loss: 2.408703088760376
Validation loss: 2.212069324267808

Epoch: 5| Step: 9
Training loss: 2.7862672805786133
Validation loss: 2.3324168369334233

Epoch: 5| Step: 10
Training loss: 3.034440040588379
Validation loss: 2.4266799060247277

Epoch: 108| Step: 0
Training loss: 2.307499647140503
Validation loss: 2.4876983037558933

Epoch: 5| Step: 1
Training loss: 2.353236675262451
Validation loss: 2.517853444622409

Epoch: 5| Step: 2
Training loss: 2.693371295928955
Validation loss: 2.457394105131908

Epoch: 5| Step: 3
Training loss: 2.90879487991333
Validation loss: 2.4258017975796937

Epoch: 5| Step: 4
Training loss: 2.039060115814209
Validation loss: 2.343449168307807

Epoch: 5| Step: 5
Training loss: 2.3403961658477783
Validation loss: 2.2767289197573097

Epoch: 5| Step: 6
Training loss: 2.683903217315674
Validation loss: 2.2289541767489527

Epoch: 5| Step: 7
Training loss: 3.386671543121338
Validation loss: 2.1727992027036604

Epoch: 5| Step: 8
Training loss: 2.4842276573181152
Validation loss: 2.12865283155954

Epoch: 5| Step: 9
Training loss: 3.1161398887634277
Validation loss: 2.0886623538950437

Epoch: 5| Step: 10
Training loss: 2.447979688644409
Validation loss: 2.0662794702796528

Epoch: 109| Step: 0
Training loss: 2.5269978046417236
Validation loss: 2.071091103297408

Epoch: 5| Step: 1
Training loss: 2.2998337745666504
Validation loss: 2.0843625940302366

Epoch: 5| Step: 2
Training loss: 3.2220757007598877
Validation loss: 2.1433838029061594

Epoch: 5| Step: 3
Training loss: 2.9257748126983643
Validation loss: 2.2027955978147444

Epoch: 5| Step: 4
Training loss: 2.2141685485839844
Validation loss: 2.195820047009376

Epoch: 5| Step: 5
Training loss: 2.9708454608917236
Validation loss: 2.176268469902777

Epoch: 5| Step: 6
Training loss: 1.6952072381973267
Validation loss: 2.127688697589341

Epoch: 5| Step: 7
Training loss: 2.391972064971924
Validation loss: 2.105527852171211

Epoch: 5| Step: 8
Training loss: 1.7828819751739502
Validation loss: 2.074160729685137

Epoch: 5| Step: 9
Training loss: 2.0705878734588623
Validation loss: 2.0709089079210834

Epoch: 5| Step: 10
Training loss: 2.6724021434783936
Validation loss: 2.073104609725296

Epoch: 110| Step: 0
Training loss: 2.448978900909424
Validation loss: 2.082650669159428

Epoch: 5| Step: 1
Training loss: 2.3096375465393066
Validation loss: 2.091574563775011

Epoch: 5| Step: 2
Training loss: 2.0990231037139893
Validation loss: 2.09137124143621

Epoch: 5| Step: 3
Training loss: 2.0052826404571533
Validation loss: 2.1180571535582184

Epoch: 5| Step: 4
Training loss: 2.2329697608947754
Validation loss: 2.1592415712213002

Epoch: 5| Step: 5
Training loss: 2.0951130390167236
Validation loss: 2.191318776017876

Epoch: 5| Step: 6
Training loss: 2.8100152015686035
Validation loss: 2.2117374661148235

Epoch: 5| Step: 7
Training loss: 2.3436264991760254
Validation loss: 2.185543098757344

Epoch: 5| Step: 8
Training loss: 3.475201368331909
Validation loss: 2.1399138435240714

Epoch: 5| Step: 9
Training loss: 2.660252809524536
Validation loss: 2.1190291809779342

Epoch: 5| Step: 10
Training loss: 2.156946897506714
Validation loss: 2.0965293697131577

Epoch: 111| Step: 0
Training loss: 2.556252956390381
Validation loss: 2.0825907184231665

Epoch: 5| Step: 1
Training loss: 3.0078225135803223
Validation loss: 2.0671681332331833

Epoch: 5| Step: 2
Training loss: 2.2593836784362793
Validation loss: 2.0724252052204584

Epoch: 5| Step: 3
Training loss: 2.6806480884552
Validation loss: 2.0684680579811014

Epoch: 5| Step: 4
Training loss: 2.308318614959717
Validation loss: 2.070231032627885

Epoch: 5| Step: 5
Training loss: 1.628156065940857
Validation loss: 2.0791454315185547

Epoch: 5| Step: 6
Training loss: 2.4556849002838135
Validation loss: 2.0881914733558573

Epoch: 5| Step: 7
Training loss: 2.345491886138916
Validation loss: 2.1081803793548257

Epoch: 5| Step: 8
Training loss: 2.430086612701416
Validation loss: 2.097208217907977

Epoch: 5| Step: 9
Training loss: 2.353898286819458
Validation loss: 2.1065078191859747

Epoch: 5| Step: 10
Training loss: 2.443016290664673
Validation loss: 2.085550405645883

Epoch: 112| Step: 0
Training loss: 2.2961385250091553
Validation loss: 2.1037033950128863

Epoch: 5| Step: 1
Training loss: 2.942586898803711
Validation loss: 2.0932745497713805

Epoch: 5| Step: 2
Training loss: 1.9691705703735352
Validation loss: 2.102838323962304

Epoch: 5| Step: 3
Training loss: 2.7288997173309326
Validation loss: 2.1061012309084655

Epoch: 5| Step: 4
Training loss: 2.506638765335083
Validation loss: 2.10253902148175

Epoch: 5| Step: 5
Training loss: 2.390726327896118
Validation loss: 2.0982528642941545

Epoch: 5| Step: 6
Training loss: 2.3851988315582275
Validation loss: 2.1023551238480436

Epoch: 5| Step: 7
Training loss: 2.4696238040924072
Validation loss: 2.1175676994426276

Epoch: 5| Step: 8
Training loss: 1.9371740818023682
Validation loss: 2.093611292941596

Epoch: 5| Step: 9
Training loss: 2.452878475189209
Validation loss: 2.095551435665418

Epoch: 5| Step: 10
Training loss: 2.1422019004821777
Validation loss: 2.104141560933923

Epoch: 113| Step: 0
Training loss: 2.0734000205993652
Validation loss: 2.089982633949608

Epoch: 5| Step: 1
Training loss: 2.325629472732544
Validation loss: 2.099831745188723

Epoch: 5| Step: 2
Training loss: 2.4721240997314453
Validation loss: 2.103282392665904

Epoch: 5| Step: 3
Training loss: 2.1840109825134277
Validation loss: 2.10315949942476

Epoch: 5| Step: 4
Training loss: 2.612689256668091
Validation loss: 2.1094550983880156

Epoch: 5| Step: 5
Training loss: 2.2474098205566406
Validation loss: 2.118855681470645

Epoch: 5| Step: 6
Training loss: 2.5401244163513184
Validation loss: 2.124094522127541

Epoch: 5| Step: 7
Training loss: 2.3562378883361816
Validation loss: 2.117653892886254

Epoch: 5| Step: 8
Training loss: 2.404277801513672
Validation loss: 2.113899439893743

Epoch: 5| Step: 9
Training loss: 2.3820109367370605
Validation loss: 2.1044588127443866

Epoch: 5| Step: 10
Training loss: 2.5778238773345947
Validation loss: 2.1116085488309144

Epoch: 114| Step: 0
Training loss: 2.4523813724517822
Validation loss: 2.1136811215390443

Epoch: 5| Step: 1
Training loss: 2.2810115814208984
Validation loss: 2.11353882410193

Epoch: 5| Step: 2
Training loss: 2.066437244415283
Validation loss: 2.107457636505045

Epoch: 5| Step: 3
Training loss: 1.865734338760376
Validation loss: 2.107762772549865

Epoch: 5| Step: 4
Training loss: 2.5766243934631348
Validation loss: 2.1197211793673936

Epoch: 5| Step: 5
Training loss: 2.2668333053588867
Validation loss: 2.1437934803706344

Epoch: 5| Step: 6
Training loss: 2.6929287910461426
Validation loss: 2.150577704111735

Epoch: 5| Step: 7
Training loss: 2.673316240310669
Validation loss: 2.1697000893213416

Epoch: 5| Step: 8
Training loss: 2.4843757152557373
Validation loss: 2.1960041035888014

Epoch: 5| Step: 9
Training loss: 1.94512939453125
Validation loss: 2.1874928730790333

Epoch: 5| Step: 10
Training loss: 2.8943445682525635
Validation loss: 2.164495124611803

Epoch: 115| Step: 0
Training loss: 2.8139984607696533
Validation loss: 2.138139997759173

Epoch: 5| Step: 1
Training loss: 1.8627456426620483
Validation loss: 2.11614042200068

Epoch: 5| Step: 2
Training loss: 2.8842813968658447
Validation loss: 2.100927519541915

Epoch: 5| Step: 3
Training loss: 2.7067205905914307
Validation loss: 2.113291612235449

Epoch: 5| Step: 4
Training loss: 2.5965609550476074
Validation loss: 2.1116030395671888

Epoch: 5| Step: 5
Training loss: 2.711325168609619
Validation loss: 2.1162377403628443

Epoch: 5| Step: 6
Training loss: 1.8842763900756836
Validation loss: 2.1097878717607066

Epoch: 5| Step: 7
Training loss: 2.1750636100769043
Validation loss: 2.1087495191122896

Epoch: 5| Step: 8
Training loss: 2.330549478530884
Validation loss: 2.1050638447525682

Epoch: 5| Step: 9
Training loss: 1.8940614461898804
Validation loss: 2.101693699436803

Epoch: 5| Step: 10
Training loss: 2.207930088043213
Validation loss: 2.1089067382197224

Epoch: 116| Step: 0
Training loss: 2.64302396774292
Validation loss: 2.0972557888236096

Epoch: 5| Step: 1
Training loss: 2.411841869354248
Validation loss: 2.1040659514806603

Epoch: 5| Step: 2
Training loss: 2.4113457202911377
Validation loss: 2.0967887011907433

Epoch: 5| Step: 3
Training loss: 1.9474719762802124
Validation loss: 2.110431512196859

Epoch: 5| Step: 4
Training loss: 2.6456151008605957
Validation loss: 2.114145889077135

Epoch: 5| Step: 5
Training loss: 2.4979653358459473
Validation loss: 2.0965977791816957

Epoch: 5| Step: 6
Training loss: 2.3103644847869873
Validation loss: 2.081436357190532

Epoch: 5| Step: 7
Training loss: 2.1681432723999023
Validation loss: 2.08063442091788

Epoch: 5| Step: 8
Training loss: 2.3512113094329834
Validation loss: 2.072288574710969

Epoch: 5| Step: 9
Training loss: 2.252218246459961
Validation loss: 2.0798157440718783

Epoch: 5| Step: 10
Training loss: 2.2469534873962402
Validation loss: 2.072017074913107

Epoch: 117| Step: 0
Training loss: 1.7516212463378906
Validation loss: 2.0846146281047533

Epoch: 5| Step: 1
Training loss: 2.238797187805176
Validation loss: 2.0897604624430337

Epoch: 5| Step: 2
Training loss: 2.1566882133483887
Validation loss: 2.082204950753079

Epoch: 5| Step: 3
Training loss: 3.0114176273345947
Validation loss: 2.088589936174372

Epoch: 5| Step: 4
Training loss: 2.3053529262542725
Validation loss: 2.0825946548933625

Epoch: 5| Step: 5
Training loss: 2.0148844718933105
Validation loss: 2.088712751224477

Epoch: 5| Step: 6
Training loss: 2.0882365703582764
Validation loss: 2.1098171331549205

Epoch: 5| Step: 7
Training loss: 2.371067523956299
Validation loss: 2.1127224147960706

Epoch: 5| Step: 8
Training loss: 2.6431193351745605
Validation loss: 2.1035524683613933

Epoch: 5| Step: 9
Training loss: 2.8428702354431152
Validation loss: 2.084670771834671

Epoch: 5| Step: 10
Training loss: 2.319270372390747
Validation loss: 2.09193685746962

Epoch: 118| Step: 0
Training loss: 2.092985153198242
Validation loss: 2.0885058551706295

Epoch: 5| Step: 1
Training loss: 2.7152228355407715
Validation loss: 2.0759408268877255

Epoch: 5| Step: 2
Training loss: 2.3186938762664795
Validation loss: 2.0811578535264537

Epoch: 5| Step: 3
Training loss: 2.0311834812164307
Validation loss: 2.085408995228429

Epoch: 5| Step: 4
Training loss: 1.8108837604522705
Validation loss: 2.083826013790664

Epoch: 5| Step: 5
Training loss: 2.0559840202331543
Validation loss: 2.0853257627897364

Epoch: 5| Step: 6
Training loss: 2.142313003540039
Validation loss: 2.093893802294167

Epoch: 5| Step: 7
Training loss: 2.772759437561035
Validation loss: 2.0935210592003277

Epoch: 5| Step: 8
Training loss: 2.5665111541748047
Validation loss: 2.097660462061564

Epoch: 5| Step: 9
Training loss: 2.6905875205993652
Validation loss: 2.101576905096731

Epoch: 5| Step: 10
Training loss: 2.512549877166748
Validation loss: 2.108534364290135

Epoch: 119| Step: 0
Training loss: 2.346271514892578
Validation loss: 2.0968171345290316

Epoch: 5| Step: 1
Training loss: 2.3994269371032715
Validation loss: 2.0890283084684804

Epoch: 5| Step: 2
Training loss: 2.031795024871826
Validation loss: 2.09920169461158

Epoch: 5| Step: 3
Training loss: 2.5626060962677
Validation loss: 2.1167048369684527

Epoch: 5| Step: 4
Training loss: 2.2231600284576416
Validation loss: 2.112917605266776

Epoch: 5| Step: 5
Training loss: 1.6174955368041992
Validation loss: 2.1144089544973066

Epoch: 5| Step: 6
Training loss: 1.9733927249908447
Validation loss: 2.1186063212733113

Epoch: 5| Step: 7
Training loss: 2.6145780086517334
Validation loss: 2.080388372944247

Epoch: 5| Step: 8
Training loss: 2.5886051654815674
Validation loss: 2.075744631469891

Epoch: 5| Step: 9
Training loss: 2.6240298748016357
Validation loss: 2.0611192423810243

Epoch: 5| Step: 10
Training loss: 2.7040741443634033
Validation loss: 2.073992790714387

Epoch: 120| Step: 0
Training loss: 2.868654489517212
Validation loss: 2.0773212294424734

Epoch: 5| Step: 1
Training loss: 2.199080228805542
Validation loss: 2.0640942512019986

Epoch: 5| Step: 2
Training loss: 2.003258466720581
Validation loss: 2.069748455478299

Epoch: 5| Step: 3
Training loss: 2.1132237911224365
Validation loss: 2.077939466763568

Epoch: 5| Step: 4
Training loss: 2.1104912757873535
Validation loss: 2.0891168758433354

Epoch: 5| Step: 5
Training loss: 2.782538890838623
Validation loss: 2.1241769149739254

Epoch: 5| Step: 6
Training loss: 2.190927267074585
Validation loss: 2.167101252463556

Epoch: 5| Step: 7
Training loss: 2.640028476715088
Validation loss: 2.158865805595152

Epoch: 5| Step: 8
Training loss: 2.0158934593200684
Validation loss: 2.1266151256458734

Epoch: 5| Step: 9
Training loss: 2.6645002365112305
Validation loss: 2.0945745898831274

Epoch: 5| Step: 10
Training loss: 2.3027427196502686
Validation loss: 2.069642456628943

Epoch: 121| Step: 0
Training loss: 1.9154293537139893
Validation loss: 2.0653711262569634

Epoch: 5| Step: 1
Training loss: 2.0823230743408203
Validation loss: 2.0679180493918796

Epoch: 5| Step: 2
Training loss: 2.628180503845215
Validation loss: 2.0754488668134137

Epoch: 5| Step: 3
Training loss: 2.7645480632781982
Validation loss: 2.0758907230951453

Epoch: 5| Step: 4
Training loss: 2.4512276649475098
Validation loss: 2.0814707561205794

Epoch: 5| Step: 5
Training loss: 2.2309741973876953
Validation loss: 2.0907598080173617

Epoch: 5| Step: 6
Training loss: 1.8967921733856201
Validation loss: 2.0878672253700996

Epoch: 5| Step: 7
Training loss: 2.7146105766296387
Validation loss: 2.0902299598980973

Epoch: 5| Step: 8
Training loss: 2.353907823562622
Validation loss: 2.0877289220850956

Epoch: 5| Step: 9
Training loss: 2.903689384460449
Validation loss: 2.088911291091673

Epoch: 5| Step: 10
Training loss: 1.7681119441986084
Validation loss: 2.089226875253903

Epoch: 122| Step: 0
Training loss: 2.949012279510498
Validation loss: 2.095373794596682

Epoch: 5| Step: 1
Training loss: 1.8358322381973267
Validation loss: 2.0877433592273342

Epoch: 5| Step: 2
Training loss: 2.9791924953460693
Validation loss: 2.0756431369371313

Epoch: 5| Step: 3
Training loss: 2.6874914169311523
Validation loss: 2.076230272170036

Epoch: 5| Step: 4
Training loss: 2.3260693550109863
Validation loss: 2.0662968043358094

Epoch: 5| Step: 5
Training loss: 2.4578781127929688
Validation loss: 2.061382011700702

Epoch: 5| Step: 6
Training loss: 1.5380977392196655
Validation loss: 2.0552429076164

Epoch: 5| Step: 7
Training loss: 2.3874495029449463
Validation loss: 2.0564185650117937

Epoch: 5| Step: 8
Training loss: 1.7193588018417358
Validation loss: 2.0604245316597725

Epoch: 5| Step: 9
Training loss: 2.253241777420044
Validation loss: 2.08571042296707

Epoch: 5| Step: 10
Training loss: 2.298449754714966
Validation loss: 2.0893052983027633

Epoch: 123| Step: 0
Training loss: 2.6297671794891357
Validation loss: 2.089409221885025

Epoch: 5| Step: 1
Training loss: 2.0663347244262695
Validation loss: 2.1068631051689066

Epoch: 5| Step: 2
Training loss: 1.632478952407837
Validation loss: 2.1121393403699322

Epoch: 5| Step: 3
Training loss: 2.3681108951568604
Validation loss: 2.0940520148123465

Epoch: 5| Step: 4
Training loss: 2.355008602142334
Validation loss: 2.089175219176918

Epoch: 5| Step: 5
Training loss: 2.3355116844177246
Validation loss: 2.083316162068357

Epoch: 5| Step: 6
Training loss: 2.8627560138702393
Validation loss: 2.102568802013192

Epoch: 5| Step: 7
Training loss: 2.1341323852539062
Validation loss: 2.0994932484883133

Epoch: 5| Step: 8
Training loss: 2.5321812629699707
Validation loss: 2.095175935376075

Epoch: 5| Step: 9
Training loss: 2.2012181282043457
Validation loss: 2.090257949726556

Epoch: 5| Step: 10
Training loss: 2.1882452964782715
Validation loss: 2.091825017365076

Epoch: 124| Step: 0
Training loss: 2.510448932647705
Validation loss: 2.0942620359441286

Epoch: 5| Step: 1
Training loss: 2.1862378120422363
Validation loss: 2.0886260450527234

Epoch: 5| Step: 2
Training loss: 1.8968260288238525
Validation loss: 2.088342379498225

Epoch: 5| Step: 3
Training loss: 2.0889153480529785
Validation loss: 2.0894793489927888

Epoch: 5| Step: 4
Training loss: 2.531003475189209
Validation loss: 2.0931823151085966

Epoch: 5| Step: 5
Training loss: 2.315062999725342
Validation loss: 2.086455475899481

Epoch: 5| Step: 6
Training loss: 2.412029504776001
Validation loss: 2.080731878998459

Epoch: 5| Step: 7
Training loss: 2.3367300033569336
Validation loss: 2.0778562291975944

Epoch: 5| Step: 8
Training loss: 2.221402645111084
Validation loss: 2.0900690735027356

Epoch: 5| Step: 9
Training loss: 2.457338333129883
Validation loss: 2.0788341055634203

Epoch: 5| Step: 10
Training loss: 2.270334005355835
Validation loss: 2.0764362606950986

Epoch: 125| Step: 0
Training loss: 2.2931933403015137
Validation loss: 2.099024498334495

Epoch: 5| Step: 1
Training loss: 1.8144721984863281
Validation loss: 2.111534362198204

Epoch: 5| Step: 2
Training loss: 2.1869874000549316
Validation loss: 2.112678325304421

Epoch: 5| Step: 3
Training loss: 2.676360607147217
Validation loss: 2.1017455798323437

Epoch: 5| Step: 4
Training loss: 2.6860504150390625
Validation loss: 2.0867635293673445

Epoch: 5| Step: 5
Training loss: 2.25407075881958
Validation loss: 2.0867814428062847

Epoch: 5| Step: 6
Training loss: 2.275179624557495
Validation loss: 2.071327699128018

Epoch: 5| Step: 7
Training loss: 2.2231523990631104
Validation loss: 2.067614678413637

Epoch: 5| Step: 8
Training loss: 2.260469436645508
Validation loss: 2.0616216044272146

Epoch: 5| Step: 9
Training loss: 2.5260379314422607
Validation loss: 2.062060299740043

Epoch: 5| Step: 10
Training loss: 1.975885272026062
Validation loss: 2.0652683396493234

Testing loss: 2.3361471626493664

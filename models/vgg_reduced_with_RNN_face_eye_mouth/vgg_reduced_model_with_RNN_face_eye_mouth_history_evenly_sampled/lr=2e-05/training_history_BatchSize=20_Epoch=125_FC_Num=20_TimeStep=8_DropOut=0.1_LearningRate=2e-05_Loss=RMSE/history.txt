Epoch: 1| Step: 0
Training loss: 5.013011405902932
Validation loss: 5.803285727749914

Epoch: 5| Step: 1
Training loss: 5.408274188940386
Validation loss: 5.7873259938208435

Epoch: 5| Step: 2
Training loss: 5.630307893346238
Validation loss: 5.773267778084434

Epoch: 5| Step: 3
Training loss: 6.717593639134263
Validation loss: 5.759441460594809

Epoch: 5| Step: 4
Training loss: 6.430329115826172
Validation loss: 5.745292345071253

Epoch: 5| Step: 5
Training loss: 5.182032265872243
Validation loss: 5.728483743395926

Epoch: 5| Step: 6
Training loss: 5.497386484817322
Validation loss: 5.709801784639693

Epoch: 5| Step: 7
Training loss: 5.1303415142399995
Validation loss: 5.688237516212985

Epoch: 5| Step: 8
Training loss: 6.4906460875093765
Validation loss: 5.663360799818078

Epoch: 5| Step: 9
Training loss: 5.462763708806819
Validation loss: 5.635973356755215

Epoch: 5| Step: 10
Training loss: 6.072252432439834
Validation loss: 5.604562995687953

Epoch: 2| Step: 0
Training loss: 5.359286204678495
Validation loss: 5.570775014783276

Epoch: 5| Step: 1
Training loss: 5.244840902673822
Validation loss: 5.534825378162909

Epoch: 5| Step: 2
Training loss: 6.056600945129613
Validation loss: 5.496262601496508

Epoch: 5| Step: 3
Training loss: 6.650549511684959
Validation loss: 5.455023493410485

Epoch: 5| Step: 4
Training loss: 5.046239665287558
Validation loss: 5.411024366059147

Epoch: 5| Step: 5
Training loss: 4.590971852722497
Validation loss: 5.362150790494231

Epoch: 5| Step: 6
Training loss: 6.334384262160995
Validation loss: 5.310980586408892

Epoch: 5| Step: 7
Training loss: 4.930480216201412
Validation loss: 5.25408992267112

Epoch: 5| Step: 8
Training loss: 4.963075672649601
Validation loss: 5.193453033120691

Epoch: 5| Step: 9
Training loss: 5.4491170675436855
Validation loss: 5.128322477780749

Epoch: 5| Step: 10
Training loss: 4.133130701286968
Validation loss: 5.057840933644377

Epoch: 3| Step: 0
Training loss: 6.29150949580781
Validation loss: 4.983957942930244

Epoch: 5| Step: 1
Training loss: 4.448452180999659
Validation loss: 4.908951895754925

Epoch: 5| Step: 2
Training loss: 4.944689188601747
Validation loss: 4.839864399582609

Epoch: 5| Step: 3
Training loss: 4.079236343199148
Validation loss: 4.772242460720827

Epoch: 5| Step: 4
Training loss: 4.4901222324513075
Validation loss: 4.712224990742385

Epoch: 5| Step: 5
Training loss: 5.743623265973801
Validation loss: 4.660902052704703

Epoch: 5| Step: 6
Training loss: 4.963383109357849
Validation loss: 4.611696445490455

Epoch: 5| Step: 7
Training loss: 3.2231103195734883
Validation loss: 4.569143495559834

Epoch: 5| Step: 8
Training loss: 4.707843631270493
Validation loss: 4.534780039453756

Epoch: 5| Step: 9
Training loss: 4.190074982985043
Validation loss: 4.500906931501577

Epoch: 5| Step: 10
Training loss: 4.844129584421133
Validation loss: 4.469318778507156

Epoch: 4| Step: 0
Training loss: 4.789253268401712
Validation loss: 4.442190064778249

Epoch: 5| Step: 1
Training loss: 4.416608174254603
Validation loss: 4.412809245760893

Epoch: 5| Step: 2
Training loss: 4.483511492700313
Validation loss: 4.383046217951322

Epoch: 5| Step: 3
Training loss: 3.582519631075796
Validation loss: 4.34630862235834

Epoch: 5| Step: 4
Training loss: 3.5207040676486865
Validation loss: 4.308719841900378

Epoch: 5| Step: 5
Training loss: 5.574604194857361
Validation loss: 4.279492925388884

Epoch: 5| Step: 6
Training loss: 3.869218635831746
Validation loss: 4.257055368548356

Epoch: 5| Step: 7
Training loss: 4.959491284647069
Validation loss: 4.234858218876978

Epoch: 5| Step: 8
Training loss: 4.284283308276446
Validation loss: 4.211633752425405

Epoch: 5| Step: 9
Training loss: 4.511577547844528
Validation loss: 4.199747441428733

Epoch: 5| Step: 10
Training loss: 4.1742681718485475
Validation loss: 4.182428667625906

Epoch: 5| Step: 0
Training loss: 4.54379829055752
Validation loss: 4.155127574312194

Epoch: 5| Step: 1
Training loss: 4.319278862421067
Validation loss: 4.128448367493721

Epoch: 5| Step: 2
Training loss: 3.8318998448417774
Validation loss: 4.104717286205369

Epoch: 5| Step: 3
Training loss: 4.380475786399735
Validation loss: 4.092377395677157

Epoch: 5| Step: 4
Training loss: 4.65041195264954
Validation loss: 4.102222269164047

Epoch: 5| Step: 5
Training loss: 4.539569452179053
Validation loss: 4.057037035071493

Epoch: 5| Step: 6
Training loss: 3.591175782777504
Validation loss: 4.047517940247356

Epoch: 5| Step: 7
Training loss: 4.314429252423769
Validation loss: 4.043539338828797

Epoch: 5| Step: 8
Training loss: 4.176849485528041
Validation loss: 4.054688802467542

Epoch: 5| Step: 9
Training loss: 3.4972186617689633
Validation loss: 4.044568644551859

Epoch: 5| Step: 10
Training loss: 4.507735068252862
Validation loss: 4.0301119511287675

Epoch: 6| Step: 0
Training loss: 3.176751958523272
Validation loss: 4.013406043344702

Epoch: 5| Step: 1
Training loss: 4.0232030707644375
Validation loss: 3.998783359800861

Epoch: 5| Step: 2
Training loss: 3.8212783310288874
Validation loss: 3.9837324563350487

Epoch: 5| Step: 3
Training loss: 4.4120134036707
Validation loss: 3.9732826013320772

Epoch: 5| Step: 4
Training loss: 4.13658849350815
Validation loss: 3.974537015164884

Epoch: 5| Step: 5
Training loss: 4.339098738511124
Validation loss: 3.957540958457848

Epoch: 5| Step: 6
Training loss: 5.363522006727519
Validation loss: 3.949471990098875

Epoch: 5| Step: 7
Training loss: 3.809845297168179
Validation loss: 3.952742511118624

Epoch: 5| Step: 8
Training loss: 4.393866703225405
Validation loss: 3.945934498824959

Epoch: 5| Step: 9
Training loss: 3.1539384289853483
Validation loss: 3.924182349234813

Epoch: 5| Step: 10
Training loss: 4.224554744936885
Validation loss: 3.9120245770913074

Epoch: 7| Step: 0
Training loss: 3.9867138748393303
Validation loss: 3.905363134633697

Epoch: 5| Step: 1
Training loss: 3.2135800813247077
Validation loss: 3.8851428350146224

Epoch: 5| Step: 2
Training loss: 4.367240345862063
Validation loss: 3.874817501537986

Epoch: 5| Step: 3
Training loss: 3.06728316481925
Validation loss: 3.863288026590549

Epoch: 5| Step: 4
Training loss: 4.221347489568413
Validation loss: 3.8543544777848986

Epoch: 5| Step: 5
Training loss: 4.120555911289933
Validation loss: 3.8409170733854556

Epoch: 5| Step: 6
Training loss: 4.118552052396665
Validation loss: 3.823707132473869

Epoch: 5| Step: 7
Training loss: 3.4240935636731784
Validation loss: 3.8082032409500797

Epoch: 5| Step: 8
Training loss: 5.01988842357639
Validation loss: 3.8045853556055724

Epoch: 5| Step: 9
Training loss: 4.526111714911528
Validation loss: 3.779270547254334

Epoch: 5| Step: 10
Training loss: 3.4868673810449984
Validation loss: 3.7804783022034276

Epoch: 8| Step: 0
Training loss: 4.508845853788698
Validation loss: 3.7768577653894995

Epoch: 5| Step: 1
Training loss: 3.7385792070390442
Validation loss: 3.7706766639315634

Epoch: 5| Step: 2
Training loss: 4.472543240248067
Validation loss: 3.7697447802868607

Epoch: 5| Step: 3
Training loss: 3.0870198729789533
Validation loss: 3.756858467338096

Epoch: 5| Step: 4
Training loss: 4.26715932227057
Validation loss: 3.7486237123077304

Epoch: 5| Step: 5
Training loss: 4.496188775060113
Validation loss: 3.7338313639241085

Epoch: 5| Step: 6
Training loss: 4.308584453610778
Validation loss: 3.7261268672560255

Epoch: 5| Step: 7
Training loss: 2.92387574589643
Validation loss: 3.725149154127001

Epoch: 5| Step: 8
Training loss: 3.505826052026522
Validation loss: 3.7235354906802374

Epoch: 5| Step: 9
Training loss: 3.0867657672581026
Validation loss: 3.7073228603948976

Epoch: 5| Step: 10
Training loss: 4.316864017015853
Validation loss: 3.7033810571795174

Epoch: 9| Step: 0
Training loss: 4.592068941987778
Validation loss: 3.699971442429725

Epoch: 5| Step: 1
Training loss: 3.0317596321748788
Validation loss: 3.6957721699641017

Epoch: 5| Step: 2
Training loss: 4.627627683059913
Validation loss: 3.690927973130526

Epoch: 5| Step: 3
Training loss: 3.1382330900508704
Validation loss: 3.685423270223394

Epoch: 5| Step: 4
Training loss: 3.7534598602144675
Validation loss: 3.6765260722384885

Epoch: 5| Step: 5
Training loss: 3.932889994728091
Validation loss: 3.6716341022127676

Epoch: 5| Step: 6
Training loss: 3.8953686333325095
Validation loss: 3.6659071061930235

Epoch: 5| Step: 7
Training loss: 3.1789375939257156
Validation loss: 3.6629379089063447

Epoch: 5| Step: 8
Training loss: 3.754562653979322
Validation loss: 3.6584120854891182

Epoch: 5| Step: 9
Training loss: 4.47794724063026
Validation loss: 3.6530609863789865

Epoch: 5| Step: 10
Training loss: 3.674340901340693
Validation loss: 3.646205764710241

Epoch: 10| Step: 0
Training loss: 4.01345992445241
Validation loss: 3.6441068498578915

Epoch: 5| Step: 1
Training loss: 3.4712927494019836
Validation loss: 3.640661918350491

Epoch: 5| Step: 2
Training loss: 3.405649674736268
Validation loss: 3.6363844698228043

Epoch: 5| Step: 3
Training loss: 3.2290458082326796
Validation loss: 3.6335924242645135

Epoch: 5| Step: 4
Training loss: 4.09044204897293
Validation loss: 3.6280008779660977

Epoch: 5| Step: 5
Training loss: 4.746664783677431
Validation loss: 3.6230673908775746

Epoch: 5| Step: 6
Training loss: 3.6330357636786976
Validation loss: 3.6145867897398505

Epoch: 5| Step: 7
Training loss: 3.5450867837847677
Validation loss: 3.6120326150325988

Epoch: 5| Step: 8
Training loss: 3.9632383507035387
Validation loss: 3.6089023692069397

Epoch: 5| Step: 9
Training loss: 2.8407395745767343
Validation loss: 3.6062583783481896

Epoch: 5| Step: 10
Training loss: 4.775627270785249
Validation loss: 3.6056256337944075

Epoch: 11| Step: 0
Training loss: 3.935965708366129
Validation loss: 3.6037534915195675

Epoch: 5| Step: 1
Training loss: 3.389413924967079
Validation loss: 3.5999685478659544

Epoch: 5| Step: 2
Training loss: 4.063500853275451
Validation loss: 3.596409720571487

Epoch: 5| Step: 3
Training loss: 4.068856299164662
Validation loss: 3.5941221125676615

Epoch: 5| Step: 4
Training loss: 3.7861280266606325
Validation loss: 3.5903515198474816

Epoch: 5| Step: 5
Training loss: 3.6796194665535915
Validation loss: 3.5862877783911844

Epoch: 5| Step: 6
Training loss: 3.5276983173729577
Validation loss: 3.5829156893119647

Epoch: 5| Step: 7
Training loss: 3.742250063697722
Validation loss: 3.5795758634765793

Epoch: 5| Step: 8
Training loss: 4.681403098766921
Validation loss: 3.575901026546817

Epoch: 5| Step: 9
Training loss: 3.2331890198530218
Validation loss: 3.5748237297350274

Epoch: 5| Step: 10
Training loss: 3.2920695352082094
Validation loss: 3.57175893307451

Epoch: 12| Step: 0
Training loss: 4.107495476412009
Validation loss: 3.5692975114412926

Epoch: 5| Step: 1
Training loss: 3.6660206977105956
Validation loss: 3.566712327717967

Epoch: 5| Step: 2
Training loss: 3.8368684528578947
Validation loss: 3.5634123866721232

Epoch: 5| Step: 3
Training loss: 3.9875313019196557
Validation loss: 3.5605201825717807

Epoch: 5| Step: 4
Training loss: 4.029573786988539
Validation loss: 3.5583211658586085

Epoch: 5| Step: 5
Training loss: 3.563153658679531
Validation loss: 3.555864987081648

Epoch: 5| Step: 6
Training loss: 3.987730640210609
Validation loss: 3.5528605785403933

Epoch: 5| Step: 7
Training loss: 3.5956831003736953
Validation loss: 3.5494216811849997

Epoch: 5| Step: 8
Training loss: 3.285535212192102
Validation loss: 3.5466427055004077

Epoch: 5| Step: 9
Training loss: 3.6172921678464784
Validation loss: 3.5437758523704437

Epoch: 5| Step: 10
Training loss: 3.611501092311964
Validation loss: 3.542155103402652

Epoch: 13| Step: 0
Training loss: 4.131464199220823
Validation loss: 3.5385763918392867

Epoch: 5| Step: 1
Training loss: 3.1110300231387362
Validation loss: 3.537387253556642

Epoch: 5| Step: 2
Training loss: 3.1163843023598865
Validation loss: 3.534619796099172

Epoch: 5| Step: 3
Training loss: 3.530729677468793
Validation loss: 3.531137028850053

Epoch: 5| Step: 4
Training loss: 3.6629354537038306
Validation loss: 3.528701577754507

Epoch: 5| Step: 5
Training loss: 4.4267971290375705
Validation loss: 3.525055058745388

Epoch: 5| Step: 6
Training loss: 3.2601035133263085
Validation loss: 3.525017191433998

Epoch: 5| Step: 7
Training loss: 3.764312515411557
Validation loss: 3.5209071547304207

Epoch: 5| Step: 8
Training loss: 4.088337355771259
Validation loss: 3.517193540913417

Epoch: 5| Step: 9
Training loss: 3.9342657460381623
Validation loss: 3.5140328922055897

Epoch: 5| Step: 10
Training loss: 3.844355093616539
Validation loss: 3.5119138319418854

Epoch: 14| Step: 0
Training loss: 3.7119265232213237
Validation loss: 3.5089815500574075

Epoch: 5| Step: 1
Training loss: 3.805033141303883
Validation loss: 3.508694904431309

Epoch: 5| Step: 2
Training loss: 4.210741820592602
Validation loss: 3.507830729961221

Epoch: 5| Step: 3
Training loss: 3.7290577082282312
Validation loss: 3.5074298448127412

Epoch: 5| Step: 4
Training loss: 3.5952828496831892
Validation loss: 3.5036559267522356

Epoch: 5| Step: 5
Training loss: 3.293579913675409
Validation loss: 3.500625358182386

Epoch: 5| Step: 6
Training loss: 3.8859024494156453
Validation loss: 3.4973990126585965

Epoch: 5| Step: 7
Training loss: 4.369798019944105
Validation loss: 3.496566328963464

Epoch: 5| Step: 8
Training loss: 3.528457077251798
Validation loss: 3.491801650928862

Epoch: 5| Step: 9
Training loss: 2.8785348805376114
Validation loss: 3.4930409702616902

Epoch: 5| Step: 10
Training loss: 3.6213411742515493
Validation loss: 3.4909692183479333

Epoch: 15| Step: 0
Training loss: 3.0247977890652273
Validation loss: 3.4878331053276304

Epoch: 5| Step: 1
Training loss: 3.134675116732734
Validation loss: 3.4869693056080617

Epoch: 5| Step: 2
Training loss: 3.5556105536604763
Validation loss: 3.48604950780078

Epoch: 5| Step: 3
Training loss: 3.937400816621907
Validation loss: 3.4831725317811917

Epoch: 5| Step: 4
Training loss: 3.055655635774574
Validation loss: 3.4815444238840576

Epoch: 5| Step: 5
Training loss: 3.779553750152651
Validation loss: 3.480595287574416

Epoch: 5| Step: 6
Training loss: 4.877063021263813
Validation loss: 3.4795397010219813

Epoch: 5| Step: 7
Training loss: 3.229213558389794
Validation loss: 3.475519759622535

Epoch: 5| Step: 8
Training loss: 3.334498503810074
Validation loss: 3.474651340543684

Epoch: 5| Step: 9
Training loss: 4.212043914102294
Validation loss: 3.473126393443341

Epoch: 5| Step: 10
Training loss: 4.13499811137847
Validation loss: 3.4717700615169966

Epoch: 16| Step: 0
Training loss: 4.016472752956771
Validation loss: 3.470411173946607

Epoch: 5| Step: 1
Training loss: 3.6367540204133513
Validation loss: 3.4689246180560085

Epoch: 5| Step: 2
Training loss: 3.830140995986088
Validation loss: 3.466566058107778

Epoch: 5| Step: 3
Training loss: 3.727940718388222
Validation loss: 3.4652608069763704

Epoch: 5| Step: 4
Training loss: 3.6117860081883744
Validation loss: 3.463956314766903

Epoch: 5| Step: 5
Training loss: 3.190508899575277
Validation loss: 3.4630652527547534

Epoch: 5| Step: 6
Training loss: 3.016131738967826
Validation loss: 3.4615362457721957

Epoch: 5| Step: 7
Training loss: 4.64029248006385
Validation loss: 3.460910599330704

Epoch: 5| Step: 8
Training loss: 3.4432613296095282
Validation loss: 3.459519243868537

Epoch: 5| Step: 9
Training loss: 3.4329607036062737
Validation loss: 3.458690901080975

Epoch: 5| Step: 10
Training loss: 3.7267857770588826
Validation loss: 3.4560531935498684

Epoch: 17| Step: 0
Training loss: 4.352584965296709
Validation loss: 3.4544804268697793

Epoch: 5| Step: 1
Training loss: 3.0585354425516678
Validation loss: 3.4529323966830154

Epoch: 5| Step: 2
Training loss: 3.4825292240184544
Validation loss: 3.4521892161066208

Epoch: 5| Step: 3
Training loss: 3.784929487209046
Validation loss: 3.4509199507012824

Epoch: 5| Step: 4
Training loss: 3.477394124550874
Validation loss: 3.449733850962001

Epoch: 5| Step: 5
Training loss: 3.889113412158277
Validation loss: 3.4487908686427113

Epoch: 5| Step: 6
Training loss: 4.152847847972892
Validation loss: 3.448058938320341

Epoch: 5| Step: 7
Training loss: 3.3430948061175934
Validation loss: 3.446563177459421

Epoch: 5| Step: 8
Training loss: 2.922148013492189
Validation loss: 3.4452997625541166

Epoch: 5| Step: 9
Training loss: 4.32187948385297
Validation loss: 3.4435148490384377

Epoch: 5| Step: 10
Training loss: 3.1835458201913496
Validation loss: 3.443213074241941

Epoch: 18| Step: 0
Training loss: 3.5886551276634506
Validation loss: 3.442233535880076

Epoch: 5| Step: 1
Training loss: 4.035345318658654
Validation loss: 3.440834942479408

Epoch: 5| Step: 2
Training loss: 3.5977360388308837
Validation loss: 3.4396507716642666

Epoch: 5| Step: 3
Training loss: 3.7949354453244246
Validation loss: 3.4385667594355964

Epoch: 5| Step: 4
Training loss: 2.984618811110028
Validation loss: 3.436656382269145

Epoch: 5| Step: 5
Training loss: 2.9706222252298082
Validation loss: 3.4354340252477873

Epoch: 5| Step: 6
Training loss: 3.8900869219563687
Validation loss: 3.434787459354275

Epoch: 5| Step: 7
Training loss: 3.5684614662804086
Validation loss: 3.435043680449745

Epoch: 5| Step: 8
Training loss: 3.750244895567941
Validation loss: 3.4330254123267974

Epoch: 5| Step: 9
Training loss: 3.892166016349781
Validation loss: 3.4317471406221896

Epoch: 5| Step: 10
Training loss: 4.065867979279057
Validation loss: 3.4307415328515978

Epoch: 19| Step: 0
Training loss: 4.006015783834002
Validation loss: 3.4287517137208297

Epoch: 5| Step: 1
Training loss: 4.1514134769017135
Validation loss: 3.4281649817098097

Epoch: 5| Step: 2
Training loss: 3.3020682991399584
Validation loss: 3.426853241901193

Epoch: 5| Step: 3
Training loss: 2.9598679837866606
Validation loss: 3.425691750657183

Epoch: 5| Step: 4
Training loss: 3.713434603923595
Validation loss: 3.424470702236156

Epoch: 5| Step: 5
Training loss: 4.135990874865148
Validation loss: 3.422912777425322

Epoch: 5| Step: 6
Training loss: 3.6439065837422397
Validation loss: 3.420728291206318

Epoch: 5| Step: 7
Training loss: 3.531671954576015
Validation loss: 3.419241380540858

Epoch: 5| Step: 8
Training loss: 4.003955077827271
Validation loss: 3.41627347911271

Epoch: 5| Step: 9
Training loss: 3.64345574736678
Validation loss: 3.4118651610205726

Epoch: 5| Step: 10
Training loss: 2.5650189742191474
Validation loss: 3.408032274181787

Epoch: 20| Step: 0
Training loss: 3.479852452712811
Validation loss: 3.40310006997295

Epoch: 5| Step: 1
Training loss: 3.3493986814202352
Validation loss: 3.399420000608263

Epoch: 5| Step: 2
Training loss: 3.6760243447943197
Validation loss: 3.3958544672289435

Epoch: 5| Step: 3
Training loss: 3.484743654737519
Validation loss: 3.3923462438591185

Epoch: 5| Step: 4
Training loss: 3.5083676177197356
Validation loss: 3.390531001501781

Epoch: 5| Step: 5
Training loss: 3.848014512786145
Validation loss: 3.3886051527098284

Epoch: 5| Step: 6
Training loss: 3.7820592597502616
Validation loss: 3.3877453863205504

Epoch: 5| Step: 7
Training loss: 3.199623240702925
Validation loss: 3.387479684674423

Epoch: 5| Step: 8
Training loss: 4.049459331827763
Validation loss: 3.3862142910898307

Epoch: 5| Step: 9
Training loss: 3.9477442130852705
Validation loss: 3.3841819504434603

Epoch: 5| Step: 10
Training loss: 3.36547283937282
Validation loss: 3.3856300371324797

Epoch: 21| Step: 0
Training loss: 3.8839296387919946
Validation loss: 3.3817148956063643

Epoch: 5| Step: 1
Training loss: 3.188215287677583
Validation loss: 3.379805647911526

Epoch: 5| Step: 2
Training loss: 3.9931299101315263
Validation loss: 3.3788622546291056

Epoch: 5| Step: 3
Training loss: 3.4104110080778467
Validation loss: 3.3763769433251807

Epoch: 5| Step: 4
Training loss: 3.5047255358798277
Validation loss: 3.376066740411003

Epoch: 5| Step: 5
Training loss: 3.9563511888240006
Validation loss: 3.3744879971911343

Epoch: 5| Step: 6
Training loss: 3.6001520601583215
Validation loss: 3.372586215482143

Epoch: 5| Step: 7
Training loss: 3.4982602018721307
Validation loss: 3.371555440721417

Epoch: 5| Step: 8
Training loss: 2.964708649342846
Validation loss: 3.3707839157449255

Epoch: 5| Step: 9
Training loss: 3.8323010146509278
Validation loss: 3.3700350113606135

Epoch: 5| Step: 10
Training loss: 3.7268388753647086
Validation loss: 3.368885406391292

Epoch: 22| Step: 0
Training loss: 3.7411245218093323
Validation loss: 3.3681113014616013

Epoch: 5| Step: 1
Training loss: 3.5919330813794956
Validation loss: 3.365910600921192

Epoch: 5| Step: 2
Training loss: 2.8695800112417094
Validation loss: 3.364461795205861

Epoch: 5| Step: 3
Training loss: 3.860691726253674
Validation loss: 3.36361810530041

Epoch: 5| Step: 4
Training loss: 3.4533152808327547
Validation loss: 3.3618442382537377

Epoch: 5| Step: 5
Training loss: 3.8555110476878123
Validation loss: 3.3619094419083053

Epoch: 5| Step: 6
Training loss: 3.6538038629717637
Validation loss: 3.3603128629826178

Epoch: 5| Step: 7
Training loss: 3.750841809200826
Validation loss: 3.3596549609765014

Epoch: 5| Step: 8
Training loss: 3.952170516872165
Validation loss: 3.358347008545819

Epoch: 5| Step: 9
Training loss: 3.0697623577774777
Validation loss: 3.3573996778253803

Epoch: 5| Step: 10
Training loss: 3.6169517885200677
Validation loss: 3.3559098821679685

Epoch: 23| Step: 0
Training loss: 4.023225826880359
Validation loss: 3.355592757100334

Epoch: 5| Step: 1
Training loss: 3.7062020052653644
Validation loss: 3.353717785892985

Epoch: 5| Step: 2
Training loss: 3.1465903038791545
Validation loss: 3.352675147897132

Epoch: 5| Step: 3
Training loss: 4.175424958490378
Validation loss: 3.3515133634094263

Epoch: 5| Step: 4
Training loss: 3.2547244432012787
Validation loss: 3.3508848260586928

Epoch: 5| Step: 5
Training loss: 3.262191062126477
Validation loss: 3.3496007264761465

Epoch: 5| Step: 6
Training loss: 3.6600099450898678
Validation loss: 3.348198359969665

Epoch: 5| Step: 7
Training loss: 4.106785876001236
Validation loss: 3.346958774043544

Epoch: 5| Step: 8
Training loss: 3.7920125586949593
Validation loss: 3.345433330218336

Epoch: 5| Step: 9
Training loss: 3.2357798303553764
Validation loss: 3.343140387164317

Epoch: 5| Step: 10
Training loss: 2.6638819837519367
Validation loss: 3.342438155363491

Epoch: 24| Step: 0
Training loss: 4.030268350001332
Validation loss: 3.3391365346907937

Epoch: 5| Step: 1
Training loss: 3.27564247177638
Validation loss: 3.3360795665181993

Epoch: 5| Step: 2
Training loss: 3.3852544344642057
Validation loss: 3.3315763493465758

Epoch: 5| Step: 3
Training loss: 3.4754620471298545
Validation loss: 3.323409464272237

Epoch: 5| Step: 4
Training loss: 3.644851523306968
Validation loss: 3.3205316510585074

Epoch: 5| Step: 5
Training loss: 3.7405602216091074
Validation loss: 3.3159603155978545

Epoch: 5| Step: 6
Training loss: 3.7803762387936914
Validation loss: 3.314615867498751

Epoch: 5| Step: 7
Training loss: 3.575894389280614
Validation loss: 3.3235239363896407

Epoch: 5| Step: 8
Training loss: 3.1507452370845903
Validation loss: 3.3095848442331843

Epoch: 5| Step: 9
Training loss: 3.7227092791179723
Validation loss: 3.317466448201176

Epoch: 5| Step: 10
Training loss: 3.2836717660309485
Validation loss: 3.3143781746003436

Epoch: 25| Step: 0
Training loss: 3.519765357086366
Validation loss: 3.3062192036334874

Epoch: 5| Step: 1
Training loss: 3.375930940310408
Validation loss: 3.3041954414076726

Epoch: 5| Step: 2
Training loss: 3.7309608334643136
Validation loss: 3.304275935843819

Epoch: 5| Step: 3
Training loss: 3.176275198460067
Validation loss: 3.3039436512627494

Epoch: 5| Step: 4
Training loss: 3.1678064871553433
Validation loss: 3.303895117816316

Epoch: 5| Step: 5
Training loss: 2.9885968608862945
Validation loss: 3.303181668062049

Epoch: 5| Step: 6
Training loss: 3.483100555405333
Validation loss: 3.314885712701667

Epoch: 5| Step: 7
Training loss: 4.502688770223745
Validation loss: 3.299000744320192

Epoch: 5| Step: 8
Training loss: 3.5514629788097127
Validation loss: 3.2973174168784465

Epoch: 5| Step: 9
Training loss: 3.9749367618928684
Validation loss: 3.2987796452977527

Epoch: 5| Step: 10
Training loss: 3.247398362188061
Validation loss: 3.297058495381672

Epoch: 26| Step: 0
Training loss: 3.8226068078357223
Validation loss: 3.2979026662038478

Epoch: 5| Step: 1
Training loss: 3.8967282632397833
Validation loss: 3.296011967255164

Epoch: 5| Step: 2
Training loss: 2.9532357654284995
Validation loss: 3.2935742486564688

Epoch: 5| Step: 3
Training loss: 2.609619403289145
Validation loss: 3.29207026332276

Epoch: 5| Step: 4
Training loss: 3.6225985761226136
Validation loss: 3.290719466237337

Epoch: 5| Step: 5
Training loss: 3.7341639247601304
Validation loss: 3.2899283531958594

Epoch: 5| Step: 6
Training loss: 3.0023092283298616
Validation loss: 3.2886535048841865

Epoch: 5| Step: 7
Training loss: 3.747438954981585
Validation loss: 3.287682085918465

Epoch: 5| Step: 8
Training loss: 3.906780847718752
Validation loss: 3.28738764350932

Epoch: 5| Step: 9
Training loss: 3.4496868157250034
Validation loss: 3.2871351648738085

Epoch: 5| Step: 10
Training loss: 3.927907734084454
Validation loss: 3.2850979464052124

Epoch: 27| Step: 0
Training loss: 3.3354591902230726
Validation loss: 3.2844595654766406

Epoch: 5| Step: 1
Training loss: 3.6531245248759983
Validation loss: 3.282892370670979

Epoch: 5| Step: 2
Training loss: 3.73913449681585
Validation loss: 3.281576719488838

Epoch: 5| Step: 3
Training loss: 3.1971243872000046
Validation loss: 3.280638688309632

Epoch: 5| Step: 4
Training loss: 3.8617808144637054
Validation loss: 3.279439176657272

Epoch: 5| Step: 5
Training loss: 3.1320662905911214
Validation loss: 3.279700044126859

Epoch: 5| Step: 6
Training loss: 3.6224061301674886
Validation loss: 3.278196726300475

Epoch: 5| Step: 7
Training loss: 3.3912192672084966
Validation loss: 3.276860156183434

Epoch: 5| Step: 8
Training loss: 3.5078338418494006
Validation loss: 3.2760294772460226

Epoch: 5| Step: 9
Training loss: 3.2585621069737005
Validation loss: 3.275230894424751

Epoch: 5| Step: 10
Training loss: 4.07090640723174
Validation loss: 3.27494622920411

Epoch: 28| Step: 0
Training loss: 2.9166405449560364
Validation loss: 3.2733877436033363

Epoch: 5| Step: 1
Training loss: 3.402605399265803
Validation loss: 3.2722141420324844

Epoch: 5| Step: 2
Training loss: 4.115425558044236
Validation loss: 3.2712294730101283

Epoch: 5| Step: 3
Training loss: 3.6433030130765647
Validation loss: 3.272022322516124

Epoch: 5| Step: 4
Training loss: 3.453420082706296
Validation loss: 3.276376073153504

Epoch: 5| Step: 5
Training loss: 3.8696580797894766
Validation loss: 3.270157700288039

Epoch: 5| Step: 6
Training loss: 4.350086667304632
Validation loss: 3.2675593125493836

Epoch: 5| Step: 7
Training loss: 3.2671252733212888
Validation loss: 3.267766729954276

Epoch: 5| Step: 8
Training loss: 3.1547063697526445
Validation loss: 3.266979563253418

Epoch: 5| Step: 9
Training loss: 2.972979133304334
Validation loss: 3.266626999454942

Epoch: 5| Step: 10
Training loss: 3.2258944431732566
Validation loss: 3.2679247825644904

Epoch: 29| Step: 0
Training loss: 3.2208265994562093
Validation loss: 3.2680365149689803

Epoch: 5| Step: 1
Training loss: 3.813605101466863
Validation loss: 3.2690286281256165

Epoch: 5| Step: 2
Training loss: 3.324402639321115
Validation loss: 3.2682212059134197

Epoch: 5| Step: 3
Training loss: 3.7426649199226776
Validation loss: 3.263061621128789

Epoch: 5| Step: 4
Training loss: 3.52027619297016
Validation loss: 3.2606274265636124

Epoch: 5| Step: 5
Training loss: 2.584725179322651
Validation loss: 3.2592551888298247

Epoch: 5| Step: 6
Training loss: 3.578040109485282
Validation loss: 3.2581512777867907

Epoch: 5| Step: 7
Training loss: 4.162890414237702
Validation loss: 3.257257890770982

Epoch: 5| Step: 8
Training loss: 2.477423966335227
Validation loss: 3.256247758591983

Epoch: 5| Step: 9
Training loss: 3.9385303178670976
Validation loss: 3.2555819756789344

Epoch: 5| Step: 10
Training loss: 3.908448112011609
Validation loss: 3.254746176450124

Epoch: 30| Step: 0
Training loss: 3.742381431316939
Validation loss: 3.2539751922324123

Epoch: 5| Step: 1
Training loss: 2.9153604761560135
Validation loss: 3.252729090422476

Epoch: 5| Step: 2
Training loss: 3.00856670485361
Validation loss: 3.252829363711256

Epoch: 5| Step: 3
Training loss: 3.082521967627839
Validation loss: 3.2517872984476703

Epoch: 5| Step: 4
Training loss: 3.0130797402116696
Validation loss: 3.251657211743163

Epoch: 5| Step: 5
Training loss: 3.886386999032649
Validation loss: 3.250346383819188

Epoch: 5| Step: 6
Training loss: 3.5266020576986365
Validation loss: 3.2488786941738974

Epoch: 5| Step: 7
Training loss: 3.379535030006786
Validation loss: 3.2482054909449887

Epoch: 5| Step: 8
Training loss: 3.9904970296416837
Validation loss: 3.2467951469152188

Epoch: 5| Step: 9
Training loss: 3.7777250900209673
Validation loss: 3.2462100735014103

Epoch: 5| Step: 10
Training loss: 4.0368128992038645
Validation loss: 3.2454665569886787

Epoch: 31| Step: 0
Training loss: 2.874694476272075
Validation loss: 3.245814772053431

Epoch: 5| Step: 1
Training loss: 3.3619078557936666
Validation loss: 3.2494324077288215

Epoch: 5| Step: 2
Training loss: 4.296627523057987
Validation loss: 3.242680948174218

Epoch: 5| Step: 3
Training loss: 3.3365922414867697
Validation loss: 3.241772681103212

Epoch: 5| Step: 4
Training loss: 4.072615954655107
Validation loss: 3.2405333661011753

Epoch: 5| Step: 5
Training loss: 3.3763451897747565
Validation loss: 3.2393858765382855

Epoch: 5| Step: 6
Training loss: 3.7693810942000794
Validation loss: 3.238782127520135

Epoch: 5| Step: 7
Training loss: 3.676161062337993
Validation loss: 3.2384260711691653

Epoch: 5| Step: 8
Training loss: 2.8309100016386832
Validation loss: 3.2369423758261373

Epoch: 5| Step: 9
Training loss: 3.0275743359764564
Validation loss: 3.2366342792538276

Epoch: 5| Step: 10
Training loss: 3.5234510332680786
Validation loss: 3.2350085577574466

Epoch: 32| Step: 0
Training loss: 2.6766561818026378
Validation loss: 3.234684728046096

Epoch: 5| Step: 1
Training loss: 4.265941789761986
Validation loss: 3.2346697456973095

Epoch: 5| Step: 2
Training loss: 3.932783662783504
Validation loss: 3.233483498643313

Epoch: 5| Step: 3
Training loss: 3.7191585027895853
Validation loss: 3.2332618640358395

Epoch: 5| Step: 4
Training loss: 3.887439695424108
Validation loss: 3.2316140471350963

Epoch: 5| Step: 5
Training loss: 3.7269774391877335
Validation loss: 3.2303439521176394

Epoch: 5| Step: 6
Training loss: 3.3680401848685904
Validation loss: 3.2293026046376516

Epoch: 5| Step: 7
Training loss: 3.5535917836062936
Validation loss: 3.228443223229644

Epoch: 5| Step: 8
Training loss: 3.3556401272264207
Validation loss: 3.2276742641609246

Epoch: 5| Step: 9
Training loss: 2.9444097860763683
Validation loss: 3.226467109010408

Epoch: 5| Step: 10
Training loss: 2.2801087542579843
Validation loss: 3.2260274266881215

Epoch: 33| Step: 0
Training loss: 4.1633302427656576
Validation loss: 3.224619405002458

Epoch: 5| Step: 1
Training loss: 3.374088446823062
Validation loss: 3.2240330967424824

Epoch: 5| Step: 2
Training loss: 3.406306975001927
Validation loss: 3.223081502342233

Epoch: 5| Step: 3
Training loss: 3.9146606231958097
Validation loss: 3.2221214748153906

Epoch: 5| Step: 4
Training loss: 3.0114095209987
Validation loss: 3.221224340260822

Epoch: 5| Step: 5
Training loss: 3.615990063594568
Validation loss: 3.220330788109052

Epoch: 5| Step: 6
Training loss: 3.3865610545754503
Validation loss: 3.2194121018600717

Epoch: 5| Step: 7
Training loss: 2.6094300955249587
Validation loss: 3.2185066609840804

Epoch: 5| Step: 8
Training loss: 3.089720442634779
Validation loss: 3.2178613152930193

Epoch: 5| Step: 9
Training loss: 3.2691479530028995
Validation loss: 3.216973186780331

Epoch: 5| Step: 10
Training loss: 4.20986885283338
Validation loss: 3.2161961065020486

Epoch: 34| Step: 0
Training loss: 2.9232859749113294
Validation loss: 3.215378839428951

Epoch: 5| Step: 1
Training loss: 3.467356633947183
Validation loss: 3.2143771172409457

Epoch: 5| Step: 2
Training loss: 3.946327245159873
Validation loss: 3.213551984368784

Epoch: 5| Step: 3
Training loss: 3.7199215365923135
Validation loss: 3.21237910899382

Epoch: 5| Step: 4
Training loss: 2.9129317393399727
Validation loss: 3.211904323263566

Epoch: 5| Step: 5
Training loss: 3.421236440222632
Validation loss: 3.2107241109819826

Epoch: 5| Step: 6
Training loss: 3.6845405470239534
Validation loss: 3.211451925454037

Epoch: 5| Step: 7
Training loss: 3.813986816655515
Validation loss: 3.2090374451366213

Epoch: 5| Step: 8
Training loss: 3.246888358388816
Validation loss: 3.2100769267797915

Epoch: 5| Step: 9
Training loss: 3.0290846808741168
Validation loss: 3.226492568389468

Epoch: 5| Step: 10
Training loss: 3.8771531060670976
Validation loss: 3.207371355895802

Epoch: 35| Step: 0
Training loss: 3.527138265747903
Validation loss: 3.2061674661885253

Epoch: 5| Step: 1
Training loss: 3.5806139156405035
Validation loss: 3.2058812769894605

Epoch: 5| Step: 2
Training loss: 4.086540104792338
Validation loss: 3.206877723558213

Epoch: 5| Step: 3
Training loss: 3.115253355153779
Validation loss: 3.2079305077626863

Epoch: 5| Step: 4
Training loss: 2.872543446450481
Validation loss: 3.2206715679061064

Epoch: 5| Step: 5
Training loss: 3.30219869482331
Validation loss: 3.2041711779029174

Epoch: 5| Step: 6
Training loss: 4.407722822194576
Validation loss: 3.2027262521774866

Epoch: 5| Step: 7
Training loss: 3.6052779026294197
Validation loss: 3.2022889029242525

Epoch: 5| Step: 8
Training loss: 3.280513862504989
Validation loss: 3.2025080853356367

Epoch: 5| Step: 9
Training loss: 3.198468056194112
Validation loss: 3.2020721922341364

Epoch: 5| Step: 10
Training loss: 2.6817522018791995
Validation loss: 3.2004149555016563

Epoch: 36| Step: 0
Training loss: 2.7988501026810693
Validation loss: 3.203321489399689

Epoch: 5| Step: 1
Training loss: 3.3860487753728683
Validation loss: 3.21715606145138

Epoch: 5| Step: 2
Training loss: 2.7984088123071547
Validation loss: 3.198020833689045

Epoch: 5| Step: 3
Training loss: 4.004186585079407
Validation loss: 3.1953824585174835

Epoch: 5| Step: 4
Training loss: 2.7994149891202333
Validation loss: 3.195333507693532

Epoch: 5| Step: 5
Training loss: 3.890928938303768
Validation loss: 3.1946341702720726

Epoch: 5| Step: 6
Training loss: 4.423097690801442
Validation loss: 3.197309643167808

Epoch: 5| Step: 7
Training loss: 2.8819989139255697
Validation loss: 3.1958396868454013

Epoch: 5| Step: 8
Training loss: 3.723674686895745
Validation loss: 3.194966081798086

Epoch: 5| Step: 9
Training loss: 3.6309256955431044
Validation loss: 3.1924133812265403

Epoch: 5| Step: 10
Training loss: 3.2225905533076302
Validation loss: 3.19093517912028

Epoch: 37| Step: 0
Training loss: 3.006834351655964
Validation loss: 3.190302490873616

Epoch: 5| Step: 1
Training loss: 3.5378937465051234
Validation loss: 3.188836419359874

Epoch: 5| Step: 2
Training loss: 3.5772092222005933
Validation loss: 3.1886731441364664

Epoch: 5| Step: 3
Training loss: 3.289511606901316
Validation loss: 3.1872932580973745

Epoch: 5| Step: 4
Training loss: 3.8538671566171105
Validation loss: 3.1870498017587807

Epoch: 5| Step: 5
Training loss: 3.188186571565771
Validation loss: 3.1869837949966784

Epoch: 5| Step: 6
Training loss: 4.201327595469452
Validation loss: 3.1855437436285254

Epoch: 5| Step: 7
Training loss: 3.2782582040493686
Validation loss: 3.1847786044378688

Epoch: 5| Step: 8
Training loss: 3.3766702828069404
Validation loss: 3.184302646816851

Epoch: 5| Step: 9
Training loss: 2.99602404019702
Validation loss: 3.1827072506734453

Epoch: 5| Step: 10
Training loss: 3.470778413470119
Validation loss: 3.1817345607947134

Epoch: 38| Step: 0
Training loss: 3.5245156679123126
Validation loss: 3.181074434337066

Epoch: 5| Step: 1
Training loss: 4.468461034008373
Validation loss: 3.1800821311438097

Epoch: 5| Step: 2
Training loss: 3.4135727869422867
Validation loss: 3.1794993579305513

Epoch: 5| Step: 3
Training loss: 3.1697135288027676
Validation loss: 3.178472245520564

Epoch: 5| Step: 4
Training loss: 3.4839578558456465
Validation loss: 3.176994401294623

Epoch: 5| Step: 5
Training loss: 2.9719197266880277
Validation loss: 3.1767499087411677

Epoch: 5| Step: 6
Training loss: 3.5678405885709217
Validation loss: 3.175161737106682

Epoch: 5| Step: 7
Training loss: 4.07094365533018
Validation loss: 3.174773493793923

Epoch: 5| Step: 8
Training loss: 2.5674054713245065
Validation loss: 3.1735972981030307

Epoch: 5| Step: 9
Training loss: 3.0578309882632824
Validation loss: 3.173330706342542

Epoch: 5| Step: 10
Training loss: 3.1376726688122405
Validation loss: 3.172016676700821

Epoch: 39| Step: 0
Training loss: 3.638014221549483
Validation loss: 3.1715612558915343

Epoch: 5| Step: 1
Training loss: 3.2219187129521276
Validation loss: 3.1705665106033902

Epoch: 5| Step: 2
Training loss: 3.3589076936967754
Validation loss: 3.170041001688984

Epoch: 5| Step: 3
Training loss: 3.10511848825626
Validation loss: 3.1693532980199906

Epoch: 5| Step: 4
Training loss: 3.402752401728739
Validation loss: 3.16839145153608

Epoch: 5| Step: 5
Training loss: 3.740729762803669
Validation loss: 3.1676055642158873

Epoch: 5| Step: 6
Training loss: 2.9444504343927584
Validation loss: 3.167201907438396

Epoch: 5| Step: 7
Training loss: 3.8337259229781475
Validation loss: 3.1660894786493805

Epoch: 5| Step: 8
Training loss: 3.693036981690317
Validation loss: 3.1652347298820036

Epoch: 5| Step: 9
Training loss: 3.6783882973808777
Validation loss: 3.164702958349751

Epoch: 5| Step: 10
Training loss: 2.9549935701303514
Validation loss: 3.1640724253881767

Epoch: 40| Step: 0
Training loss: 3.512242703499941
Validation loss: 3.163075290922345

Epoch: 5| Step: 1
Training loss: 3.7460970595169165
Validation loss: 3.1625688789762716

Epoch: 5| Step: 2
Training loss: 2.9518940292607647
Validation loss: 3.1613629170914748

Epoch: 5| Step: 3
Training loss: 3.7922429216293554
Validation loss: 3.1609154553894423

Epoch: 5| Step: 4
Training loss: 3.6238619892544315
Validation loss: 3.1599808245251397

Epoch: 5| Step: 5
Training loss: 2.9916502467761243
Validation loss: 3.1592400552381124

Epoch: 5| Step: 6
Training loss: 3.6283803503319225
Validation loss: 3.1580092430043645

Epoch: 5| Step: 7
Training loss: 3.5249009571635486
Validation loss: 3.157800273201114

Epoch: 5| Step: 8
Training loss: 2.965738797190201
Validation loss: 3.1569677556356885

Epoch: 5| Step: 9
Training loss: 3.6967613170489506
Validation loss: 3.1561045597420785

Epoch: 5| Step: 10
Training loss: 3.0626846569209927
Validation loss: 3.1552008094533086

Epoch: 41| Step: 0
Training loss: 3.408365694994513
Validation loss: 3.154161855574457

Epoch: 5| Step: 1
Training loss: 3.6924919348378777
Validation loss: 3.153727114944601

Epoch: 5| Step: 2
Training loss: 4.07626265172039
Validation loss: 3.152823053951352

Epoch: 5| Step: 3
Training loss: 3.600919918569244
Validation loss: 3.1524505359272106

Epoch: 5| Step: 4
Training loss: 3.202580996582733
Validation loss: 3.1510826860058208

Epoch: 5| Step: 5
Training loss: 4.059143792807601
Validation loss: 3.1507702489450895

Epoch: 5| Step: 6
Training loss: 3.2943184870513686
Validation loss: 3.14923946485847

Epoch: 5| Step: 7
Training loss: 2.6626383535051086
Validation loss: 3.1493483797044424

Epoch: 5| Step: 8
Training loss: 3.2184504397352813
Validation loss: 3.148097835095562

Epoch: 5| Step: 9
Training loss: 3.2035246809045423
Validation loss: 3.1470988634064696

Epoch: 5| Step: 10
Training loss: 2.8588026130481787
Validation loss: 3.1464993747603605

Epoch: 42| Step: 0
Training loss: 2.4927839085724313
Validation loss: 3.1455764882971065

Epoch: 5| Step: 1
Training loss: 3.1423408034612756
Validation loss: 3.1449735026413306

Epoch: 5| Step: 2
Training loss: 3.2262011457289264
Validation loss: 3.1440245493039716

Epoch: 5| Step: 3
Training loss: 3.059462461692314
Validation loss: 3.143673101632168

Epoch: 5| Step: 4
Training loss: 3.6889304280016884
Validation loss: 3.143533072808431

Epoch: 5| Step: 5
Training loss: 3.0686507447889295
Validation loss: 3.142265596994553

Epoch: 5| Step: 6
Training loss: 3.3303908394226864
Validation loss: 3.14130170128699

Epoch: 5| Step: 7
Training loss: 3.549427340887849
Validation loss: 3.140992809722667

Epoch: 5| Step: 8
Training loss: 4.135768359753373
Validation loss: 3.1407798802773814

Epoch: 5| Step: 9
Training loss: 3.899738347006099
Validation loss: 3.1397446943622827

Epoch: 5| Step: 10
Training loss: 3.7006138524340337
Validation loss: 3.139149766218801

Epoch: 43| Step: 0
Training loss: 2.653292816210583
Validation loss: 3.1385166726021567

Epoch: 5| Step: 1
Training loss: 3.9968027211231165
Validation loss: 3.137934397124543

Epoch: 5| Step: 2
Training loss: 3.780088009692757
Validation loss: 3.137925508331442

Epoch: 5| Step: 3
Training loss: 3.338883325449081
Validation loss: 3.1361878441186537

Epoch: 5| Step: 4
Training loss: 2.8459134099444867
Validation loss: 3.1352215549042204

Epoch: 5| Step: 5
Training loss: 3.927498846897706
Validation loss: 3.1350821077847018

Epoch: 5| Step: 6
Training loss: 3.797681683531888
Validation loss: 3.1357443548366524

Epoch: 5| Step: 7
Training loss: 3.0601065488727843
Validation loss: 3.1351249580112324

Epoch: 5| Step: 8
Training loss: 3.5578869851632917
Validation loss: 3.1369860889215375

Epoch: 5| Step: 9
Training loss: 3.192567126256213
Validation loss: 3.142292676816639

Epoch: 5| Step: 10
Training loss: 2.9822296900054823
Validation loss: 3.137266340691438

Epoch: 44| Step: 0
Training loss: 3.2700576996451898
Validation loss: 3.1361843062429013

Epoch: 5| Step: 1
Training loss: 3.5480493962355486
Validation loss: 3.1336399585721675

Epoch: 5| Step: 2
Training loss: 3.262190769784814
Validation loss: 3.1314313588159446

Epoch: 5| Step: 3
Training loss: 2.925528297643436
Validation loss: 3.129026317178499

Epoch: 5| Step: 4
Training loss: 4.343007634896865
Validation loss: 3.1278039129506063

Epoch: 5| Step: 5
Training loss: 3.3051945002078322
Validation loss: 3.126344497385503

Epoch: 5| Step: 6
Training loss: 3.8238908044555107
Validation loss: 3.1255387200772153

Epoch: 5| Step: 7
Training loss: 3.2472007140168926
Validation loss: 3.1255673966033295

Epoch: 5| Step: 8
Training loss: 3.6285312159040384
Validation loss: 3.124635630048584

Epoch: 5| Step: 9
Training loss: 2.6870282668460774
Validation loss: 3.1245175323491985

Epoch: 5| Step: 10
Training loss: 3.0329311467526563
Validation loss: 3.1234043469331323

Epoch: 45| Step: 0
Training loss: 3.5529657574803792
Validation loss: 3.1230684032735945

Epoch: 5| Step: 1
Training loss: 3.0776884282528285
Validation loss: 3.122815454335834

Epoch: 5| Step: 2
Training loss: 3.107534506031551
Validation loss: 3.1218575672853364

Epoch: 5| Step: 3
Training loss: 3.98561585495719
Validation loss: 3.1212842446452322

Epoch: 5| Step: 4
Training loss: 3.2104671532887843
Validation loss: 3.1196211761909964

Epoch: 5| Step: 5
Training loss: 4.089012141604553
Validation loss: 3.1188752596909812

Epoch: 5| Step: 6
Training loss: 3.231294205914748
Validation loss: 3.1178962180861594

Epoch: 5| Step: 7
Training loss: 3.2752146119131424
Validation loss: 3.1167862132923982

Epoch: 5| Step: 8
Training loss: 3.490890774218233
Validation loss: 3.1163196098275874

Epoch: 5| Step: 9
Training loss: 3.290033555019136
Validation loss: 3.115684367069176

Epoch: 5| Step: 10
Training loss: 2.7241241132326652
Validation loss: 3.115035981691235

Epoch: 46| Step: 0
Training loss: 3.388562678355896
Validation loss: 3.114063031948826

Epoch: 5| Step: 1
Training loss: 1.8044079320457147
Validation loss: 3.113347355277711

Epoch: 5| Step: 2
Training loss: 3.6625948896907463
Validation loss: 3.1121922801744866

Epoch: 5| Step: 3
Training loss: 3.8269452203877483
Validation loss: 3.1115536859918524

Epoch: 5| Step: 4
Training loss: 2.433653902685055
Validation loss: 3.1113001659286956

Epoch: 5| Step: 5
Training loss: 3.969991654932586
Validation loss: 3.110599380026974

Epoch: 5| Step: 6
Training loss: 3.3963877843511114
Validation loss: 3.109837667929676

Epoch: 5| Step: 7
Training loss: 3.596877884718652
Validation loss: 3.109532501144718

Epoch: 5| Step: 8
Training loss: 4.136493046324391
Validation loss: 3.1089309422293483

Epoch: 5| Step: 9
Training loss: 3.083187374749933
Validation loss: 3.107416725310402

Epoch: 5| Step: 10
Training loss: 3.2616298640583343
Validation loss: 3.1066525263330056

Epoch: 47| Step: 0
Training loss: 2.67178933524953
Validation loss: 3.1059878195597683

Epoch: 5| Step: 1
Training loss: 3.2706904936942585
Validation loss: 3.105348105501478

Epoch: 5| Step: 2
Training loss: 2.7206135644021705
Validation loss: 3.1046032088990527

Epoch: 5| Step: 3
Training loss: 3.6763858439832364
Validation loss: 3.1041629843818366

Epoch: 5| Step: 4
Training loss: 3.4970468596497852
Validation loss: 3.1035775429927077

Epoch: 5| Step: 5
Training loss: 3.447844597715378
Validation loss: 3.1028503808995342

Epoch: 5| Step: 6
Training loss: 3.3338286190825066
Validation loss: 3.1023686144260885

Epoch: 5| Step: 7
Training loss: 3.520451873038582
Validation loss: 3.101586595906982

Epoch: 5| Step: 8
Training loss: 4.179959924679162
Validation loss: 3.100878657614262

Epoch: 5| Step: 9
Training loss: 3.4842105022654244
Validation loss: 3.099905698799407

Epoch: 5| Step: 10
Training loss: 3.101141221192498
Validation loss: 3.09902448630962

Epoch: 48| Step: 0
Training loss: 3.1287005067538685
Validation loss: 3.098630515787394

Epoch: 5| Step: 1
Training loss: 3.4424089925866177
Validation loss: 3.0978408255922276

Epoch: 5| Step: 2
Training loss: 3.914335016749595
Validation loss: 3.097210147872819

Epoch: 5| Step: 3
Training loss: 2.4981614027694667
Validation loss: 3.0963717244978444

Epoch: 5| Step: 4
Training loss: 3.4973509163811416
Validation loss: 3.095646208846801

Epoch: 5| Step: 5
Training loss: 3.183573230139008
Validation loss: 3.094498733600814

Epoch: 5| Step: 6
Training loss: 3.3602405941064344
Validation loss: 3.0941744592238183

Epoch: 5| Step: 7
Training loss: 3.8098307786742693
Validation loss: 3.0934129309473293

Epoch: 5| Step: 8
Training loss: 3.6799580662867486
Validation loss: 3.092660482755243

Epoch: 5| Step: 9
Training loss: 3.3641856679945943
Validation loss: 3.0920831128560025

Epoch: 5| Step: 10
Training loss: 2.963145859248024
Validation loss: 3.091491163797364

Epoch: 49| Step: 0
Training loss: 3.5833700459100357
Validation loss: 3.090639650871335

Epoch: 5| Step: 1
Training loss: 3.002368468898291
Validation loss: 3.0900706627174594

Epoch: 5| Step: 2
Training loss: 3.296109368250432
Validation loss: 3.0890455609806162

Epoch: 5| Step: 3
Training loss: 3.6019819102322854
Validation loss: 3.08836330337406

Epoch: 5| Step: 4
Training loss: 2.9450518343497514
Validation loss: 3.087360239056211

Epoch: 5| Step: 5
Training loss: 3.711769797701351
Validation loss: 3.0865868201628444

Epoch: 5| Step: 6
Training loss: 3.0125766032123016
Validation loss: 3.0863695308749666

Epoch: 5| Step: 7
Training loss: 3.1710411441459114
Validation loss: 3.0853428828650675

Epoch: 5| Step: 8
Training loss: 3.840589014417077
Validation loss: 3.0845695740020687

Epoch: 5| Step: 9
Training loss: 3.410709646445795
Validation loss: 3.083931744836826

Epoch: 5| Step: 10
Training loss: 3.3591095575544005
Validation loss: 3.08383541847199

Epoch: 50| Step: 0
Training loss: 3.3941978931809103
Validation loss: 3.082496521751555

Epoch: 5| Step: 1
Training loss: 3.4353350671099654
Validation loss: 3.0820067799432453

Epoch: 5| Step: 2
Training loss: 3.686681365615579
Validation loss: 3.08127345649244

Epoch: 5| Step: 3
Training loss: 3.625547565014035
Validation loss: 3.0799889528944466

Epoch: 5| Step: 4
Training loss: 3.147800276948512
Validation loss: 3.0795164176704506

Epoch: 5| Step: 5
Training loss: 3.6996005383928625
Validation loss: 3.0785193429365023

Epoch: 5| Step: 6
Training loss: 3.6528412668748538
Validation loss: 3.0781508717881914

Epoch: 5| Step: 7
Training loss: 2.791749061013835
Validation loss: 3.077480238015372

Epoch: 5| Step: 8
Training loss: 3.212168362491178
Validation loss: 3.0760194439680104

Epoch: 5| Step: 9
Training loss: 3.0164645118491147
Validation loss: 3.0756245162750133

Epoch: 5| Step: 10
Training loss: 3.1743758541796265
Validation loss: 3.0743854203009744

Epoch: 51| Step: 0
Training loss: 4.010734221861764
Validation loss: 3.0733703367382077

Epoch: 5| Step: 1
Training loss: 3.439247103515257
Validation loss: 3.0730648972077343

Epoch: 5| Step: 2
Training loss: 3.386505577831893
Validation loss: 3.0741351979725517

Epoch: 5| Step: 3
Training loss: 3.703601532869056
Validation loss: 3.071533773500653

Epoch: 5| Step: 4
Training loss: 3.5750715395298522
Validation loss: 3.0710605817479926

Epoch: 5| Step: 5
Training loss: 2.784865611616109
Validation loss: 3.0704440286207837

Epoch: 5| Step: 6
Training loss: 2.9387898961080556
Validation loss: 3.0709227191144906

Epoch: 5| Step: 7
Training loss: 3.72290832370591
Validation loss: 3.0698893612251292

Epoch: 5| Step: 8
Training loss: 2.2981678963424437
Validation loss: 3.0690042814301624

Epoch: 5| Step: 9
Training loss: 3.517424398448915
Validation loss: 3.0698083829374343

Epoch: 5| Step: 10
Training loss: 3.1647506907352554
Validation loss: 3.0688144820501817

Epoch: 52| Step: 0
Training loss: 3.056172275920083
Validation loss: 3.068023998559867

Epoch: 5| Step: 1
Training loss: 3.3553701259698574
Validation loss: 3.0695659497276404

Epoch: 5| Step: 2
Training loss: 3.992428647277378
Validation loss: 3.0656404162670707

Epoch: 5| Step: 3
Training loss: 3.379412274699211
Validation loss: 3.0651312551805967

Epoch: 5| Step: 4
Training loss: 2.448516696411066
Validation loss: 3.0633115800140094

Epoch: 5| Step: 5
Training loss: 3.78585799476562
Validation loss: 3.06280546860222

Epoch: 5| Step: 6
Training loss: 2.8192725972221115
Validation loss: 3.061034856840968

Epoch: 5| Step: 7
Training loss: 3.310589797380303
Validation loss: 3.061787237516206

Epoch: 5| Step: 8
Training loss: 3.25964391811443
Validation loss: 3.0602277530244555

Epoch: 5| Step: 9
Training loss: 3.573639969788086
Validation loss: 3.058696429895986

Epoch: 5| Step: 10
Training loss: 3.638019988656771
Validation loss: 3.0552514226607212

Epoch: 53| Step: 0
Training loss: 3.7464633795092546
Validation loss: 3.0628764842141156

Epoch: 5| Step: 1
Training loss: 3.522614579621577
Validation loss: 3.1177153690256376

Epoch: 5| Step: 2
Training loss: 3.7638331538566905
Validation loss: 3.0619936389037328

Epoch: 5| Step: 3
Training loss: 3.453591431751275
Validation loss: 3.0560479455067844

Epoch: 5| Step: 4
Training loss: 3.4336781843998483
Validation loss: 3.0563825961582927

Epoch: 5| Step: 5
Training loss: 3.4340624180199364
Validation loss: 3.0790159806669823

Epoch: 5| Step: 6
Training loss: 2.5030938078690603
Validation loss: 3.1577984400572374

Epoch: 5| Step: 7
Training loss: 3.6532491775435543
Validation loss: 3.1427719197280197

Epoch: 5| Step: 8
Training loss: 2.753698029999583
Validation loss: 3.0519998324731827

Epoch: 5| Step: 9
Training loss: 2.8914206569655936
Validation loss: 3.0508531807722834

Epoch: 5| Step: 10
Training loss: 3.5209917262336834
Validation loss: 3.07208123270552

Epoch: 54| Step: 0
Training loss: 3.1491148719320377
Validation loss: 3.0861883711281144

Epoch: 5| Step: 1
Training loss: 3.1772930305359135
Validation loss: 3.084798650646819

Epoch: 5| Step: 2
Training loss: 2.826304402964506
Validation loss: 3.0651895943983942

Epoch: 5| Step: 3
Training loss: 3.7445147769660347
Validation loss: 3.063821080320041

Epoch: 5| Step: 4
Training loss: 4.116093122757896
Validation loss: 3.0549740646021624

Epoch: 5| Step: 5
Training loss: 3.29300755741516
Validation loss: 3.0529433970568864

Epoch: 5| Step: 6
Training loss: 3.4407060410936388
Validation loss: 3.0699183705206274

Epoch: 5| Step: 7
Training loss: 3.128369003062735
Validation loss: 3.0459297794376203

Epoch: 5| Step: 8
Training loss: 3.575867986345962
Validation loss: 3.049066314796952

Epoch: 5| Step: 9
Training loss: 3.151923509686281
Validation loss: 3.0504057497155435

Epoch: 5| Step: 10
Training loss: 2.9087485781191966
Validation loss: 3.0509428002762946

Epoch: 55| Step: 0
Training loss: 3.3482161313006444
Validation loss: 3.051731967296473

Epoch: 5| Step: 1
Training loss: 2.9238180136139458
Validation loss: 3.049742383884863

Epoch: 5| Step: 2
Training loss: 2.7417386548950295
Validation loss: 3.0452514748730604

Epoch: 5| Step: 3
Training loss: 3.4344681287223313
Validation loss: 3.041714862728774

Epoch: 5| Step: 4
Training loss: 3.302522423528742
Validation loss: 3.0390733127711123

Epoch: 5| Step: 5
Training loss: 3.814039951181233
Validation loss: 3.0451750374299125

Epoch: 5| Step: 6
Training loss: 3.6385427913815107
Validation loss: 3.038235140588956

Epoch: 5| Step: 7
Training loss: 3.6179164209702215
Validation loss: 3.0390506141742923

Epoch: 5| Step: 8
Training loss: 3.452139321062927
Validation loss: 3.041287874939402

Epoch: 5| Step: 9
Training loss: 3.2239764104694797
Validation loss: 3.0436483794716622

Epoch: 5| Step: 10
Training loss: 2.942575655396465
Validation loss: 3.045792001447987

Epoch: 56| Step: 0
Training loss: 3.672494519504226
Validation loss: 3.039092646260892

Epoch: 5| Step: 1
Training loss: 3.5906934925847445
Validation loss: 3.0336548332519007

Epoch: 5| Step: 2
Training loss: 3.4034916619795124
Validation loss: 3.032506450829529

Epoch: 5| Step: 3
Training loss: 3.553216046967623
Validation loss: 3.034315387147904

Epoch: 5| Step: 4
Training loss: 3.4390776741771094
Validation loss: 3.0320581902865413

Epoch: 5| Step: 5
Training loss: 3.53144795360881
Validation loss: 3.0326401880572282

Epoch: 5| Step: 6
Training loss: 2.4061025599106522
Validation loss: 3.0336042065744557

Epoch: 5| Step: 7
Training loss: 3.2112726561035547
Validation loss: 3.0288154103764016

Epoch: 5| Step: 8
Training loss: 3.2012244564111256
Validation loss: 3.0298205262161795

Epoch: 5| Step: 9
Training loss: 3.2973599190273286
Validation loss: 3.028393462460703

Epoch: 5| Step: 10
Training loss: 3.0260039878158365
Validation loss: 3.028380700074263

Epoch: 57| Step: 0
Training loss: 3.1068462267816983
Validation loss: 3.0311532097618397

Epoch: 5| Step: 1
Training loss: 3.7364321353711065
Validation loss: 3.0263641701224215

Epoch: 5| Step: 2
Training loss: 3.18293435224452
Validation loss: 3.026475155456079

Epoch: 5| Step: 3
Training loss: 2.6086436634480954
Validation loss: 3.0284685254695294

Epoch: 5| Step: 4
Training loss: 3.001107011635554
Validation loss: 3.0377441958306206

Epoch: 5| Step: 5
Training loss: 3.308138296942741
Validation loss: 3.0375660118601338

Epoch: 5| Step: 6
Training loss: 2.9956232569383228
Validation loss: 3.0336821346984655

Epoch: 5| Step: 7
Training loss: 3.3552277270829083
Validation loss: 3.0292908291524876

Epoch: 5| Step: 8
Training loss: 3.650690269520235
Validation loss: 3.0222765390560724

Epoch: 5| Step: 9
Training loss: 3.815851208279168
Validation loss: 3.0219480642343206

Epoch: 5| Step: 10
Training loss: 3.5803362414623447
Validation loss: 3.0206603501763065

Epoch: 58| Step: 0
Training loss: 3.5094547632923985
Validation loss: 3.0203709866287887

Epoch: 5| Step: 1
Training loss: 3.358923167516024
Validation loss: 3.020783714987354

Epoch: 5| Step: 2
Training loss: 3.1776314351220134
Validation loss: 3.0183447007270026

Epoch: 5| Step: 3
Training loss: 2.888325699496944
Validation loss: 3.018391909105639

Epoch: 5| Step: 4
Training loss: 3.973390524493136
Validation loss: 3.0173366729460245

Epoch: 5| Step: 5
Training loss: 3.4241060969947417
Validation loss: 3.016088009067748

Epoch: 5| Step: 6
Training loss: 3.1879961712190807
Validation loss: 3.0170915478357614

Epoch: 5| Step: 7
Training loss: 3.573449290681043
Validation loss: 3.0148168194246336

Epoch: 5| Step: 8
Training loss: 3.3092173019168976
Validation loss: 3.0142902045143907

Epoch: 5| Step: 9
Training loss: 3.244427819260223
Validation loss: 3.014160285105644

Epoch: 5| Step: 10
Training loss: 2.3796371564843652
Validation loss: 3.0104210756285705

Epoch: 59| Step: 0
Training loss: 3.007170373320209
Validation loss: 3.010783781181872

Epoch: 5| Step: 1
Training loss: 3.022594245596114
Validation loss: 3.0119670428646588

Epoch: 5| Step: 2
Training loss: 2.8235984532596423
Validation loss: 3.009726104122305

Epoch: 5| Step: 3
Training loss: 3.359314532068441
Validation loss: 3.0099545442866926

Epoch: 5| Step: 4
Training loss: 3.477171975193111
Validation loss: 3.011808341463629

Epoch: 5| Step: 5
Training loss: 2.497993713242719
Validation loss: 3.016258409982716

Epoch: 5| Step: 6
Training loss: 3.779779573906306
Validation loss: 3.031136278362578

Epoch: 5| Step: 7
Training loss: 3.435429591117376
Validation loss: 3.009701981464217

Epoch: 5| Step: 8
Training loss: 4.0439102919351155
Validation loss: 3.0102190864844003

Epoch: 5| Step: 9
Training loss: 3.1908572599069025
Validation loss: 3.012113021193087

Epoch: 5| Step: 10
Training loss: 3.47611382836024
Validation loss: 3.0143445013671397

Epoch: 60| Step: 0
Training loss: 3.1000328615969472
Validation loss: 3.006151805173918

Epoch: 5| Step: 1
Training loss: 2.8935970389555865
Validation loss: 3.0046333745001377

Epoch: 5| Step: 2
Training loss: 3.642632843506572
Validation loss: 3.0058795242483383

Epoch: 5| Step: 3
Training loss: 3.5871236995895788
Validation loss: 3.0033533614369476

Epoch: 5| Step: 4
Training loss: 2.9380646832475925
Validation loss: 3.0019567442112307

Epoch: 5| Step: 5
Training loss: 3.389017172158984
Validation loss: 3.0022790158559074

Epoch: 5| Step: 6
Training loss: 3.556410554234907
Validation loss: 3.0070819548733727

Epoch: 5| Step: 7
Training loss: 3.623175589442045
Validation loss: 3.0058737869501955

Epoch: 5| Step: 8
Training loss: 3.339035989444581
Validation loss: 3.005664527998597

Epoch: 5| Step: 9
Training loss: 2.875986344688372
Validation loss: 3.0066785716545157

Epoch: 5| Step: 10
Training loss: 3.167817475532417
Validation loss: 3.0020274059170045

Epoch: 61| Step: 0
Training loss: 2.9219668776810486
Validation loss: 2.99995724500591

Epoch: 5| Step: 1
Training loss: 3.4694269567910347
Validation loss: 3.002572503398908

Epoch: 5| Step: 2
Training loss: 3.3132887566968723
Validation loss: 3.000067223886116

Epoch: 5| Step: 3
Training loss: 3.9128069967415975
Validation loss: 2.996407302552969

Epoch: 5| Step: 4
Training loss: 3.234529500763379
Validation loss: 2.9954292452593094

Epoch: 5| Step: 5
Training loss: 3.645864882559737
Validation loss: 2.9942100824068203

Epoch: 5| Step: 6
Training loss: 2.851884045173769
Validation loss: 2.991118462963246

Epoch: 5| Step: 7
Training loss: 3.660153514164866
Validation loss: 2.9907111924943504

Epoch: 5| Step: 8
Training loss: 2.513939334182937
Validation loss: 2.990542049095279

Epoch: 5| Step: 9
Training loss: 2.7975768162439514
Validation loss: 2.9891935573704322

Epoch: 5| Step: 10
Training loss: 3.623084779114393
Validation loss: 2.98949980357363

Epoch: 62| Step: 0
Training loss: 2.9954940016572964
Validation loss: 2.9889250622963117

Epoch: 5| Step: 1
Training loss: 3.5279685108224097
Validation loss: 2.9870602356525726

Epoch: 5| Step: 2
Training loss: 3.824984761126711
Validation loss: 2.9890854126338233

Epoch: 5| Step: 3
Training loss: 3.286130345699297
Validation loss: 3.0123925758488315

Epoch: 5| Step: 4
Training loss: 2.9368847141726246
Validation loss: 2.997710452387669

Epoch: 5| Step: 5
Training loss: 2.990087025884578
Validation loss: 2.9989441582588166

Epoch: 5| Step: 6
Training loss: 2.916922222022489
Validation loss: 2.994935152630042

Epoch: 5| Step: 7
Training loss: 3.1747910198010167
Validation loss: 2.9899482865881972

Epoch: 5| Step: 8
Training loss: 3.24119269494339
Validation loss: 2.9892042709427766

Epoch: 5| Step: 9
Training loss: 3.801335857893169
Validation loss: 2.9886055453162435

Epoch: 5| Step: 10
Training loss: 3.26590796891924
Validation loss: 2.9867806051296624

Epoch: 63| Step: 0
Training loss: 3.4028726335309964
Validation loss: 2.9928667862168377

Epoch: 5| Step: 1
Training loss: 3.293110365893923
Validation loss: 2.993709237549517

Epoch: 5| Step: 2
Training loss: 2.7206825315814758
Validation loss: 2.9887810007375686

Epoch: 5| Step: 3
Training loss: 4.19509472272868
Validation loss: 2.9859265760666016

Epoch: 5| Step: 4
Training loss: 2.9264203788110326
Validation loss: 2.9834775764580397

Epoch: 5| Step: 5
Training loss: 3.4153897915587454
Validation loss: 2.9810344974249294

Epoch: 5| Step: 6
Training loss: 3.3993294783986387
Validation loss: 2.980100221406971

Epoch: 5| Step: 7
Training loss: 2.6215824405004104
Validation loss: 2.980099804184159

Epoch: 5| Step: 8
Training loss: 2.8217854972493166
Validation loss: 2.983966119252949

Epoch: 5| Step: 9
Training loss: 3.486255360760265
Validation loss: 2.98687593306883

Epoch: 5| Step: 10
Training loss: 3.4439761106612896
Validation loss: 2.9773805391155514

Epoch: 64| Step: 0
Training loss: 3.3494405364700284
Validation loss: 2.975438665979445

Epoch: 5| Step: 1
Training loss: 3.0959543409011743
Validation loss: 2.9767489534361427

Epoch: 5| Step: 2
Training loss: 3.41529108284228
Validation loss: 2.975718183753773

Epoch: 5| Step: 3
Training loss: 3.2847585176519614
Validation loss: 2.975034237495122

Epoch: 5| Step: 4
Training loss: 4.073991690550894
Validation loss: 2.974428368209125

Epoch: 5| Step: 5
Training loss: 2.9103427084771534
Validation loss: 2.9728471049713794

Epoch: 5| Step: 6
Training loss: 2.4576734422267554
Validation loss: 2.972747608582469

Epoch: 5| Step: 7
Training loss: 2.562589597298668
Validation loss: 2.97150692174986

Epoch: 5| Step: 8
Training loss: 3.7078605664815325
Validation loss: 2.9696688548590164

Epoch: 5| Step: 9
Training loss: 3.3510933927876523
Validation loss: 2.9712296117979324

Epoch: 5| Step: 10
Training loss: 3.3424019636386713
Validation loss: 2.9751584722360835

Epoch: 65| Step: 0
Training loss: 3.5731139429791097
Validation loss: 2.9891323481050263

Epoch: 5| Step: 1
Training loss: 3.226801163757576
Validation loss: 2.975556783948864

Epoch: 5| Step: 2
Training loss: 3.6708844086423875
Validation loss: 2.96914232616088

Epoch: 5| Step: 3
Training loss: 3.002096873845615
Validation loss: 2.9673124916525664

Epoch: 5| Step: 4
Training loss: 2.5868868741788043
Validation loss: 2.9652087486707392

Epoch: 5| Step: 5
Training loss: 3.278110419023667
Validation loss: 2.9653215975612546

Epoch: 5| Step: 6
Training loss: 3.3846775754470153
Validation loss: 2.9657889797398074

Epoch: 5| Step: 7
Training loss: 3.6014138147606594
Validation loss: 2.9636878558531587

Epoch: 5| Step: 8
Training loss: 3.2193327441040855
Validation loss: 2.967959241576481

Epoch: 5| Step: 9
Training loss: 3.2548354929709906
Validation loss: 2.966305931417922

Epoch: 5| Step: 10
Training loss: 2.8103310592956476
Validation loss: 2.9669129867259803

Epoch: 66| Step: 0
Training loss: 3.4788284289627134
Validation loss: 2.9649745489643733

Epoch: 5| Step: 1
Training loss: 3.0601458161959925
Validation loss: 2.959644624663015

Epoch: 5| Step: 2
Training loss: 3.269348066478263
Validation loss: 2.9614134656587843

Epoch: 5| Step: 3
Training loss: 2.89814464806763
Validation loss: 2.956964746151842

Epoch: 5| Step: 4
Training loss: 3.6785352972322283
Validation loss: 2.959348666980535

Epoch: 5| Step: 5
Training loss: 3.1577872885443665
Validation loss: 2.960197835791807

Epoch: 5| Step: 6
Training loss: 3.32031407973308
Validation loss: 2.957353056586685

Epoch: 5| Step: 7
Training loss: 3.1269346733430456
Validation loss: 2.9559327096348254

Epoch: 5| Step: 8
Training loss: 3.6113882023717667
Validation loss: 2.9559348032637778

Epoch: 5| Step: 9
Training loss: 3.0982297580500693
Validation loss: 2.954881541438803

Epoch: 5| Step: 10
Training loss: 2.8883494725320333
Validation loss: 2.9568545920698877

Epoch: 67| Step: 0
Training loss: 3.379633513225335
Validation loss: 2.9613839942536293

Epoch: 5| Step: 1
Training loss: 3.346327599248056
Validation loss: 2.9628685160536326

Epoch: 5| Step: 2
Training loss: 3.224221181348656
Validation loss: 2.975885313934229

Epoch: 5| Step: 3
Training loss: 3.4563583495004004
Validation loss: 2.9529201811781665

Epoch: 5| Step: 4
Training loss: 3.4837643212228344
Validation loss: 2.9548267644492174

Epoch: 5| Step: 5
Training loss: 2.4610889206577893
Validation loss: 2.969465639896633

Epoch: 5| Step: 6
Training loss: 3.4294162373594927
Validation loss: 3.000696140791466

Epoch: 5| Step: 7
Training loss: 3.6288124294081596
Validation loss: 2.962717222127623

Epoch: 5| Step: 8
Training loss: 2.424558764556823
Validation loss: 2.9514224263134987

Epoch: 5| Step: 9
Training loss: 3.5999568247855174
Validation loss: 2.955170374011412

Epoch: 5| Step: 10
Training loss: 3.112613697349972
Validation loss: 2.957056726504243

Epoch: 68| Step: 0
Training loss: 3.32557387024665
Validation loss: 2.9726851698278116

Epoch: 5| Step: 1
Training loss: 3.8661176955004133
Validation loss: 2.984445847770589

Epoch: 5| Step: 2
Training loss: 3.1201081707127147
Validation loss: 2.979639193282028

Epoch: 5| Step: 3
Training loss: 2.8997531588504537
Validation loss: 2.9593418510389147

Epoch: 5| Step: 4
Training loss: 2.9366729870777624
Validation loss: 2.9601914582890108

Epoch: 5| Step: 5
Training loss: 3.4461151364531344
Validation loss: 2.965994444973444

Epoch: 5| Step: 6
Training loss: 2.965782529548187
Validation loss: 3.016209046725675

Epoch: 5| Step: 7
Training loss: 3.400125579758766
Validation loss: 2.994524130978329

Epoch: 5| Step: 8
Training loss: 3.712238027660838
Validation loss: 2.947951698653091

Epoch: 5| Step: 9
Training loss: 2.571637771437519
Validation loss: 2.945870662628543

Epoch: 5| Step: 10
Training loss: 3.383783989444249
Validation loss: 2.965822519299947

Epoch: 69| Step: 0
Training loss: 3.049072412238391
Validation loss: 2.9482829760158107

Epoch: 5| Step: 1
Training loss: 3.2748016938019355
Validation loss: 2.977564456522677

Epoch: 5| Step: 2
Training loss: 3.3303620606133726
Validation loss: 2.9606550783677865

Epoch: 5| Step: 3
Training loss: 3.2312885983078328
Validation loss: 2.95227095553468

Epoch: 5| Step: 4
Training loss: 2.614822822534925
Validation loss: 2.9455469287928624

Epoch: 5| Step: 5
Training loss: 2.9934968399329365
Validation loss: 2.9421590887973035

Epoch: 5| Step: 6
Training loss: 3.232056159683514
Validation loss: 2.9406698539731995

Epoch: 5| Step: 7
Training loss: 3.899859396527406
Validation loss: 2.944229850938698

Epoch: 5| Step: 8
Training loss: 3.116255006445678
Validation loss: 2.942681271908396

Epoch: 5| Step: 9
Training loss: 3.5179514329430273
Validation loss: 2.943816096153378

Epoch: 5| Step: 10
Training loss: 3.2237955195040997
Validation loss: 2.938977870119699

Epoch: 70| Step: 0
Training loss: 2.9975262615462883
Validation loss: 2.943694036153101

Epoch: 5| Step: 1
Training loss: 3.0435190352083854
Validation loss: 2.9459159361082206

Epoch: 5| Step: 2
Training loss: 2.7302074825824536
Validation loss: 2.952997720014144

Epoch: 5| Step: 3
Training loss: 3.0382179322318783
Validation loss: 2.9844466328969914

Epoch: 5| Step: 4
Training loss: 3.8437583116891205
Validation loss: 2.958114245591161

Epoch: 5| Step: 5
Training loss: 3.125025634660483
Validation loss: 2.935363867313885

Epoch: 5| Step: 6
Training loss: 2.553897467195819
Validation loss: 2.9328836557353197

Epoch: 5| Step: 7
Training loss: 3.5936427888219975
Validation loss: 2.945225807250816

Epoch: 5| Step: 8
Training loss: 3.335831008390642
Validation loss: 2.9343116141585073

Epoch: 5| Step: 9
Training loss: 3.5088476340174704
Validation loss: 2.931631087107238

Epoch: 5| Step: 10
Training loss: 3.6509825755664522
Validation loss: 2.934338685789861

Epoch: 71| Step: 0
Training loss: 3.563402964949767
Validation loss: 2.935820588839639

Epoch: 5| Step: 1
Training loss: 2.9906971060770853
Validation loss: 2.926935377787423

Epoch: 5| Step: 2
Training loss: 3.047353227693819
Validation loss: 2.925587989718545

Epoch: 5| Step: 3
Training loss: 3.7824208362108713
Validation loss: 2.9253187860745693

Epoch: 5| Step: 4
Training loss: 2.5185028106328424
Validation loss: 2.9247950387864345

Epoch: 5| Step: 5
Training loss: 3.3740173604775228
Validation loss: 2.9234601004554808

Epoch: 5| Step: 6
Training loss: 3.194169042758736
Validation loss: 2.9225093601939065

Epoch: 5| Step: 7
Training loss: 2.8531940965756912
Validation loss: 2.9244433807090755

Epoch: 5| Step: 8
Training loss: 3.337110826754776
Validation loss: 2.9221667230253536

Epoch: 5| Step: 9
Training loss: 3.478503013557992
Validation loss: 2.9217720463265477

Epoch: 5| Step: 10
Training loss: 3.105534621205872
Validation loss: 2.9174707426510675

Epoch: 72| Step: 0
Training loss: 3.2671455603178576
Validation loss: 2.919958901047971

Epoch: 5| Step: 1
Training loss: 2.7468224287155913
Validation loss: 2.91702076275766

Epoch: 5| Step: 2
Training loss: 2.7833095168809834
Validation loss: 2.9321099083655993

Epoch: 5| Step: 3
Training loss: 3.5104413281639455
Validation loss: 2.9702085995290775

Epoch: 5| Step: 4
Training loss: 3.2204854786413257
Validation loss: 2.9252420104254218

Epoch: 5| Step: 5
Training loss: 2.0928602463184487
Validation loss: 2.9145678678815004

Epoch: 5| Step: 6
Training loss: 2.750102908203034
Validation loss: 2.9139055892019634

Epoch: 5| Step: 7
Training loss: 3.580150447433365
Validation loss: 2.915949344468988

Epoch: 5| Step: 8
Training loss: 3.7925918344683462
Validation loss: 2.922765848148128

Epoch: 5| Step: 9
Training loss: 3.3924160634777243
Validation loss: 2.9384056498278297

Epoch: 5| Step: 10
Training loss: 3.961302490271804
Validation loss: 2.9376155714463654

Epoch: 73| Step: 0
Training loss: 3.6554739364844857
Validation loss: 2.9179637395093643

Epoch: 5| Step: 1
Training loss: 3.0165277424501964
Validation loss: 2.9089254263555073

Epoch: 5| Step: 2
Training loss: 4.094406191818129
Validation loss: 2.9085860922274613

Epoch: 5| Step: 3
Training loss: 3.025947573715062
Validation loss: 2.9246151486107737

Epoch: 5| Step: 4
Training loss: 2.3146482232511927
Validation loss: 2.9736468918160233

Epoch: 5| Step: 5
Training loss: 3.5692671366060646
Validation loss: 3.1256802530174896

Epoch: 5| Step: 6
Training loss: 3.5148352520075363
Validation loss: 2.9144681464393503

Epoch: 5| Step: 7
Training loss: 3.4426890654993785
Validation loss: 2.9117314072690066

Epoch: 5| Step: 8
Training loss: 2.916146113854488
Validation loss: 2.993964599998848

Epoch: 5| Step: 9
Training loss: 3.104956319504787
Validation loss: 3.14913121053389

Epoch: 5| Step: 10
Training loss: 2.9219237950784422
Validation loss: 3.2065292808301415

Epoch: 74| Step: 0
Training loss: 3.2332676267993636
Validation loss: 3.1327817555965973

Epoch: 5| Step: 1
Training loss: 4.007947179602349
Validation loss: 3.025982338304661

Epoch: 5| Step: 2
Training loss: 3.5067213098694876
Validation loss: 2.9388342436463497

Epoch: 5| Step: 3
Training loss: 3.5074580975705723
Validation loss: 2.906830626340882

Epoch: 5| Step: 4
Training loss: 3.010241511088126
Validation loss: 2.925429202201245

Epoch: 5| Step: 5
Training loss: 3.6307545729394737
Validation loss: 3.011488259260019

Epoch: 5| Step: 6
Training loss: 2.9214327064348993
Validation loss: 3.066445730935052

Epoch: 5| Step: 7
Training loss: 3.7067128759243624
Validation loss: 3.0716219700175262

Epoch: 5| Step: 8
Training loss: 2.048289036262758
Validation loss: 3.0456284246032794

Epoch: 5| Step: 9
Training loss: 3.2833364483452234
Validation loss: 3.007764448813656

Epoch: 5| Step: 10
Training loss: 2.4013474298000235
Validation loss: 2.9442341758706254

Epoch: 75| Step: 0
Training loss: 3.0762664295983897
Validation loss: 2.913059365363081

Epoch: 5| Step: 1
Training loss: 3.0882335764348574
Validation loss: 2.9025937732343605

Epoch: 5| Step: 2
Training loss: 3.597049028178599
Validation loss: 2.901443633576683

Epoch: 5| Step: 3
Training loss: 3.8405865312702963
Validation loss: 2.906964453628058

Epoch: 5| Step: 4
Training loss: 2.940455147421743
Validation loss: 2.912795668164647

Epoch: 5| Step: 5
Training loss: 3.28865393051361
Validation loss: 2.916840890582542

Epoch: 5| Step: 6
Training loss: 2.7795665661052547
Validation loss: 2.9243768427878334

Epoch: 5| Step: 7
Training loss: 3.767029053351453
Validation loss: 2.917174172672113

Epoch: 5| Step: 8
Training loss: 3.15731627327696
Validation loss: 2.908886021331427

Epoch: 5| Step: 9
Training loss: 2.7120958651250544
Validation loss: 2.902470384211581

Epoch: 5| Step: 10
Training loss: 2.7455239715502273
Validation loss: 2.898448343834099

Epoch: 76| Step: 0
Training loss: 3.4957283383145508
Validation loss: 2.897365817616516

Epoch: 5| Step: 1
Training loss: 3.922689284941683
Validation loss: 2.895388261740073

Epoch: 5| Step: 2
Training loss: 3.4433174152381767
Validation loss: 2.8926751181027597

Epoch: 5| Step: 3
Training loss: 2.9808176618858533
Validation loss: 2.894608772901135

Epoch: 5| Step: 4
Training loss: 3.343376637942493
Validation loss: 2.899546471748555

Epoch: 5| Step: 5
Training loss: 3.0085996712443355
Validation loss: 2.9021564648954445

Epoch: 5| Step: 6
Training loss: 3.071821235193996
Validation loss: 2.9046159698574154

Epoch: 5| Step: 7
Training loss: 2.4978923496296104
Validation loss: 2.90314752838953

Epoch: 5| Step: 8
Training loss: 2.7889580373141016
Validation loss: 2.8946606836541915

Epoch: 5| Step: 9
Training loss: 3.3075204524808464
Validation loss: 2.89734548797311

Epoch: 5| Step: 10
Training loss: 2.958394672313079
Validation loss: 2.892189800956719

Epoch: 77| Step: 0
Training loss: 3.5993053454842836
Validation loss: 2.8899934486172687

Epoch: 5| Step: 1
Training loss: 2.8382836087482457
Validation loss: 2.893806556225706

Epoch: 5| Step: 2
Training loss: 3.111549575501445
Validation loss: 2.8901757225669558

Epoch: 5| Step: 3
Training loss: 3.0789137086388343
Validation loss: 2.8948891594336756

Epoch: 5| Step: 4
Training loss: 3.3405566787513417
Validation loss: 2.898664140078561

Epoch: 5| Step: 5
Training loss: 3.155986057236191
Validation loss: 2.899500121462909

Epoch: 5| Step: 6
Training loss: 3.7184870009826647
Validation loss: 2.887352676230814

Epoch: 5| Step: 7
Training loss: 3.224139839613552
Validation loss: 2.8868067015574144

Epoch: 5| Step: 8
Training loss: 2.745523450516128
Validation loss: 2.8858124238815543

Epoch: 5| Step: 9
Training loss: 3.43574909354587
Validation loss: 2.8833202598863874

Epoch: 5| Step: 10
Training loss: 2.5278172705316817
Validation loss: 2.882704433893763

Epoch: 78| Step: 0
Training loss: 2.992602765223745
Validation loss: 2.880582207996866

Epoch: 5| Step: 1
Training loss: 3.393149424992172
Validation loss: 2.8796081493221206

Epoch: 5| Step: 2
Training loss: 2.7058533587085294
Validation loss: 2.879260857369896

Epoch: 5| Step: 3
Training loss: 3.01814315098439
Validation loss: 2.8781617119994003

Epoch: 5| Step: 4
Training loss: 3.1024889583222097
Validation loss: 2.8790563704327528

Epoch: 5| Step: 5
Training loss: 3.3086882918900997
Validation loss: 2.8788498243636482

Epoch: 5| Step: 6
Training loss: 2.9199878493800866
Validation loss: 2.876306597157042

Epoch: 5| Step: 7
Training loss: 3.2543115626731627
Validation loss: 2.8787998405703976

Epoch: 5| Step: 8
Training loss: 3.744478102047583
Validation loss: 2.8776222280034713

Epoch: 5| Step: 9
Training loss: 3.243558221634659
Validation loss: 2.876326541619036

Epoch: 5| Step: 10
Training loss: 3.1685457844736384
Validation loss: 2.8760226688163946

Epoch: 79| Step: 0
Training loss: 3.8245130041925965
Validation loss: 2.8749487392141195

Epoch: 5| Step: 1
Training loss: 2.779274485344417
Validation loss: 2.8736376555231975

Epoch: 5| Step: 2
Training loss: 3.165587107364756
Validation loss: 2.872846223778897

Epoch: 5| Step: 3
Training loss: 3.693436321746047
Validation loss: 2.87264576915645

Epoch: 5| Step: 4
Training loss: 2.9655407072575093
Validation loss: 2.873938223086832

Epoch: 5| Step: 5
Training loss: 2.769029149434856
Validation loss: 2.870221875218757

Epoch: 5| Step: 6
Training loss: 3.4753190806604137
Validation loss: 2.8677919742050046

Epoch: 5| Step: 7
Training loss: 2.859161202197485
Validation loss: 2.8706445575436534

Epoch: 5| Step: 8
Training loss: 2.3371880797482234
Validation loss: 2.8694012410588736

Epoch: 5| Step: 9
Training loss: 3.5755822087757774
Validation loss: 2.871286934436344

Epoch: 5| Step: 10
Training loss: 3.0790633112415517
Validation loss: 2.8743265985339153

Epoch: 80| Step: 0
Training loss: 2.485327004421891
Validation loss: 2.879273117897211

Epoch: 5| Step: 1
Training loss: 3.339713824684105
Validation loss: 2.895364069252092

Epoch: 5| Step: 2
Training loss: 3.5741089517373643
Validation loss: 2.885762090735631

Epoch: 5| Step: 3
Training loss: 3.3324802896613575
Validation loss: 2.8778178373131023

Epoch: 5| Step: 4
Training loss: 3.3715312227457876
Validation loss: 2.8759282135408837

Epoch: 5| Step: 5
Training loss: 2.582863908591855
Validation loss: 2.8714577787929643

Epoch: 5| Step: 6
Training loss: 4.009092011428962
Validation loss: 2.86957191358581

Epoch: 5| Step: 7
Training loss: 2.8407486388238388
Validation loss: 2.8710851530083508

Epoch: 5| Step: 8
Training loss: 2.677717724949375
Validation loss: 2.867128780041214

Epoch: 5| Step: 9
Training loss: 3.7233912892101357
Validation loss: 2.865154358153301

Epoch: 5| Step: 10
Training loss: 2.380660489219158
Validation loss: 2.868127281309566

Epoch: 81| Step: 0
Training loss: 2.850721261294188
Validation loss: 2.8679127772390562

Epoch: 5| Step: 1
Training loss: 2.5183624158686757
Validation loss: 2.866648234770524

Epoch: 5| Step: 2
Training loss: 3.2531252286619607
Validation loss: 2.869306484718516

Epoch: 5| Step: 3
Training loss: 3.4444837892671125
Validation loss: 2.8638328465588248

Epoch: 5| Step: 4
Training loss: 3.163144126703111
Validation loss: 2.857083734070543

Epoch: 5| Step: 5
Training loss: 2.6258795036858027
Validation loss: 2.8589663770296303

Epoch: 5| Step: 6
Training loss: 3.162026435466482
Validation loss: 2.8603412804987887

Epoch: 5| Step: 7
Training loss: 3.9051392463738517
Validation loss: 2.857102001098769

Epoch: 5| Step: 8
Training loss: 3.3635993833641336
Validation loss: 2.857618692122248

Epoch: 5| Step: 9
Training loss: 2.954369983931113
Validation loss: 2.858640233319218

Epoch: 5| Step: 10
Training loss: 3.3223940798119997
Validation loss: 2.858164228298645

Epoch: 82| Step: 0
Training loss: 3.652353411167035
Validation loss: 2.8590411448513415

Epoch: 5| Step: 1
Training loss: 3.3360945867855696
Validation loss: 2.8587933316366083

Epoch: 5| Step: 2
Training loss: 2.5752001267967892
Validation loss: 2.859855049855105

Epoch: 5| Step: 3
Training loss: 3.8637450646597555
Validation loss: 2.857390828586005

Epoch: 5| Step: 4
Training loss: 3.011791421332625
Validation loss: 2.856284238419321

Epoch: 5| Step: 5
Training loss: 3.0953238751329035
Validation loss: 2.856889583822062

Epoch: 5| Step: 6
Training loss: 3.4210312730686163
Validation loss: 2.8524574878999576

Epoch: 5| Step: 7
Training loss: 3.0864575684197044
Validation loss: 2.849918089910947

Epoch: 5| Step: 8
Training loss: 2.963286180439314
Validation loss: 2.8486028168502004

Epoch: 5| Step: 9
Training loss: 2.90878529866976
Validation loss: 2.848981986142901

Epoch: 5| Step: 10
Training loss: 2.5645865342476335
Validation loss: 2.851563145503568

Epoch: 83| Step: 0
Training loss: 2.988524263760535
Validation loss: 2.8489115454483405

Epoch: 5| Step: 1
Training loss: 3.5453319806252614
Validation loss: 2.844538369668804

Epoch: 5| Step: 2
Training loss: 3.020497393289498
Validation loss: 2.842587664946274

Epoch: 5| Step: 3
Training loss: 3.3997671047554343
Validation loss: 2.8423693763991

Epoch: 5| Step: 4
Training loss: 3.3411841128924813
Validation loss: 2.8407741041135948

Epoch: 5| Step: 5
Training loss: 3.4601114290408908
Validation loss: 2.8416163425057053

Epoch: 5| Step: 6
Training loss: 2.8985993784815443
Validation loss: 2.8397365692644594

Epoch: 5| Step: 7
Training loss: 2.689882685999692
Validation loss: 2.8448294063065145

Epoch: 5| Step: 8
Training loss: 3.2419549697770345
Validation loss: 2.844404168553098

Epoch: 5| Step: 9
Training loss: 2.2993889370096956
Validation loss: 2.8400038401129857

Epoch: 5| Step: 10
Training loss: 3.5973891698586775
Validation loss: 2.8381901836791243

Epoch: 84| Step: 0
Training loss: 3.403735151142929
Validation loss: 2.838906269453609

Epoch: 5| Step: 1
Training loss: 2.845115417660087
Validation loss: 2.842059094633457

Epoch: 5| Step: 2
Training loss: 3.462775781989502
Validation loss: 2.844930635182618

Epoch: 5| Step: 3
Training loss: 2.9148352368020114
Validation loss: 2.8475781492804093

Epoch: 5| Step: 4
Training loss: 3.5830336304943353
Validation loss: 2.8482725540316434

Epoch: 5| Step: 5
Training loss: 2.8701746708687237
Validation loss: 2.8501340310330496

Epoch: 5| Step: 6
Training loss: 3.3871651853517397
Validation loss: 2.8495932991742627

Epoch: 5| Step: 7
Training loss: 3.545857422175691
Validation loss: 2.8476191706108374

Epoch: 5| Step: 8
Training loss: 2.851571195406263
Validation loss: 2.847536078508128

Epoch: 5| Step: 9
Training loss: 3.175136599268011
Validation loss: 2.846827954749081

Epoch: 5| Step: 10
Training loss: 2.3747010293809345
Validation loss: 2.838708562235459

Epoch: 85| Step: 0
Training loss: 3.1390876613309193
Validation loss: 2.8387043736497772

Epoch: 5| Step: 1
Training loss: 3.2980042462619443
Validation loss: 2.8397296594230643

Epoch: 5| Step: 2
Training loss: 2.8823021744588364
Validation loss: 2.835106408587908

Epoch: 5| Step: 3
Training loss: 2.5160829115459284
Validation loss: 2.836037914514613

Epoch: 5| Step: 4
Training loss: 3.3482968797941526
Validation loss: 2.832221249666202

Epoch: 5| Step: 5
Training loss: 3.936740741490408
Validation loss: 2.8359165799372654

Epoch: 5| Step: 6
Training loss: 3.194792091876237
Validation loss: 2.8344084833932865

Epoch: 5| Step: 7
Training loss: 2.636200487984156
Validation loss: 2.836188293701813

Epoch: 5| Step: 8
Training loss: 3.1409914932111493
Validation loss: 2.8415934830553535

Epoch: 5| Step: 9
Training loss: 3.174878131600124
Validation loss: 2.843844015881278

Epoch: 5| Step: 10
Training loss: 3.0851056100336396
Validation loss: 2.8478161250859477

Epoch: 86| Step: 0
Training loss: 3.443571935574466
Validation loss: 2.8395160563224286

Epoch: 5| Step: 1
Training loss: 2.8289183457565557
Validation loss: 2.8387240070273134

Epoch: 5| Step: 2
Training loss: 3.4252524234169277
Validation loss: 2.832717227423633

Epoch: 5| Step: 3
Training loss: 3.1985704745635215
Validation loss: 2.8271898305197247

Epoch: 5| Step: 4
Training loss: 3.031388741690361
Validation loss: 2.82639824993683

Epoch: 5| Step: 5
Training loss: 3.576883025532807
Validation loss: 2.8273173590678966

Epoch: 5| Step: 6
Training loss: 3.0675234952118555
Validation loss: 2.824803885809914

Epoch: 5| Step: 7
Training loss: 2.667450839217993
Validation loss: 2.8263452486947642

Epoch: 5| Step: 8
Training loss: 2.6734837379224947
Validation loss: 2.825543748719097

Epoch: 5| Step: 9
Training loss: 3.3044075633133425
Validation loss: 2.8253403599713685

Epoch: 5| Step: 10
Training loss: 3.2092773898100635
Validation loss: 2.822724658702514

Epoch: 87| Step: 0
Training loss: 3.2954607637639706
Validation loss: 2.8243452744544824

Epoch: 5| Step: 1
Training loss: 3.777098086553265
Validation loss: 2.8246804005307387

Epoch: 5| Step: 2
Training loss: 2.8988925134549968
Validation loss: 2.822438637344467

Epoch: 5| Step: 3
Training loss: 3.96512624551644
Validation loss: 2.8178653944321574

Epoch: 5| Step: 4
Training loss: 2.6502548778968715
Validation loss: 2.817999210954547

Epoch: 5| Step: 5
Training loss: 2.9612879080256196
Validation loss: 2.817346786047172

Epoch: 5| Step: 6
Training loss: 2.6657976781049233
Validation loss: 2.8168583479296623

Epoch: 5| Step: 7
Training loss: 3.2894072363315954
Validation loss: 2.817274755418697

Epoch: 5| Step: 8
Training loss: 3.5025884048882503
Validation loss: 2.8163006459946156

Epoch: 5| Step: 9
Training loss: 2.760641662257189
Validation loss: 2.815019155230343

Epoch: 5| Step: 10
Training loss: 1.9389224829521836
Validation loss: 2.8155892254486274

Epoch: 88| Step: 0
Training loss: 2.798074134907958
Validation loss: 2.8123689518412753

Epoch: 5| Step: 1
Training loss: 3.3692034640878954
Validation loss: 2.8117347852157364

Epoch: 5| Step: 2
Training loss: 3.009594040422557
Validation loss: 2.8106318751589203

Epoch: 5| Step: 3
Training loss: 3.244276729440999
Validation loss: 2.8100178648056326

Epoch: 5| Step: 4
Training loss: 3.0967763875433114
Validation loss: 2.8099346205920512

Epoch: 5| Step: 5
Training loss: 2.879273928140823
Validation loss: 2.805695714190552

Epoch: 5| Step: 6
Training loss: 2.709900407544719
Validation loss: 2.8072025994886016

Epoch: 5| Step: 7
Training loss: 3.534709349989587
Validation loss: 2.80601673024105

Epoch: 5| Step: 8
Training loss: 3.1859774973829564
Validation loss: 2.8065890985258055

Epoch: 5| Step: 9
Training loss: 3.19241283515823
Validation loss: 2.807623100741786

Epoch: 5| Step: 10
Training loss: 3.1913585869394447
Validation loss: 2.8068318520188003

Epoch: 89| Step: 0
Training loss: 2.6924244320889983
Validation loss: 2.8099162239295046

Epoch: 5| Step: 1
Training loss: 3.0898266199845166
Validation loss: 2.8309349622751143

Epoch: 5| Step: 2
Training loss: 3.2680743947053306
Validation loss: 2.8400236143525333

Epoch: 5| Step: 3
Training loss: 3.2404717375104513
Validation loss: 2.8292694477510314

Epoch: 5| Step: 4
Training loss: 2.7225001986865207
Validation loss: 2.8281351344356955

Epoch: 5| Step: 5
Training loss: 3.2149586987252694
Validation loss: 2.824562217643483

Epoch: 5| Step: 6
Training loss: 2.8797789949814767
Validation loss: 2.8091824032325303

Epoch: 5| Step: 7
Training loss: 3.5718606142570377
Validation loss: 2.806094975454296

Epoch: 5| Step: 8
Training loss: 2.435845816344585
Validation loss: 2.8028713496268103

Epoch: 5| Step: 9
Training loss: 3.780910255995453
Validation loss: 2.80138056079835

Epoch: 5| Step: 10
Training loss: 3.1268561382132742
Validation loss: 2.7964879818443107

Epoch: 90| Step: 0
Training loss: 2.4480976660076315
Validation loss: 2.7993146804064133

Epoch: 5| Step: 1
Training loss: 3.580803813734231
Validation loss: 2.800085024377775

Epoch: 5| Step: 2
Training loss: 2.9327957144710615
Validation loss: 2.8076856108714763

Epoch: 5| Step: 3
Training loss: 3.2820218586214125
Validation loss: 2.8041151412615677

Epoch: 5| Step: 4
Training loss: 2.948594265941247
Validation loss: 2.794217010142679

Epoch: 5| Step: 5
Training loss: 3.271378554725122
Validation loss: 2.797479744365483

Epoch: 5| Step: 6
Training loss: 3.449147000111962
Validation loss: 2.7939380517584897

Epoch: 5| Step: 7
Training loss: 3.027361706095942
Validation loss: 2.7948330691635204

Epoch: 5| Step: 8
Training loss: 2.9368048312029775
Validation loss: 2.7948957534472516

Epoch: 5| Step: 9
Training loss: 3.136930655688556
Validation loss: 2.789483822349576

Epoch: 5| Step: 10
Training loss: 3.05583852170558
Validation loss: 2.79235562886953

Epoch: 91| Step: 0
Training loss: 3.013643552655718
Validation loss: 2.791144684203642

Epoch: 5| Step: 1
Training loss: 2.845842869874345
Validation loss: 2.7929779454160277

Epoch: 5| Step: 2
Training loss: 2.902991981045866
Validation loss: 2.788878125488619

Epoch: 5| Step: 3
Training loss: 3.2832189556150917
Validation loss: 2.7870673526788017

Epoch: 5| Step: 4
Training loss: 2.835554112773171
Validation loss: 2.788505248743954

Epoch: 5| Step: 5
Training loss: 3.3565797043577774
Validation loss: 2.785860046676014

Epoch: 5| Step: 6
Training loss: 2.9623842755985383
Validation loss: 2.787006180255376

Epoch: 5| Step: 7
Training loss: 2.8569257074670036
Validation loss: 2.7863527051202603

Epoch: 5| Step: 8
Training loss: 3.1718556069969805
Validation loss: 2.7909347938225952

Epoch: 5| Step: 9
Training loss: 3.173441683204407
Validation loss: 2.782922521844436

Epoch: 5| Step: 10
Training loss: 3.7392041254062445
Validation loss: 2.7852684064802324

Epoch: 92| Step: 0
Training loss: 3.19042505428
Validation loss: 2.7878309084642185

Epoch: 5| Step: 1
Training loss: 3.197126922674854
Validation loss: 2.7896398393309223

Epoch: 5| Step: 2
Training loss: 2.612539417024243
Validation loss: 2.7889357077534593

Epoch: 5| Step: 3
Training loss: 3.1828079096255864
Validation loss: 2.7854783740964004

Epoch: 5| Step: 4
Training loss: 2.9449898216706374
Validation loss: 2.7883910011937276

Epoch: 5| Step: 5
Training loss: 2.9893358152494356
Validation loss: 2.7827449429648836

Epoch: 5| Step: 6
Training loss: 3.071392306243626
Validation loss: 2.7817716257987484

Epoch: 5| Step: 7
Training loss: 3.0326343010381267
Validation loss: 2.7851200349643346

Epoch: 5| Step: 8
Training loss: 3.3989550667755926
Validation loss: 2.7829937042450106

Epoch: 5| Step: 9
Training loss: 3.0941035521426614
Validation loss: 2.8017247273186188

Epoch: 5| Step: 10
Training loss: 3.450741320589113
Validation loss: 2.805373303452113

Epoch: 93| Step: 0
Training loss: 2.7213965502393775
Validation loss: 2.8086189965909822

Epoch: 5| Step: 1
Training loss: 2.7036595339725875
Validation loss: 2.7978320198197646

Epoch: 5| Step: 2
Training loss: 3.3633081875917465
Validation loss: 2.7872571752146884

Epoch: 5| Step: 3
Training loss: 2.9590145096959746
Validation loss: 2.7810590954336467

Epoch: 5| Step: 4
Training loss: 3.095992999539605
Validation loss: 2.774837747605019

Epoch: 5| Step: 5
Training loss: 3.0796839450329876
Validation loss: 2.778235055515833

Epoch: 5| Step: 6
Training loss: 3.1246113344728355
Validation loss: 2.7728281537501167

Epoch: 5| Step: 7
Training loss: 3.2964205315649355
Validation loss: 2.7730277928583944

Epoch: 5| Step: 8
Training loss: 3.4762297396050577
Validation loss: 2.7748251780737476

Epoch: 5| Step: 9
Training loss: 3.0050757702305377
Validation loss: 2.7722643563966085

Epoch: 5| Step: 10
Training loss: 3.130978473151935
Validation loss: 2.7720617611680187

Epoch: 94| Step: 0
Training loss: 2.7215486350534737
Validation loss: 2.771606954130252

Epoch: 5| Step: 1
Training loss: 2.8214443820282735
Validation loss: 2.7734626238532933

Epoch: 5| Step: 2
Training loss: 3.2487938917071144
Validation loss: 2.769480151318265

Epoch: 5| Step: 3
Training loss: 3.5181211299200656
Validation loss: 2.774955871624348

Epoch: 5| Step: 4
Training loss: 3.3929005383462614
Validation loss: 2.7710601919788207

Epoch: 5| Step: 5
Training loss: 2.718704837117028
Validation loss: 2.7739627278985624

Epoch: 5| Step: 6
Training loss: 3.3167932866397973
Validation loss: 2.771250277457273

Epoch: 5| Step: 7
Training loss: 3.0376290125195005
Validation loss: 2.772962475839877

Epoch: 5| Step: 8
Training loss: 3.161200238941468
Validation loss: 2.777876399419032

Epoch: 5| Step: 9
Training loss: 3.056720184168279
Validation loss: 2.774854347983615

Epoch: 5| Step: 10
Training loss: 2.8553212932707197
Validation loss: 2.784345724655987

Epoch: 95| Step: 0
Training loss: 3.334874718642079
Validation loss: 2.7802367549841045

Epoch: 5| Step: 1
Training loss: 3.008063288550426
Validation loss: 2.775583873867055

Epoch: 5| Step: 2
Training loss: 2.903776202960111
Validation loss: 2.773285256412596

Epoch: 5| Step: 3
Training loss: 3.152235140341672
Validation loss: 2.7675089564238107

Epoch: 5| Step: 4
Training loss: 2.61712609973452
Validation loss: 2.7614909079509085

Epoch: 5| Step: 5
Training loss: 2.9215321059139896
Validation loss: 2.760971677262323

Epoch: 5| Step: 6
Training loss: 3.1572242970433413
Validation loss: 2.756402654947551

Epoch: 5| Step: 7
Training loss: 3.2662279950352144
Validation loss: 2.7560220338702734

Epoch: 5| Step: 8
Training loss: 3.2699836226586907
Validation loss: 2.7590082083971295

Epoch: 5| Step: 9
Training loss: 2.8948498343358837
Validation loss: 2.7604690320803584

Epoch: 5| Step: 10
Training loss: 3.381285853733488
Validation loss: 2.756296317344511

Epoch: 96| Step: 0
Training loss: 2.919538900208652
Validation loss: 2.759640310791018

Epoch: 5| Step: 1
Training loss: 3.038010756010863
Validation loss: 2.7568320634448753

Epoch: 5| Step: 2
Training loss: 3.3737943403136508
Validation loss: 2.7612811876442893

Epoch: 5| Step: 3
Training loss: 3.1718811636427953
Validation loss: 2.753507216963945

Epoch: 5| Step: 4
Training loss: 3.3712233146089403
Validation loss: 2.75500755866453

Epoch: 5| Step: 5
Training loss: 3.4316989848125203
Validation loss: 2.7550552660304617

Epoch: 5| Step: 6
Training loss: 2.891989061337542
Validation loss: 2.7568308694239074

Epoch: 5| Step: 7
Training loss: 2.302375176574147
Validation loss: 2.754356450925992

Epoch: 5| Step: 8
Training loss: 3.424398945882907
Validation loss: 2.7540967821991313

Epoch: 5| Step: 9
Training loss: 2.8968034029675427
Validation loss: 2.7552960132941338

Epoch: 5| Step: 10
Training loss: 2.7999330137959344
Validation loss: 2.7516744704544647

Epoch: 97| Step: 0
Training loss: 2.8741070563141493
Validation loss: 2.752674958987435

Epoch: 5| Step: 1
Training loss: 2.906815832696885
Validation loss: 2.7501506941255602

Epoch: 5| Step: 2
Training loss: 3.3391506613631896
Validation loss: 2.750751454218986

Epoch: 5| Step: 3
Training loss: 3.031924772371073
Validation loss: 2.7534813188791922

Epoch: 5| Step: 4
Training loss: 3.509565769082011
Validation loss: 2.7500494601070566

Epoch: 5| Step: 5
Training loss: 3.3921765412057407
Validation loss: 2.748180421346986

Epoch: 5| Step: 6
Training loss: 2.557709286237925
Validation loss: 2.7486341539413126

Epoch: 5| Step: 7
Training loss: 3.62607085097412
Validation loss: 2.747506600748914

Epoch: 5| Step: 8
Training loss: 2.7215262959387485
Validation loss: 2.7487565209591462

Epoch: 5| Step: 9
Training loss: 2.851016308457191
Validation loss: 2.7474538980769276

Epoch: 5| Step: 10
Training loss: 2.7180544961121456
Validation loss: 2.748050571129589

Epoch: 98| Step: 0
Training loss: 2.8162676688502257
Validation loss: 2.7496623878903335

Epoch: 5| Step: 1
Training loss: 2.0423195502179956
Validation loss: 2.753969435765875

Epoch: 5| Step: 2
Training loss: 3.0430268856268667
Validation loss: 2.756374903452602

Epoch: 5| Step: 3
Training loss: 2.8008019354786597
Validation loss: 2.7552494768898477

Epoch: 5| Step: 4
Training loss: 3.040656684169373
Validation loss: 2.744300388448122

Epoch: 5| Step: 5
Training loss: 3.186911883981452
Validation loss: 2.7409679280095536

Epoch: 5| Step: 6
Training loss: 2.922934600129764
Validation loss: 2.7470034571589377

Epoch: 5| Step: 7
Training loss: 3.772382948205306
Validation loss: 2.7431081007114897

Epoch: 5| Step: 8
Training loss: 3.4555202824413116
Validation loss: 2.743551352579619

Epoch: 5| Step: 9
Training loss: 3.2446044737252597
Validation loss: 2.743852927005265

Epoch: 5| Step: 10
Training loss: 3.149982991248335
Validation loss: 2.741537056180839

Epoch: 99| Step: 0
Training loss: 3.0358131905480077
Validation loss: 2.7440769494565056

Epoch: 5| Step: 1
Training loss: 3.117453045158745
Validation loss: 2.740629070429235

Epoch: 5| Step: 2
Training loss: 2.903773082915645
Validation loss: 2.73833958915009

Epoch: 5| Step: 3
Training loss: 2.7756442035370648
Validation loss: 2.7386000404971393

Epoch: 5| Step: 4
Training loss: 3.0354534774429838
Validation loss: 2.7399861010713984

Epoch: 5| Step: 5
Training loss: 3.4783393348260665
Validation loss: 2.7418462396450916

Epoch: 5| Step: 6
Training loss: 2.7811689043544874
Validation loss: 2.7386082810924166

Epoch: 5| Step: 7
Training loss: 2.829303137774806
Validation loss: 2.73833121433695

Epoch: 5| Step: 8
Training loss: 3.1302554380371004
Validation loss: 2.740665966907362

Epoch: 5| Step: 9
Training loss: 3.35337272077399
Validation loss: 2.7373318662361634

Epoch: 5| Step: 10
Training loss: 3.269224382627299
Validation loss: 2.742025577137551

Epoch: 100| Step: 0
Training loss: 3.0889429836278164
Validation loss: 2.7416675272019915

Epoch: 5| Step: 1
Training loss: 2.7977654941495556
Validation loss: 2.7424420322556444

Epoch: 5| Step: 2
Training loss: 2.9668166440565016
Validation loss: 2.7390293721785715

Epoch: 5| Step: 3
Training loss: 3.543833903508563
Validation loss: 2.746775361419372

Epoch: 5| Step: 4
Training loss: 2.4859674496288187
Validation loss: 2.7434236455967342

Epoch: 5| Step: 5
Training loss: 3.5243392433264398
Validation loss: 2.7498317864692354

Epoch: 5| Step: 6
Training loss: 3.355542360859487
Validation loss: 2.743353802595845

Epoch: 5| Step: 7
Training loss: 2.475982020366756
Validation loss: 2.7428047245834972

Epoch: 5| Step: 8
Training loss: 3.24828029030137
Validation loss: 2.738080376085565

Epoch: 5| Step: 9
Training loss: 3.4597700574023205
Validation loss: 2.733993482093619

Epoch: 5| Step: 10
Training loss: 2.3251769583024546
Validation loss: 2.7332897595739842

Epoch: 101| Step: 0
Training loss: 2.9792205876803743
Validation loss: 2.7331297155072494

Epoch: 5| Step: 1
Training loss: 3.044601922784832
Validation loss: 2.7309052506929907

Epoch: 5| Step: 2
Training loss: 2.9097128306267384
Validation loss: 2.732772185812521

Epoch: 5| Step: 3
Training loss: 3.733940833482665
Validation loss: 2.7355507099992846

Epoch: 5| Step: 4
Training loss: 3.0686723439011265
Validation loss: 2.735691395209598

Epoch: 5| Step: 5
Training loss: 2.6401686415127807
Validation loss: 2.729551978686108

Epoch: 5| Step: 6
Training loss: 3.2990481188671055
Validation loss: 2.7389494497409936

Epoch: 5| Step: 7
Training loss: 3.083156597811473
Validation loss: 2.751122233568076

Epoch: 5| Step: 8
Training loss: 2.6687237633302137
Validation loss: 2.7844809094647442

Epoch: 5| Step: 9
Training loss: 3.3295769188436077
Validation loss: 2.824720130739931

Epoch: 5| Step: 10
Training loss: 2.7314982949533855
Validation loss: 2.8003848600240477

Epoch: 102| Step: 0
Training loss: 2.728029747137947
Validation loss: 2.762390713118499

Epoch: 5| Step: 1
Training loss: 2.850673756515969
Validation loss: 2.733706658365135

Epoch: 5| Step: 2
Training loss: 3.2463778338198703
Validation loss: 2.7287602939702507

Epoch: 5| Step: 3
Training loss: 2.717793877511396
Validation loss: 2.7272828498589083

Epoch: 5| Step: 4
Training loss: 2.5466815898451367
Validation loss: 2.726877879900447

Epoch: 5| Step: 5
Training loss: 3.1820716261555733
Validation loss: 2.732408102684652

Epoch: 5| Step: 6
Training loss: 3.208779010946199
Validation loss: 2.7449346692179315

Epoch: 5| Step: 7
Training loss: 2.824096339357453
Validation loss: 2.723730229999096

Epoch: 5| Step: 8
Training loss: 3.856416502685816
Validation loss: 2.722902779590394

Epoch: 5| Step: 9
Training loss: 2.9251695306669134
Validation loss: 2.744150088667649

Epoch: 5| Step: 10
Training loss: 3.504281830832438
Validation loss: 2.781734756601442

Epoch: 103| Step: 0
Training loss: 3.102992268746002
Validation loss: 2.7721492675067774

Epoch: 5| Step: 1
Training loss: 2.4057105314253295
Validation loss: 2.7546682420247666

Epoch: 5| Step: 2
Training loss: 3.1495427541536407
Validation loss: 2.743585731697477

Epoch: 5| Step: 3
Training loss: 3.2504568512474195
Validation loss: 2.743650296968483

Epoch: 5| Step: 4
Training loss: 3.392851489643177
Validation loss: 2.7381221381009797

Epoch: 5| Step: 5
Training loss: 3.092299150067519
Validation loss: 2.7378858396916987

Epoch: 5| Step: 6
Training loss: 3.3222023286794418
Validation loss: 2.7315750867611395

Epoch: 5| Step: 7
Training loss: 2.6935705775716854
Validation loss: 2.7215522286995477

Epoch: 5| Step: 8
Training loss: 3.0486573312587133
Validation loss: 2.723935112669611

Epoch: 5| Step: 9
Training loss: 2.9436759145691713
Validation loss: 2.7198456944454303

Epoch: 5| Step: 10
Training loss: 3.1118755158383533
Validation loss: 2.7217581107918902

Epoch: 104| Step: 0
Training loss: 3.4474074034672717
Validation loss: 2.7261856457562796

Epoch: 5| Step: 1
Training loss: 3.2117197239493174
Validation loss: 2.7268762797856514

Epoch: 5| Step: 2
Training loss: 2.120052974506905
Validation loss: 2.7259089588737986

Epoch: 5| Step: 3
Training loss: 3.103288530571963
Validation loss: 2.723104246838074

Epoch: 5| Step: 4
Training loss: 2.975349399340645
Validation loss: 2.721097729052353

Epoch: 5| Step: 5
Training loss: 3.1720432180512703
Validation loss: 2.7197435482737284

Epoch: 5| Step: 6
Training loss: 3.1081273628561865
Validation loss: 2.7174811952158815

Epoch: 5| Step: 7
Training loss: 3.129517303882158
Validation loss: 2.71792933763453

Epoch: 5| Step: 8
Training loss: 2.9256055547422104
Validation loss: 2.715779557428401

Epoch: 5| Step: 9
Training loss: 3.1328937776824177
Validation loss: 2.7174795471155906

Epoch: 5| Step: 10
Training loss: 3.092656414298138
Validation loss: 2.71558850621883

Epoch: 105| Step: 0
Training loss: 3.200338500715985
Validation loss: 2.7183514450118818

Epoch: 5| Step: 1
Training loss: 3.1988314998570244
Validation loss: 2.7193062354368673

Epoch: 5| Step: 2
Training loss: 2.897214235526618
Validation loss: 2.715722643354968

Epoch: 5| Step: 3
Training loss: 2.802122765775318
Validation loss: 2.715523534303502

Epoch: 5| Step: 4
Training loss: 2.8626623382511944
Validation loss: 2.7162027813265137

Epoch: 5| Step: 5
Training loss: 2.94114848067485
Validation loss: 2.7167371484685447

Epoch: 5| Step: 6
Training loss: 2.299246050782424
Validation loss: 2.7153664971599487

Epoch: 5| Step: 7
Training loss: 3.0607681048393003
Validation loss: 2.7264861885324665

Epoch: 5| Step: 8
Training loss: 3.5081963615832255
Validation loss: 2.7166051252823

Epoch: 5| Step: 9
Training loss: 3.4484243028852313
Validation loss: 2.7292238727489804

Epoch: 5| Step: 10
Training loss: 3.1601942741454674
Validation loss: 2.7222324312827992

Epoch: 106| Step: 0
Training loss: 2.7475719136345207
Validation loss: 2.7181507902881497

Epoch: 5| Step: 1
Training loss: 3.096228945386787
Validation loss: 2.71057264139905

Epoch: 5| Step: 2
Training loss: 3.374361613615007
Validation loss: 2.7123795573849856

Epoch: 5| Step: 3
Training loss: 2.795889910046374
Validation loss: 2.711065974153423

Epoch: 5| Step: 4
Training loss: 2.6857667701167083
Validation loss: 2.710879830800285

Epoch: 5| Step: 5
Training loss: 3.1864607369616595
Validation loss: 2.7092392598199817

Epoch: 5| Step: 6
Training loss: 3.1288515388572646
Validation loss: 2.704792427188833

Epoch: 5| Step: 7
Training loss: 2.9055470262462655
Validation loss: 2.7068362589073756

Epoch: 5| Step: 8
Training loss: 3.4946174104380745
Validation loss: 2.7089338196975876

Epoch: 5| Step: 9
Training loss: 2.519161794617594
Validation loss: 2.706333653353711

Epoch: 5| Step: 10
Training loss: 3.4176301450582165
Validation loss: 2.705669593141386

Epoch: 107| Step: 0
Training loss: 2.968364489772209
Validation loss: 2.7033018212851143

Epoch: 5| Step: 1
Training loss: 3.086630596117658
Validation loss: 2.702334698467773

Epoch: 5| Step: 2
Training loss: 3.0985067770518953
Validation loss: 2.7074745337446884

Epoch: 5| Step: 3
Training loss: 2.8684746038686746
Validation loss: 2.7053731038795257

Epoch: 5| Step: 4
Training loss: 3.031078373091135
Validation loss: 2.7014216150681625

Epoch: 5| Step: 5
Training loss: 3.1641568393536246
Validation loss: 2.7075429001154587

Epoch: 5| Step: 6
Training loss: 3.3158742165631616
Validation loss: 2.7080609193269223

Epoch: 5| Step: 7
Training loss: 2.8167331627514955
Validation loss: 2.7084161416039763

Epoch: 5| Step: 8
Training loss: 2.8983988386593547
Validation loss: 2.703652355054985

Epoch: 5| Step: 9
Training loss: 3.1205362101261533
Validation loss: 2.703218315145556

Epoch: 5| Step: 10
Training loss: 3.0269518395516046
Validation loss: 2.7007931636954146

Epoch: 108| Step: 0
Training loss: 3.5322593664239115
Validation loss: 2.699086942262731

Epoch: 5| Step: 1
Training loss: 3.262758886362446
Validation loss: 2.6992573997185976

Epoch: 5| Step: 2
Training loss: 3.3447181512211754
Validation loss: 2.701203469841642

Epoch: 5| Step: 3
Training loss: 2.4292738583853457
Validation loss: 2.7032503450727656

Epoch: 5| Step: 4
Training loss: 2.7522492313749503
Validation loss: 2.698055101451407

Epoch: 5| Step: 5
Training loss: 2.853091480605573
Validation loss: 2.6990629535595123

Epoch: 5| Step: 6
Training loss: 3.474776522908383
Validation loss: 2.701450410326476

Epoch: 5| Step: 7
Training loss: 2.863900360995466
Validation loss: 2.6973967148351066

Epoch: 5| Step: 8
Training loss: 2.559881780100085
Validation loss: 2.701165306377479

Epoch: 5| Step: 9
Training loss: 2.855111868248318
Validation loss: 2.698645677863537

Epoch: 5| Step: 10
Training loss: 3.2674316081541543
Validation loss: 2.6966102456548406

Epoch: 109| Step: 0
Training loss: 2.6671719569070125
Validation loss: 2.6975562815902068

Epoch: 5| Step: 1
Training loss: 2.9180388537951822
Validation loss: 2.697746588554549

Epoch: 5| Step: 2
Training loss: 3.334756610916715
Validation loss: 2.703236546474158

Epoch: 5| Step: 3
Training loss: 3.5350425406867925
Validation loss: 2.7003316155682078

Epoch: 5| Step: 4
Training loss: 3.330112204279432
Validation loss: 2.6972723209759883

Epoch: 5| Step: 5
Training loss: 2.9934611585323103
Validation loss: 2.695506318887221

Epoch: 5| Step: 6
Training loss: 2.656434804434082
Validation loss: 2.6992862883454998

Epoch: 5| Step: 7
Training loss: 3.35067431367527
Validation loss: 2.696075739431186

Epoch: 5| Step: 8
Training loss: 2.61771653934705
Validation loss: 2.693538700938885

Epoch: 5| Step: 9
Training loss: 2.974263261883314
Validation loss: 2.695256096343514

Epoch: 5| Step: 10
Training loss: 2.733957836481784
Validation loss: 2.6945808025019233

Epoch: 110| Step: 0
Training loss: 2.794757830206097
Validation loss: 2.6923284613565377

Epoch: 5| Step: 1
Training loss: 3.051348878851573
Validation loss: 2.6910170359369325

Epoch: 5| Step: 2
Training loss: 2.7039294506665903
Validation loss: 2.6911701326414303

Epoch: 5| Step: 3
Training loss: 3.3102531730429794
Validation loss: 2.6926697498785632

Epoch: 5| Step: 4
Training loss: 3.3162834585078595
Validation loss: 2.6914398968995537

Epoch: 5| Step: 5
Training loss: 2.995543348543454
Validation loss: 2.6893858744848416

Epoch: 5| Step: 6
Training loss: 2.2856809877626896
Validation loss: 2.6920094671563346

Epoch: 5| Step: 7
Training loss: 3.0962643665139042
Validation loss: 2.6918655904384887

Epoch: 5| Step: 8
Training loss: 2.9269617854140026
Validation loss: 2.693409537324514

Epoch: 5| Step: 9
Training loss: 3.5974664462011274
Validation loss: 2.6876017862911357

Epoch: 5| Step: 10
Training loss: 2.947636585387378
Validation loss: 2.6875931432007536

Epoch: 111| Step: 0
Training loss: 3.0507878145936096
Validation loss: 2.688542339651948

Epoch: 5| Step: 1
Training loss: 2.7227295437419925
Validation loss: 2.6889903959948325

Epoch: 5| Step: 2
Training loss: 3.2144269336982854
Validation loss: 2.686354841267271

Epoch: 5| Step: 3
Training loss: 2.8176152654051254
Validation loss: 2.6872105543935483

Epoch: 5| Step: 4
Training loss: 2.282057736722038
Validation loss: 2.6830688093062376

Epoch: 5| Step: 5
Training loss: 3.000834825708353
Validation loss: 2.6876757164392977

Epoch: 5| Step: 6
Training loss: 2.8588509835111817
Validation loss: 2.6847396405235653

Epoch: 5| Step: 7
Training loss: 3.657518867657821
Validation loss: 2.6896988348397493

Epoch: 5| Step: 8
Training loss: 2.9450348336146406
Validation loss: 2.6866503596097857

Epoch: 5| Step: 9
Training loss: 3.5343057020225515
Validation loss: 2.686519324638209

Epoch: 5| Step: 10
Training loss: 2.917529950221079
Validation loss: 2.6837764986507153

Epoch: 112| Step: 0
Training loss: 2.204861004312935
Validation loss: 2.6819608563009725

Epoch: 5| Step: 1
Training loss: 2.5340464650675965
Validation loss: 2.6810691130147175

Epoch: 5| Step: 2
Training loss: 3.1955182818345094
Validation loss: 2.6816683516402793

Epoch: 5| Step: 3
Training loss: 2.9032073141691774
Validation loss: 2.680667794156994

Epoch: 5| Step: 4
Training loss: 3.2297195617531256
Validation loss: 2.680631945361407

Epoch: 5| Step: 5
Training loss: 3.354619834725638
Validation loss: 2.6844453796713075

Epoch: 5| Step: 6
Training loss: 3.497385956198047
Validation loss: 2.6808189630204713

Epoch: 5| Step: 7
Training loss: 3.2063006778154883
Validation loss: 2.690875555589735

Epoch: 5| Step: 8
Training loss: 3.0199890184518594
Validation loss: 2.6925814552633045

Epoch: 5| Step: 9
Training loss: 3.2927385025709945
Validation loss: 2.6894567500933975

Epoch: 5| Step: 10
Training loss: 2.306670703094273
Validation loss: 2.6875936592496408

Epoch: 113| Step: 0
Training loss: 3.5019144544365166
Validation loss: 2.679884257920446

Epoch: 5| Step: 1
Training loss: 2.658957145308276
Validation loss: 2.6782133075724706

Epoch: 5| Step: 2
Training loss: 2.8319339288234957
Validation loss: 2.6783971507718696

Epoch: 5| Step: 3
Training loss: 2.3862482034747403
Validation loss: 2.6808396073316723

Epoch: 5| Step: 4
Training loss: 3.4942543007029347
Validation loss: 2.6785642331989172

Epoch: 5| Step: 5
Training loss: 2.710775496949321
Validation loss: 2.67455504911334

Epoch: 5| Step: 6
Training loss: 2.8650815958117457
Validation loss: 2.671597103454595

Epoch: 5| Step: 7
Training loss: 3.2250568118746896
Validation loss: 2.6721819725450846

Epoch: 5| Step: 8
Training loss: 2.8826459304117855
Validation loss: 2.6765169393846437

Epoch: 5| Step: 9
Training loss: 3.3318622362890227
Validation loss: 2.677693914364158

Epoch: 5| Step: 10
Training loss: 3.0275357486998833
Validation loss: 2.673089017917347

Epoch: 114| Step: 0
Training loss: 3.3801962798873033
Validation loss: 2.6724078789952546

Epoch: 5| Step: 1
Training loss: 2.732594460741923
Validation loss: 2.671603583085216

Epoch: 5| Step: 2
Training loss: 2.9438988002093454
Validation loss: 2.6729303010343073

Epoch: 5| Step: 3
Training loss: 3.2356930318499475
Validation loss: 2.675672744049171

Epoch: 5| Step: 4
Training loss: 3.537230971387971
Validation loss: 2.67336359865793

Epoch: 5| Step: 5
Training loss: 2.8170991381234907
Validation loss: 2.674043795235158

Epoch: 5| Step: 6
Training loss: 2.8666920246806553
Validation loss: 2.6730832923500794

Epoch: 5| Step: 7
Training loss: 2.817583449176352
Validation loss: 2.677843993147265

Epoch: 5| Step: 8
Training loss: 2.7681909069863613
Validation loss: 2.6879167360051213

Epoch: 5| Step: 9
Training loss: 3.0004634499201353
Validation loss: 2.6917732112787713

Epoch: 5| Step: 10
Training loss: 2.816218820967362
Validation loss: 2.7036680792396988

Epoch: 115| Step: 0
Training loss: 3.2899122431683527
Validation loss: 2.7362211318530947

Epoch: 5| Step: 1
Training loss: 2.8830389331910538
Validation loss: 2.7353123637089993

Epoch: 5| Step: 2
Training loss: 3.2985747236062966
Validation loss: 2.747828851465476

Epoch: 5| Step: 3
Training loss: 3.146177782480998
Validation loss: 2.6978803042971484

Epoch: 5| Step: 4
Training loss: 2.830191684337436
Validation loss: 2.672843294694647

Epoch: 5| Step: 5
Training loss: 2.9956038371659166
Validation loss: 2.6701400027643984

Epoch: 5| Step: 6
Training loss: 3.0894853888069074
Validation loss: 2.669724548020324

Epoch: 5| Step: 7
Training loss: 2.2691393725653928
Validation loss: 2.668004776632391

Epoch: 5| Step: 8
Training loss: 2.928463611545397
Validation loss: 2.674053599975364

Epoch: 5| Step: 9
Training loss: 3.226565751265765
Validation loss: 2.668350225364921

Epoch: 5| Step: 10
Training loss: 3.1068678673136856
Validation loss: 2.665431354357092

Epoch: 116| Step: 0
Training loss: 3.603317395885792
Validation loss: 2.664137375898988

Epoch: 5| Step: 1
Training loss: 2.7127403398910785
Validation loss: 2.6641261942058185

Epoch: 5| Step: 2
Training loss: 2.54185815517152
Validation loss: 2.662299555177002

Epoch: 5| Step: 3
Training loss: 3.1662721053480194
Validation loss: 2.6635794541624147

Epoch: 5| Step: 4
Training loss: 2.8857922544831873
Validation loss: 2.670076405930331

Epoch: 5| Step: 5
Training loss: 2.954608201647748
Validation loss: 2.683073774018425

Epoch: 5| Step: 6
Training loss: 3.1900343450054662
Validation loss: 2.6956315797963373

Epoch: 5| Step: 7
Training loss: 2.9286814678931643
Validation loss: 2.718180973946843

Epoch: 5| Step: 8
Training loss: 3.1368117835284925
Validation loss: 2.727938337026756

Epoch: 5| Step: 9
Training loss: 2.956043073255182
Validation loss: 2.7139218346538074

Epoch: 5| Step: 10
Training loss: 2.86367924150345
Validation loss: 2.679850189466179

Epoch: 117| Step: 0
Training loss: 3.3604642299842173
Validation loss: 2.6745388690818843

Epoch: 5| Step: 1
Training loss: 3.066179206363697
Validation loss: 2.6623993314539427

Epoch: 5| Step: 2
Training loss: 2.982040530783223
Validation loss: 2.6601157060720104

Epoch: 5| Step: 3
Training loss: 2.4062742752237867
Validation loss: 2.6586231418449784

Epoch: 5| Step: 4
Training loss: 3.060423789504046
Validation loss: 2.661563094290566

Epoch: 5| Step: 5
Training loss: 2.4784079814649385
Validation loss: 2.660107627088065

Epoch: 5| Step: 6
Training loss: 3.0483110222713456
Validation loss: 2.6639562170676823

Epoch: 5| Step: 7
Training loss: 2.733415446172296
Validation loss: 2.663579795842617

Epoch: 5| Step: 8
Training loss: 2.8120138807863064
Validation loss: 2.6587083186422418

Epoch: 5| Step: 9
Training loss: 3.533882712593499
Validation loss: 2.6587777731158138

Epoch: 5| Step: 10
Training loss: 3.3959083002782813
Validation loss: 2.659301791161834

Epoch: 118| Step: 0
Training loss: 3.038050936745909
Validation loss: 2.6560639211876356

Epoch: 5| Step: 1
Training loss: 2.5710832083853137
Validation loss: 2.6575982876893702

Epoch: 5| Step: 2
Training loss: 3.078313366698353
Validation loss: 2.66803695833427

Epoch: 5| Step: 3
Training loss: 2.9016232321386286
Validation loss: 2.6783879371841106

Epoch: 5| Step: 4
Training loss: 2.862143421868686
Validation loss: 2.681191574688953

Epoch: 5| Step: 5
Training loss: 2.6339682014938437
Validation loss: 2.670483753541831

Epoch: 5| Step: 6
Training loss: 2.932500277337909
Validation loss: 2.6706387146491304

Epoch: 5| Step: 7
Training loss: 3.3764690451551047
Validation loss: 2.6788136635321385

Epoch: 5| Step: 8
Training loss: 3.1696875033697807
Validation loss: 2.670205943587055

Epoch: 5| Step: 9
Training loss: 2.9651361253442126
Validation loss: 2.6703897802796153

Epoch: 5| Step: 10
Training loss: 3.441442967889038
Validation loss: 2.6579571010387264

Epoch: 119| Step: 0
Training loss: 2.936520677012753
Validation loss: 2.651096788174962

Epoch: 5| Step: 1
Training loss: 3.0458854437743845
Validation loss: 2.6552580011968905

Epoch: 5| Step: 2
Training loss: 3.3177016101970653
Validation loss: 2.651730048877863

Epoch: 5| Step: 3
Training loss: 2.9288026007350285
Validation loss: 2.652669840387456

Epoch: 5| Step: 4
Training loss: 3.179224678866818
Validation loss: 2.650503346195332

Epoch: 5| Step: 5
Training loss: 2.156117642528262
Validation loss: 2.650086008689523

Epoch: 5| Step: 6
Training loss: 2.9027436136266314
Validation loss: 2.6483708187315114

Epoch: 5| Step: 7
Training loss: 3.160534509529249
Validation loss: 2.648986691706079

Epoch: 5| Step: 8
Training loss: 3.4203564483760687
Validation loss: 2.648082704912507

Epoch: 5| Step: 9
Training loss: 2.817333222321368
Validation loss: 2.652354872144685

Epoch: 5| Step: 10
Training loss: 2.8960151523804885
Validation loss: 2.6556021298999504

Epoch: 120| Step: 0
Training loss: 2.8597430216439568
Validation loss: 2.6621104920925363

Epoch: 5| Step: 1
Training loss: 2.8998958897811873
Validation loss: 2.6699847688294644

Epoch: 5| Step: 2
Training loss: 2.831064119616057
Validation loss: 2.6678805277140487

Epoch: 5| Step: 3
Training loss: 2.932176514301782
Validation loss: 2.6699771229807507

Epoch: 5| Step: 4
Training loss: 2.9063615674989847
Validation loss: 2.6610546439638987

Epoch: 5| Step: 5
Training loss: 3.0034804817532046
Validation loss: 2.6582585172038566

Epoch: 5| Step: 6
Training loss: 2.985416570123501
Validation loss: 2.6567168103671275

Epoch: 5| Step: 7
Training loss: 3.230169981271908
Validation loss: 2.6578844942190307

Epoch: 5| Step: 8
Training loss: 3.229625956369282
Validation loss: 2.649617337339732

Epoch: 5| Step: 9
Training loss: 3.019705743917144
Validation loss: 2.6481823624224483

Epoch: 5| Step: 10
Training loss: 2.936596345900281
Validation loss: 2.643162626872372

Epoch: 121| Step: 0
Training loss: 3.442324218251321
Validation loss: 2.643729455653215

Epoch: 5| Step: 1
Training loss: 3.2357138106515104
Validation loss: 2.642409908076651

Epoch: 5| Step: 2
Training loss: 2.8975470066947766
Validation loss: 2.6422969202015656

Epoch: 5| Step: 3
Training loss: 3.0335610615130566
Validation loss: 2.6439125201336746

Epoch: 5| Step: 4
Training loss: 3.2497326667582223
Validation loss: 2.6409795437781014

Epoch: 5| Step: 5
Training loss: 3.033845871462417
Validation loss: 2.64119927946091

Epoch: 5| Step: 6
Training loss: 2.7742109053126733
Validation loss: 2.641499583672321

Epoch: 5| Step: 7
Training loss: 2.7297703800818875
Validation loss: 2.6434548768637627

Epoch: 5| Step: 8
Training loss: 3.0417198977755064
Validation loss: 2.639175096594978

Epoch: 5| Step: 9
Training loss: 2.9803589966890747
Validation loss: 2.639437301642631

Epoch: 5| Step: 10
Training loss: 2.0406506663836206
Validation loss: 2.644890749497411

Epoch: 122| Step: 0
Training loss: 2.4506566938682317
Validation loss: 2.640501324895144

Epoch: 5| Step: 1
Training loss: 2.9873457098666685
Validation loss: 2.644810992999718

Epoch: 5| Step: 2
Training loss: 3.014083230998884
Validation loss: 2.6390787174734385

Epoch: 5| Step: 3
Training loss: 3.043675234148551
Validation loss: 2.643381169061188

Epoch: 5| Step: 4
Training loss: 3.146119127912451
Validation loss: 2.646661631838532

Epoch: 5| Step: 5
Training loss: 3.1267660110025877
Validation loss: 2.647389458565052

Epoch: 5| Step: 6
Training loss: 3.213244127114305
Validation loss: 2.652609017888181

Epoch: 5| Step: 7
Training loss: 2.360248530281255
Validation loss: 2.658433221819006

Epoch: 5| Step: 8
Training loss: 3.1051997230168977
Validation loss: 2.6552221347140526

Epoch: 5| Step: 9
Training loss: 3.395960393919563
Validation loss: 2.652080849011744

Epoch: 5| Step: 10
Training loss: 3.041184966383311
Validation loss: 2.6454581383118847

Epoch: 123| Step: 0
Training loss: 2.4494870174302967
Validation loss: 2.6429293454533007

Epoch: 5| Step: 1
Training loss: 3.167951440352294
Validation loss: 2.6492741885401427

Epoch: 5| Step: 2
Training loss: 2.683012564858252
Validation loss: 2.6729790868011825

Epoch: 5| Step: 3
Training loss: 3.382525495727718
Validation loss: 2.6826834877022727

Epoch: 5| Step: 4
Training loss: 2.9654654553410817
Validation loss: 2.6858004848024186

Epoch: 5| Step: 5
Training loss: 3.2591481203900643
Validation loss: 2.6449293004038816

Epoch: 5| Step: 6
Training loss: 3.3662289275066857
Validation loss: 2.6412324429409875

Epoch: 5| Step: 7
Training loss: 3.0151167525275153
Validation loss: 2.6367926805007116

Epoch: 5| Step: 8
Training loss: 3.2382965639716303
Validation loss: 2.6423201726758285

Epoch: 5| Step: 9
Training loss: 2.559588755716174
Validation loss: 2.6400142574298204

Epoch: 5| Step: 10
Training loss: 2.606579225086556
Validation loss: 2.6425222973894646

Epoch: 124| Step: 0
Training loss: 2.8106471422242354
Validation loss: 2.641384232350043

Epoch: 5| Step: 1
Training loss: 2.7931529591241446
Validation loss: 2.637602616490038

Epoch: 5| Step: 2
Training loss: 2.6565377472246086
Validation loss: 2.6344233919820526

Epoch: 5| Step: 3
Training loss: 3.1440435374887863
Validation loss: 2.6340754501315655

Epoch: 5| Step: 4
Training loss: 3.612189446534643
Validation loss: 2.6311707069617154

Epoch: 5| Step: 5
Training loss: 2.909005448321515
Validation loss: 2.6356197539790482

Epoch: 5| Step: 6
Training loss: 2.9788969402432874
Validation loss: 2.636125054890521

Epoch: 5| Step: 7
Training loss: 3.067032243527432
Validation loss: 2.6527390808926445

Epoch: 5| Step: 8
Training loss: 2.947741571804677
Validation loss: 2.669533413093005

Epoch: 5| Step: 9
Training loss: 2.908228375163218
Validation loss: 2.668703732319179

Epoch: 5| Step: 10
Training loss: 2.974579398291584
Validation loss: 2.6705676688853797

Epoch: 125| Step: 0
Training loss: 2.597513616561079
Validation loss: 2.646847651039685

Epoch: 5| Step: 1
Training loss: 2.619183317401168
Validation loss: 2.629478513347825

Epoch: 5| Step: 2
Training loss: 3.2601465147829725
Validation loss: 2.6278473704631704

Epoch: 5| Step: 3
Training loss: 3.124344718894724
Validation loss: 2.6232262774989614

Epoch: 5| Step: 4
Training loss: 2.697701630927345
Validation loss: 2.6255988128014907

Epoch: 5| Step: 5
Training loss: 3.599656931107113
Validation loss: 2.625236324799985

Epoch: 5| Step: 6
Training loss: 3.135609735942548
Validation loss: 2.6271100224102097

Epoch: 5| Step: 7
Training loss: 3.0187336607307227
Validation loss: 2.6254852039869037

Epoch: 5| Step: 8
Training loss: 2.7736855839198546
Validation loss: 2.622998679365514

Epoch: 5| Step: 9
Training loss: 2.7326338974097775
Validation loss: 2.6247077941377586

Epoch: 5| Step: 10
Training loss: 2.986042615316848
Validation loss: 2.6229622388433884

Testing loss: 2.8730613050596197

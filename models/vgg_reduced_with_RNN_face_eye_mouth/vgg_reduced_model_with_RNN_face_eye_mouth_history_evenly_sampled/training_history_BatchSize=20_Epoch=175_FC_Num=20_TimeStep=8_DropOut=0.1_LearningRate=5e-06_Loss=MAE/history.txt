Epoch: 1| Step: 0
Training loss: 4.576714515686035
Validation loss: 5.159044168328726

Epoch: 5| Step: 1
Training loss: 4.949921607971191
Validation loss: 5.15387652509956

Epoch: 5| Step: 2
Training loss: 4.921764373779297
Validation loss: 5.148583914643975

Epoch: 5| Step: 3
Training loss: 4.979642391204834
Validation loss: 5.1434987180976455

Epoch: 5| Step: 4
Training loss: 3.78039288520813
Validation loss: 5.138465414765061

Epoch: 5| Step: 5
Training loss: 3.675075054168701
Validation loss: 5.133749203015399

Epoch: 5| Step: 6
Training loss: 5.529504299163818
Validation loss: 5.128344581973169

Epoch: 5| Step: 7
Training loss: 6.28087043762207
Validation loss: 5.122920102970575

Epoch: 5| Step: 8
Training loss: 5.231149673461914
Validation loss: 5.117734929566742

Epoch: 5| Step: 9
Training loss: 4.986355781555176
Validation loss: 5.111604147059943

Epoch: 5| Step: 10
Training loss: 5.384588718414307
Validation loss: 5.104969352804204

Epoch: 2| Step: 0
Training loss: 5.5431132316589355
Validation loss: 5.098091453634282

Epoch: 5| Step: 1
Training loss: 5.037291526794434
Validation loss: 5.091162707215997

Epoch: 5| Step: 2
Training loss: 5.093266010284424
Validation loss: 5.083169332114599

Epoch: 5| Step: 3
Training loss: 5.212072849273682
Validation loss: 5.075432787659348

Epoch: 5| Step: 4
Training loss: 3.462357759475708
Validation loss: 5.066687630068872

Epoch: 5| Step: 5
Training loss: 5.015491485595703
Validation loss: 5.0589483322635775

Epoch: 5| Step: 6
Training loss: 4.282097816467285
Validation loss: 5.049491015813684

Epoch: 5| Step: 7
Training loss: 4.744194984436035
Validation loss: 5.0393592414035595

Epoch: 5| Step: 8
Training loss: 4.301148414611816
Validation loss: 5.029859604374055

Epoch: 5| Step: 9
Training loss: 5.006039142608643
Validation loss: 5.018047907019175

Epoch: 5| Step: 10
Training loss: 5.802967071533203
Validation loss: 5.007670879364014

Epoch: 3| Step: 0
Training loss: 4.027445316314697
Validation loss: 4.994916495456491

Epoch: 5| Step: 1
Training loss: 4.1351776123046875
Validation loss: 4.982567925607005

Epoch: 5| Step: 2
Training loss: 4.237282752990723
Validation loss: 4.969514767328898

Epoch: 5| Step: 3
Training loss: 4.432749271392822
Validation loss: 4.957226655816519

Epoch: 5| Step: 4
Training loss: 5.079285144805908
Validation loss: 4.943141932128578

Epoch: 5| Step: 5
Training loss: 5.691562175750732
Validation loss: 4.92923100789388

Epoch: 5| Step: 6
Training loss: 3.7626068592071533
Validation loss: 4.914217056766633

Epoch: 5| Step: 7
Training loss: 4.979434013366699
Validation loss: 4.8977976255519415

Epoch: 5| Step: 8
Training loss: 4.245698928833008
Validation loss: 4.882462373343847

Epoch: 5| Step: 9
Training loss: 6.0669636726379395
Validation loss: 4.866090369480912

Epoch: 5| Step: 10
Training loss: 5.310672760009766
Validation loss: 4.848575868914204

Epoch: 4| Step: 0
Training loss: 3.870163679122925
Validation loss: 4.831210915760328

Epoch: 5| Step: 1
Training loss: 5.361102104187012
Validation loss: 4.812693293376635

Epoch: 5| Step: 2
Training loss: 3.139714002609253
Validation loss: 4.793516082148398

Epoch: 5| Step: 3
Training loss: 4.152832508087158
Validation loss: 4.773885680783179

Epoch: 5| Step: 4
Training loss: 4.775925636291504
Validation loss: 4.754001802013766

Epoch: 5| Step: 5
Training loss: 4.555999279022217
Validation loss: 4.733534079726025

Epoch: 5| Step: 6
Training loss: 4.743634223937988
Validation loss: 4.714118916501281

Epoch: 5| Step: 7
Training loss: 4.197122097015381
Validation loss: 4.693431592756702

Epoch: 5| Step: 8
Training loss: 4.181337356567383
Validation loss: 4.6730991691671395

Epoch: 5| Step: 9
Training loss: 6.088948726654053
Validation loss: 4.65099250629384

Epoch: 5| Step: 10
Training loss: 4.659862518310547
Validation loss: 4.629781605094991

Epoch: 5| Step: 0
Training loss: 4.780570983886719
Validation loss: 4.608086339889034

Epoch: 5| Step: 1
Training loss: 3.4461159706115723
Validation loss: 4.585367361704509

Epoch: 5| Step: 2
Training loss: 5.362713813781738
Validation loss: 4.564221817960021

Epoch: 5| Step: 3
Training loss: 3.868093490600586
Validation loss: 4.540970817688973

Epoch: 5| Step: 4
Training loss: 4.579514026641846
Validation loss: 4.519613286500336

Epoch: 5| Step: 5
Training loss: 3.9150688648223877
Validation loss: 4.499850162895777

Epoch: 5| Step: 6
Training loss: 4.645320415496826
Validation loss: 4.476204333766814

Epoch: 5| Step: 7
Training loss: 4.057164192199707
Validation loss: 4.454681519539125

Epoch: 5| Step: 8
Training loss: 2.977372646331787
Validation loss: 4.433130771883072

Epoch: 5| Step: 9
Training loss: 4.586471080780029
Validation loss: 4.411951770064651

Epoch: 5| Step: 10
Training loss: 5.033699989318848
Validation loss: 4.391902277546544

Epoch: 6| Step: 0
Training loss: 4.238001346588135
Validation loss: 4.370791589060137

Epoch: 5| Step: 1
Training loss: 3.6623787879943848
Validation loss: 4.3502151658458095

Epoch: 5| Step: 2
Training loss: 4.154513359069824
Validation loss: 4.330296295945362

Epoch: 5| Step: 3
Training loss: 4.2120466232299805
Validation loss: 4.3090014944794355

Epoch: 5| Step: 4
Training loss: 3.1617941856384277
Validation loss: 4.289253762973252

Epoch: 5| Step: 5
Training loss: 4.725947380065918
Validation loss: 4.268684853789627

Epoch: 5| Step: 6
Training loss: 5.175517559051514
Validation loss: 4.249391401967695

Epoch: 5| Step: 7
Training loss: 3.960069179534912
Validation loss: 4.226908347939932

Epoch: 5| Step: 8
Training loss: 3.547473192214966
Validation loss: 4.205723608693769

Epoch: 5| Step: 9
Training loss: 3.343982696533203
Validation loss: 4.184684553453999

Epoch: 5| Step: 10
Training loss: 4.759425640106201
Validation loss: 4.163453227730208

Epoch: 7| Step: 0
Training loss: 4.349334239959717
Validation loss: 4.142857418265394

Epoch: 5| Step: 1
Training loss: 4.543066501617432
Validation loss: 4.121132004645563

Epoch: 5| Step: 2
Training loss: 3.707770586013794
Validation loss: 4.099330999517954

Epoch: 5| Step: 3
Training loss: 3.9395217895507812
Validation loss: 4.079561669339416

Epoch: 5| Step: 4
Training loss: 3.0482099056243896
Validation loss: 4.060073980721095

Epoch: 5| Step: 5
Training loss: 3.9284827709198
Validation loss: 4.039540583087552

Epoch: 5| Step: 6
Training loss: 3.468926191329956
Validation loss: 4.01953221649252

Epoch: 5| Step: 7
Training loss: 3.0400636196136475
Validation loss: 4.001970573138165

Epoch: 5| Step: 8
Training loss: 4.090577125549316
Validation loss: 3.985969461420531

Epoch: 5| Step: 9
Training loss: 4.500759601593018
Validation loss: 3.9712528797887985

Epoch: 5| Step: 10
Training loss: 4.152594566345215
Validation loss: 3.953085889098465

Epoch: 8| Step: 0
Training loss: 3.3745086193084717
Validation loss: 3.9400110603660665

Epoch: 5| Step: 1
Training loss: 4.167778968811035
Validation loss: 3.923723020861226

Epoch: 5| Step: 2
Training loss: 3.6761093139648438
Validation loss: 3.9095219386521207

Epoch: 5| Step: 3
Training loss: 3.1601955890655518
Validation loss: 3.8948647104283816

Epoch: 5| Step: 4
Training loss: 3.759021043777466
Validation loss: 3.878591775894165

Epoch: 5| Step: 5
Training loss: 4.165416717529297
Validation loss: 3.8646897551833943

Epoch: 5| Step: 6
Training loss: 3.0124752521514893
Validation loss: 3.8506292322630524

Epoch: 5| Step: 7
Training loss: 4.607685565948486
Validation loss: 3.837803109999626

Epoch: 5| Step: 8
Training loss: 3.530001163482666
Validation loss: 3.8256956018427366

Epoch: 5| Step: 9
Training loss: 3.506809711456299
Validation loss: 3.8133752730584916

Epoch: 5| Step: 10
Training loss: 4.245424270629883
Validation loss: 3.8022282969567085

Epoch: 9| Step: 0
Training loss: 3.673962116241455
Validation loss: 3.7892000700837825

Epoch: 5| Step: 1
Training loss: 3.649930953979492
Validation loss: 3.780297174248644

Epoch: 5| Step: 2
Training loss: 3.936685085296631
Validation loss: 3.768600689467563

Epoch: 5| Step: 3
Training loss: 3.877995729446411
Validation loss: 3.7578189373016357

Epoch: 5| Step: 4
Training loss: 3.517429828643799
Validation loss: 3.746525390173799

Epoch: 5| Step: 5
Training loss: 3.9566352367401123
Validation loss: 3.7364233539950464

Epoch: 5| Step: 6
Training loss: 3.968163013458252
Validation loss: 3.727530961395592

Epoch: 5| Step: 7
Training loss: 2.2347121238708496
Validation loss: 3.7180755702398156

Epoch: 5| Step: 8
Training loss: 3.346083164215088
Validation loss: 3.7080056539145847

Epoch: 5| Step: 9
Training loss: 4.076549530029297
Validation loss: 3.698254898030271

Epoch: 5| Step: 10
Training loss: 3.7730488777160645
Validation loss: 3.6896890414658414

Epoch: 10| Step: 0
Training loss: 4.297728061676025
Validation loss: 3.680377350058607

Epoch: 5| Step: 1
Training loss: 3.5793025493621826
Validation loss: 3.672129343914729

Epoch: 5| Step: 2
Training loss: 2.990081548690796
Validation loss: 3.6645219915656635

Epoch: 5| Step: 3
Training loss: 3.447019577026367
Validation loss: 3.6564338335426907

Epoch: 5| Step: 4
Training loss: 3.516464948654175
Validation loss: 3.649258541804488

Epoch: 5| Step: 5
Training loss: 4.4605536460876465
Validation loss: 3.6412826302231

Epoch: 5| Step: 6
Training loss: 2.9377942085266113
Validation loss: 3.6341440882734073

Epoch: 5| Step: 7
Training loss: 2.885392427444458
Validation loss: 3.6276753153852237

Epoch: 5| Step: 8
Training loss: 4.112229347229004
Validation loss: 3.618118360478391

Epoch: 5| Step: 9
Training loss: 3.100693702697754
Validation loss: 3.6107756091702368

Epoch: 5| Step: 10
Training loss: 3.77078914642334
Validation loss: 3.601320707669822

Epoch: 11| Step: 0
Training loss: 3.663572311401367
Validation loss: 3.5955100238964124

Epoch: 5| Step: 1
Training loss: 3.643625259399414
Validation loss: 3.5886716355559645

Epoch: 5| Step: 2
Training loss: 3.5398643016815186
Validation loss: 3.582164749022453

Epoch: 5| Step: 3
Training loss: 3.5675418376922607
Validation loss: 3.574483499732069

Epoch: 5| Step: 4
Training loss: 2.9781079292297363
Validation loss: 3.5672934439874466

Epoch: 5| Step: 5
Training loss: 3.754920482635498
Validation loss: 3.5612473128944315

Epoch: 5| Step: 6
Training loss: 4.9480085372924805
Validation loss: 3.554843351405154

Epoch: 5| Step: 7
Training loss: 3.30247163772583
Validation loss: 3.5489954153696694

Epoch: 5| Step: 8
Training loss: 3.1494638919830322
Validation loss: 3.543966595844556

Epoch: 5| Step: 9
Training loss: 3.49116587638855
Validation loss: 3.5365490964663926

Epoch: 5| Step: 10
Training loss: 2.109646797180176
Validation loss: 3.531975274444908

Epoch: 12| Step: 0
Training loss: 3.152568817138672
Validation loss: 3.526551656825568

Epoch: 5| Step: 1
Training loss: 4.21409273147583
Validation loss: 3.523234277643183

Epoch: 5| Step: 2
Training loss: 3.579815626144409
Validation loss: 3.5136594874884493

Epoch: 5| Step: 3
Training loss: 3.7122108936309814
Validation loss: 3.508999773251113

Epoch: 5| Step: 4
Training loss: 3.128967761993408
Validation loss: 3.5041532080660582

Epoch: 5| Step: 5
Training loss: 3.1817386150360107
Validation loss: 3.4985091352975495

Epoch: 5| Step: 6
Training loss: 3.8943283557891846
Validation loss: 3.4902331803434636

Epoch: 5| Step: 7
Training loss: 3.649381160736084
Validation loss: 3.4826184318911646

Epoch: 5| Step: 8
Training loss: 3.107590436935425
Validation loss: 3.476729310968871

Epoch: 5| Step: 9
Training loss: 2.5025851726531982
Validation loss: 3.469612370255173

Epoch: 5| Step: 10
Training loss: 3.6347103118896484
Validation loss: 3.4653518840830815

Epoch: 13| Step: 0
Training loss: 2.2726855278015137
Validation loss: 3.461359767503636

Epoch: 5| Step: 1
Training loss: 2.9867281913757324
Validation loss: 3.4556976082504436

Epoch: 5| Step: 2
Training loss: 3.038053035736084
Validation loss: 3.451936721801758

Epoch: 5| Step: 3
Training loss: 2.534775495529175
Validation loss: 3.4473377350837953

Epoch: 5| Step: 4
Training loss: 3.689497709274292
Validation loss: 3.4408226577184533

Epoch: 5| Step: 5
Training loss: 4.612067222595215
Validation loss: 3.438217347668063

Epoch: 5| Step: 6
Training loss: 3.3792388439178467
Validation loss: 3.431370763368504

Epoch: 5| Step: 7
Training loss: 4.278389930725098
Validation loss: 3.429154303766066

Epoch: 5| Step: 8
Training loss: 3.6897387504577637
Validation loss: 3.422338736954556

Epoch: 5| Step: 9
Training loss: 3.2677345275878906
Validation loss: 3.41915177529858

Epoch: 5| Step: 10
Training loss: 3.450350761413574
Validation loss: 3.4141353535395798

Epoch: 14| Step: 0
Training loss: 4.126550197601318
Validation loss: 3.410609632410029

Epoch: 5| Step: 1
Training loss: 2.9709250926971436
Validation loss: 3.402799585814117

Epoch: 5| Step: 2
Training loss: 2.990007162094116
Validation loss: 3.3996082300780923

Epoch: 5| Step: 3
Training loss: 3.874499559402466
Validation loss: 3.394630955111596

Epoch: 5| Step: 4
Training loss: 3.2624919414520264
Validation loss: 3.3907033884397118

Epoch: 5| Step: 5
Training loss: 2.8743247985839844
Validation loss: 3.385305304681101

Epoch: 5| Step: 6
Training loss: 3.590109348297119
Validation loss: 3.3795340830279934

Epoch: 5| Step: 7
Training loss: 2.2904160022735596
Validation loss: 3.377274090243924

Epoch: 5| Step: 8
Training loss: 3.1221909523010254
Validation loss: 3.374960781425558

Epoch: 5| Step: 9
Training loss: 4.011942386627197
Validation loss: 3.3709253752103416

Epoch: 5| Step: 10
Training loss: 3.691657304763794
Validation loss: 3.367197129034227

Epoch: 15| Step: 0
Training loss: 3.6317896842956543
Validation loss: 3.3610032886587162

Epoch: 5| Step: 1
Training loss: 2.7467780113220215
Validation loss: 3.3558501710173902

Epoch: 5| Step: 2
Training loss: 2.590745449066162
Validation loss: 3.3513434574168217

Epoch: 5| Step: 3
Training loss: 3.2178776264190674
Validation loss: 3.351172980441842

Epoch: 5| Step: 4
Training loss: 3.114112615585327
Validation loss: 3.3475043901833157

Epoch: 5| Step: 5
Training loss: 3.5386765003204346
Validation loss: 3.3441531376172136

Epoch: 5| Step: 6
Training loss: 2.6311707496643066
Validation loss: 3.340037002358385

Epoch: 5| Step: 7
Training loss: 4.267214298248291
Validation loss: 3.335932905955981

Epoch: 5| Step: 8
Training loss: 4.025485038757324
Validation loss: 3.3331213458891837

Epoch: 5| Step: 9
Training loss: 3.4258110523223877
Validation loss: 3.329315677765877

Epoch: 5| Step: 10
Training loss: 3.119354248046875
Validation loss: 3.3284463831173476

Epoch: 16| Step: 0
Training loss: 3.4722485542297363
Validation loss: 3.32217772288989

Epoch: 5| Step: 1
Training loss: 3.899634599685669
Validation loss: 3.318369347562072

Epoch: 5| Step: 2
Training loss: 2.696864604949951
Validation loss: 3.3128704178717827

Epoch: 5| Step: 3
Training loss: 2.9680914878845215
Validation loss: 3.310568058362571

Epoch: 5| Step: 4
Training loss: 3.1039340496063232
Validation loss: 3.3083400623772734

Epoch: 5| Step: 5
Training loss: 3.2602341175079346
Validation loss: 3.3072885928615445

Epoch: 5| Step: 6
Training loss: 3.581399917602539
Validation loss: 3.3026895856344574

Epoch: 5| Step: 7
Training loss: 3.5794613361358643
Validation loss: 3.300212132033481

Epoch: 5| Step: 8
Training loss: 2.4117181301116943
Validation loss: 3.294019168423068

Epoch: 5| Step: 9
Training loss: 3.8345894813537598
Validation loss: 3.292651125179824

Epoch: 5| Step: 10
Training loss: 3.210758924484253
Validation loss: 3.2879316883702434

Epoch: 17| Step: 0
Training loss: 2.4660685062408447
Validation loss: 3.284309269279562

Epoch: 5| Step: 1
Training loss: 3.4296798706054688
Validation loss: 3.283873132480088

Epoch: 5| Step: 2
Training loss: 3.2919394969940186
Validation loss: 3.28159265108006

Epoch: 5| Step: 3
Training loss: 3.948125123977661
Validation loss: 3.2793620632540796

Epoch: 5| Step: 4
Training loss: 2.8898520469665527
Validation loss: 3.2733442629537275

Epoch: 5| Step: 5
Training loss: 3.978156328201294
Validation loss: 3.2707453261139574

Epoch: 5| Step: 6
Training loss: 2.8300282955169678
Validation loss: 3.268979449425974

Epoch: 5| Step: 7
Training loss: 3.3311927318573
Validation loss: 3.2655393154390397

Epoch: 5| Step: 8
Training loss: 2.9251608848571777
Validation loss: 3.259441555187266

Epoch: 5| Step: 9
Training loss: 3.4952232837677
Validation loss: 3.261276096426031

Epoch: 5| Step: 10
Training loss: 3.145247220993042
Validation loss: 3.2567071786490818

Epoch: 18| Step: 0
Training loss: 3.7780776023864746
Validation loss: 3.257895541447465

Epoch: 5| Step: 1
Training loss: 2.61731219291687
Validation loss: 3.2526879259335097

Epoch: 5| Step: 2
Training loss: 3.52946138381958
Validation loss: 3.250652100450249

Epoch: 5| Step: 3
Training loss: 3.0934581756591797
Validation loss: 3.248974084854126

Epoch: 5| Step: 4
Training loss: 3.5190367698669434
Validation loss: 3.246679518812446

Epoch: 5| Step: 5
Training loss: 3.148897647857666
Validation loss: 3.2442220026446926

Epoch: 5| Step: 6
Training loss: 3.4645838737487793
Validation loss: 3.2408013625811507

Epoch: 5| Step: 7
Training loss: 3.169398546218872
Validation loss: 3.236245573207896

Epoch: 5| Step: 8
Training loss: 3.492889881134033
Validation loss: 3.235516712229739

Epoch: 5| Step: 9
Training loss: 2.979717254638672
Validation loss: 3.230999861994097

Epoch: 5| Step: 10
Training loss: 2.6486947536468506
Validation loss: 3.2270373708458355

Epoch: 19| Step: 0
Training loss: 3.241809129714966
Validation loss: 3.23275766834136

Epoch: 5| Step: 1
Training loss: 3.4570019245147705
Validation loss: 3.2298370279291624

Epoch: 5| Step: 2
Training loss: 2.346726894378662
Validation loss: 3.225581448565247

Epoch: 5| Step: 3
Training loss: 2.9131762981414795
Validation loss: 3.215928393025552

Epoch: 5| Step: 4
Training loss: 2.496608257293701
Validation loss: 3.212275248701854

Epoch: 5| Step: 5
Training loss: 4.1943230628967285
Validation loss: 3.2125005440045427

Epoch: 5| Step: 6
Training loss: 3.219876766204834
Validation loss: 3.2105966485956663

Epoch: 5| Step: 7
Training loss: 3.314038038253784
Validation loss: 3.2046201562368744

Epoch: 5| Step: 8
Training loss: 3.454792022705078
Validation loss: 3.2056504808446413

Epoch: 5| Step: 9
Training loss: 2.672018051147461
Validation loss: 3.2008299314847557

Epoch: 5| Step: 10
Training loss: 4.105077266693115
Validation loss: 3.199233857534265

Epoch: 20| Step: 0
Training loss: 2.9723620414733887
Validation loss: 3.1986299945462133

Epoch: 5| Step: 1
Training loss: 3.947096347808838
Validation loss: 3.191552964589929

Epoch: 5| Step: 2
Training loss: 3.5198287963867188
Validation loss: 3.1875366805702128

Epoch: 5| Step: 3
Training loss: 3.094635009765625
Validation loss: 3.1878864790803645

Epoch: 5| Step: 4
Training loss: 2.615239143371582
Validation loss: 3.1816640489844867

Epoch: 5| Step: 5
Training loss: 3.1530404090881348
Validation loss: 3.179466724395752

Epoch: 5| Step: 6
Training loss: 3.2936439514160156
Validation loss: 3.1789367404035342

Epoch: 5| Step: 7
Training loss: 2.8712000846862793
Validation loss: 3.175509445128902

Epoch: 5| Step: 8
Training loss: 3.7473461627960205
Validation loss: 3.1739550790479107

Epoch: 5| Step: 9
Training loss: 2.703639268875122
Validation loss: 3.168813261934506

Epoch: 5| Step: 10
Training loss: 3.115204334259033
Validation loss: 3.166667958741547

Epoch: 21| Step: 0
Training loss: 2.971665859222412
Validation loss: 3.165254851823212

Epoch: 5| Step: 1
Training loss: 3.2319958209991455
Validation loss: 3.1620399875025593

Epoch: 5| Step: 2
Training loss: 2.527695894241333
Validation loss: 3.1608343919118247

Epoch: 5| Step: 3
Training loss: 3.832202911376953
Validation loss: 3.153945722887593

Epoch: 5| Step: 4
Training loss: 3.3287529945373535
Validation loss: 3.1553724863195933

Epoch: 5| Step: 5
Training loss: 3.475878953933716
Validation loss: 3.1575194379334808

Epoch: 5| Step: 6
Training loss: 2.323438882827759
Validation loss: 3.149730308081514

Epoch: 5| Step: 7
Training loss: 3.5138823986053467
Validation loss: 3.1492306878489833

Epoch: 5| Step: 8
Training loss: 2.2853550910949707
Validation loss: 3.1489587342867287

Epoch: 5| Step: 9
Training loss: 3.4749503135681152
Validation loss: 3.1447892573571976

Epoch: 5| Step: 10
Training loss: 3.993349313735962
Validation loss: 3.1453420936420398

Epoch: 22| Step: 0
Training loss: 3.678819179534912
Validation loss: 3.143649152530137

Epoch: 5| Step: 1
Training loss: 3.618861436843872
Validation loss: 3.1446902546831357

Epoch: 5| Step: 2
Training loss: 2.7140471935272217
Validation loss: 3.1404705098880235

Epoch: 5| Step: 3
Training loss: 3.7001945972442627
Validation loss: 3.137831605890746

Epoch: 5| Step: 4
Training loss: 3.4370028972625732
Validation loss: 3.1321027483991397

Epoch: 5| Step: 5
Training loss: 2.813429355621338
Validation loss: 3.129929680978098

Epoch: 5| Step: 6
Training loss: 2.605498790740967
Validation loss: 3.12527730131662

Epoch: 5| Step: 7
Training loss: 2.2560763359069824
Validation loss: 3.12664371408442

Epoch: 5| Step: 8
Training loss: 2.7077553272247314
Validation loss: 3.1330438839491976

Epoch: 5| Step: 9
Training loss: 3.901831865310669
Validation loss: 3.119072280904298

Epoch: 5| Step: 10
Training loss: 3.2298738956451416
Validation loss: 3.1220110872740388

Epoch: 23| Step: 0
Training loss: 3.2550156116485596
Validation loss: 3.136234468029391

Epoch: 5| Step: 1
Training loss: 3.494145154953003
Validation loss: 3.138421904656195

Epoch: 5| Step: 2
Training loss: 2.5940215587615967
Validation loss: 3.1320662139564432

Epoch: 5| Step: 3
Training loss: 4.5272674560546875
Validation loss: 3.116718405036516

Epoch: 5| Step: 4
Training loss: 3.0676465034484863
Validation loss: 3.107831019227223

Epoch: 5| Step: 5
Training loss: 2.800433397293091
Validation loss: 3.107805457166446

Epoch: 5| Step: 6
Training loss: 2.6233081817626953
Validation loss: 3.1172013231503066

Epoch: 5| Step: 7
Training loss: 2.1267447471618652
Validation loss: 3.123230564978815

Epoch: 5| Step: 8
Training loss: 2.8939857482910156
Validation loss: 3.1096663987764748

Epoch: 5| Step: 9
Training loss: 3.544149875640869
Validation loss: 3.099407357554282

Epoch: 5| Step: 10
Training loss: 3.6986098289489746
Validation loss: 3.098547838067496

Epoch: 24| Step: 0
Training loss: 3.340249538421631
Validation loss: 3.0974650408632014

Epoch: 5| Step: 1
Training loss: 3.7200450897216797
Validation loss: 3.0990067963959067

Epoch: 5| Step: 2
Training loss: 3.7800192832946777
Validation loss: 3.1048201130282496

Epoch: 5| Step: 3
Training loss: 2.7401394844055176
Validation loss: 3.1007629825222875

Epoch: 5| Step: 4
Training loss: 3.163822889328003
Validation loss: 3.094378512392762

Epoch: 5| Step: 5
Training loss: 2.332223415374756
Validation loss: 3.0907709931814544

Epoch: 5| Step: 6
Training loss: 2.9314961433410645
Validation loss: 3.0883109467003935

Epoch: 5| Step: 7
Training loss: 3.4805006980895996
Validation loss: 3.078098661156111

Epoch: 5| Step: 8
Training loss: 2.811933994293213
Validation loss: 3.073635983210738

Epoch: 5| Step: 9
Training loss: 3.63663911819458
Validation loss: 3.0718507202722694

Epoch: 5| Step: 10
Training loss: 2.228457450866699
Validation loss: 3.0683253862524547

Epoch: 25| Step: 0
Training loss: 2.237049102783203
Validation loss: 3.0671407894421647

Epoch: 5| Step: 1
Training loss: 3.571270704269409
Validation loss: 3.0677935410571355

Epoch: 5| Step: 2
Training loss: 2.277820587158203
Validation loss: 3.0632831973414265

Epoch: 5| Step: 3
Training loss: 3.0842196941375732
Validation loss: 3.0644136910797446

Epoch: 5| Step: 4
Training loss: 3.1962714195251465
Validation loss: 3.0614468820633425

Epoch: 5| Step: 5
Training loss: 2.287238597869873
Validation loss: 3.064416362393287

Epoch: 5| Step: 6
Training loss: 3.4826979637145996
Validation loss: 3.0632427097648702

Epoch: 5| Step: 7
Training loss: 3.722405195236206
Validation loss: 3.0569830479160434

Epoch: 5| Step: 8
Training loss: 3.1968119144439697
Validation loss: 3.0521185628829466

Epoch: 5| Step: 9
Training loss: 3.0778467655181885
Validation loss: 3.048539400100708

Epoch: 5| Step: 10
Training loss: 4.117246150970459
Validation loss: 3.0442143486392115

Epoch: 26| Step: 0
Training loss: 2.6969552040100098
Validation loss: 3.0439883124443794

Epoch: 5| Step: 1
Training loss: 3.899224042892456
Validation loss: 3.0401765864382506

Epoch: 5| Step: 2
Training loss: 3.317580461502075
Validation loss: 3.0404150614174466

Epoch: 5| Step: 3
Training loss: 3.2006053924560547
Validation loss: 3.0402117057513167

Epoch: 5| Step: 4
Training loss: 3.0719985961914062
Validation loss: 3.035092676839521

Epoch: 5| Step: 5
Training loss: 2.809194803237915
Validation loss: 3.035115336859098

Epoch: 5| Step: 6
Training loss: 3.9682586193084717
Validation loss: 3.0335583481737363

Epoch: 5| Step: 7
Training loss: 2.1263396739959717
Validation loss: 3.0275882674801733

Epoch: 5| Step: 8
Training loss: 2.9762415885925293
Validation loss: 3.0262124000057096

Epoch: 5| Step: 9
Training loss: 2.210155963897705
Validation loss: 3.0246560676123506

Epoch: 5| Step: 10
Training loss: 3.76143479347229
Validation loss: 3.023287357822541

Epoch: 27| Step: 0
Training loss: 3.8913612365722656
Validation loss: 3.022441787104453

Epoch: 5| Step: 1
Training loss: 2.8063182830810547
Validation loss: 3.0165444958594536

Epoch: 5| Step: 2
Training loss: 2.478013515472412
Validation loss: 3.0119882963036977

Epoch: 5| Step: 3
Training loss: 3.503754138946533
Validation loss: 3.0120117382336686

Epoch: 5| Step: 4
Training loss: 3.065561294555664
Validation loss: 3.008864179734261

Epoch: 5| Step: 5
Training loss: 3.347548246383667
Validation loss: 3.0046165194562686

Epoch: 5| Step: 6
Training loss: 2.3862266540527344
Validation loss: 3.002682790961317

Epoch: 5| Step: 7
Training loss: 3.8150413036346436
Validation loss: 3.0026510197629213

Epoch: 5| Step: 8
Training loss: 2.7111611366271973
Validation loss: 3.0001121695323656

Epoch: 5| Step: 9
Training loss: 3.1769092082977295
Validation loss: 2.998162382392473

Epoch: 5| Step: 10
Training loss: 2.4561290740966797
Validation loss: 2.999881254729404

Epoch: 28| Step: 0
Training loss: 3.1270980834960938
Validation loss: 2.9934340830772155

Epoch: 5| Step: 1
Training loss: 3.0817618370056152
Validation loss: 2.9997992284836306

Epoch: 5| Step: 2
Training loss: 2.704732894897461
Validation loss: 2.9868236049529044

Epoch: 5| Step: 3
Training loss: 2.727804660797119
Validation loss: 2.9848324355258735

Epoch: 5| Step: 4
Training loss: 3.591953992843628
Validation loss: 2.9813610507595922

Epoch: 5| Step: 5
Training loss: 3.5419018268585205
Validation loss: 2.9809061070924163

Epoch: 5| Step: 6
Training loss: 2.806082248687744
Validation loss: 2.978313112771639

Epoch: 5| Step: 7
Training loss: 2.6235203742980957
Validation loss: 2.977508329576062

Epoch: 5| Step: 8
Training loss: 2.861571788787842
Validation loss: 2.9716787517711682

Epoch: 5| Step: 9
Training loss: 3.2067768573760986
Validation loss: 2.9745151253156763

Epoch: 5| Step: 10
Training loss: 3.359729528427124
Validation loss: 2.973123724742602

Epoch: 29| Step: 0
Training loss: 3.7300095558166504
Validation loss: 2.976997060160483

Epoch: 5| Step: 1
Training loss: 2.2857413291931152
Validation loss: 2.9792618854071504

Epoch: 5| Step: 2
Training loss: 2.83023738861084
Validation loss: 2.9693375249062814

Epoch: 5| Step: 3
Training loss: 2.234168767929077
Validation loss: 2.9607745421830045

Epoch: 5| Step: 4
Training loss: 2.321610927581787
Validation loss: 2.959953410651094

Epoch: 5| Step: 5
Training loss: 2.480642795562744
Validation loss: 2.959933614218107

Epoch: 5| Step: 6
Training loss: 3.3689703941345215
Validation loss: 2.9678521181947444

Epoch: 5| Step: 7
Training loss: 3.499798536300659
Validation loss: 2.956104891274565

Epoch: 5| Step: 8
Training loss: 3.3868706226348877
Validation loss: 2.9561596185930314

Epoch: 5| Step: 9
Training loss: 3.7172508239746094
Validation loss: 2.9627179484213553

Epoch: 5| Step: 10
Training loss: 3.6873369216918945
Validation loss: 2.9647651974872877

Epoch: 30| Step: 0
Training loss: 3.277853488922119
Validation loss: 2.956828958244734

Epoch: 5| Step: 1
Training loss: 2.3685405254364014
Validation loss: 2.956775216646092

Epoch: 5| Step: 2
Training loss: 3.3108627796173096
Validation loss: 2.953922776765721

Epoch: 5| Step: 3
Training loss: 2.8171730041503906
Validation loss: 2.9499048263795915

Epoch: 5| Step: 4
Training loss: 3.2873618602752686
Validation loss: 2.954804725544427

Epoch: 5| Step: 5
Training loss: 2.8665287494659424
Validation loss: 2.954473292955788

Epoch: 5| Step: 6
Training loss: 2.90771746635437
Validation loss: 2.9417627319212882

Epoch: 5| Step: 7
Training loss: 2.6797587871551514
Validation loss: 2.9465950355734876

Epoch: 5| Step: 8
Training loss: 3.616790771484375
Validation loss: 2.9396544810264342

Epoch: 5| Step: 9
Training loss: 3.120971918106079
Validation loss: 2.938938440815095

Epoch: 5| Step: 10
Training loss: 3.066784620285034
Validation loss: 2.9394727727418304

Epoch: 31| Step: 0
Training loss: 2.9882709980010986
Validation loss: 2.936432789730769

Epoch: 5| Step: 1
Training loss: 3.1939265727996826
Validation loss: 2.933468703300722

Epoch: 5| Step: 2
Training loss: 3.533526659011841
Validation loss: 2.928763025550432

Epoch: 5| Step: 3
Training loss: 3.006103277206421
Validation loss: 2.9281905184509935

Epoch: 5| Step: 4
Training loss: 2.371872901916504
Validation loss: 2.934715450450938

Epoch: 5| Step: 5
Training loss: 3.1190085411071777
Validation loss: 2.9473765947485484

Epoch: 5| Step: 6
Training loss: 2.557799816131592
Validation loss: 2.969752739834529

Epoch: 5| Step: 7
Training loss: 3.104729652404785
Validation loss: 2.9407777683709257

Epoch: 5| Step: 8
Training loss: 3.167762517929077
Validation loss: 2.9189508576546945

Epoch: 5| Step: 9
Training loss: 3.4708714485168457
Validation loss: 2.92711309207383

Epoch: 5| Step: 10
Training loss: 2.620438814163208
Validation loss: 2.935283668579594

Epoch: 32| Step: 0
Training loss: 3.862816333770752
Validation loss: 2.950975530891008

Epoch: 5| Step: 1
Training loss: 3.054708957672119
Validation loss: 2.9549457591067076

Epoch: 5| Step: 2
Training loss: 1.927445650100708
Validation loss: 2.942576600659278

Epoch: 5| Step: 3
Training loss: 3.1762948036193848
Validation loss: 2.9406090244170158

Epoch: 5| Step: 4
Training loss: 3.0983147621154785
Validation loss: 2.9355887136151715

Epoch: 5| Step: 5
Training loss: 2.9478542804718018
Validation loss: 2.9258239551257064

Epoch: 5| Step: 6
Training loss: 3.6144912242889404
Validation loss: 2.9172249147968907

Epoch: 5| Step: 7
Training loss: 2.860722064971924
Validation loss: 2.9174208820507093

Epoch: 5| Step: 8
Training loss: 2.615805149078369
Validation loss: 2.9300404594790552

Epoch: 5| Step: 9
Training loss: 3.0903775691986084
Validation loss: 2.9397234121958413

Epoch: 5| Step: 10
Training loss: 2.920588493347168
Validation loss: 2.9359114631529777

Epoch: 33| Step: 0
Training loss: 3.193225860595703
Validation loss: 2.9426475981230378

Epoch: 5| Step: 1
Training loss: 2.8918068408966064
Validation loss: 2.937775135040283

Epoch: 5| Step: 2
Training loss: 3.2643179893493652
Validation loss: 2.930685830372636

Epoch: 5| Step: 3
Training loss: 2.947831153869629
Validation loss: 2.92427243468582

Epoch: 5| Step: 4
Training loss: 3.2176413536071777
Validation loss: 2.921306694707563

Epoch: 5| Step: 5
Training loss: 3.185034990310669
Validation loss: 2.9051161837834183

Epoch: 5| Step: 6
Training loss: 2.971588373184204
Validation loss: 2.8938216496539373

Epoch: 5| Step: 7
Training loss: 3.0655441284179688
Validation loss: 2.899872628591394

Epoch: 5| Step: 8
Training loss: 2.32120418548584
Validation loss: 2.9046349909997757

Epoch: 5| Step: 9
Training loss: 3.087674140930176
Validation loss: 2.9058750316660893

Epoch: 5| Step: 10
Training loss: 2.9324228763580322
Validation loss: 2.909795343234975

Epoch: 34| Step: 0
Training loss: 2.536959171295166
Validation loss: 2.9135336260641775

Epoch: 5| Step: 1
Training loss: 2.7987961769104004
Validation loss: 2.916166728542697

Epoch: 5| Step: 2
Training loss: 3.683631181716919
Validation loss: 2.9027618054420716

Epoch: 5| Step: 3
Training loss: 3.004366397857666
Validation loss: 2.9013624883467153

Epoch: 5| Step: 4
Training loss: 2.6884093284606934
Validation loss: 2.8945941591775544

Epoch: 5| Step: 5
Training loss: 2.7488553524017334
Validation loss: 2.885295114209575

Epoch: 5| Step: 6
Training loss: 2.745197057723999
Validation loss: 2.8842566884973997

Epoch: 5| Step: 7
Training loss: 3.6125686168670654
Validation loss: 2.886950885095904

Epoch: 5| Step: 8
Training loss: 2.7938876152038574
Validation loss: 2.888051191965739

Epoch: 5| Step: 9
Training loss: 3.1678833961486816
Validation loss: 2.886738036268501

Epoch: 5| Step: 10
Training loss: 3.101567029953003
Validation loss: 2.884423960921585

Epoch: 35| Step: 0
Training loss: 2.774984121322632
Validation loss: 2.8875424400452645

Epoch: 5| Step: 1
Training loss: 1.980881929397583
Validation loss: 2.8829414562512468

Epoch: 5| Step: 2
Training loss: 3.89697265625
Validation loss: 2.8839457547792824

Epoch: 5| Step: 3
Training loss: 2.662362575531006
Validation loss: 2.888300562417635

Epoch: 5| Step: 4
Training loss: 2.1540191173553467
Validation loss: 2.8844184157668904

Epoch: 5| Step: 5
Training loss: 3.880690097808838
Validation loss: 2.8850265625984437

Epoch: 5| Step: 6
Training loss: 3.1276650428771973
Validation loss: 2.8818847466540594

Epoch: 5| Step: 7
Training loss: 3.2213668823242188
Validation loss: 2.8796664412303636

Epoch: 5| Step: 8
Training loss: 3.2538230419158936
Validation loss: 2.8752874917881464

Epoch: 5| Step: 9
Training loss: 3.0747745037078857
Validation loss: 2.8709546263499925

Epoch: 5| Step: 10
Training loss: 2.717451572418213
Validation loss: 2.8693258095813055

Epoch: 36| Step: 0
Training loss: 2.953589916229248
Validation loss: 2.870294693977602

Epoch: 5| Step: 1
Training loss: 2.4930412769317627
Validation loss: 2.8669176409321446

Epoch: 5| Step: 2
Training loss: 2.7468204498291016
Validation loss: 2.8696735084697766

Epoch: 5| Step: 3
Training loss: 2.9722189903259277
Validation loss: 2.8696644767638175

Epoch: 5| Step: 4
Training loss: 3.592113494873047
Validation loss: 2.871147258307344

Epoch: 5| Step: 5
Training loss: 3.526435375213623
Validation loss: 2.872800196370771

Epoch: 5| Step: 6
Training loss: 3.0990395545959473
Validation loss: 2.865928401229202

Epoch: 5| Step: 7
Training loss: 2.9444589614868164
Validation loss: 2.864696805195142

Epoch: 5| Step: 8
Training loss: 2.899003028869629
Validation loss: 2.8619988528631066

Epoch: 5| Step: 9
Training loss: 3.3836607933044434
Validation loss: 2.8579254560573126

Epoch: 5| Step: 10
Training loss: 1.9169477224349976
Validation loss: 2.857649008433024

Epoch: 37| Step: 0
Training loss: 3.0852231979370117
Validation loss: 2.8582136246465866

Epoch: 5| Step: 1
Training loss: 2.5415701866149902
Validation loss: 2.859234271510955

Epoch: 5| Step: 2
Training loss: 2.662937879562378
Validation loss: 2.854761254402899

Epoch: 5| Step: 3
Training loss: 3.1347403526306152
Validation loss: 2.8582233818628455

Epoch: 5| Step: 4
Training loss: 3.391721725463867
Validation loss: 2.8534512160926737

Epoch: 5| Step: 5
Training loss: 3.2664496898651123
Validation loss: 2.855236481594783

Epoch: 5| Step: 6
Training loss: 2.7795376777648926
Validation loss: 2.853051006153066

Epoch: 5| Step: 7
Training loss: 3.5280280113220215
Validation loss: 2.8514441469664216

Epoch: 5| Step: 8
Training loss: 3.1230099201202393
Validation loss: 2.8493475196182088

Epoch: 5| Step: 9
Training loss: 2.5691540241241455
Validation loss: 2.8469041009103098

Epoch: 5| Step: 10
Training loss: 2.4619638919830322
Validation loss: 2.8463979357032367

Epoch: 38| Step: 0
Training loss: 3.3095765113830566
Validation loss: 2.848899056834559

Epoch: 5| Step: 1
Training loss: 3.1891379356384277
Validation loss: 2.8454764145676807

Epoch: 5| Step: 2
Training loss: 3.5255126953125
Validation loss: 2.8464321808148454

Epoch: 5| Step: 3
Training loss: 2.679084062576294
Validation loss: 2.844595452790619

Epoch: 5| Step: 4
Training loss: 2.459169864654541
Validation loss: 2.8430955512549287

Epoch: 5| Step: 5
Training loss: 3.269254207611084
Validation loss: 2.8443705087066977

Epoch: 5| Step: 6
Training loss: 2.816659450531006
Validation loss: 2.8451442462141796

Epoch: 5| Step: 7
Training loss: 2.613129138946533
Validation loss: 2.8366416397915093

Epoch: 5| Step: 8
Training loss: 2.8664650917053223
Validation loss: 2.839599742684313

Epoch: 5| Step: 9
Training loss: 3.0592808723449707
Validation loss: 2.841365296353576

Epoch: 5| Step: 10
Training loss: 2.693342685699463
Validation loss: 2.8477333156011437

Epoch: 39| Step: 0
Training loss: 3.2135307788848877
Validation loss: 2.8441359432794715

Epoch: 5| Step: 1
Training loss: 2.801332950592041
Validation loss: 2.8426488317469114

Epoch: 5| Step: 2
Training loss: 2.643383502960205
Validation loss: 2.8389207214437504

Epoch: 5| Step: 3
Training loss: 3.4876270294189453
Validation loss: 2.8358941001276814

Epoch: 5| Step: 4
Training loss: 2.9324848651885986
Validation loss: 2.8319980072718796

Epoch: 5| Step: 5
Training loss: 2.6861565113067627
Validation loss: 2.831764821083315

Epoch: 5| Step: 6
Training loss: 2.700829267501831
Validation loss: 2.832271191381639

Epoch: 5| Step: 7
Training loss: 3.2626724243164062
Validation loss: 2.8317291300783873

Epoch: 5| Step: 8
Training loss: 3.3111557960510254
Validation loss: 2.8330070562260126

Epoch: 5| Step: 9
Training loss: 2.652315139770508
Validation loss: 2.828902931623561

Epoch: 5| Step: 10
Training loss: 2.7624118328094482
Validation loss: 2.828705854313348

Epoch: 40| Step: 0
Training loss: 3.114288806915283
Validation loss: 2.8256302905339066

Epoch: 5| Step: 1
Training loss: 3.243427276611328
Validation loss: 2.8263143057464273

Epoch: 5| Step: 2
Training loss: 3.3079171180725098
Validation loss: 2.8243428020067114

Epoch: 5| Step: 3
Training loss: 2.506213426589966
Validation loss: 2.8211740934720604

Epoch: 5| Step: 4
Training loss: 3.3574843406677246
Validation loss: 2.8176662460450204

Epoch: 5| Step: 5
Training loss: 3.0741500854492188
Validation loss: 2.8147062922036774

Epoch: 5| Step: 6
Training loss: 2.87121319770813
Validation loss: 2.813705100808092

Epoch: 5| Step: 7
Training loss: 2.8043527603149414
Validation loss: 2.808756427098346

Epoch: 5| Step: 8
Training loss: 2.5752785205841064
Validation loss: 2.8081536703212286

Epoch: 5| Step: 9
Training loss: 2.9061970710754395
Validation loss: 2.8120428028927056

Epoch: 5| Step: 10
Training loss: 2.536013603210449
Validation loss: 2.8180004114745767

Epoch: 41| Step: 0
Training loss: 3.123835563659668
Validation loss: 2.8072763848048385

Epoch: 5| Step: 1
Training loss: 2.369141101837158
Validation loss: 2.8038887490508375

Epoch: 5| Step: 2
Training loss: 3.1351306438446045
Validation loss: 2.806486737343573

Epoch: 5| Step: 3
Training loss: 2.9121391773223877
Validation loss: 2.804373307894635

Epoch: 5| Step: 4
Training loss: 3.0919029712677
Validation loss: 2.8064325829987884

Epoch: 5| Step: 5
Training loss: 3.3564155101776123
Validation loss: 2.8051791396192325

Epoch: 5| Step: 6
Training loss: 2.74531888961792
Validation loss: 2.8059471653353785

Epoch: 5| Step: 7
Training loss: 3.3309357166290283
Validation loss: 2.7994874626077633

Epoch: 5| Step: 8
Training loss: 2.433932304382324
Validation loss: 2.7942785729644117

Epoch: 5| Step: 9
Training loss: 2.5203001499176025
Validation loss: 2.7975146411567606

Epoch: 5| Step: 10
Training loss: 3.3052823543548584
Validation loss: 2.8004844034871748

Epoch: 42| Step: 0
Training loss: 3.1479456424713135
Validation loss: 2.799048000766385

Epoch: 5| Step: 1
Training loss: 2.662471055984497
Validation loss: 2.799564907627721

Epoch: 5| Step: 2
Training loss: 2.335313081741333
Validation loss: 2.7974164357749363

Epoch: 5| Step: 3
Training loss: 2.463186740875244
Validation loss: 2.7973140003860637

Epoch: 5| Step: 4
Training loss: 3.445138931274414
Validation loss: 2.7926734801261657

Epoch: 5| Step: 5
Training loss: 3.1353390216827393
Validation loss: 2.784320572371124

Epoch: 5| Step: 6
Training loss: 2.5612170696258545
Validation loss: 2.783611056625202

Epoch: 5| Step: 7
Training loss: 2.8147802352905273
Validation loss: 2.783431245434669

Epoch: 5| Step: 8
Training loss: 3.4978744983673096
Validation loss: 2.7970483533797728

Epoch: 5| Step: 9
Training loss: 3.491692066192627
Validation loss: 2.7891507789652836

Epoch: 5| Step: 10
Training loss: 2.574289083480835
Validation loss: 2.7824607664538967

Epoch: 43| Step: 0
Training loss: 3.05519437789917
Validation loss: 2.7762920677020984

Epoch: 5| Step: 1
Training loss: 3.1085124015808105
Validation loss: 2.772668115554317

Epoch: 5| Step: 2
Training loss: 2.942574977874756
Validation loss: 2.7706882543461298

Epoch: 5| Step: 3
Training loss: 2.595055103302002
Validation loss: 2.7727099746786137

Epoch: 5| Step: 4
Training loss: 2.8059911727905273
Validation loss: 2.7682809060619724

Epoch: 5| Step: 5
Training loss: 2.8834311962127686
Validation loss: 2.764112928862213

Epoch: 5| Step: 6
Training loss: 2.63513445854187
Validation loss: 2.765790085638723

Epoch: 5| Step: 7
Training loss: 3.4062576293945312
Validation loss: 2.760787820303312

Epoch: 5| Step: 8
Training loss: 3.0038092136383057
Validation loss: 2.7573570897502284

Epoch: 5| Step: 9
Training loss: 2.6812102794647217
Validation loss: 2.7583065725141958

Epoch: 5| Step: 10
Training loss: 2.8528060913085938
Validation loss: 2.7558188028233026

Epoch: 44| Step: 0
Training loss: 2.9586472511291504
Validation loss: 2.761919957335277

Epoch: 5| Step: 1
Training loss: 3.1029212474823
Validation loss: 2.756210422003141

Epoch: 5| Step: 2
Training loss: 2.884337902069092
Validation loss: 2.744854365625689

Epoch: 5| Step: 3
Training loss: 3.1643385887145996
Validation loss: 2.7453336843880276

Epoch: 5| Step: 4
Training loss: 2.126687526702881
Validation loss: 2.743056907448717

Epoch: 5| Step: 5
Training loss: 2.9341373443603516
Validation loss: 2.746847524437853

Epoch: 5| Step: 6
Training loss: 3.1363110542297363
Validation loss: 2.7501792933351252

Epoch: 5| Step: 7
Training loss: 2.77453875541687
Validation loss: 2.7532426131668912

Epoch: 5| Step: 8
Training loss: 2.450422525405884
Validation loss: 2.753678919166647

Epoch: 5| Step: 9
Training loss: 3.4996399879455566
Validation loss: 2.7515307088052072

Epoch: 5| Step: 10
Training loss: 2.82603120803833
Validation loss: 2.7482941842848256

Epoch: 45| Step: 0
Training loss: 2.9210352897644043
Validation loss: 2.7412059973644953

Epoch: 5| Step: 1
Training loss: 3.35858154296875
Validation loss: 2.7390227061445995

Epoch: 5| Step: 2
Training loss: 3.2416725158691406
Validation loss: 2.736049472644765

Epoch: 5| Step: 3
Training loss: 2.533949375152588
Validation loss: 2.733213581064696

Epoch: 5| Step: 4
Training loss: 2.502124309539795
Validation loss: 2.730279517430131

Epoch: 5| Step: 5
Training loss: 3.752741575241089
Validation loss: 2.724469333566645

Epoch: 5| Step: 6
Training loss: 3.0036966800689697
Validation loss: 2.7239239408123876

Epoch: 5| Step: 7
Training loss: 2.759596824645996
Validation loss: 2.724575293961392

Epoch: 5| Step: 8
Training loss: 2.581085681915283
Validation loss: 2.719769231734737

Epoch: 5| Step: 9
Training loss: 2.857255697250366
Validation loss: 2.7209619834858882

Epoch: 5| Step: 10
Training loss: 2.118201971054077
Validation loss: 2.7205353911205004

Epoch: 46| Step: 0
Training loss: 2.811704635620117
Validation loss: 2.7167705464106735

Epoch: 5| Step: 1
Training loss: 2.102363109588623
Validation loss: 2.7197961961069415

Epoch: 5| Step: 2
Training loss: 2.5810816287994385
Validation loss: 2.7216628879629154

Epoch: 5| Step: 3
Training loss: 3.110931396484375
Validation loss: 2.7290297631294496

Epoch: 5| Step: 4
Training loss: 3.6725287437438965
Validation loss: 2.7321820412912676

Epoch: 5| Step: 5
Training loss: 3.2363719940185547
Validation loss: 2.7225544657758487

Epoch: 5| Step: 6
Training loss: 2.4822447299957275
Validation loss: 2.718355119869273

Epoch: 5| Step: 7
Training loss: 2.735821485519409
Validation loss: 2.7126923607241724

Epoch: 5| Step: 8
Training loss: 3.228317975997925
Validation loss: 2.7128409877900155

Epoch: 5| Step: 9
Training loss: 3.050710439682007
Validation loss: 2.7136618860306276

Epoch: 5| Step: 10
Training loss: 2.5790340900421143
Validation loss: 2.712728436275195

Epoch: 47| Step: 0
Training loss: 3.0307116508483887
Validation loss: 2.7120841844107515

Epoch: 5| Step: 1
Training loss: 3.0607786178588867
Validation loss: 2.7083160082499185

Epoch: 5| Step: 2
Training loss: 2.643707275390625
Validation loss: 2.7104750628112466

Epoch: 5| Step: 3
Training loss: 2.635143280029297
Validation loss: 2.7106499082298687

Epoch: 5| Step: 4
Training loss: 3.4034957885742188
Validation loss: 2.7057759761810303

Epoch: 5| Step: 5
Training loss: 2.243921995162964
Validation loss: 2.7109017807950258

Epoch: 5| Step: 6
Training loss: 3.441692352294922
Validation loss: 2.7079901105614117

Epoch: 5| Step: 7
Training loss: 3.320702075958252
Validation loss: 2.707675467255295

Epoch: 5| Step: 8
Training loss: 2.6602463722229004
Validation loss: 2.7098193553186234

Epoch: 5| Step: 9
Training loss: 2.6311392784118652
Validation loss: 2.7051640966887116

Epoch: 5| Step: 10
Training loss: 2.4596428871154785
Validation loss: 2.706526343540479

Epoch: 48| Step: 0
Training loss: 3.1955204010009766
Validation loss: 2.7050014772722797

Epoch: 5| Step: 1
Training loss: 3.154606580734253
Validation loss: 2.705680713858656

Epoch: 5| Step: 2
Training loss: 2.672468662261963
Validation loss: 2.7010783431350545

Epoch: 5| Step: 3
Training loss: 2.271317958831787
Validation loss: 2.701342369920464

Epoch: 5| Step: 4
Training loss: 2.3363490104675293
Validation loss: 2.6967662842042985

Epoch: 5| Step: 5
Training loss: 3.4251036643981934
Validation loss: 2.700477236060686

Epoch: 5| Step: 6
Training loss: 2.8739755153656006
Validation loss: 2.6972086378323135

Epoch: 5| Step: 7
Training loss: 3.0537562370300293
Validation loss: 2.6993069033468924

Epoch: 5| Step: 8
Training loss: 2.667635679244995
Validation loss: 2.6953164556975007

Epoch: 5| Step: 9
Training loss: 2.7977042198181152
Validation loss: 2.696308805096534

Epoch: 5| Step: 10
Training loss: 3.1399614810943604
Validation loss: 2.705259678184345

Epoch: 49| Step: 0
Training loss: 3.1350369453430176
Validation loss: 2.7015431260549896

Epoch: 5| Step: 1
Training loss: 3.1456520557403564
Validation loss: 2.7096760990799114

Epoch: 5| Step: 2
Training loss: 2.9762284755706787
Validation loss: 2.6956430276234946

Epoch: 5| Step: 3
Training loss: 3.1951658725738525
Validation loss: 2.693716736250026

Epoch: 5| Step: 4
Training loss: 2.9151763916015625
Validation loss: 2.6934544245402017

Epoch: 5| Step: 5
Training loss: 2.82430362701416
Validation loss: 2.691942758457635

Epoch: 5| Step: 6
Training loss: 2.9765467643737793
Validation loss: 2.6920430352610927

Epoch: 5| Step: 7
Training loss: 3.0139267444610596
Validation loss: 2.6940448232876357

Epoch: 5| Step: 8
Training loss: 2.2459285259246826
Validation loss: 2.692925286549394

Epoch: 5| Step: 9
Training loss: 2.8265011310577393
Validation loss: 2.6897788970701155

Epoch: 5| Step: 10
Training loss: 2.107492685317993
Validation loss: 2.69183793375569

Epoch: 50| Step: 0
Training loss: 3.455941677093506
Validation loss: 2.693586218741632

Epoch: 5| Step: 1
Training loss: 2.902883768081665
Validation loss: 2.698819609098537

Epoch: 5| Step: 2
Training loss: 2.571057081222534
Validation loss: 2.70174926839849

Epoch: 5| Step: 3
Training loss: 3.3869755268096924
Validation loss: 2.7023170173809095

Epoch: 5| Step: 4
Training loss: 3.1455705165863037
Validation loss: 2.6969621655761555

Epoch: 5| Step: 5
Training loss: 3.6332004070281982
Validation loss: 2.6969441444643083

Epoch: 5| Step: 6
Training loss: 2.6752257347106934
Validation loss: 2.6926840556565153

Epoch: 5| Step: 7
Training loss: 2.9572319984436035
Validation loss: 2.688909048675209

Epoch: 5| Step: 8
Training loss: 2.1001534461975098
Validation loss: 2.690045782314834

Epoch: 5| Step: 9
Training loss: 2.0532050132751465
Validation loss: 2.6905707415714057

Epoch: 5| Step: 10
Training loss: 2.537447929382324
Validation loss: 2.688035875238398

Epoch: 51| Step: 0
Training loss: 2.8579583168029785
Validation loss: 2.688108754414384

Epoch: 5| Step: 1
Training loss: 2.0882673263549805
Validation loss: 2.686176317994313

Epoch: 5| Step: 2
Training loss: 2.7438130378723145
Validation loss: 2.6875382008091098

Epoch: 5| Step: 3
Training loss: 2.254873752593994
Validation loss: 2.6866269419270177

Epoch: 5| Step: 4
Training loss: 2.8007330894470215
Validation loss: 2.685265382130941

Epoch: 5| Step: 5
Training loss: 3.0424587726593018
Validation loss: 2.685336797468124

Epoch: 5| Step: 6
Training loss: 3.616501569747925
Validation loss: 2.6839120708486086

Epoch: 5| Step: 7
Training loss: 3.4246439933776855
Validation loss: 2.6817784027386735

Epoch: 5| Step: 8
Training loss: 3.15098237991333
Validation loss: 2.682191059153567

Epoch: 5| Step: 9
Training loss: 3.0154147148132324
Validation loss: 2.681930572755875

Epoch: 5| Step: 10
Training loss: 2.3766539096832275
Validation loss: 2.680194636826874

Epoch: 52| Step: 0
Training loss: 2.984482765197754
Validation loss: 2.677751141209756

Epoch: 5| Step: 1
Training loss: 3.0945873260498047
Validation loss: 2.67799360265014

Epoch: 5| Step: 2
Training loss: 2.368272304534912
Validation loss: 2.681484527485345

Epoch: 5| Step: 3
Training loss: 3.3822638988494873
Validation loss: 2.6824958119341122

Epoch: 5| Step: 4
Training loss: 2.9763078689575195
Validation loss: 2.6801092496482273

Epoch: 5| Step: 5
Training loss: 2.8550899028778076
Validation loss: 2.682785103397985

Epoch: 5| Step: 6
Training loss: 3.662972927093506
Validation loss: 2.679439671577946

Epoch: 5| Step: 7
Training loss: 2.381978988647461
Validation loss: 2.6803817595205

Epoch: 5| Step: 8
Training loss: 2.902193307876587
Validation loss: 2.6770985382859425

Epoch: 5| Step: 9
Training loss: 2.080104351043701
Validation loss: 2.6784982296728317

Epoch: 5| Step: 10
Training loss: 2.634369134902954
Validation loss: 2.678154089117563

Epoch: 53| Step: 0
Training loss: 3.1320252418518066
Validation loss: 2.676467167433872

Epoch: 5| Step: 1
Training loss: 2.2739670276641846
Validation loss: 2.6769388542380383

Epoch: 5| Step: 2
Training loss: 2.6991758346557617
Validation loss: 2.6778880742288407

Epoch: 5| Step: 3
Training loss: 2.6356801986694336
Validation loss: 2.6808525234140377

Epoch: 5| Step: 4
Training loss: 3.026684522628784
Validation loss: 2.677974808600641

Epoch: 5| Step: 5
Training loss: 2.609586238861084
Validation loss: 2.6777069619906846

Epoch: 5| Step: 6
Training loss: 2.8590290546417236
Validation loss: 2.6771371005683817

Epoch: 5| Step: 7
Training loss: 3.1488242149353027
Validation loss: 2.675489946078229

Epoch: 5| Step: 8
Training loss: 2.761291742324829
Validation loss: 2.678269123518339

Epoch: 5| Step: 9
Training loss: 2.858825445175171
Validation loss: 2.676481162348101

Epoch: 5| Step: 10
Training loss: 3.46100115776062
Validation loss: 2.681158972042863

Epoch: 54| Step: 0
Training loss: 2.7176401615142822
Validation loss: 2.6740516744634157

Epoch: 5| Step: 1
Training loss: 3.0763297080993652
Validation loss: 2.674398717059884

Epoch: 5| Step: 2
Training loss: 2.6862993240356445
Validation loss: 2.67463610505545

Epoch: 5| Step: 3
Training loss: 2.9661428928375244
Validation loss: 2.672803765983992

Epoch: 5| Step: 4
Training loss: 3.5489540100097656
Validation loss: 2.6727209527005433

Epoch: 5| Step: 5
Training loss: 2.5449280738830566
Validation loss: 2.672493193739204

Epoch: 5| Step: 6
Training loss: 2.6651930809020996
Validation loss: 2.6771911164765716

Epoch: 5| Step: 7
Training loss: 2.6323578357696533
Validation loss: 2.6778713157100063

Epoch: 5| Step: 8
Training loss: 2.5527801513671875
Validation loss: 2.675938501152941

Epoch: 5| Step: 9
Training loss: 2.7444746494293213
Validation loss: 2.676162112143732

Epoch: 5| Step: 10
Training loss: 3.295228958129883
Validation loss: 2.676191491465415

Epoch: 55| Step: 0
Training loss: 2.618131160736084
Validation loss: 2.6760935360385525

Epoch: 5| Step: 1
Training loss: 2.7346174716949463
Validation loss: 2.6756238681013866

Epoch: 5| Step: 2
Training loss: 3.0008652210235596
Validation loss: 2.6759976904879332

Epoch: 5| Step: 3
Training loss: 3.0999433994293213
Validation loss: 2.6769705023816837

Epoch: 5| Step: 4
Training loss: 2.3643999099731445
Validation loss: 2.674404587796939

Epoch: 5| Step: 5
Training loss: 2.6708920001983643
Validation loss: 2.6764953623535814

Epoch: 5| Step: 6
Training loss: 3.158862352371216
Validation loss: 2.674854452892016

Epoch: 5| Step: 7
Training loss: 3.218756914138794
Validation loss: 2.675408537669848

Epoch: 5| Step: 8
Training loss: 3.2212371826171875
Validation loss: 2.675614168567042

Epoch: 5| Step: 9
Training loss: 2.609539747238159
Validation loss: 2.668752788215555

Epoch: 5| Step: 10
Training loss: 2.6262876987457275
Validation loss: 2.6726196863318004

Epoch: 56| Step: 0
Training loss: 2.7141048908233643
Validation loss: 2.6707176341805408

Epoch: 5| Step: 1
Training loss: 3.0200178623199463
Validation loss: 2.6670662023687877

Epoch: 5| Step: 2
Training loss: 3.0543923377990723
Validation loss: 2.670165090150731

Epoch: 5| Step: 3
Training loss: 2.7927072048187256
Validation loss: 2.6668890804372807

Epoch: 5| Step: 4
Training loss: 2.9545650482177734
Validation loss: 2.6720990929552304

Epoch: 5| Step: 5
Training loss: 2.7939205169677734
Validation loss: 2.668269234318887

Epoch: 5| Step: 6
Training loss: 2.635298252105713
Validation loss: 2.665931301732217

Epoch: 5| Step: 7
Training loss: 2.6696014404296875
Validation loss: 2.6668385254439486

Epoch: 5| Step: 8
Training loss: 2.7957472801208496
Validation loss: 2.6658810338666363

Epoch: 5| Step: 9
Training loss: 3.085999011993408
Validation loss: 2.668400961865661

Epoch: 5| Step: 10
Training loss: 2.775117874145508
Validation loss: 2.6649243729088896

Epoch: 57| Step: 0
Training loss: 3.0475704669952393
Validation loss: 2.6649391266607467

Epoch: 5| Step: 1
Training loss: 3.060227632522583
Validation loss: 2.6651042661359234

Epoch: 5| Step: 2
Training loss: 2.5058541297912598
Validation loss: 2.6616109212239585

Epoch: 5| Step: 3
Training loss: 3.011253595352173
Validation loss: 2.6624457810514714

Epoch: 5| Step: 4
Training loss: 2.85784649848938
Validation loss: 2.6610737154560704

Epoch: 5| Step: 5
Training loss: 3.083244800567627
Validation loss: 2.663909694199921

Epoch: 5| Step: 6
Training loss: 3.165402889251709
Validation loss: 2.6616068475989887

Epoch: 5| Step: 7
Training loss: 2.130816698074341
Validation loss: 2.659845285518195

Epoch: 5| Step: 8
Training loss: 3.0918822288513184
Validation loss: 2.6600585958009124

Epoch: 5| Step: 9
Training loss: 2.341951847076416
Validation loss: 2.6619649574320805

Epoch: 5| Step: 10
Training loss: 3.005152940750122
Validation loss: 2.6593715375469578

Epoch: 58| Step: 0
Training loss: 2.670774459838867
Validation loss: 2.6624875889029553

Epoch: 5| Step: 1
Training loss: 3.0212860107421875
Validation loss: 2.6603630896537536

Epoch: 5| Step: 2
Training loss: 2.8556602001190186
Validation loss: 2.6648838109867548

Epoch: 5| Step: 3
Training loss: 2.9617085456848145
Validation loss: 2.6604632228933354

Epoch: 5| Step: 4
Training loss: 2.727829694747925
Validation loss: 2.664083996126729

Epoch: 5| Step: 5
Training loss: 2.820021390914917
Validation loss: 2.6601228098715506

Epoch: 5| Step: 6
Training loss: 2.9242501258850098
Validation loss: 2.65921474272205

Epoch: 5| Step: 7
Training loss: 3.347724199295044
Validation loss: 2.6592859093860914

Epoch: 5| Step: 8
Training loss: 2.5967369079589844
Validation loss: 2.6541936013006393

Epoch: 5| Step: 9
Training loss: 2.5391182899475098
Validation loss: 2.6529521378137733

Epoch: 5| Step: 10
Training loss: 2.7779455184936523
Validation loss: 2.651866087349512

Epoch: 59| Step: 0
Training loss: 2.6048591136932373
Validation loss: 2.651559568220569

Epoch: 5| Step: 1
Training loss: 3.3194661140441895
Validation loss: 2.654955094860446

Epoch: 5| Step: 2
Training loss: 2.314079761505127
Validation loss: 2.652970306334957

Epoch: 5| Step: 3
Training loss: 2.1771979331970215
Validation loss: 2.654868113097324

Epoch: 5| Step: 4
Training loss: 3.062574863433838
Validation loss: 2.6528135986738306

Epoch: 5| Step: 5
Training loss: 2.9219858646392822
Validation loss: 2.6554604294479534

Epoch: 5| Step: 6
Training loss: 2.550896406173706
Validation loss: 2.651874480708953

Epoch: 5| Step: 7
Training loss: 3.509709119796753
Validation loss: 2.651793731156216

Epoch: 5| Step: 8
Training loss: 2.9168972969055176
Validation loss: 2.6548047834827053

Epoch: 5| Step: 9
Training loss: 2.7390189170837402
Validation loss: 2.6554087592709448

Epoch: 5| Step: 10
Training loss: 3.126857042312622
Validation loss: 2.6571191728755994

Epoch: 60| Step: 0
Training loss: 2.681112766265869
Validation loss: 2.651610625687466

Epoch: 5| Step: 1
Training loss: 2.6743500232696533
Validation loss: 2.6501287234726774

Epoch: 5| Step: 2
Training loss: 2.296720027923584
Validation loss: 2.6493755514903734

Epoch: 5| Step: 3
Training loss: 2.343585968017578
Validation loss: 2.6481522539610505

Epoch: 5| Step: 4
Training loss: 3.037905693054199
Validation loss: 2.6507990411532822

Epoch: 5| Step: 5
Training loss: 2.7134640216827393
Validation loss: 2.648654196851997

Epoch: 5| Step: 6
Training loss: 2.9583981037139893
Validation loss: 2.645902790049071

Epoch: 5| Step: 7
Training loss: 3.8281326293945312
Validation loss: 2.648864515366093

Epoch: 5| Step: 8
Training loss: 3.038067102432251
Validation loss: 2.648142432653776

Epoch: 5| Step: 9
Training loss: 2.600198745727539
Validation loss: 2.645262356727354

Epoch: 5| Step: 10
Training loss: 3.0022597312927246
Validation loss: 2.645635594603836

Epoch: 61| Step: 0
Training loss: 2.969883441925049
Validation loss: 2.6471713589083765

Epoch: 5| Step: 1
Training loss: 3.6738388538360596
Validation loss: 2.641366463835521

Epoch: 5| Step: 2
Training loss: 3.358248233795166
Validation loss: 2.646204997134465

Epoch: 5| Step: 3
Training loss: 2.7164359092712402
Validation loss: 2.6463147747901177

Epoch: 5| Step: 4
Training loss: 2.3240737915039062
Validation loss: 2.647410974707655

Epoch: 5| Step: 5
Training loss: 3.1313507556915283
Validation loss: 2.6490334105748

Epoch: 5| Step: 6
Training loss: 2.2182154655456543
Validation loss: 2.6439240876064507

Epoch: 5| Step: 7
Training loss: 2.020738363265991
Validation loss: 2.6421165312490156

Epoch: 5| Step: 8
Training loss: 2.3328299522399902
Validation loss: 2.641267935434977

Epoch: 5| Step: 9
Training loss: 2.9999687671661377
Validation loss: 2.644980376766574

Epoch: 5| Step: 10
Training loss: 3.432910919189453
Validation loss: 2.642196693728047

Epoch: 62| Step: 0
Training loss: 2.367565155029297
Validation loss: 2.6401447608906734

Epoch: 5| Step: 1
Training loss: 2.4388232231140137
Validation loss: 2.6394935679692093

Epoch: 5| Step: 2
Training loss: 2.5241551399230957
Validation loss: 2.6425325024512505

Epoch: 5| Step: 3
Training loss: 2.1885087490081787
Validation loss: 2.6431605713341826

Epoch: 5| Step: 4
Training loss: 3.2722396850585938
Validation loss: 2.6420856432248185

Epoch: 5| Step: 5
Training loss: 3.2433724403381348
Validation loss: 2.6426986878918064

Epoch: 5| Step: 6
Training loss: 2.514838695526123
Validation loss: 2.6391893407349944

Epoch: 5| Step: 7
Training loss: 3.9756112098693848
Validation loss: 2.6449719423888833

Epoch: 5| Step: 8
Training loss: 3.2806923389434814
Validation loss: 2.6472362677256265

Epoch: 5| Step: 9
Training loss: 2.5713305473327637
Validation loss: 2.6396687440974738

Epoch: 5| Step: 10
Training loss: 2.708383798599243
Validation loss: 2.644421849199521

Epoch: 63| Step: 0
Training loss: 2.8445022106170654
Validation loss: 2.643323459932881

Epoch: 5| Step: 1
Training loss: 2.457690715789795
Validation loss: 2.642763337781352

Epoch: 5| Step: 2
Training loss: 2.928657054901123
Validation loss: 2.636277614101287

Epoch: 5| Step: 3
Training loss: 2.8597640991210938
Validation loss: 2.637451361584407

Epoch: 5| Step: 4
Training loss: 3.1266140937805176
Validation loss: 2.6400124667793192

Epoch: 5| Step: 5
Training loss: 2.84321928024292
Validation loss: 2.637562526169644

Epoch: 5| Step: 6
Training loss: 3.329864501953125
Validation loss: 2.637939360833937

Epoch: 5| Step: 7
Training loss: 2.8132452964782715
Validation loss: 2.638279776419363

Epoch: 5| Step: 8
Training loss: 2.5111000537872314
Validation loss: 2.640533798484392

Epoch: 5| Step: 9
Training loss: 2.505617618560791
Validation loss: 2.636351118805588

Epoch: 5| Step: 10
Training loss: 2.7953732013702393
Validation loss: 2.6347199896330475

Epoch: 64| Step: 0
Training loss: 3.217928647994995
Validation loss: 2.633088363114224

Epoch: 5| Step: 1
Training loss: 2.506195306777954
Validation loss: 2.6381392171305995

Epoch: 5| Step: 2
Training loss: 3.024016857147217
Validation loss: 2.635729325714932

Epoch: 5| Step: 3
Training loss: 2.8812408447265625
Validation loss: 2.6365945851931007

Epoch: 5| Step: 4
Training loss: 3.4715018272399902
Validation loss: 2.636713825246339

Epoch: 5| Step: 5
Training loss: 2.664762496948242
Validation loss: 2.642066640238608

Epoch: 5| Step: 6
Training loss: 2.716705322265625
Validation loss: 2.6359957007951635

Epoch: 5| Step: 7
Training loss: 2.682079315185547
Validation loss: 2.633082564159106

Epoch: 5| Step: 8
Training loss: 2.6849193572998047
Validation loss: 2.638291274347613

Epoch: 5| Step: 9
Training loss: 2.629981517791748
Validation loss: 2.6386587440326648

Epoch: 5| Step: 10
Training loss: 2.4697635173797607
Validation loss: 2.6440455528997604

Epoch: 65| Step: 0
Training loss: 3.245976209640503
Validation loss: 2.635989922349171

Epoch: 5| Step: 1
Training loss: 3.5271568298339844
Validation loss: 2.6363486295105307

Epoch: 5| Step: 2
Training loss: 3.577624559402466
Validation loss: 2.6387893640866844

Epoch: 5| Step: 3
Training loss: 3.197498321533203
Validation loss: 2.633674644654797

Epoch: 5| Step: 4
Training loss: 2.1426727771759033
Validation loss: 2.6332951386769614

Epoch: 5| Step: 5
Training loss: 2.9120612144470215
Validation loss: 2.634823981151786

Epoch: 5| Step: 6
Training loss: 2.136697769165039
Validation loss: 2.6335519231775755

Epoch: 5| Step: 7
Training loss: 2.8778042793273926
Validation loss: 2.6314788454322406

Epoch: 5| Step: 8
Training loss: 3.02351975440979
Validation loss: 2.631420281625563

Epoch: 5| Step: 9
Training loss: 2.3891732692718506
Validation loss: 2.627660151450865

Epoch: 5| Step: 10
Training loss: 1.6953719854354858
Validation loss: 2.6318594537755495

Epoch: 66| Step: 0
Training loss: 2.9732251167297363
Validation loss: 2.630081630522205

Epoch: 5| Step: 1
Training loss: 3.366960048675537
Validation loss: 2.6278864337551977

Epoch: 5| Step: 2
Training loss: 3.3483529090881348
Validation loss: 2.6368574942311933

Epoch: 5| Step: 3
Training loss: 2.641814708709717
Validation loss: 2.630421256506315

Epoch: 5| Step: 4
Training loss: 2.911774158477783
Validation loss: 2.630016529431907

Epoch: 5| Step: 5
Training loss: 2.8292596340179443
Validation loss: 2.627718171765727

Epoch: 5| Step: 6
Training loss: 2.866964340209961
Validation loss: 2.626582827619327

Epoch: 5| Step: 7
Training loss: 2.8859105110168457
Validation loss: 2.6270751132759997

Epoch: 5| Step: 8
Training loss: 2.3937957286834717
Validation loss: 2.627277781886439

Epoch: 5| Step: 9
Training loss: 2.030374050140381
Validation loss: 2.6267814149138746

Epoch: 5| Step: 10
Training loss: 2.628812313079834
Validation loss: 2.6308466952334166

Epoch: 67| Step: 0
Training loss: 3.1820549964904785
Validation loss: 2.6290896605419856

Epoch: 5| Step: 1
Training loss: 2.5285980701446533
Validation loss: 2.63415390317158

Epoch: 5| Step: 2
Training loss: 3.2171401977539062
Validation loss: 2.6348359892445226

Epoch: 5| Step: 3
Training loss: 2.73889422416687
Validation loss: 2.637100117180937

Epoch: 5| Step: 4
Training loss: 2.383016586303711
Validation loss: 2.633939055986302

Epoch: 5| Step: 5
Training loss: 3.440309524536133
Validation loss: 2.6301744830223823

Epoch: 5| Step: 6
Training loss: 2.517324447631836
Validation loss: 2.6252393209806053

Epoch: 5| Step: 7
Training loss: 2.8504934310913086
Validation loss: 2.624812702978811

Epoch: 5| Step: 8
Training loss: 2.7105765342712402
Validation loss: 2.627691158684351

Epoch: 5| Step: 9
Training loss: 2.7325382232666016
Validation loss: 2.6304423629596667

Epoch: 5| Step: 10
Training loss: 2.6095612049102783
Validation loss: 2.625077052782941

Epoch: 68| Step: 0
Training loss: 2.3772459030151367
Validation loss: 2.624316951279999

Epoch: 5| Step: 1
Training loss: 2.5600197315216064
Validation loss: 2.6214302252697688

Epoch: 5| Step: 2
Training loss: 3.4040045738220215
Validation loss: 2.6270102736770466

Epoch: 5| Step: 3
Training loss: 3.236302614212036
Validation loss: 2.625306296092208

Epoch: 5| Step: 4
Training loss: 2.8063693046569824
Validation loss: 2.627339711753271

Epoch: 5| Step: 5
Training loss: 2.8214991092681885
Validation loss: 2.627251799388598

Epoch: 5| Step: 6
Training loss: 2.4370675086975098
Validation loss: 2.640958862919961

Epoch: 5| Step: 7
Training loss: 3.2264583110809326
Validation loss: 2.634178130857406

Epoch: 5| Step: 8
Training loss: 3.1268889904022217
Validation loss: 2.6293613423583326

Epoch: 5| Step: 9
Training loss: 2.5975234508514404
Validation loss: 2.6340792896927043

Epoch: 5| Step: 10
Training loss: 2.167600154876709
Validation loss: 2.635295324428107

Epoch: 69| Step: 0
Training loss: 2.6476292610168457
Validation loss: 2.635519676311042

Epoch: 5| Step: 1
Training loss: 2.7865872383117676
Validation loss: 2.6310997445096254

Epoch: 5| Step: 2
Training loss: 3.5171077251434326
Validation loss: 2.629696589644237

Epoch: 5| Step: 3
Training loss: 2.033634662628174
Validation loss: 2.6246091883669616

Epoch: 5| Step: 4
Training loss: 1.8215258121490479
Validation loss: 2.627532943602531

Epoch: 5| Step: 5
Training loss: 2.2664496898651123
Validation loss: 2.62769502721807

Epoch: 5| Step: 6
Training loss: 3.413756847381592
Validation loss: 2.626151725810061

Epoch: 5| Step: 7
Training loss: 2.954800605773926
Validation loss: 2.6263236332965154

Epoch: 5| Step: 8
Training loss: 3.5480430126190186
Validation loss: 2.6284713411843903

Epoch: 5| Step: 9
Training loss: 3.107426881790161
Validation loss: 2.6245135030438824

Epoch: 5| Step: 10
Training loss: 2.7162303924560547
Validation loss: 2.6258344675904963

Epoch: 70| Step: 0
Training loss: 2.8077290058135986
Validation loss: 2.61879494626035

Epoch: 5| Step: 1
Training loss: 2.600362539291382
Validation loss: 2.62146270403298

Epoch: 5| Step: 2
Training loss: 2.2176501750946045
Validation loss: 2.619836425268522

Epoch: 5| Step: 3
Training loss: 3.0374152660369873
Validation loss: 2.6214924089370237

Epoch: 5| Step: 4
Training loss: 2.6481051445007324
Validation loss: 2.6195218768171085

Epoch: 5| Step: 5
Training loss: 2.6484007835388184
Validation loss: 2.622812799228135

Epoch: 5| Step: 6
Training loss: 3.0070459842681885
Validation loss: 2.6231582805674565

Epoch: 5| Step: 7
Training loss: 2.9256110191345215
Validation loss: 2.6195715088998117

Epoch: 5| Step: 8
Training loss: 3.225461959838867
Validation loss: 2.621799720230923

Epoch: 5| Step: 9
Training loss: 2.6657521724700928
Validation loss: 2.620876735256564

Epoch: 5| Step: 10
Training loss: 3.0571234226226807
Validation loss: 2.615213471074258

Epoch: 71| Step: 0
Training loss: 3.431215286254883
Validation loss: 2.6208272185376895

Epoch: 5| Step: 1
Training loss: 2.1630287170410156
Validation loss: 2.6183344138565885

Epoch: 5| Step: 2
Training loss: 2.6445040702819824
Validation loss: 2.619243032188826

Epoch: 5| Step: 3
Training loss: 3.176002025604248
Validation loss: 2.621829222607356

Epoch: 5| Step: 4
Training loss: 3.1813223361968994
Validation loss: 2.6183066727012716

Epoch: 5| Step: 5
Training loss: 3.6121127605438232
Validation loss: 2.623262233631585

Epoch: 5| Step: 6
Training loss: 2.9884679317474365
Validation loss: 2.6170720772076677

Epoch: 5| Step: 7
Training loss: 2.134841203689575
Validation loss: 2.6159106941633326

Epoch: 5| Step: 8
Training loss: 2.8272571563720703
Validation loss: 2.6183346727842927

Epoch: 5| Step: 9
Training loss: 2.509453535079956
Validation loss: 2.6132968420623452

Epoch: 5| Step: 10
Training loss: 1.9432324171066284
Validation loss: 2.6183437660176265

Epoch: 72| Step: 0
Training loss: 2.1178219318389893
Validation loss: 2.6173136772647982

Epoch: 5| Step: 1
Training loss: 2.515216827392578
Validation loss: 2.6198545809715026

Epoch: 5| Step: 2
Training loss: 3.180164337158203
Validation loss: 2.6163064792592037

Epoch: 5| Step: 3
Training loss: 3.0510010719299316
Validation loss: 2.621289283998551

Epoch: 5| Step: 4
Training loss: 3.2316079139709473
Validation loss: 2.619264236060522

Epoch: 5| Step: 5
Training loss: 2.5691123008728027
Validation loss: 2.621137220372436

Epoch: 5| Step: 6
Training loss: 2.9166922569274902
Validation loss: 2.6223840636591755

Epoch: 5| Step: 7
Training loss: 2.9266974925994873
Validation loss: 2.618010426080355

Epoch: 5| Step: 8
Training loss: 3.1980364322662354
Validation loss: 2.619060649666735

Epoch: 5| Step: 9
Training loss: 2.9385528564453125
Validation loss: 2.616992176219981

Epoch: 5| Step: 10
Training loss: 1.972612977027893
Validation loss: 2.613753813569264

Epoch: 73| Step: 0
Training loss: 3.193392515182495
Validation loss: 2.6185117101156585

Epoch: 5| Step: 1
Training loss: 2.4264845848083496
Validation loss: 2.6169428607468963

Epoch: 5| Step: 2
Training loss: 2.8998446464538574
Validation loss: 2.621643320206673

Epoch: 5| Step: 3
Training loss: 3.0587003231048584
Validation loss: 2.6208986620749197

Epoch: 5| Step: 4
Training loss: 2.5286803245544434
Validation loss: 2.6242978085753736

Epoch: 5| Step: 5
Training loss: 2.2615559101104736
Validation loss: 2.6185533872214695

Epoch: 5| Step: 6
Training loss: 2.913794755935669
Validation loss: 2.6171956344317366

Epoch: 5| Step: 7
Training loss: 2.9242072105407715
Validation loss: 2.617498979773573

Epoch: 5| Step: 8
Training loss: 2.6128649711608887
Validation loss: 2.6125957222395044

Epoch: 5| Step: 9
Training loss: 2.8872082233428955
Validation loss: 2.613594134648641

Epoch: 5| Step: 10
Training loss: 3.072046995162964
Validation loss: 2.611371845327398

Epoch: 74| Step: 0
Training loss: 2.965999126434326
Validation loss: 2.6126638817530807

Epoch: 5| Step: 1
Training loss: 3.050595283508301
Validation loss: 2.6149475446311374

Epoch: 5| Step: 2
Training loss: 2.771449565887451
Validation loss: 2.6183588889337357

Epoch: 5| Step: 3
Training loss: 2.6940958499908447
Validation loss: 2.620517425639655

Epoch: 5| Step: 4
Training loss: 3.620945692062378
Validation loss: 2.6180464554858465

Epoch: 5| Step: 5
Training loss: 2.6181039810180664
Validation loss: 2.616032615784676

Epoch: 5| Step: 6
Training loss: 3.186866283416748
Validation loss: 2.6149644749138945

Epoch: 5| Step: 7
Training loss: 2.328777551651001
Validation loss: 2.613630043563022

Epoch: 5| Step: 8
Training loss: 2.2186548709869385
Validation loss: 2.6120173085120415

Epoch: 5| Step: 9
Training loss: 2.382124662399292
Validation loss: 2.6150483264718005

Epoch: 5| Step: 10
Training loss: 2.773843765258789
Validation loss: 2.6070351293010097

Epoch: 75| Step: 0
Training loss: 2.4619789123535156
Validation loss: 2.6092283315556024

Epoch: 5| Step: 1
Training loss: 3.112107515335083
Validation loss: 2.6121187389537854

Epoch: 5| Step: 2
Training loss: 3.055542469024658
Validation loss: 2.6116054673348703

Epoch: 5| Step: 3
Training loss: 2.536665439605713
Validation loss: 2.6144562767397974

Epoch: 5| Step: 4
Training loss: 2.805626392364502
Validation loss: 2.613281349981985

Epoch: 5| Step: 5
Training loss: 2.3729805946350098
Validation loss: 2.615426463465537

Epoch: 5| Step: 6
Training loss: 2.6845242977142334
Validation loss: 2.6119641719325895

Epoch: 5| Step: 7
Training loss: 3.2258129119873047
Validation loss: 2.6110105078707457

Epoch: 5| Step: 8
Training loss: 3.042046070098877
Validation loss: 2.6061570900742725

Epoch: 5| Step: 9
Training loss: 2.6451046466827393
Validation loss: 2.60519770140289

Epoch: 5| Step: 10
Training loss: 2.654378890991211
Validation loss: 2.605245982446978

Epoch: 76| Step: 0
Training loss: 2.7174463272094727
Validation loss: 2.6046793819755636

Epoch: 5| Step: 1
Training loss: 3.057933807373047
Validation loss: 2.609904614827966

Epoch: 5| Step: 2
Training loss: 2.873002052307129
Validation loss: 2.6053884619025776

Epoch: 5| Step: 3
Training loss: 2.7436423301696777
Validation loss: 2.6058869643877913

Epoch: 5| Step: 4
Training loss: 2.879459857940674
Validation loss: 2.6090706740656207

Epoch: 5| Step: 5
Training loss: 3.0856170654296875
Validation loss: 2.6096199379172376

Epoch: 5| Step: 6
Training loss: 2.7083375453948975
Validation loss: 2.6059885794116604

Epoch: 5| Step: 7
Training loss: 2.9583945274353027
Validation loss: 2.6057082735082155

Epoch: 5| Step: 8
Training loss: 2.7074050903320312
Validation loss: 2.6074238900215394

Epoch: 5| Step: 9
Training loss: 2.2758116722106934
Validation loss: 2.6079432656688075

Epoch: 5| Step: 10
Training loss: 2.536626100540161
Validation loss: 2.615583486454461

Epoch: 77| Step: 0
Training loss: 2.210221767425537
Validation loss: 2.6089416229596702

Epoch: 5| Step: 1
Training loss: 3.6587822437286377
Validation loss: 2.617863094934853

Epoch: 5| Step: 2
Training loss: 1.912965178489685
Validation loss: 2.6366046731190016

Epoch: 5| Step: 3
Training loss: 3.1788272857666016
Validation loss: 2.632124872617824

Epoch: 5| Step: 4
Training loss: 3.1025688648223877
Validation loss: 2.637546226542483

Epoch: 5| Step: 5
Training loss: 3.0540683269500732
Validation loss: 2.633666984496578

Epoch: 5| Step: 6
Training loss: 3.027219772338867
Validation loss: 2.62227391171199

Epoch: 5| Step: 7
Training loss: 2.1283769607543945
Validation loss: 2.6175947266240276

Epoch: 5| Step: 8
Training loss: 3.0743889808654785
Validation loss: 2.612331900545346

Epoch: 5| Step: 9
Training loss: 2.7683615684509277
Validation loss: 2.617126462280109

Epoch: 5| Step: 10
Training loss: 2.4613475799560547
Validation loss: 2.612309491762551

Epoch: 78| Step: 0
Training loss: 2.752274990081787
Validation loss: 2.606799958854593

Epoch: 5| Step: 1
Training loss: 3.0292654037475586
Validation loss: 2.612932018054429

Epoch: 5| Step: 2
Training loss: 2.431636333465576
Validation loss: 2.6147844714503132

Epoch: 5| Step: 3
Training loss: 2.589323043823242
Validation loss: 2.6086110863634335

Epoch: 5| Step: 4
Training loss: 3.428670883178711
Validation loss: 2.6028500987637426

Epoch: 5| Step: 5
Training loss: 2.8861382007598877
Validation loss: 2.600948605486142

Epoch: 5| Step: 6
Training loss: 3.0383410453796387
Validation loss: 2.6030462249632804

Epoch: 5| Step: 7
Training loss: 2.5501914024353027
Validation loss: 2.6040528128224034

Epoch: 5| Step: 8
Training loss: 2.3206183910369873
Validation loss: 2.614849370013001

Epoch: 5| Step: 9
Training loss: 2.0252957344055176
Validation loss: 2.6223785108135593

Epoch: 5| Step: 10
Training loss: 3.682126998901367
Validation loss: 2.6191649898405998

Epoch: 79| Step: 0
Training loss: 2.868494749069214
Validation loss: 2.621403709534676

Epoch: 5| Step: 1
Training loss: 3.408965587615967
Validation loss: 2.6154550429313415

Epoch: 5| Step: 2
Training loss: 2.7842063903808594
Validation loss: 2.6105210063278035

Epoch: 5| Step: 3
Training loss: 2.738309383392334
Validation loss: 2.6016673605929137

Epoch: 5| Step: 4
Training loss: 2.7908546924591064
Validation loss: 2.6000506749717136

Epoch: 5| Step: 5
Training loss: 2.925417423248291
Validation loss: 2.598775258628271

Epoch: 5| Step: 6
Training loss: 2.406099557876587
Validation loss: 2.6020225171119935

Epoch: 5| Step: 7
Training loss: 2.268831968307495
Validation loss: 2.603066490542504

Epoch: 5| Step: 8
Training loss: 3.229693651199341
Validation loss: 2.6008916593367055

Epoch: 5| Step: 9
Training loss: 2.5535356998443604
Validation loss: 2.6041083951150217

Epoch: 5| Step: 10
Training loss: 2.603375196456909
Validation loss: 2.5968956229507283

Epoch: 80| Step: 0
Training loss: 3.5485904216766357
Validation loss: 2.6020910816807903

Epoch: 5| Step: 1
Training loss: 2.6495513916015625
Validation loss: 2.604379177093506

Epoch: 5| Step: 2
Training loss: 2.7153213024139404
Validation loss: 2.607257012398012

Epoch: 5| Step: 3
Training loss: 3.045820713043213
Validation loss: 2.6146623857559694

Epoch: 5| Step: 4
Training loss: 2.6142020225524902
Validation loss: 2.618154684702555

Epoch: 5| Step: 5
Training loss: 2.5865111351013184
Validation loss: 2.619862933312693

Epoch: 5| Step: 6
Training loss: 2.453108549118042
Validation loss: 2.615213924838651

Epoch: 5| Step: 7
Training loss: 2.7466681003570557
Validation loss: 2.6088276575970393

Epoch: 5| Step: 8
Training loss: 2.8520474433898926
Validation loss: 2.6090985600666334

Epoch: 5| Step: 9
Training loss: 2.9633007049560547
Validation loss: 2.596238433673818

Epoch: 5| Step: 10
Training loss: 2.309393882751465
Validation loss: 2.599572279120004

Epoch: 81| Step: 0
Training loss: 2.5629823207855225
Validation loss: 2.6045180597612934

Epoch: 5| Step: 1
Training loss: 2.652731418609619
Validation loss: 2.6006504925348426

Epoch: 5| Step: 2
Training loss: 2.2237441539764404
Validation loss: 2.6037244463479645

Epoch: 5| Step: 3
Training loss: 2.7831969261169434
Validation loss: 2.6078639312457015

Epoch: 5| Step: 4
Training loss: 2.5503947734832764
Validation loss: 2.603765308216054

Epoch: 5| Step: 5
Training loss: 2.317081928253174
Validation loss: 2.5993485040562128

Epoch: 5| Step: 6
Training loss: 2.7018117904663086
Validation loss: 2.5956073832768265

Epoch: 5| Step: 7
Training loss: 3.140937089920044
Validation loss: 2.6000899909645

Epoch: 5| Step: 8
Training loss: 3.1581497192382812
Validation loss: 2.5959906988246466

Epoch: 5| Step: 9
Training loss: 3.6887869834899902
Validation loss: 2.6005598152837446

Epoch: 5| Step: 10
Training loss: 2.667198657989502
Validation loss: 2.6063001617308585

Epoch: 82| Step: 0
Training loss: 3.0594632625579834
Validation loss: 2.6146629215568624

Epoch: 5| Step: 1
Training loss: 2.283318281173706
Validation loss: 2.6100092267477386

Epoch: 5| Step: 2
Training loss: 3.098414421081543
Validation loss: 2.6117785925506265

Epoch: 5| Step: 3
Training loss: 2.7614476680755615
Validation loss: 2.6140882635629303

Epoch: 5| Step: 4
Training loss: 3.430950880050659
Validation loss: 2.617403463650775

Epoch: 5| Step: 5
Training loss: 2.7091972827911377
Validation loss: 2.6135706132458103

Epoch: 5| Step: 6
Training loss: 2.5181097984313965
Validation loss: 2.6101098547699633

Epoch: 5| Step: 7
Training loss: 2.222621440887451
Validation loss: 2.6070247260473107

Epoch: 5| Step: 8
Training loss: 2.4852166175842285
Validation loss: 2.603903626882902

Epoch: 5| Step: 9
Training loss: 2.674187421798706
Validation loss: 2.601859574676842

Epoch: 5| Step: 10
Training loss: 3.355785608291626
Validation loss: 2.5984247140986945

Epoch: 83| Step: 0
Training loss: 2.854600667953491
Validation loss: 2.591229672073036

Epoch: 5| Step: 1
Training loss: 2.7288222312927246
Validation loss: 2.5910508530114287

Epoch: 5| Step: 2
Training loss: 3.028900623321533
Validation loss: 2.5915280926612114

Epoch: 5| Step: 3
Training loss: 2.733574390411377
Validation loss: 2.5951724795884985

Epoch: 5| Step: 4
Training loss: 2.2776947021484375
Validation loss: 2.5925275715448524

Epoch: 5| Step: 5
Training loss: 3.213690996170044
Validation loss: 2.5982807400406047

Epoch: 5| Step: 6
Training loss: 2.405808925628662
Validation loss: 2.5922178247923493

Epoch: 5| Step: 7
Training loss: 2.8365626335144043
Validation loss: 2.5935588882815455

Epoch: 5| Step: 8
Training loss: 2.4727187156677246
Validation loss: 2.593167015301284

Epoch: 5| Step: 9
Training loss: 2.8641858100891113
Validation loss: 2.601346021057457

Epoch: 5| Step: 10
Training loss: 3.048830986022949
Validation loss: 2.5954494168681483

Epoch: 84| Step: 0
Training loss: 1.806884765625
Validation loss: 2.593474372740715

Epoch: 5| Step: 1
Training loss: 2.144423246383667
Validation loss: 2.593934796189749

Epoch: 5| Step: 2
Training loss: 2.659804105758667
Validation loss: 2.592441099946217

Epoch: 5| Step: 3
Training loss: 3.527653455734253
Validation loss: 2.5942138856457126

Epoch: 5| Step: 4
Training loss: 3.194732666015625
Validation loss: 2.5949088706765124

Epoch: 5| Step: 5
Training loss: 2.7899303436279297
Validation loss: 2.588669894843973

Epoch: 5| Step: 6
Training loss: 2.6997227668762207
Validation loss: 2.5883482220352336

Epoch: 5| Step: 7
Training loss: 3.0728135108947754
Validation loss: 2.5912967240938576

Epoch: 5| Step: 8
Training loss: 2.317525863647461
Validation loss: 2.592416194177443

Epoch: 5| Step: 9
Training loss: 3.1220526695251465
Validation loss: 2.590349866497901

Epoch: 5| Step: 10
Training loss: 3.1109237670898438
Validation loss: 2.590593071394069

Epoch: 85| Step: 0
Training loss: 2.497659206390381
Validation loss: 2.5868744927067913

Epoch: 5| Step: 1
Training loss: 2.513643264770508
Validation loss: 2.586517732630494

Epoch: 5| Step: 2
Training loss: 2.2119433879852295
Validation loss: 2.589697835265949

Epoch: 5| Step: 3
Training loss: 2.3780510425567627
Validation loss: 2.5898750392339562

Epoch: 5| Step: 4
Training loss: 2.4856176376342773
Validation loss: 2.5871330153557563

Epoch: 5| Step: 5
Training loss: 2.8697242736816406
Validation loss: 2.5865005575200564

Epoch: 5| Step: 6
Training loss: 2.6394591331481934
Validation loss: 2.5893431863477154

Epoch: 5| Step: 7
Training loss: 2.76157808303833
Validation loss: 2.590005226032708

Epoch: 5| Step: 8
Training loss: 2.751013994216919
Validation loss: 2.592549031780612

Epoch: 5| Step: 9
Training loss: 3.496098279953003
Validation loss: 2.5979632998025544

Epoch: 5| Step: 10
Training loss: 3.867314100265503
Validation loss: 2.5987874025939615

Epoch: 86| Step: 0
Training loss: 2.1647253036499023
Validation loss: 2.607550226232057

Epoch: 5| Step: 1
Training loss: 3.1751723289489746
Validation loss: 2.604649661689676

Epoch: 5| Step: 2
Training loss: 3.4595184326171875
Validation loss: 2.5997145611752748

Epoch: 5| Step: 3
Training loss: 2.549107551574707
Validation loss: 2.5940899643846738

Epoch: 5| Step: 4
Training loss: 2.533243417739868
Validation loss: 2.5911026770068752

Epoch: 5| Step: 5
Training loss: 2.6526846885681152
Validation loss: 2.586501213812059

Epoch: 5| Step: 6
Training loss: 3.356726884841919
Validation loss: 2.5873003569982385

Epoch: 5| Step: 7
Training loss: 2.9187979698181152
Validation loss: 2.58704057047444

Epoch: 5| Step: 8
Training loss: 3.0941433906555176
Validation loss: 2.5859072387859388

Epoch: 5| Step: 9
Training loss: 2.096102714538574
Validation loss: 2.5856836085678427

Epoch: 5| Step: 10
Training loss: 2.326754093170166
Validation loss: 2.5832563446414087

Epoch: 87| Step: 0
Training loss: 3.1613337993621826
Validation loss: 2.583835783825126

Epoch: 5| Step: 1
Training loss: 2.662621021270752
Validation loss: 2.5793554026593446

Epoch: 5| Step: 2
Training loss: 2.6017398834228516
Validation loss: 2.579583937121976

Epoch: 5| Step: 3
Training loss: 2.442732572555542
Validation loss: 2.5838851877438125

Epoch: 5| Step: 4
Training loss: 3.095660448074341
Validation loss: 2.583160231190343

Epoch: 5| Step: 5
Training loss: 2.4924025535583496
Validation loss: 2.584640097874467

Epoch: 5| Step: 6
Training loss: 2.20686674118042
Validation loss: 2.580042459631479

Epoch: 5| Step: 7
Training loss: 3.5327022075653076
Validation loss: 2.5770029508939354

Epoch: 5| Step: 8
Training loss: 2.5412027835845947
Validation loss: 2.5801677139856483

Epoch: 5| Step: 9
Training loss: 2.668616533279419
Validation loss: 2.577570656294464

Epoch: 5| Step: 10
Training loss: 2.9355530738830566
Validation loss: 2.5757997241071475

Epoch: 88| Step: 0
Training loss: 2.423719882965088
Validation loss: 2.5808809700832573

Epoch: 5| Step: 1
Training loss: 2.7323782444000244
Validation loss: 2.5861543814341226

Epoch: 5| Step: 2
Training loss: 2.776313543319702
Validation loss: 2.588115430647327

Epoch: 5| Step: 3
Training loss: 2.88257098197937
Validation loss: 2.596208546751289

Epoch: 5| Step: 4
Training loss: 3.3414275646209717
Validation loss: 2.5864445906813427

Epoch: 5| Step: 5
Training loss: 2.076519250869751
Validation loss: 2.5873876771619244

Epoch: 5| Step: 6
Training loss: 3.249063014984131
Validation loss: 2.5827365498388968

Epoch: 5| Step: 7
Training loss: 2.4063034057617188
Validation loss: 2.58186678219867

Epoch: 5| Step: 8
Training loss: 2.925201892852783
Validation loss: 2.57997025469298

Epoch: 5| Step: 9
Training loss: 2.8834915161132812
Validation loss: 2.578197871485064

Epoch: 5| Step: 10
Training loss: 2.6448161602020264
Validation loss: 2.5744220159387075

Epoch: 89| Step: 0
Training loss: 3.364311695098877
Validation loss: 2.5740784650207846

Epoch: 5| Step: 1
Training loss: 3.3444831371307373
Validation loss: 2.574055225618424

Epoch: 5| Step: 2
Training loss: 2.5249812602996826
Validation loss: 2.572436732630576

Epoch: 5| Step: 3
Training loss: 2.9426867961883545
Validation loss: 2.5760846163636897

Epoch: 5| Step: 4
Training loss: 2.8327813148498535
Validation loss: 2.576291552153967

Epoch: 5| Step: 5
Training loss: 2.4703269004821777
Validation loss: 2.5762755845182683

Epoch: 5| Step: 6
Training loss: 2.2772603034973145
Validation loss: 2.5798369094889653

Epoch: 5| Step: 7
Training loss: 3.041374921798706
Validation loss: 2.5753682787700365

Epoch: 5| Step: 8
Training loss: 2.100914716720581
Validation loss: 2.5719367073428248

Epoch: 5| Step: 9
Training loss: 2.137941360473633
Validation loss: 2.578108979809669

Epoch: 5| Step: 10
Training loss: 3.258101224899292
Validation loss: 2.573035006882042

Epoch: 90| Step: 0
Training loss: 3.034273862838745
Validation loss: 2.582087298875214

Epoch: 5| Step: 1
Training loss: 3.1968369483947754
Validation loss: 2.580613587492256

Epoch: 5| Step: 2
Training loss: 2.949982166290283
Validation loss: 2.5789429090356313

Epoch: 5| Step: 3
Training loss: 2.790902853012085
Validation loss: 2.578300542728875

Epoch: 5| Step: 4
Training loss: 2.6813788414001465
Validation loss: 2.5743594502889984

Epoch: 5| Step: 5
Training loss: 3.160128593444824
Validation loss: 2.5774380801826395

Epoch: 5| Step: 6
Training loss: 2.463707447052002
Validation loss: 2.5888734222740255

Epoch: 5| Step: 7
Training loss: 2.7090976238250732
Validation loss: 2.584073866567304

Epoch: 5| Step: 8
Training loss: 1.975464105606079
Validation loss: 2.589525553487962

Epoch: 5| Step: 9
Training loss: 2.220986843109131
Validation loss: 2.5866237865981234

Epoch: 5| Step: 10
Training loss: 3.086345911026001
Validation loss: 2.5851084314366823

Epoch: 91| Step: 0
Training loss: 2.4322915077209473
Validation loss: 2.584986679015621

Epoch: 5| Step: 1
Training loss: 2.9402389526367188
Validation loss: 2.581091860289215

Epoch: 5| Step: 2
Training loss: 2.89030122756958
Validation loss: 2.589239722938948

Epoch: 5| Step: 3
Training loss: 2.575620174407959
Validation loss: 2.584882866951727

Epoch: 5| Step: 4
Training loss: 2.8562076091766357
Validation loss: 2.5728706441899782

Epoch: 5| Step: 5
Training loss: 2.323697328567505
Validation loss: 2.569423824228266

Epoch: 5| Step: 6
Training loss: 2.693859577178955
Validation loss: 2.571471155330699

Epoch: 5| Step: 7
Training loss: 2.6584067344665527
Validation loss: 2.573890547598562

Epoch: 5| Step: 8
Training loss: 3.0751891136169434
Validation loss: 2.5784100563295427

Epoch: 5| Step: 9
Training loss: 2.4612207412719727
Validation loss: 2.5840979878620436

Epoch: 5| Step: 10
Training loss: 3.395699977874756
Validation loss: 2.584342561742311

Epoch: 92| Step: 0
Training loss: 2.9986770153045654
Validation loss: 2.580422688555974

Epoch: 5| Step: 1
Training loss: 2.74432635307312
Validation loss: 2.573310985360094

Epoch: 5| Step: 2
Training loss: 2.650941848754883
Validation loss: 2.5761466564670688

Epoch: 5| Step: 3
Training loss: 2.2088124752044678
Validation loss: 2.571294223108599

Epoch: 5| Step: 4
Training loss: 2.3666417598724365
Validation loss: 2.5676999425375335

Epoch: 5| Step: 5
Training loss: 3.299490451812744
Validation loss: 2.568878996756769

Epoch: 5| Step: 6
Training loss: 2.9659626483917236
Validation loss: 2.571256737555227

Epoch: 5| Step: 7
Training loss: 3.094095468521118
Validation loss: 2.573990160419095

Epoch: 5| Step: 8
Training loss: 2.534437656402588
Validation loss: 2.5740878043636197

Epoch: 5| Step: 9
Training loss: 2.9297032356262207
Validation loss: 2.5670808387059036

Epoch: 5| Step: 10
Training loss: 2.3797054290771484
Validation loss: 2.567749943784488

Epoch: 93| Step: 0
Training loss: 3.2021193504333496
Validation loss: 2.570130378969254

Epoch: 5| Step: 1
Training loss: 2.6366539001464844
Validation loss: 2.5687313438743673

Epoch: 5| Step: 2
Training loss: 2.458221673965454
Validation loss: 2.5678883778151644

Epoch: 5| Step: 3
Training loss: 2.319958209991455
Validation loss: 2.5669928673774964

Epoch: 5| Step: 4
Training loss: 2.74345326423645
Validation loss: 2.570871158312726

Epoch: 5| Step: 5
Training loss: 2.8467624187469482
Validation loss: 2.5689998185762795

Epoch: 5| Step: 6
Training loss: 3.519430160522461
Validation loss: 2.5762957629337104

Epoch: 5| Step: 7
Training loss: 3.062119483947754
Validation loss: 2.5695720334206857

Epoch: 5| Step: 8
Training loss: 2.5101046562194824
Validation loss: 2.575921686746741

Epoch: 5| Step: 9
Training loss: 2.5756614208221436
Validation loss: 2.576383198461225

Epoch: 5| Step: 10
Training loss: 2.208554744720459
Validation loss: 2.578220654559392

Epoch: 94| Step: 0
Training loss: 2.2637739181518555
Validation loss: 2.591122199130315

Epoch: 5| Step: 1
Training loss: 2.909154176712036
Validation loss: 2.5930638620930333

Epoch: 5| Step: 2
Training loss: 2.7709031105041504
Validation loss: 2.5978070971786336

Epoch: 5| Step: 3
Training loss: 2.267944097518921
Validation loss: 2.5968814793453423

Epoch: 5| Step: 4
Training loss: 2.930541515350342
Validation loss: 2.597340815810747

Epoch: 5| Step: 5
Training loss: 3.0081546306610107
Validation loss: 2.590969867603753

Epoch: 5| Step: 6
Training loss: 3.0208113193511963
Validation loss: 2.5875007824231218

Epoch: 5| Step: 7
Training loss: 2.7548553943634033
Validation loss: 2.5770338068726244

Epoch: 5| Step: 8
Training loss: 2.9071452617645264
Validation loss: 2.5697786526013444

Epoch: 5| Step: 9
Training loss: 2.4015188217163086
Validation loss: 2.567945586737766

Epoch: 5| Step: 10
Training loss: 2.970137596130371
Validation loss: 2.5671147556715113

Epoch: 95| Step: 0
Training loss: 2.7155299186706543
Validation loss: 2.5649128139659925

Epoch: 5| Step: 1
Training loss: 1.3452138900756836
Validation loss: 2.5709477624585553

Epoch: 5| Step: 2
Training loss: 3.107391834259033
Validation loss: 2.567431962618264

Epoch: 5| Step: 3
Training loss: 3.451674222946167
Validation loss: 2.563643373468871

Epoch: 5| Step: 4
Training loss: 3.015805959701538
Validation loss: 2.562565370272565

Epoch: 5| Step: 5
Training loss: 1.8620977401733398
Validation loss: 2.562951716043616

Epoch: 5| Step: 6
Training loss: 2.793419361114502
Validation loss: 2.5637216003992225

Epoch: 5| Step: 7
Training loss: 2.7128405570983887
Validation loss: 2.5643778795837076

Epoch: 5| Step: 8
Training loss: 3.417924404144287
Validation loss: 2.5623917682196504

Epoch: 5| Step: 9
Training loss: 3.1409192085266113
Validation loss: 2.5673537459424747

Epoch: 5| Step: 10
Training loss: 2.5357508659362793
Validation loss: 2.5641960892626035

Epoch: 96| Step: 0
Training loss: 2.692451000213623
Validation loss: 2.5670298684027886

Epoch: 5| Step: 1
Training loss: 2.9323768615722656
Validation loss: 2.5898642129795526

Epoch: 5| Step: 2
Training loss: 2.311800241470337
Validation loss: 2.588908528768888

Epoch: 5| Step: 3
Training loss: 2.867741107940674
Validation loss: 2.585471499350763

Epoch: 5| Step: 4
Training loss: 3.0710651874542236
Validation loss: 2.583293555885233

Epoch: 5| Step: 5
Training loss: 2.300490140914917
Validation loss: 2.5738262232913764

Epoch: 5| Step: 6
Training loss: 2.896156072616577
Validation loss: 2.5705351547528337

Epoch: 5| Step: 7
Training loss: 2.4327099323272705
Validation loss: 2.565699500422324

Epoch: 5| Step: 8
Training loss: 2.441138744354248
Validation loss: 2.561958997480331

Epoch: 5| Step: 9
Training loss: 2.5058963298797607
Validation loss: 2.5695426797354095

Epoch: 5| Step: 10
Training loss: 3.8121047019958496
Validation loss: 2.567308428466961

Epoch: 97| Step: 0
Training loss: 2.4263272285461426
Validation loss: 2.5612198460486626

Epoch: 5| Step: 1
Training loss: 2.977328062057495
Validation loss: 2.5672037268197663

Epoch: 5| Step: 2
Training loss: 3.1790413856506348
Validation loss: 2.565363750662855

Epoch: 5| Step: 3
Training loss: 3.1489245891571045
Validation loss: 2.563018629627843

Epoch: 5| Step: 4
Training loss: 2.134643077850342
Validation loss: 2.5614171387046896

Epoch: 5| Step: 5
Training loss: 2.5153355598449707
Validation loss: 2.561365981255808

Epoch: 5| Step: 6
Training loss: 2.300960063934326
Validation loss: 2.5627795983386297

Epoch: 5| Step: 7
Training loss: 2.729198932647705
Validation loss: 2.567233867542718

Epoch: 5| Step: 8
Training loss: 3.229353427886963
Validation loss: 2.563712584075107

Epoch: 5| Step: 9
Training loss: 2.5354740619659424
Validation loss: 2.5603887957911335

Epoch: 5| Step: 10
Training loss: 2.853649377822876
Validation loss: 2.5612630459570114

Epoch: 98| Step: 0
Training loss: 2.7088160514831543
Validation loss: 2.559342325374644

Epoch: 5| Step: 1
Training loss: 2.953299045562744
Validation loss: 2.5594429969787598

Epoch: 5| Step: 2
Training loss: 2.805084466934204
Validation loss: 2.5612263730777207

Epoch: 5| Step: 3
Training loss: 3.1920785903930664
Validation loss: 2.557845356643841

Epoch: 5| Step: 4
Training loss: 2.4190993309020996
Validation loss: 2.5570576267857708

Epoch: 5| Step: 5
Training loss: 2.7607288360595703
Validation loss: 2.563959208867883

Epoch: 5| Step: 6
Training loss: 2.957613706588745
Validation loss: 2.5673343571283485

Epoch: 5| Step: 7
Training loss: 3.036416530609131
Validation loss: 2.58114170259045

Epoch: 5| Step: 8
Training loss: 2.023970127105713
Validation loss: 2.5774396978398806

Epoch: 5| Step: 9
Training loss: 2.8974037170410156
Validation loss: 2.573964790631366

Epoch: 5| Step: 10
Training loss: 2.2526638507843018
Validation loss: 2.5710678049313125

Epoch: 99| Step: 0
Training loss: 2.5304758548736572
Validation loss: 2.5665301097336637

Epoch: 5| Step: 1
Training loss: 3.0379745960235596
Validation loss: 2.563387401642338

Epoch: 5| Step: 2
Training loss: 3.234407901763916
Validation loss: 2.5632017607329995

Epoch: 5| Step: 3
Training loss: 3.003052234649658
Validation loss: 2.5626259260280158

Epoch: 5| Step: 4
Training loss: 2.6830859184265137
Validation loss: 2.5596299812357914

Epoch: 5| Step: 5
Training loss: 3.1216773986816406
Validation loss: 2.563016653060913

Epoch: 5| Step: 6
Training loss: 2.2976889610290527
Validation loss: 2.5548002848061184

Epoch: 5| Step: 7
Training loss: 2.1855998039245605
Validation loss: 2.5590324760765157

Epoch: 5| Step: 8
Training loss: 2.579052448272705
Validation loss: 2.554308094004149

Epoch: 5| Step: 9
Training loss: 1.7928524017333984
Validation loss: 2.5564681240307388

Epoch: 5| Step: 10
Training loss: 3.667074203491211
Validation loss: 2.555585874024258

Epoch: 100| Step: 0
Training loss: 2.9713361263275146
Validation loss: 2.5522290609216176

Epoch: 5| Step: 1
Training loss: 2.731698513031006
Validation loss: 2.5496630617367324

Epoch: 5| Step: 2
Training loss: 1.6779029369354248
Validation loss: 2.5536203153671755

Epoch: 5| Step: 3
Training loss: 2.7401187419891357
Validation loss: 2.548180749339442

Epoch: 5| Step: 4
Training loss: 3.7177481651306152
Validation loss: 2.55618098474318

Epoch: 5| Step: 5
Training loss: 2.2426252365112305
Validation loss: 2.555208508686353

Epoch: 5| Step: 6
Training loss: 3.5008342266082764
Validation loss: 2.5501179105492047

Epoch: 5| Step: 7
Training loss: 2.4278476238250732
Validation loss: 2.5502565599256948

Epoch: 5| Step: 8
Training loss: 2.5717079639434814
Validation loss: 2.548877026445122

Epoch: 5| Step: 9
Training loss: 2.507874011993408
Validation loss: 2.5489851966980965

Epoch: 5| Step: 10
Training loss: 2.9043679237365723
Validation loss: 2.552258717116489

Epoch: 101| Step: 0
Training loss: 2.619389057159424
Validation loss: 2.550305951026178

Epoch: 5| Step: 1
Training loss: 3.042919635772705
Validation loss: 2.5523006377681607

Epoch: 5| Step: 2
Training loss: 2.592588424682617
Validation loss: 2.550942805505568

Epoch: 5| Step: 3
Training loss: 3.2174477577209473
Validation loss: 2.5480937803945234

Epoch: 5| Step: 4
Training loss: 2.4742536544799805
Validation loss: 2.5504386886473625

Epoch: 5| Step: 5
Training loss: 2.6868510246276855
Validation loss: 2.5492664716577016

Epoch: 5| Step: 6
Training loss: 2.403465986251831
Validation loss: 2.546778517384683

Epoch: 5| Step: 7
Training loss: 2.6602213382720947
Validation loss: 2.548726189521051

Epoch: 5| Step: 8
Training loss: 2.901453733444214
Validation loss: 2.5507022975593485

Epoch: 5| Step: 9
Training loss: 2.740204334259033
Validation loss: 2.543440262476603

Epoch: 5| Step: 10
Training loss: 2.5749287605285645
Validation loss: 2.544714025271836

Epoch: 102| Step: 0
Training loss: 2.711503028869629
Validation loss: 2.550646448648104

Epoch: 5| Step: 1
Training loss: 2.136138439178467
Validation loss: 2.548246645158337

Epoch: 5| Step: 2
Training loss: 2.3165247440338135
Validation loss: 2.551239941709785

Epoch: 5| Step: 3
Training loss: 2.4903645515441895
Validation loss: 2.5504162106462704

Epoch: 5| Step: 4
Training loss: 2.4031410217285156
Validation loss: 2.551979834033597

Epoch: 5| Step: 5
Training loss: 2.991964340209961
Validation loss: 2.550681426960935

Epoch: 5| Step: 6
Training loss: 2.8711791038513184
Validation loss: 2.5469762971324306

Epoch: 5| Step: 7
Training loss: 3.2359282970428467
Validation loss: 2.546144479064531

Epoch: 5| Step: 8
Training loss: 2.9503746032714844
Validation loss: 2.5439647628415014

Epoch: 5| Step: 9
Training loss: 2.634321689605713
Validation loss: 2.5446886375386226

Epoch: 5| Step: 10
Training loss: 3.2449398040771484
Validation loss: 2.5426013238968386

Epoch: 103| Step: 0
Training loss: 2.927152156829834
Validation loss: 2.5448397308267574

Epoch: 5| Step: 1
Training loss: 2.888406276702881
Validation loss: 2.546579325070945

Epoch: 5| Step: 2
Training loss: 3.0129523277282715
Validation loss: 2.544299346144481

Epoch: 5| Step: 3
Training loss: 3.1241848468780518
Validation loss: 2.5376139404953166

Epoch: 5| Step: 4
Training loss: 2.04899263381958
Validation loss: 2.5425335899476083

Epoch: 5| Step: 5
Training loss: 2.8415956497192383
Validation loss: 2.542424166074363

Epoch: 5| Step: 6
Training loss: 2.5556845664978027
Validation loss: 2.5397740076946955

Epoch: 5| Step: 7
Training loss: 2.7846732139587402
Validation loss: 2.5426540067118983

Epoch: 5| Step: 8
Training loss: 2.186985731124878
Validation loss: 2.542101078135993

Epoch: 5| Step: 9
Training loss: 2.5907459259033203
Validation loss: 2.541477305914766

Epoch: 5| Step: 10
Training loss: 2.935779094696045
Validation loss: 2.5429518786809777

Epoch: 104| Step: 0
Training loss: 2.31077241897583
Validation loss: 2.543201267078359

Epoch: 5| Step: 1
Training loss: 2.6637539863586426
Validation loss: 2.547729322987218

Epoch: 5| Step: 2
Training loss: 2.4192264080047607
Validation loss: 2.5424029878390733

Epoch: 5| Step: 3
Training loss: 2.9864888191223145
Validation loss: 2.555043399974864

Epoch: 5| Step: 4
Training loss: 2.7585487365722656
Validation loss: 2.5556648392831125

Epoch: 5| Step: 5
Training loss: 2.9868099689483643
Validation loss: 2.5563069056439143

Epoch: 5| Step: 6
Training loss: 2.838721990585327
Validation loss: 2.557484826733989

Epoch: 5| Step: 7
Training loss: 2.738150119781494
Validation loss: 2.5532114608313448

Epoch: 5| Step: 8
Training loss: 2.7230420112609863
Validation loss: 2.545984350224977

Epoch: 5| Step: 9
Training loss: 2.2788379192352295
Validation loss: 2.5394529052959975

Epoch: 5| Step: 10
Training loss: 3.2462215423583984
Validation loss: 2.5426581546824467

Epoch: 105| Step: 0
Training loss: 3.0395007133483887
Validation loss: 2.546019523374496

Epoch: 5| Step: 1
Training loss: 2.8342666625976562
Validation loss: 2.542484996139362

Epoch: 5| Step: 2
Training loss: 3.3044235706329346
Validation loss: 2.5507064814208658

Epoch: 5| Step: 3
Training loss: 2.0273661613464355
Validation loss: 2.5439278310345066

Epoch: 5| Step: 4
Training loss: 1.9709421396255493
Validation loss: 2.5402928424137894

Epoch: 5| Step: 5
Training loss: 3.072390556335449
Validation loss: 2.5387632821195867

Epoch: 5| Step: 6
Training loss: 2.4507503509521484
Validation loss: 2.541332370491438

Epoch: 5| Step: 7
Training loss: 2.7471165657043457
Validation loss: 2.5418292989013014

Epoch: 5| Step: 8
Training loss: 2.5758893489837646
Validation loss: 2.54222967291391

Epoch: 5| Step: 9
Training loss: 2.594599723815918
Validation loss: 2.5459276296759166

Epoch: 5| Step: 10
Training loss: 3.4133987426757812
Validation loss: 2.55309691480411

Epoch: 106| Step: 0
Training loss: 3.227912425994873
Validation loss: 2.5491297168116414

Epoch: 5| Step: 1
Training loss: 2.477404832839966
Validation loss: 2.548634070222096

Epoch: 5| Step: 2
Training loss: 2.536818027496338
Validation loss: 2.544300135745797

Epoch: 5| Step: 3
Training loss: 2.714725971221924
Validation loss: 2.547065293917092

Epoch: 5| Step: 4
Training loss: 2.618527889251709
Validation loss: 2.5474233255591443

Epoch: 5| Step: 5
Training loss: 2.870321273803711
Validation loss: 2.54565675797001

Epoch: 5| Step: 6
Training loss: 2.2694644927978516
Validation loss: 2.545645659969699

Epoch: 5| Step: 7
Training loss: 2.377654552459717
Validation loss: 2.5382561529836347

Epoch: 5| Step: 8
Training loss: 3.4684436321258545
Validation loss: 2.537399694483767

Epoch: 5| Step: 9
Training loss: 2.582712173461914
Validation loss: 2.536339977736114

Epoch: 5| Step: 10
Training loss: 2.6713030338287354
Validation loss: 2.5372456478816208

Epoch: 107| Step: 0
Training loss: 2.8469185829162598
Validation loss: 2.5347658126584944

Epoch: 5| Step: 1
Training loss: 2.2880067825317383
Validation loss: 2.5319037796348653

Epoch: 5| Step: 2
Training loss: 2.8097071647644043
Validation loss: 2.535760553934241

Epoch: 5| Step: 3
Training loss: 3.460451602935791
Validation loss: 2.535114247311828

Epoch: 5| Step: 4
Training loss: 2.798248291015625
Validation loss: 2.5386664508491434

Epoch: 5| Step: 5
Training loss: 2.131570339202881
Validation loss: 2.5382395611014417

Epoch: 5| Step: 6
Training loss: 2.8226683139801025
Validation loss: 2.547620955333915

Epoch: 5| Step: 7
Training loss: 2.914976119995117
Validation loss: 2.556227566093527

Epoch: 5| Step: 8
Training loss: 2.6104798316955566
Validation loss: 2.547156500559981

Epoch: 5| Step: 9
Training loss: 1.9923884868621826
Validation loss: 2.548745204043645

Epoch: 5| Step: 10
Training loss: 3.2922682762145996
Validation loss: 2.564137363946566

Epoch: 108| Step: 0
Training loss: 2.358783721923828
Validation loss: 2.585616784711038

Epoch: 5| Step: 1
Training loss: 2.5789287090301514
Validation loss: 2.5628247901957524

Epoch: 5| Step: 2
Training loss: 2.2715816497802734
Validation loss: 2.5517354831900647

Epoch: 5| Step: 3
Training loss: 2.275951862335205
Validation loss: 2.5378675640270276

Epoch: 5| Step: 4
Training loss: 2.798478603363037
Validation loss: 2.5325333405566472

Epoch: 5| Step: 5
Training loss: 2.525310754776001
Validation loss: 2.5301872966110066

Epoch: 5| Step: 6
Training loss: 3.3431217670440674
Validation loss: 2.530815762858237

Epoch: 5| Step: 7
Training loss: 3.2456023693084717
Validation loss: 2.5321456591288247

Epoch: 5| Step: 8
Training loss: 2.463890552520752
Validation loss: 2.532884613160164

Epoch: 5| Step: 9
Training loss: 3.0038294792175293
Validation loss: 2.537772819560061

Epoch: 5| Step: 10
Training loss: 3.1032469272613525
Validation loss: 2.539150381600985

Epoch: 109| Step: 0
Training loss: 2.250845193862915
Validation loss: 2.5391760282619025

Epoch: 5| Step: 1
Training loss: 2.5093495845794678
Validation loss: 2.533790370469452

Epoch: 5| Step: 2
Training loss: 3.466082811355591
Validation loss: 2.534929206294398

Epoch: 5| Step: 3
Training loss: 3.462507963180542
Validation loss: 2.5314494127868326

Epoch: 5| Step: 4
Training loss: 2.579875946044922
Validation loss: 2.5327823469715733

Epoch: 5| Step: 5
Training loss: 2.261490821838379
Validation loss: 2.5279687707142164

Epoch: 5| Step: 6
Training loss: 2.5625100135803223
Validation loss: 2.524964855563256

Epoch: 5| Step: 7
Training loss: 3.0222320556640625
Validation loss: 2.525518289176367

Epoch: 5| Step: 8
Training loss: 2.3200762271881104
Validation loss: 2.525851524004372

Epoch: 5| Step: 9
Training loss: 2.5607728958129883
Validation loss: 2.525333766014345

Epoch: 5| Step: 10
Training loss: 2.8856427669525146
Validation loss: 2.5224929009714434

Epoch: 110| Step: 0
Training loss: 2.648198366165161
Validation loss: 2.5271321829929145

Epoch: 5| Step: 1
Training loss: 2.287886142730713
Validation loss: 2.5241654867767007

Epoch: 5| Step: 2
Training loss: 2.5353140830993652
Validation loss: 2.5289480737460557

Epoch: 5| Step: 3
Training loss: 3.3201498985290527
Validation loss: 2.5250557314965034

Epoch: 5| Step: 4
Training loss: 2.224360466003418
Validation loss: 2.5271952793162358

Epoch: 5| Step: 5
Training loss: 2.811724901199341
Validation loss: 2.525429315464471

Epoch: 5| Step: 6
Training loss: 2.643207550048828
Validation loss: 2.5271619160970054

Epoch: 5| Step: 7
Training loss: 2.2231621742248535
Validation loss: 2.52998754542361

Epoch: 5| Step: 8
Training loss: 3.122279644012451
Validation loss: 2.5273033111326155

Epoch: 5| Step: 9
Training loss: 3.0606679916381836
Validation loss: 2.5277683196529264

Epoch: 5| Step: 10
Training loss: 2.953404664993286
Validation loss: 2.5282009545192925

Epoch: 111| Step: 0
Training loss: 2.180840253829956
Validation loss: 2.526240651325513

Epoch: 5| Step: 1
Training loss: 2.9009199142456055
Validation loss: 2.5332659290682886

Epoch: 5| Step: 2
Training loss: 2.3060386180877686
Validation loss: 2.54471012853807

Epoch: 5| Step: 3
Training loss: 2.9296557903289795
Validation loss: 2.534193556795838

Epoch: 5| Step: 4
Training loss: 2.8547170162200928
Validation loss: 2.533811806350626

Epoch: 5| Step: 5
Training loss: 2.8821091651916504
Validation loss: 2.528102728628343

Epoch: 5| Step: 6
Training loss: 2.9494471549987793
Validation loss: 2.5267498031739266

Epoch: 5| Step: 7
Training loss: 2.2709717750549316
Validation loss: 2.5149262156537784

Epoch: 5| Step: 8
Training loss: 3.086710214614868
Validation loss: 2.518948401174238

Epoch: 5| Step: 9
Training loss: 2.4553308486938477
Validation loss: 2.516280369092059

Epoch: 5| Step: 10
Training loss: 2.934131622314453
Validation loss: 2.5126908158743255

Epoch: 112| Step: 0
Training loss: 2.383645534515381
Validation loss: 2.513835771109468

Epoch: 5| Step: 1
Training loss: 2.400001049041748
Validation loss: 2.5120975791767077

Epoch: 5| Step: 2
Training loss: 2.9781525135040283
Validation loss: 2.509874008035147

Epoch: 5| Step: 3
Training loss: 2.979191541671753
Validation loss: 2.5093989474799043

Epoch: 5| Step: 4
Training loss: 3.0720160007476807
Validation loss: 2.5097385862822175

Epoch: 5| Step: 5
Training loss: 2.554608106613159
Validation loss: 2.5128530738174275

Epoch: 5| Step: 6
Training loss: 2.6207687854766846
Validation loss: 2.5135846702001428

Epoch: 5| Step: 7
Training loss: 3.0054259300231934
Validation loss: 2.5107631785895235

Epoch: 5| Step: 8
Training loss: 2.464451313018799
Validation loss: 2.506217587378717

Epoch: 5| Step: 9
Training loss: 2.8775155544281006
Validation loss: 2.5073590227352676

Epoch: 5| Step: 10
Training loss: 2.284733295440674
Validation loss: 2.5065856954102874

Epoch: 113| Step: 0
Training loss: 2.0253548622131348
Validation loss: 2.510574192129156

Epoch: 5| Step: 1
Training loss: 2.4327571392059326
Validation loss: 2.505752601931172

Epoch: 5| Step: 2
Training loss: 2.8532071113586426
Validation loss: 2.512180171987062

Epoch: 5| Step: 3
Training loss: 2.429104804992676
Validation loss: 2.508768614902291

Epoch: 5| Step: 4
Training loss: 2.307769298553467
Validation loss: 2.511311246502784

Epoch: 5| Step: 5
Training loss: 2.911432981491089
Validation loss: 2.50784376359755

Epoch: 5| Step: 6
Training loss: 3.4073493480682373
Validation loss: 2.5152758475272887

Epoch: 5| Step: 7
Training loss: 2.263763666152954
Validation loss: 2.5183679134615007

Epoch: 5| Step: 8
Training loss: 3.2591114044189453
Validation loss: 2.5168922280752533

Epoch: 5| Step: 9
Training loss: 3.010471820831299
Validation loss: 2.5128332184207056

Epoch: 5| Step: 10
Training loss: 2.7472105026245117
Validation loss: 2.5175277135705434

Epoch: 114| Step: 0
Training loss: 2.8353869915008545
Validation loss: 2.5054982733982865

Epoch: 5| Step: 1
Training loss: 2.9770689010620117
Validation loss: 2.497535208220123

Epoch: 5| Step: 2
Training loss: 2.3227806091308594
Validation loss: 2.5017177443350516

Epoch: 5| Step: 3
Training loss: 3.6936612129211426
Validation loss: 2.500784335597869

Epoch: 5| Step: 4
Training loss: 2.424163818359375
Validation loss: 2.5063861416232203

Epoch: 5| Step: 5
Training loss: 2.710409164428711
Validation loss: 2.502495781067879

Epoch: 5| Step: 6
Training loss: 2.340456962585449
Validation loss: 2.4976662922930974

Epoch: 5| Step: 7
Training loss: 2.6664206981658936
Validation loss: 2.498825829516175

Epoch: 5| Step: 8
Training loss: 2.146890163421631
Validation loss: 2.5011949231547694

Epoch: 5| Step: 9
Training loss: 2.6738228797912598
Validation loss: 2.5001817595574165

Epoch: 5| Step: 10
Training loss: 2.8430891036987305
Validation loss: 2.5040690924531672

Epoch: 115| Step: 0
Training loss: 3.0943233966827393
Validation loss: 2.507566213607788

Epoch: 5| Step: 1
Training loss: 2.352985382080078
Validation loss: 2.5086134261982416

Epoch: 5| Step: 2
Training loss: 2.614259719848633
Validation loss: 2.517754888021818

Epoch: 5| Step: 3
Training loss: 3.1293892860412598
Validation loss: 2.510489971406998

Epoch: 5| Step: 4
Training loss: 2.805133104324341
Validation loss: 2.5067426235445085

Epoch: 5| Step: 5
Training loss: 2.0603790283203125
Validation loss: 2.502515330109545

Epoch: 5| Step: 6
Training loss: 2.582578420639038
Validation loss: 2.4986703472752727

Epoch: 5| Step: 7
Training loss: 3.0519423484802246
Validation loss: 2.5021158520893385

Epoch: 5| Step: 8
Training loss: 2.7203762531280518
Validation loss: 2.4995993362959994

Epoch: 5| Step: 9
Training loss: 2.441328525543213
Validation loss: 2.496532013339381

Epoch: 5| Step: 10
Training loss: 2.7724106311798096
Validation loss: 2.4992981854305474

Epoch: 116| Step: 0
Training loss: 2.0495216846466064
Validation loss: 2.4999719691532913

Epoch: 5| Step: 1
Training loss: 2.9264934062957764
Validation loss: 2.5090994322171776

Epoch: 5| Step: 2
Training loss: 2.899801254272461
Validation loss: 2.5086271019392115

Epoch: 5| Step: 3
Training loss: 3.058403730392456
Validation loss: 2.5021089533323884

Epoch: 5| Step: 4
Training loss: 2.602388381958008
Validation loss: 2.5002678184099096

Epoch: 5| Step: 5
Training loss: 2.3027186393737793
Validation loss: 2.491470903478643

Epoch: 5| Step: 6
Training loss: 2.628380298614502
Validation loss: 2.4913495843128493

Epoch: 5| Step: 7
Training loss: 2.9865450859069824
Validation loss: 2.496620665314377

Epoch: 5| Step: 8
Training loss: 2.6010050773620605
Validation loss: 2.49523284358363

Epoch: 5| Step: 9
Training loss: 2.2012877464294434
Validation loss: 2.490868801711708

Epoch: 5| Step: 10
Training loss: 3.423093557357788
Validation loss: 2.4943202592993297

Epoch: 117| Step: 0
Training loss: 3.133941173553467
Validation loss: 2.4943077910330986

Epoch: 5| Step: 1
Training loss: 3.51025128364563
Validation loss: 2.497321467245779

Epoch: 5| Step: 2
Training loss: 2.0622687339782715
Validation loss: 2.4974137121631252

Epoch: 5| Step: 3
Training loss: 3.0423076152801514
Validation loss: 2.490617252165271

Epoch: 5| Step: 4
Training loss: 2.297497272491455
Validation loss: 2.4937835457504436

Epoch: 5| Step: 5
Training loss: 2.681709051132202
Validation loss: 2.5009684562683105

Epoch: 5| Step: 6
Training loss: 1.7779548168182373
Validation loss: 2.495574353843607

Epoch: 5| Step: 7
Training loss: 2.8301689624786377
Validation loss: 2.492982695179601

Epoch: 5| Step: 8
Training loss: 2.84795880317688
Validation loss: 2.4934400717417398

Epoch: 5| Step: 9
Training loss: 2.628077268600464
Validation loss: 2.495397247293944

Epoch: 5| Step: 10
Training loss: 2.731367588043213
Validation loss: 2.492299641332319

Epoch: 118| Step: 0
Training loss: 2.1375293731689453
Validation loss: 2.4942671175925963

Epoch: 5| Step: 1
Training loss: 2.7157225608825684
Validation loss: 2.495153181014522

Epoch: 5| Step: 2
Training loss: 2.3873343467712402
Validation loss: 2.4910688964269494

Epoch: 5| Step: 3
Training loss: 2.9022364616394043
Validation loss: 2.491429169972738

Epoch: 5| Step: 4
Training loss: 2.3837459087371826
Validation loss: 2.4920036228754188

Epoch: 5| Step: 5
Training loss: 2.4670605659484863
Validation loss: 2.4918042716159614

Epoch: 5| Step: 6
Training loss: 2.5351240634918213
Validation loss: 2.4934459783697642

Epoch: 5| Step: 7
Training loss: 3.7928078174591064
Validation loss: 2.5038469555557414

Epoch: 5| Step: 8
Training loss: 2.3972275257110596
Validation loss: 2.5057008856086322

Epoch: 5| Step: 9
Training loss: 2.236801862716675
Validation loss: 2.5144873844679965

Epoch: 5| Step: 10
Training loss: 3.740072727203369
Validation loss: 2.5115945531475927

Epoch: 119| Step: 0
Training loss: 2.6395585536956787
Validation loss: 2.4998515728981263

Epoch: 5| Step: 1
Training loss: 2.59713077545166
Validation loss: 2.496396621068319

Epoch: 5| Step: 2
Training loss: 2.7453858852386475
Validation loss: 2.492474135532174

Epoch: 5| Step: 3
Training loss: 2.5700597763061523
Validation loss: 2.4886889842248734

Epoch: 5| Step: 4
Training loss: 2.8240253925323486
Validation loss: 2.4920711081515075

Epoch: 5| Step: 5
Training loss: 2.434171199798584
Validation loss: 2.4913878722857405

Epoch: 5| Step: 6
Training loss: 2.671718120574951
Validation loss: 2.4946551938210764

Epoch: 5| Step: 7
Training loss: 2.9639880657196045
Validation loss: 2.4915079147584978

Epoch: 5| Step: 8
Training loss: 1.9339063167572021
Validation loss: 2.492314710411974

Epoch: 5| Step: 9
Training loss: 3.353382110595703
Validation loss: 2.4925786397790395

Epoch: 5| Step: 10
Training loss: 2.8845043182373047
Validation loss: 2.495546879306916

Epoch: 120| Step: 0
Training loss: 2.6843440532684326
Validation loss: 2.4930463170492523

Epoch: 5| Step: 1
Training loss: 2.8375637531280518
Validation loss: 2.4953780661347094

Epoch: 5| Step: 2
Training loss: 3.0108675956726074
Validation loss: 2.4979380100004134

Epoch: 5| Step: 3
Training loss: 2.5166046619415283
Validation loss: 2.4993606254618657

Epoch: 5| Step: 4
Training loss: 2.8571410179138184
Validation loss: 2.4936678653122275

Epoch: 5| Step: 5
Training loss: 2.459603786468506
Validation loss: 2.4911201410396124

Epoch: 5| Step: 6
Training loss: 2.6064085960388184
Validation loss: 2.488015897812382

Epoch: 5| Step: 7
Training loss: 3.2452101707458496
Validation loss: 2.489315120122766

Epoch: 5| Step: 8
Training loss: 3.0064265727996826
Validation loss: 2.487824709184708

Epoch: 5| Step: 9
Training loss: 2.1635525226593018
Validation loss: 2.4872927332437165

Epoch: 5| Step: 10
Training loss: 2.0222525596618652
Validation loss: 2.49069542782281

Epoch: 121| Step: 0
Training loss: 3.1612062454223633
Validation loss: 2.483922989137711

Epoch: 5| Step: 1
Training loss: 2.690103530883789
Validation loss: 2.4853565769810833

Epoch: 5| Step: 2
Training loss: 2.866490602493286
Validation loss: 2.4881388730900262

Epoch: 5| Step: 3
Training loss: 2.446164846420288
Validation loss: 2.48516171978366

Epoch: 5| Step: 4
Training loss: 2.53116774559021
Validation loss: 2.483822153460595

Epoch: 5| Step: 5
Training loss: 2.3728106021881104
Validation loss: 2.489401340484619

Epoch: 5| Step: 6
Training loss: 2.7909016609191895
Validation loss: 2.488752060039069

Epoch: 5| Step: 7
Training loss: 2.3332581520080566
Validation loss: 2.490727686112927

Epoch: 5| Step: 8
Training loss: 2.3926851749420166
Validation loss: 2.4898279072136007

Epoch: 5| Step: 9
Training loss: 3.165947914123535
Validation loss: 2.4926071474629063

Epoch: 5| Step: 10
Training loss: 2.7280876636505127
Validation loss: 2.4921100037072295

Epoch: 122| Step: 0
Training loss: 2.7910892963409424
Validation loss: 2.4855498780486402

Epoch: 5| Step: 1
Training loss: 2.618271589279175
Validation loss: 2.4897858917072253

Epoch: 5| Step: 2
Training loss: 2.6800267696380615
Validation loss: 2.4923758199138026

Epoch: 5| Step: 3
Training loss: 2.7635350227355957
Validation loss: 2.502288410740514

Epoch: 5| Step: 4
Training loss: 2.81095290184021
Validation loss: 2.499971805080291

Epoch: 5| Step: 5
Training loss: 3.165841579437256
Validation loss: 2.5022719854949624

Epoch: 5| Step: 6
Training loss: 2.914289951324463
Validation loss: 2.508284271404307

Epoch: 5| Step: 7
Training loss: 2.491969585418701
Validation loss: 2.508801885830459

Epoch: 5| Step: 8
Training loss: 2.1761245727539062
Validation loss: 2.4951072687743814

Epoch: 5| Step: 9
Training loss: 2.9884519577026367
Validation loss: 2.4909571652771323

Epoch: 5| Step: 10
Training loss: 1.92576265335083
Validation loss: 2.4903641798162974

Epoch: 123| Step: 0
Training loss: 2.5605149269104004
Validation loss: 2.4878880182902017

Epoch: 5| Step: 1
Training loss: 2.6197619438171387
Validation loss: 2.491632507693383

Epoch: 5| Step: 2
Training loss: 2.194037914276123
Validation loss: 2.493645542411394

Epoch: 5| Step: 3
Training loss: 2.86735463142395
Validation loss: 2.4966163994163595

Epoch: 5| Step: 4
Training loss: 3.364596128463745
Validation loss: 2.496614561286024

Epoch: 5| Step: 5
Training loss: 2.8007171154022217
Validation loss: 2.503549652714883

Epoch: 5| Step: 6
Training loss: 3.060579538345337
Validation loss: 2.5000490475726385

Epoch: 5| Step: 7
Training loss: 2.6071457862854004
Validation loss: 2.50046532897539

Epoch: 5| Step: 8
Training loss: 2.6759698390960693
Validation loss: 2.5051257661593858

Epoch: 5| Step: 9
Training loss: 2.5871880054473877
Validation loss: 2.4988115192741476

Epoch: 5| Step: 10
Training loss: 2.108903408050537
Validation loss: 2.502055985953218

Epoch: 124| Step: 0
Training loss: 3.5415279865264893
Validation loss: 2.504681764110442

Epoch: 5| Step: 1
Training loss: 2.502333641052246
Validation loss: 2.5105733768914336

Epoch: 5| Step: 2
Training loss: 2.207056760787964
Validation loss: 2.50109294409393

Epoch: 5| Step: 3
Training loss: 2.757201910018921
Validation loss: 2.5040637011169107

Epoch: 5| Step: 4
Training loss: 2.518435001373291
Validation loss: 2.506991186449605

Epoch: 5| Step: 5
Training loss: 2.436981678009033
Validation loss: 2.4950836704623316

Epoch: 5| Step: 6
Training loss: 3.1579105854034424
Validation loss: 2.5046265638002785

Epoch: 5| Step: 7
Training loss: 2.7551705837249756
Validation loss: 2.4883277928957375

Epoch: 5| Step: 8
Training loss: 2.205979824066162
Validation loss: 2.4877758769578833

Epoch: 5| Step: 9
Training loss: 2.7121243476867676
Validation loss: 2.486614458022579

Epoch: 5| Step: 10
Training loss: 2.7294259071350098
Validation loss: 2.480105628249466

Epoch: 125| Step: 0
Training loss: 3.0461928844451904
Validation loss: 2.490149736404419

Epoch: 5| Step: 1
Training loss: 2.4137580394744873
Validation loss: 2.4915197510873117

Epoch: 5| Step: 2
Training loss: 3.2013981342315674
Validation loss: 2.4885561081670944

Epoch: 5| Step: 3
Training loss: 2.782331943511963
Validation loss: 2.486390285594489

Epoch: 5| Step: 4
Training loss: 2.477792739868164
Validation loss: 2.4824845970317884

Epoch: 5| Step: 5
Training loss: 2.9889895915985107
Validation loss: 2.4794683815330587

Epoch: 5| Step: 6
Training loss: 2.4484152793884277
Validation loss: 2.476981693698514

Epoch: 5| Step: 7
Training loss: 2.883427858352661
Validation loss: 2.4851233472106276

Epoch: 5| Step: 8
Training loss: 2.571671962738037
Validation loss: 2.4827948154941684

Epoch: 5| Step: 9
Training loss: 2.2729082107543945
Validation loss: 2.4822659466856267

Epoch: 5| Step: 10
Training loss: 2.2742762565612793
Validation loss: 2.480777584096437

Epoch: 126| Step: 0
Training loss: 2.636242389678955
Validation loss: 2.4858988151755383

Epoch: 5| Step: 1
Training loss: 2.916344165802002
Validation loss: 2.4895745810642036

Epoch: 5| Step: 2
Training loss: 2.8666417598724365
Validation loss: 2.497429152970673

Epoch: 5| Step: 3
Training loss: 2.151742458343506
Validation loss: 2.4911794457384335

Epoch: 5| Step: 4
Training loss: 2.7075557708740234
Validation loss: 2.4845703878710346

Epoch: 5| Step: 5
Training loss: 2.0889813899993896
Validation loss: 2.480888298762742

Epoch: 5| Step: 6
Training loss: 2.0947515964508057
Validation loss: 2.4869918695060154

Epoch: 5| Step: 7
Training loss: 3.0143814086914062
Validation loss: 2.481409401021978

Epoch: 5| Step: 8
Training loss: 3.206982135772705
Validation loss: 2.4841951401002946

Epoch: 5| Step: 9
Training loss: 2.8305916786193848
Validation loss: 2.4888930910377094

Epoch: 5| Step: 10
Training loss: 3.0308496952056885
Validation loss: 2.4902162526243474

Epoch: 127| Step: 0
Training loss: 2.4347634315490723
Validation loss: 2.4905948844007266

Epoch: 5| Step: 1
Training loss: 2.5024025440216064
Validation loss: 2.4962487348946194

Epoch: 5| Step: 2
Training loss: 2.302504062652588
Validation loss: 2.487381927428707

Epoch: 5| Step: 3
Training loss: 3.078512668609619
Validation loss: 2.4864359260887228

Epoch: 5| Step: 4
Training loss: 2.5448946952819824
Validation loss: 2.485372566407727

Epoch: 5| Step: 5
Training loss: 2.6110777854919434
Validation loss: 2.49083782780555

Epoch: 5| Step: 6
Training loss: 3.8904693126678467
Validation loss: 2.4983753542746268

Epoch: 5| Step: 7
Training loss: 2.433485746383667
Validation loss: 2.491534609948435

Epoch: 5| Step: 8
Training loss: 2.714346170425415
Validation loss: 2.4880280135780253

Epoch: 5| Step: 9
Training loss: 2.370779037475586
Validation loss: 2.4891237751130135

Epoch: 5| Step: 10
Training loss: 2.4839396476745605
Validation loss: 2.481861447775236

Epoch: 128| Step: 0
Training loss: 2.7564620971679688
Validation loss: 2.4748483011799474

Epoch: 5| Step: 1
Training loss: 1.9251129627227783
Validation loss: 2.4770290774683796

Epoch: 5| Step: 2
Training loss: 2.7843832969665527
Validation loss: 2.4853419103930072

Epoch: 5| Step: 3
Training loss: 2.6214053630828857
Validation loss: 2.4776981825469644

Epoch: 5| Step: 4
Training loss: 2.547757625579834
Validation loss: 2.4741786141549387

Epoch: 5| Step: 5
Training loss: 2.9748523235321045
Validation loss: 2.4763851537499377

Epoch: 5| Step: 6
Training loss: 2.6348628997802734
Validation loss: 2.4782198821344683

Epoch: 5| Step: 7
Training loss: 2.785508155822754
Validation loss: 2.4774475033565233

Epoch: 5| Step: 8
Training loss: 2.8107004165649414
Validation loss: 2.4763703012979157

Epoch: 5| Step: 9
Training loss: 2.559880256652832
Validation loss: 2.4767001598112044

Epoch: 5| Step: 10
Training loss: 3.0624704360961914
Validation loss: 2.488498931290001

Epoch: 129| Step: 0
Training loss: 2.203446865081787
Validation loss: 2.486937253705917

Epoch: 5| Step: 1
Training loss: 2.0920486450195312
Validation loss: 2.5072033969304894

Epoch: 5| Step: 2
Training loss: 2.868865489959717
Validation loss: 2.507307121830602

Epoch: 5| Step: 3
Training loss: 2.407166004180908
Validation loss: 2.5245311978042766

Epoch: 5| Step: 4
Training loss: 2.7931041717529297
Validation loss: 2.5217576552462835

Epoch: 5| Step: 5
Training loss: 3.390549898147583
Validation loss: 2.517588548762824

Epoch: 5| Step: 6
Training loss: 2.748502254486084
Validation loss: 2.5137839881322717

Epoch: 5| Step: 7
Training loss: 3.193857431411743
Validation loss: 2.495858692353772

Epoch: 5| Step: 8
Training loss: 2.227403163909912
Validation loss: 2.4845822498362553

Epoch: 5| Step: 9
Training loss: 2.7581868171691895
Validation loss: 2.4775901071486937

Epoch: 5| Step: 10
Training loss: 2.9044153690338135
Validation loss: 2.4773495325478176

Epoch: 130| Step: 0
Training loss: 2.4231839179992676
Validation loss: 2.4796259864684074

Epoch: 5| Step: 1
Training loss: 2.5482192039489746
Validation loss: 2.494766814734346

Epoch: 5| Step: 2
Training loss: 2.519815444946289
Validation loss: 2.5042110002169045

Epoch: 5| Step: 3
Training loss: 2.666137218475342
Validation loss: 2.5115716021548034

Epoch: 5| Step: 4
Training loss: 2.915010929107666
Validation loss: 2.523770040081393

Epoch: 5| Step: 5
Training loss: 3.13824462890625
Validation loss: 2.519189198811849

Epoch: 5| Step: 6
Training loss: 2.5077595710754395
Validation loss: 2.519692995214975

Epoch: 5| Step: 7
Training loss: 2.9950931072235107
Validation loss: 2.4993776249629196

Epoch: 5| Step: 8
Training loss: 2.411770820617676
Validation loss: 2.4897848995782996

Epoch: 5| Step: 9
Training loss: 3.0342936515808105
Validation loss: 2.482272717260545

Epoch: 5| Step: 10
Training loss: 2.4833974838256836
Validation loss: 2.476034854048042

Epoch: 131| Step: 0
Training loss: 2.983428478240967
Validation loss: 2.4735227118256273

Epoch: 5| Step: 1
Training loss: 2.311488628387451
Validation loss: 2.473548745596281

Epoch: 5| Step: 2
Training loss: 1.8245105743408203
Validation loss: 2.4903004425828175

Epoch: 5| Step: 3
Training loss: 2.489349365234375
Validation loss: 2.5046700457090973

Epoch: 5| Step: 4
Training loss: 2.756493091583252
Validation loss: 2.5109145077325965

Epoch: 5| Step: 5
Training loss: 2.8085737228393555
Validation loss: 2.506069988332769

Epoch: 5| Step: 6
Training loss: 3.411597728729248
Validation loss: 2.510696231677968

Epoch: 5| Step: 7
Training loss: 3.1642463207244873
Validation loss: 2.5024063125733407

Epoch: 5| Step: 8
Training loss: 2.420226573944092
Validation loss: 2.487299921692059

Epoch: 5| Step: 9
Training loss: 3.3169105052948
Validation loss: 2.479738812292776

Epoch: 5| Step: 10
Training loss: 1.9740694761276245
Validation loss: 2.4809582464156614

Epoch: 132| Step: 0
Training loss: 2.636885166168213
Validation loss: 2.4795140707364647

Epoch: 5| Step: 1
Training loss: 2.242382764816284
Validation loss: 2.474874737442181

Epoch: 5| Step: 2
Training loss: 2.507556915283203
Validation loss: 2.4786478396384948

Epoch: 5| Step: 3
Training loss: 2.1862576007843018
Validation loss: 2.475569443036151

Epoch: 5| Step: 4
Training loss: 3.090934991836548
Validation loss: 2.478285397252729

Epoch: 5| Step: 5
Training loss: 2.9845986366271973
Validation loss: 2.4816652831210884

Epoch: 5| Step: 6
Training loss: 3.3342742919921875
Validation loss: 2.4826592835046912

Epoch: 5| Step: 7
Training loss: 2.193634033203125
Validation loss: 2.4854057322266283

Epoch: 5| Step: 8
Training loss: 2.952855110168457
Validation loss: 2.485621354913199

Epoch: 5| Step: 9
Training loss: 2.763136148452759
Validation loss: 2.483841580729331

Epoch: 5| Step: 10
Training loss: 2.562911033630371
Validation loss: 2.479876607976934

Epoch: 133| Step: 0
Training loss: 2.582435131072998
Validation loss: 2.480603861552413

Epoch: 5| Step: 1
Training loss: 3.2786991596221924
Validation loss: 2.4803679502138527

Epoch: 5| Step: 2
Training loss: 3.0313351154327393
Validation loss: 2.471909115391393

Epoch: 5| Step: 3
Training loss: 3.0726215839385986
Validation loss: 2.4717898548290296

Epoch: 5| Step: 4
Training loss: 2.4435977935791016
Validation loss: 2.4711865430237143

Epoch: 5| Step: 5
Training loss: 2.4767298698425293
Validation loss: 2.47424683519589

Epoch: 5| Step: 6
Training loss: 2.639418840408325
Validation loss: 2.4772409085304505

Epoch: 5| Step: 7
Training loss: 2.5352203845977783
Validation loss: 2.485403322404431

Epoch: 5| Step: 8
Training loss: 2.435446262359619
Validation loss: 2.477258279759397

Epoch: 5| Step: 9
Training loss: 2.9186177253723145
Validation loss: 2.485198977173016

Epoch: 5| Step: 10
Training loss: 1.8600969314575195
Validation loss: 2.4833593137802614

Epoch: 134| Step: 0
Training loss: 3.1687066555023193
Validation loss: 2.4711310299493934

Epoch: 5| Step: 1
Training loss: 3.0506398677825928
Validation loss: 2.464746872584025

Epoch: 5| Step: 2
Training loss: 2.5664823055267334
Validation loss: 2.4612633925612255

Epoch: 5| Step: 3
Training loss: 2.1040444374084473
Validation loss: 2.466047607442384

Epoch: 5| Step: 4
Training loss: 1.8339354991912842
Validation loss: 2.4671628167552333

Epoch: 5| Step: 5
Training loss: 2.7592034339904785
Validation loss: 2.4696853365949405

Epoch: 5| Step: 6
Training loss: 2.1626105308532715
Validation loss: 2.4683612238976265

Epoch: 5| Step: 7
Training loss: 2.6809921264648438
Validation loss: 2.472688137844045

Epoch: 5| Step: 8
Training loss: 2.9671859741210938
Validation loss: 2.4763436381534865

Epoch: 5| Step: 9
Training loss: 3.4445204734802246
Validation loss: 2.471961500824139

Epoch: 5| Step: 10
Training loss: 2.7480404376983643
Validation loss: 2.467025702999484

Epoch: 135| Step: 0
Training loss: 2.441466808319092
Validation loss: 2.4671070396259265

Epoch: 5| Step: 1
Training loss: 2.3430917263031006
Validation loss: 2.4698331612412647

Epoch: 5| Step: 2
Training loss: 2.1828088760375977
Validation loss: 2.4704121056423394

Epoch: 5| Step: 3
Training loss: 2.8156371116638184
Validation loss: 2.471290306378436

Epoch: 5| Step: 4
Training loss: 2.6898884773254395
Validation loss: 2.4749738452255086

Epoch: 5| Step: 5
Training loss: 2.5776515007019043
Validation loss: 2.4752773059311735

Epoch: 5| Step: 6
Training loss: 3.462120771408081
Validation loss: 2.488554170054774

Epoch: 5| Step: 7
Training loss: 2.9123001098632812
Validation loss: 2.4763639460327806

Epoch: 5| Step: 8
Training loss: 2.468527317047119
Validation loss: 2.488432051033102

Epoch: 5| Step: 9
Training loss: 3.1142451763153076
Validation loss: 2.4838908103204544

Epoch: 5| Step: 10
Training loss: 2.425619125366211
Validation loss: 2.479051287456225

Epoch: 136| Step: 0
Training loss: 2.3593530654907227
Validation loss: 2.4842218327265915

Epoch: 5| Step: 1
Training loss: 2.1361310482025146
Validation loss: 2.4713227697598037

Epoch: 5| Step: 2
Training loss: 2.6328604221343994
Validation loss: 2.4756293783905687

Epoch: 5| Step: 3
Training loss: 2.4859118461608887
Validation loss: 2.467291739679152

Epoch: 5| Step: 4
Training loss: 3.216967821121216
Validation loss: 2.4699323895157024

Epoch: 5| Step: 5
Training loss: 2.8675835132598877
Validation loss: 2.4716020014978226

Epoch: 5| Step: 6
Training loss: 2.3841588497161865
Validation loss: 2.473405186847974

Epoch: 5| Step: 7
Training loss: 3.137256145477295
Validation loss: 2.472577843614804

Epoch: 5| Step: 8
Training loss: 2.1057722568511963
Validation loss: 2.4750284917892946

Epoch: 5| Step: 9
Training loss: 3.086846351623535
Validation loss: 2.4719861553561304

Epoch: 5| Step: 10
Training loss: 2.989870071411133
Validation loss: 2.4718819126006095

Epoch: 137| Step: 0
Training loss: 3.051574230194092
Validation loss: 2.4652270886205856

Epoch: 5| Step: 1
Training loss: 2.395155191421509
Validation loss: 2.4616267860576673

Epoch: 5| Step: 2
Training loss: 2.8504931926727295
Validation loss: 2.459922993054954

Epoch: 5| Step: 3
Training loss: 2.1207592487335205
Validation loss: 2.458343462277484

Epoch: 5| Step: 4
Training loss: 2.639867067337036
Validation loss: 2.4627366040342595

Epoch: 5| Step: 5
Training loss: 2.452927827835083
Validation loss: 2.461848612754576

Epoch: 5| Step: 6
Training loss: 2.3391802310943604
Validation loss: 2.461013747799781

Epoch: 5| Step: 7
Training loss: 2.60605788230896
Validation loss: 2.460729886126775

Epoch: 5| Step: 8
Training loss: 2.731586456298828
Validation loss: 2.4695513940626577

Epoch: 5| Step: 9
Training loss: 3.4026997089385986
Validation loss: 2.4738272569512807

Epoch: 5| Step: 10
Training loss: 2.694537878036499
Validation loss: 2.4761082305703113

Epoch: 138| Step: 0
Training loss: 3.0990171432495117
Validation loss: 2.477282711254653

Epoch: 5| Step: 1
Training loss: 2.5203518867492676
Validation loss: 2.475970232358543

Epoch: 5| Step: 2
Training loss: 2.39176607131958
Validation loss: 2.478540253895585

Epoch: 5| Step: 3
Training loss: 3.129460096359253
Validation loss: 2.472681296769009

Epoch: 5| Step: 4
Training loss: 2.4556286334991455
Validation loss: 2.4741857846577964

Epoch: 5| Step: 5
Training loss: 1.7972049713134766
Validation loss: 2.4737566671063824

Epoch: 5| Step: 6
Training loss: 2.676301956176758
Validation loss: 2.4624865798540014

Epoch: 5| Step: 7
Training loss: 2.824828624725342
Validation loss: 2.4627575361600487

Epoch: 5| Step: 8
Training loss: 2.4317142963409424
Validation loss: 2.4588128289868756

Epoch: 5| Step: 9
Training loss: 2.637495517730713
Validation loss: 2.464635500343897

Epoch: 5| Step: 10
Training loss: 3.4840917587280273
Validation loss: 2.4683172497698056

Epoch: 139| Step: 0
Training loss: 2.5096771717071533
Validation loss: 2.4681465343762468

Epoch: 5| Step: 1
Training loss: 2.244762897491455
Validation loss: 2.4680202468749015

Epoch: 5| Step: 2
Training loss: 2.111255645751953
Validation loss: 2.481872150974889

Epoch: 5| Step: 3
Training loss: 3.1470611095428467
Validation loss: 2.4746927215207006

Epoch: 5| Step: 4
Training loss: 2.6152682304382324
Validation loss: 2.464991541318996

Epoch: 5| Step: 5
Training loss: 2.504054307937622
Validation loss: 2.4598109414500575

Epoch: 5| Step: 6
Training loss: 2.944391965866089
Validation loss: 2.4626209838415987

Epoch: 5| Step: 7
Training loss: 2.5640857219696045
Validation loss: 2.461376015857984

Epoch: 5| Step: 8
Training loss: 2.5601024627685547
Validation loss: 2.4587966806145123

Epoch: 5| Step: 9
Training loss: 3.0994410514831543
Validation loss: 2.4585219455021683

Epoch: 5| Step: 10
Training loss: 3.0270373821258545
Validation loss: 2.460252869513727

Epoch: 140| Step: 0
Training loss: 2.326385736465454
Validation loss: 2.4565176245986775

Epoch: 5| Step: 1
Training loss: 3.4900412559509277
Validation loss: 2.4606470138795915

Epoch: 5| Step: 2
Training loss: 2.3676552772521973
Validation loss: 2.461514672925395

Epoch: 5| Step: 3
Training loss: 2.7174861431121826
Validation loss: 2.462078930229269

Epoch: 5| Step: 4
Training loss: 2.940981149673462
Validation loss: 2.459553814703418

Epoch: 5| Step: 5
Training loss: 2.5247318744659424
Validation loss: 2.468043242731402

Epoch: 5| Step: 6
Training loss: 2.3345508575439453
Validation loss: 2.4751205751972813

Epoch: 5| Step: 7
Training loss: 2.9416472911834717
Validation loss: 2.470136450183007

Epoch: 5| Step: 8
Training loss: 2.55753493309021
Validation loss: 2.4780043684026247

Epoch: 5| Step: 9
Training loss: 2.765418291091919
Validation loss: 2.466574160001611

Epoch: 5| Step: 10
Training loss: 2.2672805786132812
Validation loss: 2.4674309812566286

Epoch: 141| Step: 0
Training loss: 2.0490024089813232
Validation loss: 2.4668103982043523

Epoch: 5| Step: 1
Training loss: 2.2707340717315674
Validation loss: 2.465418128557103

Epoch: 5| Step: 2
Training loss: 3.084433078765869
Validation loss: 2.466340793076382

Epoch: 5| Step: 3
Training loss: 2.3578238487243652
Validation loss: 2.464711134151746

Epoch: 5| Step: 4
Training loss: 2.988748073577881
Validation loss: 2.4642370746981714

Epoch: 5| Step: 5
Training loss: 2.7439327239990234
Validation loss: 2.4685108200196297

Epoch: 5| Step: 6
Training loss: 2.634037733078003
Validation loss: 2.4669783371751026

Epoch: 5| Step: 7
Training loss: 3.1816420555114746
Validation loss: 2.4619438238041376

Epoch: 5| Step: 8
Training loss: 2.404724359512329
Validation loss: 2.4694103630640174

Epoch: 5| Step: 9
Training loss: 2.5577175617218018
Validation loss: 2.4652342232324744

Epoch: 5| Step: 10
Training loss: 3.150413751602173
Validation loss: 2.4740081217981156

Epoch: 142| Step: 0
Training loss: 3.4394760131835938
Validation loss: 2.486569640456989

Epoch: 5| Step: 1
Training loss: 2.6091678142547607
Validation loss: 2.485739843819731

Epoch: 5| Step: 2
Training loss: 2.650088310241699
Validation loss: 2.4916248398442424

Epoch: 5| Step: 3
Training loss: 2.1028106212615967
Validation loss: 2.480811457480154

Epoch: 5| Step: 4
Training loss: 2.5061049461364746
Validation loss: 2.47003081024334

Epoch: 5| Step: 5
Training loss: 2.4716243743896484
Validation loss: 2.4678369388785413

Epoch: 5| Step: 6
Training loss: 2.4189000129699707
Validation loss: 2.4671584098569808

Epoch: 5| Step: 7
Training loss: 2.9265007972717285
Validation loss: 2.460991685108472

Epoch: 5| Step: 8
Training loss: 3.303539991378784
Validation loss: 2.464909791946411

Epoch: 5| Step: 9
Training loss: 2.468318462371826
Validation loss: 2.4618292406041133

Epoch: 5| Step: 10
Training loss: 2.3831896781921387
Validation loss: 2.4638425483498523

Epoch: 143| Step: 0
Training loss: 3.0895869731903076
Validation loss: 2.4568356096103625

Epoch: 5| Step: 1
Training loss: 2.3994054794311523
Validation loss: 2.4638943313270487

Epoch: 5| Step: 2
Training loss: 2.6340789794921875
Validation loss: 2.457378605360626

Epoch: 5| Step: 3
Training loss: 2.677932024002075
Validation loss: 2.4603759960461686

Epoch: 5| Step: 4
Training loss: 2.688530683517456
Validation loss: 2.4557358680232877

Epoch: 5| Step: 5
Training loss: 2.876652956008911
Validation loss: 2.45515783884192

Epoch: 5| Step: 6
Training loss: 2.4396567344665527
Validation loss: 2.451427036716092

Epoch: 5| Step: 7
Training loss: 2.7877814769744873
Validation loss: 2.453621748955019

Epoch: 5| Step: 8
Training loss: 3.0305657386779785
Validation loss: 2.462019092293196

Epoch: 5| Step: 9
Training loss: 2.5092244148254395
Validation loss: 2.459397787688881

Epoch: 5| Step: 10
Training loss: 2.014376401901245
Validation loss: 2.4615265015632875

Epoch: 144| Step: 0
Training loss: 2.6239326000213623
Validation loss: 2.4569093386332193

Epoch: 5| Step: 1
Training loss: 2.318480968475342
Validation loss: 2.4601615680161344

Epoch: 5| Step: 2
Training loss: 2.7960381507873535
Validation loss: 2.4640860326828493

Epoch: 5| Step: 3
Training loss: 3.3405983448028564
Validation loss: 2.4723897031558457

Epoch: 5| Step: 4
Training loss: 2.4021098613739014
Validation loss: 2.4609837070588143

Epoch: 5| Step: 5
Training loss: 2.853390693664551
Validation loss: 2.451566373148272

Epoch: 5| Step: 6
Training loss: 2.7780601978302
Validation loss: 2.4584796095407135

Epoch: 5| Step: 7
Training loss: 2.610377550125122
Validation loss: 2.452340390092583

Epoch: 5| Step: 8
Training loss: 2.8349945545196533
Validation loss: 2.452409798099149

Epoch: 5| Step: 9
Training loss: 2.468135118484497
Validation loss: 2.454614987937353

Epoch: 5| Step: 10
Training loss: 2.1611270904541016
Validation loss: 2.451916830514067

Epoch: 145| Step: 0
Training loss: 2.97520112991333
Validation loss: 2.457733669588643

Epoch: 5| Step: 1
Training loss: 1.891394853591919
Validation loss: 2.466420905564421

Epoch: 5| Step: 2
Training loss: 2.317727565765381
Validation loss: 2.4754544047899145

Epoch: 5| Step: 3
Training loss: 2.153007984161377
Validation loss: 2.477123891153643

Epoch: 5| Step: 4
Training loss: 2.299004316329956
Validation loss: 2.487111545378162

Epoch: 5| Step: 5
Training loss: 2.87431001663208
Validation loss: 2.4880370427203435

Epoch: 5| Step: 6
Training loss: 2.3601551055908203
Validation loss: 2.5012791490042083

Epoch: 5| Step: 7
Training loss: 2.2907168865203857
Validation loss: 2.497474349955077

Epoch: 5| Step: 8
Training loss: 3.6809680461883545
Validation loss: 2.490142140337216

Epoch: 5| Step: 9
Training loss: 3.5010879039764404
Validation loss: 2.483640709230977

Epoch: 5| Step: 10
Training loss: 3.076488494873047
Validation loss: 2.4663298463308685

Epoch: 146| Step: 0
Training loss: 2.9136781692504883
Validation loss: 2.4645088975147535

Epoch: 5| Step: 1
Training loss: 2.2480452060699463
Validation loss: 2.459608713785807

Epoch: 5| Step: 2
Training loss: 2.789973497390747
Validation loss: 2.455709416379211

Epoch: 5| Step: 3
Training loss: 2.6156203746795654
Validation loss: 2.456030015022524

Epoch: 5| Step: 4
Training loss: 3.028247833251953
Validation loss: 2.4512336895030034

Epoch: 5| Step: 5
Training loss: 2.042222499847412
Validation loss: 2.458430113330964

Epoch: 5| Step: 6
Training loss: 2.7885630130767822
Validation loss: 2.4533136736962105

Epoch: 5| Step: 7
Training loss: 2.4279751777648926
Validation loss: 2.4582813555194485

Epoch: 5| Step: 8
Training loss: 2.53633451461792
Validation loss: 2.4538852860850673

Epoch: 5| Step: 9
Training loss: 3.157865524291992
Validation loss: 2.4534334290412163

Epoch: 5| Step: 10
Training loss: 2.782207489013672
Validation loss: 2.45134949427779

Epoch: 147| Step: 0
Training loss: 2.4656970500946045
Validation loss: 2.4467193003623717

Epoch: 5| Step: 1
Training loss: 2.289750814437866
Validation loss: 2.447500269900086

Epoch: 5| Step: 2
Training loss: 2.2561488151550293
Validation loss: 2.443820981569188

Epoch: 5| Step: 3
Training loss: 2.9021270275115967
Validation loss: 2.456184200061265

Epoch: 5| Step: 4
Training loss: 3.028437852859497
Validation loss: 2.4564835563782723

Epoch: 5| Step: 5
Training loss: 2.514997720718384
Validation loss: 2.4572910813875097

Epoch: 5| Step: 6
Training loss: 2.826141834259033
Validation loss: 2.4603849431519866

Epoch: 5| Step: 7
Training loss: 3.1098227500915527
Validation loss: 2.467383030922182

Epoch: 5| Step: 8
Training loss: 2.1561741828918457
Validation loss: 2.4728221816401326

Epoch: 5| Step: 9
Training loss: 2.4663310050964355
Validation loss: 2.466516448605445

Epoch: 5| Step: 10
Training loss: 3.2866737842559814
Validation loss: 2.458109589033229

Epoch: 148| Step: 0
Training loss: 2.7014272212982178
Validation loss: 2.4563000432906614

Epoch: 5| Step: 1
Training loss: 2.6571736335754395
Validation loss: 2.451592542791879

Epoch: 5| Step: 2
Training loss: 2.952125072479248
Validation loss: 2.449483715077882

Epoch: 5| Step: 3
Training loss: 2.7691574096679688
Validation loss: 2.451555126456804

Epoch: 5| Step: 4
Training loss: 2.932077646255493
Validation loss: 2.448456633475519

Epoch: 5| Step: 5
Training loss: 2.9204840660095215
Validation loss: 2.4484010537465415

Epoch: 5| Step: 6
Training loss: 2.6594185829162598
Validation loss: 2.4488381801113004

Epoch: 5| Step: 7
Training loss: 2.246021270751953
Validation loss: 2.449627181535126

Epoch: 5| Step: 8
Training loss: 2.70076847076416
Validation loss: 2.4486688260109193

Epoch: 5| Step: 9
Training loss: 2.1436450481414795
Validation loss: 2.4497396433225243

Epoch: 5| Step: 10
Training loss: 2.5323939323425293
Validation loss: 2.45567149244329

Epoch: 149| Step: 0
Training loss: 2.6572728157043457
Validation loss: 2.4517352683569795

Epoch: 5| Step: 1
Training loss: 2.5394861698150635
Validation loss: 2.456852848811816

Epoch: 5| Step: 2
Training loss: 2.9768893718719482
Validation loss: 2.4701035791827786

Epoch: 5| Step: 3
Training loss: 3.0232486724853516
Validation loss: 2.478931075783186

Epoch: 5| Step: 4
Training loss: 2.3379197120666504
Validation loss: 2.4841310798480944

Epoch: 5| Step: 5
Training loss: 2.3558743000030518
Validation loss: 2.4780184684261197

Epoch: 5| Step: 6
Training loss: 3.0319151878356934
Validation loss: 2.478467151682864

Epoch: 5| Step: 7
Training loss: 3.196568727493286
Validation loss: 2.467992349337506

Epoch: 5| Step: 8
Training loss: 2.1641807556152344
Validation loss: 2.465501836551133

Epoch: 5| Step: 9
Training loss: 2.610698699951172
Validation loss: 2.4614825710173576

Epoch: 5| Step: 10
Training loss: 2.3309671878814697
Validation loss: 2.466777927132063

Epoch: 150| Step: 0
Training loss: 2.5427699089050293
Validation loss: 2.4653088687568583

Epoch: 5| Step: 1
Training loss: 2.2646045684814453
Validation loss: 2.4623157260238484

Epoch: 5| Step: 2
Training loss: 2.4007837772369385
Validation loss: 2.46837971543753

Epoch: 5| Step: 3
Training loss: 3.280693769454956
Validation loss: 2.467855509891305

Epoch: 5| Step: 4
Training loss: 2.395179510116577
Validation loss: 2.461427260470647

Epoch: 5| Step: 5
Training loss: 2.8616979122161865
Validation loss: 2.4691742543251283

Epoch: 5| Step: 6
Training loss: 3.75068736076355
Validation loss: 2.4632816955607426

Epoch: 5| Step: 7
Training loss: 1.969397783279419
Validation loss: 2.4633010946294314

Epoch: 5| Step: 8
Training loss: 2.512416124343872
Validation loss: 2.4638625473104496

Epoch: 5| Step: 9
Training loss: 2.512357473373413
Validation loss: 2.4616961299732165

Epoch: 5| Step: 10
Training loss: 2.7051451206207275
Validation loss: 2.4680933644694667

Epoch: 151| Step: 0
Training loss: 2.690218448638916
Validation loss: 2.4652371727010256

Epoch: 5| Step: 1
Training loss: 2.504756450653076
Validation loss: 2.467878213492773

Epoch: 5| Step: 2
Training loss: 2.567248821258545
Validation loss: 2.469414921217067

Epoch: 5| Step: 3
Training loss: 2.943829298019409
Validation loss: 2.4719580578547653

Epoch: 5| Step: 4
Training loss: 2.4832653999328613
Validation loss: 2.477259228306432

Epoch: 5| Step: 5
Training loss: 2.4357128143310547
Validation loss: 2.5028910611265447

Epoch: 5| Step: 6
Training loss: 3.9002773761749268
Validation loss: 2.508536392642606

Epoch: 5| Step: 7
Training loss: 2.5733470916748047
Validation loss: 2.5126321213219756

Epoch: 5| Step: 8
Training loss: 2.9149506092071533
Validation loss: 2.4927402363028577

Epoch: 5| Step: 9
Training loss: 2.0603928565979004
Validation loss: 2.479454622473768

Epoch: 5| Step: 10
Training loss: 2.0820751190185547
Validation loss: 2.4663607202550417

Epoch: 152| Step: 0
Training loss: 2.7027428150177
Validation loss: 2.457766946925912

Epoch: 5| Step: 1
Training loss: 2.7172725200653076
Validation loss: 2.450905397374143

Epoch: 5| Step: 2
Training loss: 2.3362202644348145
Validation loss: 2.446538281697099

Epoch: 5| Step: 3
Training loss: 2.3080592155456543
Validation loss: 2.445739192347373

Epoch: 5| Step: 4
Training loss: 2.482059955596924
Validation loss: 2.441039385334138

Epoch: 5| Step: 5
Training loss: 2.727093458175659
Validation loss: 2.4387612112106813

Epoch: 5| Step: 6
Training loss: 2.924591302871704
Validation loss: 2.433323396149502

Epoch: 5| Step: 7
Training loss: 2.8203816413879395
Validation loss: 2.4342904885609946

Epoch: 5| Step: 8
Training loss: 2.2341740131378174
Validation loss: 2.4376471478451966

Epoch: 5| Step: 9
Training loss: 3.219574451446533
Validation loss: 2.438745362784273

Epoch: 5| Step: 10
Training loss: 2.6371822357177734
Validation loss: 2.4374682852016982

Epoch: 153| Step: 0
Training loss: 2.485201835632324
Validation loss: 2.4362374736416723

Epoch: 5| Step: 1
Training loss: 2.425461530685425
Validation loss: 2.434966307814403

Epoch: 5| Step: 2
Training loss: 2.2291314601898193
Validation loss: 2.4393506306473927

Epoch: 5| Step: 3
Training loss: 2.983508825302124
Validation loss: 2.4356569551652476

Epoch: 5| Step: 4
Training loss: 2.411783218383789
Validation loss: 2.439507304981191

Epoch: 5| Step: 5
Training loss: 2.8480429649353027
Validation loss: 2.445936546530775

Epoch: 5| Step: 6
Training loss: 2.7573466300964355
Validation loss: 2.446785011599141

Epoch: 5| Step: 7
Training loss: 2.6571595668792725
Validation loss: 2.4547136188835226

Epoch: 5| Step: 8
Training loss: 2.832475185394287
Validation loss: 2.461127701626029

Epoch: 5| Step: 9
Training loss: 2.876095771789551
Validation loss: 2.466863037437521

Epoch: 5| Step: 10
Training loss: 2.732736825942993
Validation loss: 2.4682950819692304

Epoch: 154| Step: 0
Training loss: 2.167161464691162
Validation loss: 2.464814532187677

Epoch: 5| Step: 1
Training loss: 2.8240280151367188
Validation loss: 2.460165003294586

Epoch: 5| Step: 2
Training loss: 2.5594844818115234
Validation loss: 2.4550741898116244

Epoch: 5| Step: 3
Training loss: 2.563528299331665
Validation loss: 2.4407659833149244

Epoch: 5| Step: 4
Training loss: 3.0394294261932373
Validation loss: 2.43357821946503

Epoch: 5| Step: 5
Training loss: 2.699909210205078
Validation loss: 2.434328532988025

Epoch: 5| Step: 6
Training loss: 2.0424935817718506
Validation loss: 2.440932499465122

Epoch: 5| Step: 7
Training loss: 2.6061131954193115
Validation loss: 2.4452676234706754

Epoch: 5| Step: 8
Training loss: 2.686499834060669
Validation loss: 2.4350191829025105

Epoch: 5| Step: 9
Training loss: 2.8774960041046143
Validation loss: 2.43854881102039

Epoch: 5| Step: 10
Training loss: 3.25616717338562
Validation loss: 2.43470940538632

Epoch: 155| Step: 0
Training loss: 2.6742889881134033
Validation loss: 2.434543319927749

Epoch: 5| Step: 1
Training loss: 3.1495285034179688
Validation loss: 2.437199789990661

Epoch: 5| Step: 2
Training loss: 2.207463264465332
Validation loss: 2.430260209627049

Epoch: 5| Step: 3
Training loss: 2.6469905376434326
Validation loss: 2.4398678502728863

Epoch: 5| Step: 4
Training loss: 2.750434398651123
Validation loss: 2.4402595155982563

Epoch: 5| Step: 5
Training loss: 2.6499857902526855
Validation loss: 2.439993337918353

Epoch: 5| Step: 6
Training loss: 2.155285596847534
Validation loss: 2.4361145445095596

Epoch: 5| Step: 7
Training loss: 2.9289917945861816
Validation loss: 2.4383213186776764

Epoch: 5| Step: 8
Training loss: 3.0737242698669434
Validation loss: 2.435317213817309

Epoch: 5| Step: 9
Training loss: 2.5495152473449707
Validation loss: 2.4360658609738914

Epoch: 5| Step: 10
Training loss: 2.3101706504821777
Validation loss: 2.4335760454977713

Epoch: 156| Step: 0
Training loss: 3.0025789737701416
Validation loss: 2.4375115235646567

Epoch: 5| Step: 1
Training loss: 2.256478786468506
Validation loss: 2.4341794444668676

Epoch: 5| Step: 2
Training loss: 2.45387864112854
Validation loss: 2.4345174117754866

Epoch: 5| Step: 3
Training loss: 2.9226760864257812
Validation loss: 2.429350427401963

Epoch: 5| Step: 4
Training loss: 2.063070774078369
Validation loss: 2.440265699099469

Epoch: 5| Step: 5
Training loss: 3.0995850563049316
Validation loss: 2.4380729019000964

Epoch: 5| Step: 6
Training loss: 2.7315430641174316
Validation loss: 2.4387579194961058

Epoch: 5| Step: 7
Training loss: 1.8387835025787354
Validation loss: 2.442372356691668

Epoch: 5| Step: 8
Training loss: 2.7323975563049316
Validation loss: 2.4373137258714244

Epoch: 5| Step: 9
Training loss: 2.9074504375457764
Validation loss: 2.4468401426910074

Epoch: 5| Step: 10
Training loss: 3.2329330444335938
Validation loss: 2.4382132791703746

Epoch: 157| Step: 0
Training loss: 3.0745513439178467
Validation loss: 2.4345281226660616

Epoch: 5| Step: 1
Training loss: 3.0474276542663574
Validation loss: 2.431316949987924

Epoch: 5| Step: 2
Training loss: 2.3971879482269287
Validation loss: 2.4318090895170807

Epoch: 5| Step: 3
Training loss: 2.4630770683288574
Validation loss: 2.434691462465512

Epoch: 5| Step: 4
Training loss: 2.918488025665283
Validation loss: 2.439048379980108

Epoch: 5| Step: 5
Training loss: 2.8315114974975586
Validation loss: 2.443004726081766

Epoch: 5| Step: 6
Training loss: 2.616446018218994
Validation loss: 2.4413124361345844

Epoch: 5| Step: 7
Training loss: 2.1048128604888916
Validation loss: 2.4454599785548385

Epoch: 5| Step: 8
Training loss: 2.525888442993164
Validation loss: 2.446694415102723

Epoch: 5| Step: 9
Training loss: 2.240344762802124
Validation loss: 2.4486218498599146

Epoch: 5| Step: 10
Training loss: 2.9109811782836914
Validation loss: 2.4457787826497066

Epoch: 158| Step: 0
Training loss: 2.574544906616211
Validation loss: 2.444802094531316

Epoch: 5| Step: 1
Training loss: 2.5052828788757324
Validation loss: 2.4466409067953787

Epoch: 5| Step: 2
Training loss: 2.878649950027466
Validation loss: 2.4442387473198677

Epoch: 5| Step: 3
Training loss: 2.8809657096862793
Validation loss: 2.4400182206143617

Epoch: 5| Step: 4
Training loss: 1.8943407535552979
Validation loss: 2.444127321243286

Epoch: 5| Step: 5
Training loss: 2.846027374267578
Validation loss: 2.4510238119350967

Epoch: 5| Step: 6
Training loss: 2.415863513946533
Validation loss: 2.450681867138032

Epoch: 5| Step: 7
Training loss: 2.546811819076538
Validation loss: 2.4485655394933556

Epoch: 5| Step: 8
Training loss: 2.555022716522217
Validation loss: 2.4520925962796776

Epoch: 5| Step: 9
Training loss: 3.1248674392700195
Validation loss: 2.455165683582265

Epoch: 5| Step: 10
Training loss: 2.809079647064209
Validation loss: 2.4452783651249383

Epoch: 159| Step: 0
Training loss: 2.593526840209961
Validation loss: 2.4440894319165136

Epoch: 5| Step: 1
Training loss: 2.5043423175811768
Validation loss: 2.4351915082623883

Epoch: 5| Step: 2
Training loss: 3.1763198375701904
Validation loss: 2.437998804994809

Epoch: 5| Step: 3
Training loss: 2.5565602779388428
Validation loss: 2.4351372693174627

Epoch: 5| Step: 4
Training loss: 2.850773572921753
Validation loss: 2.438342809677124

Epoch: 5| Step: 5
Training loss: 2.779003858566284
Validation loss: 2.4352500361780964

Epoch: 5| Step: 6
Training loss: 2.2961432933807373
Validation loss: 2.4382486881748324

Epoch: 5| Step: 7
Training loss: 2.7083890438079834
Validation loss: 2.4399624383577736

Epoch: 5| Step: 8
Training loss: 2.5595669746398926
Validation loss: 2.440566498746154

Epoch: 5| Step: 9
Training loss: 2.462998867034912
Validation loss: 2.4398190077914985

Epoch: 5| Step: 10
Training loss: 2.6917736530303955
Validation loss: 2.4344839895925214

Epoch: 160| Step: 0
Training loss: 2.589675188064575
Validation loss: 2.440073838797949

Epoch: 5| Step: 1
Training loss: 2.711930751800537
Validation loss: 2.4410583511475594

Epoch: 5| Step: 2
Training loss: 3.2556838989257812
Validation loss: 2.4437246796905354

Epoch: 5| Step: 3
Training loss: 2.671030044555664
Validation loss: 2.4407581411382204

Epoch: 5| Step: 4
Training loss: 2.5069260597229004
Validation loss: 2.4426706785796792

Epoch: 5| Step: 5
Training loss: 2.739577054977417
Validation loss: 2.4419901640184465

Epoch: 5| Step: 6
Training loss: 2.3582234382629395
Validation loss: 2.434424795130248

Epoch: 5| Step: 7
Training loss: 2.3113901615142822
Validation loss: 2.430186758759201

Epoch: 5| Step: 8
Training loss: 2.792759656906128
Validation loss: 2.427882427810341

Epoch: 5| Step: 9
Training loss: 2.6440823078155518
Validation loss: 2.4302223728549097

Epoch: 5| Step: 10
Training loss: 2.60050368309021
Validation loss: 2.4361815221848024

Epoch: 161| Step: 0
Training loss: 2.797900915145874
Validation loss: 2.4343003765229256

Epoch: 5| Step: 1
Training loss: 3.0347344875335693
Validation loss: 2.4324963887532554

Epoch: 5| Step: 2
Training loss: 2.4057233333587646
Validation loss: 2.4339823722839355

Epoch: 5| Step: 3
Training loss: 2.346348524093628
Validation loss: 2.4344480704235774

Epoch: 5| Step: 4
Training loss: 2.748875141143799
Validation loss: 2.4385760804658294

Epoch: 5| Step: 5
Training loss: 2.575878858566284
Validation loss: 2.4458198508908673

Epoch: 5| Step: 6
Training loss: 2.7441399097442627
Validation loss: 2.4492068547074513

Epoch: 5| Step: 7
Training loss: 2.695068120956421
Validation loss: 2.4479284055771364

Epoch: 5| Step: 8
Training loss: 2.729238986968994
Validation loss: 2.4456610397625993

Epoch: 5| Step: 9
Training loss: 2.208874464035034
Validation loss: 2.432870921268258

Epoch: 5| Step: 10
Training loss: 2.8628499507904053
Validation loss: 2.4347765138072353

Epoch: 162| Step: 0
Training loss: 2.855037212371826
Validation loss: 2.4329372887970298

Epoch: 5| Step: 1
Training loss: 3.0668845176696777
Validation loss: 2.4387681971314135

Epoch: 5| Step: 2
Training loss: 2.0221800804138184
Validation loss: 2.440478191580824

Epoch: 5| Step: 3
Training loss: 2.5652894973754883
Validation loss: 2.4481565465209303

Epoch: 5| Step: 4
Training loss: 3.1471965312957764
Validation loss: 2.440969210799022

Epoch: 5| Step: 5
Training loss: 2.92039155960083
Validation loss: 2.441549707484502

Epoch: 5| Step: 6
Training loss: 2.810020685195923
Validation loss: 2.4377129795730754

Epoch: 5| Step: 7
Training loss: 2.174591064453125
Validation loss: 2.4317530303873043

Epoch: 5| Step: 8
Training loss: 2.5262880325317383
Validation loss: 2.4320919077883483

Epoch: 5| Step: 9
Training loss: 2.4248616695404053
Validation loss: 2.4283851987572125

Epoch: 5| Step: 10
Training loss: 2.698953151702881
Validation loss: 2.4288323079386065

Epoch: 163| Step: 0
Training loss: 3.211235523223877
Validation loss: 2.437490942657635

Epoch: 5| Step: 1
Training loss: 2.4320735931396484
Validation loss: 2.4363305594331477

Epoch: 5| Step: 2
Training loss: 2.5166473388671875
Validation loss: 2.434354559067757

Epoch: 5| Step: 3
Training loss: 1.8721526861190796
Validation loss: 2.4372688108874905

Epoch: 5| Step: 4
Training loss: 2.6396543979644775
Validation loss: 2.436335704659903

Epoch: 5| Step: 5
Training loss: 2.315294027328491
Validation loss: 2.427075086101409

Epoch: 5| Step: 6
Training loss: 2.5686492919921875
Validation loss: 2.431437784625638

Epoch: 5| Step: 7
Training loss: 2.5487561225891113
Validation loss: 2.430001699796287

Epoch: 5| Step: 8
Training loss: 3.0874905586242676
Validation loss: 2.4315966611267417

Epoch: 5| Step: 9
Training loss: 2.9256036281585693
Validation loss: 2.433682603220786

Epoch: 5| Step: 10
Training loss: 3.0919857025146484
Validation loss: 2.4374209655228483

Epoch: 164| Step: 0
Training loss: 2.946962594985962
Validation loss: 2.4382635803632837

Epoch: 5| Step: 1
Training loss: 3.165645122528076
Validation loss: 2.4337821263138966

Epoch: 5| Step: 2
Training loss: 2.8098037242889404
Validation loss: 2.434222326483778

Epoch: 5| Step: 3
Training loss: 2.3742430210113525
Validation loss: 2.434702898866387

Epoch: 5| Step: 4
Training loss: 2.228487253189087
Validation loss: 2.429777793986823

Epoch: 5| Step: 5
Training loss: 2.7413394451141357
Validation loss: 2.432583519207534

Epoch: 5| Step: 6
Training loss: 2.199409246444702
Validation loss: 2.440039850050403

Epoch: 5| Step: 7
Training loss: 2.4242796897888184
Validation loss: 2.4365822243434128

Epoch: 5| Step: 8
Training loss: 2.637240171432495
Validation loss: 2.4335185443201373

Epoch: 5| Step: 9
Training loss: 2.648939609527588
Validation loss: 2.4307859482303744

Epoch: 5| Step: 10
Training loss: 2.9043474197387695
Validation loss: 2.4319752595757924

Epoch: 165| Step: 0
Training loss: 2.7288854122161865
Validation loss: 2.4333947448320288

Epoch: 5| Step: 1
Training loss: 2.545929431915283
Validation loss: 2.4400703573739655

Epoch: 5| Step: 2
Training loss: 2.8060786724090576
Validation loss: 2.4351877858561854

Epoch: 5| Step: 3
Training loss: 2.0337095260620117
Validation loss: 2.4344012865456204

Epoch: 5| Step: 4
Training loss: 2.724555253982544
Validation loss: 2.432620338214341

Epoch: 5| Step: 5
Training loss: 2.031235933303833
Validation loss: 2.4363188525681854

Epoch: 5| Step: 6
Training loss: 3.3690528869628906
Validation loss: 2.4292440286246677

Epoch: 5| Step: 7
Training loss: 2.3587088584899902
Validation loss: 2.428147967143725

Epoch: 5| Step: 8
Training loss: 2.7277538776397705
Validation loss: 2.428934476708853

Epoch: 5| Step: 9
Training loss: 3.039966106414795
Validation loss: 2.4331133339994695

Epoch: 5| Step: 10
Training loss: 2.585784673690796
Validation loss: 2.4301864665041686

Epoch: 166| Step: 0
Training loss: 2.7764804363250732
Validation loss: 2.43096133201353

Epoch: 5| Step: 1
Training loss: 2.5455329418182373
Validation loss: 2.4281351668860323

Epoch: 5| Step: 2
Training loss: 2.98176908493042
Validation loss: 2.43579133864372

Epoch: 5| Step: 3
Training loss: 2.5245120525360107
Validation loss: 2.433332568855696

Epoch: 5| Step: 4
Training loss: 2.472493886947632
Validation loss: 2.441450306164321

Epoch: 5| Step: 5
Training loss: 2.3255674839019775
Validation loss: 2.4324983832656697

Epoch: 5| Step: 6
Training loss: 2.8570358753204346
Validation loss: 2.4348745499887774

Epoch: 5| Step: 7
Training loss: 2.8774654865264893
Validation loss: 2.4349413533364572

Epoch: 5| Step: 8
Training loss: 2.5743720531463623
Validation loss: 2.4234868916132117

Epoch: 5| Step: 9
Training loss: 2.429724931716919
Validation loss: 2.4292041588855047

Epoch: 5| Step: 10
Training loss: 2.617962121963501
Validation loss: 2.430615989110803

Epoch: 167| Step: 0
Training loss: 1.9084161520004272
Validation loss: 2.4350441527623

Epoch: 5| Step: 1
Training loss: 2.8412108421325684
Validation loss: 2.4416671722166

Epoch: 5| Step: 2
Training loss: 2.635814905166626
Validation loss: 2.449863051855436

Epoch: 5| Step: 3
Training loss: 2.6496939659118652
Validation loss: 2.4576134989338536

Epoch: 5| Step: 4
Training loss: 2.7064695358276367
Validation loss: 2.4677833587892595

Epoch: 5| Step: 5
Training loss: 3.658407211303711
Validation loss: 2.464799660508351

Epoch: 5| Step: 6
Training loss: 2.9180386066436768
Validation loss: 2.462851442316527

Epoch: 5| Step: 7
Training loss: 2.0625264644622803
Validation loss: 2.4579756516282276

Epoch: 5| Step: 8
Training loss: 2.855546474456787
Validation loss: 2.456951518212595

Epoch: 5| Step: 9
Training loss: 2.313448429107666
Validation loss: 2.438598063684279

Epoch: 5| Step: 10
Training loss: 2.464975118637085
Validation loss: 2.4224542392197477

Epoch: 168| Step: 0
Training loss: 2.714128017425537
Validation loss: 2.420428063279839

Epoch: 5| Step: 1
Training loss: 2.1928086280822754
Validation loss: 2.416993361647411

Epoch: 5| Step: 2
Training loss: 2.5573697090148926
Validation loss: 2.4178954978143015

Epoch: 5| Step: 3
Training loss: 2.9769277572631836
Validation loss: 2.4196123974297636

Epoch: 5| Step: 4
Training loss: 3.0900797843933105
Validation loss: 2.421205125829225

Epoch: 5| Step: 5
Training loss: 2.5425209999084473
Validation loss: 2.420029378706409

Epoch: 5| Step: 6
Training loss: 2.5093212127685547
Validation loss: 2.4171678853291336

Epoch: 5| Step: 7
Training loss: 2.3441672325134277
Validation loss: 2.421173685340471

Epoch: 5| Step: 8
Training loss: 2.881366491317749
Validation loss: 2.4173782487069406

Epoch: 5| Step: 9
Training loss: 2.4954848289489746
Validation loss: 2.422443619338415

Epoch: 5| Step: 10
Training loss: 2.6577255725860596
Validation loss: 2.421087872597479

Epoch: 169| Step: 0
Training loss: 2.771653175354004
Validation loss: 2.420232521590366

Epoch: 5| Step: 1
Training loss: 2.4575247764587402
Validation loss: 2.41986951264002

Epoch: 5| Step: 2
Training loss: 2.05784273147583
Validation loss: 2.4219034538474133

Epoch: 5| Step: 3
Training loss: 2.8826422691345215
Validation loss: 2.41883937774166

Epoch: 5| Step: 4
Training loss: 2.3759281635284424
Validation loss: 2.419727417730516

Epoch: 5| Step: 5
Training loss: 3.14546799659729
Validation loss: 2.4342395131305983

Epoch: 5| Step: 6
Training loss: 1.976697564125061
Validation loss: 2.4383490111238215

Epoch: 5| Step: 7
Training loss: 3.2652478218078613
Validation loss: 2.4443192148721344

Epoch: 5| Step: 8
Training loss: 2.6742382049560547
Validation loss: 2.4367498505500054

Epoch: 5| Step: 9
Training loss: 2.9958274364471436
Validation loss: 2.4492765600963304

Epoch: 5| Step: 10
Training loss: 2.3550662994384766
Validation loss: 2.4371169613253687

Epoch: 170| Step: 0
Training loss: 2.1843371391296387
Validation loss: 2.4431524943279963

Epoch: 5| Step: 1
Training loss: 2.8228211402893066
Validation loss: 2.436924652386737

Epoch: 5| Step: 2
Training loss: 2.406956195831299
Validation loss: 2.4351667127301617

Epoch: 5| Step: 3
Training loss: 3.442136287689209
Validation loss: 2.425199531739758

Epoch: 5| Step: 4
Training loss: 2.7471001148223877
Validation loss: 2.4206244586616434

Epoch: 5| Step: 5
Training loss: 2.8816025257110596
Validation loss: 2.4199655414909444

Epoch: 5| Step: 6
Training loss: 2.3513457775115967
Validation loss: 2.416967345822242

Epoch: 5| Step: 7
Training loss: 2.419909954071045
Validation loss: 2.4178809760719218

Epoch: 5| Step: 8
Training loss: 2.040248394012451
Validation loss: 2.417628485669372

Epoch: 5| Step: 9
Training loss: 3.032172203063965
Validation loss: 2.41760096626897

Epoch: 5| Step: 10
Training loss: 2.585193634033203
Validation loss: 2.4209345386874292

Epoch: 171| Step: 0
Training loss: 3.1790504455566406
Validation loss: 2.419769125600015

Epoch: 5| Step: 1
Training loss: 2.7030718326568604
Validation loss: 2.4186205812679824

Epoch: 5| Step: 2
Training loss: 2.340848684310913
Validation loss: 2.4198133048190864

Epoch: 5| Step: 3
Training loss: 2.0495657920837402
Validation loss: 2.4265582612765733

Epoch: 5| Step: 4
Training loss: 2.941011428833008
Validation loss: 2.4271715148802726

Epoch: 5| Step: 5
Training loss: 2.4522624015808105
Validation loss: 2.4261561773156606

Epoch: 5| Step: 6
Training loss: 2.522012710571289
Validation loss: 2.433505050597652

Epoch: 5| Step: 7
Training loss: 2.3363873958587646
Validation loss: 2.428871598294986

Epoch: 5| Step: 8
Training loss: 2.9764151573181152
Validation loss: 2.4250992754454255

Epoch: 5| Step: 9
Training loss: 2.9560093879699707
Validation loss: 2.4331797117828042

Epoch: 5| Step: 10
Training loss: 2.414555549621582
Validation loss: 2.4352518230356197

Epoch: 172| Step: 0
Training loss: 2.6459174156188965
Validation loss: 2.439411783731112

Epoch: 5| Step: 1
Training loss: 2.159608840942383
Validation loss: 2.436041767879199

Epoch: 5| Step: 2
Training loss: 2.8115954399108887
Validation loss: 2.4318664381580968

Epoch: 5| Step: 3
Training loss: 2.6627211570739746
Validation loss: 2.4406708261018157

Epoch: 5| Step: 4
Training loss: 2.839249849319458
Validation loss: 2.4444128826100338

Epoch: 5| Step: 5
Training loss: 2.6717748641967773
Validation loss: 2.4458546741034395

Epoch: 5| Step: 6
Training loss: 3.072084903717041
Validation loss: 2.4411325659803165

Epoch: 5| Step: 7
Training loss: 2.402015209197998
Validation loss: 2.433022524720879

Epoch: 5| Step: 8
Training loss: 2.26501727104187
Validation loss: 2.4258403598621325

Epoch: 5| Step: 9
Training loss: 3.3413162231445312
Validation loss: 2.429119502344439

Epoch: 5| Step: 10
Training loss: 1.8857439756393433
Validation loss: 2.4251596004732194

Epoch: 173| Step: 0
Training loss: 2.4007644653320312
Validation loss: 2.426533919508739

Epoch: 5| Step: 1
Training loss: 3.224623203277588
Validation loss: 2.436125273345619

Epoch: 5| Step: 2
Training loss: 2.089491367340088
Validation loss: 2.4454819233186784

Epoch: 5| Step: 3
Training loss: 2.0066773891448975
Validation loss: 2.4531167348225913

Epoch: 5| Step: 4
Training loss: 3.7528610229492188
Validation loss: 2.446555354261911

Epoch: 5| Step: 5
Training loss: 2.347721576690674
Validation loss: 2.4491738221978627

Epoch: 5| Step: 6
Training loss: 1.9313507080078125
Validation loss: 2.44148854542804

Epoch: 5| Step: 7
Training loss: 2.6208882331848145
Validation loss: 2.4330514015689975

Epoch: 5| Step: 8
Training loss: 3.4886622428894043
Validation loss: 2.4233492138565227

Epoch: 5| Step: 9
Training loss: 2.391507387161255
Validation loss: 2.413322392330375

Epoch: 5| Step: 10
Training loss: 2.5747592449188232
Validation loss: 2.413705846314789

Epoch: 174| Step: 0
Training loss: 2.578042984008789
Validation loss: 2.4141399193835515

Epoch: 5| Step: 1
Training loss: 2.902400493621826
Validation loss: 2.4118570307249665

Epoch: 5| Step: 2
Training loss: 2.630425453186035
Validation loss: 2.413393082157258

Epoch: 5| Step: 3
Training loss: 2.332777738571167
Validation loss: 2.413930969853555

Epoch: 5| Step: 4
Training loss: 1.9803212881088257
Validation loss: 2.410046669744676

Epoch: 5| Step: 5
Training loss: 2.7406463623046875
Validation loss: 2.4081141153971353

Epoch: 5| Step: 6
Training loss: 2.9126458168029785
Validation loss: 2.40541470948086

Epoch: 5| Step: 7
Training loss: 2.865774393081665
Validation loss: 2.406345505868235

Epoch: 5| Step: 8
Training loss: 2.4175822734832764
Validation loss: 2.406271952454762

Epoch: 5| Step: 9
Training loss: 2.6092617511749268
Validation loss: 2.4070221352320846

Epoch: 5| Step: 10
Training loss: 3.0554144382476807
Validation loss: 2.4008807161802888

Epoch: 175| Step: 0
Training loss: 2.995241165161133
Validation loss: 2.406740503926431

Epoch: 5| Step: 1
Training loss: 2.494725465774536
Validation loss: 2.3992942174275718

Epoch: 5| Step: 2
Training loss: 3.171560764312744
Validation loss: 2.399759115711335

Epoch: 5| Step: 3
Training loss: 2.1813604831695557
Validation loss: 2.3998754614142963

Epoch: 5| Step: 4
Training loss: 2.7154927253723145
Validation loss: 2.4058003015415643

Epoch: 5| Step: 5
Training loss: 3.0993754863739014
Validation loss: 2.404636111310733

Epoch: 5| Step: 6
Training loss: 2.6179561614990234
Validation loss: 2.4050497572909117

Epoch: 5| Step: 7
Training loss: 2.326148509979248
Validation loss: 2.4102993344747894

Epoch: 5| Step: 8
Training loss: 1.6801729202270508
Validation loss: 2.4188940576327744

Epoch: 5| Step: 9
Training loss: 2.781517505645752
Validation loss: 2.415637613624655

Epoch: 5| Step: 10
Training loss: 2.8391661643981934
Validation loss: 2.412518634591051

Testing loss: 2.567158235443963

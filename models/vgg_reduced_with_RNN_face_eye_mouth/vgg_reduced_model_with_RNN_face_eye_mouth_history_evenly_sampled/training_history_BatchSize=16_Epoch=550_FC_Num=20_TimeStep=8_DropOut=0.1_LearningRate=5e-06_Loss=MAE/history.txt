Epoch: 1| Step: 0
Training loss: 4.899344444274902
Validation loss: 5.259753545125325

Epoch: 6| Step: 1
Training loss: 4.398078918457031
Validation loss: 5.254482484632923

Epoch: 6| Step: 2
Training loss: 3.9989824295043945
Validation loss: 5.248544029010239

Epoch: 6| Step: 3
Training loss: 5.166502952575684
Validation loss: 5.242559617565524

Epoch: 6| Step: 4
Training loss: 6.579856872558594
Validation loss: 5.237462992309242

Epoch: 6| Step: 5
Training loss: 5.055756092071533
Validation loss: 5.232182082309518

Epoch: 6| Step: 6
Training loss: 5.262209892272949
Validation loss: 5.226895424627489

Epoch: 6| Step: 7
Training loss: 4.224312782287598
Validation loss: 5.2223401479823615

Epoch: 6| Step: 8
Training loss: 4.388311386108398
Validation loss: 5.216866349661222

Epoch: 6| Step: 9
Training loss: 6.975968360900879
Validation loss: 5.211593709966188

Epoch: 6| Step: 10
Training loss: 4.42872428894043
Validation loss: 5.206825169183874

Epoch: 6| Step: 11
Training loss: 5.6087517738342285
Validation loss: 5.20123468419557

Epoch: 6| Step: 12
Training loss: 4.533624172210693
Validation loss: 5.195834759742983

Epoch: 6| Step: 13
Training loss: 4.5897369384765625
Validation loss: 5.190254231934906

Epoch: 2| Step: 0
Training loss: 3.960512399673462
Validation loss: 5.1843565869075

Epoch: 6| Step: 1
Training loss: 5.253053188323975
Validation loss: 5.17860201866396

Epoch: 6| Step: 2
Training loss: 4.161093235015869
Validation loss: 5.172099318555606

Epoch: 6| Step: 3
Training loss: 5.426394939422607
Validation loss: 5.1657695257535545

Epoch: 6| Step: 4
Training loss: 5.110297203063965
Validation loss: 5.159138587213332

Epoch: 6| Step: 5
Training loss: 6.81170654296875
Validation loss: 5.152022192555089

Epoch: 6| Step: 6
Training loss: 5.728910446166992
Validation loss: 5.144671347833449

Epoch: 6| Step: 7
Training loss: 4.168352127075195
Validation loss: 5.136774806566136

Epoch: 6| Step: 8
Training loss: 3.4706802368164062
Validation loss: 5.129373681160711

Epoch: 6| Step: 9
Training loss: 4.362097263336182
Validation loss: 5.12068199342297

Epoch: 6| Step: 10
Training loss: 4.836301803588867
Validation loss: 5.111365149098058

Epoch: 6| Step: 11
Training loss: 5.849601745605469
Validation loss: 5.102024129641953

Epoch: 6| Step: 12
Training loss: 4.401932716369629
Validation loss: 5.092654166683074

Epoch: 6| Step: 13
Training loss: 5.969583034515381
Validation loss: 5.082249867018833

Epoch: 3| Step: 0
Training loss: 4.857550621032715
Validation loss: 5.071258288557812

Epoch: 6| Step: 1
Training loss: 4.572373390197754
Validation loss: 5.0600915724231355

Epoch: 6| Step: 2
Training loss: 3.626864433288574
Validation loss: 5.048375073299613

Epoch: 6| Step: 3
Training loss: 4.8741278648376465
Validation loss: 5.036067101263231

Epoch: 6| Step: 4
Training loss: 4.067410469055176
Validation loss: 5.023715629372545

Epoch: 6| Step: 5
Training loss: 5.407818794250488
Validation loss: 5.0105316613310125

Epoch: 6| Step: 6
Training loss: 5.159518718719482
Validation loss: 4.996002879194034

Epoch: 6| Step: 7
Training loss: 5.063937187194824
Validation loss: 4.9814716410893265

Epoch: 6| Step: 8
Training loss: 5.0945515632629395
Validation loss: 4.966969146523424

Epoch: 6| Step: 9
Training loss: 5.519662857055664
Validation loss: 4.951312254833919

Epoch: 6| Step: 10
Training loss: 4.460887908935547
Validation loss: 4.933843463979741

Epoch: 6| Step: 11
Training loss: 5.203161239624023
Validation loss: 4.916453597366169

Epoch: 6| Step: 12
Training loss: 3.1621508598327637
Validation loss: 4.899676325500653

Epoch: 6| Step: 13
Training loss: 6.609800338745117
Validation loss: 4.879935510696903

Epoch: 4| Step: 0
Training loss: 5.8361663818359375
Validation loss: 4.860758335359635

Epoch: 6| Step: 1
Training loss: 4.635690689086914
Validation loss: 4.841384369839904

Epoch: 6| Step: 2
Training loss: 4.368215560913086
Validation loss: 4.82021475350985

Epoch: 6| Step: 3
Training loss: 5.159136772155762
Validation loss: 4.799663477046515

Epoch: 6| Step: 4
Training loss: 3.472702980041504
Validation loss: 4.778210537407988

Epoch: 6| Step: 5
Training loss: 3.7729132175445557
Validation loss: 4.756316731053014

Epoch: 6| Step: 6
Training loss: 4.5322489738464355
Validation loss: 4.733825852794032

Epoch: 6| Step: 7
Training loss: 4.334355354309082
Validation loss: 4.711657975309638

Epoch: 6| Step: 8
Training loss: 4.597573757171631
Validation loss: 4.688427130381267

Epoch: 6| Step: 9
Training loss: 4.7304301261901855
Validation loss: 4.66630716734035

Epoch: 6| Step: 10
Training loss: 5.057986259460449
Validation loss: 4.640873765432707

Epoch: 6| Step: 11
Training loss: 4.705609321594238
Validation loss: 4.616626734374671

Epoch: 6| Step: 12
Training loss: 3.5070245265960693
Validation loss: 4.5932813306008615

Epoch: 6| Step: 13
Training loss: 4.273325443267822
Validation loss: 4.567375375378516

Epoch: 5| Step: 0
Training loss: 6.412604808807373
Validation loss: 4.542804456526233

Epoch: 6| Step: 1
Training loss: 5.880383014678955
Validation loss: 4.517168732099636

Epoch: 6| Step: 2
Training loss: 4.111253261566162
Validation loss: 4.492423242138278

Epoch: 6| Step: 3
Training loss: 4.07530403137207
Validation loss: 4.466819660637968

Epoch: 6| Step: 4
Training loss: 3.945202350616455
Validation loss: 4.439863040883054

Epoch: 6| Step: 5
Training loss: 4.1754279136657715
Validation loss: 4.415685617795554

Epoch: 6| Step: 6
Training loss: 3.587738513946533
Validation loss: 4.390767828110726

Epoch: 6| Step: 7
Training loss: 5.456170082092285
Validation loss: 4.364485166406118

Epoch: 6| Step: 8
Training loss: 3.2587268352508545
Validation loss: 4.3402924230021815

Epoch: 6| Step: 9
Training loss: 2.159247398376465
Validation loss: 4.31497052920762

Epoch: 6| Step: 10
Training loss: 4.335745811462402
Validation loss: 4.293385277512253

Epoch: 6| Step: 11
Training loss: 4.133794784545898
Validation loss: 4.268352272689984

Epoch: 6| Step: 12
Training loss: 4.143238067626953
Validation loss: 4.246952661903956

Epoch: 6| Step: 13
Training loss: 1.9390490055084229
Validation loss: 4.222297653075187

Epoch: 6| Step: 0
Training loss: 4.581284046173096
Validation loss: 4.1968239507367535

Epoch: 6| Step: 1
Training loss: 3.53550386428833
Validation loss: 4.173457478964201

Epoch: 6| Step: 2
Training loss: 4.212688446044922
Validation loss: 4.146809429250737

Epoch: 6| Step: 3
Training loss: 3.8447072505950928
Validation loss: 4.120186272487845

Epoch: 6| Step: 4
Training loss: 4.418488502502441
Validation loss: 4.094094643028834

Epoch: 6| Step: 5
Training loss: 2.668984889984131
Validation loss: 4.062711864389399

Epoch: 6| Step: 6
Training loss: 2.7188572883605957
Validation loss: 4.036843179374613

Epoch: 6| Step: 7
Training loss: 4.661513328552246
Validation loss: 4.00743963385141

Epoch: 6| Step: 8
Training loss: 3.224121570587158
Validation loss: 3.9801731622347267

Epoch: 6| Step: 9
Training loss: 5.483174800872803
Validation loss: 3.95355466617051

Epoch: 6| Step: 10
Training loss: 2.5707225799560547
Validation loss: 3.9269502906389135

Epoch: 6| Step: 11
Training loss: 3.551867961883545
Validation loss: 3.896190163909748

Epoch: 6| Step: 12
Training loss: 4.7358574867248535
Validation loss: 3.8705368195810625

Epoch: 6| Step: 13
Training loss: 4.035712242126465
Validation loss: 3.8479881337893906

Epoch: 7| Step: 0
Training loss: 3.544076919555664
Validation loss: 3.82488674758583

Epoch: 6| Step: 1
Training loss: 3.982794761657715
Validation loss: 3.803402390531314

Epoch: 6| Step: 2
Training loss: 2.985234260559082
Validation loss: 3.782365260585662

Epoch: 6| Step: 3
Training loss: 3.7133781909942627
Validation loss: 3.762788385473272

Epoch: 6| Step: 4
Training loss: 3.1737468242645264
Validation loss: 3.747416598822481

Epoch: 6| Step: 5
Training loss: 3.8772196769714355
Validation loss: 3.725972939563054

Epoch: 6| Step: 6
Training loss: 4.168129920959473
Validation loss: 3.710615957936933

Epoch: 6| Step: 7
Training loss: 3.5354623794555664
Validation loss: 3.6940919994026102

Epoch: 6| Step: 8
Training loss: 4.403595924377441
Validation loss: 3.6783597546239055

Epoch: 6| Step: 9
Training loss: 3.236698627471924
Validation loss: 3.6611699186345583

Epoch: 6| Step: 10
Training loss: 3.151376724243164
Validation loss: 3.643421544823595

Epoch: 6| Step: 11
Training loss: 3.407708168029785
Validation loss: 3.626665087156398

Epoch: 6| Step: 12
Training loss: 2.745039463043213
Validation loss: 3.6090351894337642

Epoch: 6| Step: 13
Training loss: 5.192256927490234
Validation loss: 3.597003757312734

Epoch: 8| Step: 0
Training loss: 3.1397294998168945
Validation loss: 3.5790658356041036

Epoch: 6| Step: 1
Training loss: 3.8606491088867188
Validation loss: 3.5648622128271286

Epoch: 6| Step: 2
Training loss: 2.1944830417633057
Validation loss: 3.5521882426354194

Epoch: 6| Step: 3
Training loss: 3.837940216064453
Validation loss: 3.535166786563012

Epoch: 6| Step: 4
Training loss: 3.009443759918213
Validation loss: 3.5236809176783406

Epoch: 6| Step: 5
Training loss: 4.073107719421387
Validation loss: 3.5099192434741604

Epoch: 6| Step: 6
Training loss: 4.302969932556152
Validation loss: 3.499254321539274

Epoch: 6| Step: 7
Training loss: 4.37668514251709
Validation loss: 3.490625019996397

Epoch: 6| Step: 8
Training loss: 3.707549571990967
Validation loss: 3.47880462420884

Epoch: 6| Step: 9
Training loss: 3.559723377227783
Validation loss: 3.4664383011479534

Epoch: 6| Step: 10
Training loss: 3.630801200866699
Validation loss: 3.457323299941196

Epoch: 6| Step: 11
Training loss: 2.3221564292907715
Validation loss: 3.4492049755588656

Epoch: 6| Step: 12
Training loss: 3.4453635215759277
Validation loss: 3.4398860777578046

Epoch: 6| Step: 13
Training loss: 1.8572232723236084
Validation loss: 3.431434669802266

Epoch: 9| Step: 0
Training loss: 3.7470922470092773
Validation loss: 3.4211490948994956

Epoch: 6| Step: 1
Training loss: 2.610443592071533
Validation loss: 3.411035273664741

Epoch: 6| Step: 2
Training loss: 4.243659973144531
Validation loss: 3.4070168464414534

Epoch: 6| Step: 3
Training loss: 3.0945885181427
Validation loss: 3.3966695031812115

Epoch: 6| Step: 4
Training loss: 3.4600348472595215
Validation loss: 3.3877790153667493

Epoch: 6| Step: 5
Training loss: 2.613534927368164
Validation loss: 3.3805487309732745

Epoch: 6| Step: 6
Training loss: 3.2080492973327637
Validation loss: 3.372681766427973

Epoch: 6| Step: 7
Training loss: 2.936166763305664
Validation loss: 3.363158779759561

Epoch: 6| Step: 8
Training loss: 3.8783578872680664
Validation loss: 3.356762173355267

Epoch: 6| Step: 9
Training loss: 3.5379538536071777
Validation loss: 3.3481812502748225

Epoch: 6| Step: 10
Training loss: 2.8995532989501953
Validation loss: 3.3411932094122774

Epoch: 6| Step: 11
Training loss: 3.8414652347564697
Validation loss: 3.3348952006268244

Epoch: 6| Step: 12
Training loss: 3.1662724018096924
Validation loss: 3.3276411974301903

Epoch: 6| Step: 13
Training loss: 3.3144800662994385
Validation loss: 3.321967899158437

Epoch: 10| Step: 0
Training loss: 2.8352231979370117
Validation loss: 3.319713169528592

Epoch: 6| Step: 1
Training loss: 3.5956597328186035
Validation loss: 3.3115839112189507

Epoch: 6| Step: 2
Training loss: 3.2010397911071777
Validation loss: 3.3061109409537366

Epoch: 6| Step: 3
Training loss: 2.758164644241333
Validation loss: 3.3001777228488716

Epoch: 6| Step: 4
Training loss: 3.182331085205078
Validation loss: 3.2932970703289075

Epoch: 6| Step: 5
Training loss: 2.9094808101654053
Validation loss: 3.286561401941443

Epoch: 6| Step: 6
Training loss: 4.997086524963379
Validation loss: 3.2833480578596874

Epoch: 6| Step: 7
Training loss: 2.8348164558410645
Validation loss: 3.2767536999076925

Epoch: 6| Step: 8
Training loss: 3.1153595447540283
Validation loss: 3.270301524028983

Epoch: 6| Step: 9
Training loss: 3.2275550365448
Validation loss: 3.265668322963099

Epoch: 6| Step: 10
Training loss: 3.2173049449920654
Validation loss: 3.258530770578692

Epoch: 6| Step: 11
Training loss: 3.568676471710205
Validation loss: 3.2548644517057683

Epoch: 6| Step: 12
Training loss: 2.754204750061035
Validation loss: 3.247004485899402

Epoch: 6| Step: 13
Training loss: 3.3611059188842773
Validation loss: 3.2405610956171507

Epoch: 11| Step: 0
Training loss: 2.8892548084259033
Validation loss: 3.2379848880152546

Epoch: 6| Step: 1
Training loss: 3.0483222007751465
Validation loss: 3.242654344087006

Epoch: 6| Step: 2
Training loss: 3.121858596801758
Validation loss: 3.230262420510733

Epoch: 6| Step: 3
Training loss: 3.07067608833313
Validation loss: 3.2236922043626026

Epoch: 6| Step: 4
Training loss: 2.6275863647460938
Validation loss: 3.2160606871369066

Epoch: 6| Step: 5
Training loss: 2.94207763671875
Validation loss: 3.212393001843524

Epoch: 6| Step: 6
Training loss: 3.5287742614746094
Validation loss: 3.2109130479956187

Epoch: 6| Step: 7
Training loss: 4.663877487182617
Validation loss: 3.206143574048114

Epoch: 6| Step: 8
Training loss: 3.0859482288360596
Validation loss: 3.2039759774361887

Epoch: 6| Step: 9
Training loss: 2.9218616485595703
Validation loss: 3.1980410314375356

Epoch: 6| Step: 10
Training loss: 2.7690770626068115
Validation loss: 3.1935782124919276

Epoch: 6| Step: 11
Training loss: 3.4951043128967285
Validation loss: 3.186322066091722

Epoch: 6| Step: 12
Training loss: 3.3442225456237793
Validation loss: 3.186522417171027

Epoch: 6| Step: 13
Training loss: 3.3447482585906982
Validation loss: 3.1800328198299614

Epoch: 12| Step: 0
Training loss: 3.662748098373413
Validation loss: 3.1760211657452326

Epoch: 6| Step: 1
Training loss: 3.0335750579833984
Validation loss: 3.1723388343729

Epoch: 6| Step: 2
Training loss: 4.652748107910156
Validation loss: 3.162879459319576

Epoch: 6| Step: 3
Training loss: 3.15671443939209
Validation loss: 3.1640665249157975

Epoch: 6| Step: 4
Training loss: 3.4648032188415527
Validation loss: 3.162222082896899

Epoch: 6| Step: 5
Training loss: 3.211146354675293
Validation loss: 3.159531078030986

Epoch: 6| Step: 6
Training loss: 2.4984424114227295
Validation loss: 3.1552151967120428

Epoch: 6| Step: 7
Training loss: 3.2328174114227295
Validation loss: 3.1578194813061784

Epoch: 6| Step: 8
Training loss: 3.3157660961151123
Validation loss: 3.156495004571894

Epoch: 6| Step: 9
Training loss: 1.9237958192825317
Validation loss: 3.1534036205660914

Epoch: 6| Step: 10
Training loss: 2.3112709522247314
Validation loss: 3.1487954508873726

Epoch: 6| Step: 11
Training loss: 4.108542442321777
Validation loss: 3.142528495480937

Epoch: 6| Step: 12
Training loss: 2.9627182483673096
Validation loss: 3.1329639957797144

Epoch: 6| Step: 13
Training loss: 2.366511344909668
Validation loss: 3.1282288669258036

Epoch: 13| Step: 0
Training loss: 2.8827340602874756
Validation loss: 3.127813898107057

Epoch: 6| Step: 1
Training loss: 2.579129695892334
Validation loss: 3.1285615454437914

Epoch: 6| Step: 2
Training loss: 3.024888515472412
Validation loss: 3.1256611424107708

Epoch: 6| Step: 3
Training loss: 3.687291145324707
Validation loss: 3.124922926707934

Epoch: 6| Step: 4
Training loss: 2.857577085494995
Validation loss: 3.122191285574308

Epoch: 6| Step: 5
Training loss: 3.0008935928344727
Validation loss: 3.116814290323565

Epoch: 6| Step: 6
Training loss: 3.3638784885406494
Validation loss: 3.1152304064842964

Epoch: 6| Step: 7
Training loss: 3.089984893798828
Validation loss: 3.110724569648825

Epoch: 6| Step: 8
Training loss: 3.6858439445495605
Validation loss: 3.1067381340970277

Epoch: 6| Step: 9
Training loss: 2.9027771949768066
Validation loss: 3.1013855934143066

Epoch: 6| Step: 10
Training loss: 3.0277657508850098
Validation loss: 3.1004522308226554

Epoch: 6| Step: 11
Training loss: 3.0103185176849365
Validation loss: 3.0987487864750687

Epoch: 6| Step: 12
Training loss: 3.6839871406555176
Validation loss: 3.095741512954876

Epoch: 6| Step: 13
Training loss: 2.9789929389953613
Validation loss: 3.09415251721618

Epoch: 14| Step: 0
Training loss: 3.3360142707824707
Validation loss: 3.0917129080782653

Epoch: 6| Step: 1
Training loss: 3.5010902881622314
Validation loss: 3.0877082142778622

Epoch: 6| Step: 2
Training loss: 2.8662877082824707
Validation loss: 3.0804663909378873

Epoch: 6| Step: 3
Training loss: 3.103266954421997
Validation loss: 3.0757513276992308

Epoch: 6| Step: 4
Training loss: 3.1530840396881104
Validation loss: 3.0752364794413247

Epoch: 6| Step: 5
Training loss: 2.579357624053955
Validation loss: 3.0694216682064916

Epoch: 6| Step: 6
Training loss: 3.7947263717651367
Validation loss: 3.0652786531756

Epoch: 6| Step: 7
Training loss: 2.607036590576172
Validation loss: 3.0625269566812823

Epoch: 6| Step: 8
Training loss: 3.155426263809204
Validation loss: 3.0595108770555064

Epoch: 6| Step: 9
Training loss: 3.05592679977417
Validation loss: 3.0551747916847147

Epoch: 6| Step: 10
Training loss: 2.95332670211792
Validation loss: 3.051167739334927

Epoch: 6| Step: 11
Training loss: 2.915318489074707
Validation loss: 3.0508452923067155

Epoch: 6| Step: 12
Training loss: 3.3619232177734375
Validation loss: 3.0460065487892396

Epoch: 6| Step: 13
Training loss: 2.992863655090332
Validation loss: 3.043639244571809

Epoch: 15| Step: 0
Training loss: 2.8335437774658203
Validation loss: 3.0470516245852233

Epoch: 6| Step: 1
Training loss: 4.718893051147461
Validation loss: 3.047042067332934

Epoch: 6| Step: 2
Training loss: 2.859649896621704
Validation loss: 3.0541889052237234

Epoch: 6| Step: 3
Training loss: 1.6120151281356812
Validation loss: 3.049829885523806

Epoch: 6| Step: 4
Training loss: 2.998805046081543
Validation loss: 3.0434430901722243

Epoch: 6| Step: 5
Training loss: 2.235502004623413
Validation loss: 3.035474190147974

Epoch: 6| Step: 6
Training loss: 3.2515170574188232
Validation loss: 3.0298170351213023

Epoch: 6| Step: 7
Training loss: 2.336369037628174
Validation loss: 3.0257963800943024

Epoch: 6| Step: 8
Training loss: 3.8808517456054688
Validation loss: 3.024788748833441

Epoch: 6| Step: 9
Training loss: 2.8448047637939453
Validation loss: 3.0167283960568008

Epoch: 6| Step: 10
Training loss: 4.53069543838501
Validation loss: 3.0140259035172

Epoch: 6| Step: 11
Training loss: 3.319676637649536
Validation loss: 3.014414479655604

Epoch: 6| Step: 12
Training loss: 3.1189475059509277
Validation loss: 3.009338814725158

Epoch: 6| Step: 13
Training loss: 2.0358386039733887
Validation loss: 3.0075929985251477

Epoch: 16| Step: 0
Training loss: 3.1946120262145996
Validation loss: 3.0022317747915945

Epoch: 6| Step: 1
Training loss: 2.9234280586242676
Validation loss: 2.999681054904897

Epoch: 6| Step: 2
Training loss: 2.824763298034668
Validation loss: 2.9971620087982505

Epoch: 6| Step: 3
Training loss: 3.0283408164978027
Validation loss: 2.999081857742802

Epoch: 6| Step: 4
Training loss: 2.872730016708374
Validation loss: 2.994756514026273

Epoch: 6| Step: 5
Training loss: 2.4954257011413574
Validation loss: 2.998356250024611

Epoch: 6| Step: 6
Training loss: 3.2317581176757812
Validation loss: 2.9925476351091937

Epoch: 6| Step: 7
Training loss: 3.0913963317871094
Validation loss: 2.989414720125096

Epoch: 6| Step: 8
Training loss: 3.2311782836914062
Validation loss: 2.9867179650132374

Epoch: 6| Step: 9
Training loss: 2.8515684604644775
Validation loss: 2.982420608561526

Epoch: 6| Step: 10
Training loss: 3.029477596282959
Validation loss: 2.974134393917617

Epoch: 6| Step: 11
Training loss: 2.974099636077881
Validation loss: 2.9741069527082544

Epoch: 6| Step: 12
Training loss: 3.502924919128418
Validation loss: 2.970038793420279

Epoch: 6| Step: 13
Training loss: 3.69447922706604
Validation loss: 2.964471158160958

Epoch: 17| Step: 0
Training loss: 2.78570556640625
Validation loss: 2.9664131620878815

Epoch: 6| Step: 1
Training loss: 3.129033088684082
Validation loss: 2.964377728841638

Epoch: 6| Step: 2
Training loss: 3.4539809226989746
Validation loss: 2.969179017569429

Epoch: 6| Step: 3
Training loss: 2.729024887084961
Validation loss: 2.9694264832363335

Epoch: 6| Step: 4
Training loss: 1.9208948612213135
Validation loss: 2.9662112266786638

Epoch: 6| Step: 5
Training loss: 2.690138816833496
Validation loss: 2.955001292690154

Epoch: 6| Step: 6
Training loss: 3.7330474853515625
Validation loss: 2.954331497992239

Epoch: 6| Step: 7
Training loss: 3.1655306816101074
Validation loss: 2.950112529980239

Epoch: 6| Step: 8
Training loss: 3.667987585067749
Validation loss: 2.9494792107612855

Epoch: 6| Step: 9
Training loss: 2.9274587631225586
Validation loss: 2.9480067863259265

Epoch: 6| Step: 10
Training loss: 3.185708999633789
Validation loss: 2.943281183960617

Epoch: 6| Step: 11
Training loss: 3.794858455657959
Validation loss: 2.9412842873604066

Epoch: 6| Step: 12
Training loss: 2.2565979957580566
Validation loss: 2.9391940075864076

Epoch: 6| Step: 13
Training loss: 2.7745256423950195
Validation loss: 2.935369635141024

Epoch: 18| Step: 0
Training loss: 2.808326005935669
Validation loss: 2.9357789280594035

Epoch: 6| Step: 1
Training loss: 3.230544328689575
Validation loss: 2.9331886204340125

Epoch: 6| Step: 2
Training loss: 3.169062614440918
Validation loss: 2.9420375926520235

Epoch: 6| Step: 3
Training loss: 2.9345626831054688
Validation loss: 2.9436017185129146

Epoch: 6| Step: 4
Training loss: 3.407668352127075
Validation loss: 2.942320357086838

Epoch: 6| Step: 5
Training loss: 2.7499217987060547
Validation loss: 2.940109629784861

Epoch: 6| Step: 6
Training loss: 2.4936537742614746
Validation loss: 2.9302450354381273

Epoch: 6| Step: 7
Training loss: 3.164771795272827
Validation loss: 2.9297292232513428

Epoch: 6| Step: 8
Training loss: 2.7588577270507812
Validation loss: 2.921912803444811

Epoch: 6| Step: 9
Training loss: 3.1007590293884277
Validation loss: 2.9233060447118615

Epoch: 6| Step: 10
Training loss: 3.4760842323303223
Validation loss: 2.922579267973541

Epoch: 6| Step: 11
Training loss: 2.7465078830718994
Validation loss: 2.9194861663285123

Epoch: 6| Step: 12
Training loss: 3.7731685638427734
Validation loss: 2.915125931462934

Epoch: 6| Step: 13
Training loss: 1.6512091159820557
Validation loss: 2.9098156652142926

Epoch: 19| Step: 0
Training loss: 2.7863988876342773
Validation loss: 2.9083627987933416

Epoch: 6| Step: 1
Training loss: 3.1295342445373535
Validation loss: 2.9101898439468874

Epoch: 6| Step: 2
Training loss: 2.646050214767456
Validation loss: 2.9110195200930358

Epoch: 6| Step: 3
Training loss: 2.7617745399475098
Validation loss: 2.909230537312005

Epoch: 6| Step: 4
Training loss: 3.054950714111328
Validation loss: 2.9049049141586467

Epoch: 6| Step: 5
Training loss: 2.6577847003936768
Validation loss: 2.9086050243787867

Epoch: 6| Step: 6
Training loss: 3.442080497741699
Validation loss: 2.907446963812715

Epoch: 6| Step: 7
Training loss: 3.6290249824523926
Validation loss: 2.8990646305904595

Epoch: 6| Step: 8
Training loss: 2.913534641265869
Validation loss: 2.900256126157699

Epoch: 6| Step: 9
Training loss: 3.1642026901245117
Validation loss: 2.897417317154587

Epoch: 6| Step: 10
Training loss: 3.1554183959960938
Validation loss: 2.894797637898435

Epoch: 6| Step: 11
Training loss: 3.3484385013580322
Validation loss: 2.897209116207656

Epoch: 6| Step: 12
Training loss: 2.4647674560546875
Validation loss: 2.8954318159370014

Epoch: 6| Step: 13
Training loss: 2.346961736679077
Validation loss: 2.8904971307323826

Epoch: 20| Step: 0
Training loss: 2.5221707820892334
Validation loss: 2.894379372237831

Epoch: 6| Step: 1
Training loss: 3.7904679775238037
Validation loss: 2.8910745113126692

Epoch: 6| Step: 2
Training loss: 3.316054105758667
Validation loss: 2.886271640818606

Epoch: 6| Step: 3
Training loss: 2.4743237495422363
Validation loss: 2.885527346723823

Epoch: 6| Step: 4
Training loss: 3.253828287124634
Validation loss: 2.8815577466000795

Epoch: 6| Step: 5
Training loss: 2.7124335765838623
Validation loss: 2.881084616466235

Epoch: 6| Step: 6
Training loss: 2.955595016479492
Validation loss: 2.8757840561610397

Epoch: 6| Step: 7
Training loss: 2.423656940460205
Validation loss: 2.874508447544549

Epoch: 6| Step: 8
Training loss: 2.580850839614868
Validation loss: 2.8744073708852134

Epoch: 6| Step: 9
Training loss: 3.2226076126098633
Validation loss: 2.8735677683225243

Epoch: 6| Step: 10
Training loss: 3.458528995513916
Validation loss: 2.872209679695868

Epoch: 6| Step: 11
Training loss: 3.5274765491485596
Validation loss: 2.871789093940489

Epoch: 6| Step: 12
Training loss: 2.8297531604766846
Validation loss: 2.8669581233814196

Epoch: 6| Step: 13
Training loss: 2.08695387840271
Validation loss: 2.8673160511960267

Epoch: 21| Step: 0
Training loss: 3.070816993713379
Validation loss: 2.8729478287440475

Epoch: 6| Step: 1
Training loss: 3.0299203395843506
Validation loss: 2.8765668151199177

Epoch: 6| Step: 2
Training loss: 3.36759614944458
Validation loss: 2.880165607698502

Epoch: 6| Step: 3
Training loss: 2.9111459255218506
Validation loss: 2.8698726828380297

Epoch: 6| Step: 4
Training loss: 2.7582616806030273
Validation loss: 2.8718137305269957

Epoch: 6| Step: 5
Training loss: 3.6730422973632812
Validation loss: 2.870690368836926

Epoch: 6| Step: 6
Training loss: 2.9024603366851807
Validation loss: 2.865124974199521

Epoch: 6| Step: 7
Training loss: 2.9315595626831055
Validation loss: 2.855915956599738

Epoch: 6| Step: 8
Training loss: 3.123244285583496
Validation loss: 2.854777028483729

Epoch: 6| Step: 9
Training loss: 3.1949663162231445
Validation loss: 2.855189710535029

Epoch: 6| Step: 10
Training loss: 2.7190752029418945
Validation loss: 2.8596250985258367

Epoch: 6| Step: 11
Training loss: 2.487407684326172
Validation loss: 2.8526297794875277

Epoch: 6| Step: 12
Training loss: 2.5546813011169434
Validation loss: 2.8567847231382966

Epoch: 6| Step: 13
Training loss: 2.539118766784668
Validation loss: 2.8582824455794467

Epoch: 22| Step: 0
Training loss: 2.867746114730835
Validation loss: 2.849990931890344

Epoch: 6| Step: 1
Training loss: 3.337157726287842
Validation loss: 2.850894566505186

Epoch: 6| Step: 2
Training loss: 2.1290454864501953
Validation loss: 2.852847099304199

Epoch: 6| Step: 3
Training loss: 3.585761070251465
Validation loss: 2.852915383154346

Epoch: 6| Step: 4
Training loss: 2.237875461578369
Validation loss: 2.8547897287594375

Epoch: 6| Step: 5
Training loss: 2.5884833335876465
Validation loss: 2.8645969616469515

Epoch: 6| Step: 6
Training loss: 3.6123886108398438
Validation loss: 2.878582400660361

Epoch: 6| Step: 7
Training loss: 3.1442503929138184
Validation loss: 2.881410955100931

Epoch: 6| Step: 8
Training loss: 3.6273751258850098
Validation loss: 2.8707946064651653

Epoch: 6| Step: 9
Training loss: 3.3131027221679688
Validation loss: 2.853913281553535

Epoch: 6| Step: 10
Training loss: 2.402617931365967
Validation loss: 2.840419255277162

Epoch: 6| Step: 11
Training loss: 3.6513009071350098
Validation loss: 2.8341638452263287

Epoch: 6| Step: 12
Training loss: 1.9967447519302368
Validation loss: 2.8342586486570296

Epoch: 6| Step: 13
Training loss: 2.730738639831543
Validation loss: 2.842914955590361

Epoch: 23| Step: 0
Training loss: 2.965329647064209
Validation loss: 2.8440911000774753

Epoch: 6| Step: 1
Training loss: 2.86008620262146
Validation loss: 2.8415262417126725

Epoch: 6| Step: 2
Training loss: 3.4247896671295166
Validation loss: 2.8369468489000873

Epoch: 6| Step: 3
Training loss: 2.7293951511383057
Validation loss: 2.837905499242967

Epoch: 6| Step: 4
Training loss: 3.091691493988037
Validation loss: 2.839267223112045

Epoch: 6| Step: 5
Training loss: 2.6905903816223145
Validation loss: 2.8262324281918105

Epoch: 6| Step: 6
Training loss: 3.3220248222351074
Validation loss: 2.827975409005278

Epoch: 6| Step: 7
Training loss: 1.8470067977905273
Validation loss: 2.826280504144648

Epoch: 6| Step: 8
Training loss: 3.274888038635254
Validation loss: 2.83242138226827

Epoch: 6| Step: 9
Training loss: 3.3527450561523438
Validation loss: 2.830012285581199

Epoch: 6| Step: 10
Training loss: 3.1353955268859863
Validation loss: 2.8269143155826035

Epoch: 6| Step: 11
Training loss: 2.7865636348724365
Validation loss: 2.8285467163208993

Epoch: 6| Step: 12
Training loss: 2.533616781234741
Validation loss: 2.8267422670959146

Epoch: 6| Step: 13
Training loss: 3.343790054321289
Validation loss: 2.828431619110928

Epoch: 24| Step: 0
Training loss: 3.1331515312194824
Validation loss: 2.82490804631223

Epoch: 6| Step: 1
Training loss: 3.2079343795776367
Validation loss: 2.827002212565432

Epoch: 6| Step: 2
Training loss: 3.026557445526123
Validation loss: 2.824387086335049

Epoch: 6| Step: 3
Training loss: 2.3613944053649902
Validation loss: 2.825001662777316

Epoch: 6| Step: 4
Training loss: 2.4925405979156494
Validation loss: 2.82392293407071

Epoch: 6| Step: 5
Training loss: 2.6257333755493164
Validation loss: 2.8219723983477523

Epoch: 6| Step: 6
Training loss: 3.063933849334717
Validation loss: 2.8187686627910984

Epoch: 6| Step: 7
Training loss: 3.9744503498077393
Validation loss: 2.8162622195418163

Epoch: 6| Step: 8
Training loss: 2.919034481048584
Validation loss: 2.81965624132464

Epoch: 6| Step: 9
Training loss: 2.6341309547424316
Validation loss: 2.8199173199233187

Epoch: 6| Step: 10
Training loss: 3.0619957447052
Validation loss: 2.81300030216094

Epoch: 6| Step: 11
Training loss: 2.9447875022888184
Validation loss: 2.814530218801191

Epoch: 6| Step: 12
Training loss: 2.2448630332946777
Validation loss: 2.8076691319865565

Epoch: 6| Step: 13
Training loss: 3.567305326461792
Validation loss: 2.810430260114772

Epoch: 25| Step: 0
Training loss: 2.9551730155944824
Validation loss: 2.8111879492318756

Epoch: 6| Step: 1
Training loss: 3.2356107234954834
Validation loss: 2.8101515718685683

Epoch: 6| Step: 2
Training loss: 3.56414794921875
Validation loss: 2.8095246617512037

Epoch: 6| Step: 3
Training loss: 2.3176941871643066
Validation loss: 2.824348957307877

Epoch: 6| Step: 4
Training loss: 3.1347551345825195
Validation loss: 2.845643569064397

Epoch: 6| Step: 5
Training loss: 2.9767842292785645
Validation loss: 2.854197099644651

Epoch: 6| Step: 6
Training loss: 3.5129551887512207
Validation loss: 2.8532125462767897

Epoch: 6| Step: 7
Training loss: 3.548330307006836
Validation loss: 2.827126097935502

Epoch: 6| Step: 8
Training loss: 3.0738821029663086
Validation loss: 2.810037023277693

Epoch: 6| Step: 9
Training loss: 2.3652021884918213
Validation loss: 2.804816475478552

Epoch: 6| Step: 10
Training loss: 2.479829788208008
Validation loss: 2.8006720337816464

Epoch: 6| Step: 11
Training loss: 3.0014994144439697
Validation loss: 2.8028675689492175

Epoch: 6| Step: 12
Training loss: 2.445199728012085
Validation loss: 2.8013983849556214

Epoch: 6| Step: 13
Training loss: 1.8783138990402222
Validation loss: 2.804443879794049

Epoch: 26| Step: 0
Training loss: 2.504976749420166
Validation loss: 2.802618649698073

Epoch: 6| Step: 1
Training loss: 3.565303325653076
Validation loss: 2.8009786734016995

Epoch: 6| Step: 2
Training loss: 2.6014368534088135
Validation loss: 2.8007507657492035

Epoch: 6| Step: 3
Training loss: 2.6217780113220215
Validation loss: 2.8046137850771666

Epoch: 6| Step: 4
Training loss: 2.6175224781036377
Validation loss: 2.8025443041196434

Epoch: 6| Step: 5
Training loss: 2.8509979248046875
Validation loss: 2.8013133797594296

Epoch: 6| Step: 6
Training loss: 2.270679473876953
Validation loss: 2.804306822438394

Epoch: 6| Step: 7
Training loss: 2.8083252906799316
Validation loss: 2.8070651536346762

Epoch: 6| Step: 8
Training loss: 2.6764121055603027
Validation loss: 2.8029360899361233

Epoch: 6| Step: 9
Training loss: 2.6468372344970703
Validation loss: 2.8050942677323536

Epoch: 6| Step: 10
Training loss: 3.6078100204467773
Validation loss: 2.8091702973970802

Epoch: 6| Step: 11
Training loss: 3.156855583190918
Validation loss: 2.8066155910491943

Epoch: 6| Step: 12
Training loss: 3.2928555011749268
Validation loss: 2.802444370844031

Epoch: 6| Step: 13
Training loss: 4.208962440490723
Validation loss: 2.795164677404588

Epoch: 27| Step: 0
Training loss: 2.5098986625671387
Validation loss: 2.7912064854816725

Epoch: 6| Step: 1
Training loss: 2.7211227416992188
Validation loss: 2.786526600519816

Epoch: 6| Step: 2
Training loss: 2.630979537963867
Validation loss: 2.7907473400074947

Epoch: 6| Step: 3
Training loss: 3.5470314025878906
Validation loss: 2.7932166796858593

Epoch: 6| Step: 4
Training loss: 2.6627485752105713
Validation loss: 2.791843919343846

Epoch: 6| Step: 5
Training loss: 2.290705919265747
Validation loss: 2.79562960901568

Epoch: 6| Step: 6
Training loss: 2.139720916748047
Validation loss: 2.792589915696011

Epoch: 6| Step: 7
Training loss: 3.075441360473633
Validation loss: 2.7923663123961417

Epoch: 6| Step: 8
Training loss: 3.299196720123291
Validation loss: 2.7920339056240615

Epoch: 6| Step: 9
Training loss: 3.1528210639953613
Validation loss: 2.7906999229103007

Epoch: 6| Step: 10
Training loss: 3.3445682525634766
Validation loss: 2.7881283606252363

Epoch: 6| Step: 11
Training loss: 3.387589931488037
Validation loss: 2.786151275839857

Epoch: 6| Step: 12
Training loss: 2.9532830715179443
Validation loss: 2.7879277326727427

Epoch: 6| Step: 13
Training loss: 3.2035984992980957
Validation loss: 2.787317455455821

Epoch: 28| Step: 0
Training loss: 2.5124001502990723
Validation loss: 2.786068618938487

Epoch: 6| Step: 1
Training loss: 2.8361713886260986
Validation loss: 2.784382966256911

Epoch: 6| Step: 2
Training loss: 3.2776763439178467
Validation loss: 2.786795490531511

Epoch: 6| Step: 3
Training loss: 2.452247381210327
Validation loss: 2.786055875080888

Epoch: 6| Step: 4
Training loss: 2.4587173461914062
Validation loss: 2.7873250925412743

Epoch: 6| Step: 5
Training loss: 3.614158868789673
Validation loss: 2.7919844376143588

Epoch: 6| Step: 6
Training loss: 2.4224472045898438
Validation loss: 2.796365476423694

Epoch: 6| Step: 7
Training loss: 2.641745090484619
Validation loss: 2.785109261030792

Epoch: 6| Step: 8
Training loss: 3.008946418762207
Validation loss: 2.784776577385523

Epoch: 6| Step: 9
Training loss: 3.4678778648376465
Validation loss: 2.781990002560359

Epoch: 6| Step: 10
Training loss: 2.922370433807373
Validation loss: 2.774798354794902

Epoch: 6| Step: 11
Training loss: 2.420464515686035
Validation loss: 2.7766217749605895

Epoch: 6| Step: 12
Training loss: 3.287349224090576
Validation loss: 2.7750912763739146

Epoch: 6| Step: 13
Training loss: 3.644307851791382
Validation loss: 2.7728081416058283

Epoch: 29| Step: 0
Training loss: 2.088191270828247
Validation loss: 2.7748590694960726

Epoch: 6| Step: 1
Training loss: 2.7205071449279785
Validation loss: 2.777816369969358

Epoch: 6| Step: 2
Training loss: 4.162163257598877
Validation loss: 2.7759789651440037

Epoch: 6| Step: 3
Training loss: 2.7076191902160645
Validation loss: 2.7812557374277422

Epoch: 6| Step: 4
Training loss: 3.269092082977295
Validation loss: 2.7863829135894775

Epoch: 6| Step: 5
Training loss: 2.8009111881256104
Validation loss: 2.7973192994312575

Epoch: 6| Step: 6
Training loss: 2.8796145915985107
Validation loss: 2.7890225379697737

Epoch: 6| Step: 7
Training loss: 3.274158000946045
Validation loss: 2.771621245209889

Epoch: 6| Step: 8
Training loss: 2.4724276065826416
Validation loss: 2.7718536930699504

Epoch: 6| Step: 9
Training loss: 3.4939894676208496
Validation loss: 2.7693427967768844

Epoch: 6| Step: 10
Training loss: 2.7761521339416504
Validation loss: 2.7678430644414758

Epoch: 6| Step: 11
Training loss: 2.737173557281494
Validation loss: 2.767100662313482

Epoch: 6| Step: 12
Training loss: 2.5286293029785156
Validation loss: 2.7702056438692155

Epoch: 6| Step: 13
Training loss: 2.4702539443969727
Validation loss: 2.7673857955522436

Epoch: 30| Step: 0
Training loss: 3.2142515182495117
Validation loss: 2.7695227130766837

Epoch: 6| Step: 1
Training loss: 2.8548972606658936
Validation loss: 2.769224851362167

Epoch: 6| Step: 2
Training loss: 2.9639015197753906
Validation loss: 2.7674920610202256

Epoch: 6| Step: 3
Training loss: 3.0370731353759766
Validation loss: 2.765873191177204

Epoch: 6| Step: 4
Training loss: 2.958500862121582
Validation loss: 2.7652311786528556

Epoch: 6| Step: 5
Training loss: 1.801894187927246
Validation loss: 2.7636206457691808

Epoch: 6| Step: 6
Training loss: 3.119777202606201
Validation loss: 2.7661409301142537

Epoch: 6| Step: 7
Training loss: 3.794945001602173
Validation loss: 2.7727262025238364

Epoch: 6| Step: 8
Training loss: 2.567732334136963
Validation loss: 2.7721771117179625

Epoch: 6| Step: 9
Training loss: 2.4175963401794434
Validation loss: 2.7634192410335747

Epoch: 6| Step: 10
Training loss: 3.810457944869995
Validation loss: 2.7673674091216056

Epoch: 6| Step: 11
Training loss: 2.1244845390319824
Validation loss: 2.765039800315775

Epoch: 6| Step: 12
Training loss: 3.001375198364258
Validation loss: 2.7683141077718427

Epoch: 6| Step: 13
Training loss: 2.7401185035705566
Validation loss: 2.764972602167437

Epoch: 31| Step: 0
Training loss: 3.3211073875427246
Validation loss: 2.7654178142547607

Epoch: 6| Step: 1
Training loss: 2.585282802581787
Validation loss: 2.7607829211860575

Epoch: 6| Step: 2
Training loss: 2.5643868446350098
Validation loss: 2.7592553041314565

Epoch: 6| Step: 3
Training loss: 3.28897762298584
Validation loss: 2.758150690345354

Epoch: 6| Step: 4
Training loss: 3.262002944946289
Validation loss: 2.755737796906502

Epoch: 6| Step: 5
Training loss: 3.5062828063964844
Validation loss: 2.7555170315568165

Epoch: 6| Step: 6
Training loss: 2.6822574138641357
Validation loss: 2.7530898612032653

Epoch: 6| Step: 7
Training loss: 3.4395766258239746
Validation loss: 2.7569003899892173

Epoch: 6| Step: 8
Training loss: 2.9265100955963135
Validation loss: 2.756005402534239

Epoch: 6| Step: 9
Training loss: 2.641172409057617
Validation loss: 2.754015617473151

Epoch: 6| Step: 10
Training loss: 3.1297318935394287
Validation loss: 2.7562221865500174

Epoch: 6| Step: 11
Training loss: 2.555589199066162
Validation loss: 2.7554913592594925

Epoch: 6| Step: 12
Training loss: 2.356820821762085
Validation loss: 2.7549242768236386

Epoch: 6| Step: 13
Training loss: 1.5894485712051392
Validation loss: 2.758895743277765

Epoch: 32| Step: 0
Training loss: 3.448592185974121
Validation loss: 2.766078920774562

Epoch: 6| Step: 1
Training loss: 3.058102607727051
Validation loss: 2.760108209425403

Epoch: 6| Step: 2
Training loss: 2.9181013107299805
Validation loss: 2.7527790813035864

Epoch: 6| Step: 3
Training loss: 2.971550226211548
Validation loss: 2.75920965081902

Epoch: 6| Step: 4
Training loss: 3.7842912673950195
Validation loss: 2.7532924785408923

Epoch: 6| Step: 5
Training loss: 2.524362802505493
Validation loss: 2.7542022658932592

Epoch: 6| Step: 6
Training loss: 2.369492530822754
Validation loss: 2.7520624796549478

Epoch: 6| Step: 7
Training loss: 2.3586363792419434
Validation loss: 2.7531057839752524

Epoch: 6| Step: 8
Training loss: 2.85967755317688
Validation loss: 2.7531860566908315

Epoch: 6| Step: 9
Training loss: 2.7885890007019043
Validation loss: 2.752869052271689

Epoch: 6| Step: 10
Training loss: 2.499086618423462
Validation loss: 2.7483053258670274

Epoch: 6| Step: 11
Training loss: 3.653674364089966
Validation loss: 2.7449637638625277

Epoch: 6| Step: 12
Training loss: 2.3985090255737305
Validation loss: 2.7453453822802474

Epoch: 6| Step: 13
Training loss: 2.4908387660980225
Validation loss: 2.7469452940007693

Epoch: 33| Step: 0
Training loss: 2.4662551879882812
Validation loss: 2.742953792695076

Epoch: 6| Step: 1
Training loss: 3.2299630641937256
Validation loss: 2.7433628061766266

Epoch: 6| Step: 2
Training loss: 2.4929122924804688
Validation loss: 2.7406828300927275

Epoch: 6| Step: 3
Training loss: 3.018866539001465
Validation loss: 2.7417705084687922

Epoch: 6| Step: 4
Training loss: 3.250152111053467
Validation loss: 2.744251174311484

Epoch: 6| Step: 5
Training loss: 3.441559076309204
Validation loss: 2.7420939322440856

Epoch: 6| Step: 6
Training loss: 2.819828510284424
Validation loss: 2.741182865635041

Epoch: 6| Step: 7
Training loss: 2.4194588661193848
Validation loss: 2.740404080319148

Epoch: 6| Step: 8
Training loss: 1.9351787567138672
Validation loss: 2.7415741464143157

Epoch: 6| Step: 9
Training loss: 3.177485942840576
Validation loss: 2.739977857118012

Epoch: 6| Step: 10
Training loss: 3.5324625968933105
Validation loss: 2.7463302637941096

Epoch: 6| Step: 11
Training loss: 3.7628421783447266
Validation loss: 2.7487777740724626

Epoch: 6| Step: 12
Training loss: 2.2447400093078613
Validation loss: 2.7415811554078133

Epoch: 6| Step: 13
Training loss: 2.2393083572387695
Validation loss: 2.743557007082047

Epoch: 34| Step: 0
Training loss: 2.6102051734924316
Validation loss: 2.743669463742164

Epoch: 6| Step: 1
Training loss: 2.411714553833008
Validation loss: 2.7385833853034565

Epoch: 6| Step: 2
Training loss: 2.9151573181152344
Validation loss: 2.736778187495406

Epoch: 6| Step: 3
Training loss: 3.0998780727386475
Validation loss: 2.737569857669133

Epoch: 6| Step: 4
Training loss: 3.2439937591552734
Validation loss: 2.7374049309761292

Epoch: 6| Step: 5
Training loss: 2.3466081619262695
Validation loss: 2.7345192688767628

Epoch: 6| Step: 6
Training loss: 2.5678935050964355
Validation loss: 2.7337209255464616

Epoch: 6| Step: 7
Training loss: 3.5672693252563477
Validation loss: 2.7310496555861605

Epoch: 6| Step: 8
Training loss: 2.6864161491394043
Validation loss: 2.7355819158656622

Epoch: 6| Step: 9
Training loss: 2.8028860092163086
Validation loss: 2.7338445571161087

Epoch: 6| Step: 10
Training loss: 3.0655713081359863
Validation loss: 2.732309720849478

Epoch: 6| Step: 11
Training loss: 2.6131229400634766
Validation loss: 2.729985406321864

Epoch: 6| Step: 12
Training loss: 3.1439359188079834
Validation loss: 2.73049763453904

Epoch: 6| Step: 13
Training loss: 3.3228986263275146
Validation loss: 2.7289276097410466

Epoch: 35| Step: 0
Training loss: 2.6725969314575195
Validation loss: 2.7324422200520835

Epoch: 6| Step: 1
Training loss: 2.767632007598877
Validation loss: 2.7319262258468138

Epoch: 6| Step: 2
Training loss: 3.2855663299560547
Validation loss: 2.7365006810875347

Epoch: 6| Step: 3
Training loss: 3.424787998199463
Validation loss: 2.736599555579565

Epoch: 6| Step: 4
Training loss: 2.548509120941162
Validation loss: 2.7377146623467885

Epoch: 6| Step: 5
Training loss: 3.144725799560547
Validation loss: 2.73227684984925

Epoch: 6| Step: 6
Training loss: 3.7099862098693848
Validation loss: 2.73627027644906

Epoch: 6| Step: 7
Training loss: 2.735769748687744
Validation loss: 2.7352892532143542

Epoch: 6| Step: 8
Training loss: 2.688103199005127
Validation loss: 2.7328198596995366

Epoch: 6| Step: 9
Training loss: 2.8683791160583496
Validation loss: 2.7341349714545795

Epoch: 6| Step: 10
Training loss: 1.5585556030273438
Validation loss: 2.7268134496545278

Epoch: 6| Step: 11
Training loss: 2.9794158935546875
Validation loss: 2.7250024118731098

Epoch: 6| Step: 12
Training loss: 2.79282808303833
Validation loss: 2.72644228576332

Epoch: 6| Step: 13
Training loss: 2.9241228103637695
Validation loss: 2.7232362352391726

Epoch: 36| Step: 0
Training loss: 2.186978340148926
Validation loss: 2.72278190940939

Epoch: 6| Step: 1
Training loss: 2.7803778648376465
Validation loss: 2.719191684517809

Epoch: 6| Step: 2
Training loss: 2.9368464946746826
Validation loss: 2.721333703687114

Epoch: 6| Step: 3
Training loss: 2.3536243438720703
Validation loss: 2.7165361604382916

Epoch: 6| Step: 4
Training loss: 2.8502416610717773
Validation loss: 2.718737466360933

Epoch: 6| Step: 5
Training loss: 2.8399596214294434
Validation loss: 2.719722596547937

Epoch: 6| Step: 6
Training loss: 2.942676305770874
Validation loss: 2.71813267020769

Epoch: 6| Step: 7
Training loss: 3.9074721336364746
Validation loss: 2.7194152006538967

Epoch: 6| Step: 8
Training loss: 2.7271149158477783
Validation loss: 2.7148393456653883

Epoch: 6| Step: 9
Training loss: 3.0261974334716797
Validation loss: 2.7179498544303318

Epoch: 6| Step: 10
Training loss: 3.4392590522766113
Validation loss: 2.7123669757637927

Epoch: 6| Step: 11
Training loss: 2.4929468631744385
Validation loss: 2.7112957457060456

Epoch: 6| Step: 12
Training loss: 2.808358907699585
Validation loss: 2.7119280215232604

Epoch: 6| Step: 13
Training loss: 2.63529634475708
Validation loss: 2.710853571532875

Epoch: 37| Step: 0
Training loss: 2.958390712738037
Validation loss: 2.7127578232877996

Epoch: 6| Step: 1
Training loss: 3.0013303756713867
Validation loss: 2.71341803509702

Epoch: 6| Step: 2
Training loss: 2.6537389755249023
Validation loss: 2.7150614646173294

Epoch: 6| Step: 3
Training loss: 2.4233882427215576
Validation loss: 2.7180234975712274

Epoch: 6| Step: 4
Training loss: 2.871891498565674
Validation loss: 2.713452628863755

Epoch: 6| Step: 5
Training loss: 3.0690419673919678
Validation loss: 2.718183832783853

Epoch: 6| Step: 6
Training loss: 3.4154210090637207
Validation loss: 2.718963018027685

Epoch: 6| Step: 7
Training loss: 2.688599109649658
Validation loss: 2.7178025245666504

Epoch: 6| Step: 8
Training loss: 2.437903881072998
Validation loss: 2.7167848669072634

Epoch: 6| Step: 9
Training loss: 2.238783359527588
Validation loss: 2.7172944699564288

Epoch: 6| Step: 10
Training loss: 2.102015972137451
Validation loss: 2.7091302692249255

Epoch: 6| Step: 11
Training loss: 2.9501826763153076
Validation loss: 2.703053641062911

Epoch: 6| Step: 12
Training loss: 3.6241703033447266
Validation loss: 2.7019914093837945

Epoch: 6| Step: 13
Training loss: 3.9774365425109863
Validation loss: 2.7039397506303686

Epoch: 38| Step: 0
Training loss: 2.295450210571289
Validation loss: 2.7007418627380044

Epoch: 6| Step: 1
Training loss: 2.5174665451049805
Validation loss: 2.7043810198383946

Epoch: 6| Step: 2
Training loss: 2.817436933517456
Validation loss: 2.7063270076628654

Epoch: 6| Step: 3
Training loss: 3.6681079864501953
Validation loss: 2.7011683551214074

Epoch: 6| Step: 4
Training loss: 2.4900453090667725
Validation loss: 2.7042361767061296

Epoch: 6| Step: 5
Training loss: 2.2082901000976562
Validation loss: 2.7045390657199326

Epoch: 6| Step: 6
Training loss: 2.824063301086426
Validation loss: 2.7018492337196105

Epoch: 6| Step: 7
Training loss: 2.6748592853546143
Validation loss: 2.6971507764631704

Epoch: 6| Step: 8
Training loss: 3.6022491455078125
Validation loss: 2.701853036880493

Epoch: 6| Step: 9
Training loss: 2.970921039581299
Validation loss: 2.7077977298408427

Epoch: 6| Step: 10
Training loss: 2.9021430015563965
Validation loss: 2.6997197494711926

Epoch: 6| Step: 11
Training loss: 3.9898171424865723
Validation loss: 2.6968559629173687

Epoch: 6| Step: 12
Training loss: 2.5385355949401855
Validation loss: 2.694361671324699

Epoch: 6| Step: 13
Training loss: 2.125950813293457
Validation loss: 2.697075133682579

Epoch: 39| Step: 0
Training loss: 2.912569046020508
Validation loss: 2.6996792747128393

Epoch: 6| Step: 1
Training loss: 2.300489902496338
Validation loss: 2.699340098647661

Epoch: 6| Step: 2
Training loss: 3.1896729469299316
Validation loss: 2.7021725023946455

Epoch: 6| Step: 3
Training loss: 3.392979383468628
Validation loss: 2.7002046492791947

Epoch: 6| Step: 4
Training loss: 3.4241602420806885
Validation loss: 2.701441152121431

Epoch: 6| Step: 5
Training loss: 3.0325546264648438
Validation loss: 2.7024825234566965

Epoch: 6| Step: 6
Training loss: 2.5283446311950684
Validation loss: 2.6991223032756517

Epoch: 6| Step: 7
Training loss: 2.6621029376983643
Validation loss: 2.6937456771891606

Epoch: 6| Step: 8
Training loss: 3.076849937438965
Validation loss: 2.694064571011451

Epoch: 6| Step: 9
Training loss: 2.601439952850342
Validation loss: 2.69128079055458

Epoch: 6| Step: 10
Training loss: 2.7853903770446777
Validation loss: 2.6900772535672752

Epoch: 6| Step: 11
Training loss: 2.6909260749816895
Validation loss: 2.686331815617059

Epoch: 6| Step: 12
Training loss: 2.323808431625366
Validation loss: 2.6884447733561196

Epoch: 6| Step: 13
Training loss: 2.9751639366149902
Validation loss: 2.688462083057691

Epoch: 40| Step: 0
Training loss: 3.2530717849731445
Validation loss: 2.6860473373884797

Epoch: 6| Step: 1
Training loss: 2.7305054664611816
Validation loss: 2.6894345744963615

Epoch: 6| Step: 2
Training loss: 2.0189719200134277
Validation loss: 2.6896301495131625

Epoch: 6| Step: 3
Training loss: 2.52152943611145
Validation loss: 2.6871011744263353

Epoch: 6| Step: 4
Training loss: 2.6624948978424072
Validation loss: 2.686848586605441

Epoch: 6| Step: 5
Training loss: 3.1390819549560547
Validation loss: 2.6860481692898657

Epoch: 6| Step: 6
Training loss: 2.248133659362793
Validation loss: 2.6869375705718994

Epoch: 6| Step: 7
Training loss: 2.966386318206787
Validation loss: 2.6860445622474916

Epoch: 6| Step: 8
Training loss: 3.4238646030426025
Validation loss: 2.684189247828658

Epoch: 6| Step: 9
Training loss: 3.013120651245117
Validation loss: 2.6843488011308896

Epoch: 6| Step: 10
Training loss: 3.0997986793518066
Validation loss: 2.6864943017241774

Epoch: 6| Step: 11
Training loss: 3.144775152206421
Validation loss: 2.68317065956772

Epoch: 6| Step: 12
Training loss: 3.001103162765503
Validation loss: 2.6869084450506393

Epoch: 6| Step: 13
Training loss: 2.231475830078125
Validation loss: 2.681795063839164

Epoch: 41| Step: 0
Training loss: 3.0794031620025635
Validation loss: 2.6826390143363708

Epoch: 6| Step: 1
Training loss: 2.719050645828247
Validation loss: 2.6833113085839058

Epoch: 6| Step: 2
Training loss: 2.986935615539551
Validation loss: 2.6886979046688286

Epoch: 6| Step: 3
Training loss: 3.467862606048584
Validation loss: 2.6815915646091586

Epoch: 6| Step: 4
Training loss: 2.3004837036132812
Validation loss: 2.686510632115026

Epoch: 6| Step: 5
Training loss: 2.1513774394989014
Validation loss: 2.6840406028173303

Epoch: 6| Step: 6
Training loss: 3.093503713607788
Validation loss: 2.6905328560900945

Epoch: 6| Step: 7
Training loss: 2.948995351791382
Validation loss: 2.703751076934158

Epoch: 6| Step: 8
Training loss: 2.633910655975342
Validation loss: 2.7059494449246313

Epoch: 6| Step: 9
Training loss: 3.0588197708129883
Validation loss: 2.6988719919676423

Epoch: 6| Step: 10
Training loss: 2.6183643341064453
Validation loss: 2.698398474724062

Epoch: 6| Step: 11
Training loss: 2.041686534881592
Validation loss: 2.697554749827231

Epoch: 6| Step: 12
Training loss: 3.398275375366211
Validation loss: 2.695176334791286

Epoch: 6| Step: 13
Training loss: 3.3412094116210938
Validation loss: 2.6791502801320886

Epoch: 42| Step: 0
Training loss: 2.6815450191497803
Validation loss: 2.6762181840917116

Epoch: 6| Step: 1
Training loss: 2.1800851821899414
Validation loss: 2.6711096430337555

Epoch: 6| Step: 2
Training loss: 2.508035659790039
Validation loss: 2.6792254781210296

Epoch: 6| Step: 3
Training loss: 3.187366247177124
Validation loss: 2.67933674012461

Epoch: 6| Step: 4
Training loss: 3.1043643951416016
Validation loss: 2.677788498581097

Epoch: 6| Step: 5
Training loss: 2.329596519470215
Validation loss: 2.681910760941044

Epoch: 6| Step: 6
Training loss: 3.1813695430755615
Validation loss: 2.6804347653542795

Epoch: 6| Step: 7
Training loss: 3.7471604347229004
Validation loss: 2.67795124617956

Epoch: 6| Step: 8
Training loss: 2.9036784172058105
Validation loss: 2.678677289716659

Epoch: 6| Step: 9
Training loss: 3.091214179992676
Validation loss: 2.6759371142233572

Epoch: 6| Step: 10
Training loss: 2.5679006576538086
Validation loss: 2.680260730046098

Epoch: 6| Step: 11
Training loss: 3.1105804443359375
Validation loss: 2.6735636393229165

Epoch: 6| Step: 12
Training loss: 2.084545612335205
Validation loss: 2.676344253683603

Epoch: 6| Step: 13
Training loss: 3.061955451965332
Validation loss: 2.672396139432025

Epoch: 43| Step: 0
Training loss: 3.2516183853149414
Validation loss: 2.670971893495129

Epoch: 6| Step: 1
Training loss: 2.7507424354553223
Validation loss: 2.671141614196121

Epoch: 6| Step: 2
Training loss: 3.2649781703948975
Validation loss: 2.6730385775207193

Epoch: 6| Step: 3
Training loss: 2.6151123046875
Validation loss: 2.6736037705534246

Epoch: 6| Step: 4
Training loss: 2.5788614749908447
Validation loss: 2.6724722436679307

Epoch: 6| Step: 5
Training loss: 2.896481513977051
Validation loss: 2.6690449714660645

Epoch: 6| Step: 6
Training loss: 2.556760549545288
Validation loss: 2.6732017686290126

Epoch: 6| Step: 7
Training loss: 3.043545961380005
Validation loss: 2.672872774062618

Epoch: 6| Step: 8
Training loss: 2.602189302444458
Validation loss: 2.670549464482133

Epoch: 6| Step: 9
Training loss: 3.230557441711426
Validation loss: 2.6700594194473757

Epoch: 6| Step: 10
Training loss: 3.0209884643554688
Validation loss: 2.6709812584743706

Epoch: 6| Step: 11
Training loss: 2.068369150161743
Validation loss: 2.666211405108052

Epoch: 6| Step: 12
Training loss: 2.946848154067993
Validation loss: 2.667811652665497

Epoch: 6| Step: 13
Training loss: 2.561171531677246
Validation loss: 2.6694404412341375

Epoch: 44| Step: 0
Training loss: 3.2108540534973145
Validation loss: 2.6725985773148073

Epoch: 6| Step: 1
Training loss: 2.6154868602752686
Validation loss: 2.674438658580985

Epoch: 6| Step: 2
Training loss: 2.3361198902130127
Validation loss: 2.675392976371191

Epoch: 6| Step: 3
Training loss: 3.355802059173584
Validation loss: 2.6760091038160425

Epoch: 6| Step: 4
Training loss: 3.258593797683716
Validation loss: 2.6796513731761644

Epoch: 6| Step: 5
Training loss: 2.2032470703125
Validation loss: 2.674772047227429

Epoch: 6| Step: 6
Training loss: 2.522190570831299
Validation loss: 2.6699612499565206

Epoch: 6| Step: 7
Training loss: 2.4232192039489746
Validation loss: 2.6675709883371987

Epoch: 6| Step: 8
Training loss: 3.483097553253174
Validation loss: 2.6657713485020462

Epoch: 6| Step: 9
Training loss: 2.363708734512329
Validation loss: 2.6652978312584663

Epoch: 6| Step: 10
Training loss: 3.406193256378174
Validation loss: 2.6631660589607815

Epoch: 6| Step: 11
Training loss: 2.6420860290527344
Validation loss: 2.663731123811455

Epoch: 6| Step: 12
Training loss: 2.912698984146118
Validation loss: 2.66566010444395

Epoch: 6| Step: 13
Training loss: 2.6743288040161133
Validation loss: 2.661706283528318

Epoch: 45| Step: 0
Training loss: 4.029411792755127
Validation loss: 2.663679935598886

Epoch: 6| Step: 1
Training loss: 2.5510096549987793
Validation loss: 2.6630359413803264

Epoch: 6| Step: 2
Training loss: 2.6141560077667236
Validation loss: 2.6610443053707

Epoch: 6| Step: 3
Training loss: 2.982786178588867
Validation loss: 2.6638327106352775

Epoch: 6| Step: 4
Training loss: 2.7616589069366455
Validation loss: 2.659091998172063

Epoch: 6| Step: 5
Training loss: 3.215893030166626
Validation loss: 2.6631538688495593

Epoch: 6| Step: 6
Training loss: 1.8301341533660889
Validation loss: 2.667049889923424

Epoch: 6| Step: 7
Training loss: 2.2596964836120605
Validation loss: 2.6596145476064375

Epoch: 6| Step: 8
Training loss: 2.0115771293640137
Validation loss: 2.657667052361273

Epoch: 6| Step: 9
Training loss: 2.8289783000946045
Validation loss: 2.6564019469804663

Epoch: 6| Step: 10
Training loss: 3.009819507598877
Validation loss: 2.6583103364513767

Epoch: 6| Step: 11
Training loss: 3.511776924133301
Validation loss: 2.6595224436893257

Epoch: 6| Step: 12
Training loss: 2.5149316787719727
Validation loss: 2.669016671437089

Epoch: 6| Step: 13
Training loss: 3.6819186210632324
Validation loss: 2.6667151117837555

Epoch: 46| Step: 0
Training loss: 3.5783467292785645
Validation loss: 2.6798401776180474

Epoch: 6| Step: 1
Training loss: 2.616481304168701
Validation loss: 2.671116913518598

Epoch: 6| Step: 2
Training loss: 2.6967296600341797
Validation loss: 2.667543500982305

Epoch: 6| Step: 3
Training loss: 2.853703022003174
Validation loss: 2.660228144737982

Epoch: 6| Step: 4
Training loss: 3.2801740169525146
Validation loss: 2.661497744180823

Epoch: 6| Step: 5
Training loss: 2.598231077194214
Validation loss: 2.656240112038069

Epoch: 6| Step: 6
Training loss: 3.411201238632202
Validation loss: 2.659273647492932

Epoch: 6| Step: 7
Training loss: 1.8039549589157104
Validation loss: 2.6567259706476682

Epoch: 6| Step: 8
Training loss: 2.8545069694519043
Validation loss: 2.66105491499747

Epoch: 6| Step: 9
Training loss: 2.9259064197540283
Validation loss: 2.664489830693891

Epoch: 6| Step: 10
Training loss: 2.562732458114624
Validation loss: 2.670662792780066

Epoch: 6| Step: 11
Training loss: 2.3796138763427734
Validation loss: 2.6720169974911596

Epoch: 6| Step: 12
Training loss: 3.785454511642456
Validation loss: 2.6655440817597094

Epoch: 6| Step: 13
Training loss: 1.4226895570755005
Validation loss: 2.655811586687642

Epoch: 47| Step: 0
Training loss: 2.501122236251831
Validation loss: 2.653772920690557

Epoch: 6| Step: 1
Training loss: 2.5299906730651855
Validation loss: 2.6583579817125873

Epoch: 6| Step: 2
Training loss: 2.9808318614959717
Validation loss: 2.6562806662692817

Epoch: 6| Step: 3
Training loss: 2.703280210494995
Validation loss: 2.658154741410286

Epoch: 6| Step: 4
Training loss: 3.5543198585510254
Validation loss: 2.6548615758137037

Epoch: 6| Step: 5
Training loss: 3.0266921520233154
Validation loss: 2.6599799279243714

Epoch: 6| Step: 6
Training loss: 3.043862819671631
Validation loss: 2.6634822225057952

Epoch: 6| Step: 7
Training loss: 3.1299519538879395
Validation loss: 2.6648012002309165

Epoch: 6| Step: 8
Training loss: 3.162193775177002
Validation loss: 2.6584100389993317

Epoch: 6| Step: 9
Training loss: 1.9721320867538452
Validation loss: 2.659756034933111

Epoch: 6| Step: 10
Training loss: 2.2055881023406982
Validation loss: 2.66027420566928

Epoch: 6| Step: 11
Training loss: 2.2220919132232666
Validation loss: 2.662703098789338

Epoch: 6| Step: 12
Training loss: 2.974071979522705
Validation loss: 2.665643507434476

Epoch: 6| Step: 13
Training loss: 3.7304954528808594
Validation loss: 2.6696427124802784

Epoch: 48| Step: 0
Training loss: 2.5680055618286133
Validation loss: 2.659341804442867

Epoch: 6| Step: 1
Training loss: 2.3713431358337402
Validation loss: 2.6595805280952045

Epoch: 6| Step: 2
Training loss: 2.677030086517334
Validation loss: 2.6600332439586682

Epoch: 6| Step: 3
Training loss: 2.9273390769958496
Validation loss: 2.6584988922201176

Epoch: 6| Step: 4
Training loss: 3.2352824211120605
Validation loss: 2.657017054096345

Epoch: 6| Step: 5
Training loss: 2.9611504077911377
Validation loss: 2.653487059377855

Epoch: 6| Step: 6
Training loss: 1.9207887649536133
Validation loss: 2.6529693039514686

Epoch: 6| Step: 7
Training loss: 3.501835584640503
Validation loss: 2.64782888402221

Epoch: 6| Step: 8
Training loss: 3.2203030586242676
Validation loss: 2.650994893043272

Epoch: 6| Step: 9
Training loss: 2.2184715270996094
Validation loss: 2.6488894929168043

Epoch: 6| Step: 10
Training loss: 2.708289384841919
Validation loss: 2.6479587862568517

Epoch: 6| Step: 11
Training loss: 2.701479196548462
Validation loss: 2.648603472658383

Epoch: 6| Step: 12
Training loss: 2.9007911682128906
Validation loss: 2.6445859145092707

Epoch: 6| Step: 13
Training loss: 3.792875051498413
Validation loss: 2.6462136853125786

Epoch: 49| Step: 0
Training loss: 2.7664871215820312
Validation loss: 2.646570151852023

Epoch: 6| Step: 1
Training loss: 3.26424503326416
Validation loss: 2.655438594920661

Epoch: 6| Step: 2
Training loss: 3.3658413887023926
Validation loss: 2.65169439008159

Epoch: 6| Step: 3
Training loss: 2.3175911903381348
Validation loss: 2.653119002619097

Epoch: 6| Step: 4
Training loss: 2.5438199043273926
Validation loss: 2.654488158482377

Epoch: 6| Step: 5
Training loss: 2.8068463802337646
Validation loss: 2.658071256452991

Epoch: 6| Step: 6
Training loss: 2.907923936843872
Validation loss: 2.6647359248130553

Epoch: 6| Step: 7
Training loss: 3.59066104888916
Validation loss: 2.6559715347905315

Epoch: 6| Step: 8
Training loss: 2.587214946746826
Validation loss: 2.665766328893682

Epoch: 6| Step: 9
Training loss: 2.369932174682617
Validation loss: 2.6717768792183167

Epoch: 6| Step: 10
Training loss: 2.381969690322876
Validation loss: 2.6742472648620605

Epoch: 6| Step: 11
Training loss: 2.5305919647216797
Validation loss: 2.672930350867651

Epoch: 6| Step: 12
Training loss: 3.116140842437744
Validation loss: 2.6639494229388494

Epoch: 6| Step: 13
Training loss: 2.6607067584991455
Validation loss: 2.6546032505650676

Epoch: 50| Step: 0
Training loss: 2.7617406845092773
Validation loss: 2.65753093073445

Epoch: 6| Step: 1
Training loss: 2.393893003463745
Validation loss: 2.6580930371438303

Epoch: 6| Step: 2
Training loss: 3.414512872695923
Validation loss: 2.655598053368189

Epoch: 6| Step: 3
Training loss: 3.135334014892578
Validation loss: 2.658516655686081

Epoch: 6| Step: 4
Training loss: 1.8717072010040283
Validation loss: 2.645197317164431

Epoch: 6| Step: 5
Training loss: 1.6482399702072144
Validation loss: 2.6421096145465808

Epoch: 6| Step: 6
Training loss: 3.062601089477539
Validation loss: 2.6394993412879204

Epoch: 6| Step: 7
Training loss: 3.7502903938293457
Validation loss: 2.640546667960382

Epoch: 6| Step: 8
Training loss: 3.702463150024414
Validation loss: 2.6435084035319667

Epoch: 6| Step: 9
Training loss: 1.917682409286499
Validation loss: 2.63942281405131

Epoch: 6| Step: 10
Training loss: 3.9195303916931152
Validation loss: 2.64089713814438

Epoch: 6| Step: 11
Training loss: 3.3337063789367676
Validation loss: 2.6417925152727353

Epoch: 6| Step: 12
Training loss: 1.4284858703613281
Validation loss: 2.6391132698264173

Epoch: 6| Step: 13
Training loss: 2.9069318771362305
Validation loss: 2.6417298727138068

Epoch: 51| Step: 0
Training loss: 2.4774346351623535
Validation loss: 2.6417074536764495

Epoch: 6| Step: 1
Training loss: 2.657439947128296
Validation loss: 2.638096196677095

Epoch: 6| Step: 2
Training loss: 3.3986382484436035
Validation loss: 2.6411045469263548

Epoch: 6| Step: 3
Training loss: 3.1541786193847656
Validation loss: 2.6391467099548667

Epoch: 6| Step: 4
Training loss: 2.4190683364868164
Validation loss: 2.638385236904185

Epoch: 6| Step: 5
Training loss: 3.8200111389160156
Validation loss: 2.64154823108386

Epoch: 6| Step: 6
Training loss: 3.1915576457977295
Validation loss: 2.6399909296343402

Epoch: 6| Step: 7
Training loss: 2.913809061050415
Validation loss: 2.6436216010842273

Epoch: 6| Step: 8
Training loss: 2.874856948852539
Validation loss: 2.6388561648707234

Epoch: 6| Step: 9
Training loss: 2.254650592803955
Validation loss: 2.6371534896153275

Epoch: 6| Step: 10
Training loss: 3.0742249488830566
Validation loss: 2.641159088380875

Epoch: 6| Step: 11
Training loss: 1.6998512744903564
Validation loss: 2.639976783465314

Epoch: 6| Step: 12
Training loss: 2.8319571018218994
Validation loss: 2.6381307340437368

Epoch: 6| Step: 13
Training loss: 2.183678150177002
Validation loss: 2.638343157306794

Epoch: 52| Step: 0
Training loss: 2.951202630996704
Validation loss: 2.641930572448238

Epoch: 6| Step: 1
Training loss: 2.834322929382324
Validation loss: 2.636820213769072

Epoch: 6| Step: 2
Training loss: 2.4106955528259277
Validation loss: 2.6378200592533236

Epoch: 6| Step: 3
Training loss: 2.69435977935791
Validation loss: 2.644949610515307

Epoch: 6| Step: 4
Training loss: 3.254418134689331
Validation loss: 2.639751180525749

Epoch: 6| Step: 5
Training loss: 2.481468677520752
Validation loss: 2.6407456321101033

Epoch: 6| Step: 6
Training loss: 3.37690806388855
Validation loss: 2.633689690661687

Epoch: 6| Step: 7
Training loss: 2.6436052322387695
Validation loss: 2.6393191558058544

Epoch: 6| Step: 8
Training loss: 2.3640644550323486
Validation loss: 2.6379597110133015

Epoch: 6| Step: 9
Training loss: 2.8875555992126465
Validation loss: 2.635423062950052

Epoch: 6| Step: 10
Training loss: 2.9548890590667725
Validation loss: 2.641102329377205

Epoch: 6| Step: 11
Training loss: 2.897831439971924
Validation loss: 2.641030652548677

Epoch: 6| Step: 12
Training loss: 3.5364692211151123
Validation loss: 2.640279375096803

Epoch: 6| Step: 13
Training loss: 1.0739322900772095
Validation loss: 2.641135964342343

Epoch: 53| Step: 0
Training loss: 2.8233895301818848
Validation loss: 2.635424942098638

Epoch: 6| Step: 1
Training loss: 2.905694007873535
Validation loss: 2.6330591119745725

Epoch: 6| Step: 2
Training loss: 3.491971969604492
Validation loss: 2.6319901815024753

Epoch: 6| Step: 3
Training loss: 2.241508722305298
Validation loss: 2.6331244591743714

Epoch: 6| Step: 4
Training loss: 2.378103017807007
Validation loss: 2.6351600590572564

Epoch: 6| Step: 5
Training loss: 2.4337024688720703
Validation loss: 2.632586758623841

Epoch: 6| Step: 6
Training loss: 2.3513998985290527
Validation loss: 2.6352929633150817

Epoch: 6| Step: 7
Training loss: 2.943730354309082
Validation loss: 2.632403660846013

Epoch: 6| Step: 8
Training loss: 3.652340888977051
Validation loss: 2.6351111063393216

Epoch: 6| Step: 9
Training loss: 2.608314037322998
Validation loss: 2.6267265837679625

Epoch: 6| Step: 10
Training loss: 3.40775203704834
Validation loss: 2.6315065224965415

Epoch: 6| Step: 11
Training loss: 2.4624216556549072
Validation loss: 2.629749741605533

Epoch: 6| Step: 12
Training loss: 2.6986794471740723
Validation loss: 2.627156857521303

Epoch: 6| Step: 13
Training loss: 2.5527145862579346
Validation loss: 2.6279812551313833

Epoch: 54| Step: 0
Training loss: 2.095090389251709
Validation loss: 2.6266206567005446

Epoch: 6| Step: 1
Training loss: 3.2634057998657227
Validation loss: 2.63114103194206

Epoch: 6| Step: 2
Training loss: 2.6607813835144043
Validation loss: 2.6305614568853892

Epoch: 6| Step: 3
Training loss: 2.8036787509918213
Validation loss: 2.6287322223827405

Epoch: 6| Step: 4
Training loss: 2.7978343963623047
Validation loss: 2.637809604726812

Epoch: 6| Step: 5
Training loss: 2.7428834438323975
Validation loss: 2.6357998873597834

Epoch: 6| Step: 6
Training loss: 4.078105926513672
Validation loss: 2.6368845150034916

Epoch: 6| Step: 7
Training loss: 2.8399417400360107
Validation loss: 2.6310177208274923

Epoch: 6| Step: 8
Training loss: 2.4856925010681152
Validation loss: 2.6306839963441253

Epoch: 6| Step: 9
Training loss: 3.049612522125244
Validation loss: 2.6275405627425

Epoch: 6| Step: 10
Training loss: 2.3487954139709473
Validation loss: 2.6347196461052023

Epoch: 6| Step: 11
Training loss: 2.1391031742095947
Validation loss: 2.6263110150573072

Epoch: 6| Step: 12
Training loss: 2.6456093788146973
Validation loss: 2.6230877983954644

Epoch: 6| Step: 13
Training loss: 3.272693395614624
Validation loss: 2.624865460139449

Epoch: 55| Step: 0
Training loss: 3.102186918258667
Validation loss: 2.62463564513832

Epoch: 6| Step: 1
Training loss: 3.078937292098999
Validation loss: 2.6217227366662796

Epoch: 6| Step: 2
Training loss: 3.3112287521362305
Validation loss: 2.622565964216827

Epoch: 6| Step: 3
Training loss: 2.5867936611175537
Validation loss: 2.623004459565686

Epoch: 6| Step: 4
Training loss: 2.179217576980591
Validation loss: 2.625046440350112

Epoch: 6| Step: 5
Training loss: 1.6888539791107178
Validation loss: 2.6262652617628857

Epoch: 6| Step: 6
Training loss: 3.266627550125122
Validation loss: 2.6273818041688655

Epoch: 6| Step: 7
Training loss: 2.619717836380005
Validation loss: 2.631478653159193

Epoch: 6| Step: 8
Training loss: 2.7053728103637695
Validation loss: 2.6284352733242895

Epoch: 6| Step: 9
Training loss: 2.6499152183532715
Validation loss: 2.624691473540439

Epoch: 6| Step: 10
Training loss: 3.1220874786376953
Validation loss: 2.6293573751244494

Epoch: 6| Step: 11
Training loss: 3.4559946060180664
Validation loss: 2.6240299132562455

Epoch: 6| Step: 12
Training loss: 2.8228447437286377
Validation loss: 2.6239885566055134

Epoch: 6| Step: 13
Training loss: 2.026163101196289
Validation loss: 2.6270629949467157

Epoch: 56| Step: 0
Training loss: 3.14794659614563
Validation loss: 2.6272024313608804

Epoch: 6| Step: 1
Training loss: 2.284492015838623
Validation loss: 2.622742199128674

Epoch: 6| Step: 2
Training loss: 2.0184476375579834
Validation loss: 2.6213975773062757

Epoch: 6| Step: 3
Training loss: 3.0449390411376953
Validation loss: 2.618716101492605

Epoch: 6| Step: 4
Training loss: 2.797077178955078
Validation loss: 2.620441493167672

Epoch: 6| Step: 5
Training loss: 2.6555628776550293
Validation loss: 2.623149800044234

Epoch: 6| Step: 6
Training loss: 3.372620105743408
Validation loss: 2.6210053736163723

Epoch: 6| Step: 7
Training loss: 2.3672962188720703
Validation loss: 2.6194906901287776

Epoch: 6| Step: 8
Training loss: 2.71254301071167
Validation loss: 2.6280689572775238

Epoch: 6| Step: 9
Training loss: 2.384321689605713
Validation loss: 2.6301748752593994

Epoch: 6| Step: 10
Training loss: 2.5420339107513428
Validation loss: 2.617338990652433

Epoch: 6| Step: 11
Training loss: 2.7585396766662598
Validation loss: 2.6203492764503724

Epoch: 6| Step: 12
Training loss: 3.319675922393799
Validation loss: 2.6189826098821496

Epoch: 6| Step: 13
Training loss: 4.082743167877197
Validation loss: 2.6173597279415337

Epoch: 57| Step: 0
Training loss: 3.560662269592285
Validation loss: 2.612524160774805

Epoch: 6| Step: 1
Training loss: 3.0377426147460938
Validation loss: 2.618250098279727

Epoch: 6| Step: 2
Training loss: 2.710486888885498
Validation loss: 2.6139434614489154

Epoch: 6| Step: 3
Training loss: 2.7325339317321777
Validation loss: 2.617511646721953

Epoch: 6| Step: 4
Training loss: 2.138199806213379
Validation loss: 2.6152622212645826

Epoch: 6| Step: 5
Training loss: 2.8209140300750732
Validation loss: 2.612228573009532

Epoch: 6| Step: 6
Training loss: 2.8728387355804443
Validation loss: 2.6167197535114903

Epoch: 6| Step: 7
Training loss: 2.8268070220947266
Validation loss: 2.6153748573795443

Epoch: 6| Step: 8
Training loss: 3.1187357902526855
Validation loss: 2.615732546775572

Epoch: 6| Step: 9
Training loss: 2.3104069232940674
Validation loss: 2.6151743165908323

Epoch: 6| Step: 10
Training loss: 2.6419219970703125
Validation loss: 2.613731256095312

Epoch: 6| Step: 11
Training loss: 1.7766988277435303
Validation loss: 2.61229956534601

Epoch: 6| Step: 12
Training loss: 3.4337925910949707
Validation loss: 2.6115638056109027

Epoch: 6| Step: 13
Training loss: 2.965610980987549
Validation loss: 2.6151678075072584

Epoch: 58| Step: 0
Training loss: 2.9220809936523438
Validation loss: 2.618503857684392

Epoch: 6| Step: 1
Training loss: 1.872652530670166
Validation loss: 2.6191516307092484

Epoch: 6| Step: 2
Training loss: 3.6224303245544434
Validation loss: 2.6291143663467897

Epoch: 6| Step: 3
Training loss: 2.9677834510803223
Validation loss: 2.629177326797157

Epoch: 6| Step: 4
Training loss: 2.6022932529449463
Validation loss: 2.620622299050772

Epoch: 6| Step: 5
Training loss: 2.387235641479492
Validation loss: 2.6170125981812835

Epoch: 6| Step: 6
Training loss: 2.4237351417541504
Validation loss: 2.6138732535864717

Epoch: 6| Step: 7
Training loss: 2.2432212829589844
Validation loss: 2.6130905535913285

Epoch: 6| Step: 8
Training loss: 2.569131374359131
Validation loss: 2.6136027074629262

Epoch: 6| Step: 9
Training loss: 3.3977062702178955
Validation loss: 2.615987890510149

Epoch: 6| Step: 10
Training loss: 2.8170695304870605
Validation loss: 2.6171721963472265

Epoch: 6| Step: 11
Training loss: 2.796487331390381
Validation loss: 2.6144670004485757

Epoch: 6| Step: 12
Training loss: 3.137146472930908
Validation loss: 2.6189038599691083

Epoch: 6| Step: 13
Training loss: 3.603590726852417
Validation loss: 2.6171309614694245

Epoch: 59| Step: 0
Training loss: 2.4944417476654053
Validation loss: 2.6104029019673667

Epoch: 6| Step: 1
Training loss: 3.047872304916382
Validation loss: 2.6114840353688886

Epoch: 6| Step: 2
Training loss: 3.0899407863616943
Validation loss: 2.615146034507341

Epoch: 6| Step: 3
Training loss: 2.0002176761627197
Validation loss: 2.618775147263722

Epoch: 6| Step: 4
Training loss: 2.880941867828369
Validation loss: 2.6187329523025022

Epoch: 6| Step: 5
Training loss: 3.0302860736846924
Validation loss: 2.6198558730463826

Epoch: 6| Step: 6
Training loss: 1.993889570236206
Validation loss: 2.6259433761719735

Epoch: 6| Step: 7
Training loss: 3.756941795349121
Validation loss: 2.625774409181328

Epoch: 6| Step: 8
Training loss: 2.4466423988342285
Validation loss: 2.6201284649551555

Epoch: 6| Step: 9
Training loss: 2.7802414894104004
Validation loss: 2.6197949019811486

Epoch: 6| Step: 10
Training loss: 2.4774551391601562
Validation loss: 2.6131978445155646

Epoch: 6| Step: 11
Training loss: 3.1621999740600586
Validation loss: 2.613297616281817

Epoch: 6| Step: 12
Training loss: 2.620285987854004
Validation loss: 2.609214982678813

Epoch: 6| Step: 13
Training loss: 3.4145991802215576
Validation loss: 2.609638819130518

Epoch: 60| Step: 0
Training loss: 3.013456344604492
Validation loss: 2.6054020004887737

Epoch: 6| Step: 1
Training loss: 3.4068586826324463
Validation loss: 2.6059830316933255

Epoch: 6| Step: 2
Training loss: 2.808159112930298
Validation loss: 2.60594432328337

Epoch: 6| Step: 3
Training loss: 2.005373954772949
Validation loss: 2.6059330830010037

Epoch: 6| Step: 4
Training loss: 3.2805519104003906
Validation loss: 2.6125006265537714

Epoch: 6| Step: 5
Training loss: 2.778428316116333
Validation loss: 2.607971909225628

Epoch: 6| Step: 6
Training loss: 2.5111844539642334
Validation loss: 2.60875448360238

Epoch: 6| Step: 7
Training loss: 2.183565378189087
Validation loss: 2.610838661911667

Epoch: 6| Step: 8
Training loss: 2.632870674133301
Validation loss: 2.605835053228563

Epoch: 6| Step: 9
Training loss: 3.082606315612793
Validation loss: 2.6033279716327624

Epoch: 6| Step: 10
Training loss: 3.712831974029541
Validation loss: 2.602324120460018

Epoch: 6| Step: 11
Training loss: 2.60256290435791
Validation loss: 2.600510105010002

Epoch: 6| Step: 12
Training loss: 2.1908178329467773
Validation loss: 2.600247839445709

Epoch: 6| Step: 13
Training loss: 2.471003293991089
Validation loss: 2.6035395335125666

Epoch: 61| Step: 0
Training loss: 2.574967384338379
Validation loss: 2.6135844261415544

Epoch: 6| Step: 1
Training loss: 3.0775413513183594
Validation loss: 2.613689684098767

Epoch: 6| Step: 2
Training loss: 3.04201078414917
Validation loss: 2.6237867775783745

Epoch: 6| Step: 3
Training loss: 2.567218542098999
Validation loss: 2.6151049265297512

Epoch: 6| Step: 4
Training loss: 2.711043357849121
Validation loss: 2.609910418910365

Epoch: 6| Step: 5
Training loss: 2.9203691482543945
Validation loss: 2.6087509996147564

Epoch: 6| Step: 6
Training loss: 2.079599380493164
Validation loss: 2.608038133190524

Epoch: 6| Step: 7
Training loss: 2.679248332977295
Validation loss: 2.6082958662381737

Epoch: 6| Step: 8
Training loss: 3.233171224594116
Validation loss: 2.604943270324379

Epoch: 6| Step: 9
Training loss: 2.6639528274536133
Validation loss: 2.6084127118510585

Epoch: 6| Step: 10
Training loss: 3.5622124671936035
Validation loss: 2.607779497741371

Epoch: 6| Step: 11
Training loss: 2.768174648284912
Validation loss: 2.6091370903035647

Epoch: 6| Step: 12
Training loss: 2.589423656463623
Validation loss: 2.6084463519434773

Epoch: 6| Step: 13
Training loss: 1.9751074314117432
Validation loss: 2.6073954054104385

Epoch: 62| Step: 0
Training loss: 2.7170138359069824
Validation loss: 2.613091322683519

Epoch: 6| Step: 1
Training loss: 3.0351099967956543
Validation loss: 2.615182343349662

Epoch: 6| Step: 2
Training loss: 2.4826455116271973
Validation loss: 2.617718981158349

Epoch: 6| Step: 3
Training loss: 2.541396379470825
Validation loss: 2.6196829606127996

Epoch: 6| Step: 4
Training loss: 3.4010353088378906
Validation loss: 2.62073165370572

Epoch: 6| Step: 5
Training loss: 3.058076858520508
Validation loss: 2.6172562901691725

Epoch: 6| Step: 6
Training loss: 2.7473154067993164
Validation loss: 2.6180139280134633

Epoch: 6| Step: 7
Training loss: 3.3145432472229004
Validation loss: 2.6118768825325915

Epoch: 6| Step: 8
Training loss: 2.7530484199523926
Validation loss: 2.6105680158061366

Epoch: 6| Step: 9
Training loss: 3.199690818786621
Validation loss: 2.6100615070712183

Epoch: 6| Step: 10
Training loss: 2.534738779067993
Validation loss: 2.6123569883326048

Epoch: 6| Step: 11
Training loss: 2.5090742111206055
Validation loss: 2.6082812509229107

Epoch: 6| Step: 12
Training loss: 2.4889564514160156
Validation loss: 2.6069036837547057

Epoch: 6| Step: 13
Training loss: 1.349323034286499
Validation loss: 2.6056721441207396

Epoch: 63| Step: 0
Training loss: 2.9865880012512207
Validation loss: 2.6047858653529996

Epoch: 6| Step: 1
Training loss: 1.7549324035644531
Validation loss: 2.6039559405337096

Epoch: 6| Step: 2
Training loss: 2.7290050983428955
Validation loss: 2.6044288527581

Epoch: 6| Step: 3
Training loss: 2.639345645904541
Validation loss: 2.604380456350183

Epoch: 6| Step: 4
Training loss: 2.680723190307617
Validation loss: 2.604829026806739

Epoch: 6| Step: 5
Training loss: 2.36645245552063
Validation loss: 2.603932052530268

Epoch: 6| Step: 6
Training loss: 2.971404790878296
Validation loss: 2.603435754776001

Epoch: 6| Step: 7
Training loss: 3.455923080444336
Validation loss: 2.603628220096711

Epoch: 6| Step: 8
Training loss: 2.323697090148926
Validation loss: 2.602490045691049

Epoch: 6| Step: 9
Training loss: 3.086707592010498
Validation loss: 2.6024774966701383

Epoch: 6| Step: 10
Training loss: 3.6049680709838867
Validation loss: 2.605404402620049

Epoch: 6| Step: 11
Training loss: 2.6583218574523926
Validation loss: 2.604433000728648

Epoch: 6| Step: 12
Training loss: 3.4528579711914062
Validation loss: 2.604555670933057

Epoch: 6| Step: 13
Training loss: 1.4257010221481323
Validation loss: 2.6022859029872443

Epoch: 64| Step: 0
Training loss: 2.941629409790039
Validation loss: 2.613761653182327

Epoch: 6| Step: 1
Training loss: 2.399430274963379
Validation loss: 2.6047629464057183

Epoch: 6| Step: 2
Training loss: 2.481595516204834
Validation loss: 2.6076191625287457

Epoch: 6| Step: 3
Training loss: 2.8366458415985107
Validation loss: 2.6056399678671234

Epoch: 6| Step: 4
Training loss: 1.7251718044281006
Validation loss: 2.6091247809830533

Epoch: 6| Step: 5
Training loss: 2.048696517944336
Validation loss: 2.6094277340878724

Epoch: 6| Step: 6
Training loss: 2.979557514190674
Validation loss: 2.6106265411582044

Epoch: 6| Step: 7
Training loss: 3.5353658199310303
Validation loss: 2.6152197417392524

Epoch: 6| Step: 8
Training loss: 3.6582813262939453
Validation loss: 2.617860451821358

Epoch: 6| Step: 9
Training loss: 3.668728828430176
Validation loss: 2.6238721339933333

Epoch: 6| Step: 10
Training loss: 2.2291626930236816
Validation loss: 2.6110860814330397

Epoch: 6| Step: 11
Training loss: 2.2528274059295654
Validation loss: 2.6053055896553943

Epoch: 6| Step: 12
Training loss: 3.0208139419555664
Validation loss: 2.6010705899166804

Epoch: 6| Step: 13
Training loss: 2.9823548793792725
Validation loss: 2.597660892753191

Epoch: 65| Step: 0
Training loss: 2.878810405731201
Validation loss: 2.5961527824401855

Epoch: 6| Step: 1
Training loss: 2.7067434787750244
Validation loss: 2.5982880412891345

Epoch: 6| Step: 2
Training loss: 3.2459557056427
Validation loss: 2.6005267071467575

Epoch: 6| Step: 3
Training loss: 3.2201266288757324
Validation loss: 2.60480405951059

Epoch: 6| Step: 4
Training loss: 3.296985149383545
Validation loss: 2.6010308983505412

Epoch: 6| Step: 5
Training loss: 2.171387195587158
Validation loss: 2.604542504074753

Epoch: 6| Step: 6
Training loss: 2.6456077098846436
Validation loss: 2.6030929268047376

Epoch: 6| Step: 7
Training loss: 2.2848329544067383
Validation loss: 2.6030729611714682

Epoch: 6| Step: 8
Training loss: 2.718966484069824
Validation loss: 2.6016512634933635

Epoch: 6| Step: 9
Training loss: 2.2005667686462402
Validation loss: 2.5996401053602978

Epoch: 6| Step: 10
Training loss: 2.641869068145752
Validation loss: 2.5984231656597507

Epoch: 6| Step: 11
Training loss: 3.6807730197906494
Validation loss: 2.591783654305243

Epoch: 6| Step: 12
Training loss: 2.4867312908172607
Validation loss: 2.5949844032205562

Epoch: 6| Step: 13
Training loss: 2.3631527423858643
Validation loss: 2.5945985804321947

Epoch: 66| Step: 0
Training loss: 2.804769515991211
Validation loss: 2.5931308359228153

Epoch: 6| Step: 1
Training loss: 2.0918662548065186
Validation loss: 2.5945811604940765

Epoch: 6| Step: 2
Training loss: 3.5525388717651367
Validation loss: 2.5983433825995332

Epoch: 6| Step: 3
Training loss: 3.2264087200164795
Validation loss: 2.5964320526328137

Epoch: 6| Step: 4
Training loss: 3.2590770721435547
Validation loss: 2.600330668111001

Epoch: 6| Step: 5
Training loss: 2.5630884170532227
Validation loss: 2.601490189952235

Epoch: 6| Step: 6
Training loss: 2.411679744720459
Validation loss: 2.60333990153446

Epoch: 6| Step: 7
Training loss: 2.841248035430908
Validation loss: 2.5993287178777877

Epoch: 6| Step: 8
Training loss: 2.1875600814819336
Validation loss: 2.5998836076387795

Epoch: 6| Step: 9
Training loss: 2.7279295921325684
Validation loss: 2.59979760262274

Epoch: 6| Step: 10
Training loss: 2.4242472648620605
Validation loss: 2.596515319680655

Epoch: 6| Step: 11
Training loss: 2.7320337295532227
Validation loss: 2.6015953094728532

Epoch: 6| Step: 12
Training loss: 2.8370792865753174
Validation loss: 2.6053109399734007

Epoch: 6| Step: 13
Training loss: 3.1890854835510254
Validation loss: 2.6038932261928434

Epoch: 67| Step: 0
Training loss: 2.9231624603271484
Validation loss: 2.5950084450424358

Epoch: 6| Step: 1
Training loss: 2.6427736282348633
Validation loss: 2.589594905094434

Epoch: 6| Step: 2
Training loss: 2.094306230545044
Validation loss: 2.5893528358910674

Epoch: 6| Step: 3
Training loss: 2.7522330284118652
Validation loss: 2.586305997704947

Epoch: 6| Step: 4
Training loss: 3.122127056121826
Validation loss: 2.589966662468449

Epoch: 6| Step: 5
Training loss: 2.850407123565674
Validation loss: 2.5902995870959376

Epoch: 6| Step: 6
Training loss: 2.575035810470581
Validation loss: 2.5900776411897395

Epoch: 6| Step: 7
Training loss: 3.521686553955078
Validation loss: 2.5891324115055863

Epoch: 6| Step: 8
Training loss: 3.6009950637817383
Validation loss: 2.591325354832475

Epoch: 6| Step: 9
Training loss: 2.0506715774536133
Validation loss: 2.591295588401056

Epoch: 6| Step: 10
Training loss: 2.7120869159698486
Validation loss: 2.5929909188260316

Epoch: 6| Step: 11
Training loss: 2.7989673614501953
Validation loss: 2.58845837654606

Epoch: 6| Step: 12
Training loss: 2.5809130668640137
Validation loss: 2.585968276505829

Epoch: 6| Step: 13
Training loss: 2.129743814468384
Validation loss: 2.5880736202322026

Epoch: 68| Step: 0
Training loss: 2.950634479522705
Validation loss: 2.5856318704543577

Epoch: 6| Step: 1
Training loss: 2.452691078186035
Validation loss: 2.584769046434792

Epoch: 6| Step: 2
Training loss: 2.57521653175354
Validation loss: 2.5849838410654375

Epoch: 6| Step: 3
Training loss: 2.2932491302490234
Validation loss: 2.5863449496607624

Epoch: 6| Step: 4
Training loss: 2.566655158996582
Validation loss: 2.58233231113803

Epoch: 6| Step: 5
Training loss: 3.4773683547973633
Validation loss: 2.5858590577238347

Epoch: 6| Step: 6
Training loss: 2.2236475944519043
Validation loss: 2.5833820373781267

Epoch: 6| Step: 7
Training loss: 2.373227596282959
Validation loss: 2.5848941956796954

Epoch: 6| Step: 8
Training loss: 2.781466484069824
Validation loss: 2.5815243772281113

Epoch: 6| Step: 9
Training loss: 2.901047945022583
Validation loss: 2.5882304586390013

Epoch: 6| Step: 10
Training loss: 3.2208003997802734
Validation loss: 2.5891732733736754

Epoch: 6| Step: 11
Training loss: 2.9563992023468018
Validation loss: 2.5851212522035003

Epoch: 6| Step: 12
Training loss: 3.19026780128479
Validation loss: 2.5830659251059256

Epoch: 6| Step: 13
Training loss: 2.579686164855957
Validation loss: 2.5891180397361837

Epoch: 69| Step: 0
Training loss: 3.0218141078948975
Validation loss: 2.590082017324304

Epoch: 6| Step: 1
Training loss: 2.1971864700317383
Validation loss: 2.5886620911218787

Epoch: 6| Step: 2
Training loss: 3.3959317207336426
Validation loss: 2.5898773900924192

Epoch: 6| Step: 3
Training loss: 2.4257712364196777
Validation loss: 2.5863026957358084

Epoch: 6| Step: 4
Training loss: 2.649841785430908
Validation loss: 2.5847152484360563

Epoch: 6| Step: 5
Training loss: 2.6800899505615234
Validation loss: 2.5856191112149145

Epoch: 6| Step: 6
Training loss: 2.63673996925354
Validation loss: 2.5861259942413657

Epoch: 6| Step: 7
Training loss: 2.8719167709350586
Validation loss: 2.5808489091934694

Epoch: 6| Step: 8
Training loss: 2.510312080383301
Validation loss: 2.5820289914326002

Epoch: 6| Step: 9
Training loss: 2.3492040634155273
Validation loss: 2.580496726497527

Epoch: 6| Step: 10
Training loss: 3.145423412322998
Validation loss: 2.585565597780289

Epoch: 6| Step: 11
Training loss: 3.392906665802002
Validation loss: 2.581578039353894

Epoch: 6| Step: 12
Training loss: 2.6358184814453125
Validation loss: 2.5790144525548464

Epoch: 6| Step: 13
Training loss: 2.518404960632324
Validation loss: 2.5808185479974233

Epoch: 70| Step: 0
Training loss: 2.832304000854492
Validation loss: 2.5820195495441394

Epoch: 6| Step: 1
Training loss: 1.8320027589797974
Validation loss: 2.581973715495038

Epoch: 6| Step: 2
Training loss: 2.1456961631774902
Validation loss: 2.5812884146167385

Epoch: 6| Step: 3
Training loss: 3.0185394287109375
Validation loss: 2.5857439194956133

Epoch: 6| Step: 4
Training loss: 2.299483060836792
Validation loss: 2.587192145727014

Epoch: 6| Step: 5
Training loss: 3.481333017349243
Validation loss: 2.584417766140353

Epoch: 6| Step: 6
Training loss: 4.342089653015137
Validation loss: 2.5895945974575576

Epoch: 6| Step: 7
Training loss: 1.6264057159423828
Validation loss: 2.588346322377523

Epoch: 6| Step: 8
Training loss: 2.2808828353881836
Validation loss: 2.5860916030022407

Epoch: 6| Step: 9
Training loss: 3.2676877975463867
Validation loss: 2.5806879535798104

Epoch: 6| Step: 10
Training loss: 3.114004611968994
Validation loss: 2.581462380706623

Epoch: 6| Step: 11
Training loss: 2.668881893157959
Validation loss: 2.57675447258898

Epoch: 6| Step: 12
Training loss: 3.065248966217041
Validation loss: 2.579890884378905

Epoch: 6| Step: 13
Training loss: 2.312030792236328
Validation loss: 2.576898579956383

Epoch: 71| Step: 0
Training loss: 2.0255324840545654
Validation loss: 2.576281573182793

Epoch: 6| Step: 1
Training loss: 3.2372887134552
Validation loss: 2.577200174331665

Epoch: 6| Step: 2
Training loss: 3.359744071960449
Validation loss: 2.579339793933335

Epoch: 6| Step: 3
Training loss: 2.580721855163574
Validation loss: 2.5765029461153093

Epoch: 6| Step: 4
Training loss: 2.9093685150146484
Validation loss: 2.5769911196924027

Epoch: 6| Step: 5
Training loss: 2.657985210418701
Validation loss: 2.5752741880314325

Epoch: 6| Step: 6
Training loss: 2.7826666831970215
Validation loss: 2.5798854776608047

Epoch: 6| Step: 7
Training loss: 2.2348313331604004
Validation loss: 2.577174448197888

Epoch: 6| Step: 8
Training loss: 2.724808692932129
Validation loss: 2.582745118807721

Epoch: 6| Step: 9
Training loss: 3.3271820545196533
Validation loss: 2.5808710641758417

Epoch: 6| Step: 10
Training loss: 2.598566770553589
Validation loss: 2.581429114905737

Epoch: 6| Step: 11
Training loss: 2.6454105377197266
Validation loss: 2.575968542406636

Epoch: 6| Step: 12
Training loss: 2.896259069442749
Validation loss: 2.5793366688554005

Epoch: 6| Step: 13
Training loss: 2.241367816925049
Validation loss: 2.5831808902884044

Epoch: 72| Step: 0
Training loss: 2.4511160850524902
Validation loss: 2.5795970655256704

Epoch: 6| Step: 1
Training loss: 2.598642587661743
Validation loss: 2.5856867682549263

Epoch: 6| Step: 2
Training loss: 3.103388786315918
Validation loss: 2.586627678204608

Epoch: 6| Step: 3
Training loss: 2.9898781776428223
Validation loss: 2.5908584158907653

Epoch: 6| Step: 4
Training loss: 2.9642796516418457
Validation loss: 2.59061429321125

Epoch: 6| Step: 5
Training loss: 2.4431710243225098
Validation loss: 2.5915728768994732

Epoch: 6| Step: 6
Training loss: 2.451456069946289
Validation loss: 2.5889692075790895

Epoch: 6| Step: 7
Training loss: 1.8667285442352295
Validation loss: 2.578346408823485

Epoch: 6| Step: 8
Training loss: 2.7988791465759277
Validation loss: 2.577686758451564

Epoch: 6| Step: 9
Training loss: 2.6751484870910645
Validation loss: 2.5731042174882788

Epoch: 6| Step: 10
Training loss: 2.897433280944824
Validation loss: 2.5717992423683085

Epoch: 6| Step: 11
Training loss: 3.3259615898132324
Validation loss: 2.573191822216075

Epoch: 6| Step: 12
Training loss: 2.491168737411499
Validation loss: 2.5707692484701834

Epoch: 6| Step: 13
Training loss: 3.89322566986084
Validation loss: 2.573809905718732

Epoch: 73| Step: 0
Training loss: 2.727529764175415
Validation loss: 2.5756551168298207

Epoch: 6| Step: 1
Training loss: 2.61204195022583
Validation loss: 2.5750755827914

Epoch: 6| Step: 2
Training loss: 3.0275113582611084
Validation loss: 2.577654233542822

Epoch: 6| Step: 3
Training loss: 2.2688469886779785
Validation loss: 2.5764159258975776

Epoch: 6| Step: 4
Training loss: 2.8075294494628906
Validation loss: 2.5749008578638874

Epoch: 6| Step: 5
Training loss: 2.744013786315918
Validation loss: 2.5753863626910793

Epoch: 6| Step: 6
Training loss: 3.2473368644714355
Validation loss: 2.573765144553236

Epoch: 6| Step: 7
Training loss: 3.1690404415130615
Validation loss: 2.577657397075366

Epoch: 6| Step: 8
Training loss: 2.2287089824676514
Validation loss: 2.573257651380313

Epoch: 6| Step: 9
Training loss: 3.1588950157165527
Validation loss: 2.5737450327924503

Epoch: 6| Step: 10
Training loss: 2.174579620361328
Validation loss: 2.5747529768174693

Epoch: 6| Step: 11
Training loss: 2.999917507171631
Validation loss: 2.5754297574361167

Epoch: 6| Step: 12
Training loss: 2.1821210384368896
Validation loss: 2.5776845460296958

Epoch: 6| Step: 13
Training loss: 3.4416463375091553
Validation loss: 2.5833613898164485

Epoch: 74| Step: 0
Training loss: 2.6860456466674805
Validation loss: 2.5862354847692672

Epoch: 6| Step: 1
Training loss: 2.354501247406006
Validation loss: 2.58134126663208

Epoch: 6| Step: 2
Training loss: 2.7243330478668213
Validation loss: 2.5847747633534093

Epoch: 6| Step: 3
Training loss: 2.0821657180786133
Validation loss: 2.583787415617256

Epoch: 6| Step: 4
Training loss: 3.466106653213501
Validation loss: 2.5775142664550454

Epoch: 6| Step: 5
Training loss: 2.644530773162842
Validation loss: 2.5748410737642677

Epoch: 6| Step: 6
Training loss: 2.4535627365112305
Validation loss: 2.5786105202090357

Epoch: 6| Step: 7
Training loss: 2.52427339553833
Validation loss: 2.5731300871859313

Epoch: 6| Step: 8
Training loss: 4.069523334503174
Validation loss: 2.5748243883091915

Epoch: 6| Step: 9
Training loss: 2.655182123184204
Validation loss: 2.573688930080783

Epoch: 6| Step: 10
Training loss: 2.7898945808410645
Validation loss: 2.5703020121461604

Epoch: 6| Step: 11
Training loss: 2.7727997303009033
Validation loss: 2.574517347479379

Epoch: 6| Step: 12
Training loss: 2.5276222229003906
Validation loss: 2.5792257452523835

Epoch: 6| Step: 13
Training loss: 2.372076988220215
Validation loss: 2.581952405232255

Epoch: 75| Step: 0
Training loss: 2.562891960144043
Validation loss: 2.580077335398684

Epoch: 6| Step: 1
Training loss: 2.652139663696289
Validation loss: 2.579229195912679

Epoch: 6| Step: 2
Training loss: 2.647275447845459
Validation loss: 2.579963776373094

Epoch: 6| Step: 3
Training loss: 2.865593910217285
Validation loss: 2.583143872599448

Epoch: 6| Step: 4
Training loss: 2.886274814605713
Validation loss: 2.5844350886601273

Epoch: 6| Step: 5
Training loss: 2.8198094367980957
Validation loss: 2.5795988421286307

Epoch: 6| Step: 6
Training loss: 2.8908047676086426
Validation loss: 2.5860048109485256

Epoch: 6| Step: 7
Training loss: 2.860515594482422
Validation loss: 2.571845300735966

Epoch: 6| Step: 8
Training loss: 2.3922362327575684
Validation loss: 2.567108631134033

Epoch: 6| Step: 9
Training loss: 2.329986572265625
Validation loss: 2.5689487316275157

Epoch: 6| Step: 10
Training loss: 3.0176146030426025
Validation loss: 2.5655941219740015

Epoch: 6| Step: 11
Training loss: 1.9224402904510498
Validation loss: 2.5686285880304154

Epoch: 6| Step: 12
Training loss: 3.34466814994812
Validation loss: 2.567302021929013

Epoch: 6| Step: 13
Training loss: 3.4786908626556396
Validation loss: 2.5682156957605833

Epoch: 76| Step: 0
Training loss: 2.3738341331481934
Validation loss: 2.566953989767259

Epoch: 6| Step: 1
Training loss: 2.601872444152832
Validation loss: 2.570740215239986

Epoch: 6| Step: 2
Training loss: 2.9218931198120117
Validation loss: 2.5675783977713635

Epoch: 6| Step: 3
Training loss: 3.2593233585357666
Validation loss: 2.569837575317711

Epoch: 6| Step: 4
Training loss: 2.6917431354522705
Validation loss: 2.56822298419091

Epoch: 6| Step: 5
Training loss: 3.193601608276367
Validation loss: 2.56834150386113

Epoch: 6| Step: 6
Training loss: 2.772961139678955
Validation loss: 2.5669628266365296

Epoch: 6| Step: 7
Training loss: 1.3640910387039185
Validation loss: 2.5691174717359644

Epoch: 6| Step: 8
Training loss: 2.859997034072876
Validation loss: 2.5651888770441853

Epoch: 6| Step: 9
Training loss: 2.927264928817749
Validation loss: 2.56432835261027

Epoch: 6| Step: 10
Training loss: 2.3127264976501465
Validation loss: 2.5633265510682137

Epoch: 6| Step: 11
Training loss: 3.3327136039733887
Validation loss: 2.5629496856402327

Epoch: 6| Step: 12
Training loss: 3.230217456817627
Validation loss: 2.5606699605141916

Epoch: 6| Step: 13
Training loss: 2.369210720062256
Validation loss: 2.561843510596983

Epoch: 77| Step: 0
Training loss: 2.2963809967041016
Validation loss: 2.5684631973184566

Epoch: 6| Step: 1
Training loss: 2.4704031944274902
Validation loss: 2.5673587911872455

Epoch: 6| Step: 2
Training loss: 2.4514501094818115
Validation loss: 2.5688225864082255

Epoch: 6| Step: 3
Training loss: 2.9197988510131836
Validation loss: 2.5809846949833695

Epoch: 6| Step: 4
Training loss: 2.6840975284576416
Validation loss: 2.5922502548463884

Epoch: 6| Step: 5
Training loss: 3.136638641357422
Validation loss: 2.5852619832561863

Epoch: 6| Step: 6
Training loss: 1.915784478187561
Validation loss: 2.5920771783398044

Epoch: 6| Step: 7
Training loss: 3.4167046546936035
Validation loss: 2.5884216036847842

Epoch: 6| Step: 8
Training loss: 3.237610340118408
Validation loss: 2.593581276555215

Epoch: 6| Step: 9
Training loss: 3.062931537628174
Validation loss: 2.582031685818908

Epoch: 6| Step: 10
Training loss: 3.1777172088623047
Validation loss: 2.569459722888085

Epoch: 6| Step: 11
Training loss: 2.5930047035217285
Validation loss: 2.5665196577707925

Epoch: 6| Step: 12
Training loss: 2.2409236431121826
Validation loss: 2.564533233642578

Epoch: 6| Step: 13
Training loss: 2.7509753704071045
Validation loss: 2.5607622823407574

Epoch: 78| Step: 0
Training loss: 2.5394506454467773
Validation loss: 2.5587717422875027

Epoch: 6| Step: 1
Training loss: 2.7612557411193848
Validation loss: 2.557140650287751

Epoch: 6| Step: 2
Training loss: 3.1774914264678955
Validation loss: 2.556276290647445

Epoch: 6| Step: 3
Training loss: 3.1444191932678223
Validation loss: 2.5592957235151723

Epoch: 6| Step: 4
Training loss: 2.4681408405303955
Validation loss: 2.5561786723393265

Epoch: 6| Step: 5
Training loss: 3.1934359073638916
Validation loss: 2.559323272397441

Epoch: 6| Step: 6
Training loss: 2.6449971199035645
Validation loss: 2.558428495160995

Epoch: 6| Step: 7
Training loss: 2.4372403621673584
Validation loss: 2.556932294240562

Epoch: 6| Step: 8
Training loss: 2.2584123611450195
Validation loss: 2.5621249163022606

Epoch: 6| Step: 9
Training loss: 2.010727643966675
Validation loss: 2.5593469296732256

Epoch: 6| Step: 10
Training loss: 2.3338985443115234
Validation loss: 2.5634203982609574

Epoch: 6| Step: 11
Training loss: 2.796222686767578
Validation loss: 2.570739879403063

Epoch: 6| Step: 12
Training loss: 3.0179147720336914
Validation loss: 2.5607873624370945

Epoch: 6| Step: 13
Training loss: 4.047887802124023
Validation loss: 2.5641849348621983

Epoch: 79| Step: 0
Training loss: 3.0986990928649902
Validation loss: 2.562105845379573

Epoch: 6| Step: 1
Training loss: 2.6258230209350586
Validation loss: 2.558894565028529

Epoch: 6| Step: 2
Training loss: 1.675328254699707
Validation loss: 2.558760981405935

Epoch: 6| Step: 3
Training loss: 2.6104679107666016
Validation loss: 2.560761349175566

Epoch: 6| Step: 4
Training loss: 2.0744268894195557
Validation loss: 2.5633468140837965

Epoch: 6| Step: 5
Training loss: 3.332984447479248
Validation loss: 2.560393715417513

Epoch: 6| Step: 6
Training loss: 2.1749043464660645
Validation loss: 2.5625539338716896

Epoch: 6| Step: 7
Training loss: 2.8534114360809326
Validation loss: 2.5589997306946786

Epoch: 6| Step: 8
Training loss: 3.290440082550049
Validation loss: 2.5624594226960213

Epoch: 6| Step: 9
Training loss: 3.5131726264953613
Validation loss: 2.558373917815506

Epoch: 6| Step: 10
Training loss: 2.3896536827087402
Validation loss: 2.5588972363420712

Epoch: 6| Step: 11
Training loss: 2.69276762008667
Validation loss: 2.557351094420238

Epoch: 6| Step: 12
Training loss: 2.8685195446014404
Validation loss: 2.559397743594262

Epoch: 6| Step: 13
Training loss: 3.169762372970581
Validation loss: 2.5573851113678305

Epoch: 80| Step: 0
Training loss: 2.6815807819366455
Validation loss: 2.556971808915497

Epoch: 6| Step: 1
Training loss: 2.94122314453125
Validation loss: 2.5621648526960805

Epoch: 6| Step: 2
Training loss: 3.2473764419555664
Validation loss: 2.555148588713779

Epoch: 6| Step: 3
Training loss: 2.8094282150268555
Validation loss: 2.559495272174958

Epoch: 6| Step: 4
Training loss: 2.034806966781616
Validation loss: 2.560504180128856

Epoch: 6| Step: 5
Training loss: 3.244084596633911
Validation loss: 2.56331346624641

Epoch: 6| Step: 6
Training loss: 2.6907286643981934
Validation loss: 2.5646077074030393

Epoch: 6| Step: 7
Training loss: 1.958862066268921
Validation loss: 2.565159014476243

Epoch: 6| Step: 8
Training loss: 2.8367743492126465
Validation loss: 2.567783268549109

Epoch: 6| Step: 9
Training loss: 2.4611001014709473
Validation loss: 2.5670614909100276

Epoch: 6| Step: 10
Training loss: 3.2104451656341553
Validation loss: 2.5613594388449066

Epoch: 6| Step: 11
Training loss: 2.5257351398468018
Validation loss: 2.5624738303563928

Epoch: 6| Step: 12
Training loss: 2.5338077545166016
Validation loss: 2.5579261805421565

Epoch: 6| Step: 13
Training loss: 3.2505292892456055
Validation loss: 2.555755261451967

Epoch: 81| Step: 0
Training loss: 2.9829797744750977
Validation loss: 2.552400117279381

Epoch: 6| Step: 1
Training loss: 1.93404221534729
Validation loss: 2.5491708106892084

Epoch: 6| Step: 2
Training loss: 2.726400852203369
Validation loss: 2.553383296535861

Epoch: 6| Step: 3
Training loss: 2.96687650680542
Validation loss: 2.550709396280268

Epoch: 6| Step: 4
Training loss: 3.0354716777801514
Validation loss: 2.5536006958253923

Epoch: 6| Step: 5
Training loss: 2.529977321624756
Validation loss: 2.5539942146629415

Epoch: 6| Step: 6
Training loss: 2.728376865386963
Validation loss: 2.5517481937203357

Epoch: 6| Step: 7
Training loss: 2.303372383117676
Validation loss: 2.5499025672994633

Epoch: 6| Step: 8
Training loss: 3.65614914894104
Validation loss: 2.549557134669314

Epoch: 6| Step: 9
Training loss: 2.8044495582580566
Validation loss: 2.548163124310073

Epoch: 6| Step: 10
Training loss: 2.123037576675415
Validation loss: 2.5469674653904413

Epoch: 6| Step: 11
Training loss: 3.144847869873047
Validation loss: 2.5493762544406358

Epoch: 6| Step: 12
Training loss: 2.2758259773254395
Validation loss: 2.5486792748974216

Epoch: 6| Step: 13
Training loss: 3.301898956298828
Validation loss: 2.550745392358431

Epoch: 82| Step: 0
Training loss: 2.503589630126953
Validation loss: 2.553435100022183

Epoch: 6| Step: 1
Training loss: 2.5959482192993164
Validation loss: 2.551165993495654

Epoch: 6| Step: 2
Training loss: 2.440092086791992
Validation loss: 2.554327849418886

Epoch: 6| Step: 3
Training loss: 2.4434542655944824
Validation loss: 2.5580099539090226

Epoch: 6| Step: 4
Training loss: 3.4594335556030273
Validation loss: 2.5560818641416487

Epoch: 6| Step: 5
Training loss: 2.587131977081299
Validation loss: 2.549416372852941

Epoch: 6| Step: 6
Training loss: 2.9875664710998535
Validation loss: 2.5505912996107534

Epoch: 6| Step: 7
Training loss: 3.0573692321777344
Validation loss: 2.5507966010801253

Epoch: 6| Step: 8
Training loss: 2.8449490070343018
Validation loss: 2.548206583146126

Epoch: 6| Step: 9
Training loss: 3.129624128341675
Validation loss: 2.548537528643044

Epoch: 6| Step: 10
Training loss: 3.06059193611145
Validation loss: 2.548672265903924

Epoch: 6| Step: 11
Training loss: 2.2031068801879883
Validation loss: 2.5445150841948805

Epoch: 6| Step: 12
Training loss: 2.732975959777832
Validation loss: 2.54681202929507

Epoch: 6| Step: 13
Training loss: 1.5484802722930908
Validation loss: 2.5476080204850886

Epoch: 83| Step: 0
Training loss: 2.3527779579162598
Validation loss: 2.54932096440305

Epoch: 6| Step: 1
Training loss: 2.6943068504333496
Validation loss: 2.5482940109827186

Epoch: 6| Step: 2
Training loss: 2.2557532787323
Validation loss: 2.545259552617227

Epoch: 6| Step: 3
Training loss: 3.586915969848633
Validation loss: 2.5467052818626486

Epoch: 6| Step: 4
Training loss: 3.3210651874542236
Validation loss: 2.5469425852580736

Epoch: 6| Step: 5
Training loss: 3.029649257659912
Validation loss: 2.5468232759865383

Epoch: 6| Step: 6
Training loss: 2.06788969039917
Validation loss: 2.5484657210688435

Epoch: 6| Step: 7
Training loss: 2.427833080291748
Validation loss: 2.5457623338186615

Epoch: 6| Step: 8
Training loss: 3.6706361770629883
Validation loss: 2.5471519911161034

Epoch: 6| Step: 9
Training loss: 2.3667349815368652
Validation loss: 2.5469014285713114

Epoch: 6| Step: 10
Training loss: 2.5296244621276855
Validation loss: 2.555159668768606

Epoch: 6| Step: 11
Training loss: 2.473639488220215
Validation loss: 2.560366246008104

Epoch: 6| Step: 12
Training loss: 2.9700570106506348
Validation loss: 2.566462757766888

Epoch: 6| Step: 13
Training loss: 2.098236560821533
Validation loss: 2.565677894059048

Epoch: 84| Step: 0
Training loss: 2.9306488037109375
Validation loss: 2.5737132449303903

Epoch: 6| Step: 1
Training loss: 2.490415096282959
Validation loss: 2.5567617980382775

Epoch: 6| Step: 2
Training loss: 2.643502712249756
Validation loss: 2.5518929676343034

Epoch: 6| Step: 3
Training loss: 2.508807420730591
Validation loss: 2.545409961413312

Epoch: 6| Step: 4
Training loss: 2.658515453338623
Validation loss: 2.545520956798266

Epoch: 6| Step: 5
Training loss: 2.494309902191162
Validation loss: 2.543770936227614

Epoch: 6| Step: 6
Training loss: 2.8804826736450195
Validation loss: 2.5429492970948577

Epoch: 6| Step: 7
Training loss: 2.414060115814209
Validation loss: 2.541957788569953

Epoch: 6| Step: 8
Training loss: 3.7298731803894043
Validation loss: 2.542903105417887

Epoch: 6| Step: 9
Training loss: 3.4390408992767334
Validation loss: 2.544217430135255

Epoch: 6| Step: 10
Training loss: 2.833501100540161
Validation loss: 2.5408861226932977

Epoch: 6| Step: 11
Training loss: 2.9801559448242188
Validation loss: 2.537288224825295

Epoch: 6| Step: 12
Training loss: 2.132441997528076
Validation loss: 2.5379552533549647

Epoch: 6| Step: 13
Training loss: 1.4128832817077637
Validation loss: 2.5416299925055554

Epoch: 85| Step: 0
Training loss: 3.3645501136779785
Validation loss: 2.5389444981851885

Epoch: 6| Step: 1
Training loss: 2.4267680644989014
Validation loss: 2.5377456885512157

Epoch: 6| Step: 2
Training loss: 2.619886875152588
Validation loss: 2.5354910486487934

Epoch: 6| Step: 3
Training loss: 3.498488426208496
Validation loss: 2.5329361628460627

Epoch: 6| Step: 4
Training loss: 3.079775333404541
Validation loss: 2.539652387301127

Epoch: 6| Step: 5
Training loss: 2.522115468978882
Validation loss: 2.5363885459079536

Epoch: 6| Step: 6
Training loss: 2.6937389373779297
Validation loss: 2.5368048632016746

Epoch: 6| Step: 7
Training loss: 2.782611846923828
Validation loss: 2.538802595548732

Epoch: 6| Step: 8
Training loss: 2.9993157386779785
Validation loss: 2.5385045620702926

Epoch: 6| Step: 9
Training loss: 2.5128989219665527
Validation loss: 2.5402352989360852

Epoch: 6| Step: 10
Training loss: 2.112285852432251
Validation loss: 2.544472894360942

Epoch: 6| Step: 11
Training loss: 3.0928072929382324
Validation loss: 2.544771763586229

Epoch: 6| Step: 12
Training loss: 2.55729603767395
Validation loss: 2.538531449533278

Epoch: 6| Step: 13
Training loss: 1.029762625694275
Validation loss: 2.5347806817741803

Epoch: 86| Step: 0
Training loss: 2.7734827995300293
Validation loss: 2.5457540968412995

Epoch: 6| Step: 1
Training loss: 2.1148462295532227
Validation loss: 2.5357615870814167

Epoch: 6| Step: 2
Training loss: 2.750175952911377
Validation loss: 2.531013391351187

Epoch: 6| Step: 3
Training loss: 2.3117754459381104
Validation loss: 2.532859376681748

Epoch: 6| Step: 4
Training loss: 3.127866744995117
Validation loss: 2.531125504483459

Epoch: 6| Step: 5
Training loss: 2.99276065826416
Validation loss: 2.530828606697821

Epoch: 6| Step: 6
Training loss: 2.2278547286987305
Validation loss: 2.5263767729523363

Epoch: 6| Step: 7
Training loss: 3.116154432296753
Validation loss: 2.5305640056569088

Epoch: 6| Step: 8
Training loss: 2.4852867126464844
Validation loss: 2.5290633568199734

Epoch: 6| Step: 9
Training loss: 2.981726884841919
Validation loss: 2.532047515274376

Epoch: 6| Step: 10
Training loss: 2.421760082244873
Validation loss: 2.530487883475519

Epoch: 6| Step: 11
Training loss: 3.157919406890869
Validation loss: 2.5241048823120775

Epoch: 6| Step: 12
Training loss: 2.801551342010498
Validation loss: 2.5277218716118925

Epoch: 6| Step: 13
Training loss: 2.6146960258483887
Validation loss: 2.531219723404095

Epoch: 87| Step: 0
Training loss: 2.535576581954956
Validation loss: 2.5384820251054663

Epoch: 6| Step: 1
Training loss: 2.8254852294921875
Validation loss: 2.54937655438659

Epoch: 6| Step: 2
Training loss: 2.186401605606079
Validation loss: 2.558463424764654

Epoch: 6| Step: 3
Training loss: 2.34914493560791
Validation loss: 2.576293468475342

Epoch: 6| Step: 4
Training loss: 3.0558762550354004
Validation loss: 2.5923913589087864

Epoch: 6| Step: 5
Training loss: 2.6371963024139404
Validation loss: 2.601580153229416

Epoch: 6| Step: 6
Training loss: 2.4311652183532715
Validation loss: 2.5890945208969938

Epoch: 6| Step: 7
Training loss: 2.8927881717681885
Validation loss: 2.5738986897212204

Epoch: 6| Step: 8
Training loss: 2.6080098152160645
Validation loss: 2.556352076991912

Epoch: 6| Step: 9
Training loss: 2.6061720848083496
Validation loss: 2.5491858759234027

Epoch: 6| Step: 10
Training loss: 3.6059350967407227
Validation loss: 2.533700253373833

Epoch: 6| Step: 11
Training loss: 2.883035182952881
Validation loss: 2.5297231699830744

Epoch: 6| Step: 12
Training loss: 3.120083808898926
Validation loss: 2.522875749936668

Epoch: 6| Step: 13
Training loss: 1.964363932609558
Validation loss: 2.5243437238918838

Epoch: 88| Step: 0
Training loss: 2.3634324073791504
Validation loss: 2.5281319156769784

Epoch: 6| Step: 1
Training loss: 2.9386844635009766
Validation loss: 2.527174688154651

Epoch: 6| Step: 2
Training loss: 2.7552976608276367
Validation loss: 2.526427102345292

Epoch: 6| Step: 3
Training loss: 2.993760108947754
Validation loss: 2.5261892849399197

Epoch: 6| Step: 4
Training loss: 2.4483537673950195
Validation loss: 2.5296976950860794

Epoch: 6| Step: 5
Training loss: 3.4914119243621826
Validation loss: 2.5262508469243206

Epoch: 6| Step: 6
Training loss: 2.85345458984375
Validation loss: 2.528481445004863

Epoch: 6| Step: 7
Training loss: 2.6055285930633545
Validation loss: 2.5249663655475905

Epoch: 6| Step: 8
Training loss: 2.6057538986206055
Validation loss: 2.5255718205564763

Epoch: 6| Step: 9
Training loss: 2.880474090576172
Validation loss: 2.5280513609609296

Epoch: 6| Step: 10
Training loss: 2.320699691772461
Validation loss: 2.5250629378903295

Epoch: 6| Step: 11
Training loss: 2.604886054992676
Validation loss: 2.5279934431916926

Epoch: 6| Step: 12
Training loss: 2.289283275604248
Validation loss: 2.531434626989467

Epoch: 6| Step: 13
Training loss: 2.9922945499420166
Validation loss: 2.5316707498283795

Epoch: 89| Step: 0
Training loss: 2.7882914543151855
Validation loss: 2.5336872967340613

Epoch: 6| Step: 1
Training loss: 2.323991298675537
Validation loss: 2.53493578972355

Epoch: 6| Step: 2
Training loss: 3.6957345008850098
Validation loss: 2.5342281659444175

Epoch: 6| Step: 3
Training loss: 1.8952277898788452
Validation loss: 2.523540307116765

Epoch: 6| Step: 4
Training loss: 3.3917882442474365
Validation loss: 2.5257067193267164

Epoch: 6| Step: 5
Training loss: 2.228851556777954
Validation loss: 2.535292894609513

Epoch: 6| Step: 6
Training loss: 2.012253999710083
Validation loss: 2.530578508171984

Epoch: 6| Step: 7
Training loss: 2.914477825164795
Validation loss: 2.528578637748636

Epoch: 6| Step: 8
Training loss: 3.1201467514038086
Validation loss: 2.5279795431321666

Epoch: 6| Step: 9
Training loss: 2.576964855194092
Validation loss: 2.524975258816955

Epoch: 6| Step: 10
Training loss: 2.7875783443450928
Validation loss: 2.5302874298505884

Epoch: 6| Step: 11
Training loss: 2.7757930755615234
Validation loss: 2.5277103044653453

Epoch: 6| Step: 12
Training loss: 2.737109422683716
Validation loss: 2.5267869169994066

Epoch: 6| Step: 13
Training loss: 2.5492334365844727
Validation loss: 2.5187333886341383

Epoch: 90| Step: 0
Training loss: 2.392031669616699
Validation loss: 2.518625261963055

Epoch: 6| Step: 1
Training loss: 2.610691547393799
Validation loss: 2.5207214868196877

Epoch: 6| Step: 2
Training loss: 2.775400161743164
Validation loss: 2.515519816388366

Epoch: 6| Step: 3
Training loss: 2.2073814868927
Validation loss: 2.5163268171330935

Epoch: 6| Step: 4
Training loss: 2.5565826892852783
Validation loss: 2.5161427041535736

Epoch: 6| Step: 5
Training loss: 2.8522324562072754
Validation loss: 2.5124211208794707

Epoch: 6| Step: 6
Training loss: 2.8987674713134766
Validation loss: 2.515546291105209

Epoch: 6| Step: 7
Training loss: 2.7520053386688232
Validation loss: 2.513405999829692

Epoch: 6| Step: 8
Training loss: 2.2773938179016113
Validation loss: 2.515835492841659

Epoch: 6| Step: 9
Training loss: 3.0622153282165527
Validation loss: 2.516897729648057

Epoch: 6| Step: 10
Training loss: 2.316608190536499
Validation loss: 2.5137608564028175

Epoch: 6| Step: 11
Training loss: 3.0049211978912354
Validation loss: 2.511842925061462

Epoch: 6| Step: 12
Training loss: 3.283634662628174
Validation loss: 2.5090537635228967

Epoch: 6| Step: 13
Training loss: 3.0122387409210205
Validation loss: 2.5117942287075903

Epoch: 91| Step: 0
Training loss: 2.5184953212738037
Validation loss: 2.5140936579755557

Epoch: 6| Step: 1
Training loss: 3.3571114540100098
Validation loss: 2.512061203679731

Epoch: 6| Step: 2
Training loss: 2.5948495864868164
Validation loss: 2.509918802527971

Epoch: 6| Step: 3
Training loss: 2.7507081031799316
Validation loss: 2.516836227909211

Epoch: 6| Step: 4
Training loss: 2.7526721954345703
Validation loss: 2.5166381584700717

Epoch: 6| Step: 5
Training loss: 2.292384386062622
Validation loss: 2.5152057806650796

Epoch: 6| Step: 6
Training loss: 2.051464080810547
Validation loss: 2.520208699728853

Epoch: 6| Step: 7
Training loss: 2.881235361099243
Validation loss: 2.5213239141689834

Epoch: 6| Step: 8
Training loss: 2.8413212299346924
Validation loss: 2.5224504752825667

Epoch: 6| Step: 9
Training loss: 2.645339012145996
Validation loss: 2.5313467210338962

Epoch: 6| Step: 10
Training loss: 2.8683676719665527
Validation loss: 2.5376853789052656

Epoch: 6| Step: 11
Training loss: 2.935817241668701
Validation loss: 2.5498686375156527

Epoch: 6| Step: 12
Training loss: 2.4162795543670654
Validation loss: 2.5424704269696305

Epoch: 6| Step: 13
Training loss: 3.0766494274139404
Validation loss: 2.53865159198802

Epoch: 92| Step: 0
Training loss: 3.025749444961548
Validation loss: 2.544447506627729

Epoch: 6| Step: 1
Training loss: 2.9419498443603516
Validation loss: 2.5459383610756166

Epoch: 6| Step: 2
Training loss: 2.6032485961914062
Validation loss: 2.539306120205951

Epoch: 6| Step: 3
Training loss: 2.7546849250793457
Validation loss: 2.536365606451547

Epoch: 6| Step: 4
Training loss: 2.4929518699645996
Validation loss: 2.5395353327515306

Epoch: 6| Step: 5
Training loss: 1.7887749671936035
Validation loss: 2.543960617434594

Epoch: 6| Step: 6
Training loss: 2.265377998352051
Validation loss: 2.5332512240256033

Epoch: 6| Step: 7
Training loss: 2.7748937606811523
Validation loss: 2.532498485298567

Epoch: 6| Step: 8
Training loss: 2.7324514389038086
Validation loss: 2.518789319581883

Epoch: 6| Step: 9
Training loss: 2.323403835296631
Validation loss: 2.5166045670868247

Epoch: 6| Step: 10
Training loss: 3.6546754837036133
Validation loss: 2.509248169519568

Epoch: 6| Step: 11
Training loss: 3.273421287536621
Validation loss: 2.505752081512123

Epoch: 6| Step: 12
Training loss: 2.4476916790008545
Validation loss: 2.5090467109475085

Epoch: 6| Step: 13
Training loss: 2.718439817428589
Validation loss: 2.512334480080553

Epoch: 93| Step: 0
Training loss: 2.8237080574035645
Validation loss: 2.510384036648658

Epoch: 6| Step: 1
Training loss: 3.0779242515563965
Validation loss: 2.50892880142376

Epoch: 6| Step: 2
Training loss: 2.606938362121582
Validation loss: 2.5094777243111723

Epoch: 6| Step: 3
Training loss: 2.726266860961914
Validation loss: 2.5086556557686097

Epoch: 6| Step: 4
Training loss: 2.9526565074920654
Validation loss: 2.511736931339387

Epoch: 6| Step: 5
Training loss: 2.5676932334899902
Validation loss: 2.5067994530482958

Epoch: 6| Step: 6
Training loss: 2.747501850128174
Validation loss: 2.5078913319495415

Epoch: 6| Step: 7
Training loss: 2.2454700469970703
Validation loss: 2.5075875943706882

Epoch: 6| Step: 8
Training loss: 3.108132839202881
Validation loss: 2.5046026950241416

Epoch: 6| Step: 9
Training loss: 2.271188259124756
Validation loss: 2.5056724317612185

Epoch: 6| Step: 10
Training loss: 3.147818088531494
Validation loss: 2.5062613820516937

Epoch: 6| Step: 11
Training loss: 1.9990086555480957
Validation loss: 2.5044371325482606

Epoch: 6| Step: 12
Training loss: 2.7303361892700195
Validation loss: 2.5067651733275382

Epoch: 6| Step: 13
Training loss: 2.939436912536621
Validation loss: 2.5087131479735016

Epoch: 94| Step: 0
Training loss: 1.5083534717559814
Validation loss: 2.5143388984023884

Epoch: 6| Step: 1
Training loss: 2.479326009750366
Validation loss: 2.513730074769707

Epoch: 6| Step: 2
Training loss: 2.1637136936187744
Validation loss: 2.5187654008147535

Epoch: 6| Step: 3
Training loss: 3.449223756790161
Validation loss: 2.523643050142514

Epoch: 6| Step: 4
Training loss: 3.128798723220825
Validation loss: 2.516251746044364

Epoch: 6| Step: 5
Training loss: 2.7765374183654785
Validation loss: 2.509764996908044

Epoch: 6| Step: 6
Training loss: 3.7424492835998535
Validation loss: 2.5139736155027985

Epoch: 6| Step: 7
Training loss: 2.4173765182495117
Validation loss: 2.5105260572125836

Epoch: 6| Step: 8
Training loss: 2.854952096939087
Validation loss: 2.5090751801767657

Epoch: 6| Step: 9
Training loss: 2.7196085453033447
Validation loss: 2.507365652309951

Epoch: 6| Step: 10
Training loss: 2.6368916034698486
Validation loss: 2.5062546986405567

Epoch: 6| Step: 11
Training loss: 2.7657480239868164
Validation loss: 2.5042605579540296

Epoch: 6| Step: 12
Training loss: 2.3965845108032227
Validation loss: 2.503945130173878

Epoch: 6| Step: 13
Training loss: 2.736849546432495
Validation loss: 2.504473747745637

Epoch: 95| Step: 0
Training loss: 2.8522677421569824
Validation loss: 2.5038419385110178

Epoch: 6| Step: 1
Training loss: 2.5568158626556396
Validation loss: 2.503772833014047

Epoch: 6| Step: 2
Training loss: 3.113821268081665
Validation loss: 2.5074203398919876

Epoch: 6| Step: 3
Training loss: 1.864762783050537
Validation loss: 2.503372794838362

Epoch: 6| Step: 4
Training loss: 3.236952304840088
Validation loss: 2.5132759283947688

Epoch: 6| Step: 5
Training loss: 2.9145610332489014
Validation loss: 2.5135199908287293

Epoch: 6| Step: 6
Training loss: 3.1268370151519775
Validation loss: 2.5107021075423046

Epoch: 6| Step: 7
Training loss: 1.8932355642318726
Validation loss: 2.5120890653261574

Epoch: 6| Step: 8
Training loss: 2.3726558685302734
Validation loss: 2.509541473081035

Epoch: 6| Step: 9
Training loss: 3.058405876159668
Validation loss: 2.5120134674092776

Epoch: 6| Step: 10
Training loss: 3.48209285736084
Validation loss: 2.509725985988494

Epoch: 6| Step: 11
Training loss: 2.262298107147217
Validation loss: 2.5058987166291926

Epoch: 6| Step: 12
Training loss: 2.6016664505004883
Validation loss: 2.5074122387875795

Epoch: 6| Step: 13
Training loss: 2.242249011993408
Validation loss: 2.5100100142981416

Epoch: 96| Step: 0
Training loss: 2.9576382637023926
Validation loss: 2.5018147140420894

Epoch: 6| Step: 1
Training loss: 2.1578946113586426
Validation loss: 2.5033214681891987

Epoch: 6| Step: 2
Training loss: 2.6611034870147705
Validation loss: 2.500143233165946

Epoch: 6| Step: 3
Training loss: 2.5217339992523193
Validation loss: 2.5004604554945424

Epoch: 6| Step: 4
Training loss: 2.9275283813476562
Validation loss: 2.5017158113500124

Epoch: 6| Step: 5
Training loss: 2.552386999130249
Validation loss: 2.5005123692174114

Epoch: 6| Step: 6
Training loss: 2.443081855773926
Validation loss: 2.499792521999728

Epoch: 6| Step: 7
Training loss: 3.247575521469116
Validation loss: 2.49818766245278

Epoch: 6| Step: 8
Training loss: 3.158491611480713
Validation loss: 2.498143921616257

Epoch: 6| Step: 9
Training loss: 2.6332082748413086
Validation loss: 2.5026478100848455

Epoch: 6| Step: 10
Training loss: 2.421982526779175
Validation loss: 2.5005767524883313

Epoch: 6| Step: 11
Training loss: 2.6031384468078613
Validation loss: 2.4978154474689114

Epoch: 6| Step: 12
Training loss: 2.3349575996398926
Validation loss: 2.501823886748283

Epoch: 6| Step: 13
Training loss: 3.4139208793640137
Validation loss: 2.5063534705869612

Epoch: 97| Step: 0
Training loss: 2.3713741302490234
Validation loss: 2.5065684010905604

Epoch: 6| Step: 1
Training loss: 2.334928035736084
Validation loss: 2.512306595361361

Epoch: 6| Step: 2
Training loss: 2.3625354766845703
Validation loss: 2.5067198712338685

Epoch: 6| Step: 3
Training loss: 2.2906477451324463
Validation loss: 2.521004471727597

Epoch: 6| Step: 4
Training loss: 2.914583206176758
Validation loss: 2.5241995293606996

Epoch: 6| Step: 5
Training loss: 2.9579355716705322
Validation loss: 2.526195623541391

Epoch: 6| Step: 6
Training loss: 2.74025821685791
Validation loss: 2.527836807312504

Epoch: 6| Step: 7
Training loss: 2.9466347694396973
Validation loss: 2.5346835428668606

Epoch: 6| Step: 8
Training loss: 2.5899295806884766
Validation loss: 2.5272574117106776

Epoch: 6| Step: 9
Training loss: 2.8808114528656006
Validation loss: 2.522090035100137

Epoch: 6| Step: 10
Training loss: 3.0605335235595703
Validation loss: 2.5104874974937847

Epoch: 6| Step: 11
Training loss: 3.0751945972442627
Validation loss: 2.5069023563015844

Epoch: 6| Step: 12
Training loss: 3.130168914794922
Validation loss: 2.4987996957635366

Epoch: 6| Step: 13
Training loss: 1.6739228963851929
Validation loss: 2.500353508098151

Epoch: 98| Step: 0
Training loss: 2.393679141998291
Validation loss: 2.4981424475228913

Epoch: 6| Step: 1
Training loss: 2.710655689239502
Validation loss: 2.502302377454696

Epoch: 6| Step: 2
Training loss: 3.272813320159912
Validation loss: 2.50583190302695

Epoch: 6| Step: 3
Training loss: 2.713505268096924
Validation loss: 2.5047047548396613

Epoch: 6| Step: 4
Training loss: 2.7384424209594727
Validation loss: 2.508705057123656

Epoch: 6| Step: 5
Training loss: 2.946448802947998
Validation loss: 2.5034679776878765

Epoch: 6| Step: 6
Training loss: 3.0810043811798096
Validation loss: 2.5023382607326714

Epoch: 6| Step: 7
Training loss: 2.543365001678467
Validation loss: 2.499412111056748

Epoch: 6| Step: 8
Training loss: 2.1259427070617676
Validation loss: 2.4994461254407

Epoch: 6| Step: 9
Training loss: 2.1737656593322754
Validation loss: 2.496205699059271

Epoch: 6| Step: 10
Training loss: 2.7265677452087402
Validation loss: 2.495377194496893

Epoch: 6| Step: 11
Training loss: 2.4442296028137207
Validation loss: 2.4943775105220016

Epoch: 6| Step: 12
Training loss: 2.697262763977051
Validation loss: 2.501302980607556

Epoch: 6| Step: 13
Training loss: 3.742509126663208
Validation loss: 2.498008128135435

Epoch: 99| Step: 0
Training loss: 2.3329243659973145
Validation loss: 2.50172161030513

Epoch: 6| Step: 1
Training loss: 2.530038595199585
Validation loss: 2.504107465026199

Epoch: 6| Step: 2
Training loss: 3.48386812210083
Validation loss: 2.501858539478753

Epoch: 6| Step: 3
Training loss: 2.6948347091674805
Validation loss: 2.5004127948514876

Epoch: 6| Step: 4
Training loss: 2.5764904022216797
Validation loss: 2.4998980132482385

Epoch: 6| Step: 5
Training loss: 3.1267192363739014
Validation loss: 2.498767060618247

Epoch: 6| Step: 6
Training loss: 2.963010549545288
Validation loss: 2.497390595815515

Epoch: 6| Step: 7
Training loss: 2.6779699325561523
Validation loss: 2.489871253249466

Epoch: 6| Step: 8
Training loss: 2.534106731414795
Validation loss: 2.492369241611932

Epoch: 6| Step: 9
Training loss: 1.9721604585647583
Validation loss: 2.4943872215927287

Epoch: 6| Step: 10
Training loss: 2.729698896408081
Validation loss: 2.4931558152680755

Epoch: 6| Step: 11
Training loss: 2.5610158443450928
Validation loss: 2.4985657430464223

Epoch: 6| Step: 12
Training loss: 3.18117618560791
Validation loss: 2.4932547205237934

Epoch: 6| Step: 13
Training loss: 2.2252485752105713
Validation loss: 2.4940057621207288

Epoch: 100| Step: 0
Training loss: 2.290414333343506
Validation loss: 2.4974238795618855

Epoch: 6| Step: 1
Training loss: 3.3866539001464844
Validation loss: 2.5011095090578963

Epoch: 6| Step: 2
Training loss: 1.888532042503357
Validation loss: 2.5045479523238314

Epoch: 6| Step: 3
Training loss: 2.713759422302246
Validation loss: 2.5153774599875174

Epoch: 6| Step: 4
Training loss: 3.2891428470611572
Validation loss: 2.5279979731446955

Epoch: 6| Step: 5
Training loss: 2.5631237030029297
Validation loss: 2.5406355934758342

Epoch: 6| Step: 6
Training loss: 2.151158332824707
Validation loss: 2.544290716930102

Epoch: 6| Step: 7
Training loss: 2.7019543647766113
Validation loss: 2.5590833092248566

Epoch: 6| Step: 8
Training loss: 3.4205305576324463
Validation loss: 2.5539454388362106

Epoch: 6| Step: 9
Training loss: 2.4146523475646973
Validation loss: 2.529251483178908

Epoch: 6| Step: 10
Training loss: 2.722275495529175
Validation loss: 2.5214075529447166

Epoch: 6| Step: 11
Training loss: 2.3298048973083496
Validation loss: 2.501974574981197

Epoch: 6| Step: 12
Training loss: 2.934331178665161
Validation loss: 2.495265758165749

Epoch: 6| Step: 13
Training loss: 3.1669728755950928
Validation loss: 2.4914457259639615

Epoch: 101| Step: 0
Training loss: 2.138667345046997
Validation loss: 2.4895215162666897

Epoch: 6| Step: 1
Training loss: 2.9811737537384033
Validation loss: 2.4928669980777207

Epoch: 6| Step: 2
Training loss: 2.903079032897949
Validation loss: 2.4991216839000745

Epoch: 6| Step: 3
Training loss: 3.0472934246063232
Validation loss: 2.5038894863538843

Epoch: 6| Step: 4
Training loss: 2.616865634918213
Validation loss: 2.506177261311521

Epoch: 6| Step: 5
Training loss: 2.8036909103393555
Validation loss: 2.50210242373969

Epoch: 6| Step: 6
Training loss: 2.1733906269073486
Validation loss: 2.5044261358117543

Epoch: 6| Step: 7
Training loss: 2.2115590572357178
Validation loss: 2.501568745541316

Epoch: 6| Step: 8
Training loss: 3.163475275039673
Validation loss: 2.499289817707513

Epoch: 6| Step: 9
Training loss: 2.476219654083252
Validation loss: 2.500337903217603

Epoch: 6| Step: 10
Training loss: 3.161818504333496
Validation loss: 2.498197035122943

Epoch: 6| Step: 11
Training loss: 2.2903997898101807
Validation loss: 2.497800532207694

Epoch: 6| Step: 12
Training loss: 3.0581040382385254
Validation loss: 2.5007595221201577

Epoch: 6| Step: 13
Training loss: 2.825045347213745
Validation loss: 2.500800299388106

Epoch: 102| Step: 0
Training loss: 2.7751548290252686
Validation loss: 2.501226963535432

Epoch: 6| Step: 1
Training loss: 1.856586217880249
Validation loss: 2.5025870646199873

Epoch: 6| Step: 2
Training loss: 3.429253339767456
Validation loss: 2.4987992599446285

Epoch: 6| Step: 3
Training loss: 2.968888521194458
Validation loss: 2.495550222294305

Epoch: 6| Step: 4
Training loss: 2.684669256210327
Validation loss: 2.494926949983002

Epoch: 6| Step: 5
Training loss: 2.4830751419067383
Validation loss: 2.4995690212454846

Epoch: 6| Step: 6
Training loss: 2.308736801147461
Validation loss: 2.503817799270794

Epoch: 6| Step: 7
Training loss: 2.6727781295776367
Validation loss: 2.4994233987664662

Epoch: 6| Step: 8
Training loss: 2.35532808303833
Validation loss: 2.499772669166647

Epoch: 6| Step: 9
Training loss: 3.332468032836914
Validation loss: 2.498795017119377

Epoch: 6| Step: 10
Training loss: 3.6161234378814697
Validation loss: 2.496743256045926

Epoch: 6| Step: 11
Training loss: 2.541656255722046
Validation loss: 2.494566161145446

Epoch: 6| Step: 12
Training loss: 2.1137452125549316
Validation loss: 2.4932965552935036

Epoch: 6| Step: 13
Training loss: 2.3179984092712402
Validation loss: 2.4960799294133342

Epoch: 103| Step: 0
Training loss: 2.618136405944824
Validation loss: 2.4925566616878716

Epoch: 6| Step: 1
Training loss: 1.9126007556915283
Validation loss: 2.494830035394238

Epoch: 6| Step: 2
Training loss: 3.5689942836761475
Validation loss: 2.4951696447146836

Epoch: 6| Step: 3
Training loss: 3.122743844985962
Validation loss: 2.5032225244788715

Epoch: 6| Step: 4
Training loss: 3.0633864402770996
Validation loss: 2.5043986612750637

Epoch: 6| Step: 5
Training loss: 2.748645067214966
Validation loss: 2.5006344164571455

Epoch: 6| Step: 6
Training loss: 3.0680785179138184
Validation loss: 2.4913612463141

Epoch: 6| Step: 7
Training loss: 2.602081537246704
Validation loss: 2.497233775354201

Epoch: 6| Step: 8
Training loss: 2.372941732406616
Validation loss: 2.4897806670076106

Epoch: 6| Step: 9
Training loss: 2.1063666343688965
Validation loss: 2.4887925360792424

Epoch: 6| Step: 10
Training loss: 2.2433977127075195
Validation loss: 2.494633633603332

Epoch: 6| Step: 11
Training loss: 3.237992286682129
Validation loss: 2.49060094869265

Epoch: 6| Step: 12
Training loss: 2.3553481101989746
Validation loss: 2.4879050844459125

Epoch: 6| Step: 13
Training loss: 2.450789451599121
Validation loss: 2.493920300596504

Epoch: 104| Step: 0
Training loss: 2.933156728744507
Validation loss: 2.4925298357522614

Epoch: 6| Step: 1
Training loss: 2.8997037410736084
Validation loss: 2.491155260352678

Epoch: 6| Step: 2
Training loss: 2.9136602878570557
Validation loss: 2.484404020411994

Epoch: 6| Step: 3
Training loss: 1.978623867034912
Validation loss: 2.488805168418474

Epoch: 6| Step: 4
Training loss: 2.4988512992858887
Validation loss: 2.4914928431152017

Epoch: 6| Step: 5
Training loss: 2.8316001892089844
Validation loss: 2.4856422485843783

Epoch: 6| Step: 6
Training loss: 2.726423740386963
Validation loss: 2.4863917443060104

Epoch: 6| Step: 7
Training loss: 3.1674554347991943
Validation loss: 2.4881987110260995

Epoch: 6| Step: 8
Training loss: 2.872218370437622
Validation loss: 2.4871006845146097

Epoch: 6| Step: 9
Training loss: 2.7442774772644043
Validation loss: 2.4920876513245287

Epoch: 6| Step: 10
Training loss: 2.662801742553711
Validation loss: 2.487544987791328

Epoch: 6| Step: 11
Training loss: 2.5945887565612793
Validation loss: 2.4885976750363588

Epoch: 6| Step: 12
Training loss: 2.0720555782318115
Validation loss: 2.4883074375890915

Epoch: 6| Step: 13
Training loss: 2.5766329765319824
Validation loss: 2.492309965113158

Epoch: 105| Step: 0
Training loss: 3.261284351348877
Validation loss: 2.4897384182099374

Epoch: 6| Step: 1
Training loss: 2.7960379123687744
Validation loss: 2.4920075273001068

Epoch: 6| Step: 2
Training loss: 2.2593533992767334
Validation loss: 2.494856554974792

Epoch: 6| Step: 3
Training loss: 2.4986109733581543
Validation loss: 2.496984738175587

Epoch: 6| Step: 4
Training loss: 2.6751465797424316
Validation loss: 2.4919183100423505

Epoch: 6| Step: 5
Training loss: 3.5210537910461426
Validation loss: 2.500326346325618

Epoch: 6| Step: 6
Training loss: 2.524052381515503
Validation loss: 2.4989431442752963

Epoch: 6| Step: 7
Training loss: 2.8688912391662598
Validation loss: 2.4989227658958844

Epoch: 6| Step: 8
Training loss: 2.826942205429077
Validation loss: 2.4983677966620332

Epoch: 6| Step: 9
Training loss: 2.9047656059265137
Validation loss: 2.495527034164757

Epoch: 6| Step: 10
Training loss: 1.7513326406478882
Validation loss: 2.4904066388325026

Epoch: 6| Step: 11
Training loss: 2.6276063919067383
Validation loss: 2.4880626714357765

Epoch: 6| Step: 12
Training loss: 2.0354464054107666
Validation loss: 2.482944029633717

Epoch: 6| Step: 13
Training loss: 3.2787513732910156
Validation loss: 2.483707361323859

Epoch: 106| Step: 0
Training loss: 3.0477848052978516
Validation loss: 2.482436312142239

Epoch: 6| Step: 1
Training loss: 2.544440269470215
Validation loss: 2.482021095932171

Epoch: 6| Step: 2
Training loss: 3.0168166160583496
Validation loss: 2.4868350208446546

Epoch: 6| Step: 3
Training loss: 3.150968074798584
Validation loss: 2.483807697091051

Epoch: 6| Step: 4
Training loss: 3.1109232902526855
Validation loss: 2.4809907482516382

Epoch: 6| Step: 5
Training loss: 2.672314167022705
Validation loss: 2.483335200176444

Epoch: 6| Step: 6
Training loss: 3.1287145614624023
Validation loss: 2.4826295965461322

Epoch: 6| Step: 7
Training loss: 2.3333189487457275
Validation loss: 2.483310478989796

Epoch: 6| Step: 8
Training loss: 2.8877880573272705
Validation loss: 2.481365378184985

Epoch: 6| Step: 9
Training loss: 1.765640377998352
Validation loss: 2.482600809425436

Epoch: 6| Step: 10
Training loss: 2.0569095611572266
Validation loss: 2.483675692671089

Epoch: 6| Step: 11
Training loss: 2.550732135772705
Validation loss: 2.484754862323884

Epoch: 6| Step: 12
Training loss: 2.7062807083129883
Validation loss: 2.4797160240911666

Epoch: 6| Step: 13
Training loss: 2.5584068298339844
Validation loss: 2.482566823241531

Epoch: 107| Step: 0
Training loss: 2.7156505584716797
Validation loss: 2.4823748309125184

Epoch: 6| Step: 1
Training loss: 3.2696382999420166
Validation loss: 2.4899428839324624

Epoch: 6| Step: 2
Training loss: 2.734090805053711
Validation loss: 2.4902968765586935

Epoch: 6| Step: 3
Training loss: 2.3109445571899414
Validation loss: 2.495464696679064

Epoch: 6| Step: 4
Training loss: 2.100337505340576
Validation loss: 2.502973146336053

Epoch: 6| Step: 5
Training loss: 3.2538890838623047
Validation loss: 2.5100616819115094

Epoch: 6| Step: 6
Training loss: 3.1225523948669434
Validation loss: 2.5133260475691928

Epoch: 6| Step: 7
Training loss: 2.196777820587158
Validation loss: 2.506455377865863

Epoch: 6| Step: 8
Training loss: 2.715999126434326
Validation loss: 2.509209907183083

Epoch: 6| Step: 9
Training loss: 2.038158893585205
Validation loss: 2.4875746157861527

Epoch: 6| Step: 10
Training loss: 2.830258846282959
Validation loss: 2.486447103561894

Epoch: 6| Step: 11
Training loss: 2.4866230487823486
Validation loss: 2.485271699966923

Epoch: 6| Step: 12
Training loss: 2.534492015838623
Validation loss: 2.4838111323695027

Epoch: 6| Step: 13
Training loss: 3.737856149673462
Validation loss: 2.4805804555134108

Epoch: 108| Step: 0
Training loss: 3.1445248126983643
Validation loss: 2.483060557355163

Epoch: 6| Step: 1
Training loss: 2.717724323272705
Validation loss: 2.477017359067035

Epoch: 6| Step: 2
Training loss: 2.418041706085205
Validation loss: 2.4791835328584075

Epoch: 6| Step: 3
Training loss: 2.5450479984283447
Validation loss: 2.480238776053152

Epoch: 6| Step: 4
Training loss: 2.905956745147705
Validation loss: 2.4769417060318815

Epoch: 6| Step: 5
Training loss: 3.6903586387634277
Validation loss: 2.4781903246397614

Epoch: 6| Step: 6
Training loss: 2.583824872970581
Validation loss: 2.4774452845255532

Epoch: 6| Step: 7
Training loss: 2.669753074645996
Validation loss: 2.4738511782820507

Epoch: 6| Step: 8
Training loss: 2.1508328914642334
Validation loss: 2.4780833669888076

Epoch: 6| Step: 9
Training loss: 2.2249674797058105
Validation loss: 2.4777547492775867

Epoch: 6| Step: 10
Training loss: 2.743130683898926
Validation loss: 2.4808048253418296

Epoch: 6| Step: 11
Training loss: 1.9865553379058838
Validation loss: 2.4808960473665627

Epoch: 6| Step: 12
Training loss: 2.834243059158325
Validation loss: 2.489377208935317

Epoch: 6| Step: 13
Training loss: 2.924940824508667
Validation loss: 2.4997613635114444

Epoch: 109| Step: 0
Training loss: 2.875275135040283
Validation loss: 2.50053939383517

Epoch: 6| Step: 1
Training loss: 2.545488119125366
Validation loss: 2.5052746213892454

Epoch: 6| Step: 2
Training loss: 2.849848508834839
Validation loss: 2.5175900305471113

Epoch: 6| Step: 3
Training loss: 2.504509210586548
Validation loss: 2.513417190121066

Epoch: 6| Step: 4
Training loss: 1.860642433166504
Validation loss: 2.5153113295955043

Epoch: 6| Step: 5
Training loss: 2.4502081871032715
Validation loss: 2.509987792661113

Epoch: 6| Step: 6
Training loss: 3.2351176738739014
Validation loss: 2.5043886348765385

Epoch: 6| Step: 7
Training loss: 2.5618300437927246
Validation loss: 2.4993436182698896

Epoch: 6| Step: 8
Training loss: 3.136678695678711
Validation loss: 2.497232093605944

Epoch: 6| Step: 9
Training loss: 2.575669765472412
Validation loss: 2.497666661457349

Epoch: 6| Step: 10
Training loss: 2.065309762954712
Validation loss: 2.489804844702444

Epoch: 6| Step: 11
Training loss: 2.659782648086548
Validation loss: 2.4893740043845227

Epoch: 6| Step: 12
Training loss: 3.4100394248962402
Validation loss: 2.484653462645828

Epoch: 6| Step: 13
Training loss: 3.0565950870513916
Validation loss: 2.483553514685682

Epoch: 110| Step: 0
Training loss: 2.870540142059326
Validation loss: 2.4828902418895433

Epoch: 6| Step: 1
Training loss: 3.5424675941467285
Validation loss: 2.4792126737615114

Epoch: 6| Step: 2
Training loss: 2.7938332557678223
Validation loss: 2.47845442833439

Epoch: 6| Step: 3
Training loss: 2.433236598968506
Validation loss: 2.475118685794133

Epoch: 6| Step: 4
Training loss: 2.2074060440063477
Validation loss: 2.470383551812941

Epoch: 6| Step: 5
Training loss: 2.8816089630126953
Validation loss: 2.475509300026842

Epoch: 6| Step: 6
Training loss: 2.3171072006225586
Validation loss: 2.4736362067602014

Epoch: 6| Step: 7
Training loss: 2.7483651638031006
Validation loss: 2.4743021354880383

Epoch: 6| Step: 8
Training loss: 2.412733554840088
Validation loss: 2.4733590759256834

Epoch: 6| Step: 9
Training loss: 2.6135427951812744
Validation loss: 2.4755538176464778

Epoch: 6| Step: 10
Training loss: 2.2949843406677246
Validation loss: 2.4772617483651764

Epoch: 6| Step: 11
Training loss: 1.834134578704834
Validation loss: 2.4806842521954606

Epoch: 6| Step: 12
Training loss: 2.7305893898010254
Validation loss: 2.486061026973109

Epoch: 6| Step: 13
Training loss: 4.755013942718506
Validation loss: 2.4899326139880764

Epoch: 111| Step: 0
Training loss: 2.102635383605957
Validation loss: 2.5002769142068844

Epoch: 6| Step: 1
Training loss: 3.118997573852539
Validation loss: 2.4912629204411663

Epoch: 6| Step: 2
Training loss: 3.0664405822753906
Validation loss: 2.4943108943200882

Epoch: 6| Step: 3
Training loss: 2.4460127353668213
Validation loss: 2.4962490886770268

Epoch: 6| Step: 4
Training loss: 3.4205007553100586
Validation loss: 2.4881376515152636

Epoch: 6| Step: 5
Training loss: 2.314338445663452
Validation loss: 2.481865834164363

Epoch: 6| Step: 6
Training loss: 2.1325082778930664
Validation loss: 2.479032120397014

Epoch: 6| Step: 7
Training loss: 2.8598012924194336
Validation loss: 2.4795216104035736

Epoch: 6| Step: 8
Training loss: 2.3505494594573975
Validation loss: 2.4763650919801448

Epoch: 6| Step: 9
Training loss: 2.959693670272827
Validation loss: 2.473678619630875

Epoch: 6| Step: 10
Training loss: 3.396390676498413
Validation loss: 2.478506875294511

Epoch: 6| Step: 11
Training loss: 2.57521653175354
Validation loss: 2.4724036852518716

Epoch: 6| Step: 12
Training loss: 2.59835147857666
Validation loss: 2.4741960289657756

Epoch: 6| Step: 13
Training loss: 1.8015984296798706
Validation loss: 2.4746952287612425

Epoch: 112| Step: 0
Training loss: 2.531062126159668
Validation loss: 2.4715933492106776

Epoch: 6| Step: 1
Training loss: 2.187061309814453
Validation loss: 2.4746115361490557

Epoch: 6| Step: 2
Training loss: 1.6827986240386963
Validation loss: 2.469858446428853

Epoch: 6| Step: 3
Training loss: 3.2956528663635254
Validation loss: 2.473570182759275

Epoch: 6| Step: 4
Training loss: 2.912713050842285
Validation loss: 2.474027056847849

Epoch: 6| Step: 5
Training loss: 2.9825806617736816
Validation loss: 2.4755720169313493

Epoch: 6| Step: 6
Training loss: 2.8241662979125977
Validation loss: 2.47324175219382

Epoch: 6| Step: 7
Training loss: 2.281764507293701
Validation loss: 2.480381627236643

Epoch: 6| Step: 8
Training loss: 3.350433111190796
Validation loss: 2.4799126245642222

Epoch: 6| Step: 9
Training loss: 2.827244281768799
Validation loss: 2.4833001167543474

Epoch: 6| Step: 10
Training loss: 2.08016300201416
Validation loss: 2.4858543334468717

Epoch: 6| Step: 11
Training loss: 2.4166054725646973
Validation loss: 2.4822318938470658

Epoch: 6| Step: 12
Training loss: 2.9705021381378174
Validation loss: 2.4832566374091694

Epoch: 6| Step: 13
Training loss: 3.329014778137207
Validation loss: 2.488002892463438

Epoch: 113| Step: 0
Training loss: 2.401648998260498
Validation loss: 2.4896402307735976

Epoch: 6| Step: 1
Training loss: 2.648664951324463
Validation loss: 2.4878205522414176

Epoch: 6| Step: 2
Training loss: 2.860138416290283
Validation loss: 2.488921821758311

Epoch: 6| Step: 3
Training loss: 3.3130908012390137
Validation loss: 2.486615824443038

Epoch: 6| Step: 4
Training loss: 3.0038905143737793
Validation loss: 2.497865943498509

Epoch: 6| Step: 5
Training loss: 2.5799431800842285
Validation loss: 2.483957213740195

Epoch: 6| Step: 6
Training loss: 2.8917527198791504
Validation loss: 2.4788502031756985

Epoch: 6| Step: 7
Training loss: 2.206359624862671
Validation loss: 2.4811153565683672

Epoch: 6| Step: 8
Training loss: 2.371030330657959
Validation loss: 2.4780594533489597

Epoch: 6| Step: 9
Training loss: 2.8007760047912598
Validation loss: 2.468449766917895

Epoch: 6| Step: 10
Training loss: 1.7897677421569824
Validation loss: 2.4754119842283187

Epoch: 6| Step: 11
Training loss: 2.721620559692383
Validation loss: 2.473226090913178

Epoch: 6| Step: 12
Training loss: 3.770876407623291
Validation loss: 2.4711669978275093

Epoch: 6| Step: 13
Training loss: 1.6465486288070679
Validation loss: 2.4747274357785463

Epoch: 114| Step: 0
Training loss: 2.4809751510620117
Validation loss: 2.4718972457352506

Epoch: 6| Step: 1
Training loss: 2.9479050636291504
Validation loss: 2.4717923518150084

Epoch: 6| Step: 2
Training loss: 2.8540310859680176
Validation loss: 2.473101705633184

Epoch: 6| Step: 3
Training loss: 3.0239267349243164
Validation loss: 2.4821097671344714

Epoch: 6| Step: 4
Training loss: 2.777230978012085
Validation loss: 2.484140844755275

Epoch: 6| Step: 5
Training loss: 2.5050039291381836
Validation loss: 2.4824731734491166

Epoch: 6| Step: 6
Training loss: 2.490584135055542
Validation loss: 2.4831970968554096

Epoch: 6| Step: 7
Training loss: 2.50950026512146
Validation loss: 2.478241002687844

Epoch: 6| Step: 8
Training loss: 2.834592342376709
Validation loss: 2.4749989765946583

Epoch: 6| Step: 9
Training loss: 3.7363502979278564
Validation loss: 2.4731630612445135

Epoch: 6| Step: 10
Training loss: 2.179185390472412
Validation loss: 2.471203045178485

Epoch: 6| Step: 11
Training loss: 2.7189998626708984
Validation loss: 2.4703113930199736

Epoch: 6| Step: 12
Training loss: 2.0454111099243164
Validation loss: 2.471183643546156

Epoch: 6| Step: 13
Training loss: 2.0499093532562256
Validation loss: 2.472228209177653

Epoch: 115| Step: 0
Training loss: 2.816394090652466
Validation loss: 2.4730329308458554

Epoch: 6| Step: 1
Training loss: 2.7701704502105713
Validation loss: 2.4736003568095546

Epoch: 6| Step: 2
Training loss: 2.6196811199188232
Validation loss: 2.4681426555879655

Epoch: 6| Step: 3
Training loss: 1.8689143657684326
Validation loss: 2.467817055281772

Epoch: 6| Step: 4
Training loss: 2.8486757278442383
Validation loss: 2.4789558815699753

Epoch: 6| Step: 5
Training loss: 2.88677716255188
Validation loss: 2.485495113557385

Epoch: 6| Step: 6
Training loss: 2.3269526958465576
Validation loss: 2.4898472114275862

Epoch: 6| Step: 7
Training loss: 2.3440945148468018
Validation loss: 2.4980914541470107

Epoch: 6| Step: 8
Training loss: 2.9657998085021973
Validation loss: 2.4987390707897883

Epoch: 6| Step: 9
Training loss: 2.9027512073516846
Validation loss: 2.510501036079981

Epoch: 6| Step: 10
Training loss: 2.8002758026123047
Validation loss: 2.504643622265067

Epoch: 6| Step: 11
Training loss: 2.351252317428589
Validation loss: 2.5088245804591844

Epoch: 6| Step: 12
Training loss: 3.2950496673583984
Validation loss: 2.5008826845435688

Epoch: 6| Step: 13
Training loss: 2.5974392890930176
Validation loss: 2.503384054348033

Epoch: 116| Step: 0
Training loss: 2.5469353199005127
Validation loss: 2.494050851432226

Epoch: 6| Step: 1
Training loss: 3.1937777996063232
Validation loss: 2.4850036482657156

Epoch: 6| Step: 2
Training loss: 2.9759199619293213
Validation loss: 2.472393904962847

Epoch: 6| Step: 3
Training loss: 2.3541100025177
Validation loss: 2.4740852412357124

Epoch: 6| Step: 4
Training loss: 1.8392244577407837
Validation loss: 2.4647182777363765

Epoch: 6| Step: 5
Training loss: 3.0187366008758545
Validation loss: 2.473509698785761

Epoch: 6| Step: 6
Training loss: 3.196786880493164
Validation loss: 2.4710586891379407

Epoch: 6| Step: 7
Training loss: 3.3389687538146973
Validation loss: 2.468405000625118

Epoch: 6| Step: 8
Training loss: 3.1438822746276855
Validation loss: 2.4685990169484127

Epoch: 6| Step: 9
Training loss: 2.2906665802001953
Validation loss: 2.470452380436723

Epoch: 6| Step: 10
Training loss: 2.281716823577881
Validation loss: 2.4704950701805855

Epoch: 6| Step: 11
Training loss: 1.9400606155395508
Validation loss: 2.467785507120112

Epoch: 6| Step: 12
Training loss: 2.3513023853302
Validation loss: 2.4678638904325423

Epoch: 6| Step: 13
Training loss: 3.1978180408477783
Validation loss: 2.467396290071549

Epoch: 117| Step: 0
Training loss: 2.341543436050415
Validation loss: 2.4673788573152278

Epoch: 6| Step: 1
Training loss: 3.778533697128296
Validation loss: 2.4685164318289807

Epoch: 6| Step: 2
Training loss: 1.5761535167694092
Validation loss: 2.468074890875047

Epoch: 6| Step: 3
Training loss: 2.3794302940368652
Validation loss: 2.4764997087499148

Epoch: 6| Step: 4
Training loss: 3.025254487991333
Validation loss: 2.477404227820776

Epoch: 6| Step: 5
Training loss: 2.9978878498077393
Validation loss: 2.4741533776765228

Epoch: 6| Step: 6
Training loss: 2.551025390625
Validation loss: 2.4885970366898404

Epoch: 6| Step: 7
Training loss: 2.6859030723571777
Validation loss: 2.489505555040093

Epoch: 6| Step: 8
Training loss: 2.623208522796631
Validation loss: 2.490010230771957

Epoch: 6| Step: 9
Training loss: 2.4741015434265137
Validation loss: 2.503260935506513

Epoch: 6| Step: 10
Training loss: 2.558952808380127
Validation loss: 2.506755857057469

Epoch: 6| Step: 11
Training loss: 2.356785535812378
Validation loss: 2.5003799546149468

Epoch: 6| Step: 12
Training loss: 3.3328142166137695
Validation loss: 2.5006916035888014

Epoch: 6| Step: 13
Training loss: 2.6713969707489014
Validation loss: 2.503401223049369

Epoch: 118| Step: 0
Training loss: 2.2750818729400635
Validation loss: 2.5155076314044256

Epoch: 6| Step: 1
Training loss: 2.1596741676330566
Validation loss: 2.507695985096757

Epoch: 6| Step: 2
Training loss: 2.277392864227295
Validation loss: 2.496673914694017

Epoch: 6| Step: 3
Training loss: 2.482839584350586
Validation loss: 2.494940152732275

Epoch: 6| Step: 4
Training loss: 2.8145370483398438
Validation loss: 2.4826365196576683

Epoch: 6| Step: 5
Training loss: 2.987171173095703
Validation loss: 2.4803401885494107

Epoch: 6| Step: 6
Training loss: 2.8572864532470703
Validation loss: 2.4804483126568537

Epoch: 6| Step: 7
Training loss: 2.997607707977295
Validation loss: 2.481359666393649

Epoch: 6| Step: 8
Training loss: 3.2253823280334473
Validation loss: 2.471656750607234

Epoch: 6| Step: 9
Training loss: 2.1751949787139893
Validation loss: 2.465812588250765

Epoch: 6| Step: 10
Training loss: 3.24629282951355
Validation loss: 2.461250880713104

Epoch: 6| Step: 11
Training loss: 2.8143115043640137
Validation loss: 2.4620266883603987

Epoch: 6| Step: 12
Training loss: 2.5856409072875977
Validation loss: 2.460540315156342

Epoch: 6| Step: 13
Training loss: 2.3902010917663574
Validation loss: 2.463416535367248

Epoch: 119| Step: 0
Training loss: 3.1983389854431152
Validation loss: 2.464184261137439

Epoch: 6| Step: 1
Training loss: 3.4312009811401367
Validation loss: 2.46456818426809

Epoch: 6| Step: 2
Training loss: 2.237440347671509
Validation loss: 2.4723659587162796

Epoch: 6| Step: 3
Training loss: 2.803769826889038
Validation loss: 2.4695643481387886

Epoch: 6| Step: 4
Training loss: 3.2576704025268555
Validation loss: 2.474216586800032

Epoch: 6| Step: 5
Training loss: 2.4583241939544678
Validation loss: 2.472928131780317

Epoch: 6| Step: 6
Training loss: 2.5534775257110596
Validation loss: 2.4791349954502557

Epoch: 6| Step: 7
Training loss: 2.923372745513916
Validation loss: 2.4786146507468274

Epoch: 6| Step: 8
Training loss: 2.820983409881592
Validation loss: 2.486973580493722

Epoch: 6| Step: 9
Training loss: 2.1580328941345215
Validation loss: 2.481766685362785

Epoch: 6| Step: 10
Training loss: 1.9869376420974731
Validation loss: 2.4870009627393497

Epoch: 6| Step: 11
Training loss: 2.3136703968048096
Validation loss: 2.488174125712405

Epoch: 6| Step: 12
Training loss: 2.827472686767578
Validation loss: 2.4937442246303765

Epoch: 6| Step: 13
Training loss: 2.3364133834838867
Validation loss: 2.4947947353445072

Epoch: 120| Step: 0
Training loss: 3.0514278411865234
Validation loss: 2.489797358871788

Epoch: 6| Step: 1
Training loss: 2.3118810653686523
Validation loss: 2.4977117430779243

Epoch: 6| Step: 2
Training loss: 3.3823492527008057
Validation loss: 2.5128203348446916

Epoch: 6| Step: 3
Training loss: 2.5699896812438965
Validation loss: 2.523205082903626

Epoch: 6| Step: 4
Training loss: 2.8238577842712402
Validation loss: 2.549127112152756

Epoch: 6| Step: 5
Training loss: 2.347252368927002
Validation loss: 2.547789409596433

Epoch: 6| Step: 6
Training loss: 2.866826057434082
Validation loss: 2.551782246558897

Epoch: 6| Step: 7
Training loss: 2.6303958892822266
Validation loss: 2.5170751951074086

Epoch: 6| Step: 8
Training loss: 2.396005392074585
Validation loss: 2.4897684615145446

Epoch: 6| Step: 9
Training loss: 2.625804901123047
Validation loss: 2.476590789774413

Epoch: 6| Step: 10
Training loss: 2.6336276531219482
Validation loss: 2.4721829711749987

Epoch: 6| Step: 11
Training loss: 2.693686008453369
Validation loss: 2.4644200801849365

Epoch: 6| Step: 12
Training loss: 2.360840082168579
Validation loss: 2.46772160068635

Epoch: 6| Step: 13
Training loss: 2.859304189682007
Validation loss: 2.4662998619899956

Epoch: 121| Step: 0
Training loss: 2.2920494079589844
Validation loss: 2.4702009488177556

Epoch: 6| Step: 1
Training loss: 2.723789930343628
Validation loss: 2.468307077243764

Epoch: 6| Step: 2
Training loss: 2.6631100177764893
Validation loss: 2.470899554990953

Epoch: 6| Step: 3
Training loss: 3.1764535903930664
Validation loss: 2.4755176421134704

Epoch: 6| Step: 4
Training loss: 2.678997278213501
Validation loss: 2.4732421495581187

Epoch: 6| Step: 5
Training loss: 2.771935224533081
Validation loss: 2.47467101517544

Epoch: 6| Step: 6
Training loss: 2.130953788757324
Validation loss: 2.4780802790836622

Epoch: 6| Step: 7
Training loss: 3.160747528076172
Validation loss: 2.474523300765663

Epoch: 6| Step: 8
Training loss: 3.0908336639404297
Validation loss: 2.485990342273507

Epoch: 6| Step: 9
Training loss: 2.5756916999816895
Validation loss: 2.497555835272676

Epoch: 6| Step: 10
Training loss: 3.0994091033935547
Validation loss: 2.503326710834298

Epoch: 6| Step: 11
Training loss: 2.435077428817749
Validation loss: 2.518922944222727

Epoch: 6| Step: 12
Training loss: 2.5301995277404785
Validation loss: 2.5195498338309665

Epoch: 6| Step: 13
Training loss: 1.6533793210983276
Validation loss: 2.5178703120959702

Epoch: 122| Step: 0
Training loss: 2.2098195552825928
Validation loss: 2.505654522167739

Epoch: 6| Step: 1
Training loss: 2.084441661834717
Validation loss: 2.500159660975138

Epoch: 6| Step: 2
Training loss: 3.1357369422912598
Validation loss: 2.487634215303647

Epoch: 6| Step: 3
Training loss: 3.1901419162750244
Validation loss: 2.4733082709773893

Epoch: 6| Step: 4
Training loss: 3.357145309448242
Validation loss: 2.463637759608607

Epoch: 6| Step: 5
Training loss: 2.677558422088623
Validation loss: 2.4610126915798394

Epoch: 6| Step: 6
Training loss: 2.285572052001953
Validation loss: 2.4613181955070904

Epoch: 6| Step: 7
Training loss: 2.9222445487976074
Validation loss: 2.458872018321868

Epoch: 6| Step: 8
Training loss: 2.7712554931640625
Validation loss: 2.458971461942119

Epoch: 6| Step: 9
Training loss: 3.2322354316711426
Validation loss: 2.4597806263995428

Epoch: 6| Step: 10
Training loss: 2.72236704826355
Validation loss: 2.4574893469451577

Epoch: 6| Step: 11
Training loss: 2.1709208488464355
Validation loss: 2.456196810609551

Epoch: 6| Step: 12
Training loss: 2.1421589851379395
Validation loss: 2.4572633440776537

Epoch: 6| Step: 13
Training loss: 2.5021324157714844
Validation loss: 2.458677758452713

Epoch: 123| Step: 0
Training loss: 2.997500419616699
Validation loss: 2.4649809252831245

Epoch: 6| Step: 1
Training loss: 1.7714407444000244
Validation loss: 2.4632485169236378

Epoch: 6| Step: 2
Training loss: 3.3205409049987793
Validation loss: 2.4579221561390865

Epoch: 6| Step: 3
Training loss: 2.687760353088379
Validation loss: 2.467108170191447

Epoch: 6| Step: 4
Training loss: 2.7863664627075195
Validation loss: 2.4614187209836897

Epoch: 6| Step: 5
Training loss: 2.3289496898651123
Validation loss: 2.4680678562451432

Epoch: 6| Step: 6
Training loss: 2.3156754970550537
Validation loss: 2.4649773105498283

Epoch: 6| Step: 7
Training loss: 2.7260947227478027
Validation loss: 2.464754448142103

Epoch: 6| Step: 8
Training loss: 2.8859074115753174
Validation loss: 2.46180954030765

Epoch: 6| Step: 9
Training loss: 3.1159300804138184
Validation loss: 2.4584365326871156

Epoch: 6| Step: 10
Training loss: 3.1528589725494385
Validation loss: 2.4639601784367717

Epoch: 6| Step: 11
Training loss: 2.286323070526123
Validation loss: 2.4638057549794516

Epoch: 6| Step: 12
Training loss: 2.3167951107025146
Validation loss: 2.4603260922175583

Epoch: 6| Step: 13
Training loss: 2.600337266921997
Validation loss: 2.4587479073514222

Epoch: 124| Step: 0
Training loss: 2.062612295150757
Validation loss: 2.4574508884901642

Epoch: 6| Step: 1
Training loss: 2.7699832916259766
Validation loss: 2.4542189157137306

Epoch: 6| Step: 2
Training loss: 3.4614903926849365
Validation loss: 2.4601422791839926

Epoch: 6| Step: 3
Training loss: 3.1977782249450684
Validation loss: 2.454058571528363

Epoch: 6| Step: 4
Training loss: 1.836358904838562
Validation loss: 2.4514824164811

Epoch: 6| Step: 5
Training loss: 1.9151043891906738
Validation loss: 2.4519364782558974

Epoch: 6| Step: 6
Training loss: 2.752858877182007
Validation loss: 2.4493060804182485

Epoch: 6| Step: 7
Training loss: 3.2754790782928467
Validation loss: 2.4547443184801327

Epoch: 6| Step: 8
Training loss: 2.797753095626831
Validation loss: 2.452674465794717

Epoch: 6| Step: 9
Training loss: 2.7106542587280273
Validation loss: 2.4516937501968874

Epoch: 6| Step: 10
Training loss: 2.8521506786346436
Validation loss: 2.4525453300886255

Epoch: 6| Step: 11
Training loss: 2.4627389907836914
Validation loss: 2.4503444856212986

Epoch: 6| Step: 12
Training loss: 2.3627195358276367
Validation loss: 2.453147836910781

Epoch: 6| Step: 13
Training loss: 3.0423784255981445
Validation loss: 2.4574519049736763

Epoch: 125| Step: 0
Training loss: 2.700646162033081
Validation loss: 2.457874923624018

Epoch: 6| Step: 1
Training loss: 3.6910247802734375
Validation loss: 2.461285347579628

Epoch: 6| Step: 2
Training loss: 2.701232671737671
Validation loss: 2.4614473901769167

Epoch: 6| Step: 3
Training loss: 2.941565990447998
Validation loss: 2.4669259671242005

Epoch: 6| Step: 4
Training loss: 2.637387275695801
Validation loss: 2.4721299525230163

Epoch: 6| Step: 5
Training loss: 3.5485243797302246
Validation loss: 2.473847340512019

Epoch: 6| Step: 6
Training loss: 2.0287322998046875
Validation loss: 2.477107309526013

Epoch: 6| Step: 7
Training loss: 1.962288737297058
Validation loss: 2.484135091945689

Epoch: 6| Step: 8
Training loss: 2.4676907062530518
Validation loss: 2.4924462328674974

Epoch: 6| Step: 9
Training loss: 2.872236967086792
Validation loss: 2.4941871473866124

Epoch: 6| Step: 10
Training loss: 2.265195608139038
Validation loss: 2.500185328145181

Epoch: 6| Step: 11
Training loss: 2.7262237071990967
Validation loss: 2.4846679113244496

Epoch: 6| Step: 12
Training loss: 2.106696128845215
Validation loss: 2.47524380940263

Epoch: 6| Step: 13
Training loss: 2.8254432678222656
Validation loss: 2.459933983382358

Epoch: 126| Step: 0
Training loss: 2.8129682540893555
Validation loss: 2.4546534502378075

Epoch: 6| Step: 1
Training loss: 2.8810696601867676
Validation loss: 2.4501854347926315

Epoch: 6| Step: 2
Training loss: 2.795969009399414
Validation loss: 2.447705989242882

Epoch: 6| Step: 3
Training loss: 2.0723531246185303
Validation loss: 2.4489879403063046

Epoch: 6| Step: 4
Training loss: 2.7196156978607178
Validation loss: 2.4510235658255954

Epoch: 6| Step: 5
Training loss: 2.5631375312805176
Validation loss: 2.454319343771986

Epoch: 6| Step: 6
Training loss: 2.670156717300415
Validation loss: 2.4546504610328266

Epoch: 6| Step: 7
Training loss: 2.529588460922241
Validation loss: 2.4511543114980063

Epoch: 6| Step: 8
Training loss: 2.9206290245056152
Validation loss: 2.458245833714803

Epoch: 6| Step: 9
Training loss: 2.7299821376800537
Validation loss: 2.453927648964749

Epoch: 6| Step: 10
Training loss: 2.7263927459716797
Validation loss: 2.454516028845182

Epoch: 6| Step: 11
Training loss: 2.9167981147766113
Validation loss: 2.452130506115575

Epoch: 6| Step: 12
Training loss: 2.2922375202178955
Validation loss: 2.448442469361008

Epoch: 6| Step: 13
Training loss: 2.8030476570129395
Validation loss: 2.449233921625281

Epoch: 127| Step: 0
Training loss: 2.7173986434936523
Validation loss: 2.449456743014756

Epoch: 6| Step: 1
Training loss: 2.693789005279541
Validation loss: 2.450675674664077

Epoch: 6| Step: 2
Training loss: 2.629178047180176
Validation loss: 2.455760468718826

Epoch: 6| Step: 3
Training loss: 3.1454458236694336
Validation loss: 2.463898879225536

Epoch: 6| Step: 4
Training loss: 2.60206937789917
Validation loss: 2.462006486872191

Epoch: 6| Step: 5
Training loss: 3.1011743545532227
Validation loss: 2.4641280033255137

Epoch: 6| Step: 6
Training loss: 2.3626322746276855
Validation loss: 2.4700283773483767

Epoch: 6| Step: 7
Training loss: 2.9052977561950684
Validation loss: 2.463985714861142

Epoch: 6| Step: 8
Training loss: 2.972388982772827
Validation loss: 2.4649647538379957

Epoch: 6| Step: 9
Training loss: 2.3983571529388428
Validation loss: 2.4642567352582048

Epoch: 6| Step: 10
Training loss: 2.299785614013672
Validation loss: 2.463085059196718

Epoch: 6| Step: 11
Training loss: 1.8157789707183838
Validation loss: 2.45655309513051

Epoch: 6| Step: 12
Training loss: 2.8249239921569824
Validation loss: 2.451094353070823

Epoch: 6| Step: 13
Training loss: 2.868910551071167
Validation loss: 2.446397958263274

Epoch: 128| Step: 0
Training loss: 1.8710100650787354
Validation loss: 2.4512226299573014

Epoch: 6| Step: 1
Training loss: 2.4192068576812744
Validation loss: 2.448596636454264

Epoch: 6| Step: 2
Training loss: 3.124176502227783
Validation loss: 2.4507319645215104

Epoch: 6| Step: 3
Training loss: 2.342378616333008
Validation loss: 2.4487331195544173

Epoch: 6| Step: 4
Training loss: 2.513892412185669
Validation loss: 2.457840019656766

Epoch: 6| Step: 5
Training loss: 2.092343807220459
Validation loss: 2.455856884679487

Epoch: 6| Step: 6
Training loss: 2.5880956649780273
Validation loss: 2.458250279067665

Epoch: 6| Step: 7
Training loss: 2.613807201385498
Validation loss: 2.4513582670560448

Epoch: 6| Step: 8
Training loss: 3.031754493713379
Validation loss: 2.4544619385914137

Epoch: 6| Step: 9
Training loss: 3.463221549987793
Validation loss: 2.4506724419132357

Epoch: 6| Step: 10
Training loss: 3.2651593685150146
Validation loss: 2.456227015423518

Epoch: 6| Step: 11
Training loss: 2.6956233978271484
Validation loss: 2.450687041846655

Epoch: 6| Step: 12
Training loss: 2.7559709548950195
Validation loss: 2.451058351865379

Epoch: 6| Step: 13
Training loss: 2.2598748207092285
Validation loss: 2.4495279506970475

Epoch: 129| Step: 0
Training loss: 3.1085968017578125
Validation loss: 2.446463579772621

Epoch: 6| Step: 1
Training loss: 3.0157175064086914
Validation loss: 2.4513723055521646

Epoch: 6| Step: 2
Training loss: 2.2526650428771973
Validation loss: 2.447877789056429

Epoch: 6| Step: 3
Training loss: 2.3145482540130615
Validation loss: 2.444176173979236

Epoch: 6| Step: 4
Training loss: 1.8463777303695679
Validation loss: 2.4436150468805784

Epoch: 6| Step: 5
Training loss: 3.267362117767334
Validation loss: 2.4436236043130197

Epoch: 6| Step: 6
Training loss: 2.9367542266845703
Validation loss: 2.4436468437153804

Epoch: 6| Step: 7
Training loss: 2.2642016410827637
Validation loss: 2.451090112809212

Epoch: 6| Step: 8
Training loss: 2.740729808807373
Validation loss: 2.450993848103349

Epoch: 6| Step: 9
Training loss: 2.641310930252075
Validation loss: 2.4540158241025862

Epoch: 6| Step: 10
Training loss: 2.713886260986328
Validation loss: 2.453700197640286

Epoch: 6| Step: 11
Training loss: 2.5869171619415283
Validation loss: 2.462651774447451

Epoch: 6| Step: 12
Training loss: 2.621279716491699
Validation loss: 2.4563127640754945

Epoch: 6| Step: 13
Training loss: 2.9504919052124023
Validation loss: 2.463175489056495

Epoch: 130| Step: 0
Training loss: 2.3345766067504883
Validation loss: 2.4703582563707904

Epoch: 6| Step: 1
Training loss: 2.4469685554504395
Validation loss: 2.4903797821332048

Epoch: 6| Step: 2
Training loss: 2.403155565261841
Validation loss: 2.493471209720899

Epoch: 6| Step: 3
Training loss: 2.7888498306274414
Validation loss: 2.493805323877642

Epoch: 6| Step: 4
Training loss: 3.3857614994049072
Validation loss: 2.4999457764369186

Epoch: 6| Step: 5
Training loss: 2.667149543762207
Validation loss: 2.486049718754266

Epoch: 6| Step: 6
Training loss: 2.6050682067871094
Validation loss: 2.4780091265196442

Epoch: 6| Step: 7
Training loss: 2.772780418395996
Validation loss: 2.4635306737756215

Epoch: 6| Step: 8
Training loss: 2.30704402923584
Validation loss: 2.451651327071651

Epoch: 6| Step: 9
Training loss: 2.2426183223724365
Validation loss: 2.4470782561968734

Epoch: 6| Step: 10
Training loss: 2.5710296630859375
Validation loss: 2.4448236803854666

Epoch: 6| Step: 11
Training loss: 2.7904884815216064
Validation loss: 2.4431487924309185

Epoch: 6| Step: 12
Training loss: 2.6559739112854004
Validation loss: 2.4467342053690264

Epoch: 6| Step: 13
Training loss: 3.9884958267211914
Validation loss: 2.4494402049690165

Epoch: 131| Step: 0
Training loss: 3.1069693565368652
Validation loss: 2.450683404040593

Epoch: 6| Step: 1
Training loss: 2.759751081466675
Validation loss: 2.4539658920739287

Epoch: 6| Step: 2
Training loss: 2.037571907043457
Validation loss: 2.4534725950610254

Epoch: 6| Step: 3
Training loss: 2.2907180786132812
Validation loss: 2.4500704811465357

Epoch: 6| Step: 4
Training loss: 3.095693826675415
Validation loss: 2.451348376530473

Epoch: 6| Step: 5
Training loss: 2.3859336376190186
Validation loss: 2.4495044318578576

Epoch: 6| Step: 6
Training loss: 2.228703022003174
Validation loss: 2.4480077092365553

Epoch: 6| Step: 7
Training loss: 2.0814857482910156
Validation loss: 2.4407219963689006

Epoch: 6| Step: 8
Training loss: 3.2193636894226074
Validation loss: 2.4388049392290014

Epoch: 6| Step: 9
Training loss: 3.0162410736083984
Validation loss: 2.4379905013627905

Epoch: 6| Step: 10
Training loss: 2.7040600776672363
Validation loss: 2.4406597960379814

Epoch: 6| Step: 11
Training loss: 3.207432985305786
Validation loss: 2.444702338146907

Epoch: 6| Step: 12
Training loss: 2.10067081451416
Validation loss: 2.4487218908084336

Epoch: 6| Step: 13
Training loss: 3.478161573410034
Validation loss: 2.448112823629892

Epoch: 132| Step: 0
Training loss: 3.0243759155273438
Validation loss: 2.4533923390091106

Epoch: 6| Step: 1
Training loss: 2.8121094703674316
Validation loss: 2.4571164295237553

Epoch: 6| Step: 2
Training loss: 2.3768577575683594
Validation loss: 2.4562126615996003

Epoch: 6| Step: 3
Training loss: 1.7206342220306396
Validation loss: 2.46132234860492

Epoch: 6| Step: 4
Training loss: 3.0589170455932617
Validation loss: 2.4580466311465026

Epoch: 6| Step: 5
Training loss: 2.158201217651367
Validation loss: 2.4642492468639086

Epoch: 6| Step: 6
Training loss: 2.612668514251709
Validation loss: 2.4731562470877044

Epoch: 6| Step: 7
Training loss: 2.622114896774292
Validation loss: 2.469535530254405

Epoch: 6| Step: 8
Training loss: 2.6381568908691406
Validation loss: 2.484025204053489

Epoch: 6| Step: 9
Training loss: 2.491351842880249
Validation loss: 2.4677851148830947

Epoch: 6| Step: 10
Training loss: 3.1408486366271973
Validation loss: 2.4717603011797835

Epoch: 6| Step: 11
Training loss: 3.530531406402588
Validation loss: 2.4530437684828237

Epoch: 6| Step: 12
Training loss: 2.288090705871582
Validation loss: 2.4487311737511748

Epoch: 6| Step: 13
Training loss: 2.776510238647461
Validation loss: 2.442065779880811

Epoch: 133| Step: 0
Training loss: 2.5959441661834717
Validation loss: 2.4370792629898235

Epoch: 6| Step: 1
Training loss: 3.1588683128356934
Validation loss: 2.4445469071788173

Epoch: 6| Step: 2
Training loss: 3.3721425533294678
Validation loss: 2.4413810109579437

Epoch: 6| Step: 3
Training loss: 3.495208263397217
Validation loss: 2.4428841247353503

Epoch: 6| Step: 4
Training loss: 2.6751880645751953
Validation loss: 2.4401060868335027

Epoch: 6| Step: 5
Training loss: 3.1265459060668945
Validation loss: 2.438432442244663

Epoch: 6| Step: 6
Training loss: 2.229419231414795
Validation loss: 2.439013529849309

Epoch: 6| Step: 7
Training loss: 2.9306797981262207
Validation loss: 2.438419957314768

Epoch: 6| Step: 8
Training loss: 1.7018871307373047
Validation loss: 2.435309387022449

Epoch: 6| Step: 9
Training loss: 2.4226324558258057
Validation loss: 2.4352725833974858

Epoch: 6| Step: 10
Training loss: 3.133394956588745
Validation loss: 2.44062941305099

Epoch: 6| Step: 11
Training loss: 2.074488639831543
Validation loss: 2.440775866149574

Epoch: 6| Step: 12
Training loss: 1.880190134048462
Validation loss: 2.441813176678073

Epoch: 6| Step: 13
Training loss: 2.2302467823028564
Validation loss: 2.447325009171681

Epoch: 134| Step: 0
Training loss: 2.911503791809082
Validation loss: 2.455906685962472

Epoch: 6| Step: 1
Training loss: 2.1780285835266113
Validation loss: 2.466081965354181

Epoch: 6| Step: 2
Training loss: 2.2533023357391357
Validation loss: 2.4745004766730854

Epoch: 6| Step: 3
Training loss: 3.6653687953948975
Validation loss: 2.483329934458579

Epoch: 6| Step: 4
Training loss: 2.2235708236694336
Validation loss: 2.4922081449980378

Epoch: 6| Step: 5
Training loss: 3.0668978691101074
Validation loss: 2.4794005065835933

Epoch: 6| Step: 6
Training loss: 2.6429543495178223
Validation loss: 2.4789509132344234

Epoch: 6| Step: 7
Training loss: 2.4917540550231934
Validation loss: 2.4650436268057874

Epoch: 6| Step: 8
Training loss: 3.107542037963867
Validation loss: 2.472737853245069

Epoch: 6| Step: 9
Training loss: 2.2518978118896484
Validation loss: 2.4624806629714144

Epoch: 6| Step: 10
Training loss: 2.8259682655334473
Validation loss: 2.452599899743193

Epoch: 6| Step: 11
Training loss: 2.278801679611206
Validation loss: 2.4463951408222155

Epoch: 6| Step: 12
Training loss: 2.4769582748413086
Validation loss: 2.437363393845097

Epoch: 6| Step: 13
Training loss: 2.9739224910736084
Validation loss: 2.434745796265141

Epoch: 135| Step: 0
Training loss: 2.035536527633667
Validation loss: 2.437565629200269

Epoch: 6| Step: 1
Training loss: 2.811000347137451
Validation loss: 2.4366900023593696

Epoch: 6| Step: 2
Training loss: 3.3973398208618164
Validation loss: 2.434352131300075

Epoch: 6| Step: 3
Training loss: 2.946115493774414
Validation loss: 2.4333340890945925

Epoch: 6| Step: 4
Training loss: 3.146296501159668
Validation loss: 2.4315078412332842

Epoch: 6| Step: 5
Training loss: 2.4632673263549805
Validation loss: 2.429485659445486

Epoch: 6| Step: 6
Training loss: 2.7114949226379395
Validation loss: 2.434719631748815

Epoch: 6| Step: 7
Training loss: 1.98460853099823
Validation loss: 2.435533777359993

Epoch: 6| Step: 8
Training loss: 2.855123281478882
Validation loss: 2.436148048729025

Epoch: 6| Step: 9
Training loss: 3.2711150646209717
Validation loss: 2.440748240358086

Epoch: 6| Step: 10
Training loss: 2.0414414405822754
Validation loss: 2.4447811495873237

Epoch: 6| Step: 11
Training loss: 1.9222488403320312
Validation loss: 2.4493545050262124

Epoch: 6| Step: 12
Training loss: 2.745800256729126
Validation loss: 2.4424524332887385

Epoch: 6| Step: 13
Training loss: 2.8085827827453613
Validation loss: 2.443506412608649

Epoch: 136| Step: 0
Training loss: 2.8216073513031006
Validation loss: 2.4390179649476083

Epoch: 6| Step: 1
Training loss: 2.8453617095947266
Validation loss: 2.436134789579658

Epoch: 6| Step: 2
Training loss: 3.031287670135498
Validation loss: 2.4314867681072605

Epoch: 6| Step: 3
Training loss: 2.2779479026794434
Validation loss: 2.4339841078686457

Epoch: 6| Step: 4
Training loss: 2.805860996246338
Validation loss: 2.434886775990968

Epoch: 6| Step: 5
Training loss: 2.048419713973999
Validation loss: 2.4323580265045166

Epoch: 6| Step: 6
Training loss: 2.729928970336914
Validation loss: 2.4337245187451764

Epoch: 6| Step: 7
Training loss: 2.2858526706695557
Validation loss: 2.431935315491051

Epoch: 6| Step: 8
Training loss: 1.793853521347046
Validation loss: 2.431815398636685

Epoch: 6| Step: 9
Training loss: 2.354588270187378
Validation loss: 2.435052428194272

Epoch: 6| Step: 10
Training loss: 3.476814031600952
Validation loss: 2.4376341194234867

Epoch: 6| Step: 11
Training loss: 2.823364734649658
Validation loss: 2.4422355467273342

Epoch: 6| Step: 12
Training loss: 2.987415313720703
Validation loss: 2.4383095438762377

Epoch: 6| Step: 13
Training loss: 2.7708568572998047
Validation loss: 2.437141077492827

Epoch: 137| Step: 0
Training loss: 2.8940091133117676
Validation loss: 2.4393558335560623

Epoch: 6| Step: 1
Training loss: 2.684664249420166
Validation loss: 2.4421353417058147

Epoch: 6| Step: 2
Training loss: 2.3426876068115234
Validation loss: 2.4457728503852763

Epoch: 6| Step: 3
Training loss: 2.721543312072754
Validation loss: 2.447534450920679

Epoch: 6| Step: 4
Training loss: 2.025923013687134
Validation loss: 2.4430720524121354

Epoch: 6| Step: 5
Training loss: 2.578279972076416
Validation loss: 2.4379524159175094

Epoch: 6| Step: 6
Training loss: 2.899275779724121
Validation loss: 2.4446775144146335

Epoch: 6| Step: 7
Training loss: 2.558969497680664
Validation loss: 2.4564068625050206

Epoch: 6| Step: 8
Training loss: 2.6821956634521484
Validation loss: 2.4586095604845273

Epoch: 6| Step: 9
Training loss: 3.4605584144592285
Validation loss: 2.4534926004307245

Epoch: 6| Step: 10
Training loss: 2.3403730392456055
Validation loss: 2.451277332921182

Epoch: 6| Step: 11
Training loss: 2.156311511993408
Validation loss: 2.4568645236312703

Epoch: 6| Step: 12
Training loss: 2.6839513778686523
Validation loss: 2.454414475348688

Epoch: 6| Step: 13
Training loss: 3.319599151611328
Validation loss: 2.4495316167031564

Epoch: 138| Step: 0
Training loss: 2.206979751586914
Validation loss: 2.4441608203354703

Epoch: 6| Step: 1
Training loss: 3.193718433380127
Validation loss: 2.442253407611642

Epoch: 6| Step: 2
Training loss: 2.981752872467041
Validation loss: 2.4342347652681413

Epoch: 6| Step: 3
Training loss: 2.4039056301116943
Validation loss: 2.4406073529233216

Epoch: 6| Step: 4
Training loss: 2.6298398971557617
Validation loss: 2.4383111000061035

Epoch: 6| Step: 5
Training loss: 3.586714267730713
Validation loss: 2.4334419465834096

Epoch: 6| Step: 6
Training loss: 2.2035961151123047
Validation loss: 2.4365906741029475

Epoch: 6| Step: 7
Training loss: 2.8394992351531982
Validation loss: 2.4351066748301187

Epoch: 6| Step: 8
Training loss: 3.0433199405670166
Validation loss: 2.4326117038726807

Epoch: 6| Step: 9
Training loss: 2.5470902919769287
Validation loss: 2.4392956867012927

Epoch: 6| Step: 10
Training loss: 2.8406622409820557
Validation loss: 2.4373813931659987

Epoch: 6| Step: 11
Training loss: 2.298551559448242
Validation loss: 2.4315685072252826

Epoch: 6| Step: 12
Training loss: 1.7915451526641846
Validation loss: 2.4300910375451528

Epoch: 6| Step: 13
Training loss: 2.3705902099609375
Validation loss: 2.4354683506873345

Epoch: 139| Step: 0
Training loss: 2.5948076248168945
Validation loss: 2.435297199474868

Epoch: 6| Step: 1
Training loss: 1.9216358661651611
Validation loss: 2.431138179635489

Epoch: 6| Step: 2
Training loss: 2.2286875247955322
Validation loss: 2.432844618315338

Epoch: 6| Step: 3
Training loss: 3.1818675994873047
Validation loss: 2.434464259814191

Epoch: 6| Step: 4
Training loss: 2.2482640743255615
Validation loss: 2.4352806281018

Epoch: 6| Step: 5
Training loss: 2.993007183074951
Validation loss: 2.437790678393456

Epoch: 6| Step: 6
Training loss: 2.8376991748809814
Validation loss: 2.445363547212334

Epoch: 6| Step: 7
Training loss: 1.983847737312317
Validation loss: 2.4451202551523843

Epoch: 6| Step: 8
Training loss: 2.915977716445923
Validation loss: 2.4471174773349555

Epoch: 6| Step: 9
Training loss: 2.9346683025360107
Validation loss: 2.4495720453159784

Epoch: 6| Step: 10
Training loss: 3.4620492458343506
Validation loss: 2.4542966376068773

Epoch: 6| Step: 11
Training loss: 2.8782546520233154
Validation loss: 2.4584261243061354

Epoch: 6| Step: 12
Training loss: 2.343006134033203
Validation loss: 2.454488756836102

Epoch: 6| Step: 13
Training loss: 2.5142157077789307
Validation loss: 2.4493129996843237

Epoch: 140| Step: 0
Training loss: 1.9487628936767578
Validation loss: 2.4372843568043043

Epoch: 6| Step: 1
Training loss: 1.6327645778656006
Validation loss: 2.43211962843454

Epoch: 6| Step: 2
Training loss: 2.6692721843719482
Validation loss: 2.430642161318051

Epoch: 6| Step: 3
Training loss: 2.5756173133850098
Validation loss: 2.4234571277454333

Epoch: 6| Step: 4
Training loss: 3.159778118133545
Validation loss: 2.4296541726717384

Epoch: 6| Step: 5
Training loss: 2.5842509269714355
Validation loss: 2.4284832426296767

Epoch: 6| Step: 6
Training loss: 2.322504997253418
Validation loss: 2.4320809789883193

Epoch: 6| Step: 7
Training loss: 2.8604001998901367
Validation loss: 2.4258410699905886

Epoch: 6| Step: 8
Training loss: 3.487567663192749
Validation loss: 2.4335016922284196

Epoch: 6| Step: 9
Training loss: 2.8062362670898438
Validation loss: 2.442974139285344

Epoch: 6| Step: 10
Training loss: 2.707653045654297
Validation loss: 2.4317043365970736

Epoch: 6| Step: 11
Training loss: 3.2395401000976562
Validation loss: 2.436575937014754

Epoch: 6| Step: 12
Training loss: 2.588888645172119
Validation loss: 2.435403090651317

Epoch: 6| Step: 13
Training loss: 2.3241617679595947
Validation loss: 2.434427161370554

Epoch: 141| Step: 0
Training loss: 3.065824031829834
Validation loss: 2.427419936785134

Epoch: 6| Step: 1
Training loss: 2.2145118713378906
Validation loss: 2.425433663911717

Epoch: 6| Step: 2
Training loss: 3.653559684753418
Validation loss: 2.4263620017677225

Epoch: 6| Step: 3
Training loss: 2.6980838775634766
Validation loss: 2.4242796718433337

Epoch: 6| Step: 4
Training loss: 2.9504566192626953
Validation loss: 2.421931929485772

Epoch: 6| Step: 5
Training loss: 2.1959166526794434
Validation loss: 2.4234196755193893

Epoch: 6| Step: 6
Training loss: 2.4216537475585938
Validation loss: 2.4237759882403958

Epoch: 6| Step: 7
Training loss: 2.021017074584961
Validation loss: 2.42834186297591

Epoch: 6| Step: 8
Training loss: 3.135915756225586
Validation loss: 2.4253480921509447

Epoch: 6| Step: 9
Training loss: 2.546726703643799
Validation loss: 2.4272264383172475

Epoch: 6| Step: 10
Training loss: 2.2976560592651367
Validation loss: 2.426284300383701

Epoch: 6| Step: 11
Training loss: 2.395132064819336
Validation loss: 2.430321026873845

Epoch: 6| Step: 12
Training loss: 2.5942931175231934
Validation loss: 2.426451847117434

Epoch: 6| Step: 13
Training loss: 2.901141405105591
Validation loss: 2.420719221074094

Epoch: 142| Step: 0
Training loss: 2.42911958694458
Validation loss: 2.421097319613221

Epoch: 6| Step: 1
Training loss: 2.8302133083343506
Validation loss: 2.4251730621501966

Epoch: 6| Step: 2
Training loss: 2.5894510746002197
Validation loss: 2.4221810884373163

Epoch: 6| Step: 3
Training loss: 2.9936375617980957
Validation loss: 2.4278616187393025

Epoch: 6| Step: 4
Training loss: 2.3665177822113037
Validation loss: 2.423367404168652

Epoch: 6| Step: 5
Training loss: 2.545853614807129
Validation loss: 2.4304915730671217

Epoch: 6| Step: 6
Training loss: 1.9707040786743164
Validation loss: 2.4264226164869083

Epoch: 6| Step: 7
Training loss: 2.4434399604797363
Validation loss: 2.4267159354302192

Epoch: 6| Step: 8
Training loss: 2.7107386589050293
Validation loss: 2.4315472905353834

Epoch: 6| Step: 9
Training loss: 2.9001810550689697
Validation loss: 2.4309206675457697

Epoch: 6| Step: 10
Training loss: 3.1292495727539062
Validation loss: 2.431367012762254

Epoch: 6| Step: 11
Training loss: 2.302222728729248
Validation loss: 2.4280874524065243

Epoch: 6| Step: 12
Training loss: 3.2256150245666504
Validation loss: 2.427921125965734

Epoch: 6| Step: 13
Training loss: 2.4354329109191895
Validation loss: 2.422779816453175

Epoch: 143| Step: 0
Training loss: 2.757817268371582
Validation loss: 2.4253639457046345

Epoch: 6| Step: 1
Training loss: 2.9610581398010254
Validation loss: 2.424005321277085

Epoch: 6| Step: 2
Training loss: 2.1893227100372314
Validation loss: 2.4281268453085296

Epoch: 6| Step: 3
Training loss: 3.0841290950775146
Validation loss: 2.422245015380203

Epoch: 6| Step: 4
Training loss: 2.722369909286499
Validation loss: 2.422386548852408

Epoch: 6| Step: 5
Training loss: 2.724307060241699
Validation loss: 2.4243366667019424

Epoch: 6| Step: 6
Training loss: 2.758453845977783
Validation loss: 2.4230354447518625

Epoch: 6| Step: 7
Training loss: 2.0415453910827637
Validation loss: 2.4286845473832983

Epoch: 6| Step: 8
Training loss: 2.328955888748169
Validation loss: 2.427023231342275

Epoch: 6| Step: 9
Training loss: 3.1540985107421875
Validation loss: 2.431021503222886

Epoch: 6| Step: 10
Training loss: 2.743499755859375
Validation loss: 2.427114789203931

Epoch: 6| Step: 11
Training loss: 2.4977660179138184
Validation loss: 2.428756406230311

Epoch: 6| Step: 12
Training loss: 2.5745584964752197
Validation loss: 2.427904398210587

Epoch: 6| Step: 13
Training loss: 2.314966917037964
Validation loss: 2.4317328186445337

Epoch: 144| Step: 0
Training loss: 3.3548240661621094
Validation loss: 2.432354906553863

Epoch: 6| Step: 1
Training loss: 2.171180248260498
Validation loss: 2.426324491859764

Epoch: 6| Step: 2
Training loss: 2.4527361392974854
Validation loss: 2.4254530014530307

Epoch: 6| Step: 3
Training loss: 2.9822285175323486
Validation loss: 2.4254987419292493

Epoch: 6| Step: 4
Training loss: 2.647606372833252
Validation loss: 2.4214606567095687

Epoch: 6| Step: 5
Training loss: 2.413374185562134
Validation loss: 2.4185349556707565

Epoch: 6| Step: 6
Training loss: 2.866643190383911
Validation loss: 2.4204559172353437

Epoch: 6| Step: 7
Training loss: 2.8323957920074463
Validation loss: 2.4205433501992175

Epoch: 6| Step: 8
Training loss: 2.0967421531677246
Validation loss: 2.4137478977121334

Epoch: 6| Step: 9
Training loss: 2.826791286468506
Validation loss: 2.4173922359302478

Epoch: 6| Step: 10
Training loss: 2.139444351196289
Validation loss: 2.417413891002696

Epoch: 6| Step: 11
Training loss: 1.9289970397949219
Validation loss: 2.4188197863999235

Epoch: 6| Step: 12
Training loss: 3.290621519088745
Validation loss: 2.4259167794258363

Epoch: 6| Step: 13
Training loss: 3.2381083965301514
Validation loss: 2.4248546579832673

Epoch: 145| Step: 0
Training loss: 2.6823246479034424
Validation loss: 2.439200255178636

Epoch: 6| Step: 1
Training loss: 2.76948618888855
Validation loss: 2.431720031205044

Epoch: 6| Step: 2
Training loss: 2.6833291053771973
Validation loss: 2.439598678260721

Epoch: 6| Step: 3
Training loss: 3.0786526203155518
Validation loss: 2.4479504759593675

Epoch: 6| Step: 4
Training loss: 3.28719162940979
Validation loss: 2.4493982945719073

Epoch: 6| Step: 5
Training loss: 3.0423567295074463
Validation loss: 2.4403055534567883

Epoch: 6| Step: 6
Training loss: 2.025322437286377
Validation loss: 2.431443811744772

Epoch: 6| Step: 7
Training loss: 2.4497721195220947
Validation loss: 2.4252386246958086

Epoch: 6| Step: 8
Training loss: 2.8761515617370605
Validation loss: 2.4263026534870105

Epoch: 6| Step: 9
Training loss: 2.273460865020752
Validation loss: 2.427106995736399

Epoch: 6| Step: 10
Training loss: 2.5699806213378906
Validation loss: 2.4170550120774137

Epoch: 6| Step: 11
Training loss: 2.2151899337768555
Validation loss: 2.420594274356801

Epoch: 6| Step: 12
Training loss: 1.9262678623199463
Validation loss: 2.4167743139369513

Epoch: 6| Step: 13
Training loss: 3.3919460773468018
Validation loss: 2.4201945130543043

Epoch: 146| Step: 0
Training loss: 2.441704273223877
Validation loss: 2.415064181050947

Epoch: 6| Step: 1
Training loss: 2.1140294075012207
Validation loss: 2.4215891694509857

Epoch: 6| Step: 2
Training loss: 2.97359299659729
Validation loss: 2.4273324679302912

Epoch: 6| Step: 3
Training loss: 2.431703805923462
Validation loss: 2.4228953751184608

Epoch: 6| Step: 4
Training loss: 3.1567249298095703
Validation loss: 2.417648355166117

Epoch: 6| Step: 5
Training loss: 2.2828361988067627
Validation loss: 2.4222748125753095

Epoch: 6| Step: 6
Training loss: 2.2426092624664307
Validation loss: 2.422367165165563

Epoch: 6| Step: 7
Training loss: 2.370023488998413
Validation loss: 2.4180792070204213

Epoch: 6| Step: 8
Training loss: 2.6453752517700195
Validation loss: 2.4186751945044405

Epoch: 6| Step: 9
Training loss: 3.3013200759887695
Validation loss: 2.419099054028911

Epoch: 6| Step: 10
Training loss: 3.076411008834839
Validation loss: 2.422551029471941

Epoch: 6| Step: 11
Training loss: 2.1953413486480713
Validation loss: 2.4155125284707673

Epoch: 6| Step: 12
Training loss: 2.698385715484619
Validation loss: 2.4156682875848587

Epoch: 6| Step: 13
Training loss: 3.153695821762085
Validation loss: 2.4153163612529798

Epoch: 147| Step: 0
Training loss: 1.9376142024993896
Validation loss: 2.41379540453675

Epoch: 6| Step: 1
Training loss: 3.193695068359375
Validation loss: 2.423613855915685

Epoch: 6| Step: 2
Training loss: 3.0145039558410645
Validation loss: 2.4179670272334928

Epoch: 6| Step: 3
Training loss: 2.0129849910736084
Validation loss: 2.4163746115981892

Epoch: 6| Step: 4
Training loss: 2.2891054153442383
Validation loss: 2.416116524768132

Epoch: 6| Step: 5
Training loss: 2.332550525665283
Validation loss: 2.4175865547631377

Epoch: 6| Step: 6
Training loss: 2.9549617767333984
Validation loss: 2.4213228943527385

Epoch: 6| Step: 7
Training loss: 2.8882007598876953
Validation loss: 2.4240109984592726

Epoch: 6| Step: 8
Training loss: 2.4214510917663574
Validation loss: 2.4328306669829995

Epoch: 6| Step: 9
Training loss: 2.792868137359619
Validation loss: 2.4341703050880024

Epoch: 6| Step: 10
Training loss: 2.7236530780792236
Validation loss: 2.4329626893484466

Epoch: 6| Step: 11
Training loss: 3.138679027557373
Validation loss: 2.434943732394967

Epoch: 6| Step: 12
Training loss: 2.4260568618774414
Validation loss: 2.431011156369281

Epoch: 6| Step: 13
Training loss: 2.8873291015625
Validation loss: 2.4288896078704507

Epoch: 148| Step: 0
Training loss: 2.2253806591033936
Validation loss: 2.428188967448409

Epoch: 6| Step: 1
Training loss: 2.784400463104248
Validation loss: 2.4251353458691667

Epoch: 6| Step: 2
Training loss: 2.630774974822998
Validation loss: 2.417748015414002

Epoch: 6| Step: 3
Training loss: 3.105501890182495
Validation loss: 2.414240011604883

Epoch: 6| Step: 4
Training loss: 2.0032835006713867
Validation loss: 2.4138033184953915

Epoch: 6| Step: 5
Training loss: 2.2489218711853027
Validation loss: 2.414319938228976

Epoch: 6| Step: 6
Training loss: 2.925534963607788
Validation loss: 2.415340300529234

Epoch: 6| Step: 7
Training loss: 2.532374620437622
Validation loss: 2.4138755772703435

Epoch: 6| Step: 8
Training loss: 3.310603141784668
Validation loss: 2.4152286565431984

Epoch: 6| Step: 9
Training loss: 2.8496146202087402
Validation loss: 2.4186385344433528

Epoch: 6| Step: 10
Training loss: 2.9936447143554688
Validation loss: 2.409801370354109

Epoch: 6| Step: 11
Training loss: 1.8538967370986938
Validation loss: 2.4103111887490876

Epoch: 6| Step: 12
Training loss: 2.53605318069458
Validation loss: 2.41592344673731

Epoch: 6| Step: 13
Training loss: 2.9491026401519775
Validation loss: 2.4144547600899973

Epoch: 149| Step: 0
Training loss: 3.039689540863037
Validation loss: 2.413565945881669

Epoch: 6| Step: 1
Training loss: 2.3404006958007812
Validation loss: 2.4152982260591243

Epoch: 6| Step: 2
Training loss: 2.195469617843628
Validation loss: 2.4185115932136454

Epoch: 6| Step: 3
Training loss: 2.4758713245391846
Validation loss: 2.4164496801232778

Epoch: 6| Step: 4
Training loss: 2.012742519378662
Validation loss: 2.4144843342483684

Epoch: 6| Step: 5
Training loss: 3.542637348175049
Validation loss: 2.4176732391439457

Epoch: 6| Step: 6
Training loss: 3.3858792781829834
Validation loss: 2.4227787551059516

Epoch: 6| Step: 7
Training loss: 2.374478340148926
Validation loss: 2.4218738284162296

Epoch: 6| Step: 8
Training loss: 2.740508556365967
Validation loss: 2.427910997021583

Epoch: 6| Step: 9
Training loss: 2.5226030349731445
Validation loss: 2.4321034698076147

Epoch: 6| Step: 10
Training loss: 2.3734397888183594
Validation loss: 2.4329543036799275

Epoch: 6| Step: 11
Training loss: 2.4514989852905273
Validation loss: 2.430005012019988

Epoch: 6| Step: 12
Training loss: 2.9828805923461914
Validation loss: 2.436241921558175

Epoch: 6| Step: 13
Training loss: 2.1384904384613037
Validation loss: 2.428807561115552

Epoch: 150| Step: 0
Training loss: 2.330754518508911
Validation loss: 2.4218648633649273

Epoch: 6| Step: 1
Training loss: 3.5137791633605957
Validation loss: 2.429380847561744

Epoch: 6| Step: 2
Training loss: 2.6198983192443848
Validation loss: 2.422447048207765

Epoch: 6| Step: 3
Training loss: 2.8377187252044678
Validation loss: 2.419050698639244

Epoch: 6| Step: 4
Training loss: 2.609217643737793
Validation loss: 2.417641926837224

Epoch: 6| Step: 5
Training loss: 3.1068456172943115
Validation loss: 2.4253552677810832

Epoch: 6| Step: 6
Training loss: 1.7453469038009644
Validation loss: 2.4220186792394167

Epoch: 6| Step: 7
Training loss: 3.002178430557251
Validation loss: 2.4193264233168734

Epoch: 6| Step: 8
Training loss: 2.131488561630249
Validation loss: 2.4154260825085383

Epoch: 6| Step: 9
Training loss: 2.82120418548584
Validation loss: 2.4145919148639967

Epoch: 6| Step: 10
Training loss: 1.9511898756027222
Validation loss: 2.4163049139002317

Epoch: 6| Step: 11
Training loss: 2.2768800258636475
Validation loss: 2.410043436993835

Epoch: 6| Step: 12
Training loss: 3.1418356895446777
Validation loss: 2.4076075169347946

Epoch: 6| Step: 13
Training loss: 2.789961338043213
Validation loss: 2.409634168430041

Epoch: 151| Step: 0
Training loss: 2.3503637313842773
Validation loss: 2.40936585908295

Epoch: 6| Step: 1
Training loss: 2.9419727325439453
Validation loss: 2.4112925221843104

Epoch: 6| Step: 2
Training loss: 1.9345617294311523
Validation loss: 2.4144565687384656

Epoch: 6| Step: 3
Training loss: 2.3644323348999023
Validation loss: 2.422928148700345

Epoch: 6| Step: 4
Training loss: 2.736299991607666
Validation loss: 2.432409619772306

Epoch: 6| Step: 5
Training loss: 2.704357862472534
Validation loss: 2.4492285789981967

Epoch: 6| Step: 6
Training loss: 2.3609988689422607
Validation loss: 2.456746062924785

Epoch: 6| Step: 7
Training loss: 2.3060035705566406
Validation loss: 2.455707373157624

Epoch: 6| Step: 8
Training loss: 2.8654935359954834
Validation loss: 2.4557152230252504

Epoch: 6| Step: 9
Training loss: 2.3946523666381836
Validation loss: 2.4612333633566417

Epoch: 6| Step: 10
Training loss: 3.5622963905334473
Validation loss: 2.4556107879966818

Epoch: 6| Step: 11
Training loss: 2.8598804473876953
Validation loss: 2.45244590954114

Epoch: 6| Step: 12
Training loss: 3.0726842880249023
Validation loss: 2.4575729370117188

Epoch: 6| Step: 13
Training loss: 2.3123762607574463
Validation loss: 2.436451171034126

Epoch: 152| Step: 0
Training loss: 2.5846874713897705
Validation loss: 2.4278827175017326

Epoch: 6| Step: 1
Training loss: 3.0391945838928223
Validation loss: 2.4084494472831808

Epoch: 6| Step: 2
Training loss: 3.4113550186157227
Validation loss: 2.4068085044942875

Epoch: 6| Step: 3
Training loss: 2.7033252716064453
Validation loss: 2.4048377083193873

Epoch: 6| Step: 4
Training loss: 2.605940818786621
Validation loss: 2.4047993331827144

Epoch: 6| Step: 5
Training loss: 2.851790428161621
Validation loss: 2.400034266133462

Epoch: 6| Step: 6
Training loss: 2.2148239612579346
Validation loss: 2.406553088977773

Epoch: 6| Step: 7
Training loss: 2.902315139770508
Validation loss: 2.4034688447111394

Epoch: 6| Step: 8
Training loss: 2.5536739826202393
Validation loss: 2.402726149046293

Epoch: 6| Step: 9
Training loss: 1.5798753499984741
Validation loss: 2.4041022459665933

Epoch: 6| Step: 10
Training loss: 1.9954416751861572
Validation loss: 2.4067502329426427

Epoch: 6| Step: 11
Training loss: 3.103717803955078
Validation loss: 2.4063498204754246

Epoch: 6| Step: 12
Training loss: 2.5633561611175537
Validation loss: 2.403605677748239

Epoch: 6| Step: 13
Training loss: 2.6771557331085205
Validation loss: 2.4107497276798373

Epoch: 153| Step: 0
Training loss: 3.170346736907959
Validation loss: 2.4052556253248647

Epoch: 6| Step: 1
Training loss: 3.1400628089904785
Validation loss: 2.4093339827752884

Epoch: 6| Step: 2
Training loss: 1.834786057472229
Validation loss: 2.4049417357290945

Epoch: 6| Step: 3
Training loss: 2.8055238723754883
Validation loss: 2.4118250775080856

Epoch: 6| Step: 4
Training loss: 2.566498279571533
Validation loss: 2.412323828666441

Epoch: 6| Step: 5
Training loss: 2.4215235710144043
Validation loss: 2.4114608636466404

Epoch: 6| Step: 6
Training loss: 2.760286569595337
Validation loss: 2.409202571838133

Epoch: 6| Step: 7
Training loss: 2.546065330505371
Validation loss: 2.41293014249494

Epoch: 6| Step: 8
Training loss: 2.5123000144958496
Validation loss: 2.4128072466901553

Epoch: 6| Step: 9
Training loss: 2.525068759918213
Validation loss: 2.4261098241293304

Epoch: 6| Step: 10
Training loss: 1.7796070575714111
Validation loss: 2.4269749836255143

Epoch: 6| Step: 11
Training loss: 3.039367198944092
Validation loss: 2.438758368133217

Epoch: 6| Step: 12
Training loss: 3.0125937461853027
Validation loss: 2.4395240993909937

Epoch: 6| Step: 13
Training loss: 2.802685260772705
Validation loss: 2.4380263205497497

Epoch: 154| Step: 0
Training loss: 2.346137523651123
Validation loss: 2.4360243505047214

Epoch: 6| Step: 1
Training loss: 2.643951892852783
Validation loss: 2.4219761561321955

Epoch: 6| Step: 2
Training loss: 3.1431334018707275
Validation loss: 2.416789389425708

Epoch: 6| Step: 3
Training loss: 3.004117727279663
Validation loss: 2.418015797932943

Epoch: 6| Step: 4
Training loss: 2.889594554901123
Validation loss: 2.4197316887558147

Epoch: 6| Step: 5
Training loss: 2.098597764968872
Validation loss: 2.421432838645033

Epoch: 6| Step: 6
Training loss: 2.7323124408721924
Validation loss: 2.416410889676822

Epoch: 6| Step: 7
Training loss: 3.0854713916778564
Validation loss: 2.4209072487328642

Epoch: 6| Step: 8
Training loss: 1.2496328353881836
Validation loss: 2.4159326066253004

Epoch: 6| Step: 9
Training loss: 2.3031396865844727
Validation loss: 2.423881200052077

Epoch: 6| Step: 10
Training loss: 3.567417621612549
Validation loss: 2.4362586800770094

Epoch: 6| Step: 11
Training loss: 2.839108467102051
Validation loss: 2.4301276796607563

Epoch: 6| Step: 12
Training loss: 2.181767702102661
Validation loss: 2.4308990509279313

Epoch: 6| Step: 13
Training loss: 2.7273194789886475
Validation loss: 2.439670426871187

Epoch: 155| Step: 0
Training loss: 2.4180073738098145
Validation loss: 2.4481139670136156

Epoch: 6| Step: 1
Training loss: 3.1750125885009766
Validation loss: 2.442210341012606

Epoch: 6| Step: 2
Training loss: 3.3595287799835205
Validation loss: 2.4485335965310373

Epoch: 6| Step: 3
Training loss: 2.4638571739196777
Validation loss: 2.454768442338513

Epoch: 6| Step: 4
Training loss: 2.120009422302246
Validation loss: 2.4412356089520197

Epoch: 6| Step: 5
Training loss: 2.8324873447418213
Validation loss: 2.4478507913568968

Epoch: 6| Step: 6
Training loss: 2.109949827194214
Validation loss: 2.4286413167112615

Epoch: 6| Step: 7
Training loss: 2.8777873516082764
Validation loss: 2.4135345694839314

Epoch: 6| Step: 8
Training loss: 2.277529239654541
Validation loss: 2.4128175576527915

Epoch: 6| Step: 9
Training loss: 3.1465253829956055
Validation loss: 2.4067651866584696

Epoch: 6| Step: 10
Training loss: 2.6582236289978027
Validation loss: 2.405307262174545

Epoch: 6| Step: 11
Training loss: 2.3534274101257324
Validation loss: 2.4048491190838557

Epoch: 6| Step: 12
Training loss: 2.300100803375244
Validation loss: 2.4100778948876167

Epoch: 6| Step: 13
Training loss: 2.833590507507324
Validation loss: 2.418523898688696

Epoch: 156| Step: 0
Training loss: 3.6339526176452637
Validation loss: 2.4199479138979347

Epoch: 6| Step: 1
Training loss: 2.85264253616333
Validation loss: 2.420863013113699

Epoch: 6| Step: 2
Training loss: 2.5528855323791504
Validation loss: 2.424020374974897

Epoch: 6| Step: 3
Training loss: 2.9924232959747314
Validation loss: 2.435082576608145

Epoch: 6| Step: 4
Training loss: 2.393629550933838
Validation loss: 2.4313912417299006

Epoch: 6| Step: 5
Training loss: 2.3868231773376465
Validation loss: 2.4343350856534895

Epoch: 6| Step: 6
Training loss: 1.6799988746643066
Validation loss: 2.4377509060726372

Epoch: 6| Step: 7
Training loss: 3.5634546279907227
Validation loss: 2.4408420465325795

Epoch: 6| Step: 8
Training loss: 2.411907196044922
Validation loss: 2.432121064073296

Epoch: 6| Step: 9
Training loss: 2.5666158199310303
Validation loss: 2.42303620102585

Epoch: 6| Step: 10
Training loss: 2.1618103981018066
Validation loss: 2.420963994918331

Epoch: 6| Step: 11
Training loss: 2.396632432937622
Validation loss: 2.414059346722018

Epoch: 6| Step: 12
Training loss: 2.95652174949646
Validation loss: 2.4084468426242953

Epoch: 6| Step: 13
Training loss: 2.0739872455596924
Validation loss: 2.4105412139687488

Epoch: 157| Step: 0
Training loss: 2.4529972076416016
Validation loss: 2.40372085186743

Epoch: 6| Step: 1
Training loss: 2.820603370666504
Validation loss: 2.403617130812778

Epoch: 6| Step: 2
Training loss: 2.5266003608703613
Validation loss: 2.399507599492227

Epoch: 6| Step: 3
Training loss: 2.0874037742614746
Validation loss: 2.4024304471990114

Epoch: 6| Step: 4
Training loss: 2.2176332473754883
Validation loss: 2.405557874710329

Epoch: 6| Step: 5
Training loss: 3.157846450805664
Validation loss: 2.4045755132552116

Epoch: 6| Step: 6
Training loss: 2.2344424724578857
Validation loss: 2.405814188782887

Epoch: 6| Step: 7
Training loss: 2.994821548461914
Validation loss: 2.4096069079573437

Epoch: 6| Step: 8
Training loss: 2.943920612335205
Validation loss: 2.412603450077836

Epoch: 6| Step: 9
Training loss: 2.510380506515503
Validation loss: 2.4162876311168877

Epoch: 6| Step: 10
Training loss: 2.6329715251922607
Validation loss: 2.4189342375724547

Epoch: 6| Step: 11
Training loss: 3.46518611907959
Validation loss: 2.4227117876852713

Epoch: 6| Step: 12
Training loss: 1.9045348167419434
Validation loss: 2.4200462141344623

Epoch: 6| Step: 13
Training loss: 2.92828106880188
Validation loss: 2.416267115582702

Epoch: 158| Step: 0
Training loss: 2.398815631866455
Validation loss: 2.4150976378430604

Epoch: 6| Step: 1
Training loss: 2.9310359954833984
Validation loss: 2.4128676165816603

Epoch: 6| Step: 2
Training loss: 2.372302770614624
Validation loss: 2.4099795561964794

Epoch: 6| Step: 3
Training loss: 2.7615857124328613
Validation loss: 2.4023776874747327

Epoch: 6| Step: 4
Training loss: 2.7416491508483887
Validation loss: 2.4048212010373353

Epoch: 6| Step: 5
Training loss: 1.8156253099441528
Validation loss: 2.399478937989922

Epoch: 6| Step: 6
Training loss: 2.4312610626220703
Validation loss: 2.3962604102268013

Epoch: 6| Step: 7
Training loss: 3.3584136962890625
Validation loss: 2.3988090022917716

Epoch: 6| Step: 8
Training loss: 2.5638227462768555
Validation loss: 2.400001768142946

Epoch: 6| Step: 9
Training loss: 3.203972339630127
Validation loss: 2.398068615185317

Epoch: 6| Step: 10
Training loss: 2.170093536376953
Validation loss: 2.398283839225769

Epoch: 6| Step: 11
Training loss: 2.7261760234832764
Validation loss: 2.39914458797824

Epoch: 6| Step: 12
Training loss: 2.670891761779785
Validation loss: 2.396171913352064

Epoch: 6| Step: 13
Training loss: 2.5747642517089844
Validation loss: 2.403410303977228

Epoch: 159| Step: 0
Training loss: 2.2956597805023193
Validation loss: 2.4053749679237284

Epoch: 6| Step: 1
Training loss: 2.3764467239379883
Validation loss: 2.400754080023817

Epoch: 6| Step: 2
Training loss: 2.257728338241577
Validation loss: 2.4072875156197497

Epoch: 6| Step: 3
Training loss: 2.446077346801758
Validation loss: 2.411624962283719

Epoch: 6| Step: 4
Training loss: 2.9002742767333984
Validation loss: 2.40587003256685

Epoch: 6| Step: 5
Training loss: 2.2465457916259766
Validation loss: 2.41566494459747

Epoch: 6| Step: 6
Training loss: 2.779810905456543
Validation loss: 2.4192973747048327

Epoch: 6| Step: 7
Training loss: 2.8289806842803955
Validation loss: 2.415542358993202

Epoch: 6| Step: 8
Training loss: 1.9560878276824951
Validation loss: 2.416251487629388

Epoch: 6| Step: 9
Training loss: 2.802727460861206
Validation loss: 2.4162355392209944

Epoch: 6| Step: 10
Training loss: 2.7027196884155273
Validation loss: 2.420740981255808

Epoch: 6| Step: 11
Training loss: 2.9983041286468506
Validation loss: 2.41523160729357

Epoch: 6| Step: 12
Training loss: 3.494419813156128
Validation loss: 2.4022290116997174

Epoch: 6| Step: 13
Training loss: 2.521422863006592
Validation loss: 2.4079218000494023

Epoch: 160| Step: 0
Training loss: 3.144451856613159
Validation loss: 2.404331427748485

Epoch: 6| Step: 1
Training loss: 2.454669952392578
Validation loss: 2.4027603877488004

Epoch: 6| Step: 2
Training loss: 2.488152503967285
Validation loss: 2.3979667002154934

Epoch: 6| Step: 3
Training loss: 2.0687003135681152
Validation loss: 2.404943034213076

Epoch: 6| Step: 4
Training loss: 2.6436498165130615
Validation loss: 2.4040308434476136

Epoch: 6| Step: 5
Training loss: 2.7511940002441406
Validation loss: 2.4054815923013995

Epoch: 6| Step: 6
Training loss: 2.6366724967956543
Validation loss: 2.4036180101415163

Epoch: 6| Step: 7
Training loss: 2.6949875354766846
Validation loss: 2.4073495685413318

Epoch: 6| Step: 8
Training loss: 2.9918572902679443
Validation loss: 2.4046622860816216

Epoch: 6| Step: 9
Training loss: 2.6434988975524902
Validation loss: 2.4042413234710693

Epoch: 6| Step: 10
Training loss: 3.0570592880249023
Validation loss: 2.4034777764351136

Epoch: 6| Step: 11
Training loss: 2.1157379150390625
Validation loss: 2.398477127475123

Epoch: 6| Step: 12
Training loss: 2.3233747482299805
Validation loss: 2.4023343619479927

Epoch: 6| Step: 13
Training loss: 2.626640558242798
Validation loss: 2.4023060260280484

Epoch: 161| Step: 0
Training loss: 2.2327823638916016
Validation loss: 2.406156324571179

Epoch: 6| Step: 1
Training loss: 2.253234386444092
Validation loss: 2.410192653697024

Epoch: 6| Step: 2
Training loss: 2.7838873863220215
Validation loss: 2.4054879347483316

Epoch: 6| Step: 3
Training loss: 2.055110216140747
Validation loss: 2.4107045870955273

Epoch: 6| Step: 4
Training loss: 3.315925121307373
Validation loss: 2.420659021664691

Epoch: 6| Step: 5
Training loss: 2.7082936763763428
Validation loss: 2.4302817647175123

Epoch: 6| Step: 6
Training loss: 2.941920518875122
Validation loss: 2.4282022753069477

Epoch: 6| Step: 7
Training loss: 2.1852149963378906
Validation loss: 2.428034592700261

Epoch: 6| Step: 8
Training loss: 2.824859619140625
Validation loss: 2.4322118733518865

Epoch: 6| Step: 9
Training loss: 2.442167282104492
Validation loss: 2.422903912041777

Epoch: 6| Step: 10
Training loss: 2.9356131553649902
Validation loss: 2.406385270498132

Epoch: 6| Step: 11
Training loss: 2.799940586090088
Validation loss: 2.40119356237432

Epoch: 6| Step: 12
Training loss: 2.5340495109558105
Validation loss: 2.398104016498853

Epoch: 6| Step: 13
Training loss: 2.6421005725860596
Validation loss: 2.3988507870704896

Epoch: 162| Step: 0
Training loss: 2.570977210998535
Validation loss: 2.401682183306704

Epoch: 6| Step: 1
Training loss: 1.9163131713867188
Validation loss: 2.4026000909907843

Epoch: 6| Step: 2
Training loss: 2.5511064529418945
Validation loss: 2.4031356047558528

Epoch: 6| Step: 3
Training loss: 1.7321622371673584
Validation loss: 2.4118991897952173

Epoch: 6| Step: 4
Training loss: 2.1822142601013184
Validation loss: 2.413066907595563

Epoch: 6| Step: 5
Training loss: 2.68580961227417
Validation loss: 2.4149429400761924

Epoch: 6| Step: 6
Training loss: 2.099954605102539
Validation loss: 2.4248222535656345

Epoch: 6| Step: 7
Training loss: 3.1900296211242676
Validation loss: 2.4213978603322017

Epoch: 6| Step: 8
Training loss: 2.7318222522735596
Validation loss: 2.4163132303504535

Epoch: 6| Step: 9
Training loss: 3.283844470977783
Validation loss: 2.4150228551639024

Epoch: 6| Step: 10
Training loss: 2.150238037109375
Validation loss: 2.413757085800171

Epoch: 6| Step: 11
Training loss: 3.5481977462768555
Validation loss: 2.4122294713092107

Epoch: 6| Step: 12
Training loss: 3.2890844345092773
Validation loss: 2.4017274995003977

Epoch: 6| Step: 13
Training loss: 2.8494691848754883
Validation loss: 2.39980314623925

Epoch: 163| Step: 0
Training loss: 2.6276133060455322
Validation loss: 2.393949281784796

Epoch: 6| Step: 1
Training loss: 2.9264118671417236
Validation loss: 2.3899318069540043

Epoch: 6| Step: 2
Training loss: 3.3729679584503174
Validation loss: 2.3936691476452734

Epoch: 6| Step: 3
Training loss: 2.68467378616333
Validation loss: 2.3902241542775142

Epoch: 6| Step: 4
Training loss: 2.5602669715881348
Validation loss: 2.388616925926619

Epoch: 6| Step: 5
Training loss: 3.4448838233947754
Validation loss: 2.3944793132043656

Epoch: 6| Step: 6
Training loss: 2.6715450286865234
Validation loss: 2.385303815205892

Epoch: 6| Step: 7
Training loss: 2.149885654449463
Validation loss: 2.387216644902383

Epoch: 6| Step: 8
Training loss: 2.5282750129699707
Validation loss: 2.3883464567122923

Epoch: 6| Step: 9
Training loss: 2.7719779014587402
Validation loss: 2.3861693182299213

Epoch: 6| Step: 10
Training loss: 2.504732370376587
Validation loss: 2.3898631167668167

Epoch: 6| Step: 11
Training loss: 2.1431891918182373
Validation loss: 2.398850933198006

Epoch: 6| Step: 12
Training loss: 2.004211187362671
Validation loss: 2.4052178090618503

Epoch: 6| Step: 13
Training loss: 2.0936052799224854
Validation loss: 2.413618770978784

Epoch: 164| Step: 0
Training loss: 1.4075531959533691
Validation loss: 2.4186991901807886

Epoch: 6| Step: 1
Training loss: 2.7941017150878906
Validation loss: 2.434968412563365

Epoch: 6| Step: 2
Training loss: 1.9540603160858154
Validation loss: 2.437488305953241

Epoch: 6| Step: 3
Training loss: 3.016597032546997
Validation loss: 2.446054220199585

Epoch: 6| Step: 4
Training loss: 2.839068651199341
Validation loss: 2.4457107743909283

Epoch: 6| Step: 5
Training loss: 2.1959164142608643
Validation loss: 2.426523588036978

Epoch: 6| Step: 6
Training loss: 2.3511343002319336
Validation loss: 2.416021672628259

Epoch: 6| Step: 7
Training loss: 2.848322868347168
Validation loss: 2.4027496178944907

Epoch: 6| Step: 8
Training loss: 3.160431385040283
Validation loss: 2.390163713885892

Epoch: 6| Step: 9
Training loss: 3.360832452774048
Validation loss: 2.3851725619326354

Epoch: 6| Step: 10
Training loss: 2.785313606262207
Validation loss: 2.383561923939695

Epoch: 6| Step: 11
Training loss: 2.298854351043701
Validation loss: 2.3823038890797603

Epoch: 6| Step: 12
Training loss: 3.048455238342285
Validation loss: 2.384471554909983

Epoch: 6| Step: 13
Training loss: 2.9521331787109375
Validation loss: 2.3830138893537622

Epoch: 165| Step: 0
Training loss: 2.81929349899292
Validation loss: 2.3868315604425248

Epoch: 6| Step: 1
Training loss: 2.630744695663452
Validation loss: 2.3827574778628606

Epoch: 6| Step: 2
Training loss: 2.056771993637085
Validation loss: 2.3951018548780874

Epoch: 6| Step: 3
Training loss: 2.7453014850616455
Validation loss: 2.3816822062256517

Epoch: 6| Step: 4
Training loss: 2.492030620574951
Validation loss: 2.392832825260778

Epoch: 6| Step: 5
Training loss: 2.625454902648926
Validation loss: 2.3975235275042954

Epoch: 6| Step: 6
Training loss: 2.315629482269287
Validation loss: 2.4111667730474986

Epoch: 6| Step: 7
Training loss: 2.5754153728485107
Validation loss: 2.4070427930483254

Epoch: 6| Step: 8
Training loss: 2.4611830711364746
Validation loss: 2.408303717131256

Epoch: 6| Step: 9
Training loss: 3.1718077659606934
Validation loss: 2.419036111524028

Epoch: 6| Step: 10
Training loss: 2.6282145977020264
Validation loss: 2.4119642678127495

Epoch: 6| Step: 11
Training loss: 3.187422275543213
Validation loss: 2.416913440150599

Epoch: 6| Step: 12
Training loss: 2.590224266052246
Validation loss: 2.4169252149520384

Epoch: 6| Step: 13
Training loss: 1.9456977844238281
Validation loss: 2.4168190212659937

Epoch: 166| Step: 0
Training loss: 1.749893069267273
Validation loss: 2.4101070665544078

Epoch: 6| Step: 1
Training loss: 3.1498401165008545
Validation loss: 2.4046872713232554

Epoch: 6| Step: 2
Training loss: 2.4651288986206055
Validation loss: 2.4009657290674027

Epoch: 6| Step: 3
Training loss: 3.37088680267334
Validation loss: 2.408735321414086

Epoch: 6| Step: 4
Training loss: 2.9250104427337646
Validation loss: 2.396182567842545

Epoch: 6| Step: 5
Training loss: 3.079092025756836
Validation loss: 2.3990107121006137

Epoch: 6| Step: 6
Training loss: 2.6662397384643555
Validation loss: 2.394232929393809

Epoch: 6| Step: 7
Training loss: 2.156445026397705
Validation loss: 2.397021575640607

Epoch: 6| Step: 8
Training loss: 2.6486074924468994
Validation loss: 2.3974059986811813

Epoch: 6| Step: 9
Training loss: 2.778742790222168
Validation loss: 2.392479811945269

Epoch: 6| Step: 10
Training loss: 2.298766851425171
Validation loss: 2.3985246278906382

Epoch: 6| Step: 11
Training loss: 1.7525207996368408
Validation loss: 2.3947493542907057

Epoch: 6| Step: 12
Training loss: 2.7033987045288086
Validation loss: 2.388645610501689

Epoch: 6| Step: 13
Training loss: 2.8349227905273438
Validation loss: 2.382693516310825

Epoch: 167| Step: 0
Training loss: 2.293160915374756
Validation loss: 2.3839258276006228

Epoch: 6| Step: 1
Training loss: 2.3394484519958496
Validation loss: 2.38251006731423

Epoch: 6| Step: 2
Training loss: 3.0101430416107178
Validation loss: 2.3805405580869285

Epoch: 6| Step: 3
Training loss: 3.3289127349853516
Validation loss: 2.3829236235669864

Epoch: 6| Step: 4
Training loss: 2.723598003387451
Validation loss: 2.3802370537993727

Epoch: 6| Step: 5
Training loss: 2.642259120941162
Validation loss: 2.3812748847469205

Epoch: 6| Step: 6
Training loss: 1.4054844379425049
Validation loss: 2.3759901549226496

Epoch: 6| Step: 7
Training loss: 3.074126720428467
Validation loss: 2.387931946785219

Epoch: 6| Step: 8
Training loss: 2.7550435066223145
Validation loss: 2.3856113854274956

Epoch: 6| Step: 9
Training loss: 2.462721824645996
Validation loss: 2.3789703487068095

Epoch: 6| Step: 10
Training loss: 2.736693859100342
Validation loss: 2.374126554817282

Epoch: 6| Step: 11
Training loss: 2.5084784030914307
Validation loss: 2.378486566646125

Epoch: 6| Step: 12
Training loss: 2.7130556106567383
Validation loss: 2.3815964819282613

Epoch: 6| Step: 13
Training loss: 2.466404676437378
Validation loss: 2.3813048280695432

Epoch: 168| Step: 0
Training loss: 2.4972424507141113
Validation loss: 2.382421411493773

Epoch: 6| Step: 1
Training loss: 2.3765697479248047
Validation loss: 2.3814408984235538

Epoch: 6| Step: 2
Training loss: 2.6196541786193848
Validation loss: 2.381495652660247

Epoch: 6| Step: 3
Training loss: 2.6940507888793945
Validation loss: 2.3814329331920994

Epoch: 6| Step: 4
Training loss: 3.3012142181396484
Validation loss: 2.3854210915104037

Epoch: 6| Step: 5
Training loss: 3.813082695007324
Validation loss: 2.383622920641335

Epoch: 6| Step: 6
Training loss: 2.292161703109741
Validation loss: 2.386830242731238

Epoch: 6| Step: 7
Training loss: 2.564143419265747
Validation loss: 2.393559673781036

Epoch: 6| Step: 8
Training loss: 1.802713394165039
Validation loss: 2.388149248656406

Epoch: 6| Step: 9
Training loss: 2.753269672393799
Validation loss: 2.3833445451592885

Epoch: 6| Step: 10
Training loss: 2.597752571105957
Validation loss: 2.381344569626675

Epoch: 6| Step: 11
Training loss: 2.3836922645568848
Validation loss: 2.3744837276397215

Epoch: 6| Step: 12
Training loss: 2.6215221881866455
Validation loss: 2.379530704149636

Epoch: 6| Step: 13
Training loss: 1.7773808240890503
Validation loss: 2.376752781611617

Epoch: 169| Step: 0
Training loss: 2.5084428787231445
Validation loss: 2.3810396425185667

Epoch: 6| Step: 1
Training loss: 1.9989627599716187
Validation loss: 2.3839122428688952

Epoch: 6| Step: 2
Training loss: 1.9614745378494263
Validation loss: 2.378783748995873

Epoch: 6| Step: 3
Training loss: 2.5059940814971924
Validation loss: 2.3832197163694646

Epoch: 6| Step: 4
Training loss: 3.7153093814849854
Validation loss: 2.3808008060660413

Epoch: 6| Step: 5
Training loss: 2.378720283508301
Validation loss: 2.384273757216751

Epoch: 6| Step: 6
Training loss: 2.483340263366699
Validation loss: 2.383066859296573

Epoch: 6| Step: 7
Training loss: 2.3705012798309326
Validation loss: 2.384836273808633

Epoch: 6| Step: 8
Training loss: 2.3203377723693848
Validation loss: 2.3909101076023553

Epoch: 6| Step: 9
Training loss: 3.5873656272888184
Validation loss: 2.3833886372145785

Epoch: 6| Step: 10
Training loss: 2.993884325027466
Validation loss: 2.388071080689789

Epoch: 6| Step: 11
Training loss: 2.464261054992676
Validation loss: 2.3862012483740367

Epoch: 6| Step: 12
Training loss: 2.8431200981140137
Validation loss: 2.394436743951613

Epoch: 6| Step: 13
Training loss: 2.22944974899292
Validation loss: 2.3984661640659457

Epoch: 170| Step: 0
Training loss: 2.7625694274902344
Validation loss: 2.401718279366852

Epoch: 6| Step: 1
Training loss: 2.855271100997925
Validation loss: 2.404721716398834

Epoch: 6| Step: 2
Training loss: 2.778742551803589
Validation loss: 2.4090151479167323

Epoch: 6| Step: 3
Training loss: 2.7978692054748535
Validation loss: 2.411913758964949

Epoch: 6| Step: 4
Training loss: 2.4371695518493652
Validation loss: 2.4134046569947274

Epoch: 6| Step: 5
Training loss: 2.4510385990142822
Validation loss: 2.4114644963254213

Epoch: 6| Step: 6
Training loss: 3.0070605278015137
Validation loss: 2.3991777255970943

Epoch: 6| Step: 7
Training loss: 2.7867681980133057
Validation loss: 2.3949677328909598

Epoch: 6| Step: 8
Training loss: 2.356656789779663
Validation loss: 2.3847344357480287

Epoch: 6| Step: 9
Training loss: 2.455153226852417
Validation loss: 2.3929948396580194

Epoch: 6| Step: 10
Training loss: 2.7240662574768066
Validation loss: 2.379388463112616

Epoch: 6| Step: 11
Training loss: 2.7114574909210205
Validation loss: 2.3814967319529545

Epoch: 6| Step: 12
Training loss: 2.020007848739624
Validation loss: 2.3794194831643054

Epoch: 6| Step: 13
Training loss: 2.1490015983581543
Validation loss: 2.3779018668718237

Epoch: 171| Step: 0
Training loss: 2.870029926300049
Validation loss: 2.37222719705233

Epoch: 6| Step: 1
Training loss: 2.3576135635375977
Validation loss: 2.375675091179468

Epoch: 6| Step: 2
Training loss: 2.584979295730591
Validation loss: 2.3744591256623626

Epoch: 6| Step: 3
Training loss: 2.8820486068725586
Validation loss: 2.3729604598014586

Epoch: 6| Step: 4
Training loss: 2.712747573852539
Validation loss: 2.369054807129727

Epoch: 6| Step: 5
Training loss: 3.011258602142334
Validation loss: 2.367225126553607

Epoch: 6| Step: 6
Training loss: 3.1855239868164062
Validation loss: 2.376998314293482

Epoch: 6| Step: 7
Training loss: 1.797135829925537
Validation loss: 2.37748210917237

Epoch: 6| Step: 8
Training loss: 2.086756467819214
Validation loss: 2.372325207597466

Epoch: 6| Step: 9
Training loss: 2.318507432937622
Validation loss: 2.3781095166360178

Epoch: 6| Step: 10
Training loss: 3.0425496101379395
Validation loss: 2.3775560035500476

Epoch: 6| Step: 11
Training loss: 2.575650453567505
Validation loss: 2.3841418476514917

Epoch: 6| Step: 12
Training loss: 2.409747838973999
Validation loss: 2.386384230788036

Epoch: 6| Step: 13
Training loss: 2.7379462718963623
Validation loss: 2.389266728073038

Epoch: 172| Step: 0
Training loss: 2.279353141784668
Validation loss: 2.4015025092709448

Epoch: 6| Step: 1
Training loss: 3.165541648864746
Validation loss: 2.4061244828726656

Epoch: 6| Step: 2
Training loss: 2.5123753547668457
Validation loss: 2.40775328810497

Epoch: 6| Step: 3
Training loss: 2.235994577407837
Validation loss: 2.4155213038126626

Epoch: 6| Step: 4
Training loss: 2.434537410736084
Validation loss: 2.4174568986379974

Epoch: 6| Step: 5
Training loss: 3.214054584503174
Validation loss: 2.4138242506211802

Epoch: 6| Step: 6
Training loss: 2.5122792720794678
Validation loss: 2.413917297958046

Epoch: 6| Step: 7
Training loss: 1.8900859355926514
Validation loss: 2.4218532962183796

Epoch: 6| Step: 8
Training loss: 3.291686534881592
Validation loss: 2.4197837716789654

Epoch: 6| Step: 9
Training loss: 3.24820613861084
Validation loss: 2.4165182293102307

Epoch: 6| Step: 10
Training loss: 2.793926239013672
Validation loss: 2.406077333675918

Epoch: 6| Step: 11
Training loss: 3.1205897331237793
Validation loss: 2.402952271123086

Epoch: 6| Step: 12
Training loss: 1.9100697040557861
Validation loss: 2.395270586013794

Epoch: 6| Step: 13
Training loss: 1.1360018253326416
Validation loss: 2.3878876470750376

Epoch: 173| Step: 0
Training loss: 2.2147536277770996
Validation loss: 2.387058542620751

Epoch: 6| Step: 1
Training loss: 2.3771591186523438
Validation loss: 2.382404086410358

Epoch: 6| Step: 2
Training loss: 1.9688210487365723
Validation loss: 2.384978207208777

Epoch: 6| Step: 3
Training loss: 2.3594064712524414
Validation loss: 2.3821070873609154

Epoch: 6| Step: 4
Training loss: 3.3133320808410645
Validation loss: 2.385282706188899

Epoch: 6| Step: 5
Training loss: 2.260488986968994
Validation loss: 2.3944438272906887

Epoch: 6| Step: 6
Training loss: 2.4975032806396484
Validation loss: 2.398131098798526

Epoch: 6| Step: 7
Training loss: 3.060218334197998
Validation loss: 2.3995448004814888

Epoch: 6| Step: 8
Training loss: 2.5009093284606934
Validation loss: 2.4092523667120163

Epoch: 6| Step: 9
Training loss: 2.88863468170166
Validation loss: 2.4124039552545034

Epoch: 6| Step: 10
Training loss: 3.31958270072937
Validation loss: 2.4097793896993003

Epoch: 6| Step: 11
Training loss: 2.4838974475860596
Validation loss: 2.397911079468266

Epoch: 6| Step: 12
Training loss: 2.358046531677246
Validation loss: 2.3948544430476364

Epoch: 6| Step: 13
Training loss: 3.100884437561035
Validation loss: 2.3995278112349974

Epoch: 174| Step: 0
Training loss: 2.918738603591919
Validation loss: 2.3903094312196136

Epoch: 6| Step: 1
Training loss: 2.2531166076660156
Validation loss: 2.393134686254686

Epoch: 6| Step: 2
Training loss: 2.6733837127685547
Validation loss: 2.385850880735664

Epoch: 6| Step: 3
Training loss: 2.015659809112549
Validation loss: 2.387232267728416

Epoch: 6| Step: 4
Training loss: 2.9013583660125732
Validation loss: 2.380933064286427

Epoch: 6| Step: 5
Training loss: 3.015148639678955
Validation loss: 2.3886451464827343

Epoch: 6| Step: 6
Training loss: 2.6223018169403076
Validation loss: 2.3910913710953086

Epoch: 6| Step: 7
Training loss: 3.0314388275146484
Validation loss: 2.379820300686744

Epoch: 6| Step: 8
Training loss: 2.0004091262817383
Validation loss: 2.387493292490641

Epoch: 6| Step: 9
Training loss: 2.1598753929138184
Validation loss: 2.390216801756172

Epoch: 6| Step: 10
Training loss: 2.7037899494171143
Validation loss: 2.389067296058901

Epoch: 6| Step: 11
Training loss: 2.815162181854248
Validation loss: 2.3878866370006273

Epoch: 6| Step: 12
Training loss: 2.182462453842163
Validation loss: 2.3846576047200028

Epoch: 6| Step: 13
Training loss: 3.435373067855835
Validation loss: 2.388117685112902

Epoch: 175| Step: 0
Training loss: 2.1317265033721924
Validation loss: 2.3839537136016355

Epoch: 6| Step: 1
Training loss: 2.687352180480957
Validation loss: 2.38343221141446

Epoch: 6| Step: 2
Training loss: 2.8510584831237793
Validation loss: 2.3860460699245496

Epoch: 6| Step: 3
Training loss: 2.3515987396240234
Validation loss: 2.3775828858857513

Epoch: 6| Step: 4
Training loss: 2.964937686920166
Validation loss: 2.3817252805156093

Epoch: 6| Step: 5
Training loss: 3.189857006072998
Validation loss: 2.378852721183531

Epoch: 6| Step: 6
Training loss: 2.6740031242370605
Validation loss: 2.3868955642946306

Epoch: 6| Step: 7
Training loss: 2.8377444744110107
Validation loss: 2.3844811634350846

Epoch: 6| Step: 8
Training loss: 2.124281644821167
Validation loss: 2.3907028475115375

Epoch: 6| Step: 9
Training loss: 2.822359561920166
Validation loss: 2.386238269908454

Epoch: 6| Step: 10
Training loss: 2.3971757888793945
Validation loss: 2.3837315677314677

Epoch: 6| Step: 11
Training loss: 2.02321195602417
Validation loss: 2.379878761947796

Epoch: 6| Step: 12
Training loss: 2.740896463394165
Validation loss: 2.3771135730128132

Epoch: 6| Step: 13
Training loss: 2.4404609203338623
Validation loss: 2.3795147608685236

Epoch: 176| Step: 0
Training loss: 2.2027552127838135
Validation loss: 2.3883791174939883

Epoch: 6| Step: 1
Training loss: 3.2074360847473145
Validation loss: 2.382185161754649

Epoch: 6| Step: 2
Training loss: 2.944873332977295
Validation loss: 2.4017175987202632

Epoch: 6| Step: 3
Training loss: 2.403362274169922
Validation loss: 2.389492183603266

Epoch: 6| Step: 4
Training loss: 2.892975091934204
Validation loss: 2.393605996203679

Epoch: 6| Step: 5
Training loss: 2.594552516937256
Validation loss: 2.4026519047316683

Epoch: 6| Step: 6
Training loss: 2.2047927379608154
Validation loss: 2.401644383707354

Epoch: 6| Step: 7
Training loss: 2.798140048980713
Validation loss: 2.390893923339023

Epoch: 6| Step: 8
Training loss: 2.4603967666625977
Validation loss: 2.3817535625991

Epoch: 6| Step: 9
Training loss: 2.456061840057373
Validation loss: 2.378658402350641

Epoch: 6| Step: 10
Training loss: 2.562993288040161
Validation loss: 2.3682036579296155

Epoch: 6| Step: 11
Training loss: 2.8091821670532227
Validation loss: 2.3732862626352618

Epoch: 6| Step: 12
Training loss: 2.188561201095581
Validation loss: 2.3714913937353317

Epoch: 6| Step: 13
Training loss: 2.73411226272583
Validation loss: 2.375177037331366

Epoch: 177| Step: 0
Training loss: 2.2884232997894287
Validation loss: 2.370880696081346

Epoch: 6| Step: 1
Training loss: 2.288088798522949
Validation loss: 2.36870672369516

Epoch: 6| Step: 2
Training loss: 1.919642686843872
Validation loss: 2.3829946928126837

Epoch: 6| Step: 3
Training loss: 2.3996739387512207
Validation loss: 2.384150343556558

Epoch: 6| Step: 4
Training loss: 2.7616829872131348
Validation loss: 2.3883158647885887

Epoch: 6| Step: 5
Training loss: 2.548834800720215
Validation loss: 2.3809545014494207

Epoch: 6| Step: 6
Training loss: 2.2009623050689697
Validation loss: 2.3827049834753877

Epoch: 6| Step: 7
Training loss: 3.083035707473755
Validation loss: 2.384575110609813

Epoch: 6| Step: 8
Training loss: 3.026937246322632
Validation loss: 2.393876706400225

Epoch: 6| Step: 9
Training loss: 2.927633047103882
Validation loss: 2.3908582951432917

Epoch: 6| Step: 10
Training loss: 2.0998406410217285
Validation loss: 2.3914294601768575

Epoch: 6| Step: 11
Training loss: 2.3254737854003906
Validation loss: 2.381452609133977

Epoch: 6| Step: 12
Training loss: 3.703679323196411
Validation loss: 2.387091782785231

Epoch: 6| Step: 13
Training loss: 2.80008602142334
Validation loss: 2.3720372107721146

Epoch: 178| Step: 0
Training loss: 2.518949031829834
Validation loss: 2.3695053259531655

Epoch: 6| Step: 1
Training loss: 2.7571163177490234
Validation loss: 2.371640302801645

Epoch: 6| Step: 2
Training loss: 2.6217281818389893
Validation loss: 2.3673537085133214

Epoch: 6| Step: 3
Training loss: 2.4993953704833984
Validation loss: 2.370099183051817

Epoch: 6| Step: 4
Training loss: 3.360112190246582
Validation loss: 2.3643056782343055

Epoch: 6| Step: 5
Training loss: 1.9389275312423706
Validation loss: 2.3669279262583744

Epoch: 6| Step: 6
Training loss: 2.775601387023926
Validation loss: 2.367747865697389

Epoch: 6| Step: 7
Training loss: 2.6323750019073486
Validation loss: 2.3697318133487495

Epoch: 6| Step: 8
Training loss: 2.133251428604126
Validation loss: 2.3703875310959353

Epoch: 6| Step: 9
Training loss: 2.6601128578186035
Validation loss: 2.373050437178663

Epoch: 6| Step: 10
Training loss: 2.465243339538574
Validation loss: 2.3679399413447224

Epoch: 6| Step: 11
Training loss: 3.1114649772644043
Validation loss: 2.3751607300132833

Epoch: 6| Step: 12
Training loss: 2.735875368118286
Validation loss: 2.3728675406466246

Epoch: 6| Step: 13
Training loss: 1.5746225118637085
Validation loss: 2.3734876699345087

Epoch: 179| Step: 0
Training loss: 2.8980579376220703
Validation loss: 2.3824594584844445

Epoch: 6| Step: 1
Training loss: 2.2135047912597656
Validation loss: 2.378704842700753

Epoch: 6| Step: 2
Training loss: 2.685351848602295
Validation loss: 2.3761269302778345

Epoch: 6| Step: 3
Training loss: 2.027362823486328
Validation loss: 2.3735809172353437

Epoch: 6| Step: 4
Training loss: 3.262423038482666
Validation loss: 2.3763085590895785

Epoch: 6| Step: 5
Training loss: 2.726991653442383
Validation loss: 2.377649535414993

Epoch: 6| Step: 6
Training loss: 2.7301185131073
Validation loss: 2.3760354647072415

Epoch: 6| Step: 7
Training loss: 1.6939910650253296
Validation loss: 2.382965305800079

Epoch: 6| Step: 8
Training loss: 3.5158002376556396
Validation loss: 2.376102278309484

Epoch: 6| Step: 9
Training loss: 2.6454148292541504
Validation loss: 2.3824324595030917

Epoch: 6| Step: 10
Training loss: 2.7606072425842285
Validation loss: 2.3845700781832457

Epoch: 6| Step: 11
Training loss: 2.5291645526885986
Validation loss: 2.382268487766225

Epoch: 6| Step: 12
Training loss: 1.938392162322998
Validation loss: 2.3855858259303595

Epoch: 6| Step: 13
Training loss: 2.756230115890503
Validation loss: 2.398537681948754

Epoch: 180| Step: 0
Training loss: 2.6311845779418945
Validation loss: 2.3969578076434392

Epoch: 6| Step: 1
Training loss: 2.0567989349365234
Validation loss: 2.4140054128503285

Epoch: 6| Step: 2
Training loss: 2.4459357261657715
Validation loss: 2.409691241479689

Epoch: 6| Step: 3
Training loss: 2.2175049781799316
Validation loss: 2.3952707295776694

Epoch: 6| Step: 4
Training loss: 2.675820827484131
Validation loss: 2.389255316026749

Epoch: 6| Step: 5
Training loss: 2.7290022373199463
Validation loss: 2.3800330867049513

Epoch: 6| Step: 6
Training loss: 2.236593246459961
Validation loss: 2.379562090801936

Epoch: 6| Step: 7
Training loss: 2.4552512168884277
Validation loss: 2.3811514813412904

Epoch: 6| Step: 8
Training loss: 3.0158770084381104
Validation loss: 2.3803633592462026

Epoch: 6| Step: 9
Training loss: 2.9201769828796387
Validation loss: 2.3783435231895855

Epoch: 6| Step: 10
Training loss: 3.535165309906006
Validation loss: 2.3702959424705914

Epoch: 6| Step: 11
Training loss: 2.738764762878418
Validation loss: 2.3686872092626428

Epoch: 6| Step: 12
Training loss: 2.5034749507904053
Validation loss: 2.3629396192489134

Epoch: 6| Step: 13
Training loss: 1.9545637369155884
Validation loss: 2.374833478722521

Epoch: 181| Step: 0
Training loss: 2.700809955596924
Validation loss: 2.3648387257770827

Epoch: 6| Step: 1
Training loss: 2.4238719940185547
Validation loss: 2.3791439302505983

Epoch: 6| Step: 2
Training loss: 3.0483040809631348
Validation loss: 2.3749719127531974

Epoch: 6| Step: 3
Training loss: 2.898672342300415
Validation loss: 2.376590189113412

Epoch: 6| Step: 4
Training loss: 2.275296688079834
Validation loss: 2.3816798707490325

Epoch: 6| Step: 5
Training loss: 2.667262077331543
Validation loss: 2.389080862845144

Epoch: 6| Step: 6
Training loss: 2.297650098800659
Validation loss: 2.3906981765582995

Epoch: 6| Step: 7
Training loss: 3.1772279739379883
Validation loss: 2.386825542296133

Epoch: 6| Step: 8
Training loss: 2.6277713775634766
Validation loss: 2.381729151612969

Epoch: 6| Step: 9
Training loss: 2.676205635070801
Validation loss: 2.379288827219317

Epoch: 6| Step: 10
Training loss: 2.0410866737365723
Validation loss: 2.366319519217296

Epoch: 6| Step: 11
Training loss: 2.732747793197632
Validation loss: 2.36332953873501

Epoch: 6| Step: 12
Training loss: 2.3165459632873535
Validation loss: 2.35942905692644

Epoch: 6| Step: 13
Training loss: 2.4404959678649902
Validation loss: 2.3621479836843347

Epoch: 182| Step: 0
Training loss: 2.184574842453003
Validation loss: 2.350578790069908

Epoch: 6| Step: 1
Training loss: 2.814624309539795
Validation loss: 2.35859158731276

Epoch: 6| Step: 2
Training loss: 2.3454463481903076
Validation loss: 2.3551879249593264

Epoch: 6| Step: 3
Training loss: 2.8498120307922363
Validation loss: 2.351539288797686

Epoch: 6| Step: 4
Training loss: 2.611300230026245
Validation loss: 2.357732190880724

Epoch: 6| Step: 5
Training loss: 1.893379807472229
Validation loss: 2.35956839079498

Epoch: 6| Step: 6
Training loss: 2.8510031700134277
Validation loss: 2.3553268473635436

Epoch: 6| Step: 7
Training loss: 1.9643551111221313
Validation loss: 2.363030961764756

Epoch: 6| Step: 8
Training loss: 3.2062039375305176
Validation loss: 2.357612468863046

Epoch: 6| Step: 9
Training loss: 2.5752084255218506
Validation loss: 2.360632904114262

Epoch: 6| Step: 10
Training loss: 2.6491012573242188
Validation loss: 2.357894177077919

Epoch: 6| Step: 11
Training loss: 2.771566390991211
Validation loss: 2.364637069804694

Epoch: 6| Step: 12
Training loss: 2.9256558418273926
Validation loss: 2.364551777480751

Epoch: 6| Step: 13
Training loss: 3.0568313598632812
Validation loss: 2.367682231369839

Epoch: 183| Step: 0
Training loss: 2.4296765327453613
Validation loss: 2.3717267718366397

Epoch: 6| Step: 1
Training loss: 2.7162880897521973
Validation loss: 2.3851007312856694

Epoch: 6| Step: 2
Training loss: 2.0738091468811035
Validation loss: 2.4010955492655435

Epoch: 6| Step: 3
Training loss: 2.771184206008911
Validation loss: 2.400463345230267

Epoch: 6| Step: 4
Training loss: 3.0187289714813232
Validation loss: 2.4188665574596775

Epoch: 6| Step: 5
Training loss: 2.554882287979126
Validation loss: 2.424800019110403

Epoch: 6| Step: 6
Training loss: 2.478377342224121
Validation loss: 2.4038554878645044

Epoch: 6| Step: 7
Training loss: 2.900496482849121
Validation loss: 2.411882021093881

Epoch: 6| Step: 8
Training loss: 2.32794451713562
Validation loss: 2.4128918878493772

Epoch: 6| Step: 9
Training loss: 2.69990873336792
Validation loss: 2.398185481307327

Epoch: 6| Step: 10
Training loss: 2.8568196296691895
Validation loss: 2.378262145544893

Epoch: 6| Step: 11
Training loss: 2.3366382122039795
Validation loss: 2.37430969361336

Epoch: 6| Step: 12
Training loss: 2.8841452598571777
Validation loss: 2.3682350958547285

Epoch: 6| Step: 13
Training loss: 2.379983901977539
Validation loss: 2.3578630621715257

Epoch: 184| Step: 0
Training loss: 2.2962188720703125
Validation loss: 2.3559863798079954

Epoch: 6| Step: 1
Training loss: 2.254272937774658
Validation loss: 2.3592938402647614

Epoch: 6| Step: 2
Training loss: 2.977332592010498
Validation loss: 2.356870005207677

Epoch: 6| Step: 3
Training loss: 3.1323890686035156
Validation loss: 2.3545663126053347

Epoch: 6| Step: 4
Training loss: 2.552743434906006
Validation loss: 2.3567188709012923

Epoch: 6| Step: 5
Training loss: 1.8662867546081543
Validation loss: 2.3519766099991335

Epoch: 6| Step: 6
Training loss: 2.1646156311035156
Validation loss: 2.3559544599184425

Epoch: 6| Step: 7
Training loss: 2.911198377609253
Validation loss: 2.3546505410184144

Epoch: 6| Step: 8
Training loss: 2.3226051330566406
Validation loss: 2.353753533414615

Epoch: 6| Step: 9
Training loss: 3.0384252071380615
Validation loss: 2.357425012896138

Epoch: 6| Step: 10
Training loss: 2.6778736114501953
Validation loss: 2.355948337944605

Epoch: 6| Step: 11
Training loss: 2.6537868976593018
Validation loss: 2.3588679708460325

Epoch: 6| Step: 12
Training loss: 2.929765224456787
Validation loss: 2.359041690826416

Epoch: 6| Step: 13
Training loss: 2.451026439666748
Validation loss: 2.361875528930336

Epoch: 185| Step: 0
Training loss: 2.6169626712799072
Validation loss: 2.3636014717881397

Epoch: 6| Step: 1
Training loss: 3.0649709701538086
Validation loss: 2.3654887266056512

Epoch: 6| Step: 2
Training loss: 2.336162805557251
Validation loss: 2.3751852050904305

Epoch: 6| Step: 3
Training loss: 2.704037666320801
Validation loss: 2.365798801504156

Epoch: 6| Step: 4
Training loss: 2.2676966190338135
Validation loss: 2.360674904238793

Epoch: 6| Step: 5
Training loss: 2.005876064300537
Validation loss: 2.3690321342919463

Epoch: 6| Step: 6
Training loss: 3.087075710296631
Validation loss: 2.360329863845661

Epoch: 6| Step: 7
Training loss: 2.468693971633911
Validation loss: 2.3642250363544752

Epoch: 6| Step: 8
Training loss: 2.1488544940948486
Validation loss: 2.3680510213298183

Epoch: 6| Step: 9
Training loss: 2.7453203201293945
Validation loss: 2.3735833783303537

Epoch: 6| Step: 10
Training loss: 2.645357370376587
Validation loss: 2.3763608112130115

Epoch: 6| Step: 11
Training loss: 2.834628105163574
Validation loss: 2.3713290153011197

Epoch: 6| Step: 12
Training loss: 2.194277763366699
Validation loss: 2.3709889637526644

Epoch: 6| Step: 13
Training loss: 3.536477565765381
Validation loss: 2.3757505852689027

Epoch: 186| Step: 0
Training loss: 2.2926156520843506
Validation loss: 2.3681943288413425

Epoch: 6| Step: 1
Training loss: 2.649237632751465
Validation loss: 2.371041723476943

Epoch: 6| Step: 2
Training loss: 2.880774974822998
Validation loss: 2.368861262516309

Epoch: 6| Step: 3
Training loss: 2.6414403915405273
Validation loss: 2.362501859664917

Epoch: 6| Step: 4
Training loss: 2.859389305114746
Validation loss: 2.3628271113159838

Epoch: 6| Step: 5
Training loss: 1.8707091808319092
Validation loss: 2.3612020425899054

Epoch: 6| Step: 6
Training loss: 2.328277111053467
Validation loss: 2.361227745650917

Epoch: 6| Step: 7
Training loss: 2.6218202114105225
Validation loss: 2.360923372289186

Epoch: 6| Step: 8
Training loss: 2.544252872467041
Validation loss: 2.3591177694259153

Epoch: 6| Step: 9
Training loss: 3.048825740814209
Validation loss: 2.3567468761115946

Epoch: 6| Step: 10
Training loss: 2.2920382022857666
Validation loss: 2.3585216947781142

Epoch: 6| Step: 11
Training loss: 2.5466997623443604
Validation loss: 2.3694562527441208

Epoch: 6| Step: 12
Training loss: 2.66373348236084
Validation loss: 2.369092397792365

Epoch: 6| Step: 13
Training loss: 3.153425455093384
Validation loss: 2.3702937403032855

Epoch: 187| Step: 0
Training loss: 2.5369701385498047
Validation loss: 2.367987955770185

Epoch: 6| Step: 1
Training loss: 2.260340690612793
Validation loss: 2.3721922597577496

Epoch: 6| Step: 2
Training loss: 2.4161219596862793
Validation loss: 2.3733488282849713

Epoch: 6| Step: 3
Training loss: 3.168008327484131
Validation loss: 2.3685959692924254

Epoch: 6| Step: 4
Training loss: 2.3645215034484863
Validation loss: 2.3750867741082304

Epoch: 6| Step: 5
Training loss: 2.8940792083740234
Validation loss: 2.3785121748524327

Epoch: 6| Step: 6
Training loss: 3.254617691040039
Validation loss: 2.3768889263112056

Epoch: 6| Step: 7
Training loss: 2.919374465942383
Validation loss: 2.3706269277039396

Epoch: 6| Step: 8
Training loss: 2.315986394882202
Validation loss: 2.371710149190759

Epoch: 6| Step: 9
Training loss: 2.209012269973755
Validation loss: 2.3642586328650035

Epoch: 6| Step: 10
Training loss: 2.236912250518799
Validation loss: 2.3672808498464604

Epoch: 6| Step: 11
Training loss: 2.2527647018432617
Validation loss: 2.3585178441898798

Epoch: 6| Step: 12
Training loss: 2.313419818878174
Validation loss: 2.370573389914728

Epoch: 6| Step: 13
Training loss: 3.2243435382843018
Validation loss: 2.361932034133583

Epoch: 188| Step: 0
Training loss: 2.346770763397217
Validation loss: 2.3604144998776015

Epoch: 6| Step: 1
Training loss: 2.8252053260803223
Validation loss: 2.358901910884406

Epoch: 6| Step: 2
Training loss: 3.124363422393799
Validation loss: 2.3577813717626754

Epoch: 6| Step: 3
Training loss: 3.126303195953369
Validation loss: 2.36121396095522

Epoch: 6| Step: 4
Training loss: 1.8959711790084839
Validation loss: 2.3608054807109218

Epoch: 6| Step: 5
Training loss: 3.1996188163757324
Validation loss: 2.368767576832925

Epoch: 6| Step: 6
Training loss: 2.6798014640808105
Validation loss: 2.3626163428829563

Epoch: 6| Step: 7
Training loss: 2.014258623123169
Validation loss: 2.363985207773024

Epoch: 6| Step: 8
Training loss: 2.3590221405029297
Validation loss: 2.3607820951810448

Epoch: 6| Step: 9
Training loss: 2.6046669483184814
Validation loss: 2.366462815192438

Epoch: 6| Step: 10
Training loss: 2.4117250442504883
Validation loss: 2.368745873051305

Epoch: 6| Step: 11
Training loss: 2.519756555557251
Validation loss: 2.363660027903895

Epoch: 6| Step: 12
Training loss: 2.3682615756988525
Validation loss: 2.355417554096509

Epoch: 6| Step: 13
Training loss: 2.7363815307617188
Validation loss: 2.354742165534727

Epoch: 189| Step: 0
Training loss: 2.4369802474975586
Validation loss: 2.3558266957600913

Epoch: 6| Step: 1
Training loss: 2.8013992309570312
Validation loss: 2.3505042342729467

Epoch: 6| Step: 2
Training loss: 3.277967929840088
Validation loss: 2.3506072234081965

Epoch: 6| Step: 3
Training loss: 3.032118797302246
Validation loss: 2.3471265326264086

Epoch: 6| Step: 4
Training loss: 1.472001552581787
Validation loss: 2.351687213425995

Epoch: 6| Step: 5
Training loss: 2.527554988861084
Validation loss: 2.349592767735963

Epoch: 6| Step: 6
Training loss: 2.4079785346984863
Validation loss: 2.355887864225654

Epoch: 6| Step: 7
Training loss: 2.6688265800476074
Validation loss: 2.366352870900144

Epoch: 6| Step: 8
Training loss: 2.7472496032714844
Validation loss: 2.3675538365558912

Epoch: 6| Step: 9
Training loss: 2.0134687423706055
Validation loss: 2.3558609793263097

Epoch: 6| Step: 10
Training loss: 2.7221953868865967
Validation loss: 2.3560765097218175

Epoch: 6| Step: 11
Training loss: 2.726984977722168
Validation loss: 2.3430677716450026

Epoch: 6| Step: 12
Training loss: 2.179647207260132
Validation loss: 2.3481919816745225

Epoch: 6| Step: 13
Training loss: 3.4737608432769775
Validation loss: 2.34610294013895

Epoch: 190| Step: 0
Training loss: 1.9036996364593506
Validation loss: 2.34878199074858

Epoch: 6| Step: 1
Training loss: 2.173506736755371
Validation loss: 2.3458911577860513

Epoch: 6| Step: 2
Training loss: 2.7624106407165527
Validation loss: 2.352182875397385

Epoch: 6| Step: 3
Training loss: 2.6987667083740234
Validation loss: 2.3570843588921333

Epoch: 6| Step: 4
Training loss: 2.381033420562744
Validation loss: 2.3537743629947787

Epoch: 6| Step: 5
Training loss: 2.7515597343444824
Validation loss: 2.3539794875729467

Epoch: 6| Step: 6
Training loss: 2.7946033477783203
Validation loss: 2.359757149091331

Epoch: 6| Step: 7
Training loss: 2.3868372440338135
Validation loss: 2.354333762199648

Epoch: 6| Step: 8
Training loss: 2.53726863861084
Validation loss: 2.3678366573907996

Epoch: 6| Step: 9
Training loss: 2.155041217803955
Validation loss: 2.3666796607355916

Epoch: 6| Step: 10
Training loss: 2.852890968322754
Validation loss: 2.374215410601708

Epoch: 6| Step: 11
Training loss: 2.3021557331085205
Validation loss: 2.383444934762934

Epoch: 6| Step: 12
Training loss: 3.057130813598633
Validation loss: 2.373212295193826

Epoch: 6| Step: 13
Training loss: 4.068752765655518
Validation loss: 2.360840551314815

Epoch: 191| Step: 0
Training loss: 2.564387321472168
Validation loss: 2.356905503939557

Epoch: 6| Step: 1
Training loss: 2.2332100868225098
Validation loss: 2.3500725364172332

Epoch: 6| Step: 2
Training loss: 2.974684715270996
Validation loss: 2.3451555569966636

Epoch: 6| Step: 3
Training loss: 2.5281496047973633
Validation loss: 2.353338715850666

Epoch: 6| Step: 4
Training loss: 2.8128135204315186
Validation loss: 2.3423965541265344

Epoch: 6| Step: 5
Training loss: 1.379560947418213
Validation loss: 2.342609538826891

Epoch: 6| Step: 6
Training loss: 2.7370150089263916
Validation loss: 2.345922189374124

Epoch: 6| Step: 7
Training loss: 2.4502112865448
Validation loss: 2.3434004757993963

Epoch: 6| Step: 8
Training loss: 2.878371477127075
Validation loss: 2.342860262881043

Epoch: 6| Step: 9
Training loss: 2.44142746925354
Validation loss: 2.344991945451306

Epoch: 6| Step: 10
Training loss: 3.170870780944824
Validation loss: 2.341434976106049

Epoch: 6| Step: 11
Training loss: 2.3765501976013184
Validation loss: 2.3418212757315686

Epoch: 6| Step: 12
Training loss: 2.4999825954437256
Validation loss: 2.3420215370834514

Epoch: 6| Step: 13
Training loss: 3.3756327629089355
Validation loss: 2.3379626902200843

Epoch: 192| Step: 0
Training loss: 2.7189579010009766
Validation loss: 2.339053689792592

Epoch: 6| Step: 1
Training loss: 1.7762901782989502
Validation loss: 2.345756087251889

Epoch: 6| Step: 2
Training loss: 2.7114696502685547
Validation loss: 2.3562498477197464

Epoch: 6| Step: 3
Training loss: 2.338550329208374
Validation loss: 2.3456141999972764

Epoch: 6| Step: 4
Training loss: 2.5593767166137695
Validation loss: 2.342224113402828

Epoch: 6| Step: 5
Training loss: 3.223715305328369
Validation loss: 2.3511986347936813

Epoch: 6| Step: 6
Training loss: 3.4456701278686523
Validation loss: 2.3443501995455835

Epoch: 6| Step: 7
Training loss: 1.808592677116394
Validation loss: 2.339859449735252

Epoch: 6| Step: 8
Training loss: 2.3882038593292236
Validation loss: 2.344796126888644

Epoch: 6| Step: 9
Training loss: 2.5699214935302734
Validation loss: 2.3408400858602216

Epoch: 6| Step: 10
Training loss: 2.4630846977233887
Validation loss: 2.3520519246337233

Epoch: 6| Step: 11
Training loss: 2.9074130058288574
Validation loss: 2.362023371522145

Epoch: 6| Step: 12
Training loss: 2.8901283740997314
Validation loss: 2.3665573673863567

Epoch: 6| Step: 13
Training loss: 1.725950837135315
Validation loss: 2.367279819262925

Epoch: 193| Step: 0
Training loss: 2.0673558712005615
Validation loss: 2.375313138449064

Epoch: 6| Step: 1
Training loss: 2.4683854579925537
Validation loss: 2.372789195788804

Epoch: 6| Step: 2
Training loss: 2.82084059715271
Validation loss: 2.3774211047798075

Epoch: 6| Step: 3
Training loss: 2.503061294555664
Validation loss: 2.3683933647730018

Epoch: 6| Step: 4
Training loss: 2.420180320739746
Validation loss: 2.373571652238087

Epoch: 6| Step: 5
Training loss: 2.8123135566711426
Validation loss: 2.3809819067678144

Epoch: 6| Step: 6
Training loss: 2.005967378616333
Validation loss: 2.369347628726754

Epoch: 6| Step: 7
Training loss: 2.2656002044677734
Validation loss: 2.373388995406448

Epoch: 6| Step: 8
Training loss: 2.8944356441497803
Validation loss: 2.3724126405613397

Epoch: 6| Step: 9
Training loss: 3.1369102001190186
Validation loss: 2.366621853202902

Epoch: 6| Step: 10
Training loss: 3.3009331226348877
Validation loss: 2.3526052121193177

Epoch: 6| Step: 11
Training loss: 2.1095457077026367
Validation loss: 2.3505945949144262

Epoch: 6| Step: 12
Training loss: 2.2761330604553223
Validation loss: 2.3529286846037833

Epoch: 6| Step: 13
Training loss: 3.187103509902954
Validation loss: 2.343894830314062

Epoch: 194| Step: 0
Training loss: 2.433638572692871
Validation loss: 2.344338542671614

Epoch: 6| Step: 1
Training loss: 2.3929338455200195
Validation loss: 2.350884968234647

Epoch: 6| Step: 2
Training loss: 2.996255874633789
Validation loss: 2.3477365534792662

Epoch: 6| Step: 3
Training loss: 3.4835853576660156
Validation loss: 2.3536807926752235

Epoch: 6| Step: 4
Training loss: 2.2783191204071045
Validation loss: 2.354797347899406

Epoch: 6| Step: 5
Training loss: 2.0061392784118652
Validation loss: 2.3401341028110956

Epoch: 6| Step: 6
Training loss: 2.3170783519744873
Validation loss: 2.346739613881675

Epoch: 6| Step: 7
Training loss: 2.86698055267334
Validation loss: 2.3394945283089914

Epoch: 6| Step: 8
Training loss: 2.568838357925415
Validation loss: 2.3545253738280265

Epoch: 6| Step: 9
Training loss: 2.8288042545318604
Validation loss: 2.3630063879874443

Epoch: 6| Step: 10
Training loss: 3.0915043354034424
Validation loss: 2.3575409945621284

Epoch: 6| Step: 11
Training loss: 2.2549679279327393
Validation loss: 2.3566798625453824

Epoch: 6| Step: 12
Training loss: 2.4320173263549805
Validation loss: 2.3515278344513266

Epoch: 6| Step: 13
Training loss: 1.6601532697677612
Validation loss: 2.3609694998751403

Epoch: 195| Step: 0
Training loss: 1.994940996170044
Validation loss: 2.348955944020261

Epoch: 6| Step: 1
Training loss: 2.568758010864258
Validation loss: 2.348280434967369

Epoch: 6| Step: 2
Training loss: 2.435213565826416
Validation loss: 2.3502700303190496

Epoch: 6| Step: 3
Training loss: 2.9588639736175537
Validation loss: 2.344067931175232

Epoch: 6| Step: 4
Training loss: 3.3315682411193848
Validation loss: 2.34360679247046

Epoch: 6| Step: 5
Training loss: 2.912308692932129
Validation loss: 2.3361152833507908

Epoch: 6| Step: 6
Training loss: 2.6181018352508545
Validation loss: 2.3394965407668904

Epoch: 6| Step: 7
Training loss: 2.848574638366699
Validation loss: 2.3442947479986374

Epoch: 6| Step: 8
Training loss: 1.9105939865112305
Validation loss: 2.343494407592281

Epoch: 6| Step: 9
Training loss: 2.609109401702881
Validation loss: 2.338180649665094

Epoch: 6| Step: 10
Training loss: 2.390256881713867
Validation loss: 2.3428952514484362

Epoch: 6| Step: 11
Training loss: 2.350606918334961
Validation loss: 2.345159225566413

Epoch: 6| Step: 12
Training loss: 2.7513980865478516
Validation loss: 2.3480665183836416

Epoch: 6| Step: 13
Training loss: 2.1373608112335205
Validation loss: 2.3443879235175347

Epoch: 196| Step: 0
Training loss: 2.4405670166015625
Validation loss: 2.3503550226970384

Epoch: 6| Step: 1
Training loss: 2.5979721546173096
Validation loss: 2.348710342120099

Epoch: 6| Step: 2
Training loss: 2.524646759033203
Validation loss: 2.361177957186135

Epoch: 6| Step: 3
Training loss: 3.0086684226989746
Validation loss: 2.3535496624567176

Epoch: 6| Step: 4
Training loss: 2.262547492980957
Validation loss: 2.3480170542193997

Epoch: 6| Step: 5
Training loss: 2.293850898742676
Validation loss: 2.349805910100219

Epoch: 6| Step: 6
Training loss: 2.016155242919922
Validation loss: 2.3456289486218522

Epoch: 6| Step: 7
Training loss: 2.4376866817474365
Validation loss: 2.341540618609357

Epoch: 6| Step: 8
Training loss: 1.742023229598999
Validation loss: 2.3318368234942035

Epoch: 6| Step: 9
Training loss: 2.248997211456299
Validation loss: 2.345381215054502

Epoch: 6| Step: 10
Training loss: 3.135607957839966
Validation loss: 2.3465439529829126

Epoch: 6| Step: 11
Training loss: 3.80771803855896
Validation loss: 2.3399118351679977

Epoch: 6| Step: 12
Training loss: 3.2213759422302246
Validation loss: 2.340909122138895

Epoch: 6| Step: 13
Training loss: 2.0788326263427734
Validation loss: 2.347377984754501

Epoch: 197| Step: 0
Training loss: 1.7157492637634277
Validation loss: 2.346286740354312

Epoch: 6| Step: 1
Training loss: 3.0586225986480713
Validation loss: 2.3388965565671205

Epoch: 6| Step: 2
Training loss: 2.84551739692688
Validation loss: 2.3342382997594853

Epoch: 6| Step: 3
Training loss: 3.0198230743408203
Validation loss: 2.335409733556932

Epoch: 6| Step: 4
Training loss: 2.274538040161133
Validation loss: 2.3356907829161613

Epoch: 6| Step: 5
Training loss: 2.9635605812072754
Validation loss: 2.3378776657965874

Epoch: 6| Step: 6
Training loss: 2.8366763591766357
Validation loss: 2.328014410952086

Epoch: 6| Step: 7
Training loss: 1.9252129793167114
Validation loss: 2.336307066743092

Epoch: 6| Step: 8
Training loss: 2.1768276691436768
Validation loss: 2.3341546827746975

Epoch: 6| Step: 9
Training loss: 2.966087579727173
Validation loss: 2.3340441770451044

Epoch: 6| Step: 10
Training loss: 2.16788649559021
Validation loss: 2.3370613757000176

Epoch: 6| Step: 11
Training loss: 2.970750331878662
Validation loss: 2.343038382068757

Epoch: 6| Step: 12
Training loss: 2.443133592605591
Validation loss: 2.350516865330358

Epoch: 6| Step: 13
Training loss: 2.6168506145477295
Validation loss: 2.3513385403540825

Epoch: 198| Step: 0
Training loss: 2.2438793182373047
Validation loss: 2.360656604971937

Epoch: 6| Step: 1
Training loss: 2.5116968154907227
Validation loss: 2.3589989023823894

Epoch: 6| Step: 2
Training loss: 2.7996044158935547
Validation loss: 2.376348764665665

Epoch: 6| Step: 3
Training loss: 2.675313711166382
Validation loss: 2.3613780801014235

Epoch: 6| Step: 4
Training loss: 2.135319471359253
Validation loss: 2.359470493050032

Epoch: 6| Step: 5
Training loss: 1.9749835729599
Validation loss: 2.3564361936302594

Epoch: 6| Step: 6
Training loss: 2.582641363143921
Validation loss: 2.3421216293047835

Epoch: 6| Step: 7
Training loss: 2.8645501136779785
Validation loss: 2.348605155944824

Epoch: 6| Step: 8
Training loss: 1.9353058338165283
Validation loss: 2.339513853032102

Epoch: 6| Step: 9
Training loss: 2.9893264770507812
Validation loss: 2.3346949418385825

Epoch: 6| Step: 10
Training loss: 2.4984822273254395
Validation loss: 2.3383499460835613

Epoch: 6| Step: 11
Training loss: 2.5846667289733887
Validation loss: 2.33032670328694

Epoch: 6| Step: 12
Training loss: 3.4958832263946533
Validation loss: 2.3365899773054224

Epoch: 6| Step: 13
Training loss: 2.6381521224975586
Validation loss: 2.334796731190015

Epoch: 199| Step: 0
Training loss: 2.501509666442871
Validation loss: 2.3330150932394047

Epoch: 6| Step: 1
Training loss: 2.821413993835449
Validation loss: 2.3350197730525846

Epoch: 6| Step: 2
Training loss: 2.5070412158966064
Validation loss: 2.3406915433945192

Epoch: 6| Step: 3
Training loss: 1.707312822341919
Validation loss: 2.334917219736243

Epoch: 6| Step: 4
Training loss: 2.32741641998291
Validation loss: 2.3425313490693287

Epoch: 6| Step: 5
Training loss: 3.0617222785949707
Validation loss: 2.3404876750002623

Epoch: 6| Step: 6
Training loss: 2.447848320007324
Validation loss: 2.336227391355781

Epoch: 6| Step: 7
Training loss: 2.6088380813598633
Validation loss: 2.3402135333707257

Epoch: 6| Step: 8
Training loss: 2.5857439041137695
Validation loss: 2.3372004365408294

Epoch: 6| Step: 9
Training loss: 2.5694150924682617
Validation loss: 2.341028815956526

Epoch: 6| Step: 10
Training loss: 2.948584794998169
Validation loss: 2.345000884866202

Epoch: 6| Step: 11
Training loss: 2.649722099304199
Validation loss: 2.3417584332086707

Epoch: 6| Step: 12
Training loss: 2.683748722076416
Validation loss: 2.342165050968047

Epoch: 6| Step: 13
Training loss: 2.43485951423645
Validation loss: 2.349053082927581

Epoch: 200| Step: 0
Training loss: 2.252181053161621
Validation loss: 2.3735486409997426

Epoch: 6| Step: 1
Training loss: 2.8199520111083984
Validation loss: 2.3793196575615996

Epoch: 6| Step: 2
Training loss: 3.027228832244873
Validation loss: 2.411655359370734

Epoch: 6| Step: 3
Training loss: 2.3199315071105957
Validation loss: 2.409534890164611

Epoch: 6| Step: 4
Training loss: 3.2183752059936523
Validation loss: 2.389106040359825

Epoch: 6| Step: 5
Training loss: 3.047576904296875
Validation loss: 2.363896946753225

Epoch: 6| Step: 6
Training loss: 2.142827033996582
Validation loss: 2.3612849456007763

Epoch: 6| Step: 7
Training loss: 2.279397487640381
Validation loss: 2.356923045650605

Epoch: 6| Step: 8
Training loss: 2.4159789085388184
Validation loss: 2.3470015705272718

Epoch: 6| Step: 9
Training loss: 2.405165910720825
Validation loss: 2.348291084330569

Epoch: 6| Step: 10
Training loss: 2.880457878112793
Validation loss: 2.3541068159123903

Epoch: 6| Step: 11
Training loss: 1.8827933073043823
Validation loss: 2.346142663750597

Epoch: 6| Step: 12
Training loss: 2.7524161338806152
Validation loss: 2.3436757877308834

Epoch: 6| Step: 13
Training loss: 2.576223373413086
Validation loss: 2.3346169251267628

Epoch: 201| Step: 0
Training loss: 2.6940255165100098
Validation loss: 2.336173839466546

Epoch: 6| Step: 1
Training loss: 3.222815990447998
Validation loss: 2.3392201905609458

Epoch: 6| Step: 2
Training loss: 2.3765904903411865
Validation loss: 2.337767744577059

Epoch: 6| Step: 3
Training loss: 2.5861592292785645
Validation loss: 2.3355016195645897

Epoch: 6| Step: 4
Training loss: 2.7404122352600098
Validation loss: 2.339430162983556

Epoch: 6| Step: 5
Training loss: 2.174095869064331
Validation loss: 2.336542570462791

Epoch: 6| Step: 6
Training loss: 2.4015510082244873
Validation loss: 2.331405831921485

Epoch: 6| Step: 7
Training loss: 2.173962116241455
Validation loss: 2.3253113044205533

Epoch: 6| Step: 8
Training loss: 2.295776844024658
Validation loss: 2.3268017256131737

Epoch: 6| Step: 9
Training loss: 3.117943048477173
Validation loss: 2.327323398282451

Epoch: 6| Step: 10
Training loss: 3.338296413421631
Validation loss: 2.327948236978182

Epoch: 6| Step: 11
Training loss: 2.284885883331299
Validation loss: 2.3257053564953547

Epoch: 6| Step: 12
Training loss: 2.431173324584961
Validation loss: 2.3290573832809285

Epoch: 6| Step: 13
Training loss: 2.2035417556762695
Validation loss: 2.326045590062295

Epoch: 202| Step: 0
Training loss: 2.717329502105713
Validation loss: 2.335695142387062

Epoch: 6| Step: 1
Training loss: 1.748631238937378
Validation loss: 2.3514016853865756

Epoch: 6| Step: 2
Training loss: 2.1254940032958984
Validation loss: 2.372049157337476

Epoch: 6| Step: 3
Training loss: 2.7095751762390137
Validation loss: 2.3842773309317966

Epoch: 6| Step: 4
Training loss: 3.558307409286499
Validation loss: 2.394925514856974

Epoch: 6| Step: 5
Training loss: 2.9144179821014404
Validation loss: 2.3922459028100453

Epoch: 6| Step: 6
Training loss: 1.9409151077270508
Validation loss: 2.4106750360099216

Epoch: 6| Step: 7
Training loss: 2.9474825859069824
Validation loss: 2.3877493501991354

Epoch: 6| Step: 8
Training loss: 2.1901259422302246
Validation loss: 2.3764883779710337

Epoch: 6| Step: 9
Training loss: 3.594590425491333
Validation loss: 2.3672003861396544

Epoch: 6| Step: 10
Training loss: 2.3594789505004883
Validation loss: 2.356443400024086

Epoch: 6| Step: 11
Training loss: 2.1928868293762207
Validation loss: 2.355577891872775

Epoch: 6| Step: 12
Training loss: 2.1002368927001953
Validation loss: 2.3587078740519862

Epoch: 6| Step: 13
Training loss: 3.1616029739379883
Validation loss: 2.3453610994482554

Epoch: 203| Step: 0
Training loss: 2.5382328033447266
Validation loss: 2.327344955936555

Epoch: 6| Step: 1
Training loss: 2.149251699447632
Validation loss: 2.3317675539242324

Epoch: 6| Step: 2
Training loss: 2.9449496269226074
Validation loss: 2.3248249689737954

Epoch: 6| Step: 3
Training loss: 2.305263042449951
Validation loss: 2.3269121467426257

Epoch: 6| Step: 4
Training loss: 2.6473326683044434
Validation loss: 2.321540045481856

Epoch: 6| Step: 5
Training loss: 1.9003640413284302
Validation loss: 2.3246694354600805

Epoch: 6| Step: 6
Training loss: 2.244861364364624
Validation loss: 2.3296258705918507

Epoch: 6| Step: 7
Training loss: 2.7364954948425293
Validation loss: 2.33095064983573

Epoch: 6| Step: 8
Training loss: 2.746826171875
Validation loss: 2.344972728401102

Epoch: 6| Step: 9
Training loss: 2.783384084701538
Validation loss: 2.3532337322030017

Epoch: 6| Step: 10
Training loss: 3.318477153778076
Validation loss: 2.347311076297555

Epoch: 6| Step: 11
Training loss: 2.54941725730896
Validation loss: 2.3446541319611254

Epoch: 6| Step: 12
Training loss: 2.274733543395996
Validation loss: 2.3508316624549126

Epoch: 6| Step: 13
Training loss: 2.895735740661621
Validation loss: 2.350442555642897

Epoch: 204| Step: 0
Training loss: 2.9118103981018066
Validation loss: 2.3529603635111163

Epoch: 6| Step: 1
Training loss: 1.9969558715820312
Validation loss: 2.3529729945685274

Epoch: 6| Step: 2
Training loss: 2.6895062923431396
Validation loss: 2.369564130742063

Epoch: 6| Step: 3
Training loss: 2.1079800128936768
Validation loss: 2.379829309319937

Epoch: 6| Step: 4
Training loss: 2.6952595710754395
Validation loss: 2.364209190491707

Epoch: 6| Step: 5
Training loss: 3.0129952430725098
Validation loss: 2.359210785999093

Epoch: 6| Step: 6
Training loss: 2.7085554599761963
Validation loss: 2.3533088391827

Epoch: 6| Step: 7
Training loss: 2.745821952819824
Validation loss: 2.342204483606482

Epoch: 6| Step: 8
Training loss: 2.543600082397461
Validation loss: 2.329066394477762

Epoch: 6| Step: 9
Training loss: 2.5716309547424316
Validation loss: 2.3290702681387625

Epoch: 6| Step: 10
Training loss: 2.371521472930908
Validation loss: 2.324087571072322

Epoch: 6| Step: 11
Training loss: 2.7528820037841797
Validation loss: 2.326387387450023

Epoch: 6| Step: 12
Training loss: 2.811953067779541
Validation loss: 2.3197615787547123

Epoch: 6| Step: 13
Training loss: 1.5410242080688477
Validation loss: 2.3276860560140302

Epoch: 205| Step: 0
Training loss: 3.3122315406799316
Validation loss: 2.328542863169024

Epoch: 6| Step: 1
Training loss: 2.0469839572906494
Validation loss: 2.3314136638436267

Epoch: 6| Step: 2
Training loss: 2.258983612060547
Validation loss: 2.3315335242978987

Epoch: 6| Step: 3
Training loss: 2.527578830718994
Validation loss: 2.34307574969466

Epoch: 6| Step: 4
Training loss: 1.7677156925201416
Validation loss: 2.36456964349234

Epoch: 6| Step: 5
Training loss: 2.001830577850342
Validation loss: 2.3709798794920727

Epoch: 6| Step: 6
Training loss: 3.1381847858428955
Validation loss: 2.3842754569104923

Epoch: 6| Step: 7
Training loss: 2.198370933532715
Validation loss: 2.39594405440874

Epoch: 6| Step: 8
Training loss: 2.355921745300293
Validation loss: 2.384655867853472

Epoch: 6| Step: 9
Training loss: 2.988872528076172
Validation loss: 2.390026979548957

Epoch: 6| Step: 10
Training loss: 2.8073925971984863
Validation loss: 2.3798271404799594

Epoch: 6| Step: 11
Training loss: 2.6217730045318604
Validation loss: 2.3627496893687914

Epoch: 6| Step: 12
Training loss: 3.450916290283203
Validation loss: 2.3519018875655306

Epoch: 6| Step: 13
Training loss: 2.3814268112182617
Validation loss: 2.3427402844993015

Epoch: 206| Step: 0
Training loss: 2.752218246459961
Validation loss: 2.3389295070402083

Epoch: 6| Step: 1
Training loss: 2.3474531173706055
Validation loss: 2.3270460995294715

Epoch: 6| Step: 2
Training loss: 2.4920828342437744
Validation loss: 2.327522821323846

Epoch: 6| Step: 3
Training loss: 2.560965061187744
Validation loss: 2.330615128240278

Epoch: 6| Step: 4
Training loss: 2.99070405960083
Validation loss: 2.327160853211598

Epoch: 6| Step: 5
Training loss: 2.279508113861084
Validation loss: 2.3327066052344536

Epoch: 6| Step: 6
Training loss: 2.8921852111816406
Validation loss: 2.337624224283362

Epoch: 6| Step: 7
Training loss: 3.079763889312744
Validation loss: 2.3346016022466842

Epoch: 6| Step: 8
Training loss: 2.412863254547119
Validation loss: 2.332555519637241

Epoch: 6| Step: 9
Training loss: 2.439206600189209
Validation loss: 2.3293995575238298

Epoch: 6| Step: 10
Training loss: 2.687786817550659
Validation loss: 2.332227458236038

Epoch: 6| Step: 11
Training loss: 2.4054651260375977
Validation loss: 2.331452191516917

Epoch: 6| Step: 12
Training loss: 2.365774631500244
Validation loss: 2.3307353194041918

Epoch: 6| Step: 13
Training loss: 1.7489134073257446
Validation loss: 2.3279255718313236

Epoch: 207| Step: 0
Training loss: 3.026822805404663
Validation loss: 2.3291426409957228

Epoch: 6| Step: 1
Training loss: 2.9060192108154297
Validation loss: 2.345528466727144

Epoch: 6| Step: 2
Training loss: 2.513427257537842
Validation loss: 2.3404055846634733

Epoch: 6| Step: 3
Training loss: 2.6736724376678467
Validation loss: 2.354854558103828

Epoch: 6| Step: 4
Training loss: 2.4922633171081543
Validation loss: 2.358246849429223

Epoch: 6| Step: 5
Training loss: 2.0232114791870117
Validation loss: 2.3620765927017375

Epoch: 6| Step: 6
Training loss: 2.0359790325164795
Validation loss: 2.363227963447571

Epoch: 6| Step: 7
Training loss: 3.0716958045959473
Validation loss: 2.3711440563201904

Epoch: 6| Step: 8
Training loss: 2.84084415435791
Validation loss: 2.3575829972503004

Epoch: 6| Step: 9
Training loss: 2.706494092941284
Validation loss: 2.359981903465845

Epoch: 6| Step: 10
Training loss: 2.1495184898376465
Validation loss: 2.337885666919011

Epoch: 6| Step: 11
Training loss: 2.6328752040863037
Validation loss: 2.3294472130396033

Epoch: 6| Step: 12
Training loss: 2.3833136558532715
Validation loss: 2.3220503740413214

Epoch: 6| Step: 13
Training loss: 2.2548954486846924
Validation loss: 2.314685554914577

Epoch: 208| Step: 0
Training loss: 2.1739935874938965
Validation loss: 2.3179890314737954

Epoch: 6| Step: 1
Training loss: 3.2129838466644287
Validation loss: 2.307836533874594

Epoch: 6| Step: 2
Training loss: 3.1527152061462402
Validation loss: 2.3136463652374926

Epoch: 6| Step: 3
Training loss: 2.1150059700012207
Validation loss: 2.3146066947649886

Epoch: 6| Step: 4
Training loss: 2.307888984680176
Validation loss: 2.311046956687845

Epoch: 6| Step: 5
Training loss: 1.9927537441253662
Validation loss: 2.3112929533886653

Epoch: 6| Step: 6
Training loss: 2.376662254333496
Validation loss: 2.313908674383676

Epoch: 6| Step: 7
Training loss: 3.309098243713379
Validation loss: 2.3172364875834477

Epoch: 6| Step: 8
Training loss: 2.6728837490081787
Validation loss: 2.3112235530730216

Epoch: 6| Step: 9
Training loss: 2.494265079498291
Validation loss: 2.3171524129888064

Epoch: 6| Step: 10
Training loss: 2.291520118713379
Validation loss: 2.322491740667692

Epoch: 6| Step: 11
Training loss: 2.3045506477355957
Validation loss: 2.3251678507815123

Epoch: 6| Step: 12
Training loss: 2.9188120365142822
Validation loss: 2.3313437892544653

Epoch: 6| Step: 13
Training loss: 2.5328190326690674
Validation loss: 2.332336559090563

Epoch: 209| Step: 0
Training loss: 2.8518154621124268
Validation loss: 2.3277587224078435

Epoch: 6| Step: 1
Training loss: 2.7163939476013184
Validation loss: 2.3251557491158925

Epoch: 6| Step: 2
Training loss: 2.9836432933807373
Validation loss: 2.322617453913535

Epoch: 6| Step: 3
Training loss: 2.164499282836914
Validation loss: 2.326710336951799

Epoch: 6| Step: 4
Training loss: 3.5073864459991455
Validation loss: 2.3405845088343464

Epoch: 6| Step: 5
Training loss: 2.2830352783203125
Validation loss: 2.33024081876201

Epoch: 6| Step: 6
Training loss: 2.3065969944000244
Validation loss: 2.3440747466138614

Epoch: 6| Step: 7
Training loss: 2.5538229942321777
Validation loss: 2.3367213664516324

Epoch: 6| Step: 8
Training loss: 1.9500912427902222
Validation loss: 2.349112577335809

Epoch: 6| Step: 9
Training loss: 2.9144198894500732
Validation loss: 2.3414425337186424

Epoch: 6| Step: 10
Training loss: 1.3184819221496582
Validation loss: 2.337857836036272

Epoch: 6| Step: 11
Training loss: 2.8847837448120117
Validation loss: 2.326438844844859

Epoch: 6| Step: 12
Training loss: 2.4592981338500977
Validation loss: 2.3238644881915023

Epoch: 6| Step: 13
Training loss: 2.938123941421509
Validation loss: 2.3255408630576184

Epoch: 210| Step: 0
Training loss: 2.956101894378662
Validation loss: 2.322069937183011

Epoch: 6| Step: 1
Training loss: 3.328845977783203
Validation loss: 2.323736244632352

Epoch: 6| Step: 2
Training loss: 2.122169017791748
Validation loss: 2.3214305703357985

Epoch: 6| Step: 3
Training loss: 2.180420398712158
Validation loss: 2.324538189877746

Epoch: 6| Step: 4
Training loss: 2.7019944190979004
Validation loss: 2.3225623356398715

Epoch: 6| Step: 5
Training loss: 2.1481852531433105
Validation loss: 2.324611412581577

Epoch: 6| Step: 6
Training loss: 2.478290557861328
Validation loss: 2.331921254434893

Epoch: 6| Step: 7
Training loss: 2.688514232635498
Validation loss: 2.3348415308101202

Epoch: 6| Step: 8
Training loss: 2.645674467086792
Validation loss: 2.3461156968147523

Epoch: 6| Step: 9
Training loss: 2.930448055267334
Validation loss: 2.3398975223623295

Epoch: 6| Step: 10
Training loss: 2.6767873764038086
Validation loss: 2.336495825039443

Epoch: 6| Step: 11
Training loss: 1.8954365253448486
Validation loss: 2.3339107959501204

Epoch: 6| Step: 12
Training loss: 2.5660033226013184
Validation loss: 2.3248052661136915

Epoch: 6| Step: 13
Training loss: 2.2024240493774414
Validation loss: 2.3257904770553752

Epoch: 211| Step: 0
Training loss: 2.3212637901306152
Validation loss: 2.3313769499460855

Epoch: 6| Step: 1
Training loss: 2.5136606693267822
Validation loss: 2.3403636281208327

Epoch: 6| Step: 2
Training loss: 2.6784963607788086
Validation loss: 2.3420015483774166

Epoch: 6| Step: 3
Training loss: 3.1873793601989746
Validation loss: 2.3311620553334556

Epoch: 6| Step: 4
Training loss: 2.5439963340759277
Validation loss: 2.3428836740473264

Epoch: 6| Step: 5
Training loss: 2.4753546714782715
Validation loss: 2.337316074678975

Epoch: 6| Step: 6
Training loss: 2.5524425506591797
Validation loss: 2.332939996514269

Epoch: 6| Step: 7
Training loss: 2.782400131225586
Validation loss: 2.3427649441585747

Epoch: 6| Step: 8
Training loss: 2.1854443550109863
Validation loss: 2.341497216173398

Epoch: 6| Step: 9
Training loss: 2.8332271575927734
Validation loss: 2.3410002211088776

Epoch: 6| Step: 10
Training loss: 1.9984650611877441
Validation loss: 2.344500216104651

Epoch: 6| Step: 11
Training loss: 2.4702157974243164
Validation loss: 2.3377075502949376

Epoch: 6| Step: 12
Training loss: 2.616279125213623
Validation loss: 2.343246926543533

Epoch: 6| Step: 13
Training loss: 2.749164581298828
Validation loss: 2.3507949613755748

Epoch: 212| Step: 0
Training loss: 2.916558265686035
Validation loss: 2.3326430756558656

Epoch: 6| Step: 1
Training loss: 2.7300078868865967
Validation loss: 2.3248931438692155

Epoch: 6| Step: 2
Training loss: 2.239748954772949
Validation loss: 2.333547828018024

Epoch: 6| Step: 3
Training loss: 2.2140629291534424
Validation loss: 2.3392301990139868

Epoch: 6| Step: 4
Training loss: 2.5336337089538574
Validation loss: 2.3428466012400966

Epoch: 6| Step: 5
Training loss: 2.29032564163208
Validation loss: 2.350835477152178

Epoch: 6| Step: 6
Training loss: 2.804668664932251
Validation loss: 2.346231573371477

Epoch: 6| Step: 7
Training loss: 2.9454689025878906
Validation loss: 2.3437166688262776

Epoch: 6| Step: 8
Training loss: 2.673758029937744
Validation loss: 2.3413498196550595

Epoch: 6| Step: 9
Training loss: 2.202268362045288
Validation loss: 2.3444863339906097

Epoch: 6| Step: 10
Training loss: 2.4360270500183105
Validation loss: 2.3318516349279754

Epoch: 6| Step: 11
Training loss: 2.4027254581451416
Validation loss: 2.320218168279176

Epoch: 6| Step: 12
Training loss: 2.653635263442993
Validation loss: 2.315210252679804

Epoch: 6| Step: 13
Training loss: 2.6369645595550537
Validation loss: 2.317376313670989

Epoch: 213| Step: 0
Training loss: 2.887460470199585
Validation loss: 2.3190413931364655

Epoch: 6| Step: 1
Training loss: 2.4965310096740723
Validation loss: 2.312000079821515

Epoch: 6| Step: 2
Training loss: 3.0276076793670654
Validation loss: 2.304721592574991

Epoch: 6| Step: 3
Training loss: 3.151712417602539
Validation loss: 2.3100736705205773

Epoch: 6| Step: 4
Training loss: 2.2538061141967773
Validation loss: 2.312483923409575

Epoch: 6| Step: 5
Training loss: 2.622518539428711
Validation loss: 2.3219161982177408

Epoch: 6| Step: 6
Training loss: 2.2187581062316895
Validation loss: 2.3219916820526123

Epoch: 6| Step: 7
Training loss: 2.483830451965332
Validation loss: 2.32765394385143

Epoch: 6| Step: 8
Training loss: 2.267855167388916
Validation loss: 2.323153905971076

Epoch: 6| Step: 9
Training loss: 2.648609161376953
Validation loss: 2.3270969378050936

Epoch: 6| Step: 10
Training loss: 2.783400535583496
Validation loss: 2.3319492442633516

Epoch: 6| Step: 11
Training loss: 2.142287254333496
Validation loss: 2.335283274291664

Epoch: 6| Step: 12
Training loss: 2.2532553672790527
Validation loss: 2.3504674844844367

Epoch: 6| Step: 13
Training loss: 2.304250478744507
Validation loss: 2.346344676069034

Epoch: 214| Step: 0
Training loss: 2.5806565284729004
Validation loss: 2.3443533425690024

Epoch: 6| Step: 1
Training loss: 2.2860679626464844
Validation loss: 2.355664578817224

Epoch: 6| Step: 2
Training loss: 3.0096120834350586
Validation loss: 2.344049048680131

Epoch: 6| Step: 3
Training loss: 2.0908801555633545
Validation loss: 2.3269975775031635

Epoch: 6| Step: 4
Training loss: 2.2334656715393066
Validation loss: 2.329976261302989

Epoch: 6| Step: 5
Training loss: 2.802788257598877
Validation loss: 2.324290024336948

Epoch: 6| Step: 6
Training loss: 2.267360210418701
Validation loss: 2.319570428581648

Epoch: 6| Step: 7
Training loss: 2.038961410522461
Validation loss: 2.3286637003703783

Epoch: 6| Step: 8
Training loss: 1.762505292892456
Validation loss: 2.3314534515462895

Epoch: 6| Step: 9
Training loss: 2.9035754203796387
Validation loss: 2.3426548665569675

Epoch: 6| Step: 10
Training loss: 2.8869564533233643
Validation loss: 2.3603295510815037

Epoch: 6| Step: 11
Training loss: 3.3692870140075684
Validation loss: 2.358487613739506

Epoch: 6| Step: 12
Training loss: 3.1679582595825195
Validation loss: 2.3600762928685834

Epoch: 6| Step: 13
Training loss: 2.437731981277466
Validation loss: 2.3611039948719803

Epoch: 215| Step: 0
Training loss: 2.158968448638916
Validation loss: 2.3644306249515985

Epoch: 6| Step: 1
Training loss: 2.449026584625244
Validation loss: 2.365340135430777

Epoch: 6| Step: 2
Training loss: 2.9744503498077393
Validation loss: 2.373818958959272

Epoch: 6| Step: 3
Training loss: 2.067511558532715
Validation loss: 2.3758604449610554

Epoch: 6| Step: 4
Training loss: 3.1293234825134277
Validation loss: 2.3684471461080734

Epoch: 6| Step: 5
Training loss: 2.2941031455993652
Validation loss: 2.374180237452189

Epoch: 6| Step: 6
Training loss: 2.8919949531555176
Validation loss: 2.3718043527295514

Epoch: 6| Step: 7
Training loss: 2.7019550800323486
Validation loss: 2.3710477582869993

Epoch: 6| Step: 8
Training loss: 2.1165504455566406
Validation loss: 2.3818391548689974

Epoch: 6| Step: 9
Training loss: 2.6420063972473145
Validation loss: 2.3719849996669318

Epoch: 6| Step: 10
Training loss: 2.6508865356445312
Validation loss: 2.385579652683709

Epoch: 6| Step: 11
Training loss: 2.909620761871338
Validation loss: 2.3804827454269573

Epoch: 6| Step: 12
Training loss: 2.657294750213623
Validation loss: 2.361444891140025

Epoch: 6| Step: 13
Training loss: 1.9255387783050537
Validation loss: 2.3512984732145905

Epoch: 216| Step: 0
Training loss: 2.639841318130493
Validation loss: 2.3476261092770483

Epoch: 6| Step: 1
Training loss: 2.653090238571167
Validation loss: 2.3405339769137803

Epoch: 6| Step: 2
Training loss: 2.57096004486084
Validation loss: 2.3419036301233436

Epoch: 6| Step: 3
Training loss: 2.4662137031555176
Validation loss: 2.34360332130104

Epoch: 6| Step: 4
Training loss: 2.7960968017578125
Validation loss: 2.3391173039713213

Epoch: 6| Step: 5
Training loss: 3.2629570960998535
Validation loss: 2.335373506751112

Epoch: 6| Step: 6
Training loss: 2.5255331993103027
Validation loss: 2.3324230076164327

Epoch: 6| Step: 7
Training loss: 2.341691017150879
Validation loss: 2.33028503130841

Epoch: 6| Step: 8
Training loss: 2.4412403106689453
Validation loss: 2.3297662581166914

Epoch: 6| Step: 9
Training loss: 2.371427059173584
Validation loss: 2.3321079566914547

Epoch: 6| Step: 10
Training loss: 2.4073879718780518
Validation loss: 2.326972897334765

Epoch: 6| Step: 11
Training loss: 2.8531782627105713
Validation loss: 2.308422411641767

Epoch: 6| Step: 12
Training loss: 2.051015853881836
Validation loss: 2.3259287905949417

Epoch: 6| Step: 13
Training loss: 2.4504830837249756
Validation loss: 2.3174840455414145

Epoch: 217| Step: 0
Training loss: 2.141195297241211
Validation loss: 2.315294700284158

Epoch: 6| Step: 1
Training loss: 2.369741439819336
Validation loss: 2.327326325960057

Epoch: 6| Step: 2
Training loss: 2.1110496520996094
Validation loss: 2.316455110426872

Epoch: 6| Step: 3
Training loss: 2.6173558235168457
Validation loss: 2.3180667905397314

Epoch: 6| Step: 4
Training loss: 2.749990940093994
Validation loss: 2.3050262684463174

Epoch: 6| Step: 5
Training loss: 2.7865793704986572
Validation loss: 2.298248283324703

Epoch: 6| Step: 6
Training loss: 2.5525383949279785
Validation loss: 2.2968046793373684

Epoch: 6| Step: 7
Training loss: 3.4601781368255615
Validation loss: 2.29488895785424

Epoch: 6| Step: 8
Training loss: 2.506747007369995
Validation loss: 2.29479205992914

Epoch: 6| Step: 9
Training loss: 2.77587890625
Validation loss: 2.293055729199481

Epoch: 6| Step: 10
Training loss: 2.040705680847168
Validation loss: 2.2991696852509693

Epoch: 6| Step: 11
Training loss: 2.566251516342163
Validation loss: 2.2950051523024038

Epoch: 6| Step: 12
Training loss: 1.9803026914596558
Validation loss: 2.306037970768508

Epoch: 6| Step: 13
Training loss: 2.9103891849517822
Validation loss: 2.310664317941153

Epoch: 218| Step: 0
Training loss: 3.0766806602478027
Validation loss: 2.323064019603114

Epoch: 6| Step: 1
Training loss: 2.471752643585205
Validation loss: 2.317071810845406

Epoch: 6| Step: 2
Training loss: 2.23164701461792
Validation loss: 2.3367472438402075

Epoch: 6| Step: 3
Training loss: 3.1545703411102295
Validation loss: 2.3474870650999007

Epoch: 6| Step: 4
Training loss: 2.4817395210266113
Validation loss: 2.355889653646818

Epoch: 6| Step: 5
Training loss: 2.348707675933838
Validation loss: 2.356761404263076

Epoch: 6| Step: 6
Training loss: 3.0769357681274414
Validation loss: 2.3636096446744856

Epoch: 6| Step: 7
Training loss: 2.0787761211395264
Validation loss: 2.3409074109087706

Epoch: 6| Step: 8
Training loss: 2.1429238319396973
Validation loss: 2.3351459746719687

Epoch: 6| Step: 9
Training loss: 2.0364913940429688
Validation loss: 2.313432820381657

Epoch: 6| Step: 10
Training loss: 2.7124009132385254
Validation loss: 2.2985650749616724

Epoch: 6| Step: 11
Training loss: 2.6991748809814453
Validation loss: 2.2944353011346634

Epoch: 6| Step: 12
Training loss: 2.405052900314331
Validation loss: 2.2868201194270963

Epoch: 6| Step: 13
Training loss: 2.8486156463623047
Validation loss: 2.286278288851502

Epoch: 219| Step: 0
Training loss: 3.129936456680298
Validation loss: 2.2915413507851223

Epoch: 6| Step: 1
Training loss: 2.3196544647216797
Validation loss: 2.2891644982881445

Epoch: 6| Step: 2
Training loss: 1.9174102544784546
Validation loss: 2.283299393551324

Epoch: 6| Step: 3
Training loss: 2.040271520614624
Validation loss: 2.288834179601362

Epoch: 6| Step: 4
Training loss: 2.2901062965393066
Validation loss: 2.289895667824694

Epoch: 6| Step: 5
Training loss: 2.2854013442993164
Validation loss: 2.2786618483963834

Epoch: 6| Step: 6
Training loss: 2.738194465637207
Validation loss: 2.2858115908920125

Epoch: 6| Step: 7
Training loss: 2.5549678802490234
Validation loss: 2.285026865620767

Epoch: 6| Step: 8
Training loss: 2.1686484813690186
Validation loss: 2.294771558494978

Epoch: 6| Step: 9
Training loss: 3.273983955383301
Validation loss: 2.2957407556554323

Epoch: 6| Step: 10
Training loss: 3.1400856971740723
Validation loss: 2.2895999339319046

Epoch: 6| Step: 11
Training loss: 2.5776116847991943
Validation loss: 2.2944977924387944

Epoch: 6| Step: 12
Training loss: 2.531310558319092
Validation loss: 2.3027944744274182

Epoch: 6| Step: 13
Training loss: 2.383517265319824
Validation loss: 2.298891587923932

Epoch: 220| Step: 0
Training loss: 2.203824520111084
Validation loss: 2.3113250578603437

Epoch: 6| Step: 1
Training loss: 2.3575875759124756
Validation loss: 2.330364347786032

Epoch: 6| Step: 2
Training loss: 2.881948947906494
Validation loss: 2.33304278824919

Epoch: 6| Step: 3
Training loss: 2.5716781616210938
Validation loss: 2.337326749678581

Epoch: 6| Step: 4
Training loss: 2.5411901473999023
Validation loss: 2.3615084822459886

Epoch: 6| Step: 5
Training loss: 2.4471144676208496
Validation loss: 2.3742821498583724

Epoch: 6| Step: 6
Training loss: 2.257751703262329
Validation loss: 2.3323165678208873

Epoch: 6| Step: 7
Training loss: 3.006634473800659
Validation loss: 2.3347560449313094

Epoch: 6| Step: 8
Training loss: 2.0043559074401855
Validation loss: 2.315269362541937

Epoch: 6| Step: 9
Training loss: 2.852790355682373
Validation loss: 2.287672027464836

Epoch: 6| Step: 10
Training loss: 2.459108352661133
Validation loss: 2.2811225998786187

Epoch: 6| Step: 11
Training loss: 2.4634013175964355
Validation loss: 2.2835726097065914

Epoch: 6| Step: 12
Training loss: 2.490769386291504
Validation loss: 2.2813138167063394

Epoch: 6| Step: 13
Training loss: 3.1626780033111572
Validation loss: 2.279010018994731

Epoch: 221| Step: 0
Training loss: 2.5108838081359863
Validation loss: 2.2732574734636533

Epoch: 6| Step: 1
Training loss: 2.6967227458953857
Validation loss: 2.28049736638223

Epoch: 6| Step: 2
Training loss: 2.0114808082580566
Validation loss: 2.282391817339005

Epoch: 6| Step: 3
Training loss: 2.483085870742798
Validation loss: 2.278386544155818

Epoch: 6| Step: 4
Training loss: 3.0531623363494873
Validation loss: 2.282467747247347

Epoch: 6| Step: 5
Training loss: 2.3848984241485596
Validation loss: 2.278705635378438

Epoch: 6| Step: 6
Training loss: 3.1254665851593018
Validation loss: 2.2797006971092633

Epoch: 6| Step: 7
Training loss: 2.2826499938964844
Validation loss: 2.2895087093435307

Epoch: 6| Step: 8
Training loss: 2.9446122646331787
Validation loss: 2.291046747597315

Epoch: 6| Step: 9
Training loss: 2.504324436187744
Validation loss: 2.30200352720035

Epoch: 6| Step: 10
Training loss: 2.369727611541748
Validation loss: 2.310477702848373

Epoch: 6| Step: 11
Training loss: 2.276923656463623
Validation loss: 2.3389500033470894

Epoch: 6| Step: 12
Training loss: 2.563300848007202
Validation loss: 2.3436116428785425

Epoch: 6| Step: 13
Training loss: 1.8258776664733887
Validation loss: 2.364283518124652

Epoch: 222| Step: 0
Training loss: 3.0153214931488037
Validation loss: 2.3580618084117932

Epoch: 6| Step: 1
Training loss: 2.025109052658081
Validation loss: 2.353497400078722

Epoch: 6| Step: 2
Training loss: 3.4914402961730957
Validation loss: 2.3440754823787238

Epoch: 6| Step: 3
Training loss: 2.4348807334899902
Validation loss: 2.321516793261292

Epoch: 6| Step: 4
Training loss: 2.138051748275757
Validation loss: 2.3045078733915925

Epoch: 6| Step: 5
Training loss: 2.377788543701172
Validation loss: 2.2899788310450893

Epoch: 6| Step: 6
Training loss: 2.64133358001709
Validation loss: 2.30570287089194

Epoch: 6| Step: 7
Training loss: 1.8578089475631714
Validation loss: 2.3161133950756443

Epoch: 6| Step: 8
Training loss: 2.364992380142212
Validation loss: 2.3090519546180643

Epoch: 6| Step: 9
Training loss: 2.534304141998291
Validation loss: 2.316870294591432

Epoch: 6| Step: 10
Training loss: 2.8569562435150146
Validation loss: 2.31205411623883

Epoch: 6| Step: 11
Training loss: 3.0193839073181152
Validation loss: 2.3076297390845513

Epoch: 6| Step: 12
Training loss: 2.121370792388916
Validation loss: 2.291798399340722

Epoch: 6| Step: 13
Training loss: 2.5623490810394287
Validation loss: 2.2951946386726956

Epoch: 223| Step: 0
Training loss: 2.5135209560394287
Validation loss: 2.2805808539031656

Epoch: 6| Step: 1
Training loss: 3.150716781616211
Validation loss: 2.2871144920267086

Epoch: 6| Step: 2
Training loss: 2.310168743133545
Validation loss: 2.2739834375278924

Epoch: 6| Step: 3
Training loss: 2.2210209369659424
Validation loss: 2.2819877491202405

Epoch: 6| Step: 4
Training loss: 2.1228578090667725
Validation loss: 2.2847359923906225

Epoch: 6| Step: 5
Training loss: 3.082956075668335
Validation loss: 2.279110085579657

Epoch: 6| Step: 6
Training loss: 2.3386683464050293
Validation loss: 2.286791778379871

Epoch: 6| Step: 7
Training loss: 2.408191442489624
Validation loss: 2.2813424705177225

Epoch: 6| Step: 8
Training loss: 2.6305747032165527
Validation loss: 2.2939316431681314

Epoch: 6| Step: 9
Training loss: 2.8571629524230957
Validation loss: 2.307199042330506

Epoch: 6| Step: 10
Training loss: 2.1553852558135986
Validation loss: 2.3159213553192797

Epoch: 6| Step: 11
Training loss: 2.735363245010376
Validation loss: 2.3155246626946235

Epoch: 6| Step: 12
Training loss: 2.441884994506836
Validation loss: 2.298605219010384

Epoch: 6| Step: 13
Training loss: 2.2088797092437744
Validation loss: 2.2956060542855212

Epoch: 224| Step: 0
Training loss: 2.3626251220703125
Validation loss: 2.284660247064406

Epoch: 6| Step: 1
Training loss: 2.729495048522949
Validation loss: 2.285817466756349

Epoch: 6| Step: 2
Training loss: 2.3023159503936768
Validation loss: 2.288416653551081

Epoch: 6| Step: 3
Training loss: 2.2788922786712646
Validation loss: 2.296029547209381

Epoch: 6| Step: 4
Training loss: 1.8712708950042725
Validation loss: 2.292828667548395

Epoch: 6| Step: 5
Training loss: 2.845330238342285
Validation loss: 2.2934406790682065

Epoch: 6| Step: 6
Training loss: 2.6924281120300293
Validation loss: 2.2952271046177035

Epoch: 6| Step: 7
Training loss: 2.6008858680725098
Validation loss: 2.2970659425181728

Epoch: 6| Step: 8
Training loss: 2.2918081283569336
Validation loss: 2.3001102298818608

Epoch: 6| Step: 9
Training loss: 2.737632989883423
Validation loss: 2.2947412485717447

Epoch: 6| Step: 10
Training loss: 2.8970420360565186
Validation loss: 2.2865861744009037

Epoch: 6| Step: 11
Training loss: 2.668177366256714
Validation loss: 2.2812350591023765

Epoch: 6| Step: 12
Training loss: 2.73978328704834
Validation loss: 2.2765000045940442

Epoch: 6| Step: 13
Training loss: 1.7288005352020264
Validation loss: 2.274858961823166

Epoch: 225| Step: 0
Training loss: 2.397129535675049
Validation loss: 2.2718479966604583

Epoch: 6| Step: 1
Training loss: 2.427504777908325
Validation loss: 2.2754600817157375

Epoch: 6| Step: 2
Training loss: 2.7563552856445312
Validation loss: 2.276223608242568

Epoch: 6| Step: 3
Training loss: 2.428511619567871
Validation loss: 2.2836464451205347

Epoch: 6| Step: 4
Training loss: 3.3606386184692383
Validation loss: 2.2892805504542526

Epoch: 6| Step: 5
Training loss: 3.1401925086975098
Validation loss: 2.305789286090482

Epoch: 6| Step: 6
Training loss: 1.4715558290481567
Validation loss: 2.2948219186516217

Epoch: 6| Step: 7
Training loss: 2.387869358062744
Validation loss: 2.307512062852101

Epoch: 6| Step: 8
Training loss: 2.7298154830932617
Validation loss: 2.2916239948682886

Epoch: 6| Step: 9
Training loss: 2.088914394378662
Validation loss: 2.288496367393001

Epoch: 6| Step: 10
Training loss: 2.770251512527466
Validation loss: 2.284535946384553

Epoch: 6| Step: 11
Training loss: 2.1418025493621826
Validation loss: 2.2840424058257893

Epoch: 6| Step: 12
Training loss: 2.520453453063965
Validation loss: 2.282222996475876

Epoch: 6| Step: 13
Training loss: 2.55184268951416
Validation loss: 2.2709801299597627

Epoch: 226| Step: 0
Training loss: 3.3500967025756836
Validation loss: 2.267910590735815

Epoch: 6| Step: 1
Training loss: 2.5518290996551514
Validation loss: 2.26415434704032

Epoch: 6| Step: 2
Training loss: 2.3365702629089355
Validation loss: 2.263775110244751

Epoch: 6| Step: 3
Training loss: 1.4923369884490967
Validation loss: 2.2678872846787974

Epoch: 6| Step: 4
Training loss: 3.4880897998809814
Validation loss: 2.2701367793544645

Epoch: 6| Step: 5
Training loss: 2.057041645050049
Validation loss: 2.2758251185058267

Epoch: 6| Step: 6
Training loss: 2.4856462478637695
Validation loss: 2.280950597537461

Epoch: 6| Step: 7
Training loss: 2.1426262855529785
Validation loss: 2.2901130466051

Epoch: 6| Step: 8
Training loss: 2.5958657264709473
Validation loss: 2.292590028496199

Epoch: 6| Step: 9
Training loss: 2.1767992973327637
Validation loss: 2.3112887669635076

Epoch: 6| Step: 10
Training loss: 3.1502537727355957
Validation loss: 2.3267912172502085

Epoch: 6| Step: 11
Training loss: 1.7018672227859497
Validation loss: 2.353402071101691

Epoch: 6| Step: 12
Training loss: 3.1571836471557617
Validation loss: 2.3615000888865483

Epoch: 6| Step: 13
Training loss: 2.4384732246398926
Validation loss: 2.400817632675171

Epoch: 227| Step: 0
Training loss: 2.579890727996826
Validation loss: 2.4025864216589157

Epoch: 6| Step: 1
Training loss: 3.033299446105957
Validation loss: 2.4088954464081795

Epoch: 6| Step: 2
Training loss: 2.7983741760253906
Validation loss: 2.4256511913832797

Epoch: 6| Step: 3
Training loss: 2.1493752002716064
Validation loss: 2.392279581357074

Epoch: 6| Step: 4
Training loss: 2.3761606216430664
Validation loss: 2.370397388294179

Epoch: 6| Step: 5
Training loss: 2.436624526977539
Validation loss: 2.3431960177677933

Epoch: 6| Step: 6
Training loss: 2.2847847938537598
Validation loss: 2.338215010140532

Epoch: 6| Step: 7
Training loss: 2.514192581176758
Validation loss: 2.317724855997229

Epoch: 6| Step: 8
Training loss: 2.595764636993408
Validation loss: 2.3176651821341565

Epoch: 6| Step: 9
Training loss: 2.8852431774139404
Validation loss: 2.3017748863466325

Epoch: 6| Step: 10
Training loss: 1.8549832105636597
Validation loss: 2.2975185404541674

Epoch: 6| Step: 11
Training loss: 2.327786922454834
Validation loss: 2.2913047523908716

Epoch: 6| Step: 12
Training loss: 3.0202460289001465
Validation loss: 2.2892235735411286

Epoch: 6| Step: 13
Training loss: 2.59743595123291
Validation loss: 2.290277922025291

Epoch: 228| Step: 0
Training loss: 2.643427848815918
Validation loss: 2.2917135659084527

Epoch: 6| Step: 1
Training loss: 2.5531363487243652
Validation loss: 2.281180566357028

Epoch: 6| Step: 2
Training loss: 2.2623231410980225
Validation loss: 2.288278305402366

Epoch: 6| Step: 3
Training loss: 2.60561203956604
Validation loss: 2.2921425962960846

Epoch: 6| Step: 4
Training loss: 3.116849422454834
Validation loss: 2.2941699771470923

Epoch: 6| Step: 5
Training loss: 2.0769286155700684
Validation loss: 2.3100203365407963

Epoch: 6| Step: 6
Training loss: 3.4258222579956055
Validation loss: 2.3154845442823184

Epoch: 6| Step: 7
Training loss: 2.600084066390991
Validation loss: 2.3231140798138035

Epoch: 6| Step: 8
Training loss: 1.9544248580932617
Validation loss: 2.3281709712038756

Epoch: 6| Step: 9
Training loss: 2.833035945892334
Validation loss: 2.3139744061295704

Epoch: 6| Step: 10
Training loss: 1.9181177616119385
Validation loss: 2.314680544278955

Epoch: 6| Step: 11
Training loss: 1.9257664680480957
Validation loss: 2.3148907000018704

Epoch: 6| Step: 12
Training loss: 2.362118721008301
Validation loss: 2.3131791058407036

Epoch: 6| Step: 13
Training loss: 3.0531954765319824
Validation loss: 2.302859988263858

Epoch: 229| Step: 0
Training loss: 2.1687519550323486
Validation loss: 2.297329295066095

Epoch: 6| Step: 1
Training loss: 2.248552083969116
Validation loss: 2.2828534162172707

Epoch: 6| Step: 2
Training loss: 2.302690029144287
Validation loss: 2.2734537816816762

Epoch: 6| Step: 3
Training loss: 2.697688579559326
Validation loss: 2.263359086487883

Epoch: 6| Step: 4
Training loss: 3.3592920303344727
Validation loss: 2.2685073626938688

Epoch: 6| Step: 5
Training loss: 2.2085788249969482
Validation loss: 2.2697689853688723

Epoch: 6| Step: 6
Training loss: 2.936835527420044
Validation loss: 2.2706659070907103

Epoch: 6| Step: 7
Training loss: 2.0972800254821777
Validation loss: 2.2701571167156263

Epoch: 6| Step: 8
Training loss: 2.555330514907837
Validation loss: 2.2891289239288657

Epoch: 6| Step: 9
Training loss: 2.822401523590088
Validation loss: 2.2898475072717153

Epoch: 6| Step: 10
Training loss: 1.934444546699524
Validation loss: 2.287667477002708

Epoch: 6| Step: 11
Training loss: 2.4734373092651367
Validation loss: 2.290286059020668

Epoch: 6| Step: 12
Training loss: 2.508206844329834
Validation loss: 2.283685081748552

Epoch: 6| Step: 13
Training loss: 2.950791835784912
Validation loss: 2.2728175783670075

Epoch: 230| Step: 0
Training loss: 2.185624599456787
Validation loss: 2.284919067095685

Epoch: 6| Step: 1
Training loss: 2.865330696105957
Validation loss: 2.280614470922819

Epoch: 6| Step: 2
Training loss: 3.0022332668304443
Validation loss: 2.272871337911134

Epoch: 6| Step: 3
Training loss: 2.1554880142211914
Validation loss: 2.2934606793106243

Epoch: 6| Step: 4
Training loss: 2.2214863300323486
Validation loss: 2.312800189500214

Epoch: 6| Step: 5
Training loss: 2.185635566711426
Validation loss: 2.31863126959852

Epoch: 6| Step: 6
Training loss: 2.6951093673706055
Validation loss: 2.330446307377149

Epoch: 6| Step: 7
Training loss: 2.465134620666504
Validation loss: 2.325994640268305

Epoch: 6| Step: 8
Training loss: 2.7068610191345215
Validation loss: 2.311550763345534

Epoch: 6| Step: 9
Training loss: 2.560117721557617
Validation loss: 2.309701858028289

Epoch: 6| Step: 10
Training loss: 2.8539774417877197
Validation loss: 2.2880522025528776

Epoch: 6| Step: 11
Training loss: 2.2039127349853516
Validation loss: 2.2922123965396675

Epoch: 6| Step: 12
Training loss: 2.4796903133392334
Validation loss: 2.2867819827090026

Epoch: 6| Step: 13
Training loss: 2.2351832389831543
Validation loss: 2.295260872892154

Epoch: 231| Step: 0
Training loss: 1.5884246826171875
Validation loss: 2.2942082253835534

Epoch: 6| Step: 1
Training loss: 2.443880081176758
Validation loss: 2.2951175448715047

Epoch: 6| Step: 2
Training loss: 2.128363609313965
Validation loss: 2.3121144694666707

Epoch: 6| Step: 3
Training loss: 2.4128758907318115
Validation loss: 2.310873587926229

Epoch: 6| Step: 4
Training loss: 3.0533502101898193
Validation loss: 2.337624954920943

Epoch: 6| Step: 5
Training loss: 3.033564567565918
Validation loss: 2.35704190756685

Epoch: 6| Step: 6
Training loss: 2.624423027038574
Validation loss: 2.37193158108701

Epoch: 6| Step: 7
Training loss: 2.703421115875244
Validation loss: 2.347542646110699

Epoch: 6| Step: 8
Training loss: 2.544131278991699
Validation loss: 2.343984309063163

Epoch: 6| Step: 9
Training loss: 1.968353509902954
Validation loss: 2.3195602304192

Epoch: 6| Step: 10
Training loss: 2.6689774990081787
Validation loss: 2.295688477895593

Epoch: 6| Step: 11
Training loss: 2.842219352722168
Validation loss: 2.287240784655335

Epoch: 6| Step: 12
Training loss: 2.2995944023132324
Validation loss: 2.266859305802212

Epoch: 6| Step: 13
Training loss: 2.8268041610717773
Validation loss: 2.2684850256930114

Epoch: 232| Step: 0
Training loss: 3.3508338928222656
Validation loss: 2.267225275757492

Epoch: 6| Step: 1
Training loss: 2.8168320655822754
Validation loss: 2.2675567775644283

Epoch: 6| Step: 2
Training loss: 1.7786093950271606
Validation loss: 2.271058331253708

Epoch: 6| Step: 3
Training loss: 2.010141134262085
Validation loss: 2.263837542585147

Epoch: 6| Step: 4
Training loss: 2.779193878173828
Validation loss: 2.274258598204582

Epoch: 6| Step: 5
Training loss: 2.0634632110595703
Validation loss: 2.2697876076544485

Epoch: 6| Step: 6
Training loss: 1.9941273927688599
Validation loss: 2.282613895272696

Epoch: 6| Step: 7
Training loss: 2.009873390197754
Validation loss: 2.2805938105429373

Epoch: 6| Step: 8
Training loss: 2.7357187271118164
Validation loss: 2.286630435656476

Epoch: 6| Step: 9
Training loss: 2.461851119995117
Validation loss: 2.3111270166212514

Epoch: 6| Step: 10
Training loss: 2.007404088973999
Validation loss: 2.3240787906031453

Epoch: 6| Step: 11
Training loss: 3.118398904800415
Validation loss: 2.335261898656045

Epoch: 6| Step: 12
Training loss: 2.9364171028137207
Validation loss: 2.328424089698381

Epoch: 6| Step: 13
Training loss: 4.053075790405273
Validation loss: 2.311785295445432

Epoch: 233| Step: 0
Training loss: 2.7868847846984863
Validation loss: 2.2930603078616563

Epoch: 6| Step: 1
Training loss: 2.3360860347747803
Validation loss: 2.2820932916415635

Epoch: 6| Step: 2
Training loss: 2.9489479064941406
Validation loss: 2.270638132608065

Epoch: 6| Step: 3
Training loss: 2.1987977027893066
Validation loss: 2.276757099295175

Epoch: 6| Step: 4
Training loss: 2.691380739212036
Validation loss: 2.272208813698061

Epoch: 6| Step: 5
Training loss: 2.652003765106201
Validation loss: 2.269554276620188

Epoch: 6| Step: 6
Training loss: 2.3721776008605957
Validation loss: 2.2690128613543767

Epoch: 6| Step: 7
Training loss: 2.5331673622131348
Validation loss: 2.269076742151732

Epoch: 6| Step: 8
Training loss: 1.9493764638900757
Validation loss: 2.2787255394843315

Epoch: 6| Step: 9
Training loss: 2.2968132495880127
Validation loss: 2.2804319576550554

Epoch: 6| Step: 10
Training loss: 2.443542957305908
Validation loss: 2.2740177159668296

Epoch: 6| Step: 11
Training loss: 2.61735200881958
Validation loss: 2.2831609736206713

Epoch: 6| Step: 12
Training loss: 2.9781582355499268
Validation loss: 2.2950369722099713

Epoch: 6| Step: 13
Training loss: 2.200019359588623
Validation loss: 2.2885904132678943

Epoch: 234| Step: 0
Training loss: 2.7546520233154297
Validation loss: 2.2932392140870452

Epoch: 6| Step: 1
Training loss: 3.0667471885681152
Validation loss: 2.2962370329005743

Epoch: 6| Step: 2
Training loss: 3.4337658882141113
Validation loss: 2.294792595730033

Epoch: 6| Step: 3
Training loss: 2.8932745456695557
Validation loss: 2.2754659063072613

Epoch: 6| Step: 4
Training loss: 2.309453010559082
Validation loss: 2.280674506259221

Epoch: 6| Step: 5
Training loss: 2.398800849914551
Validation loss: 2.2792134361882366

Epoch: 6| Step: 6
Training loss: 2.350543975830078
Validation loss: 2.2684605634340675

Epoch: 6| Step: 7
Training loss: 2.493765354156494
Validation loss: 2.2667272885640464

Epoch: 6| Step: 8
Training loss: 2.5413849353790283
Validation loss: 2.2642335327722694

Epoch: 6| Step: 9
Training loss: 2.2427663803100586
Validation loss: 2.2604363400449037

Epoch: 6| Step: 10
Training loss: 2.120708465576172
Validation loss: 2.2677858516734135

Epoch: 6| Step: 11
Training loss: 2.109750747680664
Validation loss: 2.261736980048559

Epoch: 6| Step: 12
Training loss: 1.4389562606811523
Validation loss: 2.273075229378157

Epoch: 6| Step: 13
Training loss: 2.9957921504974365
Validation loss: 2.2896847853096585

Epoch: 235| Step: 0
Training loss: 3.0277342796325684
Validation loss: 2.2944029531171246

Epoch: 6| Step: 1
Training loss: 2.1267683506011963
Validation loss: 2.3243902421766713

Epoch: 6| Step: 2
Training loss: 2.5091257095336914
Validation loss: 2.3280300453145015

Epoch: 6| Step: 3
Training loss: 2.576993465423584
Validation loss: 2.3464560431818806

Epoch: 6| Step: 4
Training loss: 2.190455675125122
Validation loss: 2.3554405986621814

Epoch: 6| Step: 5
Training loss: 3.1000335216522217
Validation loss: 2.3467212364237797

Epoch: 6| Step: 6
Training loss: 2.7170510292053223
Validation loss: 2.3168744194892144

Epoch: 6| Step: 7
Training loss: 2.2228448390960693
Validation loss: 2.2977846463521323

Epoch: 6| Step: 8
Training loss: 2.1254782676696777
Validation loss: 2.3050942702959945

Epoch: 6| Step: 9
Training loss: 1.792344570159912
Validation loss: 2.284868409556727

Epoch: 6| Step: 10
Training loss: 3.067594051361084
Validation loss: 2.287893743925197

Epoch: 6| Step: 11
Training loss: 2.246319055557251
Validation loss: 2.272265677810997

Epoch: 6| Step: 12
Training loss: 2.285123348236084
Validation loss: 2.274203287657871

Epoch: 6| Step: 13
Training loss: 3.1867754459381104
Validation loss: 2.276100125364078

Epoch: 236| Step: 0
Training loss: 2.160550594329834
Validation loss: 2.27336133680036

Epoch: 6| Step: 1
Training loss: 2.5189945697784424
Validation loss: 2.270458589317978

Epoch: 6| Step: 2
Training loss: 2.5754170417785645
Validation loss: 2.2698695121272916

Epoch: 6| Step: 3
Training loss: 2.7382960319519043
Validation loss: 2.2694361414960635

Epoch: 6| Step: 4
Training loss: 2.618663787841797
Validation loss: 2.2661586910165767

Epoch: 6| Step: 5
Training loss: 3.7885634899139404
Validation loss: 2.267536078729937

Epoch: 6| Step: 6
Training loss: 1.7896952629089355
Validation loss: 2.263910722988908

Epoch: 6| Step: 7
Training loss: 2.6197283267974854
Validation loss: 2.271519555840441

Epoch: 6| Step: 8
Training loss: 2.7384486198425293
Validation loss: 2.2589572039983605

Epoch: 6| Step: 9
Training loss: 2.525275707244873
Validation loss: 2.2644653063948437

Epoch: 6| Step: 10
Training loss: 1.5808000564575195
Validation loss: 2.2743904411151843

Epoch: 6| Step: 11
Training loss: 2.691180944442749
Validation loss: 2.284440658425772

Epoch: 6| Step: 12
Training loss: 2.0966360569000244
Validation loss: 2.3065839403419086

Epoch: 6| Step: 13
Training loss: 2.6131410598754883
Validation loss: 2.3004897807234075

Epoch: 237| Step: 0
Training loss: 2.5316970348358154
Validation loss: 2.3210914852798625

Epoch: 6| Step: 1
Training loss: 2.418358564376831
Validation loss: 2.325217023972542

Epoch: 6| Step: 2
Training loss: 2.626415729522705
Validation loss: 2.3187038616467546

Epoch: 6| Step: 3
Training loss: 3.236804485321045
Validation loss: 2.326852736934539

Epoch: 6| Step: 4
Training loss: 2.7533907890319824
Validation loss: 2.3096878144048874

Epoch: 6| Step: 5
Training loss: 2.2917022705078125
Validation loss: 2.298532332143476

Epoch: 6| Step: 6
Training loss: 2.354224920272827
Validation loss: 2.287063434559812

Epoch: 6| Step: 7
Training loss: 2.55065655708313
Validation loss: 2.275532102072111

Epoch: 6| Step: 8
Training loss: 2.699392795562744
Validation loss: 2.2723797341828704

Epoch: 6| Step: 9
Training loss: 2.1151883602142334
Validation loss: 2.261537767225696

Epoch: 6| Step: 10
Training loss: 3.305428981781006
Validation loss: 2.2622934464485414

Epoch: 6| Step: 11
Training loss: 1.8228836059570312
Validation loss: 2.254658329871393

Epoch: 6| Step: 12
Training loss: 2.2130672931671143
Validation loss: 2.256905919762068

Epoch: 6| Step: 13
Training loss: 1.8699986934661865
Validation loss: 2.250054513254473

Epoch: 238| Step: 0
Training loss: 2.7431907653808594
Validation loss: 2.2601163746208273

Epoch: 6| Step: 1
Training loss: 2.4309754371643066
Validation loss: 2.2640716645025436

Epoch: 6| Step: 2
Training loss: 2.295936107635498
Validation loss: 2.27169479093244

Epoch: 6| Step: 3
Training loss: 2.6042168140411377
Validation loss: 2.2639625303206907

Epoch: 6| Step: 4
Training loss: 2.0831127166748047
Validation loss: 2.2781709906875447

Epoch: 6| Step: 5
Training loss: 2.26617693901062
Validation loss: 2.291770963258641

Epoch: 6| Step: 6
Training loss: 2.4127960205078125
Validation loss: 2.2922741828426236

Epoch: 6| Step: 7
Training loss: 3.718705415725708
Validation loss: 2.301602231558933

Epoch: 6| Step: 8
Training loss: 2.366236686706543
Validation loss: 2.288758057419972

Epoch: 6| Step: 9
Training loss: 2.2399990558624268
Validation loss: 2.295921684593283

Epoch: 6| Step: 10
Training loss: 3.0463545322418213
Validation loss: 2.2805909149108397

Epoch: 6| Step: 11
Training loss: 1.980622410774231
Validation loss: 2.271375302345522

Epoch: 6| Step: 12
Training loss: 2.0766611099243164
Validation loss: 2.2642375730699107

Epoch: 6| Step: 13
Training loss: 3.0572662353515625
Validation loss: 2.2730421994322088

Epoch: 239| Step: 0
Training loss: 2.929840087890625
Validation loss: 2.275527665691991

Epoch: 6| Step: 1
Training loss: 2.204655170440674
Validation loss: 2.26706495336307

Epoch: 6| Step: 2
Training loss: 2.4853434562683105
Validation loss: 2.2854368661039617

Epoch: 6| Step: 3
Training loss: 2.7588694095611572
Validation loss: 2.3033694221127416

Epoch: 6| Step: 4
Training loss: 2.999685287475586
Validation loss: 2.3021431661421254

Epoch: 6| Step: 5
Training loss: 2.101130485534668
Validation loss: 2.3087099034299134

Epoch: 6| Step: 6
Training loss: 2.965463876724243
Validation loss: 2.3147728584145986

Epoch: 6| Step: 7
Training loss: 1.6337120532989502
Validation loss: 2.3077567033870245

Epoch: 6| Step: 8
Training loss: 2.1255204677581787
Validation loss: 2.2892059356935563

Epoch: 6| Step: 9
Training loss: 2.955420970916748
Validation loss: 2.2719460712966097

Epoch: 6| Step: 10
Training loss: 2.661341905593872
Validation loss: 2.2757243366651636

Epoch: 6| Step: 11
Training loss: 2.0225579738616943
Validation loss: 2.2682285155019453

Epoch: 6| Step: 12
Training loss: 2.8143322467803955
Validation loss: 2.2672318489320817

Epoch: 6| Step: 13
Training loss: 2.141200304031372
Validation loss: 2.2659490890400384

Epoch: 240| Step: 0
Training loss: 2.0321590900421143
Validation loss: 2.2644660985598

Epoch: 6| Step: 1
Training loss: 2.0247836112976074
Validation loss: 2.2778982693149197

Epoch: 6| Step: 2
Training loss: 2.5389208793640137
Validation loss: 2.2773528688697406

Epoch: 6| Step: 3
Training loss: 2.4091033935546875
Validation loss: 2.29808565878099

Epoch: 6| Step: 4
Training loss: 2.4333484172821045
Validation loss: 2.2966815297321608

Epoch: 6| Step: 5
Training loss: 2.281662940979004
Validation loss: 2.2966552062701155

Epoch: 6| Step: 6
Training loss: 2.380991220474243
Validation loss: 2.3002577699640745

Epoch: 6| Step: 7
Training loss: 2.800779342651367
Validation loss: 2.2917757470120668

Epoch: 6| Step: 8
Training loss: 3.6106631755828857
Validation loss: 2.2757399594911965

Epoch: 6| Step: 9
Training loss: 2.6240339279174805
Validation loss: 2.265994615452264

Epoch: 6| Step: 10
Training loss: 2.0888006687164307
Validation loss: 2.2609391315009004

Epoch: 6| Step: 11
Training loss: 3.3916711807250977
Validation loss: 2.2419883461408716

Epoch: 6| Step: 12
Training loss: 1.7322003841400146
Validation loss: 2.23942131380881

Epoch: 6| Step: 13
Training loss: 2.643454074859619
Validation loss: 2.2493267751509145

Epoch: 241| Step: 0
Training loss: 2.9471521377563477
Validation loss: 2.2415019517303794

Epoch: 6| Step: 1
Training loss: 2.4153714179992676
Validation loss: 2.2397450221482145

Epoch: 6| Step: 2
Training loss: 2.6943929195404053
Validation loss: 2.2470240849320606

Epoch: 6| Step: 3
Training loss: 1.595794677734375
Validation loss: 2.2479413452968804

Epoch: 6| Step: 4
Training loss: 2.2753889560699463
Validation loss: 2.251102783346689

Epoch: 6| Step: 5
Training loss: 1.6369962692260742
Validation loss: 2.251785086047265

Epoch: 6| Step: 6
Training loss: 3.1094682216644287
Validation loss: 2.2666106813697406

Epoch: 6| Step: 7
Training loss: 2.808729887008667
Validation loss: 2.310972853373456

Epoch: 6| Step: 8
Training loss: 2.383518695831299
Validation loss: 2.3171635853346957

Epoch: 6| Step: 9
Training loss: 2.9179368019104004
Validation loss: 2.327437098308276

Epoch: 6| Step: 10
Training loss: 2.427234172821045
Validation loss: 2.348470928848431

Epoch: 6| Step: 11
Training loss: 2.6580123901367188
Validation loss: 2.3311972771921465

Epoch: 6| Step: 12
Training loss: 2.5340261459350586
Validation loss: 2.3102291476341987

Epoch: 6| Step: 13
Training loss: 2.6459197998046875
Validation loss: 2.2990931208415697

Epoch: 242| Step: 0
Training loss: 3.20141339302063
Validation loss: 2.283056684719619

Epoch: 6| Step: 1
Training loss: 2.3363680839538574
Validation loss: 2.268207485957812

Epoch: 6| Step: 2
Training loss: 2.525622606277466
Validation loss: 2.2770833943479802

Epoch: 6| Step: 3
Training loss: 2.7782180309295654
Validation loss: 2.2577715637863323

Epoch: 6| Step: 4
Training loss: 1.8430054187774658
Validation loss: 2.248349061576269

Epoch: 6| Step: 5
Training loss: 2.2132396697998047
Validation loss: 2.2534464841247885

Epoch: 6| Step: 6
Training loss: 3.0600221157073975
Validation loss: 2.2530307436502106

Epoch: 6| Step: 7
Training loss: 1.6025906801223755
Validation loss: 2.2537918142093125

Epoch: 6| Step: 8
Training loss: 2.3587965965270996
Validation loss: 2.255587129182713

Epoch: 6| Step: 9
Training loss: 2.357649564743042
Validation loss: 2.250078611476447

Epoch: 6| Step: 10
Training loss: 2.590038776397705
Validation loss: 2.2634148238807597

Epoch: 6| Step: 11
Training loss: 3.1888279914855957
Validation loss: 2.256822040004115

Epoch: 6| Step: 12
Training loss: 2.3581204414367676
Validation loss: 2.242665062668503

Epoch: 6| Step: 13
Training loss: 2.217331886291504
Validation loss: 2.2506483780440463

Epoch: 243| Step: 0
Training loss: 2.1651716232299805
Validation loss: 2.251009605264151

Epoch: 6| Step: 1
Training loss: 2.1480534076690674
Validation loss: 2.2507685204987884

Epoch: 6| Step: 2
Training loss: 2.9490933418273926
Validation loss: 2.2522183541328675

Epoch: 6| Step: 3
Training loss: 2.8804306983947754
Validation loss: 2.248962704853345

Epoch: 6| Step: 4
Training loss: 2.507460355758667
Validation loss: 2.258836902597899

Epoch: 6| Step: 5
Training loss: 2.09718656539917
Validation loss: 2.25745379283864

Epoch: 6| Step: 6
Training loss: 1.8537846803665161
Validation loss: 2.2606106496626333

Epoch: 6| Step: 7
Training loss: 2.2054975032806396
Validation loss: 2.268709580103556

Epoch: 6| Step: 8
Training loss: 1.712754249572754
Validation loss: 2.2940699669622604

Epoch: 6| Step: 9
Training loss: 3.0709521770477295
Validation loss: 2.2970824856911936

Epoch: 6| Step: 10
Training loss: 2.3498101234436035
Validation loss: 2.2901035201164985

Epoch: 6| Step: 11
Training loss: 2.3847742080688477
Validation loss: 2.288020036553824

Epoch: 6| Step: 12
Training loss: 3.310058116912842
Validation loss: 2.280084171602803

Epoch: 6| Step: 13
Training loss: 3.456449508666992
Validation loss: 2.2825802872257848

Epoch: 244| Step: 0
Training loss: 2.6807737350463867
Validation loss: 2.264890524648851

Epoch: 6| Step: 1
Training loss: 2.222374439239502
Validation loss: 2.2646561335491877

Epoch: 6| Step: 2
Training loss: 2.9704675674438477
Validation loss: 2.263507007270731

Epoch: 6| Step: 3
Training loss: 2.0863070487976074
Validation loss: 2.2535180609713317

Epoch: 6| Step: 4
Training loss: 2.41975736618042
Validation loss: 2.252474146504556

Epoch: 6| Step: 5
Training loss: 3.023634433746338
Validation loss: 2.2518692913875786

Epoch: 6| Step: 6
Training loss: 3.168884038925171
Validation loss: 2.242284773498453

Epoch: 6| Step: 7
Training loss: 2.750199317932129
Validation loss: 2.2434390052672355

Epoch: 6| Step: 8
Training loss: 2.3156991004943848
Validation loss: 2.248269183661348

Epoch: 6| Step: 9
Training loss: 1.8143666982650757
Validation loss: 2.2403247766597296

Epoch: 6| Step: 10
Training loss: 2.3088502883911133
Validation loss: 2.250012279838644

Epoch: 6| Step: 11
Training loss: 2.0713417530059814
Validation loss: 2.24428577064186

Epoch: 6| Step: 12
Training loss: 2.261007785797119
Validation loss: 2.2445471543137745

Epoch: 6| Step: 13
Training loss: 2.6820766925811768
Validation loss: 2.2563863620963147

Epoch: 245| Step: 0
Training loss: 2.3193678855895996
Validation loss: 2.2630216178073677

Epoch: 6| Step: 1
Training loss: 1.9629253149032593
Validation loss: 2.2640484917548394

Epoch: 6| Step: 2
Training loss: 2.3484296798706055
Validation loss: 2.288400491078695

Epoch: 6| Step: 3
Training loss: 2.884186029434204
Validation loss: 2.3220910410727225

Epoch: 6| Step: 4
Training loss: 1.8986636400222778
Validation loss: 2.3128828105106147

Epoch: 6| Step: 5
Training loss: 2.672574520111084
Validation loss: 2.287707690269716

Epoch: 6| Step: 6
Training loss: 2.0651423931121826
Validation loss: 2.2889512790146695

Epoch: 6| Step: 7
Training loss: 2.6986637115478516
Validation loss: 2.2915333317172144

Epoch: 6| Step: 8
Training loss: 2.2808520793914795
Validation loss: 2.276421723827239

Epoch: 6| Step: 9
Training loss: 3.182985782623291
Validation loss: 2.268492970415341

Epoch: 6| Step: 10
Training loss: 1.9294681549072266
Validation loss: 2.2598658736034105

Epoch: 6| Step: 11
Training loss: 2.5431442260742188
Validation loss: 2.257739267041606

Epoch: 6| Step: 12
Training loss: 3.0361790657043457
Validation loss: 2.271456109580173

Epoch: 6| Step: 13
Training loss: 3.2441911697387695
Validation loss: 2.2566262060596096

Epoch: 246| Step: 0
Training loss: 3.4204421043395996
Validation loss: 2.265060224840718

Epoch: 6| Step: 1
Training loss: 1.8750789165496826
Validation loss: 2.268256659148842

Epoch: 6| Step: 2
Training loss: 1.5146281719207764
Validation loss: 2.265285579107141

Epoch: 6| Step: 3
Training loss: 3.671769618988037
Validation loss: 2.2643345940497612

Epoch: 6| Step: 4
Training loss: 1.7335267066955566
Validation loss: 2.2534107879925798

Epoch: 6| Step: 5
Training loss: 2.2423887252807617
Validation loss: 2.26697733325343

Epoch: 6| Step: 6
Training loss: 2.082040548324585
Validation loss: 2.2604344557690363

Epoch: 6| Step: 7
Training loss: 2.9851012229919434
Validation loss: 2.2550589076934324

Epoch: 6| Step: 8
Training loss: 1.8574411869049072
Validation loss: 2.2585839251036286

Epoch: 6| Step: 9
Training loss: 1.7725653648376465
Validation loss: 2.2668901156353694

Epoch: 6| Step: 10
Training loss: 2.806802749633789
Validation loss: 2.263348358933644

Epoch: 6| Step: 11
Training loss: 3.2609074115753174
Validation loss: 2.252445673429838

Epoch: 6| Step: 12
Training loss: 2.4936511516571045
Validation loss: 2.2553320828304497

Epoch: 6| Step: 13
Training loss: 3.2795605659484863
Validation loss: 2.252626424194664

Epoch: 247| Step: 0
Training loss: 2.425645351409912
Validation loss: 2.2388294537862143

Epoch: 6| Step: 1
Training loss: 2.6687347888946533
Validation loss: 2.251204982880623

Epoch: 6| Step: 2
Training loss: 1.8653357028961182
Validation loss: 2.25888539514234

Epoch: 6| Step: 3
Training loss: 2.3061630725860596
Validation loss: 2.2811027188454904

Epoch: 6| Step: 4
Training loss: 2.569152355194092
Validation loss: 2.3121221116794053

Epoch: 6| Step: 5
Training loss: 2.3314566612243652
Validation loss: 2.3298334255013415

Epoch: 6| Step: 6
Training loss: 3.163856029510498
Validation loss: 2.3018158994695193

Epoch: 6| Step: 7
Training loss: 1.7271746397018433
Validation loss: 2.276856806970412

Epoch: 6| Step: 8
Training loss: 2.7223145961761475
Validation loss: 2.267042424089165

Epoch: 6| Step: 9
Training loss: 2.8201522827148438
Validation loss: 2.255567185340389

Epoch: 6| Step: 10
Training loss: 2.4776735305786133
Validation loss: 2.260602362694279

Epoch: 6| Step: 11
Training loss: 2.9023122787475586
Validation loss: 2.253008122085243

Epoch: 6| Step: 12
Training loss: 2.6845335960388184
Validation loss: 2.2473305950882616

Epoch: 6| Step: 13
Training loss: 1.6732981204986572
Validation loss: 2.2585025551498576

Epoch: 248| Step: 0
Training loss: 2.5606741905212402
Validation loss: 2.253597056993874

Epoch: 6| Step: 1
Training loss: 2.3834757804870605
Validation loss: 2.2533606072907806

Epoch: 6| Step: 2
Training loss: 2.557986259460449
Validation loss: 2.256541000899448

Epoch: 6| Step: 3
Training loss: 2.5595407485961914
Validation loss: 2.2655015158396896

Epoch: 6| Step: 4
Training loss: 2.436171531677246
Validation loss: 2.2624528523414367

Epoch: 6| Step: 5
Training loss: 2.4642200469970703
Validation loss: 2.2682217474906676

Epoch: 6| Step: 6
Training loss: 2.794377326965332
Validation loss: 2.276262593525712

Epoch: 6| Step: 7
Training loss: 1.7103943824768066
Validation loss: 2.2807301769974413

Epoch: 6| Step: 8
Training loss: 2.7265191078186035
Validation loss: 2.280934141528222

Epoch: 6| Step: 9
Training loss: 2.7486815452575684
Validation loss: 2.2931096861439366

Epoch: 6| Step: 10
Training loss: 2.7150607109069824
Validation loss: 2.2958759171988374

Epoch: 6| Step: 11
Training loss: 1.9098615646362305
Validation loss: 2.291985752762005

Epoch: 6| Step: 12
Training loss: 2.9181160926818848
Validation loss: 2.2668651675665252

Epoch: 6| Step: 13
Training loss: 1.803112506866455
Validation loss: 2.254611071719918

Epoch: 249| Step: 0
Training loss: 2.1582283973693848
Validation loss: 2.2507519798894084

Epoch: 6| Step: 1
Training loss: 2.295896053314209
Validation loss: 2.2481711808071343

Epoch: 6| Step: 2
Training loss: 2.890562057495117
Validation loss: 2.2418128162302

Epoch: 6| Step: 3
Training loss: 2.7118277549743652
Validation loss: 2.2435292633630897

Epoch: 6| Step: 4
Training loss: 2.348026752471924
Validation loss: 2.2401990018865114

Epoch: 6| Step: 5
Training loss: 2.935697078704834
Validation loss: 2.233154530166298

Epoch: 6| Step: 6
Training loss: 2.2076098918914795
Validation loss: 2.234934896551153

Epoch: 6| Step: 7
Training loss: 1.7325425148010254
Validation loss: 2.2555102789273827

Epoch: 6| Step: 8
Training loss: 2.0368056297302246
Validation loss: 2.262158801478724

Epoch: 6| Step: 9
Training loss: 3.458547592163086
Validation loss: 2.2891065971825713

Epoch: 6| Step: 10
Training loss: 2.133953809738159
Validation loss: 2.2814877469052552

Epoch: 6| Step: 11
Training loss: 2.9184179306030273
Validation loss: 2.2777909053269254

Epoch: 6| Step: 12
Training loss: 1.954490303993225
Validation loss: 2.2863048532957673

Epoch: 6| Step: 13
Training loss: 3.091648817062378
Validation loss: 2.279409962315713

Epoch: 250| Step: 0
Training loss: 1.9975476264953613
Validation loss: 2.272734635619707

Epoch: 6| Step: 1
Training loss: 3.0637991428375244
Validation loss: 2.265992595303443

Epoch: 6| Step: 2
Training loss: 2.568861484527588
Validation loss: 2.266468240368751

Epoch: 6| Step: 3
Training loss: 2.210041046142578
Validation loss: 2.2627848861038045

Epoch: 6| Step: 4
Training loss: 2.6883597373962402
Validation loss: 2.2567450961759015

Epoch: 6| Step: 5
Training loss: 2.7652335166931152
Validation loss: 2.2555021060410367

Epoch: 6| Step: 6
Training loss: 2.3825550079345703
Validation loss: 2.249039565363238

Epoch: 6| Step: 7
Training loss: 2.494675397872925
Validation loss: 2.2543038757898475

Epoch: 6| Step: 8
Training loss: 2.591413974761963
Validation loss: 2.2550806691569667

Epoch: 6| Step: 9
Training loss: 2.125722885131836
Validation loss: 2.2654594926423925

Epoch: 6| Step: 10
Training loss: 1.9118034839630127
Validation loss: 2.2568668396242204

Epoch: 6| Step: 11
Training loss: 3.0054843425750732
Validation loss: 2.2645568950201875

Epoch: 6| Step: 12
Training loss: 2.073727607727051
Validation loss: 2.259039801935996

Epoch: 6| Step: 13
Training loss: 2.6426424980163574
Validation loss: 2.2576555898112636

Epoch: 251| Step: 0
Training loss: 3.533661365509033
Validation loss: 2.2613773935584613

Epoch: 6| Step: 1
Training loss: 2.4836182594299316
Validation loss: 2.2663223615256687

Epoch: 6| Step: 2
Training loss: 2.8969616889953613
Validation loss: 2.267751601434523

Epoch: 6| Step: 3
Training loss: 2.279949426651001
Validation loss: 2.2724647932155158

Epoch: 6| Step: 4
Training loss: 2.780231475830078
Validation loss: 2.262548885037822

Epoch: 6| Step: 5
Training loss: 2.4880294799804688
Validation loss: 2.2851895722009803

Epoch: 6| Step: 6
Training loss: 1.8622058629989624
Validation loss: 2.275557324450503

Epoch: 6| Step: 7
Training loss: 2.7136406898498535
Validation loss: 2.291896568831577

Epoch: 6| Step: 8
Training loss: 1.921033263206482
Validation loss: 2.288856021819576

Epoch: 6| Step: 9
Training loss: 1.7237352132797241
Validation loss: 2.2954079592099754

Epoch: 6| Step: 10
Training loss: 2.5319230556488037
Validation loss: 2.306497420034101

Epoch: 6| Step: 11
Training loss: 2.004727363586426
Validation loss: 2.2917482186389226

Epoch: 6| Step: 12
Training loss: 2.7551307678222656
Validation loss: 2.2739649049697386

Epoch: 6| Step: 13
Training loss: 2.4863505363464355
Validation loss: 2.2750531217103362

Epoch: 252| Step: 0
Training loss: 2.8424363136291504
Validation loss: 2.26628698841218

Epoch: 6| Step: 1
Training loss: 2.623006820678711
Validation loss: 2.2447512611266105

Epoch: 6| Step: 2
Training loss: 2.4331607818603516
Validation loss: 2.248733451289515

Epoch: 6| Step: 3
Training loss: 2.87347674369812
Validation loss: 2.2541935264423327

Epoch: 6| Step: 4
Training loss: 2.3194785118103027
Validation loss: 2.241485790539813

Epoch: 6| Step: 5
Training loss: 2.420321226119995
Validation loss: 2.2394359137422297

Epoch: 6| Step: 6
Training loss: 2.4645841121673584
Validation loss: 2.236379361921741

Epoch: 6| Step: 7
Training loss: 2.438241958618164
Validation loss: 2.240556852791899

Epoch: 6| Step: 8
Training loss: 2.3596417903900146
Validation loss: 2.2479424758624007

Epoch: 6| Step: 9
Training loss: 2.2880444526672363
Validation loss: 2.2511354902739167

Epoch: 6| Step: 10
Training loss: 2.799607276916504
Validation loss: 2.2667054540367535

Epoch: 6| Step: 11
Training loss: 2.828132152557373
Validation loss: 2.2585342186753468

Epoch: 6| Step: 12
Training loss: 1.5352287292480469
Validation loss: 2.277709271318169

Epoch: 6| Step: 13
Training loss: 2.1680424213409424
Validation loss: 2.2818476948686826

Epoch: 253| Step: 0
Training loss: 3.198526620864868
Validation loss: 2.314846561801049

Epoch: 6| Step: 1
Training loss: 2.867225170135498
Validation loss: 2.3237352114851757

Epoch: 6| Step: 2
Training loss: 2.8410634994506836
Validation loss: 2.32104568840355

Epoch: 6| Step: 3
Training loss: 2.6958179473876953
Validation loss: 2.3152082761128745

Epoch: 6| Step: 4
Training loss: 2.934830904006958
Validation loss: 2.2967983138176704

Epoch: 6| Step: 5
Training loss: 2.2730565071105957
Validation loss: 2.2887505818438787

Epoch: 6| Step: 6
Training loss: 1.785089135169983
Validation loss: 2.2527287032014582

Epoch: 6| Step: 7
Training loss: 1.9509716033935547
Validation loss: 2.249761314802272

Epoch: 6| Step: 8
Training loss: 2.3804454803466797
Validation loss: 2.243960144699261

Epoch: 6| Step: 9
Training loss: 2.5080666542053223
Validation loss: 2.245271964739728

Epoch: 6| Step: 10
Training loss: 2.575840473175049
Validation loss: 2.239145063584851

Epoch: 6| Step: 11
Training loss: 1.739877700805664
Validation loss: 2.238895006077264

Epoch: 6| Step: 12
Training loss: 2.4643168449401855
Validation loss: 2.239423264739334

Epoch: 6| Step: 13
Training loss: 2.389932155609131
Validation loss: 2.2340079148610434

Epoch: 254| Step: 0
Training loss: 1.9955161809921265
Validation loss: 2.2428903066983787

Epoch: 6| Step: 1
Training loss: 2.44136381149292
Validation loss: 2.2705966529025825

Epoch: 6| Step: 2
Training loss: 1.6899120807647705
Validation loss: 2.2696423287032754

Epoch: 6| Step: 3
Training loss: 2.54727840423584
Validation loss: 2.2880379717837096

Epoch: 6| Step: 4
Training loss: 2.898937225341797
Validation loss: 2.2832492256677277

Epoch: 6| Step: 5
Training loss: 2.479297637939453
Validation loss: 2.2789562107414327

Epoch: 6| Step: 6
Training loss: 2.0183393955230713
Validation loss: 2.2671501559595906

Epoch: 6| Step: 7
Training loss: 2.2469990253448486
Validation loss: 2.2765189370801373

Epoch: 6| Step: 8
Training loss: 2.216425895690918
Validation loss: 2.2677792067168863

Epoch: 6| Step: 9
Training loss: 3.486679792404175
Validation loss: 2.2552822648838

Epoch: 6| Step: 10
Training loss: 2.563300609588623
Validation loss: 2.260884297791348

Epoch: 6| Step: 11
Training loss: 2.6739187240600586
Validation loss: 2.2517715782247563

Epoch: 6| Step: 12
Training loss: 2.601378917694092
Validation loss: 2.257712400087746

Epoch: 6| Step: 13
Training loss: 2.5367441177368164
Validation loss: 2.2574104673119

Epoch: 255| Step: 0
Training loss: 3.046039581298828
Validation loss: 2.24380906679297

Epoch: 6| Step: 1
Training loss: 2.9977924823760986
Validation loss: 2.253243060522182

Epoch: 6| Step: 2
Training loss: 1.8122296333312988
Validation loss: 2.268776565469721

Epoch: 6| Step: 3
Training loss: 2.702515125274658
Validation loss: 2.264569067185925

Epoch: 6| Step: 4
Training loss: 2.4008922576904297
Validation loss: 2.2659598550488873

Epoch: 6| Step: 5
Training loss: 2.3671646118164062
Validation loss: 2.2660874653888006

Epoch: 6| Step: 6
Training loss: 2.3961541652679443
Validation loss: 2.2590940947173745

Epoch: 6| Step: 7
Training loss: 2.8238296508789062
Validation loss: 2.2683587920281196

Epoch: 6| Step: 8
Training loss: 1.8957490921020508
Validation loss: 2.2590947971549085

Epoch: 6| Step: 9
Training loss: 2.4089746475219727
Validation loss: 2.280744949976603

Epoch: 6| Step: 10
Training loss: 1.9260172843933105
Validation loss: 2.295293608019429

Epoch: 6| Step: 11
Training loss: 1.7540357112884521
Validation loss: 2.292578310094854

Epoch: 6| Step: 12
Training loss: 3.6919517517089844
Validation loss: 2.296932876750987

Epoch: 6| Step: 13
Training loss: 2.196593761444092
Validation loss: 2.3021215725970525

Epoch: 256| Step: 0
Training loss: 2.26827335357666
Validation loss: 2.2844207914926673

Epoch: 6| Step: 1
Training loss: 2.0759177207946777
Validation loss: 2.2545616242193405

Epoch: 6| Step: 2
Training loss: 2.1153206825256348
Validation loss: 2.245225111643473

Epoch: 6| Step: 3
Training loss: 2.1460647583007812
Validation loss: 2.2422151027187223

Epoch: 6| Step: 4
Training loss: 2.9235734939575195
Validation loss: 2.2531053327745005

Epoch: 6| Step: 5
Training loss: 2.252822160720825
Validation loss: 2.23523889177589

Epoch: 6| Step: 6
Training loss: 2.5577964782714844
Validation loss: 2.2430586353425057

Epoch: 6| Step: 7
Training loss: 2.7875418663024902
Validation loss: 2.2454536396970033

Epoch: 6| Step: 8
Training loss: 2.262641668319702
Validation loss: 2.228662234480663

Epoch: 6| Step: 9
Training loss: 2.8833425045013428
Validation loss: 2.23442385273595

Epoch: 6| Step: 10
Training loss: 2.45682430267334
Validation loss: 2.2334491924573014

Epoch: 6| Step: 11
Training loss: 2.1101603507995605
Validation loss: 2.2263841039390972

Epoch: 6| Step: 12
Training loss: 3.1322667598724365
Validation loss: 2.2386759019667104

Epoch: 6| Step: 13
Training loss: 2.5143442153930664
Validation loss: 2.232279505780948

Epoch: 257| Step: 0
Training loss: 2.220327854156494
Validation loss: 2.230463320209134

Epoch: 6| Step: 1
Training loss: 1.994184970855713
Validation loss: 2.2433156531344176

Epoch: 6| Step: 2
Training loss: 2.6400368213653564
Validation loss: 2.2607901942345405

Epoch: 6| Step: 3
Training loss: 1.6051673889160156
Validation loss: 2.2594472772331646

Epoch: 6| Step: 4
Training loss: 1.8351874351501465
Validation loss: 2.2540014789950464

Epoch: 6| Step: 5
Training loss: 2.7598347663879395
Validation loss: 2.2429078984004196

Epoch: 6| Step: 6
Training loss: 2.3655078411102295
Validation loss: 2.2448083918581725

Epoch: 6| Step: 7
Training loss: 2.668625831604004
Validation loss: 2.228392513849402

Epoch: 6| Step: 8
Training loss: 2.5110905170440674
Validation loss: 2.2220074604916316

Epoch: 6| Step: 9
Training loss: 2.6789700984954834
Validation loss: 2.2116241506350938

Epoch: 6| Step: 10
Training loss: 2.8313002586364746
Validation loss: 2.2178446836369012

Epoch: 6| Step: 11
Training loss: 2.6937947273254395
Validation loss: 2.2251113460909937

Epoch: 6| Step: 12
Training loss: 3.449911594390869
Validation loss: 2.2237388087857153

Epoch: 6| Step: 13
Training loss: 1.8972570896148682
Validation loss: 2.235198623390608

Epoch: 258| Step: 0
Training loss: 2.5539135932922363
Validation loss: 2.2372373944969586

Epoch: 6| Step: 1
Training loss: 2.4808743000030518
Validation loss: 2.2315710180549213

Epoch: 6| Step: 2
Training loss: 2.6203691959381104
Validation loss: 2.230189226006949

Epoch: 6| Step: 3
Training loss: 2.363830327987671
Validation loss: 2.22674088580634

Epoch: 6| Step: 4
Training loss: 1.9914052486419678
Validation loss: 2.2424449484835387

Epoch: 6| Step: 5
Training loss: 2.4397106170654297
Validation loss: 2.234985186207679

Epoch: 6| Step: 6
Training loss: 2.3539037704467773
Validation loss: 2.236180931009272

Epoch: 6| Step: 7
Training loss: 2.9319286346435547
Validation loss: 2.242783275983667

Epoch: 6| Step: 8
Training loss: 1.7773215770721436
Validation loss: 2.2473951437140025

Epoch: 6| Step: 9
Training loss: 2.993555784225464
Validation loss: 2.2630754478516115

Epoch: 6| Step: 10
Training loss: 1.9523977041244507
Validation loss: 2.2814977399764524

Epoch: 6| Step: 11
Training loss: 2.1112964153289795
Validation loss: 2.289972646262056

Epoch: 6| Step: 12
Training loss: 2.931955575942993
Validation loss: 2.2820677449626308

Epoch: 6| Step: 13
Training loss: 2.840228319168091
Validation loss: 2.2811786641356764

Epoch: 259| Step: 0
Training loss: 2.460526943206787
Validation loss: 2.289763883877826

Epoch: 6| Step: 1
Training loss: 2.2736976146698
Validation loss: 2.2593137397561023

Epoch: 6| Step: 2
Training loss: 2.4553439617156982
Validation loss: 2.269636400284306

Epoch: 6| Step: 3
Training loss: 2.8961684703826904
Validation loss: 2.2708250399558776

Epoch: 6| Step: 4
Training loss: 2.8232858180999756
Validation loss: 2.2494704133720806

Epoch: 6| Step: 5
Training loss: 1.9866068363189697
Validation loss: 2.244584056638902

Epoch: 6| Step: 6
Training loss: 2.9344215393066406
Validation loss: 2.2462864665574926

Epoch: 6| Step: 7
Training loss: 2.5105667114257812
Validation loss: 2.251800858846275

Epoch: 6| Step: 8
Training loss: 1.8890244960784912
Validation loss: 2.2362803925750074

Epoch: 6| Step: 9
Training loss: 2.1120948791503906
Validation loss: 2.252911667669973

Epoch: 6| Step: 10
Training loss: 2.453381061553955
Validation loss: 2.2346661834306616

Epoch: 6| Step: 11
Training loss: 3.270580768585205
Validation loss: 2.229350810409874

Epoch: 6| Step: 12
Training loss: 1.9374622106552124
Validation loss: 2.2254743806777464

Epoch: 6| Step: 13
Training loss: 1.9679170846939087
Validation loss: 2.2432819143418343

Epoch: 260| Step: 0
Training loss: 2.414583683013916
Validation loss: 2.230982824038434

Epoch: 6| Step: 1
Training loss: 2.7271904945373535
Validation loss: 2.234757584910239

Epoch: 6| Step: 2
Training loss: 1.6533572673797607
Validation loss: 2.2323646494137344

Epoch: 6| Step: 3
Training loss: 3.073887586593628
Validation loss: 2.2528471664715837

Epoch: 6| Step: 4
Training loss: 3.002617359161377
Validation loss: 2.246188571376185

Epoch: 6| Step: 5
Training loss: 2.5668182373046875
Validation loss: 2.269140448621524

Epoch: 6| Step: 6
Training loss: 2.277015447616577
Validation loss: 2.279675973358975

Epoch: 6| Step: 7
Training loss: 3.4150562286376953
Validation loss: 2.283715025071175

Epoch: 6| Step: 8
Training loss: 1.8046075105667114
Validation loss: 2.274141493663993

Epoch: 6| Step: 9
Training loss: 1.9707400798797607
Validation loss: 2.2567807012988674

Epoch: 6| Step: 10
Training loss: 1.8643410205841064
Validation loss: 2.248308671418057

Epoch: 6| Step: 11
Training loss: 2.2206966876983643
Validation loss: 2.2448818606715046

Epoch: 6| Step: 12
Training loss: 2.747541904449463
Validation loss: 2.2421958523411907

Epoch: 6| Step: 13
Training loss: 2.3951425552368164
Validation loss: 2.2458777414855136

Epoch: 261| Step: 0
Training loss: 2.4147868156433105
Validation loss: 2.2441423913483978

Epoch: 6| Step: 1
Training loss: 2.2795491218566895
Validation loss: 2.239246924718221

Epoch: 6| Step: 2
Training loss: 2.267449378967285
Validation loss: 2.2333883213740524

Epoch: 6| Step: 3
Training loss: 2.9411909580230713
Validation loss: 2.237145500798379

Epoch: 6| Step: 4
Training loss: 2.738393545150757
Validation loss: 2.2251535025976037

Epoch: 6| Step: 5
Training loss: 1.702010154724121
Validation loss: 2.2300476053709626

Epoch: 6| Step: 6
Training loss: 1.9748940467834473
Validation loss: 2.234019348698278

Epoch: 6| Step: 7
Training loss: 2.4700021743774414
Validation loss: 2.222170373444916

Epoch: 6| Step: 8
Training loss: 2.326127529144287
Validation loss: 2.2308104243329776

Epoch: 6| Step: 9
Training loss: 2.604295492172241
Validation loss: 2.2236874923911145

Epoch: 6| Step: 10
Training loss: 2.9915366172790527
Validation loss: 2.2407014677601476

Epoch: 6| Step: 11
Training loss: 2.222350597381592
Validation loss: 2.2397547409098637

Epoch: 6| Step: 12
Training loss: 2.99812912940979
Validation loss: 2.254825238258608

Epoch: 6| Step: 13
Training loss: 2.1945724487304688
Validation loss: 2.255460698117492

Epoch: 262| Step: 0
Training loss: 2.1892855167388916
Validation loss: 2.2697809062978274

Epoch: 6| Step: 1
Training loss: 2.037982702255249
Validation loss: 2.28379149078041

Epoch: 6| Step: 2
Training loss: 2.2255733013153076
Validation loss: 2.2705087507924726

Epoch: 6| Step: 3
Training loss: 3.028379440307617
Validation loss: 2.2585107959726805

Epoch: 6| Step: 4
Training loss: 1.8854024410247803
Validation loss: 2.2546650799371863

Epoch: 6| Step: 5
Training loss: 2.586944580078125
Validation loss: 2.2519622438697406

Epoch: 6| Step: 6
Training loss: 2.4861350059509277
Validation loss: 2.2212889399579776

Epoch: 6| Step: 7
Training loss: 2.383253335952759
Validation loss: 2.2357342063739734

Epoch: 6| Step: 8
Training loss: 2.097304344177246
Validation loss: 2.2312118802019345

Epoch: 6| Step: 9
Training loss: 3.0309865474700928
Validation loss: 2.2202730204469416

Epoch: 6| Step: 10
Training loss: 2.6546411514282227
Validation loss: 2.240192154402374

Epoch: 6| Step: 11
Training loss: 2.416637420654297
Validation loss: 2.2377801351649786

Epoch: 6| Step: 12
Training loss: 2.546851634979248
Validation loss: 2.2404985068946757

Epoch: 6| Step: 13
Training loss: 2.7234764099121094
Validation loss: 2.2541039118202786

Epoch: 263| Step: 0
Training loss: 2.477280616760254
Validation loss: 2.2651677182925645

Epoch: 6| Step: 1
Training loss: 3.0750925540924072
Validation loss: 2.2607050685472387

Epoch: 6| Step: 2
Training loss: 2.490360736846924
Validation loss: 2.275375839202635

Epoch: 6| Step: 3
Training loss: 2.5283360481262207
Validation loss: 2.2714368361298756

Epoch: 6| Step: 4
Training loss: 2.775512218475342
Validation loss: 2.251597155806839

Epoch: 6| Step: 5
Training loss: 2.080334424972534
Validation loss: 2.2608902146739345

Epoch: 6| Step: 6
Training loss: 2.4524354934692383
Validation loss: 2.2434575814072804

Epoch: 6| Step: 7
Training loss: 2.788550615310669
Validation loss: 2.2415394936838458

Epoch: 6| Step: 8
Training loss: 1.4548379182815552
Validation loss: 2.2356922523949736

Epoch: 6| Step: 9
Training loss: 2.3392677307128906
Validation loss: 2.2368359527280255

Epoch: 6| Step: 10
Training loss: 2.5146448612213135
Validation loss: 2.2435816616140385

Epoch: 6| Step: 11
Training loss: 1.9767487049102783
Validation loss: 2.2492059110313334

Epoch: 6| Step: 12
Training loss: 2.579564094543457
Validation loss: 2.2542803133687666

Epoch: 6| Step: 13
Training loss: 2.6852657794952393
Validation loss: 2.2481104968696513

Epoch: 264| Step: 0
Training loss: 2.451343536376953
Validation loss: 2.251171886280019

Epoch: 6| Step: 1
Training loss: 2.57775616645813
Validation loss: 2.256816610213249

Epoch: 6| Step: 2
Training loss: 1.9209972620010376
Validation loss: 2.276536344200052

Epoch: 6| Step: 3
Training loss: 2.0790281295776367
Validation loss: 2.257229294828189

Epoch: 6| Step: 4
Training loss: 2.365875482559204
Validation loss: 2.281566554500211

Epoch: 6| Step: 5
Training loss: 2.0813589096069336
Validation loss: 2.2662679251804145

Epoch: 6| Step: 6
Training loss: 2.655989646911621
Validation loss: 2.25912223580063

Epoch: 6| Step: 7
Training loss: 2.7726197242736816
Validation loss: 2.270618804039494

Epoch: 6| Step: 8
Training loss: 2.4072604179382324
Validation loss: 2.256278012388496

Epoch: 6| Step: 9
Training loss: 2.227390766143799
Validation loss: 2.243926771225468

Epoch: 6| Step: 10
Training loss: 2.590815782546997
Validation loss: 2.248754075778428

Epoch: 6| Step: 11
Training loss: 2.5545542240142822
Validation loss: 2.2385679893596198

Epoch: 6| Step: 12
Training loss: 2.3522987365722656
Validation loss: 2.241555103691675

Epoch: 6| Step: 13
Training loss: 3.356215238571167
Validation loss: 2.2399677948285173

Epoch: 265| Step: 0
Training loss: 2.8061509132385254
Validation loss: 2.23236616452535

Epoch: 6| Step: 1
Training loss: 2.8323211669921875
Validation loss: 2.226548538413099

Epoch: 6| Step: 2
Training loss: 2.7528557777404785
Validation loss: 2.218188957501483

Epoch: 6| Step: 3
Training loss: 2.253016233444214
Validation loss: 2.2220652359788136

Epoch: 6| Step: 4
Training loss: 2.5957679748535156
Validation loss: 2.2198989365690496

Epoch: 6| Step: 5
Training loss: 2.1710948944091797
Validation loss: 2.226511210523626

Epoch: 6| Step: 6
Training loss: 2.7628164291381836
Validation loss: 2.2248679104671685

Epoch: 6| Step: 7
Training loss: 2.105609655380249
Validation loss: 2.226090220994847

Epoch: 6| Step: 8
Training loss: 2.1662991046905518
Validation loss: 2.226356270492718

Epoch: 6| Step: 9
Training loss: 2.521789073944092
Validation loss: 2.229015668233236

Epoch: 6| Step: 10
Training loss: 2.0425117015838623
Validation loss: 2.265880464225687

Epoch: 6| Step: 11
Training loss: 2.2353124618530273
Validation loss: 2.265884599378032

Epoch: 6| Step: 12
Training loss: 2.5760321617126465
Validation loss: 2.29074199481677

Epoch: 6| Step: 13
Training loss: 2.2085297107696533
Validation loss: 2.2917050264214955

Epoch: 266| Step: 0
Training loss: 1.5921692848205566
Validation loss: 2.287284640855687

Epoch: 6| Step: 1
Training loss: 2.4174323081970215
Validation loss: 2.2734375384546097

Epoch: 6| Step: 2
Training loss: 2.488567352294922
Validation loss: 2.277756501269597

Epoch: 6| Step: 3
Training loss: 2.3259944915771484
Validation loss: 2.2657879091078237

Epoch: 6| Step: 4
Training loss: 3.113800525665283
Validation loss: 2.2598120474046275

Epoch: 6| Step: 5
Training loss: 2.42928409576416
Validation loss: 2.257285548794654

Epoch: 6| Step: 6
Training loss: 2.628816604614258
Validation loss: 2.2605346095177437

Epoch: 6| Step: 7
Training loss: 2.343721389770508
Validation loss: 2.2582288224210023

Epoch: 6| Step: 8
Training loss: 2.927626132965088
Validation loss: 2.2526688678290254

Epoch: 6| Step: 9
Training loss: 1.662988543510437
Validation loss: 2.2399239155554

Epoch: 6| Step: 10
Training loss: 2.4573652744293213
Validation loss: 2.223587918025191

Epoch: 6| Step: 11
Training loss: 2.1253833770751953
Validation loss: 2.2442603508631387

Epoch: 6| Step: 12
Training loss: 2.989164352416992
Validation loss: 2.228763372667374

Epoch: 6| Step: 13
Training loss: 2.587297201156616
Validation loss: 2.230683857394803

Epoch: 267| Step: 0
Training loss: 2.579134702682495
Validation loss: 2.2273719618397374

Epoch: 6| Step: 1
Training loss: 2.6311397552490234
Validation loss: 2.2325619266879175

Epoch: 6| Step: 2
Training loss: 2.2729833126068115
Validation loss: 2.2223267670600646

Epoch: 6| Step: 3
Training loss: 2.2400455474853516
Validation loss: 2.2215653234912502

Epoch: 6| Step: 4
Training loss: 2.7073593139648438
Validation loss: 2.2282785625867945

Epoch: 6| Step: 5
Training loss: 2.6482274532318115
Validation loss: 2.2377569380626885

Epoch: 6| Step: 6
Training loss: 2.7934410572052
Validation loss: 2.2360143789681057

Epoch: 6| Step: 7
Training loss: 2.4491419792175293
Validation loss: 2.2529536139580513

Epoch: 6| Step: 8
Training loss: 2.2362003326416016
Validation loss: 2.2550048161578435

Epoch: 6| Step: 9
Training loss: 2.8089590072631836
Validation loss: 2.2367585961536696

Epoch: 6| Step: 10
Training loss: 2.2318010330200195
Validation loss: 2.2250891218903246

Epoch: 6| Step: 11
Training loss: 2.0440282821655273
Validation loss: 2.2096130207020748

Epoch: 6| Step: 12
Training loss: 2.0165657997131348
Validation loss: 2.2152715216400805

Epoch: 6| Step: 13
Training loss: 2.4358537197113037
Validation loss: 2.2280723330795125

Epoch: 268| Step: 0
Training loss: 2.6419148445129395
Validation loss: 2.2431011276860393

Epoch: 6| Step: 1
Training loss: 2.1930160522460938
Validation loss: 2.2331191262891217

Epoch: 6| Step: 2
Training loss: 3.3369393348693848
Validation loss: 2.251427132596252

Epoch: 6| Step: 3
Training loss: 1.9980379343032837
Validation loss: 2.26732567305206

Epoch: 6| Step: 4
Training loss: 2.4137816429138184
Validation loss: 2.257868536057011

Epoch: 6| Step: 5
Training loss: 2.0872435569763184
Validation loss: 2.2521503663832143

Epoch: 6| Step: 6
Training loss: 2.9377551078796387
Validation loss: 2.261571370145326

Epoch: 6| Step: 7
Training loss: 2.4008889198303223
Validation loss: 2.259829459651824

Epoch: 6| Step: 8
Training loss: 2.1790266036987305
Validation loss: 2.2402389946804253

Epoch: 6| Step: 9
Training loss: 2.7643775939941406
Validation loss: 2.2438778928531113

Epoch: 6| Step: 10
Training loss: 2.6684417724609375
Validation loss: 2.2383221016135266

Epoch: 6| Step: 11
Training loss: 2.4223592281341553
Validation loss: 2.24075686418882

Epoch: 6| Step: 12
Training loss: 1.7221481800079346
Validation loss: 2.2547837021530315

Epoch: 6| Step: 13
Training loss: 1.9568781852722168
Validation loss: 2.2619058509026804

Epoch: 269| Step: 0
Training loss: 2.7523880004882812
Validation loss: 2.2728310272257817

Epoch: 6| Step: 1
Training loss: 2.589275360107422
Validation loss: 2.2819798300343175

Epoch: 6| Step: 2
Training loss: 2.935624122619629
Validation loss: 2.2990965638109433

Epoch: 6| Step: 3
Training loss: 3.095898151397705
Validation loss: 2.297028144200643

Epoch: 6| Step: 4
Training loss: 2.4672114849090576
Validation loss: 2.2948239900732554

Epoch: 6| Step: 5
Training loss: 2.483804702758789
Validation loss: 2.28439918128393

Epoch: 6| Step: 6
Training loss: 1.6333701610565186
Validation loss: 2.2603551956915084

Epoch: 6| Step: 7
Training loss: 2.2498931884765625
Validation loss: 2.2466418948224796

Epoch: 6| Step: 8
Training loss: 2.6657934188842773
Validation loss: 2.219802656481343

Epoch: 6| Step: 9
Training loss: 2.1178717613220215
Validation loss: 2.219410434845955

Epoch: 6| Step: 10
Training loss: 2.448805332183838
Validation loss: 2.2261185184601815

Epoch: 6| Step: 11
Training loss: 2.1980724334716797
Validation loss: 2.2199280569630284

Epoch: 6| Step: 12
Training loss: 2.016453981399536
Validation loss: 2.213154759458316

Epoch: 6| Step: 13
Training loss: 2.156639575958252
Validation loss: 2.2204081768630655

Epoch: 270| Step: 0
Training loss: 2.2046284675598145
Validation loss: 2.228956400707204

Epoch: 6| Step: 1
Training loss: 2.8410935401916504
Validation loss: 2.2279482862000823

Epoch: 6| Step: 2
Training loss: 2.8057022094726562
Validation loss: 2.2329754214132986

Epoch: 6| Step: 3
Training loss: 2.082688331604004
Validation loss: 2.232652771857477

Epoch: 6| Step: 4
Training loss: 2.4152731895446777
Validation loss: 2.252843333828834

Epoch: 6| Step: 5
Training loss: 2.0029029846191406
Validation loss: 2.2601746974452848

Epoch: 6| Step: 6
Training loss: 2.0038259029388428
Validation loss: 2.287184683225488

Epoch: 6| Step: 7
Training loss: 2.1468565464019775
Validation loss: 2.318119174690657

Epoch: 6| Step: 8
Training loss: 2.077341318130493
Validation loss: 2.3062650772833053

Epoch: 6| Step: 9
Training loss: 2.8629589080810547
Validation loss: 2.3072128654808126

Epoch: 6| Step: 10
Training loss: 2.187166213989258
Validation loss: 2.2733703659426783

Epoch: 6| Step: 11
Training loss: 2.5870158672332764
Validation loss: 2.245279727443572

Epoch: 6| Step: 12
Training loss: 3.179914951324463
Validation loss: 2.2293895406107747

Epoch: 6| Step: 13
Training loss: 2.7907094955444336
Validation loss: 2.218283196931244

Epoch: 271| Step: 0
Training loss: 2.850656509399414
Validation loss: 2.217150988117341

Epoch: 6| Step: 1
Training loss: 2.765650749206543
Validation loss: 2.216326951980591

Epoch: 6| Step: 2
Training loss: 2.1850802898406982
Validation loss: 2.210788026932747

Epoch: 6| Step: 3
Training loss: 1.8076972961425781
Validation loss: 2.223708574489881

Epoch: 6| Step: 4
Training loss: 2.8078222274780273
Validation loss: 2.2170947751691266

Epoch: 6| Step: 5
Training loss: 2.096569061279297
Validation loss: 2.242947277202401

Epoch: 6| Step: 6
Training loss: 2.0366029739379883
Validation loss: 2.2568185073073193

Epoch: 6| Step: 7
Training loss: 2.0763440132141113
Validation loss: 2.259840529452088

Epoch: 6| Step: 8
Training loss: 2.8909859657287598
Validation loss: 2.2592808713195143

Epoch: 6| Step: 9
Training loss: 2.6622610092163086
Validation loss: 2.2738632796913065

Epoch: 6| Step: 10
Training loss: 2.303351879119873
Validation loss: 2.2620333471605854

Epoch: 6| Step: 11
Training loss: 2.4902095794677734
Validation loss: 2.2569910172493226

Epoch: 6| Step: 12
Training loss: 2.274293899536133
Validation loss: 2.240038446200791

Epoch: 6| Step: 13
Training loss: 2.8420286178588867
Validation loss: 2.2308625072561283

Epoch: 272| Step: 0
Training loss: 2.6908321380615234
Validation loss: 2.2318104928539646

Epoch: 6| Step: 1
Training loss: 1.9416868686676025
Validation loss: 2.2308398241637857

Epoch: 6| Step: 2
Training loss: 2.707303047180176
Validation loss: 2.2247484627590386

Epoch: 6| Step: 3
Training loss: 2.1644134521484375
Validation loss: 2.220082011274112

Epoch: 6| Step: 4
Training loss: 3.3944075107574463
Validation loss: 2.222118978859276

Epoch: 6| Step: 5
Training loss: 2.3854846954345703
Validation loss: 2.220408785727716

Epoch: 6| Step: 6
Training loss: 2.167248249053955
Validation loss: 2.223707790015846

Epoch: 6| Step: 7
Training loss: 1.8308277130126953
Validation loss: 2.223034817685363

Epoch: 6| Step: 8
Training loss: 2.5547828674316406
Validation loss: 2.223221763487785

Epoch: 6| Step: 9
Training loss: 2.9670286178588867
Validation loss: 2.2202598510249967

Epoch: 6| Step: 10
Training loss: 1.9268630743026733
Validation loss: 2.2276285566309446

Epoch: 6| Step: 11
Training loss: 2.5385634899139404
Validation loss: 2.2597563958937124

Epoch: 6| Step: 12
Training loss: 2.198439121246338
Validation loss: 2.2592751826009443

Epoch: 6| Step: 13
Training loss: 2.492140531539917
Validation loss: 2.286468393059187

Epoch: 273| Step: 0
Training loss: 2.437462329864502
Validation loss: 2.2830728125828568

Epoch: 6| Step: 1
Training loss: 2.3300576210021973
Validation loss: 2.2667495422465826

Epoch: 6| Step: 2
Training loss: 2.61787748336792
Validation loss: 2.273025894677767

Epoch: 6| Step: 3
Training loss: 1.895088791847229
Validation loss: 2.2506841459581928

Epoch: 6| Step: 4
Training loss: 2.25233793258667
Validation loss: 2.237510747807

Epoch: 6| Step: 5
Training loss: 2.137721061706543
Validation loss: 2.2337886287320043

Epoch: 6| Step: 6
Training loss: 2.5039303302764893
Validation loss: 2.220384141450287

Epoch: 6| Step: 7
Training loss: 2.482783794403076
Validation loss: 2.214990190280381

Epoch: 6| Step: 8
Training loss: 2.348789691925049
Validation loss: 2.2205091496949554

Epoch: 6| Step: 9
Training loss: 2.4938669204711914
Validation loss: 2.214951422906691

Epoch: 6| Step: 10
Training loss: 2.3435192108154297
Validation loss: 2.2156242580824

Epoch: 6| Step: 11
Training loss: 2.3206825256347656
Validation loss: 2.206009887879895

Epoch: 6| Step: 12
Training loss: 2.476613759994507
Validation loss: 2.227368562452255

Epoch: 6| Step: 13
Training loss: 3.5976028442382812
Validation loss: 2.2017100190603607

Epoch: 274| Step: 0
Training loss: 2.5184695720672607
Validation loss: 2.2192882517332673

Epoch: 6| Step: 1
Training loss: 2.1046645641326904
Validation loss: 2.212487123345816

Epoch: 6| Step: 2
Training loss: 2.22748064994812
Validation loss: 2.217742730212468

Epoch: 6| Step: 3
Training loss: 2.9606244564056396
Validation loss: 2.225168220458492

Epoch: 6| Step: 4
Training loss: 2.1944475173950195
Validation loss: 2.2370150473810013

Epoch: 6| Step: 5
Training loss: 2.1196818351745605
Validation loss: 2.2638635353375505

Epoch: 6| Step: 6
Training loss: 3.2002854347229004
Validation loss: 2.267336473670057

Epoch: 6| Step: 7
Training loss: 2.6663765907287598
Validation loss: 2.275665911295081

Epoch: 6| Step: 8
Training loss: 2.29716157913208
Validation loss: 2.2885189671670236

Epoch: 6| Step: 9
Training loss: 2.6229774951934814
Validation loss: 2.2975557875889603

Epoch: 6| Step: 10
Training loss: 2.2091221809387207
Validation loss: 2.2724993869822514

Epoch: 6| Step: 11
Training loss: 2.123443603515625
Validation loss: 2.2762248208445888

Epoch: 6| Step: 12
Training loss: 2.4379751682281494
Validation loss: 2.257488748078705

Epoch: 6| Step: 13
Training loss: 1.6605910062789917
Validation loss: 2.2396268293421757

Epoch: 275| Step: 0
Training loss: 3.0839529037475586
Validation loss: 2.2190274948714883

Epoch: 6| Step: 1
Training loss: 2.7118396759033203
Validation loss: 2.2154198128690004

Epoch: 6| Step: 2
Training loss: 2.3216896057128906
Validation loss: 2.2150433319871143

Epoch: 6| Step: 3
Training loss: 1.6603747606277466
Validation loss: 2.2194349688868367

Epoch: 6| Step: 4
Training loss: 2.080265998840332
Validation loss: 2.200326329918318

Epoch: 6| Step: 5
Training loss: 2.2471742630004883
Validation loss: 2.205408662878057

Epoch: 6| Step: 6
Training loss: 2.4692580699920654
Validation loss: 2.2180611087429907

Epoch: 6| Step: 7
Training loss: 2.5128321647644043
Validation loss: 2.2176032655982563

Epoch: 6| Step: 8
Training loss: 3.16917085647583
Validation loss: 2.2188640204809045

Epoch: 6| Step: 9
Training loss: 2.1578052043914795
Validation loss: 2.223219879211918

Epoch: 6| Step: 10
Training loss: 1.7438788414001465
Validation loss: 2.2129080795472666

Epoch: 6| Step: 11
Training loss: 3.228557586669922
Validation loss: 2.225203062898369

Epoch: 6| Step: 12
Training loss: 1.859325647354126
Validation loss: 2.211477482190696

Epoch: 6| Step: 13
Training loss: 2.6508560180664062
Validation loss: 2.222971903380527

Epoch: 276| Step: 0
Training loss: 2.581860303878784
Validation loss: 2.2296547466708767

Epoch: 6| Step: 1
Training loss: 2.7268290519714355
Validation loss: 2.222534846234065

Epoch: 6| Step: 2
Training loss: 3.1797914505004883
Validation loss: 2.2419653554116525

Epoch: 6| Step: 3
Training loss: 2.544980525970459
Validation loss: 2.24119964209936

Epoch: 6| Step: 4
Training loss: 1.9370254278182983
Validation loss: 2.2606737126586256

Epoch: 6| Step: 5
Training loss: 2.3132481575012207
Validation loss: 2.2662627850809405

Epoch: 6| Step: 6
Training loss: 2.04921293258667
Validation loss: 2.256845235824585

Epoch: 6| Step: 7
Training loss: 2.198620080947876
Validation loss: 2.255064651530276

Epoch: 6| Step: 8
Training loss: 1.7482061386108398
Validation loss: 2.243972668083765

Epoch: 6| Step: 9
Training loss: 2.5837931632995605
Validation loss: 2.242476542790731

Epoch: 6| Step: 10
Training loss: 2.258615732192993
Validation loss: 2.2258713835029194

Epoch: 6| Step: 11
Training loss: 2.988798141479492
Validation loss: 2.2288652056006977

Epoch: 6| Step: 12
Training loss: 2.6113133430480957
Validation loss: 2.2295884265694568

Epoch: 6| Step: 13
Training loss: 1.758420467376709
Validation loss: 2.2212930828012447

Epoch: 277| Step: 0
Training loss: 1.8253285884857178
Validation loss: 2.2197790312510666

Epoch: 6| Step: 1
Training loss: 3.663784980773926
Validation loss: 2.220233676254108

Epoch: 6| Step: 2
Training loss: 1.690977931022644
Validation loss: 2.2110782002889984

Epoch: 6| Step: 3
Training loss: 2.1587073802948
Validation loss: 2.212609460276942

Epoch: 6| Step: 4
Training loss: 2.512826919555664
Validation loss: 2.2069512656939927

Epoch: 6| Step: 5
Training loss: 1.8321613073349
Validation loss: 2.205532976376113

Epoch: 6| Step: 6
Training loss: 1.8687243461608887
Validation loss: 2.219222850697015

Epoch: 6| Step: 7
Training loss: 2.7996182441711426
Validation loss: 2.2199145850314888

Epoch: 6| Step: 8
Training loss: 3.1864538192749023
Validation loss: 2.2243056733121156

Epoch: 6| Step: 9
Training loss: 2.411057710647583
Validation loss: 2.2259478261393886

Epoch: 6| Step: 10
Training loss: 2.5557565689086914
Validation loss: 2.25494215308979

Epoch: 6| Step: 11
Training loss: 2.6684410572052
Validation loss: 2.2641961702736477

Epoch: 6| Step: 12
Training loss: 2.3080811500549316
Validation loss: 2.251336513027068

Epoch: 6| Step: 13
Training loss: 1.9791842699050903
Validation loss: 2.2422882997861473

Epoch: 278| Step: 0
Training loss: 2.7017247676849365
Validation loss: 2.225755150600146

Epoch: 6| Step: 1
Training loss: 2.3963160514831543
Validation loss: 2.2084864749703357

Epoch: 6| Step: 2
Training loss: 2.2298216819763184
Validation loss: 2.20988412954474

Epoch: 6| Step: 3
Training loss: 2.6446070671081543
Validation loss: 2.2162996248532365

Epoch: 6| Step: 4
Training loss: 2.1247334480285645
Validation loss: 2.2173208523822088

Epoch: 6| Step: 5
Training loss: 2.9282970428466797
Validation loss: 2.217865159434657

Epoch: 6| Step: 6
Training loss: 2.6493277549743652
Validation loss: 2.2249744425537767

Epoch: 6| Step: 7
Training loss: 2.081007242202759
Validation loss: 2.2344978637592767

Epoch: 6| Step: 8
Training loss: 2.264730215072632
Validation loss: 2.22750723361969

Epoch: 6| Step: 9
Training loss: 2.4251089096069336
Validation loss: 2.225432721517419

Epoch: 6| Step: 10
Training loss: 2.0306155681610107
Validation loss: 2.2238841749006704

Epoch: 6| Step: 11
Training loss: 1.9118270874023438
Validation loss: 2.222515451010837

Epoch: 6| Step: 12
Training loss: 3.0642666816711426
Validation loss: 2.2248492497269825

Epoch: 6| Step: 13
Training loss: 2.187530517578125
Validation loss: 2.2180144504834245

Epoch: 279| Step: 0
Training loss: 2.1533350944519043
Validation loss: 2.202100758911461

Epoch: 6| Step: 1
Training loss: 2.498450994491577
Validation loss: 2.208061279789094

Epoch: 6| Step: 2
Training loss: 2.270278215408325
Validation loss: 2.2069896087851575

Epoch: 6| Step: 3
Training loss: 2.943725347518921
Validation loss: 2.211123579291887

Epoch: 6| Step: 4
Training loss: 2.799473285675049
Validation loss: 2.2036040521437124

Epoch: 6| Step: 5
Training loss: 2.0012543201446533
Validation loss: 2.194905441294434

Epoch: 6| Step: 6
Training loss: 2.1143405437469482
Validation loss: 2.2024601774830974

Epoch: 6| Step: 7
Training loss: 2.3932065963745117
Validation loss: 2.215084873219972

Epoch: 6| Step: 8
Training loss: 2.6278765201568604
Validation loss: 2.2336170801552395

Epoch: 6| Step: 9
Training loss: 2.51908278465271
Validation loss: 2.2296805586866153

Epoch: 6| Step: 10
Training loss: 2.8511219024658203
Validation loss: 2.2407091356092885

Epoch: 6| Step: 11
Training loss: 1.8013780117034912
Validation loss: 2.243204911549886

Epoch: 6| Step: 12
Training loss: 2.1528759002685547
Validation loss: 2.2518560835110244

Epoch: 6| Step: 13
Training loss: 2.8160219192504883
Validation loss: 2.2553546761953704

Epoch: 280| Step: 0
Training loss: 2.9345855712890625
Validation loss: 2.2539777114827144

Epoch: 6| Step: 1
Training loss: 2.335794687271118
Validation loss: 2.2504775601048626

Epoch: 6| Step: 2
Training loss: 2.5102665424346924
Validation loss: 2.2306196945969776

Epoch: 6| Step: 3
Training loss: 2.646446704864502
Validation loss: 2.2293692993861374

Epoch: 6| Step: 4
Training loss: 2.226001262664795
Validation loss: 2.2185412619703557

Epoch: 6| Step: 5
Training loss: 2.6690855026245117
Validation loss: 2.214743411669167

Epoch: 6| Step: 6
Training loss: 2.725722312927246
Validation loss: 2.2063485819806337

Epoch: 6| Step: 7
Training loss: 2.3369760513305664
Validation loss: 2.2142192445775515

Epoch: 6| Step: 8
Training loss: 2.7459211349487305
Validation loss: 2.207389405978623

Epoch: 6| Step: 9
Training loss: 1.5575196743011475
Validation loss: 2.2031035692461076

Epoch: 6| Step: 10
Training loss: 2.2399981021881104
Validation loss: 2.2203416362885506

Epoch: 6| Step: 11
Training loss: 2.2745773792266846
Validation loss: 2.2336209127979894

Epoch: 6| Step: 12
Training loss: 2.1138482093811035
Validation loss: 2.2195713673868487

Epoch: 6| Step: 13
Training loss: 2.3646786212921143
Validation loss: 2.2350655063506095

Epoch: 281| Step: 0
Training loss: 2.287334442138672
Validation loss: 2.242273438361383

Epoch: 6| Step: 1
Training loss: 2.4455461502075195
Validation loss: 2.2408980861786874

Epoch: 6| Step: 2
Training loss: 2.942617893218994
Validation loss: 2.2443238906962897

Epoch: 6| Step: 3
Training loss: 2.6617743968963623
Validation loss: 2.2459518678726687

Epoch: 6| Step: 4
Training loss: 1.42763090133667
Validation loss: 2.2355072344503095

Epoch: 6| Step: 5
Training loss: 2.4759068489074707
Validation loss: 2.2444881675063924

Epoch: 6| Step: 6
Training loss: 2.900031089782715
Validation loss: 2.2550129967351116

Epoch: 6| Step: 7
Training loss: 2.8441038131713867
Validation loss: 2.2465811621758247

Epoch: 6| Step: 8
Training loss: 2.8641583919525146
Validation loss: 2.2326827536347094

Epoch: 6| Step: 9
Training loss: 2.8815197944641113
Validation loss: 2.235738149253271

Epoch: 6| Step: 10
Training loss: 1.6653099060058594
Validation loss: 2.2124764150188816

Epoch: 6| Step: 11
Training loss: 1.3815839290618896
Validation loss: 2.2054203274429485

Epoch: 6| Step: 12
Training loss: 1.9066457748413086
Validation loss: 2.208341060146209

Epoch: 6| Step: 13
Training loss: 3.1186017990112305
Validation loss: 2.199844503915438

Epoch: 282| Step: 0
Training loss: 1.9879165887832642
Validation loss: 2.2066893269938808

Epoch: 6| Step: 1
Training loss: 2.2878036499023438
Validation loss: 2.209943086870255

Epoch: 6| Step: 2
Training loss: 1.9126129150390625
Validation loss: 2.2170421269632157

Epoch: 6| Step: 3
Training loss: 2.3835158348083496
Validation loss: 2.2311397867818035

Epoch: 6| Step: 4
Training loss: 2.9430041313171387
Validation loss: 2.227604842955066

Epoch: 6| Step: 5
Training loss: 2.3447844982147217
Validation loss: 2.233770026955553

Epoch: 6| Step: 6
Training loss: 2.7622904777526855
Validation loss: 2.2286755936120146

Epoch: 6| Step: 7
Training loss: 2.0575897693634033
Validation loss: 2.2185473749714513

Epoch: 6| Step: 8
Training loss: 2.2565810680389404
Validation loss: 2.196664000070223

Epoch: 6| Step: 9
Training loss: 2.309314250946045
Validation loss: 2.213842009985319

Epoch: 6| Step: 10
Training loss: 2.7708098888397217
Validation loss: 2.2337856446543047

Epoch: 6| Step: 11
Training loss: 2.7911505699157715
Validation loss: 2.237637445490847

Epoch: 6| Step: 12
Training loss: 2.386460065841675
Validation loss: 2.2448422690873504

Epoch: 6| Step: 13
Training loss: 2.4027998447418213
Validation loss: 2.246393907454706

Epoch: 283| Step: 0
Training loss: 1.9962921142578125
Validation loss: 2.2533371845881143

Epoch: 6| Step: 1
Training loss: 2.3502092361450195
Validation loss: 2.2845662050349738

Epoch: 6| Step: 2
Training loss: 2.4389824867248535
Validation loss: 2.3019738940782446

Epoch: 6| Step: 3
Training loss: 1.4262691736221313
Validation loss: 2.301689624786377

Epoch: 6| Step: 4
Training loss: 2.8520405292510986
Validation loss: 2.317552952356236

Epoch: 6| Step: 5
Training loss: 2.1569302082061768
Validation loss: 2.3284233770062848

Epoch: 6| Step: 6
Training loss: 2.888223171234131
Validation loss: 2.3131544923269622

Epoch: 6| Step: 7
Training loss: 2.87717342376709
Validation loss: 2.26366667080951

Epoch: 6| Step: 8
Training loss: 2.2198338508605957
Validation loss: 2.248745149181735

Epoch: 6| Step: 9
Training loss: 2.7938876152038574
Validation loss: 2.225244078584897

Epoch: 6| Step: 10
Training loss: 2.0126867294311523
Validation loss: 2.217127416723518

Epoch: 6| Step: 11
Training loss: 2.507582187652588
Validation loss: 2.2196458744746383

Epoch: 6| Step: 12
Training loss: 2.5644824504852295
Validation loss: 2.2050193586657123

Epoch: 6| Step: 13
Training loss: 2.915762186050415
Validation loss: 2.1958412534447125

Epoch: 284| Step: 0
Training loss: 2.1466214656829834
Validation loss: 2.1853112892438005

Epoch: 6| Step: 1
Training loss: 2.4292876720428467
Validation loss: 2.19235590452789

Epoch: 6| Step: 2
Training loss: 2.5070536136627197
Validation loss: 2.2002873907807055

Epoch: 6| Step: 3
Training loss: 3.049884796142578
Validation loss: 2.2013917200026976

Epoch: 6| Step: 4
Training loss: 2.5038392543792725
Validation loss: 2.2006907796347015

Epoch: 6| Step: 5
Training loss: 1.9140276908874512
Validation loss: 2.198305591460197

Epoch: 6| Step: 6
Training loss: 2.2954866886138916
Validation loss: 2.2074556940345356

Epoch: 6| Step: 7
Training loss: 2.2257046699523926
Validation loss: 2.223606432637861

Epoch: 6| Step: 8
Training loss: 1.6212599277496338
Validation loss: 2.229631221422585

Epoch: 6| Step: 9
Training loss: 2.317173719406128
Validation loss: 2.272693157196045

Epoch: 6| Step: 10
Training loss: 2.322603702545166
Validation loss: 2.2991808152967885

Epoch: 6| Step: 11
Training loss: 2.565812826156616
Validation loss: 2.316170738589379

Epoch: 6| Step: 12
Training loss: 2.95218825340271
Validation loss: 2.347211527568038

Epoch: 6| Step: 13
Training loss: 3.0226213932037354
Validation loss: 2.3392012760203373

Epoch: 285| Step: 0
Training loss: 2.7786402702331543
Validation loss: 2.307326829561623

Epoch: 6| Step: 1
Training loss: 2.0706074237823486
Validation loss: 2.272525715571578

Epoch: 6| Step: 2
Training loss: 2.0899534225463867
Validation loss: 2.2340798313899706

Epoch: 6| Step: 3
Training loss: 2.559718370437622
Validation loss: 2.2222464494807745

Epoch: 6| Step: 4
Training loss: 2.396812915802002
Validation loss: 2.220293087344016

Epoch: 6| Step: 5
Training loss: 2.7993898391723633
Validation loss: 2.1973645097465924

Epoch: 6| Step: 6
Training loss: 2.379767894744873
Validation loss: 2.186896934304186

Epoch: 6| Step: 7
Training loss: 2.1684505939483643
Validation loss: 2.1866145723609516

Epoch: 6| Step: 8
Training loss: 2.599256992340088
Validation loss: 2.1876338476775796

Epoch: 6| Step: 9
Training loss: 2.5961480140686035
Validation loss: 2.195475175816526

Epoch: 6| Step: 10
Training loss: 1.9223312139511108
Validation loss: 2.1858802328827562

Epoch: 6| Step: 11
Training loss: 2.4364259243011475
Validation loss: 2.1809651492744364

Epoch: 6| Step: 12
Training loss: 2.4665489196777344
Validation loss: 2.187178534846152

Epoch: 6| Step: 13
Training loss: 2.151881217956543
Validation loss: 2.1958701661838

Epoch: 286| Step: 0
Training loss: 2.746708869934082
Validation loss: 2.2093484863158195

Epoch: 6| Step: 1
Training loss: 2.164430618286133
Validation loss: 2.213466595577937

Epoch: 6| Step: 2
Training loss: 2.5377707481384277
Validation loss: 2.2327418211967713

Epoch: 6| Step: 3
Training loss: 2.4449572563171387
Validation loss: 2.235425510714131

Epoch: 6| Step: 4
Training loss: 1.6818556785583496
Validation loss: 2.228198471889701

Epoch: 6| Step: 5
Training loss: 2.8702876567840576
Validation loss: 2.2394210805175123

Epoch: 6| Step: 6
Training loss: 2.2976951599121094
Validation loss: 2.237283028582091

Epoch: 6| Step: 7
Training loss: 1.7580254077911377
Validation loss: 2.2351706540712746

Epoch: 6| Step: 8
Training loss: 2.1233320236206055
Validation loss: 2.2451457438930387

Epoch: 6| Step: 9
Training loss: 2.865691661834717
Validation loss: 2.2454846315486456

Epoch: 6| Step: 10
Training loss: 2.230571985244751
Validation loss: 2.269212010086224

Epoch: 6| Step: 11
Training loss: 2.4655637741088867
Validation loss: 2.2776992936288156

Epoch: 6| Step: 12
Training loss: 2.6294636726379395
Validation loss: 2.2740615208943686

Epoch: 6| Step: 13
Training loss: 2.413175344467163
Validation loss: 2.2758773962656655

Epoch: 287| Step: 0
Training loss: 1.8733452558517456
Validation loss: 2.250259425050469

Epoch: 6| Step: 1
Training loss: 2.6524412631988525
Validation loss: 2.222115821735833

Epoch: 6| Step: 2
Training loss: 3.4479732513427734
Validation loss: 2.2239370422978557

Epoch: 6| Step: 3
Training loss: 2.2669379711151123
Validation loss: 2.213352657133533

Epoch: 6| Step: 4
Training loss: 2.1689562797546387
Validation loss: 2.2057084985958633

Epoch: 6| Step: 5
Training loss: 2.374964714050293
Validation loss: 2.2033968510166293

Epoch: 6| Step: 6
Training loss: 2.6626136302948
Validation loss: 2.205488015246648

Epoch: 6| Step: 7
Training loss: 1.9023338556289673
Validation loss: 2.195926134304334

Epoch: 6| Step: 8
Training loss: 2.597616672515869
Validation loss: 2.1916932828964724

Epoch: 6| Step: 9
Training loss: 2.3010754585266113
Validation loss: 2.2130744149607997

Epoch: 6| Step: 10
Training loss: 2.382941246032715
Validation loss: 2.219132036291143

Epoch: 6| Step: 11
Training loss: 2.736999750137329
Validation loss: 2.2305710956614506

Epoch: 6| Step: 12
Training loss: 1.6855590343475342
Validation loss: 2.2408126246544624

Epoch: 6| Step: 13
Training loss: 2.2106828689575195
Validation loss: 2.2903788730662358

Epoch: 288| Step: 0
Training loss: 2.8061652183532715
Validation loss: 2.3141767055757585

Epoch: 6| Step: 1
Training loss: 2.3629932403564453
Validation loss: 2.3107990218747045

Epoch: 6| Step: 2
Training loss: 2.312601089477539
Validation loss: 2.2990337469244517

Epoch: 6| Step: 3
Training loss: 1.9656307697296143
Validation loss: 2.271533032899262

Epoch: 6| Step: 4
Training loss: 2.381502151489258
Validation loss: 2.265228243284328

Epoch: 6| Step: 5
Training loss: 1.4673547744750977
Validation loss: 2.227849457853584

Epoch: 6| Step: 6
Training loss: 1.8436583280563354
Validation loss: 2.208262352533238

Epoch: 6| Step: 7
Training loss: 1.8160816431045532
Validation loss: 2.208314323938021

Epoch: 6| Step: 8
Training loss: 2.930464506149292
Validation loss: 2.199184407470047

Epoch: 6| Step: 9
Training loss: 2.229339838027954
Validation loss: 2.1852736267992245

Epoch: 6| Step: 10
Training loss: 3.0439538955688477
Validation loss: 2.2048980625726844

Epoch: 6| Step: 11
Training loss: 2.941941022872925
Validation loss: 2.1932067153274373

Epoch: 6| Step: 12
Training loss: 2.957320213317871
Validation loss: 2.2088639838721162

Epoch: 6| Step: 13
Training loss: 1.93545663356781
Validation loss: 2.1987234238655335

Epoch: 289| Step: 0
Training loss: 2.4004762172698975
Validation loss: 2.2071657078240507

Epoch: 6| Step: 1
Training loss: 2.244352102279663
Validation loss: 2.2143936695591098

Epoch: 6| Step: 2
Training loss: 2.325716972351074
Validation loss: 2.212940844156409

Epoch: 6| Step: 3
Training loss: 2.216907501220703
Validation loss: 2.227852488076815

Epoch: 6| Step: 4
Training loss: 3.445075511932373
Validation loss: 2.2451129574929514

Epoch: 6| Step: 5
Training loss: 2.407557249069214
Validation loss: 2.2354845718670915

Epoch: 6| Step: 6
Training loss: 1.9635164737701416
Validation loss: 2.2358282099487963

Epoch: 6| Step: 7
Training loss: 2.282580852508545
Validation loss: 2.235532260710193

Epoch: 6| Step: 8
Training loss: 2.974710702896118
Validation loss: 2.2151894825761036

Epoch: 6| Step: 9
Training loss: 1.7111200094223022
Validation loss: 2.235448441197795

Epoch: 6| Step: 10
Training loss: 1.8717741966247559
Validation loss: 2.2269697343149493

Epoch: 6| Step: 11
Training loss: 2.5450453758239746
Validation loss: 2.2510166270758516

Epoch: 6| Step: 12
Training loss: 2.4073266983032227
Validation loss: 2.267561517735963

Epoch: 6| Step: 13
Training loss: 2.308724880218506
Validation loss: 2.2660215054788897

Epoch: 290| Step: 0
Training loss: 2.2864012718200684
Validation loss: 2.2678539163322857

Epoch: 6| Step: 1
Training loss: 3.099684715270996
Validation loss: 2.257041185132919

Epoch: 6| Step: 2
Training loss: 3.034241199493408
Validation loss: 2.2430013918107554

Epoch: 6| Step: 3
Training loss: 2.0772147178649902
Validation loss: 2.231923190496301

Epoch: 6| Step: 4
Training loss: 2.085604667663574
Validation loss: 2.222459780272617

Epoch: 6| Step: 5
Training loss: 2.1310935020446777
Validation loss: 2.214108431211082

Epoch: 6| Step: 6
Training loss: 2.0078511238098145
Validation loss: 2.210621920965051

Epoch: 6| Step: 7
Training loss: 3.2211103439331055
Validation loss: 2.200760474769018

Epoch: 6| Step: 8
Training loss: 2.498715877532959
Validation loss: 2.1937649198757705

Epoch: 6| Step: 9
Training loss: 2.161294460296631
Validation loss: 2.20044054267227

Epoch: 6| Step: 10
Training loss: 1.9566758871078491
Validation loss: 2.2134635935547533

Epoch: 6| Step: 11
Training loss: 1.7884328365325928
Validation loss: 2.2293499080083703

Epoch: 6| Step: 12
Training loss: 2.678192138671875
Validation loss: 2.2486631754905946

Epoch: 6| Step: 13
Training loss: 2.0180134773254395
Validation loss: 2.2715954011486423

Epoch: 291| Step: 0
Training loss: 2.423616886138916
Validation loss: 2.244280976633872

Epoch: 6| Step: 1
Training loss: 2.642070770263672
Validation loss: 2.2504975898291475

Epoch: 6| Step: 2
Training loss: 1.7861648797988892
Validation loss: 2.2261515355879262

Epoch: 6| Step: 3
Training loss: 2.954350471496582
Validation loss: 2.2266747079869753

Epoch: 6| Step: 4
Training loss: 2.322335958480835
Validation loss: 2.227089769096785

Epoch: 6| Step: 5
Training loss: 2.4878506660461426
Validation loss: 2.2103502878578762

Epoch: 6| Step: 6
Training loss: 2.3211495876312256
Validation loss: 2.2018667574851745

Epoch: 6| Step: 7
Training loss: 2.8604540824890137
Validation loss: 2.1947254750036422

Epoch: 6| Step: 8
Training loss: 2.152175188064575
Validation loss: 2.2094412644704184

Epoch: 6| Step: 9
Training loss: 2.0900120735168457
Validation loss: 2.1894644050187964

Epoch: 6| Step: 10
Training loss: 2.1452746391296387
Validation loss: 2.1986287845078336

Epoch: 6| Step: 11
Training loss: 2.155701160430908
Validation loss: 2.2034177344332457

Epoch: 6| Step: 12
Training loss: 2.7168121337890625
Validation loss: 2.21269198899628

Epoch: 6| Step: 13
Training loss: 1.6568732261657715
Validation loss: 2.2231968756644958

Epoch: 292| Step: 0
Training loss: 2.8833820819854736
Validation loss: 2.2369414465401762

Epoch: 6| Step: 1
Training loss: 1.9946415424346924
Validation loss: 2.2488507942486833

Epoch: 6| Step: 2
Training loss: 2.1710495948791504
Validation loss: 2.270356301338442

Epoch: 6| Step: 3
Training loss: 3.0064196586608887
Validation loss: 2.2701262709914998

Epoch: 6| Step: 4
Training loss: 2.623481512069702
Validation loss: 2.252449507354408

Epoch: 6| Step: 5
Training loss: 2.618274688720703
Validation loss: 2.2378436826890513

Epoch: 6| Step: 6
Training loss: 1.5208079814910889
Validation loss: 2.2253038191026255

Epoch: 6| Step: 7
Training loss: 2.6933469772338867
Validation loss: 2.2199618021647134

Epoch: 6| Step: 8
Training loss: 2.431286096572876
Validation loss: 2.225670142840314

Epoch: 6| Step: 9
Training loss: 2.526468276977539
Validation loss: 2.2461228011756815

Epoch: 6| Step: 10
Training loss: 2.503589391708374
Validation loss: 2.2327179549842753

Epoch: 6| Step: 11
Training loss: 1.8286261558532715
Validation loss: 2.244162021144744

Epoch: 6| Step: 12
Training loss: 2.50901460647583
Validation loss: 2.241963017371393

Epoch: 6| Step: 13
Training loss: 1.5088144540786743
Validation loss: 2.2374429959122852

Epoch: 293| Step: 0
Training loss: 2.5956473350524902
Validation loss: 2.2357072240562847

Epoch: 6| Step: 1
Training loss: 2.209547758102417
Validation loss: 2.216747622336111

Epoch: 6| Step: 2
Training loss: 2.1994199752807617
Validation loss: 2.2307156798660115

Epoch: 6| Step: 3
Training loss: 2.3652098178863525
Validation loss: 2.2322047551472983

Epoch: 6| Step: 4
Training loss: 2.38527512550354
Validation loss: 2.2423552582340855

Epoch: 6| Step: 5
Training loss: 3.111403465270996
Validation loss: 2.257754433539606

Epoch: 6| Step: 6
Training loss: 1.9819400310516357
Validation loss: 2.2262779153803343

Epoch: 6| Step: 7
Training loss: 2.571187973022461
Validation loss: 2.2535199824199883

Epoch: 6| Step: 8
Training loss: 2.3240160942077637
Validation loss: 2.226853407839293

Epoch: 6| Step: 9
Training loss: 2.169161081314087
Validation loss: 2.2091478327269196

Epoch: 6| Step: 10
Training loss: 2.3856842517852783
Validation loss: 2.204084022070772

Epoch: 6| Step: 11
Training loss: 1.5466234683990479
Validation loss: 2.2253874847965855

Epoch: 6| Step: 12
Training loss: 2.0950748920440674
Validation loss: 2.2083938813978627

Epoch: 6| Step: 13
Training loss: 3.5596554279327393
Validation loss: 2.208207030450144

Epoch: 294| Step: 0
Training loss: 2.287464141845703
Validation loss: 2.2136193552324848

Epoch: 6| Step: 1
Training loss: 1.7458641529083252
Validation loss: 2.2048383348731586

Epoch: 6| Step: 2
Training loss: 2.0603554248809814
Validation loss: 2.2355262194910357

Epoch: 6| Step: 3
Training loss: 2.5650644302368164
Validation loss: 2.2468339268879225

Epoch: 6| Step: 4
Training loss: 2.091576099395752
Validation loss: 2.2786719952860186

Epoch: 6| Step: 5
Training loss: 2.796967029571533
Validation loss: 2.291798239113182

Epoch: 6| Step: 6
Training loss: 2.7532081604003906
Validation loss: 2.277287447324363

Epoch: 6| Step: 7
Training loss: 1.8552056550979614
Validation loss: 2.270252912275253

Epoch: 6| Step: 8
Training loss: 2.3247973918914795
Validation loss: 2.260602161448489

Epoch: 6| Step: 9
Training loss: 2.0024242401123047
Validation loss: 2.2595392965501353

Epoch: 6| Step: 10
Training loss: 2.753044605255127
Validation loss: 2.2506578583871164

Epoch: 6| Step: 11
Training loss: 2.343958854675293
Validation loss: 2.219377557436625

Epoch: 6| Step: 12
Training loss: 2.6016550064086914
Validation loss: 2.222294710015738

Epoch: 6| Step: 13
Training loss: 3.2767696380615234
Validation loss: 2.220138006312873

Epoch: 295| Step: 0
Training loss: 3.2859716415405273
Validation loss: 2.231041428863361

Epoch: 6| Step: 1
Training loss: 2.10774827003479
Validation loss: 2.2056373191136185

Epoch: 6| Step: 2
Training loss: 2.990570306777954
Validation loss: 2.2263270936986452

Epoch: 6| Step: 3
Training loss: 3.0393576622009277
Validation loss: 2.232936536112139

Epoch: 6| Step: 4
Training loss: 2.1838486194610596
Validation loss: 2.243525516602301

Epoch: 6| Step: 5
Training loss: 1.8425871133804321
Validation loss: 2.2368079821268716

Epoch: 6| Step: 6
Training loss: 2.2654027938842773
Validation loss: 2.2562776098969164

Epoch: 6| Step: 7
Training loss: 1.684575080871582
Validation loss: 2.2443402710781304

Epoch: 6| Step: 8
Training loss: 2.665627956390381
Validation loss: 2.246003561122443

Epoch: 6| Step: 9
Training loss: 2.116842269897461
Validation loss: 2.240294051426713

Epoch: 6| Step: 10
Training loss: 1.9958171844482422
Validation loss: 2.22148637617788

Epoch: 6| Step: 11
Training loss: 2.2070648670196533
Validation loss: 2.209244796024856

Epoch: 6| Step: 12
Training loss: 2.2843146324157715
Validation loss: 2.2098055219137542

Epoch: 6| Step: 13
Training loss: 2.1959738731384277
Validation loss: 2.206888134761523

Epoch: 296| Step: 0
Training loss: 2.553067684173584
Validation loss: 2.2158398474416425

Epoch: 6| Step: 1
Training loss: 2.093575954437256
Validation loss: 2.2113321314575853

Epoch: 6| Step: 2
Training loss: 2.6050803661346436
Validation loss: 2.1865386757799374

Epoch: 6| Step: 3
Training loss: 2.4983701705932617
Validation loss: 2.1945731537316435

Epoch: 6| Step: 4
Training loss: 2.213709831237793
Validation loss: 2.208549109838342

Epoch: 6| Step: 5
Training loss: 1.0931410789489746
Validation loss: 2.1998548443599413

Epoch: 6| Step: 6
Training loss: 3.102631092071533
Validation loss: 2.220203202257874

Epoch: 6| Step: 7
Training loss: 2.3665273189544678
Validation loss: 2.2196353763662358

Epoch: 6| Step: 8
Training loss: 2.911457061767578
Validation loss: 2.2275321663066907

Epoch: 6| Step: 9
Training loss: 2.421116352081299
Validation loss: 2.229937835406232

Epoch: 6| Step: 10
Training loss: 2.3119988441467285
Validation loss: 2.2251692433511057

Epoch: 6| Step: 11
Training loss: 2.6386237144470215
Validation loss: 2.221164241913826

Epoch: 6| Step: 12
Training loss: 2.0163931846618652
Validation loss: 2.2290534498871013

Epoch: 6| Step: 13
Training loss: 2.0401298999786377
Validation loss: 2.2278564181379092

Epoch: 297| Step: 0
Training loss: 2.6184325218200684
Validation loss: 2.2185102611459713

Epoch: 6| Step: 1
Training loss: 1.595566749572754
Validation loss: 2.219188851694907

Epoch: 6| Step: 2
Training loss: 2.39510440826416
Validation loss: 2.2224894287765666

Epoch: 6| Step: 3
Training loss: 2.2518553733825684
Validation loss: 2.2276458330051874

Epoch: 6| Step: 4
Training loss: 2.6072115898132324
Validation loss: 2.2306173950113277

Epoch: 6| Step: 5
Training loss: 2.4156737327575684
Validation loss: 2.2174512993904854

Epoch: 6| Step: 6
Training loss: 2.1645050048828125
Validation loss: 2.2177557099250054

Epoch: 6| Step: 7
Training loss: 3.494835376739502
Validation loss: 2.2185272657743065

Epoch: 6| Step: 8
Training loss: 1.9030327796936035
Validation loss: 2.227762173580867

Epoch: 6| Step: 9
Training loss: 2.1430764198303223
Validation loss: 2.2284018070467058

Epoch: 6| Step: 10
Training loss: 2.3322768211364746
Validation loss: 2.2196511517288866

Epoch: 6| Step: 11
Training loss: 2.076970338821411
Validation loss: 2.212591637847244

Epoch: 6| Step: 12
Training loss: 2.3411450386047363
Validation loss: 2.206288342834801

Epoch: 6| Step: 13
Training loss: 2.6161398887634277
Validation loss: 2.2147135298739196

Epoch: 298| Step: 0
Training loss: 1.854612946510315
Validation loss: 2.2089300027457615

Epoch: 6| Step: 1
Training loss: 2.6205945014953613
Validation loss: 2.20356390553136

Epoch: 6| Step: 2
Training loss: 2.507120370864868
Validation loss: 2.190004946083151

Epoch: 6| Step: 3
Training loss: 2.2078773975372314
Validation loss: 2.1961928285578245

Epoch: 6| Step: 4
Training loss: 2.995967388153076
Validation loss: 2.19303975823105

Epoch: 6| Step: 5
Training loss: 2.684666872024536
Validation loss: 2.2010899218179847

Epoch: 6| Step: 6
Training loss: 2.4174580574035645
Validation loss: 2.2104740373549925

Epoch: 6| Step: 7
Training loss: 2.4824905395507812
Validation loss: 2.2098262899665424

Epoch: 6| Step: 8
Training loss: 2.2075862884521484
Validation loss: 2.2224037621610906

Epoch: 6| Step: 9
Training loss: 2.4411778450012207
Validation loss: 2.23558376168692

Epoch: 6| Step: 10
Training loss: 1.4805066585540771
Validation loss: 2.238123009281774

Epoch: 6| Step: 11
Training loss: 1.2229149341583252
Validation loss: 2.2363299580030542

Epoch: 6| Step: 12
Training loss: 2.8784008026123047
Validation loss: 2.2015186919960925

Epoch: 6| Step: 13
Training loss: 3.2596278190612793
Validation loss: 2.2113928102677867

Epoch: 299| Step: 0
Training loss: 2.2279510498046875
Validation loss: 2.217262368048391

Epoch: 6| Step: 1
Training loss: 2.032254219055176
Validation loss: 2.240440409670594

Epoch: 6| Step: 2
Training loss: 2.0701451301574707
Validation loss: 2.2531129647326726

Epoch: 6| Step: 3
Training loss: 2.339017868041992
Validation loss: 2.2461155691454486

Epoch: 6| Step: 4
Training loss: 2.7133946418762207
Validation loss: 2.2399634622758433

Epoch: 6| Step: 5
Training loss: 2.6892752647399902
Validation loss: 2.252097842513874

Epoch: 6| Step: 6
Training loss: 3.0006601810455322
Validation loss: 2.232115396889307

Epoch: 6| Step: 7
Training loss: 2.222503662109375
Validation loss: 2.208228926504812

Epoch: 6| Step: 8
Training loss: 2.6072003841400146
Validation loss: 2.2142972074529177

Epoch: 6| Step: 9
Training loss: 1.7950845956802368
Validation loss: 2.1965656024153515

Epoch: 6| Step: 10
Training loss: 1.9951491355895996
Validation loss: 2.190057954480571

Epoch: 6| Step: 11
Training loss: 1.6738176345825195
Validation loss: 2.205605688915458

Epoch: 6| Step: 12
Training loss: 2.601316452026367
Validation loss: 2.1975509274390435

Epoch: 6| Step: 13
Training loss: 3.0016908645629883
Validation loss: 2.2201619763528146

Epoch: 300| Step: 0
Training loss: 2.4836392402648926
Validation loss: 2.2110193365363666

Epoch: 6| Step: 1
Training loss: 1.9676058292388916
Validation loss: 2.2394643701532835

Epoch: 6| Step: 2
Training loss: 1.740290880203247
Validation loss: 2.237655944721673

Epoch: 6| Step: 3
Training loss: 1.8256831169128418
Validation loss: 2.2660194314936155

Epoch: 6| Step: 4
Training loss: 2.3358335494995117
Validation loss: 2.257587827661986

Epoch: 6| Step: 5
Training loss: 2.293328285217285
Validation loss: 2.2670962169606197

Epoch: 6| Step: 6
Training loss: 2.0880966186523438
Validation loss: 2.26882581556997

Epoch: 6| Step: 7
Training loss: 2.2062718868255615
Validation loss: 2.244047867354526

Epoch: 6| Step: 8
Training loss: 2.9480109214782715
Validation loss: 2.2541067420795398

Epoch: 6| Step: 9
Training loss: 2.5059242248535156
Validation loss: 2.251373896034815

Epoch: 6| Step: 10
Training loss: 2.6178441047668457
Validation loss: 2.2483516739260767

Epoch: 6| Step: 11
Training loss: 2.081529140472412
Validation loss: 2.230597226850448

Epoch: 6| Step: 12
Training loss: 2.805081605911255
Validation loss: 2.213366614874973

Epoch: 6| Step: 13
Training loss: 3.0702767372131348
Validation loss: 2.2209624885230936

Epoch: 301| Step: 0
Training loss: 2.5157933235168457
Validation loss: 2.202398512953071

Epoch: 6| Step: 1
Training loss: 2.5268306732177734
Validation loss: 2.1923490519164712

Epoch: 6| Step: 2
Training loss: 2.3587918281555176
Validation loss: 2.180669092362927

Epoch: 6| Step: 3
Training loss: 2.5814743041992188
Validation loss: 2.183912851477182

Epoch: 6| Step: 4
Training loss: 2.7807881832122803
Validation loss: 2.1863132292224514

Epoch: 6| Step: 5
Training loss: 1.683429479598999
Validation loss: 2.182953970406645

Epoch: 6| Step: 6
Training loss: 2.5942554473876953
Validation loss: 2.1917242388571463

Epoch: 6| Step: 7
Training loss: 3.527259349822998
Validation loss: 2.189928006100398

Epoch: 6| Step: 8
Training loss: 2.0855753421783447
Validation loss: 2.194797626105688

Epoch: 6| Step: 9
Training loss: 1.790918231010437
Validation loss: 2.193509571013912

Epoch: 6| Step: 10
Training loss: 2.251375675201416
Validation loss: 2.2201909326737925

Epoch: 6| Step: 11
Training loss: 2.623185157775879
Validation loss: 2.2242436511542207

Epoch: 6| Step: 12
Training loss: 1.2267842292785645
Validation loss: 2.237187190722394

Epoch: 6| Step: 13
Training loss: 2.1339471340179443
Validation loss: 2.242636593439246

Epoch: 302| Step: 0
Training loss: 1.9932661056518555
Validation loss: 2.2470496111018683

Epoch: 6| Step: 1
Training loss: 2.6125192642211914
Validation loss: 2.247289034628099

Epoch: 6| Step: 2
Training loss: 2.1956403255462646
Validation loss: 2.2836024427926667

Epoch: 6| Step: 3
Training loss: 2.4771065711975098
Validation loss: 2.252187559681554

Epoch: 6| Step: 4
Training loss: 2.5268361568450928
Validation loss: 2.2546531615718717

Epoch: 6| Step: 5
Training loss: 2.2194533348083496
Validation loss: 2.2394832616211264

Epoch: 6| Step: 6
Training loss: 2.9409079551696777
Validation loss: 2.222637079095328

Epoch: 6| Step: 7
Training loss: 2.243171215057373
Validation loss: 2.204923757942774

Epoch: 6| Step: 8
Training loss: 1.9827156066894531
Validation loss: 2.1993358519769486

Epoch: 6| Step: 9
Training loss: 1.7955759763717651
Validation loss: 2.1894973529282438

Epoch: 6| Step: 10
Training loss: 2.6901941299438477
Validation loss: 2.1788106451752367

Epoch: 6| Step: 11
Training loss: 2.940917730331421
Validation loss: 2.1808043038973244

Epoch: 6| Step: 12
Training loss: 1.8403124809265137
Validation loss: 2.1915738556974675

Epoch: 6| Step: 13
Training loss: 2.607034921646118
Validation loss: 2.1702359876325055

Epoch: 303| Step: 0
Training loss: 2.2254586219787598
Validation loss: 2.1773231273056357

Epoch: 6| Step: 1
Training loss: 2.7614941596984863
Validation loss: 2.190641913362729

Epoch: 6| Step: 2
Training loss: 2.2810041904449463
Validation loss: 2.197551450421733

Epoch: 6| Step: 3
Training loss: 3.1859354972839355
Validation loss: 2.2153447853621615

Epoch: 6| Step: 4
Training loss: 2.3960931301116943
Validation loss: 2.226279986801968

Epoch: 6| Step: 5
Training loss: 2.2705724239349365
Validation loss: 2.2394178810939995

Epoch: 6| Step: 6
Training loss: 2.534463882446289
Validation loss: 2.2151246634862756

Epoch: 6| Step: 7
Training loss: 2.5184926986694336
Validation loss: 2.2115392736209336

Epoch: 6| Step: 8
Training loss: 1.8902033567428589
Validation loss: 2.2320026377195954

Epoch: 6| Step: 9
Training loss: 2.2236907482147217
Validation loss: 2.2295693082194172

Epoch: 6| Step: 10
Training loss: 2.579740524291992
Validation loss: 2.234959399828347

Epoch: 6| Step: 11
Training loss: 1.7452867031097412
Validation loss: 2.2226855395942606

Epoch: 6| Step: 12
Training loss: 2.4543471336364746
Validation loss: 2.233439696732388

Epoch: 6| Step: 13
Training loss: 1.4735841751098633
Validation loss: 2.2248738452952397

Epoch: 304| Step: 0
Training loss: 2.378918170928955
Validation loss: 2.2278444869543916

Epoch: 6| Step: 1
Training loss: 1.868543028831482
Validation loss: 2.2396161581880305

Epoch: 6| Step: 2
Training loss: 1.9057368040084839
Validation loss: 2.2523688590654762

Epoch: 6| Step: 3
Training loss: 1.9840502738952637
Validation loss: 2.2428567063423896

Epoch: 6| Step: 4
Training loss: 2.3945322036743164
Validation loss: 2.2327502517290014

Epoch: 6| Step: 5
Training loss: 3.351365566253662
Validation loss: 2.2202553390174784

Epoch: 6| Step: 6
Training loss: 1.7936375141143799
Validation loss: 2.201053347638858

Epoch: 6| Step: 7
Training loss: 2.7797744274139404
Validation loss: 2.1938098143505793

Epoch: 6| Step: 8
Training loss: 2.137312889099121
Validation loss: 2.187133873662641

Epoch: 6| Step: 9
Training loss: 2.6182358264923096
Validation loss: 2.1876842001433014

Epoch: 6| Step: 10
Training loss: 1.7077481746673584
Validation loss: 2.2005178902738836

Epoch: 6| Step: 11
Training loss: 2.7163851261138916
Validation loss: 2.1979833238868305

Epoch: 6| Step: 12
Training loss: 2.4410810470581055
Validation loss: 2.212154575573501

Epoch: 6| Step: 13
Training loss: 2.7004454135894775
Validation loss: 2.2115108223371607

Epoch: 305| Step: 0
Training loss: 1.7758122682571411
Validation loss: 2.2093518780123804

Epoch: 6| Step: 1
Training loss: 1.966079592704773
Validation loss: 2.2249536309190976

Epoch: 6| Step: 2
Training loss: 2.388906240463257
Validation loss: 2.2427792741406347

Epoch: 6| Step: 3
Training loss: 2.659475326538086
Validation loss: 2.2534834800227994

Epoch: 6| Step: 4
Training loss: 1.907102108001709
Validation loss: 2.2494787708405526

Epoch: 6| Step: 5
Training loss: 2.4025933742523193
Validation loss: 2.2587517256377847

Epoch: 6| Step: 6
Training loss: 2.745898485183716
Validation loss: 2.255237023035685

Epoch: 6| Step: 7
Training loss: 2.0054500102996826
Validation loss: 2.231895749286939

Epoch: 6| Step: 8
Training loss: 2.9940335750579834
Validation loss: 2.1926181572739796

Epoch: 6| Step: 9
Training loss: 2.089552402496338
Validation loss: 2.203529906529252

Epoch: 6| Step: 10
Training loss: 1.7152177095413208
Validation loss: 2.2014953346662622

Epoch: 6| Step: 11
Training loss: 2.779036521911621
Validation loss: 2.2151033263052664

Epoch: 6| Step: 12
Training loss: 2.91190242767334
Validation loss: 2.2106239564957155

Epoch: 6| Step: 13
Training loss: 2.3052098751068115
Validation loss: 2.2222488221301826

Epoch: 306| Step: 0
Training loss: 2.691722869873047
Validation loss: 2.2255727514143913

Epoch: 6| Step: 1
Training loss: 2.612551689147949
Validation loss: 2.2139037270699777

Epoch: 6| Step: 2
Training loss: 1.7431519031524658
Validation loss: 2.2087459538572576

Epoch: 6| Step: 3
Training loss: 2.3364102840423584
Validation loss: 2.2046645995109313

Epoch: 6| Step: 4
Training loss: 3.0657026767730713
Validation loss: 2.191260004556307

Epoch: 6| Step: 5
Training loss: 2.453372001647949
Validation loss: 2.1924292502864713

Epoch: 6| Step: 6
Training loss: 2.432164430618286
Validation loss: 2.1928838158166535

Epoch: 6| Step: 7
Training loss: 2.7564244270324707
Validation loss: 2.1957155555807133

Epoch: 6| Step: 8
Training loss: 2.203338623046875
Validation loss: 2.206264081821647

Epoch: 6| Step: 9
Training loss: 2.333232879638672
Validation loss: 2.214206741702172

Epoch: 6| Step: 10
Training loss: 2.127913475036621
Validation loss: 2.2321738889140468

Epoch: 6| Step: 11
Training loss: 1.4526097774505615
Validation loss: 2.2401539664114676

Epoch: 6| Step: 12
Training loss: 2.1790781021118164
Validation loss: 2.2518159086986254

Epoch: 6| Step: 13
Training loss: 1.8604302406311035
Validation loss: 2.2548790388209845

Epoch: 307| Step: 0
Training loss: 3.4237284660339355
Validation loss: 2.2712541933982604

Epoch: 6| Step: 1
Training loss: 2.4143800735473633
Validation loss: 2.2860789196465605

Epoch: 6| Step: 2
Training loss: 2.1406702995300293
Validation loss: 2.282366250150947

Epoch: 6| Step: 3
Training loss: 1.3083467483520508
Validation loss: 2.288253361178983

Epoch: 6| Step: 4
Training loss: 2.331597328186035
Validation loss: 2.2933892216733707

Epoch: 6| Step: 5
Training loss: 2.2679519653320312
Validation loss: 2.253108242506622

Epoch: 6| Step: 6
Training loss: 2.5480573177337646
Validation loss: 2.2598689115175636

Epoch: 6| Step: 7
Training loss: 2.8756184577941895
Validation loss: 2.25020747159117

Epoch: 6| Step: 8
Training loss: 2.9584782123565674
Validation loss: 2.210029576414375

Epoch: 6| Step: 9
Training loss: 1.0061519145965576
Validation loss: 2.193561356554749

Epoch: 6| Step: 10
Training loss: 1.823048710823059
Validation loss: 2.193637409517842

Epoch: 6| Step: 11
Training loss: 2.231560707092285
Validation loss: 2.1914660892178937

Epoch: 6| Step: 12
Training loss: 2.8420844078063965
Validation loss: 2.193187831550516

Epoch: 6| Step: 13
Training loss: 2.7874886989593506
Validation loss: 2.198285054135066

Epoch: 308| Step: 0
Training loss: 2.192047595977783
Validation loss: 2.203967978877406

Epoch: 6| Step: 1
Training loss: 3.384754180908203
Validation loss: 2.218922233068815

Epoch: 6| Step: 2
Training loss: 2.1622397899627686
Validation loss: 2.213194436924432

Epoch: 6| Step: 3
Training loss: 2.309021472930908
Validation loss: 2.222297304420061

Epoch: 6| Step: 4
Training loss: 2.3931808471679688
Validation loss: 2.220514153921476

Epoch: 6| Step: 5
Training loss: 1.2052037715911865
Validation loss: 2.2255289554595947

Epoch: 6| Step: 6
Training loss: 2.4433774948120117
Validation loss: 2.24496627366671

Epoch: 6| Step: 7
Training loss: 1.892500638961792
Validation loss: 2.2481771643443773

Epoch: 6| Step: 8
Training loss: 1.9870271682739258
Validation loss: 2.2402157475871425

Epoch: 6| Step: 9
Training loss: 2.1147820949554443
Validation loss: 2.248063862964671

Epoch: 6| Step: 10
Training loss: 3.0305464267730713
Validation loss: 2.249941046519946

Epoch: 6| Step: 11
Training loss: 2.0014383792877197
Validation loss: 2.250930796387375

Epoch: 6| Step: 12
Training loss: 2.8952343463897705
Validation loss: 2.2271636352744153

Epoch: 6| Step: 13
Training loss: 2.8816070556640625
Validation loss: 2.2184181726107033

Epoch: 309| Step: 0
Training loss: 1.874645471572876
Validation loss: 2.2119559703334684

Epoch: 6| Step: 1
Training loss: 2.0366525650024414
Validation loss: 2.2189873751773628

Epoch: 6| Step: 2
Training loss: 1.994433879852295
Validation loss: 2.2182687200525755

Epoch: 6| Step: 3
Training loss: 2.634087085723877
Validation loss: 2.239946162828835

Epoch: 6| Step: 4
Training loss: 2.4114785194396973
Validation loss: 2.248436668867706

Epoch: 6| Step: 5
Training loss: 1.967288851737976
Validation loss: 2.2477487799941853

Epoch: 6| Step: 6
Training loss: 2.3686983585357666
Validation loss: 2.2686086367535334

Epoch: 6| Step: 7
Training loss: 2.3842835426330566
Validation loss: 2.2947617653877503

Epoch: 6| Step: 8
Training loss: 2.328916072845459
Validation loss: 2.2737395968488467

Epoch: 6| Step: 9
Training loss: 3.0825417041778564
Validation loss: 2.277109839582956

Epoch: 6| Step: 10
Training loss: 2.386037826538086
Validation loss: 2.2577620116613244

Epoch: 6| Step: 11
Training loss: 2.37930965423584
Validation loss: 2.2356347460900583

Epoch: 6| Step: 12
Training loss: 1.9478600025177002
Validation loss: 2.2084591850157707

Epoch: 6| Step: 13
Training loss: 3.3316609859466553
Validation loss: 2.2038265838417956

Epoch: 310| Step: 0
Training loss: 2.8755054473876953
Validation loss: 2.183672607585948

Epoch: 6| Step: 1
Training loss: 3.0730185508728027
Validation loss: 2.174308310272873

Epoch: 6| Step: 2
Training loss: 2.211958408355713
Validation loss: 2.16988459966516

Epoch: 6| Step: 3
Training loss: 2.079313039779663
Validation loss: 2.1726656113901446

Epoch: 6| Step: 4
Training loss: 1.965364694595337
Validation loss: 2.1581809469448623

Epoch: 6| Step: 5
Training loss: 1.489179253578186
Validation loss: 2.177076006448397

Epoch: 6| Step: 6
Training loss: 3.171542167663574
Validation loss: 2.1627447143677743

Epoch: 6| Step: 7
Training loss: 2.4677186012268066
Validation loss: 2.1688309946367816

Epoch: 6| Step: 8
Training loss: 2.714754581451416
Validation loss: 2.167108807512509

Epoch: 6| Step: 9
Training loss: 1.6762831211090088
Validation loss: 2.14525431202304

Epoch: 6| Step: 10
Training loss: 2.4838955402374268
Validation loss: 2.1614884330380346

Epoch: 6| Step: 11
Training loss: 2.0671815872192383
Validation loss: 2.1734701946217525

Epoch: 6| Step: 12
Training loss: 2.0660109519958496
Validation loss: 2.1826629202852965

Epoch: 6| Step: 13
Training loss: 3.021915912628174
Validation loss: 2.1918494150202763

Epoch: 311| Step: 0
Training loss: 1.707175374031067
Validation loss: 2.2046266345567602

Epoch: 6| Step: 1
Training loss: 1.9867686033248901
Validation loss: 2.229205159730809

Epoch: 6| Step: 2
Training loss: 1.835094928741455
Validation loss: 2.25568058670208

Epoch: 6| Step: 3
Training loss: 2.220036029815674
Validation loss: 2.278754788060342

Epoch: 6| Step: 4
Training loss: 1.8392267227172852
Validation loss: 2.2568442462592997

Epoch: 6| Step: 5
Training loss: 2.5045344829559326
Validation loss: 2.2630918333607335

Epoch: 6| Step: 6
Training loss: 2.7728230953216553
Validation loss: 2.2359404038357478

Epoch: 6| Step: 7
Training loss: 1.5388333797454834
Validation loss: 2.2580637342186383

Epoch: 6| Step: 8
Training loss: 2.7198519706726074
Validation loss: 2.2299960069758917

Epoch: 6| Step: 9
Training loss: 2.4069406986236572
Validation loss: 2.236460462693245

Epoch: 6| Step: 10
Training loss: 3.1933212280273438
Validation loss: 2.231880013660718

Epoch: 6| Step: 11
Training loss: 1.756796956062317
Validation loss: 2.2282386979749127

Epoch: 6| Step: 12
Training loss: 3.560314655303955
Validation loss: 2.1920077364931823

Epoch: 6| Step: 13
Training loss: 2.497225761413574
Validation loss: 2.2013676243443645

Epoch: 312| Step: 0
Training loss: 2.6455929279327393
Validation loss: 2.1934626358811573

Epoch: 6| Step: 1
Training loss: 2.154797077178955
Validation loss: 2.1724499553762455

Epoch: 6| Step: 2
Training loss: 2.455465078353882
Validation loss: 2.184672935034639

Epoch: 6| Step: 3
Training loss: 2.544923782348633
Validation loss: 2.1826732748298237

Epoch: 6| Step: 4
Training loss: 2.3126392364501953
Validation loss: 2.1742445268938617

Epoch: 6| Step: 5
Training loss: 1.8561301231384277
Validation loss: 2.163714729329591

Epoch: 6| Step: 6
Training loss: 1.9357067346572876
Validation loss: 2.1582753645476473

Epoch: 6| Step: 7
Training loss: 2.3805885314941406
Validation loss: 2.1730436971110683

Epoch: 6| Step: 8
Training loss: 2.0216243267059326
Validation loss: 2.183872128045687

Epoch: 6| Step: 9
Training loss: 2.0318801403045654
Validation loss: 2.1907592306854906

Epoch: 6| Step: 10
Training loss: 2.2929601669311523
Validation loss: 2.2034426722475278

Epoch: 6| Step: 11
Training loss: 2.65187406539917
Validation loss: 2.2335240917821086

Epoch: 6| Step: 12
Training loss: 2.59751033782959
Validation loss: 2.2694594026893697

Epoch: 6| Step: 13
Training loss: 2.9141931533813477
Validation loss: 2.2687611964441117

Epoch: 313| Step: 0
Training loss: 1.342169165611267
Validation loss: 2.2699548352149224

Epoch: 6| Step: 1
Training loss: 2.3673453330993652
Validation loss: 2.249870361820344

Epoch: 6| Step: 2
Training loss: 2.174626350402832
Validation loss: 2.2446331772752988

Epoch: 6| Step: 3
Training loss: 1.9429563283920288
Validation loss: 2.2090290541289956

Epoch: 6| Step: 4
Training loss: 2.7255654335021973
Validation loss: 2.207211796955396

Epoch: 6| Step: 5
Training loss: 2.1568689346313477
Validation loss: 2.2075666663467244

Epoch: 6| Step: 6
Training loss: 2.051846981048584
Validation loss: 2.2032980713793027

Epoch: 6| Step: 7
Training loss: 3.3082973957061768
Validation loss: 2.2264872584291684

Epoch: 6| Step: 8
Training loss: 1.8186955451965332
Validation loss: 2.224278114175284

Epoch: 6| Step: 9
Training loss: 2.1740000247955322
Validation loss: 2.207943086983055

Epoch: 6| Step: 10
Training loss: 2.2252182960510254
Validation loss: 2.2242120273651613

Epoch: 6| Step: 11
Training loss: 2.9660422801971436
Validation loss: 2.2094480196634927

Epoch: 6| Step: 12
Training loss: 2.3292689323425293
Validation loss: 2.2115627399054905

Epoch: 6| Step: 13
Training loss: 3.0518369674682617
Validation loss: 2.2111979966522544

Epoch: 314| Step: 0
Training loss: 2.590360164642334
Validation loss: 2.2098348627808275

Epoch: 6| Step: 1
Training loss: 1.8041019439697266
Validation loss: 2.2100140074247956

Epoch: 6| Step: 2
Training loss: 1.9448851346969604
Validation loss: 2.2077922897954143

Epoch: 6| Step: 3
Training loss: 1.9504435062408447
Validation loss: 2.217926213818212

Epoch: 6| Step: 4
Training loss: 2.0475902557373047
Validation loss: 2.2372392659546225

Epoch: 6| Step: 5
Training loss: 2.1009416580200195
Validation loss: 2.242889286369406

Epoch: 6| Step: 6
Training loss: 2.3277578353881836
Validation loss: 2.2230472154514764

Epoch: 6| Step: 7
Training loss: 2.162687301635742
Validation loss: 2.2189161649314304

Epoch: 6| Step: 8
Training loss: 2.62368106842041
Validation loss: 2.2113126400978333

Epoch: 6| Step: 9
Training loss: 2.127697229385376
Validation loss: 2.2204669342246106

Epoch: 6| Step: 10
Training loss: 2.5057897567749023
Validation loss: 2.204570508772327

Epoch: 6| Step: 11
Training loss: 2.0304372310638428
Validation loss: 2.1952525236273326

Epoch: 6| Step: 12
Training loss: 3.0788955688476562
Validation loss: 2.188164666134824

Epoch: 6| Step: 13
Training loss: 3.4352469444274902
Validation loss: 2.1774242795923704

Epoch: 315| Step: 0
Training loss: 2.338263750076294
Validation loss: 2.1746667046700754

Epoch: 6| Step: 1
Training loss: 2.2615585327148438
Validation loss: 2.158103612161452

Epoch: 6| Step: 2
Training loss: 3.063439130783081
Validation loss: 2.1737595322311565

Epoch: 6| Step: 3
Training loss: 2.3264729976654053
Validation loss: 2.1844453991100354

Epoch: 6| Step: 4
Training loss: 1.8907963037490845
Validation loss: 2.1765261337321293

Epoch: 6| Step: 5
Training loss: 1.8146698474884033
Validation loss: 2.190879697440773

Epoch: 6| Step: 6
Training loss: 3.1115238666534424
Validation loss: 2.2018489594100625

Epoch: 6| Step: 7
Training loss: 2.8151001930236816
Validation loss: 2.190959838128859

Epoch: 6| Step: 8
Training loss: 1.9995934963226318
Validation loss: 2.2137992664050032

Epoch: 6| Step: 9
Training loss: 2.149017333984375
Validation loss: 2.2021553003659813

Epoch: 6| Step: 10
Training loss: 1.6957426071166992
Validation loss: 2.21271445674281

Epoch: 6| Step: 11
Training loss: 2.0756540298461914
Validation loss: 2.2030894423043854

Epoch: 6| Step: 12
Training loss: 2.582526206970215
Validation loss: 2.2013511632078435

Epoch: 6| Step: 13
Training loss: 1.9674025774002075
Validation loss: 2.2066222519002934

Epoch: 316| Step: 0
Training loss: 2.2822840213775635
Validation loss: 2.19769040999874

Epoch: 6| Step: 1
Training loss: 2.660027503967285
Validation loss: 2.188041707520844

Epoch: 6| Step: 2
Training loss: 1.5724117755889893
Validation loss: 2.1888593473742084

Epoch: 6| Step: 3
Training loss: 3.190406322479248
Validation loss: 2.1790004058550765

Epoch: 6| Step: 4
Training loss: 3.248868227005005
Validation loss: 2.169433811659454

Epoch: 6| Step: 5
Training loss: 1.6653282642364502
Validation loss: 2.184542514944589

Epoch: 6| Step: 6
Training loss: 2.645443916320801
Validation loss: 2.1776191688353017

Epoch: 6| Step: 7
Training loss: 2.7352280616760254
Validation loss: 2.183092704383276

Epoch: 6| Step: 8
Training loss: 1.8564589023590088
Validation loss: 2.176836593176729

Epoch: 6| Step: 9
Training loss: 2.055586338043213
Validation loss: 2.1776382974399033

Epoch: 6| Step: 10
Training loss: 2.0039777755737305
Validation loss: 2.173374186279953

Epoch: 6| Step: 11
Training loss: 1.8878929615020752
Validation loss: 2.1786004599704536

Epoch: 6| Step: 12
Training loss: 1.9347448348999023
Validation loss: 2.232979912911692

Epoch: 6| Step: 13
Training loss: 2.254837989807129
Validation loss: 2.2396400564460346

Epoch: 317| Step: 0
Training loss: 2.3123412132263184
Validation loss: 2.2775910951757945

Epoch: 6| Step: 1
Training loss: 2.7573471069335938
Validation loss: 2.3196780861064954

Epoch: 6| Step: 2
Training loss: 1.9849388599395752
Validation loss: 2.3303898662649174

Epoch: 6| Step: 3
Training loss: 2.066044807434082
Validation loss: 2.3438631680703934

Epoch: 6| Step: 4
Training loss: 2.5747923851013184
Validation loss: 2.32967016132929

Epoch: 6| Step: 5
Training loss: 2.191777229309082
Validation loss: 2.2818401705834175

Epoch: 6| Step: 6
Training loss: 1.603543758392334
Validation loss: 2.2663462444018294

Epoch: 6| Step: 7
Training loss: 2.613964080810547
Validation loss: 2.2276247162972727

Epoch: 6| Step: 8
Training loss: 1.7293813228607178
Validation loss: 2.187144228207168

Epoch: 6| Step: 9
Training loss: 3.3073573112487793
Validation loss: 2.1962524024389123

Epoch: 6| Step: 10
Training loss: 2.4028937816619873
Validation loss: 2.1759735281749437

Epoch: 6| Step: 11
Training loss: 2.7484140396118164
Validation loss: 2.1820018650383077

Epoch: 6| Step: 12
Training loss: 1.9135568141937256
Validation loss: 2.1808803389149327

Epoch: 6| Step: 13
Training loss: 2.322310447692871
Validation loss: 2.1919762806225846

Epoch: 318| Step: 0
Training loss: 2.197788715362549
Validation loss: 2.1938252243944394

Epoch: 6| Step: 1
Training loss: 2.6840462684631348
Validation loss: 2.18917170904016

Epoch: 6| Step: 2
Training loss: 2.6263058185577393
Validation loss: 2.1828544037316435

Epoch: 6| Step: 3
Training loss: 1.8134337663650513
Validation loss: 2.1660657928835962

Epoch: 6| Step: 4
Training loss: 2.558356761932373
Validation loss: 2.1855807945292485

Epoch: 6| Step: 5
Training loss: 2.105135679244995
Validation loss: 2.1947908273307224

Epoch: 6| Step: 6
Training loss: 2.3327174186706543
Validation loss: 2.219844005441153

Epoch: 6| Step: 7
Training loss: 1.8658554553985596
Validation loss: 2.242814120425973

Epoch: 6| Step: 8
Training loss: 2.3948116302490234
Validation loss: 2.2555996858945457

Epoch: 6| Step: 9
Training loss: 2.414738416671753
Validation loss: 2.2774769734310847

Epoch: 6| Step: 10
Training loss: 2.606107234954834
Validation loss: 2.2699936589887066

Epoch: 6| Step: 11
Training loss: 2.0570669174194336
Validation loss: 2.272963978910959

Epoch: 6| Step: 12
Training loss: 2.3114373683929443
Validation loss: 2.2654880221172045

Epoch: 6| Step: 13
Training loss: 2.726907968521118
Validation loss: 2.249452385851132

Epoch: 319| Step: 0
Training loss: 2.2609705924987793
Validation loss: 2.219202246717227

Epoch: 6| Step: 1
Training loss: 2.6462719440460205
Validation loss: 2.223594839854907

Epoch: 6| Step: 2
Training loss: 1.6560442447662354
Validation loss: 2.2219570298348703

Epoch: 6| Step: 3
Training loss: 2.726285457611084
Validation loss: 2.2006363202166814

Epoch: 6| Step: 4
Training loss: 2.4697794914245605
Validation loss: 2.19512479664177

Epoch: 6| Step: 5
Training loss: 2.8464601039886475
Validation loss: 2.187382269931096

Epoch: 6| Step: 6
Training loss: 2.5237345695495605
Validation loss: 2.2006392530215684

Epoch: 6| Step: 7
Training loss: 2.3111002445220947
Validation loss: 2.177250010992891

Epoch: 6| Step: 8
Training loss: 1.9061546325683594
Validation loss: 2.1728661111606065

Epoch: 6| Step: 9
Training loss: 2.599769353866577
Validation loss: 2.185429165440221

Epoch: 6| Step: 10
Training loss: 2.398982524871826
Validation loss: 2.194618066151937

Epoch: 6| Step: 11
Training loss: 1.6424791812896729
Validation loss: 2.202290631109668

Epoch: 6| Step: 12
Training loss: 2.287611961364746
Validation loss: 2.20430955835568

Epoch: 6| Step: 13
Training loss: 1.6037946939468384
Validation loss: 2.2242650780626523

Epoch: 320| Step: 0
Training loss: 2.1395952701568604
Validation loss: 2.234125875657605

Epoch: 6| Step: 1
Training loss: 2.45857572555542
Validation loss: 2.228370774176813

Epoch: 6| Step: 2
Training loss: 2.528898000717163
Validation loss: 2.2235269687509023

Epoch: 6| Step: 3
Training loss: 2.8069586753845215
Validation loss: 2.214191439331219

Epoch: 6| Step: 4
Training loss: 2.3839612007141113
Validation loss: 2.2020168022442888

Epoch: 6| Step: 5
Training loss: 2.5310871601104736
Validation loss: 2.2086743641925115

Epoch: 6| Step: 6
Training loss: 2.099144458770752
Validation loss: 2.2067268599746046

Epoch: 6| Step: 7
Training loss: 1.799716830253601
Validation loss: 2.20386766874662

Epoch: 6| Step: 8
Training loss: 1.935896873474121
Validation loss: 2.2037622364618445

Epoch: 6| Step: 9
Training loss: 1.6126333475112915
Validation loss: 2.225094879827192

Epoch: 6| Step: 10
Training loss: 2.3652544021606445
Validation loss: 2.220997493754151

Epoch: 6| Step: 11
Training loss: 2.4567818641662598
Validation loss: 2.232028671490249

Epoch: 6| Step: 12
Training loss: 2.635683536529541
Validation loss: 2.247409095046341

Epoch: 6| Step: 13
Training loss: 2.2837860584259033
Validation loss: 2.2310056609492146

Epoch: 321| Step: 0
Training loss: 2.102511167526245
Validation loss: 2.2296206310231197

Epoch: 6| Step: 1
Training loss: 2.2125933170318604
Validation loss: 2.2313452215604883

Epoch: 6| Step: 2
Training loss: 2.35224986076355
Validation loss: 2.246192245073216

Epoch: 6| Step: 3
Training loss: 2.7433505058288574
Validation loss: 2.2422997977143977

Epoch: 6| Step: 4
Training loss: 2.335015058517456
Validation loss: 2.228401425064251

Epoch: 6| Step: 5
Training loss: 1.7827644348144531
Validation loss: 2.2147143040933917

Epoch: 6| Step: 6
Training loss: 2.4961559772491455
Validation loss: 2.214434882645966

Epoch: 6| Step: 7
Training loss: 2.3262505531311035
Validation loss: 2.210527364925672

Epoch: 6| Step: 8
Training loss: 2.5446994304656982
Validation loss: 2.1796153437706733

Epoch: 6| Step: 9
Training loss: 2.2972705364227295
Validation loss: 2.1915032620071084

Epoch: 6| Step: 10
Training loss: 2.0148348808288574
Validation loss: 2.191113305348222

Epoch: 6| Step: 11
Training loss: 2.2786271572113037
Validation loss: 2.1885345135965655

Epoch: 6| Step: 12
Training loss: 2.018681049346924
Validation loss: 2.184648826558103

Epoch: 6| Step: 13
Training loss: 2.8217477798461914
Validation loss: 2.1949795907543552

Epoch: 322| Step: 0
Training loss: 1.6269876956939697
Validation loss: 2.213129243543071

Epoch: 6| Step: 1
Training loss: 2.280992031097412
Validation loss: 2.2192438058955695

Epoch: 6| Step: 2
Training loss: 2.5407023429870605
Validation loss: 2.22503508803665

Epoch: 6| Step: 3
Training loss: 2.048980236053467
Validation loss: 2.2362870939316286

Epoch: 6| Step: 4
Training loss: 2.4157238006591797
Validation loss: 2.2401989454864175

Epoch: 6| Step: 5
Training loss: 1.8522400856018066
Validation loss: 2.2460852720404185

Epoch: 6| Step: 6
Training loss: 2.5992228984832764
Validation loss: 2.2356488140680457

Epoch: 6| Step: 7
Training loss: 2.1855945587158203
Validation loss: 2.2359912164749636

Epoch: 6| Step: 8
Training loss: 2.4306533336639404
Validation loss: 2.221310878312716

Epoch: 6| Step: 9
Training loss: 2.3687334060668945
Validation loss: 2.2113095457835863

Epoch: 6| Step: 10
Training loss: 2.532233238220215
Validation loss: 2.212338221970425

Epoch: 6| Step: 11
Training loss: 1.918566107749939
Validation loss: 2.191228974250055

Epoch: 6| Step: 12
Training loss: 2.6793923377990723
Validation loss: 2.1848534358445035

Epoch: 6| Step: 13
Training loss: 2.649609088897705
Validation loss: 2.1874783577457553

Epoch: 323| Step: 0
Training loss: 1.4795448780059814
Validation loss: 2.192747971063019

Epoch: 6| Step: 1
Training loss: 2.830841064453125
Validation loss: 2.1829149748689387

Epoch: 6| Step: 2
Training loss: 2.854511260986328
Validation loss: 2.1915647778459775

Epoch: 6| Step: 3
Training loss: 2.0655813217163086
Validation loss: 2.197443116095758

Epoch: 6| Step: 4
Training loss: 2.2163963317871094
Validation loss: 2.2140276662765013

Epoch: 6| Step: 5
Training loss: 2.5141239166259766
Validation loss: 2.222685529339698

Epoch: 6| Step: 6
Training loss: 1.8120343685150146
Validation loss: 2.208466940028693

Epoch: 6| Step: 7
Training loss: 2.0032448768615723
Validation loss: 2.2477932924865396

Epoch: 6| Step: 8
Training loss: 2.4084999561309814
Validation loss: 2.2230605630464453

Epoch: 6| Step: 9
Training loss: 2.3512721061706543
Validation loss: 2.2333983657180623

Epoch: 6| Step: 10
Training loss: 2.8346076011657715
Validation loss: 2.231426239013672

Epoch: 6| Step: 11
Training loss: 2.378660202026367
Validation loss: 2.2138186475282073

Epoch: 6| Step: 12
Training loss: 1.932607889175415
Validation loss: 2.228596600153113

Epoch: 6| Step: 13
Training loss: 2.5015199184417725
Validation loss: 2.226265412504955

Epoch: 324| Step: 0
Training loss: 2.4616925716400146
Validation loss: 2.218193151617563

Epoch: 6| Step: 1
Training loss: 2.1920008659362793
Validation loss: 2.2135108773426344

Epoch: 6| Step: 2
Training loss: 2.599886894226074
Validation loss: 2.222356096390755

Epoch: 6| Step: 3
Training loss: 2.4602065086364746
Validation loss: 2.211999672715382

Epoch: 6| Step: 4
Training loss: 2.6123533248901367
Validation loss: 2.199584953246578

Epoch: 6| Step: 5
Training loss: 2.1578128337860107
Validation loss: 2.2013585003473426

Epoch: 6| Step: 6
Training loss: 2.640817642211914
Validation loss: 2.190060597594066

Epoch: 6| Step: 7
Training loss: 2.559095859527588
Validation loss: 2.206069166942309

Epoch: 6| Step: 8
Training loss: 1.4494898319244385
Validation loss: 2.1905837853749595

Epoch: 6| Step: 9
Training loss: 2.169189929962158
Validation loss: 2.189924508012751

Epoch: 6| Step: 10
Training loss: 2.557145118713379
Validation loss: 2.1965353322285477

Epoch: 6| Step: 11
Training loss: 1.4597316980361938
Validation loss: 2.2027839563226186

Epoch: 6| Step: 12
Training loss: 2.418011426925659
Validation loss: 2.2010451850070747

Epoch: 6| Step: 13
Training loss: 2.148423194885254
Validation loss: 2.194177512199648

Epoch: 325| Step: 0
Training loss: 2.24433970451355
Validation loss: 2.1814704530982563

Epoch: 6| Step: 1
Training loss: 2.8533830642700195
Validation loss: 2.1659091031679543

Epoch: 6| Step: 2
Training loss: 1.9388705492019653
Validation loss: 2.1662535257236932

Epoch: 6| Step: 3
Training loss: 2.3391666412353516
Validation loss: 2.1648674203503515

Epoch: 6| Step: 4
Training loss: 2.1687192916870117
Validation loss: 2.150282206073884

Epoch: 6| Step: 5
Training loss: 1.8519515991210938
Validation loss: 2.1560102637096117

Epoch: 6| Step: 6
Training loss: 2.5446724891662598
Validation loss: 2.1643577903829594

Epoch: 6| Step: 7
Training loss: 2.4792075157165527
Validation loss: 2.188467756394417

Epoch: 6| Step: 8
Training loss: 1.626983404159546
Validation loss: 2.1790338946926977

Epoch: 6| Step: 9
Training loss: 2.193293571472168
Validation loss: 2.1979099371099986

Epoch: 6| Step: 10
Training loss: 2.3763275146484375
Validation loss: 2.187885388251274

Epoch: 6| Step: 11
Training loss: 2.2621657848358154
Validation loss: 2.192518065052648

Epoch: 6| Step: 12
Training loss: 2.4365782737731934
Validation loss: 2.190438816624303

Epoch: 6| Step: 13
Training loss: 2.9276275634765625
Validation loss: 2.2050789940741753

Epoch: 326| Step: 0
Training loss: 1.368156909942627
Validation loss: 2.2217269559060373

Epoch: 6| Step: 1
Training loss: 2.45453143119812
Validation loss: 2.2180342340982087

Epoch: 6| Step: 2
Training loss: 1.677862286567688
Validation loss: 2.213971640474053

Epoch: 6| Step: 3
Training loss: 2.313804864883423
Validation loss: 2.21810180910172

Epoch: 6| Step: 4
Training loss: 1.7804043292999268
Validation loss: 2.2184074104473157

Epoch: 6| Step: 5
Training loss: 1.789884090423584
Validation loss: 2.2018775863032185

Epoch: 6| Step: 6
Training loss: 2.1694936752319336
Validation loss: 2.2103262716724026

Epoch: 6| Step: 7
Training loss: 2.1213011741638184
Validation loss: 2.200408658673686

Epoch: 6| Step: 8
Training loss: 1.9919319152832031
Validation loss: 2.215449135790589

Epoch: 6| Step: 9
Training loss: 2.7256476879119873
Validation loss: 2.2162997850807766

Epoch: 6| Step: 10
Training loss: 2.8762032985687256
Validation loss: 2.204877407320084

Epoch: 6| Step: 11
Training loss: 3.2146072387695312
Validation loss: 2.217492731668616

Epoch: 6| Step: 12
Training loss: 2.9498560428619385
Validation loss: 2.2083335576518888

Epoch: 6| Step: 13
Training loss: 2.3384084701538086
Validation loss: 2.207263603005358

Epoch: 327| Step: 0
Training loss: 2.3681859970092773
Validation loss: 2.2012807887087584

Epoch: 6| Step: 1
Training loss: 2.908459186553955
Validation loss: 2.2108242934749973

Epoch: 6| Step: 2
Training loss: 2.215196132659912
Validation loss: 2.2069545907358967

Epoch: 6| Step: 3
Training loss: 1.7724937200546265
Validation loss: 2.2098290586984284

Epoch: 6| Step: 4
Training loss: 2.17678165435791
Validation loss: 2.2205215346428657

Epoch: 6| Step: 5
Training loss: 2.44294810295105
Validation loss: 2.2181106216164044

Epoch: 6| Step: 6
Training loss: 2.544083595275879
Validation loss: 2.2336950225214802

Epoch: 6| Step: 7
Training loss: 2.405355453491211
Validation loss: 2.2282879019296296

Epoch: 6| Step: 8
Training loss: 2.81907057762146
Validation loss: 2.230358482688986

Epoch: 6| Step: 9
Training loss: 1.7554922103881836
Validation loss: 2.2363222337538198

Epoch: 6| Step: 10
Training loss: 2.074570655822754
Validation loss: 2.2480714962046635

Epoch: 6| Step: 11
Training loss: 2.412905216217041
Validation loss: 2.236905610689553

Epoch: 6| Step: 12
Training loss: 1.8159050941467285
Validation loss: 2.2242531353427517

Epoch: 6| Step: 13
Training loss: 1.5903363227844238
Validation loss: 2.2111395277002805

Epoch: 328| Step: 0
Training loss: 2.5005276203155518
Validation loss: 2.2108423863687823

Epoch: 6| Step: 1
Training loss: 2.421267032623291
Validation loss: 2.19767935814396

Epoch: 6| Step: 2
Training loss: 2.8891987800598145
Validation loss: 2.180216677727238

Epoch: 6| Step: 3
Training loss: 2.491661310195923
Validation loss: 2.1878775319745465

Epoch: 6| Step: 4
Training loss: 1.6276252269744873
Validation loss: 2.1706454036056355

Epoch: 6| Step: 5
Training loss: 2.0705509185791016
Validation loss: 2.1672273733282603

Epoch: 6| Step: 6
Training loss: 2.006108283996582
Validation loss: 2.190097778074203

Epoch: 6| Step: 7
Training loss: 2.200840950012207
Validation loss: 2.186212334581601

Epoch: 6| Step: 8
Training loss: 2.891420841217041
Validation loss: 2.2172197834137948

Epoch: 6| Step: 9
Training loss: 2.8102798461914062
Validation loss: 2.2290612318182506

Epoch: 6| Step: 10
Training loss: 1.2215070724487305
Validation loss: 2.2342412330771007

Epoch: 6| Step: 11
Training loss: 3.2280924320220947
Validation loss: 2.2353076293904293

Epoch: 6| Step: 12
Training loss: 1.7447997331619263
Validation loss: 2.2139212828810497

Epoch: 6| Step: 13
Training loss: 1.016324520111084
Validation loss: 2.225542535064041

Epoch: 329| Step: 0
Training loss: 2.4534153938293457
Validation loss: 2.214492415869108

Epoch: 6| Step: 1
Training loss: 2.7478718757629395
Validation loss: 2.2042514175497074

Epoch: 6| Step: 2
Training loss: 1.4247064590454102
Validation loss: 2.2063441250913884

Epoch: 6| Step: 3
Training loss: 2.187220811843872
Validation loss: 2.2034418967462357

Epoch: 6| Step: 4
Training loss: 2.259982109069824
Validation loss: 2.211141429921632

Epoch: 6| Step: 5
Training loss: 2.1920292377471924
Validation loss: 2.2099639395231843

Epoch: 6| Step: 6
Training loss: 2.6065614223480225
Validation loss: 2.2189690425831783

Epoch: 6| Step: 7
Training loss: 1.8396825790405273
Validation loss: 2.212701884649133

Epoch: 6| Step: 8
Training loss: 1.7534241676330566
Validation loss: 2.2037615314606698

Epoch: 6| Step: 9
Training loss: 2.784299373626709
Validation loss: 2.2097793215064594

Epoch: 6| Step: 10
Training loss: 2.088491678237915
Validation loss: 2.2166586088877853

Epoch: 6| Step: 11
Training loss: 2.411287307739258
Validation loss: 2.2024330785197597

Epoch: 6| Step: 12
Training loss: 2.5435872077941895
Validation loss: 2.195739120565435

Epoch: 6| Step: 13
Training loss: 2.515047788619995
Validation loss: 2.2146862501739175

Epoch: 330| Step: 0
Training loss: 1.977471113204956
Validation loss: 2.207493579515847

Epoch: 6| Step: 1
Training loss: 3.2515482902526855
Validation loss: 2.2192273216862834

Epoch: 6| Step: 2
Training loss: 2.717075824737549
Validation loss: 2.2266622794571744

Epoch: 6| Step: 3
Training loss: 1.9668214321136475
Validation loss: 2.21947060092803

Epoch: 6| Step: 4
Training loss: 2.465212106704712
Validation loss: 2.2247313171304683

Epoch: 6| Step: 5
Training loss: 2.597198963165283
Validation loss: 2.2447101659672235

Epoch: 6| Step: 6
Training loss: 2.2287521362304688
Validation loss: 2.2324445888560307

Epoch: 6| Step: 7
Training loss: 1.7338204383850098
Validation loss: 2.209526837513011

Epoch: 6| Step: 8
Training loss: 2.5099730491638184
Validation loss: 2.214876405654415

Epoch: 6| Step: 9
Training loss: 1.2097132205963135
Validation loss: 2.1961510309609036

Epoch: 6| Step: 10
Training loss: 2.2917046546936035
Validation loss: 2.185988172408073

Epoch: 6| Step: 11
Training loss: 1.9833382368087769
Validation loss: 2.1800411760166125

Epoch: 6| Step: 12
Training loss: 2.282254219055176
Validation loss: 2.1797523088352655

Epoch: 6| Step: 13
Training loss: 2.181636333465576
Validation loss: 2.1675341667667514

Epoch: 331| Step: 0
Training loss: 1.5008231401443481
Validation loss: 2.179324614104404

Epoch: 6| Step: 1
Training loss: 2.1807117462158203
Validation loss: 2.18332859264907

Epoch: 6| Step: 2
Training loss: 2.435769557952881
Validation loss: 2.1964743291178057

Epoch: 6| Step: 3
Training loss: 2.4758622646331787
Validation loss: 2.1990114386363695

Epoch: 6| Step: 4
Training loss: 2.115527868270874
Validation loss: 2.2111600393890054

Epoch: 6| Step: 5
Training loss: 2.5901103019714355
Validation loss: 2.2183340723796556

Epoch: 6| Step: 6
Training loss: 2.326665163040161
Validation loss: 2.21429536163166

Epoch: 6| Step: 7
Training loss: 2.5347375869750977
Validation loss: 2.223582198542933

Epoch: 6| Step: 8
Training loss: 1.9722143411636353
Validation loss: 2.206402083878876

Epoch: 6| Step: 9
Training loss: 1.4298312664031982
Validation loss: 2.2054588128161687

Epoch: 6| Step: 10
Training loss: 2.7875869274139404
Validation loss: 2.2175472180048623

Epoch: 6| Step: 11
Training loss: 1.9276789426803589
Validation loss: 2.232044058461343

Epoch: 6| Step: 12
Training loss: 1.9615137577056885
Validation loss: 2.2214832972454768

Epoch: 6| Step: 13
Training loss: 3.7794129848480225
Validation loss: 2.220630116360162

Epoch: 332| Step: 0
Training loss: 1.7972722053527832
Validation loss: 2.2243831593503236

Epoch: 6| Step: 1
Training loss: 2.4513494968414307
Validation loss: 2.222898970368088

Epoch: 6| Step: 2
Training loss: 1.631947636604309
Validation loss: 2.231509870098483

Epoch: 6| Step: 3
Training loss: 1.8259081840515137
Validation loss: 2.2436436273718394

Epoch: 6| Step: 4
Training loss: 2.851128101348877
Validation loss: 2.255889815668906

Epoch: 6| Step: 5
Training loss: 2.0047712326049805
Validation loss: 2.2615606579729306

Epoch: 6| Step: 6
Training loss: 2.3600924015045166
Validation loss: 2.2546529821170274

Epoch: 6| Step: 7
Training loss: 2.010080337524414
Validation loss: 2.251231412733755

Epoch: 6| Step: 8
Training loss: 2.0854008197784424
Validation loss: 2.238573128177274

Epoch: 6| Step: 9
Training loss: 2.4817521572113037
Validation loss: 2.200335136023901

Epoch: 6| Step: 10
Training loss: 2.111344575881958
Validation loss: 2.1854685685967885

Epoch: 6| Step: 11
Training loss: 2.146078586578369
Validation loss: 2.171279811090039

Epoch: 6| Step: 12
Training loss: 3.7573795318603516
Validation loss: 2.1648219631564234

Epoch: 6| Step: 13
Training loss: 1.6528524160385132
Validation loss: 2.1575856875347834

Epoch: 333| Step: 0
Training loss: 2.4565014839172363
Validation loss: 2.1590840380678893

Epoch: 6| Step: 1
Training loss: 1.9942198991775513
Validation loss: 2.1617142000506

Epoch: 6| Step: 2
Training loss: 1.9786267280578613
Validation loss: 2.162227269141905

Epoch: 6| Step: 3
Training loss: 2.7612218856811523
Validation loss: 2.155151151841687

Epoch: 6| Step: 4
Training loss: 2.5380654335021973
Validation loss: 2.1557325701559744

Epoch: 6| Step: 5
Training loss: 2.4902708530426025
Validation loss: 2.153524628249548

Epoch: 6| Step: 6
Training loss: 2.5383200645446777
Validation loss: 2.167719861512543

Epoch: 6| Step: 7
Training loss: 2.067218780517578
Validation loss: 2.1569368403445006

Epoch: 6| Step: 8
Training loss: 2.2301602363586426
Validation loss: 2.161622155097223

Epoch: 6| Step: 9
Training loss: 2.9691357612609863
Validation loss: 2.1712234725234327

Epoch: 6| Step: 10
Training loss: 1.9975895881652832
Validation loss: 2.1803358152348506

Epoch: 6| Step: 11
Training loss: 1.5407239198684692
Validation loss: 2.20076649419723

Epoch: 6| Step: 12
Training loss: 2.103942632675171
Validation loss: 2.2134133833710865

Epoch: 6| Step: 13
Training loss: 1.806383728981018
Validation loss: 2.2315957418052097

Epoch: 334| Step: 0
Training loss: 2.233513593673706
Validation loss: 2.2705297726456837

Epoch: 6| Step: 1
Training loss: 1.802706003189087
Validation loss: 2.2644847490454234

Epoch: 6| Step: 2
Training loss: 2.545473337173462
Validation loss: 2.248610322193433

Epoch: 6| Step: 3
Training loss: 1.9893593788146973
Validation loss: 2.226699677846765

Epoch: 6| Step: 4
Training loss: 1.7570112943649292
Validation loss: 2.192608118057251

Epoch: 6| Step: 5
Training loss: 2.0845532417297363
Validation loss: 2.1743824238418252

Epoch: 6| Step: 6
Training loss: 2.3251137733459473
Validation loss: 2.168303123084448

Epoch: 6| Step: 7
Training loss: 1.9030697345733643
Validation loss: 2.168566164150033

Epoch: 6| Step: 8
Training loss: 2.5932188034057617
Validation loss: 2.1835749200595322

Epoch: 6| Step: 9
Training loss: 2.8768134117126465
Validation loss: 2.1951982795551257

Epoch: 6| Step: 10
Training loss: 2.6639750003814697
Validation loss: 2.2144911032851025

Epoch: 6| Step: 11
Training loss: 2.1538758277893066
Validation loss: 2.223704017618651

Epoch: 6| Step: 12
Training loss: 2.2406060695648193
Validation loss: 2.2432277279515422

Epoch: 6| Step: 13
Training loss: 2.4042999744415283
Validation loss: 2.2484043131592455

Epoch: 335| Step: 0
Training loss: 2.595499038696289
Validation loss: 2.235536539426414

Epoch: 6| Step: 1
Training loss: 2.276273488998413
Validation loss: 2.230514826313142

Epoch: 6| Step: 2
Training loss: 1.9193793535232544
Validation loss: 2.2236924863630727

Epoch: 6| Step: 3
Training loss: 2.1462340354919434
Validation loss: 2.222824218452618

Epoch: 6| Step: 4
Training loss: 1.7420001029968262
Validation loss: 2.21066435434485

Epoch: 6| Step: 5
Training loss: 2.885125160217285
Validation loss: 2.254382687230264

Epoch: 6| Step: 6
Training loss: 2.668492317199707
Validation loss: 2.240136423418599

Epoch: 6| Step: 7
Training loss: 2.0765693187713623
Validation loss: 2.236038674590408

Epoch: 6| Step: 8
Training loss: 2.0985336303710938
Validation loss: 2.236752284470425

Epoch: 6| Step: 9
Training loss: 2.155461311340332
Validation loss: 2.195792918564171

Epoch: 6| Step: 10
Training loss: 2.13104248046875
Validation loss: 2.190283666374863

Epoch: 6| Step: 11
Training loss: 2.231076240539551
Validation loss: 2.182205928269253

Epoch: 6| Step: 12
Training loss: 2.2663211822509766
Validation loss: 2.1891318880101687

Epoch: 6| Step: 13
Training loss: 2.186213970184326
Validation loss: 2.171654151331994

Epoch: 336| Step: 0
Training loss: 2.135272264480591
Validation loss: 2.1719685395558677

Epoch: 6| Step: 1
Training loss: 2.868687629699707
Validation loss: 2.177912594169699

Epoch: 6| Step: 2
Training loss: 2.1401679515838623
Validation loss: 2.170432495814498

Epoch: 6| Step: 3
Training loss: 2.4517884254455566
Validation loss: 2.182225742647725

Epoch: 6| Step: 4
Training loss: 2.2940993309020996
Validation loss: 2.1779859348009993

Epoch: 6| Step: 5
Training loss: 2.5768465995788574
Validation loss: 2.178329811301283

Epoch: 6| Step: 6
Training loss: 1.7580766677856445
Validation loss: 2.20578194433643

Epoch: 6| Step: 7
Training loss: 2.123932361602783
Validation loss: 2.2067795876533753

Epoch: 6| Step: 8
Training loss: 2.747988700866699
Validation loss: 2.2142918443167083

Epoch: 6| Step: 9
Training loss: 1.9273254871368408
Validation loss: 2.2304689576548915

Epoch: 6| Step: 10
Training loss: 2.1797876358032227
Validation loss: 2.237989435913742

Epoch: 6| Step: 11
Training loss: 1.291182041168213
Validation loss: 2.2493081785017446

Epoch: 6| Step: 12
Training loss: 2.2815704345703125
Validation loss: 2.2541919959488737

Epoch: 6| Step: 13
Training loss: 2.602936029434204
Validation loss: 2.243329236584325

Epoch: 337| Step: 0
Training loss: 2.6156399250030518
Validation loss: 2.228236457352997

Epoch: 6| Step: 1
Training loss: 2.353379249572754
Validation loss: 2.2019362526555217

Epoch: 6| Step: 2
Training loss: 1.412787675857544
Validation loss: 2.1861367482011036

Epoch: 6| Step: 3
Training loss: 2.1020941734313965
Validation loss: 2.195527184394098

Epoch: 6| Step: 4
Training loss: 2.695857048034668
Validation loss: 2.18918167391131

Epoch: 6| Step: 5
Training loss: 2.2756974697113037
Validation loss: 2.1983782988722607

Epoch: 6| Step: 6
Training loss: 1.4497642517089844
Validation loss: 2.183467157425419

Epoch: 6| Step: 7
Training loss: 2.641040563583374
Validation loss: 2.179573897392519

Epoch: 6| Step: 8
Training loss: 1.9318485260009766
Validation loss: 2.190192635341357

Epoch: 6| Step: 9
Training loss: 2.5454001426696777
Validation loss: 2.1982488760384182

Epoch: 6| Step: 10
Training loss: 2.0794665813446045
Validation loss: 2.212968317411279

Epoch: 6| Step: 11
Training loss: 1.4059514999389648
Validation loss: 2.2395736171353247

Epoch: 6| Step: 12
Training loss: 3.0509119033813477
Validation loss: 2.2592766387488252

Epoch: 6| Step: 13
Training loss: 3.1503751277923584
Validation loss: 2.249802020288283

Epoch: 338| Step: 0
Training loss: 1.8300672769546509
Validation loss: 2.270297796495499

Epoch: 6| Step: 1
Training loss: 2.418030261993408
Validation loss: 2.2493221836705364

Epoch: 6| Step: 2
Training loss: 2.4223849773406982
Validation loss: 2.236991733633062

Epoch: 6| Step: 3
Training loss: 2.5969161987304688
Validation loss: 2.212245833489203

Epoch: 6| Step: 4
Training loss: 3.349419593811035
Validation loss: 2.2311404981920795

Epoch: 6| Step: 5
Training loss: 1.883178949356079
Validation loss: 2.208935501754925

Epoch: 6| Step: 6
Training loss: 2.346538782119751
Validation loss: 2.2358372519093175

Epoch: 6| Step: 7
Training loss: 2.3130698204040527
Validation loss: 2.2066043294886106

Epoch: 6| Step: 8
Training loss: 1.932084560394287
Validation loss: 2.20749238229567

Epoch: 6| Step: 9
Training loss: 1.4990694522857666
Validation loss: 2.2003454136592087

Epoch: 6| Step: 10
Training loss: 2.1326189041137695
Validation loss: 2.209372579410512

Epoch: 6| Step: 11
Training loss: 2.355438709259033
Validation loss: 2.17899392753519

Epoch: 6| Step: 12
Training loss: 1.669116735458374
Validation loss: 2.200331889173036

Epoch: 6| Step: 13
Training loss: 2.557650089263916
Validation loss: 2.2028289353975685

Epoch: 339| Step: 0
Training loss: 1.6051559448242188
Validation loss: 2.2001277041691605

Epoch: 6| Step: 1
Training loss: 2.3288187980651855
Validation loss: 2.2057786064763225

Epoch: 6| Step: 2
Training loss: 2.712538480758667
Validation loss: 2.202612559000651

Epoch: 6| Step: 3
Training loss: 2.5721852779388428
Validation loss: 2.2166457650482014

Epoch: 6| Step: 4
Training loss: 2.6181812286376953
Validation loss: 2.2113016754068355

Epoch: 6| Step: 5
Training loss: 2.9016060829162598
Validation loss: 2.226353404342487

Epoch: 6| Step: 6
Training loss: 2.181699275970459
Validation loss: 2.2418186562035674

Epoch: 6| Step: 7
Training loss: 2.306199789047241
Validation loss: 2.236791882463681

Epoch: 6| Step: 8
Training loss: 2.124861001968384
Validation loss: 2.227457619482471

Epoch: 6| Step: 9
Training loss: 1.89005446434021
Validation loss: 2.22806223746269

Epoch: 6| Step: 10
Training loss: 2.5722451210021973
Validation loss: 2.2083629074917046

Epoch: 6| Step: 11
Training loss: 1.6837538480758667
Validation loss: 2.2202655064162387

Epoch: 6| Step: 12
Training loss: 1.498875379562378
Validation loss: 2.2072298155036023

Epoch: 6| Step: 13
Training loss: 2.1093389987945557
Validation loss: 2.206507544363699

Epoch: 340| Step: 0
Training loss: 2.2599575519561768
Validation loss: 2.2076741687713133

Epoch: 6| Step: 1
Training loss: 1.5603185892105103
Validation loss: 2.2106354928785756

Epoch: 6| Step: 2
Training loss: 2.0659828186035156
Validation loss: 2.197777170006947

Epoch: 6| Step: 3
Training loss: 2.5872819423675537
Validation loss: 2.201339219206123

Epoch: 6| Step: 4
Training loss: 2.6736841201782227
Validation loss: 2.1984916989521315

Epoch: 6| Step: 5
Training loss: 2.4381954669952393
Validation loss: 2.190271782618697

Epoch: 6| Step: 6
Training loss: 2.223447322845459
Validation loss: 2.210616534756076

Epoch: 6| Step: 7
Training loss: 1.8804856538772583
Validation loss: 2.1865775867175032

Epoch: 6| Step: 8
Training loss: 2.585061550140381
Validation loss: 2.198628329461621

Epoch: 6| Step: 9
Training loss: 2.2765164375305176
Validation loss: 2.206869991876746

Epoch: 6| Step: 10
Training loss: 2.066892385482788
Validation loss: 2.1868998363453853

Epoch: 6| Step: 11
Training loss: 1.6011579036712646
Validation loss: 2.169412100186912

Epoch: 6| Step: 12
Training loss: 2.577711582183838
Validation loss: 2.1717046127524426

Epoch: 6| Step: 13
Training loss: 2.462914228439331
Validation loss: 2.163029275914674

Epoch: 341| Step: 0
Training loss: 2.9939675331115723
Validation loss: 2.1747041492051977

Epoch: 6| Step: 1
Training loss: 1.5039955377578735
Validation loss: 2.1744874126167706

Epoch: 6| Step: 2
Training loss: 1.8585426807403564
Validation loss: 2.194500106637196

Epoch: 6| Step: 3
Training loss: 2.528128147125244
Validation loss: 2.203495176889563

Epoch: 6| Step: 4
Training loss: 1.7171070575714111
Validation loss: 2.2109141542065527

Epoch: 6| Step: 5
Training loss: 2.362123966217041
Validation loss: 2.2136199628153155

Epoch: 6| Step: 6
Training loss: 2.2818028926849365
Validation loss: 2.2190861471237673

Epoch: 6| Step: 7
Training loss: 2.7990360260009766
Validation loss: 2.222190474951139

Epoch: 6| Step: 8
Training loss: 2.554401159286499
Validation loss: 2.224168139119302

Epoch: 6| Step: 9
Training loss: 2.0764784812927246
Validation loss: 2.216890710656361

Epoch: 6| Step: 10
Training loss: 2.0059022903442383
Validation loss: 2.224187945806852

Epoch: 6| Step: 11
Training loss: 2.283860921859741
Validation loss: 2.2083645584762737

Epoch: 6| Step: 12
Training loss: 2.141288995742798
Validation loss: 2.1907854874928794

Epoch: 6| Step: 13
Training loss: 1.6352229118347168
Validation loss: 2.1864090376002814

Epoch: 342| Step: 0
Training loss: 2.088012218475342
Validation loss: 2.1813376501042354

Epoch: 6| Step: 1
Training loss: 2.6588525772094727
Validation loss: 2.1945234652488463

Epoch: 6| Step: 2
Training loss: 2.0936927795410156
Validation loss: 2.1983191326100338

Epoch: 6| Step: 3
Training loss: 2.119562864303589
Validation loss: 2.2050338278534594

Epoch: 6| Step: 4
Training loss: 3.068039894104004
Validation loss: 2.2164683970071937

Epoch: 6| Step: 5
Training loss: 2.6986939907073975
Validation loss: 2.2122993930693595

Epoch: 6| Step: 6
Training loss: 1.6686880588531494
Validation loss: 2.236879394900414

Epoch: 6| Step: 7
Training loss: 1.7876942157745361
Validation loss: 2.227539047118156

Epoch: 6| Step: 8
Training loss: 2.226057291030884
Validation loss: 2.207867640320973

Epoch: 6| Step: 9
Training loss: 1.9373440742492676
Validation loss: 2.1898133729093816

Epoch: 6| Step: 10
Training loss: 1.6276960372924805
Validation loss: 2.1839529442530807

Epoch: 6| Step: 11
Training loss: 2.291137456893921
Validation loss: 2.186234243454472

Epoch: 6| Step: 12
Training loss: 1.9063761234283447
Validation loss: 2.176789552934708

Epoch: 6| Step: 13
Training loss: 3.314673662185669
Validation loss: 2.1764909657098914

Epoch: 343| Step: 0
Training loss: 2.0844178199768066
Validation loss: 2.178371355097781

Epoch: 6| Step: 1
Training loss: 1.9806474447250366
Validation loss: 2.1956830409265335

Epoch: 6| Step: 2
Training loss: 2.4505414962768555
Validation loss: 2.2181092257140786

Epoch: 6| Step: 3
Training loss: 2.7168917655944824
Validation loss: 2.2180475547749507

Epoch: 6| Step: 4
Training loss: 2.128405809402466
Validation loss: 2.2349738279978433

Epoch: 6| Step: 5
Training loss: 1.9466224908828735
Validation loss: 2.2074117788704495

Epoch: 6| Step: 6
Training loss: 2.7271411418914795
Validation loss: 2.209930467349227

Epoch: 6| Step: 7
Training loss: 2.0628232955932617
Validation loss: 2.205605145423643

Epoch: 6| Step: 8
Training loss: 2.426797866821289
Validation loss: 2.1933936944571872

Epoch: 6| Step: 9
Training loss: 2.3101534843444824
Validation loss: 2.2116382404040267

Epoch: 6| Step: 10
Training loss: 1.9448511600494385
Validation loss: 2.2100668620037776

Epoch: 6| Step: 11
Training loss: 1.8387556076049805
Validation loss: 2.217458789066602

Epoch: 6| Step: 12
Training loss: 2.248047113418579
Validation loss: 2.2294708067371

Epoch: 6| Step: 13
Training loss: 1.7686799764633179
Validation loss: 2.2436811411252586

Epoch: 344| Step: 0
Training loss: 2.3784542083740234
Validation loss: 2.2154797892416678

Epoch: 6| Step: 1
Training loss: 2.7465240955352783
Validation loss: 2.222403223796557

Epoch: 6| Step: 2
Training loss: 2.2139806747436523
Validation loss: 2.225848223573418

Epoch: 6| Step: 3
Training loss: 2.1041297912597656
Validation loss: 2.2173298097425893

Epoch: 6| Step: 4
Training loss: 1.985935926437378
Validation loss: 2.205797346689368

Epoch: 6| Step: 5
Training loss: 2.130573272705078
Validation loss: 2.1841343628462924

Epoch: 6| Step: 6
Training loss: 2.5933194160461426
Validation loss: 2.1929274528257308

Epoch: 6| Step: 7
Training loss: 2.554363250732422
Validation loss: 2.197158149493638

Epoch: 6| Step: 8
Training loss: 1.8753207921981812
Validation loss: 2.185955309098767

Epoch: 6| Step: 9
Training loss: 1.6272838115692139
Validation loss: 2.19825739501625

Epoch: 6| Step: 10
Training loss: 2.5976686477661133
Validation loss: 2.191697459067068

Epoch: 6| Step: 11
Training loss: 1.9762088060379028
Validation loss: 2.1972251297325216

Epoch: 6| Step: 12
Training loss: 2.5466716289520264
Validation loss: 2.211814116406184

Epoch: 6| Step: 13
Training loss: 1.2638651132583618
Validation loss: 2.2330773774013726

Epoch: 345| Step: 0
Training loss: 2.2244977951049805
Validation loss: 2.263036563832273

Epoch: 6| Step: 1
Training loss: 2.5402657985687256
Validation loss: 2.2859601692486833

Epoch: 6| Step: 2
Training loss: 2.3380467891693115
Validation loss: 2.280400900430577

Epoch: 6| Step: 3
Training loss: 2.118116617202759
Validation loss: 2.3115690574851087

Epoch: 6| Step: 4
Training loss: 2.2923970222473145
Validation loss: 2.3052208756887786

Epoch: 6| Step: 5
Training loss: 2.0916733741760254
Validation loss: 2.293550727187946

Epoch: 6| Step: 6
Training loss: 2.2269036769866943
Validation loss: 2.2567523499970794

Epoch: 6| Step: 7
Training loss: 1.3265955448150635
Validation loss: 2.239331083913003

Epoch: 6| Step: 8
Training loss: 2.4266324043273926
Validation loss: 2.20200014627108

Epoch: 6| Step: 9
Training loss: 2.6795907020568848
Validation loss: 2.195142561389554

Epoch: 6| Step: 10
Training loss: 2.5025510787963867
Validation loss: 2.1743350208446546

Epoch: 6| Step: 11
Training loss: 2.1933753490448
Validation loss: 2.1837385290412494

Epoch: 6| Step: 12
Training loss: 1.874308466911316
Validation loss: 2.172604371142644

Epoch: 6| Step: 13
Training loss: 1.9625325202941895
Validation loss: 2.1726025048122612

Epoch: 346| Step: 0
Training loss: 1.612640142440796
Validation loss: 2.166821361869894

Epoch: 6| Step: 1
Training loss: 2.6598262786865234
Validation loss: 2.1864091068185787

Epoch: 6| Step: 2
Training loss: 1.379719853401184
Validation loss: 2.1893208052522395

Epoch: 6| Step: 3
Training loss: 2.5612850189208984
Validation loss: 2.1835379177524197

Epoch: 6| Step: 4
Training loss: 2.1022136211395264
Validation loss: 2.20227062830361

Epoch: 6| Step: 5
Training loss: 2.0661919116973877
Validation loss: 2.1999285067281416

Epoch: 6| Step: 6
Training loss: 1.5508480072021484
Validation loss: 2.2065004789701073

Epoch: 6| Step: 7
Training loss: 2.2302098274230957
Validation loss: 2.2229860469859135

Epoch: 6| Step: 8
Training loss: 2.7934818267822266
Validation loss: 2.2105312296139297

Epoch: 6| Step: 9
Training loss: 1.9658149480819702
Validation loss: 2.1947671905640633

Epoch: 6| Step: 10
Training loss: 2.2583346366882324
Validation loss: 2.186383612694279

Epoch: 6| Step: 11
Training loss: 3.076873779296875
Validation loss: 2.1958589425650974

Epoch: 6| Step: 12
Training loss: 2.405155658721924
Validation loss: 2.204134084845102

Epoch: 6| Step: 13
Training loss: 1.9005309343338013
Validation loss: 2.2006860240813224

Epoch: 347| Step: 0
Training loss: 1.541601538658142
Validation loss: 2.1818543441833986

Epoch: 6| Step: 1
Training loss: 2.3609445095062256
Validation loss: 2.184121334424583

Epoch: 6| Step: 2
Training loss: 2.3179564476013184
Validation loss: 2.1843977897397933

Epoch: 6| Step: 3
Training loss: 2.4635586738586426
Validation loss: 2.1711205500428394

Epoch: 6| Step: 4
Training loss: 2.90417742729187
Validation loss: 2.1700810693925425

Epoch: 6| Step: 5
Training loss: 1.64532470703125
Validation loss: 2.1840199373101674

Epoch: 6| Step: 6
Training loss: 1.8036739826202393
Validation loss: 2.214889439203406

Epoch: 6| Step: 7
Training loss: 2.5416102409362793
Validation loss: 2.2218680471502323

Epoch: 6| Step: 8
Training loss: 2.201289176940918
Validation loss: 2.2306973408627253

Epoch: 6| Step: 9
Training loss: 2.35733699798584
Validation loss: 2.24563786291307

Epoch: 6| Step: 10
Training loss: 2.2787396907806396
Validation loss: 2.2342011262011785

Epoch: 6| Step: 11
Training loss: 2.4865589141845703
Validation loss: 2.2261244071427213

Epoch: 6| Step: 12
Training loss: 2.132704257965088
Validation loss: 2.2229716803437922

Epoch: 6| Step: 13
Training loss: 1.4454874992370605
Validation loss: 2.2054814189992924

Epoch: 348| Step: 0
Training loss: 2.4559450149536133
Validation loss: 2.192065888835538

Epoch: 6| Step: 1
Training loss: 1.9604421854019165
Validation loss: 2.1959091578760455

Epoch: 6| Step: 2
Training loss: 1.8920170068740845
Validation loss: 2.1751926047827608

Epoch: 6| Step: 3
Training loss: 1.481410026550293
Validation loss: 2.194683870961589

Epoch: 6| Step: 4
Training loss: 1.6251804828643799
Validation loss: 2.1780184956007105

Epoch: 6| Step: 5
Training loss: 2.4120078086853027
Validation loss: 2.1881312106245305

Epoch: 6| Step: 6
Training loss: 2.362152576446533
Validation loss: 2.220856310218893

Epoch: 6| Step: 7
Training loss: 1.9714479446411133
Validation loss: 2.207868068448959

Epoch: 6| Step: 8
Training loss: 2.0014095306396484
Validation loss: 2.201323450252574

Epoch: 6| Step: 9
Training loss: 2.5726077556610107
Validation loss: 2.2289750114563973

Epoch: 6| Step: 10
Training loss: 3.0363826751708984
Validation loss: 2.2254661360094623

Epoch: 6| Step: 11
Training loss: 2.2720866203308105
Validation loss: 2.2289318858936267

Epoch: 6| Step: 12
Training loss: 3.1749157905578613
Validation loss: 2.2431086852986324

Epoch: 6| Step: 13
Training loss: 1.1799014806747437
Validation loss: 2.2429140306288198

Epoch: 349| Step: 0
Training loss: 1.9972691535949707
Validation loss: 2.2266268755799983

Epoch: 6| Step: 1
Training loss: 1.390568733215332
Validation loss: 2.2344240834636073

Epoch: 6| Step: 2
Training loss: 2.7609424591064453
Validation loss: 2.2041541812240437

Epoch: 6| Step: 3
Training loss: 2.8501594066619873
Validation loss: 2.203015124926003

Epoch: 6| Step: 4
Training loss: 2.058945417404175
Validation loss: 2.175038227470972

Epoch: 6| Step: 5
Training loss: 3.048867702484131
Validation loss: 2.168508488644836

Epoch: 6| Step: 6
Training loss: 2.6666345596313477
Validation loss: 2.1704498132069907

Epoch: 6| Step: 7
Training loss: 2.0788419246673584
Validation loss: 2.174745262310069

Epoch: 6| Step: 8
Training loss: 1.3297624588012695
Validation loss: 2.170626535210558

Epoch: 6| Step: 9
Training loss: 1.9325156211853027
Validation loss: 2.1823934611453804

Epoch: 6| Step: 10
Training loss: 2.330014705657959
Validation loss: 2.192863946319908

Epoch: 6| Step: 11
Training loss: 2.574340581893921
Validation loss: 2.2205487681973364

Epoch: 6| Step: 12
Training loss: 2.153538465499878
Validation loss: 2.224001266623056

Epoch: 6| Step: 13
Training loss: 1.1662763357162476
Validation loss: 2.259061449317522

Epoch: 350| Step: 0
Training loss: 2.5877180099487305
Validation loss: 2.2605430490227154

Epoch: 6| Step: 1
Training loss: 2.1606228351593018
Validation loss: 2.254881525552401

Epoch: 6| Step: 2
Training loss: 2.19468355178833
Validation loss: 2.247385871025824

Epoch: 6| Step: 3
Training loss: 1.7781505584716797
Validation loss: 2.2179576709706295

Epoch: 6| Step: 4
Training loss: 2.425551414489746
Validation loss: 2.2025189297173613

Epoch: 6| Step: 5
Training loss: 2.0997519493103027
Validation loss: 2.1898113027695687

Epoch: 6| Step: 6
Training loss: 2.041628360748291
Validation loss: 2.188854420056907

Epoch: 6| Step: 7
Training loss: 2.3036746978759766
Validation loss: 2.1767269898486394

Epoch: 6| Step: 8
Training loss: 2.912972927093506
Validation loss: 2.16624056395664

Epoch: 6| Step: 9
Training loss: 1.81118905544281
Validation loss: 2.1758809371661116

Epoch: 6| Step: 10
Training loss: 2.3511641025543213
Validation loss: 2.1859220150978333

Epoch: 6| Step: 11
Training loss: 2.2421507835388184
Validation loss: 2.1757738449240245

Epoch: 6| Step: 12
Training loss: 1.6124839782714844
Validation loss: 2.1774735835290726

Epoch: 6| Step: 13
Training loss: 2.2320609092712402
Validation loss: 2.1966843810132755

Epoch: 351| Step: 0
Training loss: 1.7307313680648804
Validation loss: 2.1965983093425794

Epoch: 6| Step: 1
Training loss: 1.9171769618988037
Validation loss: 2.191025669856738

Epoch: 6| Step: 2
Training loss: 2.545630693435669
Validation loss: 2.2044351652104366

Epoch: 6| Step: 3
Training loss: 1.411670207977295
Validation loss: 2.218723681665236

Epoch: 6| Step: 4
Training loss: 2.4232077598571777
Validation loss: 2.2274395163341234

Epoch: 6| Step: 5
Training loss: 2.578840494155884
Validation loss: 2.2173619577961583

Epoch: 6| Step: 6
Training loss: 2.5897650718688965
Validation loss: 2.2172637985598658

Epoch: 6| Step: 7
Training loss: 1.8082126379013062
Validation loss: 2.22302048437057

Epoch: 6| Step: 8
Training loss: 2.3015518188476562
Validation loss: 2.212289415380006

Epoch: 6| Step: 9
Training loss: 2.1300487518310547
Validation loss: 2.2071967253121

Epoch: 6| Step: 10
Training loss: 2.2267379760742188
Validation loss: 2.2305411946388984

Epoch: 6| Step: 11
Training loss: 2.5192036628723145
Validation loss: 2.20580788581602

Epoch: 6| Step: 12
Training loss: 2.3827016353607178
Validation loss: 2.215777679156232

Epoch: 6| Step: 13
Training loss: 1.9778450727462769
Validation loss: 2.1915875993749148

Epoch: 352| Step: 0
Training loss: 2.035879611968994
Validation loss: 2.199116263338315

Epoch: 6| Step: 1
Training loss: 2.764836072921753
Validation loss: 2.1792938837441067

Epoch: 6| Step: 2
Training loss: 2.044093132019043
Validation loss: 2.15829441624303

Epoch: 6| Step: 3
Training loss: 1.7162201404571533
Validation loss: 2.1676740697635117

Epoch: 6| Step: 4
Training loss: 2.558668375015259
Validation loss: 2.18713475555502

Epoch: 6| Step: 5
Training loss: 2.056303024291992
Validation loss: 2.1945597176910727

Epoch: 6| Step: 6
Training loss: 1.9362751245498657
Validation loss: 2.194570237590421

Epoch: 6| Step: 7
Training loss: 2.3720314502716064
Validation loss: 2.19278190212865

Epoch: 6| Step: 8
Training loss: 1.4723315238952637
Validation loss: 2.1996076260843584

Epoch: 6| Step: 9
Training loss: 2.869925022125244
Validation loss: 2.2127112137374056

Epoch: 6| Step: 10
Training loss: 1.9191172122955322
Validation loss: 2.2083074482538367

Epoch: 6| Step: 11
Training loss: 1.6616401672363281
Validation loss: 2.1877245774833103

Epoch: 6| Step: 12
Training loss: 2.2551915645599365
Validation loss: 2.208794870684224

Epoch: 6| Step: 13
Training loss: 3.3405818939208984
Validation loss: 2.1898099594218756

Epoch: 353| Step: 0
Training loss: 2.415337085723877
Validation loss: 2.1810847264464184

Epoch: 6| Step: 1
Training loss: 2.3991637229919434
Validation loss: 2.173166887734526

Epoch: 6| Step: 2
Training loss: 2.6559462547302246
Validation loss: 2.1759523268668883

Epoch: 6| Step: 3
Training loss: 3.167959213256836
Validation loss: 2.1588750731560493

Epoch: 6| Step: 4
Training loss: 1.8684608936309814
Validation loss: 2.1644135893032117

Epoch: 6| Step: 5
Training loss: 2.2335290908813477
Validation loss: 2.1709045107646654

Epoch: 6| Step: 6
Training loss: 2.347648859024048
Validation loss: 2.1541855514690442

Epoch: 6| Step: 7
Training loss: 1.692994236946106
Validation loss: 2.188032329723399

Epoch: 6| Step: 8
Training loss: 1.745347499847412
Validation loss: 2.195099474281393

Epoch: 6| Step: 9
Training loss: 2.1342217922210693
Validation loss: 2.207047390681441

Epoch: 6| Step: 10
Training loss: 2.7369589805603027
Validation loss: 2.2530880743457424

Epoch: 6| Step: 11
Training loss: 1.6285583972930908
Validation loss: 2.2436194932588966

Epoch: 6| Step: 12
Training loss: 1.930877685546875
Validation loss: 2.237801057036205

Epoch: 6| Step: 13
Training loss: 1.3894981145858765
Validation loss: 2.2442827609277542

Epoch: 354| Step: 0
Training loss: 2.406096935272217
Validation loss: 2.238491481350314

Epoch: 6| Step: 1
Training loss: 2.2349300384521484
Validation loss: 2.211189875038721

Epoch: 6| Step: 2
Training loss: 2.045133113861084
Validation loss: 2.2340096376275502

Epoch: 6| Step: 3
Training loss: 2.037816286087036
Validation loss: 2.2373031570065405

Epoch: 6| Step: 4
Training loss: 2.3685905933380127
Validation loss: 2.211628193496376

Epoch: 6| Step: 5
Training loss: 2.3186886310577393
Validation loss: 2.193266801936652

Epoch: 6| Step: 6
Training loss: 1.620964527130127
Validation loss: 2.1779089973818873

Epoch: 6| Step: 7
Training loss: 1.7480883598327637
Validation loss: 2.1736306272527224

Epoch: 6| Step: 8
Training loss: 2.9759435653686523
Validation loss: 2.1756089784765757

Epoch: 6| Step: 9
Training loss: 2.7189927101135254
Validation loss: 2.1572079709781113

Epoch: 6| Step: 10
Training loss: 1.9650144577026367
Validation loss: 2.1483451217733402

Epoch: 6| Step: 11
Training loss: 2.148390293121338
Validation loss: 2.157396721583541

Epoch: 6| Step: 12
Training loss: 2.0444631576538086
Validation loss: 2.156561500282698

Epoch: 6| Step: 13
Training loss: 1.370847225189209
Validation loss: 2.1549675695357786

Epoch: 355| Step: 0
Training loss: 2.265735149383545
Validation loss: 2.1772440582193355

Epoch: 6| Step: 1
Training loss: 2.122622013092041
Validation loss: 2.1758944565250027

Epoch: 6| Step: 2
Training loss: 2.611398696899414
Validation loss: 2.1851064774297897

Epoch: 6| Step: 3
Training loss: 2.313763380050659
Validation loss: 2.188374464229871

Epoch: 6| Step: 4
Training loss: 2.207404375076294
Validation loss: 2.1944035868490896

Epoch: 6| Step: 5
Training loss: 1.1612091064453125
Validation loss: 2.2033922620998916

Epoch: 6| Step: 6
Training loss: 2.348487377166748
Validation loss: 2.210956032558154

Epoch: 6| Step: 7
Training loss: 2.5674805641174316
Validation loss: 2.2032241436742965

Epoch: 6| Step: 8
Training loss: 2.388465404510498
Validation loss: 2.2197845776875815

Epoch: 6| Step: 9
Training loss: 2.452639579772949
Validation loss: 2.2242677301488896

Epoch: 6| Step: 10
Training loss: 2.382857322692871
Validation loss: 2.1897851818351337

Epoch: 6| Step: 11
Training loss: 1.4822843074798584
Validation loss: 2.174755883473222

Epoch: 6| Step: 12
Training loss: 2.1392130851745605
Validation loss: 2.158467690149943

Epoch: 6| Step: 13
Training loss: 1.8369532823562622
Validation loss: 2.1631177035711144

Epoch: 356| Step: 0
Training loss: 2.1209864616394043
Validation loss: 2.1551434557924987

Epoch: 6| Step: 1
Training loss: 2.019601821899414
Validation loss: 2.1625272689327115

Epoch: 6| Step: 2
Training loss: 1.9551748037338257
Validation loss: 2.1659918497967463

Epoch: 6| Step: 3
Training loss: 1.9058291912078857
Validation loss: 2.17086096220119

Epoch: 6| Step: 4
Training loss: 1.1901105642318726
Validation loss: 2.168390930339854

Epoch: 6| Step: 5
Training loss: 1.8585028648376465
Validation loss: 2.1790383105636923

Epoch: 6| Step: 6
Training loss: 2.52870774269104
Validation loss: 2.187896079914544

Epoch: 6| Step: 7
Training loss: 2.9390313625335693
Validation loss: 2.1819693619205105

Epoch: 6| Step: 8
Training loss: 1.6779115200042725
Validation loss: 2.175172359712662

Epoch: 6| Step: 9
Training loss: 2.216270923614502
Validation loss: 2.193696073306504

Epoch: 6| Step: 10
Training loss: 2.27376651763916
Validation loss: 2.2060644088252896

Epoch: 6| Step: 11
Training loss: 3.174776315689087
Validation loss: 2.2072113355000815

Epoch: 6| Step: 12
Training loss: 2.3464303016662598
Validation loss: 2.198984294809321

Epoch: 6| Step: 13
Training loss: 1.8962892293930054
Validation loss: 2.1948194606329805

Epoch: 357| Step: 0
Training loss: 2.6797194480895996
Validation loss: 2.206837366986018

Epoch: 6| Step: 1
Training loss: 2.0601301193237305
Validation loss: 2.180583820548109

Epoch: 6| Step: 2
Training loss: 2.11198353767395
Validation loss: 2.1710725497174006

Epoch: 6| Step: 3
Training loss: 2.2500455379486084
Validation loss: 2.1689651653330815

Epoch: 6| Step: 4
Training loss: 2.3684909343719482
Validation loss: 2.1715681681068997

Epoch: 6| Step: 5
Training loss: 2.023305892944336
Validation loss: 2.1701132277006745

Epoch: 6| Step: 6
Training loss: 2.091991901397705
Validation loss: 2.175448863737045

Epoch: 6| Step: 7
Training loss: 1.4566481113433838
Validation loss: 2.1750640946049846

Epoch: 6| Step: 8
Training loss: 2.375946521759033
Validation loss: 2.174137435933595

Epoch: 6| Step: 9
Training loss: 1.9162578582763672
Validation loss: 2.175598462422689

Epoch: 6| Step: 10
Training loss: 2.2146079540252686
Validation loss: 2.1993111307903

Epoch: 6| Step: 11
Training loss: 2.291386127471924
Validation loss: 2.205811157021471

Epoch: 6| Step: 12
Training loss: 2.176379919052124
Validation loss: 2.215868143625157

Epoch: 6| Step: 13
Training loss: 2.271334409713745
Validation loss: 2.228667838599092

Epoch: 358| Step: 0
Training loss: 2.436833143234253
Validation loss: 2.2564920404905915

Epoch: 6| Step: 1
Training loss: 3.4053516387939453
Validation loss: 2.2570367910528697

Epoch: 6| Step: 2
Training loss: 2.207180976867676
Validation loss: 2.268795495392174

Epoch: 6| Step: 3
Training loss: 2.4872312545776367
Validation loss: 2.270267809590986

Epoch: 6| Step: 4
Training loss: 2.21333646774292
Validation loss: 2.2436769854637886

Epoch: 6| Step: 5
Training loss: 2.0523133277893066
Validation loss: 2.2214905574757564

Epoch: 6| Step: 6
Training loss: 2.194739818572998
Validation loss: 2.1812753959368636

Epoch: 6| Step: 7
Training loss: 2.1128432750701904
Validation loss: 2.199517159051793

Epoch: 6| Step: 8
Training loss: 2.8765392303466797
Validation loss: 2.168643114387348

Epoch: 6| Step: 9
Training loss: 1.7208795547485352
Validation loss: 2.1761235895977227

Epoch: 6| Step: 10
Training loss: 1.4663455486297607
Validation loss: 2.1625149788395053

Epoch: 6| Step: 11
Training loss: 1.8898711204528809
Validation loss: 2.1644311669052287

Epoch: 6| Step: 12
Training loss: 1.4659194946289062
Validation loss: 2.1810152915216263

Epoch: 6| Step: 13
Training loss: 1.8807188272476196
Validation loss: 2.182917679509809

Epoch: 359| Step: 0
Training loss: 1.8639081716537476
Validation loss: 2.1865959628935783

Epoch: 6| Step: 1
Training loss: 1.782920479774475
Validation loss: 2.180486338112944

Epoch: 6| Step: 2
Training loss: 2.420941114425659
Validation loss: 2.1725699568307526

Epoch: 6| Step: 3
Training loss: 2.679903984069824
Validation loss: 2.1792998416449434

Epoch: 6| Step: 4
Training loss: 2.089693307876587
Validation loss: 2.1760557082391556

Epoch: 6| Step: 5
Training loss: 2.516170024871826
Validation loss: 2.1774391769081034

Epoch: 6| Step: 6
Training loss: 1.6352430582046509
Validation loss: 2.1994490520928496

Epoch: 6| Step: 7
Training loss: 2.603997230529785
Validation loss: 2.1888573810618412

Epoch: 6| Step: 8
Training loss: 2.722079277038574
Validation loss: 2.1820859473238707

Epoch: 6| Step: 9
Training loss: 2.0042972564697266
Validation loss: 2.187621557584373

Epoch: 6| Step: 10
Training loss: 2.008004903793335
Validation loss: 2.1684179600848945

Epoch: 6| Step: 11
Training loss: 1.3026351928710938
Validation loss: 2.173389966769885

Epoch: 6| Step: 12
Training loss: 2.041224718093872
Validation loss: 2.1935387016624532

Epoch: 6| Step: 13
Training loss: 2.7822048664093018
Validation loss: 2.2008146816684353

Epoch: 360| Step: 0
Training loss: 1.6767637729644775
Validation loss: 2.2282785536140524

Epoch: 6| Step: 1
Training loss: 2.1620659828186035
Validation loss: 2.225778677130258

Epoch: 6| Step: 2
Training loss: 2.0467958450317383
Validation loss: 2.2239799627693753

Epoch: 6| Step: 3
Training loss: 1.56928551197052
Validation loss: 2.2075244585673013

Epoch: 6| Step: 4
Training loss: 2.3634743690490723
Validation loss: 2.180356640969553

Epoch: 6| Step: 5
Training loss: 2.626376152038574
Validation loss: 2.1633564836235455

Epoch: 6| Step: 6
Training loss: 1.9026052951812744
Validation loss: 2.1767752990927747

Epoch: 6| Step: 7
Training loss: 2.9685893058776855
Validation loss: 2.1703425120281916

Epoch: 6| Step: 8
Training loss: 2.638547420501709
Validation loss: 2.1810022272089475

Epoch: 6| Step: 9
Training loss: 1.7087650299072266
Validation loss: 2.1807472680204656

Epoch: 6| Step: 10
Training loss: 2.1953914165496826
Validation loss: 2.1689837876186577

Epoch: 6| Step: 11
Training loss: 2.625624656677246
Validation loss: 2.178785075423538

Epoch: 6| Step: 12
Training loss: 1.6791255474090576
Validation loss: 2.1792718825801725

Epoch: 6| Step: 13
Training loss: 1.7009167671203613
Validation loss: 2.188180519688514

Epoch: 361| Step: 0
Training loss: 2.87056040763855
Validation loss: 2.2061718228042766

Epoch: 6| Step: 1
Training loss: 1.8568326234817505
Validation loss: 2.2026345922100927

Epoch: 6| Step: 2
Training loss: 2.0638279914855957
Validation loss: 2.1943735332899195

Epoch: 6| Step: 3
Training loss: 2.488097667694092
Validation loss: 2.190260426972502

Epoch: 6| Step: 4
Training loss: 2.3574438095092773
Validation loss: 2.1737991020243657

Epoch: 6| Step: 5
Training loss: 1.879494309425354
Validation loss: 2.167974095190725

Epoch: 6| Step: 6
Training loss: 1.4426896572113037
Validation loss: 2.186144005867743

Epoch: 6| Step: 7
Training loss: 2.2989046573638916
Validation loss: 2.177955545404906

Epoch: 6| Step: 8
Training loss: 1.9737248420715332
Validation loss: 2.192042507151122

Epoch: 6| Step: 9
Training loss: 2.343994140625
Validation loss: 2.200892745807607

Epoch: 6| Step: 10
Training loss: 2.3759615421295166
Validation loss: 2.2032857966679398

Epoch: 6| Step: 11
Training loss: 2.1549744606018066
Validation loss: 2.2049284506869573

Epoch: 6| Step: 12
Training loss: 2.2716281414031982
Validation loss: 2.206282728461809

Epoch: 6| Step: 13
Training loss: 1.7173504829406738
Validation loss: 2.2179933363391506

Epoch: 362| Step: 0
Training loss: 2.181364059448242
Validation loss: 2.2170025007699126

Epoch: 6| Step: 1
Training loss: 1.9631109237670898
Validation loss: 2.206473321043035

Epoch: 6| Step: 2
Training loss: 2.638432741165161
Validation loss: 2.224686127836986

Epoch: 6| Step: 3
Training loss: 1.6763739585876465
Validation loss: 2.2306763484913814

Epoch: 6| Step: 4
Training loss: 2.1332879066467285
Validation loss: 2.216191137990644

Epoch: 6| Step: 5
Training loss: 2.4492454528808594
Validation loss: 2.2074739317740164

Epoch: 6| Step: 6
Training loss: 1.8275072574615479
Validation loss: 2.1698631214839157

Epoch: 6| Step: 7
Training loss: 2.7304840087890625
Validation loss: 2.1781064771836802

Epoch: 6| Step: 8
Training loss: 1.7501894235610962
Validation loss: 2.1678412524602746

Epoch: 6| Step: 9
Training loss: 2.6934990882873535
Validation loss: 2.1726522343133086

Epoch: 6| Step: 10
Training loss: 1.6141979694366455
Validation loss: 2.160323002005136

Epoch: 6| Step: 11
Training loss: 2.367647647857666
Validation loss: 2.161654209577909

Epoch: 6| Step: 12
Training loss: 2.0323081016540527
Validation loss: 2.15112098955339

Epoch: 6| Step: 13
Training loss: 1.6873421669006348
Validation loss: 2.171232646511447

Epoch: 363| Step: 0
Training loss: 2.112213134765625
Validation loss: 2.1676450057696273

Epoch: 6| Step: 1
Training loss: 2.571605920791626
Validation loss: 2.184233004047025

Epoch: 6| Step: 2
Training loss: 1.9520940780639648
Validation loss: 2.192219921337661

Epoch: 6| Step: 3
Training loss: 2.0854363441467285
Validation loss: 2.172045887157481

Epoch: 6| Step: 4
Training loss: 2.4600937366485596
Validation loss: 2.1818292115324285

Epoch: 6| Step: 5
Training loss: 2.12416934967041
Validation loss: 2.190918271259595

Epoch: 6| Step: 6
Training loss: 2.042264461517334
Validation loss: 2.1886024346915622

Epoch: 6| Step: 7
Training loss: 1.9722480773925781
Validation loss: 2.1722022448816607

Epoch: 6| Step: 8
Training loss: 1.770419955253601
Validation loss: 2.191922859479022

Epoch: 6| Step: 9
Training loss: 2.5224428176879883
Validation loss: 2.1917327603986188

Epoch: 6| Step: 10
Training loss: 2.304569721221924
Validation loss: 2.168506153168217

Epoch: 6| Step: 11
Training loss: 1.6031465530395508
Validation loss: 2.1837486836218063

Epoch: 6| Step: 12
Training loss: 1.6722919940948486
Validation loss: 2.1896996549380723

Epoch: 6| Step: 13
Training loss: 3.2931289672851562
Validation loss: 2.183671787220945

Epoch: 364| Step: 0
Training loss: 2.7061095237731934
Validation loss: 2.163287333262864

Epoch: 6| Step: 1
Training loss: 2.0723958015441895
Validation loss: 2.1684853697335846

Epoch: 6| Step: 2
Training loss: 2.873239517211914
Validation loss: 2.1945174355660715

Epoch: 6| Step: 3
Training loss: 2.472134828567505
Validation loss: 2.2072661025549776

Epoch: 6| Step: 4
Training loss: 1.3625671863555908
Validation loss: 2.1929765849985103

Epoch: 6| Step: 5
Training loss: 2.282087564468384
Validation loss: 2.219177455030462

Epoch: 6| Step: 6
Training loss: 1.9014796018600464
Validation loss: 2.199897607167562

Epoch: 6| Step: 7
Training loss: 1.859699010848999
Validation loss: 2.2203944729220484

Epoch: 6| Step: 8
Training loss: 2.323509931564331
Validation loss: 2.2133341143208165

Epoch: 6| Step: 9
Training loss: 1.9274871349334717
Validation loss: 2.2002382124623945

Epoch: 6| Step: 10
Training loss: 2.486931562423706
Validation loss: 2.1955385823403635

Epoch: 6| Step: 11
Training loss: 1.7733867168426514
Validation loss: 2.1968092482577086

Epoch: 6| Step: 12
Training loss: 1.7747901678085327
Validation loss: 2.2188484361094813

Epoch: 6| Step: 13
Training loss: 2.1733856201171875
Validation loss: 2.2369075423927716

Epoch: 365| Step: 0
Training loss: 2.3026154041290283
Validation loss: 2.224377765450426

Epoch: 6| Step: 1
Training loss: 2.946877956390381
Validation loss: 2.2067084030438493

Epoch: 6| Step: 2
Training loss: 2.6442410945892334
Validation loss: 2.212483057411768

Epoch: 6| Step: 3
Training loss: 2.095210075378418
Validation loss: 2.207241781296269

Epoch: 6| Step: 4
Training loss: 1.9079711437225342
Validation loss: 2.199045968312089

Epoch: 6| Step: 5
Training loss: 1.546170711517334
Validation loss: 2.1912689132075154

Epoch: 6| Step: 6
Training loss: 2.0651936531066895
Validation loss: 2.187256015757079

Epoch: 6| Step: 7
Training loss: 1.7603058815002441
Validation loss: 2.180607347078221

Epoch: 6| Step: 8
Training loss: 2.870903968811035
Validation loss: 2.169163079671962

Epoch: 6| Step: 9
Training loss: 1.444918155670166
Validation loss: 2.155727215992507

Epoch: 6| Step: 10
Training loss: 1.801881194114685
Validation loss: 2.1628657771695043

Epoch: 6| Step: 11
Training loss: 1.841524362564087
Validation loss: 2.1605870454542098

Epoch: 6| Step: 12
Training loss: 2.151045322418213
Validation loss: 2.174015704021659

Epoch: 6| Step: 13
Training loss: 2.877063035964966
Validation loss: 2.191527348692699

Epoch: 366| Step: 0
Training loss: 2.723980188369751
Validation loss: 2.185272119378531

Epoch: 6| Step: 1
Training loss: 2.423086166381836
Validation loss: 2.187246186758882

Epoch: 6| Step: 2
Training loss: 1.829843521118164
Validation loss: 2.185360518834924

Epoch: 6| Step: 3
Training loss: 2.5392794609069824
Validation loss: 2.2075788769670712

Epoch: 6| Step: 4
Training loss: 1.5416696071624756
Validation loss: 2.215790053849579

Epoch: 6| Step: 5
Training loss: 1.5155997276306152
Validation loss: 2.2102160684524046

Epoch: 6| Step: 6
Training loss: 1.9833393096923828
Validation loss: 2.1854568412227016

Epoch: 6| Step: 7
Training loss: 1.9730808734893799
Validation loss: 2.181069512521067

Epoch: 6| Step: 8
Training loss: 2.3599820137023926
Validation loss: 2.1719784070086736

Epoch: 6| Step: 9
Training loss: 2.283146858215332
Validation loss: 2.1711412475955103

Epoch: 6| Step: 10
Training loss: 2.0488312244415283
Validation loss: 2.1706687096626527

Epoch: 6| Step: 11
Training loss: 2.012091636657715
Validation loss: 2.174279055287761

Epoch: 6| Step: 12
Training loss: 2.620668888092041
Validation loss: 2.183324542096866

Epoch: 6| Step: 13
Training loss: 1.8310250043869019
Validation loss: 2.18845413320808

Epoch: 367| Step: 0
Training loss: 1.5702096223831177
Validation loss: 2.199216170977521

Epoch: 6| Step: 1
Training loss: 1.6381336450576782
Validation loss: 2.200943608437815

Epoch: 6| Step: 2
Training loss: 2.051732063293457
Validation loss: 2.2164270608655867

Epoch: 6| Step: 3
Training loss: 1.6236268281936646
Validation loss: 2.2174965258567565

Epoch: 6| Step: 4
Training loss: 2.3898510932922363
Validation loss: 2.2216199623641146

Epoch: 6| Step: 5
Training loss: 2.069272518157959
Validation loss: 2.183234932602093

Epoch: 6| Step: 6
Training loss: 1.8290667533874512
Validation loss: 2.2000765851748887

Epoch: 6| Step: 7
Training loss: 2.0354034900665283
Validation loss: 2.1994575403069936

Epoch: 6| Step: 8
Training loss: 1.9676192998886108
Validation loss: 2.2040746237642024

Epoch: 6| Step: 9
Training loss: 3.01693058013916
Validation loss: 2.1931767925139396

Epoch: 6| Step: 10
Training loss: 2.6153876781463623
Validation loss: 2.1860867495177896

Epoch: 6| Step: 11
Training loss: 1.991546869277954
Validation loss: 2.186973223122217

Epoch: 6| Step: 12
Training loss: 2.9048168659210205
Validation loss: 2.1667292374436573

Epoch: 6| Step: 13
Training loss: 2.1177773475646973
Validation loss: 2.152638417418285

Epoch: 368| Step: 0
Training loss: 1.743014931678772
Validation loss: 2.170645752260762

Epoch: 6| Step: 1
Training loss: 2.768348455429077
Validation loss: 2.1551795108343965

Epoch: 6| Step: 2
Training loss: 2.2100207805633545
Validation loss: 2.157460238343926

Epoch: 6| Step: 3
Training loss: 2.4456562995910645
Validation loss: 2.17481965275221

Epoch: 6| Step: 4
Training loss: 2.1793289184570312
Validation loss: 2.1905663910732476

Epoch: 6| Step: 5
Training loss: 1.941019058227539
Validation loss: 2.1929416195038827

Epoch: 6| Step: 6
Training loss: 2.034301996231079
Validation loss: 2.1959487699693248

Epoch: 6| Step: 7
Training loss: 2.4137144088745117
Validation loss: 2.1760370244262037

Epoch: 6| Step: 8
Training loss: 1.9047280550003052
Validation loss: 2.1907600869414625

Epoch: 6| Step: 9
Training loss: 2.003848075866699
Validation loss: 2.175747258688814

Epoch: 6| Step: 10
Training loss: 1.840479850769043
Validation loss: 2.162236790503225

Epoch: 6| Step: 11
Training loss: 2.201035499572754
Validation loss: 2.1590713224103375

Epoch: 6| Step: 12
Training loss: 2.106656551361084
Validation loss: 2.162710105219195

Epoch: 6| Step: 13
Training loss: 1.7511109113693237
Validation loss: 2.153371839113133

Epoch: 369| Step: 0
Training loss: 2.028923749923706
Validation loss: 2.1516903984931206

Epoch: 6| Step: 1
Training loss: 1.6947386264801025
Validation loss: 2.142326151171038

Epoch: 6| Step: 2
Training loss: 2.0462164878845215
Validation loss: 2.147260924821259

Epoch: 6| Step: 3
Training loss: 1.6009657382965088
Validation loss: 2.16733258129448

Epoch: 6| Step: 4
Training loss: 2.4548749923706055
Validation loss: 2.1757552726294405

Epoch: 6| Step: 5
Training loss: 1.9994544982910156
Validation loss: 2.162175823283452

Epoch: 6| Step: 6
Training loss: 1.8356330394744873
Validation loss: 2.1606310631639216

Epoch: 6| Step: 7
Training loss: 2.382274627685547
Validation loss: 2.173422731379027

Epoch: 6| Step: 8
Training loss: 2.658653736114502
Validation loss: 2.156807614910987

Epoch: 6| Step: 9
Training loss: 1.7785943746566772
Validation loss: 2.183884543757285

Epoch: 6| Step: 10
Training loss: 1.8531914949417114
Validation loss: 2.179628379883305

Epoch: 6| Step: 11
Training loss: 2.844303607940674
Validation loss: 2.202696038830665

Epoch: 6| Step: 12
Training loss: 2.023030996322632
Validation loss: 2.202517801715482

Epoch: 6| Step: 13
Training loss: 2.711338996887207
Validation loss: 2.213390146532366

Epoch: 370| Step: 0
Training loss: 2.3645191192626953
Validation loss: 2.2166425181973364

Epoch: 6| Step: 1
Training loss: 2.1614620685577393
Validation loss: 2.2218239358676377

Epoch: 6| Step: 2
Training loss: 2.5969271659851074
Validation loss: 2.190723178207233

Epoch: 6| Step: 3
Training loss: 1.5296425819396973
Validation loss: 2.2085037192990704

Epoch: 6| Step: 4
Training loss: 2.189507484436035
Validation loss: 2.179324952504968

Epoch: 6| Step: 5
Training loss: 2.5082035064697266
Validation loss: 2.186607690267665

Epoch: 6| Step: 6
Training loss: 1.7496782541275024
Validation loss: 2.179881816269249

Epoch: 6| Step: 7
Training loss: 1.909019947052002
Validation loss: 2.1672190927690074

Epoch: 6| Step: 8
Training loss: 1.985090970993042
Validation loss: 2.166497112602316

Epoch: 6| Step: 9
Training loss: 2.1947755813598633
Validation loss: 2.160054970813054

Epoch: 6| Step: 10
Training loss: 2.3687894344329834
Validation loss: 2.1565422781052126

Epoch: 6| Step: 11
Training loss: 1.817716121673584
Validation loss: 2.177576122745391

Epoch: 6| Step: 12
Training loss: 2.3645565509796143
Validation loss: 2.184461280863772

Epoch: 6| Step: 13
Training loss: 1.8832253217697144
Validation loss: 2.1882908805724113

Epoch: 371| Step: 0
Training loss: 2.081604242324829
Validation loss: 2.182227588468982

Epoch: 6| Step: 1
Training loss: 1.5262563228607178
Validation loss: 2.1978004696548625

Epoch: 6| Step: 2
Training loss: 1.6754519939422607
Validation loss: 2.211233728675432

Epoch: 6| Step: 3
Training loss: 2.2037887573242188
Validation loss: 2.200404340221036

Epoch: 6| Step: 4
Training loss: 1.2741198539733887
Validation loss: 2.2040784128250612

Epoch: 6| Step: 5
Training loss: 2.2515201568603516
Validation loss: 2.188174945051952

Epoch: 6| Step: 6
Training loss: 1.9133996963500977
Validation loss: 2.179472461823494

Epoch: 6| Step: 7
Training loss: 2.6191611289978027
Validation loss: 2.1962494978340725

Epoch: 6| Step: 8
Training loss: 2.448498249053955
Validation loss: 2.158401740494595

Epoch: 6| Step: 9
Training loss: 2.903045654296875
Validation loss: 2.1445443604582097

Epoch: 6| Step: 10
Training loss: 1.7010060548782349
Validation loss: 2.1395798857494066

Epoch: 6| Step: 11
Training loss: 2.4007203578948975
Validation loss: 2.1494450863971504

Epoch: 6| Step: 12
Training loss: 2.1886959075927734
Validation loss: 2.1523477390248287

Epoch: 6| Step: 13
Training loss: 2.6193904876708984
Validation loss: 2.1615725383963635

Epoch: 372| Step: 0
Training loss: 2.668517589569092
Validation loss: 2.152968765586935

Epoch: 6| Step: 1
Training loss: 1.9530770778656006
Validation loss: 2.200910101654709

Epoch: 6| Step: 2
Training loss: 1.462720513343811
Validation loss: 2.208181501716696

Epoch: 6| Step: 3
Training loss: 2.0676498413085938
Validation loss: 2.20883418667701

Epoch: 6| Step: 4
Training loss: 1.9387792348861694
Validation loss: 2.2039649383996123

Epoch: 6| Step: 5
Training loss: 2.5126187801361084
Validation loss: 2.1751509071678243

Epoch: 6| Step: 6
Training loss: 1.7050045728683472
Validation loss: 2.150853528771349

Epoch: 6| Step: 7
Training loss: 1.9348852634429932
Validation loss: 2.1483790156661824

Epoch: 6| Step: 8
Training loss: 2.2617459297180176
Validation loss: 2.138670949525731

Epoch: 6| Step: 9
Training loss: 2.164072036743164
Validation loss: 2.1376973275215394

Epoch: 6| Step: 10
Training loss: 2.2740602493286133
Validation loss: 2.1350925045628704

Epoch: 6| Step: 11
Training loss: 2.514962673187256
Validation loss: 2.151728509574808

Epoch: 6| Step: 12
Training loss: 1.9940178394317627
Validation loss: 2.1441798287053264

Epoch: 6| Step: 13
Training loss: 2.173131227493286
Validation loss: 2.1525472748664116

Epoch: 373| Step: 0
Training loss: 1.982064962387085
Validation loss: 2.1696714970373336

Epoch: 6| Step: 1
Training loss: 2.3499104976654053
Validation loss: 2.2089638479294313

Epoch: 6| Step: 2
Training loss: 2.4781455993652344
Validation loss: 2.2252957026163735

Epoch: 6| Step: 3
Training loss: 1.6980140209197998
Validation loss: 2.234460525615241

Epoch: 6| Step: 4
Training loss: 2.3122987747192383
Validation loss: 2.2605616507991666

Epoch: 6| Step: 5
Training loss: 1.4637062549591064
Validation loss: 2.249437214225851

Epoch: 6| Step: 6
Training loss: 1.7102749347686768
Validation loss: 2.219830410454863

Epoch: 6| Step: 7
Training loss: 2.0622358322143555
Validation loss: 2.1703649156837055

Epoch: 6| Step: 8
Training loss: 2.0448360443115234
Validation loss: 2.1584805698804956

Epoch: 6| Step: 9
Training loss: 2.429069995880127
Validation loss: 2.1642345433594077

Epoch: 6| Step: 10
Training loss: 2.0211000442504883
Validation loss: 2.1454198206624677

Epoch: 6| Step: 11
Training loss: 2.4172253608703613
Validation loss: 2.145809660675705

Epoch: 6| Step: 12
Training loss: 2.4992847442626953
Validation loss: 2.1586138330480105

Epoch: 6| Step: 13
Training loss: 2.3524768352508545
Validation loss: 2.150154129151375

Epoch: 374| Step: 0
Training loss: 2.6371231079101562
Validation loss: 2.1540467508377565

Epoch: 6| Step: 1
Training loss: 2.394770860671997
Validation loss: 2.1650642002782514

Epoch: 6| Step: 2
Training loss: 1.9700227975845337
Validation loss: 2.1631426708672636

Epoch: 6| Step: 3
Training loss: 1.5364446640014648
Validation loss: 2.1524609968226445

Epoch: 6| Step: 4
Training loss: 2.040271520614624
Validation loss: 2.146842510469498

Epoch: 6| Step: 5
Training loss: 1.7364052534103394
Validation loss: 2.144877633740825

Epoch: 6| Step: 6
Training loss: 2.9260997772216797
Validation loss: 2.1514981741546304

Epoch: 6| Step: 7
Training loss: 1.7587708234786987
Validation loss: 2.1558067901160127

Epoch: 6| Step: 8
Training loss: 2.708415985107422
Validation loss: 2.17678955934381

Epoch: 6| Step: 9
Training loss: 1.5846673250198364
Validation loss: 2.194926469556747

Epoch: 6| Step: 10
Training loss: 2.898433208465576
Validation loss: 2.225469578978836

Epoch: 6| Step: 11
Training loss: 1.7183489799499512
Validation loss: 2.2292963433009323

Epoch: 6| Step: 12
Training loss: 1.8611541986465454
Validation loss: 2.2419663231859923

Epoch: 6| Step: 13
Training loss: 1.7595715522766113
Validation loss: 2.221104157868252

Epoch: 375| Step: 0
Training loss: 1.6821227073669434
Validation loss: 2.218880106044072

Epoch: 6| Step: 1
Training loss: 1.9007989168167114
Validation loss: 2.1634473287931053

Epoch: 6| Step: 2
Training loss: 1.9766135215759277
Validation loss: 2.163440360817858

Epoch: 6| Step: 3
Training loss: 1.272101640701294
Validation loss: 2.148770788664459

Epoch: 6| Step: 4
Training loss: 2.207484722137451
Validation loss: 2.154190994078113

Epoch: 6| Step: 5
Training loss: 1.9325571060180664
Validation loss: 2.133177613699308

Epoch: 6| Step: 6
Training loss: 2.307164192199707
Validation loss: 2.1384226788756666

Epoch: 6| Step: 7
Training loss: 2.1220781803131104
Validation loss: 2.144267178350879

Epoch: 6| Step: 8
Training loss: 1.786573052406311
Validation loss: 2.158596069582047

Epoch: 6| Step: 9
Training loss: 2.231511116027832
Validation loss: 2.1562435832074893

Epoch: 6| Step: 10
Training loss: 2.512760877609253
Validation loss: 2.167485744722428

Epoch: 6| Step: 11
Training loss: 2.2344789505004883
Validation loss: 2.191023567671417

Epoch: 6| Step: 12
Training loss: 2.941338539123535
Validation loss: 2.1923882935636785

Epoch: 6| Step: 13
Training loss: 2.563231945037842
Validation loss: 2.1960609061743623

Epoch: 376| Step: 0
Training loss: 1.6700546741485596
Validation loss: 2.2016796501733924

Epoch: 6| Step: 1
Training loss: 2.532344341278076
Validation loss: 2.216882631342898

Epoch: 6| Step: 2
Training loss: 2.1369895935058594
Validation loss: 2.227319743043633

Epoch: 6| Step: 3
Training loss: 1.8972349166870117
Validation loss: 2.2230134420497443

Epoch: 6| Step: 4
Training loss: 2.4823050498962402
Validation loss: 2.219003931168587

Epoch: 6| Step: 5
Training loss: 2.1488003730773926
Validation loss: 2.2129107111243793

Epoch: 6| Step: 6
Training loss: 2.28776216506958
Validation loss: 2.213608528978081

Epoch: 6| Step: 7
Training loss: 1.8799937963485718
Validation loss: 2.1986652535776936

Epoch: 6| Step: 8
Training loss: 2.023026943206787
Validation loss: 2.1820278321543047

Epoch: 6| Step: 9
Training loss: 1.9884881973266602
Validation loss: 2.1680819039703696

Epoch: 6| Step: 10
Training loss: 1.6533981561660767
Validation loss: 2.162056951112645

Epoch: 6| Step: 11
Training loss: 2.054683208465576
Validation loss: 2.140301009660126

Epoch: 6| Step: 12
Training loss: 2.394667387008667
Validation loss: 2.1411592729630007

Epoch: 6| Step: 13
Training loss: 2.49442195892334
Validation loss: 2.1391407071903186

Epoch: 377| Step: 0
Training loss: 1.2863775491714478
Validation loss: 2.1509092879551712

Epoch: 6| Step: 1
Training loss: 2.683763027191162
Validation loss: 2.143826841026224

Epoch: 6| Step: 2
Training loss: 2.2136154174804688
Validation loss: 2.148462972333354

Epoch: 6| Step: 3
Training loss: 2.626232147216797
Validation loss: 2.162027462836235

Epoch: 6| Step: 4
Training loss: 2.035396099090576
Validation loss: 2.1645510709413918

Epoch: 6| Step: 5
Training loss: 2.209350824356079
Validation loss: 2.193529495628931

Epoch: 6| Step: 6
Training loss: 2.547783136367798
Validation loss: 2.179136501845493

Epoch: 6| Step: 7
Training loss: 1.5804444551467896
Validation loss: 2.210297784497661

Epoch: 6| Step: 8
Training loss: 1.959767460823059
Validation loss: 2.211842554871754

Epoch: 6| Step: 9
Training loss: 1.274512529373169
Validation loss: 2.2037535393109886

Epoch: 6| Step: 10
Training loss: 2.258586883544922
Validation loss: 2.1587787776864986

Epoch: 6| Step: 11
Training loss: 1.942530632019043
Validation loss: 2.170488581862501

Epoch: 6| Step: 12
Training loss: 2.0859689712524414
Validation loss: 2.13350413691613

Epoch: 6| Step: 13
Training loss: 2.9785330295562744
Validation loss: 2.1497741886364516

Epoch: 378| Step: 0
Training loss: 2.5281121730804443
Validation loss: 2.1351996185959026

Epoch: 6| Step: 1
Training loss: 2.6131842136383057
Validation loss: 2.1501003747345298

Epoch: 6| Step: 2
Training loss: 1.6646921634674072
Validation loss: 2.135782023911835

Epoch: 6| Step: 3
Training loss: 1.950703501701355
Validation loss: 2.1424572698531614

Epoch: 6| Step: 4
Training loss: 2.7948756217956543
Validation loss: 2.145530298192014

Epoch: 6| Step: 5
Training loss: 2.097107410430908
Validation loss: 2.1477204343324066

Epoch: 6| Step: 6
Training loss: 1.7908437252044678
Validation loss: 2.1728492218961

Epoch: 6| Step: 7
Training loss: 1.9700031280517578
Validation loss: 2.183230218066964

Epoch: 6| Step: 8
Training loss: 1.8896448612213135
Validation loss: 2.2086662118152907

Epoch: 6| Step: 9
Training loss: 1.8370379209518433
Validation loss: 2.2254934259640273

Epoch: 6| Step: 10
Training loss: 1.9418840408325195
Validation loss: 2.250030040740967

Epoch: 6| Step: 11
Training loss: 2.3238625526428223
Validation loss: 2.259651924974175

Epoch: 6| Step: 12
Training loss: 1.5663763284683228
Validation loss: 2.2435646134038127

Epoch: 6| Step: 13
Training loss: 2.7073469161987305
Validation loss: 2.2579614641845867

Epoch: 379| Step: 0
Training loss: 2.0859193801879883
Validation loss: 2.192572121979088

Epoch: 6| Step: 1
Training loss: 1.872817873954773
Validation loss: 2.1553992584187496

Epoch: 6| Step: 2
Training loss: 2.832261085510254
Validation loss: 2.1402455042767268

Epoch: 6| Step: 3
Training loss: 2.0526726245880127
Validation loss: 2.1190411185705536

Epoch: 6| Step: 4
Training loss: 1.9763257503509521
Validation loss: 2.1181072778599237

Epoch: 6| Step: 5
Training loss: 1.5223698616027832
Validation loss: 2.1148751704923567

Epoch: 6| Step: 6
Training loss: 2.295571804046631
Validation loss: 2.110917527188537

Epoch: 6| Step: 7
Training loss: 2.811873435974121
Validation loss: 2.1178553668401574

Epoch: 6| Step: 8
Training loss: 1.3071167469024658
Validation loss: 2.1120056285653064

Epoch: 6| Step: 9
Training loss: 2.0520708560943604
Validation loss: 2.127062923164778

Epoch: 6| Step: 10
Training loss: 2.194416046142578
Validation loss: 2.126431626658286

Epoch: 6| Step: 11
Training loss: 2.300929546356201
Validation loss: 2.1419269679695048

Epoch: 6| Step: 12
Training loss: 2.105104446411133
Validation loss: 2.138233184814453

Epoch: 6| Step: 13
Training loss: 2.5237319469451904
Validation loss: 2.1426482559532247

Epoch: 380| Step: 0
Training loss: 1.7628216743469238
Validation loss: 2.159236572122061

Epoch: 6| Step: 1
Training loss: 2.1346242427825928
Validation loss: 2.168176699710149

Epoch: 6| Step: 2
Training loss: 1.803041696548462
Validation loss: 2.166541399494294

Epoch: 6| Step: 3
Training loss: 2.722379207611084
Validation loss: 2.1919417227468183

Epoch: 6| Step: 4
Training loss: 2.300058364868164
Validation loss: 2.2164932092030845

Epoch: 6| Step: 5
Training loss: 1.7265465259552002
Validation loss: 2.230991960853659

Epoch: 6| Step: 6
Training loss: 2.0950660705566406
Validation loss: 2.23409435056871

Epoch: 6| Step: 7
Training loss: 1.9886130094528198
Validation loss: 2.2334320160650436

Epoch: 6| Step: 8
Training loss: 2.2614259719848633
Validation loss: 2.19668552952428

Epoch: 6| Step: 9
Training loss: 1.9564188718795776
Validation loss: 2.195110951700518

Epoch: 6| Step: 10
Training loss: 2.000511646270752
Validation loss: 2.1672481054900796

Epoch: 6| Step: 11
Training loss: 2.408024311065674
Validation loss: 2.1563871727194837

Epoch: 6| Step: 12
Training loss: 2.3770675659179688
Validation loss: 2.1351478253641436

Epoch: 6| Step: 13
Training loss: 1.6989398002624512
Validation loss: 2.13712046223302

Epoch: 381| Step: 0
Training loss: 2.3718252182006836
Validation loss: 2.1197483001216764

Epoch: 6| Step: 1
Training loss: 2.1060190200805664
Validation loss: 2.117210395874516

Epoch: 6| Step: 2
Training loss: 1.8196909427642822
Validation loss: 2.126776854197184

Epoch: 6| Step: 3
Training loss: 2.101268768310547
Validation loss: 2.105566309344384

Epoch: 6| Step: 4
Training loss: 2.5628647804260254
Validation loss: 2.128005555880967

Epoch: 6| Step: 5
Training loss: 1.716171145439148
Validation loss: 2.1304639962411698

Epoch: 6| Step: 6
Training loss: 1.8264870643615723
Validation loss: 2.1433039762640513

Epoch: 6| Step: 7
Training loss: 1.8220455646514893
Validation loss: 2.1689272567790043

Epoch: 6| Step: 8
Training loss: 2.613757610321045
Validation loss: 2.1557614111131236

Epoch: 6| Step: 9
Training loss: 2.3415367603302
Validation loss: 2.1864648378023537

Epoch: 6| Step: 10
Training loss: 1.5339524745941162
Validation loss: 2.1946231075512466

Epoch: 6| Step: 11
Training loss: 2.4563348293304443
Validation loss: 2.1834706003947923

Epoch: 6| Step: 12
Training loss: 2.106761932373047
Validation loss: 2.189362079866471

Epoch: 6| Step: 13
Training loss: 1.4548001289367676
Validation loss: 2.2126373142324467

Epoch: 382| Step: 0
Training loss: 2.2513444423675537
Validation loss: 2.2098439406323176

Epoch: 6| Step: 1
Training loss: 2.224019765853882
Validation loss: 2.201480678332749

Epoch: 6| Step: 2
Training loss: 1.4989492893218994
Validation loss: 2.1928723781339583

Epoch: 6| Step: 3
Training loss: 2.3101985454559326
Validation loss: 2.1843820618044947

Epoch: 6| Step: 4
Training loss: 1.6910609006881714
Validation loss: 2.1832030537307903

Epoch: 6| Step: 5
Training loss: 2.361865520477295
Validation loss: 2.1745564347954205

Epoch: 6| Step: 6
Training loss: 2.820972204208374
Validation loss: 2.1560341235130065

Epoch: 6| Step: 7
Training loss: 1.7823556661605835
Validation loss: 2.143728486953243

Epoch: 6| Step: 8
Training loss: 2.115459680557251
Validation loss: 2.144475652325538

Epoch: 6| Step: 9
Training loss: 2.462113618850708
Validation loss: 2.161282317612761

Epoch: 6| Step: 10
Training loss: 2.5448317527770996
Validation loss: 2.1526370740705922

Epoch: 6| Step: 11
Training loss: 1.857665777206421
Validation loss: 2.1412870422486336

Epoch: 6| Step: 12
Training loss: 2.0184836387634277
Validation loss: 2.15733072321902

Epoch: 6| Step: 13
Training loss: 0.8205868601799011
Validation loss: 2.1558184392990603

Epoch: 383| Step: 0
Training loss: 1.5035008192062378
Validation loss: 2.1469480786272275

Epoch: 6| Step: 1
Training loss: 1.8906927108764648
Validation loss: 2.174572188367126

Epoch: 6| Step: 2
Training loss: 2.7745957374572754
Validation loss: 2.1745932691840717

Epoch: 6| Step: 3
Training loss: 1.566946268081665
Validation loss: 2.19656353868464

Epoch: 6| Step: 4
Training loss: 2.3040547370910645
Validation loss: 2.2152417141904115

Epoch: 6| Step: 5
Training loss: 2.3384227752685547
Validation loss: 2.2020666368546022

Epoch: 6| Step: 6
Training loss: 2.501558303833008
Validation loss: 2.193477581906062

Epoch: 6| Step: 7
Training loss: 2.0725700855255127
Validation loss: 2.174865730347172

Epoch: 6| Step: 8
Training loss: 2.338524103164673
Validation loss: 2.185352881749471

Epoch: 6| Step: 9
Training loss: 1.6968189477920532
Validation loss: 2.150919137462493

Epoch: 6| Step: 10
Training loss: 1.839751958847046
Validation loss: 2.1440748527485836

Epoch: 6| Step: 11
Training loss: 2.5030508041381836
Validation loss: 2.151740394612794

Epoch: 6| Step: 12
Training loss: 1.8741180896759033
Validation loss: 2.158753502753473

Epoch: 6| Step: 13
Training loss: 1.8784241676330566
Validation loss: 2.153000754694785

Epoch: 384| Step: 0
Training loss: 2.6672544479370117
Validation loss: 2.1607105578145673

Epoch: 6| Step: 1
Training loss: 1.8409392833709717
Validation loss: 2.1756638890953472

Epoch: 6| Step: 2
Training loss: 2.64786434173584
Validation loss: 2.1772564688036518

Epoch: 6| Step: 3
Training loss: 2.494717597961426
Validation loss: 2.2119986177772604

Epoch: 6| Step: 4
Training loss: 2.421036720275879
Validation loss: 2.2121609974932928

Epoch: 6| Step: 5
Training loss: 1.9404973983764648
Validation loss: 2.2073396457138883

Epoch: 6| Step: 6
Training loss: 2.190992832183838
Validation loss: 2.2128564157793598

Epoch: 6| Step: 7
Training loss: 1.7774113416671753
Validation loss: 2.1952676721798476

Epoch: 6| Step: 8
Training loss: 1.7229692935943604
Validation loss: 2.1669617532401957

Epoch: 6| Step: 9
Training loss: 2.635012626647949
Validation loss: 2.17383506221156

Epoch: 6| Step: 10
Training loss: 1.8033289909362793
Validation loss: 2.150867723649548

Epoch: 6| Step: 11
Training loss: 1.2169755697250366
Validation loss: 2.128257736083

Epoch: 6| Step: 12
Training loss: 1.8961970806121826
Validation loss: 2.141854634848974

Epoch: 6| Step: 13
Training loss: 1.5074280500411987
Validation loss: 2.133850941094019

Epoch: 385| Step: 0
Training loss: 1.7869365215301514
Validation loss: 2.133294513148646

Epoch: 6| Step: 1
Training loss: 2.0159196853637695
Validation loss: 2.140368705154747

Epoch: 6| Step: 2
Training loss: 1.6913866996765137
Validation loss: 2.156297336342514

Epoch: 6| Step: 3
Training loss: 2.3458337783813477
Validation loss: 2.1686656628885577

Epoch: 6| Step: 4
Training loss: 1.9992083311080933
Validation loss: 2.1635905388862855

Epoch: 6| Step: 5
Training loss: 2.201765537261963
Validation loss: 2.156962266532324

Epoch: 6| Step: 6
Training loss: 1.7776180505752563
Validation loss: 2.1600730162794872

Epoch: 6| Step: 7
Training loss: 2.317333221435547
Validation loss: 2.171422573827928

Epoch: 6| Step: 8
Training loss: 2.6744112968444824
Validation loss: 2.1720080298762166

Epoch: 6| Step: 9
Training loss: 1.3425309658050537
Validation loss: 2.1752570470174155

Epoch: 6| Step: 10
Training loss: 1.4130240678787231
Validation loss: 2.15975171519864

Epoch: 6| Step: 11
Training loss: 2.058788776397705
Validation loss: 2.1641126140471427

Epoch: 6| Step: 12
Training loss: 3.105635166168213
Validation loss: 2.14964799727163

Epoch: 6| Step: 13
Training loss: 2.0981600284576416
Validation loss: 2.156047673635585

Epoch: 386| Step: 0
Training loss: 1.8937668800354004
Validation loss: 2.14938715452789

Epoch: 6| Step: 1
Training loss: 1.9446868896484375
Validation loss: 2.140305830586341

Epoch: 6| Step: 2
Training loss: 1.8147320747375488
Validation loss: 2.140319166644927

Epoch: 6| Step: 3
Training loss: 2.399728775024414
Validation loss: 2.1203313899296585

Epoch: 6| Step: 4
Training loss: 1.7289727926254272
Validation loss: 2.1284494156478555

Epoch: 6| Step: 5
Training loss: 1.9185327291488647
Validation loss: 2.128487886921052

Epoch: 6| Step: 6
Training loss: 1.8114714622497559
Validation loss: 2.1298311013047413

Epoch: 6| Step: 7
Training loss: 2.4155783653259277
Validation loss: 2.1491132128623223

Epoch: 6| Step: 8
Training loss: 2.6480250358581543
Validation loss: 2.153376115265713

Epoch: 6| Step: 9
Training loss: 1.468736171722412
Validation loss: 2.1707093433667253

Epoch: 6| Step: 10
Training loss: 2.4348583221435547
Validation loss: 2.176257651339295

Epoch: 6| Step: 11
Training loss: 2.404484272003174
Validation loss: 2.1736975716006373

Epoch: 6| Step: 12
Training loss: 1.8258638381958008
Validation loss: 2.219649412298715

Epoch: 6| Step: 13
Training loss: 2.5706899166107178
Validation loss: 2.229004103650329

Epoch: 387| Step: 0
Training loss: 2.3088274002075195
Validation loss: 2.210114834129169

Epoch: 6| Step: 1
Training loss: 2.3793020248413086
Validation loss: 2.2082052807654104

Epoch: 6| Step: 2
Training loss: 1.4451055526733398
Validation loss: 2.1725348118812806

Epoch: 6| Step: 3
Training loss: 1.834259271621704
Validation loss: 2.151765069653911

Epoch: 6| Step: 4
Training loss: 2.4122862815856934
Validation loss: 2.153685538999496

Epoch: 6| Step: 5
Training loss: 1.0353763103485107
Validation loss: 2.1541455279114428

Epoch: 6| Step: 6
Training loss: 2.294605255126953
Validation loss: 2.1493194872333157

Epoch: 6| Step: 7
Training loss: 2.095559597015381
Validation loss: 2.155491582808956

Epoch: 6| Step: 8
Training loss: 2.2748775482177734
Validation loss: 2.1266286398774836

Epoch: 6| Step: 9
Training loss: 2.180161476135254
Validation loss: 2.1175467019440024

Epoch: 6| Step: 10
Training loss: 2.0824241638183594
Validation loss: 2.1219776317637455

Epoch: 6| Step: 11
Training loss: 2.579108476638794
Validation loss: 2.1393871743191957

Epoch: 6| Step: 12
Training loss: 1.7744293212890625
Validation loss: 2.1431179995177896

Epoch: 6| Step: 13
Training loss: 2.190974473953247
Validation loss: 2.153323294014059

Epoch: 388| Step: 0
Training loss: 2.510704517364502
Validation loss: 2.163599001464023

Epoch: 6| Step: 1
Training loss: 1.6323814392089844
Validation loss: 2.190179101882442

Epoch: 6| Step: 2
Training loss: 2.4277267456054688
Validation loss: 2.203385588943317

Epoch: 6| Step: 3
Training loss: 1.834019422531128
Validation loss: 2.189005095471618

Epoch: 6| Step: 4
Training loss: 2.1709518432617188
Validation loss: 2.1931747941560644

Epoch: 6| Step: 5
Training loss: 2.359858989715576
Validation loss: 2.2047659722707604

Epoch: 6| Step: 6
Training loss: 1.9425561428070068
Validation loss: 2.184703747431437

Epoch: 6| Step: 7
Training loss: 1.6759734153747559
Validation loss: 2.1844481934783277

Epoch: 6| Step: 8
Training loss: 1.7600398063659668
Validation loss: 2.1875784012579147

Epoch: 6| Step: 9
Training loss: 1.9869329929351807
Validation loss: 2.154231313736208

Epoch: 6| Step: 10
Training loss: 1.4656858444213867
Validation loss: 2.1259758869806924

Epoch: 6| Step: 11
Training loss: 1.988191843032837
Validation loss: 2.1177024866945002

Epoch: 6| Step: 12
Training loss: 2.652343273162842
Validation loss: 2.1159460595858994

Epoch: 6| Step: 13
Training loss: 2.6916720867156982
Validation loss: 2.119645985223914

Epoch: 389| Step: 0
Training loss: 2.322618007659912
Validation loss: 2.106275273907569

Epoch: 6| Step: 1
Training loss: 2.172055959701538
Validation loss: 2.108204128921673

Epoch: 6| Step: 2
Training loss: 1.8394893407821655
Validation loss: 2.09848302795041

Epoch: 6| Step: 3
Training loss: 1.5865991115570068
Validation loss: 2.1108716892939743

Epoch: 6| Step: 4
Training loss: 2.1934750080108643
Validation loss: 2.112870177915019

Epoch: 6| Step: 5
Training loss: 2.3100829124450684
Validation loss: 2.1196381379199285

Epoch: 6| Step: 6
Training loss: 1.7369054555892944
Validation loss: 2.120231047753365

Epoch: 6| Step: 7
Training loss: 2.3524467945098877
Validation loss: 2.106325667391541

Epoch: 6| Step: 8
Training loss: 1.8353813886642456
Validation loss: 2.1292079430754467

Epoch: 6| Step: 9
Training loss: 1.7092392444610596
Validation loss: 2.1301018948196084

Epoch: 6| Step: 10
Training loss: 2.2419381141662598
Validation loss: 2.135149566076135

Epoch: 6| Step: 11
Training loss: 1.756809115409851
Validation loss: 2.139304088008019

Epoch: 6| Step: 12
Training loss: 2.7189958095550537
Validation loss: 2.1689698696136475

Epoch: 6| Step: 13
Training loss: 2.0798959732055664
Validation loss: 2.1865041153405302

Epoch: 390| Step: 0
Training loss: 2.4584481716156006
Validation loss: 2.2003641436176915

Epoch: 6| Step: 1
Training loss: 1.648813009262085
Validation loss: 2.1947550542892946

Epoch: 6| Step: 2
Training loss: 1.819097876548767
Validation loss: 2.205884382288943

Epoch: 6| Step: 3
Training loss: 1.5072917938232422
Validation loss: 2.198731169905714

Epoch: 6| Step: 4
Training loss: 2.1473569869995117
Validation loss: 2.22401653182122

Epoch: 6| Step: 5
Training loss: 2.00663161277771
Validation loss: 2.177264472489716

Epoch: 6| Step: 6
Training loss: 1.3390088081359863
Validation loss: 2.1602499613197903

Epoch: 6| Step: 7
Training loss: 2.6356921195983887
Validation loss: 2.143564802344127

Epoch: 6| Step: 8
Training loss: 1.892488718032837
Validation loss: 2.1363894272876043

Epoch: 6| Step: 9
Training loss: 1.9979969263076782
Validation loss: 2.1329444275107434

Epoch: 6| Step: 10
Training loss: 2.34631609916687
Validation loss: 2.1284080448971

Epoch: 6| Step: 11
Training loss: 2.0753166675567627
Validation loss: 2.1322261774411766

Epoch: 6| Step: 12
Training loss: 2.6573472023010254
Validation loss: 2.1401383902436946

Epoch: 6| Step: 13
Training loss: 2.7221806049346924
Validation loss: 2.1395985618714364

Epoch: 391| Step: 0
Training loss: 2.189368724822998
Validation loss: 2.144319188210272

Epoch: 6| Step: 1
Training loss: 1.7312138080596924
Validation loss: 2.1526073332755797

Epoch: 6| Step: 2
Training loss: 1.6902689933776855
Validation loss: 2.16656659495446

Epoch: 6| Step: 3
Training loss: 1.768211841583252
Validation loss: 2.1693917397529847

Epoch: 6| Step: 4
Training loss: 2.05184268951416
Validation loss: 2.1966750698704876

Epoch: 6| Step: 5
Training loss: 1.9317421913146973
Validation loss: 2.195478480349305

Epoch: 6| Step: 6
Training loss: 2.583315372467041
Validation loss: 2.2298459237621677

Epoch: 6| Step: 7
Training loss: 1.6257121562957764
Validation loss: 2.2373288857039584

Epoch: 6| Step: 8
Training loss: 2.5461695194244385
Validation loss: 2.2530403034661406

Epoch: 6| Step: 9
Training loss: 2.251782178878784
Validation loss: 2.2602592616952877

Epoch: 6| Step: 10
Training loss: 1.4527617692947388
Validation loss: 2.1880758090685775

Epoch: 6| Step: 11
Training loss: 1.997754454612732
Validation loss: 2.138326744879446

Epoch: 6| Step: 12
Training loss: 2.4373486042022705
Validation loss: 2.1155540520145046

Epoch: 6| Step: 13
Training loss: 2.8841066360473633
Validation loss: 2.1296082619697816

Epoch: 392| Step: 0
Training loss: 1.8765538930892944
Validation loss: 2.1060791912899224

Epoch: 6| Step: 1
Training loss: 2.661334991455078
Validation loss: 2.1001869145260064

Epoch: 6| Step: 2
Training loss: 2.643946647644043
Validation loss: 2.102955056775001

Epoch: 6| Step: 3
Training loss: 2.1358284950256348
Validation loss: 2.085010390127859

Epoch: 6| Step: 4
Training loss: 1.9377574920654297
Validation loss: 2.099143126959442

Epoch: 6| Step: 5
Training loss: 2.0879688262939453
Validation loss: 2.0970696403134252

Epoch: 6| Step: 6
Training loss: 1.9740458726882935
Validation loss: 2.098401044004707

Epoch: 6| Step: 7
Training loss: 1.9599913358688354
Validation loss: 2.0999516940885976

Epoch: 6| Step: 8
Training loss: 1.9733920097351074
Validation loss: 2.121928484209122

Epoch: 6| Step: 9
Training loss: 2.2020835876464844
Validation loss: 2.1319563824643373

Epoch: 6| Step: 10
Training loss: 1.977723240852356
Validation loss: 2.1487377792276363

Epoch: 6| Step: 11
Training loss: 1.697849154472351
Validation loss: 2.1850510361374065

Epoch: 6| Step: 12
Training loss: 2.007796287536621
Validation loss: 2.2056527291574786

Epoch: 6| Step: 13
Training loss: 2.0857701301574707
Validation loss: 2.21581631834789

Epoch: 393| Step: 0
Training loss: 2.3971736431121826
Validation loss: 2.2399309630035074

Epoch: 6| Step: 1
Training loss: 1.8690543174743652
Validation loss: 2.2195948041895384

Epoch: 6| Step: 2
Training loss: 1.6965410709381104
Validation loss: 2.2178276559358

Epoch: 6| Step: 3
Training loss: 2.394123077392578
Validation loss: 2.171038099514541

Epoch: 6| Step: 4
Training loss: 1.6892216205596924
Validation loss: 2.1685526755548294

Epoch: 6| Step: 5
Training loss: 2.3842577934265137
Validation loss: 2.1313413625122397

Epoch: 6| Step: 6
Training loss: 1.86074697971344
Validation loss: 2.142400214748998

Epoch: 6| Step: 7
Training loss: 2.018444061279297
Validation loss: 2.1279001261598323

Epoch: 6| Step: 8
Training loss: 2.0789432525634766
Validation loss: 2.137310738204628

Epoch: 6| Step: 9
Training loss: 2.3456649780273438
Validation loss: 2.13358965996773

Epoch: 6| Step: 10
Training loss: 2.3125662803649902
Validation loss: 2.122233513862856

Epoch: 6| Step: 11
Training loss: 1.9845823049545288
Validation loss: 2.1422687038298576

Epoch: 6| Step: 12
Training loss: 2.4512972831726074
Validation loss: 2.155682599672707

Epoch: 6| Step: 13
Training loss: 1.0198633670806885
Validation loss: 2.171074113538188

Epoch: 394| Step: 0
Training loss: 1.8649414777755737
Validation loss: 2.1915686143341886

Epoch: 6| Step: 1
Training loss: 1.3692175149917603
Validation loss: 2.187047517427834

Epoch: 6| Step: 2
Training loss: 1.8473780155181885
Validation loss: 2.173151554599885

Epoch: 6| Step: 3
Training loss: 2.80434250831604
Validation loss: 2.2016031972823606

Epoch: 6| Step: 4
Training loss: 1.896228313446045
Validation loss: 2.1829612639642533

Epoch: 6| Step: 5
Training loss: 2.208899974822998
Validation loss: 2.1871656679338023

Epoch: 6| Step: 6
Training loss: 1.6722780466079712
Validation loss: 2.1423760242359613

Epoch: 6| Step: 7
Training loss: 1.4073152542114258
Validation loss: 2.169167269942581

Epoch: 6| Step: 8
Training loss: 1.3653864860534668
Validation loss: 2.166956332422072

Epoch: 6| Step: 9
Training loss: 2.43632173538208
Validation loss: 2.1734695434570312

Epoch: 6| Step: 10
Training loss: 2.6120781898498535
Validation loss: 2.1620540375350625

Epoch: 6| Step: 11
Training loss: 2.6956334114074707
Validation loss: 2.145879819828977

Epoch: 6| Step: 12
Training loss: 2.2668910026550293
Validation loss: 2.139991106525544

Epoch: 6| Step: 13
Training loss: 2.275803565979004
Validation loss: 2.14564916651736

Epoch: 395| Step: 0
Training loss: 2.4914519786834717
Validation loss: 2.1387560829039542

Epoch: 6| Step: 1
Training loss: 2.0612645149230957
Validation loss: 2.126375829019854

Epoch: 6| Step: 2
Training loss: 2.186567783355713
Validation loss: 2.1226365002252723

Epoch: 6| Step: 3
Training loss: 2.0824413299560547
Validation loss: 2.124115128670969

Epoch: 6| Step: 4
Training loss: 1.5494532585144043
Validation loss: 2.1423998699393323

Epoch: 6| Step: 5
Training loss: 1.4953079223632812
Validation loss: 2.163395171524376

Epoch: 6| Step: 6
Training loss: 2.0162343978881836
Validation loss: 2.161620875840546

Epoch: 6| Step: 7
Training loss: 2.3568923473358154
Validation loss: 2.1941015130730084

Epoch: 6| Step: 8
Training loss: 2.1854405403137207
Validation loss: 2.1950021225919008

Epoch: 6| Step: 9
Training loss: 1.7352255582809448
Validation loss: 2.196812028525978

Epoch: 6| Step: 10
Training loss: 2.74954891204834
Validation loss: 2.193636317406931

Epoch: 6| Step: 11
Training loss: 2.2363555431365967
Validation loss: 2.1764669751608245

Epoch: 6| Step: 12
Training loss: 1.6247237920761108
Validation loss: 2.1802241750942764

Epoch: 6| Step: 13
Training loss: 1.587329626083374
Validation loss: 2.1557141914162585

Epoch: 396| Step: 0
Training loss: 2.3300819396972656
Validation loss: 2.1448049506833478

Epoch: 6| Step: 1
Training loss: 2.338672637939453
Validation loss: 2.153417874408025

Epoch: 6| Step: 2
Training loss: 1.6899170875549316
Validation loss: 2.1493854061249764

Epoch: 6| Step: 3
Training loss: 2.1861863136291504
Validation loss: 2.1651902903792677

Epoch: 6| Step: 4
Training loss: 2.417698860168457
Validation loss: 2.1718158183559293

Epoch: 6| Step: 5
Training loss: 1.7570977210998535
Validation loss: 2.167258252379715

Epoch: 6| Step: 6
Training loss: 2.1480395793914795
Validation loss: 2.1751013007215274

Epoch: 6| Step: 7
Training loss: 1.6452090740203857
Validation loss: 2.1602336411835044

Epoch: 6| Step: 8
Training loss: 2.489971160888672
Validation loss: 2.1878934855102212

Epoch: 6| Step: 9
Training loss: 2.3280107975006104
Validation loss: 2.151177731893396

Epoch: 6| Step: 10
Training loss: 1.8422071933746338
Validation loss: 2.1612179407509426

Epoch: 6| Step: 11
Training loss: 1.63447904586792
Validation loss: 2.1705063081556752

Epoch: 6| Step: 12
Training loss: 2.0186703205108643
Validation loss: 2.157355549514935

Epoch: 6| Step: 13
Training loss: 0.9144589900970459
Validation loss: 2.1545954481247933

Epoch: 397| Step: 0
Training loss: 2.047135591506958
Validation loss: 2.169739874460364

Epoch: 6| Step: 1
Training loss: 2.2825119495391846
Validation loss: 2.147831542517549

Epoch: 6| Step: 2
Training loss: 1.5029628276824951
Validation loss: 2.1663859839080484

Epoch: 6| Step: 3
Training loss: 1.1354727745056152
Validation loss: 2.1613490914785736

Epoch: 6| Step: 4
Training loss: 1.634628415107727
Validation loss: 2.182928467309603

Epoch: 6| Step: 5
Training loss: 2.596579074859619
Validation loss: 2.1780284630355013

Epoch: 6| Step: 6
Training loss: 2.01174259185791
Validation loss: 2.1811119215462798

Epoch: 6| Step: 7
Training loss: 2.5231122970581055
Validation loss: 2.1665495211078274

Epoch: 6| Step: 8
Training loss: 2.203591823577881
Validation loss: 2.1444239513848418

Epoch: 6| Step: 9
Training loss: 1.1563650369644165
Validation loss: 2.1568287598189486

Epoch: 6| Step: 10
Training loss: 2.3606910705566406
Validation loss: 2.144576302138708

Epoch: 6| Step: 11
Training loss: 2.471402883529663
Validation loss: 2.1340352104556177

Epoch: 6| Step: 12
Training loss: 2.3746602535247803
Validation loss: 2.132490857954948

Epoch: 6| Step: 13
Training loss: 2.2049708366394043
Validation loss: 2.117313451664422

Epoch: 398| Step: 0
Training loss: 1.2073900699615479
Validation loss: 2.124857225725728

Epoch: 6| Step: 1
Training loss: 1.7449617385864258
Validation loss: 2.1392264263604277

Epoch: 6| Step: 2
Training loss: 2.6384031772613525
Validation loss: 2.1240944298364783

Epoch: 6| Step: 3
Training loss: 2.012981414794922
Validation loss: 2.1076491212332122

Epoch: 6| Step: 4
Training loss: 2.266493558883667
Validation loss: 2.124858799801078

Epoch: 6| Step: 5
Training loss: 2.400686025619507
Validation loss: 2.1338836992940595

Epoch: 6| Step: 6
Training loss: 1.9464311599731445
Validation loss: 2.1540699056399766

Epoch: 6| Step: 7
Training loss: 1.574389100074768
Validation loss: 2.1554309527079263

Epoch: 6| Step: 8
Training loss: 2.071735143661499
Validation loss: 2.152670601362823

Epoch: 6| Step: 9
Training loss: 1.375583291053772
Validation loss: 2.1219140304032194

Epoch: 6| Step: 10
Training loss: 2.2181107997894287
Validation loss: 2.1176124567626626

Epoch: 6| Step: 11
Training loss: 2.3365039825439453
Validation loss: 2.1411231051209154

Epoch: 6| Step: 12
Training loss: 2.4761552810668945
Validation loss: 2.126479738502092

Epoch: 6| Step: 13
Training loss: 2.0751028060913086
Validation loss: 2.133060421994937

Epoch: 399| Step: 0
Training loss: 1.9689831733703613
Validation loss: 2.157832448200513

Epoch: 6| Step: 1
Training loss: 1.6485137939453125
Validation loss: 2.181181743580808

Epoch: 6| Step: 2
Training loss: 2.2336745262145996
Validation loss: 2.1790896308037544

Epoch: 6| Step: 3
Training loss: 1.7974369525909424
Validation loss: 2.1951991101746917

Epoch: 6| Step: 4
Training loss: 2.215188503265381
Validation loss: 2.1583044669961415

Epoch: 6| Step: 5
Training loss: 2.302576780319214
Validation loss: 2.192276982850926

Epoch: 6| Step: 6
Training loss: 2.517906904220581
Validation loss: 2.183041944298693

Epoch: 6| Step: 7
Training loss: 1.9892958402633667
Validation loss: 2.188736090096094

Epoch: 6| Step: 8
Training loss: 2.2441368103027344
Validation loss: 2.1719770585336993

Epoch: 6| Step: 9
Training loss: 1.6214014291763306
Validation loss: 2.1505376267176803

Epoch: 6| Step: 10
Training loss: 2.5790112018585205
Validation loss: 2.12997918616059

Epoch: 6| Step: 11
Training loss: 1.4076764583587646
Validation loss: 2.1181021826241606

Epoch: 6| Step: 12
Training loss: 2.0229763984680176
Validation loss: 2.1084669174686557

Epoch: 6| Step: 13
Training loss: 1.3216958045959473
Validation loss: 2.089709863867811

Epoch: 400| Step: 0
Training loss: 1.7938987016677856
Validation loss: 2.096913565871536

Epoch: 6| Step: 1
Training loss: 2.0121192932128906
Validation loss: 2.0772452354431152

Epoch: 6| Step: 2
Training loss: 2.8952176570892334
Validation loss: 2.0958287305729364

Epoch: 6| Step: 3
Training loss: 1.9561784267425537
Validation loss: 2.0950593820182224

Epoch: 6| Step: 4
Training loss: 2.1204833984375
Validation loss: 2.1049124976640106

Epoch: 6| Step: 5
Training loss: 1.8554936647415161
Validation loss: 2.134105807991438

Epoch: 6| Step: 6
Training loss: 2.082247734069824
Validation loss: 2.159764112964753

Epoch: 6| Step: 7
Training loss: 1.7040873765945435
Validation loss: 2.193312309121573

Epoch: 6| Step: 8
Training loss: 1.543559193611145
Validation loss: 2.1959361824938046

Epoch: 6| Step: 9
Training loss: 1.787601351737976
Validation loss: 2.233704120882096

Epoch: 6| Step: 10
Training loss: 2.2003493309020996
Validation loss: 2.2481738418661137

Epoch: 6| Step: 11
Training loss: 2.43098521232605
Validation loss: 2.2457819241349415

Epoch: 6| Step: 12
Training loss: 2.4773592948913574
Validation loss: 2.246060036843823

Epoch: 6| Step: 13
Training loss: 1.055983066558838
Validation loss: 2.2076480978278705

Epoch: 401| Step: 0
Training loss: 1.6268024444580078
Validation loss: 2.2044994061993015

Epoch: 6| Step: 1
Training loss: 2.335824966430664
Validation loss: 2.1569975601729525

Epoch: 6| Step: 2
Training loss: 1.3681261539459229
Validation loss: 2.142747914919289

Epoch: 6| Step: 3
Training loss: 2.14184832572937
Validation loss: 2.1106919370671755

Epoch: 6| Step: 4
Training loss: 2.6996827125549316
Validation loss: 2.1136563183158956

Epoch: 6| Step: 5
Training loss: 1.4610474109649658
Validation loss: 2.1001252987051524

Epoch: 6| Step: 6
Training loss: 1.739033818244934
Validation loss: 2.105837034922774

Epoch: 6| Step: 7
Training loss: 2.1609740257263184
Validation loss: 2.104170762082582

Epoch: 6| Step: 8
Training loss: 2.3843817710876465
Validation loss: 2.1070157251050396

Epoch: 6| Step: 9
Training loss: 1.9116075038909912
Validation loss: 2.1212432948491906

Epoch: 6| Step: 10
Training loss: 2.509368658065796
Validation loss: 2.1348000495664534

Epoch: 6| Step: 11
Training loss: 1.7362253665924072
Validation loss: 2.133013350989229

Epoch: 6| Step: 12
Training loss: 1.8878843784332275
Validation loss: 2.1314312091437717

Epoch: 6| Step: 13
Training loss: 2.5251290798187256
Validation loss: 2.149148675703233

Epoch: 402| Step: 0
Training loss: 1.8803153038024902
Validation loss: 2.141661718327512

Epoch: 6| Step: 1
Training loss: 1.6186821460723877
Validation loss: 2.146824303493705

Epoch: 6| Step: 2
Training loss: 1.990321159362793
Validation loss: 2.124598074984807

Epoch: 6| Step: 3
Training loss: 1.462036371231079
Validation loss: 2.12803074108657

Epoch: 6| Step: 4
Training loss: 1.5461047887802124
Validation loss: 2.1343883955350487

Epoch: 6| Step: 5
Training loss: 2.3854477405548096
Validation loss: 2.139916394346504

Epoch: 6| Step: 6
Training loss: 2.202908992767334
Validation loss: 2.1551331935390348

Epoch: 6| Step: 7
Training loss: 2.246589183807373
Validation loss: 2.137248913447062

Epoch: 6| Step: 8
Training loss: 1.554292917251587
Validation loss: 2.1429193519776866

Epoch: 6| Step: 9
Training loss: 1.9275012016296387
Validation loss: 2.149958518243605

Epoch: 6| Step: 10
Training loss: 2.407609462738037
Validation loss: 2.1689349015553794

Epoch: 6| Step: 11
Training loss: 2.2231805324554443
Validation loss: 2.1766500729386524

Epoch: 6| Step: 12
Training loss: 2.514732837677002
Validation loss: 2.168852421545213

Epoch: 6| Step: 13
Training loss: 1.9792948961257935
Validation loss: 2.1514637483063566

Epoch: 403| Step: 0
Training loss: 2.0671770572662354
Validation loss: 2.142674635815364

Epoch: 6| Step: 1
Training loss: 2.009272813796997
Validation loss: 2.12524252681322

Epoch: 6| Step: 2
Training loss: 1.4106048345565796
Validation loss: 2.1193729715962566

Epoch: 6| Step: 3
Training loss: 2.059386968612671
Validation loss: 2.1299784696230324

Epoch: 6| Step: 4
Training loss: 2.113079071044922
Validation loss: 2.118201360907606

Epoch: 6| Step: 5
Training loss: 2.0178139209747314
Validation loss: 2.1174873280268844

Epoch: 6| Step: 6
Training loss: 2.144902229309082
Validation loss: 2.1281733871788107

Epoch: 6| Step: 7
Training loss: 2.208037853240967
Validation loss: 2.1422629535839124

Epoch: 6| Step: 8
Training loss: 2.388064384460449
Validation loss: 2.1363615579502557

Epoch: 6| Step: 9
Training loss: 1.7711875438690186
Validation loss: 2.1538512924666047

Epoch: 6| Step: 10
Training loss: 2.6469650268554688
Validation loss: 2.130509368834957

Epoch: 6| Step: 11
Training loss: 1.3383665084838867
Validation loss: 2.1204575415580504

Epoch: 6| Step: 12
Training loss: 2.0135653018951416
Validation loss: 2.1063290642153834

Epoch: 6| Step: 13
Training loss: 1.711772084236145
Validation loss: 2.1202502840308735

Epoch: 404| Step: 0
Training loss: 1.7044957876205444
Validation loss: 2.1224080439536803

Epoch: 6| Step: 1
Training loss: 1.5343194007873535
Validation loss: 2.128904806670322

Epoch: 6| Step: 2
Training loss: 2.3121209144592285
Validation loss: 2.1110250770404773

Epoch: 6| Step: 3
Training loss: 2.5295214653015137
Validation loss: 2.1030970414479575

Epoch: 6| Step: 4
Training loss: 1.8397703170776367
Validation loss: 2.11283443820092

Epoch: 6| Step: 5
Training loss: 2.219569206237793
Validation loss: 2.1266691492449854

Epoch: 6| Step: 6
Training loss: 1.5555733442306519
Validation loss: 2.1135632453426236

Epoch: 6| Step: 7
Training loss: 1.730298638343811
Validation loss: 2.1095164027265323

Epoch: 6| Step: 8
Training loss: 1.9749727249145508
Validation loss: 2.125229266382033

Epoch: 6| Step: 9
Training loss: 2.0088396072387695
Validation loss: 2.1240866222689228

Epoch: 6| Step: 10
Training loss: 2.438917636871338
Validation loss: 2.1306990282509917

Epoch: 6| Step: 11
Training loss: 2.258281707763672
Validation loss: 2.1059257215069187

Epoch: 6| Step: 12
Training loss: 1.7902326583862305
Validation loss: 2.127406330518825

Epoch: 6| Step: 13
Training loss: 2.2032859325408936
Validation loss: 2.1486639361227713

Epoch: 405| Step: 0
Training loss: 2.266183376312256
Validation loss: 2.1438104388534382

Epoch: 6| Step: 1
Training loss: 2.47861385345459
Validation loss: 2.147431678669427

Epoch: 6| Step: 2
Training loss: 1.7683104276657104
Validation loss: 2.149539778309484

Epoch: 6| Step: 3
Training loss: 2.413857936859131
Validation loss: 2.1560337851124425

Epoch: 6| Step: 4
Training loss: 1.978447675704956
Validation loss: 2.1279121419434905

Epoch: 6| Step: 5
Training loss: 1.9557762145996094
Validation loss: 2.123854708927934

Epoch: 6| Step: 6
Training loss: 2.3685455322265625
Validation loss: 2.13763835096872

Epoch: 6| Step: 7
Training loss: 1.4402610063552856
Validation loss: 2.1356000746450117

Epoch: 6| Step: 8
Training loss: 2.0849711894989014
Validation loss: 2.122973095986151

Epoch: 6| Step: 9
Training loss: 1.620509386062622
Validation loss: 2.100182448664019

Epoch: 6| Step: 10
Training loss: 2.8306522369384766
Validation loss: 2.09395848038376

Epoch: 6| Step: 11
Training loss: 1.9476193189620972
Validation loss: 2.1060128775976037

Epoch: 6| Step: 12
Training loss: 1.3501116037368774
Validation loss: 2.1015977705678632

Epoch: 6| Step: 13
Training loss: 1.3139904737472534
Validation loss: 2.110031963676535

Epoch: 406| Step: 0
Training loss: 2.384054660797119
Validation loss: 2.106118885419702

Epoch: 6| Step: 1
Training loss: 2.5662806034088135
Validation loss: 2.1273686373105614

Epoch: 6| Step: 2
Training loss: 2.32555890083313
Validation loss: 2.142322183937155

Epoch: 6| Step: 3
Training loss: 2.098479747772217
Validation loss: 2.1541423695061797

Epoch: 6| Step: 4
Training loss: 2.0910651683807373
Validation loss: 2.1712080919614403

Epoch: 6| Step: 5
Training loss: 2.0023138523101807
Validation loss: 2.1596790449593657

Epoch: 6| Step: 6
Training loss: 1.6739475727081299
Validation loss: 2.162205239777924

Epoch: 6| Step: 7
Training loss: 2.1169960498809814
Validation loss: 2.1608326973453647

Epoch: 6| Step: 8
Training loss: 1.6934674978256226
Validation loss: 2.167757826466714

Epoch: 6| Step: 9
Training loss: 1.9912352561950684
Validation loss: 2.149919584233274

Epoch: 6| Step: 10
Training loss: 1.8168911933898926
Validation loss: 2.143378141105816

Epoch: 6| Step: 11
Training loss: 1.5722452402114868
Validation loss: 2.130176600589547

Epoch: 6| Step: 12
Training loss: 1.9064613580703735
Validation loss: 2.112052984135125

Epoch: 6| Step: 13
Training loss: 1.5834459066390991
Validation loss: 2.1154304165993967

Epoch: 407| Step: 0
Training loss: 1.665567398071289
Validation loss: 2.104872013932915

Epoch: 6| Step: 1
Training loss: 1.7432355880737305
Validation loss: 2.0982857596489692

Epoch: 6| Step: 2
Training loss: 2.1161177158355713
Validation loss: 2.1285319418035527

Epoch: 6| Step: 3
Training loss: 1.785355567932129
Validation loss: 2.1306981450767926

Epoch: 6| Step: 4
Training loss: 1.6706281900405884
Validation loss: 2.1525663547618414

Epoch: 6| Step: 5
Training loss: 2.277360200881958
Validation loss: 2.1515659991131035

Epoch: 6| Step: 6
Training loss: 1.8610689640045166
Validation loss: 2.146421368404101

Epoch: 6| Step: 7
Training loss: 2.3113651275634766
Validation loss: 2.135443497729558

Epoch: 6| Step: 8
Training loss: 1.5529680252075195
Validation loss: 2.1539630428437264

Epoch: 6| Step: 9
Training loss: 2.154038906097412
Validation loss: 2.137037742522455

Epoch: 6| Step: 10
Training loss: 2.0746469497680664
Validation loss: 2.1068103236536824

Epoch: 6| Step: 11
Training loss: 1.4607011079788208
Validation loss: 2.120751369384027

Epoch: 6| Step: 12
Training loss: 2.8929731845855713
Validation loss: 2.103865401719206

Epoch: 6| Step: 13
Training loss: 2.4154350757598877
Validation loss: 2.101668211721605

Epoch: 408| Step: 0
Training loss: 2.0087194442749023
Validation loss: 2.1033395259611067

Epoch: 6| Step: 1
Training loss: 1.4086555242538452
Validation loss: 2.0837001659536876

Epoch: 6| Step: 2
Training loss: 2.0234460830688477
Validation loss: 2.0916161075715096

Epoch: 6| Step: 3
Training loss: 1.7958347797393799
Validation loss: 2.0934259558236725

Epoch: 6| Step: 4
Training loss: 2.1789684295654297
Validation loss: 2.0910904958683956

Epoch: 6| Step: 5
Training loss: 2.041752338409424
Validation loss: 2.098418334478973

Epoch: 6| Step: 6
Training loss: 1.7887725830078125
Validation loss: 2.1207487929251885

Epoch: 6| Step: 7
Training loss: 1.812484860420227
Validation loss: 2.1211730946776686

Epoch: 6| Step: 8
Training loss: 2.159409999847412
Validation loss: 2.1451896288061656

Epoch: 6| Step: 9
Training loss: 2.184694290161133
Validation loss: 2.140341451091151

Epoch: 6| Step: 10
Training loss: 2.531424045562744
Validation loss: 2.164688270579102

Epoch: 6| Step: 11
Training loss: 2.2705862522125244
Validation loss: 2.1428212055595974

Epoch: 6| Step: 12
Training loss: 1.894191026687622
Validation loss: 2.159861949182326

Epoch: 6| Step: 13
Training loss: 1.6528446674346924
Validation loss: 2.1625053472416376

Epoch: 409| Step: 0
Training loss: 2.11686372756958
Validation loss: 2.1737471549741683

Epoch: 6| Step: 1
Training loss: 1.9123780727386475
Validation loss: 2.1724439487662366

Epoch: 6| Step: 2
Training loss: 2.763322353363037
Validation loss: 2.1652542493676625

Epoch: 6| Step: 3
Training loss: 2.013901710510254
Validation loss: 2.1595526203032462

Epoch: 6| Step: 4
Training loss: 2.2846240997314453
Validation loss: 2.165344020371796

Epoch: 6| Step: 5
Training loss: 2.331071376800537
Validation loss: 2.1632708516172183

Epoch: 6| Step: 6
Training loss: 1.899208664894104
Validation loss: 2.180910310437602

Epoch: 6| Step: 7
Training loss: 1.6086039543151855
Validation loss: 2.1828348636627197

Epoch: 6| Step: 8
Training loss: 2.438678741455078
Validation loss: 2.1761548493498113

Epoch: 6| Step: 9
Training loss: 1.7120085954666138
Validation loss: 2.1810019605903217

Epoch: 6| Step: 10
Training loss: 2.0754811763763428
Validation loss: 2.1487688005611463

Epoch: 6| Step: 11
Training loss: 1.7120614051818848
Validation loss: 2.152659708453763

Epoch: 6| Step: 12
Training loss: 1.8782808780670166
Validation loss: 2.1389406656706207

Epoch: 6| Step: 13
Training loss: 0.7375061511993408
Validation loss: 2.1179054347417687

Epoch: 410| Step: 0
Training loss: 2.3803598880767822
Validation loss: 2.119878245938209

Epoch: 6| Step: 1
Training loss: 1.582077145576477
Validation loss: 2.119203943078236

Epoch: 6| Step: 2
Training loss: 1.8702319860458374
Validation loss: 2.0938108275013585

Epoch: 6| Step: 3
Training loss: 2.0751893520355225
Validation loss: 2.1436156739470777

Epoch: 6| Step: 4
Training loss: 1.9238121509552002
Validation loss: 2.092747679320715

Epoch: 6| Step: 5
Training loss: 1.662519931793213
Validation loss: 2.094199054984636

Epoch: 6| Step: 6
Training loss: 2.533999443054199
Validation loss: 2.1222922186697684

Epoch: 6| Step: 7
Training loss: 1.3574138879776
Validation loss: 2.1182004264605943

Epoch: 6| Step: 8
Training loss: 2.516645908355713
Validation loss: 2.119011814876269

Epoch: 6| Step: 9
Training loss: 2.590327739715576
Validation loss: 2.1261546124694166

Epoch: 6| Step: 10
Training loss: 1.8401036262512207
Validation loss: 2.130405088906647

Epoch: 6| Step: 11
Training loss: 1.9513511657714844
Validation loss: 2.143098779903945

Epoch: 6| Step: 12
Training loss: 1.9493834972381592
Validation loss: 2.128272528289467

Epoch: 6| Step: 13
Training loss: 1.188504695892334
Validation loss: 2.1233248633723103

Epoch: 411| Step: 0
Training loss: 2.1772825717926025
Validation loss: 2.1344913897975797

Epoch: 6| Step: 1
Training loss: 1.0169883966445923
Validation loss: 2.1108509263684674

Epoch: 6| Step: 2
Training loss: 2.145663261413574
Validation loss: 2.1038117024206344

Epoch: 6| Step: 3
Training loss: 2.013282299041748
Validation loss: 2.0775656879589124

Epoch: 6| Step: 4
Training loss: 1.856701135635376
Validation loss: 2.1052576598300727

Epoch: 6| Step: 5
Training loss: 2.4543776512145996
Validation loss: 2.095305765828779

Epoch: 6| Step: 6
Training loss: 2.406745433807373
Validation loss: 2.1075464051256896

Epoch: 6| Step: 7
Training loss: 1.9081721305847168
Validation loss: 2.112483180979247

Epoch: 6| Step: 8
Training loss: 2.518242359161377
Validation loss: 2.109724098636258

Epoch: 6| Step: 9
Training loss: 2.252141237258911
Validation loss: 2.1120370421358334

Epoch: 6| Step: 10
Training loss: 1.086240291595459
Validation loss: 2.1178076113423994

Epoch: 6| Step: 11
Training loss: 1.9927990436553955
Validation loss: 2.1313911804588894

Epoch: 6| Step: 12
Training loss: 1.8165944814682007
Validation loss: 2.1361440817515054

Epoch: 6| Step: 13
Training loss: 2.175645112991333
Validation loss: 2.11160921794112

Epoch: 412| Step: 0
Training loss: 1.501006007194519
Validation loss: 2.117111690582768

Epoch: 6| Step: 1
Training loss: 2.4995851516723633
Validation loss: 2.102794517752945

Epoch: 6| Step: 2
Training loss: 2.1355080604553223
Validation loss: 2.1072264512379966

Epoch: 6| Step: 3
Training loss: 1.4787836074829102
Validation loss: 2.122679420696792

Epoch: 6| Step: 4
Training loss: 2.304379940032959
Validation loss: 2.1200949171537995

Epoch: 6| Step: 5
Training loss: 2.2341837882995605
Validation loss: 2.103735880185199

Epoch: 6| Step: 6
Training loss: 1.7193418741226196
Validation loss: 2.1001243963036487

Epoch: 6| Step: 7
Training loss: 2.0990843772888184
Validation loss: 2.1297543894860054

Epoch: 6| Step: 8
Training loss: 1.6873936653137207
Validation loss: 2.1227040188286894

Epoch: 6| Step: 9
Training loss: 2.02425479888916
Validation loss: 2.1266863628100325

Epoch: 6| Step: 10
Training loss: 1.7284941673278809
Validation loss: 2.1271214151895173

Epoch: 6| Step: 11
Training loss: 2.2174365520477295
Validation loss: 2.130110289460869

Epoch: 6| Step: 12
Training loss: 1.902185320854187
Validation loss: 2.1114350544509066

Epoch: 6| Step: 13
Training loss: 2.0570240020751953
Validation loss: 2.1254176952505626

Epoch: 413| Step: 0
Training loss: 2.7599570751190186
Validation loss: 2.1265462649765836

Epoch: 6| Step: 1
Training loss: 1.7092080116271973
Validation loss: 2.1275656287388136

Epoch: 6| Step: 2
Training loss: 1.7923520803451538
Validation loss: 2.116066631450448

Epoch: 6| Step: 3
Training loss: 1.7281686067581177
Validation loss: 2.150137775687761

Epoch: 6| Step: 4
Training loss: 1.4239860773086548
Validation loss: 2.1375704567919493

Epoch: 6| Step: 5
Training loss: 1.7010154724121094
Validation loss: 2.156331128971551

Epoch: 6| Step: 6
Training loss: 1.8842360973358154
Validation loss: 2.1557225232483237

Epoch: 6| Step: 7
Training loss: 2.5345027446746826
Validation loss: 2.1166205816371466

Epoch: 6| Step: 8
Training loss: 1.6038713455200195
Validation loss: 2.1474414666493735

Epoch: 6| Step: 9
Training loss: 2.017383575439453
Validation loss: 2.1606056485124814

Epoch: 6| Step: 10
Training loss: 1.6713027954101562
Validation loss: 2.172064765807121

Epoch: 6| Step: 11
Training loss: 1.925518274307251
Validation loss: 2.197745648763513

Epoch: 6| Step: 12
Training loss: 2.65126633644104
Validation loss: 2.1849183472253944

Epoch: 6| Step: 13
Training loss: 2.692660331726074
Validation loss: 2.1904298182456725

Epoch: 414| Step: 0
Training loss: 1.8372471332550049
Validation loss: 2.1839090085798696

Epoch: 6| Step: 1
Training loss: 1.5847951173782349
Validation loss: 2.1646717889334566

Epoch: 6| Step: 2
Training loss: 1.886584758758545
Validation loss: 2.1479551715235554

Epoch: 6| Step: 3
Training loss: 2.2853641510009766
Validation loss: 2.1377613083008797

Epoch: 6| Step: 4
Training loss: 2.0550575256347656
Validation loss: 2.1473909321651665

Epoch: 6| Step: 5
Training loss: 2.489151954650879
Validation loss: 2.1328454440639866

Epoch: 6| Step: 6
Training loss: 1.884718894958496
Validation loss: 2.1202569161691973

Epoch: 6| Step: 7
Training loss: 2.231839179992676
Validation loss: 2.128104653409732

Epoch: 6| Step: 8
Training loss: 1.4101791381835938
Validation loss: 2.1512093492733535

Epoch: 6| Step: 9
Training loss: 2.3376500606536865
Validation loss: 2.1160876904764483

Epoch: 6| Step: 10
Training loss: 1.6694728136062622
Validation loss: 2.1356183354572584

Epoch: 6| Step: 11
Training loss: 1.4735870361328125
Validation loss: 2.12996486438218

Epoch: 6| Step: 12
Training loss: 2.730072021484375
Validation loss: 2.149513336919969

Epoch: 6| Step: 13
Training loss: 1.6782493591308594
Validation loss: 2.1223873169191423

Epoch: 415| Step: 0
Training loss: 2.143679141998291
Validation loss: 2.1059195277511433

Epoch: 6| Step: 1
Training loss: 2.053666591644287
Validation loss: 2.125244499534689

Epoch: 6| Step: 2
Training loss: 1.745902419090271
Validation loss: 2.131904773814704

Epoch: 6| Step: 3
Training loss: 1.732574701309204
Validation loss: 2.127169224523729

Epoch: 6| Step: 4
Training loss: 2.285344123840332
Validation loss: 2.103149913972424

Epoch: 6| Step: 5
Training loss: 1.6044561862945557
Validation loss: 2.1081145437814857

Epoch: 6| Step: 6
Training loss: 1.8759877681732178
Validation loss: 2.0932959215615385

Epoch: 6| Step: 7
Training loss: 2.245419979095459
Validation loss: 2.1270642690761115

Epoch: 6| Step: 8
Training loss: 1.6546685695648193
Validation loss: 2.111807930854059

Epoch: 6| Step: 9
Training loss: 2.192011594772339
Validation loss: 2.121016558780465

Epoch: 6| Step: 10
Training loss: 1.6876399517059326
Validation loss: 2.1349728517634894

Epoch: 6| Step: 11
Training loss: 1.706034541130066
Validation loss: 2.1414669739302767

Epoch: 6| Step: 12
Training loss: 2.4968106746673584
Validation loss: 2.14596329709535

Epoch: 6| Step: 13
Training loss: 2.124148368835449
Validation loss: 2.1439772241859028

Epoch: 416| Step: 0
Training loss: 2.0866763591766357
Validation loss: 2.1043440423985964

Epoch: 6| Step: 1
Training loss: 2.3500804901123047
Validation loss: 2.0994930882607736

Epoch: 6| Step: 2
Training loss: 1.5751664638519287
Validation loss: 2.0953807817992343

Epoch: 6| Step: 3
Training loss: 1.4253326654434204
Validation loss: 2.0969003605586227

Epoch: 6| Step: 4
Training loss: 2.4904983043670654
Validation loss: 2.0931296912572717

Epoch: 6| Step: 5
Training loss: 2.012147903442383
Validation loss: 2.093752673877183

Epoch: 6| Step: 6
Training loss: 2.456223964691162
Validation loss: 2.0984221594308012

Epoch: 6| Step: 7
Training loss: 2.160228967666626
Validation loss: 2.095691398907733

Epoch: 6| Step: 8
Training loss: 1.8680121898651123
Validation loss: 2.1091048704680575

Epoch: 6| Step: 9
Training loss: 1.5844050645828247
Validation loss: 2.112541861431573

Epoch: 6| Step: 10
Training loss: 1.6656510829925537
Validation loss: 2.117047340639176

Epoch: 6| Step: 11
Training loss: 2.236544609069824
Validation loss: 2.1369134123607347

Epoch: 6| Step: 12
Training loss: 1.8843960762023926
Validation loss: 2.142052985006763

Epoch: 6| Step: 13
Training loss: 1.5941922664642334
Validation loss: 2.153972311686444

Epoch: 417| Step: 0
Training loss: 2.6593799591064453
Validation loss: 2.183376040509952

Epoch: 6| Step: 1
Training loss: 2.3624038696289062
Validation loss: 2.193529135437422

Epoch: 6| Step: 2
Training loss: 2.1526403427124023
Validation loss: 2.1812658079208864

Epoch: 6| Step: 3
Training loss: 2.22444224357605
Validation loss: 2.1830968626083864

Epoch: 6| Step: 4
Training loss: 1.5094387531280518
Validation loss: 2.1549292943810903

Epoch: 6| Step: 5
Training loss: 2.5909814834594727
Validation loss: 2.1457807146092898

Epoch: 6| Step: 6
Training loss: 2.230419158935547
Validation loss: 2.124745402284848

Epoch: 6| Step: 7
Training loss: 1.4265501499176025
Validation loss: 2.1185904818196453

Epoch: 6| Step: 8
Training loss: 1.184037685394287
Validation loss: 2.1276169387243127

Epoch: 6| Step: 9
Training loss: 1.7311952114105225
Validation loss: 2.110552195579775

Epoch: 6| Step: 10
Training loss: 1.4175026416778564
Validation loss: 2.109153833440555

Epoch: 6| Step: 11
Training loss: 2.155156135559082
Validation loss: 2.1104200963051087

Epoch: 6| Step: 12
Training loss: 1.934748888015747
Validation loss: 2.091484172369844

Epoch: 6| Step: 13
Training loss: 2.127255916595459
Validation loss: 2.0956500627661265

Epoch: 418| Step: 0
Training loss: 1.4575629234313965
Validation loss: 2.1020504172130297

Epoch: 6| Step: 1
Training loss: 1.4731667041778564
Validation loss: 2.124291886565506

Epoch: 6| Step: 2
Training loss: 1.7145183086395264
Validation loss: 2.138567945008637

Epoch: 6| Step: 3
Training loss: 2.3902201652526855
Validation loss: 2.149549397089148

Epoch: 6| Step: 4
Training loss: 2.2664215564727783
Validation loss: 2.1650309972865607

Epoch: 6| Step: 5
Training loss: 2.269232988357544
Validation loss: 2.1488621568167083

Epoch: 6| Step: 6
Training loss: 2.141603946685791
Validation loss: 2.1683043715774373

Epoch: 6| Step: 7
Training loss: 2.3621551990509033
Validation loss: 2.1657355472605717

Epoch: 6| Step: 8
Training loss: 2.2467474937438965
Validation loss: 2.1442797107081257

Epoch: 6| Step: 9
Training loss: 1.6434168815612793
Validation loss: 2.124970031040971

Epoch: 6| Step: 10
Training loss: 2.0004162788391113
Validation loss: 2.1255803954216743

Epoch: 6| Step: 11
Training loss: 1.3994897603988647
Validation loss: 2.0941415217614945

Epoch: 6| Step: 12
Training loss: 1.9190770387649536
Validation loss: 2.089082346167616

Epoch: 6| Step: 13
Training loss: 2.178866386413574
Validation loss: 2.092390247570571

Epoch: 419| Step: 0
Training loss: 2.427316188812256
Validation loss: 2.0815384439242783

Epoch: 6| Step: 1
Training loss: 1.524366855621338
Validation loss: 2.083980594911883

Epoch: 6| Step: 2
Training loss: 1.9539391994476318
Validation loss: 2.0920414745166735

Epoch: 6| Step: 3
Training loss: 2.4703989028930664
Validation loss: 2.0935847118336666

Epoch: 6| Step: 4
Training loss: 2.5815963745117188
Validation loss: 2.1051103594482585

Epoch: 6| Step: 5
Training loss: 1.3846440315246582
Validation loss: 2.10456855066361

Epoch: 6| Step: 6
Training loss: 2.127486228942871
Validation loss: 2.085413322653822

Epoch: 6| Step: 7
Training loss: 1.7729926109313965
Validation loss: 2.1080692173332296

Epoch: 6| Step: 8
Training loss: 2.0302934646606445
Validation loss: 2.134509030208793

Epoch: 6| Step: 9
Training loss: 2.1567773818969727
Validation loss: 2.1306022444079

Epoch: 6| Step: 10
Training loss: 1.377161979675293
Validation loss: 2.163158360347953

Epoch: 6| Step: 11
Training loss: 1.5539655685424805
Validation loss: 2.1749019866348593

Epoch: 6| Step: 12
Training loss: 2.381694793701172
Validation loss: 2.1841210819059804

Epoch: 6| Step: 13
Training loss: 1.4626245498657227
Validation loss: 2.167761584763886

Epoch: 420| Step: 0
Training loss: 1.9324917793273926
Validation loss: 2.1682255627006612

Epoch: 6| Step: 1
Training loss: 2.441507339477539
Validation loss: 2.1608150774432766

Epoch: 6| Step: 2
Training loss: 2.1928441524505615
Validation loss: 2.1168969138976066

Epoch: 6| Step: 3
Training loss: 2.2153801918029785
Validation loss: 2.1139509036976802

Epoch: 6| Step: 4
Training loss: 2.0651135444641113
Validation loss: 2.0908853110446723

Epoch: 6| Step: 5
Training loss: 2.121695041656494
Validation loss: 2.105531354104319

Epoch: 6| Step: 6
Training loss: 1.7492711544036865
Validation loss: 2.0830736057732695

Epoch: 6| Step: 7
Training loss: 1.0536768436431885
Validation loss: 2.083860544748204

Epoch: 6| Step: 8
Training loss: 1.9125711917877197
Validation loss: 2.09430927999558

Epoch: 6| Step: 9
Training loss: 1.9958832263946533
Validation loss: 2.109916703675383

Epoch: 6| Step: 10
Training loss: 1.509484887123108
Validation loss: 2.119563748759608

Epoch: 6| Step: 11
Training loss: 1.975296974182129
Validation loss: 2.131058974932599

Epoch: 6| Step: 12
Training loss: 2.250150680541992
Validation loss: 2.141121184954079

Epoch: 6| Step: 13
Training loss: 1.838215708732605
Validation loss: 2.1319024127016784

Epoch: 421| Step: 0
Training loss: 2.038389205932617
Validation loss: 2.123243075545116

Epoch: 6| Step: 1
Training loss: 1.5679662227630615
Validation loss: 2.1310125038187993

Epoch: 6| Step: 2
Training loss: 1.8074719905853271
Validation loss: 2.1331529489127536

Epoch: 6| Step: 3
Training loss: 1.7591361999511719
Validation loss: 2.140916303921771

Epoch: 6| Step: 4
Training loss: 2.1255006790161133
Validation loss: 2.1319291514735066

Epoch: 6| Step: 5
Training loss: 1.7616324424743652
Validation loss: 2.1188066390252884

Epoch: 6| Step: 6
Training loss: 2.173142433166504
Validation loss: 2.1304519253392376

Epoch: 6| Step: 7
Training loss: 2.20363187789917
Validation loss: 2.1151580336273357

Epoch: 6| Step: 8
Training loss: 1.9406507015228271
Validation loss: 2.1006377614954466

Epoch: 6| Step: 9
Training loss: 3.0386810302734375
Validation loss: 2.0969090128457673

Epoch: 6| Step: 10
Training loss: 1.5537030696868896
Validation loss: 2.091745891878682

Epoch: 6| Step: 11
Training loss: 1.9361741542816162
Validation loss: 2.0745873502505723

Epoch: 6| Step: 12
Training loss: 1.7040588855743408
Validation loss: 2.085711536868926

Epoch: 6| Step: 13
Training loss: 1.5080201625823975
Validation loss: 2.0740429637252644

Epoch: 422| Step: 0
Training loss: 2.2215704917907715
Validation loss: 2.076303957611002

Epoch: 6| Step: 1
Training loss: 2.18827486038208
Validation loss: 2.0996141920807543

Epoch: 6| Step: 2
Training loss: 1.8835318088531494
Validation loss: 2.093861233803534

Epoch: 6| Step: 3
Training loss: 1.5047801733016968
Validation loss: 2.134097937614687

Epoch: 6| Step: 4
Training loss: 1.8412466049194336
Validation loss: 2.130161052109093

Epoch: 6| Step: 5
Training loss: 2.159266471862793
Validation loss: 2.1291896117630826

Epoch: 6| Step: 6
Training loss: 1.8966057300567627
Validation loss: 2.1287584150991132

Epoch: 6| Step: 7
Training loss: 2.438143253326416
Validation loss: 2.1301193237304688

Epoch: 6| Step: 8
Training loss: 1.5363993644714355
Validation loss: 2.1223294965682493

Epoch: 6| Step: 9
Training loss: 1.3616262674331665
Validation loss: 2.116892821045332

Epoch: 6| Step: 10
Training loss: 2.1941349506378174
Validation loss: 2.1235914384165118

Epoch: 6| Step: 11
Training loss: 2.2492752075195312
Validation loss: 2.1056673552400325

Epoch: 6| Step: 12
Training loss: 2.108264207839966
Validation loss: 2.114855791932793

Epoch: 6| Step: 13
Training loss: 1.222793459892273
Validation loss: 2.110681877341322

Epoch: 423| Step: 0
Training loss: 2.4884865283966064
Validation loss: 2.1120349437959733

Epoch: 6| Step: 1
Training loss: 1.1752796173095703
Validation loss: 2.1090808953008344

Epoch: 6| Step: 2
Training loss: 1.7591959238052368
Validation loss: 2.1079429118863997

Epoch: 6| Step: 3
Training loss: 2.111480474472046
Validation loss: 2.116112365517565

Epoch: 6| Step: 4
Training loss: 2.2490668296813965
Validation loss: 2.1117129107957244

Epoch: 6| Step: 5
Training loss: 1.9648747444152832
Validation loss: 2.109974591962753

Epoch: 6| Step: 6
Training loss: 2.2448291778564453
Validation loss: 2.1294281764697005

Epoch: 6| Step: 7
Training loss: 1.8420302867889404
Validation loss: 2.121935038156407

Epoch: 6| Step: 8
Training loss: 2.061680316925049
Validation loss: 2.149094384203675

Epoch: 6| Step: 9
Training loss: 1.7217435836791992
Validation loss: 2.134058355003275

Epoch: 6| Step: 10
Training loss: 1.974640965461731
Validation loss: 2.1444467344591693

Epoch: 6| Step: 11
Training loss: 1.6353822946548462
Validation loss: 2.124722865320021

Epoch: 6| Step: 12
Training loss: 2.3022096157073975
Validation loss: 2.1045435064582416

Epoch: 6| Step: 13
Training loss: 1.333506464958191
Validation loss: 2.089060532149448

Epoch: 424| Step: 0
Training loss: 1.9178402423858643
Validation loss: 2.0736838617632465

Epoch: 6| Step: 1
Training loss: 2.330878734588623
Validation loss: 2.0869635087187572

Epoch: 6| Step: 2
Training loss: 2.317592144012451
Validation loss: 2.0842744688833914

Epoch: 6| Step: 3
Training loss: 2.3949084281921387
Validation loss: 2.102261168982393

Epoch: 6| Step: 4
Training loss: 1.917830467224121
Validation loss: 2.1184176475771013

Epoch: 6| Step: 5
Training loss: 1.4866786003112793
Validation loss: 2.1240745013759983

Epoch: 6| Step: 6
Training loss: 1.8364181518554688
Validation loss: 2.1300736319634224

Epoch: 6| Step: 7
Training loss: 2.1232714653015137
Validation loss: 2.1489973632238244

Epoch: 6| Step: 8
Training loss: 1.8550224304199219
Validation loss: 2.145898162677724

Epoch: 6| Step: 9
Training loss: 2.4209723472595215
Validation loss: 2.151296179781678

Epoch: 6| Step: 10
Training loss: 1.5697240829467773
Validation loss: 2.141390210838728

Epoch: 6| Step: 11
Training loss: 1.8946928977966309
Validation loss: 2.120634137943227

Epoch: 6| Step: 12
Training loss: 1.503697395324707
Validation loss: 2.125385267760164

Epoch: 6| Step: 13
Training loss: 2.113276720046997
Validation loss: 2.1355779504263275

Epoch: 425| Step: 0
Training loss: 1.9200339317321777
Validation loss: 2.1275451926774878

Epoch: 6| Step: 1
Training loss: 2.4538168907165527
Validation loss: 2.1152124199815976

Epoch: 6| Step: 2
Training loss: 2.3130440711975098
Validation loss: 2.09294214043566

Epoch: 6| Step: 3
Training loss: 2.306694507598877
Validation loss: 2.0861712988986763

Epoch: 6| Step: 4
Training loss: 1.421894907951355
Validation loss: 2.092284825540358

Epoch: 6| Step: 5
Training loss: 2.5334877967834473
Validation loss: 2.0834750257512575

Epoch: 6| Step: 6
Training loss: 2.55896258354187
Validation loss: 2.065227407281117

Epoch: 6| Step: 7
Training loss: 1.869590401649475
Validation loss: 2.057108823971082

Epoch: 6| Step: 8
Training loss: 2.073805093765259
Validation loss: 2.0762454489225983

Epoch: 6| Step: 9
Training loss: 1.2254486083984375
Validation loss: 2.063867891988447

Epoch: 6| Step: 10
Training loss: 1.0598783493041992
Validation loss: 2.0774023353412585

Epoch: 6| Step: 11
Training loss: 1.8997306823730469
Validation loss: 2.095677835966951

Epoch: 6| Step: 12
Training loss: 1.7299569845199585
Validation loss: 2.1234586085042646

Epoch: 6| Step: 13
Training loss: 2.1568708419799805
Validation loss: 2.1386031002126713

Epoch: 426| Step: 0
Training loss: 2.521652936935425
Validation loss: 2.1521189417890323

Epoch: 6| Step: 1
Training loss: 1.697403907775879
Validation loss: 2.1256773189831804

Epoch: 6| Step: 2
Training loss: 0.8977771401405334
Validation loss: 2.1455245005187167

Epoch: 6| Step: 3
Training loss: 1.9186439514160156
Validation loss: 2.1434073140544276

Epoch: 6| Step: 4
Training loss: 2.228675127029419
Validation loss: 2.131810668976076

Epoch: 6| Step: 5
Training loss: 2.3035144805908203
Validation loss: 2.1237207176864787

Epoch: 6| Step: 6
Training loss: 1.9665014743804932
Validation loss: 2.1295533974965415

Epoch: 6| Step: 7
Training loss: 1.8543466329574585
Validation loss: 2.127277893404807

Epoch: 6| Step: 8
Training loss: 1.9366769790649414
Validation loss: 2.1302328443014495

Epoch: 6| Step: 9
Training loss: 2.0566422939300537
Validation loss: 2.1127582814103816

Epoch: 6| Step: 10
Training loss: 1.6101558208465576
Validation loss: 2.1412011256781955

Epoch: 6| Step: 11
Training loss: 1.8616399765014648
Validation loss: 2.118664495406612

Epoch: 6| Step: 12
Training loss: 1.9240801334381104
Validation loss: 2.116619979181597

Epoch: 6| Step: 13
Training loss: 2.3903844356536865
Validation loss: 2.114377813954507

Epoch: 427| Step: 0
Training loss: 1.6756478548049927
Validation loss: 2.1182300506099576

Epoch: 6| Step: 1
Training loss: 1.7161049842834473
Validation loss: 2.1242189253530195

Epoch: 6| Step: 2
Training loss: 1.7584065198898315
Validation loss: 2.134200665258592

Epoch: 6| Step: 3
Training loss: 1.6276273727416992
Validation loss: 2.1215260157021145

Epoch: 6| Step: 4
Training loss: 1.7307229042053223
Validation loss: 2.12872116796432

Epoch: 6| Step: 5
Training loss: 2.378040075302124
Validation loss: 2.1352487738414476

Epoch: 6| Step: 6
Training loss: 1.8054112195968628
Validation loss: 2.141290267308553

Epoch: 6| Step: 7
Training loss: 2.1299259662628174
Validation loss: 2.124381393514654

Epoch: 6| Step: 8
Training loss: 1.8962732553482056
Validation loss: 2.1137406133836314

Epoch: 6| Step: 9
Training loss: 2.1606407165527344
Validation loss: 2.1214130950230423

Epoch: 6| Step: 10
Training loss: 2.172990083694458
Validation loss: 2.1291690705924906

Epoch: 6| Step: 11
Training loss: 1.9071166515350342
Validation loss: 2.1316530883953138

Epoch: 6| Step: 12
Training loss: 1.9185409545898438
Validation loss: 2.108608789341424

Epoch: 6| Step: 13
Training loss: 1.8846334218978882
Validation loss: 2.134377828208349

Epoch: 428| Step: 0
Training loss: 1.7624225616455078
Validation loss: 2.106177355653496

Epoch: 6| Step: 1
Training loss: 1.7989609241485596
Validation loss: 2.106785712703582

Epoch: 6| Step: 2
Training loss: 1.8702446222305298
Validation loss: 2.10120730246267

Epoch: 6| Step: 3
Training loss: 2.0331315994262695
Validation loss: 2.140578685268279

Epoch: 6| Step: 4
Training loss: 1.9550632238388062
Validation loss: 2.1423881566652687

Epoch: 6| Step: 5
Training loss: 1.832982063293457
Validation loss: 2.1797337237224785

Epoch: 6| Step: 6
Training loss: 1.9290592670440674
Validation loss: 2.1585134870262555

Epoch: 6| Step: 7
Training loss: 1.6955852508544922
Validation loss: 2.1326807301531554

Epoch: 6| Step: 8
Training loss: 1.7105093002319336
Validation loss: 2.125484284534249

Epoch: 6| Step: 9
Training loss: 2.415881633758545
Validation loss: 2.1141556873116443

Epoch: 6| Step: 10
Training loss: 1.5046868324279785
Validation loss: 2.1094599231596916

Epoch: 6| Step: 11
Training loss: 1.8129656314849854
Validation loss: 2.088389406922043

Epoch: 6| Step: 12
Training loss: 2.2869338989257812
Validation loss: 2.0937912259050595

Epoch: 6| Step: 13
Training loss: 2.3695127964019775
Validation loss: 2.090774159277639

Epoch: 429| Step: 0
Training loss: 1.6631643772125244
Validation loss: 2.0701908270517984

Epoch: 6| Step: 1
Training loss: 1.9403992891311646
Validation loss: 2.079329349661386

Epoch: 6| Step: 2
Training loss: 2.563443660736084
Validation loss: 2.053506674305085

Epoch: 6| Step: 3
Training loss: 2.2625133991241455
Validation loss: 2.0797997905362036

Epoch: 6| Step: 4
Training loss: 2.033646583557129
Validation loss: 2.118338713081934

Epoch: 6| Step: 5
Training loss: 1.671721339225769
Validation loss: 2.12710980702472

Epoch: 6| Step: 6
Training loss: 1.7171297073364258
Validation loss: 2.127937411749235

Epoch: 6| Step: 7
Training loss: 2.0661656856536865
Validation loss: 2.118603383341143

Epoch: 6| Step: 8
Training loss: 2.698317050933838
Validation loss: 2.1441970845704437

Epoch: 6| Step: 9
Training loss: 1.3143638372421265
Validation loss: 2.1333775225506035

Epoch: 6| Step: 10
Training loss: 1.688394546508789
Validation loss: 2.121345945583877

Epoch: 6| Step: 11
Training loss: 1.999481439590454
Validation loss: 2.12081253656777

Epoch: 6| Step: 12
Training loss: 1.590001106262207
Validation loss: 2.103962484226432

Epoch: 6| Step: 13
Training loss: 1.6346497535705566
Validation loss: 2.0888805697041173

Epoch: 430| Step: 0
Training loss: 1.7449815273284912
Validation loss: 2.0999635547719975

Epoch: 6| Step: 1
Training loss: 2.374619483947754
Validation loss: 2.1032911167349866

Epoch: 6| Step: 2
Training loss: 1.842482328414917
Validation loss: 2.1114657873748452

Epoch: 6| Step: 3
Training loss: 2.0929274559020996
Validation loss: 2.113138514180337

Epoch: 6| Step: 4
Training loss: 1.2901957035064697
Validation loss: 2.1026550672387563

Epoch: 6| Step: 5
Training loss: 1.7655936479568481
Validation loss: 2.1167136828104653

Epoch: 6| Step: 6
Training loss: 2.018444061279297
Validation loss: 2.1068387672465336

Epoch: 6| Step: 7
Training loss: 2.008122682571411
Validation loss: 2.113025398664577

Epoch: 6| Step: 8
Training loss: 2.685619354248047
Validation loss: 2.1123911308985885

Epoch: 6| Step: 9
Training loss: 2.190558910369873
Validation loss: 2.1373922324949697

Epoch: 6| Step: 10
Training loss: 1.4469352960586548
Validation loss: 2.1373916031211935

Epoch: 6| Step: 11
Training loss: 2.054323673248291
Validation loss: 2.1498487072606243

Epoch: 6| Step: 12
Training loss: 1.8208736181259155
Validation loss: 2.117333983862272

Epoch: 6| Step: 13
Training loss: 1.6122978925704956
Validation loss: 2.1019165208262782

Epoch: 431| Step: 0
Training loss: 1.9152030944824219
Validation loss: 2.1119474672502085

Epoch: 6| Step: 1
Training loss: 1.8103001117706299
Validation loss: 2.0984715325858003

Epoch: 6| Step: 2
Training loss: 1.7670612335205078
Validation loss: 2.084863985738447

Epoch: 6| Step: 3
Training loss: 2.373260021209717
Validation loss: 2.094244146859774

Epoch: 6| Step: 4
Training loss: 2.151968479156494
Validation loss: 2.098121594357234

Epoch: 6| Step: 5
Training loss: 1.962288737297058
Validation loss: 2.0952457638197046

Epoch: 6| Step: 6
Training loss: 1.7468676567077637
Validation loss: 2.094479453179144

Epoch: 6| Step: 7
Training loss: 1.9480187892913818
Validation loss: 2.073416999591294

Epoch: 6| Step: 8
Training loss: 1.656409740447998
Validation loss: 2.112715698057605

Epoch: 6| Step: 9
Training loss: 2.673037052154541
Validation loss: 2.1272332796486477

Epoch: 6| Step: 10
Training loss: 1.146723985671997
Validation loss: 2.138683957438315

Epoch: 6| Step: 11
Training loss: 1.4725227355957031
Validation loss: 2.1504474660401702

Epoch: 6| Step: 12
Training loss: 1.8033697605133057
Validation loss: 2.1616339196440992

Epoch: 6| Step: 13
Training loss: 2.513596534729004
Validation loss: 2.172631532915177

Epoch: 432| Step: 0
Training loss: 1.8996450901031494
Validation loss: 2.1323944830125376

Epoch: 6| Step: 1
Training loss: 1.9451022148132324
Validation loss: 2.135699937420507

Epoch: 6| Step: 2
Training loss: 1.9553377628326416
Validation loss: 2.1095030025769304

Epoch: 6| Step: 3
Training loss: 2.5643081665039062
Validation loss: 2.0928577633314234

Epoch: 6| Step: 4
Training loss: 2.178617000579834
Validation loss: 2.099993384012612

Epoch: 6| Step: 5
Training loss: 1.6395832300186157
Validation loss: 2.105859596242187

Epoch: 6| Step: 6
Training loss: 1.8863095045089722
Validation loss: 2.128289991809476

Epoch: 6| Step: 7
Training loss: 1.621185064315796
Validation loss: 2.119832633644022

Epoch: 6| Step: 8
Training loss: 1.7014036178588867
Validation loss: 2.12918500233722

Epoch: 6| Step: 9
Training loss: 1.8642001152038574
Validation loss: 2.1378513382327173

Epoch: 6| Step: 10
Training loss: 2.430473566055298
Validation loss: 2.129943909183625

Epoch: 6| Step: 11
Training loss: 1.7717640399932861
Validation loss: 2.11595312241585

Epoch: 6| Step: 12
Training loss: 1.430901288986206
Validation loss: 2.113500564329086

Epoch: 6| Step: 13
Training loss: 1.4347631931304932
Validation loss: 2.108395225258284

Epoch: 433| Step: 0
Training loss: 1.9328343868255615
Validation loss: 2.106680986701801

Epoch: 6| Step: 1
Training loss: 1.9321043491363525
Validation loss: 2.0913807807430143

Epoch: 6| Step: 2
Training loss: 1.884535312652588
Validation loss: 2.0996566690424436

Epoch: 6| Step: 3
Training loss: 2.494154214859009
Validation loss: 2.089435277446624

Epoch: 6| Step: 4
Training loss: 1.714098572731018
Validation loss: 2.0931719067276164

Epoch: 6| Step: 5
Training loss: 2.183168649673462
Validation loss: 2.0986619200757755

Epoch: 6| Step: 6
Training loss: 2.163458824157715
Validation loss: 2.0976376264326033

Epoch: 6| Step: 7
Training loss: 1.7757829427719116
Validation loss: 2.1191011782615417

Epoch: 6| Step: 8
Training loss: 1.2853190898895264
Validation loss: 2.1169845557981923

Epoch: 6| Step: 9
Training loss: 2.2550246715545654
Validation loss: 2.1011244045790805

Epoch: 6| Step: 10
Training loss: 1.4652082920074463
Validation loss: 2.111717067739015

Epoch: 6| Step: 11
Training loss: 1.9266910552978516
Validation loss: 2.1237392284536876

Epoch: 6| Step: 12
Training loss: 1.634101152420044
Validation loss: 2.135485626036121

Epoch: 6| Step: 13
Training loss: 2.1042888164520264
Validation loss: 2.12932566929889

Epoch: 434| Step: 0
Training loss: 1.814913272857666
Validation loss: 2.137341965911209

Epoch: 6| Step: 1
Training loss: 1.888671636581421
Validation loss: 2.154592655038321

Epoch: 6| Step: 2
Training loss: 1.7084705829620361
Validation loss: 2.164891968491257

Epoch: 6| Step: 3
Training loss: 2.216416597366333
Validation loss: 2.1878082559954737

Epoch: 6| Step: 4
Training loss: 2.091351270675659
Validation loss: 2.170124896111027

Epoch: 6| Step: 5
Training loss: 1.2944920063018799
Validation loss: 2.168245297606273

Epoch: 6| Step: 6
Training loss: 1.7340755462646484
Validation loss: 2.1188920031311693

Epoch: 6| Step: 7
Training loss: 1.7018773555755615
Validation loss: 2.105742818565779

Epoch: 6| Step: 8
Training loss: 1.7454066276550293
Validation loss: 2.1125122731731785

Epoch: 6| Step: 9
Training loss: 2.278064250946045
Validation loss: 2.0878694608647335

Epoch: 6| Step: 10
Training loss: 2.760467767715454
Validation loss: 2.0741830871951197

Epoch: 6| Step: 11
Training loss: 1.437861442565918
Validation loss: 2.0701794701237834

Epoch: 6| Step: 12
Training loss: 2.180969715118408
Validation loss: 2.0819923672624814

Epoch: 6| Step: 13
Training loss: 1.7112096548080444
Validation loss: 2.094807013388603

Epoch: 435| Step: 0
Training loss: 1.8356716632843018
Validation loss: 2.110096964784848

Epoch: 6| Step: 1
Training loss: 2.1547656059265137
Validation loss: 2.1232250326423237

Epoch: 6| Step: 2
Training loss: 3.150179624557495
Validation loss: 2.1196240660964802

Epoch: 6| Step: 3
Training loss: 1.8879773616790771
Validation loss: 2.1435557180835354

Epoch: 6| Step: 4
Training loss: 1.3805718421936035
Validation loss: 2.136905224092545

Epoch: 6| Step: 5
Training loss: 1.6188218593597412
Validation loss: 2.103830352906258

Epoch: 6| Step: 6
Training loss: 2.220419406890869
Validation loss: 2.133296658915858

Epoch: 6| Step: 7
Training loss: 1.1447982788085938
Validation loss: 2.1078656591394895

Epoch: 6| Step: 8
Training loss: 2.518956422805786
Validation loss: 2.101057296158165

Epoch: 6| Step: 9
Training loss: 1.850848913192749
Validation loss: 2.083887618075135

Epoch: 6| Step: 10
Training loss: 1.9870599508285522
Validation loss: 2.077619202675358

Epoch: 6| Step: 11
Training loss: 1.422177791595459
Validation loss: 2.081591078030166

Epoch: 6| Step: 12
Training loss: 1.3921256065368652
Validation loss: 2.0805735870074202

Epoch: 6| Step: 13
Training loss: 2.0862622261047363
Validation loss: 2.1196303726524435

Epoch: 436| Step: 0
Training loss: 1.2614349126815796
Validation loss: 2.1151787850164596

Epoch: 6| Step: 1
Training loss: 1.5467000007629395
Validation loss: 2.1539395983501146

Epoch: 6| Step: 2
Training loss: 1.7643142938613892
Validation loss: 2.161947270875336

Epoch: 6| Step: 3
Training loss: 1.2740764617919922
Validation loss: 2.1629769917457335

Epoch: 6| Step: 4
Training loss: 2.6034584045410156
Validation loss: 2.1680650839241604

Epoch: 6| Step: 5
Training loss: 2.4021687507629395
Validation loss: 2.1893565526572605

Epoch: 6| Step: 6
Training loss: 2.508321762084961
Validation loss: 2.188578409533347

Epoch: 6| Step: 7
Training loss: 2.1741461753845215
Validation loss: 2.1773920597568637

Epoch: 6| Step: 8
Training loss: 2.000546932220459
Validation loss: 2.1476990292149205

Epoch: 6| Step: 9
Training loss: 1.47538161277771
Validation loss: 2.1175141501170334

Epoch: 6| Step: 10
Training loss: 1.9636900424957275
Validation loss: 2.0912380910688833

Epoch: 6| Step: 11
Training loss: 1.6937882900238037
Validation loss: 2.074708331015802

Epoch: 6| Step: 12
Training loss: 1.6844968795776367
Validation loss: 2.0717700642924153

Epoch: 6| Step: 13
Training loss: 2.477921724319458
Validation loss: 2.05469923891047

Epoch: 437| Step: 0
Training loss: 1.6077274084091187
Validation loss: 2.0708383526853336

Epoch: 6| Step: 1
Training loss: 2.2201695442199707
Validation loss: 2.059355790897082

Epoch: 6| Step: 2
Training loss: 1.6772377490997314
Validation loss: 2.0540979498176166

Epoch: 6| Step: 3
Training loss: 2.6178672313690186
Validation loss: 2.0639661922249743

Epoch: 6| Step: 4
Training loss: 2.0607261657714844
Validation loss: 2.054054255126625

Epoch: 6| Step: 5
Training loss: 1.6240757703781128
Validation loss: 2.06506057195766

Epoch: 6| Step: 6
Training loss: 1.669973611831665
Validation loss: 2.072180296785088

Epoch: 6| Step: 7
Training loss: 2.3754658699035645
Validation loss: 2.1075954693619923

Epoch: 6| Step: 8
Training loss: 2.57479190826416
Validation loss: 2.120952037072951

Epoch: 6| Step: 9
Training loss: 1.4115588665008545
Validation loss: 2.141032930343382

Epoch: 6| Step: 10
Training loss: 0.9945127964019775
Validation loss: 2.1250809777167534

Epoch: 6| Step: 11
Training loss: 2.3183202743530273
Validation loss: 2.1181255002175607

Epoch: 6| Step: 12
Training loss: 1.7687374353408813
Validation loss: 2.1380164520714873

Epoch: 6| Step: 13
Training loss: 1.4561131000518799
Validation loss: 2.101346892695273

Epoch: 438| Step: 0
Training loss: 2.606978178024292
Validation loss: 2.115716354821318

Epoch: 6| Step: 1
Training loss: 1.7353330850601196
Validation loss: 2.1202624715784544

Epoch: 6| Step: 2
Training loss: 1.0809199810028076
Validation loss: 2.1167366107304892

Epoch: 6| Step: 3
Training loss: 1.9402133226394653
Validation loss: 2.1504579103121193

Epoch: 6| Step: 4
Training loss: 2.0701422691345215
Validation loss: 2.137443502744039

Epoch: 6| Step: 5
Training loss: 0.9226250648498535
Validation loss: 2.135487728221442

Epoch: 6| Step: 6
Training loss: 2.361600399017334
Validation loss: 2.1413638694311983

Epoch: 6| Step: 7
Training loss: 2.383451461791992
Validation loss: 2.1451009447856615

Epoch: 6| Step: 8
Training loss: 2.043929100036621
Validation loss: 2.118866261615548

Epoch: 6| Step: 9
Training loss: 1.7187705039978027
Validation loss: 2.122216247743176

Epoch: 6| Step: 10
Training loss: 2.1271729469299316
Validation loss: 2.122083781867899

Epoch: 6| Step: 11
Training loss: 1.3843138217926025
Validation loss: 2.1291637984655236

Epoch: 6| Step: 12
Training loss: 1.7687684297561646
Validation loss: 2.1167569480916506

Epoch: 6| Step: 13
Training loss: 2.4757113456726074
Validation loss: 2.1166272394118772

Epoch: 439| Step: 0
Training loss: 1.909016489982605
Validation loss: 2.0810727868028867

Epoch: 6| Step: 1
Training loss: 1.8120757341384888
Validation loss: 2.049103216458392

Epoch: 6| Step: 2
Training loss: 1.9459724426269531
Validation loss: 2.0567428578612623

Epoch: 6| Step: 3
Training loss: 1.417630910873413
Validation loss: 2.056436707896571

Epoch: 6| Step: 4
Training loss: 2.101663112640381
Validation loss: 2.0484107284135717

Epoch: 6| Step: 5
Training loss: 1.1263693571090698
Validation loss: 2.047728729504411

Epoch: 6| Step: 6
Training loss: 2.089844226837158
Validation loss: 2.0698956507508472

Epoch: 6| Step: 7
Training loss: 2.0840489864349365
Validation loss: 2.101994818256747

Epoch: 6| Step: 8
Training loss: 2.2252755165100098
Validation loss: 2.0978869943208593

Epoch: 6| Step: 9
Training loss: 1.7828704118728638
Validation loss: 2.112384024486747

Epoch: 6| Step: 10
Training loss: 2.723585605621338
Validation loss: 2.0895755829349643

Epoch: 6| Step: 11
Training loss: 1.504987120628357
Validation loss: 2.0863027341904177

Epoch: 6| Step: 12
Training loss: 1.5960112810134888
Validation loss: 2.0787936384959886

Epoch: 6| Step: 13
Training loss: 2.4217329025268555
Validation loss: 2.0931274019261843

Epoch: 440| Step: 0
Training loss: 2.1679484844207764
Validation loss: 2.0964856852767286

Epoch: 6| Step: 1
Training loss: 1.6217224597930908
Validation loss: 2.094055038626476

Epoch: 6| Step: 2
Training loss: 2.0396389961242676
Validation loss: 2.1022836495471258

Epoch: 6| Step: 3
Training loss: 1.7945013046264648
Validation loss: 2.1152559787996355

Epoch: 6| Step: 4
Training loss: 1.7780393362045288
Validation loss: 2.130956304970608

Epoch: 6| Step: 5
Training loss: 1.815862774848938
Validation loss: 2.107300924998458

Epoch: 6| Step: 6
Training loss: 2.0233733654022217
Validation loss: 2.1058328766976633

Epoch: 6| Step: 7
Training loss: 1.7641352415084839
Validation loss: 2.0945538064484954

Epoch: 6| Step: 8
Training loss: 1.721196174621582
Validation loss: 2.082176927597292

Epoch: 6| Step: 9
Training loss: 1.6654009819030762
Validation loss: 2.093292966965706

Epoch: 6| Step: 10
Training loss: 1.242624044418335
Validation loss: 2.089802053666884

Epoch: 6| Step: 11
Training loss: 2.3573157787323
Validation loss: 2.104034448182711

Epoch: 6| Step: 12
Training loss: 2.4165821075439453
Validation loss: 2.140160578553395

Epoch: 6| Step: 13
Training loss: 1.697210669517517
Validation loss: 2.145415267636699

Epoch: 441| Step: 0
Training loss: 1.9508634805679321
Validation loss: 2.17030611858573

Epoch: 6| Step: 1
Training loss: 1.0082169771194458
Validation loss: 2.1583786164560625

Epoch: 6| Step: 2
Training loss: 2.091837167739868
Validation loss: 2.156627607601945

Epoch: 6| Step: 3
Training loss: 1.9040658473968506
Validation loss: 2.1602990499106784

Epoch: 6| Step: 4
Training loss: 1.7841242551803589
Validation loss: 2.1648410251063686

Epoch: 6| Step: 5
Training loss: 1.757490634918213
Validation loss: 2.1524594650473645

Epoch: 6| Step: 6
Training loss: 1.8441486358642578
Validation loss: 2.1277041973606234

Epoch: 6| Step: 7
Training loss: 2.00300931930542
Validation loss: 2.141017395962951

Epoch: 6| Step: 8
Training loss: 1.6564124822616577
Validation loss: 2.1462330074720484

Epoch: 6| Step: 9
Training loss: 1.6664897203445435
Validation loss: 2.144234016377439

Epoch: 6| Step: 10
Training loss: 2.2360973358154297
Validation loss: 2.147088122624223

Epoch: 6| Step: 11
Training loss: 2.284456729888916
Validation loss: 2.149731102810111

Epoch: 6| Step: 12
Training loss: 2.131972312927246
Validation loss: 2.126258844970375

Epoch: 6| Step: 13
Training loss: 1.9316706657409668
Validation loss: 2.122393583738676

Epoch: 442| Step: 0
Training loss: 1.7878694534301758
Validation loss: 2.1016133190483175

Epoch: 6| Step: 1
Training loss: 1.782474160194397
Validation loss: 2.0905350869701755

Epoch: 6| Step: 2
Training loss: 1.8654944896697998
Validation loss: 2.069038374449617

Epoch: 6| Step: 3
Training loss: 1.7735611200332642
Validation loss: 2.074484973825434

Epoch: 6| Step: 4
Training loss: 1.5177977085113525
Validation loss: 2.055092809020832

Epoch: 6| Step: 5
Training loss: 1.448488712310791
Validation loss: 2.0547544930570867

Epoch: 6| Step: 6
Training loss: 2.417111873626709
Validation loss: 2.065384175187798

Epoch: 6| Step: 7
Training loss: 2.5053248405456543
Validation loss: 2.065826177597046

Epoch: 6| Step: 8
Training loss: 2.136218309402466
Validation loss: 2.076402553948023

Epoch: 6| Step: 9
Training loss: 2.197415828704834
Validation loss: 2.1000462629461802

Epoch: 6| Step: 10
Training loss: 1.922788143157959
Validation loss: 2.0920904759437806

Epoch: 6| Step: 11
Training loss: 1.1333637237548828
Validation loss: 2.1170583873666744

Epoch: 6| Step: 12
Training loss: 2.0934054851531982
Validation loss: 2.137448310852051

Epoch: 6| Step: 13
Training loss: 1.2696729898452759
Validation loss: 2.1594640606193134

Epoch: 443| Step: 0
Training loss: 1.5485889911651611
Validation loss: 2.1848875322649555

Epoch: 6| Step: 1
Training loss: 1.6903921365737915
Validation loss: 2.1590708583913822

Epoch: 6| Step: 2
Training loss: 1.687104344367981
Validation loss: 2.119871821454776

Epoch: 6| Step: 3
Training loss: 1.9368603229522705
Validation loss: 2.118206690716487

Epoch: 6| Step: 4
Training loss: 2.381903648376465
Validation loss: 2.1003174653617283

Epoch: 6| Step: 5
Training loss: 1.516669511795044
Validation loss: 2.1021895946994906

Epoch: 6| Step: 6
Training loss: 1.9958610534667969
Validation loss: 2.1082879215158443

Epoch: 6| Step: 7
Training loss: 1.8324377536773682
Validation loss: 2.1362804161605013

Epoch: 6| Step: 8
Training loss: 1.9301972389221191
Validation loss: 2.1397349090986353

Epoch: 6| Step: 9
Training loss: 1.292779564857483
Validation loss: 2.128712918168755

Epoch: 6| Step: 10
Training loss: 2.1165661811828613
Validation loss: 2.1534728260450464

Epoch: 6| Step: 11
Training loss: 2.3811516761779785
Validation loss: 2.1442507774599138

Epoch: 6| Step: 12
Training loss: 2.113147020339966
Validation loss: 2.185858113791353

Epoch: 6| Step: 13
Training loss: 2.117652416229248
Validation loss: 2.18443343716283

Epoch: 444| Step: 0
Training loss: 2.0479769706726074
Validation loss: 2.1812400100051716

Epoch: 6| Step: 1
Training loss: 1.997580885887146
Validation loss: 2.1654120683670044

Epoch: 6| Step: 2
Training loss: 2.212463855743408
Validation loss: 2.1504749867223922

Epoch: 6| Step: 3
Training loss: 1.8745797872543335
Validation loss: 2.1340893673640426

Epoch: 6| Step: 4
Training loss: 2.1647520065307617
Validation loss: 2.123172788209813

Epoch: 6| Step: 5
Training loss: 1.5096790790557861
Validation loss: 2.1055968333316106

Epoch: 6| Step: 6
Training loss: 1.3132976293563843
Validation loss: 2.0867678465381747

Epoch: 6| Step: 7
Training loss: 1.8619444370269775
Validation loss: 2.1025195839584514

Epoch: 6| Step: 8
Training loss: 2.0331366062164307
Validation loss: 2.099863585605416

Epoch: 6| Step: 9
Training loss: 2.022486448287964
Validation loss: 2.1021409034729004

Epoch: 6| Step: 10
Training loss: 1.9540796279907227
Validation loss: 2.0975983347944034

Epoch: 6| Step: 11
Training loss: 1.7518385648727417
Validation loss: 2.0875884768783406

Epoch: 6| Step: 12
Training loss: 1.7441099882125854
Validation loss: 2.0798767112916514

Epoch: 6| Step: 13
Training loss: 1.4341036081314087
Validation loss: 2.0974142089966805

Epoch: 445| Step: 0
Training loss: 1.997736930847168
Validation loss: 2.1221907215733684

Epoch: 6| Step: 1
Training loss: 2.0384724140167236
Validation loss: 2.10335365674829

Epoch: 6| Step: 2
Training loss: 2.228048324584961
Validation loss: 2.133238407873338

Epoch: 6| Step: 3
Training loss: 2.021759510040283
Validation loss: 2.109050166222357

Epoch: 6| Step: 4
Training loss: 1.402181625366211
Validation loss: 2.1213348911654566

Epoch: 6| Step: 5
Training loss: 1.2701337337493896
Validation loss: 2.1144815747455885

Epoch: 6| Step: 6
Training loss: 1.629330039024353
Validation loss: 2.12518487950807

Epoch: 6| Step: 7
Training loss: 2.453803539276123
Validation loss: 2.1339589600921958

Epoch: 6| Step: 8
Training loss: 2.1045830249786377
Validation loss: 2.1191695069753997

Epoch: 6| Step: 9
Training loss: 2.216489315032959
Validation loss: 2.135903609696255

Epoch: 6| Step: 10
Training loss: 1.2751127481460571
Validation loss: 2.1463636993080057

Epoch: 6| Step: 11
Training loss: 1.5920696258544922
Validation loss: 2.1576578335095475

Epoch: 6| Step: 12
Training loss: 1.897196650505066
Validation loss: 2.140230937670636

Epoch: 6| Step: 13
Training loss: 1.841574788093567
Validation loss: 2.112086357608918

Epoch: 446| Step: 0
Training loss: 0.8531272411346436
Validation loss: 2.119054684074976

Epoch: 6| Step: 1
Training loss: 1.9443331956863403
Validation loss: 2.087542954311576

Epoch: 6| Step: 2
Training loss: 2.4564599990844727
Validation loss: 2.080821657693514

Epoch: 6| Step: 3
Training loss: 2.514174222946167
Validation loss: 2.0837696918877224

Epoch: 6| Step: 4
Training loss: 1.750563383102417
Validation loss: 2.076203053997409

Epoch: 6| Step: 5
Training loss: 1.604312539100647
Validation loss: 2.085518272974158

Epoch: 6| Step: 6
Training loss: 1.8384579420089722
Validation loss: 2.0852384208351054

Epoch: 6| Step: 7
Training loss: 2.063870429992676
Validation loss: 2.1097123289621003

Epoch: 6| Step: 8
Training loss: 2.112598419189453
Validation loss: 2.112329065158803

Epoch: 6| Step: 9
Training loss: 1.1487020254135132
Validation loss: 2.1171719912559754

Epoch: 6| Step: 10
Training loss: 2.2030937671661377
Validation loss: 2.1484916543447845

Epoch: 6| Step: 11
Training loss: 1.79414963722229
Validation loss: 2.1378507383408083

Epoch: 6| Step: 12
Training loss: 2.1202752590179443
Validation loss: 2.1759872885160547

Epoch: 6| Step: 13
Training loss: 1.1616590023040771
Validation loss: 2.1705137491226196

Epoch: 447| Step: 0
Training loss: 1.48659086227417
Validation loss: 2.174286691091394

Epoch: 6| Step: 1
Training loss: 1.9461162090301514
Validation loss: 2.193994261885202

Epoch: 6| Step: 2
Training loss: 1.7625524997711182
Validation loss: 2.18052343655658

Epoch: 6| Step: 3
Training loss: 2.5279316902160645
Validation loss: 2.161335245255501

Epoch: 6| Step: 4
Training loss: 2.125511646270752
Validation loss: 2.1411228167113436

Epoch: 6| Step: 5
Training loss: 1.8148239850997925
Validation loss: 2.1092215994352936

Epoch: 6| Step: 6
Training loss: 1.8167779445648193
Validation loss: 2.077697641106062

Epoch: 6| Step: 7
Training loss: 1.6447584629058838
Validation loss: 2.0830647919767644

Epoch: 6| Step: 8
Training loss: 1.5580413341522217
Validation loss: 2.0655139415494856

Epoch: 6| Step: 9
Training loss: 1.729171633720398
Validation loss: 2.063211392330867

Epoch: 6| Step: 10
Training loss: 2.2398691177368164
Validation loss: 2.052850325902303

Epoch: 6| Step: 11
Training loss: 1.3597099781036377
Validation loss: 2.0770375190242643

Epoch: 6| Step: 12
Training loss: 2.1508843898773193
Validation loss: 2.084779547106835

Epoch: 6| Step: 13
Training loss: 1.7375551462173462
Validation loss: 2.104046288356986

Epoch: 448| Step: 0
Training loss: 1.9243080615997314
Validation loss: 2.0913805115607476

Epoch: 6| Step: 1
Training loss: 1.9223721027374268
Validation loss: 2.0963657543223393

Epoch: 6| Step: 2
Training loss: 2.291482448577881
Validation loss: 2.130737089341687

Epoch: 6| Step: 3
Training loss: 2.128685235977173
Validation loss: 2.114376944880332

Epoch: 6| Step: 4
Training loss: 2.5552408695220947
Validation loss: 2.103007105089003

Epoch: 6| Step: 5
Training loss: 2.0478076934814453
Validation loss: 2.09143748078295

Epoch: 6| Step: 6
Training loss: 1.6990755796432495
Validation loss: 2.084655789918797

Epoch: 6| Step: 7
Training loss: 1.6390230655670166
Validation loss: 2.1053321310268935

Epoch: 6| Step: 8
Training loss: 1.3267362117767334
Validation loss: 2.0893977918932514

Epoch: 6| Step: 9
Training loss: 1.4020874500274658
Validation loss: 2.122680079552435

Epoch: 6| Step: 10
Training loss: 1.2368708848953247
Validation loss: 2.122071163628691

Epoch: 6| Step: 11
Training loss: 1.748732328414917
Validation loss: 2.1207679779298845

Epoch: 6| Step: 12
Training loss: 1.6979265213012695
Validation loss: 2.1336455447699434

Epoch: 6| Step: 13
Training loss: 2.675246000289917
Validation loss: 2.1168748512062976

Epoch: 449| Step: 0
Training loss: 0.7484201788902283
Validation loss: 2.1549440609511508

Epoch: 6| Step: 1
Training loss: 2.1248979568481445
Validation loss: 2.1336516321346326

Epoch: 6| Step: 2
Training loss: 1.9407464265823364
Validation loss: 2.109544741210117

Epoch: 6| Step: 3
Training loss: 2.436124086380005
Validation loss: 2.0994880019977527

Epoch: 6| Step: 4
Training loss: 2.605087995529175
Validation loss: 2.1101904171769337

Epoch: 6| Step: 5
Training loss: 1.3249342441558838
Validation loss: 2.101124476361018

Epoch: 6| Step: 6
Training loss: 2.042635917663574
Validation loss: 2.085868767512742

Epoch: 6| Step: 7
Training loss: 1.786190390586853
Validation loss: 2.1074426122891006

Epoch: 6| Step: 8
Training loss: 1.8910316228866577
Validation loss: 2.076626496930276

Epoch: 6| Step: 9
Training loss: 1.361536979675293
Validation loss: 2.0820137044434905

Epoch: 6| Step: 10
Training loss: 1.622040867805481
Validation loss: 2.118912858347739

Epoch: 6| Step: 11
Training loss: 1.9246948957443237
Validation loss: 2.1069777575872277

Epoch: 6| Step: 12
Training loss: 1.8789416551589966
Validation loss: 2.087853034337362

Epoch: 6| Step: 13
Training loss: 2.173232078552246
Validation loss: 2.1003727195083455

Epoch: 450| Step: 0
Training loss: 1.8134557008743286
Validation loss: 2.124435276113531

Epoch: 6| Step: 1
Training loss: 1.6743364334106445
Validation loss: 2.128494272949875

Epoch: 6| Step: 2
Training loss: 1.5756560564041138
Validation loss: 2.1421263910109

Epoch: 6| Step: 3
Training loss: 1.51361882686615
Validation loss: 2.147129435693064

Epoch: 6| Step: 4
Training loss: 1.5649852752685547
Validation loss: 2.1687769659103884

Epoch: 6| Step: 5
Training loss: 2.8313913345336914
Validation loss: 2.161883618241997

Epoch: 6| Step: 6
Training loss: 2.39449405670166
Validation loss: 2.1363080855338805

Epoch: 6| Step: 7
Training loss: 1.6587018966674805
Validation loss: 2.1353949718577887

Epoch: 6| Step: 8
Training loss: 1.7070255279541016
Validation loss: 2.1212328057135306

Epoch: 6| Step: 9
Training loss: 2.5064806938171387
Validation loss: 2.1361200066022974

Epoch: 6| Step: 10
Training loss: 1.955190658569336
Validation loss: 2.1258948900366343

Epoch: 6| Step: 11
Training loss: 1.4020469188690186
Validation loss: 2.1000019709269204

Epoch: 6| Step: 12
Training loss: 1.9437239170074463
Validation loss: 2.0915824392790436

Epoch: 6| Step: 13
Training loss: 0.8377302885055542
Validation loss: 2.0748875807690363

Epoch: 451| Step: 0
Training loss: 1.5624184608459473
Validation loss: 2.075014970635855

Epoch: 6| Step: 1
Training loss: 1.3787076473236084
Validation loss: 2.076355844415644

Epoch: 6| Step: 2
Training loss: 1.131011724472046
Validation loss: 2.0933312549385974

Epoch: 6| Step: 3
Training loss: 2.1027958393096924
Validation loss: 2.126565070562465

Epoch: 6| Step: 4
Training loss: 1.4060865640640259
Validation loss: 2.150042433892527

Epoch: 6| Step: 5
Training loss: 1.933036208152771
Validation loss: 2.155920144050352

Epoch: 6| Step: 6
Training loss: 2.219543933868408
Validation loss: 2.144982402042676

Epoch: 6| Step: 7
Training loss: 2.2702796459198
Validation loss: 2.1464942962892595

Epoch: 6| Step: 8
Training loss: 1.7369799613952637
Validation loss: 2.164359343949185

Epoch: 6| Step: 9
Training loss: 2.3934407234191895
Validation loss: 2.146935673170192

Epoch: 6| Step: 10
Training loss: 1.9714818000793457
Validation loss: 2.1127885669790287

Epoch: 6| Step: 11
Training loss: 1.4101290702819824
Validation loss: 2.123128115489919

Epoch: 6| Step: 12
Training loss: 2.054776191711426
Validation loss: 2.129755227796493

Epoch: 6| Step: 13
Training loss: 2.2392003536224365
Validation loss: 2.1197328131685973

Epoch: 452| Step: 0
Training loss: 1.9836602210998535
Validation loss: 2.11415684094993

Epoch: 6| Step: 1
Training loss: 1.5253437757492065
Validation loss: 2.1150488366362867

Epoch: 6| Step: 2
Training loss: 2.0240511894226074
Validation loss: 2.1024874858958746

Epoch: 6| Step: 3
Training loss: 1.4121787548065186
Validation loss: 2.091904399215534

Epoch: 6| Step: 4
Training loss: 1.4360754489898682
Validation loss: 2.098633103473212

Epoch: 6| Step: 5
Training loss: 1.618714451789856
Validation loss: 2.123874533560968

Epoch: 6| Step: 6
Training loss: 1.9764400720596313
Validation loss: 2.1498858326224872

Epoch: 6| Step: 7
Training loss: 1.7104865312576294
Validation loss: 2.1631456754540883

Epoch: 6| Step: 8
Training loss: 1.8129937648773193
Validation loss: 2.1616920553227907

Epoch: 6| Step: 9
Training loss: 2.4420533180236816
Validation loss: 2.19707554130144

Epoch: 6| Step: 10
Training loss: 2.5410594940185547
Validation loss: 2.165308033266375

Epoch: 6| Step: 11
Training loss: 2.2037906646728516
Validation loss: 2.1039980508947886

Epoch: 6| Step: 12
Training loss: 1.4431712627410889
Validation loss: 2.1054442851774153

Epoch: 6| Step: 13
Training loss: 1.4348034858703613
Validation loss: 2.1010931409815305

Epoch: 453| Step: 0
Training loss: 1.8425235748291016
Validation loss: 2.0980583980519283

Epoch: 6| Step: 1
Training loss: 1.7475459575653076
Validation loss: 2.0776944596280336

Epoch: 6| Step: 2
Training loss: 1.5407657623291016
Validation loss: 2.0711714913768153

Epoch: 6| Step: 3
Training loss: 1.625173807144165
Validation loss: 2.075296537850493

Epoch: 6| Step: 4
Training loss: 2.133199691772461
Validation loss: 2.0869340255696285

Epoch: 6| Step: 5
Training loss: 2.1603174209594727
Validation loss: 2.098584823710944

Epoch: 6| Step: 6
Training loss: 1.9787973165512085
Validation loss: 2.0958458198014127

Epoch: 6| Step: 7
Training loss: 1.824148416519165
Validation loss: 2.09052707815683

Epoch: 6| Step: 8
Training loss: 1.2529783248901367
Validation loss: 2.074194274922853

Epoch: 6| Step: 9
Training loss: 1.7073774337768555
Validation loss: 2.10949190714026

Epoch: 6| Step: 10
Training loss: 2.3521101474761963
Validation loss: 2.1076121753261936

Epoch: 6| Step: 11
Training loss: 2.1044931411743164
Validation loss: 2.1366348471692813

Epoch: 6| Step: 12
Training loss: 1.8837157487869263
Validation loss: 2.154339084061243

Epoch: 6| Step: 13
Training loss: 1.004766583442688
Validation loss: 2.1579446972057386

Epoch: 454| Step: 0
Training loss: 1.4903998374938965
Validation loss: 2.1676913794650825

Epoch: 6| Step: 1
Training loss: 1.818193793296814
Validation loss: 2.1666071748220794

Epoch: 6| Step: 2
Training loss: 1.92557954788208
Validation loss: 2.145085870578725

Epoch: 6| Step: 3
Training loss: 1.876865267753601
Validation loss: 2.1324424589833906

Epoch: 6| Step: 4
Training loss: 2.1919405460357666
Validation loss: 2.112372462467481

Epoch: 6| Step: 5
Training loss: 2.3174734115600586
Validation loss: 2.0963771753413702

Epoch: 6| Step: 6
Training loss: 1.6967097520828247
Validation loss: 2.0880472583155476

Epoch: 6| Step: 7
Training loss: 1.4819128513336182
Validation loss: 2.0628333694191388

Epoch: 6| Step: 8
Training loss: 1.9647341966629028
Validation loss: 2.086528516584827

Epoch: 6| Step: 9
Training loss: 1.5511949062347412
Validation loss: 2.0670811899246706

Epoch: 6| Step: 10
Training loss: 1.6751430034637451
Validation loss: 2.0812561550448017

Epoch: 6| Step: 11
Training loss: 2.452582359313965
Validation loss: 2.109738293514457

Epoch: 6| Step: 12
Training loss: 1.947306752204895
Validation loss: 2.124262317534416

Epoch: 6| Step: 13
Training loss: 0.9408261775970459
Validation loss: 2.1247149693068637

Epoch: 455| Step: 0
Training loss: 1.9170423746109009
Validation loss: 2.145507538190452

Epoch: 6| Step: 1
Training loss: 1.5794899463653564
Validation loss: 2.14836472593328

Epoch: 6| Step: 2
Training loss: 1.4774945974349976
Validation loss: 2.1287683492065756

Epoch: 6| Step: 3
Training loss: 2.098749876022339
Validation loss: 2.124147933016541

Epoch: 6| Step: 4
Training loss: 1.7504093647003174
Validation loss: 2.110283079967704

Epoch: 6| Step: 5
Training loss: 1.8231000900268555
Validation loss: 2.0940529146502094

Epoch: 6| Step: 6
Training loss: 2.2003772258758545
Validation loss: 2.052573788550592

Epoch: 6| Step: 7
Training loss: 1.6773664951324463
Validation loss: 2.0649799890415643

Epoch: 6| Step: 8
Training loss: 1.3029303550720215
Validation loss: 2.055361804141793

Epoch: 6| Step: 9
Training loss: 1.7679669857025146
Validation loss: 2.05610627512778

Epoch: 6| Step: 10
Training loss: 2.0049257278442383
Validation loss: 2.06392599433981

Epoch: 6| Step: 11
Training loss: 2.929910898208618
Validation loss: 2.064755824304396

Epoch: 6| Step: 12
Training loss: 1.4325425624847412
Validation loss: 2.071892494796425

Epoch: 6| Step: 13
Training loss: 1.790905237197876
Validation loss: 2.088613494749992

Epoch: 456| Step: 0
Training loss: 1.7416975498199463
Validation loss: 2.0940756361971617

Epoch: 6| Step: 1
Training loss: 2.299233913421631
Validation loss: 2.082555168418474

Epoch: 6| Step: 2
Training loss: 1.363976240158081
Validation loss: 2.120599890267977

Epoch: 6| Step: 3
Training loss: 2.1199183464050293
Validation loss: 2.108646638931767

Epoch: 6| Step: 4
Training loss: 2.465355634689331
Validation loss: 2.1247884124837895

Epoch: 6| Step: 5
Training loss: 1.4710568189620972
Validation loss: 2.142923016701975

Epoch: 6| Step: 6
Training loss: 1.8735431432724
Validation loss: 2.1359072346841135

Epoch: 6| Step: 7
Training loss: 1.727349877357483
Validation loss: 2.13425697177969

Epoch: 6| Step: 8
Training loss: 1.6414154767990112
Validation loss: 2.1357014820139897

Epoch: 6| Step: 9
Training loss: 2.0327298641204834
Validation loss: 2.1620335014917518

Epoch: 6| Step: 10
Training loss: 1.9279987812042236
Validation loss: 2.1600646985474454

Epoch: 6| Step: 11
Training loss: 2.056922435760498
Validation loss: 2.12651288893915

Epoch: 6| Step: 12
Training loss: 1.4464912414550781
Validation loss: 2.095391345280473

Epoch: 6| Step: 13
Training loss: 0.6950724720954895
Validation loss: 2.1049996037637033

Epoch: 457| Step: 0
Training loss: 1.5979785919189453
Validation loss: 2.1040730527652207

Epoch: 6| Step: 1
Training loss: 1.6042892932891846
Validation loss: 2.1271891183750604

Epoch: 6| Step: 2
Training loss: 2.0886220932006836
Validation loss: 2.1064498885985343

Epoch: 6| Step: 3
Training loss: 2.0098180770874023
Validation loss: 2.1138386521288144

Epoch: 6| Step: 4
Training loss: 2.117372512817383
Validation loss: 2.1119601931623233

Epoch: 6| Step: 5
Training loss: 2.241630792617798
Validation loss: 2.103852779634537

Epoch: 6| Step: 6
Training loss: 1.9190540313720703
Validation loss: 2.1026566284959034

Epoch: 6| Step: 7
Training loss: 1.5109599828720093
Validation loss: 2.1258948259456183

Epoch: 6| Step: 8
Training loss: 1.9647812843322754
Validation loss: 2.090307248535977

Epoch: 6| Step: 9
Training loss: 2.403140068054199
Validation loss: 2.1239953874259867

Epoch: 6| Step: 10
Training loss: 1.8326715230941772
Validation loss: 2.111274334692186

Epoch: 6| Step: 11
Training loss: 1.0088690519332886
Validation loss: 2.089623310232675

Epoch: 6| Step: 12
Training loss: 1.443763256072998
Validation loss: 2.11301786668839

Epoch: 6| Step: 13
Training loss: 1.737992286682129
Validation loss: 2.1065073987489105

Epoch: 458| Step: 0
Training loss: 1.593886137008667
Validation loss: 2.1108633036254556

Epoch: 6| Step: 1
Training loss: 1.6431918144226074
Validation loss: 2.1385448286610265

Epoch: 6| Step: 2
Training loss: 1.5398286581039429
Validation loss: 2.1212934345327397

Epoch: 6| Step: 3
Training loss: 1.5241260528564453
Validation loss: 2.1173780861721245

Epoch: 6| Step: 4
Training loss: 1.9603192806243896
Validation loss: 2.1351289287690194

Epoch: 6| Step: 5
Training loss: 1.4871459007263184
Validation loss: 2.112703231073195

Epoch: 6| Step: 6
Training loss: 1.8846924304962158
Validation loss: 2.078589577828684

Epoch: 6| Step: 7
Training loss: 1.7315269708633423
Validation loss: 2.094092115279167

Epoch: 6| Step: 8
Training loss: 1.770927906036377
Validation loss: 2.0740673593295518

Epoch: 6| Step: 9
Training loss: 2.629488945007324
Validation loss: 2.069811151873681

Epoch: 6| Step: 10
Training loss: 1.6500229835510254
Validation loss: 2.0597749602410103

Epoch: 6| Step: 11
Training loss: 2.0640206336975098
Validation loss: 2.085114041964213

Epoch: 6| Step: 12
Training loss: 2.1367008686065674
Validation loss: 2.0737732841122534

Epoch: 6| Step: 13
Training loss: 1.9024721384048462
Validation loss: 2.0729018718965593

Epoch: 459| Step: 0
Training loss: 2.0906848907470703
Validation loss: 2.080439547056793

Epoch: 6| Step: 1
Training loss: 1.5155818462371826
Validation loss: 2.1175780937235844

Epoch: 6| Step: 2
Training loss: 1.5264079570770264
Validation loss: 2.1070518186015468

Epoch: 6| Step: 3
Training loss: 1.3887602090835571
Validation loss: 2.112530428876159

Epoch: 6| Step: 4
Training loss: 2.0696561336517334
Validation loss: 2.12340848804802

Epoch: 6| Step: 5
Training loss: 1.990535855293274
Validation loss: 2.104242799102619

Epoch: 6| Step: 6
Training loss: 1.8080034255981445
Validation loss: 2.1172906967901413

Epoch: 6| Step: 7
Training loss: 1.884892225265503
Validation loss: 2.108680329015178

Epoch: 6| Step: 8
Training loss: 2.0268969535827637
Validation loss: 2.0958113003802556

Epoch: 6| Step: 9
Training loss: 2.170644998550415
Validation loss: 2.0826815200108353

Epoch: 6| Step: 10
Training loss: 1.6030564308166504
Validation loss: 2.080771259082261

Epoch: 6| Step: 11
Training loss: 0.9363201856613159
Validation loss: 2.078905487573275

Epoch: 6| Step: 12
Training loss: 2.5357184410095215
Validation loss: 2.0852242003205004

Epoch: 6| Step: 13
Training loss: 1.582695484161377
Validation loss: 2.0851805210113525

Epoch: 460| Step: 0
Training loss: 1.9476985931396484
Validation loss: 2.108439471132012

Epoch: 6| Step: 1
Training loss: 1.779181718826294
Validation loss: 2.125826399813416

Epoch: 6| Step: 2
Training loss: 1.7633676528930664
Validation loss: 2.1090544731386247

Epoch: 6| Step: 3
Training loss: 1.3506019115447998
Validation loss: 2.1331291673003987

Epoch: 6| Step: 4
Training loss: 1.5040162801742554
Validation loss: 2.1301694223957677

Epoch: 6| Step: 5
Training loss: 2.340381622314453
Validation loss: 2.138894360552552

Epoch: 6| Step: 6
Training loss: 1.9705190658569336
Validation loss: 2.170252089859337

Epoch: 6| Step: 7
Training loss: 2.0466485023498535
Validation loss: 2.150913164179812

Epoch: 6| Step: 8
Training loss: 1.2471834421157837
Validation loss: 2.155338074571343

Epoch: 6| Step: 9
Training loss: 1.1760926246643066
Validation loss: 2.1295043306965984

Epoch: 6| Step: 10
Training loss: 2.2761123180389404
Validation loss: 2.1074592964623564

Epoch: 6| Step: 11
Training loss: 2.3052456378936768
Validation loss: 2.1012149805663736

Epoch: 6| Step: 12
Training loss: 1.7224721908569336
Validation loss: 2.0808993129320044

Epoch: 6| Step: 13
Training loss: 2.0317089557647705
Validation loss: 2.0734557772195465

Epoch: 461| Step: 0
Training loss: 1.4232569932937622
Validation loss: 2.0382824943911646

Epoch: 6| Step: 1
Training loss: 2.4401650428771973
Validation loss: 2.0678203721200266

Epoch: 6| Step: 2
Training loss: 2.0191335678100586
Validation loss: 2.0505446234057025

Epoch: 6| Step: 3
Training loss: 1.2093560695648193
Validation loss: 2.0678806817659767

Epoch: 6| Step: 4
Training loss: 1.8846192359924316
Validation loss: 2.0777721969030236

Epoch: 6| Step: 5
Training loss: 1.8649954795837402
Validation loss: 2.0953393033755723

Epoch: 6| Step: 6
Training loss: 1.4246537685394287
Validation loss: 2.1183098541793

Epoch: 6| Step: 7
Training loss: 2.003067970275879
Validation loss: 2.13289289833397

Epoch: 6| Step: 8
Training loss: 1.7166504859924316
Validation loss: 2.1383162339528403

Epoch: 6| Step: 9
Training loss: 1.8184514045715332
Validation loss: 2.1952553743957193

Epoch: 6| Step: 10
Training loss: 1.6349811553955078
Validation loss: 2.2158839779515422

Epoch: 6| Step: 11
Training loss: 1.9178575277328491
Validation loss: 2.243352723378007

Epoch: 6| Step: 12
Training loss: 2.057502508163452
Validation loss: 2.2479396071485294

Epoch: 6| Step: 13
Training loss: 2.1208133697509766
Validation loss: 2.2130652589182698

Epoch: 462| Step: 0
Training loss: 2.30316424369812
Validation loss: 2.183240636702507

Epoch: 6| Step: 1
Training loss: 1.2153656482696533
Validation loss: 2.1201827064637215

Epoch: 6| Step: 2
Training loss: 0.9654048681259155
Validation loss: 2.0891818218333746

Epoch: 6| Step: 3
Training loss: 1.9310994148254395
Validation loss: 2.090777007482385

Epoch: 6| Step: 4
Training loss: 1.4058573246002197
Validation loss: 2.0869692781920075

Epoch: 6| Step: 5
Training loss: 1.9399535655975342
Validation loss: 2.067448642946059

Epoch: 6| Step: 6
Training loss: 2.3329365253448486
Validation loss: 2.0814636420178156

Epoch: 6| Step: 7
Training loss: 2.4654994010925293
Validation loss: 2.0674598883557063

Epoch: 6| Step: 8
Training loss: 1.9429610967636108
Validation loss: 2.065284759767594

Epoch: 6| Step: 9
Training loss: 2.457522392272949
Validation loss: 2.0711871808575046

Epoch: 6| Step: 10
Training loss: 1.7406092882156372
Validation loss: 2.092999847986365

Epoch: 6| Step: 11
Training loss: 1.6908667087554932
Validation loss: 2.100341558456421

Epoch: 6| Step: 12
Training loss: 1.4305448532104492
Validation loss: 2.1194121222342215

Epoch: 6| Step: 13
Training loss: 1.4149659872055054
Validation loss: 2.1324407413441646

Epoch: 463| Step: 0
Training loss: 1.9003310203552246
Validation loss: 2.129023193031229

Epoch: 6| Step: 1
Training loss: 1.493545413017273
Validation loss: 2.160755776589917

Epoch: 6| Step: 2
Training loss: 2.0275983810424805
Validation loss: 2.122930480587867

Epoch: 6| Step: 3
Training loss: 2.576904535293579
Validation loss: 2.099733169360827

Epoch: 6| Step: 4
Training loss: 2.2842907905578613
Validation loss: 2.0813704741898404

Epoch: 6| Step: 5
Training loss: 1.7474287748336792
Validation loss: 2.0658030522766935

Epoch: 6| Step: 6
Training loss: 1.6140525341033936
Validation loss: 2.057055498964043

Epoch: 6| Step: 7
Training loss: 1.6876429319381714
Validation loss: 2.0633829742349605

Epoch: 6| Step: 8
Training loss: 1.5557606220245361
Validation loss: 2.053620722986037

Epoch: 6| Step: 9
Training loss: 1.7513129711151123
Validation loss: 2.049551135750227

Epoch: 6| Step: 10
Training loss: 2.0919480323791504
Validation loss: 2.036111283045943

Epoch: 6| Step: 11
Training loss: 1.0745320320129395
Validation loss: 2.0451154324316208

Epoch: 6| Step: 12
Training loss: 1.7748483419418335
Validation loss: 2.076063524010361

Epoch: 6| Step: 13
Training loss: 1.8740005493164062
Validation loss: 2.0847022725689794

Epoch: 464| Step: 0
Training loss: 1.1625559329986572
Validation loss: 2.089313817280595

Epoch: 6| Step: 1
Training loss: 1.5646919012069702
Validation loss: 2.116356254905783

Epoch: 6| Step: 2
Training loss: 2.0295727252960205
Validation loss: 2.1085693195301998

Epoch: 6| Step: 3
Training loss: 1.549649953842163
Validation loss: 2.126286422052691

Epoch: 6| Step: 4
Training loss: 1.9529688358306885
Validation loss: 2.15611534221198

Epoch: 6| Step: 5
Training loss: 2.4028701782226562
Validation loss: 2.1524118095315914

Epoch: 6| Step: 6
Training loss: 1.7174378633499146
Validation loss: 2.1478024708327426

Epoch: 6| Step: 7
Training loss: 1.4105651378631592
Validation loss: 2.1575844492963565

Epoch: 6| Step: 8
Training loss: 2.186879873275757
Validation loss: 2.143142077230638

Epoch: 6| Step: 9
Training loss: 1.8762221336364746
Validation loss: 2.1327259796921925

Epoch: 6| Step: 10
Training loss: 1.4297027587890625
Validation loss: 2.142539704999616

Epoch: 6| Step: 11
Training loss: 1.5591506958007812
Validation loss: 2.1325914987953762

Epoch: 6| Step: 12
Training loss: 2.2187604904174805
Validation loss: 2.12619492443659

Epoch: 6| Step: 13
Training loss: 2.122866153717041
Validation loss: 2.1042705710216234

Epoch: 465| Step: 0
Training loss: 1.540792465209961
Validation loss: 2.0999442813217

Epoch: 6| Step: 1
Training loss: 1.4976574182510376
Validation loss: 2.0692268674091627

Epoch: 6| Step: 2
Training loss: 2.230128288269043
Validation loss: 2.061264145758844

Epoch: 6| Step: 3
Training loss: 1.8753734827041626
Validation loss: 2.0570343617470033

Epoch: 6| Step: 4
Training loss: 1.6720969676971436
Validation loss: 2.0640150372700026

Epoch: 6| Step: 5
Training loss: 2.463301181793213
Validation loss: 2.0559764087841077

Epoch: 6| Step: 6
Training loss: 1.9610815048217773
Validation loss: 2.0403668265188895

Epoch: 6| Step: 7
Training loss: 2.3629884719848633
Validation loss: 2.0584430002397105

Epoch: 6| Step: 8
Training loss: 2.033360481262207
Validation loss: 2.0753372202637377

Epoch: 6| Step: 9
Training loss: 1.7305638790130615
Validation loss: 2.075149961697158

Epoch: 6| Step: 10
Training loss: 1.3779308795928955
Validation loss: 2.074483225422521

Epoch: 6| Step: 11
Training loss: 1.2115836143493652
Validation loss: 2.111655048144761

Epoch: 6| Step: 12
Training loss: 1.5623650550842285
Validation loss: 2.1213410836394115

Epoch: 6| Step: 13
Training loss: 1.5967572927474976
Validation loss: 2.129050257385418

Epoch: 466| Step: 0
Training loss: 1.8022420406341553
Validation loss: 2.1568308568769887

Epoch: 6| Step: 1
Training loss: 2.0998313426971436
Validation loss: 2.116107371545607

Epoch: 6| Step: 2
Training loss: 1.9501965045928955
Validation loss: 2.0988901469015304

Epoch: 6| Step: 3
Training loss: 1.760601282119751
Validation loss: 2.107763418587305

Epoch: 6| Step: 4
Training loss: 1.976097583770752
Validation loss: 2.1062757186992194

Epoch: 6| Step: 5
Training loss: 2.3558459281921387
Validation loss: 2.093770933407609

Epoch: 6| Step: 6
Training loss: 1.8153507709503174
Validation loss: 2.084439553240294

Epoch: 6| Step: 7
Training loss: 1.4908119440078735
Validation loss: 2.108084653013496

Epoch: 6| Step: 8
Training loss: 1.4759708642959595
Validation loss: 2.1230376997301654

Epoch: 6| Step: 9
Training loss: 1.6356614828109741
Validation loss: 2.1117833532312864

Epoch: 6| Step: 10
Training loss: 1.7986135482788086
Validation loss: 2.0996613348684003

Epoch: 6| Step: 11
Training loss: 1.5142221450805664
Validation loss: 2.1324274078492196

Epoch: 6| Step: 12
Training loss: 2.204280376434326
Validation loss: 2.119289387938797

Epoch: 6| Step: 13
Training loss: 0.5260761380195618
Validation loss: 2.1484188161870486

Epoch: 467| Step: 0
Training loss: 2.2171382904052734
Validation loss: 2.150956109005918

Epoch: 6| Step: 1
Training loss: 2.2156782150268555
Validation loss: 2.1638284716554868

Epoch: 6| Step: 2
Training loss: 1.5258840322494507
Validation loss: 2.1569862365722656

Epoch: 6| Step: 3
Training loss: 1.635368824005127
Validation loss: 2.1251574382987073

Epoch: 6| Step: 4
Training loss: 2.151754379272461
Validation loss: 2.1175906004444247

Epoch: 6| Step: 5
Training loss: 1.7783275842666626
Validation loss: 2.095493966533292

Epoch: 6| Step: 6
Training loss: 1.1787221431732178
Validation loss: 2.0975382968943608

Epoch: 6| Step: 7
Training loss: 1.4280222654342651
Validation loss: 2.0710612009930354

Epoch: 6| Step: 8
Training loss: 0.8748986721038818
Validation loss: 2.070304557841311

Epoch: 6| Step: 9
Training loss: 1.974033236503601
Validation loss: 2.064282709552396

Epoch: 6| Step: 10
Training loss: 2.1919803619384766
Validation loss: 2.0668186654326735

Epoch: 6| Step: 11
Training loss: 2.3017289638519287
Validation loss: 2.0703887170360935

Epoch: 6| Step: 12
Training loss: 1.7476660013198853
Validation loss: 2.090742713661604

Epoch: 6| Step: 13
Training loss: 1.650801658630371
Validation loss: 2.069051683589976

Epoch: 468| Step: 0
Training loss: 2.0661232471466064
Validation loss: 2.1018889450257823

Epoch: 6| Step: 1
Training loss: 1.5871258974075317
Validation loss: 2.1375942012315154

Epoch: 6| Step: 2
Training loss: 2.20139741897583
Validation loss: 2.157128967264647

Epoch: 6| Step: 3
Training loss: 1.5068416595458984
Validation loss: 2.1879668594688497

Epoch: 6| Step: 4
Training loss: 2.447868585586548
Validation loss: 2.185684786047987

Epoch: 6| Step: 5
Training loss: 0.9791717529296875
Validation loss: 2.1483810486332064

Epoch: 6| Step: 6
Training loss: 1.9379057884216309
Validation loss: 2.1042032267457698

Epoch: 6| Step: 7
Training loss: 1.718168020248413
Validation loss: 2.0810795958324144

Epoch: 6| Step: 8
Training loss: 1.23299241065979
Validation loss: 2.081428991850986

Epoch: 6| Step: 9
Training loss: 1.7634567022323608
Validation loss: 2.0892834073753765

Epoch: 6| Step: 10
Training loss: 1.874056339263916
Validation loss: 2.063130768396521

Epoch: 6| Step: 11
Training loss: 1.7032556533813477
Validation loss: 2.0738712818391862

Epoch: 6| Step: 12
Training loss: 1.699728012084961
Validation loss: 2.0665643330543273

Epoch: 6| Step: 13
Training loss: 2.6814889907836914
Validation loss: 2.0861451830915225

Epoch: 469| Step: 0
Training loss: 1.093602180480957
Validation loss: 2.096151436528852

Epoch: 6| Step: 1
Training loss: 2.006402015686035
Validation loss: 2.0880044570533176

Epoch: 6| Step: 2
Training loss: 1.6514487266540527
Validation loss: 2.1210705105976393

Epoch: 6| Step: 3
Training loss: 1.4952213764190674
Validation loss: 2.1549307146380023

Epoch: 6| Step: 4
Training loss: 1.9732155799865723
Validation loss: 2.145678139502002

Epoch: 6| Step: 5
Training loss: 1.7878015041351318
Validation loss: 2.153269496015323

Epoch: 6| Step: 6
Training loss: 0.9616104960441589
Validation loss: 2.1572045344178394

Epoch: 6| Step: 7
Training loss: 2.0757405757904053
Validation loss: 2.155182155229712

Epoch: 6| Step: 8
Training loss: 2.1904397010803223
Validation loss: 2.1270143447383756

Epoch: 6| Step: 9
Training loss: 1.5808987617492676
Validation loss: 2.1482163834315475

Epoch: 6| Step: 10
Training loss: 1.876259684562683
Validation loss: 2.1154296564799484

Epoch: 6| Step: 11
Training loss: 2.076113700866699
Validation loss: 2.090488810693064

Epoch: 6| Step: 12
Training loss: 2.153451919555664
Validation loss: 2.086178269437564

Epoch: 6| Step: 13
Training loss: 2.12544584274292
Validation loss: 2.0569857653751167

Epoch: 470| Step: 0
Training loss: 1.566699743270874
Validation loss: 2.0518807185593473

Epoch: 6| Step: 1
Training loss: 2.0610604286193848
Validation loss: 2.034768250680739

Epoch: 6| Step: 2
Training loss: 1.4293601512908936
Validation loss: 2.036257727171785

Epoch: 6| Step: 3
Training loss: 2.1858131885528564
Validation loss: 2.029900666206114

Epoch: 6| Step: 4
Training loss: 2.169097423553467
Validation loss: 2.028927469766268

Epoch: 6| Step: 5
Training loss: 1.4991371631622314
Validation loss: 2.016652176457067

Epoch: 6| Step: 6
Training loss: 1.4701762199401855
Validation loss: 2.0390884158431843

Epoch: 6| Step: 7
Training loss: 1.2530906200408936
Validation loss: 2.074297910095543

Epoch: 6| Step: 8
Training loss: 1.9610891342163086
Validation loss: 2.075878231756149

Epoch: 6| Step: 9
Training loss: 1.923962116241455
Validation loss: 2.1301154910877185

Epoch: 6| Step: 10
Training loss: 1.5537878274917603
Validation loss: 2.171272700832736

Epoch: 6| Step: 11
Training loss: 1.8761907815933228
Validation loss: 2.1890794820682977

Epoch: 6| Step: 12
Training loss: 1.5441877841949463
Validation loss: 2.173722549151349

Epoch: 6| Step: 13
Training loss: 2.9030966758728027
Validation loss: 2.165312044082149

Epoch: 471| Step: 0
Training loss: 1.4822051525115967
Validation loss: 2.1779715091951433

Epoch: 6| Step: 1
Training loss: 2.111459255218506
Validation loss: 2.184261348939711

Epoch: 6| Step: 2
Training loss: 1.9852687120437622
Validation loss: 2.178808384044196

Epoch: 6| Step: 3
Training loss: 1.334591269493103
Validation loss: 2.151952182092974

Epoch: 6| Step: 4
Training loss: 1.9350303411483765
Validation loss: 2.141608270265723

Epoch: 6| Step: 5
Training loss: 2.5792198181152344
Validation loss: 2.107880307782081

Epoch: 6| Step: 6
Training loss: 1.3536763191223145
Validation loss: 2.0994251774203394

Epoch: 6| Step: 7
Training loss: 1.8002533912658691
Validation loss: 2.098271941625944

Epoch: 6| Step: 8
Training loss: 1.351892113685608
Validation loss: 2.0796852060543594

Epoch: 6| Step: 9
Training loss: 2.5147736072540283
Validation loss: 2.085521453170366

Epoch: 6| Step: 10
Training loss: 2.3676528930664062
Validation loss: 2.1043240665107645

Epoch: 6| Step: 11
Training loss: 0.985014796257019
Validation loss: 2.0852652185706684

Epoch: 6| Step: 12
Training loss: 1.3313822746276855
Validation loss: 2.0857245281178463

Epoch: 6| Step: 13
Training loss: 1.5026065111160278
Validation loss: 2.0795535015803512

Epoch: 472| Step: 0
Training loss: 1.9755098819732666
Validation loss: 2.0915185815544537

Epoch: 6| Step: 1
Training loss: 2.1722960472106934
Validation loss: 2.0819528102874756

Epoch: 6| Step: 2
Training loss: 1.4917794466018677
Validation loss: 2.100963495110953

Epoch: 6| Step: 3
Training loss: 1.2855474948883057
Validation loss: 2.1035700216088244

Epoch: 6| Step: 4
Training loss: 1.3715136051177979
Validation loss: 2.0985413674385316

Epoch: 6| Step: 5
Training loss: 2.8568053245544434
Validation loss: 2.0901076075851277

Epoch: 6| Step: 6
Training loss: 2.4867398738861084
Validation loss: 2.134679727656867

Epoch: 6| Step: 7
Training loss: 1.7610645294189453
Validation loss: 2.1371804206602034

Epoch: 6| Step: 8
Training loss: 1.6283562183380127
Validation loss: 2.1482033985917286

Epoch: 6| Step: 9
Training loss: 1.5624299049377441
Validation loss: 2.111086268578806

Epoch: 6| Step: 10
Training loss: 1.5722589492797852
Validation loss: 2.112595440239035

Epoch: 6| Step: 11
Training loss: 1.2673583030700684
Validation loss: 2.107329301936652

Epoch: 6| Step: 12
Training loss: 1.8868186473846436
Validation loss: 2.0902005933946177

Epoch: 6| Step: 13
Training loss: 1.09120774269104
Validation loss: 2.0720926882118307

Epoch: 473| Step: 0
Training loss: 1.9377418756484985
Validation loss: 2.0774550643018497

Epoch: 6| Step: 1
Training loss: 2.1078810691833496
Validation loss: 2.0751662228697088

Epoch: 6| Step: 2
Training loss: 1.6611342430114746
Validation loss: 2.0861646718876337

Epoch: 6| Step: 3
Training loss: 1.2907023429870605
Validation loss: 2.0817172617040653

Epoch: 6| Step: 4
Training loss: 1.590165376663208
Validation loss: 2.092534008846488

Epoch: 6| Step: 5
Training loss: 1.7433619499206543
Validation loss: 2.104967645419541

Epoch: 6| Step: 6
Training loss: 1.6646116971969604
Validation loss: 2.0847183606957875

Epoch: 6| Step: 7
Training loss: 2.042722225189209
Validation loss: 2.1157785179794475

Epoch: 6| Step: 8
Training loss: 1.4360153675079346
Validation loss: 2.123240488831715

Epoch: 6| Step: 9
Training loss: 2.0933947563171387
Validation loss: 2.127328598371116

Epoch: 6| Step: 10
Training loss: 1.907155156135559
Validation loss: 2.1286451855013446

Epoch: 6| Step: 11
Training loss: 1.8411329984664917
Validation loss: 2.1213219986167005

Epoch: 6| Step: 12
Training loss: 1.1538505554199219
Validation loss: 2.1226930592649724

Epoch: 6| Step: 13
Training loss: 2.1811869144439697
Validation loss: 2.1115804487659084

Epoch: 474| Step: 0
Training loss: 2.2840006351470947
Validation loss: 2.0759714418841946

Epoch: 6| Step: 1
Training loss: 1.999691128730774
Validation loss: 2.1088408424008276

Epoch: 6| Step: 2
Training loss: 2.0217387676239014
Validation loss: 2.124470273653666

Epoch: 6| Step: 3
Training loss: 1.6134017705917358
Validation loss: 2.1065405350859447

Epoch: 6| Step: 4
Training loss: 0.7807354927062988
Validation loss: 2.1228814125061035

Epoch: 6| Step: 5
Training loss: 1.5859146118164062
Validation loss: 2.0941493383017917

Epoch: 6| Step: 6
Training loss: 2.203275203704834
Validation loss: 2.086358945856812

Epoch: 6| Step: 7
Training loss: 1.7327717542648315
Validation loss: 2.075668009378577

Epoch: 6| Step: 8
Training loss: 1.5860373973846436
Validation loss: 2.062377247759091

Epoch: 6| Step: 9
Training loss: 1.511777400970459
Validation loss: 2.041597135605351

Epoch: 6| Step: 10
Training loss: 2.1139230728149414
Validation loss: 2.0523621600161315

Epoch: 6| Step: 11
Training loss: 2.2377772331237793
Validation loss: 2.038111927688763

Epoch: 6| Step: 12
Training loss: 1.4789409637451172
Validation loss: 2.044057753778273

Epoch: 6| Step: 13
Training loss: 1.3108768463134766
Validation loss: 2.0646184182936147

Epoch: 475| Step: 0
Training loss: 1.4274177551269531
Validation loss: 2.098457608171689

Epoch: 6| Step: 1
Training loss: 1.0018537044525146
Validation loss: 2.1308823349655315

Epoch: 6| Step: 2
Training loss: 2.0018386840820312
Validation loss: 2.1406477087287494

Epoch: 6| Step: 3
Training loss: 1.6364195346832275
Validation loss: 2.1323018356036116

Epoch: 6| Step: 4
Training loss: 2.413266181945801
Validation loss: 2.1778702761537287

Epoch: 6| Step: 5
Training loss: 2.1257283687591553
Validation loss: 2.1466014077586513

Epoch: 6| Step: 6
Training loss: 1.899067759513855
Validation loss: 2.126277360864865

Epoch: 6| Step: 7
Training loss: 2.556497573852539
Validation loss: 2.1202117127756916

Epoch: 6| Step: 8
Training loss: 1.4050016403198242
Validation loss: 2.1083209335163073

Epoch: 6| Step: 9
Training loss: 1.4264037609100342
Validation loss: 2.0773744839493946

Epoch: 6| Step: 10
Training loss: 1.3815569877624512
Validation loss: 2.075961189885293

Epoch: 6| Step: 11
Training loss: 1.6002190113067627
Validation loss: 2.0842723705435313

Epoch: 6| Step: 12
Training loss: 1.3948438167572021
Validation loss: 2.0743010710644465

Epoch: 6| Step: 13
Training loss: 2.4374895095825195
Validation loss: 2.0853969640629266

Epoch: 476| Step: 0
Training loss: 1.369055986404419
Validation loss: 2.070595282380299

Epoch: 6| Step: 1
Training loss: 2.047229766845703
Validation loss: 2.0821642234761226

Epoch: 6| Step: 2
Training loss: 1.065422534942627
Validation loss: 2.0593180810251543

Epoch: 6| Step: 3
Training loss: 1.9046189785003662
Validation loss: 2.0644604031757643

Epoch: 6| Step: 4
Training loss: 1.5957353115081787
Validation loss: 2.0771394186122443

Epoch: 6| Step: 5
Training loss: 1.3983691930770874
Validation loss: 2.095117417714929

Epoch: 6| Step: 6
Training loss: 1.3923561573028564
Validation loss: 2.087750110574948

Epoch: 6| Step: 7
Training loss: 1.5005476474761963
Validation loss: 2.085870376197241

Epoch: 6| Step: 8
Training loss: 2.2246506214141846
Validation loss: 2.103602700336005

Epoch: 6| Step: 9
Training loss: 2.092879295349121
Validation loss: 2.1332611935113066

Epoch: 6| Step: 10
Training loss: 2.81523060798645
Validation loss: 2.113123583537276

Epoch: 6| Step: 11
Training loss: 1.7324973344802856
Validation loss: 2.1137037969404653

Epoch: 6| Step: 12
Training loss: 1.631321668624878
Validation loss: 2.1450714398455877

Epoch: 6| Step: 13
Training loss: 1.3799681663513184
Validation loss: 2.1401684873847553

Epoch: 477| Step: 0
Training loss: 2.055812358856201
Validation loss: 2.114958865668184

Epoch: 6| Step: 1
Training loss: 1.7218269109725952
Validation loss: 2.0908358161167433

Epoch: 6| Step: 2
Training loss: 2.335777759552002
Validation loss: 2.0737363112870084

Epoch: 6| Step: 3
Training loss: 1.643370509147644
Validation loss: 2.0464259885972544

Epoch: 6| Step: 4
Training loss: 2.0098061561584473
Validation loss: 2.044506034543437

Epoch: 6| Step: 5
Training loss: 1.772097110748291
Validation loss: 2.044651322467353

Epoch: 6| Step: 6
Training loss: 1.2544589042663574
Validation loss: 2.0590002831592353

Epoch: 6| Step: 7
Training loss: 2.1135950088500977
Validation loss: 2.084020212132444

Epoch: 6| Step: 8
Training loss: 1.6943273544311523
Validation loss: 2.117509695791429

Epoch: 6| Step: 9
Training loss: 1.7851219177246094
Validation loss: 2.139809823805286

Epoch: 6| Step: 10
Training loss: 1.9508684873580933
Validation loss: 2.1638560795014903

Epoch: 6| Step: 11
Training loss: 1.0830271244049072
Validation loss: 2.1637165520780828

Epoch: 6| Step: 12
Training loss: 1.6136198043823242
Validation loss: 2.1617624657128447

Epoch: 6| Step: 13
Training loss: 1.1718626022338867
Validation loss: 2.1541871511808006

Epoch: 478| Step: 0
Training loss: 1.346798300743103
Validation loss: 2.1130814654852754

Epoch: 6| Step: 1
Training loss: 1.85442054271698
Validation loss: 2.0960293687799925

Epoch: 6| Step: 2
Training loss: 1.8494415283203125
Validation loss: 2.06266132093245

Epoch: 6| Step: 3
Training loss: 1.8232712745666504
Validation loss: 2.0621176470992384

Epoch: 6| Step: 4
Training loss: 2.3663477897644043
Validation loss: 2.0647567754150717

Epoch: 6| Step: 5
Training loss: 1.765663981437683
Validation loss: 2.067730616497737

Epoch: 6| Step: 6
Training loss: 1.9294524192810059
Validation loss: 2.0980937301471667

Epoch: 6| Step: 7
Training loss: 1.3520010709762573
Validation loss: 2.0895453063390588

Epoch: 6| Step: 8
Training loss: 1.4280450344085693
Validation loss: 2.10183810675016

Epoch: 6| Step: 9
Training loss: 1.437613606452942
Validation loss: 2.1097086706469135

Epoch: 6| Step: 10
Training loss: 2.092461109161377
Validation loss: 2.090877285567663

Epoch: 6| Step: 11
Training loss: 1.4179267883300781
Validation loss: 2.0657977365678355

Epoch: 6| Step: 12
Training loss: 1.562530755996704
Validation loss: 2.085694605304349

Epoch: 6| Step: 13
Training loss: 2.5009098052978516
Validation loss: 2.072139511826218

Epoch: 479| Step: 0
Training loss: 2.290895462036133
Validation loss: 2.0910047151709117

Epoch: 6| Step: 1
Training loss: 1.7667572498321533
Validation loss: 2.0834518735126784

Epoch: 6| Step: 2
Training loss: 2.001800537109375
Validation loss: 2.1027602611049527

Epoch: 6| Step: 3
Training loss: 1.1935467720031738
Validation loss: 2.077436647107524

Epoch: 6| Step: 4
Training loss: 2.3628334999084473
Validation loss: 2.0967726938186155

Epoch: 6| Step: 5
Training loss: 1.9519708156585693
Validation loss: 2.108362688813158

Epoch: 6| Step: 6
Training loss: 2.0327227115631104
Validation loss: 2.100577139085339

Epoch: 6| Step: 7
Training loss: 1.1504242420196533
Validation loss: 2.135909959834109

Epoch: 6| Step: 8
Training loss: 1.2945339679718018
Validation loss: 2.096406041934926

Epoch: 6| Step: 9
Training loss: 1.9113479852676392
Validation loss: 2.112303569752683

Epoch: 6| Step: 10
Training loss: 0.9673020243644714
Validation loss: 2.1068980065725182

Epoch: 6| Step: 11
Training loss: 1.2956116199493408
Validation loss: 2.106874862024861

Epoch: 6| Step: 12
Training loss: 2.2315785884857178
Validation loss: 2.082197295722141

Epoch: 6| Step: 13
Training loss: 1.5413763523101807
Validation loss: 2.091102183506053

Epoch: 480| Step: 0
Training loss: 1.4063575267791748
Validation loss: 2.088991900925995

Epoch: 6| Step: 1
Training loss: 1.7773761749267578
Validation loss: 2.086970886876506

Epoch: 6| Step: 2
Training loss: 1.3235588073730469
Validation loss: 2.1080202799971386

Epoch: 6| Step: 3
Training loss: 1.6818093061447144
Validation loss: 2.111632075361026

Epoch: 6| Step: 4
Training loss: 1.9508967399597168
Validation loss: 2.097374418730377

Epoch: 6| Step: 5
Training loss: 1.0336703062057495
Validation loss: 2.0917101085826917

Epoch: 6| Step: 6
Training loss: 1.6933012008666992
Validation loss: 2.112360178783376

Epoch: 6| Step: 7
Training loss: 1.5373424291610718
Validation loss: 2.084683543892317

Epoch: 6| Step: 8
Training loss: 1.7714550495147705
Validation loss: 2.118545437371859

Epoch: 6| Step: 9
Training loss: 2.470694065093994
Validation loss: 2.0928151992059525

Epoch: 6| Step: 10
Training loss: 1.96525239944458
Validation loss: 2.0926703817100933

Epoch: 6| Step: 11
Training loss: 2.217498540878296
Validation loss: 2.1029883738487

Epoch: 6| Step: 12
Training loss: 1.4803569316864014
Validation loss: 2.078162218934746

Epoch: 6| Step: 13
Training loss: 1.7322046756744385
Validation loss: 2.075512506628549

Epoch: 481| Step: 0
Training loss: 1.5407688617706299
Validation loss: 2.0650717237944245

Epoch: 6| Step: 1
Training loss: 1.6028746366500854
Validation loss: 2.0644825325217298

Epoch: 6| Step: 2
Training loss: 1.7587615251541138
Validation loss: 2.066278206404819

Epoch: 6| Step: 3
Training loss: 2.173198938369751
Validation loss: 2.0799767227583033

Epoch: 6| Step: 4
Training loss: 1.497861385345459
Validation loss: 2.0770201683044434

Epoch: 6| Step: 5
Training loss: 1.841674566268921
Validation loss: 2.0852031041217107

Epoch: 6| Step: 6
Training loss: 1.8980076313018799
Validation loss: 2.079671723868257

Epoch: 6| Step: 7
Training loss: 1.8363137245178223
Validation loss: 2.0862302139241207

Epoch: 6| Step: 8
Training loss: 0.892989993095398
Validation loss: 2.0879219026975733

Epoch: 6| Step: 9
Training loss: 1.7683440446853638
Validation loss: 2.104635670620908

Epoch: 6| Step: 10
Training loss: 1.639647364616394
Validation loss: 2.1253873186726726

Epoch: 6| Step: 11
Training loss: 1.924992561340332
Validation loss: 2.1103017958261634

Epoch: 6| Step: 12
Training loss: 2.135125160217285
Validation loss: 2.11192225897184

Epoch: 6| Step: 13
Training loss: 1.4665321111679077
Validation loss: 2.0719583777971167

Epoch: 482| Step: 0
Training loss: 2.1913976669311523
Validation loss: 2.0773732841655774

Epoch: 6| Step: 1
Training loss: 1.8633543252944946
Validation loss: 2.077366329008533

Epoch: 6| Step: 2
Training loss: 2.468717098236084
Validation loss: 2.046655321633944

Epoch: 6| Step: 3
Training loss: 1.6646599769592285
Validation loss: 2.0687831422334075

Epoch: 6| Step: 4
Training loss: 1.558156967163086
Validation loss: 2.0554389620339997

Epoch: 6| Step: 5
Training loss: 1.7197163105010986
Validation loss: 2.0773836797283542

Epoch: 6| Step: 6
Training loss: 1.3080594539642334
Validation loss: 2.05644319903466

Epoch: 6| Step: 7
Training loss: 1.5806636810302734
Validation loss: 2.0518367367406047

Epoch: 6| Step: 8
Training loss: 1.9748473167419434
Validation loss: 2.0886484884446666

Epoch: 6| Step: 9
Training loss: 1.4930410385131836
Validation loss: 2.1005388459851666

Epoch: 6| Step: 10
Training loss: 2.0252485275268555
Validation loss: 2.0809473478665916

Epoch: 6| Step: 11
Training loss: 1.3483638763427734
Validation loss: 2.12326039806489

Epoch: 6| Step: 12
Training loss: 1.0409462451934814
Validation loss: 2.1206894843809065

Epoch: 6| Step: 13
Training loss: 1.5247434377670288
Validation loss: 2.09769941401738

Epoch: 483| Step: 0
Training loss: 1.5486390590667725
Validation loss: 2.087795788241971

Epoch: 6| Step: 1
Training loss: 1.665714979171753
Validation loss: 2.118126566692065

Epoch: 6| Step: 2
Training loss: 1.5745877027511597
Validation loss: 2.1276736092823807

Epoch: 6| Step: 3
Training loss: 1.2261428833007812
Validation loss: 2.1331890526638237

Epoch: 6| Step: 4
Training loss: 2.43233585357666
Validation loss: 2.136262188675583

Epoch: 6| Step: 5
Training loss: 1.5999001264572144
Validation loss: 2.092228494664674

Epoch: 6| Step: 6
Training loss: 1.906404733657837
Validation loss: 2.0932489377196117

Epoch: 6| Step: 7
Training loss: 1.287163257598877
Validation loss: 2.1040134801659534

Epoch: 6| Step: 8
Training loss: 1.5511281490325928
Validation loss: 2.075179330764278

Epoch: 6| Step: 9
Training loss: 1.7453160285949707
Validation loss: 2.0917306741078696

Epoch: 6| Step: 10
Training loss: 1.418965458869934
Validation loss: 2.0785123276454147

Epoch: 6| Step: 11
Training loss: 2.496852159500122
Validation loss: 2.088178798716555

Epoch: 6| Step: 12
Training loss: 1.7706373929977417
Validation loss: 2.071046693350679

Epoch: 6| Step: 13
Training loss: 1.7955275774002075
Validation loss: 2.0685234415915703

Epoch: 484| Step: 0
Training loss: 1.6491920948028564
Validation loss: 2.0598057008558706

Epoch: 6| Step: 1
Training loss: 1.9079382419586182
Validation loss: 2.0546882050011748

Epoch: 6| Step: 2
Training loss: 1.4986274242401123
Validation loss: 2.0733177354258876

Epoch: 6| Step: 3
Training loss: 1.7774498462677002
Validation loss: 2.0781906343275502

Epoch: 6| Step: 4
Training loss: 2.3746771812438965
Validation loss: 2.086823918486154

Epoch: 6| Step: 5
Training loss: 2.0116422176361084
Validation loss: 2.097709401961296

Epoch: 6| Step: 6
Training loss: 1.7688109874725342
Validation loss: 2.0984554316407893

Epoch: 6| Step: 7
Training loss: 2.1539506912231445
Validation loss: 2.132741238481255

Epoch: 6| Step: 8
Training loss: 1.335654377937317
Validation loss: 2.1360972901826263

Epoch: 6| Step: 9
Training loss: 1.5751609802246094
Validation loss: 2.1811241924121814

Epoch: 6| Step: 10
Training loss: 1.780780553817749
Validation loss: 2.147358253437986

Epoch: 6| Step: 11
Training loss: 0.9817702770233154
Validation loss: 2.1445975765105216

Epoch: 6| Step: 12
Training loss: 1.723145604133606
Validation loss: 2.1083930666728685

Epoch: 6| Step: 13
Training loss: 1.114540696144104
Validation loss: 2.1070759091325986

Epoch: 485| Step: 0
Training loss: 1.7330842018127441
Validation loss: 2.093102780721521

Epoch: 6| Step: 1
Training loss: 1.464151382446289
Validation loss: 2.1009170342517156

Epoch: 6| Step: 2
Training loss: 1.7113666534423828
Validation loss: 2.1206966369382796

Epoch: 6| Step: 3
Training loss: 1.3781750202178955
Validation loss: 2.144453787034558

Epoch: 6| Step: 4
Training loss: 1.5249847173690796
Validation loss: 2.1291296084721885

Epoch: 6| Step: 5
Training loss: 1.891093373298645
Validation loss: 2.1197726982896046

Epoch: 6| Step: 6
Training loss: 1.7312358617782593
Validation loss: 2.154560717203284

Epoch: 6| Step: 7
Training loss: 1.635134220123291
Validation loss: 2.1153267250266126

Epoch: 6| Step: 8
Training loss: 1.75281822681427
Validation loss: 2.0685854983586136

Epoch: 6| Step: 9
Training loss: 1.9040607213974
Validation loss: 2.097734723039853

Epoch: 6| Step: 10
Training loss: 2.066023588180542
Validation loss: 2.061780564246639

Epoch: 6| Step: 11
Training loss: 1.3275071382522583
Validation loss: 2.0490606702784055

Epoch: 6| Step: 12
Training loss: 1.851611614227295
Validation loss: 2.057715630018583

Epoch: 6| Step: 13
Training loss: 1.9980634450912476
Validation loss: 2.0630573816196893

Epoch: 486| Step: 0
Training loss: 1.705686330795288
Validation loss: 2.0602937052326817

Epoch: 6| Step: 1
Training loss: 1.261725664138794
Validation loss: 2.059558165970669

Epoch: 6| Step: 2
Training loss: 2.0293898582458496
Validation loss: 2.0564470086046445

Epoch: 6| Step: 3
Training loss: 1.7255401611328125
Validation loss: 2.0787909261641966

Epoch: 6| Step: 4
Training loss: 1.8513195514678955
Validation loss: 2.0744793107432704

Epoch: 6| Step: 5
Training loss: 1.2847754955291748
Validation loss: 2.105573983602626

Epoch: 6| Step: 6
Training loss: 1.5824378728866577
Validation loss: 2.094618561447308

Epoch: 6| Step: 7
Training loss: 1.557384729385376
Validation loss: 2.128085192813668

Epoch: 6| Step: 8
Training loss: 1.9017510414123535
Validation loss: 2.161161276601976

Epoch: 6| Step: 9
Training loss: 1.8473681211471558
Validation loss: 2.1769659467922744

Epoch: 6| Step: 10
Training loss: 1.472216248512268
Validation loss: 2.157933109550066

Epoch: 6| Step: 11
Training loss: 1.8424017429351807
Validation loss: 2.136197372149396

Epoch: 6| Step: 12
Training loss: 1.804804801940918
Validation loss: 2.1109247938279183

Epoch: 6| Step: 13
Training loss: 1.9223448038101196
Validation loss: 2.0854903587730984

Epoch: 487| Step: 0
Training loss: 1.560039758682251
Validation loss: 2.104499911749235

Epoch: 6| Step: 1
Training loss: 1.658315658569336
Validation loss: 2.0894172627438783

Epoch: 6| Step: 2
Training loss: 1.7140557765960693
Validation loss: 2.0678524458280174

Epoch: 6| Step: 3
Training loss: 1.441906452178955
Validation loss: 2.109387723348474

Epoch: 6| Step: 4
Training loss: 2.1797869205474854
Validation loss: 2.0945562701071463

Epoch: 6| Step: 5
Training loss: 1.561409592628479
Validation loss: 2.0998949030394196

Epoch: 6| Step: 6
Training loss: 1.648308277130127
Validation loss: 2.1009194017738424

Epoch: 6| Step: 7
Training loss: 1.619936466217041
Validation loss: 2.10601834840672

Epoch: 6| Step: 8
Training loss: 1.5549042224884033
Validation loss: 2.11577271902433

Epoch: 6| Step: 9
Training loss: 1.6901427507400513
Validation loss: 2.115532159805298

Epoch: 6| Step: 10
Training loss: 1.7891201972961426
Validation loss: 2.105143936731482

Epoch: 6| Step: 11
Training loss: 1.6142059564590454
Validation loss: 2.1000148916757233

Epoch: 6| Step: 12
Training loss: 1.6761577129364014
Validation loss: 2.080563449090527

Epoch: 6| Step: 13
Training loss: 2.075078010559082
Validation loss: 2.0910070237293037

Epoch: 488| Step: 0
Training loss: 1.8981868028640747
Validation loss: 2.087444390020063

Epoch: 6| Step: 1
Training loss: 1.7996926307678223
Validation loss: 2.0791352000287784

Epoch: 6| Step: 2
Training loss: 1.662533164024353
Validation loss: 2.0896620314608336

Epoch: 6| Step: 3
Training loss: 2.209718704223633
Validation loss: 2.083324368282031

Epoch: 6| Step: 4
Training loss: 0.8656895160675049
Validation loss: 2.0810996601658482

Epoch: 6| Step: 5
Training loss: 1.6776490211486816
Validation loss: 2.0838091219625166

Epoch: 6| Step: 6
Training loss: 1.6388839483261108
Validation loss: 2.104604805669477

Epoch: 6| Step: 7
Training loss: 2.2323801517486572
Validation loss: 2.114951890002015

Epoch: 6| Step: 8
Training loss: 1.417991280555725
Validation loss: 2.1091833563261133

Epoch: 6| Step: 9
Training loss: 1.4005542993545532
Validation loss: 2.1275849624346663

Epoch: 6| Step: 10
Training loss: 1.810003638267517
Validation loss: 2.125389036311898

Epoch: 6| Step: 11
Training loss: 1.0153491497039795
Validation loss: 2.1403955054539505

Epoch: 6| Step: 12
Training loss: 2.090376377105713
Validation loss: 2.1073884297442693

Epoch: 6| Step: 13
Training loss: 1.7592885494232178
Validation loss: 2.1165257077063284

Epoch: 489| Step: 0
Training loss: 1.7482099533081055
Validation loss: 2.0944813861641833

Epoch: 6| Step: 1
Training loss: 1.7557134628295898
Validation loss: 2.085945026848906

Epoch: 6| Step: 2
Training loss: 1.4308098554611206
Validation loss: 2.080434751766984

Epoch: 6| Step: 3
Training loss: 1.7390289306640625
Validation loss: 2.0658857514781337

Epoch: 6| Step: 4
Training loss: 1.7126095294952393
Validation loss: 2.0568659408118135

Epoch: 6| Step: 5
Training loss: 1.7473679780960083
Validation loss: 2.0527296117556992

Epoch: 6| Step: 6
Training loss: 0.6854840517044067
Validation loss: 2.0582035715861986

Epoch: 6| Step: 7
Training loss: 1.4066648483276367
Validation loss: 2.0653491609839985

Epoch: 6| Step: 8
Training loss: 2.4803459644317627
Validation loss: 2.0839092193111295

Epoch: 6| Step: 9
Training loss: 2.1346068382263184
Validation loss: 2.0806700593681744

Epoch: 6| Step: 10
Training loss: 2.038302421569824
Validation loss: 2.0819211185619397

Epoch: 6| Step: 11
Training loss: 1.6918609142303467
Validation loss: 2.066410332597712

Epoch: 6| Step: 12
Training loss: 1.5948375463485718
Validation loss: 2.0554901733193347

Epoch: 6| Step: 13
Training loss: 1.1400104761123657
Validation loss: 2.051025705952798

Epoch: 490| Step: 0
Training loss: 2.095305919647217
Validation loss: 2.0806808343497654

Epoch: 6| Step: 1
Training loss: 1.8202687501907349
Validation loss: 2.1061725924091954

Epoch: 6| Step: 2
Training loss: 2.2076430320739746
Validation loss: 2.1089758488439743

Epoch: 6| Step: 3
Training loss: 1.5911140441894531
Validation loss: 2.130211568647815

Epoch: 6| Step: 4
Training loss: 1.612541675567627
Validation loss: 2.111840304508004

Epoch: 6| Step: 5
Training loss: 1.1223030090332031
Validation loss: 2.1462924095892135

Epoch: 6| Step: 6
Training loss: 2.1212501525878906
Validation loss: 2.1315184357345744

Epoch: 6| Step: 7
Training loss: 1.4519062042236328
Validation loss: 2.1493007354838873

Epoch: 6| Step: 8
Training loss: 1.7310891151428223
Validation loss: 2.133836641106554

Epoch: 6| Step: 9
Training loss: 1.532613754272461
Validation loss: 2.1026920682640484

Epoch: 6| Step: 10
Training loss: 1.717592716217041
Validation loss: 2.092544760755313

Epoch: 6| Step: 11
Training loss: 1.3792510032653809
Validation loss: 2.055053767337594

Epoch: 6| Step: 12
Training loss: 1.609795093536377
Validation loss: 2.0584249752824024

Epoch: 6| Step: 13
Training loss: 1.2670356035232544
Validation loss: 2.0491017821014568

Epoch: 491| Step: 0
Training loss: 1.7669174671173096
Validation loss: 2.0566760186226136

Epoch: 6| Step: 1
Training loss: 1.174614429473877
Validation loss: 2.053007951346777

Epoch: 6| Step: 2
Training loss: 1.6237409114837646
Validation loss: 2.058779767764512

Epoch: 6| Step: 3
Training loss: 1.353713035583496
Validation loss: 2.09093746062248

Epoch: 6| Step: 4
Training loss: 2.01472806930542
Validation loss: 2.1024329136776667

Epoch: 6| Step: 5
Training loss: 2.2236974239349365
Validation loss: 2.115026512453633

Epoch: 6| Step: 6
Training loss: 1.3546173572540283
Validation loss: 2.1339318290833504

Epoch: 6| Step: 7
Training loss: 1.4429746866226196
Validation loss: 2.13674073578209

Epoch: 6| Step: 8
Training loss: 1.941361427307129
Validation loss: 2.1242605691315024

Epoch: 6| Step: 9
Training loss: 2.406522274017334
Validation loss: 2.1348172926133677

Epoch: 6| Step: 10
Training loss: 1.8200892210006714
Validation loss: 2.1207994517459663

Epoch: 6| Step: 11
Training loss: 1.5780566930770874
Validation loss: 2.097154768564368

Epoch: 6| Step: 12
Training loss: 1.049483060836792
Validation loss: 2.0997294226000385

Epoch: 6| Step: 13
Training loss: 1.6317191123962402
Validation loss: 2.09665007745066

Epoch: 492| Step: 0
Training loss: 1.8469247817993164
Validation loss: 2.0830235147988923

Epoch: 6| Step: 1
Training loss: 1.6866463422775269
Validation loss: 2.104346593221029

Epoch: 6| Step: 2
Training loss: 1.831425428390503
Validation loss: 2.0954597098853

Epoch: 6| Step: 3
Training loss: 1.8683834075927734
Validation loss: 2.0917095599635953

Epoch: 6| Step: 4
Training loss: 1.4919507503509521
Validation loss: 2.078000286574005

Epoch: 6| Step: 5
Training loss: 1.604491949081421
Validation loss: 2.0778146020827757

Epoch: 6| Step: 6
Training loss: 1.4648926258087158
Validation loss: 2.0945652146493234

Epoch: 6| Step: 7
Training loss: 1.5945653915405273
Validation loss: 2.1074639853610786

Epoch: 6| Step: 8
Training loss: 1.5100529193878174
Validation loss: 2.1181280920582433

Epoch: 6| Step: 9
Training loss: 1.8503679037094116
Validation loss: 2.138776292083084

Epoch: 6| Step: 10
Training loss: 1.4613099098205566
Validation loss: 2.11793420391698

Epoch: 6| Step: 11
Training loss: 1.3049485683441162
Validation loss: 2.160954518984723

Epoch: 6| Step: 12
Training loss: 2.230558395385742
Validation loss: 2.148015988770352

Epoch: 6| Step: 13
Training loss: 1.2389380931854248
Validation loss: 2.092009505917949

Epoch: 493| Step: 0
Training loss: 1.5531024932861328
Validation loss: 2.10374391207131

Epoch: 6| Step: 1
Training loss: 1.4233057498931885
Validation loss: 2.060762584850352

Epoch: 6| Step: 2
Training loss: 1.9877969026565552
Validation loss: 2.0717852935996106

Epoch: 6| Step: 3
Training loss: 1.3705939054489136
Validation loss: 2.0932644362090738

Epoch: 6| Step: 4
Training loss: 1.2719759941101074
Validation loss: 2.068854339661137

Epoch: 6| Step: 5
Training loss: 1.9014782905578613
Validation loss: 2.0717540274384203

Epoch: 6| Step: 6
Training loss: 1.557298183441162
Validation loss: 2.0816840856306014

Epoch: 6| Step: 7
Training loss: 1.77662992477417
Validation loss: 2.091600689836728

Epoch: 6| Step: 8
Training loss: 1.6128751039505005
Validation loss: 2.1016217021531958

Epoch: 6| Step: 9
Training loss: 1.8709299564361572
Validation loss: 2.1142612836694203

Epoch: 6| Step: 10
Training loss: 1.1364350318908691
Validation loss: 2.127149117890225

Epoch: 6| Step: 11
Training loss: 2.0345091819763184
Validation loss: 2.1352835829539965

Epoch: 6| Step: 12
Training loss: 1.9968658685684204
Validation loss: 2.095863498667235

Epoch: 6| Step: 13
Training loss: 1.7727272510528564
Validation loss: 2.098291984168432

Epoch: 494| Step: 0
Training loss: 1.7092664241790771
Validation loss: 2.084094883293234

Epoch: 6| Step: 1
Training loss: 1.46097993850708
Validation loss: 2.0942164441590667

Epoch: 6| Step: 2
Training loss: 1.4665627479553223
Validation loss: 2.0877201800705283

Epoch: 6| Step: 3
Training loss: 1.9959721565246582
Validation loss: 2.081657841641416

Epoch: 6| Step: 4
Training loss: 1.7166579961776733
Validation loss: 2.109689874033774

Epoch: 6| Step: 5
Training loss: 1.7504113912582397
Validation loss: 2.0975859421555714

Epoch: 6| Step: 6
Training loss: 1.81759512424469
Validation loss: 2.0970119930082753

Epoch: 6| Step: 7
Training loss: 0.7349152565002441
Validation loss: 2.0979160826693297

Epoch: 6| Step: 8
Training loss: 1.5573327541351318
Validation loss: 2.088898233188096

Epoch: 6| Step: 9
Training loss: 1.873081922531128
Validation loss: 2.069172751518988

Epoch: 6| Step: 10
Training loss: 1.7160756587982178
Validation loss: 2.0913211914800827

Epoch: 6| Step: 11
Training loss: 2.106055498123169
Validation loss: 2.0874282288294967

Epoch: 6| Step: 12
Training loss: 1.7595164775848389
Validation loss: 2.068690561479138

Epoch: 6| Step: 13
Training loss: 1.4663984775543213
Validation loss: 2.0729145285903767

Epoch: 495| Step: 0
Training loss: 1.6501033306121826
Validation loss: 2.0646048886801607

Epoch: 6| Step: 1
Training loss: 1.9274746179580688
Validation loss: 2.069981978785607

Epoch: 6| Step: 2
Training loss: 2.6219897270202637
Validation loss: 2.0587011101425334

Epoch: 6| Step: 3
Training loss: 1.4420406818389893
Validation loss: 2.0705585197735856

Epoch: 6| Step: 4
Training loss: 1.6694976091384888
Validation loss: 2.0654816037865094

Epoch: 6| Step: 5
Training loss: 1.3068146705627441
Validation loss: 2.0785128839554323

Epoch: 6| Step: 6
Training loss: 1.830648422241211
Validation loss: 2.0556453504870014

Epoch: 6| Step: 7
Training loss: 1.9202616214752197
Validation loss: 2.076986056502147

Epoch: 6| Step: 8
Training loss: 1.1831886768341064
Validation loss: 2.1152654181244555

Epoch: 6| Step: 9
Training loss: 1.9167770147323608
Validation loss: 2.1245327944396646

Epoch: 6| Step: 10
Training loss: 1.1440892219543457
Validation loss: 2.1276674168084257

Epoch: 6| Step: 11
Training loss: 1.7501716613769531
Validation loss: 2.098208281301683

Epoch: 6| Step: 12
Training loss: 1.1238372325897217
Validation loss: 2.0702976565207205

Epoch: 6| Step: 13
Training loss: 1.659988522529602
Validation loss: 2.0826879380851664

Epoch: 496| Step: 0
Training loss: 1.513442873954773
Validation loss: 2.0746153298244683

Epoch: 6| Step: 1
Training loss: 1.6358102560043335
Validation loss: 2.0602082565266597

Epoch: 6| Step: 2
Training loss: 1.7838252782821655
Validation loss: 2.051680572571293

Epoch: 6| Step: 3
Training loss: 1.578216791152954
Validation loss: 2.0622628042774815

Epoch: 6| Step: 4
Training loss: 1.2243658304214478
Validation loss: 2.075737258439423

Epoch: 6| Step: 5
Training loss: 1.7512398958206177
Validation loss: 2.0531017293212233

Epoch: 6| Step: 6
Training loss: 1.3287203311920166
Validation loss: 2.0896904494172786

Epoch: 6| Step: 7
Training loss: 1.4643014669418335
Validation loss: 2.103869274098386

Epoch: 6| Step: 8
Training loss: 2.7069029808044434
Validation loss: 2.1321444716504825

Epoch: 6| Step: 9
Training loss: 1.6183395385742188
Validation loss: 2.152332096971491

Epoch: 6| Step: 10
Training loss: 1.852735996246338
Validation loss: 2.1507014869361796

Epoch: 6| Step: 11
Training loss: 1.3425111770629883
Validation loss: 2.1636831170769146

Epoch: 6| Step: 12
Training loss: 1.5947527885437012
Validation loss: 2.15328658780744

Epoch: 6| Step: 13
Training loss: 1.8964658975601196
Validation loss: 2.137108049085063

Epoch: 497| Step: 0
Training loss: 1.9430043697357178
Validation loss: 2.1368034629411596

Epoch: 6| Step: 1
Training loss: 1.593217134475708
Validation loss: 2.1048109403220554

Epoch: 6| Step: 2
Training loss: 1.6874983310699463
Validation loss: 2.1178219215844267

Epoch: 6| Step: 3
Training loss: 0.9840011596679688
Validation loss: 2.0698083177689584

Epoch: 6| Step: 4
Training loss: 1.0852794647216797
Validation loss: 2.070263616500362

Epoch: 6| Step: 5
Training loss: 1.7105133533477783
Validation loss: 2.120089013089416

Epoch: 6| Step: 6
Training loss: 2.0126843452453613
Validation loss: 2.101815814613014

Epoch: 6| Step: 7
Training loss: 1.7889416217803955
Validation loss: 2.098061782057567

Epoch: 6| Step: 8
Training loss: 1.2132128477096558
Validation loss: 2.1043492030071955

Epoch: 6| Step: 9
Training loss: 1.809903621673584
Validation loss: 2.1049744582945302

Epoch: 6| Step: 10
Training loss: 1.2073475122451782
Validation loss: 2.0844443434028217

Epoch: 6| Step: 11
Training loss: 2.174670934677124
Validation loss: 2.0823438141935613

Epoch: 6| Step: 12
Training loss: 1.7955011129379272
Validation loss: 2.0626943624147804

Epoch: 6| Step: 13
Training loss: 2.082543134689331
Validation loss: 2.0641823045669065

Epoch: 498| Step: 0
Training loss: 1.392393946647644
Validation loss: 2.069862593886673

Epoch: 6| Step: 1
Training loss: 1.5063540935516357
Validation loss: 2.0572484898310837

Epoch: 6| Step: 2
Training loss: 2.070885181427002
Validation loss: 2.0635934183674474

Epoch: 6| Step: 3
Training loss: 1.638228416442871
Validation loss: 2.045251879640805

Epoch: 6| Step: 4
Training loss: 1.9482407569885254
Validation loss: 2.03289226050018

Epoch: 6| Step: 5
Training loss: 1.718434453010559
Validation loss: 2.0563414301923526

Epoch: 6| Step: 6
Training loss: 1.5639216899871826
Validation loss: 2.04862501031609

Epoch: 6| Step: 7
Training loss: 1.2216598987579346
Validation loss: 2.0527724271179526

Epoch: 6| Step: 8
Training loss: 1.762942910194397
Validation loss: 2.040290353118732

Epoch: 6| Step: 9
Training loss: 1.6369667053222656
Validation loss: 2.0586781194133144

Epoch: 6| Step: 10
Training loss: 1.8544061183929443
Validation loss: 2.0633445862800843

Epoch: 6| Step: 11
Training loss: 1.7506282329559326
Validation loss: 2.0906016288265103

Epoch: 6| Step: 12
Training loss: 1.3428189754486084
Validation loss: 2.1235226879837694

Epoch: 6| Step: 13
Training loss: 1.4397705793380737
Validation loss: 2.1853201363676336

Epoch: 499| Step: 0
Training loss: 1.8163560628890991
Validation loss: 2.2127432669362714

Epoch: 6| Step: 1
Training loss: 1.4069147109985352
Validation loss: 2.2181122943919194

Epoch: 6| Step: 2
Training loss: 2.073784351348877
Validation loss: 2.1583160687518377

Epoch: 6| Step: 3
Training loss: 1.4315993785858154
Validation loss: 2.111707389995616

Epoch: 6| Step: 4
Training loss: 1.4431054592132568
Validation loss: 2.082023685978305

Epoch: 6| Step: 5
Training loss: 1.585291862487793
Validation loss: 2.0410544833829327

Epoch: 6| Step: 6
Training loss: 1.5258662700653076
Validation loss: 2.061008719987767

Epoch: 6| Step: 7
Training loss: 1.635258436203003
Validation loss: 2.0488564660472255

Epoch: 6| Step: 8
Training loss: 1.55291748046875
Validation loss: 2.0724151608764485

Epoch: 6| Step: 9
Training loss: 0.9007912874221802
Validation loss: 2.079757568656757

Epoch: 6| Step: 10
Training loss: 2.5468833446502686
Validation loss: 2.061135063889206

Epoch: 6| Step: 11
Training loss: 2.2026000022888184
Validation loss: 2.049425128967531

Epoch: 6| Step: 12
Training loss: 1.3480380773544312
Validation loss: 2.044701405750808

Epoch: 6| Step: 13
Training loss: 1.3441312313079834
Validation loss: 2.07861731642036

Epoch: 500| Step: 0
Training loss: 1.6962367296218872
Validation loss: 2.0722611411925285

Epoch: 6| Step: 1
Training loss: 1.4152655601501465
Validation loss: 2.1022919557427846

Epoch: 6| Step: 2
Training loss: 1.1419148445129395
Validation loss: 2.1223346494859263

Epoch: 6| Step: 3
Training loss: 1.458160161972046
Validation loss: 2.1421503123416694

Epoch: 6| Step: 4
Training loss: 2.276235818862915
Validation loss: 2.1108045065274803

Epoch: 6| Step: 5
Training loss: 1.7171298265457153
Validation loss: 2.104258462946902

Epoch: 6| Step: 6
Training loss: 1.5446170568466187
Validation loss: 2.0694807101321477

Epoch: 6| Step: 7
Training loss: 1.7808221578598022
Validation loss: 2.073201499959474

Epoch: 6| Step: 8
Training loss: 1.3005127906799316
Validation loss: 2.0877626429321947

Epoch: 6| Step: 9
Training loss: 1.4627854824066162
Validation loss: 2.0914162128202376

Epoch: 6| Step: 10
Training loss: 1.5687062740325928
Validation loss: 2.1016890515563307

Epoch: 6| Step: 11
Training loss: 1.8846442699432373
Validation loss: 2.0703854368579004

Epoch: 6| Step: 12
Training loss: 1.8905143737792969
Validation loss: 2.1165921252260924

Epoch: 6| Step: 13
Training loss: 1.9347991943359375
Validation loss: 2.0956023687957437

Epoch: 501| Step: 0
Training loss: 1.5240356922149658
Validation loss: 2.071919548896051

Epoch: 6| Step: 1
Training loss: 1.7295448780059814
Validation loss: 2.0891548407975065

Epoch: 6| Step: 2
Training loss: 1.8700838088989258
Validation loss: 2.0868414960881716

Epoch: 6| Step: 3
Training loss: 1.6015617847442627
Validation loss: 2.0654122906346477

Epoch: 6| Step: 4
Training loss: 1.8109310865402222
Validation loss: 2.0902919487286638

Epoch: 6| Step: 5
Training loss: 1.8104348182678223
Validation loss: 2.0639970328218196

Epoch: 6| Step: 6
Training loss: 1.6397907733917236
Validation loss: 2.099121352677704

Epoch: 6| Step: 7
Training loss: 1.641237497329712
Validation loss: 2.1126765961288125

Epoch: 6| Step: 8
Training loss: 1.8103420734405518
Validation loss: 2.1053427111717964

Epoch: 6| Step: 9
Training loss: 1.8929846286773682
Validation loss: 2.1043807896234656

Epoch: 6| Step: 10
Training loss: 1.4557881355285645
Validation loss: 2.089364323564755

Epoch: 6| Step: 11
Training loss: 1.3342870473861694
Validation loss: 2.079753634750202

Epoch: 6| Step: 12
Training loss: 1.3109138011932373
Validation loss: 2.071140394415907

Epoch: 6| Step: 13
Training loss: 1.3269915580749512
Validation loss: 2.0805356489714755

Epoch: 502| Step: 0
Training loss: 1.5875880718231201
Validation loss: 2.110246865980087

Epoch: 6| Step: 1
Training loss: 1.5646389722824097
Validation loss: 2.1289998613378054

Epoch: 6| Step: 2
Training loss: 2.2055935859680176
Validation loss: 2.1164451952903502

Epoch: 6| Step: 3
Training loss: 1.7146775722503662
Validation loss: 2.140282238683393

Epoch: 6| Step: 4
Training loss: 1.4278135299682617
Validation loss: 2.1335226438378774

Epoch: 6| Step: 5
Training loss: 1.9602704048156738
Validation loss: 2.1606773996865876

Epoch: 6| Step: 6
Training loss: 1.501326322555542
Validation loss: 2.151328235544184

Epoch: 6| Step: 7
Training loss: 1.866105079650879
Validation loss: 2.1221615088883268

Epoch: 6| Step: 8
Training loss: 1.293906807899475
Validation loss: 2.1215735866177465

Epoch: 6| Step: 9
Training loss: 0.8980505466461182
Validation loss: 2.103966989824849

Epoch: 6| Step: 10
Training loss: 2.034012794494629
Validation loss: 2.0856379462826635

Epoch: 6| Step: 11
Training loss: 1.500645399093628
Validation loss: 2.06381558346492

Epoch: 6| Step: 12
Training loss: 1.9364287853240967
Validation loss: 2.064698987109687

Epoch: 6| Step: 13
Training loss: 1.0486749410629272
Validation loss: 2.0857652515493412

Epoch: 503| Step: 0
Training loss: 1.3063870668411255
Validation loss: 2.069812615712484

Epoch: 6| Step: 1
Training loss: 1.9596058130264282
Validation loss: 2.089578038902693

Epoch: 6| Step: 2
Training loss: 1.6086256504058838
Validation loss: 2.1013238365932176

Epoch: 6| Step: 3
Training loss: 1.5906274318695068
Validation loss: 2.0699313635467202

Epoch: 6| Step: 4
Training loss: 1.31203031539917
Validation loss: 2.1149166989070114

Epoch: 6| Step: 5
Training loss: 2.2797319889068604
Validation loss: 2.090766842647265

Epoch: 6| Step: 6
Training loss: 1.7422025203704834
Validation loss: 2.099998830467142

Epoch: 6| Step: 7
Training loss: 1.112879753112793
Validation loss: 2.094034981983964

Epoch: 6| Step: 8
Training loss: 1.4848172664642334
Validation loss: 2.0756692014714724

Epoch: 6| Step: 9
Training loss: 1.9766979217529297
Validation loss: 2.059345310734164

Epoch: 6| Step: 10
Training loss: 1.4891574382781982
Validation loss: 2.028222437827818

Epoch: 6| Step: 11
Training loss: 1.3028368949890137
Validation loss: 2.0644331004029963

Epoch: 6| Step: 12
Training loss: 1.8308123350143433
Validation loss: 2.0869121487422655

Epoch: 6| Step: 13
Training loss: 1.505760669708252
Validation loss: 2.063983545508436

Epoch: 504| Step: 0
Training loss: 1.9209307432174683
Validation loss: 2.0775398746613534

Epoch: 6| Step: 1
Training loss: 1.3964118957519531
Validation loss: 2.0865128655587473

Epoch: 6| Step: 2
Training loss: 1.2007734775543213
Validation loss: 2.1116241819115094

Epoch: 6| Step: 3
Training loss: 1.330773115158081
Validation loss: 2.115943585672686

Epoch: 6| Step: 4
Training loss: 1.5962932109832764
Validation loss: 2.103233378420594

Epoch: 6| Step: 5
Training loss: 1.5069712400436401
Validation loss: 2.1229786167862597

Epoch: 6| Step: 6
Training loss: 1.211644172668457
Validation loss: 2.0857237128801245

Epoch: 6| Step: 7
Training loss: 1.5634297132492065
Validation loss: 2.101575864258633

Epoch: 6| Step: 8
Training loss: 1.4490206241607666
Validation loss: 2.080022370943459

Epoch: 6| Step: 9
Training loss: 1.7267824411392212
Validation loss: 2.0829223612303376

Epoch: 6| Step: 10
Training loss: 1.859673023223877
Validation loss: 2.1123480155903804

Epoch: 6| Step: 11
Training loss: 2.16579008102417
Validation loss: 2.0906287854717625

Epoch: 6| Step: 12
Training loss: 1.643108606338501
Validation loss: 2.0921333605243313

Epoch: 6| Step: 13
Training loss: 2.043626070022583
Validation loss: 2.1078890331329836

Epoch: 505| Step: 0
Training loss: 1.7656866312026978
Validation loss: 2.0965268714453584

Epoch: 6| Step: 1
Training loss: 1.8086884021759033
Validation loss: 2.0719274577274116

Epoch: 6| Step: 2
Training loss: 1.7134394645690918
Validation loss: 2.06948874714554

Epoch: 6| Step: 3
Training loss: 1.6280122995376587
Validation loss: 2.0531135861591627

Epoch: 6| Step: 4
Training loss: 1.1990745067596436
Validation loss: 2.0798877798100954

Epoch: 6| Step: 5
Training loss: 0.6637955904006958
Validation loss: 2.098412661142247

Epoch: 6| Step: 6
Training loss: 2.2234325408935547
Validation loss: 2.12807624570785

Epoch: 6| Step: 7
Training loss: 1.7194846868515015
Validation loss: 2.1663907779160367

Epoch: 6| Step: 8
Training loss: 1.162168264389038
Validation loss: 2.175056824120142

Epoch: 6| Step: 9
Training loss: 1.4256261587142944
Validation loss: 2.149729500534714

Epoch: 6| Step: 10
Training loss: 2.4833388328552246
Validation loss: 2.163955142421107

Epoch: 6| Step: 11
Training loss: 1.5608162879943848
Validation loss: 2.177055181995515

Epoch: 6| Step: 12
Training loss: 1.7319484949111938
Validation loss: 2.131532302466772

Epoch: 6| Step: 13
Training loss: 1.3842263221740723
Validation loss: 2.0869501688147105

Epoch: 506| Step: 0
Training loss: 1.01409912109375
Validation loss: 2.088843801970123

Epoch: 6| Step: 1
Training loss: 0.8093302249908447
Validation loss: 2.070399220271777

Epoch: 6| Step: 2
Training loss: 1.3665251731872559
Validation loss: 2.085727337867983

Epoch: 6| Step: 3
Training loss: 1.8110103607177734
Validation loss: 2.073728642156047

Epoch: 6| Step: 4
Training loss: 1.9984443187713623
Validation loss: 2.1139996269697785

Epoch: 6| Step: 5
Training loss: 2.042123794555664
Validation loss: 2.108259256168078

Epoch: 6| Step: 6
Training loss: 1.3482847213745117
Validation loss: 2.14263762709915

Epoch: 6| Step: 7
Training loss: 1.1731436252593994
Validation loss: 2.0919722421194917

Epoch: 6| Step: 8
Training loss: 1.5516541004180908
Validation loss: 2.0796776792054534

Epoch: 6| Step: 9
Training loss: 1.6040494441986084
Validation loss: 2.0745788633182483

Epoch: 6| Step: 10
Training loss: 2.3839282989501953
Validation loss: 2.045166401452916

Epoch: 6| Step: 11
Training loss: 1.6920355558395386
Validation loss: 2.0240448803030033

Epoch: 6| Step: 12
Training loss: 2.1156811714172363
Validation loss: 2.031245639247279

Epoch: 6| Step: 13
Training loss: 1.8215501308441162
Validation loss: 2.0371616553234797

Epoch: 507| Step: 0
Training loss: 1.0647258758544922
Validation loss: 2.04322785715903

Epoch: 6| Step: 1
Training loss: 1.476670265197754
Validation loss: 2.046858896491348

Epoch: 6| Step: 2
Training loss: 0.8135102987289429
Validation loss: 2.090286761201838

Epoch: 6| Step: 3
Training loss: 1.743009328842163
Validation loss: 2.103045207197948

Epoch: 6| Step: 4
Training loss: 0.9864556193351746
Validation loss: 2.153677099494524

Epoch: 6| Step: 5
Training loss: 1.8554240465164185
Validation loss: 2.152317718792987

Epoch: 6| Step: 6
Training loss: 1.8285936117172241
Validation loss: 2.152742865265057

Epoch: 6| Step: 7
Training loss: 1.6185057163238525
Validation loss: 2.1753273010253906

Epoch: 6| Step: 8
Training loss: 1.6001032590866089
Validation loss: 2.1882937851772515

Epoch: 6| Step: 9
Training loss: 1.4308979511260986
Validation loss: 2.1687913710071194

Epoch: 6| Step: 10
Training loss: 2.6643686294555664
Validation loss: 2.113707055327713

Epoch: 6| Step: 11
Training loss: 1.7105475664138794
Validation loss: 2.105256957392539

Epoch: 6| Step: 12
Training loss: 1.9639081954956055
Validation loss: 2.077826625557356

Epoch: 6| Step: 13
Training loss: 1.8724778890609741
Validation loss: 2.0661018407473

Epoch: 508| Step: 0
Training loss: 2.070068120956421
Validation loss: 2.0691597589882473

Epoch: 6| Step: 1
Training loss: 0.9892680644989014
Validation loss: 2.036443466781288

Epoch: 6| Step: 2
Training loss: 1.10455322265625
Validation loss: 2.0291215988897506

Epoch: 6| Step: 3
Training loss: 2.006883382797241
Validation loss: 2.065743259204331

Epoch: 6| Step: 4
Training loss: 1.814877986907959
Validation loss: 2.0631321578897457

Epoch: 6| Step: 5
Training loss: 1.5316345691680908
Validation loss: 2.0764609690635436

Epoch: 6| Step: 6
Training loss: 1.004895567893982
Validation loss: 2.1193910824355258

Epoch: 6| Step: 7
Training loss: 2.093309164047241
Validation loss: 2.1658953005267727

Epoch: 6| Step: 8
Training loss: 1.634143590927124
Validation loss: 2.133651215543029

Epoch: 6| Step: 9
Training loss: 1.9535439014434814
Validation loss: 2.1598791678746543

Epoch: 6| Step: 10
Training loss: 1.088944673538208
Validation loss: 2.1381315492814585

Epoch: 6| Step: 11
Training loss: 1.234397292137146
Validation loss: 2.1229068604848718

Epoch: 6| Step: 12
Training loss: 2.125762462615967
Validation loss: 2.0949169692172798

Epoch: 6| Step: 13
Training loss: 2.0725557804107666
Validation loss: 2.1165784417942004

Epoch: 509| Step: 0
Training loss: 1.0085997581481934
Validation loss: 2.0865875803014284

Epoch: 6| Step: 1
Training loss: 1.287307620048523
Validation loss: 2.0605327134491294

Epoch: 6| Step: 2
Training loss: 1.4780975580215454
Validation loss: 2.0595670028399398

Epoch: 6| Step: 3
Training loss: 1.3463349342346191
Validation loss: 2.074167484878212

Epoch: 6| Step: 4
Training loss: 1.4888213872909546
Validation loss: 2.0507071018218994

Epoch: 6| Step: 5
Training loss: 1.962679147720337
Validation loss: 2.0686590389538835

Epoch: 6| Step: 6
Training loss: 1.6705434322357178
Validation loss: 2.069140716265607

Epoch: 6| Step: 7
Training loss: 1.7181968688964844
Validation loss: 2.0846041146145073

Epoch: 6| Step: 8
Training loss: 1.664991021156311
Validation loss: 2.0738263489097677

Epoch: 6| Step: 9
Training loss: 1.5852279663085938
Validation loss: 2.0934773440002115

Epoch: 6| Step: 10
Training loss: 1.347464919090271
Validation loss: 2.0901339207926104

Epoch: 6| Step: 11
Training loss: 1.9789193868637085
Validation loss: 2.098110839884768

Epoch: 6| Step: 12
Training loss: 2.227041244506836
Validation loss: 2.0980781303939

Epoch: 6| Step: 13
Training loss: 1.616795539855957
Validation loss: 2.104456683640839

Epoch: 510| Step: 0
Training loss: 2.110227108001709
Validation loss: 2.0887141560995452

Epoch: 6| Step: 1
Training loss: 2.459778308868408
Validation loss: 2.093413934912733

Epoch: 6| Step: 2
Training loss: 1.3905328512191772
Validation loss: 2.0975916872742357

Epoch: 6| Step: 3
Training loss: 1.4075738191604614
Validation loss: 2.089203178241689

Epoch: 6| Step: 4
Training loss: 1.3241536617279053
Validation loss: 2.1156839298945602

Epoch: 6| Step: 5
Training loss: 2.3377342224121094
Validation loss: 2.1104416578046736

Epoch: 6| Step: 6
Training loss: 1.0371780395507812
Validation loss: 2.1122231816732757

Epoch: 6| Step: 7
Training loss: 1.6080377101898193
Validation loss: 2.0966628725810716

Epoch: 6| Step: 8
Training loss: 1.551438331604004
Validation loss: 2.098198231830392

Epoch: 6| Step: 9
Training loss: 1.8961483240127563
Validation loss: 2.1007264903796616

Epoch: 6| Step: 10
Training loss: 1.4743154048919678
Validation loss: 2.0866260092745543

Epoch: 6| Step: 11
Training loss: 1.3585450649261475
Validation loss: 2.0545235897905085

Epoch: 6| Step: 12
Training loss: 0.7690980434417725
Validation loss: 2.0506535217326176

Epoch: 6| Step: 13
Training loss: 1.539705514907837
Validation loss: 2.0402929116320867

Epoch: 511| Step: 0
Training loss: 1.4546706676483154
Validation loss: 2.050310004142023

Epoch: 6| Step: 1
Training loss: 1.5070011615753174
Validation loss: 2.0482964079867125

Epoch: 6| Step: 2
Training loss: 0.98140549659729
Validation loss: 2.0622973800987325

Epoch: 6| Step: 3
Training loss: 1.9354339838027954
Validation loss: 2.0492748701444237

Epoch: 6| Step: 4
Training loss: 1.6868908405303955
Validation loss: 2.0503212982608425

Epoch: 6| Step: 5
Training loss: 1.6711359024047852
Validation loss: 2.0619879358558246

Epoch: 6| Step: 6
Training loss: 1.765171766281128
Validation loss: 2.070146429923273

Epoch: 6| Step: 7
Training loss: 1.5268696546554565
Validation loss: 2.087234911098275

Epoch: 6| Step: 8
Training loss: 1.0899635553359985
Validation loss: 2.0958595634788595

Epoch: 6| Step: 9
Training loss: 1.7771515846252441
Validation loss: 2.1182418125931934

Epoch: 6| Step: 10
Training loss: 1.6689555644989014
Validation loss: 2.1164639355033956

Epoch: 6| Step: 11
Training loss: 1.7897100448608398
Validation loss: 2.1071334833739908

Epoch: 6| Step: 12
Training loss: 1.628787636756897
Validation loss: 2.1014614694861957

Epoch: 6| Step: 13
Training loss: 1.8490214347839355
Validation loss: 2.1311554549842753

Epoch: 512| Step: 0
Training loss: 1.43586003780365
Validation loss: 2.116942205736714

Epoch: 6| Step: 1
Training loss: 1.3013834953308105
Validation loss: 2.14622752640837

Epoch: 6| Step: 2
Training loss: 1.8304165601730347
Validation loss: 2.1782800715456725

Epoch: 6| Step: 3
Training loss: 1.4860572814941406
Validation loss: 2.1653842054387575

Epoch: 6| Step: 4
Training loss: 1.6474807262420654
Validation loss: 2.151162898668679

Epoch: 6| Step: 5
Training loss: 1.5535368919372559
Validation loss: 2.1242864029381865

Epoch: 6| Step: 6
Training loss: 1.653084635734558
Validation loss: 2.088103977582788

Epoch: 6| Step: 7
Training loss: 1.8254218101501465
Validation loss: 2.075435308999913

Epoch: 6| Step: 8
Training loss: 2.0132172107696533
Validation loss: 2.040861814252792

Epoch: 6| Step: 9
Training loss: 2.4555320739746094
Validation loss: 2.026638848807222

Epoch: 6| Step: 10
Training loss: 1.6487329006195068
Validation loss: 1.9843921943377423

Epoch: 6| Step: 11
Training loss: 1.1180827617645264
Validation loss: 2.0252087500787552

Epoch: 6| Step: 12
Training loss: 1.259462594985962
Validation loss: 2.0023068266530193

Epoch: 6| Step: 13
Training loss: 0.9759886264801025
Validation loss: 2.0701295944952194

Epoch: 513| Step: 0
Training loss: 1.345468521118164
Validation loss: 2.095464887157563

Epoch: 6| Step: 1
Training loss: 1.7359684705734253
Validation loss: 2.106009325673503

Epoch: 6| Step: 2
Training loss: 1.7362864017486572
Validation loss: 2.12519488539747

Epoch: 6| Step: 3
Training loss: 1.4460999965667725
Validation loss: 2.1646495339691

Epoch: 6| Step: 4
Training loss: 1.168472409248352
Validation loss: 2.168644792290144

Epoch: 6| Step: 5
Training loss: 1.5284819602966309
Validation loss: 2.182331649206018

Epoch: 6| Step: 6
Training loss: 1.5209121704101562
Validation loss: 2.20321245860028

Epoch: 6| Step: 7
Training loss: 1.3285696506500244
Validation loss: 2.1607910458759596

Epoch: 6| Step: 8
Training loss: 1.8400113582611084
Validation loss: 2.176461944016077

Epoch: 6| Step: 9
Training loss: 2.025059223175049
Validation loss: 2.156024500887881

Epoch: 6| Step: 10
Training loss: 1.9314829111099243
Validation loss: 2.114577506178169

Epoch: 6| Step: 11
Training loss: 1.5851221084594727
Validation loss: 2.088435667817311

Epoch: 6| Step: 12
Training loss: 1.7257988452911377
Validation loss: 2.0860093075742006

Epoch: 6| Step: 13
Training loss: 1.9594612121582031
Validation loss: 2.0532662637772097

Epoch: 514| Step: 0
Training loss: 1.8258163928985596
Validation loss: 2.0445389465619157

Epoch: 6| Step: 1
Training loss: 1.7709038257598877
Validation loss: 2.051269833759595

Epoch: 6| Step: 2
Training loss: 1.6925148963928223
Validation loss: 2.0540141546598045

Epoch: 6| Step: 3
Training loss: 1.409308910369873
Validation loss: 2.0691291055371686

Epoch: 6| Step: 4
Training loss: 1.8758811950683594
Validation loss: 2.0836878797059417

Epoch: 6| Step: 5
Training loss: 1.9961198568344116
Validation loss: 2.0744743488168202

Epoch: 6| Step: 6
Training loss: 1.3342373371124268
Validation loss: 2.0758703190793275

Epoch: 6| Step: 7
Training loss: 1.587103247642517
Validation loss: 2.059624736027051

Epoch: 6| Step: 8
Training loss: 1.4042720794677734
Validation loss: 2.048068214488286

Epoch: 6| Step: 9
Training loss: 1.0779976844787598
Validation loss: 2.0822583988148677

Epoch: 6| Step: 10
Training loss: 0.8395041227340698
Validation loss: 2.0707489905818814

Epoch: 6| Step: 11
Training loss: 1.9115471839904785
Validation loss: 2.0719665224834154

Epoch: 6| Step: 12
Training loss: 1.6561118364334106
Validation loss: 2.0845887660980225

Epoch: 6| Step: 13
Training loss: 1.7282888889312744
Validation loss: 2.0703898783653014

Epoch: 515| Step: 0
Training loss: 1.6776931285858154
Validation loss: 2.0675179022614674

Epoch: 6| Step: 1
Training loss: 2.157576084136963
Validation loss: 2.060427422164589

Epoch: 6| Step: 2
Training loss: 1.7544593811035156
Validation loss: 2.0277059924217964

Epoch: 6| Step: 3
Training loss: 1.462770938873291
Validation loss: 2.034464072155696

Epoch: 6| Step: 4
Training loss: 1.4565887451171875
Validation loss: 2.033197408081383

Epoch: 6| Step: 5
Training loss: 1.6148122549057007
Validation loss: 2.0424556578359296

Epoch: 6| Step: 6
Training loss: 1.4995824098587036
Validation loss: 2.0526410251535396

Epoch: 6| Step: 7
Training loss: 0.7308154106140137
Validation loss: 2.0785017449368715

Epoch: 6| Step: 8
Training loss: 1.6661266088485718
Validation loss: 2.077414733107372

Epoch: 6| Step: 9
Training loss: 1.2190830707550049
Validation loss: 2.103832242309406

Epoch: 6| Step: 10
Training loss: 1.5593459606170654
Validation loss: 2.0846059886358117

Epoch: 6| Step: 11
Training loss: 1.3377411365509033
Validation loss: 2.083153768252301

Epoch: 6| Step: 12
Training loss: 1.9753899574279785
Validation loss: 2.064843116268035

Epoch: 6| Step: 13
Training loss: 2.202601432800293
Validation loss: 2.058373374323691

Epoch: 516| Step: 0
Training loss: 1.2713881731033325
Validation loss: 2.0390895464087047

Epoch: 6| Step: 1
Training loss: 1.379042148590088
Validation loss: 2.0399010732609737

Epoch: 6| Step: 2
Training loss: 1.0310190916061401
Validation loss: 2.0469888512806227

Epoch: 6| Step: 3
Training loss: 1.3760865926742554
Validation loss: 2.0391400039836927

Epoch: 6| Step: 4
Training loss: 1.8134827613830566
Validation loss: 2.0404089176526634

Epoch: 6| Step: 5
Training loss: 1.6851496696472168
Validation loss: 2.075937030135944

Epoch: 6| Step: 6
Training loss: 1.7562308311462402
Validation loss: 2.093188993392452

Epoch: 6| Step: 7
Training loss: 1.632717490196228
Validation loss: 2.0606506973184566

Epoch: 6| Step: 8
Training loss: 1.2922658920288086
Validation loss: 2.0863714679594962

Epoch: 6| Step: 9
Training loss: 1.574584722518921
Validation loss: 2.123952563090991

Epoch: 6| Step: 10
Training loss: 1.683398962020874
Validation loss: 2.1323418322429863

Epoch: 6| Step: 11
Training loss: 2.3492019176483154
Validation loss: 2.0945887616885606

Epoch: 6| Step: 12
Training loss: 1.083022117614746
Validation loss: 2.0963247629903976

Epoch: 6| Step: 13
Training loss: 2.2525453567504883
Validation loss: 2.102838175271147

Epoch: 517| Step: 0
Training loss: 1.6602630615234375
Validation loss: 2.108090421204926

Epoch: 6| Step: 1
Training loss: 1.7137047052383423
Validation loss: 2.0803196981389034

Epoch: 6| Step: 2
Training loss: 2.097449779510498
Validation loss: 2.073741594950358

Epoch: 6| Step: 3
Training loss: 1.3464946746826172
Validation loss: 2.033021903807117

Epoch: 6| Step: 4
Training loss: 1.3256230354309082
Validation loss: 2.014868008193149

Epoch: 6| Step: 5
Training loss: 1.6240731477737427
Validation loss: 2.0341597039212465

Epoch: 6| Step: 6
Training loss: 1.249118447303772
Validation loss: 2.0294716640185286

Epoch: 6| Step: 7
Training loss: 1.626232385635376
Validation loss: 2.0500744619677143

Epoch: 6| Step: 8
Training loss: 1.8917944431304932
Validation loss: 2.05025904152983

Epoch: 6| Step: 9
Training loss: 1.4933671951293945
Validation loss: 2.066586071445096

Epoch: 6| Step: 10
Training loss: 1.6669800281524658
Validation loss: 2.0753334324846984

Epoch: 6| Step: 11
Training loss: 1.064349889755249
Validation loss: 2.0726540678290912

Epoch: 6| Step: 12
Training loss: 1.721757173538208
Validation loss: 2.0701379929819415

Epoch: 6| Step: 13
Training loss: 1.4751461744308472
Validation loss: 2.0395652888923563

Epoch: 518| Step: 0
Training loss: 1.2960904836654663
Validation loss: 2.0209339921192457

Epoch: 6| Step: 1
Training loss: 1.107147216796875
Validation loss: 2.0409476500685497

Epoch: 6| Step: 2
Training loss: 0.7911822199821472
Validation loss: 2.050434427876626

Epoch: 6| Step: 3
Training loss: 1.783423662185669
Validation loss: 2.0589563615860476

Epoch: 6| Step: 4
Training loss: 0.9202370643615723
Validation loss: 2.0730390702524493

Epoch: 6| Step: 5
Training loss: 2.397655725479126
Validation loss: 2.105846763938986

Epoch: 6| Step: 6
Training loss: 1.5914571285247803
Validation loss: 2.1338565708488546

Epoch: 6| Step: 7
Training loss: 1.1068274974822998
Validation loss: 2.141035274792743

Epoch: 6| Step: 8
Training loss: 1.8766629695892334
Validation loss: 2.1555763188228814

Epoch: 6| Step: 9
Training loss: 1.2860848903656006
Validation loss: 2.1203244065725677

Epoch: 6| Step: 10
Training loss: 1.65285062789917
Validation loss: 2.1302432757551952

Epoch: 6| Step: 11
Training loss: 1.9607435464859009
Validation loss: 2.109935050369591

Epoch: 6| Step: 12
Training loss: 2.3724637031555176
Validation loss: 2.125934946921564

Epoch: 6| Step: 13
Training loss: 2.0711262226104736
Validation loss: 2.1179479834853963

Epoch: 519| Step: 0
Training loss: 1.8304433822631836
Validation loss: 2.143590214431927

Epoch: 6| Step: 1
Training loss: 1.637123942375183
Validation loss: 2.0956567359227005

Epoch: 6| Step: 2
Training loss: 1.7394931316375732
Validation loss: 2.071454401939146

Epoch: 6| Step: 3
Training loss: 1.2128450870513916
Validation loss: 2.0738290779052244

Epoch: 6| Step: 4
Training loss: 1.8854150772094727
Validation loss: 2.066442729324423

Epoch: 6| Step: 5
Training loss: 2.0084550380706787
Validation loss: 2.079517026101389

Epoch: 6| Step: 6
Training loss: 1.1068246364593506
Validation loss: 2.0831630511950423

Epoch: 6| Step: 7
Training loss: 2.237922191619873
Validation loss: 2.0689036000159478

Epoch: 6| Step: 8
Training loss: 1.2457960844039917
Validation loss: 2.0383477441726194

Epoch: 6| Step: 9
Training loss: 1.3854507207870483
Validation loss: 2.0322930543653426

Epoch: 6| Step: 10
Training loss: 1.769828200340271
Validation loss: 2.049842434544717

Epoch: 6| Step: 11
Training loss: 1.1264748573303223
Validation loss: 2.047858891948577

Epoch: 6| Step: 12
Training loss: 1.483885407447815
Validation loss: 2.091108391361852

Epoch: 6| Step: 13
Training loss: 1.3617768287658691
Validation loss: 2.1174610635285736

Epoch: 520| Step: 0
Training loss: 1.2359108924865723
Validation loss: 2.1208713131566204

Epoch: 6| Step: 1
Training loss: 1.6482665538787842
Validation loss: 2.138580350465672

Epoch: 6| Step: 2
Training loss: 1.6602373123168945
Validation loss: 2.1104849384677027

Epoch: 6| Step: 3
Training loss: 0.8674346208572388
Validation loss: 2.064620383324162

Epoch: 6| Step: 4
Training loss: 1.5071072578430176
Validation loss: 2.055434109062277

Epoch: 6| Step: 5
Training loss: 1.9712817668914795
Validation loss: 2.068200147280129

Epoch: 6| Step: 6
Training loss: 1.0668268203735352
Validation loss: 2.055909772073069

Epoch: 6| Step: 7
Training loss: 1.7557897567749023
Validation loss: 2.04620119320449

Epoch: 6| Step: 8
Training loss: 1.5148102045059204
Validation loss: 2.0795669965846564

Epoch: 6| Step: 9
Training loss: 2.149998664855957
Validation loss: 2.0716165111910914

Epoch: 6| Step: 10
Training loss: 1.543323040008545
Validation loss: 2.0702391939778484

Epoch: 6| Step: 11
Training loss: 1.0999679565429688
Validation loss: 2.0664033992316133

Epoch: 6| Step: 12
Training loss: 2.1412649154663086
Validation loss: 2.0928526642501994

Epoch: 6| Step: 13
Training loss: 1.773114800453186
Validation loss: 2.0967843686380694

Epoch: 521| Step: 0
Training loss: 1.4501190185546875
Validation loss: 2.1550384157447406

Epoch: 6| Step: 1
Training loss: 1.2845354080200195
Validation loss: 2.123418649037679

Epoch: 6| Step: 2
Training loss: 1.8307228088378906
Validation loss: 2.0961982780887234

Epoch: 6| Step: 3
Training loss: 1.8315818309783936
Validation loss: 2.08335143263622

Epoch: 6| Step: 4
Training loss: 1.7152636051177979
Validation loss: 2.0577243528058453

Epoch: 6| Step: 5
Training loss: 1.84290611743927
Validation loss: 2.0331227497387956

Epoch: 6| Step: 6
Training loss: 2.206381320953369
Validation loss: 2.0462369893186834

Epoch: 6| Step: 7
Training loss: 1.4885565042495728
Validation loss: 2.0234956766969416

Epoch: 6| Step: 8
Training loss: 1.4211024045944214
Validation loss: 2.0071599329671552

Epoch: 6| Step: 9
Training loss: 1.5936459302902222
Validation loss: 2.0228645468270905

Epoch: 6| Step: 10
Training loss: 1.3302927017211914
Validation loss: 2.0280095005548127

Epoch: 6| Step: 11
Training loss: 0.9592946767807007
Validation loss: 2.0533845373379287

Epoch: 6| Step: 12
Training loss: 1.1744464635849
Validation loss: 2.0865839860772573

Epoch: 6| Step: 13
Training loss: 1.5918306112289429
Validation loss: 2.1069274679307015

Epoch: 522| Step: 0
Training loss: 1.958350658416748
Validation loss: 2.169020652770996

Epoch: 6| Step: 1
Training loss: 1.640526533126831
Validation loss: 2.1650236293833744

Epoch: 6| Step: 2
Training loss: 1.7703890800476074
Validation loss: 2.187702419937298

Epoch: 6| Step: 3
Training loss: 1.6845098733901978
Validation loss: 2.155299325143137

Epoch: 6| Step: 4
Training loss: 1.1012240648269653
Validation loss: 2.1011920949464202

Epoch: 6| Step: 5
Training loss: 0.8750223517417908
Validation loss: 2.0832034541714575

Epoch: 6| Step: 6
Training loss: 2.315541982650757
Validation loss: 2.0615441927345852

Epoch: 6| Step: 7
Training loss: 2.0622706413269043
Validation loss: 2.0342214504877725

Epoch: 6| Step: 8
Training loss: 1.6997511386871338
Validation loss: 2.057609041531881

Epoch: 6| Step: 9
Training loss: 1.1806308031082153
Validation loss: 2.042779504611928

Epoch: 6| Step: 10
Training loss: 0.8142427802085876
Validation loss: 2.0397935913455103

Epoch: 6| Step: 11
Training loss: 1.448344111442566
Validation loss: 2.0405423077203895

Epoch: 6| Step: 12
Training loss: 1.7773959636688232
Validation loss: 2.063373881001626

Epoch: 6| Step: 13
Training loss: 1.5738683938980103
Validation loss: 2.0776233032185543

Epoch: 523| Step: 0
Training loss: 1.1823740005493164
Validation loss: 2.0762873067650744

Epoch: 6| Step: 1
Training loss: 2.059351921081543
Validation loss: 2.082238692109303

Epoch: 6| Step: 2
Training loss: 1.0511277914047241
Validation loss: 2.0890897115071616

Epoch: 6| Step: 3
Training loss: 1.1132543087005615
Validation loss: 2.087447251043012

Epoch: 6| Step: 4
Training loss: 1.4087026119232178
Validation loss: 2.1155171419984553

Epoch: 6| Step: 5
Training loss: 0.7440081834793091
Validation loss: 2.1161021724823983

Epoch: 6| Step: 6
Training loss: 1.8685072660446167
Validation loss: 2.0709032166388726

Epoch: 6| Step: 7
Training loss: 1.4410839080810547
Validation loss: 2.0494060567630235

Epoch: 6| Step: 8
Training loss: 1.7309186458587646
Validation loss: 2.0771248802062003

Epoch: 6| Step: 9
Training loss: 1.5473053455352783
Validation loss: 2.0693156949935423

Epoch: 6| Step: 10
Training loss: 2.346919059753418
Validation loss: 2.061039622111987

Epoch: 6| Step: 11
Training loss: 1.5685474872589111
Validation loss: 2.053876571757819

Epoch: 6| Step: 12
Training loss: 1.6243622303009033
Validation loss: 2.017702699989401

Epoch: 6| Step: 13
Training loss: 2.015460729598999
Validation loss: 2.0269635441482707

Epoch: 524| Step: 0
Training loss: 1.8362975120544434
Validation loss: 2.068870157323858

Epoch: 6| Step: 1
Training loss: 1.694643497467041
Validation loss: 2.0705311400915987

Epoch: 6| Step: 2
Training loss: 1.2987598180770874
Validation loss: 2.0508956575906403

Epoch: 6| Step: 3
Training loss: 1.9516797065734863
Validation loss: 2.0949860798415316

Epoch: 6| Step: 4
Training loss: 1.5763204097747803
Validation loss: 2.0616184460219515

Epoch: 6| Step: 5
Training loss: 1.4020357131958008
Validation loss: 2.095572770282786

Epoch: 6| Step: 6
Training loss: 1.5972163677215576
Validation loss: 2.092614230289254

Epoch: 6| Step: 7
Training loss: 1.2347383499145508
Validation loss: 2.072796308866111

Epoch: 6| Step: 8
Training loss: 0.6351646184921265
Validation loss: 2.086821195899799

Epoch: 6| Step: 9
Training loss: 2.5520803928375244
Validation loss: 2.0871475870891283

Epoch: 6| Step: 10
Training loss: 1.3970980644226074
Validation loss: 2.11345419063363

Epoch: 6| Step: 11
Training loss: 1.4005303382873535
Validation loss: 2.080248373810963

Epoch: 6| Step: 12
Training loss: 1.387447476387024
Validation loss: 2.0691418058128765

Epoch: 6| Step: 13
Training loss: 1.603714942932129
Validation loss: 2.035164992014567

Epoch: 525| Step: 0
Training loss: 2.09334659576416
Validation loss: 2.059236629034883

Epoch: 6| Step: 1
Training loss: 1.979904294013977
Validation loss: 2.058256572292697

Epoch: 6| Step: 2
Training loss: 0.879163920879364
Validation loss: 2.0399949191718973

Epoch: 6| Step: 3
Training loss: 1.4048302173614502
Validation loss: 2.0377607781399965

Epoch: 6| Step: 4
Training loss: 2.31162691116333
Validation loss: 2.036859525147305

Epoch: 6| Step: 5
Training loss: 0.7896724939346313
Validation loss: 2.0554693821937806

Epoch: 6| Step: 6
Training loss: 1.9839324951171875
Validation loss: 2.0735679031700216

Epoch: 6| Step: 7
Training loss: 1.3503506183624268
Validation loss: 2.0947501967030187

Epoch: 6| Step: 8
Training loss: 1.493985891342163
Validation loss: 2.1039672026070217

Epoch: 6| Step: 9
Training loss: 1.4356192350387573
Validation loss: 2.085244835063975

Epoch: 6| Step: 10
Training loss: 1.7387489080429077
Validation loss: 2.09787477472777

Epoch: 6| Step: 11
Training loss: 1.2218962907791138
Validation loss: 2.0803243242284304

Epoch: 6| Step: 12
Training loss: 1.3801414966583252
Validation loss: 2.0317975962033836

Epoch: 6| Step: 13
Training loss: 1.198469877243042
Validation loss: 2.044386456089635

Epoch: 526| Step: 0
Training loss: 1.576124906539917
Validation loss: 2.047161493250119

Epoch: 6| Step: 1
Training loss: 0.9191949963569641
Validation loss: 2.03991961479187

Epoch: 6| Step: 2
Training loss: 1.7940421104431152
Validation loss: 2.0380750804819088

Epoch: 6| Step: 3
Training loss: 1.5939053297042847
Validation loss: 2.0360759983780565

Epoch: 6| Step: 4
Training loss: 1.3296915292739868
Validation loss: 2.0183128387697282

Epoch: 6| Step: 5
Training loss: 1.0997462272644043
Validation loss: 2.0420887316426923

Epoch: 6| Step: 6
Training loss: 1.386795997619629
Validation loss: 2.071771944722822

Epoch: 6| Step: 7
Training loss: 1.7889981269836426
Validation loss: 2.0806698619678454

Epoch: 6| Step: 8
Training loss: 1.7009389400482178
Validation loss: 2.0663526340197493

Epoch: 6| Step: 9
Training loss: 1.3751204013824463
Validation loss: 2.064685430578006

Epoch: 6| Step: 10
Training loss: 1.87177574634552
Validation loss: 2.098090002613683

Epoch: 6| Step: 11
Training loss: 1.9513376951217651
Validation loss: 2.1155080833742694

Epoch: 6| Step: 12
Training loss: 1.577904462814331
Validation loss: 2.1656024199660107

Epoch: 6| Step: 13
Training loss: 1.2126386165618896
Validation loss: 2.1455392837524414

Epoch: 527| Step: 0
Training loss: 1.7909340858459473
Validation loss: 2.213480790456136

Epoch: 6| Step: 1
Training loss: 1.4707367420196533
Validation loss: 2.1891196722625406

Epoch: 6| Step: 2
Training loss: 1.6862189769744873
Validation loss: 2.183460133050078

Epoch: 6| Step: 3
Training loss: 1.1384602785110474
Validation loss: 2.1257795902990524

Epoch: 6| Step: 4
Training loss: 0.9900319576263428
Validation loss: 2.106424331665039

Epoch: 6| Step: 5
Training loss: 2.1389362812042236
Validation loss: 2.118930603868218

Epoch: 6| Step: 6
Training loss: 0.8514245748519897
Validation loss: 2.0790229170553145

Epoch: 6| Step: 7
Training loss: 1.4232165813446045
Validation loss: 2.073651744473365

Epoch: 6| Step: 8
Training loss: 1.7168653011322021
Validation loss: 2.0390491254868044

Epoch: 6| Step: 9
Training loss: 1.384911060333252
Validation loss: 2.0553170878400087

Epoch: 6| Step: 10
Training loss: 2.0183186531066895
Validation loss: 2.0258073524762223

Epoch: 6| Step: 11
Training loss: 2.02130389213562
Validation loss: 2.0373578058776034

Epoch: 6| Step: 12
Training loss: 1.203708291053772
Validation loss: 2.0307035907622306

Epoch: 6| Step: 13
Training loss: 1.6821684837341309
Validation loss: 2.0448440121066187

Epoch: 528| Step: 0
Training loss: 1.1533334255218506
Validation loss: 2.05645380866143

Epoch: 6| Step: 1
Training loss: 1.463200330734253
Validation loss: 2.1003675717179493

Epoch: 6| Step: 2
Training loss: 1.7039690017700195
Validation loss: 2.1185740040194605

Epoch: 6| Step: 3
Training loss: 1.1543997526168823
Validation loss: 2.129029704678443

Epoch: 6| Step: 4
Training loss: 1.372194528579712
Validation loss: 2.1028241726659958

Epoch: 6| Step: 5
Training loss: 1.4612923860549927
Validation loss: 2.082088929350658

Epoch: 6| Step: 6
Training loss: 1.5856468677520752
Validation loss: 2.0762562623587986

Epoch: 6| Step: 7
Training loss: 1.8506033420562744
Validation loss: 2.071443623112094

Epoch: 6| Step: 8
Training loss: 1.620830774307251
Validation loss: 2.0357263267681165

Epoch: 6| Step: 9
Training loss: 1.6410897970199585
Validation loss: 1.9986596889393304

Epoch: 6| Step: 10
Training loss: 1.6241925954818726
Validation loss: 1.9992961934817735

Epoch: 6| Step: 11
Training loss: 0.6826085448265076
Validation loss: 2.0310983683473323

Epoch: 6| Step: 12
Training loss: 2.1884407997131348
Validation loss: 2.048752433510237

Epoch: 6| Step: 13
Training loss: 2.2450082302093506
Validation loss: 2.0660000654958908

Epoch: 529| Step: 0
Training loss: 1.2537821531295776
Validation loss: 2.088147094172816

Epoch: 6| Step: 1
Training loss: 1.3957037925720215
Validation loss: 2.037024477476715

Epoch: 6| Step: 2
Training loss: 1.5641582012176514
Validation loss: 2.05877847056235

Epoch: 6| Step: 3
Training loss: 2.0848093032836914
Validation loss: 2.0680685581699496

Epoch: 6| Step: 4
Training loss: 1.3858044147491455
Validation loss: 2.0737662392277874

Epoch: 6| Step: 5
Training loss: 1.706438660621643
Validation loss: 2.0556498368581138

Epoch: 6| Step: 6
Training loss: 2.0277838706970215
Validation loss: 2.0549272209085445

Epoch: 6| Step: 7
Training loss: 1.2383395433425903
Validation loss: 2.0550782590784054

Epoch: 6| Step: 8
Training loss: 1.7380592823028564
Validation loss: 2.0837386269723215

Epoch: 6| Step: 9
Training loss: 1.259879231452942
Validation loss: 2.122144745242211

Epoch: 6| Step: 10
Training loss: 1.7376874685287476
Validation loss: 2.110907067534744

Epoch: 6| Step: 11
Training loss: 1.3574092388153076
Validation loss: 2.1167078697553245

Epoch: 6| Step: 12
Training loss: 1.297593355178833
Validation loss: 2.1034320169879543

Epoch: 6| Step: 13
Training loss: 0.999944269657135
Validation loss: 2.0713427412894463

Epoch: 530| Step: 0
Training loss: 2.0643699169158936
Validation loss: 2.052684335298436

Epoch: 6| Step: 1
Training loss: 1.8885760307312012
Validation loss: 2.0632395282868417

Epoch: 6| Step: 2
Training loss: 1.4683722257614136
Validation loss: 2.01199423625905

Epoch: 6| Step: 3
Training loss: 1.8460665941238403
Validation loss: 2.0262458965342534

Epoch: 6| Step: 4
Training loss: 1.5781840085983276
Validation loss: 2.0375407947007047

Epoch: 6| Step: 5
Training loss: 1.8724281787872314
Validation loss: 2.0093985321701213

Epoch: 6| Step: 6
Training loss: 1.4538213014602661
Validation loss: 2.0153933827595045

Epoch: 6| Step: 7
Training loss: 1.9870494604110718
Validation loss: 2.022885784026115

Epoch: 6| Step: 8
Training loss: 1.0524232387542725
Validation loss: 2.029713543512488

Epoch: 6| Step: 9
Training loss: 1.3785288333892822
Validation loss: 2.040323967574745

Epoch: 6| Step: 10
Training loss: 0.9250501394271851
Validation loss: 2.0715592779139036

Epoch: 6| Step: 11
Training loss: 0.9856914281845093
Validation loss: 2.0885262027863534

Epoch: 6| Step: 12
Training loss: 1.3827245235443115
Validation loss: 2.09381277074096

Epoch: 6| Step: 13
Training loss: 1.3412306308746338
Validation loss: 2.1465377756344375

Epoch: 531| Step: 0
Training loss: 1.387477159500122
Validation loss: 2.1012276039328626

Epoch: 6| Step: 1
Training loss: 1.5751076936721802
Validation loss: 2.1340512883278633

Epoch: 6| Step: 2
Training loss: 1.5678915977478027
Validation loss: 2.1006080437732

Epoch: 6| Step: 3
Training loss: 1.0218806266784668
Validation loss: 2.1195418578322216

Epoch: 6| Step: 4
Training loss: 1.585892915725708
Validation loss: 2.094961791910151

Epoch: 6| Step: 5
Training loss: 1.6988096237182617
Validation loss: 2.106043184957197

Epoch: 6| Step: 6
Training loss: 1.474197506904602
Validation loss: 2.0852525567495697

Epoch: 6| Step: 7
Training loss: 1.114402174949646
Validation loss: 2.1063638169278383

Epoch: 6| Step: 8
Training loss: 1.2320853471755981
Validation loss: 2.1012755747764342

Epoch: 6| Step: 9
Training loss: 1.7281038761138916
Validation loss: 2.0891898934559157

Epoch: 6| Step: 10
Training loss: 1.5900763273239136
Validation loss: 2.074547783021004

Epoch: 6| Step: 11
Training loss: 2.0311343669891357
Validation loss: 2.033626846087876

Epoch: 6| Step: 12
Training loss: 1.6106033325195312
Validation loss: 2.0451436247876895

Epoch: 6| Step: 13
Training loss: 1.5008454322814941
Validation loss: 2.004757760673441

Epoch: 532| Step: 0
Training loss: 1.3126035928726196
Validation loss: 2.0033322790617585

Epoch: 6| Step: 1
Training loss: 1.7636866569519043
Validation loss: 2.0091342413297264

Epoch: 6| Step: 2
Training loss: 1.8814470767974854
Validation loss: 2.0027386078270535

Epoch: 6| Step: 3
Training loss: 1.364588737487793
Validation loss: 2.0415689047946723

Epoch: 6| Step: 4
Training loss: 1.340867280960083
Validation loss: 2.0309622210841023

Epoch: 6| Step: 5
Training loss: 0.512154221534729
Validation loss: 2.061615677290065

Epoch: 6| Step: 6
Training loss: 2.1538991928100586
Validation loss: 2.07939435333334

Epoch: 6| Step: 7
Training loss: 1.5708801746368408
Validation loss: 2.091136642681655

Epoch: 6| Step: 8
Training loss: 1.3598111867904663
Validation loss: 2.0746388537909395

Epoch: 6| Step: 9
Training loss: 1.629037857055664
Validation loss: 2.159739807087888

Epoch: 6| Step: 10
Training loss: 2.1066980361938477
Validation loss: 2.1506768503496723

Epoch: 6| Step: 11
Training loss: 0.9826982617378235
Validation loss: 2.123881591263638

Epoch: 6| Step: 12
Training loss: 1.3917313814163208
Validation loss: 2.125465267448015

Epoch: 6| Step: 13
Training loss: 1.9895597696304321
Validation loss: 2.1172577386261313

Epoch: 533| Step: 0
Training loss: 1.6243572235107422
Validation loss: 2.0700470683395222

Epoch: 6| Step: 1
Training loss: 0.9122441411018372
Validation loss: 2.071415001346219

Epoch: 6| Step: 2
Training loss: 1.3877233266830444
Validation loss: 2.051410795539938

Epoch: 6| Step: 3
Training loss: 2.2192678451538086
Validation loss: 2.041328783958189

Epoch: 6| Step: 4
Training loss: 0.9859451055526733
Validation loss: 2.033994023517896

Epoch: 6| Step: 5
Training loss: 1.7786482572555542
Validation loss: 2.0276680402858283

Epoch: 6| Step: 6
Training loss: 1.7101118564605713
Validation loss: 2.0332961351640764

Epoch: 6| Step: 7
Training loss: 1.6128311157226562
Validation loss: 2.034961595330187

Epoch: 6| Step: 8
Training loss: 1.3170685768127441
Validation loss: 2.051627118100402

Epoch: 6| Step: 9
Training loss: 1.3474760055541992
Validation loss: 2.081967048747565

Epoch: 6| Step: 10
Training loss: 1.707099437713623
Validation loss: 2.0878571797442693

Epoch: 6| Step: 11
Training loss: 1.3889648914337158
Validation loss: 2.124658038539271

Epoch: 6| Step: 12
Training loss: 1.306602954864502
Validation loss: 2.148960080198062

Epoch: 6| Step: 13
Training loss: 2.275773525238037
Validation loss: 2.1495392963450444

Epoch: 534| Step: 0
Training loss: 1.8024497032165527
Validation loss: 2.122199659706444

Epoch: 6| Step: 1
Training loss: 1.5637829303741455
Validation loss: 2.1189311704327984

Epoch: 6| Step: 2
Training loss: 1.6293377876281738
Validation loss: 2.0730280055794665

Epoch: 6| Step: 3
Training loss: 1.2253789901733398
Validation loss: 2.0433401189824587

Epoch: 6| Step: 4
Training loss: 1.3787862062454224
Validation loss: 2.010651188512002

Epoch: 6| Step: 5
Training loss: 1.491976022720337
Validation loss: 1.9983171378412554

Epoch: 6| Step: 6
Training loss: 1.1084821224212646
Validation loss: 1.9969258500683693

Epoch: 6| Step: 7
Training loss: 1.657243013381958
Validation loss: 2.0415218478889874

Epoch: 6| Step: 8
Training loss: 1.9800554513931274
Validation loss: 2.048522633890952

Epoch: 6| Step: 9
Training loss: 1.7673276662826538
Validation loss: 2.0544016092054305

Epoch: 6| Step: 10
Training loss: 1.4152075052261353
Validation loss: 2.0764258253958916

Epoch: 6| Step: 11
Training loss: 1.786468505859375
Validation loss: 2.1267501333708405

Epoch: 6| Step: 12
Training loss: 0.6855851411819458
Validation loss: 2.105146218371648

Epoch: 6| Step: 13
Training loss: 1.9260112047195435
Validation loss: 2.1328196012845604

Epoch: 535| Step: 0
Training loss: 1.4375584125518799
Validation loss: 2.163482291724092

Epoch: 6| Step: 1
Training loss: 0.8462468385696411
Validation loss: 2.125693686546818

Epoch: 6| Step: 2
Training loss: 2.044031858444214
Validation loss: 2.0783903470603367

Epoch: 6| Step: 3
Training loss: 1.8064169883728027
Validation loss: 2.0414567532077914

Epoch: 6| Step: 4
Training loss: 1.6813817024230957
Validation loss: 2.027178761779621

Epoch: 6| Step: 5
Training loss: 1.3898990154266357
Validation loss: 2.0302410612824144

Epoch: 6| Step: 6
Training loss: 0.7720413208007812
Validation loss: 1.9895709432581419

Epoch: 6| Step: 7
Training loss: 1.0866377353668213
Validation loss: 2.0055752749084146

Epoch: 6| Step: 8
Training loss: 2.0635199546813965
Validation loss: 2.036308500074571

Epoch: 6| Step: 9
Training loss: 1.5357005596160889
Validation loss: 2.0326683777634815

Epoch: 6| Step: 10
Training loss: 1.1433769464492798
Validation loss: 2.0432178666514735

Epoch: 6| Step: 11
Training loss: 1.7875597476959229
Validation loss: 2.0823613546227895

Epoch: 6| Step: 12
Training loss: 1.8348076343536377
Validation loss: 2.1004575965225056

Epoch: 6| Step: 13
Training loss: 1.5384612083435059
Validation loss: 2.1349985881518294

Epoch: 536| Step: 0
Training loss: 1.2738592624664307
Validation loss: 2.1390466279880975

Epoch: 6| Step: 1
Training loss: 1.7255891561508179
Validation loss: 2.1544375163252636

Epoch: 6| Step: 2
Training loss: 1.795251727104187
Validation loss: 2.0907047512710735

Epoch: 6| Step: 3
Training loss: 0.9690088033676147
Validation loss: 2.0713410685139317

Epoch: 6| Step: 4
Training loss: 1.6078540086746216
Validation loss: 2.0220480721483947

Epoch: 6| Step: 5
Training loss: 1.9232089519500732
Validation loss: 2.045907364096693

Epoch: 6| Step: 6
Training loss: 0.7758609056472778
Validation loss: 2.0167350076859996

Epoch: 6| Step: 7
Training loss: 1.1281449794769287
Validation loss: 2.0409646675150883

Epoch: 6| Step: 8
Training loss: 1.6716288328170776
Validation loss: 2.028580137478408

Epoch: 6| Step: 9
Training loss: 1.5903995037078857
Validation loss: 2.064778417669317

Epoch: 6| Step: 10
Training loss: 1.5489346981048584
Validation loss: 2.079150067862644

Epoch: 6| Step: 11
Training loss: 1.623307704925537
Validation loss: 2.120461553655645

Epoch: 6| Step: 12
Training loss: 1.4280834197998047
Validation loss: 2.0874478740076863

Epoch: 6| Step: 13
Training loss: 2.1592023372650146
Validation loss: 2.088627520427909

Epoch: 537| Step: 0
Training loss: 1.3522371053695679
Validation loss: 2.0967590501231532

Epoch: 6| Step: 1
Training loss: 1.7978370189666748
Validation loss: 2.097639291517196

Epoch: 6| Step: 2
Training loss: 1.6464512348175049
Validation loss: 2.0851559895341114

Epoch: 6| Step: 3
Training loss: 1.737166166305542
Validation loss: 2.081583639626862

Epoch: 6| Step: 4
Training loss: 1.5618623495101929
Validation loss: 2.065255857283069

Epoch: 6| Step: 5
Training loss: 1.2297399044036865
Validation loss: 2.048998163592431

Epoch: 6| Step: 6
Training loss: 1.12906014919281
Validation loss: 2.0352199487788702

Epoch: 6| Step: 7
Training loss: 1.2782444953918457
Validation loss: 2.0359365709366335

Epoch: 6| Step: 8
Training loss: 1.7457431554794312
Validation loss: 2.0402483427396385

Epoch: 6| Step: 9
Training loss: 1.386992335319519
Validation loss: 2.0468934543671145

Epoch: 6| Step: 10
Training loss: 1.5971580743789673
Validation loss: 2.0403595765431723

Epoch: 6| Step: 11
Training loss: 1.5919253826141357
Validation loss: 2.075411710687863

Epoch: 6| Step: 12
Training loss: 1.049623966217041
Validation loss: 2.0760194229823288

Epoch: 6| Step: 13
Training loss: 1.6858611106872559
Validation loss: 2.0596655466223277

Epoch: 538| Step: 0
Training loss: 1.226207971572876
Validation loss: 2.0725169925279516

Epoch: 6| Step: 1
Training loss: 1.2263062000274658
Validation loss: 2.0873551061076503

Epoch: 6| Step: 2
Training loss: 1.6071807146072388
Validation loss: 2.0969458267252934

Epoch: 6| Step: 3
Training loss: 1.9321799278259277
Validation loss: 2.092051686779145

Epoch: 6| Step: 4
Training loss: 1.4649999141693115
Validation loss: 2.089584599259079

Epoch: 6| Step: 5
Training loss: 1.0485056638717651
Validation loss: 2.0796108822668753

Epoch: 6| Step: 6
Training loss: 0.8375883102416992
Validation loss: 2.05824948382634

Epoch: 6| Step: 7
Training loss: 2.0273540019989014
Validation loss: 2.027746703035088

Epoch: 6| Step: 8
Training loss: 1.736595630645752
Validation loss: 2.0076982795551257

Epoch: 6| Step: 9
Training loss: 1.8727716207504272
Validation loss: 2.0029055085233463

Epoch: 6| Step: 10
Training loss: 1.1524385213851929
Validation loss: 2.0075256568129345

Epoch: 6| Step: 11
Training loss: 1.8179938793182373
Validation loss: 2.006492086636123

Epoch: 6| Step: 12
Training loss: 1.3583662509918213
Validation loss: 1.9788991507663523

Epoch: 6| Step: 13
Training loss: 1.6099005937576294
Validation loss: 2.0096411756289903

Epoch: 539| Step: 0
Training loss: 1.8530683517456055
Validation loss: 2.037624284785281

Epoch: 6| Step: 1
Training loss: 1.5498913526535034
Validation loss: 2.037367590012089

Epoch: 6| Step: 2
Training loss: 1.2641223669052124
Validation loss: 2.0457846272376274

Epoch: 6| Step: 3
Training loss: 1.0024014711380005
Validation loss: 2.0650225249669885

Epoch: 6| Step: 4
Training loss: 1.1667383909225464
Validation loss: 2.0787779336334555

Epoch: 6| Step: 5
Training loss: 1.7607526779174805
Validation loss: 2.1132743871340187

Epoch: 6| Step: 6
Training loss: 1.8134770393371582
Validation loss: 2.109523811647969

Epoch: 6| Step: 7
Training loss: 0.9568067789077759
Validation loss: 2.0980845497500513

Epoch: 6| Step: 8
Training loss: 2.06604266166687
Validation loss: 2.0889867095537085

Epoch: 6| Step: 9
Training loss: 1.9508979320526123
Validation loss: 2.076706968328004

Epoch: 6| Step: 10
Training loss: 1.4790611267089844
Validation loss: 2.0813373429800874

Epoch: 6| Step: 11
Training loss: 1.6147253513336182
Validation loss: 2.042733479571599

Epoch: 6| Step: 12
Training loss: 1.1443150043487549
Validation loss: 2.047652330449832

Epoch: 6| Step: 13
Training loss: 0.999613344669342
Validation loss: 2.0375706995687177

Epoch: 540| Step: 0
Training loss: 1.8881938457489014
Validation loss: 2.062546850532614

Epoch: 6| Step: 1
Training loss: 1.9868535995483398
Validation loss: 2.0593349959260676

Epoch: 6| Step: 2
Training loss: 0.914515495300293
Validation loss: 2.055451367491035

Epoch: 6| Step: 3
Training loss: 1.3867876529693604
Validation loss: 2.0658759711891093

Epoch: 6| Step: 4
Training loss: 1.3408238887786865
Validation loss: 2.071342864344197

Epoch: 6| Step: 5
Training loss: 1.6363662481307983
Validation loss: 2.086011012395223

Epoch: 6| Step: 6
Training loss: 1.7265350818634033
Validation loss: 2.101984464994041

Epoch: 6| Step: 7
Training loss: 1.857109546661377
Validation loss: 2.098088042710417

Epoch: 6| Step: 8
Training loss: 1.3558112382888794
Validation loss: 2.1081971609464256

Epoch: 6| Step: 9
Training loss: 1.314127802848816
Validation loss: 2.120536435034967

Epoch: 6| Step: 10
Training loss: 1.1148056983947754
Validation loss: 2.1121413323187057

Epoch: 6| Step: 11
Training loss: 1.1302459239959717
Validation loss: 2.0974265965082313

Epoch: 6| Step: 12
Training loss: 1.3371955156326294
Validation loss: 2.1140816673155753

Epoch: 6| Step: 13
Training loss: 1.8666272163391113
Validation loss: 2.084237748576749

Epoch: 541| Step: 0
Training loss: 1.7129464149475098
Validation loss: 2.068326429654193

Epoch: 6| Step: 1
Training loss: 1.8327131271362305
Validation loss: 2.0845016433346655

Epoch: 6| Step: 2
Training loss: 1.3492892980575562
Validation loss: 2.068875406378059

Epoch: 6| Step: 3
Training loss: 1.793785572052002
Validation loss: 2.0582990697635117

Epoch: 6| Step: 4
Training loss: 1.5171558856964111
Validation loss: 2.025200843811035

Epoch: 6| Step: 5
Training loss: 1.3810040950775146
Validation loss: 2.0305215697134695

Epoch: 6| Step: 6
Training loss: 1.4143035411834717
Validation loss: 2.0376166823089763

Epoch: 6| Step: 7
Training loss: 0.952975869178772
Validation loss: 2.008618011269518

Epoch: 6| Step: 8
Training loss: 1.4051103591918945
Validation loss: 2.0328390547024306

Epoch: 6| Step: 9
Training loss: 2.0697479248046875
Validation loss: 2.0382583756600656

Epoch: 6| Step: 10
Training loss: 1.071758508682251
Validation loss: 2.077287017658193

Epoch: 6| Step: 11
Training loss: 1.465854287147522
Validation loss: 2.0889160940724034

Epoch: 6| Step: 12
Training loss: 1.5052270889282227
Validation loss: 2.055660496475876

Epoch: 6| Step: 13
Training loss: 1.07481849193573
Validation loss: 2.079445054454188

Epoch: 542| Step: 0
Training loss: 1.2371724843978882
Validation loss: 2.1019899178576726

Epoch: 6| Step: 1
Training loss: 1.6260300874710083
Validation loss: 2.070055279680478

Epoch: 6| Step: 2
Training loss: 1.4193792343139648
Validation loss: 2.069133463726249

Epoch: 6| Step: 3
Training loss: 1.0595836639404297
Validation loss: 2.0711556429504068

Epoch: 6| Step: 4
Training loss: 2.071843385696411
Validation loss: 2.02928198922065

Epoch: 6| Step: 5
Training loss: 1.088409185409546
Validation loss: 2.000250526653823

Epoch: 6| Step: 6
Training loss: 1.6873345375061035
Validation loss: 1.9917780686450262

Epoch: 6| Step: 7
Training loss: 2.355973243713379
Validation loss: 1.9839682053494196

Epoch: 6| Step: 8
Training loss: 1.6145960092544556
Validation loss: 1.990694688212487

Epoch: 6| Step: 9
Training loss: 1.7322741746902466
Validation loss: 2.000628671338481

Epoch: 6| Step: 10
Training loss: 1.398911952972412
Validation loss: 1.9653063756163403

Epoch: 6| Step: 11
Training loss: 1.3406437635421753
Validation loss: 2.008447024130052

Epoch: 6| Step: 12
Training loss: 1.0205388069152832
Validation loss: 2.0131574394882366

Epoch: 6| Step: 13
Training loss: 0.774466872215271
Validation loss: 2.0752812995705554

Epoch: 543| Step: 0
Training loss: 1.232922911643982
Validation loss: 2.1296286967492875

Epoch: 6| Step: 1
Training loss: 1.668712854385376
Validation loss: 2.130503721134637

Epoch: 6| Step: 2
Training loss: 1.3256969451904297
Validation loss: 2.113161926628441

Epoch: 6| Step: 3
Training loss: 1.7825576066970825
Validation loss: 2.115814685821533

Epoch: 6| Step: 4
Training loss: 0.910161018371582
Validation loss: 2.0765409020967383

Epoch: 6| Step: 5
Training loss: 1.821645975112915
Validation loss: 2.03861149408484

Epoch: 6| Step: 6
Training loss: 1.1748924255371094
Validation loss: 2.003887264959274

Epoch: 6| Step: 7
Training loss: 2.136718273162842
Validation loss: 2.027390605659895

Epoch: 6| Step: 8
Training loss: 1.5174930095672607
Validation loss: 2.026610110395698

Epoch: 6| Step: 9
Training loss: 1.5223311185836792
Validation loss: 2.008056740606985

Epoch: 6| Step: 10
Training loss: 1.6588990688323975
Validation loss: 1.9838869494776572

Epoch: 6| Step: 11
Training loss: 1.2345836162567139
Validation loss: 2.02962064743042

Epoch: 6| Step: 12
Training loss: 1.3864951133728027
Validation loss: 2.0073736611232964

Epoch: 6| Step: 13
Training loss: 1.239197015762329
Validation loss: 2.0430656658705844

Epoch: 544| Step: 0
Training loss: 1.7137401103973389
Validation loss: 2.0808869408022974

Epoch: 6| Step: 1
Training loss: 1.804107427597046
Validation loss: 2.0999741784987913

Epoch: 6| Step: 2
Training loss: 1.436044692993164
Validation loss: 2.12540235570682

Epoch: 6| Step: 3
Training loss: 2.141249179840088
Validation loss: 2.1434357627745597

Epoch: 6| Step: 4
Training loss: 1.310935616493225
Validation loss: 2.1694864752472087

Epoch: 6| Step: 5
Training loss: 1.6879891157150269
Validation loss: 2.1234285510996336

Epoch: 6| Step: 6
Training loss: 1.5016603469848633
Validation loss: 2.079786298095539

Epoch: 6| Step: 7
Training loss: 1.4604055881500244
Validation loss: 2.0521789891745454

Epoch: 6| Step: 8
Training loss: 1.318992257118225
Validation loss: 2.001658337090605

Epoch: 6| Step: 9
Training loss: 1.3608949184417725
Validation loss: 2.0356200459182903

Epoch: 6| Step: 10
Training loss: 1.5441110134124756
Validation loss: 2.0128147307262627

Epoch: 6| Step: 11
Training loss: 1.2002798318862915
Validation loss: 1.9954544549347253

Epoch: 6| Step: 12
Training loss: 1.1313388347625732
Validation loss: 1.999849480967368

Epoch: 6| Step: 13
Training loss: 1.0908681154251099
Validation loss: 1.9995601023397138

Epoch: 545| Step: 0
Training loss: 1.2017897367477417
Validation loss: 2.0162170266592376

Epoch: 6| Step: 1
Training loss: 0.6033172607421875
Validation loss: 2.060822286913472

Epoch: 6| Step: 2
Training loss: 1.3591067790985107
Validation loss: 2.088985381587859

Epoch: 6| Step: 3
Training loss: 1.3898124694824219
Validation loss: 2.0735186223060853

Epoch: 6| Step: 4
Training loss: 0.9828367233276367
Validation loss: 2.1085224548975625

Epoch: 6| Step: 5
Training loss: 1.2810529470443726
Validation loss: 2.121146518697021

Epoch: 6| Step: 6
Training loss: 1.9008125066757202
Validation loss: 2.115231124303674

Epoch: 6| Step: 7
Training loss: 1.6460270881652832
Validation loss: 2.0725401037482807

Epoch: 6| Step: 8
Training loss: 1.803292989730835
Validation loss: 2.075132054667319

Epoch: 6| Step: 9
Training loss: 1.2300825119018555
Validation loss: 2.0850366943625995

Epoch: 6| Step: 10
Training loss: 2.381269931793213
Validation loss: 2.0784468548272246

Epoch: 6| Step: 11
Training loss: 2.2432937622070312
Validation loss: 2.066294659850418

Epoch: 6| Step: 12
Training loss: 1.0859458446502686
Validation loss: 2.068398288501206

Epoch: 6| Step: 13
Training loss: 1.311727523803711
Validation loss: 2.051574045611966

Epoch: 546| Step: 0
Training loss: 1.049257516860962
Validation loss: 2.01030876944142

Epoch: 6| Step: 1
Training loss: 1.5180211067199707
Validation loss: 2.023355307117585

Epoch: 6| Step: 2
Training loss: 1.3397464752197266
Validation loss: 1.9935771778065672

Epoch: 6| Step: 3
Training loss: 1.9373606443405151
Validation loss: 2.022454737335123

Epoch: 6| Step: 4
Training loss: 1.6766955852508545
Validation loss: 2.0023421343936714

Epoch: 6| Step: 5
Training loss: 1.3924672603607178
Validation loss: 2.045528914338799

Epoch: 6| Step: 6
Training loss: 0.8278799057006836
Validation loss: 2.071702085515504

Epoch: 6| Step: 7
Training loss: 1.4948785305023193
Validation loss: 2.030724661324614

Epoch: 6| Step: 8
Training loss: 1.895876169204712
Validation loss: 2.087139992303746

Epoch: 6| Step: 9
Training loss: 1.8703885078430176
Validation loss: 2.0836079043726765

Epoch: 6| Step: 10
Training loss: 0.9286978840827942
Validation loss: 2.0997160621868667

Epoch: 6| Step: 11
Training loss: 1.871593952178955
Validation loss: 2.127546013042491

Epoch: 6| Step: 12
Training loss: 1.3331586122512817
Validation loss: 2.0966247371447984

Epoch: 6| Step: 13
Training loss: 1.674692153930664
Validation loss: 2.040219977337827

Epoch: 547| Step: 0
Training loss: 1.4391855001449585
Validation loss: 2.011971689039661

Epoch: 6| Step: 1
Training loss: 1.4213509559631348
Validation loss: 2.034632731509465

Epoch: 6| Step: 2
Training loss: 1.7536327838897705
Validation loss: 2.025053729293167

Epoch: 6| Step: 3
Training loss: 1.4541784524917603
Validation loss: 2.009531336445962

Epoch: 6| Step: 4
Training loss: 1.3616154193878174
Validation loss: 2.036704609470983

Epoch: 6| Step: 5
Training loss: 1.754495620727539
Validation loss: 2.0484283970248316

Epoch: 6| Step: 6
Training loss: 1.1910728216171265
Validation loss: 2.046922960589009

Epoch: 6| Step: 7
Training loss: 1.8245527744293213
Validation loss: 2.04250903027032

Epoch: 6| Step: 8
Training loss: 0.549208402633667
Validation loss: 2.041259173424013

Epoch: 6| Step: 9
Training loss: 1.1149190664291382
Validation loss: 2.080600525743218

Epoch: 6| Step: 10
Training loss: 1.502933144569397
Validation loss: 2.0801508580484698

Epoch: 6| Step: 11
Training loss: 1.8577048778533936
Validation loss: 2.088118557007082

Epoch: 6| Step: 12
Training loss: 1.1240168809890747
Validation loss: 2.0959090519976873

Epoch: 6| Step: 13
Training loss: 2.2168142795562744
Validation loss: 2.1052522402937695

Epoch: 548| Step: 0
Training loss: 1.1644794940948486
Validation loss: 2.1082045109041276

Epoch: 6| Step: 1
Training loss: 1.2231372594833374
Validation loss: 2.104028009599255

Epoch: 6| Step: 2
Training loss: 1.2045800685882568
Validation loss: 2.088718027196905

Epoch: 6| Step: 3
Training loss: 1.9910156726837158
Validation loss: 2.0552006613823677

Epoch: 6| Step: 4
Training loss: 0.8265482783317566
Validation loss: 2.064686285552158

Epoch: 6| Step: 5
Training loss: 1.8112200498580933
Validation loss: 2.066798040943761

Epoch: 6| Step: 6
Training loss: 1.1180988550186157
Validation loss: 2.059506716266755

Epoch: 6| Step: 7
Training loss: 1.6041955947875977
Validation loss: 2.036448573553434

Epoch: 6| Step: 8
Training loss: 2.0189332962036133
Validation loss: 2.0244122782061176

Epoch: 6| Step: 9
Training loss: 1.1788790225982666
Validation loss: 2.0550739739530828

Epoch: 6| Step: 10
Training loss: 2.0687572956085205
Validation loss: 2.033413456332299

Epoch: 6| Step: 11
Training loss: 1.6003254652023315
Validation loss: 2.040750959868072

Epoch: 6| Step: 12
Training loss: 0.7821901440620422
Validation loss: 2.0405245955272386

Epoch: 6| Step: 13
Training loss: 2.2984113693237305
Validation loss: 2.0597449246273247

Epoch: 549| Step: 0
Training loss: 1.8185691833496094
Validation loss: 2.0729794873986194

Epoch: 6| Step: 1
Training loss: 1.819854497909546
Validation loss: 2.0565685700344782

Epoch: 6| Step: 2
Training loss: 0.9235328435897827
Validation loss: 2.1078368181823404

Epoch: 6| Step: 3
Training loss: 1.1311428546905518
Validation loss: 2.0772084741182226

Epoch: 6| Step: 4
Training loss: 1.0531103610992432
Validation loss: 2.059113206401948

Epoch: 6| Step: 5
Training loss: 1.2383614778518677
Validation loss: 2.0704284739750687

Epoch: 6| Step: 6
Training loss: 1.5568541288375854
Validation loss: 2.0857971816934566

Epoch: 6| Step: 7
Training loss: 1.2922120094299316
Validation loss: 2.0412366172318817

Epoch: 6| Step: 8
Training loss: 1.6149873733520508
Validation loss: 2.07205109442434

Epoch: 6| Step: 9
Training loss: 1.9015358686447144
Validation loss: 2.066040958127668

Epoch: 6| Step: 10
Training loss: 1.4212350845336914
Validation loss: 2.049245967659899

Epoch: 6| Step: 11
Training loss: 1.4775264263153076
Validation loss: 2.087562809708298

Epoch: 6| Step: 12
Training loss: 1.388020396232605
Validation loss: 2.0803053276513213

Epoch: 6| Step: 13
Training loss: 1.9359164237976074
Validation loss: 2.065473715464274

Epoch: 550| Step: 0
Training loss: 0.745564341545105
Validation loss: 2.0853844688784693

Epoch: 6| Step: 1
Training loss: 1.1393959522247314
Validation loss: 2.084998433307935

Epoch: 6| Step: 2
Training loss: 1.4153319597244263
Validation loss: 2.0929024398967786

Epoch: 6| Step: 3
Training loss: 1.3652558326721191
Validation loss: 2.095510349478773

Epoch: 6| Step: 4
Training loss: 1.5904054641723633
Validation loss: 2.066224617342795

Epoch: 6| Step: 5
Training loss: 1.0071126222610474
Validation loss: 2.054514008183633

Epoch: 6| Step: 6
Training loss: 1.8425283432006836
Validation loss: 2.0593801416376585

Epoch: 6| Step: 7
Training loss: 1.3723084926605225
Validation loss: 2.0059760052670716

Epoch: 6| Step: 8
Training loss: 1.6551084518432617
Validation loss: 2.008641060962472

Epoch: 6| Step: 9
Training loss: 1.3699581623077393
Validation loss: 1.9928632000441193

Epoch: 6| Step: 10
Training loss: 1.3359453678131104
Validation loss: 1.996961175754506

Epoch: 6| Step: 11
Training loss: 2.0214507579803467
Validation loss: 1.9918504312474241

Epoch: 6| Step: 12
Training loss: 1.8908586502075195
Validation loss: 1.99301665829074

Epoch: 6| Step: 13
Training loss: 1.527549386024475
Validation loss: 2.01042983224315

Testing loss: 2.1826990922292073

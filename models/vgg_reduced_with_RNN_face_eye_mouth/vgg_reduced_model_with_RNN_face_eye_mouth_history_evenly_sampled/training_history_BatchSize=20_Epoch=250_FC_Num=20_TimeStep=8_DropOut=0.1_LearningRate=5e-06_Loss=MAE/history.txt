Epoch: 1| Step: 0
Training loss: 5.127603530883789
Validation loss: 5.1292824463177755

Epoch: 5| Step: 1
Training loss: 4.828183174133301
Validation loss: 5.1252227598621

Epoch: 5| Step: 2
Training loss: 5.274352550506592
Validation loss: 5.1206657912141536

Epoch: 5| Step: 3
Training loss: 4.898985862731934
Validation loss: 5.116505966391615

Epoch: 5| Step: 4
Training loss: 6.003841400146484
Validation loss: 5.112326478445402

Epoch: 5| Step: 5
Training loss: 5.013116359710693
Validation loss: 5.108127732430735

Epoch: 5| Step: 6
Training loss: 4.787946701049805
Validation loss: 5.103056964053903

Epoch: 5| Step: 7
Training loss: 4.612271308898926
Validation loss: 5.098584503255864

Epoch: 5| Step: 8
Training loss: 4.7360758781433105
Validation loss: 5.093882232583979

Epoch: 5| Step: 9
Training loss: 4.517610549926758
Validation loss: 5.088810772024175

Epoch: 5| Step: 10
Training loss: 4.015544891357422
Validation loss: 5.083582570475917

Epoch: 2| Step: 0
Training loss: 5.225048065185547
Validation loss: 5.078054663955524

Epoch: 5| Step: 1
Training loss: 5.1936140060424805
Validation loss: 5.072291943334764

Epoch: 5| Step: 2
Training loss: 5.961939811706543
Validation loss: 5.066450595855713

Epoch: 5| Step: 3
Training loss: 3.900216579437256
Validation loss: 5.060830828964069

Epoch: 5| Step: 4
Training loss: 5.55364990234375
Validation loss: 5.053759933799825

Epoch: 5| Step: 5
Training loss: 5.022674083709717
Validation loss: 5.0475678136271815

Epoch: 5| Step: 6
Training loss: 4.251210689544678
Validation loss: 5.040729050995202

Epoch: 5| Step: 7
Training loss: 3.8618178367614746
Validation loss: 5.033490944934147

Epoch: 5| Step: 8
Training loss: 5.179867744445801
Validation loss: 5.026348114013672

Epoch: 5| Step: 9
Training loss: 4.503025054931641
Validation loss: 5.018390142789451

Epoch: 5| Step: 10
Training loss: 4.575260639190674
Validation loss: 5.010509398675734

Epoch: 3| Step: 0
Training loss: 4.866388320922852
Validation loss: 5.002237304564445

Epoch: 5| Step: 1
Training loss: 3.6630096435546875
Validation loss: 4.993267490017798

Epoch: 5| Step: 2
Training loss: 5.383025169372559
Validation loss: 4.984375215345813

Epoch: 5| Step: 3
Training loss: 3.883695125579834
Validation loss: 4.974544248273296

Epoch: 5| Step: 4
Training loss: 5.899090766906738
Validation loss: 4.965161882421022

Epoch: 5| Step: 5
Training loss: 3.3731250762939453
Validation loss: 4.954164822896321

Epoch: 5| Step: 6
Training loss: 5.434386253356934
Validation loss: 4.944123529618786

Epoch: 5| Step: 7
Training loss: 4.647149562835693
Validation loss: 4.933437726830923

Epoch: 5| Step: 8
Training loss: 5.171379566192627
Validation loss: 4.921096750485

Epoch: 5| Step: 9
Training loss: 4.7015790939331055
Validation loss: 4.909172273451282

Epoch: 5| Step: 10
Training loss: 5.254437446594238
Validation loss: 4.8963622944329375

Epoch: 4| Step: 0
Training loss: 5.995225429534912
Validation loss: 4.882720226882606

Epoch: 5| Step: 1
Training loss: 4.171534538269043
Validation loss: 4.868580136247861

Epoch: 5| Step: 2
Training loss: 5.1771979331970215
Validation loss: 4.854775567208567

Epoch: 5| Step: 3
Training loss: 4.714604377746582
Validation loss: 4.8387674208610285

Epoch: 5| Step: 4
Training loss: 4.720272064208984
Validation loss: 4.823203835436093

Epoch: 5| Step: 5
Training loss: 4.749711036682129
Validation loss: 4.807144811076503

Epoch: 5| Step: 6
Training loss: 3.547696590423584
Validation loss: 4.790056495256321

Epoch: 5| Step: 7
Training loss: 3.393160343170166
Validation loss: 4.771740349390173

Epoch: 5| Step: 8
Training loss: 5.744913578033447
Validation loss: 4.754469630538776

Epoch: 5| Step: 9
Training loss: 4.00205135345459
Validation loss: 4.734946199642715

Epoch: 5| Step: 10
Training loss: 4.236913204193115
Validation loss: 4.71561848732733

Epoch: 5| Step: 0
Training loss: 4.464341640472412
Validation loss: 4.6960647952172065

Epoch: 5| Step: 1
Training loss: 4.893069267272949
Validation loss: 4.6752133266900175

Epoch: 5| Step: 2
Training loss: 3.471979856491089
Validation loss: 4.653154303950648

Epoch: 5| Step: 3
Training loss: 3.5595264434814453
Validation loss: 4.63090164943408

Epoch: 5| Step: 4
Training loss: 5.14056396484375
Validation loss: 4.609089482215143

Epoch: 5| Step: 5
Training loss: 4.116813659667969
Validation loss: 4.586970442084856

Epoch: 5| Step: 6
Training loss: 4.681203365325928
Validation loss: 4.563783158538162

Epoch: 5| Step: 7
Training loss: 4.57395076751709
Validation loss: 4.538639655677221

Epoch: 5| Step: 8
Training loss: 3.9965500831604004
Validation loss: 4.514053283199187

Epoch: 5| Step: 9
Training loss: 3.872131824493408
Validation loss: 4.487909527235134

Epoch: 5| Step: 10
Training loss: 5.4543046951293945
Validation loss: 4.461136735895629

Epoch: 6| Step: 0
Training loss: 3.442729949951172
Validation loss: 4.433421145203293

Epoch: 5| Step: 1
Training loss: 3.5175373554229736
Validation loss: 4.408113987215104

Epoch: 5| Step: 2
Training loss: 3.9045586585998535
Validation loss: 4.3797628084818525

Epoch: 5| Step: 3
Training loss: 3.7351584434509277
Validation loss: 4.351186865119524

Epoch: 5| Step: 4
Training loss: 4.357579231262207
Validation loss: 4.3224678167732815

Epoch: 5| Step: 5
Training loss: 3.976292371749878
Validation loss: 4.294039639093542

Epoch: 5| Step: 6
Training loss: 5.005364418029785
Validation loss: 4.263039358200565

Epoch: 5| Step: 7
Training loss: 4.510027885437012
Validation loss: 4.2326588733221895

Epoch: 5| Step: 8
Training loss: 4.181324005126953
Validation loss: 4.202831863075175

Epoch: 5| Step: 9
Training loss: 4.204415798187256
Validation loss: 4.172834962926885

Epoch: 5| Step: 10
Training loss: 4.173757076263428
Validation loss: 4.144815032200147

Epoch: 7| Step: 0
Training loss: 3.860745668411255
Validation loss: 4.114600781471498

Epoch: 5| Step: 1
Training loss: 4.695220947265625
Validation loss: 4.0860494311137865

Epoch: 5| Step: 2
Training loss: 3.450972080230713
Validation loss: 4.057643613507671

Epoch: 5| Step: 3
Training loss: 4.609911918640137
Validation loss: 4.032520617208173

Epoch: 5| Step: 4
Training loss: 4.702091217041016
Validation loss: 4.0063546242252475

Epoch: 5| Step: 5
Training loss: 4.696408271789551
Validation loss: 3.9799802457132647

Epoch: 5| Step: 6
Training loss: 3.8043739795684814
Validation loss: 3.9578116504094933

Epoch: 5| Step: 7
Training loss: 2.631621837615967
Validation loss: 3.935256883662234

Epoch: 5| Step: 8
Training loss: 3.8895041942596436
Validation loss: 3.9140086456011702

Epoch: 5| Step: 9
Training loss: 3.0469093322753906
Validation loss: 3.893973048015307

Epoch: 5| Step: 10
Training loss: 2.7290139198303223
Validation loss: 3.8758689024115123

Epoch: 8| Step: 0
Training loss: 4.376640319824219
Validation loss: 3.857206247186148

Epoch: 5| Step: 1
Training loss: 4.365988254547119
Validation loss: 3.8407579032323693

Epoch: 5| Step: 2
Training loss: 2.6230106353759766
Validation loss: 3.8249389638182936

Epoch: 5| Step: 3
Training loss: 4.143957614898682
Validation loss: 3.807509360774871

Epoch: 5| Step: 4
Training loss: 3.8382983207702637
Validation loss: 3.7916169833111506

Epoch: 5| Step: 5
Training loss: 3.291954755783081
Validation loss: 3.775798566879765

Epoch: 5| Step: 6
Training loss: 3.2251529693603516
Validation loss: 3.760726821038031

Epoch: 5| Step: 7
Training loss: 3.7457778453826904
Validation loss: 3.745768985440654

Epoch: 5| Step: 8
Training loss: 3.957233428955078
Validation loss: 3.730861520254484

Epoch: 5| Step: 9
Training loss: 3.0520129203796387
Validation loss: 3.71881958746141

Epoch: 5| Step: 10
Training loss: 3.6819472312927246
Validation loss: 3.705484644059212

Epoch: 9| Step: 0
Training loss: 2.47078013420105
Validation loss: 3.6941907072579987

Epoch: 5| Step: 1
Training loss: 3.7539970874786377
Validation loss: 3.6811794388678765

Epoch: 5| Step: 2
Training loss: 4.0122857093811035
Validation loss: 3.6700324422569683

Epoch: 5| Step: 3
Training loss: 3.6408591270446777
Validation loss: 3.65777862200173

Epoch: 5| Step: 4
Training loss: 3.5575623512268066
Validation loss: 3.644678931082449

Epoch: 5| Step: 5
Training loss: 3.3744800090789795
Validation loss: 3.633566830747871

Epoch: 5| Step: 6
Training loss: 4.86940860748291
Validation loss: 3.619039217631022

Epoch: 5| Step: 7
Training loss: 3.4842917919158936
Validation loss: 3.60918422924575

Epoch: 5| Step: 8
Training loss: 4.181503772735596
Validation loss: 3.5964369081681773

Epoch: 5| Step: 9
Training loss: 2.497493267059326
Validation loss: 3.585050754649665

Epoch: 5| Step: 10
Training loss: 3.053081750869751
Validation loss: 3.5738681516339703

Epoch: 10| Step: 0
Training loss: 2.9512627124786377
Validation loss: 3.5625269823176886

Epoch: 5| Step: 1
Training loss: 3.671651840209961
Validation loss: 3.551313231068273

Epoch: 5| Step: 2
Training loss: 3.877302646636963
Validation loss: 3.5417094615197953

Epoch: 5| Step: 3
Training loss: 3.511324405670166
Validation loss: 3.5324607331265687

Epoch: 5| Step: 4
Training loss: 4.042440891265869
Validation loss: 3.521537483379405

Epoch: 5| Step: 5
Training loss: 3.448561191558838
Validation loss: 3.510281632023473

Epoch: 5| Step: 6
Training loss: 3.3274383544921875
Validation loss: 3.500544132724885

Epoch: 5| Step: 7
Training loss: 3.2659554481506348
Validation loss: 3.4875011649183048

Epoch: 5| Step: 8
Training loss: 3.188375949859619
Validation loss: 3.4772755561336393

Epoch: 5| Step: 9
Training loss: 3.0719451904296875
Validation loss: 3.467315714846375

Epoch: 5| Step: 10
Training loss: 3.48929500579834
Validation loss: 3.458468988377561

Epoch: 11| Step: 0
Training loss: 3.124267578125
Validation loss: 3.4487444072641353

Epoch: 5| Step: 1
Training loss: 1.5718185901641846
Validation loss: 3.438610794723675

Epoch: 5| Step: 2
Training loss: 3.1292195320129395
Validation loss: 3.428875582192534

Epoch: 5| Step: 3
Training loss: 4.266974449157715
Validation loss: 3.416639579239712

Epoch: 5| Step: 4
Training loss: 3.094414234161377
Validation loss: 3.4062523534221034

Epoch: 5| Step: 5
Training loss: 3.7316155433654785
Validation loss: 3.3947079668762865

Epoch: 5| Step: 6
Training loss: 3.39690899848938
Validation loss: 3.3802080231328167

Epoch: 5| Step: 7
Training loss: 3.3108150959014893
Validation loss: 3.3692347259931665

Epoch: 5| Step: 8
Training loss: 3.8477065563201904
Validation loss: 3.362364140889978

Epoch: 5| Step: 9
Training loss: 3.9244415760040283
Validation loss: 3.3504503619286323

Epoch: 5| Step: 10
Training loss: 3.367067813873291
Validation loss: 3.3389447991565993

Epoch: 12| Step: 0
Training loss: 2.6726748943328857
Validation loss: 3.3315959566382953

Epoch: 5| Step: 1
Training loss: 3.163747549057007
Validation loss: 3.3228888101475214

Epoch: 5| Step: 2
Training loss: 4.058539390563965
Validation loss: 3.3135903676350913

Epoch: 5| Step: 3
Training loss: 2.964136838912964
Validation loss: 3.3049431231714066

Epoch: 5| Step: 4
Training loss: 3.56506085395813
Validation loss: 3.2949849482505553

Epoch: 5| Step: 5
Training loss: 2.3842411041259766
Validation loss: 3.28709852310919

Epoch: 5| Step: 6
Training loss: 3.0072171688079834
Validation loss: 3.2775627643831315

Epoch: 5| Step: 7
Training loss: 3.4396088123321533
Validation loss: 3.272411133653374

Epoch: 5| Step: 8
Training loss: 3.9677982330322266
Validation loss: 3.2648574588119343

Epoch: 5| Step: 9
Training loss: 3.430224895477295
Validation loss: 3.2572325019426245

Epoch: 5| Step: 10
Training loss: 3.2202048301696777
Validation loss: 3.254384422814974

Epoch: 13| Step: 0
Training loss: 3.3240160942077637
Validation loss: 3.244635269206057

Epoch: 5| Step: 1
Training loss: 3.9093832969665527
Validation loss: 3.238154965062295

Epoch: 5| Step: 2
Training loss: 3.077756404876709
Validation loss: 3.234321471183531

Epoch: 5| Step: 3
Training loss: 3.507925510406494
Validation loss: 3.2273745280440136

Epoch: 5| Step: 4
Training loss: 2.3618733882904053
Validation loss: 3.2224788511953046

Epoch: 5| Step: 5
Training loss: 3.240588665008545
Validation loss: 3.2166948703027542

Epoch: 5| Step: 6
Training loss: 3.326296329498291
Validation loss: 3.2106617419950423

Epoch: 5| Step: 7
Training loss: 3.5878090858459473
Validation loss: 3.20569489079137

Epoch: 5| Step: 8
Training loss: 3.0936577320098877
Validation loss: 3.2009479179177234

Epoch: 5| Step: 9
Training loss: 2.5742645263671875
Validation loss: 3.1957751550982074

Epoch: 5| Step: 10
Training loss: 3.3107755184173584
Validation loss: 3.1912624707785984

Epoch: 14| Step: 0
Training loss: 2.9689505100250244
Validation loss: 3.1856900543294926

Epoch: 5| Step: 1
Training loss: 3.126636028289795
Validation loss: 3.1814502516100482

Epoch: 5| Step: 2
Training loss: 2.8718008995056152
Validation loss: 3.177598609719225

Epoch: 5| Step: 3
Training loss: 2.792293071746826
Validation loss: 3.171557669998497

Epoch: 5| Step: 4
Training loss: 3.4283764362335205
Validation loss: 3.166440922726867

Epoch: 5| Step: 5
Training loss: 2.9081778526306152
Validation loss: 3.1622736633464856

Epoch: 5| Step: 6
Training loss: 2.139347553253174
Validation loss: 3.15631680334768

Epoch: 5| Step: 7
Training loss: 3.5564026832580566
Validation loss: 3.1524743905631443

Epoch: 5| Step: 8
Training loss: 3.6950206756591797
Validation loss: 3.1476539360579623

Epoch: 5| Step: 9
Training loss: 3.9175994396209717
Validation loss: 3.144082489834037

Epoch: 5| Step: 10
Training loss: 3.4787325859069824
Validation loss: 3.137460552236085

Epoch: 15| Step: 0
Training loss: 2.9773738384246826
Validation loss: 3.133010518166327

Epoch: 5| Step: 1
Training loss: 2.6785216331481934
Validation loss: 3.128108339924966

Epoch: 5| Step: 2
Training loss: 3.529473066329956
Validation loss: 3.1247279900376514

Epoch: 5| Step: 3
Training loss: 3.0890650749206543
Validation loss: 3.118028733038133

Epoch: 5| Step: 4
Training loss: 2.1414027214050293
Validation loss: 3.11433074807608

Epoch: 5| Step: 5
Training loss: 3.0725150108337402
Validation loss: 3.1102828774400937

Epoch: 5| Step: 6
Training loss: 4.143557548522949
Validation loss: 3.103345283897974

Epoch: 5| Step: 7
Training loss: 3.3312439918518066
Validation loss: 3.1009107071866273

Epoch: 5| Step: 8
Training loss: 3.0809853076934814
Validation loss: 3.0943961733131

Epoch: 5| Step: 9
Training loss: 3.585521697998047
Validation loss: 3.0966089079456944

Epoch: 5| Step: 10
Training loss: 2.7281932830810547
Validation loss: 3.0880254109700522

Epoch: 16| Step: 0
Training loss: 2.044321298599243
Validation loss: 3.0815615782173733

Epoch: 5| Step: 1
Training loss: 3.585843563079834
Validation loss: 3.0794553423440583

Epoch: 5| Step: 2
Training loss: 3.308328628540039
Validation loss: 3.0707527950245845

Epoch: 5| Step: 3
Training loss: 3.3662631511688232
Validation loss: 3.0665909269804597

Epoch: 5| Step: 4
Training loss: 2.852515697479248
Validation loss: 3.064248646459272

Epoch: 5| Step: 5
Training loss: 3.601377010345459
Validation loss: 3.0614481485018166

Epoch: 5| Step: 6
Training loss: 3.600010633468628
Validation loss: 3.053637096958776

Epoch: 5| Step: 7
Training loss: 2.001420736312866
Validation loss: 3.0486418713805494

Epoch: 5| Step: 8
Training loss: 2.947887420654297
Validation loss: 3.0466745950842418

Epoch: 5| Step: 9
Training loss: 3.3949227333068848
Validation loss: 3.045452687048143

Epoch: 5| Step: 10
Training loss: 3.4448089599609375
Validation loss: 3.0443498857559694

Epoch: 17| Step: 0
Training loss: 3.6470909118652344
Validation loss: 3.0303603577357467

Epoch: 5| Step: 1
Training loss: 3.181741952896118
Validation loss: 3.0472349274543022

Epoch: 5| Step: 2
Training loss: 2.894955635070801
Validation loss: 3.052732421505836

Epoch: 5| Step: 3
Training loss: 2.4979262351989746
Validation loss: 3.0385854398050616

Epoch: 5| Step: 4
Training loss: 2.51314115524292
Validation loss: 3.0257307739667993

Epoch: 5| Step: 5
Training loss: 2.549041271209717
Validation loss: 3.0176114702737458

Epoch: 5| Step: 6
Training loss: 3.4704456329345703
Validation loss: 3.013626306287704

Epoch: 5| Step: 7
Training loss: 3.0368571281433105
Validation loss: 3.010414805463565

Epoch: 5| Step: 8
Training loss: 3.1356942653656006
Validation loss: 3.006882259922643

Epoch: 5| Step: 9
Training loss: 4.26308536529541
Validation loss: 3.003382052144697

Epoch: 5| Step: 10
Training loss: 2.5688698291778564
Validation loss: 2.997783525015718

Epoch: 18| Step: 0
Training loss: 2.261439085006714
Validation loss: 2.993154933375697

Epoch: 5| Step: 1
Training loss: 3.16424298286438
Validation loss: 2.9936786185028734

Epoch: 5| Step: 2
Training loss: 2.6179490089416504
Validation loss: 2.9894262488170336

Epoch: 5| Step: 3
Training loss: 3.2656197547912598
Validation loss: 2.98876025343454

Epoch: 5| Step: 4
Training loss: 3.14329195022583
Validation loss: 2.983973382621683

Epoch: 5| Step: 5
Training loss: 3.46931529045105
Validation loss: 2.9793043598051994

Epoch: 5| Step: 6
Training loss: 3.1867880821228027
Validation loss: 2.9756377486772436

Epoch: 5| Step: 7
Training loss: 2.8482730388641357
Validation loss: 2.973040178257932

Epoch: 5| Step: 8
Training loss: 3.194284200668335
Validation loss: 2.974269018378309

Epoch: 5| Step: 9
Training loss: 3.6589083671569824
Validation loss: 2.972036023293772

Epoch: 5| Step: 10
Training loss: 2.6761488914489746
Validation loss: 2.9646380947482203

Epoch: 19| Step: 0
Training loss: 2.5012478828430176
Validation loss: 2.9626039945951073

Epoch: 5| Step: 1
Training loss: 3.1225426197052
Validation loss: 2.9625071043609292

Epoch: 5| Step: 2
Training loss: 3.4248435497283936
Validation loss: 2.95884249799995

Epoch: 5| Step: 3
Training loss: 3.378159761428833
Validation loss: 2.9510888002252065

Epoch: 5| Step: 4
Training loss: 3.1368095874786377
Validation loss: 2.9458667693599576

Epoch: 5| Step: 5
Training loss: 2.4921388626098633
Validation loss: 2.945323495454686

Epoch: 5| Step: 6
Training loss: 3.5065536499023438
Validation loss: 2.946716646994314

Epoch: 5| Step: 7
Training loss: 1.614392638206482
Validation loss: 2.9435427522146576

Epoch: 5| Step: 8
Training loss: 3.631706714630127
Validation loss: 2.946009810252856

Epoch: 5| Step: 9
Training loss: 2.787189483642578
Validation loss: 2.946795848108107

Epoch: 5| Step: 10
Training loss: 3.84664249420166
Validation loss: 2.94971570404627

Epoch: 20| Step: 0
Training loss: 3.009517192840576
Validation loss: 2.934438310643678

Epoch: 5| Step: 1
Training loss: 2.3119940757751465
Validation loss: 2.924231021634994

Epoch: 5| Step: 2
Training loss: 3.0027899742126465
Validation loss: 2.9269102927177184

Epoch: 5| Step: 3
Training loss: 3.239354372024536
Validation loss: 2.926685897252893

Epoch: 5| Step: 4
Training loss: 3.063293933868408
Validation loss: 2.9214579495050574

Epoch: 5| Step: 5
Training loss: 3.445716381072998
Validation loss: 2.9193244595681467

Epoch: 5| Step: 6
Training loss: 3.2215142250061035
Validation loss: 2.9167929746771373

Epoch: 5| Step: 7
Training loss: 2.153144359588623
Validation loss: 2.913466489443215

Epoch: 5| Step: 8
Training loss: 3.3180243968963623
Validation loss: 2.9097763902397564

Epoch: 5| Step: 9
Training loss: 3.2999320030212402
Validation loss: 2.907870546463997

Epoch: 5| Step: 10
Training loss: 3.0350632667541504
Validation loss: 2.9050908755230647

Epoch: 21| Step: 0
Training loss: 2.627528667449951
Validation loss: 2.9034874080329813

Epoch: 5| Step: 1
Training loss: 3.3503875732421875
Validation loss: 2.89888959546243

Epoch: 5| Step: 2
Training loss: 2.8875250816345215
Validation loss: 2.8964049277767057

Epoch: 5| Step: 3
Training loss: 3.02139949798584
Validation loss: 2.893604037582233

Epoch: 5| Step: 4
Training loss: 2.804680109024048
Validation loss: 2.889519173611877

Epoch: 5| Step: 5
Training loss: 3.1448445320129395
Validation loss: 2.888207002352643

Epoch: 5| Step: 6
Training loss: 2.6922855377197266
Validation loss: 2.886495215918428

Epoch: 5| Step: 7
Training loss: 3.137766122817993
Validation loss: 2.883046914172429

Epoch: 5| Step: 8
Training loss: 3.0846962928771973
Validation loss: 2.882087679319484

Epoch: 5| Step: 9
Training loss: 3.1261162757873535
Validation loss: 2.8801195134398756

Epoch: 5| Step: 10
Training loss: 2.9541149139404297
Validation loss: 2.872901006411481

Epoch: 22| Step: 0
Training loss: 3.414452075958252
Validation loss: 2.866305233329855

Epoch: 5| Step: 1
Training loss: 3.2757363319396973
Validation loss: 2.863464493905344

Epoch: 5| Step: 2
Training loss: 3.1711113452911377
Validation loss: 2.866190984684934

Epoch: 5| Step: 3
Training loss: 3.297234296798706
Validation loss: 2.865831962195776

Epoch: 5| Step: 4
Training loss: 3.123544454574585
Validation loss: 2.8566773168502317

Epoch: 5| Step: 5
Training loss: 3.3202109336853027
Validation loss: 2.8517411447340444

Epoch: 5| Step: 6
Training loss: 2.5974948406219482
Validation loss: 2.854495512541904

Epoch: 5| Step: 7
Training loss: 2.2933881282806396
Validation loss: 2.85631666644927

Epoch: 5| Step: 8
Training loss: 2.8031058311462402
Validation loss: 2.8669569723067747

Epoch: 5| Step: 9
Training loss: 2.4619219303131104
Validation loss: 2.849912756232805

Epoch: 5| Step: 10
Training loss: 2.8641772270202637
Validation loss: 2.839250681220844

Epoch: 23| Step: 0
Training loss: 3.3735382556915283
Validation loss: 2.833992983705254

Epoch: 5| Step: 1
Training loss: 2.732724189758301
Validation loss: 2.8325298088853077

Epoch: 5| Step: 2
Training loss: 3.150033473968506
Validation loss: 2.8305520114078315

Epoch: 5| Step: 3
Training loss: 3.591709852218628
Validation loss: 2.8268487530369915

Epoch: 5| Step: 4
Training loss: 2.705756902694702
Validation loss: 2.8243827076368433

Epoch: 5| Step: 5
Training loss: 2.9808712005615234
Validation loss: 2.822995106379191

Epoch: 5| Step: 6
Training loss: 3.0747809410095215
Validation loss: 2.820792072562761

Epoch: 5| Step: 7
Training loss: 2.368311882019043
Validation loss: 2.818579361002932

Epoch: 5| Step: 8
Training loss: 2.6848106384277344
Validation loss: 2.8164360087404967

Epoch: 5| Step: 9
Training loss: 2.670755624771118
Validation loss: 2.8102739087996946

Epoch: 5| Step: 10
Training loss: 3.06496524810791
Validation loss: 2.811235281728929

Epoch: 24| Step: 0
Training loss: 2.6516551971435547
Validation loss: 2.8107499461020193

Epoch: 5| Step: 1
Training loss: 3.5912296772003174
Validation loss: 2.8150141905712824

Epoch: 5| Step: 2
Training loss: 3.209975481033325
Validation loss: 2.806669489029915

Epoch: 5| Step: 3
Training loss: 3.1420416831970215
Validation loss: 2.810151087340488

Epoch: 5| Step: 4
Training loss: 2.4281039237976074
Validation loss: 2.8075394553522908

Epoch: 5| Step: 5
Training loss: 2.953831434249878
Validation loss: 2.805450852199267

Epoch: 5| Step: 6
Training loss: 2.704498052597046
Validation loss: 2.799689849217733

Epoch: 5| Step: 7
Training loss: 3.058788776397705
Validation loss: 2.800521332730529

Epoch: 5| Step: 8
Training loss: 3.2830169200897217
Validation loss: 2.8028660333284767

Epoch: 5| Step: 9
Training loss: 3.0627570152282715
Validation loss: 2.789605822614444

Epoch: 5| Step: 10
Training loss: 1.9574241638183594
Validation loss: 2.788854049098107

Epoch: 25| Step: 0
Training loss: 3.2631702423095703
Validation loss: 2.802113832965974

Epoch: 5| Step: 1
Training loss: 2.215573787689209
Validation loss: 2.806886347391272

Epoch: 5| Step: 2
Training loss: 2.260206460952759
Validation loss: 2.7916633570066063

Epoch: 5| Step: 3
Training loss: 2.601994276046753
Validation loss: 2.7819044461814304

Epoch: 5| Step: 4
Training loss: 3.4522786140441895
Validation loss: 2.7812373227970575

Epoch: 5| Step: 5
Training loss: 3.3631491661071777
Validation loss: 2.779373840619159

Epoch: 5| Step: 6
Training loss: 2.706838607788086
Validation loss: 2.7765117845227643

Epoch: 5| Step: 7
Training loss: 2.187648057937622
Validation loss: 2.777719977081463

Epoch: 5| Step: 8
Training loss: 3.219778060913086
Validation loss: 2.77075445523826

Epoch: 5| Step: 9
Training loss: 3.374128818511963
Validation loss: 2.768774837575933

Epoch: 5| Step: 10
Training loss: 3.5301289558410645
Validation loss: 2.7676312282521236

Epoch: 26| Step: 0
Training loss: 3.6158013343811035
Validation loss: 2.7637296671508462

Epoch: 5| Step: 1
Training loss: 3.162564277648926
Validation loss: 2.764770000211654

Epoch: 5| Step: 2
Training loss: 2.8302693367004395
Validation loss: 2.7640830355305828

Epoch: 5| Step: 3
Training loss: 2.3638248443603516
Validation loss: 2.763250030497069

Epoch: 5| Step: 4
Training loss: 2.862720251083374
Validation loss: 2.759978896828108

Epoch: 5| Step: 5
Training loss: 3.4382777214050293
Validation loss: 2.751312312259469

Epoch: 5| Step: 6
Training loss: 3.0690388679504395
Validation loss: 2.7533055864354616

Epoch: 5| Step: 7
Training loss: 2.985358715057373
Validation loss: 2.753428605294997

Epoch: 5| Step: 8
Training loss: 2.46274995803833
Validation loss: 2.7493106934332077

Epoch: 5| Step: 9
Training loss: 2.0802664756774902
Validation loss: 2.748262290031679

Epoch: 5| Step: 10
Training loss: 3.057882308959961
Validation loss: 2.755526916955107

Epoch: 27| Step: 0
Training loss: 3.397472381591797
Validation loss: 2.7571568668529554

Epoch: 5| Step: 1
Training loss: 3.1154205799102783
Validation loss: 2.7707287675590924

Epoch: 5| Step: 2
Training loss: 2.084327459335327
Validation loss: 2.763264820139895

Epoch: 5| Step: 3
Training loss: 2.6081881523132324
Validation loss: 2.75554241929003

Epoch: 5| Step: 4
Training loss: 3.258666515350342
Validation loss: 2.747362208622758

Epoch: 5| Step: 5
Training loss: 2.8613574504852295
Validation loss: 2.742145256329608

Epoch: 5| Step: 6
Training loss: 3.3901009559631348
Validation loss: 2.7366630313217

Epoch: 5| Step: 7
Training loss: 2.810298442840576
Validation loss: 2.7419178537143174

Epoch: 5| Step: 8
Training loss: 2.7541050910949707
Validation loss: 2.7457861208146617

Epoch: 5| Step: 9
Training loss: 2.643372058868408
Validation loss: 2.752904692003804

Epoch: 5| Step: 10
Training loss: 2.8329668045043945
Validation loss: 2.732222234049151

Epoch: 28| Step: 0
Training loss: 2.342583656311035
Validation loss: 2.7297113146833194

Epoch: 5| Step: 1
Training loss: 3.739116668701172
Validation loss: 2.7236986211551133

Epoch: 5| Step: 2
Training loss: 2.6847052574157715
Validation loss: 2.729101957813386

Epoch: 5| Step: 3
Training loss: 2.8165838718414307
Validation loss: 2.7333199157509753

Epoch: 5| Step: 4
Training loss: 2.8386950492858887
Validation loss: 2.7404330699674544

Epoch: 5| Step: 5
Training loss: 3.0646064281463623
Validation loss: 2.7390846642114783

Epoch: 5| Step: 6
Training loss: 2.8912441730499268
Validation loss: 2.732466261873963

Epoch: 5| Step: 7
Training loss: 2.2470450401306152
Validation loss: 2.7294826251204296

Epoch: 5| Step: 8
Training loss: 2.756153106689453
Validation loss: 2.7270382245381675

Epoch: 5| Step: 9
Training loss: 3.400660753250122
Validation loss: 2.7260404684210338

Epoch: 5| Step: 10
Training loss: 2.8911960124969482
Validation loss: 2.7224817019636913

Epoch: 29| Step: 0
Training loss: 2.7597479820251465
Validation loss: 2.71630617367324

Epoch: 5| Step: 1
Training loss: 2.569955825805664
Validation loss: 2.7139484523445048

Epoch: 5| Step: 2
Training loss: 2.9878718852996826
Validation loss: 2.7175870146802676

Epoch: 5| Step: 3
Training loss: 2.7533607482910156
Validation loss: 2.71238185000676

Epoch: 5| Step: 4
Training loss: 3.0330042839050293
Validation loss: 2.7105773853999313

Epoch: 5| Step: 5
Training loss: 3.0847301483154297
Validation loss: 2.710530286194176

Epoch: 5| Step: 6
Training loss: 2.811845064163208
Validation loss: 2.7049017516515588

Epoch: 5| Step: 7
Training loss: 2.4942569732666016
Validation loss: 2.7075957893043436

Epoch: 5| Step: 8
Training loss: 3.309774398803711
Validation loss: 2.704193825362831

Epoch: 5| Step: 9
Training loss: 3.0187630653381348
Validation loss: 2.708026234821607

Epoch: 5| Step: 10
Training loss: 2.7433762550354004
Validation loss: 2.714445082090234

Epoch: 30| Step: 0
Training loss: 2.6402587890625
Validation loss: 2.70759150033356

Epoch: 5| Step: 1
Training loss: 3.287792205810547
Validation loss: 2.70151254951313

Epoch: 5| Step: 2
Training loss: 3.3810489177703857
Validation loss: 2.695320757486487

Epoch: 5| Step: 3
Training loss: 2.7190301418304443
Validation loss: 2.693008692033829

Epoch: 5| Step: 4
Training loss: 2.8277268409729004
Validation loss: 2.6929856987409693

Epoch: 5| Step: 5
Training loss: 2.9139132499694824
Validation loss: 2.691852964380736

Epoch: 5| Step: 6
Training loss: 2.641657590866089
Validation loss: 2.68860541107834

Epoch: 5| Step: 7
Training loss: 2.6779472827911377
Validation loss: 2.6886393998258855

Epoch: 5| Step: 8
Training loss: 2.704610824584961
Validation loss: 2.689995076066704

Epoch: 5| Step: 9
Training loss: 2.8100688457489014
Validation loss: 2.686333258946737

Epoch: 5| Step: 10
Training loss: 2.788980722427368
Validation loss: 2.686247697440527

Epoch: 31| Step: 0
Training loss: 2.3802366256713867
Validation loss: 2.6872022254492647

Epoch: 5| Step: 1
Training loss: 2.7878453731536865
Validation loss: 2.691651959573069

Epoch: 5| Step: 2
Training loss: 3.1625874042510986
Validation loss: 2.7022291767981743

Epoch: 5| Step: 3
Training loss: 3.34333872795105
Validation loss: 2.6835379497979277

Epoch: 5| Step: 4
Training loss: 2.613729953765869
Validation loss: 2.6756709134706886

Epoch: 5| Step: 5
Training loss: 2.7175447940826416
Validation loss: 2.674310420149116

Epoch: 5| Step: 6
Training loss: 2.988311767578125
Validation loss: 2.683666741976174

Epoch: 5| Step: 7
Training loss: 3.388097047805786
Validation loss: 2.6729693540962796

Epoch: 5| Step: 8
Training loss: 2.56538724899292
Validation loss: 2.6734263358577603

Epoch: 5| Step: 9
Training loss: 2.308159351348877
Validation loss: 2.6672430781907934

Epoch: 5| Step: 10
Training loss: 3.1020538806915283
Validation loss: 2.6696104029173493

Epoch: 32| Step: 0
Training loss: 2.8028197288513184
Validation loss: 2.667982083494945

Epoch: 5| Step: 1
Training loss: 2.988523244857788
Validation loss: 2.667469519440846

Epoch: 5| Step: 2
Training loss: 3.012629985809326
Validation loss: 2.666973437032392

Epoch: 5| Step: 3
Training loss: 2.9106364250183105
Validation loss: 2.663998585875316

Epoch: 5| Step: 4
Training loss: 4.023243427276611
Validation loss: 2.667584921724053

Epoch: 5| Step: 5
Training loss: 2.445457935333252
Validation loss: 2.66452040467211

Epoch: 5| Step: 6
Training loss: 2.8955793380737305
Validation loss: 2.663726511821952

Epoch: 5| Step: 7
Training loss: 2.7967817783355713
Validation loss: 2.66190682431703

Epoch: 5| Step: 8
Training loss: 2.750274419784546
Validation loss: 2.660080535437471

Epoch: 5| Step: 9
Training loss: 2.27372407913208
Validation loss: 2.658898784268287

Epoch: 5| Step: 10
Training loss: 2.2744197845458984
Validation loss: 2.656122220459805

Epoch: 33| Step: 0
Training loss: 2.0278122425079346
Validation loss: 2.6540584564208984

Epoch: 5| Step: 1
Training loss: 2.432567596435547
Validation loss: 2.6592155118142404

Epoch: 5| Step: 2
Training loss: 2.640312910079956
Validation loss: 2.655818969972672

Epoch: 5| Step: 3
Training loss: 3.0543324947357178
Validation loss: 2.651522046776228

Epoch: 5| Step: 4
Training loss: 2.5109431743621826
Validation loss: 2.650927610294793

Epoch: 5| Step: 5
Training loss: 2.9244954586029053
Validation loss: 2.6479224389599216

Epoch: 5| Step: 6
Training loss: 3.352759599685669
Validation loss: 2.6484579552886305

Epoch: 5| Step: 7
Training loss: 2.5892796516418457
Validation loss: 2.6473447481791177

Epoch: 5| Step: 8
Training loss: 3.6771552562713623
Validation loss: 2.645183301741077

Epoch: 5| Step: 9
Training loss: 3.069314479827881
Validation loss: 2.646399223676292

Epoch: 5| Step: 10
Training loss: 2.888024091720581
Validation loss: 2.645428596004363

Epoch: 34| Step: 0
Training loss: 2.9720587730407715
Validation loss: 2.6455209550037178

Epoch: 5| Step: 1
Training loss: 2.9804623126983643
Validation loss: 2.646819540249404

Epoch: 5| Step: 2
Training loss: 2.8287994861602783
Validation loss: 2.646833107035647

Epoch: 5| Step: 3
Training loss: 2.718723773956299
Validation loss: 2.6467738587369203

Epoch: 5| Step: 4
Training loss: 3.3937487602233887
Validation loss: 2.6421494689039005

Epoch: 5| Step: 5
Training loss: 2.7973716259002686
Validation loss: 2.638203574765113

Epoch: 5| Step: 6
Training loss: 2.079228162765503
Validation loss: 2.637804913264449

Epoch: 5| Step: 7
Training loss: 3.450913667678833
Validation loss: 2.647115315160444

Epoch: 5| Step: 8
Training loss: 2.3949992656707764
Validation loss: 2.6436085957352833

Epoch: 5| Step: 9
Training loss: 2.3521015644073486
Validation loss: 2.643099836123887

Epoch: 5| Step: 10
Training loss: 3.1661736965179443
Validation loss: 2.6435075600941977

Epoch: 35| Step: 0
Training loss: 3.17718768119812
Validation loss: 2.6503613559148644

Epoch: 5| Step: 1
Training loss: 2.408813953399658
Validation loss: 2.6447651745170675

Epoch: 5| Step: 2
Training loss: 2.3109378814697266
Validation loss: 2.6423072609850156

Epoch: 5| Step: 3
Training loss: 3.4457192420959473
Validation loss: 2.641994860864455

Epoch: 5| Step: 4
Training loss: 2.8976693153381348
Validation loss: 2.640600476213681

Epoch: 5| Step: 5
Training loss: 2.9465079307556152
Validation loss: 2.634555216758482

Epoch: 5| Step: 6
Training loss: 3.2240073680877686
Validation loss: 2.633678397824687

Epoch: 5| Step: 7
Training loss: 2.3789188861846924
Validation loss: 2.6322243905836538

Epoch: 5| Step: 8
Training loss: 2.8569164276123047
Validation loss: 2.63066743522562

Epoch: 5| Step: 9
Training loss: 3.1400628089904785
Validation loss: 2.6380908130317606

Epoch: 5| Step: 10
Training loss: 2.1301774978637695
Validation loss: 2.627306607461745

Epoch: 36| Step: 0
Training loss: 3.095061779022217
Validation loss: 2.6260919263285976

Epoch: 5| Step: 1
Training loss: 3.1802756786346436
Validation loss: 2.625338085236088

Epoch: 5| Step: 2
Training loss: 3.107055187225342
Validation loss: 2.6242705006753244

Epoch: 5| Step: 3
Training loss: 2.859978437423706
Validation loss: 2.621893657151089

Epoch: 5| Step: 4
Training loss: 3.0339980125427246
Validation loss: 2.6184687332440446

Epoch: 5| Step: 5
Training loss: 2.834733247756958
Validation loss: 2.6242967997827837

Epoch: 5| Step: 6
Training loss: 2.9940848350524902
Validation loss: 2.628873953255274

Epoch: 5| Step: 7
Training loss: 2.286926746368408
Validation loss: 2.6393033919795865

Epoch: 5| Step: 8
Training loss: 2.6815130710601807
Validation loss: 2.6343851756024104

Epoch: 5| Step: 9
Training loss: 2.6444432735443115
Validation loss: 2.6331845098926174

Epoch: 5| Step: 10
Training loss: 2.120624303817749
Validation loss: 2.6266139912348923

Epoch: 37| Step: 0
Training loss: 3.1263210773468018
Validation loss: 2.6266170419672483

Epoch: 5| Step: 1
Training loss: 2.7803444862365723
Validation loss: 2.627524645097794

Epoch: 5| Step: 2
Training loss: 3.197019338607788
Validation loss: 2.6237037130581435

Epoch: 5| Step: 3
Training loss: 2.7285735607147217
Validation loss: 2.6241097398983535

Epoch: 5| Step: 4
Training loss: 2.8203437328338623
Validation loss: 2.6334469933663645

Epoch: 5| Step: 5
Training loss: 3.023273468017578
Validation loss: 2.6626349982394966

Epoch: 5| Step: 6
Training loss: 2.589122772216797
Validation loss: 2.6716275856059086

Epoch: 5| Step: 7
Training loss: 2.314814329147339
Validation loss: 2.6600075716613443

Epoch: 5| Step: 8
Training loss: 2.825613021850586
Validation loss: 2.634682350261237

Epoch: 5| Step: 9
Training loss: 2.689908266067505
Validation loss: 2.6245112662674277

Epoch: 5| Step: 10
Training loss: 2.9220447540283203
Validation loss: 2.6165498405374508

Epoch: 38| Step: 0
Training loss: 2.81500244140625
Validation loss: 2.6150754754261305

Epoch: 5| Step: 1
Training loss: 3.214272975921631
Validation loss: 2.611203537192396

Epoch: 5| Step: 2
Training loss: 1.9461920261383057
Validation loss: 2.609778634963497

Epoch: 5| Step: 3
Training loss: 3.5614724159240723
Validation loss: 2.6060102985751246

Epoch: 5| Step: 4
Training loss: 2.0068233013153076
Validation loss: 2.606666264995452

Epoch: 5| Step: 5
Training loss: 3.2010045051574707
Validation loss: 2.6108479525453303

Epoch: 5| Step: 6
Training loss: 2.8779215812683105
Validation loss: 2.6108939955311437

Epoch: 5| Step: 7
Training loss: 2.851792812347412
Validation loss: 2.613101536227811

Epoch: 5| Step: 8
Training loss: 2.6542282104492188
Validation loss: 2.6094161438685592

Epoch: 5| Step: 9
Training loss: 3.042065143585205
Validation loss: 2.607905564769622

Epoch: 5| Step: 10
Training loss: 2.697382688522339
Validation loss: 2.606789021081822

Epoch: 39| Step: 0
Training loss: 2.829317331314087
Validation loss: 2.6046224819716586

Epoch: 5| Step: 1
Training loss: 2.473843812942505
Validation loss: 2.6019607743909283

Epoch: 5| Step: 2
Training loss: 3.572169780731201
Validation loss: 2.6026232345129854

Epoch: 5| Step: 3
Training loss: 3.0183730125427246
Validation loss: 2.6012252043652278

Epoch: 5| Step: 4
Training loss: 2.470646381378174
Validation loss: 2.6065103725720475

Epoch: 5| Step: 5
Training loss: 2.9669785499572754
Validation loss: 2.6014350229693997

Epoch: 5| Step: 6
Training loss: 2.5424716472625732
Validation loss: 2.602549534971996

Epoch: 5| Step: 7
Training loss: 1.952330231666565
Validation loss: 2.598693652819562

Epoch: 5| Step: 8
Training loss: 2.7604806423187256
Validation loss: 2.5968789644138788

Epoch: 5| Step: 9
Training loss: 3.2671215534210205
Validation loss: 2.5943136753574496

Epoch: 5| Step: 10
Training loss: 2.903106927871704
Validation loss: 2.5943882337180515

Epoch: 40| Step: 0
Training loss: 3.037343978881836
Validation loss: 2.5905844575615338

Epoch: 5| Step: 1
Training loss: 2.00992751121521
Validation loss: 2.5949558724639235

Epoch: 5| Step: 2
Training loss: 2.5583059787750244
Validation loss: 2.602022155638664

Epoch: 5| Step: 3
Training loss: 2.2236485481262207
Validation loss: 2.6005831636408323

Epoch: 5| Step: 4
Training loss: 3.2528834342956543
Validation loss: 2.5952379882976575

Epoch: 5| Step: 5
Training loss: 3.133784532546997
Validation loss: 2.6011379457289174

Epoch: 5| Step: 6
Training loss: 2.5568325519561768
Validation loss: 2.5969394458237516

Epoch: 5| Step: 7
Training loss: 3.1097404956817627
Validation loss: 2.597387594561423

Epoch: 5| Step: 8
Training loss: 2.5010321140289307
Validation loss: 2.5968281915110927

Epoch: 5| Step: 9
Training loss: 2.586655616760254
Validation loss: 2.6024007079421834

Epoch: 5| Step: 10
Training loss: 3.8465449810028076
Validation loss: 2.5982252141480804

Epoch: 41| Step: 0
Training loss: 2.9962074756622314
Validation loss: 2.591885143710721

Epoch: 5| Step: 1
Training loss: 2.439276933670044
Validation loss: 2.592082231275497

Epoch: 5| Step: 2
Training loss: 2.535858392715454
Validation loss: 2.5834737952037523

Epoch: 5| Step: 3
Training loss: 2.697141408920288
Validation loss: 2.58024860453862

Epoch: 5| Step: 4
Training loss: 3.088749647140503
Validation loss: 2.58117171513137

Epoch: 5| Step: 5
Training loss: 2.8061065673828125
Validation loss: 2.5789639155069985

Epoch: 5| Step: 6
Training loss: 2.071140766143799
Validation loss: 2.5832728109052105

Epoch: 5| Step: 7
Training loss: 3.013420581817627
Validation loss: 2.5894701378319853

Epoch: 5| Step: 8
Training loss: 3.080714702606201
Validation loss: 2.5799044537287887

Epoch: 5| Step: 9
Training loss: 3.092498779296875
Validation loss: 2.571149949104555

Epoch: 5| Step: 10
Training loss: 2.761734962463379
Validation loss: 2.5688885078635266

Epoch: 42| Step: 0
Training loss: 2.530385732650757
Validation loss: 2.5669965359472458

Epoch: 5| Step: 1
Training loss: 3.081141233444214
Validation loss: 2.5695095574983986

Epoch: 5| Step: 2
Training loss: 2.4128682613372803
Validation loss: 2.570448185807915

Epoch: 5| Step: 3
Training loss: 2.6206367015838623
Validation loss: 2.5685211458513812

Epoch: 5| Step: 4
Training loss: 3.0870113372802734
Validation loss: 2.5637985019273657

Epoch: 5| Step: 5
Training loss: 2.9602131843566895
Validation loss: 2.5698045658808883

Epoch: 5| Step: 6
Training loss: 2.7632687091827393
Validation loss: 2.5658217194259807

Epoch: 5| Step: 7
Training loss: 2.8637051582336426
Validation loss: 2.572037030291814

Epoch: 5| Step: 8
Training loss: 2.691652536392212
Validation loss: 2.5637905161867858

Epoch: 5| Step: 9
Training loss: 2.656752109527588
Validation loss: 2.5638425504007647

Epoch: 5| Step: 10
Training loss: 2.8207061290740967
Validation loss: 2.5606178468273533

Epoch: 43| Step: 0
Training loss: 3.055842638015747
Validation loss: 2.5625221883097002

Epoch: 5| Step: 1
Training loss: 2.6514575481414795
Validation loss: 2.563160175918251

Epoch: 5| Step: 2
Training loss: 2.7598226070404053
Validation loss: 2.5648454286718882

Epoch: 5| Step: 3
Training loss: 2.719447374343872
Validation loss: 2.564040378857684

Epoch: 5| Step: 4
Training loss: 2.4367728233337402
Validation loss: 2.571183309760145

Epoch: 5| Step: 5
Training loss: 3.438530683517456
Validation loss: 2.5648930970058648

Epoch: 5| Step: 6
Training loss: 2.4099061489105225
Validation loss: 2.5647762795930267

Epoch: 5| Step: 7
Training loss: 2.813565731048584
Validation loss: 2.5640877908275974

Epoch: 5| Step: 8
Training loss: 2.4128947257995605
Validation loss: 2.565252214349726

Epoch: 5| Step: 9
Training loss: 2.8938169479370117
Validation loss: 2.5667220136170745

Epoch: 5| Step: 10
Training loss: 2.8265087604522705
Validation loss: 2.569547417343304

Epoch: 44| Step: 0
Training loss: 3.5277152061462402
Validation loss: 2.5678182673710648

Epoch: 5| Step: 1
Training loss: 3.2801125049591064
Validation loss: 2.565304425454909

Epoch: 5| Step: 2
Training loss: 1.9208654165267944
Validation loss: 2.5649313260150213

Epoch: 5| Step: 3
Training loss: 2.5138180255889893
Validation loss: 2.564603582505257

Epoch: 5| Step: 4
Training loss: 2.3902993202209473
Validation loss: 2.570988590999316

Epoch: 5| Step: 5
Training loss: 3.4686522483825684
Validation loss: 2.563285768672984

Epoch: 5| Step: 6
Training loss: 3.241631031036377
Validation loss: 2.5623306689723844

Epoch: 5| Step: 7
Training loss: 2.6923584938049316
Validation loss: 2.562762773165139

Epoch: 5| Step: 8
Training loss: 2.5116043090820312
Validation loss: 2.559378990563013

Epoch: 5| Step: 9
Training loss: 2.6666641235351562
Validation loss: 2.5641348772151495

Epoch: 5| Step: 10
Training loss: 2.0385756492614746
Validation loss: 2.5613322668178107

Epoch: 45| Step: 0
Training loss: 2.3877105712890625
Validation loss: 2.5575951299359723

Epoch: 5| Step: 1
Training loss: 3.1587700843811035
Validation loss: 2.552307172488141

Epoch: 5| Step: 2
Training loss: 2.3119072914123535
Validation loss: 2.5519875223918627

Epoch: 5| Step: 3
Training loss: 2.312716007232666
Validation loss: 2.559604008992513

Epoch: 5| Step: 4
Training loss: 3.0484070777893066
Validation loss: 2.5581090450286865

Epoch: 5| Step: 5
Training loss: 2.2665069103240967
Validation loss: 2.5566769658878283

Epoch: 5| Step: 6
Training loss: 3.6762642860412598
Validation loss: 2.569045361652169

Epoch: 5| Step: 7
Training loss: 3.638842821121216
Validation loss: 2.5608227252960205

Epoch: 5| Step: 8
Training loss: 2.462956666946411
Validation loss: 2.5673844045208347

Epoch: 5| Step: 9
Training loss: 2.2613704204559326
Validation loss: 2.563378967264647

Epoch: 5| Step: 10
Training loss: 2.7321319580078125
Validation loss: 2.5637406687582693

Epoch: 46| Step: 0
Training loss: 2.716888666152954
Validation loss: 2.557976991899552

Epoch: 5| Step: 1
Training loss: 2.8169448375701904
Validation loss: 2.5546809550254577

Epoch: 5| Step: 2
Training loss: 2.318355083465576
Validation loss: 2.547421201582878

Epoch: 5| Step: 3
Training loss: 2.9234862327575684
Validation loss: 2.5427749951680503

Epoch: 5| Step: 4
Training loss: 3.047353744506836
Validation loss: 2.536666303552607

Epoch: 5| Step: 5
Training loss: 3.4002890586853027
Validation loss: 2.5348318981867966

Epoch: 5| Step: 6
Training loss: 2.2174320220947266
Validation loss: 2.535680296600506

Epoch: 5| Step: 7
Training loss: 2.692539930343628
Validation loss: 2.5340562661488852

Epoch: 5| Step: 8
Training loss: 2.599449634552002
Validation loss: 2.5337474371797297

Epoch: 5| Step: 9
Training loss: 2.7032108306884766
Validation loss: 2.530201893980785

Epoch: 5| Step: 10
Training loss: 2.7396771907806396
Validation loss: 2.5302261075665875

Epoch: 47| Step: 0
Training loss: 2.9096531867980957
Validation loss: 2.5309368230963267

Epoch: 5| Step: 1
Training loss: 2.193115711212158
Validation loss: 2.531550831692193

Epoch: 5| Step: 2
Training loss: 2.8728420734405518
Validation loss: 2.533855197250202

Epoch: 5| Step: 3
Training loss: 2.30250883102417
Validation loss: 2.5400915556056525

Epoch: 5| Step: 4
Training loss: 2.462263822555542
Validation loss: 2.547050204328311

Epoch: 5| Step: 5
Training loss: 2.930206298828125
Validation loss: 2.5423572140355266

Epoch: 5| Step: 6
Training loss: 2.9539787769317627
Validation loss: 2.5488838329110095

Epoch: 5| Step: 7
Training loss: 2.3956856727600098
Validation loss: 2.539020610112016

Epoch: 5| Step: 8
Training loss: 2.900859832763672
Validation loss: 2.5319409306331346

Epoch: 5| Step: 9
Training loss: 2.9087090492248535
Validation loss: 2.534515344968406

Epoch: 5| Step: 10
Training loss: 3.4140079021453857
Validation loss: 2.5460390736979823

Epoch: 48| Step: 0
Training loss: 2.660263776779175
Validation loss: 2.5485852379952707

Epoch: 5| Step: 1
Training loss: 2.562218427658081
Validation loss: 2.564376995127688

Epoch: 5| Step: 2
Training loss: 2.8436996936798096
Validation loss: 2.5719028365227485

Epoch: 5| Step: 3
Training loss: 2.872464895248413
Validation loss: 2.552318206397436

Epoch: 5| Step: 4
Training loss: 2.382620334625244
Validation loss: 2.541291444532333

Epoch: 5| Step: 5
Training loss: 3.142224073410034
Validation loss: 2.5300699536518385

Epoch: 5| Step: 6
Training loss: 2.6924006938934326
Validation loss: 2.5262729711430048

Epoch: 5| Step: 7
Training loss: 1.9888594150543213
Validation loss: 2.5346676995677333

Epoch: 5| Step: 8
Training loss: 2.4599690437316895
Validation loss: 2.5477768528846

Epoch: 5| Step: 9
Training loss: 3.0266051292419434
Validation loss: 2.5422616184398694

Epoch: 5| Step: 10
Training loss: 3.7399179935455322
Validation loss: 2.536555769622967

Epoch: 49| Step: 0
Training loss: 3.02498459815979
Validation loss: 2.5266600014061056

Epoch: 5| Step: 1
Training loss: 2.107459306716919
Validation loss: 2.5198158461560487

Epoch: 5| Step: 2
Training loss: 2.9966399669647217
Validation loss: 2.519967796981976

Epoch: 5| Step: 3
Training loss: 2.8796632289886475
Validation loss: 2.5181784783640215

Epoch: 5| Step: 4
Training loss: 3.663029432296753
Validation loss: 2.524680132506996

Epoch: 5| Step: 5
Training loss: 2.890073776245117
Validation loss: 2.5299409692005446

Epoch: 5| Step: 6
Training loss: 3.088449001312256
Validation loss: 2.5303821704720937

Epoch: 5| Step: 7
Training loss: 2.466721296310425
Validation loss: 2.5284535064492175

Epoch: 5| Step: 8
Training loss: 2.310598373413086
Validation loss: 2.5258308020971154

Epoch: 5| Step: 9
Training loss: 2.488638401031494
Validation loss: 2.5243302365785003

Epoch: 5| Step: 10
Training loss: 2.1749908924102783
Validation loss: 2.520344844428442

Epoch: 50| Step: 0
Training loss: 2.6171603202819824
Validation loss: 2.51526052464721

Epoch: 5| Step: 1
Training loss: 3.1693243980407715
Validation loss: 2.5151827540448917

Epoch: 5| Step: 2
Training loss: 2.753878116607666
Validation loss: 2.5160181548005793

Epoch: 5| Step: 3
Training loss: 2.641082286834717
Validation loss: 2.5246035206702446

Epoch: 5| Step: 4
Training loss: 3.1099541187286377
Validation loss: 2.5122243435152116

Epoch: 5| Step: 5
Training loss: 2.436440944671631
Validation loss: 2.5169379390696043

Epoch: 5| Step: 6
Training loss: 3.1713147163391113
Validation loss: 2.5202058207604194

Epoch: 5| Step: 7
Training loss: 1.9823553562164307
Validation loss: 2.5260779216725338

Epoch: 5| Step: 8
Training loss: 2.3940796852111816
Validation loss: 2.532263817325715

Epoch: 5| Step: 9
Training loss: 2.6079509258270264
Validation loss: 2.5281066561257965

Epoch: 5| Step: 10
Training loss: 3.2510995864868164
Validation loss: 2.530273414427234

Epoch: 51| Step: 0
Training loss: 2.2424569129943848
Validation loss: 2.524353870781519

Epoch: 5| Step: 1
Training loss: 2.5266237258911133
Validation loss: 2.51001190370129

Epoch: 5| Step: 2
Training loss: 3.1736884117126465
Validation loss: 2.5094034671783447

Epoch: 5| Step: 3
Training loss: 3.7041842937469482
Validation loss: 2.5095342025961926

Epoch: 5| Step: 4
Training loss: 2.239809989929199
Validation loss: 2.511265398353659

Epoch: 5| Step: 5
Training loss: 2.438364267349243
Validation loss: 2.518762219336725

Epoch: 5| Step: 6
Training loss: 2.674341917037964
Validation loss: 2.523167848587036

Epoch: 5| Step: 7
Training loss: 2.816707134246826
Validation loss: 2.5199126992174374

Epoch: 5| Step: 8
Training loss: 3.1173088550567627
Validation loss: 2.5261151662436863

Epoch: 5| Step: 9
Training loss: 3.09346079826355
Validation loss: 2.526653625631845

Epoch: 5| Step: 10
Training loss: 1.8316277265548706
Validation loss: 2.5240721779484905

Epoch: 52| Step: 0
Training loss: 3.0588107109069824
Validation loss: 2.5138860620478147

Epoch: 5| Step: 1
Training loss: 2.529399871826172
Validation loss: 2.520927075416811

Epoch: 5| Step: 2
Training loss: 3.130809783935547
Validation loss: 2.5278999933632473

Epoch: 5| Step: 3
Training loss: 1.8959243297576904
Validation loss: 2.5424159047424153

Epoch: 5| Step: 4
Training loss: 2.9563233852386475
Validation loss: 2.536412218565582

Epoch: 5| Step: 5
Training loss: 2.467303514480591
Validation loss: 2.519472001701273

Epoch: 5| Step: 6
Training loss: 3.0957248210906982
Validation loss: 2.4970348983682613

Epoch: 5| Step: 7
Training loss: 2.7231011390686035
Validation loss: 2.4931182861328125

Epoch: 5| Step: 8
Training loss: 2.657344341278076
Validation loss: 2.4928044093552457

Epoch: 5| Step: 9
Training loss: 2.206786632537842
Validation loss: 2.498306930706065

Epoch: 5| Step: 10
Training loss: 3.4249281883239746
Validation loss: 2.507515940614926

Epoch: 53| Step: 0
Training loss: 3.273789882659912
Validation loss: 2.5064581671068744

Epoch: 5| Step: 1
Training loss: 3.3864359855651855
Validation loss: 2.5158679049502135

Epoch: 5| Step: 2
Training loss: 1.7123031616210938
Validation loss: 2.513069386123329

Epoch: 5| Step: 3
Training loss: 2.7613890171051025
Validation loss: 2.5078641650497273

Epoch: 5| Step: 4
Training loss: 2.5949721336364746
Validation loss: 2.510642561861264

Epoch: 5| Step: 5
Training loss: 2.383816957473755
Validation loss: 2.5079348228311025

Epoch: 5| Step: 6
Training loss: 2.671677350997925
Validation loss: 2.517759853793729

Epoch: 5| Step: 7
Training loss: 3.331686496734619
Validation loss: 2.5185387621643724

Epoch: 5| Step: 8
Training loss: 2.8926913738250732
Validation loss: 2.52527210532978

Epoch: 5| Step: 9
Training loss: 2.582489490509033
Validation loss: 2.512325086901265

Epoch: 5| Step: 10
Training loss: 2.269937515258789
Validation loss: 2.496163833525873

Epoch: 54| Step: 0
Training loss: 2.7350785732269287
Validation loss: 2.487130493246099

Epoch: 5| Step: 1
Training loss: 2.6563658714294434
Validation loss: 2.4832187826915453

Epoch: 5| Step: 2
Training loss: 3.235970973968506
Validation loss: 2.481756797400854

Epoch: 5| Step: 3
Training loss: 2.7611212730407715
Validation loss: 2.486283676598662

Epoch: 5| Step: 4
Training loss: 2.4374711513519287
Validation loss: 2.4877636355738484

Epoch: 5| Step: 5
Training loss: 2.6090774536132812
Validation loss: 2.4920320946683168

Epoch: 5| Step: 6
Training loss: 3.233704090118408
Validation loss: 2.492139503520022

Epoch: 5| Step: 7
Training loss: 2.571582078933716
Validation loss: 2.4928596019744873

Epoch: 5| Step: 8
Training loss: 2.5670104026794434
Validation loss: 2.4886177919244252

Epoch: 5| Step: 9
Training loss: 2.352848768234253
Validation loss: 2.487451481562789

Epoch: 5| Step: 10
Training loss: 2.607051134109497
Validation loss: 2.48784726665866

Epoch: 55| Step: 0
Training loss: 3.095306158065796
Validation loss: 2.483784526906988

Epoch: 5| Step: 1
Training loss: 2.6312365531921387
Validation loss: 2.48717733352415

Epoch: 5| Step: 2
Training loss: 2.610586404800415
Validation loss: 2.4905659383343113

Epoch: 5| Step: 3
Training loss: 2.1221556663513184
Validation loss: 2.492366467752764

Epoch: 5| Step: 4
Training loss: 2.990084409713745
Validation loss: 2.5171065304868963

Epoch: 5| Step: 5
Training loss: 2.8471248149871826
Validation loss: 2.5128634386165167

Epoch: 5| Step: 6
Training loss: 2.32383394241333
Validation loss: 2.5182603559186383

Epoch: 5| Step: 7
Training loss: 2.547473669052124
Validation loss: 2.5036303945766982

Epoch: 5| Step: 8
Training loss: 3.2213034629821777
Validation loss: 2.484509914152084

Epoch: 5| Step: 9
Training loss: 2.910581111907959
Validation loss: 2.4865523153735745

Epoch: 5| Step: 10
Training loss: 2.4704811573028564
Validation loss: 2.4924731126395603

Epoch: 56| Step: 0
Training loss: 2.938744306564331
Validation loss: 2.5022884158677954

Epoch: 5| Step: 1
Training loss: 3.4422974586486816
Validation loss: 2.505208876825148

Epoch: 5| Step: 2
Training loss: 2.513073444366455
Validation loss: 2.5025168336847776

Epoch: 5| Step: 3
Training loss: 2.129396677017212
Validation loss: 2.496429740741689

Epoch: 5| Step: 4
Training loss: 3.2439587116241455
Validation loss: 2.4953240348446752

Epoch: 5| Step: 5
Training loss: 2.5292255878448486
Validation loss: 2.488414144003263

Epoch: 5| Step: 6
Training loss: 3.639070987701416
Validation loss: 2.488174787131689

Epoch: 5| Step: 7
Training loss: 2.554882526397705
Validation loss: 2.4803012750482045

Epoch: 5| Step: 8
Training loss: 2.076397180557251
Validation loss: 2.4706421641893286

Epoch: 5| Step: 9
Training loss: 2.473829507827759
Validation loss: 2.474157933265932

Epoch: 5| Step: 10
Training loss: 2.1108081340789795
Validation loss: 2.478210485109719

Epoch: 57| Step: 0
Training loss: 2.6192755699157715
Validation loss: 2.4825691202635407

Epoch: 5| Step: 1
Training loss: 2.751094341278076
Validation loss: 2.4851410722219818

Epoch: 5| Step: 2
Training loss: 2.074760913848877
Validation loss: 2.495750040136358

Epoch: 5| Step: 3
Training loss: 3.1566991806030273
Validation loss: 2.50311948919809

Epoch: 5| Step: 4
Training loss: 1.9413232803344727
Validation loss: 2.4901331265767417

Epoch: 5| Step: 5
Training loss: 2.6859467029571533
Validation loss: 2.487127750150619

Epoch: 5| Step: 6
Training loss: 2.712306499481201
Validation loss: 2.4853445817065496

Epoch: 5| Step: 7
Training loss: 2.5878758430480957
Validation loss: 2.4847987954334547

Epoch: 5| Step: 8
Training loss: 3.1944217681884766
Validation loss: 2.4859649622312157

Epoch: 5| Step: 9
Training loss: 2.890298843383789
Validation loss: 2.494089695715135

Epoch: 5| Step: 10
Training loss: 3.1708552837371826
Validation loss: 2.48515571061001

Epoch: 58| Step: 0
Training loss: 2.4821906089782715
Validation loss: 2.480652609179097

Epoch: 5| Step: 1
Training loss: 1.5822457075119019
Validation loss: 2.4718141965968634

Epoch: 5| Step: 2
Training loss: 3.1903347969055176
Validation loss: 2.4733730054670766

Epoch: 5| Step: 3
Training loss: 3.2154593467712402
Validation loss: 2.4763952019394084

Epoch: 5| Step: 4
Training loss: 2.552736520767212
Validation loss: 2.4780599942771335

Epoch: 5| Step: 5
Training loss: 2.4579379558563232
Validation loss: 2.478612410124912

Epoch: 5| Step: 6
Training loss: 3.0105929374694824
Validation loss: 2.4819692027184272

Epoch: 5| Step: 7
Training loss: 3.037998914718628
Validation loss: 2.489643435324392

Epoch: 5| Step: 8
Training loss: 3.1261305809020996
Validation loss: 2.5008716339706094

Epoch: 5| Step: 9
Training loss: 2.3960819244384766
Validation loss: 2.5047758830490934

Epoch: 5| Step: 10
Training loss: 2.4899699687957764
Validation loss: 2.5030127956021215

Epoch: 59| Step: 0
Training loss: 2.8085575103759766
Validation loss: 2.5063305875306487

Epoch: 5| Step: 1
Training loss: 3.0970473289489746
Validation loss: 2.505673985327444

Epoch: 5| Step: 2
Training loss: 3.0881009101867676
Validation loss: 2.48479486280872

Epoch: 5| Step: 3
Training loss: 2.3967103958129883
Validation loss: 2.4766570637303014

Epoch: 5| Step: 4
Training loss: 2.7430260181427
Validation loss: 2.475651130881361

Epoch: 5| Step: 5
Training loss: 2.4356467723846436
Validation loss: 2.472057263056437

Epoch: 5| Step: 6
Training loss: 2.5402636528015137
Validation loss: 2.47426797241293

Epoch: 5| Step: 7
Training loss: 2.8906216621398926
Validation loss: 2.475204098609186

Epoch: 5| Step: 8
Training loss: 2.4938807487487793
Validation loss: 2.4795604418682795

Epoch: 5| Step: 9
Training loss: 2.5904641151428223
Validation loss: 2.4782088469433528

Epoch: 5| Step: 10
Training loss: 2.5426676273345947
Validation loss: 2.473715836001981

Epoch: 60| Step: 0
Training loss: 2.419288158416748
Validation loss: 2.469149294719901

Epoch: 5| Step: 1
Training loss: 3.4539501667022705
Validation loss: 2.4711601605979343

Epoch: 5| Step: 2
Training loss: 2.9609742164611816
Validation loss: 2.4741843438917592

Epoch: 5| Step: 3
Training loss: 2.8556346893310547
Validation loss: 2.47830520906756

Epoch: 5| Step: 4
Training loss: 2.65798282623291
Validation loss: 2.4822469757449244

Epoch: 5| Step: 5
Training loss: 2.8805723190307617
Validation loss: 2.4948234224832184

Epoch: 5| Step: 6
Training loss: 2.553459882736206
Validation loss: 2.496707823968703

Epoch: 5| Step: 7
Training loss: 2.926814556121826
Validation loss: 2.502702659176242

Epoch: 5| Step: 8
Training loss: 1.884211778640747
Validation loss: 2.4988199305790726

Epoch: 5| Step: 9
Training loss: 2.533233642578125
Validation loss: 2.488601666624828

Epoch: 5| Step: 10
Training loss: 2.509373426437378
Validation loss: 2.473673928168512

Epoch: 61| Step: 0
Training loss: 2.3713855743408203
Validation loss: 2.4715803054071244

Epoch: 5| Step: 1
Training loss: 2.480379104614258
Validation loss: 2.470648750182121

Epoch: 5| Step: 2
Training loss: 2.1538589000701904
Validation loss: 2.456000816437506

Epoch: 5| Step: 3
Training loss: 3.111863613128662
Validation loss: 2.4637520492717786

Epoch: 5| Step: 4
Training loss: 3.1672439575195312
Validation loss: 2.455336473321402

Epoch: 5| Step: 5
Training loss: 2.8724868297576904
Validation loss: 2.4598277461144233

Epoch: 5| Step: 6
Training loss: 2.464076519012451
Validation loss: 2.4549033154723463

Epoch: 5| Step: 7
Training loss: 2.9779953956604004
Validation loss: 2.4596168020720124

Epoch: 5| Step: 8
Training loss: 2.537858009338379
Validation loss: 2.4558736765256493

Epoch: 5| Step: 9
Training loss: 3.0617661476135254
Validation loss: 2.456471412412582

Epoch: 5| Step: 10
Training loss: 2.1862072944641113
Validation loss: 2.466374043495424

Epoch: 62| Step: 0
Training loss: 3.0714848041534424
Validation loss: 2.4727920204080562

Epoch: 5| Step: 1
Training loss: 2.1807608604431152
Validation loss: 2.4780197733192035

Epoch: 5| Step: 2
Training loss: 3.2820801734924316
Validation loss: 2.466484413352064

Epoch: 5| Step: 3
Training loss: 2.7925920486450195
Validation loss: 2.464708261592414

Epoch: 5| Step: 4
Training loss: 2.3445487022399902
Validation loss: 2.468883434931437

Epoch: 5| Step: 5
Training loss: 3.205083131790161
Validation loss: 2.4612260967172603

Epoch: 5| Step: 6
Training loss: 2.489095449447632
Validation loss: 2.461534733413368

Epoch: 5| Step: 7
Training loss: 2.6495187282562256
Validation loss: 2.461441598912721

Epoch: 5| Step: 8
Training loss: 2.2947983741760254
Validation loss: 2.4675314887877433

Epoch: 5| Step: 9
Training loss: 2.8263325691223145
Validation loss: 2.467758345347579

Epoch: 5| Step: 10
Training loss: 2.2403359413146973
Validation loss: 2.4762040210026566

Epoch: 63| Step: 0
Training loss: 2.4973714351654053
Validation loss: 2.4797872522825837

Epoch: 5| Step: 1
Training loss: 2.71098256111145
Validation loss: 2.482107457294259

Epoch: 5| Step: 2
Training loss: 2.541933536529541
Validation loss: 2.4812131466404086

Epoch: 5| Step: 3
Training loss: 2.4832050800323486
Validation loss: 2.477507398974511

Epoch: 5| Step: 4
Training loss: 2.3147084712982178
Validation loss: 2.477992098818543

Epoch: 5| Step: 5
Training loss: 2.5322344303131104
Validation loss: 2.4733009787016016

Epoch: 5| Step: 6
Training loss: 2.6105165481567383
Validation loss: 2.4696638840501026

Epoch: 5| Step: 7
Training loss: 3.168297290802002
Validation loss: 2.467806339263916

Epoch: 5| Step: 8
Training loss: 2.5530948638916016
Validation loss: 2.4742880418736446

Epoch: 5| Step: 9
Training loss: 3.489041566848755
Validation loss: 2.4667145513719126

Epoch: 5| Step: 10
Training loss: 2.4691972732543945
Validation loss: 2.456857683838055

Epoch: 64| Step: 0
Training loss: 2.74170184135437
Validation loss: 2.455857115407144

Epoch: 5| Step: 1
Training loss: 3.6406586170196533
Validation loss: 2.451575707363826

Epoch: 5| Step: 2
Training loss: 2.430340051651001
Validation loss: 2.447569921452512

Epoch: 5| Step: 3
Training loss: 2.868614673614502
Validation loss: 2.4517870667160198

Epoch: 5| Step: 4
Training loss: 3.2050461769104004
Validation loss: 2.46336426273469

Epoch: 5| Step: 5
Training loss: 2.354279041290283
Validation loss: 2.458471005962741

Epoch: 5| Step: 6
Training loss: 2.5175857543945312
Validation loss: 2.453641137769145

Epoch: 5| Step: 7
Training loss: 2.4916257858276367
Validation loss: 2.4475761921175065

Epoch: 5| Step: 8
Training loss: 2.7759721279144287
Validation loss: 2.452448019417383

Epoch: 5| Step: 9
Training loss: 2.0870919227600098
Validation loss: 2.4440711441860405

Epoch: 5| Step: 10
Training loss: 2.286435127258301
Validation loss: 2.4474954066738004

Epoch: 65| Step: 0
Training loss: 2.8351566791534424
Validation loss: 2.4450200834581928

Epoch: 5| Step: 1
Training loss: 2.7251245975494385
Validation loss: 2.449987619153915

Epoch: 5| Step: 2
Training loss: 2.8278164863586426
Validation loss: 2.4489421895755235

Epoch: 5| Step: 3
Training loss: 2.772568702697754
Validation loss: 2.455889730043309

Epoch: 5| Step: 4
Training loss: 2.9557347297668457
Validation loss: 2.457667168750558

Epoch: 5| Step: 5
Training loss: 2.382939577102661
Validation loss: 2.4571788259731826

Epoch: 5| Step: 6
Training loss: 2.449492931365967
Validation loss: 2.461908922400526

Epoch: 5| Step: 7
Training loss: 2.8832850456237793
Validation loss: 2.4592716181150047

Epoch: 5| Step: 8
Training loss: 2.0229742527008057
Validation loss: 2.4729954145287953

Epoch: 5| Step: 9
Training loss: 2.782907247543335
Validation loss: 2.4625297002894904

Epoch: 5| Step: 10
Training loss: 2.695679187774658
Validation loss: 2.4543501228414555

Epoch: 66| Step: 0
Training loss: 2.5778398513793945
Validation loss: 2.453296181976154

Epoch: 5| Step: 1
Training loss: 2.7162349224090576
Validation loss: 2.4564277638671217

Epoch: 5| Step: 2
Training loss: 2.3202810287475586
Validation loss: 2.4482377857290287

Epoch: 5| Step: 3
Training loss: 1.8798377513885498
Validation loss: 2.4496539638888453

Epoch: 5| Step: 4
Training loss: 2.6151511669158936
Validation loss: 2.4493481369428736

Epoch: 5| Step: 5
Training loss: 2.75803542137146
Validation loss: 2.4485140615893948

Epoch: 5| Step: 6
Training loss: 3.443124771118164
Validation loss: 2.4479097397096696

Epoch: 5| Step: 7
Training loss: 3.020761489868164
Validation loss: 2.443347277179841

Epoch: 5| Step: 8
Training loss: 2.100226879119873
Validation loss: 2.4479853312174478

Epoch: 5| Step: 9
Training loss: 3.1995129585266113
Validation loss: 2.4538830608449955

Epoch: 5| Step: 10
Training loss: 2.605388641357422
Validation loss: 2.4675828718370005

Epoch: 67| Step: 0
Training loss: 2.652369737625122
Validation loss: 2.473840957046837

Epoch: 5| Step: 1
Training loss: 2.9025075435638428
Validation loss: 2.4844611819072435

Epoch: 5| Step: 2
Training loss: 2.3277993202209473
Validation loss: 2.4730443211011988

Epoch: 5| Step: 3
Training loss: 3.0729758739471436
Validation loss: 2.486583097006685

Epoch: 5| Step: 4
Training loss: 2.8821749687194824
Validation loss: 2.4640516593892086

Epoch: 5| Step: 5
Training loss: 2.4782907962799072
Validation loss: 2.4523262977600098

Epoch: 5| Step: 6
Training loss: 3.3379950523376465
Validation loss: 2.452670758770358

Epoch: 5| Step: 7
Training loss: 1.8948338031768799
Validation loss: 2.4484392314828853

Epoch: 5| Step: 8
Training loss: 2.5199508666992188
Validation loss: 2.4547561419907438

Epoch: 5| Step: 9
Training loss: 3.0388691425323486
Validation loss: 2.4668989296882384

Epoch: 5| Step: 10
Training loss: 2.2078824043273926
Validation loss: 2.4608572195934992

Epoch: 68| Step: 0
Training loss: 2.9628705978393555
Validation loss: 2.4607764879862466

Epoch: 5| Step: 1
Training loss: 3.1893162727355957
Validation loss: 2.4489415050834737

Epoch: 5| Step: 2
Training loss: 2.1410231590270996
Validation loss: 2.4348543972097416

Epoch: 5| Step: 3
Training loss: 2.7488515377044678
Validation loss: 2.4349217286673923

Epoch: 5| Step: 4
Training loss: 2.9305179119110107
Validation loss: 2.4352545981766074

Epoch: 5| Step: 5
Training loss: 2.4545178413391113
Validation loss: 2.4479805372094594

Epoch: 5| Step: 6
Training loss: 2.5492403507232666
Validation loss: 2.459173135859992

Epoch: 5| Step: 7
Training loss: 2.288010358810425
Validation loss: 2.464101947763915

Epoch: 5| Step: 8
Training loss: 2.8042151927948
Validation loss: 2.442949861608526

Epoch: 5| Step: 9
Training loss: 3.004129409790039
Validation loss: 2.4379927214755805

Epoch: 5| Step: 10
Training loss: 2.2179341316223145
Validation loss: 2.430853371979088

Epoch: 69| Step: 0
Training loss: 2.7584338188171387
Validation loss: 2.4288422510188115

Epoch: 5| Step: 1
Training loss: 3.1902542114257812
Validation loss: 2.4281641308979323

Epoch: 5| Step: 2
Training loss: 2.1285698413848877
Validation loss: 2.4403364863446964

Epoch: 5| Step: 3
Training loss: 2.8768932819366455
Validation loss: 2.4386481162040465

Epoch: 5| Step: 4
Training loss: 2.8256421089172363
Validation loss: 2.4404282710885488

Epoch: 5| Step: 5
Training loss: 2.7648632526397705
Validation loss: 2.4506149522719847

Epoch: 5| Step: 6
Training loss: 2.457634449005127
Validation loss: 2.445124687687043

Epoch: 5| Step: 7
Training loss: 2.2437000274658203
Validation loss: 2.432711660221059

Epoch: 5| Step: 8
Training loss: 2.681380271911621
Validation loss: 2.4276499158592633

Epoch: 5| Step: 9
Training loss: 2.762463092803955
Validation loss: 2.4155078370084047

Epoch: 5| Step: 10
Training loss: 2.5078094005584717
Validation loss: 2.421696057883642

Epoch: 70| Step: 0
Training loss: 2.3499693870544434
Validation loss: 2.425874161463912

Epoch: 5| Step: 1
Training loss: 2.411729335784912
Validation loss: 2.430513007666475

Epoch: 5| Step: 2
Training loss: 2.0425655841827393
Validation loss: 2.432734481749996

Epoch: 5| Step: 3
Training loss: 2.94340443611145
Validation loss: 2.4403623406605055

Epoch: 5| Step: 4
Training loss: 3.2442126274108887
Validation loss: 2.4423591988061064

Epoch: 5| Step: 5
Training loss: 2.3544211387634277
Validation loss: 2.444309462783157

Epoch: 5| Step: 6
Training loss: 3.554189682006836
Validation loss: 2.443802995066489

Epoch: 5| Step: 7
Training loss: 2.6245503425598145
Validation loss: 2.426741025781119

Epoch: 5| Step: 8
Training loss: 2.948819637298584
Validation loss: 2.416634936486521

Epoch: 5| Step: 9
Training loss: 2.479266405105591
Validation loss: 2.4102537708897747

Epoch: 5| Step: 10
Training loss: 2.189680814743042
Validation loss: 2.410001083086896

Epoch: 71| Step: 0
Training loss: 2.0410263538360596
Validation loss: 2.4099207232075353

Epoch: 5| Step: 1
Training loss: 1.9706223011016846
Validation loss: 2.4213761155323317

Epoch: 5| Step: 2
Training loss: 3.0947444438934326
Validation loss: 2.434932724122078

Epoch: 5| Step: 3
Training loss: 2.6776394844055176
Validation loss: 2.4336230831761516

Epoch: 5| Step: 4
Training loss: 3.1215288639068604
Validation loss: 2.425395337484216

Epoch: 5| Step: 5
Training loss: 2.7243893146514893
Validation loss: 2.423775888258411

Epoch: 5| Step: 6
Training loss: 3.031893730163574
Validation loss: 2.4163514901232976

Epoch: 5| Step: 7
Training loss: 3.1841864585876465
Validation loss: 2.41269088304171

Epoch: 5| Step: 8
Training loss: 2.151813507080078
Validation loss: 2.406866181281305

Epoch: 5| Step: 9
Training loss: 2.592890977859497
Validation loss: 2.408103342979185

Epoch: 5| Step: 10
Training loss: 2.5878071784973145
Validation loss: 2.405212461307485

Epoch: 72| Step: 0
Training loss: 2.692680835723877
Validation loss: 2.4062785422930153

Epoch: 5| Step: 1
Training loss: 3.07041072845459
Validation loss: 2.4030701473195064

Epoch: 5| Step: 2
Training loss: 2.95432710647583
Validation loss: 2.4041562593111427

Epoch: 5| Step: 3
Training loss: 2.425833225250244
Validation loss: 2.404557825416647

Epoch: 5| Step: 4
Training loss: 2.4554781913757324
Validation loss: 2.4071096835597867

Epoch: 5| Step: 5
Training loss: 2.9314732551574707
Validation loss: 2.411768259540681

Epoch: 5| Step: 6
Training loss: 2.5349743366241455
Validation loss: 2.4056365028504403

Epoch: 5| Step: 7
Training loss: 2.5725924968719482
Validation loss: 2.4212770077490036

Epoch: 5| Step: 8
Training loss: 2.9250617027282715
Validation loss: 2.4223956305493592

Epoch: 5| Step: 9
Training loss: 1.6207571029663086
Validation loss: 2.427578677413284

Epoch: 5| Step: 10
Training loss: 2.925309658050537
Validation loss: 2.4280645603774698

Epoch: 73| Step: 0
Training loss: 2.0103049278259277
Validation loss: 2.4268811607873566

Epoch: 5| Step: 1
Training loss: 2.7815074920654297
Validation loss: 2.432639727028467

Epoch: 5| Step: 2
Training loss: 2.5056731700897217
Validation loss: 2.43285394483997

Epoch: 5| Step: 3
Training loss: 2.7232718467712402
Validation loss: 2.4265389365534626

Epoch: 5| Step: 4
Training loss: 3.2320778369903564
Validation loss: 2.427500047991353

Epoch: 5| Step: 5
Training loss: 3.311561107635498
Validation loss: 2.4251604439109884

Epoch: 5| Step: 6
Training loss: 2.703174114227295
Validation loss: 2.421961589526105

Epoch: 5| Step: 7
Training loss: 2.518395185470581
Validation loss: 2.414290714007552

Epoch: 5| Step: 8
Training loss: 2.6939451694488525
Validation loss: 2.4255173231965754

Epoch: 5| Step: 9
Training loss: 2.670527219772339
Validation loss: 2.4212786048971195

Epoch: 5| Step: 10
Training loss: 1.7179404497146606
Validation loss: 2.4154198502981536

Epoch: 74| Step: 0
Training loss: 2.234586238861084
Validation loss: 2.409528168298865

Epoch: 5| Step: 1
Training loss: 2.6227755546569824
Validation loss: 2.4146690650652816

Epoch: 5| Step: 2
Training loss: 2.5968613624572754
Validation loss: 2.4201254178118963

Epoch: 5| Step: 3
Training loss: 2.626448392868042
Validation loss: 2.4190191504775838

Epoch: 5| Step: 4
Training loss: 2.1737537384033203
Validation loss: 2.4226815341621317

Epoch: 5| Step: 5
Training loss: 2.52795672416687
Validation loss: 2.4149862579120103

Epoch: 5| Step: 6
Training loss: 2.7771975994110107
Validation loss: 2.4088084159358853

Epoch: 5| Step: 7
Training loss: 2.671374797821045
Validation loss: 2.4051344497229463

Epoch: 5| Step: 8
Training loss: 3.061256170272827
Validation loss: 2.399766247759583

Epoch: 5| Step: 9
Training loss: 2.7634594440460205
Validation loss: 2.3953581958688717

Epoch: 5| Step: 10
Training loss: 2.930936098098755
Validation loss: 2.4016188703557497

Epoch: 75| Step: 0
Training loss: 2.343435764312744
Validation loss: 2.39212227636768

Epoch: 5| Step: 1
Training loss: 2.382472038269043
Validation loss: 2.3905573019417385

Epoch: 5| Step: 2
Training loss: 2.518660068511963
Validation loss: 2.396594903802359

Epoch: 5| Step: 3
Training loss: 2.757107973098755
Validation loss: 2.395789593778631

Epoch: 5| Step: 4
Training loss: 2.6112000942230225
Validation loss: 2.3961598821865615

Epoch: 5| Step: 5
Training loss: 2.5292418003082275
Validation loss: 2.3905621190224924

Epoch: 5| Step: 6
Training loss: 2.3733596801757812
Validation loss: 2.383346096161873

Epoch: 5| Step: 7
Training loss: 2.141531467437744
Validation loss: 2.3962237501657135

Epoch: 5| Step: 8
Training loss: 3.0822155475616455
Validation loss: 2.38960866261554

Epoch: 5| Step: 9
Training loss: 3.280663251876831
Validation loss: 2.4018942053600023

Epoch: 5| Step: 10
Training loss: 3.0322914123535156
Validation loss: 2.3995540167695735

Epoch: 76| Step: 0
Training loss: 2.2521660327911377
Validation loss: 2.4079998590612925

Epoch: 5| Step: 1
Training loss: 2.722562789916992
Validation loss: 2.412493795476934

Epoch: 5| Step: 2
Training loss: 2.8533413410186768
Validation loss: 2.3976790648634716

Epoch: 5| Step: 3
Training loss: 2.711134672164917
Validation loss: 2.3943964255753385

Epoch: 5| Step: 4
Training loss: 2.8059964179992676
Validation loss: 2.3893856207529702

Epoch: 5| Step: 5
Training loss: 1.7516136169433594
Validation loss: 2.389997056735459

Epoch: 5| Step: 6
Training loss: 2.4555776119232178
Validation loss: 2.392921378535609

Epoch: 5| Step: 7
Training loss: 2.4037702083587646
Validation loss: 2.3927434388027398

Epoch: 5| Step: 8
Training loss: 3.2620673179626465
Validation loss: 2.394394120862407

Epoch: 5| Step: 9
Training loss: 2.8939919471740723
Validation loss: 2.3886900922303558

Epoch: 5| Step: 10
Training loss: 2.843921184539795
Validation loss: 2.398328899055399

Epoch: 77| Step: 0
Training loss: 3.15388560295105
Validation loss: 2.397419334739767

Epoch: 5| Step: 1
Training loss: 1.70012629032135
Validation loss: 2.394271477576225

Epoch: 5| Step: 2
Training loss: 2.5879712104797363
Validation loss: 2.398348662161058

Epoch: 5| Step: 3
Training loss: 2.5742335319519043
Validation loss: 2.3947563312386952

Epoch: 5| Step: 4
Training loss: 2.24912428855896
Validation loss: 2.39084634729611

Epoch: 5| Step: 5
Training loss: 3.250481128692627
Validation loss: 2.383415706696049

Epoch: 5| Step: 6
Training loss: 2.0695042610168457
Validation loss: 2.3865529606419225

Epoch: 5| Step: 7
Training loss: 3.470858335494995
Validation loss: 2.3813829037450973

Epoch: 5| Step: 8
Training loss: 2.0399532318115234
Validation loss: 2.3850224812825522

Epoch: 5| Step: 9
Training loss: 2.831923246383667
Validation loss: 2.3830167349948677

Epoch: 5| Step: 10
Training loss: 3.0146782398223877
Validation loss: 2.3847208817799888

Epoch: 78| Step: 0
Training loss: 2.6359481811523438
Validation loss: 2.3849319424680484

Epoch: 5| Step: 1
Training loss: 2.0720250606536865
Validation loss: 2.382561306799612

Epoch: 5| Step: 2
Training loss: 2.4928009510040283
Validation loss: 2.3881142293253252

Epoch: 5| Step: 3
Training loss: 2.4266624450683594
Validation loss: 2.390506021438106

Epoch: 5| Step: 4
Training loss: 3.0510268211364746
Validation loss: 2.3987441575655373

Epoch: 5| Step: 5
Training loss: 2.4749042987823486
Validation loss: 2.398829437071277

Epoch: 5| Step: 6
Training loss: 2.8960204124450684
Validation loss: 2.3946875346604215

Epoch: 5| Step: 7
Training loss: 1.7773220539093018
Validation loss: 2.396092043128065

Epoch: 5| Step: 8
Training loss: 3.632124423980713
Validation loss: 2.393990303880425

Epoch: 5| Step: 9
Training loss: 2.9667751789093018
Validation loss: 2.4024314931643906

Epoch: 5| Step: 10
Training loss: 2.3923051357269287
Validation loss: 2.386028387213266

Epoch: 79| Step: 0
Training loss: 2.2672176361083984
Validation loss: 2.3817735538687757

Epoch: 5| Step: 1
Training loss: 2.8961827754974365
Validation loss: 2.3779477662937616

Epoch: 5| Step: 2
Training loss: 2.7953903675079346
Validation loss: 2.3805905567702426

Epoch: 5| Step: 3
Training loss: 3.007448196411133
Validation loss: 2.379337590227845

Epoch: 5| Step: 4
Training loss: 2.660557746887207
Validation loss: 2.380998601195633

Epoch: 5| Step: 5
Training loss: 2.827665090560913
Validation loss: 2.378516315132059

Epoch: 5| Step: 6
Training loss: 2.9352915287017822
Validation loss: 2.3851541332019273

Epoch: 5| Step: 7
Training loss: 2.6528265476226807
Validation loss: 2.3746272235788326

Epoch: 5| Step: 8
Training loss: 2.1172337532043457
Validation loss: 2.3719442864899993

Epoch: 5| Step: 9
Training loss: 2.4541141986846924
Validation loss: 2.378649637263308

Epoch: 5| Step: 10
Training loss: 2.0432801246643066
Validation loss: 2.378944197008687

Epoch: 80| Step: 0
Training loss: 2.2786002159118652
Validation loss: 2.3920690205789383

Epoch: 5| Step: 1
Training loss: 2.871716022491455
Validation loss: 2.3904675899013395

Epoch: 5| Step: 2
Training loss: 2.5101068019866943
Validation loss: 2.39249155341938

Epoch: 5| Step: 3
Training loss: 2.1453444957733154
Validation loss: 2.402151500025103

Epoch: 5| Step: 4
Training loss: 2.7388248443603516
Validation loss: 2.392697964945147

Epoch: 5| Step: 5
Training loss: 3.0245823860168457
Validation loss: 2.394723948612008

Epoch: 5| Step: 6
Training loss: 1.920994758605957
Validation loss: 2.3870720453159784

Epoch: 5| Step: 7
Training loss: 2.456512928009033
Validation loss: 2.3690400200505413

Epoch: 5| Step: 8
Training loss: 2.751079559326172
Validation loss: 2.365864638359316

Epoch: 5| Step: 9
Training loss: 3.4822304248809814
Validation loss: 2.362062624705735

Epoch: 5| Step: 10
Training loss: 2.520179033279419
Validation loss: 2.3527835517801265

Epoch: 81| Step: 0
Training loss: 2.3325493335723877
Validation loss: 2.3571948261671167

Epoch: 5| Step: 1
Training loss: 2.3275089263916016
Validation loss: 2.3582229306620937

Epoch: 5| Step: 2
Training loss: 2.4184653759002686
Validation loss: 2.34964443022205

Epoch: 5| Step: 3
Training loss: 2.6553916931152344
Validation loss: 2.357379262165357

Epoch: 5| Step: 4
Training loss: 2.437910795211792
Validation loss: 2.3423385568844375

Epoch: 5| Step: 5
Training loss: 2.743624210357666
Validation loss: 2.353053795394077

Epoch: 5| Step: 6
Training loss: 3.118227005004883
Validation loss: 2.351982924246019

Epoch: 5| Step: 7
Training loss: 2.9361379146575928
Validation loss: 2.351212970672115

Epoch: 5| Step: 8
Training loss: 2.7307066917419434
Validation loss: 2.352187318186606

Epoch: 5| Step: 9
Training loss: 2.67220139503479
Validation loss: 2.3530432537037838

Epoch: 5| Step: 10
Training loss: 2.2133030891418457
Validation loss: 2.367609780321839

Epoch: 82| Step: 0
Training loss: 2.627448797225952
Validation loss: 2.37700064720646

Epoch: 5| Step: 1
Training loss: 2.337897539138794
Validation loss: 2.4005209476717058

Epoch: 5| Step: 2
Training loss: 2.4109930992126465
Validation loss: 2.4049801698295017

Epoch: 5| Step: 3
Training loss: 2.217116355895996
Validation loss: 2.383742647786294

Epoch: 5| Step: 4
Training loss: 2.680759906768799
Validation loss: 2.3797428249030985

Epoch: 5| Step: 5
Training loss: 2.6802420616149902
Validation loss: 2.3579245023829962

Epoch: 5| Step: 6
Training loss: 2.4661922454833984
Validation loss: 2.3637015640094714

Epoch: 5| Step: 7
Training loss: 2.865522861480713
Validation loss: 2.35574238018323

Epoch: 5| Step: 8
Training loss: 2.8457629680633545
Validation loss: 2.3596357914709274

Epoch: 5| Step: 9
Training loss: 2.684312105178833
Validation loss: 2.3624821914139615

Epoch: 5| Step: 10
Training loss: 2.9826526641845703
Validation loss: 2.3620593188911356

Epoch: 83| Step: 0
Training loss: 2.697030544281006
Validation loss: 2.3560229039961293

Epoch: 5| Step: 1
Training loss: 2.038252592086792
Validation loss: 2.3636226295143046

Epoch: 5| Step: 2
Training loss: 2.55830454826355
Validation loss: 2.3587189515431723

Epoch: 5| Step: 3
Training loss: 2.823702335357666
Validation loss: 2.366435948238578

Epoch: 5| Step: 4
Training loss: 1.9173853397369385
Validation loss: 2.361587380850187

Epoch: 5| Step: 5
Training loss: 2.3638863563537598
Validation loss: 2.3707740409399873

Epoch: 5| Step: 6
Training loss: 3.188732624053955
Validation loss: 2.363090915064658

Epoch: 5| Step: 7
Training loss: 3.1519439220428467
Validation loss: 2.3597279030789613

Epoch: 5| Step: 8
Training loss: 2.672450542449951
Validation loss: 2.3570530312035674

Epoch: 5| Step: 9
Training loss: 2.8126447200775146
Validation loss: 2.3479815657420824

Epoch: 5| Step: 10
Training loss: 2.260368824005127
Validation loss: 2.354317388226909

Epoch: 84| Step: 0
Training loss: 2.455585479736328
Validation loss: 2.351198714266541

Epoch: 5| Step: 1
Training loss: 3.1644303798675537
Validation loss: 2.346511858765797

Epoch: 5| Step: 2
Training loss: 2.7288904190063477
Validation loss: 2.354063698040542

Epoch: 5| Step: 3
Training loss: 2.0642898082733154
Validation loss: 2.352412090506605

Epoch: 5| Step: 4
Training loss: 1.9433702230453491
Validation loss: 2.3471862936532624

Epoch: 5| Step: 5
Training loss: 2.264690637588501
Validation loss: 2.347801685333252

Epoch: 5| Step: 6
Training loss: 2.089808940887451
Validation loss: 2.3516470437408774

Epoch: 5| Step: 7
Training loss: 3.1973187923431396
Validation loss: 2.346349588004492

Epoch: 5| Step: 8
Training loss: 2.389059543609619
Validation loss: 2.347193210355697

Epoch: 5| Step: 9
Training loss: 2.8958888053894043
Validation loss: 2.3454026458083943

Epoch: 5| Step: 10
Training loss: 3.4984912872314453
Validation loss: 2.3587577624987532

Epoch: 85| Step: 0
Training loss: 2.1642961502075195
Validation loss: 2.3696328734838836

Epoch: 5| Step: 1
Training loss: 2.93520188331604
Validation loss: 2.3702269369556057

Epoch: 5| Step: 2
Training loss: 2.3125901222229004
Validation loss: 2.3724539895211496

Epoch: 5| Step: 3
Training loss: 2.5634734630584717
Validation loss: 2.372246185938517

Epoch: 5| Step: 4
Training loss: 2.3820691108703613
Validation loss: 2.371251900990804

Epoch: 5| Step: 5
Training loss: 2.4018425941467285
Validation loss: 2.365757478180752

Epoch: 5| Step: 6
Training loss: 3.067495822906494
Validation loss: 2.365299158198859

Epoch: 5| Step: 7
Training loss: 2.2234947681427
Validation loss: 2.343670580976753

Epoch: 5| Step: 8
Training loss: 2.6753602027893066
Validation loss: 2.3386068395389024

Epoch: 5| Step: 9
Training loss: 3.012596845626831
Validation loss: 2.3370662889172955

Epoch: 5| Step: 10
Training loss: 2.850675582885742
Validation loss: 2.342428881634948

Epoch: 86| Step: 0
Training loss: 2.2098896503448486
Validation loss: 2.341756941169821

Epoch: 5| Step: 1
Training loss: 2.3695430755615234
Validation loss: 2.347411927356515

Epoch: 5| Step: 2
Training loss: 2.593266010284424
Validation loss: 2.338141682327435

Epoch: 5| Step: 3
Training loss: 2.388775110244751
Validation loss: 2.351199673068139

Epoch: 5| Step: 4
Training loss: 2.908278703689575
Validation loss: 2.345016389764765

Epoch: 5| Step: 5
Training loss: 2.8689541816711426
Validation loss: 2.3413571209035893

Epoch: 5| Step: 6
Training loss: 3.2898948192596436
Validation loss: 2.3486351146492908

Epoch: 5| Step: 7
Training loss: 2.6399378776550293
Validation loss: 2.339450277307982

Epoch: 5| Step: 8
Training loss: 2.3685717582702637
Validation loss: 2.341108637471353

Epoch: 5| Step: 9
Training loss: 2.636542558670044
Validation loss: 2.3534300981029386

Epoch: 5| Step: 10
Training loss: 2.3251357078552246
Validation loss: 2.35943635689315

Epoch: 87| Step: 0
Training loss: 2.7907981872558594
Validation loss: 2.356066455123245

Epoch: 5| Step: 1
Training loss: 3.0664334297180176
Validation loss: 2.35431029206963

Epoch: 5| Step: 2
Training loss: 2.1967339515686035
Validation loss: 2.357242698310524

Epoch: 5| Step: 3
Training loss: 2.6607978343963623
Validation loss: 2.344820358419931

Epoch: 5| Step: 4
Training loss: 2.7754178047180176
Validation loss: 2.3519985470720517

Epoch: 5| Step: 5
Training loss: 2.594867706298828
Validation loss: 2.3538288711219706

Epoch: 5| Step: 6
Training loss: 2.1590168476104736
Validation loss: 2.3531714870083715

Epoch: 5| Step: 7
Training loss: 2.4033331871032715
Validation loss: 2.3546835043097056

Epoch: 5| Step: 8
Training loss: 2.4309725761413574
Validation loss: 2.350569099508306

Epoch: 5| Step: 9
Training loss: 2.661520004272461
Validation loss: 2.3490915734280824

Epoch: 5| Step: 10
Training loss: 2.893746852874756
Validation loss: 2.350462229021134

Epoch: 88| Step: 0
Training loss: 2.389152765274048
Validation loss: 2.351467396623345

Epoch: 5| Step: 1
Training loss: 2.532689332962036
Validation loss: 2.351530618565057

Epoch: 5| Step: 2
Training loss: 1.9994518756866455
Validation loss: 2.3482287263357513

Epoch: 5| Step: 3
Training loss: 2.770860195159912
Validation loss: 2.357956414581627

Epoch: 5| Step: 4
Training loss: 2.466177225112915
Validation loss: 2.3507572784218738

Epoch: 5| Step: 5
Training loss: 3.0776619911193848
Validation loss: 2.363013764863373

Epoch: 5| Step: 6
Training loss: 2.5599796772003174
Validation loss: 2.3663997701419297

Epoch: 5| Step: 7
Training loss: 2.6432945728302
Validation loss: 2.374144566956387

Epoch: 5| Step: 8
Training loss: 2.9058985710144043
Validation loss: 2.3773258937302457

Epoch: 5| Step: 9
Training loss: 3.043339967727661
Validation loss: 2.365517140716635

Epoch: 5| Step: 10
Training loss: 2.106215238571167
Validation loss: 2.3638707924914617

Epoch: 89| Step: 0
Training loss: 2.800520420074463
Validation loss: 2.363257582469653

Epoch: 5| Step: 1
Training loss: 2.7792320251464844
Validation loss: 2.3589488203807543

Epoch: 5| Step: 2
Training loss: 2.091655731201172
Validation loss: 2.3623873110740417

Epoch: 5| Step: 3
Training loss: 3.0463905334472656
Validation loss: 2.348897541722944

Epoch: 5| Step: 4
Training loss: 2.5415401458740234
Validation loss: 2.347245608606646

Epoch: 5| Step: 5
Training loss: 2.508035182952881
Validation loss: 2.3394376001050396

Epoch: 5| Step: 6
Training loss: 2.7275071144104004
Validation loss: 2.34385706276022

Epoch: 5| Step: 7
Training loss: 2.5612874031066895
Validation loss: 2.3370893604011944

Epoch: 5| Step: 8
Training loss: 2.1932172775268555
Validation loss: 2.3389666490657355

Epoch: 5| Step: 9
Training loss: 2.9020767211914062
Validation loss: 2.3336763715231292

Epoch: 5| Step: 10
Training loss: 2.1959879398345947
Validation loss: 2.342052618662516

Epoch: 90| Step: 0
Training loss: 1.9071038961410522
Validation loss: 2.3392589220436673

Epoch: 5| Step: 1
Training loss: 2.701850175857544
Validation loss: 2.3344601405564176

Epoch: 5| Step: 2
Training loss: 2.4587149620056152
Validation loss: 2.3423669722772416

Epoch: 5| Step: 3
Training loss: 2.564809799194336
Validation loss: 2.3340667729736655

Epoch: 5| Step: 4
Training loss: 2.3589367866516113
Validation loss: 2.334938436426142

Epoch: 5| Step: 5
Training loss: 2.6580605506896973
Validation loss: 2.332189311263382

Epoch: 5| Step: 6
Training loss: 2.254443407058716
Validation loss: 2.337724562614195

Epoch: 5| Step: 7
Training loss: 3.2607064247131348
Validation loss: 2.3347873431380077

Epoch: 5| Step: 8
Training loss: 2.8759543895721436
Validation loss: 2.3445735208449827

Epoch: 5| Step: 9
Training loss: 2.865353584289551
Validation loss: 2.3383450149207987

Epoch: 5| Step: 10
Training loss: 2.4915361404418945
Validation loss: 2.3355357800760577

Epoch: 91| Step: 0
Training loss: 2.838846206665039
Validation loss: 2.3452730512106292

Epoch: 5| Step: 1
Training loss: 2.8279361724853516
Validation loss: 2.340333789907476

Epoch: 5| Step: 2
Training loss: 2.603151559829712
Validation loss: 2.338936536542831

Epoch: 5| Step: 3
Training loss: 1.881739616394043
Validation loss: 2.34654297367219

Epoch: 5| Step: 4
Training loss: 2.9556710720062256
Validation loss: 2.352194934762934

Epoch: 5| Step: 5
Training loss: 3.134173631668091
Validation loss: 2.3472842324164604

Epoch: 5| Step: 6
Training loss: 2.7019729614257812
Validation loss: 2.3363895095804685

Epoch: 5| Step: 7
Training loss: 2.154658555984497
Validation loss: 2.3274095545532885

Epoch: 5| Step: 8
Training loss: 2.7200260162353516
Validation loss: 2.330491824816632

Epoch: 5| Step: 9
Training loss: 2.0834155082702637
Validation loss: 2.3325469647684405

Epoch: 5| Step: 10
Training loss: 2.4028728008270264
Validation loss: 2.335216240216327

Epoch: 92| Step: 0
Training loss: 2.124117374420166
Validation loss: 2.32860339713353

Epoch: 5| Step: 1
Training loss: 2.775912284851074
Validation loss: 2.323463573250719

Epoch: 5| Step: 2
Training loss: 2.1710872650146484
Validation loss: 2.3282099103414886

Epoch: 5| Step: 3
Training loss: 2.679670810699463
Validation loss: 2.3202124821242465

Epoch: 5| Step: 4
Training loss: 3.1917736530303955
Validation loss: 2.3259197614526235

Epoch: 5| Step: 5
Training loss: 2.642845630645752
Validation loss: 2.3293747722461657

Epoch: 5| Step: 6
Training loss: 2.2469685077667236
Validation loss: 2.3312069549355456

Epoch: 5| Step: 7
Training loss: 3.4698262214660645
Validation loss: 2.3363274323043

Epoch: 5| Step: 8
Training loss: 2.1277568340301514
Validation loss: 2.3277946774677565

Epoch: 5| Step: 9
Training loss: 2.454315423965454
Validation loss: 2.3307655216545187

Epoch: 5| Step: 10
Training loss: 2.457803249359131
Validation loss: 2.342212415510608

Epoch: 93| Step: 0
Training loss: 2.9522182941436768
Validation loss: 2.3490458252609416

Epoch: 5| Step: 1
Training loss: 2.4620883464813232
Validation loss: 2.343918426062471

Epoch: 5| Step: 2
Training loss: 2.215547561645508
Validation loss: 2.345609216279881

Epoch: 5| Step: 3
Training loss: 1.897275686264038
Validation loss: 2.343131452478388

Epoch: 5| Step: 4
Training loss: 2.794748067855835
Validation loss: 2.3398827455377065

Epoch: 5| Step: 5
Training loss: 2.7699642181396484
Validation loss: 2.337899818215319

Epoch: 5| Step: 6
Training loss: 2.5205678939819336
Validation loss: 2.3286953510776645

Epoch: 5| Step: 7
Training loss: 3.0839920043945312
Validation loss: 2.331151682843444

Epoch: 5| Step: 8
Training loss: 2.648214101791382
Validation loss: 2.331898150905486

Epoch: 5| Step: 9
Training loss: 2.686661958694458
Validation loss: 2.3254101007215437

Epoch: 5| Step: 10
Training loss: 2.2032320499420166
Validation loss: 2.3228475534787743

Epoch: 94| Step: 0
Training loss: 1.978367805480957
Validation loss: 2.3258447877822386

Epoch: 5| Step: 1
Training loss: 2.018505573272705
Validation loss: 2.32758338733386

Epoch: 5| Step: 2
Training loss: 2.4968955516815186
Validation loss: 2.315136606975268

Epoch: 5| Step: 3
Training loss: 2.7914581298828125
Validation loss: 2.321877289843816

Epoch: 5| Step: 4
Training loss: 3.554445743560791
Validation loss: 2.3217966633458293

Epoch: 5| Step: 5
Training loss: 2.1107921600341797
Validation loss: 2.3261737285121793

Epoch: 5| Step: 6
Training loss: 2.585202217102051
Validation loss: 2.3216086946507937

Epoch: 5| Step: 7
Training loss: 2.983103036880493
Validation loss: 2.319730556139382

Epoch: 5| Step: 8
Training loss: 2.792574405670166
Validation loss: 2.3215362794937624

Epoch: 5| Step: 9
Training loss: 2.69352388381958
Validation loss: 2.325731098010976

Epoch: 5| Step: 10
Training loss: 2.343313217163086
Validation loss: 2.328565207860803

Epoch: 95| Step: 0
Training loss: 2.4906060695648193
Validation loss: 2.3245476163843626

Epoch: 5| Step: 1
Training loss: 2.4053502082824707
Validation loss: 2.325006705458446

Epoch: 5| Step: 2
Training loss: 2.63041353225708
Validation loss: 2.323726728398313

Epoch: 5| Step: 3
Training loss: 2.8431127071380615
Validation loss: 2.322657213416151

Epoch: 5| Step: 4
Training loss: 2.542320489883423
Validation loss: 2.321040668795186

Epoch: 5| Step: 5
Training loss: 3.083345651626587
Validation loss: 2.3225862697888444

Epoch: 5| Step: 6
Training loss: 2.6766674518585205
Validation loss: 2.3229602947030017

Epoch: 5| Step: 7
Training loss: 2.3839735984802246
Validation loss: 2.326364037811115

Epoch: 5| Step: 8
Training loss: 2.2096638679504395
Validation loss: 2.3246128251475673

Epoch: 5| Step: 9
Training loss: 2.5065271854400635
Validation loss: 2.316765098161595

Epoch: 5| Step: 10
Training loss: 2.452103614807129
Validation loss: 2.327078006600821

Epoch: 96| Step: 0
Training loss: 2.0064103603363037
Validation loss: 2.322513654667844

Epoch: 5| Step: 1
Training loss: 3.0933849811553955
Validation loss: 2.3256493306929067

Epoch: 5| Step: 2
Training loss: 2.4151227474212646
Validation loss: 2.327653226032052

Epoch: 5| Step: 3
Training loss: 2.5538604259490967
Validation loss: 2.3260500277242353

Epoch: 5| Step: 4
Training loss: 2.4238362312316895
Validation loss: 2.327282113413657

Epoch: 5| Step: 5
Training loss: 2.3743345737457275
Validation loss: 2.3234190761402087

Epoch: 5| Step: 6
Training loss: 2.76121187210083
Validation loss: 2.3312777755081013

Epoch: 5| Step: 7
Training loss: 3.005840301513672
Validation loss: 2.318317626112251

Epoch: 5| Step: 8
Training loss: 2.909364938735962
Validation loss: 2.3223543833660822

Epoch: 5| Step: 9
Training loss: 2.1352949142456055
Validation loss: 2.3146566819119196

Epoch: 5| Step: 10
Training loss: 2.560729742050171
Validation loss: 2.3153713287845736

Epoch: 97| Step: 0
Training loss: 2.659146547317505
Validation loss: 2.310829672762143

Epoch: 5| Step: 1
Training loss: 2.548089027404785
Validation loss: 2.3111228968507502

Epoch: 5| Step: 2
Training loss: 2.482827663421631
Validation loss: 2.3117471023272445

Epoch: 5| Step: 3
Training loss: 3.1064388751983643
Validation loss: 2.312229815349784

Epoch: 5| Step: 4
Training loss: 2.204437017440796
Validation loss: 2.3111902629175494

Epoch: 5| Step: 5
Training loss: 2.4243240356445312
Validation loss: 2.304783216086767

Epoch: 5| Step: 6
Training loss: 3.0233497619628906
Validation loss: 2.3059669617683656

Epoch: 5| Step: 7
Training loss: 2.5996720790863037
Validation loss: 2.3147085302619526

Epoch: 5| Step: 8
Training loss: 2.0906906127929688
Validation loss: 2.306162254784697

Epoch: 5| Step: 9
Training loss: 2.390515089035034
Validation loss: 2.311845648673273

Epoch: 5| Step: 10
Training loss: 2.74135422706604
Validation loss: 2.314090403177405

Epoch: 98| Step: 0
Training loss: 2.5739853382110596
Validation loss: 2.3174187393598658

Epoch: 5| Step: 1
Training loss: 2.857433319091797
Validation loss: 2.323043602769093

Epoch: 5| Step: 2
Training loss: 2.2718918323516846
Validation loss: 2.3379861975228913

Epoch: 5| Step: 3
Training loss: 2.4189114570617676
Validation loss: 2.3425248489584973

Epoch: 5| Step: 4
Training loss: 2.34212589263916
Validation loss: 2.3532468580430552

Epoch: 5| Step: 5
Training loss: 2.7690014839172363
Validation loss: 2.35812634037387

Epoch: 5| Step: 6
Training loss: 2.28583025932312
Validation loss: 2.3585650741413073

Epoch: 5| Step: 7
Training loss: 2.55134654045105
Validation loss: 2.3550345308037213

Epoch: 5| Step: 8
Training loss: 2.917219638824463
Validation loss: 2.3468018526672036

Epoch: 5| Step: 9
Training loss: 2.539149522781372
Validation loss: 2.3439267373854116

Epoch: 5| Step: 10
Training loss: 2.787050724029541
Validation loss: 2.3346047093791347

Epoch: 99| Step: 0
Training loss: 2.564775228500366
Validation loss: 2.3248260764665503

Epoch: 5| Step: 1
Training loss: 2.0963778495788574
Validation loss: 2.3149662735641643

Epoch: 5| Step: 2
Training loss: 2.4441635608673096
Validation loss: 2.3072232097707768

Epoch: 5| Step: 3
Training loss: 2.377347469329834
Validation loss: 2.3136828304618917

Epoch: 5| Step: 4
Training loss: 2.823864698410034
Validation loss: 2.298822210681054

Epoch: 5| Step: 5
Training loss: 1.8648741245269775
Validation loss: 2.3009876179438766

Epoch: 5| Step: 6
Training loss: 2.772324800491333
Validation loss: 2.3020415895728656

Epoch: 5| Step: 7
Training loss: 3.2571609020233154
Validation loss: 2.3131476679155902

Epoch: 5| Step: 8
Training loss: 3.1205456256866455
Validation loss: 2.330910118677283

Epoch: 5| Step: 9
Training loss: 2.238790988922119
Validation loss: 2.3392922186082408

Epoch: 5| Step: 10
Training loss: 2.5775609016418457
Validation loss: 2.3605767655116257

Epoch: 100| Step: 0
Training loss: 2.57047963142395
Validation loss: 2.3587997882596907

Epoch: 5| Step: 1
Training loss: 1.9792792797088623
Validation loss: 2.3602446586854997

Epoch: 5| Step: 2
Training loss: 2.482536554336548
Validation loss: 2.3580562350570515

Epoch: 5| Step: 3
Training loss: 2.6587023735046387
Validation loss: 2.352390766143799

Epoch: 5| Step: 4
Training loss: 3.2013707160949707
Validation loss: 2.335307603241295

Epoch: 5| Step: 5
Training loss: 2.5993809700012207
Validation loss: 2.313729078538956

Epoch: 5| Step: 6
Training loss: 2.3353068828582764
Validation loss: 2.2999587571749123

Epoch: 5| Step: 7
Training loss: 2.5302770137786865
Validation loss: 2.3052906297868296

Epoch: 5| Step: 8
Training loss: 2.6200554370880127
Validation loss: 2.298773883491434

Epoch: 5| Step: 9
Training loss: 2.7874855995178223
Validation loss: 2.2974468200437483

Epoch: 5| Step: 10
Training loss: 2.5155606269836426
Validation loss: 2.298192616431944

Epoch: 101| Step: 0
Training loss: 2.515364170074463
Validation loss: 2.2921392007540633

Epoch: 5| Step: 1
Training loss: 2.9975082874298096
Validation loss: 2.297587586987403

Epoch: 5| Step: 2
Training loss: 2.3450064659118652
Validation loss: 2.305185164174726

Epoch: 5| Step: 3
Training loss: 2.482506036758423
Validation loss: 2.2956431809292046

Epoch: 5| Step: 4
Training loss: 2.31908917427063
Validation loss: 2.300561276815271

Epoch: 5| Step: 5
Training loss: 2.2508327960968018
Validation loss: 2.3009606304989068

Epoch: 5| Step: 6
Training loss: 2.460071086883545
Validation loss: 2.2934582438520206

Epoch: 5| Step: 7
Training loss: 3.045381784439087
Validation loss: 2.298850292800575

Epoch: 5| Step: 8
Training loss: 2.579603672027588
Validation loss: 2.2948331294521207

Epoch: 5| Step: 9
Training loss: 3.0872082710266113
Validation loss: 2.2957124094809256

Epoch: 5| Step: 10
Training loss: 2.135615587234497
Validation loss: 2.2887721471889044

Epoch: 102| Step: 0
Training loss: 2.2886276245117188
Validation loss: 2.299777735945999

Epoch: 5| Step: 1
Training loss: 2.426863670349121
Validation loss: 2.2950853301632788

Epoch: 5| Step: 2
Training loss: 2.9563326835632324
Validation loss: 2.297251303990682

Epoch: 5| Step: 3
Training loss: 2.6878228187561035
Validation loss: 2.3008281851327546

Epoch: 5| Step: 4
Training loss: 2.0957469940185547
Validation loss: 2.297615753707065

Epoch: 5| Step: 5
Training loss: 2.760960102081299
Validation loss: 2.3064760213257163

Epoch: 5| Step: 6
Training loss: 2.9029483795166016
Validation loss: 2.3040380888087775

Epoch: 5| Step: 7
Training loss: 2.033386707305908
Validation loss: 2.298420024174516

Epoch: 5| Step: 8
Training loss: 2.400557279586792
Validation loss: 2.297290007273356

Epoch: 5| Step: 9
Training loss: 2.6463942527770996
Validation loss: 2.303694009780884

Epoch: 5| Step: 10
Training loss: 2.961277961730957
Validation loss: 2.299250018212103

Epoch: 103| Step: 0
Training loss: 2.2898640632629395
Validation loss: 2.3015715909260575

Epoch: 5| Step: 1
Training loss: 3.0680274963378906
Validation loss: 2.3012337094994

Epoch: 5| Step: 2
Training loss: 2.350311040878296
Validation loss: 2.3146384710906656

Epoch: 5| Step: 3
Training loss: 2.2993335723876953
Validation loss: 2.3143409785404

Epoch: 5| Step: 4
Training loss: 2.9849414825439453
Validation loss: 2.3178485439669703

Epoch: 5| Step: 5
Training loss: 2.494703769683838
Validation loss: 2.326749304289459

Epoch: 5| Step: 6
Training loss: 1.9992625713348389
Validation loss: 2.326967693144275

Epoch: 5| Step: 7
Training loss: 2.7986397743225098
Validation loss: 2.3281985585407545

Epoch: 5| Step: 8
Training loss: 2.990938663482666
Validation loss: 2.334095537021596

Epoch: 5| Step: 9
Training loss: 2.3981635570526123
Validation loss: 2.329168391484086

Epoch: 5| Step: 10
Training loss: 2.3937652111053467
Validation loss: 2.316083700426163

Epoch: 104| Step: 0
Training loss: 2.932534694671631
Validation loss: 2.3322235999568814

Epoch: 5| Step: 1
Training loss: 2.398679256439209
Validation loss: 2.336816603137601

Epoch: 5| Step: 2
Training loss: 2.2085015773773193
Validation loss: 2.312616435430383

Epoch: 5| Step: 3
Training loss: 1.854675531387329
Validation loss: 2.3019437764280584

Epoch: 5| Step: 4
Training loss: 3.3659558296203613
Validation loss: 2.293291579010666

Epoch: 5| Step: 5
Training loss: 2.827409505844116
Validation loss: 2.288036523326751

Epoch: 5| Step: 6
Training loss: 2.5585758686065674
Validation loss: 2.2870831412653767

Epoch: 5| Step: 7
Training loss: 2.2375504970550537
Validation loss: 2.2909236467012795

Epoch: 5| Step: 8
Training loss: 2.6913390159606934
Validation loss: 2.2871506444869505

Epoch: 5| Step: 9
Training loss: 3.0549516677856445
Validation loss: 2.2903189415572793

Epoch: 5| Step: 10
Training loss: 1.808118462562561
Validation loss: 2.291312789404264

Epoch: 105| Step: 0
Training loss: 2.09165620803833
Validation loss: 2.2901356886791926

Epoch: 5| Step: 1
Training loss: 2.6271843910217285
Validation loss: 2.2964744542234685

Epoch: 5| Step: 2
Training loss: 3.2638039588928223
Validation loss: 2.291247626786591

Epoch: 5| Step: 3
Training loss: 2.221527099609375
Validation loss: 2.296838847539758

Epoch: 5| Step: 4
Training loss: 2.796581745147705
Validation loss: 2.286212352014357

Epoch: 5| Step: 5
Training loss: 2.852374315261841
Validation loss: 2.2825938476029264

Epoch: 5| Step: 6
Training loss: 2.4845051765441895
Validation loss: 2.2861355171408704

Epoch: 5| Step: 7
Training loss: 2.253286600112915
Validation loss: 2.2860325228783394

Epoch: 5| Step: 8
Training loss: 2.678255558013916
Validation loss: 2.292450765127777

Epoch: 5| Step: 9
Training loss: 2.2816224098205566
Validation loss: 2.2810803433900237

Epoch: 5| Step: 10
Training loss: 2.5163135528564453
Validation loss: 2.2864730486305813

Epoch: 106| Step: 0
Training loss: 2.980402946472168
Validation loss: 2.2853351100798576

Epoch: 5| Step: 1
Training loss: 2.638195514678955
Validation loss: 2.2843971406259844

Epoch: 5| Step: 2
Training loss: 2.6223764419555664
Validation loss: 2.282975519857099

Epoch: 5| Step: 3
Training loss: 2.1458804607391357
Validation loss: 2.2848019266641266

Epoch: 5| Step: 4
Training loss: 2.6036720275878906
Validation loss: 2.287446706525741

Epoch: 5| Step: 5
Training loss: 2.2342934608459473
Validation loss: 2.2864839338487193

Epoch: 5| Step: 6
Training loss: 2.393710136413574
Validation loss: 2.296205561648133

Epoch: 5| Step: 7
Training loss: 2.6634230613708496
Validation loss: 2.2999265745121944

Epoch: 5| Step: 8
Training loss: 2.343204975128174
Validation loss: 2.2988395152553434

Epoch: 5| Step: 9
Training loss: 2.7771716117858887
Validation loss: 2.2971011951405513

Epoch: 5| Step: 10
Training loss: 2.6571199893951416
Validation loss: 2.2984591837852233

Epoch: 107| Step: 0
Training loss: 3.0042967796325684
Validation loss: 2.29258361939461

Epoch: 5| Step: 1
Training loss: 2.3150668144226074
Validation loss: 2.29199069546115

Epoch: 5| Step: 2
Training loss: 2.6332411766052246
Validation loss: 2.2882901340402584

Epoch: 5| Step: 3
Training loss: 1.8306306600570679
Validation loss: 2.2810075308686946

Epoch: 5| Step: 4
Training loss: 3.024862289428711
Validation loss: 2.2784611025164203

Epoch: 5| Step: 5
Training loss: 2.1386215686798096
Validation loss: 2.2796988307788806

Epoch: 5| Step: 6
Training loss: 2.956468105316162
Validation loss: 2.2824814832338722

Epoch: 5| Step: 7
Training loss: 2.3174991607666016
Validation loss: 2.287308327613338

Epoch: 5| Step: 8
Training loss: 2.502716302871704
Validation loss: 2.3015594687513126

Epoch: 5| Step: 9
Training loss: 2.8398032188415527
Validation loss: 2.3007136621782855

Epoch: 5| Step: 10
Training loss: 2.3973448276519775
Validation loss: 2.296406966383739

Epoch: 108| Step: 0
Training loss: 2.5175065994262695
Validation loss: 2.305622213630266

Epoch: 5| Step: 1
Training loss: 2.8489441871643066
Validation loss: 2.3132417740360385

Epoch: 5| Step: 2
Training loss: 2.516021728515625
Validation loss: 2.2947057421489427

Epoch: 5| Step: 3
Training loss: 3.0404958724975586
Validation loss: 2.295616667757752

Epoch: 5| Step: 4
Training loss: 2.450230836868286
Validation loss: 2.2810917259544454

Epoch: 5| Step: 5
Training loss: 2.773733377456665
Validation loss: 2.2810346695684616

Epoch: 5| Step: 6
Training loss: 2.6944613456726074
Validation loss: 2.2735419042648806

Epoch: 5| Step: 7
Training loss: 1.9691712856292725
Validation loss: 2.277946282458562

Epoch: 5| Step: 8
Training loss: 2.9020793437957764
Validation loss: 2.285451830074351

Epoch: 5| Step: 9
Training loss: 2.3791027069091797
Validation loss: 2.291367559022801

Epoch: 5| Step: 10
Training loss: 1.9272364377975464
Validation loss: 2.2886967761542207

Epoch: 109| Step: 0
Training loss: 2.1318154335021973
Validation loss: 2.281434682107741

Epoch: 5| Step: 1
Training loss: 2.4801883697509766
Validation loss: 2.2770754316801667

Epoch: 5| Step: 2
Training loss: 2.4840924739837646
Validation loss: 2.287020311560682

Epoch: 5| Step: 3
Training loss: 2.657771348953247
Validation loss: 2.2927606721078195

Epoch: 5| Step: 4
Training loss: 2.9710330963134766
Validation loss: 2.3157936103882326

Epoch: 5| Step: 5
Training loss: 2.1401381492614746
Validation loss: 2.3345152972846903

Epoch: 5| Step: 6
Training loss: 2.3196187019348145
Validation loss: 2.3442957606366885

Epoch: 5| Step: 7
Training loss: 2.9833428859710693
Validation loss: 2.3650383718552126

Epoch: 5| Step: 8
Training loss: 2.7026798725128174
Validation loss: 2.3435914926631476

Epoch: 5| Step: 9
Training loss: 2.366669178009033
Validation loss: 2.3292936407109743

Epoch: 5| Step: 10
Training loss: 2.8311541080474854
Validation loss: 2.3109593186327206

Epoch: 110| Step: 0
Training loss: 2.3994688987731934
Validation loss: 2.2847357514084026

Epoch: 5| Step: 1
Training loss: 2.0958497524261475
Validation loss: 2.282008663300545

Epoch: 5| Step: 2
Training loss: 1.9719120264053345
Validation loss: 2.283742045843473

Epoch: 5| Step: 3
Training loss: 2.2279608249664307
Validation loss: 2.2841455013521257

Epoch: 5| Step: 4
Training loss: 2.3329694271087646
Validation loss: 2.2822575389697985

Epoch: 5| Step: 5
Training loss: 2.280545711517334
Validation loss: 2.2822473561891945

Epoch: 5| Step: 6
Training loss: 3.013434886932373
Validation loss: 2.286240323897331

Epoch: 5| Step: 7
Training loss: 3.5285027027130127
Validation loss: 2.289809637172248

Epoch: 5| Step: 8
Training loss: 2.5409483909606934
Validation loss: 2.29036190689251

Epoch: 5| Step: 9
Training loss: 2.8262829780578613
Validation loss: 2.2916551251565256

Epoch: 5| Step: 10
Training loss: 2.8091776371002197
Validation loss: 2.2850844706258466

Epoch: 111| Step: 0
Training loss: 2.3397552967071533
Validation loss: 2.2899665319791405

Epoch: 5| Step: 1
Training loss: 2.7883410453796387
Validation loss: 2.2850565705248105

Epoch: 5| Step: 2
Training loss: 2.645326614379883
Validation loss: 2.2871735326705442

Epoch: 5| Step: 3
Training loss: 2.5811729431152344
Validation loss: 2.294758915901184

Epoch: 5| Step: 4
Training loss: 2.416447401046753
Validation loss: 2.2830004486986386

Epoch: 5| Step: 5
Training loss: 2.2143146991729736
Validation loss: 2.2823054585405576

Epoch: 5| Step: 6
Training loss: 2.441220998764038
Validation loss: 2.2862688213266353

Epoch: 5| Step: 7
Training loss: 2.5201210975646973
Validation loss: 2.282064994176229

Epoch: 5| Step: 8
Training loss: 2.854936122894287
Validation loss: 2.277373326722012

Epoch: 5| Step: 9
Training loss: 2.371868133544922
Validation loss: 2.2835647521480436

Epoch: 5| Step: 10
Training loss: 2.7110116481781006
Validation loss: 2.2910028503787134

Epoch: 112| Step: 0
Training loss: 2.522498369216919
Validation loss: 2.2992988837662565

Epoch: 5| Step: 1
Training loss: 2.285179853439331
Validation loss: 2.2949422367157473

Epoch: 5| Step: 2
Training loss: 2.3611292839050293
Validation loss: 2.3151585261027017

Epoch: 5| Step: 3
Training loss: 2.6173019409179688
Validation loss: 2.3458936829720773

Epoch: 5| Step: 4
Training loss: 2.54463791847229
Validation loss: 2.3437148781232935

Epoch: 5| Step: 5
Training loss: 2.9997990131378174
Validation loss: 2.3296454388608216

Epoch: 5| Step: 6
Training loss: 2.257369041442871
Validation loss: 2.3244883885947605

Epoch: 5| Step: 7
Training loss: 2.2517037391662598
Validation loss: 2.3306073373363865

Epoch: 5| Step: 8
Training loss: 2.718475818634033
Validation loss: 2.3203307479940434

Epoch: 5| Step: 9
Training loss: 2.5140843391418457
Validation loss: 2.303717536310996

Epoch: 5| Step: 10
Training loss: 2.818084478378296
Validation loss: 2.2798315658364245

Epoch: 113| Step: 0
Training loss: 2.6580028533935547
Validation loss: 2.2856359122901835

Epoch: 5| Step: 1
Training loss: 2.006610870361328
Validation loss: 2.2733385716715167

Epoch: 5| Step: 2
Training loss: 3.2605457305908203
Validation loss: 2.2732695559019684

Epoch: 5| Step: 3
Training loss: 3.1082470417022705
Validation loss: 2.264571384717059

Epoch: 5| Step: 4
Training loss: 2.4945056438446045
Validation loss: 2.271928466776366

Epoch: 5| Step: 5
Training loss: 2.2245445251464844
Validation loss: 2.2696720002799906

Epoch: 5| Step: 6
Training loss: 1.8850419521331787
Validation loss: 2.2715526319319204

Epoch: 5| Step: 7
Training loss: 2.5749399662017822
Validation loss: 2.275500180900738

Epoch: 5| Step: 8
Training loss: 2.4789748191833496
Validation loss: 2.285969021499798

Epoch: 5| Step: 9
Training loss: 2.403252363204956
Validation loss: 2.3013811726723947

Epoch: 5| Step: 10
Training loss: 2.7979886531829834
Validation loss: 2.2899624263086626

Epoch: 114| Step: 0
Training loss: 2.8053245544433594
Validation loss: 2.3064610112097954

Epoch: 5| Step: 1
Training loss: 2.6127171516418457
Validation loss: 2.3151823782151744

Epoch: 5| Step: 2
Training loss: 2.9197216033935547
Validation loss: 2.3158629376401185

Epoch: 5| Step: 3
Training loss: 2.8114733695983887
Validation loss: 2.3058704791530484

Epoch: 5| Step: 4
Training loss: 2.9127745628356934
Validation loss: 2.2941479798286193

Epoch: 5| Step: 5
Training loss: 2.591505765914917
Validation loss: 2.2922085228786675

Epoch: 5| Step: 6
Training loss: 2.6184334754943848
Validation loss: 2.287998381481376

Epoch: 5| Step: 7
Training loss: 2.2876689434051514
Validation loss: 2.2845689173667663

Epoch: 5| Step: 8
Training loss: 2.4030649662017822
Validation loss: 2.283122388265466

Epoch: 5| Step: 9
Training loss: 2.1146926879882812
Validation loss: 2.2852956864141647

Epoch: 5| Step: 10
Training loss: 1.7943227291107178
Validation loss: 2.2821715929174937

Epoch: 115| Step: 0
Training loss: 2.6726901531219482
Validation loss: 2.279621462668142

Epoch: 5| Step: 1
Training loss: 2.4027199745178223
Validation loss: 2.277017649783883

Epoch: 5| Step: 2
Training loss: 2.0587074756622314
Validation loss: 2.279565736811648

Epoch: 5| Step: 3
Training loss: 2.9286723136901855
Validation loss: 2.2808643746119674

Epoch: 5| Step: 4
Training loss: 2.2672555446624756
Validation loss: 2.2945798981574272

Epoch: 5| Step: 5
Training loss: 2.8164703845977783
Validation loss: 2.2916838430589244

Epoch: 5| Step: 6
Training loss: 2.61808705329895
Validation loss: 2.281229057619649

Epoch: 5| Step: 7
Training loss: 2.0119547843933105
Validation loss: 2.282977132387059

Epoch: 5| Step: 8
Training loss: 2.669548511505127
Validation loss: 2.2854059947434293

Epoch: 5| Step: 9
Training loss: 2.7863967418670654
Validation loss: 2.2732208749299407

Epoch: 5| Step: 10
Training loss: 2.66052508354187
Validation loss: 2.2663714065346667

Epoch: 116| Step: 0
Training loss: 2.6482176780700684
Validation loss: 2.265459459315064

Epoch: 5| Step: 1
Training loss: 2.281193256378174
Validation loss: 2.2632278293691654

Epoch: 5| Step: 2
Training loss: 3.141467571258545
Validation loss: 2.270785803435951

Epoch: 5| Step: 3
Training loss: 3.4920411109924316
Validation loss: 2.266806807569278

Epoch: 5| Step: 4
Training loss: 2.787564516067505
Validation loss: 2.2759949314978813

Epoch: 5| Step: 5
Training loss: 2.8236517906188965
Validation loss: 2.2698697377276678

Epoch: 5| Step: 6
Training loss: 1.8449504375457764
Validation loss: 2.2728385912474764

Epoch: 5| Step: 7
Training loss: 1.7211768627166748
Validation loss: 2.27342584056239

Epoch: 5| Step: 8
Training loss: 1.734816312789917
Validation loss: 2.2787684291921635

Epoch: 5| Step: 9
Training loss: 2.814192056655884
Validation loss: 2.2756199836730957

Epoch: 5| Step: 10
Training loss: 2.5886433124542236
Validation loss: 2.2757656676794893

Epoch: 117| Step: 0
Training loss: 2.415900468826294
Validation loss: 2.2827663806176957

Epoch: 5| Step: 1
Training loss: 2.7598445415496826
Validation loss: 2.2786867951834076

Epoch: 5| Step: 2
Training loss: 2.62312650680542
Validation loss: 2.289003615738243

Epoch: 5| Step: 3
Training loss: 3.210270404815674
Validation loss: 2.2809690634409585

Epoch: 5| Step: 4
Training loss: 2.372812271118164
Validation loss: 2.2751461088016467

Epoch: 5| Step: 5
Training loss: 2.367309331893921
Validation loss: 2.2803443093453684

Epoch: 5| Step: 6
Training loss: 2.331928014755249
Validation loss: 2.2828352733324935

Epoch: 5| Step: 7
Training loss: 2.379636526107788
Validation loss: 2.274336048351821

Epoch: 5| Step: 8
Training loss: 2.386937141418457
Validation loss: 2.279009431921026

Epoch: 5| Step: 9
Training loss: 2.34120512008667
Validation loss: 2.269792195289366

Epoch: 5| Step: 10
Training loss: 2.6180760860443115
Validation loss: 2.270287290696175

Epoch: 118| Step: 0
Training loss: 2.7399070262908936
Validation loss: 2.2825508092039373

Epoch: 5| Step: 1
Training loss: 2.2743983268737793
Validation loss: 2.282410660097676

Epoch: 5| Step: 2
Training loss: 2.7440426349639893
Validation loss: 2.2956772209495626

Epoch: 5| Step: 3
Training loss: 2.3784470558166504
Validation loss: 2.3014226036687053

Epoch: 5| Step: 4
Training loss: 2.1707825660705566
Validation loss: 2.310328842491232

Epoch: 5| Step: 5
Training loss: 2.532407283782959
Validation loss: 2.323259625383603

Epoch: 5| Step: 6
Training loss: 2.517878293991089
Validation loss: 2.321936671451856

Epoch: 5| Step: 7
Training loss: 2.5043210983276367
Validation loss: 2.300630025966193

Epoch: 5| Step: 8
Training loss: 2.817042589187622
Validation loss: 2.2950904036080964

Epoch: 5| Step: 9
Training loss: 3.223801851272583
Validation loss: 2.2779166595910185

Epoch: 5| Step: 10
Training loss: 1.8067550659179688
Validation loss: 2.2670980756000807

Epoch: 119| Step: 0
Training loss: 2.490760326385498
Validation loss: 2.266164691217484

Epoch: 5| Step: 1
Training loss: 2.523494243621826
Validation loss: 2.255141022384808

Epoch: 5| Step: 2
Training loss: 2.7655999660491943
Validation loss: 2.257865551979311

Epoch: 5| Step: 3
Training loss: 2.591397523880005
Validation loss: 2.2516036084903184

Epoch: 5| Step: 4
Training loss: 2.4553544521331787
Validation loss: 2.2554940254457536

Epoch: 5| Step: 5
Training loss: 2.5486855506896973
Validation loss: 2.2530942860470025

Epoch: 5| Step: 6
Training loss: 2.29766845703125
Validation loss: 2.264701602279499

Epoch: 5| Step: 7
Training loss: 2.9734816551208496
Validation loss: 2.2546648261367634

Epoch: 5| Step: 8
Training loss: 2.5419583320617676
Validation loss: 2.2709373966340096

Epoch: 5| Step: 9
Training loss: 2.401559352874756
Validation loss: 2.276680979677426

Epoch: 5| Step: 10
Training loss: 2.0506603717803955
Validation loss: 2.2785474561875865

Epoch: 120| Step: 0
Training loss: 2.152722120285034
Validation loss: 2.290105437719694

Epoch: 5| Step: 1
Training loss: 2.9549293518066406
Validation loss: 2.2800702779523787

Epoch: 5| Step: 2
Training loss: 2.8916940689086914
Validation loss: 2.26812998197412

Epoch: 5| Step: 3
Training loss: 2.699822425842285
Validation loss: 2.2569325995701615

Epoch: 5| Step: 4
Training loss: 2.149747371673584
Validation loss: 2.262597794173866

Epoch: 5| Step: 5
Training loss: 2.383884906768799
Validation loss: 2.266745771131208

Epoch: 5| Step: 6
Training loss: 2.723004102706909
Validation loss: 2.2633259680963334

Epoch: 5| Step: 7
Training loss: 2.5223820209503174
Validation loss: 2.2599537270043486

Epoch: 5| Step: 8
Training loss: 2.2892603874206543
Validation loss: 2.2665624746712307

Epoch: 5| Step: 9
Training loss: 2.296468734741211
Validation loss: 2.2595972784103884

Epoch: 5| Step: 10
Training loss: 2.539212465286255
Validation loss: 2.2650376968486334

Epoch: 121| Step: 0
Training loss: 2.18381667137146
Validation loss: 2.273620608032391

Epoch: 5| Step: 1
Training loss: 3.05515718460083
Validation loss: 2.270922630063949

Epoch: 5| Step: 2
Training loss: 2.741520881652832
Validation loss: 2.2680123006143877

Epoch: 5| Step: 3
Training loss: 2.471238613128662
Validation loss: 2.271215190169632

Epoch: 5| Step: 4
Training loss: 2.262437343597412
Validation loss: 2.2658831445119714

Epoch: 5| Step: 5
Training loss: 2.372378349304199
Validation loss: 2.2796266194312804

Epoch: 5| Step: 6
Training loss: 2.306539297103882
Validation loss: 2.2674065507868284

Epoch: 5| Step: 7
Training loss: 3.0133731365203857
Validation loss: 2.274128662642612

Epoch: 5| Step: 8
Training loss: 2.364881992340088
Validation loss: 2.260864488540157

Epoch: 5| Step: 9
Training loss: 2.4515767097473145
Validation loss: 2.2626975685037594

Epoch: 5| Step: 10
Training loss: 2.338111162185669
Validation loss: 2.2697062261642946

Epoch: 122| Step: 0
Training loss: 2.1944780349731445
Validation loss: 2.270913626558037

Epoch: 5| Step: 1
Training loss: 3.0704703330993652
Validation loss: 2.272803501416278

Epoch: 5| Step: 2
Training loss: 2.4846928119659424
Validation loss: 2.267740889262128

Epoch: 5| Step: 3
Training loss: 2.8637022972106934
Validation loss: 2.264552903431718

Epoch: 5| Step: 4
Training loss: 2.39713716506958
Validation loss: 2.2651690411311325

Epoch: 5| Step: 5
Training loss: 2.4741580486297607
Validation loss: 2.2674851302177674

Epoch: 5| Step: 6
Training loss: 2.4764187335968018
Validation loss: 2.274831002758395

Epoch: 5| Step: 7
Training loss: 2.9632046222686768
Validation loss: 2.262249092901907

Epoch: 5| Step: 8
Training loss: 1.574194312095642
Validation loss: 2.261647614099646

Epoch: 5| Step: 9
Training loss: 2.8791263103485107
Validation loss: 2.271268506203928

Epoch: 5| Step: 10
Training loss: 2.026359796524048
Validation loss: 2.259064318031393

Epoch: 123| Step: 0
Training loss: 3.2516071796417236
Validation loss: 2.253310425307161

Epoch: 5| Step: 1
Training loss: 2.8087401390075684
Validation loss: 2.2680878331584315

Epoch: 5| Step: 2
Training loss: 2.8346898555755615
Validation loss: 2.2601724645142913

Epoch: 5| Step: 3
Training loss: 2.6474902629852295
Validation loss: 2.2679810062531502

Epoch: 5| Step: 4
Training loss: 2.5456159114837646
Validation loss: 2.259222190867188

Epoch: 5| Step: 5
Training loss: 2.2440998554229736
Validation loss: 2.267478131478833

Epoch: 5| Step: 6
Training loss: 2.338895320892334
Validation loss: 2.257679852106238

Epoch: 5| Step: 7
Training loss: 2.183927536010742
Validation loss: 2.2653658620772825

Epoch: 5| Step: 8
Training loss: 2.162567615509033
Validation loss: 2.268284225976595

Epoch: 5| Step: 9
Training loss: 2.005276679992676
Validation loss: 2.260901971529889

Epoch: 5| Step: 10
Training loss: 2.4442102909088135
Validation loss: 2.2659393869420534

Epoch: 124| Step: 0
Training loss: 2.340446710586548
Validation loss: 2.272720483041579

Epoch: 5| Step: 1
Training loss: 2.06607985496521
Validation loss: 2.272044925279515

Epoch: 5| Step: 2
Training loss: 2.541337490081787
Validation loss: 2.2743329360920894

Epoch: 5| Step: 3
Training loss: 3.2214579582214355
Validation loss: 2.2746387117652485

Epoch: 5| Step: 4
Training loss: 2.485229015350342
Validation loss: 2.2550671305707706

Epoch: 5| Step: 5
Training loss: 2.2883999347686768
Validation loss: 2.2602641428670576

Epoch: 5| Step: 6
Training loss: 2.5671303272247314
Validation loss: 2.2562999930433048

Epoch: 5| Step: 7
Training loss: 2.3722143173217773
Validation loss: 2.2548574324577086

Epoch: 5| Step: 8
Training loss: 3.042553424835205
Validation loss: 2.262425207322644

Epoch: 5| Step: 9
Training loss: 2.203622341156006
Validation loss: 2.2530968676331224

Epoch: 5| Step: 10
Training loss: 2.4112818241119385
Validation loss: 2.25774795521972

Epoch: 125| Step: 0
Training loss: 2.541541337966919
Validation loss: 2.2572749814679547

Epoch: 5| Step: 1
Training loss: 3.2727839946746826
Validation loss: 2.267229898001558

Epoch: 5| Step: 2
Training loss: 2.512421131134033
Validation loss: 2.264321675864599

Epoch: 5| Step: 3
Training loss: 2.747666835784912
Validation loss: 2.2602678832187446

Epoch: 5| Step: 4
Training loss: 2.863193988800049
Validation loss: 2.2573256377250916

Epoch: 5| Step: 5
Training loss: 2.6352248191833496
Validation loss: 2.26708060438915

Epoch: 5| Step: 6
Training loss: 2.540527582168579
Validation loss: 2.2672478755315146

Epoch: 5| Step: 7
Training loss: 2.165379762649536
Validation loss: 2.264850872819142

Epoch: 5| Step: 8
Training loss: 2.767230749130249
Validation loss: 2.261728181633898

Epoch: 5| Step: 9
Training loss: 1.7305946350097656
Validation loss: 2.268167082981397

Epoch: 5| Step: 10
Training loss: 1.6503902673721313
Validation loss: 2.271299285273398

Epoch: 126| Step: 0
Training loss: 2.481180429458618
Validation loss: 2.264053144762593

Epoch: 5| Step: 1
Training loss: 2.763073682785034
Validation loss: 2.2662534816290743

Epoch: 5| Step: 2
Training loss: 2.778873920440674
Validation loss: 2.2723991370970205

Epoch: 5| Step: 3
Training loss: 2.3960695266723633
Validation loss: 2.2648926447796565

Epoch: 5| Step: 4
Training loss: 2.6294846534729004
Validation loss: 2.244734439798581

Epoch: 5| Step: 5
Training loss: 2.611135244369507
Validation loss: 2.2313650244025776

Epoch: 5| Step: 6
Training loss: 2.319645404815674
Validation loss: 2.236661567482897

Epoch: 5| Step: 7
Training loss: 2.364622116088867
Validation loss: 2.2353551272423036

Epoch: 5| Step: 8
Training loss: 2.596879720687866
Validation loss: 2.2334798741084274

Epoch: 5| Step: 9
Training loss: 2.425443172454834
Validation loss: 2.238697680093909

Epoch: 5| Step: 10
Training loss: 2.1757922172546387
Validation loss: 2.2334540531199467

Epoch: 127| Step: 0
Training loss: 2.795118808746338
Validation loss: 2.237723499216059

Epoch: 5| Step: 1
Training loss: 2.3966281414031982
Validation loss: 2.2307911560099614

Epoch: 5| Step: 2
Training loss: 2.766719102859497
Validation loss: 2.24258090091008

Epoch: 5| Step: 3
Training loss: 2.021477222442627
Validation loss: 2.231742392304123

Epoch: 5| Step: 4
Training loss: 1.9618581533432007
Validation loss: 2.2394001894099738

Epoch: 5| Step: 5
Training loss: 2.424736976623535
Validation loss: 2.2330834763024443

Epoch: 5| Step: 6
Training loss: 2.338588237762451
Validation loss: 2.2446195899799304

Epoch: 5| Step: 7
Training loss: 3.038719654083252
Validation loss: 2.238293981039396

Epoch: 5| Step: 8
Training loss: 2.7710041999816895
Validation loss: 2.2501797240267516

Epoch: 5| Step: 9
Training loss: 2.534688949584961
Validation loss: 2.2483728649795696

Epoch: 5| Step: 10
Training loss: 2.4612064361572266
Validation loss: 2.247821136187482

Epoch: 128| Step: 0
Training loss: 2.4425361156463623
Validation loss: 2.253903330013316

Epoch: 5| Step: 1
Training loss: 2.380944013595581
Validation loss: 2.246210523830947

Epoch: 5| Step: 2
Training loss: 2.5920376777648926
Validation loss: 2.2462576435458277

Epoch: 5| Step: 3
Training loss: 2.6433370113372803
Validation loss: 2.2498315893193728

Epoch: 5| Step: 4
Training loss: 1.6013514995574951
Validation loss: 2.257182505822951

Epoch: 5| Step: 5
Training loss: 2.3381946086883545
Validation loss: 2.266038810053179

Epoch: 5| Step: 6
Training loss: 2.318744421005249
Validation loss: 2.288235423385456

Epoch: 5| Step: 7
Training loss: 2.762824296951294
Validation loss: 2.2989865144093833

Epoch: 5| Step: 8
Training loss: 2.8607616424560547
Validation loss: 2.2868223754308556

Epoch: 5| Step: 9
Training loss: 2.7539095878601074
Validation loss: 2.303792440763084

Epoch: 5| Step: 10
Training loss: 2.9431958198547363
Validation loss: 2.2958640411335933

Epoch: 129| Step: 0
Training loss: 1.985140085220337
Validation loss: 2.2824292259831584

Epoch: 5| Step: 1
Training loss: 3.0343616008758545
Validation loss: 2.2772547814153854

Epoch: 5| Step: 2
Training loss: 2.804396867752075
Validation loss: 2.2792253340444257

Epoch: 5| Step: 3
Training loss: 2.9868927001953125
Validation loss: 2.288349566921111

Epoch: 5| Step: 4
Training loss: 2.506237506866455
Validation loss: 2.2828600560465167

Epoch: 5| Step: 5
Training loss: 2.4635024070739746
Validation loss: 2.2689215957477527

Epoch: 5| Step: 6
Training loss: 2.4608845710754395
Validation loss: 2.261343789357011

Epoch: 5| Step: 7
Training loss: 2.098369598388672
Validation loss: 2.2530826035366265

Epoch: 5| Step: 8
Training loss: 2.748363971710205
Validation loss: 2.247676049509356

Epoch: 5| Step: 9
Training loss: 2.0060982704162598
Validation loss: 2.2468375057302494

Epoch: 5| Step: 10
Training loss: 2.467722177505493
Validation loss: 2.2431463477432088

Epoch: 130| Step: 0
Training loss: 2.443720579147339
Validation loss: 2.2478599163793747

Epoch: 5| Step: 1
Training loss: 2.947807550430298
Validation loss: 2.2468607964054232

Epoch: 5| Step: 2
Training loss: 2.7078983783721924
Validation loss: 2.2498143642179427

Epoch: 5| Step: 3
Training loss: 2.742041826248169
Validation loss: 2.2571878458863948

Epoch: 5| Step: 4
Training loss: 2.0524706840515137
Validation loss: 2.2559133178444317

Epoch: 5| Step: 5
Training loss: 1.627967119216919
Validation loss: 2.2527463564308743

Epoch: 5| Step: 6
Training loss: 2.1978647708892822
Validation loss: 2.2487816426061813

Epoch: 5| Step: 7
Training loss: 2.324739694595337
Validation loss: 2.243638300126599

Epoch: 5| Step: 8
Training loss: 3.345365524291992
Validation loss: 2.240091282834289

Epoch: 5| Step: 9
Training loss: 2.2079567909240723
Validation loss: 2.2420046662771576

Epoch: 5| Step: 10
Training loss: 2.9127447605133057
Validation loss: 2.2460579461948846

Epoch: 131| Step: 0
Training loss: 2.311776638031006
Validation loss: 2.2460864718242357

Epoch: 5| Step: 1
Training loss: 2.3747799396514893
Validation loss: 2.2476024396957888

Epoch: 5| Step: 2
Training loss: 2.7760424613952637
Validation loss: 2.2561884541665354

Epoch: 5| Step: 3
Training loss: 2.7171292304992676
Validation loss: 2.2539124527285175

Epoch: 5| Step: 4
Training loss: 2.2810466289520264
Validation loss: 2.2553951022445515

Epoch: 5| Step: 5
Training loss: 2.8119616508483887
Validation loss: 2.2402100511776504

Epoch: 5| Step: 6
Training loss: 2.475010395050049
Validation loss: 2.2464010023301646

Epoch: 5| Step: 7
Training loss: 2.4511077404022217
Validation loss: 2.237535471557289

Epoch: 5| Step: 8
Training loss: 2.234400510787964
Validation loss: 2.2531073324141966

Epoch: 5| Step: 9
Training loss: 2.4555649757385254
Validation loss: 2.2439533151606077

Epoch: 5| Step: 10
Training loss: 2.446951389312744
Validation loss: 2.2506789007494525

Epoch: 132| Step: 0
Training loss: 2.4397315979003906
Validation loss: 2.250235131991807

Epoch: 5| Step: 1
Training loss: 2.701083183288574
Validation loss: 2.240835443619759

Epoch: 5| Step: 2
Training loss: 2.3914356231689453
Validation loss: 2.2462246443635676

Epoch: 5| Step: 3
Training loss: 2.386533737182617
Validation loss: 2.253389717430197

Epoch: 5| Step: 4
Training loss: 3.146115303039551
Validation loss: 2.2721267669431624

Epoch: 5| Step: 5
Training loss: 1.7240867614746094
Validation loss: 2.2683788730252172

Epoch: 5| Step: 6
Training loss: 2.666482448577881
Validation loss: 2.2752411493691067

Epoch: 5| Step: 7
Training loss: 2.6891560554504395
Validation loss: 2.283207262715986

Epoch: 5| Step: 8
Training loss: 2.874330520629883
Validation loss: 2.2839113691801667

Epoch: 5| Step: 9
Training loss: 2.2267673015594482
Validation loss: 2.2880751855911745

Epoch: 5| Step: 10
Training loss: 2.0576581954956055
Validation loss: 2.2866819853423745

Epoch: 133| Step: 0
Training loss: 2.5725793838500977
Validation loss: 2.2869164456603346

Epoch: 5| Step: 1
Training loss: 2.771693468093872
Validation loss: 2.2759835784153273

Epoch: 5| Step: 2
Training loss: 2.3805527687072754
Validation loss: 2.259676043705274

Epoch: 5| Step: 3
Training loss: 1.811859369277954
Validation loss: 2.261552110795052

Epoch: 5| Step: 4
Training loss: 2.764108419418335
Validation loss: 2.2503818055634857

Epoch: 5| Step: 5
Training loss: 2.333683967590332
Validation loss: 2.2407431961387716

Epoch: 5| Step: 6
Training loss: 2.9095818996429443
Validation loss: 2.242196072814285

Epoch: 5| Step: 7
Training loss: 3.1338863372802734
Validation loss: 2.2399604756345033

Epoch: 5| Step: 8
Training loss: 2.8743929862976074
Validation loss: 2.2369945510741203

Epoch: 5| Step: 9
Training loss: 1.7047628164291382
Validation loss: 2.237549598499011

Epoch: 5| Step: 10
Training loss: 2.2179627418518066
Validation loss: 2.2280918013664985

Epoch: 134| Step: 0
Training loss: 2.464970111846924
Validation loss: 2.242095026918637

Epoch: 5| Step: 1
Training loss: 2.8149325847625732
Validation loss: 2.258860134309338

Epoch: 5| Step: 2
Training loss: 2.1448817253112793
Validation loss: 2.2769170345798617

Epoch: 5| Step: 3
Training loss: 2.3292312622070312
Validation loss: 2.2783934057399793

Epoch: 5| Step: 4
Training loss: 2.893258571624756
Validation loss: 2.2948717019891225

Epoch: 5| Step: 5
Training loss: 2.096062421798706
Validation loss: 2.3038272357756093

Epoch: 5| Step: 6
Training loss: 3.0046610832214355
Validation loss: 2.308220906924176

Epoch: 5| Step: 7
Training loss: 2.196369171142578
Validation loss: 2.294674554178792

Epoch: 5| Step: 8
Training loss: 2.681321144104004
Validation loss: 2.281086269245353

Epoch: 5| Step: 9
Training loss: 2.2698798179626465
Validation loss: 2.267402902726204

Epoch: 5| Step: 10
Training loss: 2.7665364742279053
Validation loss: 2.241790288238115

Epoch: 135| Step: 0
Training loss: 2.775259494781494
Validation loss: 2.244847115649972

Epoch: 5| Step: 1
Training loss: 3.2006568908691406
Validation loss: 2.234936891063567

Epoch: 5| Step: 2
Training loss: 3.2287983894348145
Validation loss: 2.2288207289993123

Epoch: 5| Step: 3
Training loss: 2.2965633869171143
Validation loss: 2.2362731810539

Epoch: 5| Step: 4
Training loss: 2.384690761566162
Validation loss: 2.2389391609417495

Epoch: 5| Step: 5
Training loss: 1.3258771896362305
Validation loss: 2.2322894885975826

Epoch: 5| Step: 6
Training loss: 1.7261593341827393
Validation loss: 2.2281973361968994

Epoch: 5| Step: 7
Training loss: 2.829099655151367
Validation loss: 2.223992863009053

Epoch: 5| Step: 8
Training loss: 2.0949411392211914
Validation loss: 2.2273553135574504

Epoch: 5| Step: 9
Training loss: 3.024599552154541
Validation loss: 2.2472958180212204

Epoch: 5| Step: 10
Training loss: 2.7048110961914062
Validation loss: 2.271295614140008

Epoch: 136| Step: 0
Training loss: 2.8071110248565674
Validation loss: 2.271658184707806

Epoch: 5| Step: 1
Training loss: 2.589451551437378
Validation loss: 2.2622216029833724

Epoch: 5| Step: 2
Training loss: 2.541628122329712
Validation loss: 2.2691943363476823

Epoch: 5| Step: 3
Training loss: 2.209932804107666
Validation loss: 2.2587067004173034

Epoch: 5| Step: 4
Training loss: 2.1122777462005615
Validation loss: 2.2599872132783294

Epoch: 5| Step: 5
Training loss: 2.368300676345825
Validation loss: 2.250076581073064

Epoch: 5| Step: 6
Training loss: 3.0599281787872314
Validation loss: 2.251768542874244

Epoch: 5| Step: 7
Training loss: 2.637577772140503
Validation loss: 2.2497013973933395

Epoch: 5| Step: 8
Training loss: 2.2339916229248047
Validation loss: 2.233408577980534

Epoch: 5| Step: 9
Training loss: 2.342730760574341
Validation loss: 2.23007075120044

Epoch: 5| Step: 10
Training loss: 2.6090989112854004
Validation loss: 2.2351799600867817

Epoch: 137| Step: 0
Training loss: 2.9168732166290283
Validation loss: 2.2300535235353696

Epoch: 5| Step: 1
Training loss: 2.6011948585510254
Validation loss: 2.2173561947320097

Epoch: 5| Step: 2
Training loss: 2.3914332389831543
Validation loss: 2.213510392814554

Epoch: 5| Step: 3
Training loss: 2.929771900177002
Validation loss: 2.215989238472395

Epoch: 5| Step: 4
Training loss: 1.8727562427520752
Validation loss: 2.2094184275596374

Epoch: 5| Step: 5
Training loss: 2.2851200103759766
Validation loss: 2.2209448301663963

Epoch: 5| Step: 6
Training loss: 2.5859029293060303
Validation loss: 2.222989927056015

Epoch: 5| Step: 7
Training loss: 2.5601508617401123
Validation loss: 2.2377684885455715

Epoch: 5| Step: 8
Training loss: 2.4784624576568604
Validation loss: 2.237985372543335

Epoch: 5| Step: 9
Training loss: 1.9711412191390991
Validation loss: 2.2444903748009795

Epoch: 5| Step: 10
Training loss: 2.978517532348633
Validation loss: 2.237111060850082

Epoch: 138| Step: 0
Training loss: 2.788529634475708
Validation loss: 2.228181269861037

Epoch: 5| Step: 1
Training loss: 2.5801055431365967
Validation loss: 2.2180228848611154

Epoch: 5| Step: 2
Training loss: 1.9165703058242798
Validation loss: 2.2216814012937647

Epoch: 5| Step: 3
Training loss: 2.220442533493042
Validation loss: 2.224943512229509

Epoch: 5| Step: 4
Training loss: 2.4999232292175293
Validation loss: 2.2420256599303214

Epoch: 5| Step: 5
Training loss: 2.405662775039673
Validation loss: 2.2355120182037354

Epoch: 5| Step: 6
Training loss: 2.664501905441284
Validation loss: 2.2367196006159626

Epoch: 5| Step: 7
Training loss: 2.3162662982940674
Validation loss: 2.2346153400277577

Epoch: 5| Step: 8
Training loss: 2.670117139816284
Validation loss: 2.2293157526241836

Epoch: 5| Step: 9
Training loss: 2.8532187938690186
Validation loss: 2.232227652303634

Epoch: 5| Step: 10
Training loss: 2.4126737117767334
Validation loss: 2.2450673169987176

Epoch: 139| Step: 0
Training loss: 2.37815523147583
Validation loss: 2.2309540266631753

Epoch: 5| Step: 1
Training loss: 2.7864294052124023
Validation loss: 2.226975405088035

Epoch: 5| Step: 2
Training loss: 2.763453960418701
Validation loss: 2.221056922789543

Epoch: 5| Step: 3
Training loss: 2.66795015335083
Validation loss: 2.2156260423762824

Epoch: 5| Step: 4
Training loss: 2.701563596725464
Validation loss: 2.21837019407621

Epoch: 5| Step: 5
Training loss: 2.5754284858703613
Validation loss: 2.225936289756529

Epoch: 5| Step: 6
Training loss: 2.275510311126709
Validation loss: 2.2353840950996644

Epoch: 5| Step: 7
Training loss: 1.9049558639526367
Validation loss: 2.2280916757481073

Epoch: 5| Step: 8
Training loss: 2.865222930908203
Validation loss: 2.232514060953612

Epoch: 5| Step: 9
Training loss: 1.6848846673965454
Validation loss: 2.238843051336145

Epoch: 5| Step: 10
Training loss: 2.7783868312835693
Validation loss: 2.2332330160243536

Epoch: 140| Step: 0
Training loss: 2.596179485321045
Validation loss: 2.239075811960364

Epoch: 5| Step: 1
Training loss: 2.011950731277466
Validation loss: 2.226045116301506

Epoch: 5| Step: 2
Training loss: 2.4093425273895264
Validation loss: 2.231632435193626

Epoch: 5| Step: 3
Training loss: 2.5006802082061768
Validation loss: 2.230492248330065

Epoch: 5| Step: 4
Training loss: 2.807898998260498
Validation loss: 2.2298692605828725

Epoch: 5| Step: 5
Training loss: 2.6432785987854004
Validation loss: 2.2505512904095393

Epoch: 5| Step: 6
Training loss: 2.2657980918884277
Validation loss: 2.2587660461343746

Epoch: 5| Step: 7
Training loss: 1.771532654762268
Validation loss: 2.2810702093185915

Epoch: 5| Step: 8
Training loss: 2.618159770965576
Validation loss: 2.2860335329527497

Epoch: 5| Step: 9
Training loss: 3.099925994873047
Validation loss: 2.3065647438008297

Epoch: 5| Step: 10
Training loss: 2.8299436569213867
Validation loss: 2.3072449776434127

Epoch: 141| Step: 0
Training loss: 2.6475718021392822
Validation loss: 2.2699599855689594

Epoch: 5| Step: 1
Training loss: 2.4973182678222656
Validation loss: 2.255092279885405

Epoch: 5| Step: 2
Training loss: 2.3603758811950684
Validation loss: 2.2326713787612094

Epoch: 5| Step: 3
Training loss: 3.0438742637634277
Validation loss: 2.2318282614472094

Epoch: 5| Step: 4
Training loss: 2.1044249534606934
Validation loss: 2.2329594063502487

Epoch: 5| Step: 5
Training loss: 2.6741201877593994
Validation loss: 2.2225533403376097

Epoch: 5| Step: 6
Training loss: 1.9954121112823486
Validation loss: 2.223238529697541

Epoch: 5| Step: 7
Training loss: 2.8540220260620117
Validation loss: 2.2106152901085476

Epoch: 5| Step: 8
Training loss: 1.9143654108047485
Validation loss: 2.2036326713459466

Epoch: 5| Step: 9
Training loss: 2.5632522106170654
Validation loss: 2.201991693947905

Epoch: 5| Step: 10
Training loss: 2.7735443115234375
Validation loss: 2.216075769034765

Epoch: 142| Step: 0
Training loss: 1.844098687171936
Validation loss: 2.215581242756177

Epoch: 5| Step: 1
Training loss: 1.9704627990722656
Validation loss: 2.2222068925057687

Epoch: 5| Step: 2
Training loss: 3.152367353439331
Validation loss: 2.241614877536733

Epoch: 5| Step: 3
Training loss: 2.247422695159912
Validation loss: 2.2428415770171792

Epoch: 5| Step: 4
Training loss: 3.169887065887451
Validation loss: 2.244904723218692

Epoch: 5| Step: 5
Training loss: 2.824492931365967
Validation loss: 2.2563933505806872

Epoch: 5| Step: 6
Training loss: 2.5562288761138916
Validation loss: 2.2583821614583335

Epoch: 5| Step: 7
Training loss: 2.5995564460754395
Validation loss: 2.2471209572207544

Epoch: 5| Step: 8
Training loss: 2.3895068168640137
Validation loss: 2.23099846224631

Epoch: 5| Step: 9
Training loss: 2.0343375205993652
Validation loss: 2.228534980486798

Epoch: 5| Step: 10
Training loss: 2.808912754058838
Validation loss: 2.236200846651549

Epoch: 143| Step: 0
Training loss: 3.121537208557129
Validation loss: 2.2172256349235453

Epoch: 5| Step: 1
Training loss: 2.8425943851470947
Validation loss: 2.2121540372089674

Epoch: 5| Step: 2
Training loss: 2.395473003387451
Validation loss: 2.211285288615893

Epoch: 5| Step: 3
Training loss: 2.20003080368042
Validation loss: 2.2237686854536816

Epoch: 5| Step: 4
Training loss: 2.582331895828247
Validation loss: 2.219139611849221

Epoch: 5| Step: 5
Training loss: 2.6526541709899902
Validation loss: 2.222090764712262

Epoch: 5| Step: 6
Training loss: 2.2288105487823486
Validation loss: 2.2226728803368023

Epoch: 5| Step: 7
Training loss: 2.2164652347564697
Validation loss: 2.2291023885050127

Epoch: 5| Step: 8
Training loss: 1.7460978031158447
Validation loss: 2.227548478752054

Epoch: 5| Step: 9
Training loss: 2.567803144454956
Validation loss: 2.224488171198035

Epoch: 5| Step: 10
Training loss: 2.7831225395202637
Validation loss: 2.250461059231912

Epoch: 144| Step: 0
Training loss: 2.5325961112976074
Validation loss: 2.24740033764993

Epoch: 5| Step: 1
Training loss: 2.5024704933166504
Validation loss: 2.2552693325986146

Epoch: 5| Step: 2
Training loss: 2.576756000518799
Validation loss: 2.2559135447266283

Epoch: 5| Step: 3
Training loss: 2.311659574508667
Validation loss: 2.2402490774790444

Epoch: 5| Step: 4
Training loss: 2.4966578483581543
Validation loss: 2.2367992785669144

Epoch: 5| Step: 5
Training loss: 2.114999771118164
Validation loss: 2.2198418494193786

Epoch: 5| Step: 6
Training loss: 2.2424557209014893
Validation loss: 2.218074010264489

Epoch: 5| Step: 7
Training loss: 2.6342623233795166
Validation loss: 2.2239265288076093

Epoch: 5| Step: 8
Training loss: 3.0037827491760254
Validation loss: 2.2177214096951228

Epoch: 5| Step: 9
Training loss: 2.1933600902557373
Validation loss: 2.2156558549532326

Epoch: 5| Step: 10
Training loss: 2.6377673149108887
Validation loss: 2.2192337384787937

Epoch: 145| Step: 0
Training loss: 2.172274351119995
Validation loss: 2.222303807094533

Epoch: 5| Step: 1
Training loss: 2.4188976287841797
Validation loss: 2.222792233190229

Epoch: 5| Step: 2
Training loss: 2.291342258453369
Validation loss: 2.233111935277139

Epoch: 5| Step: 3
Training loss: 3.1031908988952637
Validation loss: 2.2339689359870007

Epoch: 5| Step: 4
Training loss: 3.0310657024383545
Validation loss: 2.2362944182529243

Epoch: 5| Step: 5
Training loss: 1.829602599143982
Validation loss: 2.2330945191844815

Epoch: 5| Step: 6
Training loss: 2.301051378250122
Validation loss: 2.2282755297999226

Epoch: 5| Step: 7
Training loss: 2.367079496383667
Validation loss: 2.2285371134358067

Epoch: 5| Step: 8
Training loss: 2.261838674545288
Validation loss: 2.2309636941520115

Epoch: 5| Step: 9
Training loss: 3.0341222286224365
Validation loss: 2.237914680152811

Epoch: 5| Step: 10
Training loss: 2.2455716133117676
Validation loss: 2.2365960126282065

Epoch: 146| Step: 0
Training loss: 2.453047513961792
Validation loss: 2.2331977377655687

Epoch: 5| Step: 1
Training loss: 3.035649061203003
Validation loss: 2.246677085917483

Epoch: 5| Step: 2
Training loss: 2.276761531829834
Validation loss: 2.249461732884889

Epoch: 5| Step: 3
Training loss: 2.4100143909454346
Validation loss: 2.246805421767696

Epoch: 5| Step: 4
Training loss: 2.9380669593811035
Validation loss: 2.2594875443366265

Epoch: 5| Step: 5
Training loss: 2.2171614170074463
Validation loss: 2.253820591075446

Epoch: 5| Step: 6
Training loss: 2.069606304168701
Validation loss: 2.2338186387092835

Epoch: 5| Step: 7
Training loss: 2.6870882511138916
Validation loss: 2.22476472649523

Epoch: 5| Step: 8
Training loss: 2.582822561264038
Validation loss: 2.223969997898225

Epoch: 5| Step: 9
Training loss: 2.1226282119750977
Validation loss: 2.2229083955928846

Epoch: 5| Step: 10
Training loss: 2.359117269515991
Validation loss: 2.219088190345354

Epoch: 147| Step: 0
Training loss: 2.616151809692383
Validation loss: 2.227318207422892

Epoch: 5| Step: 1
Training loss: 2.4265291690826416
Validation loss: 2.2158642558641333

Epoch: 5| Step: 2
Training loss: 2.181687831878662
Validation loss: 2.2272172871456353

Epoch: 5| Step: 3
Training loss: 2.385979652404785
Validation loss: 2.2232688344934934

Epoch: 5| Step: 4
Training loss: 3.1019339561462402
Validation loss: 2.2213433942487164

Epoch: 5| Step: 5
Training loss: 2.036984920501709
Validation loss: 2.2233348431125766

Epoch: 5| Step: 6
Training loss: 2.7519707679748535
Validation loss: 2.2103343638040687

Epoch: 5| Step: 7
Training loss: 3.281275510787964
Validation loss: 2.2134087316451536

Epoch: 5| Step: 8
Training loss: 2.4002952575683594
Validation loss: 2.219153481145059

Epoch: 5| Step: 9
Training loss: 2.101628303527832
Validation loss: 2.215514124080699

Epoch: 5| Step: 10
Training loss: 1.6952835321426392
Validation loss: 2.2223193696750108

Epoch: 148| Step: 0
Training loss: 2.9051690101623535
Validation loss: 2.2339939225104546

Epoch: 5| Step: 1
Training loss: 2.717400074005127
Validation loss: 2.239787470909857

Epoch: 5| Step: 2
Training loss: 2.100362777709961
Validation loss: 2.2406866140263055

Epoch: 5| Step: 3
Training loss: 2.5356483459472656
Validation loss: 2.2274803397476033

Epoch: 5| Step: 4
Training loss: 2.30985689163208
Validation loss: 2.236947618505006

Epoch: 5| Step: 5
Training loss: 1.9882423877716064
Validation loss: 2.235619460382769

Epoch: 5| Step: 6
Training loss: 2.3851864337921143
Validation loss: 2.216005450935774

Epoch: 5| Step: 7
Training loss: 2.8398921489715576
Validation loss: 2.215962791955599

Epoch: 5| Step: 8
Training loss: 3.019834041595459
Validation loss: 2.219498365156112

Epoch: 5| Step: 9
Training loss: 2.57586407661438
Validation loss: 2.2243522444079

Epoch: 5| Step: 10
Training loss: 1.8132261037826538
Validation loss: 2.224655274421938

Epoch: 149| Step: 0
Training loss: 2.7151620388031006
Validation loss: 2.2266328027171474

Epoch: 5| Step: 1
Training loss: 2.43811297416687
Validation loss: 2.233700700985488

Epoch: 5| Step: 2
Training loss: 2.46689772605896
Validation loss: 2.2319295854978662

Epoch: 5| Step: 3
Training loss: 2.538682460784912
Validation loss: 2.243097033551944

Epoch: 5| Step: 4
Training loss: 2.9738361835479736
Validation loss: 2.2355048733372844

Epoch: 5| Step: 5
Training loss: 1.8090699911117554
Validation loss: 2.238077607206119

Epoch: 5| Step: 6
Training loss: 1.6595735549926758
Validation loss: 2.2469865096512662

Epoch: 5| Step: 7
Training loss: 3.087939739227295
Validation loss: 2.231463301566339

Epoch: 5| Step: 8
Training loss: 2.7547569274902344
Validation loss: 2.2372781743285475

Epoch: 5| Step: 9
Training loss: 2.1165738105773926
Validation loss: 2.2344579658200665

Epoch: 5| Step: 10
Training loss: 2.43074631690979
Validation loss: 2.2378941838459303

Epoch: 150| Step: 0
Training loss: 2.685166597366333
Validation loss: 2.2438599089140534

Epoch: 5| Step: 1
Training loss: 2.4584813117980957
Validation loss: 2.2349876050026185

Epoch: 5| Step: 2
Training loss: 2.36449933052063
Validation loss: 2.24059699684061

Epoch: 5| Step: 3
Training loss: 2.4478869438171387
Validation loss: 2.2312206824620566

Epoch: 5| Step: 4
Training loss: 2.3596067428588867
Validation loss: 2.22951764188787

Epoch: 5| Step: 5
Training loss: 2.6160531044006348
Validation loss: 2.232917913826563

Epoch: 5| Step: 6
Training loss: 1.660622000694275
Validation loss: 2.234586561879804

Epoch: 5| Step: 7
Training loss: 2.457481622695923
Validation loss: 2.223075189898091

Epoch: 5| Step: 8
Training loss: 2.7728660106658936
Validation loss: 2.225699934908139

Epoch: 5| Step: 9
Training loss: 3.0144550800323486
Validation loss: 2.229190568770132

Epoch: 5| Step: 10
Training loss: 2.043731927871704
Validation loss: 2.222235684753746

Epoch: 151| Step: 0
Training loss: 1.7819175720214844
Validation loss: 2.221847329088437

Epoch: 5| Step: 1
Training loss: 1.8767845630645752
Validation loss: 2.2241667650079213

Epoch: 5| Step: 2
Training loss: 2.698843002319336
Validation loss: 2.232730575787124

Epoch: 5| Step: 3
Training loss: 1.7677196264266968
Validation loss: 2.2300391145931777

Epoch: 5| Step: 4
Training loss: 2.5196433067321777
Validation loss: 2.2335774065345846

Epoch: 5| Step: 5
Training loss: 2.6748669147491455
Validation loss: 2.233076551909088

Epoch: 5| Step: 6
Training loss: 2.725257635116577
Validation loss: 2.2322006943405315

Epoch: 5| Step: 7
Training loss: 2.442099094390869
Validation loss: 2.239807750589104

Epoch: 5| Step: 8
Training loss: 2.5885047912597656
Validation loss: 2.226153859528162

Epoch: 5| Step: 9
Training loss: 3.231626033782959
Validation loss: 2.2150451265355593

Epoch: 5| Step: 10
Training loss: 2.6677863597869873
Validation loss: 2.2139782392850487

Epoch: 152| Step: 0
Training loss: 2.3028292655944824
Validation loss: 2.216630556250131

Epoch: 5| Step: 1
Training loss: 1.9538581371307373
Validation loss: 2.224476686087988

Epoch: 5| Step: 2
Training loss: 2.439923048019409
Validation loss: 2.221781417887698

Epoch: 5| Step: 3
Training loss: 2.3336617946624756
Validation loss: 2.223283172935568

Epoch: 5| Step: 4
Training loss: 2.298485040664673
Validation loss: 2.2301961324548207

Epoch: 5| Step: 5
Training loss: 2.938610553741455
Validation loss: 2.2371425462025467

Epoch: 5| Step: 6
Training loss: 2.8411312103271484
Validation loss: 2.2424156511983564

Epoch: 5| Step: 7
Training loss: 2.7966160774230957
Validation loss: 2.2357353523213375

Epoch: 5| Step: 8
Training loss: 2.178663492202759
Validation loss: 2.2298790947083504

Epoch: 5| Step: 9
Training loss: 2.0561678409576416
Validation loss: 2.22353881917974

Epoch: 5| Step: 10
Training loss: 2.858926773071289
Validation loss: 2.2126186791286675

Epoch: 153| Step: 0
Training loss: 2.772474765777588
Validation loss: 2.218791233595981

Epoch: 5| Step: 1
Training loss: 2.092780590057373
Validation loss: 2.2221117558017855

Epoch: 5| Step: 2
Training loss: 2.586433172225952
Validation loss: 2.230827428961313

Epoch: 5| Step: 3
Training loss: 2.0277352333068848
Validation loss: 2.221761767582227

Epoch: 5| Step: 4
Training loss: 2.2644574642181396
Validation loss: 2.2415115359008952

Epoch: 5| Step: 5
Training loss: 2.588247299194336
Validation loss: 2.23613477265963

Epoch: 5| Step: 6
Training loss: 2.271498680114746
Validation loss: 2.242029118281539

Epoch: 5| Step: 7
Training loss: 2.2423977851867676
Validation loss: 2.255556824386761

Epoch: 5| Step: 8
Training loss: 2.2906346321105957
Validation loss: 2.2362147454292542

Epoch: 5| Step: 9
Training loss: 3.080536127090454
Validation loss: 2.240258268130723

Epoch: 5| Step: 10
Training loss: 2.8253207206726074
Validation loss: 2.2247049167592037

Epoch: 154| Step: 0
Training loss: 2.8009746074676514
Validation loss: 2.21740129686171

Epoch: 5| Step: 1
Training loss: 2.177605152130127
Validation loss: 2.2049466204899613

Epoch: 5| Step: 2
Training loss: 2.7218785285949707
Validation loss: 2.2131686659269434

Epoch: 5| Step: 3
Training loss: 2.205784559249878
Validation loss: 2.2116789061536073

Epoch: 5| Step: 4
Training loss: 2.793421983718872
Validation loss: 2.206145727506248

Epoch: 5| Step: 5
Training loss: 2.483355760574341
Validation loss: 2.217607769914853

Epoch: 5| Step: 6
Training loss: 2.1469006538391113
Validation loss: 2.2256982480326006

Epoch: 5| Step: 7
Training loss: 2.122347354888916
Validation loss: 2.229589921171947

Epoch: 5| Step: 8
Training loss: 2.371690273284912
Validation loss: 2.2287012736002603

Epoch: 5| Step: 9
Training loss: 2.2634196281433105
Validation loss: 2.2225454853427027

Epoch: 5| Step: 10
Training loss: 2.953533172607422
Validation loss: 2.2174480576669016

Epoch: 155| Step: 0
Training loss: 2.6825826168060303
Validation loss: 2.2140563944334626

Epoch: 5| Step: 1
Training loss: 2.672314405441284
Validation loss: 2.2266547936265186

Epoch: 5| Step: 2
Training loss: 2.882462501525879
Validation loss: 2.2264031646072224

Epoch: 5| Step: 3
Training loss: 2.035975694656372
Validation loss: 2.21866472177608

Epoch: 5| Step: 4
Training loss: 2.6533102989196777
Validation loss: 2.2219676266434374

Epoch: 5| Step: 5
Training loss: 2.173549175262451
Validation loss: 2.2335419193390877

Epoch: 5| Step: 6
Training loss: 1.981260061264038
Validation loss: 2.2291478931262927

Epoch: 5| Step: 7
Training loss: 2.8509559631347656
Validation loss: 2.247974216297109

Epoch: 5| Step: 8
Training loss: 2.117276191711426
Validation loss: 2.2450303441734722

Epoch: 5| Step: 9
Training loss: 2.9601330757141113
Validation loss: 2.244029386069185

Epoch: 5| Step: 10
Training loss: 1.766030192375183
Validation loss: 2.2347484916769047

Epoch: 156| Step: 0
Training loss: 2.795269250869751
Validation loss: 2.2255192456706876

Epoch: 5| Step: 1
Training loss: 2.5743374824523926
Validation loss: 2.224341320735152

Epoch: 5| Step: 2
Training loss: 2.591426372528076
Validation loss: 2.2172173377006286

Epoch: 5| Step: 3
Training loss: 2.1231207847595215
Validation loss: 2.2016550469142135

Epoch: 5| Step: 4
Training loss: 2.8236327171325684
Validation loss: 2.2052795553720124

Epoch: 5| Step: 5
Training loss: 2.7810769081115723
Validation loss: 2.2098403694809123

Epoch: 5| Step: 6
Training loss: 1.584615707397461
Validation loss: 2.2030443453019664

Epoch: 5| Step: 7
Training loss: 2.6349120140075684
Validation loss: 2.206599909772155

Epoch: 5| Step: 8
Training loss: 1.9705696105957031
Validation loss: 2.205293809213946

Epoch: 5| Step: 9
Training loss: 2.500255584716797
Validation loss: 2.21143324400789

Epoch: 5| Step: 10
Training loss: 2.5396900177001953
Validation loss: 2.2098481193665536

Epoch: 157| Step: 0
Training loss: 3.2969696521759033
Validation loss: 2.2113416810189523

Epoch: 5| Step: 1
Training loss: 2.8000235557556152
Validation loss: 2.2298722190241658

Epoch: 5| Step: 2
Training loss: 2.1694462299346924
Validation loss: 2.215469475715391

Epoch: 5| Step: 3
Training loss: 2.335422992706299
Validation loss: 2.21804799443932

Epoch: 5| Step: 4
Training loss: 1.8068002462387085
Validation loss: 2.2096296356570337

Epoch: 5| Step: 5
Training loss: 2.024637460708618
Validation loss: 2.2053463869197394

Epoch: 5| Step: 6
Training loss: 2.301304578781128
Validation loss: 2.2097256619443177

Epoch: 5| Step: 7
Training loss: 2.1446144580841064
Validation loss: 2.209406893740418

Epoch: 5| Step: 8
Training loss: 2.8503825664520264
Validation loss: 2.2246814773928736

Epoch: 5| Step: 9
Training loss: 3.0853326320648193
Validation loss: 2.2321346780305267

Epoch: 5| Step: 10
Training loss: 2.13919997215271
Validation loss: 2.2406330467552267

Epoch: 158| Step: 0
Training loss: 2.610147476196289
Validation loss: 2.2394634421153734

Epoch: 5| Step: 1
Training loss: 1.8241775035858154
Validation loss: 2.228074255809989

Epoch: 5| Step: 2
Training loss: 2.490830898284912
Validation loss: 2.238328190260036

Epoch: 5| Step: 3
Training loss: 2.265202760696411
Validation loss: 2.2443353578608525

Epoch: 5| Step: 4
Training loss: 2.931760787963867
Validation loss: 2.231082030521926

Epoch: 5| Step: 5
Training loss: 2.350145101547241
Validation loss: 2.2179608883396273

Epoch: 5| Step: 6
Training loss: 2.3139872550964355
Validation loss: 2.216695393285444

Epoch: 5| Step: 7
Training loss: 2.7246437072753906
Validation loss: 2.2122561226608934

Epoch: 5| Step: 8
Training loss: 2.579153060913086
Validation loss: 2.2123742731668616

Epoch: 5| Step: 9
Training loss: 2.266777753829956
Validation loss: 2.203931966135579

Epoch: 5| Step: 10
Training loss: 2.4896087646484375
Validation loss: 2.207069489263719

Epoch: 159| Step: 0
Training loss: 1.9660851955413818
Validation loss: 2.2070403483606156

Epoch: 5| Step: 1
Training loss: 2.599691867828369
Validation loss: 2.2225717626592165

Epoch: 5| Step: 2
Training loss: 3.0087497234344482
Validation loss: 2.210591364932317

Epoch: 5| Step: 3
Training loss: 2.993809938430786
Validation loss: 2.2169908861960135

Epoch: 5| Step: 4
Training loss: 2.417437791824341
Validation loss: 2.2102167426898913

Epoch: 5| Step: 5
Training loss: 2.049116849899292
Validation loss: 2.203922558856267

Epoch: 5| Step: 6
Training loss: 1.911956548690796
Validation loss: 2.209431481617753

Epoch: 5| Step: 7
Training loss: 2.406604528427124
Validation loss: 2.2112754826904624

Epoch: 5| Step: 8
Training loss: 3.167288303375244
Validation loss: 2.2191157725549515

Epoch: 5| Step: 9
Training loss: 1.7782713174819946
Validation loss: 2.204814026432653

Epoch: 5| Step: 10
Training loss: 2.5176076889038086
Validation loss: 2.207307405369256

Epoch: 160| Step: 0
Training loss: 2.3810245990753174
Validation loss: 2.2008796225311937

Epoch: 5| Step: 1
Training loss: 2.1133599281311035
Validation loss: 2.2149542288113664

Epoch: 5| Step: 2
Training loss: 1.6151180267333984
Validation loss: 2.223882072715349

Epoch: 5| Step: 3
Training loss: 2.4883084297180176
Validation loss: 2.215459080152614

Epoch: 5| Step: 4
Training loss: 2.459768772125244
Validation loss: 2.220825623440486

Epoch: 5| Step: 5
Training loss: 2.524082899093628
Validation loss: 2.231650149950417

Epoch: 5| Step: 6
Training loss: 2.183960437774658
Validation loss: 2.2353103647949877

Epoch: 5| Step: 7
Training loss: 3.1514759063720703
Validation loss: 2.2319959825085056

Epoch: 5| Step: 8
Training loss: 2.8840155601501465
Validation loss: 2.2251151454064155

Epoch: 5| Step: 9
Training loss: 2.4326066970825195
Validation loss: 2.226914013585737

Epoch: 5| Step: 10
Training loss: 2.5924787521362305
Validation loss: 2.2295198876370668

Epoch: 161| Step: 0
Training loss: 3.0019164085388184
Validation loss: 2.2193475487411662

Epoch: 5| Step: 1
Training loss: 2.5065865516662598
Validation loss: 2.2063853010054557

Epoch: 5| Step: 2
Training loss: 1.9092495441436768
Validation loss: 2.2132805137224096

Epoch: 5| Step: 3
Training loss: 2.885700225830078
Validation loss: 2.1903661104940597

Epoch: 5| Step: 4
Training loss: 2.7286362648010254
Validation loss: 2.1961260200828634

Epoch: 5| Step: 5
Training loss: 2.632056713104248
Validation loss: 2.1863601899916127

Epoch: 5| Step: 6
Training loss: 2.3333888053894043
Validation loss: 2.195590132026262

Epoch: 5| Step: 7
Training loss: 2.1934096813201904
Validation loss: 2.198573080442285

Epoch: 5| Step: 8
Training loss: 2.4105966091156006
Validation loss: 2.1986003562968266

Epoch: 5| Step: 9
Training loss: 2.722600221633911
Validation loss: 2.192615511596844

Epoch: 5| Step: 10
Training loss: 1.3352298736572266
Validation loss: 2.2120450414637083

Epoch: 162| Step: 0
Training loss: 2.9683704376220703
Validation loss: 2.2184352772210234

Epoch: 5| Step: 1
Training loss: 2.4423069953918457
Validation loss: 2.228150116500034

Epoch: 5| Step: 2
Training loss: 2.3619818687438965
Validation loss: 2.2301542656396025

Epoch: 5| Step: 3
Training loss: 2.2921106815338135
Validation loss: 2.2328968099368516

Epoch: 5| Step: 4
Training loss: 2.2138397693634033
Validation loss: 2.2503389991739744

Epoch: 5| Step: 5
Training loss: 2.4298720359802246
Validation loss: 2.250498705012824

Epoch: 5| Step: 6
Training loss: 3.036397695541382
Validation loss: 2.257567895356045

Epoch: 5| Step: 7
Training loss: 1.954053521156311
Validation loss: 2.264887217552431

Epoch: 5| Step: 8
Training loss: 2.321758985519409
Validation loss: 2.2531636402171147

Epoch: 5| Step: 9
Training loss: 2.1391618251800537
Validation loss: 2.274391184570969

Epoch: 5| Step: 10
Training loss: 2.785820960998535
Validation loss: 2.255491869423979

Epoch: 163| Step: 0
Training loss: 2.8563246726989746
Validation loss: 2.2453346970260784

Epoch: 5| Step: 1
Training loss: 2.3910253047943115
Validation loss: 2.246006081181188

Epoch: 5| Step: 2
Training loss: 2.473196506500244
Validation loss: 2.2057414131779827

Epoch: 5| Step: 3
Training loss: 2.5254406929016113
Validation loss: 2.2121234491307247

Epoch: 5| Step: 4
Training loss: 2.041998863220215
Validation loss: 2.1939667437666204

Epoch: 5| Step: 5
Training loss: 2.8980510234832764
Validation loss: 2.1927160819371543

Epoch: 5| Step: 6
Training loss: 2.9713542461395264
Validation loss: 2.194773522756433

Epoch: 5| Step: 7
Training loss: 2.5325963497161865
Validation loss: 2.1957103590811453

Epoch: 5| Step: 8
Training loss: 1.9179909229278564
Validation loss: 2.193133838715092

Epoch: 5| Step: 9
Training loss: 2.234837055206299
Validation loss: 2.187412490126907

Epoch: 5| Step: 10
Training loss: 2.015568971633911
Validation loss: 2.1968557603897585

Epoch: 164| Step: 0
Training loss: 2.337876796722412
Validation loss: 2.2019736920633624

Epoch: 5| Step: 1
Training loss: 2.713920831680298
Validation loss: 2.2070676460061023

Epoch: 5| Step: 2
Training loss: 2.4810869693756104
Validation loss: 2.2051792196048203

Epoch: 5| Step: 3
Training loss: 2.483619213104248
Validation loss: 2.2086302439371743

Epoch: 5| Step: 4
Training loss: 2.5219173431396484
Validation loss: 2.204376023302796

Epoch: 5| Step: 5
Training loss: 2.7545571327209473
Validation loss: 2.209031319105497

Epoch: 5| Step: 6
Training loss: 2.8586883544921875
Validation loss: 2.206165749539611

Epoch: 5| Step: 7
Training loss: 2.3157825469970703
Validation loss: 2.2026981179432203

Epoch: 5| Step: 8
Training loss: 1.9080040454864502
Validation loss: 2.2024099108993367

Epoch: 5| Step: 9
Training loss: 1.8604516983032227
Validation loss: 2.199396780742112

Epoch: 5| Step: 10
Training loss: 2.5670244693756104
Validation loss: 2.2046848202264435

Epoch: 165| Step: 0
Training loss: 2.2085413932800293
Validation loss: 2.221510310326853

Epoch: 5| Step: 1
Training loss: 2.5936050415039062
Validation loss: 2.2184742432768627

Epoch: 5| Step: 2
Training loss: 2.438922166824341
Validation loss: 2.224319270862046

Epoch: 5| Step: 3
Training loss: 2.4654734134674072
Validation loss: 2.2314323943148375

Epoch: 5| Step: 4
Training loss: 2.8806698322296143
Validation loss: 2.2397693844251734

Epoch: 5| Step: 5
Training loss: 2.580758810043335
Validation loss: 2.2308450386088383

Epoch: 5| Step: 6
Training loss: 2.449073076248169
Validation loss: 2.200711270814301

Epoch: 5| Step: 7
Training loss: 2.424429416656494
Validation loss: 2.1921899780150382

Epoch: 5| Step: 8
Training loss: 2.8452670574188232
Validation loss: 2.190549037789786

Epoch: 5| Step: 9
Training loss: 2.1184957027435303
Validation loss: 2.2088497069574173

Epoch: 5| Step: 10
Training loss: 1.791590929031372
Validation loss: 2.2126688931577947

Epoch: 166| Step: 0
Training loss: 2.5970845222473145
Validation loss: 2.2276179713587605

Epoch: 5| Step: 1
Training loss: 2.1402549743652344
Validation loss: 2.2234601615577616

Epoch: 5| Step: 2
Training loss: 2.878981351852417
Validation loss: 2.2245583534240723

Epoch: 5| Step: 3
Training loss: 1.829721450805664
Validation loss: 2.2211096389319307

Epoch: 5| Step: 4
Training loss: 2.4059410095214844
Validation loss: 2.2184388406815065

Epoch: 5| Step: 5
Training loss: 2.7140188217163086
Validation loss: 2.2049955065532396

Epoch: 5| Step: 6
Training loss: 2.751234292984009
Validation loss: 2.2053451102267028

Epoch: 5| Step: 7
Training loss: 1.987048864364624
Validation loss: 2.2034531075467347

Epoch: 5| Step: 8
Training loss: 2.412142038345337
Validation loss: 2.1989846511553695

Epoch: 5| Step: 9
Training loss: 2.459777355194092
Validation loss: 2.204411209270518

Epoch: 5| Step: 10
Training loss: 2.787057876586914
Validation loss: 2.203726412147604

Epoch: 167| Step: 0
Training loss: 2.0910866260528564
Validation loss: 2.2144693123397006

Epoch: 5| Step: 1
Training loss: 2.3954615592956543
Validation loss: 2.2190234020192134

Epoch: 5| Step: 2
Training loss: 2.0813565254211426
Validation loss: 2.2040216589486725

Epoch: 5| Step: 3
Training loss: 2.545663833618164
Validation loss: 2.208514626308154

Epoch: 5| Step: 4
Training loss: 2.2322020530700684
Validation loss: 2.2073589691551785

Epoch: 5| Step: 5
Training loss: 2.984060764312744
Validation loss: 2.215653693804177

Epoch: 5| Step: 6
Training loss: 2.658970594406128
Validation loss: 2.2105095271141297

Epoch: 5| Step: 7
Training loss: 2.6512556076049805
Validation loss: 2.2141056650428363

Epoch: 5| Step: 8
Training loss: 1.7140182256698608
Validation loss: 2.2022776014061383

Epoch: 5| Step: 9
Training loss: 3.18161940574646
Validation loss: 2.217151923846173

Epoch: 5| Step: 10
Training loss: 2.0864486694335938
Validation loss: 2.2066541820444088

Epoch: 168| Step: 0
Training loss: 2.3575644493103027
Validation loss: 2.2231816348209175

Epoch: 5| Step: 1
Training loss: 1.715911626815796
Validation loss: 2.2271731412538918

Epoch: 5| Step: 2
Training loss: 2.7550668716430664
Validation loss: 2.2342908151688112

Epoch: 5| Step: 3
Training loss: 2.332062244415283
Validation loss: 2.2438698942943285

Epoch: 5| Step: 4
Training loss: 2.4735922813415527
Validation loss: 2.2300990114929857

Epoch: 5| Step: 5
Training loss: 2.168550729751587
Validation loss: 2.227319786625524

Epoch: 5| Step: 6
Training loss: 2.686495542526245
Validation loss: 2.226795865643409

Epoch: 5| Step: 7
Training loss: 2.367563247680664
Validation loss: 2.206387714673114

Epoch: 5| Step: 8
Training loss: 2.39591646194458
Validation loss: 2.2162993620800715

Epoch: 5| Step: 9
Training loss: 2.8912558555603027
Validation loss: 2.21169460973432

Epoch: 5| Step: 10
Training loss: 2.5062427520751953
Validation loss: 2.2165952459458382

Epoch: 169| Step: 0
Training loss: 2.3217215538024902
Validation loss: 2.2101509314711376

Epoch: 5| Step: 1
Training loss: 2.500276565551758
Validation loss: 2.208978640135898

Epoch: 5| Step: 2
Training loss: 2.710427761077881
Validation loss: 2.217603696289883

Epoch: 5| Step: 3
Training loss: 2.4011378288269043
Validation loss: 2.219329539165702

Epoch: 5| Step: 4
Training loss: 2.4768195152282715
Validation loss: 2.215243249811152

Epoch: 5| Step: 5
Training loss: 3.04067325592041
Validation loss: 2.2246445737859255

Epoch: 5| Step: 6
Training loss: 1.9023805856704712
Validation loss: 2.2298291934433805

Epoch: 5| Step: 7
Training loss: 2.7810120582580566
Validation loss: 2.227842216850609

Epoch: 5| Step: 8
Training loss: 2.2608041763305664
Validation loss: 2.2417676935913744

Epoch: 5| Step: 9
Training loss: 2.329244375228882
Validation loss: 2.250885437893611

Epoch: 5| Step: 10
Training loss: 1.7777179479599
Validation loss: 2.2530366348963913

Epoch: 170| Step: 0
Training loss: 2.3753225803375244
Validation loss: 2.2518701502071914

Epoch: 5| Step: 1
Training loss: 2.301069498062134
Validation loss: 2.23476678581648

Epoch: 5| Step: 2
Training loss: 2.199401378631592
Validation loss: 2.227425016382689

Epoch: 5| Step: 3
Training loss: 1.6441093683242798
Validation loss: 2.211392174484909

Epoch: 5| Step: 4
Training loss: 3.33012318611145
Validation loss: 2.2077332235151723

Epoch: 5| Step: 5
Training loss: 2.0392699241638184
Validation loss: 2.19923149642124

Epoch: 5| Step: 6
Training loss: 2.4208920001983643
Validation loss: 2.1979422377001856

Epoch: 5| Step: 7
Training loss: 3.198103666305542
Validation loss: 2.201219933007353

Epoch: 5| Step: 8
Training loss: 2.5478670597076416
Validation loss: 2.196792015465357

Epoch: 5| Step: 9
Training loss: 2.707491874694824
Validation loss: 2.1936047512997865

Epoch: 5| Step: 10
Training loss: 1.6414870023727417
Validation loss: 2.2052902611353065

Epoch: 171| Step: 0
Training loss: 2.381720542907715
Validation loss: 2.2106025859873784

Epoch: 5| Step: 1
Training loss: 2.7694637775421143
Validation loss: 2.2118538810360815

Epoch: 5| Step: 2
Training loss: 2.6946959495544434
Validation loss: 2.2119759949304725

Epoch: 5| Step: 3
Training loss: 2.5904057025909424
Validation loss: 2.2327476316882717

Epoch: 5| Step: 4
Training loss: 2.2486374378204346
Validation loss: 2.2256408506824124

Epoch: 5| Step: 5
Training loss: 2.9893784523010254
Validation loss: 2.216625339241438

Epoch: 5| Step: 6
Training loss: 2.1306004524230957
Validation loss: 2.2090042252694406

Epoch: 5| Step: 7
Training loss: 2.351229667663574
Validation loss: 2.2101975820397817

Epoch: 5| Step: 8
Training loss: 2.115903854370117
Validation loss: 2.212398539307297

Epoch: 5| Step: 9
Training loss: 1.8599802255630493
Validation loss: 2.211137653678976

Epoch: 5| Step: 10
Training loss: 2.5856614112854004
Validation loss: 2.2208990819992556

Epoch: 172| Step: 0
Training loss: 2.284802198410034
Validation loss: 2.210983714749736

Epoch: 5| Step: 1
Training loss: 2.514349937438965
Validation loss: 2.2099419024682816

Epoch: 5| Step: 2
Training loss: 2.3817291259765625
Validation loss: 2.210212443464546

Epoch: 5| Step: 3
Training loss: 2.0915942192077637
Validation loss: 2.2089722694889193

Epoch: 5| Step: 4
Training loss: 2.5167689323425293
Validation loss: 2.207320394054536

Epoch: 5| Step: 5
Training loss: 2.7832412719726562
Validation loss: 2.1975192075134604

Epoch: 5| Step: 6
Training loss: 2.422682285308838
Validation loss: 2.1944308921854985

Epoch: 5| Step: 7
Training loss: 2.457951068878174
Validation loss: 2.202907616092313

Epoch: 5| Step: 8
Training loss: 2.562093496322632
Validation loss: 2.1961475059550297

Epoch: 5| Step: 9
Training loss: 2.20448637008667
Validation loss: 2.201752121730517

Epoch: 5| Step: 10
Training loss: 2.4782657623291016
Validation loss: 2.194930648290983

Epoch: 173| Step: 0
Training loss: 2.972740650177002
Validation loss: 2.1936791584055912

Epoch: 5| Step: 1
Training loss: 2.3288323879241943
Validation loss: 2.188843978348599

Epoch: 5| Step: 2
Training loss: 2.4523141384124756
Validation loss: 2.185717841630341

Epoch: 5| Step: 3
Training loss: 2.0655441284179688
Validation loss: 2.1909818110927457

Epoch: 5| Step: 4
Training loss: 2.9076907634735107
Validation loss: 2.1922024603812926

Epoch: 5| Step: 5
Training loss: 2.368298053741455
Validation loss: 2.199803388246926

Epoch: 5| Step: 6
Training loss: 2.536987781524658
Validation loss: 2.19154720024396

Epoch: 5| Step: 7
Training loss: 2.1610350608825684
Validation loss: 2.19679396383224

Epoch: 5| Step: 8
Training loss: 2.382148265838623
Validation loss: 2.2030926930007113

Epoch: 5| Step: 9
Training loss: 2.149265766143799
Validation loss: 2.2087544407895816

Epoch: 5| Step: 10
Training loss: 2.146623134613037
Validation loss: 2.2072143067595777

Epoch: 174| Step: 0
Training loss: 2.6121490001678467
Validation loss: 2.223155560032014

Epoch: 5| Step: 1
Training loss: 2.588623046875
Validation loss: 2.202597697575887

Epoch: 5| Step: 2
Training loss: 2.9678947925567627
Validation loss: 2.2153771705524896

Epoch: 5| Step: 3
Training loss: 2.3005571365356445
Validation loss: 2.221419031902026

Epoch: 5| Step: 4
Training loss: 2.6667778491973877
Validation loss: 2.2272589847605717

Epoch: 5| Step: 5
Training loss: 2.223719835281372
Validation loss: 2.208105630772088

Epoch: 5| Step: 6
Training loss: 2.0659077167510986
Validation loss: 2.216358741124471

Epoch: 5| Step: 7
Training loss: 2.27424693107605
Validation loss: 2.2133958980601323

Epoch: 5| Step: 8
Training loss: 2.6558420658111572
Validation loss: 2.1967976503474738

Epoch: 5| Step: 9
Training loss: 2.025908946990967
Validation loss: 2.2084370736152894

Epoch: 5| Step: 10
Training loss: 2.1311371326446533
Validation loss: 2.2026746375586397

Epoch: 175| Step: 0
Training loss: 2.4853358268737793
Validation loss: 2.193137966176515

Epoch: 5| Step: 1
Training loss: 2.257290840148926
Validation loss: 2.1849180293339554

Epoch: 5| Step: 2
Training loss: 2.663271903991699
Validation loss: 2.195126918054396

Epoch: 5| Step: 3
Training loss: 2.941831350326538
Validation loss: 2.19868314907115

Epoch: 5| Step: 4
Training loss: 2.1308345794677734
Validation loss: 2.1865242783741285

Epoch: 5| Step: 5
Training loss: 3.059873342514038
Validation loss: 2.1852239959983417

Epoch: 5| Step: 6
Training loss: 2.1699624061584473
Validation loss: 2.1754892641498196

Epoch: 5| Step: 7
Training loss: 2.0756797790527344
Validation loss: 2.178077974627095

Epoch: 5| Step: 8
Training loss: 2.239924907684326
Validation loss: 2.1905806910607124

Epoch: 5| Step: 9
Training loss: 2.196218967437744
Validation loss: 2.1876741352901665

Epoch: 5| Step: 10
Training loss: 2.409240484237671
Validation loss: 2.1883676564821632

Epoch: 176| Step: 0
Training loss: 2.3291444778442383
Validation loss: 2.197302608079808

Epoch: 5| Step: 1
Training loss: 2.530895948410034
Validation loss: 2.207351992207189

Epoch: 5| Step: 2
Training loss: 2.1669952869415283
Validation loss: 2.196268017574023

Epoch: 5| Step: 3
Training loss: 2.3047051429748535
Validation loss: 2.2082120680039927

Epoch: 5| Step: 4
Training loss: 2.0350303649902344
Validation loss: 2.1941756471510856

Epoch: 5| Step: 5
Training loss: 2.194281816482544
Validation loss: 2.1964298191890923

Epoch: 5| Step: 6
Training loss: 2.371425151824951
Validation loss: 2.1850319293237503

Epoch: 5| Step: 7
Training loss: 2.3636035919189453
Validation loss: 2.1900855161810435

Epoch: 5| Step: 8
Training loss: 2.2404725551605225
Validation loss: 2.20023843806277

Epoch: 5| Step: 9
Training loss: 3.363546848297119
Validation loss: 2.236649069734799

Epoch: 5| Step: 10
Training loss: 2.7523765563964844
Validation loss: 2.2316639500279583

Epoch: 177| Step: 0
Training loss: 2.714634895324707
Validation loss: 2.2354576792768253

Epoch: 5| Step: 1
Training loss: 2.5361599922180176
Validation loss: 2.2264745004715456

Epoch: 5| Step: 2
Training loss: 2.7547552585601807
Validation loss: 2.229495853506109

Epoch: 5| Step: 3
Training loss: 2.3726065158843994
Validation loss: 2.2259761159138014

Epoch: 5| Step: 4
Training loss: 1.9319400787353516
Validation loss: 2.2239502732471754

Epoch: 5| Step: 5
Training loss: 2.4184765815734863
Validation loss: 2.223033630719749

Epoch: 5| Step: 6
Training loss: 2.479809522628784
Validation loss: 2.2373826196116786

Epoch: 5| Step: 7
Training loss: 2.2315449714660645
Validation loss: 2.2370661715025544

Epoch: 5| Step: 8
Training loss: 2.660264730453491
Validation loss: 2.2157980075446506

Epoch: 5| Step: 9
Training loss: 2.651421070098877
Validation loss: 2.2228306634451753

Epoch: 5| Step: 10
Training loss: 1.9867174625396729
Validation loss: 2.2223445933352233

Epoch: 178| Step: 0
Training loss: 2.7653489112854004
Validation loss: 2.2003642641088015

Epoch: 5| Step: 1
Training loss: 2.6062159538269043
Validation loss: 2.1825960759193666

Epoch: 5| Step: 2
Training loss: 2.528103828430176
Validation loss: 2.1723840569937103

Epoch: 5| Step: 3
Training loss: 2.655089855194092
Validation loss: 2.1684052354546

Epoch: 5| Step: 4
Training loss: 1.955255150794983
Validation loss: 2.174502823942451

Epoch: 5| Step: 5
Training loss: 2.483794689178467
Validation loss: 2.1860784151220836

Epoch: 5| Step: 6
Training loss: 2.162654399871826
Validation loss: 2.184820523826025

Epoch: 5| Step: 7
Training loss: 2.356351375579834
Validation loss: 2.1831053815862185

Epoch: 5| Step: 8
Training loss: 1.8240474462509155
Validation loss: 2.2011463103755826

Epoch: 5| Step: 9
Training loss: 2.7136478424072266
Validation loss: 2.2031959692637124

Epoch: 5| Step: 10
Training loss: 2.592141628265381
Validation loss: 2.205092150677917

Epoch: 179| Step: 0
Training loss: 2.1661934852600098
Validation loss: 2.1965131964734805

Epoch: 5| Step: 1
Training loss: 3.1556620597839355
Validation loss: 2.1969070870389222

Epoch: 5| Step: 2
Training loss: 2.0368425846099854
Validation loss: 2.2004449495705227

Epoch: 5| Step: 3
Training loss: 2.729419231414795
Validation loss: 2.217716855387534

Epoch: 5| Step: 4
Training loss: 1.9135029315948486
Validation loss: 2.215597464192298

Epoch: 5| Step: 5
Training loss: 1.9520022869110107
Validation loss: 2.1966271733724945

Epoch: 5| Step: 6
Training loss: 2.545562982559204
Validation loss: 2.1997791823520454

Epoch: 5| Step: 7
Training loss: 1.8843719959259033
Validation loss: 2.186529673555846

Epoch: 5| Step: 8
Training loss: 2.9386067390441895
Validation loss: 2.1876803444277857

Epoch: 5| Step: 9
Training loss: 2.656697988510132
Validation loss: 2.176118386689053

Epoch: 5| Step: 10
Training loss: 2.580538034439087
Validation loss: 2.1745782308681036

Epoch: 180| Step: 0
Training loss: 2.7049343585968018
Validation loss: 2.185897372102225

Epoch: 5| Step: 1
Training loss: 2.4116909503936768
Validation loss: 2.1789968782855618

Epoch: 5| Step: 2
Training loss: 2.6067121028900146
Validation loss: 2.1803453071143037

Epoch: 5| Step: 3
Training loss: 2.0226588249206543
Validation loss: 2.179022745419574

Epoch: 5| Step: 4
Training loss: 2.5430073738098145
Validation loss: 2.1790041462067635

Epoch: 5| Step: 5
Training loss: 2.132869243621826
Validation loss: 2.187947582173091

Epoch: 5| Step: 6
Training loss: 2.561264753341675
Validation loss: 2.1892018189994236

Epoch: 5| Step: 7
Training loss: 1.9778070449829102
Validation loss: 2.189915595516082

Epoch: 5| Step: 8
Training loss: 2.2885873317718506
Validation loss: 2.2005791625668927

Epoch: 5| Step: 9
Training loss: 2.32197904586792
Validation loss: 2.198768634949961

Epoch: 5| Step: 10
Training loss: 2.948397397994995
Validation loss: 2.2161567223969327

Epoch: 181| Step: 0
Training loss: 2.89141845703125
Validation loss: 2.2254739025587678

Epoch: 5| Step: 1
Training loss: 3.233264446258545
Validation loss: 2.226679766049949

Epoch: 5| Step: 2
Training loss: 2.407872200012207
Validation loss: 2.2389442446411296

Epoch: 5| Step: 3
Training loss: 1.8255748748779297
Validation loss: 2.2406636591880553

Epoch: 5| Step: 4
Training loss: 2.0062172412872314
Validation loss: 2.245047661565965

Epoch: 5| Step: 5
Training loss: 2.3448140621185303
Validation loss: 2.2396745451035036

Epoch: 5| Step: 6
Training loss: 2.566741704940796
Validation loss: 2.227577660673408

Epoch: 5| Step: 7
Training loss: 2.3364131450653076
Validation loss: 2.2305390898899367

Epoch: 5| Step: 8
Training loss: 2.3351707458496094
Validation loss: 2.208716020789198

Epoch: 5| Step: 9
Training loss: 2.0004143714904785
Validation loss: 2.204671416231381

Epoch: 5| Step: 10
Training loss: 2.614935874938965
Validation loss: 2.1887464100314724

Epoch: 182| Step: 0
Training loss: 2.449221134185791
Validation loss: 2.197607312151181

Epoch: 5| Step: 1
Training loss: 1.794103980064392
Validation loss: 2.185680663713845

Epoch: 5| Step: 2
Training loss: 1.7037525177001953
Validation loss: 2.1810311707117225

Epoch: 5| Step: 3
Training loss: 2.1152169704437256
Validation loss: 2.1721977751742125

Epoch: 5| Step: 4
Training loss: 2.7602744102478027
Validation loss: 2.1857535890353623

Epoch: 5| Step: 5
Training loss: 2.2040157318115234
Validation loss: 2.183264181178103

Epoch: 5| Step: 6
Training loss: 3.1090691089630127
Validation loss: 2.1890001476451917

Epoch: 5| Step: 7
Training loss: 2.1867268085479736
Validation loss: 2.1923707159616614

Epoch: 5| Step: 8
Training loss: 2.5790812969207764
Validation loss: 2.195407206012357

Epoch: 5| Step: 9
Training loss: 3.0913302898406982
Validation loss: 2.200231138096061

Epoch: 5| Step: 10
Training loss: 2.4811344146728516
Validation loss: 2.19907945458607

Epoch: 183| Step: 0
Training loss: 2.3565282821655273
Validation loss: 2.205232620239258

Epoch: 5| Step: 1
Training loss: 3.3932044506073
Validation loss: 2.208165749426811

Epoch: 5| Step: 2
Training loss: 2.367419719696045
Validation loss: 2.2187247481397403

Epoch: 5| Step: 3
Training loss: 2.4456024169921875
Validation loss: 2.2101778702069352

Epoch: 5| Step: 4
Training loss: 2.1102445125579834
Validation loss: 2.20760255218834

Epoch: 5| Step: 5
Training loss: 2.567202091217041
Validation loss: 2.199389403866183

Epoch: 5| Step: 6
Training loss: 1.807337999343872
Validation loss: 2.205570182492656

Epoch: 5| Step: 7
Training loss: 2.912419557571411
Validation loss: 2.1885435581207275

Epoch: 5| Step: 8
Training loss: 2.176642894744873
Validation loss: 2.184301343015445

Epoch: 5| Step: 9
Training loss: 2.1055400371551514
Validation loss: 2.185778412767636

Epoch: 5| Step: 10
Training loss: 2.1915547847747803
Validation loss: 2.192711983957598

Epoch: 184| Step: 0
Training loss: 2.199843168258667
Validation loss: 2.1938707367066415

Epoch: 5| Step: 1
Training loss: 2.194422721862793
Validation loss: 2.193468920646175

Epoch: 5| Step: 2
Training loss: 3.094330310821533
Validation loss: 2.196407051496608

Epoch: 5| Step: 3
Training loss: 2.197117567062378
Validation loss: 2.1901432980773268

Epoch: 5| Step: 4
Training loss: 2.2411417961120605
Validation loss: 2.186675194771059

Epoch: 5| Step: 5
Training loss: 2.564838171005249
Validation loss: 2.1786739646747546

Epoch: 5| Step: 6
Training loss: 2.8090598583221436
Validation loss: 2.170490898111815

Epoch: 5| Step: 7
Training loss: 2.3092098236083984
Validation loss: 2.157080787484364

Epoch: 5| Step: 8
Training loss: 2.363901138305664
Validation loss: 2.1682363710095807

Epoch: 5| Step: 9
Training loss: 2.660491704940796
Validation loss: 2.17334347386514

Epoch: 5| Step: 10
Training loss: 1.780642032623291
Validation loss: 2.184549903356901

Epoch: 185| Step: 0
Training loss: 2.5552573204040527
Validation loss: 2.212381261651234

Epoch: 5| Step: 1
Training loss: 2.6349658966064453
Validation loss: 2.211428875564247

Epoch: 5| Step: 2
Training loss: 2.241072654724121
Validation loss: 2.2252392461222987

Epoch: 5| Step: 3
Training loss: 2.2435920238494873
Validation loss: 2.2278524496222056

Epoch: 5| Step: 4
Training loss: 2.1524107456207275
Validation loss: 2.215989415363599

Epoch: 5| Step: 5
Training loss: 2.1129138469696045
Validation loss: 2.1993662054820726

Epoch: 5| Step: 6
Training loss: 2.88873291015625
Validation loss: 2.190382198620868

Epoch: 5| Step: 7
Training loss: 2.7261383533477783
Validation loss: 2.181561421322566

Epoch: 5| Step: 8
Training loss: 2.3303115367889404
Validation loss: 2.1770043552562757

Epoch: 5| Step: 9
Training loss: 2.6620330810546875
Validation loss: 2.1775720580931632

Epoch: 5| Step: 10
Training loss: 2.017536163330078
Validation loss: 2.1814182791658627

Epoch: 186| Step: 0
Training loss: 2.863591432571411
Validation loss: 2.180113709101113

Epoch: 5| Step: 1
Training loss: 1.8571220636367798
Validation loss: 2.189071363018405

Epoch: 5| Step: 2
Training loss: 2.0678868293762207
Validation loss: 2.204632492475612

Epoch: 5| Step: 3
Training loss: 2.1070334911346436
Validation loss: 2.2100056473926832

Epoch: 5| Step: 4
Training loss: 2.896728992462158
Validation loss: 2.200952419670679

Epoch: 5| Step: 5
Training loss: 2.756446361541748
Validation loss: 2.1973492637757333

Epoch: 5| Step: 6
Training loss: 1.8768936395645142
Validation loss: 2.2021657574561333

Epoch: 5| Step: 7
Training loss: 2.2749602794647217
Validation loss: 2.1928567835079726

Epoch: 5| Step: 8
Training loss: 2.5146522521972656
Validation loss: 2.211313457899196

Epoch: 5| Step: 9
Training loss: 2.509934902191162
Validation loss: 2.1961393023049958

Epoch: 5| Step: 10
Training loss: 2.6573288440704346
Validation loss: 2.1862804300041607

Epoch: 187| Step: 0
Training loss: 2.333954334259033
Validation loss: 2.189719561607607

Epoch: 5| Step: 1
Training loss: 1.8994312286376953
Validation loss: 2.192901842055782

Epoch: 5| Step: 2
Training loss: 2.4262776374816895
Validation loss: 2.186874210193593

Epoch: 5| Step: 3
Training loss: 3.2047019004821777
Validation loss: 2.187768054264848

Epoch: 5| Step: 4
Training loss: 2.353386878967285
Validation loss: 2.1921543434102047

Epoch: 5| Step: 5
Training loss: 2.0407142639160156
Validation loss: 2.178371621716407

Epoch: 5| Step: 6
Training loss: 2.6202712059020996
Validation loss: 2.1973934147947576

Epoch: 5| Step: 7
Training loss: 2.545142889022827
Validation loss: 2.1884539409350325

Epoch: 5| Step: 8
Training loss: 1.9262311458587646
Validation loss: 2.19646107509572

Epoch: 5| Step: 9
Training loss: 2.545550584793091
Validation loss: 2.188793054191015

Epoch: 5| Step: 10
Training loss: 2.2475600242614746
Validation loss: 2.18928575259383

Epoch: 188| Step: 0
Training loss: 1.7556642293930054
Validation loss: 2.1973165363393803

Epoch: 5| Step: 1
Training loss: 2.4290452003479004
Validation loss: 2.2029442364169705

Epoch: 5| Step: 2
Training loss: 3.0518088340759277
Validation loss: 2.1947242931653093

Epoch: 5| Step: 3
Training loss: 2.0839123725891113
Validation loss: 2.2066886912110033

Epoch: 5| Step: 4
Training loss: 2.438434600830078
Validation loss: 2.1920967730142737

Epoch: 5| Step: 5
Training loss: 2.5698273181915283
Validation loss: 2.1937389642961564

Epoch: 5| Step: 6
Training loss: 2.3342483043670654
Validation loss: 2.1834278157962266

Epoch: 5| Step: 7
Training loss: 2.804034471511841
Validation loss: 2.1853925015336726

Epoch: 5| Step: 8
Training loss: 1.9054969549179077
Validation loss: 2.1839329222197175

Epoch: 5| Step: 9
Training loss: 2.700941801071167
Validation loss: 2.189024317649103

Epoch: 5| Step: 10
Training loss: 2.1471354961395264
Validation loss: 2.180630915908403

Epoch: 189| Step: 0
Training loss: 2.169466495513916
Validation loss: 2.185430315233046

Epoch: 5| Step: 1
Training loss: 3.114844560623169
Validation loss: 2.188456291793495

Epoch: 5| Step: 2
Training loss: 2.729814052581787
Validation loss: 2.1851997042214997

Epoch: 5| Step: 3
Training loss: 2.160146474838257
Validation loss: 2.188701627075031

Epoch: 5| Step: 4
Training loss: 2.071657180786133
Validation loss: 2.176888189008159

Epoch: 5| Step: 5
Training loss: 2.3491361141204834
Validation loss: 2.1898634767019622

Epoch: 5| Step: 6
Training loss: 2.8060977458953857
Validation loss: 2.1985595841561594

Epoch: 5| Step: 7
Training loss: 2.5761046409606934
Validation loss: 2.1896866342072845

Epoch: 5| Step: 8
Training loss: 1.8643862009048462
Validation loss: 2.191709454341601

Epoch: 5| Step: 9
Training loss: 2.3027141094207764
Validation loss: 2.1928383945136942

Epoch: 5| Step: 10
Training loss: 2.1003432273864746
Validation loss: 2.185344152553107

Epoch: 190| Step: 0
Training loss: 2.4367785453796387
Validation loss: 2.191724997694774

Epoch: 5| Step: 1
Training loss: 2.3934237957000732
Validation loss: 2.1938538294966503

Epoch: 5| Step: 2
Training loss: 2.0732178688049316
Validation loss: 2.1831286414977042

Epoch: 5| Step: 3
Training loss: 1.9957078695297241
Validation loss: 2.1812844096973376

Epoch: 5| Step: 4
Training loss: 2.174269437789917
Validation loss: 2.182106441067111

Epoch: 5| Step: 5
Training loss: 2.162047863006592
Validation loss: 2.190516120644026

Epoch: 5| Step: 6
Training loss: 2.4132883548736572
Validation loss: 2.194676101848643

Epoch: 5| Step: 7
Training loss: 2.9058563709259033
Validation loss: 2.195148511599469

Epoch: 5| Step: 8
Training loss: 2.703050374984741
Validation loss: 2.20545970496311

Epoch: 5| Step: 9
Training loss: 2.2613396644592285
Validation loss: 2.2094396109222085

Epoch: 5| Step: 10
Training loss: 2.667689085006714
Validation loss: 2.2074083846102477

Epoch: 191| Step: 0
Training loss: 2.099799633026123
Validation loss: 2.2107243640448457

Epoch: 5| Step: 1
Training loss: 2.564840316772461
Validation loss: 2.2086445490519204

Epoch: 5| Step: 2
Training loss: 2.2379603385925293
Validation loss: 2.202448057871993

Epoch: 5| Step: 3
Training loss: 2.0518863201141357
Validation loss: 2.2104310835561445

Epoch: 5| Step: 4
Training loss: 2.268474817276001
Validation loss: 2.1917827360091673

Epoch: 5| Step: 5
Training loss: 2.557629108428955
Validation loss: 2.1879577700809767

Epoch: 5| Step: 6
Training loss: 2.9376327991485596
Validation loss: 2.171788984729398

Epoch: 5| Step: 7
Training loss: 2.3306801319122314
Validation loss: 2.1670091729010306

Epoch: 5| Step: 8
Training loss: 2.293534517288208
Validation loss: 2.1640010508157874

Epoch: 5| Step: 9
Training loss: 2.3617138862609863
Validation loss: 2.1655152074752317

Epoch: 5| Step: 10
Training loss: 2.4836530685424805
Validation loss: 2.166325210243143

Epoch: 192| Step: 0
Training loss: 1.6966642141342163
Validation loss: 2.1721081708067205

Epoch: 5| Step: 1
Training loss: 1.9710830450057983
Validation loss: 2.176397887609338

Epoch: 5| Step: 2
Training loss: 2.5762593746185303
Validation loss: 2.180782305297031

Epoch: 5| Step: 3
Training loss: 3.009410858154297
Validation loss: 2.184120180786297

Epoch: 5| Step: 4
Training loss: 2.4770731925964355
Validation loss: 2.177861516193677

Epoch: 5| Step: 5
Training loss: 2.2941601276397705
Validation loss: 2.188667795991385

Epoch: 5| Step: 6
Training loss: 2.7866501808166504
Validation loss: 2.17942145306577

Epoch: 5| Step: 7
Training loss: 2.2497751712799072
Validation loss: 2.195998704561623

Epoch: 5| Step: 8
Training loss: 2.7408432960510254
Validation loss: 2.183142118556525

Epoch: 5| Step: 9
Training loss: 1.9956676959991455
Validation loss: 2.185935733138874

Epoch: 5| Step: 10
Training loss: 2.33569598197937
Validation loss: 2.1825260603299705

Epoch: 193| Step: 0
Training loss: 2.519446611404419
Validation loss: 2.1904367887845604

Epoch: 5| Step: 1
Training loss: 2.574772596359253
Validation loss: 2.1932117028902938

Epoch: 5| Step: 2
Training loss: 2.404792070388794
Validation loss: 2.1891412555530505

Epoch: 5| Step: 3
Training loss: 2.2239646911621094
Validation loss: 2.1936280612022645

Epoch: 5| Step: 4
Training loss: 1.4774510860443115
Validation loss: 2.1813685176193074

Epoch: 5| Step: 5
Training loss: 3.0000598430633545
Validation loss: 2.190100336587557

Epoch: 5| Step: 6
Training loss: 2.055626392364502
Validation loss: 2.18947511078209

Epoch: 5| Step: 7
Training loss: 2.655341625213623
Validation loss: 2.2003142628618466

Epoch: 5| Step: 8
Training loss: 2.3268990516662598
Validation loss: 2.195810697411978

Epoch: 5| Step: 9
Training loss: 2.8740429878234863
Validation loss: 2.1857116632564093

Epoch: 5| Step: 10
Training loss: 2.042783498764038
Validation loss: 2.2010393091427383

Epoch: 194| Step: 0
Training loss: 2.6254019737243652
Validation loss: 2.1853459855561614

Epoch: 5| Step: 1
Training loss: 2.2759008407592773
Validation loss: 2.1916251028737714

Epoch: 5| Step: 2
Training loss: 2.0133614540100098
Validation loss: 2.1986210705131612

Epoch: 5| Step: 3
Training loss: 2.401921510696411
Validation loss: 2.1941595205696682

Epoch: 5| Step: 4
Training loss: 2.432716131210327
Validation loss: 2.1809015761139574

Epoch: 5| Step: 5
Training loss: 2.5913591384887695
Validation loss: 2.173807860702597

Epoch: 5| Step: 6
Training loss: 2.3662948608398438
Validation loss: 2.180005104311051

Epoch: 5| Step: 7
Training loss: 2.000124454498291
Validation loss: 2.172471861685476

Epoch: 5| Step: 8
Training loss: 2.5806734561920166
Validation loss: 2.1643410113550003

Epoch: 5| Step: 9
Training loss: 2.474315881729126
Validation loss: 2.174445726538217

Epoch: 5| Step: 10
Training loss: 2.335782766342163
Validation loss: 2.1803060834125807

Epoch: 195| Step: 0
Training loss: 2.4612374305725098
Validation loss: 2.1739208980273177

Epoch: 5| Step: 1
Training loss: 2.5188496112823486
Validation loss: 2.1882808208465576

Epoch: 5| Step: 2
Training loss: 2.3785171508789062
Validation loss: 2.1835148462685208

Epoch: 5| Step: 3
Training loss: 2.0537827014923096
Validation loss: 2.182313572975897

Epoch: 5| Step: 4
Training loss: 2.158364772796631
Validation loss: 2.1900823295757337

Epoch: 5| Step: 5
Training loss: 2.4505839347839355
Validation loss: 2.199269089647519

Epoch: 5| Step: 6
Training loss: 2.49084210395813
Validation loss: 2.1946681955809235

Epoch: 5| Step: 7
Training loss: 2.6120963096618652
Validation loss: 2.2061240237246276

Epoch: 5| Step: 8
Training loss: 2.3430447578430176
Validation loss: 2.2174205113482732

Epoch: 5| Step: 9
Training loss: 2.6986870765686035
Validation loss: 2.2086070365803216

Epoch: 5| Step: 10
Training loss: 1.801164150238037
Validation loss: 2.208329249453801

Epoch: 196| Step: 0
Training loss: 2.3633275032043457
Validation loss: 2.1876567332975325

Epoch: 5| Step: 1
Training loss: 2.298640727996826
Validation loss: 2.178303772403348

Epoch: 5| Step: 2
Training loss: 2.095336437225342
Validation loss: 2.1706058658579344

Epoch: 5| Step: 3
Training loss: 1.9517676830291748
Validation loss: 2.1663103603547618

Epoch: 5| Step: 4
Training loss: 3.28729510307312
Validation loss: 2.1605621948037097

Epoch: 5| Step: 5
Training loss: 2.06199049949646
Validation loss: 2.157795360011439

Epoch: 5| Step: 6
Training loss: 2.593026638031006
Validation loss: 2.1515212084657405

Epoch: 5| Step: 7
Training loss: 2.531920909881592
Validation loss: 2.1554000505837063

Epoch: 5| Step: 8
Training loss: 2.8486313819885254
Validation loss: 2.155592546668104

Epoch: 5| Step: 9
Training loss: 1.9992587566375732
Validation loss: 2.1616133874462498

Epoch: 5| Step: 10
Training loss: 2.0452723503112793
Validation loss: 2.178662195000597

Epoch: 197| Step: 0
Training loss: 1.5702943801879883
Validation loss: 2.1870241011342695

Epoch: 5| Step: 1
Training loss: 2.166715145111084
Validation loss: 2.2018200838437645

Epoch: 5| Step: 2
Training loss: 3.512324571609497
Validation loss: 2.207825859387716

Epoch: 5| Step: 3
Training loss: 2.0455615520477295
Validation loss: 2.195557666081254

Epoch: 5| Step: 4
Training loss: 2.0586435794830322
Validation loss: 2.1912025097877748

Epoch: 5| Step: 5
Training loss: 2.954232692718506
Validation loss: 2.15866804892017

Epoch: 5| Step: 6
Training loss: 2.222687244415283
Validation loss: 2.1567615296251033

Epoch: 5| Step: 7
Training loss: 2.503997325897217
Validation loss: 2.1661317707389913

Epoch: 5| Step: 8
Training loss: 2.3382630348205566
Validation loss: 2.158274640319168

Epoch: 5| Step: 9
Training loss: 2.5296123027801514
Validation loss: 2.1588236080702914

Epoch: 5| Step: 10
Training loss: 2.3943850994110107
Validation loss: 2.1783641487039547

Epoch: 198| Step: 0
Training loss: 2.213261127471924
Validation loss: 2.170894499747984

Epoch: 5| Step: 1
Training loss: 2.50045108795166
Validation loss: 2.1697469757449244

Epoch: 5| Step: 2
Training loss: 2.619694948196411
Validation loss: 2.167542785726568

Epoch: 5| Step: 3
Training loss: 2.317147731781006
Validation loss: 2.156578484401908

Epoch: 5| Step: 4
Training loss: 1.9639841318130493
Validation loss: 2.1673886109423894

Epoch: 5| Step: 5
Training loss: 2.3354415893554688
Validation loss: 2.1585415922185427

Epoch: 5| Step: 6
Training loss: 2.365689992904663
Validation loss: 2.1676000523310837

Epoch: 5| Step: 7
Training loss: 2.584850788116455
Validation loss: 2.172689865994197

Epoch: 5| Step: 8
Training loss: 1.9768272638320923
Validation loss: 2.1727294434783277

Epoch: 5| Step: 9
Training loss: 2.7413172721862793
Validation loss: 2.1823816940348637

Epoch: 5| Step: 10
Training loss: 2.435154676437378
Validation loss: 2.180810523289506

Epoch: 199| Step: 0
Training loss: 2.53877329826355
Validation loss: 2.1776009631413284

Epoch: 5| Step: 1
Training loss: 2.765768527984619
Validation loss: 2.169011251900786

Epoch: 5| Step: 2
Training loss: 2.2104220390319824
Validation loss: 2.1634504564346804

Epoch: 5| Step: 3
Training loss: 2.260300397872925
Validation loss: 2.1705897469674387

Epoch: 5| Step: 4
Training loss: 2.4500389099121094
Validation loss: 2.1704421363851076

Epoch: 5| Step: 5
Training loss: 2.3916327953338623
Validation loss: 2.1742794154792704

Epoch: 5| Step: 6
Training loss: 2.3779137134552
Validation loss: 2.1791507967056765

Epoch: 5| Step: 7
Training loss: 2.2836670875549316
Validation loss: 2.1747665405273438

Epoch: 5| Step: 8
Training loss: 2.352609157562256
Validation loss: 2.1707403454729306

Epoch: 5| Step: 9
Training loss: 2.1495604515075684
Validation loss: 2.1771142072575067

Epoch: 5| Step: 10
Training loss: 2.141881227493286
Validation loss: 2.1758219221586823

Epoch: 200| Step: 0
Training loss: 2.117452621459961
Validation loss: 2.1847080928023144

Epoch: 5| Step: 1
Training loss: 2.464411497116089
Validation loss: 2.1806085673711633

Epoch: 5| Step: 2
Training loss: 1.9382057189941406
Validation loss: 2.1815256149538103

Epoch: 5| Step: 3
Training loss: 2.710662841796875
Validation loss: 2.1662775124272993

Epoch: 5| Step: 4
Training loss: 3.0963551998138428
Validation loss: 2.170688459950109

Epoch: 5| Step: 5
Training loss: 2.1074423789978027
Validation loss: 2.1691253826182377

Epoch: 5| Step: 6
Training loss: 2.109632968902588
Validation loss: 2.1683746896764284

Epoch: 5| Step: 7
Training loss: 2.6865601539611816
Validation loss: 2.1815956561796126

Epoch: 5| Step: 8
Training loss: 2.0371251106262207
Validation loss: 2.1835528958228325

Epoch: 5| Step: 9
Training loss: 1.762208342552185
Validation loss: 2.183105976350846

Epoch: 5| Step: 10
Training loss: 3.104466676712036
Validation loss: 2.1925182906530236

Epoch: 201| Step: 0
Training loss: 2.6214630603790283
Validation loss: 2.1906727898505425

Epoch: 5| Step: 1
Training loss: 2.0228817462921143
Validation loss: 2.1811960422864525

Epoch: 5| Step: 2
Training loss: 1.6729711294174194
Validation loss: 2.17427199117599

Epoch: 5| Step: 3
Training loss: 2.3130319118499756
Validation loss: 2.1628442836064163

Epoch: 5| Step: 4
Training loss: 2.4772324562072754
Validation loss: 2.1667969021745908

Epoch: 5| Step: 5
Training loss: 2.7018990516662598
Validation loss: 2.151708778514657

Epoch: 5| Step: 6
Training loss: 2.3573880195617676
Validation loss: 2.159818769783102

Epoch: 5| Step: 7
Training loss: 2.584364175796509
Validation loss: 2.1652001360411286

Epoch: 5| Step: 8
Training loss: 1.946813941001892
Validation loss: 2.1672024470503612

Epoch: 5| Step: 9
Training loss: 2.7746777534484863
Validation loss: 2.1673973221932687

Epoch: 5| Step: 10
Training loss: 2.5236639976501465
Validation loss: 2.1748473490438154

Epoch: 202| Step: 0
Training loss: 2.1484248638153076
Validation loss: 2.1666664404253804

Epoch: 5| Step: 1
Training loss: 2.7369275093078613
Validation loss: 2.1895608209794566

Epoch: 5| Step: 2
Training loss: 2.297628879547119
Validation loss: 2.1839888531674623

Epoch: 5| Step: 3
Training loss: 1.8190295696258545
Validation loss: 2.1887214619626283

Epoch: 5| Step: 4
Training loss: 2.5416646003723145
Validation loss: 2.1981063196735997

Epoch: 5| Step: 5
Training loss: 2.154262065887451
Validation loss: 2.1914551488814817

Epoch: 5| Step: 6
Training loss: 2.3291659355163574
Validation loss: 2.19008368830527

Epoch: 5| Step: 7
Training loss: 2.646172046661377
Validation loss: 2.1977079709370932

Epoch: 5| Step: 8
Training loss: 2.5266239643096924
Validation loss: 2.1829926070346626

Epoch: 5| Step: 9
Training loss: 2.829890251159668
Validation loss: 2.1990232031832457

Epoch: 5| Step: 10
Training loss: 1.7870277166366577
Validation loss: 2.1853418029764646

Epoch: 203| Step: 0
Training loss: 2.6549324989318848
Validation loss: 2.1810645159854682

Epoch: 5| Step: 1
Training loss: 2.61362361907959
Validation loss: 2.1594301513446275

Epoch: 5| Step: 2
Training loss: 2.5436882972717285
Validation loss: 2.1551264716732885

Epoch: 5| Step: 3
Training loss: 2.3005731105804443
Validation loss: 2.1536389602127897

Epoch: 5| Step: 4
Training loss: 1.7449398040771484
Validation loss: 2.1576836339889036

Epoch: 5| Step: 5
Training loss: 1.9992311000823975
Validation loss: 2.1627573300433416

Epoch: 5| Step: 6
Training loss: 3.0840461254119873
Validation loss: 2.1563180928589194

Epoch: 5| Step: 7
Training loss: 1.98403000831604
Validation loss: 2.175638360361899

Epoch: 5| Step: 8
Training loss: 1.7415218353271484
Validation loss: 2.170165797715546

Epoch: 5| Step: 9
Training loss: 2.748978853225708
Validation loss: 2.18315714405429

Epoch: 5| Step: 10
Training loss: 2.611908197402954
Validation loss: 2.1806539630377166

Epoch: 204| Step: 0
Training loss: 2.2698123455047607
Validation loss: 2.181185460859729

Epoch: 5| Step: 1
Training loss: 2.7253825664520264
Validation loss: 2.1774449015176423

Epoch: 5| Step: 2
Training loss: 2.5049285888671875
Validation loss: 2.183640031404393

Epoch: 5| Step: 3
Training loss: 2.3388686180114746
Validation loss: 2.17998912513897

Epoch: 5| Step: 4
Training loss: 2.3032867908477783
Validation loss: 2.1759694609590756

Epoch: 5| Step: 5
Training loss: 2.651332139968872
Validation loss: 2.1810081928007063

Epoch: 5| Step: 6
Training loss: 1.9521644115447998
Validation loss: 2.1595534111863826

Epoch: 5| Step: 7
Training loss: 2.5601658821105957
Validation loss: 2.1477472833407822

Epoch: 5| Step: 8
Training loss: 2.220635175704956
Validation loss: 2.150744104898104

Epoch: 5| Step: 9
Training loss: 1.9462871551513672
Validation loss: 2.164339309097618

Epoch: 5| Step: 10
Training loss: 2.5330398082733154
Validation loss: 2.159922224219127

Epoch: 205| Step: 0
Training loss: 3.0442638397216797
Validation loss: 2.178346631347492

Epoch: 5| Step: 1
Training loss: 2.4906973838806152
Validation loss: 2.182261479798184

Epoch: 5| Step: 2
Training loss: 2.1640093326568604
Validation loss: 2.1756664604269047

Epoch: 5| Step: 3
Training loss: 2.0122427940368652
Validation loss: 2.170256286539057

Epoch: 5| Step: 4
Training loss: 2.253455638885498
Validation loss: 2.1760405314865934

Epoch: 5| Step: 5
Training loss: 2.6094181537628174
Validation loss: 2.1861968912104124

Epoch: 5| Step: 6
Training loss: 2.4593541622161865
Validation loss: 2.187829250930458

Epoch: 5| Step: 7
Training loss: 1.5622069835662842
Validation loss: 2.181784132475494

Epoch: 5| Step: 8
Training loss: 2.7155239582061768
Validation loss: 2.1668363899312992

Epoch: 5| Step: 9
Training loss: 2.739633798599243
Validation loss: 2.1701017195178616

Epoch: 5| Step: 10
Training loss: 2.0986902713775635
Validation loss: 2.1631275735875612

Epoch: 206| Step: 0
Training loss: 2.417041778564453
Validation loss: 2.1716796044380433

Epoch: 5| Step: 1
Training loss: 1.5972914695739746
Validation loss: 2.1888530510728077

Epoch: 5| Step: 2
Training loss: 2.558208465576172
Validation loss: 2.194075284465667

Epoch: 5| Step: 3
Training loss: 2.6834824085235596
Validation loss: 2.2076279450488347

Epoch: 5| Step: 4
Training loss: 2.893578290939331
Validation loss: 2.183286723270211

Epoch: 5| Step: 5
Training loss: 1.9422744512557983
Validation loss: 2.188813392833997

Epoch: 5| Step: 6
Training loss: 2.615142822265625
Validation loss: 2.163657775489233

Epoch: 5| Step: 7
Training loss: 2.331698179244995
Validation loss: 2.152381738026937

Epoch: 5| Step: 8
Training loss: 2.2331748008728027
Validation loss: 2.1583211396330144

Epoch: 5| Step: 9
Training loss: 2.363111734390259
Validation loss: 2.167011682705213

Epoch: 5| Step: 10
Training loss: 2.563194990158081
Validation loss: 2.1616881957618137

Epoch: 207| Step: 0
Training loss: 1.8599205017089844
Validation loss: 2.1731600530685915

Epoch: 5| Step: 1
Training loss: 2.600116491317749
Validation loss: 2.1773543819304435

Epoch: 5| Step: 2
Training loss: 2.479262590408325
Validation loss: 2.1951025737229215

Epoch: 5| Step: 3
Training loss: 2.6381781101226807
Validation loss: 2.193462856354252

Epoch: 5| Step: 4
Training loss: 2.258845329284668
Validation loss: 2.185023069381714

Epoch: 5| Step: 5
Training loss: 2.5292136669158936
Validation loss: 2.1890936820737776

Epoch: 5| Step: 6
Training loss: 2.8616783618927
Validation loss: 2.183495536927254

Epoch: 5| Step: 7
Training loss: 2.0115222930908203
Validation loss: 2.1942369040622505

Epoch: 5| Step: 8
Training loss: 2.2845513820648193
Validation loss: 2.1782106840482323

Epoch: 5| Step: 9
Training loss: 2.3515470027923584
Validation loss: 2.183358400098739

Epoch: 5| Step: 10
Training loss: 1.982134461402893
Validation loss: 2.1581274642739245

Epoch: 208| Step: 0
Training loss: 1.9300113916397095
Validation loss: 2.158384406438438

Epoch: 5| Step: 1
Training loss: 2.0825812816619873
Validation loss: 2.157532386882331

Epoch: 5| Step: 2
Training loss: 2.3855977058410645
Validation loss: 2.1483051212885047

Epoch: 5| Step: 3
Training loss: 2.153170347213745
Validation loss: 2.139383154530679

Epoch: 5| Step: 4
Training loss: 2.3128409385681152
Validation loss: 2.1493109733827653

Epoch: 5| Step: 5
Training loss: 2.839630126953125
Validation loss: 2.1522947729274793

Epoch: 5| Step: 6
Training loss: 2.8179659843444824
Validation loss: 2.150868959324334

Epoch: 5| Step: 7
Training loss: 2.5368666648864746
Validation loss: 2.144263582844888

Epoch: 5| Step: 8
Training loss: 1.935442328453064
Validation loss: 2.1507133514650407

Epoch: 5| Step: 9
Training loss: 2.71545672416687
Validation loss: 2.154925597611294

Epoch: 5| Step: 10
Training loss: 2.0564701557159424
Validation loss: 2.1629658283725863

Epoch: 209| Step: 0
Training loss: 2.2261643409729004
Validation loss: 2.1709307444992887

Epoch: 5| Step: 1
Training loss: 1.8835477828979492
Validation loss: 2.1718749371908044

Epoch: 5| Step: 2
Training loss: 2.4250540733337402
Validation loss: 2.179397685553438

Epoch: 5| Step: 3
Training loss: 2.3160786628723145
Validation loss: 2.1815790848065446

Epoch: 5| Step: 4
Training loss: 2.4847872257232666
Validation loss: 2.1677896925198135

Epoch: 5| Step: 5
Training loss: 2.3723220825195312
Validation loss: 2.174181056279008

Epoch: 5| Step: 6
Training loss: 2.6581034660339355
Validation loss: 2.1885933414582284

Epoch: 5| Step: 7
Training loss: 2.2980384826660156
Validation loss: 2.1770012250510593

Epoch: 5| Step: 8
Training loss: 1.7785303592681885
Validation loss: 2.186791754538013

Epoch: 5| Step: 9
Training loss: 2.7127299308776855
Validation loss: 2.179848099267611

Epoch: 5| Step: 10
Training loss: 2.580442190170288
Validation loss: 2.1758930196044264

Epoch: 210| Step: 0
Training loss: 2.953221082687378
Validation loss: 2.1673272079037083

Epoch: 5| Step: 1
Training loss: 1.4118953943252563
Validation loss: 2.1814272813899542

Epoch: 5| Step: 2
Training loss: 1.5956876277923584
Validation loss: 2.173284069184334

Epoch: 5| Step: 3
Training loss: 2.689941883087158
Validation loss: 2.188352084928943

Epoch: 5| Step: 4
Training loss: 2.6969761848449707
Validation loss: 2.161268513689759

Epoch: 5| Step: 5
Training loss: 3.007582902908325
Validation loss: 2.169243217796408

Epoch: 5| Step: 6
Training loss: 2.093015193939209
Validation loss: 2.162869030429471

Epoch: 5| Step: 7
Training loss: 1.755314588546753
Validation loss: 2.149457002198824

Epoch: 5| Step: 8
Training loss: 1.9175420999526978
Validation loss: 2.1573420878379577

Epoch: 5| Step: 9
Training loss: 2.6338789463043213
Validation loss: 2.1690328557004213

Epoch: 5| Step: 10
Training loss: 3.0169498920440674
Validation loss: 2.1662761011431293

Epoch: 211| Step: 0
Training loss: 2.2166900634765625
Validation loss: 2.161533601822392

Epoch: 5| Step: 1
Training loss: 2.143172025680542
Validation loss: 2.148564712975615

Epoch: 5| Step: 2
Training loss: 2.5579147338867188
Validation loss: 2.1475722071945027

Epoch: 5| Step: 3
Training loss: 1.9601894617080688
Validation loss: 2.145506235861009

Epoch: 5| Step: 4
Training loss: 2.76106333732605
Validation loss: 2.1514034245603826

Epoch: 5| Step: 5
Training loss: 2.0909667015075684
Validation loss: 2.1346127166542956

Epoch: 5| Step: 6
Training loss: 2.158313512802124
Validation loss: 2.150236721961729

Epoch: 5| Step: 7
Training loss: 1.7997995615005493
Validation loss: 2.1416065590355986

Epoch: 5| Step: 8
Training loss: 2.8722352981567383
Validation loss: 2.1553359057313655

Epoch: 5| Step: 9
Training loss: 2.917177677154541
Validation loss: 2.1457306851622877

Epoch: 5| Step: 10
Training loss: 2.3004684448242188
Validation loss: 2.146191555966613

Epoch: 212| Step: 0
Training loss: 2.410034656524658
Validation loss: 2.158760909111269

Epoch: 5| Step: 1
Training loss: 1.8259423971176147
Validation loss: 2.1584703870998916

Epoch: 5| Step: 2
Training loss: 2.3242344856262207
Validation loss: 2.1596104867996706

Epoch: 5| Step: 3
Training loss: 2.460306167602539
Validation loss: 2.1832741101582847

Epoch: 5| Step: 4
Training loss: 2.3413500785827637
Validation loss: 2.1870640195826048

Epoch: 5| Step: 5
Training loss: 2.81247615814209
Validation loss: 2.187048860775527

Epoch: 5| Step: 6
Training loss: 2.5982751846313477
Validation loss: 2.1829522604583413

Epoch: 5| Step: 7
Training loss: 1.4247000217437744
Validation loss: 2.172886076793876

Epoch: 5| Step: 8
Training loss: 2.299229860305786
Validation loss: 2.1643340715798

Epoch: 5| Step: 9
Training loss: 2.4272334575653076
Validation loss: 2.153313548334183

Epoch: 5| Step: 10
Training loss: 2.8925650119781494
Validation loss: 2.1449588037306264

Epoch: 213| Step: 0
Training loss: 2.6094186305999756
Validation loss: 2.149103900437714

Epoch: 5| Step: 1
Training loss: 2.2903449535369873
Validation loss: 2.1560492438654744

Epoch: 5| Step: 2
Training loss: 3.0815975666046143
Validation loss: 2.1414111198917514

Epoch: 5| Step: 3
Training loss: 2.3896472454071045
Validation loss: 2.1380501460003596

Epoch: 5| Step: 4
Training loss: 2.172640085220337
Validation loss: 2.1351516938978627

Epoch: 5| Step: 5
Training loss: 2.470702886581421
Validation loss: 2.1240986342071206

Epoch: 5| Step: 6
Training loss: 1.5614879131317139
Validation loss: 2.1228008936810236

Epoch: 5| Step: 7
Training loss: 2.233976125717163
Validation loss: 2.1235871930276193

Epoch: 5| Step: 8
Training loss: 2.546140193939209
Validation loss: 2.1236224430863575

Epoch: 5| Step: 9
Training loss: 1.7404201030731201
Validation loss: 2.1364960721744004

Epoch: 5| Step: 10
Training loss: 2.8890328407287598
Validation loss: 2.143869856352447

Epoch: 214| Step: 0
Training loss: 2.1936776638031006
Validation loss: 2.1423712327916133

Epoch: 5| Step: 1
Training loss: 2.463714122772217
Validation loss: 2.154711943800731

Epoch: 5| Step: 2
Training loss: 1.8825023174285889
Validation loss: 2.1794365759818786

Epoch: 5| Step: 3
Training loss: 2.7982516288757324
Validation loss: 2.174537645873203

Epoch: 5| Step: 4
Training loss: 2.4046502113342285
Validation loss: 2.1640334103697088

Epoch: 5| Step: 5
Training loss: 2.1650748252868652
Validation loss: 2.161342115812404

Epoch: 5| Step: 6
Training loss: 2.386336088180542
Validation loss: 2.159054246000064

Epoch: 5| Step: 7
Training loss: 2.550863742828369
Validation loss: 2.1620710665179836

Epoch: 5| Step: 8
Training loss: 2.4873194694519043
Validation loss: 2.1488908388281382

Epoch: 5| Step: 9
Training loss: 2.5109610557556152
Validation loss: 2.1513340691084504

Epoch: 5| Step: 10
Training loss: 1.8708583116531372
Validation loss: 2.1415141141542824

Epoch: 215| Step: 0
Training loss: 2.8572402000427246
Validation loss: 2.1507590009320166

Epoch: 5| Step: 1
Training loss: 2.2703073024749756
Validation loss: 2.1468128965746973

Epoch: 5| Step: 2
Training loss: 2.2273480892181396
Validation loss: 2.157565560392154

Epoch: 5| Step: 3
Training loss: 2.291934013366699
Validation loss: 2.161155152064498

Epoch: 5| Step: 4
Training loss: 2.6453704833984375
Validation loss: 2.1611345942302416

Epoch: 5| Step: 5
Training loss: 2.2217044830322266
Validation loss: 2.1703385742761756

Epoch: 5| Step: 6
Training loss: 2.1746630668640137
Validation loss: 2.1701970151675645

Epoch: 5| Step: 7
Training loss: 2.263611316680908
Validation loss: 2.1821396991770756

Epoch: 5| Step: 8
Training loss: 2.204268217086792
Validation loss: 2.167850507202969

Epoch: 5| Step: 9
Training loss: 2.382443904876709
Validation loss: 2.164349535460113

Epoch: 5| Step: 10
Training loss: 2.0595173835754395
Validation loss: 2.1652955060364096

Epoch: 216| Step: 0
Training loss: 2.3093647956848145
Validation loss: 2.1558911787566317

Epoch: 5| Step: 1
Training loss: 2.496570110321045
Validation loss: 2.156310760846702

Epoch: 5| Step: 2
Training loss: 2.4254207611083984
Validation loss: 2.146092040564424

Epoch: 5| Step: 3
Training loss: 2.5566134452819824
Validation loss: 2.1497514478621946

Epoch: 5| Step: 4
Training loss: 2.0333170890808105
Validation loss: 2.1369078325968918

Epoch: 5| Step: 5
Training loss: 2.3017029762268066
Validation loss: 2.156178794881349

Epoch: 5| Step: 6
Training loss: 2.148026704788208
Validation loss: 2.152515665177376

Epoch: 5| Step: 7
Training loss: 2.0981733798980713
Validation loss: 2.145840708927442

Epoch: 5| Step: 8
Training loss: 2.8472342491149902
Validation loss: 2.165887914678102

Epoch: 5| Step: 9
Training loss: 2.329732656478882
Validation loss: 2.1663406702779953

Epoch: 5| Step: 10
Training loss: 2.24685001373291
Validation loss: 2.1522948703458233

Epoch: 217| Step: 0
Training loss: 2.416999101638794
Validation loss: 2.159058973353396

Epoch: 5| Step: 1
Training loss: 2.1884169578552246
Validation loss: 2.153785403056811

Epoch: 5| Step: 2
Training loss: 2.6376278400421143
Validation loss: 2.1711498768098894

Epoch: 5| Step: 3
Training loss: 2.2746052742004395
Validation loss: 2.1644436723442486

Epoch: 5| Step: 4
Training loss: 2.155167579650879
Validation loss: 2.152736915055142

Epoch: 5| Step: 5
Training loss: 2.2679460048675537
Validation loss: 2.168192514809229

Epoch: 5| Step: 6
Training loss: 1.6896257400512695
Validation loss: 2.1744512152928177

Epoch: 5| Step: 7
Training loss: 2.737668037414551
Validation loss: 2.1796021256395566

Epoch: 5| Step: 8
Training loss: 2.145526885986328
Validation loss: 2.16772884450933

Epoch: 5| Step: 9
Training loss: 2.34773325920105
Validation loss: 2.1600477054554927

Epoch: 5| Step: 10
Training loss: 2.794926643371582
Validation loss: 2.1588083518448697

Epoch: 218| Step: 0
Training loss: 2.782268762588501
Validation loss: 2.161240385424706

Epoch: 5| Step: 1
Training loss: 2.4490854740142822
Validation loss: 2.16687172971746

Epoch: 5| Step: 2
Training loss: 2.2069344520568848
Validation loss: 2.1630163872113792

Epoch: 5| Step: 3
Training loss: 2.5069518089294434
Validation loss: 2.160459856833181

Epoch: 5| Step: 4
Training loss: 2.4179818630218506
Validation loss: 2.159419862172937

Epoch: 5| Step: 5
Training loss: 2.6056110858917236
Validation loss: 2.1404445632811515

Epoch: 5| Step: 6
Training loss: 1.8655484914779663
Validation loss: 2.1419462106561147

Epoch: 5| Step: 7
Training loss: 2.030151844024658
Validation loss: 2.1285924168043238

Epoch: 5| Step: 8
Training loss: 1.9226810932159424
Validation loss: 2.1430852797723587

Epoch: 5| Step: 9
Training loss: 2.3966121673583984
Validation loss: 2.146520781260665

Epoch: 5| Step: 10
Training loss: 2.3865439891815186
Validation loss: 2.1615908120268132

Epoch: 219| Step: 0
Training loss: 2.725963592529297
Validation loss: 2.1613693839760235

Epoch: 5| Step: 1
Training loss: 2.3908321857452393
Validation loss: 2.175326111496136

Epoch: 5| Step: 2
Training loss: 2.3525445461273193
Validation loss: 2.173072717523062

Epoch: 5| Step: 3
Training loss: 2.2237472534179688
Validation loss: 2.191710692580028

Epoch: 5| Step: 4
Training loss: 2.663165807723999
Validation loss: 2.185041476321477

Epoch: 5| Step: 5
Training loss: 1.6460157632827759
Validation loss: 2.191044574142784

Epoch: 5| Step: 6
Training loss: 2.2776317596435547
Validation loss: 2.1849128815435592

Epoch: 5| Step: 7
Training loss: 2.320815086364746
Validation loss: 2.1838213243792133

Epoch: 5| Step: 8
Training loss: 2.4846482276916504
Validation loss: 2.155951210247573

Epoch: 5| Step: 9
Training loss: 2.3533859252929688
Validation loss: 2.1636794036434543

Epoch: 5| Step: 10
Training loss: 2.3090293407440186
Validation loss: 2.145565809742097

Epoch: 220| Step: 0
Training loss: 2.9979796409606934
Validation loss: 2.1436835412056214

Epoch: 5| Step: 1
Training loss: 2.0820956230163574
Validation loss: 2.1370626880276586

Epoch: 5| Step: 2
Training loss: 3.0477709770202637
Validation loss: 2.153204074469946

Epoch: 5| Step: 3
Training loss: 2.6266989707946777
Validation loss: 2.1322016639094197

Epoch: 5| Step: 4
Training loss: 2.5000176429748535
Validation loss: 2.1170682394376366

Epoch: 5| Step: 5
Training loss: 2.1339080333709717
Validation loss: 2.1097868873227026

Epoch: 5| Step: 6
Training loss: 2.079545497894287
Validation loss: 2.1214045914270545

Epoch: 5| Step: 7
Training loss: 1.8117929697036743
Validation loss: 2.1246157576960902

Epoch: 5| Step: 8
Training loss: 2.1857476234436035
Validation loss: 2.1116740677946355

Epoch: 5| Step: 9
Training loss: 2.028485059738159
Validation loss: 2.126121413323187

Epoch: 5| Step: 10
Training loss: 2.3604931831359863
Validation loss: 2.1380744505954046

Epoch: 221| Step: 0
Training loss: 2.3323779106140137
Validation loss: 2.1372229309492212

Epoch: 5| Step: 1
Training loss: 2.538053035736084
Validation loss: 2.1378892442231536

Epoch: 5| Step: 2
Training loss: 2.4285645484924316
Validation loss: 2.1422855777125203

Epoch: 5| Step: 3
Training loss: 2.679412364959717
Validation loss: 2.156710822095153

Epoch: 5| Step: 4
Training loss: 1.4222395420074463
Validation loss: 2.165099302927653

Epoch: 5| Step: 5
Training loss: 2.774338483810425
Validation loss: 2.1692024636012253

Epoch: 5| Step: 6
Training loss: 2.571582555770874
Validation loss: 2.2003371869364092

Epoch: 5| Step: 7
Training loss: 2.1986422538757324
Validation loss: 2.177404131940616

Epoch: 5| Step: 8
Training loss: 2.448183536529541
Validation loss: 2.166564187695903

Epoch: 5| Step: 9
Training loss: 1.8481395244598389
Validation loss: 2.1575004849382626

Epoch: 5| Step: 10
Training loss: 2.409039258956909
Validation loss: 2.1523630285775788

Epoch: 222| Step: 0
Training loss: 1.8117910623550415
Validation loss: 2.1584252093427923

Epoch: 5| Step: 1
Training loss: 2.068053722381592
Validation loss: 2.1356072220751035

Epoch: 5| Step: 2
Training loss: 2.2823257446289062
Validation loss: 2.1321300178445797

Epoch: 5| Step: 3
Training loss: 2.8856711387634277
Validation loss: 2.1390853261434906

Epoch: 5| Step: 4
Training loss: 2.5778679847717285
Validation loss: 2.1329096953074136

Epoch: 5| Step: 5
Training loss: 3.061842679977417
Validation loss: 2.1265485235439834

Epoch: 5| Step: 6
Training loss: 2.518878221511841
Validation loss: 2.137384815882611

Epoch: 5| Step: 7
Training loss: 2.2917799949645996
Validation loss: 2.1286616069014355

Epoch: 5| Step: 8
Training loss: 2.037431240081787
Validation loss: 2.1356158384712796

Epoch: 5| Step: 9
Training loss: 2.272538423538208
Validation loss: 2.1322540313966813

Epoch: 5| Step: 10
Training loss: 1.656616449356079
Validation loss: 2.138919279139529

Epoch: 223| Step: 0
Training loss: 2.6332967281341553
Validation loss: 2.1293407947786394

Epoch: 5| Step: 1
Training loss: 2.4073643684387207
Validation loss: 2.1431840004459506

Epoch: 5| Step: 2
Training loss: 2.1364665031433105
Validation loss: 2.1483389728812763

Epoch: 5| Step: 3
Training loss: 1.8131099939346313
Validation loss: 2.1614184943578576

Epoch: 5| Step: 4
Training loss: 1.770288109779358
Validation loss: 2.1741172113726215

Epoch: 5| Step: 5
Training loss: 1.4556169509887695
Validation loss: 2.175248799785491

Epoch: 5| Step: 6
Training loss: 2.9983792304992676
Validation loss: 2.1858799201185986

Epoch: 5| Step: 7
Training loss: 2.7093029022216797
Validation loss: 2.173674125825205

Epoch: 5| Step: 8
Training loss: 2.9753499031066895
Validation loss: 2.156013220869085

Epoch: 5| Step: 9
Training loss: 2.680081605911255
Validation loss: 2.160259210935203

Epoch: 5| Step: 10
Training loss: 2.02600359916687
Validation loss: 2.1385361917557253

Epoch: 224| Step: 0
Training loss: 2.171785593032837
Validation loss: 2.1324256440644622

Epoch: 5| Step: 1
Training loss: 1.5946967601776123
Validation loss: 2.133532743300161

Epoch: 5| Step: 2
Training loss: 2.7657077312469482
Validation loss: 2.140002336553348

Epoch: 5| Step: 3
Training loss: 1.7871198654174805
Validation loss: 2.144442140415151

Epoch: 5| Step: 4
Training loss: 2.4371724128723145
Validation loss: 2.1440494983426985

Epoch: 5| Step: 5
Training loss: 2.1857032775878906
Validation loss: 2.1473920781125306

Epoch: 5| Step: 6
Training loss: 2.560764789581299
Validation loss: 2.148458488525883

Epoch: 5| Step: 7
Training loss: 1.9452342987060547
Validation loss: 2.1418540323934248

Epoch: 5| Step: 8
Training loss: 2.846287965774536
Validation loss: 2.1514018017758607

Epoch: 5| Step: 9
Training loss: 2.669940233230591
Validation loss: 2.13754705972569

Epoch: 5| Step: 10
Training loss: 2.733997344970703
Validation loss: 2.1305196926157963

Epoch: 225| Step: 0
Training loss: 1.868212103843689
Validation loss: 2.141874381290969

Epoch: 5| Step: 1
Training loss: 1.5632511377334595
Validation loss: 2.1405674231949674

Epoch: 5| Step: 2
Training loss: 3.0395469665527344
Validation loss: 2.130159088360366

Epoch: 5| Step: 3
Training loss: 2.506847620010376
Validation loss: 2.12251680128036

Epoch: 5| Step: 4
Training loss: 2.4584929943084717
Validation loss: 2.132539746581867

Epoch: 5| Step: 5
Training loss: 2.8769195079803467
Validation loss: 2.1304215205613004

Epoch: 5| Step: 6
Training loss: 2.4225494861602783
Validation loss: 2.1313574070571573

Epoch: 5| Step: 7
Training loss: 2.0072057247161865
Validation loss: 2.1299168858476865

Epoch: 5| Step: 8
Training loss: 1.7855390310287476
Validation loss: 2.1322259159498316

Epoch: 5| Step: 9
Training loss: 2.729854106903076
Validation loss: 2.130148651779339

Epoch: 5| Step: 10
Training loss: 2.265045642852783
Validation loss: 2.126406215852307

Epoch: 226| Step: 0
Training loss: 2.1190199851989746
Validation loss: 2.143660347948792

Epoch: 5| Step: 1
Training loss: 2.376950263977051
Validation loss: 2.1593535843715874

Epoch: 5| Step: 2
Training loss: 2.253530740737915
Validation loss: 2.14851798037047

Epoch: 5| Step: 3
Training loss: 2.698864698410034
Validation loss: 2.1585003124770297

Epoch: 5| Step: 4
Training loss: 2.046104907989502
Validation loss: 2.1570043410024335

Epoch: 5| Step: 5
Training loss: 2.369429111480713
Validation loss: 2.156881554152376

Epoch: 5| Step: 6
Training loss: 2.010478973388672
Validation loss: 2.1533739464257353

Epoch: 5| Step: 7
Training loss: 1.73282790184021
Validation loss: 2.1511484474264164

Epoch: 5| Step: 8
Training loss: 2.4807353019714355
Validation loss: 2.1386929301805395

Epoch: 5| Step: 9
Training loss: 2.535512924194336
Validation loss: 2.1569774958395187

Epoch: 5| Step: 10
Training loss: 3.014686346054077
Validation loss: 2.158925897331648

Epoch: 227| Step: 0
Training loss: 2.537968873977661
Validation loss: 2.1478834408585743

Epoch: 5| Step: 1
Training loss: 2.5810387134552
Validation loss: 2.1397582792466685

Epoch: 5| Step: 2
Training loss: 1.8244682550430298
Validation loss: 2.1565832630280526

Epoch: 5| Step: 3
Training loss: 2.34411883354187
Validation loss: 2.1517004223280054

Epoch: 5| Step: 4
Training loss: 1.7960726022720337
Validation loss: 2.1371856633053032

Epoch: 5| Step: 5
Training loss: 2.732207775115967
Validation loss: 2.1484865962818103

Epoch: 5| Step: 6
Training loss: 2.2954280376434326
Validation loss: 2.1334455038911555

Epoch: 5| Step: 7
Training loss: 2.438352108001709
Validation loss: 2.1393816471099854

Epoch: 5| Step: 8
Training loss: 2.2642581462860107
Validation loss: 2.1355409750374417

Epoch: 5| Step: 9
Training loss: 2.434197187423706
Validation loss: 2.1400062755871843

Epoch: 5| Step: 10
Training loss: 2.0348970890045166
Validation loss: 2.1386113858992055

Epoch: 228| Step: 0
Training loss: 1.9938840866088867
Validation loss: 2.1405842047865673

Epoch: 5| Step: 1
Training loss: 2.5020127296447754
Validation loss: 2.1569005648295083

Epoch: 5| Step: 2
Training loss: 2.2126545906066895
Validation loss: 2.141887419967241

Epoch: 5| Step: 3
Training loss: 3.075382709503174
Validation loss: 2.142595015546327

Epoch: 5| Step: 4
Training loss: 2.7512478828430176
Validation loss: 2.153335496943484

Epoch: 5| Step: 5
Training loss: 2.5105464458465576
Validation loss: 2.138130462297829

Epoch: 5| Step: 6
Training loss: 2.2082722187042236
Validation loss: 2.1392647784243346

Epoch: 5| Step: 7
Training loss: 1.9467308521270752
Validation loss: 2.130713980684998

Epoch: 5| Step: 8
Training loss: 2.27394437789917
Validation loss: 2.146633750648909

Epoch: 5| Step: 9
Training loss: 1.8477909564971924
Validation loss: 2.130383458188785

Epoch: 5| Step: 10
Training loss: 1.9304264783859253
Validation loss: 2.1657021840413413

Epoch: 229| Step: 0
Training loss: 2.843207836151123
Validation loss: 2.1567079956813524

Epoch: 5| Step: 1
Training loss: 2.4995505809783936
Validation loss: 2.1567171658239057

Epoch: 5| Step: 2
Training loss: 2.5041403770446777
Validation loss: 2.163125773911835

Epoch: 5| Step: 3
Training loss: 2.1144299507141113
Validation loss: 2.144226058836906

Epoch: 5| Step: 4
Training loss: 2.188481092453003
Validation loss: 2.1525223306430283

Epoch: 5| Step: 5
Training loss: 2.5067875385284424
Validation loss: 2.1449722820712673

Epoch: 5| Step: 6
Training loss: 2.6043264865875244
Validation loss: 2.1295581838136077

Epoch: 5| Step: 7
Training loss: 2.0540683269500732
Validation loss: 2.124533107203822

Epoch: 5| Step: 8
Training loss: 2.4903666973114014
Validation loss: 2.1276371120124735

Epoch: 5| Step: 9
Training loss: 2.1355137825012207
Validation loss: 2.114197079853345

Epoch: 5| Step: 10
Training loss: 1.439972996711731
Validation loss: 2.107806827432366

Epoch: 230| Step: 0
Training loss: 2.5639796257019043
Validation loss: 2.126834898866633

Epoch: 5| Step: 1
Training loss: 2.868333578109741
Validation loss: 2.125356997213056

Epoch: 5| Step: 2
Training loss: 1.8005800247192383
Validation loss: 2.126980589282128

Epoch: 5| Step: 3
Training loss: 2.5593788623809814
Validation loss: 2.136915786291963

Epoch: 5| Step: 4
Training loss: 2.299931049346924
Validation loss: 2.1346429394137476

Epoch: 5| Step: 5
Training loss: 2.396116256713867
Validation loss: 2.1438894797396917

Epoch: 5| Step: 6
Training loss: 2.391923189163208
Validation loss: 2.135395880668394

Epoch: 5| Step: 7
Training loss: 2.1879379749298096
Validation loss: 2.150907293442757

Epoch: 5| Step: 8
Training loss: 1.9742279052734375
Validation loss: 2.157933847878569

Epoch: 5| Step: 9
Training loss: 2.1779417991638184
Validation loss: 2.1693137384230092

Epoch: 5| Step: 10
Training loss: 2.0989997386932373
Validation loss: 2.165992699643617

Epoch: 231| Step: 0
Training loss: 2.297102928161621
Validation loss: 2.1887358080956245

Epoch: 5| Step: 1
Training loss: 2.2649621963500977
Validation loss: 2.1797471841176352

Epoch: 5| Step: 2
Training loss: 2.8087875843048096
Validation loss: 2.1726583011688723

Epoch: 5| Step: 3
Training loss: 2.7257752418518066
Validation loss: 2.1381111811566096

Epoch: 5| Step: 4
Training loss: 2.2465274333953857
Validation loss: 2.152739478695777

Epoch: 5| Step: 5
Training loss: 2.3217861652374268
Validation loss: 2.1490427191539476

Epoch: 5| Step: 6
Training loss: 2.257567882537842
Validation loss: 2.1381727969774635

Epoch: 5| Step: 7
Training loss: 2.1193976402282715
Validation loss: 2.1335179523755143

Epoch: 5| Step: 8
Training loss: 1.8993031978607178
Validation loss: 2.141648061813847

Epoch: 5| Step: 9
Training loss: 2.1342711448669434
Validation loss: 2.137687329323061

Epoch: 5| Step: 10
Training loss: 2.3668715953826904
Validation loss: 2.140124426093153

Epoch: 232| Step: 0
Training loss: 2.2739901542663574
Validation loss: 2.1345291265877346

Epoch: 5| Step: 1
Training loss: 2.340799331665039
Validation loss: 2.141346682784378

Epoch: 5| Step: 2
Training loss: 2.1919121742248535
Validation loss: 2.1410046597962737

Epoch: 5| Step: 3
Training loss: 2.1154532432556152
Validation loss: 2.1333089797727522

Epoch: 5| Step: 4
Training loss: 2.3662655353546143
Validation loss: 2.127734852093522

Epoch: 5| Step: 5
Training loss: 2.9248785972595215
Validation loss: 2.1345999676694154

Epoch: 5| Step: 6
Training loss: 1.9539556503295898
Validation loss: 2.1378226498121857

Epoch: 5| Step: 7
Training loss: 1.9952490329742432
Validation loss: 2.1186473241416355

Epoch: 5| Step: 8
Training loss: 2.1843762397766113
Validation loss: 2.108042227324619

Epoch: 5| Step: 9
Training loss: 2.73175048828125
Validation loss: 2.1151154246381534

Epoch: 5| Step: 10
Training loss: 2.356431007385254
Validation loss: 2.118229178972142

Epoch: 233| Step: 0
Training loss: 2.665602922439575
Validation loss: 2.1177400004479194

Epoch: 5| Step: 1
Training loss: 2.688701629638672
Validation loss: 2.117016612842519

Epoch: 5| Step: 2
Training loss: 1.633636713027954
Validation loss: 2.118478854497274

Epoch: 5| Step: 3
Training loss: 3.0779480934143066
Validation loss: 2.1324741173815984

Epoch: 5| Step: 4
Training loss: 1.673659086227417
Validation loss: 2.140614216045667

Epoch: 5| Step: 5
Training loss: 2.279182195663452
Validation loss: 2.1712610478042276

Epoch: 5| Step: 6
Training loss: 2.9270706176757812
Validation loss: 2.153003454208374

Epoch: 5| Step: 7
Training loss: 2.2250313758850098
Validation loss: 2.1700336676771923

Epoch: 5| Step: 8
Training loss: 2.1391966342926025
Validation loss: 2.1589217724338656

Epoch: 5| Step: 9
Training loss: 2.5678067207336426
Validation loss: 2.167451206073966

Epoch: 5| Step: 10
Training loss: 1.569258689880371
Validation loss: 2.148417390802855

Epoch: 234| Step: 0
Training loss: 2.6991469860076904
Validation loss: 2.1616929013242006

Epoch: 5| Step: 1
Training loss: 2.3095335960388184
Validation loss: 2.162482702603904

Epoch: 5| Step: 2
Training loss: 2.8562567234039307
Validation loss: 2.167615923830258

Epoch: 5| Step: 3
Training loss: 2.964284896850586
Validation loss: 2.1736509774320867

Epoch: 5| Step: 4
Training loss: 2.0414986610412598
Validation loss: 2.164074021001016

Epoch: 5| Step: 5
Training loss: 1.6001132726669312
Validation loss: 2.16702328958819

Epoch: 5| Step: 6
Training loss: 2.2603676319122314
Validation loss: 2.1540929527692896

Epoch: 5| Step: 7
Training loss: 1.9950138330459595
Validation loss: 2.130442229650354

Epoch: 5| Step: 8
Training loss: 2.4626946449279785
Validation loss: 2.131793070864934

Epoch: 5| Step: 9
Training loss: 1.9656641483306885
Validation loss: 2.1191301499643633

Epoch: 5| Step: 10
Training loss: 2.422956705093384
Validation loss: 2.1189869270529798

Epoch: 235| Step: 0
Training loss: 2.3329670429229736
Validation loss: 2.1161475643034904

Epoch: 5| Step: 1
Training loss: 2.9031455516815186
Validation loss: 2.1213966723411315

Epoch: 5| Step: 2
Training loss: 1.9042062759399414
Validation loss: 2.121501604715983

Epoch: 5| Step: 3
Training loss: 2.4173524379730225
Validation loss: 2.122036895444316

Epoch: 5| Step: 4
Training loss: 1.8297388553619385
Validation loss: 2.134638701715777

Epoch: 5| Step: 5
Training loss: 2.298544406890869
Validation loss: 2.1638929561902116

Epoch: 5| Step: 6
Training loss: 2.4331610202789307
Validation loss: 2.171217703050183

Epoch: 5| Step: 7
Training loss: 2.2583439350128174
Validation loss: 2.1689552581438454

Epoch: 5| Step: 8
Training loss: 2.8107728958129883
Validation loss: 2.1607353712922786

Epoch: 5| Step: 9
Training loss: 2.3849878311157227
Validation loss: 2.1314557906120055

Epoch: 5| Step: 10
Training loss: 1.9903111457824707
Validation loss: 2.1067434382695023

Epoch: 236| Step: 0
Training loss: 2.7125020027160645
Validation loss: 2.1101602123629664

Epoch: 5| Step: 1
Training loss: 2.5343222618103027
Validation loss: 2.1324899760625695

Epoch: 5| Step: 2
Training loss: 2.7631916999816895
Validation loss: 2.1390285594488985

Epoch: 5| Step: 3
Training loss: 1.9743598699569702
Validation loss: 2.135178142978299

Epoch: 5| Step: 4
Training loss: 1.9312080144882202
Validation loss: 2.164987115449803

Epoch: 5| Step: 5
Training loss: 2.298731565475464
Validation loss: 2.1619615888082855

Epoch: 5| Step: 6
Training loss: 2.578324794769287
Validation loss: 2.148652624058467

Epoch: 5| Step: 7
Training loss: 1.8449571132659912
Validation loss: 2.1536551239669963

Epoch: 5| Step: 8
Training loss: 2.024522304534912
Validation loss: 2.1444924723717476

Epoch: 5| Step: 9
Training loss: 2.840700626373291
Validation loss: 2.144756640157392

Epoch: 5| Step: 10
Training loss: 2.0979888439178467
Validation loss: 2.14056715144906

Epoch: 237| Step: 0
Training loss: 2.4604594707489014
Validation loss: 2.136164862622497

Epoch: 5| Step: 1
Training loss: 2.103480815887451
Validation loss: 2.173394899214468

Epoch: 5| Step: 2
Training loss: 1.839555025100708
Validation loss: 2.195282305440595

Epoch: 5| Step: 3
Training loss: 2.3533785343170166
Validation loss: 2.199484855897965

Epoch: 5| Step: 4
Training loss: 2.1510562896728516
Validation loss: 2.1953279177347818

Epoch: 5| Step: 5
Training loss: 2.6501355171203613
Validation loss: 2.1993361955047934

Epoch: 5| Step: 6
Training loss: 2.9020278453826904
Validation loss: 2.174757995913106

Epoch: 5| Step: 7
Training loss: 2.3423869609832764
Validation loss: 2.1437349345094416

Epoch: 5| Step: 8
Training loss: 2.751659393310547
Validation loss: 2.115108954009189

Epoch: 5| Step: 9
Training loss: 1.893700361251831
Validation loss: 2.1168765252636326

Epoch: 5| Step: 10
Training loss: 2.4744873046875
Validation loss: 2.122903662343179

Epoch: 238| Step: 0
Training loss: 1.9955106973648071
Validation loss: 2.1251352397344445

Epoch: 5| Step: 1
Training loss: 2.7607598304748535
Validation loss: 2.1338855745971843

Epoch: 5| Step: 2
Training loss: 2.3657097816467285
Validation loss: 2.138712795831824

Epoch: 5| Step: 3
Training loss: 3.402015209197998
Validation loss: 2.1287970824908187

Epoch: 5| Step: 4
Training loss: 2.0343260765075684
Validation loss: 2.106821270399196

Epoch: 5| Step: 5
Training loss: 2.392350673675537
Validation loss: 2.120956866971908

Epoch: 5| Step: 6
Training loss: 1.6949020624160767
Validation loss: 2.1291764154229114

Epoch: 5| Step: 7
Training loss: 2.147155284881592
Validation loss: 2.1263357157348306

Epoch: 5| Step: 8
Training loss: 2.24479079246521
Validation loss: 2.1357965597542385

Epoch: 5| Step: 9
Training loss: 2.3215394020080566
Validation loss: 2.1196037210443968

Epoch: 5| Step: 10
Training loss: 1.9009618759155273
Validation loss: 2.12846214027815

Epoch: 239| Step: 0
Training loss: 3.1283230781555176
Validation loss: 2.129236405895602

Epoch: 5| Step: 1
Training loss: 2.0109798908233643
Validation loss: 2.1358472813842115

Epoch: 5| Step: 2
Training loss: 2.5540146827697754
Validation loss: 2.1215639396380355

Epoch: 5| Step: 3
Training loss: 2.624393939971924
Validation loss: 2.1230186544438845

Epoch: 5| Step: 4
Training loss: 1.7418429851531982
Validation loss: 2.128381654780398

Epoch: 5| Step: 5
Training loss: 2.127711772918701
Validation loss: 2.145005541463052

Epoch: 5| Step: 6
Training loss: 2.132570743560791
Validation loss: 2.135947863260905

Epoch: 5| Step: 7
Training loss: 2.5088798999786377
Validation loss: 2.13156384037387

Epoch: 5| Step: 8
Training loss: 2.2469534873962402
Validation loss: 2.1232753235806703

Epoch: 5| Step: 9
Training loss: 1.8605258464813232
Validation loss: 2.1256022658399356

Epoch: 5| Step: 10
Training loss: 2.2480692863464355
Validation loss: 2.131021043305756

Epoch: 240| Step: 0
Training loss: 2.5685508251190186
Validation loss: 2.1215423025110716

Epoch: 5| Step: 1
Training loss: 2.3406612873077393
Validation loss: 2.1282381037230134

Epoch: 5| Step: 2
Training loss: 2.039313316345215
Validation loss: 2.1324851218090264

Epoch: 5| Step: 3
Training loss: 2.4432528018951416
Validation loss: 2.1120299780240623

Epoch: 5| Step: 4
Training loss: 2.4721519947052
Validation loss: 2.12394352369411

Epoch: 5| Step: 5
Training loss: 1.9381682872772217
Validation loss: 2.1086508343296666

Epoch: 5| Step: 6
Training loss: 2.2640345096588135
Validation loss: 2.124162586786414

Epoch: 5| Step: 7
Training loss: 2.5246903896331787
Validation loss: 2.112679345633394

Epoch: 5| Step: 8
Training loss: 2.0456128120422363
Validation loss: 2.105328584230074

Epoch: 5| Step: 9
Training loss: 1.8351646661758423
Validation loss: 2.1088235070628505

Epoch: 5| Step: 10
Training loss: 2.970597505569458
Validation loss: 2.0910181806933497

Epoch: 241| Step: 0
Training loss: 1.9772526025772095
Validation loss: 2.103102640439105

Epoch: 5| Step: 1
Training loss: 2.3003952503204346
Validation loss: 2.1288692207746607

Epoch: 5| Step: 2
Training loss: 2.6362550258636475
Validation loss: 2.1462864234883297

Epoch: 5| Step: 3
Training loss: 1.549875259399414
Validation loss: 2.134775097652148

Epoch: 5| Step: 4
Training loss: 2.553795337677002
Validation loss: 2.1467555825428297

Epoch: 5| Step: 5
Training loss: 2.2913978099823
Validation loss: 2.155317803864838

Epoch: 5| Step: 6
Training loss: 2.666499614715576
Validation loss: 2.1649279799512637

Epoch: 5| Step: 7
Training loss: 1.905050277709961
Validation loss: 2.1603518198895197

Epoch: 5| Step: 8
Training loss: 2.6423087120056152
Validation loss: 2.173692618646929

Epoch: 5| Step: 9
Training loss: 1.9459009170532227
Validation loss: 2.1682881950050272

Epoch: 5| Step: 10
Training loss: 2.929455280303955
Validation loss: 2.1710106454869753

Epoch: 242| Step: 0
Training loss: 2.529151678085327
Validation loss: 2.161931603185592

Epoch: 5| Step: 1
Training loss: 2.1080780029296875
Validation loss: 2.1728392852249967

Epoch: 5| Step: 2
Training loss: 2.303014039993286
Validation loss: 2.173514013649315

Epoch: 5| Step: 3
Training loss: 2.420588254928589
Validation loss: 2.158304516987134

Epoch: 5| Step: 4
Training loss: 1.7966556549072266
Validation loss: 2.1426480008709814

Epoch: 5| Step: 5
Training loss: 2.497795581817627
Validation loss: 2.1326605350740495

Epoch: 5| Step: 6
Training loss: 2.3940658569335938
Validation loss: 2.130941196154523

Epoch: 5| Step: 7
Training loss: 2.511986255645752
Validation loss: 2.1279684087281585

Epoch: 5| Step: 8
Training loss: 2.326261043548584
Validation loss: 2.125910371862432

Epoch: 5| Step: 9
Training loss: 2.0053651332855225
Validation loss: 2.1338908351877683

Epoch: 5| Step: 10
Training loss: 2.4362356662750244
Validation loss: 2.119212870956749

Epoch: 243| Step: 0
Training loss: 2.3971171379089355
Validation loss: 2.115930544432773

Epoch: 5| Step: 1
Training loss: 2.8185923099517822
Validation loss: 2.1165079019402944

Epoch: 5| Step: 2
Training loss: 1.8144572973251343
Validation loss: 2.1077562070661977

Epoch: 5| Step: 3
Training loss: 2.665210008621216
Validation loss: 2.111858585829376

Epoch: 5| Step: 4
Training loss: 2.0995638370513916
Validation loss: 2.1063832211238083

Epoch: 5| Step: 5
Training loss: 2.3921706676483154
Validation loss: 2.1076206494403142

Epoch: 5| Step: 6
Training loss: 2.27060866355896
Validation loss: 2.1063527291820896

Epoch: 5| Step: 7
Training loss: 2.169440746307373
Validation loss: 2.1115365502654866

Epoch: 5| Step: 8
Training loss: 2.7018933296203613
Validation loss: 2.1167839880912536

Epoch: 5| Step: 9
Training loss: 1.7598354816436768
Validation loss: 2.115915588153306

Epoch: 5| Step: 10
Training loss: 2.1917288303375244
Validation loss: 2.118638365499435

Epoch: 244| Step: 0
Training loss: 2.348673105239868
Validation loss: 2.1057549189495783

Epoch: 5| Step: 1
Training loss: 2.4263389110565186
Validation loss: 2.1106115348877443

Epoch: 5| Step: 2
Training loss: 2.418346881866455
Validation loss: 2.103910776876634

Epoch: 5| Step: 3
Training loss: 2.2947731018066406
Validation loss: 2.113081024539086

Epoch: 5| Step: 4
Training loss: 2.494729518890381
Validation loss: 2.1043758956334924

Epoch: 5| Step: 5
Training loss: 1.8340390920639038
Validation loss: 2.1022881231000348

Epoch: 5| Step: 6
Training loss: 2.3626742362976074
Validation loss: 2.114348139814151

Epoch: 5| Step: 7
Training loss: 2.283921957015991
Validation loss: 2.1137618646826795

Epoch: 5| Step: 8
Training loss: 2.513134479522705
Validation loss: 2.1160257529186945

Epoch: 5| Step: 9
Training loss: 2.197899341583252
Validation loss: 2.120295755324825

Epoch: 5| Step: 10
Training loss: 1.9699848890304565
Validation loss: 2.136182572252007

Epoch: 245| Step: 0
Training loss: 2.827782392501831
Validation loss: 2.14639106873543

Epoch: 5| Step: 1
Training loss: 1.9913049936294556
Validation loss: 2.142841205802015

Epoch: 5| Step: 2
Training loss: 2.4352965354919434
Validation loss: 2.142954205953947

Epoch: 5| Step: 3
Training loss: 1.778928518295288
Validation loss: 2.151312138444634

Epoch: 5| Step: 4
Training loss: 2.2353129386901855
Validation loss: 2.155284043281309

Epoch: 5| Step: 5
Training loss: 2.4833121299743652
Validation loss: 2.1557817382197224

Epoch: 5| Step: 6
Training loss: 1.9995521306991577
Validation loss: 2.138727052237398

Epoch: 5| Step: 7
Training loss: 2.8205296993255615
Validation loss: 2.14543818402034

Epoch: 5| Step: 8
Training loss: 2.2259011268615723
Validation loss: 2.1284795627799085

Epoch: 5| Step: 9
Training loss: 2.146204710006714
Validation loss: 2.1224673486525014

Epoch: 5| Step: 10
Training loss: 2.2800300121307373
Validation loss: 2.1291790828909924

Epoch: 246| Step: 0
Training loss: 2.6898298263549805
Validation loss: 2.1331551203163723

Epoch: 5| Step: 1
Training loss: 1.8590800762176514
Validation loss: 2.1475779471858853

Epoch: 5| Step: 2
Training loss: 2.5174808502197266
Validation loss: 2.1499485841361423

Epoch: 5| Step: 3
Training loss: 2.3869967460632324
Validation loss: 2.1320697120440903

Epoch: 5| Step: 4
Training loss: 3.2927448749542236
Validation loss: 2.11139496936593

Epoch: 5| Step: 5
Training loss: 2.173933744430542
Validation loss: 2.0973185300827026

Epoch: 5| Step: 6
Training loss: 2.2851109504699707
Validation loss: 2.0870922393696283

Epoch: 5| Step: 7
Training loss: 2.098057508468628
Validation loss: 2.1019780456378894

Epoch: 5| Step: 8
Training loss: 2.5519771575927734
Validation loss: 2.1005408661339873

Epoch: 5| Step: 9
Training loss: 1.6447111368179321
Validation loss: 2.1065446317836805

Epoch: 5| Step: 10
Training loss: 1.9490785598754883
Validation loss: 2.1217501958211265

Epoch: 247| Step: 0
Training loss: 2.971893787384033
Validation loss: 2.1234633563667216

Epoch: 5| Step: 1
Training loss: 1.9983346462249756
Validation loss: 2.1239675526977866

Epoch: 5| Step: 2
Training loss: 1.868005394935608
Validation loss: 2.134679389256303

Epoch: 5| Step: 3
Training loss: 2.4013195037841797
Validation loss: 2.13158082449308

Epoch: 5| Step: 4
Training loss: 2.7237133979797363
Validation loss: 2.136653223345357

Epoch: 5| Step: 5
Training loss: 2.102604627609253
Validation loss: 2.148611694253901

Epoch: 5| Step: 6
Training loss: 1.846337914466858
Validation loss: 2.161996900394399

Epoch: 5| Step: 7
Training loss: 2.5849270820617676
Validation loss: 2.1694498190315823

Epoch: 5| Step: 8
Training loss: 2.018418788909912
Validation loss: 2.143596910661267

Epoch: 5| Step: 9
Training loss: 2.6939635276794434
Validation loss: 2.1537413558652325

Epoch: 5| Step: 10
Training loss: 1.7993652820587158
Validation loss: 2.1464874103505123

Epoch: 248| Step: 0
Training loss: 1.6563684940338135
Validation loss: 2.1287465198065645

Epoch: 5| Step: 1
Training loss: 2.379814863204956
Validation loss: 2.1327458799526258

Epoch: 5| Step: 2
Training loss: 2.5933947563171387
Validation loss: 2.132662288604244

Epoch: 5| Step: 3
Training loss: 2.3716318607330322
Validation loss: 2.1223164386646722

Epoch: 5| Step: 4
Training loss: 2.7955269813537598
Validation loss: 2.1294180513710104

Epoch: 5| Step: 5
Training loss: 1.6826270818710327
Validation loss: 2.134790212877335

Epoch: 5| Step: 6
Training loss: 2.2377612590789795
Validation loss: 2.132584730784098

Epoch: 5| Step: 7
Training loss: 2.2994003295898438
Validation loss: 2.13190064378964

Epoch: 5| Step: 8
Training loss: 2.281285047531128
Validation loss: 2.12794078037303

Epoch: 5| Step: 9
Training loss: 1.911020040512085
Validation loss: 2.140010154375466

Epoch: 5| Step: 10
Training loss: 2.8197498321533203
Validation loss: 2.1241970651893207

Epoch: 249| Step: 0
Training loss: 1.9061033725738525
Validation loss: 2.1320281977294595

Epoch: 5| Step: 1
Training loss: 2.8087573051452637
Validation loss: 2.124770777199858

Epoch: 5| Step: 2
Training loss: 2.168717861175537
Validation loss: 2.110273176623929

Epoch: 5| Step: 3
Training loss: 1.835466980934143
Validation loss: 2.1064212886236047

Epoch: 5| Step: 4
Training loss: 1.7461521625518799
Validation loss: 2.0965109409824496

Epoch: 5| Step: 5
Training loss: 2.6185050010681152
Validation loss: 2.0953798499158633

Epoch: 5| Step: 6
Training loss: 2.281172275543213
Validation loss: 2.0935369563359085

Epoch: 5| Step: 7
Training loss: 1.9031877517700195
Validation loss: 2.1045523535820747

Epoch: 5| Step: 8
Training loss: 3.197812557220459
Validation loss: 2.096692979976695

Epoch: 5| Step: 9
Training loss: 1.8099368810653687
Validation loss: 2.097813002524837

Epoch: 5| Step: 10
Training loss: 2.936516284942627
Validation loss: 2.0910679114762174

Epoch: 250| Step: 0
Training loss: 2.215571880340576
Validation loss: 2.08957197589259

Epoch: 5| Step: 1
Training loss: 2.450038194656372
Validation loss: 2.0844987400116457

Epoch: 5| Step: 2
Training loss: 2.2318477630615234
Validation loss: 2.097419806706008

Epoch: 5| Step: 3
Training loss: 2.204751968383789
Validation loss: 2.0970379255151235

Epoch: 5| Step: 4
Training loss: 1.7346092462539673
Validation loss: 2.106223073056949

Epoch: 5| Step: 5
Training loss: 2.3327064514160156
Validation loss: 2.1077991416377406

Epoch: 5| Step: 6
Training loss: 1.9270035028457642
Validation loss: 2.1312205573563934

Epoch: 5| Step: 7
Training loss: 2.3224501609802246
Validation loss: 2.1242869002844698

Epoch: 5| Step: 8
Training loss: 2.607381820678711
Validation loss: 2.131775984200098

Epoch: 5| Step: 9
Training loss: 2.319261312484741
Validation loss: 2.1147491162823093

Epoch: 5| Step: 10
Training loss: 2.6942436695098877
Validation loss: 2.120081288840181

Testing loss: 2.2879580656687417

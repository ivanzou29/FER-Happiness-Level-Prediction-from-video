Epoch: 1| Step: 0
Training loss: 4.7061357498168945
Validation loss: 5.224820783061366

Epoch: 6| Step: 1
Training loss: 5.901124954223633
Validation loss: 5.219552737410351

Epoch: 6| Step: 2
Training loss: 5.1020917892456055
Validation loss: 5.2143759983842095

Epoch: 6| Step: 3
Training loss: 4.395754814147949
Validation loss: 5.209472804941157

Epoch: 6| Step: 4
Training loss: 4.262720584869385
Validation loss: 5.2046677989344445

Epoch: 6| Step: 5
Training loss: 6.500185012817383
Validation loss: 5.199934046755555

Epoch: 6| Step: 6
Training loss: 5.432314872741699
Validation loss: 5.195487089054559

Epoch: 6| Step: 7
Training loss: 3.9970602989196777
Validation loss: 5.190840628839308

Epoch: 6| Step: 8
Training loss: 6.0053911209106445
Validation loss: 5.186399900785056

Epoch: 6| Step: 9
Training loss: 4.8614044189453125
Validation loss: 5.181437036042572

Epoch: 6| Step: 10
Training loss: 4.9232330322265625
Validation loss: 5.176340800459667

Epoch: 6| Step: 11
Training loss: 4.472930431365967
Validation loss: 5.170636879500522

Epoch: 6| Step: 12
Training loss: 3.461369514465332
Validation loss: 5.165101735822616

Epoch: 6| Step: 13
Training loss: 6.469975471496582
Validation loss: 5.159145626970517

Epoch: 2| Step: 0
Training loss: 5.274636268615723
Validation loss: 5.153012583332677

Epoch: 6| Step: 1
Training loss: 4.089155197143555
Validation loss: 5.146262573939498

Epoch: 6| Step: 2
Training loss: 5.58279275894165
Validation loss: 5.139628600048763

Epoch: 6| Step: 3
Training loss: 4.973799705505371
Validation loss: 5.132515589396159

Epoch: 6| Step: 4
Training loss: 5.429812431335449
Validation loss: 5.124898033757364

Epoch: 6| Step: 5
Training loss: 4.083010673522949
Validation loss: 5.117556966761107

Epoch: 6| Step: 6
Training loss: 4.753237247467041
Validation loss: 5.10909757819227

Epoch: 6| Step: 7
Training loss: 4.522952079772949
Validation loss: 5.100574554935578

Epoch: 6| Step: 8
Training loss: 4.770017147064209
Validation loss: 5.091615717898133

Epoch: 6| Step: 9
Training loss: 5.613694190979004
Validation loss: 5.081917901192942

Epoch: 6| Step: 10
Training loss: 5.258046627044678
Validation loss: 5.072086944374987

Epoch: 6| Step: 11
Training loss: 5.285798072814941
Validation loss: 5.061788784560337

Epoch: 6| Step: 12
Training loss: 4.924398422241211
Validation loss: 5.051462014516194

Epoch: 6| Step: 13
Training loss: 3.297664165496826
Validation loss: 5.040708244487804

Epoch: 3| Step: 0
Training loss: 5.612394332885742
Validation loss: 5.029053083030126

Epoch: 6| Step: 1
Training loss: 3.4382376670837402
Validation loss: 5.017320725225633

Epoch: 6| Step: 2
Training loss: 4.389171600341797
Validation loss: 5.0046695073445635

Epoch: 6| Step: 3
Training loss: 4.2689619064331055
Validation loss: 4.992741733468989

Epoch: 6| Step: 4
Training loss: 3.7058205604553223
Validation loss: 4.97897159412343

Epoch: 6| Step: 5
Training loss: 6.4306511878967285
Validation loss: 4.965390569420271

Epoch: 6| Step: 6
Training loss: 5.232395172119141
Validation loss: 4.951700420789821

Epoch: 6| Step: 7
Training loss: 5.116233825683594
Validation loss: 4.9375418591242966

Epoch: 6| Step: 8
Training loss: 4.818145751953125
Validation loss: 4.92223269965059

Epoch: 6| Step: 9
Training loss: 4.775087356567383
Validation loss: 4.906326109363187

Epoch: 6| Step: 10
Training loss: 5.112732887268066
Validation loss: 4.890619277954102

Epoch: 6| Step: 11
Training loss: 4.6561713218688965
Validation loss: 4.873172088335919

Epoch: 6| Step: 12
Training loss: 4.489863872528076
Validation loss: 4.856785012829688

Epoch: 6| Step: 13
Training loss: 3.8422927856445312
Validation loss: 4.837048202432612

Epoch: 4| Step: 0
Training loss: 5.169063091278076
Validation loss: 4.818501877528365

Epoch: 6| Step: 1
Training loss: 5.2770233154296875
Validation loss: 4.80113927266931

Epoch: 6| Step: 2
Training loss: 3.9201362133026123
Validation loss: 4.780273786155126

Epoch: 6| Step: 3
Training loss: 4.44822883605957
Validation loss: 4.759714367569134

Epoch: 6| Step: 4
Training loss: 4.7935686111450195
Validation loss: 4.738605319812733

Epoch: 6| Step: 5
Training loss: 3.883333921432495
Validation loss: 4.716091032951109

Epoch: 6| Step: 6
Training loss: 3.7622196674346924
Validation loss: 4.694386313038487

Epoch: 6| Step: 7
Training loss: 4.333592891693115
Validation loss: 4.6715160185290925

Epoch: 6| Step: 8
Training loss: 4.491268157958984
Validation loss: 4.647031179038427

Epoch: 6| Step: 9
Training loss: 5.327096939086914
Validation loss: 4.624179563214702

Epoch: 6| Step: 10
Training loss: 3.6171212196350098
Validation loss: 4.598154749921573

Epoch: 6| Step: 11
Training loss: 4.615972518920898
Validation loss: 4.575625286307386

Epoch: 6| Step: 12
Training loss: 4.898102760314941
Validation loss: 4.548256030646703

Epoch: 6| Step: 13
Training loss: 3.548823356628418
Validation loss: 4.520761556522821

Epoch: 5| Step: 0
Training loss: 3.507566213607788
Validation loss: 4.493880456493747

Epoch: 6| Step: 1
Training loss: 4.048427581787109
Validation loss: 4.465949104678247

Epoch: 6| Step: 2
Training loss: 5.9199676513671875
Validation loss: 4.4380921394594255

Epoch: 6| Step: 3
Training loss: 4.778547286987305
Validation loss: 4.410938960249706

Epoch: 6| Step: 4
Training loss: 4.353093147277832
Validation loss: 4.382169790165399

Epoch: 6| Step: 5
Training loss: 4.1198248863220215
Validation loss: 4.3540870912613405

Epoch: 6| Step: 6
Training loss: 3.8615801334381104
Validation loss: 4.325897780797815

Epoch: 6| Step: 7
Training loss: 3.3972039222717285
Validation loss: 4.297142341572751

Epoch: 6| Step: 8
Training loss: 3.148739814758301
Validation loss: 4.267202085064303

Epoch: 6| Step: 9
Training loss: 3.684182643890381
Validation loss: 4.239991108576457

Epoch: 6| Step: 10
Training loss: 4.171091556549072
Validation loss: 4.212803163836079

Epoch: 6| Step: 11
Training loss: 4.247720241546631
Validation loss: 4.186775151119437

Epoch: 6| Step: 12
Training loss: 4.727774620056152
Validation loss: 4.159394648767287

Epoch: 6| Step: 13
Training loss: 3.41593074798584
Validation loss: 4.131069316658922

Epoch: 6| Step: 0
Training loss: 3.905217409133911
Validation loss: 4.1018213943768576

Epoch: 6| Step: 1
Training loss: 3.4325053691864014
Validation loss: 4.074620495560349

Epoch: 6| Step: 2
Training loss: 3.367161989212036
Validation loss: 4.049664687084896

Epoch: 6| Step: 3
Training loss: 2.710671901702881
Validation loss: 4.0271898597799325

Epoch: 6| Step: 4
Training loss: 2.241013765335083
Validation loss: 4.00323808834117

Epoch: 6| Step: 5
Training loss: 3.201932430267334
Validation loss: 3.984191245930169

Epoch: 6| Step: 6
Training loss: 5.583452224731445
Validation loss: 3.964550297747376

Epoch: 6| Step: 7
Training loss: 4.860506057739258
Validation loss: 3.9444710387978503

Epoch: 6| Step: 8
Training loss: 4.5240020751953125
Validation loss: 3.92670504764844

Epoch: 6| Step: 9
Training loss: 4.478141784667969
Validation loss: 3.907663929846979

Epoch: 6| Step: 10
Training loss: 4.587141036987305
Validation loss: 3.889521211706182

Epoch: 6| Step: 11
Training loss: 3.0139412879943848
Validation loss: 3.8709516755996214

Epoch: 6| Step: 12
Training loss: 3.2844107151031494
Validation loss: 3.8532490755922053

Epoch: 6| Step: 13
Training loss: 4.488362789154053
Validation loss: 3.8380977876724733

Epoch: 7| Step: 0
Training loss: 4.114178657531738
Validation loss: 3.8180859575989428

Epoch: 6| Step: 1
Training loss: 3.8137447834014893
Validation loss: 3.7989248024520053

Epoch: 6| Step: 2
Training loss: 4.289697647094727
Validation loss: 3.783701219866353

Epoch: 6| Step: 3
Training loss: 3.888786554336548
Validation loss: 3.765612904743482

Epoch: 6| Step: 4
Training loss: 4.360392093658447
Validation loss: 3.7494038458793395

Epoch: 6| Step: 5
Training loss: 2.8330817222595215
Validation loss: 3.732166936320643

Epoch: 6| Step: 6
Training loss: 4.611710071563721
Validation loss: 3.719670190606066

Epoch: 6| Step: 7
Training loss: 3.7525856494903564
Validation loss: 3.7028024786262104

Epoch: 6| Step: 8
Training loss: 2.950017213821411
Validation loss: 3.687937536547261

Epoch: 6| Step: 9
Training loss: 3.582671880722046
Validation loss: 3.678095586838261

Epoch: 6| Step: 10
Training loss: 3.361530303955078
Validation loss: 3.661119040622506

Epoch: 6| Step: 11
Training loss: 2.979783058166504
Validation loss: 3.650700012842814

Epoch: 6| Step: 12
Training loss: 2.3560826778411865
Validation loss: 3.6369048600555747

Epoch: 6| Step: 13
Training loss: 4.075143814086914
Validation loss: 3.6250557027837282

Epoch: 8| Step: 0
Training loss: 3.542178153991699
Validation loss: 3.6148286916876353

Epoch: 6| Step: 1
Training loss: 3.2532732486724854
Validation loss: 3.6026622428688952

Epoch: 6| Step: 2
Training loss: 2.888953685760498
Validation loss: 3.5900016856449906

Epoch: 6| Step: 3
Training loss: 3.8067126274108887
Validation loss: 3.580651813937772

Epoch: 6| Step: 4
Training loss: 3.227358818054199
Validation loss: 3.5689211173724105

Epoch: 6| Step: 5
Training loss: 3.4067306518554688
Validation loss: 3.55787645616839

Epoch: 6| Step: 6
Training loss: 3.7283778190612793
Validation loss: 3.549240037959109

Epoch: 6| Step: 7
Training loss: 3.4113781452178955
Validation loss: 3.5404991231938845

Epoch: 6| Step: 8
Training loss: 3.2179412841796875
Validation loss: 3.530945236964892

Epoch: 6| Step: 9
Training loss: 3.6082396507263184
Validation loss: 3.5219730331051733

Epoch: 6| Step: 10
Training loss: 3.649566411972046
Validation loss: 3.5106243779582362

Epoch: 6| Step: 11
Training loss: 4.6756391525268555
Validation loss: 3.5043512569960726

Epoch: 6| Step: 12
Training loss: 3.1345319747924805
Validation loss: 3.495243618565221

Epoch: 6| Step: 13
Training loss: 2.6819112300872803
Validation loss: 3.4880857134378083

Epoch: 9| Step: 0
Training loss: 3.189995288848877
Validation loss: 3.4783100312755955

Epoch: 6| Step: 1
Training loss: 3.768885612487793
Validation loss: 3.4736054840908257

Epoch: 6| Step: 2
Training loss: 3.334115505218506
Validation loss: 3.4656116013885825

Epoch: 6| Step: 3
Training loss: 3.433577060699463
Validation loss: 3.4596740661128873

Epoch: 6| Step: 4
Training loss: 3.472917079925537
Validation loss: 3.451858097507108

Epoch: 6| Step: 5
Training loss: 2.7749266624450684
Validation loss: 3.4486093316026913

Epoch: 6| Step: 6
Training loss: 3.3244893550872803
Validation loss: 3.440270321343535

Epoch: 6| Step: 7
Training loss: 3.877998113632202
Validation loss: 3.433561535291774

Epoch: 6| Step: 8
Training loss: 4.081453323364258
Validation loss: 3.427788157616892

Epoch: 6| Step: 9
Training loss: 3.9215149879455566
Validation loss: 3.423414840493151

Epoch: 6| Step: 10
Training loss: 2.782957077026367
Validation loss: 3.4159252874312864

Epoch: 6| Step: 11
Training loss: 2.985495090484619
Validation loss: 3.410166786563012

Epoch: 6| Step: 12
Training loss: 3.3117878437042236
Validation loss: 3.405688311464043

Epoch: 6| Step: 13
Training loss: 2.907526969909668
Validation loss: 3.400228695202899

Epoch: 10| Step: 0
Training loss: 3.5285470485687256
Validation loss: 3.39314745062141

Epoch: 6| Step: 1
Training loss: 3.859280824661255
Validation loss: 3.388796216698103

Epoch: 6| Step: 2
Training loss: 3.3486878871917725
Validation loss: 3.3833774059049544

Epoch: 6| Step: 3
Training loss: 2.4078643321990967
Validation loss: 3.3771866162618003

Epoch: 6| Step: 4
Training loss: 2.7415223121643066
Validation loss: 3.372317978130874

Epoch: 6| Step: 5
Training loss: 3.8852319717407227
Validation loss: 3.367358366648356

Epoch: 6| Step: 6
Training loss: 3.920544147491455
Validation loss: 3.361442430045015

Epoch: 6| Step: 7
Training loss: 3.3109982013702393
Validation loss: 3.3542584321832143

Epoch: 6| Step: 8
Training loss: 3.146162986755371
Validation loss: 3.3496064088677846

Epoch: 6| Step: 9
Training loss: 2.886112689971924
Validation loss: 3.3453805523533977

Epoch: 6| Step: 10
Training loss: 2.5266847610473633
Validation loss: 3.340956862254809

Epoch: 6| Step: 11
Training loss: 3.785879611968994
Validation loss: 3.332481630386845

Epoch: 6| Step: 12
Training loss: 3.790821075439453
Validation loss: 3.3292260118710097

Epoch: 6| Step: 13
Training loss: 3.2794888019561768
Validation loss: 3.3253886622767292

Epoch: 11| Step: 0
Training loss: 4.066368103027344
Validation loss: 3.321418241787982

Epoch: 6| Step: 1
Training loss: 3.4910426139831543
Validation loss: 3.3156644349457114

Epoch: 6| Step: 2
Training loss: 3.378929615020752
Validation loss: 3.31237853470669

Epoch: 6| Step: 3
Training loss: 3.757143974304199
Validation loss: 3.305964208418323

Epoch: 6| Step: 4
Training loss: 3.819399356842041
Validation loss: 3.3005981778585785

Epoch: 6| Step: 5
Training loss: 3.618649482727051
Validation loss: 3.295408997484433

Epoch: 6| Step: 6
Training loss: 2.465416431427002
Validation loss: 3.290417412275909

Epoch: 6| Step: 7
Training loss: 2.159975051879883
Validation loss: 3.288764243484825

Epoch: 6| Step: 8
Training loss: 3.2110555171966553
Validation loss: 3.2825549187198764

Epoch: 6| Step: 9
Training loss: 3.820763349533081
Validation loss: 3.2751218580430552

Epoch: 6| Step: 10
Training loss: 2.8130388259887695
Validation loss: 3.270653058123845

Epoch: 6| Step: 11
Training loss: 2.9471778869628906
Validation loss: 3.2681846977562032

Epoch: 6| Step: 12
Training loss: 2.7722086906433105
Validation loss: 3.2608997309079735

Epoch: 6| Step: 13
Training loss: 3.400766134262085
Validation loss: 3.258707256727321

Epoch: 12| Step: 0
Training loss: 3.4716858863830566
Validation loss: 3.2531116290759017

Epoch: 6| Step: 1
Training loss: 3.1958115100860596
Validation loss: 3.2482786178588867

Epoch: 6| Step: 2
Training loss: 2.7275631427764893
Validation loss: 3.2456224785056165

Epoch: 6| Step: 3
Training loss: 2.7271339893341064
Validation loss: 3.2405677021190686

Epoch: 6| Step: 4
Training loss: 2.848550319671631
Validation loss: 3.2387120236632643

Epoch: 6| Step: 5
Training loss: 2.927762269973755
Validation loss: 3.233315703689411

Epoch: 6| Step: 6
Training loss: 3.4414968490600586
Validation loss: 3.229132280554823

Epoch: 6| Step: 7
Training loss: 4.125706195831299
Validation loss: 3.2249504032955376

Epoch: 6| Step: 8
Training loss: 3.900953769683838
Validation loss: 3.2190914769326486

Epoch: 6| Step: 9
Training loss: 3.9683709144592285
Validation loss: 3.2157570777400846

Epoch: 6| Step: 10
Training loss: 2.570085048675537
Validation loss: 3.2088510118505007

Epoch: 6| Step: 11
Training loss: 3.0467958450317383
Validation loss: 3.2044622000827583

Epoch: 6| Step: 12
Training loss: 3.051297426223755
Validation loss: 3.202604760405838

Epoch: 6| Step: 13
Training loss: 2.8076915740966797
Validation loss: 3.194013221289522

Epoch: 13| Step: 0
Training loss: 3.599343776702881
Validation loss: 3.192755468430058

Epoch: 6| Step: 1
Training loss: 2.2634530067443848
Validation loss: 3.1889639413484963

Epoch: 6| Step: 2
Training loss: 3.0934371948242188
Validation loss: 3.183110749849709

Epoch: 6| Step: 3
Training loss: 2.86859130859375
Validation loss: 3.177890354587186

Epoch: 6| Step: 4
Training loss: 1.935788631439209
Validation loss: 3.17332959687838

Epoch: 6| Step: 5
Training loss: 3.2394747734069824
Validation loss: 3.1712826426311205

Epoch: 6| Step: 6
Training loss: 4.108434677124023
Validation loss: 3.1702811564168623

Epoch: 6| Step: 7
Training loss: 3.50563383102417
Validation loss: 3.1644858519236245

Epoch: 6| Step: 8
Training loss: 2.646383762359619
Validation loss: 3.157035304654029

Epoch: 6| Step: 9
Training loss: 3.3485138416290283
Validation loss: 3.1491672915797078

Epoch: 6| Step: 10
Training loss: 3.1786537170410156
Validation loss: 3.148171781211771

Epoch: 6| Step: 11
Training loss: 3.8756356239318848
Validation loss: 3.146182075623543

Epoch: 6| Step: 12
Training loss: 3.073594093322754
Validation loss: 3.1386141700129353

Epoch: 6| Step: 13
Training loss: 3.999406576156616
Validation loss: 3.1344800995242212

Epoch: 14| Step: 0
Training loss: 2.898477792739868
Validation loss: 3.1346475667850946

Epoch: 6| Step: 1
Training loss: 3.129051923751831
Validation loss: 3.1306432447125836

Epoch: 6| Step: 2
Training loss: 4.371051788330078
Validation loss: 3.127772808074951

Epoch: 6| Step: 3
Training loss: 3.597411632537842
Validation loss: 3.118785350553451

Epoch: 6| Step: 4
Training loss: 3.0015010833740234
Validation loss: 3.114718237230855

Epoch: 6| Step: 5
Training loss: 2.708733320236206
Validation loss: 3.1127079122809955

Epoch: 6| Step: 6
Training loss: 3.5472497940063477
Validation loss: 3.110259081727715

Epoch: 6| Step: 7
Training loss: 2.7465405464172363
Validation loss: 3.1065217961547194

Epoch: 6| Step: 8
Training loss: 2.979379177093506
Validation loss: 3.1046189031293316

Epoch: 6| Step: 9
Training loss: 2.061127185821533
Validation loss: 3.1051789099170315

Epoch: 6| Step: 10
Training loss: 4.220846652984619
Validation loss: 3.0955733996565624

Epoch: 6| Step: 11
Training loss: 1.9417258501052856
Validation loss: 3.091292360777496

Epoch: 6| Step: 12
Training loss: 4.025579452514648
Validation loss: 3.100559162837203

Epoch: 6| Step: 13
Training loss: 2.2449100017547607
Validation loss: 3.0871067790574926

Epoch: 15| Step: 0
Training loss: 3.052189826965332
Validation loss: 3.1038923340459026

Epoch: 6| Step: 1
Training loss: 3.162057399749756
Validation loss: 3.0911556777133735

Epoch: 6| Step: 2
Training loss: 3.456700325012207
Validation loss: 3.0799195074266

Epoch: 6| Step: 3
Training loss: 2.934668779373169
Validation loss: 3.07855664273744

Epoch: 6| Step: 4
Training loss: 4.247219085693359
Validation loss: 3.079919269008021

Epoch: 6| Step: 5
Training loss: 2.449007034301758
Validation loss: 3.08568823722101

Epoch: 6| Step: 6
Training loss: 2.879911422729492
Validation loss: 3.089974731527349

Epoch: 6| Step: 7
Training loss: 3.035431146621704
Validation loss: 3.105875151131743

Epoch: 6| Step: 8
Training loss: 3.2167904376983643
Validation loss: 3.0794791072927494

Epoch: 6| Step: 9
Training loss: 3.1858713626861572
Validation loss: 3.0674763392376643

Epoch: 6| Step: 10
Training loss: 2.070242404937744
Validation loss: 3.061767303815452

Epoch: 6| Step: 11
Training loss: 3.303623914718628
Validation loss: 3.059620726493097

Epoch: 6| Step: 12
Training loss: 3.345581531524658
Validation loss: 3.056543932166151

Epoch: 6| Step: 13
Training loss: 3.2341723442077637
Validation loss: 3.0541056227940384

Epoch: 16| Step: 0
Training loss: 2.728321075439453
Validation loss: 3.0522980536184003

Epoch: 6| Step: 1
Training loss: 3.353515386581421
Validation loss: 3.0551352449642715

Epoch: 6| Step: 2
Training loss: 3.8715832233428955
Validation loss: 3.0437316433075936

Epoch: 6| Step: 3
Training loss: 3.386904001235962
Validation loss: 3.0406099416876353

Epoch: 6| Step: 4
Training loss: 2.8535983562469482
Validation loss: 3.0447867711385093

Epoch: 6| Step: 5
Training loss: 3.0908472537994385
Validation loss: 3.0401710592290407

Epoch: 6| Step: 6
Training loss: 2.7789158821105957
Validation loss: 3.044125759473411

Epoch: 6| Step: 7
Training loss: 2.5311546325683594
Validation loss: 3.0461050515533774

Epoch: 6| Step: 8
Training loss: 3.6660876274108887
Validation loss: 3.039038424850792

Epoch: 6| Step: 9
Training loss: 3.29543137550354
Validation loss: 3.033863841846425

Epoch: 6| Step: 10
Training loss: 3.2533435821533203
Validation loss: 3.0297640062147573

Epoch: 6| Step: 11
Training loss: 3.5760445594787598
Validation loss: 3.0242624693019415

Epoch: 6| Step: 12
Training loss: 1.744267463684082
Validation loss: 3.0188798468600035

Epoch: 6| Step: 13
Training loss: 3.094066619873047
Validation loss: 3.016135964342343

Epoch: 17| Step: 0
Training loss: 2.75661301612854
Validation loss: 3.013958002931328

Epoch: 6| Step: 1
Training loss: 2.9316225051879883
Validation loss: 3.011151385563676

Epoch: 6| Step: 2
Training loss: 2.887878656387329
Validation loss: 3.004088491521856

Epoch: 6| Step: 3
Training loss: 3.652641773223877
Validation loss: 3.004882279262748

Epoch: 6| Step: 4
Training loss: 3.038520336151123
Validation loss: 3.003923036718881

Epoch: 6| Step: 5
Training loss: 3.0661511421203613
Validation loss: 3.00244810247934

Epoch: 6| Step: 6
Training loss: 3.866229772567749
Validation loss: 3.001047529200072

Epoch: 6| Step: 7
Training loss: 2.92913818359375
Validation loss: 2.995538173183318

Epoch: 6| Step: 8
Training loss: 2.3941853046417236
Validation loss: 2.992712656656901

Epoch: 6| Step: 9
Training loss: 2.699843645095825
Validation loss: 2.9915175796836935

Epoch: 6| Step: 10
Training loss: 4.244967460632324
Validation loss: 2.986210346221924

Epoch: 6| Step: 11
Training loss: 1.97292160987854
Validation loss: 2.9838820811240905

Epoch: 6| Step: 12
Training loss: 2.9782612323760986
Validation loss: 2.985206978295439

Epoch: 6| Step: 13
Training loss: 3.726456642150879
Validation loss: 2.979134859577302

Epoch: 18| Step: 0
Training loss: 2.6634607315063477
Validation loss: 2.97898619662049

Epoch: 6| Step: 1
Training loss: 3.5003836154937744
Validation loss: 2.979990687421573

Epoch: 6| Step: 2
Training loss: 3.0570545196533203
Validation loss: 2.9719532715376986

Epoch: 6| Step: 3
Training loss: 3.2560369968414307
Validation loss: 2.9716347750797065

Epoch: 6| Step: 4
Training loss: 2.038572072982788
Validation loss: 2.9676867249191448

Epoch: 6| Step: 5
Training loss: 1.9258058071136475
Validation loss: 2.975272588832404

Epoch: 6| Step: 6
Training loss: 3.8179001808166504
Validation loss: 3.0179903866142355

Epoch: 6| Step: 7
Training loss: 3.8181121349334717
Validation loss: 2.9753188574185936

Epoch: 6| Step: 8
Training loss: 3.044130802154541
Validation loss: 2.976831792503275

Epoch: 6| Step: 9
Training loss: 2.730963706970215
Validation loss: 2.9814208194773686

Epoch: 6| Step: 10
Training loss: 3.609774589538574
Validation loss: 2.9632933575619935

Epoch: 6| Step: 11
Training loss: 3.2009010314941406
Validation loss: 2.9644107331511793

Epoch: 6| Step: 12
Training loss: 3.231687545776367
Validation loss: 2.9661750639638593

Epoch: 6| Step: 13
Training loss: 2.6039199829101562
Validation loss: 2.9677799542744956

Epoch: 19| Step: 0
Training loss: 2.716726303100586
Validation loss: 2.9739242881856938

Epoch: 6| Step: 1
Training loss: 3.4972236156463623
Validation loss: 2.979935223056424

Epoch: 6| Step: 2
Training loss: 3.172550678253174
Validation loss: 2.9931885991045224

Epoch: 6| Step: 3
Training loss: 2.1406517028808594
Validation loss: 2.9871004653233353

Epoch: 6| Step: 4
Training loss: 3.7876763343811035
Validation loss: 2.9852569282695813

Epoch: 6| Step: 5
Training loss: 2.586332321166992
Validation loss: 2.977911759448308

Epoch: 6| Step: 6
Training loss: 3.0244598388671875
Validation loss: 2.9695664195604223

Epoch: 6| Step: 7
Training loss: 3.4871630668640137
Validation loss: 2.9637051372117895

Epoch: 6| Step: 8
Training loss: 2.3833537101745605
Validation loss: 2.9594265978823424

Epoch: 6| Step: 9
Training loss: 3.5507092475891113
Validation loss: 2.9509830423580703

Epoch: 6| Step: 10
Training loss: 2.672722101211548
Validation loss: 2.9487518495129

Epoch: 6| Step: 11
Training loss: 2.8219857215881348
Validation loss: 2.9428660818325576

Epoch: 6| Step: 12
Training loss: 2.820186138153076
Validation loss: 2.94451823542195

Epoch: 6| Step: 13
Training loss: 4.468197822570801
Validation loss: 2.9410335504880516

Epoch: 20| Step: 0
Training loss: 1.9713267087936401
Validation loss: 2.9421349007596254

Epoch: 6| Step: 1
Training loss: 2.2266910076141357
Validation loss: 2.9435163492797525

Epoch: 6| Step: 2
Training loss: 2.492332935333252
Validation loss: 2.940660768939603

Epoch: 6| Step: 3
Training loss: 3.615661144256592
Validation loss: 2.9329164105076946

Epoch: 6| Step: 4
Training loss: 1.5986206531524658
Validation loss: 2.9260554057295605

Epoch: 6| Step: 5
Training loss: 2.754225254058838
Validation loss: 2.9254465462059103

Epoch: 6| Step: 6
Training loss: 3.491643190383911
Validation loss: 2.9223433745804654

Epoch: 6| Step: 7
Training loss: 3.4875035285949707
Validation loss: 2.9222020333813084

Epoch: 6| Step: 8
Training loss: 3.3885817527770996
Validation loss: 2.9203278300582722

Epoch: 6| Step: 9
Training loss: 3.518603801727295
Validation loss: 2.9186236114912134

Epoch: 6| Step: 10
Training loss: 3.1008195877075195
Validation loss: 2.9175444059474493

Epoch: 6| Step: 11
Training loss: 3.3929829597473145
Validation loss: 2.9150695134234685

Epoch: 6| Step: 12
Training loss: 3.558642864227295
Validation loss: 2.912326112870247

Epoch: 6| Step: 13
Training loss: 4.033849239349365
Validation loss: 2.9129928773449314

Epoch: 21| Step: 0
Training loss: 2.8504679203033447
Validation loss: 2.9101898516378095

Epoch: 6| Step: 1
Training loss: 2.4576196670532227
Validation loss: 2.9099171853834584

Epoch: 6| Step: 2
Training loss: 3.4505558013916016
Validation loss: 2.9040230781801286

Epoch: 6| Step: 3
Training loss: 3.134540557861328
Validation loss: 2.9031107810235794

Epoch: 6| Step: 4
Training loss: 2.7582321166992188
Validation loss: 2.901155707656696

Epoch: 6| Step: 5
Training loss: 2.3962583541870117
Validation loss: 2.8993477334258375

Epoch: 6| Step: 6
Training loss: 3.6184606552124023
Validation loss: 2.8970302894551265

Epoch: 6| Step: 7
Training loss: 3.06011962890625
Validation loss: 2.8985542276854157

Epoch: 6| Step: 8
Training loss: 2.50101900100708
Validation loss: 2.893489601791546

Epoch: 6| Step: 9
Training loss: 3.111406087875366
Validation loss: 2.8924503916053363

Epoch: 6| Step: 10
Training loss: 3.0366601943969727
Validation loss: 2.8910437091704337

Epoch: 6| Step: 11
Training loss: 3.7175533771514893
Validation loss: 2.8873532100390364

Epoch: 6| Step: 12
Training loss: 3.0188417434692383
Validation loss: 2.884609550558111

Epoch: 6| Step: 13
Training loss: 2.6694891452789307
Validation loss: 2.8861330196421635

Epoch: 22| Step: 0
Training loss: 3.4740371704101562
Validation loss: 2.8808475899439987

Epoch: 6| Step: 1
Training loss: 2.9302549362182617
Validation loss: 2.8802625030599613

Epoch: 6| Step: 2
Training loss: 3.147270917892456
Validation loss: 2.8771512995484056

Epoch: 6| Step: 3
Training loss: 3.873962640762329
Validation loss: 2.875474896482242

Epoch: 6| Step: 4
Training loss: 2.8145031929016113
Validation loss: 2.87528625611336

Epoch: 6| Step: 5
Training loss: 2.0146541595458984
Validation loss: 2.873026822202949

Epoch: 6| Step: 6
Training loss: 3.1201705932617188
Validation loss: 2.869893796982304

Epoch: 6| Step: 7
Training loss: 2.35117244720459
Validation loss: 2.8676773296889437

Epoch: 6| Step: 8
Training loss: 2.823925256729126
Validation loss: 2.86886655130694

Epoch: 6| Step: 9
Training loss: 3.0487966537475586
Validation loss: 2.8665539269806235

Epoch: 6| Step: 10
Training loss: 3.2854080200195312
Validation loss: 2.865711922286659

Epoch: 6| Step: 11
Training loss: 2.835995674133301
Validation loss: 2.862666917103593

Epoch: 6| Step: 12
Training loss: 2.9651668071746826
Validation loss: 2.8603076986087266

Epoch: 6| Step: 13
Training loss: 2.9963297843933105
Validation loss: 2.8588279934339624

Epoch: 23| Step: 0
Training loss: 3.306396007537842
Validation loss: 2.8549145216582925

Epoch: 6| Step: 1
Training loss: 2.449749231338501
Validation loss: 2.8556648428722093

Epoch: 6| Step: 2
Training loss: 2.8599202632904053
Validation loss: 2.8542019167254047

Epoch: 6| Step: 3
Training loss: 2.8009719848632812
Validation loss: 2.857015468740976

Epoch: 6| Step: 4
Training loss: 3.781215190887451
Validation loss: 2.8536178014611684

Epoch: 6| Step: 5
Training loss: 3.400409698486328
Validation loss: 2.8507022703847578

Epoch: 6| Step: 6
Training loss: 2.547628164291382
Validation loss: 2.8499658030848347

Epoch: 6| Step: 7
Training loss: 2.13984751701355
Validation loss: 2.849161978690855

Epoch: 6| Step: 8
Training loss: 3.329888343811035
Validation loss: 2.8506742164652836

Epoch: 6| Step: 9
Training loss: 1.81998872756958
Validation loss: 2.855239401581467

Epoch: 6| Step: 10
Training loss: 3.041539192199707
Validation loss: 2.847058298767254

Epoch: 6| Step: 11
Training loss: 4.023951530456543
Validation loss: 2.8441038157350276

Epoch: 6| Step: 12
Training loss: 2.113405466079712
Validation loss: 2.8458991794176

Epoch: 6| Step: 13
Training loss: 4.58364200592041
Validation loss: 2.843989726035826

Epoch: 24| Step: 0
Training loss: 3.414912700653076
Validation loss: 2.8425667439737627

Epoch: 6| Step: 1
Training loss: 3.61484432220459
Validation loss: 2.843292784947221

Epoch: 6| Step: 2
Training loss: 2.897642135620117
Validation loss: 2.837004743596559

Epoch: 6| Step: 3
Training loss: 2.8817214965820312
Validation loss: 2.836842331835019

Epoch: 6| Step: 4
Training loss: 3.2023041248321533
Validation loss: 2.8289275271918184

Epoch: 6| Step: 5
Training loss: 1.8282915353775024
Validation loss: 2.828540894293016

Epoch: 6| Step: 6
Training loss: 3.3555431365966797
Validation loss: 2.8256757900279057

Epoch: 6| Step: 7
Training loss: 3.647057056427002
Validation loss: 2.8198792934417725

Epoch: 6| Step: 8
Training loss: 2.7842631340026855
Validation loss: 2.8210186214857202

Epoch: 6| Step: 9
Training loss: 2.478430986404419
Validation loss: 2.8193531856741956

Epoch: 6| Step: 10
Training loss: 2.4657719135284424
Validation loss: 2.8153768277937368

Epoch: 6| Step: 11
Training loss: 3.2354040145874023
Validation loss: 2.813361642181232

Epoch: 6| Step: 12
Training loss: 2.6226887702941895
Validation loss: 2.8150708213929208

Epoch: 6| Step: 13
Training loss: 2.690281867980957
Validation loss: 2.811416559321906

Epoch: 25| Step: 0
Training loss: 2.443408727645874
Validation loss: 2.8086624812054377

Epoch: 6| Step: 1
Training loss: 2.1720895767211914
Validation loss: 2.802700193979407

Epoch: 6| Step: 2
Training loss: 3.0024147033691406
Validation loss: 2.8051905837110294

Epoch: 6| Step: 3
Training loss: 3.505675792694092
Validation loss: 2.8043111139728176

Epoch: 6| Step: 4
Training loss: 2.874218463897705
Validation loss: 2.7994681763392624

Epoch: 6| Step: 5
Training loss: 2.96689772605896
Validation loss: 2.7982512712478638

Epoch: 6| Step: 6
Training loss: 3.057745933532715
Validation loss: 2.795847195450978

Epoch: 6| Step: 7
Training loss: 3.2851738929748535
Validation loss: 2.7941657753400904

Epoch: 6| Step: 8
Training loss: 2.7223634719848633
Validation loss: 2.7971051482744116

Epoch: 6| Step: 9
Training loss: 3.0910916328430176
Validation loss: 2.789745940957018

Epoch: 6| Step: 10
Training loss: 2.9462618827819824
Validation loss: 2.787042925434728

Epoch: 6| Step: 11
Training loss: 2.7450695037841797
Validation loss: 2.7856075199701453

Epoch: 6| Step: 12
Training loss: 3.102384567260742
Validation loss: 2.7822557341667915

Epoch: 6| Step: 13
Training loss: 3.198300361633301
Validation loss: 2.7823402522712626

Epoch: 26| Step: 0
Training loss: 1.8605237007141113
Validation loss: 2.7792184147783505

Epoch: 6| Step: 1
Training loss: 2.298126697540283
Validation loss: 2.778705991724486

Epoch: 6| Step: 2
Training loss: 2.954108953475952
Validation loss: 2.7701326647112445

Epoch: 6| Step: 3
Training loss: 2.3867239952087402
Validation loss: 2.767707719597765

Epoch: 6| Step: 4
Training loss: 3.1818506717681885
Validation loss: 2.7697002708270984

Epoch: 6| Step: 5
Training loss: 2.162294387817383
Validation loss: 2.7671951939982753

Epoch: 6| Step: 6
Training loss: 3.215947151184082
Validation loss: 2.7655695535803355

Epoch: 6| Step: 7
Training loss: 2.921901226043701
Validation loss: 2.763698290753108

Epoch: 6| Step: 8
Training loss: 3.63578462600708
Validation loss: 2.752926275294314

Epoch: 6| Step: 9
Training loss: 3.1316938400268555
Validation loss: 2.754475498712191

Epoch: 6| Step: 10
Training loss: 2.9460809230804443
Validation loss: 2.753973650675948

Epoch: 6| Step: 11
Training loss: 2.918959617614746
Validation loss: 2.750013666768228

Epoch: 6| Step: 12
Training loss: 3.8249855041503906
Validation loss: 2.7482381892460648

Epoch: 6| Step: 13
Training loss: 3.3854193687438965
Validation loss: 2.7444059976967434

Epoch: 27| Step: 0
Training loss: 2.550445318222046
Validation loss: 2.745539016621087

Epoch: 6| Step: 1
Training loss: 2.2378010749816895
Validation loss: 2.740120085336829

Epoch: 6| Step: 2
Training loss: 2.9978036880493164
Validation loss: 2.7436988456274873

Epoch: 6| Step: 3
Training loss: 3.1957695484161377
Validation loss: 2.7489712597221456

Epoch: 6| Step: 4
Training loss: 4.224514007568359
Validation loss: 2.7446356922067623

Epoch: 6| Step: 5
Training loss: 3.621070384979248
Validation loss: 2.741441718993648

Epoch: 6| Step: 6
Training loss: 2.2887160778045654
Validation loss: 2.763438811866186

Epoch: 6| Step: 7
Training loss: 2.59197998046875
Validation loss: 2.7385017179673716

Epoch: 6| Step: 8
Training loss: 2.286790132522583
Validation loss: 2.7352718640399236

Epoch: 6| Step: 9
Training loss: 3.630632162094116
Validation loss: 2.7332625696735997

Epoch: 6| Step: 10
Training loss: 2.0636446475982666
Validation loss: 2.7283428356211674

Epoch: 6| Step: 11
Training loss: 2.917973518371582
Validation loss: 2.7310996824695217

Epoch: 6| Step: 12
Training loss: 2.4897215366363525
Validation loss: 2.733517821117114

Epoch: 6| Step: 13
Training loss: 3.8118107318878174
Validation loss: 2.729086950261106

Epoch: 28| Step: 0
Training loss: 2.0021307468414307
Validation loss: 2.7287305529399584

Epoch: 6| Step: 1
Training loss: 2.574525833129883
Validation loss: 2.7272554469364945

Epoch: 6| Step: 2
Training loss: 1.9760980606079102
Validation loss: 2.7310786965072795

Epoch: 6| Step: 3
Training loss: 2.912177562713623
Validation loss: 2.732017029998123

Epoch: 6| Step: 4
Training loss: 2.988900661468506
Validation loss: 2.726863027900778

Epoch: 6| Step: 5
Training loss: 2.7371506690979004
Validation loss: 2.7215490905187463

Epoch: 6| Step: 6
Training loss: 2.7455990314483643
Validation loss: 2.7228222611129924

Epoch: 6| Step: 7
Training loss: 3.6302382946014404
Validation loss: 2.7258332467848256

Epoch: 6| Step: 8
Training loss: 3.45947003364563
Validation loss: 2.720605106763942

Epoch: 6| Step: 9
Training loss: 3.1609530448913574
Validation loss: 2.719223483916252

Epoch: 6| Step: 10
Training loss: 2.76119327545166
Validation loss: 2.7192318875302552

Epoch: 6| Step: 11
Training loss: 3.406531810760498
Validation loss: 2.7172342602924635

Epoch: 6| Step: 12
Training loss: 3.403583526611328
Validation loss: 2.7131113749678417

Epoch: 6| Step: 13
Training loss: 2.3628807067871094
Validation loss: 2.7138878965890534

Epoch: 29| Step: 0
Training loss: 3.3023319244384766
Validation loss: 2.7133048221629155

Epoch: 6| Step: 1
Training loss: 2.567505359649658
Validation loss: 2.7122064123871508

Epoch: 6| Step: 2
Training loss: 2.9280686378479004
Validation loss: 2.7092627145910777

Epoch: 6| Step: 3
Training loss: 2.7437758445739746
Validation loss: 2.7072079143216534

Epoch: 6| Step: 4
Training loss: 3.367363929748535
Validation loss: 2.706565708242437

Epoch: 6| Step: 5
Training loss: 2.8747997283935547
Validation loss: 2.7073363309265464

Epoch: 6| Step: 6
Training loss: 2.638535976409912
Validation loss: 2.711902985008814

Epoch: 6| Step: 7
Training loss: 2.9507155418395996
Validation loss: 2.709816966005551

Epoch: 6| Step: 8
Training loss: 2.9119315147399902
Validation loss: 2.713439787587812

Epoch: 6| Step: 9
Training loss: 3.228242874145508
Validation loss: 2.7115309392252276

Epoch: 6| Step: 10
Training loss: 3.0167009830474854
Validation loss: 2.707689751860916

Epoch: 6| Step: 11
Training loss: 3.1658310890197754
Validation loss: 2.7025429100118656

Epoch: 6| Step: 12
Training loss: 1.7436894178390503
Validation loss: 2.6993005839727258

Epoch: 6| Step: 13
Training loss: 2.7145607471466064
Validation loss: 2.700573913512691

Epoch: 30| Step: 0
Training loss: 2.4296278953552246
Validation loss: 2.6989070805170203

Epoch: 6| Step: 1
Training loss: 2.841216802597046
Validation loss: 2.699854689259683

Epoch: 6| Step: 2
Training loss: 3.487197160720825
Validation loss: 2.6978680190219673

Epoch: 6| Step: 3
Training loss: 2.975390911102295
Validation loss: 2.696943483045024

Epoch: 6| Step: 4
Training loss: 3.036638021469116
Validation loss: 2.6972498791192168

Epoch: 6| Step: 5
Training loss: 2.566575527191162
Validation loss: 2.696869788631316

Epoch: 6| Step: 6
Training loss: 2.8922278881073
Validation loss: 2.695776565100557

Epoch: 6| Step: 7
Training loss: 3.200547218322754
Validation loss: 2.6943701877388904

Epoch: 6| Step: 8
Training loss: 2.7356441020965576
Validation loss: 2.694126573942041

Epoch: 6| Step: 9
Training loss: 2.8311831951141357
Validation loss: 2.6931662072417555

Epoch: 6| Step: 10
Training loss: 2.8241970539093018
Validation loss: 2.6991727608506397

Epoch: 6| Step: 11
Training loss: 3.1291656494140625
Validation loss: 2.693274600531465

Epoch: 6| Step: 12
Training loss: 2.5446951389312744
Validation loss: 2.6947151076409126

Epoch: 6| Step: 13
Training loss: 2.4439783096313477
Validation loss: 2.691463142312983

Epoch: 31| Step: 0
Training loss: 2.609767436981201
Validation loss: 2.686752273190406

Epoch: 6| Step: 1
Training loss: 2.5832207202911377
Validation loss: 2.6858150805196455

Epoch: 6| Step: 2
Training loss: 2.8728318214416504
Validation loss: 2.6846435403311126

Epoch: 6| Step: 3
Training loss: 2.5195984840393066
Validation loss: 2.684758988759851

Epoch: 6| Step: 4
Training loss: 2.5900163650512695
Validation loss: 2.685713452677573

Epoch: 6| Step: 5
Training loss: 2.36563777923584
Validation loss: 2.687053503528718

Epoch: 6| Step: 6
Training loss: 3.3564090728759766
Validation loss: 2.683559861234439

Epoch: 6| Step: 7
Training loss: 3.091118574142456
Validation loss: 2.6812404560786423

Epoch: 6| Step: 8
Training loss: 3.4222559928894043
Validation loss: 2.679225544775686

Epoch: 6| Step: 9
Training loss: 2.3511085510253906
Validation loss: 2.680676370538691

Epoch: 6| Step: 10
Training loss: 3.3403520584106445
Validation loss: 2.6801516471370572

Epoch: 6| Step: 11
Training loss: 2.8730309009552
Validation loss: 2.680593495727867

Epoch: 6| Step: 12
Training loss: 2.9610891342163086
Validation loss: 2.6770657365040114

Epoch: 6| Step: 13
Training loss: 3.137974262237549
Validation loss: 2.679861525053619

Epoch: 32| Step: 0
Training loss: 2.9374303817749023
Validation loss: 2.6819579908924718

Epoch: 6| Step: 1
Training loss: 2.5022072792053223
Validation loss: 2.687173899783883

Epoch: 6| Step: 2
Training loss: 2.7698075771331787
Validation loss: 2.7015676293321835

Epoch: 6| Step: 3
Training loss: 2.5488271713256836
Validation loss: 2.6985938497768935

Epoch: 6| Step: 4
Training loss: 3.2778990268707275
Validation loss: 2.6863759794542865

Epoch: 6| Step: 5
Training loss: 2.5650548934936523
Validation loss: 2.678756308811967

Epoch: 6| Step: 6
Training loss: 3.8396642208099365
Validation loss: 2.679835622028638

Epoch: 6| Step: 7
Training loss: 2.5367822647094727
Validation loss: 2.676895813275409

Epoch: 6| Step: 8
Training loss: 2.8299367427825928
Validation loss: 2.683361299576298

Epoch: 6| Step: 9
Training loss: 3.1891889572143555
Validation loss: 2.6769773729385866

Epoch: 6| Step: 10
Training loss: 2.3977699279785156
Validation loss: 2.6830638634261263

Epoch: 6| Step: 11
Training loss: 3.0690183639526367
Validation loss: 2.6834856746017293

Epoch: 6| Step: 12
Training loss: 2.5194637775421143
Validation loss: 2.697176456451416

Epoch: 6| Step: 13
Training loss: 2.893033027648926
Validation loss: 2.697763330192976

Epoch: 33| Step: 0
Training loss: 3.0294010639190674
Validation loss: 2.7147785848186863

Epoch: 6| Step: 1
Training loss: 3.6293206214904785
Validation loss: 2.714587450027466

Epoch: 6| Step: 2
Training loss: 2.417876720428467
Validation loss: 2.719988238426947

Epoch: 6| Step: 3
Training loss: 3.9270248413085938
Validation loss: 2.7002648897068475

Epoch: 6| Step: 4
Training loss: 2.735961437225342
Validation loss: 2.6885018348693848

Epoch: 6| Step: 5
Training loss: 2.5062365531921387
Validation loss: 2.6757827369115685

Epoch: 6| Step: 6
Training loss: 3.3470242023468018
Validation loss: 2.6703060852584017

Epoch: 6| Step: 7
Training loss: 2.04122257232666
Validation loss: 2.6675531197619695

Epoch: 6| Step: 8
Training loss: 2.345090866088867
Validation loss: 2.6631389715338267

Epoch: 6| Step: 9
Training loss: 2.6438422203063965
Validation loss: 2.6641724981287473

Epoch: 6| Step: 10
Training loss: 2.785219669342041
Validation loss: 2.6628825715793076

Epoch: 6| Step: 11
Training loss: 2.275897741317749
Validation loss: 2.663744326560728

Epoch: 6| Step: 12
Training loss: 3.4898312091827393
Validation loss: 2.669724174725112

Epoch: 6| Step: 13
Training loss: 2.6341426372528076
Validation loss: 2.676416012548631

Epoch: 34| Step: 0
Training loss: 3.5497055053710938
Validation loss: 2.6719534320216023

Epoch: 6| Step: 1
Training loss: 2.445894718170166
Validation loss: 2.674248303136518

Epoch: 6| Step: 2
Training loss: 3.0338687896728516
Validation loss: 2.668668431620444

Epoch: 6| Step: 3
Training loss: 2.014671802520752
Validation loss: 2.6662337472361903

Epoch: 6| Step: 4
Training loss: 3.0876591205596924
Validation loss: 2.665590191400179

Epoch: 6| Step: 5
Training loss: 3.3723862171173096
Validation loss: 2.6651958675794702

Epoch: 6| Step: 6
Training loss: 2.934406280517578
Validation loss: 2.6595293603917605

Epoch: 6| Step: 7
Training loss: 3.0989434719085693
Validation loss: 2.6622908602478685

Epoch: 6| Step: 8
Training loss: 2.821521759033203
Validation loss: 2.6574769532808693

Epoch: 6| Step: 9
Training loss: 3.110517978668213
Validation loss: 2.6562052439617854

Epoch: 6| Step: 10
Training loss: 2.274930238723755
Validation loss: 2.654467293011245

Epoch: 6| Step: 11
Training loss: 2.8606138229370117
Validation loss: 2.658018345473915

Epoch: 6| Step: 12
Training loss: 2.781907081604004
Validation loss: 2.654760114608272

Epoch: 6| Step: 13
Training loss: 2.021575450897217
Validation loss: 2.65999791314525

Epoch: 35| Step: 0
Training loss: 2.1361196041107178
Validation loss: 2.659554584051973

Epoch: 6| Step: 1
Training loss: 2.8193113803863525
Validation loss: 2.6735747962869625

Epoch: 6| Step: 2
Training loss: 2.2813663482666016
Validation loss: 2.687254023808305

Epoch: 6| Step: 3
Training loss: 3.1337814331054688
Validation loss: 2.711992317630399

Epoch: 6| Step: 4
Training loss: 2.485515594482422
Validation loss: 2.724197059549311

Epoch: 6| Step: 5
Training loss: 2.9476380348205566
Validation loss: 2.7031218518492994

Epoch: 6| Step: 6
Training loss: 2.5689852237701416
Validation loss: 2.676155656896612

Epoch: 6| Step: 7
Training loss: 2.730678081512451
Validation loss: 2.6505295871406473

Epoch: 6| Step: 8
Training loss: 4.003694534301758
Validation loss: 2.660916151538972

Epoch: 6| Step: 9
Training loss: 3.5749213695526123
Validation loss: 2.672492622047342

Epoch: 6| Step: 10
Training loss: 2.4994754791259766
Validation loss: 2.6660295045504006

Epoch: 6| Step: 11
Training loss: 2.862483501434326
Validation loss: 2.6674086611757994

Epoch: 6| Step: 12
Training loss: 2.663989305496216
Validation loss: 2.662913035320979

Epoch: 6| Step: 13
Training loss: 3.2657406330108643
Validation loss: 2.6610671781724498

Epoch: 36| Step: 0
Training loss: 3.484618663787842
Validation loss: 2.652903264568698

Epoch: 6| Step: 1
Training loss: 2.5131382942199707
Validation loss: 2.6554323011829006

Epoch: 6| Step: 2
Training loss: 2.1368181705474854
Validation loss: 2.65684784612348

Epoch: 6| Step: 3
Training loss: 3.202118158340454
Validation loss: 2.6559383100078953

Epoch: 6| Step: 4
Training loss: 2.3180441856384277
Validation loss: 2.6539312024270334

Epoch: 6| Step: 5
Training loss: 2.0642497539520264
Validation loss: 2.6576301795180126

Epoch: 6| Step: 6
Training loss: 3.579195261001587
Validation loss: 2.6597890212971675

Epoch: 6| Step: 7
Training loss: 2.9962759017944336
Validation loss: 2.6634178956349692

Epoch: 6| Step: 8
Training loss: 2.623448371887207
Validation loss: 2.678867627215642

Epoch: 6| Step: 9
Training loss: 2.5419564247131348
Validation loss: 2.739343217624131

Epoch: 6| Step: 10
Training loss: 4.044225215911865
Validation loss: 2.7852827605380805

Epoch: 6| Step: 11
Training loss: 3.1111667156219482
Validation loss: 2.745928813052434

Epoch: 6| Step: 12
Training loss: 2.304111957550049
Validation loss: 2.683453393238847

Epoch: 6| Step: 13
Training loss: 3.099097967147827
Validation loss: 2.6481787953325497

Epoch: 37| Step: 0
Training loss: 3.1886279582977295
Validation loss: 2.6456584263873357

Epoch: 6| Step: 1
Training loss: 2.3187098503112793
Validation loss: 2.649255152671568

Epoch: 6| Step: 2
Training loss: 2.4316751956939697
Validation loss: 2.655705839075068

Epoch: 6| Step: 3
Training loss: 3.188910484313965
Validation loss: 2.6545040658725205

Epoch: 6| Step: 4
Training loss: 2.4637181758880615
Validation loss: 2.6607483125502065

Epoch: 6| Step: 5
Training loss: 2.7761425971984863
Validation loss: 2.6625689973113356

Epoch: 6| Step: 6
Training loss: 2.9655537605285645
Validation loss: 2.659700862822994

Epoch: 6| Step: 7
Training loss: 3.373297929763794
Validation loss: 2.6682693830100437

Epoch: 6| Step: 8
Training loss: 2.691175937652588
Validation loss: 2.666211102598457

Epoch: 6| Step: 9
Training loss: 2.671565055847168
Validation loss: 2.6704767596337105

Epoch: 6| Step: 10
Training loss: 3.2145609855651855
Validation loss: 2.676970789509435

Epoch: 6| Step: 11
Training loss: 2.3316922187805176
Validation loss: 2.6716857571755686

Epoch: 6| Step: 12
Training loss: 3.4096813201904297
Validation loss: 2.679826216031146

Epoch: 6| Step: 13
Training loss: 2.724809169769287
Validation loss: 2.6800671905599613

Epoch: 38| Step: 0
Training loss: 2.182955265045166
Validation loss: 2.673423856817266

Epoch: 6| Step: 1
Training loss: 2.451382637023926
Validation loss: 2.6688016588969896

Epoch: 6| Step: 2
Training loss: 3.390397071838379
Validation loss: 2.6746692990743988

Epoch: 6| Step: 3
Training loss: 2.6585986614227295
Validation loss: 2.677405241997011

Epoch: 6| Step: 4
Training loss: 3.6797938346862793
Validation loss: 2.666785091482183

Epoch: 6| Step: 5
Training loss: 3.756129503250122
Validation loss: 2.6652509191984772

Epoch: 6| Step: 6
Training loss: 3.101095676422119
Validation loss: 2.65202828889252

Epoch: 6| Step: 7
Training loss: 1.8138351440429688
Validation loss: 2.6482276660139843

Epoch: 6| Step: 8
Training loss: 2.835946559906006
Validation loss: 2.643824702949934

Epoch: 6| Step: 9
Training loss: 2.7679691314697266
Validation loss: 2.647782282162738

Epoch: 6| Step: 10
Training loss: 2.8176844120025635
Validation loss: 2.6430671343239407

Epoch: 6| Step: 11
Training loss: 2.6897449493408203
Validation loss: 2.6506855846733175

Epoch: 6| Step: 12
Training loss: 2.5320520401000977
Validation loss: 2.6503106291576097

Epoch: 6| Step: 13
Training loss: 3.092625379562378
Validation loss: 2.6464352351362987

Epoch: 39| Step: 0
Training loss: 1.9794787168502808
Validation loss: 2.6466688212528022

Epoch: 6| Step: 1
Training loss: 2.8010945320129395
Validation loss: 2.6411171908019693

Epoch: 6| Step: 2
Training loss: 2.357813835144043
Validation loss: 2.6348480140009234

Epoch: 6| Step: 3
Training loss: 3.3733038902282715
Validation loss: 2.636080857246153

Epoch: 6| Step: 4
Training loss: 2.4172680377960205
Validation loss: 2.635377219928208

Epoch: 6| Step: 5
Training loss: 3.0248217582702637
Validation loss: 2.635451675743185

Epoch: 6| Step: 6
Training loss: 3.4821126461029053
Validation loss: 2.639465501231532

Epoch: 6| Step: 7
Training loss: 2.605984687805176
Validation loss: 2.6333662720136743

Epoch: 6| Step: 8
Training loss: 3.6688690185546875
Validation loss: 2.634614436857162

Epoch: 6| Step: 9
Training loss: 2.347588062286377
Validation loss: 2.640711551071495

Epoch: 6| Step: 10
Training loss: 2.8309426307678223
Validation loss: 2.6551300018064437

Epoch: 6| Step: 11
Training loss: 3.136674165725708
Validation loss: 2.654393019214753

Epoch: 6| Step: 12
Training loss: 2.5067286491394043
Validation loss: 2.638260623460175

Epoch: 6| Step: 13
Training loss: 3.2202911376953125
Validation loss: 2.6303871216312533

Epoch: 40| Step: 0
Training loss: 2.4381580352783203
Validation loss: 2.6276125779715915

Epoch: 6| Step: 1
Training loss: 2.9815638065338135
Validation loss: 2.6248124132874193

Epoch: 6| Step: 2
Training loss: 2.8799214363098145
Validation loss: 2.6241304054055163

Epoch: 6| Step: 3
Training loss: 2.0835490226745605
Validation loss: 2.6223410637147966

Epoch: 6| Step: 4
Training loss: 3.16440486907959
Validation loss: 2.619006338939872

Epoch: 6| Step: 5
Training loss: 2.5655531883239746
Validation loss: 2.6216494780714794

Epoch: 6| Step: 6
Training loss: 2.7765307426452637
Validation loss: 2.6213554207996657

Epoch: 6| Step: 7
Training loss: 3.196570873260498
Validation loss: 2.6197058129054245

Epoch: 6| Step: 8
Training loss: 3.2229628562927246
Validation loss: 2.6198036234865905

Epoch: 6| Step: 9
Training loss: 2.269988775253296
Validation loss: 2.614083884864725

Epoch: 6| Step: 10
Training loss: 2.512967586517334
Validation loss: 2.6182820156056392

Epoch: 6| Step: 11
Training loss: 3.088520050048828
Validation loss: 2.6183292917025986

Epoch: 6| Step: 12
Training loss: 3.339566230773926
Validation loss: 2.6168011234652613

Epoch: 6| Step: 13
Training loss: 2.8357748985290527
Validation loss: 2.617871561358052

Epoch: 41| Step: 0
Training loss: 2.713850259780884
Validation loss: 2.6241086708602084

Epoch: 6| Step: 1
Training loss: 2.517475128173828
Validation loss: 2.635471756740283

Epoch: 6| Step: 2
Training loss: 3.1554627418518066
Validation loss: 2.653004495046472

Epoch: 6| Step: 3
Training loss: 3.4125418663024902
Validation loss: 2.668337065686462

Epoch: 6| Step: 4
Training loss: 2.7610154151916504
Validation loss: 2.6500363016641266

Epoch: 6| Step: 5
Training loss: 3.238128900527954
Validation loss: 2.6161572471741708

Epoch: 6| Step: 6
Training loss: 2.8780651092529297
Validation loss: 2.612212327218825

Epoch: 6| Step: 7
Training loss: 3.324824333190918
Validation loss: 2.6170300822104178

Epoch: 6| Step: 8
Training loss: 3.0134530067443848
Validation loss: 2.6197104812950216

Epoch: 6| Step: 9
Training loss: 2.211768627166748
Validation loss: 2.622807115636846

Epoch: 6| Step: 10
Training loss: 2.2833876609802246
Validation loss: 2.6279571210184405

Epoch: 6| Step: 11
Training loss: 2.4852294921875
Validation loss: 2.620973192235475

Epoch: 6| Step: 12
Training loss: 2.2028822898864746
Validation loss: 2.6245690212454846

Epoch: 6| Step: 13
Training loss: 3.665471076965332
Validation loss: 2.6327012021054506

Epoch: 42| Step: 0
Training loss: 3.00887393951416
Validation loss: 2.6264857707485074

Epoch: 6| Step: 1
Training loss: 2.8558363914489746
Validation loss: 2.6241370016528713

Epoch: 6| Step: 2
Training loss: 2.7878670692443848
Validation loss: 2.6191028856462046

Epoch: 6| Step: 3
Training loss: 2.65023136138916
Validation loss: 2.6155686993752756

Epoch: 6| Step: 4
Training loss: 3.1879940032958984
Validation loss: 2.6145559869786745

Epoch: 6| Step: 5
Training loss: 3.35718035697937
Validation loss: 2.612672241785193

Epoch: 6| Step: 6
Training loss: 2.373617649078369
Validation loss: 2.6137272721977642

Epoch: 6| Step: 7
Training loss: 2.538745164871216
Validation loss: 2.6170529498848865

Epoch: 6| Step: 8
Training loss: 2.2915658950805664
Validation loss: 2.6295659080628426

Epoch: 6| Step: 9
Training loss: 2.5167598724365234
Validation loss: 2.639718340289208

Epoch: 6| Step: 10
Training loss: 2.527963161468506
Validation loss: 2.655812414743567

Epoch: 6| Step: 11
Training loss: 3.3142242431640625
Validation loss: 2.6825009571608676

Epoch: 6| Step: 12
Training loss: 2.5157532691955566
Validation loss: 2.649282757953931

Epoch: 6| Step: 13
Training loss: 4.016826629638672
Validation loss: 2.6285574026005243

Epoch: 43| Step: 0
Training loss: 3.0784802436828613
Validation loss: 2.611943506425427

Epoch: 6| Step: 1
Training loss: 2.3435728549957275
Validation loss: 2.6095612305466847

Epoch: 6| Step: 2
Training loss: 2.058526039123535
Validation loss: 2.6133447359966975

Epoch: 6| Step: 3
Training loss: 2.8628616333007812
Validation loss: 2.6150247435415945

Epoch: 6| Step: 4
Training loss: 2.338972568511963
Validation loss: 2.6186876963543635

Epoch: 6| Step: 5
Training loss: 3.2019290924072266
Validation loss: 2.6161354177741596

Epoch: 6| Step: 6
Training loss: 3.1439061164855957
Validation loss: 2.613620881111391

Epoch: 6| Step: 7
Training loss: 3.1778876781463623
Validation loss: 2.6092027387311383

Epoch: 6| Step: 8
Training loss: 2.542675256729126
Validation loss: 2.6083358539048063

Epoch: 6| Step: 9
Training loss: 3.176032781600952
Validation loss: 2.6094030231557865

Epoch: 6| Step: 10
Training loss: 2.5281286239624023
Validation loss: 2.606504124979819

Epoch: 6| Step: 11
Training loss: 2.324338912963867
Validation loss: 2.604610496951688

Epoch: 6| Step: 12
Training loss: 2.9282948970794678
Validation loss: 2.6059696289800827

Epoch: 6| Step: 13
Training loss: 4.112806797027588
Validation loss: 2.5992913092336347

Epoch: 44| Step: 0
Training loss: 3.335686683654785
Validation loss: 2.6032014200764317

Epoch: 6| Step: 1
Training loss: 3.1393346786499023
Validation loss: 2.6015233967893865

Epoch: 6| Step: 2
Training loss: 2.679394483566284
Validation loss: 2.6033472784103884

Epoch: 6| Step: 3
Training loss: 2.7174036502838135
Validation loss: 2.6020316205998903

Epoch: 6| Step: 4
Training loss: 2.862424850463867
Validation loss: 2.6003481700856197

Epoch: 6| Step: 5
Training loss: 2.6332247257232666
Validation loss: 2.6013896388392292

Epoch: 6| Step: 6
Training loss: 3.129596471786499
Validation loss: 2.597053327868062

Epoch: 6| Step: 7
Training loss: 2.710150718688965
Validation loss: 2.5961251258850098

Epoch: 6| Step: 8
Training loss: 2.4633288383483887
Validation loss: 2.598173487570978

Epoch: 6| Step: 9
Training loss: 2.50357723236084
Validation loss: 2.599358161290487

Epoch: 6| Step: 10
Training loss: 3.456591844558716
Validation loss: 2.5980723878388763

Epoch: 6| Step: 11
Training loss: 2.793426990509033
Validation loss: 2.5982362865119852

Epoch: 6| Step: 12
Training loss: 2.2926783561706543
Validation loss: 2.5985608921256116

Epoch: 6| Step: 13
Training loss: 1.9993767738342285
Validation loss: 2.5994009048708024

Epoch: 45| Step: 0
Training loss: 1.8371111154556274
Validation loss: 2.597383604254774

Epoch: 6| Step: 1
Training loss: 2.717357635498047
Validation loss: 2.6003718247977634

Epoch: 6| Step: 2
Training loss: 2.8162200450897217
Validation loss: 2.603621708449497

Epoch: 6| Step: 3
Training loss: 2.984179973602295
Validation loss: 2.5993554258859284

Epoch: 6| Step: 4
Training loss: 2.3227181434631348
Validation loss: 2.5993976490471953

Epoch: 6| Step: 5
Training loss: 3.423180341720581
Validation loss: 2.5959184272314912

Epoch: 6| Step: 6
Training loss: 2.5689191818237305
Validation loss: 2.598809011520878

Epoch: 6| Step: 7
Training loss: 3.089812755584717
Validation loss: 2.599921654629451

Epoch: 6| Step: 8
Training loss: 2.6587252616882324
Validation loss: 2.5983458078035744

Epoch: 6| Step: 9
Training loss: 3.4965920448303223
Validation loss: 2.5931429016974663

Epoch: 6| Step: 10
Training loss: 3.7255778312683105
Validation loss: 2.593133585427397

Epoch: 6| Step: 11
Training loss: 2.3108410835266113
Validation loss: 2.592712458743844

Epoch: 6| Step: 12
Training loss: 2.7160446643829346
Validation loss: 2.592744411960725

Epoch: 6| Step: 13
Training loss: 1.9795502424240112
Validation loss: 2.592784766227968

Epoch: 46| Step: 0
Training loss: 3.0394821166992188
Validation loss: 2.5992507626933437

Epoch: 6| Step: 1
Training loss: 2.5633881092071533
Validation loss: 2.6000754192311275

Epoch: 6| Step: 2
Training loss: 2.514238119125366
Validation loss: 2.605351630077567

Epoch: 6| Step: 3
Training loss: 3.1003544330596924
Validation loss: 2.6106903399190595

Epoch: 6| Step: 4
Training loss: 2.3226094245910645
Validation loss: 2.613675994257773

Epoch: 6| Step: 5
Training loss: 3.2916300296783447
Validation loss: 2.6086842039579987

Epoch: 6| Step: 6
Training loss: 3.2227377891540527
Validation loss: 2.5961253361035417

Epoch: 6| Step: 7
Training loss: 2.804965019226074
Validation loss: 2.590438404390889

Epoch: 6| Step: 8
Training loss: 2.958448886871338
Validation loss: 2.58611032014252

Epoch: 6| Step: 9
Training loss: 2.2875285148620605
Validation loss: 2.5857308833829817

Epoch: 6| Step: 10
Training loss: 2.7408828735351562
Validation loss: 2.585283012800319

Epoch: 6| Step: 11
Training loss: 2.260061025619507
Validation loss: 2.590246505634759

Epoch: 6| Step: 12
Training loss: 3.310743570327759
Validation loss: 2.585308131351266

Epoch: 6| Step: 13
Training loss: 2.4638333320617676
Validation loss: 2.587143054572485

Epoch: 47| Step: 0
Training loss: 3.084908962249756
Validation loss: 2.581142258900468

Epoch: 6| Step: 1
Training loss: 2.445502758026123
Validation loss: 2.586559280272453

Epoch: 6| Step: 2
Training loss: 2.593256711959839
Validation loss: 2.5879150846953034

Epoch: 6| Step: 3
Training loss: 3.2947635650634766
Validation loss: 2.5813934059553247

Epoch: 6| Step: 4
Training loss: 3.038733720779419
Validation loss: 2.587118289803946

Epoch: 6| Step: 5
Training loss: 2.8034474849700928
Validation loss: 2.5871334306655394

Epoch: 6| Step: 6
Training loss: 3.229604721069336
Validation loss: 2.585208410857826

Epoch: 6| Step: 7
Training loss: 2.559244155883789
Validation loss: 2.591286879713817

Epoch: 6| Step: 8
Training loss: 2.299874782562256
Validation loss: 2.5854166656412105

Epoch: 6| Step: 9
Training loss: 3.1167855262756348
Validation loss: 2.5844393161035355

Epoch: 6| Step: 10
Training loss: 3.0613253116607666
Validation loss: 2.5802826163589314

Epoch: 6| Step: 11
Training loss: 2.4289190769195557
Validation loss: 2.5855338240182526

Epoch: 6| Step: 12
Training loss: 2.318934440612793
Validation loss: 2.5846984694080968

Epoch: 6| Step: 13
Training loss: 2.471418619155884
Validation loss: 2.5901695579610844

Epoch: 48| Step: 0
Training loss: 2.2003142833709717
Validation loss: 2.5816041551610476

Epoch: 6| Step: 1
Training loss: 3.184516429901123
Validation loss: 2.578014273797312

Epoch: 6| Step: 2
Training loss: 3.201626777648926
Validation loss: 2.5837397113923104

Epoch: 6| Step: 3
Training loss: 2.5205605030059814
Validation loss: 2.577669125731273

Epoch: 6| Step: 4
Training loss: 3.507225751876831
Validation loss: 2.5808018433150424

Epoch: 6| Step: 5
Training loss: 2.254930257797241
Validation loss: 2.5796507635424213

Epoch: 6| Step: 6
Training loss: 2.9838414192199707
Validation loss: 2.578451761635401

Epoch: 6| Step: 7
Training loss: 2.9821369647979736
Validation loss: 2.5842747790839082

Epoch: 6| Step: 8
Training loss: 2.2787840366363525
Validation loss: 2.5769331429594304

Epoch: 6| Step: 9
Training loss: 2.592712879180908
Validation loss: 2.5776817875523723

Epoch: 6| Step: 10
Training loss: 4.056697845458984
Validation loss: 2.585874706186274

Epoch: 6| Step: 11
Training loss: 2.2233142852783203
Validation loss: 2.578615568017447

Epoch: 6| Step: 12
Training loss: 2.9154837131500244
Validation loss: 2.578012938140541

Epoch: 6| Step: 13
Training loss: 1.2297873497009277
Validation loss: 2.5798463641956286

Epoch: 49| Step: 0
Training loss: 3.461216688156128
Validation loss: 2.5813733993038053

Epoch: 6| Step: 1
Training loss: 2.572660446166992
Validation loss: 2.5860881574692263

Epoch: 6| Step: 2
Training loss: 2.9362452030181885
Validation loss: 2.593102498721051

Epoch: 6| Step: 3
Training loss: 2.5404245853424072
Validation loss: 2.5783039164799515

Epoch: 6| Step: 4
Training loss: 3.2490968704223633
Validation loss: 2.577159022772184

Epoch: 6| Step: 5
Training loss: 2.412726402282715
Validation loss: 2.5804412954597065

Epoch: 6| Step: 6
Training loss: 2.385399341583252
Validation loss: 2.5747842173422537

Epoch: 6| Step: 7
Training loss: 1.689692497253418
Validation loss: 2.5747711273931686

Epoch: 6| Step: 8
Training loss: 3.5101451873779297
Validation loss: 2.5786964329340125

Epoch: 6| Step: 9
Training loss: 2.7673580646514893
Validation loss: 2.5726740744806107

Epoch: 6| Step: 10
Training loss: 3.280306816101074
Validation loss: 2.5778265896663872

Epoch: 6| Step: 11
Training loss: 2.6767287254333496
Validation loss: 2.577684335811164

Epoch: 6| Step: 12
Training loss: 2.912743091583252
Validation loss: 2.5776384107528196

Epoch: 6| Step: 13
Training loss: 2.177865743637085
Validation loss: 2.579569287197564

Epoch: 50| Step: 0
Training loss: 2.4640309810638428
Validation loss: 2.5775172582236667

Epoch: 6| Step: 1
Training loss: 2.495415687561035
Validation loss: 2.5783449424210416

Epoch: 6| Step: 2
Training loss: 2.9698972702026367
Validation loss: 2.582761485089538

Epoch: 6| Step: 3
Training loss: 2.5652098655700684
Validation loss: 2.5796104605479906

Epoch: 6| Step: 4
Training loss: 2.9715538024902344
Validation loss: 2.5864329030436854

Epoch: 6| Step: 5
Training loss: 2.867264747619629
Validation loss: 2.5921272641869

Epoch: 6| Step: 6
Training loss: 2.6725471019744873
Validation loss: 2.5994541952686925

Epoch: 6| Step: 7
Training loss: 2.134842872619629
Validation loss: 2.583879332388601

Epoch: 6| Step: 8
Training loss: 2.7282297611236572
Validation loss: 2.583297662837531

Epoch: 6| Step: 9
Training loss: 2.8891310691833496
Validation loss: 2.5813275767910864

Epoch: 6| Step: 10
Training loss: 2.902252674102783
Validation loss: 2.5784387332136913

Epoch: 6| Step: 11
Training loss: 2.755811929702759
Validation loss: 2.5715339311989407

Epoch: 6| Step: 12
Training loss: 3.398319721221924
Validation loss: 2.563855891586632

Epoch: 6| Step: 13
Training loss: 3.002666711807251
Validation loss: 2.5657877306784354

Epoch: 51| Step: 0
Training loss: 1.8430256843566895
Validation loss: 2.567369358513945

Epoch: 6| Step: 1
Training loss: 3.561661720275879
Validation loss: 2.56626041986609

Epoch: 6| Step: 2
Training loss: 2.811223268508911
Validation loss: 2.568743992877263

Epoch: 6| Step: 3
Training loss: 3.2169995307922363
Validation loss: 2.5756561576679187

Epoch: 6| Step: 4
Training loss: 2.2420666217803955
Validation loss: 2.564892522750362

Epoch: 6| Step: 5
Training loss: 2.8266875743865967
Validation loss: 2.566940374271844

Epoch: 6| Step: 6
Training loss: 3.1670656204223633
Validation loss: 2.561308090404798

Epoch: 6| Step: 7
Training loss: 2.789644956588745
Validation loss: 2.5623764197031655

Epoch: 6| Step: 8
Training loss: 3.4781110286712646
Validation loss: 2.5653627559702885

Epoch: 6| Step: 9
Training loss: 3.1374735832214355
Validation loss: 2.569898843765259

Epoch: 6| Step: 10
Training loss: 2.164224863052368
Validation loss: 2.5662580895167526

Epoch: 6| Step: 11
Training loss: 2.55147123336792
Validation loss: 2.5605886649059992

Epoch: 6| Step: 12
Training loss: 2.482334613800049
Validation loss: 2.562506047628259

Epoch: 6| Step: 13
Training loss: 2.0820581912994385
Validation loss: 2.559047834847563

Epoch: 52| Step: 0
Training loss: 2.4580307006835938
Validation loss: 2.5657417646018406

Epoch: 6| Step: 1
Training loss: 2.990480422973633
Validation loss: 2.577610754197644

Epoch: 6| Step: 2
Training loss: 3.3460559844970703
Validation loss: 2.588063209287582

Epoch: 6| Step: 3
Training loss: 2.837249279022217
Validation loss: 2.5966658002586773

Epoch: 6| Step: 4
Training loss: 3.2925405502319336
Validation loss: 2.5893682420894666

Epoch: 6| Step: 5
Training loss: 3.1137771606445312
Validation loss: 2.58627510839893

Epoch: 6| Step: 6
Training loss: 2.555739402770996
Validation loss: 2.5723925393114806

Epoch: 6| Step: 7
Training loss: 2.569115161895752
Validation loss: 2.56269496487033

Epoch: 6| Step: 8
Training loss: 2.2520291805267334
Validation loss: 2.5602134402080248

Epoch: 6| Step: 9
Training loss: 2.408290386199951
Validation loss: 2.55805698005102

Epoch: 6| Step: 10
Training loss: 2.2255492210388184
Validation loss: 2.5571641255450506

Epoch: 6| Step: 11
Training loss: 3.7004380226135254
Validation loss: 2.5566888650258384

Epoch: 6| Step: 12
Training loss: 2.5765647888183594
Validation loss: 2.559901075978433

Epoch: 6| Step: 13
Training loss: 1.9543241262435913
Validation loss: 2.562209794598241

Epoch: 53| Step: 0
Training loss: 2.069322109222412
Validation loss: 2.5542526116935154

Epoch: 6| Step: 1
Training loss: 2.4785025119781494
Validation loss: 2.5574849369705364

Epoch: 6| Step: 2
Training loss: 3.2274537086486816
Validation loss: 2.5556445429402013

Epoch: 6| Step: 3
Training loss: 2.566194772720337
Validation loss: 2.5562822639301257

Epoch: 6| Step: 4
Training loss: 2.9078140258789062
Validation loss: 2.5585355886849026

Epoch: 6| Step: 5
Training loss: 2.5074989795684814
Validation loss: 2.5583287028856176

Epoch: 6| Step: 6
Training loss: 2.616213321685791
Validation loss: 2.561979668114775

Epoch: 6| Step: 7
Training loss: 2.839294672012329
Validation loss: 2.569322524532195

Epoch: 6| Step: 8
Training loss: 2.760221004486084
Validation loss: 2.564249864188574

Epoch: 6| Step: 9
Training loss: 2.3483612537384033
Validation loss: 2.5613037334975375

Epoch: 6| Step: 10
Training loss: 2.411088705062866
Validation loss: 2.551921195881341

Epoch: 6| Step: 11
Training loss: 3.495187759399414
Validation loss: 2.5559127792235343

Epoch: 6| Step: 12
Training loss: 3.7769625186920166
Validation loss: 2.552202158076789

Epoch: 6| Step: 13
Training loss: 2.164944648742676
Validation loss: 2.552178357237129

Epoch: 54| Step: 0
Training loss: 2.7604970932006836
Validation loss: 2.5576153339878207

Epoch: 6| Step: 1
Training loss: 2.8775687217712402
Validation loss: 2.5574175593673543

Epoch: 6| Step: 2
Training loss: 2.089388847351074
Validation loss: 2.5592576585790163

Epoch: 6| Step: 3
Training loss: 2.9884889125823975
Validation loss: 2.561887623161398

Epoch: 6| Step: 4
Training loss: 3.230280637741089
Validation loss: 2.5451343008266982

Epoch: 6| Step: 5
Training loss: 3.1446735858917236
Validation loss: 2.540137457591231

Epoch: 6| Step: 6
Training loss: 2.745817184448242
Validation loss: 2.5429317079564577

Epoch: 6| Step: 7
Training loss: 2.84767484664917
Validation loss: 2.5407939572488107

Epoch: 6| Step: 8
Training loss: 2.8271732330322266
Validation loss: 2.5407415615615023

Epoch: 6| Step: 9
Training loss: 3.2562267780303955
Validation loss: 2.5388439688631284

Epoch: 6| Step: 10
Training loss: 2.396242380142212
Validation loss: 2.5379394844014156

Epoch: 6| Step: 11
Training loss: 2.1901910305023193
Validation loss: 2.5386767592481387

Epoch: 6| Step: 12
Training loss: 2.4525911808013916
Validation loss: 2.542297940100393

Epoch: 6| Step: 13
Training loss: 2.552302360534668
Validation loss: 2.5405061398783038

Epoch: 55| Step: 0
Training loss: 2.443730354309082
Validation loss: 2.548003940172093

Epoch: 6| Step: 1
Training loss: 2.5099573135375977
Validation loss: 2.553827706203666

Epoch: 6| Step: 2
Training loss: 2.762247323989868
Validation loss: 2.5583133979510237

Epoch: 6| Step: 3
Training loss: 2.844508171081543
Validation loss: 2.5592421972623436

Epoch: 6| Step: 4
Training loss: 2.448193073272705
Validation loss: 2.555184277155066

Epoch: 6| Step: 5
Training loss: 2.696539878845215
Validation loss: 2.5558107873444915

Epoch: 6| Step: 6
Training loss: 2.51478910446167
Validation loss: 2.5474510167234685

Epoch: 6| Step: 7
Training loss: 2.854835271835327
Validation loss: 2.5502332051595054

Epoch: 6| Step: 8
Training loss: 3.406653881072998
Validation loss: 2.5510768403289137

Epoch: 6| Step: 9
Training loss: 3.385678768157959
Validation loss: 2.5519564510673605

Epoch: 6| Step: 10
Training loss: 2.7840702533721924
Validation loss: 2.5492498028662895

Epoch: 6| Step: 11
Training loss: 2.134317636489868
Validation loss: 2.5409330244987243

Epoch: 6| Step: 12
Training loss: 2.789368152618408
Validation loss: 2.539947150855936

Epoch: 6| Step: 13
Training loss: 2.748257875442505
Validation loss: 2.5362762533208376

Epoch: 56| Step: 0
Training loss: 2.5127100944519043
Validation loss: 2.5382704939893497

Epoch: 6| Step: 1
Training loss: 3.374305248260498
Validation loss: 2.5412140379669848

Epoch: 6| Step: 2
Training loss: 2.7771971225738525
Validation loss: 2.5377077543607323

Epoch: 6| Step: 3
Training loss: 2.9239330291748047
Validation loss: 2.534476823704217

Epoch: 6| Step: 4
Training loss: 3.513096332550049
Validation loss: 2.531926937000726

Epoch: 6| Step: 5
Training loss: 2.021421432495117
Validation loss: 2.5359857159276165

Epoch: 6| Step: 6
Training loss: 2.7102861404418945
Validation loss: 2.5344796283270723

Epoch: 6| Step: 7
Training loss: 2.517669200897217
Validation loss: 2.533211088949634

Epoch: 6| Step: 8
Training loss: 2.639918088912964
Validation loss: 2.5324660449899654

Epoch: 6| Step: 9
Training loss: 2.777766704559326
Validation loss: 2.5371978590565343

Epoch: 6| Step: 10
Training loss: 2.3495140075683594
Validation loss: 2.534985419242613

Epoch: 6| Step: 11
Training loss: 2.8945047855377197
Validation loss: 2.541025028433851

Epoch: 6| Step: 12
Training loss: 2.3680014610290527
Validation loss: 2.5412431070881505

Epoch: 6| Step: 13
Training loss: 3.0689940452575684
Validation loss: 2.541631829354071

Epoch: 57| Step: 0
Training loss: 2.8092591762542725
Validation loss: 2.5490359003825853

Epoch: 6| Step: 1
Training loss: 3.105081558227539
Validation loss: 2.548000753566783

Epoch: 6| Step: 2
Training loss: 2.8020009994506836
Validation loss: 2.5450091643999984

Epoch: 6| Step: 3
Training loss: 2.055778741836548
Validation loss: 2.545396322845131

Epoch: 6| Step: 4
Training loss: 3.4507269859313965
Validation loss: 2.5439825006710586

Epoch: 6| Step: 5
Training loss: 2.154869556427002
Validation loss: 2.5483789469606135

Epoch: 6| Step: 6
Training loss: 2.534909725189209
Validation loss: 2.544874587366658

Epoch: 6| Step: 7
Training loss: 2.7143921852111816
Validation loss: 2.537699079000822

Epoch: 6| Step: 8
Training loss: 2.2488770484924316
Validation loss: 2.541624258923274

Epoch: 6| Step: 9
Training loss: 3.0995676517486572
Validation loss: 2.5340889935852378

Epoch: 6| Step: 10
Training loss: 3.408022880554199
Validation loss: 2.535376489803355

Epoch: 6| Step: 11
Training loss: 2.4461922645568848
Validation loss: 2.5252621071313017

Epoch: 6| Step: 12
Training loss: 1.896113395690918
Validation loss: 2.521678073431856

Epoch: 6| Step: 13
Training loss: 4.138241767883301
Validation loss: 2.5294188940396873

Epoch: 58| Step: 0
Training loss: 2.171599864959717
Validation loss: 2.5287359991381244

Epoch: 6| Step: 1
Training loss: 3.1084203720092773
Validation loss: 2.5268499235953055

Epoch: 6| Step: 2
Training loss: 2.5657150745391846
Validation loss: 2.5267523001599055

Epoch: 6| Step: 3
Training loss: 2.893014669418335
Validation loss: 2.52292569991081

Epoch: 6| Step: 4
Training loss: 2.554492950439453
Validation loss: 2.5262452607513755

Epoch: 6| Step: 5
Training loss: 2.1322262287139893
Validation loss: 2.5205637101204164

Epoch: 6| Step: 6
Training loss: 2.8276705741882324
Validation loss: 2.5264410126593804

Epoch: 6| Step: 7
Training loss: 2.6138405799865723
Validation loss: 2.525697587638773

Epoch: 6| Step: 8
Training loss: 2.7175490856170654
Validation loss: 2.5271212183019167

Epoch: 6| Step: 9
Training loss: 3.375227928161621
Validation loss: 2.5320916662934008

Epoch: 6| Step: 10
Training loss: 2.860227584838867
Validation loss: 2.5323135750268095

Epoch: 6| Step: 11
Training loss: 2.583690643310547
Validation loss: 2.536773766240766

Epoch: 6| Step: 12
Training loss: 2.4365053176879883
Validation loss: 2.5316050873007825

Epoch: 6| Step: 13
Training loss: 3.804468870162964
Validation loss: 2.5383920951556136

Epoch: 59| Step: 0
Training loss: 1.9772684574127197
Validation loss: 2.547525013646772

Epoch: 6| Step: 1
Training loss: 3.023785352706909
Validation loss: 2.553521474202474

Epoch: 6| Step: 2
Training loss: 3.033278703689575
Validation loss: 2.554574974121586

Epoch: 6| Step: 3
Training loss: 3.2183542251586914
Validation loss: 2.5527489928789038

Epoch: 6| Step: 4
Training loss: 3.2841603755950928
Validation loss: 2.554618345793857

Epoch: 6| Step: 5
Training loss: 2.5569024085998535
Validation loss: 2.535801984930551

Epoch: 6| Step: 6
Training loss: 2.6474742889404297
Validation loss: 2.531494909717191

Epoch: 6| Step: 7
Training loss: 2.8423681259155273
Validation loss: 2.525845296921269

Epoch: 6| Step: 8
Training loss: 3.1654584407806396
Validation loss: 2.5258443240196473

Epoch: 6| Step: 9
Training loss: 2.283046245574951
Validation loss: 2.5252096678621028

Epoch: 6| Step: 10
Training loss: 2.7005155086517334
Validation loss: 2.5246319027357202

Epoch: 6| Step: 11
Training loss: 2.855914354324341
Validation loss: 2.516477108001709

Epoch: 6| Step: 12
Training loss: 2.0091569423675537
Validation loss: 2.520112204295333

Epoch: 6| Step: 13
Training loss: 2.444178581237793
Validation loss: 2.523015535005959

Epoch: 60| Step: 0
Training loss: 2.9243292808532715
Validation loss: 2.517668024186165

Epoch: 6| Step: 1
Training loss: 2.014514923095703
Validation loss: 2.5205953890277493

Epoch: 6| Step: 2
Training loss: 3.1327462196350098
Validation loss: 2.523261067687824

Epoch: 6| Step: 3
Training loss: 2.763063907623291
Validation loss: 2.520444041939192

Epoch: 6| Step: 4
Training loss: 3.2059271335601807
Validation loss: 2.52111070130461

Epoch: 6| Step: 5
Training loss: 2.473090887069702
Validation loss: 2.524217346663116

Epoch: 6| Step: 6
Training loss: 3.086301803588867
Validation loss: 2.528003866954516

Epoch: 6| Step: 7
Training loss: 2.594646692276001
Validation loss: 2.52628485361735

Epoch: 6| Step: 8
Training loss: 2.5999398231506348
Validation loss: 2.5325389844115063

Epoch: 6| Step: 9
Training loss: 2.8512039184570312
Validation loss: 2.5334250696243776

Epoch: 6| Step: 10
Training loss: 2.3471693992614746
Validation loss: 2.546340616800452

Epoch: 6| Step: 11
Training loss: 2.450453281402588
Validation loss: 2.5447588530919885

Epoch: 6| Step: 12
Training loss: 2.987636089324951
Validation loss: 2.5351074639187066

Epoch: 6| Step: 13
Training loss: 2.6731436252593994
Validation loss: 2.5225464528606785

Epoch: 61| Step: 0
Training loss: 2.73974609375
Validation loss: 2.5173393603294127

Epoch: 6| Step: 1
Training loss: 2.2985024452209473
Validation loss: 2.5166924589423725

Epoch: 6| Step: 2
Training loss: 2.8988029956817627
Validation loss: 2.5169088225210867

Epoch: 6| Step: 3
Training loss: 2.2726330757141113
Validation loss: 2.515069043764504

Epoch: 6| Step: 4
Training loss: 2.294909954071045
Validation loss: 2.5125722654404177

Epoch: 6| Step: 5
Training loss: 2.6041743755340576
Validation loss: 2.516825791328184

Epoch: 6| Step: 6
Training loss: 2.853269100189209
Validation loss: 2.5151683463845202

Epoch: 6| Step: 7
Training loss: 3.090585708618164
Validation loss: 2.5149306097338275

Epoch: 6| Step: 8
Training loss: 3.2952771186828613
Validation loss: 2.515584641887296

Epoch: 6| Step: 9
Training loss: 3.674349308013916
Validation loss: 2.5212220145810034

Epoch: 6| Step: 10
Training loss: 2.8695907592773438
Validation loss: 2.5152270640096357

Epoch: 6| Step: 11
Training loss: 2.2256999015808105
Validation loss: 2.5214404547086327

Epoch: 6| Step: 12
Training loss: 2.409900188446045
Validation loss: 2.5313092790624148

Epoch: 6| Step: 13
Training loss: 2.3374412059783936
Validation loss: 2.5296199475565264

Epoch: 62| Step: 0
Training loss: 3.196087598800659
Validation loss: 2.5309019473291214

Epoch: 6| Step: 1
Training loss: 3.1622185707092285
Validation loss: 2.5298886722134006

Epoch: 6| Step: 2
Training loss: 2.486469268798828
Validation loss: 2.538275246979088

Epoch: 6| Step: 3
Training loss: 2.168245553970337
Validation loss: 2.536756505248367

Epoch: 6| Step: 4
Training loss: 2.888747215270996
Validation loss: 2.5339494469345256

Epoch: 6| Step: 5
Training loss: 1.9290506839752197
Validation loss: 2.5320805682930896

Epoch: 6| Step: 6
Training loss: 2.8138279914855957
Validation loss: 2.5283612474318473

Epoch: 6| Step: 7
Training loss: 3.1856446266174316
Validation loss: 2.5328154153721307

Epoch: 6| Step: 8
Training loss: 2.3314926624298096
Validation loss: 2.5268539177474154

Epoch: 6| Step: 9
Training loss: 2.558197259902954
Validation loss: 2.528160595124768

Epoch: 6| Step: 10
Training loss: 3.2936291694641113
Validation loss: 2.512098909706198

Epoch: 6| Step: 11
Training loss: 2.442007541656494
Validation loss: 2.5107618711327993

Epoch: 6| Step: 12
Training loss: 2.44073486328125
Validation loss: 2.506089584801787

Epoch: 6| Step: 13
Training loss: 3.3146378993988037
Validation loss: 2.5027819910357074

Epoch: 63| Step: 0
Training loss: 3.3486275672912598
Validation loss: 2.5041374237306657

Epoch: 6| Step: 1
Training loss: 2.128357410430908
Validation loss: 2.5033825238545737

Epoch: 6| Step: 2
Training loss: 2.615349769592285
Validation loss: 2.508087347912532

Epoch: 6| Step: 3
Training loss: 3.027376651763916
Validation loss: 2.5020771129156953

Epoch: 6| Step: 4
Training loss: 2.6618404388427734
Validation loss: 2.5049797411887877

Epoch: 6| Step: 5
Training loss: 2.403062343597412
Validation loss: 2.5053029214182208

Epoch: 6| Step: 6
Training loss: 2.416898727416992
Validation loss: 2.5075926857609905

Epoch: 6| Step: 7
Training loss: 1.830430269241333
Validation loss: 2.511685379089848

Epoch: 6| Step: 8
Training loss: 3.620522975921631
Validation loss: 2.5261866713082917

Epoch: 6| Step: 9
Training loss: 3.7562832832336426
Validation loss: 2.534726506920271

Epoch: 6| Step: 10
Training loss: 2.862671375274658
Validation loss: 2.5500531376049085

Epoch: 6| Step: 11
Training loss: 2.749880790710449
Validation loss: 2.5538093377185125

Epoch: 6| Step: 12
Training loss: 1.6799650192260742
Validation loss: 2.52833971669597

Epoch: 6| Step: 13
Training loss: 3.1429290771484375
Validation loss: 2.509770083171065

Epoch: 64| Step: 0
Training loss: 2.8061575889587402
Validation loss: 2.5057326644979496

Epoch: 6| Step: 1
Training loss: 3.212090492248535
Validation loss: 2.4987420907584568

Epoch: 6| Step: 2
Training loss: 3.3038103580474854
Validation loss: 2.4984306520031345

Epoch: 6| Step: 3
Training loss: 2.412217378616333
Validation loss: 2.5007327833483295

Epoch: 6| Step: 4
Training loss: 3.182809829711914
Validation loss: 2.500790980554396

Epoch: 6| Step: 5
Training loss: 2.499612808227539
Validation loss: 2.503768933716641

Epoch: 6| Step: 6
Training loss: 1.964212417602539
Validation loss: 2.5041540848311556

Epoch: 6| Step: 7
Training loss: 2.2154738903045654
Validation loss: 2.502079407374064

Epoch: 6| Step: 8
Training loss: 2.3813557624816895
Validation loss: 2.5074612735420145

Epoch: 6| Step: 9
Training loss: 3.7573130130767822
Validation loss: 2.5034731357328353

Epoch: 6| Step: 10
Training loss: 2.1306378841400146
Validation loss: 2.5029128161809777

Epoch: 6| Step: 11
Training loss: 2.1215102672576904
Validation loss: 2.519186153206774

Epoch: 6| Step: 12
Training loss: 3.1110992431640625
Validation loss: 2.5275321134956936

Epoch: 6| Step: 13
Training loss: 3.0056021213531494
Validation loss: 2.524003609534233

Epoch: 65| Step: 0
Training loss: 2.453282356262207
Validation loss: 2.540470684728315

Epoch: 6| Step: 1
Training loss: 3.112814426422119
Validation loss: 2.5449761011267222

Epoch: 6| Step: 2
Training loss: 2.351665496826172
Validation loss: 2.5441804624372915

Epoch: 6| Step: 3
Training loss: 2.9601004123687744
Validation loss: 2.523632816089097

Epoch: 6| Step: 4
Training loss: 3.454310894012451
Validation loss: 2.504448377957908

Epoch: 6| Step: 5
Training loss: 2.8295388221740723
Validation loss: 2.4965280896873883

Epoch: 6| Step: 6
Training loss: 2.901931047439575
Validation loss: 2.500675796180643

Epoch: 6| Step: 7
Training loss: 2.606877326965332
Validation loss: 2.4977324547306186

Epoch: 6| Step: 8
Training loss: 2.253427743911743
Validation loss: 2.503017971592565

Epoch: 6| Step: 9
Training loss: 2.1437618732452393
Validation loss: 2.5036713410449285

Epoch: 6| Step: 10
Training loss: 2.1949188709259033
Validation loss: 2.503671781991118

Epoch: 6| Step: 11
Training loss: 2.6667041778564453
Validation loss: 2.501205377681281

Epoch: 6| Step: 12
Training loss: 2.846964120864868
Validation loss: 2.5039699769789174

Epoch: 6| Step: 13
Training loss: 3.651235818862915
Validation loss: 2.498865483909525

Epoch: 66| Step: 0
Training loss: 2.458427906036377
Validation loss: 2.502582334703015

Epoch: 6| Step: 1
Training loss: 3.351681709289551
Validation loss: 2.4992104089388283

Epoch: 6| Step: 2
Training loss: 2.622544765472412
Validation loss: 2.4986037618370465

Epoch: 6| Step: 3
Training loss: 2.3693079948425293
Validation loss: 2.4947714664602794

Epoch: 6| Step: 4
Training loss: 2.491246223449707
Validation loss: 2.496518827253772

Epoch: 6| Step: 5
Training loss: 2.7772576808929443
Validation loss: 2.503348840180264

Epoch: 6| Step: 6
Training loss: 2.6231560707092285
Validation loss: 2.502506855995424

Epoch: 6| Step: 7
Training loss: 2.5471224784851074
Validation loss: 2.5036600687170543

Epoch: 6| Step: 8
Training loss: 2.697946786880493
Validation loss: 2.508794617909257

Epoch: 6| Step: 9
Training loss: 1.423261046409607
Validation loss: 2.5047984841049358

Epoch: 6| Step: 10
Training loss: 3.685295581817627
Validation loss: 2.5011591270405757

Epoch: 6| Step: 11
Training loss: 3.2867212295532227
Validation loss: 2.502587144092847

Epoch: 6| Step: 12
Training loss: 2.7118101119995117
Validation loss: 2.498036235891363

Epoch: 6| Step: 13
Training loss: 3.013993263244629
Validation loss: 2.5035509012078725

Epoch: 67| Step: 0
Training loss: 2.356462001800537
Validation loss: 2.4996837057093138

Epoch: 6| Step: 1
Training loss: 2.910426616668701
Validation loss: 2.499022929899154

Epoch: 6| Step: 2
Training loss: 2.3889899253845215
Validation loss: 2.499467211384927

Epoch: 6| Step: 3
Training loss: 2.896413803100586
Validation loss: 2.498638370985626

Epoch: 6| Step: 4
Training loss: 2.848999500274658
Validation loss: 2.502507602014849

Epoch: 6| Step: 5
Training loss: 3.1339809894561768
Validation loss: 2.491344536504438

Epoch: 6| Step: 6
Training loss: 2.3233978748321533
Validation loss: 2.495896575271442

Epoch: 6| Step: 7
Training loss: 2.633068561553955
Validation loss: 2.4916089914178334

Epoch: 6| Step: 8
Training loss: 2.868650436401367
Validation loss: 2.4914720519896476

Epoch: 6| Step: 9
Training loss: 3.0567820072174072
Validation loss: 2.4914544269602787

Epoch: 6| Step: 10
Training loss: 2.8087692260742188
Validation loss: 2.4895403462071575

Epoch: 6| Step: 11
Training loss: 2.363757610321045
Validation loss: 2.4899931594889653

Epoch: 6| Step: 12
Training loss: 3.0275723934173584
Validation loss: 2.492112592984271

Epoch: 6| Step: 13
Training loss: 1.8296093940734863
Validation loss: 2.486724635606171

Epoch: 68| Step: 0
Training loss: 3.251004219055176
Validation loss: 2.4876898770691245

Epoch: 6| Step: 1
Training loss: 2.8537285327911377
Validation loss: 2.4868877421143236

Epoch: 6| Step: 2
Training loss: 2.631234884262085
Validation loss: 2.4871899363815144

Epoch: 6| Step: 3
Training loss: 2.518069267272949
Validation loss: 2.4863939541642384

Epoch: 6| Step: 4
Training loss: 2.5008344650268555
Validation loss: 2.4877905127822713

Epoch: 6| Step: 5
Training loss: 2.3752644062042236
Validation loss: 2.486247936884562

Epoch: 6| Step: 6
Training loss: 2.0864925384521484
Validation loss: 2.48578820818214

Epoch: 6| Step: 7
Training loss: 2.602653980255127
Validation loss: 2.5026012607800063

Epoch: 6| Step: 8
Training loss: 3.3347315788269043
Validation loss: 2.496815901930614

Epoch: 6| Step: 9
Training loss: 2.6052188873291016
Validation loss: 2.4865033062555457

Epoch: 6| Step: 10
Training loss: 2.9519805908203125
Validation loss: 2.4913753950467674

Epoch: 6| Step: 11
Training loss: 3.2310521602630615
Validation loss: 2.4836270732264363

Epoch: 6| Step: 12
Training loss: 2.478135108947754
Validation loss: 2.489368369502406

Epoch: 6| Step: 13
Training loss: 2.1343464851379395
Validation loss: 2.483453386573381

Epoch: 69| Step: 0
Training loss: 2.666910409927368
Validation loss: 2.487267648020098

Epoch: 6| Step: 1
Training loss: 2.4432947635650635
Validation loss: 2.486565738595942

Epoch: 6| Step: 2
Training loss: 2.4395265579223633
Validation loss: 2.487332385073426

Epoch: 6| Step: 3
Training loss: 2.60933780670166
Validation loss: 2.4867138349881737

Epoch: 6| Step: 4
Training loss: 2.5869765281677246
Validation loss: 2.488356098051994

Epoch: 6| Step: 5
Training loss: 3.3159401416778564
Validation loss: 2.4902905520572456

Epoch: 6| Step: 6
Training loss: 3.007608652114868
Validation loss: 2.4852032648619784

Epoch: 6| Step: 7
Training loss: 3.477220058441162
Validation loss: 2.485284000314692

Epoch: 6| Step: 8
Training loss: 3.0204415321350098
Validation loss: 2.48861740737833

Epoch: 6| Step: 9
Training loss: 2.1129584312438965
Validation loss: 2.4873770231841714

Epoch: 6| Step: 10
Training loss: 2.5399091243743896
Validation loss: 2.484417207779423

Epoch: 6| Step: 11
Training loss: 1.9341288805007935
Validation loss: 2.4804919996569232

Epoch: 6| Step: 12
Training loss: 2.3326873779296875
Validation loss: 2.4765828604339273

Epoch: 6| Step: 13
Training loss: 3.6813621520996094
Validation loss: 2.4817645729229016

Epoch: 70| Step: 0
Training loss: 3.08103609085083
Validation loss: 2.480883152254166

Epoch: 6| Step: 1
Training loss: 2.8140804767608643
Validation loss: 2.4815788986862346

Epoch: 6| Step: 2
Training loss: 3.002653121948242
Validation loss: 2.4823498930982364

Epoch: 6| Step: 3
Training loss: 3.500736951828003
Validation loss: 2.4780929370593

Epoch: 6| Step: 4
Training loss: 2.6908907890319824
Validation loss: 2.4804869287757465

Epoch: 6| Step: 5
Training loss: 2.4957447052001953
Validation loss: 2.481418171236592

Epoch: 6| Step: 6
Training loss: 1.7567360401153564
Validation loss: 2.4847461715821297

Epoch: 6| Step: 7
Training loss: 3.2354564666748047
Validation loss: 2.485713707503452

Epoch: 6| Step: 8
Training loss: 2.6666040420532227
Validation loss: 2.4825192548895396

Epoch: 6| Step: 9
Training loss: 2.7512714862823486
Validation loss: 2.489126432326532

Epoch: 6| Step: 10
Training loss: 1.9888001680374146
Validation loss: 2.492149442754766

Epoch: 6| Step: 11
Training loss: 2.2208800315856934
Validation loss: 2.491564868598856

Epoch: 6| Step: 12
Training loss: 2.677626609802246
Validation loss: 2.499280383509974

Epoch: 6| Step: 13
Training loss: 3.051095724105835
Validation loss: 2.4998914528918523

Epoch: 71| Step: 0
Training loss: 1.8084309101104736
Validation loss: 2.510374088441172

Epoch: 6| Step: 1
Training loss: 3.635643243789673
Validation loss: 2.500506095988776

Epoch: 6| Step: 2
Training loss: 2.3836770057678223
Validation loss: 2.5005975179774786

Epoch: 6| Step: 3
Training loss: 2.8603568077087402
Validation loss: 2.498600841850363

Epoch: 6| Step: 4
Training loss: 3.2053563594818115
Validation loss: 2.5044199369286977

Epoch: 6| Step: 5
Training loss: 3.262603521347046
Validation loss: 2.486569194383519

Epoch: 6| Step: 6
Training loss: 2.162932872772217
Validation loss: 2.4892216241487892

Epoch: 6| Step: 7
Training loss: 2.789025068283081
Validation loss: 2.5000717845014346

Epoch: 6| Step: 8
Training loss: 2.7846760749816895
Validation loss: 2.505706338472264

Epoch: 6| Step: 9
Training loss: 1.909001350402832
Validation loss: 2.494396942918019

Epoch: 6| Step: 10
Training loss: 2.371415376663208
Validation loss: 2.4983660303136355

Epoch: 6| Step: 11
Training loss: 2.868986129760742
Validation loss: 2.502769753497134

Epoch: 6| Step: 12
Training loss: 2.5600802898406982
Validation loss: 2.5032023281179447

Epoch: 6| Step: 13
Training loss: 3.4018216133117676
Validation loss: 2.5125627466427383

Epoch: 72| Step: 0
Training loss: 2.613149642944336
Validation loss: 2.523769445316766

Epoch: 6| Step: 1
Training loss: 2.3359813690185547
Validation loss: 2.5237792384239937

Epoch: 6| Step: 2
Training loss: 2.3536036014556885
Validation loss: 2.5083577248357956

Epoch: 6| Step: 3
Training loss: 2.430339813232422
Validation loss: 2.510677760647189

Epoch: 6| Step: 4
Training loss: 2.4634804725646973
Validation loss: 2.5039550719722623

Epoch: 6| Step: 5
Training loss: 3.5475242137908936
Validation loss: 2.498986738984303

Epoch: 6| Step: 6
Training loss: 2.334710121154785
Validation loss: 2.5008480882131927

Epoch: 6| Step: 7
Training loss: 2.8952488899230957
Validation loss: 2.5016711924665715

Epoch: 6| Step: 8
Training loss: 2.7544891834259033
Validation loss: 2.50246436108825

Epoch: 6| Step: 9
Training loss: 3.2236666679382324
Validation loss: 2.495544102884108

Epoch: 6| Step: 10
Training loss: 2.8113675117492676
Validation loss: 2.49125099566675

Epoch: 6| Step: 11
Training loss: 2.363105058670044
Validation loss: 2.485504340100032

Epoch: 6| Step: 12
Training loss: 3.0717904567718506
Validation loss: 2.477915084490212

Epoch: 6| Step: 13
Training loss: 2.439549684524536
Validation loss: 2.4743036172723256

Epoch: 73| Step: 0
Training loss: 2.6777403354644775
Validation loss: 2.4696116985813266

Epoch: 6| Step: 1
Training loss: 2.408514976501465
Validation loss: 2.473389810131442

Epoch: 6| Step: 2
Training loss: 2.534517288208008
Validation loss: 2.4816485630568637

Epoch: 6| Step: 3
Training loss: 2.545484781265259
Validation loss: 2.4879651556732836

Epoch: 6| Step: 4
Training loss: 2.5298614501953125
Validation loss: 2.497479989964475

Epoch: 6| Step: 5
Training loss: 2.8326492309570312
Validation loss: 2.4868452754071964

Epoch: 6| Step: 6
Training loss: 3.2814743518829346
Validation loss: 2.479808594590874

Epoch: 6| Step: 7
Training loss: 2.730706214904785
Validation loss: 2.474421085849885

Epoch: 6| Step: 8
Training loss: 2.276109218597412
Validation loss: 2.479900578016876

Epoch: 6| Step: 9
Training loss: 2.604039430618286
Validation loss: 2.4860979613437446

Epoch: 6| Step: 10
Training loss: 3.020303964614868
Validation loss: 2.4793485544061147

Epoch: 6| Step: 11
Training loss: 3.0809998512268066
Validation loss: 2.474812147437885

Epoch: 6| Step: 12
Training loss: 2.633430242538452
Validation loss: 2.462808967918478

Epoch: 6| Step: 13
Training loss: 2.444344997406006
Validation loss: 2.4682077002781693

Epoch: 74| Step: 0
Training loss: 2.6672348976135254
Validation loss: 2.4629859437224684

Epoch: 6| Step: 1
Training loss: 2.554265022277832
Validation loss: 2.460670578864313

Epoch: 6| Step: 2
Training loss: 2.766789436340332
Validation loss: 2.459526890067644

Epoch: 6| Step: 3
Training loss: 2.7955310344696045
Validation loss: 2.46944341351909

Epoch: 6| Step: 4
Training loss: 2.6784939765930176
Validation loss: 2.4629370832955964

Epoch: 6| Step: 5
Training loss: 2.473344564437866
Validation loss: 2.4647159320051952

Epoch: 6| Step: 6
Training loss: 3.210899829864502
Validation loss: 2.4682395688949095

Epoch: 6| Step: 7
Training loss: 2.846006393432617
Validation loss: 2.4704103931303947

Epoch: 6| Step: 8
Training loss: 2.407628059387207
Validation loss: 2.4809460639953613

Epoch: 6| Step: 9
Training loss: 2.535308837890625
Validation loss: 2.4812328802642

Epoch: 6| Step: 10
Training loss: 2.5257153511047363
Validation loss: 2.484163994430214

Epoch: 6| Step: 11
Training loss: 2.9866526126861572
Validation loss: 2.4856022352813394

Epoch: 6| Step: 12
Training loss: 2.4292826652526855
Validation loss: 2.482343504505773

Epoch: 6| Step: 13
Training loss: 2.684873580932617
Validation loss: 2.480552888685657

Epoch: 75| Step: 0
Training loss: 2.8119187355041504
Validation loss: 2.489145125112226

Epoch: 6| Step: 1
Training loss: 2.101433753967285
Validation loss: 2.483806933126142

Epoch: 6| Step: 2
Training loss: 2.951758861541748
Validation loss: 2.481589622395013

Epoch: 6| Step: 3
Training loss: 2.8939101696014404
Validation loss: 2.482792300562705

Epoch: 6| Step: 4
Training loss: 2.6786623001098633
Validation loss: 2.477240052274478

Epoch: 6| Step: 5
Training loss: 2.756652593612671
Validation loss: 2.466799792423043

Epoch: 6| Step: 6
Training loss: 1.7742841243743896
Validation loss: 2.4643081183074624

Epoch: 6| Step: 7
Training loss: 2.4785056114196777
Validation loss: 2.4674333039150445

Epoch: 6| Step: 8
Training loss: 3.6070730686187744
Validation loss: 2.4708227239629275

Epoch: 6| Step: 9
Training loss: 2.767592430114746
Validation loss: 2.4677444350334907

Epoch: 6| Step: 10
Training loss: 2.6282286643981934
Validation loss: 2.4684021267839658

Epoch: 6| Step: 11
Training loss: 2.691423177719116
Validation loss: 2.465264222955191

Epoch: 6| Step: 12
Training loss: 3.1329712867736816
Validation loss: 2.469254283494847

Epoch: 6| Step: 13
Training loss: 2.040879964828491
Validation loss: 2.4662424338761197

Epoch: 76| Step: 0
Training loss: 2.0790092945098877
Validation loss: 2.465867616797006

Epoch: 6| Step: 1
Training loss: 2.7973852157592773
Validation loss: 2.4615849602606987

Epoch: 6| Step: 2
Training loss: 2.8045337200164795
Validation loss: 2.462272605588359

Epoch: 6| Step: 3
Training loss: 2.6177220344543457
Validation loss: 2.466850539689423

Epoch: 6| Step: 4
Training loss: 2.578773021697998
Validation loss: 2.4615347667406966

Epoch: 6| Step: 5
Training loss: 2.139944076538086
Validation loss: 2.4707233905792236

Epoch: 6| Step: 6
Training loss: 2.9959075450897217
Validation loss: 2.4707823876411683

Epoch: 6| Step: 7
Training loss: 2.8799514770507812
Validation loss: 2.478020865430114

Epoch: 6| Step: 8
Training loss: 2.960928440093994
Validation loss: 2.5031576643707933

Epoch: 6| Step: 9
Training loss: 3.0857625007629395
Validation loss: 2.50964520823571

Epoch: 6| Step: 10
Training loss: 2.5885109901428223
Validation loss: 2.494903069670482

Epoch: 6| Step: 11
Training loss: 2.2281086444854736
Validation loss: 2.486717762485627

Epoch: 6| Step: 12
Training loss: 2.79961895942688
Validation loss: 2.466592122149724

Epoch: 6| Step: 13
Training loss: 3.478461503982544
Validation loss: 2.4665691468023483

Epoch: 77| Step: 0
Training loss: 1.9273083209991455
Validation loss: 2.461755293671803

Epoch: 6| Step: 1
Training loss: 2.8599512577056885
Validation loss: 2.46639931842845

Epoch: 6| Step: 2
Training loss: 1.8551936149597168
Validation loss: 2.465640808946343

Epoch: 6| Step: 3
Training loss: 3.5107760429382324
Validation loss: 2.471063170381772

Epoch: 6| Step: 4
Training loss: 2.966771125793457
Validation loss: 2.466485474699287

Epoch: 6| Step: 5
Training loss: 3.221259593963623
Validation loss: 2.4739362808965866

Epoch: 6| Step: 6
Training loss: 2.674220323562622
Validation loss: 2.466741833635556

Epoch: 6| Step: 7
Training loss: 2.9129481315612793
Validation loss: 2.4694636970437984

Epoch: 6| Step: 8
Training loss: 2.2766308784484863
Validation loss: 2.466553549612722

Epoch: 6| Step: 9
Training loss: 2.673677682876587
Validation loss: 2.4656700165041032

Epoch: 6| Step: 10
Training loss: 2.6388838291168213
Validation loss: 2.461050843679777

Epoch: 6| Step: 11
Training loss: 2.760575771331787
Validation loss: 2.460272455728182

Epoch: 6| Step: 12
Training loss: 3.0444464683532715
Validation loss: 2.4614104660608436

Epoch: 6| Step: 13
Training loss: 2.095365047454834
Validation loss: 2.4639674207215667

Epoch: 78| Step: 0
Training loss: 2.6497180461883545
Validation loss: 2.4660595591350267

Epoch: 6| Step: 1
Training loss: 2.577599048614502
Validation loss: 2.478710005360265

Epoch: 6| Step: 2
Training loss: 2.7181339263916016
Validation loss: 2.4814772785350843

Epoch: 6| Step: 3
Training loss: 2.7157211303710938
Validation loss: 2.4719462343441543

Epoch: 6| Step: 4
Training loss: 2.6176154613494873
Validation loss: 2.47213569764168

Epoch: 6| Step: 5
Training loss: 2.907360792160034
Validation loss: 2.4730613154749714

Epoch: 6| Step: 6
Training loss: 2.3601088523864746
Validation loss: 2.4697188792690152

Epoch: 6| Step: 7
Training loss: 2.4480338096618652
Validation loss: 2.481778698582803

Epoch: 6| Step: 8
Training loss: 2.708064079284668
Validation loss: 2.4819370341557327

Epoch: 6| Step: 9
Training loss: 3.345860004425049
Validation loss: 2.4807367581193165

Epoch: 6| Step: 10
Training loss: 2.665834903717041
Validation loss: 2.4800338129843436

Epoch: 6| Step: 11
Training loss: 3.1223561763763428
Validation loss: 2.487700236740933

Epoch: 6| Step: 12
Training loss: 2.349855661392212
Validation loss: 2.4941513281996532

Epoch: 6| Step: 13
Training loss: 1.9852447509765625
Validation loss: 2.4886337454601

Epoch: 79| Step: 0
Training loss: 2.2091774940490723
Validation loss: 2.4846487276015745

Epoch: 6| Step: 1
Training loss: 3.406616449356079
Validation loss: 2.4735574594108005

Epoch: 6| Step: 2
Training loss: 2.9420695304870605
Validation loss: 2.465906411088923

Epoch: 6| Step: 3
Training loss: 2.4800620079040527
Validation loss: 2.4590881691184094

Epoch: 6| Step: 4
Training loss: 2.3117709159851074
Validation loss: 2.457207225984143

Epoch: 6| Step: 5
Training loss: 2.618969440460205
Validation loss: 2.457353759837407

Epoch: 6| Step: 6
Training loss: 2.385446786880493
Validation loss: 2.4595501320336455

Epoch: 6| Step: 7
Training loss: 2.654233694076538
Validation loss: 2.458941703201622

Epoch: 6| Step: 8
Training loss: 3.304274559020996
Validation loss: 2.460048311500139

Epoch: 6| Step: 9
Training loss: 2.9789557456970215
Validation loss: 2.4620153160505396

Epoch: 6| Step: 10
Training loss: 2.3175110816955566
Validation loss: 2.4626511425100346

Epoch: 6| Step: 11
Training loss: 2.6273083686828613
Validation loss: 2.471053954093687

Epoch: 6| Step: 12
Training loss: 2.7611656188964844
Validation loss: 2.471134016590734

Epoch: 6| Step: 13
Training loss: 2.4316725730895996
Validation loss: 2.475247670245427

Epoch: 80| Step: 0
Training loss: 3.0148444175720215
Validation loss: 2.4814461815741753

Epoch: 6| Step: 1
Training loss: 2.9133007526397705
Validation loss: 2.484396611490557

Epoch: 6| Step: 2
Training loss: 4.0654120445251465
Validation loss: 2.4827708223814606

Epoch: 6| Step: 3
Training loss: 2.9612531661987305
Validation loss: 2.465157103794877

Epoch: 6| Step: 4
Training loss: 2.792591094970703
Validation loss: 2.4783490396315053

Epoch: 6| Step: 5
Training loss: 2.1133127212524414
Validation loss: 2.4749031400167816

Epoch: 6| Step: 6
Training loss: 2.3118796348571777
Validation loss: 2.4690731084474953

Epoch: 6| Step: 7
Training loss: 2.3328096866607666
Validation loss: 2.4751983278541156

Epoch: 6| Step: 8
Training loss: 3.0553817749023438
Validation loss: 2.479332127878743

Epoch: 6| Step: 9
Training loss: 1.5444128513336182
Validation loss: 2.4752107922748854

Epoch: 6| Step: 10
Training loss: 2.2722272872924805
Validation loss: 2.47770591192348

Epoch: 6| Step: 11
Training loss: 3.1120221614837646
Validation loss: 2.4748671285567747

Epoch: 6| Step: 12
Training loss: 2.3459455966949463
Validation loss: 2.488865840819574

Epoch: 6| Step: 13
Training loss: 2.528630018234253
Validation loss: 2.4850642629849014

Epoch: 81| Step: 0
Training loss: 1.9604668617248535
Validation loss: 2.493596902457617

Epoch: 6| Step: 1
Training loss: 3.0690736770629883
Validation loss: 2.5256673007883053

Epoch: 6| Step: 2
Training loss: 2.3477261066436768
Validation loss: 2.533464898345291

Epoch: 6| Step: 3
Training loss: 3.2070260047912598
Validation loss: 2.548633613894063

Epoch: 6| Step: 4
Training loss: 2.4611809253692627
Validation loss: 2.552591682762228

Epoch: 6| Step: 5
Training loss: 2.5540812015533447
Validation loss: 2.552310218093216

Epoch: 6| Step: 6
Training loss: 3.0143048763275146
Validation loss: 2.5402145924106723

Epoch: 6| Step: 7
Training loss: 2.174492835998535
Validation loss: 2.5168486102934806

Epoch: 6| Step: 8
Training loss: 2.734787940979004
Validation loss: 2.5049099511997674

Epoch: 6| Step: 9
Training loss: 2.985112190246582
Validation loss: 2.4858498060575096

Epoch: 6| Step: 10
Training loss: 3.2560975551605225
Validation loss: 2.460799506915513

Epoch: 6| Step: 11
Training loss: 1.672200322151184
Validation loss: 2.458384685618903

Epoch: 6| Step: 12
Training loss: 2.7661099433898926
Validation loss: 2.4558716358677035

Epoch: 6| Step: 13
Training loss: 4.048952102661133
Validation loss: 2.457793561361169

Epoch: 82| Step: 0
Training loss: 2.874673843383789
Validation loss: 2.4706539236089236

Epoch: 6| Step: 1
Training loss: 1.8005785942077637
Validation loss: 2.484325849881736

Epoch: 6| Step: 2
Training loss: 2.3367104530334473
Validation loss: 2.4893708203428533

Epoch: 6| Step: 3
Training loss: 2.7136197090148926
Validation loss: 2.5234317548813356

Epoch: 6| Step: 4
Training loss: 2.837313175201416
Validation loss: 2.5158723579939974

Epoch: 6| Step: 5
Training loss: 2.246882915496826
Validation loss: 2.494987703138782

Epoch: 6| Step: 6
Training loss: 2.3697404861450195
Validation loss: 2.4793506642823577

Epoch: 6| Step: 7
Training loss: 3.515388011932373
Validation loss: 2.4732322949235157

Epoch: 6| Step: 8
Training loss: 3.5791265964508057
Validation loss: 2.460952043533325

Epoch: 6| Step: 9
Training loss: 2.797973394393921
Validation loss: 2.4546773843867804

Epoch: 6| Step: 10
Training loss: 2.897970199584961
Validation loss: 2.4536919004173687

Epoch: 6| Step: 11
Training loss: 2.0422744750976562
Validation loss: 2.4486235187899683

Epoch: 6| Step: 12
Training loss: 3.326261043548584
Validation loss: 2.4571142453019337

Epoch: 6| Step: 13
Training loss: 2.4174766540527344
Validation loss: 2.467956578859719

Epoch: 83| Step: 0
Training loss: 2.497870445251465
Validation loss: 2.4781852101766937

Epoch: 6| Step: 1
Training loss: 2.9843244552612305
Validation loss: 2.489087527798068

Epoch: 6| Step: 2
Training loss: 1.7991341352462769
Validation loss: 2.488060046267766

Epoch: 6| Step: 3
Training loss: 2.5938892364501953
Validation loss: 2.474784307582404

Epoch: 6| Step: 4
Training loss: 2.763589859008789
Validation loss: 2.464649240175883

Epoch: 6| Step: 5
Training loss: 2.027139186859131
Validation loss: 2.4527666696938137

Epoch: 6| Step: 6
Training loss: 3.1441650390625
Validation loss: 2.453425443300637

Epoch: 6| Step: 7
Training loss: 2.9209394454956055
Validation loss: 2.4463653487543904

Epoch: 6| Step: 8
Training loss: 2.703604221343994
Validation loss: 2.445649185488301

Epoch: 6| Step: 9
Training loss: 3.0128746032714844
Validation loss: 2.451378450598768

Epoch: 6| Step: 10
Training loss: 2.1727592945098877
Validation loss: 2.4460785850401847

Epoch: 6| Step: 11
Training loss: 2.845411777496338
Validation loss: 2.446549843716365

Epoch: 6| Step: 12
Training loss: 2.930413246154785
Validation loss: 2.447762394464144

Epoch: 6| Step: 13
Training loss: 3.4667670726776123
Validation loss: 2.4551265752443703

Epoch: 84| Step: 0
Training loss: 2.037170886993408
Validation loss: 2.461109289558985

Epoch: 6| Step: 1
Training loss: 2.944472074508667
Validation loss: 2.466232899696596

Epoch: 6| Step: 2
Training loss: 2.5010643005371094
Validation loss: 2.47932707366123

Epoch: 6| Step: 3
Training loss: 2.954655647277832
Validation loss: 2.4887258057953208

Epoch: 6| Step: 4
Training loss: 3.1267404556274414
Validation loss: 2.4829500823892574

Epoch: 6| Step: 5
Training loss: 2.8768458366394043
Validation loss: 2.482426215243596

Epoch: 6| Step: 6
Training loss: 2.5565459728240967
Validation loss: 2.481219273741527

Epoch: 6| Step: 7
Training loss: 2.950920581817627
Validation loss: 2.48061095001877

Epoch: 6| Step: 8
Training loss: 2.708498477935791
Validation loss: 2.463987950355776

Epoch: 6| Step: 9
Training loss: 1.8128386735916138
Validation loss: 2.4662382051508915

Epoch: 6| Step: 10
Training loss: 2.882819652557373
Validation loss: 2.4637306300542687

Epoch: 6| Step: 11
Training loss: 2.5919859409332275
Validation loss: 2.4591577565798195

Epoch: 6| Step: 12
Training loss: 3.086099624633789
Validation loss: 2.458918409962808

Epoch: 6| Step: 13
Training loss: 2.2933356761932373
Validation loss: 2.454278502413022

Epoch: 85| Step: 0
Training loss: 2.6014814376831055
Validation loss: 2.448598961676321

Epoch: 6| Step: 1
Training loss: 2.7732443809509277
Validation loss: 2.447891719879643

Epoch: 6| Step: 2
Training loss: 3.633671283721924
Validation loss: 2.44709163583735

Epoch: 6| Step: 3
Training loss: 2.3350634574890137
Validation loss: 2.4462896034281743

Epoch: 6| Step: 4
Training loss: 2.8542861938476562
Validation loss: 2.4456220801158617

Epoch: 6| Step: 5
Training loss: 2.3700339794158936
Validation loss: 2.4494968793725453

Epoch: 6| Step: 6
Training loss: 2.4628000259399414
Validation loss: 2.449712081622052

Epoch: 6| Step: 7
Training loss: 2.193986415863037
Validation loss: 2.447396478345317

Epoch: 6| Step: 8
Training loss: 2.795607089996338
Validation loss: 2.4499528766960226

Epoch: 6| Step: 9
Training loss: 2.7142088413238525
Validation loss: 2.450624922270416

Epoch: 6| Step: 10
Training loss: 2.860814332962036
Validation loss: 2.4441798579308296

Epoch: 6| Step: 11
Training loss: 2.9492855072021484
Validation loss: 2.4461575477353987

Epoch: 6| Step: 12
Training loss: 1.9093137979507446
Validation loss: 2.448074174183671

Epoch: 6| Step: 13
Training loss: 3.2054669857025146
Validation loss: 2.45075201219128

Epoch: 86| Step: 0
Training loss: 2.315640687942505
Validation loss: 2.446325699488322

Epoch: 6| Step: 1
Training loss: 2.9346976280212402
Validation loss: 2.4631733535438456

Epoch: 6| Step: 2
Training loss: 1.9267462491989136
Validation loss: 2.464089580761489

Epoch: 6| Step: 3
Training loss: 2.820619821548462
Validation loss: 2.4665410185372956

Epoch: 6| Step: 4
Training loss: 2.5508873462677
Validation loss: 2.4675900807944675

Epoch: 6| Step: 5
Training loss: 3.260308027267456
Validation loss: 2.477938880202591

Epoch: 6| Step: 6
Training loss: 3.3630049228668213
Validation loss: 2.4752229721315446

Epoch: 6| Step: 7
Training loss: 2.3705873489379883
Validation loss: 2.4687891314106603

Epoch: 6| Step: 8
Training loss: 2.8782567977905273
Validation loss: 2.4694404409777735

Epoch: 6| Step: 9
Training loss: 2.474196434020996
Validation loss: 2.4627826316382295

Epoch: 6| Step: 10
Training loss: 2.852391004562378
Validation loss: 2.455882259594497

Epoch: 6| Step: 11
Training loss: 2.9537038803100586
Validation loss: 2.456406872759583

Epoch: 6| Step: 12
Training loss: 2.6584181785583496
Validation loss: 2.4528248720271613

Epoch: 6| Step: 13
Training loss: 1.416338324546814
Validation loss: 2.4501886137070192

Epoch: 87| Step: 0
Training loss: 3.0043935775756836
Validation loss: 2.4478181844116538

Epoch: 6| Step: 1
Training loss: 1.9332910776138306
Validation loss: 2.450380863681916

Epoch: 6| Step: 2
Training loss: 2.7185752391815186
Validation loss: 2.447959492283483

Epoch: 6| Step: 3
Training loss: 3.0214362144470215
Validation loss: 2.4466892698759675

Epoch: 6| Step: 4
Training loss: 2.834411859512329
Validation loss: 2.44653046002952

Epoch: 6| Step: 5
Training loss: 2.385997772216797
Validation loss: 2.45174479228194

Epoch: 6| Step: 6
Training loss: 2.585686683654785
Validation loss: 2.446221051677581

Epoch: 6| Step: 7
Training loss: 2.4002907276153564
Validation loss: 2.446352897151824

Epoch: 6| Step: 8
Training loss: 2.4973394870758057
Validation loss: 2.4410756890491774

Epoch: 6| Step: 9
Training loss: 2.585545539855957
Validation loss: 2.4406868642376316

Epoch: 6| Step: 10
Training loss: 3.5728580951690674
Validation loss: 2.4502463776578187

Epoch: 6| Step: 11
Training loss: 3.030848979949951
Validation loss: 2.4415263258000857

Epoch: 6| Step: 12
Training loss: 1.9373383522033691
Validation loss: 2.4426136427028204

Epoch: 6| Step: 13
Training loss: 2.8491013050079346
Validation loss: 2.439056278556906

Epoch: 88| Step: 0
Training loss: 2.1042842864990234
Validation loss: 2.4370173023593042

Epoch: 6| Step: 1
Training loss: 2.712461471557617
Validation loss: 2.436878932419644

Epoch: 6| Step: 2
Training loss: 2.2097060680389404
Validation loss: 2.434520600944437

Epoch: 6| Step: 3
Training loss: 2.593153953552246
Validation loss: 2.437950006095312

Epoch: 6| Step: 4
Training loss: 2.382286787033081
Validation loss: 2.441230514998077

Epoch: 6| Step: 5
Training loss: 2.530488967895508
Validation loss: 2.441767061910322

Epoch: 6| Step: 6
Training loss: 2.6499781608581543
Validation loss: 2.441722248190193

Epoch: 6| Step: 7
Training loss: 3.452165126800537
Validation loss: 2.441461158055131

Epoch: 6| Step: 8
Training loss: 2.6515793800354004
Validation loss: 2.445886917011712

Epoch: 6| Step: 9
Training loss: 2.317509174346924
Validation loss: 2.4429010806545133

Epoch: 6| Step: 10
Training loss: 3.8745388984680176
Validation loss: 2.447195347919259

Epoch: 6| Step: 11
Training loss: 2.4294190406799316
Validation loss: 2.4382293878063077

Epoch: 6| Step: 12
Training loss: 3.0993852615356445
Validation loss: 2.447746851110971

Epoch: 6| Step: 13
Training loss: 2.0033934116363525
Validation loss: 2.4460960434329126

Epoch: 89| Step: 0
Training loss: 3.1654484272003174
Validation loss: 2.441748224278932

Epoch: 6| Step: 1
Training loss: 2.195185422897339
Validation loss: 2.4494943208591913

Epoch: 6| Step: 2
Training loss: 2.6877646446228027
Validation loss: 2.4471909974211004

Epoch: 6| Step: 3
Training loss: 2.61495304107666
Validation loss: 2.443468350236134

Epoch: 6| Step: 4
Training loss: 3.436209201812744
Validation loss: 2.4378442995009886

Epoch: 6| Step: 5
Training loss: 3.232062578201294
Validation loss: 2.4410321738130305

Epoch: 6| Step: 6
Training loss: 2.9034042358398438
Validation loss: 2.441385622947447

Epoch: 6| Step: 7
Training loss: 3.006753444671631
Validation loss: 2.439509927585561

Epoch: 6| Step: 8
Training loss: 2.5644946098327637
Validation loss: 2.44336304613339

Epoch: 6| Step: 9
Training loss: 2.984266757965088
Validation loss: 2.4487122566469255

Epoch: 6| Step: 10
Training loss: 2.3631036281585693
Validation loss: 2.4590278466542563

Epoch: 6| Step: 11
Training loss: 1.8794646263122559
Validation loss: 2.44974075594256

Epoch: 6| Step: 12
Training loss: 2.0188539028167725
Validation loss: 2.4683696557116765

Epoch: 6| Step: 13
Training loss: 1.8779326677322388
Validation loss: 2.4594894557870846

Epoch: 90| Step: 0
Training loss: 3.1372475624084473
Validation loss: 2.4521427231450237

Epoch: 6| Step: 1
Training loss: 2.550802230834961
Validation loss: 2.4495418456292923

Epoch: 6| Step: 2
Training loss: 3.0425922870635986
Validation loss: 2.4492779649713987

Epoch: 6| Step: 3
Training loss: 2.482889175415039
Validation loss: 2.4347729939286427

Epoch: 6| Step: 4
Training loss: 2.9417760372161865
Validation loss: 2.438889739333942

Epoch: 6| Step: 5
Training loss: 3.2860634326934814
Validation loss: 2.437597285034836

Epoch: 6| Step: 6
Training loss: 2.5129263401031494
Validation loss: 2.434125349085818

Epoch: 6| Step: 7
Training loss: 2.9305267333984375
Validation loss: 2.435571906387165

Epoch: 6| Step: 8
Training loss: 2.1632442474365234
Validation loss: 2.433547119940481

Epoch: 6| Step: 9
Training loss: 2.681915760040283
Validation loss: 2.433543174497543

Epoch: 6| Step: 10
Training loss: 2.246351718902588
Validation loss: 2.4379802160365607

Epoch: 6| Step: 11
Training loss: 2.6593127250671387
Validation loss: 2.439156796342583

Epoch: 6| Step: 12
Training loss: 2.1768007278442383
Validation loss: 2.4450706153787594

Epoch: 6| Step: 13
Training loss: 2.14220929145813
Validation loss: 2.4472373762438373

Epoch: 91| Step: 0
Training loss: 2.994593620300293
Validation loss: 2.447151373791438

Epoch: 6| Step: 1
Training loss: 2.7966389656066895
Validation loss: 2.4535354670657905

Epoch: 6| Step: 2
Training loss: 3.1886415481567383
Validation loss: 2.4724902158142417

Epoch: 6| Step: 3
Training loss: 2.3600645065307617
Validation loss: 2.4696573442028416

Epoch: 6| Step: 4
Training loss: 2.619540214538574
Validation loss: 2.473117877078313

Epoch: 6| Step: 5
Training loss: 2.7491793632507324
Validation loss: 2.4527148277528825

Epoch: 6| Step: 6
Training loss: 1.8010059595108032
Validation loss: 2.4486707436141146

Epoch: 6| Step: 7
Training loss: 3.0462453365325928
Validation loss: 2.439774372244394

Epoch: 6| Step: 8
Training loss: 2.2674107551574707
Validation loss: 2.4381338973199167

Epoch: 6| Step: 9
Training loss: 3.016181468963623
Validation loss: 2.4372977338811403

Epoch: 6| Step: 10
Training loss: 3.2167153358459473
Validation loss: 2.44046958287557

Epoch: 6| Step: 11
Training loss: 2.6769237518310547
Validation loss: 2.4356674994191816

Epoch: 6| Step: 12
Training loss: 1.955939531326294
Validation loss: 2.437980813364829

Epoch: 6| Step: 13
Training loss: 2.463362693786621
Validation loss: 2.433283882756387

Epoch: 92| Step: 0
Training loss: 2.40946888923645
Validation loss: 2.4333982903470277

Epoch: 6| Step: 1
Training loss: 2.9406840801239014
Validation loss: 2.431512232749693

Epoch: 6| Step: 2
Training loss: 2.5813751220703125
Validation loss: 2.4276884217416086

Epoch: 6| Step: 3
Training loss: 2.211232900619507
Validation loss: 2.423851249038532

Epoch: 6| Step: 4
Training loss: 2.67110538482666
Validation loss: 2.4319615543529554

Epoch: 6| Step: 5
Training loss: 2.3428635597229004
Validation loss: 2.431734731120448

Epoch: 6| Step: 6
Training loss: 2.201730728149414
Validation loss: 2.4371692006305983

Epoch: 6| Step: 7
Training loss: 3.061070203781128
Validation loss: 2.4408138798129175

Epoch: 6| Step: 8
Training loss: 3.165853977203369
Validation loss: 2.4474410882560154

Epoch: 6| Step: 9
Training loss: 2.5384349822998047
Validation loss: 2.4473372121011057

Epoch: 6| Step: 10
Training loss: 3.247358798980713
Validation loss: 2.455233399586011

Epoch: 6| Step: 11
Training loss: 2.637204170227051
Validation loss: 2.4500359924890662

Epoch: 6| Step: 12
Training loss: 2.4643383026123047
Validation loss: 2.447657016015822

Epoch: 6| Step: 13
Training loss: 2.8979098796844482
Validation loss: 2.44927736507949

Epoch: 93| Step: 0
Training loss: 2.5703845024108887
Validation loss: 2.4377586098127466

Epoch: 6| Step: 1
Training loss: 2.6325695514678955
Validation loss: 2.42684894095185

Epoch: 6| Step: 2
Training loss: 2.8800742626190186
Validation loss: 2.4270683539811

Epoch: 6| Step: 3
Training loss: 2.7355031967163086
Validation loss: 2.432343534244004

Epoch: 6| Step: 4
Training loss: 2.5393121242523193
Validation loss: 2.434211374610983

Epoch: 6| Step: 5
Training loss: 1.9549829959869385
Validation loss: 2.4332515655025357

Epoch: 6| Step: 6
Training loss: 2.8379411697387695
Validation loss: 2.4397127538598995

Epoch: 6| Step: 7
Training loss: 2.9834182262420654
Validation loss: 2.4422731860991447

Epoch: 6| Step: 8
Training loss: 2.2531633377075195
Validation loss: 2.4418222032567507

Epoch: 6| Step: 9
Training loss: 2.3988566398620605
Validation loss: 2.4348867477909213

Epoch: 6| Step: 10
Training loss: 3.416184425354004
Validation loss: 2.439651285448382

Epoch: 6| Step: 11
Training loss: 3.3707172870635986
Validation loss: 2.4322179286710677

Epoch: 6| Step: 12
Training loss: 2.169804573059082
Validation loss: 2.4284834502845682

Epoch: 6| Step: 13
Training loss: 2.6887762546539307
Validation loss: 2.425877606996926

Epoch: 94| Step: 0
Training loss: 2.3007776737213135
Validation loss: 2.4325367532750612

Epoch: 6| Step: 1
Training loss: 2.4947474002838135
Validation loss: 2.437535988387241

Epoch: 6| Step: 2
Training loss: 2.336042642593384
Validation loss: 2.443597347505631

Epoch: 6| Step: 3
Training loss: 1.9663819074630737
Validation loss: 2.463965318536246

Epoch: 6| Step: 4
Training loss: 2.838449001312256
Validation loss: 2.494720928130611

Epoch: 6| Step: 5
Training loss: 3.425640821456909
Validation loss: 2.4972536743328138

Epoch: 6| Step: 6
Training loss: 3.2256903648376465
Validation loss: 2.4775194480854976

Epoch: 6| Step: 7
Training loss: 2.2073516845703125
Validation loss: 2.458999756843813

Epoch: 6| Step: 8
Training loss: 2.461230516433716
Validation loss: 2.4385935439858386

Epoch: 6| Step: 9
Training loss: 2.3423166275024414
Validation loss: 2.449479164615754

Epoch: 6| Step: 10
Training loss: 2.952362537384033
Validation loss: 2.4382969333279516

Epoch: 6| Step: 11
Training loss: 2.4759459495544434
Validation loss: 2.4345799069250784

Epoch: 6| Step: 12
Training loss: 3.200225353240967
Validation loss: 2.434632649985693

Epoch: 6| Step: 13
Training loss: 3.4770002365112305
Validation loss: 2.434739310254333

Epoch: 95| Step: 0
Training loss: 2.9805808067321777
Validation loss: 2.4289687679659937

Epoch: 6| Step: 1
Training loss: 2.7689123153686523
Validation loss: 2.428817223477107

Epoch: 6| Step: 2
Training loss: 2.339817523956299
Validation loss: 2.4298310690028693

Epoch: 6| Step: 3
Training loss: 2.757525682449341
Validation loss: 2.424247339207639

Epoch: 6| Step: 4
Training loss: 2.8077914714813232
Validation loss: 2.4292133341553392

Epoch: 6| Step: 5
Training loss: 2.8805270195007324
Validation loss: 2.424434223482686

Epoch: 6| Step: 6
Training loss: 2.6721155643463135
Validation loss: 2.4265193785390546

Epoch: 6| Step: 7
Training loss: 2.647284984588623
Validation loss: 2.436424127189062

Epoch: 6| Step: 8
Training loss: 2.415468454360962
Validation loss: 2.431516606320617

Epoch: 6| Step: 9
Training loss: 2.7876174449920654
Validation loss: 2.447723998818346

Epoch: 6| Step: 10
Training loss: 2.976047992706299
Validation loss: 2.470530894494826

Epoch: 6| Step: 11
Training loss: 1.9439436197280884
Validation loss: 2.469149722847887

Epoch: 6| Step: 12
Training loss: 2.4699323177337646
Validation loss: 2.4769241002298172

Epoch: 6| Step: 13
Training loss: 2.8144447803497314
Validation loss: 2.4566612730744066

Epoch: 96| Step: 0
Training loss: 2.6817893981933594
Validation loss: 2.4436524427065285

Epoch: 6| Step: 1
Training loss: 3.1367921829223633
Validation loss: 2.4386829996621735

Epoch: 6| Step: 2
Training loss: 2.9484305381774902
Validation loss: 2.4331769327963553

Epoch: 6| Step: 3
Training loss: 2.133664131164551
Validation loss: 2.4257368323623494

Epoch: 6| Step: 4
Training loss: 3.0826339721679688
Validation loss: 2.422070203288909

Epoch: 6| Step: 5
Training loss: 2.302088737487793
Validation loss: 2.4223943295017367

Epoch: 6| Step: 6
Training loss: 2.4087958335876465
Validation loss: 2.4199140097505305

Epoch: 6| Step: 7
Training loss: 1.9405517578125
Validation loss: 2.4200774469683246

Epoch: 6| Step: 8
Training loss: 2.8668034076690674
Validation loss: 2.4223005669091338

Epoch: 6| Step: 9
Training loss: 3.1628191471099854
Validation loss: 2.418172210775396

Epoch: 6| Step: 10
Training loss: 3.150463819503784
Validation loss: 2.4182610306688535

Epoch: 6| Step: 11
Training loss: 2.0095605850219727
Validation loss: 2.4230868893284954

Epoch: 6| Step: 12
Training loss: 2.346710205078125
Validation loss: 2.420856434811828

Epoch: 6| Step: 13
Training loss: 3.3510658740997314
Validation loss: 2.417685244673042

Epoch: 97| Step: 0
Training loss: 2.3543026447296143
Validation loss: 2.4254151108444377

Epoch: 6| Step: 1
Training loss: 2.3320841789245605
Validation loss: 2.4250292008922947

Epoch: 6| Step: 2
Training loss: 3.034736156463623
Validation loss: 2.42454937196547

Epoch: 6| Step: 3
Training loss: 3.2377161979675293
Validation loss: 2.419281610878565

Epoch: 6| Step: 4
Training loss: 2.110351324081421
Validation loss: 2.427071423940761

Epoch: 6| Step: 5
Training loss: 2.887545108795166
Validation loss: 2.425876150849045

Epoch: 6| Step: 6
Training loss: 1.9873204231262207
Validation loss: 2.4300573871981714

Epoch: 6| Step: 7
Training loss: 2.1396732330322266
Validation loss: 2.4221341494591004

Epoch: 6| Step: 8
Training loss: 3.0671005249023438
Validation loss: 2.4339129706864715

Epoch: 6| Step: 9
Training loss: 2.577688455581665
Validation loss: 2.441217671158493

Epoch: 6| Step: 10
Training loss: 3.1492209434509277
Validation loss: 2.4516676574624996

Epoch: 6| Step: 11
Training loss: 2.849526882171631
Validation loss: 2.445054356769849

Epoch: 6| Step: 12
Training loss: 2.993809938430786
Validation loss: 2.450032993029523

Epoch: 6| Step: 13
Training loss: 2.307468891143799
Validation loss: 2.439540756646023

Epoch: 98| Step: 0
Training loss: 2.2235236167907715
Validation loss: 2.438111703882935

Epoch: 6| Step: 1
Training loss: 3.069049835205078
Validation loss: 2.428882272012772

Epoch: 6| Step: 2
Training loss: 2.6272573471069336
Validation loss: 2.437498620761338

Epoch: 6| Step: 3
Training loss: 2.502716302871704
Validation loss: 2.429052463141821

Epoch: 6| Step: 4
Training loss: 2.5072779655456543
Validation loss: 2.423576534435313

Epoch: 6| Step: 5
Training loss: 3.229912281036377
Validation loss: 2.4199613345566617

Epoch: 6| Step: 6
Training loss: 2.25339674949646
Validation loss: 2.4125584274209957

Epoch: 6| Step: 7
Training loss: 2.893634080886841
Validation loss: 2.422031448733422

Epoch: 6| Step: 8
Training loss: 2.997882843017578
Validation loss: 2.4275757856266473

Epoch: 6| Step: 9
Training loss: 2.8747658729553223
Validation loss: 2.4208873292451263

Epoch: 6| Step: 10
Training loss: 2.993694305419922
Validation loss: 2.4197668644689743

Epoch: 6| Step: 11
Training loss: 2.573167085647583
Validation loss: 2.4185839391523793

Epoch: 6| Step: 12
Training loss: 2.058037042617798
Validation loss: 2.416814386203725

Epoch: 6| Step: 13
Training loss: 2.434453010559082
Validation loss: 2.414842128753662

Epoch: 99| Step: 0
Training loss: 2.4670519828796387
Validation loss: 2.409864479495633

Epoch: 6| Step: 1
Training loss: 3.5277085304260254
Validation loss: 2.413799465343516

Epoch: 6| Step: 2
Training loss: 2.198301315307617
Validation loss: 2.4102857420521397

Epoch: 6| Step: 3
Training loss: 2.836494207382202
Validation loss: 2.425695639784618

Epoch: 6| Step: 4
Training loss: 2.2106752395629883
Validation loss: 2.4304378058320735

Epoch: 6| Step: 5
Training loss: 2.277080535888672
Validation loss: 2.4285003190399497

Epoch: 6| Step: 6
Training loss: 2.414304256439209
Validation loss: 2.4295900457648822

Epoch: 6| Step: 7
Training loss: 2.656174659729004
Validation loss: 2.4427573193785963

Epoch: 6| Step: 8
Training loss: 2.4598238468170166
Validation loss: 2.461729311173962

Epoch: 6| Step: 9
Training loss: 2.6126246452331543
Validation loss: 2.470448238875276

Epoch: 6| Step: 10
Training loss: 3.3036484718322754
Validation loss: 2.4661069864867837

Epoch: 6| Step: 11
Training loss: 2.4683849811553955
Validation loss: 2.469272244361139

Epoch: 6| Step: 12
Training loss: 2.858121871948242
Validation loss: 2.4680439502962175

Epoch: 6| Step: 13
Training loss: 3.068718194961548
Validation loss: 2.461023515270602

Epoch: 100| Step: 0
Training loss: 2.6123480796813965
Validation loss: 2.4510865134577595

Epoch: 6| Step: 1
Training loss: 2.699800729751587
Validation loss: 2.452936944141183

Epoch: 6| Step: 2
Training loss: 2.1214921474456787
Validation loss: 2.444435865648331

Epoch: 6| Step: 3
Training loss: 2.4589812755584717
Validation loss: 2.4398083379191737

Epoch: 6| Step: 4
Training loss: 2.7778196334838867
Validation loss: 2.4294662501222346

Epoch: 6| Step: 5
Training loss: 2.4983887672424316
Validation loss: 2.4239433375738

Epoch: 6| Step: 6
Training loss: 2.9832403659820557
Validation loss: 2.4286054283060055

Epoch: 6| Step: 7
Training loss: 2.5612754821777344
Validation loss: 2.4329814680161013

Epoch: 6| Step: 8
Training loss: 2.3958396911621094
Validation loss: 2.431976123522687

Epoch: 6| Step: 9
Training loss: 2.487774133682251
Validation loss: 2.434146390166334

Epoch: 6| Step: 10
Training loss: 3.2037134170532227
Validation loss: 2.428791020506172

Epoch: 6| Step: 11
Training loss: 2.773047685623169
Validation loss: 2.4281093407702703

Epoch: 6| Step: 12
Training loss: 2.217829704284668
Validation loss: 2.431050057052284

Epoch: 6| Step: 13
Training loss: 3.8976211547851562
Validation loss: 2.4345052652461554

Epoch: 101| Step: 0
Training loss: 2.7041420936584473
Validation loss: 2.432419443643221

Epoch: 6| Step: 1
Training loss: 3.0930166244506836
Validation loss: 2.422981272461594

Epoch: 6| Step: 2
Training loss: 2.5081655979156494
Validation loss: 2.416546501139159

Epoch: 6| Step: 3
Training loss: 2.7277932167053223
Validation loss: 2.414939042060606

Epoch: 6| Step: 4
Training loss: 3.2946557998657227
Validation loss: 2.4212452057869203

Epoch: 6| Step: 5
Training loss: 2.6468849182128906
Validation loss: 2.418261343433011

Epoch: 6| Step: 6
Training loss: 2.70402455329895
Validation loss: 2.420023420805572

Epoch: 6| Step: 7
Training loss: 2.407150983810425
Validation loss: 2.423657827479865

Epoch: 6| Step: 8
Training loss: 2.733189105987549
Validation loss: 2.429070347098894

Epoch: 6| Step: 9
Training loss: 1.7969834804534912
Validation loss: 2.432693058444608

Epoch: 6| Step: 10
Training loss: 2.630401134490967
Validation loss: 2.4410256467839724

Epoch: 6| Step: 11
Training loss: 2.2250866889953613
Validation loss: 2.443045667422715

Epoch: 6| Step: 12
Training loss: 2.9002747535705566
Validation loss: 2.450290608149703

Epoch: 6| Step: 13
Training loss: 2.7672195434570312
Validation loss: 2.4629819264975925

Epoch: 102| Step: 0
Training loss: 2.2258174419403076
Validation loss: 2.4601996534614154

Epoch: 6| Step: 1
Training loss: 2.6229398250579834
Validation loss: 2.446251136000438

Epoch: 6| Step: 2
Training loss: 2.6521425247192383
Validation loss: 2.4338442997265886

Epoch: 6| Step: 3
Training loss: 2.0442986488342285
Validation loss: 2.425911775199316

Epoch: 6| Step: 4
Training loss: 2.0062429904937744
Validation loss: 2.4261595638849403

Epoch: 6| Step: 5
Training loss: 3.2936816215515137
Validation loss: 2.4169772876206266

Epoch: 6| Step: 6
Training loss: 2.5492115020751953
Validation loss: 2.4117324788083314

Epoch: 6| Step: 7
Training loss: 2.9437694549560547
Validation loss: 2.4130983198842695

Epoch: 6| Step: 8
Training loss: 2.9222474098205566
Validation loss: 2.4142501636218

Epoch: 6| Step: 9
Training loss: 2.4929893016815186
Validation loss: 2.411939267189272

Epoch: 6| Step: 10
Training loss: 2.9026074409484863
Validation loss: 2.422811669688071

Epoch: 6| Step: 11
Training loss: 2.464456796646118
Validation loss: 2.414881019182103

Epoch: 6| Step: 12
Training loss: 2.9436230659484863
Validation loss: 2.4268899399747133

Epoch: 6| Step: 13
Training loss: 3.3538849353790283
Validation loss: 2.4169883753663752

Epoch: 103| Step: 0
Training loss: 2.3529205322265625
Validation loss: 2.406916654238137

Epoch: 6| Step: 1
Training loss: 2.645087718963623
Validation loss: 2.4129788696124987

Epoch: 6| Step: 2
Training loss: 2.8617169857025146
Validation loss: 2.4163665284392652

Epoch: 6| Step: 3
Training loss: 3.0410478115081787
Validation loss: 2.4129091078235256

Epoch: 6| Step: 4
Training loss: 2.826826572418213
Validation loss: 2.408542157501303

Epoch: 6| Step: 5
Training loss: 2.12477970123291
Validation loss: 2.4075725770765737

Epoch: 6| Step: 6
Training loss: 2.2241640090942383
Validation loss: 2.4050252873410463

Epoch: 6| Step: 7
Training loss: 2.5933754444122314
Validation loss: 2.407305207303775

Epoch: 6| Step: 8
Training loss: 2.803447723388672
Validation loss: 2.4135265709251486

Epoch: 6| Step: 9
Training loss: 2.301644802093506
Validation loss: 2.4190822673100296

Epoch: 6| Step: 10
Training loss: 3.2431745529174805
Validation loss: 2.4442980802187355

Epoch: 6| Step: 11
Training loss: 2.3357086181640625
Validation loss: 2.4561558641413206

Epoch: 6| Step: 12
Training loss: 2.7383522987365723
Validation loss: 2.4764983487385575

Epoch: 6| Step: 13
Training loss: 3.485990524291992
Validation loss: 2.473212931745796

Epoch: 104| Step: 0
Training loss: 2.439507246017456
Validation loss: 2.5044430584035893

Epoch: 6| Step: 1
Training loss: 3.1021549701690674
Validation loss: 2.4907892211791007

Epoch: 6| Step: 2
Training loss: 2.529594898223877
Validation loss: 2.4774765583776657

Epoch: 6| Step: 3
Training loss: 2.3083510398864746
Validation loss: 2.4677249821283485

Epoch: 6| Step: 4
Training loss: 3.2680892944335938
Validation loss: 2.446773364979734

Epoch: 6| Step: 5
Training loss: 2.771237850189209
Validation loss: 2.4339277744293213

Epoch: 6| Step: 6
Training loss: 2.2367730140686035
Validation loss: 2.4220817268535657

Epoch: 6| Step: 7
Training loss: 3.2967336177825928
Validation loss: 2.416648490454561

Epoch: 6| Step: 8
Training loss: 2.654531955718994
Validation loss: 2.4147416750590005

Epoch: 6| Step: 9
Training loss: 2.744174003601074
Validation loss: 2.4141980781350085

Epoch: 6| Step: 10
Training loss: 2.5438027381896973
Validation loss: 2.4075127750314693

Epoch: 6| Step: 11
Training loss: 2.349712371826172
Validation loss: 2.4051474025172572

Epoch: 6| Step: 12
Training loss: 2.393583297729492
Validation loss: 2.406677815221971

Epoch: 6| Step: 13
Training loss: 2.2595582008361816
Validation loss: 2.404742363960512

Epoch: 105| Step: 0
Training loss: 2.830946445465088
Validation loss: 2.409090539460541

Epoch: 6| Step: 1
Training loss: 3.0435400009155273
Validation loss: 2.402866091779483

Epoch: 6| Step: 2
Training loss: 3.062682867050171
Validation loss: 2.4069481741997505

Epoch: 6| Step: 3
Training loss: 3.288445234298706
Validation loss: 2.4015748218823503

Epoch: 6| Step: 4
Training loss: 2.6241068840026855
Validation loss: 2.4055803463023198

Epoch: 6| Step: 5
Training loss: 2.2499802112579346
Validation loss: 2.404348870759369

Epoch: 6| Step: 6
Training loss: 2.603316307067871
Validation loss: 2.405724666451895

Epoch: 6| Step: 7
Training loss: 2.523564100265503
Validation loss: 2.4172326364824848

Epoch: 6| Step: 8
Training loss: 2.5402660369873047
Validation loss: 2.4284569396767566

Epoch: 6| Step: 9
Training loss: 1.835207462310791
Validation loss: 2.434986032465453

Epoch: 6| Step: 10
Training loss: 2.6507749557495117
Validation loss: 2.448332768614574

Epoch: 6| Step: 11
Training loss: 2.347757339477539
Validation loss: 2.4486615580897175

Epoch: 6| Step: 12
Training loss: 2.867177963256836
Validation loss: 2.4241445987455306

Epoch: 6| Step: 13
Training loss: 2.587787628173828
Validation loss: 2.4208476389608076

Epoch: 106| Step: 0
Training loss: 2.410243511199951
Validation loss: 2.418204238337855

Epoch: 6| Step: 1
Training loss: 2.802639961242676
Validation loss: 2.419994227347835

Epoch: 6| Step: 2
Training loss: 2.5113279819488525
Validation loss: 2.4022009039437897

Epoch: 6| Step: 3
Training loss: 2.848660469055176
Validation loss: 2.4105029029230916

Epoch: 6| Step: 4
Training loss: 3.105226993560791
Validation loss: 2.410014083308558

Epoch: 6| Step: 5
Training loss: 2.4660487174987793
Validation loss: 2.4131296193727882

Epoch: 6| Step: 6
Training loss: 2.3016741275787354
Validation loss: 2.4003046020384757

Epoch: 6| Step: 7
Training loss: 2.4988632202148438
Validation loss: 2.406096896817607

Epoch: 6| Step: 8
Training loss: 2.873105525970459
Validation loss: 2.4121995343956897

Epoch: 6| Step: 9
Training loss: 2.890578031539917
Validation loss: 2.4099163137456423

Epoch: 6| Step: 10
Training loss: 2.361618995666504
Validation loss: 2.414116423617127

Epoch: 6| Step: 11
Training loss: 1.9175704717636108
Validation loss: 2.4191006947589178

Epoch: 6| Step: 12
Training loss: 3.1880898475646973
Validation loss: 2.4222608868793776

Epoch: 6| Step: 13
Training loss: 2.7631118297576904
Validation loss: 2.4185277082586802

Epoch: 107| Step: 0
Training loss: 2.083198308944702
Validation loss: 2.4218004390757573

Epoch: 6| Step: 1
Training loss: 2.806825637817383
Validation loss: 2.4203228412135953

Epoch: 6| Step: 2
Training loss: 2.44401478767395
Validation loss: 2.4277968329768025

Epoch: 6| Step: 3
Training loss: 3.37265682220459
Validation loss: 2.42249309632086

Epoch: 6| Step: 4
Training loss: 2.236076831817627
Validation loss: 2.414731538423928

Epoch: 6| Step: 5
Training loss: 2.9138927459716797
Validation loss: 2.4230874917840444

Epoch: 6| Step: 6
Training loss: 2.7320055961608887
Validation loss: 2.405992307970601

Epoch: 6| Step: 7
Training loss: 1.9936456680297852
Validation loss: 2.408512948661722

Epoch: 6| Step: 8
Training loss: 2.6228771209716797
Validation loss: 2.4065694706414336

Epoch: 6| Step: 9
Training loss: 2.7200005054473877
Validation loss: 2.4034286237532094

Epoch: 6| Step: 10
Training loss: 2.657203197479248
Validation loss: 2.400290568669637

Epoch: 6| Step: 11
Training loss: 3.614917755126953
Validation loss: 2.396672817968553

Epoch: 6| Step: 12
Training loss: 2.6079771518707275
Validation loss: 2.3978679821055424

Epoch: 6| Step: 13
Training loss: 1.6914812326431274
Validation loss: 2.392743915639898

Epoch: 108| Step: 0
Training loss: 2.974109649658203
Validation loss: 2.4029130499850035

Epoch: 6| Step: 1
Training loss: 2.509019374847412
Validation loss: 2.40794869904877

Epoch: 6| Step: 2
Training loss: 1.942141056060791
Validation loss: 2.4204707017508884

Epoch: 6| Step: 3
Training loss: 2.556131362915039
Validation loss: 2.417453473614108

Epoch: 6| Step: 4
Training loss: 2.8598196506500244
Validation loss: 2.4248219766924457

Epoch: 6| Step: 5
Training loss: 2.714622974395752
Validation loss: 2.4242139067701114

Epoch: 6| Step: 6
Training loss: 3.0850045680999756
Validation loss: 2.431093454360962

Epoch: 6| Step: 7
Training loss: 2.50626277923584
Validation loss: 2.4189087960027877

Epoch: 6| Step: 8
Training loss: 1.846179723739624
Validation loss: 2.4212775794408654

Epoch: 6| Step: 9
Training loss: 2.6477813720703125
Validation loss: 2.4031983447331253

Epoch: 6| Step: 10
Training loss: 2.868122100830078
Validation loss: 2.4089281417990245

Epoch: 6| Step: 11
Training loss: 3.0641679763793945
Validation loss: 2.414149622763357

Epoch: 6| Step: 12
Training loss: 2.373661518096924
Validation loss: 2.4141697114513767

Epoch: 6| Step: 13
Training loss: 3.362394094467163
Validation loss: 2.405627809545045

Epoch: 109| Step: 0
Training loss: 2.9031176567077637
Validation loss: 2.4023748315790647

Epoch: 6| Step: 1
Training loss: 2.5664682388305664
Validation loss: 2.4041791474947365

Epoch: 6| Step: 2
Training loss: 2.6563949584960938
Validation loss: 2.4093369771075506

Epoch: 6| Step: 3
Training loss: 2.546696186065674
Validation loss: 2.4213610438890356

Epoch: 6| Step: 4
Training loss: 2.6625585556030273
Validation loss: 2.4191128617973736

Epoch: 6| Step: 5
Training loss: 3.3169045448303223
Validation loss: 2.423708431182369

Epoch: 6| Step: 6
Training loss: 2.6669960021972656
Validation loss: 2.4317521561858473

Epoch: 6| Step: 7
Training loss: 2.2630515098571777
Validation loss: 2.4320998960925686

Epoch: 6| Step: 8
Training loss: 2.379561424255371
Validation loss: 2.4348879116837696

Epoch: 6| Step: 9
Training loss: 3.0591845512390137
Validation loss: 2.435773129104286

Epoch: 6| Step: 10
Training loss: 2.543753147125244
Validation loss: 2.427674021772159

Epoch: 6| Step: 11
Training loss: 3.3852100372314453
Validation loss: 2.418100457037649

Epoch: 6| Step: 12
Training loss: 1.5904266834259033
Validation loss: 2.4071525604494157

Epoch: 6| Step: 13
Training loss: 2.2929270267486572
Validation loss: 2.39717689893579

Epoch: 110| Step: 0
Training loss: 3.015810489654541
Validation loss: 2.3965905584314817

Epoch: 6| Step: 1
Training loss: 2.642892599105835
Validation loss: 2.394930024300852

Epoch: 6| Step: 2
Training loss: 2.442594528198242
Validation loss: 2.3952566859542683

Epoch: 6| Step: 3
Training loss: 2.407471179962158
Validation loss: 2.3984465188877557

Epoch: 6| Step: 4
Training loss: 2.475780963897705
Validation loss: 2.39659438338331

Epoch: 6| Step: 5
Training loss: 2.9054296016693115
Validation loss: 2.3946108638599353

Epoch: 6| Step: 6
Training loss: 2.6150503158569336
Validation loss: 2.3922894129189114

Epoch: 6| Step: 7
Training loss: 2.2090418338775635
Validation loss: 2.387762697794104

Epoch: 6| Step: 8
Training loss: 2.9748191833496094
Validation loss: 2.38625140600307

Epoch: 6| Step: 9
Training loss: 3.1533961296081543
Validation loss: 2.3936458761974047

Epoch: 6| Step: 10
Training loss: 2.346041679382324
Validation loss: 2.3922912664310907

Epoch: 6| Step: 11
Training loss: 2.5521631240844727
Validation loss: 2.3920465438596663

Epoch: 6| Step: 12
Training loss: 2.5143988132476807
Validation loss: 2.3942379848931425

Epoch: 6| Step: 13
Training loss: 2.6706840991973877
Validation loss: 2.3967096626117663

Epoch: 111| Step: 0
Training loss: 2.6381731033325195
Validation loss: 2.400418448191817

Epoch: 6| Step: 1
Training loss: 2.1999502182006836
Validation loss: 2.401170297335553

Epoch: 6| Step: 2
Training loss: 2.2826642990112305
Validation loss: 2.4046432407953406

Epoch: 6| Step: 3
Training loss: 2.888350486755371
Validation loss: 2.4019960895661385

Epoch: 6| Step: 4
Training loss: 2.972165107727051
Validation loss: 2.3980081696664133

Epoch: 6| Step: 5
Training loss: 2.8803186416625977
Validation loss: 2.396503912505283

Epoch: 6| Step: 6
Training loss: 2.460554599761963
Validation loss: 2.3882939636066394

Epoch: 6| Step: 7
Training loss: 2.360391139984131
Validation loss: 2.3885279791329497

Epoch: 6| Step: 8
Training loss: 2.9661660194396973
Validation loss: 2.3915193670539447

Epoch: 6| Step: 9
Training loss: 2.426847457885742
Validation loss: 2.388980975715063

Epoch: 6| Step: 10
Training loss: 3.2055768966674805
Validation loss: 2.3858170099155878

Epoch: 6| Step: 11
Training loss: 2.544067859649658
Validation loss: 2.394306646880283

Epoch: 6| Step: 12
Training loss: 2.3483946323394775
Validation loss: 2.3872688226802374

Epoch: 6| Step: 13
Training loss: 2.703831195831299
Validation loss: 2.3941680615948093

Epoch: 112| Step: 0
Training loss: 3.072011947631836
Validation loss: 2.396826764588715

Epoch: 6| Step: 1
Training loss: 2.033156394958496
Validation loss: 2.405054200080133

Epoch: 6| Step: 2
Training loss: 2.953361988067627
Validation loss: 2.4017161066814134

Epoch: 6| Step: 3
Training loss: 3.0828046798706055
Validation loss: 2.4022565554547053

Epoch: 6| Step: 4
Training loss: 2.619969606399536
Validation loss: 2.407230891207213

Epoch: 6| Step: 5
Training loss: 2.538337230682373
Validation loss: 2.4093948589858187

Epoch: 6| Step: 6
Training loss: 2.930824041366577
Validation loss: 2.3949553402521278

Epoch: 6| Step: 7
Training loss: 2.473996639251709
Validation loss: 2.3907777519636255

Epoch: 6| Step: 8
Training loss: 2.5638599395751953
Validation loss: 2.385380134787611

Epoch: 6| Step: 9
Training loss: 2.2143449783325195
Validation loss: 2.3854157258105535

Epoch: 6| Step: 10
Training loss: 2.4207520484924316
Validation loss: 2.3872041086996756

Epoch: 6| Step: 11
Training loss: 2.7551896572113037
Validation loss: 2.382488573751142

Epoch: 6| Step: 12
Training loss: 2.569218635559082
Validation loss: 2.384777956111457

Epoch: 6| Step: 13
Training loss: 2.674931526184082
Validation loss: 2.3860823569759244

Epoch: 113| Step: 0
Training loss: 3.070863723754883
Validation loss: 2.394779984669019

Epoch: 6| Step: 1
Training loss: 2.075462818145752
Validation loss: 2.4012355342988045

Epoch: 6| Step: 2
Training loss: 3.094642162322998
Validation loss: 2.3967609738790863

Epoch: 6| Step: 3
Training loss: 2.426539659500122
Validation loss: 2.39672609554824

Epoch: 6| Step: 4
Training loss: 2.6652469635009766
Validation loss: 2.3857576923985637

Epoch: 6| Step: 5
Training loss: 2.137272357940674
Validation loss: 2.3843098840405865

Epoch: 6| Step: 6
Training loss: 2.8207268714904785
Validation loss: 2.3828319016323296

Epoch: 6| Step: 7
Training loss: 2.86187744140625
Validation loss: 2.3867172425793064

Epoch: 6| Step: 8
Training loss: 2.9562909603118896
Validation loss: 2.3915091817097

Epoch: 6| Step: 9
Training loss: 2.478914737701416
Validation loss: 2.3971819928897324

Epoch: 6| Step: 10
Training loss: 2.5030293464660645
Validation loss: 2.4072516938691497

Epoch: 6| Step: 11
Training loss: 2.679142475128174
Validation loss: 2.4137312789117136

Epoch: 6| Step: 12
Training loss: 2.8282105922698975
Validation loss: 2.4189307228211434

Epoch: 6| Step: 13
Training loss: 2.0054142475128174
Validation loss: 2.4309384515208583

Epoch: 114| Step: 0
Training loss: 3.107046365737915
Validation loss: 2.4266644421444146

Epoch: 6| Step: 1
Training loss: 2.4245314598083496
Validation loss: 2.4128387538335656

Epoch: 6| Step: 2
Training loss: 2.6564314365386963
Validation loss: 2.423715217139131

Epoch: 6| Step: 3
Training loss: 2.4810397624969482
Validation loss: 2.419490727045203

Epoch: 6| Step: 4
Training loss: 2.516472816467285
Validation loss: 2.418569718637774

Epoch: 6| Step: 5
Training loss: 2.690176486968994
Validation loss: 2.4140505047254663

Epoch: 6| Step: 6
Training loss: 2.8660025596618652
Validation loss: 2.4063008011028333

Epoch: 6| Step: 7
Training loss: 2.2797045707702637
Validation loss: 2.404438562290643

Epoch: 6| Step: 8
Training loss: 2.788644552230835
Validation loss: 2.398426722454768

Epoch: 6| Step: 9
Training loss: 2.2572312355041504
Validation loss: 2.39396103864075

Epoch: 6| Step: 10
Training loss: 2.410958766937256
Validation loss: 2.382070892600603

Epoch: 6| Step: 11
Training loss: 3.553637742996216
Validation loss: 2.3855126544993412

Epoch: 6| Step: 12
Training loss: 2.2305655479431152
Validation loss: 2.386199817862562

Epoch: 6| Step: 13
Training loss: 2.7078256607055664
Validation loss: 2.3856876665546047

Epoch: 115| Step: 0
Training loss: 2.845107316970825
Validation loss: 2.3920393605386057

Epoch: 6| Step: 1
Training loss: 2.7501516342163086
Validation loss: 2.386668369334231

Epoch: 6| Step: 2
Training loss: 2.3308210372924805
Validation loss: 2.393530479041479

Epoch: 6| Step: 3
Training loss: 2.3939990997314453
Validation loss: 2.38888713108596

Epoch: 6| Step: 4
Training loss: 2.595334768295288
Validation loss: 2.389525011021604

Epoch: 6| Step: 5
Training loss: 2.992347240447998
Validation loss: 2.3851736668617494

Epoch: 6| Step: 6
Training loss: 2.669063091278076
Validation loss: 2.3834328548882597

Epoch: 6| Step: 7
Training loss: 2.4485998153686523
Validation loss: 2.388472457085886

Epoch: 6| Step: 8
Training loss: 2.119640588760376
Validation loss: 2.388750071166664

Epoch: 6| Step: 9
Training loss: 3.084937334060669
Validation loss: 2.3957650610195693

Epoch: 6| Step: 10
Training loss: 2.828857421875
Validation loss: 2.4077782605284

Epoch: 6| Step: 11
Training loss: 2.5282106399536133
Validation loss: 2.421028624298752

Epoch: 6| Step: 12
Training loss: 2.892636299133301
Validation loss: 2.421824985934842

Epoch: 6| Step: 13
Training loss: 2.0814013481140137
Validation loss: 2.4277412711933093

Epoch: 116| Step: 0
Training loss: 2.7811427116394043
Validation loss: 2.438877938896097

Epoch: 6| Step: 1
Training loss: 1.9430956840515137
Validation loss: 2.42862795757991

Epoch: 6| Step: 2
Training loss: 3.4304232597351074
Validation loss: 2.433224206329674

Epoch: 6| Step: 3
Training loss: 2.3170571327209473
Validation loss: 2.4292908971027662

Epoch: 6| Step: 4
Training loss: 2.7243852615356445
Validation loss: 2.4275006863378708

Epoch: 6| Step: 5
Training loss: 2.719557285308838
Validation loss: 2.426336963971456

Epoch: 6| Step: 6
Training loss: 2.558353900909424
Validation loss: 2.41519340392082

Epoch: 6| Step: 7
Training loss: 2.9437029361724854
Validation loss: 2.407264406963061

Epoch: 6| Step: 8
Training loss: 1.4801533222198486
Validation loss: 2.3991037491829164

Epoch: 6| Step: 9
Training loss: 2.8094468116760254
Validation loss: 2.3921627459987516

Epoch: 6| Step: 10
Training loss: 2.383209705352783
Validation loss: 2.390495995039581

Epoch: 6| Step: 11
Training loss: 3.466567039489746
Validation loss: 2.3882974040123726

Epoch: 6| Step: 12
Training loss: 2.543057680130005
Validation loss: 2.384474756897137

Epoch: 6| Step: 13
Training loss: 2.8220698833465576
Validation loss: 2.3877204259236655

Epoch: 117| Step: 0
Training loss: 3.470858573913574
Validation loss: 2.3879663354607037

Epoch: 6| Step: 1
Training loss: 2.14939546585083
Validation loss: 2.3795700509061097

Epoch: 6| Step: 2
Training loss: 2.4762258529663086
Validation loss: 2.3803717987511748

Epoch: 6| Step: 3
Training loss: 2.2738404273986816
Validation loss: 2.3784783399233254

Epoch: 6| Step: 4
Training loss: 2.840463399887085
Validation loss: 2.377257397097926

Epoch: 6| Step: 5
Training loss: 2.8579487800598145
Validation loss: 2.3791759321766515

Epoch: 6| Step: 6
Training loss: 2.459568500518799
Validation loss: 2.3824981156215874

Epoch: 6| Step: 7
Training loss: 2.1861400604248047
Validation loss: 2.3748040814553537

Epoch: 6| Step: 8
Training loss: 2.471470355987549
Validation loss: 2.37308568339194

Epoch: 6| Step: 9
Training loss: 2.54919695854187
Validation loss: 2.3779148081297516

Epoch: 6| Step: 10
Training loss: 2.38464617729187
Validation loss: 2.3786327403078795

Epoch: 6| Step: 11
Training loss: 2.726651668548584
Validation loss: 2.3724873373585362

Epoch: 6| Step: 12
Training loss: 3.0873022079467773
Validation loss: 2.381321786552347

Epoch: 6| Step: 13
Training loss: 3.1661624908447266
Validation loss: 2.3789683208670667

Epoch: 118| Step: 0
Training loss: 2.471736431121826
Validation loss: 2.38324317368128

Epoch: 6| Step: 1
Training loss: 2.442188262939453
Validation loss: 2.3882595980039207

Epoch: 6| Step: 2
Training loss: 1.6170517206192017
Validation loss: 2.3861595610136628

Epoch: 6| Step: 3
Training loss: 3.251425266265869
Validation loss: 2.3987979273642264

Epoch: 6| Step: 4
Training loss: 2.5546460151672363
Validation loss: 2.4013920804505706

Epoch: 6| Step: 5
Training loss: 2.317276954650879
Validation loss: 2.4111073555484897

Epoch: 6| Step: 6
Training loss: 2.2416350841522217
Validation loss: 2.409059586063508

Epoch: 6| Step: 7
Training loss: 2.9224047660827637
Validation loss: 2.419455115513135

Epoch: 6| Step: 8
Training loss: 3.202653408050537
Validation loss: 2.424979422682075

Epoch: 6| Step: 9
Training loss: 3.510826587677002
Validation loss: 2.416575921479092

Epoch: 6| Step: 10
Training loss: 2.2860240936279297
Validation loss: 2.4108221915460404

Epoch: 6| Step: 11
Training loss: 3.0463926792144775
Validation loss: 2.414209432499383

Epoch: 6| Step: 12
Training loss: 2.2142934799194336
Validation loss: 2.3991972220841276

Epoch: 6| Step: 13
Training loss: 2.7498903274536133
Validation loss: 2.391651940602128

Epoch: 119| Step: 0
Training loss: 2.1571743488311768
Validation loss: 2.3927689418997815

Epoch: 6| Step: 1
Training loss: 2.688345432281494
Validation loss: 2.38857259801639

Epoch: 6| Step: 2
Training loss: 2.200981616973877
Validation loss: 2.3924599539849067

Epoch: 6| Step: 3
Training loss: 2.3719983100891113
Validation loss: 2.3897160663399646

Epoch: 6| Step: 4
Training loss: 2.5571999549865723
Validation loss: 2.400354234121179

Epoch: 6| Step: 5
Training loss: 2.596630573272705
Validation loss: 2.402090921196886

Epoch: 6| Step: 6
Training loss: 2.443939685821533
Validation loss: 2.407971707723474

Epoch: 6| Step: 7
Training loss: 3.5024094581604004
Validation loss: 2.4052318526852514

Epoch: 6| Step: 8
Training loss: 3.2610621452331543
Validation loss: 2.4013533925497406

Epoch: 6| Step: 9
Training loss: 2.0207319259643555
Validation loss: 2.4038195879228654

Epoch: 6| Step: 10
Training loss: 2.4804375171661377
Validation loss: 2.399330928761472

Epoch: 6| Step: 11
Training loss: 2.812225341796875
Validation loss: 2.391226996657669

Epoch: 6| Step: 12
Training loss: 3.109480857849121
Validation loss: 2.3894727204435613

Epoch: 6| Step: 13
Training loss: 2.6190030574798584
Validation loss: 2.3928556108987458

Epoch: 120| Step: 0
Training loss: 2.263352155685425
Validation loss: 2.4018568582432245

Epoch: 6| Step: 1
Training loss: 2.752450942993164
Validation loss: 2.3866299916339178

Epoch: 6| Step: 2
Training loss: 1.4492597579956055
Validation loss: 2.383293185182797

Epoch: 6| Step: 3
Training loss: 2.8826794624328613
Validation loss: 2.3807662379357124

Epoch: 6| Step: 4
Training loss: 2.8891592025756836
Validation loss: 2.378000492690712

Epoch: 6| Step: 5
Training loss: 3.155247688293457
Validation loss: 2.3717793828697613

Epoch: 6| Step: 6
Training loss: 2.409679889678955
Validation loss: 2.381084601084391

Epoch: 6| Step: 7
Training loss: 2.0655670166015625
Validation loss: 2.380168245684716

Epoch: 6| Step: 8
Training loss: 2.3565752506256104
Validation loss: 2.3670518295739287

Epoch: 6| Step: 9
Training loss: 3.390805721282959
Validation loss: 2.3760416943539857

Epoch: 6| Step: 10
Training loss: 2.3703620433807373
Validation loss: 2.3679880942067792

Epoch: 6| Step: 11
Training loss: 2.266406774520874
Validation loss: 2.3775368005998674

Epoch: 6| Step: 12
Training loss: 3.490109443664551
Validation loss: 2.36808596375168

Epoch: 6| Step: 13
Training loss: 3.3086040019989014
Validation loss: 2.3726576835878435

Epoch: 121| Step: 0
Training loss: 2.2163169384002686
Validation loss: 2.3786031623040476

Epoch: 6| Step: 1
Training loss: 2.461975574493408
Validation loss: 2.3797388589510353

Epoch: 6| Step: 2
Training loss: 2.8277533054351807
Validation loss: 2.385271562043057

Epoch: 6| Step: 3
Training loss: 2.6178693771362305
Validation loss: 2.385265347778156

Epoch: 6| Step: 4
Training loss: 1.9864319562911987
Validation loss: 2.393823303202147

Epoch: 6| Step: 5
Training loss: 2.519003391265869
Validation loss: 2.4051320911735616

Epoch: 6| Step: 6
Training loss: 3.3989949226379395
Validation loss: 2.413712957853912

Epoch: 6| Step: 7
Training loss: 2.408275604248047
Validation loss: 2.421880232390537

Epoch: 6| Step: 8
Training loss: 2.5301856994628906
Validation loss: 2.4448079345046834

Epoch: 6| Step: 9
Training loss: 3.0207431316375732
Validation loss: 2.437164665550314

Epoch: 6| Step: 10
Training loss: 3.0167131423950195
Validation loss: 2.429418240824053

Epoch: 6| Step: 11
Training loss: 2.653304100036621
Validation loss: 2.4251384145470074

Epoch: 6| Step: 12
Training loss: 3.2937684059143066
Validation loss: 2.39918428338984

Epoch: 6| Step: 13
Training loss: 1.4075636863708496
Validation loss: 2.3840187672645814

Epoch: 122| Step: 0
Training loss: 2.6497650146484375
Validation loss: 2.379466656715639

Epoch: 6| Step: 1
Training loss: 2.6858882904052734
Validation loss: 2.3732116324927217

Epoch: 6| Step: 2
Training loss: 3.113154888153076
Validation loss: 2.376774880193895

Epoch: 6| Step: 3
Training loss: 2.011350154876709
Validation loss: 2.369825839996338

Epoch: 6| Step: 4
Training loss: 2.591545820236206
Validation loss: 2.3755121987353087

Epoch: 6| Step: 5
Training loss: 3.262040615081787
Validation loss: 2.3721155402480916

Epoch: 6| Step: 6
Training loss: 2.484034538269043
Validation loss: 2.3708211234820786

Epoch: 6| Step: 7
Training loss: 2.2841291427612305
Validation loss: 2.376373608907064

Epoch: 6| Step: 8
Training loss: 1.9302852153778076
Validation loss: 2.3674894097030803

Epoch: 6| Step: 9
Training loss: 2.2186038494110107
Validation loss: 2.373291154061594

Epoch: 6| Step: 10
Training loss: 2.4143338203430176
Validation loss: 2.377029718891267

Epoch: 6| Step: 11
Training loss: 3.3905527591705322
Validation loss: 2.3746652833877073

Epoch: 6| Step: 12
Training loss: 2.997849225997925
Validation loss: 2.374468545759878

Epoch: 6| Step: 13
Training loss: 3.017221689224243
Validation loss: 2.380178297719648

Epoch: 123| Step: 0
Training loss: 2.560426712036133
Validation loss: 2.3745416543817006

Epoch: 6| Step: 1
Training loss: 1.6816060543060303
Validation loss: 2.374597636602258

Epoch: 6| Step: 2
Training loss: 3.0286965370178223
Validation loss: 2.381010563142838

Epoch: 6| Step: 3
Training loss: 2.65356183052063
Validation loss: 2.4024618889695857

Epoch: 6| Step: 4
Training loss: 2.8235087394714355
Validation loss: 2.4009392863960675

Epoch: 6| Step: 5
Training loss: 3.144932746887207
Validation loss: 2.401418133448529

Epoch: 6| Step: 6
Training loss: 2.1024205684661865
Validation loss: 2.4003149565830024

Epoch: 6| Step: 7
Training loss: 2.5778441429138184
Validation loss: 2.3977159812886226

Epoch: 6| Step: 8
Training loss: 2.674511194229126
Validation loss: 2.3934409003103934

Epoch: 6| Step: 9
Training loss: 2.569864273071289
Validation loss: 2.397189550502326

Epoch: 6| Step: 10
Training loss: 2.8367152214050293
Validation loss: 2.3890370835540113

Epoch: 6| Step: 11
Training loss: 2.7255358695983887
Validation loss: 2.3848096657824773

Epoch: 6| Step: 12
Training loss: 3.298168420791626
Validation loss: 2.3789046015790714

Epoch: 6| Step: 13
Training loss: 1.814807415008545
Validation loss: 2.379664473636176

Epoch: 124| Step: 0
Training loss: 2.7281808853149414
Validation loss: 2.3694741008102254

Epoch: 6| Step: 1
Training loss: 2.1340367794036865
Validation loss: 2.3732121888027398

Epoch: 6| Step: 2
Training loss: 2.3072493076324463
Validation loss: 2.3673807305674397

Epoch: 6| Step: 3
Training loss: 2.572624683380127
Validation loss: 2.3636651064759944

Epoch: 6| Step: 4
Training loss: 3.0981006622314453
Validation loss: 2.372676941656297

Epoch: 6| Step: 5
Training loss: 3.2244503498077393
Validation loss: 2.3681426561006935

Epoch: 6| Step: 6
Training loss: 2.527973175048828
Validation loss: 2.369590856695688

Epoch: 6| Step: 7
Training loss: 2.838107109069824
Validation loss: 2.3706788478359098

Epoch: 6| Step: 8
Training loss: 2.5770504474639893
Validation loss: 2.363441672376407

Epoch: 6| Step: 9
Training loss: 2.7451133728027344
Validation loss: 2.3677913668335124

Epoch: 6| Step: 10
Training loss: 2.6236915588378906
Validation loss: 2.3792008738363943

Epoch: 6| Step: 11
Training loss: 2.433568239212036
Validation loss: 2.3892325406433432

Epoch: 6| Step: 12
Training loss: 1.698542594909668
Validation loss: 2.4066403322322394

Epoch: 6| Step: 13
Training loss: 3.573894500732422
Validation loss: 2.4178981088822886

Epoch: 125| Step: 0
Training loss: 2.857194185256958
Validation loss: 2.4094947820068686

Epoch: 6| Step: 1
Training loss: 2.7251858711242676
Validation loss: 2.3862206038608345

Epoch: 6| Step: 2
Training loss: 2.959552764892578
Validation loss: 2.3855352504279024

Epoch: 6| Step: 3
Training loss: 3.0653023719787598
Validation loss: 2.3841386584825415

Epoch: 6| Step: 4
Training loss: 2.863236427307129
Validation loss: 2.377515064772739

Epoch: 6| Step: 5
Training loss: 1.975306749343872
Validation loss: 2.3789526877864713

Epoch: 6| Step: 6
Training loss: 2.614720106124878
Validation loss: 2.3845864316468597

Epoch: 6| Step: 7
Training loss: 2.688599109649658
Validation loss: 2.376634669560258

Epoch: 6| Step: 8
Training loss: 2.2722039222717285
Validation loss: 2.3761954410101778

Epoch: 6| Step: 9
Training loss: 2.104915142059326
Validation loss: 2.3892626685480916

Epoch: 6| Step: 10
Training loss: 2.0192177295684814
Validation loss: 2.3879514253267677

Epoch: 6| Step: 11
Training loss: 2.93057918548584
Validation loss: 2.371129881951117

Epoch: 6| Step: 12
Training loss: 2.9125988483428955
Validation loss: 2.3790772473940285

Epoch: 6| Step: 13
Training loss: 2.6083922386169434
Validation loss: 2.371836805856356

Epoch: 126| Step: 0
Training loss: 2.461535930633545
Validation loss: 2.3704633482040895

Epoch: 6| Step: 1
Training loss: 2.8463544845581055
Validation loss: 2.3672395624140257

Epoch: 6| Step: 2
Training loss: 2.305116891860962
Validation loss: 2.368088130028017

Epoch: 6| Step: 3
Training loss: 2.4764997959136963
Validation loss: 2.36142683798267

Epoch: 6| Step: 4
Training loss: 2.1241822242736816
Validation loss: 2.3639424283017396

Epoch: 6| Step: 5
Training loss: 3.10577130317688
Validation loss: 2.359982198284518

Epoch: 6| Step: 6
Training loss: 3.4149317741394043
Validation loss: 2.359814756660051

Epoch: 6| Step: 7
Training loss: 2.928783893585205
Validation loss: 2.3522284056550715

Epoch: 6| Step: 8
Training loss: 2.482631206512451
Validation loss: 2.3579756598318777

Epoch: 6| Step: 9
Training loss: 1.9811763763427734
Validation loss: 2.3618645642393377

Epoch: 6| Step: 10
Training loss: 2.539095401763916
Validation loss: 2.3569054501031035

Epoch: 6| Step: 11
Training loss: 3.174971103668213
Validation loss: 2.354617072689918

Epoch: 6| Step: 12
Training loss: 2.6885972023010254
Validation loss: 2.3647198010516424

Epoch: 6| Step: 13
Training loss: 1.635872483253479
Validation loss: 2.3593001827116935

Epoch: 127| Step: 0
Training loss: 3.1782357692718506
Validation loss: 2.3592631381045104

Epoch: 6| Step: 1
Training loss: 2.0025432109832764
Validation loss: 2.361998114534604

Epoch: 6| Step: 2
Training loss: 2.6088743209838867
Validation loss: 2.361446334469703

Epoch: 6| Step: 3
Training loss: 2.338230848312378
Validation loss: 2.3545260839564826

Epoch: 6| Step: 4
Training loss: 2.6359333992004395
Validation loss: 2.3663578956357894

Epoch: 6| Step: 5
Training loss: 2.870985984802246
Validation loss: 2.36593464625779

Epoch: 6| Step: 6
Training loss: 1.7803070545196533
Validation loss: 2.362753955266809

Epoch: 6| Step: 7
Training loss: 2.216827869415283
Validation loss: 2.374930750939154

Epoch: 6| Step: 8
Training loss: 3.8301353454589844
Validation loss: 2.373056037451631

Epoch: 6| Step: 9
Training loss: 2.0808072090148926
Validation loss: 2.37689882709134

Epoch: 6| Step: 10
Training loss: 2.6481149196624756
Validation loss: 2.3879320570217666

Epoch: 6| Step: 11
Training loss: 3.313220500946045
Validation loss: 2.3848340408776396

Epoch: 6| Step: 12
Training loss: 2.7677841186523438
Validation loss: 2.3733158150026874

Epoch: 6| Step: 13
Training loss: 2.145108461380005
Validation loss: 2.3754685899262786

Epoch: 128| Step: 0
Training loss: 2.722337007522583
Validation loss: 2.386923484904792

Epoch: 6| Step: 1
Training loss: 2.7140841484069824
Validation loss: 2.3850218288360105

Epoch: 6| Step: 2
Training loss: 2.127063274383545
Validation loss: 2.3902063754297074

Epoch: 6| Step: 3
Training loss: 2.449385643005371
Validation loss: 2.373215559990175

Epoch: 6| Step: 4
Training loss: 2.3426952362060547
Validation loss: 2.373521540754585

Epoch: 6| Step: 5
Training loss: 3.305520534515381
Validation loss: 2.369807186947074

Epoch: 6| Step: 6
Training loss: 3.097111225128174
Validation loss: 2.356522657537973

Epoch: 6| Step: 7
Training loss: 1.9971731901168823
Validation loss: 2.3618380279951197

Epoch: 6| Step: 8
Training loss: 3.2881417274475098
Validation loss: 2.3592497738458778

Epoch: 6| Step: 9
Training loss: 2.150589942932129
Validation loss: 2.364300666316863

Epoch: 6| Step: 10
Training loss: 2.456695556640625
Validation loss: 2.362123579107305

Epoch: 6| Step: 11
Training loss: 3.1350226402282715
Validation loss: 2.355969880216865

Epoch: 6| Step: 12
Training loss: 2.1762990951538086
Validation loss: 2.3569697974830546

Epoch: 6| Step: 13
Training loss: 2.680795907974243
Validation loss: 2.355875572850627

Epoch: 129| Step: 0
Training loss: 2.6175427436828613
Validation loss: 2.363781647015643

Epoch: 6| Step: 1
Training loss: 2.7538795471191406
Validation loss: 2.359313464933826

Epoch: 6| Step: 2
Training loss: 2.3965659141540527
Validation loss: 2.3678694130271993

Epoch: 6| Step: 3
Training loss: 3.1526970863342285
Validation loss: 2.365665184554233

Epoch: 6| Step: 4
Training loss: 2.4378488063812256
Validation loss: 2.3722205110775527

Epoch: 6| Step: 5
Training loss: 3.350616455078125
Validation loss: 2.3828694794767644

Epoch: 6| Step: 6
Training loss: 1.973427414894104
Validation loss: 2.392347381960961

Epoch: 6| Step: 7
Training loss: 2.8615710735321045
Validation loss: 2.391407287249001

Epoch: 6| Step: 8
Training loss: 2.7839488983154297
Validation loss: 2.3994956401086625

Epoch: 6| Step: 9
Training loss: 2.418039560317993
Validation loss: 2.407862481250558

Epoch: 6| Step: 10
Training loss: 1.954380989074707
Validation loss: 2.3839269761116273

Epoch: 6| Step: 11
Training loss: 2.5989675521850586
Validation loss: 2.37916281530934

Epoch: 6| Step: 12
Training loss: 2.942458152770996
Validation loss: 2.3705051816919798

Epoch: 6| Step: 13
Training loss: 2.2885429859161377
Validation loss: 2.358030037213397

Epoch: 130| Step: 0
Training loss: 3.3749332427978516
Validation loss: 2.3525483480063816

Epoch: 6| Step: 1
Training loss: 2.626159429550171
Validation loss: 2.3535624857871764

Epoch: 6| Step: 2
Training loss: 2.286846160888672
Validation loss: 2.353966607842394

Epoch: 6| Step: 3
Training loss: 2.8027846813201904
Validation loss: 2.357681287232266

Epoch: 6| Step: 4
Training loss: 3.107391357421875
Validation loss: 2.348614106896103

Epoch: 6| Step: 5
Training loss: 2.527662754058838
Validation loss: 2.354088842227895

Epoch: 6| Step: 6
Training loss: 2.503058433532715
Validation loss: 2.3641747095251597

Epoch: 6| Step: 7
Training loss: 1.9835009574890137
Validation loss: 2.359425865193849

Epoch: 6| Step: 8
Training loss: 3.123047351837158
Validation loss: 2.3650984571826075

Epoch: 6| Step: 9
Training loss: 1.9843523502349854
Validation loss: 2.361737507645802

Epoch: 6| Step: 10
Training loss: 3.4076285362243652
Validation loss: 2.355245162081975

Epoch: 6| Step: 11
Training loss: 2.2881076335906982
Validation loss: 2.356163550448674

Epoch: 6| Step: 12
Training loss: 2.5491244792938232
Validation loss: 2.3471629978508077

Epoch: 6| Step: 13
Training loss: 1.9932204484939575
Validation loss: 2.3488824623887257

Epoch: 131| Step: 0
Training loss: 3.0113792419433594
Validation loss: 2.3535669362673195

Epoch: 6| Step: 1
Training loss: 2.279489040374756
Validation loss: 2.368723230977212

Epoch: 6| Step: 2
Training loss: 2.428670644760132
Validation loss: 2.368990239276681

Epoch: 6| Step: 3
Training loss: 2.738675832748413
Validation loss: 2.386293093363444

Epoch: 6| Step: 4
Training loss: 1.8967238664627075
Validation loss: 2.4160121076850483

Epoch: 6| Step: 5
Training loss: 3.2487220764160156
Validation loss: 2.455766442001507

Epoch: 6| Step: 6
Training loss: 3.356344223022461
Validation loss: 2.4693863930240756

Epoch: 6| Step: 7
Training loss: 1.8397234678268433
Validation loss: 2.449787806439143

Epoch: 6| Step: 8
Training loss: 2.4922986030578613
Validation loss: 2.4367283851869646

Epoch: 6| Step: 9
Training loss: 2.693603992462158
Validation loss: 2.414587664347823

Epoch: 6| Step: 10
Training loss: 2.3657493591308594
Validation loss: 2.389308714097546

Epoch: 6| Step: 11
Training loss: 2.2516093254089355
Validation loss: 2.3684293198329147

Epoch: 6| Step: 12
Training loss: 2.925581216812134
Validation loss: 2.3609485395493044

Epoch: 6| Step: 13
Training loss: 3.6813998222351074
Validation loss: 2.3560183150793916

Epoch: 132| Step: 0
Training loss: 2.706064224243164
Validation loss: 2.354080838541831

Epoch: 6| Step: 1
Training loss: 2.368852376937866
Validation loss: 2.3515948826266873

Epoch: 6| Step: 2
Training loss: 2.8787548542022705
Validation loss: 2.3596574491070164

Epoch: 6| Step: 3
Training loss: 2.7520360946655273
Validation loss: 2.3612092592382945

Epoch: 6| Step: 4
Training loss: 2.7064573764801025
Validation loss: 2.3760064930044194

Epoch: 6| Step: 5
Training loss: 3.3599050045013428
Validation loss: 2.3728363001218407

Epoch: 6| Step: 6
Training loss: 2.834752082824707
Validation loss: 2.368669379142023

Epoch: 6| Step: 7
Training loss: 1.9332754611968994
Validation loss: 2.3518249770646453

Epoch: 6| Step: 8
Training loss: 2.0733134746551514
Validation loss: 2.35337721660573

Epoch: 6| Step: 9
Training loss: 2.5964784622192383
Validation loss: 2.3497449685168523

Epoch: 6| Step: 10
Training loss: 2.4401912689208984
Validation loss: 2.353251967378842

Epoch: 6| Step: 11
Training loss: 2.2989962100982666
Validation loss: 2.349292726926906

Epoch: 6| Step: 12
Training loss: 2.577157974243164
Validation loss: 2.3535926085646435

Epoch: 6| Step: 13
Training loss: 3.346299648284912
Validation loss: 2.3476415449573147

Epoch: 133| Step: 0
Training loss: 2.8225040435791016
Validation loss: 2.363049032867596

Epoch: 6| Step: 1
Training loss: 2.9992923736572266
Validation loss: 2.357936002874887

Epoch: 6| Step: 2
Training loss: 2.261164665222168
Validation loss: 2.367775768362066

Epoch: 6| Step: 3
Training loss: 2.6939892768859863
Validation loss: 2.369913334487587

Epoch: 6| Step: 4
Training loss: 1.8327116966247559
Validation loss: 2.3813334998264106

Epoch: 6| Step: 5
Training loss: 2.3510513305664062
Validation loss: 2.378911301653872

Epoch: 6| Step: 6
Training loss: 2.9266700744628906
Validation loss: 2.41108303172614

Epoch: 6| Step: 7
Training loss: 2.642984390258789
Validation loss: 2.4184041100163616

Epoch: 6| Step: 8
Training loss: 2.6385819911956787
Validation loss: 2.4648077257217897

Epoch: 6| Step: 9
Training loss: 2.829902172088623
Validation loss: 2.460248601052069

Epoch: 6| Step: 10
Training loss: 2.899374485015869
Validation loss: 2.4676887591679892

Epoch: 6| Step: 11
Training loss: 2.8547139167785645
Validation loss: 2.449999463173651

Epoch: 6| Step: 12
Training loss: 2.3821780681610107
Validation loss: 2.442408351488011

Epoch: 6| Step: 13
Training loss: 2.6525356769561768
Validation loss: 2.445357402165731

Epoch: 134| Step: 0
Training loss: 2.419731378555298
Validation loss: 2.443380222525648

Epoch: 6| Step: 1
Training loss: 3.081730365753174
Validation loss: 2.438898489039431

Epoch: 6| Step: 2
Training loss: 2.4775853157043457
Validation loss: 2.4359933650621803

Epoch: 6| Step: 3
Training loss: 2.943614959716797
Validation loss: 2.4351842557230303

Epoch: 6| Step: 4
Training loss: 2.8522043228149414
Validation loss: 2.44055111177506

Epoch: 6| Step: 5
Training loss: 2.802119731903076
Validation loss: 2.442432657364876

Epoch: 6| Step: 6
Training loss: 3.3813345432281494
Validation loss: 2.4461370975740495

Epoch: 6| Step: 7
Training loss: 1.7485260963439941
Validation loss: 2.4378662058102187

Epoch: 6| Step: 8
Training loss: 3.2597315311431885
Validation loss: 2.441764006050684

Epoch: 6| Step: 9
Training loss: 2.508415699005127
Validation loss: 2.4337779885979107

Epoch: 6| Step: 10
Training loss: 2.602590322494507
Validation loss: 2.430903729572091

Epoch: 6| Step: 11
Training loss: 1.8699108362197876
Validation loss: 2.4422765777957056

Epoch: 6| Step: 12
Training loss: 2.6736111640930176
Validation loss: 2.4472174157378492

Epoch: 6| Step: 13
Training loss: 2.552863121032715
Validation loss: 2.4537085922815467

Epoch: 135| Step: 0
Training loss: 2.3103485107421875
Validation loss: 2.4486212089497554

Epoch: 6| Step: 1
Training loss: 3.034846782684326
Validation loss: 2.452960668071624

Epoch: 6| Step: 2
Training loss: 2.063642978668213
Validation loss: 2.437321698793801

Epoch: 6| Step: 3
Training loss: 2.6355018615722656
Validation loss: 2.451139526982461

Epoch: 6| Step: 4
Training loss: 2.597491502761841
Validation loss: 2.444778933319994

Epoch: 6| Step: 5
Training loss: 3.002870559692383
Validation loss: 2.4347809078872844

Epoch: 6| Step: 6
Training loss: 2.99360990524292
Validation loss: 2.4330473587077153

Epoch: 6| Step: 7
Training loss: 2.5989701747894287
Validation loss: 2.4410928436504897

Epoch: 6| Step: 8
Training loss: 2.383373498916626
Validation loss: 2.4300646089738414

Epoch: 6| Step: 9
Training loss: 2.6954469680786133
Validation loss: 2.441382260732753

Epoch: 6| Step: 10
Training loss: 2.7228589057922363
Validation loss: 2.438618202363291

Epoch: 6| Step: 11
Training loss: 2.6210763454437256
Validation loss: 2.429095014449089

Epoch: 6| Step: 12
Training loss: 2.897909164428711
Validation loss: 2.437486210177022

Epoch: 6| Step: 13
Training loss: 2.2918245792388916
Validation loss: 2.428045500991165

Epoch: 136| Step: 0
Training loss: 2.228736162185669
Validation loss: 2.4264505088970227

Epoch: 6| Step: 1
Training loss: 2.829740047454834
Validation loss: 2.42651786855472

Epoch: 6| Step: 2
Training loss: 2.3488330841064453
Validation loss: 2.4178776433390956

Epoch: 6| Step: 3
Training loss: 3.0134310722351074
Validation loss: 2.4275186882224133

Epoch: 6| Step: 4
Training loss: 2.098505973815918
Validation loss: 2.412983884093582

Epoch: 6| Step: 5
Training loss: 2.864431619644165
Validation loss: 2.396741849119945

Epoch: 6| Step: 6
Training loss: 2.0329384803771973
Validation loss: 2.376102373164187

Epoch: 6| Step: 7
Training loss: 2.5397191047668457
Validation loss: 2.360969437065945

Epoch: 6| Step: 8
Training loss: 2.8322436809539795
Validation loss: 2.370484495675692

Epoch: 6| Step: 9
Training loss: 2.7839088439941406
Validation loss: 2.385181624402282

Epoch: 6| Step: 10
Training loss: 2.3529977798461914
Validation loss: 2.373095986663654

Epoch: 6| Step: 11
Training loss: 2.8771488666534424
Validation loss: 2.3694778667983187

Epoch: 6| Step: 12
Training loss: 3.327806234359741
Validation loss: 2.3625537964605514

Epoch: 6| Step: 13
Training loss: 2.376967191696167
Validation loss: 2.3698641305328696

Epoch: 137| Step: 0
Training loss: 1.8579914569854736
Validation loss: 2.3606537311307845

Epoch: 6| Step: 1
Training loss: 2.5476043224334717
Validation loss: 2.3609690204743417

Epoch: 6| Step: 2
Training loss: 2.3718454837799072
Validation loss: 2.3652831841540594

Epoch: 6| Step: 3
Training loss: 2.7581570148468018
Validation loss: 2.3762818254450315

Epoch: 6| Step: 4
Training loss: 2.9027063846588135
Validation loss: 2.388480478717435

Epoch: 6| Step: 5
Training loss: 2.4555208683013916
Validation loss: 2.3727778004061792

Epoch: 6| Step: 6
Training loss: 2.1896138191223145
Validation loss: 2.3717258617442143

Epoch: 6| Step: 7
Training loss: 2.8008549213409424
Validation loss: 2.3733701052204257

Epoch: 6| Step: 8
Training loss: 2.980611801147461
Validation loss: 2.3575239104609333

Epoch: 6| Step: 9
Training loss: 2.4864566326141357
Validation loss: 2.358057193858649

Epoch: 6| Step: 10
Training loss: 3.117161273956299
Validation loss: 2.3496391927042315

Epoch: 6| Step: 11
Training loss: 2.966913938522339
Validation loss: 2.347783045102191

Epoch: 6| Step: 12
Training loss: 2.292715072631836
Validation loss: 2.342490000109519

Epoch: 6| Step: 13
Training loss: 2.70980167388916
Validation loss: 2.33568073344487

Epoch: 138| Step: 0
Training loss: 3.4004385471343994
Validation loss: 2.339828811666017

Epoch: 6| Step: 1
Training loss: 2.1514101028442383
Validation loss: 2.339738051096598

Epoch: 6| Step: 2
Training loss: 2.92474365234375
Validation loss: 2.349065065383911

Epoch: 6| Step: 3
Training loss: 2.4015440940856934
Validation loss: 2.344092430606965

Epoch: 6| Step: 4
Training loss: 2.960174083709717
Validation loss: 2.3380401852310344

Epoch: 6| Step: 5
Training loss: 2.951843738555908
Validation loss: 2.3435897160601873

Epoch: 6| Step: 6
Training loss: 3.027379274368286
Validation loss: 2.3388985818432224

Epoch: 6| Step: 7
Training loss: 2.4820327758789062
Validation loss: 2.347088513835784

Epoch: 6| Step: 8
Training loss: 2.4109888076782227
Validation loss: 2.3416408313217985

Epoch: 6| Step: 9
Training loss: 1.7517573833465576
Validation loss: 2.348987892109861

Epoch: 6| Step: 10
Training loss: 2.2410569190979004
Validation loss: 2.3506828379887406

Epoch: 6| Step: 11
Training loss: 2.460164785385132
Validation loss: 2.343460031735

Epoch: 6| Step: 12
Training loss: 2.7202672958374023
Validation loss: 2.3536304479004233

Epoch: 6| Step: 13
Training loss: 2.459995746612549
Validation loss: 2.3533077880900395

Epoch: 139| Step: 0
Training loss: 2.504509925842285
Validation loss: 2.352346202378632

Epoch: 6| Step: 1
Training loss: 3.0430119037628174
Validation loss: 2.353536105925037

Epoch: 6| Step: 2
Training loss: 2.2004268169403076
Validation loss: 2.341902712339996

Epoch: 6| Step: 3
Training loss: 2.5185201168060303
Validation loss: 2.3515360893741732

Epoch: 6| Step: 4
Training loss: 2.9939048290252686
Validation loss: 2.346921013247582

Epoch: 6| Step: 5
Training loss: 2.7836966514587402
Validation loss: 2.3525542161797963

Epoch: 6| Step: 6
Training loss: 2.2224836349487305
Validation loss: 2.344685185340143

Epoch: 6| Step: 7
Training loss: 2.8266823291778564
Validation loss: 2.3444383875016244

Epoch: 6| Step: 8
Training loss: 2.5414156913757324
Validation loss: 2.3479933738708496

Epoch: 6| Step: 9
Training loss: 2.2601866722106934
Validation loss: 2.3473115992802445

Epoch: 6| Step: 10
Training loss: 2.3029112815856934
Validation loss: 2.3460846716357815

Epoch: 6| Step: 11
Training loss: 2.7363977432250977
Validation loss: 2.3551513302710747

Epoch: 6| Step: 12
Training loss: 3.1433279514312744
Validation loss: 2.3411867695470012

Epoch: 6| Step: 13
Training loss: 1.8506829738616943
Validation loss: 2.3514850011435886

Epoch: 140| Step: 0
Training loss: 2.2338738441467285
Validation loss: 2.372328999221966

Epoch: 6| Step: 1
Training loss: 2.590615749359131
Validation loss: 2.3666981061299643

Epoch: 6| Step: 2
Training loss: 2.2240805625915527
Validation loss: 2.3663781432695288

Epoch: 6| Step: 3
Training loss: 2.498322010040283
Validation loss: 2.3726233256760465

Epoch: 6| Step: 4
Training loss: 2.7615342140197754
Validation loss: 2.359111685906687

Epoch: 6| Step: 5
Training loss: 2.1876027584075928
Validation loss: 2.3526811445913007

Epoch: 6| Step: 6
Training loss: 2.68092679977417
Validation loss: 2.3551756412752214

Epoch: 6| Step: 7
Training loss: 2.7978577613830566
Validation loss: 2.3491597765235492

Epoch: 6| Step: 8
Training loss: 2.9438018798828125
Validation loss: 2.346125687322309

Epoch: 6| Step: 9
Training loss: 2.0027637481689453
Validation loss: 2.3410337355829056

Epoch: 6| Step: 10
Training loss: 3.3533287048339844
Validation loss: 2.3464044088958413

Epoch: 6| Step: 11
Training loss: 3.5912513732910156
Validation loss: 2.350082940952752

Epoch: 6| Step: 12
Training loss: 2.333491802215576
Validation loss: 2.3642912039192776

Epoch: 6| Step: 13
Training loss: 1.750328540802002
Validation loss: 2.353668902509956

Epoch: 141| Step: 0
Training loss: 2.3395938873291016
Validation loss: 2.3660845154075214

Epoch: 6| Step: 1
Training loss: 2.172344923019409
Validation loss: 2.368016848000147

Epoch: 6| Step: 2
Training loss: 1.906010389328003
Validation loss: 2.3781700057368123

Epoch: 6| Step: 3
Training loss: 3.1343297958374023
Validation loss: 2.3752630705474527

Epoch: 6| Step: 4
Training loss: 2.635807514190674
Validation loss: 2.3746671343362458

Epoch: 6| Step: 5
Training loss: 2.0559701919555664
Validation loss: 2.3921249105084326

Epoch: 6| Step: 6
Training loss: 2.339752197265625
Validation loss: 2.3852634301749607

Epoch: 6| Step: 7
Training loss: 3.2404050827026367
Validation loss: 2.386265347080846

Epoch: 6| Step: 8
Training loss: 2.7028512954711914
Validation loss: 2.3791267692401843

Epoch: 6| Step: 9
Training loss: 2.725949764251709
Validation loss: 2.3637745021491923

Epoch: 6| Step: 10
Training loss: 2.836345911026001
Validation loss: 2.3658015292177916

Epoch: 6| Step: 11
Training loss: 2.6752877235412598
Validation loss: 2.3543280042627805

Epoch: 6| Step: 12
Training loss: 2.539163112640381
Validation loss: 2.349555507782967

Epoch: 6| Step: 13
Training loss: 3.5333521366119385
Validation loss: 2.352729812745125

Epoch: 142| Step: 0
Training loss: 2.5309319496154785
Validation loss: 2.360100374426893

Epoch: 6| Step: 1
Training loss: 3.097341299057007
Validation loss: 2.3800146451560398

Epoch: 6| Step: 2
Training loss: 3.449068069458008
Validation loss: 2.3970936600879957

Epoch: 6| Step: 3
Training loss: 2.4024715423583984
Validation loss: 2.407241277797248

Epoch: 6| Step: 4
Training loss: 2.209076404571533
Validation loss: 2.4087736709143526

Epoch: 6| Step: 5
Training loss: 2.90622878074646
Validation loss: 2.418991268322032

Epoch: 6| Step: 6
Training loss: 2.4392592906951904
Validation loss: 2.4252846087178876

Epoch: 6| Step: 7
Training loss: 2.2275490760803223
Validation loss: 2.431154648462931

Epoch: 6| Step: 8
Training loss: 3.289257526397705
Validation loss: 2.4307797365291144

Epoch: 6| Step: 9
Training loss: 2.6724038124084473
Validation loss: 2.4348998941401

Epoch: 6| Step: 10
Training loss: 2.35589337348938
Validation loss: 2.4464091716274137

Epoch: 6| Step: 11
Training loss: 2.7068419456481934
Validation loss: 2.4468011727897068

Epoch: 6| Step: 12
Training loss: 2.51588773727417
Validation loss: 2.470653239116874

Epoch: 6| Step: 13
Training loss: 1.3726630210876465
Validation loss: 2.4616400426433933

Epoch: 143| Step: 0
Training loss: 2.698873281478882
Validation loss: 2.4558610095772693

Epoch: 6| Step: 1
Training loss: 3.0227479934692383
Validation loss: 2.4592366744113225

Epoch: 6| Step: 2
Training loss: 3.009366273880005
Validation loss: 2.431945446998842

Epoch: 6| Step: 3
Training loss: 2.256160020828247
Validation loss: 2.428501154786797

Epoch: 6| Step: 4
Training loss: 2.8636715412139893
Validation loss: 2.4173365152010353

Epoch: 6| Step: 5
Training loss: 2.3145148754119873
Validation loss: 2.415060832936277

Epoch: 6| Step: 6
Training loss: 2.680940628051758
Validation loss: 2.4077415107398905

Epoch: 6| Step: 7
Training loss: 2.3809316158294678
Validation loss: 2.4118690131812968

Epoch: 6| Step: 8
Training loss: 2.601759433746338
Validation loss: 2.399575671842021

Epoch: 6| Step: 9
Training loss: 2.7775392532348633
Validation loss: 2.4012978000025593

Epoch: 6| Step: 10
Training loss: 2.8712992668151855
Validation loss: 2.404419675950081

Epoch: 6| Step: 11
Training loss: 2.22823166847229
Validation loss: 2.401099233217137

Epoch: 6| Step: 12
Training loss: 2.707197666168213
Validation loss: 2.4028334591978338

Epoch: 6| Step: 13
Training loss: 2.3601856231689453
Validation loss: 2.389674804543936

Epoch: 144| Step: 0
Training loss: 1.6683297157287598
Validation loss: 2.3920099350713913

Epoch: 6| Step: 1
Training loss: 2.7162678241729736
Validation loss: 2.3942474780544156

Epoch: 6| Step: 2
Training loss: 2.472658395767212
Validation loss: 2.378669995133595

Epoch: 6| Step: 3
Training loss: 2.8798880577087402
Validation loss: 2.3644396207665883

Epoch: 6| Step: 4
Training loss: 2.9482064247131348
Validation loss: 2.35483129050142

Epoch: 6| Step: 5
Training loss: 2.6050610542297363
Validation loss: 2.361360473017539

Epoch: 6| Step: 6
Training loss: 3.0495824813842773
Validation loss: 2.370519152251623

Epoch: 6| Step: 7
Training loss: 2.4285144805908203
Validation loss: 2.363886594772339

Epoch: 6| Step: 8
Training loss: 2.4256715774536133
Validation loss: 2.35541194997808

Epoch: 6| Step: 9
Training loss: 2.383465528488159
Validation loss: 2.364685022702781

Epoch: 6| Step: 10
Training loss: 3.077486515045166
Validation loss: 2.3544401225223335

Epoch: 6| Step: 11
Training loss: 2.8520185947418213
Validation loss: 2.346884730041668

Epoch: 6| Step: 12
Training loss: 2.283944606781006
Validation loss: 2.3358861092598207

Epoch: 6| Step: 13
Training loss: 2.3745059967041016
Validation loss: 2.3329325106836136

Epoch: 145| Step: 0
Training loss: 2.5445592403411865
Validation loss: 2.327439652976169

Epoch: 6| Step: 1
Training loss: 2.819856643676758
Validation loss: 2.3318913957124114

Epoch: 6| Step: 2
Training loss: 2.3179173469543457
Validation loss: 2.325882283590173

Epoch: 6| Step: 3
Training loss: 3.231332302093506
Validation loss: 2.3291029109749743

Epoch: 6| Step: 4
Training loss: 1.7514653205871582
Validation loss: 2.33480449773932

Epoch: 6| Step: 5
Training loss: 2.79348087310791
Validation loss: 2.3284453371519684

Epoch: 6| Step: 6
Training loss: 3.280451774597168
Validation loss: 2.327302702011601

Epoch: 6| Step: 7
Training loss: 2.5030717849731445
Validation loss: 2.3360093973016225

Epoch: 6| Step: 8
Training loss: 2.9341349601745605
Validation loss: 2.351936791532783

Epoch: 6| Step: 9
Training loss: 2.4356014728546143
Validation loss: 2.3798850326127905

Epoch: 6| Step: 10
Training loss: 2.2692437171936035
Validation loss: 2.401269237200419

Epoch: 6| Step: 11
Training loss: 3.1334311962127686
Validation loss: 2.3992154367508425

Epoch: 6| Step: 12
Training loss: 2.0300917625427246
Validation loss: 2.374782987820205

Epoch: 6| Step: 13
Training loss: 2.022859573364258
Validation loss: 2.361741699198241

Epoch: 146| Step: 0
Training loss: 2.530963182449341
Validation loss: 2.341253703640353

Epoch: 6| Step: 1
Training loss: 2.0780797004699707
Validation loss: 2.3534868737702728

Epoch: 6| Step: 2
Training loss: 2.7497026920318604
Validation loss: 2.348359807845085

Epoch: 6| Step: 3
Training loss: 3.575176477432251
Validation loss: 2.3470150193860455

Epoch: 6| Step: 4
Training loss: 2.175173282623291
Validation loss: 2.335157914828229

Epoch: 6| Step: 5
Training loss: 2.5022201538085938
Validation loss: 2.3311208960830525

Epoch: 6| Step: 6
Training loss: 2.8957858085632324
Validation loss: 2.329834812430925

Epoch: 6| Step: 7
Training loss: 2.801455020904541
Validation loss: 2.3432029447247906

Epoch: 6| Step: 8
Training loss: 3.0708298683166504
Validation loss: 2.339687503794188

Epoch: 6| Step: 9
Training loss: 1.7549439668655396
Validation loss: 2.340547074553787

Epoch: 6| Step: 10
Training loss: 2.511136770248413
Validation loss: 2.342495882382957

Epoch: 6| Step: 11
Training loss: 2.7328290939331055
Validation loss: 2.3423597351197274

Epoch: 6| Step: 12
Training loss: 2.07846736907959
Validation loss: 2.337848319802233

Epoch: 6| Step: 13
Training loss: 2.8274035453796387
Validation loss: 2.334170731165076

Epoch: 147| Step: 0
Training loss: 2.9388556480407715
Validation loss: 2.3316870556082776

Epoch: 6| Step: 1
Training loss: 2.2046337127685547
Validation loss: 2.322276540981826

Epoch: 6| Step: 2
Training loss: 2.16762638092041
Validation loss: 2.3222320643804406

Epoch: 6| Step: 3
Training loss: 2.319080114364624
Validation loss: 2.3306998539996404

Epoch: 6| Step: 4
Training loss: 2.075282096862793
Validation loss: 2.3324892777268604

Epoch: 6| Step: 5
Training loss: 2.502603530883789
Validation loss: 2.3316419265603505

Epoch: 6| Step: 6
Training loss: 2.8900132179260254
Validation loss: 2.340428029337237

Epoch: 6| Step: 7
Training loss: 2.896745204925537
Validation loss: 2.3525007514543432

Epoch: 6| Step: 8
Training loss: 3.4032535552978516
Validation loss: 2.3632186894775717

Epoch: 6| Step: 9
Training loss: 2.165440082550049
Validation loss: 2.372026030735303

Epoch: 6| Step: 10
Training loss: 3.138556957244873
Validation loss: 2.3581441294762397

Epoch: 6| Step: 11
Training loss: 2.319082736968994
Validation loss: 2.366667301424088

Epoch: 6| Step: 12
Training loss: 2.5469353199005127
Validation loss: 2.345769090037192

Epoch: 6| Step: 13
Training loss: 2.669196128845215
Validation loss: 2.3381322096752863

Epoch: 148| Step: 0
Training loss: 2.8470897674560547
Validation loss: 2.348213641874252

Epoch: 6| Step: 1
Training loss: 2.405972719192505
Validation loss: 2.3336196868650374

Epoch: 6| Step: 2
Training loss: 2.4910728931427
Validation loss: 2.332683886251142

Epoch: 6| Step: 3
Training loss: 2.851837635040283
Validation loss: 2.331013084739767

Epoch: 6| Step: 4
Training loss: 2.6906721591949463
Validation loss: 2.3256613977493776

Epoch: 6| Step: 5
Training loss: 2.0350098609924316
Validation loss: 2.3230109291691936

Epoch: 6| Step: 6
Training loss: 2.2675886154174805
Validation loss: 2.3281827485689552

Epoch: 6| Step: 7
Training loss: 3.0230941772460938
Validation loss: 2.321313455540647

Epoch: 6| Step: 8
Training loss: 2.49027681350708
Validation loss: 2.3309870612236763

Epoch: 6| Step: 9
Training loss: 3.3295669555664062
Validation loss: 2.334636817696274

Epoch: 6| Step: 10
Training loss: 2.150106430053711
Validation loss: 2.3319165565634288

Epoch: 6| Step: 11
Training loss: 2.1008734703063965
Validation loss: 2.34102028159685

Epoch: 6| Step: 12
Training loss: 2.696728229522705
Validation loss: 2.335114299610097

Epoch: 6| Step: 13
Training loss: 3.0218522548675537
Validation loss: 2.3465973997628815

Epoch: 149| Step: 0
Training loss: 2.487274646759033
Validation loss: 2.3370447466450353

Epoch: 6| Step: 1
Training loss: 2.35109806060791
Validation loss: 2.350307792745611

Epoch: 6| Step: 2
Training loss: 2.3613696098327637
Validation loss: 2.359062953661847

Epoch: 6| Step: 3
Training loss: 1.8402619361877441
Validation loss: 2.3790547745202177

Epoch: 6| Step: 4
Training loss: 2.866107940673828
Validation loss: 2.3866305069256852

Epoch: 6| Step: 5
Training loss: 2.0395150184631348
Validation loss: 2.396776509541337

Epoch: 6| Step: 6
Training loss: 2.6353235244750977
Validation loss: 2.4044632501499628

Epoch: 6| Step: 7
Training loss: 3.13266658782959
Validation loss: 2.4217208175249

Epoch: 6| Step: 8
Training loss: 3.1686158180236816
Validation loss: 2.39362265986781

Epoch: 6| Step: 9
Training loss: 3.2100369930267334
Validation loss: 2.3511998858503116

Epoch: 6| Step: 10
Training loss: 2.3490710258483887
Validation loss: 2.3322315498064925

Epoch: 6| Step: 11
Training loss: 2.7672083377838135
Validation loss: 2.323034606954103

Epoch: 6| Step: 12
Training loss: 3.0555572509765625
Validation loss: 2.3166905192918676

Epoch: 6| Step: 13
Training loss: 1.6253809928894043
Validation loss: 2.3209560737814954

Epoch: 150| Step: 0
Training loss: 2.375507116317749
Validation loss: 2.3218876443883425

Epoch: 6| Step: 1
Training loss: 1.5704649686813354
Validation loss: 2.3216195670507287

Epoch: 6| Step: 2
Training loss: 3.2444629669189453
Validation loss: 2.3310373008892102

Epoch: 6| Step: 3
Training loss: 2.3251938819885254
Validation loss: 2.326206276493688

Epoch: 6| Step: 4
Training loss: 3.7493057250976562
Validation loss: 2.332976092574417

Epoch: 6| Step: 5
Training loss: 2.493340253829956
Validation loss: 2.33377428208628

Epoch: 6| Step: 6
Training loss: 2.318854331970215
Validation loss: 2.3255222176992767

Epoch: 6| Step: 7
Training loss: 2.8471407890319824
Validation loss: 2.323088756171606

Epoch: 6| Step: 8
Training loss: 2.481513261795044
Validation loss: 2.3199310277097966

Epoch: 6| Step: 9
Training loss: 2.7196826934814453
Validation loss: 2.316207742178312

Epoch: 6| Step: 10
Training loss: 2.4715662002563477
Validation loss: 2.311382491101501

Epoch: 6| Step: 11
Training loss: 2.0270397663116455
Validation loss: 2.320475898763185

Epoch: 6| Step: 12
Training loss: 2.5892844200134277
Validation loss: 2.319366652478454

Epoch: 6| Step: 13
Training loss: 3.8638696670532227
Validation loss: 2.3204682975687008

Epoch: 151| Step: 0
Training loss: 2.7728755474090576
Validation loss: 2.3273443252809587

Epoch: 6| Step: 1
Training loss: 2.8935985565185547
Validation loss: 2.332248439070999

Epoch: 6| Step: 2
Training loss: 2.572822093963623
Validation loss: 2.3434608982455347

Epoch: 6| Step: 3
Training loss: 2.635148525238037
Validation loss: 2.340116234235866

Epoch: 6| Step: 4
Training loss: 1.990946888923645
Validation loss: 2.347453442952966

Epoch: 6| Step: 5
Training loss: 1.996065378189087
Validation loss: 2.3389194908962456

Epoch: 6| Step: 6
Training loss: 3.0196237564086914
Validation loss: 2.335296336040702

Epoch: 6| Step: 7
Training loss: 2.466757297515869
Validation loss: 2.3335005749938307

Epoch: 6| Step: 8
Training loss: 2.340498208999634
Validation loss: 2.3260952862360145

Epoch: 6| Step: 9
Training loss: 2.449181079864502
Validation loss: 2.335248093451223

Epoch: 6| Step: 10
Training loss: 3.427452802658081
Validation loss: 2.3364431422243834

Epoch: 6| Step: 11
Training loss: 2.4929111003875732
Validation loss: 2.331712304904897

Epoch: 6| Step: 12
Training loss: 1.8244247436523438
Validation loss: 2.3316232491565008

Epoch: 6| Step: 13
Training loss: 3.860515594482422
Validation loss: 2.3305191480985252

Epoch: 152| Step: 0
Training loss: 2.1038260459899902
Validation loss: 2.3351911908836773

Epoch: 6| Step: 1
Training loss: 1.96951162815094
Validation loss: 2.3310584765608593

Epoch: 6| Step: 2
Training loss: 3.6311757564544678
Validation loss: 2.34416179503164

Epoch: 6| Step: 3
Training loss: 2.6066513061523438
Validation loss: 2.331152226335259

Epoch: 6| Step: 4
Training loss: 3.0016260147094727
Validation loss: 2.3389460271404636

Epoch: 6| Step: 5
Training loss: 2.6515161991119385
Validation loss: 2.341632845581219

Epoch: 6| Step: 6
Training loss: 2.6734840869903564
Validation loss: 2.3391125227815364

Epoch: 6| Step: 7
Training loss: 2.1517419815063477
Validation loss: 2.337698616007323

Epoch: 6| Step: 8
Training loss: 2.553196907043457
Validation loss: 2.3311186323883715

Epoch: 6| Step: 9
Training loss: 2.038179636001587
Validation loss: 2.3297739259658323

Epoch: 6| Step: 10
Training loss: 2.3010876178741455
Validation loss: 2.340733371755128

Epoch: 6| Step: 11
Training loss: 3.1910958290100098
Validation loss: 2.343513460569484

Epoch: 6| Step: 12
Training loss: 2.587635040283203
Validation loss: 2.349851841567665

Epoch: 6| Step: 13
Training loss: 2.8904430866241455
Validation loss: 2.348741715954196

Epoch: 153| Step: 0
Training loss: 3.5207865238189697
Validation loss: 2.3353917560269757

Epoch: 6| Step: 1
Training loss: 2.431622266769409
Validation loss: 2.3210860298525904

Epoch: 6| Step: 2
Training loss: 2.514930486679077
Validation loss: 2.3281087619002148

Epoch: 6| Step: 3
Training loss: 2.10086727142334
Validation loss: 2.318340088731499

Epoch: 6| Step: 4
Training loss: 3.1993539333343506
Validation loss: 2.3141844887887277

Epoch: 6| Step: 5
Training loss: 2.4668374061584473
Validation loss: 2.3213461829769995

Epoch: 6| Step: 6
Training loss: 2.1767334938049316
Validation loss: 2.3200972849322903

Epoch: 6| Step: 7
Training loss: 2.301529884338379
Validation loss: 2.3240924214804046

Epoch: 6| Step: 8
Training loss: 2.4117214679718018
Validation loss: 2.3258367456415647

Epoch: 6| Step: 9
Training loss: 3.2859809398651123
Validation loss: 2.3246371233335106

Epoch: 6| Step: 10
Training loss: 2.0077829360961914
Validation loss: 2.3348086931372203

Epoch: 6| Step: 11
Training loss: 2.5636556148529053
Validation loss: 2.3378544007578204

Epoch: 6| Step: 12
Training loss: 2.643324375152588
Validation loss: 2.3384056937310005

Epoch: 6| Step: 13
Training loss: 2.4308724403381348
Validation loss: 2.341220325039279

Epoch: 154| Step: 0
Training loss: 2.3406996726989746
Validation loss: 2.329228160201862

Epoch: 6| Step: 1
Training loss: 2.3590054512023926
Validation loss: 2.321309376788396

Epoch: 6| Step: 2
Training loss: 3.277998924255371
Validation loss: 2.3274477374169136

Epoch: 6| Step: 3
Training loss: 2.4865264892578125
Validation loss: 2.3254324287496586

Epoch: 6| Step: 4
Training loss: 2.5462138652801514
Validation loss: 2.317687237134544

Epoch: 6| Step: 5
Training loss: 2.1861343383789062
Validation loss: 2.3188749000590336

Epoch: 6| Step: 6
Training loss: 3.2102103233337402
Validation loss: 2.3177898289054952

Epoch: 6| Step: 7
Training loss: 2.21621036529541
Validation loss: 2.3125542286903626

Epoch: 6| Step: 8
Training loss: 3.045743227005005
Validation loss: 2.3088167111078897

Epoch: 6| Step: 9
Training loss: 2.605556011199951
Validation loss: 2.3134917982162966

Epoch: 6| Step: 10
Training loss: 1.558953046798706
Validation loss: 2.314175272500643

Epoch: 6| Step: 11
Training loss: 2.7398929595947266
Validation loss: 2.308433009732154

Epoch: 6| Step: 12
Training loss: 2.6119513511657715
Validation loss: 2.314132800666235

Epoch: 6| Step: 13
Training loss: 3.1993234157562256
Validation loss: 2.31483196186763

Epoch: 155| Step: 0
Training loss: 2.994659423828125
Validation loss: 2.3158577693405973

Epoch: 6| Step: 1
Training loss: 2.3177642822265625
Validation loss: 2.328176657358805

Epoch: 6| Step: 2
Training loss: 2.5905511379241943
Validation loss: 2.3363815738308813

Epoch: 6| Step: 3
Training loss: 3.0546226501464844
Validation loss: 2.33718861943932

Epoch: 6| Step: 4
Training loss: 2.5821731090545654
Validation loss: 2.341411477775984

Epoch: 6| Step: 5
Training loss: 2.0670223236083984
Validation loss: 2.332472719171996

Epoch: 6| Step: 6
Training loss: 2.842592239379883
Validation loss: 2.3358988966993106

Epoch: 6| Step: 7
Training loss: 2.250460624694824
Validation loss: 2.3395925106540805

Epoch: 6| Step: 8
Training loss: 2.7292840480804443
Validation loss: 2.3510300882401003

Epoch: 6| Step: 9
Training loss: 2.4628286361694336
Validation loss: 2.350800501402988

Epoch: 6| Step: 10
Training loss: 2.246188163757324
Validation loss: 2.3485498197617067

Epoch: 6| Step: 11
Training loss: 2.7080440521240234
Validation loss: 2.3457726022248626

Epoch: 6| Step: 12
Training loss: 2.565819263458252
Validation loss: 2.3283306142335296

Epoch: 6| Step: 13
Training loss: 2.541297674179077
Validation loss: 2.3287542122666554

Epoch: 156| Step: 0
Training loss: 1.9683191776275635
Validation loss: 2.3224173848347

Epoch: 6| Step: 1
Training loss: 2.1845316886901855
Validation loss: 2.326133205044654

Epoch: 6| Step: 2
Training loss: 1.940604567527771
Validation loss: 2.3223888797144734

Epoch: 6| Step: 3
Training loss: 2.5235700607299805
Validation loss: 2.3140979172081075

Epoch: 6| Step: 4
Training loss: 2.5333123207092285
Validation loss: 2.3122828852745796

Epoch: 6| Step: 5
Training loss: 2.0050437450408936
Validation loss: 2.316653302920762

Epoch: 6| Step: 6
Training loss: 2.786900520324707
Validation loss: 2.3162615824771184

Epoch: 6| Step: 7
Training loss: 2.1995558738708496
Validation loss: 2.3243494482450586

Epoch: 6| Step: 8
Training loss: 2.7954468727111816
Validation loss: 2.3240921779345443

Epoch: 6| Step: 9
Training loss: 3.063687801361084
Validation loss: 2.328754163557483

Epoch: 6| Step: 10
Training loss: 3.50425386428833
Validation loss: 2.3306195171930457

Epoch: 6| Step: 11
Training loss: 2.413393974304199
Validation loss: 2.3359930515289307

Epoch: 6| Step: 12
Training loss: 3.1708292961120605
Validation loss: 2.3408666605590494

Epoch: 6| Step: 13
Training loss: 3.2147982120513916
Validation loss: 2.327394293200585

Epoch: 157| Step: 0
Training loss: 1.9664978981018066
Validation loss: 2.3292213768087406

Epoch: 6| Step: 1
Training loss: 2.874983549118042
Validation loss: 2.318137243229856

Epoch: 6| Step: 2
Training loss: 3.161581039428711
Validation loss: 2.311237157032054

Epoch: 6| Step: 3
Training loss: 2.620173931121826
Validation loss: 2.312007542579405

Epoch: 6| Step: 4
Training loss: 2.4141829013824463
Validation loss: 2.315355552140103

Epoch: 6| Step: 5
Training loss: 2.207056760787964
Validation loss: 2.3074186899328746

Epoch: 6| Step: 6
Training loss: 2.9392433166503906
Validation loss: 2.307150504922354

Epoch: 6| Step: 7
Training loss: 3.1658151149749756
Validation loss: 2.306741419658866

Epoch: 6| Step: 8
Training loss: 2.0932469367980957
Validation loss: 2.3069512536448817

Epoch: 6| Step: 9
Training loss: 2.6258201599121094
Validation loss: 2.305137609922758

Epoch: 6| Step: 10
Training loss: 2.1091561317443848
Validation loss: 2.300300649417344

Epoch: 6| Step: 11
Training loss: 2.6770005226135254
Validation loss: 2.3206725094908025

Epoch: 6| Step: 12
Training loss: 2.309356689453125
Validation loss: 2.3299154338016304

Epoch: 6| Step: 13
Training loss: 3.0512096881866455
Validation loss: 2.324825886757143

Epoch: 158| Step: 0
Training loss: 2.86106276512146
Validation loss: 2.332303503508209

Epoch: 6| Step: 1
Training loss: 2.1355161666870117
Validation loss: 2.347403818561185

Epoch: 6| Step: 2
Training loss: 2.760021686553955
Validation loss: 2.3456739610241306

Epoch: 6| Step: 3
Training loss: 1.9649631977081299
Validation loss: 2.3375672012247066

Epoch: 6| Step: 4
Training loss: 1.801365613937378
Validation loss: 2.3220992831773657

Epoch: 6| Step: 5
Training loss: 2.791785717010498
Validation loss: 2.3211001298760854

Epoch: 6| Step: 6
Training loss: 2.8482446670532227
Validation loss: 2.323950723935199

Epoch: 6| Step: 7
Training loss: 2.653923511505127
Validation loss: 2.323179551350173

Epoch: 6| Step: 8
Training loss: 2.244553565979004
Validation loss: 2.33138382563027

Epoch: 6| Step: 9
Training loss: 2.633833885192871
Validation loss: 2.3263339124700075

Epoch: 6| Step: 10
Training loss: 2.911858558654785
Validation loss: 2.3399158267564673

Epoch: 6| Step: 11
Training loss: 2.904067039489746
Validation loss: 2.3413865322707803

Epoch: 6| Step: 12
Training loss: 2.7213637828826904
Validation loss: 2.3574385745550996

Epoch: 6| Step: 13
Training loss: 2.81597638130188
Validation loss: 2.338404206819432

Epoch: 159| Step: 0
Training loss: 3.423877716064453
Validation loss: 2.3234982311084704

Epoch: 6| Step: 1
Training loss: 1.9940801858901978
Validation loss: 2.3098513823683544

Epoch: 6| Step: 2
Training loss: 1.8811678886413574
Validation loss: 2.3005868593851724

Epoch: 6| Step: 3
Training loss: 2.2888293266296387
Validation loss: 2.3000080893116612

Epoch: 6| Step: 4
Training loss: 2.560892343521118
Validation loss: 2.296830815653647

Epoch: 6| Step: 5
Training loss: 2.146292209625244
Validation loss: 2.302800747656053

Epoch: 6| Step: 6
Training loss: 2.83280873298645
Validation loss: 2.297918309447586

Epoch: 6| Step: 7
Training loss: 2.587845802307129
Validation loss: 2.2988524565132717

Epoch: 6| Step: 8
Training loss: 2.7383248805999756
Validation loss: 2.298172864862668

Epoch: 6| Step: 9
Training loss: 2.592604637145996
Validation loss: 2.2954649117685135

Epoch: 6| Step: 10
Training loss: 2.434342861175537
Validation loss: 2.2973088141410583

Epoch: 6| Step: 11
Training loss: 3.2338497638702393
Validation loss: 2.297580652339484

Epoch: 6| Step: 12
Training loss: 2.5735535621643066
Validation loss: 2.292941918937109

Epoch: 6| Step: 13
Training loss: 2.884953498840332
Validation loss: 2.297419291670604

Epoch: 160| Step: 0
Training loss: 2.9249935150146484
Validation loss: 2.3072383121777604

Epoch: 6| Step: 1
Training loss: 2.4699344635009766
Validation loss: 2.3003483997878207

Epoch: 6| Step: 2
Training loss: 2.6198973655700684
Validation loss: 2.3056255104721233

Epoch: 6| Step: 3
Training loss: 2.2850780487060547
Validation loss: 2.302557791433027

Epoch: 6| Step: 4
Training loss: 2.5817933082580566
Validation loss: 2.3038723186780046

Epoch: 6| Step: 5
Training loss: 2.712127447128296
Validation loss: 2.306476713508688

Epoch: 6| Step: 6
Training loss: 2.7155232429504395
Validation loss: 2.3047392111952587

Epoch: 6| Step: 7
Training loss: 2.7545366287231445
Validation loss: 2.306776115971227

Epoch: 6| Step: 8
Training loss: 2.0782716274261475
Validation loss: 2.3235385430756437

Epoch: 6| Step: 9
Training loss: 2.9227182865142822
Validation loss: 2.318259726288498

Epoch: 6| Step: 10
Training loss: 2.231477975845337
Validation loss: 2.3180557450940533

Epoch: 6| Step: 11
Training loss: 2.2031242847442627
Validation loss: 2.344172159830729

Epoch: 6| Step: 12
Training loss: 2.373520851135254
Validation loss: 2.335007385541034

Epoch: 6| Step: 13
Training loss: 3.3545026779174805
Validation loss: 2.3383784973493187

Epoch: 161| Step: 0
Training loss: 2.7971208095550537
Validation loss: 2.340176247781323

Epoch: 6| Step: 1
Training loss: 2.48922061920166
Validation loss: 2.3365153702356483

Epoch: 6| Step: 2
Training loss: 2.252650737762451
Validation loss: 2.3314437814938125

Epoch: 6| Step: 3
Training loss: 3.3441247940063477
Validation loss: 2.3420152715457383

Epoch: 6| Step: 4
Training loss: 3.253910541534424
Validation loss: 2.336977461332916

Epoch: 6| Step: 5
Training loss: 2.35943341255188
Validation loss: 2.318201070190758

Epoch: 6| Step: 6
Training loss: 1.857855200767517
Validation loss: 2.3051411387740925

Epoch: 6| Step: 7
Training loss: 1.8136597871780396
Validation loss: 2.305722741670506

Epoch: 6| Step: 8
Training loss: 1.9005117416381836
Validation loss: 2.3164325221892326

Epoch: 6| Step: 9
Training loss: 2.9738659858703613
Validation loss: 2.3147120321950605

Epoch: 6| Step: 10
Training loss: 1.9427568912506104
Validation loss: 2.319052783391809

Epoch: 6| Step: 11
Training loss: 3.0708322525024414
Validation loss: 2.3203926958063597

Epoch: 6| Step: 12
Training loss: 3.2969164848327637
Validation loss: 2.329446195274271

Epoch: 6| Step: 13
Training loss: 2.718813180923462
Validation loss: 2.351583206525413

Epoch: 162| Step: 0
Training loss: 3.134334087371826
Validation loss: 2.333503894908454

Epoch: 6| Step: 1
Training loss: 2.6347641944885254
Validation loss: 2.3320202058361423

Epoch: 6| Step: 2
Training loss: 2.0774316787719727
Validation loss: 2.3138471777721117

Epoch: 6| Step: 3
Training loss: 2.397979974746704
Validation loss: 2.3203069317725395

Epoch: 6| Step: 4
Training loss: 2.3291869163513184
Validation loss: 2.308908361260609

Epoch: 6| Step: 5
Training loss: 2.3437211513519287
Validation loss: 2.300340170501381

Epoch: 6| Step: 6
Training loss: 3.4806230068206787
Validation loss: 2.2940751301345004

Epoch: 6| Step: 7
Training loss: 2.7150216102600098
Validation loss: 2.2957851348384732

Epoch: 6| Step: 8
Training loss: 2.597227096557617
Validation loss: 2.2992918337545087

Epoch: 6| Step: 9
Training loss: 2.734898805618286
Validation loss: 2.2917660615777455

Epoch: 6| Step: 10
Training loss: 2.1361887454986572
Validation loss: 2.294311385000906

Epoch: 6| Step: 11
Training loss: 1.6213057041168213
Validation loss: 2.2895256908991004

Epoch: 6| Step: 12
Training loss: 2.3276913166046143
Validation loss: 2.2996222742142214

Epoch: 6| Step: 13
Training loss: 3.87749981880188
Validation loss: 2.3082870232161654

Epoch: 163| Step: 0
Training loss: 2.8271098136901855
Validation loss: 2.3134481445435555

Epoch: 6| Step: 1
Training loss: 2.2401437759399414
Validation loss: 2.3178076077533025

Epoch: 6| Step: 2
Training loss: 2.5197412967681885
Validation loss: 2.317451682142032

Epoch: 6| Step: 3
Training loss: 1.8269245624542236
Validation loss: 2.3417007256579656

Epoch: 6| Step: 4
Training loss: 3.3664937019348145
Validation loss: 2.339433741825883

Epoch: 6| Step: 5
Training loss: 2.5807275772094727
Validation loss: 2.3469980762850855

Epoch: 6| Step: 6
Training loss: 2.8959555625915527
Validation loss: 2.370328426361084

Epoch: 6| Step: 7
Training loss: 2.4424071311950684
Validation loss: 2.383999820678465

Epoch: 6| Step: 8
Training loss: 1.8058100938796997
Validation loss: 2.3717637959346978

Epoch: 6| Step: 9
Training loss: 2.6951582431793213
Validation loss: 2.3603693541660102

Epoch: 6| Step: 10
Training loss: 2.496156692504883
Validation loss: 2.361277567443027

Epoch: 6| Step: 11
Training loss: 2.4063148498535156
Validation loss: 2.377717148873114

Epoch: 6| Step: 12
Training loss: 3.44600248336792
Validation loss: 2.36093750820365

Epoch: 6| Step: 13
Training loss: 2.34627628326416
Validation loss: 2.343637325430429

Epoch: 164| Step: 0
Training loss: 2.0686166286468506
Validation loss: 2.343658842066283

Epoch: 6| Step: 1
Training loss: 2.5282387733459473
Validation loss: 2.3295262526440363

Epoch: 6| Step: 2
Training loss: 3.120298385620117
Validation loss: 2.3172575696822135

Epoch: 6| Step: 3
Training loss: 2.5325422286987305
Validation loss: 2.3163024148633404

Epoch: 6| Step: 4
Training loss: 3.359718084335327
Validation loss: 2.3170500660455353

Epoch: 6| Step: 5
Training loss: 2.54840087890625
Validation loss: 2.315739805980395

Epoch: 6| Step: 6
Training loss: 1.9123079776763916
Validation loss: 2.308876883599066

Epoch: 6| Step: 7
Training loss: 2.4548802375793457
Validation loss: 2.314026581343784

Epoch: 6| Step: 8
Training loss: 3.113320827484131
Validation loss: 2.3154193380827546

Epoch: 6| Step: 9
Training loss: 2.672640323638916
Validation loss: 2.327783692267633

Epoch: 6| Step: 10
Training loss: 2.2094085216522217
Validation loss: 2.3395155296530774

Epoch: 6| Step: 11
Training loss: 1.9101612567901611
Validation loss: 2.3609825949515066

Epoch: 6| Step: 12
Training loss: 2.9981324672698975
Validation loss: 2.358957664940947

Epoch: 6| Step: 13
Training loss: 2.609893798828125
Validation loss: 2.3755125358540523

Epoch: 165| Step: 0
Training loss: 3.3125925064086914
Validation loss: 2.3848750155459166

Epoch: 6| Step: 1
Training loss: 2.595327138900757
Validation loss: 2.355189464425528

Epoch: 6| Step: 2
Training loss: 2.9884231090545654
Validation loss: 2.352762299199258

Epoch: 6| Step: 3
Training loss: 2.733560085296631
Validation loss: 2.3295770332377446

Epoch: 6| Step: 4
Training loss: 2.2084133625030518
Validation loss: 2.332450894899266

Epoch: 6| Step: 5
Training loss: 2.206312656402588
Validation loss: 2.323997761613579

Epoch: 6| Step: 6
Training loss: 2.1263926029205322
Validation loss: 2.3230388395247923

Epoch: 6| Step: 7
Training loss: 2.889652729034424
Validation loss: 2.3152656093720467

Epoch: 6| Step: 8
Training loss: 2.6449759006500244
Validation loss: 2.3157638298567904

Epoch: 6| Step: 9
Training loss: 2.988123655319214
Validation loss: 2.318001731749504

Epoch: 6| Step: 10
Training loss: 1.6679140329360962
Validation loss: 2.3051411310831704

Epoch: 6| Step: 11
Training loss: 2.8496756553649902
Validation loss: 2.3205804145464333

Epoch: 6| Step: 12
Training loss: 1.9972171783447266
Validation loss: 2.3160744482471096

Epoch: 6| Step: 13
Training loss: 2.8787386417388916
Validation loss: 2.326733548154113

Epoch: 166| Step: 0
Training loss: 2.8061652183532715
Validation loss: 2.329267842795259

Epoch: 6| Step: 1
Training loss: 2.722848892211914
Validation loss: 2.323187494790682

Epoch: 6| Step: 2
Training loss: 2.6528005599975586
Validation loss: 2.328475083074262

Epoch: 6| Step: 3
Training loss: 2.509519338607788
Validation loss: 2.33776641661121

Epoch: 6| Step: 4
Training loss: 2.548271656036377
Validation loss: 2.341032205089446

Epoch: 6| Step: 5
Training loss: 2.841905117034912
Validation loss: 2.3404221509092595

Epoch: 6| Step: 6
Training loss: 1.8260897397994995
Validation loss: 2.328705354403424

Epoch: 6| Step: 7
Training loss: 2.763094425201416
Validation loss: 2.3420612248041297

Epoch: 6| Step: 8
Training loss: 2.936042547225952
Validation loss: 2.326316273340615

Epoch: 6| Step: 9
Training loss: 2.9224672317504883
Validation loss: 2.3390271714938584

Epoch: 6| Step: 10
Training loss: 2.5931472778320312
Validation loss: 2.3115262267410115

Epoch: 6| Step: 11
Training loss: 2.2166671752929688
Validation loss: 2.3211370924467682

Epoch: 6| Step: 12
Training loss: 2.1348938941955566
Validation loss: 2.320141956370364

Epoch: 6| Step: 13
Training loss: 2.334167003631592
Validation loss: 2.310229368107293

Epoch: 167| Step: 0
Training loss: 2.8265738487243652
Validation loss: 2.3091482885422243

Epoch: 6| Step: 1
Training loss: 2.4099297523498535
Validation loss: 2.3279048191603793

Epoch: 6| Step: 2
Training loss: 2.9972798824310303
Validation loss: 2.33715303995276

Epoch: 6| Step: 3
Training loss: 2.290278434753418
Validation loss: 2.3588917588674896

Epoch: 6| Step: 4
Training loss: 2.257190704345703
Validation loss: 2.360885812390235

Epoch: 6| Step: 5
Training loss: 2.0228123664855957
Validation loss: 2.3695268182344336

Epoch: 6| Step: 6
Training loss: 3.3851475715637207
Validation loss: 2.3689326317079606

Epoch: 6| Step: 7
Training loss: 2.8220672607421875
Validation loss: 2.362279253621255

Epoch: 6| Step: 8
Training loss: 1.8201059103012085
Validation loss: 2.3552880594807286

Epoch: 6| Step: 9
Training loss: 2.65021014213562
Validation loss: 2.3561437924702964

Epoch: 6| Step: 10
Training loss: 2.6907615661621094
Validation loss: 2.339686386046871

Epoch: 6| Step: 11
Training loss: 2.4525508880615234
Validation loss: 2.329496006811819

Epoch: 6| Step: 12
Training loss: 3.108644485473633
Validation loss: 2.3095153531720563

Epoch: 6| Step: 13
Training loss: 1.8001024723052979
Validation loss: 2.310245588261594

Epoch: 168| Step: 0
Training loss: 2.5262179374694824
Validation loss: 2.301718011979134

Epoch: 6| Step: 1
Training loss: 3.217933177947998
Validation loss: 2.302854085481295

Epoch: 6| Step: 2
Training loss: 1.9735004901885986
Validation loss: 2.3008170717506

Epoch: 6| Step: 3
Training loss: 3.099370002746582
Validation loss: 2.3022182397944952

Epoch: 6| Step: 4
Training loss: 2.143592119216919
Validation loss: 2.3030473442487818

Epoch: 6| Step: 5
Training loss: 2.994954824447632
Validation loss: 2.3065552737123225

Epoch: 6| Step: 6
Training loss: 2.518094539642334
Validation loss: 2.3158604098904516

Epoch: 6| Step: 7
Training loss: 2.6325716972351074
Validation loss: 2.3104334236473165

Epoch: 6| Step: 8
Training loss: 2.086003303527832
Validation loss: 2.3071587649724816

Epoch: 6| Step: 9
Training loss: 2.7509732246398926
Validation loss: 2.28880508740743

Epoch: 6| Step: 10
Training loss: 2.0894651412963867
Validation loss: 2.2937307947425434

Epoch: 6| Step: 11
Training loss: 2.5804505348205566
Validation loss: 2.2849064962838286

Epoch: 6| Step: 12
Training loss: 2.4607250690460205
Validation loss: 2.296454378353652

Epoch: 6| Step: 13
Training loss: 2.8464810848236084
Validation loss: 2.293759251153597

Epoch: 169| Step: 0
Training loss: 2.5727717876434326
Validation loss: 2.2993243381541264

Epoch: 6| Step: 1
Training loss: 2.604914903640747
Validation loss: 2.2981430612584597

Epoch: 6| Step: 2
Training loss: 3.09077787399292
Validation loss: 2.2972229065433627

Epoch: 6| Step: 3
Training loss: 3.044325828552246
Validation loss: 2.297251752627793

Epoch: 6| Step: 4
Training loss: 1.7029976844787598
Validation loss: 2.29027388429129

Epoch: 6| Step: 5
Training loss: 2.78497576713562
Validation loss: 2.286662990047086

Epoch: 6| Step: 6
Training loss: 2.5300395488739014
Validation loss: 2.2858713955007572

Epoch: 6| Step: 7
Training loss: 2.3957748413085938
Validation loss: 2.2854056255791777

Epoch: 6| Step: 8
Training loss: 2.982977867126465
Validation loss: 2.285271108791392

Epoch: 6| Step: 9
Training loss: 3.023836374282837
Validation loss: 2.291280774660008

Epoch: 6| Step: 10
Training loss: 2.0817370414733887
Validation loss: 2.2890897950818463

Epoch: 6| Step: 11
Training loss: 2.066269636154175
Validation loss: 2.302443483824371

Epoch: 6| Step: 12
Training loss: 2.2054314613342285
Validation loss: 2.2973079476305234

Epoch: 6| Step: 13
Training loss: 2.8617658615112305
Validation loss: 2.3061737245129

Epoch: 170| Step: 0
Training loss: 2.5059657096862793
Validation loss: 2.3095206111989994

Epoch: 6| Step: 1
Training loss: 2.6671595573425293
Validation loss: 2.3026926209849696

Epoch: 6| Step: 2
Training loss: 2.3165342807769775
Validation loss: 2.303588241659185

Epoch: 6| Step: 3
Training loss: 2.1767401695251465
Validation loss: 2.296803588508278

Epoch: 6| Step: 4
Training loss: 2.3129682540893555
Validation loss: 2.2932008825322634

Epoch: 6| Step: 5
Training loss: 2.1140944957733154
Validation loss: 2.28829147354249

Epoch: 6| Step: 6
Training loss: 2.1166512966156006
Validation loss: 2.294025557015532

Epoch: 6| Step: 7
Training loss: 3.2421655654907227
Validation loss: 2.302445633437044

Epoch: 6| Step: 8
Training loss: 2.356112241744995
Validation loss: 2.3132646955469602

Epoch: 6| Step: 9
Training loss: 3.0306150913238525
Validation loss: 2.321964185724976

Epoch: 6| Step: 10
Training loss: 2.98945951461792
Validation loss: 2.3341653321378972

Epoch: 6| Step: 11
Training loss: 2.275452136993408
Validation loss: 2.338112541424331

Epoch: 6| Step: 12
Training loss: 2.7497477531433105
Validation loss: 2.3423101607189385

Epoch: 6| Step: 13
Training loss: 3.0717713832855225
Validation loss: 2.3280384643103487

Epoch: 171| Step: 0
Training loss: 2.4434049129486084
Validation loss: 2.3373133469653387

Epoch: 6| Step: 1
Training loss: 2.6486940383911133
Validation loss: 2.325970601010066

Epoch: 6| Step: 2
Training loss: 2.8789725303649902
Validation loss: 2.3352052614253056

Epoch: 6| Step: 3
Training loss: 2.7372400760650635
Validation loss: 2.3470320599053496

Epoch: 6| Step: 4
Training loss: 2.2706503868103027
Validation loss: 2.3353082390241724

Epoch: 6| Step: 5
Training loss: 2.083561897277832
Validation loss: 2.3376448487722747

Epoch: 6| Step: 6
Training loss: 2.5048813819885254
Validation loss: 2.3336427006670224

Epoch: 6| Step: 7
Training loss: 2.325845956802368
Validation loss: 2.3283426915445635

Epoch: 6| Step: 8
Training loss: 2.769075632095337
Validation loss: 2.319873158649732

Epoch: 6| Step: 9
Training loss: 3.1082680225372314
Validation loss: 2.3065705709559943

Epoch: 6| Step: 10
Training loss: 3.218097448348999
Validation loss: 2.288642765373312

Epoch: 6| Step: 11
Training loss: 1.9786745309829712
Validation loss: 2.2844859297557543

Epoch: 6| Step: 12
Training loss: 2.2890615463256836
Validation loss: 2.2805737756913707

Epoch: 6| Step: 13
Training loss: 2.3425517082214355
Validation loss: 2.276158253351847

Epoch: 172| Step: 0
Training loss: 2.3532967567443848
Validation loss: 2.2738048466303016

Epoch: 6| Step: 1
Training loss: 2.483574390411377
Validation loss: 2.277311348146008

Epoch: 6| Step: 2
Training loss: 3.243932008743286
Validation loss: 2.276048502614421

Epoch: 6| Step: 3
Training loss: 2.8930861949920654
Validation loss: 2.283186633099792

Epoch: 6| Step: 4
Training loss: 3.2205140590667725
Validation loss: 2.27323648493777

Epoch: 6| Step: 5
Training loss: 2.460401773452759
Validation loss: 2.2773522869233163

Epoch: 6| Step: 6
Training loss: 2.577024221420288
Validation loss: 2.287984550640147

Epoch: 6| Step: 7
Training loss: 2.628659725189209
Validation loss: 2.28525117135817

Epoch: 6| Step: 8
Training loss: 2.286759853363037
Validation loss: 2.2797030454040854

Epoch: 6| Step: 9
Training loss: 1.5854830741882324
Validation loss: 2.285071693440919

Epoch: 6| Step: 10
Training loss: 2.233194589614868
Validation loss: 2.2835009328780638

Epoch: 6| Step: 11
Training loss: 2.215884208679199
Validation loss: 2.3013418464250464

Epoch: 6| Step: 12
Training loss: 2.4769914150238037
Validation loss: 2.2950488008478636

Epoch: 6| Step: 13
Training loss: 3.366101026535034
Validation loss: 2.291748836476316

Epoch: 173| Step: 0
Training loss: 3.02418851852417
Validation loss: 2.2903414016128867

Epoch: 6| Step: 1
Training loss: 3.7046310901641846
Validation loss: 2.2961386429366244

Epoch: 6| Step: 2
Training loss: 2.2730703353881836
Validation loss: 2.2823772635511173

Epoch: 6| Step: 3
Training loss: 2.3238351345062256
Validation loss: 2.2775965941849576

Epoch: 6| Step: 4
Training loss: 2.9943063259124756
Validation loss: 2.274626419108401

Epoch: 6| Step: 5
Training loss: 2.3329224586486816
Validation loss: 2.2787426056400424

Epoch: 6| Step: 6
Training loss: 2.74603533744812
Validation loss: 2.278001090531708

Epoch: 6| Step: 7
Training loss: 2.5830507278442383
Validation loss: 2.272649498396022

Epoch: 6| Step: 8
Training loss: 2.6109676361083984
Validation loss: 2.2820003596685265

Epoch: 6| Step: 9
Training loss: 2.4188954830169678
Validation loss: 2.282434857019814

Epoch: 6| Step: 10
Training loss: 1.656886339187622
Validation loss: 2.276134588385141

Epoch: 6| Step: 11
Training loss: 2.3169960975646973
Validation loss: 2.2822598821373394

Epoch: 6| Step: 12
Training loss: 2.1615755558013916
Validation loss: 2.2789320099738335

Epoch: 6| Step: 13
Training loss: 2.296130895614624
Validation loss: 2.2861778172113563

Epoch: 174| Step: 0
Training loss: 2.4716317653656006
Validation loss: 2.2890151213574153

Epoch: 6| Step: 1
Training loss: 1.926527500152588
Validation loss: 2.2846393944114767

Epoch: 6| Step: 2
Training loss: 3.0458896160125732
Validation loss: 2.291406854506462

Epoch: 6| Step: 3
Training loss: 3.0241763591766357
Validation loss: 2.291110493803537

Epoch: 6| Step: 4
Training loss: 3.0123350620269775
Validation loss: 2.2827308049765964

Epoch: 6| Step: 5
Training loss: 2.736172676086426
Validation loss: 2.2895653581106536

Epoch: 6| Step: 6
Training loss: 1.9987852573394775
Validation loss: 2.277505397796631

Epoch: 6| Step: 7
Training loss: 2.191511631011963
Validation loss: 2.2699001681420112

Epoch: 6| Step: 8
Training loss: 2.1487300395965576
Validation loss: 2.2796152689123668

Epoch: 6| Step: 9
Training loss: 2.5051867961883545
Validation loss: 2.285721094377579

Epoch: 6| Step: 10
Training loss: 3.1421561241149902
Validation loss: 2.3007259625260548

Epoch: 6| Step: 11
Training loss: 2.440045118331909
Validation loss: 2.3005704008122927

Epoch: 6| Step: 12
Training loss: 2.1763362884521484
Validation loss: 2.3115839637735838

Epoch: 6| Step: 13
Training loss: 2.97575044631958
Validation loss: 2.2985335960183093

Epoch: 175| Step: 0
Training loss: 2.63392972946167
Validation loss: 2.305838497736121

Epoch: 6| Step: 1
Training loss: 2.7698159217834473
Validation loss: 2.306195484694614

Epoch: 6| Step: 2
Training loss: 2.9867238998413086
Validation loss: 2.290146291896861

Epoch: 6| Step: 3
Training loss: 2.813715934753418
Validation loss: 2.2826676035440094

Epoch: 6| Step: 4
Training loss: 2.904933452606201
Validation loss: 2.280214571183728

Epoch: 6| Step: 5
Training loss: 2.8844738006591797
Validation loss: 2.2780975167469313

Epoch: 6| Step: 6
Training loss: 2.4957692623138428
Validation loss: 2.2780501150315806

Epoch: 6| Step: 7
Training loss: 2.1038498878479004
Validation loss: 2.291503949831891

Epoch: 6| Step: 8
Training loss: 2.7340877056121826
Validation loss: 2.2918366860317927

Epoch: 6| Step: 9
Training loss: 1.8657066822052002
Validation loss: 2.294218594028104

Epoch: 6| Step: 10
Training loss: 2.3930904865264893
Validation loss: 2.3128374597077728

Epoch: 6| Step: 11
Training loss: 1.9985225200653076
Validation loss: 2.301272681964341

Epoch: 6| Step: 12
Training loss: 2.62937068939209
Validation loss: 2.304975102024694

Epoch: 6| Step: 13
Training loss: 2.323436975479126
Validation loss: 2.307614859714303

Epoch: 176| Step: 0
Training loss: 2.7940833568573
Validation loss: 2.288388172785441

Epoch: 6| Step: 1
Training loss: 2.038372039794922
Validation loss: 2.2773074744850077

Epoch: 6| Step: 2
Training loss: 1.7678422927856445
Validation loss: 2.271618555950862

Epoch: 6| Step: 3
Training loss: 3.280975580215454
Validation loss: 2.275635496262581

Epoch: 6| Step: 4
Training loss: 2.616142749786377
Validation loss: 2.268338732821967

Epoch: 6| Step: 5
Training loss: 4.017081260681152
Validation loss: 2.274991868644632

Epoch: 6| Step: 6
Training loss: 2.4488844871520996
Validation loss: 2.280990619813242

Epoch: 6| Step: 7
Training loss: 1.951237440109253
Validation loss: 2.27139473730518

Epoch: 6| Step: 8
Training loss: 1.8167715072631836
Validation loss: 2.2739380392976987

Epoch: 6| Step: 9
Training loss: 2.394886016845703
Validation loss: 2.263886615794192

Epoch: 6| Step: 10
Training loss: 2.842649221420288
Validation loss: 2.2773030727140364

Epoch: 6| Step: 11
Training loss: 3.253718376159668
Validation loss: 2.2741409911904285

Epoch: 6| Step: 12
Training loss: 2.1622610092163086
Validation loss: 2.271956674514278

Epoch: 6| Step: 13
Training loss: 1.9119834899902344
Validation loss: 2.287392303507815

Epoch: 177| Step: 0
Training loss: 2.24631929397583
Validation loss: 2.281227766826589

Epoch: 6| Step: 1
Training loss: 2.9461703300476074
Validation loss: 2.2842104742603917

Epoch: 6| Step: 2
Training loss: 2.7527029514312744
Validation loss: 2.2718518549396145

Epoch: 6| Step: 3
Training loss: 2.325314521789551
Validation loss: 2.2721708436166086

Epoch: 6| Step: 4
Training loss: 2.9360508918762207
Validation loss: 2.2681095856492237

Epoch: 6| Step: 5
Training loss: 2.9502410888671875
Validation loss: 2.2742519968299457

Epoch: 6| Step: 6
Training loss: 2.481593608856201
Validation loss: 2.2831571102142334

Epoch: 6| Step: 7
Training loss: 1.8452867269515991
Validation loss: 2.279777116672967

Epoch: 6| Step: 8
Training loss: 2.036263942718506
Validation loss: 2.2843972816262195

Epoch: 6| Step: 9
Training loss: 2.921591281890869
Validation loss: 2.299358606338501

Epoch: 6| Step: 10
Training loss: 1.9045090675354004
Validation loss: 2.2962807557916127

Epoch: 6| Step: 11
Training loss: 2.447768211364746
Validation loss: 2.307491120471749

Epoch: 6| Step: 12
Training loss: 3.436573028564453
Validation loss: 2.3046569644763903

Epoch: 6| Step: 13
Training loss: 2.336676836013794
Validation loss: 2.304296980621994

Epoch: 178| Step: 0
Training loss: 2.7986819744110107
Validation loss: 2.302673967935706

Epoch: 6| Step: 1
Training loss: 2.74031925201416
Validation loss: 2.2827985081621396

Epoch: 6| Step: 2
Training loss: 2.5225892066955566
Validation loss: 2.279348396485852

Epoch: 6| Step: 3
Training loss: 2.7439045906066895
Validation loss: 2.2701919309554563

Epoch: 6| Step: 4
Training loss: 2.97904634475708
Validation loss: 2.2737028534694383

Epoch: 6| Step: 5
Training loss: 2.7170934677124023
Validation loss: 2.2772215079235774

Epoch: 6| Step: 6
Training loss: 2.4901669025421143
Validation loss: 2.275569882444156

Epoch: 6| Step: 7
Training loss: 2.171741485595703
Validation loss: 2.267501997691329

Epoch: 6| Step: 8
Training loss: 2.4251527786254883
Validation loss: 2.2823401381892543

Epoch: 6| Step: 9
Training loss: 2.207376480102539
Validation loss: 2.2874868069925616

Epoch: 6| Step: 10
Training loss: 1.932121753692627
Validation loss: 2.2780879133491108

Epoch: 6| Step: 11
Training loss: 2.702622890472412
Validation loss: 2.2841974112295333

Epoch: 6| Step: 12
Training loss: 2.4929213523864746
Validation loss: 2.273411273956299

Epoch: 6| Step: 13
Training loss: 2.5483157634735107
Validation loss: 2.271381502510399

Epoch: 179| Step: 0
Training loss: 2.580091953277588
Validation loss: 2.268179134656024

Epoch: 6| Step: 1
Training loss: 2.4097399711608887
Validation loss: 2.268566926320394

Epoch: 6| Step: 2
Training loss: 1.7197983264923096
Validation loss: 2.260499950378172

Epoch: 6| Step: 3
Training loss: 2.5275394916534424
Validation loss: 2.2651409820843766

Epoch: 6| Step: 4
Training loss: 1.9409273862838745
Validation loss: 2.2677035818817797

Epoch: 6| Step: 5
Training loss: 3.433255910873413
Validation loss: 2.2586142580996276

Epoch: 6| Step: 6
Training loss: 1.3318589925765991
Validation loss: 2.2668642382467947

Epoch: 6| Step: 7
Training loss: 2.457289934158325
Validation loss: 2.264932249182014

Epoch: 6| Step: 8
Training loss: 2.580522060394287
Validation loss: 2.2620895601088002

Epoch: 6| Step: 9
Training loss: 3.9931387901306152
Validation loss: 2.2775528738575597

Epoch: 6| Step: 10
Training loss: 2.4777801036834717
Validation loss: 2.28633988031777

Epoch: 6| Step: 11
Training loss: 2.554865837097168
Validation loss: 2.313134454911755

Epoch: 6| Step: 12
Training loss: 2.892703056335449
Validation loss: 2.3185627614298174

Epoch: 6| Step: 13
Training loss: 2.9920706748962402
Validation loss: 2.3185691115676716

Epoch: 180| Step: 0
Training loss: 2.3634324073791504
Validation loss: 2.304466780795846

Epoch: 6| Step: 1
Training loss: 2.6583683490753174
Validation loss: 2.2966321322225753

Epoch: 6| Step: 2
Training loss: 2.0904479026794434
Validation loss: 2.2818298442389375

Epoch: 6| Step: 3
Training loss: 3.4020590782165527
Validation loss: 2.2807578527799217

Epoch: 6| Step: 4
Training loss: 2.533552646636963
Validation loss: 2.276330655620944

Epoch: 6| Step: 5
Training loss: 2.846424102783203
Validation loss: 2.2717486043130197

Epoch: 6| Step: 6
Training loss: 2.292495012283325
Validation loss: 2.2650640395379837

Epoch: 6| Step: 7
Training loss: 2.955782413482666
Validation loss: 2.262401701301657

Epoch: 6| Step: 8
Training loss: 1.9937691688537598
Validation loss: 2.278726234230944

Epoch: 6| Step: 9
Training loss: 2.3830301761627197
Validation loss: 2.280375970307217

Epoch: 6| Step: 10
Training loss: 2.3325934410095215
Validation loss: 2.282794616555655

Epoch: 6| Step: 11
Training loss: 2.510361909866333
Validation loss: 2.2740501203844623

Epoch: 6| Step: 12
Training loss: 2.3846187591552734
Validation loss: 2.28227073530997

Epoch: 6| Step: 13
Training loss: 2.7671291828155518
Validation loss: 2.2836223981713735

Epoch: 181| Step: 0
Training loss: 3.4019579887390137
Validation loss: 2.30573280139636

Epoch: 6| Step: 1
Training loss: 3.2523624897003174
Validation loss: 2.300899908106814

Epoch: 6| Step: 2
Training loss: 1.9173182249069214
Validation loss: 2.2981333040422007

Epoch: 6| Step: 3
Training loss: 2.4726624488830566
Validation loss: 2.2989521564975863

Epoch: 6| Step: 4
Training loss: 2.2745137214660645
Validation loss: 2.3152281289459555

Epoch: 6| Step: 5
Training loss: 2.8470444679260254
Validation loss: 2.3246193855039534

Epoch: 6| Step: 6
Training loss: 2.3489553928375244
Validation loss: 2.312790428438494

Epoch: 6| Step: 7
Training loss: 2.1699676513671875
Validation loss: 2.306662292890651

Epoch: 6| Step: 8
Training loss: 2.3377864360809326
Validation loss: 2.287063838333212

Epoch: 6| Step: 9
Training loss: 1.8652489185333252
Validation loss: 2.266550212778071

Epoch: 6| Step: 10
Training loss: 2.377234935760498
Validation loss: 2.2586798821726153

Epoch: 6| Step: 11
Training loss: 3.2120509147644043
Validation loss: 2.2659443373321206

Epoch: 6| Step: 12
Training loss: 1.923027515411377
Validation loss: 2.2669669376906527

Epoch: 6| Step: 13
Training loss: 3.6385715007781982
Validation loss: 2.27016282081604

Epoch: 182| Step: 0
Training loss: 2.5128750801086426
Validation loss: 2.2835468656273297

Epoch: 6| Step: 1
Training loss: 2.5026803016662598
Validation loss: 2.287085167823299

Epoch: 6| Step: 2
Training loss: 2.7842259407043457
Validation loss: 2.287828832544306

Epoch: 6| Step: 3
Training loss: 2.210899829864502
Validation loss: 2.3040820193547074

Epoch: 6| Step: 4
Training loss: 2.746969223022461
Validation loss: 2.312872548257151

Epoch: 6| Step: 5
Training loss: 2.510066032409668
Validation loss: 2.311208237883865

Epoch: 6| Step: 6
Training loss: 3.313310146331787
Validation loss: 2.3116115908468924

Epoch: 6| Step: 7
Training loss: 2.411496877670288
Validation loss: 2.2899028562730357

Epoch: 6| Step: 8
Training loss: 2.7093758583068848
Validation loss: 2.2860977700961533

Epoch: 6| Step: 9
Training loss: 2.1626243591308594
Validation loss: 2.2796398696079048

Epoch: 6| Step: 10
Training loss: 1.9930249452590942
Validation loss: 2.276223349314864

Epoch: 6| Step: 11
Training loss: 2.9892072677612305
Validation loss: 2.2800667542283253

Epoch: 6| Step: 12
Training loss: 2.1249799728393555
Validation loss: 2.282069185728668

Epoch: 6| Step: 13
Training loss: 2.659208059310913
Validation loss: 2.2877343687959897

Epoch: 183| Step: 0
Training loss: 3.220263957977295
Validation loss: 2.277006405656056

Epoch: 6| Step: 1
Training loss: 2.635441780090332
Validation loss: 2.2838462962899158

Epoch: 6| Step: 2
Training loss: 1.965299367904663
Validation loss: 2.273440487923161

Epoch: 6| Step: 3
Training loss: 2.9571497440338135
Validation loss: 2.2839609320445726

Epoch: 6| Step: 4
Training loss: 2.9923880100250244
Validation loss: 2.2780365764453845

Epoch: 6| Step: 5
Training loss: 2.738593339920044
Validation loss: 2.278553908871066

Epoch: 6| Step: 6
Training loss: 2.2717556953430176
Validation loss: 2.2748223504712506

Epoch: 6| Step: 7
Training loss: 2.8381972312927246
Validation loss: 2.2770461267040623

Epoch: 6| Step: 8
Training loss: 2.1740498542785645
Validation loss: 2.2754594484965005

Epoch: 6| Step: 9
Training loss: 2.298043727874756
Validation loss: 2.2632213151583107

Epoch: 6| Step: 10
Training loss: 2.956080913543701
Validation loss: 2.268727271787582

Epoch: 6| Step: 11
Training loss: 2.058412551879883
Validation loss: 2.262635379709223

Epoch: 6| Step: 12
Training loss: 1.8470925092697144
Validation loss: 2.2641185791261735

Epoch: 6| Step: 13
Training loss: 2.425466775894165
Validation loss: 2.27208403618105

Epoch: 184| Step: 0
Training loss: 2.831299066543579
Validation loss: 2.2795365138720443

Epoch: 6| Step: 1
Training loss: 1.8288829326629639
Validation loss: 2.296026510577048

Epoch: 6| Step: 2
Training loss: 2.2076549530029297
Validation loss: 2.3098399536584013

Epoch: 6| Step: 3
Training loss: 2.8360934257507324
Validation loss: 2.309424820766654

Epoch: 6| Step: 4
Training loss: 1.8846993446350098
Validation loss: 2.312360621267749

Epoch: 6| Step: 5
Training loss: 3.4083642959594727
Validation loss: 2.3012719410721973

Epoch: 6| Step: 6
Training loss: 2.0848541259765625
Validation loss: 2.302828642629808

Epoch: 6| Step: 7
Training loss: 2.415550708770752
Validation loss: 2.2878472779386785

Epoch: 6| Step: 8
Training loss: 2.0863544940948486
Validation loss: 2.2774670726509503

Epoch: 6| Step: 9
Training loss: 2.8501436710357666
Validation loss: 2.2864862283070884

Epoch: 6| Step: 10
Training loss: 2.830888271331787
Validation loss: 2.272956023934067

Epoch: 6| Step: 11
Training loss: 2.9082982540130615
Validation loss: 2.2876140891864734

Epoch: 6| Step: 12
Training loss: 3.1917178630828857
Validation loss: 2.275502645841209

Epoch: 6| Step: 13
Training loss: 1.6649287939071655
Validation loss: 2.2825571644690728

Epoch: 185| Step: 0
Training loss: 1.7921018600463867
Validation loss: 2.2673909433426394

Epoch: 6| Step: 1
Training loss: 3.1145992279052734
Validation loss: 2.2774063361588346

Epoch: 6| Step: 2
Training loss: 2.747452735900879
Validation loss: 2.2754422336496334

Epoch: 6| Step: 3
Training loss: 1.743974208831787
Validation loss: 2.2731783774591263

Epoch: 6| Step: 4
Training loss: 3.4223713874816895
Validation loss: 2.265859778209399

Epoch: 6| Step: 5
Training loss: 2.7670810222625732
Validation loss: 2.260388010291643

Epoch: 6| Step: 6
Training loss: 2.442725896835327
Validation loss: 2.264277401790824

Epoch: 6| Step: 7
Training loss: 2.766493320465088
Validation loss: 2.267912151992962

Epoch: 6| Step: 8
Training loss: 2.268181800842285
Validation loss: 2.264370543982393

Epoch: 6| Step: 9
Training loss: 2.498457908630371
Validation loss: 2.283207662643925

Epoch: 6| Step: 10
Training loss: 1.9716899394989014
Validation loss: 2.2882009770280574

Epoch: 6| Step: 11
Training loss: 2.508500814437866
Validation loss: 2.3057114360153035

Epoch: 6| Step: 12
Training loss: 3.0142602920532227
Validation loss: 2.3015029840571906

Epoch: 6| Step: 13
Training loss: 2.487231492996216
Validation loss: 2.3072034812742666

Epoch: 186| Step: 0
Training loss: 2.0568695068359375
Validation loss: 2.2930851982485865

Epoch: 6| Step: 1
Training loss: 2.7720327377319336
Validation loss: 2.2965872941478604

Epoch: 6| Step: 2
Training loss: 3.1130259037017822
Validation loss: 2.307339260655065

Epoch: 6| Step: 3
Training loss: 2.717388153076172
Validation loss: 2.3162581023349555

Epoch: 6| Step: 4
Training loss: 1.7782044410705566
Validation loss: 2.306517434376542

Epoch: 6| Step: 5
Training loss: 2.8727450370788574
Validation loss: 2.2935392702779462

Epoch: 6| Step: 6
Training loss: 2.5568244457244873
Validation loss: 2.2780642842733734

Epoch: 6| Step: 7
Training loss: 2.6369636058807373
Validation loss: 2.2877854608720347

Epoch: 6| Step: 8
Training loss: 2.0977725982666016
Validation loss: 2.2882545673719017

Epoch: 6| Step: 9
Training loss: 2.505791425704956
Validation loss: 2.2637281084573395

Epoch: 6| Step: 10
Training loss: 2.373302459716797
Validation loss: 2.268836098332559

Epoch: 6| Step: 11
Training loss: 2.2529263496398926
Validation loss: 2.260120960973924

Epoch: 6| Step: 12
Training loss: 3.0388519763946533
Validation loss: 2.255136528322774

Epoch: 6| Step: 13
Training loss: 2.513481378555298
Validation loss: 2.252006499998031

Epoch: 187| Step: 0
Training loss: 2.036308526992798
Validation loss: 2.2523200511932373

Epoch: 6| Step: 1
Training loss: 2.763333797454834
Validation loss: 2.2575659815983107

Epoch: 6| Step: 2
Training loss: 3.7067782878875732
Validation loss: 2.263952327030961

Epoch: 6| Step: 3
Training loss: 2.6434950828552246
Validation loss: 2.26505660369832

Epoch: 6| Step: 4
Training loss: 2.0612497329711914
Validation loss: 2.2639515092295985

Epoch: 6| Step: 5
Training loss: 2.2160067558288574
Validation loss: 2.2862434874298754

Epoch: 6| Step: 6
Training loss: 2.479726552963257
Validation loss: 2.2728452631222305

Epoch: 6| Step: 7
Training loss: 2.607417583465576
Validation loss: 2.278469212593571

Epoch: 6| Step: 8
Training loss: 2.2684006690979004
Validation loss: 2.2765022708523657

Epoch: 6| Step: 9
Training loss: 2.025880813598633
Validation loss: 2.272425707950387

Epoch: 6| Step: 10
Training loss: 3.099071502685547
Validation loss: 2.2906645190331245

Epoch: 6| Step: 11
Training loss: 2.283147096633911
Validation loss: 2.2832823107319493

Epoch: 6| Step: 12
Training loss: 2.726079225540161
Validation loss: 2.2791308997779764

Epoch: 6| Step: 13
Training loss: 2.237473964691162
Validation loss: 2.2579542872726277

Epoch: 188| Step: 0
Training loss: 2.9152135848999023
Validation loss: 2.2765683076714955

Epoch: 6| Step: 1
Training loss: 2.9202475547790527
Validation loss: 2.270601249510242

Epoch: 6| Step: 2
Training loss: 2.5703277587890625
Validation loss: 2.279550216531241

Epoch: 6| Step: 3
Training loss: 2.0964341163635254
Validation loss: 2.272193862545875

Epoch: 6| Step: 4
Training loss: 2.580665349960327
Validation loss: 2.2751398137820664

Epoch: 6| Step: 5
Training loss: 2.114152431488037
Validation loss: 2.265487658080234

Epoch: 6| Step: 6
Training loss: 2.4387288093566895
Validation loss: 2.264348165963286

Epoch: 6| Step: 7
Training loss: 2.1121788024902344
Validation loss: 2.2629452110618673

Epoch: 6| Step: 8
Training loss: 2.096196174621582
Validation loss: 2.2603622713396625

Epoch: 6| Step: 9
Training loss: 2.087803363800049
Validation loss: 2.260627664545531

Epoch: 6| Step: 10
Training loss: 3.0925731658935547
Validation loss: 2.258809761334491

Epoch: 6| Step: 11
Training loss: 2.290881633758545
Validation loss: 2.252746323103546

Epoch: 6| Step: 12
Training loss: 2.6720967292785645
Validation loss: 2.245501974577545

Epoch: 6| Step: 13
Training loss: 3.7862958908081055
Validation loss: 2.2436912700694096

Epoch: 189| Step: 0
Training loss: 2.27285099029541
Validation loss: 2.2335327735511203

Epoch: 6| Step: 1
Training loss: 2.2060794830322266
Validation loss: 2.2405195672024965

Epoch: 6| Step: 2
Training loss: 2.5898361206054688
Validation loss: 2.244836853396508

Epoch: 6| Step: 3
Training loss: 2.8987772464752197
Validation loss: 2.2424559721382717

Epoch: 6| Step: 4
Training loss: 3.1162497997283936
Validation loss: 2.2518999576568604

Epoch: 6| Step: 5
Training loss: 2.2786245346069336
Validation loss: 2.253423934341759

Epoch: 6| Step: 6
Training loss: 1.9651538133621216
Validation loss: 2.2501842744888796

Epoch: 6| Step: 7
Training loss: 2.8283982276916504
Validation loss: 2.2462037763287945

Epoch: 6| Step: 8
Training loss: 3.097031831741333
Validation loss: 2.2497081269500074

Epoch: 6| Step: 9
Training loss: 2.5955867767333984
Validation loss: 2.245495888494676

Epoch: 6| Step: 10
Training loss: 2.118136405944824
Validation loss: 2.243870178858439

Epoch: 6| Step: 11
Training loss: 2.4850406646728516
Validation loss: 2.246588609551871

Epoch: 6| Step: 12
Training loss: 2.9990100860595703
Validation loss: 2.237338673683905

Epoch: 6| Step: 13
Training loss: 2.0201475620269775
Validation loss: 2.241178753555462

Epoch: 190| Step: 0
Training loss: 2.3935065269470215
Validation loss: 2.246691729432793

Epoch: 6| Step: 1
Training loss: 3.1104073524475098
Validation loss: 2.258460203806559

Epoch: 6| Step: 2
Training loss: 1.9961395263671875
Validation loss: 2.254707669699064

Epoch: 6| Step: 3
Training loss: 2.2386128902435303
Validation loss: 2.2641305538915817

Epoch: 6| Step: 4
Training loss: 2.3792190551757812
Validation loss: 2.2972877064058856

Epoch: 6| Step: 5
Training loss: 2.3248233795166016
Validation loss: 2.3194980288064606

Epoch: 6| Step: 6
Training loss: 1.9792038202285767
Validation loss: 2.3352955951485583

Epoch: 6| Step: 7
Training loss: 2.95343017578125
Validation loss: 2.363339727924716

Epoch: 6| Step: 8
Training loss: 3.338595390319824
Validation loss: 2.323702448157854

Epoch: 6| Step: 9
Training loss: 3.2549843788146973
Validation loss: 2.314691323106007

Epoch: 6| Step: 10
Training loss: 2.41929292678833
Validation loss: 2.2827709387707453

Epoch: 6| Step: 11
Training loss: 2.1185524463653564
Validation loss: 2.276714765897361

Epoch: 6| Step: 12
Training loss: 2.816227436065674
Validation loss: 2.2566898381838234

Epoch: 6| Step: 13
Training loss: 1.890433669090271
Validation loss: 2.2500078960131575

Epoch: 191| Step: 0
Training loss: 3.017904758453369
Validation loss: 2.24662612330529

Epoch: 6| Step: 1
Training loss: 2.7299513816833496
Validation loss: 2.2408492142154324

Epoch: 6| Step: 2
Training loss: 2.628685474395752
Validation loss: 2.2428232521139164

Epoch: 6| Step: 3
Training loss: 2.9434657096862793
Validation loss: 2.239790995915731

Epoch: 6| Step: 4
Training loss: 1.4027289152145386
Validation loss: 2.2377889771615305

Epoch: 6| Step: 5
Training loss: 2.659536361694336
Validation loss: 2.237648084599485

Epoch: 6| Step: 6
Training loss: 3.1931557655334473
Validation loss: 2.237662616596427

Epoch: 6| Step: 7
Training loss: 2.5622878074645996
Validation loss: 2.240821402559998

Epoch: 6| Step: 8
Training loss: 2.4637222290039062
Validation loss: 2.2372918513513382

Epoch: 6| Step: 9
Training loss: 2.1712722778320312
Validation loss: 2.2384841198562295

Epoch: 6| Step: 10
Training loss: 2.2847347259521484
Validation loss: 2.239507862316665

Epoch: 6| Step: 11
Training loss: 2.627187728881836
Validation loss: 2.2432850458288707

Epoch: 6| Step: 12
Training loss: 2.4125781059265137
Validation loss: 2.2337347294694636

Epoch: 6| Step: 13
Training loss: 2.17164945602417
Validation loss: 2.237475117047628

Epoch: 192| Step: 0
Training loss: 2.999720811843872
Validation loss: 2.2504721354412776

Epoch: 6| Step: 1
Training loss: 2.4752931594848633
Validation loss: 2.2605287131442817

Epoch: 6| Step: 2
Training loss: 2.5335865020751953
Validation loss: 2.266122661611085

Epoch: 6| Step: 3
Training loss: 2.495544195175171
Validation loss: 2.2718424181784354

Epoch: 6| Step: 4
Training loss: 2.103469133377075
Validation loss: 2.277575926114154

Epoch: 6| Step: 5
Training loss: 2.835249900817871
Validation loss: 2.291132593667635

Epoch: 6| Step: 6
Training loss: 2.473403215408325
Validation loss: 2.2743554576750724

Epoch: 6| Step: 7
Training loss: 2.380323648452759
Validation loss: 2.279364671758426

Epoch: 6| Step: 8
Training loss: 2.6042275428771973
Validation loss: 2.2688069112839235

Epoch: 6| Step: 9
Training loss: 3.216513156890869
Validation loss: 2.270066398446278

Epoch: 6| Step: 10
Training loss: 2.5103588104248047
Validation loss: 2.2612705512713362

Epoch: 6| Step: 11
Training loss: 2.3695292472839355
Validation loss: 2.2538671698621524

Epoch: 6| Step: 12
Training loss: 2.1414132118225098
Validation loss: 2.254716783441523

Epoch: 6| Step: 13
Training loss: 1.9050401449203491
Validation loss: 2.263752075933641

Epoch: 193| Step: 0
Training loss: 2.63675856590271
Validation loss: 2.2609324788534515

Epoch: 6| Step: 1
Training loss: 2.3904008865356445
Validation loss: 2.262661621134768

Epoch: 6| Step: 2
Training loss: 2.807076930999756
Validation loss: 2.246520411583685

Epoch: 6| Step: 3
Training loss: 2.6485116481781006
Validation loss: 2.2558694167803695

Epoch: 6| Step: 4
Training loss: 1.9714292287826538
Validation loss: 2.2624766570265575

Epoch: 6| Step: 5
Training loss: 2.288408041000366
Validation loss: 2.270527716605894

Epoch: 6| Step: 6
Training loss: 2.95619535446167
Validation loss: 2.2832214755396687

Epoch: 6| Step: 7
Training loss: 2.2522549629211426
Validation loss: 2.2873413152592157

Epoch: 6| Step: 8
Training loss: 2.3668575286865234
Validation loss: 2.2758956865597795

Epoch: 6| Step: 9
Training loss: 2.1593871116638184
Validation loss: 2.283366600672404

Epoch: 6| Step: 10
Training loss: 2.5754504203796387
Validation loss: 2.265441640730827

Epoch: 6| Step: 11
Training loss: 2.710866928100586
Validation loss: 2.277448356792491

Epoch: 6| Step: 12
Training loss: 2.8814029693603516
Validation loss: 2.278742672294699

Epoch: 6| Step: 13
Training loss: 2.516832113265991
Validation loss: 2.2788204813516266

Epoch: 194| Step: 0
Training loss: 2.484921455383301
Validation loss: 2.283740399986185

Epoch: 6| Step: 1
Training loss: 2.6236414909362793
Validation loss: 2.292447877186601

Epoch: 6| Step: 2
Training loss: 2.0426995754241943
Validation loss: 2.2938068733420423

Epoch: 6| Step: 3
Training loss: 2.3075246810913086
Validation loss: 2.2986676616053425

Epoch: 6| Step: 4
Training loss: 2.3414292335510254
Validation loss: 2.2711871118955713

Epoch: 6| Step: 5
Training loss: 3.0577712059020996
Validation loss: 2.2573735162776005

Epoch: 6| Step: 6
Training loss: 2.103214740753174
Validation loss: 2.25145830903002

Epoch: 6| Step: 7
Training loss: 2.597717761993408
Validation loss: 2.25805841466432

Epoch: 6| Step: 8
Training loss: 2.552093505859375
Validation loss: 2.2495231013144217

Epoch: 6| Step: 9
Training loss: 2.273437738418579
Validation loss: 2.248497170786704

Epoch: 6| Step: 10
Training loss: 2.8067708015441895
Validation loss: 2.260551119363436

Epoch: 6| Step: 11
Training loss: 2.8084635734558105
Validation loss: 2.2394915037257697

Epoch: 6| Step: 12
Training loss: 2.3375706672668457
Validation loss: 2.2495584821188324

Epoch: 6| Step: 13
Training loss: 3.2837119102478027
Validation loss: 2.2641887305885233

Epoch: 195| Step: 0
Training loss: 2.2006285190582275
Validation loss: 2.256408291478311

Epoch: 6| Step: 1
Training loss: 2.079502820968628
Validation loss: 2.2607454817782164

Epoch: 6| Step: 2
Training loss: 2.8592417240142822
Validation loss: 2.2699370307307087

Epoch: 6| Step: 3
Training loss: 2.847459077835083
Validation loss: 2.259102241967314

Epoch: 6| Step: 4
Training loss: 2.231826066970825
Validation loss: 2.255254719846992

Epoch: 6| Step: 5
Training loss: 2.669222831726074
Validation loss: 2.248320194982713

Epoch: 6| Step: 6
Training loss: 2.659029483795166
Validation loss: 2.228965151694513

Epoch: 6| Step: 7
Training loss: 2.282808542251587
Validation loss: 2.23164257695598

Epoch: 6| Step: 8
Training loss: 2.353745937347412
Validation loss: 2.2304546845856534

Epoch: 6| Step: 9
Training loss: 3.19838285446167
Validation loss: 2.236465979647893

Epoch: 6| Step: 10
Training loss: 2.355290651321411
Validation loss: 2.227313221141856

Epoch: 6| Step: 11
Training loss: 2.8975820541381836
Validation loss: 2.2364324908102713

Epoch: 6| Step: 12
Training loss: 2.3451592922210693
Validation loss: 2.2281877917628132

Epoch: 6| Step: 13
Training loss: 2.453080177307129
Validation loss: 2.2348359861681537

Epoch: 196| Step: 0
Training loss: 2.3029987812042236
Validation loss: 2.2317753043226016

Epoch: 6| Step: 1
Training loss: 2.793083906173706
Validation loss: 2.234710044758294

Epoch: 6| Step: 2
Training loss: 2.471780300140381
Validation loss: 2.245449064880289

Epoch: 6| Step: 3
Training loss: 2.4966917037963867
Validation loss: 2.2514223590973885

Epoch: 6| Step: 4
Training loss: 2.886962890625
Validation loss: 2.269981380431883

Epoch: 6| Step: 5
Training loss: 2.866875171661377
Validation loss: 2.2895803810447775

Epoch: 6| Step: 6
Training loss: 1.9255731105804443
Validation loss: 2.321833306743253

Epoch: 6| Step: 7
Training loss: 2.386794328689575
Validation loss: 2.324724507588212

Epoch: 6| Step: 8
Training loss: 2.3725762367248535
Validation loss: 2.3450921338091613

Epoch: 6| Step: 9
Training loss: 3.055138349533081
Validation loss: 2.333507542969078

Epoch: 6| Step: 10
Training loss: 2.195988655090332
Validation loss: 2.3170221672263196

Epoch: 6| Step: 11
Training loss: 2.4778850078582764
Validation loss: 2.2956718783224783

Epoch: 6| Step: 12
Training loss: 2.1264002323150635
Validation loss: 2.293828982178883

Epoch: 6| Step: 13
Training loss: 3.4375832080841064
Validation loss: 2.2774953201252925

Epoch: 197| Step: 0
Training loss: 2.575023651123047
Validation loss: 2.26333660592315

Epoch: 6| Step: 1
Training loss: 2.009110689163208
Validation loss: 2.254681364182503

Epoch: 6| Step: 2
Training loss: 2.6143546104431152
Validation loss: 2.243957463131156

Epoch: 6| Step: 3
Training loss: 3.288754940032959
Validation loss: 2.2404047507111744

Epoch: 6| Step: 4
Training loss: 2.927424430847168
Validation loss: 2.2303571931777464

Epoch: 6| Step: 5
Training loss: 2.9924635887145996
Validation loss: 2.2336622066395257

Epoch: 6| Step: 6
Training loss: 1.7834306955337524
Validation loss: 2.235065007722506

Epoch: 6| Step: 7
Training loss: 2.5976080894470215
Validation loss: 2.2390701232417936

Epoch: 6| Step: 8
Training loss: 2.566800117492676
Validation loss: 2.228442750951295

Epoch: 6| Step: 9
Training loss: 1.955274224281311
Validation loss: 2.23115627227291

Epoch: 6| Step: 10
Training loss: 2.265826940536499
Validation loss: 2.2291831252395466

Epoch: 6| Step: 11
Training loss: 2.7244458198547363
Validation loss: 2.237059062527072

Epoch: 6| Step: 12
Training loss: 2.3732962608337402
Validation loss: 2.228468072029852

Epoch: 6| Step: 13
Training loss: 2.5084304809570312
Validation loss: 2.234303253953175

Epoch: 198| Step: 0
Training loss: 2.0402443408966064
Validation loss: 2.238980083055394

Epoch: 6| Step: 1
Training loss: 2.411599636077881
Validation loss: 2.2324019324394966

Epoch: 6| Step: 2
Training loss: 2.411607265472412
Validation loss: 2.2397198471971738

Epoch: 6| Step: 3
Training loss: 2.5218167304992676
Validation loss: 2.2355149022994505

Epoch: 6| Step: 4
Training loss: 2.49123477935791
Validation loss: 2.241021183870172

Epoch: 6| Step: 5
Training loss: 2.899909257888794
Validation loss: 2.2471345368252007

Epoch: 6| Step: 6
Training loss: 2.4811596870422363
Validation loss: 2.252191728161227

Epoch: 6| Step: 7
Training loss: 2.4268131256103516
Validation loss: 2.2545185332657187

Epoch: 6| Step: 8
Training loss: 3.073176860809326
Validation loss: 2.263064492133356

Epoch: 6| Step: 9
Training loss: 2.146900177001953
Validation loss: 2.2582264177260862

Epoch: 6| Step: 10
Training loss: 1.8253337144851685
Validation loss: 2.2626974326308056

Epoch: 6| Step: 11
Training loss: 3.060713052749634
Validation loss: 2.2528103782284643

Epoch: 6| Step: 12
Training loss: 2.8395490646362305
Validation loss: 2.2352306958167785

Epoch: 6| Step: 13
Training loss: 2.5716869831085205
Validation loss: 2.2382828779118036

Epoch: 199| Step: 0
Training loss: 2.1288833618164062
Validation loss: 2.243235008690947

Epoch: 6| Step: 1
Training loss: 2.9263076782226562
Validation loss: 2.2485932201467533

Epoch: 6| Step: 2
Training loss: 2.3100290298461914
Validation loss: 2.2722590713090796

Epoch: 6| Step: 3
Training loss: 2.5055429935455322
Validation loss: 2.2866736868376374

Epoch: 6| Step: 4
Training loss: 2.665069818496704
Validation loss: 2.304457367107432

Epoch: 6| Step: 5
Training loss: 2.2467188835144043
Validation loss: 2.3527427155484437

Epoch: 6| Step: 6
Training loss: 3.070713996887207
Validation loss: 2.340521122819634

Epoch: 6| Step: 7
Training loss: 2.687305450439453
Validation loss: 2.3414934553125852

Epoch: 6| Step: 8
Training loss: 2.9565839767456055
Validation loss: 2.3189276687560545

Epoch: 6| Step: 9
Training loss: 2.4134140014648438
Validation loss: 2.3141116403764292

Epoch: 6| Step: 10
Training loss: 2.87113881111145
Validation loss: 2.289575997219291

Epoch: 6| Step: 11
Training loss: 2.31583833694458
Validation loss: 2.2659721092511247

Epoch: 6| Step: 12
Training loss: 1.7855055332183838
Validation loss: 2.234549964627912

Epoch: 6| Step: 13
Training loss: 2.647373914718628
Validation loss: 2.2336467645501576

Epoch: 200| Step: 0
Training loss: 2.2670772075653076
Validation loss: 2.2340911126905874

Epoch: 6| Step: 1
Training loss: 2.450852870941162
Validation loss: 2.223759989584646

Epoch: 6| Step: 2
Training loss: 3.031898260116577
Validation loss: 2.2211427047688472

Epoch: 6| Step: 3
Training loss: 2.5465073585510254
Validation loss: 2.223630074531801

Epoch: 6| Step: 4
Training loss: 2.5006141662597656
Validation loss: 2.224595856922929

Epoch: 6| Step: 5
Training loss: 2.5439023971557617
Validation loss: 2.2205342118458082

Epoch: 6| Step: 6
Training loss: 2.4150900840759277
Validation loss: 2.218783417055684

Epoch: 6| Step: 7
Training loss: 3.638491630554199
Validation loss: 2.2223123658087944

Epoch: 6| Step: 8
Training loss: 2.138735055923462
Validation loss: 2.2157290161296888

Epoch: 6| Step: 9
Training loss: 1.7847574949264526
Validation loss: 2.227183508616622

Epoch: 6| Step: 10
Training loss: 2.6544177532196045
Validation loss: 2.2409511343125375

Epoch: 6| Step: 11
Training loss: 2.150458812713623
Validation loss: 2.255701421409525

Epoch: 6| Step: 12
Training loss: 2.389558792114258
Validation loss: 2.2671184385976484

Epoch: 6| Step: 13
Training loss: 2.8829078674316406
Validation loss: 2.2796083496462916

Epoch: 201| Step: 0
Training loss: 2.1862123012542725
Validation loss: 2.2945183592457927

Epoch: 6| Step: 1
Training loss: 2.1984992027282715
Validation loss: 2.3108312417102117

Epoch: 6| Step: 2
Training loss: 3.1768624782562256
Validation loss: 2.297060938291652

Epoch: 6| Step: 3
Training loss: 2.52998948097229
Validation loss: 2.2999901143453454

Epoch: 6| Step: 4
Training loss: 2.950199604034424
Validation loss: 2.290811871969572

Epoch: 6| Step: 5
Training loss: 2.3001761436462402
Validation loss: 2.2705806327122513

Epoch: 6| Step: 6
Training loss: 2.849808692932129
Validation loss: 2.2625512435872066

Epoch: 6| Step: 7
Training loss: 3.101348876953125
Validation loss: 2.249556972134498

Epoch: 6| Step: 8
Training loss: 2.221712112426758
Validation loss: 2.242196427878513

Epoch: 6| Step: 9
Training loss: 2.22595477104187
Validation loss: 2.2372499076269006

Epoch: 6| Step: 10
Training loss: 2.327491521835327
Validation loss: 2.2276419901078746

Epoch: 6| Step: 11
Training loss: 2.2533164024353027
Validation loss: 2.2285558100669616

Epoch: 6| Step: 12
Training loss: 2.362710952758789
Validation loss: 2.226686982698338

Epoch: 6| Step: 13
Training loss: 2.576176404953003
Validation loss: 2.219055647491127

Epoch: 202| Step: 0
Training loss: 2.413060426712036
Validation loss: 2.2251989123641804

Epoch: 6| Step: 1
Training loss: 2.5742828845977783
Validation loss: 2.215877534240805

Epoch: 6| Step: 2
Training loss: 2.9777984619140625
Validation loss: 2.227058074807608

Epoch: 6| Step: 3
Training loss: 2.786661148071289
Validation loss: 2.2308414431028467

Epoch: 6| Step: 4
Training loss: 2.3677215576171875
Validation loss: 2.224380070163358

Epoch: 6| Step: 5
Training loss: 2.58596134185791
Validation loss: 2.2259770567699144

Epoch: 6| Step: 6
Training loss: 2.655721664428711
Validation loss: 2.212398450861695

Epoch: 6| Step: 7
Training loss: 2.628490924835205
Validation loss: 2.2245872789813625

Epoch: 6| Step: 8
Training loss: 2.4878182411193848
Validation loss: 2.2173409461975098

Epoch: 6| Step: 9
Training loss: 2.2738189697265625
Validation loss: 2.2090144990592875

Epoch: 6| Step: 10
Training loss: 2.451491355895996
Validation loss: 2.214995943089967

Epoch: 6| Step: 11
Training loss: 1.9481804370880127
Validation loss: 2.217075999065112

Epoch: 6| Step: 12
Training loss: 2.1546032428741455
Validation loss: 2.217680597818026

Epoch: 6| Step: 13
Training loss: 2.726778984069824
Validation loss: 2.2389145499916485

Epoch: 203| Step: 0
Training loss: 2.719919443130493
Validation loss: 2.2356522621647006

Epoch: 6| Step: 1
Training loss: 2.514025926589966
Validation loss: 2.2476123276577202

Epoch: 6| Step: 2
Training loss: 2.6003618240356445
Validation loss: 2.241701231207899

Epoch: 6| Step: 3
Training loss: 2.720747470855713
Validation loss: 2.2431961285170687

Epoch: 6| Step: 4
Training loss: 1.9649105072021484
Validation loss: 2.244588854492352

Epoch: 6| Step: 5
Training loss: 2.56422758102417
Validation loss: 2.2407238509065364

Epoch: 6| Step: 6
Training loss: 2.4742164611816406
Validation loss: 2.230164973966537

Epoch: 6| Step: 7
Training loss: 3.20314884185791
Validation loss: 2.229318436755929

Epoch: 6| Step: 8
Training loss: 2.7367806434631348
Validation loss: 2.231832847800306

Epoch: 6| Step: 9
Training loss: 2.366387367248535
Validation loss: 2.2275872435621036

Epoch: 6| Step: 10
Training loss: 2.4299352169036865
Validation loss: 2.2192537707667195

Epoch: 6| Step: 11
Training loss: 1.7602614164352417
Validation loss: 2.2101962899649017

Epoch: 6| Step: 12
Training loss: 2.467158555984497
Validation loss: 2.214530739732968

Epoch: 6| Step: 13
Training loss: 2.40146541595459
Validation loss: 2.2210093108556603

Epoch: 204| Step: 0
Training loss: 2.6393730640411377
Validation loss: 2.223840146936396

Epoch: 6| Step: 1
Training loss: 1.970759391784668
Validation loss: 2.22433994149649

Epoch: 6| Step: 2
Training loss: 2.7559194564819336
Validation loss: 2.2176406947515344

Epoch: 6| Step: 3
Training loss: 2.5108046531677246
Validation loss: 2.2275674009835846

Epoch: 6| Step: 4
Training loss: 2.5544252395629883
Validation loss: 2.2253010144797702

Epoch: 6| Step: 5
Training loss: 2.6449902057647705
Validation loss: 2.231279803860572

Epoch: 6| Step: 6
Training loss: 1.5798966884613037
Validation loss: 2.244350110330889

Epoch: 6| Step: 7
Training loss: 2.8870224952697754
Validation loss: 2.2414984215972242

Epoch: 6| Step: 8
Training loss: 2.826289176940918
Validation loss: 2.2213329704858924

Epoch: 6| Step: 9
Training loss: 2.525692939758301
Validation loss: 2.231482054597588

Epoch: 6| Step: 10
Training loss: 2.524484157562256
Validation loss: 2.22408805354949

Epoch: 6| Step: 11
Training loss: 1.871471881866455
Validation loss: 2.2322829641321653

Epoch: 6| Step: 12
Training loss: 2.9205055236816406
Validation loss: 2.217265716163061

Epoch: 6| Step: 13
Training loss: 2.953723430633545
Validation loss: 2.2225570704347346

Epoch: 205| Step: 0
Training loss: 2.092315912246704
Validation loss: 2.216904399215534

Epoch: 6| Step: 1
Training loss: 2.3214833736419678
Validation loss: 2.222102188294934

Epoch: 6| Step: 2
Training loss: 2.733366012573242
Validation loss: 2.2207243570717434

Epoch: 6| Step: 3
Training loss: 2.9314939975738525
Validation loss: 2.2201172331328034

Epoch: 6| Step: 4
Training loss: 2.2044053077697754
Validation loss: 2.214765123141709

Epoch: 6| Step: 5
Training loss: 3.134230852127075
Validation loss: 2.210861552146173

Epoch: 6| Step: 6
Training loss: 1.9936200380325317
Validation loss: 2.2138462323014454

Epoch: 6| Step: 7
Training loss: 2.6826345920562744
Validation loss: 2.2104227286513134

Epoch: 6| Step: 8
Training loss: 2.994525671005249
Validation loss: 2.2028517235991774

Epoch: 6| Step: 9
Training loss: 1.9114134311676025
Validation loss: 2.211631897957094

Epoch: 6| Step: 10
Training loss: 2.304248332977295
Validation loss: 2.2140021042157243

Epoch: 6| Step: 11
Training loss: 2.4042296409606934
Validation loss: 2.2054403930582027

Epoch: 6| Step: 12
Training loss: 2.526695966720581
Validation loss: 2.215486554689305

Epoch: 6| Step: 13
Training loss: 2.884641647338867
Validation loss: 2.21319777734818

Epoch: 206| Step: 0
Training loss: 2.5180258750915527
Validation loss: 2.222985271484621

Epoch: 6| Step: 1
Training loss: 2.395144462585449
Validation loss: 2.2351253891503937

Epoch: 6| Step: 2
Training loss: 2.9823157787323
Validation loss: 2.2406223794465423

Epoch: 6| Step: 3
Training loss: 2.1321463584899902
Validation loss: 2.246763460097774

Epoch: 6| Step: 4
Training loss: 2.274916172027588
Validation loss: 2.240864162803978

Epoch: 6| Step: 5
Training loss: 3.1511971950531006
Validation loss: 2.2515729165846303

Epoch: 6| Step: 6
Training loss: 2.7922158241271973
Validation loss: 2.237872827437616

Epoch: 6| Step: 7
Training loss: 2.31866455078125
Validation loss: 2.240806287334811

Epoch: 6| Step: 8
Training loss: 2.4295902252197266
Validation loss: 2.2408458827644266

Epoch: 6| Step: 9
Training loss: 2.466954231262207
Validation loss: 2.2434911163904334

Epoch: 6| Step: 10
Training loss: 2.56571626663208
Validation loss: 2.2414376299868346

Epoch: 6| Step: 11
Training loss: 2.1878607273101807
Validation loss: 2.2269306208497737

Epoch: 6| Step: 12
Training loss: 1.6438381671905518
Validation loss: 2.2284797160856185

Epoch: 6| Step: 13
Training loss: 3.3263027667999268
Validation loss: 2.2273222938660653

Epoch: 207| Step: 0
Training loss: 2.4954075813293457
Validation loss: 2.230868790739326

Epoch: 6| Step: 1
Training loss: 2.625217914581299
Validation loss: 2.2292184765620897

Epoch: 6| Step: 2
Training loss: 2.7230844497680664
Validation loss: 2.233697557962069

Epoch: 6| Step: 3
Training loss: 2.6045708656311035
Validation loss: 2.222309579131424

Epoch: 6| Step: 4
Training loss: 2.4272584915161133
Validation loss: 2.213379539469237

Epoch: 6| Step: 5
Training loss: 2.434598922729492
Validation loss: 2.2184280400635092

Epoch: 6| Step: 6
Training loss: 2.937819719314575
Validation loss: 2.227217061545259

Epoch: 6| Step: 7
Training loss: 3.2091054916381836
Validation loss: 2.223766150013093

Epoch: 6| Step: 8
Training loss: 2.3961448669433594
Validation loss: 2.222120605489259

Epoch: 6| Step: 9
Training loss: 2.5352976322174072
Validation loss: 2.2266054973807385

Epoch: 6| Step: 10
Training loss: 2.2403831481933594
Validation loss: 2.2332363256844143

Epoch: 6| Step: 11
Training loss: 1.72842276096344
Validation loss: 2.237055596484933

Epoch: 6| Step: 12
Training loss: 2.176471471786499
Validation loss: 2.240526145504367

Epoch: 6| Step: 13
Training loss: 2.0964126586914062
Validation loss: 2.236372910520082

Epoch: 208| Step: 0
Training loss: 2.1005985736846924
Validation loss: 2.243700140266008

Epoch: 6| Step: 1
Training loss: 3.276942729949951
Validation loss: 2.2287976075244207

Epoch: 6| Step: 2
Training loss: 2.4585280418395996
Validation loss: 2.219168666870363

Epoch: 6| Step: 3
Training loss: 2.344510793685913
Validation loss: 2.2149759313111663

Epoch: 6| Step: 4
Training loss: 3.299922466278076
Validation loss: 2.215173598258726

Epoch: 6| Step: 5
Training loss: 1.6477670669555664
Validation loss: 2.224826758907687

Epoch: 6| Step: 6
Training loss: 2.509448528289795
Validation loss: 2.2144451192630235

Epoch: 6| Step: 7
Training loss: 2.427306652069092
Validation loss: 2.2243871047932613

Epoch: 6| Step: 8
Training loss: 1.8916633129119873
Validation loss: 2.217055283566957

Epoch: 6| Step: 9
Training loss: 2.8047783374786377
Validation loss: 2.232726748271655

Epoch: 6| Step: 10
Training loss: 2.9344515800476074
Validation loss: 2.213314608861041

Epoch: 6| Step: 11
Training loss: 2.6445679664611816
Validation loss: 2.216554944233228

Epoch: 6| Step: 12
Training loss: 1.9720280170440674
Validation loss: 2.2174787136816208

Epoch: 6| Step: 13
Training loss: 2.5399208068847656
Validation loss: 2.210415208211509

Epoch: 209| Step: 0
Training loss: 2.0901145935058594
Validation loss: 2.2100691487712245

Epoch: 6| Step: 1
Training loss: 3.096935987472534
Validation loss: 2.2117399272098335

Epoch: 6| Step: 2
Training loss: 2.7474021911621094
Validation loss: 2.2051383474821686

Epoch: 6| Step: 3
Training loss: 2.228559732437134
Validation loss: 2.2156841293458016

Epoch: 6| Step: 4
Training loss: 2.558774471282959
Validation loss: 2.227863937295893

Epoch: 6| Step: 5
Training loss: 2.589939832687378
Validation loss: 2.230511301307268

Epoch: 6| Step: 6
Training loss: 2.898763656616211
Validation loss: 2.2223990219895557

Epoch: 6| Step: 7
Training loss: 2.616502523422241
Validation loss: 2.217113901210088

Epoch: 6| Step: 8
Training loss: 2.7459874153137207
Validation loss: 2.222529672807263

Epoch: 6| Step: 9
Training loss: 2.5331714153289795
Validation loss: 2.218735561575941

Epoch: 6| Step: 10
Training loss: 2.0178303718566895
Validation loss: 2.2306098335532734

Epoch: 6| Step: 11
Training loss: 2.0474658012390137
Validation loss: 2.2267117026031658

Epoch: 6| Step: 12
Training loss: 2.132309913635254
Validation loss: 2.235645273680328

Epoch: 6| Step: 13
Training loss: 2.405764579772949
Validation loss: 2.241067350551646

Epoch: 210| Step: 0
Training loss: 2.124642848968506
Validation loss: 2.2208839898468344

Epoch: 6| Step: 1
Training loss: 1.895456314086914
Validation loss: 2.230435158616753

Epoch: 6| Step: 2
Training loss: 2.514190673828125
Validation loss: 2.2239161050447853

Epoch: 6| Step: 3
Training loss: 2.1559271812438965
Validation loss: 2.226442060162944

Epoch: 6| Step: 4
Training loss: 2.5404648780822754
Validation loss: 2.2312728563944497

Epoch: 6| Step: 5
Training loss: 3.0491113662719727
Validation loss: 2.2165254495477162

Epoch: 6| Step: 6
Training loss: 1.965868353843689
Validation loss: 2.2081829732464207

Epoch: 6| Step: 7
Training loss: 2.3648667335510254
Validation loss: 2.215564291964295

Epoch: 6| Step: 8
Training loss: 2.5619637966156006
Validation loss: 2.212425588279642

Epoch: 6| Step: 9
Training loss: 2.2655985355377197
Validation loss: 2.2216933491409465

Epoch: 6| Step: 10
Training loss: 3.2251548767089844
Validation loss: 2.2273022743963424

Epoch: 6| Step: 11
Training loss: 2.6265945434570312
Validation loss: 2.239566764523906

Epoch: 6| Step: 12
Training loss: 3.140231132507324
Validation loss: 2.2576429510629303

Epoch: 6| Step: 13
Training loss: 2.12339448928833
Validation loss: 2.2458192071607037

Epoch: 211| Step: 0
Training loss: 2.449103832244873
Validation loss: 2.230838939707766

Epoch: 6| Step: 1
Training loss: 3.040253162384033
Validation loss: 2.23009475328589

Epoch: 6| Step: 2
Training loss: 2.526731014251709
Validation loss: 2.206756240578108

Epoch: 6| Step: 3
Training loss: 2.273771047592163
Validation loss: 2.2064751707097536

Epoch: 6| Step: 4
Training loss: 2.3773012161254883
Validation loss: 2.211999290732927

Epoch: 6| Step: 5
Training loss: 3.0000412464141846
Validation loss: 2.218224694651942

Epoch: 6| Step: 6
Training loss: 2.058199882507324
Validation loss: 2.2165756417858984

Epoch: 6| Step: 7
Training loss: 3.0629563331604004
Validation loss: 2.2086232580164427

Epoch: 6| Step: 8
Training loss: 2.631007194519043
Validation loss: 2.2123703982240412

Epoch: 6| Step: 9
Training loss: 2.6507015228271484
Validation loss: 2.2129906082666047

Epoch: 6| Step: 10
Training loss: 2.2660012245178223
Validation loss: 2.216578665600028

Epoch: 6| Step: 11
Training loss: 2.0819637775421143
Validation loss: 2.2159457770727014

Epoch: 6| Step: 12
Training loss: 1.8379640579223633
Validation loss: 2.23249981223896

Epoch: 6| Step: 13
Training loss: 2.7096824645996094
Validation loss: 2.2261737008248605

Epoch: 212| Step: 0
Training loss: 2.260122776031494
Validation loss: 2.213701460951118

Epoch: 6| Step: 1
Training loss: 2.0969622135162354
Validation loss: 2.2297471953976538

Epoch: 6| Step: 2
Training loss: 2.65576171875
Validation loss: 2.245740431611256

Epoch: 6| Step: 3
Training loss: 2.4684247970581055
Validation loss: 2.2461693133077314

Epoch: 6| Step: 4
Training loss: 2.015374183654785
Validation loss: 2.241524580986269

Epoch: 6| Step: 5
Training loss: 2.744205951690674
Validation loss: 2.2360194319037983

Epoch: 6| Step: 6
Training loss: 3.4268062114715576
Validation loss: 2.236905341507286

Epoch: 6| Step: 7
Training loss: 2.111720561981201
Validation loss: 2.2148437551272813

Epoch: 6| Step: 8
Training loss: 3.043221950531006
Validation loss: 2.2192864007847284

Epoch: 6| Step: 9
Training loss: 2.679689407348633
Validation loss: 2.212952154938893

Epoch: 6| Step: 10
Training loss: 2.3199124336242676
Validation loss: 2.2111863384964647

Epoch: 6| Step: 11
Training loss: 2.0960593223571777
Validation loss: 2.2167411619617092

Epoch: 6| Step: 12
Training loss: 2.0880722999572754
Validation loss: 2.2138852098936677

Epoch: 6| Step: 13
Training loss: 2.9249637126922607
Validation loss: 2.197139581044515

Epoch: 213| Step: 0
Training loss: 2.5732226371765137
Validation loss: 2.204549979138118

Epoch: 6| Step: 1
Training loss: 1.7555770874023438
Validation loss: 2.20097525658146

Epoch: 6| Step: 2
Training loss: 2.728365421295166
Validation loss: 2.195504903793335

Epoch: 6| Step: 3
Training loss: 2.706523895263672
Validation loss: 2.206086561244021

Epoch: 6| Step: 4
Training loss: 2.7329154014587402
Validation loss: 2.2038612839996174

Epoch: 6| Step: 5
Training loss: 2.4053831100463867
Validation loss: 2.199693828500727

Epoch: 6| Step: 6
Training loss: 2.4608798027038574
Validation loss: 2.2038827865354476

Epoch: 6| Step: 7
Training loss: 2.4594154357910156
Validation loss: 2.1966530584519908

Epoch: 6| Step: 8
Training loss: 3.057875633239746
Validation loss: 2.19825775905322

Epoch: 6| Step: 9
Training loss: 2.1195080280303955
Validation loss: 2.2043467772904264

Epoch: 6| Step: 10
Training loss: 2.1317191123962402
Validation loss: 2.1955552921500257

Epoch: 6| Step: 11
Training loss: 2.4650983810424805
Validation loss: 2.204649261249009

Epoch: 6| Step: 12
Training loss: 2.882554292678833
Validation loss: 2.2207588175291657

Epoch: 6| Step: 13
Training loss: 2.549558162689209
Validation loss: 2.2304817861126316

Epoch: 214| Step: 0
Training loss: 1.88234281539917
Validation loss: 2.2348832648287535

Epoch: 6| Step: 1
Training loss: 1.9784225225448608
Validation loss: 2.229785293661138

Epoch: 6| Step: 2
Training loss: 2.6270487308502197
Validation loss: 2.2279715999480216

Epoch: 6| Step: 3
Training loss: 2.425976037979126
Validation loss: 2.2213052677851852

Epoch: 6| Step: 4
Training loss: 3.01924204826355
Validation loss: 2.237982439738448

Epoch: 6| Step: 5
Training loss: 2.7174949645996094
Validation loss: 2.2418811295622136

Epoch: 6| Step: 6
Training loss: 2.2573843002319336
Validation loss: 2.2480218051582255

Epoch: 6| Step: 7
Training loss: 2.542715072631836
Validation loss: 2.2497902352322816

Epoch: 6| Step: 8
Training loss: 2.4196760654449463
Validation loss: 2.246227874550768

Epoch: 6| Step: 9
Training loss: 2.6729896068573
Validation loss: 2.2319965388185237

Epoch: 6| Step: 10
Training loss: 2.2440555095672607
Validation loss: 2.227875283969346

Epoch: 6| Step: 11
Training loss: 2.268400192260742
Validation loss: 2.2169640192421536

Epoch: 6| Step: 12
Training loss: 2.7443294525146484
Validation loss: 2.2267378786558747

Epoch: 6| Step: 13
Training loss: 3.565664052963257
Validation loss: 2.2136118899109545

Epoch: 215| Step: 0
Training loss: 2.527097225189209
Validation loss: 2.2076402915421354

Epoch: 6| Step: 1
Training loss: 2.872875213623047
Validation loss: 2.2077687248106925

Epoch: 6| Step: 2
Training loss: 2.29123592376709
Validation loss: 2.217107049880489

Epoch: 6| Step: 3
Training loss: 3.072587251663208
Validation loss: 2.2022127130980134

Epoch: 6| Step: 4
Training loss: 2.6532411575317383
Validation loss: 2.207959451983052

Epoch: 6| Step: 5
Training loss: 2.452605724334717
Validation loss: 2.207890648995676

Epoch: 6| Step: 6
Training loss: 3.047377586364746
Validation loss: 2.2289397601158387

Epoch: 6| Step: 7
Training loss: 3.443673610687256
Validation loss: 2.2232010108168407

Epoch: 6| Step: 8
Training loss: 1.8374075889587402
Validation loss: 2.227283695692657

Epoch: 6| Step: 9
Training loss: 1.980800986289978
Validation loss: 2.2446140268797516

Epoch: 6| Step: 10
Training loss: 2.1453757286071777
Validation loss: 2.231245502348869

Epoch: 6| Step: 11
Training loss: 1.9045658111572266
Validation loss: 2.221290367905812

Epoch: 6| Step: 12
Training loss: 2.456286907196045
Validation loss: 2.2070022859881

Epoch: 6| Step: 13
Training loss: 1.7964178323745728
Validation loss: 2.197008802044776

Epoch: 216| Step: 0
Training loss: 2.63132905960083
Validation loss: 2.195781732118258

Epoch: 6| Step: 1
Training loss: 3.084977149963379
Validation loss: 2.2065873043511504

Epoch: 6| Step: 2
Training loss: 2.3347349166870117
Validation loss: 2.198346088009496

Epoch: 6| Step: 3
Training loss: 2.7032968997955322
Validation loss: 2.2002934922454176

Epoch: 6| Step: 4
Training loss: 2.501842498779297
Validation loss: 2.19723185928919

Epoch: 6| Step: 5
Training loss: 2.3983075618743896
Validation loss: 2.207298965864284

Epoch: 6| Step: 6
Training loss: 2.4147789478302
Validation loss: 2.2077892980267926

Epoch: 6| Step: 7
Training loss: 2.1995954513549805
Validation loss: 2.209071095271777

Epoch: 6| Step: 8
Training loss: 1.9660980701446533
Validation loss: 2.2045794379326606

Epoch: 6| Step: 9
Training loss: 1.4824442863464355
Validation loss: 2.2122493713132796

Epoch: 6| Step: 10
Training loss: 2.8043932914733887
Validation loss: 2.210012000094178

Epoch: 6| Step: 11
Training loss: 2.4899682998657227
Validation loss: 2.2099884069094093

Epoch: 6| Step: 12
Training loss: 2.5669150352478027
Validation loss: 2.212209750247258

Epoch: 6| Step: 13
Training loss: 3.549501657485962
Validation loss: 2.208819403443285

Epoch: 217| Step: 0
Training loss: 1.8030979633331299
Validation loss: 2.2061177863869617

Epoch: 6| Step: 1
Training loss: 3.1539742946624756
Validation loss: 2.203831808541411

Epoch: 6| Step: 2
Training loss: 3.026120662689209
Validation loss: 2.2088395959587506

Epoch: 6| Step: 3
Training loss: 2.5239391326904297
Validation loss: 2.214259145080402

Epoch: 6| Step: 4
Training loss: 2.5834994316101074
Validation loss: 2.202514740728563

Epoch: 6| Step: 5
Training loss: 2.3737173080444336
Validation loss: 2.22149404146338

Epoch: 6| Step: 6
Training loss: 2.139188766479492
Validation loss: 2.212121012390301

Epoch: 6| Step: 7
Training loss: 2.251734972000122
Validation loss: 2.225679955174846

Epoch: 6| Step: 8
Training loss: 3.0490143299102783
Validation loss: 2.2353011946524344

Epoch: 6| Step: 9
Training loss: 1.7666230201721191
Validation loss: 2.232552541199551

Epoch: 6| Step: 10
Training loss: 2.5911691188812256
Validation loss: 2.226650176509734

Epoch: 6| Step: 11
Training loss: 2.7266407012939453
Validation loss: 2.225150144228371

Epoch: 6| Step: 12
Training loss: 2.2412729263305664
Validation loss: 2.2114901311935915

Epoch: 6| Step: 13
Training loss: 2.2085092067718506
Validation loss: 2.2045852599605436

Epoch: 218| Step: 0
Training loss: 2.176647663116455
Validation loss: 2.21123904566611

Epoch: 6| Step: 1
Training loss: 2.192716598510742
Validation loss: 2.2045206792892946

Epoch: 6| Step: 2
Training loss: 1.8833582401275635
Validation loss: 2.2062647393954697

Epoch: 6| Step: 3
Training loss: 1.9973173141479492
Validation loss: 2.20946228888727

Epoch: 6| Step: 4
Training loss: 2.472259044647217
Validation loss: 2.196817946690385

Epoch: 6| Step: 5
Training loss: 2.5808708667755127
Validation loss: 2.202237541957568

Epoch: 6| Step: 6
Training loss: 2.470885753631592
Validation loss: 2.2104392641334125

Epoch: 6| Step: 7
Training loss: 3.4578466415405273
Validation loss: 2.2129162460245113

Epoch: 6| Step: 8
Training loss: 2.863077402114868
Validation loss: 2.233753401746032

Epoch: 6| Step: 9
Training loss: 2.5051651000976562
Validation loss: 2.2209073305130005

Epoch: 6| Step: 10
Training loss: 2.9242682456970215
Validation loss: 2.212686356677804

Epoch: 6| Step: 11
Training loss: 2.3307318687438965
Validation loss: 2.20361614483659

Epoch: 6| Step: 12
Training loss: 2.5445289611816406
Validation loss: 2.200638000683118

Epoch: 6| Step: 13
Training loss: 1.8270424604415894
Validation loss: 2.1911517599577546

Epoch: 219| Step: 0
Training loss: 2.4568686485290527
Validation loss: 2.2015348288320724

Epoch: 6| Step: 1
Training loss: 1.6525731086730957
Validation loss: 2.1895299265461583

Epoch: 6| Step: 2
Training loss: 1.9316520690917969
Validation loss: 2.1955118281866914

Epoch: 6| Step: 3
Training loss: 2.3894870281219482
Validation loss: 2.190587438562865

Epoch: 6| Step: 4
Training loss: 2.4284167289733887
Validation loss: 2.20547737613801

Epoch: 6| Step: 5
Training loss: 2.3715903759002686
Validation loss: 2.207858383014638

Epoch: 6| Step: 6
Training loss: 2.8116343021392822
Validation loss: 2.2167864281644105

Epoch: 6| Step: 7
Training loss: 3.1314258575439453
Validation loss: 2.2087972856337026

Epoch: 6| Step: 8
Training loss: 2.282639503479004
Validation loss: 2.2069216633355744

Epoch: 6| Step: 9
Training loss: 2.5282602310180664
Validation loss: 2.1964983837578886

Epoch: 6| Step: 10
Training loss: 2.5725691318511963
Validation loss: 2.184734126572968

Epoch: 6| Step: 11
Training loss: 2.261000633239746
Validation loss: 2.20154732529835

Epoch: 6| Step: 12
Training loss: 2.61106538772583
Validation loss: 2.201587694947438

Epoch: 6| Step: 13
Training loss: 3.5675954818725586
Validation loss: 2.196559380459529

Epoch: 220| Step: 0
Training loss: 2.3713765144348145
Validation loss: 2.188632221632106

Epoch: 6| Step: 1
Training loss: 1.9106156826019287
Validation loss: 2.2009748540898806

Epoch: 6| Step: 2
Training loss: 3.097769260406494
Validation loss: 2.1899177720469813

Epoch: 6| Step: 3
Training loss: 3.02718448638916
Validation loss: 2.210656263495004

Epoch: 6| Step: 4
Training loss: 2.690220594406128
Validation loss: 2.220559899524976

Epoch: 6| Step: 5
Training loss: 1.6451330184936523
Validation loss: 2.2212345856492237

Epoch: 6| Step: 6
Training loss: 3.176988124847412
Validation loss: 2.2224757491901355

Epoch: 6| Step: 7
Training loss: 2.250491142272949
Validation loss: 2.2162216632596907

Epoch: 6| Step: 8
Training loss: 2.4077541828155518
Validation loss: 2.2148575167502127

Epoch: 6| Step: 9
Training loss: 2.7991998195648193
Validation loss: 2.212449845447335

Epoch: 6| Step: 10
Training loss: 2.2500123977661133
Validation loss: 2.1848652773005988

Epoch: 6| Step: 11
Training loss: 3.16642427444458
Validation loss: 2.1849026961993148

Epoch: 6| Step: 12
Training loss: 1.9165242910385132
Validation loss: 2.1838045145875666

Epoch: 6| Step: 13
Training loss: 1.5220646858215332
Validation loss: 2.1853748880406862

Epoch: 221| Step: 0
Training loss: 2.990450859069824
Validation loss: 2.1834210554758706

Epoch: 6| Step: 1
Training loss: 2.343400239944458
Validation loss: 2.184727386761737

Epoch: 6| Step: 2
Training loss: 1.9778090715408325
Validation loss: 2.1800662497038483

Epoch: 6| Step: 3
Training loss: 2.6072933673858643
Validation loss: 2.1794579157265286

Epoch: 6| Step: 4
Training loss: 1.7302422523498535
Validation loss: 2.1755539409575926

Epoch: 6| Step: 5
Training loss: 2.499114751815796
Validation loss: 2.1849543202307915

Epoch: 6| Step: 6
Training loss: 2.76151180267334
Validation loss: 2.1845022760411745

Epoch: 6| Step: 7
Training loss: 3.194810152053833
Validation loss: 2.190915374345677

Epoch: 6| Step: 8
Training loss: 2.8179187774658203
Validation loss: 2.1961564428062847

Epoch: 6| Step: 9
Training loss: 1.7834701538085938
Validation loss: 2.1996257792236986

Epoch: 6| Step: 10
Training loss: 2.401923656463623
Validation loss: 2.2114875521711124

Epoch: 6| Step: 11
Training loss: 2.4109482765197754
Validation loss: 2.212374758976762

Epoch: 6| Step: 12
Training loss: 2.855109691619873
Validation loss: 2.2018976967821837

Epoch: 6| Step: 13
Training loss: 2.1547117233276367
Validation loss: 2.20133460978026

Epoch: 222| Step: 0
Training loss: 2.9483413696289062
Validation loss: 2.215975343540151

Epoch: 6| Step: 1
Training loss: 2.2533044815063477
Validation loss: 2.2022669802429857

Epoch: 6| Step: 2
Training loss: 2.2357192039489746
Validation loss: 2.2022834426613263

Epoch: 6| Step: 3
Training loss: 2.5279946327209473
Validation loss: 2.206344304546233

Epoch: 6| Step: 4
Training loss: 1.8911981582641602
Validation loss: 2.1868265546778196

Epoch: 6| Step: 5
Training loss: 1.9234811067581177
Validation loss: 2.190051555633545

Epoch: 6| Step: 6
Training loss: 2.482121467590332
Validation loss: 2.184641884219262

Epoch: 6| Step: 7
Training loss: 3.1614928245544434
Validation loss: 2.1797864514012493

Epoch: 6| Step: 8
Training loss: 2.6589176654815674
Validation loss: 2.188419832978197

Epoch: 6| Step: 9
Training loss: 2.970285415649414
Validation loss: 2.191777206236316

Epoch: 6| Step: 10
Training loss: 1.8669072389602661
Validation loss: 2.192760694411493

Epoch: 6| Step: 11
Training loss: 2.6102757453918457
Validation loss: 2.194035268598987

Epoch: 6| Step: 12
Training loss: 2.0628061294555664
Validation loss: 2.191670935641053

Epoch: 6| Step: 13
Training loss: 3.2078561782836914
Validation loss: 2.1961119174957275

Epoch: 223| Step: 0
Training loss: 2.4586410522460938
Validation loss: 2.201926060902175

Epoch: 6| Step: 1
Training loss: 2.6592867374420166
Validation loss: 2.209085782368978

Epoch: 6| Step: 2
Training loss: 3.0459775924682617
Validation loss: 2.225172601720338

Epoch: 6| Step: 3
Training loss: 2.8394598960876465
Validation loss: 2.2072707863264185

Epoch: 6| Step: 4
Training loss: 2.527129650115967
Validation loss: 2.209193362984606

Epoch: 6| Step: 5
Training loss: 2.270397663116455
Validation loss: 2.1959051470602713

Epoch: 6| Step: 6
Training loss: 2.256300687789917
Validation loss: 2.2059788037371892

Epoch: 6| Step: 7
Training loss: 1.8358198404312134
Validation loss: 2.201054257731284

Epoch: 6| Step: 8
Training loss: 2.504225730895996
Validation loss: 2.212284795699581

Epoch: 6| Step: 9
Training loss: 2.501208543777466
Validation loss: 2.1915600222925984

Epoch: 6| Step: 10
Training loss: 2.529353380203247
Validation loss: 2.2032669000728156

Epoch: 6| Step: 11
Training loss: 2.7321624755859375
Validation loss: 2.196806776908136

Epoch: 6| Step: 12
Training loss: 2.1735105514526367
Validation loss: 2.2019021049622567

Epoch: 6| Step: 13
Training loss: 1.8005532026290894
Validation loss: 2.2020559721095587

Epoch: 224| Step: 0
Training loss: 1.9081707000732422
Validation loss: 2.208961458616359

Epoch: 6| Step: 1
Training loss: 2.7014288902282715
Validation loss: 2.2076053132293043

Epoch: 6| Step: 2
Training loss: 2.835780382156372
Validation loss: 2.224466799407877

Epoch: 6| Step: 3
Training loss: 2.580979108810425
Validation loss: 2.2339803993061023

Epoch: 6| Step: 4
Training loss: 2.7256178855895996
Validation loss: 2.249439443311384

Epoch: 6| Step: 5
Training loss: 1.621989369392395
Validation loss: 2.253307347656578

Epoch: 6| Step: 6
Training loss: 3.023418664932251
Validation loss: 2.223787320557461

Epoch: 6| Step: 7
Training loss: 2.032078742980957
Validation loss: 2.210582028153122

Epoch: 6| Step: 8
Training loss: 2.538545608520508
Validation loss: 2.20124706914348

Epoch: 6| Step: 9
Training loss: 2.5547523498535156
Validation loss: 2.196761031304636

Epoch: 6| Step: 10
Training loss: 2.3812570571899414
Validation loss: 2.18437821121626

Epoch: 6| Step: 11
Training loss: 3.0353798866271973
Validation loss: 2.186314229042299

Epoch: 6| Step: 12
Training loss: 2.1136622428894043
Validation loss: 2.181294959078553

Epoch: 6| Step: 13
Training loss: 2.4098334312438965
Validation loss: 2.17700255814419

Epoch: 225| Step: 0
Training loss: 2.6619818210601807
Validation loss: 2.19066713189566

Epoch: 6| Step: 1
Training loss: 1.9319217205047607
Validation loss: 2.1868747690672516

Epoch: 6| Step: 2
Training loss: 2.656846046447754
Validation loss: 2.187688171222646

Epoch: 6| Step: 3
Training loss: 2.04372501373291
Validation loss: 2.195866797560005

Epoch: 6| Step: 4
Training loss: 2.8750009536743164
Validation loss: 2.19038027076311

Epoch: 6| Step: 5
Training loss: 3.549151659011841
Validation loss: 2.2104615934433474

Epoch: 6| Step: 6
Training loss: 2.360701084136963
Validation loss: 2.2210559370697185

Epoch: 6| Step: 7
Training loss: 2.6979146003723145
Validation loss: 2.2196269125066777

Epoch: 6| Step: 8
Training loss: 2.613868236541748
Validation loss: 2.1928536533027567

Epoch: 6| Step: 9
Training loss: 1.8089853525161743
Validation loss: 2.1997933900484474

Epoch: 6| Step: 10
Training loss: 2.3719871044158936
Validation loss: 2.2048515709497596

Epoch: 6| Step: 11
Training loss: 2.6694836616516113
Validation loss: 2.1946734766806326

Epoch: 6| Step: 12
Training loss: 2.33072566986084
Validation loss: 2.191866551676104

Epoch: 6| Step: 13
Training loss: 1.7557454109191895
Validation loss: 2.1960380384998937

Epoch: 226| Step: 0
Training loss: 2.6197640895843506
Validation loss: 2.210770091702861

Epoch: 6| Step: 1
Training loss: 1.7448886632919312
Validation loss: 2.2125318024748113

Epoch: 6| Step: 2
Training loss: 2.8038289546966553
Validation loss: 2.2171556898342666

Epoch: 6| Step: 3
Training loss: 2.0907540321350098
Validation loss: 2.198474076486403

Epoch: 6| Step: 4
Training loss: 2.5980069637298584
Validation loss: 2.1976697932007494

Epoch: 6| Step: 5
Training loss: 2.764357328414917
Validation loss: 2.1981665242102837

Epoch: 6| Step: 6
Training loss: 2.553431510925293
Validation loss: 2.195498128091135

Epoch: 6| Step: 7
Training loss: 2.716428279876709
Validation loss: 2.1922120535245506

Epoch: 6| Step: 8
Training loss: 2.1818432807922363
Validation loss: 2.1905385499359458

Epoch: 6| Step: 9
Training loss: 2.8909335136413574
Validation loss: 2.188415294052452

Epoch: 6| Step: 10
Training loss: 2.0118513107299805
Validation loss: 2.201383739389399

Epoch: 6| Step: 11
Training loss: 2.8018088340759277
Validation loss: 2.2167563182051464

Epoch: 6| Step: 12
Training loss: 2.126960277557373
Validation loss: 2.2163157719437794

Epoch: 6| Step: 13
Training loss: 2.742495536804199
Validation loss: 2.215451407176192

Epoch: 227| Step: 0
Training loss: 2.294610023498535
Validation loss: 2.2213939415511263

Epoch: 6| Step: 1
Training loss: 2.6114256381988525
Validation loss: 2.221488188671809

Epoch: 6| Step: 2
Training loss: 1.9015498161315918
Validation loss: 2.216353065224104

Epoch: 6| Step: 3
Training loss: 2.2995924949645996
Validation loss: 2.2248820540725545

Epoch: 6| Step: 4
Training loss: 2.2738451957702637
Validation loss: 2.2084175156008814

Epoch: 6| Step: 5
Training loss: 1.6986901760101318
Validation loss: 2.2102350650295133

Epoch: 6| Step: 6
Training loss: 3.2038919925689697
Validation loss: 2.2024215447005404

Epoch: 6| Step: 7
Training loss: 2.6798877716064453
Validation loss: 2.2142194471051617

Epoch: 6| Step: 8
Training loss: 2.425654411315918
Validation loss: 2.221172055890483

Epoch: 6| Step: 9
Training loss: 2.382605791091919
Validation loss: 2.2550476084473314

Epoch: 6| Step: 10
Training loss: 2.363793134689331
Validation loss: 2.2439954767944994

Epoch: 6| Step: 11
Training loss: 2.6292386054992676
Validation loss: 2.260939541683402

Epoch: 6| Step: 12
Training loss: 2.681328773498535
Validation loss: 2.2432932674243884

Epoch: 6| Step: 13
Training loss: 3.408381462097168
Validation loss: 2.239584163952899

Epoch: 228| Step: 0
Training loss: 2.5429906845092773
Validation loss: 2.222929626382807

Epoch: 6| Step: 1
Training loss: 2.348616361618042
Validation loss: 2.1918006404753654

Epoch: 6| Step: 2
Training loss: 2.346752882003784
Validation loss: 2.1855596086030364

Epoch: 6| Step: 3
Training loss: 2.581298828125
Validation loss: 2.178982934644145

Epoch: 6| Step: 4
Training loss: 3.0539698600769043
Validation loss: 2.1857093559798373

Epoch: 6| Step: 5
Training loss: 2.229902744293213
Validation loss: 2.1867052790939168

Epoch: 6| Step: 6
Training loss: 1.7775300741195679
Validation loss: 2.174658320283377

Epoch: 6| Step: 7
Training loss: 2.5893611907958984
Validation loss: 2.1694558102597474

Epoch: 6| Step: 8
Training loss: 2.61177921295166
Validation loss: 2.162638730900262

Epoch: 6| Step: 9
Training loss: 2.2575440406799316
Validation loss: 2.165522362596245

Epoch: 6| Step: 10
Training loss: 2.1120128631591797
Validation loss: 2.1758322433758805

Epoch: 6| Step: 11
Training loss: 2.9447696208953857
Validation loss: 2.176660055755287

Epoch: 6| Step: 12
Training loss: 2.648434638977051
Validation loss: 2.1816624185090423

Epoch: 6| Step: 13
Training loss: 2.5605268478393555
Validation loss: 2.184748221469182

Epoch: 229| Step: 0
Training loss: 2.18498158454895
Validation loss: 2.196060465228173

Epoch: 6| Step: 1
Training loss: 2.7503662109375
Validation loss: 2.1854917157080864

Epoch: 6| Step: 2
Training loss: 1.566899061203003
Validation loss: 2.1799375203347977

Epoch: 6| Step: 3
Training loss: 2.511409282684326
Validation loss: 2.1789561727995514

Epoch: 6| Step: 4
Training loss: 2.7377920150756836
Validation loss: 2.179591276312387

Epoch: 6| Step: 5
Training loss: 2.719336986541748
Validation loss: 2.162642866052607

Epoch: 6| Step: 6
Training loss: 2.0624492168426514
Validation loss: 2.1653189915482716

Epoch: 6| Step: 7
Training loss: 2.6298880577087402
Validation loss: 2.178765368717973

Epoch: 6| Step: 8
Training loss: 1.7215875387191772
Validation loss: 2.1758247985634753

Epoch: 6| Step: 9
Training loss: 2.9255270957946777
Validation loss: 2.1757008811478973

Epoch: 6| Step: 10
Training loss: 3.0269265174865723
Validation loss: 2.168863534927368

Epoch: 6| Step: 11
Training loss: 2.5334954261779785
Validation loss: 2.1826461412573375

Epoch: 6| Step: 12
Training loss: 2.612786054611206
Validation loss: 2.19390235793206

Epoch: 6| Step: 13
Training loss: 2.660740375518799
Validation loss: 2.202762060267951

Epoch: 230| Step: 0
Training loss: 1.8439226150512695
Validation loss: 2.213547024675595

Epoch: 6| Step: 1
Training loss: 2.625216245651245
Validation loss: 2.2362698585756364

Epoch: 6| Step: 2
Training loss: 2.0530521869659424
Validation loss: 2.2411361843027096

Epoch: 6| Step: 3
Training loss: 2.5461039543151855
Validation loss: 2.238044374732561

Epoch: 6| Step: 4
Training loss: 2.5792789459228516
Validation loss: 2.2605344685175086

Epoch: 6| Step: 5
Training loss: 2.270020008087158
Validation loss: 2.241375671919956

Epoch: 6| Step: 6
Training loss: 2.8825981616973877
Validation loss: 2.214120611067741

Epoch: 6| Step: 7
Training loss: 2.2522008419036865
Validation loss: 2.1976156978196997

Epoch: 6| Step: 8
Training loss: 3.2255311012268066
Validation loss: 2.187663456445099

Epoch: 6| Step: 9
Training loss: 3.0403454303741455
Validation loss: 2.189031095914943

Epoch: 6| Step: 10
Training loss: 2.137319803237915
Validation loss: 2.1717653915446293

Epoch: 6| Step: 11
Training loss: 2.7469112873077393
Validation loss: 2.1740519359547603

Epoch: 6| Step: 12
Training loss: 2.2688074111938477
Validation loss: 2.1740385486233618

Epoch: 6| Step: 13
Training loss: 1.9038289785385132
Validation loss: 2.181674926511703

Epoch: 231| Step: 0
Training loss: 1.8280330896377563
Validation loss: 2.1805736454584266

Epoch: 6| Step: 1
Training loss: 2.1995108127593994
Validation loss: 2.181982819752027

Epoch: 6| Step: 2
Training loss: 2.5824179649353027
Validation loss: 2.1912783653505388

Epoch: 6| Step: 3
Training loss: 2.752577304840088
Validation loss: 2.187348638811419

Epoch: 6| Step: 4
Training loss: 3.1673078536987305
Validation loss: 2.186798131594094

Epoch: 6| Step: 5
Training loss: 1.8478217124938965
Validation loss: 2.178199427102202

Epoch: 6| Step: 6
Training loss: 2.2032899856567383
Validation loss: 2.197487836243004

Epoch: 6| Step: 7
Training loss: 2.887542724609375
Validation loss: 2.1941667910545104

Epoch: 6| Step: 8
Training loss: 2.791891574859619
Validation loss: 2.182854962605302

Epoch: 6| Step: 9
Training loss: 2.367677927017212
Validation loss: 2.190966026757353

Epoch: 6| Step: 10
Training loss: 2.491842031478882
Validation loss: 2.2036523742060505

Epoch: 6| Step: 11
Training loss: 2.772106647491455
Validation loss: 2.2094738406519734

Epoch: 6| Step: 12
Training loss: 2.294893503189087
Validation loss: 2.2302063626627766

Epoch: 6| Step: 13
Training loss: 2.7714641094207764
Validation loss: 2.233698016853743

Epoch: 232| Step: 0
Training loss: 3.2163097858428955
Validation loss: 2.242263481181155

Epoch: 6| Step: 1
Training loss: 1.9127600193023682
Validation loss: 2.2675915687314925

Epoch: 6| Step: 2
Training loss: 2.0555880069732666
Validation loss: 2.290124475315053

Epoch: 6| Step: 3
Training loss: 1.9564785957336426
Validation loss: 2.29815766631916

Epoch: 6| Step: 4
Training loss: 3.4480817317962646
Validation loss: 2.3122934577285603

Epoch: 6| Step: 5
Training loss: 2.852808952331543
Validation loss: 2.2851465555929367

Epoch: 6| Step: 6
Training loss: 2.3666179180145264
Validation loss: 2.2708317900216706

Epoch: 6| Step: 7
Training loss: 2.5751190185546875
Validation loss: 2.2634213611643803

Epoch: 6| Step: 8
Training loss: 3.195740222930908
Validation loss: 2.243915347642796

Epoch: 6| Step: 9
Training loss: 1.7748808860778809
Validation loss: 2.2197877899292977

Epoch: 6| Step: 10
Training loss: 2.0766875743865967
Validation loss: 2.204518538649364

Epoch: 6| Step: 11
Training loss: 2.193063735961914
Validation loss: 2.187663471826943

Epoch: 6| Step: 12
Training loss: 2.329237937927246
Validation loss: 2.166895063974524

Epoch: 6| Step: 13
Training loss: 2.9050233364105225
Validation loss: 2.170870347689557

Epoch: 233| Step: 0
Training loss: 2.92966628074646
Validation loss: 2.158835164962276

Epoch: 6| Step: 1
Training loss: 2.660418748855591
Validation loss: 2.1634005808061167

Epoch: 6| Step: 2
Training loss: 2.7650537490844727
Validation loss: 2.163298897845771

Epoch: 6| Step: 3
Training loss: 2.717020273208618
Validation loss: 2.158285676792104

Epoch: 6| Step: 4
Training loss: 2.0339198112487793
Validation loss: 2.158730427424113

Epoch: 6| Step: 5
Training loss: 3.346035957336426
Validation loss: 2.1635832504559587

Epoch: 6| Step: 6
Training loss: 2.056114435195923
Validation loss: 2.16869475764613

Epoch: 6| Step: 7
Training loss: 2.2648630142211914
Validation loss: 2.166348329154394

Epoch: 6| Step: 8
Training loss: 2.689070701599121
Validation loss: 2.1581343386762883

Epoch: 6| Step: 9
Training loss: 2.173769950866699
Validation loss: 2.1560431193279963

Epoch: 6| Step: 10
Training loss: 2.376789093017578
Validation loss: 2.162131768400951

Epoch: 6| Step: 11
Training loss: 2.936171770095825
Validation loss: 2.166500081298172

Epoch: 6| Step: 12
Training loss: 1.7479650974273682
Validation loss: 2.155747393126129

Epoch: 6| Step: 13
Training loss: 1.4361978769302368
Validation loss: 2.17435900626644

Epoch: 234| Step: 0
Training loss: 2.466799736022949
Validation loss: 2.1739559583766486

Epoch: 6| Step: 1
Training loss: 2.0707712173461914
Validation loss: 2.196723643169608

Epoch: 6| Step: 2
Training loss: 2.4073541164398193
Validation loss: 2.1983541211774273

Epoch: 6| Step: 3
Training loss: 2.5886011123657227
Validation loss: 2.20695528804615

Epoch: 6| Step: 4
Training loss: 2.2179181575775146
Validation loss: 2.2254042574154433

Epoch: 6| Step: 5
Training loss: 2.517835855484009
Validation loss: 2.2476976827908586

Epoch: 6| Step: 6
Training loss: 2.1232736110687256
Validation loss: 2.243085315150599

Epoch: 6| Step: 7
Training loss: 2.6120967864990234
Validation loss: 2.2413166928034958

Epoch: 6| Step: 8
Training loss: 2.3883113861083984
Validation loss: 2.232963167211061

Epoch: 6| Step: 9
Training loss: 2.277475357055664
Validation loss: 2.21632235024565

Epoch: 6| Step: 10
Training loss: 3.315469264984131
Validation loss: 2.208066799307382

Epoch: 6| Step: 11
Training loss: 2.1502394676208496
Validation loss: 2.194154011305942

Epoch: 6| Step: 12
Training loss: 2.466503620147705
Validation loss: 2.1851713721470167

Epoch: 6| Step: 13
Training loss: 3.3004324436187744
Validation loss: 2.1699277149733676

Epoch: 235| Step: 0
Training loss: 1.7520509958267212
Validation loss: 2.1684895523132814

Epoch: 6| Step: 1
Training loss: 2.6410489082336426
Validation loss: 2.167844395483694

Epoch: 6| Step: 2
Training loss: 2.425053358078003
Validation loss: 2.1693150356251705

Epoch: 6| Step: 3
Training loss: 2.8047029972076416
Validation loss: 2.1675725880489556

Epoch: 6| Step: 4
Training loss: 2.6370391845703125
Validation loss: 2.165253657166676

Epoch: 6| Step: 5
Training loss: 2.4316890239715576
Validation loss: 2.1589412689208984

Epoch: 6| Step: 6
Training loss: 2.586637496948242
Validation loss: 2.167618397743471

Epoch: 6| Step: 7
Training loss: 2.5206024646759033
Validation loss: 2.1614108764997093

Epoch: 6| Step: 8
Training loss: 2.799091100692749
Validation loss: 2.157005822786721

Epoch: 6| Step: 9
Training loss: 2.325207233428955
Validation loss: 2.1718928685752292

Epoch: 6| Step: 10
Training loss: 2.2521986961364746
Validation loss: 2.1692450277266966

Epoch: 6| Step: 11
Training loss: 2.5700700283050537
Validation loss: 2.1899785149481987

Epoch: 6| Step: 12
Training loss: 2.332528591156006
Validation loss: 2.1777826919350574

Epoch: 6| Step: 13
Training loss: 2.230634927749634
Validation loss: 2.1938360993580153

Epoch: 236| Step: 0
Training loss: 2.982905149459839
Validation loss: 2.2006896183054936

Epoch: 6| Step: 1
Training loss: 2.7837018966674805
Validation loss: 2.222486485717117

Epoch: 6| Step: 2
Training loss: 2.0929343700408936
Validation loss: 2.2049015286148235

Epoch: 6| Step: 3
Training loss: 2.4350662231445312
Validation loss: 2.2081463670217865

Epoch: 6| Step: 4
Training loss: 2.0202748775482178
Validation loss: 2.1929475568955943

Epoch: 6| Step: 5
Training loss: 2.883056163787842
Validation loss: 2.208868149788149

Epoch: 6| Step: 6
Training loss: 2.5074973106384277
Validation loss: 2.202978262337305

Epoch: 6| Step: 7
Training loss: 2.6323442459106445
Validation loss: 2.1950596429968394

Epoch: 6| Step: 8
Training loss: 2.396709680557251
Validation loss: 2.196291237749079

Epoch: 6| Step: 9
Training loss: 2.668205738067627
Validation loss: 2.182724873224894

Epoch: 6| Step: 10
Training loss: 2.1353394985198975
Validation loss: 2.1793225260191065

Epoch: 6| Step: 11
Training loss: 2.106316089630127
Validation loss: 2.170186908014359

Epoch: 6| Step: 12
Training loss: 2.3367600440979004
Validation loss: 2.182268440082509

Epoch: 6| Step: 13
Training loss: 2.094677686691284
Validation loss: 2.1786022173461093

Epoch: 237| Step: 0
Training loss: 2.0374064445495605
Validation loss: 2.172811891442986

Epoch: 6| Step: 1
Training loss: 2.467607259750366
Validation loss: 2.181293538821641

Epoch: 6| Step: 2
Training loss: 1.9450452327728271
Validation loss: 2.1952401796976724

Epoch: 6| Step: 3
Training loss: 2.395946741104126
Validation loss: 2.210065510965163

Epoch: 6| Step: 4
Training loss: 1.889706015586853
Validation loss: 2.1954095337980535

Epoch: 6| Step: 5
Training loss: 2.872551202774048
Validation loss: 2.2129208913413425

Epoch: 6| Step: 6
Training loss: 3.289547920227051
Validation loss: 2.1881270485539592

Epoch: 6| Step: 7
Training loss: 2.426272392272949
Validation loss: 2.1875895915492887

Epoch: 6| Step: 8
Training loss: 2.0319597721099854
Validation loss: 2.189585262729276

Epoch: 6| Step: 9
Training loss: 2.66314435005188
Validation loss: 2.1753874876165904

Epoch: 6| Step: 10
Training loss: 2.084998607635498
Validation loss: 2.1821304213616157

Epoch: 6| Step: 11
Training loss: 2.8076980113983154
Validation loss: 2.176433447868593

Epoch: 6| Step: 12
Training loss: 2.5739188194274902
Validation loss: 2.175480822081207

Epoch: 6| Step: 13
Training loss: 2.9398765563964844
Validation loss: 2.1821577369525866

Epoch: 238| Step: 0
Training loss: 1.9553364515304565
Validation loss: 2.1762368653410222

Epoch: 6| Step: 1
Training loss: 3.1206252574920654
Validation loss: 2.1758203275742067

Epoch: 6| Step: 2
Training loss: 2.9309754371643066
Validation loss: 2.1760543854005876

Epoch: 6| Step: 3
Training loss: 2.640627861022949
Validation loss: 2.1802736687403854

Epoch: 6| Step: 4
Training loss: 2.7633562088012695
Validation loss: 2.1791491418756466

Epoch: 6| Step: 5
Training loss: 1.7116138935089111
Validation loss: 2.1772296172316357

Epoch: 6| Step: 6
Training loss: 2.4933362007141113
Validation loss: 2.1719801964298373

Epoch: 6| Step: 7
Training loss: 1.8499414920806885
Validation loss: 2.172052284722687

Epoch: 6| Step: 8
Training loss: 2.726630687713623
Validation loss: 2.173159222449026

Epoch: 6| Step: 9
Training loss: 2.306870698928833
Validation loss: 2.169529514928018

Epoch: 6| Step: 10
Training loss: 2.234757900238037
Validation loss: 2.166045242740262

Epoch: 6| Step: 11
Training loss: 2.1998982429504395
Validation loss: 2.174016588477678

Epoch: 6| Step: 12
Training loss: 2.747103452682495
Validation loss: 2.189733148902975

Epoch: 6| Step: 13
Training loss: 2.454235076904297
Validation loss: 2.183317921494925

Epoch: 239| Step: 0
Training loss: 2.820704936981201
Validation loss: 2.2072348517756306

Epoch: 6| Step: 1
Training loss: 2.734830379486084
Validation loss: 2.235128407837242

Epoch: 6| Step: 2
Training loss: 2.8770697116851807
Validation loss: 2.240527440142888

Epoch: 6| Step: 3
Training loss: 2.253723621368408
Validation loss: 2.234332797347858

Epoch: 6| Step: 4
Training loss: 2.7025954723358154
Validation loss: 2.2155238530969106

Epoch: 6| Step: 5
Training loss: 2.138878107070923
Validation loss: 2.2100149367445256

Epoch: 6| Step: 6
Training loss: 2.168191909790039
Validation loss: 2.186507648037326

Epoch: 6| Step: 7
Training loss: 2.196531295776367
Validation loss: 2.1833615315857755

Epoch: 6| Step: 8
Training loss: 3.3331427574157715
Validation loss: 2.1720651708623415

Epoch: 6| Step: 9
Training loss: 1.8624253273010254
Validation loss: 2.1702323831537718

Epoch: 6| Step: 10
Training loss: 2.496809720993042
Validation loss: 2.166880871659966

Epoch: 6| Step: 11
Training loss: 2.021742820739746
Validation loss: 2.166154148758099

Epoch: 6| Step: 12
Training loss: 1.9542109966278076
Validation loss: 2.171944519524933

Epoch: 6| Step: 13
Training loss: 2.738861083984375
Validation loss: 2.1642250783981813

Epoch: 240| Step: 0
Training loss: 2.669851064682007
Validation loss: 2.174667453253141

Epoch: 6| Step: 1
Training loss: 2.44632625579834
Validation loss: 2.1701286146717687

Epoch: 6| Step: 2
Training loss: 1.724321961402893
Validation loss: 2.1852542405487387

Epoch: 6| Step: 3
Training loss: 2.803483486175537
Validation loss: 2.1898794853559105

Epoch: 6| Step: 4
Training loss: 2.928595781326294
Validation loss: 2.192480120607602

Epoch: 6| Step: 5
Training loss: 2.8307318687438965
Validation loss: 2.190599667128696

Epoch: 6| Step: 6
Training loss: 2.899890899658203
Validation loss: 2.177484581547399

Epoch: 6| Step: 7
Training loss: 2.314150810241699
Validation loss: 2.187101189808179

Epoch: 6| Step: 8
Training loss: 2.428128242492676
Validation loss: 2.1838733996114423

Epoch: 6| Step: 9
Training loss: 1.701879620552063
Validation loss: 2.19842856032874

Epoch: 6| Step: 10
Training loss: 2.013155937194824
Validation loss: 2.1852464727176133

Epoch: 6| Step: 11
Training loss: 2.4427895545959473
Validation loss: 2.1799590510706746

Epoch: 6| Step: 12
Training loss: 2.586040735244751
Validation loss: 2.1849028295086277

Epoch: 6| Step: 13
Training loss: 2.2060043811798096
Validation loss: 2.192096174404185

Epoch: 241| Step: 0
Training loss: 2.0368096828460693
Validation loss: 2.2043993614053212

Epoch: 6| Step: 1
Training loss: 2.863297939300537
Validation loss: 2.2197268957732827

Epoch: 6| Step: 2
Training loss: 2.2705435752868652
Validation loss: 2.2301968041286675

Epoch: 6| Step: 3
Training loss: 2.2266347408294678
Validation loss: 2.235752285167735

Epoch: 6| Step: 4
Training loss: 2.071427822113037
Validation loss: 2.2602920378408125

Epoch: 6| Step: 5
Training loss: 2.7022597789764404
Validation loss: 2.3035182773426013

Epoch: 6| Step: 6
Training loss: 2.455108165740967
Validation loss: 2.3053682798980386

Epoch: 6| Step: 7
Training loss: 2.6712331771850586
Validation loss: 2.294866959253947

Epoch: 6| Step: 8
Training loss: 2.06032657623291
Validation loss: 2.279881556828817

Epoch: 6| Step: 9
Training loss: 2.3303146362304688
Validation loss: 2.260393173463883

Epoch: 6| Step: 10
Training loss: 2.2966997623443604
Validation loss: 2.230342344571185

Epoch: 6| Step: 11
Training loss: 2.8340742588043213
Validation loss: 2.227739567397743

Epoch: 6| Step: 12
Training loss: 2.79150390625
Validation loss: 2.2098382967774586

Epoch: 6| Step: 13
Training loss: 2.729018211364746
Validation loss: 2.206302046775818

Epoch: 242| Step: 0
Training loss: 2.9649181365966797
Validation loss: 2.188219275525821

Epoch: 6| Step: 1
Training loss: 2.5546822547912598
Validation loss: 2.1861254553641043

Epoch: 6| Step: 2
Training loss: 2.7487692832946777
Validation loss: 2.167642585692867

Epoch: 6| Step: 3
Training loss: 2.1769814491271973
Validation loss: 2.16044178060306

Epoch: 6| Step: 4
Training loss: 2.082028865814209
Validation loss: 2.156746748955019

Epoch: 6| Step: 5
Training loss: 1.5920131206512451
Validation loss: 2.1735979241709553

Epoch: 6| Step: 6
Training loss: 2.8866872787475586
Validation loss: 2.1940624111442157

Epoch: 6| Step: 7
Training loss: 1.8821446895599365
Validation loss: 2.2092247496369066

Epoch: 6| Step: 8
Training loss: 3.127075672149658
Validation loss: 2.241240373221777

Epoch: 6| Step: 9
Training loss: 2.5861284732818604
Validation loss: 2.2323707175511185

Epoch: 6| Step: 10
Training loss: 2.0681920051574707
Validation loss: 2.2348393983738397

Epoch: 6| Step: 11
Training loss: 3.2654943466186523
Validation loss: 2.218237534646065

Epoch: 6| Step: 12
Training loss: 2.5143842697143555
Validation loss: 2.1744436179437945

Epoch: 6| Step: 13
Training loss: 1.5165163278579712
Validation loss: 2.1616341452444754

Epoch: 243| Step: 0
Training loss: 1.9451688528060913
Validation loss: 2.1393109777922272

Epoch: 6| Step: 1
Training loss: 3.574794292449951
Validation loss: 2.137446916231545

Epoch: 6| Step: 2
Training loss: 1.742722988128662
Validation loss: 2.1474634857587915

Epoch: 6| Step: 3
Training loss: 2.7767014503479004
Validation loss: 2.146309962836645

Epoch: 6| Step: 4
Training loss: 2.363792896270752
Validation loss: 2.1433929217759

Epoch: 6| Step: 5
Training loss: 2.1703169345855713
Validation loss: 2.1437215087234334

Epoch: 6| Step: 6
Training loss: 2.424203395843506
Validation loss: 2.1391937963424192

Epoch: 6| Step: 7
Training loss: 2.4574532508850098
Validation loss: 2.144511862467694

Epoch: 6| Step: 8
Training loss: 1.8848727941513062
Validation loss: 2.1529807864978747

Epoch: 6| Step: 9
Training loss: 2.751389503479004
Validation loss: 2.1598210411687053

Epoch: 6| Step: 10
Training loss: 2.547804117202759
Validation loss: 2.1591731553436606

Epoch: 6| Step: 11
Training loss: 2.7310595512390137
Validation loss: 2.1713286099895353

Epoch: 6| Step: 12
Training loss: 2.0705628395080566
Validation loss: 2.1546679248091993

Epoch: 6| Step: 13
Training loss: 2.9916203022003174
Validation loss: 2.1674141627486034

Epoch: 244| Step: 0
Training loss: 1.942946195602417
Validation loss: 2.1693681029863257

Epoch: 6| Step: 1
Training loss: 1.573803186416626
Validation loss: 2.1754287365944154

Epoch: 6| Step: 2
Training loss: 2.9753599166870117
Validation loss: 2.174620270729065

Epoch: 6| Step: 3
Training loss: 2.204418659210205
Validation loss: 2.1861169363862727

Epoch: 6| Step: 4
Training loss: 3.3413734436035156
Validation loss: 2.171561961532921

Epoch: 6| Step: 5
Training loss: 2.284616470336914
Validation loss: 2.174146967549478

Epoch: 6| Step: 6
Training loss: 2.6103787422180176
Validation loss: 2.1853717052808372

Epoch: 6| Step: 7
Training loss: 2.527420997619629
Validation loss: 2.169058040906024

Epoch: 6| Step: 8
Training loss: 1.756392002105713
Validation loss: 2.1843531900836575

Epoch: 6| Step: 9
Training loss: 3.113903522491455
Validation loss: 2.178663138420351

Epoch: 6| Step: 10
Training loss: 2.482512950897217
Validation loss: 2.1717176386105117

Epoch: 6| Step: 11
Training loss: 2.6829752922058105
Validation loss: 2.1826034463861936

Epoch: 6| Step: 12
Training loss: 2.4366097450256348
Validation loss: 2.1670631349727674

Epoch: 6| Step: 13
Training loss: 1.7102795839309692
Validation loss: 2.169896019402371

Epoch: 245| Step: 0
Training loss: 2.4831767082214355
Validation loss: 2.181376744342107

Epoch: 6| Step: 1
Training loss: 2.1519968509674072
Validation loss: 2.182589791154349

Epoch: 6| Step: 2
Training loss: 2.4445178508758545
Validation loss: 2.192404238126611

Epoch: 6| Step: 3
Training loss: 1.666748285293579
Validation loss: 2.2018220117015224

Epoch: 6| Step: 4
Training loss: 2.3015871047973633
Validation loss: 2.2038044775685957

Epoch: 6| Step: 5
Training loss: 2.110732078552246
Validation loss: 2.2107593115939888

Epoch: 6| Step: 6
Training loss: 2.052687644958496
Validation loss: 2.205728289901569

Epoch: 6| Step: 7
Training loss: 2.111280918121338
Validation loss: 2.1940867170210807

Epoch: 6| Step: 8
Training loss: 2.2437744140625
Validation loss: 2.194423237154561

Epoch: 6| Step: 9
Training loss: 3.2254838943481445
Validation loss: 2.17094870152012

Epoch: 6| Step: 10
Training loss: 2.949763298034668
Validation loss: 2.1635424860062136

Epoch: 6| Step: 11
Training loss: 2.8608131408691406
Validation loss: 2.165083118664321

Epoch: 6| Step: 12
Training loss: 2.26660418510437
Validation loss: 2.1578585383712605

Epoch: 6| Step: 13
Training loss: 3.7614800930023193
Validation loss: 2.1454896567970194

Epoch: 246| Step: 0
Training loss: 2.3618218898773193
Validation loss: 2.1407727503007457

Epoch: 6| Step: 1
Training loss: 2.851318836212158
Validation loss: 2.150448250514205

Epoch: 6| Step: 2
Training loss: 1.5789138078689575
Validation loss: 2.1574737718028407

Epoch: 6| Step: 3
Training loss: 2.6806092262268066
Validation loss: 2.1500731437436995

Epoch: 6| Step: 4
Training loss: 2.294313907623291
Validation loss: 2.156349133419734

Epoch: 6| Step: 5
Training loss: 2.566528797149658
Validation loss: 2.1525950649733185

Epoch: 6| Step: 6
Training loss: 2.628793478012085
Validation loss: 2.172602841931005

Epoch: 6| Step: 7
Training loss: 2.7818803787231445
Validation loss: 2.183193834879065

Epoch: 6| Step: 8
Training loss: 2.2725460529327393
Validation loss: 2.1878526338966946

Epoch: 6| Step: 9
Training loss: 2.584254264831543
Validation loss: 2.2100909653530327

Epoch: 6| Step: 10
Training loss: 2.037937641143799
Validation loss: 2.226134552750536

Epoch: 6| Step: 11
Training loss: 1.936205267906189
Validation loss: 2.2255050431015673

Epoch: 6| Step: 12
Training loss: 2.832840919494629
Validation loss: 2.254776693159534

Epoch: 6| Step: 13
Training loss: 2.8521063327789307
Validation loss: 2.248082689059678

Epoch: 247| Step: 0
Training loss: 2.2445952892303467
Validation loss: 2.2005632718404136

Epoch: 6| Step: 1
Training loss: 2.3485872745513916
Validation loss: 2.175410173272574

Epoch: 6| Step: 2
Training loss: 2.1378889083862305
Validation loss: 2.1600997832513626

Epoch: 6| Step: 3
Training loss: 2.2678048610687256
Validation loss: 2.150458626849677

Epoch: 6| Step: 4
Training loss: 2.5311689376831055
Validation loss: 2.1581356089602233

Epoch: 6| Step: 5
Training loss: 2.4180502891540527
Validation loss: 2.148349376134975

Epoch: 6| Step: 6
Training loss: 2.104592800140381
Validation loss: 2.1640432675679526

Epoch: 6| Step: 7
Training loss: 1.7428648471832275
Validation loss: 2.157389143461822

Epoch: 6| Step: 8
Training loss: 3.020737409591675
Validation loss: 2.155462961043081

Epoch: 6| Step: 9
Training loss: 2.5854294300079346
Validation loss: 2.151173919759771

Epoch: 6| Step: 10
Training loss: 3.0550155639648438
Validation loss: 2.1477957771670435

Epoch: 6| Step: 11
Training loss: 2.377707004547119
Validation loss: 2.1643414074374783

Epoch: 6| Step: 12
Training loss: 2.4290077686309814
Validation loss: 2.157972393497344

Epoch: 6| Step: 13
Training loss: 2.8796629905700684
Validation loss: 2.151906230116403

Epoch: 248| Step: 0
Training loss: 2.2728984355926514
Validation loss: 2.1590370593532437

Epoch: 6| Step: 1
Training loss: 2.978764533996582
Validation loss: 2.163552863623506

Epoch: 6| Step: 2
Training loss: 2.4396870136260986
Validation loss: 2.164114858514519

Epoch: 6| Step: 3
Training loss: 2.1245694160461426
Validation loss: 2.155893213005476

Epoch: 6| Step: 4
Training loss: 3.0559310913085938
Validation loss: 2.1645034872075564

Epoch: 6| Step: 5
Training loss: 2.2586703300476074
Validation loss: 2.16511966336158

Epoch: 6| Step: 6
Training loss: 2.0744690895080566
Validation loss: 2.158406913921397

Epoch: 6| Step: 7
Training loss: 2.5302734375
Validation loss: 2.1653464173757904

Epoch: 6| Step: 8
Training loss: 1.895946979522705
Validation loss: 2.161702520103865

Epoch: 6| Step: 9
Training loss: 2.724074363708496
Validation loss: 2.156739673306865

Epoch: 6| Step: 10
Training loss: 2.9454221725463867
Validation loss: 2.159676688973622

Epoch: 6| Step: 11
Training loss: 2.656778573989868
Validation loss: 2.1719061123427523

Epoch: 6| Step: 12
Training loss: 1.9324084520339966
Validation loss: 2.1707093267030615

Epoch: 6| Step: 13
Training loss: 1.6081223487854004
Validation loss: 2.1557676510144304

Epoch: 249| Step: 0
Training loss: 2.125544786453247
Validation loss: 2.1859118015535417

Epoch: 6| Step: 1
Training loss: 1.8374600410461426
Validation loss: 2.200163528483401

Epoch: 6| Step: 2
Training loss: 2.3918464183807373
Validation loss: 2.2235795861931256

Epoch: 6| Step: 3
Training loss: 2.6082818508148193
Validation loss: 2.2108877833171556

Epoch: 6| Step: 4
Training loss: 2.2417662143707275
Validation loss: 2.2170405567333265

Epoch: 6| Step: 5
Training loss: 2.49920916557312
Validation loss: 2.2218303808601956

Epoch: 6| Step: 6
Training loss: 2.4464192390441895
Validation loss: 2.226761269313033

Epoch: 6| Step: 7
Training loss: 2.752081871032715
Validation loss: 2.2111089101401706

Epoch: 6| Step: 8
Training loss: 2.7260806560516357
Validation loss: 2.205287589821764

Epoch: 6| Step: 9
Training loss: 1.9481924772262573
Validation loss: 2.185106026229038

Epoch: 6| Step: 10
Training loss: 2.536680221557617
Validation loss: 2.1721955319886566

Epoch: 6| Step: 11
Training loss: 2.9295692443847656
Validation loss: 2.157027631677607

Epoch: 6| Step: 12
Training loss: 2.458357810974121
Validation loss: 2.1639973681460143

Epoch: 6| Step: 13
Training loss: 2.667184352874756
Validation loss: 2.161399564435405

Epoch: 250| Step: 0
Training loss: 2.1911532878875732
Validation loss: 2.1521271762027534

Epoch: 6| Step: 1
Training loss: 2.371288299560547
Validation loss: 2.150296761143592

Epoch: 6| Step: 2
Training loss: 2.712524652481079
Validation loss: 2.151210077347294

Epoch: 6| Step: 3
Training loss: 2.5137836933135986
Validation loss: 2.1579223294411936

Epoch: 6| Step: 4
Training loss: 2.9547042846679688
Validation loss: 2.1566360970979095

Epoch: 6| Step: 5
Training loss: 2.1546878814697266
Validation loss: 2.1637509151171614

Epoch: 6| Step: 6
Training loss: 2.663752555847168
Validation loss: 2.155939353409634

Epoch: 6| Step: 7
Training loss: 2.185488224029541
Validation loss: 2.1593737525324666

Epoch: 6| Step: 8
Training loss: 3.238767623901367
Validation loss: 2.1564989192511446

Epoch: 6| Step: 9
Training loss: 1.8973793983459473
Validation loss: 2.158930701594199

Epoch: 6| Step: 10
Training loss: 2.532827854156494
Validation loss: 2.14619549628227

Epoch: 6| Step: 11
Training loss: 2.113939046859741
Validation loss: 2.154960693851594

Epoch: 6| Step: 12
Training loss: 2.534575939178467
Validation loss: 2.1652069912161878

Epoch: 6| Step: 13
Training loss: 1.421328067779541
Validation loss: 2.167270157926826

Testing loss: 2.358452722761366

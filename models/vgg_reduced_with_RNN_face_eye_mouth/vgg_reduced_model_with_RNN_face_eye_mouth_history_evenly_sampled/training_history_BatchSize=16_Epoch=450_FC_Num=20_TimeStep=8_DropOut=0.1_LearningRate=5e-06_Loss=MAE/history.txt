Epoch: 1| Step: 0
Training loss: 3.260093927383423
Validation loss: 5.153105197414275

Epoch: 6| Step: 1
Training loss: 4.924468994140625
Validation loss: 5.14938844660277

Epoch: 6| Step: 2
Training loss: 5.39005184173584
Validation loss: 5.14604514644992

Epoch: 6| Step: 3
Training loss: 5.086737632751465
Validation loss: 5.1425353634742

Epoch: 6| Step: 4
Training loss: 5.895997047424316
Validation loss: 5.139538949535739

Epoch: 6| Step: 5
Training loss: 5.268342018127441
Validation loss: 5.1363064345493115

Epoch: 6| Step: 6
Training loss: 5.632001876831055
Validation loss: 5.13316003225183

Epoch: 6| Step: 7
Training loss: 4.673629283905029
Validation loss: 5.130312996525919

Epoch: 6| Step: 8
Training loss: 4.958029270172119
Validation loss: 5.127136363778063

Epoch: 6| Step: 9
Training loss: 5.293946743011475
Validation loss: 5.124060533379995

Epoch: 6| Step: 10
Training loss: 2.902695655822754
Validation loss: 5.120497447188183

Epoch: 6| Step: 11
Training loss: 5.8846435546875
Validation loss: 5.117324408664499

Epoch: 6| Step: 12
Training loss: 4.950549602508545
Validation loss: 5.113662935072376

Epoch: 6| Step: 13
Training loss: 4.839126110076904
Validation loss: 5.11011448214131

Epoch: 2| Step: 0
Training loss: 5.901352882385254
Validation loss: 5.105726257447274

Epoch: 6| Step: 1
Training loss: 5.421454429626465
Validation loss: 5.101613142157114

Epoch: 6| Step: 2
Training loss: 5.502915382385254
Validation loss: 5.09727341641662

Epoch: 6| Step: 3
Training loss: 4.8063859939575195
Validation loss: 5.09223081732309

Epoch: 6| Step: 4
Training loss: 5.220493793487549
Validation loss: 5.087472372157599

Epoch: 6| Step: 5
Training loss: 4.234415054321289
Validation loss: 5.082899765301776

Epoch: 6| Step: 6
Training loss: 4.6150665283203125
Validation loss: 5.076972141060778

Epoch: 6| Step: 7
Training loss: 5.329828262329102
Validation loss: 5.071200442570512

Epoch: 6| Step: 8
Training loss: 4.437333583831787
Validation loss: 5.065532186979889

Epoch: 6| Step: 9
Training loss: 4.312189102172852
Validation loss: 5.059291249962263

Epoch: 6| Step: 10
Training loss: 4.556789398193359
Validation loss: 5.053037761360087

Epoch: 6| Step: 11
Training loss: 4.6770782470703125
Validation loss: 5.045683609542026

Epoch: 6| Step: 12
Training loss: 5.715709686279297
Validation loss: 5.038544049827001

Epoch: 6| Step: 13
Training loss: 2.3087377548217773
Validation loss: 5.030291300947948

Epoch: 3| Step: 0
Training loss: 4.293127059936523
Validation loss: 5.022415678988221

Epoch: 6| Step: 1
Training loss: 4.744732856750488
Validation loss: 5.012973118853825

Epoch: 6| Step: 2
Training loss: 4.13422155380249
Validation loss: 5.0040064986034105

Epoch: 6| Step: 3
Training loss: 3.7351577281951904
Validation loss: 4.993941830050561

Epoch: 6| Step: 4
Training loss: 5.329305171966553
Validation loss: 4.983758480318131

Epoch: 6| Step: 5
Training loss: 5.169724464416504
Validation loss: 4.9733873900546826

Epoch: 6| Step: 6
Training loss: 4.026160717010498
Validation loss: 4.96142420717465

Epoch: 6| Step: 7
Training loss: 6.504715919494629
Validation loss: 4.949910717625772

Epoch: 6| Step: 8
Training loss: 4.178572654724121
Validation loss: 4.936997095743815

Epoch: 6| Step: 9
Training loss: 4.613367080688477
Validation loss: 4.923636010898057

Epoch: 6| Step: 10
Training loss: 5.5207672119140625
Validation loss: 4.909104429265504

Epoch: 6| Step: 11
Training loss: 4.481362819671631
Validation loss: 4.8950895699121615

Epoch: 6| Step: 12
Training loss: 4.879570484161377
Validation loss: 4.877794475965603

Epoch: 6| Step: 13
Training loss: 4.905691623687744
Validation loss: 4.861861557088872

Epoch: 4| Step: 0
Training loss: 4.919031143188477
Validation loss: 4.844457482778898

Epoch: 6| Step: 1
Training loss: 4.977795600891113
Validation loss: 4.826646512554538

Epoch: 6| Step: 2
Training loss: 4.042758941650391
Validation loss: 4.806004867758802

Epoch: 6| Step: 3
Training loss: 3.9313859939575195
Validation loss: 4.78725911724952

Epoch: 6| Step: 4
Training loss: 5.0936126708984375
Validation loss: 4.764215433469382

Epoch: 6| Step: 5
Training loss: 4.2091217041015625
Validation loss: 4.743538133559689

Epoch: 6| Step: 6
Training loss: 4.694540977478027
Validation loss: 4.719554716540921

Epoch: 6| Step: 7
Training loss: 3.5330066680908203
Validation loss: 4.69832327032602

Epoch: 6| Step: 8
Training loss: 3.857567071914673
Validation loss: 4.674203677843976

Epoch: 6| Step: 9
Training loss: 4.560274600982666
Validation loss: 4.64878874440347

Epoch: 6| Step: 10
Training loss: 5.057126998901367
Validation loss: 4.62255323061379

Epoch: 6| Step: 11
Training loss: 5.722589492797852
Validation loss: 4.597236192354592

Epoch: 6| Step: 12
Training loss: 3.9929375648498535
Validation loss: 4.572209604324833

Epoch: 6| Step: 13
Training loss: 4.20698356628418
Validation loss: 4.54411586894784

Epoch: 5| Step: 0
Training loss: 5.979062080383301
Validation loss: 4.518568541413995

Epoch: 6| Step: 1
Training loss: 3.2707135677337646
Validation loss: 4.49110254164665

Epoch: 6| Step: 2
Training loss: 3.601820468902588
Validation loss: 4.462189069358251

Epoch: 6| Step: 3
Training loss: 3.961904525756836
Validation loss: 4.435570645075972

Epoch: 6| Step: 4
Training loss: 3.587688446044922
Validation loss: 4.408779574978736

Epoch: 6| Step: 5
Training loss: 3.2935726642608643
Validation loss: 4.38258050077705

Epoch: 6| Step: 6
Training loss: 3.1979856491088867
Validation loss: 4.358014827133507

Epoch: 6| Step: 7
Training loss: 4.502079010009766
Validation loss: 4.33256878904117

Epoch: 6| Step: 8
Training loss: 4.589100360870361
Validation loss: 4.308545445883146

Epoch: 6| Step: 9
Training loss: 3.9597373008728027
Validation loss: 4.284135787717758

Epoch: 6| Step: 10
Training loss: 3.9083375930786133
Validation loss: 4.260242841577017

Epoch: 6| Step: 11
Training loss: 6.009154319763184
Validation loss: 4.2388255903797765

Epoch: 6| Step: 12
Training loss: 4.618598937988281
Validation loss: 4.213478852343815

Epoch: 6| Step: 13
Training loss: 3.1366517543792725
Validation loss: 4.193541342212308

Epoch: 6| Step: 0
Training loss: 5.353906631469727
Validation loss: 4.16804116002975

Epoch: 6| Step: 1
Training loss: 5.0897040367126465
Validation loss: 4.146905719593007

Epoch: 6| Step: 2
Training loss: 3.923597812652588
Validation loss: 4.124694713982203

Epoch: 6| Step: 3
Training loss: 3.773577928543091
Validation loss: 4.102929730569163

Epoch: 6| Step: 4
Training loss: 3.936986207962036
Validation loss: 4.082507892321515

Epoch: 6| Step: 5
Training loss: 3.8898203372955322
Validation loss: 4.060249923377909

Epoch: 6| Step: 6
Training loss: 3.3181591033935547
Validation loss: 4.042401908546366

Epoch: 6| Step: 7
Training loss: 3.4232611656188965
Validation loss: 4.022601578825263

Epoch: 6| Step: 8
Training loss: 3.2792108058929443
Validation loss: 4.002957833710537

Epoch: 6| Step: 9
Training loss: 3.16973876953125
Validation loss: 3.985975811558385

Epoch: 6| Step: 10
Training loss: 4.629104137420654
Validation loss: 3.970330528033677

Epoch: 6| Step: 11
Training loss: 3.2065255641937256
Validation loss: 3.952259340593892

Epoch: 6| Step: 12
Training loss: 3.4400010108947754
Validation loss: 3.9349850916093394

Epoch: 6| Step: 13
Training loss: 3.8272628784179688
Validation loss: 3.917002365153323

Epoch: 7| Step: 0
Training loss: 4.346848011016846
Validation loss: 3.898837766339702

Epoch: 6| Step: 1
Training loss: 2.805954933166504
Validation loss: 3.8782889330258934

Epoch: 6| Step: 2
Training loss: 3.7126736640930176
Validation loss: 3.8609772856517504

Epoch: 6| Step: 3
Training loss: 3.0857205390930176
Validation loss: 3.840647384684573

Epoch: 6| Step: 4
Training loss: 3.5171661376953125
Validation loss: 3.8181026981722925

Epoch: 6| Step: 5
Training loss: 2.9200751781463623
Validation loss: 3.8024046190323366

Epoch: 6| Step: 6
Training loss: 3.747380256652832
Validation loss: 3.7830605506896973

Epoch: 6| Step: 7
Training loss: 4.3316264152526855
Validation loss: 3.7660314806046022

Epoch: 6| Step: 8
Training loss: 4.493870735168457
Validation loss: 3.7505583506758495

Epoch: 6| Step: 9
Training loss: 3.773141860961914
Validation loss: 3.734480099011493

Epoch: 6| Step: 10
Training loss: 3.923299551010132
Validation loss: 3.721572942631219

Epoch: 6| Step: 11
Training loss: 3.231243133544922
Validation loss: 3.708337599231351

Epoch: 6| Step: 12
Training loss: 3.280670166015625
Validation loss: 3.6955071162152033

Epoch: 6| Step: 13
Training loss: 4.605001449584961
Validation loss: 3.68676580408568

Epoch: 8| Step: 0
Training loss: 4.5364990234375
Validation loss: 3.674755827073128

Epoch: 6| Step: 1
Training loss: 3.2262933254241943
Validation loss: 3.6656344885467202

Epoch: 6| Step: 2
Training loss: 3.6542611122131348
Validation loss: 3.653175979532221

Epoch: 6| Step: 3
Training loss: 3.555058240890503
Validation loss: 3.6411618494218394

Epoch: 6| Step: 4
Training loss: 2.907532215118408
Validation loss: 3.628739580031364

Epoch: 6| Step: 5
Training loss: 4.205580711364746
Validation loss: 3.6184572891522477

Epoch: 6| Step: 6
Training loss: 3.657947063446045
Validation loss: 3.6030208115936606

Epoch: 6| Step: 7
Training loss: 3.7919204235076904
Validation loss: 3.5928441683451333

Epoch: 6| Step: 8
Training loss: 3.4446463584899902
Validation loss: 3.579080448355726

Epoch: 6| Step: 9
Training loss: 3.701554298400879
Validation loss: 3.5702342653787262

Epoch: 6| Step: 10
Training loss: 2.8529233932495117
Validation loss: 3.5623560515783166

Epoch: 6| Step: 11
Training loss: 2.158539056777954
Validation loss: 3.5511180149611605

Epoch: 6| Step: 12
Training loss: 4.117734909057617
Validation loss: 3.5471035152353267

Epoch: 6| Step: 13
Training loss: 3.55263614654541
Validation loss: 3.535797490868517

Epoch: 9| Step: 0
Training loss: 2.981759548187256
Validation loss: 3.5272604983340026

Epoch: 6| Step: 1
Training loss: 4.817196369171143
Validation loss: 3.516163323515205

Epoch: 6| Step: 2
Training loss: 4.445815563201904
Validation loss: 3.506199621385144

Epoch: 6| Step: 3
Training loss: 3.685953140258789
Validation loss: 3.4943415221347602

Epoch: 6| Step: 4
Training loss: 2.649401903152466
Validation loss: 3.4876076790594284

Epoch: 6| Step: 5
Training loss: 2.930940866470337
Validation loss: 3.478087671341435

Epoch: 6| Step: 6
Training loss: 3.2925193309783936
Validation loss: 3.468038220559397

Epoch: 6| Step: 7
Training loss: 3.130589485168457
Validation loss: 3.4612676200046333

Epoch: 6| Step: 8
Training loss: 4.149409294128418
Validation loss: 3.451075948694701

Epoch: 6| Step: 9
Training loss: 2.839658737182617
Validation loss: 3.4418457297868628

Epoch: 6| Step: 10
Training loss: 3.1225738525390625
Validation loss: 3.4351532792532318

Epoch: 6| Step: 11
Training loss: 3.0038251876831055
Validation loss: 3.4247765079621346

Epoch: 6| Step: 12
Training loss: 4.033761024475098
Validation loss: 3.4171498360172397

Epoch: 6| Step: 13
Training loss: 2.009429931640625
Validation loss: 3.404621813886909

Epoch: 10| Step: 0
Training loss: 3.9275379180908203
Validation loss: 3.394244496540357

Epoch: 6| Step: 1
Training loss: 3.0281219482421875
Validation loss: 3.385823808690553

Epoch: 6| Step: 2
Training loss: 3.6536593437194824
Validation loss: 3.372326784236457

Epoch: 6| Step: 3
Training loss: 2.3809125423431396
Validation loss: 3.3634061915900118

Epoch: 6| Step: 4
Training loss: 3.2802734375
Validation loss: 3.355350696912376

Epoch: 6| Step: 5
Training loss: 2.721383571624756
Validation loss: 3.351291315529936

Epoch: 6| Step: 6
Training loss: 3.3631820678710938
Validation loss: 3.3426378183467413

Epoch: 6| Step: 7
Training loss: 2.839021682739258
Validation loss: 3.3395255047787904

Epoch: 6| Step: 8
Training loss: 2.2999181747436523
Validation loss: 3.3295555883838284

Epoch: 6| Step: 9
Training loss: 3.9990358352661133
Validation loss: 3.3207242770861556

Epoch: 6| Step: 10
Training loss: 2.964892864227295
Validation loss: 3.3100642568321637

Epoch: 6| Step: 11
Training loss: 3.4509122371673584
Validation loss: 3.3027083976294405

Epoch: 6| Step: 12
Training loss: 4.908433437347412
Validation loss: 3.2973938808646253

Epoch: 6| Step: 13
Training loss: 3.5427749156951904
Validation loss: 3.2907606632478776

Epoch: 11| Step: 0
Training loss: 2.8983511924743652
Validation loss: 3.284947838834537

Epoch: 6| Step: 1
Training loss: 3.321187973022461
Validation loss: 3.2784303567742787

Epoch: 6| Step: 2
Training loss: 3.19391131401062
Validation loss: 3.2717458227629304

Epoch: 6| Step: 3
Training loss: 2.6207871437072754
Validation loss: 3.2661370564532537

Epoch: 6| Step: 4
Training loss: 3.6466667652130127
Validation loss: 3.2588308626605618

Epoch: 6| Step: 5
Training loss: 4.2502760887146
Validation loss: 3.251865312617312

Epoch: 6| Step: 6
Training loss: 3.7358179092407227
Validation loss: 3.2447425139847623

Epoch: 6| Step: 7
Training loss: 4.021141529083252
Validation loss: 3.236113655951715

Epoch: 6| Step: 8
Training loss: 2.432943344116211
Validation loss: 3.2312453587849936

Epoch: 6| Step: 9
Training loss: 2.8273298740386963
Validation loss: 3.2247415434929634

Epoch: 6| Step: 10
Training loss: 2.928213119506836
Validation loss: 3.2191596646462717

Epoch: 6| Step: 11
Training loss: 4.203036785125732
Validation loss: 3.215341209083475

Epoch: 6| Step: 12
Training loss: 2.506953239440918
Validation loss: 3.2079109145749

Epoch: 6| Step: 13
Training loss: 2.1443090438842773
Validation loss: 3.20171106502574

Epoch: 12| Step: 0
Training loss: 2.190112352371216
Validation loss: 3.1993990687913794

Epoch: 6| Step: 1
Training loss: 2.9704055786132812
Validation loss: 3.202039049517724

Epoch: 6| Step: 2
Training loss: 2.9875402450561523
Validation loss: 3.1985812110285603

Epoch: 6| Step: 3
Training loss: 2.6344709396362305
Validation loss: 3.1861063357322448

Epoch: 6| Step: 4
Training loss: 3.5700411796569824
Validation loss: 3.179925654524116

Epoch: 6| Step: 5
Training loss: 3.1790666580200195
Validation loss: 3.178237094674059

Epoch: 6| Step: 6
Training loss: 4.288342475891113
Validation loss: 3.177259168317241

Epoch: 6| Step: 7
Training loss: 2.6965932846069336
Validation loss: 3.1743641104749454

Epoch: 6| Step: 8
Training loss: 3.3383164405822754
Validation loss: 3.1700786800794702

Epoch: 6| Step: 9
Training loss: 2.9323532581329346
Validation loss: 3.1649907276194584

Epoch: 6| Step: 10
Training loss: 4.167874813079834
Validation loss: 3.1611263572528796

Epoch: 6| Step: 11
Training loss: 4.044766426086426
Validation loss: 3.158991531659198

Epoch: 6| Step: 12
Training loss: 2.0437304973602295
Validation loss: 3.1564425550481325

Epoch: 6| Step: 13
Training loss: 3.6014997959136963
Validation loss: 3.150735250083349

Epoch: 13| Step: 0
Training loss: 2.8402633666992188
Validation loss: 3.1485822585321244

Epoch: 6| Step: 1
Training loss: 3.004549264907837
Validation loss: 3.1431365397668656

Epoch: 6| Step: 2
Training loss: 3.5164852142333984
Validation loss: 3.1407101692691928

Epoch: 6| Step: 3
Training loss: 3.295064687728882
Validation loss: 3.138478745696365

Epoch: 6| Step: 4
Training loss: 2.3803277015686035
Validation loss: 3.132994205720963

Epoch: 6| Step: 5
Training loss: 2.278189182281494
Validation loss: 3.1295130611747823

Epoch: 6| Step: 6
Training loss: 2.7462263107299805
Validation loss: 3.123386513802313

Epoch: 6| Step: 7
Training loss: 3.898559808731079
Validation loss: 3.1230007961232173

Epoch: 6| Step: 8
Training loss: 3.1667661666870117
Validation loss: 3.1163553960861696

Epoch: 6| Step: 9
Training loss: 2.6033616065979004
Validation loss: 3.116086582983694

Epoch: 6| Step: 10
Training loss: 3.202615976333618
Validation loss: 3.1112070006708943

Epoch: 6| Step: 11
Training loss: 4.447645664215088
Validation loss: 3.113465916725897

Epoch: 6| Step: 12
Training loss: 3.244886636734009
Validation loss: 3.110023501098797

Epoch: 6| Step: 13
Training loss: 3.50518536567688
Validation loss: 3.1040928902164584

Epoch: 14| Step: 0
Training loss: 3.7213149070739746
Validation loss: 3.10027527552779

Epoch: 6| Step: 1
Training loss: 4.073785305023193
Validation loss: 3.098307696721887

Epoch: 6| Step: 2
Training loss: 2.751373529434204
Validation loss: 3.0939973810667634

Epoch: 6| Step: 3
Training loss: 3.050137519836426
Validation loss: 3.09325837063533

Epoch: 6| Step: 4
Training loss: 3.5122275352478027
Validation loss: 3.0941184233593684

Epoch: 6| Step: 5
Training loss: 3.172415256500244
Validation loss: 3.0888699382864018

Epoch: 6| Step: 6
Training loss: 2.843096971511841
Validation loss: 3.085770712103895

Epoch: 6| Step: 7
Training loss: 3.289210319519043
Validation loss: 3.0825959174863753

Epoch: 6| Step: 8
Training loss: 3.5055956840515137
Validation loss: 3.0806231626900296

Epoch: 6| Step: 9
Training loss: 1.9882855415344238
Validation loss: 3.0774582996163318

Epoch: 6| Step: 10
Training loss: 3.1079916954040527
Validation loss: 3.0762243347783245

Epoch: 6| Step: 11
Training loss: 3.386826276779175
Validation loss: 3.0731179688566472

Epoch: 6| Step: 12
Training loss: 2.4118194580078125
Validation loss: 3.0717881161679506

Epoch: 6| Step: 13
Training loss: 2.623204231262207
Validation loss: 3.0699493654312624

Epoch: 15| Step: 0
Training loss: 3.1455349922180176
Validation loss: 3.066750831501458

Epoch: 6| Step: 1
Training loss: 3.2978675365448
Validation loss: 3.0649099375611994

Epoch: 6| Step: 2
Training loss: 3.2668120861053467
Validation loss: 3.0649311542510986

Epoch: 6| Step: 3
Training loss: 3.0394089221954346
Validation loss: 3.0630684386017504

Epoch: 6| Step: 4
Training loss: 3.796426773071289
Validation loss: 3.0584389830148346

Epoch: 6| Step: 5
Training loss: 1.2634860277175903
Validation loss: 3.0546071119205926

Epoch: 6| Step: 6
Training loss: 1.779293179512024
Validation loss: 3.053800598267586

Epoch: 6| Step: 7
Training loss: 3.4924933910369873
Validation loss: 3.0527039650947816

Epoch: 6| Step: 8
Training loss: 3.918503761291504
Validation loss: 3.052686737429711

Epoch: 6| Step: 9
Training loss: 3.025244951248169
Validation loss: 3.0522118178747033

Epoch: 6| Step: 10
Training loss: 3.799522876739502
Validation loss: 3.0492338647124586

Epoch: 6| Step: 11
Training loss: 2.743511199951172
Validation loss: 3.0492058671930784

Epoch: 6| Step: 12
Training loss: 3.5612242221832275
Validation loss: 3.0458258223789993

Epoch: 6| Step: 13
Training loss: 3.3488214015960693
Validation loss: 3.044904742189633

Epoch: 16| Step: 0
Training loss: 2.583791732788086
Validation loss: 3.041734495470601

Epoch: 6| Step: 1
Training loss: 2.9869771003723145
Validation loss: 3.037356292047808

Epoch: 6| Step: 2
Training loss: 3.336336612701416
Validation loss: 3.0351645100501274

Epoch: 6| Step: 3
Training loss: 3.0081863403320312
Validation loss: 3.030608800149733

Epoch: 6| Step: 4
Training loss: 3.1602160930633545
Validation loss: 3.0285731874486452

Epoch: 6| Step: 5
Training loss: 2.9783406257629395
Validation loss: 3.0251128237734557

Epoch: 6| Step: 6
Training loss: 3.4747366905212402
Validation loss: 3.021609837009061

Epoch: 6| Step: 7
Training loss: 2.513289213180542
Validation loss: 3.019197146097819

Epoch: 6| Step: 8
Training loss: 2.655083179473877
Validation loss: 3.017523668145621

Epoch: 6| Step: 9
Training loss: 3.032799243927002
Validation loss: 3.0167847756416566

Epoch: 6| Step: 10
Training loss: 3.6767635345458984
Validation loss: 3.0139930966079875

Epoch: 6| Step: 11
Training loss: 3.3959426879882812
Validation loss: 3.0122397330499466

Epoch: 6| Step: 12
Training loss: 2.495048761367798
Validation loss: 3.0071879791957077

Epoch: 6| Step: 13
Training loss: 4.336417198181152
Validation loss: 3.0186124488871586

Epoch: 17| Step: 0
Training loss: 3.5769639015197754
Validation loss: 3.006555441887148

Epoch: 6| Step: 1
Training loss: 3.1560068130493164
Validation loss: 3.004012769268405

Epoch: 6| Step: 2
Training loss: 3.4960737228393555
Validation loss: 3.0057841013836604

Epoch: 6| Step: 3
Training loss: 3.19608736038208
Validation loss: 3.004084066678119

Epoch: 6| Step: 4
Training loss: 3.5119469165802
Validation loss: 3.004780666802519

Epoch: 6| Step: 5
Training loss: 3.355219602584839
Validation loss: 3.0013573836254817

Epoch: 6| Step: 6
Training loss: 3.082655668258667
Validation loss: 2.9959028485000774

Epoch: 6| Step: 7
Training loss: 2.9233055114746094
Validation loss: 2.9953732772540023

Epoch: 6| Step: 8
Training loss: 2.7452194690704346
Validation loss: 2.9951345536016647

Epoch: 6| Step: 9
Training loss: 2.7233362197875977
Validation loss: 2.9913572957438808

Epoch: 6| Step: 10
Training loss: 2.7387237548828125
Validation loss: 2.994486188375822

Epoch: 6| Step: 11
Training loss: 2.062530040740967
Validation loss: 2.9989601847946004

Epoch: 6| Step: 12
Training loss: 3.3016765117645264
Validation loss: 3.0034591100549184

Epoch: 6| Step: 13
Training loss: 2.9755728244781494
Validation loss: 3.0088386510008123

Epoch: 18| Step: 0
Training loss: 3.7083072662353516
Validation loss: 2.9873962145979687

Epoch: 6| Step: 1
Training loss: 3.251312255859375
Validation loss: 2.9856405309451524

Epoch: 6| Step: 2
Training loss: 3.4076128005981445
Validation loss: 2.986837053811678

Epoch: 6| Step: 3
Training loss: 2.4590210914611816
Validation loss: 2.9945088227589927

Epoch: 6| Step: 4
Training loss: 3.505025625228882
Validation loss: 2.990194220696726

Epoch: 6| Step: 5
Training loss: 2.639155387878418
Validation loss: 2.9882056379830964

Epoch: 6| Step: 6
Training loss: 2.911642551422119
Validation loss: 2.984409022074874

Epoch: 6| Step: 7
Training loss: 3.6543688774108887
Validation loss: 2.9782061064115135

Epoch: 6| Step: 8
Training loss: 3.0494189262390137
Validation loss: 2.979866435450892

Epoch: 6| Step: 9
Training loss: 3.079392910003662
Validation loss: 2.981742238485685

Epoch: 6| Step: 10
Training loss: 2.7776010036468506
Validation loss: 2.987161695316274

Epoch: 6| Step: 11
Training loss: 3.262718677520752
Validation loss: 2.9852139770343737

Epoch: 6| Step: 12
Training loss: 2.757809638977051
Validation loss: 2.9838178080897175

Epoch: 6| Step: 13
Training loss: 1.809600830078125
Validation loss: 2.9779706872919554

Epoch: 19| Step: 0
Training loss: 3.046661138534546
Validation loss: 2.979865453576529

Epoch: 6| Step: 1
Training loss: 2.7138547897338867
Validation loss: 2.970486661439301

Epoch: 6| Step: 2
Training loss: 4.379447937011719
Validation loss: 2.9694795916157384

Epoch: 6| Step: 3
Training loss: 2.7346553802490234
Validation loss: 2.965562464088522

Epoch: 6| Step: 4
Training loss: 2.904888391494751
Validation loss: 2.9638641111312376

Epoch: 6| Step: 5
Training loss: 3.0937459468841553
Validation loss: 2.9641822435522593

Epoch: 6| Step: 6
Training loss: 3.2635626792907715
Validation loss: 2.9649401787788636

Epoch: 6| Step: 7
Training loss: 2.771365165710449
Validation loss: 2.961413747520857

Epoch: 6| Step: 8
Training loss: 2.6410951614379883
Validation loss: 2.968104047159995

Epoch: 6| Step: 9
Training loss: 3.2358505725860596
Validation loss: 2.9785726762587026

Epoch: 6| Step: 10
Training loss: 3.1387388706207275
Validation loss: 2.9884944192824827

Epoch: 6| Step: 11
Training loss: 2.9285695552825928
Validation loss: 2.9771232528071248

Epoch: 6| Step: 12
Training loss: 2.861016273498535
Validation loss: 2.9641565968913417

Epoch: 6| Step: 13
Training loss: 2.8676540851593018
Validation loss: 2.96042872500676

Epoch: 20| Step: 0
Training loss: 3.4045357704162598
Validation loss: 2.954691817683558

Epoch: 6| Step: 1
Training loss: 2.511331081390381
Validation loss: 2.9544462080924743

Epoch: 6| Step: 2
Training loss: 2.5701825618743896
Validation loss: 2.954659249192925

Epoch: 6| Step: 3
Training loss: 2.159862995147705
Validation loss: 2.9588444284213486

Epoch: 6| Step: 4
Training loss: 3.4689383506774902
Validation loss: 2.9613566167892946

Epoch: 6| Step: 5
Training loss: 3.570094585418701
Validation loss: 2.961644939197007

Epoch: 6| Step: 6
Training loss: 4.135228157043457
Validation loss: 2.9630180789578344

Epoch: 6| Step: 7
Training loss: 2.744582176208496
Validation loss: 2.955492352926603

Epoch: 6| Step: 8
Training loss: 2.7363247871398926
Validation loss: 2.952364654951198

Epoch: 6| Step: 9
Training loss: 3.0359294414520264
Validation loss: 2.9464598137845277

Epoch: 6| Step: 10
Training loss: 3.3045549392700195
Validation loss: 2.9450825157985894

Epoch: 6| Step: 11
Training loss: 2.755596160888672
Validation loss: 2.941475719533941

Epoch: 6| Step: 12
Training loss: 3.84749174118042
Validation loss: 2.9407143849198536

Epoch: 6| Step: 13
Training loss: 1.6120989322662354
Validation loss: 2.939277284888811

Epoch: 21| Step: 0
Training loss: 2.7782950401306152
Validation loss: 2.9381725634298017

Epoch: 6| Step: 1
Training loss: 2.9148168563842773
Validation loss: 2.9421317423543623

Epoch: 6| Step: 2
Training loss: 3.0336904525756836
Validation loss: 2.9394671173505884

Epoch: 6| Step: 3
Training loss: 2.806849241256714
Validation loss: 2.9382501263772287

Epoch: 6| Step: 4
Training loss: 3.24588680267334
Validation loss: 2.932393879018804

Epoch: 6| Step: 5
Training loss: 3.774207830429077
Validation loss: 2.9298501424891974

Epoch: 6| Step: 6
Training loss: 2.3663458824157715
Validation loss: 2.929952311259444

Epoch: 6| Step: 7
Training loss: 2.0409679412841797
Validation loss: 2.9271392899174846

Epoch: 6| Step: 8
Training loss: 2.9846692085266113
Validation loss: 2.9276966228280017

Epoch: 6| Step: 9
Training loss: 3.9717791080474854
Validation loss: 2.9281093946067234

Epoch: 6| Step: 10
Training loss: 2.6325457096099854
Validation loss: 2.926319278696532

Epoch: 6| Step: 11
Training loss: 2.8736021518707275
Validation loss: 2.923956714650636

Epoch: 6| Step: 12
Training loss: 3.641686201095581
Validation loss: 2.9252977217397382

Epoch: 6| Step: 13
Training loss: 3.404761552810669
Validation loss: 2.9203975200653076

Epoch: 22| Step: 0
Training loss: 3.3969273567199707
Validation loss: 2.9190292102034374

Epoch: 6| Step: 1
Training loss: 3.0779690742492676
Validation loss: 2.918649596552695

Epoch: 6| Step: 2
Training loss: 2.428946018218994
Validation loss: 2.915244507533248

Epoch: 6| Step: 3
Training loss: 2.379272937774658
Validation loss: 2.911815087000529

Epoch: 6| Step: 4
Training loss: 2.7824313640594482
Validation loss: 2.910286936708676

Epoch: 6| Step: 5
Training loss: 2.4198145866394043
Validation loss: 2.906535635712326

Epoch: 6| Step: 6
Training loss: 3.1762163639068604
Validation loss: 2.9052201906840005

Epoch: 6| Step: 7
Training loss: 3.1476845741271973
Validation loss: 2.9026934587827293

Epoch: 6| Step: 8
Training loss: 4.191479682922363
Validation loss: 2.9012939391597623

Epoch: 6| Step: 9
Training loss: 2.3796634674072266
Validation loss: 2.894998788833618

Epoch: 6| Step: 10
Training loss: 3.143249988555908
Validation loss: 2.8955250247832267

Epoch: 6| Step: 11
Training loss: 3.519232988357544
Validation loss: 2.8964558980798207

Epoch: 6| Step: 12
Training loss: 2.662548065185547
Validation loss: 2.891463400215231

Epoch: 6| Step: 13
Training loss: 3.4801025390625
Validation loss: 2.890129466210642

Epoch: 23| Step: 0
Training loss: 2.976081371307373
Validation loss: 2.889301089830296

Epoch: 6| Step: 1
Training loss: 2.9959468841552734
Validation loss: 2.8871881064548286

Epoch: 6| Step: 2
Training loss: 3.040283203125
Validation loss: 2.8873386844511955

Epoch: 6| Step: 3
Training loss: 2.788707733154297
Validation loss: 2.8864467605467765

Epoch: 6| Step: 4
Training loss: 3.4751553535461426
Validation loss: 2.8854321279833393

Epoch: 6| Step: 5
Training loss: 2.091768741607666
Validation loss: 2.8851413214078514

Epoch: 6| Step: 6
Training loss: 3.5802102088928223
Validation loss: 2.8817309615432576

Epoch: 6| Step: 7
Training loss: 2.617527484893799
Validation loss: 2.878989670866279

Epoch: 6| Step: 8
Training loss: 2.339445114135742
Validation loss: 2.8757679539342083

Epoch: 6| Step: 9
Training loss: 3.0883045196533203
Validation loss: 2.8763773505405714

Epoch: 6| Step: 10
Training loss: 2.709416151046753
Validation loss: 2.874738718873711

Epoch: 6| Step: 11
Training loss: 2.7849950790405273
Validation loss: 2.8733268245573966

Epoch: 6| Step: 12
Training loss: 4.108181953430176
Validation loss: 2.874480370552309

Epoch: 6| Step: 13
Training loss: 3.3133604526519775
Validation loss: 2.8704717338726087

Epoch: 24| Step: 0
Training loss: 2.26478910446167
Validation loss: 2.870694145079582

Epoch: 6| Step: 1
Training loss: 3.0668649673461914
Validation loss: 2.8666501532318773

Epoch: 6| Step: 2
Training loss: 3.4240145683288574
Validation loss: 2.8678548592393116

Epoch: 6| Step: 3
Training loss: 3.1572721004486084
Validation loss: 2.866973848753078

Epoch: 6| Step: 4
Training loss: 2.8533272743225098
Validation loss: 2.8663339230322067

Epoch: 6| Step: 5
Training loss: 3.44246244430542
Validation loss: 2.8643977565150105

Epoch: 6| Step: 6
Training loss: 3.052790403366089
Validation loss: 2.864266126386581

Epoch: 6| Step: 7
Training loss: 3.157874584197998
Validation loss: 2.862635104886947

Epoch: 6| Step: 8
Training loss: 2.7105324268341064
Validation loss: 2.864061778591525

Epoch: 6| Step: 9
Training loss: 2.7581725120544434
Validation loss: 2.8695272348260366

Epoch: 6| Step: 10
Training loss: 2.68411922454834
Validation loss: 2.8701609462820072

Epoch: 6| Step: 11
Training loss: 2.782714366912842
Validation loss: 2.8746781708091818

Epoch: 6| Step: 12
Training loss: 3.6135404109954834
Validation loss: 2.8710420182956162

Epoch: 6| Step: 13
Training loss: 2.3305623531341553
Validation loss: 2.8611015350587907

Epoch: 25| Step: 0
Training loss: 3.354757785797119
Validation loss: 2.853361342542915

Epoch: 6| Step: 1
Training loss: 2.504523277282715
Validation loss: 2.8569415051450013

Epoch: 6| Step: 2
Training loss: 2.372483253479004
Validation loss: 2.856458528067476

Epoch: 6| Step: 3
Training loss: 3.0577330589294434
Validation loss: 2.8605295535056823

Epoch: 6| Step: 4
Training loss: 2.9049928188323975
Validation loss: 2.8672840492699736

Epoch: 6| Step: 5
Training loss: 3.2220277786254883
Validation loss: 2.8715678902082544

Epoch: 6| Step: 6
Training loss: 3.546116352081299
Validation loss: 2.864520190864481

Epoch: 6| Step: 7
Training loss: 2.1379776000976562
Validation loss: 2.860409805851598

Epoch: 6| Step: 8
Training loss: 4.215628623962402
Validation loss: 2.8565970954074653

Epoch: 6| Step: 9
Training loss: 2.5811493396759033
Validation loss: 2.8532957646154586

Epoch: 6| Step: 10
Training loss: 2.5060839653015137
Validation loss: 2.8515520018915974

Epoch: 6| Step: 11
Training loss: 3.1591458320617676
Validation loss: 2.847829008615145

Epoch: 6| Step: 12
Training loss: 3.4417879581451416
Validation loss: 2.847535276925692

Epoch: 6| Step: 13
Training loss: 2.2263097763061523
Validation loss: 2.8465485034450406

Epoch: 26| Step: 0
Training loss: 3.204758644104004
Validation loss: 2.8451832725155737

Epoch: 6| Step: 1
Training loss: 3.36837100982666
Validation loss: 2.8455859512411137

Epoch: 6| Step: 2
Training loss: 2.5730347633361816
Validation loss: 2.8457601762587026

Epoch: 6| Step: 3
Training loss: 3.2934296131134033
Validation loss: 2.8417404800333004

Epoch: 6| Step: 4
Training loss: 2.6132893562316895
Validation loss: 2.840858280017812

Epoch: 6| Step: 5
Training loss: 2.0959715843200684
Validation loss: 2.8446732413384224

Epoch: 6| Step: 6
Training loss: 3.6297154426574707
Validation loss: 2.8414793809254966

Epoch: 6| Step: 7
Training loss: 3.957670211791992
Validation loss: 2.84025590906861

Epoch: 6| Step: 8
Training loss: 2.748696804046631
Validation loss: 2.8393212595293598

Epoch: 6| Step: 9
Training loss: 2.81360125541687
Validation loss: 2.8369614719062723

Epoch: 6| Step: 10
Training loss: 2.566101551055908
Validation loss: 2.8364329735438027

Epoch: 6| Step: 11
Training loss: 3.0997910499572754
Validation loss: 2.8381347784432034

Epoch: 6| Step: 12
Training loss: 2.567471981048584
Validation loss: 2.8346731867841495

Epoch: 6| Step: 13
Training loss: 2.7981395721435547
Validation loss: 2.835019234688051

Epoch: 27| Step: 0
Training loss: 2.781679630279541
Validation loss: 2.834921672780027

Epoch: 6| Step: 1
Training loss: 2.9066390991210938
Validation loss: 2.835922215574531

Epoch: 6| Step: 2
Training loss: 2.6978750228881836
Validation loss: 2.8321639927484656

Epoch: 6| Step: 3
Training loss: 3.3625125885009766
Validation loss: 2.8331148137328444

Epoch: 6| Step: 4
Training loss: 3.0827975273132324
Validation loss: 2.8310270386357463

Epoch: 6| Step: 5
Training loss: 2.9583582878112793
Validation loss: 2.832629801124655

Epoch: 6| Step: 6
Training loss: 3.166956901550293
Validation loss: 2.832114019701558

Epoch: 6| Step: 7
Training loss: 2.297698974609375
Validation loss: 2.8288643052501063

Epoch: 6| Step: 8
Training loss: 2.502837896347046
Validation loss: 2.8297728594913276

Epoch: 6| Step: 9
Training loss: 2.8503780364990234
Validation loss: 2.827773899160406

Epoch: 6| Step: 10
Training loss: 3.3529577255249023
Validation loss: 2.830842843619726

Epoch: 6| Step: 11
Training loss: 2.7131600379943848
Validation loss: 2.8287239177252657

Epoch: 6| Step: 12
Training loss: 3.768354892730713
Validation loss: 2.8282440349619877

Epoch: 6| Step: 13
Training loss: 2.846316337585449
Validation loss: 2.8263379630222114

Epoch: 28| Step: 0
Training loss: 1.6904456615447998
Validation loss: 2.8287344363427933

Epoch: 6| Step: 1
Training loss: 3.5474209785461426
Validation loss: 2.826147961360152

Epoch: 6| Step: 2
Training loss: 2.2989370822906494
Validation loss: 2.82499954008287

Epoch: 6| Step: 3
Training loss: 3.1686618328094482
Validation loss: 2.8255073485835904

Epoch: 6| Step: 4
Training loss: 3.230464458465576
Validation loss: 2.8276933957171697

Epoch: 6| Step: 5
Training loss: 3.342052936553955
Validation loss: 2.8270908735131703

Epoch: 6| Step: 6
Training loss: 2.3348042964935303
Validation loss: 2.8222146239331973

Epoch: 6| Step: 7
Training loss: 3.486839771270752
Validation loss: 2.827921818661433

Epoch: 6| Step: 8
Training loss: 2.076321601867676
Validation loss: 2.8259705087190032

Epoch: 6| Step: 9
Training loss: 3.041952133178711
Validation loss: 2.8214786796159643

Epoch: 6| Step: 10
Training loss: 3.361137866973877
Validation loss: 2.823304522422052

Epoch: 6| Step: 11
Training loss: 3.882662773132324
Validation loss: 2.8242279227061937

Epoch: 6| Step: 12
Training loss: 2.902413845062256
Validation loss: 2.825697568155104

Epoch: 6| Step: 13
Training loss: 2.882946252822876
Validation loss: 2.8263164156226703

Epoch: 29| Step: 0
Training loss: 2.795060873031616
Validation loss: 2.820139959294309

Epoch: 6| Step: 1
Training loss: 3.2614986896514893
Validation loss: 2.8198082985416537

Epoch: 6| Step: 2
Training loss: 2.7602014541625977
Validation loss: 2.8216976786172516

Epoch: 6| Step: 3
Training loss: 3.4380064010620117
Validation loss: 2.819739108444542

Epoch: 6| Step: 4
Training loss: 1.7406635284423828
Validation loss: 2.818773323489774

Epoch: 6| Step: 5
Training loss: 2.4364895820617676
Validation loss: 2.8164069062920025

Epoch: 6| Step: 6
Training loss: 3.0617408752441406
Validation loss: 2.815370700692618

Epoch: 6| Step: 7
Training loss: 3.8267810344696045
Validation loss: 2.819531725298974

Epoch: 6| Step: 8
Training loss: 2.5444064140319824
Validation loss: 2.816369287429317

Epoch: 6| Step: 9
Training loss: 3.8064494132995605
Validation loss: 2.8149667914195726

Epoch: 6| Step: 10
Training loss: 2.72278094291687
Validation loss: 2.8132763883118987

Epoch: 6| Step: 11
Training loss: 3.011199474334717
Validation loss: 2.8119246677685807

Epoch: 6| Step: 12
Training loss: 3.02150559425354
Validation loss: 2.809651554271739

Epoch: 6| Step: 13
Training loss: 2.6027116775512695
Validation loss: 2.812766041806949

Epoch: 30| Step: 0
Training loss: 1.917715311050415
Validation loss: 2.808473346053913

Epoch: 6| Step: 1
Training loss: 2.7588491439819336
Validation loss: 2.8161489937895086

Epoch: 6| Step: 2
Training loss: 2.9389026165008545
Validation loss: 2.8144366484816357

Epoch: 6| Step: 3
Training loss: 2.395833730697632
Validation loss: 2.8085429796608548

Epoch: 6| Step: 4
Training loss: 2.4382596015930176
Validation loss: 2.8093644367751254

Epoch: 6| Step: 5
Training loss: 3.616114377975464
Validation loss: 2.80899239355518

Epoch: 6| Step: 6
Training loss: 3.0669057369232178
Validation loss: 2.809077206478324

Epoch: 6| Step: 7
Training loss: 2.9682164192199707
Validation loss: 2.807692366261636

Epoch: 6| Step: 8
Training loss: 3.307996988296509
Validation loss: 2.8045265110590125

Epoch: 6| Step: 9
Training loss: 3.324570894241333
Validation loss: 2.802682271567724

Epoch: 6| Step: 10
Training loss: 3.7934939861297607
Validation loss: 2.803847805146248

Epoch: 6| Step: 11
Training loss: 1.889188289642334
Validation loss: 2.8038261064919094

Epoch: 6| Step: 12
Training loss: 3.348599433898926
Validation loss: 2.8075202075383996

Epoch: 6| Step: 13
Training loss: 3.5833065509796143
Validation loss: 2.8042549112791657

Epoch: 31| Step: 0
Training loss: 2.645735740661621
Validation loss: 2.8041741078899753

Epoch: 6| Step: 1
Training loss: 3.160618782043457
Validation loss: 2.801106660596786

Epoch: 6| Step: 2
Training loss: 3.348008155822754
Validation loss: 2.8007367605804117

Epoch: 6| Step: 3
Training loss: 3.245919704437256
Validation loss: 2.802481820506434

Epoch: 6| Step: 4
Training loss: 3.0528106689453125
Validation loss: 2.806106895528814

Epoch: 6| Step: 5
Training loss: 3.4018754959106445
Validation loss: 2.8079442952268865

Epoch: 6| Step: 6
Training loss: 1.8166040182113647
Validation loss: 2.8135090617723364

Epoch: 6| Step: 7
Training loss: 2.7809338569641113
Validation loss: 2.8329943482593825

Epoch: 6| Step: 8
Training loss: 2.6743664741516113
Validation loss: 2.818090510624711

Epoch: 6| Step: 9
Training loss: 2.1950910091400146
Validation loss: 2.8012022536288024

Epoch: 6| Step: 10
Training loss: 2.5520827770233154
Validation loss: 2.8031301472776677

Epoch: 6| Step: 11
Training loss: 4.398581504821777
Validation loss: 2.7993294961990847

Epoch: 6| Step: 12
Training loss: 2.7660250663757324
Validation loss: 2.8000514840566986

Epoch: 6| Step: 13
Training loss: 3.153986930847168
Validation loss: 2.804437727056524

Epoch: 32| Step: 0
Training loss: 3.374267339706421
Validation loss: 2.8009423184138473

Epoch: 6| Step: 1
Training loss: 3.0133485794067383
Validation loss: 2.7974148873359925

Epoch: 6| Step: 2
Training loss: 3.539443016052246
Validation loss: 2.797308844904746

Epoch: 6| Step: 3
Training loss: 2.4923949241638184
Validation loss: 2.7959125785417456

Epoch: 6| Step: 4
Training loss: 2.944573402404785
Validation loss: 2.7948516030465402

Epoch: 6| Step: 5
Training loss: 3.622439384460449
Validation loss: 2.7915903214485414

Epoch: 6| Step: 6
Training loss: 2.8137624263763428
Validation loss: 2.7911357930911485

Epoch: 6| Step: 7
Training loss: 2.64082670211792
Validation loss: 2.791827606898482

Epoch: 6| Step: 8
Training loss: 2.5017995834350586
Validation loss: 2.7924871906157462

Epoch: 6| Step: 9
Training loss: 3.177765369415283
Validation loss: 2.793537142456219

Epoch: 6| Step: 10
Training loss: 2.641002655029297
Validation loss: 2.791615081089799

Epoch: 6| Step: 11
Training loss: 2.457942247390747
Validation loss: 2.7917994504333823

Epoch: 6| Step: 12
Training loss: 2.925722360610962
Validation loss: 2.791070097236223

Epoch: 6| Step: 13
Training loss: 2.8208460807800293
Validation loss: 2.7918749317046134

Epoch: 33| Step: 0
Training loss: 2.7612464427948
Validation loss: 2.7891150289966213

Epoch: 6| Step: 1
Training loss: 2.713165760040283
Validation loss: 2.788882650354857

Epoch: 6| Step: 2
Training loss: 2.726778507232666
Validation loss: 2.7911839844078146

Epoch: 6| Step: 3
Training loss: 2.891263961791992
Validation loss: 2.791376559965072

Epoch: 6| Step: 4
Training loss: 2.4351072311401367
Validation loss: 2.7931937299748903

Epoch: 6| Step: 5
Training loss: 2.79410982131958
Validation loss: 2.791823676837388

Epoch: 6| Step: 6
Training loss: 3.146379232406616
Validation loss: 2.7891729570204213

Epoch: 6| Step: 7
Training loss: 3.437160015106201
Validation loss: 2.787717914068571

Epoch: 6| Step: 8
Training loss: 3.287611246109009
Validation loss: 2.792649410104239

Epoch: 6| Step: 9
Training loss: 3.829723358154297
Validation loss: 2.7894035564955844

Epoch: 6| Step: 10
Training loss: 3.0963239669799805
Validation loss: 2.7870273718269925

Epoch: 6| Step: 11
Training loss: 2.5094199180603027
Validation loss: 2.788522252472498

Epoch: 6| Step: 12
Training loss: 2.1197054386138916
Validation loss: 2.790879987901257

Epoch: 6| Step: 13
Training loss: 3.468036651611328
Validation loss: 2.7872226520251204

Epoch: 34| Step: 0
Training loss: 2.250516891479492
Validation loss: 2.7889200692535727

Epoch: 6| Step: 1
Training loss: 3.4243698120117188
Validation loss: 2.787377203664472

Epoch: 6| Step: 2
Training loss: 3.0608034133911133
Validation loss: 2.787956060901765

Epoch: 6| Step: 3
Training loss: 2.5211644172668457
Validation loss: 2.7832541824668966

Epoch: 6| Step: 4
Training loss: 3.3623836040496826
Validation loss: 2.78369080123081

Epoch: 6| Step: 5
Training loss: 2.5104141235351562
Validation loss: 2.7839817359883297

Epoch: 6| Step: 6
Training loss: 2.5913209915161133
Validation loss: 2.779395821273968

Epoch: 6| Step: 7
Training loss: 2.825939416885376
Validation loss: 2.7805494057234896

Epoch: 6| Step: 8
Training loss: 3.4290943145751953
Validation loss: 2.7796888659077306

Epoch: 6| Step: 9
Training loss: 3.3873887062072754
Validation loss: 2.779184094039343

Epoch: 6| Step: 10
Training loss: 3.457143545150757
Validation loss: 2.7826488812764487

Epoch: 6| Step: 11
Training loss: 2.5504536628723145
Validation loss: 2.782098757323398

Epoch: 6| Step: 12
Training loss: 2.586963176727295
Validation loss: 2.7762084135445217

Epoch: 6| Step: 13
Training loss: 2.9620141983032227
Validation loss: 2.7859373195196993

Epoch: 35| Step: 0
Training loss: 2.7929553985595703
Validation loss: 2.7869652676325973

Epoch: 6| Step: 1
Training loss: 2.3350930213928223
Validation loss: 2.7823454359526276

Epoch: 6| Step: 2
Training loss: 3.3115391731262207
Validation loss: 2.78353089158253

Epoch: 6| Step: 3
Training loss: 2.3794100284576416
Validation loss: 2.7834435842370473

Epoch: 6| Step: 4
Training loss: 3.4705615043640137
Validation loss: 2.7776277321641163

Epoch: 6| Step: 5
Training loss: 2.6058902740478516
Validation loss: 2.7790888201805855

Epoch: 6| Step: 6
Training loss: 3.644205331802368
Validation loss: 2.7776333721735145

Epoch: 6| Step: 7
Training loss: 2.4644415378570557
Validation loss: 2.778372308259369

Epoch: 6| Step: 8
Training loss: 2.2039971351623535
Validation loss: 2.776520311191518

Epoch: 6| Step: 9
Training loss: 2.8607118129730225
Validation loss: 2.7792216500928326

Epoch: 6| Step: 10
Training loss: 3.3930366039276123
Validation loss: 2.77884018036627

Epoch: 6| Step: 11
Training loss: 3.4957950115203857
Validation loss: 2.7780608413039998

Epoch: 6| Step: 12
Training loss: 3.371243476867676
Validation loss: 2.7778333489612868

Epoch: 6| Step: 13
Training loss: 2.244223117828369
Validation loss: 2.7819281188390588

Epoch: 36| Step: 0
Training loss: 4.116877555847168
Validation loss: 2.7781000419329573

Epoch: 6| Step: 1
Training loss: 3.1120686531066895
Validation loss: 2.7805732655268844

Epoch: 6| Step: 2
Training loss: 3.3041234016418457
Validation loss: 2.7783756025375856

Epoch: 6| Step: 3
Training loss: 2.587880849838257
Validation loss: 2.778110252913608

Epoch: 6| Step: 4
Training loss: 2.6474990844726562
Validation loss: 2.780951715284778

Epoch: 6| Step: 5
Training loss: 2.765773296356201
Validation loss: 2.7776301060953448

Epoch: 6| Step: 6
Training loss: 2.3446123600006104
Validation loss: 2.778886318206787

Epoch: 6| Step: 7
Training loss: 2.6415505409240723
Validation loss: 2.780343896599226

Epoch: 6| Step: 8
Training loss: 2.9045984745025635
Validation loss: 2.781308627897693

Epoch: 6| Step: 9
Training loss: 3.4397506713867188
Validation loss: 2.777774756954562

Epoch: 6| Step: 10
Training loss: 2.226484775543213
Validation loss: 2.777984324321952

Epoch: 6| Step: 11
Training loss: 3.3906233310699463
Validation loss: 2.7758496858740367

Epoch: 6| Step: 12
Training loss: 2.868328094482422
Validation loss: 2.774416861995574

Epoch: 6| Step: 13
Training loss: 2.1314077377319336
Validation loss: 2.7737642975263697

Epoch: 37| Step: 0
Training loss: 2.3356785774230957
Validation loss: 2.773950602418633

Epoch: 6| Step: 1
Training loss: 2.4207332134246826
Validation loss: 2.777528655144476

Epoch: 6| Step: 2
Training loss: 2.838843584060669
Validation loss: 2.7797253208775676

Epoch: 6| Step: 3
Training loss: 2.8269128799438477
Validation loss: 2.7795310122992403

Epoch: 6| Step: 4
Training loss: 1.7869778871536255
Validation loss: 2.7781551884066675

Epoch: 6| Step: 5
Training loss: 3.2758073806762695
Validation loss: 2.780059401706983

Epoch: 6| Step: 6
Training loss: 3.0913829803466797
Validation loss: 2.7867433153172976

Epoch: 6| Step: 7
Training loss: 2.878279209136963
Validation loss: 2.7842629212205128

Epoch: 6| Step: 8
Training loss: 2.814042329788208
Validation loss: 2.7827341223275788

Epoch: 6| Step: 9
Training loss: 3.7557356357574463
Validation loss: 2.782546902215609

Epoch: 6| Step: 10
Training loss: 2.9864065647125244
Validation loss: 2.7749101936176257

Epoch: 6| Step: 11
Training loss: 3.1360456943511963
Validation loss: 2.7774542159931634

Epoch: 6| Step: 12
Training loss: 3.770944118499756
Validation loss: 2.773578320780108

Epoch: 6| Step: 13
Training loss: 2.9043450355529785
Validation loss: 2.774327847265428

Epoch: 38| Step: 0
Training loss: 2.089867115020752
Validation loss: 2.773920187386133

Epoch: 6| Step: 1
Training loss: 2.697788953781128
Validation loss: 2.7726697870480117

Epoch: 6| Step: 2
Training loss: 3.2270870208740234
Validation loss: 2.772193388272357

Epoch: 6| Step: 3
Training loss: 2.999880313873291
Validation loss: 2.770611186181345

Epoch: 6| Step: 4
Training loss: 2.922027587890625
Validation loss: 2.771138566796498

Epoch: 6| Step: 5
Training loss: 3.283010721206665
Validation loss: 2.7715905251041537

Epoch: 6| Step: 6
Training loss: 3.5899200439453125
Validation loss: 2.773637435769522

Epoch: 6| Step: 7
Training loss: 2.2799577713012695
Validation loss: 2.7687810108225834

Epoch: 6| Step: 8
Training loss: 3.3149094581604004
Validation loss: 2.7666070307454755

Epoch: 6| Step: 9
Training loss: 2.5039737224578857
Validation loss: 2.7669885030356784

Epoch: 6| Step: 10
Training loss: 2.2709641456604004
Validation loss: 2.7663690326034382

Epoch: 6| Step: 11
Training loss: 3.1008691787719727
Validation loss: 2.764522708872313

Epoch: 6| Step: 12
Training loss: 3.4730052947998047
Validation loss: 2.7684290460360947

Epoch: 6| Step: 13
Training loss: 3.1162497997283936
Validation loss: 2.768597223425424

Epoch: 39| Step: 0
Training loss: 2.1969804763793945
Validation loss: 2.7690714866884294

Epoch: 6| Step: 1
Training loss: 2.404297351837158
Validation loss: 2.7665177570876254

Epoch: 6| Step: 2
Training loss: 2.9563679695129395
Validation loss: 2.7656412457907074

Epoch: 6| Step: 3
Training loss: 3.626293182373047
Validation loss: 2.766620195040139

Epoch: 6| Step: 4
Training loss: 2.9796524047851562
Validation loss: 2.769071840470837

Epoch: 6| Step: 5
Training loss: 2.382291555404663
Validation loss: 2.773225907356508

Epoch: 6| Step: 6
Training loss: 3.6464476585388184
Validation loss: 2.7687191130012594

Epoch: 6| Step: 7
Training loss: 2.537990093231201
Validation loss: 2.7700028316949004

Epoch: 6| Step: 8
Training loss: 2.7583041191101074
Validation loss: 2.7656431428847776

Epoch: 6| Step: 9
Training loss: 2.931344509124756
Validation loss: 2.763865816977716

Epoch: 6| Step: 10
Training loss: 3.328902006149292
Validation loss: 2.762914055137224

Epoch: 6| Step: 11
Training loss: 2.7655434608459473
Validation loss: 2.763191674345283

Epoch: 6| Step: 12
Training loss: 3.005984306335449
Validation loss: 2.7627688095133793

Epoch: 6| Step: 13
Training loss: 3.3860342502593994
Validation loss: 2.7637640276262836

Epoch: 40| Step: 0
Training loss: 2.7076311111450195
Validation loss: 2.7623741960012786

Epoch: 6| Step: 1
Training loss: 2.398186206817627
Validation loss: 2.7642845928028064

Epoch: 6| Step: 2
Training loss: 3.52160906791687
Validation loss: 2.7636754461514053

Epoch: 6| Step: 3
Training loss: 3.436371326446533
Validation loss: 2.7633726161013366

Epoch: 6| Step: 4
Training loss: 1.964468240737915
Validation loss: 2.7641718541422198

Epoch: 6| Step: 5
Training loss: 2.8997552394866943
Validation loss: 2.761550711047265

Epoch: 6| Step: 6
Training loss: 2.62835693359375
Validation loss: 2.761212387392598

Epoch: 6| Step: 7
Training loss: 3.825094223022461
Validation loss: 2.7638297132266465

Epoch: 6| Step: 8
Training loss: 1.9774482250213623
Validation loss: 2.7625047468370005

Epoch: 6| Step: 9
Training loss: 3.072540283203125
Validation loss: 2.766330506211968

Epoch: 6| Step: 10
Training loss: 2.826472282409668
Validation loss: 2.7678882357894734

Epoch: 6| Step: 11
Training loss: 3.0641160011291504
Validation loss: 2.7708542962228098

Epoch: 6| Step: 12
Training loss: 3.3233375549316406
Validation loss: 2.7722396389130624

Epoch: 6| Step: 13
Training loss: 3.086280345916748
Validation loss: 2.789642341675297

Epoch: 41| Step: 0
Training loss: 2.645352840423584
Validation loss: 2.779346707046673

Epoch: 6| Step: 1
Training loss: 2.6923797130584717
Validation loss: 2.784324143522529

Epoch: 6| Step: 2
Training loss: 3.0437469482421875
Validation loss: 2.789783298328359

Epoch: 6| Step: 3
Training loss: 2.2610762119293213
Validation loss: 2.784365421982222

Epoch: 6| Step: 4
Training loss: 3.4891505241394043
Validation loss: 2.7684957160744617

Epoch: 6| Step: 5
Training loss: 2.3049893379211426
Validation loss: 2.764107532398675

Epoch: 6| Step: 6
Training loss: 3.466708183288574
Validation loss: 2.760964855071037

Epoch: 6| Step: 7
Training loss: 2.6209418773651123
Validation loss: 2.7599089043114775

Epoch: 6| Step: 8
Training loss: 3.5513391494750977
Validation loss: 2.76394054197496

Epoch: 6| Step: 9
Training loss: 3.272322177886963
Validation loss: 2.763714700616816

Epoch: 6| Step: 10
Training loss: 3.3885912895202637
Validation loss: 2.7646819596649497

Epoch: 6| Step: 11
Training loss: 2.1374268531799316
Validation loss: 2.770934525356498

Epoch: 6| Step: 12
Training loss: 3.038884401321411
Validation loss: 2.772507708559754

Epoch: 6| Step: 13
Training loss: 2.788606882095337
Validation loss: 2.7682189069768435

Epoch: 42| Step: 0
Training loss: 3.281883716583252
Validation loss: 2.7760924011148433

Epoch: 6| Step: 1
Training loss: 3.496162176132202
Validation loss: 2.78231458510122

Epoch: 6| Step: 2
Training loss: 3.0215401649475098
Validation loss: 2.7680202761004047

Epoch: 6| Step: 3
Training loss: 3.1589455604553223
Validation loss: 2.7617879529153146

Epoch: 6| Step: 4
Training loss: 2.6871023178100586
Validation loss: 2.757637490508377

Epoch: 6| Step: 5
Training loss: 3.593524217605591
Validation loss: 2.7581794082477527

Epoch: 6| Step: 6
Training loss: 3.1668975353240967
Validation loss: 2.757294426682175

Epoch: 6| Step: 7
Training loss: 2.2066550254821777
Validation loss: 2.759662884537892

Epoch: 6| Step: 8
Training loss: 2.790576457977295
Validation loss: 2.7640139851518857

Epoch: 6| Step: 9
Training loss: 2.009631633758545
Validation loss: 2.759815477555798

Epoch: 6| Step: 10
Training loss: 2.474130153656006
Validation loss: 2.763496357907531

Epoch: 6| Step: 11
Training loss: 2.5039191246032715
Validation loss: 2.7631505458585677

Epoch: 6| Step: 12
Training loss: 3.4714455604553223
Validation loss: 2.7599620998546643

Epoch: 6| Step: 13
Training loss: 2.6674821376800537
Validation loss: 2.7618361596138246

Epoch: 43| Step: 0
Training loss: 2.8184823989868164
Validation loss: 2.757800045833793

Epoch: 6| Step: 1
Training loss: 3.403576135635376
Validation loss: 2.761059581592519

Epoch: 6| Step: 2
Training loss: 2.063495635986328
Validation loss: 2.7725608323210027

Epoch: 6| Step: 3
Training loss: 3.1931934356689453
Validation loss: 2.7755806138438563

Epoch: 6| Step: 4
Training loss: 2.785536050796509
Validation loss: 2.781270355306646

Epoch: 6| Step: 5
Training loss: 2.7472546100616455
Validation loss: 2.771967649459839

Epoch: 6| Step: 6
Training loss: 2.441990375518799
Validation loss: 2.7674669988693728

Epoch: 6| Step: 7
Training loss: 2.3301868438720703
Validation loss: 2.7631955403153614

Epoch: 6| Step: 8
Training loss: 2.694690227508545
Validation loss: 2.75773956442392

Epoch: 6| Step: 9
Training loss: 3.481565237045288
Validation loss: 2.758075596183859

Epoch: 6| Step: 10
Training loss: 2.973407745361328
Validation loss: 2.753574712302095

Epoch: 6| Step: 11
Training loss: 3.126202344894409
Validation loss: 2.7557395171093684

Epoch: 6| Step: 12
Training loss: 2.7756526470184326
Validation loss: 2.751646564852807

Epoch: 6| Step: 13
Training loss: 4.4924139976501465
Validation loss: 2.7531937732491443

Epoch: 44| Step: 0
Training loss: 2.7224619388580322
Validation loss: 2.7546508286588933

Epoch: 6| Step: 1
Training loss: 3.4821746349334717
Validation loss: 2.7584206032496628

Epoch: 6| Step: 2
Training loss: 2.3603639602661133
Validation loss: 2.7663910465855754

Epoch: 6| Step: 3
Training loss: 4.074270248413086
Validation loss: 2.7606075758575113

Epoch: 6| Step: 4
Training loss: 2.9949350357055664
Validation loss: 2.7645351476566766

Epoch: 6| Step: 5
Training loss: 2.781432867050171
Validation loss: 2.764280747341853

Epoch: 6| Step: 6
Training loss: 2.1999669075012207
Validation loss: 2.766511473604428

Epoch: 6| Step: 7
Training loss: 3.477260112762451
Validation loss: 2.7632548501414638

Epoch: 6| Step: 8
Training loss: 2.7041239738464355
Validation loss: 2.7626632362283687

Epoch: 6| Step: 9
Training loss: 2.7217724323272705
Validation loss: 2.757344333074426

Epoch: 6| Step: 10
Training loss: 3.2031259536743164
Validation loss: 2.754018327241303

Epoch: 6| Step: 11
Training loss: 3.4667305946350098
Validation loss: 2.752417154209588

Epoch: 6| Step: 12
Training loss: 2.074650526046753
Validation loss: 2.7521782075205157

Epoch: 6| Step: 13
Training loss: 1.9482759237289429
Validation loss: 2.74709778447305

Epoch: 45| Step: 0
Training loss: 2.899782419204712
Validation loss: 2.7481804637498755

Epoch: 6| Step: 1
Training loss: 3.0775978565216064
Validation loss: 2.751889703094318

Epoch: 6| Step: 2
Training loss: 2.6360859870910645
Validation loss: 2.7529098295396373

Epoch: 6| Step: 3
Training loss: 2.8981947898864746
Validation loss: 2.7538217011318413

Epoch: 6| Step: 4
Training loss: 2.3736839294433594
Validation loss: 2.7538954032364713

Epoch: 6| Step: 5
Training loss: 3.525726556777954
Validation loss: 2.750573886338101

Epoch: 6| Step: 6
Training loss: 3.212296724319458
Validation loss: 2.7534723897134104

Epoch: 6| Step: 7
Training loss: 3.3031764030456543
Validation loss: 2.7511579067476335

Epoch: 6| Step: 8
Training loss: 2.349307060241699
Validation loss: 2.7507990867860856

Epoch: 6| Step: 9
Training loss: 2.886195659637451
Validation loss: 2.7462045428573445

Epoch: 6| Step: 10
Training loss: 2.5847835540771484
Validation loss: 2.7433395821561097

Epoch: 6| Step: 11
Training loss: 3.072349786758423
Validation loss: 2.746921080414967

Epoch: 6| Step: 12
Training loss: 3.5950417518615723
Validation loss: 2.744717836380005

Epoch: 6| Step: 13
Training loss: 1.453347086906433
Validation loss: 2.7474716863324566

Epoch: 46| Step: 0
Training loss: 3.863953113555908
Validation loss: 2.745624772963985

Epoch: 6| Step: 1
Training loss: 3.4268970489501953
Validation loss: 2.744513875694685

Epoch: 6| Step: 2
Training loss: 1.8749854564666748
Validation loss: 2.743269233293431

Epoch: 6| Step: 3
Training loss: 2.323539972305298
Validation loss: 2.74291923481931

Epoch: 6| Step: 4
Training loss: 3.45881986618042
Validation loss: 2.744541411758751

Epoch: 6| Step: 5
Training loss: 2.8586716651916504
Validation loss: 2.741688523241269

Epoch: 6| Step: 6
Training loss: 2.1308579444885254
Validation loss: 2.744607989506055

Epoch: 6| Step: 7
Training loss: 2.652650833129883
Validation loss: 2.746146261051137

Epoch: 6| Step: 8
Training loss: 3.2896063327789307
Validation loss: 2.7433182167750534

Epoch: 6| Step: 9
Training loss: 2.573586940765381
Validation loss: 2.7463436203618206

Epoch: 6| Step: 10
Training loss: 3.3281571865081787
Validation loss: 2.7452674783686155

Epoch: 6| Step: 11
Training loss: 2.6728484630584717
Validation loss: 2.7457744588134108

Epoch: 6| Step: 12
Training loss: 3.0119359493255615
Validation loss: 2.7441711887236564

Epoch: 6| Step: 13
Training loss: 2.9978625774383545
Validation loss: 2.7470398436310473

Epoch: 47| Step: 0
Training loss: 2.563459873199463
Validation loss: 2.7417732284915064

Epoch: 6| Step: 1
Training loss: 2.720602035522461
Validation loss: 2.7481452726548716

Epoch: 6| Step: 2
Training loss: 2.1551449298858643
Validation loss: 2.7410129116427515

Epoch: 6| Step: 3
Training loss: 3.5716392993927
Validation loss: 2.7411793816474175

Epoch: 6| Step: 4
Training loss: 3.105255126953125
Validation loss: 2.7424789705584125

Epoch: 6| Step: 5
Training loss: 2.325228214263916
Validation loss: 2.7409516944680163

Epoch: 6| Step: 6
Training loss: 3.2632627487182617
Validation loss: 2.741126968014625

Epoch: 6| Step: 7
Training loss: 3.246255397796631
Validation loss: 2.738312844307192

Epoch: 6| Step: 8
Training loss: 2.8199872970581055
Validation loss: 2.743363836760162

Epoch: 6| Step: 9
Training loss: 2.4077837467193604
Validation loss: 2.7392758861664803

Epoch: 6| Step: 10
Training loss: 2.930558681488037
Validation loss: 2.7383896202169438

Epoch: 6| Step: 11
Training loss: 3.1977646350860596
Validation loss: 2.739350685509302

Epoch: 6| Step: 12
Training loss: 3.106828451156616
Validation loss: 2.740000045427712

Epoch: 6| Step: 13
Training loss: 2.988504648208618
Validation loss: 2.739424067158853

Epoch: 48| Step: 0
Training loss: 3.2506704330444336
Validation loss: 2.738980234310191

Epoch: 6| Step: 1
Training loss: 2.9028635025024414
Validation loss: 2.739956819882957

Epoch: 6| Step: 2
Training loss: 3.147245407104492
Validation loss: 2.740504350713504

Epoch: 6| Step: 3
Training loss: 2.575963020324707
Validation loss: 2.7412608208194857

Epoch: 6| Step: 4
Training loss: 3.1223526000976562
Validation loss: 2.7391106697820846

Epoch: 6| Step: 5
Training loss: 2.1055281162261963
Validation loss: 2.741583070447368

Epoch: 6| Step: 6
Training loss: 2.4950127601623535
Validation loss: 2.74366517220774

Epoch: 6| Step: 7
Training loss: 2.8851540088653564
Validation loss: 2.7421707696812128

Epoch: 6| Step: 8
Training loss: 2.8014469146728516
Validation loss: 2.7452532911813385

Epoch: 6| Step: 9
Training loss: 3.2491393089294434
Validation loss: 2.747908728097075

Epoch: 6| Step: 10
Training loss: 2.683964967727661
Validation loss: 2.7524078610122844

Epoch: 6| Step: 11
Training loss: 3.1074302196502686
Validation loss: 2.7437654003020255

Epoch: 6| Step: 12
Training loss: 3.115633249282837
Validation loss: 2.749865508848621

Epoch: 6| Step: 13
Training loss: 2.83950138092041
Validation loss: 2.744921117700556

Epoch: 49| Step: 0
Training loss: 2.6485941410064697
Validation loss: 2.7428110030389603

Epoch: 6| Step: 1
Training loss: 2.341142416000366
Validation loss: 2.7388099419173373

Epoch: 6| Step: 2
Training loss: 2.7730002403259277
Validation loss: 2.7355863586548836

Epoch: 6| Step: 3
Training loss: 2.6403799057006836
Validation loss: 2.7357775370279946

Epoch: 6| Step: 4
Training loss: 2.7131826877593994
Validation loss: 2.73923300414957

Epoch: 6| Step: 5
Training loss: 2.023362636566162
Validation loss: 2.7373083637606714

Epoch: 6| Step: 6
Training loss: 2.676480293273926
Validation loss: 2.735769279541508

Epoch: 6| Step: 7
Training loss: 2.884786605834961
Validation loss: 2.742884361615745

Epoch: 6| Step: 8
Training loss: 2.7058660984039307
Validation loss: 2.7409867676355506

Epoch: 6| Step: 9
Training loss: 3.6455655097961426
Validation loss: 2.746444776494016

Epoch: 6| Step: 10
Training loss: 3.390932083129883
Validation loss: 2.746914020148657

Epoch: 6| Step: 11
Training loss: 3.6336545944213867
Validation loss: 2.739043420360934

Epoch: 6| Step: 12
Training loss: 2.8457565307617188
Validation loss: 2.7407686864176104

Epoch: 6| Step: 13
Training loss: 3.846729278564453
Validation loss: 2.7389341297970025

Epoch: 50| Step: 0
Training loss: 2.3902196884155273
Validation loss: 2.7348333430546585

Epoch: 6| Step: 1
Training loss: 3.971689224243164
Validation loss: 2.7328392228772564

Epoch: 6| Step: 2
Training loss: 2.884957790374756
Validation loss: 2.7300175184844644

Epoch: 6| Step: 3
Training loss: 1.9676196575164795
Validation loss: 2.735356471871817

Epoch: 6| Step: 4
Training loss: 2.0853235721588135
Validation loss: 2.7296833710003923

Epoch: 6| Step: 5
Training loss: 2.896233558654785
Validation loss: 2.7315345553941626

Epoch: 6| Step: 6
Training loss: 2.161907434463501
Validation loss: 2.7319457607884563

Epoch: 6| Step: 7
Training loss: 3.7302794456481934
Validation loss: 2.7305732773196314

Epoch: 6| Step: 8
Training loss: 2.614349603652954
Validation loss: 2.7355349576601418

Epoch: 6| Step: 9
Training loss: 3.019181251525879
Validation loss: 2.7322277433128765

Epoch: 6| Step: 10
Training loss: 3.240426540374756
Validation loss: 2.728809525889735

Epoch: 6| Step: 11
Training loss: 2.90916109085083
Validation loss: 2.7335095892670336

Epoch: 6| Step: 12
Training loss: 3.624375343322754
Validation loss: 2.732355576689525

Epoch: 6| Step: 13
Training loss: 2.7231645584106445
Validation loss: 2.729102611541748

Epoch: 51| Step: 0
Training loss: 2.7490181922912598
Validation loss: 2.730702861662834

Epoch: 6| Step: 1
Training loss: 2.625361442565918
Validation loss: 2.7271898202998663

Epoch: 6| Step: 2
Training loss: 2.7419486045837402
Validation loss: 2.731020191664337

Epoch: 6| Step: 3
Training loss: 2.0806961059570312
Validation loss: 2.726798024228824

Epoch: 6| Step: 4
Training loss: 2.7369627952575684
Validation loss: 2.732107147093742

Epoch: 6| Step: 5
Training loss: 2.515948534011841
Validation loss: 2.730815138868106

Epoch: 6| Step: 6
Training loss: 3.0398783683776855
Validation loss: 2.7292865296845794

Epoch: 6| Step: 7
Training loss: 3.2395148277282715
Validation loss: 2.733510673687022

Epoch: 6| Step: 8
Training loss: 3.5686421394348145
Validation loss: 2.7327613369111092

Epoch: 6| Step: 9
Training loss: 2.9332871437072754
Validation loss: 2.7294101253632577

Epoch: 6| Step: 10
Training loss: 2.843944549560547
Validation loss: 2.7356605401603122

Epoch: 6| Step: 11
Training loss: 3.304140567779541
Validation loss: 2.7375805147232546

Epoch: 6| Step: 12
Training loss: 2.8842241764068604
Validation loss: 2.737932192381992

Epoch: 6| Step: 13
Training loss: 3.0881733894348145
Validation loss: 2.7309663475200696

Epoch: 52| Step: 0
Training loss: 1.9185618162155151
Validation loss: 2.7265803788297918

Epoch: 6| Step: 1
Training loss: 3.9395313262939453
Validation loss: 2.728767348874

Epoch: 6| Step: 2
Training loss: 2.380398988723755
Validation loss: 2.729767858341176

Epoch: 6| Step: 3
Training loss: 2.694819450378418
Validation loss: 2.729129901496313

Epoch: 6| Step: 4
Training loss: 3.397509813308716
Validation loss: 2.725385086510771

Epoch: 6| Step: 5
Training loss: 2.253807306289673
Validation loss: 2.7245682618951284

Epoch: 6| Step: 6
Training loss: 3.2838332653045654
Validation loss: 2.7270232118586057

Epoch: 6| Step: 7
Training loss: 2.9875245094299316
Validation loss: 2.724406857644358

Epoch: 6| Step: 8
Training loss: 3.27376127243042
Validation loss: 2.722320064421623

Epoch: 6| Step: 9
Training loss: 2.488799810409546
Validation loss: 2.7236153105253815

Epoch: 6| Step: 10
Training loss: 3.419297695159912
Validation loss: 2.727796777602165

Epoch: 6| Step: 11
Training loss: 2.946854829788208
Validation loss: 2.7254641671334543

Epoch: 6| Step: 12
Training loss: 2.4757771492004395
Validation loss: 2.723306866102321

Epoch: 6| Step: 13
Training loss: 2.6128149032592773
Validation loss: 2.7235434439874466

Epoch: 53| Step: 0
Training loss: 2.9177041053771973
Validation loss: 2.7252795311712448

Epoch: 6| Step: 1
Training loss: 3.2604072093963623
Validation loss: 2.724317337877007

Epoch: 6| Step: 2
Training loss: 2.8048593997955322
Validation loss: 2.7260772053913405

Epoch: 6| Step: 3
Training loss: 2.532038688659668
Validation loss: 2.725615850058935

Epoch: 6| Step: 4
Training loss: 3.068009376525879
Validation loss: 2.723018851331485

Epoch: 6| Step: 5
Training loss: 2.7471964359283447
Validation loss: 2.7272259137963735

Epoch: 6| Step: 6
Training loss: 2.3720178604125977
Validation loss: 2.7258844631974415

Epoch: 6| Step: 7
Training loss: 2.6898367404937744
Validation loss: 2.728622136577483

Epoch: 6| Step: 8
Training loss: 3.2931571006774902
Validation loss: 2.7321781804484706

Epoch: 6| Step: 9
Training loss: 2.6530909538269043
Validation loss: 2.7273781376500286

Epoch: 6| Step: 10
Training loss: 2.4538896083831787
Validation loss: 2.725479918141519

Epoch: 6| Step: 11
Training loss: 3.402390241622925
Validation loss: 2.7202031663669053

Epoch: 6| Step: 12
Training loss: 3.3484997749328613
Validation loss: 2.7191597774464595

Epoch: 6| Step: 13
Training loss: 2.5423591136932373
Validation loss: 2.720771405004686

Epoch: 54| Step: 0
Training loss: 3.850414752960205
Validation loss: 2.72274419312836

Epoch: 6| Step: 1
Training loss: 2.4526915550231934
Validation loss: 2.7204530290378037

Epoch: 6| Step: 2
Training loss: 2.876798152923584
Validation loss: 2.7241297152734574

Epoch: 6| Step: 3
Training loss: 2.63516902923584
Validation loss: 2.7213776239784817

Epoch: 6| Step: 4
Training loss: 2.499011993408203
Validation loss: 2.7206820903285855

Epoch: 6| Step: 5
Training loss: 1.7050111293792725
Validation loss: 2.7241520958562053

Epoch: 6| Step: 6
Training loss: 2.6633901596069336
Validation loss: 2.724851146821053

Epoch: 6| Step: 7
Training loss: 3.54418683052063
Validation loss: 2.7247669107170513

Epoch: 6| Step: 8
Training loss: 2.1907529830932617
Validation loss: 2.7272584053777877

Epoch: 6| Step: 9
Training loss: 3.1741693019866943
Validation loss: 2.72276331276022

Epoch: 6| Step: 10
Training loss: 3.706596851348877
Validation loss: 2.7250922879865094

Epoch: 6| Step: 11
Training loss: 2.9754228591918945
Validation loss: 2.724807362402639

Epoch: 6| Step: 12
Training loss: 2.9338722229003906
Validation loss: 2.718031047492899

Epoch: 6| Step: 13
Training loss: 3.0707223415374756
Validation loss: 2.7173295405603226

Epoch: 55| Step: 0
Training loss: 3.180929183959961
Validation loss: 2.7248992689194216

Epoch: 6| Step: 1
Training loss: 2.7439305782318115
Validation loss: 2.725803675190095

Epoch: 6| Step: 2
Training loss: 2.216552734375
Validation loss: 2.727454634122951

Epoch: 6| Step: 3
Training loss: 2.9503588676452637
Validation loss: 2.7288794030425367

Epoch: 6| Step: 4
Training loss: 2.631443500518799
Validation loss: 2.7345070659473376

Epoch: 6| Step: 5
Training loss: 4.066954612731934
Validation loss: 2.7414429418502317

Epoch: 6| Step: 6
Training loss: 2.911163330078125
Validation loss: 2.743264877667991

Epoch: 6| Step: 7
Training loss: 2.757924795150757
Validation loss: 2.7348794321860037

Epoch: 6| Step: 8
Training loss: 1.9201455116271973
Validation loss: 2.7233465461320776

Epoch: 6| Step: 9
Training loss: 3.5769131183624268
Validation loss: 2.721004968048424

Epoch: 6| Step: 10
Training loss: 2.728322982788086
Validation loss: 2.7185006090389785

Epoch: 6| Step: 11
Training loss: 2.680636167526245
Validation loss: 2.7143889370784966

Epoch: 6| Step: 12
Training loss: 2.6820194721221924
Validation loss: 2.7141354724925053

Epoch: 6| Step: 13
Training loss: 3.333258628845215
Validation loss: 2.7172766629085747

Epoch: 56| Step: 0
Training loss: 2.5183229446411133
Validation loss: 2.716238352560228

Epoch: 6| Step: 1
Training loss: 2.8865082263946533
Validation loss: 2.7153899874738467

Epoch: 6| Step: 2
Training loss: 3.5245728492736816
Validation loss: 2.717056220577609

Epoch: 6| Step: 3
Training loss: 3.806218147277832
Validation loss: 2.7165052608777116

Epoch: 6| Step: 4
Training loss: 3.274022340774536
Validation loss: 2.7235288363631054

Epoch: 6| Step: 5
Training loss: 1.8710604906082153
Validation loss: 2.7210614553061863

Epoch: 6| Step: 6
Training loss: 3.184842109680176
Validation loss: 2.7229049077597995

Epoch: 6| Step: 7
Training loss: 2.384768009185791
Validation loss: 2.7201839159893733

Epoch: 6| Step: 8
Training loss: 3.156151294708252
Validation loss: 2.717519280731037

Epoch: 6| Step: 9
Training loss: 2.4917407035827637
Validation loss: 2.724371633222026

Epoch: 6| Step: 10
Training loss: 2.801687002182007
Validation loss: 2.725975380148939

Epoch: 6| Step: 11
Training loss: 2.500699996948242
Validation loss: 2.7224006319558747

Epoch: 6| Step: 12
Training loss: 3.0982863903045654
Validation loss: 2.731142149176649

Epoch: 6| Step: 13
Training loss: 2.5061399936676025
Validation loss: 2.7235851928751957

Epoch: 57| Step: 0
Training loss: 2.603236675262451
Validation loss: 2.7210471604460027

Epoch: 6| Step: 1
Training loss: 2.6694116592407227
Validation loss: 2.7189967504111667

Epoch: 6| Step: 2
Training loss: 3.249051332473755
Validation loss: 2.7160074633936726

Epoch: 6| Step: 3
Training loss: 3.4289302825927734
Validation loss: 2.7126055122703634

Epoch: 6| Step: 4
Training loss: 3.1957578659057617
Validation loss: 2.715503292699014

Epoch: 6| Step: 5
Training loss: 2.0534465312957764
Validation loss: 2.7208631089938584

Epoch: 6| Step: 6
Training loss: 3.171205520629883
Validation loss: 2.714549946528609

Epoch: 6| Step: 7
Training loss: 2.188161849975586
Validation loss: 2.7113293319620113

Epoch: 6| Step: 8
Training loss: 2.323577880859375
Validation loss: 2.7158381810752292

Epoch: 6| Step: 9
Training loss: 3.350994825363159
Validation loss: 2.714268925369427

Epoch: 6| Step: 10
Training loss: 2.805478096008301
Validation loss: 2.714140999701715

Epoch: 6| Step: 11
Training loss: 3.0496792793273926
Validation loss: 2.717947890681605

Epoch: 6| Step: 12
Training loss: 2.722198963165283
Validation loss: 2.7197731617958314

Epoch: 6| Step: 13
Training loss: 3.5327651500701904
Validation loss: 2.7179818076472126

Epoch: 58| Step: 0
Training loss: 2.0558929443359375
Validation loss: 2.713051524213565

Epoch: 6| Step: 1
Training loss: 3.124589443206787
Validation loss: 2.713725789900749

Epoch: 6| Step: 2
Training loss: 2.817836284637451
Validation loss: 2.71548629319796

Epoch: 6| Step: 3
Training loss: 2.3554277420043945
Validation loss: 2.7168274310327347

Epoch: 6| Step: 4
Training loss: 3.751063346862793
Validation loss: 2.7157151545247724

Epoch: 6| Step: 5
Training loss: 2.9535646438598633
Validation loss: 2.712779096377793

Epoch: 6| Step: 6
Training loss: 2.9495956897735596
Validation loss: 2.715594599323888

Epoch: 6| Step: 7
Training loss: 2.9472155570983887
Validation loss: 2.7169329863722607

Epoch: 6| Step: 8
Training loss: 2.9344425201416016
Validation loss: 2.7177054574412685

Epoch: 6| Step: 9
Training loss: 3.1077942848205566
Validation loss: 2.718043883641561

Epoch: 6| Step: 10
Training loss: 2.175107479095459
Validation loss: 2.7270407420332714

Epoch: 6| Step: 11
Training loss: 2.8577327728271484
Validation loss: 2.720632888937509

Epoch: 6| Step: 12
Training loss: 3.5598466396331787
Validation loss: 2.7161507273233063

Epoch: 6| Step: 13
Training loss: 2.2955198287963867
Validation loss: 2.71245563414789

Epoch: 59| Step: 0
Training loss: 3.050600051879883
Validation loss: 2.711649000003774

Epoch: 6| Step: 1
Training loss: 2.852410078048706
Validation loss: 2.709061361128284

Epoch: 6| Step: 2
Training loss: 3.039436101913452
Validation loss: 2.7078276552179807

Epoch: 6| Step: 3
Training loss: 2.8052077293395996
Validation loss: 2.707489393090689

Epoch: 6| Step: 4
Training loss: 2.5371670722961426
Validation loss: 2.7083905140558877

Epoch: 6| Step: 5
Training loss: 2.618534564971924
Validation loss: 2.706638795073314

Epoch: 6| Step: 6
Training loss: 2.764575481414795
Validation loss: 2.7061868329201975

Epoch: 6| Step: 7
Training loss: 2.465712785720825
Validation loss: 2.70875685445724

Epoch: 6| Step: 8
Training loss: 3.3994956016540527
Validation loss: 2.708690289528139

Epoch: 6| Step: 9
Training loss: 3.14286208152771
Validation loss: 2.7063379646629415

Epoch: 6| Step: 10
Training loss: 3.3077597618103027
Validation loss: 2.7050044716045423

Epoch: 6| Step: 11
Training loss: 2.7116317749023438
Validation loss: 2.705869556755148

Epoch: 6| Step: 12
Training loss: 2.4396708011627197
Validation loss: 2.7047725313453266

Epoch: 6| Step: 13
Training loss: 2.7618813514709473
Validation loss: 2.70401261186087

Epoch: 60| Step: 0
Training loss: 3.7218830585479736
Validation loss: 2.713405604003578

Epoch: 6| Step: 1
Training loss: 3.018183708190918
Validation loss: 2.7129505116452455

Epoch: 6| Step: 2
Training loss: 3.1176443099975586
Validation loss: 2.7177229081430743

Epoch: 6| Step: 3
Training loss: 2.8217549324035645
Validation loss: 2.7215179166486188

Epoch: 6| Step: 4
Training loss: 2.3956236839294434
Validation loss: 2.720071510602069

Epoch: 6| Step: 5
Training loss: 3.216672658920288
Validation loss: 2.726284650064284

Epoch: 6| Step: 6
Training loss: 2.852612018585205
Validation loss: 2.7216739372540544

Epoch: 6| Step: 7
Training loss: 2.412426471710205
Validation loss: 2.7138530310764106

Epoch: 6| Step: 8
Training loss: 2.8827316761016846
Validation loss: 2.7095295639448267

Epoch: 6| Step: 9
Training loss: 2.7917184829711914
Validation loss: 2.706669215233095

Epoch: 6| Step: 10
Training loss: 2.9444570541381836
Validation loss: 2.7060123233384985

Epoch: 6| Step: 11
Training loss: 3.025270938873291
Validation loss: 2.703409587183306

Epoch: 6| Step: 12
Training loss: 1.8687039613723755
Validation loss: 2.701526936664376

Epoch: 6| Step: 13
Training loss: 2.9487979412078857
Validation loss: 2.704719515256984

Epoch: 61| Step: 0
Training loss: 3.0056653022766113
Validation loss: 2.7022302945454917

Epoch: 6| Step: 1
Training loss: 3.1224265098571777
Validation loss: 2.707483730008525

Epoch: 6| Step: 2
Training loss: 3.0990304946899414
Validation loss: 2.7066126177387853

Epoch: 6| Step: 3
Training loss: 2.4044981002807617
Validation loss: 2.702275040329144

Epoch: 6| Step: 4
Training loss: 2.209106683731079
Validation loss: 2.7011309874955045

Epoch: 6| Step: 5
Training loss: 2.415848731994629
Validation loss: 2.702750910994827

Epoch: 6| Step: 6
Training loss: 2.8316221237182617
Validation loss: 2.7026312940864154

Epoch: 6| Step: 7
Training loss: 3.035999059677124
Validation loss: 2.7022123029155116

Epoch: 6| Step: 8
Training loss: 3.0046615600585938
Validation loss: 2.7037142835637575

Epoch: 6| Step: 9
Training loss: 3.156395196914673
Validation loss: 2.70346761262545

Epoch: 6| Step: 10
Training loss: 2.922126531600952
Validation loss: 2.7035684841935352

Epoch: 6| Step: 11
Training loss: 3.7137768268585205
Validation loss: 2.701570218609225

Epoch: 6| Step: 12
Training loss: 2.5178167819976807
Validation loss: 2.6995980508865847

Epoch: 6| Step: 13
Training loss: 2.1914889812469482
Validation loss: 2.695747688252439

Epoch: 62| Step: 0
Training loss: 2.97042179107666
Validation loss: 2.696037300171391

Epoch: 6| Step: 1
Training loss: 1.9468083381652832
Validation loss: 2.6988471297807592

Epoch: 6| Step: 2
Training loss: 4.339379787445068
Validation loss: 2.699296923093898

Epoch: 6| Step: 3
Training loss: 2.8348946571350098
Validation loss: 2.698809790354903

Epoch: 6| Step: 4
Training loss: 1.6551365852355957
Validation loss: 2.6999693019415743

Epoch: 6| Step: 5
Training loss: 2.9314498901367188
Validation loss: 2.6959117484349076

Epoch: 6| Step: 6
Training loss: 2.4150145053863525
Validation loss: 2.6989116745610393

Epoch: 6| Step: 7
Training loss: 2.5115697383880615
Validation loss: 2.6991151866092475

Epoch: 6| Step: 8
Training loss: 3.059093952178955
Validation loss: 2.6984655062357583

Epoch: 6| Step: 9
Training loss: 2.841230869293213
Validation loss: 2.7009891925319547

Epoch: 6| Step: 10
Training loss: 3.180116653442383
Validation loss: 2.7023678107928206

Epoch: 6| Step: 11
Training loss: 2.979487657546997
Validation loss: 2.6979494351212696

Epoch: 6| Step: 12
Training loss: 3.571593761444092
Validation loss: 2.6969183029667025

Epoch: 6| Step: 13
Training loss: 2.4697701930999756
Validation loss: 2.6967148780822754

Epoch: 63| Step: 0
Training loss: 3.0389597415924072
Validation loss: 2.700152297173777

Epoch: 6| Step: 1
Training loss: 2.8086280822753906
Validation loss: 2.6957187319314606

Epoch: 6| Step: 2
Training loss: 2.6237239837646484
Validation loss: 2.70367601097271

Epoch: 6| Step: 3
Training loss: 3.2181897163391113
Validation loss: 2.7008144419680358

Epoch: 6| Step: 4
Training loss: 3.0112013816833496
Validation loss: 2.6981608354917137

Epoch: 6| Step: 5
Training loss: 2.7933311462402344
Validation loss: 2.696018229248703

Epoch: 6| Step: 6
Training loss: 3.173665761947632
Validation loss: 2.6944033689396356

Epoch: 6| Step: 7
Training loss: 2.9643726348876953
Validation loss: 2.6958706276391142

Epoch: 6| Step: 8
Training loss: 3.1947906017303467
Validation loss: 2.699022093126851

Epoch: 6| Step: 9
Training loss: 2.2982587814331055
Validation loss: 2.6979905328442975

Epoch: 6| Step: 10
Training loss: 2.0211434364318848
Validation loss: 2.6978463178039878

Epoch: 6| Step: 11
Training loss: 2.901144027709961
Validation loss: 2.7014688317493727

Epoch: 6| Step: 12
Training loss: 2.907667636871338
Validation loss: 2.699799660713442

Epoch: 6| Step: 13
Training loss: 2.861548900604248
Validation loss: 2.695880563028397

Epoch: 64| Step: 0
Training loss: 2.160155773162842
Validation loss: 2.694402761356805

Epoch: 6| Step: 1
Training loss: 3.6406073570251465
Validation loss: 2.6977037768210135

Epoch: 6| Step: 2
Training loss: 3.4406630992889404
Validation loss: 2.697524232249106

Epoch: 6| Step: 3
Training loss: 3.221045970916748
Validation loss: 2.6978290901389173

Epoch: 6| Step: 4
Training loss: 3.3253087997436523
Validation loss: 2.695614814758301

Epoch: 6| Step: 5
Training loss: 3.5538430213928223
Validation loss: 2.693037002317367

Epoch: 6| Step: 6
Training loss: 1.7905681133270264
Validation loss: 2.6960159629903813

Epoch: 6| Step: 7
Training loss: 3.019285202026367
Validation loss: 2.6996585553692234

Epoch: 6| Step: 8
Training loss: 2.483304977416992
Validation loss: 2.6974492944696897

Epoch: 6| Step: 9
Training loss: 2.6966710090637207
Validation loss: 2.697593417218936

Epoch: 6| Step: 10
Training loss: 2.761308431625366
Validation loss: 2.7033380616095757

Epoch: 6| Step: 11
Training loss: 2.594618320465088
Validation loss: 2.70030612843011

Epoch: 6| Step: 12
Training loss: 2.67364239692688
Validation loss: 2.700767970854236

Epoch: 6| Step: 13
Training loss: 2.0885367393493652
Validation loss: 2.693256239737234

Epoch: 65| Step: 0
Training loss: 1.6697168350219727
Validation loss: 2.689484867998349

Epoch: 6| Step: 1
Training loss: 2.9820027351379395
Validation loss: 2.691014484692645

Epoch: 6| Step: 2
Training loss: 3.085937976837158
Validation loss: 2.69075358042153

Epoch: 6| Step: 3
Training loss: 2.3572568893432617
Validation loss: 2.6924224848388345

Epoch: 6| Step: 4
Training loss: 2.2975270748138428
Validation loss: 2.692447767462782

Epoch: 6| Step: 5
Training loss: 3.3315281867980957
Validation loss: 2.68962864721975

Epoch: 6| Step: 6
Training loss: 2.786884307861328
Validation loss: 2.69418700023364

Epoch: 6| Step: 7
Training loss: 2.396366596221924
Validation loss: 2.6952825848774244

Epoch: 6| Step: 8
Training loss: 3.0316057205200195
Validation loss: 2.693627690756193

Epoch: 6| Step: 9
Training loss: 3.0944173336029053
Validation loss: 2.6934158648214033

Epoch: 6| Step: 10
Training loss: 2.8506689071655273
Validation loss: 2.6934714240412556

Epoch: 6| Step: 11
Training loss: 3.268892526626587
Validation loss: 2.6892998013445126

Epoch: 6| Step: 12
Training loss: 3.1605777740478516
Validation loss: 2.689664622788788

Epoch: 6| Step: 13
Training loss: 3.9066200256347656
Validation loss: 2.6899334897277174

Epoch: 66| Step: 0
Training loss: 3.1424150466918945
Validation loss: 2.688417355219523

Epoch: 6| Step: 1
Training loss: 3.0601353645324707
Validation loss: 2.689864786722327

Epoch: 6| Step: 2
Training loss: 3.1117162704467773
Validation loss: 2.6938262408779514

Epoch: 6| Step: 3
Training loss: 3.532334804534912
Validation loss: 2.6854815585638887

Epoch: 6| Step: 4
Training loss: 2.4762330055236816
Validation loss: 2.6945772555566605

Epoch: 6| Step: 5
Training loss: 2.2479801177978516
Validation loss: 2.695844263158819

Epoch: 6| Step: 6
Training loss: 2.998115062713623
Validation loss: 2.702980805468816

Epoch: 6| Step: 7
Training loss: 3.2803759574890137
Validation loss: 2.700212045382428

Epoch: 6| Step: 8
Training loss: 3.0537023544311523
Validation loss: 2.7008968758326706

Epoch: 6| Step: 9
Training loss: 2.1120381355285645
Validation loss: 2.708671003259638

Epoch: 6| Step: 10
Training loss: 2.0327460765838623
Validation loss: 2.709808216300062

Epoch: 6| Step: 11
Training loss: 3.141091823577881
Validation loss: 2.703796012427217

Epoch: 6| Step: 12
Training loss: 2.891683578491211
Validation loss: 2.704583142393379

Epoch: 6| Step: 13
Training loss: 2.532074213027954
Validation loss: 2.6917940852462605

Epoch: 67| Step: 0
Training loss: 2.031681776046753
Validation loss: 2.689752068570865

Epoch: 6| Step: 1
Training loss: 2.960994005203247
Validation loss: 2.6856597162062124

Epoch: 6| Step: 2
Training loss: 2.9665284156799316
Validation loss: 2.6870217989849787

Epoch: 6| Step: 3
Training loss: 2.2794575691223145
Validation loss: 2.6865801657399824

Epoch: 6| Step: 4
Training loss: 2.6616616249084473
Validation loss: 2.6915584917991393

Epoch: 6| Step: 5
Training loss: 2.092644214630127
Validation loss: 2.6921191958970923

Epoch: 6| Step: 6
Training loss: 3.156336545944214
Validation loss: 2.6901736413278887

Epoch: 6| Step: 7
Training loss: 2.8660545349121094
Validation loss: 2.691927912414715

Epoch: 6| Step: 8
Training loss: 2.493180513381958
Validation loss: 2.692453266471945

Epoch: 6| Step: 9
Training loss: 3.094411849975586
Validation loss: 2.6933197846976658

Epoch: 6| Step: 10
Training loss: 3.200303077697754
Validation loss: 2.698119307077059

Epoch: 6| Step: 11
Training loss: 3.3758225440979004
Validation loss: 2.699540481772474

Epoch: 6| Step: 12
Training loss: 3.2950825691223145
Validation loss: 2.6948147127705235

Epoch: 6| Step: 13
Training loss: 3.5127038955688477
Validation loss: 2.692579105336179

Epoch: 68| Step: 0
Training loss: 2.8534884452819824
Validation loss: 2.6920971357694237

Epoch: 6| Step: 1
Training loss: 3.5209975242614746
Validation loss: 2.690624539570142

Epoch: 6| Step: 2
Training loss: 2.193544864654541
Validation loss: 2.6981531061151975

Epoch: 6| Step: 3
Training loss: 2.5571789741516113
Validation loss: 2.701565250273674

Epoch: 6| Step: 4
Training loss: 2.5099964141845703
Validation loss: 2.708727798154277

Epoch: 6| Step: 5
Training loss: 3.2096962928771973
Validation loss: 2.70353937661776

Epoch: 6| Step: 6
Training loss: 2.7096996307373047
Validation loss: 2.697565619663526

Epoch: 6| Step: 7
Training loss: 2.1320056915283203
Validation loss: 2.688038064587501

Epoch: 6| Step: 8
Training loss: 2.804981231689453
Validation loss: 2.688840648179413

Epoch: 6| Step: 9
Training loss: 2.920050621032715
Validation loss: 2.6870295719433854

Epoch: 6| Step: 10
Training loss: 2.7266616821289062
Validation loss: 2.68553311850435

Epoch: 6| Step: 11
Training loss: 3.424564838409424
Validation loss: 2.68674821494728

Epoch: 6| Step: 12
Training loss: 3.1140244007110596
Validation loss: 2.6862776176903838

Epoch: 6| Step: 13
Training loss: 3.1268832683563232
Validation loss: 2.687887881391792

Epoch: 69| Step: 0
Training loss: 2.7282137870788574
Validation loss: 2.6882545332754813

Epoch: 6| Step: 1
Training loss: 3.201180934906006
Validation loss: 2.6865821833251626

Epoch: 6| Step: 2
Training loss: 2.8244152069091797
Validation loss: 2.6900480793368433

Epoch: 6| Step: 3
Training loss: 3.070324659347534
Validation loss: 2.6941416750672045

Epoch: 6| Step: 4
Training loss: 2.833460807800293
Validation loss: 2.700660833748438

Epoch: 6| Step: 5
Training loss: 2.375223159790039
Validation loss: 2.6967182467060704

Epoch: 6| Step: 6
Training loss: 2.970649242401123
Validation loss: 2.6980394753076697

Epoch: 6| Step: 7
Training loss: 2.8790972232818604
Validation loss: 2.6911532648148073

Epoch: 6| Step: 8
Training loss: 2.3362369537353516
Validation loss: 2.6885550637398996

Epoch: 6| Step: 9
Training loss: 3.349231243133545
Validation loss: 2.6866681088683424

Epoch: 6| Step: 10
Training loss: 3.0955569744110107
Validation loss: 2.6880614424264557

Epoch: 6| Step: 11
Training loss: 2.049715042114258
Validation loss: 2.6870492453216226

Epoch: 6| Step: 12
Training loss: 3.849644899368286
Validation loss: 2.6843982717042327

Epoch: 6| Step: 13
Training loss: 1.4481004476547241
Validation loss: 2.6810849943468646

Epoch: 70| Step: 0
Training loss: 3.2200684547424316
Validation loss: 2.684844816884687

Epoch: 6| Step: 1
Training loss: 2.6280357837677
Validation loss: 2.684422044343846

Epoch: 6| Step: 2
Training loss: 3.235218048095703
Validation loss: 2.688880633282405

Epoch: 6| Step: 3
Training loss: 3.1582231521606445
Validation loss: 2.6841683182665097

Epoch: 6| Step: 4
Training loss: 2.582465648651123
Validation loss: 2.687522557473952

Epoch: 6| Step: 5
Training loss: 2.812103033065796
Validation loss: 2.6872009333743843

Epoch: 6| Step: 6
Training loss: 1.94440495967865
Validation loss: 2.6905468048587924

Epoch: 6| Step: 7
Training loss: 2.612492084503174
Validation loss: 2.6885862735009964

Epoch: 6| Step: 8
Training loss: 3.2894749641418457
Validation loss: 2.686478845534786

Epoch: 6| Step: 9
Training loss: 2.874042272567749
Validation loss: 2.684289660505069

Epoch: 6| Step: 10
Training loss: 2.762256145477295
Validation loss: 2.6830253293437343

Epoch: 6| Step: 11
Training loss: 2.666139602661133
Validation loss: 2.683544825482112

Epoch: 6| Step: 12
Training loss: 2.4232430458068848
Validation loss: 2.6802444355462187

Epoch: 6| Step: 13
Training loss: 3.811978578567505
Validation loss: 2.6842866841182915

Epoch: 71| Step: 0
Training loss: 2.5501301288604736
Validation loss: 2.681865128137732

Epoch: 6| Step: 1
Training loss: 2.9856855869293213
Validation loss: 2.6791003545125327

Epoch: 6| Step: 2
Training loss: 2.810868263244629
Validation loss: 2.67891356765583

Epoch: 6| Step: 3
Training loss: 2.5089848041534424
Validation loss: 2.684761126836141

Epoch: 6| Step: 4
Training loss: 3.8225579261779785
Validation loss: 2.6851175779937417

Epoch: 6| Step: 5
Training loss: 2.7657885551452637
Validation loss: 2.684824343650572

Epoch: 6| Step: 6
Training loss: 1.9695556163787842
Validation loss: 2.691148219570037

Epoch: 6| Step: 7
Training loss: 2.996668815612793
Validation loss: 2.6834447999154367

Epoch: 6| Step: 8
Training loss: 2.4986352920532227
Validation loss: 2.6868355607473724

Epoch: 6| Step: 9
Training loss: 2.969949722290039
Validation loss: 2.6815073310687976

Epoch: 6| Step: 10
Training loss: 3.5779199600219727
Validation loss: 2.678037743414602

Epoch: 6| Step: 11
Training loss: 2.970111608505249
Validation loss: 2.6742026523877214

Epoch: 6| Step: 12
Training loss: 2.895346164703369
Validation loss: 2.6743365821018013

Epoch: 6| Step: 13
Training loss: 1.8200678825378418
Validation loss: 2.6676010290781655

Epoch: 72| Step: 0
Training loss: 2.780742883682251
Validation loss: 2.6657743095069804

Epoch: 6| Step: 1
Training loss: 2.5482053756713867
Validation loss: 2.673277057627196

Epoch: 6| Step: 2
Training loss: 2.707180976867676
Validation loss: 2.6715826065309587

Epoch: 6| Step: 3
Training loss: 3.0405080318450928
Validation loss: 2.6728470863834506

Epoch: 6| Step: 4
Training loss: 2.4972543716430664
Validation loss: 2.6722581053292878

Epoch: 6| Step: 5
Training loss: 3.318794012069702
Validation loss: 2.6715871313566804

Epoch: 6| Step: 6
Training loss: 2.8121519088745117
Validation loss: 2.6666938771483717

Epoch: 6| Step: 7
Training loss: 2.3282127380371094
Validation loss: 2.6659989228812595

Epoch: 6| Step: 8
Training loss: 2.9323902130126953
Validation loss: 2.6628934747429303

Epoch: 6| Step: 9
Training loss: 3.1250319480895996
Validation loss: 2.6574849697851364

Epoch: 6| Step: 10
Training loss: 2.751453399658203
Validation loss: 2.656175034020537

Epoch: 6| Step: 11
Training loss: 3.333946704864502
Validation loss: 2.6630256381086124

Epoch: 6| Step: 12
Training loss: 2.949524164199829
Validation loss: 2.6687594254811606

Epoch: 6| Step: 13
Training loss: 1.9948076009750366
Validation loss: 2.67272679011027

Epoch: 73| Step: 0
Training loss: 3.049330472946167
Validation loss: 2.6773782801884476

Epoch: 6| Step: 1
Training loss: 2.992154836654663
Validation loss: 2.6696024069222073

Epoch: 6| Step: 2
Training loss: 2.1360983848571777
Validation loss: 2.663259006315662

Epoch: 6| Step: 3
Training loss: 2.54333233833313
Validation loss: 2.6606179898785007

Epoch: 6| Step: 4
Training loss: 2.4234814643859863
Validation loss: 2.6527231585594917

Epoch: 6| Step: 5
Training loss: 2.3099000453948975
Validation loss: 2.6481806539720103

Epoch: 6| Step: 6
Training loss: 3.5029137134552
Validation loss: 2.649710032247728

Epoch: 6| Step: 7
Training loss: 3.252075672149658
Validation loss: 2.647144740627658

Epoch: 6| Step: 8
Training loss: 3.2124462127685547
Validation loss: 2.650848227162515

Epoch: 6| Step: 9
Training loss: 2.5803768634796143
Validation loss: 2.648033190799016

Epoch: 6| Step: 10
Training loss: 2.939821243286133
Validation loss: 2.6539373987464496

Epoch: 6| Step: 11
Training loss: 3.0535213947296143
Validation loss: 2.649836189003401

Epoch: 6| Step: 12
Training loss: 2.697075366973877
Validation loss: 2.655652489713443

Epoch: 6| Step: 13
Training loss: 2.8064804077148438
Validation loss: 2.6568809555422876

Epoch: 74| Step: 0
Training loss: 2.2305493354797363
Validation loss: 2.6482848044364684

Epoch: 6| Step: 1
Training loss: 3.3629086017608643
Validation loss: 2.6490277192925893

Epoch: 6| Step: 2
Training loss: 2.361985445022583
Validation loss: 2.6491190951357604

Epoch: 6| Step: 3
Training loss: 2.7831192016601562
Validation loss: 2.647588740112961

Epoch: 6| Step: 4
Training loss: 3.157891273498535
Validation loss: 2.653657349207068

Epoch: 6| Step: 5
Training loss: 2.4671213626861572
Validation loss: 2.6475742734888548

Epoch: 6| Step: 6
Training loss: 3.140827178955078
Validation loss: 2.643614443399573

Epoch: 6| Step: 7
Training loss: 2.3563313484191895
Validation loss: 2.6472113337568057

Epoch: 6| Step: 8
Training loss: 2.8786306381225586
Validation loss: 2.644567056368756

Epoch: 6| Step: 9
Training loss: 2.867669105529785
Validation loss: 2.648021339088358

Epoch: 6| Step: 10
Training loss: 2.656498908996582
Validation loss: 2.6471219703715336

Epoch: 6| Step: 11
Training loss: 2.1654727458953857
Validation loss: 2.646360763939478

Epoch: 6| Step: 12
Training loss: 3.9137792587280273
Validation loss: 2.6469065297034478

Epoch: 6| Step: 13
Training loss: 2.956974506378174
Validation loss: 2.6509553206864225

Epoch: 75| Step: 0
Training loss: 2.383161783218384
Validation loss: 2.6456778305833057

Epoch: 6| Step: 1
Training loss: 2.9031386375427246
Validation loss: 2.6472188349693053

Epoch: 6| Step: 2
Training loss: 2.1588234901428223
Validation loss: 2.6498651965971916

Epoch: 6| Step: 3
Training loss: 3.278510808944702
Validation loss: 2.6496152416352303

Epoch: 6| Step: 4
Training loss: 2.2575771808624268
Validation loss: 2.649993186355919

Epoch: 6| Step: 5
Training loss: 2.0483150482177734
Validation loss: 2.647098648932672

Epoch: 6| Step: 6
Training loss: 2.78157114982605
Validation loss: 2.6505196248331377

Epoch: 6| Step: 7
Training loss: 2.7870163917541504
Validation loss: 2.6506878509316394

Epoch: 6| Step: 8
Training loss: 2.706721782684326
Validation loss: 2.6481411175061296

Epoch: 6| Step: 9
Training loss: 3.697784900665283
Validation loss: 2.6467776131886307

Epoch: 6| Step: 10
Training loss: 2.851719856262207
Validation loss: 2.648385804186585

Epoch: 6| Step: 11
Training loss: 2.3729023933410645
Validation loss: 2.644936433402441

Epoch: 6| Step: 12
Training loss: 3.5345749855041504
Validation loss: 2.641072762909756

Epoch: 6| Step: 13
Training loss: 3.9310543537139893
Validation loss: 2.641634302754556

Epoch: 76| Step: 0
Training loss: 2.4763312339782715
Validation loss: 2.638152899280671

Epoch: 6| Step: 1
Training loss: 3.1706960201263428
Validation loss: 2.6386453054284535

Epoch: 6| Step: 2
Training loss: 2.9207637310028076
Validation loss: 2.637443429680281

Epoch: 6| Step: 3
Training loss: 2.943291664123535
Validation loss: 2.640218324558709

Epoch: 6| Step: 4
Training loss: 3.579556941986084
Validation loss: 2.639768659427602

Epoch: 6| Step: 5
Training loss: 3.1730093955993652
Validation loss: 2.636926030599943

Epoch: 6| Step: 6
Training loss: 2.4083380699157715
Validation loss: 2.635679688504947

Epoch: 6| Step: 7
Training loss: 3.4288525581359863
Validation loss: 2.635219466301703

Epoch: 6| Step: 8
Training loss: 1.667374849319458
Validation loss: 2.6317178715941725

Epoch: 6| Step: 9
Training loss: 3.271467685699463
Validation loss: 2.636016022774481

Epoch: 6| Step: 10
Training loss: 2.2676703929901123
Validation loss: 2.6347002803638415

Epoch: 6| Step: 11
Training loss: 2.509451389312744
Validation loss: 2.63875929258203

Epoch: 6| Step: 12
Training loss: 2.7873287200927734
Validation loss: 2.6448695198182137

Epoch: 6| Step: 13
Training loss: 2.391187906265259
Validation loss: 2.6435439868639876

Epoch: 77| Step: 0
Training loss: 3.2260963916778564
Validation loss: 2.6382812274399625

Epoch: 6| Step: 1
Training loss: 2.6196975708007812
Validation loss: 2.6289565165837607

Epoch: 6| Step: 2
Training loss: 2.106717109680176
Validation loss: 2.63086837081499

Epoch: 6| Step: 3
Training loss: 2.9941067695617676
Validation loss: 2.6357282438585834

Epoch: 6| Step: 4
Training loss: 3.4890518188476562
Validation loss: 2.6299724476311797

Epoch: 6| Step: 5
Training loss: 2.9386117458343506
Validation loss: 2.6290465888156684

Epoch: 6| Step: 6
Training loss: 2.9643449783325195
Validation loss: 2.6308683618422477

Epoch: 6| Step: 7
Training loss: 2.9807891845703125
Validation loss: 2.6323574332780737

Epoch: 6| Step: 8
Training loss: 1.9041757583618164
Validation loss: 2.632303043078351

Epoch: 6| Step: 9
Training loss: 2.0657966136932373
Validation loss: 2.6357498604764222

Epoch: 6| Step: 10
Training loss: 3.3277602195739746
Validation loss: 2.6307186285654702

Epoch: 6| Step: 11
Training loss: 2.7655115127563477
Validation loss: 2.6344403656580115

Epoch: 6| Step: 12
Training loss: 3.1227810382843018
Validation loss: 2.63327278629426

Epoch: 6| Step: 13
Training loss: 2.4218976497650146
Validation loss: 2.6342037159909486

Epoch: 78| Step: 0
Training loss: 2.383385419845581
Validation loss: 2.6418122271055817

Epoch: 6| Step: 1
Training loss: 3.252089738845825
Validation loss: 2.6451746289448073

Epoch: 6| Step: 2
Training loss: 2.4326584339141846
Validation loss: 2.6489577370305217

Epoch: 6| Step: 3
Training loss: 2.3260300159454346
Validation loss: 2.649774953883181

Epoch: 6| Step: 4
Training loss: 2.5672450065612793
Validation loss: 2.641515131919615

Epoch: 6| Step: 5
Training loss: 3.221331834793091
Validation loss: 2.635278368508944

Epoch: 6| Step: 6
Training loss: 3.6525330543518066
Validation loss: 2.634411983592536

Epoch: 6| Step: 7
Training loss: 2.2647247314453125
Validation loss: 2.6288384827234412

Epoch: 6| Step: 8
Training loss: 3.5448694229125977
Validation loss: 2.6306989910782024

Epoch: 6| Step: 9
Training loss: 2.4240736961364746
Validation loss: 2.629821095415341

Epoch: 6| Step: 10
Training loss: 2.390223979949951
Validation loss: 2.6311971679810555

Epoch: 6| Step: 11
Training loss: 2.978209972381592
Validation loss: 2.627929392681327

Epoch: 6| Step: 12
Training loss: 2.990029811859131
Validation loss: 2.6276679115910686

Epoch: 6| Step: 13
Training loss: 2.6613969802856445
Validation loss: 2.6316224990352506

Epoch: 79| Step: 0
Training loss: 3.058328866958618
Validation loss: 2.6320165895646617

Epoch: 6| Step: 1
Training loss: 3.827970504760742
Validation loss: 2.629103886183872

Epoch: 6| Step: 2
Training loss: 2.718170642852783
Validation loss: 2.6306575267545638

Epoch: 6| Step: 3
Training loss: 3.2215564250946045
Validation loss: 2.6309332463049118

Epoch: 6| Step: 4
Training loss: 3.43373703956604
Validation loss: 2.632268795403101

Epoch: 6| Step: 5
Training loss: 2.438535213470459
Validation loss: 2.6342094482914096

Epoch: 6| Step: 6
Training loss: 2.4496278762817383
Validation loss: 2.633454274105769

Epoch: 6| Step: 7
Training loss: 3.2297565937042236
Validation loss: 2.631481034781343

Epoch: 6| Step: 8
Training loss: 2.635828733444214
Validation loss: 2.6382761975770355

Epoch: 6| Step: 9
Training loss: 2.809398651123047
Validation loss: 2.636549900936824

Epoch: 6| Step: 10
Training loss: 2.3270175457000732
Validation loss: 2.6314429006268902

Epoch: 6| Step: 11
Training loss: 1.956095814704895
Validation loss: 2.6315955833722184

Epoch: 6| Step: 12
Training loss: 2.368131160736084
Validation loss: 2.631268124426565

Epoch: 6| Step: 13
Training loss: 2.4882211685180664
Validation loss: 2.6290255797806608

Epoch: 80| Step: 0
Training loss: 3.2260327339172363
Validation loss: 2.6280284979010142

Epoch: 6| Step: 1
Training loss: 2.6072230339050293
Validation loss: 2.638834230361446

Epoch: 6| Step: 2
Training loss: 3.003013849258423
Validation loss: 2.6365617577747633

Epoch: 6| Step: 3
Training loss: 2.295135021209717
Validation loss: 2.6294731170900407

Epoch: 6| Step: 4
Training loss: 2.2397985458374023
Validation loss: 2.6302843042599258

Epoch: 6| Step: 5
Training loss: 2.7834887504577637
Validation loss: 2.629720454574913

Epoch: 6| Step: 6
Training loss: 2.0159873962402344
Validation loss: 2.625494223768993

Epoch: 6| Step: 7
Training loss: 3.1677794456481934
Validation loss: 2.622321741555327

Epoch: 6| Step: 8
Training loss: 2.8060755729675293
Validation loss: 2.6238023824589227

Epoch: 6| Step: 9
Training loss: 2.7852439880371094
Validation loss: 2.621943476379559

Epoch: 6| Step: 10
Training loss: 3.0725858211517334
Validation loss: 2.623812678039715

Epoch: 6| Step: 11
Training loss: 3.6850218772888184
Validation loss: 2.623022476832072

Epoch: 6| Step: 12
Training loss: 2.54636287689209
Validation loss: 2.623480732722949

Epoch: 6| Step: 13
Training loss: 2.8205738067626953
Validation loss: 2.624845193278405

Epoch: 81| Step: 0
Training loss: 2.4848737716674805
Validation loss: 2.627069152811522

Epoch: 6| Step: 1
Training loss: 2.9850478172302246
Validation loss: 2.6240848982205955

Epoch: 6| Step: 2
Training loss: 2.0357580184936523
Validation loss: 2.6306758721669516

Epoch: 6| Step: 3
Training loss: 3.0248241424560547
Validation loss: 2.6320003796649236

Epoch: 6| Step: 4
Training loss: 2.3236896991729736
Validation loss: 2.6235682656688075

Epoch: 6| Step: 5
Training loss: 3.3998541831970215
Validation loss: 2.624931404667516

Epoch: 6| Step: 6
Training loss: 2.668389081954956
Validation loss: 2.624665178278441

Epoch: 6| Step: 7
Training loss: 2.866267442703247
Validation loss: 2.6331618370548373

Epoch: 6| Step: 8
Training loss: 2.338327407836914
Validation loss: 2.6297247204729306

Epoch: 6| Step: 9
Training loss: 2.6550052165985107
Validation loss: 2.629688875649565

Epoch: 6| Step: 10
Training loss: 2.9088478088378906
Validation loss: 2.6225853735400784

Epoch: 6| Step: 11
Training loss: 2.8118324279785156
Validation loss: 2.6210941063460482

Epoch: 6| Step: 12
Training loss: 3.641465663909912
Validation loss: 2.619956365195654

Epoch: 6| Step: 13
Training loss: 2.9325926303863525
Validation loss: 2.6204920122700353

Epoch: 82| Step: 0
Training loss: 2.2266063690185547
Validation loss: 2.621077405509128

Epoch: 6| Step: 1
Training loss: 2.933387279510498
Validation loss: 2.618226141057989

Epoch: 6| Step: 2
Training loss: 2.4173202514648438
Validation loss: 2.6249685313111994

Epoch: 6| Step: 3
Training loss: 2.6532044410705566
Validation loss: 2.6254943416964625

Epoch: 6| Step: 4
Training loss: 2.3477578163146973
Validation loss: 2.6210403698746876

Epoch: 6| Step: 5
Training loss: 2.751420497894287
Validation loss: 2.621213218217255

Epoch: 6| Step: 6
Training loss: 2.91939115524292
Validation loss: 2.618846849728656

Epoch: 6| Step: 7
Training loss: 2.7409892082214355
Validation loss: 2.6236087301725983

Epoch: 6| Step: 8
Training loss: 3.4349124431610107
Validation loss: 2.619856026864821

Epoch: 6| Step: 9
Training loss: 2.715038299560547
Validation loss: 2.6212212449760846

Epoch: 6| Step: 10
Training loss: 2.596705436706543
Validation loss: 2.6207144234770086

Epoch: 6| Step: 11
Training loss: 2.9292423725128174
Validation loss: 2.6154715912316435

Epoch: 6| Step: 12
Training loss: 3.4242711067199707
Validation loss: 2.6209482454484507

Epoch: 6| Step: 13
Training loss: 2.8770570755004883
Validation loss: 2.618058140559863

Epoch: 83| Step: 0
Training loss: 1.7556188106536865
Validation loss: 2.6229798075973347

Epoch: 6| Step: 1
Training loss: 3.525967597961426
Validation loss: 2.62045717752108

Epoch: 6| Step: 2
Training loss: 2.286790370941162
Validation loss: 2.62180341956436

Epoch: 6| Step: 3
Training loss: 2.782094717025757
Validation loss: 2.632505606579524

Epoch: 6| Step: 4
Training loss: 3.5748207569122314
Validation loss: 2.630930180190712

Epoch: 6| Step: 5
Training loss: 2.8731040954589844
Validation loss: 2.625100520349318

Epoch: 6| Step: 6
Training loss: 2.6558737754821777
Validation loss: 2.6238786097495788

Epoch: 6| Step: 7
Training loss: 3.238255739212036
Validation loss: 2.621045774029147

Epoch: 6| Step: 8
Training loss: 2.318817138671875
Validation loss: 2.6241909457791235

Epoch: 6| Step: 9
Training loss: 3.186398983001709
Validation loss: 2.620101185255153

Epoch: 6| Step: 10
Training loss: 2.4936017990112305
Validation loss: 2.621098082552674

Epoch: 6| Step: 11
Training loss: 3.068272590637207
Validation loss: 2.6230786667075208

Epoch: 6| Step: 12
Training loss: 2.0603396892547607
Validation loss: 2.622861382781818

Epoch: 6| Step: 13
Training loss: 3.358323097229004
Validation loss: 2.626917105849071

Epoch: 84| Step: 0
Training loss: 2.6522250175476074
Validation loss: 2.6343657483336744

Epoch: 6| Step: 1
Training loss: 2.1454291343688965
Validation loss: 2.6438142638052664

Epoch: 6| Step: 2
Training loss: 2.2152576446533203
Validation loss: 2.6531272934329126

Epoch: 6| Step: 3
Training loss: 3.6705524921417236
Validation loss: 2.6396965365256033

Epoch: 6| Step: 4
Training loss: 2.205913782119751
Validation loss: 2.6368631611588182

Epoch: 6| Step: 5
Training loss: 2.5224268436431885
Validation loss: 2.6323992539477605

Epoch: 6| Step: 6
Training loss: 3.9550294876098633
Validation loss: 2.6204861825512302

Epoch: 6| Step: 7
Training loss: 2.54772686958313
Validation loss: 2.6172849209077897

Epoch: 6| Step: 8
Training loss: 3.1298699378967285
Validation loss: 2.615715608801893

Epoch: 6| Step: 9
Training loss: 3.381013870239258
Validation loss: 2.620448253488028

Epoch: 6| Step: 10
Training loss: 2.871612548828125
Validation loss: 2.620290638298117

Epoch: 6| Step: 11
Training loss: 2.915903329849243
Validation loss: 2.618437966992778

Epoch: 6| Step: 12
Training loss: 2.6617236137390137
Validation loss: 2.6202673091683337

Epoch: 6| Step: 13
Training loss: 1.8077144622802734
Validation loss: 2.618103055543797

Epoch: 85| Step: 0
Training loss: 3.01820707321167
Validation loss: 2.616243067608085

Epoch: 6| Step: 1
Training loss: 2.6737725734710693
Validation loss: 2.6211993437941357

Epoch: 6| Step: 2
Training loss: 2.254659652709961
Validation loss: 2.6281030075524443

Epoch: 6| Step: 3
Training loss: 2.9867448806762695
Validation loss: 2.6291881748425063

Epoch: 6| Step: 4
Training loss: 2.063814640045166
Validation loss: 2.632699256302208

Epoch: 6| Step: 5
Training loss: 3.01847243309021
Validation loss: 2.6300077515263713

Epoch: 6| Step: 6
Training loss: 2.781559467315674
Validation loss: 2.633053097673642

Epoch: 6| Step: 7
Training loss: 2.5678257942199707
Validation loss: 2.6423643186528194

Epoch: 6| Step: 8
Training loss: 2.3591556549072266
Validation loss: 2.6356834006565872

Epoch: 6| Step: 9
Training loss: 2.6725306510925293
Validation loss: 2.636849887909428

Epoch: 6| Step: 10
Training loss: 3.092357873916626
Validation loss: 2.6329234184757357

Epoch: 6| Step: 11
Training loss: 3.1623430252075195
Validation loss: 2.6285581588745117

Epoch: 6| Step: 12
Training loss: 3.0169200897216797
Validation loss: 2.6295239899748113

Epoch: 6| Step: 13
Training loss: 3.689316987991333
Validation loss: 2.6214104621641097

Epoch: 86| Step: 0
Training loss: 3.0562405586242676
Validation loss: 2.6132152875264487

Epoch: 6| Step: 1
Training loss: 3.1974539756774902
Validation loss: 2.61255810594046

Epoch: 6| Step: 2
Training loss: 2.4865593910217285
Validation loss: 2.6129592413543374

Epoch: 6| Step: 3
Training loss: 2.166110038757324
Validation loss: 2.6118486363400697

Epoch: 6| Step: 4
Training loss: 2.636597156524658
Validation loss: 2.6090081058522707

Epoch: 6| Step: 5
Training loss: 2.8136751651763916
Validation loss: 2.6118230076246363

Epoch: 6| Step: 6
Training loss: 2.571610450744629
Validation loss: 2.6110952849029214

Epoch: 6| Step: 7
Training loss: 3.6033880710601807
Validation loss: 2.611246621736916

Epoch: 6| Step: 8
Training loss: 2.8706541061401367
Validation loss: 2.6110328910171345

Epoch: 6| Step: 9
Training loss: 2.0687408447265625
Validation loss: 2.6144066215843282

Epoch: 6| Step: 10
Training loss: 2.7157142162323
Validation loss: 2.6104650574345745

Epoch: 6| Step: 11
Training loss: 2.2673680782318115
Validation loss: 2.613937795803111

Epoch: 6| Step: 12
Training loss: 3.1785459518432617
Validation loss: 2.6132431568637973

Epoch: 6| Step: 13
Training loss: 3.594996213912964
Validation loss: 2.6148351725711616

Epoch: 87| Step: 0
Training loss: 2.426210880279541
Validation loss: 2.6106402822720107

Epoch: 6| Step: 1
Training loss: 2.6273322105407715
Validation loss: 2.610961303916029

Epoch: 6| Step: 2
Training loss: 3.3099751472473145
Validation loss: 2.6162028953593266

Epoch: 6| Step: 3
Training loss: 2.8101634979248047
Validation loss: 2.6127988651234615

Epoch: 6| Step: 4
Training loss: 2.989176034927368
Validation loss: 2.6145605297498804

Epoch: 6| Step: 5
Training loss: 2.80218243598938
Validation loss: 2.6129714904292936

Epoch: 6| Step: 6
Training loss: 3.256472110748291
Validation loss: 2.62388414208607

Epoch: 6| Step: 7
Training loss: 2.0457377433776855
Validation loss: 2.6271704935258433

Epoch: 6| Step: 8
Training loss: 2.856377363204956
Validation loss: 2.6339090947181947

Epoch: 6| Step: 9
Training loss: 3.1553049087524414
Validation loss: 2.6441088799507386

Epoch: 6| Step: 10
Training loss: 2.6668028831481934
Validation loss: 2.619806420418524

Epoch: 6| Step: 11
Training loss: 2.4131054878234863
Validation loss: 2.61200148828568

Epoch: 6| Step: 12
Training loss: 2.686964988708496
Validation loss: 2.6094991212250083

Epoch: 6| Step: 13
Training loss: 2.9053566455841064
Validation loss: 2.611316068198091

Epoch: 88| Step: 0
Training loss: 2.678422212600708
Validation loss: 2.6127884439242783

Epoch: 6| Step: 1
Training loss: 3.72544527053833
Validation loss: 2.6182611565436087

Epoch: 6| Step: 2
Training loss: 1.7198231220245361
Validation loss: 2.621231012446906

Epoch: 6| Step: 3
Training loss: 3.429824113845825
Validation loss: 2.6230182955341954

Epoch: 6| Step: 4
Training loss: 3.606563091278076
Validation loss: 2.620930992146974

Epoch: 6| Step: 5
Training loss: 2.4606285095214844
Validation loss: 2.6179957364195134

Epoch: 6| Step: 6
Training loss: 2.4544677734375
Validation loss: 2.6145429380478395

Epoch: 6| Step: 7
Training loss: 2.290740489959717
Validation loss: 2.613773379274594

Epoch: 6| Step: 8
Training loss: 2.9341349601745605
Validation loss: 2.6084781410873576

Epoch: 6| Step: 9
Training loss: 2.489957332611084
Validation loss: 2.6091908383113083

Epoch: 6| Step: 10
Training loss: 2.746133804321289
Validation loss: 2.604444621711649

Epoch: 6| Step: 11
Training loss: 2.7735342979431152
Validation loss: 2.6069551257676977

Epoch: 6| Step: 12
Training loss: 2.873516798019409
Validation loss: 2.6027937781426216

Epoch: 6| Step: 13
Training loss: 2.8482446670532227
Validation loss: 2.6054332435771985

Epoch: 89| Step: 0
Training loss: 2.2925009727478027
Validation loss: 2.6059386499466433

Epoch: 6| Step: 1
Training loss: 2.7230074405670166
Validation loss: 2.607815750183598

Epoch: 6| Step: 2
Training loss: 2.8186633586883545
Validation loss: 2.604502526662683

Epoch: 6| Step: 3
Training loss: 3.134620428085327
Validation loss: 2.6051539503118044

Epoch: 6| Step: 4
Training loss: 2.6364564895629883
Validation loss: 2.604057983685565

Epoch: 6| Step: 5
Training loss: 3.1376466751098633
Validation loss: 2.604600424407631

Epoch: 6| Step: 6
Training loss: 2.6039927005767822
Validation loss: 2.605882457507554

Epoch: 6| Step: 7
Training loss: 2.5727458000183105
Validation loss: 2.600782340572726

Epoch: 6| Step: 8
Training loss: 2.688546895980835
Validation loss: 2.604308056574996

Epoch: 6| Step: 9
Training loss: 2.309506893157959
Validation loss: 2.601648602434384

Epoch: 6| Step: 10
Training loss: 2.082277774810791
Validation loss: 2.600521546538158

Epoch: 6| Step: 11
Training loss: 3.258023738861084
Validation loss: 2.6014700371731996

Epoch: 6| Step: 12
Training loss: 3.603712797164917
Validation loss: 2.601445200622723

Epoch: 6| Step: 13
Training loss: 3.005574941635132
Validation loss: 2.6032449994035947

Epoch: 90| Step: 0
Training loss: 2.776815891265869
Validation loss: 2.602301477104105

Epoch: 6| Step: 1
Training loss: 2.677175521850586
Validation loss: 2.5982795120567403

Epoch: 6| Step: 2
Training loss: 2.6883418560028076
Validation loss: 2.604939935027912

Epoch: 6| Step: 3
Training loss: 2.7103490829467773
Validation loss: 2.6121131809808875

Epoch: 6| Step: 4
Training loss: 2.700658082962036
Validation loss: 2.608625663224087

Epoch: 6| Step: 5
Training loss: 2.5157546997070312
Validation loss: 2.609933009711645

Epoch: 6| Step: 6
Training loss: 3.0071349143981934
Validation loss: 2.60469344610809

Epoch: 6| Step: 7
Training loss: 2.2194716930389404
Validation loss: 2.602371797766737

Epoch: 6| Step: 8
Training loss: 2.8691353797912598
Validation loss: 2.601686082860475

Epoch: 6| Step: 9
Training loss: 2.911832332611084
Validation loss: 2.602788202224239

Epoch: 6| Step: 10
Training loss: 2.160126209259033
Validation loss: 2.60202105070955

Epoch: 6| Step: 11
Training loss: 3.845763683319092
Validation loss: 2.5989293693214335

Epoch: 6| Step: 12
Training loss: 3.092623233795166
Validation loss: 2.600733187890822

Epoch: 6| Step: 13
Training loss: 2.4085099697113037
Validation loss: 2.6015891028988745

Epoch: 91| Step: 0
Training loss: 2.588883876800537
Validation loss: 2.599519411722819

Epoch: 6| Step: 1
Training loss: 2.4677212238311768
Validation loss: 2.6091530758847474

Epoch: 6| Step: 2
Training loss: 3.301797389984131
Validation loss: 2.6195505178102882

Epoch: 6| Step: 3
Training loss: 2.743887186050415
Validation loss: 2.61467485274038

Epoch: 6| Step: 4
Training loss: 3.556018829345703
Validation loss: 2.6158561219451246

Epoch: 6| Step: 5
Training loss: 2.5164430141448975
Validation loss: 2.612498337222684

Epoch: 6| Step: 6
Training loss: 3.4807512760162354
Validation loss: 2.6088006291338193

Epoch: 6| Step: 7
Training loss: 2.6591193675994873
Validation loss: 2.6051360586638093

Epoch: 6| Step: 8
Training loss: 2.240138053894043
Validation loss: 2.6054192742993756

Epoch: 6| Step: 9
Training loss: 3.0124728679656982
Validation loss: 2.60674621212867

Epoch: 6| Step: 10
Training loss: 2.7478699684143066
Validation loss: 2.6084217768843456

Epoch: 6| Step: 11
Training loss: 2.3189220428466797
Validation loss: 2.606373163961595

Epoch: 6| Step: 12
Training loss: 2.40798282623291
Validation loss: 2.6132182741677887

Epoch: 6| Step: 13
Training loss: 2.55062198638916
Validation loss: 2.6171750689065583

Epoch: 92| Step: 0
Training loss: 3.0454964637756348
Validation loss: 2.6117379844829602

Epoch: 6| Step: 1
Training loss: 3.0648560523986816
Validation loss: 2.613093729942076

Epoch: 6| Step: 2
Training loss: 3.3885233402252197
Validation loss: 2.6160447571867254

Epoch: 6| Step: 3
Training loss: 1.7951724529266357
Validation loss: 2.622302727032733

Epoch: 6| Step: 4
Training loss: 3.1756091117858887
Validation loss: 2.6185388513790664

Epoch: 6| Step: 5
Training loss: 2.8746581077575684
Validation loss: 2.627461561592676

Epoch: 6| Step: 6
Training loss: 2.7941253185272217
Validation loss: 2.634305320760255

Epoch: 6| Step: 7
Training loss: 2.9452760219573975
Validation loss: 2.6133030435090423

Epoch: 6| Step: 8
Training loss: 2.6324141025543213
Validation loss: 2.6094878386425715

Epoch: 6| Step: 9
Training loss: 2.7001352310180664
Validation loss: 2.6024331097961753

Epoch: 6| Step: 10
Training loss: 2.646275281906128
Validation loss: 2.6009899339368268

Epoch: 6| Step: 11
Training loss: 2.2895660400390625
Validation loss: 2.596245701595019

Epoch: 6| Step: 12
Training loss: 2.6146256923675537
Validation loss: 2.5954115980414936

Epoch: 6| Step: 13
Training loss: 2.7788496017456055
Validation loss: 2.5959176453210975

Epoch: 93| Step: 0
Training loss: 2.7434911727905273
Validation loss: 2.5930099410395466

Epoch: 6| Step: 1
Training loss: 2.7332117557525635
Validation loss: 2.5941574419698408

Epoch: 6| Step: 2
Training loss: 3.2700862884521484
Validation loss: 2.594393114889822

Epoch: 6| Step: 3
Training loss: 2.535259485244751
Validation loss: 2.5934741266312136

Epoch: 6| Step: 4
Training loss: 3.0301620960235596
Validation loss: 2.5967922082511325

Epoch: 6| Step: 5
Training loss: 2.364779472351074
Validation loss: 2.599936763445536

Epoch: 6| Step: 6
Training loss: 2.2368416786193848
Validation loss: 2.6030276231868292

Epoch: 6| Step: 7
Training loss: 3.128095865249634
Validation loss: 2.6045774490602556

Epoch: 6| Step: 8
Training loss: 2.923891305923462
Validation loss: 2.593475005959952

Epoch: 6| Step: 9
Training loss: 2.295820474624634
Validation loss: 2.5978756207291798

Epoch: 6| Step: 10
Training loss: 3.2098681926727295
Validation loss: 2.59703109341283

Epoch: 6| Step: 11
Training loss: 2.443631410598755
Validation loss: 2.598109617028185

Epoch: 6| Step: 12
Training loss: 3.5852136611938477
Validation loss: 2.5924060780514955

Epoch: 6| Step: 13
Training loss: 1.7377772331237793
Validation loss: 2.5934706554617932

Epoch: 94| Step: 0
Training loss: 2.6054906845092773
Validation loss: 2.5937220152988227

Epoch: 6| Step: 1
Training loss: 3.743140697479248
Validation loss: 2.5994195271563787

Epoch: 6| Step: 2
Training loss: 2.8371975421905518
Validation loss: 2.597767624803769

Epoch: 6| Step: 3
Training loss: 2.6494014263153076
Validation loss: 2.606681785275859

Epoch: 6| Step: 4
Training loss: 2.9396286010742188
Validation loss: 2.6142593635025846

Epoch: 6| Step: 5
Training loss: 2.766932964324951
Validation loss: 2.6134143439672326

Epoch: 6| Step: 6
Training loss: 2.773799180984497
Validation loss: 2.608227345251268

Epoch: 6| Step: 7
Training loss: 2.253417730331421
Validation loss: 2.6117379588465535

Epoch: 6| Step: 8
Training loss: 2.5634450912475586
Validation loss: 2.6053084660601873

Epoch: 6| Step: 9
Training loss: 3.110663414001465
Validation loss: 2.600030711902085

Epoch: 6| Step: 10
Training loss: 2.793684482574463
Validation loss: 2.605287690316477

Epoch: 6| Step: 11
Training loss: 3.2785439491271973
Validation loss: 2.59822339652687

Epoch: 6| Step: 12
Training loss: 1.6688029766082764
Validation loss: 2.5991322173867175

Epoch: 6| Step: 13
Training loss: 2.638190269470215
Validation loss: 2.5976071203908613

Epoch: 95| Step: 0
Training loss: 2.0649056434631348
Validation loss: 2.592583199983002

Epoch: 6| Step: 1
Training loss: 2.9573092460632324
Validation loss: 2.5957370009473575

Epoch: 6| Step: 2
Training loss: 2.8137550354003906
Validation loss: 2.5966696739196777

Epoch: 6| Step: 3
Training loss: 3.0234036445617676
Validation loss: 2.5978687219722296

Epoch: 6| Step: 4
Training loss: 2.8048129081726074
Validation loss: 2.6037771496721493

Epoch: 6| Step: 5
Training loss: 3.3876867294311523
Validation loss: 2.6024383575685563

Epoch: 6| Step: 6
Training loss: 2.618241310119629
Validation loss: 2.6045465751360823

Epoch: 6| Step: 7
Training loss: 3.3002190589904785
Validation loss: 2.5973139475750666

Epoch: 6| Step: 8
Training loss: 3.3886313438415527
Validation loss: 2.6012533351939213

Epoch: 6| Step: 9
Training loss: 1.8994568586349487
Validation loss: 2.592101853380921

Epoch: 6| Step: 10
Training loss: 2.7384912967681885
Validation loss: 2.5908580544174358

Epoch: 6| Step: 11
Training loss: 2.389418601989746
Validation loss: 2.5937082844395793

Epoch: 6| Step: 12
Training loss: 2.769603967666626
Validation loss: 2.5931057955629084

Epoch: 6| Step: 13
Training loss: 2.341341733932495
Validation loss: 2.5885273538609987

Epoch: 96| Step: 0
Training loss: 2.7813568115234375
Validation loss: 2.5947705609824068

Epoch: 6| Step: 1
Training loss: 2.7629570960998535
Validation loss: 2.596881745963968

Epoch: 6| Step: 2
Training loss: 2.9407849311828613
Validation loss: 2.6120171777663694

Epoch: 6| Step: 3
Training loss: 2.316568374633789
Validation loss: 2.6162474437426497

Epoch: 6| Step: 4
Training loss: 2.357973098754883
Validation loss: 2.6118198620375765

Epoch: 6| Step: 5
Training loss: 2.9098801612854004
Validation loss: 2.6116069234827513

Epoch: 6| Step: 6
Training loss: 3.2731475830078125
Validation loss: 2.6043542097973567

Epoch: 6| Step: 7
Training loss: 3.0333144664764404
Validation loss: 2.603738953990321

Epoch: 6| Step: 8
Training loss: 3.441854953765869
Validation loss: 2.597410732699979

Epoch: 6| Step: 9
Training loss: 2.2387747764587402
Validation loss: 2.5966460499712216

Epoch: 6| Step: 10
Training loss: 2.790273666381836
Validation loss: 2.5885562871092107

Epoch: 6| Step: 11
Training loss: 2.4654250144958496
Validation loss: 2.586976037230543

Epoch: 6| Step: 12
Training loss: 2.771852493286133
Validation loss: 2.585283064073132

Epoch: 6| Step: 13
Training loss: 2.4932286739349365
Validation loss: 2.591927933436568

Epoch: 97| Step: 0
Training loss: 2.4741287231445312
Validation loss: 2.597779171441191

Epoch: 6| Step: 1
Training loss: 2.708047866821289
Validation loss: 2.5973691683943554

Epoch: 6| Step: 2
Training loss: 2.9212188720703125
Validation loss: 2.597647036275556

Epoch: 6| Step: 3
Training loss: 3.3601036071777344
Validation loss: 2.6063531521827943

Epoch: 6| Step: 4
Training loss: 2.822814702987671
Validation loss: 2.6106440610783075

Epoch: 6| Step: 5
Training loss: 2.804701805114746
Validation loss: 2.5998954362766717

Epoch: 6| Step: 6
Training loss: 2.936343193054199
Validation loss: 2.5891986226522796

Epoch: 6| Step: 7
Training loss: 2.9936470985412598
Validation loss: 2.5868446442388717

Epoch: 6| Step: 8
Training loss: 3.091771125793457
Validation loss: 2.5880411337780695

Epoch: 6| Step: 9
Training loss: 2.8915300369262695
Validation loss: 2.584310644416399

Epoch: 6| Step: 10
Training loss: 2.2839088439941406
Validation loss: 2.584366759946269

Epoch: 6| Step: 11
Training loss: 2.076005697250366
Validation loss: 2.582438776569982

Epoch: 6| Step: 12
Training loss: 3.433133602142334
Validation loss: 2.5924773575157247

Epoch: 6| Step: 13
Training loss: 1.1403005123138428
Validation loss: 2.590586006000478

Epoch: 98| Step: 0
Training loss: 2.6227498054504395
Validation loss: 2.5979570035011537

Epoch: 6| Step: 1
Training loss: 2.4389326572418213
Validation loss: 2.5937819198895524

Epoch: 6| Step: 2
Training loss: 2.9440054893493652
Validation loss: 2.5901529122424383

Epoch: 6| Step: 3
Training loss: 3.0138916969299316
Validation loss: 2.5840504630919425

Epoch: 6| Step: 4
Training loss: 3.0785329341888428
Validation loss: 2.588478251170087

Epoch: 6| Step: 5
Training loss: 2.3898062705993652
Validation loss: 2.587110291245163

Epoch: 6| Step: 6
Training loss: 2.500488758087158
Validation loss: 2.589373424489011

Epoch: 6| Step: 7
Training loss: 2.519336223602295
Validation loss: 2.5854066084789973

Epoch: 6| Step: 8
Training loss: 2.7247724533081055
Validation loss: 2.5839721054159184

Epoch: 6| Step: 9
Training loss: 2.9984169006347656
Validation loss: 2.5849012482550835

Epoch: 6| Step: 10
Training loss: 3.466032028198242
Validation loss: 2.5848057603323333

Epoch: 6| Step: 11
Training loss: 3.114023208618164
Validation loss: 2.581140797625306

Epoch: 6| Step: 12
Training loss: 2.0384163856506348
Validation loss: 2.5812304250655638

Epoch: 6| Step: 13
Training loss: 2.8190689086914062
Validation loss: 2.582943916320801

Epoch: 99| Step: 0
Training loss: 2.197549343109131
Validation loss: 2.5785720168903308

Epoch: 6| Step: 1
Training loss: 2.8415334224700928
Validation loss: 2.5856886063852618

Epoch: 6| Step: 2
Training loss: 2.7345714569091797
Validation loss: 2.5845299459272817

Epoch: 6| Step: 3
Training loss: 2.849623203277588
Validation loss: 2.5844704925373034

Epoch: 6| Step: 4
Training loss: 2.840160369873047
Validation loss: 2.5847151484540714

Epoch: 6| Step: 5
Training loss: 3.3838164806365967
Validation loss: 2.584049917036487

Epoch: 6| Step: 6
Training loss: 2.7894301414489746
Validation loss: 2.5829357741981425

Epoch: 6| Step: 7
Training loss: 2.625154495239258
Validation loss: 2.581523146680606

Epoch: 6| Step: 8
Training loss: 3.164968967437744
Validation loss: 2.579612869088368

Epoch: 6| Step: 9
Training loss: 2.2258942127227783
Validation loss: 2.5860477980747016

Epoch: 6| Step: 10
Training loss: 3.7978925704956055
Validation loss: 2.581098448845648

Epoch: 6| Step: 11
Training loss: 2.196286201477051
Validation loss: 2.5813775498379945

Epoch: 6| Step: 12
Training loss: 2.101600170135498
Validation loss: 2.583003733747749

Epoch: 6| Step: 13
Training loss: 2.7777798175811768
Validation loss: 2.582547213441582

Epoch: 100| Step: 0
Training loss: 2.7889328002929688
Validation loss: 2.579293515092583

Epoch: 6| Step: 1
Training loss: 1.9483797550201416
Validation loss: 2.580919208065156

Epoch: 6| Step: 2
Training loss: 2.9452667236328125
Validation loss: 2.5808537416560675

Epoch: 6| Step: 3
Training loss: 2.235027313232422
Validation loss: 2.577813212589551

Epoch: 6| Step: 4
Training loss: 3.007765769958496
Validation loss: 2.578257945276076

Epoch: 6| Step: 5
Training loss: 3.1183784008026123
Validation loss: 2.579680273609777

Epoch: 6| Step: 6
Training loss: 3.147526264190674
Validation loss: 2.586810229926981

Epoch: 6| Step: 7
Training loss: 3.0603041648864746
Validation loss: 2.587844305140998

Epoch: 6| Step: 8
Training loss: 2.8970699310302734
Validation loss: 2.59467569217887

Epoch: 6| Step: 9
Training loss: 2.4849495887756348
Validation loss: 2.5864487976156254

Epoch: 6| Step: 10
Training loss: 3.01346755027771
Validation loss: 2.5859540611185055

Epoch: 6| Step: 11
Training loss: 3.0826942920684814
Validation loss: 2.5871616204579673

Epoch: 6| Step: 12
Training loss: 2.2968106269836426
Validation loss: 2.5817765189755346

Epoch: 6| Step: 13
Training loss: 2.319518804550171
Validation loss: 2.5821473444661787

Epoch: 101| Step: 0
Training loss: 3.465301513671875
Validation loss: 2.57643554031208

Epoch: 6| Step: 1
Training loss: 3.3218741416931152
Validation loss: 2.5770847694848174

Epoch: 6| Step: 2
Training loss: 2.5449156761169434
Validation loss: 2.576892945074266

Epoch: 6| Step: 3
Training loss: 2.373748302459717
Validation loss: 2.5784955768175024

Epoch: 6| Step: 4
Training loss: 2.269366979598999
Validation loss: 2.5762082069150862

Epoch: 6| Step: 5
Training loss: 2.9245100021362305
Validation loss: 2.5789429090356313

Epoch: 6| Step: 6
Training loss: 2.3792619705200195
Validation loss: 2.5829404041331303

Epoch: 6| Step: 7
Training loss: 3.0900964736938477
Validation loss: 2.5823971174096547

Epoch: 6| Step: 8
Training loss: 3.302384376525879
Validation loss: 2.5766598563040457

Epoch: 6| Step: 9
Training loss: 2.603877067565918
Validation loss: 2.582833828464631

Epoch: 6| Step: 10
Training loss: 2.5886621475219727
Validation loss: 2.5839485635039625

Epoch: 6| Step: 11
Training loss: 2.087151527404785
Validation loss: 2.5839579643741732

Epoch: 6| Step: 12
Training loss: 2.4833340644836426
Validation loss: 2.58048173176345

Epoch: 6| Step: 13
Training loss: 3.3321704864501953
Validation loss: 2.583799610855759

Epoch: 102| Step: 0
Training loss: 3.2555480003356934
Validation loss: 2.5877151899440314

Epoch: 6| Step: 1
Training loss: 3.000206470489502
Validation loss: 2.6057844649079027

Epoch: 6| Step: 2
Training loss: 2.498324394226074
Validation loss: 2.605160754214051

Epoch: 6| Step: 3
Training loss: 2.4218597412109375
Validation loss: 2.6097085347739597

Epoch: 6| Step: 4
Training loss: 2.2308874130249023
Validation loss: 2.6186259997788297

Epoch: 6| Step: 5
Training loss: 2.5704431533813477
Validation loss: 2.610194831766108

Epoch: 6| Step: 6
Training loss: 2.7604427337646484
Validation loss: 2.623450997055218

Epoch: 6| Step: 7
Training loss: 3.151477336883545
Validation loss: 2.6141399798854703

Epoch: 6| Step: 8
Training loss: 2.933892011642456
Validation loss: 2.5953910017526276

Epoch: 6| Step: 9
Training loss: 3.00199294090271
Validation loss: 2.588386489499

Epoch: 6| Step: 10
Training loss: 2.4769225120544434
Validation loss: 2.5789815007999377

Epoch: 6| Step: 11
Training loss: 2.759631395339966
Validation loss: 2.576287502883583

Epoch: 6| Step: 12
Training loss: 2.4383997917175293
Validation loss: 2.5763427237028718

Epoch: 6| Step: 13
Training loss: 3.314154863357544
Validation loss: 2.578048090780935

Epoch: 103| Step: 0
Training loss: 1.8222728967666626
Validation loss: 2.577889347589144

Epoch: 6| Step: 1
Training loss: 3.5825629234313965
Validation loss: 2.5833329718600035

Epoch: 6| Step: 2
Training loss: 3.1131227016448975
Validation loss: 2.5990716590676257

Epoch: 6| Step: 3
Training loss: 3.116697311401367
Validation loss: 2.611759131954562

Epoch: 6| Step: 4
Training loss: 2.1848511695861816
Validation loss: 2.6065387597648044

Epoch: 6| Step: 5
Training loss: 2.3255763053894043
Validation loss: 2.602879757522255

Epoch: 6| Step: 6
Training loss: 2.9101619720458984
Validation loss: 2.5897398994814966

Epoch: 6| Step: 7
Training loss: 2.031106472015381
Validation loss: 2.5865136449055006

Epoch: 6| Step: 8
Training loss: 3.506455898284912
Validation loss: 2.5848820773504113

Epoch: 6| Step: 9
Training loss: 3.072896718978882
Validation loss: 2.5814417715995543

Epoch: 6| Step: 10
Training loss: 2.43937349319458
Validation loss: 2.5774468068153626

Epoch: 6| Step: 11
Training loss: 2.47147274017334
Validation loss: 2.580315614259371

Epoch: 6| Step: 12
Training loss: 3.3205726146698
Validation loss: 2.5814320836015927

Epoch: 6| Step: 13
Training loss: 2.5638644695281982
Validation loss: 2.5833667888436267

Epoch: 104| Step: 0
Training loss: 2.327103614807129
Validation loss: 2.5830434996594667

Epoch: 6| Step: 1
Training loss: 2.8209598064422607
Validation loss: 2.5894136351923787

Epoch: 6| Step: 2
Training loss: 2.9995341300964355
Validation loss: 2.5854592413030644

Epoch: 6| Step: 3
Training loss: 2.3739585876464844
Validation loss: 2.5808824646857476

Epoch: 6| Step: 4
Training loss: 2.5097689628601074
Validation loss: 2.587216200367097

Epoch: 6| Step: 5
Training loss: 3.440531015396118
Validation loss: 2.5867074458829817

Epoch: 6| Step: 6
Training loss: 2.139040470123291
Validation loss: 2.584922326508389

Epoch: 6| Step: 7
Training loss: 3.133075475692749
Validation loss: 2.582864094805974

Epoch: 6| Step: 8
Training loss: 3.5376648902893066
Validation loss: 2.5752989707454557

Epoch: 6| Step: 9
Training loss: 2.8499362468719482
Validation loss: 2.572868847077893

Epoch: 6| Step: 10
Training loss: 2.550414800643921
Validation loss: 2.574461770314042

Epoch: 6| Step: 11
Training loss: 2.633998394012451
Validation loss: 2.573362171009023

Epoch: 6| Step: 12
Training loss: 3.039349317550659
Validation loss: 2.57396254744581

Epoch: 6| Step: 13
Training loss: 1.616973876953125
Validation loss: 2.571798586076306

Epoch: 105| Step: 0
Training loss: 2.7370572090148926
Validation loss: 2.571869768122191

Epoch: 6| Step: 1
Training loss: 2.3728883266448975
Validation loss: 2.571222571916478

Epoch: 6| Step: 2
Training loss: 3.063174247741699
Validation loss: 2.572691309836603

Epoch: 6| Step: 3
Training loss: 3.1144235134124756
Validation loss: 2.573665770151282

Epoch: 6| Step: 4
Training loss: 2.215416431427002
Validation loss: 2.57542694768598

Epoch: 6| Step: 5
Training loss: 2.9732470512390137
Validation loss: 2.5724413933292514

Epoch: 6| Step: 6
Training loss: 2.6086032390594482
Validation loss: 2.5704603374645276

Epoch: 6| Step: 7
Training loss: 2.4736201763153076
Validation loss: 2.568744300514139

Epoch: 6| Step: 8
Training loss: 2.5477118492126465
Validation loss: 2.5711268301933043

Epoch: 6| Step: 9
Training loss: 3.0043692588806152
Validation loss: 2.570724123267717

Epoch: 6| Step: 10
Training loss: 2.92777156829834
Validation loss: 2.569331463947091

Epoch: 6| Step: 11
Training loss: 3.037943124771118
Validation loss: 2.5684965707922496

Epoch: 6| Step: 12
Training loss: 2.8873424530029297
Validation loss: 2.5647316748096096

Epoch: 6| Step: 13
Training loss: 2.2631044387817383
Validation loss: 2.5674654283831195

Epoch: 106| Step: 0
Training loss: 2.3006591796875
Validation loss: 2.5686959707608787

Epoch: 6| Step: 1
Training loss: 2.3712780475616455
Validation loss: 2.564605810308969

Epoch: 6| Step: 2
Training loss: 2.16507887840271
Validation loss: 2.5647633408987396

Epoch: 6| Step: 3
Training loss: 3.405454635620117
Validation loss: 2.570308195647373

Epoch: 6| Step: 4
Training loss: 3.031996011734009
Validation loss: 2.5708083721899215

Epoch: 6| Step: 5
Training loss: 2.632781982421875
Validation loss: 2.5655856568326234

Epoch: 6| Step: 6
Training loss: 2.718942642211914
Validation loss: 2.5679689607312604

Epoch: 6| Step: 7
Training loss: 1.7308118343353271
Validation loss: 2.5659044455456477

Epoch: 6| Step: 8
Training loss: 2.866682291030884
Validation loss: 2.5703831385540705

Epoch: 6| Step: 9
Training loss: 3.3563456535339355
Validation loss: 2.5679231395003614

Epoch: 6| Step: 10
Training loss: 2.929910659790039
Validation loss: 2.5717970684010494

Epoch: 6| Step: 11
Training loss: 2.645488739013672
Validation loss: 2.5658109213716243

Epoch: 6| Step: 12
Training loss: 3.0507171154022217
Validation loss: 2.5652616382927023

Epoch: 6| Step: 13
Training loss: 3.505859613418579
Validation loss: 2.5688544627158874

Epoch: 107| Step: 0
Training loss: 3.1521096229553223
Validation loss: 2.5625472530241935

Epoch: 6| Step: 1
Training loss: 3.351252317428589
Validation loss: 2.564449869176393

Epoch: 6| Step: 2
Training loss: 2.6244542598724365
Validation loss: 2.560651743283836

Epoch: 6| Step: 3
Training loss: 3.14827036857605
Validation loss: 2.5637761495446645

Epoch: 6| Step: 4
Training loss: 3.2740962505340576
Validation loss: 2.5615185537645893

Epoch: 6| Step: 5
Training loss: 2.180466651916504
Validation loss: 2.559006829415598

Epoch: 6| Step: 6
Training loss: 2.779106616973877
Validation loss: 2.5646473105235765

Epoch: 6| Step: 7
Training loss: 2.064258575439453
Validation loss: 2.5613871697456605

Epoch: 6| Step: 8
Training loss: 3.0235161781311035
Validation loss: 2.5617318179017756

Epoch: 6| Step: 9
Training loss: 2.536489486694336
Validation loss: 2.562945532542403

Epoch: 6| Step: 10
Training loss: 2.6481804847717285
Validation loss: 2.5592102876273533

Epoch: 6| Step: 11
Training loss: 2.551077365875244
Validation loss: 2.5622264903078795

Epoch: 6| Step: 12
Training loss: 2.7923550605773926
Validation loss: 2.564827101204985

Epoch: 6| Step: 13
Training loss: 1.6948550939559937
Validation loss: 2.564480845646192

Epoch: 108| Step: 0
Training loss: 2.965463399887085
Validation loss: 2.566944770915534

Epoch: 6| Step: 1
Training loss: 2.532320976257324
Validation loss: 2.5663967517114457

Epoch: 6| Step: 2
Training loss: 3.516178607940674
Validation loss: 2.57350000258415

Epoch: 6| Step: 3
Training loss: 2.223038673400879
Validation loss: 2.569625049509028

Epoch: 6| Step: 4
Training loss: 1.7892732620239258
Validation loss: 2.572061892478697

Epoch: 6| Step: 5
Training loss: 2.9156296253204346
Validation loss: 2.5713756238260577

Epoch: 6| Step: 6
Training loss: 2.93466854095459
Validation loss: 2.572594517020769

Epoch: 6| Step: 7
Training loss: 2.5793650150299072
Validation loss: 2.5698369395348335

Epoch: 6| Step: 8
Training loss: 3.478411912918091
Validation loss: 2.565515584843133

Epoch: 6| Step: 9
Training loss: 2.567653179168701
Validation loss: 2.5741909857719176

Epoch: 6| Step: 10
Training loss: 2.367554187774658
Validation loss: 2.5739020557813745

Epoch: 6| Step: 11
Training loss: 3.00862717628479
Validation loss: 2.567662718475506

Epoch: 6| Step: 12
Training loss: 2.817701816558838
Validation loss: 2.5707330165370816

Epoch: 6| Step: 13
Training loss: 2.554141044616699
Validation loss: 2.56334747293944

Epoch: 109| Step: 0
Training loss: 2.323147773742676
Validation loss: 2.5630698306586153

Epoch: 6| Step: 1
Training loss: 3.013180732727051
Validation loss: 2.5635926697843816

Epoch: 6| Step: 2
Training loss: 2.4537353515625
Validation loss: 2.5586270414372927

Epoch: 6| Step: 3
Training loss: 2.2442710399627686
Validation loss: 2.5629913935097317

Epoch: 6| Step: 4
Training loss: 2.762004852294922
Validation loss: 2.557734881677935

Epoch: 6| Step: 5
Training loss: 3.763841390609741
Validation loss: 2.5605405043530207

Epoch: 6| Step: 6
Training loss: 2.2905476093292236
Validation loss: 2.5605176828240834

Epoch: 6| Step: 7
Training loss: 2.9601950645446777
Validation loss: 2.5594325911614204

Epoch: 6| Step: 8
Training loss: 1.7232435941696167
Validation loss: 2.5605819045856433

Epoch: 6| Step: 9
Training loss: 2.2490620613098145
Validation loss: 2.5580794272884244

Epoch: 6| Step: 10
Training loss: 3.777195930480957
Validation loss: 2.558803545531406

Epoch: 6| Step: 11
Training loss: 3.3193740844726562
Validation loss: 2.5590112401593115

Epoch: 6| Step: 12
Training loss: 2.9075911045074463
Validation loss: 2.5591026121570217

Epoch: 6| Step: 13
Training loss: 2.2576916217803955
Validation loss: 2.5566260148120183

Epoch: 110| Step: 0
Training loss: 3.201310157775879
Validation loss: 2.5614602206855692

Epoch: 6| Step: 1
Training loss: 2.4241414070129395
Validation loss: 2.560031401213779

Epoch: 6| Step: 2
Training loss: 2.4041457176208496
Validation loss: 2.5580286800220446

Epoch: 6| Step: 3
Training loss: 2.5210022926330566
Validation loss: 2.559657217353903

Epoch: 6| Step: 4
Training loss: 2.330272912979126
Validation loss: 2.552255020346693

Epoch: 6| Step: 5
Training loss: 3.2485499382019043
Validation loss: 2.560315188541207

Epoch: 6| Step: 6
Training loss: 2.056583881378174
Validation loss: 2.559230750606906

Epoch: 6| Step: 7
Training loss: 2.665961265563965
Validation loss: 2.5611758450026154

Epoch: 6| Step: 8
Training loss: 3.0889081954956055
Validation loss: 2.5581657450686217

Epoch: 6| Step: 9
Training loss: 2.2657222747802734
Validation loss: 2.556580046171783

Epoch: 6| Step: 10
Training loss: 3.427201747894287
Validation loss: 2.5584288720161683

Epoch: 6| Step: 11
Training loss: 2.877936363220215
Validation loss: 2.5566154372307563

Epoch: 6| Step: 12
Training loss: 3.347043514251709
Validation loss: 2.5528666332203853

Epoch: 6| Step: 13
Training loss: 2.052760362625122
Validation loss: 2.555619168025191

Epoch: 111| Step: 0
Training loss: 3.037464141845703
Validation loss: 2.5565165345386793

Epoch: 6| Step: 1
Training loss: 3.130156993865967
Validation loss: 2.562988542741345

Epoch: 6| Step: 2
Training loss: 3.3679051399230957
Validation loss: 2.5581824676964873

Epoch: 6| Step: 3
Training loss: 3.4183764457702637
Validation loss: 2.559525097570112

Epoch: 6| Step: 4
Training loss: 3.0723938941955566
Validation loss: 2.557127647502448

Epoch: 6| Step: 5
Training loss: 2.8638484477996826
Validation loss: 2.5698384982283398

Epoch: 6| Step: 6
Training loss: 2.9754629135131836
Validation loss: 2.5684712497136926

Epoch: 6| Step: 7
Training loss: 2.8387248516082764
Validation loss: 2.5655341353467715

Epoch: 6| Step: 8
Training loss: 2.29704213142395
Validation loss: 2.55998589915614

Epoch: 6| Step: 9
Training loss: 1.8387234210968018
Validation loss: 2.5515869843062533

Epoch: 6| Step: 10
Training loss: 2.67087984085083
Validation loss: 2.5516767424921833

Epoch: 6| Step: 11
Training loss: 2.455167055130005
Validation loss: 2.557066730273667

Epoch: 6| Step: 12
Training loss: 2.243008852005005
Validation loss: 2.5503257269500406

Epoch: 6| Step: 13
Training loss: 1.5579638481140137
Validation loss: 2.553271142385339

Epoch: 112| Step: 0
Training loss: 2.634018898010254
Validation loss: 2.5508193533907653

Epoch: 6| Step: 1
Training loss: 2.7836077213287354
Validation loss: 2.5476247623402584

Epoch: 6| Step: 2
Training loss: 3.2308731079101562
Validation loss: 2.5547015410597607

Epoch: 6| Step: 3
Training loss: 2.35347056388855
Validation loss: 2.5537140574506534

Epoch: 6| Step: 4
Training loss: 2.53248929977417
Validation loss: 2.5553245826434066

Epoch: 6| Step: 5
Training loss: 2.0746850967407227
Validation loss: 2.5614555753687376

Epoch: 6| Step: 6
Training loss: 3.074652671813965
Validation loss: 2.5571160906104633

Epoch: 6| Step: 7
Training loss: 2.9099531173706055
Validation loss: 2.565517089700186

Epoch: 6| Step: 8
Training loss: 3.2649621963500977
Validation loss: 2.561137132747199

Epoch: 6| Step: 9
Training loss: 1.876855731010437
Validation loss: 2.5656001490931355

Epoch: 6| Step: 10
Training loss: 3.075453758239746
Validation loss: 2.55525872271548

Epoch: 6| Step: 11
Training loss: 2.4881393909454346
Validation loss: 2.561148538384386

Epoch: 6| Step: 12
Training loss: 3.0829458236694336
Validation loss: 2.5492530202352874

Epoch: 6| Step: 13
Training loss: 2.930396795272827
Validation loss: 2.5513255185978387

Epoch: 113| Step: 0
Training loss: 2.9448299407958984
Validation loss: 2.5488563045378654

Epoch: 6| Step: 1
Training loss: 2.180110454559326
Validation loss: 2.554919404368247

Epoch: 6| Step: 2
Training loss: 2.9974894523620605
Validation loss: 2.5483710458201747

Epoch: 6| Step: 3
Training loss: 3.0244555473327637
Validation loss: 2.5506520527665333

Epoch: 6| Step: 4
Training loss: 2.488999843597412
Validation loss: 2.54867717014846

Epoch: 6| Step: 5
Training loss: 2.6119489669799805
Validation loss: 2.5472421235935663

Epoch: 6| Step: 6
Training loss: 2.747453212738037
Validation loss: 2.5489347468140306

Epoch: 6| Step: 7
Training loss: 2.0250585079193115
Validation loss: 2.5521135253290974

Epoch: 6| Step: 8
Training loss: 2.9323177337646484
Validation loss: 2.551037742245582

Epoch: 6| Step: 9
Training loss: 2.776444673538208
Validation loss: 2.5511520216541905

Epoch: 6| Step: 10
Training loss: 2.910353660583496
Validation loss: 2.546700018708424

Epoch: 6| Step: 11
Training loss: 3.076371669769287
Validation loss: 2.546448871653567

Epoch: 6| Step: 12
Training loss: 3.07810115814209
Validation loss: 2.5483566586689284

Epoch: 6| Step: 13
Training loss: 2.168139934539795
Validation loss: 2.548259945325954

Epoch: 114| Step: 0
Training loss: 2.938843250274658
Validation loss: 2.544592347196353

Epoch: 6| Step: 1
Training loss: 2.529766798019409
Validation loss: 2.552502193758565

Epoch: 6| Step: 2
Training loss: 2.8609187602996826
Validation loss: 2.5493667407702376

Epoch: 6| Step: 3
Training loss: 2.805117607116699
Validation loss: 2.545423210308116

Epoch: 6| Step: 4
Training loss: 2.07130765914917
Validation loss: 2.543226057483304

Epoch: 6| Step: 5
Training loss: 2.9283134937286377
Validation loss: 2.5440947753126903

Epoch: 6| Step: 6
Training loss: 2.375520706176758
Validation loss: 2.5436989517622095

Epoch: 6| Step: 7
Training loss: 3.1369340419769287
Validation loss: 2.542658398228307

Epoch: 6| Step: 8
Training loss: 2.0475528240203857
Validation loss: 2.5447346574516705

Epoch: 6| Step: 9
Training loss: 3.057171106338501
Validation loss: 2.5468666604770127

Epoch: 6| Step: 10
Training loss: 3.0218772888183594
Validation loss: 2.5442495910070275

Epoch: 6| Step: 11
Training loss: 2.282888174057007
Validation loss: 2.5420583191738335

Epoch: 6| Step: 12
Training loss: 3.6841580867767334
Validation loss: 2.5499416987101235

Epoch: 6| Step: 13
Training loss: 2.2431154251098633
Validation loss: 2.547898131032144

Epoch: 115| Step: 0
Training loss: 2.523594379425049
Validation loss: 2.5491880652725056

Epoch: 6| Step: 1
Training loss: 2.774322509765625
Validation loss: 2.544273220082765

Epoch: 6| Step: 2
Training loss: 2.9317121505737305
Validation loss: 2.547212839126587

Epoch: 6| Step: 3
Training loss: 3.0766561031341553
Validation loss: 2.5526880295045915

Epoch: 6| Step: 4
Training loss: 3.228583335876465
Validation loss: 2.550334466400967

Epoch: 6| Step: 5
Training loss: 2.1994822025299072
Validation loss: 2.5559669771502094

Epoch: 6| Step: 6
Training loss: 2.8961124420166016
Validation loss: 2.5620876563492643

Epoch: 6| Step: 7
Training loss: 1.8977413177490234
Validation loss: 2.5607430729814755

Epoch: 6| Step: 8
Training loss: 2.642421245574951
Validation loss: 2.5652895947938323

Epoch: 6| Step: 9
Training loss: 2.531259536743164
Validation loss: 2.5712021063732844

Epoch: 6| Step: 10
Training loss: 3.491814613342285
Validation loss: 2.563582120403167

Epoch: 6| Step: 11
Training loss: 2.816307783126831
Validation loss: 2.5641705759109987

Epoch: 6| Step: 12
Training loss: 2.7932868003845215
Validation loss: 2.558210039651522

Epoch: 6| Step: 13
Training loss: 2.002788543701172
Validation loss: 2.551439357060258

Epoch: 116| Step: 0
Training loss: 2.8007631301879883
Validation loss: 2.5472309179203485

Epoch: 6| Step: 1
Training loss: 2.749629497528076
Validation loss: 2.5441090778637956

Epoch: 6| Step: 2
Training loss: 2.662139415740967
Validation loss: 2.542466578945037

Epoch: 6| Step: 3
Training loss: 3.1012887954711914
Validation loss: 2.5427595671787055

Epoch: 6| Step: 4
Training loss: 2.205833673477173
Validation loss: 2.542032698149322

Epoch: 6| Step: 5
Training loss: 3.948383092880249
Validation loss: 2.5447686026173253

Epoch: 6| Step: 6
Training loss: 2.70063853263855
Validation loss: 2.5393153441849576

Epoch: 6| Step: 7
Training loss: 2.8842482566833496
Validation loss: 2.540289799372355

Epoch: 6| Step: 8
Training loss: 2.5549063682556152
Validation loss: 2.548175405430537

Epoch: 6| Step: 9
Training loss: 2.411271572113037
Validation loss: 2.5485164285987936

Epoch: 6| Step: 10
Training loss: 2.6303393840789795
Validation loss: 2.5501113783928657

Epoch: 6| Step: 11
Training loss: 2.855862617492676
Validation loss: 2.5506715005443943

Epoch: 6| Step: 12
Training loss: 2.0290138721466064
Validation loss: 2.545522479600804

Epoch: 6| Step: 13
Training loss: 2.549774646759033
Validation loss: 2.5410744810617096

Epoch: 117| Step: 0
Training loss: 3.3992538452148438
Validation loss: 2.5454082976105394

Epoch: 6| Step: 1
Training loss: 2.7914888858795166
Validation loss: 2.5430289673548874

Epoch: 6| Step: 2
Training loss: 1.8771042823791504
Validation loss: 2.5429107578851844

Epoch: 6| Step: 3
Training loss: 3.617325782775879
Validation loss: 2.5400886843281407

Epoch: 6| Step: 4
Training loss: 3.244286060333252
Validation loss: 2.540089976403021

Epoch: 6| Step: 5
Training loss: 2.4136128425598145
Validation loss: 2.539554993311564

Epoch: 6| Step: 6
Training loss: 2.727644920349121
Validation loss: 2.539932002303421

Epoch: 6| Step: 7
Training loss: 2.250276565551758
Validation loss: 2.5437800653519167

Epoch: 6| Step: 8
Training loss: 2.128408432006836
Validation loss: 2.543451509168071

Epoch: 6| Step: 9
Training loss: 2.198115587234497
Validation loss: 2.5408277127050583

Epoch: 6| Step: 10
Training loss: 3.219681978225708
Validation loss: 2.5443203987613803

Epoch: 6| Step: 11
Training loss: 2.7088677883148193
Validation loss: 2.542642039637412

Epoch: 6| Step: 12
Training loss: 2.6471152305603027
Validation loss: 2.5413357442425144

Epoch: 6| Step: 13
Training loss: 3.038281202316284
Validation loss: 2.538238004971576

Epoch: 118| Step: 0
Training loss: 2.261383533477783
Validation loss: 2.540058902514878

Epoch: 6| Step: 1
Training loss: 2.8213138580322266
Validation loss: 2.5511050352486233

Epoch: 6| Step: 2
Training loss: 2.229292869567871
Validation loss: 2.552086781429988

Epoch: 6| Step: 3
Training loss: 3.3782856464385986
Validation loss: 2.5542864235498572

Epoch: 6| Step: 4
Training loss: 2.8965859413146973
Validation loss: 2.562219222386678

Epoch: 6| Step: 5
Training loss: 2.1636648178100586
Validation loss: 2.5635272610572075

Epoch: 6| Step: 6
Training loss: 3.287806510925293
Validation loss: 2.548176770569176

Epoch: 6| Step: 7
Training loss: 3.1290643215179443
Validation loss: 2.5566333570787982

Epoch: 6| Step: 8
Training loss: 3.1915571689605713
Validation loss: 2.548025805463073

Epoch: 6| Step: 9
Training loss: 2.2871952056884766
Validation loss: 2.54870641744265

Epoch: 6| Step: 10
Training loss: 2.463974952697754
Validation loss: 2.546624534873552

Epoch: 6| Step: 11
Training loss: 1.8309839963912964
Validation loss: 2.5501487203823623

Epoch: 6| Step: 12
Training loss: 3.4226431846618652
Validation loss: 2.552195697702387

Epoch: 6| Step: 13
Training loss: 2.7904341220855713
Validation loss: 2.5400446358547417

Epoch: 119| Step: 0
Training loss: 2.4267306327819824
Validation loss: 2.545873962422853

Epoch: 6| Step: 1
Training loss: 3.596904754638672
Validation loss: 2.5413919392452446

Epoch: 6| Step: 2
Training loss: 2.5370864868164062
Validation loss: 2.539935250436106

Epoch: 6| Step: 3
Training loss: 2.335594415664673
Validation loss: 2.541104068038284

Epoch: 6| Step: 4
Training loss: 2.8044817447662354
Validation loss: 2.543392153196437

Epoch: 6| Step: 5
Training loss: 2.209160327911377
Validation loss: 2.536728838438629

Epoch: 6| Step: 6
Training loss: 3.685007095336914
Validation loss: 2.542126881178989

Epoch: 6| Step: 7
Training loss: 3.40598726272583
Validation loss: 2.5359324947480233

Epoch: 6| Step: 8
Training loss: 1.849083662033081
Validation loss: 2.5369029762924358

Epoch: 6| Step: 9
Training loss: 2.4031929969787598
Validation loss: 2.5347987785134265

Epoch: 6| Step: 10
Training loss: 2.8247060775756836
Validation loss: 2.5356218417485556

Epoch: 6| Step: 11
Training loss: 2.533207416534424
Validation loss: 2.5393195562465216

Epoch: 6| Step: 12
Training loss: 2.4607391357421875
Validation loss: 2.540196695635396

Epoch: 6| Step: 13
Training loss: 3.155524492263794
Validation loss: 2.5372361239566597

Epoch: 120| Step: 0
Training loss: 3.222714424133301
Validation loss: 2.541672986040833

Epoch: 6| Step: 1
Training loss: 2.594271421432495
Validation loss: 2.539668434409685

Epoch: 6| Step: 2
Training loss: 3.440164089202881
Validation loss: 2.541529608029191

Epoch: 6| Step: 3
Training loss: 2.151517868041992
Validation loss: 2.5451599910695064

Epoch: 6| Step: 4
Training loss: 2.2477869987487793
Validation loss: 2.550735673596782

Epoch: 6| Step: 5
Training loss: 2.718787670135498
Validation loss: 2.5542999877724597

Epoch: 6| Step: 6
Training loss: 2.289146661758423
Validation loss: 2.5519173888749975

Epoch: 6| Step: 7
Training loss: 2.6552953720092773
Validation loss: 2.5554513982547227

Epoch: 6| Step: 8
Training loss: 2.8420820236206055
Validation loss: 2.562567541676183

Epoch: 6| Step: 9
Training loss: 2.747964382171631
Validation loss: 2.5543269957265546

Epoch: 6| Step: 10
Training loss: 3.1901936531066895
Validation loss: 2.5573555859186317

Epoch: 6| Step: 11
Training loss: 2.3992950916290283
Validation loss: 2.5504992238936888

Epoch: 6| Step: 12
Training loss: 2.494062900543213
Validation loss: 2.5446145662697415

Epoch: 6| Step: 13
Training loss: 3.401339530944824
Validation loss: 2.545170473796065

Epoch: 121| Step: 0
Training loss: 3.5128798484802246
Validation loss: 2.540822216259536

Epoch: 6| Step: 1
Training loss: 2.335292339324951
Validation loss: 2.536689732664375

Epoch: 6| Step: 2
Training loss: 2.836195707321167
Validation loss: 2.5446308376968547

Epoch: 6| Step: 3
Training loss: 2.883251428604126
Validation loss: 2.544295444283434

Epoch: 6| Step: 4
Training loss: 2.801278591156006
Validation loss: 2.547739169930899

Epoch: 6| Step: 5
Training loss: 2.5865910053253174
Validation loss: 2.552804103461645

Epoch: 6| Step: 6
Training loss: 2.3256282806396484
Validation loss: 2.5551306457929712

Epoch: 6| Step: 7
Training loss: 2.982738494873047
Validation loss: 2.566158915078768

Epoch: 6| Step: 8
Training loss: 3.3977713584899902
Validation loss: 2.5656996029679493

Epoch: 6| Step: 9
Training loss: 2.2376770973205566
Validation loss: 2.553272790806268

Epoch: 6| Step: 10
Training loss: 2.508906126022339
Validation loss: 2.5513525701338247

Epoch: 6| Step: 11
Training loss: 1.8769819736480713
Validation loss: 2.55371840025789

Epoch: 6| Step: 12
Training loss: 2.5592312812805176
Validation loss: 2.546240255396853

Epoch: 6| Step: 13
Training loss: 3.7598283290863037
Validation loss: 2.539751386129728

Epoch: 122| Step: 0
Training loss: 2.9581542015075684
Validation loss: 2.5378612343982985

Epoch: 6| Step: 1
Training loss: 2.122218608856201
Validation loss: 2.535435394574237

Epoch: 6| Step: 2
Training loss: 2.909301519393921
Validation loss: 2.5397203583871164

Epoch: 6| Step: 3
Training loss: 2.205719232559204
Validation loss: 2.5359940836506505

Epoch: 6| Step: 4
Training loss: 2.3888676166534424
Validation loss: 2.5412913573685514

Epoch: 6| Step: 5
Training loss: 2.813507080078125
Validation loss: 2.5404184095321165

Epoch: 6| Step: 6
Training loss: 4.088777542114258
Validation loss: 2.53776688473199

Epoch: 6| Step: 7
Training loss: 2.5017127990722656
Validation loss: 2.541445909007903

Epoch: 6| Step: 8
Training loss: 2.519731283187866
Validation loss: 2.553550269014092

Epoch: 6| Step: 9
Training loss: 2.832318067550659
Validation loss: 2.5515361037305606

Epoch: 6| Step: 10
Training loss: 2.411261558532715
Validation loss: 2.5414882193329515

Epoch: 6| Step: 11
Training loss: 3.0092742443084717
Validation loss: 2.5367589971070648

Epoch: 6| Step: 12
Training loss: 2.5452966690063477
Validation loss: 2.534461357260263

Epoch: 6| Step: 13
Training loss: 2.654572010040283
Validation loss: 2.5333105261607836

Epoch: 123| Step: 0
Training loss: 2.5277771949768066
Validation loss: 2.5347736215078704

Epoch: 6| Step: 1
Training loss: 3.1068570613861084
Validation loss: 2.5332697847838044

Epoch: 6| Step: 2
Training loss: 2.0741794109344482
Validation loss: 2.5312420296412643

Epoch: 6| Step: 3
Training loss: 2.2515954971313477
Validation loss: 2.5310582960805585

Epoch: 6| Step: 4
Training loss: 2.2349355220794678
Validation loss: 2.5386072922778387

Epoch: 6| Step: 5
Training loss: 2.971803665161133
Validation loss: 2.5382384625814294

Epoch: 6| Step: 6
Training loss: 2.1944127082824707
Validation loss: 2.540269228719896

Epoch: 6| Step: 7
Training loss: 3.0384721755981445
Validation loss: 2.530764102935791

Epoch: 6| Step: 8
Training loss: 2.1656999588012695
Validation loss: 2.533796642416267

Epoch: 6| Step: 9
Training loss: 3.250789165496826
Validation loss: 2.5317947710714033

Epoch: 6| Step: 10
Training loss: 3.198335886001587
Validation loss: 2.525583367193899

Epoch: 6| Step: 11
Training loss: 2.951752185821533
Validation loss: 2.5299587070301013

Epoch: 6| Step: 12
Training loss: 3.249159812927246
Validation loss: 2.5317879953692035

Epoch: 6| Step: 13
Training loss: 2.817608594894409
Validation loss: 2.5288567107210875

Epoch: 124| Step: 0
Training loss: 2.069423198699951
Validation loss: 2.5329437666041876

Epoch: 6| Step: 1
Training loss: 2.021237850189209
Validation loss: 2.5247632585546023

Epoch: 6| Step: 2
Training loss: 3.2991864681243896
Validation loss: 2.5314342616706766

Epoch: 6| Step: 3
Training loss: 2.7046594619750977
Validation loss: 2.5288915890519337

Epoch: 6| Step: 4
Training loss: 2.851552724838257
Validation loss: 2.5308192135185323

Epoch: 6| Step: 5
Training loss: 2.6702897548675537
Validation loss: 2.533169392616518

Epoch: 6| Step: 6
Training loss: 2.553377628326416
Validation loss: 2.531563256376533

Epoch: 6| Step: 7
Training loss: 2.801727533340454
Validation loss: 2.5346450498027187

Epoch: 6| Step: 8
Training loss: 2.666757822036743
Validation loss: 2.5328457663136144

Epoch: 6| Step: 9
Training loss: 3.2751989364624023
Validation loss: 2.538193900098083

Epoch: 6| Step: 10
Training loss: 3.583606243133545
Validation loss: 2.5339167271890948

Epoch: 6| Step: 11
Training loss: 2.1988556385040283
Validation loss: 2.5376548613271406

Epoch: 6| Step: 12
Training loss: 2.45377779006958
Validation loss: 2.533615614778252

Epoch: 6| Step: 13
Training loss: 2.9290263652801514
Validation loss: 2.5286693444816013

Epoch: 125| Step: 0
Training loss: 3.4522838592529297
Validation loss: 2.5277663174495903

Epoch: 6| Step: 1
Training loss: 2.1551904678344727
Validation loss: 2.526126259116716

Epoch: 6| Step: 2
Training loss: 2.631521463394165
Validation loss: 2.5274958482352634

Epoch: 6| Step: 3
Training loss: 2.782392978668213
Validation loss: 2.530082110435732

Epoch: 6| Step: 4
Training loss: 3.7271997928619385
Validation loss: 2.5326533497020765

Epoch: 6| Step: 5
Training loss: 2.9897525310516357
Validation loss: 2.535939754978303

Epoch: 6| Step: 6
Training loss: 2.225893974304199
Validation loss: 2.531502959548786

Epoch: 6| Step: 7
Training loss: 2.837672710418701
Validation loss: 2.5319752590630644

Epoch: 6| Step: 8
Training loss: 2.7013721466064453
Validation loss: 2.5338912856194282

Epoch: 6| Step: 9
Training loss: 3.2191343307495117
Validation loss: 2.5330322096424718

Epoch: 6| Step: 10
Training loss: 2.6964035034179688
Validation loss: 2.5313280884937575

Epoch: 6| Step: 11
Training loss: 2.211858034133911
Validation loss: 2.5301327397746425

Epoch: 6| Step: 12
Training loss: 2.0908637046813965
Validation loss: 2.5308739113551315

Epoch: 6| Step: 13
Training loss: 1.8900244235992432
Validation loss: 2.530919705667803

Epoch: 126| Step: 0
Training loss: 4.1716437339782715
Validation loss: 2.525046169116933

Epoch: 6| Step: 1
Training loss: 2.5207858085632324
Validation loss: 2.5258663059562765

Epoch: 6| Step: 2
Training loss: 2.461836338043213
Validation loss: 2.5287670371352986

Epoch: 6| Step: 3
Training loss: 2.45918607711792
Validation loss: 2.5307411429702595

Epoch: 6| Step: 4
Training loss: 2.5236573219299316
Validation loss: 2.533427205137027

Epoch: 6| Step: 5
Training loss: 2.300882339477539
Validation loss: 2.535676228102817

Epoch: 6| Step: 6
Training loss: 2.4692184925079346
Validation loss: 2.538679540798228

Epoch: 6| Step: 7
Training loss: 3.362940788269043
Validation loss: 2.5385908285776773

Epoch: 6| Step: 8
Training loss: 2.145325183868408
Validation loss: 2.53648865607477

Epoch: 6| Step: 9
Training loss: 2.9866256713867188
Validation loss: 2.5385915874153056

Epoch: 6| Step: 10
Training loss: 2.8525166511535645
Validation loss: 2.5287743435111096

Epoch: 6| Step: 11
Training loss: 3.111571788787842
Validation loss: 2.5233723681460143

Epoch: 6| Step: 12
Training loss: 2.419445037841797
Validation loss: 2.5210403857692594

Epoch: 6| Step: 13
Training loss: 1.5756421089172363
Validation loss: 2.5205232815075944

Epoch: 127| Step: 0
Training loss: 2.89498233795166
Validation loss: 2.523525676419658

Epoch: 6| Step: 1
Training loss: 2.57887601852417
Validation loss: 2.526966561553299

Epoch: 6| Step: 2
Training loss: 3.3639817237854004
Validation loss: 2.526997302168159

Epoch: 6| Step: 3
Training loss: 3.316192626953125
Validation loss: 2.525748319523309

Epoch: 6| Step: 4
Training loss: 2.59609317779541
Validation loss: 2.530496948508806

Epoch: 6| Step: 5
Training loss: 1.9348831176757812
Validation loss: 2.5345279170620825

Epoch: 6| Step: 6
Training loss: 3.644775152206421
Validation loss: 2.533220716702041

Epoch: 6| Step: 7
Training loss: 2.773740291595459
Validation loss: 2.5281854316752446

Epoch: 6| Step: 8
Training loss: 2.355229377746582
Validation loss: 2.531248679725073

Epoch: 6| Step: 9
Training loss: 2.4717161655426025
Validation loss: 2.530026025669549

Epoch: 6| Step: 10
Training loss: 1.7130142450332642
Validation loss: 2.5299532977483605

Epoch: 6| Step: 11
Training loss: 3.0924696922302246
Validation loss: 2.52409444573105

Epoch: 6| Step: 12
Training loss: 2.817829132080078
Validation loss: 2.523257068408433

Epoch: 6| Step: 13
Training loss: 2.3109278678894043
Validation loss: 2.5217698056210756

Epoch: 128| Step: 0
Training loss: 2.7400624752044678
Validation loss: 2.5228363929256314

Epoch: 6| Step: 1
Training loss: 2.697225332260132
Validation loss: 2.525173694856705

Epoch: 6| Step: 2
Training loss: 3.166454553604126
Validation loss: 2.5236915670415407

Epoch: 6| Step: 3
Training loss: 2.394791603088379
Validation loss: 2.5288454345477525

Epoch: 6| Step: 4
Training loss: 2.4604947566986084
Validation loss: 2.5254703644783265

Epoch: 6| Step: 5
Training loss: 2.6454148292541504
Validation loss: 2.5325556134664886

Epoch: 6| Step: 6
Training loss: 2.869994878768921
Validation loss: 2.526788006546677

Epoch: 6| Step: 7
Training loss: 2.7134850025177
Validation loss: 2.527789710670389

Epoch: 6| Step: 8
Training loss: 2.65879487991333
Validation loss: 2.5341837636886106

Epoch: 6| Step: 9
Training loss: 3.4140288829803467
Validation loss: 2.534236974613641

Epoch: 6| Step: 10
Training loss: 2.72088885307312
Validation loss: 2.5282472871964976

Epoch: 6| Step: 11
Training loss: 2.8920793533325195
Validation loss: 2.5319383631470385

Epoch: 6| Step: 12
Training loss: 2.2668964862823486
Validation loss: 2.5316874980926514

Epoch: 6| Step: 13
Training loss: 1.8208584785461426
Validation loss: 2.538404767231275

Epoch: 129| Step: 0
Training loss: 2.7393386363983154
Validation loss: 2.5312195772765786

Epoch: 6| Step: 1
Training loss: 3.4336085319519043
Validation loss: 2.5437087987058904

Epoch: 6| Step: 2
Training loss: 3.4473681449890137
Validation loss: 2.5321860467233965

Epoch: 6| Step: 3
Training loss: 3.105226755142212
Validation loss: 2.5411300377179216

Epoch: 6| Step: 4
Training loss: 2.687039613723755
Validation loss: 2.522011754333332

Epoch: 6| Step: 5
Training loss: 2.1599979400634766
Validation loss: 2.536556120841734

Epoch: 6| Step: 6
Training loss: 2.6419777870178223
Validation loss: 2.52460991438999

Epoch: 6| Step: 7
Training loss: 2.6815433502197266
Validation loss: 2.5323400625618557

Epoch: 6| Step: 8
Training loss: 2.384519100189209
Validation loss: 2.524232566997569

Epoch: 6| Step: 9
Training loss: 2.3210082054138184
Validation loss: 2.5271023024794874

Epoch: 6| Step: 10
Training loss: 2.7495572566986084
Validation loss: 2.533045507246448

Epoch: 6| Step: 11
Training loss: 2.8991951942443848
Validation loss: 2.529140333975515

Epoch: 6| Step: 12
Training loss: 2.287559986114502
Validation loss: 2.531585319067842

Epoch: 6| Step: 13
Training loss: 2.1123647689819336
Validation loss: 2.530792200437156

Epoch: 130| Step: 0
Training loss: 2.8547186851501465
Validation loss: 2.5240531762441

Epoch: 6| Step: 1
Training loss: 3.0172407627105713
Validation loss: 2.5367510728938605

Epoch: 6| Step: 2
Training loss: 2.0006637573242188
Validation loss: 2.5367321455350487

Epoch: 6| Step: 3
Training loss: 3.1283628940582275
Validation loss: 2.5295872278110956

Epoch: 6| Step: 4
Training loss: 3.1868736743927
Validation loss: 2.536831432773221

Epoch: 6| Step: 5
Training loss: 2.028398036956787
Validation loss: 2.532593615593449

Epoch: 6| Step: 6
Training loss: 2.467571258544922
Validation loss: 2.526566215740737

Epoch: 6| Step: 7
Training loss: 3.3093419075012207
Validation loss: 2.5262606118315007

Epoch: 6| Step: 8
Training loss: 2.8700923919677734
Validation loss: 2.520933258918024

Epoch: 6| Step: 9
Training loss: 2.1943440437316895
Validation loss: 2.5229941491157777

Epoch: 6| Step: 10
Training loss: 2.7603960037231445
Validation loss: 2.5162176675693964

Epoch: 6| Step: 11
Training loss: 2.2905023097991943
Validation loss: 2.5190061894796227

Epoch: 6| Step: 12
Training loss: 3.2391228675842285
Validation loss: 2.5230930184805267

Epoch: 6| Step: 13
Training loss: 2.3893518447875977
Validation loss: 2.5195974560194117

Epoch: 131| Step: 0
Training loss: 1.9907445907592773
Validation loss: 2.5226664748243106

Epoch: 6| Step: 1
Training loss: 2.573005199432373
Validation loss: 2.518464147403676

Epoch: 6| Step: 2
Training loss: 3.20072603225708
Validation loss: 2.5204777409953456

Epoch: 6| Step: 3
Training loss: 2.684504270553589
Validation loss: 2.5203363792870634

Epoch: 6| Step: 4
Training loss: 2.1347107887268066
Validation loss: 2.5181513499188166

Epoch: 6| Step: 5
Training loss: 2.9679222106933594
Validation loss: 2.51787079534223

Epoch: 6| Step: 6
Training loss: 2.4209282398223877
Validation loss: 2.5137304234248337

Epoch: 6| Step: 7
Training loss: 3.0682501792907715
Validation loss: 2.517295245201357

Epoch: 6| Step: 8
Training loss: 2.883683443069458
Validation loss: 2.519257281416206

Epoch: 6| Step: 9
Training loss: 3.328854560852051
Validation loss: 2.5138335689421623

Epoch: 6| Step: 10
Training loss: 3.091583728790283
Validation loss: 2.5153445838600077

Epoch: 6| Step: 11
Training loss: 2.704739570617676
Validation loss: 2.519021398277693

Epoch: 6| Step: 12
Training loss: 2.4033055305480957
Validation loss: 2.5155801952526136

Epoch: 6| Step: 13
Training loss: 2.0460925102233887
Validation loss: 2.5175216864514094

Epoch: 132| Step: 0
Training loss: 3.182121753692627
Validation loss: 2.515042963848319

Epoch: 6| Step: 1
Training loss: 2.6778035163879395
Validation loss: 2.5169181926276094

Epoch: 6| Step: 2
Training loss: 2.4544262886047363
Validation loss: 2.5158663667658323

Epoch: 6| Step: 3
Training loss: 3.4079363346099854
Validation loss: 2.515189540001654

Epoch: 6| Step: 4
Training loss: 2.602269172668457
Validation loss: 2.5233520205302904

Epoch: 6| Step: 5
Training loss: 3.1111724376678467
Validation loss: 2.5240789895416587

Epoch: 6| Step: 6
Training loss: 1.722689151763916
Validation loss: 2.522388671034126

Epoch: 6| Step: 7
Training loss: 2.1068379878997803
Validation loss: 2.5312157907793598

Epoch: 6| Step: 8
Training loss: 2.797421455383301
Validation loss: 2.5220059963964645

Epoch: 6| Step: 9
Training loss: 3.001016616821289
Validation loss: 2.5248358403482745

Epoch: 6| Step: 10
Training loss: 2.265089988708496
Validation loss: 2.5232226361510572

Epoch: 6| Step: 11
Training loss: 2.8070907592773438
Validation loss: 2.519417688410769

Epoch: 6| Step: 12
Training loss: 2.9979069232940674
Validation loss: 2.5172627895109114

Epoch: 6| Step: 13
Training loss: 2.714150905609131
Validation loss: 2.517162243525187

Epoch: 133| Step: 0
Training loss: 1.9831900596618652
Validation loss: 2.5159287811607443

Epoch: 6| Step: 1
Training loss: 3.5800986289978027
Validation loss: 2.5169431522328365

Epoch: 6| Step: 2
Training loss: 2.7809829711914062
Validation loss: 2.5152020351861113

Epoch: 6| Step: 3
Training loss: 2.5482025146484375
Validation loss: 2.51826722134826

Epoch: 6| Step: 4
Training loss: 2.255335807800293
Validation loss: 2.5188901014225458

Epoch: 6| Step: 5
Training loss: 2.051237106323242
Validation loss: 2.5142446922999557

Epoch: 6| Step: 6
Training loss: 2.623577117919922
Validation loss: 2.514055282838883

Epoch: 6| Step: 7
Training loss: 3.578158140182495
Validation loss: 2.514620691217402

Epoch: 6| Step: 8
Training loss: 2.42059063911438
Validation loss: 2.515441481785108

Epoch: 6| Step: 9
Training loss: 2.4899377822875977
Validation loss: 2.5116990176580285

Epoch: 6| Step: 10
Training loss: 3.213188409805298
Validation loss: 2.5107891508328017

Epoch: 6| Step: 11
Training loss: 3.04807710647583
Validation loss: 2.5119369952909407

Epoch: 6| Step: 12
Training loss: 2.6663825511932373
Validation loss: 2.513286293193858

Epoch: 6| Step: 13
Training loss: 2.5183613300323486
Validation loss: 2.5105158641774166

Epoch: 134| Step: 0
Training loss: 3.5936379432678223
Validation loss: 2.507851714728981

Epoch: 6| Step: 1
Training loss: 2.115124225616455
Validation loss: 2.5079883503657516

Epoch: 6| Step: 2
Training loss: 2.5195670127868652
Validation loss: 2.5128066821764876

Epoch: 6| Step: 3
Training loss: 3.165548086166382
Validation loss: 2.509233913113994

Epoch: 6| Step: 4
Training loss: 3.3725924491882324
Validation loss: 2.5076657469554613

Epoch: 6| Step: 5
Training loss: 2.9548394680023193
Validation loss: 2.5116849663437053

Epoch: 6| Step: 6
Training loss: 2.631876230239868
Validation loss: 2.509145011184036

Epoch: 6| Step: 7
Training loss: 2.730795383453369
Validation loss: 2.5074731944709696

Epoch: 6| Step: 8
Training loss: 1.7887678146362305
Validation loss: 2.5102673551087737

Epoch: 6| Step: 9
Training loss: 2.766573667526245
Validation loss: 2.5110169277396253

Epoch: 6| Step: 10
Training loss: 2.704387664794922
Validation loss: 2.508346314071327

Epoch: 6| Step: 11
Training loss: 2.4990320205688477
Validation loss: 2.5072395340088875

Epoch: 6| Step: 12
Training loss: 2.6600961685180664
Validation loss: 2.509319677147814

Epoch: 6| Step: 13
Training loss: 1.95137357711792
Validation loss: 2.509069086402975

Epoch: 135| Step: 0
Training loss: 2.9048571586608887
Validation loss: 2.5109672264386247

Epoch: 6| Step: 1
Training loss: 2.46623158454895
Validation loss: 2.510782298221383

Epoch: 6| Step: 2
Training loss: 2.993013381958008
Validation loss: 2.507922823711108

Epoch: 6| Step: 3
Training loss: 2.315279960632324
Validation loss: 2.5098351611885974

Epoch: 6| Step: 4
Training loss: 3.3148984909057617
Validation loss: 2.5076976976087018

Epoch: 6| Step: 5
Training loss: 1.9501738548278809
Validation loss: 2.5063027925388788

Epoch: 6| Step: 6
Training loss: 3.530153274536133
Validation loss: 2.509311729861844

Epoch: 6| Step: 7
Training loss: 3.066344976425171
Validation loss: 2.506011801381265

Epoch: 6| Step: 8
Training loss: 2.4658379554748535
Validation loss: 2.505787093152282

Epoch: 6| Step: 9
Training loss: 1.9593786001205444
Validation loss: 2.507330617597026

Epoch: 6| Step: 10
Training loss: 2.1785261631011963
Validation loss: 2.5102172590071157

Epoch: 6| Step: 11
Training loss: 3.0656683444976807
Validation loss: 2.5125921080189366

Epoch: 6| Step: 12
Training loss: 2.4569993019104004
Validation loss: 2.5141303436730498

Epoch: 6| Step: 13
Training loss: 3.3795273303985596
Validation loss: 2.515394631252494

Epoch: 136| Step: 0
Training loss: 3.35101580619812
Validation loss: 2.5140351274962067

Epoch: 6| Step: 1
Training loss: 3.212451219558716
Validation loss: 2.517218510309855

Epoch: 6| Step: 2
Training loss: 2.1826493740081787
Validation loss: 2.51485276222229

Epoch: 6| Step: 3
Training loss: 3.7745933532714844
Validation loss: 2.5107389111672678

Epoch: 6| Step: 4
Training loss: 2.022780179977417
Validation loss: 2.5129811712490615

Epoch: 6| Step: 5
Training loss: 2.9679453372955322
Validation loss: 2.514413365753748

Epoch: 6| Step: 6
Training loss: 2.3883328437805176
Validation loss: 2.5170366123158443

Epoch: 6| Step: 7
Training loss: 2.826230764389038
Validation loss: 2.514198262204406

Epoch: 6| Step: 8
Training loss: 2.6898365020751953
Validation loss: 2.51607374734776

Epoch: 6| Step: 9
Training loss: 2.703352212905884
Validation loss: 2.513209673666185

Epoch: 6| Step: 10
Training loss: 2.565805196762085
Validation loss: 2.5162964354279223

Epoch: 6| Step: 11
Training loss: 2.8655874729156494
Validation loss: 2.515865474618891

Epoch: 6| Step: 12
Training loss: 1.5344380140304565
Validation loss: 2.518213379767633

Epoch: 6| Step: 13
Training loss: 2.6187968254089355
Validation loss: 2.5084607678074993

Epoch: 137| Step: 0
Training loss: 2.5485758781433105
Validation loss: 2.5121512464297715

Epoch: 6| Step: 1
Training loss: 2.9744834899902344
Validation loss: 2.518163770757696

Epoch: 6| Step: 2
Training loss: 3.180166006088257
Validation loss: 2.5175478150767665

Epoch: 6| Step: 3
Training loss: 2.630748748779297
Validation loss: 2.5050774569152505

Epoch: 6| Step: 4
Training loss: 3.2212271690368652
Validation loss: 2.5084580554757068

Epoch: 6| Step: 5
Training loss: 1.8059718608856201
Validation loss: 2.506156393276748

Epoch: 6| Step: 6
Training loss: 3.429999828338623
Validation loss: 2.506816059030512

Epoch: 6| Step: 7
Training loss: 2.389080762863159
Validation loss: 2.506498613665181

Epoch: 6| Step: 8
Training loss: 2.766911506652832
Validation loss: 2.5078167223161265

Epoch: 6| Step: 9
Training loss: 2.4984540939331055
Validation loss: 2.507640154130997

Epoch: 6| Step: 10
Training loss: 3.3492980003356934
Validation loss: 2.507179908854987

Epoch: 6| Step: 11
Training loss: 2.2654449939727783
Validation loss: 2.5118778354378155

Epoch: 6| Step: 12
Training loss: 1.8443430662155151
Validation loss: 2.5053550274141374

Epoch: 6| Step: 13
Training loss: 2.9636647701263428
Validation loss: 2.5083678691617903

Epoch: 138| Step: 0
Training loss: 2.540700912475586
Validation loss: 2.513733643357472

Epoch: 6| Step: 1
Training loss: 3.0878472328186035
Validation loss: 2.512296953508931

Epoch: 6| Step: 2
Training loss: 2.661644458770752
Validation loss: 2.5188109451724636

Epoch: 6| Step: 3
Training loss: 2.5199995040893555
Validation loss: 2.519704818725586

Epoch: 6| Step: 4
Training loss: 3.4819247722625732
Validation loss: 2.5211830446797032

Epoch: 6| Step: 5
Training loss: 2.8848752975463867
Validation loss: 2.526346065664804

Epoch: 6| Step: 6
Training loss: 2.016331195831299
Validation loss: 2.5185807981798725

Epoch: 6| Step: 7
Training loss: 2.8036692142486572
Validation loss: 2.5096016417267504

Epoch: 6| Step: 8
Training loss: 2.411911964416504
Validation loss: 2.5151155661511164

Epoch: 6| Step: 9
Training loss: 2.5850911140441895
Validation loss: 2.5049039010078675

Epoch: 6| Step: 10
Training loss: 2.510718822479248
Validation loss: 2.5049784362957044

Epoch: 6| Step: 11
Training loss: 3.2795798778533936
Validation loss: 2.504860003789266

Epoch: 6| Step: 12
Training loss: 2.0320067405700684
Validation loss: 2.50818141301473

Epoch: 6| Step: 13
Training loss: 3.1022543907165527
Validation loss: 2.508393697841193

Epoch: 139| Step: 0
Training loss: 2.7170510292053223
Validation loss: 2.507442202619327

Epoch: 6| Step: 1
Training loss: 2.938119888305664
Validation loss: 2.509936922339983

Epoch: 6| Step: 2
Training loss: 2.645836114883423
Validation loss: 2.505574628870974

Epoch: 6| Step: 3
Training loss: 3.0512256622314453
Validation loss: 2.5087703094687512

Epoch: 6| Step: 4
Training loss: 2.5538840293884277
Validation loss: 2.5072066681359404

Epoch: 6| Step: 5
Training loss: 2.5024662017822266
Validation loss: 2.506764281180597

Epoch: 6| Step: 6
Training loss: 2.503624677658081
Validation loss: 2.506626216314172

Epoch: 6| Step: 7
Training loss: 1.7687264680862427
Validation loss: 2.509347418303131

Epoch: 6| Step: 8
Training loss: 2.652146100997925
Validation loss: 2.5061427136903167

Epoch: 6| Step: 9
Training loss: 2.7195167541503906
Validation loss: 2.5054440395806425

Epoch: 6| Step: 10
Training loss: 2.0870420932769775
Validation loss: 2.5074564615885415

Epoch: 6| Step: 11
Training loss: 3.2245874404907227
Validation loss: 2.5074361498637865

Epoch: 6| Step: 12
Training loss: 3.2070021629333496
Validation loss: 2.511926022909021

Epoch: 6| Step: 13
Training loss: 3.4645862579345703
Validation loss: 2.509233877223025

Epoch: 140| Step: 0
Training loss: 2.8888163566589355
Validation loss: 2.507895403010871

Epoch: 6| Step: 1
Training loss: 2.5115654468536377
Validation loss: 2.5133255015137377

Epoch: 6| Step: 2
Training loss: 2.7545762062072754
Validation loss: 2.5103994441288773

Epoch: 6| Step: 3
Training loss: 2.6969106197357178
Validation loss: 2.511776931824223

Epoch: 6| Step: 4
Training loss: 2.289869785308838
Validation loss: 2.510430566726192

Epoch: 6| Step: 5
Training loss: 2.7310962677001953
Validation loss: 2.5126683994006087

Epoch: 6| Step: 6
Training loss: 2.977100372314453
Validation loss: 2.512899121930522

Epoch: 6| Step: 7
Training loss: 2.4149627685546875
Validation loss: 2.5176042408071537

Epoch: 6| Step: 8
Training loss: 2.819697380065918
Validation loss: 2.5201085511074273

Epoch: 6| Step: 9
Training loss: 2.727327585220337
Validation loss: 2.5196588321398665

Epoch: 6| Step: 10
Training loss: 3.27044677734375
Validation loss: 2.514087438583374

Epoch: 6| Step: 11
Training loss: 3.1090774536132812
Validation loss: 2.514595554720971

Epoch: 6| Step: 12
Training loss: 2.5538241863250732
Validation loss: 2.5086149425916773

Epoch: 6| Step: 13
Training loss: 1.3147013187408447
Validation loss: 2.5061417856524066

Epoch: 141| Step: 0
Training loss: 2.602647066116333
Validation loss: 2.508374547445646

Epoch: 6| Step: 1
Training loss: 3.1105852127075195
Validation loss: 2.5007898602434384

Epoch: 6| Step: 2
Training loss: 2.708284854888916
Validation loss: 2.5044474806836856

Epoch: 6| Step: 3
Training loss: 3.2839486598968506
Validation loss: 2.509140068484891

Epoch: 6| Step: 4
Training loss: 2.3046932220458984
Validation loss: 2.514820903860113

Epoch: 6| Step: 5
Training loss: 2.4052155017852783
Validation loss: 2.503769895081879

Epoch: 6| Step: 6
Training loss: 3.60860538482666
Validation loss: 2.5153952670353714

Epoch: 6| Step: 7
Training loss: 2.948869466781616
Validation loss: 2.509677284507341

Epoch: 6| Step: 8
Training loss: 2.0558767318725586
Validation loss: 2.513836496619768

Epoch: 6| Step: 9
Training loss: 2.612365961074829
Validation loss: 2.510737826747279

Epoch: 6| Step: 10
Training loss: 2.0358777046203613
Validation loss: 2.502212416741156

Epoch: 6| Step: 11
Training loss: 2.4326913356781006
Validation loss: 2.5049133839145785

Epoch: 6| Step: 12
Training loss: 3.362818717956543
Validation loss: 2.5024927021354757

Epoch: 6| Step: 13
Training loss: 1.7922570705413818
Validation loss: 2.5009221774275585

Epoch: 142| Step: 0
Training loss: 2.290410041809082
Validation loss: 2.4991766406643774

Epoch: 6| Step: 1
Training loss: 2.8533935546875
Validation loss: 2.5000020227124615

Epoch: 6| Step: 2
Training loss: 2.8742141723632812
Validation loss: 2.5035872946503344

Epoch: 6| Step: 3
Training loss: 2.661224365234375
Validation loss: 2.499529077160743

Epoch: 6| Step: 4
Training loss: 2.618382215499878
Validation loss: 2.506644013107464

Epoch: 6| Step: 5
Training loss: 2.791104316711426
Validation loss: 2.503494985641972

Epoch: 6| Step: 6
Training loss: 2.2936089038848877
Validation loss: 2.5052193544244252

Epoch: 6| Step: 7
Training loss: 3.23858642578125
Validation loss: 2.4990505479997203

Epoch: 6| Step: 8
Training loss: 2.68983793258667
Validation loss: 2.504443348094981

Epoch: 6| Step: 9
Training loss: 3.0831220149993896
Validation loss: 2.4974739064452467

Epoch: 6| Step: 10
Training loss: 1.6481976509094238
Validation loss: 2.5026058971240954

Epoch: 6| Step: 11
Training loss: 2.247950792312622
Validation loss: 2.4963166085622643

Epoch: 6| Step: 12
Training loss: 3.203425407409668
Validation loss: 2.495517538439843

Epoch: 6| Step: 13
Training loss: 3.6618380546569824
Validation loss: 2.497857701393866

Epoch: 143| Step: 0
Training loss: 2.4538216590881348
Validation loss: 2.4988762614547566

Epoch: 6| Step: 1
Training loss: 2.362596273422241
Validation loss: 2.499578029878678

Epoch: 6| Step: 2
Training loss: 1.555094599723816
Validation loss: 2.499179683705812

Epoch: 6| Step: 3
Training loss: 2.018190622329712
Validation loss: 2.501797470995175

Epoch: 6| Step: 4
Training loss: 2.6996140480041504
Validation loss: 2.5119611537584694

Epoch: 6| Step: 5
Training loss: 2.6782543659210205
Validation loss: 2.508702178155222

Epoch: 6| Step: 6
Training loss: 3.247711420059204
Validation loss: 2.5173799145606255

Epoch: 6| Step: 7
Training loss: 3.0711910724639893
Validation loss: 2.512320828694169

Epoch: 6| Step: 8
Training loss: 3.0770461559295654
Validation loss: 2.5084262099317325

Epoch: 6| Step: 9
Training loss: 2.4290270805358887
Validation loss: 2.517714972137123

Epoch: 6| Step: 10
Training loss: 3.4191620349884033
Validation loss: 2.5048688406585367

Epoch: 6| Step: 11
Training loss: 4.006508827209473
Validation loss: 2.5150572253811743

Epoch: 6| Step: 12
Training loss: 2.076341152191162
Validation loss: 2.507434627061249

Epoch: 6| Step: 13
Training loss: 2.4540932178497314
Validation loss: 2.508217224510767

Epoch: 144| Step: 0
Training loss: 3.6738646030426025
Validation loss: 2.50076663237746

Epoch: 6| Step: 1
Training loss: 2.425424814224243
Validation loss: 2.5019569781518753

Epoch: 6| Step: 2
Training loss: 2.2919299602508545
Validation loss: 2.497786762893841

Epoch: 6| Step: 3
Training loss: 2.3614342212677
Validation loss: 2.5024415549411567

Epoch: 6| Step: 4
Training loss: 2.333512783050537
Validation loss: 2.5057774487362114

Epoch: 6| Step: 5
Training loss: 2.647573947906494
Validation loss: 2.511517240155128

Epoch: 6| Step: 6
Training loss: 2.420529842376709
Validation loss: 2.5073856205068608

Epoch: 6| Step: 7
Training loss: 3.0815553665161133
Validation loss: 2.510472333559426

Epoch: 6| Step: 8
Training loss: 3.1154427528381348
Validation loss: 2.5085600832457184

Epoch: 6| Step: 9
Training loss: 2.8388147354125977
Validation loss: 2.5067120136753207

Epoch: 6| Step: 10
Training loss: 3.1250665187835693
Validation loss: 2.5079076931040776

Epoch: 6| Step: 11
Training loss: 2.329268455505371
Validation loss: 2.5077559435239403

Epoch: 6| Step: 12
Training loss: 2.313220739364624
Validation loss: 2.5061076892319547

Epoch: 6| Step: 13
Training loss: 2.826615333557129
Validation loss: 2.5101703392562045

Epoch: 145| Step: 0
Training loss: 3.004349946975708
Validation loss: 2.5180322713749383

Epoch: 6| Step: 1
Training loss: 2.176504611968994
Validation loss: 2.5104740614532144

Epoch: 6| Step: 2
Training loss: 2.30104923248291
Validation loss: 2.5100857416788735

Epoch: 6| Step: 3
Training loss: 2.6149539947509766
Validation loss: 2.5038924396678968

Epoch: 6| Step: 4
Training loss: 2.7656807899475098
Validation loss: 2.505868040105348

Epoch: 6| Step: 5
Training loss: 2.3003616333007812
Validation loss: 2.5138964832469983

Epoch: 6| Step: 6
Training loss: 3.2049663066864014
Validation loss: 2.504312325549382

Epoch: 6| Step: 7
Training loss: 2.9861836433410645
Validation loss: 2.5086937309593282

Epoch: 6| Step: 8
Training loss: 2.27097749710083
Validation loss: 2.4976354286234868

Epoch: 6| Step: 9
Training loss: 2.532466173171997
Validation loss: 2.498092289893858

Epoch: 6| Step: 10
Training loss: 3.1228270530700684
Validation loss: 2.4939291836113058

Epoch: 6| Step: 11
Training loss: 3.2606818675994873
Validation loss: 2.4939778709924347

Epoch: 6| Step: 12
Training loss: 2.312014102935791
Validation loss: 2.494696496635355

Epoch: 6| Step: 13
Training loss: 2.956892967224121
Validation loss: 2.49418217648742

Epoch: 146| Step: 0
Training loss: 2.667124032974243
Validation loss: 2.5011326343782487

Epoch: 6| Step: 1
Training loss: 3.0348944664001465
Validation loss: 2.5106533855520268

Epoch: 6| Step: 2
Training loss: 2.6337645053863525
Validation loss: 2.5120303553919636

Epoch: 6| Step: 3
Training loss: 3.309225082397461
Validation loss: 2.514434322234123

Epoch: 6| Step: 4
Training loss: 2.5198092460632324
Validation loss: 2.520049648900186

Epoch: 6| Step: 5
Training loss: 2.6247596740722656
Validation loss: 2.5197212824257473

Epoch: 6| Step: 6
Training loss: 2.7512271404266357
Validation loss: 2.500869143393732

Epoch: 6| Step: 7
Training loss: 1.866678237915039
Validation loss: 2.5027317744429394

Epoch: 6| Step: 8
Training loss: 2.759512424468994
Validation loss: 2.4950181386804067

Epoch: 6| Step: 9
Training loss: 2.62253999710083
Validation loss: 2.4949311017990112

Epoch: 6| Step: 10
Training loss: 2.8421459197998047
Validation loss: 2.4914616102813394

Epoch: 6| Step: 11
Training loss: 2.915571689605713
Validation loss: 2.48943522925018

Epoch: 6| Step: 12
Training loss: 2.5148415565490723
Validation loss: 2.4913779766328874

Epoch: 6| Step: 13
Training loss: 2.5918264389038086
Validation loss: 2.494069109680832

Epoch: 147| Step: 0
Training loss: 2.778461456298828
Validation loss: 2.49262697081412

Epoch: 6| Step: 1
Training loss: 2.417855978012085
Validation loss: 2.4939126994020198

Epoch: 6| Step: 2
Training loss: 3.141474962234497
Validation loss: 2.4985947455129316

Epoch: 6| Step: 3
Training loss: 3.8425636291503906
Validation loss: 2.5152753001900128

Epoch: 6| Step: 4
Training loss: 2.3648016452789307
Validation loss: 2.5031878896938857

Epoch: 6| Step: 5
Training loss: 2.785372257232666
Validation loss: 2.503997818116219

Epoch: 6| Step: 6
Training loss: 2.1696691513061523
Validation loss: 2.4960019127015145

Epoch: 6| Step: 7
Training loss: 2.4174768924713135
Validation loss: 2.4970075109953522

Epoch: 6| Step: 8
Training loss: 2.8088901042938232
Validation loss: 2.494041940217377

Epoch: 6| Step: 9
Training loss: 2.7305643558502197
Validation loss: 2.4892166840132846

Epoch: 6| Step: 10
Training loss: 3.5052335262298584
Validation loss: 2.490255735253775

Epoch: 6| Step: 11
Training loss: 2.3916733264923096
Validation loss: 2.492439657129267

Epoch: 6| Step: 12
Training loss: 2.25593638420105
Validation loss: 2.488717640599897

Epoch: 6| Step: 13
Training loss: 1.6069852113723755
Validation loss: 2.4863972304969706

Epoch: 148| Step: 0
Training loss: 2.9351754188537598
Validation loss: 2.4860505083555817

Epoch: 6| Step: 1
Training loss: 2.640174388885498
Validation loss: 2.4878536988330144

Epoch: 6| Step: 2
Training loss: 3.0491116046905518
Validation loss: 2.490724825089978

Epoch: 6| Step: 3
Training loss: 2.687347412109375
Validation loss: 2.484983418577461

Epoch: 6| Step: 4
Training loss: 3.060609817504883
Validation loss: 2.4872216973253476

Epoch: 6| Step: 5
Training loss: 1.882652759552002
Validation loss: 2.484462668818812

Epoch: 6| Step: 6
Training loss: 3.6450648307800293
Validation loss: 2.488477455672397

Epoch: 6| Step: 7
Training loss: 2.573395252227783
Validation loss: 2.483474798099969

Epoch: 6| Step: 8
Training loss: 1.8133394718170166
Validation loss: 2.488777929736722

Epoch: 6| Step: 9
Training loss: 2.55930233001709
Validation loss: 2.4870347463956444

Epoch: 6| Step: 10
Training loss: 2.274685859680176
Validation loss: 2.4889904299089984

Epoch: 6| Step: 11
Training loss: 2.9531025886535645
Validation loss: 2.4907871677029516

Epoch: 6| Step: 12
Training loss: 2.991781234741211
Validation loss: 2.4898003224403626

Epoch: 6| Step: 13
Training loss: 2.3314366340637207
Validation loss: 2.4937028910524104

Epoch: 149| Step: 0
Training loss: 3.049574136734009
Validation loss: 2.4912169697464153

Epoch: 6| Step: 1
Training loss: 2.257150650024414
Validation loss: 2.4893984615161853

Epoch: 6| Step: 2
Training loss: 2.8871874809265137
Validation loss: 2.4917648543593702

Epoch: 6| Step: 3
Training loss: 3.1098368167877197
Validation loss: 2.4940625185607583

Epoch: 6| Step: 4
Training loss: 2.1794586181640625
Validation loss: 2.4904635721637356

Epoch: 6| Step: 5
Training loss: 2.847024917602539
Validation loss: 2.492265203947662

Epoch: 6| Step: 6
Training loss: 3.2357125282287598
Validation loss: 2.4846875641935613

Epoch: 6| Step: 7
Training loss: 2.8724961280822754
Validation loss: 2.4908271630605063

Epoch: 6| Step: 8
Training loss: 2.5385947227478027
Validation loss: 2.488703804631387

Epoch: 6| Step: 9
Training loss: 3.212153911590576
Validation loss: 2.488896326352191

Epoch: 6| Step: 10
Training loss: 1.8014514446258545
Validation loss: 2.49177812504512

Epoch: 6| Step: 11
Training loss: 2.169948101043701
Validation loss: 2.4939952178667952

Epoch: 6| Step: 12
Training loss: 2.7626194953918457
Validation loss: 2.4911533530040453

Epoch: 6| Step: 13
Training loss: 2.5576274394989014
Validation loss: 2.491278914995091

Epoch: 150| Step: 0
Training loss: 2.431666374206543
Validation loss: 2.4973864145176385

Epoch: 6| Step: 1
Training loss: 3.6541478633880615
Validation loss: 2.4974269815670547

Epoch: 6| Step: 2
Training loss: 3.6546730995178223
Validation loss: 2.493225395038564

Epoch: 6| Step: 3
Training loss: 1.9598519802093506
Validation loss: 2.496376642616846

Epoch: 6| Step: 4
Training loss: 1.967934012413025
Validation loss: 2.4899127675640966

Epoch: 6| Step: 5
Training loss: 2.8590872287750244
Validation loss: 2.4959640374747654

Epoch: 6| Step: 6
Training loss: 2.4554922580718994
Validation loss: 2.4902184112097627

Epoch: 6| Step: 7
Training loss: 2.513026237487793
Validation loss: 2.492169234060472

Epoch: 6| Step: 8
Training loss: 2.413558006286621
Validation loss: 2.500276788588493

Epoch: 6| Step: 9
Training loss: 2.4914965629577637
Validation loss: 2.4968154507298626

Epoch: 6| Step: 10
Training loss: 2.922968864440918
Validation loss: 2.491392029229031

Epoch: 6| Step: 11
Training loss: 2.6167654991149902
Validation loss: 2.491185970203851

Epoch: 6| Step: 12
Training loss: 2.8456783294677734
Validation loss: 2.4927578997868363

Epoch: 6| Step: 13
Training loss: 2.818209648132324
Validation loss: 2.4926192017011743

Epoch: 151| Step: 0
Training loss: 1.7291897535324097
Validation loss: 2.4870615249039023

Epoch: 6| Step: 1
Training loss: 3.226255416870117
Validation loss: 2.489765827373792

Epoch: 6| Step: 2
Training loss: 2.08193302154541
Validation loss: 2.4919311769547

Epoch: 6| Step: 3
Training loss: 2.4256460666656494
Validation loss: 2.4934077006514355

Epoch: 6| Step: 4
Training loss: 3.2511837482452393
Validation loss: 2.4990119805899997

Epoch: 6| Step: 5
Training loss: 3.806476593017578
Validation loss: 2.4932488805504254

Epoch: 6| Step: 6
Training loss: 3.2614798545837402
Validation loss: 2.4892996126605618

Epoch: 6| Step: 7
Training loss: 2.043196201324463
Validation loss: 2.4895448043782222

Epoch: 6| Step: 8
Training loss: 3.1416640281677246
Validation loss: 2.482966599925872

Epoch: 6| Step: 9
Training loss: 1.6186519861221313
Validation loss: 2.4858611347854778

Epoch: 6| Step: 10
Training loss: 2.3920116424560547
Validation loss: 2.4871399274436374

Epoch: 6| Step: 11
Training loss: 3.21968936920166
Validation loss: 2.4842363890781196

Epoch: 6| Step: 12
Training loss: 2.2454981803894043
Validation loss: 2.4881382244889454

Epoch: 6| Step: 13
Training loss: 3.2729618549346924
Validation loss: 2.4919410649166314

Epoch: 152| Step: 0
Training loss: 2.69392728805542
Validation loss: 2.4918238321940103

Epoch: 6| Step: 1
Training loss: 2.916027545928955
Validation loss: 2.4915838395395586

Epoch: 6| Step: 2
Training loss: 2.6857967376708984
Validation loss: 2.4987502456993185

Epoch: 6| Step: 3
Training loss: 2.866156816482544
Validation loss: 2.499584746617143

Epoch: 6| Step: 4
Training loss: 2.0099849700927734
Validation loss: 2.4999800907668246

Epoch: 6| Step: 5
Training loss: 1.9705865383148193
Validation loss: 2.496803040145546

Epoch: 6| Step: 6
Training loss: 2.899949789047241
Validation loss: 2.4937232027771654

Epoch: 6| Step: 7
Training loss: 2.9224929809570312
Validation loss: 2.488971487168343

Epoch: 6| Step: 8
Training loss: 2.9707679748535156
Validation loss: 2.4920678805279475

Epoch: 6| Step: 9
Training loss: 2.856146812438965
Validation loss: 2.4910920358473256

Epoch: 6| Step: 10
Training loss: 2.998141288757324
Validation loss: 2.4916686524627027

Epoch: 6| Step: 11
Training loss: 3.0178184509277344
Validation loss: 2.491753150058049

Epoch: 6| Step: 12
Training loss: 1.5477402210235596
Validation loss: 2.4909334208375666

Epoch: 6| Step: 13
Training loss: 3.353864908218384
Validation loss: 2.4922108957844396

Epoch: 153| Step: 0
Training loss: 3.052211284637451
Validation loss: 2.493155117957823

Epoch: 6| Step: 1
Training loss: 2.8433938026428223
Validation loss: 2.490653380270927

Epoch: 6| Step: 2
Training loss: 2.9918065071105957
Validation loss: 2.4890779397820912

Epoch: 6| Step: 3
Training loss: 2.373666763305664
Validation loss: 2.4949637023351525

Epoch: 6| Step: 4
Training loss: 3.012089729309082
Validation loss: 2.491465227578276

Epoch: 6| Step: 5
Training loss: 2.6513304710388184
Validation loss: 2.4887668753183014

Epoch: 6| Step: 6
Training loss: 2.3382043838500977
Validation loss: 2.4895058524224067

Epoch: 6| Step: 7
Training loss: 3.4481563568115234
Validation loss: 2.492597592774258

Epoch: 6| Step: 8
Training loss: 2.0606160163879395
Validation loss: 2.4872973349786576

Epoch: 6| Step: 9
Training loss: 1.998591423034668
Validation loss: 2.487016890638618

Epoch: 6| Step: 10
Training loss: 3.496891975402832
Validation loss: 2.485202104814591

Epoch: 6| Step: 11
Training loss: 2.6707301139831543
Validation loss: 2.4846597897109164

Epoch: 6| Step: 12
Training loss: 1.886324405670166
Validation loss: 2.488349712023171

Epoch: 6| Step: 13
Training loss: 2.7055463790893555
Validation loss: 2.488358759110974

Epoch: 154| Step: 0
Training loss: 2.5294528007507324
Validation loss: 2.488827197782455

Epoch: 6| Step: 1
Training loss: 2.736837863922119
Validation loss: 2.4887366833225375

Epoch: 6| Step: 2
Training loss: 3.0365445613861084
Validation loss: 2.4871342207795832

Epoch: 6| Step: 3
Training loss: 3.0372681617736816
Validation loss: 2.49032768126457

Epoch: 6| Step: 4
Training loss: 2.6535401344299316
Validation loss: 2.4954732592387865

Epoch: 6| Step: 5
Training loss: 2.395132541656494
Validation loss: 2.488385276127887

Epoch: 6| Step: 6
Training loss: 2.0740346908569336
Validation loss: 2.4947150189389466

Epoch: 6| Step: 7
Training loss: 2.675652503967285
Validation loss: 2.490417262559296

Epoch: 6| Step: 8
Training loss: 2.71378755569458
Validation loss: 2.489869681737756

Epoch: 6| Step: 9
Training loss: 2.5724663734436035
Validation loss: 2.4822567278339016

Epoch: 6| Step: 10
Training loss: 2.971290111541748
Validation loss: 2.4821958131687616

Epoch: 6| Step: 11
Training loss: 2.586510181427002
Validation loss: 2.483546749238045

Epoch: 6| Step: 12
Training loss: 2.742323875427246
Validation loss: 2.484111810243258

Epoch: 6| Step: 13
Training loss: 2.6972949504852295
Validation loss: 2.4848279081365114

Epoch: 155| Step: 0
Training loss: 2.061157703399658
Validation loss: 2.488183529146256

Epoch: 6| Step: 1
Training loss: 3.380037784576416
Validation loss: 2.4861251513163247

Epoch: 6| Step: 2
Training loss: 3.043452739715576
Validation loss: 2.4815064168745473

Epoch: 6| Step: 3
Training loss: 2.520021438598633
Validation loss: 2.491950793932843

Epoch: 6| Step: 4
Training loss: 3.072248935699463
Validation loss: 2.4890281154263403

Epoch: 6| Step: 5
Training loss: 2.5882890224456787
Validation loss: 2.489384928057271

Epoch: 6| Step: 6
Training loss: 2.16694974899292
Validation loss: 2.492758092059884

Epoch: 6| Step: 7
Training loss: 2.8268556594848633
Validation loss: 2.489196974744079

Epoch: 6| Step: 8
Training loss: 3.3060531616210938
Validation loss: 2.4939520666676183

Epoch: 6| Step: 9
Training loss: 2.8227150440216064
Validation loss: 2.4884302334118913

Epoch: 6| Step: 10
Training loss: 1.970080852508545
Validation loss: 2.485248742565032

Epoch: 6| Step: 11
Training loss: 2.884526252746582
Validation loss: 2.4853597379499868

Epoch: 6| Step: 12
Training loss: 2.386645793914795
Validation loss: 2.4886708362128145

Epoch: 6| Step: 13
Training loss: 2.1325976848602295
Validation loss: 2.4836601082996657

Epoch: 156| Step: 0
Training loss: 3.309067726135254
Validation loss: 2.4847348505450833

Epoch: 6| Step: 1
Training loss: 3.2917351722717285
Validation loss: 2.4849058633209555

Epoch: 6| Step: 2
Training loss: 1.8462233543395996
Validation loss: 2.483979445631786

Epoch: 6| Step: 3
Training loss: 2.708026885986328
Validation loss: 2.4852613274769118

Epoch: 6| Step: 4
Training loss: 3.1628060340881348
Validation loss: 2.485972632643997

Epoch: 6| Step: 5
Training loss: 2.4818100929260254
Validation loss: 2.483347395414947

Epoch: 6| Step: 6
Training loss: 2.2840514183044434
Validation loss: 2.485915809549311

Epoch: 6| Step: 7
Training loss: 2.1880030632019043
Validation loss: 2.493141184570969

Epoch: 6| Step: 8
Training loss: 1.9115352630615234
Validation loss: 2.4924857308787685

Epoch: 6| Step: 9
Training loss: 2.2380073070526123
Validation loss: 2.49223732179211

Epoch: 6| Step: 10
Training loss: 3.3749938011169434
Validation loss: 2.4916641532733874

Epoch: 6| Step: 11
Training loss: 3.314323902130127
Validation loss: 2.4871473261105117

Epoch: 6| Step: 12
Training loss: 2.6294870376586914
Validation loss: 2.4891020277495026

Epoch: 6| Step: 13
Training loss: 2.6129150390625
Validation loss: 2.479133085537982

Epoch: 157| Step: 0
Training loss: 2.04560923576355
Validation loss: 2.478707087937222

Epoch: 6| Step: 1
Training loss: 2.240934133529663
Validation loss: 2.48278522235091

Epoch: 6| Step: 2
Training loss: 3.151592969894409
Validation loss: 2.4837230097862983

Epoch: 6| Step: 3
Training loss: 3.6683855056762695
Validation loss: 2.486727809393278

Epoch: 6| Step: 4
Training loss: 2.994263172149658
Validation loss: 2.4872905054400043

Epoch: 6| Step: 5
Training loss: 3.2639172077178955
Validation loss: 2.4831087307263444

Epoch: 6| Step: 6
Training loss: 2.813441276550293
Validation loss: 2.483719159198064

Epoch: 6| Step: 7
Training loss: 2.9132370948791504
Validation loss: 2.4834750865095403

Epoch: 6| Step: 8
Training loss: 2.3092427253723145
Validation loss: 2.4810072773246357

Epoch: 6| Step: 9
Training loss: 2.3000736236572266
Validation loss: 2.4834027277526034

Epoch: 6| Step: 10
Training loss: 3.228019952774048
Validation loss: 2.479184445514474

Epoch: 6| Step: 11
Training loss: 2.166224956512451
Validation loss: 2.4809587924711165

Epoch: 6| Step: 12
Training loss: 2.320507049560547
Validation loss: 2.4833864909346386

Epoch: 6| Step: 13
Training loss: 1.379705548286438
Validation loss: 2.4905698632681244

Epoch: 158| Step: 0
Training loss: 2.3112242221832275
Validation loss: 2.5162643591562905

Epoch: 6| Step: 1
Training loss: 2.965500831604004
Validation loss: 2.537958170778008

Epoch: 6| Step: 2
Training loss: 3.0757083892822266
Validation loss: 2.5523679769167336

Epoch: 6| Step: 3
Training loss: 3.006917953491211
Validation loss: 2.5524312219312115

Epoch: 6| Step: 4
Training loss: 3.0188426971435547
Validation loss: 2.55483518620973

Epoch: 6| Step: 5
Training loss: 2.6252357959747314
Validation loss: 2.548910394791634

Epoch: 6| Step: 6
Training loss: 2.1539769172668457
Validation loss: 2.550881870331303

Epoch: 6| Step: 7
Training loss: 2.201172113418579
Validation loss: 2.5429861545562744

Epoch: 6| Step: 8
Training loss: 3.0516772270202637
Validation loss: 2.5387385276056107

Epoch: 6| Step: 9
Training loss: 2.0634756088256836
Validation loss: 2.5448655261788318

Epoch: 6| Step: 10
Training loss: 2.6399741172790527
Validation loss: 2.5431482509900163

Epoch: 6| Step: 11
Training loss: 3.469453811645508
Validation loss: 2.548837818125243

Epoch: 6| Step: 12
Training loss: 2.4693002700805664
Validation loss: 2.547147099689771

Epoch: 6| Step: 13
Training loss: 2.8786749839782715
Validation loss: 2.561867031999814

Epoch: 159| Step: 0
Training loss: 2.8266210556030273
Validation loss: 2.565940474951139

Epoch: 6| Step: 1
Training loss: 2.7108969688415527
Validation loss: 2.5551291255540747

Epoch: 6| Step: 2
Training loss: 3.0116126537323
Validation loss: 2.537478462342293

Epoch: 6| Step: 3
Training loss: 2.268049716949463
Validation loss: 2.5324918762330086

Epoch: 6| Step: 4
Training loss: 2.681274890899658
Validation loss: 2.53482880643619

Epoch: 6| Step: 5
Training loss: 2.5642807483673096
Validation loss: 2.527393930701799

Epoch: 6| Step: 6
Training loss: 2.925410747528076
Validation loss: 2.5299936058700725

Epoch: 6| Step: 7
Training loss: 3.0269179344177246
Validation loss: 2.5234638490984516

Epoch: 6| Step: 8
Training loss: 3.4639182090759277
Validation loss: 2.5270280812376287

Epoch: 6| Step: 9
Training loss: 2.350658893585205
Validation loss: 2.5297200961779525

Epoch: 6| Step: 10
Training loss: 3.5285515785217285
Validation loss: 2.527909630088396

Epoch: 6| Step: 11
Training loss: 2.2904844284057617
Validation loss: 2.52985357469128

Epoch: 6| Step: 12
Training loss: 2.1778650283813477
Validation loss: 2.5323650170398015

Epoch: 6| Step: 13
Training loss: 1.508474349975586
Validation loss: 2.5347739291447464

Epoch: 160| Step: 0
Training loss: 2.4855217933654785
Validation loss: 2.5453811640380533

Epoch: 6| Step: 1
Training loss: 2.6865694522857666
Validation loss: 2.5413197855795584

Epoch: 6| Step: 2
Training loss: 3.101994276046753
Validation loss: 2.550962842920775

Epoch: 6| Step: 3
Training loss: 2.5079386234283447
Validation loss: 2.544234252745105

Epoch: 6| Step: 4
Training loss: 4.347431182861328
Validation loss: 2.542554501564272

Epoch: 6| Step: 5
Training loss: 1.9233180284500122
Validation loss: 2.538792653750348

Epoch: 6| Step: 6
Training loss: 1.7072912454605103
Validation loss: 2.5311786154265046

Epoch: 6| Step: 7
Training loss: 2.5716311931610107
Validation loss: 2.528204023197133

Epoch: 6| Step: 8
Training loss: 3.0775861740112305
Validation loss: 2.523117673012518

Epoch: 6| Step: 9
Training loss: 2.7530837059020996
Validation loss: 2.5242106273610103

Epoch: 6| Step: 10
Training loss: 2.839298725128174
Validation loss: 2.5245224711715535

Epoch: 6| Step: 11
Training loss: 2.2821779251098633
Validation loss: 2.5221481348878596

Epoch: 6| Step: 12
Training loss: 2.6201086044311523
Validation loss: 2.520704120718023

Epoch: 6| Step: 13
Training loss: 3.3118464946746826
Validation loss: 2.517625493388022

Epoch: 161| Step: 0
Training loss: 2.188962936401367
Validation loss: 2.5097354919679704

Epoch: 6| Step: 1
Training loss: 2.2390313148498535
Validation loss: 2.5117661799153974

Epoch: 6| Step: 2
Training loss: 2.669039249420166
Validation loss: 2.515289660423033

Epoch: 6| Step: 3
Training loss: 3.247922897338867
Validation loss: 2.51781097022436

Epoch: 6| Step: 4
Training loss: 2.661226272583008
Validation loss: 2.491502546495007

Epoch: 6| Step: 5
Training loss: 3.2393956184387207
Validation loss: 2.4905625671468754

Epoch: 6| Step: 6
Training loss: 2.2799010276794434
Validation loss: 2.4928243852430776

Epoch: 6| Step: 7
Training loss: 2.560514211654663
Validation loss: 2.4980679481260237

Epoch: 6| Step: 8
Training loss: 3.0360801219940186
Validation loss: 2.494066256348805

Epoch: 6| Step: 9
Training loss: 3.022341251373291
Validation loss: 2.485339895371468

Epoch: 6| Step: 10
Training loss: 2.955148696899414
Validation loss: 2.493078834267073

Epoch: 6| Step: 11
Training loss: 2.330639600753784
Validation loss: 2.4798492590586343

Epoch: 6| Step: 12
Training loss: 2.542351245880127
Validation loss: 2.486203009082425

Epoch: 6| Step: 13
Training loss: 2.3467607498168945
Validation loss: 2.4818694360794558

Epoch: 162| Step: 0
Training loss: 2.3528127670288086
Validation loss: 2.4744053758600706

Epoch: 6| Step: 1
Training loss: 3.2956490516662598
Validation loss: 2.4785641393353863

Epoch: 6| Step: 2
Training loss: 2.611085891723633
Validation loss: 2.4764411782705658

Epoch: 6| Step: 3
Training loss: 2.65433931350708
Validation loss: 2.4820730942551807

Epoch: 6| Step: 4
Training loss: 2.1433935165405273
Validation loss: 2.479319146884385

Epoch: 6| Step: 5
Training loss: 2.4765286445617676
Validation loss: 2.479067278164689

Epoch: 6| Step: 6
Training loss: 2.4694511890411377
Validation loss: 2.485178211683868

Epoch: 6| Step: 7
Training loss: 2.7619469165802
Validation loss: 2.4895354957990747

Epoch: 6| Step: 8
Training loss: 3.4783289432525635
Validation loss: 2.4881911303407405

Epoch: 6| Step: 9
Training loss: 3.3788225650787354
Validation loss: 2.493805980169645

Epoch: 6| Step: 10
Training loss: 2.638270378112793
Validation loss: 2.491122127861105

Epoch: 6| Step: 11
Training loss: 2.6149582862854004
Validation loss: 2.49876473539619

Epoch: 6| Step: 12
Training loss: 2.2396466732025146
Validation loss: 2.489717880884806

Epoch: 6| Step: 13
Training loss: 2.0274620056152344
Validation loss: 2.493222480179161

Epoch: 163| Step: 0
Training loss: 2.6559667587280273
Validation loss: 2.480112055296539

Epoch: 6| Step: 1
Training loss: 2.6103408336639404
Validation loss: 2.471822584829023

Epoch: 6| Step: 2
Training loss: 2.2409005165100098
Validation loss: 2.4784031670580626

Epoch: 6| Step: 3
Training loss: 3.005547046661377
Validation loss: 2.4782578406795377

Epoch: 6| Step: 4
Training loss: 2.754446029663086
Validation loss: 2.480278884210894

Epoch: 6| Step: 5
Training loss: 3.1955623626708984
Validation loss: 2.4743263644556843

Epoch: 6| Step: 6
Training loss: 2.370540142059326
Validation loss: 2.4723110980885004

Epoch: 6| Step: 7
Training loss: 3.1529674530029297
Validation loss: 2.4680672384077504

Epoch: 6| Step: 8
Training loss: 2.508695602416992
Validation loss: 2.4679833432679534

Epoch: 6| Step: 9
Training loss: 2.2970807552337646
Validation loss: 2.4672998407835602

Epoch: 6| Step: 10
Training loss: 2.8535382747650146
Validation loss: 2.4640734144436416

Epoch: 6| Step: 11
Training loss: 2.718386650085449
Validation loss: 2.465620260084829

Epoch: 6| Step: 12
Training loss: 2.667384147644043
Validation loss: 2.464839409756404

Epoch: 6| Step: 13
Training loss: 2.1573312282562256
Validation loss: 2.4601129998442945

Epoch: 164| Step: 0
Training loss: 2.5368833541870117
Validation loss: 2.461309766256681

Epoch: 6| Step: 1
Training loss: 2.3416748046875
Validation loss: 2.4592531932297574

Epoch: 6| Step: 2
Training loss: 2.395872116088867
Validation loss: 2.4627616097850185

Epoch: 6| Step: 3
Training loss: 3.3778553009033203
Validation loss: 2.4630542185998734

Epoch: 6| Step: 4
Training loss: 2.489417314529419
Validation loss: 2.459736721490019

Epoch: 6| Step: 5
Training loss: 3.1609978675842285
Validation loss: 2.4639674848125828

Epoch: 6| Step: 6
Training loss: 2.1326346397399902
Validation loss: 2.4669859973333215

Epoch: 6| Step: 7
Training loss: 2.372039318084717
Validation loss: 2.4632158228146133

Epoch: 6| Step: 8
Training loss: 2.092569351196289
Validation loss: 2.4601318413211453

Epoch: 6| Step: 9
Training loss: 2.6329262256622314
Validation loss: 2.46451203028361

Epoch: 6| Step: 10
Training loss: 2.9330873489379883
Validation loss: 2.4739527676695134

Epoch: 6| Step: 11
Training loss: 2.578352451324463
Validation loss: 2.469679501748854

Epoch: 6| Step: 12
Training loss: 3.225717067718506
Validation loss: 2.471008103380921

Epoch: 6| Step: 13
Training loss: 3.470738410949707
Validation loss: 2.470738108440112

Epoch: 165| Step: 0
Training loss: 2.641936779022217
Validation loss: 2.479095494875344

Epoch: 6| Step: 1
Training loss: 2.8022379875183105
Validation loss: 2.4846111959026707

Epoch: 6| Step: 2
Training loss: 2.8842098712921143
Validation loss: 2.4816313815373245

Epoch: 6| Step: 3
Training loss: 2.5571913719177246
Validation loss: 2.498759064623105

Epoch: 6| Step: 4
Training loss: 2.1386404037475586
Validation loss: 2.4965763681678363

Epoch: 6| Step: 5
Training loss: 3.834291934967041
Validation loss: 2.4928642626731627

Epoch: 6| Step: 6
Training loss: 2.0178751945495605
Validation loss: 2.494055217312228

Epoch: 6| Step: 7
Training loss: 2.5853681564331055
Validation loss: 2.4938967561209076

Epoch: 6| Step: 8
Training loss: 2.467397451400757
Validation loss: 2.4853869202316448

Epoch: 6| Step: 9
Training loss: 3.1599111557006836
Validation loss: 2.4808715748530563

Epoch: 6| Step: 10
Training loss: 2.3563356399536133
Validation loss: 2.4774086321553876

Epoch: 6| Step: 11
Training loss: 3.004882335662842
Validation loss: 2.4760363640323764

Epoch: 6| Step: 12
Training loss: 2.6013128757476807
Validation loss: 2.477721973132062

Epoch: 6| Step: 13
Training loss: 2.3157103061676025
Validation loss: 2.480097734799949

Epoch: 166| Step: 0
Training loss: 2.2657861709594727
Validation loss: 2.4694964488347373

Epoch: 6| Step: 1
Training loss: 2.6609716415405273
Validation loss: 2.4680782876988894

Epoch: 6| Step: 2
Training loss: 2.6990878582000732
Validation loss: 2.4682221105021815

Epoch: 6| Step: 3
Training loss: 2.2221622467041016
Validation loss: 2.4665366808573403

Epoch: 6| Step: 4
Training loss: 2.813392162322998
Validation loss: 2.4660725183384393

Epoch: 6| Step: 5
Training loss: 2.3853089809417725
Validation loss: 2.467244550745974

Epoch: 6| Step: 6
Training loss: 2.766904830932617
Validation loss: 2.4644335021254835

Epoch: 6| Step: 7
Training loss: 2.705005645751953
Validation loss: 2.463409598155688

Epoch: 6| Step: 8
Training loss: 2.691175699234009
Validation loss: 2.4610856066467943

Epoch: 6| Step: 9
Training loss: 2.5366005897521973
Validation loss: 2.4592058786781887

Epoch: 6| Step: 10
Training loss: 2.865631103515625
Validation loss: 2.4575255481145715

Epoch: 6| Step: 11
Training loss: 2.594573974609375
Validation loss: 2.45830044182398

Epoch: 6| Step: 12
Training loss: 3.0145316123962402
Validation loss: 2.454492125459897

Epoch: 6| Step: 13
Training loss: 3.4539296627044678
Validation loss: 2.4622310105190484

Epoch: 167| Step: 0
Training loss: 2.28861141204834
Validation loss: 2.457669129935644

Epoch: 6| Step: 1
Training loss: 1.73481285572052
Validation loss: 2.4642524847420315

Epoch: 6| Step: 2
Training loss: 2.88653564453125
Validation loss: 2.4610428323027906

Epoch: 6| Step: 3
Training loss: 2.89101243019104
Validation loss: 2.458689038471509

Epoch: 6| Step: 4
Training loss: 2.7050318717956543
Validation loss: 2.456550746835688

Epoch: 6| Step: 5
Training loss: 3.0605592727661133
Validation loss: 2.4602209650060183

Epoch: 6| Step: 6
Training loss: 2.3349690437316895
Validation loss: 2.458104161806004

Epoch: 6| Step: 7
Training loss: 3.5173966884613037
Validation loss: 2.459824941491568

Epoch: 6| Step: 8
Training loss: 2.5250296592712402
Validation loss: 2.468334359507407

Epoch: 6| Step: 9
Training loss: 3.490623950958252
Validation loss: 2.4690520071214244

Epoch: 6| Step: 10
Training loss: 2.554976463317871
Validation loss: 2.4696666425274265

Epoch: 6| Step: 11
Training loss: 2.8196394443511963
Validation loss: 2.4718335931019118

Epoch: 6| Step: 12
Training loss: 2.4905307292938232
Validation loss: 2.4663340199378228

Epoch: 6| Step: 13
Training loss: 1.6127676963806152
Validation loss: 2.4648875626184608

Epoch: 168| Step: 0
Training loss: 3.610222101211548
Validation loss: 2.4727774845656527

Epoch: 6| Step: 1
Training loss: 2.135352849960327
Validation loss: 2.4804575007448912

Epoch: 6| Step: 2
Training loss: 2.2253270149230957
Validation loss: 2.4774275415687153

Epoch: 6| Step: 3
Training loss: 2.719542980194092
Validation loss: 2.4810085552994923

Epoch: 6| Step: 4
Training loss: 3.031754970550537
Validation loss: 2.491150274071642

Epoch: 6| Step: 5
Training loss: 3.2768731117248535
Validation loss: 2.488244669411772

Epoch: 6| Step: 6
Training loss: 2.805178165435791
Validation loss: 2.4943318854096117

Epoch: 6| Step: 7
Training loss: 1.835242509841919
Validation loss: 2.4937150068180536

Epoch: 6| Step: 8
Training loss: 2.5885539054870605
Validation loss: 2.4918210250075146

Epoch: 6| Step: 9
Training loss: 3.0156302452087402
Validation loss: 2.492723736711728

Epoch: 6| Step: 10
Training loss: 2.3009207248687744
Validation loss: 2.4667126388959986

Epoch: 6| Step: 11
Training loss: 3.498629093170166
Validation loss: 2.4618130037861485

Epoch: 6| Step: 12
Training loss: 2.0302255153656006
Validation loss: 2.462669018776186

Epoch: 6| Step: 13
Training loss: 2.1669600009918213
Validation loss: 2.4638244618651686

Epoch: 169| Step: 0
Training loss: 3.1916913986206055
Validation loss: 2.462629448982977

Epoch: 6| Step: 1
Training loss: 2.5023374557495117
Validation loss: 2.4737982878121

Epoch: 6| Step: 2
Training loss: 2.7695140838623047
Validation loss: 2.4603927725104877

Epoch: 6| Step: 3
Training loss: 1.995434284210205
Validation loss: 2.4699935579812653

Epoch: 6| Step: 4
Training loss: 2.78882098197937
Validation loss: 2.473690971251457

Epoch: 6| Step: 5
Training loss: 2.1416244506835938
Validation loss: 2.4722781283881075

Epoch: 6| Step: 6
Training loss: 2.8340675830841064
Validation loss: 2.48473874984249

Epoch: 6| Step: 7
Training loss: 2.3166656494140625
Validation loss: 2.485179331994826

Epoch: 6| Step: 8
Training loss: 3.4691739082336426
Validation loss: 2.488535104259368

Epoch: 6| Step: 9
Training loss: 2.5669848918914795
Validation loss: 2.478768158984441

Epoch: 6| Step: 10
Training loss: 2.358447551727295
Validation loss: 2.478242535744944

Epoch: 6| Step: 11
Training loss: 3.060159683227539
Validation loss: 2.4710973796024116

Epoch: 6| Step: 12
Training loss: 2.6959409713745117
Validation loss: 2.4655123756777857

Epoch: 6| Step: 13
Training loss: 2.5438125133514404
Validation loss: 2.4652284473501225

Epoch: 170| Step: 0
Training loss: 2.331465244293213
Validation loss: 2.4719390305139686

Epoch: 6| Step: 1
Training loss: 2.5104711055755615
Validation loss: 2.466920409151303

Epoch: 6| Step: 2
Training loss: 3.0805158615112305
Validation loss: 2.4654377339988627

Epoch: 6| Step: 3
Training loss: 3.2523577213287354
Validation loss: 2.47089881794427

Epoch: 6| Step: 4
Training loss: 2.6802825927734375
Validation loss: 2.4645302013684343

Epoch: 6| Step: 5
Training loss: 2.3718442916870117
Validation loss: 2.461208728051955

Epoch: 6| Step: 6
Training loss: 2.5269670486450195
Validation loss: 2.4561028070347284

Epoch: 6| Step: 7
Training loss: 2.379004955291748
Validation loss: 2.4554453101209415

Epoch: 6| Step: 8
Training loss: 2.7678256034851074
Validation loss: 2.453754637831001

Epoch: 6| Step: 9
Training loss: 2.2108054161071777
Validation loss: 2.4581688065682687

Epoch: 6| Step: 10
Training loss: 2.514200210571289
Validation loss: 2.461274890489476

Epoch: 6| Step: 11
Training loss: 2.421518325805664
Validation loss: 2.4663962753870154

Epoch: 6| Step: 12
Training loss: 3.6236743927001953
Validation loss: 2.463056231057772

Epoch: 6| Step: 13
Training loss: 2.499345541000366
Validation loss: 2.472481007217079

Epoch: 171| Step: 0
Training loss: 2.8295116424560547
Validation loss: 2.4699091270405757

Epoch: 6| Step: 1
Training loss: 2.121647357940674
Validation loss: 2.4702214938338085

Epoch: 6| Step: 2
Training loss: 2.507829427719116
Validation loss: 2.470252101139356

Epoch: 6| Step: 3
Training loss: 3.6976428031921387
Validation loss: 2.4729712701612905

Epoch: 6| Step: 4
Training loss: 2.2663516998291016
Validation loss: 2.4702394829001477

Epoch: 6| Step: 5
Training loss: 2.1329212188720703
Validation loss: 2.48157516858911

Epoch: 6| Step: 6
Training loss: 2.058356285095215
Validation loss: 2.4745430561804

Epoch: 6| Step: 7
Training loss: 2.493504047393799
Validation loss: 2.4738296719007593

Epoch: 6| Step: 8
Training loss: 3.32924485206604
Validation loss: 2.474129246127221

Epoch: 6| Step: 9
Training loss: 2.216928005218506
Validation loss: 2.4699173845270628

Epoch: 6| Step: 10
Training loss: 3.029174327850342
Validation loss: 2.470973296832013

Epoch: 6| Step: 11
Training loss: 2.9605493545532227
Validation loss: 2.469269278228924

Epoch: 6| Step: 12
Training loss: 3.2386977672576904
Validation loss: 2.4692284650700067

Epoch: 6| Step: 13
Training loss: 2.09330415725708
Validation loss: 2.4588566980054303

Epoch: 172| Step: 0
Training loss: 2.785374402999878
Validation loss: 2.459626038869222

Epoch: 6| Step: 1
Training loss: 1.833330512046814
Validation loss: 2.45693483660298

Epoch: 6| Step: 2
Training loss: 2.7610437870025635
Validation loss: 2.4622268753667034

Epoch: 6| Step: 3
Training loss: 3.0744829177856445
Validation loss: 2.461255265820411

Epoch: 6| Step: 4
Training loss: 2.522736072540283
Validation loss: 2.459882608024023

Epoch: 6| Step: 5
Training loss: 2.9416534900665283
Validation loss: 2.460093677684825

Epoch: 6| Step: 6
Training loss: 2.194983959197998
Validation loss: 2.461878340731385

Epoch: 6| Step: 7
Training loss: 2.6080987453460693
Validation loss: 2.459603378849645

Epoch: 6| Step: 8
Training loss: 2.6654629707336426
Validation loss: 2.460460452623265

Epoch: 6| Step: 9
Training loss: 2.9657840728759766
Validation loss: 2.4661194714166785

Epoch: 6| Step: 10
Training loss: 2.9914259910583496
Validation loss: 2.4696015286189255

Epoch: 6| Step: 11
Training loss: 2.7351725101470947
Validation loss: 2.4773703646916214

Epoch: 6| Step: 12
Training loss: 2.114210367202759
Validation loss: 2.4708306917580227

Epoch: 6| Step: 13
Training loss: 3.391392946243286
Validation loss: 2.473134720197288

Epoch: 173| Step: 0
Training loss: 2.7833495140075684
Validation loss: 2.472383568363805

Epoch: 6| Step: 1
Training loss: 2.4652719497680664
Validation loss: 2.470618078785558

Epoch: 6| Step: 2
Training loss: 2.484339714050293
Validation loss: 2.4660828536556614

Epoch: 6| Step: 3
Training loss: 3.4964141845703125
Validation loss: 2.4638508263454644

Epoch: 6| Step: 4
Training loss: 2.780709981918335
Validation loss: 2.4611385304440736

Epoch: 6| Step: 5
Training loss: 2.4777402877807617
Validation loss: 2.4573113482485534

Epoch: 6| Step: 6
Training loss: 2.239999294281006
Validation loss: 2.4559715768342376

Epoch: 6| Step: 7
Training loss: 2.651089668273926
Validation loss: 2.4594248828067573

Epoch: 6| Step: 8
Training loss: 3.283700942993164
Validation loss: 2.4575772567461898

Epoch: 6| Step: 9
Training loss: 3.0626742839813232
Validation loss: 2.4591184764780025

Epoch: 6| Step: 10
Training loss: 3.0197978019714355
Validation loss: 2.457120595439788

Epoch: 6| Step: 11
Training loss: 1.7589480876922607
Validation loss: 2.4578343309381956

Epoch: 6| Step: 12
Training loss: 2.099128246307373
Validation loss: 2.4548249590781426

Epoch: 6| Step: 13
Training loss: 2.488720417022705
Validation loss: 2.4594326711470083

Epoch: 174| Step: 0
Training loss: 2.66542387008667
Validation loss: 2.4642824229373725

Epoch: 6| Step: 1
Training loss: 2.7627620697021484
Validation loss: 2.4598401951533493

Epoch: 6| Step: 2
Training loss: 2.4489383697509766
Validation loss: 2.465951309409193

Epoch: 6| Step: 3
Training loss: 2.3927438259124756
Validation loss: 2.4642209391440115

Epoch: 6| Step: 4
Training loss: 2.3296165466308594
Validation loss: 2.4825389167313934

Epoch: 6| Step: 5
Training loss: 2.7732622623443604
Validation loss: 2.4607260047748523

Epoch: 6| Step: 6
Training loss: 2.972475528717041
Validation loss: 2.4685237330775105

Epoch: 6| Step: 7
Training loss: 2.8326449394226074
Validation loss: 2.465699029225175

Epoch: 6| Step: 8
Training loss: 3.1327974796295166
Validation loss: 2.4684230871098016

Epoch: 6| Step: 9
Training loss: 2.2734742164611816
Validation loss: 2.4701863296570314

Epoch: 6| Step: 10
Training loss: 2.110935688018799
Validation loss: 2.4727859984162035

Epoch: 6| Step: 11
Training loss: 2.7364182472229004
Validation loss: 2.4727912436249437

Epoch: 6| Step: 12
Training loss: 3.098862886428833
Validation loss: 2.473199754632929

Epoch: 6| Step: 13
Training loss: 2.7339816093444824
Validation loss: 2.472019277593141

Epoch: 175| Step: 0
Training loss: 2.8799943923950195
Validation loss: 2.4663571132126676

Epoch: 6| Step: 1
Training loss: 2.121004819869995
Validation loss: 2.458599393085767

Epoch: 6| Step: 2
Training loss: 3.03363037109375
Validation loss: 2.4450713075617307

Epoch: 6| Step: 3
Training loss: 2.7959115505218506
Validation loss: 2.445311556580246

Epoch: 6| Step: 4
Training loss: 2.418527364730835
Validation loss: 2.4436395450304915

Epoch: 6| Step: 5
Training loss: 2.6517488956451416
Validation loss: 2.4503925269649875

Epoch: 6| Step: 6
Training loss: 2.811828851699829
Validation loss: 2.449759773028794

Epoch: 6| Step: 7
Training loss: 2.7648932933807373
Validation loss: 2.4484557003103276

Epoch: 6| Step: 8
Training loss: 3.2006280422210693
Validation loss: 2.4567642340096096

Epoch: 6| Step: 9
Training loss: 2.758517265319824
Validation loss: 2.4524026083689865

Epoch: 6| Step: 10
Training loss: 2.5471887588500977
Validation loss: 2.457811087690374

Epoch: 6| Step: 11
Training loss: 2.715144634246826
Validation loss: 2.4554759097355667

Epoch: 6| Step: 12
Training loss: 2.2463483810424805
Validation loss: 2.4532261202412267

Epoch: 6| Step: 13
Training loss: 2.0543885231018066
Validation loss: 2.450058788381597

Epoch: 176| Step: 0
Training loss: 2.8214149475097656
Validation loss: 2.4479073709057224

Epoch: 6| Step: 1
Training loss: 2.407580614089966
Validation loss: 2.4518997925584034

Epoch: 6| Step: 2
Training loss: 2.3919315338134766
Validation loss: 2.452718270722256

Epoch: 6| Step: 3
Training loss: 3.222926139831543
Validation loss: 2.4565631881836922

Epoch: 6| Step: 4
Training loss: 3.093794345855713
Validation loss: 2.4696144544950096

Epoch: 6| Step: 5
Training loss: 1.3855726718902588
Validation loss: 2.4745963799056185

Epoch: 6| Step: 6
Training loss: 2.61200213432312
Validation loss: 2.4888030944332

Epoch: 6| Step: 7
Training loss: 2.8613405227661133
Validation loss: 2.495253478327105

Epoch: 6| Step: 8
Training loss: 2.0291967391967773
Validation loss: 2.5066626828203917

Epoch: 6| Step: 9
Training loss: 3.133165121078491
Validation loss: 2.486509984539401

Epoch: 6| Step: 10
Training loss: 2.4639034271240234
Validation loss: 2.4838391106615783

Epoch: 6| Step: 11
Training loss: 3.2942886352539062
Validation loss: 2.475316127141317

Epoch: 6| Step: 12
Training loss: 2.7036967277526855
Validation loss: 2.461650312587779

Epoch: 6| Step: 13
Training loss: 3.0350723266601562
Validation loss: 2.456041548841743

Epoch: 177| Step: 0
Training loss: 2.5304079055786133
Validation loss: 2.452616512134511

Epoch: 6| Step: 1
Training loss: 2.8372080326080322
Validation loss: 2.4445397571850846

Epoch: 6| Step: 2
Training loss: 2.2503936290740967
Validation loss: 2.4503746827443442

Epoch: 6| Step: 3
Training loss: 2.3986093997955322
Validation loss: 2.455831884056009

Epoch: 6| Step: 4
Training loss: 2.6815850734710693
Validation loss: 2.4493721736374723

Epoch: 6| Step: 5
Training loss: 2.886505126953125
Validation loss: 2.45020527737115

Epoch: 6| Step: 6
Training loss: 2.509063959121704
Validation loss: 2.454482409261888

Epoch: 6| Step: 7
Training loss: 2.469207286834717
Validation loss: 2.456953501188627

Epoch: 6| Step: 8
Training loss: 2.827568769454956
Validation loss: 2.456910043634394

Epoch: 6| Step: 9
Training loss: 2.3492679595947266
Validation loss: 2.462838085748816

Epoch: 6| Step: 10
Training loss: 3.2351036071777344
Validation loss: 2.46403496496139

Epoch: 6| Step: 11
Training loss: 2.114640951156616
Validation loss: 2.451202420778172

Epoch: 6| Step: 12
Training loss: 3.052914619445801
Validation loss: 2.45930189599273

Epoch: 6| Step: 13
Training loss: 3.2813732624053955
Validation loss: 2.455989347991123

Epoch: 178| Step: 0
Training loss: 3.653449773788452
Validation loss: 2.448329681991249

Epoch: 6| Step: 1
Training loss: 2.696073055267334
Validation loss: 2.4439734156413744

Epoch: 6| Step: 2
Training loss: 1.9650800228118896
Validation loss: 2.446928572911088

Epoch: 6| Step: 3
Training loss: 2.7481889724731445
Validation loss: 2.4430729035408265

Epoch: 6| Step: 4
Training loss: 3.2356643676757812
Validation loss: 2.442988731527841

Epoch: 6| Step: 5
Training loss: 2.8413281440734863
Validation loss: 2.4470130038517777

Epoch: 6| Step: 6
Training loss: 2.358912944793701
Validation loss: 2.4555570745980866

Epoch: 6| Step: 7
Training loss: 2.8077239990234375
Validation loss: 2.445816816822175

Epoch: 6| Step: 8
Training loss: 2.6449713706970215
Validation loss: 2.449487623348031

Epoch: 6| Step: 9
Training loss: 2.8024027347564697
Validation loss: 2.45037869996922

Epoch: 6| Step: 10
Training loss: 2.8657312393188477
Validation loss: 2.44673349267693

Epoch: 6| Step: 11
Training loss: 2.0469279289245605
Validation loss: 2.4465642077948457

Epoch: 6| Step: 12
Training loss: 2.0519189834594727
Validation loss: 2.4482537559283677

Epoch: 6| Step: 13
Training loss: 2.166679620742798
Validation loss: 2.4433923844368226

Epoch: 179| Step: 0
Training loss: 2.4373278617858887
Validation loss: 2.4418334294390935

Epoch: 6| Step: 1
Training loss: 2.419029712677002
Validation loss: 2.446061606048256

Epoch: 6| Step: 2
Training loss: 3.547520160675049
Validation loss: 2.4426641489869807

Epoch: 6| Step: 3
Training loss: 2.7876064777374268
Validation loss: 2.445230601936258

Epoch: 6| Step: 4
Training loss: 2.4475831985473633
Validation loss: 2.4424377487551783

Epoch: 6| Step: 5
Training loss: 2.191554546356201
Validation loss: 2.4442106831458306

Epoch: 6| Step: 6
Training loss: 2.989161252975464
Validation loss: 2.4456718019259873

Epoch: 6| Step: 7
Training loss: 3.091331958770752
Validation loss: 2.4421972151725524

Epoch: 6| Step: 8
Training loss: 2.775956630706787
Validation loss: 2.446125151008688

Epoch: 6| Step: 9
Training loss: 1.9869784116744995
Validation loss: 2.4433250350336873

Epoch: 6| Step: 10
Training loss: 2.574225902557373
Validation loss: 2.4490306326138076

Epoch: 6| Step: 11
Training loss: 2.3044416904449463
Validation loss: 2.450589382520286

Epoch: 6| Step: 12
Training loss: 2.574061393737793
Validation loss: 2.447375802583592

Epoch: 6| Step: 13
Training loss: 3.1720008850097656
Validation loss: 2.4503761055648967

Epoch: 180| Step: 0
Training loss: 2.857067584991455
Validation loss: 2.4571041753215175

Epoch: 6| Step: 1
Training loss: 2.707216739654541
Validation loss: 2.4676925648925123

Epoch: 6| Step: 2
Training loss: 2.138641357421875
Validation loss: 2.4617235891280638

Epoch: 6| Step: 3
Training loss: 3.0050394535064697
Validation loss: 2.4705134309748167

Epoch: 6| Step: 4
Training loss: 2.8928680419921875
Validation loss: 2.473102302961452

Epoch: 6| Step: 5
Training loss: 1.6723662614822388
Validation loss: 2.476115829201155

Epoch: 6| Step: 6
Training loss: 2.1018688678741455
Validation loss: 2.4863175986915507

Epoch: 6| Step: 7
Training loss: 3.1791703701019287
Validation loss: 2.479448946573401

Epoch: 6| Step: 8
Training loss: 2.7909746170043945
Validation loss: 2.469893665723903

Epoch: 6| Step: 9
Training loss: 2.9160890579223633
Validation loss: 2.465168200513368

Epoch: 6| Step: 10
Training loss: 3.21614670753479
Validation loss: 2.46315658989773

Epoch: 6| Step: 11
Training loss: 2.4985191822052
Validation loss: 2.45545869745234

Epoch: 6| Step: 12
Training loss: 2.1493048667907715
Validation loss: 2.4547932917071926

Epoch: 6| Step: 13
Training loss: 3.2499396800994873
Validation loss: 2.4456194421296478

Epoch: 181| Step: 0
Training loss: 1.9947023391723633
Validation loss: 2.443978868505006

Epoch: 6| Step: 1
Training loss: 2.5317788124084473
Validation loss: 2.4388151335459884

Epoch: 6| Step: 2
Training loss: 2.6058547496795654
Validation loss: 2.4331277775508102

Epoch: 6| Step: 3
Training loss: 2.5934290885925293
Validation loss: 2.433211762418029

Epoch: 6| Step: 4
Training loss: 2.1551501750946045
Validation loss: 2.435288583078692

Epoch: 6| Step: 5
Training loss: 3.2154107093811035
Validation loss: 2.436918435558196

Epoch: 6| Step: 6
Training loss: 3.204526901245117
Validation loss: 2.4356956276842343

Epoch: 6| Step: 7
Training loss: 2.313490390777588
Validation loss: 2.436948596790273

Epoch: 6| Step: 8
Training loss: 2.1763782501220703
Validation loss: 2.440755451879194

Epoch: 6| Step: 9
Training loss: 3.0796189308166504
Validation loss: 2.4423402304290445

Epoch: 6| Step: 10
Training loss: 2.3036935329437256
Validation loss: 2.4386052546962613

Epoch: 6| Step: 11
Training loss: 3.2734134197235107
Validation loss: 2.4392616159172467

Epoch: 6| Step: 12
Training loss: 3.2691614627838135
Validation loss: 2.442548780031102

Epoch: 6| Step: 13
Training loss: 2.0877878665924072
Validation loss: 2.4411761017255884

Epoch: 182| Step: 0
Training loss: 2.807445764541626
Validation loss: 2.440624870279784

Epoch: 6| Step: 1
Training loss: 2.337146282196045
Validation loss: 2.4427228896848616

Epoch: 6| Step: 2
Training loss: 2.4486937522888184
Validation loss: 2.44391272657661

Epoch: 6| Step: 3
Training loss: 3.7852091789245605
Validation loss: 2.4415512623325473

Epoch: 6| Step: 4
Training loss: 2.4618494510650635
Validation loss: 2.4432387428898967

Epoch: 6| Step: 5
Training loss: 2.221468925476074
Validation loss: 2.4452442866499706

Epoch: 6| Step: 6
Training loss: 3.0643882751464844
Validation loss: 2.441945732280772

Epoch: 6| Step: 7
Training loss: 3.170214891433716
Validation loss: 2.4447525342305503

Epoch: 6| Step: 8
Training loss: 2.217827081680298
Validation loss: 2.449272317271079

Epoch: 6| Step: 9
Training loss: 2.457421064376831
Validation loss: 2.450266850891934

Epoch: 6| Step: 10
Training loss: 2.7808213233947754
Validation loss: 2.4506179748042936

Epoch: 6| Step: 11
Training loss: 2.5646274089813232
Validation loss: 2.449601868147491

Epoch: 6| Step: 12
Training loss: 2.2886576652526855
Validation loss: 2.454446085037724

Epoch: 6| Step: 13
Training loss: 2.334853172302246
Validation loss: 2.460306727758018

Epoch: 183| Step: 0
Training loss: 2.5640597343444824
Validation loss: 2.453358011861001

Epoch: 6| Step: 1
Training loss: 2.441873788833618
Validation loss: 2.4450564807461155

Epoch: 6| Step: 2
Training loss: 2.029541492462158
Validation loss: 2.449641858377764

Epoch: 6| Step: 3
Training loss: 2.566433906555176
Validation loss: 2.4453526081577426

Epoch: 6| Step: 4
Training loss: 2.741508960723877
Validation loss: 2.4339494295017694

Epoch: 6| Step: 5
Training loss: 2.7328531742095947
Validation loss: 2.4313628622280654

Epoch: 6| Step: 6
Training loss: 2.9793734550476074
Validation loss: 2.4347832561821066

Epoch: 6| Step: 7
Training loss: 2.8882627487182617
Validation loss: 2.4359827631263324

Epoch: 6| Step: 8
Training loss: 2.2613329887390137
Validation loss: 2.4408423028966433

Epoch: 6| Step: 9
Training loss: 3.355560779571533
Validation loss: 2.4366223581375612

Epoch: 6| Step: 10
Training loss: 3.0789411067962646
Validation loss: 2.443459046784268

Epoch: 6| Step: 11
Training loss: 1.9294294118881226
Validation loss: 2.444725554476502

Epoch: 6| Step: 12
Training loss: 2.70658016204834
Validation loss: 2.441841451070642

Epoch: 6| Step: 13
Training loss: 2.7457170486450195
Validation loss: 2.4432118400450675

Epoch: 184| Step: 0
Training loss: 2.346312999725342
Validation loss: 2.4396472618144047

Epoch: 6| Step: 1
Training loss: 2.8751845359802246
Validation loss: 2.442675531551402

Epoch: 6| Step: 2
Training loss: 1.772704839706421
Validation loss: 2.4420917136694795

Epoch: 6| Step: 3
Training loss: 2.9218363761901855
Validation loss: 2.443784030534888

Epoch: 6| Step: 4
Training loss: 2.4849419593811035
Validation loss: 2.448323103689378

Epoch: 6| Step: 5
Training loss: 3.099262237548828
Validation loss: 2.4490579097501692

Epoch: 6| Step: 6
Training loss: 2.8427317142486572
Validation loss: 2.4477697905673774

Epoch: 6| Step: 7
Training loss: 2.629784107208252
Validation loss: 2.44114290770664

Epoch: 6| Step: 8
Training loss: 2.756115674972534
Validation loss: 2.4481365821694814

Epoch: 6| Step: 9
Training loss: 2.5550577640533447
Validation loss: 2.442485806762531

Epoch: 6| Step: 10
Training loss: 2.6758193969726562
Validation loss: 2.4404529986842984

Epoch: 6| Step: 11
Training loss: 2.9099063873291016
Validation loss: 2.4332603869899625

Epoch: 6| Step: 12
Training loss: 2.3964521884918213
Validation loss: 2.433297357251567

Epoch: 6| Step: 13
Training loss: 2.8834171295166016
Validation loss: 2.429640854558637

Epoch: 185| Step: 0
Training loss: 2.910818576812744
Validation loss: 2.4293058495367728

Epoch: 6| Step: 1
Training loss: 2.4264063835144043
Validation loss: 2.4302192990497877

Epoch: 6| Step: 2
Training loss: 2.2449564933776855
Validation loss: 2.4312575658162436

Epoch: 6| Step: 3
Training loss: 2.9656686782836914
Validation loss: 2.4312097308456257

Epoch: 6| Step: 4
Training loss: 2.577418327331543
Validation loss: 2.4283544222513833

Epoch: 6| Step: 5
Training loss: 2.431936502456665
Validation loss: 2.428516008520639

Epoch: 6| Step: 6
Training loss: 2.0382485389709473
Validation loss: 2.430924529670387

Epoch: 6| Step: 7
Training loss: 3.1624345779418945
Validation loss: 2.432888553988549

Epoch: 6| Step: 8
Training loss: 2.4846184253692627
Validation loss: 2.4396046464161207

Epoch: 6| Step: 9
Training loss: 3.1880173683166504
Validation loss: 2.4347581555766444

Epoch: 6| Step: 10
Training loss: 2.6818909645080566
Validation loss: 2.440321686447308

Epoch: 6| Step: 11
Training loss: 2.064682960510254
Validation loss: 2.4407489479229016

Epoch: 6| Step: 12
Training loss: 2.687457799911499
Validation loss: 2.4452425510652605

Epoch: 6| Step: 13
Training loss: 3.5444562435150146
Validation loss: 2.4483896993821666

Epoch: 186| Step: 0
Training loss: 2.966461658477783
Validation loss: 2.4456787596466723

Epoch: 6| Step: 1
Training loss: 2.2707862854003906
Validation loss: 2.444860012300553

Epoch: 6| Step: 2
Training loss: 1.6528369188308716
Validation loss: 2.4456969512406217

Epoch: 6| Step: 3
Training loss: 2.5520591735839844
Validation loss: 2.446679917714929

Epoch: 6| Step: 4
Training loss: 1.7711564302444458
Validation loss: 2.4485299330885693

Epoch: 6| Step: 5
Training loss: 2.3064446449279785
Validation loss: 2.4432008292085383

Epoch: 6| Step: 6
Training loss: 2.5671603679656982
Validation loss: 2.4474589183766353

Epoch: 6| Step: 7
Training loss: 2.8939943313598633
Validation loss: 2.438102135094263

Epoch: 6| Step: 8
Training loss: 3.695077896118164
Validation loss: 2.4479785555152485

Epoch: 6| Step: 9
Training loss: 3.1863207817077637
Validation loss: 2.43716190707299

Epoch: 6| Step: 10
Training loss: 2.4750473499298096
Validation loss: 2.4405303103949434

Epoch: 6| Step: 11
Training loss: 3.3197848796844482
Validation loss: 2.4355513100982993

Epoch: 6| Step: 12
Training loss: 2.3879339694976807
Validation loss: 2.4395225637702533

Epoch: 6| Step: 13
Training loss: 3.051907777786255
Validation loss: 2.433974850562311

Epoch: 187| Step: 0
Training loss: 2.215578079223633
Validation loss: 2.4336649576822915

Epoch: 6| Step: 1
Training loss: 2.9965195655822754
Validation loss: 2.4393660560730965

Epoch: 6| Step: 2
Training loss: 2.7629430294036865
Validation loss: 2.4431553643236876

Epoch: 6| Step: 3
Training loss: 2.4814300537109375
Validation loss: 2.44569165988635

Epoch: 6| Step: 4
Training loss: 3.5058646202087402
Validation loss: 2.4558876637489564

Epoch: 6| Step: 5
Training loss: 2.4982151985168457
Validation loss: 2.4651449982837965

Epoch: 6| Step: 6
Training loss: 2.547396659851074
Validation loss: 2.466281091013262

Epoch: 6| Step: 7
Training loss: 2.9927101135253906
Validation loss: 2.4727528838701147

Epoch: 6| Step: 8
Training loss: 3.1652090549468994
Validation loss: 2.469341234494281

Epoch: 6| Step: 9
Training loss: 1.8812651634216309
Validation loss: 2.4612858731259584

Epoch: 6| Step: 10
Training loss: 2.4229965209960938
Validation loss: 2.4699509348920596

Epoch: 6| Step: 11
Training loss: 2.500901222229004
Validation loss: 2.4610584012923704

Epoch: 6| Step: 12
Training loss: 2.6525866985321045
Validation loss: 2.456088947993453

Epoch: 6| Step: 13
Training loss: 2.225924491882324
Validation loss: 2.455089174291139

Epoch: 188| Step: 0
Training loss: 2.982882022857666
Validation loss: 2.4501881343062206

Epoch: 6| Step: 1
Training loss: 3.3084423542022705
Validation loss: 2.4410129939356158

Epoch: 6| Step: 2
Training loss: 2.052227020263672
Validation loss: 2.446310853445402

Epoch: 6| Step: 3
Training loss: 2.4357728958129883
Validation loss: 2.431140063911356

Epoch: 6| Step: 4
Training loss: 2.3995466232299805
Validation loss: 2.4341588943235335

Epoch: 6| Step: 5
Training loss: 2.173168182373047
Validation loss: 2.425966601217947

Epoch: 6| Step: 6
Training loss: 3.2436962127685547
Validation loss: 2.430058075535682

Epoch: 6| Step: 7
Training loss: 3.033088445663452
Validation loss: 2.4292966178668443

Epoch: 6| Step: 8
Training loss: 2.1972508430480957
Validation loss: 2.4249273602680494

Epoch: 6| Step: 9
Training loss: 3.154165744781494
Validation loss: 2.4237092925656225

Epoch: 6| Step: 10
Training loss: 3.4783551692962646
Validation loss: 2.4286830912354174

Epoch: 6| Step: 11
Training loss: 2.585338592529297
Validation loss: 2.4294736692982335

Epoch: 6| Step: 12
Training loss: 1.8767595291137695
Validation loss: 2.4267090443641908

Epoch: 6| Step: 13
Training loss: 1.4743927717208862
Validation loss: 2.4260744715249665

Epoch: 189| Step: 0
Training loss: 2.2667770385742188
Validation loss: 2.4193005382373767

Epoch: 6| Step: 1
Training loss: 2.1682357788085938
Validation loss: 2.4234601246413363

Epoch: 6| Step: 2
Training loss: 2.3675174713134766
Validation loss: 2.4289734158464658

Epoch: 6| Step: 3
Training loss: 2.7251522541046143
Validation loss: 2.437010826603059

Epoch: 6| Step: 4
Training loss: 2.6647863388061523
Validation loss: 2.4402737694401897

Epoch: 6| Step: 5
Training loss: 3.9150612354278564
Validation loss: 2.447033941104848

Epoch: 6| Step: 6
Training loss: 3.4927029609680176
Validation loss: 2.440845699720485

Epoch: 6| Step: 7
Training loss: 2.6069788932800293
Validation loss: 2.44089614447727

Epoch: 6| Step: 8
Training loss: 2.7077996730804443
Validation loss: 2.4335985491352696

Epoch: 6| Step: 9
Training loss: 1.8739073276519775
Validation loss: 2.4336074654774

Epoch: 6| Step: 10
Training loss: 2.7231874465942383
Validation loss: 2.4336286872945805

Epoch: 6| Step: 11
Training loss: 3.2078161239624023
Validation loss: 2.436271498280187

Epoch: 6| Step: 12
Training loss: 1.9877433776855469
Validation loss: 2.4374435460695656

Epoch: 6| Step: 13
Training loss: 1.7719889879226685
Validation loss: 2.4344390182084936

Epoch: 190| Step: 0
Training loss: 3.105482816696167
Validation loss: 2.439789838688348

Epoch: 6| Step: 1
Training loss: 3.029090642929077
Validation loss: 2.4359183285825994

Epoch: 6| Step: 2
Training loss: 2.6918845176696777
Validation loss: 2.4396921357800885

Epoch: 6| Step: 3
Training loss: 2.178539276123047
Validation loss: 2.435618605664981

Epoch: 6| Step: 4
Training loss: 2.7986974716186523
Validation loss: 2.432437063545309

Epoch: 6| Step: 5
Training loss: 2.826608180999756
Validation loss: 2.4407861796758508

Epoch: 6| Step: 6
Training loss: 2.687889337539673
Validation loss: 2.443270242342385

Epoch: 6| Step: 7
Training loss: 1.9495680332183838
Validation loss: 2.438246375770979

Epoch: 6| Step: 8
Training loss: 2.8108534812927246
Validation loss: 2.4428783488529984

Epoch: 6| Step: 9
Training loss: 2.6878440380096436
Validation loss: 2.4453050039147817

Epoch: 6| Step: 10
Training loss: 2.891160011291504
Validation loss: 2.438694692427112

Epoch: 6| Step: 11
Training loss: 2.2521328926086426
Validation loss: 2.4373603661855063

Epoch: 6| Step: 12
Training loss: 2.398198127746582
Validation loss: 2.4321422243631012

Epoch: 6| Step: 13
Training loss: 2.481795310974121
Validation loss: 2.434517301538939

Epoch: 191| Step: 0
Training loss: 2.055326461791992
Validation loss: 2.4420350546477945

Epoch: 6| Step: 1
Training loss: 1.8798564672470093
Validation loss: 2.439956065147154

Epoch: 6| Step: 2
Training loss: 2.6289525032043457
Validation loss: 2.449297164076118

Epoch: 6| Step: 3
Training loss: 2.636734962463379
Validation loss: 2.4451855792794177

Epoch: 6| Step: 4
Training loss: 2.7095494270324707
Validation loss: 2.455512515960201

Epoch: 6| Step: 5
Training loss: 2.7327725887298584
Validation loss: 2.4536785874315488

Epoch: 6| Step: 6
Training loss: 2.9658453464508057
Validation loss: 2.447873548794818

Epoch: 6| Step: 7
Training loss: 2.7831649780273438
Validation loss: 2.450831908051686

Epoch: 6| Step: 8
Training loss: 2.692009925842285
Validation loss: 2.45694532445682

Epoch: 6| Step: 9
Training loss: 3.042929172515869
Validation loss: 2.4520088652128815

Epoch: 6| Step: 10
Training loss: 2.8651645183563232
Validation loss: 2.4437601566314697

Epoch: 6| Step: 11
Training loss: 2.0964958667755127
Validation loss: 2.44167173036965

Epoch: 6| Step: 12
Training loss: 2.954772472381592
Validation loss: 2.4306215419564197

Epoch: 6| Step: 13
Training loss: 2.942251443862915
Validation loss: 2.4318599188199608

Epoch: 192| Step: 0
Training loss: 2.81636381149292
Validation loss: 2.4356453636641144

Epoch: 6| Step: 1
Training loss: 3.0627565383911133
Validation loss: 2.4390356079224618

Epoch: 6| Step: 2
Training loss: 3.212331771850586
Validation loss: 2.433384959415723

Epoch: 6| Step: 3
Training loss: 2.641486167907715
Validation loss: 2.4455251847544024

Epoch: 6| Step: 4
Training loss: 2.3830723762512207
Validation loss: 2.4474442287157943

Epoch: 6| Step: 5
Training loss: 2.077030658721924
Validation loss: 2.441990929265176

Epoch: 6| Step: 6
Training loss: 2.2103466987609863
Validation loss: 2.4412179531589633

Epoch: 6| Step: 7
Training loss: 2.7043709754943848
Validation loss: 2.4321492359202397

Epoch: 6| Step: 8
Training loss: 2.624763011932373
Validation loss: 2.434437936352145

Epoch: 6| Step: 9
Training loss: 3.3389596939086914
Validation loss: 2.4297673548421552

Epoch: 6| Step: 10
Training loss: 1.9843007326126099
Validation loss: 2.430526920544204

Epoch: 6| Step: 11
Training loss: 2.317314624786377
Validation loss: 2.4354382150916645

Epoch: 6| Step: 12
Training loss: 2.984438896179199
Validation loss: 2.4389202646029893

Epoch: 6| Step: 13
Training loss: 2.4424290657043457
Validation loss: 2.4344934545537478

Epoch: 193| Step: 0
Training loss: 2.3298287391662598
Validation loss: 2.4427469238158195

Epoch: 6| Step: 1
Training loss: 3.110346794128418
Validation loss: 2.4388443321310063

Epoch: 6| Step: 2
Training loss: 2.2162604331970215
Validation loss: 2.4425989350964947

Epoch: 6| Step: 3
Training loss: 3.2034225463867188
Validation loss: 2.435770955137027

Epoch: 6| Step: 4
Training loss: 3.1404500007629395
Validation loss: 2.4319494565327964

Epoch: 6| Step: 5
Training loss: 2.5745058059692383
Validation loss: 2.4414645318062074

Epoch: 6| Step: 6
Training loss: 2.538100242614746
Validation loss: 2.4426665203545683

Epoch: 6| Step: 7
Training loss: 1.8757033348083496
Validation loss: 2.4432063000176543

Epoch: 6| Step: 8
Training loss: 2.4759092330932617
Validation loss: 2.4400858725270917

Epoch: 6| Step: 9
Training loss: 2.5134942531585693
Validation loss: 2.427910921394184

Epoch: 6| Step: 10
Training loss: 2.8195247650146484
Validation loss: 2.434912020160306

Epoch: 6| Step: 11
Training loss: 2.2834036350250244
Validation loss: 2.4335886496369556

Epoch: 6| Step: 12
Training loss: 3.1766295433044434
Validation loss: 2.4338829594273723

Epoch: 6| Step: 13
Training loss: 2.5558016300201416
Validation loss: 2.4188652166756253

Epoch: 194| Step: 0
Training loss: 3.7840733528137207
Validation loss: 2.419852738739342

Epoch: 6| Step: 1
Training loss: 2.9353747367858887
Validation loss: 2.4145904587161158

Epoch: 6| Step: 2
Training loss: 1.9856023788452148
Validation loss: 2.4223914569424045

Epoch: 6| Step: 3
Training loss: 3.4017953872680664
Validation loss: 2.422746765998102

Epoch: 6| Step: 4
Training loss: 1.8249708414077759
Validation loss: 2.4283120298898346

Epoch: 6| Step: 5
Training loss: 2.786949634552002
Validation loss: 2.424356122170725

Epoch: 6| Step: 6
Training loss: 2.36344051361084
Validation loss: 2.418950103944348

Epoch: 6| Step: 7
Training loss: 2.066922664642334
Validation loss: 2.42938212297296

Epoch: 6| Step: 8
Training loss: 2.6137354373931885
Validation loss: 2.427645560233824

Epoch: 6| Step: 9
Training loss: 2.1965065002441406
Validation loss: 2.423073181542017

Epoch: 6| Step: 10
Training loss: 2.4044811725616455
Validation loss: 2.427957506589992

Epoch: 6| Step: 11
Training loss: 2.4788455963134766
Validation loss: 2.4299827391101467

Epoch: 6| Step: 12
Training loss: 3.5796494483947754
Validation loss: 2.428651016245606

Epoch: 6| Step: 13
Training loss: 2.2252349853515625
Validation loss: 2.4260992388571463

Epoch: 195| Step: 0
Training loss: 2.553506851196289
Validation loss: 2.4327251962436143

Epoch: 6| Step: 1
Training loss: 1.7497614622116089
Validation loss: 2.425371205934914

Epoch: 6| Step: 2
Training loss: 2.6108360290527344
Validation loss: 2.441427166743945

Epoch: 6| Step: 3
Training loss: 2.99338960647583
Validation loss: 2.4425420914926836

Epoch: 6| Step: 4
Training loss: 1.8231148719787598
Validation loss: 2.439753024808822

Epoch: 6| Step: 5
Training loss: 2.6816840171813965
Validation loss: 2.4452283869507494

Epoch: 6| Step: 6
Training loss: 3.106203079223633
Validation loss: 2.4402467845588602

Epoch: 6| Step: 7
Training loss: 2.723639726638794
Validation loss: 2.44421318013181

Epoch: 6| Step: 8
Training loss: 2.4087207317352295
Validation loss: 2.440729598845205

Epoch: 6| Step: 9
Training loss: 2.4849023818969727
Validation loss: 2.4400690576081634

Epoch: 6| Step: 10
Training loss: 2.7357184886932373
Validation loss: 2.441795100447952

Epoch: 6| Step: 11
Training loss: 3.1089680194854736
Validation loss: 2.433072741313647

Epoch: 6| Step: 12
Training loss: 3.1417758464813232
Validation loss: 2.4361517813897904

Epoch: 6| Step: 13
Training loss: 2.6474790573120117
Validation loss: 2.4243285553429716

Epoch: 196| Step: 0
Training loss: 2.1331560611724854
Validation loss: 2.4254500173753306

Epoch: 6| Step: 1
Training loss: 1.876650333404541
Validation loss: 2.423464326448338

Epoch: 6| Step: 2
Training loss: 1.981674075126648
Validation loss: 2.4192716485710553

Epoch: 6| Step: 3
Training loss: 2.7752232551574707
Validation loss: 2.4212555859678533

Epoch: 6| Step: 4
Training loss: 3.1235623359680176
Validation loss: 2.42960677864731

Epoch: 6| Step: 5
Training loss: 3.169262170791626
Validation loss: 2.4344887297640563

Epoch: 6| Step: 6
Training loss: 3.47597074508667
Validation loss: 2.4362866340144986

Epoch: 6| Step: 7
Training loss: 2.717266798019409
Validation loss: 2.438328904490317

Epoch: 6| Step: 8
Training loss: 3.0269064903259277
Validation loss: 2.4402434364441903

Epoch: 6| Step: 9
Training loss: 2.3466410636901855
Validation loss: 2.4346389334688903

Epoch: 6| Step: 10
Training loss: 2.7595770359039307
Validation loss: 2.4274462576835387

Epoch: 6| Step: 11
Training loss: 2.001889705657959
Validation loss: 2.430622854540425

Epoch: 6| Step: 12
Training loss: 2.711038589477539
Validation loss: 2.4243984555685394

Epoch: 6| Step: 13
Training loss: 2.494297981262207
Validation loss: 2.4210222280153664

Epoch: 197| Step: 0
Training loss: 2.0945229530334473
Validation loss: 2.4164844533448577

Epoch: 6| Step: 1
Training loss: 1.9905877113342285
Validation loss: 2.413176146886682

Epoch: 6| Step: 2
Training loss: 3.2674620151519775
Validation loss: 2.414556352041101

Epoch: 6| Step: 3
Training loss: 2.2477059364318848
Validation loss: 2.4138710755173878

Epoch: 6| Step: 4
Training loss: 3.6638827323913574
Validation loss: 2.4172509934312556

Epoch: 6| Step: 5
Training loss: 3.4919967651367188
Validation loss: 2.4161594811306206

Epoch: 6| Step: 6
Training loss: 2.6298975944519043
Validation loss: 2.42175506776379

Epoch: 6| Step: 7
Training loss: 2.1162986755371094
Validation loss: 2.424894919959448

Epoch: 6| Step: 8
Training loss: 1.9747583866119385
Validation loss: 2.4164816487220024

Epoch: 6| Step: 9
Training loss: 3.1080245971679688
Validation loss: 2.4322558936252388

Epoch: 6| Step: 10
Training loss: 2.304966449737549
Validation loss: 2.4273472165548675

Epoch: 6| Step: 11
Training loss: 2.6500725746154785
Validation loss: 2.426017174156763

Epoch: 6| Step: 12
Training loss: 2.585754871368408
Validation loss: 2.4357811943177254

Epoch: 6| Step: 13
Training loss: 2.6733381748199463
Validation loss: 2.43655296807648

Epoch: 198| Step: 0
Training loss: 2.724252462387085
Validation loss: 2.440041926599318

Epoch: 6| Step: 1
Training loss: 2.751162528991699
Validation loss: 2.438626091967347

Epoch: 6| Step: 2
Training loss: 2.7663800716400146
Validation loss: 2.442370935152936

Epoch: 6| Step: 3
Training loss: 2.457019329071045
Validation loss: 2.449655794328259

Epoch: 6| Step: 4
Training loss: 2.8317580223083496
Validation loss: 2.4438157440513693

Epoch: 6| Step: 5
Training loss: 1.9384963512420654
Validation loss: 2.442160303874682

Epoch: 6| Step: 6
Training loss: 2.5383899211883545
Validation loss: 2.4477205814853793

Epoch: 6| Step: 7
Training loss: 3.4695329666137695
Validation loss: 2.4505496486540763

Epoch: 6| Step: 8
Training loss: 2.275324821472168
Validation loss: 2.4396008701734644

Epoch: 6| Step: 9
Training loss: 3.236396312713623
Validation loss: 2.442322172144408

Epoch: 6| Step: 10
Training loss: 2.3767123222351074
Validation loss: 2.441314758793

Epoch: 6| Step: 11
Training loss: 2.793426513671875
Validation loss: 2.435822145913237

Epoch: 6| Step: 12
Training loss: 2.335864543914795
Validation loss: 2.4311482906341553

Epoch: 6| Step: 13
Training loss: 2.058899402618408
Validation loss: 2.432245182734664

Epoch: 199| Step: 0
Training loss: 2.617189407348633
Validation loss: 2.424463088794421

Epoch: 6| Step: 1
Training loss: 2.172583818435669
Validation loss: 2.4265796753668014

Epoch: 6| Step: 2
Training loss: 2.4573493003845215
Validation loss: 2.4216770561792518

Epoch: 6| Step: 3
Training loss: 3.3984484672546387
Validation loss: 2.4298949651820685

Epoch: 6| Step: 4
Training loss: 3.3438239097595215
Validation loss: 2.423485891793364

Epoch: 6| Step: 5
Training loss: 1.8578195571899414
Validation loss: 2.4282757800112487

Epoch: 6| Step: 6
Training loss: 2.7198171615600586
Validation loss: 2.4233279061573807

Epoch: 6| Step: 7
Training loss: 2.018972873687744
Validation loss: 2.4287635408422

Epoch: 6| Step: 8
Training loss: 2.651353359222412
Validation loss: 2.4174518508295857

Epoch: 6| Step: 9
Training loss: 2.650141716003418
Validation loss: 2.4186332610345658

Epoch: 6| Step: 10
Training loss: 2.697732448577881
Validation loss: 2.422105573838757

Epoch: 6| Step: 11
Training loss: 2.8461103439331055
Validation loss: 2.4273881758413007

Epoch: 6| Step: 12
Training loss: 2.578662395477295
Validation loss: 2.4336698106540147

Epoch: 6| Step: 13
Training loss: 2.6223440170288086
Validation loss: 2.434298397392355

Epoch: 200| Step: 0
Training loss: 2.3183908462524414
Validation loss: 2.4328057535233034

Epoch: 6| Step: 1
Training loss: 3.0047521591186523
Validation loss: 2.438018568100468

Epoch: 6| Step: 2
Training loss: 2.443636655807495
Validation loss: 2.439520153948056

Epoch: 6| Step: 3
Training loss: 2.3862295150756836
Validation loss: 2.4393418681237007

Epoch: 6| Step: 4
Training loss: 2.4046835899353027
Validation loss: 2.444216807683309

Epoch: 6| Step: 5
Training loss: 1.9643144607543945
Validation loss: 2.451089192462224

Epoch: 6| Step: 6
Training loss: 3.155280351638794
Validation loss: 2.442301739928543

Epoch: 6| Step: 7
Training loss: 2.601180076599121
Validation loss: 2.448735192257871

Epoch: 6| Step: 8
Training loss: 3.7525248527526855
Validation loss: 2.4327253423711306

Epoch: 6| Step: 9
Training loss: 2.8311641216278076
Validation loss: 2.4373025765983005

Epoch: 6| Step: 10
Training loss: 3.085766315460205
Validation loss: 2.4197208727559736

Epoch: 6| Step: 11
Training loss: 2.369229316711426
Validation loss: 2.4232101671157347

Epoch: 6| Step: 12
Training loss: 2.1279942989349365
Validation loss: 2.432300406117593

Epoch: 6| Step: 13
Training loss: 1.9138641357421875
Validation loss: 2.4227091189353698

Epoch: 201| Step: 0
Training loss: 2.572448492050171
Validation loss: 2.4396602953633955

Epoch: 6| Step: 1
Training loss: 2.1169955730438232
Validation loss: 2.436490730572772

Epoch: 6| Step: 2
Training loss: 3.24792218208313
Validation loss: 2.440217876947054

Epoch: 6| Step: 3
Training loss: 2.2763891220092773
Validation loss: 2.446390741614885

Epoch: 6| Step: 4
Training loss: 3.128726005554199
Validation loss: 2.437946158070718

Epoch: 6| Step: 5
Training loss: 2.779480457305908
Validation loss: 2.438961016234531

Epoch: 6| Step: 6
Training loss: 2.389664649963379
Validation loss: 2.440570241661482

Epoch: 6| Step: 7
Training loss: 2.826582431793213
Validation loss: 2.430666164685321

Epoch: 6| Step: 8
Training loss: 2.728461742401123
Validation loss: 2.428438601955291

Epoch: 6| Step: 9
Training loss: 2.571767568588257
Validation loss: 2.4236743091255106

Epoch: 6| Step: 10
Training loss: 3.0007994174957275
Validation loss: 2.4162992405635055

Epoch: 6| Step: 11
Training loss: 3.0680975914001465
Validation loss: 2.4189113775889077

Epoch: 6| Step: 12
Training loss: 1.834049940109253
Validation loss: 2.4209403581516717

Epoch: 6| Step: 13
Training loss: 1.7511123418807983
Validation loss: 2.4197606655859176

Epoch: 202| Step: 0
Training loss: 2.8719186782836914
Validation loss: 2.4387893369120937

Epoch: 6| Step: 1
Training loss: 2.444790840148926
Validation loss: 2.4609786054139495

Epoch: 6| Step: 2
Training loss: 3.309283494949341
Validation loss: 2.4781237879107074

Epoch: 6| Step: 3
Training loss: 1.9244496822357178
Validation loss: 2.5251856644948325

Epoch: 6| Step: 4
Training loss: 3.2113280296325684
Validation loss: 2.5306199930047475

Epoch: 6| Step: 5
Training loss: 2.396314859390259
Validation loss: 2.5350594110386346

Epoch: 6| Step: 6
Training loss: 2.2575223445892334
Validation loss: 2.502685293074577

Epoch: 6| Step: 7
Training loss: 1.8383803367614746
Validation loss: 2.494034395422987

Epoch: 6| Step: 8
Training loss: 2.1924843788146973
Validation loss: 2.430602559479334

Epoch: 6| Step: 9
Training loss: 2.4628491401672363
Validation loss: 2.411080665485833

Epoch: 6| Step: 10
Training loss: 2.50772762298584
Validation loss: 2.4106447953049854

Epoch: 6| Step: 11
Training loss: 2.8271076679229736
Validation loss: 2.4056358157947497

Epoch: 6| Step: 12
Training loss: 3.479820728302002
Validation loss: 2.4064944969710482

Epoch: 6| Step: 13
Training loss: 3.672063112258911
Validation loss: 2.404416952081906

Epoch: 203| Step: 0
Training loss: 2.5925745964050293
Validation loss: 2.4128257279754965

Epoch: 6| Step: 1
Training loss: 3.018949031829834
Validation loss: 2.4058206055753972

Epoch: 6| Step: 2
Training loss: 3.4828736782073975
Validation loss: 2.4108026463498353

Epoch: 6| Step: 3
Training loss: 2.443127155303955
Validation loss: 2.408362901338967

Epoch: 6| Step: 4
Training loss: 1.9692648649215698
Validation loss: 2.405626486706477

Epoch: 6| Step: 5
Training loss: 3.1029677391052246
Validation loss: 2.4117291512027865

Epoch: 6| Step: 6
Training loss: 2.5570602416992188
Validation loss: 2.4108937094288487

Epoch: 6| Step: 7
Training loss: 2.3567566871643066
Validation loss: 2.4072036230435936

Epoch: 6| Step: 8
Training loss: 2.796834707260132
Validation loss: 2.414224865616009

Epoch: 6| Step: 9
Training loss: 2.308361053466797
Validation loss: 2.4159792033574914

Epoch: 6| Step: 10
Training loss: 2.7174434661865234
Validation loss: 2.4083799162218646

Epoch: 6| Step: 11
Training loss: 2.822535991668701
Validation loss: 2.4108669783479426

Epoch: 6| Step: 12
Training loss: 2.175854206085205
Validation loss: 2.4178143419245237

Epoch: 6| Step: 13
Training loss: 2.1338725090026855
Validation loss: 2.427539615220921

Epoch: 204| Step: 0
Training loss: 2.3820981979370117
Validation loss: 2.4293594180896716

Epoch: 6| Step: 1
Training loss: 3.677668571472168
Validation loss: 2.4338814968703897

Epoch: 6| Step: 2
Training loss: 2.5058021545410156
Validation loss: 2.4354525355882544

Epoch: 6| Step: 3
Training loss: 2.9054267406463623
Validation loss: 2.4327366300808486

Epoch: 6| Step: 4
Training loss: 1.7113490104675293
Validation loss: 2.445262829462687

Epoch: 6| Step: 5
Training loss: 2.1078128814697266
Validation loss: 2.446453017573203

Epoch: 6| Step: 6
Training loss: 2.990217685699463
Validation loss: 2.453602519086612

Epoch: 6| Step: 7
Training loss: 2.9356346130371094
Validation loss: 2.4533800437886226

Epoch: 6| Step: 8
Training loss: 3.244357109069824
Validation loss: 2.458610837177564

Epoch: 6| Step: 9
Training loss: 2.2094273567199707
Validation loss: 2.451232200027794

Epoch: 6| Step: 10
Training loss: 1.9016963243484497
Validation loss: 2.446318725103973

Epoch: 6| Step: 11
Training loss: 2.7877817153930664
Validation loss: 2.4282532430464223

Epoch: 6| Step: 12
Training loss: 3.1204617023468018
Validation loss: 2.438646229364539

Epoch: 6| Step: 13
Training loss: 1.7580772638320923
Validation loss: 2.425906786354639

Epoch: 205| Step: 0
Training loss: 3.147679328918457
Validation loss: 2.420675495619415

Epoch: 6| Step: 1
Training loss: 2.494935989379883
Validation loss: 2.4222610637705815

Epoch: 6| Step: 2
Training loss: 2.692458391189575
Validation loss: 2.424635846127746

Epoch: 6| Step: 3
Training loss: 2.1541006565093994
Validation loss: 2.4182842008529173

Epoch: 6| Step: 4
Training loss: 2.114494800567627
Validation loss: 2.4086543513882543

Epoch: 6| Step: 5
Training loss: 3.18483829498291
Validation loss: 2.411926156731062

Epoch: 6| Step: 6
Training loss: 3.3160557746887207
Validation loss: 2.4162982035708684

Epoch: 6| Step: 7
Training loss: 2.9779505729675293
Validation loss: 2.4096379485181583

Epoch: 6| Step: 8
Training loss: 2.4449145793914795
Validation loss: 2.4219178281804568

Epoch: 6| Step: 9
Training loss: 2.901643753051758
Validation loss: 2.4056057571082987

Epoch: 6| Step: 10
Training loss: 2.246574878692627
Validation loss: 2.415414100052208

Epoch: 6| Step: 11
Training loss: 1.8664746284484863
Validation loss: 2.4110994723535355

Epoch: 6| Step: 12
Training loss: 2.358001708984375
Validation loss: 2.430485963821411

Epoch: 6| Step: 13
Training loss: 2.7601544857025146
Validation loss: 2.444408698748517

Epoch: 206| Step: 0
Training loss: 2.747598648071289
Validation loss: 2.437904632219704

Epoch: 6| Step: 1
Training loss: 3.2355194091796875
Validation loss: 2.428578597243114

Epoch: 6| Step: 2
Training loss: 2.824504852294922
Validation loss: 2.424400016825686

Epoch: 6| Step: 3
Training loss: 2.1968634128570557
Validation loss: 2.4287751490069973

Epoch: 6| Step: 4
Training loss: 2.6405560970306396
Validation loss: 2.423538132380414

Epoch: 6| Step: 5
Training loss: 2.3767476081848145
Validation loss: 2.427136936495381

Epoch: 6| Step: 6
Training loss: 3.1637344360351562
Validation loss: 2.428387870070755

Epoch: 6| Step: 7
Training loss: 2.259612560272217
Validation loss: 2.420964161554972

Epoch: 6| Step: 8
Training loss: 2.5044636726379395
Validation loss: 2.426863193511963

Epoch: 6| Step: 9
Training loss: 3.042933940887451
Validation loss: 2.430437416158697

Epoch: 6| Step: 10
Training loss: 2.3575286865234375
Validation loss: 2.430115540822347

Epoch: 6| Step: 11
Training loss: 2.179593563079834
Validation loss: 2.439169145399524

Epoch: 6| Step: 12
Training loss: 2.2206735610961914
Validation loss: 2.434414807186332

Epoch: 6| Step: 13
Training loss: 2.818131446838379
Validation loss: 2.4356187876834663

Epoch: 207| Step: 0
Training loss: 3.4605021476745605
Validation loss: 2.4351503054300943

Epoch: 6| Step: 1
Training loss: 2.557525634765625
Validation loss: 2.422497057145642

Epoch: 6| Step: 2
Training loss: 3.394317388534546
Validation loss: 2.4204291553907495

Epoch: 6| Step: 3
Training loss: 1.9024982452392578
Validation loss: 2.4190681877956597

Epoch: 6| Step: 4
Training loss: 2.450652599334717
Validation loss: 2.4174037543676232

Epoch: 6| Step: 5
Training loss: 2.9906206130981445
Validation loss: 2.4143063611881708

Epoch: 6| Step: 6
Training loss: 2.367755889892578
Validation loss: 2.415436108907064

Epoch: 6| Step: 7
Training loss: 2.543294906616211
Validation loss: 2.4111439694640455

Epoch: 6| Step: 8
Training loss: 2.531904697418213
Validation loss: 2.413641914244621

Epoch: 6| Step: 9
Training loss: 3.3203930854797363
Validation loss: 2.417086321820495

Epoch: 6| Step: 10
Training loss: 2.2542200088500977
Validation loss: 2.4120261643522527

Epoch: 6| Step: 11
Training loss: 2.643209218978882
Validation loss: 2.418236832464895

Epoch: 6| Step: 12
Training loss: 1.7767667770385742
Validation loss: 2.4346763549312467

Epoch: 6| Step: 13
Training loss: 2.0835156440734863
Validation loss: 2.459134388995427

Epoch: 208| Step: 0
Training loss: 2.145378589630127
Validation loss: 2.4753303579104844

Epoch: 6| Step: 1
Training loss: 2.594895839691162
Validation loss: 2.4836093507787234

Epoch: 6| Step: 2
Training loss: 2.8538401126861572
Validation loss: 2.499451352703956

Epoch: 6| Step: 3
Training loss: 2.505892753601074
Validation loss: 2.5043474038441977

Epoch: 6| Step: 4
Training loss: 2.140366554260254
Validation loss: 2.4916534346918904

Epoch: 6| Step: 5
Training loss: 2.480884075164795
Validation loss: 2.4780139154003513

Epoch: 6| Step: 6
Training loss: 2.870299816131592
Validation loss: 2.4504182466896633

Epoch: 6| Step: 7
Training loss: 2.8510818481445312
Validation loss: 2.443344816084831

Epoch: 6| Step: 8
Training loss: 3.199601411819458
Validation loss: 2.4320111146537204

Epoch: 6| Step: 9
Training loss: 2.534302234649658
Validation loss: 2.43301926120635

Epoch: 6| Step: 10
Training loss: 2.653992176055908
Validation loss: 2.4204575656562723

Epoch: 6| Step: 11
Training loss: 2.706601619720459
Validation loss: 2.4128436426962576

Epoch: 6| Step: 12
Training loss: 2.3202970027923584
Validation loss: 2.4117913194881972

Epoch: 6| Step: 13
Training loss: 2.9904229640960693
Validation loss: 2.4077723551821966

Epoch: 209| Step: 0
Training loss: 2.531418800354004
Validation loss: 2.4160539386092976

Epoch: 6| Step: 1
Training loss: 2.0100858211517334
Validation loss: 2.4118819416210218

Epoch: 6| Step: 2
Training loss: 2.771435260772705
Validation loss: 2.4209792972892843

Epoch: 6| Step: 3
Training loss: 2.5048394203186035
Validation loss: 2.4209148140363794

Epoch: 6| Step: 4
Training loss: 2.3739192485809326
Validation loss: 2.416833951909055

Epoch: 6| Step: 5
Training loss: 2.8832311630249023
Validation loss: 2.4176686656090522

Epoch: 6| Step: 6
Training loss: 2.867901086807251
Validation loss: 2.4156790189845587

Epoch: 6| Step: 7
Training loss: 2.7324390411376953
Validation loss: 2.4266714895925214

Epoch: 6| Step: 8
Training loss: 2.584164619445801
Validation loss: 2.421623270998719

Epoch: 6| Step: 9
Training loss: 2.71516752243042
Validation loss: 2.41463598000106

Epoch: 6| Step: 10
Training loss: 2.7313003540039062
Validation loss: 2.4167206415566067

Epoch: 6| Step: 11
Training loss: 2.323946952819824
Validation loss: 2.4059568733297367

Epoch: 6| Step: 12
Training loss: 2.3396706581115723
Validation loss: 2.4098154972958308

Epoch: 6| Step: 13
Training loss: 3.659440279006958
Validation loss: 2.413803446677423

Epoch: 210| Step: 0
Training loss: 2.815638780593872
Validation loss: 2.4192581176757812

Epoch: 6| Step: 1
Training loss: 3.3522090911865234
Validation loss: 2.4158064126968384

Epoch: 6| Step: 2
Training loss: 1.9060611724853516
Validation loss: 2.4069454105951453

Epoch: 6| Step: 3
Training loss: 2.496584892272949
Validation loss: 2.4259458049651115

Epoch: 6| Step: 4
Training loss: 2.3466970920562744
Validation loss: 2.433695395787557

Epoch: 6| Step: 5
Training loss: 1.6495227813720703
Validation loss: 2.442707969296363

Epoch: 6| Step: 6
Training loss: 2.7730584144592285
Validation loss: 2.461600236995246

Epoch: 6| Step: 7
Training loss: 2.9677317142486572
Validation loss: 2.4661056918482624

Epoch: 6| Step: 8
Training loss: 2.96236515045166
Validation loss: 2.4560268027808076

Epoch: 6| Step: 9
Training loss: 3.365243434906006
Validation loss: 2.435304118740943

Epoch: 6| Step: 10
Training loss: 1.6195131540298462
Validation loss: 2.4186552391257337

Epoch: 6| Step: 11
Training loss: 3.169337511062622
Validation loss: 2.4190766913916475

Epoch: 6| Step: 12
Training loss: 2.4680981636047363
Validation loss: 2.4060488977739887

Epoch: 6| Step: 13
Training loss: 2.5385944843292236
Validation loss: 2.4046993050523984

Epoch: 211| Step: 0
Training loss: 3.2340574264526367
Validation loss: 2.3958145110837874

Epoch: 6| Step: 1
Training loss: 2.8170671463012695
Validation loss: 2.3905783596859185

Epoch: 6| Step: 2
Training loss: 1.813222885131836
Validation loss: 2.3991306494641047

Epoch: 6| Step: 3
Training loss: 2.2275338172912598
Validation loss: 2.397372079151933

Epoch: 6| Step: 4
Training loss: 2.6083486080169678
Validation loss: 2.393724017245795

Epoch: 6| Step: 5
Training loss: 3.054147720336914
Validation loss: 2.3976918292301956

Epoch: 6| Step: 6
Training loss: 2.1769943237304688
Validation loss: 2.4102371354256906

Epoch: 6| Step: 7
Training loss: 2.300938606262207
Validation loss: 2.4081303252968738

Epoch: 6| Step: 8
Training loss: 2.5136189460754395
Validation loss: 2.410757795456917

Epoch: 6| Step: 9
Training loss: 2.925422430038452
Validation loss: 2.4228106929409887

Epoch: 6| Step: 10
Training loss: 3.2802627086639404
Validation loss: 2.417819469205795

Epoch: 6| Step: 11
Training loss: 2.432709217071533
Validation loss: 2.423249495926724

Epoch: 6| Step: 12
Training loss: 2.077406406402588
Validation loss: 2.424515652400191

Epoch: 6| Step: 13
Training loss: 3.5863561630249023
Validation loss: 2.4096181828488588

Epoch: 212| Step: 0
Training loss: 2.6556406021118164
Validation loss: 2.413691230999526

Epoch: 6| Step: 1
Training loss: 2.0728354454040527
Validation loss: 2.408207424225346

Epoch: 6| Step: 2
Training loss: 3.7544500827789307
Validation loss: 2.402062387876613

Epoch: 6| Step: 3
Training loss: 2.636775255203247
Validation loss: 2.412003347950597

Epoch: 6| Step: 4
Training loss: 2.797170877456665
Validation loss: 2.4022353054374777

Epoch: 6| Step: 5
Training loss: 2.639721393585205
Validation loss: 2.4039816215474117

Epoch: 6| Step: 6
Training loss: 2.6591689586639404
Validation loss: 2.4025570038826234

Epoch: 6| Step: 7
Training loss: 2.6336231231689453
Validation loss: 2.4005941139754428

Epoch: 6| Step: 8
Training loss: 1.443857192993164
Validation loss: 2.408271807496266

Epoch: 6| Step: 9
Training loss: 3.37362003326416
Validation loss: 2.4060757672914894

Epoch: 6| Step: 10
Training loss: 1.6672706604003906
Validation loss: 2.4056666948462047

Epoch: 6| Step: 11
Training loss: 2.4755136966705322
Validation loss: 2.3999039357708347

Epoch: 6| Step: 12
Training loss: 2.7858448028564453
Validation loss: 2.4060801998261483

Epoch: 6| Step: 13
Training loss: 3.1359188556671143
Validation loss: 2.412433596067531

Epoch: 213| Step: 0
Training loss: 2.382741928100586
Validation loss: 2.4198581480210826

Epoch: 6| Step: 1
Training loss: 3.2805044651031494
Validation loss: 2.432163893535573

Epoch: 6| Step: 2
Training loss: 2.0125784873962402
Validation loss: 2.4443163371855214

Epoch: 6| Step: 3
Training loss: 2.8955190181732178
Validation loss: 2.4710250362273185

Epoch: 6| Step: 4
Training loss: 2.9601473808288574
Validation loss: 2.5152603169923187

Epoch: 6| Step: 5
Training loss: 3.0109853744506836
Validation loss: 2.5373687026321248

Epoch: 6| Step: 6
Training loss: 2.528308868408203
Validation loss: 2.5572667301342054

Epoch: 6| Step: 7
Training loss: 2.1914546489715576
Validation loss: 2.528554565163069

Epoch: 6| Step: 8
Training loss: 2.245079755783081
Validation loss: 2.4807351686621226

Epoch: 6| Step: 9
Training loss: 1.7623265981674194
Validation loss: 2.437027228775845

Epoch: 6| Step: 10
Training loss: 2.9225211143493652
Validation loss: 2.4260668780214045

Epoch: 6| Step: 11
Training loss: 2.8808743953704834
Validation loss: 2.421766306764336

Epoch: 6| Step: 12
Training loss: 2.4753541946411133
Validation loss: 2.4406294309964744

Epoch: 6| Step: 13
Training loss: 3.9214489459991455
Validation loss: 2.444326834012103

Epoch: 214| Step: 0
Training loss: 2.814311981201172
Validation loss: 2.4816457276703208

Epoch: 6| Step: 1
Training loss: 2.553220272064209
Validation loss: 2.481345607388404

Epoch: 6| Step: 2
Training loss: 2.9175190925598145
Validation loss: 2.446646580132105

Epoch: 6| Step: 3
Training loss: 2.37153959274292
Validation loss: 2.425757279960058

Epoch: 6| Step: 4
Training loss: 3.992187976837158
Validation loss: 2.407166911709693

Epoch: 6| Step: 5
Training loss: 2.378793478012085
Validation loss: 2.393824441458589

Epoch: 6| Step: 6
Training loss: 2.2124931812286377
Validation loss: 2.3853847903590046

Epoch: 6| Step: 7
Training loss: 2.639531135559082
Validation loss: 2.388657522457902

Epoch: 6| Step: 8
Training loss: 2.3130884170532227
Validation loss: 2.385618794348932

Epoch: 6| Step: 9
Training loss: 2.849821090698242
Validation loss: 2.393840506512632

Epoch: 6| Step: 10
Training loss: 2.3221163749694824
Validation loss: 2.3932437819819294

Epoch: 6| Step: 11
Training loss: 2.320368766784668
Validation loss: 2.402937435334729

Epoch: 6| Step: 12
Training loss: 2.61881160736084
Validation loss: 2.4010885812902965

Epoch: 6| Step: 13
Training loss: 2.631549835205078
Validation loss: 2.410348774284445

Epoch: 215| Step: 0
Training loss: 2.9104161262512207
Validation loss: 2.407930789455291

Epoch: 6| Step: 1
Training loss: 2.5692780017852783
Validation loss: 2.410299144765382

Epoch: 6| Step: 2
Training loss: 1.7895115613937378
Validation loss: 2.4111593795079056

Epoch: 6| Step: 3
Training loss: 3.1169841289520264
Validation loss: 2.4021517486982447

Epoch: 6| Step: 4
Training loss: 2.101534843444824
Validation loss: 2.402418698033979

Epoch: 6| Step: 5
Training loss: 2.2646048069000244
Validation loss: 2.39398939635164

Epoch: 6| Step: 6
Training loss: 3.5038223266601562
Validation loss: 2.3914225139925556

Epoch: 6| Step: 7
Training loss: 2.27225661277771
Validation loss: 2.3911792078325824

Epoch: 6| Step: 8
Training loss: 2.2344210147857666
Validation loss: 2.405928337445823

Epoch: 6| Step: 9
Training loss: 3.036996364593506
Validation loss: 2.421345982500302

Epoch: 6| Step: 10
Training loss: 3.378282070159912
Validation loss: 2.432636263549969

Epoch: 6| Step: 11
Training loss: 2.9465951919555664
Validation loss: 2.4458316244104856

Epoch: 6| Step: 12
Training loss: 2.0551681518554688
Validation loss: 2.442160890948388

Epoch: 6| Step: 13
Training loss: 2.7964468002319336
Validation loss: 2.4618323259456183

Epoch: 216| Step: 0
Training loss: 3.356935501098633
Validation loss: 2.452820611256425

Epoch: 6| Step: 1
Training loss: 2.606473207473755
Validation loss: 2.449620162287066

Epoch: 6| Step: 2
Training loss: 1.8916631937026978
Validation loss: 2.4341468811035156

Epoch: 6| Step: 3
Training loss: 2.23909854888916
Validation loss: 2.430526041215466

Epoch: 6| Step: 4
Training loss: 2.4239776134490967
Validation loss: 2.443835912212249

Epoch: 6| Step: 5
Training loss: 2.4723339080810547
Validation loss: 2.4489821259693434

Epoch: 6| Step: 6
Training loss: 2.1947927474975586
Validation loss: 2.4362816413243613

Epoch: 6| Step: 7
Training loss: 2.866633892059326
Validation loss: 2.4267854793097383

Epoch: 6| Step: 8
Training loss: 2.648723840713501
Validation loss: 2.433537147378409

Epoch: 6| Step: 9
Training loss: 3.1775734424591064
Validation loss: 2.433249722244919

Epoch: 6| Step: 10
Training loss: 2.1963984966278076
Validation loss: 2.4129097846246537

Epoch: 6| Step: 11
Training loss: 3.197690963745117
Validation loss: 2.4102759668903966

Epoch: 6| Step: 12
Training loss: 2.5682296752929688
Validation loss: 2.404380472757483

Epoch: 6| Step: 13
Training loss: 2.864225149154663
Validation loss: 2.3922394526902067

Epoch: 217| Step: 0
Training loss: 2.5309762954711914
Validation loss: 2.392228267526114

Epoch: 6| Step: 1
Training loss: 2.608391523361206
Validation loss: 2.3898105467519453

Epoch: 6| Step: 2
Training loss: 2.705345630645752
Validation loss: 2.393327238739178

Epoch: 6| Step: 3
Training loss: 2.3397321701049805
Validation loss: 2.3953837220386793

Epoch: 6| Step: 4
Training loss: 2.7146222591400146
Validation loss: 2.391417921230357

Epoch: 6| Step: 5
Training loss: 2.6068849563598633
Validation loss: 2.4008965876794632

Epoch: 6| Step: 6
Training loss: 3.001157283782959
Validation loss: 2.3982225233508694

Epoch: 6| Step: 7
Training loss: 2.854329824447632
Validation loss: 2.399377335784256

Epoch: 6| Step: 8
Training loss: 2.8337011337280273
Validation loss: 2.4000791657355522

Epoch: 6| Step: 9
Training loss: 2.286242723464966
Validation loss: 2.403406084224742

Epoch: 6| Step: 10
Training loss: 2.0355257987976074
Validation loss: 2.404130615213866

Epoch: 6| Step: 11
Training loss: 2.7776031494140625
Validation loss: 2.4084900681690504

Epoch: 6| Step: 12
Training loss: 2.2336182594299316
Validation loss: 2.411185610678888

Epoch: 6| Step: 13
Training loss: 2.812516927719116
Validation loss: 2.417030216545187

Epoch: 218| Step: 0
Training loss: 2.28922176361084
Validation loss: 2.43313165121181

Epoch: 6| Step: 1
Training loss: 2.617417335510254
Validation loss: 2.4440844443536576

Epoch: 6| Step: 2
Training loss: 2.62839937210083
Validation loss: 2.4445717821839037

Epoch: 6| Step: 3
Training loss: 2.3157758712768555
Validation loss: 2.422142631264143

Epoch: 6| Step: 4
Training loss: 3.09565806388855
Validation loss: 2.415178491223243

Epoch: 6| Step: 5
Training loss: 2.6703829765319824
Validation loss: 2.407358190064789

Epoch: 6| Step: 6
Training loss: 2.1300158500671387
Validation loss: 2.4137290062442904

Epoch: 6| Step: 7
Training loss: 2.683730125427246
Validation loss: 2.423881648689188

Epoch: 6| Step: 8
Training loss: 2.307732582092285
Validation loss: 2.419022660101614

Epoch: 6| Step: 9
Training loss: 2.029013156890869
Validation loss: 2.417044440905253

Epoch: 6| Step: 10
Training loss: 2.721564769744873
Validation loss: 2.4150096319055043

Epoch: 6| Step: 11
Training loss: 3.3136355876922607
Validation loss: 2.413686788210305

Epoch: 6| Step: 12
Training loss: 2.7207508087158203
Validation loss: 2.3969582050077376

Epoch: 6| Step: 13
Training loss: 3.0760018825531006
Validation loss: 2.407473876912107

Epoch: 219| Step: 0
Training loss: 3.364910364151001
Validation loss: 2.4053191420852498

Epoch: 6| Step: 1
Training loss: 2.134566307067871
Validation loss: 2.4034031975653862

Epoch: 6| Step: 2
Training loss: 2.882503032684326
Validation loss: 2.4074385422532276

Epoch: 6| Step: 3
Training loss: 3.3652262687683105
Validation loss: 2.388715608145601

Epoch: 6| Step: 4
Training loss: 1.7671356201171875
Validation loss: 2.3909969099106325

Epoch: 6| Step: 5
Training loss: 2.591799259185791
Validation loss: 2.382881554224158

Epoch: 6| Step: 6
Training loss: 2.3475143909454346
Validation loss: 2.3796732143689225

Epoch: 6| Step: 7
Training loss: 2.8862452507019043
Validation loss: 2.3845588904555126

Epoch: 6| Step: 8
Training loss: 2.711672306060791
Validation loss: 2.3768561963112123

Epoch: 6| Step: 9
Training loss: 2.0784707069396973
Validation loss: 2.3756508032480874

Epoch: 6| Step: 10
Training loss: 3.01936674118042
Validation loss: 2.3849050716687272

Epoch: 6| Step: 11
Training loss: 2.9284656047821045
Validation loss: 2.3890085194700506

Epoch: 6| Step: 12
Training loss: 2.0319995880126953
Validation loss: 2.389269223777197

Epoch: 6| Step: 13
Training loss: 2.0241706371307373
Validation loss: 2.392425629400438

Epoch: 220| Step: 0
Training loss: 1.6937196254730225
Validation loss: 2.4017280276103685

Epoch: 6| Step: 1
Training loss: 2.818009614944458
Validation loss: 2.4051287225497666

Epoch: 6| Step: 2
Training loss: 2.4137485027313232
Validation loss: 2.4152624504540556

Epoch: 6| Step: 3
Training loss: 2.8074474334716797
Validation loss: 2.4128848301467074

Epoch: 6| Step: 4
Training loss: 3.1023757457733154
Validation loss: 2.4196107438815537

Epoch: 6| Step: 5
Training loss: 3.2056832313537598
Validation loss: 2.422843187086044

Epoch: 6| Step: 6
Training loss: 2.440310478210449
Validation loss: 2.413505913108908

Epoch: 6| Step: 7
Training loss: 2.0132975578308105
Validation loss: 2.4153055990895917

Epoch: 6| Step: 8
Training loss: 2.2777419090270996
Validation loss: 2.415682203026228

Epoch: 6| Step: 9
Training loss: 2.738102436065674
Validation loss: 2.410626875456943

Epoch: 6| Step: 10
Training loss: 3.0024020671844482
Validation loss: 2.4001700185960337

Epoch: 6| Step: 11
Training loss: 2.6148037910461426
Validation loss: 2.3951032725713586

Epoch: 6| Step: 12
Training loss: 2.3670945167541504
Validation loss: 2.4007236009003012

Epoch: 6| Step: 13
Training loss: 2.8272335529327393
Validation loss: 2.40953690262251

Epoch: 221| Step: 0
Training loss: 1.4740623235702515
Validation loss: 2.4008308302971626

Epoch: 6| Step: 1
Training loss: 2.2790870666503906
Validation loss: 2.4070954220269316

Epoch: 6| Step: 2
Training loss: 2.480865001678467
Validation loss: 2.4053474805688344

Epoch: 6| Step: 3
Training loss: 2.3545684814453125
Validation loss: 2.4097704836117324

Epoch: 6| Step: 4
Training loss: 2.3276116847991943
Validation loss: 2.4116542211142917

Epoch: 6| Step: 5
Training loss: 2.5512616634368896
Validation loss: 2.4094030472540084

Epoch: 6| Step: 6
Training loss: 3.2542128562927246
Validation loss: 2.4202599576724473

Epoch: 6| Step: 7
Training loss: 3.0915513038635254
Validation loss: 2.4265446355265956

Epoch: 6| Step: 8
Training loss: 2.6791367530822754
Validation loss: 2.4095418350670927

Epoch: 6| Step: 9
Training loss: 3.715665102005005
Validation loss: 2.421607255935669

Epoch: 6| Step: 10
Training loss: 2.326512336730957
Validation loss: 2.4282006961043163

Epoch: 6| Step: 11
Training loss: 2.093956470489502
Validation loss: 2.4349723272426154

Epoch: 6| Step: 12
Training loss: 3.3708691596984863
Validation loss: 2.4394610748496106

Epoch: 6| Step: 13
Training loss: 1.9823551177978516
Validation loss: 2.445077138562356

Epoch: 222| Step: 0
Training loss: 2.123488426208496
Validation loss: 2.4310358339740383

Epoch: 6| Step: 1
Training loss: 2.528622627258301
Validation loss: 2.4387948487394597

Epoch: 6| Step: 2
Training loss: 2.396646499633789
Validation loss: 2.438356722554853

Epoch: 6| Step: 3
Training loss: 3.1528592109680176
Validation loss: 2.4302077677942093

Epoch: 6| Step: 4
Training loss: 2.81718111038208
Validation loss: 2.4296220015454035

Epoch: 6| Step: 5
Training loss: 2.284043788909912
Validation loss: 2.4318026906700543

Epoch: 6| Step: 6
Training loss: 2.8066439628601074
Validation loss: 2.4204716323524393

Epoch: 6| Step: 7
Training loss: 2.3667192459106445
Validation loss: 2.418744082091957

Epoch: 6| Step: 8
Training loss: 1.9116460084915161
Validation loss: 2.409210030750562

Epoch: 6| Step: 9
Training loss: 3.175579071044922
Validation loss: 2.4156426588694253

Epoch: 6| Step: 10
Training loss: 2.3602232933044434
Validation loss: 2.4209240226335424

Epoch: 6| Step: 11
Training loss: 2.2795913219451904
Validation loss: 2.4068136676665275

Epoch: 6| Step: 12
Training loss: 3.1846461296081543
Validation loss: 2.415909849187379

Epoch: 6| Step: 13
Training loss: 3.077460765838623
Validation loss: 2.4237323499494985

Epoch: 223| Step: 0
Training loss: 3.3174643516540527
Validation loss: 2.4095835429365917

Epoch: 6| Step: 1
Training loss: 1.9044106006622314
Validation loss: 2.3908894420951925

Epoch: 6| Step: 2
Training loss: 1.9947625398635864
Validation loss: 2.39802388991079

Epoch: 6| Step: 3
Training loss: 2.2818849086761475
Validation loss: 2.387202001387073

Epoch: 6| Step: 4
Training loss: 2.386526346206665
Validation loss: 2.384674290175079

Epoch: 6| Step: 5
Training loss: 3.2164111137390137
Validation loss: 2.390408108311315

Epoch: 6| Step: 6
Training loss: 2.5197935104370117
Validation loss: 2.387251659106183

Epoch: 6| Step: 7
Training loss: 2.384021282196045
Validation loss: 2.393570238544095

Epoch: 6| Step: 8
Training loss: 2.6511776447296143
Validation loss: 2.4039237678691907

Epoch: 6| Step: 9
Training loss: 2.525439739227295
Validation loss: 2.393065098793276

Epoch: 6| Step: 10
Training loss: 2.3429155349731445
Validation loss: 2.40667135741121

Epoch: 6| Step: 11
Training loss: 3.1194982528686523
Validation loss: 2.413767240380728

Epoch: 6| Step: 12
Training loss: 2.7533183097839355
Validation loss: 2.4217963577598653

Epoch: 6| Step: 13
Training loss: 2.9643774032592773
Validation loss: 2.418430748806205

Epoch: 224| Step: 0
Training loss: 2.3824996948242188
Validation loss: 2.4164989456053703

Epoch: 6| Step: 1
Training loss: 2.7631969451904297
Validation loss: 2.414052330037599

Epoch: 6| Step: 2
Training loss: 2.3402419090270996
Validation loss: 2.425556226443219

Epoch: 6| Step: 3
Training loss: 2.436643600463867
Validation loss: 2.4039941090409473

Epoch: 6| Step: 4
Training loss: 3.2916388511657715
Validation loss: 2.4021300320984214

Epoch: 6| Step: 5
Training loss: 2.4192888736724854
Validation loss: 2.3980446989818285

Epoch: 6| Step: 6
Training loss: 1.8245359659194946
Validation loss: 2.401157454777789

Epoch: 6| Step: 7
Training loss: 2.445493459701538
Validation loss: 2.397127946217855

Epoch: 6| Step: 8
Training loss: 3.0671334266662598
Validation loss: 2.4028359382383284

Epoch: 6| Step: 9
Training loss: 3.24063777923584
Validation loss: 2.404380759885234

Epoch: 6| Step: 10
Training loss: 2.502699613571167
Validation loss: 2.42329902033652

Epoch: 6| Step: 11
Training loss: 2.721202850341797
Validation loss: 2.437075868729622

Epoch: 6| Step: 12
Training loss: 2.1743111610412598
Validation loss: 2.4295336815618698

Epoch: 6| Step: 13
Training loss: 2.85308575630188
Validation loss: 2.430666999150348

Epoch: 225| Step: 0
Training loss: 2.979811906814575
Validation loss: 2.4328099399484615

Epoch: 6| Step: 1
Training loss: 1.717965841293335
Validation loss: 2.4118987514126684

Epoch: 6| Step: 2
Training loss: 3.2298593521118164
Validation loss: 2.3981316525449037

Epoch: 6| Step: 3
Training loss: 2.076524257659912
Validation loss: 2.3960717467851538

Epoch: 6| Step: 4
Training loss: 2.325209617614746
Validation loss: 2.3979261472661006

Epoch: 6| Step: 5
Training loss: 3.0276503562927246
Validation loss: 2.401479418559741

Epoch: 6| Step: 6
Training loss: 2.5497169494628906
Validation loss: 2.3994011058602283

Epoch: 6| Step: 7
Training loss: 3.651989459991455
Validation loss: 2.397298594956757

Epoch: 6| Step: 8
Training loss: 2.5526983737945557
Validation loss: 2.403716115541356

Epoch: 6| Step: 9
Training loss: 2.1101200580596924
Validation loss: 2.407685499037466

Epoch: 6| Step: 10
Training loss: 1.956390142440796
Validation loss: 2.400412319808878

Epoch: 6| Step: 11
Training loss: 2.9318628311157227
Validation loss: 2.403373892589282

Epoch: 6| Step: 12
Training loss: 2.6047098636627197
Validation loss: 2.3965444590455744

Epoch: 6| Step: 13
Training loss: 2.4604227542877197
Validation loss: 2.398092798007432

Epoch: 226| Step: 0
Training loss: 2.2885608673095703
Validation loss: 2.3979973869939006

Epoch: 6| Step: 1
Training loss: 2.6713759899139404
Validation loss: 2.3864945724446285

Epoch: 6| Step: 2
Training loss: 3.2387261390686035
Validation loss: 2.3928213760416996

Epoch: 6| Step: 3
Training loss: 2.391610622406006
Validation loss: 2.3902731608319026

Epoch: 6| Step: 4
Training loss: 2.416079521179199
Validation loss: 2.3990507741128244

Epoch: 6| Step: 5
Training loss: 1.8988443613052368
Validation loss: 2.388340914121238

Epoch: 6| Step: 6
Training loss: 2.569732666015625
Validation loss: 2.4024908927179154

Epoch: 6| Step: 7
Training loss: 2.245868444442749
Validation loss: 2.4092763854611303

Epoch: 6| Step: 8
Training loss: 2.3495614528656006
Validation loss: 2.422851231790358

Epoch: 6| Step: 9
Training loss: 3.289072036743164
Validation loss: 2.4340984334227858

Epoch: 6| Step: 10
Training loss: 2.7648544311523438
Validation loss: 2.454856582867202

Epoch: 6| Step: 11
Training loss: 2.015089988708496
Validation loss: 2.456921656926473

Epoch: 6| Step: 12
Training loss: 2.621471881866455
Validation loss: 2.470846750402963

Epoch: 6| Step: 13
Training loss: 4.029004096984863
Validation loss: 2.4802522197846444

Epoch: 227| Step: 0
Training loss: 1.7650789022445679
Validation loss: 2.465964845431748

Epoch: 6| Step: 1
Training loss: 2.741819381713867
Validation loss: 2.4818815338996147

Epoch: 6| Step: 2
Training loss: 2.66216778755188
Validation loss: 2.477938154692291

Epoch: 6| Step: 3
Training loss: 2.4986565113067627
Validation loss: 2.4360367431435535

Epoch: 6| Step: 4
Training loss: 3.335406541824341
Validation loss: 2.4145458488054174

Epoch: 6| Step: 5
Training loss: 2.3135910034179688
Validation loss: 2.3928025332830285

Epoch: 6| Step: 6
Training loss: 1.8630998134613037
Validation loss: 2.3884591876819568

Epoch: 6| Step: 7
Training loss: 2.7662856578826904
Validation loss: 2.391567764743682

Epoch: 6| Step: 8
Training loss: 2.817992687225342
Validation loss: 2.3876599547683552

Epoch: 6| Step: 9
Training loss: 2.8338682651519775
Validation loss: 2.3946405072366037

Epoch: 6| Step: 10
Training loss: 2.363776206970215
Validation loss: 2.390950725924584

Epoch: 6| Step: 11
Training loss: 3.1053194999694824
Validation loss: 2.396708688428325

Epoch: 6| Step: 12
Training loss: 2.5639703273773193
Validation loss: 2.392663235305458

Epoch: 6| Step: 13
Training loss: 2.4743728637695312
Validation loss: 2.390241023032896

Epoch: 228| Step: 0
Training loss: 2.4403882026672363
Validation loss: 2.4226874946266093

Epoch: 6| Step: 1
Training loss: 2.025630474090576
Validation loss: 2.4291836420694985

Epoch: 6| Step: 2
Training loss: 2.590869903564453
Validation loss: 2.445760517991999

Epoch: 6| Step: 3
Training loss: 3.3222384452819824
Validation loss: 2.4400934301396853

Epoch: 6| Step: 4
Training loss: 2.6408796310424805
Validation loss: 2.438596808782188

Epoch: 6| Step: 5
Training loss: 3.078165054321289
Validation loss: 2.4176893106070896

Epoch: 6| Step: 6
Training loss: 2.6051652431488037
Validation loss: 2.385609849806755

Epoch: 6| Step: 7
Training loss: 2.2779040336608887
Validation loss: 2.376486014294368

Epoch: 6| Step: 8
Training loss: 2.2351016998291016
Validation loss: 2.3804443574720815

Epoch: 6| Step: 9
Training loss: 2.8594889640808105
Validation loss: 2.3775407627064693

Epoch: 6| Step: 10
Training loss: 1.5273325443267822
Validation loss: 2.3806964787103797

Epoch: 6| Step: 11
Training loss: 2.8930187225341797
Validation loss: 2.3813102347876436

Epoch: 6| Step: 12
Training loss: 2.768951892852783
Validation loss: 2.3739679064801944

Epoch: 6| Step: 13
Training loss: 3.1587705612182617
Validation loss: 2.384816272284395

Epoch: 229| Step: 0
Training loss: 2.0219509601593018
Validation loss: 2.383189985829015

Epoch: 6| Step: 1
Training loss: 3.561981201171875
Validation loss: 2.3860619273237003

Epoch: 6| Step: 2
Training loss: 2.595785140991211
Validation loss: 2.3871690534776255

Epoch: 6| Step: 3
Training loss: 2.326968193054199
Validation loss: 2.38867417458565

Epoch: 6| Step: 4
Training loss: 2.8352417945861816
Validation loss: 2.405663800495927

Epoch: 6| Step: 5
Training loss: 2.3574812412261963
Validation loss: 2.398588248478469

Epoch: 6| Step: 6
Training loss: 2.3976287841796875
Validation loss: 2.4050549230267926

Epoch: 6| Step: 7
Training loss: 2.5792012214660645
Validation loss: 2.4055052598317466

Epoch: 6| Step: 8
Training loss: 2.2694385051727295
Validation loss: 2.4139347589144142

Epoch: 6| Step: 9
Training loss: 2.382904052734375
Validation loss: 2.4300452688688874

Epoch: 6| Step: 10
Training loss: 2.955824851989746
Validation loss: 2.420757524428829

Epoch: 6| Step: 11
Training loss: 2.2358453273773193
Validation loss: 2.430212341329103

Epoch: 6| Step: 12
Training loss: 2.624427318572998
Validation loss: 2.4309648185647945

Epoch: 6| Step: 13
Training loss: 3.043504238128662
Validation loss: 2.4244641180961364

Epoch: 230| Step: 0
Training loss: 2.1048409938812256
Validation loss: 2.4134651768592095

Epoch: 6| Step: 1
Training loss: 2.2595767974853516
Validation loss: 2.4323346884019914

Epoch: 6| Step: 2
Training loss: 2.99332594871521
Validation loss: 2.419474547909152

Epoch: 6| Step: 3
Training loss: 2.204115390777588
Validation loss: 2.4377474195213726

Epoch: 6| Step: 4
Training loss: 3.1028876304626465
Validation loss: 2.439745738942136

Epoch: 6| Step: 5
Training loss: 2.0343008041381836
Validation loss: 2.4407668113708496

Epoch: 6| Step: 6
Training loss: 2.782587766647339
Validation loss: 2.422323014146538

Epoch: 6| Step: 7
Training loss: 2.9971816539764404
Validation loss: 2.4133348516238633

Epoch: 6| Step: 8
Training loss: 2.7848856449127197
Validation loss: 2.4037597487049718

Epoch: 6| Step: 9
Training loss: 1.9732398986816406
Validation loss: 2.4109163027937695

Epoch: 6| Step: 10
Training loss: 3.03310227394104
Validation loss: 2.4030104914019184

Epoch: 6| Step: 11
Training loss: 2.1004271507263184
Validation loss: 2.406125598056342

Epoch: 6| Step: 12
Training loss: 2.6399824619293213
Validation loss: 2.402497440256098

Epoch: 6| Step: 13
Training loss: 3.3341639041900635
Validation loss: 2.40625270335905

Epoch: 231| Step: 0
Training loss: 2.373586654663086
Validation loss: 2.3993876749469387

Epoch: 6| Step: 1
Training loss: 3.148094654083252
Validation loss: 2.3978654159012662

Epoch: 6| Step: 2
Training loss: 2.612107038497925
Validation loss: 2.3980486187883603

Epoch: 6| Step: 3
Training loss: 2.683695077896118
Validation loss: 2.3914654588186615

Epoch: 6| Step: 4
Training loss: 2.496940851211548
Validation loss: 2.398152030924315

Epoch: 6| Step: 5
Training loss: 3.2653756141662598
Validation loss: 2.4024668829415434

Epoch: 6| Step: 6
Training loss: 2.4946508407592773
Validation loss: 2.3950780950566775

Epoch: 6| Step: 7
Training loss: 2.3156981468200684
Validation loss: 2.3887335818300963

Epoch: 6| Step: 8
Training loss: 2.2825794219970703
Validation loss: 2.3863365291267313

Epoch: 6| Step: 9
Training loss: 2.742244243621826
Validation loss: 2.398549856678132

Epoch: 6| Step: 10
Training loss: 1.7475264072418213
Validation loss: 2.3917192015596616

Epoch: 6| Step: 11
Training loss: 2.379023313522339
Validation loss: 2.3999022745317027

Epoch: 6| Step: 12
Training loss: 2.8517255783081055
Validation loss: 2.4070363916376585

Epoch: 6| Step: 13
Training loss: 2.525315761566162
Validation loss: 2.4129363747053247

Epoch: 232| Step: 0
Training loss: 2.6089279651641846
Validation loss: 2.446922920083487

Epoch: 6| Step: 1
Training loss: 2.0670220851898193
Validation loss: 2.4530775931573685

Epoch: 6| Step: 2
Training loss: 2.437582492828369
Validation loss: 2.427724817747711

Epoch: 6| Step: 3
Training loss: 3.570058822631836
Validation loss: 2.4265055810251543

Epoch: 6| Step: 4
Training loss: 2.536376953125
Validation loss: 2.4193523494146203

Epoch: 6| Step: 5
Training loss: 2.580845355987549
Validation loss: 2.395556084571346

Epoch: 6| Step: 6
Training loss: 2.79073429107666
Validation loss: 2.3950616826293287

Epoch: 6| Step: 7
Training loss: 3.0311155319213867
Validation loss: 2.397366390433363

Epoch: 6| Step: 8
Training loss: 1.8164629936218262
Validation loss: 2.387395143508911

Epoch: 6| Step: 9
Training loss: 2.924537181854248
Validation loss: 2.379239151554723

Epoch: 6| Step: 10
Training loss: 2.5245141983032227
Validation loss: 2.3898708717797392

Epoch: 6| Step: 11
Training loss: 2.2711424827575684
Validation loss: 2.3814980509460613

Epoch: 6| Step: 12
Training loss: 2.1308131217956543
Validation loss: 2.3743434003604356

Epoch: 6| Step: 13
Training loss: 2.8588051795959473
Validation loss: 2.3776336177702873

Epoch: 233| Step: 0
Training loss: 2.7924671173095703
Validation loss: 2.3675066194226666

Epoch: 6| Step: 1
Training loss: 2.051394462585449
Validation loss: 2.3755366366396666

Epoch: 6| Step: 2
Training loss: 2.148650884628296
Validation loss: 2.376519822305249

Epoch: 6| Step: 3
Training loss: 2.2704219818115234
Validation loss: 2.3724825612960325

Epoch: 6| Step: 4
Training loss: 2.1879329681396484
Validation loss: 2.381220820129559

Epoch: 6| Step: 5
Training loss: 2.8952128887176514
Validation loss: 2.3789409745124077

Epoch: 6| Step: 6
Training loss: 3.1798505783081055
Validation loss: 2.3702579967437254

Epoch: 6| Step: 7
Training loss: 2.4529247283935547
Validation loss: 2.3828568407284316

Epoch: 6| Step: 8
Training loss: 2.9957518577575684
Validation loss: 2.3691443217697965

Epoch: 6| Step: 9
Training loss: 2.84731388092041
Validation loss: 2.3890658424746607

Epoch: 6| Step: 10
Training loss: 3.1536405086517334
Validation loss: 2.3858291513176373

Epoch: 6| Step: 11
Training loss: 2.1989634037017822
Validation loss: 2.3768577344955935

Epoch: 6| Step: 12
Training loss: 2.1196129322052
Validation loss: 2.391911152870424

Epoch: 6| Step: 13
Training loss: 2.7683005332946777
Validation loss: 2.39017237899124

Epoch: 234| Step: 0
Training loss: 1.937729001045227
Validation loss: 2.39795470494096

Epoch: 6| Step: 1
Training loss: 2.4925079345703125
Validation loss: 2.409018554995137

Epoch: 6| Step: 2
Training loss: 2.6740589141845703
Validation loss: 2.4242639541625977

Epoch: 6| Step: 3
Training loss: 2.6828155517578125
Validation loss: 2.4554610149834746

Epoch: 6| Step: 4
Training loss: 2.9423513412475586
Validation loss: 2.449344524773218

Epoch: 6| Step: 5
Training loss: 2.3828887939453125
Validation loss: 2.4578364638872046

Epoch: 6| Step: 6
Training loss: 2.188692569732666
Validation loss: 2.443384867842479

Epoch: 6| Step: 7
Training loss: 2.5235774517059326
Validation loss: 2.4473400551785707

Epoch: 6| Step: 8
Training loss: 3.9581196308135986
Validation loss: 2.4261389778506373

Epoch: 6| Step: 9
Training loss: 2.59804105758667
Validation loss: 2.407448927561442

Epoch: 6| Step: 10
Training loss: 2.2406702041625977
Validation loss: 2.4023591831166256

Epoch: 6| Step: 11
Training loss: 2.512371063232422
Validation loss: 2.382231245758713

Epoch: 6| Step: 12
Training loss: 2.470940113067627
Validation loss: 2.3817885357846498

Epoch: 6| Step: 13
Training loss: 2.254783868789673
Validation loss: 2.378656215565179

Epoch: 235| Step: 0
Training loss: 1.8430569171905518
Validation loss: 2.3686478625061693

Epoch: 6| Step: 1
Training loss: 2.6592257022857666
Validation loss: 2.374483775067073

Epoch: 6| Step: 2
Training loss: 2.7943644523620605
Validation loss: 2.3786929730446107

Epoch: 6| Step: 3
Training loss: 2.8532607555389404
Validation loss: 2.3815117036142657

Epoch: 6| Step: 4
Training loss: 3.116171360015869
Validation loss: 2.3735781946489887

Epoch: 6| Step: 5
Training loss: 1.9449108839035034
Validation loss: 2.3730775207601567

Epoch: 6| Step: 6
Training loss: 2.808680534362793
Validation loss: 2.37593012984081

Epoch: 6| Step: 7
Training loss: 3.355020523071289
Validation loss: 2.381571098040509

Epoch: 6| Step: 8
Training loss: 2.186500072479248
Validation loss: 2.3757284469501947

Epoch: 6| Step: 9
Training loss: 2.1404850482940674
Validation loss: 2.3796701456910823

Epoch: 6| Step: 10
Training loss: 2.4964170455932617
Validation loss: 2.374699910481771

Epoch: 6| Step: 11
Training loss: 2.6596455574035645
Validation loss: 2.3853913712245163

Epoch: 6| Step: 12
Training loss: 2.1802167892456055
Validation loss: 2.3872876628752677

Epoch: 6| Step: 13
Training loss: 3.1861066818237305
Validation loss: 2.3854050841382755

Epoch: 236| Step: 0
Training loss: 2.9576821327209473
Validation loss: 2.3988055311223513

Epoch: 6| Step: 1
Training loss: 2.2583329677581787
Validation loss: 2.4089007351988103

Epoch: 6| Step: 2
Training loss: 2.523069381713867
Validation loss: 2.4124638008814987

Epoch: 6| Step: 3
Training loss: 2.641496181488037
Validation loss: 2.4057542457375476

Epoch: 6| Step: 4
Training loss: 2.685398578643799
Validation loss: 2.4252651583763862

Epoch: 6| Step: 5
Training loss: 1.8567023277282715
Validation loss: 2.3942556406861994

Epoch: 6| Step: 6
Training loss: 2.7293946743011475
Validation loss: 2.4119567563456874

Epoch: 6| Step: 7
Training loss: 2.0015716552734375
Validation loss: 2.4269261026895173

Epoch: 6| Step: 8
Training loss: 2.5160560607910156
Validation loss: 2.4074881128085557

Epoch: 6| Step: 9
Training loss: 3.201418399810791
Validation loss: 2.4185702877659954

Epoch: 6| Step: 10
Training loss: 2.6817076206207275
Validation loss: 2.4131007861065608

Epoch: 6| Step: 11
Training loss: 2.1646132469177246
Validation loss: 2.404980008320142

Epoch: 6| Step: 12
Training loss: 3.0898046493530273
Validation loss: 2.4078913914260043

Epoch: 6| Step: 13
Training loss: 2.6825530529022217
Validation loss: 2.4085893348980973

Epoch: 237| Step: 0
Training loss: 2.186439037322998
Validation loss: 2.4100485309477775

Epoch: 6| Step: 1
Training loss: 3.276127576828003
Validation loss: 2.3859338939830823

Epoch: 6| Step: 2
Training loss: 2.5615930557250977
Validation loss: 2.3838190135135444

Epoch: 6| Step: 3
Training loss: 2.5473997592926025
Validation loss: 2.368942919597831

Epoch: 6| Step: 4
Training loss: 3.101329803466797
Validation loss: 2.3671094704699773

Epoch: 6| Step: 5
Training loss: 2.680359125137329
Validation loss: 2.363674335582282

Epoch: 6| Step: 6
Training loss: 2.7171876430511475
Validation loss: 2.3561997259816816

Epoch: 6| Step: 7
Training loss: 2.639740228652954
Validation loss: 2.362771995605961

Epoch: 6| Step: 8
Training loss: 2.5762758255004883
Validation loss: 2.3667197535114903

Epoch: 6| Step: 9
Training loss: 1.9991426467895508
Validation loss: 2.357163883024646

Epoch: 6| Step: 10
Training loss: 2.851036787033081
Validation loss: 2.378551742082001

Epoch: 6| Step: 11
Training loss: 1.8458187580108643
Validation loss: 2.3865414152863207

Epoch: 6| Step: 12
Training loss: 2.3629231452941895
Validation loss: 2.369981999038368

Epoch: 6| Step: 13
Training loss: 2.6022870540618896
Validation loss: 2.3885022286445863

Epoch: 238| Step: 0
Training loss: 2.7419073581695557
Validation loss: 2.371323177891393

Epoch: 6| Step: 1
Training loss: 3.2133889198303223
Validation loss: 2.374684808074787

Epoch: 6| Step: 2
Training loss: 2.1393775939941406
Validation loss: 2.384489954158824

Epoch: 6| Step: 3
Training loss: 2.587114095687866
Validation loss: 2.3765768107547554

Epoch: 6| Step: 4
Training loss: 2.253355026245117
Validation loss: 2.390352549091462

Epoch: 6| Step: 5
Training loss: 2.9955074787139893
Validation loss: 2.39482105675564

Epoch: 6| Step: 6
Training loss: 3.629000663757324
Validation loss: 2.3971287204373266

Epoch: 6| Step: 7
Training loss: 2.901042938232422
Validation loss: 2.4011156584626887

Epoch: 6| Step: 8
Training loss: 2.375382423400879
Validation loss: 2.4127280225035963

Epoch: 6| Step: 9
Training loss: 2.139312505722046
Validation loss: 2.4125204214485745

Epoch: 6| Step: 10
Training loss: 2.588430881500244
Validation loss: 2.4159546770075315

Epoch: 6| Step: 11
Training loss: 1.6648035049438477
Validation loss: 2.412488063176473

Epoch: 6| Step: 12
Training loss: 1.9201719760894775
Validation loss: 2.4314959959317277

Epoch: 6| Step: 13
Training loss: 2.723565101623535
Validation loss: 2.421810314219485

Epoch: 239| Step: 0
Training loss: 2.597853183746338
Validation loss: 2.4347680461022163

Epoch: 6| Step: 1
Training loss: 2.8986613750457764
Validation loss: 2.434705782962102

Epoch: 6| Step: 2
Training loss: 2.1734585762023926
Validation loss: 2.4356579780578613

Epoch: 6| Step: 3
Training loss: 1.897798776626587
Validation loss: 2.431251513060703

Epoch: 6| Step: 4
Training loss: 2.094987392425537
Validation loss: 2.434496073312657

Epoch: 6| Step: 5
Training loss: 2.3093550205230713
Validation loss: 2.4443426593657462

Epoch: 6| Step: 6
Training loss: 3.5135552883148193
Validation loss: 2.436455770205426

Epoch: 6| Step: 7
Training loss: 2.135572671890259
Validation loss: 2.4330187100236134

Epoch: 6| Step: 8
Training loss: 2.2489185333251953
Validation loss: 2.4056391946731077

Epoch: 6| Step: 9
Training loss: 2.479503870010376
Validation loss: 2.400374294609152

Epoch: 6| Step: 10
Training loss: 3.4080567359924316
Validation loss: 2.3836659564766833

Epoch: 6| Step: 11
Training loss: 3.3234617710113525
Validation loss: 2.371688824827953

Epoch: 6| Step: 12
Training loss: 2.046961784362793
Validation loss: 2.3688750369574434

Epoch: 6| Step: 13
Training loss: 2.5641698837280273
Validation loss: 2.3693410504248833

Epoch: 240| Step: 0
Training loss: 2.9528796672821045
Validation loss: 2.3660594929930983

Epoch: 6| Step: 1
Training loss: 2.9435856342315674
Validation loss: 2.363625490537254

Epoch: 6| Step: 2
Training loss: 2.215089797973633
Validation loss: 2.3638501859480336

Epoch: 6| Step: 3
Training loss: 2.1615242958068848
Validation loss: 2.357073965892997

Epoch: 6| Step: 4
Training loss: 2.328941822052002
Validation loss: 2.363696967401812

Epoch: 6| Step: 5
Training loss: 2.470993995666504
Validation loss: 2.361738230592461

Epoch: 6| Step: 6
Training loss: 2.7008252143859863
Validation loss: 2.359887742227124

Epoch: 6| Step: 7
Training loss: 2.5244784355163574
Validation loss: 2.3595376476164787

Epoch: 6| Step: 8
Training loss: 2.8441481590270996
Validation loss: 2.3555143879305933

Epoch: 6| Step: 9
Training loss: 2.550985813140869
Validation loss: 2.3602945919959777

Epoch: 6| Step: 10
Training loss: 2.6629152297973633
Validation loss: 2.3618686968280422

Epoch: 6| Step: 11
Training loss: 2.958435535430908
Validation loss: 2.3682611219344603

Epoch: 6| Step: 12
Training loss: 2.1808876991271973
Validation loss: 2.3768939561741327

Epoch: 6| Step: 13
Training loss: 2.526723623275757
Validation loss: 2.3727322163120395

Epoch: 241| Step: 0
Training loss: 3.094756603240967
Validation loss: 2.3865475270055954

Epoch: 6| Step: 1
Training loss: 2.5916593074798584
Validation loss: 2.3780640581602692

Epoch: 6| Step: 2
Training loss: 2.443509578704834
Validation loss: 2.3901918498418664

Epoch: 6| Step: 3
Training loss: 2.0782246589660645
Validation loss: 2.3872805692816295

Epoch: 6| Step: 4
Training loss: 3.030219078063965
Validation loss: 2.412947139432353

Epoch: 6| Step: 5
Training loss: 2.454885482788086
Validation loss: 2.3893261968448596

Epoch: 6| Step: 6
Training loss: 1.9331598281860352
Validation loss: 2.412916132198867

Epoch: 6| Step: 7
Training loss: 1.8983774185180664
Validation loss: 2.4214100786434707

Epoch: 6| Step: 8
Training loss: 3.3322677612304688
Validation loss: 2.433051340041622

Epoch: 6| Step: 9
Training loss: 2.5433311462402344
Validation loss: 2.433556390065019

Epoch: 6| Step: 10
Training loss: 2.663534164428711
Validation loss: 2.4260887920215564

Epoch: 6| Step: 11
Training loss: 3.122713565826416
Validation loss: 2.4157116131115983

Epoch: 6| Step: 12
Training loss: 2.4588894844055176
Validation loss: 2.3896160100096013

Epoch: 6| Step: 13
Training loss: 1.6566407680511475
Validation loss: 2.3840999859635548

Epoch: 242| Step: 0
Training loss: 2.1511497497558594
Validation loss: 2.3748778015054683

Epoch: 6| Step: 1
Training loss: 1.7943592071533203
Validation loss: 2.3761346955453195

Epoch: 6| Step: 2
Training loss: 3.3288707733154297
Validation loss: 2.3835050521358365

Epoch: 6| Step: 3
Training loss: 3.0752410888671875
Validation loss: 2.377182481109455

Epoch: 6| Step: 4
Training loss: 3.384963035583496
Validation loss: 2.383382534468046

Epoch: 6| Step: 5
Training loss: 1.6802095174789429
Validation loss: 2.3813865338602374

Epoch: 6| Step: 6
Training loss: 2.9184741973876953
Validation loss: 2.383395705171811

Epoch: 6| Step: 7
Training loss: 1.879234790802002
Validation loss: 2.3835138223504506

Epoch: 6| Step: 8
Training loss: 2.7865023612976074
Validation loss: 2.3859031200408936

Epoch: 6| Step: 9
Training loss: 3.0901038646698
Validation loss: 2.383067143860684

Epoch: 6| Step: 10
Training loss: 2.263704776763916
Validation loss: 2.3888531397747736

Epoch: 6| Step: 11
Training loss: 2.3346657752990723
Validation loss: 2.405231821921564

Epoch: 6| Step: 12
Training loss: 2.576777696609497
Validation loss: 2.392771415812995

Epoch: 6| Step: 13
Training loss: 2.317974328994751
Validation loss: 2.4060649333461637

Epoch: 243| Step: 0
Training loss: 2.8371450901031494
Validation loss: 2.4178718405385173

Epoch: 6| Step: 1
Training loss: 2.1273903846740723
Validation loss: 2.4249106325129026

Epoch: 6| Step: 2
Training loss: 2.773972272872925
Validation loss: 2.436460082248975

Epoch: 6| Step: 3
Training loss: 2.146712303161621
Validation loss: 2.4439556009025982

Epoch: 6| Step: 4
Training loss: 2.831839084625244
Validation loss: 2.456440246233376

Epoch: 6| Step: 5
Training loss: 2.7745652198791504
Validation loss: 2.452668415602817

Epoch: 6| Step: 6
Training loss: 3.323652744293213
Validation loss: 2.4611684596666725

Epoch: 6| Step: 7
Training loss: 2.505704402923584
Validation loss: 2.424212089148901

Epoch: 6| Step: 8
Training loss: 2.854222297668457
Validation loss: 2.4141304595496065

Epoch: 6| Step: 9
Training loss: 2.359649181365967
Validation loss: 2.399753103974045

Epoch: 6| Step: 10
Training loss: 2.249211311340332
Validation loss: 2.3789550847904657

Epoch: 6| Step: 11
Training loss: 2.3210320472717285
Validation loss: 2.367138993355536

Epoch: 6| Step: 12
Training loss: 2.5415139198303223
Validation loss: 2.3632420929529334

Epoch: 6| Step: 13
Training loss: 1.8818520307540894
Validation loss: 2.360158015322942

Epoch: 244| Step: 0
Training loss: 2.136770248413086
Validation loss: 2.3614762290831535

Epoch: 6| Step: 1
Training loss: 3.3177242279052734
Validation loss: 2.3604471337410713

Epoch: 6| Step: 2
Training loss: 2.465468406677246
Validation loss: 2.3550126296217724

Epoch: 6| Step: 3
Training loss: 1.8337574005126953
Validation loss: 2.3639859999379804

Epoch: 6| Step: 4
Training loss: 2.6078455448150635
Validation loss: 2.3653827533927014

Epoch: 6| Step: 5
Training loss: 1.6336417198181152
Validation loss: 2.3566725369422667

Epoch: 6| Step: 6
Training loss: 2.362311840057373
Validation loss: 2.3640139923300794

Epoch: 6| Step: 7
Training loss: 2.483100414276123
Validation loss: 2.357862598152571

Epoch: 6| Step: 8
Training loss: 2.7139580249786377
Validation loss: 2.365345116584532

Epoch: 6| Step: 9
Training loss: 2.9261422157287598
Validation loss: 2.361324874303674

Epoch: 6| Step: 10
Training loss: 3.139972448348999
Validation loss: 2.3698098710788194

Epoch: 6| Step: 11
Training loss: 3.4538018703460693
Validation loss: 2.3754910217818392

Epoch: 6| Step: 12
Training loss: 2.1667239665985107
Validation loss: 2.3853909738602175

Epoch: 6| Step: 13
Training loss: 2.5259127616882324
Validation loss: 2.407861363503241

Epoch: 245| Step: 0
Training loss: 2.750478982925415
Validation loss: 2.4287982525364047

Epoch: 6| Step: 1
Training loss: 2.2679755687713623
Validation loss: 2.40948078965628

Epoch: 6| Step: 2
Training loss: 2.936018466949463
Validation loss: 2.4412554105122886

Epoch: 6| Step: 3
Training loss: 2.168313503265381
Validation loss: 2.4529497226079306

Epoch: 6| Step: 4
Training loss: 2.840009927749634
Validation loss: 2.432847910029914

Epoch: 6| Step: 5
Training loss: 2.722301959991455
Validation loss: 2.4238524936860606

Epoch: 6| Step: 6
Training loss: 2.282841205596924
Validation loss: 2.3931699286225023

Epoch: 6| Step: 7
Training loss: 2.4923059940338135
Validation loss: 2.375330485323424

Epoch: 6| Step: 8
Training loss: 2.950700521469116
Validation loss: 2.36513110386428

Epoch: 6| Step: 9
Training loss: 2.6748530864715576
Validation loss: 2.363564283617081

Epoch: 6| Step: 10
Training loss: 2.506230115890503
Validation loss: 2.3668363504512335

Epoch: 6| Step: 11
Training loss: 2.7053565979003906
Validation loss: 2.367164963035173

Epoch: 6| Step: 12
Training loss: 2.218904733657837
Validation loss: 2.35870462079202

Epoch: 6| Step: 13
Training loss: 2.142693281173706
Validation loss: 2.3530135616179435

Epoch: 246| Step: 0
Training loss: 2.1096506118774414
Validation loss: 2.37028593145391

Epoch: 6| Step: 1
Training loss: 3.1463522911071777
Validation loss: 2.353563383061399

Epoch: 6| Step: 2
Training loss: 2.646247148513794
Validation loss: 2.3608543719014814

Epoch: 6| Step: 3
Training loss: 2.6170034408569336
Validation loss: 2.3792416998135146

Epoch: 6| Step: 4
Training loss: 2.3369743824005127
Validation loss: 2.3794925161587295

Epoch: 6| Step: 5
Training loss: 1.470908761024475
Validation loss: 2.3920569599315686

Epoch: 6| Step: 6
Training loss: 2.4723105430603027
Validation loss: 2.408125323633994

Epoch: 6| Step: 7
Training loss: 2.571702480316162
Validation loss: 2.417514926643782

Epoch: 6| Step: 8
Training loss: 2.6678080558776855
Validation loss: 2.424553571208831

Epoch: 6| Step: 9
Training loss: 2.747983694076538
Validation loss: 2.405225564074773

Epoch: 6| Step: 10
Training loss: 3.098881721496582
Validation loss: 2.3791639830476496

Epoch: 6| Step: 11
Training loss: 2.529310941696167
Validation loss: 2.379346537333663

Epoch: 6| Step: 12
Training loss: 2.7329061031341553
Validation loss: 2.3971878508085847

Epoch: 6| Step: 13
Training loss: 2.4223597049713135
Validation loss: 2.3836372308833624

Epoch: 247| Step: 0
Training loss: 2.9391844272613525
Validation loss: 2.3773833192804807

Epoch: 6| Step: 1
Training loss: 2.193378448486328
Validation loss: 2.3861925294322353

Epoch: 6| Step: 2
Training loss: 2.722764492034912
Validation loss: 2.391608411265958

Epoch: 6| Step: 3
Training loss: 2.110175848007202
Validation loss: 2.377953279402948

Epoch: 6| Step: 4
Training loss: 1.8617572784423828
Validation loss: 2.4076428644118772

Epoch: 6| Step: 5
Training loss: 3.087101697921753
Validation loss: 2.3819417210035425

Epoch: 6| Step: 6
Training loss: 1.9742146730422974
Validation loss: 2.3787113928025767

Epoch: 6| Step: 7
Training loss: 2.4679667949676514
Validation loss: 2.369304995382986

Epoch: 6| Step: 8
Training loss: 2.825563430786133
Validation loss: 2.370168980731759

Epoch: 6| Step: 9
Training loss: 3.092712879180908
Validation loss: 2.3672903301895305

Epoch: 6| Step: 10
Training loss: 2.124034881591797
Validation loss: 2.3677761657263643

Epoch: 6| Step: 11
Training loss: 2.573810338973999
Validation loss: 2.365197963612054

Epoch: 6| Step: 12
Training loss: 2.9525938034057617
Validation loss: 2.3731000192703737

Epoch: 6| Step: 13
Training loss: 2.4726076126098633
Validation loss: 2.367869359190746

Epoch: 248| Step: 0
Training loss: 2.7468008995056152
Validation loss: 2.3639468351999917

Epoch: 6| Step: 1
Training loss: 2.3200201988220215
Validation loss: 2.3572530156822613

Epoch: 6| Step: 2
Training loss: 3.402644157409668
Validation loss: 2.3631834624915995

Epoch: 6| Step: 3
Training loss: 2.801647901535034
Validation loss: 2.3678549540940153

Epoch: 6| Step: 4
Training loss: 2.7748663425445557
Validation loss: 2.3587227213767266

Epoch: 6| Step: 5
Training loss: 2.098581075668335
Validation loss: 2.36073846842653

Epoch: 6| Step: 6
Training loss: 2.2852466106414795
Validation loss: 2.361745777950492

Epoch: 6| Step: 7
Training loss: 2.4498839378356934
Validation loss: 2.3514212741646716

Epoch: 6| Step: 8
Training loss: 2.6024930477142334
Validation loss: 2.3633793605271207

Epoch: 6| Step: 9
Training loss: 2.6627721786499023
Validation loss: 2.362402880063621

Epoch: 6| Step: 10
Training loss: 3.00570011138916
Validation loss: 2.365882590252866

Epoch: 6| Step: 11
Training loss: 2.3835272789001465
Validation loss: 2.3776011518252793

Epoch: 6| Step: 12
Training loss: 1.8940694332122803
Validation loss: 2.3955195437195482

Epoch: 6| Step: 13
Training loss: 1.8543843030929565
Validation loss: 2.4031780214719873

Epoch: 249| Step: 0
Training loss: 3.099579334259033
Validation loss: 2.398755147892942

Epoch: 6| Step: 1
Training loss: 1.802713394165039
Validation loss: 2.400212328921082

Epoch: 6| Step: 2
Training loss: 1.8589437007904053
Validation loss: 2.4070601437681463

Epoch: 6| Step: 3
Training loss: 2.7761430740356445
Validation loss: 2.4300909606359338

Epoch: 6| Step: 4
Training loss: 3.183159112930298
Validation loss: 2.4245097239812217

Epoch: 6| Step: 5
Training loss: 2.6171774864196777
Validation loss: 2.4041685647861932

Epoch: 6| Step: 6
Training loss: 3.2389307022094727
Validation loss: 2.3947097101519184

Epoch: 6| Step: 7
Training loss: 2.0822155475616455
Validation loss: 2.376772872863277

Epoch: 6| Step: 8
Training loss: 2.3369064331054688
Validation loss: 2.3793217238559516

Epoch: 6| Step: 9
Training loss: 2.734055995941162
Validation loss: 2.3854248190438874

Epoch: 6| Step: 10
Training loss: 2.6498007774353027
Validation loss: 2.3709981518407024

Epoch: 6| Step: 11
Training loss: 1.6735272407531738
Validation loss: 2.375359776199505

Epoch: 6| Step: 12
Training loss: 2.8222222328186035
Validation loss: 2.377415116115283

Epoch: 6| Step: 13
Training loss: 3.0214874744415283
Validation loss: 2.378960756845372

Epoch: 250| Step: 0
Training loss: 2.7225632667541504
Validation loss: 2.376782932589131

Epoch: 6| Step: 1
Training loss: 2.233386754989624
Validation loss: 2.3786177045555523

Epoch: 6| Step: 2
Training loss: 2.367992877960205
Validation loss: 2.3888574338728383

Epoch: 6| Step: 3
Training loss: 2.2591841220855713
Validation loss: 2.39941890393534

Epoch: 6| Step: 4
Training loss: 3.1884069442749023
Validation loss: 2.414865988557057

Epoch: 6| Step: 5
Training loss: 1.6630058288574219
Validation loss: 2.416871199043848

Epoch: 6| Step: 6
Training loss: 2.8734564781188965
Validation loss: 2.4136891954688617

Epoch: 6| Step: 7
Training loss: 2.3402247428894043
Validation loss: 2.4085083417995

Epoch: 6| Step: 8
Training loss: 2.664443254470825
Validation loss: 2.4206434193477837

Epoch: 6| Step: 9
Training loss: 2.701202630996704
Validation loss: 2.3882423165023967

Epoch: 6| Step: 10
Training loss: 2.8489322662353516
Validation loss: 2.378680513751122

Epoch: 6| Step: 11
Training loss: 2.626217842102051
Validation loss: 2.382030156350905

Epoch: 6| Step: 12
Training loss: 2.383298397064209
Validation loss: 2.3649974176960606

Epoch: 6| Step: 13
Training loss: 2.8282644748687744
Validation loss: 2.3642903374087427

Epoch: 251| Step: 0
Training loss: 3.015538215637207
Validation loss: 2.347263733545939

Epoch: 6| Step: 1
Training loss: 1.7939096689224243
Validation loss: 2.351537319921678

Epoch: 6| Step: 2
Training loss: 2.01829195022583
Validation loss: 2.346316309385402

Epoch: 6| Step: 3
Training loss: 3.1143898963928223
Validation loss: 2.3549313391408613

Epoch: 6| Step: 4
Training loss: 2.145270347595215
Validation loss: 2.350108059503699

Epoch: 6| Step: 5
Training loss: 2.420304775238037
Validation loss: 2.348420230291223

Epoch: 6| Step: 6
Training loss: 2.1328423023223877
Validation loss: 2.3588179849809214

Epoch: 6| Step: 7
Training loss: 3.245478630065918
Validation loss: 2.36558965457383

Epoch: 6| Step: 8
Training loss: 2.8025031089782715
Validation loss: 2.365359516553981

Epoch: 6| Step: 9
Training loss: 2.2597928047180176
Validation loss: 2.361220572584419

Epoch: 6| Step: 10
Training loss: 2.999443531036377
Validation loss: 2.366105853870351

Epoch: 6| Step: 11
Training loss: 2.3587141036987305
Validation loss: 2.3674654781177478

Epoch: 6| Step: 12
Training loss: 2.738574981689453
Validation loss: 2.3795057714626355

Epoch: 6| Step: 13
Training loss: 2.3701155185699463
Validation loss: 2.3918085944268013

Epoch: 252| Step: 0
Training loss: 1.7705962657928467
Validation loss: 2.3752114516432568

Epoch: 6| Step: 1
Training loss: 3.7436914443969727
Validation loss: 2.401162803813975

Epoch: 6| Step: 2
Training loss: 3.5172767639160156
Validation loss: 2.3891671011524815

Epoch: 6| Step: 3
Training loss: 2.7260844707489014
Validation loss: 2.3878003653659614

Epoch: 6| Step: 4
Training loss: 2.409243583679199
Validation loss: 2.378514164237566

Epoch: 6| Step: 5
Training loss: 2.616553783416748
Validation loss: 2.3916931101070937

Epoch: 6| Step: 6
Training loss: 2.1477200984954834
Validation loss: 2.3970826595060286

Epoch: 6| Step: 7
Training loss: 1.8299596309661865
Validation loss: 2.388663818759303

Epoch: 6| Step: 8
Training loss: 2.022010087966919
Validation loss: 2.386353664500739

Epoch: 6| Step: 9
Training loss: 2.5564916133880615
Validation loss: 2.374759010089341

Epoch: 6| Step: 10
Training loss: 3.718838691711426
Validation loss: 2.3708688264252036

Epoch: 6| Step: 11
Training loss: 2.9251441955566406
Validation loss: 2.368624689758465

Epoch: 6| Step: 12
Training loss: 1.5855586528778076
Validation loss: 2.3552232609000257

Epoch: 6| Step: 13
Training loss: 1.4354703426361084
Validation loss: 2.3594113575514926

Epoch: 253| Step: 0
Training loss: 2.6300086975097656
Validation loss: 2.361324023174983

Epoch: 6| Step: 1
Training loss: 2.7471261024475098
Validation loss: 2.3710387111991964

Epoch: 6| Step: 2
Training loss: 3.0271077156066895
Validation loss: 2.3539462781721547

Epoch: 6| Step: 3
Training loss: 2.8859801292419434
Validation loss: 2.3639842899896766

Epoch: 6| Step: 4
Training loss: 2.381695032119751
Validation loss: 2.3597274441872873

Epoch: 6| Step: 5
Training loss: 2.357682228088379
Validation loss: 2.3642145215824084

Epoch: 6| Step: 6
Training loss: 2.3684182167053223
Validation loss: 2.348990450623215

Epoch: 6| Step: 7
Training loss: 2.5274176597595215
Validation loss: 2.3544330455923594

Epoch: 6| Step: 8
Training loss: 2.447061538696289
Validation loss: 2.359112657526488

Epoch: 6| Step: 9
Training loss: 2.5132651329040527
Validation loss: 2.373865394182103

Epoch: 6| Step: 10
Training loss: 2.4003114700317383
Validation loss: 2.380710214696905

Epoch: 6| Step: 11
Training loss: 1.3097443580627441
Validation loss: 2.397468366930562

Epoch: 6| Step: 12
Training loss: 2.822770595550537
Validation loss: 2.3951662432762886

Epoch: 6| Step: 13
Training loss: 3.34436297416687
Validation loss: 2.385288110343359

Epoch: 254| Step: 0
Training loss: 3.3642578125
Validation loss: 2.388380060913742

Epoch: 6| Step: 1
Training loss: 1.9948281049728394
Validation loss: 2.3777231093375915

Epoch: 6| Step: 2
Training loss: 3.2033488750457764
Validation loss: 2.38447904586792

Epoch: 6| Step: 3
Training loss: 1.2849617004394531
Validation loss: 2.3799597601736746

Epoch: 6| Step: 4
Training loss: 2.7808938026428223
Validation loss: 2.372311535701957

Epoch: 6| Step: 5
Training loss: 2.6979033946990967
Validation loss: 2.3680781728477887

Epoch: 6| Step: 6
Training loss: 2.4071857929229736
Validation loss: 2.352711900588005

Epoch: 6| Step: 7
Training loss: 2.4483983516693115
Validation loss: 2.3610615935376895

Epoch: 6| Step: 8
Training loss: 2.6407201290130615
Validation loss: 2.348452739818122

Epoch: 6| Step: 9
Training loss: 2.694796562194824
Validation loss: 2.3520006031118412

Epoch: 6| Step: 10
Training loss: 2.278383255004883
Validation loss: 2.355985200533303

Epoch: 6| Step: 11
Training loss: 2.909325361251831
Validation loss: 2.346885606806765

Epoch: 6| Step: 12
Training loss: 2.217074394226074
Validation loss: 2.341891227229949

Epoch: 6| Step: 13
Training loss: 2.4545037746429443
Validation loss: 2.345629438277214

Epoch: 255| Step: 0
Training loss: 2.481346607208252
Validation loss: 2.344516779786797

Epoch: 6| Step: 1
Training loss: 2.6722216606140137
Validation loss: 2.3563365808097263

Epoch: 6| Step: 2
Training loss: 2.8880107402801514
Validation loss: 2.3721754063842115

Epoch: 6| Step: 3
Training loss: 2.4647481441497803
Validation loss: 2.378465670411305

Epoch: 6| Step: 4
Training loss: 3.681556224822998
Validation loss: 2.3943822768426712

Epoch: 6| Step: 5
Training loss: 2.281142234802246
Validation loss: 2.391636566449237

Epoch: 6| Step: 6
Training loss: 2.5072765350341797
Validation loss: 2.399677220211234

Epoch: 6| Step: 7
Training loss: 2.505863666534424
Validation loss: 2.3699917767637517

Epoch: 6| Step: 8
Training loss: 2.5558695793151855
Validation loss: 2.3664198613935903

Epoch: 6| Step: 9
Training loss: 2.6364262104034424
Validation loss: 2.3633432388305664

Epoch: 6| Step: 10
Training loss: 2.322005271911621
Validation loss: 2.3509656331872426

Epoch: 6| Step: 11
Training loss: 2.1924333572387695
Validation loss: 2.341317158873363

Epoch: 6| Step: 12
Training loss: 2.2031590938568115
Validation loss: 2.3457822466409333

Epoch: 6| Step: 13
Training loss: 1.8207323551177979
Validation loss: 2.347763035887031

Epoch: 256| Step: 0
Training loss: 2.066225290298462
Validation loss: 2.350069735639839

Epoch: 6| Step: 1
Training loss: 2.1914427280426025
Validation loss: 2.346379259581207

Epoch: 6| Step: 2
Training loss: 3.1375370025634766
Validation loss: 2.358678497293944

Epoch: 6| Step: 3
Training loss: 2.4960057735443115
Validation loss: 2.3674089600962978

Epoch: 6| Step: 4
Training loss: 1.6639025211334229
Validation loss: 2.366289979668074

Epoch: 6| Step: 5
Training loss: 3.0783023834228516
Validation loss: 2.380148603070167

Epoch: 6| Step: 6
Training loss: 2.54880952835083
Validation loss: 2.37968683755526

Epoch: 6| Step: 7
Training loss: 2.156607151031494
Validation loss: 2.381887585886063

Epoch: 6| Step: 8
Training loss: 2.9660191535949707
Validation loss: 2.3886173130363546

Epoch: 6| Step: 9
Training loss: 3.579418420791626
Validation loss: 2.3828008123623428

Epoch: 6| Step: 10
Training loss: 2.488407611846924
Validation loss: 2.3771609234553512

Epoch: 6| Step: 11
Training loss: 1.896087408065796
Validation loss: 2.395129813942858

Epoch: 6| Step: 12
Training loss: 2.6052122116088867
Validation loss: 2.3741061495196436

Epoch: 6| Step: 13
Training loss: 3.118528127670288
Validation loss: 2.3781249292435183

Epoch: 257| Step: 0
Training loss: 2.4146835803985596
Validation loss: 2.3587405232972998

Epoch: 6| Step: 1
Training loss: 2.998262405395508
Validation loss: 2.351083201746787

Epoch: 6| Step: 2
Training loss: 2.4997575283050537
Validation loss: 2.3391458578007196

Epoch: 6| Step: 3
Training loss: 2.779893636703491
Validation loss: 2.3351431328763246

Epoch: 6| Step: 4
Training loss: 3.2118098735809326
Validation loss: 2.340253427464475

Epoch: 6| Step: 5
Training loss: 1.5580803155899048
Validation loss: 2.3376201198947046

Epoch: 6| Step: 6
Training loss: 2.6392951011657715
Validation loss: 2.3415395931531022

Epoch: 6| Step: 7
Training loss: 2.3381810188293457
Validation loss: 2.332674718672229

Epoch: 6| Step: 8
Training loss: 1.9626166820526123
Validation loss: 2.3454474595285233

Epoch: 6| Step: 9
Training loss: 2.440432071685791
Validation loss: 2.3519614409374934

Epoch: 6| Step: 10
Training loss: 2.6353766918182373
Validation loss: 2.350187850254838

Epoch: 6| Step: 11
Training loss: 3.1669135093688965
Validation loss: 2.3465729580130628

Epoch: 6| Step: 12
Training loss: 2.350785493850708
Validation loss: 2.3599450716408352

Epoch: 6| Step: 13
Training loss: 2.471163749694824
Validation loss: 2.3418192427645446

Epoch: 258| Step: 0
Training loss: 2.7732059955596924
Validation loss: 2.3564898096105105

Epoch: 6| Step: 1
Training loss: 2.4698410034179688
Validation loss: 2.35323081990724

Epoch: 6| Step: 2
Training loss: 3.3532462120056152
Validation loss: 2.3555590516777447

Epoch: 6| Step: 3
Training loss: 3.1014938354492188
Validation loss: 2.349908874880883

Epoch: 6| Step: 4
Training loss: 2.4538698196411133
Validation loss: 2.3697122143160914

Epoch: 6| Step: 5
Training loss: 2.1814723014831543
Validation loss: 2.359016356929656

Epoch: 6| Step: 6
Training loss: 2.394235610961914
Validation loss: 2.35341303835633

Epoch: 6| Step: 7
Training loss: 2.467177629470825
Validation loss: 2.371589647826328

Epoch: 6| Step: 8
Training loss: 2.4273290634155273
Validation loss: 2.3597864386855916

Epoch: 6| Step: 9
Training loss: 2.1598408222198486
Validation loss: 2.3878003679296023

Epoch: 6| Step: 10
Training loss: 2.494441032409668
Validation loss: 2.3826067678390013

Epoch: 6| Step: 11
Training loss: 2.0485222339630127
Validation loss: 2.366213675468199

Epoch: 6| Step: 12
Training loss: 2.294039249420166
Validation loss: 2.3675597278020715

Epoch: 6| Step: 13
Training loss: 2.7085533142089844
Validation loss: 2.3748021561612367

Epoch: 259| Step: 0
Training loss: 1.8241419792175293
Validation loss: 2.360834067867648

Epoch: 6| Step: 1
Training loss: 1.9561362266540527
Validation loss: 2.373108316493291

Epoch: 6| Step: 2
Training loss: 2.2275283336639404
Validation loss: 2.375709077363373

Epoch: 6| Step: 3
Training loss: 3.021510362625122
Validation loss: 2.3691373512309086

Epoch: 6| Step: 4
Training loss: 2.5674142837524414
Validation loss: 2.3667136059012464

Epoch: 6| Step: 5
Training loss: 2.113949775695801
Validation loss: 2.368890454692225

Epoch: 6| Step: 6
Training loss: 2.064908504486084
Validation loss: 2.3765707682537776

Epoch: 6| Step: 7
Training loss: 3.211261749267578
Validation loss: 2.3787041633359847

Epoch: 6| Step: 8
Training loss: 3.569955825805664
Validation loss: 2.363843822991976

Epoch: 6| Step: 9
Training loss: 2.287820339202881
Validation loss: 2.3585085766289824

Epoch: 6| Step: 10
Training loss: 2.514188051223755
Validation loss: 2.363799472008982

Epoch: 6| Step: 11
Training loss: 2.6624603271484375
Validation loss: 2.3589137600314234

Epoch: 6| Step: 12
Training loss: 2.386991024017334
Validation loss: 2.359967782933225

Epoch: 6| Step: 13
Training loss: 3.0141241550445557
Validation loss: 2.373391333446708

Epoch: 260| Step: 0
Training loss: 3.4508678913116455
Validation loss: 2.381085175339894

Epoch: 6| Step: 1
Training loss: 2.0213615894317627
Validation loss: 2.402057304177233

Epoch: 6| Step: 2
Training loss: 2.906661033630371
Validation loss: 2.3900060217867614

Epoch: 6| Step: 3
Training loss: 2.3363356590270996
Validation loss: 2.390375029656195

Epoch: 6| Step: 4
Training loss: 1.7231330871582031
Validation loss: 2.4075296976233043

Epoch: 6| Step: 5
Training loss: 2.0280964374542236
Validation loss: 2.391676310570009

Epoch: 6| Step: 6
Training loss: 2.3386356830596924
Validation loss: 2.403278071393249

Epoch: 6| Step: 7
Training loss: 2.464175224304199
Validation loss: 2.395060170081354

Epoch: 6| Step: 8
Training loss: 2.098062515258789
Validation loss: 2.3822074731191

Epoch: 6| Step: 9
Training loss: 2.445312976837158
Validation loss: 2.369096158653177

Epoch: 6| Step: 10
Training loss: 2.5138885974884033
Validation loss: 2.358273729201286

Epoch: 6| Step: 11
Training loss: 3.2351837158203125
Validation loss: 2.346444691381147

Epoch: 6| Step: 12
Training loss: 2.5944409370422363
Validation loss: 2.3447453591131393

Epoch: 6| Step: 13
Training loss: 3.4276602268218994
Validation loss: 2.3427483317672566

Epoch: 261| Step: 0
Training loss: 2.662797451019287
Validation loss: 2.3350292482683734

Epoch: 6| Step: 1
Training loss: 2.645401954650879
Validation loss: 2.334470446391772

Epoch: 6| Step: 2
Training loss: 2.5475916862487793
Validation loss: 2.3309231317171486

Epoch: 6| Step: 3
Training loss: 2.501638412475586
Validation loss: 2.3281674051797516

Epoch: 6| Step: 4
Training loss: 3.0914387702941895
Validation loss: 2.3435802613535235

Epoch: 6| Step: 5
Training loss: 1.4823360443115234
Validation loss: 2.3480737234956477

Epoch: 6| Step: 6
Training loss: 2.3932836055755615
Validation loss: 2.3474009857382825

Epoch: 6| Step: 7
Training loss: 3.1735153198242188
Validation loss: 2.364622905690183

Epoch: 6| Step: 8
Training loss: 2.5959465503692627
Validation loss: 2.356923882679273

Epoch: 6| Step: 9
Training loss: 2.4446864128112793
Validation loss: 2.3467009195717434

Epoch: 6| Step: 10
Training loss: 2.036910057067871
Validation loss: 2.3506344928536365

Epoch: 6| Step: 11
Training loss: 2.447005271911621
Validation loss: 2.3418716435791342

Epoch: 6| Step: 12
Training loss: 2.9459824562072754
Validation loss: 2.364865226130332

Epoch: 6| Step: 13
Training loss: 2.00760555267334
Validation loss: 2.3596232296318136

Epoch: 262| Step: 0
Training loss: 2.2326550483703613
Validation loss: 2.3632676729591946

Epoch: 6| Step: 1
Training loss: 3.2723493576049805
Validation loss: 2.349106752744285

Epoch: 6| Step: 2
Training loss: 3.1456820964813232
Validation loss: 2.3711476928444317

Epoch: 6| Step: 3
Training loss: 2.910399913787842
Validation loss: 2.359988184385402

Epoch: 6| Step: 4
Training loss: 2.389824151992798
Validation loss: 2.364301586663851

Epoch: 6| Step: 5
Training loss: 1.9899234771728516
Validation loss: 2.344316146707022

Epoch: 6| Step: 6
Training loss: 1.791200876235962
Validation loss: 2.357561062741023

Epoch: 6| Step: 7
Training loss: 2.6116843223571777
Validation loss: 2.331784511125216

Epoch: 6| Step: 8
Training loss: 2.2879815101623535
Validation loss: 2.3260196280735794

Epoch: 6| Step: 9
Training loss: 2.407376766204834
Validation loss: 2.3338190099244476

Epoch: 6| Step: 10
Training loss: 3.7422447204589844
Validation loss: 2.334488166275845

Epoch: 6| Step: 11
Training loss: 1.4811208248138428
Validation loss: 2.3377882844658306

Epoch: 6| Step: 12
Training loss: 2.441709518432617
Validation loss: 2.3413554929917857

Epoch: 6| Step: 13
Training loss: 2.571772575378418
Validation loss: 2.3461821207436184

Epoch: 263| Step: 0
Training loss: 3.1089179515838623
Validation loss: 2.3533547206591536

Epoch: 6| Step: 1
Training loss: 2.2285070419311523
Validation loss: 2.3550953993233303

Epoch: 6| Step: 2
Training loss: 2.7534098625183105
Validation loss: 2.3562611738840737

Epoch: 6| Step: 3
Training loss: 2.663341522216797
Validation loss: 2.3830946722338275

Epoch: 6| Step: 4
Training loss: 1.943379521369934
Validation loss: 2.379807064610143

Epoch: 6| Step: 5
Training loss: 1.7900118827819824
Validation loss: 2.384226947702387

Epoch: 6| Step: 6
Training loss: 2.4536666870117188
Validation loss: 2.4061302395277124

Epoch: 6| Step: 7
Training loss: 3.0757579803466797
Validation loss: 2.4254526053705523

Epoch: 6| Step: 8
Training loss: 3.2971348762512207
Validation loss: 2.4491644367094962

Epoch: 6| Step: 9
Training loss: 2.49019193649292
Validation loss: 2.449999314482494

Epoch: 6| Step: 10
Training loss: 2.063053607940674
Validation loss: 2.398098450835033

Epoch: 6| Step: 11
Training loss: 3.7122750282287598
Validation loss: 2.4009116029226654

Epoch: 6| Step: 12
Training loss: 1.6986639499664307
Validation loss: 2.3769369086911603

Epoch: 6| Step: 13
Training loss: 2.1816508769989014
Validation loss: 2.364962024073447

Epoch: 264| Step: 0
Training loss: 1.8765684366226196
Validation loss: 2.3472415631817234

Epoch: 6| Step: 1
Training loss: 1.9804142713546753
Validation loss: 2.344756769877608

Epoch: 6| Step: 2
Training loss: 2.580904006958008
Validation loss: 2.3433457420718287

Epoch: 6| Step: 3
Training loss: 2.9490418434143066
Validation loss: 2.3175408250542096

Epoch: 6| Step: 4
Training loss: 2.4215385913848877
Validation loss: 2.327084696421059

Epoch: 6| Step: 5
Training loss: 2.4476797580718994
Validation loss: 2.3164632525495303

Epoch: 6| Step: 6
Training loss: 2.637444019317627
Validation loss: 2.3231916658339964

Epoch: 6| Step: 7
Training loss: 2.373884677886963
Validation loss: 2.3212896700828307

Epoch: 6| Step: 8
Training loss: 2.274129867553711
Validation loss: 2.321404093055315

Epoch: 6| Step: 9
Training loss: 2.5248024463653564
Validation loss: 2.3275351998626546

Epoch: 6| Step: 10
Training loss: 2.644160747528076
Validation loss: 2.331590816538821

Epoch: 6| Step: 11
Training loss: 2.651233673095703
Validation loss: 2.3329807020002797

Epoch: 6| Step: 12
Training loss: 2.759258270263672
Validation loss: 2.332625012243948

Epoch: 6| Step: 13
Training loss: 3.808825969696045
Validation loss: 2.3479277754342682

Epoch: 265| Step: 0
Training loss: 2.7668581008911133
Validation loss: 2.3583898595584336

Epoch: 6| Step: 1
Training loss: 2.6705286502838135
Validation loss: 2.3488956087379047

Epoch: 6| Step: 2
Training loss: 1.5145117044448853
Validation loss: 2.348155197276864

Epoch: 6| Step: 3
Training loss: 2.49153995513916
Validation loss: 2.3463424303198375

Epoch: 6| Step: 4
Training loss: 3.102281332015991
Validation loss: 2.3418470890291276

Epoch: 6| Step: 5
Training loss: 1.8197760581970215
Validation loss: 2.346568356278122

Epoch: 6| Step: 6
Training loss: 2.3555779457092285
Validation loss: 2.3473730702554025

Epoch: 6| Step: 7
Training loss: 3.2125866413116455
Validation loss: 2.3390660593586583

Epoch: 6| Step: 8
Training loss: 2.7559146881103516
Validation loss: 2.3322136607221378

Epoch: 6| Step: 9
Training loss: 2.7772512435913086
Validation loss: 2.343628878234535

Epoch: 6| Step: 10
Training loss: 2.5890958309173584
Validation loss: 2.3430383000322568

Epoch: 6| Step: 11
Training loss: 2.209500789642334
Validation loss: 2.3411805014456473

Epoch: 6| Step: 12
Training loss: 2.1132004261016846
Validation loss: 2.3600209836036927

Epoch: 6| Step: 13
Training loss: 2.4840304851531982
Validation loss: 2.3650282121473745

Epoch: 266| Step: 0
Training loss: 2.1252989768981934
Validation loss: 2.3997178077697754

Epoch: 6| Step: 1
Training loss: 2.715048313140869
Validation loss: 2.4254345509313766

Epoch: 6| Step: 2
Training loss: 2.252467632293701
Validation loss: 2.459706296202957

Epoch: 6| Step: 3
Training loss: 2.4788100719451904
Validation loss: 2.467954651001961

Epoch: 6| Step: 4
Training loss: 2.5105643272399902
Validation loss: 2.4522486168851136

Epoch: 6| Step: 5
Training loss: 2.4264254570007324
Validation loss: 2.445725438415363

Epoch: 6| Step: 6
Training loss: 2.287290573120117
Validation loss: 2.444842112961636

Epoch: 6| Step: 7
Training loss: 2.635280132293701
Validation loss: 2.4272637726158224

Epoch: 6| Step: 8
Training loss: 2.5266830921173096
Validation loss: 2.4351026550416024

Epoch: 6| Step: 9
Training loss: 2.86857533454895
Validation loss: 2.420689744334067

Epoch: 6| Step: 10
Training loss: 2.983320713043213
Validation loss: 2.4066299571785876

Epoch: 6| Step: 11
Training loss: 2.668600559234619
Validation loss: 2.3822372113504717

Epoch: 6| Step: 12
Training loss: 2.522244930267334
Validation loss: 2.367593837040727

Epoch: 6| Step: 13
Training loss: 2.3949644565582275
Validation loss: 2.355248446105629

Epoch: 267| Step: 0
Training loss: 2.606339931488037
Validation loss: 2.3656084101687194

Epoch: 6| Step: 1
Training loss: 3.0923686027526855
Validation loss: 2.34989696164285

Epoch: 6| Step: 2
Training loss: 1.5079684257507324
Validation loss: 2.3478659096584527

Epoch: 6| Step: 3
Training loss: 3.5219621658325195
Validation loss: 2.332191813376642

Epoch: 6| Step: 4
Training loss: 2.5840559005737305
Validation loss: 2.3334987496816986

Epoch: 6| Step: 5
Training loss: 3.0312342643737793
Validation loss: 2.3296368429737706

Epoch: 6| Step: 6
Training loss: 2.4188902378082275
Validation loss: 2.323328653971354

Epoch: 6| Step: 7
Training loss: 2.379249095916748
Validation loss: 2.3232517575704925

Epoch: 6| Step: 8
Training loss: 2.918308734893799
Validation loss: 2.318945946231965

Epoch: 6| Step: 9
Training loss: 1.9882023334503174
Validation loss: 2.3246777467830206

Epoch: 6| Step: 10
Training loss: 2.516775131225586
Validation loss: 2.3203274998613583

Epoch: 6| Step: 11
Training loss: 2.2783703804016113
Validation loss: 2.315548360988658

Epoch: 6| Step: 12
Training loss: 2.398367166519165
Validation loss: 2.323881995293402

Epoch: 6| Step: 13
Training loss: 1.7727723121643066
Validation loss: 2.3294104350510465

Epoch: 268| Step: 0
Training loss: 1.64288330078125
Validation loss: 2.3415687109834407

Epoch: 6| Step: 1
Training loss: 2.7726783752441406
Validation loss: 2.3499339113953295

Epoch: 6| Step: 2
Training loss: 2.2571468353271484
Validation loss: 2.36745540044641

Epoch: 6| Step: 3
Training loss: 3.152539014816284
Validation loss: 2.3836775620778403

Epoch: 6| Step: 4
Training loss: 2.2124760150909424
Validation loss: 2.379490752373972

Epoch: 6| Step: 5
Training loss: 2.882233142852783
Validation loss: 2.3766821725394136

Epoch: 6| Step: 6
Training loss: 2.1776254177093506
Validation loss: 2.3891934015417613

Epoch: 6| Step: 7
Training loss: 2.95214581489563
Validation loss: 2.3860170033670243

Epoch: 6| Step: 8
Training loss: 2.5754995346069336
Validation loss: 2.3614609074848953

Epoch: 6| Step: 9
Training loss: 2.262281894683838
Validation loss: 2.3570380774877404

Epoch: 6| Step: 10
Training loss: 3.2207143306732178
Validation loss: 2.356959391665715

Epoch: 6| Step: 11
Training loss: 2.1021344661712646
Validation loss: 2.3508989503306728

Epoch: 6| Step: 12
Training loss: 2.5099613666534424
Validation loss: 2.341953459606376

Epoch: 6| Step: 13
Training loss: 2.3974874019622803
Validation loss: 2.3521678191359325

Epoch: 269| Step: 0
Training loss: 1.9474966526031494
Validation loss: 2.3380151307711037

Epoch: 6| Step: 1
Training loss: 2.73979115486145
Validation loss: 2.3359554211298623

Epoch: 6| Step: 2
Training loss: 2.334645986557007
Validation loss: 2.3277833231033815

Epoch: 6| Step: 3
Training loss: 2.374128580093384
Validation loss: 2.3449158694154475

Epoch: 6| Step: 4
Training loss: 1.8858168125152588
Validation loss: 2.3435600778108

Epoch: 6| Step: 5
Training loss: 2.7957496643066406
Validation loss: 2.32133747172612

Epoch: 6| Step: 6
Training loss: 2.9756035804748535
Validation loss: 2.3407645302434124

Epoch: 6| Step: 7
Training loss: 2.722695827484131
Validation loss: 2.3394238154093423

Epoch: 6| Step: 8
Training loss: 3.2954025268554688
Validation loss: 2.338880326158257

Epoch: 6| Step: 9
Training loss: 1.7176076173782349
Validation loss: 2.3559987698831866

Epoch: 6| Step: 10
Training loss: 2.4822800159454346
Validation loss: 2.3561904840571906

Epoch: 6| Step: 11
Training loss: 3.1654715538024902
Validation loss: 2.346893900184221

Epoch: 6| Step: 12
Training loss: 1.8101227283477783
Validation loss: 2.3504080336580992

Epoch: 6| Step: 13
Training loss: 2.967804431915283
Validation loss: 2.358617872320196

Epoch: 270| Step: 0
Training loss: 3.2260398864746094
Validation loss: 2.357729788749449

Epoch: 6| Step: 1
Training loss: 3.0550174713134766
Validation loss: 2.3329629616070817

Epoch: 6| Step: 2
Training loss: 2.4978044033050537
Validation loss: 2.345146902145878

Epoch: 6| Step: 3
Training loss: 2.5800232887268066
Validation loss: 2.3344372831365114

Epoch: 6| Step: 4
Training loss: 2.58423113822937
Validation loss: 2.3304811472533853

Epoch: 6| Step: 5
Training loss: 2.3944251537323
Validation loss: 2.3472734369257444

Epoch: 6| Step: 6
Training loss: 2.4792258739471436
Validation loss: 2.349106882208137

Epoch: 6| Step: 7
Training loss: 2.7065844535827637
Validation loss: 2.3556443183652815

Epoch: 6| Step: 8
Training loss: 2.3736562728881836
Validation loss: 2.3555195639210362

Epoch: 6| Step: 9
Training loss: 2.088897466659546
Validation loss: 2.364227852513713

Epoch: 6| Step: 10
Training loss: 2.344947338104248
Validation loss: 2.3662319439713673

Epoch: 6| Step: 11
Training loss: 2.3386712074279785
Validation loss: 2.374681818869806

Epoch: 6| Step: 12
Training loss: 2.853269100189209
Validation loss: 2.3894422361927647

Epoch: 6| Step: 13
Training loss: 0.8647536039352417
Validation loss: 2.3828521108114593

Epoch: 271| Step: 0
Training loss: 2.492753505706787
Validation loss: 2.3573383874790643

Epoch: 6| Step: 1
Training loss: 2.6812424659729004
Validation loss: 2.3633227579055296

Epoch: 6| Step: 2
Training loss: 1.6309716701507568
Validation loss: 2.3977283611092517

Epoch: 6| Step: 3
Training loss: 3.2723910808563232
Validation loss: 2.3791643419573383

Epoch: 6| Step: 4
Training loss: 2.8478641510009766
Validation loss: 2.375121439656904

Epoch: 6| Step: 5
Training loss: 2.899718761444092
Validation loss: 2.3542471008916057

Epoch: 6| Step: 6
Training loss: 2.5467264652252197
Validation loss: 2.3344398185771

Epoch: 6| Step: 7
Training loss: 2.2818448543548584
Validation loss: 2.3282842354107927

Epoch: 6| Step: 8
Training loss: 2.8393683433532715
Validation loss: 2.319993016540363

Epoch: 6| Step: 9
Training loss: 2.0513052940368652
Validation loss: 2.3027240948010514

Epoch: 6| Step: 10
Training loss: 2.519275426864624
Validation loss: 2.306259027091406

Epoch: 6| Step: 11
Training loss: 2.418771743774414
Validation loss: 2.305072961315032

Epoch: 6| Step: 12
Training loss: 2.863572120666504
Validation loss: 2.3093781112342753

Epoch: 6| Step: 13
Training loss: 1.483461856842041
Validation loss: 2.3095253052250033

Epoch: 272| Step: 0
Training loss: 2.4185774326324463
Validation loss: 2.314004703234601

Epoch: 6| Step: 1
Training loss: 2.363222122192383
Validation loss: 2.3094968218957224

Epoch: 6| Step: 2
Training loss: 2.534881114959717
Validation loss: 2.3243375426979473

Epoch: 6| Step: 3
Training loss: 2.7658298015594482
Validation loss: 2.3218660457159883

Epoch: 6| Step: 4
Training loss: 3.4632272720336914
Validation loss: 2.3578489134388585

Epoch: 6| Step: 5
Training loss: 2.1592330932617188
Validation loss: 2.360460045517132

Epoch: 6| Step: 6
Training loss: 2.683465003967285
Validation loss: 2.3833748563643424

Epoch: 6| Step: 7
Training loss: 2.564384937286377
Validation loss: 2.396631166499148

Epoch: 6| Step: 8
Training loss: 2.601461887359619
Validation loss: 2.411655556771063

Epoch: 6| Step: 9
Training loss: 1.9402897357940674
Validation loss: 2.3978069392583703

Epoch: 6| Step: 10
Training loss: 2.201138973236084
Validation loss: 2.377212384695648

Epoch: 6| Step: 11
Training loss: 2.62825345993042
Validation loss: 2.3791820362050045

Epoch: 6| Step: 12
Training loss: 2.3216261863708496
Validation loss: 2.3635047866452124

Epoch: 6| Step: 13
Training loss: 2.2362072467803955
Validation loss: 2.340185817851815

Epoch: 273| Step: 0
Training loss: 2.124768018722534
Validation loss: 2.3320923543745473

Epoch: 6| Step: 1
Training loss: 3.3307502269744873
Validation loss: 2.32479360283062

Epoch: 6| Step: 2
Training loss: 2.587881326675415
Validation loss: 2.3233021561817457

Epoch: 6| Step: 3
Training loss: 3.2213754653930664
Validation loss: 2.317575370111773

Epoch: 6| Step: 4
Training loss: 2.694228172302246
Validation loss: 2.3123304433720087

Epoch: 6| Step: 5
Training loss: 2.6834616661071777
Validation loss: 2.3017457864617787

Epoch: 6| Step: 6
Training loss: 2.2012696266174316
Validation loss: 2.2933392909265335

Epoch: 6| Step: 7
Training loss: 2.0617761611938477
Validation loss: 2.3081595795128935

Epoch: 6| Step: 8
Training loss: 2.976529598236084
Validation loss: 2.301405001712102

Epoch: 6| Step: 9
Training loss: 1.5831809043884277
Validation loss: 2.3009399367916967

Epoch: 6| Step: 10
Training loss: 2.004733085632324
Validation loss: 2.3051517112280733

Epoch: 6| Step: 11
Training loss: 2.6316967010498047
Validation loss: 2.3209185805372012

Epoch: 6| Step: 12
Training loss: 2.2520346641540527
Validation loss: 2.315302566815448

Epoch: 6| Step: 13
Training loss: 3.063985586166382
Validation loss: 2.320792467363419

Epoch: 274| Step: 0
Training loss: 2.4354000091552734
Validation loss: 2.330730240832093

Epoch: 6| Step: 1
Training loss: 2.3092219829559326
Validation loss: 2.3232969468639744

Epoch: 6| Step: 2
Training loss: 3.052316665649414
Validation loss: 2.3392083439775693

Epoch: 6| Step: 3
Training loss: 2.607177734375
Validation loss: 2.3401403478396836

Epoch: 6| Step: 4
Training loss: 2.1291565895080566
Validation loss: 2.336078566889609

Epoch: 6| Step: 5
Training loss: 2.5296709537506104
Validation loss: 2.3369084250542427

Epoch: 6| Step: 6
Training loss: 2.2224297523498535
Validation loss: 2.344627054788733

Epoch: 6| Step: 7
Training loss: 2.5855610370635986
Validation loss: 2.34864503722037

Epoch: 6| Step: 8
Training loss: 2.3915491104125977
Validation loss: 2.3498045731616277

Epoch: 6| Step: 9
Training loss: 2.2684552669525146
Validation loss: 2.3575444772679317

Epoch: 6| Step: 10
Training loss: 2.3830151557922363
Validation loss: 2.3613736193667174

Epoch: 6| Step: 11
Training loss: 2.4574813842773438
Validation loss: 2.3629045281358945

Epoch: 6| Step: 12
Training loss: 2.994619846343994
Validation loss: 2.3948908928901917

Epoch: 6| Step: 13
Training loss: 2.6590471267700195
Validation loss: 2.3721053061946744

Epoch: 275| Step: 0
Training loss: 3.1388158798217773
Validation loss: 2.352702832991077

Epoch: 6| Step: 1
Training loss: 2.2634129524230957
Validation loss: 2.3164596275616716

Epoch: 6| Step: 2
Training loss: 2.5337648391723633
Validation loss: 2.3283235539672194

Epoch: 6| Step: 3
Training loss: 2.45674991607666
Validation loss: 2.309520242034748

Epoch: 6| Step: 4
Training loss: 2.7936153411865234
Validation loss: 2.3155942578469553

Epoch: 6| Step: 5
Training loss: 2.222015380859375
Validation loss: 2.3244731016056512

Epoch: 6| Step: 6
Training loss: 2.309554100036621
Validation loss: 2.3109689143396195

Epoch: 6| Step: 7
Training loss: 2.5144057273864746
Validation loss: 2.3033550913615892

Epoch: 6| Step: 8
Training loss: 2.8845720291137695
Validation loss: 2.3079328331896054

Epoch: 6| Step: 9
Training loss: 1.555854320526123
Validation loss: 2.3129637625909623

Epoch: 6| Step: 10
Training loss: 2.134361743927002
Validation loss: 2.312359720148066

Epoch: 6| Step: 11
Training loss: 2.5395283699035645
Validation loss: 2.327260673687022

Epoch: 6| Step: 12
Training loss: 3.1783056259155273
Validation loss: 2.3186675143498245

Epoch: 6| Step: 13
Training loss: 2.1749396324157715
Validation loss: 2.3221318337225143

Epoch: 276| Step: 0
Training loss: 2.291635036468506
Validation loss: 2.3463119947782127

Epoch: 6| Step: 1
Training loss: 1.9041954278945923
Validation loss: 2.3564070809272026

Epoch: 6| Step: 2
Training loss: 2.013293743133545
Validation loss: 2.3730954841900895

Epoch: 6| Step: 3
Training loss: 2.1878304481506348
Validation loss: 2.3755983767970914

Epoch: 6| Step: 4
Training loss: 3.275228500366211
Validation loss: 2.3932965263243644

Epoch: 6| Step: 5
Training loss: 2.674288749694824
Validation loss: 2.4029248606774116

Epoch: 6| Step: 6
Training loss: 2.4152932167053223
Validation loss: 2.3771303315316477

Epoch: 6| Step: 7
Training loss: 2.7126898765563965
Validation loss: 2.3780532652331936

Epoch: 6| Step: 8
Training loss: 1.668424367904663
Validation loss: 2.3602171174941526

Epoch: 6| Step: 9
Training loss: 3.7403225898742676
Validation loss: 2.340305397587438

Epoch: 6| Step: 10
Training loss: 2.7191178798675537
Validation loss: 2.3222204049428306

Epoch: 6| Step: 11
Training loss: 2.535447835922241
Validation loss: 2.3277937109752367

Epoch: 6| Step: 12
Training loss: 2.112165927886963
Validation loss: 2.3238610939313005

Epoch: 6| Step: 13
Training loss: 3.028042793273926
Validation loss: 2.321784832144296

Epoch: 277| Step: 0
Training loss: 2.4909791946411133
Validation loss: 2.3157086974831036

Epoch: 6| Step: 1
Training loss: 2.359907388687134
Validation loss: 2.2953324061568066

Epoch: 6| Step: 2
Training loss: 2.417741537094116
Validation loss: 2.304262617582916

Epoch: 6| Step: 3
Training loss: 1.9813569784164429
Validation loss: 2.2957282502164125

Epoch: 6| Step: 4
Training loss: 2.9128823280334473
Validation loss: 2.298607240441025

Epoch: 6| Step: 5
Training loss: 1.8661184310913086
Validation loss: 2.30102688266385

Epoch: 6| Step: 6
Training loss: 2.5744757652282715
Validation loss: 2.3049141335231003

Epoch: 6| Step: 7
Training loss: 2.5234909057617188
Validation loss: 2.3134827178011657

Epoch: 6| Step: 8
Training loss: 3.1836085319519043
Validation loss: 2.328212190699834

Epoch: 6| Step: 9
Training loss: 1.9980406761169434
Validation loss: 2.3416934731186076

Epoch: 6| Step: 10
Training loss: 2.3749749660491943
Validation loss: 2.3447861222810644

Epoch: 6| Step: 11
Training loss: 3.569626808166504
Validation loss: 2.3590846882071546

Epoch: 6| Step: 12
Training loss: 2.43019700050354
Validation loss: 2.3665389681375153

Epoch: 6| Step: 13
Training loss: 2.115572929382324
Validation loss: 2.3524529369928504

Epoch: 278| Step: 0
Training loss: 2.4196343421936035
Validation loss: 2.3587704191925707

Epoch: 6| Step: 1
Training loss: 2.2922589778900146
Validation loss: 2.3331374250432497

Epoch: 6| Step: 2
Training loss: 2.637017011642456
Validation loss: 2.334892862586565

Epoch: 6| Step: 3
Training loss: 2.699037790298462
Validation loss: 2.328965287054739

Epoch: 6| Step: 4
Training loss: 2.4346323013305664
Validation loss: 2.3408644507008214

Epoch: 6| Step: 5
Training loss: 2.398369789123535
Validation loss: 2.3569210370381675

Epoch: 6| Step: 6
Training loss: 1.5449345111846924
Validation loss: 2.3497118872980916

Epoch: 6| Step: 7
Training loss: 2.429448366165161
Validation loss: 2.351513580609393

Epoch: 6| Step: 8
Training loss: 2.322035789489746
Validation loss: 2.3585956404286046

Epoch: 6| Step: 9
Training loss: 3.44832444190979
Validation loss: 2.332322036066363

Epoch: 6| Step: 10
Training loss: 2.715365409851074
Validation loss: 2.3551185284891436

Epoch: 6| Step: 11
Training loss: 2.8352479934692383
Validation loss: 2.346341179263207

Epoch: 6| Step: 12
Training loss: 2.589271068572998
Validation loss: 2.362204800369919

Epoch: 6| Step: 13
Training loss: 1.5126123428344727
Validation loss: 2.3504148285876036

Epoch: 279| Step: 0
Training loss: 1.7642357349395752
Validation loss: 2.346731580713744

Epoch: 6| Step: 1
Training loss: 2.3523006439208984
Validation loss: 2.342038910876038

Epoch: 6| Step: 2
Training loss: 2.639207363128662
Validation loss: 2.3476354114470945

Epoch: 6| Step: 3
Training loss: 1.999252438545227
Validation loss: 2.320078598555698

Epoch: 6| Step: 4
Training loss: 2.7487986087799072
Validation loss: 2.3346367779598443

Epoch: 6| Step: 5
Training loss: 2.973432779312134
Validation loss: 2.310178336276803

Epoch: 6| Step: 6
Training loss: 2.6778740882873535
Validation loss: 2.3283856427797707

Epoch: 6| Step: 7
Training loss: 2.9254636764526367
Validation loss: 2.333536955618089

Epoch: 6| Step: 8
Training loss: 1.4039783477783203
Validation loss: 2.315606673558553

Epoch: 6| Step: 9
Training loss: 3.0863425731658936
Validation loss: 2.316725605277605

Epoch: 6| Step: 10
Training loss: 3.061659812927246
Validation loss: 2.3163967850387737

Epoch: 6| Step: 11
Training loss: 2.71744441986084
Validation loss: 2.300586054402013

Epoch: 6| Step: 12
Training loss: 2.608548164367676
Validation loss: 2.31715549448485

Epoch: 6| Step: 13
Training loss: 1.1372909545898438
Validation loss: 2.3135092155907744

Epoch: 280| Step: 0
Training loss: 2.6137237548828125
Validation loss: 2.335785637619675

Epoch: 6| Step: 1
Training loss: 2.3512606620788574
Validation loss: 2.3352164760712655

Epoch: 6| Step: 2
Training loss: 2.898317575454712
Validation loss: 2.333323246689253

Epoch: 6| Step: 3
Training loss: 2.2740018367767334
Validation loss: 2.3331205101423365

Epoch: 6| Step: 4
Training loss: 1.8714369535446167
Validation loss: 2.337792645218552

Epoch: 6| Step: 5
Training loss: 1.6337776184082031
Validation loss: 2.3393782184969996

Epoch: 6| Step: 6
Training loss: 3.323507785797119
Validation loss: 2.33044010080317

Epoch: 6| Step: 7
Training loss: 2.007026195526123
Validation loss: 2.3115418828943723

Epoch: 6| Step: 8
Training loss: 3.127742290496826
Validation loss: 2.3213911133427776

Epoch: 6| Step: 9
Training loss: 2.2370777130126953
Validation loss: 2.3208899010894117

Epoch: 6| Step: 10
Training loss: 3.2758913040161133
Validation loss: 2.325896716886951

Epoch: 6| Step: 11
Training loss: 2.282886028289795
Validation loss: 2.34073938990152

Epoch: 6| Step: 12
Training loss: 2.6970090866088867
Validation loss: 2.335925284252372

Epoch: 6| Step: 13
Training loss: 1.6522152423858643
Validation loss: 2.3485074402183614

Epoch: 281| Step: 0
Training loss: 2.9418065547943115
Validation loss: 2.363046071862662

Epoch: 6| Step: 1
Training loss: 2.4249701499938965
Validation loss: 2.3631474587225143

Epoch: 6| Step: 2
Training loss: 1.8467321395874023
Validation loss: 2.379145094143447

Epoch: 6| Step: 3
Training loss: 2.8589231967926025
Validation loss: 2.364472896822037

Epoch: 6| Step: 4
Training loss: 2.4415273666381836
Validation loss: 2.3909490159762803

Epoch: 6| Step: 5
Training loss: 2.2986364364624023
Validation loss: 2.3857696235820813

Epoch: 6| Step: 6
Training loss: 2.803635835647583
Validation loss: 2.3770579855929137

Epoch: 6| Step: 7
Training loss: 1.6047054529190063
Validation loss: 2.3403010111983105

Epoch: 6| Step: 8
Training loss: 2.548830986022949
Validation loss: 2.30967592423962

Epoch: 6| Step: 9
Training loss: 2.1770143508911133
Validation loss: 2.3038842062796316

Epoch: 6| Step: 10
Training loss: 2.7440123558044434
Validation loss: 2.299780244468361

Epoch: 6| Step: 11
Training loss: 3.1947803497314453
Validation loss: 2.307220394893359

Epoch: 6| Step: 12
Training loss: 2.5921883583068848
Validation loss: 2.32487960784666

Epoch: 6| Step: 13
Training loss: 2.484938621520996
Validation loss: 2.3325656819087204

Epoch: 282| Step: 0
Training loss: 2.5275094509124756
Validation loss: 2.335378841687274

Epoch: 6| Step: 1
Training loss: 2.0153603553771973
Validation loss: 2.3365515406413744

Epoch: 6| Step: 2
Training loss: 2.6376965045928955
Validation loss: 2.34328551958966

Epoch: 6| Step: 3
Training loss: 1.6908906698226929
Validation loss: 2.3460737787267214

Epoch: 6| Step: 4
Training loss: 3.2793993949890137
Validation loss: 2.3221887144991147

Epoch: 6| Step: 5
Training loss: 3.1986474990844727
Validation loss: 2.31214936061572

Epoch: 6| Step: 6
Training loss: 2.2876906394958496
Validation loss: 2.2982231442646315

Epoch: 6| Step: 7
Training loss: 1.9130158424377441
Validation loss: 2.2931117524382887

Epoch: 6| Step: 8
Training loss: 2.0447375774383545
Validation loss: 2.2919256994801183

Epoch: 6| Step: 9
Training loss: 2.9933321475982666
Validation loss: 2.2977551183392926

Epoch: 6| Step: 10
Training loss: 2.2354001998901367
Validation loss: 2.2976557311191352

Epoch: 6| Step: 11
Training loss: 2.5334599018096924
Validation loss: 2.308831822487616

Epoch: 6| Step: 12
Training loss: 3.021472454071045
Validation loss: 2.327353767169419

Epoch: 6| Step: 13
Training loss: 2.865276336669922
Validation loss: 2.333337696649695

Epoch: 283| Step: 0
Training loss: 2.569582939147949
Validation loss: 2.3417174098312215

Epoch: 6| Step: 1
Training loss: 2.6109490394592285
Validation loss: 2.3556395987028718

Epoch: 6| Step: 2
Training loss: 3.0330774784088135
Validation loss: 2.3528266799065376

Epoch: 6| Step: 3
Training loss: 2.3030171394348145
Validation loss: 2.345171933533043

Epoch: 6| Step: 4
Training loss: 2.381916046142578
Validation loss: 2.34854031121859

Epoch: 6| Step: 5
Training loss: 2.51625919342041
Validation loss: 2.3565519702049995

Epoch: 6| Step: 6
Training loss: 3.0002520084381104
Validation loss: 2.32893116499788

Epoch: 6| Step: 7
Training loss: 2.608222484588623
Validation loss: 2.338975175734489

Epoch: 6| Step: 8
Training loss: 2.6752612590789795
Validation loss: 2.3565877124827397

Epoch: 6| Step: 9
Training loss: 2.825615882873535
Validation loss: 2.3385758220508532

Epoch: 6| Step: 10
Training loss: 2.3025949001312256
Validation loss: 2.31647373912155

Epoch: 6| Step: 11
Training loss: 1.9234415292739868
Validation loss: 2.3052355115131666

Epoch: 6| Step: 12
Training loss: 2.1540427207946777
Validation loss: 2.300815789930282

Epoch: 6| Step: 13
Training loss: 1.5887033939361572
Validation loss: 2.3087067142609627

Epoch: 284| Step: 0
Training loss: 2.4630157947540283
Validation loss: 2.3008482302388837

Epoch: 6| Step: 1
Training loss: 2.889540910720825
Validation loss: 2.296734502238612

Epoch: 6| Step: 2
Training loss: 1.8912944793701172
Validation loss: 2.291340938178442

Epoch: 6| Step: 3
Training loss: 2.602447986602783
Validation loss: 2.288492069449476

Epoch: 6| Step: 4
Training loss: 2.6464169025421143
Validation loss: 2.282486161878032

Epoch: 6| Step: 5
Training loss: 3.0765018463134766
Validation loss: 2.2895745308168474

Epoch: 6| Step: 6
Training loss: 2.232131242752075
Validation loss: 2.283968690902956

Epoch: 6| Step: 7
Training loss: 1.5124493837356567
Validation loss: 2.293737088480303

Epoch: 6| Step: 8
Training loss: 2.456259250640869
Validation loss: 2.3059410715615876

Epoch: 6| Step: 9
Training loss: 2.4574081897735596
Validation loss: 2.305443540696175

Epoch: 6| Step: 10
Training loss: 3.575411796569824
Validation loss: 2.33683354367492

Epoch: 6| Step: 11
Training loss: 2.7803821563720703
Validation loss: 2.3317964615360385

Epoch: 6| Step: 12
Training loss: 2.1542022228240967
Validation loss: 2.342414732902281

Epoch: 6| Step: 13
Training loss: 1.82496178150177
Validation loss: 2.346296751370994

Epoch: 285| Step: 0
Training loss: 2.049412250518799
Validation loss: 2.3513037825143464

Epoch: 6| Step: 1
Training loss: 2.274169445037842
Validation loss: 2.339751225645824

Epoch: 6| Step: 2
Training loss: 2.6905786991119385
Validation loss: 2.3310812570715465

Epoch: 6| Step: 3
Training loss: 2.3145198822021484
Validation loss: 2.325191354238859

Epoch: 6| Step: 4
Training loss: 2.566922187805176
Validation loss: 2.321140056015343

Epoch: 6| Step: 5
Training loss: 2.376845598220825
Validation loss: 2.3398650717991654

Epoch: 6| Step: 6
Training loss: 2.025128126144409
Validation loss: 2.3472987708225044

Epoch: 6| Step: 7
Training loss: 2.727489948272705
Validation loss: 2.3532516341055594

Epoch: 6| Step: 8
Training loss: 2.242947578430176
Validation loss: 2.3427390616427184

Epoch: 6| Step: 9
Training loss: 3.119100570678711
Validation loss: 2.3407297775309575

Epoch: 6| Step: 10
Training loss: 1.9202210903167725
Validation loss: 2.3688048496041247

Epoch: 6| Step: 11
Training loss: 3.5376298427581787
Validation loss: 2.370206868776711

Epoch: 6| Step: 12
Training loss: 2.62981915473938
Validation loss: 2.339559490962695

Epoch: 6| Step: 13
Training loss: 2.2496087551116943
Validation loss: 2.3123788269617225

Epoch: 286| Step: 0
Training loss: 3.049903154373169
Validation loss: 2.307288315988356

Epoch: 6| Step: 1
Training loss: 1.8267338275909424
Validation loss: 2.3114071174334456

Epoch: 6| Step: 2
Training loss: 2.363964557647705
Validation loss: 2.3029136785896878

Epoch: 6| Step: 3
Training loss: 1.7380039691925049
Validation loss: 2.3093784752712456

Epoch: 6| Step: 4
Training loss: 2.7687087059020996
Validation loss: 2.318281929980042

Epoch: 6| Step: 5
Training loss: 2.579932928085327
Validation loss: 2.305917457867694

Epoch: 6| Step: 6
Training loss: 2.771385431289673
Validation loss: 2.3068013191223145

Epoch: 6| Step: 7
Training loss: 2.5637145042419434
Validation loss: 2.300092658688945

Epoch: 6| Step: 8
Training loss: 2.333811044692993
Validation loss: 2.29717710966705

Epoch: 6| Step: 9
Training loss: 3.0053794384002686
Validation loss: 2.2938078295799995

Epoch: 6| Step: 10
Training loss: 2.1024281978607178
Validation loss: 2.2991070465375016

Epoch: 6| Step: 11
Training loss: 2.854492425918579
Validation loss: 2.3147814504561888

Epoch: 6| Step: 12
Training loss: 2.3514139652252197
Validation loss: 2.3008072555706067

Epoch: 6| Step: 13
Training loss: 2.3167428970336914
Validation loss: 2.3136154861860376

Epoch: 287| Step: 0
Training loss: 2.6537022590637207
Validation loss: 2.328847542885811

Epoch: 6| Step: 1
Training loss: 1.9963061809539795
Validation loss: 2.32333589753797

Epoch: 6| Step: 2
Training loss: 2.369786262512207
Validation loss: 2.350869337717692

Epoch: 6| Step: 3
Training loss: 2.5125789642333984
Validation loss: 2.358204554486018

Epoch: 6| Step: 4
Training loss: 2.3508739471435547
Validation loss: 2.3771702422890613

Epoch: 6| Step: 5
Training loss: 2.666611909866333
Validation loss: 2.3709033766100482

Epoch: 6| Step: 6
Training loss: 3.677933692932129
Validation loss: 2.3655630824386433

Epoch: 6| Step: 7
Training loss: 2.5204107761383057
Validation loss: 2.3723338752664547

Epoch: 6| Step: 8
Training loss: 2.5550174713134766
Validation loss: 2.380923760834561

Epoch: 6| Step: 9
Training loss: 1.9810144901275635
Validation loss: 2.3636336454781155

Epoch: 6| Step: 10
Training loss: 2.231008291244507
Validation loss: 2.3383785934858423

Epoch: 6| Step: 11
Training loss: 2.614635467529297
Validation loss: 2.315275042287765

Epoch: 6| Step: 12
Training loss: 2.6190052032470703
Validation loss: 2.318380614762665

Epoch: 6| Step: 13
Training loss: 1.418953537940979
Validation loss: 2.3010081552690074

Epoch: 288| Step: 0
Training loss: 2.4760024547576904
Validation loss: 2.2924278551532375

Epoch: 6| Step: 1
Training loss: 2.8612167835235596
Validation loss: 2.281343383173789

Epoch: 6| Step: 2
Training loss: 1.677180290222168
Validation loss: 2.2935637222823275

Epoch: 6| Step: 3
Training loss: 1.961506962776184
Validation loss: 2.2915475804318666

Epoch: 6| Step: 4
Training loss: 2.9817733764648438
Validation loss: 2.291528687682203

Epoch: 6| Step: 5
Training loss: 2.5530776977539062
Validation loss: 2.295586116852299

Epoch: 6| Step: 6
Training loss: 2.041093349456787
Validation loss: 2.2984439929326377

Epoch: 6| Step: 7
Training loss: 2.789633274078369
Validation loss: 2.2955510270211006

Epoch: 6| Step: 8
Training loss: 3.2142488956451416
Validation loss: 2.307372127809832

Epoch: 6| Step: 9
Training loss: 2.477670669555664
Validation loss: 2.3148582340568624

Epoch: 6| Step: 10
Training loss: 2.2131667137145996
Validation loss: 2.327876449913107

Epoch: 6| Step: 11
Training loss: 3.2556304931640625
Validation loss: 2.3282429428510767

Epoch: 6| Step: 12
Training loss: 1.937364101409912
Validation loss: 2.3021145815490396

Epoch: 6| Step: 13
Training loss: 2.474588632583618
Validation loss: 2.309680743884015

Epoch: 289| Step: 0
Training loss: 2.1594409942626953
Validation loss: 2.2956001861121065

Epoch: 6| Step: 1
Training loss: 2.0681302547454834
Validation loss: 2.284866109971077

Epoch: 6| Step: 2
Training loss: 2.528446674346924
Validation loss: 2.293048269005232

Epoch: 6| Step: 3
Training loss: 2.031109571456909
Validation loss: 2.3027503413538777

Epoch: 6| Step: 4
Training loss: 2.664367198944092
Validation loss: 2.3046025563311834

Epoch: 6| Step: 5
Training loss: 2.5930256843566895
Validation loss: 2.315543520835138

Epoch: 6| Step: 6
Training loss: 2.740779399871826
Validation loss: 2.3231787015033025

Epoch: 6| Step: 7
Training loss: 2.458503246307373
Validation loss: 2.321976160490385

Epoch: 6| Step: 8
Training loss: 2.3420495986938477
Validation loss: 2.3218055079060216

Epoch: 6| Step: 9
Training loss: 3.5190181732177734
Validation loss: 2.3302219247305267

Epoch: 6| Step: 10
Training loss: 2.193044424057007
Validation loss: 2.342347785990725

Epoch: 6| Step: 11
Training loss: 3.2076594829559326
Validation loss: 2.352867900684316

Epoch: 6| Step: 12
Training loss: 1.8747905492782593
Validation loss: 2.3490002885941537

Epoch: 6| Step: 13
Training loss: 2.194727659225464
Validation loss: 2.3556110500007548

Epoch: 290| Step: 0
Training loss: 3.336841106414795
Validation loss: 2.335945371658571

Epoch: 6| Step: 1
Training loss: 2.3125312328338623
Validation loss: 2.3066365770114365

Epoch: 6| Step: 2
Training loss: 1.9547218084335327
Validation loss: 2.3001995137942735

Epoch: 6| Step: 3
Training loss: 2.7510626316070557
Validation loss: 2.2888763181624876

Epoch: 6| Step: 4
Training loss: 2.5947370529174805
Validation loss: 2.2853752400285456

Epoch: 6| Step: 5
Training loss: 2.711273431777954
Validation loss: 2.2760308916850756

Epoch: 6| Step: 6
Training loss: 2.8855652809143066
Validation loss: 2.276749567318988

Epoch: 6| Step: 7
Training loss: 1.6896734237670898
Validation loss: 2.275084267380417

Epoch: 6| Step: 8
Training loss: 2.720651149749756
Validation loss: 2.2735340761882004

Epoch: 6| Step: 9
Training loss: 2.171947717666626
Validation loss: 2.279073258881928

Epoch: 6| Step: 10
Training loss: 2.9053683280944824
Validation loss: 2.2764617448211997

Epoch: 6| Step: 11
Training loss: 2.3538012504577637
Validation loss: 2.2843969457892963

Epoch: 6| Step: 12
Training loss: 1.9223718643188477
Validation loss: 2.276104457916752

Epoch: 6| Step: 13
Training loss: 2.516735076904297
Validation loss: 2.277259010140614

Epoch: 291| Step: 0
Training loss: 2.87856388092041
Validation loss: 2.2948280636982252

Epoch: 6| Step: 1
Training loss: 2.392768621444702
Validation loss: 2.316831511835898

Epoch: 6| Step: 2
Training loss: 2.295253276824951
Validation loss: 2.310142858054048

Epoch: 6| Step: 3
Training loss: 1.8944100141525269
Validation loss: 2.3147117168672624

Epoch: 6| Step: 4
Training loss: 1.9713265895843506
Validation loss: 2.328941519542407

Epoch: 6| Step: 5
Training loss: 1.6589818000793457
Validation loss: 2.3245872759049937

Epoch: 6| Step: 6
Training loss: 2.7691001892089844
Validation loss: 2.340744977356285

Epoch: 6| Step: 7
Training loss: 3.1895813941955566
Validation loss: 2.3440999061830583

Epoch: 6| Step: 8
Training loss: 2.251551389694214
Validation loss: 2.334551880436559

Epoch: 6| Step: 9
Training loss: 3.2433271408081055
Validation loss: 2.326854531482984

Epoch: 6| Step: 10
Training loss: 2.12394380569458
Validation loss: 2.3094289687372025

Epoch: 6| Step: 11
Training loss: 2.8337087631225586
Validation loss: 2.3269359783459733

Epoch: 6| Step: 12
Training loss: 2.374887466430664
Validation loss: 2.3156978520013953

Epoch: 6| Step: 13
Training loss: 2.6208510398864746
Validation loss: 2.29592312535932

Epoch: 292| Step: 0
Training loss: 2.9886136054992676
Validation loss: 2.2975055735598326

Epoch: 6| Step: 1
Training loss: 2.2593226432800293
Validation loss: 2.2972221092511247

Epoch: 6| Step: 2
Training loss: 1.9186022281646729
Validation loss: 2.304381150071339

Epoch: 6| Step: 3
Training loss: 2.5827884674072266
Validation loss: 2.318196271055488

Epoch: 6| Step: 4
Training loss: 1.4507981538772583
Validation loss: 2.320948795605731

Epoch: 6| Step: 5
Training loss: 2.724116802215576
Validation loss: 2.317273816754741

Epoch: 6| Step: 6
Training loss: 2.5540876388549805
Validation loss: 2.3169624959268877

Epoch: 6| Step: 7
Training loss: 1.960496187210083
Validation loss: 2.2951493981064006

Epoch: 6| Step: 8
Training loss: 3.0672812461853027
Validation loss: 2.298954802174722

Epoch: 6| Step: 9
Training loss: 2.9749772548675537
Validation loss: 2.2839865223053963

Epoch: 6| Step: 10
Training loss: 3.603074550628662
Validation loss: 2.2873987408094507

Epoch: 6| Step: 11
Training loss: 2.0718698501586914
Validation loss: 2.29447607071169

Epoch: 6| Step: 12
Training loss: 2.0485548973083496
Validation loss: 2.292736190621571

Epoch: 6| Step: 13
Training loss: 2.2648303508758545
Validation loss: 2.294466503204838

Epoch: 293| Step: 0
Training loss: 1.8623906373977661
Validation loss: 2.2922765131919616

Epoch: 6| Step: 1
Training loss: 2.1672234535217285
Validation loss: 2.3051976721773864

Epoch: 6| Step: 2
Training loss: 2.5671799182891846
Validation loss: 2.2964056653361165

Epoch: 6| Step: 3
Training loss: 2.1741161346435547
Validation loss: 2.2938667138417563

Epoch: 6| Step: 4
Training loss: 1.8014471530914307
Validation loss: 2.305859899008146

Epoch: 6| Step: 5
Training loss: 3.2443838119506836
Validation loss: 2.3068516177515828

Epoch: 6| Step: 6
Training loss: 2.599170446395874
Validation loss: 2.29391138527983

Epoch: 6| Step: 7
Training loss: 2.0226025581359863
Validation loss: 2.3040164029726418

Epoch: 6| Step: 8
Training loss: 2.890200614929199
Validation loss: 2.3195235831763155

Epoch: 6| Step: 9
Training loss: 2.926736354827881
Validation loss: 2.314660951655398

Epoch: 6| Step: 10
Training loss: 2.553586006164551
Validation loss: 2.3046579386598323

Epoch: 6| Step: 11
Training loss: 1.865173101425171
Validation loss: 2.326107625038393

Epoch: 6| Step: 12
Training loss: 2.774061918258667
Validation loss: 2.3318459192911782

Epoch: 6| Step: 13
Training loss: 3.263801097869873
Validation loss: 2.3517126652502243

Epoch: 294| Step: 0
Training loss: 2.7367124557495117
Validation loss: 2.331311833473944

Epoch: 6| Step: 1
Training loss: 2.4429140090942383
Validation loss: 2.3386968438343336

Epoch: 6| Step: 2
Training loss: 3.124727249145508
Validation loss: 2.327639777173278

Epoch: 6| Step: 3
Training loss: 2.4625906944274902
Validation loss: 2.318716892632105

Epoch: 6| Step: 4
Training loss: 1.942955732345581
Validation loss: 2.3154162796594764

Epoch: 6| Step: 5
Training loss: 3.108698844909668
Validation loss: 2.2981244107728362

Epoch: 6| Step: 6
Training loss: 1.8981108665466309
Validation loss: 2.287992048007186

Epoch: 6| Step: 7
Training loss: 3.0131278038024902
Validation loss: 2.2916471035249772

Epoch: 6| Step: 8
Training loss: 2.5358409881591797
Validation loss: 2.2907747581440914

Epoch: 6| Step: 9
Training loss: 2.676762104034424
Validation loss: 2.2855798198330786

Epoch: 6| Step: 10
Training loss: 2.027621269226074
Validation loss: 2.2851380789151756

Epoch: 6| Step: 11
Training loss: 1.8575694561004639
Validation loss: 2.2853206126920638

Epoch: 6| Step: 12
Training loss: 2.1581337451934814
Validation loss: 2.3013492579101236

Epoch: 6| Step: 13
Training loss: 2.4481866359710693
Validation loss: 2.3129132178521927

Epoch: 295| Step: 0
Training loss: 2.5361552238464355
Validation loss: 2.302191360022432

Epoch: 6| Step: 1
Training loss: 2.7783522605895996
Validation loss: 2.308667516195646

Epoch: 6| Step: 2
Training loss: 2.271318197250366
Validation loss: 2.3000577829217397

Epoch: 6| Step: 3
Training loss: 1.98042631149292
Validation loss: 2.2997458365655716

Epoch: 6| Step: 4
Training loss: 1.9624592065811157
Validation loss: 2.2999234558433614

Epoch: 6| Step: 5
Training loss: 3.0218472480773926
Validation loss: 2.3175071798345095

Epoch: 6| Step: 6
Training loss: 1.883315086364746
Validation loss: 2.302239053992815

Epoch: 6| Step: 7
Training loss: 2.208347797393799
Validation loss: 2.2964447954649567

Epoch: 6| Step: 8
Training loss: 2.6865696907043457
Validation loss: 2.286705340108564

Epoch: 6| Step: 9
Training loss: 2.6790831089019775
Validation loss: 2.282318925344816

Epoch: 6| Step: 10
Training loss: 2.603423595428467
Validation loss: 2.2871184887424594

Epoch: 6| Step: 11
Training loss: 2.458519697189331
Validation loss: 2.278055937059464

Epoch: 6| Step: 12
Training loss: 2.743858814239502
Validation loss: 2.2957439499516643

Epoch: 6| Step: 13
Training loss: 2.4249961376190186
Validation loss: 2.3110005112104517

Epoch: 296| Step: 0
Training loss: 2.105239152908325
Validation loss: 2.3183324131914365

Epoch: 6| Step: 1
Training loss: 2.681124210357666
Validation loss: 2.3484789145890104

Epoch: 6| Step: 2
Training loss: 2.765707015991211
Validation loss: 2.330824790462371

Epoch: 6| Step: 3
Training loss: 2.486130714416504
Validation loss: 2.3635055941920124

Epoch: 6| Step: 4
Training loss: 2.771778106689453
Validation loss: 2.3624752490751204

Epoch: 6| Step: 5
Training loss: 2.5377726554870605
Validation loss: 2.333153638788449

Epoch: 6| Step: 6
Training loss: 2.2546536922454834
Validation loss: 2.3216570705495854

Epoch: 6| Step: 7
Training loss: 2.6344172954559326
Validation loss: 2.318948689327445

Epoch: 6| Step: 8
Training loss: 2.5529744625091553
Validation loss: 2.31564361305647

Epoch: 6| Step: 9
Training loss: 2.4260635375976562
Validation loss: 2.2911157556759414

Epoch: 6| Step: 10
Training loss: 1.8001654148101807
Validation loss: 2.281465397086195

Epoch: 6| Step: 11
Training loss: 2.6106367111206055
Validation loss: 2.2837802953617548

Epoch: 6| Step: 12
Training loss: 2.3702526092529297
Validation loss: 2.2736609110268216

Epoch: 6| Step: 13
Training loss: 2.6211910247802734
Validation loss: 2.2725601042470625

Epoch: 297| Step: 0
Training loss: 2.7497682571411133
Validation loss: 2.272122031898909

Epoch: 6| Step: 1
Training loss: 2.538945198059082
Validation loss: 2.2799078495271745

Epoch: 6| Step: 2
Training loss: 2.3326334953308105
Validation loss: 2.2761452428756224

Epoch: 6| Step: 3
Training loss: 2.7342886924743652
Validation loss: 2.2765853994636127

Epoch: 6| Step: 4
Training loss: 3.290010929107666
Validation loss: 2.279994546726186

Epoch: 6| Step: 5
Training loss: 2.4916412830352783
Validation loss: 2.2848196414209183

Epoch: 6| Step: 6
Training loss: 2.3661952018737793
Validation loss: 2.2772126095269316

Epoch: 6| Step: 7
Training loss: 2.451448440551758
Validation loss: 2.275553595635199

Epoch: 6| Step: 8
Training loss: 2.905038356781006
Validation loss: 2.2898776133855185

Epoch: 6| Step: 9
Training loss: 1.903944492340088
Validation loss: 2.2811272785227787

Epoch: 6| Step: 10
Training loss: 3.356783390045166
Validation loss: 2.314469688682146

Epoch: 6| Step: 11
Training loss: 1.005619764328003
Validation loss: 2.31415121786056

Epoch: 6| Step: 12
Training loss: 2.2462358474731445
Validation loss: 2.3375139159540974

Epoch: 6| Step: 13
Training loss: 1.6311925649642944
Validation loss: 2.3472740496358564

Epoch: 298| Step: 0
Training loss: 2.009040594100952
Validation loss: 2.3703143430012528

Epoch: 6| Step: 1
Training loss: 2.2857422828674316
Validation loss: 2.344248484539729

Epoch: 6| Step: 2
Training loss: 2.9155049324035645
Validation loss: 2.3565791165956886

Epoch: 6| Step: 3
Training loss: 2.3961215019226074
Validation loss: 2.343917505715483

Epoch: 6| Step: 4
Training loss: 2.5275893211364746
Validation loss: 2.35839726719805

Epoch: 6| Step: 5
Training loss: 2.5290677547454834
Validation loss: 2.3375189483806653

Epoch: 6| Step: 6
Training loss: 2.294954538345337
Validation loss: 2.300182988566737

Epoch: 6| Step: 7
Training loss: 2.330955743789673
Validation loss: 2.288600847285281

Epoch: 6| Step: 8
Training loss: 2.8697376251220703
Validation loss: 2.2780770281309723

Epoch: 6| Step: 9
Training loss: 2.497490406036377
Validation loss: 2.285904610028831

Epoch: 6| Step: 10
Training loss: 2.6440064907073975
Validation loss: 2.263786559463829

Epoch: 6| Step: 11
Training loss: 2.44846773147583
Validation loss: 2.2804728682323168

Epoch: 6| Step: 12
Training loss: 1.9106881618499756
Validation loss: 2.290351413911389

Epoch: 6| Step: 13
Training loss: 2.8790273666381836
Validation loss: 2.2843659770104194

Epoch: 299| Step: 0
Training loss: 1.8129959106445312
Validation loss: 2.2936443231439076

Epoch: 6| Step: 1
Training loss: 2.245356559753418
Validation loss: 2.2930691960037395

Epoch: 6| Step: 2
Training loss: 2.3437106609344482
Validation loss: 2.2838065085872525

Epoch: 6| Step: 3
Training loss: 2.4171783924102783
Validation loss: 2.288860482554282

Epoch: 6| Step: 4
Training loss: 2.421940565109253
Validation loss: 2.3033463672925065

Epoch: 6| Step: 5
Training loss: 2.3945999145507812
Validation loss: 2.3162484604825258

Epoch: 6| Step: 6
Training loss: 2.9506311416625977
Validation loss: 2.3324120788164038

Epoch: 6| Step: 7
Training loss: 3.3702127933502197
Validation loss: 2.3148114860698743

Epoch: 6| Step: 8
Training loss: 2.4705429077148438
Validation loss: 2.342247570714643

Epoch: 6| Step: 9
Training loss: 2.736572742462158
Validation loss: 2.3428214621800247

Epoch: 6| Step: 10
Training loss: 2.739318370819092
Validation loss: 2.3405796122807327

Epoch: 6| Step: 11
Training loss: 1.7005839347839355
Validation loss: 2.343906451297063

Epoch: 6| Step: 12
Training loss: 2.4869093894958496
Validation loss: 2.3648620907978346

Epoch: 6| Step: 13
Training loss: 2.182450532913208
Validation loss: 2.34049512750359

Epoch: 300| Step: 0
Training loss: 2.4057464599609375
Validation loss: 2.353883053666802

Epoch: 6| Step: 1
Training loss: 2.5731539726257324
Validation loss: 2.3433561991619807

Epoch: 6| Step: 2
Training loss: 2.986266613006592
Validation loss: 2.3551998933156333

Epoch: 6| Step: 3
Training loss: 1.9111249446868896
Validation loss: 2.3431388434543403

Epoch: 6| Step: 4
Training loss: 2.4576504230499268
Validation loss: 2.3432743574983332

Epoch: 6| Step: 5
Training loss: 3.6310439109802246
Validation loss: 2.3514462645335863

Epoch: 6| Step: 6
Training loss: 1.286050796508789
Validation loss: 2.333690712528844

Epoch: 6| Step: 7
Training loss: 2.9306488037109375
Validation loss: 2.3369686731728176

Epoch: 6| Step: 8
Training loss: 2.4918746948242188
Validation loss: 2.332404610931232

Epoch: 6| Step: 9
Training loss: 2.5394785404205322
Validation loss: 2.3108210268840996

Epoch: 6| Step: 10
Training loss: 1.7556593418121338
Validation loss: 2.30558334114731

Epoch: 6| Step: 11
Training loss: 2.367324113845825
Validation loss: 2.284026053643996

Epoch: 6| Step: 12
Training loss: 2.743420124053955
Validation loss: 2.2851980860515306

Epoch: 6| Step: 13
Training loss: 2.412796974182129
Validation loss: 2.278502469421715

Epoch: 301| Step: 0
Training loss: 3.3724350929260254
Validation loss: 2.2674140173901796

Epoch: 6| Step: 1
Training loss: 2.1599044799804688
Validation loss: 2.2854158557871336

Epoch: 6| Step: 2
Training loss: 2.2645161151885986
Validation loss: 2.269714593887329

Epoch: 6| Step: 3
Training loss: 2.0782241821289062
Validation loss: 2.2708490587049917

Epoch: 6| Step: 4
Training loss: 2.318308115005493
Validation loss: 2.2630954762940765

Epoch: 6| Step: 5
Training loss: 2.435908555984497
Validation loss: 2.2818485870156238

Epoch: 6| Step: 6
Training loss: 1.9358255863189697
Validation loss: 2.2725712304474204

Epoch: 6| Step: 7
Training loss: 2.974238872528076
Validation loss: 2.2607387291487826

Epoch: 6| Step: 8
Training loss: 2.6178784370422363
Validation loss: 2.2788790707947104

Epoch: 6| Step: 9
Training loss: 2.179745674133301
Validation loss: 2.2884357155010266

Epoch: 6| Step: 10
Training loss: 2.4973249435424805
Validation loss: 2.2998848781790784

Epoch: 6| Step: 11
Training loss: 2.601121425628662
Validation loss: 2.30353965297822

Epoch: 6| Step: 12
Training loss: 2.012284517288208
Validation loss: 2.328066687430105

Epoch: 6| Step: 13
Training loss: 3.4496731758117676
Validation loss: 2.320454793591653

Epoch: 302| Step: 0
Training loss: 2.0787243843078613
Validation loss: 2.3215380817331295

Epoch: 6| Step: 1
Training loss: 2.8994483947753906
Validation loss: 2.316500320229479

Epoch: 6| Step: 2
Training loss: 2.013105869293213
Validation loss: 2.3062403125147664

Epoch: 6| Step: 3
Training loss: 1.7570815086364746
Validation loss: 2.3077247386337607

Epoch: 6| Step: 4
Training loss: 1.9337444305419922
Validation loss: 2.28506048264042

Epoch: 6| Step: 5
Training loss: 2.4111990928649902
Validation loss: 2.293566019304337

Epoch: 6| Step: 6
Training loss: 2.435859441757202
Validation loss: 2.3038597235115628

Epoch: 6| Step: 7
Training loss: 2.4081764221191406
Validation loss: 2.287993992528608

Epoch: 6| Step: 8
Training loss: 2.77516508102417
Validation loss: 2.294594439127112

Epoch: 6| Step: 9
Training loss: 2.905099868774414
Validation loss: 2.307849271323091

Epoch: 6| Step: 10
Training loss: 2.4048376083374023
Validation loss: 2.3019948967041506

Epoch: 6| Step: 11
Training loss: 3.1791577339172363
Validation loss: 2.301172112905851

Epoch: 6| Step: 12
Training loss: 2.488985538482666
Validation loss: 2.2918781644554547

Epoch: 6| Step: 13
Training loss: 2.2990949153900146
Validation loss: 2.302176272997292

Epoch: 303| Step: 0
Training loss: 3.00469970703125
Validation loss: 2.2907098057449504

Epoch: 6| Step: 1
Training loss: 2.974843740463257
Validation loss: 2.305068326252763

Epoch: 6| Step: 2
Training loss: 3.053544521331787
Validation loss: 2.295402390982515

Epoch: 6| Step: 3
Training loss: 2.193023681640625
Validation loss: 2.2964868750623477

Epoch: 6| Step: 4
Training loss: 1.6076090335845947
Validation loss: 2.29947595186131

Epoch: 6| Step: 5
Training loss: 3.090301513671875
Validation loss: 2.3041131265701784

Epoch: 6| Step: 6
Training loss: 1.8592928647994995
Validation loss: 2.3011745714372203

Epoch: 6| Step: 7
Training loss: 2.993312120437622
Validation loss: 2.301830573748517

Epoch: 6| Step: 8
Training loss: 2.2075154781341553
Validation loss: 2.302817472847559

Epoch: 6| Step: 9
Training loss: 2.6930084228515625
Validation loss: 2.2992836967591317

Epoch: 6| Step: 10
Training loss: 2.336535930633545
Validation loss: 2.302548711017896

Epoch: 6| Step: 11
Training loss: 1.3698508739471436
Validation loss: 2.296911381906079

Epoch: 6| Step: 12
Training loss: 2.2481460571289062
Validation loss: 2.310744126637777

Epoch: 6| Step: 13
Training loss: 2.7780866622924805
Validation loss: 2.2799620705266155

Epoch: 304| Step: 0
Training loss: 2.4378528594970703
Validation loss: 2.2716467662524154

Epoch: 6| Step: 1
Training loss: 2.749915838241577
Validation loss: 2.2947964693910334

Epoch: 6| Step: 2
Training loss: 2.1754894256591797
Validation loss: 2.2759187042072253

Epoch: 6| Step: 3
Training loss: 3.5139684677124023
Validation loss: 2.2868890044509724

Epoch: 6| Step: 4
Training loss: 2.3965916633605957
Validation loss: 2.2866956136559926

Epoch: 6| Step: 5
Training loss: 2.512413263320923
Validation loss: 2.282353321711222

Epoch: 6| Step: 6
Training loss: 1.830174207687378
Validation loss: 2.2721408362029702

Epoch: 6| Step: 7
Training loss: 2.3854119777679443
Validation loss: 2.27984106925226

Epoch: 6| Step: 8
Training loss: 2.2831122875213623
Validation loss: 2.2629280756878596

Epoch: 6| Step: 9
Training loss: 2.2730817794799805
Validation loss: 2.2797542695076234

Epoch: 6| Step: 10
Training loss: 2.7761738300323486
Validation loss: 2.2727116987269413

Epoch: 6| Step: 11
Training loss: 2.4634459018707275
Validation loss: 2.2776541325353805

Epoch: 6| Step: 12
Training loss: 1.8922064304351807
Validation loss: 2.281825516813545

Epoch: 6| Step: 13
Training loss: 2.4602982997894287
Validation loss: 2.2654333165896836

Epoch: 305| Step: 0
Training loss: 2.4620113372802734
Validation loss: 2.2790445755886775

Epoch: 6| Step: 1
Training loss: 2.05045747756958
Validation loss: 2.299140207229122

Epoch: 6| Step: 2
Training loss: 3.338926315307617
Validation loss: 2.2899561646164104

Epoch: 6| Step: 3
Training loss: 2.0951404571533203
Validation loss: 2.295539138137653

Epoch: 6| Step: 4
Training loss: 2.0448455810546875
Validation loss: 2.3045797322386052

Epoch: 6| Step: 5
Training loss: 2.2212257385253906
Validation loss: 2.2970939708012406

Epoch: 6| Step: 6
Training loss: 2.829792022705078
Validation loss: 2.293920647713446

Epoch: 6| Step: 7
Training loss: 2.418102741241455
Validation loss: 2.310420991272055

Epoch: 6| Step: 8
Training loss: 2.324424982070923
Validation loss: 2.287942676134007

Epoch: 6| Step: 9
Training loss: 2.152100086212158
Validation loss: 2.2983814029283423

Epoch: 6| Step: 10
Training loss: 2.2778542041778564
Validation loss: 2.291565497716268

Epoch: 6| Step: 11
Training loss: 2.042728900909424
Validation loss: 2.2940491322548158

Epoch: 6| Step: 12
Training loss: 3.007451057434082
Validation loss: 2.309847657398511

Epoch: 6| Step: 13
Training loss: 3.0498671531677246
Validation loss: 2.303570734557285

Epoch: 306| Step: 0
Training loss: 2.5271143913269043
Validation loss: 2.2934074824856174

Epoch: 6| Step: 1
Training loss: 2.759547233581543
Validation loss: 2.2767613062294583

Epoch: 6| Step: 2
Training loss: 2.679311752319336
Validation loss: 2.2697269121805825

Epoch: 6| Step: 3
Training loss: 3.0906941890716553
Validation loss: 2.2497805369797574

Epoch: 6| Step: 4
Training loss: 1.9685075283050537
Validation loss: 2.267595696192916

Epoch: 6| Step: 5
Training loss: 2.276240348815918
Validation loss: 2.2594059282733547

Epoch: 6| Step: 6
Training loss: 1.9537423849105835
Validation loss: 2.265697104956514

Epoch: 6| Step: 7
Training loss: 2.9584784507751465
Validation loss: 2.2737771259841097

Epoch: 6| Step: 8
Training loss: 2.116567611694336
Validation loss: 2.2688502393743044

Epoch: 6| Step: 9
Training loss: 2.2843334674835205
Validation loss: 2.2696469932474117

Epoch: 6| Step: 10
Training loss: 2.2773709297180176
Validation loss: 2.2665304201905445

Epoch: 6| Step: 11
Training loss: 2.8269033432006836
Validation loss: 2.2888167186449935

Epoch: 6| Step: 12
Training loss: 2.0696325302124023
Validation loss: 2.276952556384507

Epoch: 6| Step: 13
Training loss: 2.0726547241210938
Validation loss: 2.288045134595645

Epoch: 307| Step: 0
Training loss: 3.362011671066284
Validation loss: 2.297849439805554

Epoch: 6| Step: 1
Training loss: 1.9963996410369873
Validation loss: 2.2969293568723943

Epoch: 6| Step: 2
Training loss: 2.5611209869384766
Validation loss: 2.3004024913234096

Epoch: 6| Step: 3
Training loss: 2.93194580078125
Validation loss: 2.3061405945849676

Epoch: 6| Step: 4
Training loss: 2.2944986820220947
Validation loss: 2.2958378996900333

Epoch: 6| Step: 5
Training loss: 2.6808128356933594
Validation loss: 2.2930968961408063

Epoch: 6| Step: 6
Training loss: 2.4133777618408203
Validation loss: 2.2852235583848852

Epoch: 6| Step: 7
Training loss: 1.945966362953186
Validation loss: 2.2865076013790664

Epoch: 6| Step: 8
Training loss: 2.6152992248535156
Validation loss: 2.290841825546757

Epoch: 6| Step: 9
Training loss: 2.12610125541687
Validation loss: 2.274645084975868

Epoch: 6| Step: 10
Training loss: 2.4667325019836426
Validation loss: 2.2755236830762637

Epoch: 6| Step: 11
Training loss: 2.2386727333068848
Validation loss: 2.2705061256244616

Epoch: 6| Step: 12
Training loss: 2.2223963737487793
Validation loss: 2.2708777048254527

Epoch: 6| Step: 13
Training loss: 2.1239500045776367
Validation loss: 2.2748604359165316

Epoch: 308| Step: 0
Training loss: 1.3927266597747803
Validation loss: 2.2692808669100524

Epoch: 6| Step: 1
Training loss: 2.48628306388855
Validation loss: 2.273349127461833

Epoch: 6| Step: 2
Training loss: 2.581139326095581
Validation loss: 2.3178624017264253

Epoch: 6| Step: 3
Training loss: 2.5512773990631104
Validation loss: 2.3278621524892826

Epoch: 6| Step: 4
Training loss: 2.5598201751708984
Validation loss: 2.3231565362663678

Epoch: 6| Step: 5
Training loss: 2.5157828330993652
Validation loss: 2.3588927522782357

Epoch: 6| Step: 6
Training loss: 3.493016242980957
Validation loss: 2.3821460739258797

Epoch: 6| Step: 7
Training loss: 2.2622976303100586
Validation loss: 2.3655682994473364

Epoch: 6| Step: 8
Training loss: 2.6586880683898926
Validation loss: 2.376753660940355

Epoch: 6| Step: 9
Training loss: 2.8643240928649902
Validation loss: 2.3445177462793167

Epoch: 6| Step: 10
Training loss: 2.0180916786193848
Validation loss: 2.3122341786661456

Epoch: 6| Step: 11
Training loss: 2.6283233165740967
Validation loss: 2.299888333966655

Epoch: 6| Step: 12
Training loss: 2.169621467590332
Validation loss: 2.27075150833335

Epoch: 6| Step: 13
Training loss: 2.2695226669311523
Validation loss: 2.2704653611747165

Epoch: 309| Step: 0
Training loss: 2.1267266273498535
Validation loss: 2.252878014759351

Epoch: 6| Step: 1
Training loss: 2.3742003440856934
Validation loss: 2.2628475991628503

Epoch: 6| Step: 2
Training loss: 2.4509329795837402
Validation loss: 2.245687254013554

Epoch: 6| Step: 3
Training loss: 2.588261127471924
Validation loss: 2.238173879602904

Epoch: 6| Step: 4
Training loss: 2.804842472076416
Validation loss: 2.2453676885174167

Epoch: 6| Step: 5
Training loss: 1.7931667566299438
Validation loss: 2.242205012229181

Epoch: 6| Step: 6
Training loss: 3.0962376594543457
Validation loss: 2.2443715756939304

Epoch: 6| Step: 7
Training loss: 1.780026912689209
Validation loss: 2.2526171117700557

Epoch: 6| Step: 8
Training loss: 2.417738437652588
Validation loss: 2.2454326998802925

Epoch: 6| Step: 9
Training loss: 2.7724924087524414
Validation loss: 2.2465160687764487

Epoch: 6| Step: 10
Training loss: 2.52923583984375
Validation loss: 2.257410016111148

Epoch: 6| Step: 11
Training loss: 2.593012571334839
Validation loss: 2.251444883244012

Epoch: 6| Step: 12
Training loss: 2.2443161010742188
Validation loss: 2.265894559121901

Epoch: 6| Step: 13
Training loss: 2.8689355850219727
Validation loss: 2.265667202652142

Epoch: 310| Step: 0
Training loss: 2.2588491439819336
Validation loss: 2.2616218136202906

Epoch: 6| Step: 1
Training loss: 2.903817653656006
Validation loss: 2.2659892446251324

Epoch: 6| Step: 2
Training loss: 2.493464231491089
Validation loss: 2.2681194274656233

Epoch: 6| Step: 3
Training loss: 2.910219430923462
Validation loss: 2.2735038008741153

Epoch: 6| Step: 4
Training loss: 2.8082115650177
Validation loss: 2.268156420799994

Epoch: 6| Step: 5
Training loss: 2.1932570934295654
Validation loss: 2.2741032620911956

Epoch: 6| Step: 6
Training loss: 2.4781665802001953
Validation loss: 2.263451801833286

Epoch: 6| Step: 7
Training loss: 1.7349478006362915
Validation loss: 2.271135558364212

Epoch: 6| Step: 8
Training loss: 1.8512675762176514
Validation loss: 2.2651216189066568

Epoch: 6| Step: 9
Training loss: 2.994307518005371
Validation loss: 2.2746715930200394

Epoch: 6| Step: 10
Training loss: 2.3461508750915527
Validation loss: 2.286026713668659

Epoch: 6| Step: 11
Training loss: 2.773874282836914
Validation loss: 2.2806576862130115

Epoch: 6| Step: 12
Training loss: 2.0968284606933594
Validation loss: 2.2969282109250306

Epoch: 6| Step: 13
Training loss: 2.1262638568878174
Validation loss: 2.304586274649507

Epoch: 311| Step: 0
Training loss: 2.7079970836639404
Validation loss: 2.3100115099260883

Epoch: 6| Step: 1
Training loss: 2.464052200317383
Validation loss: 2.2989769956117034

Epoch: 6| Step: 2
Training loss: 2.4570775032043457
Validation loss: 2.304555203325005

Epoch: 6| Step: 3
Training loss: 2.544747829437256
Validation loss: 2.304475176718927

Epoch: 6| Step: 4
Training loss: 2.762545585632324
Validation loss: 2.31551073187141

Epoch: 6| Step: 5
Training loss: 1.7186311483383179
Validation loss: 2.293548996730517

Epoch: 6| Step: 6
Training loss: 2.7163636684417725
Validation loss: 2.30371444712403

Epoch: 6| Step: 7
Training loss: 2.810629367828369
Validation loss: 2.2951435658239547

Epoch: 6| Step: 8
Training loss: 1.8647222518920898
Validation loss: 2.2815986269263813

Epoch: 6| Step: 9
Training loss: 2.0279412269592285
Validation loss: 2.2900125544558287

Epoch: 6| Step: 10
Training loss: 1.7971726655960083
Validation loss: 2.2842553841170443

Epoch: 6| Step: 11
Training loss: 2.504530906677246
Validation loss: 2.2891832218375257

Epoch: 6| Step: 12
Training loss: 2.683682441711426
Validation loss: 2.2766488226511146

Epoch: 6| Step: 13
Training loss: 3.180866241455078
Validation loss: 2.2819093837532947

Epoch: 312| Step: 0
Training loss: 2.248581647872925
Validation loss: 2.280626896888979

Epoch: 6| Step: 1
Training loss: 1.7592318058013916
Validation loss: 2.275545786785823

Epoch: 6| Step: 2
Training loss: 2.284550189971924
Validation loss: 2.2721385673810075

Epoch: 6| Step: 3
Training loss: 2.1968517303466797
Validation loss: 2.2814484450124923

Epoch: 6| Step: 4
Training loss: 2.4943621158599854
Validation loss: 2.2645275438985517

Epoch: 6| Step: 5
Training loss: 2.7751717567443848
Validation loss: 2.282085418701172

Epoch: 6| Step: 6
Training loss: 1.7426886558532715
Validation loss: 2.2891115526999197

Epoch: 6| Step: 7
Training loss: 2.2535510063171387
Validation loss: 2.285086008810228

Epoch: 6| Step: 8
Training loss: 2.290586233139038
Validation loss: 2.2888986397815008

Epoch: 6| Step: 9
Training loss: 3.1496634483337402
Validation loss: 2.2805203673660115

Epoch: 6| Step: 10
Training loss: 2.7763750553131104
Validation loss: 2.2780164980119273

Epoch: 6| Step: 11
Training loss: 2.3423166275024414
Validation loss: 2.2749369657167824

Epoch: 6| Step: 12
Training loss: 3.0169856548309326
Validation loss: 2.2778021930366434

Epoch: 6| Step: 13
Training loss: 2.666226625442505
Validation loss: 2.2735934667689826

Epoch: 313| Step: 0
Training loss: 1.7952179908752441
Validation loss: 2.2789146912995206

Epoch: 6| Step: 1
Training loss: 2.319791555404663
Validation loss: 2.2698205183911067

Epoch: 6| Step: 2
Training loss: 2.268341541290283
Validation loss: 2.2701955713251585

Epoch: 6| Step: 3
Training loss: 2.362581729888916
Validation loss: 2.275460094533941

Epoch: 6| Step: 4
Training loss: 2.2250099182128906
Validation loss: 2.273477941431025

Epoch: 6| Step: 5
Training loss: 2.162562847137451
Validation loss: 2.2870048707531345

Epoch: 6| Step: 6
Training loss: 2.9089419841766357
Validation loss: 2.266722233064713

Epoch: 6| Step: 7
Training loss: 2.3489084243774414
Validation loss: 2.2777940765503915

Epoch: 6| Step: 8
Training loss: 2.325193166732788
Validation loss: 2.2822594463184314

Epoch: 6| Step: 9
Training loss: 2.7771472930908203
Validation loss: 2.287225669430148

Epoch: 6| Step: 10
Training loss: 2.6966097354888916
Validation loss: 2.291090391015494

Epoch: 6| Step: 11
Training loss: 3.223332166671753
Validation loss: 2.2983071291318504

Epoch: 6| Step: 12
Training loss: 2.152904510498047
Validation loss: 2.282422384908122

Epoch: 6| Step: 13
Training loss: 2.0736448764801025
Validation loss: 2.2737886110941568

Epoch: 314| Step: 0
Training loss: 2.17615008354187
Validation loss: 2.2768662950044036

Epoch: 6| Step: 1
Training loss: 1.9418278932571411
Validation loss: 2.2678492107698993

Epoch: 6| Step: 2
Training loss: 2.5405654907226562
Validation loss: 2.251763721948029

Epoch: 6| Step: 3
Training loss: 2.208254337310791
Validation loss: 2.250002679004464

Epoch: 6| Step: 4
Training loss: 2.118896961212158
Validation loss: 2.25870483152328

Epoch: 6| Step: 5
Training loss: 1.7747266292572021
Validation loss: 2.2531614867589806

Epoch: 6| Step: 6
Training loss: 2.4657864570617676
Validation loss: 2.2577861098832983

Epoch: 6| Step: 7
Training loss: 2.841313600540161
Validation loss: 2.2693188267369426

Epoch: 6| Step: 8
Training loss: 3.272840976715088
Validation loss: 2.271302084768972

Epoch: 6| Step: 9
Training loss: 3.0149545669555664
Validation loss: 2.252164897098336

Epoch: 6| Step: 10
Training loss: 2.0894906520843506
Validation loss: 2.2502485270141275

Epoch: 6| Step: 11
Training loss: 2.6054840087890625
Validation loss: 2.265810925473449

Epoch: 6| Step: 12
Training loss: 2.1363043785095215
Validation loss: 2.261744132605932

Epoch: 6| Step: 13
Training loss: 2.7376856803894043
Validation loss: 2.277135423434678

Epoch: 315| Step: 0
Training loss: 2.64503812789917
Validation loss: 2.2740233303398214

Epoch: 6| Step: 1
Training loss: 2.405360698699951
Validation loss: 2.295015481210524

Epoch: 6| Step: 2
Training loss: 3.153407573699951
Validation loss: 2.3039423573401665

Epoch: 6| Step: 3
Training loss: 2.9029722213745117
Validation loss: 2.300870539039694

Epoch: 6| Step: 4
Training loss: 1.930328607559204
Validation loss: 2.3041883078954553

Epoch: 6| Step: 5
Training loss: 2.2352449893951416
Validation loss: 2.288961923250588

Epoch: 6| Step: 6
Training loss: 2.5485048294067383
Validation loss: 2.290182546902728

Epoch: 6| Step: 7
Training loss: 1.7018916606903076
Validation loss: 2.2868646908831853

Epoch: 6| Step: 8
Training loss: 2.227259397506714
Validation loss: 2.273390031629993

Epoch: 6| Step: 9
Training loss: 1.4592738151550293
Validation loss: 2.259543640639192

Epoch: 6| Step: 10
Training loss: 1.7025985717773438
Validation loss: 2.2535664804520144

Epoch: 6| Step: 11
Training loss: 2.666412591934204
Validation loss: 2.259271175630631

Epoch: 6| Step: 12
Training loss: 3.437641143798828
Validation loss: 2.253516907333046

Epoch: 6| Step: 13
Training loss: 3.0618200302124023
Validation loss: 2.2419471715086248

Epoch: 316| Step: 0
Training loss: 2.6912131309509277
Validation loss: 2.2604718746677523

Epoch: 6| Step: 1
Training loss: 2.8992013931274414
Validation loss: 2.2627469596042427

Epoch: 6| Step: 2
Training loss: 2.0685222148895264
Validation loss: 2.2633274447533394

Epoch: 6| Step: 3
Training loss: 2.3231310844421387
Validation loss: 2.2695248332074893

Epoch: 6| Step: 4
Training loss: 2.495203971862793
Validation loss: 2.263311952672979

Epoch: 6| Step: 5
Training loss: 2.323178768157959
Validation loss: 2.2585784107126217

Epoch: 6| Step: 6
Training loss: 2.173299789428711
Validation loss: 2.258134083081317

Epoch: 6| Step: 7
Training loss: 2.848902463912964
Validation loss: 2.254736385037822

Epoch: 6| Step: 8
Training loss: 2.0223639011383057
Validation loss: 2.257165688340382

Epoch: 6| Step: 9
Training loss: 2.696359872817993
Validation loss: 2.2619658003571215

Epoch: 6| Step: 10
Training loss: 2.449342966079712
Validation loss: 2.241667286042244

Epoch: 6| Step: 11
Training loss: 2.167180061340332
Validation loss: 2.256565910513683

Epoch: 6| Step: 12
Training loss: 1.7884018421173096
Validation loss: 2.2666999960458405

Epoch: 6| Step: 13
Training loss: 3.055558204650879
Validation loss: 2.2931677551679712

Epoch: 317| Step: 0
Training loss: 2.416131019592285
Validation loss: 2.2936807858046664

Epoch: 6| Step: 1
Training loss: 2.3858871459960938
Validation loss: 2.278011852695096

Epoch: 6| Step: 2
Training loss: 2.8912477493286133
Validation loss: 2.276933011188302

Epoch: 6| Step: 3
Training loss: 1.5873916149139404
Validation loss: 2.2791265710707633

Epoch: 6| Step: 4
Training loss: 2.709076166152954
Validation loss: 2.283614138121246

Epoch: 6| Step: 5
Training loss: 2.3128955364227295
Validation loss: 2.2540800315077587

Epoch: 6| Step: 6
Training loss: 1.9860790967941284
Validation loss: 2.277926460389168

Epoch: 6| Step: 7
Training loss: 2.5610756874084473
Validation loss: 2.288445059971143

Epoch: 6| Step: 8
Training loss: 2.2922868728637695
Validation loss: 2.3009088321398665

Epoch: 6| Step: 9
Training loss: 2.7422358989715576
Validation loss: 2.3403242070187806

Epoch: 6| Step: 10
Training loss: 2.238874912261963
Validation loss: 2.3288227178717174

Epoch: 6| Step: 11
Training loss: 2.614950180053711
Validation loss: 2.3418100931311168

Epoch: 6| Step: 12
Training loss: 2.4966840744018555
Validation loss: 2.347762115540043

Epoch: 6| Step: 13
Training loss: 3.394925117492676
Validation loss: 2.3011909941191315

Epoch: 318| Step: 0
Training loss: 2.168613910675049
Validation loss: 2.288676082447011

Epoch: 6| Step: 1
Training loss: 2.6768269538879395
Validation loss: 2.2600535782434608

Epoch: 6| Step: 2
Training loss: 2.169403076171875
Validation loss: 2.2556085202001754

Epoch: 6| Step: 3
Training loss: 3.236586570739746
Validation loss: 2.2515761903537217

Epoch: 6| Step: 4
Training loss: 2.111027240753174
Validation loss: 2.246869489710818

Epoch: 6| Step: 5
Training loss: 2.1656060218811035
Validation loss: 2.242636396038917

Epoch: 6| Step: 6
Training loss: 2.7505218982696533
Validation loss: 2.2606826828372095

Epoch: 6| Step: 7
Training loss: 2.611726760864258
Validation loss: 2.271043462138022

Epoch: 6| Step: 8
Training loss: 2.7934207916259766
Validation loss: 2.269187086371965

Epoch: 6| Step: 9
Training loss: 2.729851484298706
Validation loss: 2.269151763249469

Epoch: 6| Step: 10
Training loss: 2.438723087310791
Validation loss: 2.273262054689469

Epoch: 6| Step: 11
Training loss: 2.314570426940918
Validation loss: 2.2654333678624963

Epoch: 6| Step: 12
Training loss: 2.383296251296997
Validation loss: 2.2674668758146224

Epoch: 6| Step: 13
Training loss: 1.6051703691482544
Validation loss: 2.267206904708698

Epoch: 319| Step: 0
Training loss: 2.378469228744507
Validation loss: 2.2591011139654342

Epoch: 6| Step: 1
Training loss: 3.191577911376953
Validation loss: 2.2411827733439784

Epoch: 6| Step: 2
Training loss: 2.486934185028076
Validation loss: 2.2644586973292853

Epoch: 6| Step: 3
Training loss: 2.2076117992401123
Validation loss: 2.286408826869021

Epoch: 6| Step: 4
Training loss: 2.2920875549316406
Validation loss: 2.2775917899224067

Epoch: 6| Step: 5
Training loss: 2.491560935974121
Validation loss: 2.3038367404732654

Epoch: 6| Step: 6
Training loss: 2.37662935256958
Validation loss: 2.308956789714034

Epoch: 6| Step: 7
Training loss: 3.079986572265625
Validation loss: 2.3217578113719983

Epoch: 6| Step: 8
Training loss: 2.3001301288604736
Validation loss: 2.297870420640515

Epoch: 6| Step: 9
Training loss: 2.2596588134765625
Validation loss: 2.296028921681066

Epoch: 6| Step: 10
Training loss: 2.1443932056427
Validation loss: 2.2692962769539125

Epoch: 6| Step: 11
Training loss: 2.143078088760376
Validation loss: 2.2690505212353123

Epoch: 6| Step: 12
Training loss: 2.1079888343811035
Validation loss: 2.2650701230572117

Epoch: 6| Step: 13
Training loss: 2.5371813774108887
Validation loss: 2.245370075266848

Epoch: 320| Step: 0
Training loss: 2.7690508365631104
Validation loss: 2.2566096295592604

Epoch: 6| Step: 1
Training loss: 2.530691623687744
Validation loss: 2.2395062600412676

Epoch: 6| Step: 2
Training loss: 1.7740564346313477
Validation loss: 2.2365309756289244

Epoch: 6| Step: 3
Training loss: 2.638442039489746
Validation loss: 2.2307023181710193

Epoch: 6| Step: 4
Training loss: 1.826076865196228
Validation loss: 2.2522971476277998

Epoch: 6| Step: 5
Training loss: 2.153731107711792
Validation loss: 2.234179059664408

Epoch: 6| Step: 6
Training loss: 2.559049129486084
Validation loss: 2.2337544400204896

Epoch: 6| Step: 7
Training loss: 2.690995454788208
Validation loss: 2.24695163132042

Epoch: 6| Step: 8
Training loss: 3.1027472019195557
Validation loss: 2.2489863185472387

Epoch: 6| Step: 9
Training loss: 2.443521022796631
Validation loss: 2.23975274896109

Epoch: 6| Step: 10
Training loss: 2.753995895385742
Validation loss: 2.2484071857185772

Epoch: 6| Step: 11
Training loss: 1.8646159172058105
Validation loss: 2.2327133660675376

Epoch: 6| Step: 12
Training loss: 2.683544158935547
Validation loss: 2.252858874618366

Epoch: 6| Step: 13
Training loss: 1.9963029623031616
Validation loss: 2.2486599888852847

Epoch: 321| Step: 0
Training loss: 2.020268440246582
Validation loss: 2.2295445037144486

Epoch: 6| Step: 1
Training loss: 2.549548625946045
Validation loss: 2.2354847128673265

Epoch: 6| Step: 2
Training loss: 2.167119026184082
Validation loss: 2.2293307909401516

Epoch: 6| Step: 3
Training loss: 2.220757484436035
Validation loss: 2.2365708992045414

Epoch: 6| Step: 4
Training loss: 2.2432732582092285
Validation loss: 2.2401408585168983

Epoch: 6| Step: 5
Training loss: 2.6104044914245605
Validation loss: 2.228286948255313

Epoch: 6| Step: 6
Training loss: 2.686814546585083
Validation loss: 2.244074208762056

Epoch: 6| Step: 7
Training loss: 2.2001311779022217
Validation loss: 2.2507288481599543

Epoch: 6| Step: 8
Training loss: 3.246299982070923
Validation loss: 2.2441463111549296

Epoch: 6| Step: 9
Training loss: 2.238340139389038
Validation loss: 2.2389886225423505

Epoch: 6| Step: 10
Training loss: 3.634389877319336
Validation loss: 2.2317414181206816

Epoch: 6| Step: 11
Training loss: 1.7332621812820435
Validation loss: 2.2451608334818194

Epoch: 6| Step: 12
Training loss: 2.761613368988037
Validation loss: 2.228651273635126

Epoch: 6| Step: 13
Training loss: 1.0908271074295044
Validation loss: 2.248618036188105

Epoch: 322| Step: 0
Training loss: 2.1670260429382324
Validation loss: 2.2488577417148057

Epoch: 6| Step: 1
Training loss: 2.3573501110076904
Validation loss: 2.277272883281913

Epoch: 6| Step: 2
Training loss: 2.7918379306793213
Validation loss: 2.3014821032042145

Epoch: 6| Step: 3
Training loss: 2.3559792041778564
Validation loss: 2.309730173439108

Epoch: 6| Step: 4
Training loss: 2.0249218940734863
Validation loss: 2.3260633484009774

Epoch: 6| Step: 5
Training loss: 2.924732208251953
Validation loss: 2.3246703609343498

Epoch: 6| Step: 6
Training loss: 2.031662940979004
Validation loss: 2.3218371842497136

Epoch: 6| Step: 7
Training loss: 1.3239340782165527
Validation loss: 2.2960206436854538

Epoch: 6| Step: 8
Training loss: 1.8272688388824463
Validation loss: 2.2779628538316294

Epoch: 6| Step: 9
Training loss: 3.5031752586364746
Validation loss: 2.291883599373602

Epoch: 6| Step: 10
Training loss: 2.4440901279449463
Validation loss: 2.274090484906268

Epoch: 6| Step: 11
Training loss: 2.935072898864746
Validation loss: 2.304216064432616

Epoch: 6| Step: 12
Training loss: 2.6115670204162598
Validation loss: 2.280965070570669

Epoch: 6| Step: 13
Training loss: 2.744992971420288
Validation loss: 2.277942044760591

Epoch: 323| Step: 0
Training loss: 2.4104132652282715
Validation loss: 2.2719642398177937

Epoch: 6| Step: 1
Training loss: 2.8426780700683594
Validation loss: 2.2600742950234363

Epoch: 6| Step: 2
Training loss: 2.022162675857544
Validation loss: 2.264265698771323

Epoch: 6| Step: 3
Training loss: 2.324049234390259
Validation loss: 2.25295824902032

Epoch: 6| Step: 4
Training loss: 2.020453453063965
Validation loss: 2.259200039730277

Epoch: 6| Step: 5
Training loss: 2.6928367614746094
Validation loss: 2.2732831034609067

Epoch: 6| Step: 6
Training loss: 2.6111292839050293
Validation loss: 2.263869047164917

Epoch: 6| Step: 7
Training loss: 2.5271384716033936
Validation loss: 2.2592506677873674

Epoch: 6| Step: 8
Training loss: 2.4337031841278076
Validation loss: 2.268440600364439

Epoch: 6| Step: 9
Training loss: 2.4440746307373047
Validation loss: 2.26013328823992

Epoch: 6| Step: 10
Training loss: 1.795062780380249
Validation loss: 2.2584657156339256

Epoch: 6| Step: 11
Training loss: 2.0765042304992676
Validation loss: 2.2746134957959576

Epoch: 6| Step: 12
Training loss: 2.871379852294922
Validation loss: 2.253969077141054

Epoch: 6| Step: 13
Training loss: 2.5817244052886963
Validation loss: 2.256293204522902

Epoch: 324| Step: 0
Training loss: 1.4868977069854736
Validation loss: 2.268350960105978

Epoch: 6| Step: 1
Training loss: 2.7201333045959473
Validation loss: 2.2623289528713433

Epoch: 6| Step: 2
Training loss: 2.4267401695251465
Validation loss: 2.2770684214048487

Epoch: 6| Step: 3
Training loss: 2.023496150970459
Validation loss: 2.285300108694261

Epoch: 6| Step: 4
Training loss: 2.4349160194396973
Validation loss: 2.2692856955271896

Epoch: 6| Step: 5
Training loss: 2.633608341217041
Validation loss: 2.266828162695772

Epoch: 6| Step: 6
Training loss: 1.926450490951538
Validation loss: 2.254572694019605

Epoch: 6| Step: 7
Training loss: 2.5351405143737793
Validation loss: 2.2716093755537465

Epoch: 6| Step: 8
Training loss: 2.506002902984619
Validation loss: 2.3018507060184272

Epoch: 6| Step: 9
Training loss: 2.811326742172241
Validation loss: 2.275273420477426

Epoch: 6| Step: 10
Training loss: 2.10891056060791
Validation loss: 2.2737517369690763

Epoch: 6| Step: 11
Training loss: 3.1035871505737305
Validation loss: 2.2537737892520044

Epoch: 6| Step: 12
Training loss: 2.213371753692627
Validation loss: 2.2445962531592256

Epoch: 6| Step: 13
Training loss: 2.7826943397521973
Validation loss: 2.256271737878041

Epoch: 325| Step: 0
Training loss: 2.384035110473633
Validation loss: 2.241174333839006

Epoch: 6| Step: 1
Training loss: 2.6727685928344727
Validation loss: 2.2254040279696063

Epoch: 6| Step: 2
Training loss: 2.189182996749878
Validation loss: 2.2316110287943194

Epoch: 6| Step: 3
Training loss: 2.7050280570983887
Validation loss: 2.2368488978314143

Epoch: 6| Step: 4
Training loss: 2.019181251525879
Validation loss: 2.2448658840630644

Epoch: 6| Step: 5
Training loss: 2.0554163455963135
Validation loss: 2.227702472799568

Epoch: 6| Step: 6
Training loss: 2.4931817054748535
Validation loss: 2.2362037653564126

Epoch: 6| Step: 7
Training loss: 2.950870990753174
Validation loss: 2.2265734057272635

Epoch: 6| Step: 8
Training loss: 2.1176183223724365
Validation loss: 2.2133438715370755

Epoch: 6| Step: 9
Training loss: 1.9702887535095215
Validation loss: 2.2221435270001813

Epoch: 6| Step: 10
Training loss: 2.4301629066467285
Validation loss: 2.223079612178187

Epoch: 6| Step: 11
Training loss: 2.5946176052093506
Validation loss: 2.248274698052355

Epoch: 6| Step: 12
Training loss: 2.694563865661621
Validation loss: 2.2545541024977163

Epoch: 6| Step: 13
Training loss: 2.3657712936401367
Validation loss: 2.247932687882454

Epoch: 326| Step: 0
Training loss: 1.5547866821289062
Validation loss: 2.27735903955275

Epoch: 6| Step: 1
Training loss: 2.151127338409424
Validation loss: 2.253518881336335

Epoch: 6| Step: 2
Training loss: 2.2107388973236084
Validation loss: 2.277613247594526

Epoch: 6| Step: 3
Training loss: 2.918679714202881
Validation loss: 2.2875242387094805

Epoch: 6| Step: 4
Training loss: 2.676724910736084
Validation loss: 2.2798496497574674

Epoch: 6| Step: 5
Training loss: 2.101572275161743
Validation loss: 2.2660291271824993

Epoch: 6| Step: 6
Training loss: 2.2795181274414062
Validation loss: 2.267703656227358

Epoch: 6| Step: 7
Training loss: 2.1188714504241943
Validation loss: 2.2562216020399526

Epoch: 6| Step: 8
Training loss: 2.567310333251953
Validation loss: 2.2272590924334783

Epoch: 6| Step: 9
Training loss: 2.42862606048584
Validation loss: 2.252367606727026

Epoch: 6| Step: 10
Training loss: 2.908027172088623
Validation loss: 2.2535445433790966

Epoch: 6| Step: 11
Training loss: 2.3305721282958984
Validation loss: 2.248149615462108

Epoch: 6| Step: 12
Training loss: 2.3173160552978516
Validation loss: 2.2497918169985534

Epoch: 6| Step: 13
Training loss: 3.2884955406188965
Validation loss: 2.236690875022642

Epoch: 327| Step: 0
Training loss: 1.9757640361785889
Validation loss: 2.253316407562584

Epoch: 6| Step: 1
Training loss: 1.8316044807434082
Validation loss: 2.247305508582823

Epoch: 6| Step: 2
Training loss: 2.517695903778076
Validation loss: 2.253211554660592

Epoch: 6| Step: 3
Training loss: 2.3290295600891113
Validation loss: 2.2596994651261197

Epoch: 6| Step: 4
Training loss: 2.4554443359375
Validation loss: 2.2430218163356987

Epoch: 6| Step: 5
Training loss: 2.2995901107788086
Validation loss: 2.2531486044647875

Epoch: 6| Step: 6
Training loss: 2.966926336288452
Validation loss: 2.247170400875871

Epoch: 6| Step: 7
Training loss: 2.279022693634033
Validation loss: 2.25148037428497

Epoch: 6| Step: 8
Training loss: 2.038054943084717
Validation loss: 2.2875681333644415

Epoch: 6| Step: 9
Training loss: 2.520113945007324
Validation loss: 2.306846369979202

Epoch: 6| Step: 10
Training loss: 2.797874927520752
Validation loss: 2.292491025822137

Epoch: 6| Step: 11
Training loss: 2.326772689819336
Validation loss: 2.2947434148480816

Epoch: 6| Step: 12
Training loss: 3.0853374004364014
Validation loss: 2.295426405886168

Epoch: 6| Step: 13
Training loss: 1.6675209999084473
Validation loss: 2.28539207930206

Epoch: 328| Step: 0
Training loss: 2.108583927154541
Validation loss: 2.262070853223083

Epoch: 6| Step: 1
Training loss: 2.5104618072509766
Validation loss: 2.2691772881374566

Epoch: 6| Step: 2
Training loss: 2.3261938095092773
Validation loss: 2.2481669584910073

Epoch: 6| Step: 3
Training loss: 3.3596906661987305
Validation loss: 2.2472063597812446

Epoch: 6| Step: 4
Training loss: 2.419957160949707
Validation loss: 2.244757595882621

Epoch: 6| Step: 5
Training loss: 1.850099802017212
Validation loss: 2.2522111887572915

Epoch: 6| Step: 6
Training loss: 2.3131508827209473
Validation loss: 2.2376713650200957

Epoch: 6| Step: 7
Training loss: 2.7063775062561035
Validation loss: 2.234914092607396

Epoch: 6| Step: 8
Training loss: 2.023555278778076
Validation loss: 2.2449620564778647

Epoch: 6| Step: 9
Training loss: 2.0282671451568604
Validation loss: 2.241665913212684

Epoch: 6| Step: 10
Training loss: 2.6442365646362305
Validation loss: 2.2476805897169214

Epoch: 6| Step: 11
Training loss: 2.520179510116577
Validation loss: 2.2382696956716557

Epoch: 6| Step: 12
Training loss: 2.3662171363830566
Validation loss: 2.2679664678471063

Epoch: 6| Step: 13
Training loss: 2.073744297027588
Validation loss: 2.2769518078014417

Epoch: 329| Step: 0
Training loss: 3.037055492401123
Validation loss: 2.2703229611919773

Epoch: 6| Step: 1
Training loss: 2.257429599761963
Validation loss: 2.2712952808667253

Epoch: 6| Step: 2
Training loss: 2.576737403869629
Validation loss: 2.2874892475784465

Epoch: 6| Step: 3
Training loss: 2.5188074111938477
Validation loss: 2.2782363173782185

Epoch: 6| Step: 4
Training loss: 2.1587343215942383
Validation loss: 2.270935853322347

Epoch: 6| Step: 5
Training loss: 2.6188037395477295
Validation loss: 2.280685081276842

Epoch: 6| Step: 6
Training loss: 2.4245896339416504
Validation loss: 2.264966308429677

Epoch: 6| Step: 7
Training loss: 2.3725943565368652
Validation loss: 2.245343908186882

Epoch: 6| Step: 8
Training loss: 1.9289569854736328
Validation loss: 2.2568253906824256

Epoch: 6| Step: 9
Training loss: 2.1177685260772705
Validation loss: 2.2421150079337497

Epoch: 6| Step: 10
Training loss: 2.4283273220062256
Validation loss: 2.2553311086470083

Epoch: 6| Step: 11
Training loss: 2.37304949760437
Validation loss: 2.2350865358947427

Epoch: 6| Step: 12
Training loss: 2.566380739212036
Validation loss: 2.238257492742231

Epoch: 6| Step: 13
Training loss: 1.713938593864441
Validation loss: 2.2482147011705624

Epoch: 330| Step: 0
Training loss: 2.2636280059814453
Validation loss: 2.2460861564964376

Epoch: 6| Step: 1
Training loss: 1.8139715194702148
Validation loss: 2.2444096701119536

Epoch: 6| Step: 2
Training loss: 2.0995771884918213
Validation loss: 2.2453625561088644

Epoch: 6| Step: 3
Training loss: 1.9458775520324707
Validation loss: 2.255830349460725

Epoch: 6| Step: 4
Training loss: 2.7800824642181396
Validation loss: 2.2981781574987594

Epoch: 6| Step: 5
Training loss: 2.245929002761841
Validation loss: 2.2970534242609495

Epoch: 6| Step: 6
Training loss: 2.3150463104248047
Validation loss: 2.3072462620273715

Epoch: 6| Step: 7
Training loss: 2.7730016708374023
Validation loss: 2.307783321667743

Epoch: 6| Step: 8
Training loss: 2.5956666469573975
Validation loss: 2.3041767843307985

Epoch: 6| Step: 9
Training loss: 2.4460272789001465
Validation loss: 2.2774150217733076

Epoch: 6| Step: 10
Training loss: 3.316589117050171
Validation loss: 2.262497619916034

Epoch: 6| Step: 11
Training loss: 2.023303985595703
Validation loss: 2.2350919041582333

Epoch: 6| Step: 12
Training loss: 2.0868587493896484
Validation loss: 2.2305787891469975

Epoch: 6| Step: 13
Training loss: 3.0898513793945312
Validation loss: 2.1934788534718175

Epoch: 331| Step: 0
Training loss: 2.3908181190490723
Validation loss: 2.228550649458362

Epoch: 6| Step: 1
Training loss: 2.703104019165039
Validation loss: 2.2248790251311434

Epoch: 6| Step: 2
Training loss: 2.505131244659424
Validation loss: 2.242499182301183

Epoch: 6| Step: 3
Training loss: 2.225431203842163
Validation loss: 2.2693345162176315

Epoch: 6| Step: 4
Training loss: 2.5656566619873047
Validation loss: 2.245648514839911

Epoch: 6| Step: 5
Training loss: 2.894759178161621
Validation loss: 2.254140776972617

Epoch: 6| Step: 6
Training loss: 2.488020658493042
Validation loss: 2.2383075491074593

Epoch: 6| Step: 7
Training loss: 2.317971706390381
Validation loss: 2.2176931340207338

Epoch: 6| Step: 8
Training loss: 2.287349224090576
Validation loss: 2.2250086389562136

Epoch: 6| Step: 9
Training loss: 2.530302047729492
Validation loss: 2.221366472141717

Epoch: 6| Step: 10
Training loss: 2.3632924556732178
Validation loss: 2.209162392923909

Epoch: 6| Step: 11
Training loss: 2.449575185775757
Validation loss: 2.211070975949687

Epoch: 6| Step: 12
Training loss: 2.1980690956115723
Validation loss: 2.2113632796913065

Epoch: 6| Step: 13
Training loss: 2.002324104309082
Validation loss: 2.219825244718982

Epoch: 332| Step: 0
Training loss: 2.561727523803711
Validation loss: 2.227490081582018

Epoch: 6| Step: 1
Training loss: 2.9254298210144043
Validation loss: 2.2263966273236018

Epoch: 6| Step: 2
Training loss: 1.273667573928833
Validation loss: 2.233208079491892

Epoch: 6| Step: 3
Training loss: 2.3475704193115234
Validation loss: 2.2445023457209268

Epoch: 6| Step: 4
Training loss: 1.799947738647461
Validation loss: 2.265877616020941

Epoch: 6| Step: 5
Training loss: 1.810522198677063
Validation loss: 2.266450571757491

Epoch: 6| Step: 6
Training loss: 2.7521615028381348
Validation loss: 2.287869252184386

Epoch: 6| Step: 7
Training loss: 3.0115323066711426
Validation loss: 2.2935779863788235

Epoch: 6| Step: 8
Training loss: 2.0644311904907227
Validation loss: 2.271405020067769

Epoch: 6| Step: 9
Training loss: 2.3687620162963867
Validation loss: 2.281320923118181

Epoch: 6| Step: 10
Training loss: 2.8703560829162598
Validation loss: 2.2685745736604095

Epoch: 6| Step: 11
Training loss: 2.870920419692993
Validation loss: 2.2639623124112367

Epoch: 6| Step: 12
Training loss: 2.636451005935669
Validation loss: 2.2701508896325224

Epoch: 6| Step: 13
Training loss: 2.05176043510437
Validation loss: 2.2557978809520765

Epoch: 333| Step: 0
Training loss: 1.9938335418701172
Validation loss: 2.2380704213214178

Epoch: 6| Step: 1
Training loss: 2.3444418907165527
Validation loss: 2.2471939581696705

Epoch: 6| Step: 2
Training loss: 2.3093128204345703
Validation loss: 2.236740340468704

Epoch: 6| Step: 3
Training loss: 1.7487318515777588
Validation loss: 2.2313116699136715

Epoch: 6| Step: 4
Training loss: 2.723371744155884
Validation loss: 2.2417620715274604

Epoch: 6| Step: 5
Training loss: 1.8450061082839966
Validation loss: 2.2370306368797057

Epoch: 6| Step: 6
Training loss: 2.7404398918151855
Validation loss: 2.2363461550845893

Epoch: 6| Step: 7
Training loss: 2.050534725189209
Validation loss: 2.251096430645194

Epoch: 6| Step: 8
Training loss: 2.673759698867798
Validation loss: 2.2359680450090798

Epoch: 6| Step: 9
Training loss: 2.6015686988830566
Validation loss: 2.2434393616132837

Epoch: 6| Step: 10
Training loss: 2.246006488800049
Validation loss: 2.2441297808001117

Epoch: 6| Step: 11
Training loss: 2.4796485900878906
Validation loss: 2.268101415326518

Epoch: 6| Step: 12
Training loss: 2.9806928634643555
Validation loss: 2.2476835917401057

Epoch: 6| Step: 13
Training loss: 2.569593667984009
Validation loss: 2.246235816709457

Epoch: 334| Step: 0
Training loss: 2.7939815521240234
Validation loss: 2.261868792195474

Epoch: 6| Step: 1
Training loss: 2.1221797466278076
Validation loss: 2.2573975875813472

Epoch: 6| Step: 2
Training loss: 3.261148452758789
Validation loss: 2.254977810767389

Epoch: 6| Step: 3
Training loss: 3.0493106842041016
Validation loss: 2.2786730233059136

Epoch: 6| Step: 4
Training loss: 2.0151796340942383
Validation loss: 2.283467618368005

Epoch: 6| Step: 5
Training loss: 2.810356616973877
Validation loss: 2.2860734052555536

Epoch: 6| Step: 6
Training loss: 1.6404244899749756
Validation loss: 2.2932435927852506

Epoch: 6| Step: 7
Training loss: 2.40629506111145
Validation loss: 2.273080902714883

Epoch: 6| Step: 8
Training loss: 1.7618329524993896
Validation loss: 2.2627984169990785

Epoch: 6| Step: 9
Training loss: 2.2224783897399902
Validation loss: 2.255211330229236

Epoch: 6| Step: 10
Training loss: 2.9393129348754883
Validation loss: 2.2444436165594284

Epoch: 6| Step: 11
Training loss: 2.3226256370544434
Validation loss: 2.2300209076173845

Epoch: 6| Step: 12
Training loss: 1.61186945438385
Validation loss: 2.228022577942059

Epoch: 6| Step: 13
Training loss: 2.292147636413574
Validation loss: 2.2368265300668697

Epoch: 335| Step: 0
Training loss: 1.6057796478271484
Validation loss: 2.2234241859887236

Epoch: 6| Step: 1
Training loss: 2.284345865249634
Validation loss: 2.233187355020995

Epoch: 6| Step: 2
Training loss: 2.348485231399536
Validation loss: 2.2272621957204675

Epoch: 6| Step: 3
Training loss: 2.652505397796631
Validation loss: 2.22517115710884

Epoch: 6| Step: 4
Training loss: 2.786864757537842
Validation loss: 2.2357722379828013

Epoch: 6| Step: 5
Training loss: 2.882004499435425
Validation loss: 2.229766345793201

Epoch: 6| Step: 6
Training loss: 3.1419687271118164
Validation loss: 2.2179670385135117

Epoch: 6| Step: 7
Training loss: 2.997354030609131
Validation loss: 2.2188960147160355

Epoch: 6| Step: 8
Training loss: 2.2502212524414062
Validation loss: 2.2235234642541535

Epoch: 6| Step: 9
Training loss: 2.1720454692840576
Validation loss: 2.225918259671939

Epoch: 6| Step: 10
Training loss: 2.1154379844665527
Validation loss: 2.222689877274216

Epoch: 6| Step: 11
Training loss: 1.8094955682754517
Validation loss: 2.226310819707891

Epoch: 6| Step: 12
Training loss: 1.9941328763961792
Validation loss: 2.2175702074522614

Epoch: 6| Step: 13
Training loss: 2.0405516624450684
Validation loss: 2.216973799531178

Epoch: 336| Step: 0
Training loss: 2.23380184173584
Validation loss: 2.2437232079044467

Epoch: 6| Step: 1
Training loss: 2.0569958686828613
Validation loss: 2.2526384912511355

Epoch: 6| Step: 2
Training loss: 2.3563902378082275
Validation loss: 2.2156199178388043

Epoch: 6| Step: 3
Training loss: 1.9672276973724365
Validation loss: 2.2458127185862553

Epoch: 6| Step: 4
Training loss: 2.5049190521240234
Validation loss: 2.233243833306015

Epoch: 6| Step: 5
Training loss: 1.8745801448822021
Validation loss: 2.2324356571320565

Epoch: 6| Step: 6
Training loss: 2.748652219772339
Validation loss: 2.228185474231679

Epoch: 6| Step: 7
Training loss: 1.546154499053955
Validation loss: 2.2352315277181645

Epoch: 6| Step: 8
Training loss: 2.6627540588378906
Validation loss: 2.2280104724309777

Epoch: 6| Step: 9
Training loss: 2.5230002403259277
Validation loss: 2.2399014657543552

Epoch: 6| Step: 10
Training loss: 2.841902256011963
Validation loss: 2.247715054019805

Epoch: 6| Step: 11
Training loss: 2.6572132110595703
Validation loss: 2.2495778350419897

Epoch: 6| Step: 12
Training loss: 3.055058479309082
Validation loss: 2.262295320469846

Epoch: 6| Step: 13
Training loss: 2.011113405227661
Validation loss: 2.235083326216667

Epoch: 337| Step: 0
Training loss: 2.23313045501709
Validation loss: 2.23202762808851

Epoch: 6| Step: 1
Training loss: 2.236199378967285
Validation loss: 2.238251829660067

Epoch: 6| Step: 2
Training loss: 2.4420323371887207
Validation loss: 2.24653338104166

Epoch: 6| Step: 3
Training loss: 2.929514169692993
Validation loss: 2.2540628499882196

Epoch: 6| Step: 4
Training loss: 1.920387625694275
Validation loss: 2.2686282434771137

Epoch: 6| Step: 5
Training loss: 3.008856773376465
Validation loss: 2.2575443149894796

Epoch: 6| Step: 6
Training loss: 2.5721888542175293
Validation loss: 2.2449938199853383

Epoch: 6| Step: 7
Training loss: 1.8588120937347412
Validation loss: 2.2695558430046163

Epoch: 6| Step: 8
Training loss: 2.693589687347412
Validation loss: 2.2576264463445193

Epoch: 6| Step: 9
Training loss: 2.0136609077453613
Validation loss: 2.230831205203969

Epoch: 6| Step: 10
Training loss: 2.495584487915039
Validation loss: 2.2477881767416514

Epoch: 6| Step: 11
Training loss: 2.689760684967041
Validation loss: 2.238334030233404

Epoch: 6| Step: 12
Training loss: 2.0144906044006348
Validation loss: 2.226121546119772

Epoch: 6| Step: 13
Training loss: 1.735985279083252
Validation loss: 2.206884766137728

Epoch: 338| Step: 0
Training loss: 2.7748119831085205
Validation loss: 2.212160313001243

Epoch: 6| Step: 1
Training loss: 2.0195374488830566
Validation loss: 2.220673479059691

Epoch: 6| Step: 2
Training loss: 2.843369722366333
Validation loss: 2.217654517901841

Epoch: 6| Step: 3
Training loss: 1.9682530164718628
Validation loss: 2.2248816156900055

Epoch: 6| Step: 4
Training loss: 3.4220471382141113
Validation loss: 2.2189161290404615

Epoch: 6| Step: 5
Training loss: 2.4553472995758057
Validation loss: 2.214747605785247

Epoch: 6| Step: 6
Training loss: 2.523010015487671
Validation loss: 2.2160374708073114

Epoch: 6| Step: 7
Training loss: 2.2435338497161865
Validation loss: 2.2183968367115146

Epoch: 6| Step: 8
Training loss: 2.162141799926758
Validation loss: 2.2149476902459257

Epoch: 6| Step: 9
Training loss: 1.418066143989563
Validation loss: 2.220076914756529

Epoch: 6| Step: 10
Training loss: 2.76806378364563
Validation loss: 2.226335563967305

Epoch: 6| Step: 11
Training loss: 1.5773584842681885
Validation loss: 2.2164234807414394

Epoch: 6| Step: 12
Training loss: 2.493082046508789
Validation loss: 2.234378069959661

Epoch: 6| Step: 13
Training loss: 2.8606112003326416
Validation loss: 2.2375581777223976

Epoch: 339| Step: 0
Training loss: 2.324808120727539
Validation loss: 2.237446447854401

Epoch: 6| Step: 1
Training loss: 3.3569297790527344
Validation loss: 2.223417417977446

Epoch: 6| Step: 2
Training loss: 2.61899733543396
Validation loss: 2.232190085995582

Epoch: 6| Step: 3
Training loss: 2.4862821102142334
Validation loss: 2.2355907142803235

Epoch: 6| Step: 4
Training loss: 2.524465560913086
Validation loss: 2.2283723213339366

Epoch: 6| Step: 5
Training loss: 2.1404738426208496
Validation loss: 2.224328746077835

Epoch: 6| Step: 6
Training loss: 2.5929808616638184
Validation loss: 2.2205113774986676

Epoch: 6| Step: 7
Training loss: 2.1426262855529785
Validation loss: 2.2285211137546006

Epoch: 6| Step: 8
Training loss: 2.533329486846924
Validation loss: 2.20915344197263

Epoch: 6| Step: 9
Training loss: 2.3190183639526367
Validation loss: 2.2257828199735252

Epoch: 6| Step: 10
Training loss: 1.9762382507324219
Validation loss: 2.2053401495820735

Epoch: 6| Step: 11
Training loss: 2.484506130218506
Validation loss: 2.2191375096639

Epoch: 6| Step: 12
Training loss: 1.6626659631729126
Validation loss: 2.2173738659069104

Epoch: 6| Step: 13
Training loss: 2.1552979946136475
Validation loss: 2.230270278069281

Epoch: 340| Step: 0
Training loss: 2.3370985984802246
Validation loss: 2.2230720571292344

Epoch: 6| Step: 1
Training loss: 2.408446788787842
Validation loss: 2.242132281744352

Epoch: 6| Step: 2
Training loss: 2.2680134773254395
Validation loss: 2.253740672142275

Epoch: 6| Step: 3
Training loss: 2.4000773429870605
Validation loss: 2.2541766243596233

Epoch: 6| Step: 4
Training loss: 3.1411759853363037
Validation loss: 2.2465204372200915

Epoch: 6| Step: 5
Training loss: 1.9881420135498047
Validation loss: 2.2620374259128364

Epoch: 6| Step: 6
Training loss: 2.470219612121582
Validation loss: 2.2396871428335867

Epoch: 6| Step: 7
Training loss: 2.328901767730713
Validation loss: 2.2443442524120374

Epoch: 6| Step: 8
Training loss: 1.6083641052246094
Validation loss: 2.241582250082365

Epoch: 6| Step: 9
Training loss: 2.3701419830322266
Validation loss: 2.2359953567545903

Epoch: 6| Step: 10
Training loss: 2.3486428260803223
Validation loss: 2.2578257104401946

Epoch: 6| Step: 11
Training loss: 2.379760265350342
Validation loss: 2.2419421595911824

Epoch: 6| Step: 12
Training loss: 2.899350643157959
Validation loss: 2.238179931076624

Epoch: 6| Step: 13
Training loss: 1.9403108358383179
Validation loss: 2.234102063281562

Epoch: 341| Step: 0
Training loss: 1.9610093832015991
Validation loss: 2.2407338619232178

Epoch: 6| Step: 1
Training loss: 2.8627665042877197
Validation loss: 2.243878182544503

Epoch: 6| Step: 2
Training loss: 2.1721906661987305
Validation loss: 2.252729464602727

Epoch: 6| Step: 3
Training loss: 1.8082013130187988
Validation loss: 2.2488899359139065

Epoch: 6| Step: 4
Training loss: 2.996462345123291
Validation loss: 2.2736539404879332

Epoch: 6| Step: 5
Training loss: 2.445688247680664
Validation loss: 2.2628840797690937

Epoch: 6| Step: 6
Training loss: 2.329662799835205
Validation loss: 2.2411605670887935

Epoch: 6| Step: 7
Training loss: 2.103144645690918
Validation loss: 2.2528354583248014

Epoch: 6| Step: 8
Training loss: 1.626579999923706
Validation loss: 2.240890214520116

Epoch: 6| Step: 9
Training loss: 3.0294480323791504
Validation loss: 2.248140997784112

Epoch: 6| Step: 10
Training loss: 2.507835865020752
Validation loss: 2.2450509866078696

Epoch: 6| Step: 11
Training loss: 2.5014219284057617
Validation loss: 2.237679354606136

Epoch: 6| Step: 12
Training loss: 2.4654898643493652
Validation loss: 2.213193319177115

Epoch: 6| Step: 13
Training loss: 2.144778251647949
Validation loss: 2.226031841770295

Epoch: 342| Step: 0
Training loss: 2.817471981048584
Validation loss: 2.2146831532960296

Epoch: 6| Step: 1
Training loss: 2.3112599849700928
Validation loss: 2.206530063383041

Epoch: 6| Step: 2
Training loss: 3.3560314178466797
Validation loss: 2.226300101126394

Epoch: 6| Step: 3
Training loss: 1.6119261980056763
Validation loss: 2.2067318013919297

Epoch: 6| Step: 4
Training loss: 2.1332507133483887
Validation loss: 2.2135016277272213

Epoch: 6| Step: 5
Training loss: 1.8656288385391235
Validation loss: 2.213849808580132

Epoch: 6| Step: 6
Training loss: 2.5402672290802
Validation loss: 2.2092892713444208

Epoch: 6| Step: 7
Training loss: 2.1112189292907715
Validation loss: 2.21970005958311

Epoch: 6| Step: 8
Training loss: 2.1370275020599365
Validation loss: 2.2118590595901653

Epoch: 6| Step: 9
Training loss: 2.128024101257324
Validation loss: 2.2023428499057727

Epoch: 6| Step: 10
Training loss: 3.4080519676208496
Validation loss: 2.2120279855625604

Epoch: 6| Step: 11
Training loss: 2.0360240936279297
Validation loss: 2.2234423698917514

Epoch: 6| Step: 12
Training loss: 1.834776759147644
Validation loss: 2.217363816435619

Epoch: 6| Step: 13
Training loss: 3.075955867767334
Validation loss: 2.2387838902011996

Epoch: 343| Step: 0
Training loss: 2.9194204807281494
Validation loss: 2.25105183867998

Epoch: 6| Step: 1
Training loss: 2.515719413757324
Validation loss: 2.2625446883581017

Epoch: 6| Step: 2
Training loss: 3.395951271057129
Validation loss: 2.2561896167775637

Epoch: 6| Step: 3
Training loss: 2.775158166885376
Validation loss: 2.2515936205464024

Epoch: 6| Step: 4
Training loss: 2.492461919784546
Validation loss: 2.2566496120986117

Epoch: 6| Step: 5
Training loss: 2.441885471343994
Validation loss: 2.2459987748053765

Epoch: 6| Step: 6
Training loss: 1.5905617475509644
Validation loss: 2.236235923664544

Epoch: 6| Step: 7
Training loss: 2.4274768829345703
Validation loss: 2.2171798598381782

Epoch: 6| Step: 8
Training loss: 2.3596272468566895
Validation loss: 2.1999126095925607

Epoch: 6| Step: 9
Training loss: 2.1525468826293945
Validation loss: 2.2018113943838302

Epoch: 6| Step: 10
Training loss: 2.034708023071289
Validation loss: 2.2160361928324543

Epoch: 6| Step: 11
Training loss: 2.0305397510528564
Validation loss: 2.2101826719058457

Epoch: 6| Step: 12
Training loss: 1.8941004276275635
Validation loss: 2.219960751072053

Epoch: 6| Step: 13
Training loss: 1.8454984426498413
Validation loss: 2.2380862364204983

Epoch: 344| Step: 0
Training loss: 2.6529345512390137
Validation loss: 2.2327473676332863

Epoch: 6| Step: 1
Training loss: 2.015209674835205
Validation loss: 2.226849017604705

Epoch: 6| Step: 2
Training loss: 2.16090989112854
Validation loss: 2.233673664831346

Epoch: 6| Step: 3
Training loss: 2.5008955001831055
Validation loss: 2.232873724352929

Epoch: 6| Step: 4
Training loss: 2.655973434448242
Validation loss: 2.2290709608344623

Epoch: 6| Step: 5
Training loss: 2.042759895324707
Validation loss: 2.2206523110789638

Epoch: 6| Step: 6
Training loss: 2.018012523651123
Validation loss: 2.216242468485268

Epoch: 6| Step: 7
Training loss: 2.8351292610168457
Validation loss: 2.204953178282707

Epoch: 6| Step: 8
Training loss: 2.3002383708953857
Validation loss: 2.2335639948486

Epoch: 6| Step: 9
Training loss: 2.7956321239471436
Validation loss: 2.226954734453591

Epoch: 6| Step: 10
Training loss: 1.7854485511779785
Validation loss: 2.2238659474157516

Epoch: 6| Step: 11
Training loss: 2.270315408706665
Validation loss: 2.2000359001980034

Epoch: 6| Step: 12
Training loss: 2.4003095626831055
Validation loss: 2.2083490356322257

Epoch: 6| Step: 13
Training loss: 2.6561288833618164
Validation loss: 2.205268362516998

Epoch: 345| Step: 0
Training loss: 2.4792895317077637
Validation loss: 2.203371622229135

Epoch: 6| Step: 1
Training loss: 2.60357666015625
Validation loss: 2.2333900723406064

Epoch: 6| Step: 2
Training loss: 2.379793167114258
Validation loss: 2.2041790498200284

Epoch: 6| Step: 3
Training loss: 1.9589110612869263
Validation loss: 2.2282580047525387

Epoch: 6| Step: 4
Training loss: 2.5035367012023926
Validation loss: 2.21187069595501

Epoch: 6| Step: 5
Training loss: 2.2155661582946777
Validation loss: 2.2338343512627388

Epoch: 6| Step: 6
Training loss: 1.9640812873840332
Validation loss: 2.213783912761237

Epoch: 6| Step: 7
Training loss: 1.9626903533935547
Validation loss: 2.232323661927254

Epoch: 6| Step: 8
Training loss: 2.0973663330078125
Validation loss: 2.22604186560518

Epoch: 6| Step: 9
Training loss: 2.306434154510498
Validation loss: 2.22858497147919

Epoch: 6| Step: 10
Training loss: 2.589402437210083
Validation loss: 2.2472273431798464

Epoch: 6| Step: 11
Training loss: 2.9881820678710938
Validation loss: 2.2171436561051237

Epoch: 6| Step: 12
Training loss: 2.41733455657959
Validation loss: 2.2178889628379577

Epoch: 6| Step: 13
Training loss: 2.644190549850464
Validation loss: 2.2261215794471

Epoch: 346| Step: 0
Training loss: 3.131558418273926
Validation loss: 2.2534138566704205

Epoch: 6| Step: 1
Training loss: 1.9669239521026611
Validation loss: 2.2438265469766434

Epoch: 6| Step: 2
Training loss: 2.180415153503418
Validation loss: 2.2413811760564006

Epoch: 6| Step: 3
Training loss: 2.4206106662750244
Validation loss: 2.2364559942676174

Epoch: 6| Step: 4
Training loss: 1.6565845012664795
Validation loss: 2.2309853107698503

Epoch: 6| Step: 5
Training loss: 2.4387965202331543
Validation loss: 2.2602563724722913

Epoch: 6| Step: 6
Training loss: 2.8137195110321045
Validation loss: 2.252877422558364

Epoch: 6| Step: 7
Training loss: 2.6375205516815186
Validation loss: 2.2427181864297516

Epoch: 6| Step: 8
Training loss: 2.1624562740325928
Validation loss: 2.2545629752579557

Epoch: 6| Step: 9
Training loss: 3.2041497230529785
Validation loss: 2.2563587645048737

Epoch: 6| Step: 10
Training loss: 2.5630152225494385
Validation loss: 2.2704731879695768

Epoch: 6| Step: 11
Training loss: 1.9982342720031738
Validation loss: 2.2541120257428897

Epoch: 6| Step: 12
Training loss: 1.5962765216827393
Validation loss: 2.228968717718637

Epoch: 6| Step: 13
Training loss: 2.1477105617523193
Validation loss: 2.224825354032619

Epoch: 347| Step: 0
Training loss: 2.555746555328369
Validation loss: 2.19775071964469

Epoch: 6| Step: 1
Training loss: 1.8894609212875366
Validation loss: 2.2003780077862483

Epoch: 6| Step: 2
Training loss: 2.3952715396881104
Validation loss: 2.2163998132110923

Epoch: 6| Step: 3
Training loss: 2.7734880447387695
Validation loss: 2.216274538347798

Epoch: 6| Step: 4
Training loss: 2.639991044998169
Validation loss: 2.2220594831692275

Epoch: 6| Step: 5
Training loss: 2.647437334060669
Validation loss: 2.216982541545745

Epoch: 6| Step: 6
Training loss: 2.731588840484619
Validation loss: 2.21607728158274

Epoch: 6| Step: 7
Training loss: 1.6725513935089111
Validation loss: 2.2009414126796107

Epoch: 6| Step: 8
Training loss: 2.0233654975891113
Validation loss: 2.2216403586890108

Epoch: 6| Step: 9
Training loss: 2.226461172103882
Validation loss: 2.2085904767436366

Epoch: 6| Step: 10
Training loss: 3.3355488777160645
Validation loss: 2.2604798680992535

Epoch: 6| Step: 11
Training loss: 2.0324513912200928
Validation loss: 2.2528569749606553

Epoch: 6| Step: 12
Training loss: 1.7814232110977173
Validation loss: 2.2383812524939097

Epoch: 6| Step: 13
Training loss: 2.5029919147491455
Validation loss: 2.220711241486252

Epoch: 348| Step: 0
Training loss: 2.2265403270721436
Validation loss: 2.239869699683241

Epoch: 6| Step: 1
Training loss: 2.0131032466888428
Validation loss: 2.2304797967274985

Epoch: 6| Step: 2
Training loss: 2.5446057319641113
Validation loss: 2.2214762036518385

Epoch: 6| Step: 3
Training loss: 2.0644381046295166
Validation loss: 2.2138214188237346

Epoch: 6| Step: 4
Training loss: 2.1644725799560547
Validation loss: 2.209658488150566

Epoch: 6| Step: 5
Training loss: 2.9864249229431152
Validation loss: 2.2197713493019022

Epoch: 6| Step: 6
Training loss: 2.69978404045105
Validation loss: 2.207345660014819

Epoch: 6| Step: 7
Training loss: 2.7083067893981934
Validation loss: 2.2156915100671912

Epoch: 6| Step: 8
Training loss: 1.879425287246704
Validation loss: 2.2031657042041903

Epoch: 6| Step: 9
Training loss: 2.3849143981933594
Validation loss: 2.200509504605365

Epoch: 6| Step: 10
Training loss: 3.265207052230835
Validation loss: 2.2060093751517673

Epoch: 6| Step: 11
Training loss: 2.278144598007202
Validation loss: 2.212988740654402

Epoch: 6| Step: 12
Training loss: 1.9557640552520752
Validation loss: 2.195445445276076

Epoch: 6| Step: 13
Training loss: 1.508448839187622
Validation loss: 2.223884585083172

Epoch: 349| Step: 0
Training loss: 2.128462314605713
Validation loss: 2.215424459467652

Epoch: 6| Step: 1
Training loss: 3.1195266246795654
Validation loss: 2.236501773198446

Epoch: 6| Step: 2
Training loss: 2.125030994415283
Validation loss: 2.2503420845154793

Epoch: 6| Step: 3
Training loss: 2.451977014541626
Validation loss: 2.251123064307756

Epoch: 6| Step: 4
Training loss: 1.6055693626403809
Validation loss: 2.250514006101957

Epoch: 6| Step: 5
Training loss: 2.6426279544830322
Validation loss: 2.2834187681956957

Epoch: 6| Step: 6
Training loss: 2.0396337509155273
Validation loss: 2.2826387600232194

Epoch: 6| Step: 7
Training loss: 1.9210542440414429
Validation loss: 2.27505934879344

Epoch: 6| Step: 8
Training loss: 2.152991533279419
Validation loss: 2.2520007702612106

Epoch: 6| Step: 9
Training loss: 2.4771220684051514
Validation loss: 2.2583062789773427

Epoch: 6| Step: 10
Training loss: 2.5793747901916504
Validation loss: 2.2373126732405795

Epoch: 6| Step: 11
Training loss: 2.382683277130127
Validation loss: 2.2312526215789137

Epoch: 6| Step: 12
Training loss: 3.277120351791382
Validation loss: 2.2224016099847774

Epoch: 6| Step: 13
Training loss: 1.8651235103607178
Validation loss: 2.202655610217843

Epoch: 350| Step: 0
Training loss: 2.7484166622161865
Validation loss: 2.2035179830366567

Epoch: 6| Step: 1
Training loss: 2.8843295574188232
Validation loss: 2.195600345570554

Epoch: 6| Step: 2
Training loss: 2.686876058578491
Validation loss: 2.192374557577154

Epoch: 6| Step: 3
Training loss: 2.5793800354003906
Validation loss: 2.1859892555462417

Epoch: 6| Step: 4
Training loss: 2.8295931816101074
Validation loss: 2.1897963080354916

Epoch: 6| Step: 5
Training loss: 1.8215135335922241
Validation loss: 2.2065251924658336

Epoch: 6| Step: 6
Training loss: 2.4261910915374756
Validation loss: 2.2337714472124652

Epoch: 6| Step: 7
Training loss: 1.6136574745178223
Validation loss: 2.2415972204618555

Epoch: 6| Step: 8
Training loss: 2.0328660011291504
Validation loss: 2.231018574007096

Epoch: 6| Step: 9
Training loss: 2.6544933319091797
Validation loss: 2.2533374960704515

Epoch: 6| Step: 10
Training loss: 1.6642935276031494
Validation loss: 2.250901840066397

Epoch: 6| Step: 11
Training loss: 2.2444992065429688
Validation loss: 2.240985926761422

Epoch: 6| Step: 12
Training loss: 2.3507800102233887
Validation loss: 2.2524364379144486

Epoch: 6| Step: 13
Training loss: 2.8278703689575195
Validation loss: 2.2194934660388577

Epoch: 351| Step: 0
Training loss: 2.102932929992676
Validation loss: 2.219821983768094

Epoch: 6| Step: 1
Training loss: 2.526646137237549
Validation loss: 2.217935582642914

Epoch: 6| Step: 2
Training loss: 2.9056708812713623
Validation loss: 2.2246611618226573

Epoch: 6| Step: 3
Training loss: 2.317737102508545
Validation loss: 2.1995659182148595

Epoch: 6| Step: 4
Training loss: 1.451841115951538
Validation loss: 2.2113665021875852

Epoch: 6| Step: 5
Training loss: 2.0424275398254395
Validation loss: 2.2259761159138014

Epoch: 6| Step: 6
Training loss: 2.5167901515960693
Validation loss: 2.209996238831551

Epoch: 6| Step: 7
Training loss: 2.0406298637390137
Validation loss: 2.2122060893684306

Epoch: 6| Step: 8
Training loss: 2.548464059829712
Validation loss: 2.215622691697972

Epoch: 6| Step: 9
Training loss: 2.4764060974121094
Validation loss: 2.2178509722473803

Epoch: 6| Step: 10
Training loss: 2.7099273204803467
Validation loss: 2.232013738283547

Epoch: 6| Step: 11
Training loss: 2.3059778213500977
Validation loss: 2.248441952531056

Epoch: 6| Step: 12
Training loss: 2.209712505340576
Validation loss: 2.2441017396988405

Epoch: 6| Step: 13
Training loss: 3.2698428630828857
Validation loss: 2.244701288079703

Epoch: 352| Step: 0
Training loss: 1.943312644958496
Validation loss: 2.252964909358691

Epoch: 6| Step: 1
Training loss: 1.616827130317688
Validation loss: 2.235265367774553

Epoch: 6| Step: 2
Training loss: 2.312258243560791
Validation loss: 2.2198287620339343

Epoch: 6| Step: 3
Training loss: 2.6499900817871094
Validation loss: 2.224055444040606

Epoch: 6| Step: 4
Training loss: 2.955883026123047
Validation loss: 2.208558435081154

Epoch: 6| Step: 5
Training loss: 1.8858706951141357
Validation loss: 2.196814001247447

Epoch: 6| Step: 6
Training loss: 2.5888900756835938
Validation loss: 2.188956663172732

Epoch: 6| Step: 7
Training loss: 2.905884265899658
Validation loss: 2.193514157367009

Epoch: 6| Step: 8
Training loss: 2.316498041152954
Validation loss: 2.184990144545032

Epoch: 6| Step: 9
Training loss: 2.469907283782959
Validation loss: 2.1876508382058915

Epoch: 6| Step: 10
Training loss: 2.1791789531707764
Validation loss: 2.1929920565697456

Epoch: 6| Step: 11
Training loss: 3.222663402557373
Validation loss: 2.196271491307084

Epoch: 6| Step: 12
Training loss: 1.6170217990875244
Validation loss: 2.1861039592373754

Epoch: 6| Step: 13
Training loss: 2.2611541748046875
Validation loss: 2.193151979036229

Epoch: 353| Step: 0
Training loss: 1.9984526634216309
Validation loss: 2.1893480144521242

Epoch: 6| Step: 1
Training loss: 1.3758469820022583
Validation loss: 2.1940210403934604

Epoch: 6| Step: 2
Training loss: 3.0129568576812744
Validation loss: 2.1978602973363732

Epoch: 6| Step: 3
Training loss: 2.403096914291382
Validation loss: 2.193138991632769

Epoch: 6| Step: 4
Training loss: 1.783144235610962
Validation loss: 2.20301224852121

Epoch: 6| Step: 5
Training loss: 2.5515055656433105
Validation loss: 2.217520192105283

Epoch: 6| Step: 6
Training loss: 2.856386661529541
Validation loss: 2.2245953185583955

Epoch: 6| Step: 7
Training loss: 2.490480899810791
Validation loss: 2.2378670374552407

Epoch: 6| Step: 8
Training loss: 2.600311756134033
Validation loss: 2.2528271726382676

Epoch: 6| Step: 9
Training loss: 2.7158501148223877
Validation loss: 2.2579792648233394

Epoch: 6| Step: 10
Training loss: 2.318638801574707
Validation loss: 2.273600547544418

Epoch: 6| Step: 11
Training loss: 2.6278512477874756
Validation loss: 2.2750540779482935

Epoch: 6| Step: 12
Training loss: 1.8316601514816284
Validation loss: 2.2466679542295394

Epoch: 6| Step: 13
Training loss: 2.432558298110962
Validation loss: 2.249157485141549

Epoch: 354| Step: 0
Training loss: 2.0949361324310303
Validation loss: 2.238509529380388

Epoch: 6| Step: 1
Training loss: 2.1546831130981445
Validation loss: 2.2395756603569112

Epoch: 6| Step: 2
Training loss: 2.430940628051758
Validation loss: 2.2266936212457638

Epoch: 6| Step: 3
Training loss: 2.536642551422119
Validation loss: 2.2291593013271207

Epoch: 6| Step: 4
Training loss: 1.9775736331939697
Validation loss: 2.220629853586997

Epoch: 6| Step: 5
Training loss: 2.595292568206787
Validation loss: 2.216272323362289

Epoch: 6| Step: 6
Training loss: 1.9794940948486328
Validation loss: 2.2092001476595478

Epoch: 6| Step: 7
Training loss: 2.840360164642334
Validation loss: 2.2097024507420038

Epoch: 6| Step: 8
Training loss: 2.614990711212158
Validation loss: 2.2073429938285583

Epoch: 6| Step: 9
Training loss: 2.38374662399292
Validation loss: 2.1851878858381704

Epoch: 6| Step: 10
Training loss: 1.8621071577072144
Validation loss: 2.182162048996136

Epoch: 6| Step: 11
Training loss: 2.18398380279541
Validation loss: 2.1888009502041723

Epoch: 6| Step: 12
Training loss: 2.3192992210388184
Validation loss: 2.1866419469156573

Epoch: 6| Step: 13
Training loss: 3.220855951309204
Validation loss: 2.173259558216218

Epoch: 355| Step: 0
Training loss: 2.6319825649261475
Validation loss: 2.1750874442438923

Epoch: 6| Step: 1
Training loss: 2.216888427734375
Validation loss: 2.2010727287620626

Epoch: 6| Step: 2
Training loss: 2.2966556549072266
Validation loss: 2.20111749761848

Epoch: 6| Step: 3
Training loss: 1.1844596862792969
Validation loss: 2.190154439659529

Epoch: 6| Step: 4
Training loss: 2.7559280395507812
Validation loss: 2.2015024590235885

Epoch: 6| Step: 5
Training loss: 1.7212858200073242
Validation loss: 2.215910009158555

Epoch: 6| Step: 6
Training loss: 2.1962709426879883
Validation loss: 2.2204872728675924

Epoch: 6| Step: 7
Training loss: 2.394073724746704
Validation loss: 2.23773834782262

Epoch: 6| Step: 8
Training loss: 2.175769329071045
Validation loss: 2.2403017936214322

Epoch: 6| Step: 9
Training loss: 2.600440740585327
Validation loss: 2.245154923008334

Epoch: 6| Step: 10
Training loss: 3.248002290725708
Validation loss: 2.2750590975566576

Epoch: 6| Step: 11
Training loss: 2.576125383377075
Validation loss: 2.2522600825114916

Epoch: 6| Step: 12
Training loss: 2.559880256652832
Validation loss: 2.246694972438197

Epoch: 6| Step: 13
Training loss: 2.3670461177825928
Validation loss: 2.260325624096778

Epoch: 356| Step: 0
Training loss: 2.488224744796753
Validation loss: 2.240803426311862

Epoch: 6| Step: 1
Training loss: 1.129591941833496
Validation loss: 2.2232452182359594

Epoch: 6| Step: 2
Training loss: 2.724029302597046
Validation loss: 2.2274412365369898

Epoch: 6| Step: 3
Training loss: 2.028074264526367
Validation loss: 2.216700589785012

Epoch: 6| Step: 4
Training loss: 2.4195806980133057
Validation loss: 2.2056899583467873

Epoch: 6| Step: 5
Training loss: 2.729175090789795
Validation loss: 2.217238428772137

Epoch: 6| Step: 6
Training loss: 2.7467384338378906
Validation loss: 2.210767412698397

Epoch: 6| Step: 7
Training loss: 2.852571487426758
Validation loss: 2.211255858021398

Epoch: 6| Step: 8
Training loss: 2.184196949005127
Validation loss: 2.199970086415609

Epoch: 6| Step: 9
Training loss: 2.6389713287353516
Validation loss: 2.2028684616088867

Epoch: 6| Step: 10
Training loss: 2.69095516204834
Validation loss: 2.1883073929817445

Epoch: 6| Step: 11
Training loss: 1.7872440814971924
Validation loss: 2.1830472997439805

Epoch: 6| Step: 12
Training loss: 2.1536641120910645
Validation loss: 2.196001229747649

Epoch: 6| Step: 13
Training loss: 2.061842679977417
Validation loss: 2.1905793041311283

Epoch: 357| Step: 0
Training loss: 2.4108710289001465
Validation loss: 2.1884880937555784

Epoch: 6| Step: 1
Training loss: 2.0027313232421875
Validation loss: 2.206647624251663

Epoch: 6| Step: 2
Training loss: 2.1458990573883057
Validation loss: 2.22169909169597

Epoch: 6| Step: 3
Training loss: 3.435248613357544
Validation loss: 2.23005860851657

Epoch: 6| Step: 4
Training loss: 2.162126064300537
Validation loss: 2.2351599444625196

Epoch: 6| Step: 5
Training loss: 1.9521111249923706
Validation loss: 2.2321464579592467

Epoch: 6| Step: 6
Training loss: 2.314340114593506
Validation loss: 2.2246983769119426

Epoch: 6| Step: 7
Training loss: 2.534075975418091
Validation loss: 2.237907889068768

Epoch: 6| Step: 8
Training loss: 2.127105236053467
Validation loss: 2.2219289220789427

Epoch: 6| Step: 9
Training loss: 2.0800023078918457
Validation loss: 2.2154854805238786

Epoch: 6| Step: 10
Training loss: 2.8097569942474365
Validation loss: 2.1986011740981892

Epoch: 6| Step: 11
Training loss: 2.2281579971313477
Validation loss: 2.182327813999627

Epoch: 6| Step: 12
Training loss: 2.1567139625549316
Validation loss: 2.198202566433978

Epoch: 6| Step: 13
Training loss: 2.303152322769165
Validation loss: 2.2046870954575075

Epoch: 358| Step: 0
Training loss: 1.895684003829956
Validation loss: 2.2148658844732467

Epoch: 6| Step: 1
Training loss: 1.9287686347961426
Validation loss: 2.2130809163534515

Epoch: 6| Step: 2
Training loss: 2.1658778190612793
Validation loss: 2.245717335772771

Epoch: 6| Step: 3
Training loss: 2.2508256435394287
Validation loss: 2.240306609420366

Epoch: 6| Step: 4
Training loss: 2.6014530658721924
Validation loss: 2.245533322775236

Epoch: 6| Step: 5
Training loss: 2.1170315742492676
Validation loss: 2.2338198051657727

Epoch: 6| Step: 6
Training loss: 2.7872848510742188
Validation loss: 2.218724145684191

Epoch: 6| Step: 7
Training loss: 2.2312843799591064
Validation loss: 2.2335333619066464

Epoch: 6| Step: 8
Training loss: 3.0331220626831055
Validation loss: 2.2371350731900943

Epoch: 6| Step: 9
Training loss: 1.7926546335220337
Validation loss: 2.211751710984015

Epoch: 6| Step: 10
Training loss: 2.222353458404541
Validation loss: 2.2239186789399836

Epoch: 6| Step: 11
Training loss: 2.610280752182007
Validation loss: 2.220246840548772

Epoch: 6| Step: 12
Training loss: 1.9976533651351929
Validation loss: 2.199055858837661

Epoch: 6| Step: 13
Training loss: 3.7848832607269287
Validation loss: 2.2105768342171945

Epoch: 359| Step: 0
Training loss: 2.0104479789733887
Validation loss: 2.2072080232763804

Epoch: 6| Step: 1
Training loss: 2.053053855895996
Validation loss: 2.194287847447139

Epoch: 6| Step: 2
Training loss: 1.7291456460952759
Validation loss: 2.1932465286665064

Epoch: 6| Step: 3
Training loss: 2.1854028701782227
Validation loss: 2.1884231208473124

Epoch: 6| Step: 4
Training loss: 2.717155694961548
Validation loss: 2.1922884320700042

Epoch: 6| Step: 5
Training loss: 3.15244197845459
Validation loss: 2.1776134403803016

Epoch: 6| Step: 6
Training loss: 2.36335825920105
Validation loss: 2.173186289366855

Epoch: 6| Step: 7
Training loss: 3.101315975189209
Validation loss: 2.169132199338687

Epoch: 6| Step: 8
Training loss: 2.3345422744750977
Validation loss: 2.1648580079437583

Epoch: 6| Step: 9
Training loss: 2.2914133071899414
Validation loss: 2.1648439989295056

Epoch: 6| Step: 10
Training loss: 3.4139790534973145
Validation loss: 2.1649641888115996

Epoch: 6| Step: 11
Training loss: 1.6160781383514404
Validation loss: 2.160588582356771

Epoch: 6| Step: 12
Training loss: 1.9235832691192627
Validation loss: 2.144426714989447

Epoch: 6| Step: 13
Training loss: 2.026942014694214
Validation loss: 2.15620695647373

Epoch: 360| Step: 0
Training loss: 3.138026714324951
Validation loss: 2.1721896317697342

Epoch: 6| Step: 1
Training loss: 2.4870409965515137
Validation loss: 2.1873679878891155

Epoch: 6| Step: 2
Training loss: 2.6188724040985107
Validation loss: 2.189944818455686

Epoch: 6| Step: 3
Training loss: 2.1744742393493652
Validation loss: 2.2020080986843316

Epoch: 6| Step: 4
Training loss: 2.4728734493255615
Validation loss: 2.2267246528338362

Epoch: 6| Step: 5
Training loss: 2.4953839778900146
Validation loss: 2.219321835425592

Epoch: 6| Step: 6
Training loss: 2.183237075805664
Validation loss: 2.227182190905335

Epoch: 6| Step: 7
Training loss: 2.1996216773986816
Validation loss: 2.23920339410023

Epoch: 6| Step: 8
Training loss: 2.512765645980835
Validation loss: 2.2374319081665366

Epoch: 6| Step: 9
Training loss: 1.4073395729064941
Validation loss: 2.2138833820178943

Epoch: 6| Step: 10
Training loss: 1.582819938659668
Validation loss: 2.222416288109236

Epoch: 6| Step: 11
Training loss: 2.837334632873535
Validation loss: 2.216230279655867

Epoch: 6| Step: 12
Training loss: 2.1901750564575195
Validation loss: 2.209706606403474

Epoch: 6| Step: 13
Training loss: 2.1454641819000244
Validation loss: 2.210888006353891

Epoch: 361| Step: 0
Training loss: 2.757331609725952
Validation loss: 2.2075709476265857

Epoch: 6| Step: 1
Training loss: 1.772522211074829
Validation loss: 2.185325725104219

Epoch: 6| Step: 2
Training loss: 2.071255683898926
Validation loss: 2.1951399823670745

Epoch: 6| Step: 3
Training loss: 1.7299880981445312
Validation loss: 2.2063854612329954

Epoch: 6| Step: 4
Training loss: 1.737006664276123
Validation loss: 2.227682308484149

Epoch: 6| Step: 5
Training loss: 2.5592265129089355
Validation loss: 2.2287010274907595

Epoch: 6| Step: 6
Training loss: 2.2568938732147217
Validation loss: 2.241488500307965

Epoch: 6| Step: 7
Training loss: 1.501960277557373
Validation loss: 2.233153291927871

Epoch: 6| Step: 8
Training loss: 2.873976707458496
Validation loss: 2.2250290839902815

Epoch: 6| Step: 9
Training loss: 2.379581928253174
Validation loss: 2.2165018909720966

Epoch: 6| Step: 10
Training loss: 2.34476637840271
Validation loss: 2.2116118733600905

Epoch: 6| Step: 11
Training loss: 3.032179832458496
Validation loss: 2.206830575901975

Epoch: 6| Step: 12
Training loss: 2.9453465938568115
Validation loss: 2.206337498080346

Epoch: 6| Step: 13
Training loss: 2.8326621055603027
Validation loss: 2.201175617915328

Epoch: 362| Step: 0
Training loss: 2.433162212371826
Validation loss: 2.186115981430136

Epoch: 6| Step: 1
Training loss: 2.789167642593384
Validation loss: 2.2025377852942354

Epoch: 6| Step: 2
Training loss: 1.854662537574768
Validation loss: 2.200288664910101

Epoch: 6| Step: 3
Training loss: 3.081300735473633
Validation loss: 2.2034467984271306

Epoch: 6| Step: 4
Training loss: 2.6778316497802734
Validation loss: 2.1901828499250513

Epoch: 6| Step: 5
Training loss: 2.2462172508239746
Validation loss: 2.1914006445997503

Epoch: 6| Step: 6
Training loss: 1.7448428869247437
Validation loss: 2.182373777512581

Epoch: 6| Step: 7
Training loss: 1.6823643445968628
Validation loss: 2.1715056665482058

Epoch: 6| Step: 8
Training loss: 2.618103504180908
Validation loss: 2.189337238188713

Epoch: 6| Step: 9
Training loss: 2.46697998046875
Validation loss: 2.211532394091288

Epoch: 6| Step: 10
Training loss: 2.638284683227539
Validation loss: 2.2071995273713143

Epoch: 6| Step: 11
Training loss: 2.3457326889038086
Validation loss: 2.230731612892561

Epoch: 6| Step: 12
Training loss: 1.7699278593063354
Validation loss: 2.2394950928226596

Epoch: 6| Step: 13
Training loss: 2.4641306400299072
Validation loss: 2.246964047032018

Epoch: 363| Step: 0
Training loss: 2.4180073738098145
Validation loss: 2.2418521834957983

Epoch: 6| Step: 1
Training loss: 2.199795961380005
Validation loss: 2.23340255214322

Epoch: 6| Step: 2
Training loss: 2.369428873062134
Validation loss: 2.2150168521429903

Epoch: 6| Step: 3
Training loss: 2.3717103004455566
Validation loss: 2.2153581855117634

Epoch: 6| Step: 4
Training loss: 1.7423921823501587
Validation loss: 2.2301649765301774

Epoch: 6| Step: 5
Training loss: 2.273874282836914
Validation loss: 2.235759997880587

Epoch: 6| Step: 6
Training loss: 2.7773172855377197
Validation loss: 2.216908798422865

Epoch: 6| Step: 7
Training loss: 2.5163862705230713
Validation loss: 2.209696476177503

Epoch: 6| Step: 8
Training loss: 2.305257797241211
Validation loss: 2.2214254486945366

Epoch: 6| Step: 9
Training loss: 2.195216655731201
Validation loss: 2.2093993104914182

Epoch: 6| Step: 10
Training loss: 2.121846914291382
Validation loss: 2.200251317793323

Epoch: 6| Step: 11
Training loss: 1.9747974872589111
Validation loss: 2.2104956847365185

Epoch: 6| Step: 12
Training loss: 1.9460049867630005
Validation loss: 2.2163522653682257

Epoch: 6| Step: 13
Training loss: 3.926241636276245
Validation loss: 2.2104368594384964

Epoch: 364| Step: 0
Training loss: 3.0085580348968506
Validation loss: 2.1994042652909473

Epoch: 6| Step: 1
Training loss: 1.6538517475128174
Validation loss: 2.1912355704974105

Epoch: 6| Step: 2
Training loss: 2.3441617488861084
Validation loss: 2.1874043069859987

Epoch: 6| Step: 3
Training loss: 2.330538272857666
Validation loss: 2.1687383574824177

Epoch: 6| Step: 4
Training loss: 2.5675930976867676
Validation loss: 2.1936748591802453

Epoch: 6| Step: 5
Training loss: 1.7955620288848877
Validation loss: 2.1653793806670816

Epoch: 6| Step: 6
Training loss: 2.3517544269561768
Validation loss: 2.179622542473578

Epoch: 6| Step: 7
Training loss: 2.993058443069458
Validation loss: 2.175841498118575

Epoch: 6| Step: 8
Training loss: 1.80525803565979
Validation loss: 2.1610593462503083

Epoch: 6| Step: 9
Training loss: 2.4222755432128906
Validation loss: 2.1851426939810477

Epoch: 6| Step: 10
Training loss: 2.1946606636047363
Validation loss: 2.1812174179220714

Epoch: 6| Step: 11
Training loss: 2.3526535034179688
Validation loss: 2.189244552325177

Epoch: 6| Step: 12
Training loss: 2.199596643447876
Validation loss: 2.1975654530268844

Epoch: 6| Step: 13
Training loss: 2.7748842239379883
Validation loss: 2.2228783356246127

Epoch: 365| Step: 0
Training loss: 2.4445462226867676
Validation loss: 2.2125084656541065

Epoch: 6| Step: 1
Training loss: 2.8484628200531006
Validation loss: 2.235338221314133

Epoch: 6| Step: 2
Training loss: 1.847059965133667
Validation loss: 2.2041393556902484

Epoch: 6| Step: 3
Training loss: 2.5518622398376465
Validation loss: 2.2247369084306943

Epoch: 6| Step: 4
Training loss: 2.4098198413848877
Validation loss: 2.192907319274

Epoch: 6| Step: 5
Training loss: 1.8235398530960083
Validation loss: 2.2105510196378155

Epoch: 6| Step: 6
Training loss: 2.512479066848755
Validation loss: 2.2001108379774195

Epoch: 6| Step: 7
Training loss: 2.8493783473968506
Validation loss: 2.200632864429105

Epoch: 6| Step: 8
Training loss: 2.888896942138672
Validation loss: 2.1988506137683825

Epoch: 6| Step: 9
Training loss: 1.8144420385360718
Validation loss: 2.2021586946261826

Epoch: 6| Step: 10
Training loss: 1.7059754133224487
Validation loss: 2.17891352663758

Epoch: 6| Step: 11
Training loss: 2.1907668113708496
Validation loss: 2.1881822360459195

Epoch: 6| Step: 12
Training loss: 2.282838821411133
Validation loss: 2.1963213976993354

Epoch: 6| Step: 13
Training loss: 2.235137462615967
Validation loss: 2.1869562723303355

Epoch: 366| Step: 0
Training loss: 2.468658447265625
Validation loss: 2.192114040415774

Epoch: 6| Step: 1
Training loss: 2.9374146461486816
Validation loss: 2.202696293912908

Epoch: 6| Step: 2
Training loss: 2.7178773880004883
Validation loss: 2.203581920234106

Epoch: 6| Step: 3
Training loss: 1.6632267236709595
Validation loss: 2.1893040698061705

Epoch: 6| Step: 4
Training loss: 2.3956336975097656
Validation loss: 2.1933512969683577

Epoch: 6| Step: 5
Training loss: 2.626744270324707
Validation loss: 2.211123933074295

Epoch: 6| Step: 6
Training loss: 2.1344072818756104
Validation loss: 2.2307588054287817

Epoch: 6| Step: 7
Training loss: 1.909906029701233
Validation loss: 2.220683369585263

Epoch: 6| Step: 8
Training loss: 2.3398594856262207
Validation loss: 2.240389712395207

Epoch: 6| Step: 9
Training loss: 1.9309031963348389
Validation loss: 2.23233897455277

Epoch: 6| Step: 10
Training loss: 1.9070813655853271
Validation loss: 2.248640206552321

Epoch: 6| Step: 11
Training loss: 2.1003456115722656
Validation loss: 2.2537290883320633

Epoch: 6| Step: 12
Training loss: 2.2648215293884277
Validation loss: 2.231824859496086

Epoch: 6| Step: 13
Training loss: 3.7046399116516113
Validation loss: 2.212887853704473

Epoch: 367| Step: 0
Training loss: 1.5188384056091309
Validation loss: 2.2041277013799196

Epoch: 6| Step: 1
Training loss: 2.213125467300415
Validation loss: 2.183469962048274

Epoch: 6| Step: 2
Training loss: 2.187638282775879
Validation loss: 2.181144146509068

Epoch: 6| Step: 3
Training loss: 2.262033462524414
Validation loss: 2.1856412990118868

Epoch: 6| Step: 4
Training loss: 2.025822401046753
Validation loss: 2.1835662805905907

Epoch: 6| Step: 5
Training loss: 2.429410457611084
Validation loss: 2.178134782339937

Epoch: 6| Step: 6
Training loss: 1.8417832851409912
Validation loss: 2.166641419933688

Epoch: 6| Step: 7
Training loss: 2.683560371398926
Validation loss: 2.174362749181768

Epoch: 6| Step: 8
Training loss: 2.4724855422973633
Validation loss: 2.1862509071186023

Epoch: 6| Step: 9
Training loss: 2.799178123474121
Validation loss: 2.180349175648023

Epoch: 6| Step: 10
Training loss: 2.333085775375366
Validation loss: 2.202000161652924

Epoch: 6| Step: 11
Training loss: 2.8076553344726562
Validation loss: 2.1997386345299343

Epoch: 6| Step: 12
Training loss: 2.5594239234924316
Validation loss: 2.189215906204716

Epoch: 6| Step: 13
Training loss: 2.3843064308166504
Validation loss: 2.182684449739354

Epoch: 368| Step: 0
Training loss: 2.6145551204681396
Validation loss: 2.180725887257566

Epoch: 6| Step: 1
Training loss: 2.522678852081299
Validation loss: 2.193668632097142

Epoch: 6| Step: 2
Training loss: 2.159794807434082
Validation loss: 2.201650224706178

Epoch: 6| Step: 3
Training loss: 1.910740852355957
Validation loss: 2.1961168025129583

Epoch: 6| Step: 4
Training loss: 3.289407253265381
Validation loss: 2.1889935001250236

Epoch: 6| Step: 5
Training loss: 2.401406764984131
Validation loss: 2.1704322522686375

Epoch: 6| Step: 6
Training loss: 1.9866400957107544
Validation loss: 2.2102148814867904

Epoch: 6| Step: 7
Training loss: 2.1542365550994873
Validation loss: 2.2203271414643977

Epoch: 6| Step: 8
Training loss: 1.658718466758728
Validation loss: 2.2047092171125513

Epoch: 6| Step: 9
Training loss: 1.5240733623504639
Validation loss: 2.1956137047019055

Epoch: 6| Step: 10
Training loss: 2.1945858001708984
Validation loss: 2.195492325290557

Epoch: 6| Step: 11
Training loss: 2.695995569229126
Validation loss: 2.205950234525947

Epoch: 6| Step: 12
Training loss: 2.480454444885254
Validation loss: 2.221274939916467

Epoch: 6| Step: 13
Training loss: 3.1183207035064697
Validation loss: 2.203995191922752

Epoch: 369| Step: 0
Training loss: 1.641707181930542
Validation loss: 2.1983976466681368

Epoch: 6| Step: 1
Training loss: 2.0153517723083496
Validation loss: 2.2102705406886276

Epoch: 6| Step: 2
Training loss: 1.8874715566635132
Validation loss: 2.2110532535019742

Epoch: 6| Step: 3
Training loss: 2.3271162509918213
Validation loss: 2.207186859141114

Epoch: 6| Step: 4
Training loss: 2.039904832839966
Validation loss: 2.1733171016939226

Epoch: 6| Step: 5
Training loss: 2.2111587524414062
Validation loss: 2.19127683742072

Epoch: 6| Step: 6
Training loss: 2.7506182193756104
Validation loss: 2.1974447863076323

Epoch: 6| Step: 7
Training loss: 2.7989330291748047
Validation loss: 2.175673906521131

Epoch: 6| Step: 8
Training loss: 1.8921327590942383
Validation loss: 2.1743543481314056

Epoch: 6| Step: 9
Training loss: 2.148099660873413
Validation loss: 2.192316016843242

Epoch: 6| Step: 10
Training loss: 2.4274070262908936
Validation loss: 2.1702720939472155

Epoch: 6| Step: 11
Training loss: 3.1395082473754883
Validation loss: 2.167429498446885

Epoch: 6| Step: 12
Training loss: 2.8806562423706055
Validation loss: 2.176661411921183

Epoch: 6| Step: 13
Training loss: 2.2412211894989014
Validation loss: 2.174876877056655

Epoch: 370| Step: 0
Training loss: 2.7050695419311523
Validation loss: 2.1716175861256097

Epoch: 6| Step: 1
Training loss: 2.6283130645751953
Validation loss: 2.182735530279016

Epoch: 6| Step: 2
Training loss: 2.534043312072754
Validation loss: 2.1747724240826023

Epoch: 6| Step: 3
Training loss: 2.9401283264160156
Validation loss: 2.1770245054716706

Epoch: 6| Step: 4
Training loss: 2.3096017837524414
Validation loss: 2.1808884220738567

Epoch: 6| Step: 5
Training loss: 2.2319650650024414
Validation loss: 2.1865068071631977

Epoch: 6| Step: 6
Training loss: 2.032982349395752
Validation loss: 2.175234548507198

Epoch: 6| Step: 7
Training loss: 1.67612886428833
Validation loss: 2.2081160955531622

Epoch: 6| Step: 8
Training loss: 2.883944272994995
Validation loss: 2.220759701985185

Epoch: 6| Step: 9
Training loss: 1.9185526371002197
Validation loss: 2.216841720765637

Epoch: 6| Step: 10
Training loss: 2.121934413909912
Validation loss: 2.210684053359493

Epoch: 6| Step: 11
Training loss: 1.8882083892822266
Validation loss: 2.198853199199964

Epoch: 6| Step: 12
Training loss: 2.4178082942962646
Validation loss: 2.192270235348773

Epoch: 6| Step: 13
Training loss: 1.8182826042175293
Validation loss: 2.175554170403429

Epoch: 371| Step: 0
Training loss: 3.0745513439178467
Validation loss: 2.1942439502285374

Epoch: 6| Step: 1
Training loss: 2.9436328411102295
Validation loss: 2.1868949474826938

Epoch: 6| Step: 2
Training loss: 2.0160810947418213
Validation loss: 2.1978950551761094

Epoch: 6| Step: 3
Training loss: 1.6539132595062256
Validation loss: 2.173949462111278

Epoch: 6| Step: 4
Training loss: 1.8651378154754639
Validation loss: 2.187866110955515

Epoch: 6| Step: 5
Training loss: 2.4653053283691406
Validation loss: 2.1922049496763494

Epoch: 6| Step: 6
Training loss: 2.8917288780212402
Validation loss: 2.198889583669683

Epoch: 6| Step: 7
Training loss: 2.381709098815918
Validation loss: 2.177754807215865

Epoch: 6| Step: 8
Training loss: 2.655783176422119
Validation loss: 2.1778718579200005

Epoch: 6| Step: 9
Training loss: 1.742790699005127
Validation loss: 2.1894674198601836

Epoch: 6| Step: 10
Training loss: 2.33872127532959
Validation loss: 2.1643212174856536

Epoch: 6| Step: 11
Training loss: 2.3319153785705566
Validation loss: 2.1698845586469098

Epoch: 6| Step: 12
Training loss: 2.239124059677124
Validation loss: 2.188056707382202

Epoch: 6| Step: 13
Training loss: 1.8488192558288574
Validation loss: 2.20525118612474

Epoch: 372| Step: 0
Training loss: 3.0200462341308594
Validation loss: 2.23866706637926

Epoch: 6| Step: 1
Training loss: 2.043318271636963
Validation loss: 2.275859676381593

Epoch: 6| Step: 2
Training loss: 3.2894036769866943
Validation loss: 2.296412685865997

Epoch: 6| Step: 3
Training loss: 2.356757402420044
Validation loss: 2.2980060820938437

Epoch: 6| Step: 4
Training loss: 1.8945873975753784
Validation loss: 2.3022707316183273

Epoch: 6| Step: 5
Training loss: 2.4357168674468994
Validation loss: 2.2976129106296006

Epoch: 6| Step: 6
Training loss: 1.830778956413269
Validation loss: 2.2744822630318264

Epoch: 6| Step: 7
Training loss: 1.966423511505127
Validation loss: 2.2653224878413702

Epoch: 6| Step: 8
Training loss: 2.6152377128601074
Validation loss: 2.2719347605141262

Epoch: 6| Step: 9
Training loss: 2.2221426963806152
Validation loss: 2.263268001617924

Epoch: 6| Step: 10
Training loss: 1.9106943607330322
Validation loss: 2.2126514322014263

Epoch: 6| Step: 11
Training loss: 2.1302378177642822
Validation loss: 2.1922607114238124

Epoch: 6| Step: 12
Training loss: 2.4482665061950684
Validation loss: 2.1939814629093295

Epoch: 6| Step: 13
Training loss: 2.671186685562134
Validation loss: 2.1871116135710027

Epoch: 373| Step: 0
Training loss: 1.5871400833129883
Validation loss: 2.163663725699148

Epoch: 6| Step: 1
Training loss: 2.5713112354278564
Validation loss: 2.169615968581169

Epoch: 6| Step: 2
Training loss: 2.521613836288452
Validation loss: 2.167354699104063

Epoch: 6| Step: 3
Training loss: 2.3974130153656006
Validation loss: 2.1731752400757163

Epoch: 6| Step: 4
Training loss: 1.9637833833694458
Validation loss: 2.195089909338182

Epoch: 6| Step: 5
Training loss: 2.3955554962158203
Validation loss: 2.1761410402995285

Epoch: 6| Step: 6
Training loss: 3.240211009979248
Validation loss: 2.182551100689878

Epoch: 6| Step: 7
Training loss: 2.480416774749756
Validation loss: 2.1882273304846978

Epoch: 6| Step: 8
Training loss: 2.3673744201660156
Validation loss: 2.1890125249021795

Epoch: 6| Step: 9
Training loss: 1.3845276832580566
Validation loss: 2.1962256098306305

Epoch: 6| Step: 10
Training loss: 2.1236400604248047
Validation loss: 2.1769126333216184

Epoch: 6| Step: 11
Training loss: 2.1021316051483154
Validation loss: 2.1850383256071355

Epoch: 6| Step: 12
Training loss: 2.6018710136413574
Validation loss: 2.1830755869547525

Epoch: 6| Step: 13
Training loss: 2.947359085083008
Validation loss: 2.199821432431539

Epoch: 374| Step: 0
Training loss: 2.0660288333892822
Validation loss: 2.218727104125484

Epoch: 6| Step: 1
Training loss: 2.390604257583618
Validation loss: 2.2140073724972305

Epoch: 6| Step: 2
Training loss: 2.06547212600708
Validation loss: 2.2235581028846

Epoch: 6| Step: 3
Training loss: 1.5292768478393555
Validation loss: 2.2098120848337808

Epoch: 6| Step: 4
Training loss: 2.6563491821289062
Validation loss: 2.226585949620893

Epoch: 6| Step: 5
Training loss: 1.9740550518035889
Validation loss: 2.2361438325656358

Epoch: 6| Step: 6
Training loss: 2.042421817779541
Validation loss: 2.213577813999627

Epoch: 6| Step: 7
Training loss: 2.070833206176758
Validation loss: 2.19573127326145

Epoch: 6| Step: 8
Training loss: 2.8073158264160156
Validation loss: 2.2045854419790287

Epoch: 6| Step: 9
Training loss: 2.3192691802978516
Validation loss: 2.1803531416000856

Epoch: 6| Step: 10
Training loss: 2.7659263610839844
Validation loss: 2.147303299237323

Epoch: 6| Step: 11
Training loss: 2.6593563556671143
Validation loss: 2.173902016814037

Epoch: 6| Step: 12
Training loss: 2.81364107131958
Validation loss: 2.16947518369203

Epoch: 6| Step: 13
Training loss: 1.9957212209701538
Validation loss: 2.182637591515818

Epoch: 375| Step: 0
Training loss: 2.3824057579040527
Validation loss: 2.193510942561652

Epoch: 6| Step: 1
Training loss: 2.3486106395721436
Validation loss: 2.177566087374123

Epoch: 6| Step: 2
Training loss: 2.705888509750366
Validation loss: 2.19001531088224

Epoch: 6| Step: 3
Training loss: 1.6698668003082275
Validation loss: 2.2032391409720145

Epoch: 6| Step: 4
Training loss: 2.76884388923645
Validation loss: 2.1825486998404227

Epoch: 6| Step: 5
Training loss: 2.0250325202941895
Validation loss: 2.1825592338397937

Epoch: 6| Step: 6
Training loss: 2.4058942794799805
Validation loss: 2.1750842038021294

Epoch: 6| Step: 7
Training loss: 2.289926767349243
Validation loss: 2.1776448629235707

Epoch: 6| Step: 8
Training loss: 2.8008499145507812
Validation loss: 2.179500595215828

Epoch: 6| Step: 9
Training loss: 1.801558494567871
Validation loss: 2.178996350175591

Epoch: 6| Step: 10
Training loss: 1.8951897621154785
Validation loss: 2.1730644882366223

Epoch: 6| Step: 11
Training loss: 2.2692205905914307
Validation loss: 2.182489420778008

Epoch: 6| Step: 12
Training loss: 2.853367805480957
Validation loss: 2.190439929244339

Epoch: 6| Step: 13
Training loss: 3.1733334064483643
Validation loss: 2.1827440082385974

Epoch: 376| Step: 0
Training loss: 2.751739978790283
Validation loss: 2.176542338504586

Epoch: 6| Step: 1
Training loss: 2.036515712738037
Validation loss: 2.1767648625117477

Epoch: 6| Step: 2
Training loss: 2.5560760498046875
Validation loss: 2.1818462187244045

Epoch: 6| Step: 3
Training loss: 1.7620952129364014
Validation loss: 2.171078530691003

Epoch: 6| Step: 4
Training loss: 2.0861687660217285
Validation loss: 2.1749694847291514

Epoch: 6| Step: 5
Training loss: 3.1671643257141113
Validation loss: 2.197480929795132

Epoch: 6| Step: 6
Training loss: 2.495312452316284
Validation loss: 2.1832112163625736

Epoch: 6| Step: 7
Training loss: 2.179271936416626
Validation loss: 2.1900880106033815

Epoch: 6| Step: 8
Training loss: 2.1807804107666016
Validation loss: 2.2016381371405815

Epoch: 6| Step: 9
Training loss: 2.8098111152648926
Validation loss: 2.1916106644497124

Epoch: 6| Step: 10
Training loss: 1.9963874816894531
Validation loss: 2.192492651683028

Epoch: 6| Step: 11
Training loss: 2.69209885597229
Validation loss: 2.1691471774091005

Epoch: 6| Step: 12
Training loss: 1.6225271224975586
Validation loss: 2.1738497287996355

Epoch: 6| Step: 13
Training loss: 1.9943703413009644
Validation loss: 2.1825057973143873

Epoch: 377| Step: 0
Training loss: 1.6784485578536987
Validation loss: 2.1502317254261305

Epoch: 6| Step: 1
Training loss: 2.548297643661499
Validation loss: 2.1712802174270793

Epoch: 6| Step: 2
Training loss: 2.3099303245544434
Validation loss: 2.1828006275238527

Epoch: 6| Step: 3
Training loss: 1.9696764945983887
Validation loss: 2.19051928417657

Epoch: 6| Step: 4
Training loss: 2.418670415878296
Validation loss: 2.196610261035222

Epoch: 6| Step: 5
Training loss: 1.9233028888702393
Validation loss: 2.198264502709912

Epoch: 6| Step: 6
Training loss: 2.4045803546905518
Validation loss: 2.2250158427863993

Epoch: 6| Step: 7
Training loss: 2.78853440284729
Validation loss: 2.234886856489284

Epoch: 6| Step: 8
Training loss: 2.645883083343506
Validation loss: 2.254339664213119

Epoch: 6| Step: 9
Training loss: 1.997815489768982
Validation loss: 2.2681007128889843

Epoch: 6| Step: 10
Training loss: 2.671323299407959
Validation loss: 2.247280237495258

Epoch: 6| Step: 11
Training loss: 2.3754935264587402
Validation loss: 2.260403835645286

Epoch: 6| Step: 12
Training loss: 2.6136507987976074
Validation loss: 2.2144048470322804

Epoch: 6| Step: 13
Training loss: 1.891837239265442
Validation loss: 2.212898213376281

Epoch: 378| Step: 0
Training loss: 3.4680140018463135
Validation loss: 2.189141324771348

Epoch: 6| Step: 1
Training loss: 2.9724340438842773
Validation loss: 2.1576271980039534

Epoch: 6| Step: 2
Training loss: 1.935577630996704
Validation loss: 2.174341667083002

Epoch: 6| Step: 3
Training loss: 2.089073657989502
Validation loss: 2.168245412970102

Epoch: 6| Step: 4
Training loss: 2.3127856254577637
Validation loss: 2.1928280553510113

Epoch: 6| Step: 5
Training loss: 1.8656671047210693
Validation loss: 2.214605059674991

Epoch: 6| Step: 6
Training loss: 1.513922095298767
Validation loss: 2.201170193251743

Epoch: 6| Step: 7
Training loss: 2.783893585205078
Validation loss: 2.20856378411734

Epoch: 6| Step: 8
Training loss: 2.672168254852295
Validation loss: 2.186165399448846

Epoch: 6| Step: 9
Training loss: 2.121593475341797
Validation loss: 2.1797645040737685

Epoch: 6| Step: 10
Training loss: 2.0271265506744385
Validation loss: 2.1654104314824587

Epoch: 6| Step: 11
Training loss: 2.452887535095215
Validation loss: 2.1874638398488364

Epoch: 6| Step: 12
Training loss: 1.9827008247375488
Validation loss: 2.17213354315809

Epoch: 6| Step: 13
Training loss: 2.2007224559783936
Validation loss: 2.173966307793894

Epoch: 379| Step: 0
Training loss: 1.6330864429473877
Validation loss: 2.1782865344837146

Epoch: 6| Step: 1
Training loss: 2.105224609375
Validation loss: 2.191912156279369

Epoch: 6| Step: 2
Training loss: 2.5753026008605957
Validation loss: 2.1855590112747683

Epoch: 6| Step: 3
Training loss: 2.1429295539855957
Validation loss: 2.21457911563176

Epoch: 6| Step: 4
Training loss: 2.1584177017211914
Validation loss: 2.234016623548282

Epoch: 6| Step: 5
Training loss: 2.270995855331421
Validation loss: 2.2400169423831406

Epoch: 6| Step: 6
Training loss: 2.3874125480651855
Validation loss: 2.2473179448035454

Epoch: 6| Step: 7
Training loss: 2.693082809448242
Validation loss: 2.238719774830726

Epoch: 6| Step: 8
Training loss: 2.434614658355713
Validation loss: 2.2139189371498684

Epoch: 6| Step: 9
Training loss: 2.7224767208099365
Validation loss: 2.2123391269355692

Epoch: 6| Step: 10
Training loss: 2.43874454498291
Validation loss: 2.226325975951328

Epoch: 6| Step: 11
Training loss: 2.560582399368286
Validation loss: 2.2151416501691266

Epoch: 6| Step: 12
Training loss: 1.9744246006011963
Validation loss: 2.204709745222522

Epoch: 6| Step: 13
Training loss: 2.5588271617889404
Validation loss: 2.220268962203815

Epoch: 380| Step: 0
Training loss: 2.257448196411133
Validation loss: 2.1927817137010637

Epoch: 6| Step: 1
Training loss: 2.2684240341186523
Validation loss: 2.17096701745064

Epoch: 6| Step: 2
Training loss: 2.1305761337280273
Validation loss: 2.1668257841499905

Epoch: 6| Step: 3
Training loss: 2.0388848781585693
Validation loss: 2.1695691667577273

Epoch: 6| Step: 4
Training loss: 1.6280721426010132
Validation loss: 2.177668650945028

Epoch: 6| Step: 5
Training loss: 2.9951019287109375
Validation loss: 2.175689284519483

Epoch: 6| Step: 6
Training loss: 2.5930099487304688
Validation loss: 2.1792908458299536

Epoch: 6| Step: 7
Training loss: 1.908092975616455
Validation loss: 2.1865055766156924

Epoch: 6| Step: 8
Training loss: 2.645479679107666
Validation loss: 2.165288035587598

Epoch: 6| Step: 9
Training loss: 1.5394904613494873
Validation loss: 2.1741689071860364

Epoch: 6| Step: 10
Training loss: 2.55880069732666
Validation loss: 2.1848404689501693

Epoch: 6| Step: 11
Training loss: 2.7619528770446777
Validation loss: 2.186712831579229

Epoch: 6| Step: 12
Training loss: 2.088135004043579
Validation loss: 2.213437100892426

Epoch: 6| Step: 13
Training loss: 2.889943838119507
Validation loss: 2.1929780731918993

Epoch: 381| Step: 0
Training loss: 1.7450695037841797
Validation loss: 2.2234774789502545

Epoch: 6| Step: 1
Training loss: 2.0312986373901367
Validation loss: 2.2113826787599953

Epoch: 6| Step: 2
Training loss: 3.027691602706909
Validation loss: 2.2237042586008706

Epoch: 6| Step: 3
Training loss: 2.8586301803588867
Validation loss: 2.2172188861395723

Epoch: 6| Step: 4
Training loss: 2.3640737533569336
Validation loss: 2.204521043326265

Epoch: 6| Step: 5
Training loss: 1.768064260482788
Validation loss: 2.210020588290307

Epoch: 6| Step: 6
Training loss: 1.3282716274261475
Validation loss: 2.20570897030574

Epoch: 6| Step: 7
Training loss: 2.2956323623657227
Validation loss: 2.185602016346429

Epoch: 6| Step: 8
Training loss: 2.406712770462036
Validation loss: 2.193051948342272

Epoch: 6| Step: 9
Training loss: 2.529170036315918
Validation loss: 2.1793224734644734

Epoch: 6| Step: 10
Training loss: 2.3159372806549072
Validation loss: 2.1752170106416107

Epoch: 6| Step: 11
Training loss: 3.0858545303344727
Validation loss: 2.1787673888667936

Epoch: 6| Step: 12
Training loss: 1.8615379333496094
Validation loss: 2.1690581331970873

Epoch: 6| Step: 13
Training loss: 2.596745014190674
Validation loss: 2.1632219552993774

Epoch: 382| Step: 0
Training loss: 1.878646731376648
Validation loss: 2.167383042714929

Epoch: 6| Step: 1
Training loss: 2.5905275344848633
Validation loss: 2.1579562002612698

Epoch: 6| Step: 2
Training loss: 2.617640495300293
Validation loss: 2.1638241660210396

Epoch: 6| Step: 3
Training loss: 2.4152727127075195
Validation loss: 2.162172330323086

Epoch: 6| Step: 4
Training loss: 1.7902140617370605
Validation loss: 2.168557410599083

Epoch: 6| Step: 5
Training loss: 2.392765522003174
Validation loss: 2.1629938733193184

Epoch: 6| Step: 6
Training loss: 2.3062143325805664
Validation loss: 2.1553859056965

Epoch: 6| Step: 7
Training loss: 2.069366693496704
Validation loss: 2.1595217002335416

Epoch: 6| Step: 8
Training loss: 2.436995506286621
Validation loss: 2.174767553165395

Epoch: 6| Step: 9
Training loss: 2.059584617614746
Validation loss: 2.1887889369841544

Epoch: 6| Step: 10
Training loss: 2.3751702308654785
Validation loss: 2.186556493082354

Epoch: 6| Step: 11
Training loss: 2.4805922508239746
Validation loss: 2.1974124421355543

Epoch: 6| Step: 12
Training loss: 2.293931007385254
Validation loss: 2.192081179670108

Epoch: 6| Step: 13
Training loss: 2.5138444900512695
Validation loss: 2.220609370098319

Epoch: 383| Step: 0
Training loss: 1.738251805305481
Validation loss: 2.2072478443063717

Epoch: 6| Step: 1
Training loss: 2.5568737983703613
Validation loss: 2.2098165378775647

Epoch: 6| Step: 2
Training loss: 2.3045284748077393
Validation loss: 2.181174808932889

Epoch: 6| Step: 3
Training loss: 1.9991235733032227
Validation loss: 2.16187398792595

Epoch: 6| Step: 4
Training loss: 3.5121989250183105
Validation loss: 2.173388934904529

Epoch: 6| Step: 5
Training loss: 2.2741684913635254
Validation loss: 2.1514649621901976

Epoch: 6| Step: 6
Training loss: 2.602543830871582
Validation loss: 2.1651672496590564

Epoch: 6| Step: 7
Training loss: 1.8962955474853516
Validation loss: 2.148693960200074

Epoch: 6| Step: 8
Training loss: 1.8406941890716553
Validation loss: 2.1438347908758346

Epoch: 6| Step: 9
Training loss: 1.8846635818481445
Validation loss: 2.165897553966891

Epoch: 6| Step: 10
Training loss: 2.0216877460479736
Validation loss: 2.160934907133861

Epoch: 6| Step: 11
Training loss: 3.0396151542663574
Validation loss: 2.1582338784330632

Epoch: 6| Step: 12
Training loss: 2.365184783935547
Validation loss: 2.171755394627971

Epoch: 6| Step: 13
Training loss: 1.6967759132385254
Validation loss: 2.175882377932149

Epoch: 384| Step: 0
Training loss: 2.590118885040283
Validation loss: 2.1593004477921354

Epoch: 6| Step: 1
Training loss: 2.7230448722839355
Validation loss: 2.17697972636069

Epoch: 6| Step: 2
Training loss: 1.246006965637207
Validation loss: 2.1833471469981696

Epoch: 6| Step: 3
Training loss: 2.721851348876953
Validation loss: 2.19679279481211

Epoch: 6| Step: 4
Training loss: 2.404590129852295
Validation loss: 2.2073064388767367

Epoch: 6| Step: 5
Training loss: 1.9560635089874268
Validation loss: 2.192631503587128

Epoch: 6| Step: 6
Training loss: 2.8790149688720703
Validation loss: 2.198558340790451

Epoch: 6| Step: 7
Training loss: 2.036665439605713
Validation loss: 2.186479192908092

Epoch: 6| Step: 8
Training loss: 2.202580690383911
Validation loss: 2.1641363533594276

Epoch: 6| Step: 9
Training loss: 2.5059566497802734
Validation loss: 2.15222869893556

Epoch: 6| Step: 10
Training loss: 1.670546054840088
Validation loss: 2.1437683002923125

Epoch: 6| Step: 11
Training loss: 2.7494616508483887
Validation loss: 2.1630909289083173

Epoch: 6| Step: 12
Training loss: 2.3632497787475586
Validation loss: 2.166489924153974

Epoch: 6| Step: 13
Training loss: 1.9658796787261963
Validation loss: 2.173592666144012

Epoch: 385| Step: 0
Training loss: 2.5733137130737305
Validation loss: 2.175410214290824

Epoch: 6| Step: 1
Training loss: 2.628634214401245
Validation loss: 2.1809226748763875

Epoch: 6| Step: 2
Training loss: 2.3198037147521973
Validation loss: 2.1964698171102874

Epoch: 6| Step: 3
Training loss: 2.5452418327331543
Validation loss: 2.1750701858151342

Epoch: 6| Step: 4
Training loss: 2.2560973167419434
Validation loss: 2.187895231349494

Epoch: 6| Step: 5
Training loss: 2.560765027999878
Validation loss: 2.1955156249384724

Epoch: 6| Step: 6
Training loss: 2.2986485958099365
Validation loss: 2.199886352785172

Epoch: 6| Step: 7
Training loss: 2.4824674129486084
Validation loss: 2.206321582999281

Epoch: 6| Step: 8
Training loss: 1.5704569816589355
Validation loss: 2.178425988843364

Epoch: 6| Step: 9
Training loss: 2.1743223667144775
Validation loss: 2.1968550374430995

Epoch: 6| Step: 10
Training loss: 1.6607111692428589
Validation loss: 2.202833101313601

Epoch: 6| Step: 11
Training loss: 3.017186403274536
Validation loss: 2.217494390344107

Epoch: 6| Step: 12
Training loss: 2.164747714996338
Validation loss: 2.1842271999646257

Epoch: 6| Step: 13
Training loss: 1.4327692985534668
Validation loss: 2.189426851528947

Epoch: 386| Step: 0
Training loss: 2.714838981628418
Validation loss: 2.1852195211636123

Epoch: 6| Step: 1
Training loss: 1.2807202339172363
Validation loss: 2.1756206417596466

Epoch: 6| Step: 2
Training loss: 3.0896177291870117
Validation loss: 2.1729865356158187

Epoch: 6| Step: 3
Training loss: 2.0662057399749756
Validation loss: 2.1794228963954474

Epoch: 6| Step: 4
Training loss: 2.370643377304077
Validation loss: 2.1710441445791595

Epoch: 6| Step: 5
Training loss: 2.0023746490478516
Validation loss: 2.176885017784693

Epoch: 6| Step: 6
Training loss: 2.7070109844207764
Validation loss: 2.162731662873299

Epoch: 6| Step: 7
Training loss: 2.4070258140563965
Validation loss: 2.1747126604921077

Epoch: 6| Step: 8
Training loss: 2.462045192718506
Validation loss: 2.166120019010318

Epoch: 6| Step: 9
Training loss: 1.8885623216629028
Validation loss: 2.1720651657350603

Epoch: 6| Step: 10
Training loss: 2.3997385501861572
Validation loss: 2.1649575054004626

Epoch: 6| Step: 11
Training loss: 2.4195022583007812
Validation loss: 2.189889750173015

Epoch: 6| Step: 12
Training loss: 2.383586883544922
Validation loss: 2.181398160995976

Epoch: 6| Step: 13
Training loss: 1.4658112525939941
Validation loss: 2.1708451624839538

Epoch: 387| Step: 0
Training loss: 1.911281943321228
Validation loss: 2.1810668283893215

Epoch: 6| Step: 1
Training loss: 3.183255672454834
Validation loss: 2.1799706361627065

Epoch: 6| Step: 2
Training loss: 2.063405990600586
Validation loss: 2.1762191775024577

Epoch: 6| Step: 3
Training loss: 2.411846160888672
Validation loss: 2.175896067773142

Epoch: 6| Step: 4
Training loss: 2.2297215461730957
Validation loss: 2.1784914360251477

Epoch: 6| Step: 5
Training loss: 1.6142029762268066
Validation loss: 2.1781358539417224

Epoch: 6| Step: 6
Training loss: 2.5256857872009277
Validation loss: 2.2012162311102754

Epoch: 6| Step: 7
Training loss: 2.7705142498016357
Validation loss: 2.2163740396499634

Epoch: 6| Step: 8
Training loss: 2.4739222526550293
Validation loss: 2.219147236116471

Epoch: 6| Step: 9
Training loss: 2.9012773036956787
Validation loss: 2.2174983204052015

Epoch: 6| Step: 10
Training loss: 1.9141514301300049
Validation loss: 2.210130040363599

Epoch: 6| Step: 11
Training loss: 1.267329216003418
Validation loss: 2.2013761817768054

Epoch: 6| Step: 12
Training loss: 2.291764497756958
Validation loss: 2.204547543679514

Epoch: 6| Step: 13
Training loss: 2.4799413681030273
Validation loss: 2.1716858853576

Epoch: 388| Step: 0
Training loss: 2.4292802810668945
Validation loss: 2.1832288875374743

Epoch: 6| Step: 1
Training loss: 3.244194746017456
Validation loss: 2.1664803374198174

Epoch: 6| Step: 2
Training loss: 2.5120153427124023
Validation loss: 2.180369346372543

Epoch: 6| Step: 3
Training loss: 1.873757243156433
Validation loss: 2.1443341291078957

Epoch: 6| Step: 4
Training loss: 2.322885513305664
Validation loss: 2.1490271194006807

Epoch: 6| Step: 5
Training loss: 2.263298273086548
Validation loss: 2.1790464308954056

Epoch: 6| Step: 6
Training loss: 2.818453073501587
Validation loss: 2.1268619901390484

Epoch: 6| Step: 7
Training loss: 1.8719456195831299
Validation loss: 2.139872516355207

Epoch: 6| Step: 8
Training loss: 2.0773210525512695
Validation loss: 2.1561418887107604

Epoch: 6| Step: 9
Training loss: 2.3763625621795654
Validation loss: 2.145565632850893

Epoch: 6| Step: 10
Training loss: 2.2815442085266113
Validation loss: 2.1405455194493777

Epoch: 6| Step: 11
Training loss: 1.880812406539917
Validation loss: 2.1505867588904595

Epoch: 6| Step: 12
Training loss: 1.4793455600738525
Validation loss: 2.150200387483002

Epoch: 6| Step: 13
Training loss: 2.6853551864624023
Validation loss: 2.1559687391404183

Epoch: 389| Step: 0
Training loss: 2.0288262367248535
Validation loss: 2.186190297526698

Epoch: 6| Step: 1
Training loss: 2.6483078002929688
Validation loss: 2.1773197086908485

Epoch: 6| Step: 2
Training loss: 1.553567886352539
Validation loss: 2.210530093921128

Epoch: 6| Step: 3
Training loss: 2.2349655628204346
Validation loss: 2.2095446022607947

Epoch: 6| Step: 4
Training loss: 2.5487852096557617
Validation loss: 2.2473299452053603

Epoch: 6| Step: 5
Training loss: 2.664766788482666
Validation loss: 2.272959537403558

Epoch: 6| Step: 6
Training loss: 1.8038616180419922
Validation loss: 2.2330941641202537

Epoch: 6| Step: 7
Training loss: 2.2623848915100098
Validation loss: 2.2041983783886

Epoch: 6| Step: 8
Training loss: 2.5980398654937744
Validation loss: 2.195529665998233

Epoch: 6| Step: 9
Training loss: 2.1037750244140625
Validation loss: 2.180770777886914

Epoch: 6| Step: 10
Training loss: 2.509727954864502
Validation loss: 2.176833306589434

Epoch: 6| Step: 11
Training loss: 2.394874095916748
Validation loss: 2.1789745284665014

Epoch: 6| Step: 12
Training loss: 2.456930637359619
Validation loss: 2.1685452691970335

Epoch: 6| Step: 13
Training loss: 1.9735304117202759
Validation loss: 2.148064838942661

Epoch: 390| Step: 0
Training loss: 2.7661991119384766
Validation loss: 2.1687277773375153

Epoch: 6| Step: 1
Training loss: 2.0003397464752197
Validation loss: 2.165657224193696

Epoch: 6| Step: 2
Training loss: 2.442286729812622
Validation loss: 2.1548251080256637

Epoch: 6| Step: 3
Training loss: 2.0199666023254395
Validation loss: 2.1535005902731292

Epoch: 6| Step: 4
Training loss: 2.8397140502929688
Validation loss: 2.1621538041740336

Epoch: 6| Step: 5
Training loss: 2.4567036628723145
Validation loss: 2.1608928659910798

Epoch: 6| Step: 6
Training loss: 2.397724151611328
Validation loss: 2.1542027791341147

Epoch: 6| Step: 7
Training loss: 2.669762134552002
Validation loss: 2.1583177633182977

Epoch: 6| Step: 8
Training loss: 2.505868911743164
Validation loss: 2.172382370118172

Epoch: 6| Step: 9
Training loss: 2.2848970890045166
Validation loss: 2.1827799325348227

Epoch: 6| Step: 10
Training loss: 1.1298315525054932
Validation loss: 2.1879486371112127

Epoch: 6| Step: 11
Training loss: 2.1671738624572754
Validation loss: 2.188871460576211

Epoch: 6| Step: 12
Training loss: 1.8073943853378296
Validation loss: 2.1776434016484085

Epoch: 6| Step: 13
Training loss: 2.626328706741333
Validation loss: 2.1643272599866314

Epoch: 391| Step: 0
Training loss: 2.1431093215942383
Validation loss: 2.1864194895631526

Epoch: 6| Step: 1
Training loss: 1.314276099205017
Validation loss: 2.1751995753216486

Epoch: 6| Step: 2
Training loss: 1.4172234535217285
Validation loss: 2.1815546558749292

Epoch: 6| Step: 3
Training loss: 2.9331490993499756
Validation loss: 2.20097085993777

Epoch: 6| Step: 4
Training loss: 1.6646509170532227
Validation loss: 2.188334059971635

Epoch: 6| Step: 5
Training loss: 2.8905248641967773
Validation loss: 2.2246189399432112

Epoch: 6| Step: 6
Training loss: 2.2642412185668945
Validation loss: 2.2111978120701288

Epoch: 6| Step: 7
Training loss: 2.6488094329833984
Validation loss: 2.2202670035823697

Epoch: 6| Step: 8
Training loss: 2.2552831172943115
Validation loss: 2.2106525436524422

Epoch: 6| Step: 9
Training loss: 2.542151927947998
Validation loss: 2.17804580350076

Epoch: 6| Step: 10
Training loss: 2.2805302143096924
Validation loss: 2.1583369598593762

Epoch: 6| Step: 11
Training loss: 2.512296199798584
Validation loss: 2.16506524496181

Epoch: 6| Step: 12
Training loss: 2.5855374336242676
Validation loss: 2.1566711702654437

Epoch: 6| Step: 13
Training loss: 2.5586259365081787
Validation loss: 2.1359443792732815

Epoch: 392| Step: 0
Training loss: 1.8377227783203125
Validation loss: 2.126138993488845

Epoch: 6| Step: 1
Training loss: 2.522141218185425
Validation loss: 2.155100971139887

Epoch: 6| Step: 2
Training loss: 1.9019241333007812
Validation loss: 2.1377255865322646

Epoch: 6| Step: 3
Training loss: 2.9979090690612793
Validation loss: 2.1274204766878517

Epoch: 6| Step: 4
Training loss: 2.1982736587524414
Validation loss: 2.1325625373471166

Epoch: 6| Step: 5
Training loss: 2.809772491455078
Validation loss: 2.1446376282681703

Epoch: 6| Step: 6
Training loss: 2.50628924369812
Validation loss: 2.1359258313332834

Epoch: 6| Step: 7
Training loss: 1.9253789186477661
Validation loss: 2.1532202420696134

Epoch: 6| Step: 8
Training loss: 1.720951795578003
Validation loss: 2.1415990193684897

Epoch: 6| Step: 9
Training loss: 2.0510902404785156
Validation loss: 2.134149069427162

Epoch: 6| Step: 10
Training loss: 2.2468671798706055
Validation loss: 2.1440444428433656

Epoch: 6| Step: 11
Training loss: 2.4137637615203857
Validation loss: 2.1434750890219085

Epoch: 6| Step: 12
Training loss: 2.414796829223633
Validation loss: 2.1496881977204354

Epoch: 6| Step: 13
Training loss: 2.2979726791381836
Validation loss: 2.1542138591889413

Epoch: 393| Step: 0
Training loss: 2.007913589477539
Validation loss: 2.1567277472506285

Epoch: 6| Step: 1
Training loss: 2.19985294342041
Validation loss: 2.195452869579356

Epoch: 6| Step: 2
Training loss: 2.258896827697754
Validation loss: 2.1966254480423464

Epoch: 6| Step: 3
Training loss: 2.5842397212982178
Validation loss: 2.1979357068256666

Epoch: 6| Step: 4
Training loss: 2.412426471710205
Validation loss: 2.2276491836834977

Epoch: 6| Step: 5
Training loss: 2.108827590942383
Validation loss: 2.222713972932549

Epoch: 6| Step: 6
Training loss: 3.2262790203094482
Validation loss: 2.2114733085837415

Epoch: 6| Step: 7
Training loss: 1.9791197776794434
Validation loss: 2.2345904765590543

Epoch: 6| Step: 8
Training loss: 2.3937594890594482
Validation loss: 2.230137930121473

Epoch: 6| Step: 9
Training loss: 1.9858065843582153
Validation loss: 2.2077627207643244

Epoch: 6| Step: 10
Training loss: 2.4083657264709473
Validation loss: 2.237645677340928

Epoch: 6| Step: 11
Training loss: 2.2691173553466797
Validation loss: 2.2143970574102094

Epoch: 6| Step: 12
Training loss: 1.558535099029541
Validation loss: 2.167993450677523

Epoch: 6| Step: 13
Training loss: 2.793597459793091
Validation loss: 2.160265240617978

Epoch: 394| Step: 0
Training loss: 1.9652771949768066
Validation loss: 2.162357489267985

Epoch: 6| Step: 1
Training loss: 2.383828639984131
Validation loss: 2.170139494762626

Epoch: 6| Step: 2
Training loss: 1.775053858757019
Validation loss: 2.149237335369151

Epoch: 6| Step: 3
Training loss: 2.438749313354492
Validation loss: 2.137032174294995

Epoch: 6| Step: 4
Training loss: 2.591428756713867
Validation loss: 2.16149309373671

Epoch: 6| Step: 5
Training loss: 2.148162841796875
Validation loss: 2.15188261770433

Epoch: 6| Step: 6
Training loss: 1.731274127960205
Validation loss: 2.1464050790315032

Epoch: 6| Step: 7
Training loss: 1.7462018728256226
Validation loss: 2.1417409373867895

Epoch: 6| Step: 8
Training loss: 2.649916410446167
Validation loss: 2.1339406736435427

Epoch: 6| Step: 9
Training loss: 2.5972275733947754
Validation loss: 2.1463223452209146

Epoch: 6| Step: 10
Training loss: 2.6973273754119873
Validation loss: 2.1742039547171643

Epoch: 6| Step: 11
Training loss: 2.8484692573547363
Validation loss: 2.176609931453582

Epoch: 6| Step: 12
Training loss: 2.5795743465423584
Validation loss: 2.1886279531704482

Epoch: 6| Step: 13
Training loss: 1.57947838306427
Validation loss: 2.1860626256594093

Epoch: 395| Step: 0
Training loss: 2.3946259021759033
Validation loss: 2.1820943355560303

Epoch: 6| Step: 1
Training loss: 1.9506261348724365
Validation loss: 2.1768286433271182

Epoch: 6| Step: 2
Training loss: 2.3204562664031982
Validation loss: 2.1727075499873005

Epoch: 6| Step: 3
Training loss: 2.2756025791168213
Validation loss: 2.15838106729651

Epoch: 6| Step: 4
Training loss: 2.199284553527832
Validation loss: 2.1548949031419653

Epoch: 6| Step: 5
Training loss: 2.0505969524383545
Validation loss: 2.1562369638873684

Epoch: 6| Step: 6
Training loss: 2.760394334793091
Validation loss: 2.1350215763174076

Epoch: 6| Step: 7
Training loss: 2.418428897857666
Validation loss: 2.1616193274016022

Epoch: 6| Step: 8
Training loss: 2.7698516845703125
Validation loss: 2.1629101999344362

Epoch: 6| Step: 9
Training loss: 1.8853974342346191
Validation loss: 2.185535466799172

Epoch: 6| Step: 10
Training loss: 2.652477741241455
Validation loss: 2.1787688039964244

Epoch: 6| Step: 11
Training loss: 1.962504506111145
Validation loss: 2.1467847131913707

Epoch: 6| Step: 12
Training loss: 2.019894599914551
Validation loss: 2.146828395064159

Epoch: 6| Step: 13
Training loss: 2.2114098072052
Validation loss: 2.156019950425753

Epoch: 396| Step: 0
Training loss: 2.390815258026123
Validation loss: 2.1468115878361527

Epoch: 6| Step: 1
Training loss: 2.2601003646850586
Validation loss: 2.17892857520811

Epoch: 6| Step: 2
Training loss: 2.537764072418213
Validation loss: 2.1696829360018492

Epoch: 6| Step: 3
Training loss: 2.3659820556640625
Validation loss: 2.1492273012797036

Epoch: 6| Step: 4
Training loss: 2.9391236305236816
Validation loss: 2.153652257816766

Epoch: 6| Step: 5
Training loss: 2.063413143157959
Validation loss: 2.156151133198892

Epoch: 6| Step: 6
Training loss: 2.703477382659912
Validation loss: 2.1410738268206195

Epoch: 6| Step: 7
Training loss: 1.5977113246917725
Validation loss: 2.1367943825260287

Epoch: 6| Step: 8
Training loss: 1.7661640644073486
Validation loss: 2.1513925880514164

Epoch: 6| Step: 9
Training loss: 2.7608160972595215
Validation loss: 2.155793301520809

Epoch: 6| Step: 10
Training loss: 2.6771397590637207
Validation loss: 2.154724577421783

Epoch: 6| Step: 11
Training loss: 2.0754780769348145
Validation loss: 2.1470278565601637

Epoch: 6| Step: 12
Training loss: 1.555374264717102
Validation loss: 2.169616804328016

Epoch: 6| Step: 13
Training loss: 1.5450423955917358
Validation loss: 2.1601772462168047

Epoch: 397| Step: 0
Training loss: 2.8265180587768555
Validation loss: 2.181061403725737

Epoch: 6| Step: 1
Training loss: 2.2767834663391113
Validation loss: 2.1922310244652534

Epoch: 6| Step: 2
Training loss: 1.8682680130004883
Validation loss: 2.178991097275929

Epoch: 6| Step: 3
Training loss: 2.3882553577423096
Validation loss: 2.179604240643081

Epoch: 6| Step: 4
Training loss: 2.1786160469055176
Validation loss: 2.1695990690621

Epoch: 6| Step: 5
Training loss: 2.338188409805298
Validation loss: 2.1533825961492394

Epoch: 6| Step: 6
Training loss: 2.4238438606262207
Validation loss: 2.1612415441902737

Epoch: 6| Step: 7
Training loss: 2.0005640983581543
Validation loss: 2.155403624298752

Epoch: 6| Step: 8
Training loss: 2.470931053161621
Validation loss: 2.1558488286951536

Epoch: 6| Step: 9
Training loss: 1.6927342414855957
Validation loss: 2.1477143097949285

Epoch: 6| Step: 10
Training loss: 2.0550308227539062
Validation loss: 2.1660740708792083

Epoch: 6| Step: 11
Training loss: 2.052678346633911
Validation loss: 2.1609975279018445

Epoch: 6| Step: 12
Training loss: 2.962317943572998
Validation loss: 2.1576995618881716

Epoch: 6| Step: 13
Training loss: 1.742897868156433
Validation loss: 2.17644561747069

Epoch: 398| Step: 0
Training loss: 2.348733901977539
Validation loss: 2.1597734625621507

Epoch: 6| Step: 1
Training loss: 1.816386342048645
Validation loss: 2.1550668157557005

Epoch: 6| Step: 2
Training loss: 2.592329978942871
Validation loss: 2.1655176634429605

Epoch: 6| Step: 3
Training loss: 2.1188602447509766
Validation loss: 2.1411242869592484

Epoch: 6| Step: 4
Training loss: 2.517697811126709
Validation loss: 2.1432513524127264

Epoch: 6| Step: 5
Training loss: 2.421696662902832
Validation loss: 2.1482279403235323

Epoch: 6| Step: 6
Training loss: 1.689989447593689
Validation loss: 2.143290806842107

Epoch: 6| Step: 7
Training loss: 2.1350557804107666
Validation loss: 2.2007732519539456

Epoch: 6| Step: 8
Training loss: 2.2412593364715576
Validation loss: 2.1783956045745523

Epoch: 6| Step: 9
Training loss: 2.643334150314331
Validation loss: 2.174559445791347

Epoch: 6| Step: 10
Training loss: 1.7996799945831299
Validation loss: 2.1862800403307845

Epoch: 6| Step: 11
Training loss: 2.367967128753662
Validation loss: 2.1560776874583256

Epoch: 6| Step: 12
Training loss: 2.1462700366973877
Validation loss: 2.1661103233214347

Epoch: 6| Step: 13
Training loss: 2.9641337394714355
Validation loss: 2.1631161884594987

Epoch: 399| Step: 0
Training loss: 2.5401525497436523
Validation loss: 2.167984832999527

Epoch: 6| Step: 1
Training loss: 1.2625792026519775
Validation loss: 2.1851836148128716

Epoch: 6| Step: 2
Training loss: 1.8506495952606201
Validation loss: 2.1882957489259782

Epoch: 6| Step: 3
Training loss: 2.3120317459106445
Validation loss: 2.184487990153733

Epoch: 6| Step: 4
Training loss: 2.140501022338867
Validation loss: 2.1900109065476285

Epoch: 6| Step: 5
Training loss: 2.4954710006713867
Validation loss: 2.1790624639039398

Epoch: 6| Step: 6
Training loss: 2.607900619506836
Validation loss: 2.1993217327261485

Epoch: 6| Step: 7
Training loss: 2.829470634460449
Validation loss: 2.1882676975701445

Epoch: 6| Step: 8
Training loss: 1.7305233478546143
Validation loss: 2.181188719246977

Epoch: 6| Step: 9
Training loss: 2.136773109436035
Validation loss: 2.164845370477246

Epoch: 6| Step: 10
Training loss: 2.1368823051452637
Validation loss: 2.1752928431316088

Epoch: 6| Step: 11
Training loss: 2.5782129764556885
Validation loss: 2.172114941381639

Epoch: 6| Step: 12
Training loss: 2.5558109283447266
Validation loss: 2.1614345504391577

Epoch: 6| Step: 13
Training loss: 2.4261255264282227
Validation loss: 2.148716672774284

Epoch: 400| Step: 0
Training loss: 2.6499617099761963
Validation loss: 2.1492718265902613

Epoch: 6| Step: 1
Training loss: 1.6536697149276733
Validation loss: 2.1522425707950386

Epoch: 6| Step: 2
Training loss: 2.354783535003662
Validation loss: 2.156242148850554

Epoch: 6| Step: 3
Training loss: 2.40413761138916
Validation loss: 2.1597516857167727

Epoch: 6| Step: 4
Training loss: 2.535332202911377
Validation loss: 2.144986516685896

Epoch: 6| Step: 5
Training loss: 2.506263256072998
Validation loss: 2.147831852718066

Epoch: 6| Step: 6
Training loss: 2.027371883392334
Validation loss: 2.162916601345103

Epoch: 6| Step: 7
Training loss: 2.317582845687866
Validation loss: 2.130170327360912

Epoch: 6| Step: 8
Training loss: 2.280794143676758
Validation loss: 2.1318046610842467

Epoch: 6| Step: 9
Training loss: 1.9113819599151611
Validation loss: 2.1305200822891726

Epoch: 6| Step: 10
Training loss: 2.466834306716919
Validation loss: 2.1326200090428835

Epoch: 6| Step: 11
Training loss: 2.067265033721924
Validation loss: 2.1331216186605473

Epoch: 6| Step: 12
Training loss: 2.2203869819641113
Validation loss: 2.1476165607411373

Epoch: 6| Step: 13
Training loss: 2.330096483230591
Validation loss: 2.163585259068397

Epoch: 401| Step: 0
Training loss: 2.135899305343628
Validation loss: 2.154428279528054

Epoch: 6| Step: 1
Training loss: 2.73049259185791
Validation loss: 2.169149701313306

Epoch: 6| Step: 2
Training loss: 2.346165418624878
Validation loss: 2.166998750420027

Epoch: 6| Step: 3
Training loss: 2.0987133979797363
Validation loss: 2.178132854482179

Epoch: 6| Step: 4
Training loss: 2.173276424407959
Validation loss: 2.1877083316926034

Epoch: 6| Step: 5
Training loss: 2.615784168243408
Validation loss: 2.184525047579119

Epoch: 6| Step: 6
Training loss: 1.6213492155075073
Validation loss: 2.172319499395227

Epoch: 6| Step: 7
Training loss: 2.697272300720215
Validation loss: 2.193131387874644

Epoch: 6| Step: 8
Training loss: 2.074636936187744
Validation loss: 2.182747471717096

Epoch: 6| Step: 9
Training loss: 2.390563726425171
Validation loss: 2.1713847960195234

Epoch: 6| Step: 10
Training loss: 1.8925402164459229
Validation loss: 2.1853712835619525

Epoch: 6| Step: 11
Training loss: 2.1093344688415527
Validation loss: 2.1733033426346315

Epoch: 6| Step: 12
Training loss: 1.9745652675628662
Validation loss: 2.185085197930695

Epoch: 6| Step: 13
Training loss: 2.853440999984741
Validation loss: 2.182914178858521

Epoch: 402| Step: 0
Training loss: 2.392174243927002
Validation loss: 2.201938321513514

Epoch: 6| Step: 1
Training loss: 3.044562578201294
Validation loss: 2.173254128425352

Epoch: 6| Step: 2
Training loss: 2.0235097408294678
Validation loss: 2.200010730374244

Epoch: 6| Step: 3
Training loss: 2.971796751022339
Validation loss: 2.1817450792558732

Epoch: 6| Step: 4
Training loss: 2.4093923568725586
Validation loss: 2.189930961978051

Epoch: 6| Step: 5
Training loss: 1.9119412899017334
Validation loss: 2.169187940577025

Epoch: 6| Step: 6
Training loss: 2.400726318359375
Validation loss: 2.1722304590286745

Epoch: 6| Step: 7
Training loss: 2.252802610397339
Validation loss: 2.1619317326494443

Epoch: 6| Step: 8
Training loss: 2.008612632751465
Validation loss: 2.138503255382661

Epoch: 6| Step: 9
Training loss: 1.8039637804031372
Validation loss: 2.1658910602651615

Epoch: 6| Step: 10
Training loss: 1.961927056312561
Validation loss: 2.1639188169151224

Epoch: 6| Step: 11
Training loss: 2.514230489730835
Validation loss: 2.157394806543986

Epoch: 6| Step: 12
Training loss: 2.2962393760681152
Validation loss: 2.175676399661649

Epoch: 6| Step: 13
Training loss: 1.781868577003479
Validation loss: 2.167844835148063

Epoch: 403| Step: 0
Training loss: 2.092423915863037
Validation loss: 2.175651632329469

Epoch: 6| Step: 1
Training loss: 1.97718346118927
Validation loss: 2.1689076244190173

Epoch: 6| Step: 2
Training loss: 2.554927349090576
Validation loss: 2.1751553627752487

Epoch: 6| Step: 3
Training loss: 2.2194039821624756
Validation loss: 2.1756221889167704

Epoch: 6| Step: 4
Training loss: 2.194218873977661
Validation loss: 2.1713982525692193

Epoch: 6| Step: 5
Training loss: 1.9235515594482422
Validation loss: 2.1682105654029438

Epoch: 6| Step: 6
Training loss: 2.241061210632324
Validation loss: 2.1713480923765447

Epoch: 6| Step: 7
Training loss: 2.1209335327148438
Validation loss: 2.1520840967855146

Epoch: 6| Step: 8
Training loss: 2.542984962463379
Validation loss: 2.161790012031473

Epoch: 6| Step: 9
Training loss: 2.117760181427002
Validation loss: 2.1584885325483096

Epoch: 6| Step: 10
Training loss: 1.488640546798706
Validation loss: 2.1571387424263904

Epoch: 6| Step: 11
Training loss: 2.261387825012207
Validation loss: 2.138887083658608

Epoch: 6| Step: 12
Training loss: 3.169917583465576
Validation loss: 2.1394754661026822

Epoch: 6| Step: 13
Training loss: 2.657641887664795
Validation loss: 2.126115342622162

Epoch: 404| Step: 0
Training loss: 2.0833168029785156
Validation loss: 2.121491071998432

Epoch: 6| Step: 1
Training loss: 2.1397409439086914
Validation loss: 2.129583630510556

Epoch: 6| Step: 2
Training loss: 2.2232604026794434
Validation loss: 2.1275616563776487

Epoch: 6| Step: 3
Training loss: 2.5968291759490967
Validation loss: 2.129403172000762

Epoch: 6| Step: 4
Training loss: 2.1817970275878906
Validation loss: 2.1515911958550893

Epoch: 6| Step: 5
Training loss: 3.3296327590942383
Validation loss: 2.146143827387082

Epoch: 6| Step: 6
Training loss: 1.9784170389175415
Validation loss: 2.1402740914334535

Epoch: 6| Step: 7
Training loss: 2.370450973510742
Validation loss: 2.1066916091467744

Epoch: 6| Step: 8
Training loss: 1.8059426546096802
Validation loss: 2.121442538435741

Epoch: 6| Step: 9
Training loss: 2.5616278648376465
Validation loss: 2.123856270185081

Epoch: 6| Step: 10
Training loss: 2.5400800704956055
Validation loss: 2.1230069693698677

Epoch: 6| Step: 11
Training loss: 2.0842199325561523
Validation loss: 2.1457175567585933

Epoch: 6| Step: 12
Training loss: 1.2977313995361328
Validation loss: 2.1239622510889524

Epoch: 6| Step: 13
Training loss: 2.5122666358947754
Validation loss: 2.145981013133962

Epoch: 405| Step: 0
Training loss: 2.1098108291625977
Validation loss: 2.134317180161835

Epoch: 6| Step: 1
Training loss: 2.635211944580078
Validation loss: 2.1207664397455033

Epoch: 6| Step: 2
Training loss: 2.343895435333252
Validation loss: 2.125297684823313

Epoch: 6| Step: 3
Training loss: 2.5196101665496826
Validation loss: 2.121336724168511

Epoch: 6| Step: 4
Training loss: 1.8088181018829346
Validation loss: 2.1299674639137844

Epoch: 6| Step: 5
Training loss: 1.9954440593719482
Validation loss: 2.1197084188461304

Epoch: 6| Step: 6
Training loss: 2.9182238578796387
Validation loss: 2.1267767131969495

Epoch: 6| Step: 7
Training loss: 1.9818806648254395
Validation loss: 2.1222645364781862

Epoch: 6| Step: 8
Training loss: 1.7240641117095947
Validation loss: 2.126169894331245

Epoch: 6| Step: 9
Training loss: 2.2803444862365723
Validation loss: 2.1192488362712245

Epoch: 6| Step: 10
Training loss: 2.97578501701355
Validation loss: 2.1376397455892255

Epoch: 6| Step: 11
Training loss: 2.5114071369171143
Validation loss: 2.133523924376375

Epoch: 6| Step: 12
Training loss: 1.5706565380096436
Validation loss: 2.140555718893646

Epoch: 6| Step: 13
Training loss: 1.9765459299087524
Validation loss: 2.143997202637375

Epoch: 406| Step: 0
Training loss: 2.7190046310424805
Validation loss: 2.153944753831433

Epoch: 6| Step: 1
Training loss: 2.423389434814453
Validation loss: 2.1508990128835044

Epoch: 6| Step: 2
Training loss: 2.034043312072754
Validation loss: 2.143958612154889

Epoch: 6| Step: 3
Training loss: 2.1226983070373535
Validation loss: 2.1545390698217575

Epoch: 6| Step: 4
Training loss: 1.5834505558013916
Validation loss: 2.1531132036639797

Epoch: 6| Step: 5
Training loss: 2.463298797607422
Validation loss: 2.16225536664327

Epoch: 6| Step: 6
Training loss: 2.6637558937072754
Validation loss: 2.1673427448477796

Epoch: 6| Step: 7
Training loss: 2.15602445602417
Validation loss: 2.1745386892749416

Epoch: 6| Step: 8
Training loss: 1.9566984176635742
Validation loss: 2.1823286292373494

Epoch: 6| Step: 9
Training loss: 2.068169116973877
Validation loss: 2.192229927227061

Epoch: 6| Step: 10
Training loss: 1.9188215732574463
Validation loss: 2.1694206550557125

Epoch: 6| Step: 11
Training loss: 2.377206325531006
Validation loss: 2.169819506265784

Epoch: 6| Step: 12
Training loss: 2.3427717685699463
Validation loss: 2.15417996785974

Epoch: 6| Step: 13
Training loss: 2.699740409851074
Validation loss: 2.1767651137485298

Epoch: 407| Step: 0
Training loss: 2.0862035751342773
Validation loss: 2.17813431063006

Epoch: 6| Step: 1
Training loss: 2.3702890872955322
Validation loss: 2.1639676478601273

Epoch: 6| Step: 2
Training loss: 2.5704753398895264
Validation loss: 2.147487135343654

Epoch: 6| Step: 3
Training loss: 2.433185577392578
Validation loss: 2.144215203100635

Epoch: 6| Step: 4
Training loss: 2.442858934402466
Validation loss: 2.14738396675356

Epoch: 6| Step: 5
Training loss: 2.5581016540527344
Validation loss: 2.1317082605054303

Epoch: 6| Step: 6
Training loss: 1.6454862356185913
Validation loss: 2.1293186577417518

Epoch: 6| Step: 7
Training loss: 2.177095413208008
Validation loss: 2.1242311616097727

Epoch: 6| Step: 8
Training loss: 1.964741826057434
Validation loss: 2.109295580976753

Epoch: 6| Step: 9
Training loss: 2.1690943241119385
Validation loss: 2.1263902597529913

Epoch: 6| Step: 10
Training loss: 2.0320401191711426
Validation loss: 2.1329772523654404

Epoch: 6| Step: 11
Training loss: 2.4359521865844727
Validation loss: 2.1396524495975946

Epoch: 6| Step: 12
Training loss: 2.1352744102478027
Validation loss: 2.1557244664879254

Epoch: 6| Step: 13
Training loss: 2.317337989807129
Validation loss: 2.1852015256881714

Epoch: 408| Step: 0
Training loss: 2.023486614227295
Validation loss: 2.166842301686605

Epoch: 6| Step: 1
Training loss: 2.2734639644622803
Validation loss: 2.1761772504416843

Epoch: 6| Step: 2
Training loss: 1.9571374654769897
Validation loss: 2.198162971004363

Epoch: 6| Step: 3
Training loss: 3.3153514862060547
Validation loss: 2.1954082699232202

Epoch: 6| Step: 4
Training loss: 2.3030219078063965
Validation loss: 2.193408663554858

Epoch: 6| Step: 5
Training loss: 2.67525577545166
Validation loss: 2.1940045715660177

Epoch: 6| Step: 6
Training loss: 2.179918050765991
Validation loss: 2.167690405281641

Epoch: 6| Step: 7
Training loss: 2.6652727127075195
Validation loss: 2.1677966348586546

Epoch: 6| Step: 8
Training loss: 1.745793104171753
Validation loss: 2.1514047858535603

Epoch: 6| Step: 9
Training loss: 1.6436115503311157
Validation loss: 2.1583424947595082

Epoch: 6| Step: 10
Training loss: 1.644464135169983
Validation loss: 2.1645786352055048

Epoch: 6| Step: 11
Training loss: 2.6110129356384277
Validation loss: 2.1597185045160274

Epoch: 6| Step: 12
Training loss: 2.3134140968322754
Validation loss: 2.1525806688493296

Epoch: 6| Step: 13
Training loss: 1.9495446681976318
Validation loss: 2.1282656679871264

Epoch: 409| Step: 0
Training loss: 2.5051677227020264
Validation loss: 2.1213591893514

Epoch: 6| Step: 1
Training loss: 2.0715808868408203
Validation loss: 2.1196583906809487

Epoch: 6| Step: 2
Training loss: 1.4618816375732422
Validation loss: 2.1185331165149646

Epoch: 6| Step: 3
Training loss: 1.8650295734405518
Validation loss: 2.120598413610971

Epoch: 6| Step: 4
Training loss: 2.3169350624084473
Validation loss: 2.1379961249648884

Epoch: 6| Step: 5
Training loss: 1.680612325668335
Validation loss: 2.137758816442182

Epoch: 6| Step: 6
Training loss: 2.4805705547332764
Validation loss: 2.137834623295774

Epoch: 6| Step: 7
Training loss: 1.905061960220337
Validation loss: 2.1385736798727386

Epoch: 6| Step: 8
Training loss: 2.50590443611145
Validation loss: 2.152185700273001

Epoch: 6| Step: 9
Training loss: 2.422104597091675
Validation loss: 2.1629658129907425

Epoch: 6| Step: 10
Training loss: 2.840865135192871
Validation loss: 2.1600388814044256

Epoch: 6| Step: 11
Training loss: 2.3625612258911133
Validation loss: 2.188745626839258

Epoch: 6| Step: 12
Training loss: 2.4797544479370117
Validation loss: 2.2020232703096125

Epoch: 6| Step: 13
Training loss: 2.854997158050537
Validation loss: 2.195573114579724

Epoch: 410| Step: 0
Training loss: 2.0450873374938965
Validation loss: 2.172918096665413

Epoch: 6| Step: 1
Training loss: 3.002840042114258
Validation loss: 2.1580769477352018

Epoch: 6| Step: 2
Training loss: 2.3824329376220703
Validation loss: 2.1711916308249197

Epoch: 6| Step: 3
Training loss: 2.0840201377868652
Validation loss: 2.1299166064108572

Epoch: 6| Step: 4
Training loss: 1.63446044921875
Validation loss: 2.127115270142914

Epoch: 6| Step: 5
Training loss: 2.751852035522461
Validation loss: 2.1292685308764057

Epoch: 6| Step: 6
Training loss: 2.165163993835449
Validation loss: 2.113218366458852

Epoch: 6| Step: 7
Training loss: 2.2061877250671387
Validation loss: 2.128213151808708

Epoch: 6| Step: 8
Training loss: 2.6560540199279785
Validation loss: 2.118623584829351

Epoch: 6| Step: 9
Training loss: 2.3858072757720947
Validation loss: 2.122867331709913

Epoch: 6| Step: 10
Training loss: 2.405470848083496
Validation loss: 2.0954376369394283

Epoch: 6| Step: 11
Training loss: 1.851534366607666
Validation loss: 2.128524778991617

Epoch: 6| Step: 12
Training loss: 2.017974853515625
Validation loss: 2.1344331490096224

Epoch: 6| Step: 13
Training loss: 1.8784070014953613
Validation loss: 2.1549676938723494

Epoch: 411| Step: 0
Training loss: 1.9969196319580078
Validation loss: 2.1390431555368568

Epoch: 6| Step: 1
Training loss: 2.237637519836426
Validation loss: 2.1585625243443314

Epoch: 6| Step: 2
Training loss: 2.269196033477783
Validation loss: 2.1578106213641424

Epoch: 6| Step: 3
Training loss: 2.44612455368042
Validation loss: 2.168006222735169

Epoch: 6| Step: 4
Training loss: 1.8194265365600586
Validation loss: 2.1503942243514524

Epoch: 6| Step: 5
Training loss: 2.463932991027832
Validation loss: 2.1437737262377174

Epoch: 6| Step: 6
Training loss: 2.8605613708496094
Validation loss: 2.157354372803883

Epoch: 6| Step: 7
Training loss: 1.873439073562622
Validation loss: 2.154108449976931

Epoch: 6| Step: 8
Training loss: 1.974137544631958
Validation loss: 2.1283721846918904

Epoch: 6| Step: 9
Training loss: 2.319934368133545
Validation loss: 2.1336414775540753

Epoch: 6| Step: 10
Training loss: 1.9767102003097534
Validation loss: 2.135342820998161

Epoch: 6| Step: 11
Training loss: 2.5146474838256836
Validation loss: 2.1210619582924792

Epoch: 6| Step: 12
Training loss: 2.3733551502227783
Validation loss: 2.116515303170809

Epoch: 6| Step: 13
Training loss: 1.9770410060882568
Validation loss: 2.117294439705469

Epoch: 412| Step: 0
Training loss: 2.8152737617492676
Validation loss: 2.1352694983123452

Epoch: 6| Step: 1
Training loss: 2.3399691581726074
Validation loss: 2.123316134175947

Epoch: 6| Step: 2
Training loss: 2.0863709449768066
Validation loss: 2.113220686553627

Epoch: 6| Step: 3
Training loss: 1.5548335313796997
Validation loss: 2.1206371348391295

Epoch: 6| Step: 4
Training loss: 2.045886993408203
Validation loss: 2.1400749683380127

Epoch: 6| Step: 5
Training loss: 1.87603759765625
Validation loss: 2.160160836353097

Epoch: 6| Step: 6
Training loss: 2.8552770614624023
Validation loss: 2.140799691600184

Epoch: 6| Step: 7
Training loss: 2.0145487785339355
Validation loss: 2.168755503110988

Epoch: 6| Step: 8
Training loss: 1.8716918230056763
Validation loss: 2.1622767243334042

Epoch: 6| Step: 9
Training loss: 2.1502625942230225
Validation loss: 2.143175548122775

Epoch: 6| Step: 10
Training loss: 2.000944137573242
Validation loss: 2.156741331982356

Epoch: 6| Step: 11
Training loss: 2.4488754272460938
Validation loss: 2.1395299729480537

Epoch: 6| Step: 12
Training loss: 2.698291778564453
Validation loss: 2.146600333593225

Epoch: 6| Step: 13
Training loss: 2.393693685531616
Validation loss: 2.1470875637505644

Epoch: 413| Step: 0
Training loss: 2.503575086593628
Validation loss: 2.1534975459498744

Epoch: 6| Step: 1
Training loss: 2.4714784622192383
Validation loss: 2.130802262213922

Epoch: 6| Step: 2
Training loss: 2.057603359222412
Validation loss: 2.1197647766400407

Epoch: 6| Step: 3
Training loss: 1.9895682334899902
Validation loss: 2.114924415465324

Epoch: 6| Step: 4
Training loss: 1.7667402029037476
Validation loss: 2.0999012224135862

Epoch: 6| Step: 5
Training loss: 2.2421388626098633
Validation loss: 2.106752516120993

Epoch: 6| Step: 6
Training loss: 3.242901086807251
Validation loss: 2.1109021761084117

Epoch: 6| Step: 7
Training loss: 1.9245216846466064
Validation loss: 2.104901970073741

Epoch: 6| Step: 8
Training loss: 2.121708869934082
Validation loss: 2.1142387531136952

Epoch: 6| Step: 9
Training loss: 2.770599842071533
Validation loss: 2.130921225393972

Epoch: 6| Step: 10
Training loss: 1.5039784908294678
Validation loss: 2.1328667466358473

Epoch: 6| Step: 11
Training loss: 1.9016326665878296
Validation loss: 2.1292585583143335

Epoch: 6| Step: 12
Training loss: 2.308380603790283
Validation loss: 2.1122564833651305

Epoch: 6| Step: 13
Training loss: 2.536332845687866
Validation loss: 2.1674406579745713

Epoch: 414| Step: 0
Training loss: 2.010301113128662
Validation loss: 2.1547887120195615

Epoch: 6| Step: 1
Training loss: 2.3698370456695557
Validation loss: 2.1558739728825067

Epoch: 6| Step: 2
Training loss: 2.3128552436828613
Validation loss: 2.141676897643715

Epoch: 6| Step: 3
Training loss: 2.4944510459899902
Validation loss: 2.152339848138953

Epoch: 6| Step: 4
Training loss: 2.110030174255371
Validation loss: 2.154443687008273

Epoch: 6| Step: 5
Training loss: 2.180542469024658
Validation loss: 2.1407188907746346

Epoch: 6| Step: 6
Training loss: 2.461782455444336
Validation loss: 2.172148855783606

Epoch: 6| Step: 7
Training loss: 2.2593483924865723
Validation loss: 2.1436703987019037

Epoch: 6| Step: 8
Training loss: 1.808060646057129
Validation loss: 2.141969657713367

Epoch: 6| Step: 9
Training loss: 2.840406894683838
Validation loss: 2.1417704987269577

Epoch: 6| Step: 10
Training loss: 1.9090672731399536
Validation loss: 2.134945748954691

Epoch: 6| Step: 11
Training loss: 2.175863742828369
Validation loss: 2.1345865726470947

Epoch: 6| Step: 12
Training loss: 2.4817349910736084
Validation loss: 2.138862056116904

Epoch: 6| Step: 13
Training loss: 1.609028935432434
Validation loss: 2.131481155272453

Epoch: 415| Step: 0
Training loss: 2.696953535079956
Validation loss: 2.175001767373854

Epoch: 6| Step: 1
Training loss: 2.764496326446533
Validation loss: 2.175630925804056

Epoch: 6| Step: 2
Training loss: 1.8950082063674927
Validation loss: 2.172368857168382

Epoch: 6| Step: 3
Training loss: 2.3983640670776367
Validation loss: 2.1909203901085803

Epoch: 6| Step: 4
Training loss: 2.039745807647705
Validation loss: 2.1700002339578446

Epoch: 6| Step: 5
Training loss: 1.7941151857376099
Validation loss: 2.1541184674027147

Epoch: 6| Step: 6
Training loss: 2.2536661624908447
Validation loss: 2.1467627786820933

Epoch: 6| Step: 7
Training loss: 2.2070016860961914
Validation loss: 2.1390129635410924

Epoch: 6| Step: 8
Training loss: 1.87541925907135
Validation loss: 2.1415088792001047

Epoch: 6| Step: 9
Training loss: 1.927670955657959
Validation loss: 2.130402764966411

Epoch: 6| Step: 10
Training loss: 2.090972661972046
Validation loss: 2.1410418710400982

Epoch: 6| Step: 11
Training loss: 2.2901570796966553
Validation loss: 2.131100740484012

Epoch: 6| Step: 12
Training loss: 2.791792869567871
Validation loss: 2.1327969951014363

Epoch: 6| Step: 13
Training loss: 2.1921370029449463
Validation loss: 2.1388018900348293

Epoch: 416| Step: 0
Training loss: 1.860392451286316
Validation loss: 2.1168722824383805

Epoch: 6| Step: 1
Training loss: 1.9238121509552002
Validation loss: 2.114306124307776

Epoch: 6| Step: 2
Training loss: 1.9224002361297607
Validation loss: 2.134635358728388

Epoch: 6| Step: 3
Training loss: 2.1863226890563965
Validation loss: 2.125051047212334

Epoch: 6| Step: 4
Training loss: 1.6183487176895142
Validation loss: 2.1503443923047794

Epoch: 6| Step: 5
Training loss: 3.2299790382385254
Validation loss: 2.128783115776636

Epoch: 6| Step: 6
Training loss: 2.5248894691467285
Validation loss: 2.119623330331618

Epoch: 6| Step: 7
Training loss: 1.794281005859375
Validation loss: 2.138599411133797

Epoch: 6| Step: 8
Training loss: 2.0860259532928467
Validation loss: 2.1199096659178376

Epoch: 6| Step: 9
Training loss: 2.3029773235321045
Validation loss: 2.119106308106453

Epoch: 6| Step: 10
Training loss: 2.5780577659606934
Validation loss: 2.129748782803935

Epoch: 6| Step: 11
Training loss: 2.879049777984619
Validation loss: 2.1250193990686888

Epoch: 6| Step: 12
Training loss: 1.8226587772369385
Validation loss: 2.1347743490690827

Epoch: 6| Step: 13
Training loss: 2.1347978115081787
Validation loss: 2.1378997654043217

Epoch: 417| Step: 0
Training loss: 2.0571951866149902
Validation loss: 2.1379926153408584

Epoch: 6| Step: 1
Training loss: 2.45379638671875
Validation loss: 2.1286776129917433

Epoch: 6| Step: 2
Training loss: 1.7705814838409424
Validation loss: 2.118483938196654

Epoch: 6| Step: 3
Training loss: 2.2451012134552
Validation loss: 2.130242966836499

Epoch: 6| Step: 4
Training loss: 3.0998220443725586
Validation loss: 2.1240111320249495

Epoch: 6| Step: 5
Training loss: 3.046414375305176
Validation loss: 2.1143566972465924

Epoch: 6| Step: 6
Training loss: 1.5318999290466309
Validation loss: 2.1205037921987553

Epoch: 6| Step: 7
Training loss: 2.207662582397461
Validation loss: 2.122206805854715

Epoch: 6| Step: 8
Training loss: 2.014711856842041
Validation loss: 2.107641089347101

Epoch: 6| Step: 9
Training loss: 2.277559280395508
Validation loss: 2.109769537884702

Epoch: 6| Step: 10
Training loss: 2.0633599758148193
Validation loss: 2.1153202415794454

Epoch: 6| Step: 11
Training loss: 2.213240146636963
Validation loss: 2.11661276766049

Epoch: 6| Step: 12
Training loss: 2.0697875022888184
Validation loss: 2.1252180786542993

Epoch: 6| Step: 13
Training loss: 1.5605777502059937
Validation loss: 2.145839609125609

Epoch: 418| Step: 0
Training loss: 2.115424633026123
Validation loss: 2.1083232869384108

Epoch: 6| Step: 1
Training loss: 3.144674301147461
Validation loss: 2.118322482673071

Epoch: 6| Step: 2
Training loss: 2.09615421295166
Validation loss: 2.109676017556139

Epoch: 6| Step: 3
Training loss: 1.7564061880111694
Validation loss: 2.121258975357138

Epoch: 6| Step: 4
Training loss: 1.6230493783950806
Validation loss: 2.11081503411775

Epoch: 6| Step: 5
Training loss: 2.5064125061035156
Validation loss: 2.0947856159620386

Epoch: 6| Step: 6
Training loss: 1.9909923076629639
Validation loss: 2.104642791132773

Epoch: 6| Step: 7
Training loss: 2.093088388442993
Validation loss: 2.110438228935324

Epoch: 6| Step: 8
Training loss: 2.348238468170166
Validation loss: 2.1087976899198306

Epoch: 6| Step: 9
Training loss: 2.42848539352417
Validation loss: 2.1119153755967335

Epoch: 6| Step: 10
Training loss: 3.0949649810791016
Validation loss: 2.1168036614694903

Epoch: 6| Step: 11
Training loss: 1.9336318969726562
Validation loss: 2.0960174324691936

Epoch: 6| Step: 12
Training loss: 1.7899688482284546
Validation loss: 2.133024372080321

Epoch: 6| Step: 13
Training loss: 1.9395215511322021
Validation loss: 2.153993523249062

Epoch: 419| Step: 0
Training loss: 1.9801539182662964
Validation loss: 2.163516472744685

Epoch: 6| Step: 1
Training loss: 2.380746364593506
Validation loss: 2.1471869317434167

Epoch: 6| Step: 2
Training loss: 1.2844825983047485
Validation loss: 2.1648430824279785

Epoch: 6| Step: 3
Training loss: 1.695408582687378
Validation loss: 2.1498994852906916

Epoch: 6| Step: 4
Training loss: 2.845111608505249
Validation loss: 2.128246904701315

Epoch: 6| Step: 5
Training loss: 2.768242835998535
Validation loss: 2.131185785416634

Epoch: 6| Step: 6
Training loss: 2.295283794403076
Validation loss: 2.10274217718391

Epoch: 6| Step: 7
Training loss: 2.7522239685058594
Validation loss: 2.1176091983754146

Epoch: 6| Step: 8
Training loss: 1.9520236253738403
Validation loss: 2.126694363932456

Epoch: 6| Step: 9
Training loss: 1.821073293685913
Validation loss: 2.133388833333087

Epoch: 6| Step: 10
Training loss: 2.8216090202331543
Validation loss: 2.1530953145796254

Epoch: 6| Step: 11
Training loss: 1.9229861497879028
Validation loss: 2.154978677790652

Epoch: 6| Step: 12
Training loss: 2.5488760471343994
Validation loss: 2.1770848663904334

Epoch: 6| Step: 13
Training loss: 1.941407561302185
Validation loss: 2.155825217564901

Epoch: 420| Step: 0
Training loss: 2.309553384780884
Validation loss: 2.1781686711054977

Epoch: 6| Step: 1
Training loss: 2.227865219116211
Validation loss: 2.158601091754052

Epoch: 6| Step: 2
Training loss: 1.7629420757293701
Validation loss: 2.147364775339762

Epoch: 6| Step: 3
Training loss: 2.507341146469116
Validation loss: 2.1684244409684212

Epoch: 6| Step: 4
Training loss: 2.264604330062866
Validation loss: 2.1409212120117678

Epoch: 6| Step: 5
Training loss: 1.805649757385254
Validation loss: 2.1405276560014292

Epoch: 6| Step: 6
Training loss: 2.225060224533081
Validation loss: 2.1343504510900027

Epoch: 6| Step: 7
Training loss: 1.7533745765686035
Validation loss: 2.122989900650517

Epoch: 6| Step: 8
Training loss: 2.380636215209961
Validation loss: 2.130434133673227

Epoch: 6| Step: 9
Training loss: 1.9378631114959717
Validation loss: 2.1321877612862536

Epoch: 6| Step: 10
Training loss: 1.607009768486023
Validation loss: 2.1332930852008123

Epoch: 6| Step: 11
Training loss: 3.0062143802642822
Validation loss: 2.121893839169574

Epoch: 6| Step: 12
Training loss: 2.5184311866760254
Validation loss: 2.106698941158992

Epoch: 6| Step: 13
Training loss: 2.9547550678253174
Validation loss: 2.1057705674120175

Epoch: 421| Step: 0
Training loss: 2.5697731971740723
Validation loss: 2.118644957901329

Epoch: 6| Step: 1
Training loss: 2.1991138458251953
Validation loss: 2.0936486874857256

Epoch: 6| Step: 2
Training loss: 2.4236860275268555
Validation loss: 2.1082683019740607

Epoch: 6| Step: 3
Training loss: 3.030860185623169
Validation loss: 2.1073644802134526

Epoch: 6| Step: 4
Training loss: 2.223454475402832
Validation loss: 2.100718594366504

Epoch: 6| Step: 5
Training loss: 1.92996346950531
Validation loss: 2.105498424140356

Epoch: 6| Step: 6
Training loss: 1.5021388530731201
Validation loss: 2.1144467963967273

Epoch: 6| Step: 7
Training loss: 2.550717830657959
Validation loss: 2.117048280213469

Epoch: 6| Step: 8
Training loss: 2.0990254878997803
Validation loss: 2.1399100031903995

Epoch: 6| Step: 9
Training loss: 1.9524619579315186
Validation loss: 2.1513378133055983

Epoch: 6| Step: 10
Training loss: 2.8852691650390625
Validation loss: 2.1506467147540023

Epoch: 6| Step: 11
Training loss: 1.8525166511535645
Validation loss: 2.152300618028128

Epoch: 6| Step: 12
Training loss: 2.3682992458343506
Validation loss: 2.1426714261372886

Epoch: 6| Step: 13
Training loss: 0.8299877643585205
Validation loss: 2.137255145657447

Epoch: 422| Step: 0
Training loss: 1.9048233032226562
Validation loss: 2.1337366950127388

Epoch: 6| Step: 1
Training loss: 2.657381534576416
Validation loss: 2.1327619116793395

Epoch: 6| Step: 2
Training loss: 2.161817789077759
Validation loss: 2.1182580891475884

Epoch: 6| Step: 3
Training loss: 2.186495780944824
Validation loss: 2.091613018384544

Epoch: 6| Step: 4
Training loss: 1.6236337423324585
Validation loss: 2.1031593968791347

Epoch: 6| Step: 5
Training loss: 2.0451133251190186
Validation loss: 2.098581306395992

Epoch: 6| Step: 6
Training loss: 2.2031240463256836
Validation loss: 2.097840448861481

Epoch: 6| Step: 7
Training loss: 2.614037275314331
Validation loss: 2.0929012452402422

Epoch: 6| Step: 8
Training loss: 1.8458216190338135
Validation loss: 2.088873945256715

Epoch: 6| Step: 9
Training loss: 2.229555606842041
Validation loss: 2.0988518986650693

Epoch: 6| Step: 10
Training loss: 2.724212646484375
Validation loss: 2.119785862584268

Epoch: 6| Step: 11
Training loss: 2.537874221801758
Validation loss: 2.120546876743276

Epoch: 6| Step: 12
Training loss: 2.231706380844116
Validation loss: 2.1204734245936074

Epoch: 6| Step: 13
Training loss: 1.5357095003128052
Validation loss: 2.1362345859568608

Epoch: 423| Step: 0
Training loss: 1.9677940607070923
Validation loss: 2.151825338281611

Epoch: 6| Step: 1
Training loss: 2.007441759109497
Validation loss: 2.1400166519226564

Epoch: 6| Step: 2
Training loss: 2.1156842708587646
Validation loss: 2.131891841529518

Epoch: 6| Step: 3
Training loss: 1.4105165004730225
Validation loss: 2.142130753045441

Epoch: 6| Step: 4
Training loss: 2.681004524230957
Validation loss: 2.1443841457366943

Epoch: 6| Step: 5
Training loss: 2.142221212387085
Validation loss: 2.1385650686038438

Epoch: 6| Step: 6
Training loss: 2.3556432723999023
Validation loss: 2.1239470717727498

Epoch: 6| Step: 7
Training loss: 1.698575496673584
Validation loss: 2.1275783610600296

Epoch: 6| Step: 8
Training loss: 2.4022414684295654
Validation loss: 2.1232958506512385

Epoch: 6| Step: 9
Training loss: 2.9121012687683105
Validation loss: 2.1247389995923607

Epoch: 6| Step: 10
Training loss: 2.371922016143799
Validation loss: 2.1365538156160744

Epoch: 6| Step: 11
Training loss: 2.9890058040618896
Validation loss: 2.1194580831835346

Epoch: 6| Step: 12
Training loss: 1.5392024517059326
Validation loss: 2.1234595442330964

Epoch: 6| Step: 13
Training loss: 2.4441823959350586
Validation loss: 2.116787028569047

Epoch: 424| Step: 0
Training loss: 1.8560044765472412
Validation loss: 2.1126382607285694

Epoch: 6| Step: 1
Training loss: 2.7383475303649902
Validation loss: 2.115250454154066

Epoch: 6| Step: 2
Training loss: 1.9220821857452393
Validation loss: 2.124051358110161

Epoch: 6| Step: 3
Training loss: 1.8012884855270386
Validation loss: 2.106996702891524

Epoch: 6| Step: 4
Training loss: 1.7902164459228516
Validation loss: 2.0999741400441816

Epoch: 6| Step: 5
Training loss: 2.2393651008605957
Validation loss: 2.1106288086983467

Epoch: 6| Step: 6
Training loss: 2.4974679946899414
Validation loss: 2.112786659630396

Epoch: 6| Step: 7
Training loss: 2.5291507244110107
Validation loss: 2.104584760563348

Epoch: 6| Step: 8
Training loss: 2.8521342277526855
Validation loss: 2.1020289633863714

Epoch: 6| Step: 9
Training loss: 1.634381890296936
Validation loss: 2.1165797351509013

Epoch: 6| Step: 10
Training loss: 1.865037202835083
Validation loss: 2.0945402550440964

Epoch: 6| Step: 11
Training loss: 2.17950439453125
Validation loss: 2.1040394357455674

Epoch: 6| Step: 12
Training loss: 2.528097629547119
Validation loss: 2.1040003953441495

Epoch: 6| Step: 13
Training loss: 2.5347931385040283
Validation loss: 2.1007660832456363

Epoch: 425| Step: 0
Training loss: 1.533307433128357
Validation loss: 2.1096508349141767

Epoch: 6| Step: 1
Training loss: 2.0480031967163086
Validation loss: 2.1072639521732124

Epoch: 6| Step: 2
Training loss: 2.870203733444214
Validation loss: 2.0937004217537503

Epoch: 6| Step: 3
Training loss: 3.0467591285705566
Validation loss: 2.099176155623569

Epoch: 6| Step: 4
Training loss: 2.3868722915649414
Validation loss: 2.1049501229357976

Epoch: 6| Step: 5
Training loss: 2.1573472023010254
Validation loss: 2.102308891152823

Epoch: 6| Step: 6
Training loss: 2.232097625732422
Validation loss: 2.1067986872888382

Epoch: 6| Step: 7
Training loss: 1.7859327793121338
Validation loss: 2.117978270335864

Epoch: 6| Step: 8
Training loss: 2.0004212856292725
Validation loss: 2.1274574366948937

Epoch: 6| Step: 9
Training loss: 2.509410858154297
Validation loss: 2.130080492265763

Epoch: 6| Step: 10
Training loss: 2.048069477081299
Validation loss: 2.1131009286449802

Epoch: 6| Step: 11
Training loss: 1.9746770858764648
Validation loss: 2.1215197681098856

Epoch: 6| Step: 12
Training loss: 2.520998954772949
Validation loss: 2.115238715243596

Epoch: 6| Step: 13
Training loss: 1.1047377586364746
Validation loss: 2.1122772309087936

Epoch: 426| Step: 0
Training loss: 2.1325230598449707
Validation loss: 2.122922138501239

Epoch: 6| Step: 1
Training loss: 2.958624839782715
Validation loss: 2.1103380264774447

Epoch: 6| Step: 2
Training loss: 1.969456434249878
Validation loss: 2.0987326099026586

Epoch: 6| Step: 3
Training loss: 2.5021915435791016
Validation loss: 2.1040562173371673

Epoch: 6| Step: 4
Training loss: 1.829916000366211
Validation loss: 2.0872452425700363

Epoch: 6| Step: 5
Training loss: 2.1804885864257812
Validation loss: 2.1050893286223054

Epoch: 6| Step: 6
Training loss: 2.017089605331421
Validation loss: 2.1034835128374

Epoch: 6| Step: 7
Training loss: 2.239179849624634
Validation loss: 2.0954799523917575

Epoch: 6| Step: 8
Training loss: 2.1683106422424316
Validation loss: 2.079956721234065

Epoch: 6| Step: 9
Training loss: 1.2802636623382568
Validation loss: 2.1109218084683983

Epoch: 6| Step: 10
Training loss: 2.03775691986084
Validation loss: 2.128011857309649

Epoch: 6| Step: 11
Training loss: 2.6433708667755127
Validation loss: 2.1390313102353002

Epoch: 6| Step: 12
Training loss: 2.3439362049102783
Validation loss: 2.1457214483650784

Epoch: 6| Step: 13
Training loss: 2.541261911392212
Validation loss: 2.147561389912841

Epoch: 427| Step: 0
Training loss: 1.985755443572998
Validation loss: 2.1612869949751

Epoch: 6| Step: 1
Training loss: 2.5441577434539795
Validation loss: 2.1686739972842637

Epoch: 6| Step: 2
Training loss: 1.9227445125579834
Validation loss: 2.1770729005977674

Epoch: 6| Step: 3
Training loss: 2.624221086502075
Validation loss: 2.1766839117132206

Epoch: 6| Step: 4
Training loss: 2.6670761108398438
Validation loss: 2.159676415945894

Epoch: 6| Step: 5
Training loss: 1.7195169925689697
Validation loss: 2.137426026405827

Epoch: 6| Step: 6
Training loss: 2.3176703453063965
Validation loss: 2.131866729387673

Epoch: 6| Step: 7
Training loss: 1.3555604219436646
Validation loss: 2.1142617758884223

Epoch: 6| Step: 8
Training loss: 2.3949196338653564
Validation loss: 2.13249885395009

Epoch: 6| Step: 9
Training loss: 2.3344202041625977
Validation loss: 2.115044927084318

Epoch: 6| Step: 10
Training loss: 1.6872365474700928
Validation loss: 2.1218601067860923

Epoch: 6| Step: 11
Training loss: 1.9736509323120117
Validation loss: 2.105056906259188

Epoch: 6| Step: 12
Training loss: 2.4484951496124268
Validation loss: 2.1241205417981712

Epoch: 6| Step: 13
Training loss: 3.061229705810547
Validation loss: 2.097697147759058

Epoch: 428| Step: 0
Training loss: 1.7611098289489746
Validation loss: 2.1200841870359195

Epoch: 6| Step: 1
Training loss: 2.236820697784424
Validation loss: 2.1207294335929294

Epoch: 6| Step: 2
Training loss: 2.3366758823394775
Validation loss: 2.1219921586334065

Epoch: 6| Step: 3
Training loss: 2.1166939735412598
Validation loss: 2.1138605251107165

Epoch: 6| Step: 4
Training loss: 2.2135677337646484
Validation loss: 2.1409222951499363

Epoch: 6| Step: 5
Training loss: 1.565395712852478
Validation loss: 2.1458875556145944

Epoch: 6| Step: 6
Training loss: 2.8254311084747314
Validation loss: 2.1619933292429936

Epoch: 6| Step: 7
Training loss: 1.7974271774291992
Validation loss: 2.154675486267254

Epoch: 6| Step: 8
Training loss: 2.098668098449707
Validation loss: 2.1237916997683945

Epoch: 6| Step: 9
Training loss: 2.5303330421447754
Validation loss: 2.0735920731739332

Epoch: 6| Step: 10
Training loss: 2.2949023246765137
Validation loss: 2.1264167959972093

Epoch: 6| Step: 11
Training loss: 2.041618585586548
Validation loss: 2.114295426235404

Epoch: 6| Step: 12
Training loss: 2.013530731201172
Validation loss: 2.0916100753250944

Epoch: 6| Step: 13
Training loss: 3.239834785461426
Validation loss: 2.0997027171555387

Epoch: 429| Step: 0
Training loss: 1.4498393535614014
Validation loss: 2.1044074950679654

Epoch: 6| Step: 1
Training loss: 2.173297882080078
Validation loss: 2.100077029197447

Epoch: 6| Step: 2
Training loss: 2.8570291996002197
Validation loss: 2.093888523758099

Epoch: 6| Step: 3
Training loss: 2.7903523445129395
Validation loss: 2.1011870868744387

Epoch: 6| Step: 4
Training loss: 1.7180465459823608
Validation loss: 2.12626886880526

Epoch: 6| Step: 5
Training loss: 2.2539567947387695
Validation loss: 2.1052138805389404

Epoch: 6| Step: 6
Training loss: 2.319828510284424
Validation loss: 2.1271549899091005

Epoch: 6| Step: 7
Training loss: 2.680539131164551
Validation loss: 2.1213181672557706

Epoch: 6| Step: 8
Training loss: 2.356142282485962
Validation loss: 2.1228760878245034

Epoch: 6| Step: 9
Training loss: 2.1538307666778564
Validation loss: 2.1174773118829213

Epoch: 6| Step: 10
Training loss: 1.529569149017334
Validation loss: 2.103441422985446

Epoch: 6| Step: 11
Training loss: 1.3068631887435913
Validation loss: 2.131748104608187

Epoch: 6| Step: 12
Training loss: 2.687624931335449
Validation loss: 2.139719993837418

Epoch: 6| Step: 13
Training loss: 2.55759334564209
Validation loss: 2.1407876155709706

Epoch: 430| Step: 0
Training loss: 2.1658780574798584
Validation loss: 2.121379972786032

Epoch: 6| Step: 1
Training loss: 1.9300506114959717
Validation loss: 2.106213686286762

Epoch: 6| Step: 2
Training loss: 2.4850988388061523
Validation loss: 2.09452865072476

Epoch: 6| Step: 3
Training loss: 1.9287986755371094
Validation loss: 2.0913573644494496

Epoch: 6| Step: 4
Training loss: 3.053715467453003
Validation loss: 2.091168543343903

Epoch: 6| Step: 5
Training loss: 2.2964162826538086
Validation loss: 2.103607008534093

Epoch: 6| Step: 6
Training loss: 2.142745018005371
Validation loss: 2.113655856860581

Epoch: 6| Step: 7
Training loss: 2.1772005558013916
Validation loss: 2.1057708904307377

Epoch: 6| Step: 8
Training loss: 1.6838033199310303
Validation loss: 2.09960804318869

Epoch: 6| Step: 9
Training loss: 2.7553348541259766
Validation loss: 2.0967483341052966

Epoch: 6| Step: 10
Training loss: 2.1787068843841553
Validation loss: 2.099953700137395

Epoch: 6| Step: 11
Training loss: 1.95241117477417
Validation loss: 2.092676467792962

Epoch: 6| Step: 12
Training loss: 1.7094359397888184
Validation loss: 2.0944741028611378

Epoch: 6| Step: 13
Training loss: 2.1590399742126465
Validation loss: 2.0841551006481214

Epoch: 431| Step: 0
Training loss: 2.192497968673706
Validation loss: 2.1214114376293716

Epoch: 6| Step: 1
Training loss: 2.3721768856048584
Validation loss: 2.11795368758581

Epoch: 6| Step: 2
Training loss: 1.455385684967041
Validation loss: 2.123796042575631

Epoch: 6| Step: 3
Training loss: 2.0670933723449707
Validation loss: 2.141430026741438

Epoch: 6| Step: 4
Training loss: 2.20090913772583
Validation loss: 2.1410386857166084

Epoch: 6| Step: 5
Training loss: 2.415005683898926
Validation loss: 2.147917024550899

Epoch: 6| Step: 6
Training loss: 1.8379148244857788
Validation loss: 2.1434735662193707

Epoch: 6| Step: 7
Training loss: 2.5886120796203613
Validation loss: 2.1286130528296194

Epoch: 6| Step: 8
Training loss: 2.156867504119873
Validation loss: 2.1450990758916384

Epoch: 6| Step: 9
Training loss: 2.6661343574523926
Validation loss: 2.133680392337102

Epoch: 6| Step: 10
Training loss: 1.924910545349121
Validation loss: 2.1189852196683168

Epoch: 6| Step: 11
Training loss: 2.4462203979492188
Validation loss: 2.155256845617807

Epoch: 6| Step: 12
Training loss: 1.9570200443267822
Validation loss: 2.1287758799009424

Epoch: 6| Step: 13
Training loss: 2.5276217460632324
Validation loss: 2.106010760030439

Epoch: 432| Step: 0
Training loss: 2.40470027923584
Validation loss: 2.10762264908001

Epoch: 6| Step: 1
Training loss: 2.4476664066314697
Validation loss: 2.102233971318891

Epoch: 6| Step: 2
Training loss: 2.2792882919311523
Validation loss: 2.0966530615283596

Epoch: 6| Step: 3
Training loss: 2.50510835647583
Validation loss: 2.1299511796684674

Epoch: 6| Step: 4
Training loss: 2.7359912395477295
Validation loss: 2.115406051758797

Epoch: 6| Step: 5
Training loss: 2.061399459838867
Validation loss: 2.1115323189766175

Epoch: 6| Step: 6
Training loss: 1.5606361627578735
Validation loss: 2.11025744868863

Epoch: 6| Step: 7
Training loss: 1.6205171346664429
Validation loss: 2.0913237166661087

Epoch: 6| Step: 8
Training loss: 1.2575790882110596
Validation loss: 2.0985865592956543

Epoch: 6| Step: 9
Training loss: 2.4396543502807617
Validation loss: 2.1072811977837675

Epoch: 6| Step: 10
Training loss: 1.8849583864212036
Validation loss: 2.0844259954267934

Epoch: 6| Step: 11
Training loss: 2.485962390899658
Validation loss: 2.0849775396367556

Epoch: 6| Step: 12
Training loss: 2.363499164581299
Validation loss: 2.0877599857186757

Epoch: 6| Step: 13
Training loss: 2.6322033405303955
Validation loss: 2.082629857524749

Epoch: 433| Step: 0
Training loss: 2.85831356048584
Validation loss: 2.0786330674284246

Epoch: 6| Step: 1
Training loss: 2.911921977996826
Validation loss: 2.093824637833462

Epoch: 6| Step: 2
Training loss: 1.7323832511901855
Validation loss: 2.0991035443480297

Epoch: 6| Step: 3
Training loss: 1.5527983903884888
Validation loss: 2.1165212815807712

Epoch: 6| Step: 4
Training loss: 1.746739149093628
Validation loss: 2.1041340443395797

Epoch: 6| Step: 5
Training loss: 1.8419737815856934
Validation loss: 2.1181457863059094

Epoch: 6| Step: 6
Training loss: 2.4210500717163086
Validation loss: 2.11821303572706

Epoch: 6| Step: 7
Training loss: 2.2393360137939453
Validation loss: 2.122564738796603

Epoch: 6| Step: 8
Training loss: 2.1539041996002197
Validation loss: 2.0963926353762226

Epoch: 6| Step: 9
Training loss: 1.9892380237579346
Validation loss: 2.104787549664897

Epoch: 6| Step: 10
Training loss: 2.7546591758728027
Validation loss: 2.081718753742915

Epoch: 6| Step: 11
Training loss: 1.603784441947937
Validation loss: 2.1205735937241585

Epoch: 6| Step: 12
Training loss: 2.4095940589904785
Validation loss: 2.120663986411146

Epoch: 6| Step: 13
Training loss: 2.2697248458862305
Validation loss: 2.136636039262177

Epoch: 434| Step: 0
Training loss: 2.851393222808838
Validation loss: 2.127242913810156

Epoch: 6| Step: 1
Training loss: 2.211665153503418
Validation loss: 2.1524176443776777

Epoch: 6| Step: 2
Training loss: 2.038614273071289
Validation loss: 2.1376104303585586

Epoch: 6| Step: 3
Training loss: 1.7705227136611938
Validation loss: 2.127604320485105

Epoch: 6| Step: 4
Training loss: 2.187704563140869
Validation loss: 2.125877598280548

Epoch: 6| Step: 5
Training loss: 2.0613369941711426
Validation loss: 2.120833094402026

Epoch: 6| Step: 6
Training loss: 1.621167778968811
Validation loss: 2.09568739193742

Epoch: 6| Step: 7
Training loss: 1.7467395067214966
Validation loss: 2.134330813602735

Epoch: 6| Step: 8
Training loss: 1.9915800094604492
Validation loss: 2.1268261606975267

Epoch: 6| Step: 9
Training loss: 2.2028985023498535
Validation loss: 2.142376721546214

Epoch: 6| Step: 10
Training loss: 2.87589693069458
Validation loss: 2.107195569622901

Epoch: 6| Step: 11
Training loss: 2.886017084121704
Validation loss: 2.115275761132599

Epoch: 6| Step: 12
Training loss: 1.5130727291107178
Validation loss: 2.1057827575232393

Epoch: 6| Step: 13
Training loss: 2.459657907485962
Validation loss: 2.10141674933895

Epoch: 435| Step: 0
Training loss: 1.1939568519592285
Validation loss: 2.0780185602044545

Epoch: 6| Step: 1
Training loss: 2.394132614135742
Validation loss: 2.093386052757181

Epoch: 6| Step: 2
Training loss: 1.262505054473877
Validation loss: 2.096037846739574

Epoch: 6| Step: 3
Training loss: 2.119403839111328
Validation loss: 2.085685727416828

Epoch: 6| Step: 4
Training loss: 2.285090208053589
Validation loss: 2.090496240123626

Epoch: 6| Step: 5
Training loss: 2.174032211303711
Validation loss: 2.1054478794015865

Epoch: 6| Step: 6
Training loss: 2.1333677768707275
Validation loss: 2.10778155121752

Epoch: 6| Step: 7
Training loss: 2.776071786880493
Validation loss: 2.1010977888620026

Epoch: 6| Step: 8
Training loss: 2.5379796028137207
Validation loss: 2.1204924942344747

Epoch: 6| Step: 9
Training loss: 2.661832809448242
Validation loss: 2.098467078260196

Epoch: 6| Step: 10
Training loss: 2.5919055938720703
Validation loss: 2.108197973620507

Epoch: 6| Step: 11
Training loss: 1.8441182374954224
Validation loss: 2.105450021323337

Epoch: 6| Step: 12
Training loss: 2.337001323699951
Validation loss: 2.1110263729608185

Epoch: 6| Step: 13
Training loss: 1.9207234382629395
Validation loss: 2.1056695471527758

Epoch: 436| Step: 0
Training loss: 2.05938982963562
Validation loss: 2.10659299870973

Epoch: 6| Step: 1
Training loss: 2.5622777938842773
Validation loss: 2.118560142414544

Epoch: 6| Step: 2
Training loss: 1.6036388874053955
Validation loss: 2.111812267252194

Epoch: 6| Step: 3
Training loss: 2.3481130599975586
Validation loss: 2.1066999666152464

Epoch: 6| Step: 4
Training loss: 1.267281413078308
Validation loss: 2.109589745921473

Epoch: 6| Step: 5
Training loss: 2.6413135528564453
Validation loss: 2.0924323399861655

Epoch: 6| Step: 6
Training loss: 2.114377975463867
Validation loss: 2.1153724193573

Epoch: 6| Step: 7
Training loss: 1.843135118484497
Validation loss: 2.1132501709845757

Epoch: 6| Step: 8
Training loss: 1.7293305397033691
Validation loss: 2.1011777103588147

Epoch: 6| Step: 9
Training loss: 2.2794392108917236
Validation loss: 2.1121820647229432

Epoch: 6| Step: 10
Training loss: 2.521017074584961
Validation loss: 2.1006749432574034

Epoch: 6| Step: 11
Training loss: 2.459773063659668
Validation loss: 2.1172988414764404

Epoch: 6| Step: 12
Training loss: 2.6925277709960938
Validation loss: 2.1073360596933672

Epoch: 6| Step: 13
Training loss: 2.0573160648345947
Validation loss: 2.0948714838233045

Epoch: 437| Step: 0
Training loss: 2.117832899093628
Validation loss: 2.100113571331065

Epoch: 6| Step: 1
Training loss: 1.9948147535324097
Validation loss: 2.0883460993407876

Epoch: 6| Step: 2
Training loss: 2.7266550064086914
Validation loss: 2.088849199715481

Epoch: 6| Step: 3
Training loss: 2.282506227493286
Validation loss: 2.0791877674800094

Epoch: 6| Step: 4
Training loss: 2.3716418743133545
Validation loss: 2.0916165177540114

Epoch: 6| Step: 5
Training loss: 2.595363140106201
Validation loss: 2.0967920621236167

Epoch: 6| Step: 6
Training loss: 1.9109175205230713
Validation loss: 2.0989600894271687

Epoch: 6| Step: 7
Training loss: 2.1721882820129395
Validation loss: 2.08471280272289

Epoch: 6| Step: 8
Training loss: 1.8932044506072998
Validation loss: 2.1076742577296432

Epoch: 6| Step: 9
Training loss: 2.045738697052002
Validation loss: 2.133763829867045

Epoch: 6| Step: 10
Training loss: 1.4766943454742432
Validation loss: 2.1166668194596485

Epoch: 6| Step: 11
Training loss: 2.4328842163085938
Validation loss: 2.1176589124946186

Epoch: 6| Step: 12
Training loss: 1.7834806442260742
Validation loss: 2.115380071824597

Epoch: 6| Step: 13
Training loss: 2.9700381755828857
Validation loss: 2.1320386753287366

Epoch: 438| Step: 0
Training loss: 2.2030792236328125
Validation loss: 2.124586643711213

Epoch: 6| Step: 1
Training loss: 2.3786354064941406
Validation loss: 2.1536915353549424

Epoch: 6| Step: 2
Training loss: 1.5264514684677124
Validation loss: 2.1456169800091813

Epoch: 6| Step: 3
Training loss: 2.6589672565460205
Validation loss: 2.1333966921734553

Epoch: 6| Step: 4
Training loss: 2.640986204147339
Validation loss: 2.1677108272429435

Epoch: 6| Step: 5
Training loss: 1.7472535371780396
Validation loss: 2.140897866218321

Epoch: 6| Step: 6
Training loss: 2.4973225593566895
Validation loss: 2.130245557395361

Epoch: 6| Step: 7
Training loss: 1.9726219177246094
Validation loss: 2.101951819594188

Epoch: 6| Step: 8
Training loss: 2.440192699432373
Validation loss: 2.105464873775359

Epoch: 6| Step: 9
Training loss: 2.003682851791382
Validation loss: 2.117277314586024

Epoch: 6| Step: 10
Training loss: 2.112848997116089
Validation loss: 2.114527674131496

Epoch: 6| Step: 11
Training loss: 2.1584157943725586
Validation loss: 2.1264022934821343

Epoch: 6| Step: 12
Training loss: 2.177879810333252
Validation loss: 2.100407406847964

Epoch: 6| Step: 13
Training loss: 2.171698570251465
Validation loss: 2.1261962024114465

Epoch: 439| Step: 0
Training loss: 2.183518409729004
Validation loss: 2.109722593779205

Epoch: 6| Step: 1
Training loss: 1.4668984413146973
Validation loss: 2.1000745168296238

Epoch: 6| Step: 2
Training loss: 2.570949077606201
Validation loss: 2.101597502667417

Epoch: 6| Step: 3
Training loss: 2.0199103355407715
Validation loss: 2.090653973241006

Epoch: 6| Step: 4
Training loss: 2.5413408279418945
Validation loss: 2.0880009948566394

Epoch: 6| Step: 5
Training loss: 1.9077732563018799
Validation loss: 2.069510923918857

Epoch: 6| Step: 6
Training loss: 2.590536117553711
Validation loss: 2.10561881783188

Epoch: 6| Step: 7
Training loss: 2.8547682762145996
Validation loss: 2.0874855441431843

Epoch: 6| Step: 8
Training loss: 2.0782032012939453
Validation loss: 2.0821396304715063

Epoch: 6| Step: 9
Training loss: 1.2456824779510498
Validation loss: 2.1235305750241844

Epoch: 6| Step: 10
Training loss: 1.8505306243896484
Validation loss: 2.125860244997086

Epoch: 6| Step: 11
Training loss: 3.034512519836426
Validation loss: 2.1387584132532917

Epoch: 6| Step: 12
Training loss: 2.097031354904175
Validation loss: 2.139745371316069

Epoch: 6| Step: 13
Training loss: 1.7255504131317139
Validation loss: 2.1200116398513957

Epoch: 440| Step: 0
Training loss: 2.328620433807373
Validation loss: 2.1200267422583794

Epoch: 6| Step: 1
Training loss: 1.8633027076721191
Validation loss: 2.1194560502165105

Epoch: 6| Step: 2
Training loss: 1.6373093128204346
Validation loss: 2.1140383289706324

Epoch: 6| Step: 3
Training loss: 1.8659130334854126
Validation loss: 2.0862466558333366

Epoch: 6| Step: 4
Training loss: 2.6244382858276367
Validation loss: 2.1092459642758934

Epoch: 6| Step: 5
Training loss: 2.169121742248535
Validation loss: 2.105055288601947

Epoch: 6| Step: 6
Training loss: 2.2343015670776367
Validation loss: 2.113118447283263

Epoch: 6| Step: 7
Training loss: 1.9269533157348633
Validation loss: 2.129124318399737

Epoch: 6| Step: 8
Training loss: 2.3762354850769043
Validation loss: 2.1118698363663047

Epoch: 6| Step: 9
Training loss: 1.9900662899017334
Validation loss: 2.100666835743894

Epoch: 6| Step: 10
Training loss: 2.1973378658294678
Validation loss: 2.115437728102489

Epoch: 6| Step: 11
Training loss: 2.101555824279785
Validation loss: 2.0858714208807996

Epoch: 6| Step: 12
Training loss: 2.4672417640686035
Validation loss: 2.1080051596446703

Epoch: 6| Step: 13
Training loss: 3.100050210952759
Validation loss: 2.1213337426544516

Epoch: 441| Step: 0
Training loss: 1.8569133281707764
Validation loss: 2.1141564000037407

Epoch: 6| Step: 1
Training loss: 2.444624423980713
Validation loss: 2.112412647534442

Epoch: 6| Step: 2
Training loss: 2.557439088821411
Validation loss: 2.1181116668126916

Epoch: 6| Step: 3
Training loss: 1.9788141250610352
Validation loss: 2.103323180188415

Epoch: 6| Step: 4
Training loss: 2.379791736602783
Validation loss: 2.1031539478609638

Epoch: 6| Step: 5
Training loss: 2.110764503479004
Validation loss: 2.0879586819679505

Epoch: 6| Step: 6
Training loss: 2.2972798347473145
Validation loss: 2.0893069723600983

Epoch: 6| Step: 7
Training loss: 2.071842670440674
Validation loss: 2.0853195651885

Epoch: 6| Step: 8
Training loss: 2.792426586151123
Validation loss: 2.0782357108208442

Epoch: 6| Step: 9
Training loss: 2.2034406661987305
Validation loss: 2.099971684076453

Epoch: 6| Step: 10
Training loss: 1.8075579404830933
Validation loss: 2.096324215653122

Epoch: 6| Step: 11
Training loss: 1.5999151468276978
Validation loss: 2.0762250577249834

Epoch: 6| Step: 12
Training loss: 1.863792896270752
Validation loss: 2.0972402531613588

Epoch: 6| Step: 13
Training loss: 2.558382987976074
Validation loss: 2.0971234831758725

Epoch: 442| Step: 0
Training loss: 1.7636735439300537
Validation loss: 2.081876629142351

Epoch: 6| Step: 1
Training loss: 2.694105625152588
Validation loss: 2.100908415291899

Epoch: 6| Step: 2
Training loss: 2.3148984909057617
Validation loss: 2.1221039295196533

Epoch: 6| Step: 3
Training loss: 2.492567777633667
Validation loss: 2.1196320313279347

Epoch: 6| Step: 4
Training loss: 1.1043901443481445
Validation loss: 2.115366760120597

Epoch: 6| Step: 5
Training loss: 2.058201313018799
Validation loss: 2.1037105796157674

Epoch: 6| Step: 6
Training loss: 1.9733824729919434
Validation loss: 2.094699352018295

Epoch: 6| Step: 7
Training loss: 2.3330798149108887
Validation loss: 2.10318083916941

Epoch: 6| Step: 8
Training loss: 2.3505165576934814
Validation loss: 2.0949939527819232

Epoch: 6| Step: 9
Training loss: 1.8492836952209473
Validation loss: 2.100100281418011

Epoch: 6| Step: 10
Training loss: 2.2192792892456055
Validation loss: 2.1116079848299742

Epoch: 6| Step: 11
Training loss: 2.2531471252441406
Validation loss: 2.103010928759011

Epoch: 6| Step: 12
Training loss: 2.2460267543792725
Validation loss: 2.103814568570865

Epoch: 6| Step: 13
Training loss: 3.0312321186065674
Validation loss: 2.127558535145175

Epoch: 443| Step: 0
Training loss: 2.967233419418335
Validation loss: 2.113650398869668

Epoch: 6| Step: 1
Training loss: 2.748332977294922
Validation loss: 2.1292478115327897

Epoch: 6| Step: 2
Training loss: 2.567927122116089
Validation loss: 2.1275041128999446

Epoch: 6| Step: 3
Training loss: 2.766509771347046
Validation loss: 2.1323358615239463

Epoch: 6| Step: 4
Training loss: 1.228714942932129
Validation loss: 2.1102071731321272

Epoch: 6| Step: 5
Training loss: 1.4279706478118896
Validation loss: 2.0912659950153802

Epoch: 6| Step: 6
Training loss: 1.687428593635559
Validation loss: 2.0860060722597185

Epoch: 6| Step: 7
Training loss: 2.1746435165405273
Validation loss: 2.096504439589798

Epoch: 6| Step: 8
Training loss: 2.0472564697265625
Validation loss: 2.0804811498170257

Epoch: 6| Step: 9
Training loss: 2.211224317550659
Validation loss: 2.088739920687932

Epoch: 6| Step: 10
Training loss: 2.566409111022949
Validation loss: 2.0735502909588557

Epoch: 6| Step: 11
Training loss: 1.728056788444519
Validation loss: 2.0705503776509273

Epoch: 6| Step: 12
Training loss: 2.2026596069335938
Validation loss: 2.0714698760740218

Epoch: 6| Step: 13
Training loss: 1.9758622646331787
Validation loss: 2.070126397635347

Epoch: 444| Step: 0
Training loss: 2.030935049057007
Validation loss: 2.0828118080733926

Epoch: 6| Step: 1
Training loss: 2.7914648056030273
Validation loss: 2.077456287158433

Epoch: 6| Step: 2
Training loss: 1.982761025428772
Validation loss: 2.0922810082794516

Epoch: 6| Step: 3
Training loss: 2.063964366912842
Validation loss: 2.104268635472944

Epoch: 6| Step: 4
Training loss: 2.505424976348877
Validation loss: 2.1126577213246334

Epoch: 6| Step: 5
Training loss: 1.7295405864715576
Validation loss: 2.139946050541375

Epoch: 6| Step: 6
Training loss: 2.3202719688415527
Validation loss: 2.1265056825453237

Epoch: 6| Step: 7
Training loss: 1.9181406497955322
Validation loss: 2.17463150460233

Epoch: 6| Step: 8
Training loss: 2.5087761878967285
Validation loss: 2.164734594283565

Epoch: 6| Step: 9
Training loss: 1.8565815687179565
Validation loss: 2.1321538827752553

Epoch: 6| Step: 10
Training loss: 2.265779733657837
Validation loss: 2.136557940513857

Epoch: 6| Step: 11
Training loss: 1.4625099897384644
Validation loss: 2.120853231799218

Epoch: 6| Step: 12
Training loss: 2.2629406452178955
Validation loss: 2.133514817043017

Epoch: 6| Step: 13
Training loss: 2.7057533264160156
Validation loss: 2.137400698918168

Epoch: 445| Step: 0
Training loss: 2.8191490173339844
Validation loss: 2.088736464900355

Epoch: 6| Step: 1
Training loss: 1.988655686378479
Validation loss: 2.0925129036749563

Epoch: 6| Step: 2
Training loss: 2.8514368534088135
Validation loss: 2.0850829539760465

Epoch: 6| Step: 3
Training loss: 2.353527784347534
Validation loss: 2.070181626145558

Epoch: 6| Step: 4
Training loss: 1.7407662868499756
Validation loss: 2.0489575029701315

Epoch: 6| Step: 5
Training loss: 1.9041504859924316
Validation loss: 2.0592071830585437

Epoch: 6| Step: 6
Training loss: 1.9479248523712158
Validation loss: 2.0719449955929994

Epoch: 6| Step: 7
Training loss: 1.9109708070755005
Validation loss: 2.0673092539592455

Epoch: 6| Step: 8
Training loss: 2.057075023651123
Validation loss: 2.0684917883206437

Epoch: 6| Step: 9
Training loss: 1.051565170288086
Validation loss: 2.0814088698356383

Epoch: 6| Step: 10
Training loss: 2.0613269805908203
Validation loss: 2.093718456965621

Epoch: 6| Step: 11
Training loss: 2.5830636024475098
Validation loss: 2.109878881003267

Epoch: 6| Step: 12
Training loss: 2.67509126663208
Validation loss: 2.1079158706049763

Epoch: 6| Step: 13
Training loss: 2.2706689834594727
Validation loss: 2.1033345396800707

Epoch: 446| Step: 0
Training loss: 1.9465610980987549
Validation loss: 2.104278282452655

Epoch: 6| Step: 1
Training loss: 2.9772744178771973
Validation loss: 2.083089495217928

Epoch: 6| Step: 2
Training loss: 1.7474989891052246
Validation loss: 2.0958227726720993

Epoch: 6| Step: 3
Training loss: 2.544395923614502
Validation loss: 2.096178347064603

Epoch: 6| Step: 4
Training loss: 2.2922675609588623
Validation loss: 2.102326605909614

Epoch: 6| Step: 5
Training loss: 1.590200662612915
Validation loss: 2.0924468835194907

Epoch: 6| Step: 6
Training loss: 2.003596067428589
Validation loss: 2.074685392841216

Epoch: 6| Step: 7
Training loss: 2.5286149978637695
Validation loss: 2.0901802073242846

Epoch: 6| Step: 8
Training loss: 2.0847434997558594
Validation loss: 2.097859708211755

Epoch: 6| Step: 9
Training loss: 2.4117841720581055
Validation loss: 2.0987674625970985

Epoch: 6| Step: 10
Training loss: 2.0403199195861816
Validation loss: 2.0725406972310876

Epoch: 6| Step: 11
Training loss: 2.5335168838500977
Validation loss: 2.0786707990912983

Epoch: 6| Step: 12
Training loss: 1.837586522102356
Validation loss: 2.0829611157858245

Epoch: 6| Step: 13
Training loss: 1.5765290260314941
Validation loss: 2.0915540828499743

Epoch: 447| Step: 0
Training loss: 2.0762815475463867
Validation loss: 2.08790192168246

Epoch: 6| Step: 1
Training loss: 2.586646556854248
Validation loss: 2.09122210420588

Epoch: 6| Step: 2
Training loss: 2.1928865909576416
Validation loss: 2.0837980470349713

Epoch: 6| Step: 3
Training loss: 2.109100818634033
Validation loss: 2.0924872877777263

Epoch: 6| Step: 4
Training loss: 2.3691115379333496
Validation loss: 2.083767796075472

Epoch: 6| Step: 5
Training loss: 1.5990387201309204
Validation loss: 2.1201896180388746

Epoch: 6| Step: 6
Training loss: 2.618511915206909
Validation loss: 2.109652521789715

Epoch: 6| Step: 7
Training loss: 2.682009696960449
Validation loss: 2.1022974906429166

Epoch: 6| Step: 8
Training loss: 1.898754596710205
Validation loss: 2.1208566952777166

Epoch: 6| Step: 9
Training loss: 2.3268280029296875
Validation loss: 2.118866225724579

Epoch: 6| Step: 10
Training loss: 1.7524831295013428
Validation loss: 2.1224241641259964

Epoch: 6| Step: 11
Training loss: 1.8813998699188232
Validation loss: 2.0916758275801137

Epoch: 6| Step: 12
Training loss: 1.688584327697754
Validation loss: 2.1184382797569357

Epoch: 6| Step: 13
Training loss: 2.539637565612793
Validation loss: 2.087218315370621

Epoch: 448| Step: 0
Training loss: 1.7342168092727661
Validation loss: 2.087519904618622

Epoch: 6| Step: 1
Training loss: 1.8048012256622314
Validation loss: 2.1095280108913297

Epoch: 6| Step: 2
Training loss: 2.1481246948242188
Validation loss: 2.1056723158846617

Epoch: 6| Step: 3
Training loss: 2.5859193801879883
Validation loss: 2.0911501069222727

Epoch: 6| Step: 4
Training loss: 2.338107109069824
Validation loss: 2.0927995507435133

Epoch: 6| Step: 5
Training loss: 2.2161426544189453
Validation loss: 2.101001421610514

Epoch: 6| Step: 6
Training loss: 1.9192116260528564
Validation loss: 2.0883519444414365

Epoch: 6| Step: 7
Training loss: 1.2021253108978271
Validation loss: 2.094319635821927

Epoch: 6| Step: 8
Training loss: 2.126072883605957
Validation loss: 2.084744886685443

Epoch: 6| Step: 9
Training loss: 2.3238046169281006
Validation loss: 2.0970956561385945

Epoch: 6| Step: 10
Training loss: 2.339210033416748
Validation loss: 2.084138116528911

Epoch: 6| Step: 11
Training loss: 2.1345362663269043
Validation loss: 2.076968377636325

Epoch: 6| Step: 12
Training loss: 2.3907229900360107
Validation loss: 2.0778814336305023

Epoch: 6| Step: 13
Training loss: 3.2061662673950195
Validation loss: 2.09302693797696

Epoch: 449| Step: 0
Training loss: 2.0839123725891113
Validation loss: 2.1093930147027455

Epoch: 6| Step: 1
Training loss: 2.0099565982818604
Validation loss: 2.109869385278353

Epoch: 6| Step: 2
Training loss: 1.0246115922927856
Validation loss: 2.0919914117423435

Epoch: 6| Step: 3
Training loss: 2.228184223175049
Validation loss: 2.0905772460404264

Epoch: 6| Step: 4
Training loss: 2.55126953125
Validation loss: 2.1119036007952947

Epoch: 6| Step: 5
Training loss: 1.8775131702423096
Validation loss: 2.1003528794934674

Epoch: 6| Step: 6
Training loss: 2.8554458618164062
Validation loss: 2.100120334215062

Epoch: 6| Step: 7
Training loss: 1.8050810098648071
Validation loss: 2.090255811650266

Epoch: 6| Step: 8
Training loss: 1.881523609161377
Validation loss: 2.120138019643804

Epoch: 6| Step: 9
Training loss: 2.0873358249664307
Validation loss: 2.086092951477215

Epoch: 6| Step: 10
Training loss: 2.3454031944274902
Validation loss: 2.1170687239657164

Epoch: 6| Step: 11
Training loss: 2.319775104522705
Validation loss: 2.106614120544926

Epoch: 6| Step: 12
Training loss: 2.4775922298431396
Validation loss: 2.100639979044596

Epoch: 6| Step: 13
Training loss: 2.4077422618865967
Validation loss: 2.1038249077335482

Epoch: 450| Step: 0
Training loss: 2.139777660369873
Validation loss: 2.085024054332446

Epoch: 6| Step: 1
Training loss: 1.6674351692199707
Validation loss: 2.095997469399565

Epoch: 6| Step: 2
Training loss: 2.5704569816589355
Validation loss: 2.0885488474240868

Epoch: 6| Step: 3
Training loss: 2.1017231941223145
Validation loss: 2.0919466467313867

Epoch: 6| Step: 4
Training loss: 2.439262866973877
Validation loss: 2.086747807841147

Epoch: 6| Step: 5
Training loss: 1.6806464195251465
Validation loss: 2.087493504247358

Epoch: 6| Step: 6
Training loss: 1.677171230316162
Validation loss: 2.0898342914478754

Epoch: 6| Step: 7
Training loss: 1.1267222166061401
Validation loss: 2.0866533748565184

Epoch: 6| Step: 8
Training loss: 2.8216028213500977
Validation loss: 2.0820352544066725

Epoch: 6| Step: 9
Training loss: 2.3104934692382812
Validation loss: 2.0693809242658716

Epoch: 6| Step: 10
Training loss: 3.0496912002563477
Validation loss: 2.0616797708695933

Epoch: 6| Step: 11
Training loss: 2.738734722137451
Validation loss: 2.0839271571046565

Epoch: 6| Step: 12
Training loss: 1.553587555885315
Validation loss: 2.0588882661634877

Epoch: 6| Step: 13
Training loss: 1.929921269416809
Validation loss: 2.090789137348052

Testing loss: 2.3555128203497993

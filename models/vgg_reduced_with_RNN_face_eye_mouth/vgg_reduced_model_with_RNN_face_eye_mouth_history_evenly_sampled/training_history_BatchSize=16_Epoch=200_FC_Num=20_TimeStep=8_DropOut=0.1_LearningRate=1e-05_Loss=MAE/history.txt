Epoch: 1| Step: 0
Training loss: 4.702974796295166
Validation loss: 5.246249111749792

Epoch: 6| Step: 1
Training loss: 5.160265922546387
Validation loss: 5.232847890546245

Epoch: 6| Step: 2
Training loss: 4.0690016746521
Validation loss: 5.219696696086596

Epoch: 6| Step: 3
Training loss: 4.9545087814331055
Validation loss: 5.206829594027612

Epoch: 6| Step: 4
Training loss: 5.380517959594727
Validation loss: 5.193048666882259

Epoch: 6| Step: 5
Training loss: 5.428987979888916
Validation loss: 5.177838710046584

Epoch: 6| Step: 6
Training loss: 3.6786069869995117
Validation loss: 5.162109077617687

Epoch: 6| Step: 7
Training loss: 5.094335556030273
Validation loss: 5.145561110588812

Epoch: 6| Step: 8
Training loss: 4.878108024597168
Validation loss: 5.127636299338392

Epoch: 6| Step: 9
Training loss: 4.826174259185791
Validation loss: 5.108597683650191

Epoch: 6| Step: 10
Training loss: 6.288332939147949
Validation loss: 5.0888010865898545

Epoch: 6| Step: 11
Training loss: 4.91975212097168
Validation loss: 5.067368558658067

Epoch: 6| Step: 12
Training loss: 4.8679304122924805
Validation loss: 5.045921412847376

Epoch: 6| Step: 13
Training loss: 4.978587627410889
Validation loss: 5.021848842661868

Epoch: 2| Step: 0
Training loss: 6.005568504333496
Validation loss: 4.997943667955296

Epoch: 6| Step: 1
Training loss: 4.91886043548584
Validation loss: 4.972632977270311

Epoch: 6| Step: 2
Training loss: 3.0673093795776367
Validation loss: 4.945675475623018

Epoch: 6| Step: 3
Training loss: 4.682761192321777
Validation loss: 4.918605368624451

Epoch: 6| Step: 4
Training loss: 4.392116546630859
Validation loss: 4.8916533993136495

Epoch: 6| Step: 5
Training loss: 3.731907367706299
Validation loss: 4.861117132248417

Epoch: 6| Step: 6
Training loss: 4.7438154220581055
Validation loss: 4.831519860093311

Epoch: 6| Step: 7
Training loss: 4.9235405921936035
Validation loss: 4.802352987309938

Epoch: 6| Step: 8
Training loss: 6.611225128173828
Validation loss: 4.7709145494686656

Epoch: 6| Step: 9
Training loss: 3.672344207763672
Validation loss: 4.740689462231051

Epoch: 6| Step: 10
Training loss: 4.606690883636475
Validation loss: 4.710500911999774

Epoch: 6| Step: 11
Training loss: 3.460479736328125
Validation loss: 4.681367817745413

Epoch: 6| Step: 12
Training loss: 4.6794819831848145
Validation loss: 4.652195284443517

Epoch: 6| Step: 13
Training loss: 5.339735984802246
Validation loss: 4.623416090524325

Epoch: 3| Step: 0
Training loss: 4.32869815826416
Validation loss: 4.597149989938223

Epoch: 6| Step: 1
Training loss: 4.866986274719238
Validation loss: 4.568615544226862

Epoch: 6| Step: 2
Training loss: 3.9589414596557617
Validation loss: 4.542553965763379

Epoch: 6| Step: 3
Training loss: 4.430918216705322
Validation loss: 4.518515268961589

Epoch: 6| Step: 4
Training loss: 3.62321400642395
Validation loss: 4.4921081912133

Epoch: 6| Step: 5
Training loss: 4.90727424621582
Validation loss: 4.46831617816802

Epoch: 6| Step: 6
Training loss: 4.033807754516602
Validation loss: 4.446120323673371

Epoch: 6| Step: 7
Training loss: 3.4431233406066895
Validation loss: 4.424889595277848

Epoch: 6| Step: 8
Training loss: 4.6584296226501465
Validation loss: 4.402291682458693

Epoch: 6| Step: 9
Training loss: 4.2992401123046875
Validation loss: 4.381652016793528

Epoch: 6| Step: 10
Training loss: 3.8944509029388428
Validation loss: 4.359964770655478

Epoch: 6| Step: 11
Training loss: 4.076231956481934
Validation loss: 4.3389631035507366

Epoch: 6| Step: 12
Training loss: 5.412266731262207
Validation loss: 4.31529595774989

Epoch: 6| Step: 13
Training loss: 2.915832042694092
Validation loss: 4.291989326477051

Epoch: 4| Step: 0
Training loss: 4.7888970375061035
Validation loss: 4.269181231016754

Epoch: 6| Step: 1
Training loss: 2.9859681129455566
Validation loss: 4.24435087942308

Epoch: 6| Step: 2
Training loss: 2.62495493888855
Validation loss: 4.2183067721705285

Epoch: 6| Step: 3
Training loss: 4.65226936340332
Validation loss: 4.195466151801488

Epoch: 6| Step: 4
Training loss: 4.0898756980896
Validation loss: 4.1672953174960226

Epoch: 6| Step: 5
Training loss: 3.8815975189208984
Validation loss: 4.14343717534055

Epoch: 6| Step: 6
Training loss: 4.224393367767334
Validation loss: 4.1192739086766395

Epoch: 6| Step: 7
Training loss: 3.8545141220092773
Validation loss: 4.09430677916414

Epoch: 6| Step: 8
Training loss: 4.504091739654541
Validation loss: 4.0704932110283965

Epoch: 6| Step: 9
Training loss: 4.010125160217285
Validation loss: 4.047730256152409

Epoch: 6| Step: 10
Training loss: 4.426145076751709
Validation loss: 4.025298267282466

Epoch: 6| Step: 11
Training loss: 2.7878458499908447
Validation loss: 4.003657330748855

Epoch: 6| Step: 12
Training loss: 4.761589527130127
Validation loss: 3.9831329109848186

Epoch: 6| Step: 13
Training loss: 3.5061569213867188
Validation loss: 3.9598069601161505

Epoch: 5| Step: 0
Training loss: 3.640275001525879
Validation loss: 3.9383216365691154

Epoch: 6| Step: 1
Training loss: 3.2417092323303223
Validation loss: 3.9177842755471506

Epoch: 6| Step: 2
Training loss: 3.4352810382843018
Validation loss: 3.895899498334495

Epoch: 6| Step: 3
Training loss: 3.507441520690918
Validation loss: 3.8782271877411874

Epoch: 6| Step: 4
Training loss: 2.993846893310547
Validation loss: 3.854583381324686

Epoch: 6| Step: 5
Training loss: 3.260387420654297
Validation loss: 3.8360158986942743

Epoch: 6| Step: 6
Training loss: 3.9858713150024414
Validation loss: 3.8201296150043444

Epoch: 6| Step: 7
Training loss: 4.3938751220703125
Validation loss: 3.8035498229406213

Epoch: 6| Step: 8
Training loss: 4.299356937408447
Validation loss: 3.7886722600588234

Epoch: 6| Step: 9
Training loss: 1.7365087270736694
Validation loss: 3.7710749872269167

Epoch: 6| Step: 10
Training loss: 5.484681129455566
Validation loss: 3.752958469493415

Epoch: 6| Step: 11
Training loss: 3.2755472660064697
Validation loss: 3.735862457624046

Epoch: 6| Step: 12
Training loss: 4.4966912269592285
Validation loss: 3.725029227554157

Epoch: 6| Step: 13
Training loss: 4.267177104949951
Validation loss: 3.7137345011516283

Epoch: 6| Step: 0
Training loss: 3.7023563385009766
Validation loss: 3.6978250575321976

Epoch: 6| Step: 1
Training loss: 3.500758409500122
Validation loss: 3.6794235680692937

Epoch: 6| Step: 2
Training loss: 3.719831705093384
Validation loss: 3.663596558314498

Epoch: 6| Step: 3
Training loss: 3.7625813484191895
Validation loss: 3.6509352319984028

Epoch: 6| Step: 4
Training loss: 2.7485852241516113
Validation loss: 3.635777058139924

Epoch: 6| Step: 5
Training loss: 2.968707799911499
Validation loss: 3.62513247869348

Epoch: 6| Step: 6
Training loss: 3.9360005855560303
Validation loss: 3.6107125795015724

Epoch: 6| Step: 7
Training loss: 3.622154712677002
Validation loss: 3.5930630519825923

Epoch: 6| Step: 8
Training loss: 3.469532012939453
Validation loss: 3.5809841617461173

Epoch: 6| Step: 9
Training loss: 3.8521182537078857
Validation loss: 3.571707489669964

Epoch: 6| Step: 10
Training loss: 3.330219268798828
Validation loss: 3.5546562902389036

Epoch: 6| Step: 11
Training loss: 3.127926826477051
Validation loss: 3.532281437227803

Epoch: 6| Step: 12
Training loss: 3.6447715759277344
Validation loss: 3.501780043366135

Epoch: 6| Step: 13
Training loss: 4.175536632537842
Validation loss: 3.4808007183895318

Epoch: 7| Step: 0
Training loss: 2.948899745941162
Validation loss: 3.468872372822095

Epoch: 6| Step: 1
Training loss: 4.057363510131836
Validation loss: 3.45525868990088

Epoch: 6| Step: 2
Training loss: 4.684825897216797
Validation loss: 3.4515743870888986

Epoch: 6| Step: 3
Training loss: 3.0029640197753906
Validation loss: 3.424837476463728

Epoch: 6| Step: 4
Training loss: 3.1481099128723145
Validation loss: 3.4153138796488443

Epoch: 6| Step: 5
Training loss: 2.7359015941619873
Validation loss: 3.410209442979546

Epoch: 6| Step: 6
Training loss: 3.025397300720215
Validation loss: 3.401708859269337

Epoch: 6| Step: 7
Training loss: 3.3654234409332275
Validation loss: 3.3983385255259853

Epoch: 6| Step: 8
Training loss: 3.019258975982666
Validation loss: 3.3872555302035425

Epoch: 6| Step: 9
Training loss: 3.0051469802856445
Validation loss: 3.3821542237394597

Epoch: 6| Step: 10
Training loss: 3.8862342834472656
Validation loss: 3.3755740478474605

Epoch: 6| Step: 11
Training loss: 3.2249813079833984
Validation loss: 3.3687108691020677

Epoch: 6| Step: 12
Training loss: 3.632598400115967
Validation loss: 3.3619751084235405

Epoch: 6| Step: 13
Training loss: 3.1749961376190186
Validation loss: 3.3528423591326644

Epoch: 8| Step: 0
Training loss: 4.250696182250977
Validation loss: 3.346033839769261

Epoch: 6| Step: 1
Training loss: 4.071802139282227
Validation loss: 3.3385570767105266

Epoch: 6| Step: 2
Training loss: 2.1592235565185547
Validation loss: 3.3311657162122827

Epoch: 6| Step: 3
Training loss: 3.17641544342041
Validation loss: 3.322997731547202

Epoch: 6| Step: 4
Training loss: 2.5915451049804688
Validation loss: 3.3161983131080546

Epoch: 6| Step: 5
Training loss: 3.220104694366455
Validation loss: 3.3086680699420232

Epoch: 6| Step: 6
Training loss: 2.757791757583618
Validation loss: 3.304376197117631

Epoch: 6| Step: 7
Training loss: 3.0418622493743896
Validation loss: 3.3047397982689644

Epoch: 6| Step: 8
Training loss: 2.738636016845703
Validation loss: 3.2946419254426034

Epoch: 6| Step: 9
Training loss: 2.9858274459838867
Validation loss: 3.2887967350662395

Epoch: 6| Step: 10
Training loss: 3.6813645362854004
Validation loss: 3.2872328142965994

Epoch: 6| Step: 11
Training loss: 3.637580394744873
Validation loss: 3.2755701541900635

Epoch: 6| Step: 12
Training loss: 3.810985565185547
Validation loss: 3.273636353913174

Epoch: 6| Step: 13
Training loss: 4.063694953918457
Validation loss: 3.273165720765309

Epoch: 9| Step: 0
Training loss: 3.1206326484680176
Validation loss: 3.2636614486735356

Epoch: 6| Step: 1
Training loss: 2.483916759490967
Validation loss: 3.259992289286788

Epoch: 6| Step: 2
Training loss: 2.799306631088257
Validation loss: 3.260995239339849

Epoch: 6| Step: 3
Training loss: 3.6138644218444824
Validation loss: 3.263405056409938

Epoch: 6| Step: 4
Training loss: 3.9295406341552734
Validation loss: 3.2547672230710267

Epoch: 6| Step: 5
Training loss: 3.28973388671875
Validation loss: 3.2499395647356586

Epoch: 6| Step: 6
Training loss: 3.466350555419922
Validation loss: 3.242707290957051

Epoch: 6| Step: 7
Training loss: 3.375988483428955
Validation loss: 3.2328766033213627

Epoch: 6| Step: 8
Training loss: 3.179697275161743
Validation loss: 3.2242299459313832

Epoch: 6| Step: 9
Training loss: 2.3930108547210693
Validation loss: 3.2230884080292075

Epoch: 6| Step: 10
Training loss: 2.6525967121124268
Validation loss: 3.222191433752737

Epoch: 6| Step: 11
Training loss: 2.608436346054077
Validation loss: 3.221445301527618

Epoch: 6| Step: 12
Training loss: 4.522702217102051
Validation loss: 3.215487690382106

Epoch: 6| Step: 13
Training loss: 4.078360557556152
Validation loss: 3.205186220907396

Epoch: 10| Step: 0
Training loss: 3.7975616455078125
Validation loss: 3.203916447136992

Epoch: 6| Step: 1
Training loss: 2.660405397415161
Validation loss: 3.2038995117269535

Epoch: 6| Step: 2
Training loss: 3.066549777984619
Validation loss: 3.2055290052967687

Epoch: 6| Step: 3
Training loss: 2.574368953704834
Validation loss: 3.195669292121805

Epoch: 6| Step: 4
Training loss: 4.0098161697387695
Validation loss: 3.1952209575201875

Epoch: 6| Step: 5
Training loss: 2.1232481002807617
Validation loss: 3.1899936968280422

Epoch: 6| Step: 6
Training loss: 3.559933662414551
Validation loss: 3.1805770499731905

Epoch: 6| Step: 7
Training loss: 2.3361191749572754
Validation loss: 3.1771210393598004

Epoch: 6| Step: 8
Training loss: 3.2061073780059814
Validation loss: 3.175032300333823

Epoch: 6| Step: 9
Training loss: 2.995692253112793
Validation loss: 3.1682406215257544

Epoch: 6| Step: 10
Training loss: 3.4727392196655273
Validation loss: 3.1666744755160425

Epoch: 6| Step: 11
Training loss: 3.5299341678619385
Validation loss: 3.1640855163656254

Epoch: 6| Step: 12
Training loss: 3.539679527282715
Validation loss: 3.1615374498469855

Epoch: 6| Step: 13
Training loss: 4.110458850860596
Validation loss: 3.159350500311903

Epoch: 11| Step: 0
Training loss: 3.2622342109680176
Validation loss: 3.15174255576185

Epoch: 6| Step: 1
Training loss: 3.3737659454345703
Validation loss: 3.1484412018970778

Epoch: 6| Step: 2
Training loss: 2.401745080947876
Validation loss: 3.1457100017096407

Epoch: 6| Step: 3
Training loss: 3.5381176471710205
Validation loss: 3.142025227187782

Epoch: 6| Step: 4
Training loss: 3.327887773513794
Validation loss: 3.141885870246477

Epoch: 6| Step: 5
Training loss: 2.3975205421447754
Validation loss: 3.1345553757042013

Epoch: 6| Step: 6
Training loss: 2.7220895290374756
Validation loss: 3.1316709467159805

Epoch: 6| Step: 7
Training loss: 3.8067867755889893
Validation loss: 3.130618610689717

Epoch: 6| Step: 8
Training loss: 3.222654342651367
Validation loss: 3.1274150699697514

Epoch: 6| Step: 9
Training loss: 3.711500644683838
Validation loss: 3.12549518000695

Epoch: 6| Step: 10
Training loss: 3.4431207180023193
Validation loss: 3.118835077490858

Epoch: 6| Step: 11
Training loss: 3.2383127212524414
Validation loss: 3.1151127507609706

Epoch: 6| Step: 12
Training loss: 2.620734453201294
Validation loss: 3.1111231414220666

Epoch: 6| Step: 13
Training loss: 2.9167685508728027
Validation loss: 3.1114067826219785

Epoch: 12| Step: 0
Training loss: 3.089988946914673
Validation loss: 3.1096195456802205

Epoch: 6| Step: 1
Training loss: 3.1255569458007812
Validation loss: 3.1058353454835954

Epoch: 6| Step: 2
Training loss: 3.1084108352661133
Validation loss: 3.103773534938853

Epoch: 6| Step: 3
Training loss: 2.948894500732422
Validation loss: 3.102449535041727

Epoch: 6| Step: 4
Training loss: 2.321430206298828
Validation loss: 3.0967401971099195

Epoch: 6| Step: 5
Training loss: 2.6155292987823486
Validation loss: 3.0938575934338313

Epoch: 6| Step: 6
Training loss: 3.8241240978240967
Validation loss: 3.089140974065309

Epoch: 6| Step: 7
Training loss: 2.926879405975342
Validation loss: 3.0852058856718

Epoch: 6| Step: 8
Training loss: 3.012169599533081
Validation loss: 3.0828699117065756

Epoch: 6| Step: 9
Training loss: 2.8588876724243164
Validation loss: 3.083646007763442

Epoch: 6| Step: 10
Training loss: 3.243190288543701
Validation loss: 3.080816807285432

Epoch: 6| Step: 11
Training loss: 4.004035472869873
Validation loss: 3.077493431747601

Epoch: 6| Step: 12
Training loss: 3.0580570697784424
Validation loss: 3.074517321842973

Epoch: 6| Step: 13
Training loss: 3.9269609451293945
Validation loss: 3.072350202068206

Epoch: 13| Step: 0
Training loss: 2.850944995880127
Validation loss: 3.0727880795796714

Epoch: 6| Step: 1
Training loss: 4.165326118469238
Validation loss: 3.0691572953295965

Epoch: 6| Step: 2
Training loss: 2.815682888031006
Validation loss: 3.065861232819096

Epoch: 6| Step: 3
Training loss: 2.229916572570801
Validation loss: 3.067202562926918

Epoch: 6| Step: 4
Training loss: 3.3365442752838135
Validation loss: 3.073414551314487

Epoch: 6| Step: 5
Training loss: 3.0976438522338867
Validation loss: 3.061722855414114

Epoch: 6| Step: 6
Training loss: 3.3674850463867188
Validation loss: 3.054359979526971

Epoch: 6| Step: 7
Training loss: 3.561156988143921
Validation loss: 3.0555546258085515

Epoch: 6| Step: 8
Training loss: 2.553572177886963
Validation loss: 3.0509152027868454

Epoch: 6| Step: 9
Training loss: 3.437396287918091
Validation loss: 3.047021145461708

Epoch: 6| Step: 10
Training loss: 2.7579145431518555
Validation loss: 3.046840619015437

Epoch: 6| Step: 11
Training loss: 2.6829795837402344
Validation loss: 3.0508234244520946

Epoch: 6| Step: 12
Training loss: 3.369802951812744
Validation loss: 3.0607849141602874

Epoch: 6| Step: 13
Training loss: 3.22175931930542
Validation loss: 3.0479991384731826

Epoch: 14| Step: 0
Training loss: 2.469599962234497
Validation loss: 3.038502029193345

Epoch: 6| Step: 1
Training loss: 3.6057443618774414
Validation loss: 3.034135087843864

Epoch: 6| Step: 2
Training loss: 3.0689339637756348
Validation loss: 3.043978819283106

Epoch: 6| Step: 3
Training loss: 3.0419797897338867
Validation loss: 3.042713419083626

Epoch: 6| Step: 4
Training loss: 3.1799445152282715
Validation loss: 3.036493944865401

Epoch: 6| Step: 5
Training loss: 3.3847904205322266
Validation loss: 3.0390368379572386

Epoch: 6| Step: 6
Training loss: 4.269347667694092
Validation loss: 3.033436316315846

Epoch: 6| Step: 7
Training loss: 4.48983097076416
Validation loss: 3.0259828823868946

Epoch: 6| Step: 8
Training loss: 2.7688770294189453
Validation loss: 3.019323613054009

Epoch: 6| Step: 9
Training loss: 2.849245548248291
Validation loss: 3.0186459479793424

Epoch: 6| Step: 10
Training loss: 2.5507538318634033
Validation loss: 3.01284352425606

Epoch: 6| Step: 11
Training loss: 3.362154006958008
Validation loss: 3.010399362092377

Epoch: 6| Step: 12
Training loss: 2.039085865020752
Validation loss: 3.0068568439893824

Epoch: 6| Step: 13
Training loss: 1.354288101196289
Validation loss: 3.003559184330766

Epoch: 15| Step: 0
Training loss: 2.870269775390625
Validation loss: 3.002438417045019

Epoch: 6| Step: 1
Training loss: 3.9134397506713867
Validation loss: 3.003184285215152

Epoch: 6| Step: 2
Training loss: 2.464580535888672
Validation loss: 2.999250478641961

Epoch: 6| Step: 3
Training loss: 2.0315403938293457
Validation loss: 3.002966632125198

Epoch: 6| Step: 4
Training loss: 3.2558133602142334
Validation loss: 3.0256165048127532

Epoch: 6| Step: 5
Training loss: 2.7251951694488525
Validation loss: 3.0383899698975267

Epoch: 6| Step: 6
Training loss: 2.5888280868530273
Validation loss: 3.0347463879533993

Epoch: 6| Step: 7
Training loss: 3.3316915035247803
Validation loss: 3.0047033627827964

Epoch: 6| Step: 8
Training loss: 3.3468008041381836
Validation loss: 2.9945529712143766

Epoch: 6| Step: 9
Training loss: 3.240297794342041
Validation loss: 2.9862467037734164

Epoch: 6| Step: 10
Training loss: 4.295243263244629
Validation loss: 2.987272554828275

Epoch: 6| Step: 11
Training loss: 2.325808048248291
Validation loss: 2.9840184975695867

Epoch: 6| Step: 12
Training loss: 3.6046814918518066
Validation loss: 2.9812447255657566

Epoch: 6| Step: 13
Training loss: 2.7639214992523193
Validation loss: 2.978134755165346

Epoch: 16| Step: 0
Training loss: 3.184109926223755
Validation loss: 2.981902301952403

Epoch: 6| Step: 1
Training loss: 3.4077677726745605
Validation loss: 2.983349184836111

Epoch: 6| Step: 2
Training loss: 3.0060184001922607
Validation loss: 2.9732107347057712

Epoch: 6| Step: 3
Training loss: 3.966733932495117
Validation loss: 2.9761914489089802

Epoch: 6| Step: 4
Training loss: 2.8420939445495605
Validation loss: 2.9854608402457288

Epoch: 6| Step: 5
Training loss: 2.942664861679077
Validation loss: 2.9950520838460615

Epoch: 6| Step: 6
Training loss: 2.975864887237549
Validation loss: 3.0318673323559504

Epoch: 6| Step: 7
Training loss: 3.3651108741760254
Validation loss: 3.0721700447861866

Epoch: 6| Step: 8
Training loss: 3.4307870864868164
Validation loss: 2.9992972266289497

Epoch: 6| Step: 9
Training loss: 2.4968647956848145
Validation loss: 2.961220041398079

Epoch: 6| Step: 10
Training loss: 2.7786927223205566
Validation loss: 2.9697417930890153

Epoch: 6| Step: 11
Training loss: 3.177980422973633
Validation loss: 2.9896120768721386

Epoch: 6| Step: 12
Training loss: 2.1962804794311523
Validation loss: 3.0308641131206224

Epoch: 6| Step: 13
Training loss: 2.9315381050109863
Validation loss: 3.03792078264298

Epoch: 17| Step: 0
Training loss: 3.5728704929351807
Validation loss: 3.0371052629204205

Epoch: 6| Step: 1
Training loss: 3.374610662460327
Validation loss: 3.0156924263123543

Epoch: 6| Step: 2
Training loss: 2.878387212753296
Validation loss: 2.998419430948073

Epoch: 6| Step: 3
Training loss: 3.9806315898895264
Validation loss: 2.9660617997569423

Epoch: 6| Step: 4
Training loss: 3.0842490196228027
Validation loss: 2.9615033390701457

Epoch: 6| Step: 5
Training loss: 3.4476494789123535
Validation loss: 2.9707440278863393

Epoch: 6| Step: 6
Training loss: 3.1535589694976807
Validation loss: 2.969707296740624

Epoch: 6| Step: 7
Training loss: 2.32273006439209
Validation loss: 2.961992830358526

Epoch: 6| Step: 8
Training loss: 2.963095188140869
Validation loss: 2.948802419888076

Epoch: 6| Step: 9
Training loss: 2.6245479583740234
Validation loss: 2.950802121111142

Epoch: 6| Step: 10
Training loss: 2.298396110534668
Validation loss: 2.9486862895309285

Epoch: 6| Step: 11
Training loss: 3.210268020629883
Validation loss: 2.9480409058191444

Epoch: 6| Step: 12
Training loss: 2.453956127166748
Validation loss: 2.947982495830905

Epoch: 6| Step: 13
Training loss: 3.480957269668579
Validation loss: 2.9324139292522142

Epoch: 18| Step: 0
Training loss: 3.4986250400543213
Validation loss: 2.9331440720506894

Epoch: 6| Step: 1
Training loss: 3.130397081375122
Validation loss: 2.924624991673295

Epoch: 6| Step: 2
Training loss: 2.699073553085327
Validation loss: 2.923325659126364

Epoch: 6| Step: 3
Training loss: 2.051584243774414
Validation loss: 2.920815570380098

Epoch: 6| Step: 4
Training loss: 3.812418222427368
Validation loss: 2.9176152649746148

Epoch: 6| Step: 5
Training loss: 2.4517550468444824
Validation loss: 2.919009357370356

Epoch: 6| Step: 6
Training loss: 3.014195442199707
Validation loss: 2.9172945253310667

Epoch: 6| Step: 7
Training loss: 3.153820514678955
Validation loss: 2.9127966716725338

Epoch: 6| Step: 8
Training loss: 3.4564006328582764
Validation loss: 2.914502907824773

Epoch: 6| Step: 9
Training loss: 2.8195371627807617
Validation loss: 2.9115761633842223

Epoch: 6| Step: 10
Training loss: 3.5077056884765625
Validation loss: 2.906216411180394

Epoch: 6| Step: 11
Training loss: 2.269237756729126
Validation loss: 2.9065472951499363

Epoch: 6| Step: 12
Training loss: 3.2020163536071777
Validation loss: 2.907971538523192

Epoch: 6| Step: 13
Training loss: 2.882378339767456
Validation loss: 2.9128747037661973

Epoch: 19| Step: 0
Training loss: 2.731294870376587
Validation loss: 2.9068145187952186

Epoch: 6| Step: 1
Training loss: 2.949615001678467
Validation loss: 2.8996764998282156

Epoch: 6| Step: 2
Training loss: 3.5176937580108643
Validation loss: 2.898075129396172

Epoch: 6| Step: 3
Training loss: 3.282750368118286
Validation loss: 2.8966791296517975

Epoch: 6| Step: 4
Training loss: 2.2812108993530273
Validation loss: 2.894557124824934

Epoch: 6| Step: 5
Training loss: 2.5402774810791016
Validation loss: 2.896303412734821

Epoch: 6| Step: 6
Training loss: 2.6551690101623535
Validation loss: 2.8892241703566683

Epoch: 6| Step: 7
Training loss: 3.4624273777008057
Validation loss: 2.88796397947496

Epoch: 6| Step: 8
Training loss: 2.6396431922912598
Validation loss: 2.8817554109839985

Epoch: 6| Step: 9
Training loss: 2.936680316925049
Validation loss: 2.8825395491815384

Epoch: 6| Step: 10
Training loss: 3.0179553031921387
Validation loss: 2.8820160358182845

Epoch: 6| Step: 11
Training loss: 2.8705997467041016
Validation loss: 2.881433286974507

Epoch: 6| Step: 12
Training loss: 3.433631420135498
Validation loss: 2.877058582921182

Epoch: 6| Step: 13
Training loss: 3.6782002449035645
Validation loss: 2.876913688516104

Epoch: 20| Step: 0
Training loss: 1.7300214767456055
Validation loss: 2.874296977955808

Epoch: 6| Step: 1
Training loss: 3.2369322776794434
Validation loss: 2.871794044330556

Epoch: 6| Step: 2
Training loss: 1.8350205421447754
Validation loss: 2.867753646707022

Epoch: 6| Step: 3
Training loss: 2.99082612991333
Validation loss: 2.8799000119650238

Epoch: 6| Step: 4
Training loss: 3.111910820007324
Validation loss: 2.8644139997420774

Epoch: 6| Step: 5
Training loss: 3.891981363296509
Validation loss: 2.8697145805563977

Epoch: 6| Step: 6
Training loss: 3.250894069671631
Validation loss: 2.866386910920502

Epoch: 6| Step: 7
Training loss: 2.8775954246520996
Validation loss: 2.8627871082675074

Epoch: 6| Step: 8
Training loss: 3.093475341796875
Validation loss: 2.8611054907562914

Epoch: 6| Step: 9
Training loss: 3.2286972999572754
Validation loss: 2.8633796425275904

Epoch: 6| Step: 10
Training loss: 3.4079971313476562
Validation loss: 2.86510475989311

Epoch: 6| Step: 11
Training loss: 3.2524447441101074
Validation loss: 2.87330227000739

Epoch: 6| Step: 12
Training loss: 3.076899290084839
Validation loss: 2.871269054310296

Epoch: 6| Step: 13
Training loss: 2.0752596855163574
Validation loss: 2.8611495776843

Epoch: 21| Step: 0
Training loss: 3.4737257957458496
Validation loss: 2.8497624525459866

Epoch: 6| Step: 1
Training loss: 2.4568684101104736
Validation loss: 2.8482777457083426

Epoch: 6| Step: 2
Training loss: 2.6474266052246094
Validation loss: 2.844878714571717

Epoch: 6| Step: 3
Training loss: 3.587768077850342
Validation loss: 2.844667670547321

Epoch: 6| Step: 4
Training loss: 3.207418441772461
Validation loss: 2.841471802803778

Epoch: 6| Step: 5
Training loss: 2.2092792987823486
Validation loss: 2.842086368991483

Epoch: 6| Step: 6
Training loss: 2.8309237957000732
Validation loss: 2.8402126373783236

Epoch: 6| Step: 7
Training loss: 4.0402727127075195
Validation loss: 2.842632983320503

Epoch: 6| Step: 8
Training loss: 3.0236282348632812
Validation loss: 2.836037822948989

Epoch: 6| Step: 9
Training loss: 2.829902172088623
Validation loss: 2.840868934508293

Epoch: 6| Step: 10
Training loss: 2.8003337383270264
Validation loss: 2.8344621299415507

Epoch: 6| Step: 11
Training loss: 3.2173428535461426
Validation loss: 2.8315146328300558

Epoch: 6| Step: 12
Training loss: 2.5330848693847656
Validation loss: 2.8217382507939495

Epoch: 6| Step: 13
Training loss: 1.8947317600250244
Validation loss: 2.822681837184455

Epoch: 22| Step: 0
Training loss: 2.6820900440216064
Validation loss: 2.8165889529771704

Epoch: 6| Step: 1
Training loss: 3.2903363704681396
Validation loss: 2.8141070386414886

Epoch: 6| Step: 2
Training loss: 2.905533790588379
Validation loss: 2.812715084322037

Epoch: 6| Step: 3
Training loss: 2.5326778888702393
Validation loss: 2.8145604236151582

Epoch: 6| Step: 4
Training loss: 2.691358804702759
Validation loss: 2.8133017401541434

Epoch: 6| Step: 5
Training loss: 3.040487051010132
Validation loss: 2.8143991347282165

Epoch: 6| Step: 6
Training loss: 2.797332763671875
Validation loss: 2.809113302538472

Epoch: 6| Step: 7
Training loss: 3.203415870666504
Validation loss: 2.814560497960737

Epoch: 6| Step: 8
Training loss: 2.7384934425354004
Validation loss: 2.806465388626181

Epoch: 6| Step: 9
Training loss: 3.8602516651153564
Validation loss: 2.7943168583736626

Epoch: 6| Step: 10
Training loss: 3.082486629486084
Validation loss: 2.7916073209495953

Epoch: 6| Step: 11
Training loss: 3.0699310302734375
Validation loss: 2.795947320999638

Epoch: 6| Step: 12
Training loss: 2.366340398788452
Validation loss: 2.802375767820625

Epoch: 6| Step: 13
Training loss: 2.484506845474243
Validation loss: 2.8477407860499557

Epoch: 23| Step: 0
Training loss: 2.773780584335327
Validation loss: 2.7970927197446107

Epoch: 6| Step: 1
Training loss: 1.9944570064544678
Validation loss: 2.793714825825025

Epoch: 6| Step: 2
Training loss: 2.745767831802368
Validation loss: 2.8067030111948648

Epoch: 6| Step: 3
Training loss: 2.916790008544922
Validation loss: 2.824804041975288

Epoch: 6| Step: 4
Training loss: 3.97051739692688
Validation loss: 2.896143356959025

Epoch: 6| Step: 5
Training loss: 2.9321250915527344
Validation loss: 2.8273958980396228

Epoch: 6| Step: 6
Training loss: 3.1425483226776123
Validation loss: 2.7993429168578117

Epoch: 6| Step: 7
Training loss: 3.4185221195220947
Validation loss: 2.7951376822686966

Epoch: 6| Step: 8
Training loss: 2.636955499649048
Validation loss: 2.7928773203203754

Epoch: 6| Step: 9
Training loss: 2.778846025466919
Validation loss: 2.7949427507256948

Epoch: 6| Step: 10
Training loss: 2.9259138107299805
Validation loss: 2.7953975918472453

Epoch: 6| Step: 11
Training loss: 3.167318344116211
Validation loss: 2.7937880844198246

Epoch: 6| Step: 12
Training loss: 3.0006184577941895
Validation loss: 2.7946137510320193

Epoch: 6| Step: 13
Training loss: 2.03948974609375
Validation loss: 2.7931358224602154

Epoch: 24| Step: 0
Training loss: 3.7026469707489014
Validation loss: 2.7837201267160396

Epoch: 6| Step: 1
Training loss: 2.2668957710266113
Validation loss: 2.7847050595027145

Epoch: 6| Step: 2
Training loss: 2.477891683578491
Validation loss: 2.7832357742453135

Epoch: 6| Step: 3
Training loss: 2.2303128242492676
Validation loss: 2.7792060939214562

Epoch: 6| Step: 4
Training loss: 3.4048283100128174
Validation loss: 2.7720695977569907

Epoch: 6| Step: 5
Training loss: 2.2248432636260986
Validation loss: 2.7703292651843

Epoch: 6| Step: 6
Training loss: 2.5468356609344482
Validation loss: 2.7726181040528

Epoch: 6| Step: 7
Training loss: 3.0487213134765625
Validation loss: 2.765691298310475

Epoch: 6| Step: 8
Training loss: 3.0535168647766113
Validation loss: 2.7622253817896687

Epoch: 6| Step: 9
Training loss: 3.349292755126953
Validation loss: 2.778107935382474

Epoch: 6| Step: 10
Training loss: 3.068253755569458
Validation loss: 2.7836957644390803

Epoch: 6| Step: 11
Training loss: 2.5784831047058105
Validation loss: 2.774245544146466

Epoch: 6| Step: 12
Training loss: 3.340381145477295
Validation loss: 2.7618129330296672

Epoch: 6| Step: 13
Training loss: 3.450223207473755
Validation loss: 2.756796352324947

Epoch: 25| Step: 0
Training loss: 3.2445058822631836
Validation loss: 2.75682472413586

Epoch: 6| Step: 1
Training loss: 2.487980365753174
Validation loss: 2.775313554271575

Epoch: 6| Step: 2
Training loss: 3.392223596572876
Validation loss: 2.7817713111959477

Epoch: 6| Step: 3
Training loss: 3.3425374031066895
Validation loss: 2.762380974267119

Epoch: 6| Step: 4
Training loss: 2.2745375633239746
Validation loss: 2.752893791403822

Epoch: 6| Step: 5
Training loss: 2.848935127258301
Validation loss: 2.736354310025451

Epoch: 6| Step: 6
Training loss: 2.6387674808502197
Validation loss: 2.7405026087196926

Epoch: 6| Step: 7
Training loss: 3.3025317192077637
Validation loss: 2.733112596696423

Epoch: 6| Step: 8
Training loss: 2.601512908935547
Validation loss: 2.7393682823386243

Epoch: 6| Step: 9
Training loss: 3.2807302474975586
Validation loss: 2.7331013525685957

Epoch: 6| Step: 10
Training loss: 2.7136456966400146
Validation loss: 2.7353061040242515

Epoch: 6| Step: 11
Training loss: 2.257819652557373
Validation loss: 2.732410233507874

Epoch: 6| Step: 12
Training loss: 3.1357412338256836
Validation loss: 2.7329161884964153

Epoch: 6| Step: 13
Training loss: 2.5421857833862305
Validation loss: 2.7286805568202848

Epoch: 26| Step: 0
Training loss: 2.6219563484191895
Validation loss: 2.7231717391680648

Epoch: 6| Step: 1
Training loss: 2.0878758430480957
Validation loss: 2.7254978277349986

Epoch: 6| Step: 2
Training loss: 2.9123826026916504
Validation loss: 2.720784900008991

Epoch: 6| Step: 3
Training loss: 3.447814464569092
Validation loss: 2.7222864858565794

Epoch: 6| Step: 4
Training loss: 1.9718348979949951
Validation loss: 2.72575036428308

Epoch: 6| Step: 5
Training loss: 2.97937273979187
Validation loss: 2.7326512336730957

Epoch: 6| Step: 6
Training loss: 3.3982644081115723
Validation loss: 2.7388079089503132

Epoch: 6| Step: 7
Training loss: 3.2255892753601074
Validation loss: 2.7394000586643013

Epoch: 6| Step: 8
Training loss: 2.7953124046325684
Validation loss: 2.7289080312175136

Epoch: 6| Step: 9
Training loss: 2.7339558601379395
Validation loss: 2.727446581727715

Epoch: 6| Step: 10
Training loss: 2.5842885971069336
Validation loss: 2.725776377544608

Epoch: 6| Step: 11
Training loss: 2.6333444118499756
Validation loss: 2.722570853848611

Epoch: 6| Step: 12
Training loss: 3.256517171859741
Validation loss: 2.711242591181109

Epoch: 6| Step: 13
Training loss: 3.723782539367676
Validation loss: 2.7066350598489084

Epoch: 27| Step: 0
Training loss: 2.926555871963501
Validation loss: 2.7080277140422533

Epoch: 6| Step: 1
Training loss: 2.740635871887207
Validation loss: 2.7030543537550074

Epoch: 6| Step: 2
Training loss: 3.3565549850463867
Validation loss: 2.708130713432066

Epoch: 6| Step: 3
Training loss: 2.277099132537842
Validation loss: 2.7009932379568777

Epoch: 6| Step: 4
Training loss: 2.9506583213806152
Validation loss: 2.7184685225127847

Epoch: 6| Step: 5
Training loss: 2.9760236740112305
Validation loss: 2.7014741487400507

Epoch: 6| Step: 6
Training loss: 2.166231155395508
Validation loss: 2.698201423050255

Epoch: 6| Step: 7
Training loss: 2.518470525741577
Validation loss: 2.7020329377984487

Epoch: 6| Step: 8
Training loss: 2.896615982055664
Validation loss: 2.700516903272239

Epoch: 6| Step: 9
Training loss: 3.616544246673584
Validation loss: 2.694252551242869

Epoch: 6| Step: 10
Training loss: 2.715075731277466
Validation loss: 2.6877602146517847

Epoch: 6| Step: 11
Training loss: 2.8713817596435547
Validation loss: 2.687194901127969

Epoch: 6| Step: 12
Training loss: 3.054255723953247
Validation loss: 2.6890721500560804

Epoch: 6| Step: 13
Training loss: 2.8609132766723633
Validation loss: 2.6883567943367908

Epoch: 28| Step: 0
Training loss: 3.0282607078552246
Validation loss: 2.6840328836953766

Epoch: 6| Step: 1
Training loss: 2.9129037857055664
Validation loss: 2.7025544258856002

Epoch: 6| Step: 2
Training loss: 3.3755955696105957
Validation loss: 2.7205655626071397

Epoch: 6| Step: 3
Training loss: 3.345268487930298
Validation loss: 2.743359842608052

Epoch: 6| Step: 4
Training loss: 2.2330667972564697
Validation loss: 2.6908446717005905

Epoch: 6| Step: 5
Training loss: 2.3191518783569336
Validation loss: 2.6830489097102994

Epoch: 6| Step: 6
Training loss: 3.3368897438049316
Validation loss: 2.6848045343993814

Epoch: 6| Step: 7
Training loss: 1.8617005348205566
Validation loss: 2.6962230333717923

Epoch: 6| Step: 8
Training loss: 3.2642245292663574
Validation loss: 2.7072143605960313

Epoch: 6| Step: 9
Training loss: 2.5177271366119385
Validation loss: 2.71183943235746

Epoch: 6| Step: 10
Training loss: 2.5607924461364746
Validation loss: 2.702470579454976

Epoch: 6| Step: 11
Training loss: 3.2912139892578125
Validation loss: 2.6963389945286576

Epoch: 6| Step: 12
Training loss: 2.5298080444335938
Validation loss: 2.7130950163769465

Epoch: 6| Step: 13
Training loss: 3.7959251403808594
Validation loss: 2.701570649300852

Epoch: 29| Step: 0
Training loss: 2.7054357528686523
Validation loss: 2.710311310265654

Epoch: 6| Step: 1
Training loss: 3.0981252193450928
Validation loss: 2.7137056243035103

Epoch: 6| Step: 2
Training loss: 2.7890684604644775
Validation loss: 2.7135192373747468

Epoch: 6| Step: 3
Training loss: 2.5023345947265625
Validation loss: 2.698970687004828

Epoch: 6| Step: 4
Training loss: 3.44683837890625
Validation loss: 2.686989766295238

Epoch: 6| Step: 5
Training loss: 2.3307878971099854
Validation loss: 2.674572660077003

Epoch: 6| Step: 6
Training loss: 2.3464579582214355
Validation loss: 2.6677548064980456

Epoch: 6| Step: 7
Training loss: 2.8641700744628906
Validation loss: 2.6664423532383417

Epoch: 6| Step: 8
Training loss: 3.271782875061035
Validation loss: 2.6703344878330024

Epoch: 6| Step: 9
Training loss: 2.8086328506469727
Validation loss: 2.6718256422268447

Epoch: 6| Step: 10
Training loss: 2.015261650085449
Validation loss: 2.6714271960719937

Epoch: 6| Step: 11
Training loss: 2.698509931564331
Validation loss: 2.674220021053027

Epoch: 6| Step: 12
Training loss: 3.4270973205566406
Validation loss: 2.6795793887107604

Epoch: 6| Step: 13
Training loss: 3.8554911613464355
Validation loss: 2.6692717024075088

Epoch: 30| Step: 0
Training loss: 2.837312936782837
Validation loss: 2.6654642140993507

Epoch: 6| Step: 1
Training loss: 2.9555017948150635
Validation loss: 2.6618603057758783

Epoch: 6| Step: 2
Training loss: 3.049077033996582
Validation loss: 2.661487207617811

Epoch: 6| Step: 3
Training loss: 3.0897269248962402
Validation loss: 2.6620425178158666

Epoch: 6| Step: 4
Training loss: 3.513946533203125
Validation loss: 2.6596680559137815

Epoch: 6| Step: 5
Training loss: 1.8025362491607666
Validation loss: 2.660842390470607

Epoch: 6| Step: 6
Training loss: 2.9092776775360107
Validation loss: 2.6532593516893286

Epoch: 6| Step: 7
Training loss: 2.8268485069274902
Validation loss: 2.6475917857180358

Epoch: 6| Step: 8
Training loss: 4.044398784637451
Validation loss: 2.65080120999326

Epoch: 6| Step: 9
Training loss: 3.069668769836426
Validation loss: 2.6533103168651624

Epoch: 6| Step: 10
Training loss: 2.1434528827667236
Validation loss: 2.6557198314256567

Epoch: 6| Step: 11
Training loss: 2.2361087799072266
Validation loss: 2.651210736202937

Epoch: 6| Step: 12
Training loss: 2.239281177520752
Validation loss: 2.6453230637376026

Epoch: 6| Step: 13
Training loss: 2.6980020999908447
Validation loss: 2.638466114638954

Epoch: 31| Step: 0
Training loss: 2.3120269775390625
Validation loss: 2.6427602127034175

Epoch: 6| Step: 1
Training loss: 2.7866673469543457
Validation loss: 2.6424192792625836

Epoch: 6| Step: 2
Training loss: 4.056230545043945
Validation loss: 2.6462053252804663

Epoch: 6| Step: 3
Training loss: 3.013838291168213
Validation loss: 2.650976111811976

Epoch: 6| Step: 4
Training loss: 2.2597556114196777
Validation loss: 2.6451946099599204

Epoch: 6| Step: 5
Training loss: 3.1159863471984863
Validation loss: 2.636504850079936

Epoch: 6| Step: 6
Training loss: 2.800640106201172
Validation loss: 2.6305292985772573

Epoch: 6| Step: 7
Training loss: 3.3387763500213623
Validation loss: 2.6278093835358978

Epoch: 6| Step: 8
Training loss: 1.2831224203109741
Validation loss: 2.6200282214790263

Epoch: 6| Step: 9
Training loss: 2.691793441772461
Validation loss: 2.619960774657547

Epoch: 6| Step: 10
Training loss: 2.7243194580078125
Validation loss: 2.6169862388282694

Epoch: 6| Step: 11
Training loss: 2.5967955589294434
Validation loss: 2.6165849188322663

Epoch: 6| Step: 12
Training loss: 3.3844995498657227
Validation loss: 2.612003613543767

Epoch: 6| Step: 13
Training loss: 2.7848381996154785
Validation loss: 2.615527945180093

Epoch: 32| Step: 0
Training loss: 2.5654025077819824
Validation loss: 2.617381570159748

Epoch: 6| Step: 1
Training loss: 2.230954170227051
Validation loss: 2.6367946850356234

Epoch: 6| Step: 2
Training loss: 3.2773630619049072
Validation loss: 2.6542427873098724

Epoch: 6| Step: 3
Training loss: 3.2195725440979004
Validation loss: 2.6445132968246297

Epoch: 6| Step: 4
Training loss: 3.121523380279541
Validation loss: 2.6365512365935952

Epoch: 6| Step: 5
Training loss: 2.4219369888305664
Validation loss: 2.624091625213623

Epoch: 6| Step: 6
Training loss: 3.1028170585632324
Validation loss: 2.6056649659269597

Epoch: 6| Step: 7
Training loss: 3.1478850841522217
Validation loss: 2.608169588991391

Epoch: 6| Step: 8
Training loss: 2.6478824615478516
Validation loss: 2.613411931581395

Epoch: 6| Step: 9
Training loss: 3.057420253753662
Validation loss: 2.6116965688684934

Epoch: 6| Step: 10
Training loss: 2.5848312377929688
Validation loss: 2.6128967500502065

Epoch: 6| Step: 11
Training loss: 3.1678555011749268
Validation loss: 2.6140166969709497

Epoch: 6| Step: 12
Training loss: 2.303405523300171
Validation loss: 2.6079773441437752

Epoch: 6| Step: 13
Training loss: 1.7592427730560303
Validation loss: 2.6015407167455202

Epoch: 33| Step: 0
Training loss: 2.7418904304504395
Validation loss: 2.5974308624062488

Epoch: 6| Step: 1
Training loss: 3.616278648376465
Validation loss: 2.600479038812781

Epoch: 6| Step: 2
Training loss: 2.753216028213501
Validation loss: 2.6004270789443806

Epoch: 6| Step: 3
Training loss: 2.555429458618164
Validation loss: 2.617564132136683

Epoch: 6| Step: 4
Training loss: 2.624464511871338
Validation loss: 2.6422864083320863

Epoch: 6| Step: 5
Training loss: 3.470980405807495
Validation loss: 2.682798377929195

Epoch: 6| Step: 6
Training loss: 3.0806326866149902
Validation loss: 2.6340014293629634

Epoch: 6| Step: 7
Training loss: 2.474543571472168
Validation loss: 2.6022583053958033

Epoch: 6| Step: 8
Training loss: 1.7587862014770508
Validation loss: 2.5968998311668314

Epoch: 6| Step: 9
Training loss: 3.326982259750366
Validation loss: 2.611686416851577

Epoch: 6| Step: 10
Training loss: 2.0526485443115234
Validation loss: 2.6349564034451722

Epoch: 6| Step: 11
Training loss: 2.908132553100586
Validation loss: 2.633251344003985

Epoch: 6| Step: 12
Training loss: 2.9361066818237305
Validation loss: 2.64357509920674

Epoch: 6| Step: 13
Training loss: 2.773967742919922
Validation loss: 2.625925674233385

Epoch: 34| Step: 0
Training loss: 2.626950979232788
Validation loss: 2.612408405991011

Epoch: 6| Step: 1
Training loss: 3.919829845428467
Validation loss: 2.5955723511275424

Epoch: 6| Step: 2
Training loss: 3.0031275749206543
Validation loss: 2.5882204912042104

Epoch: 6| Step: 3
Training loss: 2.495180606842041
Validation loss: 2.597486403680617

Epoch: 6| Step: 4
Training loss: 2.6906399726867676
Validation loss: 2.6031913629142185

Epoch: 6| Step: 5
Training loss: 3.2647550106048584
Validation loss: 2.6158461134920836

Epoch: 6| Step: 6
Training loss: 2.966505527496338
Validation loss: 2.6146579993668424

Epoch: 6| Step: 7
Training loss: 3.168118953704834
Validation loss: 2.5983401319032073

Epoch: 6| Step: 8
Training loss: 1.8704793453216553
Validation loss: 2.611161414013114

Epoch: 6| Step: 9
Training loss: 3.4520440101623535
Validation loss: 2.589562672440724

Epoch: 6| Step: 10
Training loss: 2.5029356479644775
Validation loss: 2.585082692484702

Epoch: 6| Step: 11
Training loss: 2.0478758811950684
Validation loss: 2.5811899438981087

Epoch: 6| Step: 12
Training loss: 2.5187735557556152
Validation loss: 2.58134215365174

Epoch: 6| Step: 13
Training loss: 2.0511021614074707
Validation loss: 2.580563916954943

Epoch: 35| Step: 0
Training loss: 3.1141433715820312
Validation loss: 2.5819747832513626

Epoch: 6| Step: 1
Training loss: 2.297736167907715
Validation loss: 2.5935392918125277

Epoch: 6| Step: 2
Training loss: 2.1251869201660156
Validation loss: 2.591114487699283

Epoch: 6| Step: 3
Training loss: 3.2624893188476562
Validation loss: 2.5859712195652786

Epoch: 6| Step: 4
Training loss: 2.713257312774658
Validation loss: 2.5873892307281494

Epoch: 6| Step: 5
Training loss: 3.034903049468994
Validation loss: 2.5881870690212456

Epoch: 6| Step: 6
Training loss: 2.325064182281494
Validation loss: 2.583481563034878

Epoch: 6| Step: 7
Training loss: 2.558952569961548
Validation loss: 2.581954981691094

Epoch: 6| Step: 8
Training loss: 2.34464430809021
Validation loss: 2.5834052767804874

Epoch: 6| Step: 9
Training loss: 2.989968776702881
Validation loss: 2.577601896819248

Epoch: 6| Step: 10
Training loss: 2.727797508239746
Validation loss: 2.5796874697490404

Epoch: 6| Step: 11
Training loss: 3.3038768768310547
Validation loss: 2.5761813937976794

Epoch: 6| Step: 12
Training loss: 2.7929935455322266
Validation loss: 2.5653918122732513

Epoch: 6| Step: 13
Training loss: 3.282142162322998
Validation loss: 2.568180080383055

Epoch: 36| Step: 0
Training loss: 3.1864171028137207
Validation loss: 2.57436228054826

Epoch: 6| Step: 1
Training loss: 2.5460309982299805
Validation loss: 2.569672892170568

Epoch: 6| Step: 2
Training loss: 3.142322063446045
Validation loss: 2.565244238863709

Epoch: 6| Step: 3
Training loss: 2.5417697429656982
Validation loss: 2.5673430581246652

Epoch: 6| Step: 4
Training loss: 3.3886969089508057
Validation loss: 2.560626527314545

Epoch: 6| Step: 5
Training loss: 2.2371206283569336
Validation loss: 2.5631118333467873

Epoch: 6| Step: 6
Training loss: 3.0056228637695312
Validation loss: 2.5671017657044115

Epoch: 6| Step: 7
Training loss: 2.451202869415283
Validation loss: 2.5674184471048336

Epoch: 6| Step: 8
Training loss: 2.760495662689209
Validation loss: 2.5669171169239986

Epoch: 6| Step: 9
Training loss: 2.609996795654297
Validation loss: 2.5769178585339616

Epoch: 6| Step: 10
Training loss: 2.5887553691864014
Validation loss: 2.5794534401227067

Epoch: 6| Step: 11
Training loss: 2.5295004844665527
Validation loss: 2.571118805998115

Epoch: 6| Step: 12
Training loss: 3.216724157333374
Validation loss: 2.5691268700425343

Epoch: 6| Step: 13
Training loss: 1.9675791263580322
Validation loss: 2.5722029106591338

Epoch: 37| Step: 0
Training loss: 3.4537553787231445
Validation loss: 2.5601561479671027

Epoch: 6| Step: 1
Training loss: 2.032580852508545
Validation loss: 2.564436217790009

Epoch: 6| Step: 2
Training loss: 2.4791829586029053
Validation loss: 2.5682949045652985

Epoch: 6| Step: 3
Training loss: 2.3256890773773193
Validation loss: 2.569828510284424

Epoch: 6| Step: 4
Training loss: 3.550168752670288
Validation loss: 2.5740121513284664

Epoch: 6| Step: 5
Training loss: 2.7562999725341797
Validation loss: 2.5926760089012886

Epoch: 6| Step: 6
Training loss: 2.692812442779541
Validation loss: 2.571615042225007

Epoch: 6| Step: 7
Training loss: 2.762605667114258
Validation loss: 2.5649690807506604

Epoch: 6| Step: 8
Training loss: 2.686666488647461
Validation loss: 2.558061533076789

Epoch: 6| Step: 9
Training loss: 2.828476905822754
Validation loss: 2.5551094393576346

Epoch: 6| Step: 10
Training loss: 3.0119717121124268
Validation loss: 2.556432444562194

Epoch: 6| Step: 11
Training loss: 1.933821439743042
Validation loss: 2.5605417810460573

Epoch: 6| Step: 12
Training loss: 2.8462941646575928
Validation loss: 2.5567028753219114

Epoch: 6| Step: 13
Training loss: 3.491969585418701
Validation loss: 2.5655460152574765

Epoch: 38| Step: 0
Training loss: 1.8647526502609253
Validation loss: 2.587326772751347

Epoch: 6| Step: 1
Training loss: 3.1405763626098633
Validation loss: 2.6447422632607083

Epoch: 6| Step: 2
Training loss: 2.7087769508361816
Validation loss: 2.670848200398107

Epoch: 6| Step: 3
Training loss: 2.5364537239074707
Validation loss: 2.653314898090978

Epoch: 6| Step: 4
Training loss: 2.6873910427093506
Validation loss: 2.5965435171640046

Epoch: 6| Step: 5
Training loss: 3.1150012016296387
Validation loss: 2.560496796843826

Epoch: 6| Step: 6
Training loss: 3.0320260524749756
Validation loss: 2.5570649434161443

Epoch: 6| Step: 7
Training loss: 2.0268566608428955
Validation loss: 2.5563055135870494

Epoch: 6| Step: 8
Training loss: 2.1862306594848633
Validation loss: 2.560035756839219

Epoch: 6| Step: 9
Training loss: 2.539156675338745
Validation loss: 2.557592732931978

Epoch: 6| Step: 10
Training loss: 3.4428138732910156
Validation loss: 2.558992488409883

Epoch: 6| Step: 11
Training loss: 3.524722099304199
Validation loss: 2.552416516888526

Epoch: 6| Step: 12
Training loss: 2.8183157444000244
Validation loss: 2.5564925619350967

Epoch: 6| Step: 13
Training loss: 3.226760149002075
Validation loss: 2.5499797098098265

Epoch: 39| Step: 0
Training loss: 2.4120516777038574
Validation loss: 2.5531171239832395

Epoch: 6| Step: 1
Training loss: 2.6291465759277344
Validation loss: 2.5535325901482695

Epoch: 6| Step: 2
Training loss: 2.8992958068847656
Validation loss: 2.560455758084533

Epoch: 6| Step: 3
Training loss: 2.527927875518799
Validation loss: 2.5674581425164336

Epoch: 6| Step: 4
Training loss: 2.5064640045166016
Validation loss: 2.5601248715513494

Epoch: 6| Step: 5
Training loss: 3.0940585136413574
Validation loss: 2.5590310148013535

Epoch: 6| Step: 6
Training loss: 2.7612385749816895
Validation loss: 2.5522327551277737

Epoch: 6| Step: 7
Training loss: 2.019160509109497
Validation loss: 2.5498964889075166

Epoch: 6| Step: 8
Training loss: 2.4107236862182617
Validation loss: 2.5477375215099705

Epoch: 6| Step: 9
Training loss: 2.745622158050537
Validation loss: 2.5444257028641237

Epoch: 6| Step: 10
Training loss: 3.784101963043213
Validation loss: 2.544337700772029

Epoch: 6| Step: 11
Training loss: 2.8296377658843994
Validation loss: 2.5444296867616716

Epoch: 6| Step: 12
Training loss: 2.9078171253204346
Validation loss: 2.5440147563975346

Epoch: 6| Step: 13
Training loss: 2.8587887287139893
Validation loss: 2.5423753389748196

Epoch: 40| Step: 0
Training loss: 2.5903403759002686
Validation loss: 2.5453793502623037

Epoch: 6| Step: 1
Training loss: 3.1408474445343018
Validation loss: 2.5473307204502884

Epoch: 6| Step: 2
Training loss: 2.639944076538086
Validation loss: 2.5489788157965547

Epoch: 6| Step: 3
Training loss: 2.620267152786255
Validation loss: 2.5534889698028564

Epoch: 6| Step: 4
Training loss: 2.0014233589172363
Validation loss: 2.5475086473649546

Epoch: 6| Step: 5
Training loss: 3.3920702934265137
Validation loss: 2.5595923444276214

Epoch: 6| Step: 6
Training loss: 2.902723550796509
Validation loss: 2.5542972959497923

Epoch: 6| Step: 7
Training loss: 2.4404296875
Validation loss: 2.546205694957446

Epoch: 6| Step: 8
Training loss: 2.3931753635406494
Validation loss: 2.551456651379985

Epoch: 6| Step: 9
Training loss: 2.9035634994506836
Validation loss: 2.537779890080934

Epoch: 6| Step: 10
Training loss: 2.864234447479248
Validation loss: 2.5331907426157305

Epoch: 6| Step: 11
Training loss: 3.187107563018799
Validation loss: 2.5365175867593415

Epoch: 6| Step: 12
Training loss: 2.7313225269317627
Validation loss: 2.540259486885481

Epoch: 6| Step: 13
Training loss: 2.0368247032165527
Validation loss: 2.543215218410697

Epoch: 41| Step: 0
Training loss: 2.8073630332946777
Validation loss: 2.5419297474686817

Epoch: 6| Step: 1
Training loss: 3.2828221321105957
Validation loss: 2.543339257599205

Epoch: 6| Step: 2
Training loss: 2.3886775970458984
Validation loss: 2.5577941351039435

Epoch: 6| Step: 3
Training loss: 3.137120246887207
Validation loss: 2.5471634787897908

Epoch: 6| Step: 4
Training loss: 2.510404109954834
Validation loss: 2.5339654645612164

Epoch: 6| Step: 5
Training loss: 2.7637362480163574
Validation loss: 2.537215732759045

Epoch: 6| Step: 6
Training loss: 2.710634231567383
Validation loss: 2.5338194652270247

Epoch: 6| Step: 7
Training loss: 2.370544910430908
Validation loss: 2.530897699376588

Epoch: 6| Step: 8
Training loss: 2.71171236038208
Validation loss: 2.52895607999576

Epoch: 6| Step: 9
Training loss: 2.476919651031494
Validation loss: 2.533096895422987

Epoch: 6| Step: 10
Training loss: 2.8570921421051025
Validation loss: 2.5352326823819067

Epoch: 6| Step: 11
Training loss: 2.483731746673584
Validation loss: 2.541637489872594

Epoch: 6| Step: 12
Training loss: 2.9874486923217773
Validation loss: 2.5432268752846667

Epoch: 6| Step: 13
Training loss: 2.806601047515869
Validation loss: 2.551487353540236

Epoch: 42| Step: 0
Training loss: 2.9707562923431396
Validation loss: 2.5790225510956137

Epoch: 6| Step: 1
Training loss: 2.3283627033233643
Validation loss: 2.577868923064201

Epoch: 6| Step: 2
Training loss: 1.6976392269134521
Validation loss: 2.572002092997233

Epoch: 6| Step: 3
Training loss: 3.4888148307800293
Validation loss: 2.535598808719266

Epoch: 6| Step: 4
Training loss: 2.0531396865844727
Validation loss: 2.5256260851378083

Epoch: 6| Step: 5
Training loss: 2.463365316390991
Validation loss: 2.52599351380461

Epoch: 6| Step: 6
Training loss: 2.69035267829895
Validation loss: 2.530271227641772

Epoch: 6| Step: 7
Training loss: 2.7406468391418457
Validation loss: 2.5337752808806715

Epoch: 6| Step: 8
Training loss: 3.2885539531707764
Validation loss: 2.5421118018447713

Epoch: 6| Step: 9
Training loss: 3.3951523303985596
Validation loss: 2.5519192218780518

Epoch: 6| Step: 10
Training loss: 2.68137264251709
Validation loss: 2.5539063689529256

Epoch: 6| Step: 11
Training loss: 2.7844326496124268
Validation loss: 2.549725317185925

Epoch: 6| Step: 12
Training loss: 3.153541088104248
Validation loss: 2.555168495383314

Epoch: 6| Step: 13
Training loss: 2.727644205093384
Validation loss: 2.5406571126753286

Epoch: 43| Step: 0
Training loss: 3.1429343223571777
Validation loss: 2.533037652251541

Epoch: 6| Step: 1
Training loss: 2.7112250328063965
Validation loss: 2.528283260201895

Epoch: 6| Step: 2
Training loss: 2.9898195266723633
Validation loss: 2.5296554949975785

Epoch: 6| Step: 3
Training loss: 2.5388641357421875
Validation loss: 2.530592144176524

Epoch: 6| Step: 4
Training loss: 3.0115089416503906
Validation loss: 2.5275308752572663

Epoch: 6| Step: 5
Training loss: 3.1071982383728027
Validation loss: 2.529774914505661

Epoch: 6| Step: 6
Training loss: 2.264249324798584
Validation loss: 2.535591789471206

Epoch: 6| Step: 7
Training loss: 2.5225117206573486
Validation loss: 2.5538968706643708

Epoch: 6| Step: 8
Training loss: 2.6878228187561035
Validation loss: 2.580369592994772

Epoch: 6| Step: 9
Training loss: 2.5962042808532715
Validation loss: 2.567428304303077

Epoch: 6| Step: 10
Training loss: 2.5684869289398193
Validation loss: 2.551695418614213

Epoch: 6| Step: 11
Training loss: 2.734017848968506
Validation loss: 2.5322515144143054

Epoch: 6| Step: 12
Training loss: 2.6202452182769775
Validation loss: 2.5221729124746015

Epoch: 6| Step: 13
Training loss: 2.5834286212921143
Validation loss: 2.5148912296500257

Epoch: 44| Step: 0
Training loss: 3.4301931858062744
Validation loss: 2.513903925495763

Epoch: 6| Step: 1
Training loss: 2.028996467590332
Validation loss: 2.5165947432159097

Epoch: 6| Step: 2
Training loss: 2.6465952396392822
Validation loss: 2.5138203149200766

Epoch: 6| Step: 3
Training loss: 2.9381296634674072
Validation loss: 2.513273387826899

Epoch: 6| Step: 4
Training loss: 3.024956226348877
Validation loss: 2.5154982561706216

Epoch: 6| Step: 5
Training loss: 2.349088430404663
Validation loss: 2.512179181139956

Epoch: 6| Step: 6
Training loss: 2.1368141174316406
Validation loss: 2.5121905265315885

Epoch: 6| Step: 7
Training loss: 3.054189682006836
Validation loss: 2.514676450401224

Epoch: 6| Step: 8
Training loss: 2.9777257442474365
Validation loss: 2.5111959236924366

Epoch: 6| Step: 9
Training loss: 2.127192497253418
Validation loss: 2.511831548906142

Epoch: 6| Step: 10
Training loss: 2.646620273590088
Validation loss: 2.511740912673294

Epoch: 6| Step: 11
Training loss: 2.5104196071624756
Validation loss: 2.516526483720349

Epoch: 6| Step: 12
Training loss: 3.4100565910339355
Validation loss: 2.521921452655587

Epoch: 6| Step: 13
Training loss: 2.841573476791382
Validation loss: 2.5212774815097934

Epoch: 45| Step: 0
Training loss: 3.171190023422241
Validation loss: 2.537248742195868

Epoch: 6| Step: 1
Training loss: 2.7139453887939453
Validation loss: 2.5526995915238575

Epoch: 6| Step: 2
Training loss: 2.632233142852783
Validation loss: 2.5580224375570975

Epoch: 6| Step: 3
Training loss: 2.8104898929595947
Validation loss: 2.541404821539438

Epoch: 6| Step: 4
Training loss: 2.38838791847229
Validation loss: 2.521391660936417

Epoch: 6| Step: 5
Training loss: 2.2332582473754883
Validation loss: 2.5172770651437903

Epoch: 6| Step: 6
Training loss: 2.984680652618408
Validation loss: 2.515892449245658

Epoch: 6| Step: 7
Training loss: 2.773695230484009
Validation loss: 2.5206960478136615

Epoch: 6| Step: 8
Training loss: 2.710341453552246
Validation loss: 2.5137770560479935

Epoch: 6| Step: 9
Training loss: 2.510110378265381
Validation loss: 2.5148461531567317

Epoch: 6| Step: 10
Training loss: 3.1340479850769043
Validation loss: 2.5180147232547885

Epoch: 6| Step: 11
Training loss: 2.281820297241211
Validation loss: 2.5181550569431757

Epoch: 6| Step: 12
Training loss: 2.718843460083008
Validation loss: 2.523956514173938

Epoch: 6| Step: 13
Training loss: 3.0561933517456055
Validation loss: 2.525647468464349

Epoch: 46| Step: 0
Training loss: 2.504662036895752
Validation loss: 2.529482526163901

Epoch: 6| Step: 1
Training loss: 2.0172128677368164
Validation loss: 2.527589269863662

Epoch: 6| Step: 2
Training loss: 2.1626408100128174
Validation loss: 2.527530017719474

Epoch: 6| Step: 3
Training loss: 2.437643527984619
Validation loss: 2.5383776131496636

Epoch: 6| Step: 4
Training loss: 3.244659900665283
Validation loss: 2.5345212413418676

Epoch: 6| Step: 5
Training loss: 2.9516518115997314
Validation loss: 2.546577694595501

Epoch: 6| Step: 6
Training loss: 2.9899706840515137
Validation loss: 2.5362019820879866

Epoch: 6| Step: 7
Training loss: 3.036160469055176
Validation loss: 2.514744912424395

Epoch: 6| Step: 8
Training loss: 3.181321620941162
Validation loss: 2.5104460049700994

Epoch: 6| Step: 9
Training loss: 2.3170714378356934
Validation loss: 2.5071978671576387

Epoch: 6| Step: 10
Training loss: 3.3216636180877686
Validation loss: 2.5118274765629924

Epoch: 6| Step: 11
Training loss: 2.9751553535461426
Validation loss: 2.506520689174693

Epoch: 6| Step: 12
Training loss: 2.738734006881714
Validation loss: 2.5091591483803204

Epoch: 6| Step: 13
Training loss: 1.7520699501037598
Validation loss: 2.522260591547976

Epoch: 47| Step: 0
Training loss: 3.0552191734313965
Validation loss: 2.52627924437164

Epoch: 6| Step: 1
Training loss: 2.56782865524292
Validation loss: 2.517028973948571

Epoch: 6| Step: 2
Training loss: 2.501115322113037
Validation loss: 2.515708197829544

Epoch: 6| Step: 3
Training loss: 1.9806772470474243
Validation loss: 2.5083904074084376

Epoch: 6| Step: 4
Training loss: 2.5711469650268555
Validation loss: 2.5058255990346274

Epoch: 6| Step: 5
Training loss: 2.2984514236450195
Validation loss: 2.502917315370293

Epoch: 6| Step: 6
Training loss: 2.8918654918670654
Validation loss: 2.5037742045617875

Epoch: 6| Step: 7
Training loss: 2.649799346923828
Validation loss: 2.5024583544782413

Epoch: 6| Step: 8
Training loss: 3.2177670001983643
Validation loss: 2.508075778202344

Epoch: 6| Step: 9
Training loss: 3.1148226261138916
Validation loss: 2.509523199450585

Epoch: 6| Step: 10
Training loss: 2.646592140197754
Validation loss: 2.508449049406154

Epoch: 6| Step: 11
Training loss: 2.775217056274414
Validation loss: 2.50766174767607

Epoch: 6| Step: 12
Training loss: 3.1493518352508545
Validation loss: 2.499349317243022

Epoch: 6| Step: 13
Training loss: 2.5285403728485107
Validation loss: 2.502421604689731

Epoch: 48| Step: 0
Training loss: 1.7896913290023804
Validation loss: 2.5066871232883905

Epoch: 6| Step: 1
Training loss: 2.5906131267547607
Validation loss: 2.5206076457936275

Epoch: 6| Step: 2
Training loss: 2.258911609649658
Validation loss: 2.5365427130012104

Epoch: 6| Step: 3
Training loss: 3.3314905166625977
Validation loss: 2.556950605043801

Epoch: 6| Step: 4
Training loss: 2.8843653202056885
Validation loss: 2.5727315231036116

Epoch: 6| Step: 5
Training loss: 2.4307777881622314
Validation loss: 2.5511715540321926

Epoch: 6| Step: 6
Training loss: 3.040083885192871
Validation loss: 2.551730194399434

Epoch: 6| Step: 7
Training loss: 2.9686546325683594
Validation loss: 2.5429534989018596

Epoch: 6| Step: 8
Training loss: 3.2860569953918457
Validation loss: 2.5264023042494252

Epoch: 6| Step: 9
Training loss: 2.919027805328369
Validation loss: 2.5170642111891057

Epoch: 6| Step: 10
Training loss: 2.4762697219848633
Validation loss: 2.513105538583571

Epoch: 6| Step: 11
Training loss: 2.2303483486175537
Validation loss: 2.5076733763499925

Epoch: 6| Step: 12
Training loss: 2.7610068321228027
Validation loss: 2.5028051253288024

Epoch: 6| Step: 13
Training loss: 3.2711164951324463
Validation loss: 2.4966437175709713

Epoch: 49| Step: 0
Training loss: 3.257537603378296
Validation loss: 2.4930683246222873

Epoch: 6| Step: 1
Training loss: 2.3096094131469727
Validation loss: 2.491692832721177

Epoch: 6| Step: 2
Training loss: 2.1404221057891846
Validation loss: 2.4956067787703646

Epoch: 6| Step: 3
Training loss: 2.304680585861206
Validation loss: 2.4982742801789315

Epoch: 6| Step: 4
Training loss: 2.499413013458252
Validation loss: 2.4950203126476658

Epoch: 6| Step: 5
Training loss: 2.272183895111084
Validation loss: 2.4937345058687272

Epoch: 6| Step: 6
Training loss: 3.7350106239318848
Validation loss: 2.4928774987497637

Epoch: 6| Step: 7
Training loss: 2.9415245056152344
Validation loss: 2.4989339228599303

Epoch: 6| Step: 8
Training loss: 3.278132915496826
Validation loss: 2.495332520495179

Epoch: 6| Step: 9
Training loss: 2.6252830028533936
Validation loss: 2.4966169839264243

Epoch: 6| Step: 10
Training loss: 2.956116199493408
Validation loss: 2.493012635938583

Epoch: 6| Step: 11
Training loss: 3.077310562133789
Validation loss: 2.495745743474653

Epoch: 6| Step: 12
Training loss: 1.8220988512039185
Validation loss: 2.4954347225927536

Epoch: 6| Step: 13
Training loss: 2.5408363342285156
Validation loss: 2.5055673891498196

Epoch: 50| Step: 0
Training loss: 2.6068313121795654
Validation loss: 2.501027786603538

Epoch: 6| Step: 1
Training loss: 2.4253334999084473
Validation loss: 2.5016884803771973

Epoch: 6| Step: 2
Training loss: 2.957430601119995
Validation loss: 2.49297063068677

Epoch: 6| Step: 3
Training loss: 2.8301451206207275
Validation loss: 2.4939017706019904

Epoch: 6| Step: 4
Training loss: 2.539836883544922
Validation loss: 2.4890631527029057

Epoch: 6| Step: 5
Training loss: 2.9262285232543945
Validation loss: 2.492665865088022

Epoch: 6| Step: 6
Training loss: 2.4215328693389893
Validation loss: 2.4896098208683792

Epoch: 6| Step: 7
Training loss: 2.289574146270752
Validation loss: 2.4910348897339194

Epoch: 6| Step: 8
Training loss: 2.871432304382324
Validation loss: 2.49222828495887

Epoch: 6| Step: 9
Training loss: 2.0102760791778564
Validation loss: 2.4920691110754527

Epoch: 6| Step: 10
Training loss: 2.698657989501953
Validation loss: 2.4951519145760486

Epoch: 6| Step: 11
Training loss: 2.744826555252075
Validation loss: 2.4971152403021373

Epoch: 6| Step: 12
Training loss: 3.330570697784424
Validation loss: 2.505813919087892

Epoch: 6| Step: 13
Training loss: 3.400416851043701
Validation loss: 2.50845520470732

Epoch: 51| Step: 0
Training loss: 2.5688552856445312
Validation loss: 2.494703059555382

Epoch: 6| Step: 1
Training loss: 2.7275619506835938
Validation loss: 2.49356758722695

Epoch: 6| Step: 2
Training loss: 3.333037853240967
Validation loss: 2.4923876229152886

Epoch: 6| Step: 3
Training loss: 2.938713550567627
Validation loss: 2.4930110080267793

Epoch: 6| Step: 4
Training loss: 2.799604654312134
Validation loss: 2.4889064719600063

Epoch: 6| Step: 5
Training loss: 1.8527623414993286
Validation loss: 2.4930098056793213

Epoch: 6| Step: 6
Training loss: 2.9079251289367676
Validation loss: 2.488067667971375

Epoch: 6| Step: 7
Training loss: 2.6197824478149414
Validation loss: 2.493380549133465

Epoch: 6| Step: 8
Training loss: 2.1004202365875244
Validation loss: 2.4940362181714786

Epoch: 6| Step: 9
Training loss: 2.377373695373535
Validation loss: 2.495243162237188

Epoch: 6| Step: 10
Training loss: 2.461221694946289
Validation loss: 2.496560911978445

Epoch: 6| Step: 11
Training loss: 2.474168300628662
Validation loss: 2.4962518497179915

Epoch: 6| Step: 12
Training loss: 3.6608850955963135
Validation loss: 2.4938413686649774

Epoch: 6| Step: 13
Training loss: 2.7798233032226562
Validation loss: 2.496643715007331

Epoch: 52| Step: 0
Training loss: 3.454836130142212
Validation loss: 2.492131645961474

Epoch: 6| Step: 1
Training loss: 2.4148106575012207
Validation loss: 2.485564101126886

Epoch: 6| Step: 2
Training loss: 1.8794907331466675
Validation loss: 2.4855851486165035

Epoch: 6| Step: 3
Training loss: 2.324258327484131
Validation loss: 2.4863982713350685

Epoch: 6| Step: 4
Training loss: 1.2224860191345215
Validation loss: 2.483249079796576

Epoch: 6| Step: 5
Training loss: 3.429955005645752
Validation loss: 2.482502527134393

Epoch: 6| Step: 6
Training loss: 3.546290874481201
Validation loss: 2.483999216428367

Epoch: 6| Step: 7
Training loss: 2.5796120166778564
Validation loss: 2.4857948185295187

Epoch: 6| Step: 8
Training loss: 2.48796010017395
Validation loss: 2.487826639606107

Epoch: 6| Step: 9
Training loss: 3.2665576934814453
Validation loss: 2.4886815317215456

Epoch: 6| Step: 10
Training loss: 2.321500539779663
Validation loss: 2.4825490264482397

Epoch: 6| Step: 11
Training loss: 3.192655324935913
Validation loss: 2.4935750089665896

Epoch: 6| Step: 12
Training loss: 2.887751817703247
Validation loss: 2.4992485072023127

Epoch: 6| Step: 13
Training loss: 2.4238429069519043
Validation loss: 2.5094708627270115

Epoch: 53| Step: 0
Training loss: 3.0976955890655518
Validation loss: 2.499301310508482

Epoch: 6| Step: 1
Training loss: 2.6949665546417236
Validation loss: 2.488921214175481

Epoch: 6| Step: 2
Training loss: 2.765404224395752
Validation loss: 2.4829324855599353

Epoch: 6| Step: 3
Training loss: 2.7060089111328125
Validation loss: 2.4808777788633942

Epoch: 6| Step: 4
Training loss: 2.807060718536377
Validation loss: 2.485258886891027

Epoch: 6| Step: 5
Training loss: 2.7132999897003174
Validation loss: 2.482050895690918

Epoch: 6| Step: 6
Training loss: 1.8526297807693481
Validation loss: 2.4867252226798766

Epoch: 6| Step: 7
Training loss: 2.6392083168029785
Validation loss: 2.486062243420591

Epoch: 6| Step: 8
Training loss: 2.4033989906311035
Validation loss: 2.487059406054917

Epoch: 6| Step: 9
Training loss: 3.370211124420166
Validation loss: 2.497981027890277

Epoch: 6| Step: 10
Training loss: 1.6679614782333374
Validation loss: 2.5051825072175715

Epoch: 6| Step: 11
Training loss: 3.2629995346069336
Validation loss: 2.5032138491189606

Epoch: 6| Step: 12
Training loss: 2.9441487789154053
Validation loss: 2.4937541279741513

Epoch: 6| Step: 13
Training loss: 2.7910494804382324
Validation loss: 2.4880768970776628

Epoch: 54| Step: 0
Training loss: 2.6746604442596436
Validation loss: 2.4856128333717264

Epoch: 6| Step: 1
Training loss: 2.8699703216552734
Validation loss: 2.479834366870183

Epoch: 6| Step: 2
Training loss: 2.5622870922088623
Validation loss: 2.4786015402886177

Epoch: 6| Step: 3
Training loss: 2.6967148780822754
Validation loss: 2.4784584404319845

Epoch: 6| Step: 4
Training loss: 2.5524425506591797
Validation loss: 2.476403997790429

Epoch: 6| Step: 5
Training loss: 3.157804489135742
Validation loss: 2.479989549165131

Epoch: 6| Step: 6
Training loss: 3.3287501335144043
Validation loss: 2.475943160313432

Epoch: 6| Step: 7
Training loss: 2.399254083633423
Validation loss: 2.4726242096193376

Epoch: 6| Step: 8
Training loss: 2.137129545211792
Validation loss: 2.4777495297052528

Epoch: 6| Step: 9
Training loss: 2.447556972503662
Validation loss: 2.477503325349541

Epoch: 6| Step: 10
Training loss: 3.469310760498047
Validation loss: 2.483331388042819

Epoch: 6| Step: 11
Training loss: 2.1873090267181396
Validation loss: 2.477301841141075

Epoch: 6| Step: 12
Training loss: 2.773236036300659
Validation loss: 2.4833803023061445

Epoch: 6| Step: 13
Training loss: 2.0865836143493652
Validation loss: 2.4913037207818802

Epoch: 55| Step: 0
Training loss: 3.2990775108337402
Validation loss: 2.5218770760361866

Epoch: 6| Step: 1
Training loss: 2.67234468460083
Validation loss: 2.5709597372239634

Epoch: 6| Step: 2
Training loss: 3.466932773590088
Validation loss: 2.572758225984471

Epoch: 6| Step: 3
Training loss: 2.697336435317993
Validation loss: 2.561842110849196

Epoch: 6| Step: 4
Training loss: 2.889667272567749
Validation loss: 2.5466513582455215

Epoch: 6| Step: 5
Training loss: 2.820486545562744
Validation loss: 2.5446352343405447

Epoch: 6| Step: 6
Training loss: 2.8901848793029785
Validation loss: 2.530204637076265

Epoch: 6| Step: 7
Training loss: 2.4055826663970947
Validation loss: 2.4975363592947684

Epoch: 6| Step: 8
Training loss: 2.598642587661743
Validation loss: 2.4809609946384223

Epoch: 6| Step: 9
Training loss: 3.096254348754883
Validation loss: 2.4778883405911025

Epoch: 6| Step: 10
Training loss: 2.384606122970581
Validation loss: 2.468693866524645

Epoch: 6| Step: 11
Training loss: 2.0699543952941895
Validation loss: 2.47723380596407

Epoch: 6| Step: 12
Training loss: 2.20035982131958
Validation loss: 2.482233370504072

Epoch: 6| Step: 13
Training loss: 1.8063428401947021
Validation loss: 2.49423223926175

Epoch: 56| Step: 0
Training loss: 2.6851463317871094
Validation loss: 2.4892373495204474

Epoch: 6| Step: 1
Training loss: 2.829294204711914
Validation loss: 2.487623535176759

Epoch: 6| Step: 2
Training loss: 2.495117664337158
Validation loss: 2.4836122323107976

Epoch: 6| Step: 3
Training loss: 3.1523072719573975
Validation loss: 2.4752838893603255

Epoch: 6| Step: 4
Training loss: 2.353231430053711
Validation loss: 2.472569416928035

Epoch: 6| Step: 5
Training loss: 3.073296070098877
Validation loss: 2.4667228242402435

Epoch: 6| Step: 6
Training loss: 2.718932867050171
Validation loss: 2.464500155500186

Epoch: 6| Step: 7
Training loss: 2.449770450592041
Validation loss: 2.4667515062516734

Epoch: 6| Step: 8
Training loss: 2.827404737472534
Validation loss: 2.464011048757902

Epoch: 6| Step: 9
Training loss: 3.1969916820526123
Validation loss: 2.467660839839648

Epoch: 6| Step: 10
Training loss: 2.137054920196533
Validation loss: 2.468130321912868

Epoch: 6| Step: 11
Training loss: 3.2255687713623047
Validation loss: 2.46676290932522

Epoch: 6| Step: 12
Training loss: 2.3294875621795654
Validation loss: 2.470237301241967

Epoch: 6| Step: 13
Training loss: 1.5785435438156128
Validation loss: 2.4794283989937074

Epoch: 57| Step: 0
Training loss: 3.1798720359802246
Validation loss: 2.4704740893456245

Epoch: 6| Step: 1
Training loss: 3.5158843994140625
Validation loss: 2.4644107536603044

Epoch: 6| Step: 2
Training loss: 2.1743671894073486
Validation loss: 2.4616782178160963

Epoch: 6| Step: 3
Training loss: 2.471616268157959
Validation loss: 2.4701298436810895

Epoch: 6| Step: 4
Training loss: 3.1136417388916016
Validation loss: 2.471970563293785

Epoch: 6| Step: 5
Training loss: 2.268601417541504
Validation loss: 2.4691165519017044

Epoch: 6| Step: 6
Training loss: 3.3257856369018555
Validation loss: 2.46667202185559

Epoch: 6| Step: 7
Training loss: 2.670748472213745
Validation loss: 2.4650076025275776

Epoch: 6| Step: 8
Training loss: 1.7497919797897339
Validation loss: 2.4606356543879353

Epoch: 6| Step: 9
Training loss: 2.5786681175231934
Validation loss: 2.4578583061054187

Epoch: 6| Step: 10
Training loss: 3.2060070037841797
Validation loss: 2.458762995658382

Epoch: 6| Step: 11
Training loss: 2.566218376159668
Validation loss: 2.4592012718159664

Epoch: 6| Step: 12
Training loss: 1.876853346824646
Validation loss: 2.4610886163609003

Epoch: 6| Step: 13
Training loss: 2.8890953063964844
Validation loss: 2.4602927007982807

Epoch: 58| Step: 0
Training loss: 2.767977714538574
Validation loss: 2.4646330777034966

Epoch: 6| Step: 1
Training loss: 2.1832175254821777
Validation loss: 2.4680228130791777

Epoch: 6| Step: 2
Training loss: 2.7365329265594482
Validation loss: 2.465032608278336

Epoch: 6| Step: 3
Training loss: 2.4270002841949463
Validation loss: 2.4633346014125372

Epoch: 6| Step: 4
Training loss: 1.6678032875061035
Validation loss: 2.4675143482864543

Epoch: 6| Step: 5
Training loss: 2.769411563873291
Validation loss: 2.4781600044619654

Epoch: 6| Step: 6
Training loss: 2.3900139331817627
Validation loss: 2.473552842294016

Epoch: 6| Step: 7
Training loss: 2.796945810317993
Validation loss: 2.465865445393388

Epoch: 6| Step: 8
Training loss: 3.211622476577759
Validation loss: 2.458736506841516

Epoch: 6| Step: 9
Training loss: 3.4105796813964844
Validation loss: 2.4537940538057716

Epoch: 6| Step: 10
Training loss: 3.3288090229034424
Validation loss: 2.4559982720241753

Epoch: 6| Step: 11
Training loss: 2.9907004833221436
Validation loss: 2.452892816194924

Epoch: 6| Step: 12
Training loss: 2.464372158050537
Validation loss: 2.451536706698838

Epoch: 6| Step: 13
Training loss: 2.1600334644317627
Validation loss: 2.4514061148448656

Epoch: 59| Step: 0
Training loss: 2.8669300079345703
Validation loss: 2.4501451548709663

Epoch: 6| Step: 1
Training loss: 1.734659194946289
Validation loss: 2.4514272597528275

Epoch: 6| Step: 2
Training loss: 2.465456008911133
Validation loss: 2.4468380225602018

Epoch: 6| Step: 3
Training loss: 3.2201015949249268
Validation loss: 2.44483196479018

Epoch: 6| Step: 4
Training loss: 2.6536288261413574
Validation loss: 2.4443300411265385

Epoch: 6| Step: 5
Training loss: 2.8570756912231445
Validation loss: 2.446371019527476

Epoch: 6| Step: 6
Training loss: 3.1406097412109375
Validation loss: 2.449228104724679

Epoch: 6| Step: 7
Training loss: 2.3831987380981445
Validation loss: 2.4600838204865814

Epoch: 6| Step: 8
Training loss: 2.253251552581787
Validation loss: 2.4623819576796664

Epoch: 6| Step: 9
Training loss: 2.7728633880615234
Validation loss: 2.470927876810874

Epoch: 6| Step: 10
Training loss: 2.7933616638183594
Validation loss: 2.4809725669122513

Epoch: 6| Step: 11
Training loss: 2.828456163406372
Validation loss: 2.4982662457291798

Epoch: 6| Step: 12
Training loss: 2.8641014099121094
Validation loss: 2.4676031630526305

Epoch: 6| Step: 13
Training loss: 2.5648515224456787
Validation loss: 2.4522790703722226

Epoch: 60| Step: 0
Training loss: 2.412421703338623
Validation loss: 2.44606009093664

Epoch: 6| Step: 1
Training loss: 2.367767095565796
Validation loss: 2.456093693292269

Epoch: 6| Step: 2
Training loss: 2.7318451404571533
Validation loss: 2.460567840965845

Epoch: 6| Step: 3
Training loss: 2.3160977363586426
Validation loss: 2.461288364984656

Epoch: 6| Step: 4
Training loss: 2.7974321842193604
Validation loss: 2.4944862447759157

Epoch: 6| Step: 5
Training loss: 2.8855738639831543
Validation loss: 2.5079378966362245

Epoch: 6| Step: 6
Training loss: 2.463493824005127
Validation loss: 2.508903646981844

Epoch: 6| Step: 7
Training loss: 2.5282769203186035
Validation loss: 2.509886318637479

Epoch: 6| Step: 8
Training loss: 3.1979033946990967
Validation loss: 2.4990753973684003

Epoch: 6| Step: 9
Training loss: 2.779890537261963
Validation loss: 2.458398001168364

Epoch: 6| Step: 10
Training loss: 3.1619176864624023
Validation loss: 2.451389612690095

Epoch: 6| Step: 11
Training loss: 2.6184098720550537
Validation loss: 2.4485792344616306

Epoch: 6| Step: 12
Training loss: 2.636582851409912
Validation loss: 2.444366611460204

Epoch: 6| Step: 13
Training loss: 2.5203540325164795
Validation loss: 2.444216112936697

Epoch: 61| Step: 0
Training loss: 2.8847217559814453
Validation loss: 2.44177810863782

Epoch: 6| Step: 1
Training loss: 3.1153509616851807
Validation loss: 2.447858674551851

Epoch: 6| Step: 2
Training loss: 2.962843418121338
Validation loss: 2.439724694016159

Epoch: 6| Step: 3
Training loss: 2.121985912322998
Validation loss: 2.43798251305857

Epoch: 6| Step: 4
Training loss: 2.4043314456939697
Validation loss: 2.4407458510450137

Epoch: 6| Step: 5
Training loss: 2.4304232597351074
Validation loss: 2.4442164359554166

Epoch: 6| Step: 6
Training loss: 2.549617290496826
Validation loss: 2.448951687864078

Epoch: 6| Step: 7
Training loss: 3.3180174827575684
Validation loss: 2.4483552876339165

Epoch: 6| Step: 8
Training loss: 2.2391715049743652
Validation loss: 2.4553770608799432

Epoch: 6| Step: 9
Training loss: 2.3210599422454834
Validation loss: 2.457701867626559

Epoch: 6| Step: 10
Training loss: 2.541172742843628
Validation loss: 2.4587025514212986

Epoch: 6| Step: 11
Training loss: 3.136225700378418
Validation loss: 2.4626739819844565

Epoch: 6| Step: 12
Training loss: 3.035771369934082
Validation loss: 2.477952375206896

Epoch: 6| Step: 13
Training loss: 1.634904384613037
Validation loss: 2.471029676416869

Epoch: 62| Step: 0
Training loss: 3.5943031311035156
Validation loss: 2.465959684823149

Epoch: 6| Step: 1
Training loss: 2.480433464050293
Validation loss: 2.450040755733367

Epoch: 6| Step: 2
Training loss: 2.1147642135620117
Validation loss: 2.4395903618104997

Epoch: 6| Step: 3
Training loss: 2.6377005577087402
Validation loss: 2.428845738851896

Epoch: 6| Step: 4
Training loss: 2.901883125305176
Validation loss: 2.4267622552892214

Epoch: 6| Step: 5
Training loss: 2.6477389335632324
Validation loss: 2.4272380516093266

Epoch: 6| Step: 6
Training loss: 2.568546772003174
Validation loss: 2.4289779329812653

Epoch: 6| Step: 7
Training loss: 2.1462435722351074
Validation loss: 2.4311379360896286

Epoch: 6| Step: 8
Training loss: 3.043274164199829
Validation loss: 2.4312417199534755

Epoch: 6| Step: 9
Training loss: 2.2623178958892822
Validation loss: 2.437609936601372

Epoch: 6| Step: 10
Training loss: 2.820831775665283
Validation loss: 2.440870843907838

Epoch: 6| Step: 11
Training loss: 2.908705234527588
Validation loss: 2.4367712902766403

Epoch: 6| Step: 12
Training loss: 2.9171266555786133
Validation loss: 2.431626542921989

Epoch: 6| Step: 13
Training loss: 2.205301284790039
Validation loss: 2.4248871700738066

Epoch: 63| Step: 0
Training loss: 3.229508399963379
Validation loss: 2.4255707520310597

Epoch: 6| Step: 1
Training loss: 2.5212461948394775
Validation loss: 2.427768558584234

Epoch: 6| Step: 2
Training loss: 2.890268564224243
Validation loss: 2.4305307865142822

Epoch: 6| Step: 3
Training loss: 3.045684337615967
Validation loss: 2.4248638229985393

Epoch: 6| Step: 4
Training loss: 2.093766689300537
Validation loss: 2.4302791626222673

Epoch: 6| Step: 5
Training loss: 2.051093101501465
Validation loss: 2.4262554773720364

Epoch: 6| Step: 6
Training loss: 2.9173696041107178
Validation loss: 2.4237344777712257

Epoch: 6| Step: 7
Training loss: 2.403271198272705
Validation loss: 2.4300522445350565

Epoch: 6| Step: 8
Training loss: 2.9999773502349854
Validation loss: 2.4291492995395454

Epoch: 6| Step: 9
Training loss: 2.8846230506896973
Validation loss: 2.4292291902726695

Epoch: 6| Step: 10
Training loss: 2.6063666343688965
Validation loss: 2.4263613095847507

Epoch: 6| Step: 11
Training loss: 2.3933730125427246
Validation loss: 2.4290380708632933

Epoch: 6| Step: 12
Training loss: 2.490117073059082
Validation loss: 2.444067765307683

Epoch: 6| Step: 13
Training loss: 2.558743476867676
Validation loss: 2.454552960652177

Epoch: 64| Step: 0
Training loss: 2.1075210571289062
Validation loss: 2.471766261644261

Epoch: 6| Step: 1
Training loss: 1.970300316810608
Validation loss: 2.468536082134452

Epoch: 6| Step: 2
Training loss: 2.608053684234619
Validation loss: 2.4636888350209882

Epoch: 6| Step: 3
Training loss: 2.2812652587890625
Validation loss: 2.4684990964910036

Epoch: 6| Step: 4
Training loss: 3.1193504333496094
Validation loss: 2.4802595261604554

Epoch: 6| Step: 5
Training loss: 3.136155128479004
Validation loss: 2.4774621840446227

Epoch: 6| Step: 6
Training loss: 3.0287675857543945
Validation loss: 2.477273920530914

Epoch: 6| Step: 7
Training loss: 2.5702600479125977
Validation loss: 2.467825133313415

Epoch: 6| Step: 8
Training loss: 2.028348684310913
Validation loss: 2.4547643482044177

Epoch: 6| Step: 9
Training loss: 2.8147342205047607
Validation loss: 2.439563242338037

Epoch: 6| Step: 10
Training loss: 2.176697015762329
Validation loss: 2.431193623491513

Epoch: 6| Step: 11
Training loss: 3.4398531913757324
Validation loss: 2.426418083970265

Epoch: 6| Step: 12
Training loss: 2.7353286743164062
Validation loss: 2.4224484505191928

Epoch: 6| Step: 13
Training loss: 3.537369966506958
Validation loss: 2.4169508898130028

Epoch: 65| Step: 0
Training loss: 3.086876392364502
Validation loss: 2.4083087777578704

Epoch: 6| Step: 1
Training loss: 2.28830623626709
Validation loss: 2.410215547007899

Epoch: 6| Step: 2
Training loss: 2.954489231109619
Validation loss: 2.4098091228033907

Epoch: 6| Step: 3
Training loss: 2.755436658859253
Validation loss: 2.408712415285008

Epoch: 6| Step: 4
Training loss: 3.4960055351257324
Validation loss: 2.4077422157410653

Epoch: 6| Step: 5
Training loss: 2.7915937900543213
Validation loss: 2.4087993355207544

Epoch: 6| Step: 6
Training loss: 2.5034642219543457
Validation loss: 2.4132446717190486

Epoch: 6| Step: 7
Training loss: 2.5761067867279053
Validation loss: 2.4141204100783153

Epoch: 6| Step: 8
Training loss: 2.423696517944336
Validation loss: 2.4169541610184537

Epoch: 6| Step: 9
Training loss: 2.519214153289795
Validation loss: 2.4148276518749934

Epoch: 6| Step: 10
Training loss: 2.3109264373779297
Validation loss: 2.4121367085364556

Epoch: 6| Step: 11
Training loss: 2.290701389312744
Validation loss: 2.414331792503275

Epoch: 6| Step: 12
Training loss: 2.785365104675293
Validation loss: 2.4121472656085925

Epoch: 6| Step: 13
Training loss: 2.0776255130767822
Validation loss: 2.416112005069692

Epoch: 66| Step: 0
Training loss: 2.4087073802948
Validation loss: 2.415575635048651

Epoch: 6| Step: 1
Training loss: 3.1890413761138916
Validation loss: 2.4179114731409217

Epoch: 6| Step: 2
Training loss: 2.220442533493042
Validation loss: 2.426294831819432

Epoch: 6| Step: 3
Training loss: 2.6854329109191895
Validation loss: 2.4353151218865507

Epoch: 6| Step: 4
Training loss: 2.6781957149505615
Validation loss: 2.4350769314714658

Epoch: 6| Step: 5
Training loss: 2.7320477962493896
Validation loss: 2.4292305925840973

Epoch: 6| Step: 6
Training loss: 3.012474536895752
Validation loss: 2.423416704259893

Epoch: 6| Step: 7
Training loss: 2.107098340988159
Validation loss: 2.4135231497467204

Epoch: 6| Step: 8
Training loss: 3.5337753295898438
Validation loss: 2.406560738881429

Epoch: 6| Step: 9
Training loss: 1.9734139442443848
Validation loss: 2.403591796916018

Epoch: 6| Step: 10
Training loss: 2.5167970657348633
Validation loss: 2.4026252556872625

Epoch: 6| Step: 11
Training loss: 2.6303977966308594
Validation loss: 2.411108975769371

Epoch: 6| Step: 12
Training loss: 3.03688907623291
Validation loss: 2.422287820487894

Epoch: 6| Step: 13
Training loss: 2.208123207092285
Validation loss: 2.416311945966495

Epoch: 67| Step: 0
Training loss: 2.1594393253326416
Validation loss: 2.416048942073699

Epoch: 6| Step: 1
Training loss: 2.7084219455718994
Validation loss: 2.4081544722280195

Epoch: 6| Step: 2
Training loss: 2.087373971939087
Validation loss: 2.4053282430095058

Epoch: 6| Step: 3
Training loss: 2.3034520149230957
Validation loss: 2.4065763053073677

Epoch: 6| Step: 4
Training loss: 2.9944255352020264
Validation loss: 2.410035425616849

Epoch: 6| Step: 5
Training loss: 3.5125818252563477
Validation loss: 2.41863686807694

Epoch: 6| Step: 6
Training loss: 2.1825013160705566
Validation loss: 2.4314443142183366

Epoch: 6| Step: 7
Training loss: 2.4850058555603027
Validation loss: 2.4507632204281387

Epoch: 6| Step: 8
Training loss: 2.925013542175293
Validation loss: 2.440830453749626

Epoch: 6| Step: 9
Training loss: 2.5260767936706543
Validation loss: 2.4233435353925152

Epoch: 6| Step: 10
Training loss: 3.0045337677001953
Validation loss: 2.4029200641057824

Epoch: 6| Step: 11
Training loss: 2.9448089599609375
Validation loss: 2.401317395189757

Epoch: 6| Step: 12
Training loss: 2.6958839893341064
Validation loss: 2.4038786913758967

Epoch: 6| Step: 13
Training loss: 2.464048385620117
Validation loss: 2.411366860071818

Epoch: 68| Step: 0
Training loss: 2.5656113624572754
Validation loss: 2.416204635814954

Epoch: 6| Step: 1
Training loss: 2.273196220397949
Validation loss: 2.4227950752422376

Epoch: 6| Step: 2
Training loss: 3.380768299102783
Validation loss: 2.424744826491161

Epoch: 6| Step: 3
Training loss: 2.9895882606506348
Validation loss: 2.422462735124814

Epoch: 6| Step: 4
Training loss: 2.9496939182281494
Validation loss: 2.4241782490925123

Epoch: 6| Step: 5
Training loss: 2.8599894046783447
Validation loss: 2.4175856215979463

Epoch: 6| Step: 6
Training loss: 1.6260957717895508
Validation loss: 2.41567156135395

Epoch: 6| Step: 7
Training loss: 2.269221067428589
Validation loss: 2.4112941962416454

Epoch: 6| Step: 8
Training loss: 2.082883358001709
Validation loss: 2.405293938934162

Epoch: 6| Step: 9
Training loss: 2.3779821395874023
Validation loss: 2.4050748117508425

Epoch: 6| Step: 10
Training loss: 2.136262893676758
Validation loss: 2.400568357077978

Epoch: 6| Step: 11
Training loss: 3.7178244590759277
Validation loss: 2.4026656458454747

Epoch: 6| Step: 12
Training loss: 3.219139575958252
Validation loss: 2.4055741730556695

Epoch: 6| Step: 13
Training loss: 3.0157723426818848
Validation loss: 2.4048423331270934

Epoch: 69| Step: 0
Training loss: 2.164210319519043
Validation loss: 2.410253260725288

Epoch: 6| Step: 1
Training loss: 3.2064716815948486
Validation loss: 2.4278136940412622

Epoch: 6| Step: 2
Training loss: 2.31276535987854
Validation loss: 2.442009208022907

Epoch: 6| Step: 3
Training loss: 2.5898797512054443
Validation loss: 2.4382509698149977

Epoch: 6| Step: 4
Training loss: 2.693756103515625
Validation loss: 2.4496471933139268

Epoch: 6| Step: 5
Training loss: 2.015775203704834
Validation loss: 2.468291122426269

Epoch: 6| Step: 6
Training loss: 2.9143104553222656
Validation loss: 2.4693579250766384

Epoch: 6| Step: 7
Training loss: 2.487326145172119
Validation loss: 2.4704342055064377

Epoch: 6| Step: 8
Training loss: 2.5827062129974365
Validation loss: 2.4364075045431814

Epoch: 6| Step: 9
Training loss: 3.6955628395080566
Validation loss: 2.4173120093602005

Epoch: 6| Step: 10
Training loss: 3.33096981048584
Validation loss: 2.4033274189118417

Epoch: 6| Step: 11
Training loss: 1.9466235637664795
Validation loss: 2.3948330469028924

Epoch: 6| Step: 12
Training loss: 2.570676803588867
Validation loss: 2.3931535367042787

Epoch: 6| Step: 13
Training loss: 2.487889051437378
Validation loss: 2.3958290571807535

Epoch: 70| Step: 0
Training loss: 2.5487890243530273
Validation loss: 2.39217447978194

Epoch: 6| Step: 1
Training loss: 1.8839797973632812
Validation loss: 2.3886263396150325

Epoch: 6| Step: 2
Training loss: 2.802602767944336
Validation loss: 2.385762578697615

Epoch: 6| Step: 3
Training loss: 2.2974801063537598
Validation loss: 2.389809816114364

Epoch: 6| Step: 4
Training loss: 2.342679977416992
Validation loss: 2.391084204437912

Epoch: 6| Step: 5
Training loss: 3.111478567123413
Validation loss: 2.3874505719830914

Epoch: 6| Step: 6
Training loss: 3.407097816467285
Validation loss: 2.388933571436072

Epoch: 6| Step: 7
Training loss: 2.064941883087158
Validation loss: 2.3868923366710706

Epoch: 6| Step: 8
Training loss: 2.335383892059326
Validation loss: 2.387312742971605

Epoch: 6| Step: 9
Training loss: 3.045931339263916
Validation loss: 2.378594765099146

Epoch: 6| Step: 10
Training loss: 2.7776007652282715
Validation loss: 2.3844419781879713

Epoch: 6| Step: 11
Training loss: 3.1814780235290527
Validation loss: 2.3834278122071297

Epoch: 6| Step: 12
Training loss: 2.3973772525787354
Validation loss: 2.383501887321472

Epoch: 6| Step: 13
Training loss: 2.840111017227173
Validation loss: 2.3829887938755814

Epoch: 71| Step: 0
Training loss: 2.7074365615844727
Validation loss: 2.382257999912385

Epoch: 6| Step: 1
Training loss: 3.236769676208496
Validation loss: 2.378565788269043

Epoch: 6| Step: 2
Training loss: 2.9295496940612793
Validation loss: 2.3796986969568397

Epoch: 6| Step: 3
Training loss: 1.7163431644439697
Validation loss: 2.3874819688899542

Epoch: 6| Step: 4
Training loss: 2.53092885017395
Validation loss: 2.3799561044221282

Epoch: 6| Step: 5
Training loss: 2.8869636058807373
Validation loss: 2.3791825873877412

Epoch: 6| Step: 6
Training loss: 2.411710262298584
Validation loss: 2.376376836530624

Epoch: 6| Step: 7
Training loss: 2.5272650718688965
Validation loss: 2.3772735672612346

Epoch: 6| Step: 8
Training loss: 2.799654483795166
Validation loss: 2.3971606121268323

Epoch: 6| Step: 9
Training loss: 3.1488747596740723
Validation loss: 2.412792677520424

Epoch: 6| Step: 10
Training loss: 1.802356481552124
Validation loss: 2.427935000388853

Epoch: 6| Step: 11
Training loss: 2.4625563621520996
Validation loss: 2.443177346260317

Epoch: 6| Step: 12
Training loss: 2.467923164367676
Validation loss: 2.434885440334197

Epoch: 6| Step: 13
Training loss: 3.5402004718780518
Validation loss: 2.4306549897757908

Epoch: 72| Step: 0
Training loss: 1.6915175914764404
Validation loss: 2.425294558207194

Epoch: 6| Step: 1
Training loss: 3.2043190002441406
Validation loss: 2.4160829795304166

Epoch: 6| Step: 2
Training loss: 3.486886978149414
Validation loss: 2.3913118403445006

Epoch: 6| Step: 3
Training loss: 2.1240954399108887
Validation loss: 2.3852585464395504

Epoch: 6| Step: 4
Training loss: 2.6060361862182617
Validation loss: 2.3689243896033174

Epoch: 6| Step: 5
Training loss: 2.2047834396362305
Validation loss: 2.3655772106621855

Epoch: 6| Step: 6
Training loss: 2.2568695545196533
Validation loss: 2.3649456321552234

Epoch: 6| Step: 7
Training loss: 2.785515785217285
Validation loss: 2.3671962189418014

Epoch: 6| Step: 8
Training loss: 2.8729939460754395
Validation loss: 2.368181218383133

Epoch: 6| Step: 9
Training loss: 2.4801979064941406
Validation loss: 2.363493163098571

Epoch: 6| Step: 10
Training loss: 2.6628761291503906
Validation loss: 2.372758685901601

Epoch: 6| Step: 11
Training loss: 2.7243340015411377
Validation loss: 2.3748274413488244

Epoch: 6| Step: 12
Training loss: 3.3237361907958984
Validation loss: 2.3781277902664675

Epoch: 6| Step: 13
Training loss: 1.7606697082519531
Validation loss: 2.3758132175732682

Epoch: 73| Step: 0
Training loss: 3.518690586090088
Validation loss: 2.376799278361823

Epoch: 6| Step: 1
Training loss: 3.0054774284362793
Validation loss: 2.3798220695987826

Epoch: 6| Step: 2
Training loss: 3.0487008094787598
Validation loss: 2.381871102958597

Epoch: 6| Step: 3
Training loss: 1.6347957849502563
Validation loss: 2.382807395791495

Epoch: 6| Step: 4
Training loss: 2.5890417098999023
Validation loss: 2.3879574139912925

Epoch: 6| Step: 5
Training loss: 2.6250216960906982
Validation loss: 2.4072635917253393

Epoch: 6| Step: 6
Training loss: 1.9676165580749512
Validation loss: 2.4140968476572344

Epoch: 6| Step: 7
Training loss: 2.7779572010040283
Validation loss: 2.4007467557025213

Epoch: 6| Step: 8
Training loss: 2.0218117237091064
Validation loss: 2.4080911938862135

Epoch: 6| Step: 9
Training loss: 2.4804940223693848
Validation loss: 2.3943002864878666

Epoch: 6| Step: 10
Training loss: 2.2126259803771973
Validation loss: 2.398336433595227

Epoch: 6| Step: 11
Training loss: 3.16048526763916
Validation loss: 2.3885508609074417

Epoch: 6| Step: 12
Training loss: 2.7050960063934326
Validation loss: 2.384839048949621

Epoch: 6| Step: 13
Training loss: 3.063584089279175
Validation loss: 2.3749025278193976

Epoch: 74| Step: 0
Training loss: 2.663877010345459
Validation loss: 2.360161183982767

Epoch: 6| Step: 1
Training loss: 2.9017162322998047
Validation loss: 2.3633633608459146

Epoch: 6| Step: 2
Training loss: 1.7666034698486328
Validation loss: 2.357841281480687

Epoch: 6| Step: 3
Training loss: 2.4310030937194824
Validation loss: 2.3566482067108154

Epoch: 6| Step: 4
Training loss: 2.795473098754883
Validation loss: 2.356088069177443

Epoch: 6| Step: 5
Training loss: 2.9519271850585938
Validation loss: 2.358302116394043

Epoch: 6| Step: 6
Training loss: 2.7790207862854004
Validation loss: 2.363174764058923

Epoch: 6| Step: 7
Training loss: 3.196626663208008
Validation loss: 2.3675833158595587

Epoch: 6| Step: 8
Training loss: 3.260194778442383
Validation loss: 2.374343666979062

Epoch: 6| Step: 9
Training loss: 2.369475841522217
Validation loss: 2.3767723934624785

Epoch: 6| Step: 10
Training loss: 2.166090488433838
Validation loss: 2.3850778431020756

Epoch: 6| Step: 11
Training loss: 2.1183369159698486
Validation loss: 2.382083341639529

Epoch: 6| Step: 12
Training loss: 2.9490551948547363
Validation loss: 2.3706705800948606

Epoch: 6| Step: 13
Training loss: 2.08478045463562
Validation loss: 2.3725118944721837

Epoch: 75| Step: 0
Training loss: 2.1280932426452637
Validation loss: 2.3743975598325013

Epoch: 6| Step: 1
Training loss: 2.920161008834839
Validation loss: 2.389804950324438

Epoch: 6| Step: 2
Training loss: 2.6788792610168457
Validation loss: 2.3859533879064743

Epoch: 6| Step: 3
Training loss: 2.7751858234405518
Validation loss: 2.4004116596714145

Epoch: 6| Step: 4
Training loss: 2.546478748321533
Validation loss: 2.390977382659912

Epoch: 6| Step: 5
Training loss: 2.7488532066345215
Validation loss: 2.3769168623032106

Epoch: 6| Step: 6
Training loss: 2.420823812484741
Validation loss: 2.366154448960417

Epoch: 6| Step: 7
Training loss: 2.695396661758423
Validation loss: 2.3638315892988637

Epoch: 6| Step: 8
Training loss: 2.090707778930664
Validation loss: 2.3596158258376585

Epoch: 6| Step: 9
Training loss: 2.255976915359497
Validation loss: 2.3631761561157885

Epoch: 6| Step: 10
Training loss: 3.307363986968994
Validation loss: 2.3621715730236423

Epoch: 6| Step: 11
Training loss: 2.551241159439087
Validation loss: 2.367809012372007

Epoch: 6| Step: 12
Training loss: 2.8764004707336426
Validation loss: 2.370030221118722

Epoch: 6| Step: 13
Training loss: 2.5072686672210693
Validation loss: 2.3704915200510333

Epoch: 76| Step: 0
Training loss: 2.472872734069824
Validation loss: 2.369175903258785

Epoch: 6| Step: 1
Training loss: 2.3901970386505127
Validation loss: 2.3623921102093113

Epoch: 6| Step: 2
Training loss: 3.3941893577575684
Validation loss: 2.3640771014716035

Epoch: 6| Step: 3
Training loss: 2.3323845863342285
Validation loss: 2.3651066108416487

Epoch: 6| Step: 4
Training loss: 2.5945754051208496
Validation loss: 2.3599948139600855

Epoch: 6| Step: 5
Training loss: 2.579253673553467
Validation loss: 2.3530741814644105

Epoch: 6| Step: 6
Training loss: 2.180363655090332
Validation loss: 2.3611433506011963

Epoch: 6| Step: 7
Training loss: 2.524693012237549
Validation loss: 2.356417989218107

Epoch: 6| Step: 8
Training loss: 2.2236170768737793
Validation loss: 2.3649582093761814

Epoch: 6| Step: 9
Training loss: 2.733970880508423
Validation loss: 2.3629743258158364

Epoch: 6| Step: 10
Training loss: 2.4627325534820557
Validation loss: 2.3854532831458637

Epoch: 6| Step: 11
Training loss: 3.2760486602783203
Validation loss: 2.412497869101904

Epoch: 6| Step: 12
Training loss: 2.5211710929870605
Validation loss: 2.4139897336242018

Epoch: 6| Step: 13
Training loss: 2.9094245433807373
Validation loss: 2.4319218768868396

Epoch: 77| Step: 0
Training loss: 2.816232919692993
Validation loss: 2.4907840118613294

Epoch: 6| Step: 1
Training loss: 2.470546007156372
Validation loss: 2.5729795373896116

Epoch: 6| Step: 2
Training loss: 1.8490979671478271
Validation loss: 2.647815114708357

Epoch: 6| Step: 3
Training loss: 3.974311113357544
Validation loss: 2.6899455157659387

Epoch: 6| Step: 4
Training loss: 2.9124326705932617
Validation loss: 2.626840724739977

Epoch: 6| Step: 5
Training loss: 2.3864030838012695
Validation loss: 2.542862005131219

Epoch: 6| Step: 6
Training loss: 3.0095510482788086
Validation loss: 2.4273716352319203

Epoch: 6| Step: 7
Training loss: 2.6781115531921387
Validation loss: 2.3792979178890103

Epoch: 6| Step: 8
Training loss: 1.9022634029388428
Validation loss: 2.3648868645391157

Epoch: 6| Step: 9
Training loss: 3.122195243835449
Validation loss: 2.361025397495557

Epoch: 6| Step: 10
Training loss: 2.8092610836029053
Validation loss: 2.371796718207739

Epoch: 6| Step: 11
Training loss: 1.9965996742248535
Validation loss: 2.375486327755836

Epoch: 6| Step: 12
Training loss: 3.1182260513305664
Validation loss: 2.371845668362033

Epoch: 6| Step: 13
Training loss: 2.298405170440674
Validation loss: 2.3795187832206808

Epoch: 78| Step: 0
Training loss: 2.7758960723876953
Validation loss: 2.3717749246986966

Epoch: 6| Step: 1
Training loss: 2.5531044006347656
Validation loss: 2.3583038365969093

Epoch: 6| Step: 2
Training loss: 2.7864513397216797
Validation loss: 2.354890020944739

Epoch: 6| Step: 3
Training loss: 2.773953914642334
Validation loss: 2.3560550597406205

Epoch: 6| Step: 4
Training loss: 2.8734915256500244
Validation loss: 2.347629616337438

Epoch: 6| Step: 5
Training loss: 2.509866714477539
Validation loss: 2.3486077554764284

Epoch: 6| Step: 6
Training loss: 2.6615939140319824
Validation loss: 2.344877968552292

Epoch: 6| Step: 7
Training loss: 2.9562361240386963
Validation loss: 2.344424324650918

Epoch: 6| Step: 8
Training loss: 2.5985653400421143
Validation loss: 2.3426177168405182

Epoch: 6| Step: 9
Training loss: 2.6759285926818848
Validation loss: 2.350992556541197

Epoch: 6| Step: 10
Training loss: 2.150787591934204
Validation loss: 2.352801174245855

Epoch: 6| Step: 11
Training loss: 2.520066022872925
Validation loss: 2.3511962403533277

Epoch: 6| Step: 12
Training loss: 2.139085292816162
Validation loss: 2.3512758298586776

Epoch: 6| Step: 13
Training loss: 2.496826648712158
Validation loss: 2.3591340357257473

Epoch: 79| Step: 0
Training loss: 2.4027419090270996
Validation loss: 2.354055143171741

Epoch: 6| Step: 1
Training loss: 2.3954286575317383
Validation loss: 2.3532584892806185

Epoch: 6| Step: 2
Training loss: 2.2652041912078857
Validation loss: 2.344294665962137

Epoch: 6| Step: 3
Training loss: 3.032771587371826
Validation loss: 2.341051132448258

Epoch: 6| Step: 4
Training loss: 3.009868621826172
Validation loss: 2.339893315428047

Epoch: 6| Step: 5
Training loss: 2.6185731887817383
Validation loss: 2.3411667193135908

Epoch: 6| Step: 6
Training loss: 2.511073350906372
Validation loss: 2.3401543709539596

Epoch: 6| Step: 7
Training loss: 2.1089963912963867
Validation loss: 2.34090284891026

Epoch: 6| Step: 8
Training loss: 3.05851411819458
Validation loss: 2.3458151766048965

Epoch: 6| Step: 9
Training loss: 2.4105472564697266
Validation loss: 2.3454603572045603

Epoch: 6| Step: 10
Training loss: 2.2934796810150146
Validation loss: 2.3407588543430453

Epoch: 6| Step: 11
Training loss: 3.5929887294769287
Validation loss: 2.3502478291911464

Epoch: 6| Step: 12
Training loss: 2.5067226886749268
Validation loss: 2.3589038464330856

Epoch: 6| Step: 13
Training loss: 1.8034733533859253
Validation loss: 2.3696068102313625

Epoch: 80| Step: 0
Training loss: 2.4212279319763184
Validation loss: 2.414706458327591

Epoch: 6| Step: 1
Training loss: 2.962569236755371
Validation loss: 2.441818824378393

Epoch: 6| Step: 2
Training loss: 2.301098346710205
Validation loss: 2.4640975049746934

Epoch: 6| Step: 3
Training loss: 2.1744303703308105
Validation loss: 2.4891921576633247

Epoch: 6| Step: 4
Training loss: 2.9723029136657715
Validation loss: 2.454052404690814

Epoch: 6| Step: 5
Training loss: 2.5611653327941895
Validation loss: 2.407120702087238

Epoch: 6| Step: 6
Training loss: 2.3866963386535645
Validation loss: 2.3636258186832553

Epoch: 6| Step: 7
Training loss: 2.473759651184082
Validation loss: 2.3604952750667447

Epoch: 6| Step: 8
Training loss: 2.2417874336242676
Validation loss: 2.392571385188769

Epoch: 6| Step: 9
Training loss: 2.0893874168395996
Validation loss: 2.400056577497913

Epoch: 6| Step: 10
Training loss: 3.484795570373535
Validation loss: 2.4243215771131617

Epoch: 6| Step: 11
Training loss: 3.2385847568511963
Validation loss: 2.43002752847569

Epoch: 6| Step: 12
Training loss: 2.8806934356689453
Validation loss: 2.437137019249701

Epoch: 6| Step: 13
Training loss: 3.073718547821045
Validation loss: 2.4042297409426783

Epoch: 81| Step: 0
Training loss: 2.813585042953491
Validation loss: 2.3465947053765737

Epoch: 6| Step: 1
Training loss: 2.8508591651916504
Validation loss: 2.339324184643325

Epoch: 6| Step: 2
Training loss: 2.756979465484619
Validation loss: 2.331677193282753

Epoch: 6| Step: 3
Training loss: 2.3389511108398438
Validation loss: 2.337271644223121

Epoch: 6| Step: 4
Training loss: 1.9442260265350342
Validation loss: 2.352860276417066

Epoch: 6| Step: 5
Training loss: 2.7274131774902344
Validation loss: 2.3637934141261603

Epoch: 6| Step: 6
Training loss: 2.4333293437957764
Validation loss: 2.385082342291391

Epoch: 6| Step: 7
Training loss: 3.0517075061798096
Validation loss: 2.4013514723829044

Epoch: 6| Step: 8
Training loss: 2.5160934925079346
Validation loss: 2.4147369348874657

Epoch: 6| Step: 9
Training loss: 2.3856396675109863
Validation loss: 2.3972839052959154

Epoch: 6| Step: 10
Training loss: 2.8330318927764893
Validation loss: 2.3877359346676896

Epoch: 6| Step: 11
Training loss: 2.6511120796203613
Validation loss: 2.362244972618677

Epoch: 6| Step: 12
Training loss: 2.767594814300537
Validation loss: 2.350478564539263

Epoch: 6| Step: 13
Training loss: 2.7290706634521484
Validation loss: 2.342957636361481

Epoch: 82| Step: 0
Training loss: 2.828665018081665
Validation loss: 2.3371931814378306

Epoch: 6| Step: 1
Training loss: 3.1239542961120605
Validation loss: 2.3357004042594665

Epoch: 6| Step: 2
Training loss: 2.630760669708252
Validation loss: 2.334436306389429

Epoch: 6| Step: 3
Training loss: 2.994339942932129
Validation loss: 2.330830622744817

Epoch: 6| Step: 4
Training loss: 1.960315465927124
Validation loss: 2.3245370670031478

Epoch: 6| Step: 5
Training loss: 2.0728392601013184
Validation loss: 2.3240307531049176

Epoch: 6| Step: 6
Training loss: 2.676748037338257
Validation loss: 2.331387476254535

Epoch: 6| Step: 7
Training loss: 2.60882568359375
Validation loss: 2.3274186554775445

Epoch: 6| Step: 8
Training loss: 2.789830207824707
Validation loss: 2.3376257752859466

Epoch: 6| Step: 9
Training loss: 3.3914527893066406
Validation loss: 2.330104443334764

Epoch: 6| Step: 10
Training loss: 2.4819962978363037
Validation loss: 2.3335101655734483

Epoch: 6| Step: 11
Training loss: 2.7530417442321777
Validation loss: 2.3275037427102365

Epoch: 6| Step: 12
Training loss: 1.488526463508606
Validation loss: 2.328132414048718

Epoch: 6| Step: 13
Training loss: 2.4104490280151367
Validation loss: 2.3359970072264313

Epoch: 83| Step: 0
Training loss: 3.103248357772827
Validation loss: 2.3353310195348596

Epoch: 6| Step: 1
Training loss: 2.078416585922241
Validation loss: 2.331918961258345

Epoch: 6| Step: 2
Training loss: 2.6707823276519775
Validation loss: 2.3357960280551704

Epoch: 6| Step: 3
Training loss: 2.3345189094543457
Validation loss: 2.335852425585511

Epoch: 6| Step: 4
Training loss: 2.2153449058532715
Validation loss: 2.338913327904158

Epoch: 6| Step: 5
Training loss: 3.326570510864258
Validation loss: 2.348728568323197

Epoch: 6| Step: 6
Training loss: 2.8618881702423096
Validation loss: 2.3709895444172684

Epoch: 6| Step: 7
Training loss: 2.85850191116333
Validation loss: 2.401168279750373

Epoch: 6| Step: 8
Training loss: 2.3724026679992676
Validation loss: 2.394026193567502

Epoch: 6| Step: 9
Training loss: 1.9520671367645264
Validation loss: 2.38418246341008

Epoch: 6| Step: 10
Training loss: 3.1152262687683105
Validation loss: 2.3687004222664783

Epoch: 6| Step: 11
Training loss: 2.8402504920959473
Validation loss: 2.3350009226029917

Epoch: 6| Step: 12
Training loss: 2.5054807662963867
Validation loss: 2.323767844066825

Epoch: 6| Step: 13
Training loss: 1.9202401638031006
Validation loss: 2.3150798736080045

Epoch: 84| Step: 0
Training loss: 1.8872443437576294
Validation loss: 2.320277742160264

Epoch: 6| Step: 1
Training loss: 2.2512290477752686
Validation loss: 2.322759520622992

Epoch: 6| Step: 2
Training loss: 2.758746862411499
Validation loss: 2.32227845345774

Epoch: 6| Step: 3
Training loss: 2.873631715774536
Validation loss: 2.3248017398259972

Epoch: 6| Step: 4
Training loss: 2.405231475830078
Validation loss: 2.320913848056588

Epoch: 6| Step: 5
Training loss: 2.961829662322998
Validation loss: 2.3204881068198913

Epoch: 6| Step: 6
Training loss: 2.2574682235717773
Validation loss: 2.31951041119073

Epoch: 6| Step: 7
Training loss: 3.3059263229370117
Validation loss: 2.314911570600284

Epoch: 6| Step: 8
Training loss: 2.5584793090820312
Validation loss: 2.3216636103968464

Epoch: 6| Step: 9
Training loss: 2.5507259368896484
Validation loss: 2.3259939352671304

Epoch: 6| Step: 10
Training loss: 2.9022107124328613
Validation loss: 2.335734885226014

Epoch: 6| Step: 11
Training loss: 2.5907936096191406
Validation loss: 2.3318881655252106

Epoch: 6| Step: 12
Training loss: 2.3518147468566895
Validation loss: 2.3385558333448184

Epoch: 6| Step: 13
Training loss: 2.8419768810272217
Validation loss: 2.3417586177907963

Epoch: 85| Step: 0
Training loss: 2.0785977840423584
Validation loss: 2.339920941219535

Epoch: 6| Step: 1
Training loss: 1.9287559986114502
Validation loss: 2.336457831885225

Epoch: 6| Step: 2
Training loss: 2.1466588973999023
Validation loss: 2.3342016512347805

Epoch: 6| Step: 3
Training loss: 2.56353759765625
Validation loss: 2.3341588204906834

Epoch: 6| Step: 4
Training loss: 2.920029401779175
Validation loss: 2.3306526522482596

Epoch: 6| Step: 5
Training loss: 2.475170612335205
Validation loss: 2.3267246113028577

Epoch: 6| Step: 6
Training loss: 2.6630327701568604
Validation loss: 2.322286513543898

Epoch: 6| Step: 7
Training loss: 2.985293388366699
Validation loss: 2.317527274931631

Epoch: 6| Step: 8
Training loss: 2.9580211639404297
Validation loss: 2.319877475820562

Epoch: 6| Step: 9
Training loss: 2.537771701812744
Validation loss: 2.3188591746873755

Epoch: 6| Step: 10
Training loss: 2.3349032402038574
Validation loss: 2.315396252498832

Epoch: 6| Step: 11
Training loss: 3.2313694953918457
Validation loss: 2.318761792234195

Epoch: 6| Step: 12
Training loss: 2.3594038486480713
Validation loss: 2.3208199649728756

Epoch: 6| Step: 13
Training loss: 3.4747776985168457
Validation loss: 2.3182363920314337

Epoch: 86| Step: 0
Training loss: 2.61483097076416
Validation loss: 2.3192701519176526

Epoch: 6| Step: 1
Training loss: 2.2232260704040527
Validation loss: 2.3159164997839157

Epoch: 6| Step: 2
Training loss: 2.8179590702056885
Validation loss: 2.32609885995106

Epoch: 6| Step: 3
Training loss: 1.9819965362548828
Validation loss: 2.323276263411327

Epoch: 6| Step: 4
Training loss: 3.1197307109832764
Validation loss: 2.316285223089239

Epoch: 6| Step: 5
Training loss: 2.9890949726104736
Validation loss: 2.3296330000764582

Epoch: 6| Step: 6
Training loss: 2.540184736251831
Validation loss: 2.324406111112205

Epoch: 6| Step: 7
Training loss: 3.137852430343628
Validation loss: 2.3231978313897246

Epoch: 6| Step: 8
Training loss: 2.252415657043457
Validation loss: 2.320770926372979

Epoch: 6| Step: 9
Training loss: 2.3536806106567383
Validation loss: 2.324627261007986

Epoch: 6| Step: 10
Training loss: 1.8200770616531372
Validation loss: 2.322928205613167

Epoch: 6| Step: 11
Training loss: 2.5417208671569824
Validation loss: 2.3243633816319127

Epoch: 6| Step: 12
Training loss: 2.808359146118164
Validation loss: 2.3235556079495336

Epoch: 6| Step: 13
Training loss: 3.2206382751464844
Validation loss: 2.325933938385338

Epoch: 87| Step: 0
Training loss: 2.651726245880127
Validation loss: 2.333171729118593

Epoch: 6| Step: 1
Training loss: 2.2966558933258057
Validation loss: 2.332964551064276

Epoch: 6| Step: 2
Training loss: 2.4549527168273926
Validation loss: 2.3279068213637157

Epoch: 6| Step: 3
Training loss: 2.7430267333984375
Validation loss: 2.324330986187022

Epoch: 6| Step: 4
Training loss: 3.072174072265625
Validation loss: 2.326912474888627

Epoch: 6| Step: 5
Training loss: 2.7087960243225098
Validation loss: 2.3314824796492055

Epoch: 6| Step: 6
Training loss: 2.5670528411865234
Validation loss: 2.3352490625073834

Epoch: 6| Step: 7
Training loss: 2.6659185886383057
Validation loss: 2.3292084509326565

Epoch: 6| Step: 8
Training loss: 2.3063979148864746
Validation loss: 2.3179007986540436

Epoch: 6| Step: 9
Training loss: 2.3704123497009277
Validation loss: 2.310026438005509

Epoch: 6| Step: 10
Training loss: 2.2136244773864746
Validation loss: 2.3064469240045034

Epoch: 6| Step: 11
Training loss: 2.3649117946624756
Validation loss: 2.306474002458716

Epoch: 6| Step: 12
Training loss: 2.41194224357605
Validation loss: 2.3067353335759972

Epoch: 6| Step: 13
Training loss: 3.640852928161621
Validation loss: 2.30663618733806

Epoch: 88| Step: 0
Training loss: 2.7713472843170166
Validation loss: 2.313264218709802

Epoch: 6| Step: 1
Training loss: 3.080676555633545
Validation loss: 2.3148383081600232

Epoch: 6| Step: 2
Training loss: 2.709878444671631
Validation loss: 2.3236036685205277

Epoch: 6| Step: 3
Training loss: 1.7312794923782349
Validation loss: 2.3262826806755474

Epoch: 6| Step: 4
Training loss: 2.320314645767212
Validation loss: 2.357210925830308

Epoch: 6| Step: 5
Training loss: 2.6177845001220703
Validation loss: 2.3755551307432112

Epoch: 6| Step: 6
Training loss: 2.523557186126709
Validation loss: 2.379345973332723

Epoch: 6| Step: 7
Training loss: 2.6314573287963867
Validation loss: 2.356918365724625

Epoch: 6| Step: 8
Training loss: 3.021296501159668
Validation loss: 2.3291816608880156

Epoch: 6| Step: 9
Training loss: 2.5144176483154297
Validation loss: 2.30887032324268

Epoch: 6| Step: 10
Training loss: 2.442535400390625
Validation loss: 2.301537557314801

Epoch: 6| Step: 11
Training loss: 2.7985854148864746
Validation loss: 2.300081450452087

Epoch: 6| Step: 12
Training loss: 2.3407044410705566
Validation loss: 2.2984929841051818

Epoch: 6| Step: 13
Training loss: 2.9067327976226807
Validation loss: 2.295694984415526

Epoch: 89| Step: 0
Training loss: 2.0602807998657227
Validation loss: 2.2978110697961625

Epoch: 6| Step: 1
Training loss: 2.4531302452087402
Validation loss: 2.296870605919951

Epoch: 6| Step: 2
Training loss: 2.1352877616882324
Validation loss: 2.297872163916147

Epoch: 6| Step: 3
Training loss: 2.0920119285583496
Validation loss: 2.297119476461923

Epoch: 6| Step: 4
Training loss: 2.4269824028015137
Validation loss: 2.2972479610032934

Epoch: 6| Step: 5
Training loss: 2.901142120361328
Validation loss: 2.2964488537080827

Epoch: 6| Step: 6
Training loss: 2.3172502517700195
Validation loss: 2.295120443067243

Epoch: 6| Step: 7
Training loss: 2.5662989616394043
Validation loss: 2.2958519189588484

Epoch: 6| Step: 8
Training loss: 2.482858657836914
Validation loss: 2.3004367454077608

Epoch: 6| Step: 9
Training loss: 3.0606589317321777
Validation loss: 2.3012821161618797

Epoch: 6| Step: 10
Training loss: 2.36305570602417
Validation loss: 2.3009934861172914

Epoch: 6| Step: 11
Training loss: 3.6109201908111572
Validation loss: 2.308705004312659

Epoch: 6| Step: 12
Training loss: 2.8352603912353516
Validation loss: 2.3244795901800996

Epoch: 6| Step: 13
Training loss: 2.923447847366333
Validation loss: 2.342059794292655

Epoch: 90| Step: 0
Training loss: 2.864713191986084
Validation loss: 2.3687173551128757

Epoch: 6| Step: 1
Training loss: 2.89424991607666
Validation loss: 2.383703052356679

Epoch: 6| Step: 2
Training loss: 2.6850171089172363
Validation loss: 2.377463745814498

Epoch: 6| Step: 3
Training loss: 2.345928192138672
Validation loss: 2.3536265870576263

Epoch: 6| Step: 4
Training loss: 2.249845027923584
Validation loss: 2.3607290944745465

Epoch: 6| Step: 5
Training loss: 2.073774814605713
Validation loss: 2.3438875829019854

Epoch: 6| Step: 6
Training loss: 3.013714551925659
Validation loss: 2.335880437204915

Epoch: 6| Step: 7
Training loss: 3.0889651775360107
Validation loss: 2.321264067003804

Epoch: 6| Step: 8
Training loss: 2.6788582801818848
Validation loss: 2.302199996927733

Epoch: 6| Step: 9
Training loss: 2.4723329544067383
Validation loss: 2.3011732998714653

Epoch: 6| Step: 10
Training loss: 1.84108304977417
Validation loss: 2.295649224712003

Epoch: 6| Step: 11
Training loss: 2.8150782585144043
Validation loss: 2.2965828577677407

Epoch: 6| Step: 12
Training loss: 2.8127453327178955
Validation loss: 2.292984554844518

Epoch: 6| Step: 13
Training loss: 2.3014628887176514
Validation loss: 2.29077471199856

Epoch: 91| Step: 0
Training loss: 2.6388509273529053
Validation loss: 2.2986131201508226

Epoch: 6| Step: 1
Training loss: 2.2921624183654785
Validation loss: 2.3006040434683523

Epoch: 6| Step: 2
Training loss: 2.8489785194396973
Validation loss: 2.3319449552925686

Epoch: 6| Step: 3
Training loss: 2.986815929412842
Validation loss: 2.3368138677330426

Epoch: 6| Step: 4
Training loss: 2.449631690979004
Validation loss: 2.3550971323443997

Epoch: 6| Step: 5
Training loss: 2.820047378540039
Validation loss: 2.368276913960775

Epoch: 6| Step: 6
Training loss: 3.2481746673583984
Validation loss: 2.3647186986861692

Epoch: 6| Step: 7
Training loss: 2.164187431335449
Validation loss: 2.3511287243135515

Epoch: 6| Step: 8
Training loss: 2.7185041904449463
Validation loss: 2.325844375036096

Epoch: 6| Step: 9
Training loss: 2.4398276805877686
Validation loss: 2.308484988827859

Epoch: 6| Step: 10
Training loss: 2.2806026935577393
Validation loss: 2.315323491250315

Epoch: 6| Step: 11
Training loss: 2.198507070541382
Validation loss: 2.3003436544890046

Epoch: 6| Step: 12
Training loss: 2.3273632526397705
Validation loss: 2.28676434229779

Epoch: 6| Step: 13
Training loss: 2.820720911026001
Validation loss: 2.2891891515383156

Epoch: 92| Step: 0
Training loss: 2.4723916053771973
Validation loss: 2.2939032995572655

Epoch: 6| Step: 1
Training loss: 2.3753228187561035
Validation loss: 2.29719050212573

Epoch: 6| Step: 2
Training loss: 2.003331184387207
Validation loss: 2.2981899771639096

Epoch: 6| Step: 3
Training loss: 2.2319931983947754
Validation loss: 2.2968147031722532

Epoch: 6| Step: 4
Training loss: 2.568885326385498
Validation loss: 2.2994186596203874

Epoch: 6| Step: 5
Training loss: 2.6612610816955566
Validation loss: 2.2964713265818935

Epoch: 6| Step: 6
Training loss: 2.4601781368255615
Validation loss: 2.2988957615308863

Epoch: 6| Step: 7
Training loss: 2.547067642211914
Validation loss: 2.2995596572916996

Epoch: 6| Step: 8
Training loss: 2.7173914909362793
Validation loss: 2.298653964073427

Epoch: 6| Step: 9
Training loss: 2.867513656616211
Validation loss: 2.2976374779978106

Epoch: 6| Step: 10
Training loss: 2.9075963497161865
Validation loss: 2.313732385635376

Epoch: 6| Step: 11
Training loss: 3.3636631965637207
Validation loss: 2.3228519526861047

Epoch: 6| Step: 12
Training loss: 2.531658172607422
Validation loss: 2.326931793202636

Epoch: 6| Step: 13
Training loss: 2.3514835834503174
Validation loss: 2.347408551041798

Epoch: 93| Step: 0
Training loss: 2.8267557621002197
Validation loss: 2.3470063876080256

Epoch: 6| Step: 1
Training loss: 2.9141244888305664
Validation loss: 2.3617409916334253

Epoch: 6| Step: 2
Training loss: 1.545081377029419
Validation loss: 2.360558009916736

Epoch: 6| Step: 3
Training loss: 2.8859527111053467
Validation loss: 2.362307265240659

Epoch: 6| Step: 4
Training loss: 3.000364303588867
Validation loss: 2.3576146043756956

Epoch: 6| Step: 5
Training loss: 2.568345785140991
Validation loss: 2.3564226550440632

Epoch: 6| Step: 6
Training loss: 3.1058449745178223
Validation loss: 2.3362821481561147

Epoch: 6| Step: 7
Training loss: 2.3272414207458496
Validation loss: 2.3219136525225896

Epoch: 6| Step: 8
Training loss: 1.3744163513183594
Validation loss: 2.320333742326306

Epoch: 6| Step: 9
Training loss: 2.9377567768096924
Validation loss: 2.3044635788086922

Epoch: 6| Step: 10
Training loss: 2.38539457321167
Validation loss: 2.302147514076643

Epoch: 6| Step: 11
Training loss: 2.7999749183654785
Validation loss: 2.3027339314901702

Epoch: 6| Step: 12
Training loss: 2.333648681640625
Validation loss: 2.29266454327491

Epoch: 6| Step: 13
Training loss: 3.0121889114379883
Validation loss: 2.292901800524804

Epoch: 94| Step: 0
Training loss: 2.448418140411377
Validation loss: 2.2931697548076673

Epoch: 6| Step: 1
Training loss: 2.8124210834503174
Validation loss: 2.2879324728442776

Epoch: 6| Step: 2
Training loss: 2.6321218013763428
Validation loss: 2.2954449397261425

Epoch: 6| Step: 3
Training loss: 2.1093392372131348
Validation loss: 2.298350143176253

Epoch: 6| Step: 4
Training loss: 2.823943853378296
Validation loss: 2.295132875442505

Epoch: 6| Step: 5
Training loss: 2.1192831993103027
Validation loss: 2.302839397102274

Epoch: 6| Step: 6
Training loss: 2.849494457244873
Validation loss: 2.3065522409254506

Epoch: 6| Step: 7
Training loss: 2.338672637939453
Validation loss: 2.309570053572296

Epoch: 6| Step: 8
Training loss: 3.22342848777771
Validation loss: 2.317943093597248

Epoch: 6| Step: 9
Training loss: 2.1869635581970215
Validation loss: 2.3135558174502466

Epoch: 6| Step: 10
Training loss: 2.61997652053833
Validation loss: 2.314375321070353

Epoch: 6| Step: 11
Training loss: 1.857959508895874
Validation loss: 2.3201293073674685

Epoch: 6| Step: 12
Training loss: 3.2692790031433105
Validation loss: 2.311381088790073

Epoch: 6| Step: 13
Training loss: 2.625383138656616
Validation loss: 2.315927122228889

Epoch: 95| Step: 0
Training loss: 1.7690842151641846
Validation loss: 2.3102919875934558

Epoch: 6| Step: 1
Training loss: 3.112535238265991
Validation loss: 2.3147628896979877

Epoch: 6| Step: 2
Training loss: 2.832599639892578
Validation loss: 2.309459988788892

Epoch: 6| Step: 3
Training loss: 2.5392959117889404
Validation loss: 2.3082626788846907

Epoch: 6| Step: 4
Training loss: 2.412930965423584
Validation loss: 2.2977907503804853

Epoch: 6| Step: 5
Training loss: 3.2445220947265625
Validation loss: 2.295324453743555

Epoch: 6| Step: 6
Training loss: 2.3419008255004883
Validation loss: 2.2913867299274733

Epoch: 6| Step: 7
Training loss: 2.726799249649048
Validation loss: 2.2851572318743636

Epoch: 6| Step: 8
Training loss: 2.7311296463012695
Validation loss: 2.292710916970366

Epoch: 6| Step: 9
Training loss: 2.0989270210266113
Validation loss: 2.2930742053575415

Epoch: 6| Step: 10
Training loss: 2.275085926055908
Validation loss: 2.292001783206899

Epoch: 6| Step: 11
Training loss: 3.462618112564087
Validation loss: 2.3015518726841098

Epoch: 6| Step: 12
Training loss: 2.1887378692626953
Validation loss: 2.3040411831230245

Epoch: 6| Step: 13
Training loss: 1.7660664319992065
Validation loss: 2.2992392278486684

Epoch: 96| Step: 0
Training loss: 2.8521151542663574
Validation loss: 2.3031545403183147

Epoch: 6| Step: 1
Training loss: 2.4877736568450928
Validation loss: 2.3262432595734954

Epoch: 6| Step: 2
Training loss: 3.1013641357421875
Validation loss: 2.358246951974848

Epoch: 6| Step: 3
Training loss: 2.405494451522827
Validation loss: 2.357710705008558

Epoch: 6| Step: 4
Training loss: 2.8998217582702637
Validation loss: 2.363122788808679

Epoch: 6| Step: 5
Training loss: 3.0237765312194824
Validation loss: 2.3186945017947944

Epoch: 6| Step: 6
Training loss: 2.6641845703125
Validation loss: 2.302855569829223

Epoch: 6| Step: 7
Training loss: 2.2764430046081543
Validation loss: 2.291955873530398

Epoch: 6| Step: 8
Training loss: 2.3762824535369873
Validation loss: 2.304363532732892

Epoch: 6| Step: 9
Training loss: 2.4097721576690674
Validation loss: 2.310225502137215

Epoch: 6| Step: 10
Training loss: 1.261256456375122
Validation loss: 2.313406677656276

Epoch: 6| Step: 11
Training loss: 2.654024124145508
Validation loss: 2.3183052796189503

Epoch: 6| Step: 12
Training loss: 2.9775614738464355
Validation loss: 2.3068522778890466

Epoch: 6| Step: 13
Training loss: 2.3706116676330566
Validation loss: 2.2904633745070426

Epoch: 97| Step: 0
Training loss: 2.8580715656280518
Validation loss: 2.2848201131307952

Epoch: 6| Step: 1
Training loss: 2.7850263118743896
Validation loss: 2.291992284918344

Epoch: 6| Step: 2
Training loss: 2.4415664672851562
Validation loss: 2.291184495854121

Epoch: 6| Step: 3
Training loss: 3.118265151977539
Validation loss: 2.2899904379280667

Epoch: 6| Step: 4
Training loss: 2.542719602584839
Validation loss: 2.2945032145387385

Epoch: 6| Step: 5
Training loss: 2.8204236030578613
Validation loss: 2.287766315603769

Epoch: 6| Step: 6
Training loss: 2.716731548309326
Validation loss: 2.2858373734258834

Epoch: 6| Step: 7
Training loss: 1.6757192611694336
Validation loss: 2.302876716019005

Epoch: 6| Step: 8
Training loss: 3.3295040130615234
Validation loss: 2.297166706413351

Epoch: 6| Step: 9
Training loss: 2.829756498336792
Validation loss: 2.294212064435405

Epoch: 6| Step: 10
Training loss: 2.3925609588623047
Validation loss: 2.297522137241979

Epoch: 6| Step: 11
Training loss: 1.972076416015625
Validation loss: 2.290298519595977

Epoch: 6| Step: 12
Training loss: 1.9289313554763794
Validation loss: 2.2865298204524542

Epoch: 6| Step: 13
Training loss: 2.305941581726074
Validation loss: 2.2767478317342777

Epoch: 98| Step: 0
Training loss: 2.2333505153656006
Validation loss: 2.2746239759588756

Epoch: 6| Step: 1
Training loss: 1.8612315654754639
Validation loss: 2.275721811479138

Epoch: 6| Step: 2
Training loss: 3.2115375995635986
Validation loss: 2.2830410157480547

Epoch: 6| Step: 3
Training loss: 2.6287546157836914
Validation loss: 2.2821972780330206

Epoch: 6| Step: 4
Training loss: 2.300903797149658
Validation loss: 2.2839872298702115

Epoch: 6| Step: 5
Training loss: 2.3688735961914062
Validation loss: 2.2978797856197564

Epoch: 6| Step: 6
Training loss: 2.5142836570739746
Validation loss: 2.2908590275754213

Epoch: 6| Step: 7
Training loss: 2.8520870208740234
Validation loss: 2.2921092100040887

Epoch: 6| Step: 8
Training loss: 2.686887502670288
Validation loss: 2.305744324961016

Epoch: 6| Step: 9
Training loss: 2.731908082962036
Validation loss: 2.2958132848944715

Epoch: 6| Step: 10
Training loss: 2.7819817066192627
Validation loss: 2.289048417921989

Epoch: 6| Step: 11
Training loss: 2.3486757278442383
Validation loss: 2.296014280729396

Epoch: 6| Step: 12
Training loss: 2.9375460147857666
Validation loss: 2.2963836372539563

Epoch: 6| Step: 13
Training loss: 2.108081579208374
Validation loss: 2.294752520899619

Epoch: 99| Step: 0
Training loss: 2.649029493331909
Validation loss: 2.3067054774171565

Epoch: 6| Step: 1
Training loss: 2.1330668926239014
Validation loss: 2.340967773109354

Epoch: 6| Step: 2
Training loss: 3.0724706649780273
Validation loss: 2.3497946800724154

Epoch: 6| Step: 3
Training loss: 1.934758186340332
Validation loss: 2.3612022246083906

Epoch: 6| Step: 4
Training loss: 2.1822526454925537
Validation loss: 2.356942633146881

Epoch: 6| Step: 5
Training loss: 2.643461227416992
Validation loss: 2.3430435554955595

Epoch: 6| Step: 6
Training loss: 2.8155999183654785
Validation loss: 2.3373839111738306

Epoch: 6| Step: 7
Training loss: 1.9987393617630005
Validation loss: 2.3156833315408356

Epoch: 6| Step: 8
Training loss: 3.339101552963257
Validation loss: 2.30361746972607

Epoch: 6| Step: 9
Training loss: 2.6776466369628906
Validation loss: 2.3036343077177643

Epoch: 6| Step: 10
Training loss: 2.565474510192871
Validation loss: 2.2978489757865987

Epoch: 6| Step: 11
Training loss: 2.1397054195404053
Validation loss: 2.28314975512925

Epoch: 6| Step: 12
Training loss: 2.682455539703369
Validation loss: 2.278042939401442

Epoch: 6| Step: 13
Training loss: 3.4554972648620605
Validation loss: 2.269458870733938

Epoch: 100| Step: 0
Training loss: 2.1720962524414062
Validation loss: 2.275513841259864

Epoch: 6| Step: 1
Training loss: 2.642427682876587
Validation loss: 2.2805859811844362

Epoch: 6| Step: 2
Training loss: 2.586832284927368
Validation loss: 2.2835489421762447

Epoch: 6| Step: 3
Training loss: 2.19693660736084
Validation loss: 2.2802329422325216

Epoch: 6| Step: 4
Training loss: 3.4630446434020996
Validation loss: 2.2855979793815204

Epoch: 6| Step: 5
Training loss: 2.655669689178467
Validation loss: 2.2840058470285065

Epoch: 6| Step: 6
Training loss: 2.0507497787475586
Validation loss: 2.285892645517985

Epoch: 6| Step: 7
Training loss: 2.862335205078125
Validation loss: 2.287616919445735

Epoch: 6| Step: 8
Training loss: 2.248420238494873
Validation loss: 2.2800444172274683

Epoch: 6| Step: 9
Training loss: 2.2747368812561035
Validation loss: 2.2830703335423626

Epoch: 6| Step: 10
Training loss: 2.6387898921966553
Validation loss: 2.2811607724876812

Epoch: 6| Step: 11
Training loss: 3.1300272941589355
Validation loss: 2.287833013842183

Epoch: 6| Step: 12
Training loss: 2.63551664352417
Validation loss: 2.2971996748319237

Epoch: 6| Step: 13
Training loss: 2.094012498855591
Validation loss: 2.29656909870845

Epoch: 101| Step: 0
Training loss: 2.563873052597046
Validation loss: 2.2930846419385684

Epoch: 6| Step: 1
Training loss: 2.1423983573913574
Validation loss: 2.284662510759087

Epoch: 6| Step: 2
Training loss: 2.894070625305176
Validation loss: 2.2826666626878964

Epoch: 6| Step: 3
Training loss: 3.001319169998169
Validation loss: 2.281528990755799

Epoch: 6| Step: 4
Training loss: 2.7972989082336426
Validation loss: 2.2709145520323064

Epoch: 6| Step: 5
Training loss: 2.6131558418273926
Validation loss: 2.2659862784929174

Epoch: 6| Step: 6
Training loss: 3.119499444961548
Validation loss: 2.260846285409825

Epoch: 6| Step: 7
Training loss: 2.4682517051696777
Validation loss: 2.254578698065973

Epoch: 6| Step: 8
Training loss: 2.7642064094543457
Validation loss: 2.256511280613561

Epoch: 6| Step: 9
Training loss: 1.7298294305801392
Validation loss: 2.256910624042634

Epoch: 6| Step: 10
Training loss: 2.2509093284606934
Validation loss: 2.261524946458878

Epoch: 6| Step: 11
Training loss: 2.550143241882324
Validation loss: 2.254178814990546

Epoch: 6| Step: 12
Training loss: 1.7324998378753662
Validation loss: 2.260917176482498

Epoch: 6| Step: 13
Training loss: 3.3877551555633545
Validation loss: 2.2623061774879374

Epoch: 102| Step: 0
Training loss: 2.752755641937256
Validation loss: 2.2646240752230407

Epoch: 6| Step: 1
Training loss: 2.4386305809020996
Validation loss: 2.2667246223777853

Epoch: 6| Step: 2
Training loss: 1.9568909406661987
Validation loss: 2.278374736027051

Epoch: 6| Step: 3
Training loss: 2.4120326042175293
Validation loss: 2.2962395632138817

Epoch: 6| Step: 4
Training loss: 2.278855085372925
Validation loss: 2.289099053670001

Epoch: 6| Step: 5
Training loss: 2.4943201541900635
Validation loss: 2.2906999382921445

Epoch: 6| Step: 6
Training loss: 2.7714176177978516
Validation loss: 2.269247193490305

Epoch: 6| Step: 7
Training loss: 3.586613655090332
Validation loss: 2.26287220626749

Epoch: 6| Step: 8
Training loss: 1.7715795040130615
Validation loss: 2.2597336589649157

Epoch: 6| Step: 9
Training loss: 2.2038588523864746
Validation loss: 2.247504516314435

Epoch: 6| Step: 10
Training loss: 2.4810595512390137
Validation loss: 2.249926733714278

Epoch: 6| Step: 11
Training loss: 2.7223060131073
Validation loss: 2.247607684904529

Epoch: 6| Step: 12
Training loss: 2.519620895385742
Validation loss: 2.246950658418799

Epoch: 6| Step: 13
Training loss: 3.4482710361480713
Validation loss: 2.252558764591012

Epoch: 103| Step: 0
Training loss: 2.05473256111145
Validation loss: 2.2548774493637906

Epoch: 6| Step: 1
Training loss: 2.850398540496826
Validation loss: 2.2517248328014086

Epoch: 6| Step: 2
Training loss: 2.5757558345794678
Validation loss: 2.260893057751399

Epoch: 6| Step: 3
Training loss: 2.4645655155181885
Validation loss: 2.2489197561817784

Epoch: 6| Step: 4
Training loss: 2.704681873321533
Validation loss: 2.2537088547983477

Epoch: 6| Step: 5
Training loss: 2.1153712272644043
Validation loss: 2.2431118052492858

Epoch: 6| Step: 6
Training loss: 2.652945041656494
Validation loss: 2.2392589956201534

Epoch: 6| Step: 7
Training loss: 2.1643218994140625
Validation loss: 2.254352206824928

Epoch: 6| Step: 8
Training loss: 2.3033533096313477
Validation loss: 2.2520922076317573

Epoch: 6| Step: 9
Training loss: 2.836984157562256
Validation loss: 2.2484567844739525

Epoch: 6| Step: 10
Training loss: 2.678917169570923
Validation loss: 2.245072780116912

Epoch: 6| Step: 11
Training loss: 3.3095288276672363
Validation loss: 2.2536355423670944

Epoch: 6| Step: 12
Training loss: 2.3699634075164795
Validation loss: 2.2544716532512377

Epoch: 6| Step: 13
Training loss: 2.37892746925354
Validation loss: 2.261502618430763

Epoch: 104| Step: 0
Training loss: 2.720885753631592
Validation loss: 2.2621744960866947

Epoch: 6| Step: 1
Training loss: 2.452148914337158
Validation loss: 2.258249867346979

Epoch: 6| Step: 2
Training loss: 3.005368947982788
Validation loss: 2.267292197032641

Epoch: 6| Step: 3
Training loss: 2.3862438201904297
Validation loss: 2.28701509967927

Epoch: 6| Step: 4
Training loss: 2.1969716548919678
Validation loss: 2.2969804886848695

Epoch: 6| Step: 5
Training loss: 2.3551836013793945
Validation loss: 2.305001130668066

Epoch: 6| Step: 6
Training loss: 2.9734244346618652
Validation loss: 2.2873485626712924

Epoch: 6| Step: 7
Training loss: 2.6211705207824707
Validation loss: 2.268735788201773

Epoch: 6| Step: 8
Training loss: 2.1770853996276855
Validation loss: 2.257948775445261

Epoch: 6| Step: 9
Training loss: 2.5303955078125
Validation loss: 2.2504477603461153

Epoch: 6| Step: 10
Training loss: 3.0500173568725586
Validation loss: 2.237182991479033

Epoch: 6| Step: 11
Training loss: 2.292630195617676
Validation loss: 2.230332692464193

Epoch: 6| Step: 12
Training loss: 2.859933376312256
Validation loss: 2.23169578659919

Epoch: 6| Step: 13
Training loss: 1.3975895643234253
Validation loss: 2.236081584807365

Epoch: 105| Step: 0
Training loss: 2.718219041824341
Validation loss: 2.231971476667671

Epoch: 6| Step: 1
Training loss: 2.1735801696777344
Validation loss: 2.2354454173836658

Epoch: 6| Step: 2
Training loss: 2.549269676208496
Validation loss: 2.2391110030553674

Epoch: 6| Step: 3
Training loss: 3.026447296142578
Validation loss: 2.232735800486739

Epoch: 6| Step: 4
Training loss: 1.9213993549346924
Validation loss: 2.2463885148366294

Epoch: 6| Step: 5
Training loss: 2.9257121086120605
Validation loss: 2.266873936499319

Epoch: 6| Step: 6
Training loss: 2.9748411178588867
Validation loss: 2.3038838986427552

Epoch: 6| Step: 7
Training loss: 2.9866750240325928
Validation loss: 2.3068891648323304

Epoch: 6| Step: 8
Training loss: 2.137089490890503
Validation loss: 2.301844991663451

Epoch: 6| Step: 9
Training loss: 2.2600083351135254
Validation loss: 2.2968695317545245

Epoch: 6| Step: 10
Training loss: 2.4944350719451904
Validation loss: 2.277344411419284

Epoch: 6| Step: 11
Training loss: 2.2991833686828613
Validation loss: 2.2579030631690897

Epoch: 6| Step: 12
Training loss: 2.7120115756988525
Validation loss: 2.2580782918519873

Epoch: 6| Step: 13
Training loss: 1.987984538078308
Validation loss: 2.2537423282541256

Epoch: 106| Step: 0
Training loss: 3.0676987171173096
Validation loss: 2.24696792838394

Epoch: 6| Step: 1
Training loss: 1.8267347812652588
Validation loss: 2.2536234035286853

Epoch: 6| Step: 2
Training loss: 3.091996192932129
Validation loss: 2.254842760742352

Epoch: 6| Step: 3
Training loss: 2.2415318489074707
Validation loss: 2.25199447395981

Epoch: 6| Step: 4
Training loss: 3.213975429534912
Validation loss: 2.2470322808911725

Epoch: 6| Step: 5
Training loss: 2.2697267532348633
Validation loss: 2.2546317064633934

Epoch: 6| Step: 6
Training loss: 2.874140501022339
Validation loss: 2.252891953273486

Epoch: 6| Step: 7
Training loss: 2.364192485809326
Validation loss: 2.254451333835561

Epoch: 6| Step: 8
Training loss: 2.2728986740112305
Validation loss: 2.264034584004392

Epoch: 6| Step: 9
Training loss: 2.29703426361084
Validation loss: 2.2629186235448366

Epoch: 6| Step: 10
Training loss: 2.4053103923797607
Validation loss: 2.2684206449857323

Epoch: 6| Step: 11
Training loss: 2.8671135902404785
Validation loss: 2.2662567143799155

Epoch: 6| Step: 12
Training loss: 2.2880325317382812
Validation loss: 2.253657235894152

Epoch: 6| Step: 13
Training loss: 1.9669227600097656
Validation loss: 2.2473316243899766

Epoch: 107| Step: 0
Training loss: 2.52264142036438
Validation loss: 2.237916831047304

Epoch: 6| Step: 1
Training loss: 2.499500274658203
Validation loss: 2.2301816940307617

Epoch: 6| Step: 2
Training loss: 2.3205950260162354
Validation loss: 2.2265163134503108

Epoch: 6| Step: 3
Training loss: 2.7411282062530518
Validation loss: 2.221162840884219

Epoch: 6| Step: 4
Training loss: 1.8588048219680786
Validation loss: 2.2271460871542654

Epoch: 6| Step: 5
Training loss: 1.8818484544754028
Validation loss: 2.2242056733818463

Epoch: 6| Step: 6
Training loss: 2.5172619819641113
Validation loss: 2.2277565412623908

Epoch: 6| Step: 7
Training loss: 2.580106019973755
Validation loss: 2.2263924831985147

Epoch: 6| Step: 8
Training loss: 2.5392396450042725
Validation loss: 2.2233946733577277

Epoch: 6| Step: 9
Training loss: 2.6099133491516113
Validation loss: 2.2305335511443434

Epoch: 6| Step: 10
Training loss: 3.226024627685547
Validation loss: 2.2448450006464475

Epoch: 6| Step: 11
Training loss: 2.31013822555542
Validation loss: 2.2525476242906306

Epoch: 6| Step: 12
Training loss: 2.6453464031219482
Validation loss: 2.2522750516091623

Epoch: 6| Step: 13
Training loss: 3.29634952545166
Validation loss: 2.2666576652116674

Epoch: 108| Step: 0
Training loss: 3.361595869064331
Validation loss: 2.26541123595289

Epoch: 6| Step: 1
Training loss: 2.688779830932617
Validation loss: 2.2582034269968667

Epoch: 6| Step: 2
Training loss: 2.4434051513671875
Validation loss: 2.2593892389728176

Epoch: 6| Step: 3
Training loss: 2.4987740516662598
Validation loss: 2.2465814082853255

Epoch: 6| Step: 4
Training loss: 1.8361446857452393
Validation loss: 2.2393329092251357

Epoch: 6| Step: 5
Training loss: 3.024789333343506
Validation loss: 2.226180084290043

Epoch: 6| Step: 6
Training loss: 1.8516356945037842
Validation loss: 2.2260966018963884

Epoch: 6| Step: 7
Training loss: 3.062717914581299
Validation loss: 2.2196960705582813

Epoch: 6| Step: 8
Training loss: 2.1700491905212402
Validation loss: 2.2202180162552865

Epoch: 6| Step: 9
Training loss: 2.8306117057800293
Validation loss: 2.222098232597433

Epoch: 6| Step: 10
Training loss: 1.8800781965255737
Validation loss: 2.2276564926229496

Epoch: 6| Step: 11
Training loss: 2.7423439025878906
Validation loss: 2.219688783409775

Epoch: 6| Step: 12
Training loss: 2.2455949783325195
Validation loss: 2.2160849109772713

Epoch: 6| Step: 13
Training loss: 3.1199653148651123
Validation loss: 2.2243643191552933

Epoch: 109| Step: 0
Training loss: 3.4623329639434814
Validation loss: 2.2208911577860513

Epoch: 6| Step: 1
Training loss: 2.506053924560547
Validation loss: 2.232032001659434

Epoch: 6| Step: 2
Training loss: 1.7379869222640991
Validation loss: 2.2473094335166355

Epoch: 6| Step: 3
Training loss: 2.75793194770813
Validation loss: 2.2565565801435903

Epoch: 6| Step: 4
Training loss: 2.692410469055176
Validation loss: 2.2752869308635755

Epoch: 6| Step: 5
Training loss: 2.383774995803833
Validation loss: 2.283658722395538

Epoch: 6| Step: 6
Training loss: 2.1857314109802246
Validation loss: 2.292335085971381

Epoch: 6| Step: 7
Training loss: 3.3785958290100098
Validation loss: 2.2975195300194526

Epoch: 6| Step: 8
Training loss: 2.221341133117676
Validation loss: 2.30438038867007

Epoch: 6| Step: 9
Training loss: 2.8128864765167236
Validation loss: 2.290382727499931

Epoch: 6| Step: 10
Training loss: 1.9875988960266113
Validation loss: 2.2822674320590113

Epoch: 6| Step: 11
Training loss: 2.6681718826293945
Validation loss: 2.2759118772322133

Epoch: 6| Step: 12
Training loss: 1.9642908573150635
Validation loss: 2.2716584308173067

Epoch: 6| Step: 13
Training loss: 2.7044503688812256
Validation loss: 2.279491086159983

Epoch: 110| Step: 0
Training loss: 2.644049644470215
Validation loss: 2.260383580320625

Epoch: 6| Step: 1
Training loss: 3.2477569580078125
Validation loss: 2.2792567668422574

Epoch: 6| Step: 2
Training loss: 2.25642728805542
Validation loss: 2.2816294623959448

Epoch: 6| Step: 3
Training loss: 1.8540831804275513
Validation loss: 2.291231991142355

Epoch: 6| Step: 4
Training loss: 2.2822399139404297
Validation loss: 2.2726493291957404

Epoch: 6| Step: 5
Training loss: 2.2963063716888428
Validation loss: 2.250301161120015

Epoch: 6| Step: 6
Training loss: 3.1158299446105957
Validation loss: 2.2401288709332867

Epoch: 6| Step: 7
Training loss: 2.5652761459350586
Validation loss: 2.2367186187415995

Epoch: 6| Step: 8
Training loss: 3.247143268585205
Validation loss: 2.233345926448863

Epoch: 6| Step: 9
Training loss: 2.2637996673583984
Validation loss: 2.2311223578709427

Epoch: 6| Step: 10
Training loss: 1.7427804470062256
Validation loss: 2.220798910305064

Epoch: 6| Step: 11
Training loss: 2.372952461242676
Validation loss: 2.213664042052402

Epoch: 6| Step: 12
Training loss: 2.7871696949005127
Validation loss: 2.2309843635046356

Epoch: 6| Step: 13
Training loss: 2.584292411804199
Validation loss: 2.223378348094161

Epoch: 111| Step: 0
Training loss: 2.5600829124450684
Validation loss: 2.2328954178799867

Epoch: 6| Step: 1
Training loss: 2.063420057296753
Validation loss: 2.2683328556758102

Epoch: 6| Step: 2
Training loss: 3.365344524383545
Validation loss: 2.2796829387705815

Epoch: 6| Step: 3
Training loss: 2.561591625213623
Validation loss: 2.27589907697452

Epoch: 6| Step: 4
Training loss: 2.493480682373047
Validation loss: 2.2864449331837315

Epoch: 6| Step: 5
Training loss: 2.6802444458007812
Validation loss: 2.28031228691019

Epoch: 6| Step: 6
Training loss: 2.7268476486206055
Validation loss: 2.283336421494843

Epoch: 6| Step: 7
Training loss: 2.3105337619781494
Validation loss: 2.2713088604711715

Epoch: 6| Step: 8
Training loss: 2.0256433486938477
Validation loss: 2.2526356443282096

Epoch: 6| Step: 9
Training loss: 2.2122764587402344
Validation loss: 2.2323078519554547

Epoch: 6| Step: 10
Training loss: 2.2273054122924805
Validation loss: 2.2245454531843945

Epoch: 6| Step: 11
Training loss: 2.4330332279205322
Validation loss: 2.2256253534747708

Epoch: 6| Step: 12
Training loss: 2.385721206665039
Validation loss: 2.2266286470556773

Epoch: 6| Step: 13
Training loss: 3.87290096282959
Validation loss: 2.227998190028693

Epoch: 112| Step: 0
Training loss: 2.930912494659424
Validation loss: 2.2296194594393492

Epoch: 6| Step: 1
Training loss: 2.9723494052886963
Validation loss: 2.224745706845355

Epoch: 6| Step: 2
Training loss: 1.9829455614089966
Validation loss: 2.2277907761194373

Epoch: 6| Step: 3
Training loss: 2.8696837425231934
Validation loss: 2.2396042064953874

Epoch: 6| Step: 4
Training loss: 2.404780864715576
Validation loss: 2.241410529741677

Epoch: 6| Step: 5
Training loss: 2.8098340034484863
Validation loss: 2.2646530956350346

Epoch: 6| Step: 6
Training loss: 2.042553186416626
Validation loss: 2.278949886239985

Epoch: 6| Step: 7
Training loss: 2.850707530975342
Validation loss: 2.2968731208514144

Epoch: 6| Step: 8
Training loss: 1.9869880676269531
Validation loss: 2.3226880155583864

Epoch: 6| Step: 9
Training loss: 1.9657964706420898
Validation loss: 2.3159474852264568

Epoch: 6| Step: 10
Training loss: 2.575697422027588
Validation loss: 2.3111794802450363

Epoch: 6| Step: 11
Training loss: 3.0319366455078125
Validation loss: 2.261713333027337

Epoch: 6| Step: 12
Training loss: 2.5993638038635254
Validation loss: 2.2357143176499235

Epoch: 6| Step: 13
Training loss: 1.8218274116516113
Validation loss: 2.239918867746989

Epoch: 113| Step: 0
Training loss: 2.7831008434295654
Validation loss: 2.2386714617411294

Epoch: 6| Step: 1
Training loss: 3.0144660472869873
Validation loss: 2.230216468534162

Epoch: 6| Step: 2
Training loss: 3.1260745525360107
Validation loss: 2.229164095335109

Epoch: 6| Step: 3
Training loss: 2.583129405975342
Validation loss: 2.218680017737932

Epoch: 6| Step: 4
Training loss: 2.853872776031494
Validation loss: 2.228761670409992

Epoch: 6| Step: 5
Training loss: 2.341317892074585
Validation loss: 2.2316419411731023

Epoch: 6| Step: 6
Training loss: 1.504860281944275
Validation loss: 2.2328332880491852

Epoch: 6| Step: 7
Training loss: 2.213846445083618
Validation loss: 2.2327529512425905

Epoch: 6| Step: 8
Training loss: 2.287001371383667
Validation loss: 2.2379317411812405

Epoch: 6| Step: 9
Training loss: 2.2210159301757812
Validation loss: 2.23465310629978

Epoch: 6| Step: 10
Training loss: 3.1101346015930176
Validation loss: 2.2399299042199248

Epoch: 6| Step: 11
Training loss: 1.9427399635314941
Validation loss: 2.2546318192635812

Epoch: 6| Step: 12
Training loss: 3.5262699127197266
Validation loss: 2.2660813972514164

Epoch: 6| Step: 13
Training loss: 1.5771862268447876
Validation loss: 2.2788100973252328

Epoch: 114| Step: 0
Training loss: 2.5559351444244385
Validation loss: 2.285048289965558

Epoch: 6| Step: 1
Training loss: 2.648632526397705
Validation loss: 2.291058176307268

Epoch: 6| Step: 2
Training loss: 2.5426931381225586
Validation loss: 2.2903892327380437

Epoch: 6| Step: 3
Training loss: 2.54919171333313
Validation loss: 2.298773414345198

Epoch: 6| Step: 4
Training loss: 2.1751439571380615
Validation loss: 2.2885963968051377

Epoch: 6| Step: 5
Training loss: 2.629549026489258
Validation loss: 2.282260362819959

Epoch: 6| Step: 6
Training loss: 2.2352378368377686
Validation loss: 2.2557077574473556

Epoch: 6| Step: 7
Training loss: 2.678380012512207
Validation loss: 2.233595871156262

Epoch: 6| Step: 8
Training loss: 2.254568099975586
Validation loss: 2.2268512582266204

Epoch: 6| Step: 9
Training loss: 2.7168426513671875
Validation loss: 2.224352257226103

Epoch: 6| Step: 10
Training loss: 2.8318896293640137
Validation loss: 2.2157871466810986

Epoch: 6| Step: 11
Training loss: 2.6990420818328857
Validation loss: 2.2150814994688957

Epoch: 6| Step: 12
Training loss: 2.360790967941284
Validation loss: 2.2100894015322448

Epoch: 6| Step: 13
Training loss: 2.353816032409668
Validation loss: 2.216147058753557

Epoch: 115| Step: 0
Training loss: 2.1267924308776855
Validation loss: 2.218257975834672

Epoch: 6| Step: 1
Training loss: 3.061509132385254
Validation loss: 2.2266241376117994

Epoch: 6| Step: 2
Training loss: 2.9021310806274414
Validation loss: 2.2398845957171534

Epoch: 6| Step: 3
Training loss: 2.4984097480773926
Validation loss: 2.247249734017157

Epoch: 6| Step: 4
Training loss: 2.1456451416015625
Validation loss: 2.2551349183564544

Epoch: 6| Step: 5
Training loss: 2.325511932373047
Validation loss: 2.252292004964685

Epoch: 6| Step: 6
Training loss: 1.9801310300827026
Validation loss: 2.255208376915224

Epoch: 6| Step: 7
Training loss: 2.506934881210327
Validation loss: 2.279417919856246

Epoch: 6| Step: 8
Training loss: 2.041029691696167
Validation loss: 2.26983372370402

Epoch: 6| Step: 9
Training loss: 2.730194330215454
Validation loss: 2.289003749047556

Epoch: 6| Step: 10
Training loss: 2.4024157524108887
Validation loss: 2.2758670571029826

Epoch: 6| Step: 11
Training loss: 2.6860599517822266
Validation loss: 2.2668645510109524

Epoch: 6| Step: 12
Training loss: 2.883039951324463
Validation loss: 2.2652306018337125

Epoch: 6| Step: 13
Training loss: 3.137190103530884
Validation loss: 2.247569248240481

Epoch: 116| Step: 0
Training loss: 2.743391513824463
Validation loss: 2.2515412710046254

Epoch: 6| Step: 1
Training loss: 2.3036274909973145
Validation loss: 2.2502989358799432

Epoch: 6| Step: 2
Training loss: 2.8787105083465576
Validation loss: 2.270115008918188

Epoch: 6| Step: 3
Training loss: 2.647829294204712
Validation loss: 2.2703241314939273

Epoch: 6| Step: 4
Training loss: 2.5620999336242676
Validation loss: 2.2717332378510506

Epoch: 6| Step: 5
Training loss: 2.651008129119873
Validation loss: 2.2624724090740247

Epoch: 6| Step: 6
Training loss: 1.9905829429626465
Validation loss: 2.274138027621854

Epoch: 6| Step: 7
Training loss: 2.4086899757385254
Validation loss: 2.2711551907241985

Epoch: 6| Step: 8
Training loss: 1.7899335622787476
Validation loss: 2.2735786566170315

Epoch: 6| Step: 9
Training loss: 2.3350141048431396
Validation loss: 2.252938232114238

Epoch: 6| Step: 10
Training loss: 2.7926340103149414
Validation loss: 2.248745690109909

Epoch: 6| Step: 11
Training loss: 2.3990187644958496
Validation loss: 2.2390078703562417

Epoch: 6| Step: 12
Training loss: 2.737339735031128
Validation loss: 2.224771882898064

Epoch: 6| Step: 13
Training loss: 3.010249137878418
Validation loss: 2.213756970179978

Epoch: 117| Step: 0
Training loss: 1.905268907546997
Validation loss: 2.2086251525468725

Epoch: 6| Step: 1
Training loss: 3.1112403869628906
Validation loss: 2.2035535561141146

Epoch: 6| Step: 2
Training loss: 2.7135870456695557
Validation loss: 2.2023271514523413

Epoch: 6| Step: 3
Training loss: 2.357686996459961
Validation loss: 2.2119811991209626

Epoch: 6| Step: 4
Training loss: 2.4533772468566895
Validation loss: 2.20795746644338

Epoch: 6| Step: 5
Training loss: 1.8281018733978271
Validation loss: 2.1984636578508603

Epoch: 6| Step: 6
Training loss: 2.482353925704956
Validation loss: 2.2090702646522113

Epoch: 6| Step: 7
Training loss: 2.936466932296753
Validation loss: 2.2050800836214455

Epoch: 6| Step: 8
Training loss: 3.0565948486328125
Validation loss: 2.207645866178697

Epoch: 6| Step: 9
Training loss: 2.7068843841552734
Validation loss: 2.2138779778634348

Epoch: 6| Step: 10
Training loss: 1.731466293334961
Validation loss: 2.2152224022855043

Epoch: 6| Step: 11
Training loss: 2.819366455078125
Validation loss: 2.234734945399787

Epoch: 6| Step: 12
Training loss: 2.3013768196105957
Validation loss: 2.2504829104228685

Epoch: 6| Step: 13
Training loss: 2.5115880966186523
Validation loss: 2.2729780033070552

Epoch: 118| Step: 0
Training loss: 2.1586971282958984
Validation loss: 2.282673981881911

Epoch: 6| Step: 1
Training loss: 2.152897834777832
Validation loss: 2.2609151358245523

Epoch: 6| Step: 2
Training loss: 3.2994472980499268
Validation loss: 2.2559010162148425

Epoch: 6| Step: 3
Training loss: 2.509369134902954
Validation loss: 2.230819199674873

Epoch: 6| Step: 4
Training loss: 2.4489848613739014
Validation loss: 2.21210878638811

Epoch: 6| Step: 5
Training loss: 2.3614859580993652
Validation loss: 2.204640958898811

Epoch: 6| Step: 6
Training loss: 2.2742533683776855
Validation loss: 2.199202732373309

Epoch: 6| Step: 7
Training loss: 2.1734671592712402
Validation loss: 2.207678066786899

Epoch: 6| Step: 8
Training loss: 2.7810378074645996
Validation loss: 2.22335873880694

Epoch: 6| Step: 9
Training loss: 2.517138957977295
Validation loss: 2.2326126765179377

Epoch: 6| Step: 10
Training loss: 2.534132480621338
Validation loss: 2.228593208456552

Epoch: 6| Step: 11
Training loss: 2.7155489921569824
Validation loss: 2.2313944139788227

Epoch: 6| Step: 12
Training loss: 2.661130905151367
Validation loss: 2.225387860369939

Epoch: 6| Step: 13
Training loss: 2.425750970840454
Validation loss: 2.2190794714035524

Epoch: 119| Step: 0
Training loss: 1.6006836891174316
Validation loss: 2.2279697874540925

Epoch: 6| Step: 1
Training loss: 2.638740062713623
Validation loss: 2.221114996940859

Epoch: 6| Step: 2
Training loss: 2.655989170074463
Validation loss: 2.223645016711245

Epoch: 6| Step: 3
Training loss: 1.9431370496749878
Validation loss: 2.2230773625835294

Epoch: 6| Step: 4
Training loss: 2.780076503753662
Validation loss: 2.2187638744231193

Epoch: 6| Step: 5
Training loss: 2.057595729827881
Validation loss: 2.227401010451778

Epoch: 6| Step: 6
Training loss: 2.8267457485198975
Validation loss: 2.230566176035071

Epoch: 6| Step: 7
Training loss: 2.33449649810791
Validation loss: 2.2372180620829263

Epoch: 6| Step: 8
Training loss: 2.171665906906128
Validation loss: 2.2677364400638047

Epoch: 6| Step: 9
Training loss: 2.6695992946624756
Validation loss: 2.2815377789158977

Epoch: 6| Step: 10
Training loss: 2.2378573417663574
Validation loss: 2.3004697266445366

Epoch: 6| Step: 11
Training loss: 2.9207873344421387
Validation loss: 2.353261286212552

Epoch: 6| Step: 12
Training loss: 3.6653079986572266
Validation loss: 2.3691738318371516

Epoch: 6| Step: 13
Training loss: 3.0060555934906006
Validation loss: 2.4063545606469594

Epoch: 120| Step: 0
Training loss: 2.01259446144104
Validation loss: 2.3756468860051965

Epoch: 6| Step: 1
Training loss: 2.0982577800750732
Validation loss: 2.3598249702043432

Epoch: 6| Step: 2
Training loss: 2.3774290084838867
Validation loss: 2.343201352703956

Epoch: 6| Step: 3
Training loss: 2.1893672943115234
Validation loss: 2.3668018515392015

Epoch: 6| Step: 4
Training loss: 2.581894874572754
Validation loss: 2.40527613957723

Epoch: 6| Step: 5
Training loss: 2.542581796646118
Validation loss: 2.414110860516948

Epoch: 6| Step: 6
Training loss: 2.999811887741089
Validation loss: 2.3906814308576685

Epoch: 6| Step: 7
Training loss: 2.44980525970459
Validation loss: 2.341559435731621

Epoch: 6| Step: 8
Training loss: 3.1089043617248535
Validation loss: 2.2948205599220852

Epoch: 6| Step: 9
Training loss: 2.6125354766845703
Validation loss: 2.26008084512526

Epoch: 6| Step: 10
Training loss: 2.698101282119751
Validation loss: 2.2272142902497323

Epoch: 6| Step: 11
Training loss: 2.508657455444336
Validation loss: 2.2069382462450253

Epoch: 6| Step: 12
Training loss: 2.8049120903015137
Validation loss: 2.219392648307226

Epoch: 6| Step: 13
Training loss: 3.0293331146240234
Validation loss: 2.2339331770455964

Epoch: 121| Step: 0
Training loss: 2.2034924030303955
Validation loss: 2.293761066211167

Epoch: 6| Step: 1
Training loss: 2.575986385345459
Validation loss: 2.3573357084746003

Epoch: 6| Step: 2
Training loss: 3.0417537689208984
Validation loss: 2.3970140359734975

Epoch: 6| Step: 3
Training loss: 2.989629030227661
Validation loss: 2.400261999458395

Epoch: 6| Step: 4
Training loss: 3.3109986782073975
Validation loss: 2.390429524965184

Epoch: 6| Step: 5
Training loss: 2.8522233963012695
Validation loss: 2.368782017820625

Epoch: 6| Step: 6
Training loss: 2.5731217861175537
Validation loss: 2.3086061810934417

Epoch: 6| Step: 7
Training loss: 1.3268033266067505
Validation loss: 2.2624119199732298

Epoch: 6| Step: 8
Training loss: 2.3897314071655273
Validation loss: 2.23247270430288

Epoch: 6| Step: 9
Training loss: 2.911487579345703
Validation loss: 2.2066377055260444

Epoch: 6| Step: 10
Training loss: 2.5194976329803467
Validation loss: 2.19856842102543

Epoch: 6| Step: 11
Training loss: 2.4787697792053223
Validation loss: 2.194854256927326

Epoch: 6| Step: 12
Training loss: 2.6167826652526855
Validation loss: 2.1979777633502917

Epoch: 6| Step: 13
Training loss: 1.4940752983093262
Validation loss: 2.2007433368313696

Epoch: 122| Step: 0
Training loss: 1.3872193098068237
Validation loss: 2.1961708479030158

Epoch: 6| Step: 1
Training loss: 2.1112380027770996
Validation loss: 2.1990676118481542

Epoch: 6| Step: 2
Training loss: 2.7514865398406982
Validation loss: 2.206657604504657

Epoch: 6| Step: 3
Training loss: 2.4785561561584473
Validation loss: 2.2418822293640464

Epoch: 6| Step: 4
Training loss: 2.836921453475952
Validation loss: 2.2494027973503194

Epoch: 6| Step: 5
Training loss: 1.8648267984390259
Validation loss: 2.263474092688612

Epoch: 6| Step: 6
Training loss: 2.9053733348846436
Validation loss: 2.282958358846685

Epoch: 6| Step: 7
Training loss: 1.9741411209106445
Validation loss: 2.292730200675226

Epoch: 6| Step: 8
Training loss: 2.6287894248962402
Validation loss: 2.300705358546267

Epoch: 6| Step: 9
Training loss: 2.9899144172668457
Validation loss: 2.313185430342151

Epoch: 6| Step: 10
Training loss: 2.350008964538574
Validation loss: 2.2982537592611005

Epoch: 6| Step: 11
Training loss: 2.691267967224121
Validation loss: 2.3135007889040056

Epoch: 6| Step: 12
Training loss: 3.1725025177001953
Validation loss: 2.299648401557758

Epoch: 6| Step: 13
Training loss: 3.101182222366333
Validation loss: 2.257808905775829

Epoch: 123| Step: 0
Training loss: 2.7842602729797363
Validation loss: 2.233264192458122

Epoch: 6| Step: 1
Training loss: 2.421672821044922
Validation loss: 2.2152942790780017

Epoch: 6| Step: 2
Training loss: 1.8960483074188232
Validation loss: 2.1960046650261007

Epoch: 6| Step: 3
Training loss: 2.1995983123779297
Validation loss: 2.1866217531183714

Epoch: 6| Step: 4
Training loss: 2.6043715476989746
Validation loss: 2.192991766878354

Epoch: 6| Step: 5
Training loss: 2.577781915664673
Validation loss: 2.1929964301406697

Epoch: 6| Step: 6
Training loss: 3.1524386405944824
Validation loss: 2.1919237157349944

Epoch: 6| Step: 7
Training loss: 1.8665250539779663
Validation loss: 2.1997887216588503

Epoch: 6| Step: 8
Training loss: 2.0833492279052734
Validation loss: 2.191373887882438

Epoch: 6| Step: 9
Training loss: 2.5211739540100098
Validation loss: 2.196085017214539

Epoch: 6| Step: 10
Training loss: 3.000267505645752
Validation loss: 2.198690882293127

Epoch: 6| Step: 11
Training loss: 2.7493135929107666
Validation loss: 2.188016968388711

Epoch: 6| Step: 12
Training loss: 2.484961748123169
Validation loss: 2.1920217954984276

Epoch: 6| Step: 13
Training loss: 2.504016637802124
Validation loss: 2.194284733905587

Epoch: 124| Step: 0
Training loss: 2.204408645629883
Validation loss: 2.1743923079582954

Epoch: 6| Step: 1
Training loss: 2.2011144161224365
Validation loss: 2.1788644841922227

Epoch: 6| Step: 2
Training loss: 2.7822470664978027
Validation loss: 2.186678195512423

Epoch: 6| Step: 3
Training loss: 2.656343936920166
Validation loss: 2.185640832429291

Epoch: 6| Step: 4
Training loss: 2.026909828186035
Validation loss: 2.1920671962922618

Epoch: 6| Step: 5
Training loss: 2.973613739013672
Validation loss: 2.193154658040693

Epoch: 6| Step: 6
Training loss: 2.4931559562683105
Validation loss: 2.1886662680615663

Epoch: 6| Step: 7
Training loss: 2.231405258178711
Validation loss: 2.1886434837054183

Epoch: 6| Step: 8
Training loss: 3.313432216644287
Validation loss: 2.1952328810127835

Epoch: 6| Step: 9
Training loss: 3.366645336151123
Validation loss: 2.1937526631098923

Epoch: 6| Step: 10
Training loss: 1.9711581468582153
Validation loss: 2.1843418511011268

Epoch: 6| Step: 11
Training loss: 1.8384101390838623
Validation loss: 2.1911782321109565

Epoch: 6| Step: 12
Training loss: 2.2716281414031982
Validation loss: 2.193405492331392

Epoch: 6| Step: 13
Training loss: 2.407850503921509
Validation loss: 2.181068456301125

Epoch: 125| Step: 0
Training loss: 2.655437707901001
Validation loss: 2.1773494764040877

Epoch: 6| Step: 1
Training loss: 2.361044406890869
Validation loss: 2.176140898017473

Epoch: 6| Step: 2
Training loss: 2.543353796005249
Validation loss: 2.1789046743864655

Epoch: 6| Step: 3
Training loss: 2.3797826766967773
Validation loss: 2.1875568615492953

Epoch: 6| Step: 4
Training loss: 2.817601442337036
Validation loss: 2.1920475485504314

Epoch: 6| Step: 5
Training loss: 2.523406505584717
Validation loss: 2.1946232395787395

Epoch: 6| Step: 6
Training loss: 2.9773824214935303
Validation loss: 2.190956272104735

Epoch: 6| Step: 7
Training loss: 2.8948116302490234
Validation loss: 2.1884008043555805

Epoch: 6| Step: 8
Training loss: 2.0148978233337402
Validation loss: 2.188753035760695

Epoch: 6| Step: 9
Training loss: 2.0614466667175293
Validation loss: 2.198481116243588

Epoch: 6| Step: 10
Training loss: 2.1405391693115234
Validation loss: 2.196909501988401

Epoch: 6| Step: 11
Training loss: 2.2487716674804688
Validation loss: 2.2176023965240805

Epoch: 6| Step: 12
Training loss: 2.917675495147705
Validation loss: 2.2350981620050248

Epoch: 6| Step: 13
Training loss: 1.8541717529296875
Validation loss: 2.2340098606642855

Epoch: 126| Step: 0
Training loss: 1.2743844985961914
Validation loss: 2.258291236815914

Epoch: 6| Step: 1
Training loss: 2.5280587673187256
Validation loss: 2.2743327028007916

Epoch: 6| Step: 2
Training loss: 2.4048755168914795
Validation loss: 2.2746048665815786

Epoch: 6| Step: 3
Training loss: 1.9560354948043823
Validation loss: 2.2519968350728354

Epoch: 6| Step: 4
Training loss: 2.351458787918091
Validation loss: 2.228025392819476

Epoch: 6| Step: 5
Training loss: 3.0664656162261963
Validation loss: 2.2110777260154806

Epoch: 6| Step: 6
Training loss: 2.396721601486206
Validation loss: 2.2064855816543743

Epoch: 6| Step: 7
Training loss: 2.8473386764526367
Validation loss: 2.192979338348553

Epoch: 6| Step: 8
Training loss: 3.4198410511016846
Validation loss: 2.186774612754904

Epoch: 6| Step: 9
Training loss: 2.552837371826172
Validation loss: 2.1817830249827397

Epoch: 6| Step: 10
Training loss: 2.3521676063537598
Validation loss: 2.181386152903239

Epoch: 6| Step: 11
Training loss: 2.5361294746398926
Validation loss: 2.1769990100655505

Epoch: 6| Step: 12
Training loss: 2.4787864685058594
Validation loss: 2.1837009153058453

Epoch: 6| Step: 13
Training loss: 2.5196499824523926
Validation loss: 2.1766188977867045

Epoch: 127| Step: 0
Training loss: 1.8835511207580566
Validation loss: 2.1709337977952856

Epoch: 6| Step: 1
Training loss: 3.1259403228759766
Validation loss: 2.1769008905656877

Epoch: 6| Step: 2
Training loss: 2.9666953086853027
Validation loss: 2.175826580293717

Epoch: 6| Step: 3
Training loss: 1.8727405071258545
Validation loss: 2.1833972418180077

Epoch: 6| Step: 4
Training loss: 2.8077139854431152
Validation loss: 2.1805661096367785

Epoch: 6| Step: 5
Training loss: 2.2257137298583984
Validation loss: 2.1781796639965427

Epoch: 6| Step: 6
Training loss: 2.3288397789001465
Validation loss: 2.181673087099547

Epoch: 6| Step: 7
Training loss: 2.6907100677490234
Validation loss: 2.196950425383865

Epoch: 6| Step: 8
Training loss: 2.903934955596924
Validation loss: 2.1952751298104562

Epoch: 6| Step: 9
Training loss: 1.5357146263122559
Validation loss: 2.1817001296627905

Epoch: 6| Step: 10
Training loss: 1.4548758268356323
Validation loss: 2.1881352598949144

Epoch: 6| Step: 11
Training loss: 2.3870203495025635
Validation loss: 2.1872087832420104

Epoch: 6| Step: 12
Training loss: 3.345113754272461
Validation loss: 2.180940064050818

Epoch: 6| Step: 13
Training loss: 3.167963981628418
Validation loss: 2.190153293712165

Epoch: 128| Step: 0
Training loss: 1.905177354812622
Validation loss: 2.1855656921222644

Epoch: 6| Step: 1
Training loss: 2.3842825889587402
Validation loss: 2.170985157771777

Epoch: 6| Step: 2
Training loss: 2.613859176635742
Validation loss: 2.1748511534865185

Epoch: 6| Step: 3
Training loss: 2.5686633586883545
Validation loss: 2.172684329812245

Epoch: 6| Step: 4
Training loss: 1.9775538444519043
Validation loss: 2.163970919065578

Epoch: 6| Step: 5
Training loss: 1.8352748155593872
Validation loss: 2.1620163199722127

Epoch: 6| Step: 6
Training loss: 1.753298282623291
Validation loss: 2.1648090834258706

Epoch: 6| Step: 7
Training loss: 2.1225333213806152
Validation loss: 2.1794220568031393

Epoch: 6| Step: 8
Training loss: 2.7869009971618652
Validation loss: 2.1913904349009194

Epoch: 6| Step: 9
Training loss: 2.298682689666748
Validation loss: 2.2094007179301274

Epoch: 6| Step: 10
Training loss: 3.2788119316101074
Validation loss: 2.2241737637468564

Epoch: 6| Step: 11
Training loss: 3.2718820571899414
Validation loss: 2.2306221223646596

Epoch: 6| Step: 12
Training loss: 2.9132583141326904
Validation loss: 2.2424558516471618

Epoch: 6| Step: 13
Training loss: 2.948306083679199
Validation loss: 2.240834028490128

Epoch: 129| Step: 0
Training loss: 2.813570976257324
Validation loss: 2.2386703439938125

Epoch: 6| Step: 1
Training loss: 3.1220576763153076
Validation loss: 2.215784618931432

Epoch: 6| Step: 2
Training loss: 3.0364956855773926
Validation loss: 2.206746075742988

Epoch: 6| Step: 3
Training loss: 1.8935574293136597
Validation loss: 2.1945580872156287

Epoch: 6| Step: 4
Training loss: 2.0286855697631836
Validation loss: 2.1985488937747095

Epoch: 6| Step: 5
Training loss: 2.5115649700164795
Validation loss: 2.1885431453745854

Epoch: 6| Step: 6
Training loss: 2.2036781311035156
Validation loss: 2.186293832717403

Epoch: 6| Step: 7
Training loss: 2.5832927227020264
Validation loss: 2.1867409726624847

Epoch: 6| Step: 8
Training loss: 1.7532376050949097
Validation loss: 2.1785831477052424

Epoch: 6| Step: 9
Training loss: 3.10726261138916
Validation loss: 2.1781834710028862

Epoch: 6| Step: 10
Training loss: 2.154963970184326
Validation loss: 2.1810112217421174

Epoch: 6| Step: 11
Training loss: 2.667081832885742
Validation loss: 2.1707925617053943

Epoch: 6| Step: 12
Training loss: 2.125990867614746
Validation loss: 2.1776958511721705

Epoch: 6| Step: 13
Training loss: 2.2959959506988525
Validation loss: 2.16531305415656

Epoch: 130| Step: 0
Training loss: 1.7617125511169434
Validation loss: 2.1615933936129332

Epoch: 6| Step: 1
Training loss: 1.6781551837921143
Validation loss: 2.166675249735514

Epoch: 6| Step: 2
Training loss: 2.4978103637695312
Validation loss: 2.19171650948063

Epoch: 6| Step: 3
Training loss: 2.2380361557006836
Validation loss: 2.203077168874843

Epoch: 6| Step: 4
Training loss: 2.25099515914917
Validation loss: 2.209734606486495

Epoch: 6| Step: 5
Training loss: 2.826364517211914
Validation loss: 2.1945465969783005

Epoch: 6| Step: 6
Training loss: 2.8078320026397705
Validation loss: 2.187313682289534

Epoch: 6| Step: 7
Training loss: 2.438479423522949
Validation loss: 2.168419058604907

Epoch: 6| Step: 8
Training loss: 2.5844154357910156
Validation loss: 2.1723987812637002

Epoch: 6| Step: 9
Training loss: 2.704906940460205
Validation loss: 2.1638194258495043

Epoch: 6| Step: 10
Training loss: 2.653254985809326
Validation loss: 2.171673249172908

Epoch: 6| Step: 11
Training loss: 2.2736401557922363
Validation loss: 2.167987561994983

Epoch: 6| Step: 12
Training loss: 2.7776124477386475
Validation loss: 2.1809329217480076

Epoch: 6| Step: 13
Training loss: 3.322458505630493
Validation loss: 2.200130976656432

Epoch: 131| Step: 0
Training loss: 2.8649449348449707
Validation loss: 2.2035133595107705

Epoch: 6| Step: 1
Training loss: 2.219216823577881
Validation loss: 2.2126768506983274

Epoch: 6| Step: 2
Training loss: 2.4568228721618652
Validation loss: 2.2105682639665503

Epoch: 6| Step: 3
Training loss: 2.0077133178710938
Validation loss: 2.2175911972599645

Epoch: 6| Step: 4
Training loss: 2.052309513092041
Validation loss: 2.190924363751565

Epoch: 6| Step: 5
Training loss: 2.732306957244873
Validation loss: 2.194432261169598

Epoch: 6| Step: 6
Training loss: 3.1346185207366943
Validation loss: 2.1900478242546

Epoch: 6| Step: 7
Training loss: 1.8864622116088867
Validation loss: 2.188607854227866

Epoch: 6| Step: 8
Training loss: 2.8175251483917236
Validation loss: 2.195556604734031

Epoch: 6| Step: 9
Training loss: 1.937208652496338
Validation loss: 2.1954021556403047

Epoch: 6| Step: 10
Training loss: 2.8339996337890625
Validation loss: 2.2081338180008756

Epoch: 6| Step: 11
Training loss: 2.18867564201355
Validation loss: 2.224303509599419

Epoch: 6| Step: 12
Training loss: 3.1206674575805664
Validation loss: 2.2381011080998245

Epoch: 6| Step: 13
Training loss: 1.6978861093521118
Validation loss: 2.2366048264247116

Epoch: 132| Step: 0
Training loss: 2.6437549591064453
Validation loss: 2.239367320973386

Epoch: 6| Step: 1
Training loss: 2.668463706970215
Validation loss: 2.2161586297455655

Epoch: 6| Step: 2
Training loss: 2.521174430847168
Validation loss: 2.195387363433838

Epoch: 6| Step: 3
Training loss: 1.4871666431427002
Validation loss: 2.1973983228847547

Epoch: 6| Step: 4
Training loss: 2.5864996910095215
Validation loss: 2.1829080914938324

Epoch: 6| Step: 5
Training loss: 2.787332057952881
Validation loss: 2.1950326273518224

Epoch: 6| Step: 6
Training loss: 2.568660020828247
Validation loss: 2.1993280380002913

Epoch: 6| Step: 7
Training loss: 2.797281265258789
Validation loss: 2.194218199740174

Epoch: 6| Step: 8
Training loss: 2.3077335357666016
Validation loss: 2.181788908537998

Epoch: 6| Step: 9
Training loss: 2.298837184906006
Validation loss: 2.1734618397169214

Epoch: 6| Step: 10
Training loss: 1.952146291732788
Validation loss: 2.1681532834165838

Epoch: 6| Step: 11
Training loss: 2.781050443649292
Validation loss: 2.1609415443994666

Epoch: 6| Step: 12
Training loss: 2.6614785194396973
Validation loss: 2.1631848581375612

Epoch: 6| Step: 13
Training loss: 2.3147876262664795
Validation loss: 2.169581961888139

Epoch: 133| Step: 0
Training loss: 2.434131145477295
Validation loss: 2.208763148195

Epoch: 6| Step: 1
Training loss: 2.482781410217285
Validation loss: 2.2598025670615574

Epoch: 6| Step: 2
Training loss: 2.867276668548584
Validation loss: 2.2515507808295627

Epoch: 6| Step: 3
Training loss: 2.9404945373535156
Validation loss: 2.2360082313578618

Epoch: 6| Step: 4
Training loss: 2.1816701889038086
Validation loss: 2.203437646230062

Epoch: 6| Step: 5
Training loss: 2.6377179622650146
Validation loss: 2.1998820279234197

Epoch: 6| Step: 6
Training loss: 2.180410861968994
Validation loss: 2.1740903521096833

Epoch: 6| Step: 7
Training loss: 2.956350803375244
Validation loss: 2.1659340820004864

Epoch: 6| Step: 8
Training loss: 2.69553279876709
Validation loss: 2.1606379580754105

Epoch: 6| Step: 9
Training loss: 2.125476121902466
Validation loss: 2.163199950289983

Epoch: 6| Step: 10
Training loss: 1.6814298629760742
Validation loss: 2.1619240930003505

Epoch: 6| Step: 11
Training loss: 2.3838891983032227
Validation loss: 2.167282788984237

Epoch: 6| Step: 12
Training loss: 2.3498806953430176
Validation loss: 2.1796472675056866

Epoch: 6| Step: 13
Training loss: 2.4658703804016113
Validation loss: 2.1808211982891126

Epoch: 134| Step: 0
Training loss: 2.5314550399780273
Validation loss: 2.1977490891692457

Epoch: 6| Step: 1
Training loss: 2.8778076171875
Validation loss: 2.2193156519243793

Epoch: 6| Step: 2
Training loss: 1.7296545505523682
Validation loss: 2.212550195314551

Epoch: 6| Step: 3
Training loss: 2.213076591491699
Validation loss: 2.1697031900446904

Epoch: 6| Step: 4
Training loss: 2.3592147827148438
Validation loss: 2.1501751586955082

Epoch: 6| Step: 5
Training loss: 2.2433433532714844
Validation loss: 2.1483173113997265

Epoch: 6| Step: 6
Training loss: 1.9464480876922607
Validation loss: 2.161178165866483

Epoch: 6| Step: 7
Training loss: 2.0409932136535645
Validation loss: 2.168097239668651

Epoch: 6| Step: 8
Training loss: 2.6000936031341553
Validation loss: 2.1784883776018695

Epoch: 6| Step: 9
Training loss: 3.3742666244506836
Validation loss: 2.1858277320861816

Epoch: 6| Step: 10
Training loss: 2.3007102012634277
Validation loss: 2.1871654974517

Epoch: 6| Step: 11
Training loss: 2.6424732208251953
Validation loss: 2.180907633996779

Epoch: 6| Step: 12
Training loss: 2.664064407348633
Validation loss: 2.1784310007608063

Epoch: 6| Step: 13
Training loss: 3.4823355674743652
Validation loss: 2.1937794839182208

Epoch: 135| Step: 0
Training loss: 2.6442131996154785
Validation loss: 2.2235887024992254

Epoch: 6| Step: 1
Training loss: 2.399512767791748
Validation loss: 2.267138209394229

Epoch: 6| Step: 2
Training loss: 2.2625253200531006
Validation loss: 2.3198368357073877

Epoch: 6| Step: 3
Training loss: 2.243609666824341
Validation loss: 2.344609514359505

Epoch: 6| Step: 4
Training loss: 2.85691237449646
Validation loss: 2.3278944825613372

Epoch: 6| Step: 5
Training loss: 1.996508240699768
Validation loss: 2.307233113114552

Epoch: 6| Step: 6
Training loss: 2.403001308441162
Validation loss: 2.278022430276358

Epoch: 6| Step: 7
Training loss: 2.8210089206695557
Validation loss: 2.277514765339513

Epoch: 6| Step: 8
Training loss: 2.1298184394836426
Validation loss: 2.2861830137109243

Epoch: 6| Step: 9
Training loss: 2.6429851055145264
Validation loss: 2.3025931119918823

Epoch: 6| Step: 10
Training loss: 2.6632862091064453
Validation loss: 2.3110754976990404

Epoch: 6| Step: 11
Training loss: 2.2622697353363037
Validation loss: 2.311742910774805

Epoch: 6| Step: 12
Training loss: 2.7883682250976562
Validation loss: 2.2717054185046943

Epoch: 6| Step: 13
Training loss: 2.769597053527832
Validation loss: 2.2359097362846456

Epoch: 136| Step: 0
Training loss: 1.842841625213623
Validation loss: 2.2039526841973744

Epoch: 6| Step: 1
Training loss: 2.3116092681884766
Validation loss: 2.199838281959616

Epoch: 6| Step: 2
Training loss: 3.359437942504883
Validation loss: 2.1875352910769883

Epoch: 6| Step: 3
Training loss: 2.325439453125
Validation loss: 2.1730395235041136

Epoch: 6| Step: 4
Training loss: 2.071488380432129
Validation loss: 2.1763719486933883

Epoch: 6| Step: 5
Training loss: 2.5761423110961914
Validation loss: 2.1689975876961984

Epoch: 6| Step: 6
Training loss: 2.078650712966919
Validation loss: 2.167477830763786

Epoch: 6| Step: 7
Training loss: 2.9432528018951416
Validation loss: 2.1715447723224597

Epoch: 6| Step: 8
Training loss: 2.6188578605651855
Validation loss: 2.1700443208858533

Epoch: 6| Step: 9
Training loss: 1.7700923681259155
Validation loss: 2.1787050129264913

Epoch: 6| Step: 10
Training loss: 3.0693821907043457
Validation loss: 2.16453525071503

Epoch: 6| Step: 11
Training loss: 2.7564005851745605
Validation loss: 2.1693570024223736

Epoch: 6| Step: 12
Training loss: 2.5970466136932373
Validation loss: 2.1745331338656846

Epoch: 6| Step: 13
Training loss: 1.572872519493103
Validation loss: 2.1668027267661145

Epoch: 137| Step: 0
Training loss: 2.9344117641448975
Validation loss: 2.185999988227762

Epoch: 6| Step: 1
Training loss: 1.973044753074646
Validation loss: 2.208433961355558

Epoch: 6| Step: 2
Training loss: 2.0643656253814697
Validation loss: 2.2165957343193794

Epoch: 6| Step: 3
Training loss: 1.5510427951812744
Validation loss: 2.248749548389066

Epoch: 6| Step: 4
Training loss: 3.03690505027771
Validation loss: 2.2710152531182892

Epoch: 6| Step: 5
Training loss: 2.3359475135803223
Validation loss: 2.2819619947864163

Epoch: 6| Step: 6
Training loss: 1.7272288799285889
Validation loss: 2.272516340337774

Epoch: 6| Step: 7
Training loss: 2.7515785694122314
Validation loss: 2.288690044033912

Epoch: 6| Step: 8
Training loss: 2.479677438735962
Validation loss: 2.284226658523724

Epoch: 6| Step: 9
Training loss: 2.690865993499756
Validation loss: 2.27337142216262

Epoch: 6| Step: 10
Training loss: 2.3893914222717285
Validation loss: 2.269782922601187

Epoch: 6| Step: 11
Training loss: 2.9084439277648926
Validation loss: 2.250867020699286

Epoch: 6| Step: 12
Training loss: 2.6033825874328613
Validation loss: 2.237319113105856

Epoch: 6| Step: 13
Training loss: 2.5386955738067627
Validation loss: 2.2161743538354033

Epoch: 138| Step: 0
Training loss: 2.274364471435547
Validation loss: 2.192233507351209

Epoch: 6| Step: 1
Training loss: 2.3596932888031006
Validation loss: 2.1918266306641283

Epoch: 6| Step: 2
Training loss: 2.8348023891448975
Validation loss: 2.195137681499604

Epoch: 6| Step: 3
Training loss: 2.9668452739715576
Validation loss: 2.194730999649212

Epoch: 6| Step: 4
Training loss: 2.906219959259033
Validation loss: 2.2029904550121677

Epoch: 6| Step: 5
Training loss: 2.005298137664795
Validation loss: 2.2040520457811255

Epoch: 6| Step: 6
Training loss: 2.2960681915283203
Validation loss: 2.20992874586454

Epoch: 6| Step: 7
Training loss: 1.8208942413330078
Validation loss: 2.2072290182113647

Epoch: 6| Step: 8
Training loss: 2.4024860858917236
Validation loss: 2.2085982291929183

Epoch: 6| Step: 9
Training loss: 2.586822509765625
Validation loss: 2.193742170128771

Epoch: 6| Step: 10
Training loss: 1.938460350036621
Validation loss: 2.190270085488596

Epoch: 6| Step: 11
Training loss: 2.976137161254883
Validation loss: 2.198619650256249

Epoch: 6| Step: 12
Training loss: 1.9440498352050781
Validation loss: 2.190521369698227

Epoch: 6| Step: 13
Training loss: 2.863572120666504
Validation loss: 2.1522789885920863

Epoch: 139| Step: 0
Training loss: 2.4398369789123535
Validation loss: 2.138458744172127

Epoch: 6| Step: 1
Training loss: 2.4549458026885986
Validation loss: 2.143012441614623

Epoch: 6| Step: 2
Training loss: 2.553436040878296
Validation loss: 2.1493518326872136

Epoch: 6| Step: 3
Training loss: 2.529507875442505
Validation loss: 2.166662267459336

Epoch: 6| Step: 4
Training loss: 2.353609085083008
Validation loss: 2.155657079912001

Epoch: 6| Step: 5
Training loss: 2.6188831329345703
Validation loss: 2.1634275323601178

Epoch: 6| Step: 6
Training loss: 2.91845965385437
Validation loss: 2.1812997582138225

Epoch: 6| Step: 7
Training loss: 2.17879581451416
Validation loss: 2.1893317981432845

Epoch: 6| Step: 8
Training loss: 2.9619340896606445
Validation loss: 2.200580386705296

Epoch: 6| Step: 9
Training loss: 2.1051344871520996
Validation loss: 2.2128973750657934

Epoch: 6| Step: 10
Training loss: 2.715902328491211
Validation loss: 2.2294565375133226

Epoch: 6| Step: 11
Training loss: 1.5286238193511963
Validation loss: 2.2449199614986295

Epoch: 6| Step: 12
Training loss: 2.2883424758911133
Validation loss: 2.2499454405999955

Epoch: 6| Step: 13
Training loss: 2.8253355026245117
Validation loss: 2.2796667109253588

Epoch: 140| Step: 0
Training loss: 2.3697566986083984
Validation loss: 2.255219567206598

Epoch: 6| Step: 1
Training loss: 2.4278337955474854
Validation loss: 2.219856507034712

Epoch: 6| Step: 2
Training loss: 2.9311022758483887
Validation loss: 2.211986921166861

Epoch: 6| Step: 3
Training loss: 2.942821979522705
Validation loss: 2.196981037816694

Epoch: 6| Step: 4
Training loss: 1.7938251495361328
Validation loss: 2.1922769367053943

Epoch: 6| Step: 5
Training loss: 3.0825116634368896
Validation loss: 2.1836963263891076

Epoch: 6| Step: 6
Training loss: 2.500589370727539
Validation loss: 2.1799172534737536

Epoch: 6| Step: 7
Training loss: 2.699767827987671
Validation loss: 2.1707183519999185

Epoch: 6| Step: 8
Training loss: 1.8275787830352783
Validation loss: 2.1701766060244654

Epoch: 6| Step: 9
Training loss: 1.9754514694213867
Validation loss: 2.1707288347264773

Epoch: 6| Step: 10
Training loss: 2.647010326385498
Validation loss: 2.1585526620188067

Epoch: 6| Step: 11
Training loss: 1.8815674781799316
Validation loss: 2.1598376304872575

Epoch: 6| Step: 12
Training loss: 1.9846522808074951
Validation loss: 2.165989578411143

Epoch: 6| Step: 13
Training loss: 3.1443142890930176
Validation loss: 2.185267739398505

Epoch: 141| Step: 0
Training loss: 2.1264445781707764
Validation loss: 2.2216879462683075

Epoch: 6| Step: 1
Training loss: 2.0212650299072266
Validation loss: 2.2508719326347433

Epoch: 6| Step: 2
Training loss: 1.9005496501922607
Validation loss: 2.2621521334494314

Epoch: 6| Step: 3
Training loss: 2.218714714050293
Validation loss: 2.2840409176324004

Epoch: 6| Step: 4
Training loss: 3.1682841777801514
Validation loss: 2.3023408805170367

Epoch: 6| Step: 5
Training loss: 2.9807510375976562
Validation loss: 2.306269548272574

Epoch: 6| Step: 6
Training loss: 2.1478025913238525
Validation loss: 2.31811854147142

Epoch: 6| Step: 7
Training loss: 2.5002565383911133
Validation loss: 2.2887979220318537

Epoch: 6| Step: 8
Training loss: 2.6095733642578125
Validation loss: 2.2868925422750492

Epoch: 6| Step: 9
Training loss: 2.280770778656006
Validation loss: 2.2728667361761934

Epoch: 6| Step: 10
Training loss: 1.7908759117126465
Validation loss: 2.2531582475990377

Epoch: 6| Step: 11
Training loss: 2.802098274230957
Validation loss: 2.2524253322232153

Epoch: 6| Step: 12
Training loss: 2.583756446838379
Validation loss: 2.23620024547782

Epoch: 6| Step: 13
Training loss: 2.9641451835632324
Validation loss: 2.2173006508940007

Epoch: 142| Step: 0
Training loss: 2.267240047454834
Validation loss: 2.1967907720996487

Epoch: 6| Step: 1
Training loss: 3.1157946586608887
Validation loss: 2.198307301408501

Epoch: 6| Step: 2
Training loss: 2.369434118270874
Validation loss: 2.192158550344488

Epoch: 6| Step: 3
Training loss: 2.6440720558166504
Validation loss: 2.1991177528135237

Epoch: 6| Step: 4
Training loss: 1.9947952032089233
Validation loss: 2.202358950850784

Epoch: 6| Step: 5
Training loss: 2.6233205795288086
Validation loss: 2.1931757709031463

Epoch: 6| Step: 6
Training loss: 2.4707956314086914
Validation loss: 2.2043752055014334

Epoch: 6| Step: 7
Training loss: 2.51111102104187
Validation loss: 2.214873675377138

Epoch: 6| Step: 8
Training loss: 2.9085195064544678
Validation loss: 2.2577057974312895

Epoch: 6| Step: 9
Training loss: 2.4437265396118164
Validation loss: 2.2487487664786716

Epoch: 6| Step: 10
Training loss: 2.0652408599853516
Validation loss: 2.2168967993028703

Epoch: 6| Step: 11
Training loss: 2.212991714477539
Validation loss: 2.18333077174361

Epoch: 6| Step: 12
Training loss: 2.174053430557251
Validation loss: 2.148254020239717

Epoch: 6| Step: 13
Training loss: 1.7835911512374878
Validation loss: 2.156115952358451

Epoch: 143| Step: 0
Training loss: 1.835497498512268
Validation loss: 2.1520828380379626

Epoch: 6| Step: 1
Training loss: 2.4918437004089355
Validation loss: 2.174405359452771

Epoch: 6| Step: 2
Training loss: 2.5143210887908936
Validation loss: 2.175897864885228

Epoch: 6| Step: 3
Training loss: 2.3805856704711914
Validation loss: 2.176686870154514

Epoch: 6| Step: 4
Training loss: 2.4733657836914062
Validation loss: 2.1809070597412767

Epoch: 6| Step: 5
Training loss: 2.3852500915527344
Validation loss: 2.164545105349633

Epoch: 6| Step: 6
Training loss: 2.570341110229492
Validation loss: 2.154959927323044

Epoch: 6| Step: 7
Training loss: 2.3129496574401855
Validation loss: 2.1343527942575435

Epoch: 6| Step: 8
Training loss: 2.5798277854919434
Validation loss: 2.127917571734357

Epoch: 6| Step: 9
Training loss: 3.0955166816711426
Validation loss: 2.1304980606161137

Epoch: 6| Step: 10
Training loss: 2.550149917602539
Validation loss: 2.1630507746050434

Epoch: 6| Step: 11
Training loss: 3.0822129249572754
Validation loss: 2.1708535686615975

Epoch: 6| Step: 12
Training loss: 1.6542925834655762
Validation loss: 2.1830747435169835

Epoch: 6| Step: 13
Training loss: 2.4885284900665283
Validation loss: 2.1922940002974642

Epoch: 144| Step: 0
Training loss: 2.3472094535827637
Validation loss: 2.252005313032417

Epoch: 6| Step: 1
Training loss: 2.4904298782348633
Validation loss: 2.286697074931155

Epoch: 6| Step: 2
Training loss: 2.645453453063965
Validation loss: 2.2965658326302805

Epoch: 6| Step: 3
Training loss: 2.2106881141662598
Validation loss: 2.290333876045801

Epoch: 6| Step: 4
Training loss: 2.767587661743164
Validation loss: 2.320959242441321

Epoch: 6| Step: 5
Training loss: 1.5344901084899902
Validation loss: 2.2953153271828928

Epoch: 6| Step: 6
Training loss: 2.683375358581543
Validation loss: 2.3017536312021236

Epoch: 6| Step: 7
Training loss: 2.652045726776123
Validation loss: 2.293568052271361

Epoch: 6| Step: 8
Training loss: 1.9737939834594727
Validation loss: 2.2561309286343154

Epoch: 6| Step: 9
Training loss: 2.571462631225586
Validation loss: 2.2544504083612913

Epoch: 6| Step: 10
Training loss: 2.279390811920166
Validation loss: 2.2215450079210344

Epoch: 6| Step: 11
Training loss: 2.821559429168701
Validation loss: 2.2092300820094284

Epoch: 6| Step: 12
Training loss: 2.69197416305542
Validation loss: 2.189989343766243

Epoch: 6| Step: 13
Training loss: 1.8326058387756348
Validation loss: 2.201267488541142

Epoch: 145| Step: 0
Training loss: 2.9754319190979004
Validation loss: 2.1830037281077397

Epoch: 6| Step: 1
Training loss: 2.8033008575439453
Validation loss: 2.1658263206481934

Epoch: 6| Step: 2
Training loss: 2.5147829055786133
Validation loss: 2.160138245551817

Epoch: 6| Step: 3
Training loss: 2.77801775932312
Validation loss: 2.1703571093979703

Epoch: 6| Step: 4
Training loss: 1.6608245372772217
Validation loss: 2.1474032402038574

Epoch: 6| Step: 5
Training loss: 2.638725996017456
Validation loss: 2.146322214475242

Epoch: 6| Step: 6
Training loss: 1.4616246223449707
Validation loss: 2.1489063642358266

Epoch: 6| Step: 7
Training loss: 2.5984506607055664
Validation loss: 2.1437079598826747

Epoch: 6| Step: 8
Training loss: 2.5901408195495605
Validation loss: 2.132673135367773

Epoch: 6| Step: 9
Training loss: 2.5628347396850586
Validation loss: 2.1437103568866687

Epoch: 6| Step: 10
Training loss: 1.9692522287368774
Validation loss: 2.155366782219179

Epoch: 6| Step: 11
Training loss: 2.3860554695129395
Validation loss: 2.1711866240347586

Epoch: 6| Step: 12
Training loss: 2.4449703693389893
Validation loss: 2.206912008664941

Epoch: 6| Step: 13
Training loss: 2.0668845176696777
Validation loss: 2.24553192815473

Epoch: 146| Step: 0
Training loss: 2.444566249847412
Validation loss: 2.342556979066582

Epoch: 6| Step: 1
Training loss: 2.8553857803344727
Validation loss: 2.4615908309977543

Epoch: 6| Step: 2
Training loss: 2.7057294845581055
Validation loss: 2.546347015647478

Epoch: 6| Step: 3
Training loss: 3.1233460903167725
Validation loss: 2.5813665056741364

Epoch: 6| Step: 4
Training loss: 3.1437973976135254
Validation loss: 2.585022800712175

Epoch: 6| Step: 5
Training loss: 1.6342365741729736
Validation loss: 2.494680304681101

Epoch: 6| Step: 6
Training loss: 1.973957896232605
Validation loss: 2.409206723654142

Epoch: 6| Step: 7
Training loss: 2.599943161010742
Validation loss: 2.3376221451708066

Epoch: 6| Step: 8
Training loss: 2.207460880279541
Validation loss: 2.2679348376489457

Epoch: 6| Step: 9
Training loss: 2.377995729446411
Validation loss: 2.2382984648468676

Epoch: 6| Step: 10
Training loss: 2.074922561645508
Validation loss: 2.202876208930887

Epoch: 6| Step: 11
Training loss: 2.9094300270080566
Validation loss: 2.209783977077853

Epoch: 6| Step: 12
Training loss: 3.0440306663513184
Validation loss: 2.2194928328196206

Epoch: 6| Step: 13
Training loss: 1.63709557056427
Validation loss: 2.227878344956265

Epoch: 147| Step: 0
Training loss: 2.7567296028137207
Validation loss: 2.23656198029877

Epoch: 6| Step: 1
Training loss: 2.5440831184387207
Validation loss: 2.2230487408176547

Epoch: 6| Step: 2
Training loss: 2.7316174507141113
Validation loss: 2.210267415610693

Epoch: 6| Step: 3
Training loss: 3.5032215118408203
Validation loss: 2.2043084918811755

Epoch: 6| Step: 4
Training loss: 1.7559301853179932
Validation loss: 2.186131477355957

Epoch: 6| Step: 5
Training loss: 2.0966169834136963
Validation loss: 2.1631858912847375

Epoch: 6| Step: 6
Training loss: 1.7744863033294678
Validation loss: 2.1442351289974746

Epoch: 6| Step: 7
Training loss: 2.888418436050415
Validation loss: 2.139322034774288

Epoch: 6| Step: 8
Training loss: 2.733565330505371
Validation loss: 2.1494018570069344

Epoch: 6| Step: 9
Training loss: 1.5398857593536377
Validation loss: 2.1622219829149145

Epoch: 6| Step: 10
Training loss: 2.2931666374206543
Validation loss: 2.1764525828822965

Epoch: 6| Step: 11
Training loss: 2.418687582015991
Validation loss: 2.2099466272579726

Epoch: 6| Step: 12
Training loss: 2.7208006381988525
Validation loss: 2.2734783593044487

Epoch: 6| Step: 13
Training loss: 2.29170823097229
Validation loss: 2.322753188430622

Epoch: 148| Step: 0
Training loss: 2.3179891109466553
Validation loss: 2.343130039912398

Epoch: 6| Step: 1
Training loss: 2.382068157196045
Validation loss: 2.298455512651833

Epoch: 6| Step: 2
Training loss: 2.7231788635253906
Validation loss: 2.250865226150841

Epoch: 6| Step: 3
Training loss: 2.0784120559692383
Validation loss: 2.1882452605873026

Epoch: 6| Step: 4
Training loss: 1.8183228969573975
Validation loss: 2.169647532124673

Epoch: 6| Step: 5
Training loss: 2.752000093460083
Validation loss: 2.1564651868676625

Epoch: 6| Step: 6
Training loss: 2.7448956966400146
Validation loss: 2.165773130232288

Epoch: 6| Step: 7
Training loss: 2.121647834777832
Validation loss: 2.178832671975577

Epoch: 6| Step: 8
Training loss: 1.954053282737732
Validation loss: 2.1948224088197112

Epoch: 6| Step: 9
Training loss: 2.0291709899902344
Validation loss: 2.21665354441571

Epoch: 6| Step: 10
Training loss: 2.142211437225342
Validation loss: 2.228419934549639

Epoch: 6| Step: 11
Training loss: 2.7954978942871094
Validation loss: 2.228440782075287

Epoch: 6| Step: 12
Training loss: 3.3610401153564453
Validation loss: 2.2585689790787233

Epoch: 6| Step: 13
Training loss: 3.49298357963562
Validation loss: 2.2600325563902497

Epoch: 149| Step: 0
Training loss: 2.043887138366699
Validation loss: 2.256324486065936

Epoch: 6| Step: 1
Training loss: 2.415696620941162
Validation loss: 2.2631590750909623

Epoch: 6| Step: 2
Training loss: 2.763731002807617
Validation loss: 2.268320055418117

Epoch: 6| Step: 3
Training loss: 2.466808319091797
Validation loss: 2.2726573277545232

Epoch: 6| Step: 4
Training loss: 3.000426769256592
Validation loss: 2.277837109822099

Epoch: 6| Step: 5
Training loss: 2.2850208282470703
Validation loss: 2.2719336120031213

Epoch: 6| Step: 6
Training loss: 1.9800156354904175
Validation loss: 2.234521442844022

Epoch: 6| Step: 7
Training loss: 2.0777807235717773
Validation loss: 2.19636369264254

Epoch: 6| Step: 8
Training loss: 1.8877114057540894
Validation loss: 2.205423402529891

Epoch: 6| Step: 9
Training loss: 2.9375104904174805
Validation loss: 2.204625847519085

Epoch: 6| Step: 10
Training loss: 2.0970711708068848
Validation loss: 2.220316729237956

Epoch: 6| Step: 11
Training loss: 2.556148052215576
Validation loss: 2.2081310697781142

Epoch: 6| Step: 12
Training loss: 2.527284622192383
Validation loss: 2.2265748952024724

Epoch: 6| Step: 13
Training loss: 2.6260104179382324
Validation loss: 2.231378032315162

Epoch: 150| Step: 0
Training loss: 1.751976728439331
Validation loss: 2.2516725473506476

Epoch: 6| Step: 1
Training loss: 2.3476784229278564
Validation loss: 2.2614061576063915

Epoch: 6| Step: 2
Training loss: 2.6663424968719482
Validation loss: 2.2845137619203135

Epoch: 6| Step: 3
Training loss: 2.488905429840088
Validation loss: 2.3083179086767216

Epoch: 6| Step: 4
Training loss: 1.8724905252456665
Validation loss: 2.2752806268712527

Epoch: 6| Step: 5
Training loss: 2.3284008502960205
Validation loss: 2.2584296272646998

Epoch: 6| Step: 6
Training loss: 1.843492031097412
Validation loss: 2.206232729778495

Epoch: 6| Step: 7
Training loss: 2.826209783554077
Validation loss: 2.184011661878196

Epoch: 6| Step: 8
Training loss: 2.384655475616455
Validation loss: 2.1631311742208337

Epoch: 6| Step: 9
Training loss: 1.6144123077392578
Validation loss: 2.15474114366757

Epoch: 6| Step: 10
Training loss: 2.713913917541504
Validation loss: 2.1586599760158087

Epoch: 6| Step: 11
Training loss: 3.0297183990478516
Validation loss: 2.1543468275377826

Epoch: 6| Step: 12
Training loss: 2.8943405151367188
Validation loss: 2.1547446199642715

Epoch: 6| Step: 13
Training loss: 2.7205872535705566
Validation loss: 2.1610443386980283

Epoch: 151| Step: 0
Training loss: 2.3670461177825928
Validation loss: 2.1620500728648198

Epoch: 6| Step: 1
Training loss: 1.8025426864624023
Validation loss: 2.1843155404572845

Epoch: 6| Step: 2
Training loss: 1.852655291557312
Validation loss: 2.1826498046998055

Epoch: 6| Step: 3
Training loss: 2.110881805419922
Validation loss: 2.1961837148153656

Epoch: 6| Step: 4
Training loss: 2.3282852172851562
Validation loss: 2.209032534271158

Epoch: 6| Step: 5
Training loss: 2.5359506607055664
Validation loss: 2.2448894849387546

Epoch: 6| Step: 6
Training loss: 2.6280202865600586
Validation loss: 2.256238611795569

Epoch: 6| Step: 7
Training loss: 1.9880330562591553
Validation loss: 2.2649793086513395

Epoch: 6| Step: 8
Training loss: 2.5774900913238525
Validation loss: 2.2745485972332697

Epoch: 6| Step: 9
Training loss: 3.0733108520507812
Validation loss: 2.2758995948299283

Epoch: 6| Step: 10
Training loss: 2.9660751819610596
Validation loss: 2.2717951523360385

Epoch: 6| Step: 11
Training loss: 2.695061683654785
Validation loss: 2.23614699609818

Epoch: 6| Step: 12
Training loss: 2.677464008331299
Validation loss: 2.2195600707043885

Epoch: 6| Step: 13
Training loss: 1.7555166482925415
Validation loss: 2.203241039347905

Epoch: 152| Step: 0
Training loss: 2.1357643604278564
Validation loss: 2.1747668020186888

Epoch: 6| Step: 1
Training loss: 1.8048021793365479
Validation loss: 2.161785356460079

Epoch: 6| Step: 2
Training loss: 2.789640426635742
Validation loss: 2.157681731767552

Epoch: 6| Step: 3
Training loss: 2.4338393211364746
Validation loss: 2.139805878362348

Epoch: 6| Step: 4
Training loss: 2.8020355701446533
Validation loss: 2.1377444292909358

Epoch: 6| Step: 5
Training loss: 2.648473024368286
Validation loss: 2.150344387177498

Epoch: 6| Step: 6
Training loss: 2.1491918563842773
Validation loss: 2.1415626618169967

Epoch: 6| Step: 7
Training loss: 2.6895792484283447
Validation loss: 2.141543319148402

Epoch: 6| Step: 8
Training loss: 1.419600486755371
Validation loss: 2.1540726564263784

Epoch: 6| Step: 9
Training loss: 2.6407551765441895
Validation loss: 2.150880212424904

Epoch: 6| Step: 10
Training loss: 2.668455123901367
Validation loss: 2.1702885704655803

Epoch: 6| Step: 11
Training loss: 2.9143621921539307
Validation loss: 2.170082502467658

Epoch: 6| Step: 12
Training loss: 2.2960660457611084
Validation loss: 2.1743247534639094

Epoch: 6| Step: 13
Training loss: 1.4306055307388306
Validation loss: 2.181643424495574

Epoch: 153| Step: 0
Training loss: 2.255495071411133
Validation loss: 2.1871624121101956

Epoch: 6| Step: 1
Training loss: 2.0933308601379395
Validation loss: 2.217655639494619

Epoch: 6| Step: 2
Training loss: 2.3711490631103516
Validation loss: 2.2232962423755276

Epoch: 6| Step: 3
Training loss: 2.725778579711914
Validation loss: 2.2087912482600056

Epoch: 6| Step: 4
Training loss: 2.545621633529663
Validation loss: 2.226626580761325

Epoch: 6| Step: 5
Training loss: 2.232309341430664
Validation loss: 2.217151521354593

Epoch: 6| Step: 6
Training loss: 1.9931955337524414
Validation loss: 2.2154103222713677

Epoch: 6| Step: 7
Training loss: 2.8217673301696777
Validation loss: 2.206927445627028

Epoch: 6| Step: 8
Training loss: 2.0268964767456055
Validation loss: 2.203417783142418

Epoch: 6| Step: 9
Training loss: 2.747096300125122
Validation loss: 2.2021305971248175

Epoch: 6| Step: 10
Training loss: 1.4432423114776611
Validation loss: 2.2067551869218067

Epoch: 6| Step: 11
Training loss: 2.7578835487365723
Validation loss: 2.1906349069328717

Epoch: 6| Step: 12
Training loss: 2.4575798511505127
Validation loss: 2.189853440048874

Epoch: 6| Step: 13
Training loss: 2.4116921424865723
Validation loss: 2.1974085120744604

Epoch: 154| Step: 0
Training loss: 2.3840627670288086
Validation loss: 2.214586057970601

Epoch: 6| Step: 1
Training loss: 2.126646041870117
Validation loss: 2.2460600188983384

Epoch: 6| Step: 2
Training loss: 2.4009883403778076
Validation loss: 2.2708670349531275

Epoch: 6| Step: 3
Training loss: 2.55200457572937
Validation loss: 2.2989107844650105

Epoch: 6| Step: 4
Training loss: 2.4113714694976807
Validation loss: 2.290484514287723

Epoch: 6| Step: 5
Training loss: 2.986311912536621
Validation loss: 2.3020363007822344

Epoch: 6| Step: 6
Training loss: 2.207487106323242
Validation loss: 2.295600057930075

Epoch: 6| Step: 7
Training loss: 2.290961265563965
Validation loss: 2.252981755041307

Epoch: 6| Step: 8
Training loss: 2.0639052391052246
Validation loss: 2.2031674513252835

Epoch: 6| Step: 9
Training loss: 2.1479287147521973
Validation loss: 2.1657640857081257

Epoch: 6| Step: 10
Training loss: 2.6890177726745605
Validation loss: 2.146355869949505

Epoch: 6| Step: 11
Training loss: 1.7920951843261719
Validation loss: 2.146310511455741

Epoch: 6| Step: 12
Training loss: 2.231686592102051
Validation loss: 2.135642300369919

Epoch: 6| Step: 13
Training loss: 2.9674506187438965
Validation loss: 2.127661628107871

Epoch: 155| Step: 0
Training loss: 2.7038791179656982
Validation loss: 2.129321180364137

Epoch: 6| Step: 1
Training loss: 1.7674739360809326
Validation loss: 2.130319364609257

Epoch: 6| Step: 2
Training loss: 2.2813024520874023
Validation loss: 2.1361643088761197

Epoch: 6| Step: 3
Training loss: 2.9817357063293457
Validation loss: 2.129378581559786

Epoch: 6| Step: 4
Training loss: 1.6627002954483032
Validation loss: 2.1370139916737876

Epoch: 6| Step: 5
Training loss: 2.16247820854187
Validation loss: 2.150371338731499

Epoch: 6| Step: 6
Training loss: 2.4398062229156494
Validation loss: 2.1625730581181024

Epoch: 6| Step: 7
Training loss: 3.1732640266418457
Validation loss: 2.188972148843991

Epoch: 6| Step: 8
Training loss: 1.8958098888397217
Validation loss: 2.2013139750367854

Epoch: 6| Step: 9
Training loss: 2.051377296447754
Validation loss: 2.2191500458666074

Epoch: 6| Step: 10
Training loss: 2.370664596557617
Validation loss: 2.2403754034349994

Epoch: 6| Step: 11
Training loss: 3.086606740951538
Validation loss: 2.2440552788396038

Epoch: 6| Step: 12
Training loss: 2.100888252258301
Validation loss: 2.2338798584476596

Epoch: 6| Step: 13
Training loss: 2.518317461013794
Validation loss: 2.234305365111238

Epoch: 156| Step: 0
Training loss: 2.039607048034668
Validation loss: 2.213411408085977

Epoch: 6| Step: 1
Training loss: 2.4532527923583984
Validation loss: 2.202399794773389

Epoch: 6| Step: 2
Training loss: 2.2506980895996094
Validation loss: 2.193822835081367

Epoch: 6| Step: 3
Training loss: 2.6205222606658936
Validation loss: 2.1758695981835805

Epoch: 6| Step: 4
Training loss: 2.6371777057647705
Validation loss: 2.191779282785231

Epoch: 6| Step: 5
Training loss: 1.56504487991333
Validation loss: 2.191052193282753

Epoch: 6| Step: 6
Training loss: 2.6196587085723877
Validation loss: 2.206323289102124

Epoch: 6| Step: 7
Training loss: 2.1570816040039062
Validation loss: 2.2099075394292034

Epoch: 6| Step: 8
Training loss: 2.41428804397583
Validation loss: 2.2264789894062984

Epoch: 6| Step: 9
Training loss: 2.2634170055389404
Validation loss: 2.217986922110281

Epoch: 6| Step: 10
Training loss: 2.1733510494232178
Validation loss: 2.2270865491641465

Epoch: 6| Step: 11
Training loss: 2.5209860801696777
Validation loss: 2.2119847856542116

Epoch: 6| Step: 12
Training loss: 1.8808834552764893
Validation loss: 2.2153806160855036

Epoch: 6| Step: 13
Training loss: 3.5296969413757324
Validation loss: 2.215602162063763

Epoch: 157| Step: 0
Training loss: 2.2150042057037354
Validation loss: 2.2413532682644424

Epoch: 6| Step: 1
Training loss: 2.1773605346679688
Validation loss: 2.2407051670935845

Epoch: 6| Step: 2
Training loss: 2.948638916015625
Validation loss: 2.2465049553942937

Epoch: 6| Step: 3
Training loss: 2.002153158187866
Validation loss: 2.2265058307237524

Epoch: 6| Step: 4
Training loss: 2.1660990715026855
Validation loss: 2.2188118734667377

Epoch: 6| Step: 5
Training loss: 2.460819721221924
Validation loss: 2.209903511949765

Epoch: 6| Step: 6
Training loss: 3.035900592803955
Validation loss: 2.191015653712775

Epoch: 6| Step: 7
Training loss: 1.7619129419326782
Validation loss: 2.1791782250968357

Epoch: 6| Step: 8
Training loss: 2.4216957092285156
Validation loss: 2.170469989058792

Epoch: 6| Step: 9
Training loss: 2.4771556854248047
Validation loss: 2.16286261235514

Epoch: 6| Step: 10
Training loss: 2.0173492431640625
Validation loss: 2.162009149469355

Epoch: 6| Step: 11
Training loss: 2.4785516262054443
Validation loss: 2.158451892996347

Epoch: 6| Step: 12
Training loss: 2.2773492336273193
Validation loss: 2.161875670956027

Epoch: 6| Step: 13
Training loss: 1.796618938446045
Validation loss: 2.165590270873039

Epoch: 158| Step: 0
Training loss: 3.2979578971862793
Validation loss: 2.2124630507602485

Epoch: 6| Step: 1
Training loss: 2.948611259460449
Validation loss: 2.236567363944105

Epoch: 6| Step: 2
Training loss: 1.8733502626419067
Validation loss: 2.2665492232127855

Epoch: 6| Step: 3
Training loss: 2.2314562797546387
Validation loss: 2.3149233838563323

Epoch: 6| Step: 4
Training loss: 1.85434091091156
Validation loss: 2.355387010881978

Epoch: 6| Step: 5
Training loss: 1.390927791595459
Validation loss: 2.360883282076928

Epoch: 6| Step: 6
Training loss: 2.1403188705444336
Validation loss: 2.360868861598353

Epoch: 6| Step: 7
Training loss: 3.1157922744750977
Validation loss: 2.379212446110223

Epoch: 6| Step: 8
Training loss: 2.073333740234375
Validation loss: 2.349036516681794

Epoch: 6| Step: 9
Training loss: 2.563230037689209
Validation loss: 2.3070049388434297

Epoch: 6| Step: 10
Training loss: 2.451559066772461
Validation loss: 2.2582587836891093

Epoch: 6| Step: 11
Training loss: 2.2418220043182373
Validation loss: 2.2126933220894105

Epoch: 6| Step: 12
Training loss: 2.2024545669555664
Validation loss: 2.183419371163973

Epoch: 6| Step: 13
Training loss: 2.3035173416137695
Validation loss: 2.1682933786863923

Epoch: 159| Step: 0
Training loss: 2.665879249572754
Validation loss: 2.1675735365959907

Epoch: 6| Step: 1
Training loss: 2.79304838180542
Validation loss: 2.171216218702255

Epoch: 6| Step: 2
Training loss: 1.9452083110809326
Validation loss: 2.161000518388646

Epoch: 6| Step: 3
Training loss: 2.0950002670288086
Validation loss: 2.1641160864983835

Epoch: 6| Step: 4
Training loss: 3.2923784255981445
Validation loss: 2.1519546995880785

Epoch: 6| Step: 5
Training loss: 2.015235424041748
Validation loss: 2.173389636060243

Epoch: 6| Step: 6
Training loss: 2.458951950073242
Validation loss: 2.176698539846687

Epoch: 6| Step: 7
Training loss: 1.9327489137649536
Validation loss: 2.1716502353709233

Epoch: 6| Step: 8
Training loss: 1.9781162738800049
Validation loss: 2.175926254641625

Epoch: 6| Step: 9
Training loss: 1.706634521484375
Validation loss: 2.1916619628988285

Epoch: 6| Step: 10
Training loss: 1.9275606870651245
Validation loss: 2.205162081667172

Epoch: 6| Step: 11
Training loss: 2.7505433559417725
Validation loss: 2.22784698137673

Epoch: 6| Step: 12
Training loss: 2.580345630645752
Validation loss: 2.199782450993856

Epoch: 6| Step: 13
Training loss: 2.5128173828125
Validation loss: 2.1889839659455004

Epoch: 160| Step: 0
Training loss: 3.258664846420288
Validation loss: 2.183511882699946

Epoch: 6| Step: 1
Training loss: 2.136423110961914
Validation loss: 2.1858680145714873

Epoch: 6| Step: 2
Training loss: 1.521425485610962
Validation loss: 2.184994125878939

Epoch: 6| Step: 3
Training loss: 1.7100474834442139
Validation loss: 2.185417395766063

Epoch: 6| Step: 4
Training loss: 2.7531614303588867
Validation loss: 2.208129054756575

Epoch: 6| Step: 5
Training loss: 1.8763446807861328
Validation loss: 2.228966482224003

Epoch: 6| Step: 6
Training loss: 2.604451894760132
Validation loss: 2.24614808892691

Epoch: 6| Step: 7
Training loss: 1.9435534477233887
Validation loss: 2.2532884972069853

Epoch: 6| Step: 8
Training loss: 2.1146721839904785
Validation loss: 2.2556688042097193

Epoch: 6| Step: 9
Training loss: 2.868549346923828
Validation loss: 2.2618937466734197

Epoch: 6| Step: 10
Training loss: 1.9544832706451416
Validation loss: 2.2667912616524646

Epoch: 6| Step: 11
Training loss: 2.7476820945739746
Validation loss: 2.282985261691514

Epoch: 6| Step: 12
Training loss: 2.1559770107269287
Validation loss: 2.29404216940685

Epoch: 6| Step: 13
Training loss: 3.2475531101226807
Validation loss: 2.2773283348288587

Epoch: 161| Step: 0
Training loss: 1.8857543468475342
Validation loss: 2.304975360952398

Epoch: 6| Step: 1
Training loss: 3.2559008598327637
Validation loss: 2.272235160232872

Epoch: 6| Step: 2
Training loss: 2.3116676807403564
Validation loss: 2.2216280737230854

Epoch: 6| Step: 3
Training loss: 2.3610634803771973
Validation loss: 2.187866637783666

Epoch: 6| Step: 4
Training loss: 2.3917508125305176
Validation loss: 2.1699913752976285

Epoch: 6| Step: 5
Training loss: 1.8414828777313232
Validation loss: 2.151316673524918

Epoch: 6| Step: 6
Training loss: 2.5097756385803223
Validation loss: 2.159228656881599

Epoch: 6| Step: 7
Training loss: 2.334148645401001
Validation loss: 2.157488989573653

Epoch: 6| Step: 8
Training loss: 2.7097787857055664
Validation loss: 2.1511336629108717

Epoch: 6| Step: 9
Training loss: 1.7750828266143799
Validation loss: 2.1474765193077827

Epoch: 6| Step: 10
Training loss: 2.0700979232788086
Validation loss: 2.1486230242636895

Epoch: 6| Step: 11
Training loss: 2.478304624557495
Validation loss: 2.1615751584370932

Epoch: 6| Step: 12
Training loss: 2.3426382541656494
Validation loss: 2.1704556506167174

Epoch: 6| Step: 13
Training loss: 2.3259623050689697
Validation loss: 2.1733044424364643

Epoch: 162| Step: 0
Training loss: 1.997923731803894
Validation loss: 2.183305181482787

Epoch: 6| Step: 1
Training loss: 1.8744518756866455
Validation loss: 2.2085708956564627

Epoch: 6| Step: 2
Training loss: 2.2116951942443848
Validation loss: 2.223684787750244

Epoch: 6| Step: 3
Training loss: 2.688577890396118
Validation loss: 2.2356096877846667

Epoch: 6| Step: 4
Training loss: 2.5011820793151855
Validation loss: 2.252275382318804

Epoch: 6| Step: 5
Training loss: 1.6785722970962524
Validation loss: 2.248982083412909

Epoch: 6| Step: 6
Training loss: 2.2772419452667236
Validation loss: 2.2236107690359956

Epoch: 6| Step: 7
Training loss: 2.281115770339966
Validation loss: 2.2177437402868785

Epoch: 6| Step: 8
Training loss: 2.724693775177002
Validation loss: 2.2108438719985304

Epoch: 6| Step: 9
Training loss: 2.6364448070526123
Validation loss: 2.2034775057146625

Epoch: 6| Step: 10
Training loss: 2.5492758750915527
Validation loss: 2.2179893703870874

Epoch: 6| Step: 11
Training loss: 2.0879054069519043
Validation loss: 2.237292681970904

Epoch: 6| Step: 12
Training loss: 2.3190441131591797
Validation loss: 2.2307659323497484

Epoch: 6| Step: 13
Training loss: 2.265232563018799
Validation loss: 2.215263687154298

Epoch: 163| Step: 0
Training loss: 1.7355115413665771
Validation loss: 2.1956370492135324

Epoch: 6| Step: 1
Training loss: 2.356529712677002
Validation loss: 2.194587387064452

Epoch: 6| Step: 2
Training loss: 2.2602689266204834
Validation loss: 2.1790102374169136

Epoch: 6| Step: 3
Training loss: 2.2143194675445557
Validation loss: 2.163726470803702

Epoch: 6| Step: 4
Training loss: 1.6848474740982056
Validation loss: 2.162159562110901

Epoch: 6| Step: 5
Training loss: 2.420220375061035
Validation loss: 2.1690838516399427

Epoch: 6| Step: 6
Training loss: 2.8862452507019043
Validation loss: 2.207793535724763

Epoch: 6| Step: 7
Training loss: 2.5752205848693848
Validation loss: 2.2025633653004966

Epoch: 6| Step: 8
Training loss: 2.4293627738952637
Validation loss: 2.2033410969600884

Epoch: 6| Step: 9
Training loss: 1.6914254426956177
Validation loss: 2.2225367715281825

Epoch: 6| Step: 10
Training loss: 2.8175148963928223
Validation loss: 2.2140613038052797

Epoch: 6| Step: 11
Training loss: 1.7191013097763062
Validation loss: 2.229316606316515

Epoch: 6| Step: 12
Training loss: 2.938234806060791
Validation loss: 2.216579709001767

Epoch: 6| Step: 13
Training loss: 2.0913422107696533
Validation loss: 2.224761037416356

Epoch: 164| Step: 0
Training loss: 2.067538261413574
Validation loss: 2.2376470027431363

Epoch: 6| Step: 1
Training loss: 1.985421895980835
Validation loss: 2.2445148832054547

Epoch: 6| Step: 2
Training loss: 2.4015002250671387
Validation loss: 2.23037709728364

Epoch: 6| Step: 3
Training loss: 2.3419201374053955
Validation loss: 2.2209754656719904

Epoch: 6| Step: 4
Training loss: 1.8955507278442383
Validation loss: 2.210992902837774

Epoch: 6| Step: 5
Training loss: 1.844761610031128
Validation loss: 2.211940183434435

Epoch: 6| Step: 6
Training loss: 2.785395860671997
Validation loss: 2.200637094436153

Epoch: 6| Step: 7
Training loss: 2.1010334491729736
Validation loss: 2.195784435477308

Epoch: 6| Step: 8
Training loss: 1.9212870597839355
Validation loss: 2.2186409734910533

Epoch: 6| Step: 9
Training loss: 2.827531576156616
Validation loss: 2.2194231146125385

Epoch: 6| Step: 10
Training loss: 1.8393062353134155
Validation loss: 2.240994837976271

Epoch: 6| Step: 11
Training loss: 3.3371405601501465
Validation loss: 2.2380970088384484

Epoch: 6| Step: 12
Training loss: 2.292783498764038
Validation loss: 2.215157831868818

Epoch: 6| Step: 13
Training loss: 1.939099669456482
Validation loss: 2.223770838911815

Epoch: 165| Step: 0
Training loss: 1.7492060661315918
Validation loss: 2.2102398744193454

Epoch: 6| Step: 1
Training loss: 2.595693588256836
Validation loss: 2.2073650052470546

Epoch: 6| Step: 2
Training loss: 1.768498182296753
Validation loss: 2.214550369529314

Epoch: 6| Step: 3
Training loss: 1.6294255256652832
Validation loss: 2.222324550792735

Epoch: 6| Step: 4
Training loss: 2.8045167922973633
Validation loss: 2.233631128905922

Epoch: 6| Step: 5
Training loss: 2.3765063285827637
Validation loss: 2.2231724390419583

Epoch: 6| Step: 6
Training loss: 2.006568431854248
Validation loss: 2.2394910858523462

Epoch: 6| Step: 7
Training loss: 2.4670867919921875
Validation loss: 2.2506887848659227

Epoch: 6| Step: 8
Training loss: 2.126540184020996
Validation loss: 2.277058034814814

Epoch: 6| Step: 9
Training loss: 1.6993651390075684
Validation loss: 2.2760032658935874

Epoch: 6| Step: 10
Training loss: 2.6233558654785156
Validation loss: 2.262889240377693

Epoch: 6| Step: 11
Training loss: 2.751453399658203
Validation loss: 2.2687223983067337

Epoch: 6| Step: 12
Training loss: 2.79653263092041
Validation loss: 2.2559451492883826

Epoch: 6| Step: 13
Training loss: 2.1737468242645264
Validation loss: 2.239406898457517

Epoch: 166| Step: 0
Training loss: 2.1204888820648193
Validation loss: 2.236041117739934

Epoch: 6| Step: 1
Training loss: 2.332697629928589
Validation loss: 2.2083224263242496

Epoch: 6| Step: 2
Training loss: 2.207775354385376
Validation loss: 2.2063448070197977

Epoch: 6| Step: 3
Training loss: 2.2175450325012207
Validation loss: 2.2109408942602014

Epoch: 6| Step: 4
Training loss: 1.9714915752410889
Validation loss: 2.2129150757225613

Epoch: 6| Step: 5
Training loss: 2.714129686355591
Validation loss: 2.223876678815452

Epoch: 6| Step: 6
Training loss: 1.8640382289886475
Validation loss: 2.240798875849734

Epoch: 6| Step: 7
Training loss: 2.7570056915283203
Validation loss: 2.2513533074368715

Epoch: 6| Step: 8
Training loss: 1.9877798557281494
Validation loss: 2.2448185284932456

Epoch: 6| Step: 9
Training loss: 2.5971598625183105
Validation loss: 2.268598420645601

Epoch: 6| Step: 10
Training loss: 2.600998878479004
Validation loss: 2.2603247473316808

Epoch: 6| Step: 11
Training loss: 1.9362940788269043
Validation loss: 2.2487509404459307

Epoch: 6| Step: 12
Training loss: 1.8390679359436035
Validation loss: 2.2269701316792476

Epoch: 6| Step: 13
Training loss: 2.9166650772094727
Validation loss: 2.225206587904243

Epoch: 167| Step: 0
Training loss: 3.312800884246826
Validation loss: 2.1850841993926675

Epoch: 6| Step: 1
Training loss: 2.7101683616638184
Validation loss: 2.150832445390763

Epoch: 6| Step: 2
Training loss: 1.5833028554916382
Validation loss: 2.159682863502092

Epoch: 6| Step: 3
Training loss: 1.9386085271835327
Validation loss: 2.159245426936816

Epoch: 6| Step: 4
Training loss: 2.4553637504577637
Validation loss: 2.162767412841961

Epoch: 6| Step: 5
Training loss: 2.7557499408721924
Validation loss: 2.166575397214582

Epoch: 6| Step: 6
Training loss: 1.1649223566055298
Validation loss: 2.158149582083507

Epoch: 6| Step: 7
Training loss: 2.1349551677703857
Validation loss: 2.1671062361809517

Epoch: 6| Step: 8
Training loss: 2.3748397827148438
Validation loss: 2.176954778291846

Epoch: 6| Step: 9
Training loss: 2.891500949859619
Validation loss: 2.1901155979402605

Epoch: 6| Step: 10
Training loss: 1.386952519416809
Validation loss: 2.197193407243298

Epoch: 6| Step: 11
Training loss: 2.043837070465088
Validation loss: 2.2141548613066315

Epoch: 6| Step: 12
Training loss: 2.087336301803589
Validation loss: 2.2224881354198662

Epoch: 6| Step: 13
Training loss: 3.019749641418457
Validation loss: 2.2476659000560804

Epoch: 168| Step: 0
Training loss: 1.8221937417984009
Validation loss: 2.2491115190649547

Epoch: 6| Step: 1
Training loss: 2.3601810932159424
Validation loss: 2.2421646784710627

Epoch: 6| Step: 2
Training loss: 2.1318087577819824
Validation loss: 2.2313696312647995

Epoch: 6| Step: 3
Training loss: 1.9997540712356567
Validation loss: 2.220326474917832

Epoch: 6| Step: 4
Training loss: 1.7054100036621094
Validation loss: 2.209830617391935

Epoch: 6| Step: 5
Training loss: 2.0900886058807373
Validation loss: 2.2024814826185986

Epoch: 6| Step: 6
Training loss: 2.2935562133789062
Validation loss: 2.2156219431149062

Epoch: 6| Step: 7
Training loss: 2.40798282623291
Validation loss: 2.2299478336047103

Epoch: 6| Step: 8
Training loss: 2.576910972595215
Validation loss: 2.211636189491518

Epoch: 6| Step: 9
Training loss: 1.2880175113677979
Validation loss: 2.1973154621739543

Epoch: 6| Step: 10
Training loss: 2.5309197902679443
Validation loss: 2.209222726924445

Epoch: 6| Step: 11
Training loss: 2.8008370399475098
Validation loss: 2.207073128351601

Epoch: 6| Step: 12
Training loss: 2.6205382347106934
Validation loss: 2.1975572211768037

Epoch: 6| Step: 13
Training loss: 2.517509698867798
Validation loss: 2.1926001476985153

Epoch: 169| Step: 0
Training loss: 2.1778266429901123
Validation loss: 2.216352375604773

Epoch: 6| Step: 1
Training loss: 2.4566898345947266
Validation loss: 2.1995362261290192

Epoch: 6| Step: 2
Training loss: 2.2825498580932617
Validation loss: 2.2108336776815434

Epoch: 6| Step: 3
Training loss: 2.1972525119781494
Validation loss: 2.238226321435744

Epoch: 6| Step: 4
Training loss: 2.112107276916504
Validation loss: 2.2664240457678355

Epoch: 6| Step: 5
Training loss: 2.2942943572998047
Validation loss: 2.283080113831387

Epoch: 6| Step: 6
Training loss: 2.6420388221740723
Validation loss: 2.321944658474256

Epoch: 6| Step: 7
Training loss: 1.4739269018173218
Validation loss: 2.334665811190041

Epoch: 6| Step: 8
Training loss: 2.814373254776001
Validation loss: 2.344219417982204

Epoch: 6| Step: 9
Training loss: 2.179762840270996
Validation loss: 2.3438292677684496

Epoch: 6| Step: 10
Training loss: 2.1303911209106445
Validation loss: 2.3256409706607943

Epoch: 6| Step: 11
Training loss: 2.0454864501953125
Validation loss: 2.292556237148982

Epoch: 6| Step: 12
Training loss: 2.3430614471435547
Validation loss: 2.261112851481284

Epoch: 6| Step: 13
Training loss: 2.001207113265991
Validation loss: 2.2442481902337845

Epoch: 170| Step: 0
Training loss: 2.5117621421813965
Validation loss: 2.2571355091628207

Epoch: 6| Step: 1
Training loss: 2.21205472946167
Validation loss: 2.259300196042625

Epoch: 6| Step: 2
Training loss: 2.2074906826019287
Validation loss: 2.25620577924995

Epoch: 6| Step: 3
Training loss: 1.9858028888702393
Validation loss: 2.2548926414981967

Epoch: 6| Step: 4
Training loss: 1.706777811050415
Validation loss: 2.232684381546513

Epoch: 6| Step: 5
Training loss: 2.634943962097168
Validation loss: 2.2275047789337816

Epoch: 6| Step: 6
Training loss: 2.4346747398376465
Validation loss: 2.2013027616726455

Epoch: 6| Step: 7
Training loss: 2.443035840988159
Validation loss: 2.1843851125368507

Epoch: 6| Step: 8
Training loss: 1.9604148864746094
Validation loss: 2.1557129839415192

Epoch: 6| Step: 9
Training loss: 2.215395450592041
Validation loss: 2.1436047246379237

Epoch: 6| Step: 10
Training loss: 2.415278911590576
Validation loss: 2.1397003948047595

Epoch: 6| Step: 11
Training loss: 2.486509084701538
Validation loss: 2.1437131794550086

Epoch: 6| Step: 12
Training loss: 2.0538339614868164
Validation loss: 2.1432822289005404

Epoch: 6| Step: 13
Training loss: 2.3514833450317383
Validation loss: 2.14989496046497

Epoch: 171| Step: 0
Training loss: 1.5361602306365967
Validation loss: 2.1674848730846117

Epoch: 6| Step: 1
Training loss: 2.791266441345215
Validation loss: 2.1914117618273665

Epoch: 6| Step: 2
Training loss: 2.5171422958374023
Validation loss: 2.1892498923886206

Epoch: 6| Step: 3
Training loss: 2.6816654205322266
Validation loss: 2.1991240657785887

Epoch: 6| Step: 4
Training loss: 2.348979949951172
Validation loss: 2.2281504087550665

Epoch: 6| Step: 5
Training loss: 2.1476244926452637
Validation loss: 2.247395369314378

Epoch: 6| Step: 6
Training loss: 1.7837156057357788
Validation loss: 2.2541610130699734

Epoch: 6| Step: 7
Training loss: 2.1951262950897217
Validation loss: 2.275442602813885

Epoch: 6| Step: 8
Training loss: 2.345336437225342
Validation loss: 2.27362367158295

Epoch: 6| Step: 9
Training loss: 2.4818170070648193
Validation loss: 2.2927478359591578

Epoch: 6| Step: 10
Training loss: 2.0795302391052246
Validation loss: 2.2832169686594317

Epoch: 6| Step: 11
Training loss: 2.560436964035034
Validation loss: 2.3072399657259703

Epoch: 6| Step: 12
Training loss: 1.8624188899993896
Validation loss: 2.2953346160150345

Epoch: 6| Step: 13
Training loss: 0.9125823974609375
Validation loss: 2.2585859298706055

Epoch: 172| Step: 0
Training loss: 2.656930923461914
Validation loss: 2.2317108197878768

Epoch: 6| Step: 1
Training loss: 2.5515923500061035
Validation loss: 2.2024956134057816

Epoch: 6| Step: 2
Training loss: 2.1718456745147705
Validation loss: 2.1936946274131857

Epoch: 6| Step: 3
Training loss: 1.8093957901000977
Validation loss: 2.196815231794952

Epoch: 6| Step: 4
Training loss: 1.7491276264190674
Validation loss: 2.198305099241195

Epoch: 6| Step: 5
Training loss: 1.9024699926376343
Validation loss: 2.1954951311952327

Epoch: 6| Step: 6
Training loss: 3.087899923324585
Validation loss: 2.181150077491678

Epoch: 6| Step: 7
Training loss: 1.823779582977295
Validation loss: 2.168934222190611

Epoch: 6| Step: 8
Training loss: 2.192452907562256
Validation loss: 2.176012627540096

Epoch: 6| Step: 9
Training loss: 1.8520007133483887
Validation loss: 2.1809119896222184

Epoch: 6| Step: 10
Training loss: 2.6331381797790527
Validation loss: 2.1790658838005474

Epoch: 6| Step: 11
Training loss: 2.067722797393799
Validation loss: 2.2000889957592054

Epoch: 6| Step: 12
Training loss: 2.2300491333007812
Validation loss: 2.251498514606107

Epoch: 6| Step: 13
Training loss: 2.0380711555480957
Validation loss: 2.2720436614046813

Epoch: 173| Step: 0
Training loss: 2.7722883224487305
Validation loss: 2.284724512407857

Epoch: 6| Step: 1
Training loss: 2.566883087158203
Validation loss: 2.2828798665795276

Epoch: 6| Step: 2
Training loss: 1.7206681966781616
Validation loss: 2.3017458095345447

Epoch: 6| Step: 3
Training loss: 2.8283982276916504
Validation loss: 2.302669066254811

Epoch: 6| Step: 4
Training loss: 0.843504011631012
Validation loss: 2.306805431201894

Epoch: 6| Step: 5
Training loss: 3.1096673011779785
Validation loss: 2.293554962322276

Epoch: 6| Step: 6
Training loss: 2.80544376373291
Validation loss: 2.281833517935968

Epoch: 6| Step: 7
Training loss: 2.112745761871338
Validation loss: 2.2722136743607058

Epoch: 6| Step: 8
Training loss: 1.1570241451263428
Validation loss: 2.270245293135284

Epoch: 6| Step: 9
Training loss: 2.106281280517578
Validation loss: 2.2359679745089625

Epoch: 6| Step: 10
Training loss: 2.7854886054992676
Validation loss: 2.2209472387067732

Epoch: 6| Step: 11
Training loss: 1.611715316772461
Validation loss: 2.186983778912534

Epoch: 6| Step: 12
Training loss: 1.9412274360656738
Validation loss: 2.1697591350924585

Epoch: 6| Step: 13
Training loss: 1.8173211812973022
Validation loss: 2.1569892168045044

Epoch: 174| Step: 0
Training loss: 1.789068579673767
Validation loss: 2.1529298059401976

Epoch: 6| Step: 1
Training loss: 2.261157512664795
Validation loss: 2.1508730393584057

Epoch: 6| Step: 2
Training loss: 2.08120059967041
Validation loss: 2.159250796482127

Epoch: 6| Step: 3
Training loss: 2.4121925830841064
Validation loss: 2.151080072567027

Epoch: 6| Step: 4
Training loss: 2.495217800140381
Validation loss: 2.161503550826862

Epoch: 6| Step: 5
Training loss: 2.687981605529785
Validation loss: 2.1576133569081626

Epoch: 6| Step: 6
Training loss: 1.3914659023284912
Validation loss: 2.1972755232164936

Epoch: 6| Step: 7
Training loss: 1.9983818531036377
Validation loss: 2.2291769481474355

Epoch: 6| Step: 8
Training loss: 3.0348877906799316
Validation loss: 2.2566534319231586

Epoch: 6| Step: 9
Training loss: 1.9544377326965332
Validation loss: 2.2509573787771244

Epoch: 6| Step: 10
Training loss: 1.6531943082809448
Validation loss: 2.2408759875964095

Epoch: 6| Step: 11
Training loss: 2.193483352661133
Validation loss: 2.2243688901265464

Epoch: 6| Step: 12
Training loss: 2.3813695907592773
Validation loss: 2.2252210545283493

Epoch: 6| Step: 13
Training loss: 1.9547464847564697
Validation loss: 2.2470475345529537

Epoch: 175| Step: 0
Training loss: 2.0965418815612793
Validation loss: 2.2434200497083765

Epoch: 6| Step: 1
Training loss: 2.3458304405212402
Validation loss: 2.258920951556134

Epoch: 6| Step: 2
Training loss: 1.7189632654190063
Validation loss: 2.254014571507772

Epoch: 6| Step: 3
Training loss: 1.8122493028640747
Validation loss: 2.2410135422983477

Epoch: 6| Step: 4
Training loss: 2.5203707218170166
Validation loss: 2.243469816382213

Epoch: 6| Step: 5
Training loss: 2.07177472114563
Validation loss: 2.2290800104859056

Epoch: 6| Step: 6
Training loss: 1.9405710697174072
Validation loss: 2.245445354010469

Epoch: 6| Step: 7
Training loss: 2.7804617881774902
Validation loss: 2.2397770727834394

Epoch: 6| Step: 8
Training loss: 2.431276321411133
Validation loss: 2.2576197501151793

Epoch: 6| Step: 9
Training loss: 2.408451557159424
Validation loss: 2.2724746068318686

Epoch: 6| Step: 10
Training loss: 2.5881781578063965
Validation loss: 2.293563376190842

Epoch: 6| Step: 11
Training loss: 1.9357273578643799
Validation loss: 2.2948521029564644

Epoch: 6| Step: 12
Training loss: 1.473379135131836
Validation loss: 2.268541800078525

Epoch: 6| Step: 13
Training loss: 2.1166720390319824
Validation loss: 2.2309626148593042

Epoch: 176| Step: 0
Training loss: 2.3527536392211914
Validation loss: 2.2007919050032094

Epoch: 6| Step: 1
Training loss: 2.519176959991455
Validation loss: 2.172451675579112

Epoch: 6| Step: 2
Training loss: 2.5632948875427246
Validation loss: 2.16857409989962

Epoch: 6| Step: 3
Training loss: 1.9466135501861572
Validation loss: 2.168361807382235

Epoch: 6| Step: 4
Training loss: 2.472407817840576
Validation loss: 2.18509364897205

Epoch: 6| Step: 5
Training loss: 2.0968730449676514
Validation loss: 2.1929315264507006

Epoch: 6| Step: 6
Training loss: 2.2163469791412354
Validation loss: 2.219046814467317

Epoch: 6| Step: 7
Training loss: 2.7692971229553223
Validation loss: 2.2230410088774977

Epoch: 6| Step: 8
Training loss: 1.6541918516159058
Validation loss: 2.2491464230322067

Epoch: 6| Step: 9
Training loss: 1.4352928400039673
Validation loss: 2.243204762858729

Epoch: 6| Step: 10
Training loss: 1.7695841789245605
Validation loss: 2.2548869809796734

Epoch: 6| Step: 11
Training loss: 2.1238718032836914
Validation loss: 2.2768653746574157

Epoch: 6| Step: 12
Training loss: 2.2130987644195557
Validation loss: 2.27034802462465

Epoch: 6| Step: 13
Training loss: 1.5358198881149292
Validation loss: 2.2891134549212713

Epoch: 177| Step: 0
Training loss: 2.9102065563201904
Validation loss: 2.28494155663316

Epoch: 6| Step: 1
Training loss: 1.6954963207244873
Validation loss: 2.301069698026103

Epoch: 6| Step: 2
Training loss: 2.1782007217407227
Validation loss: 2.314627375653995

Epoch: 6| Step: 3
Training loss: 1.441210150718689
Validation loss: 2.295214012104978

Epoch: 6| Step: 4
Training loss: 1.708457589149475
Validation loss: 2.2806857811507357

Epoch: 6| Step: 5
Training loss: 2.168203115463257
Validation loss: 2.2720988565875637

Epoch: 6| Step: 6
Training loss: 2.363600254058838
Validation loss: 2.279989586081556

Epoch: 6| Step: 7
Training loss: 2.1622772216796875
Validation loss: 2.257460419849683

Epoch: 6| Step: 8
Training loss: 2.9192824363708496
Validation loss: 2.223822587279863

Epoch: 6| Step: 9
Training loss: 1.3718156814575195
Validation loss: 2.2242911425969933

Epoch: 6| Step: 10
Training loss: 2.254943370819092
Validation loss: 2.2083214329135035

Epoch: 6| Step: 11
Training loss: 2.67995023727417
Validation loss: 2.218552645816598

Epoch: 6| Step: 12
Training loss: 2.069990396499634
Validation loss: 2.2184231512008177

Epoch: 6| Step: 13
Training loss: 2.125181198120117
Validation loss: 2.1884204956793014

Epoch: 178| Step: 0
Training loss: 2.1356563568115234
Validation loss: 2.150038721740887

Epoch: 6| Step: 1
Training loss: 2.3136329650878906
Validation loss: 2.1221505236882034

Epoch: 6| Step: 2
Training loss: 2.452665328979492
Validation loss: 2.1096176921680407

Epoch: 6| Step: 3
Training loss: 2.782703399658203
Validation loss: 2.0877132467044297

Epoch: 6| Step: 4
Training loss: 1.9638028144836426
Validation loss: 2.0933893188353507

Epoch: 6| Step: 5
Training loss: 2.1734189987182617
Validation loss: 2.1082196453566193

Epoch: 6| Step: 6
Training loss: 2.4689602851867676
Validation loss: 2.115045347521382

Epoch: 6| Step: 7
Training loss: 1.6566755771636963
Validation loss: 2.122231428341199

Epoch: 6| Step: 8
Training loss: 2.179568290710449
Validation loss: 2.13417447510586

Epoch: 6| Step: 9
Training loss: 1.3817402124404907
Validation loss: 2.1290558358674407

Epoch: 6| Step: 10
Training loss: 2.1297495365142822
Validation loss: 2.151730168250299

Epoch: 6| Step: 11
Training loss: 1.6213921308517456
Validation loss: 2.178791583225291

Epoch: 6| Step: 12
Training loss: 2.023176670074463
Validation loss: 2.1974509915997906

Epoch: 6| Step: 13
Training loss: 3.22757625579834
Validation loss: 2.2420808858768915

Epoch: 179| Step: 0
Training loss: 2.8385210037231445
Validation loss: 2.262357529773507

Epoch: 6| Step: 1
Training loss: 2.0073142051696777
Validation loss: 2.2669538297960834

Epoch: 6| Step: 2
Training loss: 2.3065624237060547
Validation loss: 2.258151285109981

Epoch: 6| Step: 3
Training loss: 2.304561138153076
Validation loss: 2.272012646480273

Epoch: 6| Step: 4
Training loss: 1.7640612125396729
Validation loss: 2.2718638707232732

Epoch: 6| Step: 5
Training loss: 2.25314998626709
Validation loss: 2.2899977545584402

Epoch: 6| Step: 6
Training loss: 2.15024471282959
Validation loss: 2.2842469984485256

Epoch: 6| Step: 7
Training loss: 2.354130983352661
Validation loss: 2.2437158118012133

Epoch: 6| Step: 8
Training loss: 2.269861936569214
Validation loss: 2.2108223053716842

Epoch: 6| Step: 9
Training loss: 2.0495285987854004
Validation loss: 2.172736979299976

Epoch: 6| Step: 10
Training loss: 1.9568142890930176
Validation loss: 2.1729726265835505

Epoch: 6| Step: 11
Training loss: 2.4165546894073486
Validation loss: 2.1796854926693823

Epoch: 6| Step: 12
Training loss: 1.7754504680633545
Validation loss: 2.1876333913495465

Epoch: 6| Step: 13
Training loss: 1.1653770208358765
Validation loss: 2.222930739002843

Epoch: 180| Step: 0
Training loss: 1.7491247653961182
Validation loss: 2.238098557277392

Epoch: 6| Step: 1
Training loss: 2.265120029449463
Validation loss: 2.2568142362820205

Epoch: 6| Step: 2
Training loss: 1.8875904083251953
Validation loss: 2.2722859305720173

Epoch: 6| Step: 3
Training loss: 2.3278653621673584
Validation loss: 2.2704925972928285

Epoch: 6| Step: 4
Training loss: 1.757529377937317
Validation loss: 2.2696148785211707

Epoch: 6| Step: 5
Training loss: 1.9824436902999878
Validation loss: 2.2854374929140975

Epoch: 6| Step: 6
Training loss: 2.2343387603759766
Validation loss: 2.281145182988977

Epoch: 6| Step: 7
Training loss: 2.3872134685516357
Validation loss: 2.2665078973257415

Epoch: 6| Step: 8
Training loss: 2.883570671081543
Validation loss: 2.2602649324683735

Epoch: 6| Step: 9
Training loss: 1.616837501525879
Validation loss: 2.2243992602953346

Epoch: 6| Step: 10
Training loss: 2.0699079036712646
Validation loss: 2.203188614178729

Epoch: 6| Step: 11
Training loss: 2.390885829925537
Validation loss: 2.204761266708374

Epoch: 6| Step: 12
Training loss: 2.3001465797424316
Validation loss: 2.2108489492888093

Epoch: 6| Step: 13
Training loss: 2.1622183322906494
Validation loss: 2.200241847704816

Epoch: 181| Step: 0
Training loss: 2.468255043029785
Validation loss: 2.201883815949963

Epoch: 6| Step: 1
Training loss: 1.650794506072998
Validation loss: 2.178235570589701

Epoch: 6| Step: 2
Training loss: 1.9157558679580688
Validation loss: 2.176733637368807

Epoch: 6| Step: 3
Training loss: 1.8833039999008179
Validation loss: 2.180980213226811

Epoch: 6| Step: 4
Training loss: 2.4181861877441406
Validation loss: 2.157787831880713

Epoch: 6| Step: 5
Training loss: 2.89279842376709
Validation loss: 2.1446717413522864

Epoch: 6| Step: 6
Training loss: 2.456190347671509
Validation loss: 2.1499957397419918

Epoch: 6| Step: 7
Training loss: 1.6301500797271729
Validation loss: 2.1279643709941576

Epoch: 6| Step: 8
Training loss: 1.9488461017608643
Validation loss: 2.1095625392852293

Epoch: 6| Step: 9
Training loss: 2.1237616539001465
Validation loss: 2.122804485341554

Epoch: 6| Step: 10
Training loss: 2.2722878456115723
Validation loss: 2.110462745030721

Epoch: 6| Step: 11
Training loss: 1.7045872211456299
Validation loss: 2.109066160776282

Epoch: 6| Step: 12
Training loss: 2.3025221824645996
Validation loss: 2.1061631582116567

Epoch: 6| Step: 13
Training loss: 1.5661441087722778
Validation loss: 2.130558862481066

Epoch: 182| Step: 0
Training loss: 2.111283540725708
Validation loss: 2.155082095053888

Epoch: 6| Step: 1
Training loss: 2.0314760208129883
Validation loss: 2.1833275697564565

Epoch: 6| Step: 2
Training loss: 2.23610258102417
Validation loss: 2.207173398745957

Epoch: 6| Step: 3
Training loss: 2.1895248889923096
Validation loss: 2.237460174868184

Epoch: 6| Step: 4
Training loss: 2.28981614112854
Validation loss: 2.252465578817552

Epoch: 6| Step: 5
Training loss: 1.6714227199554443
Validation loss: 2.2883824648395663

Epoch: 6| Step: 6
Training loss: 2.5051655769348145
Validation loss: 2.2928458259951685

Epoch: 6| Step: 7
Training loss: 2.1253061294555664
Validation loss: 2.290860458086896

Epoch: 6| Step: 8
Training loss: 1.3150269985198975
Validation loss: 2.2901286040582964

Epoch: 6| Step: 9
Training loss: 2.6042165756225586
Validation loss: 2.2634316734088364

Epoch: 6| Step: 10
Training loss: 2.3384828567504883
Validation loss: 2.223890607075025

Epoch: 6| Step: 11
Training loss: 2.1514856815338135
Validation loss: 2.2105891858377764

Epoch: 6| Step: 12
Training loss: 1.4105651378631592
Validation loss: 2.1894591290463685

Epoch: 6| Step: 13
Training loss: 2.3124918937683105
Validation loss: 2.171149751191498

Epoch: 183| Step: 0
Training loss: 2.126897096633911
Validation loss: 2.1487385995926394

Epoch: 6| Step: 1
Training loss: 2.3210654258728027
Validation loss: 2.1309643227566957

Epoch: 6| Step: 2
Training loss: 1.7085187435150146
Validation loss: 2.1205671551407024

Epoch: 6| Step: 3
Training loss: 1.9085402488708496
Validation loss: 2.118495289997388

Epoch: 6| Step: 4
Training loss: 1.585412859916687
Validation loss: 2.129247957660306

Epoch: 6| Step: 5
Training loss: 2.081110954284668
Validation loss: 2.13783295564754

Epoch: 6| Step: 6
Training loss: 2.349545478820801
Validation loss: 2.1674075665012484

Epoch: 6| Step: 7
Training loss: 1.416195273399353
Validation loss: 2.1767675517707743

Epoch: 6| Step: 8
Training loss: 2.672952175140381
Validation loss: 2.1770250502453057

Epoch: 6| Step: 9
Training loss: 1.989463210105896
Validation loss: 2.16012547093053

Epoch: 6| Step: 10
Training loss: 2.1410410404205322
Validation loss: 2.1665635903676352

Epoch: 6| Step: 11
Training loss: 2.5041370391845703
Validation loss: 2.1509475849008046

Epoch: 6| Step: 12
Training loss: 2.130093812942505
Validation loss: 2.1445328984209286

Epoch: 6| Step: 13
Training loss: 2.7256898880004883
Validation loss: 2.1373085616737284

Epoch: 184| Step: 0
Training loss: 1.944448471069336
Validation loss: 2.1141371957717405

Epoch: 6| Step: 1
Training loss: 2.0159335136413574
Validation loss: 2.1144049911088842

Epoch: 6| Step: 2
Training loss: 2.4338152408599854
Validation loss: 2.116211845028785

Epoch: 6| Step: 3
Training loss: 1.507828712463379
Validation loss: 2.1160499485590125

Epoch: 6| Step: 4
Training loss: 1.8364087343215942
Validation loss: 2.1257060022764307

Epoch: 6| Step: 5
Training loss: 2.1141016483306885
Validation loss: 2.134246487771311

Epoch: 6| Step: 6
Training loss: 2.178558349609375
Validation loss: 2.153342354682184

Epoch: 6| Step: 7
Training loss: 1.853518009185791
Validation loss: 2.1587675758587417

Epoch: 6| Step: 8
Training loss: 2.39852237701416
Validation loss: 2.158735018904491

Epoch: 6| Step: 9
Training loss: 2.3506858348846436
Validation loss: 2.1766818159370014

Epoch: 6| Step: 10
Training loss: 1.7564661502838135
Validation loss: 2.1689020382460726

Epoch: 6| Step: 11
Training loss: 2.2176575660705566
Validation loss: 2.200100661605917

Epoch: 6| Step: 12
Training loss: 2.1342437267303467
Validation loss: 2.216439590659193

Epoch: 6| Step: 13
Training loss: 2.602295398712158
Validation loss: 2.1843970039839387

Epoch: 185| Step: 0
Training loss: 2.2674429416656494
Validation loss: 2.181791643942556

Epoch: 6| Step: 1
Training loss: 1.7147250175476074
Validation loss: 2.1586998149912846

Epoch: 6| Step: 2
Training loss: 1.9175251722335815
Validation loss: 2.1563291549682617

Epoch: 6| Step: 3
Training loss: 1.9059926271438599
Validation loss: 2.1579110058405067

Epoch: 6| Step: 4
Training loss: 1.4253442287445068
Validation loss: 2.147316430204658

Epoch: 6| Step: 5
Training loss: 1.5456926822662354
Validation loss: 2.1484516269417218

Epoch: 6| Step: 6
Training loss: 2.7691783905029297
Validation loss: 2.1399569306322324

Epoch: 6| Step: 7
Training loss: 2.2465033531188965
Validation loss: 2.1304102866880354

Epoch: 6| Step: 8
Training loss: 2.696849822998047
Validation loss: 2.1291494343870427

Epoch: 6| Step: 9
Training loss: 2.293968915939331
Validation loss: 2.131293753141998

Epoch: 6| Step: 10
Training loss: 1.4933271408081055
Validation loss: 2.142028010019692

Epoch: 6| Step: 11
Training loss: 2.678452968597412
Validation loss: 2.148527673495713

Epoch: 6| Step: 12
Training loss: 1.802371621131897
Validation loss: 2.1548734441880257

Epoch: 6| Step: 13
Training loss: 1.5646642446517944
Validation loss: 2.142872938545801

Epoch: 186| Step: 0
Training loss: 2.702969551086426
Validation loss: 2.1584733288775206

Epoch: 6| Step: 1
Training loss: 2.4308502674102783
Validation loss: 2.1907743177106305

Epoch: 6| Step: 2
Training loss: 1.8366941213607788
Validation loss: 2.2104904728551067

Epoch: 6| Step: 3
Training loss: 1.6832475662231445
Validation loss: 2.2269465269580966

Epoch: 6| Step: 4
Training loss: 1.379789113998413
Validation loss: 2.207798224623485

Epoch: 6| Step: 5
Training loss: 2.1759185791015625
Validation loss: 2.208968867537796

Epoch: 6| Step: 6
Training loss: 1.7817038297653198
Validation loss: 2.2131997000786567

Epoch: 6| Step: 7
Training loss: 2.1179287433624268
Validation loss: 2.2030367133437947

Epoch: 6| Step: 8
Training loss: 1.578138828277588
Validation loss: 2.2111499283903386

Epoch: 6| Step: 9
Training loss: 2.6981558799743652
Validation loss: 2.2080219061143938

Epoch: 6| Step: 10
Training loss: 2.14753794670105
Validation loss: 2.2131169073043333

Epoch: 6| Step: 11
Training loss: 2.587801456451416
Validation loss: 2.218996876029558

Epoch: 6| Step: 12
Training loss: 2.2668206691741943
Validation loss: 2.2131976542934293

Epoch: 6| Step: 13
Training loss: 0.7392534017562866
Validation loss: 2.202941991949594

Epoch: 187| Step: 0
Training loss: 2.4219210147857666
Validation loss: 2.1671063899993896

Epoch: 6| Step: 1
Training loss: 1.9570832252502441
Validation loss: 2.1595977249965874

Epoch: 6| Step: 2
Training loss: 2.155503273010254
Validation loss: 2.1687816932637203

Epoch: 6| Step: 3
Training loss: 1.4171981811523438
Validation loss: 2.164301671007628

Epoch: 6| Step: 4
Training loss: 2.0985920429229736
Validation loss: 2.160232763136587

Epoch: 6| Step: 5
Training loss: 1.9033844470977783
Validation loss: 2.1419980397788425

Epoch: 6| Step: 6
Training loss: 1.7794201374053955
Validation loss: 2.1364534926670853

Epoch: 6| Step: 7
Training loss: 1.4258136749267578
Validation loss: 2.133859078089396

Epoch: 6| Step: 8
Training loss: 2.06438946723938
Validation loss: 2.1291682284365416

Epoch: 6| Step: 9
Training loss: 2.2685346603393555
Validation loss: 2.1616079217644146

Epoch: 6| Step: 10
Training loss: 1.486350178718567
Validation loss: 2.16542641065454

Epoch: 6| Step: 11
Training loss: 2.410706043243408
Validation loss: 2.185648569496729

Epoch: 6| Step: 12
Training loss: 2.574781894683838
Validation loss: 2.1998476930843887

Epoch: 6| Step: 13
Training loss: 3.014369010925293
Validation loss: 2.1909825891576786

Epoch: 188| Step: 0
Training loss: 2.5912585258483887
Validation loss: 2.1992906703743884

Epoch: 6| Step: 1
Training loss: 2.4030184745788574
Validation loss: 2.1876106595480316

Epoch: 6| Step: 2
Training loss: 1.947876214981079
Validation loss: 2.189391407915341

Epoch: 6| Step: 3
Training loss: 2.751699447631836
Validation loss: 2.186143967413133

Epoch: 6| Step: 4
Training loss: 1.609075665473938
Validation loss: 2.1743086871280464

Epoch: 6| Step: 5
Training loss: 1.6699550151824951
Validation loss: 2.169078767940562

Epoch: 6| Step: 6
Training loss: 1.8002400398254395
Validation loss: 2.152566571389475

Epoch: 6| Step: 7
Training loss: 1.498417854309082
Validation loss: 2.1349321514047603

Epoch: 6| Step: 8
Training loss: 2.112001895904541
Validation loss: 2.1249312482854372

Epoch: 6| Step: 9
Training loss: 2.3932957649230957
Validation loss: 2.1178720394770303

Epoch: 6| Step: 10
Training loss: 2.0876121520996094
Validation loss: 2.1311712508560507

Epoch: 6| Step: 11
Training loss: 1.6739230155944824
Validation loss: 2.131424202713915

Epoch: 6| Step: 12
Training loss: 1.8055157661437988
Validation loss: 2.115249110806373

Epoch: 6| Step: 13
Training loss: 1.9845194816589355
Validation loss: 2.123544418683616

Epoch: 189| Step: 0
Training loss: 1.3392138481140137
Validation loss: 2.1164991009619927

Epoch: 6| Step: 1
Training loss: 2.6926276683807373
Validation loss: 2.1242535627016457

Epoch: 6| Step: 2
Training loss: 2.3714568614959717
Validation loss: 2.1363107978656726

Epoch: 6| Step: 3
Training loss: 2.159851312637329
Validation loss: 2.120750691301079

Epoch: 6| Step: 4
Training loss: 1.6975798606872559
Validation loss: 2.128475114863406

Epoch: 6| Step: 5
Training loss: 1.894381046295166
Validation loss: 2.131873748635733

Epoch: 6| Step: 6
Training loss: 2.159417152404785
Validation loss: 2.1242559827784055

Epoch: 6| Step: 7
Training loss: 1.8973325490951538
Validation loss: 2.104900736962595

Epoch: 6| Step: 8
Training loss: 2.1550376415252686
Validation loss: 2.118409518272646

Epoch: 6| Step: 9
Training loss: 1.9170267581939697
Validation loss: 2.116885413405716

Epoch: 6| Step: 10
Training loss: 2.000755548477173
Validation loss: 2.128124137078562

Epoch: 6| Step: 11
Training loss: 2.36964750289917
Validation loss: 2.131469270234467

Epoch: 6| Step: 12
Training loss: 2.1992015838623047
Validation loss: 2.1405420892982074

Epoch: 6| Step: 13
Training loss: 0.8261087536811829
Validation loss: 2.137013130290534

Epoch: 190| Step: 0
Training loss: 2.9230289459228516
Validation loss: 2.152325468678628

Epoch: 6| Step: 1
Training loss: 2.420842170715332
Validation loss: 2.174471729545183

Epoch: 6| Step: 2
Training loss: 1.616590142250061
Validation loss: 2.167423335454797

Epoch: 6| Step: 3
Training loss: 1.7211699485778809
Validation loss: 2.1851003400741087

Epoch: 6| Step: 4
Training loss: 1.264884352684021
Validation loss: 2.1931777077336467

Epoch: 6| Step: 5
Training loss: 1.9447886943817139
Validation loss: 2.1763579230154715

Epoch: 6| Step: 6
Training loss: 1.9578289985656738
Validation loss: 2.1726193274221113

Epoch: 6| Step: 7
Training loss: 2.460519313812256
Validation loss: 2.1755912996107534

Epoch: 6| Step: 8
Training loss: 2.032485246658325
Validation loss: 2.17594821991459

Epoch: 6| Step: 9
Training loss: 1.8860986232757568
Validation loss: 2.167371121785974

Epoch: 6| Step: 10
Training loss: 1.9023568630218506
Validation loss: 2.1502906686516217

Epoch: 6| Step: 11
Training loss: 1.8733880519866943
Validation loss: 2.15323882461876

Epoch: 6| Step: 12
Training loss: 1.4947664737701416
Validation loss: 2.1212064322604927

Epoch: 6| Step: 13
Training loss: 2.894078493118286
Validation loss: 2.139524032992701

Epoch: 191| Step: 0
Training loss: 2.0851473808288574
Validation loss: 2.1340227165529804

Epoch: 6| Step: 1
Training loss: 2.097376585006714
Validation loss: 2.1340375484958773

Epoch: 6| Step: 2
Training loss: 2.058558225631714
Validation loss: 2.1286535724516837

Epoch: 6| Step: 3
Training loss: 1.7256577014923096
Validation loss: 2.1424678730708298

Epoch: 6| Step: 4
Training loss: 1.9899650812149048
Validation loss: 2.13944496134276

Epoch: 6| Step: 5
Training loss: 1.7422078847885132
Validation loss: 2.1323110980372273

Epoch: 6| Step: 6
Training loss: 1.84330153465271
Validation loss: 2.136111233824043

Epoch: 6| Step: 7
Training loss: 2.044732093811035
Validation loss: 2.1451087485077562

Epoch: 6| Step: 8
Training loss: 2.0940499305725098
Validation loss: 2.136253187733312

Epoch: 6| Step: 9
Training loss: 2.327746629714966
Validation loss: 2.1457148905723327

Epoch: 6| Step: 10
Training loss: 1.7676799297332764
Validation loss: 2.1424310694458666

Epoch: 6| Step: 11
Training loss: 2.108196496963501
Validation loss: 2.1461212865767942

Epoch: 6| Step: 12
Training loss: 1.6616435050964355
Validation loss: 2.1489751723504837

Epoch: 6| Step: 13
Training loss: 2.32210636138916
Validation loss: 2.1389396677735033

Epoch: 192| Step: 0
Training loss: 1.8255146741867065
Validation loss: 2.1280397061378724

Epoch: 6| Step: 1
Training loss: 1.8983972072601318
Validation loss: 2.1283141823225122

Epoch: 6| Step: 2
Training loss: 1.9248167276382446
Validation loss: 2.1246048917052565

Epoch: 6| Step: 3
Training loss: 2.1651244163513184
Validation loss: 2.1220140405880508

Epoch: 6| Step: 4
Training loss: 2.327866792678833
Validation loss: 2.127755891892218

Epoch: 6| Step: 5
Training loss: 1.572014570236206
Validation loss: 2.1439290969602522

Epoch: 6| Step: 6
Training loss: 1.4888184070587158
Validation loss: 2.155493779848981

Epoch: 6| Step: 7
Training loss: 1.6555418968200684
Validation loss: 2.1748871675101658

Epoch: 6| Step: 8
Training loss: 3.0966479778289795
Validation loss: 2.1736680435877975

Epoch: 6| Step: 9
Training loss: 1.4213060140609741
Validation loss: 2.1808015454200005

Epoch: 6| Step: 10
Training loss: 2.0781469345092773
Validation loss: 2.1757814371457664

Epoch: 6| Step: 11
Training loss: 1.9139333963394165
Validation loss: 2.173031710809277

Epoch: 6| Step: 12
Training loss: 2.3921399116516113
Validation loss: 2.152375385325442

Epoch: 6| Step: 13
Training loss: 1.9373029470443726
Validation loss: 2.146477796698129

Epoch: 193| Step: 0
Training loss: 3.0239109992980957
Validation loss: 2.1314642019169305

Epoch: 6| Step: 1
Training loss: 2.308871030807495
Validation loss: 2.1288130616628997

Epoch: 6| Step: 2
Training loss: 2.0414206981658936
Validation loss: 2.1152223348617554

Epoch: 6| Step: 3
Training loss: 1.2712844610214233
Validation loss: 2.1222479907415246

Epoch: 6| Step: 4
Training loss: 1.9915419816970825
Validation loss: 2.1273064844069944

Epoch: 6| Step: 5
Training loss: 2.1821393966674805
Validation loss: 2.1388605345961866

Epoch: 6| Step: 6
Training loss: 1.8808051347732544
Validation loss: 2.14227315046454

Epoch: 6| Step: 7
Training loss: 2.1163806915283203
Validation loss: 2.119456947490733

Epoch: 6| Step: 8
Training loss: 1.6753852367401123
Validation loss: 2.1387260472902687

Epoch: 6| Step: 9
Training loss: 1.6132034063339233
Validation loss: 2.1386471973952426

Epoch: 6| Step: 10
Training loss: 2.114333152770996
Validation loss: 2.1534595425410936

Epoch: 6| Step: 11
Training loss: 1.9924895763397217
Validation loss: 2.1391062351965133

Epoch: 6| Step: 12
Training loss: 1.7498950958251953
Validation loss: 2.158701060920633

Epoch: 6| Step: 13
Training loss: 1.4759008884429932
Validation loss: 2.1634300114006124

Epoch: 194| Step: 0
Training loss: 1.915757417678833
Validation loss: 2.167224294395857

Epoch: 6| Step: 1
Training loss: 1.8778796195983887
Validation loss: 2.175348604879072

Epoch: 6| Step: 2
Training loss: 2.5210187435150146
Validation loss: 2.1733597914377847

Epoch: 6| Step: 3
Training loss: 1.519381046295166
Validation loss: 2.162127051302182

Epoch: 6| Step: 4
Training loss: 1.827626347541809
Validation loss: 2.169233152943273

Epoch: 6| Step: 5
Training loss: 2.3881258964538574
Validation loss: 2.1835016153192006

Epoch: 6| Step: 6
Training loss: 2.149083137512207
Validation loss: 2.173082519603032

Epoch: 6| Step: 7
Training loss: 2.4565224647521973
Validation loss: 2.166053118244294

Epoch: 6| Step: 8
Training loss: 1.6465492248535156
Validation loss: 2.163858293205179

Epoch: 6| Step: 9
Training loss: 1.3948886394500732
Validation loss: 2.1724562209139586

Epoch: 6| Step: 10
Training loss: 1.4873042106628418
Validation loss: 2.161247653345908

Epoch: 6| Step: 11
Training loss: 2.4718525409698486
Validation loss: 2.1807160838957755

Epoch: 6| Step: 12
Training loss: 1.951441764831543
Validation loss: 2.1653855410955285

Epoch: 6| Step: 13
Training loss: 1.7950263023376465
Validation loss: 2.1549789649184032

Epoch: 195| Step: 0
Training loss: 2.0983052253723145
Validation loss: 2.1539803538271176

Epoch: 6| Step: 1
Training loss: 1.7940711975097656
Validation loss: 2.136069033735542

Epoch: 6| Step: 2
Training loss: 1.9068320989608765
Validation loss: 2.156615436718028

Epoch: 6| Step: 3
Training loss: 2.267059326171875
Validation loss: 2.169162493880077

Epoch: 6| Step: 4
Training loss: 2.072422504425049
Validation loss: 2.161978885691653

Epoch: 6| Step: 5
Training loss: 1.5786211490631104
Validation loss: 2.1737626983273413

Epoch: 6| Step: 6
Training loss: 2.085228443145752
Validation loss: 2.1861084814994567

Epoch: 6| Step: 7
Training loss: 1.4914100170135498
Validation loss: 2.1765229958359913

Epoch: 6| Step: 8
Training loss: 1.5450730323791504
Validation loss: 2.1716560907261346

Epoch: 6| Step: 9
Training loss: 2.1129305362701416
Validation loss: 2.1707716500887306

Epoch: 6| Step: 10
Training loss: 1.9312636852264404
Validation loss: 2.160739706408593

Epoch: 6| Step: 11
Training loss: 2.4734063148498535
Validation loss: 2.1523408684679257

Epoch: 6| Step: 12
Training loss: 1.8652544021606445
Validation loss: 2.1339499719681276

Epoch: 6| Step: 13
Training loss: 2.096036672592163
Validation loss: 2.1122835220829135

Epoch: 196| Step: 0
Training loss: 2.262561798095703
Validation loss: 2.0998295481486986

Epoch: 6| Step: 1
Training loss: 1.382338285446167
Validation loss: 2.0998804799972044

Epoch: 6| Step: 2
Training loss: 1.349576711654663
Validation loss: 2.076012416552472

Epoch: 6| Step: 3
Training loss: 1.178687572479248
Validation loss: 2.0775272410403014

Epoch: 6| Step: 4
Training loss: 1.9978139400482178
Validation loss: 2.1162846883138022

Epoch: 6| Step: 5
Training loss: 2.010288715362549
Validation loss: 2.1314528578071186

Epoch: 6| Step: 6
Training loss: 2.7518131732940674
Validation loss: 2.1740283696882186

Epoch: 6| Step: 7
Training loss: 1.9393397569656372
Validation loss: 2.172353098469396

Epoch: 6| Step: 8
Training loss: 1.8292820453643799
Validation loss: 2.1860820170371764

Epoch: 6| Step: 9
Training loss: 1.8795063495635986
Validation loss: 2.1761525933460524

Epoch: 6| Step: 10
Training loss: 2.116926908493042
Validation loss: 2.1463063865579586

Epoch: 6| Step: 11
Training loss: 2.3873543739318848
Validation loss: 2.1192841401664158

Epoch: 6| Step: 12
Training loss: 2.753434658050537
Validation loss: 2.109141972757155

Epoch: 6| Step: 13
Training loss: 1.8359875679016113
Validation loss: 2.0976110696792603

Epoch: 197| Step: 0
Training loss: 2.063507556915283
Validation loss: 2.1090615410958566

Epoch: 6| Step: 1
Training loss: 2.5450873374938965
Validation loss: 2.1139131771620883

Epoch: 6| Step: 2
Training loss: 1.6073572635650635
Validation loss: 2.109632724074907

Epoch: 6| Step: 3
Training loss: 2.130040168762207
Validation loss: 2.103753697487616

Epoch: 6| Step: 4
Training loss: 2.0527219772338867
Validation loss: 2.1020207866545646

Epoch: 6| Step: 5
Training loss: 2.1005806922912598
Validation loss: 2.09671926242049

Epoch: 6| Step: 6
Training loss: 2.2181496620178223
Validation loss: 2.123591871671779

Epoch: 6| Step: 7
Training loss: 2.3284363746643066
Validation loss: 2.1033040990111647

Epoch: 6| Step: 8
Training loss: 1.515649676322937
Validation loss: 2.1222608781629995

Epoch: 6| Step: 9
Training loss: 2.2095577716827393
Validation loss: 2.142954659718339

Epoch: 6| Step: 10
Training loss: 2.006863832473755
Validation loss: 2.1601147408126504

Epoch: 6| Step: 11
Training loss: 1.7009954452514648
Validation loss: 2.1879738851260115

Epoch: 6| Step: 12
Training loss: 1.3785650730133057
Validation loss: 2.2069902804590042

Epoch: 6| Step: 13
Training loss: 1.4346967935562134
Validation loss: 2.214409553876487

Epoch: 198| Step: 0
Training loss: 2.256920576095581
Validation loss: 2.200139564852561

Epoch: 6| Step: 1
Training loss: 1.869848608970642
Validation loss: 2.1879844921891407

Epoch: 6| Step: 2
Training loss: 1.8436979055404663
Validation loss: 2.196673849577545

Epoch: 6| Step: 3
Training loss: 1.486148476600647
Validation loss: 2.171696844921317

Epoch: 6| Step: 4
Training loss: 2.155343770980835
Validation loss: 2.1671899211022163

Epoch: 6| Step: 5
Training loss: 1.6710784435272217
Validation loss: 2.1588805337106027

Epoch: 6| Step: 6
Training loss: 1.560299277305603
Validation loss: 2.1445111074755268

Epoch: 6| Step: 7
Training loss: 1.9362139701843262
Validation loss: 2.1456271781716296

Epoch: 6| Step: 8
Training loss: 1.4518067836761475
Validation loss: 2.1315861671201644

Epoch: 6| Step: 9
Training loss: 2.797708034515381
Validation loss: 2.1247718116288543

Epoch: 6| Step: 10
Training loss: 1.9277958869934082
Validation loss: 2.1223512106044318

Epoch: 6| Step: 11
Training loss: 1.8391547203063965
Validation loss: 2.1155547211247105

Epoch: 6| Step: 12
Training loss: 2.0430774688720703
Validation loss: 2.1182730966998684

Epoch: 6| Step: 13
Training loss: 2.2660577297210693
Validation loss: 2.1126885439759944

Epoch: 199| Step: 0
Training loss: 1.6350069046020508
Validation loss: 2.1282698236485964

Epoch: 6| Step: 1
Training loss: 1.7848756313323975
Validation loss: 2.1480126252738376

Epoch: 6| Step: 2
Training loss: 1.2792248725891113
Validation loss: 2.1600796330359673

Epoch: 6| Step: 3
Training loss: 1.7298696041107178
Validation loss: 2.1739202891626666

Epoch: 6| Step: 4
Training loss: 2.106694221496582
Validation loss: 2.1881673310392644

Epoch: 6| Step: 5
Training loss: 2.0631794929504395
Validation loss: 2.1962614341448714

Epoch: 6| Step: 6
Training loss: 2.0288872718811035
Validation loss: 2.1759698916507024

Epoch: 6| Step: 7
Training loss: 1.7919334173202515
Validation loss: 2.1643757967538733

Epoch: 6| Step: 8
Training loss: 2.665916919708252
Validation loss: 2.14791517103872

Epoch: 6| Step: 9
Training loss: 1.3236478567123413
Validation loss: 2.1532577699230564

Epoch: 6| Step: 10
Training loss: 2.766566753387451
Validation loss: 2.140548972673314

Epoch: 6| Step: 11
Training loss: 1.6662464141845703
Validation loss: 2.117426795344199

Epoch: 6| Step: 12
Training loss: 2.0575942993164062
Validation loss: 2.101801090343024

Epoch: 6| Step: 13
Training loss: 2.0363168716430664
Validation loss: 2.1007702273707234

Epoch: 200| Step: 0
Training loss: 2.0337636470794678
Validation loss: 2.084212559525685

Epoch: 6| Step: 1
Training loss: 1.5029840469360352
Validation loss: 2.090761856366229

Epoch: 6| Step: 2
Training loss: 1.718734860420227
Validation loss: 2.099353418555311

Epoch: 6| Step: 3
Training loss: 1.4290030002593994
Validation loss: 2.095627829592715

Epoch: 6| Step: 4
Training loss: 2.1834819316864014
Validation loss: 2.111387016952679

Epoch: 6| Step: 5
Training loss: 2.388331413269043
Validation loss: 2.111408641261439

Epoch: 6| Step: 6
Training loss: 1.3714920282363892
Validation loss: 2.1273281112793954

Epoch: 6| Step: 7
Training loss: 2.1831159591674805
Validation loss: 2.1125757848062823

Epoch: 6| Step: 8
Training loss: 1.8444766998291016
Validation loss: 2.1104217319078344

Epoch: 6| Step: 9
Training loss: 2.145850896835327
Validation loss: 2.1048285204877137

Epoch: 6| Step: 10
Training loss: 1.5895917415618896
Validation loss: 2.115117843433093

Epoch: 6| Step: 11
Training loss: 1.9414944648742676
Validation loss: 2.084310450861531

Epoch: 6| Step: 12
Training loss: 2.3459038734436035
Validation loss: 2.110078127153458

Epoch: 6| Step: 13
Training loss: 2.1591787338256836
Validation loss: 2.11052595415423

Testing loss: 2.2284094280666773

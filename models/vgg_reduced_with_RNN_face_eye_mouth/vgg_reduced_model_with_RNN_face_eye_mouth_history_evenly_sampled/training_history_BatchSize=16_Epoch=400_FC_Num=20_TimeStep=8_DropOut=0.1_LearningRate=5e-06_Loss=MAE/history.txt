Epoch: 1| Step: 0
Training loss: 4.711242198944092
Validation loss: 5.161544420385874

Epoch: 6| Step: 1
Training loss: 3.4186832904815674
Validation loss: 5.157466878173172

Epoch: 6| Step: 2
Training loss: 5.685400009155273
Validation loss: 5.153647909882248

Epoch: 6| Step: 3
Training loss: 5.570469856262207
Validation loss: 5.150204966145177

Epoch: 6| Step: 4
Training loss: 5.302806854248047
Validation loss: 5.146807373210948

Epoch: 6| Step: 5
Training loss: 4.986944675445557
Validation loss: 5.143433770825786

Epoch: 6| Step: 6
Training loss: 6.051490783691406
Validation loss: 5.1395278643536315

Epoch: 6| Step: 7
Training loss: 4.9222259521484375
Validation loss: 5.135675179061069

Epoch: 6| Step: 8
Training loss: 4.43036413192749
Validation loss: 5.1316997671640046

Epoch: 6| Step: 9
Training loss: 4.007992744445801
Validation loss: 5.127617113051876

Epoch: 6| Step: 10
Training loss: 5.1725053787231445
Validation loss: 5.12318774192564

Epoch: 6| Step: 11
Training loss: 4.861969947814941
Validation loss: 5.1189152656062955

Epoch: 6| Step: 12
Training loss: 4.883142948150635
Validation loss: 5.114303004357122

Epoch: 6| Step: 13
Training loss: 5.171172142028809
Validation loss: 5.109599231391825

Epoch: 2| Step: 0
Training loss: 5.112146377563477
Validation loss: 5.104566122895928

Epoch: 6| Step: 1
Training loss: 3.1955881118774414
Validation loss: 5.099191968159009

Epoch: 6| Step: 2
Training loss: 5.298401355743408
Validation loss: 5.0937374330336045

Epoch: 6| Step: 3
Training loss: 4.371894836425781
Validation loss: 5.088223380427206

Epoch: 6| Step: 4
Training loss: 5.538542747497559
Validation loss: 5.082301811505389

Epoch: 6| Step: 5
Training loss: 4.836437225341797
Validation loss: 5.075775766885409

Epoch: 6| Step: 6
Training loss: 4.744396209716797
Validation loss: 5.068781847594886

Epoch: 6| Step: 7
Training loss: 5.834667205810547
Validation loss: 5.062623470060287

Epoch: 6| Step: 8
Training loss: 5.9318461418151855
Validation loss: 5.055178473072667

Epoch: 6| Step: 9
Training loss: 3.6950221061706543
Validation loss: 5.047571166869132

Epoch: 6| Step: 10
Training loss: 5.907309532165527
Validation loss: 5.039561989486859

Epoch: 6| Step: 11
Training loss: 5.068074703216553
Validation loss: 5.0309005347631315

Epoch: 6| Step: 12
Training loss: 4.05026388168335
Validation loss: 5.0223106158676964

Epoch: 6| Step: 13
Training loss: 4.122661113739014
Validation loss: 5.0130084612036265

Epoch: 3| Step: 0
Training loss: 4.484797954559326
Validation loss: 5.003355103154337

Epoch: 6| Step: 1
Training loss: 6.587594985961914
Validation loss: 4.993092449762488

Epoch: 6| Step: 2
Training loss: 5.028224468231201
Validation loss: 4.982208574971845

Epoch: 6| Step: 3
Training loss: 5.379278659820557
Validation loss: 4.971168302720593

Epoch: 6| Step: 4
Training loss: 3.4101905822753906
Validation loss: 4.959656489792691

Epoch: 6| Step: 5
Training loss: 5.31703519821167
Validation loss: 4.947539908911592

Epoch: 6| Step: 6
Training loss: 3.5997164249420166
Validation loss: 4.935200157985892

Epoch: 6| Step: 7
Training loss: 5.319962024688721
Validation loss: 4.921468539904523

Epoch: 6| Step: 8
Training loss: 4.342284202575684
Validation loss: 4.907418738129318

Epoch: 6| Step: 9
Training loss: 5.383397102355957
Validation loss: 4.892596931867702

Epoch: 6| Step: 10
Training loss: 3.6976962089538574
Validation loss: 4.877609232420562

Epoch: 6| Step: 11
Training loss: 3.507736921310425
Validation loss: 4.861149726375457

Epoch: 6| Step: 12
Training loss: 4.793630123138428
Validation loss: 4.84419446863154

Epoch: 6| Step: 13
Training loss: 5.5396037101745605
Validation loss: 4.827095277847782

Epoch: 4| Step: 0
Training loss: 5.174777030944824
Validation loss: 4.8072113631874

Epoch: 6| Step: 1
Training loss: 5.715608596801758
Validation loss: 4.788001270704372

Epoch: 6| Step: 2
Training loss: 5.217502593994141
Validation loss: 4.767414595491143

Epoch: 6| Step: 3
Training loss: 5.683753967285156
Validation loss: 4.745331528366253

Epoch: 6| Step: 4
Training loss: 4.074062347412109
Validation loss: 4.722778422858125

Epoch: 6| Step: 5
Training loss: 4.210439205169678
Validation loss: 4.700062651788035

Epoch: 6| Step: 6
Training loss: 3.660348415374756
Validation loss: 4.676083831376927

Epoch: 6| Step: 7
Training loss: 3.6083340644836426
Validation loss: 4.652022587355747

Epoch: 6| Step: 8
Training loss: 4.132717132568359
Validation loss: 4.627241965263121

Epoch: 6| Step: 9
Training loss: 3.625328302383423
Validation loss: 4.600780835715673

Epoch: 6| Step: 10
Training loss: 5.176919937133789
Validation loss: 4.573864331809423

Epoch: 6| Step: 11
Training loss: 4.5618896484375
Validation loss: 4.545573188412574

Epoch: 6| Step: 12
Training loss: 4.188180446624756
Validation loss: 4.518179257710774

Epoch: 6| Step: 13
Training loss: 2.3453261852264404
Validation loss: 4.488685874528782

Epoch: 5| Step: 0
Training loss: 3.999094009399414
Validation loss: 4.459189153486682

Epoch: 6| Step: 1
Training loss: 4.0411787033081055
Validation loss: 4.430699784268615

Epoch: 6| Step: 2
Training loss: 3.89089298248291
Validation loss: 4.401310500278268

Epoch: 6| Step: 3
Training loss: 5.455167293548584
Validation loss: 4.3726526280885105

Epoch: 6| Step: 4
Training loss: 3.722943067550659
Validation loss: 4.3450991927936515

Epoch: 6| Step: 5
Training loss: 4.342867851257324
Validation loss: 4.317239366551881

Epoch: 6| Step: 6
Training loss: 3.1799890995025635
Validation loss: 4.288943377874231

Epoch: 6| Step: 7
Training loss: 4.45115852355957
Validation loss: 4.26347328001453

Epoch: 6| Step: 8
Training loss: 3.214858293533325
Validation loss: 4.238766685608895

Epoch: 6| Step: 9
Training loss: 3.7411956787109375
Validation loss: 4.214868858296384

Epoch: 6| Step: 10
Training loss: 4.210110664367676
Validation loss: 4.192394843665502

Epoch: 6| Step: 11
Training loss: 4.323617935180664
Validation loss: 4.169785755936817

Epoch: 6| Step: 12
Training loss: 4.266776084899902
Validation loss: 4.146004707582535

Epoch: 6| Step: 13
Training loss: 4.655388355255127
Validation loss: 4.12485691296157

Epoch: 6| Step: 0
Training loss: 3.0518383979797363
Validation loss: 4.103134119382468

Epoch: 6| Step: 1
Training loss: 3.7188832759857178
Validation loss: 4.080219279053391

Epoch: 6| Step: 2
Training loss: 3.410989761352539
Validation loss: 4.0559658594028924

Epoch: 6| Step: 3
Training loss: 4.463547706604004
Validation loss: 4.030057591776694

Epoch: 6| Step: 4
Training loss: 4.467534065246582
Validation loss: 4.009281307138423

Epoch: 6| Step: 5
Training loss: 3.8306634426116943
Validation loss: 3.9889313841378815

Epoch: 6| Step: 6
Training loss: 4.577776908874512
Validation loss: 3.9673199961262364

Epoch: 6| Step: 7
Training loss: 3.7962517738342285
Validation loss: 3.9473761435477965

Epoch: 6| Step: 8
Training loss: 2.697462558746338
Validation loss: 3.929766901077763

Epoch: 6| Step: 9
Training loss: 2.9014763832092285
Validation loss: 3.9127916674460135

Epoch: 6| Step: 10
Training loss: 3.768371343612671
Validation loss: 3.8977607655268844

Epoch: 6| Step: 11
Training loss: 4.651754379272461
Validation loss: 3.8821900634355444

Epoch: 6| Step: 12
Training loss: 4.100214004516602
Validation loss: 3.869098319802233

Epoch: 6| Step: 13
Training loss: 4.368561744689941
Validation loss: 3.8547025444687053

Epoch: 7| Step: 0
Training loss: 2.739884614944458
Validation loss: 3.8430813102311987

Epoch: 6| Step: 1
Training loss: 4.318532943725586
Validation loss: 3.829587003236176

Epoch: 6| Step: 2
Training loss: 4.1859025955200195
Validation loss: 3.8188014440639044

Epoch: 6| Step: 3
Training loss: 4.342316150665283
Validation loss: 3.80498004985112

Epoch: 6| Step: 4
Training loss: 3.830251932144165
Validation loss: 3.7957591933588826

Epoch: 6| Step: 5
Training loss: 3.390556812286377
Validation loss: 3.781362182350569

Epoch: 6| Step: 6
Training loss: 2.752124786376953
Validation loss: 3.7720510805806806

Epoch: 6| Step: 7
Training loss: 3.351400852203369
Validation loss: 3.763581565631333

Epoch: 6| Step: 8
Training loss: 3.4437499046325684
Validation loss: 3.753448717055782

Epoch: 6| Step: 9
Training loss: 3.6957647800445557
Validation loss: 3.7462440280504126

Epoch: 6| Step: 10
Training loss: 4.195136547088623
Validation loss: 3.735665644368818

Epoch: 6| Step: 11
Training loss: 4.734396934509277
Validation loss: 3.725862426142539

Epoch: 6| Step: 12
Training loss: 2.2726690769195557
Validation loss: 3.7154612233561854

Epoch: 6| Step: 13
Training loss: 4.167749404907227
Validation loss: 3.7076491181568434

Epoch: 8| Step: 0
Training loss: 4.470407485961914
Validation loss: 3.6982266467104674

Epoch: 6| Step: 1
Training loss: 4.065535545349121
Validation loss: 3.687480649640483

Epoch: 6| Step: 2
Training loss: 3.414243221282959
Validation loss: 3.6774752063135945

Epoch: 6| Step: 3
Training loss: 2.471029281616211
Validation loss: 3.669231963414018

Epoch: 6| Step: 4
Training loss: 4.134212970733643
Validation loss: 3.6597075411068496

Epoch: 6| Step: 5
Training loss: 3.3572304248809814
Validation loss: 3.651170543445054

Epoch: 6| Step: 6
Training loss: 4.169970512390137
Validation loss: 3.643523636684623

Epoch: 6| Step: 7
Training loss: 2.5097427368164062
Validation loss: 3.634660090169599

Epoch: 6| Step: 8
Training loss: 3.7616355419158936
Validation loss: 3.6279759637771116

Epoch: 6| Step: 9
Training loss: 3.083850860595703
Validation loss: 3.619797096457533

Epoch: 6| Step: 10
Training loss: 4.345050811767578
Validation loss: 3.613234653267809

Epoch: 6| Step: 11
Training loss: 2.7037036418914795
Validation loss: 3.6060586590920725

Epoch: 6| Step: 12
Training loss: 3.438567638397217
Validation loss: 3.5974877213919036

Epoch: 6| Step: 13
Training loss: 4.017290115356445
Validation loss: 3.591038744936707

Epoch: 9| Step: 0
Training loss: 3.7077231407165527
Validation loss: 3.585063103706606

Epoch: 6| Step: 1
Training loss: 2.529660940170288
Validation loss: 3.5777391925934823

Epoch: 6| Step: 2
Training loss: 3.734346628189087
Validation loss: 3.5698266080630723

Epoch: 6| Step: 3
Training loss: 2.833184242248535
Validation loss: 3.5658256161597466

Epoch: 6| Step: 4
Training loss: 4.5475568771362305
Validation loss: 3.557777189439343

Epoch: 6| Step: 5
Training loss: 4.561916828155518
Validation loss: 3.550697511242282

Epoch: 6| Step: 6
Training loss: 2.801469087600708
Validation loss: 3.5446554255741898

Epoch: 6| Step: 7
Training loss: 1.837669014930725
Validation loss: 3.539072154670633

Epoch: 6| Step: 8
Training loss: 3.574002742767334
Validation loss: 3.5342192034567557

Epoch: 6| Step: 9
Training loss: 4.399044990539551
Validation loss: 3.5299860738938853

Epoch: 6| Step: 10
Training loss: 3.8409321308135986
Validation loss: 3.5251403470193186

Epoch: 6| Step: 11
Training loss: 3.0576348304748535
Validation loss: 3.517931527988885

Epoch: 6| Step: 12
Training loss: 3.275186061859131
Validation loss: 3.5140007644571285

Epoch: 6| Step: 13
Training loss: 4.264776229858398
Validation loss: 3.508728560581002

Epoch: 10| Step: 0
Training loss: 3.4942827224731445
Validation loss: 3.5022381556931363

Epoch: 6| Step: 1
Training loss: 3.6882448196411133
Validation loss: 3.4974245486720914

Epoch: 6| Step: 2
Training loss: 3.5477118492126465
Validation loss: 3.4939272506262666

Epoch: 6| Step: 3
Training loss: 2.9349472522735596
Validation loss: 3.488173407893027

Epoch: 6| Step: 4
Training loss: 3.782381057739258
Validation loss: 3.4834210436831237

Epoch: 6| Step: 5
Training loss: 2.970674753189087
Validation loss: 3.481304435319798

Epoch: 6| Step: 6
Training loss: 3.2264046669006348
Validation loss: 3.4779127900318434

Epoch: 6| Step: 7
Training loss: 3.925131320953369
Validation loss: 3.473026062852593

Epoch: 6| Step: 8
Training loss: 4.0250043869018555
Validation loss: 3.4672403489389727

Epoch: 6| Step: 9
Training loss: 2.6537342071533203
Validation loss: 3.4623854698673373

Epoch: 6| Step: 10
Training loss: 3.4705076217651367
Validation loss: 3.4581380736443306

Epoch: 6| Step: 11
Training loss: 3.011929988861084
Validation loss: 3.4530137790146695

Epoch: 6| Step: 12
Training loss: 3.422935724258423
Validation loss: 3.4488061730579664

Epoch: 6| Step: 13
Training loss: 3.771015167236328
Validation loss: 3.444745017636207

Epoch: 11| Step: 0
Training loss: 2.7388806343078613
Validation loss: 3.4406747433447067

Epoch: 6| Step: 1
Training loss: 3.3734750747680664
Validation loss: 3.4359293317282074

Epoch: 6| Step: 2
Training loss: 3.4271161556243896
Validation loss: 3.4296539829623316

Epoch: 6| Step: 3
Training loss: 4.210296154022217
Validation loss: 3.4279437270215762

Epoch: 6| Step: 4
Training loss: 3.077874183654785
Validation loss: 3.4207524971295427

Epoch: 6| Step: 5
Training loss: 2.5619263648986816
Validation loss: 3.418105766337405

Epoch: 6| Step: 6
Training loss: 3.8530495166778564
Validation loss: 3.413699209049184

Epoch: 6| Step: 7
Training loss: 2.578965663909912
Validation loss: 3.4077743894310406

Epoch: 6| Step: 8
Training loss: 3.722315788269043
Validation loss: 3.4024546274574856

Epoch: 6| Step: 9
Training loss: 3.848635196685791
Validation loss: 3.398529165534563

Epoch: 6| Step: 10
Training loss: 2.8346025943756104
Validation loss: 3.3949132965457056

Epoch: 6| Step: 11
Training loss: 4.630527496337891
Validation loss: 3.3882139318732807

Epoch: 6| Step: 12
Training loss: 3.423107624053955
Validation loss: 3.3825274923796296

Epoch: 6| Step: 13
Training loss: 2.3001317977905273
Validation loss: 3.3793144610620316

Epoch: 12| Step: 0
Training loss: 3.51080322265625
Validation loss: 3.373533853920557

Epoch: 6| Step: 1
Training loss: 3.373382091522217
Validation loss: 3.369787580223494

Epoch: 6| Step: 2
Training loss: 2.6879329681396484
Validation loss: 3.362450950889177

Epoch: 6| Step: 3
Training loss: 2.731100559234619
Validation loss: 3.359745628090315

Epoch: 6| Step: 4
Training loss: 4.099352836608887
Validation loss: 3.352755977261451

Epoch: 6| Step: 5
Training loss: 3.7676992416381836
Validation loss: 3.3495588020611833

Epoch: 6| Step: 6
Training loss: 2.829129695892334
Validation loss: 3.3447970062173824

Epoch: 6| Step: 7
Training loss: 3.1600613594055176
Validation loss: 3.339592479890393

Epoch: 6| Step: 8
Training loss: 3.8543014526367188
Validation loss: 3.3338360812074397

Epoch: 6| Step: 9
Training loss: 3.84446120262146
Validation loss: 3.3288984324342463

Epoch: 6| Step: 10
Training loss: 2.9738261699676514
Validation loss: 3.325236238459105

Epoch: 6| Step: 11
Training loss: 3.4506492614746094
Validation loss: 3.31862308133033

Epoch: 6| Step: 12
Training loss: 3.100062370300293
Validation loss: 3.31492857779226

Epoch: 6| Step: 13
Training loss: 2.542424440383911
Validation loss: 3.3104105380273636

Epoch: 13| Step: 0
Training loss: 3.7471299171447754
Validation loss: 3.305025239144602

Epoch: 6| Step: 1
Training loss: 3.4274134635925293
Validation loss: 3.30029272776778

Epoch: 6| Step: 2
Training loss: 3.1161956787109375
Validation loss: 3.2968304900712866

Epoch: 6| Step: 3
Training loss: 1.6988282203674316
Validation loss: 3.2906529057410454

Epoch: 6| Step: 4
Training loss: 3.623229503631592
Validation loss: 3.2865856642364175

Epoch: 6| Step: 5
Training loss: 2.974484920501709
Validation loss: 3.286508126925397

Epoch: 6| Step: 6
Training loss: 3.9355883598327637
Validation loss: 3.2816024006053968

Epoch: 6| Step: 7
Training loss: 3.1916894912719727
Validation loss: 3.2779366995698664

Epoch: 6| Step: 8
Training loss: 3.177666187286377
Validation loss: 3.276040707865069

Epoch: 6| Step: 9
Training loss: 2.7182986736297607
Validation loss: 3.270752876035629

Epoch: 6| Step: 10
Training loss: 2.6640310287475586
Validation loss: 3.2676622406128915

Epoch: 6| Step: 11
Training loss: 3.0416126251220703
Validation loss: 3.263867534616942

Epoch: 6| Step: 12
Training loss: 3.8852272033691406
Validation loss: 3.2604822804850917

Epoch: 6| Step: 13
Training loss: 5.207001209259033
Validation loss: 3.256889740626017

Epoch: 14| Step: 0
Training loss: 3.7501120567321777
Validation loss: 3.2514922567593154

Epoch: 6| Step: 1
Training loss: 2.323341131210327
Validation loss: 3.2481739008298485

Epoch: 6| Step: 2
Training loss: 3.4691550731658936
Validation loss: 3.2437186292422715

Epoch: 6| Step: 3
Training loss: 2.9755194187164307
Validation loss: 3.2379061945023073

Epoch: 6| Step: 4
Training loss: 2.6339282989501953
Validation loss: 3.2358740222069526

Epoch: 6| Step: 5
Training loss: 2.8353402614593506
Validation loss: 3.2311527985398487

Epoch: 6| Step: 6
Training loss: 3.668558120727539
Validation loss: 3.226871890406455

Epoch: 6| Step: 7
Training loss: 3.024390697479248
Validation loss: 3.2235589258132444

Epoch: 6| Step: 8
Training loss: 3.502967596054077
Validation loss: 3.2165181611173894

Epoch: 6| Step: 9
Training loss: 3.8641200065612793
Validation loss: 3.212961332772368

Epoch: 6| Step: 10
Training loss: 3.6039698123931885
Validation loss: 3.210709653874879

Epoch: 6| Step: 11
Training loss: 2.999765396118164
Validation loss: 3.2055764839213383

Epoch: 6| Step: 12
Training loss: 3.1721789836883545
Validation loss: 3.2001624056088027

Epoch: 6| Step: 13
Training loss: 3.1521284580230713
Validation loss: 3.1954695127343618

Epoch: 15| Step: 0
Training loss: 3.5122604370117188
Validation loss: 3.1914395619464178

Epoch: 6| Step: 1
Training loss: 3.152771472930908
Validation loss: 3.1893021214392876

Epoch: 6| Step: 2
Training loss: 3.651581287384033
Validation loss: 3.1837367908928984

Epoch: 6| Step: 3
Training loss: 4.1016740798950195
Validation loss: 3.1805223726457164

Epoch: 6| Step: 4
Training loss: 2.4554381370544434
Validation loss: 3.176120045364544

Epoch: 6| Step: 5
Training loss: 4.04458475112915
Validation loss: 3.1743565913169616

Epoch: 6| Step: 6
Training loss: 3.374795436859131
Validation loss: 3.169396772179552

Epoch: 6| Step: 7
Training loss: 2.1739253997802734
Validation loss: 3.16923128661289

Epoch: 6| Step: 8
Training loss: 3.0478549003601074
Validation loss: 3.1636548170479397

Epoch: 6| Step: 9
Training loss: 2.75907564163208
Validation loss: 3.159806131034769

Epoch: 6| Step: 10
Training loss: 3.4426984786987305
Validation loss: 3.1583770039261028

Epoch: 6| Step: 11
Training loss: 2.7880234718322754
Validation loss: 3.1559984863445325

Epoch: 6| Step: 12
Training loss: 3.285719871520996
Validation loss: 3.1514092286427817

Epoch: 6| Step: 13
Training loss: 2.333672523498535
Validation loss: 3.1473075779535438

Epoch: 16| Step: 0
Training loss: 1.7497211694717407
Validation loss: 3.1452593572678103

Epoch: 6| Step: 1
Training loss: 3.300092935562134
Validation loss: 3.139951172695365

Epoch: 6| Step: 2
Training loss: 3.535984516143799
Validation loss: 3.137629588445028

Epoch: 6| Step: 3
Training loss: 2.864269256591797
Validation loss: 3.136312330922773

Epoch: 6| Step: 4
Training loss: 2.8932571411132812
Validation loss: 3.1351150184549312

Epoch: 6| Step: 5
Training loss: 3.6311004161834717
Validation loss: 3.129392800792571

Epoch: 6| Step: 6
Training loss: 2.914523124694824
Validation loss: 3.129770589131181

Epoch: 6| Step: 7
Training loss: 4.116034984588623
Validation loss: 3.1257187115248812

Epoch: 6| Step: 8
Training loss: 4.197956085205078
Validation loss: 3.122503506240024

Epoch: 6| Step: 9
Training loss: 3.08907413482666
Validation loss: 3.1233872905854256

Epoch: 6| Step: 10
Training loss: 3.7208476066589355
Validation loss: 3.1172763480935046

Epoch: 6| Step: 11
Training loss: 2.305482864379883
Validation loss: 3.1148726453063307

Epoch: 6| Step: 12
Training loss: 3.3018510341644287
Validation loss: 3.1131748742954706

Epoch: 6| Step: 13
Training loss: 1.924659252166748
Validation loss: 3.110070615686396

Epoch: 17| Step: 0
Training loss: 3.9541401863098145
Validation loss: 3.109729646354593

Epoch: 6| Step: 1
Training loss: 2.8263485431671143
Validation loss: 3.108762184778849

Epoch: 6| Step: 2
Training loss: 3.384199857711792
Validation loss: 3.103526674291139

Epoch: 6| Step: 3
Training loss: 3.6850829124450684
Validation loss: 3.10031041278634

Epoch: 6| Step: 4
Training loss: 3.1221137046813965
Validation loss: 3.1002527949630574

Epoch: 6| Step: 5
Training loss: 1.838900089263916
Validation loss: 3.097434961667625

Epoch: 6| Step: 6
Training loss: 3.515392541885376
Validation loss: 3.0947568775505148

Epoch: 6| Step: 7
Training loss: 2.8681750297546387
Validation loss: 3.0933837147169214

Epoch: 6| Step: 8
Training loss: 2.551079511642456
Validation loss: 3.091613333712342

Epoch: 6| Step: 9
Training loss: 2.609511137008667
Validation loss: 3.0899482901378343

Epoch: 6| Step: 10
Training loss: 3.1672089099884033
Validation loss: 3.0873299491020942

Epoch: 6| Step: 11
Training loss: 2.742152690887451
Validation loss: 3.0870123140273558

Epoch: 6| Step: 12
Training loss: 3.8534393310546875
Validation loss: 3.0846706000707482

Epoch: 6| Step: 13
Training loss: 4.041616439819336
Validation loss: 3.0820453295143704

Epoch: 18| Step: 0
Training loss: 3.2553632259368896
Validation loss: 3.080939938945155

Epoch: 6| Step: 1
Training loss: 2.599357843399048
Validation loss: 3.0792704371995825

Epoch: 6| Step: 2
Training loss: 4.063989639282227
Validation loss: 3.076825608489334

Epoch: 6| Step: 3
Training loss: 3.410109519958496
Validation loss: 3.07423060940158

Epoch: 6| Step: 4
Training loss: 2.510138511657715
Validation loss: 3.071745252096525

Epoch: 6| Step: 5
Training loss: 2.86692214012146
Validation loss: 3.069096255046065

Epoch: 6| Step: 6
Training loss: 3.508511781692505
Validation loss: 3.0667822258446806

Epoch: 6| Step: 7
Training loss: 3.1318445205688477
Validation loss: 3.065235673740346

Epoch: 6| Step: 8
Training loss: 2.238354206085205
Validation loss: 3.0629560562872116

Epoch: 6| Step: 9
Training loss: 2.5169572830200195
Validation loss: 3.062444107506865

Epoch: 6| Step: 10
Training loss: 3.2838923931121826
Validation loss: 3.058423129461145

Epoch: 6| Step: 11
Training loss: 4.069991111755371
Validation loss: 3.057786585182272

Epoch: 6| Step: 12
Training loss: 3.3221487998962402
Validation loss: 3.05504842727415

Epoch: 6| Step: 13
Training loss: 2.4378271102905273
Validation loss: 3.0528327342002624

Epoch: 19| Step: 0
Training loss: 3.616610527038574
Validation loss: 3.0534357896415134

Epoch: 6| Step: 1
Training loss: 3.1338462829589844
Validation loss: 3.0571377661920365

Epoch: 6| Step: 2
Training loss: 1.9393589496612549
Validation loss: 3.056242714646042

Epoch: 6| Step: 3
Training loss: 1.8711930513381958
Validation loss: 3.050587728459348

Epoch: 6| Step: 4
Training loss: 3.7035818099975586
Validation loss: 3.0463343307536137

Epoch: 6| Step: 5
Training loss: 2.1016898155212402
Validation loss: 3.044151600971017

Epoch: 6| Step: 6
Training loss: 3.8641204833984375
Validation loss: 3.0453011579411005

Epoch: 6| Step: 7
Training loss: 4.075375080108643
Validation loss: 3.043426844381517

Epoch: 6| Step: 8
Training loss: 2.2467846870422363
Validation loss: 3.0378764316599858

Epoch: 6| Step: 9
Training loss: 3.761190891265869
Validation loss: 3.037284699819421

Epoch: 6| Step: 10
Training loss: 3.5583114624023438
Validation loss: 3.035088575014504

Epoch: 6| Step: 11
Training loss: 3.168107271194458
Validation loss: 3.0320237118710756

Epoch: 6| Step: 12
Training loss: 3.200589179992676
Validation loss: 3.030001268591932

Epoch: 6| Step: 13
Training loss: 3.0146777629852295
Validation loss: 3.0277185183699413

Epoch: 20| Step: 0
Training loss: 2.169440984725952
Validation loss: 3.0269813973416566

Epoch: 6| Step: 1
Training loss: 3.015214443206787
Validation loss: 3.0249789530231106

Epoch: 6| Step: 2
Training loss: 3.413456678390503
Validation loss: 3.0240168161289667

Epoch: 6| Step: 3
Training loss: 2.5633151531219482
Validation loss: 3.0200082127765944

Epoch: 6| Step: 4
Training loss: 2.5895042419433594
Validation loss: 3.0177308231271724

Epoch: 6| Step: 5
Training loss: 3.1497979164123535
Validation loss: 3.0169672914730605

Epoch: 6| Step: 6
Training loss: 3.671351194381714
Validation loss: 3.0150731840441303

Epoch: 6| Step: 7
Training loss: 4.189948081970215
Validation loss: 3.013149704984439

Epoch: 6| Step: 8
Training loss: 1.9937388896942139
Validation loss: 3.0104975879833265

Epoch: 6| Step: 9
Training loss: 3.443904161453247
Validation loss: 3.0061029003512476

Epoch: 6| Step: 10
Training loss: 2.600457191467285
Validation loss: 3.0053841093535065

Epoch: 6| Step: 11
Training loss: 3.1187586784362793
Validation loss: 3.002989076798962

Epoch: 6| Step: 12
Training loss: 4.3135504722595215
Validation loss: 3.002306604898104

Epoch: 6| Step: 13
Training loss: 2.5684447288513184
Validation loss: 2.999384944156934

Epoch: 21| Step: 0
Training loss: 3.5856473445892334
Validation loss: 2.9948788817210863

Epoch: 6| Step: 1
Training loss: 3.5202810764312744
Validation loss: 2.99365432031693

Epoch: 6| Step: 2
Training loss: 3.0241289138793945
Validation loss: 2.9929726380173878

Epoch: 6| Step: 3
Training loss: 2.5020010471343994
Validation loss: 2.9934344701869513

Epoch: 6| Step: 4
Training loss: 2.0316715240478516
Validation loss: 2.988213062286377

Epoch: 6| Step: 5
Training loss: 2.600827932357788
Validation loss: 2.9854607530819472

Epoch: 6| Step: 6
Training loss: 3.909611463546753
Validation loss: 2.985240085150606

Epoch: 6| Step: 7
Training loss: 2.6348211765289307
Validation loss: 2.984240129429807

Epoch: 6| Step: 8
Training loss: 2.360757350921631
Validation loss: 2.9804630125722578

Epoch: 6| Step: 9
Training loss: 3.173272132873535
Validation loss: 2.9749639752090618

Epoch: 6| Step: 10
Training loss: 3.3324973583221436
Validation loss: 2.9760596059983775

Epoch: 6| Step: 11
Training loss: 3.649580240249634
Validation loss: 2.971867602358582

Epoch: 6| Step: 12
Training loss: 3.838886260986328
Validation loss: 2.968673259981217

Epoch: 6| Step: 13
Training loss: 2.1963071823120117
Validation loss: 2.966745061259116

Epoch: 22| Step: 0
Training loss: 4.167038440704346
Validation loss: 2.9765210202945176

Epoch: 6| Step: 1
Training loss: 2.262681722640991
Validation loss: 2.964672250132407

Epoch: 6| Step: 2
Training loss: 2.590606212615967
Validation loss: 2.9618700806812575

Epoch: 6| Step: 3
Training loss: 3.134782075881958
Validation loss: 2.9609878422111593

Epoch: 6| Step: 4
Training loss: 2.9304752349853516
Validation loss: 2.9607095872202227

Epoch: 6| Step: 5
Training loss: 2.414073944091797
Validation loss: 2.9610133811991703

Epoch: 6| Step: 6
Training loss: 2.4575979709625244
Validation loss: 2.9577406632002963

Epoch: 6| Step: 7
Training loss: 2.5640945434570312
Validation loss: 2.9558839259609098

Epoch: 6| Step: 8
Training loss: 3.6583175659179688
Validation loss: 2.952722841693509

Epoch: 6| Step: 9
Training loss: 3.487534523010254
Validation loss: 2.952086733233544

Epoch: 6| Step: 10
Training loss: 3.563081979751587
Validation loss: 2.949316568272088

Epoch: 6| Step: 11
Training loss: 3.621356248855591
Validation loss: 2.948117079273347

Epoch: 6| Step: 12
Training loss: 2.060215950012207
Validation loss: 2.944758002476026

Epoch: 6| Step: 13
Training loss: 4.011128902435303
Validation loss: 2.9425317113117506

Epoch: 23| Step: 0
Training loss: 3.1317837238311768
Validation loss: 2.9402946579840874

Epoch: 6| Step: 1
Training loss: 3.252776622772217
Validation loss: 2.937055295513522

Epoch: 6| Step: 2
Training loss: 2.6427369117736816
Validation loss: 2.9341091135496735

Epoch: 6| Step: 3
Training loss: 2.9733147621154785
Validation loss: 2.93148196640835

Epoch: 6| Step: 4
Training loss: 3.6794087886810303
Validation loss: 2.9289536450498845

Epoch: 6| Step: 5
Training loss: 2.1154873371124268
Validation loss: 2.927852287087389

Epoch: 6| Step: 6
Training loss: 3.1255855560302734
Validation loss: 2.926661752885388

Epoch: 6| Step: 7
Training loss: 2.4086544513702393
Validation loss: 2.9273413765814995

Epoch: 6| Step: 8
Training loss: 3.130835771560669
Validation loss: 2.9277473342034126

Epoch: 6| Step: 9
Training loss: 3.052299976348877
Validation loss: 2.925122445629489

Epoch: 6| Step: 10
Training loss: 2.4075682163238525
Validation loss: 2.922093786219115

Epoch: 6| Step: 11
Training loss: 3.3430335521698
Validation loss: 2.9208670662295435

Epoch: 6| Step: 12
Training loss: 3.5377206802368164
Validation loss: 2.9200555586045787

Epoch: 6| Step: 13
Training loss: 3.74656081199646
Validation loss: 2.916394882304694

Epoch: 24| Step: 0
Training loss: 3.3159890174865723
Validation loss: 2.912340300057524

Epoch: 6| Step: 1
Training loss: 2.3559062480926514
Validation loss: 2.9111802013971473

Epoch: 6| Step: 2
Training loss: 2.3439667224884033
Validation loss: 2.9128047420132543

Epoch: 6| Step: 3
Training loss: 2.985626220703125
Validation loss: 2.9110730950550368

Epoch: 6| Step: 4
Training loss: 3.102350950241089
Validation loss: 2.9103759488751813

Epoch: 6| Step: 5
Training loss: 3.35406494140625
Validation loss: 2.908406134574644

Epoch: 6| Step: 6
Training loss: 3.3667376041412354
Validation loss: 2.9092933567621375

Epoch: 6| Step: 7
Training loss: 3.5835413932800293
Validation loss: 2.907148112532913

Epoch: 6| Step: 8
Training loss: 3.322845935821533
Validation loss: 2.9051230902312906

Epoch: 6| Step: 9
Training loss: 3.3091065883636475
Validation loss: 2.9047205883969545

Epoch: 6| Step: 10
Training loss: 3.720766544342041
Validation loss: 2.900095957581715

Epoch: 6| Step: 11
Training loss: 2.042137861251831
Validation loss: 2.8996526297702583

Epoch: 6| Step: 12
Training loss: 2.6249918937683105
Validation loss: 2.895981888617239

Epoch: 6| Step: 13
Training loss: 2.2390379905700684
Validation loss: 2.8967691083108225

Epoch: 25| Step: 0
Training loss: 2.8150453567504883
Validation loss: 2.893619788590298

Epoch: 6| Step: 1
Training loss: 2.840693950653076
Validation loss: 2.8952036134658323

Epoch: 6| Step: 2
Training loss: 2.422907829284668
Validation loss: 2.895012696584066

Epoch: 6| Step: 3
Training loss: 2.6696863174438477
Validation loss: 2.8938156840621785

Epoch: 6| Step: 4
Training loss: 2.806321620941162
Validation loss: 2.890742299377277

Epoch: 6| Step: 5
Training loss: 3.3578286170959473
Validation loss: 2.8890179998131207

Epoch: 6| Step: 6
Training loss: 3.326465129852295
Validation loss: 2.8886873465712353

Epoch: 6| Step: 7
Training loss: 3.6676697731018066
Validation loss: 2.8854964753632903

Epoch: 6| Step: 8
Training loss: 2.3901724815368652
Validation loss: 2.884295714798794

Epoch: 6| Step: 9
Training loss: 3.656831741333008
Validation loss: 2.8862470798594977

Epoch: 6| Step: 10
Training loss: 3.77227520942688
Validation loss: 2.884018010990594

Epoch: 6| Step: 11
Training loss: 3.1677379608154297
Validation loss: 2.883046224553098

Epoch: 6| Step: 12
Training loss: 2.5810227394104004
Validation loss: 2.8818289310701433

Epoch: 6| Step: 13
Training loss: 1.8962827920913696
Validation loss: 2.8812226967145036

Epoch: 26| Step: 0
Training loss: 2.8321001529693604
Validation loss: 2.8815762791582333

Epoch: 6| Step: 1
Training loss: 2.9112677574157715
Validation loss: 2.875310318444365

Epoch: 6| Step: 2
Training loss: 3.5825998783111572
Validation loss: 2.8747921553991174

Epoch: 6| Step: 3
Training loss: 3.6666641235351562
Validation loss: 2.8724420147557415

Epoch: 6| Step: 4
Training loss: 3.709419012069702
Validation loss: 2.871172248676259

Epoch: 6| Step: 5
Training loss: 2.206387519836426
Validation loss: 2.8735219919553368

Epoch: 6| Step: 6
Training loss: 3.423830509185791
Validation loss: 2.8729193569511495

Epoch: 6| Step: 7
Training loss: 2.987112045288086
Validation loss: 2.8695423705603487

Epoch: 6| Step: 8
Training loss: 1.9006083011627197
Validation loss: 2.868218542427145

Epoch: 6| Step: 9
Training loss: 3.1328887939453125
Validation loss: 2.8678410078889582

Epoch: 6| Step: 10
Training loss: 3.254075288772583
Validation loss: 2.865359103807839

Epoch: 6| Step: 11
Training loss: 2.8608813285827637
Validation loss: 2.8657907157815914

Epoch: 6| Step: 12
Training loss: 2.7027037143707275
Validation loss: 2.862619246205976

Epoch: 6| Step: 13
Training loss: 2.184424877166748
Validation loss: 2.863543846273935

Epoch: 27| Step: 0
Training loss: 2.8154282569885254
Validation loss: 2.863588551039337

Epoch: 6| Step: 1
Training loss: 3.6774847507476807
Validation loss: 2.8621450393430647

Epoch: 6| Step: 2
Training loss: 3.2561726570129395
Validation loss: 2.859915410318682

Epoch: 6| Step: 3
Training loss: 2.1990151405334473
Validation loss: 2.8594572390279462

Epoch: 6| Step: 4
Training loss: 3.012063503265381
Validation loss: 2.8656448702658377

Epoch: 6| Step: 5
Training loss: 2.9752936363220215
Validation loss: 2.8664427880317933

Epoch: 6| Step: 6
Training loss: 2.791266441345215
Validation loss: 2.86048097507928

Epoch: 6| Step: 7
Training loss: 2.7460129261016846
Validation loss: 2.859162894628381

Epoch: 6| Step: 8
Training loss: 2.6097517013549805
Validation loss: 2.854527409358691

Epoch: 6| Step: 9
Training loss: 3.163491725921631
Validation loss: 2.8548371125293035

Epoch: 6| Step: 10
Training loss: 2.7305731773376465
Validation loss: 2.8548756543026177

Epoch: 6| Step: 11
Training loss: 3.541132688522339
Validation loss: 2.857124956705237

Epoch: 6| Step: 12
Training loss: 3.4391748905181885
Validation loss: 2.8535170324387087

Epoch: 6| Step: 13
Training loss: 2.3642091751098633
Validation loss: 2.8537290891011557

Epoch: 28| Step: 0
Training loss: 3.135826587677002
Validation loss: 2.8513656534174436

Epoch: 6| Step: 1
Training loss: 2.7129967212677
Validation loss: 2.8507395585378013

Epoch: 6| Step: 2
Training loss: 3.0277225971221924
Validation loss: 2.851654262952907

Epoch: 6| Step: 3
Training loss: 2.2224698066711426
Validation loss: 2.849180770176713

Epoch: 6| Step: 4
Training loss: 3.118739128112793
Validation loss: 2.8496019968422512

Epoch: 6| Step: 5
Training loss: 3.129802703857422
Validation loss: 2.8469801820734495

Epoch: 6| Step: 6
Training loss: 2.965693712234497
Validation loss: 2.8445324179946736

Epoch: 6| Step: 7
Training loss: 2.955723762512207
Validation loss: 2.843006600615799

Epoch: 6| Step: 8
Training loss: 2.2725160121917725
Validation loss: 2.8429685574705883

Epoch: 6| Step: 9
Training loss: 3.2614784240722656
Validation loss: 2.8404356920590965

Epoch: 6| Step: 10
Training loss: 3.168884515762329
Validation loss: 2.8403629667015484

Epoch: 6| Step: 11
Training loss: 3.016260862350464
Validation loss: 2.8398347567486506

Epoch: 6| Step: 12
Training loss: 3.0915908813476562
Validation loss: 2.8403994703805573

Epoch: 6| Step: 13
Training loss: 3.7416677474975586
Validation loss: 2.8416774529282764

Epoch: 29| Step: 0
Training loss: 2.5727336406707764
Validation loss: 2.8518881515790055

Epoch: 6| Step: 1
Training loss: 3.192533254623413
Validation loss: 2.8413198788960776

Epoch: 6| Step: 2
Training loss: 2.8463141918182373
Validation loss: 2.8381664265868483

Epoch: 6| Step: 3
Training loss: 3.0055932998657227
Validation loss: 2.835733998206354

Epoch: 6| Step: 4
Training loss: 2.432089328765869
Validation loss: 2.8347441816842682

Epoch: 6| Step: 5
Training loss: 3.3341832160949707
Validation loss: 2.833447451232582

Epoch: 6| Step: 6
Training loss: 3.2375078201293945
Validation loss: 2.8330359843469437

Epoch: 6| Step: 7
Training loss: 2.744168281555176
Validation loss: 2.832649979540097

Epoch: 6| Step: 8
Training loss: 2.672097682952881
Validation loss: 2.8329325183745353

Epoch: 6| Step: 9
Training loss: 3.514530658721924
Validation loss: 2.832333061002916

Epoch: 6| Step: 10
Training loss: 3.0455880165100098
Validation loss: 2.8313428368619693

Epoch: 6| Step: 11
Training loss: 2.9972612857818604
Validation loss: 2.828391387898435

Epoch: 6| Step: 12
Training loss: 2.428232192993164
Validation loss: 2.829525614297518

Epoch: 6| Step: 13
Training loss: 3.682447671890259
Validation loss: 2.8295167928100913

Epoch: 30| Step: 0
Training loss: 2.4232983589172363
Validation loss: 2.8265643324903262

Epoch: 6| Step: 1
Training loss: 2.5521116256713867
Validation loss: 2.8268714284384124

Epoch: 6| Step: 2
Training loss: 3.877017021179199
Validation loss: 2.824938933054606

Epoch: 6| Step: 3
Training loss: 3.8019754886627197
Validation loss: 2.8250167190387683

Epoch: 6| Step: 4
Training loss: 3.3418383598327637
Validation loss: 2.824322554372972

Epoch: 6| Step: 5
Training loss: 3.356724500656128
Validation loss: 2.824682653591197

Epoch: 6| Step: 6
Training loss: 2.603433132171631
Validation loss: 2.8252624978301344

Epoch: 6| Step: 7
Training loss: 3.057783842086792
Validation loss: 2.823416345862932

Epoch: 6| Step: 8
Training loss: 3.3895046710968018
Validation loss: 2.821564999959802

Epoch: 6| Step: 9
Training loss: 2.8209972381591797
Validation loss: 2.8223691858271116

Epoch: 6| Step: 10
Training loss: 2.400479793548584
Validation loss: 2.820762759895735

Epoch: 6| Step: 11
Training loss: 2.820608139038086
Validation loss: 2.8231479173065512

Epoch: 6| Step: 12
Training loss: 2.6080482006073
Validation loss: 2.8238865483191704

Epoch: 6| Step: 13
Training loss: 1.6583316326141357
Validation loss: 2.821177021149666

Epoch: 31| Step: 0
Training loss: 2.358664035797119
Validation loss: 2.8194659217711417

Epoch: 6| Step: 1
Training loss: 2.85062313079834
Validation loss: 2.8200650394603772

Epoch: 6| Step: 2
Training loss: 3.239920139312744
Validation loss: 2.821582950571532

Epoch: 6| Step: 3
Training loss: 2.886188268661499
Validation loss: 2.822095578716647

Epoch: 6| Step: 4
Training loss: 2.68331241607666
Validation loss: 2.822676367657159

Epoch: 6| Step: 5
Training loss: 2.5482113361358643
Validation loss: 2.8186253219522457

Epoch: 6| Step: 6
Training loss: 2.3895211219787598
Validation loss: 2.8203293559371785

Epoch: 6| Step: 7
Training loss: 2.9192867279052734
Validation loss: 2.820134591030818

Epoch: 6| Step: 8
Training loss: 2.802927017211914
Validation loss: 2.8203367392222085

Epoch: 6| Step: 9
Training loss: 3.0516247749328613
Validation loss: 2.818041222069853

Epoch: 6| Step: 10
Training loss: 3.2424135208129883
Validation loss: 2.8174262995361

Epoch: 6| Step: 11
Training loss: 2.966214418411255
Validation loss: 2.817809368974419

Epoch: 6| Step: 12
Training loss: 4.515722274780273
Validation loss: 2.8165963439531225

Epoch: 6| Step: 13
Training loss: 2.6759278774261475
Validation loss: 2.815887902372627

Epoch: 32| Step: 0
Training loss: 4.1195197105407715
Validation loss: 2.815547909787906

Epoch: 6| Step: 1
Training loss: 2.7517104148864746
Validation loss: 2.8146517635673605

Epoch: 6| Step: 2
Training loss: 3.1862049102783203
Validation loss: 2.8151728824902604

Epoch: 6| Step: 3
Training loss: 2.673652410507202
Validation loss: 2.81373030395918

Epoch: 6| Step: 4
Training loss: 3.3330283164978027
Validation loss: 2.8133925032872025

Epoch: 6| Step: 5
Training loss: 3.7765445709228516
Validation loss: 2.8109623642377954

Epoch: 6| Step: 6
Training loss: 2.8357443809509277
Validation loss: 2.8096594118302867

Epoch: 6| Step: 7
Training loss: 2.614142656326294
Validation loss: 2.811391753535117

Epoch: 6| Step: 8
Training loss: 2.4352142810821533
Validation loss: 2.814750494495515

Epoch: 6| Step: 9
Training loss: 2.920243740081787
Validation loss: 2.8145802328663487

Epoch: 6| Step: 10
Training loss: 3.0358564853668213
Validation loss: 2.8254448239521315

Epoch: 6| Step: 11
Training loss: 2.143980026245117
Validation loss: 2.812116651124852

Epoch: 6| Step: 12
Training loss: 2.7278223037719727
Validation loss: 2.8092222367563555

Epoch: 6| Step: 13
Training loss: 2.398345470428467
Validation loss: 2.8109114375165714

Epoch: 33| Step: 0
Training loss: 3.3606600761413574
Validation loss: 2.811276540961317

Epoch: 6| Step: 1
Training loss: 2.9042983055114746
Validation loss: 2.8123848976627475

Epoch: 6| Step: 2
Training loss: 2.32448148727417
Validation loss: 2.8122688519057406

Epoch: 6| Step: 3
Training loss: 3.3080804347991943
Validation loss: 2.8116926044546147

Epoch: 6| Step: 4
Training loss: 3.635068893432617
Validation loss: 2.8121709208334646

Epoch: 6| Step: 5
Training loss: 2.595120906829834
Validation loss: 2.8125784012579147

Epoch: 6| Step: 6
Training loss: 2.533539295196533
Validation loss: 2.807385870205459

Epoch: 6| Step: 7
Training loss: 3.8016812801361084
Validation loss: 2.8045320023772535

Epoch: 6| Step: 8
Training loss: 1.552587628364563
Validation loss: 2.8028316138893046

Epoch: 6| Step: 9
Training loss: 2.3271331787109375
Validation loss: 2.801901699394308

Epoch: 6| Step: 10
Training loss: 2.8291707038879395
Validation loss: 2.8031731805493756

Epoch: 6| Step: 11
Training loss: 3.023623466491699
Validation loss: 2.8040638995426956

Epoch: 6| Step: 12
Training loss: 3.6480960845947266
Validation loss: 2.804268778011363

Epoch: 6| Step: 13
Training loss: 3.571937084197998
Validation loss: 2.8041404242156656

Epoch: 34| Step: 0
Training loss: 2.680593967437744
Validation loss: 2.80204846525705

Epoch: 6| Step: 1
Training loss: 2.443861484527588
Validation loss: 2.8000784868835122

Epoch: 6| Step: 2
Training loss: 2.387331247329712
Validation loss: 2.8001981499374553

Epoch: 6| Step: 3
Training loss: 2.5970559120178223
Validation loss: 2.7980913449359197

Epoch: 6| Step: 4
Training loss: 2.0171778202056885
Validation loss: 2.7978157228039158

Epoch: 6| Step: 5
Training loss: 2.8626317977905273
Validation loss: 2.7976779630107265

Epoch: 6| Step: 6
Training loss: 3.132263660430908
Validation loss: 2.7972131877817135

Epoch: 6| Step: 7
Training loss: 3.7787647247314453
Validation loss: 2.797290302092029

Epoch: 6| Step: 8
Training loss: 3.057802438735962
Validation loss: 2.7976109366263113

Epoch: 6| Step: 9
Training loss: 3.904139518737793
Validation loss: 2.7978259414754887

Epoch: 6| Step: 10
Training loss: 3.0896055698394775
Validation loss: 2.7967379657171105

Epoch: 6| Step: 11
Training loss: 3.725219249725342
Validation loss: 2.8010259494986585

Epoch: 6| Step: 12
Training loss: 2.3014442920684814
Validation loss: 2.796536925018475

Epoch: 6| Step: 13
Training loss: 3.206408977508545
Validation loss: 2.7962537427102365

Epoch: 35| Step: 0
Training loss: 3.3749232292175293
Validation loss: 2.7973652193623204

Epoch: 6| Step: 1
Training loss: 2.927359104156494
Validation loss: 2.7940212834265923

Epoch: 6| Step: 2
Training loss: 2.7362451553344727
Validation loss: 2.7958097227158083

Epoch: 6| Step: 3
Training loss: 2.770888328552246
Validation loss: 2.7955563324753956

Epoch: 6| Step: 4
Training loss: 2.459756374359131
Validation loss: 2.7965009597039994

Epoch: 6| Step: 5
Training loss: 3.2080836296081543
Validation loss: 2.796238301902689

Epoch: 6| Step: 6
Training loss: 2.7293965816497803
Validation loss: 2.7967855417600243

Epoch: 6| Step: 7
Training loss: 2.838474988937378
Validation loss: 2.794447760428152

Epoch: 6| Step: 8
Training loss: 3.3444597721099854
Validation loss: 2.795318124114826

Epoch: 6| Step: 9
Training loss: 2.3299639225006104
Validation loss: 2.792832515572989

Epoch: 6| Step: 10
Training loss: 3.171581745147705
Validation loss: 2.7920821738499466

Epoch: 6| Step: 11
Training loss: 3.4314799308776855
Validation loss: 2.7907803596988803

Epoch: 6| Step: 12
Training loss: 2.4436306953430176
Validation loss: 2.7886880367032942

Epoch: 6| Step: 13
Training loss: 3.540910005569458
Validation loss: 2.7889641638725036

Epoch: 36| Step: 0
Training loss: 2.7763376235961914
Validation loss: 2.7907245851332143

Epoch: 6| Step: 1
Training loss: 2.6350114345550537
Validation loss: 2.7903763273710847

Epoch: 6| Step: 2
Training loss: 2.967158794403076
Validation loss: 2.789950091351745

Epoch: 6| Step: 3
Training loss: 2.4367711544036865
Validation loss: 2.7916504542032876

Epoch: 6| Step: 4
Training loss: 3.100480079650879
Validation loss: 2.7890451185164915

Epoch: 6| Step: 5
Training loss: 3.5016658306121826
Validation loss: 2.7881405251000517

Epoch: 6| Step: 6
Training loss: 2.890982151031494
Validation loss: 2.788133121305896

Epoch: 6| Step: 7
Training loss: 2.5265557765960693
Validation loss: 2.788152986957181

Epoch: 6| Step: 8
Training loss: 3.440342426300049
Validation loss: 2.7837045833628666

Epoch: 6| Step: 9
Training loss: 3.9995615482330322
Validation loss: 2.7843722015298824

Epoch: 6| Step: 10
Training loss: 3.004072666168213
Validation loss: 2.7827640169410297

Epoch: 6| Step: 11
Training loss: 2.4821786880493164
Validation loss: 2.7847039955918507

Epoch: 6| Step: 12
Training loss: 2.512549877166748
Validation loss: 2.791648808346

Epoch: 6| Step: 13
Training loss: 2.517484426498413
Validation loss: 2.8052403606394285

Epoch: 37| Step: 0
Training loss: 2.9485530853271484
Validation loss: 2.817553843221357

Epoch: 6| Step: 1
Training loss: 2.3342208862304688
Validation loss: 2.8213973532440844

Epoch: 6| Step: 2
Training loss: 2.8426876068115234
Validation loss: 2.8232355092161443

Epoch: 6| Step: 3
Training loss: 2.6551108360290527
Validation loss: 2.825694255931403

Epoch: 6| Step: 4
Training loss: 3.065650224685669
Validation loss: 2.8197425027047434

Epoch: 6| Step: 5
Training loss: 3.6092607975006104
Validation loss: 2.815810226625012

Epoch: 6| Step: 6
Training loss: 2.7816338539123535
Validation loss: 2.8171155862910773

Epoch: 6| Step: 7
Training loss: 2.413027048110962
Validation loss: 2.816694167352492

Epoch: 6| Step: 8
Training loss: 3.625077724456787
Validation loss: 2.8187458079348326

Epoch: 6| Step: 9
Training loss: 3.2448716163635254
Validation loss: 2.8165467323795443

Epoch: 6| Step: 10
Training loss: 2.8627190589904785
Validation loss: 2.8162641140722458

Epoch: 6| Step: 11
Training loss: 3.3329625129699707
Validation loss: 2.8135218594663884

Epoch: 6| Step: 12
Training loss: 2.8068928718566895
Validation loss: 2.8119997311663885

Epoch: 6| Step: 13
Training loss: 2.5236053466796875
Validation loss: 2.8110672171397875

Epoch: 38| Step: 0
Training loss: 2.7991819381713867
Validation loss: 2.8118311999946513

Epoch: 6| Step: 1
Training loss: 3.253972053527832
Validation loss: 2.8080873258652224

Epoch: 6| Step: 2
Training loss: 3.2988638877868652
Validation loss: 2.8090919294664936

Epoch: 6| Step: 3
Training loss: 4.108997821807861
Validation loss: 2.8085948856928016

Epoch: 6| Step: 4
Training loss: 2.5174787044525146
Validation loss: 2.8064156398978284

Epoch: 6| Step: 5
Training loss: 3.142249345779419
Validation loss: 2.8050074551695134

Epoch: 6| Step: 6
Training loss: 2.6344046592712402
Validation loss: 2.805383054159021

Epoch: 6| Step: 7
Training loss: 3.064389228820801
Validation loss: 2.805364124236568

Epoch: 6| Step: 8
Training loss: 1.7932521104812622
Validation loss: 2.8016213293998473

Epoch: 6| Step: 9
Training loss: 3.0801596641540527
Validation loss: 2.8032340465053434

Epoch: 6| Step: 10
Training loss: 3.3656396865844727
Validation loss: 2.8043763047905377

Epoch: 6| Step: 11
Training loss: 2.8452377319335938
Validation loss: 2.8008859311380694

Epoch: 6| Step: 12
Training loss: 2.6668033599853516
Validation loss: 2.803149510455388

Epoch: 6| Step: 13
Training loss: 2.2799863815307617
Validation loss: 2.8007262727265716

Epoch: 39| Step: 0
Training loss: 3.8221657276153564
Validation loss: 2.802099158686976

Epoch: 6| Step: 1
Training loss: 2.8093929290771484
Validation loss: 2.8016393799935617

Epoch: 6| Step: 2
Training loss: 2.719825267791748
Validation loss: 2.8009882024539414

Epoch: 6| Step: 3
Training loss: 2.7268872261047363
Validation loss: 2.800328231626941

Epoch: 6| Step: 4
Training loss: 2.8868494033813477
Validation loss: 2.7992975814368135

Epoch: 6| Step: 5
Training loss: 3.609757661819458
Validation loss: 2.797837911113616

Epoch: 6| Step: 6
Training loss: 3.102567434310913
Validation loss: 2.7970638557146956

Epoch: 6| Step: 7
Training loss: 2.3502752780914307
Validation loss: 2.795826406889064

Epoch: 6| Step: 8
Training loss: 2.734066963195801
Validation loss: 2.7972602100782495

Epoch: 6| Step: 9
Training loss: 3.0541586875915527
Validation loss: 2.7960891159631873

Epoch: 6| Step: 10
Training loss: 2.222015857696533
Validation loss: 2.7922067206393004

Epoch: 6| Step: 11
Training loss: 2.4924733638763428
Validation loss: 2.792020110673802

Epoch: 6| Step: 12
Training loss: 3.1422600746154785
Validation loss: 2.79173860755018

Epoch: 6| Step: 13
Training loss: 3.7182672023773193
Validation loss: 2.7911451683249524

Epoch: 40| Step: 0
Training loss: 2.4870710372924805
Validation loss: 2.788935287024385

Epoch: 6| Step: 1
Training loss: 3.0365042686462402
Validation loss: 2.7844468085996565

Epoch: 6| Step: 2
Training loss: 2.9976792335510254
Validation loss: 2.786610536677863

Epoch: 6| Step: 3
Training loss: 1.9073375463485718
Validation loss: 2.7845089691941456

Epoch: 6| Step: 4
Training loss: 2.93831205368042
Validation loss: 2.7848861935318157

Epoch: 6| Step: 5
Training loss: 2.7568273544311523
Validation loss: 2.781846155402481

Epoch: 6| Step: 6
Training loss: 2.872204303741455
Validation loss: 2.7833519879207818

Epoch: 6| Step: 7
Training loss: 2.65187668800354
Validation loss: 2.78020836717339

Epoch: 6| Step: 8
Training loss: 2.99881649017334
Validation loss: 2.7803128919293805

Epoch: 6| Step: 9
Training loss: 3.168851852416992
Validation loss: 2.7792330916209886

Epoch: 6| Step: 10
Training loss: 2.5821242332458496
Validation loss: 2.7774825993404595

Epoch: 6| Step: 11
Training loss: 3.5416340827941895
Validation loss: 2.777556655227497

Epoch: 6| Step: 12
Training loss: 3.7080180644989014
Validation loss: 2.777033944283762

Epoch: 6| Step: 13
Training loss: 3.516765594482422
Validation loss: 2.781323450867848

Epoch: 41| Step: 0
Training loss: 2.59549617767334
Validation loss: 2.7930474717129945

Epoch: 6| Step: 1
Training loss: 2.659031391143799
Validation loss: 2.776348808760284

Epoch: 6| Step: 2
Training loss: 2.21110200881958
Validation loss: 2.7753811908024613

Epoch: 6| Step: 3
Training loss: 3.0588631629943848
Validation loss: 2.7755652499455277

Epoch: 6| Step: 4
Training loss: 3.23060941696167
Validation loss: 2.7776354179587415

Epoch: 6| Step: 5
Training loss: 2.982513904571533
Validation loss: 2.7735531971018803

Epoch: 6| Step: 6
Training loss: 2.848998785018921
Validation loss: 2.778191876667802

Epoch: 6| Step: 7
Training loss: 3.945051670074463
Validation loss: 2.778876935282061

Epoch: 6| Step: 8
Training loss: 2.932706832885742
Validation loss: 2.7751252471759753

Epoch: 6| Step: 9
Training loss: 2.724933624267578
Validation loss: 2.774566355571952

Epoch: 6| Step: 10
Training loss: 3.0218167304992676
Validation loss: 2.772648193502939

Epoch: 6| Step: 11
Training loss: 2.6703929901123047
Validation loss: 2.773230137363557

Epoch: 6| Step: 12
Training loss: 2.976161241531372
Validation loss: 2.7717094805932816

Epoch: 6| Step: 13
Training loss: 3.050919771194458
Validation loss: 2.7698130453786542

Epoch: 42| Step: 0
Training loss: 2.4595398902893066
Validation loss: 2.7720320840035715

Epoch: 6| Step: 1
Training loss: 2.8639822006225586
Validation loss: 2.769481405135124

Epoch: 6| Step: 2
Training loss: 2.3093996047973633
Validation loss: 2.772239533803796

Epoch: 6| Step: 3
Training loss: 3.247856616973877
Validation loss: 2.7699171009884087

Epoch: 6| Step: 4
Training loss: 3.4014387130737305
Validation loss: 2.7705661507063013

Epoch: 6| Step: 5
Training loss: 3.2907097339630127
Validation loss: 2.7680684981807584

Epoch: 6| Step: 6
Training loss: 2.5636181831359863
Validation loss: 2.7685773244468113

Epoch: 6| Step: 7
Training loss: 2.848886489868164
Validation loss: 2.7664705425180416

Epoch: 6| Step: 8
Training loss: 3.083933115005493
Validation loss: 2.7661977737180647

Epoch: 6| Step: 9
Training loss: 3.4798710346221924
Validation loss: 2.768495854511056

Epoch: 6| Step: 10
Training loss: 2.9438579082489014
Validation loss: 2.767221881497291

Epoch: 6| Step: 11
Training loss: 2.509308338165283
Validation loss: 2.765243004727107

Epoch: 6| Step: 12
Training loss: 2.7737202644348145
Validation loss: 2.7662220334493988

Epoch: 6| Step: 13
Training loss: 3.104053497314453
Validation loss: 2.7656948104981454

Epoch: 43| Step: 0
Training loss: 2.9352755546569824
Validation loss: 2.765052713373656

Epoch: 6| Step: 1
Training loss: 2.9771888256073
Validation loss: 2.7641825957964827

Epoch: 6| Step: 2
Training loss: 3.255479335784912
Validation loss: 2.764335137541576

Epoch: 6| Step: 3
Training loss: 2.303769111633301
Validation loss: 2.763257102299762

Epoch: 6| Step: 4
Training loss: 2.8450510501861572
Validation loss: 2.761325354217201

Epoch: 6| Step: 5
Training loss: 2.7749009132385254
Validation loss: 2.7641910071014077

Epoch: 6| Step: 6
Training loss: 3.1901888847351074
Validation loss: 2.763726649745818

Epoch: 6| Step: 7
Training loss: 3.0955140590667725
Validation loss: 2.7640125033675984

Epoch: 6| Step: 8
Training loss: 2.4598755836486816
Validation loss: 2.7641192046544885

Epoch: 6| Step: 9
Training loss: 2.9381442070007324
Validation loss: 2.763531349038565

Epoch: 6| Step: 10
Training loss: 2.635591506958008
Validation loss: 2.7610769861487934

Epoch: 6| Step: 11
Training loss: 2.75957989692688
Validation loss: 2.7616948825056835

Epoch: 6| Step: 12
Training loss: 3.4601686000823975
Validation loss: 2.7629743288922053

Epoch: 6| Step: 13
Training loss: 3.2784104347229004
Validation loss: 2.7624343595197125

Epoch: 44| Step: 0
Training loss: 2.515695571899414
Validation loss: 2.7620535255760275

Epoch: 6| Step: 1
Training loss: 2.5633459091186523
Validation loss: 2.761562165393624

Epoch: 6| Step: 2
Training loss: 2.1881213188171387
Validation loss: 2.7611951212729178

Epoch: 6| Step: 3
Training loss: 3.917977809906006
Validation loss: 2.7598508865602556

Epoch: 6| Step: 4
Training loss: 2.246610641479492
Validation loss: 2.7622385435206915

Epoch: 6| Step: 5
Training loss: 3.1369972229003906
Validation loss: 2.7625434731924408

Epoch: 6| Step: 6
Training loss: 3.1851577758789062
Validation loss: 2.7611334708429154

Epoch: 6| Step: 7
Training loss: 2.219820022583008
Validation loss: 2.76282315100393

Epoch: 6| Step: 8
Training loss: 3.5695416927337646
Validation loss: 2.7622138915523404

Epoch: 6| Step: 9
Training loss: 2.462120771408081
Validation loss: 2.7615502495919504

Epoch: 6| Step: 10
Training loss: 3.181119441986084
Validation loss: 2.761463560083861

Epoch: 6| Step: 11
Training loss: 3.23881196975708
Validation loss: 2.757361024938604

Epoch: 6| Step: 12
Training loss: 3.8862414360046387
Validation loss: 2.7577850869906846

Epoch: 6| Step: 13
Training loss: 1.9790844917297363
Validation loss: 2.7579260077527774

Epoch: 45| Step: 0
Training loss: 3.672635316848755
Validation loss: 2.7585365977338565

Epoch: 6| Step: 1
Training loss: 2.8191046714782715
Validation loss: 2.7585218004001084

Epoch: 6| Step: 2
Training loss: 2.215672492980957
Validation loss: 2.759526188655566

Epoch: 6| Step: 3
Training loss: 2.51291823387146
Validation loss: 2.7569091012400966

Epoch: 6| Step: 4
Training loss: 2.559478759765625
Validation loss: 2.755719297675676

Epoch: 6| Step: 5
Training loss: 3.2780065536499023
Validation loss: 2.755666522569554

Epoch: 6| Step: 6
Training loss: 3.0549628734588623
Validation loss: 2.7559418550101658

Epoch: 6| Step: 7
Training loss: 3.0342800617218018
Validation loss: 2.756607065918625

Epoch: 6| Step: 8
Training loss: 3.3581299781799316
Validation loss: 2.7564200175705778

Epoch: 6| Step: 9
Training loss: 3.4780161380767822
Validation loss: 2.7563157004694783

Epoch: 6| Step: 10
Training loss: 2.359712600708008
Validation loss: 2.7541465451640468

Epoch: 6| Step: 11
Training loss: 2.9341931343078613
Validation loss: 2.757673801914338

Epoch: 6| Step: 12
Training loss: 2.4402668476104736
Validation loss: 2.754283981938516

Epoch: 6| Step: 13
Training loss: 2.9654886722564697
Validation loss: 2.7537411412885113

Epoch: 46| Step: 0
Training loss: 2.5947656631469727
Validation loss: 2.757765910958731

Epoch: 6| Step: 1
Training loss: 2.6297545433044434
Validation loss: 2.758704816141436

Epoch: 6| Step: 2
Training loss: 2.923992156982422
Validation loss: 2.75618649554509

Epoch: 6| Step: 3
Training loss: 3.451099395751953
Validation loss: 2.75961906679215

Epoch: 6| Step: 4
Training loss: 2.1515095233917236
Validation loss: 2.7597890207844396

Epoch: 6| Step: 5
Training loss: 2.5551812648773193
Validation loss: 2.7557593443060435

Epoch: 6| Step: 6
Training loss: 2.9673256874084473
Validation loss: 2.754568253794024

Epoch: 6| Step: 7
Training loss: 3.364438533782959
Validation loss: 2.7554368049867692

Epoch: 6| Step: 8
Training loss: 2.9026594161987305
Validation loss: 2.751516362672211

Epoch: 6| Step: 9
Training loss: 2.7897086143493652
Validation loss: 2.7554329902895036

Epoch: 6| Step: 10
Training loss: 2.747291326522827
Validation loss: 2.7522430983922814

Epoch: 6| Step: 11
Training loss: 2.7217724323272705
Validation loss: 2.751789500636439

Epoch: 6| Step: 12
Training loss: 3.6422131061553955
Validation loss: 2.7558868674821753

Epoch: 6| Step: 13
Training loss: 3.4748127460479736
Validation loss: 2.7521020007389847

Epoch: 47| Step: 0
Training loss: 2.4214863777160645
Validation loss: 2.7557133884840113

Epoch: 6| Step: 1
Training loss: 2.8446640968322754
Validation loss: 2.7528052868381625

Epoch: 6| Step: 2
Training loss: 2.946591854095459
Validation loss: 2.75112505625653

Epoch: 6| Step: 3
Training loss: 3.6220481395721436
Validation loss: 2.75123195750739

Epoch: 6| Step: 4
Training loss: 3.571470022201538
Validation loss: 2.751348244246616

Epoch: 6| Step: 5
Training loss: 3.0780673027038574
Validation loss: 2.752766104154689

Epoch: 6| Step: 6
Training loss: 2.4030816555023193
Validation loss: 2.7506099695800454

Epoch: 6| Step: 7
Training loss: 1.9592050313949585
Validation loss: 2.7518934921551774

Epoch: 6| Step: 8
Training loss: 3.109415054321289
Validation loss: 2.749945484181886

Epoch: 6| Step: 9
Training loss: 3.313821792602539
Validation loss: 2.7497948600399877

Epoch: 6| Step: 10
Training loss: 2.9652762413024902
Validation loss: 2.750675993580972

Epoch: 6| Step: 11
Training loss: 2.0020227432250977
Validation loss: 2.749152191223637

Epoch: 6| Step: 12
Training loss: 3.8473684787750244
Validation loss: 2.7557273475072717

Epoch: 6| Step: 13
Training loss: 2.235422134399414
Validation loss: 2.7565770661959084

Epoch: 48| Step: 0
Training loss: 3.2795441150665283
Validation loss: 2.753877216769803

Epoch: 6| Step: 1
Training loss: 2.742924928665161
Validation loss: 2.7559720880241803

Epoch: 6| Step: 2
Training loss: 1.6312744617462158
Validation loss: 2.751186386231453

Epoch: 6| Step: 3
Training loss: 3.120972156524658
Validation loss: 2.754054010555308

Epoch: 6| Step: 4
Training loss: 2.6686530113220215
Validation loss: 2.752199657501713

Epoch: 6| Step: 5
Training loss: 3.0676157474517822
Validation loss: 2.752307617536155

Epoch: 6| Step: 6
Training loss: 2.573254108428955
Validation loss: 2.7511840353729906

Epoch: 6| Step: 7
Training loss: 2.8614706993103027
Validation loss: 2.7484563627550678

Epoch: 6| Step: 8
Training loss: 3.0381994247436523
Validation loss: 2.7477210311479467

Epoch: 6| Step: 9
Training loss: 3.774395227432251
Validation loss: 2.7489525528364283

Epoch: 6| Step: 10
Training loss: 3.0798771381378174
Validation loss: 2.747521787561396

Epoch: 6| Step: 11
Training loss: 2.736177921295166
Validation loss: 2.7463775962911625

Epoch: 6| Step: 12
Training loss: 3.0539186000823975
Validation loss: 2.7484725906002905

Epoch: 6| Step: 13
Training loss: 3.0183334350585938
Validation loss: 2.748414724103866

Epoch: 49| Step: 0
Training loss: 2.570173740386963
Validation loss: 2.7454149056506414

Epoch: 6| Step: 1
Training loss: 3.108396053314209
Validation loss: 2.747447300982732

Epoch: 6| Step: 2
Training loss: 3.0301146507263184
Validation loss: 2.7481513843741467

Epoch: 6| Step: 3
Training loss: 2.468777894973755
Validation loss: 2.74558913066823

Epoch: 6| Step: 4
Training loss: 3.5584208965301514
Validation loss: 2.7487604566799697

Epoch: 6| Step: 5
Training loss: 2.912612199783325
Validation loss: 2.7474295759713776

Epoch: 6| Step: 6
Training loss: 3.208550453186035
Validation loss: 2.7454588708057197

Epoch: 6| Step: 7
Training loss: 3.225409507751465
Validation loss: 2.7446124886953704

Epoch: 6| Step: 8
Training loss: 3.0145621299743652
Validation loss: 2.743951939767407

Epoch: 6| Step: 9
Training loss: 2.806105613708496
Validation loss: 2.744934692177721

Epoch: 6| Step: 10
Training loss: 2.428896188735962
Validation loss: 2.745622109341365

Epoch: 6| Step: 11
Training loss: 3.1375668048858643
Validation loss: 2.7448533196603098

Epoch: 6| Step: 12
Training loss: 2.6809606552124023
Validation loss: 2.743314589223554

Epoch: 6| Step: 13
Training loss: 2.0383288860321045
Validation loss: 2.745221053400347

Epoch: 50| Step: 0
Training loss: 2.2731800079345703
Validation loss: 2.7461792730516

Epoch: 6| Step: 1
Training loss: 3.062777042388916
Validation loss: 2.7480511562798613

Epoch: 6| Step: 2
Training loss: 2.4037399291992188
Validation loss: 2.7514699171948176

Epoch: 6| Step: 3
Training loss: 2.897836208343506
Validation loss: 2.7489446106777398

Epoch: 6| Step: 4
Training loss: 2.5100083351135254
Validation loss: 2.7466693591046076

Epoch: 6| Step: 5
Training loss: 3.1546030044555664
Validation loss: 2.749158279870146

Epoch: 6| Step: 6
Training loss: 3.380852699279785
Validation loss: 2.7477527382553264

Epoch: 6| Step: 7
Training loss: 2.618393898010254
Validation loss: 2.744986985319404

Epoch: 6| Step: 8
Training loss: 3.7904982566833496
Validation loss: 2.7446554963306715

Epoch: 6| Step: 9
Training loss: 2.8177404403686523
Validation loss: 2.7442366564145653

Epoch: 6| Step: 10
Training loss: 2.5279288291931152
Validation loss: 2.744170691377373

Epoch: 6| Step: 11
Training loss: 2.46042799949646
Validation loss: 2.7449700242729596

Epoch: 6| Step: 12
Training loss: 3.4176857471466064
Validation loss: 2.741447587167063

Epoch: 6| Step: 13
Training loss: 3.488680601119995
Validation loss: 2.742963224328974

Epoch: 51| Step: 0
Training loss: 2.557772159576416
Validation loss: 2.7422602843212824

Epoch: 6| Step: 1
Training loss: 3.296031951904297
Validation loss: 2.741219056549893

Epoch: 6| Step: 2
Training loss: 3.6350762844085693
Validation loss: 2.740799496250768

Epoch: 6| Step: 3
Training loss: 1.3792258501052856
Validation loss: 2.740370317171979

Epoch: 6| Step: 4
Training loss: 3.554321527481079
Validation loss: 2.7417252755934194

Epoch: 6| Step: 5
Training loss: 2.827303886413574
Validation loss: 2.739753535998765

Epoch: 6| Step: 6
Training loss: 3.1282076835632324
Validation loss: 2.737563761331702

Epoch: 6| Step: 7
Training loss: 3.293728828430176
Validation loss: 2.7393598146336053

Epoch: 6| Step: 8
Training loss: 3.1391043663024902
Validation loss: 2.7398924955757717

Epoch: 6| Step: 9
Training loss: 4.041201591491699
Validation loss: 2.7396143354395384

Epoch: 6| Step: 10
Training loss: 2.6824989318847656
Validation loss: 2.740029640095208

Epoch: 6| Step: 11
Training loss: 2.1002421379089355
Validation loss: 2.7375541374247563

Epoch: 6| Step: 12
Training loss: 2.249495029449463
Validation loss: 2.7384900790388866

Epoch: 6| Step: 13
Training loss: 2.3687548637390137
Validation loss: 2.7394540053541943

Epoch: 52| Step: 0
Training loss: 3.247596502304077
Validation loss: 2.7406455137396373

Epoch: 6| Step: 1
Training loss: 3.3518710136413574
Validation loss: 2.742034322472029

Epoch: 6| Step: 2
Training loss: 2.2130324840545654
Validation loss: 2.7395616603154007

Epoch: 6| Step: 3
Training loss: 2.5956454277038574
Validation loss: 2.736681571570776

Epoch: 6| Step: 4
Training loss: 2.6929996013641357
Validation loss: 2.7375360688855572

Epoch: 6| Step: 5
Training loss: 2.7234559059143066
Validation loss: 2.7375667736094487

Epoch: 6| Step: 6
Training loss: 3.6813364028930664
Validation loss: 2.738740905638664

Epoch: 6| Step: 7
Training loss: 2.8645825386047363
Validation loss: 2.736505639168524

Epoch: 6| Step: 8
Training loss: 3.5315370559692383
Validation loss: 2.7360294301022767

Epoch: 6| Step: 9
Training loss: 2.4012389183044434
Validation loss: 2.737048449054841

Epoch: 6| Step: 10
Training loss: 2.7995729446411133
Validation loss: 2.738904271074521

Epoch: 6| Step: 11
Training loss: 2.571176528930664
Validation loss: 2.739373286565145

Epoch: 6| Step: 12
Training loss: 3.19625186920166
Validation loss: 2.7407585087642876

Epoch: 6| Step: 13
Training loss: 2.395501136779785
Validation loss: 2.7390549618710756

Epoch: 53| Step: 0
Training loss: 2.6353678703308105
Validation loss: 2.740086870808755

Epoch: 6| Step: 1
Training loss: 2.823512077331543
Validation loss: 2.737679663524833

Epoch: 6| Step: 2
Training loss: 2.2533457279205322
Validation loss: 2.739176111836587

Epoch: 6| Step: 3
Training loss: 3.24676251411438
Validation loss: 2.739387337879468

Epoch: 6| Step: 4
Training loss: 3.435971260070801
Validation loss: 2.739954710006714

Epoch: 6| Step: 5
Training loss: 2.5875513553619385
Validation loss: 2.7384360169851654

Epoch: 6| Step: 6
Training loss: 2.160857677459717
Validation loss: 2.7372233560008388

Epoch: 6| Step: 7
Training loss: 3.7577366828918457
Validation loss: 2.74025551478068

Epoch: 6| Step: 8
Training loss: 3.871954917907715
Validation loss: 2.7413592620562484

Epoch: 6| Step: 9
Training loss: 2.779855728149414
Validation loss: 2.7371958250640542

Epoch: 6| Step: 10
Training loss: 2.9213685989379883
Validation loss: 2.7332951535460768

Epoch: 6| Step: 11
Training loss: 2.7879443168640137
Validation loss: 2.73510419425144

Epoch: 6| Step: 12
Training loss: 2.3819851875305176
Validation loss: 2.7325633597630326

Epoch: 6| Step: 13
Training loss: 2.73401141166687
Validation loss: 2.7362083158185406

Epoch: 54| Step: 0
Training loss: 2.759760856628418
Validation loss: 2.734400031387165

Epoch: 6| Step: 1
Training loss: 2.2342324256896973
Validation loss: 2.733589123654109

Epoch: 6| Step: 2
Training loss: 2.785717487335205
Validation loss: 2.7357196346406014

Epoch: 6| Step: 3
Training loss: 3.498192310333252
Validation loss: 2.733292902669599

Epoch: 6| Step: 4
Training loss: 2.158998727798462
Validation loss: 2.734319145961474

Epoch: 6| Step: 5
Training loss: 3.089406967163086
Validation loss: 2.7342647916527203

Epoch: 6| Step: 6
Training loss: 2.987905502319336
Validation loss: 2.7348518474127657

Epoch: 6| Step: 7
Training loss: 2.4537525177001953
Validation loss: 2.737244644472676

Epoch: 6| Step: 8
Training loss: 3.221527099609375
Validation loss: 2.733479848472021

Epoch: 6| Step: 9
Training loss: 3.207852363586426
Validation loss: 2.7332578935930805

Epoch: 6| Step: 10
Training loss: 3.486955165863037
Validation loss: 2.732434247129707

Epoch: 6| Step: 11
Training loss: 2.8444395065307617
Validation loss: 2.7337525121627317

Epoch: 6| Step: 12
Training loss: 3.153822183609009
Validation loss: 2.731143859124953

Epoch: 6| Step: 13
Training loss: 2.263685703277588
Validation loss: 2.7357524159134075

Epoch: 55| Step: 0
Training loss: 2.425588846206665
Validation loss: 2.732631810249821

Epoch: 6| Step: 1
Training loss: 3.013587236404419
Validation loss: 2.7362465550822597

Epoch: 6| Step: 2
Training loss: 2.4279980659484863
Validation loss: 2.7364962177891887

Epoch: 6| Step: 3
Training loss: 3.5766139030456543
Validation loss: 2.7355596737195085

Epoch: 6| Step: 4
Training loss: 3.1449413299560547
Validation loss: 2.734786695049655

Epoch: 6| Step: 5
Training loss: 2.73953914642334
Validation loss: 2.7346270007471882

Epoch: 6| Step: 6
Training loss: 3.28016996383667
Validation loss: 2.7330127685300765

Epoch: 6| Step: 7
Training loss: 2.446444511413574
Validation loss: 2.731613625762283

Epoch: 6| Step: 8
Training loss: 2.9830665588378906
Validation loss: 2.7302715880896455

Epoch: 6| Step: 9
Training loss: 3.094740390777588
Validation loss: 2.7308659322800173

Epoch: 6| Step: 10
Training loss: 2.7480247020721436
Validation loss: 2.730180622428976

Epoch: 6| Step: 11
Training loss: 3.2982735633850098
Validation loss: 2.730706896833194

Epoch: 6| Step: 12
Training loss: 2.364891529083252
Validation loss: 2.729544283241354

Epoch: 6| Step: 13
Training loss: 2.8434295654296875
Validation loss: 2.732191857471261

Epoch: 56| Step: 0
Training loss: 1.9429380893707275
Validation loss: 2.7291872911555792

Epoch: 6| Step: 1
Training loss: 2.3535728454589844
Validation loss: 2.7286255667286534

Epoch: 6| Step: 2
Training loss: 3.1558494567871094
Validation loss: 2.729954581106863

Epoch: 6| Step: 3
Training loss: 3.1157994270324707
Validation loss: 2.72952240256853

Epoch: 6| Step: 4
Training loss: 3.447019100189209
Validation loss: 2.731788235325967

Epoch: 6| Step: 5
Training loss: 2.6617989540100098
Validation loss: 2.7325775315684657

Epoch: 6| Step: 6
Training loss: 2.80680513381958
Validation loss: 2.73043716338373

Epoch: 6| Step: 7
Training loss: 3.278669834136963
Validation loss: 2.731724798038442

Epoch: 6| Step: 8
Training loss: 3.026116132736206
Validation loss: 2.7303094684436755

Epoch: 6| Step: 9
Training loss: 2.7582359313964844
Validation loss: 2.7296248738483717

Epoch: 6| Step: 10
Training loss: 3.275019645690918
Validation loss: 2.731597464571717

Epoch: 6| Step: 11
Training loss: 2.789609909057617
Validation loss: 2.7331020293697232

Epoch: 6| Step: 12
Training loss: 2.5659751892089844
Validation loss: 2.7298280167323288

Epoch: 6| Step: 13
Training loss: 3.44116473197937
Validation loss: 2.728144625181793

Epoch: 57| Step: 0
Training loss: 3.508775234222412
Validation loss: 2.729354912234891

Epoch: 6| Step: 1
Training loss: 2.983733654022217
Validation loss: 2.7307730490161526

Epoch: 6| Step: 2
Training loss: 4.281229496002197
Validation loss: 2.7261983117749615

Epoch: 6| Step: 3
Training loss: 1.4175223112106323
Validation loss: 2.725936761466406

Epoch: 6| Step: 4
Training loss: 3.0766615867614746
Validation loss: 2.7303311440252487

Epoch: 6| Step: 5
Training loss: 3.028393268585205
Validation loss: 2.731435657829367

Epoch: 6| Step: 6
Training loss: 2.97894287109375
Validation loss: 2.729011376698812

Epoch: 6| Step: 7
Training loss: 2.509263753890991
Validation loss: 2.729540527507823

Epoch: 6| Step: 8
Training loss: 2.4341518878936768
Validation loss: 2.7307670629152687

Epoch: 6| Step: 9
Training loss: 3.2311272621154785
Validation loss: 2.734537027215445

Epoch: 6| Step: 10
Training loss: 2.72059965133667
Validation loss: 2.7316764939215874

Epoch: 6| Step: 11
Training loss: 3.3201797008514404
Validation loss: 2.7311133774377967

Epoch: 6| Step: 12
Training loss: 2.4848690032958984
Validation loss: 2.7275760327616045

Epoch: 6| Step: 13
Training loss: 2.005387783050537
Validation loss: 2.729731741771903

Epoch: 58| Step: 0
Training loss: 2.9671144485473633
Validation loss: 2.7276424079812984

Epoch: 6| Step: 1
Training loss: 2.531426429748535
Validation loss: 2.7284191116209953

Epoch: 6| Step: 2
Training loss: 3.1492185592651367
Validation loss: 2.7259993963344122

Epoch: 6| Step: 3
Training loss: 1.9765613079071045
Validation loss: 2.7239685520049064

Epoch: 6| Step: 4
Training loss: 3.5030713081359863
Validation loss: 2.7271094065840527

Epoch: 6| Step: 5
Training loss: 2.9665164947509766
Validation loss: 2.7309327330640567

Epoch: 6| Step: 6
Training loss: 2.818847417831421
Validation loss: 2.732663885239632

Epoch: 6| Step: 7
Training loss: 2.9602317810058594
Validation loss: 2.740604523689516

Epoch: 6| Step: 8
Training loss: 2.3889787197113037
Validation loss: 2.7325853301632788

Epoch: 6| Step: 9
Training loss: 2.8556299209594727
Validation loss: 2.7322054216938634

Epoch: 6| Step: 10
Training loss: 2.493561267852783
Validation loss: 2.725258993846114

Epoch: 6| Step: 11
Training loss: 3.866567611694336
Validation loss: 2.7235198815663657

Epoch: 6| Step: 12
Training loss: 3.3627891540527344
Validation loss: 2.7232946529183337

Epoch: 6| Step: 13
Training loss: 2.038135528564453
Validation loss: 2.7251688588050103

Epoch: 59| Step: 0
Training loss: 2.245169162750244
Validation loss: 2.724720747240128

Epoch: 6| Step: 1
Training loss: 2.359395742416382
Validation loss: 2.7256556839071293

Epoch: 6| Step: 2
Training loss: 2.8629708290100098
Validation loss: 2.724974266944393

Epoch: 6| Step: 3
Training loss: 3.0244672298431396
Validation loss: 2.7275896482570197

Epoch: 6| Step: 4
Training loss: 2.8120369911193848
Validation loss: 2.725400040226598

Epoch: 6| Step: 5
Training loss: 2.859341621398926
Validation loss: 2.7235099372043403

Epoch: 6| Step: 6
Training loss: 2.5089852809906006
Validation loss: 2.72615449659286

Epoch: 6| Step: 7
Training loss: 3.5571675300598145
Validation loss: 2.724665549493605

Epoch: 6| Step: 8
Training loss: 2.490037202835083
Validation loss: 2.723705291748047

Epoch: 6| Step: 9
Training loss: 2.700108051300049
Validation loss: 2.7214370799321

Epoch: 6| Step: 10
Training loss: 2.6846866607666016
Validation loss: 2.722012835164224

Epoch: 6| Step: 11
Training loss: 3.9007620811462402
Validation loss: 2.720124577963224

Epoch: 6| Step: 12
Training loss: 3.0539801120758057
Validation loss: 2.720606275784072

Epoch: 6| Step: 13
Training loss: 3.4502649307250977
Validation loss: 2.7215938721933672

Epoch: 60| Step: 0
Training loss: 2.7628612518310547
Validation loss: 2.723773010315434

Epoch: 6| Step: 1
Training loss: 3.3459367752075195
Validation loss: 2.722763720379081

Epoch: 6| Step: 2
Training loss: 2.4591665267944336
Validation loss: 2.7220952023742018

Epoch: 6| Step: 3
Training loss: 3.34842586517334
Validation loss: 2.72142534102163

Epoch: 6| Step: 4
Training loss: 3.2397916316986084
Validation loss: 2.7201636914283998

Epoch: 6| Step: 5
Training loss: 2.3773884773254395
Validation loss: 2.720155685178695

Epoch: 6| Step: 6
Training loss: 3.131319999694824
Validation loss: 2.7184671330195602

Epoch: 6| Step: 7
Training loss: 3.075305938720703
Validation loss: 2.7218950307497414

Epoch: 6| Step: 8
Training loss: 2.966566801071167
Validation loss: 2.7190049130429506

Epoch: 6| Step: 9
Training loss: 2.893782138824463
Validation loss: 2.719501792743642

Epoch: 6| Step: 10
Training loss: 2.469364881515503
Validation loss: 2.7201901853725476

Epoch: 6| Step: 11
Training loss: 2.8968167304992676
Validation loss: 2.718317134405977

Epoch: 6| Step: 12
Training loss: 2.438854694366455
Validation loss: 2.7165728179357385

Epoch: 6| Step: 13
Training loss: 2.8519952297210693
Validation loss: 2.7204399621614845

Epoch: 61| Step: 0
Training loss: 3.1520771980285645
Validation loss: 2.716298577606037

Epoch: 6| Step: 1
Training loss: 2.830551862716675
Validation loss: 2.7207566820165163

Epoch: 6| Step: 2
Training loss: 3.4839894771575928
Validation loss: 2.718343109212896

Epoch: 6| Step: 3
Training loss: 2.3744935989379883
Validation loss: 2.7179021220053396

Epoch: 6| Step: 4
Training loss: 2.609452724456787
Validation loss: 2.719942726114745

Epoch: 6| Step: 5
Training loss: 2.7270126342773438
Validation loss: 2.723400910695394

Epoch: 6| Step: 6
Training loss: 3.6512608528137207
Validation loss: 2.721945849798059

Epoch: 6| Step: 7
Training loss: 2.546401023864746
Validation loss: 2.719508219790715

Epoch: 6| Step: 8
Training loss: 2.730560064315796
Validation loss: 2.720278199001025

Epoch: 6| Step: 9
Training loss: 2.9865846633911133
Validation loss: 2.719471093147032

Epoch: 6| Step: 10
Training loss: 1.8321975469589233
Validation loss: 2.7235855184575564

Epoch: 6| Step: 11
Training loss: 2.8673388957977295
Validation loss: 2.721899660684729

Epoch: 6| Step: 12
Training loss: 3.5768048763275146
Validation loss: 2.7169192452584543

Epoch: 6| Step: 13
Training loss: 2.8005354404449463
Validation loss: 2.713836654540031

Epoch: 62| Step: 0
Training loss: 3.1056132316589355
Validation loss: 2.716084918668193

Epoch: 6| Step: 1
Training loss: 3.108515739440918
Validation loss: 2.72049972831562

Epoch: 6| Step: 2
Training loss: 1.6817126274108887
Validation loss: 2.7339394118196223

Epoch: 6| Step: 3
Training loss: 2.795605182647705
Validation loss: 2.751594502438781

Epoch: 6| Step: 4
Training loss: 2.9159977436065674
Validation loss: 2.763143377919351

Epoch: 6| Step: 5
Training loss: 3.121119976043701
Validation loss: 2.751570470871464

Epoch: 6| Step: 6
Training loss: 2.239297866821289
Validation loss: 2.7360254590229323

Epoch: 6| Step: 7
Training loss: 3.092585563659668
Validation loss: 2.7255937976221882

Epoch: 6| Step: 8
Training loss: 2.730524778366089
Validation loss: 2.723905363390523

Epoch: 6| Step: 9
Training loss: 3.274977207183838
Validation loss: 2.7246557999682683

Epoch: 6| Step: 10
Training loss: 3.456454277038574
Validation loss: 2.7308620022189234

Epoch: 6| Step: 11
Training loss: 3.5060689449310303
Validation loss: 2.734043287974532

Epoch: 6| Step: 12
Training loss: 2.6471123695373535
Validation loss: 2.7441302320008636

Epoch: 6| Step: 13
Training loss: 2.418850898742676
Validation loss: 2.744786859840475

Epoch: 63| Step: 0
Training loss: 3.3238344192504883
Validation loss: 2.749608988402992

Epoch: 6| Step: 1
Training loss: 2.487359046936035
Validation loss: 2.7645762607615483

Epoch: 6| Step: 2
Training loss: 2.988088607788086
Validation loss: 2.7663241099285822

Epoch: 6| Step: 3
Training loss: 3.040966272354126
Validation loss: 2.7511503645168838

Epoch: 6| Step: 4
Training loss: 2.9769444465637207
Validation loss: 2.7364139069793043

Epoch: 6| Step: 5
Training loss: 3.4018473625183105
Validation loss: 2.7373055411923315

Epoch: 6| Step: 6
Training loss: 3.5819976329803467
Validation loss: 2.7283618885983705

Epoch: 6| Step: 7
Training loss: 3.3267107009887695
Validation loss: 2.7274938552610335

Epoch: 6| Step: 8
Training loss: 2.1480302810668945
Validation loss: 2.721325530800768

Epoch: 6| Step: 9
Training loss: 2.4175050258636475
Validation loss: 2.71989913909666

Epoch: 6| Step: 10
Training loss: 2.8774282932281494
Validation loss: 2.72647023970081

Epoch: 6| Step: 11
Training loss: 2.124350070953369
Validation loss: 2.724967097723356

Epoch: 6| Step: 12
Training loss: 2.548534393310547
Validation loss: 2.7335781487085486

Epoch: 6| Step: 13
Training loss: 3.3673834800720215
Validation loss: 2.7244007459250827

Epoch: 64| Step: 0
Training loss: 2.90212082862854
Validation loss: 2.718782132671725

Epoch: 6| Step: 1
Training loss: 3.105104684829712
Validation loss: 2.7171650804499143

Epoch: 6| Step: 2
Training loss: 2.8836772441864014
Validation loss: 2.7150321878412718

Epoch: 6| Step: 3
Training loss: 2.7361044883728027
Validation loss: 2.7187050491250973

Epoch: 6| Step: 4
Training loss: 2.8551032543182373
Validation loss: 2.7165488350775933

Epoch: 6| Step: 5
Training loss: 2.3286306858062744
Validation loss: 2.7191149368081042

Epoch: 6| Step: 6
Training loss: 3.514519214630127
Validation loss: 2.7165345171446442

Epoch: 6| Step: 7
Training loss: 3.3226230144500732
Validation loss: 2.718007969599898

Epoch: 6| Step: 8
Training loss: 2.4082870483398438
Validation loss: 2.7229375134232225

Epoch: 6| Step: 9
Training loss: 2.573000907897949
Validation loss: 2.7145862912618988

Epoch: 6| Step: 10
Training loss: 2.765662670135498
Validation loss: 2.7162137672465336

Epoch: 6| Step: 11
Training loss: 2.8992366790771484
Validation loss: 2.7159918226221555

Epoch: 6| Step: 12
Training loss: 2.8106918334960938
Validation loss: 2.7131149884193175

Epoch: 6| Step: 13
Training loss: 3.211012125015259
Validation loss: 2.7110843299537577

Epoch: 65| Step: 0
Training loss: 3.375002861022949
Validation loss: 2.711889002912788

Epoch: 6| Step: 1
Training loss: 2.3861424922943115
Validation loss: 2.708520471408803

Epoch: 6| Step: 2
Training loss: 2.770475149154663
Validation loss: 2.710614086479269

Epoch: 6| Step: 3
Training loss: 3.445387125015259
Validation loss: 2.7099172786999772

Epoch: 6| Step: 4
Training loss: 2.7858633995056152
Validation loss: 2.710713947972944

Epoch: 6| Step: 5
Training loss: 2.4697775840759277
Validation loss: 2.7109437424649476

Epoch: 6| Step: 6
Training loss: 2.1191930770874023
Validation loss: 2.7083535604579474

Epoch: 6| Step: 7
Training loss: 2.0656561851501465
Validation loss: 2.709588640479631

Epoch: 6| Step: 8
Training loss: 2.109673023223877
Validation loss: 2.7107438400227535

Epoch: 6| Step: 9
Training loss: 3.474881172180176
Validation loss: 2.709323024237028

Epoch: 6| Step: 10
Training loss: 3.486044406890869
Validation loss: 2.7108000093890774

Epoch: 6| Step: 11
Training loss: 2.488567590713501
Validation loss: 2.711674585137316

Epoch: 6| Step: 12
Training loss: 4.264612674713135
Validation loss: 2.7115911104345836

Epoch: 6| Step: 13
Training loss: 2.8520307540893555
Validation loss: 2.7120940608362996

Epoch: 66| Step: 0
Training loss: 3.783724784851074
Validation loss: 2.7147644591587845

Epoch: 6| Step: 1
Training loss: 2.9947638511657715
Validation loss: 2.7140075955339658

Epoch: 6| Step: 2
Training loss: 2.4086251258850098
Validation loss: 2.715266127740183

Epoch: 6| Step: 3
Training loss: 3.257763385772705
Validation loss: 2.7100166787383375

Epoch: 6| Step: 4
Training loss: 2.0013232231140137
Validation loss: 2.708946825355612

Epoch: 6| Step: 5
Training loss: 3.3604817390441895
Validation loss: 2.7094556798217115

Epoch: 6| Step: 6
Training loss: 3.366354465484619
Validation loss: 2.7077356077009633

Epoch: 6| Step: 7
Training loss: 2.021096706390381
Validation loss: 2.705401748739263

Epoch: 6| Step: 8
Training loss: 2.8491032123565674
Validation loss: 2.703722962769129

Epoch: 6| Step: 9
Training loss: 2.4168519973754883
Validation loss: 2.7056565900002756

Epoch: 6| Step: 10
Training loss: 3.7481842041015625
Validation loss: 2.705727005517611

Epoch: 6| Step: 11
Training loss: 2.891122579574585
Validation loss: 2.7057382393908758

Epoch: 6| Step: 12
Training loss: 3.016204833984375
Validation loss: 2.705621063068349

Epoch: 6| Step: 13
Training loss: 1.2036749124526978
Validation loss: 2.706805080495855

Epoch: 67| Step: 0
Training loss: 3.0391225814819336
Validation loss: 2.704359528838947

Epoch: 6| Step: 1
Training loss: 3.3914296627044678
Validation loss: 2.708012516780566

Epoch: 6| Step: 2
Training loss: 3.4538683891296387
Validation loss: 2.706041641132806

Epoch: 6| Step: 3
Training loss: 3.3542592525482178
Validation loss: 2.707036179880942

Epoch: 6| Step: 4
Training loss: 3.2200756072998047
Validation loss: 2.7061801571999826

Epoch: 6| Step: 5
Training loss: 2.289057731628418
Validation loss: 2.7026781984554824

Epoch: 6| Step: 6
Training loss: 2.2072296142578125
Validation loss: 2.7040097662197646

Epoch: 6| Step: 7
Training loss: 3.06111478805542
Validation loss: 2.7028808491204375

Epoch: 6| Step: 8
Training loss: 2.961970329284668
Validation loss: 2.7032976996514106

Epoch: 6| Step: 9
Training loss: 2.567747116088867
Validation loss: 2.7022334990962857

Epoch: 6| Step: 10
Training loss: 2.521942138671875
Validation loss: 2.7043027467625116

Epoch: 6| Step: 11
Training loss: 2.5523602962493896
Validation loss: 2.7109823893475276

Epoch: 6| Step: 12
Training loss: 2.944892168045044
Validation loss: 2.7075267094437794

Epoch: 6| Step: 13
Training loss: 2.115028142929077
Validation loss: 2.7053236166636148

Epoch: 68| Step: 0
Training loss: 2.546635150909424
Validation loss: 2.7056187070826048

Epoch: 6| Step: 1
Training loss: 3.3697943687438965
Validation loss: 2.702622862272365

Epoch: 6| Step: 2
Training loss: 2.9659628868103027
Validation loss: 2.70083228875232

Epoch: 6| Step: 3
Training loss: 2.4895071983337402
Validation loss: 2.7025102210301224

Epoch: 6| Step: 4
Training loss: 2.0524721145629883
Validation loss: 2.6994045883096676

Epoch: 6| Step: 5
Training loss: 2.6064653396606445
Validation loss: 2.7017043380327124

Epoch: 6| Step: 6
Training loss: 2.6039648056030273
Validation loss: 2.7005978758617113

Epoch: 6| Step: 7
Training loss: 3.0350325107574463
Validation loss: 2.699719593089114

Epoch: 6| Step: 8
Training loss: 3.2007570266723633
Validation loss: 2.7015455948409213

Epoch: 6| Step: 9
Training loss: 2.6791396141052246
Validation loss: 2.700857759803854

Epoch: 6| Step: 10
Training loss: 3.7068066596984863
Validation loss: 2.7020816187704764

Epoch: 6| Step: 11
Training loss: 3.327488422393799
Validation loss: 2.6995731502450924

Epoch: 6| Step: 12
Training loss: 2.6140530109405518
Validation loss: 2.6988738300979778

Epoch: 6| Step: 13
Training loss: 2.667888641357422
Validation loss: 2.6991860597364363

Epoch: 69| Step: 0
Training loss: 2.1897025108337402
Validation loss: 2.70178287259994

Epoch: 6| Step: 1
Training loss: 3.7713449001312256
Validation loss: 2.7033523205787904

Epoch: 6| Step: 2
Training loss: 2.97013521194458
Validation loss: 2.704364163901216

Epoch: 6| Step: 3
Training loss: 2.9481687545776367
Validation loss: 2.7023224240990094

Epoch: 6| Step: 4
Training loss: 2.6658382415771484
Validation loss: 2.7031136738356722

Epoch: 6| Step: 5
Training loss: 3.668409585952759
Validation loss: 2.7056344503997476

Epoch: 6| Step: 6
Training loss: 2.244138240814209
Validation loss: 2.708381704104844

Epoch: 6| Step: 7
Training loss: 2.7282724380493164
Validation loss: 2.711110689306772

Epoch: 6| Step: 8
Training loss: 2.930394172668457
Validation loss: 2.714965112747685

Epoch: 6| Step: 9
Training loss: 3.3472023010253906
Validation loss: 2.7034979456214496

Epoch: 6| Step: 10
Training loss: 2.6701135635375977
Validation loss: 2.6981994464833248

Epoch: 6| Step: 11
Training loss: 1.9490337371826172
Validation loss: 2.69806101757993

Epoch: 6| Step: 12
Training loss: 2.9224822521209717
Validation loss: 2.694806411702146

Epoch: 6| Step: 13
Training loss: 2.954472303390503
Validation loss: 2.696522125633814

Epoch: 70| Step: 0
Training loss: 2.850264549255371
Validation loss: 2.693723186369865

Epoch: 6| Step: 1
Training loss: 1.7549270391464233
Validation loss: 2.6949197964001725

Epoch: 6| Step: 2
Training loss: 2.7982702255249023
Validation loss: 2.697079366253268

Epoch: 6| Step: 3
Training loss: 2.6767454147338867
Validation loss: 2.698521719183973

Epoch: 6| Step: 4
Training loss: 2.4091715812683105
Validation loss: 2.6949957493812806

Epoch: 6| Step: 5
Training loss: 2.8942816257476807
Validation loss: 2.6967636744181314

Epoch: 6| Step: 6
Training loss: 2.8911008834838867
Validation loss: 2.6975254345965642

Epoch: 6| Step: 7
Training loss: 2.849940061569214
Validation loss: 2.6959716786620436

Epoch: 6| Step: 8
Training loss: 2.306466817855835
Validation loss: 2.6961472265182005

Epoch: 6| Step: 9
Training loss: 3.352017879486084
Validation loss: 2.6927548429017425

Epoch: 6| Step: 10
Training loss: 3.3027725219726562
Validation loss: 2.6935588723869732

Epoch: 6| Step: 11
Training loss: 3.9312045574188232
Validation loss: 2.693957869724561

Epoch: 6| Step: 12
Training loss: 3.5386736392974854
Validation loss: 2.6940196816639235

Epoch: 6| Step: 13
Training loss: 2.016862154006958
Validation loss: 2.691609872284756

Epoch: 71| Step: 0
Training loss: 2.58811354637146
Validation loss: 2.6925770185327016

Epoch: 6| Step: 1
Training loss: 1.6677260398864746
Validation loss: 2.6934272294403403

Epoch: 6| Step: 2
Training loss: 3.052215576171875
Validation loss: 2.69371437257336

Epoch: 6| Step: 3
Training loss: 2.5719664096832275
Validation loss: 2.695215255983414

Epoch: 6| Step: 4
Training loss: 2.8365962505340576
Validation loss: 2.6961611932323826

Epoch: 6| Step: 5
Training loss: 2.7030532360076904
Validation loss: 2.69670440817392

Epoch: 6| Step: 6
Training loss: 2.7028043270111084
Validation loss: 2.698228105421989

Epoch: 6| Step: 7
Training loss: 3.631019115447998
Validation loss: 2.6975323692444833

Epoch: 6| Step: 8
Training loss: 2.4764795303344727
Validation loss: 2.694099659560829

Epoch: 6| Step: 9
Training loss: 3.412400722503662
Validation loss: 2.6921185293505268

Epoch: 6| Step: 10
Training loss: 2.4199275970458984
Validation loss: 2.6889696839035198

Epoch: 6| Step: 11
Training loss: 3.2139809131622314
Validation loss: 2.690922539721253

Epoch: 6| Step: 12
Training loss: 3.540823459625244
Validation loss: 2.689702864616148

Epoch: 6| Step: 13
Training loss: 3.2668910026550293
Validation loss: 2.690910336791828

Epoch: 72| Step: 0
Training loss: 3.1648898124694824
Validation loss: 2.6931257324834026

Epoch: 6| Step: 1
Training loss: 2.3073549270629883
Validation loss: 2.6926995477368756

Epoch: 6| Step: 2
Training loss: 2.649139404296875
Validation loss: 2.691141425922353

Epoch: 6| Step: 3
Training loss: 3.138212203979492
Validation loss: 2.698200438612251

Epoch: 6| Step: 4
Training loss: 3.7933764457702637
Validation loss: 2.6971413397019908

Epoch: 6| Step: 5
Training loss: 2.6595115661621094
Validation loss: 2.6960355697139615

Epoch: 6| Step: 6
Training loss: 2.605762004852295
Validation loss: 2.694805560573455

Epoch: 6| Step: 7
Training loss: 2.5482003688812256
Validation loss: 2.697411355151925

Epoch: 6| Step: 8
Training loss: 2.4596288204193115
Validation loss: 2.6919088978921213

Epoch: 6| Step: 9
Training loss: 3.2170982360839844
Validation loss: 2.6908499681821434

Epoch: 6| Step: 10
Training loss: 3.037344455718994
Validation loss: 2.6896668967380317

Epoch: 6| Step: 11
Training loss: 3.468118667602539
Validation loss: 2.6896525659868793

Epoch: 6| Step: 12
Training loss: 2.738889694213867
Validation loss: 2.6885048112561627

Epoch: 6| Step: 13
Training loss: 1.432015061378479
Validation loss: 2.688306229088896

Epoch: 73| Step: 0
Training loss: 2.3564045429229736
Validation loss: 2.6848981739372335

Epoch: 6| Step: 1
Training loss: 3.672459602355957
Validation loss: 2.6861286317148516

Epoch: 6| Step: 2
Training loss: 3.5102109909057617
Validation loss: 2.6884983431908394

Epoch: 6| Step: 3
Training loss: 3.2052206993103027
Validation loss: 2.693268117084298

Epoch: 6| Step: 4
Training loss: 2.2695813179016113
Validation loss: 2.697221968763618

Epoch: 6| Step: 5
Training loss: 2.7980268001556396
Validation loss: 2.702677401163245

Epoch: 6| Step: 6
Training loss: 3.138493537902832
Validation loss: 2.710987203864641

Epoch: 6| Step: 7
Training loss: 2.004080057144165
Validation loss: 2.702891489510895

Epoch: 6| Step: 8
Training loss: 3.317991256713867
Validation loss: 2.7015650041641726

Epoch: 6| Step: 9
Training loss: 2.7157421112060547
Validation loss: 2.700487167604508

Epoch: 6| Step: 10
Training loss: 2.968696117401123
Validation loss: 2.697532284644342

Epoch: 6| Step: 11
Training loss: 2.4731149673461914
Validation loss: 2.6892319994588054

Epoch: 6| Step: 12
Training loss: 2.615935802459717
Validation loss: 2.6864391065412954

Epoch: 6| Step: 13
Training loss: 2.782927989959717
Validation loss: 2.68414395342591

Epoch: 74| Step: 0
Training loss: 3.1051459312438965
Validation loss: 2.6886796976930354

Epoch: 6| Step: 1
Training loss: 3.2490227222442627
Validation loss: 2.691028646243516

Epoch: 6| Step: 2
Training loss: 2.0427627563476562
Validation loss: 2.6980167870880454

Epoch: 6| Step: 3
Training loss: 3.2343921661376953
Validation loss: 2.7017826111085954

Epoch: 6| Step: 4
Training loss: 2.7935221195220947
Validation loss: 2.699023390329012

Epoch: 6| Step: 5
Training loss: 3.025928497314453
Validation loss: 2.692776961993146

Epoch: 6| Step: 6
Training loss: 2.4579806327819824
Validation loss: 2.6899554062915105

Epoch: 6| Step: 7
Training loss: 2.9831204414367676
Validation loss: 2.691196687759892

Epoch: 6| Step: 8
Training loss: 2.7667641639709473
Validation loss: 2.6872217014271724

Epoch: 6| Step: 9
Training loss: 3.144202709197998
Validation loss: 2.6860567985042447

Epoch: 6| Step: 10
Training loss: 3.341458797454834
Validation loss: 2.683222929636637

Epoch: 6| Step: 11
Training loss: 2.8998446464538574
Validation loss: 2.686360151537003

Epoch: 6| Step: 12
Training loss: 2.2233705520629883
Validation loss: 2.683111749669557

Epoch: 6| Step: 13
Training loss: 2.538490056991577
Validation loss: 2.6857746134522142

Epoch: 75| Step: 0
Training loss: 3.1926803588867188
Validation loss: 2.6903441106119463

Epoch: 6| Step: 1
Training loss: 3.007155179977417
Validation loss: 2.6915731686417774

Epoch: 6| Step: 2
Training loss: 3.2705202102661133
Validation loss: 2.6949610684507634

Epoch: 6| Step: 3
Training loss: 2.4137072563171387
Validation loss: 2.6943937732327368

Epoch: 6| Step: 4
Training loss: 1.86009681224823
Validation loss: 2.689138397093742

Epoch: 6| Step: 5
Training loss: 2.411665678024292
Validation loss: 2.690902594597109

Epoch: 6| Step: 6
Training loss: 2.6183242797851562
Validation loss: 2.68937079111735

Epoch: 6| Step: 7
Training loss: 2.74113130569458
Validation loss: 2.687452857212354

Epoch: 6| Step: 8
Training loss: 2.8330819606781006
Validation loss: 2.687401625417894

Epoch: 6| Step: 9
Training loss: 3.382533550262451
Validation loss: 2.690497852140857

Epoch: 6| Step: 10
Training loss: 3.6498847007751465
Validation loss: 2.6842911576712005

Epoch: 6| Step: 11
Training loss: 2.6037721633911133
Validation loss: 2.6863954221048663

Epoch: 6| Step: 12
Training loss: 2.6215081214904785
Validation loss: 2.6876028250622492

Epoch: 6| Step: 13
Training loss: 3.4479458332061768
Validation loss: 2.6897814658380326

Epoch: 76| Step: 0
Training loss: 3.2491321563720703
Validation loss: 2.689042409261068

Epoch: 6| Step: 1
Training loss: 2.9288668632507324
Validation loss: 2.691605960169146

Epoch: 6| Step: 2
Training loss: 2.805999755859375
Validation loss: 2.6938036872494604

Epoch: 6| Step: 3
Training loss: 2.09763240814209
Validation loss: 2.6968980655875257

Epoch: 6| Step: 4
Training loss: 2.874415397644043
Validation loss: 2.704870218871742

Epoch: 6| Step: 5
Training loss: 2.9317328929901123
Validation loss: 2.710037431409282

Epoch: 6| Step: 6
Training loss: 3.371954917907715
Validation loss: 2.6940106217579176

Epoch: 6| Step: 7
Training loss: 3.1708483695983887
Validation loss: 2.68645719815326

Epoch: 6| Step: 8
Training loss: 3.0794930458068848
Validation loss: 2.6861821374585553

Epoch: 6| Step: 9
Training loss: 3.1142425537109375
Validation loss: 2.6832138338396625

Epoch: 6| Step: 10
Training loss: 2.799886703491211
Validation loss: 2.6908065478006997

Epoch: 6| Step: 11
Training loss: 2.1331353187561035
Validation loss: 2.690046630879884

Epoch: 6| Step: 12
Training loss: 2.0150389671325684
Validation loss: 2.689714462526383

Epoch: 6| Step: 13
Training loss: 3.4086081981658936
Validation loss: 2.6909095446268716

Epoch: 77| Step: 0
Training loss: 2.717268466949463
Validation loss: 2.6895591469221216

Epoch: 6| Step: 1
Training loss: 3.096280574798584
Validation loss: 2.6866447156475437

Epoch: 6| Step: 2
Training loss: 2.062833309173584
Validation loss: 2.6811410380947973

Epoch: 6| Step: 3
Training loss: 2.5313901901245117
Validation loss: 2.680286451052594

Epoch: 6| Step: 4
Training loss: 3.146608829498291
Validation loss: 2.677948805593675

Epoch: 6| Step: 5
Training loss: 2.8667373657226562
Validation loss: 2.678632559314851

Epoch: 6| Step: 6
Training loss: 2.8360648155212402
Validation loss: 2.675443687746602

Epoch: 6| Step: 7
Training loss: 3.259636402130127
Validation loss: 2.67864594920989

Epoch: 6| Step: 8
Training loss: 2.5061895847320557
Validation loss: 2.674362108271609

Epoch: 6| Step: 9
Training loss: 2.615391969680786
Validation loss: 2.6782040647281113

Epoch: 6| Step: 10
Training loss: 3.482215166091919
Validation loss: 2.6753724698097474

Epoch: 6| Step: 11
Training loss: 2.416564702987671
Validation loss: 2.6758872129583873

Epoch: 6| Step: 12
Training loss: 3.312575340270996
Validation loss: 2.6764345835613947

Epoch: 6| Step: 13
Training loss: 2.896080255508423
Validation loss: 2.6760786220591557

Epoch: 78| Step: 0
Training loss: 3.054720163345337
Validation loss: 2.677207613504061

Epoch: 6| Step: 1
Training loss: 2.865006446838379
Validation loss: 2.68051980644144

Epoch: 6| Step: 2
Training loss: 3.789574146270752
Validation loss: 2.6790703291534097

Epoch: 6| Step: 3
Training loss: 3.5720982551574707
Validation loss: 2.6853693992860856

Epoch: 6| Step: 4
Training loss: 2.569316864013672
Validation loss: 2.6838867997610443

Epoch: 6| Step: 5
Training loss: 2.5471129417419434
Validation loss: 2.685355535117529

Epoch: 6| Step: 6
Training loss: 2.355400562286377
Validation loss: 2.6899513454847437

Epoch: 6| Step: 7
Training loss: 3.431767463684082
Validation loss: 2.6930547401469243

Epoch: 6| Step: 8
Training loss: 2.683455228805542
Validation loss: 2.6881819181544806

Epoch: 6| Step: 9
Training loss: 3.1120691299438477
Validation loss: 2.6861715803864183

Epoch: 6| Step: 10
Training loss: 2.4685707092285156
Validation loss: 2.680495733855873

Epoch: 6| Step: 11
Training loss: 2.448566198348999
Validation loss: 2.677865300127255

Epoch: 6| Step: 12
Training loss: 2.4613728523254395
Validation loss: 2.6751091018799813

Epoch: 6| Step: 13
Training loss: 1.9540960788726807
Validation loss: 2.675503994828911

Epoch: 79| Step: 0
Training loss: 3.4849722385406494
Validation loss: 2.6730131205692085

Epoch: 6| Step: 1
Training loss: 2.2943029403686523
Validation loss: 2.676052875416253

Epoch: 6| Step: 2
Training loss: 2.3271429538726807
Validation loss: 2.6749878750052503

Epoch: 6| Step: 3
Training loss: 3.285778284072876
Validation loss: 2.67476160295548

Epoch: 6| Step: 4
Training loss: 3.2247354984283447
Validation loss: 2.6809202060904553

Epoch: 6| Step: 5
Training loss: 2.3407833576202393
Validation loss: 2.6822381942502913

Epoch: 6| Step: 6
Training loss: 3.554851531982422
Validation loss: 2.6763374420904342

Epoch: 6| Step: 7
Training loss: 2.1866817474365234
Validation loss: 2.676754459258049

Epoch: 6| Step: 8
Training loss: 2.85528564453125
Validation loss: 2.6731158841040825

Epoch: 6| Step: 9
Training loss: 2.7726473808288574
Validation loss: 2.6711186747397146

Epoch: 6| Step: 10
Training loss: 3.147310733795166
Validation loss: 2.6744190390392015

Epoch: 6| Step: 11
Training loss: 2.8004684448242188
Validation loss: 2.6704102664865474

Epoch: 6| Step: 12
Training loss: 2.5582613945007324
Validation loss: 2.6709477440003426

Epoch: 6| Step: 13
Training loss: 2.872910737991333
Validation loss: 2.6688584896825973

Epoch: 80| Step: 0
Training loss: 2.1644210815429688
Validation loss: 2.6717340048923286

Epoch: 6| Step: 1
Training loss: 2.1160998344421387
Validation loss: 2.6729381468988236

Epoch: 6| Step: 2
Training loss: 3.0692763328552246
Validation loss: 2.6717547062904603

Epoch: 6| Step: 3
Training loss: 3.483896255493164
Validation loss: 2.6738567019021637

Epoch: 6| Step: 4
Training loss: 2.8338468074798584
Validation loss: 2.673479651892057

Epoch: 6| Step: 5
Training loss: 1.902109146118164
Validation loss: 2.6725650141316075

Epoch: 6| Step: 6
Training loss: 3.927319288253784
Validation loss: 2.6777998888364403

Epoch: 6| Step: 7
Training loss: 3.0435683727264404
Validation loss: 2.677354379366803

Epoch: 6| Step: 8
Training loss: 2.8623805046081543
Validation loss: 2.680077987332498

Epoch: 6| Step: 9
Training loss: 2.1983776092529297
Validation loss: 2.6711025186764297

Epoch: 6| Step: 10
Training loss: 3.0131967067718506
Validation loss: 2.66945711258919

Epoch: 6| Step: 11
Training loss: 3.3910136222839355
Validation loss: 2.6699569404766126

Epoch: 6| Step: 12
Training loss: 2.3418004512786865
Validation loss: 2.6680612820450977

Epoch: 6| Step: 13
Training loss: 3.58579158782959
Validation loss: 2.6657847025061168

Epoch: 81| Step: 0
Training loss: 2.601256847381592
Validation loss: 2.6647754484607327

Epoch: 6| Step: 1
Training loss: 4.302388668060303
Validation loss: 2.66502115290652

Epoch: 6| Step: 2
Training loss: 2.72751784324646
Validation loss: 2.6646224324421217

Epoch: 6| Step: 3
Training loss: 2.9739387035369873
Validation loss: 2.6648726437681463

Epoch: 6| Step: 4
Training loss: 3.179165840148926
Validation loss: 2.6637798150380454

Epoch: 6| Step: 5
Training loss: 2.2931416034698486
Validation loss: 2.6661075340804232

Epoch: 6| Step: 6
Training loss: 3.5702600479125977
Validation loss: 2.669320885853101

Epoch: 6| Step: 7
Training loss: 1.4110358953475952
Validation loss: 2.675096768204884

Epoch: 6| Step: 8
Training loss: 2.7432363033294678
Validation loss: 2.6762636528220227

Epoch: 6| Step: 9
Training loss: 2.3139805793762207
Validation loss: 2.6817452138470066

Epoch: 6| Step: 10
Training loss: 3.539304256439209
Validation loss: 2.673319257715697

Epoch: 6| Step: 11
Training loss: 2.6333353519439697
Validation loss: 2.6699888116569928

Epoch: 6| Step: 12
Training loss: 2.5175724029541016
Validation loss: 2.6679464155627834

Epoch: 6| Step: 13
Training loss: 2.7676069736480713
Validation loss: 2.665355400372577

Epoch: 82| Step: 0
Training loss: 2.4449777603149414
Validation loss: 2.664020405020765

Epoch: 6| Step: 1
Training loss: 2.878126859664917
Validation loss: 2.664165340444093

Epoch: 6| Step: 2
Training loss: 2.9692749977111816
Validation loss: 2.664466006781465

Epoch: 6| Step: 3
Training loss: 2.710214138031006
Validation loss: 2.6612531703005553

Epoch: 6| Step: 4
Training loss: 3.235517978668213
Validation loss: 2.6647120803915043

Epoch: 6| Step: 5
Training loss: 3.318206548690796
Validation loss: 2.66364550077787

Epoch: 6| Step: 6
Training loss: 2.1076087951660156
Validation loss: 2.6656659726173646

Epoch: 6| Step: 7
Training loss: 2.722393751144409
Validation loss: 2.6662678334020797

Epoch: 6| Step: 8
Training loss: 3.175485134124756
Validation loss: 2.665535867855113

Epoch: 6| Step: 9
Training loss: 3.5290591716766357
Validation loss: 2.664354788359775

Epoch: 6| Step: 10
Training loss: 2.8211820125579834
Validation loss: 2.6648286722039662

Epoch: 6| Step: 11
Training loss: 2.7522096633911133
Validation loss: 2.662742417345765

Epoch: 6| Step: 12
Training loss: 1.8372578620910645
Validation loss: 2.661412139092722

Epoch: 6| Step: 13
Training loss: 3.1578290462493896
Validation loss: 2.663555047845328

Epoch: 83| Step: 0
Training loss: 2.698004722595215
Validation loss: 2.66613600074604

Epoch: 6| Step: 1
Training loss: 3.154665470123291
Validation loss: 2.6691471607454362

Epoch: 6| Step: 2
Training loss: 2.896090507507324
Validation loss: 2.66990331680544

Epoch: 6| Step: 3
Training loss: 2.9541773796081543
Validation loss: 2.6707830736714024

Epoch: 6| Step: 4
Training loss: 3.3049263954162598
Validation loss: 2.6719070839625534

Epoch: 6| Step: 5
Training loss: 2.941777229309082
Validation loss: 2.6746432550491823

Epoch: 6| Step: 6
Training loss: 2.5296339988708496
Validation loss: 2.672390345604189

Epoch: 6| Step: 7
Training loss: 3.0205001831054688
Validation loss: 2.6760054480644966

Epoch: 6| Step: 8
Training loss: 1.9351862668991089
Validation loss: 2.675930018066078

Epoch: 6| Step: 9
Training loss: 2.54081392288208
Validation loss: 2.6727151204180974

Epoch: 6| Step: 10
Training loss: 2.615570306777954
Validation loss: 2.668700402782809

Epoch: 6| Step: 11
Training loss: 3.1618711948394775
Validation loss: 2.6671777079182286

Epoch: 6| Step: 12
Training loss: 3.229306697845459
Validation loss: 2.6640514942907516

Epoch: 6| Step: 13
Training loss: 2.3429367542266846
Validation loss: 2.663198822288103

Epoch: 84| Step: 0
Training loss: 2.3067569732666016
Validation loss: 2.6609682421530447

Epoch: 6| Step: 1
Training loss: 2.0993869304656982
Validation loss: 2.660330767272621

Epoch: 6| Step: 2
Training loss: 3.051668643951416
Validation loss: 2.658458432843608

Epoch: 6| Step: 3
Training loss: 2.8125858306884766
Validation loss: 2.660120407740275

Epoch: 6| Step: 4
Training loss: 3.541175603866577
Validation loss: 2.6581967133347706

Epoch: 6| Step: 5
Training loss: 3.548041343688965
Validation loss: 2.6598861755863314

Epoch: 6| Step: 6
Training loss: 2.79652738571167
Validation loss: 2.6585619141978603

Epoch: 6| Step: 7
Training loss: 3.6703319549560547
Validation loss: 2.659220767277543

Epoch: 6| Step: 8
Training loss: 2.92518949508667
Validation loss: 2.659672673030566

Epoch: 6| Step: 9
Training loss: 2.3356547355651855
Validation loss: 2.6600354076713644

Epoch: 6| Step: 10
Training loss: 2.7625770568847656
Validation loss: 2.6605700523622575

Epoch: 6| Step: 11
Training loss: 2.1917877197265625
Validation loss: 2.6607419034486175

Epoch: 6| Step: 12
Training loss: 2.7705607414245605
Validation loss: 2.6579386803411666

Epoch: 6| Step: 13
Training loss: 2.498120069503784
Validation loss: 2.658412810294859

Epoch: 85| Step: 0
Training loss: 2.6080141067504883
Validation loss: 2.6564432010855725

Epoch: 6| Step: 1
Training loss: 2.8665997982025146
Validation loss: 2.6563761029192197

Epoch: 6| Step: 2
Training loss: 2.848576545715332
Validation loss: 2.658706595820765

Epoch: 6| Step: 3
Training loss: 3.423034191131592
Validation loss: 2.6538572542129026

Epoch: 6| Step: 4
Training loss: 3.038517951965332
Validation loss: 2.655518783036099

Epoch: 6| Step: 5
Training loss: 2.0893330574035645
Validation loss: 2.6551486907466764

Epoch: 6| Step: 6
Training loss: 3.2927842140197754
Validation loss: 2.6528857702850015

Epoch: 6| Step: 7
Training loss: 2.0456454753875732
Validation loss: 2.654826348827731

Epoch: 6| Step: 8
Training loss: 2.530719041824341
Validation loss: 2.6540796474743913

Epoch: 6| Step: 9
Training loss: 2.738732099533081
Validation loss: 2.654421498698573

Epoch: 6| Step: 10
Training loss: 3.11087703704834
Validation loss: 2.654225234062441

Epoch: 6| Step: 11
Training loss: 2.854309558868408
Validation loss: 2.657627631259221

Epoch: 6| Step: 12
Training loss: 2.675285816192627
Validation loss: 2.6550560792287192

Epoch: 6| Step: 13
Training loss: 3.7023086547851562
Validation loss: 2.6565186977386475

Epoch: 86| Step: 0
Training loss: 2.576449155807495
Validation loss: 2.6551235004137923

Epoch: 6| Step: 1
Training loss: 3.300595283508301
Validation loss: 2.6571051766795497

Epoch: 6| Step: 2
Training loss: 3.011681079864502
Validation loss: 2.6572937478301344

Epoch: 6| Step: 3
Training loss: 3.11163592338562
Validation loss: 2.654441992441813

Epoch: 6| Step: 4
Training loss: 2.6230010986328125
Validation loss: 2.6534001442693893

Epoch: 6| Step: 5
Training loss: 2.871213436126709
Validation loss: 2.652918300321025

Epoch: 6| Step: 6
Training loss: 3.083472728729248
Validation loss: 2.6546898734185005

Epoch: 6| Step: 7
Training loss: 2.352302074432373
Validation loss: 2.6514834434755388

Epoch: 6| Step: 8
Training loss: 2.0115256309509277
Validation loss: 2.6510925549332813

Epoch: 6| Step: 9
Training loss: 2.630280017852783
Validation loss: 2.6505043532258723

Epoch: 6| Step: 10
Training loss: 2.908336639404297
Validation loss: 2.652852286574661

Epoch: 6| Step: 11
Training loss: 3.1707348823547363
Validation loss: 2.6522641438309864

Epoch: 6| Step: 12
Training loss: 3.2945220470428467
Validation loss: 2.654702445512177

Epoch: 6| Step: 13
Training loss: 2.114461660385132
Validation loss: 2.651458263397217

Epoch: 87| Step: 0
Training loss: 2.1228718757629395
Validation loss: 2.655701514213316

Epoch: 6| Step: 1
Training loss: 2.5240228176116943
Validation loss: 2.6530697191915205

Epoch: 6| Step: 2
Training loss: 3.4772799015045166
Validation loss: 2.6565582598409345

Epoch: 6| Step: 3
Training loss: 2.5183169841766357
Validation loss: 2.6538886895743747

Epoch: 6| Step: 4
Training loss: 2.829216718673706
Validation loss: 2.6564929536593858

Epoch: 6| Step: 5
Training loss: 3.3025643825531006
Validation loss: 2.6505841029587613

Epoch: 6| Step: 6
Training loss: 2.736109495162964
Validation loss: 2.6520849966233775

Epoch: 6| Step: 7
Training loss: 2.540834426879883
Validation loss: 2.651688926963396

Epoch: 6| Step: 8
Training loss: 2.6530463695526123
Validation loss: 2.6488612108333136

Epoch: 6| Step: 9
Training loss: 2.2275779247283936
Validation loss: 2.6510251619482554

Epoch: 6| Step: 10
Training loss: 3.290032148361206
Validation loss: 2.65322458359503

Epoch: 6| Step: 11
Training loss: 2.8360700607299805
Validation loss: 2.6558670946346816

Epoch: 6| Step: 12
Training loss: 3.070807456970215
Validation loss: 2.6556691751685193

Epoch: 6| Step: 13
Training loss: 3.5512163639068604
Validation loss: 2.6533501455860753

Epoch: 88| Step: 0
Training loss: 2.645394802093506
Validation loss: 2.6509919474201817

Epoch: 6| Step: 1
Training loss: 3.1053996086120605
Validation loss: 2.648219344436481

Epoch: 6| Step: 2
Training loss: 2.9228744506835938
Validation loss: 2.650616212557721

Epoch: 6| Step: 3
Training loss: 3.144352912902832
Validation loss: 2.649546436084214

Epoch: 6| Step: 4
Training loss: 3.363856315612793
Validation loss: 2.649830969431067

Epoch: 6| Step: 5
Training loss: 3.1714515686035156
Validation loss: 2.647397328448552

Epoch: 6| Step: 6
Training loss: 2.2243311405181885
Validation loss: 2.6494664761327926

Epoch: 6| Step: 7
Training loss: 3.018519878387451
Validation loss: 2.6485780208341536

Epoch: 6| Step: 8
Training loss: 3.168900489807129
Validation loss: 2.647573588996805

Epoch: 6| Step: 9
Training loss: 2.460221290588379
Validation loss: 2.6482411097454768

Epoch: 6| Step: 10
Training loss: 2.4663586616516113
Validation loss: 2.648146326823901

Epoch: 6| Step: 11
Training loss: 2.7165942192077637
Validation loss: 2.6479972536845873

Epoch: 6| Step: 12
Training loss: 2.3796262741088867
Validation loss: 2.6474489678618727

Epoch: 6| Step: 13
Training loss: 2.270879030227661
Validation loss: 2.6468815700982207

Epoch: 89| Step: 0
Training loss: 2.72611927986145
Validation loss: 2.6440660133156726

Epoch: 6| Step: 1
Training loss: 3.3679733276367188
Validation loss: 2.646875327633273

Epoch: 6| Step: 2
Training loss: 2.570986270904541
Validation loss: 2.643820039687618

Epoch: 6| Step: 3
Training loss: 3.034888982772827
Validation loss: 2.6452363562840286

Epoch: 6| Step: 4
Training loss: 3.0554141998291016
Validation loss: 2.6437013303079913

Epoch: 6| Step: 5
Training loss: 3.0878148078918457
Validation loss: 2.6441993790288127

Epoch: 6| Step: 6
Training loss: 2.5415689945220947
Validation loss: 2.6424182384244856

Epoch: 6| Step: 7
Training loss: 2.5960638523101807
Validation loss: 2.6459282752006286

Epoch: 6| Step: 8
Training loss: 2.6933915615081787
Validation loss: 2.6421355637170936

Epoch: 6| Step: 9
Training loss: 2.770618438720703
Validation loss: 2.6419189617198002

Epoch: 6| Step: 10
Training loss: 2.637192726135254
Validation loss: 2.643165703742735

Epoch: 6| Step: 11
Training loss: 2.5940423011779785
Validation loss: 2.642888876699632

Epoch: 6| Step: 12
Training loss: 3.042222023010254
Validation loss: 2.6476212419489378

Epoch: 6| Step: 13
Training loss: 2.37679386138916
Validation loss: 2.650050832379249

Epoch: 90| Step: 0
Training loss: 2.940716505050659
Validation loss: 2.658125886353113

Epoch: 6| Step: 1
Training loss: 3.152186393737793
Validation loss: 2.6454377276923067

Epoch: 6| Step: 2
Training loss: 3.5070977210998535
Validation loss: 2.645622671291392

Epoch: 6| Step: 3
Training loss: 2.939488410949707
Validation loss: 2.643955553731611

Epoch: 6| Step: 4
Training loss: 2.472576141357422
Validation loss: 2.6426090399424234

Epoch: 6| Step: 5
Training loss: 3.533804416656494
Validation loss: 2.643397993938897

Epoch: 6| Step: 6
Training loss: 1.9389902353286743
Validation loss: 2.647034563044066

Epoch: 6| Step: 7
Training loss: 2.2195513248443604
Validation loss: 2.64532757061784

Epoch: 6| Step: 8
Training loss: 2.2272980213165283
Validation loss: 2.6488887110064105

Epoch: 6| Step: 9
Training loss: 2.5601632595062256
Validation loss: 2.649623288903185

Epoch: 6| Step: 10
Training loss: 2.6618568897247314
Validation loss: 2.6452117914794595

Epoch: 6| Step: 11
Training loss: 3.2178454399108887
Validation loss: 2.6460855058444444

Epoch: 6| Step: 12
Training loss: 2.1574866771698
Validation loss: 2.645732351528701

Epoch: 6| Step: 13
Training loss: 4.636081695556641
Validation loss: 2.6447938052556847

Epoch: 91| Step: 0
Training loss: 2.9905314445495605
Validation loss: 2.6442595707472933

Epoch: 6| Step: 1
Training loss: 2.598475933074951
Validation loss: 2.641540555543797

Epoch: 6| Step: 2
Training loss: 2.7209978103637695
Validation loss: 2.6421184616704143

Epoch: 6| Step: 3
Training loss: 2.9927420616149902
Validation loss: 2.6395037263952275

Epoch: 6| Step: 4
Training loss: 2.9761157035827637
Validation loss: 2.639052470525106

Epoch: 6| Step: 5
Training loss: 2.666421413421631
Validation loss: 2.6405657132466636

Epoch: 6| Step: 6
Training loss: 2.830889940261841
Validation loss: 2.6381775717581473

Epoch: 6| Step: 7
Training loss: 2.0198068618774414
Validation loss: 2.6377042583239976

Epoch: 6| Step: 8
Training loss: 3.6301424503326416
Validation loss: 2.6370385564783567

Epoch: 6| Step: 9
Training loss: 3.1695895195007324
Validation loss: 2.639552418903638

Epoch: 6| Step: 10
Training loss: 2.9399170875549316
Validation loss: 2.636561603956325

Epoch: 6| Step: 11
Training loss: 2.2017898559570312
Validation loss: 2.642328934002948

Epoch: 6| Step: 12
Training loss: 3.089846134185791
Validation loss: 2.641016096197149

Epoch: 6| Step: 13
Training loss: 2.1763198375701904
Validation loss: 2.6429272646545083

Epoch: 92| Step: 0
Training loss: 2.20098876953125
Validation loss: 2.6419769692164596

Epoch: 6| Step: 1
Training loss: 2.7196569442749023
Validation loss: 2.643632214556458

Epoch: 6| Step: 2
Training loss: 2.8676698207855225
Validation loss: 2.642791755737797

Epoch: 6| Step: 3
Training loss: 2.1978163719177246
Validation loss: 2.639967151867446

Epoch: 6| Step: 4
Training loss: 3.299245595932007
Validation loss: 2.6397370317930817

Epoch: 6| Step: 5
Training loss: 3.7936878204345703
Validation loss: 2.6364172120248117

Epoch: 6| Step: 6
Training loss: 2.749990463256836
Validation loss: 2.637979458737117

Epoch: 6| Step: 7
Training loss: 2.551966667175293
Validation loss: 2.63784965392082

Epoch: 6| Step: 8
Training loss: 2.6850521564483643
Validation loss: 2.6382862906302176

Epoch: 6| Step: 9
Training loss: 2.787287712097168
Validation loss: 2.636590447477115

Epoch: 6| Step: 10
Training loss: 2.9555864334106445
Validation loss: 2.6365011199828117

Epoch: 6| Step: 11
Training loss: 2.4258861541748047
Validation loss: 2.6364509110809653

Epoch: 6| Step: 12
Training loss: 2.9215850830078125
Validation loss: 2.635345010347264

Epoch: 6| Step: 13
Training loss: 3.2982301712036133
Validation loss: 2.6368290506383425

Epoch: 93| Step: 0
Training loss: 2.181431531906128
Validation loss: 2.6356034176324004

Epoch: 6| Step: 1
Training loss: 2.6485109329223633
Validation loss: 2.634691135857695

Epoch: 6| Step: 2
Training loss: 2.654817581176758
Validation loss: 2.6378098251999065

Epoch: 6| Step: 3
Training loss: 3.8342299461364746
Validation loss: 2.6354453512417373

Epoch: 6| Step: 4
Training loss: 2.3052430152893066
Validation loss: 2.634326414395404

Epoch: 6| Step: 5
Training loss: 2.435701847076416
Validation loss: 2.6367987022604993

Epoch: 6| Step: 6
Training loss: 2.68241286277771
Validation loss: 2.6365183784115698

Epoch: 6| Step: 7
Training loss: 3.423435688018799
Validation loss: 2.637931175129388

Epoch: 6| Step: 8
Training loss: 2.8996996879577637
Validation loss: 2.638571205959525

Epoch: 6| Step: 9
Training loss: 2.692798614501953
Validation loss: 2.638653165550642

Epoch: 6| Step: 10
Training loss: 2.6121771335601807
Validation loss: 2.641494463848811

Epoch: 6| Step: 11
Training loss: 2.4314892292022705
Validation loss: 2.6398472580858456

Epoch: 6| Step: 12
Training loss: 3.0706286430358887
Validation loss: 2.642267493791478

Epoch: 6| Step: 13
Training loss: 3.6889736652374268
Validation loss: 2.6380398170922392

Epoch: 94| Step: 0
Training loss: 2.804607391357422
Validation loss: 2.6364691642022904

Epoch: 6| Step: 1
Training loss: 2.103867292404175
Validation loss: 2.633941927263814

Epoch: 6| Step: 2
Training loss: 2.6905722618103027
Validation loss: 2.6336129403883413

Epoch: 6| Step: 3
Training loss: 3.0846962928771973
Validation loss: 2.63218165469426

Epoch: 6| Step: 4
Training loss: 3.5023107528686523
Validation loss: 2.6364850972288396

Epoch: 6| Step: 5
Training loss: 2.760354995727539
Validation loss: 2.630618131288918

Epoch: 6| Step: 6
Training loss: 1.8575624227523804
Validation loss: 2.635233517616026

Epoch: 6| Step: 7
Training loss: 2.8312716484069824
Validation loss: 2.6386751077508412

Epoch: 6| Step: 8
Training loss: 2.0942609310150146
Validation loss: 2.6360178403956915

Epoch: 6| Step: 9
Training loss: 2.684663772583008
Validation loss: 2.6378134758241716

Epoch: 6| Step: 10
Training loss: 3.6097421646118164
Validation loss: 2.6396783885135444

Epoch: 6| Step: 11
Training loss: 3.4114022254943848
Validation loss: 2.639346791851905

Epoch: 6| Step: 12
Training loss: 2.968886613845825
Validation loss: 2.63353205239901

Epoch: 6| Step: 13
Training loss: 2.835850238800049
Validation loss: 2.6287024713331655

Epoch: 95| Step: 0
Training loss: 3.594512939453125
Validation loss: 2.633054594839773

Epoch: 6| Step: 1
Training loss: 2.533216714859009
Validation loss: 2.632731194137245

Epoch: 6| Step: 2
Training loss: 2.3880605697631836
Validation loss: 2.6281557980404107

Epoch: 6| Step: 3
Training loss: 3.2741096019744873
Validation loss: 2.6293526387983754

Epoch: 6| Step: 4
Training loss: 3.168929100036621
Validation loss: 2.6307537658240205

Epoch: 6| Step: 5
Training loss: 2.687978506088257
Validation loss: 2.632025836616434

Epoch: 6| Step: 6
Training loss: 2.633352756500244
Validation loss: 2.6368805131604596

Epoch: 6| Step: 7
Training loss: 2.81200909614563
Validation loss: 2.637775126323905

Epoch: 6| Step: 8
Training loss: 2.9135756492614746
Validation loss: 2.6403592965936147

Epoch: 6| Step: 9
Training loss: 2.7623298168182373
Validation loss: 2.6350613358200237

Epoch: 6| Step: 10
Training loss: 2.8038573265075684
Validation loss: 2.632964116270824

Epoch: 6| Step: 11
Training loss: 2.547858953475952
Validation loss: 2.633283625366867

Epoch: 6| Step: 12
Training loss: 2.742569923400879
Validation loss: 2.6349618947634132

Epoch: 6| Step: 13
Training loss: 1.8372159004211426
Validation loss: 2.630063631201303

Epoch: 96| Step: 0
Training loss: 2.7021870613098145
Validation loss: 2.6277411778767905

Epoch: 6| Step: 1
Training loss: 2.7852046489715576
Validation loss: 2.6288947392535467

Epoch: 6| Step: 2
Training loss: 2.585230588912964
Validation loss: 2.629702252726401

Epoch: 6| Step: 3
Training loss: 2.34641170501709
Validation loss: 2.628895354527299

Epoch: 6| Step: 4
Training loss: 3.1132545471191406
Validation loss: 2.633387501521777

Epoch: 6| Step: 5
Training loss: 2.530564308166504
Validation loss: 2.632149129785517

Epoch: 6| Step: 6
Training loss: 3.2199010848999023
Validation loss: 2.6325063090170584

Epoch: 6| Step: 7
Training loss: 3.149308681488037
Validation loss: 2.635812287689537

Epoch: 6| Step: 8
Training loss: 2.2614619731903076
Validation loss: 2.632568356811359

Epoch: 6| Step: 9
Training loss: 2.678671360015869
Validation loss: 2.6327752733743317

Epoch: 6| Step: 10
Training loss: 3.008575439453125
Validation loss: 2.6282648348039195

Epoch: 6| Step: 11
Training loss: 1.9660522937774658
Validation loss: 2.6363419922449256

Epoch: 6| Step: 12
Training loss: 3.9629640579223633
Validation loss: 2.630551602250786

Epoch: 6| Step: 13
Training loss: 2.8065013885498047
Validation loss: 2.6287271720106884

Epoch: 97| Step: 0
Training loss: 3.0847606658935547
Validation loss: 2.632610228753859

Epoch: 6| Step: 1
Training loss: 2.2802939414978027
Validation loss: 2.6299065646304878

Epoch: 6| Step: 2
Training loss: 3.2591090202331543
Validation loss: 2.6322987541075675

Epoch: 6| Step: 3
Training loss: 3.1867308616638184
Validation loss: 2.628654528689641

Epoch: 6| Step: 4
Training loss: 3.0717978477478027
Validation loss: 2.6262035241691013

Epoch: 6| Step: 5
Training loss: 2.6057167053222656
Validation loss: 2.623400172879619

Epoch: 6| Step: 6
Training loss: 1.7848095893859863
Validation loss: 2.6278379681289836

Epoch: 6| Step: 7
Training loss: 3.1973307132720947
Validation loss: 2.624177409756568

Epoch: 6| Step: 8
Training loss: 2.6235690116882324
Validation loss: 2.6229510153493574

Epoch: 6| Step: 9
Training loss: 2.614994525909424
Validation loss: 2.625122844531972

Epoch: 6| Step: 10
Training loss: 2.338162422180176
Validation loss: 2.6252431074778237

Epoch: 6| Step: 11
Training loss: 3.4538567066192627
Validation loss: 2.6228689788490214

Epoch: 6| Step: 12
Training loss: 2.641040802001953
Validation loss: 2.6226204133802846

Epoch: 6| Step: 13
Training loss: 3.0521817207336426
Validation loss: 2.6213763734345794

Epoch: 98| Step: 0
Training loss: 2.666440010070801
Validation loss: 2.62154709651906

Epoch: 6| Step: 1
Training loss: 2.348076105117798
Validation loss: 2.623276197782127

Epoch: 6| Step: 2
Training loss: 2.8183236122131348
Validation loss: 2.6242293004066712

Epoch: 6| Step: 3
Training loss: 3.477283239364624
Validation loss: 2.6202118037849345

Epoch: 6| Step: 4
Training loss: 2.573761463165283
Validation loss: 2.621519386127431

Epoch: 6| Step: 5
Training loss: 3.2677488327026367
Validation loss: 2.6190626800701184

Epoch: 6| Step: 6
Training loss: 2.173246383666992
Validation loss: 2.6185144814111854

Epoch: 6| Step: 7
Training loss: 3.0776548385620117
Validation loss: 2.6191706811228106

Epoch: 6| Step: 8
Training loss: 3.025531768798828
Validation loss: 2.6207466817671254

Epoch: 6| Step: 9
Training loss: 2.3543262481689453
Validation loss: 2.6196567320054576

Epoch: 6| Step: 10
Training loss: 3.1825311183929443
Validation loss: 2.624484077576668

Epoch: 6| Step: 11
Training loss: 2.8992040157318115
Validation loss: 2.6222072057826544

Epoch: 6| Step: 12
Training loss: 2.6055703163146973
Validation loss: 2.6212339119244645

Epoch: 6| Step: 13
Training loss: 2.330199956893921
Validation loss: 2.6239816270848757

Epoch: 99| Step: 0
Training loss: 2.585022449493408
Validation loss: 2.621094375528315

Epoch: 6| Step: 1
Training loss: 3.0496153831481934
Validation loss: 2.6199540092099096

Epoch: 6| Step: 2
Training loss: 2.267606735229492
Validation loss: 2.6192879343545563

Epoch: 6| Step: 3
Training loss: 3.2095890045166016
Validation loss: 2.61886255715483

Epoch: 6| Step: 4
Training loss: 2.7343993186950684
Validation loss: 2.619588336636943

Epoch: 6| Step: 5
Training loss: 2.81105899810791
Validation loss: 2.623535886887581

Epoch: 6| Step: 6
Training loss: 2.5940778255462646
Validation loss: 2.6209928271591023

Epoch: 6| Step: 7
Training loss: 3.1730031967163086
Validation loss: 2.620916933141729

Epoch: 6| Step: 8
Training loss: 2.0805251598358154
Validation loss: 2.61869917890077

Epoch: 6| Step: 9
Training loss: 3.085552215576172
Validation loss: 2.6224216979037047

Epoch: 6| Step: 10
Training loss: 3.7316017150878906
Validation loss: 2.6200760410678003

Epoch: 6| Step: 11
Training loss: 2.305023670196533
Validation loss: 2.6181804416000203

Epoch: 6| Step: 12
Training loss: 2.661423683166504
Validation loss: 2.6187728425507903

Epoch: 6| Step: 13
Training loss: 2.6919713020324707
Validation loss: 2.617770997426843

Epoch: 100| Step: 0
Training loss: 2.985189199447632
Validation loss: 2.618355002454532

Epoch: 6| Step: 1
Training loss: 2.386439800262451
Validation loss: 2.6164150340582735

Epoch: 6| Step: 2
Training loss: 2.1979176998138428
Validation loss: 2.618040430930353

Epoch: 6| Step: 3
Training loss: 3.1257784366607666
Validation loss: 2.6149487777422835

Epoch: 6| Step: 4
Training loss: 3.7145137786865234
Validation loss: 2.6196671019318285

Epoch: 6| Step: 5
Training loss: 3.364241123199463
Validation loss: 2.6201852265224663

Epoch: 6| Step: 6
Training loss: 1.8848806619644165
Validation loss: 2.6144452966669554

Epoch: 6| Step: 7
Training loss: 2.469296455383301
Validation loss: 2.6175927064752065

Epoch: 6| Step: 8
Training loss: 2.7672982215881348
Validation loss: 2.614065247197305

Epoch: 6| Step: 9
Training loss: 2.786229133605957
Validation loss: 2.6137706105427077

Epoch: 6| Step: 10
Training loss: 3.003310441970825
Validation loss: 2.615639627620738

Epoch: 6| Step: 11
Training loss: 2.6414570808410645
Validation loss: 2.617037175804056

Epoch: 6| Step: 12
Training loss: 2.7474091053009033
Validation loss: 2.6160598365209435

Epoch: 6| Step: 13
Training loss: 3.015697479248047
Validation loss: 2.613828759039602

Epoch: 101| Step: 0
Training loss: 2.763693332672119
Validation loss: 2.612499067860265

Epoch: 6| Step: 1
Training loss: 3.249436140060425
Validation loss: 2.611805644086612

Epoch: 6| Step: 2
Training loss: 2.754335880279541
Validation loss: 2.614806741796514

Epoch: 6| Step: 3
Training loss: 2.427757501602173
Validation loss: 2.6140100161234536

Epoch: 6| Step: 4
Training loss: 3.2709884643554688
Validation loss: 2.6130752935204455

Epoch: 6| Step: 5
Training loss: 2.6535849571228027
Validation loss: 2.6111800362986903

Epoch: 6| Step: 6
Training loss: 2.5914154052734375
Validation loss: 2.614074804449594

Epoch: 6| Step: 7
Training loss: 2.863159418106079
Validation loss: 2.6165621306306575

Epoch: 6| Step: 8
Training loss: 3.577333450317383
Validation loss: 2.6160861420375046

Epoch: 6| Step: 9
Training loss: 2.5766944885253906
Validation loss: 2.6157980144664807

Epoch: 6| Step: 10
Training loss: 2.4250454902648926
Validation loss: 2.622202234883462

Epoch: 6| Step: 11
Training loss: 2.501701831817627
Validation loss: 2.614545137651505

Epoch: 6| Step: 12
Training loss: 2.8805325031280518
Validation loss: 2.614737520935715

Epoch: 6| Step: 13
Training loss: 2.1645262241363525
Validation loss: 2.6156341670661845

Epoch: 102| Step: 0
Training loss: 2.668966293334961
Validation loss: 2.614907762055756

Epoch: 6| Step: 1
Training loss: 2.838897705078125
Validation loss: 2.6156660895193777

Epoch: 6| Step: 2
Training loss: 2.30755615234375
Validation loss: 2.613535342677947

Epoch: 6| Step: 3
Training loss: 2.6667556762695312
Validation loss: 2.61826551857815

Epoch: 6| Step: 4
Training loss: 2.9504289627075195
Validation loss: 2.6180988537367953

Epoch: 6| Step: 5
Training loss: 3.161529541015625
Validation loss: 2.6163345562514437

Epoch: 6| Step: 6
Training loss: 3.0235376358032227
Validation loss: 2.6180499061461417

Epoch: 6| Step: 7
Training loss: 3.0865824222564697
Validation loss: 2.617959245558708

Epoch: 6| Step: 8
Training loss: 2.643148183822632
Validation loss: 2.6181755604282504

Epoch: 6| Step: 9
Training loss: 2.5805552005767822
Validation loss: 2.6174862000250045

Epoch: 6| Step: 10
Training loss: 2.6678576469421387
Validation loss: 2.612596329822335

Epoch: 6| Step: 11
Training loss: 3.1198320388793945
Validation loss: 2.6111641596722346

Epoch: 6| Step: 12
Training loss: 2.451115131378174
Validation loss: 2.612476066876483

Epoch: 6| Step: 13
Training loss: 2.6651949882507324
Validation loss: 2.6093779148594027

Epoch: 103| Step: 0
Training loss: 3.922577142715454
Validation loss: 2.6109368365298034

Epoch: 6| Step: 1
Training loss: 2.6670868396759033
Validation loss: 2.6121276629868375

Epoch: 6| Step: 2
Training loss: 3.3093061447143555
Validation loss: 2.611258396538355

Epoch: 6| Step: 3
Training loss: 3.134115695953369
Validation loss: 2.6140223113439416

Epoch: 6| Step: 4
Training loss: 2.6696219444274902
Validation loss: 2.6152896958012737

Epoch: 6| Step: 5
Training loss: 1.788414478302002
Validation loss: 2.6143924036333637

Epoch: 6| Step: 6
Training loss: 2.887582778930664
Validation loss: 2.611422454157183

Epoch: 6| Step: 7
Training loss: 2.98007869720459
Validation loss: 2.610450736937984

Epoch: 6| Step: 8
Training loss: 2.0723438262939453
Validation loss: 2.61010076666391

Epoch: 6| Step: 9
Training loss: 2.525827407836914
Validation loss: 2.6063115058406705

Epoch: 6| Step: 10
Training loss: 2.6528520584106445
Validation loss: 2.6077387845644386

Epoch: 6| Step: 11
Training loss: 2.919271945953369
Validation loss: 2.6117851605979343

Epoch: 6| Step: 12
Training loss: 2.4948298931121826
Validation loss: 2.606312572315175

Epoch: 6| Step: 13
Training loss: 2.9954330921173096
Validation loss: 2.6065192581504903

Epoch: 104| Step: 0
Training loss: 2.5949952602386475
Validation loss: 2.606914738173126

Epoch: 6| Step: 1
Training loss: 3.2080869674682617
Validation loss: 2.6112113870600218

Epoch: 6| Step: 2
Training loss: 2.42057466506958
Validation loss: 2.614440256549466

Epoch: 6| Step: 3
Training loss: 3.01792049407959
Validation loss: 2.6155369422769033

Epoch: 6| Step: 4
Training loss: 2.943251132965088
Validation loss: 2.6237087839393207

Epoch: 6| Step: 5
Training loss: 3.3539137840270996
Validation loss: 2.617647837567073

Epoch: 6| Step: 6
Training loss: 2.7320427894592285
Validation loss: 2.621626651415261

Epoch: 6| Step: 7
Training loss: 2.4708504676818848
Validation loss: 2.6263302654348393

Epoch: 6| Step: 8
Training loss: 3.184338092803955
Validation loss: 2.6192296012755363

Epoch: 6| Step: 9
Training loss: 2.843946933746338
Validation loss: 2.619878407447569

Epoch: 6| Step: 10
Training loss: 2.0327069759368896
Validation loss: 2.6210588050144974

Epoch: 6| Step: 11
Training loss: 2.7274887561798096
Validation loss: 2.6159678684767855

Epoch: 6| Step: 12
Training loss: 3.038365125656128
Validation loss: 2.611372140146071

Epoch: 6| Step: 13
Training loss: 1.9989378452301025
Validation loss: 2.6062036432245725

Epoch: 105| Step: 0
Training loss: 2.7984533309936523
Validation loss: 2.605327452382734

Epoch: 6| Step: 1
Training loss: 3.3770718574523926
Validation loss: 2.6051767436406945

Epoch: 6| Step: 2
Training loss: 2.5100064277648926
Validation loss: 2.6040611625999532

Epoch: 6| Step: 3
Training loss: 2.26760196685791
Validation loss: 2.607631655149562

Epoch: 6| Step: 4
Training loss: 3.70306396484375
Validation loss: 2.6449002091602614

Epoch: 6| Step: 5
Training loss: 2.930704355239868
Validation loss: 2.6133778787428334

Epoch: 6| Step: 6
Training loss: 1.900609016418457
Validation loss: 2.6112821871234524

Epoch: 6| Step: 7
Training loss: 2.3966758251190186
Validation loss: 2.605175687420753

Epoch: 6| Step: 8
Training loss: 2.997978687286377
Validation loss: 2.6100635669564687

Epoch: 6| Step: 9
Training loss: 3.4436872005462646
Validation loss: 2.609860192063034

Epoch: 6| Step: 10
Training loss: 2.7482597827911377
Validation loss: 2.611799560567384

Epoch: 6| Step: 11
Training loss: 2.310392141342163
Validation loss: 2.618320334342218

Epoch: 6| Step: 12
Training loss: 2.7708420753479004
Validation loss: 2.6149002044431624

Epoch: 6| Step: 13
Training loss: 2.670278787612915
Validation loss: 2.6224356902542936

Epoch: 106| Step: 0
Training loss: 2.9995815753936768
Validation loss: 2.6201208227424213

Epoch: 6| Step: 1
Training loss: 2.9240927696228027
Validation loss: 2.6205667834128104

Epoch: 6| Step: 2
Training loss: 2.7970707416534424
Validation loss: 2.6165975755260837

Epoch: 6| Step: 3
Training loss: 2.8459441661834717
Validation loss: 2.6191205183664956

Epoch: 6| Step: 4
Training loss: 2.405179977416992
Validation loss: 2.6111083107609905

Epoch: 6| Step: 5
Training loss: 2.1805224418640137
Validation loss: 2.6102090433079708

Epoch: 6| Step: 6
Training loss: 3.315337896347046
Validation loss: 2.608663543578117

Epoch: 6| Step: 7
Training loss: 2.880903959274292
Validation loss: 2.605762425289359

Epoch: 6| Step: 8
Training loss: 3.1311254501342773
Validation loss: 2.6062064863020376

Epoch: 6| Step: 9
Training loss: 2.9342427253723145
Validation loss: 2.6032712767201085

Epoch: 6| Step: 10
Training loss: 2.224013090133667
Validation loss: 2.6063682315170125

Epoch: 6| Step: 11
Training loss: 2.664041757583618
Validation loss: 2.599647488645328

Epoch: 6| Step: 12
Training loss: 2.20460844039917
Validation loss: 2.603683066624467

Epoch: 6| Step: 13
Training loss: 3.6745998859405518
Validation loss: 2.601073206111949

Epoch: 107| Step: 0
Training loss: 2.7082719802856445
Validation loss: 2.6003857299845707

Epoch: 6| Step: 1
Training loss: 2.45599627494812
Validation loss: 2.6002448733134935

Epoch: 6| Step: 2
Training loss: 2.9512133598327637
Validation loss: 2.601041358004334

Epoch: 6| Step: 3
Training loss: 2.4941089153289795
Validation loss: 2.601450558631651

Epoch: 6| Step: 4
Training loss: 3.137193202972412
Validation loss: 2.5986463690316803

Epoch: 6| Step: 5
Training loss: 3.316650867462158
Validation loss: 2.598336186460269

Epoch: 6| Step: 6
Training loss: 2.5392937660217285
Validation loss: 2.599140654328049

Epoch: 6| Step: 7
Training loss: 1.9510772228240967
Validation loss: 2.6008899852793705

Epoch: 6| Step: 8
Training loss: 3.526465892791748
Validation loss: 2.6013238917114916

Epoch: 6| Step: 9
Training loss: 2.513827085494995
Validation loss: 2.5969210106839418

Epoch: 6| Step: 10
Training loss: 2.9628870487213135
Validation loss: 2.5978590391015493

Epoch: 6| Step: 11
Training loss: 3.1655125617980957
Validation loss: 2.5967780082456526

Epoch: 6| Step: 12
Training loss: 2.2501282691955566
Validation loss: 2.598997659580682

Epoch: 6| Step: 13
Training loss: 2.8849971294403076
Validation loss: 2.5995509496299167

Epoch: 108| Step: 0
Training loss: 2.161506175994873
Validation loss: 2.6028780527012323

Epoch: 6| Step: 1
Training loss: 2.5953316688537598
Validation loss: 2.6033213343671573

Epoch: 6| Step: 2
Training loss: 2.2135837078094482
Validation loss: 2.6059211710447907

Epoch: 6| Step: 3
Training loss: 3.422377586364746
Validation loss: 2.604807412752541

Epoch: 6| Step: 4
Training loss: 2.375229835510254
Validation loss: 2.6075389897951515

Epoch: 6| Step: 5
Training loss: 2.383713722229004
Validation loss: 2.606252552360617

Epoch: 6| Step: 6
Training loss: 2.763424873352051
Validation loss: 2.6076698457041094

Epoch: 6| Step: 7
Training loss: 3.361906051635742
Validation loss: 2.6051975424571703

Epoch: 6| Step: 8
Training loss: 2.3643300533294678
Validation loss: 2.602062897015643

Epoch: 6| Step: 9
Training loss: 3.6071956157684326
Validation loss: 2.600605772387597

Epoch: 6| Step: 10
Training loss: 3.063570261001587
Validation loss: 2.597597829757198

Epoch: 6| Step: 11
Training loss: 2.882695198059082
Validation loss: 2.59776041071902

Epoch: 6| Step: 12
Training loss: 2.802088975906372
Validation loss: 2.5973509793640464

Epoch: 6| Step: 13
Training loss: 2.8180840015411377
Validation loss: 2.5967979482425156

Epoch: 109| Step: 0
Training loss: 2.831049919128418
Validation loss: 2.5930313243660876

Epoch: 6| Step: 1
Training loss: 2.7009541988372803
Validation loss: 2.594924734484765

Epoch: 6| Step: 2
Training loss: 2.487504005432129
Validation loss: 2.5948087502551336

Epoch: 6| Step: 3
Training loss: 3.470371961593628
Validation loss: 2.5957870791035313

Epoch: 6| Step: 4
Training loss: 2.8093454837799072
Validation loss: 2.5937449957734797

Epoch: 6| Step: 5
Training loss: 3.0439682006835938
Validation loss: 2.5931528896413822

Epoch: 6| Step: 6
Training loss: 3.560809850692749
Validation loss: 2.593678343680597

Epoch: 6| Step: 7
Training loss: 1.9998979568481445
Validation loss: 2.594479409597253

Epoch: 6| Step: 8
Training loss: 2.7648532390594482
Validation loss: 2.593464089978126

Epoch: 6| Step: 9
Training loss: 2.442701578140259
Validation loss: 2.5961040783953924

Epoch: 6| Step: 10
Training loss: 2.7029783725738525
Validation loss: 2.5932581040167038

Epoch: 6| Step: 11
Training loss: 2.611955165863037
Validation loss: 2.5929519104701217

Epoch: 6| Step: 12
Training loss: 2.867673397064209
Validation loss: 2.589839930175453

Epoch: 6| Step: 13
Training loss: 2.0863425731658936
Validation loss: 2.593066938461796

Epoch: 110| Step: 0
Training loss: 3.142925500869751
Validation loss: 2.5935692838443223

Epoch: 6| Step: 1
Training loss: 2.986013412475586
Validation loss: 2.59128249332469

Epoch: 6| Step: 2
Training loss: 3.0054430961608887
Validation loss: 2.5943729441653014

Epoch: 6| Step: 3
Training loss: 2.721174716949463
Validation loss: 2.5915531881393923

Epoch: 6| Step: 4
Training loss: 3.1182212829589844
Validation loss: 2.595318773741363

Epoch: 6| Step: 5
Training loss: 2.3310680389404297
Validation loss: 2.591346322849233

Epoch: 6| Step: 6
Training loss: 1.979183554649353
Validation loss: 2.600181177098264

Epoch: 6| Step: 7
Training loss: 3.285080909729004
Validation loss: 2.6031395696824595

Epoch: 6| Step: 8
Training loss: 2.4025490283966064
Validation loss: 2.6011639897541334

Epoch: 6| Step: 9
Training loss: 2.9650962352752686
Validation loss: 2.5961123717728483

Epoch: 6| Step: 10
Training loss: 2.8755574226379395
Validation loss: 2.59600749323445

Epoch: 6| Step: 11
Training loss: 2.655977964401245
Validation loss: 2.5939127219620572

Epoch: 6| Step: 12
Training loss: 2.838475465774536
Validation loss: 2.5946643326872136

Epoch: 6| Step: 13
Training loss: 2.0877797603607178
Validation loss: 2.5961164197614117

Epoch: 111| Step: 0
Training loss: 3.006545066833496
Validation loss: 2.6022832291100615

Epoch: 6| Step: 1
Training loss: 2.970266819000244
Validation loss: 2.606070413384386

Epoch: 6| Step: 2
Training loss: 2.5083298683166504
Validation loss: 2.606570905254733

Epoch: 6| Step: 3
Training loss: 3.147653579711914
Validation loss: 2.608342819316413

Epoch: 6| Step: 4
Training loss: 2.874112367630005
Validation loss: 2.6027160588131157

Epoch: 6| Step: 5
Training loss: 2.5421056747436523
Validation loss: 2.594238519668579

Epoch: 6| Step: 6
Training loss: 2.400397300720215
Validation loss: 2.5965114896015455

Epoch: 6| Step: 7
Training loss: 1.7458090782165527
Validation loss: 2.5955860512230986

Epoch: 6| Step: 8
Training loss: 2.493412494659424
Validation loss: 2.589759608750702

Epoch: 6| Step: 9
Training loss: 3.666933536529541
Validation loss: 2.5916740561044342

Epoch: 6| Step: 10
Training loss: 3.051422595977783
Validation loss: 2.5943000880620812

Epoch: 6| Step: 11
Training loss: 2.929410934448242
Validation loss: 2.5935537815093994

Epoch: 6| Step: 12
Training loss: 2.2739672660827637
Validation loss: 2.5932684098520586

Epoch: 6| Step: 13
Training loss: 3.298027753829956
Validation loss: 2.5907682885405836

Epoch: 112| Step: 0
Training loss: 3.6299853324890137
Validation loss: 2.5886435457455215

Epoch: 6| Step: 1
Training loss: 2.0727641582489014
Validation loss: 2.588812412754182

Epoch: 6| Step: 2
Training loss: 2.074130058288574
Validation loss: 2.5896923260022233

Epoch: 6| Step: 3
Training loss: 3.0351176261901855
Validation loss: 2.588986809535693

Epoch: 6| Step: 4
Training loss: 3.024658679962158
Validation loss: 2.589527004508562

Epoch: 6| Step: 5
Training loss: 3.360224962234497
Validation loss: 2.5877641913711384

Epoch: 6| Step: 6
Training loss: 3.043200969696045
Validation loss: 2.5878968546467442

Epoch: 6| Step: 7
Training loss: 2.7769947052001953
Validation loss: 2.586275267344649

Epoch: 6| Step: 8
Training loss: 3.3280935287475586
Validation loss: 2.5899343490600586

Epoch: 6| Step: 9
Training loss: 2.249471664428711
Validation loss: 2.587647981541131

Epoch: 6| Step: 10
Training loss: 2.30902361869812
Validation loss: 2.589285191669259

Epoch: 6| Step: 11
Training loss: 2.1490225791931152
Validation loss: 2.5836436338322137

Epoch: 6| Step: 12
Training loss: 2.4521918296813965
Validation loss: 2.5872232708879697

Epoch: 6| Step: 13
Training loss: 3.4905574321746826
Validation loss: 2.5898755647802867

Epoch: 113| Step: 0
Training loss: 1.9675946235656738
Validation loss: 2.5895612470565306

Epoch: 6| Step: 1
Training loss: 3.4181454181671143
Validation loss: 2.5924744477836033

Epoch: 6| Step: 2
Training loss: 2.935037136077881
Validation loss: 2.599402855801326

Epoch: 6| Step: 3
Training loss: 2.049067735671997
Validation loss: 2.5980874723003757

Epoch: 6| Step: 4
Training loss: 2.5666277408599854
Validation loss: 2.598583177853656

Epoch: 6| Step: 5
Training loss: 2.030863046646118
Validation loss: 2.5944518145694526

Epoch: 6| Step: 6
Training loss: 2.814526081085205
Validation loss: 2.59230298637062

Epoch: 6| Step: 7
Training loss: 3.2893459796905518
Validation loss: 2.587334745673723

Epoch: 6| Step: 8
Training loss: 1.7817702293395996
Validation loss: 2.5872917098383748

Epoch: 6| Step: 9
Training loss: 4.100921154022217
Validation loss: 2.588385030787478

Epoch: 6| Step: 10
Training loss: 2.267669677734375
Validation loss: 2.5841742100254184

Epoch: 6| Step: 11
Training loss: 3.1175105571746826
Validation loss: 2.585625922808083

Epoch: 6| Step: 12
Training loss: 3.127620220184326
Validation loss: 2.583140355284496

Epoch: 6| Step: 13
Training loss: 3.5488431453704834
Validation loss: 2.5832613052860385

Epoch: 114| Step: 0
Training loss: 2.2265191078186035
Validation loss: 2.5815883733892955

Epoch: 6| Step: 1
Training loss: 2.9202065467834473
Validation loss: 2.5811978104293987

Epoch: 6| Step: 2
Training loss: 3.4309802055358887
Validation loss: 2.581969153496527

Epoch: 6| Step: 3
Training loss: 3.055896043777466
Validation loss: 2.5827674814449844

Epoch: 6| Step: 4
Training loss: 3.6360552310943604
Validation loss: 2.58149367250422

Epoch: 6| Step: 5
Training loss: 2.059739112854004
Validation loss: 2.5831372558429675

Epoch: 6| Step: 6
Training loss: 2.97782564163208
Validation loss: 2.581580259466684

Epoch: 6| Step: 7
Training loss: 2.5461337566375732
Validation loss: 2.5831731545027865

Epoch: 6| Step: 8
Training loss: 2.74364972114563
Validation loss: 2.5808281398588613

Epoch: 6| Step: 9
Training loss: 3.0633387565612793
Validation loss: 2.582265656481507

Epoch: 6| Step: 10
Training loss: 2.4235427379608154
Validation loss: 2.5823803870908675

Epoch: 6| Step: 11
Training loss: 2.3817803859710693
Validation loss: 2.58053175351953

Epoch: 6| Step: 12
Training loss: 2.3220913410186768
Validation loss: 2.5843592254064416

Epoch: 6| Step: 13
Training loss: 2.8151819705963135
Validation loss: 2.5814029324439263

Epoch: 115| Step: 0
Training loss: 3.3299143314361572
Validation loss: 2.5843837568836827

Epoch: 6| Step: 1
Training loss: 3.062389612197876
Validation loss: 2.5828465774495113

Epoch: 6| Step: 2
Training loss: 2.5163016319274902
Validation loss: 2.582977097521546

Epoch: 6| Step: 3
Training loss: 3.2821760177612305
Validation loss: 2.582792620505056

Epoch: 6| Step: 4
Training loss: 2.5706405639648438
Validation loss: 2.5820868220380557

Epoch: 6| Step: 5
Training loss: 2.7056736946105957
Validation loss: 2.5805696723281697

Epoch: 6| Step: 6
Training loss: 2.7560200691223145
Validation loss: 2.5807388828646753

Epoch: 6| Step: 7
Training loss: 2.7731947898864746
Validation loss: 2.580931240512479

Epoch: 6| Step: 8
Training loss: 2.7445826530456543
Validation loss: 2.581230858320831

Epoch: 6| Step: 9
Training loss: 3.3739442825317383
Validation loss: 2.579868842196721

Epoch: 6| Step: 10
Training loss: 1.6891124248504639
Validation loss: 2.5781610088963665

Epoch: 6| Step: 11
Training loss: 2.5499353408813477
Validation loss: 2.57888618592293

Epoch: 6| Step: 12
Training loss: 2.300290107727051
Validation loss: 2.5789539865268174

Epoch: 6| Step: 13
Training loss: 2.975623607635498
Validation loss: 2.582136061883742

Epoch: 116| Step: 0
Training loss: 2.9760961532592773
Validation loss: 2.577449060255481

Epoch: 6| Step: 1
Training loss: 3.0148658752441406
Validation loss: 2.5763317308118268

Epoch: 6| Step: 2
Training loss: 2.25386643409729
Validation loss: 2.579286398426179

Epoch: 6| Step: 3
Training loss: 2.248955726623535
Validation loss: 2.577390701540055

Epoch: 6| Step: 4
Training loss: 2.909026622772217
Validation loss: 2.574926842925369

Epoch: 6| Step: 5
Training loss: 3.0765466690063477
Validation loss: 2.5761910561592347

Epoch: 6| Step: 6
Training loss: 3.281641721725464
Validation loss: 2.574507180080619

Epoch: 6| Step: 7
Training loss: 1.9788028001785278
Validation loss: 2.5750900160881782

Epoch: 6| Step: 8
Training loss: 1.9157131910324097
Validation loss: 2.5758194051763064

Epoch: 6| Step: 9
Training loss: 3.2729263305664062
Validation loss: 2.5749904981223484

Epoch: 6| Step: 10
Training loss: 3.3287761211395264
Validation loss: 2.578449151849234

Epoch: 6| Step: 11
Training loss: 2.5450916290283203
Validation loss: 2.5765387447931434

Epoch: 6| Step: 12
Training loss: 3.0711679458618164
Validation loss: 2.5766046124119915

Epoch: 6| Step: 13
Training loss: 2.549227476119995
Validation loss: 2.5773963928222656

Epoch: 117| Step: 0
Training loss: 2.3546817302703857
Validation loss: 2.574749885066863

Epoch: 6| Step: 1
Training loss: 2.7102808952331543
Validation loss: 2.5755600083258843

Epoch: 6| Step: 2
Training loss: 2.4288203716278076
Validation loss: 2.5767711823986423

Epoch: 6| Step: 3
Training loss: 3.2308578491210938
Validation loss: 2.5753922052280878

Epoch: 6| Step: 4
Training loss: 2.106949806213379
Validation loss: 2.57694209647435

Epoch: 6| Step: 5
Training loss: 2.779533863067627
Validation loss: 2.5744483188916276

Epoch: 6| Step: 6
Training loss: 2.782515287399292
Validation loss: 2.580555180067657

Epoch: 6| Step: 7
Training loss: 2.748849391937256
Validation loss: 2.579539934794108

Epoch: 6| Step: 8
Training loss: 2.1071224212646484
Validation loss: 2.5807770785465034

Epoch: 6| Step: 9
Training loss: 2.4374492168426514
Validation loss: 2.585785306910033

Epoch: 6| Step: 10
Training loss: 3.901337146759033
Validation loss: 2.580874522527059

Epoch: 6| Step: 11
Training loss: 2.9885993003845215
Validation loss: 2.586027609404697

Epoch: 6| Step: 12
Training loss: 2.882843017578125
Validation loss: 2.5843120800551547

Epoch: 6| Step: 13
Training loss: 3.2240819931030273
Validation loss: 2.5893159579205256

Epoch: 118| Step: 0
Training loss: 3.575474739074707
Validation loss: 2.5851726429436797

Epoch: 6| Step: 1
Training loss: 3.023940324783325
Validation loss: 2.5852804722324496

Epoch: 6| Step: 2
Training loss: 2.7902817726135254
Validation loss: 2.5791253120668474

Epoch: 6| Step: 3
Training loss: 2.7402687072753906
Validation loss: 2.5764527833589943

Epoch: 6| Step: 4
Training loss: 2.4541659355163574
Validation loss: 2.5773795343214467

Epoch: 6| Step: 5
Training loss: 2.7632761001586914
Validation loss: 2.5766013617156656

Epoch: 6| Step: 6
Training loss: 2.675863742828369
Validation loss: 2.573517866032098

Epoch: 6| Step: 7
Training loss: 2.6898930072784424
Validation loss: 2.57381187459474

Epoch: 6| Step: 8
Training loss: 3.1836349964141846
Validation loss: 2.573720637188163

Epoch: 6| Step: 9
Training loss: 2.729090690612793
Validation loss: 2.571987244390672

Epoch: 6| Step: 10
Training loss: 2.321493625640869
Validation loss: 2.5716918104438373

Epoch: 6| Step: 11
Training loss: 1.9362711906433105
Validation loss: 2.572295988759687

Epoch: 6| Step: 12
Training loss: 3.1352438926696777
Validation loss: 2.569106945427515

Epoch: 6| Step: 13
Training loss: 2.13198184967041
Validation loss: 2.5699703539571455

Epoch: 119| Step: 0
Training loss: 3.314873456954956
Validation loss: 2.5690707468217417

Epoch: 6| Step: 1
Training loss: 3.0209743976593018
Validation loss: 2.5697330685072046

Epoch: 6| Step: 2
Training loss: 3.4182915687561035
Validation loss: 2.568497811594317

Epoch: 6| Step: 3
Training loss: 2.8959012031555176
Validation loss: 2.570870066201815

Epoch: 6| Step: 4
Training loss: 2.48256254196167
Validation loss: 2.5691085400119906

Epoch: 6| Step: 5
Training loss: 2.91005802154541
Validation loss: 2.5700275051978325

Epoch: 6| Step: 6
Training loss: 2.268617868423462
Validation loss: 2.57050984649248

Epoch: 6| Step: 7
Training loss: 2.6199398040771484
Validation loss: 2.569301497551703

Epoch: 6| Step: 8
Training loss: 2.8163247108459473
Validation loss: 2.5672919006757837

Epoch: 6| Step: 9
Training loss: 2.722452163696289
Validation loss: 2.5673514181567776

Epoch: 6| Step: 10
Training loss: 2.657350540161133
Validation loss: 2.570327220424529

Epoch: 6| Step: 11
Training loss: 2.0124824047088623
Validation loss: 2.5656979417288177

Epoch: 6| Step: 12
Training loss: 2.8254802227020264
Validation loss: 2.56834170638874

Epoch: 6| Step: 13
Training loss: 2.226036310195923
Validation loss: 2.5688382553797897

Epoch: 120| Step: 0
Training loss: 2.322650671005249
Validation loss: 2.5687433827307915

Epoch: 6| Step: 1
Training loss: 2.279261589050293
Validation loss: 2.5684400835344867

Epoch: 6| Step: 2
Training loss: 2.568129539489746
Validation loss: 2.569501922976586

Epoch: 6| Step: 3
Training loss: 2.7437686920166016
Validation loss: 2.5716440113641883

Epoch: 6| Step: 4
Training loss: 2.6595187187194824
Validation loss: 2.575591195014215

Epoch: 6| Step: 5
Training loss: 3.3652710914611816
Validation loss: 2.573633575952181

Epoch: 6| Step: 6
Training loss: 3.488101005554199
Validation loss: 2.5801059353736138

Epoch: 6| Step: 7
Training loss: 2.6671934127807617
Validation loss: 2.58023851917636

Epoch: 6| Step: 8
Training loss: 2.693674087524414
Validation loss: 2.586950194451117

Epoch: 6| Step: 9
Training loss: 2.296011209487915
Validation loss: 2.582758982976278

Epoch: 6| Step: 10
Training loss: 3.1281545162200928
Validation loss: 2.5811605940582933

Epoch: 6| Step: 11
Training loss: 3.416731357574463
Validation loss: 2.577434524413078

Epoch: 6| Step: 12
Training loss: 2.1292612552642822
Validation loss: 2.5711325650574057

Epoch: 6| Step: 13
Training loss: 2.738020658493042
Validation loss: 2.5686882157479562

Epoch: 121| Step: 0
Training loss: 3.110024929046631
Validation loss: 2.5669522311097834

Epoch: 6| Step: 1
Training loss: 2.1323049068450928
Validation loss: 2.5645391941070557

Epoch: 6| Step: 2
Training loss: 3.1831135749816895
Validation loss: 2.5674658462565434

Epoch: 6| Step: 3
Training loss: 3.3427205085754395
Validation loss: 2.5675868654763825

Epoch: 6| Step: 4
Training loss: 3.174644947052002
Validation loss: 2.5673449167641262

Epoch: 6| Step: 5
Training loss: 2.9493584632873535
Validation loss: 2.56761368115743

Epoch: 6| Step: 6
Training loss: 2.2880687713623047
Validation loss: 2.5643470159140964

Epoch: 6| Step: 7
Training loss: 2.2912516593933105
Validation loss: 2.5675441757325204

Epoch: 6| Step: 8
Training loss: 2.170783758163452
Validation loss: 2.568705653631559

Epoch: 6| Step: 9
Training loss: 3.226752519607544
Validation loss: 2.567307464538082

Epoch: 6| Step: 10
Training loss: 2.775836944580078
Validation loss: 2.5653058764755086

Epoch: 6| Step: 11
Training loss: 2.715818405151367
Validation loss: 2.56892974915043

Epoch: 6| Step: 12
Training loss: 2.2387912273406982
Validation loss: 2.5682661328264462

Epoch: 6| Step: 13
Training loss: 2.9771716594696045
Validation loss: 2.567422866821289

Epoch: 122| Step: 0
Training loss: 2.3720977306365967
Validation loss: 2.5711363848819526

Epoch: 6| Step: 1
Training loss: 2.94974422454834
Validation loss: 2.5690058098044446

Epoch: 6| Step: 2
Training loss: 2.5743281841278076
Validation loss: 2.5724012364623365

Epoch: 6| Step: 3
Training loss: 2.630547523498535
Validation loss: 2.570518560307

Epoch: 6| Step: 4
Training loss: 3.5710268020629883
Validation loss: 2.5712997631360124

Epoch: 6| Step: 5
Training loss: 2.296678304672241
Validation loss: 2.572599434083508

Epoch: 6| Step: 6
Training loss: 2.8190369606018066
Validation loss: 2.5720418358361847

Epoch: 6| Step: 7
Training loss: 2.3378379344940186
Validation loss: 2.5727159361685477

Epoch: 6| Step: 8
Training loss: 3.249542236328125
Validation loss: 2.570331706795641

Epoch: 6| Step: 9
Training loss: 2.699545383453369
Validation loss: 2.5701682080504713

Epoch: 6| Step: 10
Training loss: 2.641191005706787
Validation loss: 2.57002249328039

Epoch: 6| Step: 11
Training loss: 2.4690542221069336
Validation loss: 2.5675022499535674

Epoch: 6| Step: 12
Training loss: 3.1846671104431152
Validation loss: 2.568592853443597

Epoch: 6| Step: 13
Training loss: 2.4697065353393555
Validation loss: 2.568059123972411

Epoch: 123| Step: 0
Training loss: 2.882079601287842
Validation loss: 2.562614510136266

Epoch: 6| Step: 1
Training loss: 3.2522144317626953
Validation loss: 2.566756622765654

Epoch: 6| Step: 2
Training loss: 2.4891183376312256
Validation loss: 2.5644197643444104

Epoch: 6| Step: 3
Training loss: 2.621119737625122
Validation loss: 2.5680628207422074

Epoch: 6| Step: 4
Training loss: 2.818145513534546
Validation loss: 2.564158190963089

Epoch: 6| Step: 5
Training loss: 2.8902852535247803
Validation loss: 2.561512308736001

Epoch: 6| Step: 6
Training loss: 1.9286658763885498
Validation loss: 2.5647245504522838

Epoch: 6| Step: 7
Training loss: 2.629869222640991
Validation loss: 2.5644544016930366

Epoch: 6| Step: 8
Training loss: 2.267138719558716
Validation loss: 2.565475497194516

Epoch: 6| Step: 9
Training loss: 3.3879172801971436
Validation loss: 2.558632714774019

Epoch: 6| Step: 10
Training loss: 2.9692955017089844
Validation loss: 2.560702003458495

Epoch: 6| Step: 11
Training loss: 3.1766059398651123
Validation loss: 2.5599196495548373

Epoch: 6| Step: 12
Training loss: 2.333662748336792
Validation loss: 2.5599011298148864

Epoch: 6| Step: 13
Training loss: 2.7040317058563232
Validation loss: 2.560399627172819

Epoch: 124| Step: 0
Training loss: 1.8894370794296265
Validation loss: 2.5595315348717476

Epoch: 6| Step: 1
Training loss: 2.660874366760254
Validation loss: 2.558486084784231

Epoch: 6| Step: 2
Training loss: 2.5899853706359863
Validation loss: 2.559067782535348

Epoch: 6| Step: 3
Training loss: 3.701254367828369
Validation loss: 2.55881086216178

Epoch: 6| Step: 4
Training loss: 2.4454381465911865
Validation loss: 2.559306983024843

Epoch: 6| Step: 5
Training loss: 2.565763473510742
Validation loss: 2.555788045288414

Epoch: 6| Step: 6
Training loss: 2.828693389892578
Validation loss: 2.559083369470412

Epoch: 6| Step: 7
Training loss: 2.749436855316162
Validation loss: 2.5573241146661903

Epoch: 6| Step: 8
Training loss: 2.9581103324890137
Validation loss: 2.5564961228319394

Epoch: 6| Step: 9
Training loss: 2.2030396461486816
Validation loss: 2.5569913387298584

Epoch: 6| Step: 10
Training loss: 3.224484920501709
Validation loss: 2.559797397223852

Epoch: 6| Step: 11
Training loss: 3.036332130432129
Validation loss: 2.557204325993856

Epoch: 6| Step: 12
Training loss: 2.4735660552978516
Validation loss: 2.554931955952798

Epoch: 6| Step: 13
Training loss: 3.200606346130371
Validation loss: 2.5592823720747426

Epoch: 125| Step: 0
Training loss: 3.0961222648620605
Validation loss: 2.5596188396535893

Epoch: 6| Step: 1
Training loss: 2.5438179969787598
Validation loss: 2.563554033156364

Epoch: 6| Step: 2
Training loss: 3.201162338256836
Validation loss: 2.5607866933268886

Epoch: 6| Step: 3
Training loss: 2.272216796875
Validation loss: 2.5607274937373337

Epoch: 6| Step: 4
Training loss: 2.9785897731781006
Validation loss: 2.5616693112158004

Epoch: 6| Step: 5
Training loss: 2.2290761470794678
Validation loss: 2.561834348145352

Epoch: 6| Step: 6
Training loss: 2.4534201622009277
Validation loss: 2.5599675460528304

Epoch: 6| Step: 7
Training loss: 2.9139764308929443
Validation loss: 2.5642388225883566

Epoch: 6| Step: 8
Training loss: 2.9684090614318848
Validation loss: 2.5610457312676216

Epoch: 6| Step: 9
Training loss: 2.3089449405670166
Validation loss: 2.558820124595396

Epoch: 6| Step: 10
Training loss: 2.6648478507995605
Validation loss: 2.5626367163914505

Epoch: 6| Step: 11
Training loss: 3.666020393371582
Validation loss: 2.557305448798723

Epoch: 6| Step: 12
Training loss: 2.7157645225524902
Validation loss: 2.558784523317891

Epoch: 6| Step: 13
Training loss: 1.9081645011901855
Validation loss: 2.554134399660172

Epoch: 126| Step: 0
Training loss: 2.7530553340911865
Validation loss: 2.5557706266321163

Epoch: 6| Step: 1
Training loss: 3.143815517425537
Validation loss: 2.554403817781838

Epoch: 6| Step: 2
Training loss: 2.15655517578125
Validation loss: 2.5545612201895764

Epoch: 6| Step: 3
Training loss: 3.2522149085998535
Validation loss: 2.5540312387609996

Epoch: 6| Step: 4
Training loss: 2.265451669692993
Validation loss: 2.55285152312248

Epoch: 6| Step: 5
Training loss: 2.4073009490966797
Validation loss: 2.5513168765652563

Epoch: 6| Step: 6
Training loss: 3.264317274093628
Validation loss: 2.554753752164943

Epoch: 6| Step: 7
Training loss: 2.5537946224212646
Validation loss: 2.5530291013820197

Epoch: 6| Step: 8
Training loss: 2.913841724395752
Validation loss: 2.554982021290769

Epoch: 6| Step: 9
Training loss: 2.3439624309539795
Validation loss: 2.5522828948113228

Epoch: 6| Step: 10
Training loss: 2.9250948429107666
Validation loss: 2.551232817352459

Epoch: 6| Step: 11
Training loss: 2.7463197708129883
Validation loss: 2.549905151449224

Epoch: 6| Step: 12
Training loss: 2.623744010925293
Validation loss: 2.549189959802935

Epoch: 6| Step: 13
Training loss: 3.0175275802612305
Validation loss: 2.5500267603064097

Epoch: 127| Step: 0
Training loss: 2.8057236671447754
Validation loss: 2.5485357417855212

Epoch: 6| Step: 1
Training loss: 2.7736895084381104
Validation loss: 2.5493360591191117

Epoch: 6| Step: 2
Training loss: 3.434540033340454
Validation loss: 2.5487842200904764

Epoch: 6| Step: 3
Training loss: 3.6319732666015625
Validation loss: 2.551231309931765

Epoch: 6| Step: 4
Training loss: 3.5469446182250977
Validation loss: 2.550577304696524

Epoch: 6| Step: 5
Training loss: 2.963015556335449
Validation loss: 2.5466502046072357

Epoch: 6| Step: 6
Training loss: 1.4025927782058716
Validation loss: 2.5499957223092355

Epoch: 6| Step: 7
Training loss: 2.5491251945495605
Validation loss: 2.551225831431727

Epoch: 6| Step: 8
Training loss: 2.3076910972595215
Validation loss: 2.5500258450867026

Epoch: 6| Step: 9
Training loss: 2.8869576454162598
Validation loss: 2.546308655892649

Epoch: 6| Step: 10
Training loss: 2.5004079341888428
Validation loss: 2.5463622052182435

Epoch: 6| Step: 11
Training loss: 2.4607253074645996
Validation loss: 2.5464508969296693

Epoch: 6| Step: 12
Training loss: 2.2987310886383057
Validation loss: 2.5462662917311474

Epoch: 6| Step: 13
Training loss: 2.5899484157562256
Validation loss: 2.548816675780922

Epoch: 128| Step: 0
Training loss: 2.896789073944092
Validation loss: 2.550111183556177

Epoch: 6| Step: 1
Training loss: 2.832336187362671
Validation loss: 2.548156440898936

Epoch: 6| Step: 2
Training loss: 3.022536277770996
Validation loss: 2.5497605134082097

Epoch: 6| Step: 3
Training loss: 2.8208248615264893
Validation loss: 2.555055067103396

Epoch: 6| Step: 4
Training loss: 1.544944405555725
Validation loss: 2.5530636977123957

Epoch: 6| Step: 5
Training loss: 2.394394874572754
Validation loss: 2.556271522275863

Epoch: 6| Step: 6
Training loss: 2.7052321434020996
Validation loss: 2.5491681816757366

Epoch: 6| Step: 7
Training loss: 2.3759093284606934
Validation loss: 2.5470602768723682

Epoch: 6| Step: 8
Training loss: 3.4663467407226562
Validation loss: 2.5481528056565153

Epoch: 6| Step: 9
Training loss: 2.554365873336792
Validation loss: 2.5466888925080657

Epoch: 6| Step: 10
Training loss: 2.8197154998779297
Validation loss: 2.5463647637315976

Epoch: 6| Step: 11
Training loss: 2.9130144119262695
Validation loss: 2.54791175165484

Epoch: 6| Step: 12
Training loss: 2.613966941833496
Validation loss: 2.5473100908340944

Epoch: 6| Step: 13
Training loss: 3.6874656677246094
Validation loss: 2.5463722418713313

Epoch: 129| Step: 0
Training loss: 2.9968948364257812
Validation loss: 2.5467555164009013

Epoch: 6| Step: 1
Training loss: 2.2596049308776855
Validation loss: 2.5474182200688187

Epoch: 6| Step: 2
Training loss: 2.698622226715088
Validation loss: 2.5470571928126837

Epoch: 6| Step: 3
Training loss: 3.3837239742279053
Validation loss: 2.545962502879481

Epoch: 6| Step: 4
Training loss: 2.969235420227051
Validation loss: 2.549295162641874

Epoch: 6| Step: 5
Training loss: 2.493361473083496
Validation loss: 2.544282736316804

Epoch: 6| Step: 6
Training loss: 2.9698455333709717
Validation loss: 2.5422964326796995

Epoch: 6| Step: 7
Training loss: 2.9793972969055176
Validation loss: 2.543553667683755

Epoch: 6| Step: 8
Training loss: 3.5301706790924072
Validation loss: 2.5431289160123436

Epoch: 6| Step: 9
Training loss: 1.7440423965454102
Validation loss: 2.543317843508977

Epoch: 6| Step: 10
Training loss: 2.49550724029541
Validation loss: 2.5441287179147043

Epoch: 6| Step: 11
Training loss: 2.5126476287841797
Validation loss: 2.544954164053804

Epoch: 6| Step: 12
Training loss: 2.330733060836792
Validation loss: 2.5463368841396865

Epoch: 6| Step: 13
Training loss: 2.839482307434082
Validation loss: 2.5466324462685535

Epoch: 130| Step: 0
Training loss: 2.9545059204101562
Validation loss: 2.552486986242315

Epoch: 6| Step: 1
Training loss: 2.2782623767852783
Validation loss: 2.55224185861567

Epoch: 6| Step: 2
Training loss: 2.607466697692871
Validation loss: 2.5494706297433503

Epoch: 6| Step: 3
Training loss: 1.8576085567474365
Validation loss: 2.5554688002473567

Epoch: 6| Step: 4
Training loss: 2.655778646469116
Validation loss: 2.555682559167185

Epoch: 6| Step: 5
Training loss: 2.6792221069335938
Validation loss: 2.5487493827778804

Epoch: 6| Step: 6
Training loss: 2.3842978477478027
Validation loss: 2.5470781121202695

Epoch: 6| Step: 7
Training loss: 3.4991188049316406
Validation loss: 2.542751484019782

Epoch: 6| Step: 8
Training loss: 3.558675765991211
Validation loss: 2.541525063976165

Epoch: 6| Step: 9
Training loss: 2.4871981143951416
Validation loss: 2.5427127269006546

Epoch: 6| Step: 10
Training loss: 2.8776602745056152
Validation loss: 2.5438726845607964

Epoch: 6| Step: 11
Training loss: 2.776481866836548
Validation loss: 2.5451787287189114

Epoch: 6| Step: 12
Training loss: 2.649294376373291
Validation loss: 2.548166641625025

Epoch: 6| Step: 13
Training loss: 3.20066237449646
Validation loss: 2.5476511498933196

Epoch: 131| Step: 0
Training loss: 3.0950093269348145
Validation loss: 2.546683229425902

Epoch: 6| Step: 1
Training loss: 2.8684463500976562
Validation loss: 2.54272690767883

Epoch: 6| Step: 2
Training loss: 2.4370739459991455
Validation loss: 2.54094503002782

Epoch: 6| Step: 3
Training loss: 2.9371914863586426
Validation loss: 2.539911549578431

Epoch: 6| Step: 4
Training loss: 2.252913236618042
Validation loss: 2.541536029948983

Epoch: 6| Step: 5
Training loss: 3.158982276916504
Validation loss: 2.5421250520213956

Epoch: 6| Step: 6
Training loss: 2.4352169036865234
Validation loss: 2.541335880115468

Epoch: 6| Step: 7
Training loss: 2.4829797744750977
Validation loss: 2.543704694317233

Epoch: 6| Step: 8
Training loss: 3.175320625305176
Validation loss: 2.541995862478851

Epoch: 6| Step: 9
Training loss: 2.459839105606079
Validation loss: 2.5484618653533277

Epoch: 6| Step: 10
Training loss: 2.9709341526031494
Validation loss: 2.5444814928116335

Epoch: 6| Step: 11
Training loss: 3.4213149547576904
Validation loss: 2.55185450789749

Epoch: 6| Step: 12
Training loss: 2.494793653488159
Validation loss: 2.551526854115148

Epoch: 6| Step: 13
Training loss: 1.4002681970596313
Validation loss: 2.5453427325012865

Epoch: 132| Step: 0
Training loss: 1.8590142726898193
Validation loss: 2.543823257569344

Epoch: 6| Step: 1
Training loss: 3.361159324645996
Validation loss: 2.544083377366425

Epoch: 6| Step: 2
Training loss: 2.8447132110595703
Validation loss: 2.5397014720465547

Epoch: 6| Step: 3
Training loss: 2.4035706520080566
Validation loss: 2.5355301928776566

Epoch: 6| Step: 4
Training loss: 2.5332393646240234
Validation loss: 2.5370252952780774

Epoch: 6| Step: 5
Training loss: 3.016690254211426
Validation loss: 2.5369637768755675

Epoch: 6| Step: 6
Training loss: 3.1012837886810303
Validation loss: 2.5382032061135895

Epoch: 6| Step: 7
Training loss: 3.2283730506896973
Validation loss: 2.5338779675063265

Epoch: 6| Step: 8
Training loss: 2.0864522457122803
Validation loss: 2.5395557136945826

Epoch: 6| Step: 9
Training loss: 3.067770004272461
Validation loss: 2.5394523400132374

Epoch: 6| Step: 10
Training loss: 2.359389066696167
Validation loss: 2.5413363928435952

Epoch: 6| Step: 11
Training loss: 3.2266597747802734
Validation loss: 2.5411381798405803

Epoch: 6| Step: 12
Training loss: 2.295677661895752
Validation loss: 2.552079141780894

Epoch: 6| Step: 13
Training loss: 2.7702476978302
Validation loss: 2.5526506977696575

Epoch: 133| Step: 0
Training loss: 2.033010244369507
Validation loss: 2.5650416010169574

Epoch: 6| Step: 1
Training loss: 2.0945096015930176
Validation loss: 2.589254458745321

Epoch: 6| Step: 2
Training loss: 2.395961046218872
Validation loss: 2.6128894488016763

Epoch: 6| Step: 3
Training loss: 2.4708080291748047
Validation loss: 2.6044273350828435

Epoch: 6| Step: 4
Training loss: 3.08487606048584
Validation loss: 2.595945022439444

Epoch: 6| Step: 5
Training loss: 2.546900987625122
Validation loss: 2.572649601967104

Epoch: 6| Step: 6
Training loss: 3.2042548656463623
Validation loss: 2.560756814095282

Epoch: 6| Step: 7
Training loss: 2.0747737884521484
Validation loss: 2.54093510617492

Epoch: 6| Step: 8
Training loss: 2.8377113342285156
Validation loss: 2.5387464954007055

Epoch: 6| Step: 9
Training loss: 2.9133191108703613
Validation loss: 2.545610458620133

Epoch: 6| Step: 10
Training loss: 2.7426483631134033
Validation loss: 2.542481283987722

Epoch: 6| Step: 11
Training loss: 3.032602071762085
Validation loss: 2.545306821023264

Epoch: 6| Step: 12
Training loss: 4.1598405838012695
Validation loss: 2.553709494170322

Epoch: 6| Step: 13
Training loss: 2.537562131881714
Validation loss: 2.5492022293870167

Epoch: 134| Step: 0
Training loss: 3.2924652099609375
Validation loss: 2.5558890758022184

Epoch: 6| Step: 1
Training loss: 2.786931037902832
Validation loss: 2.5605151371289323

Epoch: 6| Step: 2
Training loss: 2.817837715148926
Validation loss: 2.5554287971988803

Epoch: 6| Step: 3
Training loss: 2.479445219039917
Validation loss: 2.5488654618622153

Epoch: 6| Step: 4
Training loss: 2.536302089691162
Validation loss: 2.547801838126234

Epoch: 6| Step: 5
Training loss: 2.293915271759033
Validation loss: 2.542624932463451

Epoch: 6| Step: 6
Training loss: 2.395029067993164
Validation loss: 2.5418316266869985

Epoch: 6| Step: 7
Training loss: 2.450338363647461
Validation loss: 2.538591049050772

Epoch: 6| Step: 8
Training loss: 2.7599809169769287
Validation loss: 2.535441824184951

Epoch: 6| Step: 9
Training loss: 2.700193405151367
Validation loss: 2.536793178127658

Epoch: 6| Step: 10
Training loss: 3.5662431716918945
Validation loss: 2.533781069581227

Epoch: 6| Step: 11
Training loss: 2.4143693447113037
Validation loss: 2.538582350618096

Epoch: 6| Step: 12
Training loss: 3.2775673866271973
Validation loss: 2.54009775705235

Epoch: 6| Step: 13
Training loss: 1.9951705932617188
Validation loss: 2.5364248342411493

Epoch: 135| Step: 0
Training loss: 3.3568124771118164
Validation loss: 2.534954394063642

Epoch: 6| Step: 1
Training loss: 2.7258262634277344
Validation loss: 2.537894928327171

Epoch: 6| Step: 2
Training loss: 2.8420979976654053
Validation loss: 2.5367047658530613

Epoch: 6| Step: 3
Training loss: 3.1165966987609863
Validation loss: 2.5396952141997633

Epoch: 6| Step: 4
Training loss: 2.1283528804779053
Validation loss: 2.5340940516482116

Epoch: 6| Step: 5
Training loss: 2.6976165771484375
Validation loss: 2.5393430571402273

Epoch: 6| Step: 6
Training loss: 2.3153843879699707
Validation loss: 2.5374142585262174

Epoch: 6| Step: 7
Training loss: 3.119410991668701
Validation loss: 2.5348671764455815

Epoch: 6| Step: 8
Training loss: 3.0887300968170166
Validation loss: 2.5352900566593295

Epoch: 6| Step: 9
Training loss: 2.4745163917541504
Validation loss: 2.5363233063810613

Epoch: 6| Step: 10
Training loss: 2.165189266204834
Validation loss: 2.5337782290674027

Epoch: 6| Step: 11
Training loss: 2.366823673248291
Validation loss: 2.5371961978174027

Epoch: 6| Step: 12
Training loss: 2.960075855255127
Validation loss: 2.5412429532697125

Epoch: 6| Step: 13
Training loss: 2.64872407913208
Validation loss: 2.538896294050319

Epoch: 136| Step: 0
Training loss: 3.2366886138916016
Validation loss: 2.537931026950959

Epoch: 6| Step: 1
Training loss: 2.2805778980255127
Validation loss: 2.54874228149332

Epoch: 6| Step: 2
Training loss: 2.814469575881958
Validation loss: 2.5562145991991927

Epoch: 6| Step: 3
Training loss: 2.8533153533935547
Validation loss: 2.5470521860225226

Epoch: 6| Step: 4
Training loss: 2.919377088546753
Validation loss: 2.541839284281577

Epoch: 6| Step: 5
Training loss: 2.575033664703369
Validation loss: 2.5278443315977692

Epoch: 6| Step: 6
Training loss: 2.7304985523223877
Validation loss: 2.5318993291547223

Epoch: 6| Step: 7
Training loss: 3.117418050765991
Validation loss: 2.5297476783875497

Epoch: 6| Step: 8
Training loss: 3.5441222190856934
Validation loss: 2.532245236058389

Epoch: 6| Step: 9
Training loss: 2.7302064895629883
Validation loss: 2.530716326928908

Epoch: 6| Step: 10
Training loss: 1.9430739879608154
Validation loss: 2.531789905281477

Epoch: 6| Step: 11
Training loss: 2.0849852561950684
Validation loss: 2.53253512741417

Epoch: 6| Step: 12
Training loss: 2.8642826080322266
Validation loss: 2.531853163114158

Epoch: 6| Step: 13
Training loss: 2.1377649307250977
Validation loss: 2.532380844957085

Epoch: 137| Step: 0
Training loss: 3.0208547115325928
Validation loss: 2.534601289738891

Epoch: 6| Step: 1
Training loss: 3.403390407562256
Validation loss: 2.532332322930777

Epoch: 6| Step: 2
Training loss: 2.421156883239746
Validation loss: 2.534825717249224

Epoch: 6| Step: 3
Training loss: 3.036855936050415
Validation loss: 2.5359291645788375

Epoch: 6| Step: 4
Training loss: 3.493471145629883
Validation loss: 2.5322065763576056

Epoch: 6| Step: 5
Training loss: 2.28662109375
Validation loss: 2.5316931227202057

Epoch: 6| Step: 6
Training loss: 2.8394007682800293
Validation loss: 2.5347003808585544

Epoch: 6| Step: 7
Training loss: 2.43147611618042
Validation loss: 2.535694173587266

Epoch: 6| Step: 8
Training loss: 2.982365846633911
Validation loss: 2.5363740946656916

Epoch: 6| Step: 9
Training loss: 2.417675495147705
Validation loss: 2.539065166186261

Epoch: 6| Step: 10
Training loss: 2.424222946166992
Validation loss: 2.5344128301066737

Epoch: 6| Step: 11
Training loss: 2.7952628135681152
Validation loss: 2.535579435286983

Epoch: 6| Step: 12
Training loss: 1.8302613496780396
Validation loss: 2.5287034203929286

Epoch: 6| Step: 13
Training loss: 2.5716657638549805
Validation loss: 2.5256917886836554

Epoch: 138| Step: 0
Training loss: 2.9498372077941895
Validation loss: 2.529779049658006

Epoch: 6| Step: 1
Training loss: 2.6134862899780273
Validation loss: 2.5227971487147833

Epoch: 6| Step: 2
Training loss: 1.916999101638794
Validation loss: 2.5240174596027662

Epoch: 6| Step: 3
Training loss: 2.7861838340759277
Validation loss: 2.524343077854444

Epoch: 6| Step: 4
Training loss: 3.219644069671631
Validation loss: 2.5265105103933685

Epoch: 6| Step: 5
Training loss: 1.8301523923873901
Validation loss: 2.5311877291689635

Epoch: 6| Step: 6
Training loss: 2.969008207321167
Validation loss: 2.527788508322931

Epoch: 6| Step: 7
Training loss: 3.7624282836914062
Validation loss: 2.52569031459029

Epoch: 6| Step: 8
Training loss: 2.5505123138427734
Validation loss: 2.523588821452151

Epoch: 6| Step: 9
Training loss: 2.675323963165283
Validation loss: 2.5243246504055556

Epoch: 6| Step: 10
Training loss: 2.8586301803588867
Validation loss: 2.5257508165092877

Epoch: 6| Step: 11
Training loss: 2.273749828338623
Validation loss: 2.525007468397899

Epoch: 6| Step: 12
Training loss: 2.8713743686676025
Validation loss: 2.528360269402945

Epoch: 6| Step: 13
Training loss: 2.6855263710021973
Validation loss: 2.5317281497422086

Epoch: 139| Step: 0
Training loss: 2.745586395263672
Validation loss: 2.5403473941228722

Epoch: 6| Step: 1
Training loss: 2.7794599533081055
Validation loss: 2.5460900081101285

Epoch: 6| Step: 2
Training loss: 3.110318660736084
Validation loss: 2.567609530623241

Epoch: 6| Step: 3
Training loss: 2.916403293609619
Validation loss: 2.5698221627102105

Epoch: 6| Step: 4
Training loss: 1.968668818473816
Validation loss: 2.567402414096299

Epoch: 6| Step: 5
Training loss: 2.30281138420105
Validation loss: 2.5649882849826606

Epoch: 6| Step: 6
Training loss: 2.9912893772125244
Validation loss: 2.554342664698119

Epoch: 6| Step: 7
Training loss: 2.516430377960205
Validation loss: 2.548781548776934

Epoch: 6| Step: 8
Training loss: 3.2570128440856934
Validation loss: 2.5403252032495316

Epoch: 6| Step: 9
Training loss: 2.769778251647949
Validation loss: 2.5315718984091156

Epoch: 6| Step: 10
Training loss: 2.8240463733673096
Validation loss: 2.5319272651467273

Epoch: 6| Step: 11
Training loss: 2.2763214111328125
Validation loss: 2.526384581801712

Epoch: 6| Step: 12
Training loss: 2.5024304389953613
Validation loss: 2.5229775803063506

Epoch: 6| Step: 13
Training loss: 3.444075584411621
Validation loss: 2.5236587857687347

Epoch: 140| Step: 0
Training loss: 2.0438075065612793
Validation loss: 2.5206979064531225

Epoch: 6| Step: 1
Training loss: 2.8446812629699707
Validation loss: 2.5177328458396335

Epoch: 6| Step: 2
Training loss: 3.1605782508850098
Validation loss: 2.5215725642378612

Epoch: 6| Step: 3
Training loss: 2.886976718902588
Validation loss: 2.5220494142142673

Epoch: 6| Step: 4
Training loss: 3.0609750747680664
Validation loss: 2.522374055718863

Epoch: 6| Step: 5
Training loss: 3.0923566818237305
Validation loss: 2.525009819256362

Epoch: 6| Step: 6
Training loss: 2.2591371536254883
Validation loss: 2.52938574616627

Epoch: 6| Step: 7
Training loss: 2.2502260208129883
Validation loss: 2.5320884848153717

Epoch: 6| Step: 8
Training loss: 2.868903636932373
Validation loss: 2.52854964938215

Epoch: 6| Step: 9
Training loss: 2.020082950592041
Validation loss: 2.5313206872632428

Epoch: 6| Step: 10
Training loss: 2.257721424102783
Validation loss: 2.528690004861483

Epoch: 6| Step: 11
Training loss: 3.2127456665039062
Validation loss: 2.526903619048416

Epoch: 6| Step: 12
Training loss: 2.4081759452819824
Validation loss: 2.533846991036528

Epoch: 6| Step: 13
Training loss: 4.361217021942139
Validation loss: 2.5359471638997397

Epoch: 141| Step: 0
Training loss: 2.1445953845977783
Validation loss: 2.5385126734292633

Epoch: 6| Step: 1
Training loss: 3.091339111328125
Validation loss: 2.5413960333793395

Epoch: 6| Step: 2
Training loss: 2.5158214569091797
Validation loss: 2.5362729385334957

Epoch: 6| Step: 3
Training loss: 3.090207099914551
Validation loss: 2.5334298815778507

Epoch: 6| Step: 4
Training loss: 3.4348225593566895
Validation loss: 2.5328121826212895

Epoch: 6| Step: 5
Training loss: 2.8431692123413086
Validation loss: 2.5318182796560307

Epoch: 6| Step: 6
Training loss: 2.335864782333374
Validation loss: 2.5313995602310344

Epoch: 6| Step: 7
Training loss: 2.092982530593872
Validation loss: 2.5353897950982534

Epoch: 6| Step: 8
Training loss: 2.298896312713623
Validation loss: 2.529098387687437

Epoch: 6| Step: 9
Training loss: 2.267343044281006
Validation loss: 2.52930308670126

Epoch: 6| Step: 10
Training loss: 3.298633098602295
Validation loss: 2.5311415913284465

Epoch: 6| Step: 11
Training loss: 3.160435676574707
Validation loss: 2.5353888157875306

Epoch: 6| Step: 12
Training loss: 2.6059255599975586
Validation loss: 2.531281489197926

Epoch: 6| Step: 13
Training loss: 2.7609426975250244
Validation loss: 2.5365217578026558

Epoch: 142| Step: 0
Training loss: 2.402240514755249
Validation loss: 2.5359375220473095

Epoch: 6| Step: 1
Training loss: 2.7817654609680176
Validation loss: 2.5298534106182795

Epoch: 6| Step: 2
Training loss: 2.4370503425598145
Validation loss: 2.5272279708616194

Epoch: 6| Step: 3
Training loss: 3.032883405685425
Validation loss: 2.522883394713043

Epoch: 6| Step: 4
Training loss: 3.126268148422241
Validation loss: 2.524263858795166

Epoch: 6| Step: 5
Training loss: 2.397270917892456
Validation loss: 2.5197979968081237

Epoch: 6| Step: 6
Training loss: 3.0287070274353027
Validation loss: 2.521410931823074

Epoch: 6| Step: 7
Training loss: 2.712744951248169
Validation loss: 2.5165929640493085

Epoch: 6| Step: 8
Training loss: 3.0767388343811035
Validation loss: 2.517359627190457

Epoch: 6| Step: 9
Training loss: 2.0270395278930664
Validation loss: 2.5143132337959866

Epoch: 6| Step: 10
Training loss: 2.9583663940429688
Validation loss: 2.516043539970152

Epoch: 6| Step: 11
Training loss: 2.6842331886291504
Validation loss: 2.516508383135642

Epoch: 6| Step: 12
Training loss: 2.6750993728637695
Validation loss: 2.515199348490725

Epoch: 6| Step: 13
Training loss: 2.5437397956848145
Validation loss: 2.520750707195651

Epoch: 143| Step: 0
Training loss: 2.6350650787353516
Validation loss: 2.520759549192203

Epoch: 6| Step: 1
Training loss: 2.761904716491699
Validation loss: 2.525520327270672

Epoch: 6| Step: 2
Training loss: 3.3084030151367188
Validation loss: 2.5349770797196256

Epoch: 6| Step: 3
Training loss: 1.9327657222747803
Validation loss: 2.535188421126335

Epoch: 6| Step: 4
Training loss: 3.3538243770599365
Validation loss: 2.5372335936433528

Epoch: 6| Step: 5
Training loss: 2.163106918334961
Validation loss: 2.5260039965311685

Epoch: 6| Step: 6
Training loss: 2.4954776763916016
Validation loss: 2.523684419611449

Epoch: 6| Step: 7
Training loss: 2.404794454574585
Validation loss: 2.5170550551465762

Epoch: 6| Step: 8
Training loss: 3.3783130645751953
Validation loss: 2.5162231614512782

Epoch: 6| Step: 9
Training loss: 3.1331748962402344
Validation loss: 2.518977462604482

Epoch: 6| Step: 10
Training loss: 2.8123183250427246
Validation loss: 2.5130281243273007

Epoch: 6| Step: 11
Training loss: 2.747587203979492
Validation loss: 2.5136611307820966

Epoch: 6| Step: 12
Training loss: 2.353203773498535
Validation loss: 2.512220690327306

Epoch: 6| Step: 13
Training loss: 2.281200647354126
Validation loss: 2.514377136384287

Epoch: 144| Step: 0
Training loss: 2.7814745903015137
Validation loss: 2.514208188620947

Epoch: 6| Step: 1
Training loss: 2.694568634033203
Validation loss: 2.5156506876791678

Epoch: 6| Step: 2
Training loss: 2.610560417175293
Validation loss: 2.511885094386275

Epoch: 6| Step: 3
Training loss: 2.876737594604492
Validation loss: 2.5107216040293374

Epoch: 6| Step: 4
Training loss: 3.1129469871520996
Validation loss: 2.5138521117548787

Epoch: 6| Step: 5
Training loss: 2.8528566360473633
Validation loss: 2.5142679470841602

Epoch: 6| Step: 6
Training loss: 2.534764289855957
Validation loss: 2.512024402618408

Epoch: 6| Step: 7
Training loss: 3.261197566986084
Validation loss: 2.51327762808851

Epoch: 6| Step: 8
Training loss: 2.6035714149475098
Validation loss: 2.5152373006266933

Epoch: 6| Step: 9
Training loss: 2.6179847717285156
Validation loss: 2.5193419943573656

Epoch: 6| Step: 10
Training loss: 2.6310999393463135
Validation loss: 2.522621365003688

Epoch: 6| Step: 11
Training loss: 3.307738780975342
Validation loss: 2.519367592309111

Epoch: 6| Step: 12
Training loss: 1.8858425617218018
Validation loss: 2.5129677134175457

Epoch: 6| Step: 13
Training loss: 1.560097336769104
Validation loss: 2.514556805292765

Epoch: 145| Step: 0
Training loss: 2.6355865001678467
Validation loss: 2.517551391355453

Epoch: 6| Step: 1
Training loss: 2.5738320350646973
Validation loss: 2.51303756108848

Epoch: 6| Step: 2
Training loss: 2.4166882038116455
Validation loss: 2.512752935450564

Epoch: 6| Step: 3
Training loss: 3.3982913494110107
Validation loss: 2.5135967039292857

Epoch: 6| Step: 4
Training loss: 2.6309075355529785
Validation loss: 2.512995239227049

Epoch: 6| Step: 5
Training loss: 2.703885316848755
Validation loss: 2.513178530559745

Epoch: 6| Step: 6
Training loss: 3.257521867752075
Validation loss: 2.515261527030699

Epoch: 6| Step: 7
Training loss: 2.03928279876709
Validation loss: 2.512911263332572

Epoch: 6| Step: 8
Training loss: 2.8295040130615234
Validation loss: 2.517575817723428

Epoch: 6| Step: 9
Training loss: 2.4866671562194824
Validation loss: 2.51325294022919

Epoch: 6| Step: 10
Training loss: 3.327230453491211
Validation loss: 2.5087105433146157

Epoch: 6| Step: 11
Training loss: 2.4865078926086426
Validation loss: 2.5142941449278142

Epoch: 6| Step: 12
Training loss: 2.159626007080078
Validation loss: 2.51390831188489

Epoch: 6| Step: 13
Training loss: 3.006088972091675
Validation loss: 2.514005009846021

Epoch: 146| Step: 0
Training loss: 3.2838492393493652
Validation loss: 2.512003226946759

Epoch: 6| Step: 1
Training loss: 3.2411580085754395
Validation loss: 2.516436502497683

Epoch: 6| Step: 2
Training loss: 2.950345993041992
Validation loss: 2.5142581450041903

Epoch: 6| Step: 3
Training loss: 3.366976499557495
Validation loss: 2.517454549830447

Epoch: 6| Step: 4
Training loss: 2.6729750633239746
Validation loss: 2.514883633582823

Epoch: 6| Step: 5
Training loss: 2.1528120040893555
Validation loss: 2.51641518838944

Epoch: 6| Step: 6
Training loss: 2.868311882019043
Validation loss: 2.518716763424617

Epoch: 6| Step: 7
Training loss: 2.3094940185546875
Validation loss: 2.522671966142552

Epoch: 6| Step: 8
Training loss: 2.8988242149353027
Validation loss: 2.5175321512324835

Epoch: 6| Step: 9
Training loss: 2.3786122798919678
Validation loss: 2.5159188188532347

Epoch: 6| Step: 10
Training loss: 1.938575029373169
Validation loss: 2.5140976290549

Epoch: 6| Step: 11
Training loss: 1.9103870391845703
Validation loss: 2.512920820584861

Epoch: 6| Step: 12
Training loss: 3.19621205329895
Validation loss: 2.510904869725627

Epoch: 6| Step: 13
Training loss: 2.624091386795044
Validation loss: 2.5113551103940575

Epoch: 147| Step: 0
Training loss: 2.5657782554626465
Validation loss: 2.512719295358145

Epoch: 6| Step: 1
Training loss: 2.5507564544677734
Validation loss: 2.5147602968318488

Epoch: 6| Step: 2
Training loss: 3.1681809425354004
Validation loss: 2.509375108185635

Epoch: 6| Step: 3
Training loss: 2.185056447982788
Validation loss: 2.5135948863080753

Epoch: 6| Step: 4
Training loss: 2.865208148956299
Validation loss: 2.5153701228480183

Epoch: 6| Step: 5
Training loss: 2.2504231929779053
Validation loss: 2.5167904746147896

Epoch: 6| Step: 6
Training loss: 2.059941291809082
Validation loss: 2.5130716190543225

Epoch: 6| Step: 7
Training loss: 3.3982768058776855
Validation loss: 2.512982532542239

Epoch: 6| Step: 8
Training loss: 3.313758611679077
Validation loss: 2.512859793119533

Epoch: 6| Step: 9
Training loss: 2.695624351501465
Validation loss: 2.509207010269165

Epoch: 6| Step: 10
Training loss: 2.989798069000244
Validation loss: 2.5081951515648955

Epoch: 6| Step: 11
Training loss: 2.08774995803833
Validation loss: 2.5088071925665743

Epoch: 6| Step: 12
Training loss: 3.245182514190674
Validation loss: 2.511369341163225

Epoch: 6| Step: 13
Training loss: 2.1802868843078613
Validation loss: 2.511183759217621

Epoch: 148| Step: 0
Training loss: 1.9722726345062256
Validation loss: 2.509133661946943

Epoch: 6| Step: 1
Training loss: 3.1590120792388916
Validation loss: 2.516635843502578

Epoch: 6| Step: 2
Training loss: 3.790132522583008
Validation loss: 2.5126422015569543

Epoch: 6| Step: 3
Training loss: 2.4204936027526855
Validation loss: 2.514938695456392

Epoch: 6| Step: 4
Training loss: 1.8795610666275024
Validation loss: 2.515921602966965

Epoch: 6| Step: 5
Training loss: 2.7480521202087402
Validation loss: 2.5166009215898413

Epoch: 6| Step: 6
Training loss: 3.544386148452759
Validation loss: 2.5181070681541198

Epoch: 6| Step: 7
Training loss: 2.1822566986083984
Validation loss: 2.5133482538243777

Epoch: 6| Step: 8
Training loss: 2.881824016571045
Validation loss: 2.51799096856066

Epoch: 6| Step: 9
Training loss: 2.374319553375244
Validation loss: 2.5099082531467563

Epoch: 6| Step: 10
Training loss: 2.1086246967315674
Validation loss: 2.5140039074805474

Epoch: 6| Step: 11
Training loss: 2.6388754844665527
Validation loss: 2.514025124170447

Epoch: 6| Step: 12
Training loss: 3.4601821899414062
Validation loss: 2.513200916269774

Epoch: 6| Step: 13
Training loss: 2.6176092624664307
Validation loss: 2.5154875786073747

Epoch: 149| Step: 0
Training loss: 3.332413673400879
Validation loss: 2.5153573789904193

Epoch: 6| Step: 1
Training loss: 2.043397903442383
Validation loss: 2.5144148078016055

Epoch: 6| Step: 2
Training loss: 2.676999092102051
Validation loss: 2.5102535217039046

Epoch: 6| Step: 3
Training loss: 2.021937131881714
Validation loss: 2.511521977763022

Epoch: 6| Step: 4
Training loss: 3.170229196548462
Validation loss: 2.510986997235206

Epoch: 6| Step: 5
Training loss: 2.4696788787841797
Validation loss: 2.5124116251545567

Epoch: 6| Step: 6
Training loss: 2.4032604694366455
Validation loss: 2.513108899516444

Epoch: 6| Step: 7
Training loss: 2.6837844848632812
Validation loss: 2.5122155322823474

Epoch: 6| Step: 8
Training loss: 2.256875514984131
Validation loss: 2.509755447346677

Epoch: 6| Step: 9
Training loss: 1.965428352355957
Validation loss: 2.507329456267818

Epoch: 6| Step: 10
Training loss: 3.5597293376922607
Validation loss: 2.5072513139376076

Epoch: 6| Step: 11
Training loss: 3.2913622856140137
Validation loss: 2.5060556575816166

Epoch: 6| Step: 12
Training loss: 2.9504940509796143
Validation loss: 2.505883386058192

Epoch: 6| Step: 13
Training loss: 3.049966812133789
Validation loss: 2.5036344502561834

Epoch: 150| Step: 0
Training loss: 1.842710018157959
Validation loss: 2.503425380235077

Epoch: 6| Step: 1
Training loss: 2.847444534301758
Validation loss: 2.506263250945717

Epoch: 6| Step: 2
Training loss: 3.2740120887756348
Validation loss: 2.5025001059296312

Epoch: 6| Step: 3
Training loss: 2.3686397075653076
Validation loss: 2.5016858475182646

Epoch: 6| Step: 4
Training loss: 3.0007262229919434
Validation loss: 2.503243851405318

Epoch: 6| Step: 5
Training loss: 2.818748950958252
Validation loss: 2.504872860447053

Epoch: 6| Step: 6
Training loss: 3.271329402923584
Validation loss: 2.509686582831926

Epoch: 6| Step: 7
Training loss: 2.1266582012176514
Validation loss: 2.519405257317328

Epoch: 6| Step: 8
Training loss: 2.7428696155548096
Validation loss: 2.511605114065191

Epoch: 6| Step: 9
Training loss: 2.2278378009796143
Validation loss: 2.516238622767951

Epoch: 6| Step: 10
Training loss: 2.768263816833496
Validation loss: 2.515013369180823

Epoch: 6| Step: 11
Training loss: 3.148963451385498
Validation loss: 2.511388058303505

Epoch: 6| Step: 12
Training loss: 1.881101131439209
Validation loss: 2.509758851861441

Epoch: 6| Step: 13
Training loss: 4.023308277130127
Validation loss: 2.5052238766865065

Epoch: 151| Step: 0
Training loss: 2.051248073577881
Validation loss: 2.500164367819345

Epoch: 6| Step: 1
Training loss: 1.9194133281707764
Validation loss: 2.4973026116689048

Epoch: 6| Step: 2
Training loss: 2.606947422027588
Validation loss: 2.5038036915563766

Epoch: 6| Step: 3
Training loss: 3.374114513397217
Validation loss: 2.499047686976771

Epoch: 6| Step: 4
Training loss: 2.809405565261841
Validation loss: 2.499493022118845

Epoch: 6| Step: 5
Training loss: 3.3042283058166504
Validation loss: 2.4974988557959117

Epoch: 6| Step: 6
Training loss: 3.5840325355529785
Validation loss: 2.500428448441208

Epoch: 6| Step: 7
Training loss: 2.7754247188568115
Validation loss: 2.500157889499459

Epoch: 6| Step: 8
Training loss: 3.0140912532806396
Validation loss: 2.5007464398619947

Epoch: 6| Step: 9
Training loss: 2.9733729362487793
Validation loss: 2.5019600109387468

Epoch: 6| Step: 10
Training loss: 2.205153703689575
Validation loss: 2.4983374405932683

Epoch: 6| Step: 11
Training loss: 1.7404898405075073
Validation loss: 2.503931027586742

Epoch: 6| Step: 12
Training loss: 2.3613719940185547
Validation loss: 2.504575721679195

Epoch: 6| Step: 13
Training loss: 3.2967138290405273
Validation loss: 2.5268588348101546

Epoch: 152| Step: 0
Training loss: 2.960895538330078
Validation loss: 2.5050216259494906

Epoch: 6| Step: 1
Training loss: 2.394920587539673
Validation loss: 2.500071930628951

Epoch: 6| Step: 2
Training loss: 2.611626148223877
Validation loss: 2.50372298302189

Epoch: 6| Step: 3
Training loss: 2.855567693710327
Validation loss: 2.505194707583356

Epoch: 6| Step: 4
Training loss: 2.5994460582733154
Validation loss: 2.502302667146088

Epoch: 6| Step: 5
Training loss: 2.644718647003174
Validation loss: 2.503177391585483

Epoch: 6| Step: 6
Training loss: 2.849478244781494
Validation loss: 2.5037040812994844

Epoch: 6| Step: 7
Training loss: 2.3365283012390137
Validation loss: 2.505559223954396

Epoch: 6| Step: 8
Training loss: 2.333167791366577
Validation loss: 2.5077746427187355

Epoch: 6| Step: 9
Training loss: 2.600890636444092
Validation loss: 2.511605760102631

Epoch: 6| Step: 10
Training loss: 2.809215545654297
Validation loss: 2.510663351704997

Epoch: 6| Step: 11
Training loss: 2.6460719108581543
Validation loss: 2.5075942598363405

Epoch: 6| Step: 12
Training loss: 3.3825528621673584
Validation loss: 2.5063066123634257

Epoch: 6| Step: 13
Training loss: 2.7815470695495605
Validation loss: 2.5051654154254543

Epoch: 153| Step: 0
Training loss: 3.229947090148926
Validation loss: 2.5024031003316245

Epoch: 6| Step: 1
Training loss: 2.471893310546875
Validation loss: 2.5042690205317673

Epoch: 6| Step: 2
Training loss: 2.06512451171875
Validation loss: 2.501633213412377

Epoch: 6| Step: 3
Training loss: 3.5997097492218018
Validation loss: 2.4973551432291665

Epoch: 6| Step: 4
Training loss: 2.8807835578918457
Validation loss: 2.5027150851424023

Epoch: 6| Step: 5
Training loss: 1.5149388313293457
Validation loss: 2.5060125807280182

Epoch: 6| Step: 6
Training loss: 2.957146644592285
Validation loss: 2.5115827847552556

Epoch: 6| Step: 7
Training loss: 2.1979002952575684
Validation loss: 2.5170020775128434

Epoch: 6| Step: 8
Training loss: 2.1161482334136963
Validation loss: 2.51431078808282

Epoch: 6| Step: 9
Training loss: 2.54608154296875
Validation loss: 2.5081136431745303

Epoch: 6| Step: 10
Training loss: 2.680589437484741
Validation loss: 2.5074279410864717

Epoch: 6| Step: 11
Training loss: 2.9306459426879883
Validation loss: 2.503141390380039

Epoch: 6| Step: 12
Training loss: 3.528278350830078
Validation loss: 2.5070448306299027

Epoch: 6| Step: 13
Training loss: 3.1302478313446045
Validation loss: 2.5009590784708657

Epoch: 154| Step: 0
Training loss: 1.9625343084335327
Validation loss: 2.4962249186731156

Epoch: 6| Step: 1
Training loss: 2.206685781478882
Validation loss: 2.497083915177212

Epoch: 6| Step: 2
Training loss: 2.9806275367736816
Validation loss: 2.4941904557648527

Epoch: 6| Step: 3
Training loss: 3.3525493144989014
Validation loss: 2.4959776427156184

Epoch: 6| Step: 4
Training loss: 2.418574094772339
Validation loss: 2.496006424709033

Epoch: 6| Step: 5
Training loss: 2.582258701324463
Validation loss: 2.495256018894975

Epoch: 6| Step: 6
Training loss: 1.8044648170471191
Validation loss: 2.4915823910825994

Epoch: 6| Step: 7
Training loss: 3.421097755432129
Validation loss: 2.4990939504356793

Epoch: 6| Step: 8
Training loss: 2.761220932006836
Validation loss: 2.496847680819932

Epoch: 6| Step: 9
Training loss: 2.892308235168457
Validation loss: 2.498680294200938

Epoch: 6| Step: 10
Training loss: 3.1366138458251953
Validation loss: 2.4980314957198275

Epoch: 6| Step: 11
Training loss: 2.205838203430176
Validation loss: 2.4974149516833726

Epoch: 6| Step: 12
Training loss: 3.221926212310791
Validation loss: 2.505988590178951

Epoch: 6| Step: 13
Training loss: 2.772643804550171
Validation loss: 2.5033739023311163

Epoch: 155| Step: 0
Training loss: 2.5721004009246826
Validation loss: 2.5020703782317457

Epoch: 6| Step: 1
Training loss: 2.621121883392334
Validation loss: 2.506234868880241

Epoch: 6| Step: 2
Training loss: 2.2328665256500244
Validation loss: 2.498757267511019

Epoch: 6| Step: 3
Training loss: 2.196079969406128
Validation loss: 2.5013301962165424

Epoch: 6| Step: 4
Training loss: 3.468987226486206
Validation loss: 2.4984636947672856

Epoch: 6| Step: 5
Training loss: 2.28617000579834
Validation loss: 2.497630406451482

Epoch: 6| Step: 6
Training loss: 3.1525120735168457
Validation loss: 2.4981842117924846

Epoch: 6| Step: 7
Training loss: 2.861421585083008
Validation loss: 2.500844268388646

Epoch: 6| Step: 8
Training loss: 2.623615264892578
Validation loss: 2.4996423259858163

Epoch: 6| Step: 9
Training loss: 2.48162841796875
Validation loss: 2.501614570617676

Epoch: 6| Step: 10
Training loss: 3.7101659774780273
Validation loss: 2.5019028391889346

Epoch: 6| Step: 11
Training loss: 2.882720708847046
Validation loss: 2.4926688132747525

Epoch: 6| Step: 12
Training loss: 2.4269652366638184
Validation loss: 2.4942763441352436

Epoch: 6| Step: 13
Training loss: 1.7569730281829834
Validation loss: 2.4922333737855316

Epoch: 156| Step: 0
Training loss: 3.1289584636688232
Validation loss: 2.494197560894874

Epoch: 6| Step: 1
Training loss: 2.0500431060791016
Validation loss: 2.490334949185771

Epoch: 6| Step: 2
Training loss: 2.2068867683410645
Validation loss: 2.491936383708831

Epoch: 6| Step: 3
Training loss: 2.244960308074951
Validation loss: 2.496842416383887

Epoch: 6| Step: 4
Training loss: 3.0513243675231934
Validation loss: 2.4919824433583084

Epoch: 6| Step: 5
Training loss: 2.867302179336548
Validation loss: 2.4999010101441415

Epoch: 6| Step: 6
Training loss: 2.833925247192383
Validation loss: 2.501078121123775

Epoch: 6| Step: 7
Training loss: 2.7603988647460938
Validation loss: 2.50243942968307

Epoch: 6| Step: 8
Training loss: 2.8203318119049072
Validation loss: 2.504938430683587

Epoch: 6| Step: 9
Training loss: 3.044004440307617
Validation loss: 2.507228712881765

Epoch: 6| Step: 10
Training loss: 3.118893623352051
Validation loss: 2.5038551412602907

Epoch: 6| Step: 11
Training loss: 2.6154162883758545
Validation loss: 2.505517557103147

Epoch: 6| Step: 12
Training loss: 2.814100503921509
Validation loss: 2.497713065916492

Epoch: 6| Step: 13
Training loss: 1.6177268028259277
Validation loss: 2.4930586302152244

Epoch: 157| Step: 0
Training loss: 3.273723602294922
Validation loss: 2.489485579152261

Epoch: 6| Step: 1
Training loss: 2.3110995292663574
Validation loss: 2.4914308671028382

Epoch: 6| Step: 2
Training loss: 3.012876033782959
Validation loss: 2.4906416144422305

Epoch: 6| Step: 3
Training loss: 2.4085116386413574
Validation loss: 2.4909371740074566

Epoch: 6| Step: 4
Training loss: 3.0430917739868164
Validation loss: 2.4887523369122575

Epoch: 6| Step: 5
Training loss: 2.545199394226074
Validation loss: 2.489043956161827

Epoch: 6| Step: 6
Training loss: 3.1391854286193848
Validation loss: 2.4904029933355187

Epoch: 6| Step: 7
Training loss: 2.5219640731811523
Validation loss: 2.4884000619252524

Epoch: 6| Step: 8
Training loss: 3.1271753311157227
Validation loss: 2.4887714309077107

Epoch: 6| Step: 9
Training loss: 2.3884811401367188
Validation loss: 2.487363717889273

Epoch: 6| Step: 10
Training loss: 2.190927028656006
Validation loss: 2.4862836663440993

Epoch: 6| Step: 11
Training loss: 3.0346741676330566
Validation loss: 2.4861139994795605

Epoch: 6| Step: 12
Training loss: 2.2940683364868164
Validation loss: 2.4881830112908476

Epoch: 6| Step: 13
Training loss: 2.039851665496826
Validation loss: 2.48304945294575

Epoch: 158| Step: 0
Training loss: 3.076016426086426
Validation loss: 2.4877330846683954

Epoch: 6| Step: 1
Training loss: 2.0535974502563477
Validation loss: 2.4903593883719495

Epoch: 6| Step: 2
Training loss: 2.9332213401794434
Validation loss: 2.4898684768266577

Epoch: 6| Step: 3
Training loss: 3.0324459075927734
Validation loss: 2.490559265177737

Epoch: 6| Step: 4
Training loss: 2.4057505130767822
Validation loss: 2.493722111948075

Epoch: 6| Step: 5
Training loss: 1.873936653137207
Validation loss: 2.4971482958844913

Epoch: 6| Step: 6
Training loss: 2.379983425140381
Validation loss: 2.502542144508772

Epoch: 6| Step: 7
Training loss: 2.8557002544403076
Validation loss: 2.5093052105237077

Epoch: 6| Step: 8
Training loss: 2.8836426734924316
Validation loss: 2.503044759073565

Epoch: 6| Step: 9
Training loss: 2.822160243988037
Validation loss: 2.4992465844718357

Epoch: 6| Step: 10
Training loss: 2.62740421295166
Validation loss: 2.4973571377415813

Epoch: 6| Step: 11
Training loss: 2.938352584838867
Validation loss: 2.4878494226804344

Epoch: 6| Step: 12
Training loss: 2.8271584510803223
Validation loss: 2.4908643768679712

Epoch: 6| Step: 13
Training loss: 3.0762548446655273
Validation loss: 2.4912282215651644

Epoch: 159| Step: 0
Training loss: 2.9546847343444824
Validation loss: 2.4855476784449753

Epoch: 6| Step: 1
Training loss: 2.867621421813965
Validation loss: 2.4898075698524393

Epoch: 6| Step: 2
Training loss: 2.8610334396362305
Validation loss: 2.488165960516981

Epoch: 6| Step: 3
Training loss: 2.4283957481384277
Validation loss: 2.4865584988747873

Epoch: 6| Step: 4
Training loss: 2.704098701477051
Validation loss: 2.4853064885703464

Epoch: 6| Step: 5
Training loss: 2.391007900238037
Validation loss: 2.490005272690968

Epoch: 6| Step: 6
Training loss: 2.963219165802002
Validation loss: 2.484673543642926

Epoch: 6| Step: 7
Training loss: 2.1589951515197754
Validation loss: 2.485610444058654

Epoch: 6| Step: 8
Training loss: 2.0486040115356445
Validation loss: 2.4894639779162664

Epoch: 6| Step: 9
Training loss: 2.940291404724121
Validation loss: 2.488255198283862

Epoch: 6| Step: 10
Training loss: 3.6232287883758545
Validation loss: 2.4904704337478965

Epoch: 6| Step: 11
Training loss: 3.126800537109375
Validation loss: 2.4890759580878803

Epoch: 6| Step: 12
Training loss: 2.0632410049438477
Validation loss: 2.4902194905024704

Epoch: 6| Step: 13
Training loss: 2.2246809005737305
Validation loss: 2.485613807555168

Epoch: 160| Step: 0
Training loss: 3.1051137447357178
Validation loss: 2.492984858892297

Epoch: 6| Step: 1
Training loss: 3.085813045501709
Validation loss: 2.4927455712390203

Epoch: 6| Step: 2
Training loss: 3.198410749435425
Validation loss: 2.4924576179955595

Epoch: 6| Step: 3
Training loss: 3.2702560424804688
Validation loss: 2.492151565449212

Epoch: 6| Step: 4
Training loss: 3.350419521331787
Validation loss: 2.494873103275094

Epoch: 6| Step: 5
Training loss: 2.397035598754883
Validation loss: 2.4948684502673406

Epoch: 6| Step: 6
Training loss: 2.261831521987915
Validation loss: 2.495359141339538

Epoch: 6| Step: 7
Training loss: 2.8664002418518066
Validation loss: 2.4933191243038384

Epoch: 6| Step: 8
Training loss: 2.90240216255188
Validation loss: 2.4909392274836057

Epoch: 6| Step: 9
Training loss: 2.252401113510132
Validation loss: 2.4820016917362007

Epoch: 6| Step: 10
Training loss: 1.8642404079437256
Validation loss: 2.4821752835345525

Epoch: 6| Step: 11
Training loss: 2.860431671142578
Validation loss: 2.4868561016616

Epoch: 6| Step: 12
Training loss: 1.7409162521362305
Validation loss: 2.4846121393224245

Epoch: 6| Step: 13
Training loss: 2.2389755249023438
Validation loss: 2.48303843826376

Epoch: 161| Step: 0
Training loss: 3.064857006072998
Validation loss: 2.480713559735206

Epoch: 6| Step: 1
Training loss: 2.2384495735168457
Validation loss: 2.484902443424348

Epoch: 6| Step: 2
Training loss: 2.4327750205993652
Validation loss: 2.4871435165405273

Epoch: 6| Step: 3
Training loss: 2.7073147296905518
Validation loss: 2.4888687210698284

Epoch: 6| Step: 4
Training loss: 2.3452558517456055
Validation loss: 2.4879960988157537

Epoch: 6| Step: 5
Training loss: 2.033196449279785
Validation loss: 2.492779921459895

Epoch: 6| Step: 6
Training loss: 3.265026807785034
Validation loss: 2.4935403562361196

Epoch: 6| Step: 7
Training loss: 2.7225587368011475
Validation loss: 2.502234466614262

Epoch: 6| Step: 8
Training loss: 2.602060317993164
Validation loss: 2.5068981673127864

Epoch: 6| Step: 9
Training loss: 2.4733705520629883
Validation loss: 2.5158753318171345

Epoch: 6| Step: 10
Training loss: 2.616442918777466
Validation loss: 2.5133038618231334

Epoch: 6| Step: 11
Training loss: 3.540668487548828
Validation loss: 2.503199058194314

Epoch: 6| Step: 12
Training loss: 2.756981372833252
Validation loss: 2.4984585315950456

Epoch: 6| Step: 13
Training loss: 2.7952370643615723
Validation loss: 2.4899125881092523

Epoch: 162| Step: 0
Training loss: 2.3604893684387207
Validation loss: 2.491211170791298

Epoch: 6| Step: 1
Training loss: 3.4796719551086426
Validation loss: 2.491671164830526

Epoch: 6| Step: 2
Training loss: 2.945032835006714
Validation loss: 2.4913977897295387

Epoch: 6| Step: 3
Training loss: 2.9488747119903564
Validation loss: 2.485697300203385

Epoch: 6| Step: 4
Training loss: 2.8634262084960938
Validation loss: 2.4902758162508727

Epoch: 6| Step: 5
Training loss: 2.6586289405822754
Validation loss: 2.4905098945863786

Epoch: 6| Step: 6
Training loss: 2.9853060245513916
Validation loss: 2.492570682238507

Epoch: 6| Step: 7
Training loss: 2.7903099060058594
Validation loss: 2.493746929271247

Epoch: 6| Step: 8
Training loss: 2.3449630737304688
Validation loss: 2.4907874420124996

Epoch: 6| Step: 9
Training loss: 3.1928627490997314
Validation loss: 2.495067109343826

Epoch: 6| Step: 10
Training loss: 1.5966229438781738
Validation loss: 2.490857198674192

Epoch: 6| Step: 11
Training loss: 2.195549488067627
Validation loss: 2.4916841009611725

Epoch: 6| Step: 12
Training loss: 2.621407985687256
Validation loss: 2.494363825808289

Epoch: 6| Step: 13
Training loss: 2.4780688285827637
Validation loss: 2.495148909989224

Epoch: 163| Step: 0
Training loss: 2.1497302055358887
Validation loss: 2.494594612429219

Epoch: 6| Step: 1
Training loss: 1.7908546924591064
Validation loss: 2.5019190798523607

Epoch: 6| Step: 2
Training loss: 3.4081296920776367
Validation loss: 2.4967391311481433

Epoch: 6| Step: 3
Training loss: 2.520705223083496
Validation loss: 2.500809413130565

Epoch: 6| Step: 4
Training loss: 2.666959762573242
Validation loss: 2.4995148258824504

Epoch: 6| Step: 5
Training loss: 2.1786227226257324
Validation loss: 2.503381775271508

Epoch: 6| Step: 6
Training loss: 3.078052043914795
Validation loss: 2.4968232852156445

Epoch: 6| Step: 7
Training loss: 3.1658382415771484
Validation loss: 2.5031147336447113

Epoch: 6| Step: 8
Training loss: 2.9665579795837402
Validation loss: 2.5054635565768004

Epoch: 6| Step: 9
Training loss: 2.8950650691986084
Validation loss: 2.503872476598268

Epoch: 6| Step: 10
Training loss: 3.1018786430358887
Validation loss: 2.5029908264836958

Epoch: 6| Step: 11
Training loss: 2.954631805419922
Validation loss: 2.497232949861916

Epoch: 6| Step: 12
Training loss: 2.2348289489746094
Validation loss: 2.4946739776160127

Epoch: 6| Step: 13
Training loss: 2.10733699798584
Validation loss: 2.48972180838226

Epoch: 164| Step: 0
Training loss: 2.417811870574951
Validation loss: 2.486192669919742

Epoch: 6| Step: 1
Training loss: 3.4727821350097656
Validation loss: 2.4805871030335784

Epoch: 6| Step: 2
Training loss: 3.1766419410705566
Validation loss: 2.4827654182270007

Epoch: 6| Step: 3
Training loss: 2.2489585876464844
Validation loss: 2.484096944973033

Epoch: 6| Step: 4
Training loss: 2.745220899581909
Validation loss: 2.4822060882404284

Epoch: 6| Step: 5
Training loss: 2.730602502822876
Validation loss: 2.483112250604937

Epoch: 6| Step: 6
Training loss: 2.4538958072662354
Validation loss: 2.4771787325541177

Epoch: 6| Step: 7
Training loss: 2.4545273780822754
Validation loss: 2.4803332718469764

Epoch: 6| Step: 8
Training loss: 2.685074806213379
Validation loss: 2.4786854815739456

Epoch: 6| Step: 9
Training loss: 2.033446788787842
Validation loss: 2.477316277001494

Epoch: 6| Step: 10
Training loss: 2.5593385696411133
Validation loss: 2.480916146309145

Epoch: 6| Step: 11
Training loss: 2.8183135986328125
Validation loss: 2.484990894153554

Epoch: 6| Step: 12
Training loss: 2.3977932929992676
Validation loss: 2.486616098752586

Epoch: 6| Step: 13
Training loss: 3.7126214504241943
Validation loss: 2.4908172879167783

Epoch: 165| Step: 0
Training loss: 2.3896591663360596
Validation loss: 2.490035492886779

Epoch: 6| Step: 1
Training loss: 3.3976762294769287
Validation loss: 2.4902326855608212

Epoch: 6| Step: 2
Training loss: 2.6622533798217773
Validation loss: 2.490355965911701

Epoch: 6| Step: 3
Training loss: 2.2036328315734863
Validation loss: 2.4906933256374892

Epoch: 6| Step: 4
Training loss: 2.705139636993408
Validation loss: 2.4876923894369476

Epoch: 6| Step: 5
Training loss: 2.5400824546813965
Validation loss: 2.487899713618781

Epoch: 6| Step: 6
Training loss: 2.301713228225708
Validation loss: 2.4845097718700284

Epoch: 6| Step: 7
Training loss: 3.053145408630371
Validation loss: 2.484416377159857

Epoch: 6| Step: 8
Training loss: 3.040604591369629
Validation loss: 2.4841022722182737

Epoch: 6| Step: 9
Training loss: 2.5959560871124268
Validation loss: 2.4795374870300293

Epoch: 6| Step: 10
Training loss: 2.1040244102478027
Validation loss: 2.480809606531615

Epoch: 6| Step: 11
Training loss: 3.2278378009796143
Validation loss: 2.4755622392059653

Epoch: 6| Step: 12
Training loss: 2.436490058898926
Validation loss: 2.4795515306534304

Epoch: 6| Step: 13
Training loss: 2.919334888458252
Validation loss: 2.4807620766342326

Epoch: 166| Step: 0
Training loss: 3.1916232109069824
Validation loss: 2.476951153047623

Epoch: 6| Step: 1
Training loss: 3.0941836833953857
Validation loss: 2.482757891378095

Epoch: 6| Step: 2
Training loss: 2.3461179733276367
Validation loss: 2.4824474037334485

Epoch: 6| Step: 3
Training loss: 2.181471824645996
Validation loss: 2.4929960953292025

Epoch: 6| Step: 4
Training loss: 2.047390937805176
Validation loss: 2.5060251605126167

Epoch: 6| Step: 5
Training loss: 2.233480930328369
Validation loss: 2.5013952050157773

Epoch: 6| Step: 6
Training loss: 2.841263771057129
Validation loss: 2.5059123962156233

Epoch: 6| Step: 7
Training loss: 2.5983290672302246
Validation loss: 2.5098324078385548

Epoch: 6| Step: 8
Training loss: 2.981339693069458
Validation loss: 2.498103000784433

Epoch: 6| Step: 9
Training loss: 2.9520487785339355
Validation loss: 2.49255286493609

Epoch: 6| Step: 10
Training loss: 2.0379796028137207
Validation loss: 2.4856574612279094

Epoch: 6| Step: 11
Training loss: 3.494288444519043
Validation loss: 2.4937406637335338

Epoch: 6| Step: 12
Training loss: 2.7077856063842773
Validation loss: 2.486084320211923

Epoch: 6| Step: 13
Training loss: 2.8305861949920654
Validation loss: 2.4845611280010593

Epoch: 167| Step: 0
Training loss: 3.1141395568847656
Validation loss: 2.4850053171957693

Epoch: 6| Step: 1
Training loss: 2.663173198699951
Validation loss: 2.4873984398380404

Epoch: 6| Step: 2
Training loss: 2.3125076293945312
Validation loss: 2.4798472055824856

Epoch: 6| Step: 3
Training loss: 2.7905662059783936
Validation loss: 2.4780733585357666

Epoch: 6| Step: 4
Training loss: 3.060476303100586
Validation loss: 2.4745289817933114

Epoch: 6| Step: 5
Training loss: 2.2039737701416016
Validation loss: 2.484935857916391

Epoch: 6| Step: 6
Training loss: 3.418168067932129
Validation loss: 2.4873926690829697

Epoch: 6| Step: 7
Training loss: 2.6975858211517334
Validation loss: 2.4907959404812066

Epoch: 6| Step: 8
Training loss: 2.576573371887207
Validation loss: 2.488596731616605

Epoch: 6| Step: 9
Training loss: 2.388275146484375
Validation loss: 2.4891520264328166

Epoch: 6| Step: 10
Training loss: 2.6750080585479736
Validation loss: 2.4897340138753257

Epoch: 6| Step: 11
Training loss: 2.7333788871765137
Validation loss: 2.4826779416812363

Epoch: 6| Step: 12
Training loss: 2.79229998588562
Validation loss: 2.4758670048047136

Epoch: 6| Step: 13
Training loss: 1.5614352226257324
Validation loss: 2.4853260414574736

Epoch: 168| Step: 0
Training loss: 2.682342052459717
Validation loss: 2.4968692077103483

Epoch: 6| Step: 1
Training loss: 3.2248008251190186
Validation loss: 2.4920604126427763

Epoch: 6| Step: 2
Training loss: 3.3838839530944824
Validation loss: 2.5035492758597098

Epoch: 6| Step: 3
Training loss: 1.8805361986160278
Validation loss: 2.509256321896789

Epoch: 6| Step: 4
Training loss: 2.716430187225342
Validation loss: 2.5149521878970567

Epoch: 6| Step: 5
Training loss: 2.8058831691741943
Validation loss: 2.5055708372464744

Epoch: 6| Step: 6
Training loss: 2.8936123847961426
Validation loss: 2.507835280510687

Epoch: 6| Step: 7
Training loss: 2.9348244667053223
Validation loss: 2.4896796839211577

Epoch: 6| Step: 8
Training loss: 2.528954029083252
Validation loss: 2.4776603252657

Epoch: 6| Step: 9
Training loss: 2.2841014862060547
Validation loss: 2.4771380398863103

Epoch: 6| Step: 10
Training loss: 1.7034661769866943
Validation loss: 2.4683018063986175

Epoch: 6| Step: 11
Training loss: 3.0086140632629395
Validation loss: 2.468283430222542

Epoch: 6| Step: 12
Training loss: 2.729617118835449
Validation loss: 2.472233646659441

Epoch: 6| Step: 13
Training loss: 2.6879537105560303
Validation loss: 2.465578294569446

Epoch: 169| Step: 0
Training loss: 2.0962393283843994
Validation loss: 2.465648635741203

Epoch: 6| Step: 1
Training loss: 2.592874526977539
Validation loss: 2.469608094102593

Epoch: 6| Step: 2
Training loss: 3.1835641860961914
Validation loss: 2.4658848470257175

Epoch: 6| Step: 3
Training loss: 2.460585117340088
Validation loss: 2.468441940123035

Epoch: 6| Step: 4
Training loss: 3.002734899520874
Validation loss: 2.4658688678536365

Epoch: 6| Step: 5
Training loss: 2.3713200092315674
Validation loss: 2.4673964977264404

Epoch: 6| Step: 6
Training loss: 2.4730374813079834
Validation loss: 2.462415841317946

Epoch: 6| Step: 7
Training loss: 2.8585093021392822
Validation loss: 2.4650025367736816

Epoch: 6| Step: 8
Training loss: 2.5188210010528564
Validation loss: 2.4651386712187078

Epoch: 6| Step: 9
Training loss: 3.0255861282348633
Validation loss: 2.4719561146151636

Epoch: 6| Step: 10
Training loss: 2.582247734069824
Validation loss: 2.474609964637346

Epoch: 6| Step: 11
Training loss: 2.5906996726989746
Validation loss: 2.4796219974435787

Epoch: 6| Step: 12
Training loss: 2.8411710262298584
Validation loss: 2.484491240593695

Epoch: 6| Step: 13
Training loss: 2.744558572769165
Validation loss: 2.4793497746990574

Epoch: 170| Step: 0
Training loss: 3.1379714012145996
Validation loss: 2.473041014004779

Epoch: 6| Step: 1
Training loss: 2.5115015506744385
Validation loss: 2.466757079606415

Epoch: 6| Step: 2
Training loss: 2.9661383628845215
Validation loss: 2.46206267674764

Epoch: 6| Step: 3
Training loss: 3.2107439041137695
Validation loss: 2.457134487808392

Epoch: 6| Step: 4
Training loss: 3.378483772277832
Validation loss: 2.4567976638834965

Epoch: 6| Step: 5
Training loss: 3.014232635498047
Validation loss: 2.457011035693589

Epoch: 6| Step: 6
Training loss: 2.267029285430908
Validation loss: 2.4550312667764644

Epoch: 6| Step: 7
Training loss: 2.50240421295166
Validation loss: 2.458408991495768

Epoch: 6| Step: 8
Training loss: 2.402146816253662
Validation loss: 2.4589377731405277

Epoch: 6| Step: 9
Training loss: 2.2007832527160645
Validation loss: 2.457267407448061

Epoch: 6| Step: 10
Training loss: 1.9389246702194214
Validation loss: 2.4601059395779847

Epoch: 6| Step: 11
Training loss: 2.86318302154541
Validation loss: 2.454701026280721

Epoch: 6| Step: 12
Training loss: 2.6749114990234375
Validation loss: 2.4580158982225644

Epoch: 6| Step: 13
Training loss: 2.060822010040283
Validation loss: 2.4610814843126523

Epoch: 171| Step: 0
Training loss: 1.7553915977478027
Validation loss: 2.456637385070965

Epoch: 6| Step: 1
Training loss: 2.869093179702759
Validation loss: 2.4613414092730452

Epoch: 6| Step: 2
Training loss: 2.5098204612731934
Validation loss: 2.4650645973861858

Epoch: 6| Step: 3
Training loss: 2.7960071563720703
Validation loss: 2.466059661680652

Epoch: 6| Step: 4
Training loss: 2.916886806488037
Validation loss: 2.46439270306659

Epoch: 6| Step: 5
Training loss: 3.1972193717956543
Validation loss: 2.4665460868548323

Epoch: 6| Step: 6
Training loss: 2.8869569301605225
Validation loss: 2.473112029414023

Epoch: 6| Step: 7
Training loss: 2.6174991130828857
Validation loss: 2.470079775779478

Epoch: 6| Step: 8
Training loss: 2.6740097999572754
Validation loss: 2.4791790144417876

Epoch: 6| Step: 9
Training loss: 2.3743090629577637
Validation loss: 2.4771933504330215

Epoch: 6| Step: 10
Training loss: 2.4058499336242676
Validation loss: 2.4719748086826776

Epoch: 6| Step: 11
Training loss: 3.3336663246154785
Validation loss: 2.4710297661442913

Epoch: 6| Step: 12
Training loss: 2.6309940814971924
Validation loss: 2.4674261000848587

Epoch: 6| Step: 13
Training loss: 1.8894144296646118
Validation loss: 2.46251001152941

Epoch: 172| Step: 0
Training loss: 2.6293296813964844
Validation loss: 2.4679821127204487

Epoch: 6| Step: 1
Training loss: 2.766378164291382
Validation loss: 2.46266294294788

Epoch: 6| Step: 2
Training loss: 2.7503349781036377
Validation loss: 2.468046654937088

Epoch: 6| Step: 3
Training loss: 3.4293689727783203
Validation loss: 2.4661922506106797

Epoch: 6| Step: 4
Training loss: 2.5415356159210205
Validation loss: 2.465827844476187

Epoch: 6| Step: 5
Training loss: 2.981910467147827
Validation loss: 2.463856771428098

Epoch: 6| Step: 6
Training loss: 3.3847837448120117
Validation loss: 2.4611386791352303

Epoch: 6| Step: 7
Training loss: 1.77437424659729
Validation loss: 2.4670304406073784

Epoch: 6| Step: 8
Training loss: 2.7700493335723877
Validation loss: 2.464159188732024

Epoch: 6| Step: 9
Training loss: 1.787498950958252
Validation loss: 2.4624988494380826

Epoch: 6| Step: 10
Training loss: 2.39296555519104
Validation loss: 2.461427096397646

Epoch: 6| Step: 11
Training loss: 2.4677822589874268
Validation loss: 2.4607009451876403

Epoch: 6| Step: 12
Training loss: 2.7199435234069824
Validation loss: 2.458453780861311

Epoch: 6| Step: 13
Training loss: 2.776486396789551
Validation loss: 2.4613120555877686

Epoch: 173| Step: 0
Training loss: 1.8935234546661377
Validation loss: 2.467222456009157

Epoch: 6| Step: 1
Training loss: 2.847215175628662
Validation loss: 2.4716868528755764

Epoch: 6| Step: 2
Training loss: 3.181967258453369
Validation loss: 2.482784927532237

Epoch: 6| Step: 3
Training loss: 3.0429608821868896
Validation loss: 2.484522137590634

Epoch: 6| Step: 4
Training loss: 3.0189027786254883
Validation loss: 2.4810350069435696

Epoch: 6| Step: 5
Training loss: 2.235598087310791
Validation loss: 2.476562794818673

Epoch: 6| Step: 6
Training loss: 1.9594573974609375
Validation loss: 2.465796883388232

Epoch: 6| Step: 7
Training loss: 2.6886239051818848
Validation loss: 2.4624455103310208

Epoch: 6| Step: 8
Training loss: 3.4866766929626465
Validation loss: 2.46281668191315

Epoch: 6| Step: 9
Training loss: 2.730491876602173
Validation loss: 2.4645811588533464

Epoch: 6| Step: 10
Training loss: 2.624965190887451
Validation loss: 2.464082384622225

Epoch: 6| Step: 11
Training loss: 2.51963472366333
Validation loss: 2.468225158670897

Epoch: 6| Step: 12
Training loss: 2.111827850341797
Validation loss: 2.470185782319756

Epoch: 6| Step: 13
Training loss: 2.916383981704712
Validation loss: 2.472128104138118

Epoch: 174| Step: 0
Training loss: 2.7251486778259277
Validation loss: 2.456098264263522

Epoch: 6| Step: 1
Training loss: 1.9735536575317383
Validation loss: 2.4514986673990884

Epoch: 6| Step: 2
Training loss: 3.2205920219421387
Validation loss: 2.456952674414522

Epoch: 6| Step: 3
Training loss: 2.1821908950805664
Validation loss: 2.449831985658215

Epoch: 6| Step: 4
Training loss: 2.899167537689209
Validation loss: 2.4495511170356505

Epoch: 6| Step: 5
Training loss: 2.129845142364502
Validation loss: 2.44661101987285

Epoch: 6| Step: 6
Training loss: 3.014975070953369
Validation loss: 2.4511926840710383

Epoch: 6| Step: 7
Training loss: 2.4281060695648193
Validation loss: 2.4502189441393782

Epoch: 6| Step: 8
Training loss: 2.977214813232422
Validation loss: 2.4589040587025304

Epoch: 6| Step: 9
Training loss: 3.319333553314209
Validation loss: 2.464751628137404

Epoch: 6| Step: 10
Training loss: 2.613196849822998
Validation loss: 2.4695257653472242

Epoch: 6| Step: 11
Training loss: 2.85001802444458
Validation loss: 2.472964873877905

Epoch: 6| Step: 12
Training loss: 2.3633413314819336
Validation loss: 2.472500967723067

Epoch: 6| Step: 13
Training loss: 2.3398585319519043
Validation loss: 2.4778299280392226

Epoch: 175| Step: 0
Training loss: 2.8474221229553223
Validation loss: 2.4748250053774927

Epoch: 6| Step: 1
Training loss: 2.3884005546569824
Validation loss: 2.477804009632398

Epoch: 6| Step: 2
Training loss: 2.2890543937683105
Validation loss: 2.469642064904654

Epoch: 6| Step: 3
Training loss: 2.640629529953003
Validation loss: 2.4657885541198072

Epoch: 6| Step: 4
Training loss: 2.844036340713501
Validation loss: 2.455945819936773

Epoch: 6| Step: 5
Training loss: 2.5101189613342285
Validation loss: 2.4522381726131646

Epoch: 6| Step: 6
Training loss: 2.938157081604004
Validation loss: 2.447406158652357

Epoch: 6| Step: 7
Training loss: 3.0496678352355957
Validation loss: 2.4476494737850722

Epoch: 6| Step: 8
Training loss: 3.1071794033050537
Validation loss: 2.4469662199738207

Epoch: 6| Step: 9
Training loss: 2.740433692932129
Validation loss: 2.454156373136787

Epoch: 6| Step: 10
Training loss: 2.8449668884277344
Validation loss: 2.4526405539563907

Epoch: 6| Step: 11
Training loss: 1.8513591289520264
Validation loss: 2.449890854538128

Epoch: 6| Step: 12
Training loss: 2.830397844314575
Validation loss: 2.453733023776803

Epoch: 6| Step: 13
Training loss: 2.0911550521850586
Validation loss: 2.4520068117367324

Epoch: 176| Step: 0
Training loss: 3.1701455116271973
Validation loss: 2.4575286911379908

Epoch: 6| Step: 1
Training loss: 2.8793294429779053
Validation loss: 2.4663999670295307

Epoch: 6| Step: 2
Training loss: 3.2164626121520996
Validation loss: 2.4625522910907702

Epoch: 6| Step: 3
Training loss: 2.0627849102020264
Validation loss: 2.46079255688575

Epoch: 6| Step: 4
Training loss: 2.477299451828003
Validation loss: 2.451804781472811

Epoch: 6| Step: 5
Training loss: 2.3381571769714355
Validation loss: 2.4625006516774497

Epoch: 6| Step: 6
Training loss: 2.7370591163635254
Validation loss: 2.4693253988860757

Epoch: 6| Step: 7
Training loss: 2.5039584636688232
Validation loss: 2.4685645052181777

Epoch: 6| Step: 8
Training loss: 2.736189603805542
Validation loss: 2.464837207589098

Epoch: 6| Step: 9
Training loss: 2.5403690338134766
Validation loss: 2.464501221974691

Epoch: 6| Step: 10
Training loss: 2.088120937347412
Validation loss: 2.463116112575736

Epoch: 6| Step: 11
Training loss: 3.0687241554260254
Validation loss: 2.4584999033199844

Epoch: 6| Step: 12
Training loss: 3.105649471282959
Validation loss: 2.463049401519119

Epoch: 6| Step: 13
Training loss: 1.9044263362884521
Validation loss: 2.4726952660468315

Epoch: 177| Step: 0
Training loss: 2.4802539348602295
Validation loss: 2.4675255642142346

Epoch: 6| Step: 1
Training loss: 2.2697560787200928
Validation loss: 2.465798497200012

Epoch: 6| Step: 2
Training loss: 2.390444040298462
Validation loss: 2.4630393135932183

Epoch: 6| Step: 3
Training loss: 2.6504225730895996
Validation loss: 2.461005062185308

Epoch: 6| Step: 4
Training loss: 2.4433655738830566
Validation loss: 2.456589893628192

Epoch: 6| Step: 5
Training loss: 2.299996852874756
Validation loss: 2.457211193218026

Epoch: 6| Step: 6
Training loss: 3.028658866882324
Validation loss: 2.45737781575931

Epoch: 6| Step: 7
Training loss: 3.2539305686950684
Validation loss: 2.4504243122634066

Epoch: 6| Step: 8
Training loss: 2.6212306022644043
Validation loss: 2.4517687546309603

Epoch: 6| Step: 9
Training loss: 2.190382719039917
Validation loss: 2.4520301767574844

Epoch: 6| Step: 10
Training loss: 3.3625636100769043
Validation loss: 2.4480207350946244

Epoch: 6| Step: 11
Training loss: 1.9531701803207397
Validation loss: 2.4503806739725094

Epoch: 6| Step: 12
Training loss: 3.1246485710144043
Validation loss: 2.4474656043514127

Epoch: 6| Step: 13
Training loss: 3.369868755340576
Validation loss: 2.449428132785264

Epoch: 178| Step: 0
Training loss: 3.1729092597961426
Validation loss: 2.4530097771716375

Epoch: 6| Step: 1
Training loss: 2.1719563007354736
Validation loss: 2.4542672634124756

Epoch: 6| Step: 2
Training loss: 3.3542518615722656
Validation loss: 2.4562609144436416

Epoch: 6| Step: 3
Training loss: 3.120854139328003
Validation loss: 2.454808344123184

Epoch: 6| Step: 4
Training loss: 2.784522294998169
Validation loss: 2.456983191992647

Epoch: 6| Step: 5
Training loss: 2.6292357444763184
Validation loss: 2.459790042651597

Epoch: 6| Step: 6
Training loss: 2.7806129455566406
Validation loss: 2.453061270457442

Epoch: 6| Step: 7
Training loss: 2.8614635467529297
Validation loss: 2.4515152054448284

Epoch: 6| Step: 8
Training loss: 1.5744675397872925
Validation loss: 2.4534370591563563

Epoch: 6| Step: 9
Training loss: 2.56093168258667
Validation loss: 2.449455153557562

Epoch: 6| Step: 10
Training loss: 1.8767516613006592
Validation loss: 2.4494095130633284

Epoch: 6| Step: 11
Training loss: 3.0764057636260986
Validation loss: 2.4457104180448797

Epoch: 6| Step: 12
Training loss: 2.902458906173706
Validation loss: 2.438013950983683

Epoch: 6| Step: 13
Training loss: 1.9579582214355469
Validation loss: 2.4399231582559566

Epoch: 179| Step: 0
Training loss: 3.0882372856140137
Validation loss: 2.4385881372677383

Epoch: 6| Step: 1
Training loss: 3.50738525390625
Validation loss: 2.439513079581722

Epoch: 6| Step: 2
Training loss: 2.1309738159179688
Validation loss: 2.442840976099814

Epoch: 6| Step: 3
Training loss: 2.6107044219970703
Validation loss: 2.441681577313331

Epoch: 6| Step: 4
Training loss: 3.267216682434082
Validation loss: 2.4453889118727816

Epoch: 6| Step: 5
Training loss: 1.9038546085357666
Validation loss: 2.4455237875702562

Epoch: 6| Step: 6
Training loss: 2.461195468902588
Validation loss: 2.447856782585062

Epoch: 6| Step: 7
Training loss: 2.4400994777679443
Validation loss: 2.446946028740175

Epoch: 6| Step: 8
Training loss: 2.191378116607666
Validation loss: 2.4539571936412523

Epoch: 6| Step: 9
Training loss: 3.072432518005371
Validation loss: 2.446819043928577

Epoch: 6| Step: 10
Training loss: 2.8580801486968994
Validation loss: 2.455612713290799

Epoch: 6| Step: 11
Training loss: 2.616072654724121
Validation loss: 2.4512111089562856

Epoch: 6| Step: 12
Training loss: 2.3287529945373535
Validation loss: 2.4528423534926547

Epoch: 6| Step: 13
Training loss: 2.8457775115966797
Validation loss: 2.441287789293515

Epoch: 180| Step: 0
Training loss: 2.7114651203155518
Validation loss: 2.4451696975256807

Epoch: 6| Step: 1
Training loss: 2.9276866912841797
Validation loss: 2.4486272155597644

Epoch: 6| Step: 2
Training loss: 2.895925521850586
Validation loss: 2.4426711143985873

Epoch: 6| Step: 3
Training loss: 2.054194450378418
Validation loss: 2.440367744814965

Epoch: 6| Step: 4
Training loss: 1.8909059762954712
Validation loss: 2.437675142800936

Epoch: 6| Step: 5
Training loss: 2.28127384185791
Validation loss: 2.4353936436355754

Epoch: 6| Step: 6
Training loss: 3.0510940551757812
Validation loss: 2.4391786693244852

Epoch: 6| Step: 7
Training loss: 2.3414974212646484
Validation loss: 2.438882835449711

Epoch: 6| Step: 8
Training loss: 2.795281410217285
Validation loss: 2.4371829750717326

Epoch: 6| Step: 9
Training loss: 3.0352907180786133
Validation loss: 2.4433926869464178

Epoch: 6| Step: 10
Training loss: 2.5605244636535645
Validation loss: 2.441105168352845

Epoch: 6| Step: 11
Training loss: 2.8578367233276367
Validation loss: 2.440318895924476

Epoch: 6| Step: 12
Training loss: 2.881849527359009
Validation loss: 2.4456256256308606

Epoch: 6| Step: 13
Training loss: 2.951739549636841
Validation loss: 2.44786326859587

Epoch: 181| Step: 0
Training loss: 1.6665128469467163
Validation loss: 2.450183545389483

Epoch: 6| Step: 1
Training loss: 2.7981386184692383
Validation loss: 2.453493438741212

Epoch: 6| Step: 2
Training loss: 2.6306934356689453
Validation loss: 2.456852910339191

Epoch: 6| Step: 3
Training loss: 2.554924488067627
Validation loss: 2.4626732564741567

Epoch: 6| Step: 4
Training loss: 2.770357131958008
Validation loss: 2.456034650084793

Epoch: 6| Step: 5
Training loss: 2.6456148624420166
Validation loss: 2.4685129939868884

Epoch: 6| Step: 6
Training loss: 2.4495625495910645
Validation loss: 2.4665574873647382

Epoch: 6| Step: 7
Training loss: 2.779552936553955
Validation loss: 2.465839575695735

Epoch: 6| Step: 8
Training loss: 3.1646103858947754
Validation loss: 2.4648117044920563

Epoch: 6| Step: 9
Training loss: 2.5558600425720215
Validation loss: 2.465242270500429

Epoch: 6| Step: 10
Training loss: 2.544764518737793
Validation loss: 2.46868634223938

Epoch: 6| Step: 11
Training loss: 2.385908365249634
Validation loss: 2.457966273830783

Epoch: 6| Step: 12
Training loss: 3.3093252182006836
Validation loss: 2.442009961733254

Epoch: 6| Step: 13
Training loss: 2.9849624633789062
Validation loss: 2.4433610952028664

Epoch: 182| Step: 0
Training loss: 1.7390378713607788
Validation loss: 2.4389986914973103

Epoch: 6| Step: 1
Training loss: 3.0595614910125732
Validation loss: 2.438246152734244

Epoch: 6| Step: 2
Training loss: 2.161226272583008
Validation loss: 2.4396355587949037

Epoch: 6| Step: 3
Training loss: 3.120175361633301
Validation loss: 2.435134146803169

Epoch: 6| Step: 4
Training loss: 2.9522557258605957
Validation loss: 2.436985306842353

Epoch: 6| Step: 5
Training loss: 2.3713746070861816
Validation loss: 2.4398741978470997

Epoch: 6| Step: 6
Training loss: 3.396360158920288
Validation loss: 2.4385764445027998

Epoch: 6| Step: 7
Training loss: 2.3961708545684814
Validation loss: 2.4372166074732298

Epoch: 6| Step: 8
Training loss: 2.0972166061401367
Validation loss: 2.43803447164515

Epoch: 6| Step: 9
Training loss: 2.6587228775024414
Validation loss: 2.440576130344022

Epoch: 6| Step: 10
Training loss: 3.6311910152435303
Validation loss: 2.443310099263345

Epoch: 6| Step: 11
Training loss: 2.477353572845459
Validation loss: 2.445895525716966

Epoch: 6| Step: 12
Training loss: 2.627288818359375
Validation loss: 2.453663231224142

Epoch: 6| Step: 13
Training loss: 2.1495368480682373
Validation loss: 2.446937022670623

Epoch: 183| Step: 0
Training loss: 2.4980411529541016
Validation loss: 2.4504523046555056

Epoch: 6| Step: 1
Training loss: 2.647988796234131
Validation loss: 2.4603370786995016

Epoch: 6| Step: 2
Training loss: 1.936582088470459
Validation loss: 2.4511688063221593

Epoch: 6| Step: 3
Training loss: 3.344024419784546
Validation loss: 2.4495340495981197

Epoch: 6| Step: 4
Training loss: 2.9191079139709473
Validation loss: 2.448498220853908

Epoch: 6| Step: 5
Training loss: 2.7584242820739746
Validation loss: 2.44402047639252

Epoch: 6| Step: 6
Training loss: 1.9889849424362183
Validation loss: 2.4452396592786236

Epoch: 6| Step: 7
Training loss: 2.8551998138427734
Validation loss: 2.44623153696778

Epoch: 6| Step: 8
Training loss: 2.7702622413635254
Validation loss: 2.4445937115658998

Epoch: 6| Step: 9
Training loss: 2.7282934188842773
Validation loss: 2.4457949592221166

Epoch: 6| Step: 10
Training loss: 3.167788028717041
Validation loss: 2.4376908886817192

Epoch: 6| Step: 11
Training loss: 2.2763848304748535
Validation loss: 2.435346918721353

Epoch: 6| Step: 12
Training loss: 2.9458200931549072
Validation loss: 2.4374417181937926

Epoch: 6| Step: 13
Training loss: 1.7886918783187866
Validation loss: 2.4369263033713064

Epoch: 184| Step: 0
Training loss: 2.4357471466064453
Validation loss: 2.4327903716794905

Epoch: 6| Step: 1
Training loss: 2.5347485542297363
Validation loss: 2.434577682966827

Epoch: 6| Step: 2
Training loss: 3.4495697021484375
Validation loss: 2.4341386825807634

Epoch: 6| Step: 3
Training loss: 2.9444711208343506
Validation loss: 2.4338638115954656

Epoch: 6| Step: 4
Training loss: 1.615543007850647
Validation loss: 2.4376824901949976

Epoch: 6| Step: 5
Training loss: 3.126580238342285
Validation loss: 2.4374784397822555

Epoch: 6| Step: 6
Training loss: 2.5205636024475098
Validation loss: 2.435261468733511

Epoch: 6| Step: 7
Training loss: 2.0026845932006836
Validation loss: 2.4444012667543147

Epoch: 6| Step: 8
Training loss: 2.726316452026367
Validation loss: 2.4545297468862226

Epoch: 6| Step: 9
Training loss: 3.111158609390259
Validation loss: 2.4494769470666045

Epoch: 6| Step: 10
Training loss: 2.3450849056243896
Validation loss: 2.4600191270151446

Epoch: 6| Step: 11
Training loss: 2.765474319458008
Validation loss: 2.456258535385132

Epoch: 6| Step: 12
Training loss: 3.064732074737549
Validation loss: 2.4510384221230783

Epoch: 6| Step: 13
Training loss: 2.009019136428833
Validation loss: 2.4544510738824004

Epoch: 185| Step: 0
Training loss: 1.9186246395111084
Validation loss: 2.4525226393053607

Epoch: 6| Step: 1
Training loss: 2.437163829803467
Validation loss: 2.4483013793986332

Epoch: 6| Step: 2
Training loss: 2.0941922664642334
Validation loss: 2.456681213071269

Epoch: 6| Step: 3
Training loss: 3.215799331665039
Validation loss: 2.452105317064511

Epoch: 6| Step: 4
Training loss: 2.876300811767578
Validation loss: 2.4522449072971138

Epoch: 6| Step: 5
Training loss: 2.6249237060546875
Validation loss: 2.454836173724103

Epoch: 6| Step: 6
Training loss: 2.1214518547058105
Validation loss: 2.456324459404074

Epoch: 6| Step: 7
Training loss: 2.3181300163269043
Validation loss: 2.4516218913498746

Epoch: 6| Step: 8
Training loss: 2.120647668838501
Validation loss: 2.4491034220623713

Epoch: 6| Step: 9
Training loss: 3.3051669597625732
Validation loss: 2.43887815167827

Epoch: 6| Step: 10
Training loss: 3.1731722354888916
Validation loss: 2.4350844301203245

Epoch: 6| Step: 11
Training loss: 3.1249325275421143
Validation loss: 2.4325002495960524

Epoch: 6| Step: 12
Training loss: 2.9501123428344727
Validation loss: 2.4327155108092935

Epoch: 6| Step: 13
Training loss: 2.792313814163208
Validation loss: 2.4295033152385423

Epoch: 186| Step: 0
Training loss: 2.7921929359436035
Validation loss: 2.4315148168994534

Epoch: 6| Step: 1
Training loss: 2.333150863647461
Validation loss: 2.4306080828430834

Epoch: 6| Step: 2
Training loss: 2.1253349781036377
Validation loss: 2.4313321113586426

Epoch: 6| Step: 3
Training loss: 2.2407360076904297
Validation loss: 2.431765669135637

Epoch: 6| Step: 4
Training loss: 2.283604621887207
Validation loss: 2.4300281283675984

Epoch: 6| Step: 5
Training loss: 2.7409918308258057
Validation loss: 2.429674474141931

Epoch: 6| Step: 6
Training loss: 2.76779842376709
Validation loss: 2.427659265456661

Epoch: 6| Step: 7
Training loss: 3.1602375507354736
Validation loss: 2.426314118087933

Epoch: 6| Step: 8
Training loss: 3.314208507537842
Validation loss: 2.426313433595883

Epoch: 6| Step: 9
Training loss: 2.630323886871338
Validation loss: 2.426970063999135

Epoch: 6| Step: 10
Training loss: 2.0458016395568848
Validation loss: 2.4265326146156556

Epoch: 6| Step: 11
Training loss: 2.746652841567993
Validation loss: 2.4307496983517884

Epoch: 6| Step: 12
Training loss: 2.975759983062744
Validation loss: 2.4321069461043163

Epoch: 6| Step: 13
Training loss: 3.0706605911254883
Validation loss: 2.432885028982675

Epoch: 187| Step: 0
Training loss: 2.684730052947998
Validation loss: 2.440513559567031

Epoch: 6| Step: 1
Training loss: 2.688518524169922
Validation loss: 2.4432210973514024

Epoch: 6| Step: 2
Training loss: 1.4711204767227173
Validation loss: 2.4434812376576085

Epoch: 6| Step: 3
Training loss: 2.324646472930908
Validation loss: 2.4478177844837146

Epoch: 6| Step: 4
Training loss: 3.06889009475708
Validation loss: 2.4518867359366467

Epoch: 6| Step: 5
Training loss: 2.1650404930114746
Validation loss: 2.45692935041202

Epoch: 6| Step: 6
Training loss: 2.9936728477478027
Validation loss: 2.4673262642275904

Epoch: 6| Step: 7
Training loss: 2.6591076850891113
Validation loss: 2.4679234335499425

Epoch: 6| Step: 8
Training loss: 3.072936773300171
Validation loss: 2.4661069172684864

Epoch: 6| Step: 9
Training loss: 3.552440881729126
Validation loss: 2.4589833239073395

Epoch: 6| Step: 10
Training loss: 2.913510322570801
Validation loss: 2.450220784833354

Epoch: 6| Step: 11
Training loss: 2.5151309967041016
Validation loss: 2.4454600529004167

Epoch: 6| Step: 12
Training loss: 2.514832019805908
Validation loss: 2.436355167819608

Epoch: 6| Step: 13
Training loss: 2.067537784576416
Validation loss: 2.438468038394887

Epoch: 188| Step: 0
Training loss: 1.50382399559021
Validation loss: 2.4296600639179187

Epoch: 6| Step: 1
Training loss: 2.5966055393218994
Validation loss: 2.4317738189492175

Epoch: 6| Step: 2
Training loss: 2.189924478530884
Validation loss: 2.4247095610505793

Epoch: 6| Step: 3
Training loss: 3.2876651287078857
Validation loss: 2.4287867674263577

Epoch: 6| Step: 4
Training loss: 2.8101646900177
Validation loss: 2.430938033647435

Epoch: 6| Step: 5
Training loss: 2.7476601600646973
Validation loss: 2.43343428386155

Epoch: 6| Step: 6
Training loss: 3.0729868412017822
Validation loss: 2.4342413897155435

Epoch: 6| Step: 7
Training loss: 2.4509334564208984
Validation loss: 2.431991151584092

Epoch: 6| Step: 8
Training loss: 3.1196868419647217
Validation loss: 2.4346060240140526

Epoch: 6| Step: 9
Training loss: 2.605435371398926
Validation loss: 2.43679981077871

Epoch: 6| Step: 10
Training loss: 2.755397081375122
Validation loss: 2.433300810475503

Epoch: 6| Step: 11
Training loss: 2.2511565685272217
Validation loss: 2.4338363806406655

Epoch: 6| Step: 12
Training loss: 2.6101720333099365
Validation loss: 2.431790826141193

Epoch: 6| Step: 13
Training loss: 3.1705896854400635
Validation loss: 2.434493572481217

Epoch: 189| Step: 0
Training loss: 3.2514917850494385
Validation loss: 2.4309107154928227

Epoch: 6| Step: 1
Training loss: 1.9761583805084229
Validation loss: 2.420379759162985

Epoch: 6| Step: 2
Training loss: 1.4068293571472168
Validation loss: 2.429980158805847

Epoch: 6| Step: 3
Training loss: 2.589210033416748
Validation loss: 2.4301320019588677

Epoch: 6| Step: 4
Training loss: 2.796889066696167
Validation loss: 2.4297862181099514

Epoch: 6| Step: 5
Training loss: 2.6655983924865723
Validation loss: 2.436605335563742

Epoch: 6| Step: 6
Training loss: 3.021451473236084
Validation loss: 2.4454857687796316

Epoch: 6| Step: 7
Training loss: 2.363206148147583
Validation loss: 2.445173155876898

Epoch: 6| Step: 8
Training loss: 2.6619255542755127
Validation loss: 2.4549598322119763

Epoch: 6| Step: 9
Training loss: 3.4451489448547363
Validation loss: 2.4528582993374077

Epoch: 6| Step: 10
Training loss: 2.8729147911071777
Validation loss: 2.452197882436937

Epoch: 6| Step: 11
Training loss: 2.996962547302246
Validation loss: 2.4439785916318177

Epoch: 6| Step: 12
Training loss: 2.214197874069214
Validation loss: 2.4376025789527485

Epoch: 6| Step: 13
Training loss: 2.6832926273345947
Validation loss: 2.4358932356680594

Epoch: 190| Step: 0
Training loss: 1.7559294700622559
Validation loss: 2.433579901213287

Epoch: 6| Step: 1
Training loss: 2.7072486877441406
Validation loss: 2.4301197862112396

Epoch: 6| Step: 2
Training loss: 2.2898788452148438
Validation loss: 2.432864501912107

Epoch: 6| Step: 3
Training loss: 2.7379627227783203
Validation loss: 2.4293894075578257

Epoch: 6| Step: 4
Training loss: 2.447519302368164
Validation loss: 2.433901886786184

Epoch: 6| Step: 5
Training loss: 2.766068935394287
Validation loss: 2.4358359664999027

Epoch: 6| Step: 6
Training loss: 3.36160945892334
Validation loss: 2.430022975449921

Epoch: 6| Step: 7
Training loss: 2.1902213096618652
Validation loss: 2.425461123066564

Epoch: 6| Step: 8
Training loss: 2.7677996158599854
Validation loss: 2.429458107999576

Epoch: 6| Step: 9
Training loss: 2.2520179748535156
Validation loss: 2.4305914525062806

Epoch: 6| Step: 10
Training loss: 3.8146684169769287
Validation loss: 2.4325072842259563

Epoch: 6| Step: 11
Training loss: 2.20511531829834
Validation loss: 2.4345856199982348

Epoch: 6| Step: 12
Training loss: 3.095731735229492
Validation loss: 2.435716285500475

Epoch: 6| Step: 13
Training loss: 2.391585350036621
Validation loss: 2.447806466010309

Epoch: 191| Step: 0
Training loss: 2.608502149581909
Validation loss: 2.4624295029588925

Epoch: 6| Step: 1
Training loss: 2.5736565589904785
Validation loss: 2.48836673203335

Epoch: 6| Step: 2
Training loss: 3.2798447608947754
Validation loss: 2.5092626540891585

Epoch: 6| Step: 3
Training loss: 2.22839093208313
Validation loss: 2.517819404602051

Epoch: 6| Step: 4
Training loss: 2.242959499359131
Validation loss: 2.4956727104802288

Epoch: 6| Step: 5
Training loss: 2.510847568511963
Validation loss: 2.4845245653583157

Epoch: 6| Step: 6
Training loss: 2.5058977603912354
Validation loss: 2.478495746530512

Epoch: 6| Step: 7
Training loss: 2.7922253608703613
Validation loss: 2.466949898709533

Epoch: 6| Step: 8
Training loss: 1.5040309429168701
Validation loss: 2.446047298369869

Epoch: 6| Step: 9
Training loss: 2.645833730697632
Validation loss: 2.4359848140388407

Epoch: 6| Step: 10
Training loss: 3.00083065032959
Validation loss: 2.434964700411725

Epoch: 6| Step: 11
Training loss: 2.847019910812378
Validation loss: 2.4291476870095856

Epoch: 6| Step: 12
Training loss: 3.4033660888671875
Validation loss: 2.420416437169557

Epoch: 6| Step: 13
Training loss: 2.8179304599761963
Validation loss: 2.4220943527836956

Epoch: 192| Step: 0
Training loss: 2.9318861961364746
Validation loss: 2.4265075704102874

Epoch: 6| Step: 1
Training loss: 2.343871831893921
Validation loss: 2.4314573913492183

Epoch: 6| Step: 2
Training loss: 2.707801580429077
Validation loss: 2.423399866268199

Epoch: 6| Step: 3
Training loss: 2.701604127883911
Validation loss: 2.4242119378941034

Epoch: 6| Step: 4
Training loss: 2.44938325881958
Validation loss: 2.4213442494792323

Epoch: 6| Step: 5
Training loss: 2.6002910137176514
Validation loss: 2.4233054268744683

Epoch: 6| Step: 6
Training loss: 2.6781206130981445
Validation loss: 2.4193881198924077

Epoch: 6| Step: 7
Training loss: 2.5435407161712646
Validation loss: 2.4189827570351223

Epoch: 6| Step: 8
Training loss: 2.626208782196045
Validation loss: 2.419495808180942

Epoch: 6| Step: 9
Training loss: 2.838472366333008
Validation loss: 2.4173288999065274

Epoch: 6| Step: 10
Training loss: 2.0814456939697266
Validation loss: 2.418054667852258

Epoch: 6| Step: 11
Training loss: 2.359714984893799
Validation loss: 2.4168822098803777

Epoch: 6| Step: 12
Training loss: 3.9751198291778564
Validation loss: 2.4211145318964475

Epoch: 6| Step: 13
Training loss: 1.9654653072357178
Validation loss: 2.4177422933681036

Epoch: 193| Step: 0
Training loss: 2.9021694660186768
Validation loss: 2.4205748675971903

Epoch: 6| Step: 1
Training loss: 2.752976417541504
Validation loss: 2.420383322623468

Epoch: 6| Step: 2
Training loss: 2.7356581687927246
Validation loss: 2.424404364760204

Epoch: 6| Step: 3
Training loss: 2.5332603454589844
Validation loss: 2.4276936772049114

Epoch: 6| Step: 4
Training loss: 2.274141788482666
Validation loss: 2.4261298333444903

Epoch: 6| Step: 5
Training loss: 2.690459966659546
Validation loss: 2.4281779437936764

Epoch: 6| Step: 6
Training loss: 2.797802209854126
Validation loss: 2.4275535178440872

Epoch: 6| Step: 7
Training loss: 1.87729811668396
Validation loss: 2.4299629836954098

Epoch: 6| Step: 8
Training loss: 3.02163028717041
Validation loss: 2.434016168758433

Epoch: 6| Step: 9
Training loss: 2.715620994567871
Validation loss: 2.437808862296484

Epoch: 6| Step: 10
Training loss: 2.7676451206207275
Validation loss: 2.4348637596253426

Epoch: 6| Step: 11
Training loss: 2.823298931121826
Validation loss: 2.4424392638667936

Epoch: 6| Step: 12
Training loss: 1.927907943725586
Validation loss: 2.4429754082874586

Epoch: 6| Step: 13
Training loss: 3.3021011352539062
Validation loss: 2.4507867905401413

Epoch: 194| Step: 0
Training loss: 3.2359209060668945
Validation loss: 2.44389683200467

Epoch: 6| Step: 1
Training loss: 2.012455463409424
Validation loss: 2.433365814147457

Epoch: 6| Step: 2
Training loss: 2.89280104637146
Validation loss: 2.425120319089582

Epoch: 6| Step: 3
Training loss: 3.1423749923706055
Validation loss: 2.4251165954015588

Epoch: 6| Step: 4
Training loss: 2.7268967628479004
Validation loss: 2.425427721392724

Epoch: 6| Step: 5
Training loss: 2.7556161880493164
Validation loss: 2.4242158551369943

Epoch: 6| Step: 6
Training loss: 3.0764565467834473
Validation loss: 2.4185283542961202

Epoch: 6| Step: 7
Training loss: 2.9415130615234375
Validation loss: 2.417463799958588

Epoch: 6| Step: 8
Training loss: 2.375718116760254
Validation loss: 2.4147720683005547

Epoch: 6| Step: 9
Training loss: 2.2653751373291016
Validation loss: 2.4150303999582925

Epoch: 6| Step: 10
Training loss: 2.9852852821350098
Validation loss: 2.4162998071280857

Epoch: 6| Step: 11
Training loss: 1.954477310180664
Validation loss: 2.4115257545184066

Epoch: 6| Step: 12
Training loss: 2.1345276832580566
Validation loss: 2.4134846887280865

Epoch: 6| Step: 13
Training loss: 2.1700284481048584
Validation loss: 2.4166222233926096

Epoch: 195| Step: 0
Training loss: 2.4447951316833496
Validation loss: 2.415495213641915

Epoch: 6| Step: 1
Training loss: 2.2331814765930176
Validation loss: 2.4163917700449624

Epoch: 6| Step: 2
Training loss: 4.03051233291626
Validation loss: 2.419084348986226

Epoch: 6| Step: 3
Training loss: 2.624460220336914
Validation loss: 2.42244234905448

Epoch: 6| Step: 4
Training loss: 2.664459228515625
Validation loss: 2.4236265510641117

Epoch: 6| Step: 5
Training loss: 1.7266852855682373
Validation loss: 2.4255482599299443

Epoch: 6| Step: 6
Training loss: 3.191927909851074
Validation loss: 2.4225594920496785

Epoch: 6| Step: 7
Training loss: 2.6715097427368164
Validation loss: 2.429291814886114

Epoch: 6| Step: 8
Training loss: 2.7924070358276367
Validation loss: 2.4292056714334795

Epoch: 6| Step: 9
Training loss: 3.0057244300842285
Validation loss: 2.425589669135309

Epoch: 6| Step: 10
Training loss: 2.5527091026306152
Validation loss: 2.4330773481758694

Epoch: 6| Step: 11
Training loss: 2.2892706394195557
Validation loss: 2.431950392261628

Epoch: 6| Step: 12
Training loss: 2.3541946411132812
Validation loss: 2.4326400705563125

Epoch: 6| Step: 13
Training loss: 1.8368265628814697
Validation loss: 2.4415470195072952

Epoch: 196| Step: 0
Training loss: 2.830742359161377
Validation loss: 2.4479882819678194

Epoch: 6| Step: 1
Training loss: 2.5699448585510254
Validation loss: 2.453377431438815

Epoch: 6| Step: 2
Training loss: 2.3017959594726562
Validation loss: 2.4649896314067226

Epoch: 6| Step: 3
Training loss: 2.4226155281066895
Validation loss: 2.46188296041181

Epoch: 6| Step: 4
Training loss: 2.7086830139160156
Validation loss: 2.4713229235782417

Epoch: 6| Step: 5
Training loss: 3.3562145233154297
Validation loss: 2.470606355256932

Epoch: 6| Step: 6
Training loss: 3.381563663482666
Validation loss: 2.4719205620468303

Epoch: 6| Step: 7
Training loss: 2.512890338897705
Validation loss: 2.454220933298911

Epoch: 6| Step: 8
Training loss: 2.3755860328674316
Validation loss: 2.4304564281176497

Epoch: 6| Step: 9
Training loss: 3.2697510719299316
Validation loss: 2.4251124551219325

Epoch: 6| Step: 10
Training loss: 2.2542786598205566
Validation loss: 2.4156964991682317

Epoch: 6| Step: 11
Training loss: 2.263731002807617
Validation loss: 2.4170552812596804

Epoch: 6| Step: 12
Training loss: 2.1769495010375977
Validation loss: 2.42314955624201

Epoch: 6| Step: 13
Training loss: 2.4166107177734375
Validation loss: 2.4204740934474493

Epoch: 197| Step: 0
Training loss: 1.9207024574279785
Validation loss: 2.419437846829814

Epoch: 6| Step: 1
Training loss: 2.826819896697998
Validation loss: 2.4228716511880197

Epoch: 6| Step: 2
Training loss: 1.812467098236084
Validation loss: 2.4238178755647395

Epoch: 6| Step: 3
Training loss: 3.2154717445373535
Validation loss: 2.4208266888895342

Epoch: 6| Step: 4
Training loss: 2.679466724395752
Validation loss: 2.422462107032858

Epoch: 6| Step: 5
Training loss: 3.171297073364258
Validation loss: 2.424677074596446

Epoch: 6| Step: 6
Training loss: 2.868112087249756
Validation loss: 2.418239373032765

Epoch: 6| Step: 7
Training loss: 2.88258695602417
Validation loss: 2.41522107842148

Epoch: 6| Step: 8
Training loss: 2.547604560852051
Validation loss: 2.4145022053872385

Epoch: 6| Step: 9
Training loss: 3.0975937843322754
Validation loss: 2.4158328835682203

Epoch: 6| Step: 10
Training loss: 2.136192798614502
Validation loss: 2.4206872678572133

Epoch: 6| Step: 11
Training loss: 3.1329121589660645
Validation loss: 2.4182352994077947

Epoch: 6| Step: 12
Training loss: 2.5367016792297363
Validation loss: 2.425794034875849

Epoch: 6| Step: 13
Training loss: 1.575657606124878
Validation loss: 2.424567522541169

Epoch: 198| Step: 0
Training loss: 2.726945638656616
Validation loss: 2.4258161693490963

Epoch: 6| Step: 1
Training loss: 2.416184186935425
Validation loss: 2.434765021006266

Epoch: 6| Step: 2
Training loss: 2.444413423538208
Validation loss: 2.4373997360147457

Epoch: 6| Step: 3
Training loss: 1.9409892559051514
Validation loss: 2.445253510628977

Epoch: 6| Step: 4
Training loss: 2.7763473987579346
Validation loss: 2.4562144792208107

Epoch: 6| Step: 5
Training loss: 2.2709403038024902
Validation loss: 2.4572590140886206

Epoch: 6| Step: 6
Training loss: 2.89644193649292
Validation loss: 2.467128517807171

Epoch: 6| Step: 7
Training loss: 2.545280933380127
Validation loss: 2.455543236065936

Epoch: 6| Step: 8
Training loss: 2.6792244911193848
Validation loss: 2.4530632521516536

Epoch: 6| Step: 9
Training loss: 3.7692768573760986
Validation loss: 2.4515742332704606

Epoch: 6| Step: 10
Training loss: 2.526482582092285
Validation loss: 2.440680939664123

Epoch: 6| Step: 11
Training loss: 3.0251803398132324
Validation loss: 2.4278641926345004

Epoch: 6| Step: 12
Training loss: 2.467238426208496
Validation loss: 2.424599321939612

Epoch: 6| Step: 13
Training loss: 2.1158323287963867
Validation loss: 2.412898458460326

Epoch: 199| Step: 0
Training loss: 1.7891767024993896
Validation loss: 2.414915818040089

Epoch: 6| Step: 1
Training loss: 2.765022039413452
Validation loss: 2.416662517414298

Epoch: 6| Step: 2
Training loss: 2.2790749073028564
Validation loss: 2.41327848229357

Epoch: 6| Step: 3
Training loss: 2.191551923751831
Validation loss: 2.415025488022835

Epoch: 6| Step: 4
Training loss: 2.580913782119751
Validation loss: 2.415216494632024

Epoch: 6| Step: 5
Training loss: 3.3427748680114746
Validation loss: 2.416459968013148

Epoch: 6| Step: 6
Training loss: 2.423062801361084
Validation loss: 2.4319547760871147

Epoch: 6| Step: 7
Training loss: 2.7449264526367188
Validation loss: 2.4398569112182944

Epoch: 6| Step: 8
Training loss: 3.218191623687744
Validation loss: 2.4321449879677064

Epoch: 6| Step: 9
Training loss: 2.913848638534546
Validation loss: 2.4294298592434136

Epoch: 6| Step: 10
Training loss: 2.3611204624176025
Validation loss: 2.4321382225200696

Epoch: 6| Step: 11
Training loss: 3.022157669067383
Validation loss: 2.429506717189666

Epoch: 6| Step: 12
Training loss: 2.5579428672790527
Validation loss: 2.4195897989375617

Epoch: 6| Step: 13
Training loss: 2.6692349910736084
Validation loss: 2.416860939354025

Epoch: 200| Step: 0
Training loss: 2.1924211978912354
Validation loss: 2.410625324454359

Epoch: 6| Step: 1
Training loss: 2.4215617179870605
Validation loss: 2.409192645421592

Epoch: 6| Step: 2
Training loss: 2.6384081840515137
Validation loss: 2.4048730404146257

Epoch: 6| Step: 3
Training loss: 2.5791380405426025
Validation loss: 2.4092366259585143

Epoch: 6| Step: 4
Training loss: 2.4499053955078125
Validation loss: 2.4067793661548245

Epoch: 6| Step: 5
Training loss: 2.744201898574829
Validation loss: 2.410446120846656

Epoch: 6| Step: 6
Training loss: 2.7006328105926514
Validation loss: 2.41126916485448

Epoch: 6| Step: 7
Training loss: 2.4658823013305664
Validation loss: 2.4085916703747166

Epoch: 6| Step: 8
Training loss: 2.321115493774414
Validation loss: 2.4116004359337593

Epoch: 6| Step: 9
Training loss: 3.505427837371826
Validation loss: 2.4176498805322955

Epoch: 6| Step: 10
Training loss: 3.2942309379577637
Validation loss: 2.4133463495521137

Epoch: 6| Step: 11
Training loss: 2.7427942752838135
Validation loss: 2.4182904868997555

Epoch: 6| Step: 12
Training loss: 2.4710774421691895
Validation loss: 2.413889636275589

Epoch: 6| Step: 13
Training loss: 1.8770427703857422
Validation loss: 2.4108705418084257

Epoch: 201| Step: 0
Training loss: 2.550631046295166
Validation loss: 2.4269380082366285

Epoch: 6| Step: 1
Training loss: 2.504776954650879
Validation loss: 2.4348919878723803

Epoch: 6| Step: 2
Training loss: 3.569606304168701
Validation loss: 2.467294180265037

Epoch: 6| Step: 3
Training loss: 2.8646273612976074
Validation loss: 2.466990804159513

Epoch: 6| Step: 4
Training loss: 2.423546075820923
Validation loss: 2.4540586471557617

Epoch: 6| Step: 5
Training loss: 2.310016632080078
Validation loss: 2.4359517123109553

Epoch: 6| Step: 6
Training loss: 2.6101958751678467
Validation loss: 2.4235657440718783

Epoch: 6| Step: 7
Training loss: 2.4018714427948
Validation loss: 2.418088597636069

Epoch: 6| Step: 8
Training loss: 3.4326043128967285
Validation loss: 2.4155283615153325

Epoch: 6| Step: 9
Training loss: 2.629622459411621
Validation loss: 2.4157995793127243

Epoch: 6| Step: 10
Training loss: 1.9756146669387817
Validation loss: 2.4156958774853776

Epoch: 6| Step: 11
Training loss: 2.3498902320861816
Validation loss: 2.422736299935208

Epoch: 6| Step: 12
Training loss: 2.419312000274658
Validation loss: 2.4180412805208595

Epoch: 6| Step: 13
Training loss: 2.7989377975463867
Validation loss: 2.4230452019681215

Epoch: 202| Step: 0
Training loss: 2.514906883239746
Validation loss: 2.4259831572091706

Epoch: 6| Step: 1
Training loss: 2.6658387184143066
Validation loss: 2.4267350909530476

Epoch: 6| Step: 2
Training loss: 1.8353712558746338
Validation loss: 2.426479088362827

Epoch: 6| Step: 3
Training loss: 2.3278286457061768
Validation loss: 2.4225016716987855

Epoch: 6| Step: 4
Training loss: 2.4147753715515137
Validation loss: 2.4158255336105183

Epoch: 6| Step: 5
Training loss: 2.9624009132385254
Validation loss: 2.4163400588497037

Epoch: 6| Step: 6
Training loss: 2.742316246032715
Validation loss: 2.4114934218827115

Epoch: 6| Step: 7
Training loss: 2.4240503311157227
Validation loss: 2.41275086966894

Epoch: 6| Step: 8
Training loss: 2.677866220474243
Validation loss: 2.4214897489035003

Epoch: 6| Step: 9
Training loss: 2.3189547061920166
Validation loss: 2.414345472089706

Epoch: 6| Step: 10
Training loss: 3.1097512245178223
Validation loss: 2.43279480677779

Epoch: 6| Step: 11
Training loss: 2.4988722801208496
Validation loss: 2.4264082985539592

Epoch: 6| Step: 12
Training loss: 3.654231548309326
Validation loss: 2.4350822638439875

Epoch: 6| Step: 13
Training loss: 2.2598447799682617
Validation loss: 2.432395076238981

Epoch: 203| Step: 0
Training loss: 3.201287269592285
Validation loss: 2.4460831867751254

Epoch: 6| Step: 1
Training loss: 2.7935938835144043
Validation loss: 2.4430611723212787

Epoch: 6| Step: 2
Training loss: 3.061847448348999
Validation loss: 2.452181157245431

Epoch: 6| Step: 3
Training loss: 1.690230131149292
Validation loss: 2.440769051992765

Epoch: 6| Step: 4
Training loss: 2.597726821899414
Validation loss: 2.428037581905242

Epoch: 6| Step: 5
Training loss: 2.2105860710144043
Validation loss: 2.424639763370637

Epoch: 6| Step: 6
Training loss: 2.452633857727051
Validation loss: 2.4300393109680503

Epoch: 6| Step: 7
Training loss: 2.5826468467712402
Validation loss: 2.429474033335204

Epoch: 6| Step: 8
Training loss: 3.006218910217285
Validation loss: 2.417565550855411

Epoch: 6| Step: 9
Training loss: 2.343562364578247
Validation loss: 2.4120195873322023

Epoch: 6| Step: 10
Training loss: 3.5798606872558594
Validation loss: 2.4154371010359896

Epoch: 6| Step: 11
Training loss: 2.4914865493774414
Validation loss: 2.4195026197741107

Epoch: 6| Step: 12
Training loss: 2.027454137802124
Validation loss: 2.4212295701426845

Epoch: 6| Step: 13
Training loss: 2.6201529502868652
Validation loss: 2.416945698440716

Epoch: 204| Step: 0
Training loss: 2.6379356384277344
Validation loss: 2.418139209029495

Epoch: 6| Step: 1
Training loss: 3.179352045059204
Validation loss: 2.413831990252259

Epoch: 6| Step: 2
Training loss: 2.3732190132141113
Validation loss: 2.409351664204751

Epoch: 6| Step: 3
Training loss: 2.665003538131714
Validation loss: 2.4047091596870014

Epoch: 6| Step: 4
Training loss: 2.267129421234131
Validation loss: 2.40301307939714

Epoch: 6| Step: 5
Training loss: 2.395958662033081
Validation loss: 2.4055212672038744

Epoch: 6| Step: 6
Training loss: 2.7725772857666016
Validation loss: 2.4021526741725143

Epoch: 6| Step: 7
Training loss: 2.55891752243042
Validation loss: 2.408889737180484

Epoch: 6| Step: 8
Training loss: 2.401090145111084
Validation loss: 2.4153645448787238

Epoch: 6| Step: 9
Training loss: 2.9839119911193848
Validation loss: 2.4212227688040784

Epoch: 6| Step: 10
Training loss: 2.250514507293701
Validation loss: 2.4285620784246795

Epoch: 6| Step: 11
Training loss: 2.5057809352874756
Validation loss: 2.4519771991237516

Epoch: 6| Step: 12
Training loss: 2.5205397605895996
Validation loss: 2.448328641153151

Epoch: 6| Step: 13
Training loss: 3.5034408569335938
Validation loss: 2.4287570176586026

Epoch: 205| Step: 0
Training loss: 2.4535956382751465
Validation loss: 2.4107729414457917

Epoch: 6| Step: 1
Training loss: 2.889092206954956
Validation loss: 2.4087640777710946

Epoch: 6| Step: 2
Training loss: 2.532526969909668
Validation loss: 2.3999327510915776

Epoch: 6| Step: 3
Training loss: 2.072565793991089
Validation loss: 2.404936364901963

Epoch: 6| Step: 4
Training loss: 2.1743035316467285
Validation loss: 2.4036579311534925

Epoch: 6| Step: 5
Training loss: 3.0171685218811035
Validation loss: 2.400073153998262

Epoch: 6| Step: 6
Training loss: 2.3947360515594482
Validation loss: 2.400588312456685

Epoch: 6| Step: 7
Training loss: 2.5315067768096924
Validation loss: 2.3972718484940065

Epoch: 6| Step: 8
Training loss: 2.8916335105895996
Validation loss: 2.3999953833959435

Epoch: 6| Step: 9
Training loss: 2.823716402053833
Validation loss: 2.400788837863553

Epoch: 6| Step: 10
Training loss: 2.5397067070007324
Validation loss: 2.401763995488485

Epoch: 6| Step: 11
Training loss: 3.2380266189575195
Validation loss: 2.4105095042977283

Epoch: 6| Step: 12
Training loss: 2.602837562561035
Validation loss: 2.404563569253491

Epoch: 6| Step: 13
Training loss: 2.4485867023468018
Validation loss: 2.4108718800288376

Epoch: 206| Step: 0
Training loss: 2.4524707794189453
Validation loss: 2.414166445373207

Epoch: 6| Step: 1
Training loss: 2.541586399078369
Validation loss: 2.4062456161745134

Epoch: 6| Step: 2
Training loss: 2.5594444274902344
Validation loss: 2.4044644114791707

Epoch: 6| Step: 3
Training loss: 2.2764551639556885
Validation loss: 2.41685398163334

Epoch: 6| Step: 4
Training loss: 2.8886799812316895
Validation loss: 2.4108691241151545

Epoch: 6| Step: 5
Training loss: 2.6662747859954834
Validation loss: 2.4156514547204457

Epoch: 6| Step: 6
Training loss: 1.7872447967529297
Validation loss: 2.4118481246373986

Epoch: 6| Step: 7
Training loss: 2.5163817405700684
Validation loss: 2.413117531807192

Epoch: 6| Step: 8
Training loss: 1.9897074699401855
Validation loss: 2.419033827320222

Epoch: 6| Step: 9
Training loss: 2.5153956413269043
Validation loss: 2.421043190904843

Epoch: 6| Step: 10
Training loss: 3.022724151611328
Validation loss: 2.4283302189201437

Epoch: 6| Step: 11
Training loss: 3.1155920028686523
Validation loss: 2.440846109902987

Epoch: 6| Step: 12
Training loss: 3.0524420738220215
Validation loss: 2.43753880839194

Epoch: 6| Step: 13
Training loss: 3.647392749786377
Validation loss: 2.4292753306768273

Epoch: 207| Step: 0
Training loss: 2.4157323837280273
Validation loss: 2.432634840729416

Epoch: 6| Step: 1
Training loss: 2.8498167991638184
Validation loss: 2.426838013433641

Epoch: 6| Step: 2
Training loss: 2.4469099044799805
Validation loss: 2.4240343750164075

Epoch: 6| Step: 3
Training loss: 2.5694613456726074
Validation loss: 2.4215367712000364

Epoch: 6| Step: 4
Training loss: 3.2857303619384766
Validation loss: 2.4135211334433606

Epoch: 6| Step: 5
Training loss: 3.1873390674591064
Validation loss: 2.41311982370192

Epoch: 6| Step: 6
Training loss: 2.619354009628296
Validation loss: 2.403742931222403

Epoch: 6| Step: 7
Training loss: 2.7043509483337402
Validation loss: 2.405583622635052

Epoch: 6| Step: 8
Training loss: 2.356334686279297
Validation loss: 2.398535664363574

Epoch: 6| Step: 9
Training loss: 1.6460150480270386
Validation loss: 2.4005132695680023

Epoch: 6| Step: 10
Training loss: 3.2758703231811523
Validation loss: 2.4057783285776773

Epoch: 6| Step: 11
Training loss: 2.506411552429199
Validation loss: 2.4048163660110964

Epoch: 6| Step: 12
Training loss: 2.1489100456237793
Validation loss: 2.3998839675739245

Epoch: 6| Step: 13
Training loss: 2.4643685817718506
Validation loss: 2.4109875438033894

Epoch: 208| Step: 0
Training loss: 2.175924301147461
Validation loss: 2.4148517706060924

Epoch: 6| Step: 1
Training loss: 2.109447717666626
Validation loss: 2.4082987667411886

Epoch: 6| Step: 2
Training loss: 3.278014659881592
Validation loss: 2.413619427270787

Epoch: 6| Step: 3
Training loss: 2.2226405143737793
Validation loss: 2.4194917140468473

Epoch: 6| Step: 4
Training loss: 2.7441349029541016
Validation loss: 2.42399840201101

Epoch: 6| Step: 5
Training loss: 2.4924659729003906
Validation loss: 2.424049649187314

Epoch: 6| Step: 6
Training loss: 2.9261112213134766
Validation loss: 2.418223432315293

Epoch: 6| Step: 7
Training loss: 3.15978741645813
Validation loss: 2.4320203847782587

Epoch: 6| Step: 8
Training loss: 2.821885347366333
Validation loss: 2.4258883947967202

Epoch: 6| Step: 9
Training loss: 2.854602336883545
Validation loss: 2.4298265262316634

Epoch: 6| Step: 10
Training loss: 2.286268711090088
Validation loss: 2.427623312960389

Epoch: 6| Step: 11
Training loss: 2.2873613834381104
Validation loss: 2.4304570536459646

Epoch: 6| Step: 12
Training loss: 2.547823429107666
Validation loss: 2.424838104555684

Epoch: 6| Step: 13
Training loss: 2.5704307556152344
Validation loss: 2.4227343861774733

Epoch: 209| Step: 0
Training loss: 2.8936080932617188
Validation loss: 2.422906473118772

Epoch: 6| Step: 1
Training loss: 2.5502419471740723
Validation loss: 2.4220180972929923

Epoch: 6| Step: 2
Training loss: 3.1239919662475586
Validation loss: 2.4201454193361345

Epoch: 6| Step: 3
Training loss: 2.686936855316162
Validation loss: 2.418034981655818

Epoch: 6| Step: 4
Training loss: 2.581512212753296
Validation loss: 2.4120534645613803

Epoch: 6| Step: 5
Training loss: 2.3605878353118896
Validation loss: 2.4112910750091716

Epoch: 6| Step: 6
Training loss: 2.6640965938568115
Validation loss: 2.41140741173939

Epoch: 6| Step: 7
Training loss: 2.627474546432495
Validation loss: 2.4001053943428943

Epoch: 6| Step: 8
Training loss: 2.469348907470703
Validation loss: 2.3923303286234536

Epoch: 6| Step: 9
Training loss: 2.620893955230713
Validation loss: 2.390200573910949

Epoch: 6| Step: 10
Training loss: 2.128213882446289
Validation loss: 2.3934657240426667

Epoch: 6| Step: 11
Training loss: 2.1852002143859863
Validation loss: 2.392816105196553

Epoch: 6| Step: 12
Training loss: 2.8168187141418457
Validation loss: 2.398990523430609

Epoch: 6| Step: 13
Training loss: 2.925051212310791
Validation loss: 2.3918371200561523

Epoch: 210| Step: 0
Training loss: 2.9166085720062256
Validation loss: 2.393761263098768

Epoch: 6| Step: 1
Training loss: 2.2890615463256836
Validation loss: 2.3989923884791713

Epoch: 6| Step: 2
Training loss: 2.709111213684082
Validation loss: 2.401804890683902

Epoch: 6| Step: 3
Training loss: 2.445720672607422
Validation loss: 2.398450912967805

Epoch: 6| Step: 4
Training loss: 2.2727200984954834
Validation loss: 2.3957002316751788

Epoch: 6| Step: 5
Training loss: 2.6459712982177734
Validation loss: 2.394868581525741

Epoch: 6| Step: 6
Training loss: 2.76486873626709
Validation loss: 2.4013402692733274

Epoch: 6| Step: 7
Training loss: 2.346810817718506
Validation loss: 2.4012978128207627

Epoch: 6| Step: 8
Training loss: 2.8689916133880615
Validation loss: 2.4069813272004486

Epoch: 6| Step: 9
Training loss: 2.867526054382324
Validation loss: 2.4027278372036514

Epoch: 6| Step: 10
Training loss: 2.3168563842773438
Validation loss: 2.4073474048286356

Epoch: 6| Step: 11
Training loss: 2.7230405807495117
Validation loss: 2.403904632855487

Epoch: 6| Step: 12
Training loss: 2.293205738067627
Validation loss: 2.4039506271321285

Epoch: 6| Step: 13
Training loss: 3.5890071392059326
Validation loss: 2.4045445483217955

Epoch: 211| Step: 0
Training loss: 2.6991748809814453
Validation loss: 2.4128651772775958

Epoch: 6| Step: 1
Training loss: 2.577742576599121
Validation loss: 2.40203163700719

Epoch: 6| Step: 2
Training loss: 2.67889404296875
Validation loss: 2.4066623667235016

Epoch: 6| Step: 3
Training loss: 2.8920650482177734
Validation loss: 2.4079666804241877

Epoch: 6| Step: 4
Training loss: 2.5316693782806396
Validation loss: 2.4115062041949202

Epoch: 6| Step: 5
Training loss: 2.6857707500457764
Validation loss: 2.4130807794550413

Epoch: 6| Step: 6
Training loss: 2.6336402893066406
Validation loss: 2.397947226801226

Epoch: 6| Step: 7
Training loss: 2.1612939834594727
Validation loss: 2.400559312553816

Epoch: 6| Step: 8
Training loss: 2.7852888107299805
Validation loss: 2.391269935074673

Epoch: 6| Step: 9
Training loss: 2.6592628955841064
Validation loss: 2.401914788830665

Epoch: 6| Step: 10
Training loss: 1.8296618461608887
Validation loss: 2.399555847208987

Epoch: 6| Step: 11
Training loss: 2.281357526779175
Validation loss: 2.3979721902519144

Epoch: 6| Step: 12
Training loss: 2.831888437271118
Validation loss: 2.3984892214498212

Epoch: 6| Step: 13
Training loss: 3.8473641872406006
Validation loss: 2.406347781099299

Epoch: 212| Step: 0
Training loss: 2.4251999855041504
Validation loss: 2.404045102416828

Epoch: 6| Step: 1
Training loss: 2.3872334957122803
Validation loss: 2.3894758660306215

Epoch: 6| Step: 2
Training loss: 3.0423057079315186
Validation loss: 2.3912782515248945

Epoch: 6| Step: 3
Training loss: 3.092082977294922
Validation loss: 2.392848225050075

Epoch: 6| Step: 4
Training loss: 2.316194772720337
Validation loss: 2.3856201351329847

Epoch: 6| Step: 5
Training loss: 3.0606961250305176
Validation loss: 2.383666697368827

Epoch: 6| Step: 6
Training loss: 2.7836103439331055
Validation loss: 2.3866645008005123

Epoch: 6| Step: 7
Training loss: 2.2625622749328613
Validation loss: 2.384818366778794

Epoch: 6| Step: 8
Training loss: 2.5300283432006836
Validation loss: 2.388376628198931

Epoch: 6| Step: 9
Training loss: 3.132483959197998
Validation loss: 2.3866310145265315

Epoch: 6| Step: 10
Training loss: 2.4931416511535645
Validation loss: 2.3902689205702914

Epoch: 6| Step: 11
Training loss: 2.4643096923828125
Validation loss: 2.396036617217525

Epoch: 6| Step: 12
Training loss: 2.284573554992676
Validation loss: 2.4014314284888645

Epoch: 6| Step: 13
Training loss: 2.0428450107574463
Validation loss: 2.3976639855292534

Epoch: 213| Step: 0
Training loss: 3.0383810997009277
Validation loss: 2.4042277054120134

Epoch: 6| Step: 1
Training loss: 2.1090786457061768
Validation loss: 2.3984675586864515

Epoch: 6| Step: 2
Training loss: 2.333568811416626
Validation loss: 2.395013150348458

Epoch: 6| Step: 3
Training loss: 3.1017956733703613
Validation loss: 2.391990907730595

Epoch: 6| Step: 4
Training loss: 2.519296646118164
Validation loss: 2.398890406854691

Epoch: 6| Step: 5
Training loss: 2.5891547203063965
Validation loss: 2.4020317574983

Epoch: 6| Step: 6
Training loss: 2.808913230895996
Validation loss: 2.3935169789098922

Epoch: 6| Step: 7
Training loss: 2.295499324798584
Validation loss: 2.4042178225773636

Epoch: 6| Step: 8
Training loss: 2.9333505630493164
Validation loss: 2.4012762910576275

Epoch: 6| Step: 9
Training loss: 3.4887890815734863
Validation loss: 2.4089766548525904

Epoch: 6| Step: 10
Training loss: 2.7940011024475098
Validation loss: 2.4111599922180176

Epoch: 6| Step: 11
Training loss: 1.7985339164733887
Validation loss: 2.4167362361825924

Epoch: 6| Step: 12
Training loss: 2.387873649597168
Validation loss: 2.40798673834852

Epoch: 6| Step: 13
Training loss: 1.9944055080413818
Validation loss: 2.41138824852564

Epoch: 214| Step: 0
Training loss: 2.8322291374206543
Validation loss: 2.4015673591244604

Epoch: 6| Step: 1
Training loss: 2.5593957901000977
Validation loss: 2.3980422019958496

Epoch: 6| Step: 2
Training loss: 2.041771650314331
Validation loss: 2.3996245604689403

Epoch: 6| Step: 3
Training loss: 2.7619824409484863
Validation loss: 2.3960399935322423

Epoch: 6| Step: 4
Training loss: 2.653733253479004
Validation loss: 2.4030951505066245

Epoch: 6| Step: 5
Training loss: 2.583216667175293
Validation loss: 2.401842901783605

Epoch: 6| Step: 6
Training loss: 1.9171332120895386
Validation loss: 2.405158612035936

Epoch: 6| Step: 7
Training loss: 2.2612266540527344
Validation loss: 2.3976005149143997

Epoch: 6| Step: 8
Training loss: 2.8738322257995605
Validation loss: 2.4143683910369873

Epoch: 6| Step: 9
Training loss: 3.079726457595825
Validation loss: 2.4260704850637786

Epoch: 6| Step: 10
Training loss: 2.85006046295166
Validation loss: 2.4104040591947493

Epoch: 6| Step: 11
Training loss: 3.034813165664673
Validation loss: 2.4186226475623345

Epoch: 6| Step: 12
Training loss: 2.454162120819092
Validation loss: 2.4118964518270185

Epoch: 6| Step: 13
Training loss: 2.490323781967163
Validation loss: 2.404405217016897

Epoch: 215| Step: 0
Training loss: 2.626587152481079
Validation loss: 2.4036295721607823

Epoch: 6| Step: 1
Training loss: 2.079550266265869
Validation loss: 2.3933497244311916

Epoch: 6| Step: 2
Training loss: 3.1510443687438965
Validation loss: 2.39668091394568

Epoch: 6| Step: 3
Training loss: 3.3291544914245605
Validation loss: 2.390590101160029

Epoch: 6| Step: 4
Training loss: 1.6064622402191162
Validation loss: 2.389471064331711

Epoch: 6| Step: 5
Training loss: 1.9503915309906006
Validation loss: 2.393512333593061

Epoch: 6| Step: 6
Training loss: 2.6910083293914795
Validation loss: 2.393284966868739

Epoch: 6| Step: 7
Training loss: 3.19370698928833
Validation loss: 2.3899988410293416

Epoch: 6| Step: 8
Training loss: 2.425689935684204
Validation loss: 2.393442351330993

Epoch: 6| Step: 9
Training loss: 3.2464864253997803
Validation loss: 2.395005569663099

Epoch: 6| Step: 10
Training loss: 2.5770084857940674
Validation loss: 2.400828528147872

Epoch: 6| Step: 11
Training loss: 2.426891803741455
Validation loss: 2.399725635846456

Epoch: 6| Step: 12
Training loss: 2.9772915840148926
Validation loss: 2.4102100300532516

Epoch: 6| Step: 13
Training loss: 1.8757890462875366
Validation loss: 2.405543311949699

Epoch: 216| Step: 0
Training loss: 1.993891954421997
Validation loss: 2.407048204893707

Epoch: 6| Step: 1
Training loss: 2.073404312133789
Validation loss: 2.3987420169256066

Epoch: 6| Step: 2
Training loss: 2.4644899368286133
Validation loss: 2.3915733137438373

Epoch: 6| Step: 3
Training loss: 3.2012925148010254
Validation loss: 2.38103832480728

Epoch: 6| Step: 4
Training loss: 2.9989800453186035
Validation loss: 2.382767415815784

Epoch: 6| Step: 5
Training loss: 3.115690231323242
Validation loss: 2.382006638793535

Epoch: 6| Step: 6
Training loss: 2.2405200004577637
Validation loss: 2.388014967723559

Epoch: 6| Step: 7
Training loss: 2.3740177154541016
Validation loss: 2.3825365651038384

Epoch: 6| Step: 8
Training loss: 2.466218948364258
Validation loss: 2.3849675270818893

Epoch: 6| Step: 9
Training loss: 2.905820369720459
Validation loss: 2.3873396970892466

Epoch: 6| Step: 10
Training loss: 1.8967015743255615
Validation loss: 2.392055683238532

Epoch: 6| Step: 11
Training loss: 2.792468309402466
Validation loss: 2.391171637401786

Epoch: 6| Step: 12
Training loss: 3.2958290576934814
Validation loss: 2.4057627160062074

Epoch: 6| Step: 13
Training loss: 2.6112489700317383
Validation loss: 2.4050445505367812

Epoch: 217| Step: 0
Training loss: 2.554112434387207
Validation loss: 2.414506863522273

Epoch: 6| Step: 1
Training loss: 2.3941802978515625
Validation loss: 2.4170780745885705

Epoch: 6| Step: 2
Training loss: 2.357787847518921
Validation loss: 2.424393694887879

Epoch: 6| Step: 3
Training loss: 2.0043253898620605
Validation loss: 2.4232737146398073

Epoch: 6| Step: 4
Training loss: 2.638547897338867
Validation loss: 2.433598123570924

Epoch: 6| Step: 5
Training loss: 2.4997448921203613
Validation loss: 2.424109443541496

Epoch: 6| Step: 6
Training loss: 2.8062400817871094
Validation loss: 2.428313106618902

Epoch: 6| Step: 7
Training loss: 2.2727673053741455
Validation loss: 2.4244684121942006

Epoch: 6| Step: 8
Training loss: 2.4571332931518555
Validation loss: 2.415765182946318

Epoch: 6| Step: 9
Training loss: 2.9889020919799805
Validation loss: 2.4029156097801785

Epoch: 6| Step: 10
Training loss: 2.938798189163208
Validation loss: 2.408634293463922

Epoch: 6| Step: 11
Training loss: 2.6205382347106934
Validation loss: 2.3931237933456257

Epoch: 6| Step: 12
Training loss: 3.16725492477417
Validation loss: 2.3896344015675206

Epoch: 6| Step: 13
Training loss: 2.6488571166992188
Validation loss: 2.3886987932266726

Epoch: 218| Step: 0
Training loss: 2.9323248863220215
Validation loss: 2.3827078906438683

Epoch: 6| Step: 1
Training loss: 2.4046661853790283
Validation loss: 2.3919078739740516

Epoch: 6| Step: 2
Training loss: 2.3567705154418945
Validation loss: 2.381387610589304

Epoch: 6| Step: 3
Training loss: 2.7644495964050293
Validation loss: 2.3918280114409742

Epoch: 6| Step: 4
Training loss: 2.5146775245666504
Validation loss: 2.390627048348868

Epoch: 6| Step: 5
Training loss: 2.281609058380127
Validation loss: 2.393517336537761

Epoch: 6| Step: 6
Training loss: 2.4233932495117188
Validation loss: 2.389677437402869

Epoch: 6| Step: 7
Training loss: 3.0723652839660645
Validation loss: 2.3895554337450253

Epoch: 6| Step: 8
Training loss: 3.025714874267578
Validation loss: 2.3952922667226484

Epoch: 6| Step: 9
Training loss: 2.2339320182800293
Validation loss: 2.39552572209348

Epoch: 6| Step: 10
Training loss: 2.9672670364379883
Validation loss: 2.3949867192135064

Epoch: 6| Step: 11
Training loss: 2.388559579849243
Validation loss: 2.403278584121376

Epoch: 6| Step: 12
Training loss: 2.4197468757629395
Validation loss: 2.405376003634545

Epoch: 6| Step: 13
Training loss: 2.3370614051818848
Validation loss: 2.4050711354901715

Epoch: 219| Step: 0
Training loss: 2.155421018600464
Validation loss: 2.417416295697612

Epoch: 6| Step: 1
Training loss: 2.421802282333374
Validation loss: 2.426662116922358

Epoch: 6| Step: 2
Training loss: 2.4302895069122314
Validation loss: 2.4484087703048543

Epoch: 6| Step: 3
Training loss: 2.741339683532715
Validation loss: 2.461469168304115

Epoch: 6| Step: 4
Training loss: 3.10384464263916
Validation loss: 2.488555359584029

Epoch: 6| Step: 5
Training loss: 2.071410655975342
Validation loss: 2.4754395920743226

Epoch: 6| Step: 6
Training loss: 2.270226240158081
Validation loss: 2.479844826523976

Epoch: 6| Step: 7
Training loss: 2.2271852493286133
Validation loss: 2.456361375829225

Epoch: 6| Step: 8
Training loss: 2.967308521270752
Validation loss: 2.432762366469188

Epoch: 6| Step: 9
Training loss: 3.018301010131836
Validation loss: 2.4146689240650465

Epoch: 6| Step: 10
Training loss: 2.947303533554077
Validation loss: 2.4004452356728176

Epoch: 6| Step: 11
Training loss: 2.571803331375122
Validation loss: 2.396962078668738

Epoch: 6| Step: 12
Training loss: 2.481180429458618
Validation loss: 2.3899957262059695

Epoch: 6| Step: 13
Training loss: 3.5001091957092285
Validation loss: 2.3922638944400254

Epoch: 220| Step: 0
Training loss: 2.2637686729431152
Validation loss: 2.389564155250467

Epoch: 6| Step: 1
Training loss: 2.722823143005371
Validation loss: 2.3830312375099427

Epoch: 6| Step: 2
Training loss: 2.461759567260742
Validation loss: 2.3796271765103905

Epoch: 6| Step: 3
Training loss: 2.787733793258667
Validation loss: 2.3753596428901917

Epoch: 6| Step: 4
Training loss: 2.365474224090576
Validation loss: 2.3781226937488844

Epoch: 6| Step: 5
Training loss: 2.518437385559082
Validation loss: 2.377612965081328

Epoch: 6| Step: 6
Training loss: 2.2207272052764893
Validation loss: 2.377315198221514

Epoch: 6| Step: 7
Training loss: 3.1756398677825928
Validation loss: 2.3775340716044107

Epoch: 6| Step: 8
Training loss: 2.6397671699523926
Validation loss: 2.3768515740671465

Epoch: 6| Step: 9
Training loss: 2.0280356407165527
Validation loss: 2.3850592259437806

Epoch: 6| Step: 10
Training loss: 2.497269630432129
Validation loss: 2.3879931331962667

Epoch: 6| Step: 11
Training loss: 2.5589120388031006
Validation loss: 2.4048858406723186

Epoch: 6| Step: 12
Training loss: 3.6552071571350098
Validation loss: 2.4153708898892967

Epoch: 6| Step: 13
Training loss: 2.5721945762634277
Validation loss: 2.4376046401198193

Epoch: 221| Step: 0
Training loss: 3.036454200744629
Validation loss: 2.4418895142052763

Epoch: 6| Step: 1
Training loss: 2.8252525329589844
Validation loss: 2.458781350043512

Epoch: 6| Step: 2
Training loss: 2.2485904693603516
Validation loss: 2.457570383625646

Epoch: 6| Step: 3
Training loss: 2.942308187484741
Validation loss: 2.443684093413814

Epoch: 6| Step: 4
Training loss: 2.751922845840454
Validation loss: 2.415720690963089

Epoch: 6| Step: 5
Training loss: 2.454806327819824
Validation loss: 2.398149464720039

Epoch: 6| Step: 6
Training loss: 2.651867389678955
Validation loss: 2.3875308549532326

Epoch: 6| Step: 7
Training loss: 3.058281898498535
Validation loss: 2.374303698539734

Epoch: 6| Step: 8
Training loss: 2.5066354274749756
Validation loss: 2.373279212623514

Epoch: 6| Step: 9
Training loss: 2.4051764011383057
Validation loss: 2.3731225921261694

Epoch: 6| Step: 10
Training loss: 2.1491594314575195
Validation loss: 2.3720246720057663

Epoch: 6| Step: 11
Training loss: 2.104989528656006
Validation loss: 2.384581110810721

Epoch: 6| Step: 12
Training loss: 2.3465335369110107
Validation loss: 2.3905263869993147

Epoch: 6| Step: 13
Training loss: 3.4760522842407227
Validation loss: 2.381051876211679

Epoch: 222| Step: 0
Training loss: 2.3509297370910645
Validation loss: 2.3859587254062777

Epoch: 6| Step: 1
Training loss: 2.5275211334228516
Validation loss: 2.3769376252287175

Epoch: 6| Step: 2
Training loss: 2.101672649383545
Validation loss: 2.377933863670595

Epoch: 6| Step: 3
Training loss: 2.7453694343566895
Validation loss: 2.3805093790895198

Epoch: 6| Step: 4
Training loss: 2.5642495155334473
Validation loss: 2.3828098594501452

Epoch: 6| Step: 5
Training loss: 2.6437597274780273
Validation loss: 2.3939148738820064

Epoch: 6| Step: 6
Training loss: 2.9288387298583984
Validation loss: 2.3969195529978764

Epoch: 6| Step: 7
Training loss: 2.6152875423431396
Validation loss: 2.3945771058400473

Epoch: 6| Step: 8
Training loss: 3.0314395427703857
Validation loss: 2.3973140062824374

Epoch: 6| Step: 9
Training loss: 2.61474609375
Validation loss: 2.405562690509263

Epoch: 6| Step: 10
Training loss: 2.6954164505004883
Validation loss: 2.396926333827357

Epoch: 6| Step: 11
Training loss: 1.9400413036346436
Validation loss: 2.408472066284508

Epoch: 6| Step: 12
Training loss: 2.849916934967041
Validation loss: 2.4175700064628356

Epoch: 6| Step: 13
Training loss: 2.77358078956604
Validation loss: 2.4057678535420406

Epoch: 223| Step: 0
Training loss: 2.717411518096924
Validation loss: 2.40119146403446

Epoch: 6| Step: 1
Training loss: 2.0810952186584473
Validation loss: 2.389847088885564

Epoch: 6| Step: 2
Training loss: 2.257228374481201
Validation loss: 2.394043304586923

Epoch: 6| Step: 3
Training loss: 2.0752763748168945
Validation loss: 2.401649095678842

Epoch: 6| Step: 4
Training loss: 2.339343547821045
Validation loss: 2.398914178212484

Epoch: 6| Step: 5
Training loss: 3.4387197494506836
Validation loss: 2.397638609332423

Epoch: 6| Step: 6
Training loss: 2.4874348640441895
Validation loss: 2.4139476770995767

Epoch: 6| Step: 7
Training loss: 2.7886736392974854
Validation loss: 2.4214991574646323

Epoch: 6| Step: 8
Training loss: 2.7280149459838867
Validation loss: 2.411828681986819

Epoch: 6| Step: 9
Training loss: 2.281426429748535
Validation loss: 2.4277819638611167

Epoch: 6| Step: 10
Training loss: 3.1981630325317383
Validation loss: 2.4292427519316315

Epoch: 6| Step: 11
Training loss: 3.1878137588500977
Validation loss: 2.4267340988241215

Epoch: 6| Step: 12
Training loss: 2.3143999576568604
Validation loss: 2.426269728650329

Epoch: 6| Step: 13
Training loss: 2.31693696975708
Validation loss: 2.4226014665378037

Epoch: 224| Step: 0
Training loss: 1.8820278644561768
Validation loss: 2.4028154765405962

Epoch: 6| Step: 1
Training loss: 1.9967551231384277
Validation loss: 2.402088601102111

Epoch: 6| Step: 2
Training loss: 2.811431407928467
Validation loss: 2.4132203081602692

Epoch: 6| Step: 3
Training loss: 2.898545503616333
Validation loss: 2.4176576739998272

Epoch: 6| Step: 4
Training loss: 2.5610008239746094
Validation loss: 2.419374583869852

Epoch: 6| Step: 5
Training loss: 2.651841402053833
Validation loss: 2.422672721647447

Epoch: 6| Step: 6
Training loss: 2.384653091430664
Validation loss: 2.4045551771758706

Epoch: 6| Step: 7
Training loss: 2.080325126647949
Validation loss: 2.3924399473333873

Epoch: 6| Step: 8
Training loss: 3.6331024169921875
Validation loss: 2.38156041278634

Epoch: 6| Step: 9
Training loss: 3.200336456298828
Validation loss: 2.3732882135657856

Epoch: 6| Step: 10
Training loss: 2.6542818546295166
Validation loss: 2.373476405297556

Epoch: 6| Step: 11
Training loss: 2.1322598457336426
Validation loss: 2.3653134248589955

Epoch: 6| Step: 12
Training loss: 2.1313588619232178
Validation loss: 2.368494474759666

Epoch: 6| Step: 13
Training loss: 3.6683688163757324
Validation loss: 2.363958543346774

Epoch: 225| Step: 0
Training loss: 2.8816161155700684
Validation loss: 2.3666927006936844

Epoch: 6| Step: 1
Training loss: 2.499940872192383
Validation loss: 2.36055451695637

Epoch: 6| Step: 2
Training loss: 2.661675453186035
Validation loss: 2.3602560386862805

Epoch: 6| Step: 3
Training loss: 2.073178768157959
Validation loss: 2.356433383880123

Epoch: 6| Step: 4
Training loss: 2.5235986709594727
Validation loss: 2.3653771377378896

Epoch: 6| Step: 5
Training loss: 2.6496658325195312
Validation loss: 2.3578550328490553

Epoch: 6| Step: 6
Training loss: 2.1460118293762207
Validation loss: 2.3629910099890923

Epoch: 6| Step: 7
Training loss: 2.5666565895080566
Validation loss: 2.365057550450807

Epoch: 6| Step: 8
Training loss: 3.0875117778778076
Validation loss: 2.3777851827682985

Epoch: 6| Step: 9
Training loss: 2.5308139324188232
Validation loss: 2.373572398257512

Epoch: 6| Step: 10
Training loss: 2.49643611907959
Validation loss: 2.3726405969230075

Epoch: 6| Step: 11
Training loss: 2.1784400939941406
Validation loss: 2.374731045897289

Epoch: 6| Step: 12
Training loss: 3.1120188236236572
Validation loss: 2.386863595695906

Epoch: 6| Step: 13
Training loss: 3.0556716918945312
Validation loss: 2.3913767671072357

Epoch: 226| Step: 0
Training loss: 2.357571601867676
Validation loss: 2.398975149277718

Epoch: 6| Step: 1
Training loss: 2.2134604454040527
Validation loss: 2.417356252670288

Epoch: 6| Step: 2
Training loss: 2.929332733154297
Validation loss: 2.422791673291114

Epoch: 6| Step: 3
Training loss: 2.178030252456665
Validation loss: 2.4228372317488476

Epoch: 6| Step: 4
Training loss: 2.57749605178833
Validation loss: 2.4166409815511396

Epoch: 6| Step: 5
Training loss: 2.4322280883789062
Validation loss: 2.4077456612740793

Epoch: 6| Step: 6
Training loss: 3.2513413429260254
Validation loss: 2.3944992455103065

Epoch: 6| Step: 7
Training loss: 2.9866323471069336
Validation loss: 2.3938823951187955

Epoch: 6| Step: 8
Training loss: 2.6956229209899902
Validation loss: 2.3797437478137273

Epoch: 6| Step: 9
Training loss: 2.2639944553375244
Validation loss: 2.3674184686394146

Epoch: 6| Step: 10
Training loss: 2.9682886600494385
Validation loss: 2.366189818228445

Epoch: 6| Step: 11
Training loss: 3.1588504314422607
Validation loss: 2.363215918182045

Epoch: 6| Step: 12
Training loss: 1.7590469121932983
Validation loss: 2.366376911440203

Epoch: 6| Step: 13
Training loss: 2.3870928287506104
Validation loss: 2.3742001902672554

Epoch: 227| Step: 0
Training loss: 1.7391682863235474
Validation loss: 2.373947176882016

Epoch: 6| Step: 1
Training loss: 2.232433795928955
Validation loss: 2.3747040674250615

Epoch: 6| Step: 2
Training loss: 2.0202927589416504
Validation loss: 2.383949769440518

Epoch: 6| Step: 3
Training loss: 2.478377342224121
Validation loss: 2.387808028087821

Epoch: 6| Step: 4
Training loss: 2.2391164302825928
Validation loss: 2.3990556552845943

Epoch: 6| Step: 5
Training loss: 3.458340644836426
Validation loss: 2.403344359449161

Epoch: 6| Step: 6
Training loss: 2.316096782684326
Validation loss: 2.41505072193761

Epoch: 6| Step: 7
Training loss: 3.2733380794525146
Validation loss: 2.421057335792049

Epoch: 6| Step: 8
Training loss: 2.4726333618164062
Validation loss: 2.4160669849764917

Epoch: 6| Step: 9
Training loss: 2.5546875
Validation loss: 2.3960148467812488

Epoch: 6| Step: 10
Training loss: 3.3950839042663574
Validation loss: 2.3900238519073813

Epoch: 6| Step: 11
Training loss: 2.9780850410461426
Validation loss: 2.3786755249064457

Epoch: 6| Step: 12
Training loss: 2.771393299102783
Validation loss: 2.372464867048366

Epoch: 6| Step: 13
Training loss: 2.2218997478485107
Validation loss: 2.3642967670194563

Epoch: 228| Step: 0
Training loss: 1.9263060092926025
Validation loss: 2.362519941022319

Epoch: 6| Step: 1
Training loss: 2.3503499031066895
Validation loss: 2.3637075949740667

Epoch: 6| Step: 2
Training loss: 2.5071816444396973
Validation loss: 2.3686651029894428

Epoch: 6| Step: 3
Training loss: 2.9716856479644775
Validation loss: 2.3691112841329267

Epoch: 6| Step: 4
Training loss: 1.9941771030426025
Validation loss: 2.3683897013305337

Epoch: 6| Step: 5
Training loss: 3.5108330249786377
Validation loss: 2.3628135522206626

Epoch: 6| Step: 6
Training loss: 2.8040709495544434
Validation loss: 2.3706605280599287

Epoch: 6| Step: 7
Training loss: 1.342688798904419
Validation loss: 2.3738218635641117

Epoch: 6| Step: 8
Training loss: 3.989760398864746
Validation loss: 2.373026245383806

Epoch: 6| Step: 9
Training loss: 2.69608473777771
Validation loss: 2.3695587804240565

Epoch: 6| Step: 10
Training loss: 2.0773661136627197
Validation loss: 2.368923279546922

Epoch: 6| Step: 11
Training loss: 2.5933549404144287
Validation loss: 2.3674063785101778

Epoch: 6| Step: 12
Training loss: 2.4771029949188232
Validation loss: 2.3715304584913355

Epoch: 6| Step: 13
Training loss: 3.12599778175354
Validation loss: 2.386324405670166

Epoch: 229| Step: 0
Training loss: 2.8716683387756348
Validation loss: 2.3848120474046275

Epoch: 6| Step: 1
Training loss: 2.986268997192383
Validation loss: 2.3971110518260668

Epoch: 6| Step: 2
Training loss: 2.0817463397979736
Validation loss: 2.3906463423082904

Epoch: 6| Step: 3
Training loss: 2.521416425704956
Validation loss: 2.3838893931399108

Epoch: 6| Step: 4
Training loss: 3.3304295539855957
Validation loss: 2.382411132576645

Epoch: 6| Step: 5
Training loss: 2.8826093673706055
Validation loss: 2.3856275825090307

Epoch: 6| Step: 6
Training loss: 3.100285053253174
Validation loss: 2.375537263449802

Epoch: 6| Step: 7
Training loss: 2.3232944011688232
Validation loss: 2.3840249712749193

Epoch: 6| Step: 8
Training loss: 2.2821574211120605
Validation loss: 2.3737555883264028

Epoch: 6| Step: 9
Training loss: 2.4717941284179688
Validation loss: 2.3762630621592202

Epoch: 6| Step: 10
Training loss: 2.3535990715026855
Validation loss: 2.3775324565108105

Epoch: 6| Step: 11
Training loss: 3.176316261291504
Validation loss: 2.3760569646794307

Epoch: 6| Step: 12
Training loss: 1.9231572151184082
Validation loss: 2.3811496252654702

Epoch: 6| Step: 13
Training loss: 1.2841284275054932
Validation loss: 2.375754071820167

Epoch: 230| Step: 0
Training loss: 2.3581314086914062
Validation loss: 2.3860140154438634

Epoch: 6| Step: 1
Training loss: 1.7789978981018066
Validation loss: 2.3920123961664017

Epoch: 6| Step: 2
Training loss: 3.0254218578338623
Validation loss: 2.381548435457291

Epoch: 6| Step: 3
Training loss: 2.803609609603882
Validation loss: 2.3823907349699285

Epoch: 6| Step: 4
Training loss: 2.661158323287964
Validation loss: 2.376768317273868

Epoch: 6| Step: 5
Training loss: 2.449134588241577
Validation loss: 2.3821595227846535

Epoch: 6| Step: 6
Training loss: 2.6080398559570312
Validation loss: 2.386249421745218

Epoch: 6| Step: 7
Training loss: 2.025249719619751
Validation loss: 2.387598024901523

Epoch: 6| Step: 8
Training loss: 2.9978525638580322
Validation loss: 2.3911097075349543

Epoch: 6| Step: 9
Training loss: 3.1952853202819824
Validation loss: 2.3929165588912142

Epoch: 6| Step: 10
Training loss: 1.8136024475097656
Validation loss: 2.3843935023071947

Epoch: 6| Step: 11
Training loss: 3.0579962730407715
Validation loss: 2.4046968131937008

Epoch: 6| Step: 12
Training loss: 2.3098692893981934
Validation loss: 2.3937463786012385

Epoch: 6| Step: 13
Training loss: 3.2585713863372803
Validation loss: 2.412388914374895

Epoch: 231| Step: 0
Training loss: 1.7684030532836914
Validation loss: 2.3979343265615483

Epoch: 6| Step: 1
Training loss: 2.999937057495117
Validation loss: 2.3978850277521278

Epoch: 6| Step: 2
Training loss: 2.3060414791107178
Validation loss: 2.386216045707785

Epoch: 6| Step: 3
Training loss: 2.074448823928833
Validation loss: 2.387247880299886

Epoch: 6| Step: 4
Training loss: 2.5286412239074707
Validation loss: 2.3857929245118172

Epoch: 6| Step: 5
Training loss: 3.40492582321167
Validation loss: 2.389437316566385

Epoch: 6| Step: 6
Training loss: 2.080125331878662
Validation loss: 2.3827405975710962

Epoch: 6| Step: 7
Training loss: 3.1777467727661133
Validation loss: 2.3764377845230924

Epoch: 6| Step: 8
Training loss: 2.521331548690796
Validation loss: 2.379698707211402

Epoch: 6| Step: 9
Training loss: 2.3996400833129883
Validation loss: 2.38269313304655

Epoch: 6| Step: 10
Training loss: 2.4985251426696777
Validation loss: 2.378138675484606

Epoch: 6| Step: 11
Training loss: 3.1218981742858887
Validation loss: 2.379670548182662

Epoch: 6| Step: 12
Training loss: 2.433838129043579
Validation loss: 2.3828047885689685

Epoch: 6| Step: 13
Training loss: 2.728239059448242
Validation loss: 2.3856158461622012

Epoch: 232| Step: 0
Training loss: 2.2592692375183105
Validation loss: 2.3870003531056065

Epoch: 6| Step: 1
Training loss: 2.0394372940063477
Validation loss: 2.385859279222386

Epoch: 6| Step: 2
Training loss: 2.4289867877960205
Validation loss: 2.3850669783930623

Epoch: 6| Step: 3
Training loss: 2.6315317153930664
Validation loss: 2.374599451659828

Epoch: 6| Step: 4
Training loss: 2.3875632286071777
Validation loss: 2.3859381829538653

Epoch: 6| Step: 5
Training loss: 2.8038647174835205
Validation loss: 2.383868378977622

Epoch: 6| Step: 6
Training loss: 3.3178324699401855
Validation loss: 2.3849003366244736

Epoch: 6| Step: 7
Training loss: 2.8303189277648926
Validation loss: 2.390577221429476

Epoch: 6| Step: 8
Training loss: 2.8378963470458984
Validation loss: 2.379936500262189

Epoch: 6| Step: 9
Training loss: 2.279245376586914
Validation loss: 2.369436840857229

Epoch: 6| Step: 10
Training loss: 2.862060546875
Validation loss: 2.3833610242412937

Epoch: 6| Step: 11
Training loss: 2.2067127227783203
Validation loss: 2.3845039798367407

Epoch: 6| Step: 12
Training loss: 2.615105628967285
Validation loss: 2.3863036119809715

Epoch: 6| Step: 13
Training loss: 2.5880725383758545
Validation loss: 2.4020529331699496

Epoch: 233| Step: 0
Training loss: 2.4117488861083984
Validation loss: 2.398981299451602

Epoch: 6| Step: 1
Training loss: 2.0831546783447266
Validation loss: 2.420770255468225

Epoch: 6| Step: 2
Training loss: 2.91519832611084
Validation loss: 2.430590063013056

Epoch: 6| Step: 3
Training loss: 3.1070926189422607
Validation loss: 2.4450306764212986

Epoch: 6| Step: 4
Training loss: 1.7738604545593262
Validation loss: 2.430751495463874

Epoch: 6| Step: 5
Training loss: 3.298495292663574
Validation loss: 2.4251281728026686

Epoch: 6| Step: 6
Training loss: 2.7730040550231934
Validation loss: 2.4262539161148893

Epoch: 6| Step: 7
Training loss: 2.9630215167999268
Validation loss: 2.414562109977968

Epoch: 6| Step: 8
Training loss: 2.423037528991699
Validation loss: 2.404816847975536

Epoch: 6| Step: 9
Training loss: 2.421841621398926
Validation loss: 2.399251250810521

Epoch: 6| Step: 10
Training loss: 2.390334367752075
Validation loss: 2.3811100887995895

Epoch: 6| Step: 11
Training loss: 2.8757288455963135
Validation loss: 2.387160301208496

Epoch: 6| Step: 12
Training loss: 1.9919544458389282
Validation loss: 2.391915846896428

Epoch: 6| Step: 13
Training loss: 2.915302276611328
Validation loss: 2.390073166098646

Epoch: 234| Step: 0
Training loss: 2.695197105407715
Validation loss: 2.3787818288290374

Epoch: 6| Step: 1
Training loss: 2.3039450645446777
Validation loss: 2.378736565189977

Epoch: 6| Step: 2
Training loss: 2.1989493370056152
Validation loss: 2.377852814171904

Epoch: 6| Step: 3
Training loss: 2.6821484565734863
Validation loss: 2.3783048275978333

Epoch: 6| Step: 4
Training loss: 2.831435203552246
Validation loss: 2.3709772325331167

Epoch: 6| Step: 5
Training loss: 2.65883469581604
Validation loss: 2.367258225717852

Epoch: 6| Step: 6
Training loss: 2.213982343673706
Validation loss: 2.366627485521378

Epoch: 6| Step: 7
Training loss: 2.914567708969116
Validation loss: 2.3665937864652244

Epoch: 6| Step: 8
Training loss: 2.8423025608062744
Validation loss: 2.363375545829855

Epoch: 6| Step: 9
Training loss: 2.3454835414886475
Validation loss: 2.3606354485275927

Epoch: 6| Step: 10
Training loss: 2.2429442405700684
Validation loss: 2.3620861166266987

Epoch: 6| Step: 11
Training loss: 2.907703399658203
Validation loss: 2.359942397763652

Epoch: 6| Step: 12
Training loss: 3.0687966346740723
Validation loss: 2.369571796027563

Epoch: 6| Step: 13
Training loss: 1.695313572883606
Validation loss: 2.365815367749942

Epoch: 235| Step: 0
Training loss: 2.6402368545532227
Validation loss: 2.3688310269386537

Epoch: 6| Step: 1
Training loss: 2.1278886795043945
Validation loss: 2.367893308721563

Epoch: 6| Step: 2
Training loss: 2.8162527084350586
Validation loss: 2.3718055884043374

Epoch: 6| Step: 3
Training loss: 2.2402758598327637
Validation loss: 2.366050999651673

Epoch: 6| Step: 4
Training loss: 2.6427271366119385
Validation loss: 2.386470056349231

Epoch: 6| Step: 5
Training loss: 3.0162246227264404
Validation loss: 2.3808599748919086

Epoch: 6| Step: 6
Training loss: 2.501580238342285
Validation loss: 2.385118588324516

Epoch: 6| Step: 7
Training loss: 2.19921612739563
Validation loss: 2.3885408293816353

Epoch: 6| Step: 8
Training loss: 2.0908613204956055
Validation loss: 2.3830170041771344

Epoch: 6| Step: 9
Training loss: 2.7024428844451904
Validation loss: 2.3756422304338023

Epoch: 6| Step: 10
Training loss: 2.6527271270751953
Validation loss: 2.3823364524431128

Epoch: 6| Step: 11
Training loss: 3.0538315773010254
Validation loss: 2.3919374635142665

Epoch: 6| Step: 12
Training loss: 2.8605103492736816
Validation loss: 2.3923045794169107

Epoch: 6| Step: 13
Training loss: 2.362250804901123
Validation loss: 2.3899156790907665

Epoch: 236| Step: 0
Training loss: 2.4241483211517334
Validation loss: 2.4084281254840154

Epoch: 6| Step: 1
Training loss: 2.7820966243743896
Validation loss: 2.396701943489813

Epoch: 6| Step: 2
Training loss: 2.7875640392303467
Validation loss: 2.399156319197788

Epoch: 6| Step: 3
Training loss: 2.595715284347534
Validation loss: 2.4002676369041525

Epoch: 6| Step: 4
Training loss: 2.6811609268188477
Validation loss: 2.3769577626259095

Epoch: 6| Step: 5
Training loss: 2.4000608921051025
Validation loss: 2.3719077674291467

Epoch: 6| Step: 6
Training loss: 3.025259017944336
Validation loss: 2.3732551797743766

Epoch: 6| Step: 7
Training loss: 3.0605645179748535
Validation loss: 2.3686898651943413

Epoch: 6| Step: 8
Training loss: 1.9682856798171997
Validation loss: 2.3636897840807514

Epoch: 6| Step: 9
Training loss: 2.808870553970337
Validation loss: 2.367666229124992

Epoch: 6| Step: 10
Training loss: 2.647195339202881
Validation loss: 2.3535365545621483

Epoch: 6| Step: 11
Training loss: 1.6718316078186035
Validation loss: 2.3520323884102607

Epoch: 6| Step: 12
Training loss: 2.672584295272827
Validation loss: 2.354618469874064

Epoch: 6| Step: 13
Training loss: 2.6031334400177
Validation loss: 2.351812983071932

Epoch: 237| Step: 0
Training loss: 2.216947078704834
Validation loss: 2.3482767561430573

Epoch: 6| Step: 1
Training loss: 2.3846778869628906
Validation loss: 2.356127610770605

Epoch: 6| Step: 2
Training loss: 2.24999737739563
Validation loss: 2.3538769086201987

Epoch: 6| Step: 3
Training loss: 2.323563814163208
Validation loss: 2.3704967691052343

Epoch: 6| Step: 4
Training loss: 2.5628652572631836
Validation loss: 2.3790463555243706

Epoch: 6| Step: 5
Training loss: 3.3079943656921387
Validation loss: 2.3911203799709195

Epoch: 6| Step: 6
Training loss: 2.109731674194336
Validation loss: 2.3909813614301783

Epoch: 6| Step: 7
Training loss: 2.158921718597412
Validation loss: 2.391942698468444

Epoch: 6| Step: 8
Training loss: 3.604921579360962
Validation loss: 2.3811213970184326

Epoch: 6| Step: 9
Training loss: 2.487849473953247
Validation loss: 2.3757003379124466

Epoch: 6| Step: 10
Training loss: 2.6677122116088867
Validation loss: 2.3836601652124876

Epoch: 6| Step: 11
Training loss: 3.056607961654663
Validation loss: 2.3598247189675607

Epoch: 6| Step: 12
Training loss: 2.419902801513672
Validation loss: 2.35097090659603

Epoch: 6| Step: 13
Training loss: 2.6021740436553955
Validation loss: 2.3541705710913545

Epoch: 238| Step: 0
Training loss: 3.183040142059326
Validation loss: 2.3509349617906796

Epoch: 6| Step: 1
Training loss: 2.599827289581299
Validation loss: 2.3583090177146335

Epoch: 6| Step: 2
Training loss: 2.4752559661865234
Validation loss: 2.3559015053574757

Epoch: 6| Step: 3
Training loss: 2.1598727703094482
Validation loss: 2.3628848137394076

Epoch: 6| Step: 4
Training loss: 2.9477012157440186
Validation loss: 2.3600134516275055

Epoch: 6| Step: 5
Training loss: 2.663001537322998
Validation loss: 2.367740951558595

Epoch: 6| Step: 6
Training loss: 2.6357192993164062
Validation loss: 2.372906343911284

Epoch: 6| Step: 7
Training loss: 2.093966484069824
Validation loss: 2.371740505259524

Epoch: 6| Step: 8
Training loss: 2.176743984222412
Validation loss: 2.3710453741012083

Epoch: 6| Step: 9
Training loss: 2.5697779655456543
Validation loss: 2.3729287962759695

Epoch: 6| Step: 10
Training loss: 1.962132215499878
Validation loss: 2.3716344448827926

Epoch: 6| Step: 11
Training loss: 2.752382278442383
Validation loss: 2.377168719486524

Epoch: 6| Step: 12
Training loss: 3.075542688369751
Validation loss: 2.3881383147290958

Epoch: 6| Step: 13
Training loss: 2.7860333919525146
Validation loss: 2.396662122459822

Epoch: 239| Step: 0
Training loss: 1.8649046421051025
Validation loss: 2.401369253794352

Epoch: 6| Step: 1
Training loss: 2.763798713684082
Validation loss: 2.400123252663561

Epoch: 6| Step: 2
Training loss: 3.680119514465332
Validation loss: 2.4069202305168234

Epoch: 6| Step: 3
Training loss: 2.574615716934204
Validation loss: 2.40670903395581

Epoch: 6| Step: 4
Training loss: 1.7488890886306763
Validation loss: 2.3961064482247956

Epoch: 6| Step: 5
Training loss: 2.8059070110321045
Validation loss: 2.3848263602102957

Epoch: 6| Step: 6
Training loss: 1.9193894863128662
Validation loss: 2.377152096840643

Epoch: 6| Step: 7
Training loss: 2.671358585357666
Validation loss: 2.3757317707102787

Epoch: 6| Step: 8
Training loss: 2.3276660442352295
Validation loss: 2.3718317785570697

Epoch: 6| Step: 9
Training loss: 2.730548858642578
Validation loss: 2.3741901228504796

Epoch: 6| Step: 10
Training loss: 2.699246406555176
Validation loss: 2.3620290346043085

Epoch: 6| Step: 11
Training loss: 2.4594664573669434
Validation loss: 2.3508633362349642

Epoch: 6| Step: 12
Training loss: 2.8247766494750977
Validation loss: 2.3514451608862927

Epoch: 6| Step: 13
Training loss: 3.3361828327178955
Validation loss: 2.3521217505137124

Epoch: 240| Step: 0
Training loss: 1.468026876449585
Validation loss: 2.3511179852229294

Epoch: 6| Step: 1
Training loss: 2.3397984504699707
Validation loss: 2.34034231657623

Epoch: 6| Step: 2
Training loss: 2.8335819244384766
Validation loss: 2.340734061374459

Epoch: 6| Step: 3
Training loss: 2.717517375946045
Validation loss: 2.351496578544699

Epoch: 6| Step: 4
Training loss: 2.261612892150879
Validation loss: 2.3561551596528743

Epoch: 6| Step: 5
Training loss: 3.1044812202453613
Validation loss: 2.3635152232262397

Epoch: 6| Step: 6
Training loss: 2.4881463050842285
Validation loss: 2.365660993001794

Epoch: 6| Step: 7
Training loss: 2.946927547454834
Validation loss: 2.3652263251684045

Epoch: 6| Step: 8
Training loss: 2.360858917236328
Validation loss: 2.3869569275968816

Epoch: 6| Step: 9
Training loss: 2.7765727043151855
Validation loss: 2.385894480571952

Epoch: 6| Step: 10
Training loss: 2.401622772216797
Validation loss: 2.390289075912968

Epoch: 6| Step: 11
Training loss: 3.1282918453216553
Validation loss: 2.391991981896021

Epoch: 6| Step: 12
Training loss: 2.4938719272613525
Validation loss: 2.390355643405709

Epoch: 6| Step: 13
Training loss: 2.645465612411499
Validation loss: 2.3805369664263982

Epoch: 241| Step: 0
Training loss: 3.292440891265869
Validation loss: 2.366391933092507

Epoch: 6| Step: 1
Training loss: 2.510958194732666
Validation loss: 2.3637561849368516

Epoch: 6| Step: 2
Training loss: 2.6095147132873535
Validation loss: 2.361102892506507

Epoch: 6| Step: 3
Training loss: 2.128247022628784
Validation loss: 2.359637083545808

Epoch: 6| Step: 4
Training loss: 2.980823040008545
Validation loss: 2.3620618517680834

Epoch: 6| Step: 5
Training loss: 2.7105374336242676
Validation loss: 2.355939071665528

Epoch: 6| Step: 6
Training loss: 2.540426254272461
Validation loss: 2.3458817876795286

Epoch: 6| Step: 7
Training loss: 2.3296849727630615
Validation loss: 2.355477520214614

Epoch: 6| Step: 8
Training loss: 2.0063328742980957
Validation loss: 2.355646341077743

Epoch: 6| Step: 9
Training loss: 2.405137300491333
Validation loss: 2.3646173451536443

Epoch: 6| Step: 10
Training loss: 2.078809976577759
Validation loss: 2.3671200403603176

Epoch: 6| Step: 11
Training loss: 3.09952449798584
Validation loss: 2.376341019907305

Epoch: 6| Step: 12
Training loss: 2.372633934020996
Validation loss: 2.3785526726835515

Epoch: 6| Step: 13
Training loss: 3.2192506790161133
Validation loss: 2.3758366953942085

Epoch: 242| Step: 0
Training loss: 2.602120876312256
Validation loss: 2.3799008297663864

Epoch: 6| Step: 1
Training loss: 2.2794384956359863
Validation loss: 2.3770552527519966

Epoch: 6| Step: 2
Training loss: 3.0523762702941895
Validation loss: 2.3994257398830947

Epoch: 6| Step: 3
Training loss: 2.458118438720703
Validation loss: 2.3893157359092467

Epoch: 6| Step: 4
Training loss: 2.161731243133545
Validation loss: 2.3953111274268037

Epoch: 6| Step: 5
Training loss: 2.381922721862793
Validation loss: 2.3773859598303355

Epoch: 6| Step: 6
Training loss: 2.906714916229248
Validation loss: 2.3794313271840415

Epoch: 6| Step: 7
Training loss: 2.975590705871582
Validation loss: 2.3687699379459506

Epoch: 6| Step: 8
Training loss: 2.853292465209961
Validation loss: 2.376962936052712

Epoch: 6| Step: 9
Training loss: 1.7661898136138916
Validation loss: 2.365113522416802

Epoch: 6| Step: 10
Training loss: 1.9811060428619385
Validation loss: 2.378556595053724

Epoch: 6| Step: 11
Training loss: 3.3963136672973633
Validation loss: 2.3635956215602096

Epoch: 6| Step: 12
Training loss: 2.5145301818847656
Validation loss: 2.365048218798894

Epoch: 6| Step: 13
Training loss: 2.353672504425049
Validation loss: 2.3616117200543805

Epoch: 243| Step: 0
Training loss: 2.7193450927734375
Validation loss: 2.358479135779924

Epoch: 6| Step: 1
Training loss: 2.480513095855713
Validation loss: 2.358136179626629

Epoch: 6| Step: 2
Training loss: 2.7117724418640137
Validation loss: 2.3622677582566456

Epoch: 6| Step: 3
Training loss: 2.560953140258789
Validation loss: 2.351241034846152

Epoch: 6| Step: 4
Training loss: 2.4942235946655273
Validation loss: 2.354804359456544

Epoch: 6| Step: 5
Training loss: 2.864840507507324
Validation loss: 2.3512306726107033

Epoch: 6| Step: 6
Training loss: 2.9227890968322754
Validation loss: 2.357872768114972

Epoch: 6| Step: 7
Training loss: 2.159714698791504
Validation loss: 2.3529465044698408

Epoch: 6| Step: 8
Training loss: 2.9567244052886963
Validation loss: 2.3639079934807232

Epoch: 6| Step: 9
Training loss: 2.0540237426757812
Validation loss: 2.3601713641997306

Epoch: 6| Step: 10
Training loss: 2.4562411308288574
Validation loss: 2.3704660118267102

Epoch: 6| Step: 11
Training loss: 2.739466667175293
Validation loss: 2.3735341205391833

Epoch: 6| Step: 12
Training loss: 1.911860704421997
Validation loss: 2.3747849067052207

Epoch: 6| Step: 13
Training loss: 2.7603447437286377
Validation loss: 2.369231918806671

Epoch: 244| Step: 0
Training loss: 1.9292142391204834
Validation loss: 2.3688725194623395

Epoch: 6| Step: 1
Training loss: 2.4125773906707764
Validation loss: 2.3710580179768224

Epoch: 6| Step: 2
Training loss: 3.0714645385742188
Validation loss: 2.375723336332588

Epoch: 6| Step: 3
Training loss: 2.2350125312805176
Validation loss: 2.3655539225506526

Epoch: 6| Step: 4
Training loss: 3.3732919692993164
Validation loss: 2.3639549298952987

Epoch: 6| Step: 5
Training loss: 2.4182839393615723
Validation loss: 2.3662928150546167

Epoch: 6| Step: 6
Training loss: 2.6900134086608887
Validation loss: 2.3524025024906283

Epoch: 6| Step: 7
Training loss: 1.8759651184082031
Validation loss: 2.341768782625916

Epoch: 6| Step: 8
Training loss: 2.73317551612854
Validation loss: 2.3593004249757334

Epoch: 6| Step: 9
Training loss: 2.733271837234497
Validation loss: 2.3562453357122277

Epoch: 6| Step: 10
Training loss: 2.727039337158203
Validation loss: 2.350246484561633

Epoch: 6| Step: 11
Training loss: 2.544206142425537
Validation loss: 2.3631662809720604

Epoch: 6| Step: 12
Training loss: 2.505727529525757
Validation loss: 2.361393174817485

Epoch: 6| Step: 13
Training loss: 2.5783283710479736
Validation loss: 2.35826624080699

Epoch: 245| Step: 0
Training loss: 3.1605207920074463
Validation loss: 2.3594476356301257

Epoch: 6| Step: 1
Training loss: 2.8148882389068604
Validation loss: 2.3611260088541175

Epoch: 6| Step: 2
Training loss: 1.6994630098342896
Validation loss: 2.365688588029595

Epoch: 6| Step: 3
Training loss: 3.3239216804504395
Validation loss: 2.3658945457909697

Epoch: 6| Step: 4
Training loss: 2.324221611022949
Validation loss: 2.3610095003599763

Epoch: 6| Step: 5
Training loss: 2.5113742351531982
Validation loss: 2.3713734483206146

Epoch: 6| Step: 6
Training loss: 3.383920431137085
Validation loss: 2.3623639050350396

Epoch: 6| Step: 7
Training loss: 2.4979000091552734
Validation loss: 2.3667447361894833

Epoch: 6| Step: 8
Training loss: 2.9906187057495117
Validation loss: 2.3696501639581498

Epoch: 6| Step: 9
Training loss: 2.1346216201782227
Validation loss: 2.376086324773809

Epoch: 6| Step: 10
Training loss: 1.8395944833755493
Validation loss: 2.3773031414196057

Epoch: 6| Step: 11
Training loss: 1.6327913999557495
Validation loss: 2.3756459066944737

Epoch: 6| Step: 12
Training loss: 2.866551399230957
Validation loss: 2.3729846272417294

Epoch: 6| Step: 13
Training loss: 2.5833773612976074
Validation loss: 2.378342233678346

Epoch: 246| Step: 0
Training loss: 2.984581708908081
Validation loss: 2.379723543761879

Epoch: 6| Step: 1
Training loss: 2.753276824951172
Validation loss: 2.3727216489853395

Epoch: 6| Step: 2
Training loss: 3.1286137104034424
Validation loss: 2.3721984958135955

Epoch: 6| Step: 3
Training loss: 1.9977535009384155
Validation loss: 2.386172027998073

Epoch: 6| Step: 4
Training loss: 2.576254367828369
Validation loss: 2.3797778314159763

Epoch: 6| Step: 5
Training loss: 2.440321207046509
Validation loss: 2.3680471707415838

Epoch: 6| Step: 6
Training loss: 2.6575889587402344
Validation loss: 2.3713048606790523

Epoch: 6| Step: 7
Training loss: 2.6383166313171387
Validation loss: 2.3762768981277302

Epoch: 6| Step: 8
Training loss: 2.7634360790252686
Validation loss: 2.3730293320071314

Epoch: 6| Step: 9
Training loss: 2.2308878898620605
Validation loss: 2.3728107483156267

Epoch: 6| Step: 10
Training loss: 2.281531572341919
Validation loss: 2.365598064596935

Epoch: 6| Step: 11
Training loss: 2.5525050163269043
Validation loss: 2.3713680326297717

Epoch: 6| Step: 12
Training loss: 2.6447763442993164
Validation loss: 2.376112038089383

Epoch: 6| Step: 13
Training loss: 1.5657771825790405
Validation loss: 2.3788693489566928

Epoch: 247| Step: 0
Training loss: 2.5637097358703613
Validation loss: 2.376894045901555

Epoch: 6| Step: 1
Training loss: 3.213019609451294
Validation loss: 2.3892346761559926

Epoch: 6| Step: 2
Training loss: 2.721086025238037
Validation loss: 2.393593108782204

Epoch: 6| Step: 3
Training loss: 2.8253350257873535
Validation loss: 2.4041322969621226

Epoch: 6| Step: 4
Training loss: 2.836874485015869
Validation loss: 2.417891930508357

Epoch: 6| Step: 5
Training loss: 2.1748878955841064
Validation loss: 2.4045009279763825

Epoch: 6| Step: 6
Training loss: 2.3882431983947754
Validation loss: 2.392370557272306

Epoch: 6| Step: 7
Training loss: 2.38429594039917
Validation loss: 2.3879911797021025

Epoch: 6| Step: 8
Training loss: 2.179762601852417
Validation loss: 2.3959292160567416

Epoch: 6| Step: 9
Training loss: 2.6542434692382812
Validation loss: 2.3798055110439176

Epoch: 6| Step: 10
Training loss: 2.94506573677063
Validation loss: 2.3677521187772035

Epoch: 6| Step: 11
Training loss: 2.591099262237549
Validation loss: 2.349512013055945

Epoch: 6| Step: 12
Training loss: 2.1635055541992188
Validation loss: 2.344119346269997

Epoch: 6| Step: 13
Training loss: 1.7695525884628296
Validation loss: 2.3449902175575175

Epoch: 248| Step: 0
Training loss: 2.4381518363952637
Validation loss: 2.346873414131903

Epoch: 6| Step: 1
Training loss: 2.0794882774353027
Validation loss: 2.3620737444969917

Epoch: 6| Step: 2
Training loss: 2.278137445449829
Validation loss: 2.3618349208626697

Epoch: 6| Step: 3
Training loss: 3.189016819000244
Validation loss: 2.364702437513618

Epoch: 6| Step: 4
Training loss: 2.764071464538574
Validation loss: 2.388110396682575

Epoch: 6| Step: 5
Training loss: 3.3057351112365723
Validation loss: 2.398175352363176

Epoch: 6| Step: 6
Training loss: 2.2398500442504883
Validation loss: 2.392718089524136

Epoch: 6| Step: 7
Training loss: 2.845484733581543
Validation loss: 2.381042016449795

Epoch: 6| Step: 8
Training loss: 2.3159117698669434
Validation loss: 2.360719368021975

Epoch: 6| Step: 9
Training loss: 2.6350274085998535
Validation loss: 2.358900575227635

Epoch: 6| Step: 10
Training loss: 2.0587692260742188
Validation loss: 2.366231941407727

Epoch: 6| Step: 11
Training loss: 2.2829391956329346
Validation loss: 2.360108301203738

Epoch: 6| Step: 12
Training loss: 2.544534683227539
Validation loss: 2.3730224588865876

Epoch: 6| Step: 13
Training loss: 2.9964094161987305
Validation loss: 2.373303908173756

Epoch: 249| Step: 0
Training loss: 2.359161376953125
Validation loss: 2.3702544345650622

Epoch: 6| Step: 1
Training loss: 2.3007349967956543
Validation loss: 2.3753940187474734

Epoch: 6| Step: 2
Training loss: 2.8397650718688965
Validation loss: 2.375072104956514

Epoch: 6| Step: 3
Training loss: 2.3305625915527344
Validation loss: 2.358572180553149

Epoch: 6| Step: 4
Training loss: 2.5442934036254883
Validation loss: 2.364060043006815

Epoch: 6| Step: 5
Training loss: 1.7455716133117676
Validation loss: 2.3498558485379784

Epoch: 6| Step: 6
Training loss: 1.9096157550811768
Validation loss: 2.3443941864916074

Epoch: 6| Step: 7
Training loss: 3.0937840938568115
Validation loss: 2.3433325418861966

Epoch: 6| Step: 8
Training loss: 2.550337791442871
Validation loss: 2.3448113805504254

Epoch: 6| Step: 9
Training loss: 2.3347368240356445
Validation loss: 2.352059054118331

Epoch: 6| Step: 10
Training loss: 3.202387809753418
Validation loss: 2.3468973341808526

Epoch: 6| Step: 11
Training loss: 2.711402177810669
Validation loss: 2.339429029854395

Epoch: 6| Step: 12
Training loss: 2.870894432067871
Validation loss: 2.333652683483657

Epoch: 6| Step: 13
Training loss: 3.07289457321167
Validation loss: 2.3375590283383607

Epoch: 250| Step: 0
Training loss: 2.0861122608184814
Validation loss: 2.328409456437634

Epoch: 6| Step: 1
Training loss: 3.0433311462402344
Validation loss: 2.3333489305229596

Epoch: 6| Step: 2
Training loss: 2.8625900745391846
Validation loss: 2.3323069182775353

Epoch: 6| Step: 3
Training loss: 2.544069766998291
Validation loss: 2.3423041425725466

Epoch: 6| Step: 4
Training loss: 2.121537446975708
Validation loss: 2.357495561722786

Epoch: 6| Step: 5
Training loss: 1.793677806854248
Validation loss: 2.359488002715572

Epoch: 6| Step: 6
Training loss: 2.3904080390930176
Validation loss: 2.3723718850843367

Epoch: 6| Step: 7
Training loss: 3.236478090286255
Validation loss: 2.363551334668231

Epoch: 6| Step: 8
Training loss: 3.066654920578003
Validation loss: 2.3623405528324906

Epoch: 6| Step: 9
Training loss: 2.257978916168213
Validation loss: 2.372594723137476

Epoch: 6| Step: 10
Training loss: 2.1827378273010254
Validation loss: 2.37514373307587

Epoch: 6| Step: 11
Training loss: 2.966050624847412
Validation loss: 2.365872795863818

Epoch: 6| Step: 12
Training loss: 2.6340529918670654
Validation loss: 2.3589882940374394

Epoch: 6| Step: 13
Training loss: 2.445244550704956
Validation loss: 2.3565081857865855

Epoch: 251| Step: 0
Training loss: 2.0230603218078613
Validation loss: 2.3454035738463044

Epoch: 6| Step: 1
Training loss: 2.5558395385742188
Validation loss: 2.3495357921046596

Epoch: 6| Step: 2
Training loss: 1.5119593143463135
Validation loss: 2.3500331089060795

Epoch: 6| Step: 3
Training loss: 2.537590503692627
Validation loss: 2.3597725616988314

Epoch: 6| Step: 4
Training loss: 2.3666584491729736
Validation loss: 2.3641969901259228

Epoch: 6| Step: 5
Training loss: 2.9166574478149414
Validation loss: 2.3771137037584857

Epoch: 6| Step: 6
Training loss: 3.2893457412719727
Validation loss: 2.3862219625903713

Epoch: 6| Step: 7
Training loss: 2.736255168914795
Validation loss: 2.3855788887188

Epoch: 6| Step: 8
Training loss: 2.6686348915100098
Validation loss: 2.382021942446309

Epoch: 6| Step: 9
Training loss: 1.811314344406128
Validation loss: 2.394126820307906

Epoch: 6| Step: 10
Training loss: 2.936753273010254
Validation loss: 2.3699514840238836

Epoch: 6| Step: 11
Training loss: 2.5939230918884277
Validation loss: 2.3571526081331315

Epoch: 6| Step: 12
Training loss: 2.6144349575042725
Validation loss: 2.342219352722168

Epoch: 6| Step: 13
Training loss: 3.304478883743286
Validation loss: 2.349784722892187

Epoch: 252| Step: 0
Training loss: 2.962575912475586
Validation loss: 2.346679456772343

Epoch: 6| Step: 1
Training loss: 2.068795919418335
Validation loss: 2.350108401749724

Epoch: 6| Step: 2
Training loss: 2.384941577911377
Validation loss: 2.3720499264296664

Epoch: 6| Step: 3
Training loss: 2.6048476696014404
Validation loss: 2.3640440023073586

Epoch: 6| Step: 4
Training loss: 2.2785181999206543
Validation loss: 2.37956335467677

Epoch: 6| Step: 5
Training loss: 2.481962203979492
Validation loss: 2.398299224915043

Epoch: 6| Step: 6
Training loss: 3.1225428581237793
Validation loss: 2.388959789788851

Epoch: 6| Step: 7
Training loss: 2.2876622676849365
Validation loss: 2.3917104915906022

Epoch: 6| Step: 8
Training loss: 2.666907548904419
Validation loss: 2.3698393965280182

Epoch: 6| Step: 9
Training loss: 3.025465965270996
Validation loss: 2.355098560292234

Epoch: 6| Step: 10
Training loss: 3.1336779594421387
Validation loss: 2.3462530592436432

Epoch: 6| Step: 11
Training loss: 1.9313408136367798
Validation loss: 2.332975128645538

Epoch: 6| Step: 12
Training loss: 2.413703680038452
Validation loss: 2.328676403209727

Epoch: 6| Step: 13
Training loss: 2.7605350017547607
Validation loss: 2.3210777313478532

Epoch: 253| Step: 0
Training loss: 2.3343849182128906
Validation loss: 2.325724288981448

Epoch: 6| Step: 1
Training loss: 3.068791389465332
Validation loss: 2.3377434233183503

Epoch: 6| Step: 2
Training loss: 2.7944114208221436
Validation loss: 2.3483004339279665

Epoch: 6| Step: 3
Training loss: 2.861771821975708
Validation loss: 2.364519211553758

Epoch: 6| Step: 4
Training loss: 2.1184322834014893
Validation loss: 2.375843555696549

Epoch: 6| Step: 5
Training loss: 2.227409601211548
Validation loss: 2.3671679650583575

Epoch: 6| Step: 6
Training loss: 2.719513416290283
Validation loss: 2.378033309854487

Epoch: 6| Step: 7
Training loss: 3.2623226642608643
Validation loss: 2.3652943564999487

Epoch: 6| Step: 8
Training loss: 1.9307944774627686
Validation loss: 2.36120286808219

Epoch: 6| Step: 9
Training loss: 1.843510389328003
Validation loss: 2.3644216829730618

Epoch: 6| Step: 10
Training loss: 2.5879528522491455
Validation loss: 2.365529429528021

Epoch: 6| Step: 11
Training loss: 2.941746473312378
Validation loss: 2.3768007575824694

Epoch: 6| Step: 12
Training loss: 2.675212860107422
Validation loss: 2.378817617252309

Epoch: 6| Step: 13
Training loss: 1.9743620157241821
Validation loss: 2.37791411594678

Epoch: 254| Step: 0
Training loss: 3.0477709770202637
Validation loss: 2.3780367118056103

Epoch: 6| Step: 1
Training loss: 2.103837013244629
Validation loss: 2.368404742210142

Epoch: 6| Step: 2
Training loss: 3.0972275733947754
Validation loss: 2.3613281916546565

Epoch: 6| Step: 3
Training loss: 2.156496524810791
Validation loss: 2.3661105196963073

Epoch: 6| Step: 4
Training loss: 2.4676966667175293
Validation loss: 2.3583173956922305

Epoch: 6| Step: 5
Training loss: 2.5280752182006836
Validation loss: 2.357917211389029

Epoch: 6| Step: 6
Training loss: 2.3721702098846436
Validation loss: 2.361585488883398

Epoch: 6| Step: 7
Training loss: 3.0073347091674805
Validation loss: 2.3642043477745465

Epoch: 6| Step: 8
Training loss: 2.961261749267578
Validation loss: 2.3577131327762397

Epoch: 6| Step: 9
Training loss: 2.0099306106567383
Validation loss: 2.360347701657203

Epoch: 6| Step: 10
Training loss: 2.236294746398926
Validation loss: 2.3566889711605605

Epoch: 6| Step: 11
Training loss: 2.0667407512664795
Validation loss: 2.354122716893432

Epoch: 6| Step: 12
Training loss: 2.4442811012268066
Validation loss: 2.3547756466814267

Epoch: 6| Step: 13
Training loss: 3.2507972717285156
Validation loss: 2.3568495986282185

Epoch: 255| Step: 0
Training loss: 2.445582389831543
Validation loss: 2.3607046411883448

Epoch: 6| Step: 1
Training loss: 2.493776559829712
Validation loss: 2.35389179952683

Epoch: 6| Step: 2
Training loss: 2.482884168624878
Validation loss: 2.361169481790194

Epoch: 6| Step: 3
Training loss: 2.9452083110809326
Validation loss: 2.3637672547371156

Epoch: 6| Step: 4
Training loss: 1.8862539529800415
Validation loss: 2.3622478823507986

Epoch: 6| Step: 5
Training loss: 1.8091453313827515
Validation loss: 2.372237140132535

Epoch: 6| Step: 6
Training loss: 2.9468159675598145
Validation loss: 2.3631985187530518

Epoch: 6| Step: 7
Training loss: 2.139493227005005
Validation loss: 2.37652983204011

Epoch: 6| Step: 8
Training loss: 2.381413698196411
Validation loss: 2.3865173080916047

Epoch: 6| Step: 9
Training loss: 2.268950939178467
Validation loss: 2.3772030107436644

Epoch: 6| Step: 10
Training loss: 2.818354606628418
Validation loss: 2.3811080353234404

Epoch: 6| Step: 11
Training loss: 3.114908456802368
Validation loss: 2.380918933499244

Epoch: 6| Step: 12
Training loss: 2.8533711433410645
Validation loss: 2.38033034980938

Epoch: 6| Step: 13
Training loss: 3.027837038040161
Validation loss: 2.3770366637937483

Epoch: 256| Step: 0
Training loss: 2.9702701568603516
Validation loss: 2.382500098597619

Epoch: 6| Step: 1
Training loss: 2.519765853881836
Validation loss: 2.3767010114526235

Epoch: 6| Step: 2
Training loss: 2.7914345264434814
Validation loss: 2.3752604966522544

Epoch: 6| Step: 3
Training loss: 2.173508644104004
Validation loss: 2.3609680668000252

Epoch: 6| Step: 4
Training loss: 2.3717689514160156
Validation loss: 2.359158132665901

Epoch: 6| Step: 5
Training loss: 2.6182680130004883
Validation loss: 2.3548843194079656

Epoch: 6| Step: 6
Training loss: 2.2063655853271484
Validation loss: 2.3526617403953307

Epoch: 6| Step: 7
Training loss: 2.3397293090820312
Validation loss: 2.357728681256694

Epoch: 6| Step: 8
Training loss: 2.41372013092041
Validation loss: 2.351521074130971

Epoch: 6| Step: 9
Training loss: 2.4803965091705322
Validation loss: 2.360803274698155

Epoch: 6| Step: 10
Training loss: 2.22544527053833
Validation loss: 2.35402278618146

Epoch: 6| Step: 11
Training loss: 3.282470941543579
Validation loss: 2.348094727403374

Epoch: 6| Step: 12
Training loss: 2.8524248600006104
Validation loss: 2.356242528525732

Epoch: 6| Step: 13
Training loss: 1.8940660953521729
Validation loss: 2.351504515576106

Epoch: 257| Step: 0
Training loss: 2.4058046340942383
Validation loss: 2.34458993070869

Epoch: 6| Step: 1
Training loss: 2.645263195037842
Validation loss: 2.348491358500655

Epoch: 6| Step: 2
Training loss: 2.0098159313201904
Validation loss: 2.3450967983532975

Epoch: 6| Step: 3
Training loss: 2.447127342224121
Validation loss: 2.3535244913511377

Epoch: 6| Step: 4
Training loss: 2.9989380836486816
Validation loss: 2.3606429176945842

Epoch: 6| Step: 5
Training loss: 2.4351186752319336
Validation loss: 2.3685130380815074

Epoch: 6| Step: 6
Training loss: 2.1075196266174316
Validation loss: 2.3639814315303678

Epoch: 6| Step: 7
Training loss: 2.7828636169433594
Validation loss: 2.353557802015735

Epoch: 6| Step: 8
Training loss: 2.3815016746520996
Validation loss: 2.3486992338652253

Epoch: 6| Step: 9
Training loss: 2.261380672454834
Validation loss: 2.350054625541933

Epoch: 6| Step: 10
Training loss: 2.934947967529297
Validation loss: 2.3402554924770067

Epoch: 6| Step: 11
Training loss: 2.3237221240997314
Validation loss: 2.357330970866706

Epoch: 6| Step: 12
Training loss: 3.552517890930176
Validation loss: 2.3648958001085507

Epoch: 6| Step: 13
Training loss: 1.7295523881912231
Validation loss: 2.354519437718135

Epoch: 258| Step: 0
Training loss: 2.5556387901306152
Validation loss: 2.3483735874135006

Epoch: 6| Step: 1
Training loss: 3.276885986328125
Validation loss: 2.3506170216427056

Epoch: 6| Step: 2
Training loss: 2.2251226902008057
Validation loss: 2.3554940454421507

Epoch: 6| Step: 3
Training loss: 3.3479909896850586
Validation loss: 2.3532447353486092

Epoch: 6| Step: 4
Training loss: 2.3295233249664307
Validation loss: 2.3512617259897213

Epoch: 6| Step: 5
Training loss: 3.086228847503662
Validation loss: 2.353419639730966

Epoch: 6| Step: 6
Training loss: 1.608290195465088
Validation loss: 2.354678384719356

Epoch: 6| Step: 7
Training loss: 2.152836322784424
Validation loss: 2.3513796585862354

Epoch: 6| Step: 8
Training loss: 2.5160794258117676
Validation loss: 2.3474334568105717

Epoch: 6| Step: 9
Training loss: 2.544398307800293
Validation loss: 2.352687669056718

Epoch: 6| Step: 10
Training loss: 2.7150354385375977
Validation loss: 2.3502172962311776

Epoch: 6| Step: 11
Training loss: 1.9884581565856934
Validation loss: 2.346389628225757

Epoch: 6| Step: 12
Training loss: 2.323967695236206
Validation loss: 2.334657694703789

Epoch: 6| Step: 13
Training loss: 2.9675521850585938
Validation loss: 2.3382166995797107

Epoch: 259| Step: 0
Training loss: 3.887531042098999
Validation loss: 2.340862899698237

Epoch: 6| Step: 1
Training loss: 2.311418056488037
Validation loss: 2.3336933966605895

Epoch: 6| Step: 2
Training loss: 3.018934726715088
Validation loss: 2.3327562937172512

Epoch: 6| Step: 3
Training loss: 2.085662364959717
Validation loss: 2.3410289646476827

Epoch: 6| Step: 4
Training loss: 2.575915813446045
Validation loss: 2.3451241395806752

Epoch: 6| Step: 5
Training loss: 2.8023667335510254
Validation loss: 2.362725983383835

Epoch: 6| Step: 6
Training loss: 2.3336267471313477
Validation loss: 2.3879995910070275

Epoch: 6| Step: 7
Training loss: 1.8903802633285522
Validation loss: 2.3978499071572417

Epoch: 6| Step: 8
Training loss: 2.132827043533325
Validation loss: 2.4056174832005657

Epoch: 6| Step: 9
Training loss: 1.8378980159759521
Validation loss: 2.4143266242037535

Epoch: 6| Step: 10
Training loss: 2.0060372352600098
Validation loss: 2.4197641777735885

Epoch: 6| Step: 11
Training loss: 2.9472341537475586
Validation loss: 2.3949458752909014

Epoch: 6| Step: 12
Training loss: 2.9090373516082764
Validation loss: 2.3797187779539373

Epoch: 6| Step: 13
Training loss: 2.9965808391571045
Validation loss: 2.35890535641742

Epoch: 260| Step: 0
Training loss: 1.9627732038497925
Validation loss: 2.3477648329991165

Epoch: 6| Step: 1
Training loss: 2.933359146118164
Validation loss: 2.327087486943891

Epoch: 6| Step: 2
Training loss: 1.7660667896270752
Validation loss: 2.334451652342273

Epoch: 6| Step: 3
Training loss: 2.817248821258545
Validation loss: 2.3349810210607385

Epoch: 6| Step: 4
Training loss: 1.7944356203079224
Validation loss: 2.3447430146637784

Epoch: 6| Step: 5
Training loss: 2.4712300300598145
Validation loss: 2.3518542884498514

Epoch: 6| Step: 6
Training loss: 3.2254199981689453
Validation loss: 2.3422894836753927

Epoch: 6| Step: 7
Training loss: 2.9165141582489014
Validation loss: 2.3456137718692904

Epoch: 6| Step: 8
Training loss: 2.396845817565918
Validation loss: 2.34170897288989

Epoch: 6| Step: 9
Training loss: 2.2992563247680664
Validation loss: 2.343126425179102

Epoch: 6| Step: 10
Training loss: 2.788996934890747
Validation loss: 2.3468788669955347

Epoch: 6| Step: 11
Training loss: 2.2025556564331055
Validation loss: 2.3430305680921

Epoch: 6| Step: 12
Training loss: 3.2140400409698486
Validation loss: 2.3440115477449153

Epoch: 6| Step: 13
Training loss: 2.4120583534240723
Validation loss: 2.341209858976385

Epoch: 261| Step: 0
Training loss: 2.4807159900665283
Validation loss: 2.3521836726896224

Epoch: 6| Step: 1
Training loss: 3.433326244354248
Validation loss: 2.3609542769770466

Epoch: 6| Step: 2
Training loss: 2.466491937637329
Validation loss: 2.3506176497346614

Epoch: 6| Step: 3
Training loss: 2.057335376739502
Validation loss: 2.3582050364504576

Epoch: 6| Step: 4
Training loss: 2.9192628860473633
Validation loss: 2.3492231522836993

Epoch: 6| Step: 5
Training loss: 3.247711658477783
Validation loss: 2.3388089774757304

Epoch: 6| Step: 6
Training loss: 2.1438188552856445
Validation loss: 2.3413142952867734

Epoch: 6| Step: 7
Training loss: 2.180595636367798
Validation loss: 2.3385162327879216

Epoch: 6| Step: 8
Training loss: 2.6635689735412598
Validation loss: 2.3416424130880706

Epoch: 6| Step: 9
Training loss: 2.193783760070801
Validation loss: 2.354384596629809

Epoch: 6| Step: 10
Training loss: 2.616875171661377
Validation loss: 2.358180671609858

Epoch: 6| Step: 11
Training loss: 2.4810614585876465
Validation loss: 2.3572238901610016

Epoch: 6| Step: 12
Training loss: 2.082582950592041
Validation loss: 2.358746328661519

Epoch: 6| Step: 13
Training loss: 2.15942645072937
Validation loss: 2.3484189535981868

Epoch: 262| Step: 0
Training loss: 3.0917181968688965
Validation loss: 2.355002421204762

Epoch: 6| Step: 1
Training loss: 2.8368351459503174
Validation loss: 2.348469823919317

Epoch: 6| Step: 2
Training loss: 3.0148305892944336
Validation loss: 2.3431656668263097

Epoch: 6| Step: 3
Training loss: 2.458747625350952
Validation loss: 2.3476254324759207

Epoch: 6| Step: 4
Training loss: 3.2324748039245605
Validation loss: 2.3463500699689313

Epoch: 6| Step: 5
Training loss: 1.9021220207214355
Validation loss: 2.349470353895618

Epoch: 6| Step: 6
Training loss: 3.2869577407836914
Validation loss: 2.3453822751199045

Epoch: 6| Step: 7
Training loss: 1.8189327716827393
Validation loss: 2.355494576115762

Epoch: 6| Step: 8
Training loss: 2.174107313156128
Validation loss: 2.353711784526866

Epoch: 6| Step: 9
Training loss: 2.2592968940734863
Validation loss: 2.357202358143304

Epoch: 6| Step: 10
Training loss: 2.8842556476593018
Validation loss: 2.3556627765778573

Epoch: 6| Step: 11
Training loss: 2.0064985752105713
Validation loss: 2.354594535725091

Epoch: 6| Step: 12
Training loss: 2.0132179260253906
Validation loss: 2.3435082371516893

Epoch: 6| Step: 13
Training loss: 1.9388593435287476
Validation loss: 2.348198644576534

Epoch: 263| Step: 0
Training loss: 2.138655185699463
Validation loss: 2.3486684060865834

Epoch: 6| Step: 1
Training loss: 2.975829601287842
Validation loss: 2.330816756012619

Epoch: 6| Step: 2
Training loss: 2.9070682525634766
Validation loss: 2.33871828997007

Epoch: 6| Step: 3
Training loss: 2.4573349952697754
Validation loss: 2.334794993041664

Epoch: 6| Step: 4
Training loss: 2.660499095916748
Validation loss: 2.335491612393369

Epoch: 6| Step: 5
Training loss: 2.8119051456451416
Validation loss: 2.3281741014090915

Epoch: 6| Step: 6
Training loss: 2.4552202224731445
Validation loss: 2.3295491587731147

Epoch: 6| Step: 7
Training loss: 1.8416955471038818
Validation loss: 2.336026196838707

Epoch: 6| Step: 8
Training loss: 2.6537702083587646
Validation loss: 2.331459517120033

Epoch: 6| Step: 9
Training loss: 2.5306873321533203
Validation loss: 2.347290410790392

Epoch: 6| Step: 10
Training loss: 2.7062580585479736
Validation loss: 2.338679903297014

Epoch: 6| Step: 11
Training loss: 2.5976617336273193
Validation loss: 2.3601476043783207

Epoch: 6| Step: 12
Training loss: 2.128945827484131
Validation loss: 2.367236637300061

Epoch: 6| Step: 13
Training loss: 2.1213033199310303
Validation loss: 2.379452020891251

Epoch: 264| Step: 0
Training loss: 2.1405458450317383
Validation loss: 2.382140464680169

Epoch: 6| Step: 1
Training loss: 2.5540847778320312
Validation loss: 2.365650705111924

Epoch: 6| Step: 2
Training loss: 2.072857618331909
Validation loss: 2.359478558263471

Epoch: 6| Step: 3
Training loss: 3.6718833446502686
Validation loss: 2.34027773334134

Epoch: 6| Step: 4
Training loss: 2.1096973419189453
Validation loss: 2.3313926496813373

Epoch: 6| Step: 5
Training loss: 2.6894102096557617
Validation loss: 2.324853766349054

Epoch: 6| Step: 6
Training loss: 2.9290242195129395
Validation loss: 2.322655503467847

Epoch: 6| Step: 7
Training loss: 1.9208011627197266
Validation loss: 2.313802480697632

Epoch: 6| Step: 8
Training loss: 2.1512885093688965
Validation loss: 2.30659774298309

Epoch: 6| Step: 9
Training loss: 3.106651782989502
Validation loss: 2.315354638202216

Epoch: 6| Step: 10
Training loss: 2.7679080963134766
Validation loss: 2.314493158812164

Epoch: 6| Step: 11
Training loss: 2.3846473693847656
Validation loss: 2.31621523826353

Epoch: 6| Step: 12
Training loss: 2.451188087463379
Validation loss: 2.3192659116560415

Epoch: 6| Step: 13
Training loss: 2.19858455657959
Validation loss: 2.3131602297547045

Epoch: 265| Step: 0
Training loss: 2.3064403533935547
Validation loss: 2.3106386264165244

Epoch: 6| Step: 1
Training loss: 2.062417984008789
Validation loss: 2.326531981909147

Epoch: 6| Step: 2
Training loss: 2.629671096801758
Validation loss: 2.321510389286985

Epoch: 6| Step: 3
Training loss: 2.7298073768615723
Validation loss: 2.325344257457282

Epoch: 6| Step: 4
Training loss: 2.449091672897339
Validation loss: 2.3144765412935646

Epoch: 6| Step: 5
Training loss: 2.451530933380127
Validation loss: 2.315470587822699

Epoch: 6| Step: 6
Training loss: 2.126079559326172
Validation loss: 2.320001748300368

Epoch: 6| Step: 7
Training loss: 2.0022473335266113
Validation loss: 2.3198545889187883

Epoch: 6| Step: 8
Training loss: 3.1594314575195312
Validation loss: 2.319132202415056

Epoch: 6| Step: 9
Training loss: 2.5681066513061523
Validation loss: 2.324323558038281

Epoch: 6| Step: 10
Training loss: 2.786642074584961
Validation loss: 2.3438921974551294

Epoch: 6| Step: 11
Training loss: 2.5929245948791504
Validation loss: 2.3435207797634985

Epoch: 6| Step: 12
Training loss: 2.5310018062591553
Validation loss: 2.3588073099813154

Epoch: 6| Step: 13
Training loss: 3.2312982082366943
Validation loss: 2.3525658909992506

Epoch: 266| Step: 0
Training loss: 2.4165713787078857
Validation loss: 2.3529557566488943

Epoch: 6| Step: 1
Training loss: 2.767031669616699
Validation loss: 2.35403351629934

Epoch: 6| Step: 2
Training loss: 2.504241943359375
Validation loss: 2.341990250413136

Epoch: 6| Step: 3
Training loss: 2.389113426208496
Validation loss: 2.325584319330031

Epoch: 6| Step: 4
Training loss: 2.6330108642578125
Validation loss: 2.31712773794769

Epoch: 6| Step: 5
Training loss: 2.8481860160827637
Validation loss: 2.314735451052266

Epoch: 6| Step: 6
Training loss: 2.838562488555908
Validation loss: 2.301348832345778

Epoch: 6| Step: 7
Training loss: 1.863127589225769
Validation loss: 2.311186928902903

Epoch: 6| Step: 8
Training loss: 3.407346725463867
Validation loss: 2.309128963819114

Epoch: 6| Step: 9
Training loss: 2.521907091140747
Validation loss: 2.3062035191443657

Epoch: 6| Step: 10
Training loss: 1.6787819862365723
Validation loss: 2.3078626817272556

Epoch: 6| Step: 11
Training loss: 3.1464426517486572
Validation loss: 2.3214029983807634

Epoch: 6| Step: 12
Training loss: 2.267011880874634
Validation loss: 2.331419760181058

Epoch: 6| Step: 13
Training loss: 1.4655276536941528
Validation loss: 2.3437315828056744

Epoch: 267| Step: 0
Training loss: 3.227123975753784
Validation loss: 2.38791932341873

Epoch: 6| Step: 1
Training loss: 2.4790658950805664
Validation loss: 2.4149093653566096

Epoch: 6| Step: 2
Training loss: 1.7997784614562988
Validation loss: 2.418108509432885

Epoch: 6| Step: 3
Training loss: 1.934819221496582
Validation loss: 2.4224390381125995

Epoch: 6| Step: 4
Training loss: 2.561580181121826
Validation loss: 2.406414039673344

Epoch: 6| Step: 5
Training loss: 2.5459890365600586
Validation loss: 2.3946067492167153

Epoch: 6| Step: 6
Training loss: 3.1580281257629395
Validation loss: 2.3752666134988107

Epoch: 6| Step: 7
Training loss: 2.7589471340179443
Validation loss: 2.372622641183997

Epoch: 6| Step: 8
Training loss: 2.7410449981689453
Validation loss: 2.359166231206668

Epoch: 6| Step: 9
Training loss: 2.193690538406372
Validation loss: 2.3449927350526214

Epoch: 6| Step: 10
Training loss: 2.155790090560913
Validation loss: 2.332592600135393

Epoch: 6| Step: 11
Training loss: 2.1344640254974365
Validation loss: 2.3336081069002867

Epoch: 6| Step: 12
Training loss: 3.453085422515869
Validation loss: 2.328387527055638

Epoch: 6| Step: 13
Training loss: 1.7151565551757812
Validation loss: 2.325276541453536

Epoch: 268| Step: 0
Training loss: 1.6969218254089355
Validation loss: 2.3296792558444444

Epoch: 6| Step: 1
Training loss: 2.838057041168213
Validation loss: 2.3263363889468613

Epoch: 6| Step: 2
Training loss: 1.8573963642120361
Validation loss: 2.3293019597248366

Epoch: 6| Step: 3
Training loss: 2.0718913078308105
Validation loss: 2.3218923512325493

Epoch: 6| Step: 4
Training loss: 2.644542694091797
Validation loss: 2.32572276361527

Epoch: 6| Step: 5
Training loss: 1.8483545780181885
Validation loss: 2.3290480413744525

Epoch: 6| Step: 6
Training loss: 3.090362548828125
Validation loss: 2.3413806640973656

Epoch: 6| Step: 7
Training loss: 2.423882484436035
Validation loss: 2.3505023346152356

Epoch: 6| Step: 8
Training loss: 3.328352212905884
Validation loss: 2.369057019551595

Epoch: 6| Step: 9
Training loss: 2.806623935699463
Validation loss: 2.3642709639764603

Epoch: 6| Step: 10
Training loss: 3.3426766395568848
Validation loss: 2.362224260965983

Epoch: 6| Step: 11
Training loss: 2.2744107246398926
Validation loss: 2.353850333921371

Epoch: 6| Step: 12
Training loss: 2.286334991455078
Validation loss: 2.360707272765457

Epoch: 6| Step: 13
Training loss: 2.9635391235351562
Validation loss: 2.3554519863538843

Epoch: 269| Step: 0
Training loss: 2.0382561683654785
Validation loss: 2.354528800133736

Epoch: 6| Step: 1
Training loss: 2.3698699474334717
Validation loss: 2.344097847579628

Epoch: 6| Step: 2
Training loss: 2.6370177268981934
Validation loss: 2.343893348529775

Epoch: 6| Step: 3
Training loss: 2.454411029815674
Validation loss: 2.346265877446821

Epoch: 6| Step: 4
Training loss: 2.250925302505493
Validation loss: 2.3477218458729405

Epoch: 6| Step: 5
Training loss: 2.6474125385284424
Validation loss: 2.340597209110055

Epoch: 6| Step: 6
Training loss: 3.218545913696289
Validation loss: 2.3349173748365013

Epoch: 6| Step: 7
Training loss: 2.0978214740753174
Validation loss: 2.333462663876113

Epoch: 6| Step: 8
Training loss: 2.175891399383545
Validation loss: 2.323429871630925

Epoch: 6| Step: 9
Training loss: 2.437617778778076
Validation loss: 2.3219042695978636

Epoch: 6| Step: 10
Training loss: 2.195420026779175
Validation loss: 2.310707898550136

Epoch: 6| Step: 11
Training loss: 3.700875759124756
Validation loss: 2.312097869893556

Epoch: 6| Step: 12
Training loss: 2.509552478790283
Validation loss: 2.306700357826807

Epoch: 6| Step: 13
Training loss: 2.259209632873535
Validation loss: 2.312993029112457

Epoch: 270| Step: 0
Training loss: 2.442336082458496
Validation loss: 2.3030343158270723

Epoch: 6| Step: 1
Training loss: 2.279242992401123
Validation loss: 2.320221321557158

Epoch: 6| Step: 2
Training loss: 3.0321030616760254
Validation loss: 2.3040719186106036

Epoch: 6| Step: 3
Training loss: 1.8855180740356445
Validation loss: 2.293599546596568

Epoch: 6| Step: 4
Training loss: 2.7047271728515625
Validation loss: 2.292746200356432

Epoch: 6| Step: 5
Training loss: 2.611640453338623
Validation loss: 2.2990404636629167

Epoch: 6| Step: 6
Training loss: 3.032473087310791
Validation loss: 2.292852081278319

Epoch: 6| Step: 7
Training loss: 1.8585983514785767
Validation loss: 2.2999139549911662

Epoch: 6| Step: 8
Training loss: 2.440234661102295
Validation loss: 2.3028138222232943

Epoch: 6| Step: 9
Training loss: 3.57771372795105
Validation loss: 2.315113434227564

Epoch: 6| Step: 10
Training loss: 1.7414387464523315
Validation loss: 2.3236398901990665

Epoch: 6| Step: 11
Training loss: 2.079312324523926
Validation loss: 2.3312832129898893

Epoch: 6| Step: 12
Training loss: 2.761319160461426
Validation loss: 2.3329637024992254

Epoch: 6| Step: 13
Training loss: 3.0547590255737305
Validation loss: 2.3487708799300657

Epoch: 271| Step: 0
Training loss: 3.1058762073516846
Validation loss: 2.354484999051658

Epoch: 6| Step: 1
Training loss: 3.2849345207214355
Validation loss: 2.353329973836099

Epoch: 6| Step: 2
Training loss: 2.446500778198242
Validation loss: 2.3467084412933676

Epoch: 6| Step: 3
Training loss: 2.3939757347106934
Validation loss: 2.3532994306215675

Epoch: 6| Step: 4
Training loss: 2.935459613800049
Validation loss: 2.350260019302368

Epoch: 6| Step: 5
Training loss: 1.563145637512207
Validation loss: 2.344658878541762

Epoch: 6| Step: 6
Training loss: 2.882856845855713
Validation loss: 2.3445620203530915

Epoch: 6| Step: 7
Training loss: 2.334989547729492
Validation loss: 2.336182230262346

Epoch: 6| Step: 8
Training loss: 2.756829023361206
Validation loss: 2.339709694667529

Epoch: 6| Step: 9
Training loss: 2.520873546600342
Validation loss: 2.3408047588922645

Epoch: 6| Step: 10
Training loss: 2.354891777038574
Validation loss: 2.3357915545022614

Epoch: 6| Step: 11
Training loss: 2.212315082550049
Validation loss: 2.3390860890829437

Epoch: 6| Step: 12
Training loss: 2.130007743835449
Validation loss: 2.3427956065823956

Epoch: 6| Step: 13
Training loss: 2.084648609161377
Validation loss: 2.3371088889337357

Epoch: 272| Step: 0
Training loss: 2.817746639251709
Validation loss: 2.347330847094136

Epoch: 6| Step: 1
Training loss: 2.5813112258911133
Validation loss: 2.344163807489539

Epoch: 6| Step: 2
Training loss: 3.143887519836426
Validation loss: 2.3365636051342054

Epoch: 6| Step: 3
Training loss: 2.0981831550598145
Validation loss: 2.3236630244921614

Epoch: 6| Step: 4
Training loss: 2.44051456451416
Validation loss: 2.3168526029074066

Epoch: 6| Step: 5
Training loss: 2.553744316101074
Validation loss: 2.323263542626494

Epoch: 6| Step: 6
Training loss: 1.7188332080841064
Validation loss: 2.310179284823838

Epoch: 6| Step: 7
Training loss: 2.5692780017852783
Validation loss: 2.3186915689899075

Epoch: 6| Step: 8
Training loss: 2.614834785461426
Validation loss: 2.317273527063349

Epoch: 6| Step: 9
Training loss: 2.7154178619384766
Validation loss: 2.313788696001935

Epoch: 6| Step: 10
Training loss: 2.7626516819000244
Validation loss: 2.322504387106947

Epoch: 6| Step: 11
Training loss: 2.4378068447113037
Validation loss: 2.335775133102171

Epoch: 6| Step: 12
Training loss: 2.2785215377807617
Validation loss: 2.3292063641291794

Epoch: 6| Step: 13
Training loss: 2.199937105178833
Validation loss: 2.324322046772126

Epoch: 273| Step: 0
Training loss: 2.219059944152832
Validation loss: 2.339633341758482

Epoch: 6| Step: 1
Training loss: 2.0344276428222656
Validation loss: 2.341933440136653

Epoch: 6| Step: 2
Training loss: 3.2179200649261475
Validation loss: 2.346248024253435

Epoch: 6| Step: 3
Training loss: 2.475130796432495
Validation loss: 2.33998506299911

Epoch: 6| Step: 4
Training loss: 2.358009099960327
Validation loss: 2.325939078484812

Epoch: 6| Step: 5
Training loss: 2.2114555835723877
Validation loss: 2.3045893561455513

Epoch: 6| Step: 6
Training loss: 2.6542487144470215
Validation loss: 2.2976312278419413

Epoch: 6| Step: 7
Training loss: 2.330624580383301
Validation loss: 2.294058701043488

Epoch: 6| Step: 8
Training loss: 2.8262248039245605
Validation loss: 2.2985112077446392

Epoch: 6| Step: 9
Training loss: 2.627225637435913
Validation loss: 2.300637511796849

Epoch: 6| Step: 10
Training loss: 3.0412380695343018
Validation loss: 2.298978882451211

Epoch: 6| Step: 11
Training loss: 1.6740789413452148
Validation loss: 2.3008257240377445

Epoch: 6| Step: 12
Training loss: 2.4944376945495605
Validation loss: 2.30159826688869

Epoch: 6| Step: 13
Training loss: 3.2650363445281982
Validation loss: 2.31313556496815

Epoch: 274| Step: 0
Training loss: 1.5380189418792725
Validation loss: 2.3070383302627073

Epoch: 6| Step: 1
Training loss: 2.1778035163879395
Validation loss: 2.3162841463601715

Epoch: 6| Step: 2
Training loss: 2.263059139251709
Validation loss: 2.3188943760369414

Epoch: 6| Step: 3
Training loss: 2.1957592964172363
Validation loss: 2.325347997808969

Epoch: 6| Step: 4
Training loss: 2.244959592819214
Validation loss: 2.3128813133444837

Epoch: 6| Step: 5
Training loss: 2.4158506393432617
Validation loss: 2.333113606258105

Epoch: 6| Step: 6
Training loss: 2.9133691787719727
Validation loss: 2.3512358909012168

Epoch: 6| Step: 7
Training loss: 3.142836570739746
Validation loss: 2.3515737364369054

Epoch: 6| Step: 8
Training loss: 2.0804338455200195
Validation loss: 2.356080619237756

Epoch: 6| Step: 9
Training loss: 2.487741708755493
Validation loss: 2.3669914353278374

Epoch: 6| Step: 10
Training loss: 3.1324925422668457
Validation loss: 2.3732556399478706

Epoch: 6| Step: 11
Training loss: 3.154249668121338
Validation loss: 2.3664890386724986

Epoch: 6| Step: 12
Training loss: 3.1825318336486816
Validation loss: 2.3457016765430407

Epoch: 6| Step: 13
Training loss: 2.046915292739868
Validation loss: 2.341813072081535

Epoch: 275| Step: 0
Training loss: 2.970673084259033
Validation loss: 2.323952454392628

Epoch: 6| Step: 1
Training loss: 1.7602641582489014
Validation loss: 2.3199671545336322

Epoch: 6| Step: 2
Training loss: 2.030764579772949
Validation loss: 2.3120668575327885

Epoch: 6| Step: 3
Training loss: 2.3910417556762695
Validation loss: 2.2907471682435725

Epoch: 6| Step: 4
Training loss: 3.0691323280334473
Validation loss: 2.2870593019711074

Epoch: 6| Step: 5
Training loss: 2.675733804702759
Validation loss: 2.2772360155659337

Epoch: 6| Step: 6
Training loss: 2.0054633617401123
Validation loss: 2.2772060671160297

Epoch: 6| Step: 7
Training loss: 2.2705061435699463
Validation loss: 2.2811182352804367

Epoch: 6| Step: 8
Training loss: 3.255603790283203
Validation loss: 2.2716733024966334

Epoch: 6| Step: 9
Training loss: 1.969280481338501
Validation loss: 2.270555901271041

Epoch: 6| Step: 10
Training loss: 2.846419334411621
Validation loss: 2.274146141544465

Epoch: 6| Step: 11
Training loss: 2.6136648654937744
Validation loss: 2.273069368895664

Epoch: 6| Step: 12
Training loss: 3.057816982269287
Validation loss: 2.283969845823062

Epoch: 6| Step: 13
Training loss: 1.9978383779525757
Validation loss: 2.2976220525721067

Epoch: 276| Step: 0
Training loss: 2.6707658767700195
Validation loss: 2.2961472477964175

Epoch: 6| Step: 1
Training loss: 2.694398880004883
Validation loss: 2.3033549606159167

Epoch: 6| Step: 2
Training loss: 2.3173270225524902
Validation loss: 2.3286816202184206

Epoch: 6| Step: 3
Training loss: 2.8137896060943604
Validation loss: 2.3281920443299

Epoch: 6| Step: 4
Training loss: 2.288919448852539
Validation loss: 2.3377427900991132

Epoch: 6| Step: 5
Training loss: 2.8622829914093018
Validation loss: 2.3489686840323993

Epoch: 6| Step: 6
Training loss: 2.034027099609375
Validation loss: 2.3390954514985443

Epoch: 6| Step: 7
Training loss: 2.798532485961914
Validation loss: 2.3368584930255847

Epoch: 6| Step: 8
Training loss: 2.005643129348755
Validation loss: 2.3391247590382895

Epoch: 6| Step: 9
Training loss: 2.2830350399017334
Validation loss: 2.3352080852754655

Epoch: 6| Step: 10
Training loss: 2.6051278114318848
Validation loss: 2.316876680620255

Epoch: 6| Step: 11
Training loss: 2.430410623550415
Validation loss: 2.326791368505006

Epoch: 6| Step: 12
Training loss: 2.5905323028564453
Validation loss: 2.321245798500635

Epoch: 6| Step: 13
Training loss: 2.807640552520752
Validation loss: 2.326518649696022

Epoch: 277| Step: 0
Training loss: 3.1080822944641113
Validation loss: 2.333390394846598

Epoch: 6| Step: 1
Training loss: 1.9470206499099731
Validation loss: 2.339014986509918

Epoch: 6| Step: 2
Training loss: 2.258194923400879
Validation loss: 2.3375625046350623

Epoch: 6| Step: 3
Training loss: 1.8170362710952759
Validation loss: 2.3332709522657495

Epoch: 6| Step: 4
Training loss: 2.4616684913635254
Validation loss: 2.3378659422679613

Epoch: 6| Step: 5
Training loss: 3.3221611976623535
Validation loss: 2.3351989638420845

Epoch: 6| Step: 6
Training loss: 2.57820987701416
Validation loss: 2.3310723304748535

Epoch: 6| Step: 7
Training loss: 2.7659912109375
Validation loss: 2.3352561791737876

Epoch: 6| Step: 8
Training loss: 2.5409164428710938
Validation loss: 2.3244539486464633

Epoch: 6| Step: 9
Training loss: 2.421678066253662
Validation loss: 2.32146442192857

Epoch: 6| Step: 10
Training loss: 2.6238980293273926
Validation loss: 2.330909479048944

Epoch: 6| Step: 11
Training loss: 2.537402391433716
Validation loss: 2.324985579777789

Epoch: 6| Step: 12
Training loss: 2.410452365875244
Validation loss: 2.3171695329809703

Epoch: 6| Step: 13
Training loss: 1.9813575744628906
Validation loss: 2.340584611379972

Epoch: 278| Step: 0
Training loss: 2.510313034057617
Validation loss: 2.326706532509096

Epoch: 6| Step: 1
Training loss: 2.7772321701049805
Validation loss: 2.3169657261140886

Epoch: 6| Step: 2
Training loss: 2.0120716094970703
Validation loss: 2.3311082470801567

Epoch: 6| Step: 3
Training loss: 2.587606191635132
Validation loss: 2.316559499309909

Epoch: 6| Step: 4
Training loss: 2.7174365520477295
Validation loss: 2.323025334265924

Epoch: 6| Step: 5
Training loss: 2.249234914779663
Validation loss: 2.3257699525484474

Epoch: 6| Step: 6
Training loss: 1.7444813251495361
Validation loss: 2.327098402925717

Epoch: 6| Step: 7
Training loss: 2.96342396736145
Validation loss: 2.3178461802903043

Epoch: 6| Step: 8
Training loss: 3.050424098968506
Validation loss: 2.3303793322655464

Epoch: 6| Step: 9
Training loss: 2.4198617935180664
Validation loss: 2.321305810764272

Epoch: 6| Step: 10
Training loss: 2.9774703979492188
Validation loss: 2.326163127858152

Epoch: 6| Step: 11
Training loss: 2.121540069580078
Validation loss: 2.3251437192322104

Epoch: 6| Step: 12
Training loss: 2.2770206928253174
Validation loss: 2.3250503501584454

Epoch: 6| Step: 13
Training loss: 2.5527968406677246
Validation loss: 2.3126022738795124

Epoch: 279| Step: 0
Training loss: 3.6598715782165527
Validation loss: 2.320949446770453

Epoch: 6| Step: 1
Training loss: 2.136946678161621
Validation loss: 2.3257345307257866

Epoch: 6| Step: 2
Training loss: 2.2985644340515137
Validation loss: 2.3201024455408894

Epoch: 6| Step: 3
Training loss: 1.8791556358337402
Validation loss: 2.311912170020483

Epoch: 6| Step: 4
Training loss: 2.6374053955078125
Validation loss: 2.29352375512482

Epoch: 6| Step: 5
Training loss: 2.656961441040039
Validation loss: 2.2904971594451577

Epoch: 6| Step: 6
Training loss: 2.855100631713867
Validation loss: 2.289603764011014

Epoch: 6| Step: 7
Training loss: 2.2972378730773926
Validation loss: 2.3060441811879477

Epoch: 6| Step: 8
Training loss: 1.5116534233093262
Validation loss: 2.311019923097344

Epoch: 6| Step: 9
Training loss: 2.4898781776428223
Validation loss: 2.3234793755315963

Epoch: 6| Step: 10
Training loss: 2.280850887298584
Validation loss: 2.314126178782473

Epoch: 6| Step: 11
Training loss: 2.5798964500427246
Validation loss: 2.3166562690529773

Epoch: 6| Step: 12
Training loss: 2.343332052230835
Validation loss: 2.3140093177877445

Epoch: 6| Step: 13
Training loss: 3.9081850051879883
Validation loss: 2.2979252415318645

Epoch: 280| Step: 0
Training loss: 2.945817232131958
Validation loss: 2.290524269944878

Epoch: 6| Step: 1
Training loss: 2.961988925933838
Validation loss: 2.2970933401456444

Epoch: 6| Step: 2
Training loss: 3.063401699066162
Validation loss: 2.279853992564704

Epoch: 6| Step: 3
Training loss: 2.3149213790893555
Validation loss: 2.27945279049617

Epoch: 6| Step: 4
Training loss: 3.431097984313965
Validation loss: 2.2730096424779584

Epoch: 6| Step: 5
Training loss: 2.149535894393921
Validation loss: 2.277297058413106

Epoch: 6| Step: 6
Training loss: 2.7422876358032227
Validation loss: 2.281032713510657

Epoch: 6| Step: 7
Training loss: 1.7891209125518799
Validation loss: 2.284182962550912

Epoch: 6| Step: 8
Training loss: 2.7998058795928955
Validation loss: 2.3003837831558718

Epoch: 6| Step: 9
Training loss: 1.6880731582641602
Validation loss: 2.295370399311025

Epoch: 6| Step: 10
Training loss: 1.5008784532546997
Validation loss: 2.3127963555756437

Epoch: 6| Step: 11
Training loss: 2.09786319732666
Validation loss: 2.3091343090098393

Epoch: 6| Step: 12
Training loss: 3.1001036167144775
Validation loss: 2.318733963915097

Epoch: 6| Step: 13
Training loss: 2.0806493759155273
Validation loss: 2.3354118126694874

Epoch: 281| Step: 0
Training loss: 2.834601640701294
Validation loss: 2.3320668410229426

Epoch: 6| Step: 1
Training loss: 2.6933445930480957
Validation loss: 2.338351857277655

Epoch: 6| Step: 2
Training loss: 2.268890857696533
Validation loss: 2.3317010095042567

Epoch: 6| Step: 3
Training loss: 2.5084118843078613
Validation loss: 2.331014153777912

Epoch: 6| Step: 4
Training loss: 2.771049976348877
Validation loss: 2.3196020100706365

Epoch: 6| Step: 5
Training loss: 2.308804512023926
Validation loss: 2.3081553469422045

Epoch: 6| Step: 6
Training loss: 2.9811973571777344
Validation loss: 2.29740931013579

Epoch: 6| Step: 7
Training loss: 3.0780935287475586
Validation loss: 2.289181219634189

Epoch: 6| Step: 8
Training loss: 2.0294556617736816
Validation loss: 2.288938476193336

Epoch: 6| Step: 9
Training loss: 1.5510258674621582
Validation loss: 2.2860763662604877

Epoch: 6| Step: 10
Training loss: 2.175018787384033
Validation loss: 2.2899981929409887

Epoch: 6| Step: 11
Training loss: 2.2888827323913574
Validation loss: 2.290891706302602

Epoch: 6| Step: 12
Training loss: 3.064194440841675
Validation loss: 2.2878286876986103

Epoch: 6| Step: 13
Training loss: 1.9284322261810303
Validation loss: 2.298131360802599

Epoch: 282| Step: 0
Training loss: 2.251175880432129
Validation loss: 2.2943897580587738

Epoch: 6| Step: 1
Training loss: 2.630917549133301
Validation loss: 2.3033967838492444

Epoch: 6| Step: 2
Training loss: 2.0869078636169434
Validation loss: 2.294710748939104

Epoch: 6| Step: 3
Training loss: 2.75030517578125
Validation loss: 2.308492442613007

Epoch: 6| Step: 4
Training loss: 2.2234325408935547
Validation loss: 2.3056876954211982

Epoch: 6| Step: 5
Training loss: 2.3493261337280273
Validation loss: 2.3155719951916764

Epoch: 6| Step: 6
Training loss: 2.9502081871032715
Validation loss: 2.3174005118749474

Epoch: 6| Step: 7
Training loss: 2.5853028297424316
Validation loss: 2.313481460335434

Epoch: 6| Step: 8
Training loss: 2.0955018997192383
Validation loss: 2.303745723539783

Epoch: 6| Step: 9
Training loss: 2.19883394241333
Validation loss: 2.301007765595631

Epoch: 6| Step: 10
Training loss: 2.1331870555877686
Validation loss: 2.2958613518745667

Epoch: 6| Step: 11
Training loss: 2.591867685317993
Validation loss: 2.2978887250346522

Epoch: 6| Step: 12
Training loss: 3.113647222518921
Validation loss: 2.286564216818861

Epoch: 6| Step: 13
Training loss: 2.90303897857666
Validation loss: 2.2960862216129097

Epoch: 283| Step: 0
Training loss: 2.3716585636138916
Validation loss: 2.2992195993341427

Epoch: 6| Step: 1
Training loss: 1.8861076831817627
Validation loss: 2.307773946433939

Epoch: 6| Step: 2
Training loss: 2.3558883666992188
Validation loss: 2.3196988618502052

Epoch: 6| Step: 3
Training loss: 2.3807144165039062
Validation loss: 2.3232769350851736

Epoch: 6| Step: 4
Training loss: 3.2245171070098877
Validation loss: 2.323133555791711

Epoch: 6| Step: 5
Training loss: 2.8002572059631348
Validation loss: 2.304942984734812

Epoch: 6| Step: 6
Training loss: 2.676456928253174
Validation loss: 2.2906231213641424

Epoch: 6| Step: 7
Training loss: 1.9310815334320068
Validation loss: 2.2902125414981636

Epoch: 6| Step: 8
Training loss: 2.6692519187927246
Validation loss: 2.298289324647637

Epoch: 6| Step: 9
Training loss: 2.1425275802612305
Validation loss: 2.291654904683431

Epoch: 6| Step: 10
Training loss: 2.9464993476867676
Validation loss: 2.3020720494690763

Epoch: 6| Step: 11
Training loss: 3.1218950748443604
Validation loss: 2.290845404389084

Epoch: 6| Step: 12
Training loss: 1.784948468208313
Validation loss: 2.295712399226363

Epoch: 6| Step: 13
Training loss: 2.364077568054199
Validation loss: 2.3075440263235443

Epoch: 284| Step: 0
Training loss: 2.4912631511688232
Validation loss: 2.2909124000098116

Epoch: 6| Step: 1
Training loss: 2.363927125930786
Validation loss: 2.300939010035607

Epoch: 6| Step: 2
Training loss: 2.132155656814575
Validation loss: 2.2990148144383586

Epoch: 6| Step: 3
Training loss: 2.296708583831787
Validation loss: 2.316354618277601

Epoch: 6| Step: 4
Training loss: 3.101861000061035
Validation loss: 2.3164064627821728

Epoch: 6| Step: 5
Training loss: 3.051532745361328
Validation loss: 2.3281086388454644

Epoch: 6| Step: 6
Training loss: 1.4880788326263428
Validation loss: 2.3144361562626337

Epoch: 6| Step: 7
Training loss: 2.896655559539795
Validation loss: 2.324844476997211

Epoch: 6| Step: 8
Training loss: 2.834028720855713
Validation loss: 2.321420677246586

Epoch: 6| Step: 9
Training loss: 2.401883602142334
Validation loss: 2.322097546310835

Epoch: 6| Step: 10
Training loss: 2.530733585357666
Validation loss: 2.3163484732309976

Epoch: 6| Step: 11
Training loss: 2.3926072120666504
Validation loss: 2.3048097779673915

Epoch: 6| Step: 12
Training loss: 2.4138002395629883
Validation loss: 2.3040779303478938

Epoch: 6| Step: 13
Training loss: 1.7623835802078247
Validation loss: 2.298009346890193

Epoch: 285| Step: 0
Training loss: 2.448878288269043
Validation loss: 2.3114282085049536

Epoch: 6| Step: 1
Training loss: 2.623656749725342
Validation loss: 2.306261884268894

Epoch: 6| Step: 2
Training loss: 1.999380111694336
Validation loss: 2.317469202062135

Epoch: 6| Step: 3
Training loss: 2.674407958984375
Validation loss: 2.3134620779304096

Epoch: 6| Step: 4
Training loss: 2.8968429565429688
Validation loss: 2.3140672253024195

Epoch: 6| Step: 5
Training loss: 2.8274765014648438
Validation loss: 2.3128385748914493

Epoch: 6| Step: 6
Training loss: 2.0887274742126465
Validation loss: 2.312370966839534

Epoch: 6| Step: 7
Training loss: 1.5208855867385864
Validation loss: 2.305045558560279

Epoch: 6| Step: 8
Training loss: 2.2996153831481934
Validation loss: 2.2959009383314397

Epoch: 6| Step: 9
Training loss: 2.0004594326019287
Validation loss: 2.298585996832899

Epoch: 6| Step: 10
Training loss: 2.6410694122314453
Validation loss: 2.3021244490018455

Epoch: 6| Step: 11
Training loss: 2.775143623352051
Validation loss: 2.3041268266657347

Epoch: 6| Step: 12
Training loss: 2.873168468475342
Validation loss: 2.324090685895694

Epoch: 6| Step: 13
Training loss: 3.4264309406280518
Validation loss: 2.340635948283698

Epoch: 286| Step: 0
Training loss: 2.5467982292175293
Validation loss: 2.3487839314245407

Epoch: 6| Step: 1
Training loss: 2.2784829139709473
Validation loss: 2.358684580813172

Epoch: 6| Step: 2
Training loss: 2.2851858139038086
Validation loss: 2.3507225564731065

Epoch: 6| Step: 3
Training loss: 2.62823486328125
Validation loss: 2.3360138708545315

Epoch: 6| Step: 4
Training loss: 2.4544527530670166
Validation loss: 2.3220152547282558

Epoch: 6| Step: 5
Training loss: 2.2697606086730957
Validation loss: 2.3159421541357554

Epoch: 6| Step: 6
Training loss: 2.039034128189087
Validation loss: 2.312683197759813

Epoch: 6| Step: 7
Training loss: 3.5515952110290527
Validation loss: 2.293566103904478

Epoch: 6| Step: 8
Training loss: 2.594604969024658
Validation loss: 2.2804287761770268

Epoch: 6| Step: 9
Training loss: 2.410292148590088
Validation loss: 2.2890269012861353

Epoch: 6| Step: 10
Training loss: 1.8701057434082031
Validation loss: 2.2992060671570482

Epoch: 6| Step: 11
Training loss: 2.5962107181549072
Validation loss: 2.3108114529681463

Epoch: 6| Step: 12
Training loss: 2.2793893814086914
Validation loss: 2.2993502616882324

Epoch: 6| Step: 13
Training loss: 3.2530155181884766
Validation loss: 2.311791189255253

Epoch: 287| Step: 0
Training loss: 2.7315433025360107
Validation loss: 2.302414683885472

Epoch: 6| Step: 1
Training loss: 3.2220206260681152
Validation loss: 2.2962475181907736

Epoch: 6| Step: 2
Training loss: 2.333362579345703
Validation loss: 2.291192603367631

Epoch: 6| Step: 3
Training loss: 2.0343732833862305
Validation loss: 2.287203718257207

Epoch: 6| Step: 4
Training loss: 2.804513931274414
Validation loss: 2.281374736498761

Epoch: 6| Step: 5
Training loss: 2.504483222961426
Validation loss: 2.29651552631009

Epoch: 6| Step: 6
Training loss: 2.6059341430664062
Validation loss: 2.318399453675875

Epoch: 6| Step: 7
Training loss: 2.4021782875061035
Validation loss: 2.344612824019565

Epoch: 6| Step: 8
Training loss: 2.2877492904663086
Validation loss: 2.3800832276703208

Epoch: 6| Step: 9
Training loss: 1.9922879934310913
Validation loss: 2.4064691271833194

Epoch: 6| Step: 10
Training loss: 1.81585693359375
Validation loss: 2.440342769827894

Epoch: 6| Step: 11
Training loss: 3.3495802879333496
Validation loss: 2.4891428819266697

Epoch: 6| Step: 12
Training loss: 2.767411708831787
Validation loss: 2.488136596577142

Epoch: 6| Step: 13
Training loss: 2.763669729232788
Validation loss: 2.4767290725502917

Epoch: 288| Step: 0
Training loss: 2.3521289825439453
Validation loss: 2.4179110296310915

Epoch: 6| Step: 1
Training loss: 2.427335262298584
Validation loss: 2.400666913678569

Epoch: 6| Step: 2
Training loss: 2.6288509368896484
Validation loss: 2.346646531935661

Epoch: 6| Step: 3
Training loss: 1.7959444522857666
Validation loss: 2.326264250663019

Epoch: 6| Step: 4
Training loss: 2.2641477584838867
Validation loss: 2.3327610851615987

Epoch: 6| Step: 5
Training loss: 2.4910953044891357
Validation loss: 2.3446641019595567

Epoch: 6| Step: 6
Training loss: 2.078763961791992
Validation loss: 2.359415554231213

Epoch: 6| Step: 7
Training loss: 2.2972686290740967
Validation loss: 2.381004143786687

Epoch: 6| Step: 8
Training loss: 3.59031343460083
Validation loss: 2.362353048016948

Epoch: 6| Step: 9
Training loss: 2.663360118865967
Validation loss: 2.3559326279547905

Epoch: 6| Step: 10
Training loss: 2.85429048538208
Validation loss: 2.324329706930345

Epoch: 6| Step: 11
Training loss: 2.1313469409942627
Validation loss: 2.2981996382436445

Epoch: 6| Step: 12
Training loss: 3.1699414253234863
Validation loss: 2.2896444259151334

Epoch: 6| Step: 13
Training loss: 2.65409517288208
Validation loss: 2.266900417625263

Epoch: 289| Step: 0
Training loss: 2.7197065353393555
Validation loss: 2.2830562194188437

Epoch: 6| Step: 1
Training loss: 1.9694045782089233
Validation loss: 2.280863869574762

Epoch: 6| Step: 2
Training loss: 2.2317872047424316
Validation loss: 2.2803498109181723

Epoch: 6| Step: 3
Training loss: 1.7833902835845947
Validation loss: 2.2676043612982637

Epoch: 6| Step: 4
Training loss: 2.9464263916015625
Validation loss: 2.27325782468242

Epoch: 6| Step: 5
Training loss: 2.0558619499206543
Validation loss: 2.2809700760790097

Epoch: 6| Step: 6
Training loss: 3.405022144317627
Validation loss: 2.2806730206294725

Epoch: 6| Step: 7
Training loss: 2.8355960845947266
Validation loss: 2.291009956790555

Epoch: 6| Step: 8
Training loss: 1.6625068187713623
Validation loss: 2.2889399246502946

Epoch: 6| Step: 9
Training loss: 2.542786121368408
Validation loss: 2.300245013288272

Epoch: 6| Step: 10
Training loss: 2.734208345413208
Validation loss: 2.302476462497506

Epoch: 6| Step: 11
Training loss: 1.9499318599700928
Validation loss: 2.3157097575485066

Epoch: 6| Step: 12
Training loss: 2.741273880004883
Validation loss: 2.314812683290051

Epoch: 6| Step: 13
Training loss: 3.8146913051605225
Validation loss: 2.3005525809462353

Epoch: 290| Step: 0
Training loss: 2.2249889373779297
Validation loss: 2.298917974195173

Epoch: 6| Step: 1
Training loss: 2.7454771995544434
Validation loss: 2.2894607000453497

Epoch: 6| Step: 2
Training loss: 2.700141429901123
Validation loss: 2.293955987499606

Epoch: 6| Step: 3
Training loss: 3.403470993041992
Validation loss: 2.290468536397462

Epoch: 6| Step: 4
Training loss: 2.3204803466796875
Validation loss: 2.2995884008305048

Epoch: 6| Step: 5
Training loss: 2.7851343154907227
Validation loss: 2.2807400252229426

Epoch: 6| Step: 6
Training loss: 1.7125349044799805
Validation loss: 2.2810696068630425

Epoch: 6| Step: 7
Training loss: 3.2802133560180664
Validation loss: 2.2768998453694005

Epoch: 6| Step: 8
Training loss: 2.530378818511963
Validation loss: 2.280992697643977

Epoch: 6| Step: 9
Training loss: 2.1676578521728516
Validation loss: 2.27193017928831

Epoch: 6| Step: 10
Training loss: 2.1503703594207764
Validation loss: 2.274167145452192

Epoch: 6| Step: 11
Training loss: 2.4095916748046875
Validation loss: 2.2611418436932307

Epoch: 6| Step: 12
Training loss: 1.6434608697891235
Validation loss: 2.2634221392293132

Epoch: 6| Step: 13
Training loss: 2.2725396156311035
Validation loss: 2.2742301443571686

Epoch: 291| Step: 0
Training loss: 2.636652946472168
Validation loss: 2.275730004874609

Epoch: 6| Step: 1
Training loss: 2.883989095687866
Validation loss: 2.2991961330495854

Epoch: 6| Step: 2
Training loss: 2.5393049716949463
Validation loss: 2.2962407783795427

Epoch: 6| Step: 3
Training loss: 2.1084699630737305
Validation loss: 2.3164901912853284

Epoch: 6| Step: 4
Training loss: 2.8515589237213135
Validation loss: 2.3085516191297963

Epoch: 6| Step: 5
Training loss: 2.5567526817321777
Validation loss: 2.3342722308251167

Epoch: 6| Step: 6
Training loss: 2.5368220806121826
Validation loss: 2.32979703205888

Epoch: 6| Step: 7
Training loss: 2.696962833404541
Validation loss: 2.3312646086497972

Epoch: 6| Step: 8
Training loss: 2.443150043487549
Validation loss: 2.336783907746756

Epoch: 6| Step: 9
Training loss: 2.3618364334106445
Validation loss: 2.3414770864671275

Epoch: 6| Step: 10
Training loss: 2.4024412631988525
Validation loss: 2.33434925412619

Epoch: 6| Step: 11
Training loss: 2.621086359024048
Validation loss: 2.3341962278530164

Epoch: 6| Step: 12
Training loss: 2.1538777351379395
Validation loss: 2.3082580617679063

Epoch: 6| Step: 13
Training loss: 1.1807994842529297
Validation loss: 2.290892190830682

Epoch: 292| Step: 0
Training loss: 2.498509407043457
Validation loss: 2.2872723815261677

Epoch: 6| Step: 1
Training loss: 2.3650145530700684
Validation loss: 2.2843542739909184

Epoch: 6| Step: 2
Training loss: 2.418179988861084
Validation loss: 2.282808365360383

Epoch: 6| Step: 3
Training loss: 2.362415075302124
Validation loss: 2.297994259865053

Epoch: 6| Step: 4
Training loss: 2.2194998264312744
Validation loss: 2.301303166215138

Epoch: 6| Step: 5
Training loss: 2.404423236846924
Validation loss: 2.307614590532036

Epoch: 6| Step: 6
Training loss: 2.7856602668762207
Validation loss: 2.304489758706862

Epoch: 6| Step: 7
Training loss: 2.4696874618530273
Validation loss: 2.2925324722002913

Epoch: 6| Step: 8
Training loss: 3.0453133583068848
Validation loss: 2.288203699614412

Epoch: 6| Step: 9
Training loss: 2.2249832153320312
Validation loss: 2.297595741928265

Epoch: 6| Step: 10
Training loss: 2.847496747970581
Validation loss: 2.296907704363587

Epoch: 6| Step: 11
Training loss: 1.6605703830718994
Validation loss: 2.290355272190545

Epoch: 6| Step: 12
Training loss: 2.677705764770508
Validation loss: 2.308369654481129

Epoch: 6| Step: 13
Training loss: 2.2507810592651367
Validation loss: 2.3048646860225226

Epoch: 293| Step: 0
Training loss: 2.676523447036743
Validation loss: 2.302343730003603

Epoch: 6| Step: 1
Training loss: 2.4290170669555664
Validation loss: 2.294047232597105

Epoch: 6| Step: 2
Training loss: 2.8019652366638184
Validation loss: 2.298451631299911

Epoch: 6| Step: 3
Training loss: 2.4424171447753906
Validation loss: 2.2932359480088755

Epoch: 6| Step: 4
Training loss: 2.4917116165161133
Validation loss: 2.302206870048277

Epoch: 6| Step: 5
Training loss: 2.452122688293457
Validation loss: 2.2917676664167836

Epoch: 6| Step: 6
Training loss: 2.7237343788146973
Validation loss: 2.289414600659442

Epoch: 6| Step: 7
Training loss: 2.73354172706604
Validation loss: 2.2931984804009877

Epoch: 6| Step: 8
Training loss: 1.977309226989746
Validation loss: 2.276151454576882

Epoch: 6| Step: 9
Training loss: 2.376887798309326
Validation loss: 2.2736109610526793

Epoch: 6| Step: 10
Training loss: 2.017174243927002
Validation loss: 2.279574642899216

Epoch: 6| Step: 11
Training loss: 2.9353833198547363
Validation loss: 2.269809969009892

Epoch: 6| Step: 12
Training loss: 2.141256332397461
Validation loss: 2.273714991026027

Epoch: 6| Step: 13
Training loss: 1.7008987665176392
Validation loss: 2.2742923203335015

Epoch: 294| Step: 0
Training loss: 2.400989055633545
Validation loss: 2.2719551171025922

Epoch: 6| Step: 1
Training loss: 2.9824392795562744
Validation loss: 2.2787976700772523

Epoch: 6| Step: 2
Training loss: 1.9878607988357544
Validation loss: 2.2794893736480386

Epoch: 6| Step: 3
Training loss: 2.0455989837646484
Validation loss: 2.2657170346988145

Epoch: 6| Step: 4
Training loss: 2.484349012374878
Validation loss: 2.274718621725677

Epoch: 6| Step: 5
Training loss: 1.9317843914031982
Validation loss: 2.2848721063265236

Epoch: 6| Step: 6
Training loss: 3.0061025619506836
Validation loss: 2.2976755583158104

Epoch: 6| Step: 7
Training loss: 2.3817710876464844
Validation loss: 2.303204769729286

Epoch: 6| Step: 8
Training loss: 2.7429537773132324
Validation loss: 2.3050509986057075

Epoch: 6| Step: 9
Training loss: 2.9836983680725098
Validation loss: 2.299022559196718

Epoch: 6| Step: 10
Training loss: 2.631941795349121
Validation loss: 2.2909189988208074

Epoch: 6| Step: 11
Training loss: 1.6398484706878662
Validation loss: 2.2841384436494563

Epoch: 6| Step: 12
Training loss: 2.7105798721313477
Validation loss: 2.293624711293046

Epoch: 6| Step: 13
Training loss: 2.2270686626434326
Validation loss: 2.286152785824191

Epoch: 295| Step: 0
Training loss: 1.6560243368148804
Validation loss: 2.287028238337527

Epoch: 6| Step: 1
Training loss: 2.913294792175293
Validation loss: 2.289500122429222

Epoch: 6| Step: 2
Training loss: 2.572263479232788
Validation loss: 2.2906194912490023

Epoch: 6| Step: 3
Training loss: 2.774050235748291
Validation loss: 2.279606834534676

Epoch: 6| Step: 4
Training loss: 2.3115835189819336
Validation loss: 2.278789084444764

Epoch: 6| Step: 5
Training loss: 2.6284499168395996
Validation loss: 2.2715824009269796

Epoch: 6| Step: 6
Training loss: 2.32077693939209
Validation loss: 2.26484352286144

Epoch: 6| Step: 7
Training loss: 1.507310390472412
Validation loss: 2.2693957641560543

Epoch: 6| Step: 8
Training loss: 2.748093605041504
Validation loss: 2.2799104285496536

Epoch: 6| Step: 9
Training loss: 2.955632209777832
Validation loss: 2.2634886182764524

Epoch: 6| Step: 10
Training loss: 2.7534539699554443
Validation loss: 2.2655734298049763

Epoch: 6| Step: 11
Training loss: 1.921504259109497
Validation loss: 2.2765424456647647

Epoch: 6| Step: 12
Training loss: 2.722520351409912
Validation loss: 2.2860016797178533

Epoch: 6| Step: 13
Training loss: 2.5699822902679443
Validation loss: 2.2898095474448255

Epoch: 296| Step: 0
Training loss: 3.082707405090332
Validation loss: 2.295902677761611

Epoch: 6| Step: 1
Training loss: 2.8076677322387695
Validation loss: 2.3049456086210025

Epoch: 6| Step: 2
Training loss: 2.7667722702026367
Validation loss: 2.296539834750596

Epoch: 6| Step: 3
Training loss: 2.129099130630493
Validation loss: 2.2949279739010717

Epoch: 6| Step: 4
Training loss: 2.76359486579895
Validation loss: 2.2903791076393536

Epoch: 6| Step: 5
Training loss: 2.0748233795166016
Validation loss: 2.277010408780908

Epoch: 6| Step: 6
Training loss: 2.592823028564453
Validation loss: 2.2754066618539954

Epoch: 6| Step: 7
Training loss: 1.6460012197494507
Validation loss: 2.277803895294025

Epoch: 6| Step: 8
Training loss: 3.1404054164886475
Validation loss: 2.2742337026903705

Epoch: 6| Step: 9
Training loss: 2.483337879180908
Validation loss: 2.2686892581242386

Epoch: 6| Step: 10
Training loss: 2.6310338973999023
Validation loss: 2.256872433488087

Epoch: 6| Step: 11
Training loss: 2.047483444213867
Validation loss: 2.277139599605273

Epoch: 6| Step: 12
Training loss: 1.8559794425964355
Validation loss: 2.2735933539687947

Epoch: 6| Step: 13
Training loss: 1.9614615440368652
Validation loss: 2.2603638005513016

Epoch: 297| Step: 0
Training loss: 2.432173490524292
Validation loss: 2.288798624469388

Epoch: 6| Step: 1
Training loss: 2.6022720336914062
Validation loss: 2.2906349858930035

Epoch: 6| Step: 2
Training loss: 2.417630195617676
Validation loss: 2.2901902634610414

Epoch: 6| Step: 3
Training loss: 2.4250893592834473
Validation loss: 2.3036031864022695

Epoch: 6| Step: 4
Training loss: 2.638927459716797
Validation loss: 2.286670256686467

Epoch: 6| Step: 5
Training loss: 2.4821605682373047
Validation loss: 2.292167371319186

Epoch: 6| Step: 6
Training loss: 1.9139552116394043
Validation loss: 2.2997795125489593

Epoch: 6| Step: 7
Training loss: 2.9365453720092773
Validation loss: 2.271549722199799

Epoch: 6| Step: 8
Training loss: 2.398829460144043
Validation loss: 2.2861017565573416

Epoch: 6| Step: 9
Training loss: 2.163435459136963
Validation loss: 2.2814784819079983

Epoch: 6| Step: 10
Training loss: 2.878436803817749
Validation loss: 2.2706189283760647

Epoch: 6| Step: 11
Training loss: 1.6418046951293945
Validation loss: 2.2724095852144304

Epoch: 6| Step: 12
Training loss: 3.015514850616455
Validation loss: 2.268043633430235

Epoch: 6| Step: 13
Training loss: 2.1065075397491455
Validation loss: 2.281656721586822

Epoch: 298| Step: 0
Training loss: 2.460998296737671
Validation loss: 2.2915914673959055

Epoch: 6| Step: 1
Training loss: 2.244000196456909
Validation loss: 2.312032486802788

Epoch: 6| Step: 2
Training loss: 2.347620725631714
Validation loss: 2.3168385490294425

Epoch: 6| Step: 3
Training loss: 2.043313503265381
Validation loss: 2.320936711885596

Epoch: 6| Step: 4
Training loss: 2.9461731910705566
Validation loss: 2.3049815111262824

Epoch: 6| Step: 5
Training loss: 3.002991199493408
Validation loss: 2.2874329115754817

Epoch: 6| Step: 6
Training loss: 2.263820171356201
Validation loss: 2.27484961991669

Epoch: 6| Step: 7
Training loss: 2.3723607063293457
Validation loss: 2.27635524606192

Epoch: 6| Step: 8
Training loss: 1.9658048152923584
Validation loss: 2.269372683699413

Epoch: 6| Step: 9
Training loss: 2.177379608154297
Validation loss: 2.2646792550240793

Epoch: 6| Step: 10
Training loss: 2.097917079925537
Validation loss: 2.268908800617341

Epoch: 6| Step: 11
Training loss: 2.164721965789795
Validation loss: 2.2729389231692076

Epoch: 6| Step: 12
Training loss: 3.3123536109924316
Validation loss: 2.2667785921404437

Epoch: 6| Step: 13
Training loss: 2.969959020614624
Validation loss: 2.2684609531074442

Epoch: 299| Step: 0
Training loss: 2.0150437355041504
Validation loss: 2.2735697812931512

Epoch: 6| Step: 1
Training loss: 2.904879570007324
Validation loss: 2.280273668227657

Epoch: 6| Step: 2
Training loss: 2.3711893558502197
Validation loss: 2.274923147693757

Epoch: 6| Step: 3
Training loss: 2.127760410308838
Validation loss: 2.295922702358615

Epoch: 6| Step: 4
Training loss: 1.3506622314453125
Validation loss: 2.309635293099188

Epoch: 6| Step: 5
Training loss: 2.1663167476654053
Validation loss: 2.307776751056794

Epoch: 6| Step: 6
Training loss: 3.056190252304077
Validation loss: 2.319504543017316

Epoch: 6| Step: 7
Training loss: 2.1949543952941895
Validation loss: 2.305294306047501

Epoch: 6| Step: 8
Training loss: 3.2839879989624023
Validation loss: 2.298212138555383

Epoch: 6| Step: 9
Training loss: 2.2130250930786133
Validation loss: 2.295261111310733

Epoch: 6| Step: 10
Training loss: 2.50002384185791
Validation loss: 2.2845568221102477

Epoch: 6| Step: 11
Training loss: 2.811455488204956
Validation loss: 2.289126765343451

Epoch: 6| Step: 12
Training loss: 2.6587395668029785
Validation loss: 2.285235269095308

Epoch: 6| Step: 13
Training loss: 2.3183608055114746
Validation loss: 2.2842030627753145

Epoch: 300| Step: 0
Training loss: 2.204987049102783
Validation loss: 2.2578392080081406

Epoch: 6| Step: 1
Training loss: 2.3564882278442383
Validation loss: 2.2834598941187703

Epoch: 6| Step: 2
Training loss: 2.986513614654541
Validation loss: 2.281111399332682

Epoch: 6| Step: 3
Training loss: 2.4834108352661133
Validation loss: 2.2877178653593986

Epoch: 6| Step: 4
Training loss: 1.8551304340362549
Validation loss: 2.298382777039723

Epoch: 6| Step: 5
Training loss: 1.772684931755066
Validation loss: 2.3226323537929083

Epoch: 6| Step: 6
Training loss: 2.946690559387207
Validation loss: 2.324779438716109

Epoch: 6| Step: 7
Training loss: 2.108074188232422
Validation loss: 2.329320474337506

Epoch: 6| Step: 8
Training loss: 2.389695644378662
Validation loss: 2.3108328375765073

Epoch: 6| Step: 9
Training loss: 2.2544515132904053
Validation loss: 2.2978576767829155

Epoch: 6| Step: 10
Training loss: 2.622230052947998
Validation loss: 2.286392634914767

Epoch: 6| Step: 11
Training loss: 2.6914639472961426
Validation loss: 2.274972064520723

Epoch: 6| Step: 12
Training loss: 2.8817577362060547
Validation loss: 2.2605400354631486

Epoch: 6| Step: 13
Training loss: 2.475553274154663
Validation loss: 2.2546073903319654

Epoch: 301| Step: 0
Training loss: 2.595170736312866
Validation loss: 2.251737517695273

Epoch: 6| Step: 1
Training loss: 2.5660367012023926
Validation loss: 2.2437950103513655

Epoch: 6| Step: 2
Training loss: 2.753990888595581
Validation loss: 2.235254297974289

Epoch: 6| Step: 3
Training loss: 2.810328245162964
Validation loss: 2.242496987824799

Epoch: 6| Step: 4
Training loss: 1.9214987754821777
Validation loss: 2.237837217187369

Epoch: 6| Step: 5
Training loss: 2.865659236907959
Validation loss: 2.2347561133805143

Epoch: 6| Step: 6
Training loss: 2.062464475631714
Validation loss: 2.2414613282808693

Epoch: 6| Step: 7
Training loss: 2.6989388465881348
Validation loss: 2.2333275220727407

Epoch: 6| Step: 8
Training loss: 2.663449764251709
Validation loss: 2.231129115627658

Epoch: 6| Step: 9
Training loss: 2.7665910720825195
Validation loss: 2.2334758773926766

Epoch: 6| Step: 10
Training loss: 2.296342611312866
Validation loss: 2.232610607659945

Epoch: 6| Step: 11
Training loss: 2.182300090789795
Validation loss: 2.2418159207990094

Epoch: 6| Step: 12
Training loss: 1.7248867750167847
Validation loss: 2.2436681793582056

Epoch: 6| Step: 13
Training loss: 2.7107021808624268
Validation loss: 2.2593470337570354

Epoch: 302| Step: 0
Training loss: 2.924494981765747
Validation loss: 2.2593167699793333

Epoch: 6| Step: 1
Training loss: 2.4391682147979736
Validation loss: 2.260314840142445

Epoch: 6| Step: 2
Training loss: 2.2899246215820312
Validation loss: 2.280081546434792

Epoch: 6| Step: 3
Training loss: 2.213618516921997
Validation loss: 2.3015606070077546

Epoch: 6| Step: 4
Training loss: 2.1898694038391113
Validation loss: 2.3132008455132924

Epoch: 6| Step: 5
Training loss: 3.1605844497680664
Validation loss: 2.346147808977353

Epoch: 6| Step: 6
Training loss: 2.6743814945220947
Validation loss: 2.326971641150854

Epoch: 6| Step: 7
Training loss: 2.2932088375091553
Validation loss: 2.3230191430737896

Epoch: 6| Step: 8
Training loss: 2.866412878036499
Validation loss: 2.316953069420271

Epoch: 6| Step: 9
Training loss: 2.650325059890747
Validation loss: 2.285008822717974

Epoch: 6| Step: 10
Training loss: 2.8677070140838623
Validation loss: 2.284163982637467

Epoch: 6| Step: 11
Training loss: 1.6917879581451416
Validation loss: 2.279184491403641

Epoch: 6| Step: 12
Training loss: 1.6944555044174194
Validation loss: 2.2638523296643327

Epoch: 6| Step: 13
Training loss: 2.194197177886963
Validation loss: 2.266954693742978

Epoch: 303| Step: 0
Training loss: 2.609119176864624
Validation loss: 2.2574640653466664

Epoch: 6| Step: 1
Training loss: 2.845609426498413
Validation loss: 2.254712256052161

Epoch: 6| Step: 2
Training loss: 2.4273910522460938
Validation loss: 2.243523118316486

Epoch: 6| Step: 3
Training loss: 2.733232021331787
Validation loss: 2.2491869311178885

Epoch: 6| Step: 4
Training loss: 2.117155075073242
Validation loss: 2.248311855459726

Epoch: 6| Step: 5
Training loss: 2.626537561416626
Validation loss: 2.254797876522105

Epoch: 6| Step: 6
Training loss: 2.2681965827941895
Validation loss: 2.252406104918449

Epoch: 6| Step: 7
Training loss: 3.2159523963928223
Validation loss: 2.2548121534368044

Epoch: 6| Step: 8
Training loss: 1.891769289970398
Validation loss: 2.263920384068643

Epoch: 6| Step: 9
Training loss: 1.9069545269012451
Validation loss: 2.266302752238448

Epoch: 6| Step: 10
Training loss: 2.8102567195892334
Validation loss: 2.2872545667873916

Epoch: 6| Step: 11
Training loss: 2.094897747039795
Validation loss: 2.2896505043070805

Epoch: 6| Step: 12
Training loss: 2.244776725769043
Validation loss: 2.30052581397436

Epoch: 6| Step: 13
Training loss: 2.3838353157043457
Validation loss: 2.2972698109124297

Epoch: 304| Step: 0
Training loss: 1.9196950197219849
Validation loss: 2.3067329134992374

Epoch: 6| Step: 1
Training loss: 2.0353410243988037
Validation loss: 2.296094758536226

Epoch: 6| Step: 2
Training loss: 2.6090564727783203
Validation loss: 2.300737298944945

Epoch: 6| Step: 3
Training loss: 2.2343883514404297
Validation loss: 2.3053606915217575

Epoch: 6| Step: 4
Training loss: 2.363521099090576
Validation loss: 2.3112971987775577

Epoch: 6| Step: 5
Training loss: 2.91263484954834
Validation loss: 2.29657458489941

Epoch: 6| Step: 6
Training loss: 2.247799873352051
Validation loss: 2.2973992875827256

Epoch: 6| Step: 7
Training loss: 2.534064292907715
Validation loss: 2.3044747101363314

Epoch: 6| Step: 8
Training loss: 3.164905071258545
Validation loss: 2.291118916644845

Epoch: 6| Step: 9
Training loss: 2.280905246734619
Validation loss: 2.2835118796235774

Epoch: 6| Step: 10
Training loss: 2.8925867080688477
Validation loss: 2.2705249145466793

Epoch: 6| Step: 11
Training loss: 1.9216554164886475
Validation loss: 2.2557849230304843

Epoch: 6| Step: 12
Training loss: 1.8516855239868164
Validation loss: 2.27340054768388

Epoch: 6| Step: 13
Training loss: 3.4627904891967773
Validation loss: 2.2667910642521356

Epoch: 305| Step: 0
Training loss: 2.5792770385742188
Validation loss: 2.2614487114773003

Epoch: 6| Step: 1
Training loss: 2.6699466705322266
Validation loss: 2.2672206330043014

Epoch: 6| Step: 2
Training loss: 2.2556726932525635
Validation loss: 2.26563706192919

Epoch: 6| Step: 3
Training loss: 2.2736079692840576
Validation loss: 2.257481146884221

Epoch: 6| Step: 4
Training loss: 1.701343297958374
Validation loss: 2.250508277646957

Epoch: 6| Step: 5
Training loss: 2.7592365741729736
Validation loss: 2.254432567986109

Epoch: 6| Step: 6
Training loss: 2.0495357513427734
Validation loss: 2.2540119642852456

Epoch: 6| Step: 7
Training loss: 2.511791706085205
Validation loss: 2.2627569885664087

Epoch: 6| Step: 8
Training loss: 2.9370803833007812
Validation loss: 2.2625582987262356

Epoch: 6| Step: 9
Training loss: 2.064180612564087
Validation loss: 2.2686825618948987

Epoch: 6| Step: 10
Training loss: 2.444204807281494
Validation loss: 2.259622804580196

Epoch: 6| Step: 11
Training loss: 2.4464728832244873
Validation loss: 2.2559706600763465

Epoch: 6| Step: 12
Training loss: 2.7156763076782227
Validation loss: 2.265534762413271

Epoch: 6| Step: 13
Training loss: 2.6218013763427734
Validation loss: 2.2737328262739283

Epoch: 306| Step: 0
Training loss: 2.5220818519592285
Validation loss: 2.259036474330451

Epoch: 6| Step: 1
Training loss: 1.9239964485168457
Validation loss: 2.2629310110563874

Epoch: 6| Step: 2
Training loss: 2.9647631645202637
Validation loss: 2.2558230866668043

Epoch: 6| Step: 3
Training loss: 3.020474910736084
Validation loss: 2.2630112530082784

Epoch: 6| Step: 4
Training loss: 2.6210508346557617
Validation loss: 2.264709480347172

Epoch: 6| Step: 5
Training loss: 1.9487500190734863
Validation loss: 2.263099637082828

Epoch: 6| Step: 6
Training loss: 2.3555736541748047
Validation loss: 2.2683644115283923

Epoch: 6| Step: 7
Training loss: 2.1822164058685303
Validation loss: 2.2811977324947232

Epoch: 6| Step: 8
Training loss: 2.6389636993408203
Validation loss: 2.2911509570255073

Epoch: 6| Step: 9
Training loss: 2.2209384441375732
Validation loss: 2.2967767766726914

Epoch: 6| Step: 10
Training loss: 2.5064444541931152
Validation loss: 2.299791794951244

Epoch: 6| Step: 11
Training loss: 2.5094263553619385
Validation loss: 2.2963095224031838

Epoch: 6| Step: 12
Training loss: 2.2658393383026123
Validation loss: 2.2777405682430474

Epoch: 6| Step: 13
Training loss: 2.134514331817627
Validation loss: 2.27085139161797

Epoch: 307| Step: 0
Training loss: 1.8115227222442627
Validation loss: 2.259862276815599

Epoch: 6| Step: 1
Training loss: 2.8283889293670654
Validation loss: 2.267770431374991

Epoch: 6| Step: 2
Training loss: 1.9102345705032349
Validation loss: 2.265291604944455

Epoch: 6| Step: 3
Training loss: 2.8215436935424805
Validation loss: 2.2681992438531693

Epoch: 6| Step: 4
Training loss: 2.1299376487731934
Validation loss: 2.2824583591953402

Epoch: 6| Step: 5
Training loss: 2.3707334995269775
Validation loss: 2.2863924682781263

Epoch: 6| Step: 6
Training loss: 2.2574081420898438
Validation loss: 2.2758746480429046

Epoch: 6| Step: 7
Training loss: 2.560732126235962
Validation loss: 2.2800616320743354

Epoch: 6| Step: 8
Training loss: 2.157681465148926
Validation loss: 2.290941419139985

Epoch: 6| Step: 9
Training loss: 2.3330907821655273
Validation loss: 2.2949203419429

Epoch: 6| Step: 10
Training loss: 3.3034262657165527
Validation loss: 2.2875397730899114

Epoch: 6| Step: 11
Training loss: 2.496276378631592
Validation loss: 2.273610475242779

Epoch: 6| Step: 12
Training loss: 2.7530910968780518
Validation loss: 2.256816933231969

Epoch: 6| Step: 13
Training loss: 1.9231840372085571
Validation loss: 2.2550982531680854

Epoch: 308| Step: 0
Training loss: 1.8972753286361694
Validation loss: 2.2377374633666007

Epoch: 6| Step: 1
Training loss: 3.4024267196655273
Validation loss: 2.2256186700636342

Epoch: 6| Step: 2
Training loss: 2.203835964202881
Validation loss: 2.2351520843403314

Epoch: 6| Step: 3
Training loss: 2.8224921226501465
Validation loss: 2.228116081606957

Epoch: 6| Step: 4
Training loss: 2.5506484508514404
Validation loss: 2.233701873851079

Epoch: 6| Step: 5
Training loss: 1.5644619464874268
Validation loss: 2.2310089436910485

Epoch: 6| Step: 6
Training loss: 2.739739179611206
Validation loss: 2.2284369237961306

Epoch: 6| Step: 7
Training loss: 2.178091049194336
Validation loss: 2.242709631560951

Epoch: 6| Step: 8
Training loss: 2.249114513397217
Validation loss: 2.256371146889143

Epoch: 6| Step: 9
Training loss: 2.760901927947998
Validation loss: 2.276583115259806

Epoch: 6| Step: 10
Training loss: 1.542141079902649
Validation loss: 2.3018606221804054

Epoch: 6| Step: 11
Training loss: 2.6851212978363037
Validation loss: 2.285819543305264

Epoch: 6| Step: 12
Training loss: 2.785388946533203
Validation loss: 2.29171053055794

Epoch: 6| Step: 13
Training loss: 2.4167935848236084
Validation loss: 2.293703258678477

Epoch: 309| Step: 0
Training loss: 2.1671061515808105
Validation loss: 2.2827012820910384

Epoch: 6| Step: 1
Training loss: 2.6295218467712402
Validation loss: 2.2748734540836786

Epoch: 6| Step: 2
Training loss: 2.16229510307312
Validation loss: 2.2862727565150105

Epoch: 6| Step: 3
Training loss: 2.986525774002075
Validation loss: 2.2873159916170183

Epoch: 6| Step: 4
Training loss: 2.426804542541504
Validation loss: 2.2856061663678897

Epoch: 6| Step: 5
Training loss: 3.048722267150879
Validation loss: 2.2750076504163843

Epoch: 6| Step: 6
Training loss: 1.901510238647461
Validation loss: 2.263831659029889

Epoch: 6| Step: 7
Training loss: 2.348525047302246
Validation loss: 2.2475496440805416

Epoch: 6| Step: 8
Training loss: 1.8750766515731812
Validation loss: 2.2477779106427263

Epoch: 6| Step: 9
Training loss: 2.2058990001678467
Validation loss: 2.236646686830828

Epoch: 6| Step: 10
Training loss: 2.5051159858703613
Validation loss: 2.2336274782816568

Epoch: 6| Step: 11
Training loss: 2.78486704826355
Validation loss: 2.2375723085095807

Epoch: 6| Step: 12
Training loss: 2.6080803871154785
Validation loss: 2.23461474398131

Epoch: 6| Step: 13
Training loss: 1.965985655784607
Validation loss: 2.236883886398808

Epoch: 310| Step: 0
Training loss: 2.4056806564331055
Validation loss: 2.2405556530080815

Epoch: 6| Step: 1
Training loss: 2.4624714851379395
Validation loss: 2.2398035936458136

Epoch: 6| Step: 2
Training loss: 1.98030686378479
Validation loss: 2.245333779242731

Epoch: 6| Step: 3
Training loss: 2.836573839187622
Validation loss: 2.2502184016730196

Epoch: 6| Step: 4
Training loss: 2.3942325115203857
Validation loss: 2.265007377952658

Epoch: 6| Step: 5
Training loss: 2.8831984996795654
Validation loss: 2.269383732990552

Epoch: 6| Step: 6
Training loss: 2.734694242477417
Validation loss: 2.255795371147894

Epoch: 6| Step: 7
Training loss: 1.840132236480713
Validation loss: 2.267262899747459

Epoch: 6| Step: 8
Training loss: 1.9190666675567627
Validation loss: 2.2771993298684396

Epoch: 6| Step: 9
Training loss: 2.3834681510925293
Validation loss: 2.276497253807642

Epoch: 6| Step: 10
Training loss: 2.169651985168457
Validation loss: 2.274790943309825

Epoch: 6| Step: 11
Training loss: 2.590097188949585
Validation loss: 2.267300549373832

Epoch: 6| Step: 12
Training loss: 2.4049875736236572
Validation loss: 2.288352176707278

Epoch: 6| Step: 13
Training loss: 2.8283941745758057
Validation loss: 2.2785340868016726

Epoch: 311| Step: 0
Training loss: 2.142636299133301
Validation loss: 2.2968080274520384

Epoch: 6| Step: 1
Training loss: 2.680995225906372
Validation loss: 2.29390089742599

Epoch: 6| Step: 2
Training loss: 2.6061558723449707
Validation loss: 2.282773211438169

Epoch: 6| Step: 3
Training loss: 2.783390760421753
Validation loss: 2.282750186099801

Epoch: 6| Step: 4
Training loss: 2.9124398231506348
Validation loss: 2.26729775756918

Epoch: 6| Step: 5
Training loss: 1.950305461883545
Validation loss: 2.2600771714282293

Epoch: 6| Step: 6
Training loss: 2.7948899269104004
Validation loss: 2.252476728090676

Epoch: 6| Step: 7
Training loss: 1.7046432495117188
Validation loss: 2.24541288293818

Epoch: 6| Step: 8
Training loss: 2.4938838481903076
Validation loss: 2.250730212016772

Epoch: 6| Step: 9
Training loss: 2.635131359100342
Validation loss: 2.2442978120619252

Epoch: 6| Step: 10
Training loss: 1.7891438007354736
Validation loss: 2.2505990151436097

Epoch: 6| Step: 11
Training loss: 2.352531909942627
Validation loss: 2.244738353196011

Epoch: 6| Step: 12
Training loss: 2.486912965774536
Validation loss: 2.250222016406316

Epoch: 6| Step: 13
Training loss: 2.269247055053711
Validation loss: 2.2569380306428477

Epoch: 312| Step: 0
Training loss: 2.9438109397888184
Validation loss: 2.2603006542369886

Epoch: 6| Step: 1
Training loss: 1.913520097732544
Validation loss: 2.2665830683964554

Epoch: 6| Step: 2
Training loss: 2.8318424224853516
Validation loss: 2.26333874271762

Epoch: 6| Step: 3
Training loss: 2.6666641235351562
Validation loss: 2.274159009738635

Epoch: 6| Step: 4
Training loss: 1.6180591583251953
Validation loss: 2.2730448271638606

Epoch: 6| Step: 5
Training loss: 2.9197535514831543
Validation loss: 2.277332070053265

Epoch: 6| Step: 6
Training loss: 2.7116527557373047
Validation loss: 2.269203814127112

Epoch: 6| Step: 7
Training loss: 1.6300925016403198
Validation loss: 2.2686515726068968

Epoch: 6| Step: 8
Training loss: 2.250795841217041
Validation loss: 2.256897682784706

Epoch: 6| Step: 9
Training loss: 2.5627565383911133
Validation loss: 2.2537270720287035

Epoch: 6| Step: 10
Training loss: 2.319387674331665
Validation loss: 2.2574623259164954

Epoch: 6| Step: 11
Training loss: 2.805778980255127
Validation loss: 2.2503825515829106

Epoch: 6| Step: 12
Training loss: 2.04073429107666
Validation loss: 2.269786047679122

Epoch: 6| Step: 13
Training loss: 2.487916946411133
Validation loss: 2.279171610391268

Epoch: 313| Step: 0
Training loss: 2.5770530700683594
Validation loss: 2.2833258105862524

Epoch: 6| Step: 1
Training loss: 2.352560043334961
Validation loss: 2.3014177276242163

Epoch: 6| Step: 2
Training loss: 2.4238409996032715
Validation loss: 2.2866514677642495

Epoch: 6| Step: 3
Training loss: 2.3916406631469727
Validation loss: 2.275103788222036

Epoch: 6| Step: 4
Training loss: 2.028027057647705
Validation loss: 2.2746537064993255

Epoch: 6| Step: 5
Training loss: 1.5684822797775269
Validation loss: 2.2575548336070073

Epoch: 6| Step: 6
Training loss: 2.340017795562744
Validation loss: 2.2516741239896385

Epoch: 6| Step: 7
Training loss: 1.90241539478302
Validation loss: 2.257916978610459

Epoch: 6| Step: 8
Training loss: 2.2345166206359863
Validation loss: 2.2502541234416347

Epoch: 6| Step: 9
Training loss: 2.757643222808838
Validation loss: 2.2630136653941166

Epoch: 6| Step: 10
Training loss: 3.250718116760254
Validation loss: 2.2505788162190425

Epoch: 6| Step: 11
Training loss: 2.3993539810180664
Validation loss: 2.2457517808483494

Epoch: 6| Step: 12
Training loss: 2.698202610015869
Validation loss: 2.2459609918696906

Epoch: 6| Step: 13
Training loss: 3.1791064739227295
Validation loss: 2.252818745951499

Epoch: 314| Step: 0
Training loss: 2.0878803730010986
Validation loss: 2.2532811087946736

Epoch: 6| Step: 1
Training loss: 3.050431251525879
Validation loss: 2.256321063605688

Epoch: 6| Step: 2
Training loss: 2.410043239593506
Validation loss: 2.2698144258991366

Epoch: 6| Step: 3
Training loss: 2.9991884231567383
Validation loss: 2.2468444660145748

Epoch: 6| Step: 4
Training loss: 2.62748384475708
Validation loss: 2.25366666752805

Epoch: 6| Step: 5
Training loss: 2.721768856048584
Validation loss: 2.2448218842988372

Epoch: 6| Step: 6
Training loss: 2.274675130844116
Validation loss: 2.233827947288431

Epoch: 6| Step: 7
Training loss: 2.0936875343322754
Validation loss: 2.2366196673403502

Epoch: 6| Step: 8
Training loss: 2.066871166229248
Validation loss: 2.225910699495705

Epoch: 6| Step: 9
Training loss: 3.0077402591705322
Validation loss: 2.22765709764214

Epoch: 6| Step: 10
Training loss: 2.610037326812744
Validation loss: 2.231625606936793

Epoch: 6| Step: 11
Training loss: 1.7834179401397705
Validation loss: 2.226451937870313

Epoch: 6| Step: 12
Training loss: 1.7363524436950684
Validation loss: 2.236644164208443

Epoch: 6| Step: 13
Training loss: 2.0083699226379395
Validation loss: 2.2346371348186205

Epoch: 315| Step: 0
Training loss: 2.450713634490967
Validation loss: 2.247185540455644

Epoch: 6| Step: 1
Training loss: 2.5476231575012207
Validation loss: 2.248181099532753

Epoch: 6| Step: 2
Training loss: 2.79339599609375
Validation loss: 2.2678478892131517

Epoch: 6| Step: 3
Training loss: 2.5816738605499268
Validation loss: 2.272366080232846

Epoch: 6| Step: 4
Training loss: 1.869554042816162
Validation loss: 2.292182032779981

Epoch: 6| Step: 5
Training loss: 2.3284695148468018
Validation loss: 2.2826976955577893

Epoch: 6| Step: 6
Training loss: 2.464315891265869
Validation loss: 2.288123684544717

Epoch: 6| Step: 7
Training loss: 2.5535876750946045
Validation loss: 2.2857775188261464

Epoch: 6| Step: 8
Training loss: 2.5318944454193115
Validation loss: 2.2722366920081516

Epoch: 6| Step: 9
Training loss: 2.640158176422119
Validation loss: 2.2585461857498332

Epoch: 6| Step: 10
Training loss: 1.7696399688720703
Validation loss: 2.2570244791687175

Epoch: 6| Step: 11
Training loss: 2.1611766815185547
Validation loss: 2.2644232870430074

Epoch: 6| Step: 12
Training loss: 2.76175594329834
Validation loss: 2.2557873533618067

Epoch: 6| Step: 13
Training loss: 1.9094526767730713
Validation loss: 2.259244380458709

Epoch: 316| Step: 0
Training loss: 2.1786367893218994
Validation loss: 2.2681583640395955

Epoch: 6| Step: 1
Training loss: 2.0662596225738525
Validation loss: 2.27325350751159

Epoch: 6| Step: 2
Training loss: 3.08366322517395
Validation loss: 2.2599797505204395

Epoch: 6| Step: 3
Training loss: 2.3605618476867676
Validation loss: 2.2760767167614353

Epoch: 6| Step: 4
Training loss: 2.235489845275879
Validation loss: 2.2630877776812484

Epoch: 6| Step: 5
Training loss: 3.0091021060943604
Validation loss: 2.251117002579474

Epoch: 6| Step: 6
Training loss: 2.7155563831329346
Validation loss: 2.228990303572788

Epoch: 6| Step: 7
Training loss: 2.298844337463379
Validation loss: 2.225687024413898

Epoch: 6| Step: 8
Training loss: 2.1712958812713623
Validation loss: 2.2128793834358134

Epoch: 6| Step: 9
Training loss: 2.2479028701782227
Validation loss: 2.224663357580862

Epoch: 6| Step: 10
Training loss: 2.417473077774048
Validation loss: 2.218236718126523

Epoch: 6| Step: 11
Training loss: 1.6808393001556396
Validation loss: 2.220577306644891

Epoch: 6| Step: 12
Training loss: 2.8617990016937256
Validation loss: 2.216644046127155

Epoch: 6| Step: 13
Training loss: 2.2000832557678223
Validation loss: 2.2337934381218365

Epoch: 317| Step: 0
Training loss: 1.9826542139053345
Validation loss: 2.2423404544912358

Epoch: 6| Step: 1
Training loss: 2.0406858921051025
Validation loss: 2.265132848934461

Epoch: 6| Step: 2
Training loss: 2.870156764984131
Validation loss: 2.26653132900115

Epoch: 6| Step: 3
Training loss: 2.5234861373901367
Validation loss: 2.268835944514121

Epoch: 6| Step: 4
Training loss: 2.8026466369628906
Validation loss: 2.267555708526283

Epoch: 6| Step: 5
Training loss: 2.7488701343536377
Validation loss: 2.2716728897504908

Epoch: 6| Step: 6
Training loss: 2.7135417461395264
Validation loss: 2.2559871724856797

Epoch: 6| Step: 7
Training loss: 2.636742115020752
Validation loss: 2.256107876377721

Epoch: 6| Step: 8
Training loss: 1.8373181819915771
Validation loss: 2.247055948421519

Epoch: 6| Step: 9
Training loss: 2.873806953430176
Validation loss: 2.2473222209561254

Epoch: 6| Step: 10
Training loss: 2.0985114574432373
Validation loss: 2.2501474426638697

Epoch: 6| Step: 11
Training loss: 1.8295308351516724
Validation loss: 2.254113689545662

Epoch: 6| Step: 12
Training loss: 1.9901565313339233
Validation loss: 2.2617734786002868

Epoch: 6| Step: 13
Training loss: 2.8971498012542725
Validation loss: 2.2512564864209903

Epoch: 318| Step: 0
Training loss: 2.397753953933716
Validation loss: 2.264547355713383

Epoch: 6| Step: 1
Training loss: 2.268805503845215
Validation loss: 2.2656807899475098

Epoch: 6| Step: 2
Training loss: 1.7104268074035645
Validation loss: 2.274273928775582

Epoch: 6| Step: 3
Training loss: 2.7197189331054688
Validation loss: 2.2831873175918416

Epoch: 6| Step: 4
Training loss: 1.9153048992156982
Validation loss: 2.2711498455334733

Epoch: 6| Step: 5
Training loss: 2.011430501937866
Validation loss: 2.2760666570355816

Epoch: 6| Step: 6
Training loss: 2.0458054542541504
Validation loss: 2.274461002760036

Epoch: 6| Step: 7
Training loss: 1.893356204032898
Validation loss: 2.2771040085823304

Epoch: 6| Step: 8
Training loss: 2.728273630142212
Validation loss: 2.2633710894533383

Epoch: 6| Step: 9
Training loss: 2.885101795196533
Validation loss: 2.2713462947517313

Epoch: 6| Step: 10
Training loss: 2.1180832386016846
Validation loss: 2.2787884807073944

Epoch: 6| Step: 11
Training loss: 3.305875778198242
Validation loss: 2.2704312647542646

Epoch: 6| Step: 12
Training loss: 2.922123908996582
Validation loss: 2.256937911433558

Epoch: 6| Step: 13
Training loss: 2.8540103435516357
Validation loss: 2.251354294438516

Epoch: 319| Step: 0
Training loss: 2.2815017700195312
Validation loss: 2.241188451807986

Epoch: 6| Step: 1
Training loss: 2.936249256134033
Validation loss: 2.222217834124001

Epoch: 6| Step: 2
Training loss: 2.9361979961395264
Validation loss: 2.2349476532269548

Epoch: 6| Step: 3
Training loss: 2.8164093494415283
Validation loss: 2.22732045829937

Epoch: 6| Step: 4
Training loss: 2.290048599243164
Validation loss: 2.2268670348710913

Epoch: 6| Step: 5
Training loss: 2.312567949295044
Validation loss: 2.2315084754779773

Epoch: 6| Step: 6
Training loss: 2.302560806274414
Validation loss: 2.231904293901177

Epoch: 6| Step: 7
Training loss: 2.105915069580078
Validation loss: 2.2269822089902815

Epoch: 6| Step: 8
Training loss: 1.7842419147491455
Validation loss: 2.221798917298676

Epoch: 6| Step: 9
Training loss: 1.8300175666809082
Validation loss: 2.2286178911885908

Epoch: 6| Step: 10
Training loss: 2.766847848892212
Validation loss: 2.226020331023842

Epoch: 6| Step: 11
Training loss: 1.8875467777252197
Validation loss: 2.2313141579269082

Epoch: 6| Step: 12
Training loss: 3.133305549621582
Validation loss: 2.24438633970035

Epoch: 6| Step: 13
Training loss: 2.1414577960968018
Validation loss: 2.266048587778563

Epoch: 320| Step: 0
Training loss: 2.704740047454834
Validation loss: 2.2711137776733725

Epoch: 6| Step: 1
Training loss: 2.680880546569824
Validation loss: 2.283471731729405

Epoch: 6| Step: 2
Training loss: 2.335681915283203
Validation loss: 2.298284935694869

Epoch: 6| Step: 3
Training loss: 2.7073655128479004
Validation loss: 2.284333162410285

Epoch: 6| Step: 4
Training loss: 2.8648247718811035
Validation loss: 2.285787326033397

Epoch: 6| Step: 5
Training loss: 2.358607769012451
Validation loss: 2.299174347231465

Epoch: 6| Step: 6
Training loss: 2.1923179626464844
Validation loss: 2.287519324210382

Epoch: 6| Step: 7
Training loss: 2.7784178256988525
Validation loss: 2.2927536323506343

Epoch: 6| Step: 8
Training loss: 2.2630064487457275
Validation loss: 2.280746665052188

Epoch: 6| Step: 9
Training loss: 1.5535379648208618
Validation loss: 2.2539062076999294

Epoch: 6| Step: 10
Training loss: 2.17838191986084
Validation loss: 2.238906321987029

Epoch: 6| Step: 11
Training loss: 3.040921211242676
Validation loss: 2.239295526217389

Epoch: 6| Step: 12
Training loss: 1.777114748954773
Validation loss: 2.2382888922127346

Epoch: 6| Step: 13
Training loss: 2.5284111499786377
Validation loss: 2.235413756421817

Epoch: 321| Step: 0
Training loss: 2.7080864906311035
Validation loss: 2.2335294831183647

Epoch: 6| Step: 1
Training loss: 1.9819211959838867
Validation loss: 2.2438213773953017

Epoch: 6| Step: 2
Training loss: 2.111344575881958
Validation loss: 2.233879481592486

Epoch: 6| Step: 3
Training loss: 2.154867649078369
Validation loss: 2.231280760098529

Epoch: 6| Step: 4
Training loss: 2.2769434452056885
Validation loss: 2.2358062498031126

Epoch: 6| Step: 5
Training loss: 2.2628753185272217
Validation loss: 2.2285883054938367

Epoch: 6| Step: 6
Training loss: 2.354879379272461
Validation loss: 2.2216437016763995

Epoch: 6| Step: 7
Training loss: 2.4916958808898926
Validation loss: 2.2078119298463226

Epoch: 6| Step: 8
Training loss: 2.5275063514709473
Validation loss: 2.2175620550750406

Epoch: 6| Step: 9
Training loss: 2.83842134475708
Validation loss: 2.206695874532064

Epoch: 6| Step: 10
Training loss: 2.8098807334899902
Validation loss: 2.2094000872745307

Epoch: 6| Step: 11
Training loss: 2.442563533782959
Validation loss: 2.2238029139016264

Epoch: 6| Step: 12
Training loss: 2.3151028156280518
Validation loss: 2.218731052132063

Epoch: 6| Step: 13
Training loss: 2.4349851608276367
Validation loss: 2.2270214583284114

Epoch: 322| Step: 0
Training loss: 2.17336106300354
Validation loss: 2.224346786416987

Epoch: 6| Step: 1
Training loss: 1.6759381294250488
Validation loss: 2.2415550396006596

Epoch: 6| Step: 2
Training loss: 3.0790977478027344
Validation loss: 2.2536260389512583

Epoch: 6| Step: 3
Training loss: 2.1152496337890625
Validation loss: 2.2467862636812272

Epoch: 6| Step: 4
Training loss: 3.209096908569336
Validation loss: 2.263196634989913

Epoch: 6| Step: 5
Training loss: 2.2078464031219482
Validation loss: 2.26541309202871

Epoch: 6| Step: 6
Training loss: 2.150865077972412
Validation loss: 2.244787600732619

Epoch: 6| Step: 7
Training loss: 2.21549391746521
Validation loss: 2.2343277828667754

Epoch: 6| Step: 8
Training loss: 2.227640151977539
Validation loss: 2.214906995014478

Epoch: 6| Step: 9
Training loss: 2.167658567428589
Validation loss: 2.217692791774709

Epoch: 6| Step: 10
Training loss: 2.589456081390381
Validation loss: 2.2172070472471175

Epoch: 6| Step: 11
Training loss: 3.0475361347198486
Validation loss: 2.2253481085582445

Epoch: 6| Step: 12
Training loss: 2.5449752807617188
Validation loss: 2.2235221196246404

Epoch: 6| Step: 13
Training loss: 2.0707473754882812
Validation loss: 2.2192324797312417

Epoch: 323| Step: 0
Training loss: 2.181644916534424
Validation loss: 2.2376519518513835

Epoch: 6| Step: 1
Training loss: 1.483469009399414
Validation loss: 2.2489860544922533

Epoch: 6| Step: 2
Training loss: 2.68542218208313
Validation loss: 2.2584570069466867

Epoch: 6| Step: 3
Training loss: 2.7934978008270264
Validation loss: 2.263594082606736

Epoch: 6| Step: 4
Training loss: 2.2026376724243164
Validation loss: 2.2720085779825845

Epoch: 6| Step: 5
Training loss: 2.5194132328033447
Validation loss: 2.2712998800380255

Epoch: 6| Step: 6
Training loss: 2.7726736068725586
Validation loss: 2.277137492292671

Epoch: 6| Step: 7
Training loss: 2.862293004989624
Validation loss: 2.2717420772839616

Epoch: 6| Step: 8
Training loss: 2.9890785217285156
Validation loss: 2.2827792449664046

Epoch: 6| Step: 9
Training loss: 2.23669695854187
Validation loss: 2.2695959460350776

Epoch: 6| Step: 10
Training loss: 2.284264087677002
Validation loss: 2.278220033132902

Epoch: 6| Step: 11
Training loss: 2.187049388885498
Validation loss: 2.26134003362348

Epoch: 6| Step: 12
Training loss: 2.1641252040863037
Validation loss: 2.240549234933751

Epoch: 6| Step: 13
Training loss: 1.804038166999817
Validation loss: 2.2455555418486237

Epoch: 324| Step: 0
Training loss: 2.8703713417053223
Validation loss: 2.2321596786540043

Epoch: 6| Step: 1
Training loss: 2.557546854019165
Validation loss: 2.231801691875663

Epoch: 6| Step: 2
Training loss: 2.4720983505249023
Validation loss: 2.247006477848176

Epoch: 6| Step: 3
Training loss: 2.3972060680389404
Validation loss: 2.2438669230348323

Epoch: 6| Step: 4
Training loss: 1.7122966051101685
Validation loss: 2.2464094008168867

Epoch: 6| Step: 5
Training loss: 2.1984333992004395
Validation loss: 2.2584082862382293

Epoch: 6| Step: 6
Training loss: 2.7231664657592773
Validation loss: 2.258912250559817

Epoch: 6| Step: 7
Training loss: 2.146585464477539
Validation loss: 2.2719143385528238

Epoch: 6| Step: 8
Training loss: 2.19354248046875
Validation loss: 2.264219953167823

Epoch: 6| Step: 9
Training loss: 2.473341226577759
Validation loss: 2.2708926354685137

Epoch: 6| Step: 10
Training loss: 2.995272159576416
Validation loss: 2.273058873350902

Epoch: 6| Step: 11
Training loss: 2.6653432846069336
Validation loss: 2.2509707635448826

Epoch: 6| Step: 12
Training loss: 1.6809754371643066
Validation loss: 2.231027255776108

Epoch: 6| Step: 13
Training loss: 2.50508451461792
Validation loss: 2.246376404198267

Epoch: 325| Step: 0
Training loss: 3.334202289581299
Validation loss: 2.2612973643887426

Epoch: 6| Step: 1
Training loss: 1.6668329238891602
Validation loss: 2.284723985579706

Epoch: 6| Step: 2
Training loss: 2.3796701431274414
Validation loss: 2.321356778503746

Epoch: 6| Step: 3
Training loss: 2.6329216957092285
Validation loss: 2.3566658009764967

Epoch: 6| Step: 4
Training loss: 2.402533531188965
Validation loss: 2.3634093333316106

Epoch: 6| Step: 5
Training loss: 2.4059536457061768
Validation loss: 2.3647802234977804

Epoch: 6| Step: 6
Training loss: 2.9150922298431396
Validation loss: 2.358307474402971

Epoch: 6| Step: 7
Training loss: 1.8503626585006714
Validation loss: 2.3439864650849374

Epoch: 6| Step: 8
Training loss: 1.9209511280059814
Validation loss: 2.322935058224586

Epoch: 6| Step: 9
Training loss: 3.164437770843506
Validation loss: 2.3042554111890894

Epoch: 6| Step: 10
Training loss: 2.2869768142700195
Validation loss: 2.288162648036916

Epoch: 6| Step: 11
Training loss: 2.9687259197235107
Validation loss: 2.271634090331293

Epoch: 6| Step: 12
Training loss: 2.294379234313965
Validation loss: 2.2521731879121516

Epoch: 6| Step: 13
Training loss: 2.731818199157715
Validation loss: 2.244452848229357

Epoch: 326| Step: 0
Training loss: 2.6622238159179688
Validation loss: 2.2366280337815643

Epoch: 6| Step: 1
Training loss: 2.2847867012023926
Validation loss: 2.239157046041181

Epoch: 6| Step: 2
Training loss: 2.1803359985351562
Validation loss: 2.234170344568068

Epoch: 6| Step: 3
Training loss: 2.402059555053711
Validation loss: 2.2377147110559608

Epoch: 6| Step: 4
Training loss: 2.2406818866729736
Validation loss: 2.2341801940753894

Epoch: 6| Step: 5
Training loss: 2.9666647911071777
Validation loss: 2.242535238624901

Epoch: 6| Step: 6
Training loss: 2.2929694652557373
Validation loss: 2.2598486997747935

Epoch: 6| Step: 7
Training loss: 1.4617178440093994
Validation loss: 2.254159983768258

Epoch: 6| Step: 8
Training loss: 2.6442806720733643
Validation loss: 2.2519073793965

Epoch: 6| Step: 9
Training loss: 2.848036050796509
Validation loss: 2.259245966070442

Epoch: 6| Step: 10
Training loss: 2.738983631134033
Validation loss: 2.2586100870563137

Epoch: 6| Step: 11
Training loss: 2.27706241607666
Validation loss: 2.2475730424286215

Epoch: 6| Step: 12
Training loss: 1.8973431587219238
Validation loss: 2.246339083999716

Epoch: 6| Step: 13
Training loss: 2.9781863689422607
Validation loss: 2.2475704403333765

Epoch: 327| Step: 0
Training loss: 1.9403579235076904
Validation loss: 2.241597462725896

Epoch: 6| Step: 1
Training loss: 2.247837543487549
Validation loss: 2.230118974562614

Epoch: 6| Step: 2
Training loss: 1.7121790647506714
Validation loss: 2.228635036817161

Epoch: 6| Step: 3
Training loss: 2.937161445617676
Validation loss: 2.2467661070567306

Epoch: 6| Step: 4
Training loss: 2.6996309757232666
Validation loss: 2.23177763723558

Epoch: 6| Step: 5
Training loss: 2.0030055046081543
Validation loss: 2.2351015665197886

Epoch: 6| Step: 6
Training loss: 2.9866464138031006
Validation loss: 2.2469219546164236

Epoch: 6| Step: 7
Training loss: 2.5132434368133545
Validation loss: 2.259981478414228

Epoch: 6| Step: 8
Training loss: 2.681065559387207
Validation loss: 2.2625619160231722

Epoch: 6| Step: 9
Training loss: 1.6954398155212402
Validation loss: 2.245141403649443

Epoch: 6| Step: 10
Training loss: 3.0033485889434814
Validation loss: 2.2445826645820373

Epoch: 6| Step: 11
Training loss: 2.7235894203186035
Validation loss: 2.2369919617970786

Epoch: 6| Step: 12
Training loss: 1.3889362812042236
Validation loss: 2.2262164982416297

Epoch: 6| Step: 13
Training loss: 3.007753610610962
Validation loss: 2.2245724329384426

Epoch: 328| Step: 0
Training loss: 1.815765380859375
Validation loss: 2.2247014032897128

Epoch: 6| Step: 1
Training loss: 2.7275495529174805
Validation loss: 2.221000973896314

Epoch: 6| Step: 2
Training loss: 2.4812021255493164
Validation loss: 2.2246633293808147

Epoch: 6| Step: 3
Training loss: 2.3711671829223633
Validation loss: 2.2085086171345045

Epoch: 6| Step: 4
Training loss: 2.47113037109375
Validation loss: 2.22441073899628

Epoch: 6| Step: 5
Training loss: 2.742471694946289
Validation loss: 2.2185913337174283

Epoch: 6| Step: 6
Training loss: 2.416271209716797
Validation loss: 2.2231364814184045

Epoch: 6| Step: 7
Training loss: 1.9429430961608887
Validation loss: 2.2220918568231727

Epoch: 6| Step: 8
Training loss: 2.09037446975708
Validation loss: 2.2242712590002243

Epoch: 6| Step: 9
Training loss: 2.3077237606048584
Validation loss: 2.2210929650132374

Epoch: 6| Step: 10
Training loss: 2.335188388824463
Validation loss: 2.2212004584650837

Epoch: 6| Step: 11
Training loss: 2.500399112701416
Validation loss: 2.231675350537864

Epoch: 6| Step: 12
Training loss: 3.115253210067749
Validation loss: 2.23518769971786

Epoch: 6| Step: 13
Training loss: 1.6668657064437866
Validation loss: 2.241173423746581

Epoch: 329| Step: 0
Training loss: 2.7367095947265625
Validation loss: 2.239107452413087

Epoch: 6| Step: 1
Training loss: 2.6609582901000977
Validation loss: 2.2536327403078795

Epoch: 6| Step: 2
Training loss: 1.9578394889831543
Validation loss: 2.244359529146584

Epoch: 6| Step: 3
Training loss: 2.4366021156311035
Validation loss: 2.2483784152615454

Epoch: 6| Step: 4
Training loss: 1.9205405712127686
Validation loss: 2.2427547747089016

Epoch: 6| Step: 5
Training loss: 2.75007963180542
Validation loss: 2.2615619013386388

Epoch: 6| Step: 6
Training loss: 2.2269792556762695
Validation loss: 2.2440153091184554

Epoch: 6| Step: 7
Training loss: 2.054395914077759
Validation loss: 2.229215560420867

Epoch: 6| Step: 8
Training loss: 3.3215315341949463
Validation loss: 2.2359500085153887

Epoch: 6| Step: 9
Training loss: 1.8609004020690918
Validation loss: 2.2344707212140484

Epoch: 6| Step: 10
Training loss: 2.5717215538024902
Validation loss: 2.2215673000581804

Epoch: 6| Step: 11
Training loss: 2.048706531524658
Validation loss: 2.2091828635943833

Epoch: 6| Step: 12
Training loss: 2.3076438903808594
Validation loss: 2.2171039530026015

Epoch: 6| Step: 13
Training loss: 2.456163167953491
Validation loss: 2.2173417716897945

Epoch: 330| Step: 0
Training loss: 1.940279483795166
Validation loss: 2.220808671366784

Epoch: 6| Step: 1
Training loss: 1.8270165920257568
Validation loss: 2.2185913157719437

Epoch: 6| Step: 2
Training loss: 3.3033454418182373
Validation loss: 2.2373852011977986

Epoch: 6| Step: 3
Training loss: 2.514904022216797
Validation loss: 2.226815208312004

Epoch: 6| Step: 4
Training loss: 2.2683262825012207
Validation loss: 2.241429590409802

Epoch: 6| Step: 5
Training loss: 1.98848557472229
Validation loss: 2.2381362351038123

Epoch: 6| Step: 6
Training loss: 3.0579051971435547
Validation loss: 2.248737017313639

Epoch: 6| Step: 7
Training loss: 2.6480631828308105
Validation loss: 2.238062958563528

Epoch: 6| Step: 8
Training loss: 2.5950117111206055
Validation loss: 2.247240153692102

Epoch: 6| Step: 9
Training loss: 2.400458812713623
Validation loss: 2.2491041460344867

Epoch: 6| Step: 10
Training loss: 2.209804058074951
Validation loss: 2.251490527583707

Epoch: 6| Step: 11
Training loss: 2.1372551918029785
Validation loss: 2.2556707602675243

Epoch: 6| Step: 12
Training loss: 2.0824947357177734
Validation loss: 2.2592800483908704

Epoch: 6| Step: 13
Training loss: 2.072030544281006
Validation loss: 2.2694808411341842

Epoch: 331| Step: 0
Training loss: 2.3439464569091797
Validation loss: 2.2714936220517723

Epoch: 6| Step: 1
Training loss: 2.1982476711273193
Validation loss: 2.2552026728148102

Epoch: 6| Step: 2
Training loss: 3.1307740211486816
Validation loss: 2.2327827022921656

Epoch: 6| Step: 3
Training loss: 3.0543723106384277
Validation loss: 2.2166665574555755

Epoch: 6| Step: 4
Training loss: 1.4212234020233154
Validation loss: 2.196835840902021

Epoch: 6| Step: 5
Training loss: 1.937674641609192
Validation loss: 2.2063684501955585

Epoch: 6| Step: 6
Training loss: 3.058133602142334
Validation loss: 2.2043048874024422

Epoch: 6| Step: 7
Training loss: 2.5734825134277344
Validation loss: 2.205990599047753

Epoch: 6| Step: 8
Training loss: 2.580423355102539
Validation loss: 2.2118951082229614

Epoch: 6| Step: 9
Training loss: 2.008324384689331
Validation loss: 2.193482433595965

Epoch: 6| Step: 10
Training loss: 2.2813901901245117
Validation loss: 2.2217892190461517

Epoch: 6| Step: 11
Training loss: 2.057227373123169
Validation loss: 2.2129188404288342

Epoch: 6| Step: 12
Training loss: 2.5989019870758057
Validation loss: 2.2292978763580322

Epoch: 6| Step: 13
Training loss: 2.08685040473938
Validation loss: 2.2308122470814693

Epoch: 332| Step: 0
Training loss: 1.792845368385315
Validation loss: 2.2255908289263324

Epoch: 6| Step: 1
Training loss: 1.6443716287612915
Validation loss: 2.2245364573694046

Epoch: 6| Step: 2
Training loss: 2.568572521209717
Validation loss: 2.2197250512338456

Epoch: 6| Step: 3
Training loss: 2.5301713943481445
Validation loss: 2.2287321193243868

Epoch: 6| Step: 4
Training loss: 2.135498523712158
Validation loss: 2.2474649003756944

Epoch: 6| Step: 5
Training loss: 1.528522253036499
Validation loss: 2.2571839517162693

Epoch: 6| Step: 6
Training loss: 2.944204568862915
Validation loss: 2.2841040626648934

Epoch: 6| Step: 7
Training loss: 2.5715489387512207
Validation loss: 2.2941549465220463

Epoch: 6| Step: 8
Training loss: 2.328573703765869
Validation loss: 2.2862333969403337

Epoch: 6| Step: 9
Training loss: 3.210226535797119
Validation loss: 2.2999909206103255

Epoch: 6| Step: 10
Training loss: 2.1948981285095215
Validation loss: 2.3158207913880706

Epoch: 6| Step: 11
Training loss: 3.0519495010375977
Validation loss: 2.3240762243988695

Epoch: 6| Step: 12
Training loss: 2.575211524963379
Validation loss: 2.3096022682805217

Epoch: 6| Step: 13
Training loss: 2.2918121814727783
Validation loss: 2.3057931251423334

Epoch: 333| Step: 0
Training loss: 2.8173179626464844
Validation loss: 2.274390879497733

Epoch: 6| Step: 1
Training loss: 2.44027042388916
Validation loss: 2.263280483984178

Epoch: 6| Step: 2
Training loss: 1.848871111869812
Validation loss: 2.2507782213149534

Epoch: 6| Step: 3
Training loss: 1.838808298110962
Validation loss: 2.2440059749029015

Epoch: 6| Step: 4
Training loss: 2.4579825401306152
Validation loss: 2.236284884073401

Epoch: 6| Step: 5
Training loss: 2.2918081283569336
Validation loss: 2.223455967441682

Epoch: 6| Step: 6
Training loss: 2.729799509048462
Validation loss: 2.207890522095465

Epoch: 6| Step: 7
Training loss: 2.2147388458251953
Validation loss: 2.209622283135691

Epoch: 6| Step: 8
Training loss: 1.9832115173339844
Validation loss: 2.2010053511588805

Epoch: 6| Step: 9
Training loss: 2.2884254455566406
Validation loss: 2.2117480590779293

Epoch: 6| Step: 10
Training loss: 2.446146011352539
Validation loss: 2.2080810377674718

Epoch: 6| Step: 11
Training loss: 2.978175640106201
Validation loss: 2.2145383511820147

Epoch: 6| Step: 12
Training loss: 2.7196829319000244
Validation loss: 2.2245303789774575

Epoch: 6| Step: 13
Training loss: 2.2852137088775635
Validation loss: 2.2365113022506877

Epoch: 334| Step: 0
Training loss: 1.7323055267333984
Validation loss: 2.2365483981306835

Epoch: 6| Step: 1
Training loss: 2.3874928951263428
Validation loss: 2.2431993023041756

Epoch: 6| Step: 2
Training loss: 2.600104808807373
Validation loss: 2.238694748570842

Epoch: 6| Step: 3
Training loss: 2.195032835006714
Validation loss: 2.2480292884252404

Epoch: 6| Step: 4
Training loss: 2.1080164909362793
Validation loss: 2.248040158261535

Epoch: 6| Step: 5
Training loss: 2.059941291809082
Validation loss: 2.251533026336342

Epoch: 6| Step: 6
Training loss: 1.9514329433441162
Validation loss: 2.2479772337021364

Epoch: 6| Step: 7
Training loss: 1.7549529075622559
Validation loss: 2.244560423717704

Epoch: 6| Step: 8
Training loss: 2.9110326766967773
Validation loss: 2.238697487820861

Epoch: 6| Step: 9
Training loss: 2.2590768337249756
Validation loss: 2.2275627325939875

Epoch: 6| Step: 10
Training loss: 3.1023106575012207
Validation loss: 2.2196706738523257

Epoch: 6| Step: 11
Training loss: 2.529125213623047
Validation loss: 2.2107716862873366

Epoch: 6| Step: 12
Training loss: 2.619065761566162
Validation loss: 2.2012837010045208

Epoch: 6| Step: 13
Training loss: 3.4406213760375977
Validation loss: 2.1955367724100747

Epoch: 335| Step: 0
Training loss: 2.891918420791626
Validation loss: 2.194792111714681

Epoch: 6| Step: 1
Training loss: 2.533167839050293
Validation loss: 2.1867427543927263

Epoch: 6| Step: 2
Training loss: 2.0970458984375
Validation loss: 2.1851432182455577

Epoch: 6| Step: 3
Training loss: 2.2846412658691406
Validation loss: 2.187278269439615

Epoch: 6| Step: 4
Training loss: 2.072038173675537
Validation loss: 2.2024862304810555

Epoch: 6| Step: 5
Training loss: 2.193629741668701
Validation loss: 2.1973380760479997

Epoch: 6| Step: 6
Training loss: 2.8337016105651855
Validation loss: 2.208412770302065

Epoch: 6| Step: 7
Training loss: 2.379645347595215
Validation loss: 2.2101193525457896

Epoch: 6| Step: 8
Training loss: 2.1676812171936035
Validation loss: 2.224582705446469

Epoch: 6| Step: 9
Training loss: 2.0724806785583496
Validation loss: 2.243658641333221

Epoch: 6| Step: 10
Training loss: 2.8943049907684326
Validation loss: 2.249731153570196

Epoch: 6| Step: 11
Training loss: 2.418604850769043
Validation loss: 2.246104665981826

Epoch: 6| Step: 12
Training loss: 2.443453311920166
Validation loss: 2.2232898717285483

Epoch: 6| Step: 13
Training loss: 1.6739364862442017
Validation loss: 2.2193546038801952

Epoch: 336| Step: 0
Training loss: 2.423434257507324
Validation loss: 2.22349057146298

Epoch: 6| Step: 1
Training loss: 2.066084146499634
Validation loss: 2.233300765355428

Epoch: 6| Step: 2
Training loss: 2.0058794021606445
Validation loss: 2.215440441203374

Epoch: 6| Step: 3
Training loss: 2.6331992149353027
Validation loss: 2.21352824600794

Epoch: 6| Step: 4
Training loss: 2.6922504901885986
Validation loss: 2.2207490398037817

Epoch: 6| Step: 5
Training loss: 2.661102294921875
Validation loss: 2.226381660789572

Epoch: 6| Step: 6
Training loss: 2.422988176345825
Validation loss: 2.225510997156943

Epoch: 6| Step: 7
Training loss: 2.7404732704162598
Validation loss: 2.229135795306134

Epoch: 6| Step: 8
Training loss: 2.138418197631836
Validation loss: 2.2237648720382364

Epoch: 6| Step: 9
Training loss: 1.573241949081421
Validation loss: 2.220926159171648

Epoch: 6| Step: 10
Training loss: 2.774538516998291
Validation loss: 2.211165148724792

Epoch: 6| Step: 11
Training loss: 2.1915459632873535
Validation loss: 2.215071637143371

Epoch: 6| Step: 12
Training loss: 1.802910566329956
Validation loss: 2.216192335210821

Epoch: 6| Step: 13
Training loss: 3.348496437072754
Validation loss: 2.2122936992235083

Epoch: 337| Step: 0
Training loss: 2.2141590118408203
Validation loss: 2.205472715439335

Epoch: 6| Step: 1
Training loss: 2.7191762924194336
Validation loss: 2.200554196552564

Epoch: 6| Step: 2
Training loss: 2.2384660243988037
Validation loss: 2.1946814752394155

Epoch: 6| Step: 3
Training loss: 2.7532572746276855
Validation loss: 2.21026518524334

Epoch: 6| Step: 4
Training loss: 2.066577434539795
Validation loss: 2.211267335440523

Epoch: 6| Step: 5
Training loss: 2.444962978363037
Validation loss: 2.2071364028479463

Epoch: 6| Step: 6
Training loss: 2.439945697784424
Validation loss: 2.2160688074686195

Epoch: 6| Step: 7
Training loss: 2.2624778747558594
Validation loss: 2.2200642375535864

Epoch: 6| Step: 8
Training loss: 1.5178550481796265
Validation loss: 2.2157396424201226

Epoch: 6| Step: 9
Training loss: 3.228581428527832
Validation loss: 2.2325000583484607

Epoch: 6| Step: 10
Training loss: 2.835223436355591
Validation loss: 2.2200894868502052

Epoch: 6| Step: 11
Training loss: 1.7222976684570312
Validation loss: 2.2355715023574008

Epoch: 6| Step: 12
Training loss: 2.3482141494750977
Validation loss: 2.2303691910159205

Epoch: 6| Step: 13
Training loss: 2.4880928993225098
Validation loss: 2.228223341767506

Epoch: 338| Step: 0
Training loss: 2.830124855041504
Validation loss: 2.2229440442977415

Epoch: 6| Step: 1
Training loss: 2.6367688179016113
Validation loss: 2.2140475678187546

Epoch: 6| Step: 2
Training loss: 2.5721917152404785
Validation loss: 2.2182310858080463

Epoch: 6| Step: 3
Training loss: 2.5483646392822266
Validation loss: 2.2268197600559523

Epoch: 6| Step: 4
Training loss: 2.641784191131592
Validation loss: 2.2111070822643977

Epoch: 6| Step: 5
Training loss: 1.8109630346298218
Validation loss: 2.2222554888776553

Epoch: 6| Step: 6
Training loss: 3.0540966987609863
Validation loss: 2.2291839802136986

Epoch: 6| Step: 7
Training loss: 1.5543010234832764
Validation loss: 2.2403868141994683

Epoch: 6| Step: 8
Training loss: 1.934760332107544
Validation loss: 2.2305888732274375

Epoch: 6| Step: 9
Training loss: 1.8911539316177368
Validation loss: 2.2361543281103975

Epoch: 6| Step: 10
Training loss: 1.6548775434494019
Validation loss: 2.2505898796102053

Epoch: 6| Step: 11
Training loss: 2.5042362213134766
Validation loss: 2.241656018841651

Epoch: 6| Step: 12
Training loss: 3.121504545211792
Validation loss: 2.232454002544444

Epoch: 6| Step: 13
Training loss: 2.4836387634277344
Validation loss: 2.2327215902266966

Epoch: 339| Step: 0
Training loss: 2.3051037788391113
Validation loss: 2.2331863987830376

Epoch: 6| Step: 1
Training loss: 2.707280158996582
Validation loss: 2.219128090848205

Epoch: 6| Step: 2
Training loss: 2.4482295513153076
Validation loss: 2.2298666174693773

Epoch: 6| Step: 3
Training loss: 2.680614471435547
Validation loss: 2.232427659855094

Epoch: 6| Step: 4
Training loss: 2.1911797523498535
Validation loss: 2.2244422538306123

Epoch: 6| Step: 5
Training loss: 1.4871494770050049
Validation loss: 2.221064611147809

Epoch: 6| Step: 6
Training loss: 2.4291491508483887
Validation loss: 2.218230257752121

Epoch: 6| Step: 7
Training loss: 2.701493263244629
Validation loss: 2.217375055436165

Epoch: 6| Step: 8
Training loss: 2.5819272994995117
Validation loss: 2.2007776460339947

Epoch: 6| Step: 9
Training loss: 2.538984537124634
Validation loss: 2.185115847536313

Epoch: 6| Step: 10
Training loss: 1.915506362915039
Validation loss: 2.184051477780906

Epoch: 6| Step: 11
Training loss: 2.3255295753479004
Validation loss: 2.1676178004152034

Epoch: 6| Step: 12
Training loss: 2.1579623222351074
Validation loss: 2.175807160715903

Epoch: 6| Step: 13
Training loss: 2.6114847660064697
Validation loss: 2.181659403667655

Epoch: 340| Step: 0
Training loss: 2.1454832553863525
Validation loss: 2.1793774815015894

Epoch: 6| Step: 1
Training loss: 2.443329334259033
Validation loss: 2.184892346782069

Epoch: 6| Step: 2
Training loss: 2.4528160095214844
Validation loss: 2.1917766704354236

Epoch: 6| Step: 3
Training loss: 2.337074041366577
Validation loss: 2.2094418694896083

Epoch: 6| Step: 4
Training loss: 1.7945133447647095
Validation loss: 2.2092399520258748

Epoch: 6| Step: 5
Training loss: 2.9420647621154785
Validation loss: 2.2283746375832507

Epoch: 6| Step: 6
Training loss: 2.0902438163757324
Validation loss: 2.249319650793588

Epoch: 6| Step: 7
Training loss: 1.7957340478897095
Validation loss: 2.244659600719329

Epoch: 6| Step: 8
Training loss: 2.8948495388031006
Validation loss: 2.2717443589241273

Epoch: 6| Step: 9
Training loss: 3.2094664573669434
Validation loss: 2.2913010838211223

Epoch: 6| Step: 10
Training loss: 2.186605453491211
Validation loss: 2.2598965039817234

Epoch: 6| Step: 11
Training loss: 2.635587453842163
Validation loss: 2.2463773783817085

Epoch: 6| Step: 12
Training loss: 2.077927350997925
Validation loss: 2.249175194771059

Epoch: 6| Step: 13
Training loss: 1.8787599802017212
Validation loss: 2.228778418674264

Epoch: 341| Step: 0
Training loss: 2.1342902183532715
Validation loss: 2.2231609359864266

Epoch: 6| Step: 1
Training loss: 2.0911238193511963
Validation loss: 2.216423470486877

Epoch: 6| Step: 2
Training loss: 3.1194019317626953
Validation loss: 2.210539597336964

Epoch: 6| Step: 3
Training loss: 2.0665171146392822
Validation loss: 2.2101058293414373

Epoch: 6| Step: 4
Training loss: 2.240550994873047
Validation loss: 2.1972386067913425

Epoch: 6| Step: 5
Training loss: 2.2801365852355957
Validation loss: 2.2101145559741604

Epoch: 6| Step: 6
Training loss: 2.6553006172180176
Validation loss: 2.2310537087020053

Epoch: 6| Step: 7
Training loss: 2.6142096519470215
Validation loss: 2.236634623619818

Epoch: 6| Step: 8
Training loss: 2.5987548828125
Validation loss: 2.2369744546951784

Epoch: 6| Step: 9
Training loss: 2.3834784030914307
Validation loss: 2.241084062924949

Epoch: 6| Step: 10
Training loss: 2.8602139949798584
Validation loss: 2.246365944544474

Epoch: 6| Step: 11
Training loss: 1.8675498962402344
Validation loss: 2.2229816913604736

Epoch: 6| Step: 12
Training loss: 1.968598484992981
Validation loss: 2.2253983687329035

Epoch: 6| Step: 13
Training loss: 1.8869904279708862
Validation loss: 2.218368238018405

Epoch: 342| Step: 0
Training loss: 2.6671719551086426
Validation loss: 2.2215426532171105

Epoch: 6| Step: 1
Training loss: 2.2605857849121094
Validation loss: 2.2150065898895264

Epoch: 6| Step: 2
Training loss: 1.7088404893875122
Validation loss: 2.2221789513864825

Epoch: 6| Step: 3
Training loss: 2.7176356315612793
Validation loss: 2.207820064278059

Epoch: 6| Step: 4
Training loss: 2.634167194366455
Validation loss: 2.217817884619518

Epoch: 6| Step: 5
Training loss: 2.386648654937744
Validation loss: 2.2214312450860136

Epoch: 6| Step: 6
Training loss: 2.6248717308044434
Validation loss: 2.225272447832169

Epoch: 6| Step: 7
Training loss: 1.7508726119995117
Validation loss: 2.236387450207946

Epoch: 6| Step: 8
Training loss: 2.203767776489258
Validation loss: 2.2418360722962247

Epoch: 6| Step: 9
Training loss: 2.983035087585449
Validation loss: 2.2239281464648504

Epoch: 6| Step: 10
Training loss: 2.321950912475586
Validation loss: 2.2162512476726244

Epoch: 6| Step: 11
Training loss: 1.8998059034347534
Validation loss: 2.206586863404961

Epoch: 6| Step: 12
Training loss: 2.3409600257873535
Validation loss: 2.208179691786407

Epoch: 6| Step: 13
Training loss: 2.461841106414795
Validation loss: 2.218124971594862

Epoch: 343| Step: 0
Training loss: 1.7909574508666992
Validation loss: 2.213822139206753

Epoch: 6| Step: 1
Training loss: 2.8666281700134277
Validation loss: 2.217071365284663

Epoch: 6| Step: 2
Training loss: 2.6034927368164062
Validation loss: 2.249587335894185

Epoch: 6| Step: 3
Training loss: 2.1622724533081055
Validation loss: 2.2651391849722913

Epoch: 6| Step: 4
Training loss: 2.516371965408325
Validation loss: 2.2639399036284416

Epoch: 6| Step: 5
Training loss: 2.272850751876831
Validation loss: 2.2462803522745767

Epoch: 6| Step: 6
Training loss: 2.4690027236938477
Validation loss: 2.2555329286923973

Epoch: 6| Step: 7
Training loss: 2.1321802139282227
Validation loss: 2.239222308640839

Epoch: 6| Step: 8
Training loss: 2.5332980155944824
Validation loss: 2.2216662796594764

Epoch: 6| Step: 9
Training loss: 2.0538411140441895
Validation loss: 2.2226847435838435

Epoch: 6| Step: 10
Training loss: 2.8015947341918945
Validation loss: 2.2224327492457565

Epoch: 6| Step: 11
Training loss: 2.2921643257141113
Validation loss: 2.208453078423777

Epoch: 6| Step: 12
Training loss: 1.9886443614959717
Validation loss: 2.1903619458598476

Epoch: 6| Step: 13
Training loss: 2.579620599746704
Validation loss: 2.189537972532293

Epoch: 344| Step: 0
Training loss: 2.4224627017974854
Validation loss: 2.1779584115551365

Epoch: 6| Step: 1
Training loss: 2.3795530796051025
Validation loss: 2.1934054192676338

Epoch: 6| Step: 2
Training loss: 2.148259162902832
Validation loss: 2.201800648884107

Epoch: 6| Step: 3
Training loss: 2.0012669563293457
Validation loss: 2.1952671671426423

Epoch: 6| Step: 4
Training loss: 2.645369291305542
Validation loss: 2.214728219534761

Epoch: 6| Step: 5
Training loss: 2.3392934799194336
Validation loss: 2.215017403325727

Epoch: 6| Step: 6
Training loss: 2.07352352142334
Validation loss: 2.2069220517271306

Epoch: 6| Step: 7
Training loss: 3.1508374214172363
Validation loss: 2.230530790103379

Epoch: 6| Step: 8
Training loss: 2.1125001907348633
Validation loss: 2.2299840245195615

Epoch: 6| Step: 9
Training loss: 1.9974253177642822
Validation loss: 2.23495392645559

Epoch: 6| Step: 10
Training loss: 2.3246192932128906
Validation loss: 2.2392681849900113

Epoch: 6| Step: 11
Training loss: 2.6165804862976074
Validation loss: 2.228853875590909

Epoch: 6| Step: 12
Training loss: 2.3503599166870117
Validation loss: 2.213022147455523

Epoch: 6| Step: 13
Training loss: 2.0335772037506104
Validation loss: 2.2085111782114994

Epoch: 345| Step: 0
Training loss: 1.9892094135284424
Validation loss: 2.190607960506152

Epoch: 6| Step: 1
Training loss: 2.6422390937805176
Validation loss: 2.1904000184869252

Epoch: 6| Step: 2
Training loss: 3.1462461948394775
Validation loss: 2.179622814219485

Epoch: 6| Step: 3
Training loss: 2.4597973823547363
Validation loss: 2.1892121735439507

Epoch: 6| Step: 4
Training loss: 2.7030892372131348
Validation loss: 2.172540230135764

Epoch: 6| Step: 5
Training loss: 1.9826515913009644
Validation loss: 2.1753691832224527

Epoch: 6| Step: 6
Training loss: 2.697971820831299
Validation loss: 2.1786110836972474

Epoch: 6| Step: 7
Training loss: 1.9594316482543945
Validation loss: 2.186729718280095

Epoch: 6| Step: 8
Training loss: 2.6298952102661133
Validation loss: 2.1817793423129666

Epoch: 6| Step: 9
Training loss: 1.858468770980835
Validation loss: 2.1783547478337444

Epoch: 6| Step: 10
Training loss: 2.106813430786133
Validation loss: 2.1907464740096882

Epoch: 6| Step: 11
Training loss: 2.2883150577545166
Validation loss: 2.203423284715222

Epoch: 6| Step: 12
Training loss: 2.0391128063201904
Validation loss: 2.2161157797741633

Epoch: 6| Step: 13
Training loss: 2.2133867740631104
Validation loss: 2.2372302855214765

Epoch: 346| Step: 0
Training loss: 1.938951015472412
Validation loss: 2.2471511517801592

Epoch: 6| Step: 1
Training loss: 2.0405123233795166
Validation loss: 2.2359458656721216

Epoch: 6| Step: 2
Training loss: 1.8143566846847534
Validation loss: 2.2339485896530973

Epoch: 6| Step: 3
Training loss: 3.0097391605377197
Validation loss: 2.2138789443559546

Epoch: 6| Step: 4
Training loss: 2.2642555236816406
Validation loss: 2.200203590495612

Epoch: 6| Step: 5
Training loss: 3.1403937339782715
Validation loss: 2.194545620231218

Epoch: 6| Step: 6
Training loss: 2.746638298034668
Validation loss: 2.198116274290187

Epoch: 6| Step: 7
Training loss: 2.2267510890960693
Validation loss: 2.200189875018212

Epoch: 6| Step: 8
Training loss: 2.2367770671844482
Validation loss: 2.192083038309569

Epoch: 6| Step: 9
Training loss: 2.431346893310547
Validation loss: 2.1819434114681777

Epoch: 6| Step: 10
Training loss: 2.311885356903076
Validation loss: 2.1797538624014905

Epoch: 6| Step: 11
Training loss: 2.55633544921875
Validation loss: 2.193739938479598

Epoch: 6| Step: 12
Training loss: 1.8912347555160522
Validation loss: 2.204408671266289

Epoch: 6| Step: 13
Training loss: 2.2440507411956787
Validation loss: 2.2189263707848004

Epoch: 347| Step: 0
Training loss: 1.8969227075576782
Validation loss: 2.2219227167867843

Epoch: 6| Step: 1
Training loss: 2.491147756576538
Validation loss: 2.2254514963396135

Epoch: 6| Step: 2
Training loss: 2.3494491577148438
Validation loss: 2.236405021400862

Epoch: 6| Step: 3
Training loss: 2.2205376625061035
Validation loss: 2.2165456420631817

Epoch: 6| Step: 4
Training loss: 2.183023452758789
Validation loss: 2.2014068108732983

Epoch: 6| Step: 5
Training loss: 1.8192071914672852
Validation loss: 2.188172673666349

Epoch: 6| Step: 6
Training loss: 2.449122905731201
Validation loss: 2.1950330785525742

Epoch: 6| Step: 7
Training loss: 2.243541717529297
Validation loss: 2.194871271810224

Epoch: 6| Step: 8
Training loss: 2.7623062133789062
Validation loss: 2.1965688582389586

Epoch: 6| Step: 9
Training loss: 2.2738165855407715
Validation loss: 2.1907437975688646

Epoch: 6| Step: 10
Training loss: 2.2627973556518555
Validation loss: 2.1882260871189896

Epoch: 6| Step: 11
Training loss: 2.7480082511901855
Validation loss: 2.1842833924037155

Epoch: 6| Step: 12
Training loss: 2.741212844848633
Validation loss: 2.1813651131045435

Epoch: 6| Step: 13
Training loss: 2.0048623085021973
Validation loss: 2.1808627651583765

Epoch: 348| Step: 0
Training loss: 2.464667320251465
Validation loss: 2.2062593736956195

Epoch: 6| Step: 1
Training loss: 1.524856448173523
Validation loss: 2.1992777534710464

Epoch: 6| Step: 2
Training loss: 2.793816089630127
Validation loss: 2.1932532889868623

Epoch: 6| Step: 3
Training loss: 2.488534927368164
Validation loss: 2.201339675534156

Epoch: 6| Step: 4
Training loss: 2.5964226722717285
Validation loss: 2.208335304772982

Epoch: 6| Step: 5
Training loss: 2.215514659881592
Validation loss: 2.191063045173563

Epoch: 6| Step: 6
Training loss: 2.078212022781372
Validation loss: 2.1935946633738856

Epoch: 6| Step: 7
Training loss: 1.8669092655181885
Validation loss: 2.2138825308892036

Epoch: 6| Step: 8
Training loss: 2.038383960723877
Validation loss: 2.1971241402369674

Epoch: 6| Step: 9
Training loss: 2.238414764404297
Validation loss: 2.2128650398664576

Epoch: 6| Step: 10
Training loss: 2.5305733680725098
Validation loss: 2.2248441147547897

Epoch: 6| Step: 11
Training loss: 2.8285481929779053
Validation loss: 2.227082142265894

Epoch: 6| Step: 12
Training loss: 2.448578119277954
Validation loss: 2.2267631356434157

Epoch: 6| Step: 13
Training loss: 2.462050199508667
Validation loss: 2.2533049173252557

Epoch: 349| Step: 0
Training loss: 2.6374549865722656
Validation loss: 2.2384552622354157

Epoch: 6| Step: 1
Training loss: 3.3232789039611816
Validation loss: 2.2357836282381447

Epoch: 6| Step: 2
Training loss: 2.3346970081329346
Validation loss: 2.219975568914926

Epoch: 6| Step: 3
Training loss: 2.109473943710327
Validation loss: 2.2150422732035318

Epoch: 6| Step: 4
Training loss: 1.8836628198623657
Validation loss: 2.185265279585315

Epoch: 6| Step: 5
Training loss: 2.2358808517456055
Validation loss: 2.1818422937905915

Epoch: 6| Step: 6
Training loss: 2.4600822925567627
Validation loss: 2.177556037902832

Epoch: 6| Step: 7
Training loss: 1.8566045761108398
Validation loss: 2.1748258349716023

Epoch: 6| Step: 8
Training loss: 1.9139492511749268
Validation loss: 2.168900671825614

Epoch: 6| Step: 9
Training loss: 2.903526544570923
Validation loss: 2.1804402233451925

Epoch: 6| Step: 10
Training loss: 2.5846760272979736
Validation loss: 2.1768740556573354

Epoch: 6| Step: 11
Training loss: 2.370929718017578
Validation loss: 2.18404963708693

Epoch: 6| Step: 12
Training loss: 1.6781964302062988
Validation loss: 2.18429777314586

Epoch: 6| Step: 13
Training loss: 2.6509993076324463
Validation loss: 2.184058604701873

Epoch: 350| Step: 0
Training loss: 1.9996320009231567
Validation loss: 2.1729102878160376

Epoch: 6| Step: 1
Training loss: 2.682460308074951
Validation loss: 2.1780896443192677

Epoch: 6| Step: 2
Training loss: 2.4794793128967285
Validation loss: 2.187566982802524

Epoch: 6| Step: 3
Training loss: 1.9868979454040527
Validation loss: 2.185660921117311

Epoch: 6| Step: 4
Training loss: 2.2889420986175537
Validation loss: 2.185633920854138

Epoch: 6| Step: 5
Training loss: 1.4164154529571533
Validation loss: 2.2093101342519126

Epoch: 6| Step: 6
Training loss: 1.9962186813354492
Validation loss: 2.201898355637827

Epoch: 6| Step: 7
Training loss: 2.7233896255493164
Validation loss: 2.2085907702804892

Epoch: 6| Step: 8
Training loss: 2.794649124145508
Validation loss: 2.1928251020369993

Epoch: 6| Step: 9
Training loss: 2.2207720279693604
Validation loss: 2.190330272079796

Epoch: 6| Step: 10
Training loss: 1.903118371963501
Validation loss: 2.185237648666546

Epoch: 6| Step: 11
Training loss: 3.153198719024658
Validation loss: 2.1883329934971307

Epoch: 6| Step: 12
Training loss: 2.702147960662842
Validation loss: 2.186102536416823

Epoch: 6| Step: 13
Training loss: 2.172560691833496
Validation loss: 2.1699347137123026

Epoch: 351| Step: 0
Training loss: 2.1169605255126953
Validation loss: 2.1760917209809825

Epoch: 6| Step: 1
Training loss: 2.393540382385254
Validation loss: 2.185125502206946

Epoch: 6| Step: 2
Training loss: 2.168015480041504
Validation loss: 2.197804635570895

Epoch: 6| Step: 3
Training loss: 2.4067153930664062
Validation loss: 2.2007460183994745

Epoch: 6| Step: 4
Training loss: 2.3278865814208984
Validation loss: 2.1965835658452844

Epoch: 6| Step: 5
Training loss: 2.6703028678894043
Validation loss: 2.212037863269929

Epoch: 6| Step: 6
Training loss: 2.442877769470215
Validation loss: 2.199017914392615

Epoch: 6| Step: 7
Training loss: 2.2887139320373535
Validation loss: 2.2103130919958955

Epoch: 6| Step: 8
Training loss: 2.4687299728393555
Validation loss: 2.2026165595618625

Epoch: 6| Step: 9
Training loss: 2.2608237266540527
Validation loss: 2.2000099074456

Epoch: 6| Step: 10
Training loss: 2.1961169242858887
Validation loss: 2.207775878649886

Epoch: 6| Step: 11
Training loss: 1.9378154277801514
Validation loss: 2.2144466189927954

Epoch: 6| Step: 12
Training loss: 2.219770669937134
Validation loss: 2.218563272107032

Epoch: 6| Step: 13
Training loss: 2.6575615406036377
Validation loss: 2.2063106541992514

Epoch: 352| Step: 0
Training loss: 2.2234275341033936
Validation loss: 2.197901774478215

Epoch: 6| Step: 1
Training loss: 2.295454978942871
Validation loss: 2.1941399343552126

Epoch: 6| Step: 2
Training loss: 2.0323543548583984
Validation loss: 2.2000457881599345

Epoch: 6| Step: 3
Training loss: 1.9759423732757568
Validation loss: 2.1967387853130216

Epoch: 6| Step: 4
Training loss: 2.724125623703003
Validation loss: 2.195332814288396

Epoch: 6| Step: 5
Training loss: 3.0107269287109375
Validation loss: 2.18711866101911

Epoch: 6| Step: 6
Training loss: 2.658245086669922
Validation loss: 2.189207903800472

Epoch: 6| Step: 7
Training loss: 1.7711408138275146
Validation loss: 2.204534330675679

Epoch: 6| Step: 8
Training loss: 2.4933087825775146
Validation loss: 2.2084554600459274

Epoch: 6| Step: 9
Training loss: 1.6641790866851807
Validation loss: 2.21560993630399

Epoch: 6| Step: 10
Training loss: 1.6979622840881348
Validation loss: 2.209370211888385

Epoch: 6| Step: 11
Training loss: 2.4386563301086426
Validation loss: 2.220948173153785

Epoch: 6| Step: 12
Training loss: 3.0820517539978027
Validation loss: 2.2103123382855485

Epoch: 6| Step: 13
Training loss: 2.4310944080352783
Validation loss: 2.211072717943499

Epoch: 353| Step: 0
Training loss: 1.9698665142059326
Validation loss: 2.20696012948149

Epoch: 6| Step: 1
Training loss: 1.5555742979049683
Validation loss: 2.1972582583786338

Epoch: 6| Step: 2
Training loss: 2.013718605041504
Validation loss: 2.1890750392790763

Epoch: 6| Step: 3
Training loss: 2.206162929534912
Validation loss: 2.1926519665666806

Epoch: 6| Step: 4
Training loss: 2.7912392616271973
Validation loss: 2.1927633029158398

Epoch: 6| Step: 5
Training loss: 2.147698402404785
Validation loss: 2.1997631493435112

Epoch: 6| Step: 6
Training loss: 2.4248976707458496
Validation loss: 2.1804241108637985

Epoch: 6| Step: 7
Training loss: 2.487452983856201
Validation loss: 2.2010959143279702

Epoch: 6| Step: 8
Training loss: 2.3094818592071533
Validation loss: 2.199277429170506

Epoch: 6| Step: 9
Training loss: 2.295781135559082
Validation loss: 2.1991238773510022

Epoch: 6| Step: 10
Training loss: 2.595390796661377
Validation loss: 2.196886052367508

Epoch: 6| Step: 11
Training loss: 2.41497802734375
Validation loss: 2.1890567297576577

Epoch: 6| Step: 12
Training loss: 2.507124185562134
Validation loss: 2.2088424954363095

Epoch: 6| Step: 13
Training loss: 3.1285743713378906
Validation loss: 2.2192532862386396

Epoch: 354| Step: 0
Training loss: 1.9502983093261719
Validation loss: 2.2279287743312057

Epoch: 6| Step: 1
Training loss: 2.3512158393859863
Validation loss: 2.2542959977221746

Epoch: 6| Step: 2
Training loss: 2.667656183242798
Validation loss: 2.2414036258574455

Epoch: 6| Step: 3
Training loss: 2.2981131076812744
Validation loss: 2.2329720168985348

Epoch: 6| Step: 4
Training loss: 2.0863189697265625
Validation loss: 2.2345817217262844

Epoch: 6| Step: 5
Training loss: 2.9379968643188477
Validation loss: 2.2169128284659436

Epoch: 6| Step: 6
Training loss: 2.369180679321289
Validation loss: 2.2135243403014315

Epoch: 6| Step: 7
Training loss: 2.07370662689209
Validation loss: 2.20733489272415

Epoch: 6| Step: 8
Training loss: 2.28291654586792
Validation loss: 2.207024530697894

Epoch: 6| Step: 9
Training loss: 2.464477062225342
Validation loss: 2.1863888438029955

Epoch: 6| Step: 10
Training loss: 1.623165249824524
Validation loss: 2.1583896324198735

Epoch: 6| Step: 11
Training loss: 2.0018811225891113
Validation loss: 2.167833077010288

Epoch: 6| Step: 12
Training loss: 2.2123653888702393
Validation loss: 2.1603051411208285

Epoch: 6| Step: 13
Training loss: 3.657562255859375
Validation loss: 2.16577164588436

Epoch: 355| Step: 0
Training loss: 2.8985977172851562
Validation loss: 2.181515009172501

Epoch: 6| Step: 1
Training loss: 2.337587594985962
Validation loss: 2.1761715873595207

Epoch: 6| Step: 2
Training loss: 2.274667739868164
Validation loss: 2.1756130290287796

Epoch: 6| Step: 3
Training loss: 2.1280980110168457
Validation loss: 2.180691208890689

Epoch: 6| Step: 4
Training loss: 2.6016762256622314
Validation loss: 2.213050329557029

Epoch: 6| Step: 5
Training loss: 2.1621859073638916
Validation loss: 2.2424012050833753

Epoch: 6| Step: 6
Training loss: 2.047276496887207
Validation loss: 2.2461652960828555

Epoch: 6| Step: 7
Training loss: 2.494446277618408
Validation loss: 2.244987851829939

Epoch: 6| Step: 8
Training loss: 2.839958667755127
Validation loss: 2.22738972274206

Epoch: 6| Step: 9
Training loss: 1.8708131313323975
Validation loss: 2.2029415894580144

Epoch: 6| Step: 10
Training loss: 1.651303768157959
Validation loss: 2.1991600605749313

Epoch: 6| Step: 11
Training loss: 2.6917593479156494
Validation loss: 2.1911622721661805

Epoch: 6| Step: 12
Training loss: 2.3859968185424805
Validation loss: 2.1965269196418022

Epoch: 6| Step: 13
Training loss: 1.9705473184585571
Validation loss: 2.188062416609897

Epoch: 356| Step: 0
Training loss: 2.4142613410949707
Validation loss: 2.19571263815767

Epoch: 6| Step: 1
Training loss: 2.1013665199279785
Validation loss: 2.201366179732866

Epoch: 6| Step: 2
Training loss: 2.281477212905884
Validation loss: 2.18408630740258

Epoch: 6| Step: 3
Training loss: 2.7529892921447754
Validation loss: 2.1906071683411956

Epoch: 6| Step: 4
Training loss: 2.4788691997528076
Validation loss: 2.197672261986681

Epoch: 6| Step: 5
Training loss: 2.428328514099121
Validation loss: 2.182561702625726

Epoch: 6| Step: 6
Training loss: 1.3968086242675781
Validation loss: 2.1978982545996226

Epoch: 6| Step: 7
Training loss: 2.6276845932006836
Validation loss: 2.208138495363215

Epoch: 6| Step: 8
Training loss: 3.208735942840576
Validation loss: 2.2244819723149782

Epoch: 6| Step: 9
Training loss: 2.457310676574707
Validation loss: 2.2317911501853698

Epoch: 6| Step: 10
Training loss: 1.5327045917510986
Validation loss: 2.2165186559000323

Epoch: 6| Step: 11
Training loss: 2.493440628051758
Validation loss: 2.1995054803868777

Epoch: 6| Step: 12
Training loss: 1.8783767223358154
Validation loss: 2.199899024860833

Epoch: 6| Step: 13
Training loss: 2.594787120819092
Validation loss: 2.1858778897152153

Epoch: 357| Step: 0
Training loss: 2.323514461517334
Validation loss: 2.1767547194675734

Epoch: 6| Step: 1
Training loss: 2.468925952911377
Validation loss: 2.1746174212424987

Epoch: 6| Step: 2
Training loss: 1.968618631362915
Validation loss: 2.165041367212931

Epoch: 6| Step: 3
Training loss: 2.576840400695801
Validation loss: 2.164070096067203

Epoch: 6| Step: 4
Training loss: 2.6494531631469727
Validation loss: 2.173891414878189

Epoch: 6| Step: 5
Training loss: 2.4751155376434326
Validation loss: 2.1827975883278796

Epoch: 6| Step: 6
Training loss: 2.4195244312286377
Validation loss: 2.1825792302367506

Epoch: 6| Step: 7
Training loss: 1.9699251651763916
Validation loss: 2.181341630156322

Epoch: 6| Step: 8
Training loss: 2.311288356781006
Validation loss: 2.191085051464778

Epoch: 6| Step: 9
Training loss: 1.9675219058990479
Validation loss: 2.2006964452805056

Epoch: 6| Step: 10
Training loss: 2.5071561336517334
Validation loss: 2.2090449487009356

Epoch: 6| Step: 11
Training loss: 2.425213098526001
Validation loss: 2.2094694837447135

Epoch: 6| Step: 12
Training loss: 2.872210741043091
Validation loss: 2.229568022553639

Epoch: 6| Step: 13
Training loss: 1.1173065900802612
Validation loss: 2.218365661559566

Epoch: 358| Step: 0
Training loss: 2.3391613960266113
Validation loss: 2.2074560606351463

Epoch: 6| Step: 1
Training loss: 2.7152957916259766
Validation loss: 2.2221903954782793

Epoch: 6| Step: 2
Training loss: 2.8288464546203613
Validation loss: 2.2108246331573813

Epoch: 6| Step: 3
Training loss: 2.67533540725708
Validation loss: 2.2171870034228087

Epoch: 6| Step: 4
Training loss: 2.3366355895996094
Validation loss: 2.2062308916481594

Epoch: 6| Step: 5
Training loss: 2.2116756439208984
Validation loss: 2.2134599775396366

Epoch: 6| Step: 6
Training loss: 1.9573999643325806
Validation loss: 2.20366225447706

Epoch: 6| Step: 7
Training loss: 2.6215524673461914
Validation loss: 2.2066466885228313

Epoch: 6| Step: 8
Training loss: 2.427384614944458
Validation loss: 2.222251469089139

Epoch: 6| Step: 9
Training loss: 2.1972615718841553
Validation loss: 2.227850878110496

Epoch: 6| Step: 10
Training loss: 1.9295090436935425
Validation loss: 2.231287202527446

Epoch: 6| Step: 11
Training loss: 1.502899408340454
Validation loss: 2.2254925312534457

Epoch: 6| Step: 12
Training loss: 2.199164390563965
Validation loss: 2.215354225968802

Epoch: 6| Step: 13
Training loss: 2.463226318359375
Validation loss: 2.2053923965782247

Epoch: 359| Step: 0
Training loss: 1.3953250646591187
Validation loss: 2.199255963807465

Epoch: 6| Step: 1
Training loss: 2.2272799015045166
Validation loss: 2.1918946914775397

Epoch: 6| Step: 2
Training loss: 2.575077533721924
Validation loss: 2.181879675516518

Epoch: 6| Step: 3
Training loss: 2.814565658569336
Validation loss: 2.180968669153029

Epoch: 6| Step: 4
Training loss: 1.708372950553894
Validation loss: 2.1905589436972015

Epoch: 6| Step: 5
Training loss: 2.610499382019043
Validation loss: 2.1869411212141796

Epoch: 6| Step: 6
Training loss: 2.5372796058654785
Validation loss: 2.1849728873980943

Epoch: 6| Step: 7
Training loss: 3.221249580383301
Validation loss: 2.1826807529695573

Epoch: 6| Step: 8
Training loss: 2.2688722610473633
Validation loss: 2.1904081836823495

Epoch: 6| Step: 9
Training loss: 1.9386260509490967
Validation loss: 2.1889941974352767

Epoch: 6| Step: 10
Training loss: 2.2539474964141846
Validation loss: 2.1840684670273975

Epoch: 6| Step: 11
Training loss: 1.7414146661758423
Validation loss: 2.180031056045204

Epoch: 6| Step: 12
Training loss: 2.3959240913391113
Validation loss: 2.1891445139402985

Epoch: 6| Step: 13
Training loss: 2.7546911239624023
Validation loss: 2.189641098822317

Epoch: 360| Step: 0
Training loss: 2.3778319358825684
Validation loss: 2.187769338648806

Epoch: 6| Step: 1
Training loss: 2.65797758102417
Validation loss: 2.180411256769652

Epoch: 6| Step: 2
Training loss: 2.463921070098877
Validation loss: 2.196472265387094

Epoch: 6| Step: 3
Training loss: 2.184508800506592
Validation loss: 2.1789131382460236

Epoch: 6| Step: 4
Training loss: 2.3566579818725586
Validation loss: 2.183971707538892

Epoch: 6| Step: 5
Training loss: 2.4368486404418945
Validation loss: 2.174775113341629

Epoch: 6| Step: 6
Training loss: 2.1807074546813965
Validation loss: 2.172809282938639

Epoch: 6| Step: 7
Training loss: 2.168053150177002
Validation loss: 2.1788784944882957

Epoch: 6| Step: 8
Training loss: 2.6924057006835938
Validation loss: 2.178122792192685

Epoch: 6| Step: 9
Training loss: 2.3552122116088867
Validation loss: 2.1725572488641225

Epoch: 6| Step: 10
Training loss: 2.312074661254883
Validation loss: 2.172547017374346

Epoch: 6| Step: 11
Training loss: 1.925169825553894
Validation loss: 2.176675296598865

Epoch: 6| Step: 12
Training loss: 2.2613446712493896
Validation loss: 2.1825884901067263

Epoch: 6| Step: 13
Training loss: 1.537374496459961
Validation loss: 2.193053158380652

Epoch: 361| Step: 0
Training loss: 2.65316104888916
Validation loss: 2.1957710250731437

Epoch: 6| Step: 1
Training loss: 2.9510130882263184
Validation loss: 2.196730698308637

Epoch: 6| Step: 2
Training loss: 2.4308276176452637
Validation loss: 2.20611479461834

Epoch: 6| Step: 3
Training loss: 2.0639078617095947
Validation loss: 2.1722297924821095

Epoch: 6| Step: 4
Training loss: 2.395949363708496
Validation loss: 2.201077361260691

Epoch: 6| Step: 5
Training loss: 2.082810401916504
Validation loss: 2.200481025121545

Epoch: 6| Step: 6
Training loss: 1.500333547592163
Validation loss: 2.204912316414618

Epoch: 6| Step: 7
Training loss: 2.6136560440063477
Validation loss: 2.1964142399449504

Epoch: 6| Step: 8
Training loss: 2.2549657821655273
Validation loss: 2.1898058870787263

Epoch: 6| Step: 9
Training loss: 2.1660091876983643
Validation loss: 2.1899764845448155

Epoch: 6| Step: 10
Training loss: 2.351973056793213
Validation loss: 2.181679200100642

Epoch: 6| Step: 11
Training loss: 1.5690909624099731
Validation loss: 2.1818930936116043

Epoch: 6| Step: 12
Training loss: 2.9955906867980957
Validation loss: 2.18368366969529

Epoch: 6| Step: 13
Training loss: 1.8706932067871094
Validation loss: 2.179274561584637

Epoch: 362| Step: 0
Training loss: 2.1703786849975586
Validation loss: 2.183248540406586

Epoch: 6| Step: 1
Training loss: 1.5652518272399902
Validation loss: 2.1811282416825652

Epoch: 6| Step: 2
Training loss: 2.5660953521728516
Validation loss: 2.199311033371956

Epoch: 6| Step: 3
Training loss: 2.644418239593506
Validation loss: 2.19783773473514

Epoch: 6| Step: 4
Training loss: 2.416501045227051
Validation loss: 2.209651918821437

Epoch: 6| Step: 5
Training loss: 2.3968160152435303
Validation loss: 2.2029239977559736

Epoch: 6| Step: 6
Training loss: 2.35953950881958
Validation loss: 2.2096713358356106

Epoch: 6| Step: 7
Training loss: 1.6705586910247803
Validation loss: 2.221172709618845

Epoch: 6| Step: 8
Training loss: 2.0156610012054443
Validation loss: 2.226295858301142

Epoch: 6| Step: 9
Training loss: 2.7371666431427
Validation loss: 2.228903091082009

Epoch: 6| Step: 10
Training loss: 3.1133737564086914
Validation loss: 2.216253431894446

Epoch: 6| Step: 11
Training loss: 2.201449155807495
Validation loss: 2.224078345042403

Epoch: 6| Step: 12
Training loss: 2.3783164024353027
Validation loss: 2.2135710639338337

Epoch: 6| Step: 13
Training loss: 1.822084665298462
Validation loss: 2.20947927300648

Epoch: 363| Step: 0
Training loss: 2.136021614074707
Validation loss: 2.2037311651373424

Epoch: 6| Step: 1
Training loss: 1.7367058992385864
Validation loss: 2.1766001896191667

Epoch: 6| Step: 2
Training loss: 2.7040886878967285
Validation loss: 2.1591103051298406

Epoch: 6| Step: 3
Training loss: 2.3395471572875977
Validation loss: 2.1631186072544386

Epoch: 6| Step: 4
Training loss: 2.4259254932403564
Validation loss: 2.1466367629266556

Epoch: 6| Step: 5
Training loss: 2.465646743774414
Validation loss: 2.157078722471832

Epoch: 6| Step: 6
Training loss: 2.6462113857269287
Validation loss: 2.1551641571906304

Epoch: 6| Step: 7
Training loss: 1.5938525199890137
Validation loss: 2.155795224251286

Epoch: 6| Step: 8
Training loss: 2.5463061332702637
Validation loss: 2.1685830162417505

Epoch: 6| Step: 9
Training loss: 1.237440824508667
Validation loss: 2.1864359045541413

Epoch: 6| Step: 10
Training loss: 2.5433669090270996
Validation loss: 2.205671338624852

Epoch: 6| Step: 11
Training loss: 2.761843681335449
Validation loss: 2.2138327424244215

Epoch: 6| Step: 12
Training loss: 2.826798915863037
Validation loss: 2.2207091444282123

Epoch: 6| Step: 13
Training loss: 2.4601755142211914
Validation loss: 2.2084543346076884

Epoch: 364| Step: 0
Training loss: 2.545450210571289
Validation loss: 2.1946315073197886

Epoch: 6| Step: 1
Training loss: 1.9112173318862915
Validation loss: 2.1947647756145847

Epoch: 6| Step: 2
Training loss: 2.8350353240966797
Validation loss: 2.1756809578146985

Epoch: 6| Step: 3
Training loss: 2.0490317344665527
Validation loss: 2.1831689585921583

Epoch: 6| Step: 4
Training loss: 2.764633893966675
Validation loss: 2.1825443877968738

Epoch: 6| Step: 5
Training loss: 3.2980544567108154
Validation loss: 2.17266648046432

Epoch: 6| Step: 6
Training loss: 2.277794361114502
Validation loss: 2.181614396392658

Epoch: 6| Step: 7
Training loss: 1.9108830690383911
Validation loss: 2.1666693200347242

Epoch: 6| Step: 8
Training loss: 2.06257963180542
Validation loss: 2.1843613757882068

Epoch: 6| Step: 9
Training loss: 1.989290475845337
Validation loss: 2.167752896585772

Epoch: 6| Step: 10
Training loss: 2.2599353790283203
Validation loss: 2.173477377942813

Epoch: 6| Step: 11
Training loss: 1.9941911697387695
Validation loss: 2.1894773590949272

Epoch: 6| Step: 12
Training loss: 2.2373952865600586
Validation loss: 2.1826096375783286

Epoch: 6| Step: 13
Training loss: 1.552965760231018
Validation loss: 2.1818358718707995

Epoch: 365| Step: 0
Training loss: 2.800908088684082
Validation loss: 2.1911137950035835

Epoch: 6| Step: 1
Training loss: 2.2269229888916016
Validation loss: 2.1716442620882423

Epoch: 6| Step: 2
Training loss: 1.5587652921676636
Validation loss: 2.1748074331591205

Epoch: 6| Step: 3
Training loss: 2.131582736968994
Validation loss: 2.1713239556999615

Epoch: 6| Step: 4
Training loss: 2.21737003326416
Validation loss: 2.1499835034852386

Epoch: 6| Step: 5
Training loss: 2.63409423828125
Validation loss: 2.1652719538698912

Epoch: 6| Step: 6
Training loss: 1.8026130199432373
Validation loss: 2.1515580287543674

Epoch: 6| Step: 7
Training loss: 2.3718628883361816
Validation loss: 2.156793894306306

Epoch: 6| Step: 8
Training loss: 1.9543312788009644
Validation loss: 2.1674332926350255

Epoch: 6| Step: 9
Training loss: 2.0333309173583984
Validation loss: 2.1951375712630568

Epoch: 6| Step: 10
Training loss: 2.6314573287963867
Validation loss: 2.2045194564327115

Epoch: 6| Step: 11
Training loss: 2.5104005336761475
Validation loss: 2.2322880529588267

Epoch: 6| Step: 12
Training loss: 3.041494131088257
Validation loss: 2.2442181571837394

Epoch: 6| Step: 13
Training loss: 2.097773551940918
Validation loss: 2.239960124415736

Epoch: 366| Step: 0
Training loss: 2.524336814880371
Validation loss: 2.223835527255971

Epoch: 6| Step: 1
Training loss: 2.808769702911377
Validation loss: 2.2052518372894614

Epoch: 6| Step: 2
Training loss: 1.7756682634353638
Validation loss: 2.182443477774179

Epoch: 6| Step: 3
Training loss: 2.2606217861175537
Validation loss: 2.158105158036755

Epoch: 6| Step: 4
Training loss: 2.5427346229553223
Validation loss: 2.166973747232909

Epoch: 6| Step: 5
Training loss: 2.3718199729919434
Validation loss: 2.171923928363349

Epoch: 6| Step: 6
Training loss: 3.0116541385650635
Validation loss: 2.1546418769385225

Epoch: 6| Step: 7
Training loss: 2.4546680450439453
Validation loss: 2.1622408359281478

Epoch: 6| Step: 8
Training loss: 2.5114479064941406
Validation loss: 2.1804400490176294

Epoch: 6| Step: 9
Training loss: 1.7300820350646973
Validation loss: 2.154098523560391

Epoch: 6| Step: 10
Training loss: 2.188593864440918
Validation loss: 2.1723800013142247

Epoch: 6| Step: 11
Training loss: 2.1708292961120605
Validation loss: 2.176463473227716

Epoch: 6| Step: 12
Training loss: 2.0617923736572266
Validation loss: 2.180527506336089

Epoch: 6| Step: 13
Training loss: 1.9546712636947632
Validation loss: 2.202412388658011

Epoch: 367| Step: 0
Training loss: 1.7837600708007812
Validation loss: 2.197363043344149

Epoch: 6| Step: 1
Training loss: 2.4667513370513916
Validation loss: 2.252005902669763

Epoch: 6| Step: 2
Training loss: 3.074206829071045
Validation loss: 2.273035531402916

Epoch: 6| Step: 3
Training loss: 1.6987963914871216
Validation loss: 2.2726239927353395

Epoch: 6| Step: 4
Training loss: 2.107712745666504
Validation loss: 2.253950677892213

Epoch: 6| Step: 5
Training loss: 2.6672425270080566
Validation loss: 2.242501594687021

Epoch: 6| Step: 6
Training loss: 2.1786253452301025
Validation loss: 2.2212189935868785

Epoch: 6| Step: 7
Training loss: 1.7098183631896973
Validation loss: 2.2213956412448677

Epoch: 6| Step: 8
Training loss: 2.5810203552246094
Validation loss: 2.207152863984467

Epoch: 6| Step: 9
Training loss: 1.7246583700180054
Validation loss: 2.203674218987906

Epoch: 6| Step: 10
Training loss: 2.548959732055664
Validation loss: 2.2026742427579817

Epoch: 6| Step: 11
Training loss: 2.2642617225646973
Validation loss: 2.193937349063094

Epoch: 6| Step: 12
Training loss: 2.6022982597351074
Validation loss: 2.1813123482529835

Epoch: 6| Step: 13
Training loss: 3.3013057708740234
Validation loss: 2.1718786249878588

Epoch: 368| Step: 0
Training loss: 2.1213560104370117
Validation loss: 2.1778169050011584

Epoch: 6| Step: 1
Training loss: 2.2593626976013184
Validation loss: 2.182872349216092

Epoch: 6| Step: 2
Training loss: 2.032801389694214
Validation loss: 2.1788769281038673

Epoch: 6| Step: 3
Training loss: 2.2054669857025146
Validation loss: 2.1803960659170665

Epoch: 6| Step: 4
Training loss: 1.815367579460144
Validation loss: 2.1913329491051297

Epoch: 6| Step: 5
Training loss: 2.2704193592071533
Validation loss: 2.179775458510204

Epoch: 6| Step: 6
Training loss: 1.8225648403167725
Validation loss: 2.1664049343396257

Epoch: 6| Step: 7
Training loss: 2.175515651702881
Validation loss: 2.1825721045976043

Epoch: 6| Step: 8
Training loss: 2.6831390857696533
Validation loss: 2.174151546211653

Epoch: 6| Step: 9
Training loss: 1.9462552070617676
Validation loss: 2.1644750205419396

Epoch: 6| Step: 10
Training loss: 2.31671404838562
Validation loss: 2.176747665610365

Epoch: 6| Step: 11
Training loss: 2.50779390335083
Validation loss: 2.1660316208357453

Epoch: 6| Step: 12
Training loss: 3.1817665100097656
Validation loss: 2.1712667031954695

Epoch: 6| Step: 13
Training loss: 2.845289468765259
Validation loss: 2.173533175581245

Epoch: 369| Step: 0
Training loss: 2.826596260070801
Validation loss: 2.162032460653654

Epoch: 6| Step: 1
Training loss: 1.7050350904464722
Validation loss: 2.16795926965693

Epoch: 6| Step: 2
Training loss: 1.8018378019332886
Validation loss: 2.156104605684998

Epoch: 6| Step: 3
Training loss: 2.01885986328125
Validation loss: 2.1636673993961786

Epoch: 6| Step: 4
Training loss: 2.171846866607666
Validation loss: 2.1618845026980162

Epoch: 6| Step: 5
Training loss: 2.371541976928711
Validation loss: 2.1621791829345045

Epoch: 6| Step: 6
Training loss: 2.736017942428589
Validation loss: 2.1566127166953137

Epoch: 6| Step: 7
Training loss: 2.4006619453430176
Validation loss: 2.163385416871758

Epoch: 6| Step: 8
Training loss: 2.1636526584625244
Validation loss: 2.1693307276695006

Epoch: 6| Step: 9
Training loss: 2.8058953285217285
Validation loss: 2.177171743044289

Epoch: 6| Step: 10
Training loss: 2.1540956497192383
Validation loss: 2.186008014986592

Epoch: 6| Step: 11
Training loss: 1.9457786083221436
Validation loss: 2.17255691559084

Epoch: 6| Step: 12
Training loss: 2.733921766281128
Validation loss: 2.1800802394907963

Epoch: 6| Step: 13
Training loss: 2.0849556922912598
Validation loss: 2.1561625888270717

Epoch: 370| Step: 0
Training loss: 1.9889757633209229
Validation loss: 2.1675947430313274

Epoch: 6| Step: 1
Training loss: 1.785445213317871
Validation loss: 2.175762540550642

Epoch: 6| Step: 2
Training loss: 2.770254611968994
Validation loss: 2.1572372426268873

Epoch: 6| Step: 3
Training loss: 2.2853024005889893
Validation loss: 2.1659332872718893

Epoch: 6| Step: 4
Training loss: 2.1950862407684326
Validation loss: 2.1732822592540453

Epoch: 6| Step: 5
Training loss: 1.6411060094833374
Validation loss: 2.1785322594386276

Epoch: 6| Step: 6
Training loss: 2.44970703125
Validation loss: 2.1666592603088706

Epoch: 6| Step: 7
Training loss: 1.7809808254241943
Validation loss: 2.1531312106758036

Epoch: 6| Step: 8
Training loss: 3.408562183380127
Validation loss: 2.1543306945472636

Epoch: 6| Step: 9
Training loss: 1.6949732303619385
Validation loss: 2.1674304726303264

Epoch: 6| Step: 10
Training loss: 2.4298532009124756
Validation loss: 2.1437657187061925

Epoch: 6| Step: 11
Training loss: 2.372525691986084
Validation loss: 2.1445717939766507

Epoch: 6| Step: 12
Training loss: 3.024190902709961
Validation loss: 2.1533281803131104

Epoch: 6| Step: 13
Training loss: 2.0555126667022705
Validation loss: 2.147466508291101

Epoch: 371| Step: 0
Training loss: 1.6228501796722412
Validation loss: 2.157302492408342

Epoch: 6| Step: 1
Training loss: 2.350931406021118
Validation loss: 2.1659599555436

Epoch: 6| Step: 2
Training loss: 2.223390817642212
Validation loss: 2.170006016249298

Epoch: 6| Step: 3
Training loss: 1.3153661489486694
Validation loss: 2.187673312361522

Epoch: 6| Step: 4
Training loss: 2.6827430725097656
Validation loss: 2.2050076402643675

Epoch: 6| Step: 5
Training loss: 2.421369791030884
Validation loss: 2.2286538206120974

Epoch: 6| Step: 6
Training loss: 1.5008349418640137
Validation loss: 2.229847920838223

Epoch: 6| Step: 7
Training loss: 2.657315969467163
Validation loss: 2.2281428639606764

Epoch: 6| Step: 8
Training loss: 2.2764830589294434
Validation loss: 2.212220391919536

Epoch: 6| Step: 9
Training loss: 3.0684289932250977
Validation loss: 2.2008969604328112

Epoch: 6| Step: 10
Training loss: 2.6472864151000977
Validation loss: 2.2186637796381468

Epoch: 6| Step: 11
Training loss: 1.923637866973877
Validation loss: 2.206194059823149

Epoch: 6| Step: 12
Training loss: 2.899613857269287
Validation loss: 2.179225310202568

Epoch: 6| Step: 13
Training loss: 2.4114482402801514
Validation loss: 2.1697937711592643

Epoch: 372| Step: 0
Training loss: 2.882965087890625
Validation loss: 2.1695282689986692

Epoch: 6| Step: 1
Training loss: 2.793361186981201
Validation loss: 2.1616850553020353

Epoch: 6| Step: 2
Training loss: 2.3169596195220947
Validation loss: 2.1409389895777546

Epoch: 6| Step: 3
Training loss: 2.250964403152466
Validation loss: 2.1536565672966743

Epoch: 6| Step: 4
Training loss: 2.068392276763916
Validation loss: 2.146189994709466

Epoch: 6| Step: 5
Training loss: 2.244126081466675
Validation loss: 2.14435230019272

Epoch: 6| Step: 6
Training loss: 2.7196576595306396
Validation loss: 2.162947631651355

Epoch: 6| Step: 7
Training loss: 2.4894747734069824
Validation loss: 2.1563572601605485

Epoch: 6| Step: 8
Training loss: 1.731661081314087
Validation loss: 2.161332159913996

Epoch: 6| Step: 9
Training loss: 1.5841602087020874
Validation loss: 2.157869028788741

Epoch: 6| Step: 10
Training loss: 2.0990986824035645
Validation loss: 2.1664010401695006

Epoch: 6| Step: 11
Training loss: 2.611273765563965
Validation loss: 2.1798518729466263

Epoch: 6| Step: 12
Training loss: 1.8398783206939697
Validation loss: 2.200859844043691

Epoch: 6| Step: 13
Training loss: 2.3627240657806396
Validation loss: 2.2191261642722675

Epoch: 373| Step: 0
Training loss: 2.6025686264038086
Validation loss: 2.2108118610997356

Epoch: 6| Step: 1
Training loss: 2.0909383296966553
Validation loss: 2.1939541114273893

Epoch: 6| Step: 2
Training loss: 1.9620734453201294
Validation loss: 2.1655506805707048

Epoch: 6| Step: 3
Training loss: 2.353166103363037
Validation loss: 2.1776074978613083

Epoch: 6| Step: 4
Training loss: 1.8963356018066406
Validation loss: 2.1670190672720633

Epoch: 6| Step: 5
Training loss: 2.345141649246216
Validation loss: 2.1701448014987412

Epoch: 6| Step: 6
Training loss: 2.2628774642944336
Validation loss: 2.1592995325724282

Epoch: 6| Step: 7
Training loss: 2.5266079902648926
Validation loss: 2.1611254676695792

Epoch: 6| Step: 8
Training loss: 2.1837847232818604
Validation loss: 2.1797084526349138

Epoch: 6| Step: 9
Training loss: 2.313206195831299
Validation loss: 2.1897085635892806

Epoch: 6| Step: 10
Training loss: 2.2293741703033447
Validation loss: 2.1813006734335296

Epoch: 6| Step: 11
Training loss: 2.485177993774414
Validation loss: 2.1745656792835524

Epoch: 6| Step: 12
Training loss: 2.2256789207458496
Validation loss: 2.168413495504728

Epoch: 6| Step: 13
Training loss: 2.417591094970703
Validation loss: 2.1766018752128846

Epoch: 374| Step: 0
Training loss: 2.861909866333008
Validation loss: 2.1668009604177167

Epoch: 6| Step: 1
Training loss: 1.770110845565796
Validation loss: 2.1724069297954602

Epoch: 6| Step: 2
Training loss: 2.9082703590393066
Validation loss: 2.1766302329237743

Epoch: 6| Step: 3
Training loss: 2.4495697021484375
Validation loss: 2.1614050967718965

Epoch: 6| Step: 4
Training loss: 1.9179387092590332
Validation loss: 2.167999803379018

Epoch: 6| Step: 5
Training loss: 2.1946351528167725
Validation loss: 2.1669037547162784

Epoch: 6| Step: 6
Training loss: 2.7318553924560547
Validation loss: 2.1743126043709378

Epoch: 6| Step: 7
Training loss: 2.24825119972229
Validation loss: 2.1841042733961538

Epoch: 6| Step: 8
Training loss: 2.43302059173584
Validation loss: 2.1906933425575175

Epoch: 6| Step: 9
Training loss: 2.655229091644287
Validation loss: 2.1929830889548025

Epoch: 6| Step: 10
Training loss: 1.9186937808990479
Validation loss: 2.194842259089152

Epoch: 6| Step: 11
Training loss: 1.546416163444519
Validation loss: 2.1856580959853305

Epoch: 6| Step: 12
Training loss: 1.9457939863204956
Validation loss: 2.190951939552061

Epoch: 6| Step: 13
Training loss: 2.0739450454711914
Validation loss: 2.1858522097269693

Epoch: 375| Step: 0
Training loss: 1.5900019407272339
Validation loss: 2.1800720409680436

Epoch: 6| Step: 1
Training loss: 2.1587817668914795
Validation loss: 2.178710860590781

Epoch: 6| Step: 2
Training loss: 1.8371102809906006
Validation loss: 2.168188351456837

Epoch: 6| Step: 3
Training loss: 2.2803425788879395
Validation loss: 2.176215351268809

Epoch: 6| Step: 4
Training loss: 2.39886474609375
Validation loss: 2.1806720918224705

Epoch: 6| Step: 5
Training loss: 1.8114748001098633
Validation loss: 2.1844880119446786

Epoch: 6| Step: 6
Training loss: 2.6034839153289795
Validation loss: 2.1829751537692164

Epoch: 6| Step: 7
Training loss: 2.8061845302581787
Validation loss: 2.169758278836486

Epoch: 6| Step: 8
Training loss: 2.3178985118865967
Validation loss: 2.1645305438708236

Epoch: 6| Step: 9
Training loss: 2.244567394256592
Validation loss: 2.1584387735653947

Epoch: 6| Step: 10
Training loss: 2.3706247806549072
Validation loss: 2.1593502157477924

Epoch: 6| Step: 11
Training loss: 2.4457759857177734
Validation loss: 2.1652212578763246

Epoch: 6| Step: 12
Training loss: 2.0345730781555176
Validation loss: 2.1656314326870825

Epoch: 6| Step: 13
Training loss: 3.231951951980591
Validation loss: 2.1614756763622327

Epoch: 376| Step: 0
Training loss: 1.9124126434326172
Validation loss: 2.169405612894284

Epoch: 6| Step: 1
Training loss: 2.2980377674102783
Validation loss: 2.151992374850858

Epoch: 6| Step: 2
Training loss: 2.309781074523926
Validation loss: 2.1495819168706096

Epoch: 6| Step: 3
Training loss: 2.972705841064453
Validation loss: 2.1440846509830926

Epoch: 6| Step: 4
Training loss: 2.560163736343384
Validation loss: 2.1548726007502568

Epoch: 6| Step: 5
Training loss: 2.087465763092041
Validation loss: 2.1390182279771373

Epoch: 6| Step: 6
Training loss: 2.980679512023926
Validation loss: 2.137882762057807

Epoch: 6| Step: 7
Training loss: 2.342525005340576
Validation loss: 2.150870669272638

Epoch: 6| Step: 8
Training loss: 1.3915942907333374
Validation loss: 2.1632833019379647

Epoch: 6| Step: 9
Training loss: 1.9048798084259033
Validation loss: 2.1739067082764

Epoch: 6| Step: 10
Training loss: 2.4253623485565186
Validation loss: 2.1670391136600125

Epoch: 6| Step: 11
Training loss: 2.200657367706299
Validation loss: 2.180026879874609

Epoch: 6| Step: 12
Training loss: 2.4700870513916016
Validation loss: 2.177999770769509

Epoch: 6| Step: 13
Training loss: 1.8143267631530762
Validation loss: 2.174010028121292

Epoch: 377| Step: 0
Training loss: 1.7168002128601074
Validation loss: 2.167365227976153

Epoch: 6| Step: 1
Training loss: 2.2622969150543213
Validation loss: 2.164278394432478

Epoch: 6| Step: 2
Training loss: 1.999586820602417
Validation loss: 2.1615003129487396

Epoch: 6| Step: 3
Training loss: 2.2225148677825928
Validation loss: 2.178327073333084

Epoch: 6| Step: 4
Training loss: 2.723686933517456
Validation loss: 2.1722015437259468

Epoch: 6| Step: 5
Training loss: 2.063915252685547
Validation loss: 2.1787006726828952

Epoch: 6| Step: 6
Training loss: 2.5255799293518066
Validation loss: 2.157550242639357

Epoch: 6| Step: 7
Training loss: 3.671754837036133
Validation loss: 2.1608476408066286

Epoch: 6| Step: 8
Training loss: 1.4558027982711792
Validation loss: 2.140057094635502

Epoch: 6| Step: 9
Training loss: 2.847166061401367
Validation loss: 2.1323017510034705

Epoch: 6| Step: 10
Training loss: 1.6013545989990234
Validation loss: 2.1229950420318113

Epoch: 6| Step: 11
Training loss: 2.132077693939209
Validation loss: 2.1215306930644537

Epoch: 6| Step: 12
Training loss: 2.3925743103027344
Validation loss: 2.1281840365420104

Epoch: 6| Step: 13
Training loss: 2.3531994819641113
Validation loss: 2.1458689089744323

Epoch: 378| Step: 0
Training loss: 1.8156236410140991
Validation loss: 2.1520294220216813

Epoch: 6| Step: 1
Training loss: 2.424203634262085
Validation loss: 2.161163627460439

Epoch: 6| Step: 2
Training loss: 2.4629921913146973
Validation loss: 2.172350678392636

Epoch: 6| Step: 3
Training loss: 2.317582607269287
Validation loss: 2.1694641805464223

Epoch: 6| Step: 4
Training loss: 2.614751100540161
Validation loss: 2.183607188604211

Epoch: 6| Step: 5
Training loss: 1.9034380912780762
Validation loss: 2.166156432961905

Epoch: 6| Step: 6
Training loss: 2.345172166824341
Validation loss: 2.1691459814707437

Epoch: 6| Step: 7
Training loss: 1.975681185722351
Validation loss: 2.1660539309183755

Epoch: 6| Step: 8
Training loss: 2.688844680786133
Validation loss: 2.1773887270240375

Epoch: 6| Step: 9
Training loss: 2.1745445728302
Validation loss: 2.178474316032984

Epoch: 6| Step: 10
Training loss: 2.6137843132019043
Validation loss: 2.161760166127195

Epoch: 6| Step: 11
Training loss: 2.1056041717529297
Validation loss: 2.16580206091686

Epoch: 6| Step: 12
Training loss: 1.9494662284851074
Validation loss: 2.158490803933913

Epoch: 6| Step: 13
Training loss: 2.1738553047180176
Validation loss: 2.166173255571755

Epoch: 379| Step: 0
Training loss: 1.9591704607009888
Validation loss: 2.1605877337917203

Epoch: 6| Step: 1
Training loss: 2.506408214569092
Validation loss: 2.1591966203463975

Epoch: 6| Step: 2
Training loss: 2.503500461578369
Validation loss: 2.153011965495284

Epoch: 6| Step: 3
Training loss: 2.1745493412017822
Validation loss: 2.1514747745247296

Epoch: 6| Step: 4
Training loss: 2.929628372192383
Validation loss: 2.145743436710809

Epoch: 6| Step: 5
Training loss: 1.9174563884735107
Validation loss: 2.1454560884865383

Epoch: 6| Step: 6
Training loss: 2.058967113494873
Validation loss: 2.161324216473487

Epoch: 6| Step: 7
Training loss: 1.747153878211975
Validation loss: 2.1667440847683976

Epoch: 6| Step: 8
Training loss: 2.1250948905944824
Validation loss: 2.156732092621506

Epoch: 6| Step: 9
Training loss: 2.6901049613952637
Validation loss: 2.1798317996404504

Epoch: 6| Step: 10
Training loss: 2.3504552841186523
Validation loss: 2.1679367365375644

Epoch: 6| Step: 11
Training loss: 2.2041802406311035
Validation loss: 2.1659067958913822

Epoch: 6| Step: 12
Training loss: 2.4914355278015137
Validation loss: 2.1474276537536294

Epoch: 6| Step: 13
Training loss: 1.7861238718032837
Validation loss: 2.137506467039867

Epoch: 380| Step: 0
Training loss: 2.1027936935424805
Validation loss: 2.1623079699854695

Epoch: 6| Step: 1
Training loss: 2.1282267570495605
Validation loss: 2.157420919787499

Epoch: 6| Step: 2
Training loss: 3.659311294555664
Validation loss: 2.177955949178306

Epoch: 6| Step: 3
Training loss: 1.5940881967544556
Validation loss: 2.1856378816789195

Epoch: 6| Step: 4
Training loss: 2.3785791397094727
Validation loss: 2.1894926178839897

Epoch: 6| Step: 5
Training loss: 2.065248966217041
Validation loss: 2.174082249723455

Epoch: 6| Step: 6
Training loss: 2.2894182205200195
Validation loss: 2.1753735632024784

Epoch: 6| Step: 7
Training loss: 2.5759429931640625
Validation loss: 2.189753037627025

Epoch: 6| Step: 8
Training loss: 2.2147326469421387
Validation loss: 2.1690824403557727

Epoch: 6| Step: 9
Training loss: 2.1067540645599365
Validation loss: 2.1491270706217778

Epoch: 6| Step: 10
Training loss: 2.331576347351074
Validation loss: 2.1406286044787337

Epoch: 6| Step: 11
Training loss: 2.3516459465026855
Validation loss: 2.1363330066844983

Epoch: 6| Step: 12
Training loss: 1.741997241973877
Validation loss: 2.150549410491861

Epoch: 6| Step: 13
Training loss: 1.8298454284667969
Validation loss: 2.1538064736191944

Epoch: 381| Step: 0
Training loss: 2.671231269836426
Validation loss: 2.16042681406903

Epoch: 6| Step: 1
Training loss: 2.444411277770996
Validation loss: 2.1656893517381404

Epoch: 6| Step: 2
Training loss: 1.755035638809204
Validation loss: 2.1555131058539114

Epoch: 6| Step: 3
Training loss: 2.755141258239746
Validation loss: 2.166736961692892

Epoch: 6| Step: 4
Training loss: 2.815922260284424
Validation loss: 2.174858800826534

Epoch: 6| Step: 5
Training loss: 2.5131425857543945
Validation loss: 2.1734640752115557

Epoch: 6| Step: 6
Training loss: 2.2775378227233887
Validation loss: 2.1665977201154156

Epoch: 6| Step: 7
Training loss: 2.3192520141601562
Validation loss: 2.1599925384726575

Epoch: 6| Step: 8
Training loss: 2.5195584297180176
Validation loss: 2.1643747257929977

Epoch: 6| Step: 9
Training loss: 2.069727659225464
Validation loss: 2.1504342632908977

Epoch: 6| Step: 10
Training loss: 1.566215991973877
Validation loss: 2.1448465803618073

Epoch: 6| Step: 11
Training loss: 1.6034194231033325
Validation loss: 2.140632934467767

Epoch: 6| Step: 12
Training loss: 2.1784660816192627
Validation loss: 2.1410096230045443

Epoch: 6| Step: 13
Training loss: 2.3581488132476807
Validation loss: 2.1481136609149236

Epoch: 382| Step: 0
Training loss: 1.7111623287200928
Validation loss: 2.16067865843414

Epoch: 6| Step: 1
Training loss: 2.3142967224121094
Validation loss: 2.1624318540737195

Epoch: 6| Step: 2
Training loss: 2.6094517707824707
Validation loss: 2.1859917640686035

Epoch: 6| Step: 3
Training loss: 2.5274152755737305
Validation loss: 2.180727307514478

Epoch: 6| Step: 4
Training loss: 2.6564857959747314
Validation loss: 2.1930869702369935

Epoch: 6| Step: 5
Training loss: 2.0387656688690186
Validation loss: 2.1980381268326954

Epoch: 6| Step: 6
Training loss: 2.3500561714172363
Validation loss: 2.1750556730454966

Epoch: 6| Step: 7
Training loss: 2.7052173614501953
Validation loss: 2.1824943198952624

Epoch: 6| Step: 8
Training loss: 2.292802572250366
Validation loss: 2.1803413244985763

Epoch: 6| Step: 9
Training loss: 2.091312885284424
Validation loss: 2.1910719666429745

Epoch: 6| Step: 10
Training loss: 1.9139829874038696
Validation loss: 2.1913410617459204

Epoch: 6| Step: 11
Training loss: 2.064396381378174
Validation loss: 2.182169129771571

Epoch: 6| Step: 12
Training loss: 2.283313274383545
Validation loss: 2.1952846998809488

Epoch: 6| Step: 13
Training loss: 2.1426477432250977
Validation loss: 2.1894847141799105

Epoch: 383| Step: 0
Training loss: 1.8891963958740234
Validation loss: 2.1898139087102746

Epoch: 6| Step: 1
Training loss: 2.263571262359619
Validation loss: 2.197232679654193

Epoch: 6| Step: 2
Training loss: 2.1025402545928955
Validation loss: 2.1690618735487743

Epoch: 6| Step: 3
Training loss: 2.125659465789795
Validation loss: 2.161705360617689

Epoch: 6| Step: 4
Training loss: 1.860018253326416
Validation loss: 2.1565807532238703

Epoch: 6| Step: 5
Training loss: 2.0485997200012207
Validation loss: 2.1444458089849

Epoch: 6| Step: 6
Training loss: 2.256068229675293
Validation loss: 2.144576726421233

Epoch: 6| Step: 7
Training loss: 2.285839557647705
Validation loss: 2.1392913172321935

Epoch: 6| Step: 8
Training loss: 2.586127996444702
Validation loss: 2.146877381109422

Epoch: 6| Step: 9
Training loss: 2.6523873805999756
Validation loss: 2.141859633948213

Epoch: 6| Step: 10
Training loss: 2.7749578952789307
Validation loss: 2.1361152420761766

Epoch: 6| Step: 11
Training loss: 2.1457033157348633
Validation loss: 2.133700491279684

Epoch: 6| Step: 12
Training loss: 2.331921100616455
Validation loss: 2.1288375726310154

Epoch: 6| Step: 13
Training loss: 2.235703468322754
Validation loss: 2.148136966971941

Epoch: 384| Step: 0
Training loss: 2.9910593032836914
Validation loss: 2.1565438496169222

Epoch: 6| Step: 1
Training loss: 1.6776607036590576
Validation loss: 2.1571485586063837

Epoch: 6| Step: 2
Training loss: 2.2126736640930176
Validation loss: 2.1578213989093737

Epoch: 6| Step: 3
Training loss: 2.102877378463745
Validation loss: 2.156874297767557

Epoch: 6| Step: 4
Training loss: 2.202252149581909
Validation loss: 2.154118009792861

Epoch: 6| Step: 5
Training loss: 1.806687593460083
Validation loss: 2.15519203165526

Epoch: 6| Step: 6
Training loss: 2.1309359073638916
Validation loss: 2.151735262204242

Epoch: 6| Step: 7
Training loss: 1.9269100427627563
Validation loss: 2.1528331643791607

Epoch: 6| Step: 8
Training loss: 2.5452489852905273
Validation loss: 2.147899366194202

Epoch: 6| Step: 9
Training loss: 2.5859017372131348
Validation loss: 2.1349185051456576

Epoch: 6| Step: 10
Training loss: 2.649540901184082
Validation loss: 2.144535785080284

Epoch: 6| Step: 11
Training loss: 2.7744202613830566
Validation loss: 2.1496797069426505

Epoch: 6| Step: 12
Training loss: 1.5980405807495117
Validation loss: 2.146492611977362

Epoch: 6| Step: 13
Training loss: 2.277350902557373
Validation loss: 2.137912129843107

Epoch: 385| Step: 0
Training loss: 2.5599024295806885
Validation loss: 2.15497544375799

Epoch: 6| Step: 1
Training loss: 3.2004404067993164
Validation loss: 2.156221592298118

Epoch: 6| Step: 2
Training loss: 2.0694408416748047
Validation loss: 2.1725826186518513

Epoch: 6| Step: 3
Training loss: 1.8428637981414795
Validation loss: 2.1888289118325837

Epoch: 6| Step: 4
Training loss: 2.2638301849365234
Validation loss: 2.1763079602231263

Epoch: 6| Step: 5
Training loss: 2.443978786468506
Validation loss: 2.175902525583903

Epoch: 6| Step: 6
Training loss: 2.285609722137451
Validation loss: 2.181328532516315

Epoch: 6| Step: 7
Training loss: 2.0080742835998535
Validation loss: 2.171752534886842

Epoch: 6| Step: 8
Training loss: 1.7026894092559814
Validation loss: 2.1573640274745163

Epoch: 6| Step: 9
Training loss: 2.155031442642212
Validation loss: 2.1623496855458906

Epoch: 6| Step: 10
Training loss: 2.1158828735351562
Validation loss: 2.1648199801803916

Epoch: 6| Step: 11
Training loss: 2.647479295730591
Validation loss: 2.1593234833850654

Epoch: 6| Step: 12
Training loss: 1.6408480405807495
Validation loss: 2.1614993797835482

Epoch: 6| Step: 13
Training loss: 2.549614906311035
Validation loss: 2.165049679817692

Epoch: 386| Step: 0
Training loss: 3.044123649597168
Validation loss: 2.156316629020117

Epoch: 6| Step: 1
Training loss: 2.4158287048339844
Validation loss: 2.1602707370635

Epoch: 6| Step: 2
Training loss: 2.4172980785369873
Validation loss: 2.1608594386808333

Epoch: 6| Step: 3
Training loss: 2.0107877254486084
Validation loss: 2.14290847316865

Epoch: 6| Step: 4
Training loss: 1.9137158393859863
Validation loss: 2.1540589358216975

Epoch: 6| Step: 5
Training loss: 1.9904282093048096
Validation loss: 2.1513123922450568

Epoch: 6| Step: 6
Training loss: 1.8212409019470215
Validation loss: 2.148120012334598

Epoch: 6| Step: 7
Training loss: 2.5717954635620117
Validation loss: 2.1331526182031118

Epoch: 6| Step: 8
Training loss: 1.9474353790283203
Validation loss: 2.1538105113531953

Epoch: 6| Step: 9
Training loss: 2.0720338821411133
Validation loss: 2.1282192712189048

Epoch: 6| Step: 10
Training loss: 1.8518004417419434
Validation loss: 2.136603856599459

Epoch: 6| Step: 11
Training loss: 2.2883822917938232
Validation loss: 2.148788594430493

Epoch: 6| Step: 12
Training loss: 2.6712558269500732
Validation loss: 2.1421912626553605

Epoch: 6| Step: 13
Training loss: 2.509709596633911
Validation loss: 2.1532478435065157

Epoch: 387| Step: 0
Training loss: 2.3574295043945312
Validation loss: 2.147913540563276

Epoch: 6| Step: 1
Training loss: 1.9538049697875977
Validation loss: 2.140673058007353

Epoch: 6| Step: 2
Training loss: 2.737837076187134
Validation loss: 2.1552934851697696

Epoch: 6| Step: 3
Training loss: 2.579190731048584
Validation loss: 2.1360217858386297

Epoch: 6| Step: 4
Training loss: 2.2694671154022217
Validation loss: 2.1353320716529764

Epoch: 6| Step: 5
Training loss: 0.8108121156692505
Validation loss: 2.1397301945635068

Epoch: 6| Step: 6
Training loss: 2.567538261413574
Validation loss: 2.153286172497657

Epoch: 6| Step: 7
Training loss: 2.7218687534332275
Validation loss: 2.1573487148490003

Epoch: 6| Step: 8
Training loss: 2.14609432220459
Validation loss: 2.16993212187162

Epoch: 6| Step: 9
Training loss: 1.3689171075820923
Validation loss: 2.1671982580615627

Epoch: 6| Step: 10
Training loss: 2.3043630123138428
Validation loss: 2.1782389276771137

Epoch: 6| Step: 11
Training loss: 2.912318229675293
Validation loss: 2.188772573265978

Epoch: 6| Step: 12
Training loss: 2.1637043952941895
Validation loss: 2.1755087734550558

Epoch: 6| Step: 13
Training loss: 2.6871495246887207
Validation loss: 2.1709466595803537

Epoch: 388| Step: 0
Training loss: 2.2024545669555664
Validation loss: 2.161779595959571

Epoch: 6| Step: 1
Training loss: 2.118199586868286
Validation loss: 2.150859148271622

Epoch: 6| Step: 2
Training loss: 1.7591925859451294
Validation loss: 2.1542863871461604

Epoch: 6| Step: 3
Training loss: 1.708785057067871
Validation loss: 2.1480436055890975

Epoch: 6| Step: 4
Training loss: 2.587737798690796
Validation loss: 2.1600750748829176

Epoch: 6| Step: 5
Training loss: 2.000032663345337
Validation loss: 2.1539101087918846

Epoch: 6| Step: 6
Training loss: 2.878380298614502
Validation loss: 2.1779274991763535

Epoch: 6| Step: 7
Training loss: 2.1600687503814697
Validation loss: 2.1603641792010237

Epoch: 6| Step: 8
Training loss: 2.0580103397369385
Validation loss: 2.152764384464551

Epoch: 6| Step: 9
Training loss: 1.8509289026260376
Validation loss: 2.168650996300482

Epoch: 6| Step: 10
Training loss: 2.6712450981140137
Validation loss: 2.172653959643456

Epoch: 6| Step: 11
Training loss: 1.7117652893066406
Validation loss: 2.170478883609977

Epoch: 6| Step: 12
Training loss: 2.3884572982788086
Validation loss: 2.1652563464257026

Epoch: 6| Step: 13
Training loss: 3.882175922393799
Validation loss: 2.1617059143640662

Epoch: 389| Step: 0
Training loss: 2.44490909576416
Validation loss: 2.1616887943719023

Epoch: 6| Step: 1
Training loss: 2.3109889030456543
Validation loss: 2.1478675103956655

Epoch: 6| Step: 2
Training loss: 2.6442952156066895
Validation loss: 2.1388210122303297

Epoch: 6| Step: 3
Training loss: 2.2789852619171143
Validation loss: 2.1219434840704805

Epoch: 6| Step: 4
Training loss: 2.4205141067504883
Validation loss: 2.123802725986768

Epoch: 6| Step: 5
Training loss: 2.462425947189331
Validation loss: 2.1292319887427875

Epoch: 6| Step: 6
Training loss: 1.229461669921875
Validation loss: 2.1435521033502396

Epoch: 6| Step: 7
Training loss: 2.5876715183258057
Validation loss: 2.143229628121981

Epoch: 6| Step: 8
Training loss: 1.9279754161834717
Validation loss: 2.1726523906953874

Epoch: 6| Step: 9
Training loss: 1.5229171514511108
Validation loss: 2.167097371111634

Epoch: 6| Step: 10
Training loss: 2.7578847408294678
Validation loss: 2.1774885064812115

Epoch: 6| Step: 11
Training loss: 2.8084137439727783
Validation loss: 2.194119140666018

Epoch: 6| Step: 12
Training loss: 1.9759753942489624
Validation loss: 2.1853237690464145

Epoch: 6| Step: 13
Training loss: 2.077596664428711
Validation loss: 2.188185618769738

Epoch: 390| Step: 0
Training loss: 2.5882303714752197
Validation loss: 2.196535643710885

Epoch: 6| Step: 1
Training loss: 2.2359042167663574
Validation loss: 2.2002333787179764

Epoch: 6| Step: 2
Training loss: 1.9960283041000366
Validation loss: 2.1773609089595016

Epoch: 6| Step: 3
Training loss: 2.1357946395874023
Validation loss: 2.1735129151293027

Epoch: 6| Step: 4
Training loss: 2.241015911102295
Validation loss: 2.1558207952848045

Epoch: 6| Step: 5
Training loss: 2.9476704597473145
Validation loss: 2.173541094667168

Epoch: 6| Step: 6
Training loss: 1.8482463359832764
Validation loss: 2.160814557024228

Epoch: 6| Step: 7
Training loss: 3.024174451828003
Validation loss: 2.169300115236672

Epoch: 6| Step: 8
Training loss: 1.7377638816833496
Validation loss: 2.162415659555825

Epoch: 6| Step: 9
Training loss: 2.2357029914855957
Validation loss: 2.1769361034516366

Epoch: 6| Step: 10
Training loss: 1.5658965110778809
Validation loss: 2.1839129668410107

Epoch: 6| Step: 11
Training loss: 2.8853585720062256
Validation loss: 2.193010163563554

Epoch: 6| Step: 12
Training loss: 1.465633749961853
Validation loss: 2.1940426570112987

Epoch: 6| Step: 13
Training loss: 2.9075984954833984
Validation loss: 2.1816666997889036

Epoch: 391| Step: 0
Training loss: 1.8611427545547485
Validation loss: 2.1504148590949272

Epoch: 6| Step: 1
Training loss: 2.067335605621338
Validation loss: 2.1354412750531266

Epoch: 6| Step: 2
Training loss: 2.558535575866699
Validation loss: 2.1384592133183635

Epoch: 6| Step: 3
Training loss: 2.0537166595458984
Validation loss: 2.149444512141648

Epoch: 6| Step: 4
Training loss: 2.670337200164795
Validation loss: 2.158077492508837

Epoch: 6| Step: 5
Training loss: 3.029789447784424
Validation loss: 2.147407866293384

Epoch: 6| Step: 6
Training loss: 2.569181442260742
Validation loss: 2.1535876463818293

Epoch: 6| Step: 7
Training loss: 1.9451730251312256
Validation loss: 2.144201429941321

Epoch: 6| Step: 8
Training loss: 2.079667091369629
Validation loss: 2.1395308420222294

Epoch: 6| Step: 9
Training loss: 1.8170959949493408
Validation loss: 2.1281212145282375

Epoch: 6| Step: 10
Training loss: 1.8658080101013184
Validation loss: 2.134946101455278

Epoch: 6| Step: 11
Training loss: 2.398529052734375
Validation loss: 2.1341002628367436

Epoch: 6| Step: 12
Training loss: 2.126997470855713
Validation loss: 2.1352902022741174

Epoch: 6| Step: 13
Training loss: 2.444650650024414
Validation loss: 2.1404183013464815

Epoch: 392| Step: 0
Training loss: 2.3954806327819824
Validation loss: 2.148880684247581

Epoch: 6| Step: 1
Training loss: 1.659006118774414
Validation loss: 2.154614363947222

Epoch: 6| Step: 2
Training loss: 2.9478940963745117
Validation loss: 2.1571269714704124

Epoch: 6| Step: 3
Training loss: 2.387296676635742
Validation loss: 2.1612218015937397

Epoch: 6| Step: 4
Training loss: 2.2071800231933594
Validation loss: 2.1423976498265422

Epoch: 6| Step: 5
Training loss: 1.6655545234680176
Validation loss: 2.1572702982092418

Epoch: 6| Step: 6
Training loss: 2.976655960083008
Validation loss: 2.1577689814311203

Epoch: 6| Step: 7
Training loss: 2.0043210983276367
Validation loss: 2.1549219072505994

Epoch: 6| Step: 8
Training loss: 2.5587291717529297
Validation loss: 2.1437569766916256

Epoch: 6| Step: 9
Training loss: 2.1422295570373535
Validation loss: 2.1701168206430252

Epoch: 6| Step: 10
Training loss: 2.0880212783813477
Validation loss: 2.1514697382527013

Epoch: 6| Step: 11
Training loss: 1.4523491859436035
Validation loss: 2.147983089570076

Epoch: 6| Step: 12
Training loss: 2.5357983112335205
Validation loss: 2.163426955540975

Epoch: 6| Step: 13
Training loss: 2.39200758934021
Validation loss: 2.160199834454444

Epoch: 393| Step: 0
Training loss: 2.400331497192383
Validation loss: 2.176766144332065

Epoch: 6| Step: 1
Training loss: 1.4987893104553223
Validation loss: 2.1706284579410347

Epoch: 6| Step: 2
Training loss: 2.1358261108398438
Validation loss: 2.157006267578371

Epoch: 6| Step: 3
Training loss: 2.425206184387207
Validation loss: 2.161746991577969

Epoch: 6| Step: 4
Training loss: 2.6398701667785645
Validation loss: 2.1595539777509627

Epoch: 6| Step: 5
Training loss: 2.163891315460205
Validation loss: 2.1519797155933995

Epoch: 6| Step: 6
Training loss: 2.337597370147705
Validation loss: 2.1609517015436643

Epoch: 6| Step: 7
Training loss: 2.1178603172302246
Validation loss: 2.1668321189060005

Epoch: 6| Step: 8
Training loss: 2.4268980026245117
Validation loss: 2.1670863038750103

Epoch: 6| Step: 9
Training loss: 2.559635877609253
Validation loss: 2.1706080641797794

Epoch: 6| Step: 10
Training loss: 2.4161453247070312
Validation loss: 2.159859438096323

Epoch: 6| Step: 11
Training loss: 1.842266321182251
Validation loss: 2.1548110772204656

Epoch: 6| Step: 12
Training loss: 1.834258794784546
Validation loss: 2.130399146387654

Epoch: 6| Step: 13
Training loss: 2.543976306915283
Validation loss: 2.145893168705766

Epoch: 394| Step: 0
Training loss: 2.3598270416259766
Validation loss: 2.1335102717081704

Epoch: 6| Step: 1
Training loss: 2.0477447509765625
Validation loss: 2.14322353178455

Epoch: 6| Step: 2
Training loss: 2.1375865936279297
Validation loss: 2.1439903833532847

Epoch: 6| Step: 3
Training loss: 1.890271544456482
Validation loss: 2.1452960891108357

Epoch: 6| Step: 4
Training loss: 2.195791721343994
Validation loss: 2.134665709669872

Epoch: 6| Step: 5
Training loss: 1.8139190673828125
Validation loss: 2.1430604022036315

Epoch: 6| Step: 6
Training loss: 2.717663288116455
Validation loss: 2.151788321874475

Epoch: 6| Step: 7
Training loss: 3.2081360816955566
Validation loss: 2.15571488359923

Epoch: 6| Step: 8
Training loss: 1.8200072050094604
Validation loss: 2.138560600178216

Epoch: 6| Step: 9
Training loss: 2.4479119777679443
Validation loss: 2.14030038028635

Epoch: 6| Step: 10
Training loss: 1.8856391906738281
Validation loss: 2.1299327881105485

Epoch: 6| Step: 11
Training loss: 2.787654399871826
Validation loss: 2.141209283182698

Epoch: 6| Step: 12
Training loss: 1.4179096221923828
Validation loss: 2.1407880052443473

Epoch: 6| Step: 13
Training loss: 2.5316643714904785
Validation loss: 2.133909748446557

Epoch: 395| Step: 0
Training loss: 2.29888653755188
Validation loss: 2.1458210099127983

Epoch: 6| Step: 1
Training loss: 2.246682643890381
Validation loss: 2.165654864362491

Epoch: 6| Step: 2
Training loss: 3.0746359825134277
Validation loss: 2.1567614873250327

Epoch: 6| Step: 3
Training loss: 1.6943650245666504
Validation loss: 2.1467561080891597

Epoch: 6| Step: 4
Training loss: 2.1080193519592285
Validation loss: 2.121004209723524

Epoch: 6| Step: 5
Training loss: 2.832475185394287
Validation loss: 2.128839815816572

Epoch: 6| Step: 6
Training loss: 2.8062243461608887
Validation loss: 2.123245020066538

Epoch: 6| Step: 7
Training loss: 1.9179983139038086
Validation loss: 2.1241893768310547

Epoch: 6| Step: 8
Training loss: 1.7400758266448975
Validation loss: 2.1396558694942023

Epoch: 6| Step: 9
Training loss: 2.005723476409912
Validation loss: 2.1373443654788438

Epoch: 6| Step: 10
Training loss: 1.921209692955017
Validation loss: 2.1509281691684516

Epoch: 6| Step: 11
Training loss: 1.8659108877182007
Validation loss: 2.1514385233643236

Epoch: 6| Step: 12
Training loss: 2.1291208267211914
Validation loss: 2.1433073628333306

Epoch: 6| Step: 13
Training loss: 2.80548095703125
Validation loss: 2.132834865200904

Epoch: 396| Step: 0
Training loss: 2.3538432121276855
Validation loss: 2.1304645051238356

Epoch: 6| Step: 1
Training loss: 1.8738696575164795
Validation loss: 2.124180006724532

Epoch: 6| Step: 2
Training loss: 2.495753526687622
Validation loss: 2.127882142220774

Epoch: 6| Step: 3
Training loss: 2.198796033859253
Validation loss: 2.118203250310754

Epoch: 6| Step: 4
Training loss: 2.070699691772461
Validation loss: 2.12114163496161

Epoch: 6| Step: 5
Training loss: 2.5433666706085205
Validation loss: 2.1276690857384795

Epoch: 6| Step: 6
Training loss: 1.740973711013794
Validation loss: 2.1345117681769916

Epoch: 6| Step: 7
Training loss: 2.4492433071136475
Validation loss: 2.126563790023968

Epoch: 6| Step: 8
Training loss: 2.4061906337738037
Validation loss: 2.13323030676893

Epoch: 6| Step: 9
Training loss: 2.1523168087005615
Validation loss: 2.1427646298562326

Epoch: 6| Step: 10
Training loss: 2.001222610473633
Validation loss: 2.141175998154507

Epoch: 6| Step: 11
Training loss: 1.9016785621643066
Validation loss: 2.1522177239899993

Epoch: 6| Step: 12
Training loss: 2.9377143383026123
Validation loss: 2.1483732923384635

Epoch: 6| Step: 13
Training loss: 1.7019622325897217
Validation loss: 2.14558345015331

Epoch: 397| Step: 0
Training loss: 2.0706212520599365
Validation loss: 2.1460939850858463

Epoch: 6| Step: 1
Training loss: 2.4117660522460938
Validation loss: 2.13230985723516

Epoch: 6| Step: 2
Training loss: 3.049302577972412
Validation loss: 2.133799014552947

Epoch: 6| Step: 3
Training loss: 2.5968470573425293
Validation loss: 2.1252977091778993

Epoch: 6| Step: 4
Training loss: 1.8702590465545654
Validation loss: 2.1221574788452475

Epoch: 6| Step: 5
Training loss: 1.974702000617981
Validation loss: 2.121102789396881

Epoch: 6| Step: 6
Training loss: 2.22876238822937
Validation loss: 2.114923886073533

Epoch: 6| Step: 7
Training loss: 2.435429334640503
Validation loss: 2.1132673063585834

Epoch: 6| Step: 8
Training loss: 2.238034963607788
Validation loss: 2.1222843611112205

Epoch: 6| Step: 9
Training loss: 2.7400078773498535
Validation loss: 2.127823402804713

Epoch: 6| Step: 10
Training loss: 1.5087404251098633
Validation loss: 2.122173524671985

Epoch: 6| Step: 11
Training loss: 1.826764464378357
Validation loss: 2.1270498216793103

Epoch: 6| Step: 12
Training loss: 1.6183948516845703
Validation loss: 2.155221205885692

Epoch: 6| Step: 13
Training loss: 3.0422933101654053
Validation loss: 2.1492933047715055

Epoch: 398| Step: 0
Training loss: 2.210433006286621
Validation loss: 2.144271231466724

Epoch: 6| Step: 1
Training loss: 2.275510787963867
Validation loss: 2.1451373920645764

Epoch: 6| Step: 2
Training loss: 2.1637892723083496
Validation loss: 2.141481708454829

Epoch: 6| Step: 3
Training loss: 2.282726287841797
Validation loss: 2.1418258297827935

Epoch: 6| Step: 4
Training loss: 2.830005645751953
Validation loss: 2.1637487052589335

Epoch: 6| Step: 5
Training loss: 2.338649034500122
Validation loss: 2.16570496302779

Epoch: 6| Step: 6
Training loss: 1.4633533954620361
Validation loss: 2.168876137784732

Epoch: 6| Step: 7
Training loss: 1.938880443572998
Validation loss: 2.1762583896677983

Epoch: 6| Step: 8
Training loss: 2.9487295150756836
Validation loss: 2.163751963646181

Epoch: 6| Step: 9
Training loss: 2.3000237941741943
Validation loss: 2.1705211118985246

Epoch: 6| Step: 10
Training loss: 2.062464714050293
Validation loss: 2.1675819761009625

Epoch: 6| Step: 11
Training loss: 1.9995379447937012
Validation loss: 2.1555540407857587

Epoch: 6| Step: 12
Training loss: 2.277024745941162
Validation loss: 2.148094266973516

Epoch: 6| Step: 13
Training loss: 1.746794581413269
Validation loss: 2.1469873305289977

Epoch: 399| Step: 0
Training loss: 2.6422581672668457
Validation loss: 2.146923783004925

Epoch: 6| Step: 1
Training loss: 2.1439106464385986
Validation loss: 2.134816841412616

Epoch: 6| Step: 2
Training loss: 1.3271747827529907
Validation loss: 2.1352676601820093

Epoch: 6| Step: 3
Training loss: 1.628375768661499
Validation loss: 2.1471901452669533

Epoch: 6| Step: 4
Training loss: 1.7554277181625366
Validation loss: 2.1386026079936693

Epoch: 6| Step: 5
Training loss: 2.0908732414245605
Validation loss: 2.175924675438994

Epoch: 6| Step: 6
Training loss: 2.147127389907837
Validation loss: 2.183048484145954

Epoch: 6| Step: 7
Training loss: 2.8411338329315186
Validation loss: 2.188201509496217

Epoch: 6| Step: 8
Training loss: 2.4239959716796875
Validation loss: 2.184130104639197

Epoch: 6| Step: 9
Training loss: 2.02988338470459
Validation loss: 2.194456015863726

Epoch: 6| Step: 10
Training loss: 3.0829811096191406
Validation loss: 2.191771202189948

Epoch: 6| Step: 11
Training loss: 2.6386313438415527
Validation loss: 2.1696824642919723

Epoch: 6| Step: 12
Training loss: 1.8052377700805664
Validation loss: 2.156918405204691

Epoch: 6| Step: 13
Training loss: 2.6420445442199707
Validation loss: 2.141762295076924

Epoch: 400| Step: 0
Training loss: 2.3902411460876465
Validation loss: 2.128141826198947

Epoch: 6| Step: 1
Training loss: 2.791358709335327
Validation loss: 2.1153389356469594

Epoch: 6| Step: 2
Training loss: 1.1172809600830078
Validation loss: 2.1144981461186565

Epoch: 6| Step: 3
Training loss: 2.1155190467834473
Validation loss: 2.121556289734379

Epoch: 6| Step: 4
Training loss: 2.819287061691284
Validation loss: 2.114072302336334

Epoch: 6| Step: 5
Training loss: 2.4460840225219727
Validation loss: 2.123284073286159

Epoch: 6| Step: 6
Training loss: 2.2820591926574707
Validation loss: 2.12378325000886

Epoch: 6| Step: 7
Training loss: 1.640669584274292
Validation loss: 2.123919512635918

Epoch: 6| Step: 8
Training loss: 2.284196138381958
Validation loss: 2.1223246846147763

Epoch: 6| Step: 9
Training loss: 2.652087688446045
Validation loss: 2.1272348050148255

Epoch: 6| Step: 10
Training loss: 2.5068933963775635
Validation loss: 2.1224997171791653

Epoch: 6| Step: 11
Training loss: 1.979653000831604
Validation loss: 2.1493777741668043

Epoch: 6| Step: 12
Training loss: 1.7611140012741089
Validation loss: 2.143406770562613

Epoch: 6| Step: 13
Training loss: 2.4574368000030518
Validation loss: 2.1693412975598405

Testing loss: 2.2963749117321437

Epoch: 1| Step: 0
Training loss: 4.907161712646484
Validation loss: 5.200120715684788

Epoch: 5| Step: 1
Training loss: 6.456788063049316
Validation loss: 5.1892360359109855

Epoch: 5| Step: 2
Training loss: 5.4851813316345215
Validation loss: 5.177813135167604

Epoch: 5| Step: 3
Training loss: 4.764087677001953
Validation loss: 5.166292693025323

Epoch: 5| Step: 4
Training loss: 4.148741722106934
Validation loss: 5.1546556359978135

Epoch: 5| Step: 5
Training loss: 3.917677402496338
Validation loss: 5.143292729572583

Epoch: 5| Step: 6
Training loss: 4.07047176361084
Validation loss: 5.130893415020358

Epoch: 5| Step: 7
Training loss: 5.40694522857666
Validation loss: 5.117835660134593

Epoch: 5| Step: 8
Training loss: 5.444655895233154
Validation loss: 5.104745731558851

Epoch: 5| Step: 9
Training loss: 5.0493388175964355
Validation loss: 5.0897850016111965

Epoch: 5| Step: 10
Training loss: 4.590778350830078
Validation loss: 5.0744865273916595

Epoch: 2| Step: 0
Training loss: 5.598616600036621
Validation loss: 5.057234789735528

Epoch: 5| Step: 1
Training loss: 4.880791187286377
Validation loss: 5.0386371253639135

Epoch: 5| Step: 2
Training loss: 4.012166976928711
Validation loss: 5.019994463971866

Epoch: 5| Step: 3
Training loss: 4.470573902130127
Validation loss: 4.998214301242624

Epoch: 5| Step: 4
Training loss: 5.249853134155273
Validation loss: 4.9753808359945975

Epoch: 5| Step: 5
Training loss: 4.371183395385742
Validation loss: 4.9516032024096415

Epoch: 5| Step: 6
Training loss: 5.219838619232178
Validation loss: 4.926129689780614

Epoch: 5| Step: 7
Training loss: 5.710755825042725
Validation loss: 4.899361856522098

Epoch: 5| Step: 8
Training loss: 3.8962905406951904
Validation loss: 4.869853496551514

Epoch: 5| Step: 9
Training loss: 4.464805603027344
Validation loss: 4.840022302442981

Epoch: 5| Step: 10
Training loss: 4.118371963500977
Validation loss: 4.808414741228986

Epoch: 3| Step: 0
Training loss: 4.615959167480469
Validation loss: 4.774210088996477

Epoch: 5| Step: 1
Training loss: 4.321972846984863
Validation loss: 4.737317164738973

Epoch: 5| Step: 2
Training loss: 4.783908843994141
Validation loss: 4.701681470358244

Epoch: 5| Step: 3
Training loss: 4.136445999145508
Validation loss: 4.663419426128429

Epoch: 5| Step: 4
Training loss: 3.8312363624572754
Validation loss: 4.624381526823966

Epoch: 5| Step: 5
Training loss: 4.021583557128906
Validation loss: 4.583970808213757

Epoch: 5| Step: 6
Training loss: 4.124902725219727
Validation loss: 4.540572571498092

Epoch: 5| Step: 7
Training loss: 4.4414777755737305
Validation loss: 4.497185778874223

Epoch: 5| Step: 8
Training loss: 4.777663230895996
Validation loss: 4.4534814434666785

Epoch: 5| Step: 9
Training loss: 4.178576469421387
Validation loss: 4.408719152532598

Epoch: 5| Step: 10
Training loss: 4.884986400604248
Validation loss: 4.367582203239523

Epoch: 4| Step: 0
Training loss: 4.568028926849365
Validation loss: 4.325138594514581

Epoch: 5| Step: 1
Training loss: 3.7432703971862793
Validation loss: 4.287218037471976

Epoch: 5| Step: 2
Training loss: 3.8433709144592285
Validation loss: 4.247240579256448

Epoch: 5| Step: 3
Training loss: 4.419434547424316
Validation loss: 4.213390934851862

Epoch: 5| Step: 4
Training loss: 4.223805904388428
Validation loss: 4.177426440741426

Epoch: 5| Step: 5
Training loss: 3.4515891075134277
Validation loss: 4.146591232668969

Epoch: 5| Step: 6
Training loss: 4.2421698570251465
Validation loss: 4.11445072133054

Epoch: 5| Step: 7
Training loss: 4.432099342346191
Validation loss: 4.0833907229926

Epoch: 5| Step: 8
Training loss: 3.1078007221221924
Validation loss: 4.056139945983887

Epoch: 5| Step: 9
Training loss: 3.9397144317626953
Validation loss: 4.025709941822996

Epoch: 5| Step: 10
Training loss: 3.777702808380127
Validation loss: 3.998788772090789

Epoch: 5| Step: 0
Training loss: 3.3175411224365234
Validation loss: 3.97375823092717

Epoch: 5| Step: 1
Training loss: 2.774831533432007
Validation loss: 3.9482614327502508

Epoch: 5| Step: 2
Training loss: 3.798283815383911
Validation loss: 3.923795179654193

Epoch: 5| Step: 3
Training loss: 5.253303527832031
Validation loss: 3.9000610818145094

Epoch: 5| Step: 4
Training loss: 3.918564558029175
Validation loss: 3.8762677382397395

Epoch: 5| Step: 5
Training loss: 3.5842926502227783
Validation loss: 3.8536809259845364

Epoch: 5| Step: 6
Training loss: 4.210633277893066
Validation loss: 3.8328392351827314

Epoch: 5| Step: 7
Training loss: 3.1819586753845215
Validation loss: 3.811009665971161

Epoch: 5| Step: 8
Training loss: 3.0990309715270996
Validation loss: 3.789159785034836

Epoch: 5| Step: 9
Training loss: 4.022732734680176
Validation loss: 3.7685330144820677

Epoch: 5| Step: 10
Training loss: 4.0116119384765625
Validation loss: 3.7516317982827463

Epoch: 6| Step: 0
Training loss: 3.9549167156219482
Validation loss: 3.7344842726184475

Epoch: 5| Step: 1
Training loss: 3.856975555419922
Validation loss: 3.7179792875884683

Epoch: 5| Step: 2
Training loss: 3.5598769187927246
Validation loss: 3.70102927248965

Epoch: 5| Step: 3
Training loss: 3.153996229171753
Validation loss: 3.684339177223944

Epoch: 5| Step: 4
Training loss: 4.20687198638916
Validation loss: 3.6707379817962646

Epoch: 5| Step: 5
Training loss: 4.222901344299316
Validation loss: 3.654945486335344

Epoch: 5| Step: 6
Training loss: 2.955334424972534
Validation loss: 3.6409383435403146

Epoch: 5| Step: 7
Training loss: 3.801098585128784
Validation loss: 3.627017769762265

Epoch: 5| Step: 8
Training loss: 2.138213634490967
Validation loss: 3.613432691943261

Epoch: 5| Step: 9
Training loss: 3.5612881183624268
Validation loss: 3.59939565197114

Epoch: 5| Step: 10
Training loss: 3.917304515838623
Validation loss: 3.5889451273026003

Epoch: 7| Step: 0
Training loss: 3.7525794506073
Validation loss: 3.580284518580283

Epoch: 5| Step: 1
Training loss: 3.687479019165039
Validation loss: 3.5730591358677035

Epoch: 5| Step: 2
Training loss: 3.2055602073669434
Validation loss: 3.5615254115032893

Epoch: 5| Step: 3
Training loss: 4.105908393859863
Validation loss: 3.551058110370431

Epoch: 5| Step: 4
Training loss: 3.52836275100708
Validation loss: 3.540722434238721

Epoch: 5| Step: 5
Training loss: 2.238140821456909
Validation loss: 3.5331333580837456

Epoch: 5| Step: 6
Training loss: 3.1187057495117188
Validation loss: 3.525242138934392

Epoch: 5| Step: 7
Training loss: 2.9008705615997314
Validation loss: 3.517425985746486

Epoch: 5| Step: 8
Training loss: 3.8164875507354736
Validation loss: 3.50979095889676

Epoch: 5| Step: 9
Training loss: 3.796443462371826
Validation loss: 3.501606769459222

Epoch: 5| Step: 10
Training loss: 4.073610782623291
Validation loss: 3.4938800719476517

Epoch: 8| Step: 0
Training loss: 3.9278640747070312
Validation loss: 3.48924429698657

Epoch: 5| Step: 1
Training loss: 3.3667824268341064
Validation loss: 3.482390152510776

Epoch: 5| Step: 2
Training loss: 3.6209328174591064
Validation loss: 3.4746296559610674

Epoch: 5| Step: 3
Training loss: 3.598881483078003
Validation loss: 3.46633977787469

Epoch: 5| Step: 4
Training loss: 2.2642252445220947
Validation loss: 3.462446592187369

Epoch: 5| Step: 5
Training loss: 2.4797821044921875
Validation loss: 3.4517758328427552

Epoch: 5| Step: 6
Training loss: 3.9067940711975098
Validation loss: 3.4461212106930312

Epoch: 5| Step: 7
Training loss: 3.2650229930877686
Validation loss: 3.4397054513295493

Epoch: 5| Step: 8
Training loss: 3.183380603790283
Validation loss: 3.4334496682690037

Epoch: 5| Step: 9
Training loss: 4.131638050079346
Validation loss: 3.4262024741018973

Epoch: 5| Step: 10
Training loss: 3.6535067558288574
Validation loss: 3.418946353338098

Epoch: 9| Step: 0
Training loss: 3.0209574699401855
Validation loss: 3.4129529409511115

Epoch: 5| Step: 1
Training loss: 3.8359475135803223
Validation loss: 3.408564667547903

Epoch: 5| Step: 2
Training loss: 3.1220459938049316
Validation loss: 3.4030021775153374

Epoch: 5| Step: 3
Training loss: 3.7248024940490723
Validation loss: 3.3973431330855175

Epoch: 5| Step: 4
Training loss: 2.6187832355499268
Validation loss: 3.393227272136237

Epoch: 5| Step: 5
Training loss: 3.391347885131836
Validation loss: 3.3886754358968427

Epoch: 5| Step: 6
Training loss: 3.4070053100585938
Validation loss: 3.3854366451181392

Epoch: 5| Step: 7
Training loss: 3.876685380935669
Validation loss: 3.3798792977486887

Epoch: 5| Step: 8
Training loss: 2.8739724159240723
Validation loss: 3.370970377358057

Epoch: 5| Step: 9
Training loss: 3.502108097076416
Validation loss: 3.363649534922774

Epoch: 5| Step: 10
Training loss: 3.3865225315093994
Validation loss: 3.361252554001347

Epoch: 10| Step: 0
Training loss: 3.1457438468933105
Validation loss: 3.3570810235956663

Epoch: 5| Step: 1
Training loss: 2.8475959300994873
Validation loss: 3.3517442313573693

Epoch: 5| Step: 2
Training loss: 3.4071121215820312
Validation loss: 3.344882378014185

Epoch: 5| Step: 3
Training loss: 3.4348654747009277
Validation loss: 3.3369923304486018

Epoch: 5| Step: 4
Training loss: 3.5039608478546143
Validation loss: 3.3319542177261843

Epoch: 5| Step: 5
Training loss: 3.1849236488342285
Validation loss: 3.330250750305832

Epoch: 5| Step: 6
Training loss: 3.862941026687622
Validation loss: 3.3259673785137873

Epoch: 5| Step: 7
Training loss: 3.1975624561309814
Validation loss: 3.3220612515685377

Epoch: 5| Step: 8
Training loss: 3.8262572288513184
Validation loss: 3.3164764168441936

Epoch: 5| Step: 9
Training loss: 2.8058340549468994
Validation loss: 3.3107535095625025

Epoch: 5| Step: 10
Training loss: 2.9961435794830322
Validation loss: 3.3026499876412014

Epoch: 11| Step: 0
Training loss: 3.2497620582580566
Validation loss: 3.297781621256182

Epoch: 5| Step: 1
Training loss: 2.486024856567383
Validation loss: 3.2941760632299606

Epoch: 5| Step: 2
Training loss: 3.316319227218628
Validation loss: 3.2912412920305805

Epoch: 5| Step: 3
Training loss: 3.6073577404022217
Validation loss: 3.2875804080758044

Epoch: 5| Step: 4
Training loss: 3.0021450519561768
Validation loss: 3.280733052120414

Epoch: 5| Step: 5
Training loss: 2.391911745071411
Validation loss: 3.2742652918702815

Epoch: 5| Step: 6
Training loss: 3.525202989578247
Validation loss: 3.268130866430139

Epoch: 5| Step: 7
Training loss: 4.043002128601074
Validation loss: 3.2714763610593733

Epoch: 5| Step: 8
Training loss: 3.556687831878662
Validation loss: 3.263335681730701

Epoch: 5| Step: 9
Training loss: 3.4142825603485107
Validation loss: 3.2566747870496524

Epoch: 5| Step: 10
Training loss: 3.1853389739990234
Validation loss: 3.25250215940578

Epoch: 12| Step: 0
Training loss: 3.3899741172790527
Validation loss: 3.249732494354248

Epoch: 5| Step: 1
Training loss: 2.874525547027588
Validation loss: 3.2430805442153767

Epoch: 5| Step: 2
Training loss: 3.9288933277130127
Validation loss: 3.2397482343899306

Epoch: 5| Step: 3
Training loss: 3.274792432785034
Validation loss: 3.2357922036160707

Epoch: 5| Step: 4
Training loss: 3.3180553913116455
Validation loss: 3.235209277881089

Epoch: 5| Step: 5
Training loss: 3.4278035163879395
Validation loss: 3.2350956086189515

Epoch: 5| Step: 6
Training loss: 3.284630537033081
Validation loss: 3.2316637757003948

Epoch: 5| Step: 7
Training loss: 3.6492793560028076
Validation loss: 3.2264267834283973

Epoch: 5| Step: 8
Training loss: 2.8858017921447754
Validation loss: 3.2193355765393985

Epoch: 5| Step: 9
Training loss: 3.208585739135742
Validation loss: 3.2146796103446715

Epoch: 5| Step: 10
Training loss: 2.0162415504455566
Validation loss: 3.20964248975118

Epoch: 13| Step: 0
Training loss: 3.7216098308563232
Validation loss: 3.2058050222294305

Epoch: 5| Step: 1
Training loss: 2.044081211090088
Validation loss: 3.202976019151749

Epoch: 5| Step: 2
Training loss: 3.886852979660034
Validation loss: 3.19688493205655

Epoch: 5| Step: 3
Training loss: 2.88151216506958
Validation loss: 3.193787338913128

Epoch: 5| Step: 4
Training loss: 2.9542200565338135
Validation loss: 3.1897455184690413

Epoch: 5| Step: 5
Training loss: 3.717575788497925
Validation loss: 3.1842181785132295

Epoch: 5| Step: 6
Training loss: 3.245847702026367
Validation loss: 3.177905262157481

Epoch: 5| Step: 7
Training loss: 2.6940360069274902
Validation loss: 3.177140669156146

Epoch: 5| Step: 8
Training loss: 2.9136803150177
Validation loss: 3.1772333447651198

Epoch: 5| Step: 9
Training loss: 2.902108669281006
Validation loss: 3.1699374234804543

Epoch: 5| Step: 10
Training loss: 4.2662034034729
Validation loss: 3.1682559802968013

Epoch: 14| Step: 0
Training loss: 3.0780811309814453
Validation loss: 3.161395929192984

Epoch: 5| Step: 1
Training loss: 3.3213653564453125
Validation loss: 3.154315920286281

Epoch: 5| Step: 2
Training loss: 2.869445323944092
Validation loss: 3.1474129512745845

Epoch: 5| Step: 3
Training loss: 2.2868711948394775
Validation loss: 3.1376852861014743

Epoch: 5| Step: 4
Training loss: 3.8882648944854736
Validation loss: 3.1322987387257237

Epoch: 5| Step: 5
Training loss: 2.5777781009674072
Validation loss: 3.1265224000459075

Epoch: 5| Step: 6
Training loss: 3.0831103324890137
Validation loss: 3.123203582661126

Epoch: 5| Step: 7
Training loss: 3.7032203674316406
Validation loss: 3.1222816051975375

Epoch: 5| Step: 8
Training loss: 3.0756194591522217
Validation loss: 3.1243175178445797

Epoch: 5| Step: 9
Training loss: 3.3092739582061768
Validation loss: 3.152905838463896

Epoch: 5| Step: 10
Training loss: 3.5433735847473145
Validation loss: 3.116085067872078

Epoch: 15| Step: 0
Training loss: 3.0302281379699707
Validation loss: 3.108831318475867

Epoch: 5| Step: 1
Training loss: 3.123203992843628
Validation loss: 3.10812190271193

Epoch: 5| Step: 2
Training loss: 4.090477466583252
Validation loss: 3.1071746990244877

Epoch: 5| Step: 3
Training loss: 2.5511395931243896
Validation loss: 3.099890083395025

Epoch: 5| Step: 4
Training loss: 2.8388562202453613
Validation loss: 3.0995782165117163

Epoch: 5| Step: 5
Training loss: 3.4548377990722656
Validation loss: 3.0880144180790072

Epoch: 5| Step: 6
Training loss: 2.8622448444366455
Validation loss: 3.0878998079607562

Epoch: 5| Step: 7
Training loss: 3.486062526702881
Validation loss: 3.0884645267199446

Epoch: 5| Step: 8
Training loss: 2.8862476348876953
Validation loss: 3.0846095290235294

Epoch: 5| Step: 9
Training loss: 3.532278537750244
Validation loss: 3.0805040303096978

Epoch: 5| Step: 10
Training loss: 2.4469523429870605
Validation loss: 3.0802980264027915

Epoch: 16| Step: 0
Training loss: 3.292309522628784
Validation loss: 3.076019517837032

Epoch: 5| Step: 1
Training loss: 3.191490650177002
Validation loss: 3.0707265202717116

Epoch: 5| Step: 2
Training loss: 2.5401177406311035
Validation loss: 3.065686994983304

Epoch: 5| Step: 3
Training loss: 2.6089694499969482
Validation loss: 3.0606370382411505

Epoch: 5| Step: 4
Training loss: 3.2826499938964844
Validation loss: 3.05951863719571

Epoch: 5| Step: 5
Training loss: 4.0599045753479
Validation loss: 3.0587117877057803

Epoch: 5| Step: 6
Training loss: 2.0214202404022217
Validation loss: 3.055456692172635

Epoch: 5| Step: 7
Training loss: 3.3590011596679688
Validation loss: 3.0514382444402224

Epoch: 5| Step: 8
Training loss: 2.9655508995056152
Validation loss: 3.0490031165461384

Epoch: 5| Step: 9
Training loss: 2.782303810119629
Validation loss: 3.060858611137636

Epoch: 5| Step: 10
Training loss: 4.196186065673828
Validation loss: 3.072612762451172

Epoch: 17| Step: 0
Training loss: 3.914733409881592
Validation loss: 3.0486996712223178

Epoch: 5| Step: 1
Training loss: 3.0415549278259277
Validation loss: 3.036188663974885

Epoch: 5| Step: 2
Training loss: 2.4761595726013184
Validation loss: 3.0336337653539514

Epoch: 5| Step: 3
Training loss: 3.4538369178771973
Validation loss: 3.0379976636619976

Epoch: 5| Step: 4
Training loss: 2.7061867713928223
Validation loss: 3.0383557324768393

Epoch: 5| Step: 5
Training loss: 2.786221981048584
Validation loss: 3.0333901092570317

Epoch: 5| Step: 6
Training loss: 3.2416229248046875
Validation loss: 3.0319308696254605

Epoch: 5| Step: 7
Training loss: 2.547168254852295
Validation loss: 3.0298103183828373

Epoch: 5| Step: 8
Training loss: 3.5737743377685547
Validation loss: 3.0313482848546838

Epoch: 5| Step: 9
Training loss: 2.160618305206299
Validation loss: 3.029036439875121

Epoch: 5| Step: 10
Training loss: 4.217474460601807
Validation loss: 3.025934603906447

Epoch: 18| Step: 0
Training loss: 3.0703225135803223
Validation loss: 3.0178884460080053

Epoch: 5| Step: 1
Training loss: 3.793325424194336
Validation loss: 3.013753532081522

Epoch: 5| Step: 2
Training loss: 3.4489784240722656
Validation loss: 3.0086841070523827

Epoch: 5| Step: 3
Training loss: 2.3259105682373047
Validation loss: 3.011252005894979

Epoch: 5| Step: 4
Training loss: 3.816148042678833
Validation loss: 3.0110980080020044

Epoch: 5| Step: 5
Training loss: 3.2410597801208496
Validation loss: 3.0065577671092045

Epoch: 5| Step: 6
Training loss: 2.56837797164917
Validation loss: 3.002688205370339

Epoch: 5| Step: 7
Training loss: 2.574822187423706
Validation loss: 3.003750680595316

Epoch: 5| Step: 8
Training loss: 3.014390230178833
Validation loss: 3.073981515822872

Epoch: 5| Step: 9
Training loss: 2.8625407218933105
Validation loss: 3.030071489272579

Epoch: 5| Step: 10
Training loss: 3.054615020751953
Validation loss: 2.9875507098372265

Epoch: 19| Step: 0
Training loss: 2.169314384460449
Validation loss: 2.996940479483656

Epoch: 5| Step: 1
Training loss: 2.5572350025177
Validation loss: 3.011253861970799

Epoch: 5| Step: 2
Training loss: 3.8959670066833496
Validation loss: 3.0227322578430176

Epoch: 5| Step: 3
Training loss: 2.9341044425964355
Validation loss: 3.0264423995889644

Epoch: 5| Step: 4
Training loss: 2.738765001296997
Validation loss: 3.0215802987416587

Epoch: 5| Step: 5
Training loss: 3.4142680168151855
Validation loss: 3.0072818879158265

Epoch: 5| Step: 6
Training loss: 3.460022449493408
Validation loss: 2.995472974674676

Epoch: 5| Step: 7
Training loss: 2.7087368965148926
Validation loss: 2.9859100362306

Epoch: 5| Step: 8
Training loss: 3.4233222007751465
Validation loss: 2.983334587466332

Epoch: 5| Step: 9
Training loss: 2.756674289703369
Validation loss: 2.9832618954361125

Epoch: 5| Step: 10
Training loss: 3.7691421508789062
Validation loss: 3.002404307806364

Epoch: 20| Step: 0
Training loss: 3.5577685832977295
Validation loss: 3.0188280279918382

Epoch: 5| Step: 1
Training loss: 3.5048789978027344
Validation loss: 2.9943372100912113

Epoch: 5| Step: 2
Training loss: 3.4091556072235107
Validation loss: 2.976935566112559

Epoch: 5| Step: 3
Training loss: 2.4263274669647217
Validation loss: 2.9691485102458666

Epoch: 5| Step: 4
Training loss: 2.781339645385742
Validation loss: 2.9747581328115156

Epoch: 5| Step: 5
Training loss: 2.746626138687134
Validation loss: 2.973378017384519

Epoch: 5| Step: 6
Training loss: 2.9512059688568115
Validation loss: 2.9696293723198677

Epoch: 5| Step: 7
Training loss: 3.2074456214904785
Validation loss: 2.965990730511245

Epoch: 5| Step: 8
Training loss: 2.4019923210144043
Validation loss: 2.963180418937437

Epoch: 5| Step: 9
Training loss: 3.0740318298339844
Validation loss: 2.9627230936481106

Epoch: 5| Step: 10
Training loss: 3.488359212875366
Validation loss: 2.963525151693693

Epoch: 21| Step: 0
Training loss: 2.889012575149536
Validation loss: 2.963246919775522

Epoch: 5| Step: 1
Training loss: 3.3517074584960938
Validation loss: 2.96743780310436

Epoch: 5| Step: 2
Training loss: 3.5441341400146484
Validation loss: 2.9697414213611233

Epoch: 5| Step: 3
Training loss: 3.0155038833618164
Validation loss: 2.962979478220786

Epoch: 5| Step: 4
Training loss: 2.8430988788604736
Validation loss: 2.949110133673555

Epoch: 5| Step: 5
Training loss: 3.18922758102417
Validation loss: 2.950514183249525

Epoch: 5| Step: 6
Training loss: 2.9805171489715576
Validation loss: 2.9566774445195354

Epoch: 5| Step: 7
Training loss: 3.4092369079589844
Validation loss: 2.9606093488713747

Epoch: 5| Step: 8
Training loss: 2.8534839153289795
Validation loss: 2.9578510792024675

Epoch: 5| Step: 9
Training loss: 2.3935112953186035
Validation loss: 2.9520447177271687

Epoch: 5| Step: 10
Training loss: 2.905236005783081
Validation loss: 2.946866378989271

Epoch: 22| Step: 0
Training loss: 3.3935165405273438
Validation loss: 2.941306524379279

Epoch: 5| Step: 1
Training loss: 2.3146352767944336
Validation loss: 2.94470117938134

Epoch: 5| Step: 2
Training loss: 2.6013646125793457
Validation loss: 2.951654403440414

Epoch: 5| Step: 3
Training loss: 3.00413179397583
Validation loss: 2.96696977461538

Epoch: 5| Step: 4
Training loss: 3.013056993484497
Validation loss: 2.9792952358081775

Epoch: 5| Step: 5
Training loss: 2.867558479309082
Validation loss: 2.9634397824605307

Epoch: 5| Step: 6
Training loss: 3.972048282623291
Validation loss: 2.9511353354300223

Epoch: 5| Step: 7
Training loss: 3.1070008277893066
Validation loss: 2.9330292722230316

Epoch: 5| Step: 8
Training loss: 2.5044376850128174
Validation loss: 2.9324010777217087

Epoch: 5| Step: 9
Training loss: 3.2173819541931152
Validation loss: 2.935681753261115

Epoch: 5| Step: 10
Training loss: 3.3391966819763184
Validation loss: 2.9345881451842604

Epoch: 23| Step: 0
Training loss: 3.028343439102173
Validation loss: 2.9291402960336335

Epoch: 5| Step: 1
Training loss: 3.586637496948242
Validation loss: 2.9324034055074057

Epoch: 5| Step: 2
Training loss: 2.405754804611206
Validation loss: 2.9348119817754275

Epoch: 5| Step: 3
Training loss: 2.4398748874664307
Validation loss: 2.9342697948537846

Epoch: 5| Step: 4
Training loss: 2.823111057281494
Validation loss: 2.955315213049612

Epoch: 5| Step: 5
Training loss: 3.431501865386963
Validation loss: 2.9579092712812525

Epoch: 5| Step: 6
Training loss: 3.0503268241882324
Validation loss: 2.958096868248396

Epoch: 5| Step: 7
Training loss: 4.282425880432129
Validation loss: 2.9504256145928496

Epoch: 5| Step: 8
Training loss: 2.441791534423828
Validation loss: 2.921542541955107

Epoch: 5| Step: 9
Training loss: 3.1811230182647705
Validation loss: 2.9205130095122964

Epoch: 5| Step: 10
Training loss: 2.4317004680633545
Validation loss: 2.920934812996977

Epoch: 24| Step: 0
Training loss: 2.6325337886810303
Validation loss: 2.9195644394043954

Epoch: 5| Step: 1
Training loss: 2.2656424045562744
Validation loss: 2.917456455128167

Epoch: 5| Step: 2
Training loss: 2.878831386566162
Validation loss: 2.9163531359805854

Epoch: 5| Step: 3
Training loss: 3.437309741973877
Validation loss: 2.9155279103145806

Epoch: 5| Step: 4
Training loss: 3.350639820098877
Validation loss: 2.9193835976303264

Epoch: 5| Step: 5
Training loss: 3.2419140338897705
Validation loss: 2.919456487060875

Epoch: 5| Step: 6
Training loss: 3.0176820755004883
Validation loss: 2.916110372030607

Epoch: 5| Step: 7
Training loss: 3.1008057594299316
Validation loss: 2.9210037313481814

Epoch: 5| Step: 8
Training loss: 3.298872709274292
Validation loss: 2.9167714247139553

Epoch: 5| Step: 9
Training loss: 3.114680767059326
Validation loss: 2.9129454987023466

Epoch: 5| Step: 10
Training loss: 2.6779422760009766
Validation loss: 2.9125613756077264

Epoch: 25| Step: 0
Training loss: 2.7676596641540527
Validation loss: 2.909053164143716

Epoch: 5| Step: 1
Training loss: 3.2046844959259033
Validation loss: 2.9089226671444472

Epoch: 5| Step: 2
Training loss: 3.104388475418091
Validation loss: 2.9073575901728805

Epoch: 5| Step: 3
Training loss: 3.139695882797241
Validation loss: 2.907538116619151

Epoch: 5| Step: 4
Training loss: 2.923550844192505
Validation loss: 2.9054008786396315

Epoch: 5| Step: 5
Training loss: 3.1473333835601807
Validation loss: 2.9044225190275457

Epoch: 5| Step: 6
Training loss: 3.6121273040771484
Validation loss: 2.9040556492344027

Epoch: 5| Step: 7
Training loss: 2.8891255855560303
Validation loss: 2.903378904506724

Epoch: 5| Step: 8
Training loss: 2.037926197052002
Validation loss: 2.9002515167318363

Epoch: 5| Step: 9
Training loss: 2.8981516361236572
Validation loss: 2.902732297938357

Epoch: 5| Step: 10
Training loss: 3.2358272075653076
Validation loss: 2.8996689088882937

Epoch: 26| Step: 0
Training loss: 2.526500940322876
Validation loss: 2.897022306278188

Epoch: 5| Step: 1
Training loss: 3.141490936279297
Validation loss: 2.8929626044406684

Epoch: 5| Step: 2
Training loss: 2.280085563659668
Validation loss: 2.892128316305017

Epoch: 5| Step: 3
Training loss: 3.3980441093444824
Validation loss: 2.892113454880253

Epoch: 5| Step: 4
Training loss: 3.159806728363037
Validation loss: 2.8901631909032024

Epoch: 5| Step: 5
Training loss: 2.700244426727295
Validation loss: 2.892898964625533

Epoch: 5| Step: 6
Training loss: 2.8112716674804688
Validation loss: 2.8949527458478044

Epoch: 5| Step: 7
Training loss: 3.253690004348755
Validation loss: 2.8861504831621723

Epoch: 5| Step: 8
Training loss: 3.135751247406006
Validation loss: 2.8787185248508247

Epoch: 5| Step: 9
Training loss: 3.153019666671753
Validation loss: 2.882852180029756

Epoch: 5| Step: 10
Training loss: 3.2663040161132812
Validation loss: 2.879333257675171

Epoch: 27| Step: 0
Training loss: 2.9160923957824707
Validation loss: 2.8774865083797003

Epoch: 5| Step: 1
Training loss: 2.325361728668213
Validation loss: 2.874705894019014

Epoch: 5| Step: 2
Training loss: 3.476318359375
Validation loss: 2.8784875305750037

Epoch: 5| Step: 3
Training loss: 2.7239017486572266
Validation loss: 2.8734116169714157

Epoch: 5| Step: 4
Training loss: 2.890371799468994
Validation loss: 2.8690008142943024

Epoch: 5| Step: 5
Training loss: 2.9493179321289062
Validation loss: 2.873023312578919

Epoch: 5| Step: 6
Training loss: 2.4813859462738037
Validation loss: 2.867852859599616

Epoch: 5| Step: 7
Training loss: 2.7231242656707764
Validation loss: 2.86671362128309

Epoch: 5| Step: 8
Training loss: 3.6712448596954346
Validation loss: 2.870626703385384

Epoch: 5| Step: 9
Training loss: 2.6904067993164062
Validation loss: 2.8738737439596527

Epoch: 5| Step: 10
Training loss: 3.9749929904937744
Validation loss: 2.9120580406599146

Epoch: 28| Step: 0
Training loss: 2.5508198738098145
Validation loss: 2.876390682753696

Epoch: 5| Step: 1
Training loss: 3.1674838066101074
Validation loss: 2.863053657675302

Epoch: 5| Step: 2
Training loss: 3.935809373855591
Validation loss: 2.8610725941196566

Epoch: 5| Step: 3
Training loss: 3.2805373668670654
Validation loss: 2.86321347759616

Epoch: 5| Step: 4
Training loss: 2.8596599102020264
Validation loss: 2.8682460041456324

Epoch: 5| Step: 5
Training loss: 2.227281093597412
Validation loss: 2.8674693645969516

Epoch: 5| Step: 6
Training loss: 2.9517822265625
Validation loss: 2.865125422836632

Epoch: 5| Step: 7
Training loss: 2.9605469703674316
Validation loss: 2.8693231715950915

Epoch: 5| Step: 8
Training loss: 2.1957578659057617
Validation loss: 2.8654269838845856

Epoch: 5| Step: 9
Training loss: 3.263537645339966
Validation loss: 2.856237949863557

Epoch: 5| Step: 10
Training loss: 3.2853026390075684
Validation loss: 2.8523310256260697

Epoch: 29| Step: 0
Training loss: 3.478675127029419
Validation loss: 2.8547619491495113

Epoch: 5| Step: 1
Training loss: 3.2436490058898926
Validation loss: 2.8545397789247575

Epoch: 5| Step: 2
Training loss: 2.9844322204589844
Validation loss: 2.852116056667861

Epoch: 5| Step: 3
Training loss: 2.685697078704834
Validation loss: 2.840266366158762

Epoch: 5| Step: 4
Training loss: 3.203582763671875
Validation loss: 2.8444888976312455

Epoch: 5| Step: 5
Training loss: 3.142110586166382
Validation loss: 2.8444560368855796

Epoch: 5| Step: 6
Training loss: 2.5111262798309326
Validation loss: 2.847049584952734

Epoch: 5| Step: 7
Training loss: 2.675161838531494
Validation loss: 2.8469792412173365

Epoch: 5| Step: 8
Training loss: 2.5008082389831543
Validation loss: 2.8335964269535516

Epoch: 5| Step: 9
Training loss: 3.579484224319458
Validation loss: 2.8261899691756054

Epoch: 5| Step: 10
Training loss: 2.2571256160736084
Validation loss: 2.8241731274512505

Epoch: 30| Step: 0
Training loss: 3.7187538146972656
Validation loss: 2.8224171002705893

Epoch: 5| Step: 1
Training loss: 2.877169132232666
Validation loss: 2.8222137292226157

Epoch: 5| Step: 2
Training loss: 3.2261955738067627
Validation loss: 2.815624137078562

Epoch: 5| Step: 3
Training loss: 2.7381882667541504
Validation loss: 2.8125424436343613

Epoch: 5| Step: 4
Training loss: 2.6020421981811523
Validation loss: 2.8114616742698093

Epoch: 5| Step: 5
Training loss: 2.8770885467529297
Validation loss: 2.80551589176219

Epoch: 5| Step: 6
Training loss: 2.823294162750244
Validation loss: 2.8051137949830744

Epoch: 5| Step: 7
Training loss: 2.9026737213134766
Validation loss: 2.812928110040644

Epoch: 5| Step: 8
Training loss: 3.0061607360839844
Validation loss: 2.823008983365951

Epoch: 5| Step: 9
Training loss: 2.956498146057129
Validation loss: 2.831049955019387

Epoch: 5| Step: 10
Training loss: 2.4874680042266846
Validation loss: 2.8150468616075415

Epoch: 31| Step: 0
Training loss: 3.099071979522705
Validation loss: 2.802453505095615

Epoch: 5| Step: 1
Training loss: 3.2030880451202393
Validation loss: 2.798625934508539

Epoch: 5| Step: 2
Training loss: 2.504251480102539
Validation loss: 2.797563191383116

Epoch: 5| Step: 3
Training loss: 3.1565024852752686
Validation loss: 2.7994225358450286

Epoch: 5| Step: 4
Training loss: 3.3204147815704346
Validation loss: 2.7952156041258123

Epoch: 5| Step: 5
Training loss: 2.692060708999634
Validation loss: 2.7946319336532266

Epoch: 5| Step: 6
Training loss: 2.4597201347351074
Validation loss: 2.793821660421228

Epoch: 5| Step: 7
Training loss: 2.7743709087371826
Validation loss: 2.7922567629045054

Epoch: 5| Step: 8
Training loss: 2.887380838394165
Validation loss: 2.792632210639215

Epoch: 5| Step: 9
Training loss: 2.7312893867492676
Validation loss: 2.785003523672781

Epoch: 5| Step: 10
Training loss: 3.346639633178711
Validation loss: 2.78488128928728

Epoch: 32| Step: 0
Training loss: 3.3300814628601074
Validation loss: 2.7884382611961773

Epoch: 5| Step: 1
Training loss: 3.9977269172668457
Validation loss: 2.7890201640385452

Epoch: 5| Step: 2
Training loss: 2.533500909805298
Validation loss: 2.7876688229140414

Epoch: 5| Step: 3
Training loss: 3.4499716758728027
Validation loss: 2.78584889699054

Epoch: 5| Step: 4
Training loss: 2.5529494285583496
Validation loss: 2.785424606774443

Epoch: 5| Step: 5
Training loss: 2.9569478034973145
Validation loss: 2.7876324166533766

Epoch: 5| Step: 6
Training loss: 2.0466861724853516
Validation loss: 2.7809889675468527

Epoch: 5| Step: 7
Training loss: 3.3588435649871826
Validation loss: 2.7843769545196206

Epoch: 5| Step: 8
Training loss: 2.528785467147827
Validation loss: 2.7844858605374574

Epoch: 5| Step: 9
Training loss: 2.332364797592163
Validation loss: 2.781420656429824

Epoch: 5| Step: 10
Training loss: 2.8611326217651367
Validation loss: 2.7794772271187074

Epoch: 33| Step: 0
Training loss: 3.402042865753174
Validation loss: 2.7761655469094553

Epoch: 5| Step: 1
Training loss: 2.4367594718933105
Validation loss: 2.777143504029961

Epoch: 5| Step: 2
Training loss: 2.899893045425415
Validation loss: 2.7784328332511325

Epoch: 5| Step: 3
Training loss: 3.5851924419403076
Validation loss: 2.773647969768893

Epoch: 5| Step: 4
Training loss: 2.742945432662964
Validation loss: 2.7769336546621015

Epoch: 5| Step: 5
Training loss: 2.694535493850708
Validation loss: 2.7768121919324322

Epoch: 5| Step: 6
Training loss: 3.2775521278381348
Validation loss: 2.773893399905133

Epoch: 5| Step: 7
Training loss: 2.7618367671966553
Validation loss: 2.772197300387967

Epoch: 5| Step: 8
Training loss: 3.068000316619873
Validation loss: 2.771185228901525

Epoch: 5| Step: 9
Training loss: 2.7012391090393066
Validation loss: 2.770911265445012

Epoch: 5| Step: 10
Training loss: 2.3050403594970703
Validation loss: 2.779470674453243

Epoch: 34| Step: 0
Training loss: 3.1571574211120605
Validation loss: 2.7813582317803496

Epoch: 5| Step: 1
Training loss: 3.2101879119873047
Validation loss: 2.792802474832022

Epoch: 5| Step: 2
Training loss: 3.2242865562438965
Validation loss: 2.796495024875928

Epoch: 5| Step: 3
Training loss: 2.9130783081054688
Validation loss: 2.7964656096632763

Epoch: 5| Step: 4
Training loss: 2.0676140785217285
Validation loss: 2.815123511898902

Epoch: 5| Step: 5
Training loss: 2.199352741241455
Validation loss: 2.789985679811047

Epoch: 5| Step: 6
Training loss: 3.1209230422973633
Validation loss: 2.779962998564525

Epoch: 5| Step: 7
Training loss: 2.936551570892334
Validation loss: 2.769156458557293

Epoch: 5| Step: 8
Training loss: 3.1365885734558105
Validation loss: 2.7683429051471014

Epoch: 5| Step: 9
Training loss: 2.9021499156951904
Validation loss: 2.762770155424713

Epoch: 5| Step: 10
Training loss: 3.090733289718628
Validation loss: 2.7648656214437177

Epoch: 35| Step: 0
Training loss: 3.149486780166626
Validation loss: 2.76531123089534

Epoch: 5| Step: 1
Training loss: 2.6408638954162598
Validation loss: 2.7657352211654826

Epoch: 5| Step: 2
Training loss: 3.3708980083465576
Validation loss: 2.7613646035553305

Epoch: 5| Step: 3
Training loss: 2.508767604827881
Validation loss: 2.758567366548764

Epoch: 5| Step: 4
Training loss: 2.482386350631714
Validation loss: 2.7586765750761955

Epoch: 5| Step: 5
Training loss: 3.7688450813293457
Validation loss: 2.765629950390067

Epoch: 5| Step: 6
Training loss: 2.9633004665374756
Validation loss: 2.7657241308560936

Epoch: 5| Step: 7
Training loss: 2.4423513412475586
Validation loss: 2.7670221226189726

Epoch: 5| Step: 8
Training loss: 2.723145008087158
Validation loss: 2.771069424126738

Epoch: 5| Step: 9
Training loss: 3.1220154762268066
Validation loss: 2.7935191764626452

Epoch: 5| Step: 10
Training loss: 2.748481512069702
Validation loss: 2.821140473888766

Epoch: 36| Step: 0
Training loss: 3.369004011154175
Validation loss: 2.782820265780213

Epoch: 5| Step: 1
Training loss: 2.261697292327881
Validation loss: 2.7645740791033675

Epoch: 5| Step: 2
Training loss: 2.5323081016540527
Validation loss: 2.758101758136544

Epoch: 5| Step: 3
Training loss: 2.7689895629882812
Validation loss: 2.7600842086217736

Epoch: 5| Step: 4
Training loss: 3.4780449867248535
Validation loss: 2.7595312210821334

Epoch: 5| Step: 5
Training loss: 2.691725730895996
Validation loss: 2.756526795766687

Epoch: 5| Step: 6
Training loss: 2.8075191974639893
Validation loss: 2.7589979274298555

Epoch: 5| Step: 7
Training loss: 3.3221325874328613
Validation loss: 2.756707432449505

Epoch: 5| Step: 8
Training loss: 2.8055992126464844
Validation loss: 2.755806394802627

Epoch: 5| Step: 9
Training loss: 2.9242019653320312
Validation loss: 2.755708143275271

Epoch: 5| Step: 10
Training loss: 2.885159730911255
Validation loss: 2.7544688204283356

Epoch: 37| Step: 0
Training loss: 2.9730217456817627
Validation loss: 2.7530450359467538

Epoch: 5| Step: 1
Training loss: 3.4748847484588623
Validation loss: 2.7514529253846858

Epoch: 5| Step: 2
Training loss: 3.222527265548706
Validation loss: 2.750801488917361

Epoch: 5| Step: 3
Training loss: 3.567884922027588
Validation loss: 2.7528572390156407

Epoch: 5| Step: 4
Training loss: 3.0968756675720215
Validation loss: 2.758666761459843

Epoch: 5| Step: 5
Training loss: 1.9888683557510376
Validation loss: 2.776768366495768

Epoch: 5| Step: 6
Training loss: 3.007279634475708
Validation loss: 2.7694779288384224

Epoch: 5| Step: 7
Training loss: 3.2446486949920654
Validation loss: 2.7621975893615396

Epoch: 5| Step: 8
Training loss: 2.506998062133789
Validation loss: 2.7604671088598107

Epoch: 5| Step: 9
Training loss: 2.2378242015838623
Validation loss: 2.7492608434410504

Epoch: 5| Step: 10
Training loss: 2.3475284576416016
Validation loss: 2.748101306217973

Epoch: 38| Step: 0
Training loss: 3.2579236030578613
Validation loss: 2.7486009546505508

Epoch: 5| Step: 1
Training loss: 2.59505295753479
Validation loss: 2.744109220402215

Epoch: 5| Step: 2
Training loss: 2.5015289783477783
Validation loss: 2.7418110396272395

Epoch: 5| Step: 3
Training loss: 3.161111831665039
Validation loss: 2.7474118278872584

Epoch: 5| Step: 4
Training loss: 3.6854946613311768
Validation loss: 2.746203484073762

Epoch: 5| Step: 5
Training loss: 3.570903778076172
Validation loss: 2.7547211621397283

Epoch: 5| Step: 6
Training loss: 2.788778305053711
Validation loss: 2.756466401520596

Epoch: 5| Step: 7
Training loss: 2.2757320404052734
Validation loss: 2.755573534196423

Epoch: 5| Step: 8
Training loss: 2.6862833499908447
Validation loss: 2.748014183454616

Epoch: 5| Step: 9
Training loss: 2.215743064880371
Validation loss: 2.7420757662865425

Epoch: 5| Step: 10
Training loss: 2.962467908859253
Validation loss: 2.7375808710693033

Epoch: 39| Step: 0
Training loss: 3.188847780227661
Validation loss: 2.735505724465975

Epoch: 5| Step: 1
Training loss: 3.7220559120178223
Validation loss: 2.7405900032289567

Epoch: 5| Step: 2
Training loss: 2.73846173286438
Validation loss: 2.7424522010228967

Epoch: 5| Step: 3
Training loss: 2.5726258754730225
Validation loss: 2.7362199291106193

Epoch: 5| Step: 4
Training loss: 2.89643931388855
Validation loss: 2.7277818495227444

Epoch: 5| Step: 5
Training loss: 2.689683437347412
Validation loss: 2.72796973874492

Epoch: 5| Step: 6
Training loss: 3.01652455329895
Validation loss: 2.7256133582002375

Epoch: 5| Step: 7
Training loss: 2.7233877182006836
Validation loss: 2.7239082628680813

Epoch: 5| Step: 8
Training loss: 2.779301404953003
Validation loss: 2.724154782551591

Epoch: 5| Step: 9
Training loss: 2.718278408050537
Validation loss: 2.7277029842458744

Epoch: 5| Step: 10
Training loss: 2.4727389812469482
Validation loss: 2.7312641297617266

Epoch: 40| Step: 0
Training loss: 2.0612213611602783
Validation loss: 2.7269990777456634

Epoch: 5| Step: 1
Training loss: 3.621840715408325
Validation loss: 2.72141686818933

Epoch: 5| Step: 2
Training loss: 3.1647567749023438
Validation loss: 2.723495988435643

Epoch: 5| Step: 3
Training loss: 2.1869277954101562
Validation loss: 2.7176256359264417

Epoch: 5| Step: 4
Training loss: 1.8813947439193726
Validation loss: 2.7161053970295894

Epoch: 5| Step: 5
Training loss: 3.229790449142456
Validation loss: 2.7148641309430523

Epoch: 5| Step: 6
Training loss: 3.342851161956787
Validation loss: 2.714467389609224

Epoch: 5| Step: 7
Training loss: 2.9836037158966064
Validation loss: 2.716345420447729

Epoch: 5| Step: 8
Training loss: 2.8181333541870117
Validation loss: 2.7137594556295745

Epoch: 5| Step: 9
Training loss: 3.3882477283477783
Validation loss: 2.711111096925633

Epoch: 5| Step: 10
Training loss: 2.7613489627838135
Validation loss: 2.709477247730378

Epoch: 41| Step: 0
Training loss: 3.078443765640259
Validation loss: 2.7177020452355825

Epoch: 5| Step: 1
Training loss: 2.267648220062256
Validation loss: 2.7216200469642557

Epoch: 5| Step: 2
Training loss: 3.7139954566955566
Validation loss: 2.7191594672459427

Epoch: 5| Step: 3
Training loss: 2.7127647399902344
Validation loss: 2.7158393911136094

Epoch: 5| Step: 4
Training loss: 3.240466356277466
Validation loss: 2.7094895506417878

Epoch: 5| Step: 5
Training loss: 3.1221439838409424
Validation loss: 2.7122574467812814

Epoch: 5| Step: 6
Training loss: 1.9436378479003906
Validation loss: 2.713277555281116

Epoch: 5| Step: 7
Training loss: 3.0627529621124268
Validation loss: 2.7088519834703013

Epoch: 5| Step: 8
Training loss: 2.9116368293762207
Validation loss: 2.707235905431932

Epoch: 5| Step: 9
Training loss: 2.4380199909210205
Validation loss: 2.7082793148615028

Epoch: 5| Step: 10
Training loss: 2.8933298587799072
Validation loss: 2.7064671952237367

Epoch: 42| Step: 0
Training loss: 3.029723644256592
Validation loss: 2.705319648147911

Epoch: 5| Step: 1
Training loss: 2.95540189743042
Validation loss: 2.7089386345237814

Epoch: 5| Step: 2
Training loss: 3.5441768169403076
Validation loss: 2.71389582849318

Epoch: 5| Step: 3
Training loss: 2.8373863697052
Validation loss: 2.714955340149582

Epoch: 5| Step: 4
Training loss: 3.303819179534912
Validation loss: 2.7124484585177515

Epoch: 5| Step: 5
Training loss: 2.709294080734253
Validation loss: 2.712893593695856

Epoch: 5| Step: 6
Training loss: 2.4814631938934326
Validation loss: 2.7056212655959593

Epoch: 5| Step: 7
Training loss: 2.072392463684082
Validation loss: 2.7044966015764462

Epoch: 5| Step: 8
Training loss: 3.3593337535858154
Validation loss: 2.7049789454347346

Epoch: 5| Step: 9
Training loss: 2.518772602081299
Validation loss: 2.70645167750697

Epoch: 5| Step: 10
Training loss: 2.4779114723205566
Validation loss: 2.715765186535415

Epoch: 43| Step: 0
Training loss: 2.3002543449401855
Validation loss: 2.724215422907183

Epoch: 5| Step: 1
Training loss: 2.8176791667938232
Validation loss: 2.750879985029979

Epoch: 5| Step: 2
Training loss: 3.0317625999450684
Validation loss: 2.731222783365557

Epoch: 5| Step: 3
Training loss: 3.9048969745635986
Validation loss: 2.7251919700253393

Epoch: 5| Step: 4
Training loss: 2.778512477874756
Validation loss: 2.7068404741184686

Epoch: 5| Step: 5
Training loss: 2.277906656265259
Validation loss: 2.699164785364623

Epoch: 5| Step: 6
Training loss: 2.6326041221618652
Validation loss: 2.706672450547577

Epoch: 5| Step: 7
Training loss: 3.1115658283233643
Validation loss: 2.718497235287902

Epoch: 5| Step: 8
Training loss: 2.541592836380005
Validation loss: 2.736368453630837

Epoch: 5| Step: 9
Training loss: 3.272130250930786
Validation loss: 2.74720613930815

Epoch: 5| Step: 10
Training loss: 2.9753835201263428
Validation loss: 2.7319060807586997

Epoch: 44| Step: 0
Training loss: 3.2362499237060547
Validation loss: 2.7118243427686792

Epoch: 5| Step: 1
Training loss: 2.955854892730713
Validation loss: 2.7034123738606772

Epoch: 5| Step: 2
Training loss: 2.546980619430542
Validation loss: 2.6976575518167145

Epoch: 5| Step: 3
Training loss: 2.6845264434814453
Validation loss: 2.697015159873552

Epoch: 5| Step: 4
Training loss: 2.532238245010376
Validation loss: 2.7104835100071405

Epoch: 5| Step: 5
Training loss: 2.749136447906494
Validation loss: 2.7357067344009236

Epoch: 5| Step: 6
Training loss: 3.035710573196411
Validation loss: 2.7501264746471117

Epoch: 5| Step: 7
Training loss: 2.8337886333465576
Validation loss: 2.744536312677527

Epoch: 5| Step: 8
Training loss: 3.1164731979370117
Validation loss: 2.707210753553657

Epoch: 5| Step: 9
Training loss: 2.967912197113037
Validation loss: 2.6987851640229583

Epoch: 5| Step: 10
Training loss: 2.840569496154785
Validation loss: 2.7013673756712224

Epoch: 45| Step: 0
Training loss: 3.1243300437927246
Validation loss: 2.704771641762026

Epoch: 5| Step: 1
Training loss: 2.3905060291290283
Validation loss: 2.712266017031926

Epoch: 5| Step: 2
Training loss: 3.0778584480285645
Validation loss: 2.7208086162485103

Epoch: 5| Step: 3
Training loss: 3.2125344276428223
Validation loss: 2.7219578091816237

Epoch: 5| Step: 4
Training loss: 2.324615955352783
Validation loss: 2.721652712873233

Epoch: 5| Step: 5
Training loss: 2.7302908897399902
Validation loss: 2.7150961096568773

Epoch: 5| Step: 6
Training loss: 3.338181972503662
Validation loss: 2.7170201732266333

Epoch: 5| Step: 7
Training loss: 2.363138198852539
Validation loss: 2.713497254156297

Epoch: 5| Step: 8
Training loss: 2.5066633224487305
Validation loss: 2.7202068990276707

Epoch: 5| Step: 9
Training loss: 3.113936424255371
Validation loss: 2.7102399744013304

Epoch: 5| Step: 10
Training loss: 3.4128341674804688
Validation loss: 2.7062959696656916

Epoch: 46| Step: 0
Training loss: 2.6299245357513428
Validation loss: 2.7149829223591793

Epoch: 5| Step: 1
Training loss: 3.2608108520507812
Validation loss: 2.718480694678522

Epoch: 5| Step: 2
Training loss: 2.6912951469421387
Validation loss: 2.709722900903353

Epoch: 5| Step: 3
Training loss: 2.826354503631592
Validation loss: 2.707647887609338

Epoch: 5| Step: 4
Training loss: 2.6692886352539062
Validation loss: 2.7093909453320246

Epoch: 5| Step: 5
Training loss: 3.3902740478515625
Validation loss: 2.7059788806464082

Epoch: 5| Step: 6
Training loss: 2.6912970542907715
Validation loss: 2.7070305244897

Epoch: 5| Step: 7
Training loss: 2.453174352645874
Validation loss: 2.7017669523915937

Epoch: 5| Step: 8
Training loss: 2.995126485824585
Validation loss: 2.7022855974012807

Epoch: 5| Step: 9
Training loss: 2.3428409099578857
Validation loss: 2.6911976183614423

Epoch: 5| Step: 10
Training loss: 3.361804246902466
Validation loss: 2.6854741188787643

Epoch: 47| Step: 0
Training loss: 3.238621234893799
Validation loss: 2.681766345936765

Epoch: 5| Step: 1
Training loss: 3.071397304534912
Validation loss: 2.683736006418864

Epoch: 5| Step: 2
Training loss: 2.887648105621338
Validation loss: 2.6833243626420216

Epoch: 5| Step: 3
Training loss: 2.7804055213928223
Validation loss: 2.6828648428763113

Epoch: 5| Step: 4
Training loss: 2.22058367729187
Validation loss: 2.682178287095921

Epoch: 5| Step: 5
Training loss: 2.6704483032226562
Validation loss: 2.681695117745348

Epoch: 5| Step: 6
Training loss: 3.7054359912872314
Validation loss: 2.681119925232344

Epoch: 5| Step: 7
Training loss: 2.496985912322998
Validation loss: 2.6778941051934355

Epoch: 5| Step: 8
Training loss: 2.3924033641815186
Validation loss: 2.6800060220943984

Epoch: 5| Step: 9
Training loss: 2.9445269107818604
Validation loss: 2.680077927086943

Epoch: 5| Step: 10
Training loss: 2.750613212585449
Validation loss: 2.678509117454611

Epoch: 48| Step: 0
Training loss: 2.8996806144714355
Validation loss: 2.6775154016351186

Epoch: 5| Step: 1
Training loss: 3.3229527473449707
Validation loss: 2.68166010866883

Epoch: 5| Step: 2
Training loss: 2.7624082565307617
Validation loss: 2.6848527667342976

Epoch: 5| Step: 3
Training loss: 3.086181163787842
Validation loss: 2.690269967561127

Epoch: 5| Step: 4
Training loss: 2.9702227115631104
Validation loss: 2.6979001619482554

Epoch: 5| Step: 5
Training loss: 3.0365679264068604
Validation loss: 2.6871137824109805

Epoch: 5| Step: 6
Training loss: 2.7691104412078857
Validation loss: 2.681237719392264

Epoch: 5| Step: 7
Training loss: 2.7126383781433105
Validation loss: 2.679785397744948

Epoch: 5| Step: 8
Training loss: 2.335172176361084
Validation loss: 2.676001576967137

Epoch: 5| Step: 9
Training loss: 3.112922191619873
Validation loss: 2.675855226414178

Epoch: 5| Step: 10
Training loss: 1.9205739498138428
Validation loss: 2.67384865719785

Epoch: 49| Step: 0
Training loss: 3.0219664573669434
Validation loss: 2.6699237413303827

Epoch: 5| Step: 1
Training loss: 2.8888771533966064
Validation loss: 2.6726240317026773

Epoch: 5| Step: 2
Training loss: 3.2064597606658936
Validation loss: 2.6773086773451937

Epoch: 5| Step: 3
Training loss: 2.881957769393921
Validation loss: 2.6701115459524174

Epoch: 5| Step: 4
Training loss: 2.7031989097595215
Validation loss: 2.6699625317768385

Epoch: 5| Step: 5
Training loss: 2.8228588104248047
Validation loss: 2.670812447865804

Epoch: 5| Step: 6
Training loss: 2.2621536254882812
Validation loss: 2.6672267375453824

Epoch: 5| Step: 7
Training loss: 2.8012776374816895
Validation loss: 2.6660128229407856

Epoch: 5| Step: 8
Training loss: 3.0454463958740234
Validation loss: 2.668046392420287

Epoch: 5| Step: 9
Training loss: 2.627861499786377
Validation loss: 2.6639231174222884

Epoch: 5| Step: 10
Training loss: 2.7404093742370605
Validation loss: 2.6666572734873784

Epoch: 50| Step: 0
Training loss: 3.168759346008301
Validation loss: 2.6722688008380193

Epoch: 5| Step: 1
Training loss: 2.835969924926758
Validation loss: 2.6692180043907574

Epoch: 5| Step: 2
Training loss: 2.2202250957489014
Validation loss: 2.6680938992449033

Epoch: 5| Step: 3
Training loss: 2.686798572540283
Validation loss: 2.666369956026795

Epoch: 5| Step: 4
Training loss: 2.5566964149475098
Validation loss: 2.6722871411231255

Epoch: 5| Step: 5
Training loss: 3.2434463500976562
Validation loss: 2.6764678237258748

Epoch: 5| Step: 6
Training loss: 3.15390944480896
Validation loss: 2.6738808565242316

Epoch: 5| Step: 7
Training loss: 2.7636163234710693
Validation loss: 2.6658469682098715

Epoch: 5| Step: 8
Training loss: 3.506903886795044
Validation loss: 2.6679548678859586

Epoch: 5| Step: 9
Training loss: 2.8249568939208984
Validation loss: 2.668666624253796

Epoch: 5| Step: 10
Training loss: 1.8742485046386719
Validation loss: 2.665272602470972

Epoch: 51| Step: 0
Training loss: 2.467611789703369
Validation loss: 2.663028609368109

Epoch: 5| Step: 1
Training loss: 2.929248571395874
Validation loss: 2.6650236857834684

Epoch: 5| Step: 2
Training loss: 2.684055805206299
Validation loss: 2.6643396141708537

Epoch: 5| Step: 3
Training loss: 2.929285764694214
Validation loss: 2.6589874503433064

Epoch: 5| Step: 4
Training loss: 2.5312695503234863
Validation loss: 2.6625025862006733

Epoch: 5| Step: 5
Training loss: 2.7416207790374756
Validation loss: 2.6644658427084646

Epoch: 5| Step: 6
Training loss: 2.0496127605438232
Validation loss: 2.6636020650145826

Epoch: 5| Step: 7
Training loss: 3.909156084060669
Validation loss: 2.662194564778318

Epoch: 5| Step: 8
Training loss: 2.9824280738830566
Validation loss: 2.6638163725535073

Epoch: 5| Step: 9
Training loss: 2.67132830619812
Validation loss: 2.668182429446969

Epoch: 5| Step: 10
Training loss: 3.023547649383545
Validation loss: 2.681582425230293

Epoch: 52| Step: 0
Training loss: 2.6290857791900635
Validation loss: 2.690377660976943

Epoch: 5| Step: 1
Training loss: 2.8889756202697754
Validation loss: 2.693949983965966

Epoch: 5| Step: 2
Training loss: 2.5482349395751953
Validation loss: 2.691307839526925

Epoch: 5| Step: 3
Training loss: 2.926654100418091
Validation loss: 2.6811899472308416

Epoch: 5| Step: 4
Training loss: 2.769693374633789
Validation loss: 2.674460567453856

Epoch: 5| Step: 5
Training loss: 3.3925747871398926
Validation loss: 2.680446591428531

Epoch: 5| Step: 6
Training loss: 3.206624984741211
Validation loss: 2.6845624216141237

Epoch: 5| Step: 7
Training loss: 2.01857590675354
Validation loss: 2.6778346107852076

Epoch: 5| Step: 8
Training loss: 2.877330780029297
Validation loss: 2.6974962090933197

Epoch: 5| Step: 9
Training loss: 2.3444466590881348
Validation loss: 2.69668330941149

Epoch: 5| Step: 10
Training loss: 3.45992374420166
Validation loss: 2.680232583835561

Epoch: 53| Step: 0
Training loss: 3.199197292327881
Validation loss: 2.6602293009399087

Epoch: 5| Step: 1
Training loss: 2.5636587142944336
Validation loss: 2.65753000269654

Epoch: 5| Step: 2
Training loss: 3.1897239685058594
Validation loss: 2.659874346948439

Epoch: 5| Step: 3
Training loss: 2.811635971069336
Validation loss: 2.6582006382685837

Epoch: 5| Step: 4
Training loss: 2.744281053543091
Validation loss: 2.656645113422025

Epoch: 5| Step: 5
Training loss: 3.3271613121032715
Validation loss: 2.6584728353766987

Epoch: 5| Step: 6
Training loss: 1.9661861658096313
Validation loss: 2.65739300686826

Epoch: 5| Step: 7
Training loss: 2.5938448905944824
Validation loss: 2.6575037305073073

Epoch: 5| Step: 8
Training loss: 2.9428298473358154
Validation loss: 2.6573750460019676

Epoch: 5| Step: 9
Training loss: 2.675398826599121
Validation loss: 2.6503656154037802

Epoch: 5| Step: 10
Training loss: 2.908252477645874
Validation loss: 2.652685821697276

Epoch: 54| Step: 0
Training loss: 2.9296011924743652
Validation loss: 2.6534884206710325

Epoch: 5| Step: 1
Training loss: 2.6555428504943848
Validation loss: 2.653600413312194

Epoch: 5| Step: 2
Training loss: 3.087653398513794
Validation loss: 2.6568043488328175

Epoch: 5| Step: 3
Training loss: 2.971804141998291
Validation loss: 2.6605338024836716

Epoch: 5| Step: 4
Training loss: 2.4711086750030518
Validation loss: 2.6585081931083434

Epoch: 5| Step: 5
Training loss: 2.1400859355926514
Validation loss: 2.663208182140063

Epoch: 5| Step: 6
Training loss: 2.791163682937622
Validation loss: 2.671172129210605

Epoch: 5| Step: 7
Training loss: 3.1125149726867676
Validation loss: 2.670425509893766

Epoch: 5| Step: 8
Training loss: 2.0942649841308594
Validation loss: 2.679990647941507

Epoch: 5| Step: 9
Training loss: 3.266782283782959
Validation loss: 2.6805571740673435

Epoch: 5| Step: 10
Training loss: 3.4163665771484375
Validation loss: 2.690238698836296

Epoch: 55| Step: 0
Training loss: 3.5097854137420654
Validation loss: 2.6611910635425198

Epoch: 5| Step: 1
Training loss: 2.5197982788085938
Validation loss: 2.6558569323632026

Epoch: 5| Step: 2
Training loss: 3.2222073078155518
Validation loss: 2.6490021469772502

Epoch: 5| Step: 3
Training loss: 2.7229154109954834
Validation loss: 2.6446429990953013

Epoch: 5| Step: 4
Training loss: 2.6765174865722656
Validation loss: 2.6418447891871133

Epoch: 5| Step: 5
Training loss: 2.656954050064087
Validation loss: 2.647068259536579

Epoch: 5| Step: 6
Training loss: 2.5051331520080566
Validation loss: 2.644847969855032

Epoch: 5| Step: 7
Training loss: 2.018648386001587
Validation loss: 2.6456369276969665

Epoch: 5| Step: 8
Training loss: 2.9941375255584717
Validation loss: 2.649457754627351

Epoch: 5| Step: 9
Training loss: 2.3111677169799805
Validation loss: 2.6450383765723116

Epoch: 5| Step: 10
Training loss: 3.736504316329956
Validation loss: 2.6431498963345765

Epoch: 56| Step: 0
Training loss: 2.8525681495666504
Validation loss: 2.6430722987780007

Epoch: 5| Step: 1
Training loss: 1.9551960229873657
Validation loss: 2.644571360721383

Epoch: 5| Step: 2
Training loss: 2.499385356903076
Validation loss: 2.6418701115474907

Epoch: 5| Step: 3
Training loss: 1.9589217901229858
Validation loss: 2.6455776870891614

Epoch: 5| Step: 4
Training loss: 3.036341667175293
Validation loss: 2.638606873891687

Epoch: 5| Step: 5
Training loss: 2.973524808883667
Validation loss: 2.6406631290271716

Epoch: 5| Step: 6
Training loss: 3.0592868328094482
Validation loss: 2.6390027692241054

Epoch: 5| Step: 7
Training loss: 3.2439186573028564
Validation loss: 2.6368864787522184

Epoch: 5| Step: 8
Training loss: 2.8868167400360107
Validation loss: 2.634389054390692

Epoch: 5| Step: 9
Training loss: 3.2697010040283203
Validation loss: 2.638755206138857

Epoch: 5| Step: 10
Training loss: 3.033893346786499
Validation loss: 2.6382078380994898

Epoch: 57| Step: 0
Training loss: 2.9957611560821533
Validation loss: 2.63504615394018

Epoch: 5| Step: 1
Training loss: 2.669368267059326
Validation loss: 2.635089394866779

Epoch: 5| Step: 2
Training loss: 2.632317543029785
Validation loss: 2.631896342000654

Epoch: 5| Step: 3
Training loss: 3.1586403846740723
Validation loss: 2.6343481027951805

Epoch: 5| Step: 4
Training loss: 2.147433280944824
Validation loss: 2.6390583284439577

Epoch: 5| Step: 5
Training loss: 2.5227394104003906
Validation loss: 2.6316285440998692

Epoch: 5| Step: 6
Training loss: 2.7706286907196045
Validation loss: 2.632477219386767

Epoch: 5| Step: 7
Training loss: 2.3935275077819824
Validation loss: 2.6325009202444427

Epoch: 5| Step: 8
Training loss: 3.1138806343078613
Validation loss: 2.630863392224876

Epoch: 5| Step: 9
Training loss: 3.0828452110290527
Validation loss: 2.6349069687627975

Epoch: 5| Step: 10
Training loss: 3.2357025146484375
Validation loss: 2.631005261534004

Epoch: 58| Step: 0
Training loss: 2.828821897506714
Validation loss: 2.627823729668894

Epoch: 5| Step: 1
Training loss: 3.090646982192993
Validation loss: 2.636474229956186

Epoch: 5| Step: 2
Training loss: 2.269437074661255
Validation loss: 2.6389659143263295

Epoch: 5| Step: 3
Training loss: 2.9784131050109863
Validation loss: 2.6329751911983696

Epoch: 5| Step: 4
Training loss: 2.5473990440368652
Validation loss: 2.640289419440813

Epoch: 5| Step: 5
Training loss: 2.2310738563537598
Validation loss: 2.638167373595699

Epoch: 5| Step: 6
Training loss: 3.249159336090088
Validation loss: 2.6402992253662436

Epoch: 5| Step: 7
Training loss: 3.628711700439453
Validation loss: 2.6415455213157077

Epoch: 5| Step: 8
Training loss: 2.9007790088653564
Validation loss: 2.637571721948603

Epoch: 5| Step: 9
Training loss: 2.137430191040039
Validation loss: 2.637468527722102

Epoch: 5| Step: 10
Training loss: 2.725468158721924
Validation loss: 2.63247327650747

Epoch: 59| Step: 0
Training loss: 2.7338802814483643
Validation loss: 2.6381327593198387

Epoch: 5| Step: 1
Training loss: 2.9818923473358154
Validation loss: 2.6355363579206568

Epoch: 5| Step: 2
Training loss: 2.5431463718414307
Validation loss: 2.632104860839023

Epoch: 5| Step: 3
Training loss: 1.95491623878479
Validation loss: 2.6345035260723484

Epoch: 5| Step: 4
Training loss: 2.823000907897949
Validation loss: 2.6335881371651926

Epoch: 5| Step: 5
Training loss: 3.305504560470581
Validation loss: 2.633329268424742

Epoch: 5| Step: 6
Training loss: 2.7449889183044434
Validation loss: 2.633377516141502

Epoch: 5| Step: 7
Training loss: 3.3504600524902344
Validation loss: 2.634163430942002

Epoch: 5| Step: 8
Training loss: 2.777709722518921
Validation loss: 2.636378167777933

Epoch: 5| Step: 9
Training loss: 2.4142580032348633
Validation loss: 2.6290899707425024

Epoch: 5| Step: 10
Training loss: 3.0436203479766846
Validation loss: 2.626129142699703

Epoch: 60| Step: 0
Training loss: 2.8426010608673096
Validation loss: 2.633665724467206

Epoch: 5| Step: 1
Training loss: 2.0021862983703613
Validation loss: 2.633233690774569

Epoch: 5| Step: 2
Training loss: 3.113865613937378
Validation loss: 2.6350892461756223

Epoch: 5| Step: 3
Training loss: 1.982108473777771
Validation loss: 2.628018048501784

Epoch: 5| Step: 4
Training loss: 2.759289503097534
Validation loss: 2.6262917364797285

Epoch: 5| Step: 5
Training loss: 2.570732355117798
Validation loss: 2.626955937313777

Epoch: 5| Step: 6
Training loss: 2.9211483001708984
Validation loss: 2.623444918663271

Epoch: 5| Step: 7
Training loss: 2.5419976711273193
Validation loss: 2.620554303610197

Epoch: 5| Step: 8
Training loss: 2.746042251586914
Validation loss: 2.6179493063239643

Epoch: 5| Step: 9
Training loss: 3.3935561180114746
Validation loss: 2.619291236323695

Epoch: 5| Step: 10
Training loss: 3.8412039279937744
Validation loss: 2.62174242542636

Epoch: 61| Step: 0
Training loss: 2.7636067867279053
Validation loss: 2.624229118388186

Epoch: 5| Step: 1
Training loss: 2.058600902557373
Validation loss: 2.6218706279672603

Epoch: 5| Step: 2
Training loss: 2.5733189582824707
Validation loss: 2.6227797872276715

Epoch: 5| Step: 3
Training loss: 2.586582899093628
Validation loss: 2.620389797354257

Epoch: 5| Step: 4
Training loss: 2.922210216522217
Validation loss: 2.6162167364551174

Epoch: 5| Step: 5
Training loss: 2.562114715576172
Validation loss: 2.623886751872237

Epoch: 5| Step: 6
Training loss: 2.2712666988372803
Validation loss: 2.6233231611149286

Epoch: 5| Step: 7
Training loss: 3.2513325214385986
Validation loss: 2.623143098687613

Epoch: 5| Step: 8
Training loss: 2.9097697734832764
Validation loss: 2.6155191698381977

Epoch: 5| Step: 9
Training loss: 3.5943222045898438
Validation loss: 2.617526954220187

Epoch: 5| Step: 10
Training loss: 3.0352115631103516
Validation loss: 2.616569785661595

Epoch: 62| Step: 0
Training loss: 2.9862847328186035
Validation loss: 2.6141129770586566

Epoch: 5| Step: 1
Training loss: 2.4878973960876465
Validation loss: 2.614442417698522

Epoch: 5| Step: 2
Training loss: 2.9262824058532715
Validation loss: 2.6148398717244468

Epoch: 5| Step: 3
Training loss: 2.640617847442627
Validation loss: 2.612952752779889

Epoch: 5| Step: 4
Training loss: 2.865323543548584
Validation loss: 2.615024689705141

Epoch: 5| Step: 5
Training loss: 3.3119614124298096
Validation loss: 2.621997653797109

Epoch: 5| Step: 6
Training loss: 2.8377747535705566
Validation loss: 2.624799600211523

Epoch: 5| Step: 7
Training loss: 2.446671962738037
Validation loss: 2.625742120127524

Epoch: 5| Step: 8
Training loss: 3.0585334300994873
Validation loss: 2.634455834665606

Epoch: 5| Step: 9
Training loss: 2.268247365951538
Validation loss: 2.6328972847230974

Epoch: 5| Step: 10
Training loss: 2.697875738143921
Validation loss: 2.624479091295632

Epoch: 63| Step: 0
Training loss: 3.2732555866241455
Validation loss: 2.6215757246940368

Epoch: 5| Step: 1
Training loss: 2.3656883239746094
Validation loss: 2.6139735226990073

Epoch: 5| Step: 2
Training loss: 2.349196434020996
Validation loss: 2.609505481617425

Epoch: 5| Step: 3
Training loss: 2.9100394248962402
Validation loss: 2.6116920030245216

Epoch: 5| Step: 4
Training loss: 2.496258497238159
Validation loss: 2.6114866887369463

Epoch: 5| Step: 5
Training loss: 2.6799464225769043
Validation loss: 2.6174702234165643

Epoch: 5| Step: 6
Training loss: 2.864644765853882
Validation loss: 2.6146530746131815

Epoch: 5| Step: 7
Training loss: 2.6526174545288086
Validation loss: 2.6168946566120272

Epoch: 5| Step: 8
Training loss: 3.0377743244171143
Validation loss: 2.613962488789712

Epoch: 5| Step: 9
Training loss: 3.177666425704956
Validation loss: 2.6108889092681227

Epoch: 5| Step: 10
Training loss: 2.7153024673461914
Validation loss: 2.6036872735587497

Epoch: 64| Step: 0
Training loss: 2.696826934814453
Validation loss: 2.6059880666835333

Epoch: 5| Step: 1
Training loss: 2.644686222076416
Validation loss: 2.6038405279959402

Epoch: 5| Step: 2
Training loss: 2.543342113494873
Validation loss: 2.6073260717494513

Epoch: 5| Step: 3
Training loss: 2.611164093017578
Validation loss: 2.599264911426011

Epoch: 5| Step: 4
Training loss: 3.4679436683654785
Validation loss: 2.600410948517502

Epoch: 5| Step: 5
Training loss: 2.945493221282959
Validation loss: 2.6026084474338

Epoch: 5| Step: 6
Training loss: 2.4683358669281006
Validation loss: 2.602853493023944

Epoch: 5| Step: 7
Training loss: 2.3681061267852783
Validation loss: 2.6017690012531896

Epoch: 5| Step: 8
Training loss: 2.679399013519287
Validation loss: 2.6029153895634476

Epoch: 5| Step: 9
Training loss: 2.7467238903045654
Validation loss: 2.602520014650078

Epoch: 5| Step: 10
Training loss: 3.321718215942383
Validation loss: 2.605179803345793

Epoch: 65| Step: 0
Training loss: 3.1682543754577637
Validation loss: 2.60006784623669

Epoch: 5| Step: 1
Training loss: 2.6284492015838623
Validation loss: 2.5980466591414584

Epoch: 5| Step: 2
Training loss: 3.4637749195098877
Validation loss: 2.5987522986627396

Epoch: 5| Step: 3
Training loss: 2.4783425331115723
Validation loss: 2.5985115779343473

Epoch: 5| Step: 4
Training loss: 2.50533127784729
Validation loss: 2.5998898372855237

Epoch: 5| Step: 5
Training loss: 2.2115540504455566
Validation loss: 2.5992417771329164

Epoch: 5| Step: 6
Training loss: 2.5758464336395264
Validation loss: 2.5986765071909916

Epoch: 5| Step: 7
Training loss: 2.9054863452911377
Validation loss: 2.602648478682323

Epoch: 5| Step: 8
Training loss: 2.4901154041290283
Validation loss: 2.604017401254305

Epoch: 5| Step: 9
Training loss: 2.7395637035369873
Validation loss: 2.608860913143363

Epoch: 5| Step: 10
Training loss: 3.274351119995117
Validation loss: 2.599627523012059

Epoch: 66| Step: 0
Training loss: 3.2661004066467285
Validation loss: 2.5967580477396646

Epoch: 5| Step: 1
Training loss: 3.317295789718628
Validation loss: 2.594128639467301

Epoch: 5| Step: 2
Training loss: 2.975736618041992
Validation loss: 2.5958989563808648

Epoch: 5| Step: 3
Training loss: 2.4899985790252686
Validation loss: 2.5953454394494333

Epoch: 5| Step: 4
Training loss: 3.1938717365264893
Validation loss: 2.5955676083923667

Epoch: 5| Step: 5
Training loss: 2.8807404041290283
Validation loss: 2.5956512240953344

Epoch: 5| Step: 6
Training loss: 2.123745918273926
Validation loss: 2.592458653193648

Epoch: 5| Step: 7
Training loss: 3.1854000091552734
Validation loss: 2.590166127809914

Epoch: 5| Step: 8
Training loss: 2.1567533016204834
Validation loss: 2.5972616390515397

Epoch: 5| Step: 9
Training loss: 2.4732956886291504
Validation loss: 2.5980342357389388

Epoch: 5| Step: 10
Training loss: 2.186638593673706
Validation loss: 2.6003205648032566

Epoch: 67| Step: 0
Training loss: 3.096937656402588
Validation loss: 2.598936193732805

Epoch: 5| Step: 1
Training loss: 2.0590431690216064
Validation loss: 2.6093167130665114

Epoch: 5| Step: 2
Training loss: 3.1066157817840576
Validation loss: 2.602462855718469

Epoch: 5| Step: 3
Training loss: 2.7514777183532715
Validation loss: 2.602708037181567

Epoch: 5| Step: 4
Training loss: 2.546314239501953
Validation loss: 2.600640471263598

Epoch: 5| Step: 5
Training loss: 2.4290308952331543
Validation loss: 2.5936475902475338

Epoch: 5| Step: 6
Training loss: 2.639878511428833
Validation loss: 2.585361385858187

Epoch: 5| Step: 7
Training loss: 3.060319423675537
Validation loss: 2.5862334056567122

Epoch: 5| Step: 8
Training loss: 2.7263293266296387
Validation loss: 2.583653055211549

Epoch: 5| Step: 9
Training loss: 2.6387932300567627
Validation loss: 2.585970691455308

Epoch: 5| Step: 10
Training loss: 3.293558120727539
Validation loss: 2.588037106298631

Epoch: 68| Step: 0
Training loss: 2.404747724533081
Validation loss: 2.5914464637797368

Epoch: 5| Step: 1
Training loss: 2.6631438732147217
Validation loss: 2.588314397360689

Epoch: 5| Step: 2
Training loss: 2.2130744457244873
Validation loss: 2.586361392851799

Epoch: 5| Step: 3
Training loss: 2.5492923259735107
Validation loss: 2.587188611748398

Epoch: 5| Step: 4
Training loss: 3.4274840354919434
Validation loss: 2.5889565560125534

Epoch: 5| Step: 5
Training loss: 2.9201807975769043
Validation loss: 2.586699808797529

Epoch: 5| Step: 6
Training loss: 3.1038639545440674
Validation loss: 2.5873830831179054

Epoch: 5| Step: 7
Training loss: 2.357612133026123
Validation loss: 2.584229815390802

Epoch: 5| Step: 8
Training loss: 2.596787214279175
Validation loss: 2.5828926255626063

Epoch: 5| Step: 9
Training loss: 3.0940377712249756
Validation loss: 2.586173406211279

Epoch: 5| Step: 10
Training loss: 2.946167469024658
Validation loss: 2.5855580709313832

Epoch: 69| Step: 0
Training loss: 2.9318249225616455
Validation loss: 2.5926031143434587

Epoch: 5| Step: 1
Training loss: 2.797152519226074
Validation loss: 2.593422177017376

Epoch: 5| Step: 2
Training loss: 3.232985734939575
Validation loss: 2.5871965244252193

Epoch: 5| Step: 3
Training loss: 3.113245964050293
Validation loss: 2.585916607610641

Epoch: 5| Step: 4
Training loss: 2.7662513256073
Validation loss: 2.5881820571038032

Epoch: 5| Step: 5
Training loss: 2.9278299808502197
Validation loss: 2.5858448525910736

Epoch: 5| Step: 6
Training loss: 2.5917582511901855
Validation loss: 2.5806332326704458

Epoch: 5| Step: 7
Training loss: 3.2468209266662598
Validation loss: 2.5810673211210515

Epoch: 5| Step: 8
Training loss: 2.2459139823913574
Validation loss: 2.5785333161712973

Epoch: 5| Step: 9
Training loss: 2.0984046459198
Validation loss: 2.5780647826451126

Epoch: 5| Step: 10
Training loss: 2.1912853717803955
Validation loss: 2.578012525394399

Epoch: 70| Step: 0
Training loss: 3.189159393310547
Validation loss: 2.577195603360412

Epoch: 5| Step: 1
Training loss: 3.5406227111816406
Validation loss: 2.575673816024616

Epoch: 5| Step: 2
Training loss: 2.249363660812378
Validation loss: 2.5780098156262468

Epoch: 5| Step: 3
Training loss: 2.9324791431427
Validation loss: 2.5804752713890484

Epoch: 5| Step: 4
Training loss: 2.841531276702881
Validation loss: 2.582513032420989

Epoch: 5| Step: 5
Training loss: 2.7143824100494385
Validation loss: 2.5869612668150213

Epoch: 5| Step: 6
Training loss: 3.299351930618286
Validation loss: 2.590210114755938

Epoch: 5| Step: 7
Training loss: 2.3511223793029785
Validation loss: 2.5798230145567205

Epoch: 5| Step: 8
Training loss: 2.285290479660034
Validation loss: 2.5787981069216164

Epoch: 5| Step: 9
Training loss: 2.0914998054504395
Validation loss: 2.5788629824115383

Epoch: 5| Step: 10
Training loss: 2.703916549682617
Validation loss: 2.585232639825472

Epoch: 71| Step: 0
Training loss: 2.861171245574951
Validation loss: 2.5788638463584324

Epoch: 5| Step: 1
Training loss: 3.463341236114502
Validation loss: 2.5797060535800074

Epoch: 5| Step: 2
Training loss: 3.0491867065429688
Validation loss: 2.586795914557672

Epoch: 5| Step: 3
Training loss: 2.680342197418213
Validation loss: 2.5851293122896584

Epoch: 5| Step: 4
Training loss: 2.3214221000671387
Validation loss: 2.5785111945162535

Epoch: 5| Step: 5
Training loss: 2.115516185760498
Validation loss: 2.5749143554318334

Epoch: 5| Step: 6
Training loss: 4.029426574707031
Validation loss: 2.5723429674743326

Epoch: 5| Step: 7
Training loss: 2.9968490600585938
Validation loss: 2.572484331746255

Epoch: 5| Step: 8
Training loss: 1.668744683265686
Validation loss: 2.5943679425024215

Epoch: 5| Step: 9
Training loss: 2.600172758102417
Validation loss: 2.595272097536313

Epoch: 5| Step: 10
Training loss: 2.486072063446045
Validation loss: 2.6066370958923013

Epoch: 72| Step: 0
Training loss: 3.6676220893859863
Validation loss: 2.601026560670586

Epoch: 5| Step: 1
Training loss: 2.7490086555480957
Validation loss: 2.602009721981582

Epoch: 5| Step: 2
Training loss: 2.777324914932251
Validation loss: 2.585832852189259

Epoch: 5| Step: 3
Training loss: 2.717486619949341
Validation loss: 2.5688478344230243

Epoch: 5| Step: 4
Training loss: 2.6239638328552246
Validation loss: 2.56697509109333

Epoch: 5| Step: 5
Training loss: 2.577868700027466
Validation loss: 2.5751350336177374

Epoch: 5| Step: 6
Training loss: 2.438026189804077
Validation loss: 2.571598704143237

Epoch: 5| Step: 7
Training loss: 2.6974830627441406
Validation loss: 2.5752126888562272

Epoch: 5| Step: 8
Training loss: 2.6684329509735107
Validation loss: 2.575463661583521

Epoch: 5| Step: 9
Training loss: 3.232292652130127
Validation loss: 2.5725012876654185

Epoch: 5| Step: 10
Training loss: 2.020426034927368
Validation loss: 2.5718401990911013

Epoch: 73| Step: 0
Training loss: 2.632497787475586
Validation loss: 2.567769378744146

Epoch: 5| Step: 1
Training loss: 2.243196964263916
Validation loss: 2.568981642364174

Epoch: 5| Step: 2
Training loss: 2.339644193649292
Validation loss: 2.567341148212392

Epoch: 5| Step: 3
Training loss: 3.2010302543640137
Validation loss: 2.5717110531304472

Epoch: 5| Step: 4
Training loss: 2.0692856311798096
Validation loss: 2.5733522292106383

Epoch: 5| Step: 5
Training loss: 3.012685775756836
Validation loss: 2.586683178460726

Epoch: 5| Step: 6
Training loss: 3.5943169593811035
Validation loss: 2.5912858670757664

Epoch: 5| Step: 7
Training loss: 2.9407477378845215
Validation loss: 2.5961029170661845

Epoch: 5| Step: 8
Training loss: 2.446176052093506
Validation loss: 2.6048747826648015

Epoch: 5| Step: 9
Training loss: 3.1886775493621826
Validation loss: 2.603089455635317

Epoch: 5| Step: 10
Training loss: 2.4826548099517822
Validation loss: 2.5884949930252565

Epoch: 74| Step: 0
Training loss: 3.5037312507629395
Validation loss: 2.5714409094984814

Epoch: 5| Step: 1
Training loss: 2.410336494445801
Validation loss: 2.5616470383059595

Epoch: 5| Step: 2
Training loss: 2.3981029987335205
Validation loss: 2.5639671125719623

Epoch: 5| Step: 3
Training loss: 3.6047005653381348
Validation loss: 2.5651226505156486

Epoch: 5| Step: 4
Training loss: 2.772209644317627
Validation loss: 2.5661678391118206

Epoch: 5| Step: 5
Training loss: 2.5033018589019775
Validation loss: 2.564229119208551

Epoch: 5| Step: 6
Training loss: 2.2762579917907715
Validation loss: 2.5674388716297765

Epoch: 5| Step: 7
Training loss: 2.579970121383667
Validation loss: 2.5654410649371404

Epoch: 5| Step: 8
Training loss: 2.0058341026306152
Validation loss: 2.562907857279624

Epoch: 5| Step: 9
Training loss: 2.853642225265503
Validation loss: 2.565852883041546

Epoch: 5| Step: 10
Training loss: 3.242244005203247
Validation loss: 2.5612571239471436

Epoch: 75| Step: 0
Training loss: 2.5539541244506836
Validation loss: 2.5595156710634948

Epoch: 5| Step: 1
Training loss: 2.0744967460632324
Validation loss: 2.5585027715211273

Epoch: 5| Step: 2
Training loss: 2.1476545333862305
Validation loss: 2.557734400995316

Epoch: 5| Step: 3
Training loss: 2.26568603515625
Validation loss: 2.55617840315706

Epoch: 5| Step: 4
Training loss: 2.3113818168640137
Validation loss: 2.557274064710063

Epoch: 5| Step: 5
Training loss: 3.254403591156006
Validation loss: 2.5535690220453406

Epoch: 5| Step: 6
Training loss: 3.355930805206299
Validation loss: 2.556638535632882

Epoch: 5| Step: 7
Training loss: 2.9369399547576904
Validation loss: 2.5559885322406726

Epoch: 5| Step: 8
Training loss: 3.5046417713165283
Validation loss: 2.5578120575156262

Epoch: 5| Step: 9
Training loss: 2.9226021766662598
Validation loss: 2.558328884904103

Epoch: 5| Step: 10
Training loss: 2.6618006229400635
Validation loss: 2.564282924898209

Epoch: 76| Step: 0
Training loss: 2.952373504638672
Validation loss: 2.5684204357926563

Epoch: 5| Step: 1
Training loss: 2.757136583328247
Validation loss: 2.578133565123363

Epoch: 5| Step: 2
Training loss: 3.513735294342041
Validation loss: 2.5702793136719735

Epoch: 5| Step: 3
Training loss: 2.786223888397217
Validation loss: 2.5686718135751705

Epoch: 5| Step: 4
Training loss: 3.0344719886779785
Validation loss: 2.5581900688909713

Epoch: 5| Step: 5
Training loss: 2.4531362056732178
Validation loss: 2.553803223435597

Epoch: 5| Step: 6
Training loss: 3.295741319656372
Validation loss: 2.553488031510384

Epoch: 5| Step: 7
Training loss: 1.9868555068969727
Validation loss: 2.5543703776533886

Epoch: 5| Step: 8
Training loss: 1.7965152263641357
Validation loss: 2.557959279706401

Epoch: 5| Step: 9
Training loss: 2.922203540802002
Validation loss: 2.563643668287544

Epoch: 5| Step: 10
Training loss: 2.5764529705047607
Validation loss: 2.5760801787017495

Epoch: 77| Step: 0
Training loss: 2.1149587631225586
Validation loss: 2.570726053689116

Epoch: 5| Step: 1
Training loss: 3.3691298961639404
Validation loss: 2.5558601284539826

Epoch: 5| Step: 2
Training loss: 3.0616161823272705
Validation loss: 2.548963341661679

Epoch: 5| Step: 3
Training loss: 2.901074171066284
Validation loss: 2.55087943999998

Epoch: 5| Step: 4
Training loss: 2.5078976154327393
Validation loss: 2.559822669593237

Epoch: 5| Step: 5
Training loss: 2.4980883598327637
Validation loss: 2.568706517578453

Epoch: 5| Step: 6
Training loss: 2.9946742057800293
Validation loss: 2.5865788152140956

Epoch: 5| Step: 7
Training loss: 2.8344016075134277
Validation loss: 2.57534626735154

Epoch: 5| Step: 8
Training loss: 2.6821160316467285
Validation loss: 2.5709141941480738

Epoch: 5| Step: 9
Training loss: 2.883817434310913
Validation loss: 2.5511883535692768

Epoch: 5| Step: 10
Training loss: 2.383542060852051
Validation loss: 2.5509405802654963

Epoch: 78| Step: 0
Training loss: 2.5753684043884277
Validation loss: 2.5511833544700377

Epoch: 5| Step: 1
Training loss: 2.71468186378479
Validation loss: 2.5471616406594553

Epoch: 5| Step: 2
Training loss: 3.17502498626709
Validation loss: 2.551014789970972

Epoch: 5| Step: 3
Training loss: 2.6866774559020996
Validation loss: 2.5511707182853454

Epoch: 5| Step: 4
Training loss: 2.874598503112793
Validation loss: 2.5517115887775215

Epoch: 5| Step: 5
Training loss: 2.6354117393493652
Validation loss: 2.5511718821781937

Epoch: 5| Step: 6
Training loss: 2.673549175262451
Validation loss: 2.5490837122804377

Epoch: 5| Step: 7
Training loss: 2.885721206665039
Validation loss: 2.548935103160079

Epoch: 5| Step: 8
Training loss: 2.245995044708252
Validation loss: 2.5465017646871586

Epoch: 5| Step: 9
Training loss: 2.7980639934539795
Validation loss: 2.5446505187660136

Epoch: 5| Step: 10
Training loss: 2.724816083908081
Validation loss: 2.5458888392294607

Epoch: 79| Step: 0
Training loss: 2.8647708892822266
Validation loss: 2.548800519717637

Epoch: 5| Step: 1
Training loss: 2.993108034133911
Validation loss: 2.559160409435149

Epoch: 5| Step: 2
Training loss: 2.0602147579193115
Validation loss: 2.5608230560056624

Epoch: 5| Step: 3
Training loss: 3.063030481338501
Validation loss: 2.5832544398564163

Epoch: 5| Step: 4
Training loss: 2.9696571826934814
Validation loss: 2.556960787824405

Epoch: 5| Step: 5
Training loss: 2.6857638359069824
Validation loss: 2.5612966014492895

Epoch: 5| Step: 6
Training loss: 2.1468067169189453
Validation loss: 2.5549678187216482

Epoch: 5| Step: 7
Training loss: 2.6022677421569824
Validation loss: 2.5401752456541984

Epoch: 5| Step: 8
Training loss: 3.204991579055786
Validation loss: 2.5383140220437

Epoch: 5| Step: 9
Training loss: 2.5941689014434814
Validation loss: 2.5389730750873523

Epoch: 5| Step: 10
Training loss: 2.8206021785736084
Validation loss: 2.536766846974691

Epoch: 80| Step: 0
Training loss: 2.4716250896453857
Validation loss: 2.537591095893614

Epoch: 5| Step: 1
Training loss: 3.1468164920806885
Validation loss: 2.5364477685702744

Epoch: 5| Step: 2
Training loss: 2.319484233856201
Validation loss: 2.534207033854659

Epoch: 5| Step: 3
Training loss: 2.456731081008911
Validation loss: 2.5388913821148615

Epoch: 5| Step: 4
Training loss: 2.2889504432678223
Validation loss: 2.5342397869274182

Epoch: 5| Step: 5
Training loss: 2.6590702533721924
Validation loss: 2.5369948802455777

Epoch: 5| Step: 6
Training loss: 3.017941474914551
Validation loss: 2.537542930213354

Epoch: 5| Step: 7
Training loss: 3.004363775253296
Validation loss: 2.531914416179862

Epoch: 5| Step: 8
Training loss: 2.7053465843200684
Validation loss: 2.5351785485462477

Epoch: 5| Step: 9
Training loss: 2.9448046684265137
Validation loss: 2.553332903051889

Epoch: 5| Step: 10
Training loss: 2.8983068466186523
Validation loss: 2.5761210200607136

Epoch: 81| Step: 0
Training loss: 3.0026257038116455
Validation loss: 2.5965787262044926

Epoch: 5| Step: 1
Training loss: 2.9097962379455566
Validation loss: 2.606424303464992

Epoch: 5| Step: 2
Training loss: 2.6961710453033447
Validation loss: 2.612023945777647

Epoch: 5| Step: 3
Training loss: 3.353273868560791
Validation loss: 2.619555632273356

Epoch: 5| Step: 4
Training loss: 2.576817274093628
Validation loss: 2.5802490224120436

Epoch: 5| Step: 5
Training loss: 3.022127866744995
Validation loss: 2.5638652437476703

Epoch: 5| Step: 6
Training loss: 2.147740602493286
Validation loss: 2.53976252771193

Epoch: 5| Step: 7
Training loss: 2.563358783721924
Validation loss: 2.538887041871266

Epoch: 5| Step: 8
Training loss: 2.4988300800323486
Validation loss: 2.5392750155541206

Epoch: 5| Step: 9
Training loss: 2.9596381187438965
Validation loss: 2.54689609876243

Epoch: 5| Step: 10
Training loss: 2.3404223918914795
Validation loss: 2.548105811560026

Epoch: 82| Step: 0
Training loss: 2.3187613487243652
Validation loss: 2.5391263090154177

Epoch: 5| Step: 1
Training loss: 2.8029356002807617
Validation loss: 2.541072941595508

Epoch: 5| Step: 2
Training loss: 2.168640613555908
Validation loss: 2.548739097451651

Epoch: 5| Step: 3
Training loss: 2.9137845039367676
Validation loss: 2.550000677826584

Epoch: 5| Step: 4
Training loss: 2.7012155055999756
Validation loss: 2.5636338315984255

Epoch: 5| Step: 5
Training loss: 3.280341625213623
Validation loss: 2.571161006086616

Epoch: 5| Step: 6
Training loss: 2.735483407974243
Validation loss: 2.5987299462800384

Epoch: 5| Step: 7
Training loss: 2.797926425933838
Validation loss: 2.62273012181764

Epoch: 5| Step: 8
Training loss: 2.9116339683532715
Validation loss: 2.602607932142032

Epoch: 5| Step: 9
Training loss: 2.09710431098938
Validation loss: 2.557235881846438

Epoch: 5| Step: 10
Training loss: 3.3823578357696533
Validation loss: 2.533500994405439

Epoch: 83| Step: 0
Training loss: 2.447713851928711
Validation loss: 2.539423809256605

Epoch: 5| Step: 1
Training loss: 2.7936971187591553
Validation loss: 2.5608937842871553

Epoch: 5| Step: 2
Training loss: 2.189887285232544
Validation loss: 2.5725634841508764

Epoch: 5| Step: 3
Training loss: 2.774244785308838
Validation loss: 2.5958453557824575

Epoch: 5| Step: 4
Training loss: 3.1399893760681152
Validation loss: 2.5810928421635784

Epoch: 5| Step: 5
Training loss: 2.5318496227264404
Validation loss: 2.557426773091798

Epoch: 5| Step: 6
Training loss: 2.591224193572998
Validation loss: 2.5404698207814205

Epoch: 5| Step: 7
Training loss: 3.3606033325195312
Validation loss: 2.5334772986750447

Epoch: 5| Step: 8
Training loss: 2.7957711219787598
Validation loss: 2.5274082435074674

Epoch: 5| Step: 9
Training loss: 2.7001736164093018
Validation loss: 2.5295431562649306

Epoch: 5| Step: 10
Training loss: 2.848576068878174
Validation loss: 2.5302413945556967

Epoch: 84| Step: 0
Training loss: 3.2302680015563965
Validation loss: 2.5453526358450613

Epoch: 5| Step: 1
Training loss: 2.8102569580078125
Validation loss: 2.5543194355503207

Epoch: 5| Step: 2
Training loss: 2.563986301422119
Validation loss: 2.5635368208731375

Epoch: 5| Step: 3
Training loss: 3.657883882522583
Validation loss: 2.57650065165694

Epoch: 5| Step: 4
Training loss: 1.8372608423233032
Validation loss: 2.5672108486134517

Epoch: 5| Step: 5
Training loss: 2.7612318992614746
Validation loss: 2.5572827503245366

Epoch: 5| Step: 6
Training loss: 2.364245653152466
Validation loss: 2.545923912396995

Epoch: 5| Step: 7
Training loss: 2.970757246017456
Validation loss: 2.54407460330635

Epoch: 5| Step: 8
Training loss: 2.3259966373443604
Validation loss: 2.5337728479857087

Epoch: 5| Step: 9
Training loss: 2.712193250656128
Validation loss: 2.5279635229418354

Epoch: 5| Step: 10
Training loss: 2.672189474105835
Validation loss: 2.528954526429535

Epoch: 85| Step: 0
Training loss: 2.704232931137085
Validation loss: 2.530453041035642

Epoch: 5| Step: 1
Training loss: 3.1539993286132812
Validation loss: 2.532197618997225

Epoch: 5| Step: 2
Training loss: 2.8685801029205322
Validation loss: 2.5297622578118437

Epoch: 5| Step: 3
Training loss: 2.7338008880615234
Validation loss: 2.5301168605845463

Epoch: 5| Step: 4
Training loss: 2.513181447982788
Validation loss: 2.5306468061221543

Epoch: 5| Step: 5
Training loss: 2.9765543937683105
Validation loss: 2.5294928166174118

Epoch: 5| Step: 6
Training loss: 2.8660099506378174
Validation loss: 2.528726613649758

Epoch: 5| Step: 7
Training loss: 3.3053619861602783
Validation loss: 2.527609173969556

Epoch: 5| Step: 8
Training loss: 2.079645872116089
Validation loss: 2.524397614181683

Epoch: 5| Step: 9
Training loss: 2.735797882080078
Validation loss: 2.5198803537635395

Epoch: 5| Step: 10
Training loss: 1.8951139450073242
Validation loss: 2.520828652125533

Epoch: 86| Step: 0
Training loss: 2.9317116737365723
Validation loss: 2.5249344328398347

Epoch: 5| Step: 1
Training loss: 2.2185440063476562
Validation loss: 2.5242758412514963

Epoch: 5| Step: 2
Training loss: 2.6106526851654053
Validation loss: 2.532254616419474

Epoch: 5| Step: 3
Training loss: 2.9201266765594482
Validation loss: 2.530464444109189

Epoch: 5| Step: 4
Training loss: 2.401568651199341
Validation loss: 2.5456995374412945

Epoch: 5| Step: 5
Training loss: 2.858076333999634
Validation loss: 2.5575453107075026

Epoch: 5| Step: 6
Training loss: 2.7547497749328613
Validation loss: 2.567855583724155

Epoch: 5| Step: 7
Training loss: 2.34515380859375
Validation loss: 2.565521481216595

Epoch: 5| Step: 8
Training loss: 2.9219677448272705
Validation loss: 2.5457430219137542

Epoch: 5| Step: 9
Training loss: 3.2067883014678955
Validation loss: 2.530386701706917

Epoch: 5| Step: 10
Training loss: 2.6981916427612305
Validation loss: 2.524312473112537

Epoch: 87| Step: 0
Training loss: 2.8955793380737305
Validation loss: 2.515679915746053

Epoch: 5| Step: 1
Training loss: 2.7772412300109863
Validation loss: 2.516725799088837

Epoch: 5| Step: 2
Training loss: 1.9108766317367554
Validation loss: 2.515700163379792

Epoch: 5| Step: 3
Training loss: 2.6843667030334473
Validation loss: 2.5176005158373105

Epoch: 5| Step: 4
Training loss: 2.414801836013794
Validation loss: 2.512601488380022

Epoch: 5| Step: 5
Training loss: 2.9616122245788574
Validation loss: 2.512793489681777

Epoch: 5| Step: 6
Training loss: 2.92242693901062
Validation loss: 2.5131146830897175

Epoch: 5| Step: 7
Training loss: 3.123141288757324
Validation loss: 2.515425702576996

Epoch: 5| Step: 8
Training loss: 2.5024964809417725
Validation loss: 2.512225818890397

Epoch: 5| Step: 9
Training loss: 3.2763161659240723
Validation loss: 2.5129250775101366

Epoch: 5| Step: 10
Training loss: 2.220189332962036
Validation loss: 2.5109521855590162

Epoch: 88| Step: 0
Training loss: 2.4735679626464844
Validation loss: 2.5090414144659556

Epoch: 5| Step: 1
Training loss: 2.9213974475860596
Validation loss: 2.512501662777316

Epoch: 5| Step: 2
Training loss: 3.0575287342071533
Validation loss: 2.5130820863990375

Epoch: 5| Step: 3
Training loss: 2.5876262187957764
Validation loss: 2.510424429370511

Epoch: 5| Step: 4
Training loss: 2.238938808441162
Validation loss: 2.509149323227585

Epoch: 5| Step: 5
Training loss: 3.072197437286377
Validation loss: 2.510856597654281

Epoch: 5| Step: 6
Training loss: 2.8399219512939453
Validation loss: 2.509928180325416

Epoch: 5| Step: 7
Training loss: 3.6285572052001953
Validation loss: 2.508655232767905

Epoch: 5| Step: 8
Training loss: 2.0626778602600098
Validation loss: 2.509885080399052

Epoch: 5| Step: 9
Training loss: 2.819965362548828
Validation loss: 2.51089560344655

Epoch: 5| Step: 10
Training loss: 1.929962158203125
Validation loss: 2.5079771472561743

Epoch: 89| Step: 0
Training loss: 3.791607618331909
Validation loss: 2.5092692093182634

Epoch: 5| Step: 1
Training loss: 2.473241090774536
Validation loss: 2.5020034800293627

Epoch: 5| Step: 2
Training loss: 2.738762378692627
Validation loss: 2.505234915723083

Epoch: 5| Step: 3
Training loss: 2.1385509967803955
Validation loss: 2.4994544188181558

Epoch: 5| Step: 4
Training loss: 2.945998430252075
Validation loss: 2.501588693229101

Epoch: 5| Step: 5
Training loss: 2.747758150100708
Validation loss: 2.5047201405289354

Epoch: 5| Step: 6
Training loss: 2.9494190216064453
Validation loss: 2.502499936729349

Epoch: 5| Step: 7
Training loss: 2.946033239364624
Validation loss: 2.5034985670479397

Epoch: 5| Step: 8
Training loss: 2.1820805072784424
Validation loss: 2.5050762314950266

Epoch: 5| Step: 9
Training loss: 2.5345263481140137
Validation loss: 2.5115768345453406

Epoch: 5| Step: 10
Training loss: 2.1525094509124756
Validation loss: 2.508110753951534

Epoch: 90| Step: 0
Training loss: 2.621410846710205
Validation loss: 2.5088851195509716

Epoch: 5| Step: 1
Training loss: 2.046936511993408
Validation loss: 2.5120306963561685

Epoch: 5| Step: 2
Training loss: 3.3697943687438965
Validation loss: 2.519019498619982

Epoch: 5| Step: 3
Training loss: 2.566553831100464
Validation loss: 2.539639396052207

Epoch: 5| Step: 4
Training loss: 2.8111915588378906
Validation loss: 2.569260440846925

Epoch: 5| Step: 5
Training loss: 2.6052615642547607
Validation loss: 2.585492839095413

Epoch: 5| Step: 6
Training loss: 3.2006449699401855
Validation loss: 2.585579210712064

Epoch: 5| Step: 7
Training loss: 2.1803410053253174
Validation loss: 2.573306909171484

Epoch: 5| Step: 8
Training loss: 3.529712677001953
Validation loss: 2.579144372734972

Epoch: 5| Step: 9
Training loss: 2.389863967895508
Validation loss: 2.5720435932118404

Epoch: 5| Step: 10
Training loss: 2.7411789894104004
Validation loss: 2.5702310710824947

Epoch: 91| Step: 0
Training loss: 2.7601945400238037
Validation loss: 2.5743032040134555

Epoch: 5| Step: 1
Training loss: 2.711477756500244
Validation loss: 2.5654653195411927

Epoch: 5| Step: 2
Training loss: 3.443101167678833
Validation loss: 2.566008347336964

Epoch: 5| Step: 3
Training loss: 2.6854305267333984
Validation loss: 2.5657362245744273

Epoch: 5| Step: 4
Training loss: 3.2522971630096436
Validation loss: 2.5634543921357844

Epoch: 5| Step: 5
Training loss: 2.826779842376709
Validation loss: 2.562094383342292

Epoch: 5| Step: 6
Training loss: 3.020230531692505
Validation loss: 2.563926162258271

Epoch: 5| Step: 7
Training loss: 2.591393232345581
Validation loss: 2.563323566990514

Epoch: 5| Step: 8
Training loss: 1.6986634731292725
Validation loss: 2.5628928933092343

Epoch: 5| Step: 9
Training loss: 2.284566879272461
Validation loss: 2.5624526059755715

Epoch: 5| Step: 10
Training loss: 2.8658182621002197
Validation loss: 2.559095744163759

Epoch: 92| Step: 0
Training loss: 3.0397307872772217
Validation loss: 2.5599194495908675

Epoch: 5| Step: 1
Training loss: 3.2558884620666504
Validation loss: 2.555296310814478

Epoch: 5| Step: 2
Training loss: 2.6876492500305176
Validation loss: 2.557266725006924

Epoch: 5| Step: 3
Training loss: 2.1988582611083984
Validation loss: 2.5552250198138657

Epoch: 5| Step: 4
Training loss: 2.2887396812438965
Validation loss: 2.5600775441815777

Epoch: 5| Step: 5
Training loss: 2.5748729705810547
Validation loss: 2.56585172940326

Epoch: 5| Step: 6
Training loss: 1.9085948467254639
Validation loss: 2.564768619434808

Epoch: 5| Step: 7
Training loss: 3.3367340564727783
Validation loss: 2.5752988374361427

Epoch: 5| Step: 8
Training loss: 2.859051465988159
Validation loss: 2.5789932922650407

Epoch: 5| Step: 9
Training loss: 2.6755950450897217
Validation loss: 2.581779654308032

Epoch: 5| Step: 10
Training loss: 3.3443901538848877
Validation loss: 2.5755397017284105

Epoch: 93| Step: 0
Training loss: 3.151254653930664
Validation loss: 2.5665479321633615

Epoch: 5| Step: 1
Training loss: 3.3725829124450684
Validation loss: 2.5598192035510974

Epoch: 5| Step: 2
Training loss: 2.0154356956481934
Validation loss: 2.5531512665492233

Epoch: 5| Step: 3
Training loss: 2.3529698848724365
Validation loss: 2.5517863073656635

Epoch: 5| Step: 4
Training loss: 3.0629336833953857
Validation loss: 2.5511799243188675

Epoch: 5| Step: 5
Training loss: 3.1257946491241455
Validation loss: 2.5514340913423927

Epoch: 5| Step: 6
Training loss: 2.479590892791748
Validation loss: 2.5504031847882014

Epoch: 5| Step: 7
Training loss: 2.3659520149230957
Validation loss: 2.551305681146601

Epoch: 5| Step: 8
Training loss: 3.063026189804077
Validation loss: 2.5481392927067255

Epoch: 5| Step: 9
Training loss: 2.376800060272217
Validation loss: 2.544775701338245

Epoch: 5| Step: 10
Training loss: 2.588958263397217
Validation loss: 2.536060271724578

Epoch: 94| Step: 0
Training loss: 2.296537399291992
Validation loss: 2.522604362938994

Epoch: 5| Step: 1
Training loss: 1.9142258167266846
Validation loss: 2.5089641822281705

Epoch: 5| Step: 2
Training loss: 2.9469332695007324
Validation loss: 2.502752113085921

Epoch: 5| Step: 3
Training loss: 2.5435988903045654
Validation loss: 2.4917118600619736

Epoch: 5| Step: 4
Training loss: 2.4539668560028076
Validation loss: 2.48957855983447

Epoch: 5| Step: 5
Training loss: 2.269103527069092
Validation loss: 2.486542473557175

Epoch: 5| Step: 6
Training loss: 3.145169258117676
Validation loss: 2.4907139808900896

Epoch: 5| Step: 7
Training loss: 2.934607982635498
Validation loss: 2.494327929712111

Epoch: 5| Step: 8
Training loss: 2.6225745677948
Validation loss: 2.4945003217266453

Epoch: 5| Step: 9
Training loss: 3.4066271781921387
Validation loss: 2.5001905015719834

Epoch: 5| Step: 10
Training loss: 3.2014873027801514
Validation loss: 2.511773722146147

Epoch: 95| Step: 0
Training loss: 2.3809850215911865
Validation loss: 2.5008191395831365

Epoch: 5| Step: 1
Training loss: 3.2678675651550293
Validation loss: 2.4967133332324285

Epoch: 5| Step: 2
Training loss: 2.556694507598877
Validation loss: 2.499211572831677

Epoch: 5| Step: 3
Training loss: 2.7872633934020996
Validation loss: 2.4979769799017135

Epoch: 5| Step: 4
Training loss: 2.4177849292755127
Validation loss: 2.4944405940271195

Epoch: 5| Step: 5
Training loss: 2.7989044189453125
Validation loss: 2.5010409560254825

Epoch: 5| Step: 6
Training loss: 2.1538193225860596
Validation loss: 2.505995350499307

Epoch: 5| Step: 7
Training loss: 2.691592216491699
Validation loss: 2.4972421789682038

Epoch: 5| Step: 8
Training loss: 2.5830721855163574
Validation loss: 2.4994969688436037

Epoch: 5| Step: 9
Training loss: 3.0830178260803223
Validation loss: 2.491051830271239

Epoch: 5| Step: 10
Training loss: 2.83841609954834
Validation loss: 2.4905607367074616

Epoch: 96| Step: 0
Training loss: 2.249293565750122
Validation loss: 2.484237447861702

Epoch: 5| Step: 1
Training loss: 2.65132212638855
Validation loss: 2.4895630574995473

Epoch: 5| Step: 2
Training loss: 2.5313987731933594
Validation loss: 2.4818311378520024

Epoch: 5| Step: 3
Training loss: 2.854961633682251
Validation loss: 2.4799808327869703

Epoch: 5| Step: 4
Training loss: 2.2502360343933105
Validation loss: 2.485287158719955

Epoch: 5| Step: 5
Training loss: 2.4603052139282227
Validation loss: 2.4864505849858767

Epoch: 5| Step: 6
Training loss: 3.7233078479766846
Validation loss: 2.487865771016767

Epoch: 5| Step: 7
Training loss: 2.7278735637664795
Validation loss: 2.487979442842545

Epoch: 5| Step: 8
Training loss: 2.672132968902588
Validation loss: 2.490014371051583

Epoch: 5| Step: 9
Training loss: 3.112422466278076
Validation loss: 2.4863256690322713

Epoch: 5| Step: 10
Training loss: 2.2888011932373047
Validation loss: 2.4980144987824144

Epoch: 97| Step: 0
Training loss: 3.294755458831787
Validation loss: 2.498821032944546

Epoch: 5| Step: 1
Training loss: 3.291858673095703
Validation loss: 2.5224861893602597

Epoch: 5| Step: 2
Training loss: 2.6214873790740967
Validation loss: 2.492331492003574

Epoch: 5| Step: 3
Training loss: 2.304819345474243
Validation loss: 2.489601960746191

Epoch: 5| Step: 4
Training loss: 2.5001580715179443
Validation loss: 2.490350236174881

Epoch: 5| Step: 5
Training loss: 2.2180263996124268
Validation loss: 2.482837741092969

Epoch: 5| Step: 6
Training loss: 2.1996910572052
Validation loss: 2.4807333536045526

Epoch: 5| Step: 7
Training loss: 2.3995022773742676
Validation loss: 2.4783740030821932

Epoch: 5| Step: 8
Training loss: 2.8488566875457764
Validation loss: 2.4826628956743466

Epoch: 5| Step: 9
Training loss: 3.020202159881592
Validation loss: 2.4809229937932824

Epoch: 5| Step: 10
Training loss: 2.878779649734497
Validation loss: 2.4842868671622327

Epoch: 98| Step: 0
Training loss: 2.4263756275177
Validation loss: 2.4811736793928247

Epoch: 5| Step: 1
Training loss: 3.024876356124878
Validation loss: 2.4823832229901384

Epoch: 5| Step: 2
Training loss: 2.820175886154175
Validation loss: 2.479120240416578

Epoch: 5| Step: 3
Training loss: 2.342703104019165
Validation loss: 2.4807772662049983

Epoch: 5| Step: 4
Training loss: 3.6600341796875
Validation loss: 2.483888328716319

Epoch: 5| Step: 5
Training loss: 1.9092075824737549
Validation loss: 2.486695202448035

Epoch: 5| Step: 6
Training loss: 3.4398231506347656
Validation loss: 2.491443164886967

Epoch: 5| Step: 7
Training loss: 2.826540231704712
Validation loss: 2.493096746424193

Epoch: 5| Step: 8
Training loss: 2.2604193687438965
Validation loss: 2.4938910648386967

Epoch: 5| Step: 9
Training loss: 2.42649507522583
Validation loss: 2.4954081094393166

Epoch: 5| Step: 10
Training loss: 2.318694591522217
Validation loss: 2.4989720993144537

Epoch: 99| Step: 0
Training loss: 2.9423165321350098
Validation loss: 2.50899479978828

Epoch: 5| Step: 1
Training loss: 2.0273423194885254
Validation loss: 2.5056022597897436

Epoch: 5| Step: 2
Training loss: 2.0663676261901855
Validation loss: 2.4905874729156494

Epoch: 5| Step: 3
Training loss: 2.446681499481201
Validation loss: 2.5027483099250385

Epoch: 5| Step: 4
Training loss: 2.796393871307373
Validation loss: 2.499541574908841

Epoch: 5| Step: 5
Training loss: 2.138831377029419
Validation loss: 2.502144726373816

Epoch: 5| Step: 6
Training loss: 3.260239362716675
Validation loss: 2.503198972312353

Epoch: 5| Step: 7
Training loss: 2.6213645935058594
Validation loss: 2.496359404697213

Epoch: 5| Step: 8
Training loss: 2.8257651329040527
Validation loss: 2.482900922016431

Epoch: 5| Step: 9
Training loss: 3.3247287273406982
Validation loss: 2.478197628451932

Epoch: 5| Step: 10
Training loss: 3.0763027667999268
Validation loss: 2.4764399425957793

Epoch: 100| Step: 0
Training loss: 2.4879860877990723
Validation loss: 2.4778112544808337

Epoch: 5| Step: 1
Training loss: 2.552198648452759
Validation loss: 2.484834269810748

Epoch: 5| Step: 2
Training loss: 2.7978148460388184
Validation loss: 2.4832503257259244

Epoch: 5| Step: 3
Training loss: 2.7064037322998047
Validation loss: 2.478358204646777

Epoch: 5| Step: 4
Training loss: 2.4409444332122803
Validation loss: 2.47485016751033

Epoch: 5| Step: 5
Training loss: 3.313610792160034
Validation loss: 2.4839438879361717

Epoch: 5| Step: 6
Training loss: 2.905442476272583
Validation loss: 2.508481438441943

Epoch: 5| Step: 7
Training loss: 2.7639479637145996
Validation loss: 2.526179144459386

Epoch: 5| Step: 8
Training loss: 2.3791987895965576
Validation loss: 2.52104200086286

Epoch: 5| Step: 9
Training loss: 2.468453884124756
Validation loss: 2.51940199764826

Epoch: 5| Step: 10
Training loss: 2.8462162017822266
Validation loss: 2.4977948742528118

Epoch: 101| Step: 0
Training loss: 1.8514535427093506
Validation loss: 2.487420005183066

Epoch: 5| Step: 1
Training loss: 2.715790271759033
Validation loss: 2.4846277954757854

Epoch: 5| Step: 2
Training loss: 2.1924386024475098
Validation loss: 2.475575436827957

Epoch: 5| Step: 3
Training loss: 2.3693337440490723
Validation loss: 2.476995719376431

Epoch: 5| Step: 4
Training loss: 2.417656421661377
Validation loss: 2.468413260675246

Epoch: 5| Step: 5
Training loss: 2.909672975540161
Validation loss: 2.4776214361190796

Epoch: 5| Step: 6
Training loss: 2.9862964153289795
Validation loss: 2.4775393675732356

Epoch: 5| Step: 7
Training loss: 3.3804218769073486
Validation loss: 2.4754288247836533

Epoch: 5| Step: 8
Training loss: 2.813969612121582
Validation loss: 2.4706834131671536

Epoch: 5| Step: 9
Training loss: 2.999180793762207
Validation loss: 2.47134147408188

Epoch: 5| Step: 10
Training loss: 2.8615102767944336
Validation loss: 2.465871354585053

Epoch: 102| Step: 0
Training loss: 2.7188620567321777
Validation loss: 2.47284004252444

Epoch: 5| Step: 1
Training loss: 3.486152172088623
Validation loss: 2.4740312253275225

Epoch: 5| Step: 2
Training loss: 2.888601779937744
Validation loss: 2.4873191156694965

Epoch: 5| Step: 3
Training loss: 2.253127336502075
Validation loss: 2.50098584031546

Epoch: 5| Step: 4
Training loss: 2.6406424045562744
Validation loss: 2.50716079947769

Epoch: 5| Step: 5
Training loss: 2.3810019493103027
Validation loss: 2.5029274673872095

Epoch: 5| Step: 6
Training loss: 2.9550554752349854
Validation loss: 2.4922252649902017

Epoch: 5| Step: 7
Training loss: 2.7139201164245605
Validation loss: 2.4883033665277625

Epoch: 5| Step: 8
Training loss: 3.059664249420166
Validation loss: 2.4789316141477196

Epoch: 5| Step: 9
Training loss: 2.165897846221924
Validation loss: 2.4650480208858365

Epoch: 5| Step: 10
Training loss: 2.125720262527466
Validation loss: 2.4599923523523475

Epoch: 103| Step: 0
Training loss: 2.4038753509521484
Validation loss: 2.466040054957072

Epoch: 5| Step: 1
Training loss: 2.8599765300750732
Validation loss: 2.4678996173284387

Epoch: 5| Step: 2
Training loss: 3.253922939300537
Validation loss: 2.4757411967041674

Epoch: 5| Step: 3
Training loss: 2.527186155319214
Validation loss: 2.4779820878018617

Epoch: 5| Step: 4
Training loss: 1.7582311630249023
Validation loss: 2.474106478434737

Epoch: 5| Step: 5
Training loss: 2.575640916824341
Validation loss: 2.4662096859306417

Epoch: 5| Step: 6
Training loss: 2.308337688446045
Validation loss: 2.4625328228037846

Epoch: 5| Step: 7
Training loss: 2.5436930656433105
Validation loss: 2.4575923591531734

Epoch: 5| Step: 8
Training loss: 3.2828586101531982
Validation loss: 2.4593030714219615

Epoch: 5| Step: 9
Training loss: 3.2508673667907715
Validation loss: 2.4550521886476906

Epoch: 5| Step: 10
Training loss: 2.817607879638672
Validation loss: 2.463262350328507

Epoch: 104| Step: 0
Training loss: 2.5281474590301514
Validation loss: 2.464705797933763

Epoch: 5| Step: 1
Training loss: 2.782827615737915
Validation loss: 2.461906228014218

Epoch: 5| Step: 2
Training loss: 2.2955195903778076
Validation loss: 2.4582680963700816

Epoch: 5| Step: 3
Training loss: 2.463087797164917
Validation loss: 2.4565134099734727

Epoch: 5| Step: 4
Training loss: 2.58787202835083
Validation loss: 2.4622605641682944

Epoch: 5| Step: 5
Training loss: 2.999829053878784
Validation loss: 2.4573438987937024

Epoch: 5| Step: 6
Training loss: 2.446349620819092
Validation loss: 2.4616879365777455

Epoch: 5| Step: 7
Training loss: 2.4560134410858154
Validation loss: 2.4740843772888184

Epoch: 5| Step: 8
Training loss: 3.310577392578125
Validation loss: 2.464022638977215

Epoch: 5| Step: 9
Training loss: 2.8183510303497314
Validation loss: 2.4625684574086177

Epoch: 5| Step: 10
Training loss: 2.684799909591675
Validation loss: 2.4588446617126465

Epoch: 105| Step: 0
Training loss: 2.478484630584717
Validation loss: 2.460018104122531

Epoch: 5| Step: 1
Training loss: 2.74105167388916
Validation loss: 2.4638133664284982

Epoch: 5| Step: 2
Training loss: 2.7044434547424316
Validation loss: 2.4665692314024894

Epoch: 5| Step: 3
Training loss: 2.714498519897461
Validation loss: 2.467448962632046

Epoch: 5| Step: 4
Training loss: 2.422708511352539
Validation loss: 2.4624820781010452

Epoch: 5| Step: 5
Training loss: 2.228848695755005
Validation loss: 2.4594215475102907

Epoch: 5| Step: 6
Training loss: 3.28961181640625
Validation loss: 2.458108271321943

Epoch: 5| Step: 7
Training loss: 2.8221962451934814
Validation loss: 2.4584581544322353

Epoch: 5| Step: 8
Training loss: 2.229160785675049
Validation loss: 2.454410404287359

Epoch: 5| Step: 9
Training loss: 3.3603768348693848
Validation loss: 2.4573191545342885

Epoch: 5| Step: 10
Training loss: 2.482799768447876
Validation loss: 2.4541709397428777

Epoch: 106| Step: 0
Training loss: 3.1802215576171875
Validation loss: 2.459072697547174

Epoch: 5| Step: 1
Training loss: 2.782651901245117
Validation loss: 2.4675674758931643

Epoch: 5| Step: 2
Training loss: 2.688222885131836
Validation loss: 2.4759234356623825

Epoch: 5| Step: 3
Training loss: 2.807755947113037
Validation loss: 2.5024229967465965

Epoch: 5| Step: 4
Training loss: 2.3086097240448
Validation loss: 2.4998776810143584

Epoch: 5| Step: 5
Training loss: 2.314072608947754
Validation loss: 2.5197297142397974

Epoch: 5| Step: 6
Training loss: 2.424243927001953
Validation loss: 2.4978724295093166

Epoch: 5| Step: 7
Training loss: 3.001911163330078
Validation loss: 2.4787500366087882

Epoch: 5| Step: 8
Training loss: 1.9223301410675049
Validation loss: 2.462623643618758

Epoch: 5| Step: 9
Training loss: 2.9710564613342285
Validation loss: 2.4603214853553363

Epoch: 5| Step: 10
Training loss: 3.1762173175811768
Validation loss: 2.4539254019337315

Epoch: 107| Step: 0
Training loss: 2.665017604827881
Validation loss: 2.4593348708204044

Epoch: 5| Step: 1
Training loss: 3.3285698890686035
Validation loss: 2.4611164985164518

Epoch: 5| Step: 2
Training loss: 2.2578911781311035
Validation loss: 2.460722520787229

Epoch: 5| Step: 3
Training loss: 2.1991937160491943
Validation loss: 2.4656940762714674

Epoch: 5| Step: 4
Training loss: 3.072010040283203
Validation loss: 2.461185163067233

Epoch: 5| Step: 5
Training loss: 3.0862715244293213
Validation loss: 2.4721103201630297

Epoch: 5| Step: 6
Training loss: 2.606407642364502
Validation loss: 2.46941553649082

Epoch: 5| Step: 7
Training loss: 2.339855909347534
Validation loss: 2.4613366588469474

Epoch: 5| Step: 8
Training loss: 2.9550087451934814
Validation loss: 2.4542621592039704

Epoch: 5| Step: 9
Training loss: 1.7715085744857788
Validation loss: 2.463925487251692

Epoch: 5| Step: 10
Training loss: 3.177117109298706
Validation loss: 2.462028277817593

Epoch: 108| Step: 0
Training loss: 2.625703811645508
Validation loss: 2.4641086004113637

Epoch: 5| Step: 1
Training loss: 3.027818202972412
Validation loss: 2.462329923465688

Epoch: 5| Step: 2
Training loss: 2.7958667278289795
Validation loss: 2.4628542802667104

Epoch: 5| Step: 3
Training loss: 2.7442538738250732
Validation loss: 2.45401523446524

Epoch: 5| Step: 4
Training loss: 2.5615925788879395
Validation loss: 2.465155170809838

Epoch: 5| Step: 5
Training loss: 2.7304959297180176
Validation loss: 2.4623306079577376

Epoch: 5| Step: 6
Training loss: 2.5582854747772217
Validation loss: 2.4697416443978586

Epoch: 5| Step: 7
Training loss: 2.619194507598877
Validation loss: 2.4683419491655085

Epoch: 5| Step: 8
Training loss: 2.323113441467285
Validation loss: 2.4856994459705968

Epoch: 5| Step: 9
Training loss: 3.1336841583251953
Validation loss: 2.487596804095853

Epoch: 5| Step: 10
Training loss: 2.1683788299560547
Validation loss: 2.4824394692656813

Epoch: 109| Step: 0
Training loss: 3.2200183868408203
Validation loss: 2.4793508437372025

Epoch: 5| Step: 1
Training loss: 2.7339987754821777
Validation loss: 2.4635167711524555

Epoch: 5| Step: 2
Training loss: 2.291881799697876
Validation loss: 2.4606382436649774

Epoch: 5| Step: 3
Training loss: 1.9303855895996094
Validation loss: 2.464495256382932

Epoch: 5| Step: 4
Training loss: 2.61822247505188
Validation loss: 2.45807643090525

Epoch: 5| Step: 5
Training loss: 2.9575889110565186
Validation loss: 2.455565006502213

Epoch: 5| Step: 6
Training loss: 3.0157418251037598
Validation loss: 2.451462580311683

Epoch: 5| Step: 7
Training loss: 2.9407479763031006
Validation loss: 2.444292455591181

Epoch: 5| Step: 8
Training loss: 2.3082926273345947
Validation loss: 2.4492598349048245

Epoch: 5| Step: 9
Training loss: 2.627471923828125
Validation loss: 2.4495883808341077

Epoch: 5| Step: 10
Training loss: 2.704038381576538
Validation loss: 2.447892471026349

Epoch: 110| Step: 0
Training loss: 3.1528403759002686
Validation loss: 2.448933470633722

Epoch: 5| Step: 1
Training loss: 2.784411907196045
Validation loss: 2.448385246338383

Epoch: 5| Step: 2
Training loss: 1.8167352676391602
Validation loss: 2.4576502205223165

Epoch: 5| Step: 3
Training loss: 2.5610668659210205
Validation loss: 2.4508404449750016

Epoch: 5| Step: 4
Training loss: 3.144303798675537
Validation loss: 2.447355052476288

Epoch: 5| Step: 5
Training loss: 3.0919647216796875
Validation loss: 2.4443055634857505

Epoch: 5| Step: 6
Training loss: 2.4795477390289307
Validation loss: 2.4439546574828444

Epoch: 5| Step: 7
Training loss: 2.6995041370391846
Validation loss: 2.4430866754183205

Epoch: 5| Step: 8
Training loss: 2.3610024452209473
Validation loss: 2.4456599271425636

Epoch: 5| Step: 9
Training loss: 2.1834938526153564
Validation loss: 2.4436657710741927

Epoch: 5| Step: 10
Training loss: 3.0647222995758057
Validation loss: 2.4548339382294686

Epoch: 111| Step: 0
Training loss: 2.834652900695801
Validation loss: 2.4459795362205914

Epoch: 5| Step: 1
Training loss: 2.951781749725342
Validation loss: 2.4537719629144155

Epoch: 5| Step: 2
Training loss: 2.455691337585449
Validation loss: 2.453828855227399

Epoch: 5| Step: 3
Training loss: 2.702160596847534
Validation loss: 2.4459243256558656

Epoch: 5| Step: 4
Training loss: 2.6853384971618652
Validation loss: 2.45200446087827

Epoch: 5| Step: 5
Training loss: 2.5466561317443848
Validation loss: 2.452598535886375

Epoch: 5| Step: 6
Training loss: 2.3908514976501465
Validation loss: 2.44786258410382

Epoch: 5| Step: 7
Training loss: 2.4207804203033447
Validation loss: 2.4442301463055354

Epoch: 5| Step: 8
Training loss: 3.0138211250305176
Validation loss: 2.4491164966296126

Epoch: 5| Step: 9
Training loss: 2.7384090423583984
Validation loss: 2.4491560894955873

Epoch: 5| Step: 10
Training loss: 2.548640727996826
Validation loss: 2.448600858770391

Epoch: 112| Step: 0
Training loss: 2.826939105987549
Validation loss: 2.4527220136375836

Epoch: 5| Step: 1
Training loss: 2.4086198806762695
Validation loss: 2.451889068849625

Epoch: 5| Step: 2
Training loss: 2.7263171672821045
Validation loss: 2.4562863534496677

Epoch: 5| Step: 3
Training loss: 2.3765921592712402
Validation loss: 2.462081034978231

Epoch: 5| Step: 4
Training loss: 2.726517677307129
Validation loss: 2.456958209314654

Epoch: 5| Step: 5
Training loss: 2.550429105758667
Validation loss: 2.45293685185012

Epoch: 5| Step: 6
Training loss: 2.31152081489563
Validation loss: 2.449260086141607

Epoch: 5| Step: 7
Training loss: 3.0444633960723877
Validation loss: 2.451434866074593

Epoch: 5| Step: 8
Training loss: 2.4669547080993652
Validation loss: 2.449221718695856

Epoch: 5| Step: 9
Training loss: 2.695645570755005
Validation loss: 2.443557631584906

Epoch: 5| Step: 10
Training loss: 3.169806718826294
Validation loss: 2.4443259469924437

Epoch: 113| Step: 0
Training loss: 2.340763568878174
Validation loss: 2.444991739847327

Epoch: 5| Step: 1
Training loss: 2.1346044540405273
Validation loss: 2.442495922888479

Epoch: 5| Step: 2
Training loss: 2.727104902267456
Validation loss: 2.441785899541711

Epoch: 5| Step: 3
Training loss: 2.4419987201690674
Validation loss: 2.4463526241240965

Epoch: 5| Step: 4
Training loss: 3.3886420726776123
Validation loss: 2.4430463108965146

Epoch: 5| Step: 5
Training loss: 2.7700448036193848
Validation loss: 2.4436370313808484

Epoch: 5| Step: 6
Training loss: 2.8419675827026367
Validation loss: 2.4484862973613124

Epoch: 5| Step: 7
Training loss: 3.0095725059509277
Validation loss: 2.446859951942198

Epoch: 5| Step: 8
Training loss: 2.9956793785095215
Validation loss: 2.445765769609841

Epoch: 5| Step: 9
Training loss: 2.350438117980957
Validation loss: 2.447906950468658

Epoch: 5| Step: 10
Training loss: 2.1309735774993896
Validation loss: 2.4425288425978793

Epoch: 114| Step: 0
Training loss: 2.2884912490844727
Validation loss: 2.441883622959096

Epoch: 5| Step: 1
Training loss: 3.0743813514709473
Validation loss: 2.445305583297565

Epoch: 5| Step: 2
Training loss: 2.0972964763641357
Validation loss: 2.4404374040583128

Epoch: 5| Step: 3
Training loss: 2.4647839069366455
Validation loss: 2.442763328552246

Epoch: 5| Step: 4
Training loss: 3.0716190338134766
Validation loss: 2.435945813373853

Epoch: 5| Step: 5
Training loss: 2.7225911617279053
Validation loss: 2.4441845263204267

Epoch: 5| Step: 6
Training loss: 3.0194573402404785
Validation loss: 2.4418397667587444

Epoch: 5| Step: 7
Training loss: 2.4527230262756348
Validation loss: 2.4353032009575957

Epoch: 5| Step: 8
Training loss: 2.0144810676574707
Validation loss: 2.4408520062764487

Epoch: 5| Step: 9
Training loss: 2.962130069732666
Validation loss: 2.4431983219679965

Epoch: 5| Step: 10
Training loss: 3.0047523975372314
Validation loss: 2.441639318261095

Epoch: 115| Step: 0
Training loss: 1.8002262115478516
Validation loss: 2.443870998197986

Epoch: 5| Step: 1
Training loss: 3.1177823543548584
Validation loss: 2.44642646081986

Epoch: 5| Step: 2
Training loss: 2.7780914306640625
Validation loss: 2.4395986962062057

Epoch: 5| Step: 3
Training loss: 1.9516656398773193
Validation loss: 2.443962712441721

Epoch: 5| Step: 4
Training loss: 2.7777884006500244
Validation loss: 2.4370088320906445

Epoch: 5| Step: 5
Training loss: 3.6074821949005127
Validation loss: 2.4418687769161758

Epoch: 5| Step: 6
Training loss: 2.146841287612915
Validation loss: 2.4430948688137915

Epoch: 5| Step: 7
Training loss: 2.7492473125457764
Validation loss: 2.4335080333935317

Epoch: 5| Step: 8
Training loss: 2.837660312652588
Validation loss: 2.4337585254382064

Epoch: 5| Step: 9
Training loss: 2.992360830307007
Validation loss: 2.430469855185478

Epoch: 5| Step: 10
Training loss: 2.3839128017425537
Validation loss: 2.4354278861835437

Epoch: 116| Step: 0
Training loss: 2.958843231201172
Validation loss: 2.432531269647742

Epoch: 5| Step: 1
Training loss: 2.878680944442749
Validation loss: 2.435733097855763

Epoch: 5| Step: 2
Training loss: 2.5499978065490723
Validation loss: 2.433834522001205

Epoch: 5| Step: 3
Training loss: 2.6556286811828613
Validation loss: 2.443966334865939

Epoch: 5| Step: 4
Training loss: 3.0711684226989746
Validation loss: 2.4438933890352965

Epoch: 5| Step: 5
Training loss: 2.6279401779174805
Validation loss: 2.445102507068265

Epoch: 5| Step: 6
Training loss: 2.1196978092193604
Validation loss: 2.4557315303433325

Epoch: 5| Step: 7
Training loss: 2.161186933517456
Validation loss: 2.465143926682011

Epoch: 5| Step: 8
Training loss: 2.574162483215332
Validation loss: 2.4536678996137393

Epoch: 5| Step: 9
Training loss: 3.596277952194214
Validation loss: 2.459272092388522

Epoch: 5| Step: 10
Training loss: 1.8271270990371704
Validation loss: 2.439932302762103

Epoch: 117| Step: 0
Training loss: 2.532294750213623
Validation loss: 2.4408203145509124

Epoch: 5| Step: 1
Training loss: 2.598341464996338
Validation loss: 2.445289255470358

Epoch: 5| Step: 2
Training loss: 2.588473320007324
Validation loss: 2.4398880773975002

Epoch: 5| Step: 3
Training loss: 2.9844932556152344
Validation loss: 2.4440284480330763

Epoch: 5| Step: 4
Training loss: 2.8691470623016357
Validation loss: 2.4470494895853023

Epoch: 5| Step: 5
Training loss: 2.875030994415283
Validation loss: 2.4539969967257593

Epoch: 5| Step: 6
Training loss: 2.216209888458252
Validation loss: 2.4602197062584663

Epoch: 5| Step: 7
Training loss: 2.6922364234924316
Validation loss: 2.461658065037061

Epoch: 5| Step: 8
Training loss: 2.6219515800476074
Validation loss: 2.4625681830990698

Epoch: 5| Step: 9
Training loss: 2.505815029144287
Validation loss: 2.4682498772939048

Epoch: 5| Step: 10
Training loss: 2.6810007095336914
Validation loss: 2.4568091771935903

Epoch: 118| Step: 0
Training loss: 2.9744040966033936
Validation loss: 2.467391934446109

Epoch: 5| Step: 1
Training loss: 2.740138053894043
Validation loss: 2.4649611442319808

Epoch: 5| Step: 2
Training loss: 2.676330327987671
Validation loss: 2.4614111633710962

Epoch: 5| Step: 3
Training loss: 2.8547616004943848
Validation loss: 2.459966754400602

Epoch: 5| Step: 4
Training loss: 2.8307697772979736
Validation loss: 2.471790308593422

Epoch: 5| Step: 5
Training loss: 1.9566236734390259
Validation loss: 2.4552216017118065

Epoch: 5| Step: 6
Training loss: 2.801029682159424
Validation loss: 2.449468530634398

Epoch: 5| Step: 7
Training loss: 2.571598768234253
Validation loss: 2.449294974727015

Epoch: 5| Step: 8
Training loss: 2.6921591758728027
Validation loss: 2.447493208351956

Epoch: 5| Step: 9
Training loss: 2.6461856365203857
Validation loss: 2.4486590149582073

Epoch: 5| Step: 10
Training loss: 2.3735997676849365
Validation loss: 2.4501601906232935

Epoch: 119| Step: 0
Training loss: 1.9870010614395142
Validation loss: 2.444250532375869

Epoch: 5| Step: 1
Training loss: 1.9693094491958618
Validation loss: 2.4511050767796014

Epoch: 5| Step: 2
Training loss: 2.914839744567871
Validation loss: 2.4551282980108775

Epoch: 5| Step: 3
Training loss: 2.8600850105285645
Validation loss: 2.4567196510171376

Epoch: 5| Step: 4
Training loss: 2.8885655403137207
Validation loss: 2.454496891267838

Epoch: 5| Step: 5
Training loss: 2.052826404571533
Validation loss: 2.447271572646274

Epoch: 5| Step: 6
Training loss: 3.080214738845825
Validation loss: 2.439822607142951

Epoch: 5| Step: 7
Training loss: 3.039527416229248
Validation loss: 2.4380388413706133

Epoch: 5| Step: 8
Training loss: 3.059427261352539
Validation loss: 2.4372722000204106

Epoch: 5| Step: 9
Training loss: 2.636352062225342
Validation loss: 2.4330366580717024

Epoch: 5| Step: 10
Training loss: 2.56974196434021
Validation loss: 2.4352756674571703

Epoch: 120| Step: 0
Training loss: 2.961637258529663
Validation loss: 2.44004427489414

Epoch: 5| Step: 1
Training loss: 2.649495840072632
Validation loss: 2.4318928718566895

Epoch: 5| Step: 2
Training loss: 2.6160755157470703
Validation loss: 2.437411605670888

Epoch: 5| Step: 3
Training loss: 2.944007396697998
Validation loss: 2.4424660654478174

Epoch: 5| Step: 4
Training loss: 2.5811188220977783
Validation loss: 2.463167154660789

Epoch: 5| Step: 5
Training loss: 1.9134773015975952
Validation loss: 2.4806320487812

Epoch: 5| Step: 6
Training loss: 3.1657907962799072
Validation loss: 2.489491149943362

Epoch: 5| Step: 7
Training loss: 2.7824056148529053
Validation loss: 2.4779528853713826

Epoch: 5| Step: 8
Training loss: 2.824410915374756
Validation loss: 2.4533121944755636

Epoch: 5| Step: 9
Training loss: 1.9997904300689697
Validation loss: 2.4339002665653022

Epoch: 5| Step: 10
Training loss: 2.7366442680358887
Validation loss: 2.433400754005678

Epoch: 121| Step: 0
Training loss: 2.4012582302093506
Validation loss: 2.424383271125055

Epoch: 5| Step: 1
Training loss: 3.3468925952911377
Validation loss: 2.439104046872867

Epoch: 5| Step: 2
Training loss: 2.3570938110351562
Validation loss: 2.4404501222795054

Epoch: 5| Step: 3
Training loss: 2.546104907989502
Validation loss: 2.4574882497069654

Epoch: 5| Step: 4
Training loss: 2.378648281097412
Validation loss: 2.4594753275635424

Epoch: 5| Step: 5
Training loss: 2.638111114501953
Validation loss: 2.4453578200391544

Epoch: 5| Step: 6
Training loss: 2.7002718448638916
Validation loss: 2.4396839910937893

Epoch: 5| Step: 7
Training loss: 2.714341402053833
Validation loss: 2.4245118659029723

Epoch: 5| Step: 8
Training loss: 2.700906276702881
Validation loss: 2.4247950687203357

Epoch: 5| Step: 9
Training loss: 2.5498008728027344
Validation loss: 2.4227033533075804

Epoch: 5| Step: 10
Training loss: 2.7742574214935303
Validation loss: 2.4250602260712655

Epoch: 122| Step: 0
Training loss: 2.6371922492980957
Validation loss: 2.426658304788733

Epoch: 5| Step: 1
Training loss: 1.6874988079071045
Validation loss: 2.4214705318532963

Epoch: 5| Step: 2
Training loss: 3.251356601715088
Validation loss: 2.4313991761976674

Epoch: 5| Step: 3
Training loss: 2.1546337604522705
Validation loss: 2.4379350395612818

Epoch: 5| Step: 4
Training loss: 3.1826701164245605
Validation loss: 2.4638151097041305

Epoch: 5| Step: 5
Training loss: 2.634427547454834
Validation loss: 2.455242828656268

Epoch: 5| Step: 6
Training loss: 2.8265228271484375
Validation loss: 2.4449603570404874

Epoch: 5| Step: 7
Training loss: 1.728067398071289
Validation loss: 2.4309827255946335

Epoch: 5| Step: 8
Training loss: 3.1695759296417236
Validation loss: 2.419913790559256

Epoch: 5| Step: 9
Training loss: 3.203554630279541
Validation loss: 2.4246904773096882

Epoch: 5| Step: 10
Training loss: 2.6452441215515137
Validation loss: 2.4308853354505313

Epoch: 123| Step: 0
Training loss: 2.876678705215454
Validation loss: 2.4410809291306363

Epoch: 5| Step: 1
Training loss: 2.9823737144470215
Validation loss: 2.444368685445478

Epoch: 5| Step: 2
Training loss: 2.0344748497009277
Validation loss: 2.4462050084144837

Epoch: 5| Step: 3
Training loss: 3.006143569946289
Validation loss: 2.4498211158219205

Epoch: 5| Step: 4
Training loss: 2.218228340148926
Validation loss: 2.4443442129319712

Epoch: 5| Step: 5
Training loss: 3.4809837341308594
Validation loss: 2.430613571597684

Epoch: 5| Step: 6
Training loss: 2.4471898078918457
Validation loss: 2.427157220020089

Epoch: 5| Step: 7
Training loss: 2.2217533588409424
Validation loss: 2.4179167170678415

Epoch: 5| Step: 8
Training loss: 2.5545201301574707
Validation loss: 2.424085973411478

Epoch: 5| Step: 9
Training loss: 3.0268754959106445
Validation loss: 2.433963880744032

Epoch: 5| Step: 10
Training loss: 2.4312779903411865
Validation loss: 2.446049649228332

Epoch: 124| Step: 0
Training loss: 2.570042371749878
Validation loss: 2.462800015685379

Epoch: 5| Step: 1
Training loss: 3.1116714477539062
Validation loss: 2.5024366160874725

Epoch: 5| Step: 2
Training loss: 2.3142342567443848
Validation loss: 2.4854863125790834

Epoch: 5| Step: 3
Training loss: 2.1246650218963623
Validation loss: 2.469807635071457

Epoch: 5| Step: 4
Training loss: 2.1215498447418213
Validation loss: 2.444234768549601

Epoch: 5| Step: 5
Training loss: 2.920783519744873
Validation loss: 2.4309136867523193

Epoch: 5| Step: 6
Training loss: 2.6708431243896484
Validation loss: 2.4177574291024158

Epoch: 5| Step: 7
Training loss: 3.1457443237304688
Validation loss: 2.4168402764105026

Epoch: 5| Step: 8
Training loss: 2.2528891563415527
Validation loss: 2.4120457762031147

Epoch: 5| Step: 9
Training loss: 3.063730001449585
Validation loss: 2.429706629886422

Epoch: 5| Step: 10
Training loss: 2.9305672645568848
Validation loss: 2.4341320401878765

Epoch: 125| Step: 0
Training loss: 2.536005735397339
Validation loss: 2.4362882747445056

Epoch: 5| Step: 1
Training loss: 2.9005331993103027
Validation loss: 2.4327753564362884

Epoch: 5| Step: 2
Training loss: 2.8429198265075684
Validation loss: 2.4226280899458033

Epoch: 5| Step: 3
Training loss: 2.286074161529541
Validation loss: 2.411310721469182

Epoch: 5| Step: 4
Training loss: 2.1927366256713867
Validation loss: 2.4091222132405927

Epoch: 5| Step: 5
Training loss: 2.641064405441284
Validation loss: 2.4094041765377088

Epoch: 5| Step: 6
Training loss: 2.680269718170166
Validation loss: 2.421902379681987

Epoch: 5| Step: 7
Training loss: 2.7486732006073
Validation loss: 2.4439225530111663

Epoch: 5| Step: 8
Training loss: 2.675776481628418
Validation loss: 2.4476526450085383

Epoch: 5| Step: 9
Training loss: 2.9529969692230225
Validation loss: 2.4795568348259054

Epoch: 5| Step: 10
Training loss: 2.7117855548858643
Validation loss: 2.5068312973104496

Epoch: 126| Step: 0
Training loss: 3.43615460395813
Validation loss: 2.4923783284361645

Epoch: 5| Step: 1
Training loss: 1.970999002456665
Validation loss: 2.4750493598240677

Epoch: 5| Step: 2
Training loss: 2.5046610832214355
Validation loss: 2.4464339722869215

Epoch: 5| Step: 3
Training loss: 2.578526020050049
Validation loss: 2.4217863698159494

Epoch: 5| Step: 4
Training loss: 2.3613600730895996
Validation loss: 2.4100118606321272

Epoch: 5| Step: 5
Training loss: 3.1789584159851074
Validation loss: 2.4024558990232405

Epoch: 5| Step: 6
Training loss: 2.6542599201202393
Validation loss: 2.4037190970554145

Epoch: 5| Step: 7
Training loss: 3.1357791423797607
Validation loss: 2.410156680691627

Epoch: 5| Step: 8
Training loss: 2.473295211791992
Validation loss: 2.41162565190305

Epoch: 5| Step: 9
Training loss: 2.2294387817382812
Validation loss: 2.4048689770442184

Epoch: 5| Step: 10
Training loss: 2.576387405395508
Validation loss: 2.414128147145753

Epoch: 127| Step: 0
Training loss: 2.2946181297302246
Validation loss: 2.422636475614322

Epoch: 5| Step: 1
Training loss: 2.553241729736328
Validation loss: 2.425483908704532

Epoch: 5| Step: 2
Training loss: 1.844854712486267
Validation loss: 2.4406942654681463

Epoch: 5| Step: 3
Training loss: 2.746088743209839
Validation loss: 2.44277959997936

Epoch: 5| Step: 4
Training loss: 3.230168581008911
Validation loss: 2.4489613143346642

Epoch: 5| Step: 5
Training loss: 2.151872396469116
Validation loss: 2.43825779679001

Epoch: 5| Step: 6
Training loss: 3.5980429649353027
Validation loss: 2.4497773108943814

Epoch: 5| Step: 7
Training loss: 2.702442169189453
Validation loss: 2.448460812209755

Epoch: 5| Step: 8
Training loss: 2.322810649871826
Validation loss: 2.4358071947610505

Epoch: 5| Step: 9
Training loss: 2.941225051879883
Validation loss: 2.4203333931584514

Epoch: 5| Step: 10
Training loss: 2.6786084175109863
Validation loss: 2.4176544425308064

Epoch: 128| Step: 0
Training loss: 2.367584705352783
Validation loss: 2.412460829622002

Epoch: 5| Step: 1
Training loss: 2.649846076965332
Validation loss: 2.4097474569915445

Epoch: 5| Step: 2
Training loss: 2.9880471229553223
Validation loss: 2.40974259889254

Epoch: 5| Step: 3
Training loss: 2.517573595046997
Validation loss: 2.409652525378812

Epoch: 5| Step: 4
Training loss: 2.3646302223205566
Validation loss: 2.40594321168879

Epoch: 5| Step: 5
Training loss: 2.317333459854126
Validation loss: 2.421324306918729

Epoch: 5| Step: 6
Training loss: 2.9046826362609863
Validation loss: 2.4265821441527335

Epoch: 5| Step: 7
Training loss: 2.4986300468444824
Validation loss: 2.4430304137609338

Epoch: 5| Step: 8
Training loss: 2.593432903289795
Validation loss: 2.451163940532233

Epoch: 5| Step: 9
Training loss: 2.879457712173462
Validation loss: 2.432545003070626

Epoch: 5| Step: 10
Training loss: 2.967130661010742
Validation loss: 2.4076558313062115

Epoch: 129| Step: 0
Training loss: 2.6928598880767822
Validation loss: 2.3980913777505197

Epoch: 5| Step: 1
Training loss: 2.6382393836975098
Validation loss: 2.398917826273108

Epoch: 5| Step: 2
Training loss: 2.8112528324127197
Validation loss: 2.4071951297021683

Epoch: 5| Step: 3
Training loss: 2.8301358222961426
Validation loss: 2.4217743155776814

Epoch: 5| Step: 4
Training loss: 2.6648502349853516
Validation loss: 2.424787062470631

Epoch: 5| Step: 5
Training loss: 2.251272201538086
Validation loss: 2.428187503609606

Epoch: 5| Step: 6
Training loss: 2.869628429412842
Validation loss: 2.4211730828849216

Epoch: 5| Step: 7
Training loss: 2.1619298458099365
Validation loss: 2.4170680712628108

Epoch: 5| Step: 8
Training loss: 2.8813247680664062
Validation loss: 2.412080234096896

Epoch: 5| Step: 9
Training loss: 3.0981147289276123
Validation loss: 2.4006087318543465

Epoch: 5| Step: 10
Training loss: 2.173828125
Validation loss: 2.3961984572872037

Epoch: 130| Step: 0
Training loss: 2.785285234451294
Validation loss: 2.40086038907369

Epoch: 5| Step: 1
Training loss: 3.0552639961242676
Validation loss: 2.413862274539086

Epoch: 5| Step: 2
Training loss: 2.1127676963806152
Validation loss: 2.4209869984657533

Epoch: 5| Step: 3
Training loss: 1.8178037405014038
Validation loss: 2.4376699693741335

Epoch: 5| Step: 4
Training loss: 2.9735376834869385
Validation loss: 2.4384641032065115

Epoch: 5| Step: 5
Training loss: 2.1721858978271484
Validation loss: 2.4460967099794777

Epoch: 5| Step: 6
Training loss: 2.90553617477417
Validation loss: 2.431872388368012

Epoch: 5| Step: 7
Training loss: 2.532879114151001
Validation loss: 2.4155285768611456

Epoch: 5| Step: 8
Training loss: 3.161532402038574
Validation loss: 2.3993664377479145

Epoch: 5| Step: 9
Training loss: 2.480379104614258
Validation loss: 2.3991172698236283

Epoch: 5| Step: 10
Training loss: 3.0310747623443604
Validation loss: 2.3999141928970174

Epoch: 131| Step: 0
Training loss: 2.465055227279663
Validation loss: 2.395593766243227

Epoch: 5| Step: 1
Training loss: 2.6757073402404785
Validation loss: 2.39773569568511

Epoch: 5| Step: 2
Training loss: 2.451253890991211
Validation loss: 2.393024859889861

Epoch: 5| Step: 3
Training loss: 2.8266830444335938
Validation loss: 2.402063515878493

Epoch: 5| Step: 4
Training loss: 2.9228439331054688
Validation loss: 2.411169244397071

Epoch: 5| Step: 5
Training loss: 2.747159242630005
Validation loss: 2.429309506570139

Epoch: 5| Step: 6
Training loss: 2.6664175987243652
Validation loss: 2.431157132630707

Epoch: 5| Step: 7
Training loss: 2.218550443649292
Validation loss: 2.451544648857527

Epoch: 5| Step: 8
Training loss: 1.9467859268188477
Validation loss: 2.434733793299685

Epoch: 5| Step: 9
Training loss: 2.7883615493774414
Validation loss: 2.4367560596876245

Epoch: 5| Step: 10
Training loss: 3.2837562561035156
Validation loss: 2.4432685016303934

Epoch: 132| Step: 0
Training loss: 2.2294249534606934
Validation loss: 2.44586976369222

Epoch: 5| Step: 1
Training loss: 3.559278964996338
Validation loss: 2.4452775960327475

Epoch: 5| Step: 2
Training loss: 2.1015331745147705
Validation loss: 2.447318584688248

Epoch: 5| Step: 3
Training loss: 2.9467968940734863
Validation loss: 2.4517976981337353

Epoch: 5| Step: 4
Training loss: 2.3556811809539795
Validation loss: 2.4519056299681306

Epoch: 5| Step: 5
Training loss: 2.919071912765503
Validation loss: 2.435615021695373

Epoch: 5| Step: 6
Training loss: 3.2340545654296875
Validation loss: 2.418872079541606

Epoch: 5| Step: 7
Training loss: 2.1324574947357178
Validation loss: 2.405217091242472

Epoch: 5| Step: 8
Training loss: 2.3289761543273926
Validation loss: 2.40379826740552

Epoch: 5| Step: 9
Training loss: 2.521237850189209
Validation loss: 2.414614123682822

Epoch: 5| Step: 10
Training loss: 2.6936964988708496
Validation loss: 2.4115272824482252

Epoch: 133| Step: 0
Training loss: 2.57222318649292
Validation loss: 2.412602522039926

Epoch: 5| Step: 1
Training loss: 2.9583308696746826
Validation loss: 2.414877022466352

Epoch: 5| Step: 2
Training loss: 2.3115036487579346
Validation loss: 2.410059226456509

Epoch: 5| Step: 3
Training loss: 2.5799660682678223
Validation loss: 2.403354024374357

Epoch: 5| Step: 4
Training loss: 2.4487805366516113
Validation loss: 2.3990044004173687

Epoch: 5| Step: 5
Training loss: 2.4440627098083496
Validation loss: 2.387482052208275

Epoch: 5| Step: 6
Training loss: 2.829025983810425
Validation loss: 2.3827332142860658

Epoch: 5| Step: 7
Training loss: 2.61942982673645
Validation loss: 2.382046802069551

Epoch: 5| Step: 8
Training loss: 3.393505096435547
Validation loss: 2.388736653071578

Epoch: 5| Step: 9
Training loss: 2.608454465866089
Validation loss: 2.4025929102333645

Epoch: 5| Step: 10
Training loss: 2.2266077995300293
Validation loss: 2.421027703951764

Epoch: 134| Step: 0
Training loss: 2.8982906341552734
Validation loss: 2.4340837514528664

Epoch: 5| Step: 1
Training loss: 2.4854273796081543
Validation loss: 2.439486975310951

Epoch: 5| Step: 2
Training loss: 3.5742790699005127
Validation loss: 2.4555034329814296

Epoch: 5| Step: 3
Training loss: 1.7747970819473267
Validation loss: 2.439285860266737

Epoch: 5| Step: 4
Training loss: 2.111539363861084
Validation loss: 2.4175064897024505

Epoch: 5| Step: 5
Training loss: 2.2650980949401855
Validation loss: 2.4139131704966226

Epoch: 5| Step: 6
Training loss: 2.239337205886841
Validation loss: 2.4201010683531403

Epoch: 5| Step: 7
Training loss: 2.89009690284729
Validation loss: 2.417536545825261

Epoch: 5| Step: 8
Training loss: 2.9231746196746826
Validation loss: 2.402668314595376

Epoch: 5| Step: 9
Training loss: 2.426903486251831
Validation loss: 2.3911678278318016

Epoch: 5| Step: 10
Training loss: 3.395935535430908
Validation loss: 2.3938267974443335

Epoch: 135| Step: 0
Training loss: 2.3055431842803955
Validation loss: 2.3962407240303616

Epoch: 5| Step: 1
Training loss: 2.7695794105529785
Validation loss: 2.4120819735270675

Epoch: 5| Step: 2
Training loss: 2.733868360519409
Validation loss: 2.4155753094662904

Epoch: 5| Step: 3
Training loss: 2.375016689300537
Validation loss: 2.419558651985661

Epoch: 5| Step: 4
Training loss: 2.7091267108917236
Validation loss: 2.417710042768909

Epoch: 5| Step: 5
Training loss: 2.5601227283477783
Validation loss: 2.4119170609340874

Epoch: 5| Step: 6
Training loss: 3.3019537925720215
Validation loss: 2.401924035882437

Epoch: 5| Step: 7
Training loss: 2.662012815475464
Validation loss: 2.398906705200031

Epoch: 5| Step: 8
Training loss: 2.7801804542541504
Validation loss: 2.3892067811822377

Epoch: 5| Step: 9
Training loss: 2.399155616760254
Validation loss: 2.3809504816609044

Epoch: 5| Step: 10
Training loss: 2.549191951751709
Validation loss: 2.3790307557711037

Epoch: 136| Step: 0
Training loss: 2.6574606895446777
Validation loss: 2.398272850180185

Epoch: 5| Step: 1
Training loss: 2.6814839839935303
Validation loss: 2.445615922251055

Epoch: 5| Step: 2
Training loss: 2.5951619148254395
Validation loss: 2.4887343478459183

Epoch: 5| Step: 3
Training loss: 2.104905605316162
Validation loss: 2.526726236907385

Epoch: 5| Step: 4
Training loss: 1.8470356464385986
Validation loss: 2.5292890866597495

Epoch: 5| Step: 5
Training loss: 3.809858798980713
Validation loss: 2.5190752885674916

Epoch: 5| Step: 6
Training loss: 2.8658595085144043
Validation loss: 2.4687988450450282

Epoch: 5| Step: 7
Training loss: 2.825655460357666
Validation loss: 2.419129753625521

Epoch: 5| Step: 8
Training loss: 2.5974183082580566
Validation loss: 2.4043063655976327

Epoch: 5| Step: 9
Training loss: 2.624500274658203
Validation loss: 2.3843238969003

Epoch: 5| Step: 10
Training loss: 2.3317222595214844
Validation loss: 2.3819027869932112

Epoch: 137| Step: 0
Training loss: 2.581663131713867
Validation loss: 2.395184570743192

Epoch: 5| Step: 1
Training loss: 3.3245162963867188
Validation loss: 2.411878780652118

Epoch: 5| Step: 2
Training loss: 2.617614269256592
Validation loss: 2.430095754643922

Epoch: 5| Step: 3
Training loss: 2.077483892440796
Validation loss: 2.4419602078776204

Epoch: 5| Step: 4
Training loss: 2.9786956310272217
Validation loss: 2.450923196731075

Epoch: 5| Step: 5
Training loss: 2.9559760093688965
Validation loss: 2.4340180709797847

Epoch: 5| Step: 6
Training loss: 2.5003910064697266
Validation loss: 2.419345967231258

Epoch: 5| Step: 7
Training loss: 2.758509635925293
Validation loss: 2.407453898460634

Epoch: 5| Step: 8
Training loss: 2.2990307807922363
Validation loss: 2.3937923472414733

Epoch: 5| Step: 9
Training loss: 2.4311928749084473
Validation loss: 2.388481684910354

Epoch: 5| Step: 10
Training loss: 2.9606943130493164
Validation loss: 2.3796350955963135

Epoch: 138| Step: 0
Training loss: 2.552708148956299
Validation loss: 2.377184502540096

Epoch: 5| Step: 1
Training loss: 2.3250339031219482
Validation loss: 2.3867483523584183

Epoch: 5| Step: 2
Training loss: 2.6700031757354736
Validation loss: 2.4195084674384004

Epoch: 5| Step: 3
Training loss: 2.766364336013794
Validation loss: 2.4519564669619323

Epoch: 5| Step: 4
Training loss: 2.1846203804016113
Validation loss: 2.493191488327519

Epoch: 5| Step: 5
Training loss: 3.407391309738159
Validation loss: 2.479516324176583

Epoch: 5| Step: 6
Training loss: 2.846914291381836
Validation loss: 2.430061122422577

Epoch: 5| Step: 7
Training loss: 1.744084119796753
Validation loss: 2.3914441806013866

Epoch: 5| Step: 8
Training loss: 2.7258617877960205
Validation loss: 2.394053746295232

Epoch: 5| Step: 9
Training loss: 2.6008903980255127
Validation loss: 2.3839658075763333

Epoch: 5| Step: 10
Training loss: 3.0931456089019775
Validation loss: 2.379563013712565

Epoch: 139| Step: 0
Training loss: 2.017725944519043
Validation loss: 2.3765481877070602

Epoch: 5| Step: 1
Training loss: 2.920177936553955
Validation loss: 2.3772408987886164

Epoch: 5| Step: 2
Training loss: 2.7193264961242676
Validation loss: 2.3784748918266705

Epoch: 5| Step: 3
Training loss: 2.3259425163269043
Validation loss: 2.3797197726465042

Epoch: 5| Step: 4
Training loss: 2.1467385292053223
Validation loss: 2.3790092340079685

Epoch: 5| Step: 5
Training loss: 3.0061206817626953
Validation loss: 2.378269441666142

Epoch: 5| Step: 6
Training loss: 2.4294333457946777
Validation loss: 2.385743596220529

Epoch: 5| Step: 7
Training loss: 2.6034646034240723
Validation loss: 2.3820979787457373

Epoch: 5| Step: 8
Training loss: 3.1318230628967285
Validation loss: 2.3736377121299825

Epoch: 5| Step: 9
Training loss: 2.338186740875244
Validation loss: 2.3758855327483146

Epoch: 5| Step: 10
Training loss: 3.2249395847320557
Validation loss: 2.3723255408707487

Epoch: 140| Step: 0
Training loss: 2.2471249103546143
Validation loss: 2.3704735002210064

Epoch: 5| Step: 1
Training loss: 2.4838385581970215
Validation loss: 2.369552320049655

Epoch: 5| Step: 2
Training loss: 2.5593996047973633
Validation loss: 2.371334807847136

Epoch: 5| Step: 3
Training loss: 2.5078792572021484
Validation loss: 2.36863830525388

Epoch: 5| Step: 4
Training loss: 3.368328094482422
Validation loss: 2.366353004209457

Epoch: 5| Step: 5
Training loss: 2.545184850692749
Validation loss: 2.3728916952686925

Epoch: 5| Step: 6
Training loss: 2.4538633823394775
Validation loss: 2.3763021961335213

Epoch: 5| Step: 7
Training loss: 2.1890270709991455
Validation loss: 2.380788662100351

Epoch: 5| Step: 8
Training loss: 3.2472450733184814
Validation loss: 2.389550132136191

Epoch: 5| Step: 9
Training loss: 2.2624831199645996
Validation loss: 2.3871396049376457

Epoch: 5| Step: 10
Training loss: 2.863759756088257
Validation loss: 2.396279582413294

Epoch: 141| Step: 0
Training loss: 1.944061517715454
Validation loss: 2.393392265483897

Epoch: 5| Step: 1
Training loss: 2.371273994445801
Validation loss: 2.402946977205174

Epoch: 5| Step: 2
Training loss: 2.6748435497283936
Validation loss: 2.402424123979384

Epoch: 5| Step: 3
Training loss: 2.3222110271453857
Validation loss: 2.3993423061986126

Epoch: 5| Step: 4
Training loss: 2.500457286834717
Validation loss: 2.404616153368386

Epoch: 5| Step: 5
Training loss: 2.9741556644439697
Validation loss: 2.391030365420926

Epoch: 5| Step: 6
Training loss: 3.3176779747009277
Validation loss: 2.3873750035480787

Epoch: 5| Step: 7
Training loss: 2.4792301654815674
Validation loss: 2.379262642193866

Epoch: 5| Step: 8
Training loss: 2.5560691356658936
Validation loss: 2.3757036732089136

Epoch: 5| Step: 9
Training loss: 2.793569803237915
Validation loss: 2.371909764505202

Epoch: 5| Step: 10
Training loss: 2.691338300704956
Validation loss: 2.371416479028681

Epoch: 142| Step: 0
Training loss: 2.1223790645599365
Validation loss: 2.3685058778332126

Epoch: 5| Step: 1
Training loss: 2.1933460235595703
Validation loss: 2.3679740685288624

Epoch: 5| Step: 2
Training loss: 2.6088171005249023
Validation loss: 2.367348490222808

Epoch: 5| Step: 3
Training loss: 2.965427875518799
Validation loss: 2.3728650872425368

Epoch: 5| Step: 4
Training loss: 2.950310230255127
Validation loss: 2.3930239831247637

Epoch: 5| Step: 5
Training loss: 2.774688720703125
Validation loss: 2.398401173212195

Epoch: 5| Step: 6
Training loss: 1.926226258277893
Validation loss: 2.405768726461677

Epoch: 5| Step: 7
Training loss: 2.7732608318328857
Validation loss: 2.403791025120725

Epoch: 5| Step: 8
Training loss: 1.9604336023330688
Validation loss: 2.397367318471273

Epoch: 5| Step: 9
Training loss: 3.1975579261779785
Validation loss: 2.395387024007818

Epoch: 5| Step: 10
Training loss: 3.303750514984131
Validation loss: 2.402577507880426

Epoch: 143| Step: 0
Training loss: 2.576125383377075
Validation loss: 2.399498962586926

Epoch: 5| Step: 1
Training loss: 2.532681703567505
Validation loss: 2.3977278560720463

Epoch: 5| Step: 2
Training loss: 2.3391926288604736
Validation loss: 2.39584776150283

Epoch: 5| Step: 3
Training loss: 2.9616265296936035
Validation loss: 2.3995976063512985

Epoch: 5| Step: 4
Training loss: 2.794858694076538
Validation loss: 2.393524090449015

Epoch: 5| Step: 5
Training loss: 2.576730728149414
Validation loss: 2.3684975754830146

Epoch: 5| Step: 6
Training loss: 2.2694296836853027
Validation loss: 2.365822112688454

Epoch: 5| Step: 7
Training loss: 2.776271343231201
Validation loss: 2.3553740080966743

Epoch: 5| Step: 8
Training loss: 2.5904581546783447
Validation loss: 2.3548155343660744

Epoch: 5| Step: 9
Training loss: 3.2497200965881348
Validation loss: 2.3535446684847594

Epoch: 5| Step: 10
Training loss: 1.8300904035568237
Validation loss: 2.356728640935754

Epoch: 144| Step: 0
Training loss: 2.2863457202911377
Validation loss: 2.3592579672413487

Epoch: 5| Step: 1
Training loss: 3.0102829933166504
Validation loss: 2.355687402909802

Epoch: 5| Step: 2
Training loss: 2.7957496643066406
Validation loss: 2.357791298179216

Epoch: 5| Step: 3
Training loss: 3.055541515350342
Validation loss: 2.3550225868020007

Epoch: 5| Step: 4
Training loss: 2.4256694316864014
Validation loss: 2.3600526253382363

Epoch: 5| Step: 5
Training loss: 1.9967691898345947
Validation loss: 2.36144862123715

Epoch: 5| Step: 6
Training loss: 2.558759927749634
Validation loss: 2.3618234895890757

Epoch: 5| Step: 7
Training loss: 2.982619524002075
Validation loss: 2.357491836752943

Epoch: 5| Step: 8
Training loss: 2.4576945304870605
Validation loss: 2.3721583376648607

Epoch: 5| Step: 9
Training loss: 1.8766285181045532
Validation loss: 2.3671485454805437

Epoch: 5| Step: 10
Training loss: 3.3303728103637695
Validation loss: 2.3773293187541347

Epoch: 145| Step: 0
Training loss: 2.632235288619995
Validation loss: 2.383647240618224

Epoch: 5| Step: 1
Training loss: 3.082331418991089
Validation loss: 2.406338099510439

Epoch: 5| Step: 2
Training loss: 2.4781107902526855
Validation loss: 2.410227055190712

Epoch: 5| Step: 3
Training loss: 2.9577012062072754
Validation loss: 2.3992100261872813

Epoch: 5| Step: 4
Training loss: 2.2145771980285645
Validation loss: 2.3905389385838665

Epoch: 5| Step: 5
Training loss: 2.675379514694214
Validation loss: 2.3959389732730005

Epoch: 5| Step: 6
Training loss: 2.019766330718994
Validation loss: 2.378033063744986

Epoch: 5| Step: 7
Training loss: 1.91061532497406
Validation loss: 2.370969988966501

Epoch: 5| Step: 8
Training loss: 2.9640839099884033
Validation loss: 2.3765825469006776

Epoch: 5| Step: 9
Training loss: 3.0425972938537598
Validation loss: 2.364160606938024

Epoch: 5| Step: 10
Training loss: 2.6233365535736084
Validation loss: 2.3652742370482414

Epoch: 146| Step: 0
Training loss: 2.7549502849578857
Validation loss: 2.3601102239342144

Epoch: 5| Step: 1
Training loss: 1.9977741241455078
Validation loss: 2.3650325241909234

Epoch: 5| Step: 2
Training loss: 2.5561985969543457
Validation loss: 2.367415097451979

Epoch: 5| Step: 3
Training loss: 2.045771360397339
Validation loss: 2.3562306101604173

Epoch: 5| Step: 4
Training loss: 2.830561399459839
Validation loss: 2.365993412592078

Epoch: 5| Step: 5
Training loss: 2.8146305084228516
Validation loss: 2.3708850952886764

Epoch: 5| Step: 6
Training loss: 2.3829476833343506
Validation loss: 2.3942707174567768

Epoch: 5| Step: 7
Training loss: 3.1665468215942383
Validation loss: 2.4247479105508454

Epoch: 5| Step: 8
Training loss: 1.935988187789917
Validation loss: 2.4272341061663885

Epoch: 5| Step: 9
Training loss: 3.058563709259033
Validation loss: 2.426909872280654

Epoch: 5| Step: 10
Training loss: 3.2299656867980957
Validation loss: 2.3976138048274542

Epoch: 147| Step: 0
Training loss: 2.7942352294921875
Validation loss: 2.371699853609967

Epoch: 5| Step: 1
Training loss: 2.1376290321350098
Validation loss: 2.3652607599894204

Epoch: 5| Step: 2
Training loss: 3.3004391193389893
Validation loss: 2.3514079252878823

Epoch: 5| Step: 3
Training loss: 2.9582464694976807
Validation loss: 2.345570492488082

Epoch: 5| Step: 4
Training loss: 2.165876626968384
Validation loss: 2.3474814366268855

Epoch: 5| Step: 5
Training loss: 2.538797378540039
Validation loss: 2.3512091405930056

Epoch: 5| Step: 6
Training loss: 2.191310167312622
Validation loss: 2.351551684000159

Epoch: 5| Step: 7
Training loss: 2.2322089672088623
Validation loss: 2.351546149100027

Epoch: 5| Step: 8
Training loss: 2.8919358253479004
Validation loss: 2.3520771149666078

Epoch: 5| Step: 9
Training loss: 2.8433680534362793
Validation loss: 2.362963076560728

Epoch: 5| Step: 10
Training loss: 2.496028184890747
Validation loss: 2.3734886851362003

Epoch: 148| Step: 0
Training loss: 2.613112688064575
Validation loss: 2.387431654878842

Epoch: 5| Step: 1
Training loss: 2.292339563369751
Validation loss: 2.3912980735942884

Epoch: 5| Step: 2
Training loss: 2.5514378547668457
Validation loss: 2.3908780826035367

Epoch: 5| Step: 3
Training loss: 2.6017661094665527
Validation loss: 2.3987704118092856

Epoch: 5| Step: 4
Training loss: 3.1112189292907715
Validation loss: 2.402587867552234

Epoch: 5| Step: 5
Training loss: 2.5270187854766846
Validation loss: 2.404220243935944

Epoch: 5| Step: 6
Training loss: 2.7354042530059814
Validation loss: 2.3913314970590736

Epoch: 5| Step: 7
Training loss: 2.2850444316864014
Validation loss: 2.3812307798734276

Epoch: 5| Step: 8
Training loss: 2.248730421066284
Validation loss: 2.370108324994323

Epoch: 5| Step: 9
Training loss: 3.4045886993408203
Validation loss: 2.3797128379985852

Epoch: 5| Step: 10
Training loss: 2.134456157684326
Validation loss: 2.3743821677341255

Epoch: 149| Step: 0
Training loss: 2.4416332244873047
Validation loss: 2.3705038178351616

Epoch: 5| Step: 1
Training loss: 2.5803394317626953
Validation loss: 2.3774206535790556

Epoch: 5| Step: 2
Training loss: 2.9576756954193115
Validation loss: 2.370434617483488

Epoch: 5| Step: 3
Training loss: 2.237545967102051
Validation loss: 2.3739135239713933

Epoch: 5| Step: 4
Training loss: 2.6621928215026855
Validation loss: 2.368349362445134

Epoch: 5| Step: 5
Training loss: 2.847180128097534
Validation loss: 2.3619877830628426

Epoch: 5| Step: 6
Training loss: 2.3480820655822754
Validation loss: 2.374092689124487

Epoch: 5| Step: 7
Training loss: 2.8986997604370117
Validation loss: 2.3670645888133715

Epoch: 5| Step: 8
Training loss: 2.223475217819214
Validation loss: 2.3694481721488376

Epoch: 5| Step: 9
Training loss: 2.162177324295044
Validation loss: 2.363543454036918

Epoch: 5| Step: 10
Training loss: 3.1396079063415527
Validation loss: 2.3752306199842885

Epoch: 150| Step: 0
Training loss: 2.932554006576538
Validation loss: 2.3846568369096324

Epoch: 5| Step: 1
Training loss: 2.7413783073425293
Validation loss: 2.3788200655291156

Epoch: 5| Step: 2
Training loss: 2.4311585426330566
Validation loss: 2.358005956936908

Epoch: 5| Step: 3
Training loss: 3.0335304737091064
Validation loss: 2.3469469021725398

Epoch: 5| Step: 4
Training loss: 2.4743199348449707
Validation loss: 2.3372926071125972

Epoch: 5| Step: 5
Training loss: 2.233935594558716
Validation loss: 2.3391209135773363

Epoch: 5| Step: 6
Training loss: 2.4528696537017822
Validation loss: 2.3343676956751014

Epoch: 5| Step: 7
Training loss: 3.5828616619110107
Validation loss: 2.343647482574627

Epoch: 5| Step: 8
Training loss: 2.334702491760254
Validation loss: 2.342722410796791

Epoch: 5| Step: 9
Training loss: 2.4282288551330566
Validation loss: 2.3379259981134886

Epoch: 5| Step: 10
Training loss: 1.7537089586257935
Validation loss: 2.343777125881564

Epoch: 151| Step: 0
Training loss: 2.8645429611206055
Validation loss: 2.341960299399591

Epoch: 5| Step: 1
Training loss: 2.7356724739074707
Validation loss: 2.354438151082685

Epoch: 5| Step: 2
Training loss: 3.0363059043884277
Validation loss: 2.354995689084453

Epoch: 5| Step: 3
Training loss: 2.709311008453369
Validation loss: 2.3634034997673443

Epoch: 5| Step: 4
Training loss: 2.555835247039795
Validation loss: 2.367739797920309

Epoch: 5| Step: 5
Training loss: 2.690455198287964
Validation loss: 2.368299432980117

Epoch: 5| Step: 6
Training loss: 2.4001190662384033
Validation loss: 2.3668105038263465

Epoch: 5| Step: 7
Training loss: 2.111693859100342
Validation loss: 2.3819133491926294

Epoch: 5| Step: 8
Training loss: 2.103787660598755
Validation loss: 2.3652522205024638

Epoch: 5| Step: 9
Training loss: 2.40421986579895
Validation loss: 2.369261516037808

Epoch: 5| Step: 10
Training loss: 2.9212634563446045
Validation loss: 2.3844948584033596

Epoch: 152| Step: 0
Training loss: 2.3080434799194336
Validation loss: 2.3612790158999863

Epoch: 5| Step: 1
Training loss: 3.0004019737243652
Validation loss: 2.34899390128351

Epoch: 5| Step: 2
Training loss: 2.675664186477661
Validation loss: 2.3411515297428256

Epoch: 5| Step: 3
Training loss: 2.5876986980438232
Validation loss: 2.3485399958907918

Epoch: 5| Step: 4
Training loss: 3.0125606060028076
Validation loss: 2.3495230931107716

Epoch: 5| Step: 5
Training loss: 2.753572940826416
Validation loss: 2.3444728107862574

Epoch: 5| Step: 6
Training loss: 1.7389593124389648
Validation loss: 2.3440518148483767

Epoch: 5| Step: 7
Training loss: 2.4834017753601074
Validation loss: 2.3403134474190335

Epoch: 5| Step: 8
Training loss: 2.1754062175750732
Validation loss: 2.341449858039938

Epoch: 5| Step: 9
Training loss: 2.929811477661133
Validation loss: 2.3420918372369584

Epoch: 5| Step: 10
Training loss: 2.729177713394165
Validation loss: 2.341452631899106

Epoch: 153| Step: 0
Training loss: 2.587014675140381
Validation loss: 2.3449744178402807

Epoch: 5| Step: 1
Training loss: 2.8235535621643066
Validation loss: 2.3459256233707553

Epoch: 5| Step: 2
Training loss: 3.010193347930908
Validation loss: 2.3558484328690397

Epoch: 5| Step: 3
Training loss: 2.791621446609497
Validation loss: 2.3613939028914257

Epoch: 5| Step: 4
Training loss: 1.8744481801986694
Validation loss: 2.3685679897185294

Epoch: 5| Step: 5
Training loss: 1.9872545003890991
Validation loss: 2.3682145828841836

Epoch: 5| Step: 6
Training loss: 1.9412953853607178
Validation loss: 2.366356183123845

Epoch: 5| Step: 7
Training loss: 2.5069468021392822
Validation loss: 2.401441281841647

Epoch: 5| Step: 8
Training loss: 2.9952473640441895
Validation loss: 2.4487666878648984

Epoch: 5| Step: 9
Training loss: 2.639692783355713
Validation loss: 2.446880348267094

Epoch: 5| Step: 10
Training loss: 3.4126977920532227
Validation loss: 2.4061204643659693

Epoch: 154| Step: 0
Training loss: 2.494765520095825
Validation loss: 2.3987356334604244

Epoch: 5| Step: 1
Training loss: 2.2639012336730957
Validation loss: 2.3739125715788973

Epoch: 5| Step: 2
Training loss: 2.6503076553344727
Validation loss: 2.3506660871608283

Epoch: 5| Step: 3
Training loss: 2.6970953941345215
Validation loss: 2.336563698707088

Epoch: 5| Step: 4
Training loss: 3.2563509941101074
Validation loss: 2.3416032355318785

Epoch: 5| Step: 5
Training loss: 2.901918888092041
Validation loss: 2.3453426437993206

Epoch: 5| Step: 6
Training loss: 1.78140127658844
Validation loss: 2.3565157331446165

Epoch: 5| Step: 7
Training loss: 2.216188430786133
Validation loss: 2.3607179093104538

Epoch: 5| Step: 8
Training loss: 2.6953012943267822
Validation loss: 2.358541060519475

Epoch: 5| Step: 9
Training loss: 2.8489418029785156
Validation loss: 2.3697037876293225

Epoch: 5| Step: 10
Training loss: 2.920231819152832
Validation loss: 2.361852438219132

Epoch: 155| Step: 0
Training loss: 2.996908664703369
Validation loss: 2.3637208579688944

Epoch: 5| Step: 1
Training loss: 2.3517119884490967
Validation loss: 2.3706289260618147

Epoch: 5| Step: 2
Training loss: 2.7310166358947754
Validation loss: 2.3591436493781304

Epoch: 5| Step: 3
Training loss: 2.60479998588562
Validation loss: 2.3660427690834127

Epoch: 5| Step: 4
Training loss: 2.6427321434020996
Validation loss: 2.3949902032011297

Epoch: 5| Step: 5
Training loss: 2.7598633766174316
Validation loss: 2.4033336229221796

Epoch: 5| Step: 6
Training loss: 2.4408087730407715
Validation loss: 2.4256905227579098

Epoch: 5| Step: 7
Training loss: 2.6410069465637207
Validation loss: 2.4437743207459808

Epoch: 5| Step: 8
Training loss: 2.655730962753296
Validation loss: 2.4671519674280638

Epoch: 5| Step: 9
Training loss: 2.987290859222412
Validation loss: 2.4551531114885883

Epoch: 5| Step: 10
Training loss: 1.9329519271850586
Validation loss: 2.448367880236718

Epoch: 156| Step: 0
Training loss: 3.2794413566589355
Validation loss: 2.440109340093469

Epoch: 5| Step: 1
Training loss: 2.8446340560913086
Validation loss: 2.4149859361751105

Epoch: 5| Step: 2
Training loss: 2.7858362197875977
Validation loss: 2.396944917658324

Epoch: 5| Step: 3
Training loss: 2.644196033477783
Validation loss: 2.363686333420456

Epoch: 5| Step: 4
Training loss: 2.775820255279541
Validation loss: 2.3652628980657107

Epoch: 5| Step: 5
Training loss: 2.3294849395751953
Validation loss: 2.354644380589967

Epoch: 5| Step: 6
Training loss: 2.4795174598693848
Validation loss: 2.3379515165923745

Epoch: 5| Step: 7
Training loss: 2.0425777435302734
Validation loss: 2.3372535218474684

Epoch: 5| Step: 8
Training loss: 2.472372531890869
Validation loss: 2.3383854640427457

Epoch: 5| Step: 9
Training loss: 1.7287921905517578
Validation loss: 2.3349925087344263

Epoch: 5| Step: 10
Training loss: 3.088623523712158
Validation loss: 2.344571821151241

Epoch: 157| Step: 0
Training loss: 2.7455270290374756
Validation loss: 2.360200671739476

Epoch: 5| Step: 1
Training loss: 2.2292633056640625
Validation loss: 2.3763980070749917

Epoch: 5| Step: 2
Training loss: 2.681138515472412
Validation loss: 2.37251861633793

Epoch: 5| Step: 3
Training loss: 2.6352486610412598
Validation loss: 2.356068993127474

Epoch: 5| Step: 4
Training loss: 2.9788002967834473
Validation loss: 2.342259171188519

Epoch: 5| Step: 5
Training loss: 2.413675308227539
Validation loss: 2.329787495315716

Epoch: 5| Step: 6
Training loss: 2.3617606163024902
Validation loss: 2.323266854850195

Epoch: 5| Step: 7
Training loss: 1.9060287475585938
Validation loss: 2.333166904346917

Epoch: 5| Step: 8
Training loss: 2.3603546619415283
Validation loss: 2.323649752524591

Epoch: 5| Step: 9
Training loss: 3.556899309158325
Validation loss: 2.324468158906506

Epoch: 5| Step: 10
Training loss: 2.4110684394836426
Validation loss: 2.3245384821327786

Epoch: 158| Step: 0
Training loss: 2.892416477203369
Validation loss: 2.323890442489296

Epoch: 5| Step: 1
Training loss: 2.0478713512420654
Validation loss: 2.323165726918046

Epoch: 5| Step: 2
Training loss: 2.587432384490967
Validation loss: 2.333088385161533

Epoch: 5| Step: 3
Training loss: 2.587665557861328
Validation loss: 2.3263907483828965

Epoch: 5| Step: 4
Training loss: 2.484905958175659
Validation loss: 2.3305819547304543

Epoch: 5| Step: 5
Training loss: 2.1688718795776367
Validation loss: 2.337651465528755

Epoch: 5| Step: 6
Training loss: 2.6692185401916504
Validation loss: 2.3496646086374917

Epoch: 5| Step: 7
Training loss: 3.1820578575134277
Validation loss: 2.3578697789099907

Epoch: 5| Step: 8
Training loss: 2.890685558319092
Validation loss: 2.3591826449158373

Epoch: 5| Step: 9
Training loss: 2.509476661682129
Validation loss: 2.34809850364603

Epoch: 5| Step: 10
Training loss: 2.218003988265991
Validation loss: 2.3585063898435203

Epoch: 159| Step: 0
Training loss: 2.098114490509033
Validation loss: 2.3433615751163934

Epoch: 5| Step: 1
Training loss: 2.190741539001465
Validation loss: 2.3501028066040366

Epoch: 5| Step: 2
Training loss: 2.5989348888397217
Validation loss: 2.3533090186375443

Epoch: 5| Step: 3
Training loss: 2.945504665374756
Validation loss: 2.3653842915770826

Epoch: 5| Step: 4
Training loss: 2.712631940841675
Validation loss: 2.3528544672073854

Epoch: 5| Step: 5
Training loss: 1.8717342615127563
Validation loss: 2.3544866269634617

Epoch: 5| Step: 6
Training loss: 2.5810763835906982
Validation loss: 2.3623344436768563

Epoch: 5| Step: 7
Training loss: 2.748243808746338
Validation loss: 2.351369479651092

Epoch: 5| Step: 8
Training loss: 2.766486644744873
Validation loss: 2.337110170754053

Epoch: 5| Step: 9
Training loss: 2.2452552318573
Validation loss: 2.3229694161363827

Epoch: 5| Step: 10
Training loss: 3.6348376274108887
Validation loss: 2.3174918825908373

Epoch: 160| Step: 0
Training loss: 2.6311135292053223
Validation loss: 2.3206258255948304

Epoch: 5| Step: 1
Training loss: 2.369696855545044
Validation loss: 2.3187728799799436

Epoch: 5| Step: 2
Training loss: 2.3452935218811035
Validation loss: 2.3165744504620953

Epoch: 5| Step: 3
Training loss: 2.9198057651519775
Validation loss: 2.313242691819386

Epoch: 5| Step: 4
Training loss: 2.837463617324829
Validation loss: 2.317667540683541

Epoch: 5| Step: 5
Training loss: 2.5415821075439453
Validation loss: 2.3150721596133326

Epoch: 5| Step: 6
Training loss: 2.8148460388183594
Validation loss: 2.309967041015625

Epoch: 5| Step: 7
Training loss: 2.2179172039031982
Validation loss: 2.302858696188978

Epoch: 5| Step: 8
Training loss: 2.7354483604431152
Validation loss: 2.311538129724482

Epoch: 5| Step: 9
Training loss: 2.3343191146850586
Validation loss: 2.311793329895184

Epoch: 5| Step: 10
Training loss: 2.535659074783325
Validation loss: 2.3365802072709605

Epoch: 161| Step: 0
Training loss: 2.3661999702453613
Validation loss: 2.37333333364097

Epoch: 5| Step: 1
Training loss: 2.3585524559020996
Validation loss: 2.4246335491057365

Epoch: 5| Step: 2
Training loss: 2.5748746395111084
Validation loss: 2.436476174221244

Epoch: 5| Step: 3
Training loss: 3.000159502029419
Validation loss: 2.4268067600906535

Epoch: 5| Step: 4
Training loss: 2.5155415534973145
Validation loss: 2.382979369932605

Epoch: 5| Step: 5
Training loss: 2.4953370094299316
Validation loss: 2.3329805404909196

Epoch: 5| Step: 6
Training loss: 2.8551878929138184
Validation loss: 2.319235496623542

Epoch: 5| Step: 7
Training loss: 2.5670809745788574
Validation loss: 2.307356067883071

Epoch: 5| Step: 8
Training loss: 3.080784559249878
Validation loss: 2.308649937311808

Epoch: 5| Step: 9
Training loss: 2.1859707832336426
Validation loss: 2.310729278031216

Epoch: 5| Step: 10
Training loss: 2.4214553833007812
Validation loss: 2.318640888378184

Epoch: 162| Step: 0
Training loss: 3.0095646381378174
Validation loss: 2.322234292184153

Epoch: 5| Step: 1
Training loss: 2.7775628566741943
Validation loss: 2.326123529864896

Epoch: 5| Step: 2
Training loss: 3.008106231689453
Validation loss: 2.318669867771928

Epoch: 5| Step: 3
Training loss: 2.4475574493408203
Validation loss: 2.3201282819112143

Epoch: 5| Step: 4
Training loss: 2.868055820465088
Validation loss: 2.3257315927936184

Epoch: 5| Step: 5
Training loss: 2.4730091094970703
Validation loss: 2.324480241344821

Epoch: 5| Step: 6
Training loss: 2.041358470916748
Validation loss: 2.3230003605606737

Epoch: 5| Step: 7
Training loss: 2.5952956676483154
Validation loss: 2.3279615679094867

Epoch: 5| Step: 8
Training loss: 2.309898614883423
Validation loss: 2.325402077808175

Epoch: 5| Step: 9
Training loss: 2.52614688873291
Validation loss: 2.331805209959707

Epoch: 5| Step: 10
Training loss: 2.214937925338745
Validation loss: 2.3379903813844085

Epoch: 163| Step: 0
Training loss: 2.1486079692840576
Validation loss: 2.341050996575304

Epoch: 5| Step: 1
Training loss: 2.681135654449463
Validation loss: 2.3672675009696715

Epoch: 5| Step: 2
Training loss: 2.4929020404815674
Validation loss: 2.3740540832601567

Epoch: 5| Step: 3
Training loss: 2.8100502490997314
Validation loss: 2.3784455407050347

Epoch: 5| Step: 4
Training loss: 2.2007510662078857
Validation loss: 2.387014430056336

Epoch: 5| Step: 5
Training loss: 2.7052550315856934
Validation loss: 2.3930333455403647

Epoch: 5| Step: 6
Training loss: 2.806906223297119
Validation loss: 2.40440611685476

Epoch: 5| Step: 7
Training loss: 2.545151710510254
Validation loss: 2.41363771500126

Epoch: 5| Step: 8
Training loss: 2.377037525177002
Validation loss: 2.4092909161762526

Epoch: 5| Step: 9
Training loss: 2.5483367443084717
Validation loss: 2.394764542579651

Epoch: 5| Step: 10
Training loss: 3.364316463470459
Validation loss: 2.3849465821378972

Epoch: 164| Step: 0
Training loss: 2.6927237510681152
Validation loss: 2.3805008190934376

Epoch: 5| Step: 1
Training loss: 2.238936424255371
Validation loss: 2.372597461105675

Epoch: 5| Step: 2
Training loss: 3.1079518795013428
Validation loss: 2.3708039534989225

Epoch: 5| Step: 3
Training loss: 2.3668787479400635
Validation loss: 2.3634734051201933

Epoch: 5| Step: 4
Training loss: 2.66981840133667
Validation loss: 2.367013003236504

Epoch: 5| Step: 5
Training loss: 2.592945098876953
Validation loss: 2.369532920980966

Epoch: 5| Step: 6
Training loss: 3.1149163246154785
Validation loss: 2.3688059852969263

Epoch: 5| Step: 7
Training loss: 2.275364637374878
Validation loss: 2.3702220352747108

Epoch: 5| Step: 8
Training loss: 2.7466962337493896
Validation loss: 2.3679351960459063

Epoch: 5| Step: 9
Training loss: 2.317225933074951
Validation loss: 2.365527952871015

Epoch: 5| Step: 10
Training loss: 2.454977512359619
Validation loss: 2.365682832656368

Epoch: 165| Step: 0
Training loss: 2.6959071159362793
Validation loss: 2.362397360545333

Epoch: 5| Step: 1
Training loss: 1.9458017349243164
Validation loss: 2.376923922569521

Epoch: 5| Step: 2
Training loss: 2.3355631828308105
Validation loss: 2.38622530557776

Epoch: 5| Step: 3
Training loss: 2.588996171951294
Validation loss: 2.3976224468600367

Epoch: 5| Step: 4
Training loss: 2.6975245475769043
Validation loss: 2.388169098925847

Epoch: 5| Step: 5
Training loss: 2.343640089035034
Validation loss: 2.3967702183672177

Epoch: 5| Step: 6
Training loss: 2.872973918914795
Validation loss: 2.3878963634531987

Epoch: 5| Step: 7
Training loss: 2.3613014221191406
Validation loss: 2.3844049438353507

Epoch: 5| Step: 8
Training loss: 2.719968795776367
Validation loss: 2.3839452292329524

Epoch: 5| Step: 9
Training loss: 3.339109420776367
Validation loss: 2.3853667474562124

Epoch: 5| Step: 10
Training loss: 2.624974489212036
Validation loss: 2.392382373091995

Epoch: 166| Step: 0
Training loss: 2.5669796466827393
Validation loss: 2.397990411327731

Epoch: 5| Step: 1
Training loss: 2.4543862342834473
Validation loss: 2.3883973629243913

Epoch: 5| Step: 2
Training loss: 2.7493982315063477
Validation loss: 2.3865981281444593

Epoch: 5| Step: 3
Training loss: 3.1823277473449707
Validation loss: 2.38566412976993

Epoch: 5| Step: 4
Training loss: 2.7911057472229004
Validation loss: 2.3825516085470877

Epoch: 5| Step: 5
Training loss: 2.1180472373962402
Validation loss: 2.370055070487402

Epoch: 5| Step: 6
Training loss: 2.363063335418701
Validation loss: 2.3746392496170534

Epoch: 5| Step: 7
Training loss: 2.8399882316589355
Validation loss: 2.3622264221150386

Epoch: 5| Step: 8
Training loss: 2.8790342807769775
Validation loss: 2.3548780820702993

Epoch: 5| Step: 9
Training loss: 2.271981954574585
Validation loss: 2.341907757584767

Epoch: 5| Step: 10
Training loss: 2.019773006439209
Validation loss: 2.344197952619163

Epoch: 167| Step: 0
Training loss: 2.2843387126922607
Validation loss: 2.3357930414138304

Epoch: 5| Step: 1
Training loss: 2.837576150894165
Validation loss: 2.3430464754822435

Epoch: 5| Step: 2
Training loss: 2.856799364089966
Validation loss: 2.3367174158814135

Epoch: 5| Step: 3
Training loss: 1.7683117389678955
Validation loss: 2.3306412004655406

Epoch: 5| Step: 4
Training loss: 2.534461259841919
Validation loss: 2.3231192916952152

Epoch: 5| Step: 5
Training loss: 2.951134204864502
Validation loss: 2.332411612233808

Epoch: 5| Step: 6
Training loss: 1.9384533166885376
Validation loss: 2.329382142712993

Epoch: 5| Step: 7
Training loss: 2.6096675395965576
Validation loss: 2.313857888662687

Epoch: 5| Step: 8
Training loss: 2.944103956222534
Validation loss: 2.326788909973637

Epoch: 5| Step: 9
Training loss: 2.9345879554748535
Validation loss: 2.3200726688549085

Epoch: 5| Step: 10
Training loss: 2.2457492351531982
Validation loss: 2.330985028256652

Epoch: 168| Step: 0
Training loss: 2.673548460006714
Validation loss: 2.330608529429282

Epoch: 5| Step: 1
Training loss: 2.5497841835021973
Validation loss: 2.3278770062231247

Epoch: 5| Step: 2
Training loss: 2.922032117843628
Validation loss: 2.3190258984924643

Epoch: 5| Step: 3
Training loss: 2.619478940963745
Validation loss: 2.309694654198103

Epoch: 5| Step: 4
Training loss: 2.7019593715667725
Validation loss: 2.2886183441326184

Epoch: 5| Step: 5
Training loss: 1.7656357288360596
Validation loss: 2.2942089675575175

Epoch: 5| Step: 6
Training loss: 2.075042724609375
Validation loss: 2.2961364228238343

Epoch: 5| Step: 7
Training loss: 2.187581777572632
Validation loss: 2.296122024136205

Epoch: 5| Step: 8
Training loss: 2.8189258575439453
Validation loss: 2.309908110608337

Epoch: 5| Step: 9
Training loss: 3.060342788696289
Validation loss: 2.322480352975989

Epoch: 5| Step: 10
Training loss: 2.7001447677612305
Validation loss: 2.328062921442011

Epoch: 169| Step: 0
Training loss: 3.302030563354492
Validation loss: 2.3384674569611907

Epoch: 5| Step: 1
Training loss: 1.9385830163955688
Validation loss: 2.3285337442992837

Epoch: 5| Step: 2
Training loss: 2.6571044921875
Validation loss: 2.3271455123860347

Epoch: 5| Step: 3
Training loss: 1.8133331537246704
Validation loss: 2.3294753643774215

Epoch: 5| Step: 4
Training loss: 3.0144872665405273
Validation loss: 2.313024241437194

Epoch: 5| Step: 5
Training loss: 2.6868298053741455
Validation loss: 2.2996057489866852

Epoch: 5| Step: 6
Training loss: 2.412065029144287
Validation loss: 2.285810165507819

Epoch: 5| Step: 7
Training loss: 2.727591037750244
Validation loss: 2.2741473951647357

Epoch: 5| Step: 8
Training loss: 2.1486058235168457
Validation loss: 2.2765886219598914

Epoch: 5| Step: 9
Training loss: 2.8068158626556396
Validation loss: 2.271324203860375

Epoch: 5| Step: 10
Training loss: 2.4793806076049805
Validation loss: 2.269593459303661

Epoch: 170| Step: 0
Training loss: 1.5630970001220703
Validation loss: 2.274163423045989

Epoch: 5| Step: 1
Training loss: 2.624784469604492
Validation loss: 2.280739943186442

Epoch: 5| Step: 2
Training loss: 2.2447690963745117
Validation loss: 2.27571593048752

Epoch: 5| Step: 3
Training loss: 2.0657811164855957
Validation loss: 2.2683227908226753

Epoch: 5| Step: 4
Training loss: 2.52974009513855
Validation loss: 2.268149729697935

Epoch: 5| Step: 5
Training loss: 3.0513570308685303
Validation loss: 2.267573397646668

Epoch: 5| Step: 6
Training loss: 3.0683202743530273
Validation loss: 2.2754949510738416

Epoch: 5| Step: 7
Training loss: 2.9580795764923096
Validation loss: 2.286288102467855

Epoch: 5| Step: 8
Training loss: 3.293027877807617
Validation loss: 2.2959158907654467

Epoch: 5| Step: 9
Training loss: 2.4538180828094482
Validation loss: 2.31465753047697

Epoch: 5| Step: 10
Training loss: 2.099130392074585
Validation loss: 2.363617463778424

Epoch: 171| Step: 0
Training loss: 1.7165195941925049
Validation loss: 2.386434273053241

Epoch: 5| Step: 1
Training loss: 2.8274879455566406
Validation loss: 2.39431522738549

Epoch: 5| Step: 2
Training loss: 2.889157772064209
Validation loss: 2.400276122554656

Epoch: 5| Step: 3
Training loss: 2.1862294673919678
Validation loss: 2.3622069281916462

Epoch: 5| Step: 4
Training loss: 2.1638927459716797
Validation loss: 2.309712653519005

Epoch: 5| Step: 5
Training loss: 2.971540927886963
Validation loss: 2.27961310263603

Epoch: 5| Step: 6
Training loss: 1.9916326999664307
Validation loss: 2.275554959492017

Epoch: 5| Step: 7
Training loss: 2.7908902168273926
Validation loss: 2.2850152779650945

Epoch: 5| Step: 8
Training loss: 3.3203701972961426
Validation loss: 2.2916113868836434

Epoch: 5| Step: 9
Training loss: 2.8001818656921387
Validation loss: 2.3032745956092753

Epoch: 5| Step: 10
Training loss: 2.6529598236083984
Validation loss: 2.3086668111944713

Epoch: 172| Step: 0
Training loss: 2.613252878189087
Validation loss: 2.312058574409895

Epoch: 5| Step: 1
Training loss: 2.697409152984619
Validation loss: 2.321188162731868

Epoch: 5| Step: 2
Training loss: 2.26138973236084
Validation loss: 2.3159109802656275

Epoch: 5| Step: 3
Training loss: 2.178452968597412
Validation loss: 2.3099320114299817

Epoch: 5| Step: 4
Training loss: 3.201824188232422
Validation loss: 2.3017827208324144

Epoch: 5| Step: 5
Training loss: 2.3105592727661133
Validation loss: 2.3038122474506335

Epoch: 5| Step: 6
Training loss: 2.927140951156616
Validation loss: 2.290165296164892

Epoch: 5| Step: 7
Training loss: 3.056530475616455
Validation loss: 2.2853120898687713

Epoch: 5| Step: 8
Training loss: 2.3111634254455566
Validation loss: 2.2846053108092277

Epoch: 5| Step: 9
Training loss: 2.329439401626587
Validation loss: 2.279347394102363

Epoch: 5| Step: 10
Training loss: 2.521652936935425
Validation loss: 2.268242105360954

Epoch: 173| Step: 0
Training loss: 3.142878293991089
Validation loss: 2.2756569026618876

Epoch: 5| Step: 1
Training loss: 1.8427536487579346
Validation loss: 2.283852913046396

Epoch: 5| Step: 2
Training loss: 2.8258705139160156
Validation loss: 2.300447948517338

Epoch: 5| Step: 3
Training loss: 2.5718436241149902
Validation loss: 2.3139643681946622

Epoch: 5| Step: 4
Training loss: 2.6638875007629395
Validation loss: 2.3455102956423195

Epoch: 5| Step: 5
Training loss: 3.2123138904571533
Validation loss: 2.356200707856045

Epoch: 5| Step: 6
Training loss: 3.105041980743408
Validation loss: 2.3526901352790093

Epoch: 5| Step: 7
Training loss: 2.0902888774871826
Validation loss: 2.3697222919874292

Epoch: 5| Step: 8
Training loss: 2.556561231613159
Validation loss: 2.340229183114985

Epoch: 5| Step: 9
Training loss: 2.365678071975708
Validation loss: 2.3340372231698807

Epoch: 5| Step: 10
Training loss: 1.5201692581176758
Validation loss: 2.3176002143531718

Epoch: 174| Step: 0
Training loss: 2.7139687538146973
Validation loss: 2.296881998738935

Epoch: 5| Step: 1
Training loss: 2.6693484783172607
Validation loss: 2.285054865703788

Epoch: 5| Step: 2
Training loss: 2.3184046745300293
Validation loss: 2.2667114760286067

Epoch: 5| Step: 3
Training loss: 2.8775558471679688
Validation loss: 2.2628761004376154

Epoch: 5| Step: 4
Training loss: 3.2031829357147217
Validation loss: 2.261378954815608

Epoch: 5| Step: 5
Training loss: 2.686765670776367
Validation loss: 2.2618414727590417

Epoch: 5| Step: 6
Training loss: 2.0753166675567627
Validation loss: 2.2650918294024724

Epoch: 5| Step: 7
Training loss: 2.4849610328674316
Validation loss: 2.2673736746593187

Epoch: 5| Step: 8
Training loss: 1.952127456665039
Validation loss: 2.2679388958920716

Epoch: 5| Step: 9
Training loss: 2.019219398498535
Validation loss: 2.271105807314637

Epoch: 5| Step: 10
Training loss: 3.0999369621276855
Validation loss: 2.279522890685707

Epoch: 175| Step: 0
Training loss: 2.639164924621582
Validation loss: 2.285111681107552

Epoch: 5| Step: 1
Training loss: 2.502037763595581
Validation loss: 2.283052590585524

Epoch: 5| Step: 2
Training loss: 2.4961438179016113
Validation loss: 2.2891419882415445

Epoch: 5| Step: 3
Training loss: 3.194507360458374
Validation loss: 2.288668547907183

Epoch: 5| Step: 4
Training loss: 2.912442445755005
Validation loss: 2.282726482678485

Epoch: 5| Step: 5
Training loss: 2.305436611175537
Validation loss: 2.2871489678659747

Epoch: 5| Step: 6
Training loss: 2.460777997970581
Validation loss: 2.2786274699754614

Epoch: 5| Step: 7
Training loss: 2.139101505279541
Validation loss: 2.2805412994918

Epoch: 5| Step: 8
Training loss: 2.4829318523406982
Validation loss: 2.2825108958828833

Epoch: 5| Step: 9
Training loss: 2.5011491775512695
Validation loss: 2.2674683255534016

Epoch: 5| Step: 10
Training loss: 2.33693265914917
Validation loss: 2.2710631483344623

Testing loss: 2.4674988322787814

Epoch: 1| Step: 0
Training loss: 5.851865304769584
Validation loss: 5.84676028710035

Epoch: 6| Step: 1
Training loss: 5.972228309963386
Validation loss: 5.840189812396693

Epoch: 6| Step: 2
Training loss: 5.561671163122554
Validation loss: 5.834398368729198

Epoch: 6| Step: 3
Training loss: 5.796373535840508
Validation loss: 5.827832230109432

Epoch: 6| Step: 4
Training loss: 6.875836408194705
Validation loss: 5.821737259764665

Epoch: 6| Step: 5
Training loss: 5.873642460239848
Validation loss: 5.815709038341958

Epoch: 6| Step: 6
Training loss: 4.820112672493985
Validation loss: 5.809183016483577

Epoch: 6| Step: 7
Training loss: 6.439233046644912
Validation loss: 5.80233222349685

Epoch: 6| Step: 8
Training loss: 5.143496375244975
Validation loss: 5.79604269830731

Epoch: 6| Step: 9
Training loss: 5.216218254309046
Validation loss: 5.788784760481627

Epoch: 6| Step: 10
Training loss: 6.112445654754259
Validation loss: 5.7815102192440015

Epoch: 6| Step: 11
Training loss: 6.113465327699854
Validation loss: 5.774368744987568

Epoch: 6| Step: 12
Training loss: 5.746188019705977
Validation loss: 5.7669283161960285

Epoch: 6| Step: 13
Training loss: 5.98418312661562
Validation loss: 5.758652444411757

Epoch: 2| Step: 0
Training loss: 6.491155328881147
Validation loss: 5.750896542685539

Epoch: 6| Step: 1
Training loss: 5.73733485696855
Validation loss: 5.741128191643732

Epoch: 6| Step: 2
Training loss: 5.825573073590459
Validation loss: 5.73156415457842

Epoch: 6| Step: 3
Training loss: 6.732828409632514
Validation loss: 5.722301387959387

Epoch: 6| Step: 4
Training loss: 5.910244675685165
Validation loss: 5.712716522999609

Epoch: 6| Step: 5
Training loss: 6.208032622084491
Validation loss: 5.7018676324725845

Epoch: 6| Step: 6
Training loss: 4.778224411623216
Validation loss: 5.690634055885386

Epoch: 6| Step: 7
Training loss: 5.902958997550111
Validation loss: 5.678697606235623

Epoch: 6| Step: 8
Training loss: 5.450396098170584
Validation loss: 5.66699293211938

Epoch: 6| Step: 9
Training loss: 5.590304032521432
Validation loss: 5.654274186973954

Epoch: 6| Step: 10
Training loss: 5.360361080817975
Validation loss: 5.642238593055248

Epoch: 6| Step: 11
Training loss: 5.763964737031802
Validation loss: 5.62843846242257

Epoch: 6| Step: 12
Training loss: 5.3176142590527435
Validation loss: 5.614055228433334

Epoch: 6| Step: 13
Training loss: 3.617774602647534
Validation loss: 5.599671690116863

Epoch: 3| Step: 0
Training loss: 4.697558787629211
Validation loss: 5.583788197403817

Epoch: 6| Step: 1
Training loss: 6.421275015691855
Validation loss: 5.569652623278124

Epoch: 6| Step: 2
Training loss: 5.800950584123551
Validation loss: 5.553009939703201

Epoch: 6| Step: 3
Training loss: 5.117536925987225
Validation loss: 5.534874632893373

Epoch: 6| Step: 4
Training loss: 4.126163665321284
Validation loss: 5.517987617428978

Epoch: 6| Step: 5
Training loss: 5.044496335917101
Validation loss: 5.500551994046529

Epoch: 6| Step: 6
Training loss: 6.140899574411538
Validation loss: 5.482541687780355

Epoch: 6| Step: 7
Training loss: 5.0431771445855365
Validation loss: 5.462624981948148

Epoch: 6| Step: 8
Training loss: 6.280095691802265
Validation loss: 5.441862278922046

Epoch: 6| Step: 9
Training loss: 5.6806774090063685
Validation loss: 5.421634059828243

Epoch: 6| Step: 10
Training loss: 5.471999069715722
Validation loss: 5.40089816717849

Epoch: 6| Step: 11
Training loss: 6.1201806863278785
Validation loss: 5.37856935242035

Epoch: 6| Step: 12
Training loss: 5.188520583905337
Validation loss: 5.356530928188567

Epoch: 6| Step: 13
Training loss: 5.44134630990407
Validation loss: 5.333142732219002

Epoch: 4| Step: 0
Training loss: 5.095773119608391
Validation loss: 5.308322216847876

Epoch: 6| Step: 1
Training loss: 4.33200219078396
Validation loss: 5.284826593898852

Epoch: 6| Step: 2
Training loss: 5.207670591786092
Validation loss: 5.262369535093268

Epoch: 6| Step: 3
Training loss: 5.172318453951469
Validation loss: 5.2373819597758455

Epoch: 6| Step: 4
Training loss: 4.287830947248006
Validation loss: 5.2137226630896185

Epoch: 6| Step: 5
Training loss: 5.277909525961334
Validation loss: 5.187801294513354

Epoch: 6| Step: 6
Training loss: 4.954560369702972
Validation loss: 5.164653077402705

Epoch: 6| Step: 7
Training loss: 5.094846642519381
Validation loss: 5.1382704311192215

Epoch: 6| Step: 8
Training loss: 5.946647905112009
Validation loss: 5.113353017458487

Epoch: 6| Step: 9
Training loss: 5.001006597284589
Validation loss: 5.086335098615635

Epoch: 6| Step: 10
Training loss: 5.753140794192571
Validation loss: 5.060108299946266

Epoch: 6| Step: 11
Training loss: 5.346314289026673
Validation loss: 5.033998760011984

Epoch: 6| Step: 12
Training loss: 5.3370840870960965
Validation loss: 5.006274963941821

Epoch: 6| Step: 13
Training loss: 6.040727669543454
Validation loss: 4.977630459947099

Epoch: 5| Step: 0
Training loss: 5.403266392143531
Validation loss: 4.950296675367146

Epoch: 6| Step: 1
Training loss: 6.084737363114054
Validation loss: 4.923254801162542

Epoch: 6| Step: 2
Training loss: 4.960150228194191
Validation loss: 4.895990940019124

Epoch: 6| Step: 3
Training loss: 5.444936118123365
Validation loss: 4.865592187733989

Epoch: 6| Step: 4
Training loss: 4.774959240484783
Validation loss: 4.839992820804254

Epoch: 6| Step: 5
Training loss: 4.250584281700557
Validation loss: 4.813688811362096

Epoch: 6| Step: 6
Training loss: 4.293088263947181
Validation loss: 4.785461730514848

Epoch: 6| Step: 7
Training loss: 5.717134841713972
Validation loss: 4.759648405549938

Epoch: 6| Step: 8
Training loss: 4.776105319409814
Validation loss: 4.7345913537363575

Epoch: 6| Step: 9
Training loss: 4.286315149238766
Validation loss: 4.708954475646026

Epoch: 6| Step: 10
Training loss: 3.5019482231212207
Validation loss: 4.685069197009106

Epoch: 6| Step: 11
Training loss: 4.841083013883821
Validation loss: 4.662443059634993

Epoch: 6| Step: 12
Training loss: 4.344035626358432
Validation loss: 4.640113386657734

Epoch: 6| Step: 13
Training loss: 4.819435769451053
Validation loss: 4.617222715317465

Epoch: 6| Step: 0
Training loss: 4.540974042148319
Validation loss: 4.595696002566695

Epoch: 6| Step: 1
Training loss: 3.4862177470709823
Validation loss: 4.572154716193681

Epoch: 6| Step: 2
Training loss: 3.6725746299894024
Validation loss: 4.552911148647614

Epoch: 6| Step: 3
Training loss: 3.2716493661753105
Validation loss: 4.530311147182653

Epoch: 6| Step: 4
Training loss: 4.97821123008974
Validation loss: 4.509308790793733

Epoch: 6| Step: 5
Training loss: 5.293844520778889
Validation loss: 4.485948057259376

Epoch: 6| Step: 6
Training loss: 5.569867858689005
Validation loss: 4.4657231635792325

Epoch: 6| Step: 7
Training loss: 4.650398417806365
Validation loss: 4.440325211858599

Epoch: 6| Step: 8
Training loss: 4.869127698162744
Validation loss: 4.416746536216869

Epoch: 6| Step: 9
Training loss: 4.768142785526556
Validation loss: 4.391236579512341

Epoch: 6| Step: 10
Training loss: 4.816577125605815
Validation loss: 4.3624471048004985

Epoch: 6| Step: 11
Training loss: 4.010323791749595
Validation loss: 4.337947305092009

Epoch: 6| Step: 12
Training loss: 4.294024933698637
Validation loss: 4.313305974913091

Epoch: 6| Step: 13
Training loss: 4.936883235991655
Validation loss: 4.289479592139066

Epoch: 7| Step: 0
Training loss: 4.8566731979197035
Validation loss: 4.2654864071479475

Epoch: 6| Step: 1
Training loss: 4.264701771298917
Validation loss: 4.241778423019204

Epoch: 6| Step: 2
Training loss: 4.9339476735557035
Validation loss: 4.218743983985686

Epoch: 6| Step: 3
Training loss: 4.825187307882705
Validation loss: 4.19648516291687

Epoch: 6| Step: 4
Training loss: 3.8332786279727764
Validation loss: 4.1714542980720015

Epoch: 6| Step: 5
Training loss: 3.9525517587089785
Validation loss: 4.150606048726163

Epoch: 6| Step: 6
Training loss: 3.3484784499818234
Validation loss: 4.131220388713589

Epoch: 6| Step: 7
Training loss: 4.236885931032145
Validation loss: 4.108838773200345

Epoch: 6| Step: 8
Training loss: 4.002592200528361
Validation loss: 4.082187787399919

Epoch: 6| Step: 9
Training loss: 4.675806948942535
Validation loss: 4.059369457233858

Epoch: 6| Step: 10
Training loss: 4.421359514040063
Validation loss: 4.03189984204991

Epoch: 6| Step: 11
Training loss: 3.7589058980526553
Validation loss: 4.008446471429495

Epoch: 6| Step: 12
Training loss: 3.998142049352644
Validation loss: 3.9840805842511067

Epoch: 6| Step: 13
Training loss: 4.046180459297892
Validation loss: 3.9642347107989875

Epoch: 8| Step: 0
Training loss: 4.180935853295886
Validation loss: 3.9434035384261263

Epoch: 6| Step: 1
Training loss: 4.926268539103441
Validation loss: 3.9250898374046583

Epoch: 6| Step: 2
Training loss: 4.0858945534310855
Validation loss: 3.910516180421834

Epoch: 6| Step: 3
Training loss: 4.935482530434845
Validation loss: 3.8941454180814183

Epoch: 6| Step: 4
Training loss: 3.1579035591537647
Validation loss: 3.8794821289290002

Epoch: 6| Step: 5
Training loss: 2.521116246092642
Validation loss: 3.8688934935177923

Epoch: 6| Step: 6
Training loss: 4.659536955261882
Validation loss: 3.855380461423817

Epoch: 6| Step: 7
Training loss: 4.262421351627334
Validation loss: 3.8451567765952896

Epoch: 6| Step: 8
Training loss: 5.187201066193892
Validation loss: 3.833439254813239

Epoch: 6| Step: 9
Training loss: 3.9963631786157157
Validation loss: 3.8177140259683155

Epoch: 6| Step: 10
Training loss: 2.3706866799458455
Validation loss: 3.8068354697079885

Epoch: 6| Step: 11
Training loss: 3.57531134576798
Validation loss: 3.7909936741498376

Epoch: 6| Step: 12
Training loss: 3.7203061990952917
Validation loss: 3.7793927894176234

Epoch: 6| Step: 13
Training loss: 2.948805783897519
Validation loss: 3.767683040440521

Epoch: 9| Step: 0
Training loss: 3.9131098214113886
Validation loss: 3.7561481678720456

Epoch: 6| Step: 1
Training loss: 4.391323916374809
Validation loss: 3.746430599480244

Epoch: 6| Step: 2
Training loss: 4.0353864399190345
Validation loss: 3.7341127540819823

Epoch: 6| Step: 3
Training loss: 4.377109781821134
Validation loss: 3.7245432069630695

Epoch: 6| Step: 4
Training loss: 3.6059585850220923
Validation loss: 3.715495918644177

Epoch: 6| Step: 5
Training loss: 2.8034917040831924
Validation loss: 3.7044818082241497

Epoch: 6| Step: 6
Training loss: 3.960081350280861
Validation loss: 3.6923843455255922

Epoch: 6| Step: 7
Training loss: 3.46134033614255
Validation loss: 3.682385882815514

Epoch: 6| Step: 8
Training loss: 4.262127794588325
Validation loss: 3.675617727130215

Epoch: 6| Step: 9
Training loss: 3.8793052629459464
Validation loss: 3.666320291269663

Epoch: 6| Step: 10
Training loss: 4.589345883902176
Validation loss: 3.655278115528166

Epoch: 6| Step: 11
Training loss: 3.2394293025810734
Validation loss: 3.6478333409183383

Epoch: 6| Step: 12
Training loss: 4.1291520001368776
Validation loss: 3.641156829488765

Epoch: 6| Step: 13
Training loss: 2.565183676376525
Validation loss: 3.6301175388602367

Epoch: 10| Step: 0
Training loss: 3.034402996841588
Validation loss: 3.623322215650507

Epoch: 6| Step: 1
Training loss: 4.6127573825387795
Validation loss: 3.615740120576724

Epoch: 6| Step: 2
Training loss: 4.098867231986079
Validation loss: 3.6104213982524525

Epoch: 6| Step: 3
Training loss: 2.568695672938809
Validation loss: 3.598930128812813

Epoch: 6| Step: 4
Training loss: 3.7507607006359467
Validation loss: 3.5950319333771654

Epoch: 6| Step: 5
Training loss: 3.545816271898139
Validation loss: 3.5874523709496144

Epoch: 6| Step: 6
Training loss: 4.036561527368733
Validation loss: 3.5800258976676824

Epoch: 6| Step: 7
Training loss: 3.508321406569939
Validation loss: 3.5727920364594037

Epoch: 6| Step: 8
Training loss: 4.325312282335425
Validation loss: 3.5675447614966522

Epoch: 6| Step: 9
Training loss: 4.1222370026338595
Validation loss: 3.5580302696988575

Epoch: 6| Step: 10
Training loss: 4.486643147815507
Validation loss: 3.550695471815751

Epoch: 6| Step: 11
Training loss: 3.3772719471570114
Validation loss: 3.543313654818318

Epoch: 6| Step: 12
Training loss: 2.8605884416147327
Validation loss: 3.5374117738905557

Epoch: 6| Step: 13
Training loss: 3.9572060970980103
Validation loss: 3.5318530518176923

Epoch: 11| Step: 0
Training loss: 3.9239797833434222
Validation loss: 3.5238158240249904

Epoch: 6| Step: 1
Training loss: 4.284440904996614
Validation loss: 3.5179659390344984

Epoch: 6| Step: 2
Training loss: 3.7664031811495744
Validation loss: 3.5095021677421006

Epoch: 6| Step: 3
Training loss: 3.8990482929738612
Validation loss: 3.50550154999568

Epoch: 6| Step: 4
Training loss: 2.34356546311278
Validation loss: 3.496958261995087

Epoch: 6| Step: 5
Training loss: 3.263515247214088
Validation loss: 3.4918048505209027

Epoch: 6| Step: 6
Training loss: 3.925273744792536
Validation loss: 3.4868090424639826

Epoch: 6| Step: 7
Training loss: 2.862823408089602
Validation loss: 3.4806474748633205

Epoch: 6| Step: 8
Training loss: 3.9025209251101844
Validation loss: 3.478489272989068

Epoch: 6| Step: 9
Training loss: 3.0132110896113202
Validation loss: 3.469770777051978

Epoch: 6| Step: 10
Training loss: 4.799560192940005
Validation loss: 3.465724009140711

Epoch: 6| Step: 11
Training loss: 4.417492531390696
Validation loss: 3.459949110516256

Epoch: 6| Step: 12
Training loss: 3.5170384257248175
Validation loss: 3.4556119159104353

Epoch: 6| Step: 13
Training loss: 2.3427167522255967
Validation loss: 3.445858174749193

Epoch: 12| Step: 0
Training loss: 3.5704537996006906
Validation loss: 3.4409843989780353

Epoch: 6| Step: 1
Training loss: 3.6515031306182717
Validation loss: 3.435708400372458

Epoch: 6| Step: 2
Training loss: 3.855144204931939
Validation loss: 3.4316029759696063

Epoch: 6| Step: 3
Training loss: 3.362560518422125
Validation loss: 3.4240166122660116

Epoch: 6| Step: 4
Training loss: 3.822680279788564
Validation loss: 3.4218173237336376

Epoch: 6| Step: 5
Training loss: 3.6836920004046503
Validation loss: 3.414651589141651

Epoch: 6| Step: 6
Training loss: 3.730830980859259
Validation loss: 3.411826057720604

Epoch: 6| Step: 7
Training loss: 4.177105200401901
Validation loss: 3.402668709786187

Epoch: 6| Step: 8
Training loss: 3.068714920123133
Validation loss: 3.3989848849174424

Epoch: 6| Step: 9
Training loss: 3.0051524103648193
Validation loss: 3.394140060810893

Epoch: 6| Step: 10
Training loss: 4.272514620198457
Validation loss: 3.388106293984647

Epoch: 6| Step: 11
Training loss: 3.5877918775962745
Validation loss: 3.383434566698986

Epoch: 6| Step: 12
Training loss: 3.219522031767621
Validation loss: 3.3776682883673135

Epoch: 6| Step: 13
Training loss: 3.5522472683095137
Validation loss: 3.3755856503005623

Epoch: 13| Step: 0
Training loss: 4.225781719414376
Validation loss: 3.3687573225791936

Epoch: 6| Step: 1
Training loss: 4.2764783567346765
Validation loss: 3.3644451962700535

Epoch: 6| Step: 2
Training loss: 3.760772998161571
Validation loss: 3.3585688118494113

Epoch: 6| Step: 3
Training loss: 3.5948870642992277
Validation loss: 3.3542073840390403

Epoch: 6| Step: 4
Training loss: 3.523150041107844
Validation loss: 3.348457583883797

Epoch: 6| Step: 5
Training loss: 3.55956803519303
Validation loss: 3.3471449211890474

Epoch: 6| Step: 6
Training loss: 3.467562773011879
Validation loss: 3.3460226477059707

Epoch: 6| Step: 7
Training loss: 2.832490702224111
Validation loss: 3.3380104497106755

Epoch: 6| Step: 8
Training loss: 3.918976582816621
Validation loss: 3.33543377168917

Epoch: 6| Step: 9
Training loss: 3.0465072018594417
Validation loss: 3.3333453014117906

Epoch: 6| Step: 10
Training loss: 3.491993602174402
Validation loss: 3.3297123905280057

Epoch: 6| Step: 11
Training loss: 2.946030425547148
Validation loss: 3.325573908020098

Epoch: 6| Step: 12
Training loss: 3.2860670788837876
Validation loss: 3.3193488833606613

Epoch: 6| Step: 13
Training loss: 3.957886613768607
Validation loss: 3.31886791868474

Epoch: 14| Step: 0
Training loss: 3.1267037896865033
Validation loss: 3.316021111779057

Epoch: 6| Step: 1
Training loss: 3.5243259840667087
Validation loss: 3.3121513605793726

Epoch: 6| Step: 2
Training loss: 3.8416770990082485
Validation loss: 3.3105622000603794

Epoch: 6| Step: 3
Training loss: 4.2483355684949204
Validation loss: 3.308705240277705

Epoch: 6| Step: 4
Training loss: 4.234372297335392
Validation loss: 3.300852722143665

Epoch: 6| Step: 5
Training loss: 3.240305158820359
Validation loss: 3.3026636491737444

Epoch: 6| Step: 6
Training loss: 3.2400458558687597
Validation loss: 3.2940234626611677

Epoch: 6| Step: 7
Training loss: 3.6507152170176407
Validation loss: 3.292443218250024

Epoch: 6| Step: 8
Training loss: 3.264400416842568
Validation loss: 3.2906111934072158

Epoch: 6| Step: 9
Training loss: 3.516816204441215
Validation loss: 3.2887985711688774

Epoch: 6| Step: 10
Training loss: 3.447511555055503
Validation loss: 3.2856901411306834

Epoch: 6| Step: 11
Training loss: 3.517665694617229
Validation loss: 3.280300711311367

Epoch: 6| Step: 12
Training loss: 3.1233132960254553
Validation loss: 3.2788897454264747

Epoch: 6| Step: 13
Training loss: 3.068130144653856
Validation loss: 3.2812320956802785

Epoch: 15| Step: 0
Training loss: 4.24323789402826
Validation loss: 3.2753498681129543

Epoch: 6| Step: 1
Training loss: 2.8237692658610207
Validation loss: 3.281490161269441

Epoch: 6| Step: 2
Training loss: 3.2401172324741245
Validation loss: 3.2754921627958717

Epoch: 6| Step: 3
Training loss: 3.07403013239669
Validation loss: 3.2719944860886936

Epoch: 6| Step: 4
Training loss: 3.651017055165502
Validation loss: 3.2705599595692125

Epoch: 6| Step: 5
Training loss: 3.816851399599992
Validation loss: 3.2644934209074914

Epoch: 6| Step: 6
Training loss: 3.047454622351737
Validation loss: 3.265390299661795

Epoch: 6| Step: 7
Training loss: 3.13229586557949
Validation loss: 3.263797929331689

Epoch: 6| Step: 8
Training loss: 3.0340446881678926
Validation loss: 3.260745558783202

Epoch: 6| Step: 9
Training loss: 3.7884481974243585
Validation loss: 3.260639380557637

Epoch: 6| Step: 10
Training loss: 3.77792464388428
Validation loss: 3.258386407495502

Epoch: 6| Step: 11
Training loss: 3.7972160782694946
Validation loss: 3.256970976176673

Epoch: 6| Step: 12
Training loss: 3.9309786122200467
Validation loss: 3.2517816930685495

Epoch: 6| Step: 13
Training loss: 3.397556936251518
Validation loss: 3.252075949009865

Epoch: 16| Step: 0
Training loss: 2.986342176225936
Validation loss: 3.254475307041422

Epoch: 6| Step: 1
Training loss: 4.0190794814148285
Validation loss: 3.2510337289934066

Epoch: 6| Step: 2
Training loss: 3.266681117071252
Validation loss: 3.2450005941338147

Epoch: 6| Step: 3
Training loss: 4.2522123973546755
Validation loss: 3.2446690910222067

Epoch: 6| Step: 4
Training loss: 3.7327225991652977
Validation loss: 3.2403695548825295

Epoch: 6| Step: 5
Training loss: 3.1185613296102623
Validation loss: 3.2382058491669037

Epoch: 6| Step: 6
Training loss: 3.2722841839482064
Validation loss: 3.2378458687589773

Epoch: 6| Step: 7
Training loss: 3.312734775500456
Validation loss: 3.2412861260588848

Epoch: 6| Step: 8
Training loss: 3.1465137747889367
Validation loss: 3.2405769721700293

Epoch: 6| Step: 9
Training loss: 3.481873028819167
Validation loss: 3.246305656690908

Epoch: 6| Step: 10
Training loss: 3.1734122323664797
Validation loss: 3.2306558616050225

Epoch: 6| Step: 11
Training loss: 3.4640515098001687
Validation loss: 3.2291946558114786

Epoch: 6| Step: 12
Training loss: 3.6972736211449453
Validation loss: 3.22982045573638

Epoch: 6| Step: 13
Training loss: 3.8668961232164722
Validation loss: 3.227351322974878

Epoch: 17| Step: 0
Training loss: 3.8796979666671265
Validation loss: 3.224676264413981

Epoch: 6| Step: 1
Training loss: 3.661900254445872
Validation loss: 3.2230061404454946

Epoch: 6| Step: 2
Training loss: 3.4924907509989183
Validation loss: 3.225879192711423

Epoch: 6| Step: 3
Training loss: 3.1585319503561933
Validation loss: 3.223789113170523

Epoch: 6| Step: 4
Training loss: 3.9438652783456276
Validation loss: 3.2262992160527424

Epoch: 6| Step: 5
Training loss: 3.9653523236401256
Validation loss: 3.2291216944732892

Epoch: 6| Step: 6
Training loss: 3.1873089602198825
Validation loss: 3.228455887184944

Epoch: 6| Step: 7
Training loss: 3.262590376380445
Validation loss: 3.218930887789874

Epoch: 6| Step: 8
Training loss: 3.539387736417989
Validation loss: 3.217965597247517

Epoch: 6| Step: 9
Training loss: 3.582761068261937
Validation loss: 3.2195694849214194

Epoch: 6| Step: 10
Training loss: 4.010852634793783
Validation loss: 3.2182426255847396

Epoch: 6| Step: 11
Training loss: 2.507611798487813
Validation loss: 3.211658770129956

Epoch: 6| Step: 12
Training loss: 3.1012843701550286
Validation loss: 3.2116984067672756

Epoch: 6| Step: 13
Training loss: 2.5088114902656526
Validation loss: 3.210952300867525

Epoch: 18| Step: 0
Training loss: 3.3362339433017096
Validation loss: 3.209894522538076

Epoch: 6| Step: 1
Training loss: 3.6610969954819756
Validation loss: 3.2084920039825287

Epoch: 6| Step: 2
Training loss: 4.382323973929311
Validation loss: 3.210925282725875

Epoch: 6| Step: 3
Training loss: 3.3906657836148875
Validation loss: 3.2207119190620337

Epoch: 6| Step: 4
Training loss: 1.9155171374871482
Validation loss: 3.2145600949405533

Epoch: 6| Step: 5
Training loss: 2.9881794432208544
Validation loss: 3.212061643197619

Epoch: 6| Step: 6
Training loss: 3.8979560386562695
Validation loss: 3.209175222075083

Epoch: 6| Step: 7
Training loss: 3.4167637384782807
Validation loss: 3.1986479413954703

Epoch: 6| Step: 8
Training loss: 3.4377113970864692
Validation loss: 3.2005978551350314

Epoch: 6| Step: 9
Training loss: 3.2383324926208044
Validation loss: 3.2008198156450827

Epoch: 6| Step: 10
Training loss: 4.256615818788415
Validation loss: 3.199503746732286

Epoch: 6| Step: 11
Training loss: 3.065244418132999
Validation loss: 3.1996547882574853

Epoch: 6| Step: 12
Training loss: 3.7247827146509236
Validation loss: 3.197875718700384

Epoch: 6| Step: 13
Training loss: 2.7134513522669583
Validation loss: 3.1945520831986327

Epoch: 19| Step: 0
Training loss: 4.103164214784284
Validation loss: 3.196473398840273

Epoch: 6| Step: 1
Training loss: 3.3063364807241094
Validation loss: 3.1964904217029124

Epoch: 6| Step: 2
Training loss: 3.519905028145721
Validation loss: 3.1992794135637643

Epoch: 6| Step: 3
Training loss: 3.0498085962490964
Validation loss: 3.199930745190644

Epoch: 6| Step: 4
Training loss: 3.1365268976392118
Validation loss: 3.199432849487515

Epoch: 6| Step: 5
Training loss: 3.2157297130325295
Validation loss: 3.201745071321876

Epoch: 6| Step: 6
Training loss: 3.2717072277174997
Validation loss: 3.2016453680763304

Epoch: 6| Step: 7
Training loss: 3.216147546600421
Validation loss: 3.1976124506991206

Epoch: 6| Step: 8
Training loss: 3.364259938542507
Validation loss: 3.1938244855766276

Epoch: 6| Step: 9
Training loss: 3.4053127854319016
Validation loss: 3.190902104018863

Epoch: 6| Step: 10
Training loss: 3.4891570936432617
Validation loss: 3.1860881785700714

Epoch: 6| Step: 11
Training loss: 3.8561301239028105
Validation loss: 3.1844790010692337

Epoch: 6| Step: 12
Training loss: 3.496883639818415
Validation loss: 3.186230437418881

Epoch: 6| Step: 13
Training loss: 3.8970362528190745
Validation loss: 3.1852386064504605

Epoch: 20| Step: 0
Training loss: 3.372687783976393
Validation loss: 3.1879186787581517

Epoch: 6| Step: 1
Training loss: 2.6702455604872752
Validation loss: 3.1922969942910493

Epoch: 6| Step: 2
Training loss: 3.388396062262477
Validation loss: 3.2010399908257736

Epoch: 6| Step: 3
Training loss: 4.190810531844449
Validation loss: 3.207104022444802

Epoch: 6| Step: 4
Training loss: 3.0365598126178033
Validation loss: 3.1860887192852885

Epoch: 6| Step: 5
Training loss: 2.506857431674019
Validation loss: 3.1801697808027334

Epoch: 6| Step: 6
Training loss: 3.0107247658026597
Validation loss: 3.178441052319307

Epoch: 6| Step: 7
Training loss: 3.6798551808762365
Validation loss: 3.1853743689457272

Epoch: 6| Step: 8
Training loss: 3.6454887154602935
Validation loss: 3.1850204225400773

Epoch: 6| Step: 9
Training loss: 3.589373104221827
Validation loss: 3.1859653404996613

Epoch: 6| Step: 10
Training loss: 3.919795484268123
Validation loss: 3.188045515349627

Epoch: 6| Step: 11
Training loss: 3.6117078499997755
Validation loss: 3.193635444038715

Epoch: 6| Step: 12
Training loss: 3.136164444037325
Validation loss: 3.1891771497081773

Epoch: 6| Step: 13
Training loss: 4.398718274439242
Validation loss: 3.185200441019667

Epoch: 21| Step: 0
Training loss: 3.374052833175131
Validation loss: 3.1789464753000884

Epoch: 6| Step: 1
Training loss: 2.7883898353970356
Validation loss: 3.1778780767069845

Epoch: 6| Step: 2
Training loss: 2.9005941505821875
Validation loss: 3.181490057012923

Epoch: 6| Step: 3
Training loss: 3.912204202311154
Validation loss: 3.1792858964955246

Epoch: 6| Step: 4
Training loss: 3.89397460877497
Validation loss: 3.18383839015371

Epoch: 6| Step: 5
Training loss: 3.064864510906615
Validation loss: 3.181391614282709

Epoch: 6| Step: 6
Training loss: 3.452549398788411
Validation loss: 3.18890074856974

Epoch: 6| Step: 7
Training loss: 3.024338856477367
Validation loss: 3.1847068134810064

Epoch: 6| Step: 8
Training loss: 4.130381916851363
Validation loss: 3.178539175092698

Epoch: 6| Step: 9
Training loss: 3.748959969303544
Validation loss: 3.1739458264659377

Epoch: 6| Step: 10
Training loss: 3.610627456619301
Validation loss: 3.171836977672744

Epoch: 6| Step: 11
Training loss: 3.100917489655106
Validation loss: 3.1708949925209517

Epoch: 6| Step: 12
Training loss: 3.621545658492099
Validation loss: 3.1706257236106876

Epoch: 6| Step: 13
Training loss: 2.924090845692853
Validation loss: 3.1698061406652416

Epoch: 22| Step: 0
Training loss: 3.2559242539557887
Validation loss: 3.1678596579863734

Epoch: 6| Step: 1
Training loss: 3.6620481765719823
Validation loss: 3.1657718280337073

Epoch: 6| Step: 2
Training loss: 2.7022721372910414
Validation loss: 3.1670086330056533

Epoch: 6| Step: 3
Training loss: 3.539489046711685
Validation loss: 3.1633635722914777

Epoch: 6| Step: 4
Training loss: 4.02797785002633
Validation loss: 3.1617438815215153

Epoch: 6| Step: 5
Training loss: 3.7978023448372955
Validation loss: 3.164577794725168

Epoch: 6| Step: 6
Training loss: 3.98555077052821
Validation loss: 3.164390857450186

Epoch: 6| Step: 7
Training loss: 3.3498028312848334
Validation loss: 3.164769365012942

Epoch: 6| Step: 8
Training loss: 3.20791208713725
Validation loss: 3.167800278357454

Epoch: 6| Step: 9
Training loss: 3.761965924755108
Validation loss: 3.1614660485391717

Epoch: 6| Step: 10
Training loss: 3.0934293946883407
Validation loss: 3.1614365923696326

Epoch: 6| Step: 11
Training loss: 3.650674726228848
Validation loss: 3.159153486835073

Epoch: 6| Step: 12
Training loss: 2.6982407737283665
Validation loss: 3.157702413654458

Epoch: 6| Step: 13
Training loss: 2.3605143846406054
Validation loss: 3.15628238881884

Epoch: 23| Step: 0
Training loss: 3.297736755848845
Validation loss: 3.15858215411229

Epoch: 6| Step: 1
Training loss: 3.177273820680174
Validation loss: 3.155212366612367

Epoch: 6| Step: 2
Training loss: 3.0536764281408733
Validation loss: 3.1536540627148844

Epoch: 6| Step: 3
Training loss: 4.606868359741179
Validation loss: 3.1525925569883038

Epoch: 6| Step: 4
Training loss: 3.7511605056515718
Validation loss: 3.1506646444926347

Epoch: 6| Step: 5
Training loss: 3.605831768569507
Validation loss: 3.1508171558189115

Epoch: 6| Step: 6
Training loss: 3.6632936887068226
Validation loss: 3.1508662832062417

Epoch: 6| Step: 7
Training loss: 3.301060916406743
Validation loss: 3.1489841014489754

Epoch: 6| Step: 8
Training loss: 3.61394789036146
Validation loss: 3.155642382440496

Epoch: 6| Step: 9
Training loss: 3.3943305093388325
Validation loss: 3.156475982162391

Epoch: 6| Step: 10
Training loss: 2.129638210070489
Validation loss: 3.157839618966712

Epoch: 6| Step: 11
Training loss: 3.2256845383525645
Validation loss: 3.1576849876506023

Epoch: 6| Step: 12
Training loss: 3.095483775746241
Validation loss: 3.153816842351193

Epoch: 6| Step: 13
Training loss: 3.336956074142912
Validation loss: 3.154359260728734

Epoch: 24| Step: 0
Training loss: 3.2157362374657215
Validation loss: 3.1491257431766084

Epoch: 6| Step: 1
Training loss: 3.0272216772263056
Validation loss: 3.1485938694932787

Epoch: 6| Step: 2
Training loss: 3.1507308596500545
Validation loss: 3.1460457262688144

Epoch: 6| Step: 3
Training loss: 3.260789713780255
Validation loss: 3.146813696581165

Epoch: 6| Step: 4
Training loss: 3.4910687525679367
Validation loss: 3.145747726265495

Epoch: 6| Step: 5
Training loss: 3.391825801730848
Validation loss: 3.1456950170246585

Epoch: 6| Step: 6
Training loss: 3.671462275258655
Validation loss: 3.145888033818266

Epoch: 6| Step: 7
Training loss: 3.8153540762680103
Validation loss: 3.1435298025376692

Epoch: 6| Step: 8
Training loss: 3.6311138816967152
Validation loss: 3.141620435581561

Epoch: 6| Step: 9
Training loss: 3.1312197015632757
Validation loss: 3.139693015230932

Epoch: 6| Step: 10
Training loss: 3.059996847013956
Validation loss: 3.141241760865965

Epoch: 6| Step: 11
Training loss: 3.525016346345701
Validation loss: 3.141584938296017

Epoch: 6| Step: 12
Training loss: 3.454594778320239
Validation loss: 3.143593049460157

Epoch: 6| Step: 13
Training loss: 3.9668192334580383
Validation loss: 3.1434604377439355

Epoch: 25| Step: 0
Training loss: 3.0007447272046783
Validation loss: 3.1403706308359953

Epoch: 6| Step: 1
Training loss: 3.7156181932135137
Validation loss: 3.134177485577597

Epoch: 6| Step: 2
Training loss: 2.7702569481561405
Validation loss: 3.1342331057374553

Epoch: 6| Step: 3
Training loss: 3.416314750473703
Validation loss: 3.135679226983622

Epoch: 6| Step: 4
Training loss: 3.3327811896475965
Validation loss: 3.1382711086138793

Epoch: 6| Step: 5
Training loss: 3.2097335006036656
Validation loss: 3.139077895416197

Epoch: 6| Step: 6
Training loss: 3.3941938190855505
Validation loss: 3.1369923766865204

Epoch: 6| Step: 7
Training loss: 3.7426948601621506
Validation loss: 3.13680078461031

Epoch: 6| Step: 8
Training loss: 3.8900266133552743
Validation loss: 3.135606336404838

Epoch: 6| Step: 9
Training loss: 2.8142645492753706
Validation loss: 3.1354984093865

Epoch: 6| Step: 10
Training loss: 3.315023648507071
Validation loss: 3.1334182382371947

Epoch: 6| Step: 11
Training loss: 3.182534482758632
Validation loss: 3.132164708825482

Epoch: 6| Step: 12
Training loss: 3.816377262876098
Validation loss: 3.133808949577547

Epoch: 6| Step: 13
Training loss: 4.062135650728798
Validation loss: 3.130453028958124

Epoch: 26| Step: 0
Training loss: 4.174063919111695
Validation loss: 3.1364232314103684

Epoch: 6| Step: 1
Training loss: 3.613252744819994
Validation loss: 3.1332763896337408

Epoch: 6| Step: 2
Training loss: 3.321165590085809
Validation loss: 3.1323630525807804

Epoch: 6| Step: 3
Training loss: 3.868801450618349
Validation loss: 3.1294332498833968

Epoch: 6| Step: 4
Training loss: 2.747885497993504
Validation loss: 3.1312565690044694

Epoch: 6| Step: 5
Training loss: 3.8335662162916115
Validation loss: 3.132905483413088

Epoch: 6| Step: 6
Training loss: 2.3819242228794852
Validation loss: 3.13104566140249

Epoch: 6| Step: 7
Training loss: 3.117462222582419
Validation loss: 3.133382760938569

Epoch: 6| Step: 8
Training loss: 2.5902196919118974
Validation loss: 3.1390260321667847

Epoch: 6| Step: 9
Training loss: 3.3658852592581363
Validation loss: 3.1367273251012207

Epoch: 6| Step: 10
Training loss: 3.5527684659407526
Validation loss: 3.1426071746584197

Epoch: 6| Step: 11
Training loss: 3.581403138513138
Validation loss: 3.130753406817846

Epoch: 6| Step: 12
Training loss: 3.407157287155811
Validation loss: 3.1223464824120817

Epoch: 6| Step: 13
Training loss: 3.5542952436390367
Validation loss: 3.119712528789963

Epoch: 27| Step: 0
Training loss: 3.5906953517572133
Validation loss: 3.1213419893545145

Epoch: 6| Step: 1
Training loss: 3.519929006034004
Validation loss: 3.122603362498357

Epoch: 6| Step: 2
Training loss: 2.6649096084638515
Validation loss: 3.122453039686748

Epoch: 6| Step: 3
Training loss: 4.044348439857589
Validation loss: 3.123304825275093

Epoch: 6| Step: 4
Training loss: 3.2334227703611
Validation loss: 3.119511552170767

Epoch: 6| Step: 5
Training loss: 3.2882145674368495
Validation loss: 3.119829889689531

Epoch: 6| Step: 6
Training loss: 3.9422568999663166
Validation loss: 3.1206689449215874

Epoch: 6| Step: 7
Training loss: 2.8048689844605956
Validation loss: 3.117013951167298

Epoch: 6| Step: 8
Training loss: 3.272085707327764
Validation loss: 3.1170755878683116

Epoch: 6| Step: 9
Training loss: 3.461538292811463
Validation loss: 3.1161091441456934

Epoch: 6| Step: 10
Training loss: 3.4254585905811403
Validation loss: 3.1178046659067875

Epoch: 6| Step: 11
Training loss: 3.13640496951725
Validation loss: 3.1189109373555466

Epoch: 6| Step: 12
Training loss: 3.521501707181418
Validation loss: 3.121438799240756

Epoch: 6| Step: 13
Training loss: 3.189868421083108
Validation loss: 3.1210942050509924

Epoch: 28| Step: 0
Training loss: 3.85823765108071
Validation loss: 3.124189414446735

Epoch: 6| Step: 1
Training loss: 3.7025049390637554
Validation loss: 3.124813574695933

Epoch: 6| Step: 2
Training loss: 2.749694200332651
Validation loss: 3.122636367950598

Epoch: 6| Step: 3
Training loss: 3.205764501745163
Validation loss: 3.1164178014417505

Epoch: 6| Step: 4
Training loss: 3.3222964833131114
Validation loss: 3.1124558678058314

Epoch: 6| Step: 5
Training loss: 3.117899954315736
Validation loss: 3.1148789472992626

Epoch: 6| Step: 6
Training loss: 3.0309781611356508
Validation loss: 3.11183760153636

Epoch: 6| Step: 7
Training loss: 4.260799375225026
Validation loss: 3.1115351257127553

Epoch: 6| Step: 8
Training loss: 3.8524478413233822
Validation loss: 3.110572431431423

Epoch: 6| Step: 9
Training loss: 3.7941073153555593
Validation loss: 3.109254640642934

Epoch: 6| Step: 10
Training loss: 2.4610626673278047
Validation loss: 3.107435645963966

Epoch: 6| Step: 11
Training loss: 2.5836735983276435
Validation loss: 3.1104804939037014

Epoch: 6| Step: 12
Training loss: 3.767482441922631
Validation loss: 3.1090481477546086

Epoch: 6| Step: 13
Training loss: 2.7602840079814284
Validation loss: 3.1168021653391054

Epoch: 29| Step: 0
Training loss: 3.4336842947034834
Validation loss: 3.1198803398786157

Epoch: 6| Step: 1
Training loss: 2.8627965915064277
Validation loss: 3.12298101197586

Epoch: 6| Step: 2
Training loss: 3.3867839380170177
Validation loss: 3.1200878265625747

Epoch: 6| Step: 3
Training loss: 3.6350237935584864
Validation loss: 3.1304377770571805

Epoch: 6| Step: 4
Training loss: 3.699654800189332
Validation loss: 3.1094603390504143

Epoch: 6| Step: 5
Training loss: 3.0825284646259945
Validation loss: 3.101683934787041

Epoch: 6| Step: 6
Training loss: 3.5646016129174205
Validation loss: 3.1014301840811305

Epoch: 6| Step: 7
Training loss: 3.478698759918119
Validation loss: 3.1014466548590063

Epoch: 6| Step: 8
Training loss: 3.7160013763786535
Validation loss: 3.0989653661283936

Epoch: 6| Step: 9
Training loss: 2.7679974568746712
Validation loss: 3.09930222504954

Epoch: 6| Step: 10
Training loss: 3.4908470636423936
Validation loss: 3.099690254593736

Epoch: 6| Step: 11
Training loss: 3.267608186409089
Validation loss: 3.102577097042922

Epoch: 6| Step: 12
Training loss: 3.552710350085411
Validation loss: 3.102443140336228

Epoch: 6| Step: 13
Training loss: 2.8608273012531207
Validation loss: 3.100788522745287

Epoch: 30| Step: 0
Training loss: 3.1777577836030866
Validation loss: 3.0999891376205815

Epoch: 6| Step: 1
Training loss: 3.2913869586037876
Validation loss: 3.1002892272280067

Epoch: 6| Step: 2
Training loss: 3.5026330580341125
Validation loss: 3.101607659834779

Epoch: 6| Step: 3
Training loss: 3.2202417564858146
Validation loss: 3.096939175024046

Epoch: 6| Step: 4
Training loss: 3.2773859432809895
Validation loss: 3.1008912522451526

Epoch: 6| Step: 5
Training loss: 3.1101229740864635
Validation loss: 3.0960728657012244

Epoch: 6| Step: 6
Training loss: 3.3246678408731647
Validation loss: 3.0975434319296196

Epoch: 6| Step: 7
Training loss: 3.4125146215775355
Validation loss: 3.093694048943525

Epoch: 6| Step: 8
Training loss: 2.679827539104571
Validation loss: 3.089815442195805

Epoch: 6| Step: 9
Training loss: 3.513601169860798
Validation loss: 3.0932008784844576

Epoch: 6| Step: 10
Training loss: 3.8734917935793742
Validation loss: 3.0920260471564505

Epoch: 6| Step: 11
Training loss: 3.9554800625239426
Validation loss: 3.090694554025909

Epoch: 6| Step: 12
Training loss: 3.664413337747706
Validation loss: 3.086563607176437

Epoch: 6| Step: 13
Training loss: 2.5503759368833436
Validation loss: 3.088366985681471

Epoch: 31| Step: 0
Training loss: 2.594255880636825
Validation loss: 3.0850759258597624

Epoch: 6| Step: 1
Training loss: 3.406254759618951
Validation loss: 3.082791378634803

Epoch: 6| Step: 2
Training loss: 3.776567696949323
Validation loss: 3.0842935249928156

Epoch: 6| Step: 3
Training loss: 3.526821363934594
Validation loss: 3.0836848352671473

Epoch: 6| Step: 4
Training loss: 3.642773694062977
Validation loss: 3.0826220133943214

Epoch: 6| Step: 5
Training loss: 2.886297832768365
Validation loss: 3.0761971848490495

Epoch: 6| Step: 6
Training loss: 2.4637025830308854
Validation loss: 3.0790280710727522

Epoch: 6| Step: 7
Training loss: 4.208920252194267
Validation loss: 3.076498583561219

Epoch: 6| Step: 8
Training loss: 3.4658331076739723
Validation loss: 3.0776934927428194

Epoch: 6| Step: 9
Training loss: 3.2634610393916827
Validation loss: 3.0750562196333244

Epoch: 6| Step: 10
Training loss: 3.3381191865134388
Validation loss: 3.069623381190473

Epoch: 6| Step: 11
Training loss: 3.5088106702037853
Validation loss: 3.067543708262619

Epoch: 6| Step: 12
Training loss: 3.5665378107579913
Validation loss: 3.0692353408833424

Epoch: 6| Step: 13
Training loss: 2.297002983420149
Validation loss: 3.066168490853094

Epoch: 32| Step: 0
Training loss: 3.1786734348786174
Validation loss: 3.0650905972279374

Epoch: 6| Step: 1
Training loss: 3.0169425023321095
Validation loss: 3.065525354522461

Epoch: 6| Step: 2
Training loss: 3.5501476095969244
Validation loss: 3.0636411930153282

Epoch: 6| Step: 3
Training loss: 4.0784486371821345
Validation loss: 3.063732835795351

Epoch: 6| Step: 4
Training loss: 4.19147678397665
Validation loss: 3.061917294551162

Epoch: 6| Step: 5
Training loss: 3.6062845317701253
Validation loss: 3.0595048761568875

Epoch: 6| Step: 6
Training loss: 3.3048397236568157
Validation loss: 3.058781301272537

Epoch: 6| Step: 7
Training loss: 3.3295595900815185
Validation loss: 3.057743488088246

Epoch: 6| Step: 8
Training loss: 3.067415613134479
Validation loss: 3.059609122509811

Epoch: 6| Step: 9
Training loss: 2.8602553716435226
Validation loss: 3.059462982889864

Epoch: 6| Step: 10
Training loss: 2.949071291843417
Validation loss: 3.059108948636418

Epoch: 6| Step: 11
Training loss: 3.3775331386292957
Validation loss: 3.0620528835022136

Epoch: 6| Step: 12
Training loss: 3.2113173508402464
Validation loss: 3.0594918815629857

Epoch: 6| Step: 13
Training loss: 2.030757903131935
Validation loss: 3.0560159674442455

Epoch: 33| Step: 0
Training loss: 2.432184929040704
Validation loss: 3.062566641374843

Epoch: 6| Step: 1
Training loss: 3.3999380723137955
Validation loss: 3.0618181446896346

Epoch: 6| Step: 2
Training loss: 3.535098249265218
Validation loss: 3.0795858025619647

Epoch: 6| Step: 3
Training loss: 3.549115385061395
Validation loss: 3.0777727434288358

Epoch: 6| Step: 4
Training loss: 3.137854421590293
Validation loss: 3.08643321984003

Epoch: 6| Step: 5
Training loss: 3.8567782789832896
Validation loss: 3.0726386348919377

Epoch: 6| Step: 6
Training loss: 3.088207173152657
Validation loss: 3.060622022234077

Epoch: 6| Step: 7
Training loss: 3.367361112929257
Validation loss: 3.0606337539283746

Epoch: 6| Step: 8
Training loss: 3.7444444830567774
Validation loss: 3.0603399789682095

Epoch: 6| Step: 9
Training loss: 3.6638842053335656
Validation loss: 3.060912943424637

Epoch: 6| Step: 10
Training loss: 2.7772467444710944
Validation loss: 3.0589103597200866

Epoch: 6| Step: 11
Training loss: 3.378756657635124
Validation loss: 3.0596662163528663

Epoch: 6| Step: 12
Training loss: 2.993076282103943
Validation loss: 3.056611364720528

Epoch: 6| Step: 13
Training loss: 3.59327429235171
Validation loss: 3.05514112522751

Epoch: 34| Step: 0
Training loss: 3.9647963647876545
Validation loss: 3.0538586434931196

Epoch: 6| Step: 1
Training loss: 2.6814559572875334
Validation loss: 3.056589183016081

Epoch: 6| Step: 2
Training loss: 2.108286824172009
Validation loss: 3.0523735509539476

Epoch: 6| Step: 3
Training loss: 3.5510809743549614
Validation loss: 3.0521921229934303

Epoch: 6| Step: 4
Training loss: 3.743108902745484
Validation loss: 3.057408155439386

Epoch: 6| Step: 5
Training loss: 2.6303240961465266
Validation loss: 3.0526997815391614

Epoch: 6| Step: 6
Training loss: 3.4483150624644976
Validation loss: 3.0526082417210736

Epoch: 6| Step: 7
Training loss: 3.738919226599075
Validation loss: 3.0505835358375695

Epoch: 6| Step: 8
Training loss: 3.2303354588678324
Validation loss: 3.053692558750069

Epoch: 6| Step: 9
Training loss: 3.3660745219584682
Validation loss: 3.053712796188905

Epoch: 6| Step: 10
Training loss: 3.6413638957289294
Validation loss: 3.0513443402696008

Epoch: 6| Step: 11
Training loss: 3.9430681851378737
Validation loss: 3.0506605251764096

Epoch: 6| Step: 12
Training loss: 2.7115827776361807
Validation loss: 3.049514926805007

Epoch: 6| Step: 13
Training loss: 3.296530755317164
Validation loss: 3.0520871394885902

Epoch: 35| Step: 0
Training loss: 2.9926655439767185
Validation loss: 3.055863646775905

Epoch: 6| Step: 1
Training loss: 3.696265970659375
Validation loss: 3.0615009103773043

Epoch: 6| Step: 2
Training loss: 4.090149672178307
Validation loss: 3.0667475897913636

Epoch: 6| Step: 3
Training loss: 4.035105672631962
Validation loss: 3.0655472633340364

Epoch: 6| Step: 4
Training loss: 2.792936567641292
Validation loss: 3.0526610491290618

Epoch: 6| Step: 5
Training loss: 3.336325843952285
Validation loss: 3.049292808231289

Epoch: 6| Step: 6
Training loss: 3.524877689485137
Validation loss: 3.046228525864163

Epoch: 6| Step: 7
Training loss: 2.1776321181849974
Validation loss: 3.0514489090973553

Epoch: 6| Step: 8
Training loss: 3.1878329832392915
Validation loss: 3.0569261796184524

Epoch: 6| Step: 9
Training loss: 3.005875238546509
Validation loss: 3.0615804747100133

Epoch: 6| Step: 10
Training loss: 3.4006200337246635
Validation loss: 3.0680721772022896

Epoch: 6| Step: 11
Training loss: 3.136580410729754
Validation loss: 3.064241037883158

Epoch: 6| Step: 12
Training loss: 3.6950807397922962
Validation loss: 3.056301907559225

Epoch: 6| Step: 13
Training loss: 3.2128453584862107
Validation loss: 3.0552070359871997

Epoch: 36| Step: 0
Training loss: 2.5410124348437892
Validation loss: 3.054378620946292

Epoch: 6| Step: 1
Training loss: 3.1338503823562065
Validation loss: 3.0543234527715613

Epoch: 6| Step: 2
Training loss: 3.5076413527029664
Validation loss: 3.05274760494473

Epoch: 6| Step: 3
Training loss: 2.6944380516509505
Validation loss: 3.053219025178254

Epoch: 6| Step: 4
Training loss: 3.287813434236305
Validation loss: 3.0514701284093375

Epoch: 6| Step: 5
Training loss: 3.698879464047292
Validation loss: 3.0525266899366503

Epoch: 6| Step: 6
Training loss: 2.596650305987403
Validation loss: 3.0493059429723277

Epoch: 6| Step: 7
Training loss: 3.5064266330338527
Validation loss: 3.0478327354198758

Epoch: 6| Step: 8
Training loss: 3.3916028278413273
Validation loss: 3.045780290023056

Epoch: 6| Step: 9
Training loss: 4.150072368312563
Validation loss: 3.0479085451881063

Epoch: 6| Step: 10
Training loss: 3.786660729009603
Validation loss: 3.0489032869556993

Epoch: 6| Step: 11
Training loss: 3.50150048925736
Validation loss: 3.051016235503252

Epoch: 6| Step: 12
Training loss: 3.2177713628597258
Validation loss: 3.05198135437996

Epoch: 6| Step: 13
Training loss: 3.1528587629659888
Validation loss: 3.0560799819553117

Epoch: 37| Step: 0
Training loss: 2.7359276423339334
Validation loss: 3.0424332048260005

Epoch: 6| Step: 1
Training loss: 2.7984295153224394
Validation loss: 3.0436424724722384

Epoch: 6| Step: 2
Training loss: 3.292421922272085
Validation loss: 3.0432339141150977

Epoch: 6| Step: 3
Training loss: 3.957175369876724
Validation loss: 3.042171062790364

Epoch: 6| Step: 4
Training loss: 3.0153696218878383
Validation loss: 3.044756018463631

Epoch: 6| Step: 5
Training loss: 3.672636821527464
Validation loss: 3.042101475115651

Epoch: 6| Step: 6
Training loss: 3.420767827090209
Validation loss: 3.0426549571600896

Epoch: 6| Step: 7
Training loss: 3.640260604500282
Validation loss: 3.038874632716394

Epoch: 6| Step: 8
Training loss: 3.6230696438384955
Validation loss: 3.0357896247918967

Epoch: 6| Step: 9
Training loss: 3.656688826872572
Validation loss: 3.037371854291544

Epoch: 6| Step: 10
Training loss: 2.508579695337604
Validation loss: 3.0353143725553053

Epoch: 6| Step: 11
Training loss: 3.1858028306697386
Validation loss: 3.0360663120187907

Epoch: 6| Step: 12
Training loss: 3.1224328750224553
Validation loss: 3.0331676368482765

Epoch: 6| Step: 13
Training loss: 3.618022385679277
Validation loss: 3.0331593014505294

Epoch: 38| Step: 0
Training loss: 3.578667478792503
Validation loss: 3.035381870919279

Epoch: 6| Step: 1
Training loss: 2.9534114315249584
Validation loss: 3.033865727524021

Epoch: 6| Step: 2
Training loss: 2.9620735988295115
Validation loss: 3.0306397054040786

Epoch: 6| Step: 3
Training loss: 3.3162531193942493
Validation loss: 3.0333117962189493

Epoch: 6| Step: 4
Training loss: 2.6769973998372416
Validation loss: 3.030985453726291

Epoch: 6| Step: 5
Training loss: 4.003488688211288
Validation loss: 3.034458390316965

Epoch: 6| Step: 6
Training loss: 3.7142737880976435
Validation loss: 3.0309140021596983

Epoch: 6| Step: 7
Training loss: 3.973636292507385
Validation loss: 3.0287359781846246

Epoch: 6| Step: 8
Training loss: 2.6453552314745363
Validation loss: 3.033329020576787

Epoch: 6| Step: 9
Training loss: 3.165816962700276
Validation loss: 3.0263165863070602

Epoch: 6| Step: 10
Training loss: 3.4921024320682705
Validation loss: 3.0320827320094463

Epoch: 6| Step: 11
Training loss: 3.2284575801553896
Validation loss: 3.029575438048115

Epoch: 6| Step: 12
Training loss: 2.9523315978576146
Validation loss: 3.0307342019413492

Epoch: 6| Step: 13
Training loss: 3.3124822939993326
Validation loss: 3.0256981200966786

Epoch: 39| Step: 0
Training loss: 2.8918601206802115
Validation loss: 3.0257757324652403

Epoch: 6| Step: 1
Training loss: 3.411713025241361
Validation loss: 3.0254082774662545

Epoch: 6| Step: 2
Training loss: 2.74850588003761
Validation loss: 3.027761426065342

Epoch: 6| Step: 3
Training loss: 3.6587393347842454
Validation loss: 3.023215516767562

Epoch: 6| Step: 4
Training loss: 3.310840694925948
Validation loss: 3.024078379225784

Epoch: 6| Step: 5
Training loss: 3.3996073496112404
Validation loss: 3.0219076829613867

Epoch: 6| Step: 6
Training loss: 3.180244715649413
Validation loss: 3.0196215571459937

Epoch: 6| Step: 7
Training loss: 3.0851844351614384
Validation loss: 3.0215030379014975

Epoch: 6| Step: 8
Training loss: 3.3400362015092857
Validation loss: 3.024534205839819

Epoch: 6| Step: 9
Training loss: 4.1131409768787055
Validation loss: 3.0291832933094995

Epoch: 6| Step: 10
Training loss: 3.4649172391269354
Validation loss: 3.0309436433118617

Epoch: 6| Step: 11
Training loss: 3.3026841316033435
Validation loss: 3.0262500497112046

Epoch: 6| Step: 12
Training loss: 3.478129310296773
Validation loss: 3.0183742359895827

Epoch: 6| Step: 13
Training loss: 2.0542284086387275
Validation loss: 3.014926364245515

Epoch: 40| Step: 0
Training loss: 3.385132168284265
Validation loss: 3.006891422757406

Epoch: 6| Step: 1
Training loss: 3.1185055195421794
Validation loss: 3.0047588716413953

Epoch: 6| Step: 2
Training loss: 3.0826740247326234
Validation loss: 3.0073449502172456

Epoch: 6| Step: 3
Training loss: 2.870454927336112
Validation loss: 3.010083420499325

Epoch: 6| Step: 4
Training loss: 3.5471836577565967
Validation loss: 3.005147717558525

Epoch: 6| Step: 5
Training loss: 2.7530311871586672
Validation loss: 3.005496482892087

Epoch: 6| Step: 6
Training loss: 3.0550900557460268
Validation loss: 3.0093520057029832

Epoch: 6| Step: 7
Training loss: 3.1746173898439185
Validation loss: 3.003957660522669

Epoch: 6| Step: 8
Training loss: 3.2038058417994435
Validation loss: 3.0054916532923777

Epoch: 6| Step: 9
Training loss: 3.435687610374247
Validation loss: 2.9985320950486125

Epoch: 6| Step: 10
Training loss: 3.713738919976345
Validation loss: 3.000726085561517

Epoch: 6| Step: 11
Training loss: 3.449452237859219
Validation loss: 2.9929593763333324

Epoch: 6| Step: 12
Training loss: 3.547109991011003
Validation loss: 2.992625442619676

Epoch: 6| Step: 13
Training loss: 3.772789309097104
Validation loss: 2.9938385519031123

Epoch: 41| Step: 0
Training loss: 2.8515385874960564
Validation loss: 2.9905988893395494

Epoch: 6| Step: 1
Training loss: 3.8190935994854667
Validation loss: 2.9934217563993357

Epoch: 6| Step: 2
Training loss: 3.2514587943070508
Validation loss: 2.98368365717839

Epoch: 6| Step: 3
Training loss: 2.530554310835211
Validation loss: 2.987103056807471

Epoch: 6| Step: 4
Training loss: 3.3657445803113193
Validation loss: 2.9809905794613902

Epoch: 6| Step: 5
Training loss: 3.395940174380101
Validation loss: 2.9854468313068248

Epoch: 6| Step: 6
Training loss: 3.2966211591870405
Validation loss: 2.9839205175430195

Epoch: 6| Step: 7
Training loss: 3.414363193145901
Validation loss: 2.9807853196764187

Epoch: 6| Step: 8
Training loss: 4.012745815840314
Validation loss: 2.9809800152490826

Epoch: 6| Step: 9
Training loss: 3.949561763548517
Validation loss: 2.980267141976079

Epoch: 6| Step: 10
Training loss: 3.2134267991535244
Validation loss: 2.9786223426231686

Epoch: 6| Step: 11
Training loss: 3.177203883817994
Validation loss: 2.977126096397685

Epoch: 6| Step: 12
Training loss: 2.376511444205497
Validation loss: 2.9770021125537154

Epoch: 6| Step: 13
Training loss: 2.32206130993818
Validation loss: 2.977617975778336

Epoch: 42| Step: 0
Training loss: 3.2124538136290313
Validation loss: 2.9765947942144875

Epoch: 6| Step: 1
Training loss: 3.771163349919993
Validation loss: 2.9744706084503525

Epoch: 6| Step: 2
Training loss: 3.286394718157007
Validation loss: 2.982214438253206

Epoch: 6| Step: 3
Training loss: 2.954134168208488
Validation loss: 2.9802933385433876

Epoch: 6| Step: 4
Training loss: 3.254609287327617
Validation loss: 2.9769126189434982

Epoch: 6| Step: 5
Training loss: 3.38272891040453
Validation loss: 2.9764187392963954

Epoch: 6| Step: 6
Training loss: 2.669311810931018
Validation loss: 2.9879432344962398

Epoch: 6| Step: 7
Training loss: 3.4423233871200525
Validation loss: 2.973921640234875

Epoch: 6| Step: 8
Training loss: 3.0644055102060896
Validation loss: 2.975854826298697

Epoch: 6| Step: 9
Training loss: 2.9780668527184297
Validation loss: 2.9761756966688298

Epoch: 6| Step: 10
Training loss: 3.639348276922299
Validation loss: 2.9771504089500627

Epoch: 6| Step: 11
Training loss: 3.352368380176721
Validation loss: 2.9749521094631706

Epoch: 6| Step: 12
Training loss: 3.265580318455539
Validation loss: 2.9738909635065607

Epoch: 6| Step: 13
Training loss: 3.4229592473699237
Validation loss: 2.976249080813431

Epoch: 43| Step: 0
Training loss: 3.446900155467619
Validation loss: 2.9720755723542216

Epoch: 6| Step: 1
Training loss: 2.6436001421794026
Validation loss: 2.967962626697739

Epoch: 6| Step: 2
Training loss: 2.813380548599242
Validation loss: 2.967519409013652

Epoch: 6| Step: 3
Training loss: 3.7492257908482545
Validation loss: 2.9712137945130492

Epoch: 6| Step: 4
Training loss: 3.112199736669826
Validation loss: 2.9720371288247707

Epoch: 6| Step: 5
Training loss: 3.356654285255283
Validation loss: 2.975025827115284

Epoch: 6| Step: 6
Training loss: 3.7286891667067805
Validation loss: 2.967671542260188

Epoch: 6| Step: 7
Training loss: 2.5111035769789356
Validation loss: 2.963928843275884

Epoch: 6| Step: 8
Training loss: 3.0459778076084794
Validation loss: 2.965649164241143

Epoch: 6| Step: 9
Training loss: 3.4549815162793607
Validation loss: 2.9642397009475223

Epoch: 6| Step: 10
Training loss: 3.503602762166892
Validation loss: 2.9624031533245305

Epoch: 6| Step: 11
Training loss: 2.432656782847166
Validation loss: 2.965004802623119

Epoch: 6| Step: 12
Training loss: 3.9628321463310954
Validation loss: 2.964784816294727

Epoch: 6| Step: 13
Training loss: 3.6473117700840105
Validation loss: 2.9663639810369795

Epoch: 44| Step: 0
Training loss: 3.1883636501351176
Validation loss: 2.9645866868313733

Epoch: 6| Step: 1
Training loss: 2.7353225374162933
Validation loss: 2.9653168088675232

Epoch: 6| Step: 2
Training loss: 3.282525241635637
Validation loss: 2.9611593443608273

Epoch: 6| Step: 3
Training loss: 3.2344326438005653
Validation loss: 2.965167196981581

Epoch: 6| Step: 4
Training loss: 3.464087574704157
Validation loss: 2.9627709152664923

Epoch: 6| Step: 5
Training loss: 2.8484726142497596
Validation loss: 2.963189533840951

Epoch: 6| Step: 6
Training loss: 3.3318469230228795
Validation loss: 2.962808275432699

Epoch: 6| Step: 7
Training loss: 4.3166810927769825
Validation loss: 2.9646578570285285

Epoch: 6| Step: 8
Training loss: 2.933988704584244
Validation loss: 2.960682297043105

Epoch: 6| Step: 9
Training loss: 2.7387365814256333
Validation loss: 2.9601584542326886

Epoch: 6| Step: 10
Training loss: 3.637599490138954
Validation loss: 2.9614331382038754

Epoch: 6| Step: 11
Training loss: 2.576542779165736
Validation loss: 2.9614858409357816

Epoch: 6| Step: 12
Training loss: 3.310829605137566
Validation loss: 2.9627055232420307

Epoch: 6| Step: 13
Training loss: 3.8547794301699327
Validation loss: 2.962543475080117

Epoch: 45| Step: 0
Training loss: 3.7019333350528196
Validation loss: 2.9607540338608294

Epoch: 6| Step: 1
Training loss: 3.775241081169157
Validation loss: 2.9586415879105767

Epoch: 6| Step: 2
Training loss: 3.2652146661124593
Validation loss: 2.9592294307842866

Epoch: 6| Step: 3
Training loss: 3.6283459184598024
Validation loss: 2.95648010061876

Epoch: 6| Step: 4
Training loss: 2.9883712455811136
Validation loss: 2.957987848502248

Epoch: 6| Step: 5
Training loss: 3.3560300275439086
Validation loss: 2.956487296021192

Epoch: 6| Step: 6
Training loss: 3.059016835460993
Validation loss: 2.9566737113656267

Epoch: 6| Step: 7
Training loss: 3.550992617385396
Validation loss: 2.9581286457615334

Epoch: 6| Step: 8
Training loss: 2.886625915603846
Validation loss: 2.957548239795469

Epoch: 6| Step: 9
Training loss: 3.2324395769908034
Validation loss: 2.958468199421156

Epoch: 6| Step: 10
Training loss: 3.006163781846986
Validation loss: 2.9611731003214854

Epoch: 6| Step: 11
Training loss: 3.0288978181002864
Validation loss: 2.954543112266192

Epoch: 6| Step: 12
Training loss: 3.131197467903903
Validation loss: 2.9539535383676827

Epoch: 6| Step: 13
Training loss: 2.4404646621768538
Validation loss: 2.956949123061863

Epoch: 46| Step: 0
Training loss: 3.6199199992239124
Validation loss: 2.9558400134638747

Epoch: 6| Step: 1
Training loss: 2.3591901694146333
Validation loss: 2.953455944478123

Epoch: 6| Step: 2
Training loss: 3.0887450761805972
Validation loss: 2.955976273116434

Epoch: 6| Step: 3
Training loss: 3.1131634660024274
Validation loss: 2.9566790715824074

Epoch: 6| Step: 4
Training loss: 2.9541780723563225
Validation loss: 2.958287654826205

Epoch: 6| Step: 5
Training loss: 3.182017079861955
Validation loss: 2.9610106972990597

Epoch: 6| Step: 6
Training loss: 3.604647889103587
Validation loss: 2.964385050280673

Epoch: 6| Step: 7
Training loss: 3.4216372612013113
Validation loss: 2.9778289792075996

Epoch: 6| Step: 8
Training loss: 3.649615536651587
Validation loss: 2.9720628683098402

Epoch: 6| Step: 9
Training loss: 3.597815560821925
Validation loss: 2.9826420766912483

Epoch: 6| Step: 10
Training loss: 3.7272748925991492
Validation loss: 2.981438557392481

Epoch: 6| Step: 11
Training loss: 2.5946548905701636
Validation loss: 2.961699031536402

Epoch: 6| Step: 12
Training loss: 3.2313699076556586
Validation loss: 2.9537034022531246

Epoch: 6| Step: 13
Training loss: 2.9545265383881794
Validation loss: 2.9569759614334625

Epoch: 47| Step: 0
Training loss: 3.4940666587510747
Validation loss: 2.949419454650314

Epoch: 6| Step: 1
Training loss: 3.338293025636337
Validation loss: 2.951105478872097

Epoch: 6| Step: 2
Training loss: 3.0516037461110197
Validation loss: 2.95030133401401

Epoch: 6| Step: 3
Training loss: 3.0104819602001323
Validation loss: 2.9513964642582926

Epoch: 6| Step: 4
Training loss: 3.353388220133726
Validation loss: 2.950202312939723

Epoch: 6| Step: 5
Training loss: 3.4687208569651324
Validation loss: 2.9488640354383486

Epoch: 6| Step: 6
Training loss: 3.043416256006434
Validation loss: 2.949443021223983

Epoch: 6| Step: 7
Training loss: 3.0261122432145036
Validation loss: 2.949166061885828

Epoch: 6| Step: 8
Training loss: 3.061066428210515
Validation loss: 2.9482819603966175

Epoch: 6| Step: 9
Training loss: 3.44949813177874
Validation loss: 2.952377769105478

Epoch: 6| Step: 10
Training loss: 2.4566834511392717
Validation loss: 2.9502969128339847

Epoch: 6| Step: 11
Training loss: 3.5574315473199465
Validation loss: 2.949378477586218

Epoch: 6| Step: 12
Training loss: 3.716499481090282
Validation loss: 2.9500809047471828

Epoch: 6| Step: 13
Training loss: 3.3173317847764467
Validation loss: 2.94827300588965

Epoch: 48| Step: 0
Training loss: 2.9525060257297047
Validation loss: 2.9491264313057215

Epoch: 6| Step: 1
Training loss: 3.1501443557359554
Validation loss: 2.947679302614358

Epoch: 6| Step: 2
Training loss: 3.8561133065255837
Validation loss: 2.9508696768026748

Epoch: 6| Step: 3
Training loss: 2.9464345560384637
Validation loss: 2.9489247069625844

Epoch: 6| Step: 4
Training loss: 3.1887662279889217
Validation loss: 2.9488142690601364

Epoch: 6| Step: 5
Training loss: 3.597848164330204
Validation loss: 2.9455240151434925

Epoch: 6| Step: 6
Training loss: 3.2882110870989276
Validation loss: 2.944178721002221

Epoch: 6| Step: 7
Training loss: 3.37813902507062
Validation loss: 2.9480529310723718

Epoch: 6| Step: 8
Training loss: 3.3957657758549504
Validation loss: 2.944710107698262

Epoch: 6| Step: 9
Training loss: 3.357371597845628
Validation loss: 2.944748198457342

Epoch: 6| Step: 10
Training loss: 2.9561942161797177
Validation loss: 2.9414921558870115

Epoch: 6| Step: 11
Training loss: 3.13035247183131
Validation loss: 2.9463885552968163

Epoch: 6| Step: 12
Training loss: 2.8566948675709574
Validation loss: 2.946594825194686

Epoch: 6| Step: 13
Training loss: 3.268384433822156
Validation loss: 2.9479547988986154

Epoch: 49| Step: 0
Training loss: 3.4961409411095743
Validation loss: 2.946753484856358

Epoch: 6| Step: 1
Training loss: 3.1537916221699454
Validation loss: 2.94172223980512

Epoch: 6| Step: 2
Training loss: 3.2152980330515577
Validation loss: 2.9455191507460987

Epoch: 6| Step: 3
Training loss: 3.363686708828051
Validation loss: 2.9426314724049973

Epoch: 6| Step: 4
Training loss: 3.653657695218999
Validation loss: 2.9444528113171677

Epoch: 6| Step: 5
Training loss: 3.2676397068128606
Validation loss: 2.940749922250291

Epoch: 6| Step: 6
Training loss: 3.002866329362098
Validation loss: 2.942942381856718

Epoch: 6| Step: 7
Training loss: 2.885430529913438
Validation loss: 2.943904222873852

Epoch: 6| Step: 8
Training loss: 3.2383697460860956
Validation loss: 2.9460107972125904

Epoch: 6| Step: 9
Training loss: 3.4310083300701475
Validation loss: 2.944554598474901

Epoch: 6| Step: 10
Training loss: 2.633270495506005
Validation loss: 2.9398871143443

Epoch: 6| Step: 11
Training loss: 3.3203518494910966
Validation loss: 2.9411641563167388

Epoch: 6| Step: 12
Training loss: 2.9245103996875885
Validation loss: 2.940149585907818

Epoch: 6| Step: 13
Training loss: 3.8800007945974264
Validation loss: 2.946581785951256

Epoch: 50| Step: 0
Training loss: 3.7305082468867874
Validation loss: 2.9431955491566058

Epoch: 6| Step: 1
Training loss: 2.9772628000426127
Validation loss: 2.942032050675437

Epoch: 6| Step: 2
Training loss: 3.801376249154282
Validation loss: 2.9475122065105546

Epoch: 6| Step: 3
Training loss: 2.927473773008119
Validation loss: 2.9413169110033386

Epoch: 6| Step: 4
Training loss: 2.7775677940685757
Validation loss: 2.9424321422432858

Epoch: 6| Step: 5
Training loss: 3.190630553536808
Validation loss: 2.9411723209710137

Epoch: 6| Step: 6
Training loss: 4.1177588976570005
Validation loss: 2.938929042416114

Epoch: 6| Step: 7
Training loss: 3.556353972780538
Validation loss: 2.9382935610419634

Epoch: 6| Step: 8
Training loss: 3.5790705202013298
Validation loss: 2.9364251583651124

Epoch: 6| Step: 9
Training loss: 3.0768608169492233
Validation loss: 2.938179031885549

Epoch: 6| Step: 10
Training loss: 2.059297792321678
Validation loss: 2.938070047747676

Epoch: 6| Step: 11
Training loss: 3.017186210391037
Validation loss: 2.937326508624001

Epoch: 6| Step: 12
Training loss: 2.83866864308291
Validation loss: 2.939788407860897

Epoch: 6| Step: 13
Training loss: 2.9809680285551647
Validation loss: 2.937647237864403

Epoch: 51| Step: 0
Training loss: 3.514719935583195
Validation loss: 2.9373082404032744

Epoch: 6| Step: 1
Training loss: 2.987542194403668
Validation loss: 2.9373547429301725

Epoch: 6| Step: 2
Training loss: 3.5381068271964735
Validation loss: 2.9358721612302174

Epoch: 6| Step: 3
Training loss: 2.822234283590033
Validation loss: 2.9353778900100775

Epoch: 6| Step: 4
Training loss: 3.172975696825307
Validation loss: 2.9352699460115432

Epoch: 6| Step: 5
Training loss: 3.1852515274929227
Validation loss: 2.9356026195553713

Epoch: 6| Step: 6
Training loss: 3.4748434896008886
Validation loss: 2.9339045913510784

Epoch: 6| Step: 7
Training loss: 3.316060006586907
Validation loss: 2.936746453946856

Epoch: 6| Step: 8
Training loss: 3.210838000758409
Validation loss: 2.934707228634562

Epoch: 6| Step: 9
Training loss: 3.754562399975211
Validation loss: 2.933929275173409

Epoch: 6| Step: 10
Training loss: 2.9637418566478897
Validation loss: 2.9357337956828844

Epoch: 6| Step: 11
Training loss: 3.0526970429679094
Validation loss: 2.936715359208406

Epoch: 6| Step: 12
Training loss: 3.2084415136817297
Validation loss: 2.933622367162966

Epoch: 6| Step: 13
Training loss: 2.75323556078328
Validation loss: 2.931299834423926

Epoch: 52| Step: 0
Training loss: 2.6314794408657223
Validation loss: 2.936062902219445

Epoch: 6| Step: 1
Training loss: 3.848786690497752
Validation loss: 2.9348908729961525

Epoch: 6| Step: 2
Training loss: 2.8280944822572196
Validation loss: 2.9391522217372303

Epoch: 6| Step: 3
Training loss: 3.470270871630247
Validation loss: 2.940054352577186

Epoch: 6| Step: 4
Training loss: 3.3155927794937594
Validation loss: 2.9391258233316973

Epoch: 6| Step: 5
Training loss: 2.7273431660485716
Validation loss: 2.944111745711435

Epoch: 6| Step: 6
Training loss: 3.174819256301162
Validation loss: 2.950033046411481

Epoch: 6| Step: 7
Training loss: 3.552574921689724
Validation loss: 2.9420485703332386

Epoch: 6| Step: 8
Training loss: 2.3014841349948107
Validation loss: 2.957413799277306

Epoch: 6| Step: 9
Training loss: 3.223138724536979
Validation loss: 2.954123955749125

Epoch: 6| Step: 10
Training loss: 3.761576775617798
Validation loss: 2.9660492318004383

Epoch: 6| Step: 11
Training loss: 3.4763024393602957
Validation loss: 2.946743874951498

Epoch: 6| Step: 12
Training loss: 3.0209314966147063
Validation loss: 2.941181168965659

Epoch: 6| Step: 13
Training loss: 3.9034223163820356
Validation loss: 2.933726463048511

Epoch: 53| Step: 0
Training loss: 3.2143874742657887
Validation loss: 2.9354981229720782

Epoch: 6| Step: 1
Training loss: 3.292828431313111
Validation loss: 2.937442348720022

Epoch: 6| Step: 2
Training loss: 3.377291996083051
Validation loss: 2.9460182931702974

Epoch: 6| Step: 3
Training loss: 3.215057328871398
Validation loss: 2.941736826535978

Epoch: 6| Step: 4
Training loss: 3.490987618513177
Validation loss: 2.9396113869732288

Epoch: 6| Step: 5
Training loss: 3.520258719299714
Validation loss: 2.9405994510202227

Epoch: 6| Step: 6
Training loss: 3.91302500849433
Validation loss: 2.936889118881085

Epoch: 6| Step: 7
Training loss: 3.510105394699278
Validation loss: 2.936526042587419

Epoch: 6| Step: 8
Training loss: 3.0453924240029093
Validation loss: 2.93496478977037

Epoch: 6| Step: 9
Training loss: 3.3592540630484353
Validation loss: 2.9343611652061625

Epoch: 6| Step: 10
Training loss: 2.7155384958622415
Validation loss: 2.9351340885232933

Epoch: 6| Step: 11
Training loss: 2.476304194219844
Validation loss: 2.932380215207578

Epoch: 6| Step: 12
Training loss: 2.889306669918739
Validation loss: 2.934282740738196

Epoch: 6| Step: 13
Training loss: 3.086909891793687
Validation loss: 2.9332761294230476

Epoch: 54| Step: 0
Training loss: 2.2975911172392274
Validation loss: 2.933707815876708

Epoch: 6| Step: 1
Training loss: 3.169057412067734
Validation loss: 2.9335142851579588

Epoch: 6| Step: 2
Training loss: 3.4054337669723296
Validation loss: 2.9334399971162473

Epoch: 6| Step: 3
Training loss: 3.9353704597556733
Validation loss: 2.9336006537293216

Epoch: 6| Step: 4
Training loss: 3.897116274733032
Validation loss: 2.933690767754122

Epoch: 6| Step: 5
Training loss: 3.0299816820956935
Validation loss: 2.932591746076085

Epoch: 6| Step: 6
Training loss: 2.8869085391454843
Validation loss: 2.931250606825155

Epoch: 6| Step: 7
Training loss: 3.870459080162318
Validation loss: 2.9304519594444876

Epoch: 6| Step: 8
Training loss: 2.9683945292426666
Validation loss: 2.932856645846861

Epoch: 6| Step: 9
Training loss: 3.325285367264391
Validation loss: 2.9330027453986713

Epoch: 6| Step: 10
Training loss: 2.606091838035246
Validation loss: 2.9371832609413295

Epoch: 6| Step: 11
Training loss: 3.483603627305744
Validation loss: 2.938781204042964

Epoch: 6| Step: 12
Training loss: 2.7978791719841922
Validation loss: 2.941938093030809

Epoch: 6| Step: 13
Training loss: 3.0115922438674785
Validation loss: 2.9499999104550727

Epoch: 55| Step: 0
Training loss: 3.199323081899208
Validation loss: 2.936847321894738

Epoch: 6| Step: 1
Training loss: 2.5801217409731247
Validation loss: 2.9404188223707433

Epoch: 6| Step: 2
Training loss: 3.830548574139899
Validation loss: 2.938812426965604

Epoch: 6| Step: 3
Training loss: 3.7614123575166296
Validation loss: 2.9343301133374777

Epoch: 6| Step: 4
Training loss: 3.5949498039635515
Validation loss: 2.9316920243246796

Epoch: 6| Step: 5
Training loss: 2.9465986524761756
Validation loss: 2.936791772061706

Epoch: 6| Step: 6
Training loss: 2.7581560390998456
Validation loss: 2.9316301776521962

Epoch: 6| Step: 7
Training loss: 2.9374964287918783
Validation loss: 2.9308252718992924

Epoch: 6| Step: 8
Training loss: 2.9344328430779716
Validation loss: 2.93894014159591

Epoch: 6| Step: 9
Training loss: 2.763004332890739
Validation loss: 2.9335776056966454

Epoch: 6| Step: 10
Training loss: 3.9054861313672906
Validation loss: 2.9356869469902627

Epoch: 6| Step: 11
Training loss: 2.4482943845403033
Validation loss: 2.931826301223582

Epoch: 6| Step: 12
Training loss: 3.617126596156412
Validation loss: 2.929627514296847

Epoch: 6| Step: 13
Training loss: 3.63216350049608
Validation loss: 2.9299664230743874

Epoch: 56| Step: 0
Training loss: 2.890335810861489
Validation loss: 2.9270407879024454

Epoch: 6| Step: 1
Training loss: 2.9254655451228073
Validation loss: 2.9283406069586997

Epoch: 6| Step: 2
Training loss: 2.8723721311576385
Validation loss: 2.924413803226733

Epoch: 6| Step: 3
Training loss: 3.18941963584684
Validation loss: 2.925208854808955

Epoch: 6| Step: 4
Training loss: 3.3348285341579236
Validation loss: 2.924285674221026

Epoch: 6| Step: 5
Training loss: 3.2029624339463436
Validation loss: 2.9213970504768665

Epoch: 6| Step: 6
Training loss: 3.3618065838749933
Validation loss: 2.921180443816373

Epoch: 6| Step: 7
Training loss: 3.731563004187874
Validation loss: 2.9201997274732294

Epoch: 6| Step: 8
Training loss: 3.2125771598603885
Validation loss: 2.9204808461960474

Epoch: 6| Step: 9
Training loss: 4.084621589531041
Validation loss: 2.92033079222999

Epoch: 6| Step: 10
Training loss: 2.562828229026312
Validation loss: 2.9212918140721436

Epoch: 6| Step: 11
Training loss: 3.5912986023930524
Validation loss: 2.9225672656950534

Epoch: 6| Step: 12
Training loss: 3.062187023129715
Validation loss: 2.9221551091971945

Epoch: 6| Step: 13
Training loss: 2.44114031754778
Validation loss: 2.921639390445636

Epoch: 57| Step: 0
Training loss: 3.0353067525225814
Validation loss: 2.9258610589258764

Epoch: 6| Step: 1
Training loss: 2.595229864211589
Validation loss: 2.925776176656889

Epoch: 6| Step: 2
Training loss: 3.204360351529018
Validation loss: 2.937109231539226

Epoch: 6| Step: 3
Training loss: 3.1105923977109673
Validation loss: 2.941011608131066

Epoch: 6| Step: 4
Training loss: 3.7932614057411276
Validation loss: 2.945899598261589

Epoch: 6| Step: 5
Training loss: 3.8940493056864582
Validation loss: 2.931150470753468

Epoch: 6| Step: 6
Training loss: 2.69382008684857
Validation loss: 2.9208662928385944

Epoch: 6| Step: 7
Training loss: 3.9146020330876055
Validation loss: 2.918120680718308

Epoch: 6| Step: 8
Training loss: 3.217317132621581
Validation loss: 2.9213412630177213

Epoch: 6| Step: 9
Training loss: 3.148027795681031
Validation loss: 2.9253205054979268

Epoch: 6| Step: 10
Training loss: 2.979571086047709
Validation loss: 2.9238795599536815

Epoch: 6| Step: 11
Training loss: 3.534909807751534
Validation loss: 2.9254061949495704

Epoch: 6| Step: 12
Training loss: 2.5730746654436114
Validation loss: 2.9214015733107637

Epoch: 6| Step: 13
Training loss: 2.952373429184805
Validation loss: 2.9223279028944593

Epoch: 58| Step: 0
Training loss: 2.7826328911455005
Validation loss: 2.925061665696459

Epoch: 6| Step: 1
Training loss: 3.539181873590362
Validation loss: 2.9288934015634975

Epoch: 6| Step: 2
Training loss: 3.0905856508965
Validation loss: 2.934907107907475

Epoch: 6| Step: 3
Training loss: 3.000806858913848
Validation loss: 2.9315067584879806

Epoch: 6| Step: 4
Training loss: 3.6541810009688622
Validation loss: 2.9296285372580817

Epoch: 6| Step: 5
Training loss: 3.828711017207187
Validation loss: 2.9164279810781144

Epoch: 6| Step: 6
Training loss: 2.9437138192773915
Validation loss: 2.915019054869447

Epoch: 6| Step: 7
Training loss: 2.9875761908228684
Validation loss: 2.9169344446574814

Epoch: 6| Step: 8
Training loss: 3.4878935561080753
Validation loss: 2.9152676005804166

Epoch: 6| Step: 9
Training loss: 3.136797798272323
Validation loss: 2.9136801364367146

Epoch: 6| Step: 10
Training loss: 2.9507084399939556
Validation loss: 2.915924038957856

Epoch: 6| Step: 11
Training loss: 3.24710775941753
Validation loss: 2.9149298974393596

Epoch: 6| Step: 12
Training loss: 2.965066491807947
Validation loss: 2.914511128145104

Epoch: 6| Step: 13
Training loss: 3.370726670383997
Validation loss: 2.9154445924085812

Epoch: 59| Step: 0
Training loss: 3.230045535354173
Validation loss: 2.916545277953944

Epoch: 6| Step: 1
Training loss: 2.4079710515205406
Validation loss: 2.913827057609321

Epoch: 6| Step: 2
Training loss: 3.011601268886502
Validation loss: 2.913544989797098

Epoch: 6| Step: 3
Training loss: 3.6345784150891736
Validation loss: 2.9166326966189926

Epoch: 6| Step: 4
Training loss: 3.5337194399731757
Validation loss: 2.912471228671376

Epoch: 6| Step: 5
Training loss: 3.3728913678325334
Validation loss: 2.9146301197050697

Epoch: 6| Step: 6
Training loss: 3.0626581890745608
Validation loss: 2.91223599654832

Epoch: 6| Step: 7
Training loss: 2.777052036422884
Validation loss: 2.9138405064927113

Epoch: 6| Step: 8
Training loss: 3.541951530818495
Validation loss: 2.915529115990669

Epoch: 6| Step: 9
Training loss: 2.619018824943363
Validation loss: 2.9144640403444075

Epoch: 6| Step: 10
Training loss: 2.9280747514156684
Validation loss: 2.9125927205778814

Epoch: 6| Step: 11
Training loss: 3.155813961313761
Validation loss: 2.9150774072898264

Epoch: 6| Step: 12
Training loss: 3.8932137207269197
Validation loss: 2.9150246851624875

Epoch: 6| Step: 13
Training loss: 3.7337022758016167
Validation loss: 2.9138927036905367

Epoch: 60| Step: 0
Training loss: 3.8269084632278956
Validation loss: 2.9137247310941734

Epoch: 6| Step: 1
Training loss: 3.9816122611858242
Validation loss: 2.913122599857068

Epoch: 6| Step: 2
Training loss: 3.2242293154093007
Validation loss: 2.9109961931204005

Epoch: 6| Step: 3
Training loss: 3.489235400414308
Validation loss: 2.910673938179776

Epoch: 6| Step: 4
Training loss: 2.716287923127691
Validation loss: 2.912224469880511

Epoch: 6| Step: 5
Training loss: 2.8972099563210567
Validation loss: 2.9105390060385616

Epoch: 6| Step: 6
Training loss: 2.266204444187511
Validation loss: 2.909828330421712

Epoch: 6| Step: 7
Training loss: 3.007069998907993
Validation loss: 2.910114234675526

Epoch: 6| Step: 8
Training loss: 3.485072455289233
Validation loss: 2.9100633589735536

Epoch: 6| Step: 9
Training loss: 2.4260615434128536
Validation loss: 2.909369992297378

Epoch: 6| Step: 10
Training loss: 2.7684649525250227
Validation loss: 2.9091222451901584

Epoch: 6| Step: 11
Training loss: 3.781927883451111
Validation loss: 2.9087047162664548

Epoch: 6| Step: 12
Training loss: 3.5932586334003775
Validation loss: 2.9092215638716614

Epoch: 6| Step: 13
Training loss: 2.6548900039789856
Validation loss: 2.9075490196560203

Epoch: 61| Step: 0
Training loss: 3.1911597092463095
Validation loss: 2.908936521040205

Epoch: 6| Step: 1
Training loss: 2.8629726436159433
Validation loss: 2.9083228330184334

Epoch: 6| Step: 2
Training loss: 3.20798239501917
Validation loss: 2.9083140111111465

Epoch: 6| Step: 3
Training loss: 2.6366418897941166
Validation loss: 2.9086742595380315

Epoch: 6| Step: 4
Training loss: 3.4941035056021637
Validation loss: 2.9083143231574846

Epoch: 6| Step: 5
Training loss: 3.72611526588623
Validation loss: 2.904742668285614

Epoch: 6| Step: 6
Training loss: 3.057840812446772
Validation loss: 2.9060505915363364

Epoch: 6| Step: 7
Training loss: 3.329499010311907
Validation loss: 2.907542522262916

Epoch: 6| Step: 8
Training loss: 3.3805478027917975
Validation loss: 2.9078212058537884

Epoch: 6| Step: 9
Training loss: 3.3135637698515112
Validation loss: 2.905974938004238

Epoch: 6| Step: 10
Training loss: 3.328417697984046
Validation loss: 2.906119637548752

Epoch: 6| Step: 11
Training loss: 2.9248159366958126
Validation loss: 2.9034000147915795

Epoch: 6| Step: 12
Training loss: 3.3343722313938247
Validation loss: 2.9067113490093357

Epoch: 6| Step: 13
Training loss: 2.8211464956460945
Validation loss: 2.909138110146191

Epoch: 62| Step: 0
Training loss: 3.435005114911087
Validation loss: 2.905416340190319

Epoch: 6| Step: 1
Training loss: 2.5428781800907685
Validation loss: 2.9061330259460494

Epoch: 6| Step: 2
Training loss: 3.316800331093256
Validation loss: 2.906235162823033

Epoch: 6| Step: 3
Training loss: 2.886225471111495
Validation loss: 2.9070986530705296

Epoch: 6| Step: 4
Training loss: 3.484692614666806
Validation loss: 2.9085940371960457

Epoch: 6| Step: 5
Training loss: 3.714905105549441
Validation loss: 2.9038112246553736

Epoch: 6| Step: 6
Training loss: 2.5324217356632213
Validation loss: 2.904890019427124

Epoch: 6| Step: 7
Training loss: 3.1026630899135212
Validation loss: 2.9074741034529428

Epoch: 6| Step: 8
Training loss: 3.6007958380216807
Validation loss: 2.910167985710645

Epoch: 6| Step: 9
Training loss: 3.0634046502955883
Validation loss: 2.9083018976932538

Epoch: 6| Step: 10
Training loss: 3.7250753510297763
Validation loss: 2.9045514574729214

Epoch: 6| Step: 11
Training loss: 3.387963722836456
Validation loss: 2.9107129917967116

Epoch: 6| Step: 12
Training loss: 2.4374768182679207
Validation loss: 2.9066337378859672

Epoch: 6| Step: 13
Training loss: 3.3223015067323045
Validation loss: 2.9077607091070488

Epoch: 63| Step: 0
Training loss: 3.297529833516247
Validation loss: 2.9047034404028884

Epoch: 6| Step: 1
Training loss: 3.2682035204525794
Validation loss: 2.91051058558708

Epoch: 6| Step: 2
Training loss: 3.417253691944012
Validation loss: 2.908405637584222

Epoch: 6| Step: 3
Training loss: 3.5825209620861425
Validation loss: 2.9096162592209436

Epoch: 6| Step: 4
Training loss: 3.457056353634136
Validation loss: 2.906470419307185

Epoch: 6| Step: 5
Training loss: 2.642960638427916
Validation loss: 2.9047750407967996

Epoch: 6| Step: 6
Training loss: 3.263844273760526
Validation loss: 2.9034578864107545

Epoch: 6| Step: 7
Training loss: 3.050190066059313
Validation loss: 2.904720225309761

Epoch: 6| Step: 8
Training loss: 2.89799261663219
Validation loss: 2.902755283923347

Epoch: 6| Step: 9
Training loss: 2.7800846873221077
Validation loss: 2.8975622422632004

Epoch: 6| Step: 10
Training loss: 3.6248168899065183
Validation loss: 2.8988827961818022

Epoch: 6| Step: 11
Training loss: 3.0931886105828803
Validation loss: 2.901518821229616

Epoch: 6| Step: 12
Training loss: 3.2894869641487694
Validation loss: 2.901064223229063

Epoch: 6| Step: 13
Training loss: 2.8904933590168493
Validation loss: 2.8989911957015635

Epoch: 64| Step: 0
Training loss: 3.9001003839338217
Validation loss: 2.8981738160599817

Epoch: 6| Step: 1
Training loss: 3.5064912274450957
Validation loss: 2.8991858062533513

Epoch: 6| Step: 2
Training loss: 3.2559120984033485
Validation loss: 2.897622802413224

Epoch: 6| Step: 3
Training loss: 3.1759473093307937
Validation loss: 2.8997238812025903

Epoch: 6| Step: 4
Training loss: 3.4891841527262133
Validation loss: 2.8964056746509743

Epoch: 6| Step: 5
Training loss: 3.17193032202371
Validation loss: 2.8997808510462364

Epoch: 6| Step: 6
Training loss: 3.0316649477385655
Validation loss: 2.9007751936893627

Epoch: 6| Step: 7
Training loss: 3.2559320159113163
Validation loss: 2.9012853113218298

Epoch: 6| Step: 8
Training loss: 3.1496740142233803
Validation loss: 2.9072994128654464

Epoch: 6| Step: 9
Training loss: 2.7498970879458766
Validation loss: 2.903350023633958

Epoch: 6| Step: 10
Training loss: 3.1727200589764983
Validation loss: 2.9067405792081886

Epoch: 6| Step: 11
Training loss: 2.4866165509257536
Validation loss: 2.9108156544969654

Epoch: 6| Step: 12
Training loss: 3.5726503026375984
Validation loss: 2.90990547222292

Epoch: 6| Step: 13
Training loss: 2.1488367437991807
Validation loss: 2.8997928426936976

Epoch: 65| Step: 0
Training loss: 3.5980766456671964
Validation loss: 2.8966045159865073

Epoch: 6| Step: 1
Training loss: 2.9449297507024284
Validation loss: 2.8958539120277944

Epoch: 6| Step: 2
Training loss: 2.350829659772038
Validation loss: 2.8953838213450434

Epoch: 6| Step: 3
Training loss: 3.327220668518119
Validation loss: 2.8938522278640155

Epoch: 6| Step: 4
Training loss: 3.2313353772569786
Validation loss: 2.8965610393353622

Epoch: 6| Step: 5
Training loss: 3.4896170695178172
Validation loss: 2.896988559866183

Epoch: 6| Step: 6
Training loss: 3.67940784209653
Validation loss: 2.8981389460596905

Epoch: 6| Step: 7
Training loss: 2.8543677340407303
Validation loss: 2.8973799516604037

Epoch: 6| Step: 8
Training loss: 2.9023335637649312
Validation loss: 2.898363413238531

Epoch: 6| Step: 9
Training loss: 4.1836899462271475
Validation loss: 2.8964506768870257

Epoch: 6| Step: 10
Training loss: 3.27336262546752
Validation loss: 2.8980248026986333

Epoch: 6| Step: 11
Training loss: 2.3142608949046903
Validation loss: 2.8983770179038024

Epoch: 6| Step: 12
Training loss: 3.2551015281957496
Validation loss: 2.896602821114727

Epoch: 6| Step: 13
Training loss: 2.728826681222599
Validation loss: 2.8985168907862406

Epoch: 66| Step: 0
Training loss: 2.7111192801705086
Validation loss: 2.8971833412195007

Epoch: 6| Step: 1
Training loss: 3.058082820461898
Validation loss: 2.8985246148205253

Epoch: 6| Step: 2
Training loss: 2.6893487825563014
Validation loss: 2.8978702348386234

Epoch: 6| Step: 3
Training loss: 3.49987084286664
Validation loss: 2.8968857075930874

Epoch: 6| Step: 4
Training loss: 2.6686087827819147
Validation loss: 2.896947922575119

Epoch: 6| Step: 5
Training loss: 2.473149496000076
Validation loss: 2.8966764696805134

Epoch: 6| Step: 6
Training loss: 3.4858581397146446
Validation loss: 2.896099955325414

Epoch: 6| Step: 7
Training loss: 3.629489354978917
Validation loss: 2.892264863985527

Epoch: 6| Step: 8
Training loss: 3.449865122936704
Validation loss: 2.895897010663204

Epoch: 6| Step: 9
Training loss: 3.3449168798500013
Validation loss: 2.894570923066805

Epoch: 6| Step: 10
Training loss: 2.80788730018597
Validation loss: 2.894924065036109

Epoch: 6| Step: 11
Training loss: 3.379344545724188
Validation loss: 2.8943566090609276

Epoch: 6| Step: 12
Training loss: 3.254466582156744
Validation loss: 2.8970250047155974

Epoch: 6| Step: 13
Training loss: 4.459142213451502
Validation loss: 2.8957508193735655

Epoch: 67| Step: 0
Training loss: 3.4390405150719934
Validation loss: 2.8970415403175074

Epoch: 6| Step: 1
Training loss: 2.9381456680074023
Validation loss: 2.8956444083239203

Epoch: 6| Step: 2
Training loss: 2.1567330303132537
Validation loss: 2.89111149670271

Epoch: 6| Step: 3
Training loss: 3.1199750554481445
Validation loss: 2.894797699374875

Epoch: 6| Step: 4
Training loss: 3.602357060960687
Validation loss: 2.890052718805686

Epoch: 6| Step: 5
Training loss: 3.0537763636759165
Validation loss: 2.8941925870574043

Epoch: 6| Step: 6
Training loss: 2.7166538269426037
Validation loss: 2.8921655400529334

Epoch: 6| Step: 7
Training loss: 3.582805653834633
Validation loss: 2.89046206116237

Epoch: 6| Step: 8
Training loss: 2.8976144778896287
Validation loss: 2.889846029864193

Epoch: 6| Step: 9
Training loss: 3.62663363773602
Validation loss: 2.890218348835289

Epoch: 6| Step: 10
Training loss: 2.9786633864475496
Validation loss: 2.893391016093627

Epoch: 6| Step: 11
Training loss: 3.498213039351515
Validation loss: 2.8911124348648514

Epoch: 6| Step: 12
Training loss: 3.4685552044033625
Validation loss: 2.8905244383521143

Epoch: 6| Step: 13
Training loss: 3.3635801034107313
Validation loss: 2.8890015730272314

Epoch: 68| Step: 0
Training loss: 3.111670791903845
Validation loss: 2.8890040221934834

Epoch: 6| Step: 1
Training loss: 3.474910180557111
Validation loss: 2.887702796482974

Epoch: 6| Step: 2
Training loss: 3.132252935726797
Validation loss: 2.8901079162689594

Epoch: 6| Step: 3
Training loss: 4.095177787302908
Validation loss: 2.89114888639597

Epoch: 6| Step: 4
Training loss: 2.633270495506005
Validation loss: 2.895410572519149

Epoch: 6| Step: 5
Training loss: 3.1620332215100313
Validation loss: 2.8915561833531553

Epoch: 6| Step: 6
Training loss: 3.956304183998708
Validation loss: 2.8933258848286765

Epoch: 6| Step: 7
Training loss: 2.690217972471822
Validation loss: 2.894700524102852

Epoch: 6| Step: 8
Training loss: 2.9498062458096097
Validation loss: 2.890044782298469

Epoch: 6| Step: 9
Training loss: 2.269178773490707
Validation loss: 2.8985156560709626

Epoch: 6| Step: 10
Training loss: 3.3368273225026486
Validation loss: 2.8966589779284897

Epoch: 6| Step: 11
Training loss: 3.104185595017313
Validation loss: 2.8985557734348273

Epoch: 6| Step: 12
Training loss: 3.515311400943801
Validation loss: 2.90032150005536

Epoch: 6| Step: 13
Training loss: 2.413160532126841
Validation loss: 2.8945664043643515

Epoch: 69| Step: 0
Training loss: 2.805861033646663
Validation loss: 2.890441571207952

Epoch: 6| Step: 1
Training loss: 2.767409185558088
Validation loss: 2.8930746361953914

Epoch: 6| Step: 2
Training loss: 3.243803278902597
Validation loss: 2.8934182180724606

Epoch: 6| Step: 3
Training loss: 3.049363904816712
Validation loss: 2.885873288344125

Epoch: 6| Step: 4
Training loss: 2.8734033546265145
Validation loss: 2.888827758559833

Epoch: 6| Step: 5
Training loss: 3.485024156527761
Validation loss: 2.8872328037763024

Epoch: 6| Step: 6
Training loss: 3.5930890346232696
Validation loss: 2.8872154679141

Epoch: 6| Step: 7
Training loss: 2.909913082332274
Validation loss: 2.8907395738966875

Epoch: 6| Step: 8
Training loss: 3.4970077257646452
Validation loss: 2.8889032071150273

Epoch: 6| Step: 9
Training loss: 3.316931010180035
Validation loss: 2.8854898245278924

Epoch: 6| Step: 10
Training loss: 3.3380120502526403
Validation loss: 2.883320965853894

Epoch: 6| Step: 11
Training loss: 3.3160597189942953
Validation loss: 2.8872255885043625

Epoch: 6| Step: 12
Training loss: 3.11394666886033
Validation loss: 2.884533436696863

Epoch: 6| Step: 13
Training loss: 3.3052441283767444
Validation loss: 2.889107470950276

Epoch: 70| Step: 0
Training loss: 2.61274127484895
Validation loss: 2.885631274463133

Epoch: 6| Step: 1
Training loss: 3.3305854120119096
Validation loss: 2.8817087915190402

Epoch: 6| Step: 2
Training loss: 3.2064699153324563
Validation loss: 2.882816043796779

Epoch: 6| Step: 3
Training loss: 3.5262331814315826
Validation loss: 2.8846794019610162

Epoch: 6| Step: 4
Training loss: 3.4934628609189646
Validation loss: 2.883578458195763

Epoch: 6| Step: 5
Training loss: 2.6339233952579946
Validation loss: 2.882333939815692

Epoch: 6| Step: 6
Training loss: 3.3369535020167094
Validation loss: 2.8801819592936364

Epoch: 6| Step: 7
Training loss: 2.3490432983279455
Validation loss: 2.8836593844632397

Epoch: 6| Step: 8
Training loss: 3.556365771831092
Validation loss: 2.8822459736281614

Epoch: 6| Step: 9
Training loss: 3.4715573059916753
Validation loss: 2.8836005794116564

Epoch: 6| Step: 10
Training loss: 3.304735981470617
Validation loss: 2.8811483879412876

Epoch: 6| Step: 11
Training loss: 2.7976369832091037
Validation loss: 2.884211567142275

Epoch: 6| Step: 12
Training loss: 3.2826479885562603
Validation loss: 2.8827191413955715

Epoch: 6| Step: 13
Training loss: 3.594442416948326
Validation loss: 2.8832859624608096

Epoch: 71| Step: 0
Training loss: 3.41854250206721
Validation loss: 2.8826597257579163

Epoch: 6| Step: 1
Training loss: 2.3960688820746543
Validation loss: 2.883773955076353

Epoch: 6| Step: 2
Training loss: 3.621476137779166
Validation loss: 2.8855392642060678

Epoch: 6| Step: 3
Training loss: 3.872013910371053
Validation loss: 2.885490640133531

Epoch: 6| Step: 4
Training loss: 2.538686495596651
Validation loss: 2.8898439681962356

Epoch: 6| Step: 5
Training loss: 3.1956879411538806
Validation loss: 2.8827191022657837

Epoch: 6| Step: 6
Training loss: 3.5004777582249593
Validation loss: 2.8810875811603616

Epoch: 6| Step: 7
Training loss: 3.0910020434330554
Validation loss: 2.8809057359839194

Epoch: 6| Step: 8
Training loss: 3.5591369281354956
Validation loss: 2.8817333610818805

Epoch: 6| Step: 9
Training loss: 2.9247878951416943
Validation loss: 2.8804035492593325

Epoch: 6| Step: 10
Training loss: 2.9371779244976377
Validation loss: 2.8812035219793266

Epoch: 6| Step: 11
Training loss: 3.14144142888239
Validation loss: 2.8825590978998945

Epoch: 6| Step: 12
Training loss: 2.7189665960215037
Validation loss: 2.8828484624431527

Epoch: 6| Step: 13
Training loss: 3.4644678851673536
Validation loss: 2.8828256720527583

Epoch: 72| Step: 0
Training loss: 2.5852546827633356
Validation loss: 2.8874185459818906

Epoch: 6| Step: 1
Training loss: 3.1911193644004547
Validation loss: 2.881148447557766

Epoch: 6| Step: 2
Training loss: 2.854469970031994
Validation loss: 2.880123068109604

Epoch: 6| Step: 3
Training loss: 3.874565592381046
Validation loss: 2.876190463397012

Epoch: 6| Step: 4
Training loss: 3.7565464099589905
Validation loss: 2.880055585923536

Epoch: 6| Step: 5
Training loss: 3.3995123906258313
Validation loss: 2.8796244430731224

Epoch: 6| Step: 6
Training loss: 2.892869562462145
Validation loss: 2.8812258198091185

Epoch: 6| Step: 7
Training loss: 2.394690453867165
Validation loss: 2.879203452120798

Epoch: 6| Step: 8
Training loss: 2.9637878709012173
Validation loss: 2.8803098188554492

Epoch: 6| Step: 9
Training loss: 2.8519316969901074
Validation loss: 2.8821159779912664

Epoch: 6| Step: 10
Training loss: 3.2192478720777995
Validation loss: 2.880835719914387

Epoch: 6| Step: 11
Training loss: 3.318262090935281
Validation loss: 2.8753197587130583

Epoch: 6| Step: 12
Training loss: 3.6142510839490867
Validation loss: 2.8767261789178713

Epoch: 6| Step: 13
Training loss: 3.360114295695782
Validation loss: 2.876647989621696

Epoch: 73| Step: 0
Training loss: 3.7992296492090603
Validation loss: 2.8801128851764366

Epoch: 6| Step: 1
Training loss: 2.81019056658224
Validation loss: 2.8769993306156696

Epoch: 6| Step: 2
Training loss: 2.7787373368922377
Validation loss: 2.879038141190081

Epoch: 6| Step: 3
Training loss: 3.704268781858567
Validation loss: 2.8791161239981946

Epoch: 6| Step: 4
Training loss: 2.6495634403152852
Validation loss: 2.8765378542225153

Epoch: 6| Step: 5
Training loss: 2.836817245013151
Validation loss: 2.874716292191442

Epoch: 6| Step: 6
Training loss: 3.273186940365923
Validation loss: 2.877007066974116

Epoch: 6| Step: 7
Training loss: 3.3157138708286586
Validation loss: 2.8779326386968647

Epoch: 6| Step: 8
Training loss: 3.0563598113819745
Validation loss: 2.8764342377198933

Epoch: 6| Step: 9
Training loss: 2.6948234667313002
Validation loss: 2.8748398852604127

Epoch: 6| Step: 10
Training loss: 2.7700194021433737
Validation loss: 2.876058965744467

Epoch: 6| Step: 11
Training loss: 3.423071247185145
Validation loss: 2.8738848470523948

Epoch: 6| Step: 12
Training loss: 3.5633942669570295
Validation loss: 2.8745834642716233

Epoch: 6| Step: 13
Training loss: 3.8260152180075413
Validation loss: 2.873491943027756

Epoch: 74| Step: 0
Training loss: 2.7584523439423907
Validation loss: 2.872813959082459

Epoch: 6| Step: 1
Training loss: 3.4892508429034086
Validation loss: 2.875035235352677

Epoch: 6| Step: 2
Training loss: 3.819532443487426
Validation loss: 2.8771824660769774

Epoch: 6| Step: 3
Training loss: 2.80063312729395
Validation loss: 2.8802366480996633

Epoch: 6| Step: 4
Training loss: 3.382716505701709
Validation loss: 2.8822631499265245

Epoch: 6| Step: 5
Training loss: 3.6036474188297447
Validation loss: 2.8818180514302543

Epoch: 6| Step: 6
Training loss: 3.182000446053407
Validation loss: 2.876779422921666

Epoch: 6| Step: 7
Training loss: 3.2171861127023176
Validation loss: 2.87430750713697

Epoch: 6| Step: 8
Training loss: 3.02028584396985
Validation loss: 2.8711992780288442

Epoch: 6| Step: 9
Training loss: 2.7736625472624508
Validation loss: 2.8748175218480196

Epoch: 6| Step: 10
Training loss: 2.9841810233303345
Validation loss: 2.8746351248612463

Epoch: 6| Step: 11
Training loss: 3.2928117780276445
Validation loss: 2.872600518934522

Epoch: 6| Step: 12
Training loss: 2.8481116748303053
Validation loss: 2.872565531271295

Epoch: 6| Step: 13
Training loss: 3.140062879673855
Validation loss: 2.8749047130257974

Epoch: 75| Step: 0
Training loss: 2.921124040518515
Validation loss: 2.8746959941082295

Epoch: 6| Step: 1
Training loss: 3.7646756696891193
Validation loss: 2.8755442784939116

Epoch: 6| Step: 2
Training loss: 2.411814510222536
Validation loss: 2.8760332923114724

Epoch: 6| Step: 3
Training loss: 2.983572647327751
Validation loss: 2.877342942277815

Epoch: 6| Step: 4
Training loss: 3.279647871980248
Validation loss: 2.881870908688977

Epoch: 6| Step: 5
Training loss: 2.858597613277572
Validation loss: 2.8767876696076127

Epoch: 6| Step: 6
Training loss: 3.3680675091263708
Validation loss: 2.876587340812409

Epoch: 6| Step: 7
Training loss: 2.981617237560576
Validation loss: 2.8774169011622663

Epoch: 6| Step: 8
Training loss: 3.462779637691932
Validation loss: 2.877929950284027

Epoch: 6| Step: 9
Training loss: 3.1672266080273888
Validation loss: 2.8794403580297243

Epoch: 6| Step: 10
Training loss: 2.7847736625539343
Validation loss: 2.878560276948253

Epoch: 6| Step: 11
Training loss: 4.135635997935732
Validation loss: 2.8858548196679106

Epoch: 6| Step: 12
Training loss: 1.9258989835889664
Validation loss: 2.880528819796673

Epoch: 6| Step: 13
Training loss: 4.270821672129024
Validation loss: 2.880862957684691

Testing loss: 3.0865522198919284

Epoch: 1| Step: 0
Training loss: 5.656140327453613
Validation loss: 5.133774890694567

Epoch: 5| Step: 1
Training loss: 4.452817440032959
Validation loss: 5.1298801770774265

Epoch: 5| Step: 2
Training loss: 4.513121128082275
Validation loss: 5.125933370282573

Epoch: 5| Step: 3
Training loss: 5.202631950378418
Validation loss: 5.1219724480823805

Epoch: 5| Step: 4
Training loss: 4.899104595184326
Validation loss: 5.117784130957819

Epoch: 5| Step: 5
Training loss: 4.238312721252441
Validation loss: 5.113125831850113

Epoch: 5| Step: 6
Training loss: 4.035156726837158
Validation loss: 5.108589941455472

Epoch: 5| Step: 7
Training loss: 5.07925271987915
Validation loss: 5.103680528620238

Epoch: 5| Step: 8
Training loss: 5.926206111907959
Validation loss: 5.099042477146272

Epoch: 5| Step: 9
Training loss: 4.816832065582275
Validation loss: 5.093895194351032

Epoch: 5| Step: 10
Training loss: 5.218456268310547
Validation loss: 5.088453805574807

Epoch: 2| Step: 0
Training loss: 4.941193580627441
Validation loss: 5.082681337992351

Epoch: 5| Step: 1
Training loss: 3.436300754547119
Validation loss: 5.076971177131899

Epoch: 5| Step: 2
Training loss: 5.1316938400268555
Validation loss: 5.070904798405145

Epoch: 5| Step: 3
Training loss: 4.857723236083984
Validation loss: 5.064469957864413

Epoch: 5| Step: 4
Training loss: 4.974991321563721
Validation loss: 5.058087471992739

Epoch: 5| Step: 5
Training loss: 4.612628936767578
Validation loss: 5.051283323636619

Epoch: 5| Step: 6
Training loss: 5.0908989906311035
Validation loss: 5.043889430261427

Epoch: 5| Step: 7
Training loss: 6.339386940002441
Validation loss: 5.036359299895584

Epoch: 5| Step: 8
Training loss: 4.658965110778809
Validation loss: 5.028195699055989

Epoch: 5| Step: 9
Training loss: 4.188848972320557
Validation loss: 5.01993146506689

Epoch: 5| Step: 10
Training loss: 5.107393741607666
Validation loss: 5.010989804421702

Epoch: 3| Step: 0
Training loss: 4.281242847442627
Validation loss: 5.002503374571441

Epoch: 5| Step: 1
Training loss: 5.796883583068848
Validation loss: 4.992197252088977

Epoch: 5| Step: 2
Training loss: 4.339747905731201
Validation loss: 4.981947339991088

Epoch: 5| Step: 3
Training loss: 5.315596580505371
Validation loss: 4.971843893809985

Epoch: 5| Step: 4
Training loss: 6.350361347198486
Validation loss: 4.960759280830302

Epoch: 5| Step: 5
Training loss: 4.933346748352051
Validation loss: 4.949593144078409

Epoch: 5| Step: 6
Training loss: 4.017998695373535
Validation loss: 4.937651741889216

Epoch: 5| Step: 7
Training loss: 4.274814605712891
Validation loss: 4.924956516552997

Epoch: 5| Step: 8
Training loss: 4.419442176818848
Validation loss: 4.912238146669122

Epoch: 5| Step: 9
Training loss: 4.090941905975342
Validation loss: 4.898336354122367

Epoch: 5| Step: 10
Training loss: 4.267024040222168
Validation loss: 4.884613513946533

Epoch: 4| Step: 0
Training loss: 3.0681965351104736
Validation loss: 4.870810713819278

Epoch: 5| Step: 1
Training loss: 5.212008476257324
Validation loss: 4.855580081221878

Epoch: 5| Step: 2
Training loss: 4.747210502624512
Validation loss: 4.839240484340216

Epoch: 5| Step: 3
Training loss: 4.692342281341553
Validation loss: 4.822782608770555

Epoch: 5| Step: 4
Training loss: 5.132164478302002
Validation loss: 4.806202062996485

Epoch: 5| Step: 5
Training loss: 4.650971412658691
Validation loss: 4.788575531334005

Epoch: 5| Step: 6
Training loss: 4.908867359161377
Validation loss: 4.771291127768896

Epoch: 5| Step: 7
Training loss: 3.878474712371826
Validation loss: 4.753084095575476

Epoch: 5| Step: 8
Training loss: 4.33591365814209
Validation loss: 4.73444143931071

Epoch: 5| Step: 9
Training loss: 5.264533996582031
Validation loss: 4.715612944736276

Epoch: 5| Step: 10
Training loss: 4.400485992431641
Validation loss: 4.695658724795106

Epoch: 5| Step: 0
Training loss: 4.233615875244141
Validation loss: 4.677243617273146

Epoch: 5| Step: 1
Training loss: 2.9790122509002686
Validation loss: 4.656834017845892

Epoch: 5| Step: 2
Training loss: 4.363241195678711
Validation loss: 4.637146580603815

Epoch: 5| Step: 3
Training loss: 5.011721611022949
Validation loss: 4.617367411172518

Epoch: 5| Step: 4
Training loss: 4.892703533172607
Validation loss: 4.596543763273505

Epoch: 5| Step: 5
Training loss: 3.720027208328247
Validation loss: 4.5756494563112975

Epoch: 5| Step: 6
Training loss: 4.693215370178223
Validation loss: 4.553575931056853

Epoch: 5| Step: 7
Training loss: 3.8017520904541016
Validation loss: 4.533298666759204

Epoch: 5| Step: 8
Training loss: 5.093595027923584
Validation loss: 4.511739884653399

Epoch: 5| Step: 9
Training loss: 4.65004825592041
Validation loss: 4.48937657571608

Epoch: 5| Step: 10
Training loss: 4.554529190063477
Validation loss: 4.468412378782867

Epoch: 6| Step: 0
Training loss: 4.852294445037842
Validation loss: 4.446289367573236

Epoch: 5| Step: 1
Training loss: 3.74120831489563
Validation loss: 4.424556486068234

Epoch: 5| Step: 2
Training loss: 4.752463340759277
Validation loss: 4.401850074850103

Epoch: 5| Step: 3
Training loss: 3.4845802783966064
Validation loss: 4.379911099710772

Epoch: 5| Step: 4
Training loss: 4.845487117767334
Validation loss: 4.358227237578361

Epoch: 5| Step: 5
Training loss: 4.435629367828369
Validation loss: 4.335110218294205

Epoch: 5| Step: 6
Training loss: 4.764615058898926
Validation loss: 4.312643363911619

Epoch: 5| Step: 7
Training loss: 3.5900325775146484
Validation loss: 4.289305727968934

Epoch: 5| Step: 8
Training loss: 3.839285373687744
Validation loss: 4.2666791459565525

Epoch: 5| Step: 9
Training loss: 3.756199598312378
Validation loss: 4.243374806578442

Epoch: 5| Step: 10
Training loss: 3.275629758834839
Validation loss: 4.219003600458945

Epoch: 7| Step: 0
Training loss: 3.3380260467529297
Validation loss: 4.19685721653764

Epoch: 5| Step: 1
Training loss: 3.6926562786102295
Validation loss: 4.172780667581866

Epoch: 5| Step: 2
Training loss: 3.898043394088745
Validation loss: 4.151335726502121

Epoch: 5| Step: 3
Training loss: 4.751739501953125
Validation loss: 4.127522153239096

Epoch: 5| Step: 4
Training loss: 3.506204605102539
Validation loss: 4.106764611377511

Epoch: 5| Step: 5
Training loss: 4.9796881675720215
Validation loss: 4.0843288719013175

Epoch: 5| Step: 6
Training loss: 4.15120792388916
Validation loss: 4.064202554764286

Epoch: 5| Step: 7
Training loss: 3.3160336017608643
Validation loss: 4.043536237491074

Epoch: 5| Step: 8
Training loss: 4.685397148132324
Validation loss: 4.020651560957714

Epoch: 5| Step: 9
Training loss: 3.4283204078674316
Validation loss: 3.9996058607614167

Epoch: 5| Step: 10
Training loss: 3.2299671173095703
Validation loss: 3.9810092987552768

Epoch: 8| Step: 0
Training loss: 4.4081878662109375
Validation loss: 3.9606476470988285

Epoch: 5| Step: 1
Training loss: 2.9553418159484863
Validation loss: 3.943510519560947

Epoch: 5| Step: 2
Training loss: 4.650254249572754
Validation loss: 3.9276232745057795

Epoch: 5| Step: 3
Training loss: 4.3260698318481445
Validation loss: 3.9088742399728424

Epoch: 5| Step: 4
Training loss: 3.3258328437805176
Validation loss: 3.8907764752705893

Epoch: 5| Step: 5
Training loss: 4.097381114959717
Validation loss: 3.8725675459830993

Epoch: 5| Step: 6
Training loss: 3.7689647674560547
Validation loss: 3.8582660664794264

Epoch: 5| Step: 7
Training loss: 2.8466238975524902
Validation loss: 3.841396875278924

Epoch: 5| Step: 8
Training loss: 3.37603759765625
Validation loss: 3.823827394875147

Epoch: 5| Step: 9
Training loss: 3.613852024078369
Validation loss: 3.809507798123103

Epoch: 5| Step: 10
Training loss: 3.829270839691162
Validation loss: 3.7943311506702053

Epoch: 9| Step: 0
Training loss: 3.1361777782440186
Validation loss: 3.7796852229743876

Epoch: 5| Step: 1
Training loss: 4.287425994873047
Validation loss: 3.764606419429984

Epoch: 5| Step: 2
Training loss: 2.7782340049743652
Validation loss: 3.7515682507586736

Epoch: 5| Step: 3
Training loss: 3.7951769828796387
Validation loss: 3.7382610100571827

Epoch: 5| Step: 4
Training loss: 4.019054889678955
Validation loss: 3.7238409698650403

Epoch: 5| Step: 5
Training loss: 4.287779808044434
Validation loss: 3.7099840820476575

Epoch: 5| Step: 6
Training loss: 4.71553373336792
Validation loss: 3.6947859692317184

Epoch: 5| Step: 7
Training loss: 2.60269832611084
Validation loss: 3.682403541380359

Epoch: 5| Step: 8
Training loss: 3.5326602458953857
Validation loss: 3.6690171149469193

Epoch: 5| Step: 9
Training loss: 3.726647138595581
Validation loss: 3.655566825661608

Epoch: 5| Step: 10
Training loss: 2.648751974105835
Validation loss: 3.6427582438274095

Epoch: 10| Step: 0
Training loss: 4.363018035888672
Validation loss: 3.630238922693396

Epoch: 5| Step: 1
Training loss: 3.135148525238037
Validation loss: 3.620147489732312

Epoch: 5| Step: 2
Training loss: 3.3945045471191406
Validation loss: 3.6071898757770495

Epoch: 5| Step: 3
Training loss: 4.652238368988037
Validation loss: 3.595743768958635

Epoch: 5| Step: 4
Training loss: 2.795307159423828
Validation loss: 3.5843033970043225

Epoch: 5| Step: 5
Training loss: 3.240776777267456
Validation loss: 3.5738387389849593

Epoch: 5| Step: 6
Training loss: 4.122472286224365
Validation loss: 3.563956588827154

Epoch: 5| Step: 7
Training loss: 4.716307163238525
Validation loss: 3.5516961389972317

Epoch: 5| Step: 8
Training loss: 2.2268385887145996
Validation loss: 3.5413740424699682

Epoch: 5| Step: 9
Training loss: 3.5406405925750732
Validation loss: 3.530319085685156

Epoch: 5| Step: 10
Training loss: 2.0703437328338623
Validation loss: 3.5212976855616414

Epoch: 11| Step: 0
Training loss: 4.034099102020264
Validation loss: 3.51427992697685

Epoch: 5| Step: 1
Training loss: 4.111815452575684
Validation loss: 3.505981881131408

Epoch: 5| Step: 2
Training loss: 3.6562416553497314
Validation loss: 3.495645069306897

Epoch: 5| Step: 3
Training loss: 3.653355360031128
Validation loss: 3.4891860767077376

Epoch: 5| Step: 4
Training loss: 2.2021994590759277
Validation loss: 3.482131040224465

Epoch: 5| Step: 5
Training loss: 2.6835479736328125
Validation loss: 3.473664809298772

Epoch: 5| Step: 6
Training loss: 2.8316242694854736
Validation loss: 3.464204583116757

Epoch: 5| Step: 7
Training loss: 4.234323501586914
Validation loss: 3.459839267115439

Epoch: 5| Step: 8
Training loss: 3.3870797157287598
Validation loss: 3.4510503456156743

Epoch: 5| Step: 9
Training loss: 3.2886180877685547
Validation loss: 3.443802026010329

Epoch: 5| Step: 10
Training loss: 3.4383037090301514
Validation loss: 3.437943761066724

Epoch: 12| Step: 0
Training loss: 3.4122424125671387
Validation loss: 3.4315018987142913

Epoch: 5| Step: 1
Training loss: 2.707487106323242
Validation loss: 3.4258734872264247

Epoch: 5| Step: 2
Training loss: 3.78680682182312
Validation loss: 3.4193298919226534

Epoch: 5| Step: 3
Training loss: 3.955677032470703
Validation loss: 3.415009472959785

Epoch: 5| Step: 4
Training loss: 2.450507164001465
Validation loss: 3.408176334955359

Epoch: 5| Step: 5
Training loss: 2.6520285606384277
Validation loss: 3.401719788069366

Epoch: 5| Step: 6
Training loss: 2.631922721862793
Validation loss: 3.4002539496267996

Epoch: 5| Step: 7
Training loss: 4.076912879943848
Validation loss: 3.388938555153467

Epoch: 5| Step: 8
Training loss: 4.274862766265869
Validation loss: 3.383238625782792

Epoch: 5| Step: 9
Training loss: 3.6270508766174316
Validation loss: 3.376105862279092

Epoch: 5| Step: 10
Training loss: 3.284666061401367
Validation loss: 3.371692749761766

Epoch: 13| Step: 0
Training loss: 2.347219705581665
Validation loss: 3.3663857008821223

Epoch: 5| Step: 1
Training loss: 2.8820998668670654
Validation loss: 3.3554503763875654

Epoch: 5| Step: 2
Training loss: 2.9807121753692627
Validation loss: 3.347862881998862

Epoch: 5| Step: 3
Training loss: 3.3780791759490967
Validation loss: 3.3392943669390935

Epoch: 5| Step: 4
Training loss: 3.620314121246338
Validation loss: 3.335851243747178

Epoch: 5| Step: 5
Training loss: 2.8283145427703857
Validation loss: 3.332812778411373

Epoch: 5| Step: 6
Training loss: 3.5676631927490234
Validation loss: 3.327767846404865

Epoch: 5| Step: 7
Training loss: 4.193509101867676
Validation loss: 3.321653396852555

Epoch: 5| Step: 8
Training loss: 3.7338995933532715
Validation loss: 3.3137734987402476

Epoch: 5| Step: 9
Training loss: 3.305567502975464
Validation loss: 3.3077994751673874

Epoch: 5| Step: 10
Training loss: 3.4356257915496826
Validation loss: 3.301507916501773

Epoch: 14| Step: 0
Training loss: 2.6882665157318115
Validation loss: 3.2973914377151

Epoch: 5| Step: 1
Training loss: 3.286485195159912
Validation loss: 3.294001322920604

Epoch: 5| Step: 2
Training loss: 2.336167097091675
Validation loss: 3.288976518056726

Epoch: 5| Step: 3
Training loss: 2.710456132888794
Validation loss: 3.2846510435945246

Epoch: 5| Step: 4
Training loss: 2.735774517059326
Validation loss: 3.2788747792602866

Epoch: 5| Step: 5
Training loss: 4.538595676422119
Validation loss: 3.2783802350362143

Epoch: 5| Step: 6
Training loss: 3.592027187347412
Validation loss: 3.2702685222830823

Epoch: 5| Step: 7
Training loss: 3.0538437366485596
Validation loss: 3.2627574833490516

Epoch: 5| Step: 8
Training loss: 3.4830946922302246
Validation loss: 3.260287777070076

Epoch: 5| Step: 9
Training loss: 3.7502598762512207
Validation loss: 3.254268010457357

Epoch: 5| Step: 10
Training loss: 3.602254867553711
Validation loss: 3.25179567644673

Epoch: 15| Step: 0
Training loss: 2.6171822547912598
Validation loss: 3.24743845898618

Epoch: 5| Step: 1
Training loss: 3.777156352996826
Validation loss: 3.2408987757980183

Epoch: 5| Step: 2
Training loss: 3.514003038406372
Validation loss: 3.235351618900094

Epoch: 5| Step: 3
Training loss: 3.921494245529175
Validation loss: 3.2311489248788483

Epoch: 5| Step: 4
Training loss: 2.927124500274658
Validation loss: 3.230221007459907

Epoch: 5| Step: 5
Training loss: 3.56268048286438
Validation loss: 3.222960841271185

Epoch: 5| Step: 6
Training loss: 2.8766164779663086
Validation loss: 3.2185873318743963

Epoch: 5| Step: 7
Training loss: 3.3311657905578613
Validation loss: 3.217465646805302

Epoch: 5| Step: 8
Training loss: 3.209261417388916
Validation loss: 3.212299167468984

Epoch: 5| Step: 9
Training loss: 2.764810085296631
Validation loss: 3.2076724062683764

Epoch: 5| Step: 10
Training loss: 2.7890424728393555
Validation loss: 3.2045489536818637

Epoch: 16| Step: 0
Training loss: 3.2383873462677
Validation loss: 3.2011695292688187

Epoch: 5| Step: 1
Training loss: 3.712674379348755
Validation loss: 3.1987854947326

Epoch: 5| Step: 2
Training loss: 2.6243367195129395
Validation loss: 3.194340411052909

Epoch: 5| Step: 3
Training loss: 3.584601879119873
Validation loss: 3.190387487411499

Epoch: 5| Step: 4
Training loss: 2.7766098976135254
Validation loss: 3.187959045492193

Epoch: 5| Step: 5
Training loss: 4.2381911277771
Validation loss: 3.1845390822297786

Epoch: 5| Step: 6
Training loss: 2.805473804473877
Validation loss: 3.1841489986706804

Epoch: 5| Step: 7
Training loss: 2.4407382011413574
Validation loss: 3.1775712890009724

Epoch: 5| Step: 8
Training loss: 3.6260859966278076
Validation loss: 3.176391573362453

Epoch: 5| Step: 9
Training loss: 2.833714008331299
Validation loss: 3.1724475019721576

Epoch: 5| Step: 10
Training loss: 3.1257431507110596
Validation loss: 3.168627295442807

Epoch: 17| Step: 0
Training loss: 3.0875840187072754
Validation loss: 3.1652841414174726

Epoch: 5| Step: 1
Training loss: 2.671577215194702
Validation loss: 3.1649713926417853

Epoch: 5| Step: 2
Training loss: 3.1849567890167236
Validation loss: 3.1605841728948776

Epoch: 5| Step: 3
Training loss: 3.014983654022217
Validation loss: 3.158910874397524

Epoch: 5| Step: 4
Training loss: 3.47361421585083
Validation loss: 3.1547930855904855

Epoch: 5| Step: 5
Training loss: 3.2010257244110107
Validation loss: 3.14806543883457

Epoch: 5| Step: 6
Training loss: 2.7914130687713623
Validation loss: 3.1519872219331804

Epoch: 5| Step: 7
Training loss: 3.1817402839660645
Validation loss: 3.1512527722184376

Epoch: 5| Step: 8
Training loss: 3.749809980392456
Validation loss: 3.1479850994643344

Epoch: 5| Step: 9
Training loss: 3.608332872390747
Validation loss: 3.1435807827980287

Epoch: 5| Step: 10
Training loss: 2.747539758682251
Validation loss: 3.1383061075723298

Epoch: 18| Step: 0
Training loss: 3.350526809692383
Validation loss: 3.136346504252444

Epoch: 5| Step: 1
Training loss: 3.879368305206299
Validation loss: 3.1354547726210726

Epoch: 5| Step: 2
Training loss: 1.9608196020126343
Validation loss: 3.13199036095732

Epoch: 5| Step: 3
Training loss: 4.474905014038086
Validation loss: 3.1290102697187856

Epoch: 5| Step: 4
Training loss: 3.3702805042266846
Validation loss: 3.1273321259406304

Epoch: 5| Step: 5
Training loss: 3.2534847259521484
Validation loss: 3.127773795076596

Epoch: 5| Step: 6
Training loss: 2.960751533508301
Validation loss: 3.121981574643043

Epoch: 5| Step: 7
Training loss: 2.967498540878296
Validation loss: 3.1164107937966623

Epoch: 5| Step: 8
Training loss: 2.6502695083618164
Validation loss: 3.118034665302564

Epoch: 5| Step: 9
Training loss: 3.351548433303833
Validation loss: 3.1159717165013796

Epoch: 5| Step: 10
Training loss: 2.1932709217071533
Validation loss: 3.117371238687987

Epoch: 19| Step: 0
Training loss: 3.065417528152466
Validation loss: 3.1113517463848157

Epoch: 5| Step: 1
Training loss: 3.0745372772216797
Validation loss: 3.1046859166955434

Epoch: 5| Step: 2
Training loss: 3.0421676635742188
Validation loss: 3.100919541492257

Epoch: 5| Step: 3
Training loss: 3.6494109630584717
Validation loss: 3.0991957136379775

Epoch: 5| Step: 4
Training loss: 2.2237160205841064
Validation loss: 3.099271082109021

Epoch: 5| Step: 5
Training loss: 3.8509521484375
Validation loss: 3.0963429199751986

Epoch: 5| Step: 6
Training loss: 3.06070613861084
Validation loss: 3.0954606045958815

Epoch: 5| Step: 7
Training loss: 3.147967576980591
Validation loss: 3.0959848691058416

Epoch: 5| Step: 8
Training loss: 2.1614489555358887
Validation loss: 3.0895164935819563

Epoch: 5| Step: 9
Training loss: 3.463548183441162
Validation loss: 3.086067053579515

Epoch: 5| Step: 10
Training loss: 3.6665117740631104
Validation loss: 3.0828100891523462

Epoch: 20| Step: 0
Training loss: 3.393502712249756
Validation loss: 3.0830565267993557

Epoch: 5| Step: 1
Training loss: 3.596195697784424
Validation loss: 3.0812304968475015

Epoch: 5| Step: 2
Training loss: 3.6942267417907715
Validation loss: 3.079285631897629

Epoch: 5| Step: 3
Training loss: 2.8599610328674316
Validation loss: 3.0762888975040887

Epoch: 5| Step: 4
Training loss: 2.5931448936462402
Validation loss: 3.0720880646859445

Epoch: 5| Step: 5
Training loss: 3.3682479858398438
Validation loss: 3.0686306517611266

Epoch: 5| Step: 6
Training loss: 2.4168412685394287
Validation loss: 3.0701166045281196

Epoch: 5| Step: 7
Training loss: 3.11010479927063
Validation loss: 3.0742870761502172

Epoch: 5| Step: 8
Training loss: 3.771024227142334
Validation loss: 3.074860170323362

Epoch: 5| Step: 9
Training loss: 2.6449837684631348
Validation loss: 3.062531594307192

Epoch: 5| Step: 10
Training loss: 2.6872000694274902
Validation loss: 3.061859033441031

Epoch: 21| Step: 0
Training loss: 4.104419708251953
Validation loss: 3.064583698908488

Epoch: 5| Step: 1
Training loss: 2.261085033416748
Validation loss: 3.0627730943823375

Epoch: 5| Step: 2
Training loss: 2.890012741088867
Validation loss: 3.0577866646551315

Epoch: 5| Step: 3
Training loss: 2.4092025756835938
Validation loss: 3.056364505521713

Epoch: 5| Step: 4
Training loss: 3.1667587757110596
Validation loss: 3.0551462532371603

Epoch: 5| Step: 5
Training loss: 3.2722365856170654
Validation loss: 3.0556280792400403

Epoch: 5| Step: 6
Training loss: 2.8936102390289307
Validation loss: 3.051976232118504

Epoch: 5| Step: 7
Training loss: 3.1204781532287598
Validation loss: 3.0536508560180664

Epoch: 5| Step: 8
Training loss: 3.3531970977783203
Validation loss: 3.0513869408638246

Epoch: 5| Step: 9
Training loss: 3.823024034500122
Validation loss: 3.049215562881962

Epoch: 5| Step: 10
Training loss: 2.7292118072509766
Validation loss: 3.04465381304423

Epoch: 22| Step: 0
Training loss: 3.3675410747528076
Validation loss: 3.0415278557808167

Epoch: 5| Step: 1
Training loss: 2.560697078704834
Validation loss: 3.0375447888528146

Epoch: 5| Step: 2
Training loss: 2.958991765975952
Validation loss: 3.032445602519538

Epoch: 5| Step: 3
Training loss: 3.444192409515381
Validation loss: 3.033734534376411

Epoch: 5| Step: 4
Training loss: 3.329686403274536
Validation loss: 3.0307928310927523

Epoch: 5| Step: 5
Training loss: 2.740023136138916
Validation loss: 3.029290122370566

Epoch: 5| Step: 6
Training loss: 3.447237730026245
Validation loss: 3.0282627895314205

Epoch: 5| Step: 7
Training loss: 2.7800450325012207
Validation loss: 3.02855061715649

Epoch: 5| Step: 8
Training loss: 3.0092389583587646
Validation loss: 3.02588499746015

Epoch: 5| Step: 9
Training loss: 3.0847580432891846
Validation loss: 3.0240220639013473

Epoch: 5| Step: 10
Training loss: 3.1406824588775635
Validation loss: 3.0202514638182936

Epoch: 23| Step: 0
Training loss: 3.414396286010742
Validation loss: 3.0211188588091122

Epoch: 5| Step: 1
Training loss: 2.9721100330352783
Validation loss: 3.0174761382482385

Epoch: 5| Step: 2
Training loss: 3.0692086219787598
Validation loss: 3.0136254936136226

Epoch: 5| Step: 3
Training loss: 2.8262758255004883
Validation loss: 3.0144122082700013

Epoch: 5| Step: 4
Training loss: 2.4570391178131104
Validation loss: 3.011620818927724

Epoch: 5| Step: 5
Training loss: 3.0092854499816895
Validation loss: 3.0108765607239096

Epoch: 5| Step: 6
Training loss: 3.4545586109161377
Validation loss: 3.006924711247926

Epoch: 5| Step: 7
Training loss: 2.851731061935425
Validation loss: 3.0047933234963367

Epoch: 5| Step: 8
Training loss: 3.2032310962677
Validation loss: 3.0030801783325853

Epoch: 5| Step: 9
Training loss: 3.197310447692871
Validation loss: 3.0019798304445002

Epoch: 5| Step: 10
Training loss: 3.3030588626861572
Validation loss: 3.0012818049359065

Epoch: 24| Step: 0
Training loss: 3.019611120223999
Validation loss: 2.9983268399392404

Epoch: 5| Step: 1
Training loss: 3.6047821044921875
Validation loss: 2.9974573043084916

Epoch: 5| Step: 2
Training loss: 2.7596383094787598
Validation loss: 2.9942146091051

Epoch: 5| Step: 3
Training loss: 3.1897084712982178
Validation loss: 2.991869903379871

Epoch: 5| Step: 4
Training loss: 3.907829761505127
Validation loss: 2.988573576814385

Epoch: 5| Step: 5
Training loss: 2.58693790435791
Validation loss: 2.9877904410003335

Epoch: 5| Step: 6
Training loss: 3.3708243370056152
Validation loss: 2.9855194245615313

Epoch: 5| Step: 7
Training loss: 2.4470934867858887
Validation loss: 2.98572277510038

Epoch: 5| Step: 8
Training loss: 3.4064838886260986
Validation loss: 2.982882962431959

Epoch: 5| Step: 9
Training loss: 3.275702714920044
Validation loss: 2.9820843973467426

Epoch: 5| Step: 10
Training loss: 1.852474570274353
Validation loss: 2.9794150552442

Epoch: 25| Step: 0
Training loss: 2.9474055767059326
Validation loss: 2.9790238923923944

Epoch: 5| Step: 1
Training loss: 3.2232155799865723
Validation loss: 2.9801951275076917

Epoch: 5| Step: 2
Training loss: 2.7122316360473633
Validation loss: 2.973740718697989

Epoch: 5| Step: 3
Training loss: 3.949598789215088
Validation loss: 2.9722475057007163

Epoch: 5| Step: 4
Training loss: 2.7644667625427246
Validation loss: 2.971000648313953

Epoch: 5| Step: 5
Training loss: 3.9118659496307373
Validation loss: 2.9740589485373548

Epoch: 5| Step: 6
Training loss: 3.1951851844787598
Validation loss: 2.971224428505026

Epoch: 5| Step: 7
Training loss: 3.173240900039673
Validation loss: 2.966584356882239

Epoch: 5| Step: 8
Training loss: 2.241665840148926
Validation loss: 2.965414665078604

Epoch: 5| Step: 9
Training loss: 2.386486530303955
Validation loss: 2.9639229159201346

Epoch: 5| Step: 10
Training loss: 2.936748743057251
Validation loss: 2.961596119788385

Epoch: 26| Step: 0
Training loss: 3.5938560962677
Validation loss: 2.960329063477055

Epoch: 5| Step: 1
Training loss: 3.037886619567871
Validation loss: 2.9619276446680867

Epoch: 5| Step: 2
Training loss: 2.291595697402954
Validation loss: 2.9612887495307514

Epoch: 5| Step: 3
Training loss: 3.0218918323516846
Validation loss: 2.959625485122845

Epoch: 5| Step: 4
Training loss: 3.458829879760742
Validation loss: 2.9575898057671

Epoch: 5| Step: 5
Training loss: 2.989466667175293
Validation loss: 2.9555313407733874

Epoch: 5| Step: 6
Training loss: 2.718437910079956
Validation loss: 2.9507593800944667

Epoch: 5| Step: 7
Training loss: 2.8402493000030518
Validation loss: 2.9473864263103855

Epoch: 5| Step: 8
Training loss: 3.081880569458008
Validation loss: 2.9440047458935807

Epoch: 5| Step: 9
Training loss: 3.1817424297332764
Validation loss: 2.9438150839139055

Epoch: 5| Step: 10
Training loss: 3.1144261360168457
Validation loss: 2.94725921077113

Epoch: 27| Step: 0
Training loss: 2.7001943588256836
Validation loss: 2.9453851997211413

Epoch: 5| Step: 1
Training loss: 2.0500283241271973
Validation loss: 2.9440435363400366

Epoch: 5| Step: 2
Training loss: 3.1039676666259766
Validation loss: 2.941218412050637

Epoch: 5| Step: 3
Training loss: 3.6251354217529297
Validation loss: 2.9402419367144184

Epoch: 5| Step: 4
Training loss: 3.074587345123291
Validation loss: 2.9357241122953353

Epoch: 5| Step: 5
Training loss: 3.0720303058624268
Validation loss: 2.9363941607936734

Epoch: 5| Step: 6
Training loss: 3.351240634918213
Validation loss: 2.934703550031108

Epoch: 5| Step: 7
Training loss: 3.2687885761260986
Validation loss: 2.9366282032382105

Epoch: 5| Step: 8
Training loss: 3.521904706954956
Validation loss: 2.942469045680056

Epoch: 5| Step: 9
Training loss: 3.3142590522766113
Validation loss: 2.9431721600153113

Epoch: 5| Step: 10
Training loss: 1.9772549867630005
Validation loss: 2.936596965277067

Epoch: 28| Step: 0
Training loss: 3.130528211593628
Validation loss: 2.935314175903156

Epoch: 5| Step: 1
Training loss: 3.399064302444458
Validation loss: 2.9315553711306666

Epoch: 5| Step: 2
Training loss: 2.980822801589966
Validation loss: 2.9295213363503896

Epoch: 5| Step: 3
Training loss: 2.8210644721984863
Validation loss: 2.9242237742229173

Epoch: 5| Step: 4
Training loss: 2.777406692504883
Validation loss: 2.9258717695871987

Epoch: 5| Step: 5
Training loss: 3.105024814605713
Validation loss: 2.9247457109471804

Epoch: 5| Step: 6
Training loss: 3.4509215354919434
Validation loss: 2.9248661687297206

Epoch: 5| Step: 7
Training loss: 3.1137607097625732
Validation loss: 2.924211755875618

Epoch: 5| Step: 8
Training loss: 2.8215785026550293
Validation loss: 2.9243974403668473

Epoch: 5| Step: 9
Training loss: 2.9162418842315674
Validation loss: 2.9233191346609466

Epoch: 5| Step: 10
Training loss: 2.544785261154175
Validation loss: 2.9214577290319625

Epoch: 29| Step: 0
Training loss: 3.011819362640381
Validation loss: 2.9188344863153275

Epoch: 5| Step: 1
Training loss: 3.1093316078186035
Validation loss: 2.919367003184493

Epoch: 5| Step: 2
Training loss: 3.1560230255126953
Validation loss: 2.9192881840531544

Epoch: 5| Step: 3
Training loss: 3.1124324798583984
Validation loss: 2.9159695333050144

Epoch: 5| Step: 4
Training loss: 2.975419521331787
Validation loss: 2.9139350204057592

Epoch: 5| Step: 5
Training loss: 2.257204055786133
Validation loss: 2.9131217105414278

Epoch: 5| Step: 6
Training loss: 4.310647964477539
Validation loss: 2.917781768306609

Epoch: 5| Step: 7
Training loss: 2.862964153289795
Validation loss: 2.918705968446629

Epoch: 5| Step: 8
Training loss: 2.4014620780944824
Validation loss: 2.9212663276221162

Epoch: 5| Step: 9
Training loss: 2.586184024810791
Validation loss: 2.9156845359392065

Epoch: 5| Step: 10
Training loss: 3.304359197616577
Validation loss: 2.9084772781659196

Epoch: 30| Step: 0
Training loss: 2.846407413482666
Validation loss: 2.9052556586521927

Epoch: 5| Step: 1
Training loss: 3.1341588497161865
Validation loss: 2.9026441522823867

Epoch: 5| Step: 2
Training loss: 3.098860502243042
Validation loss: 2.9039664755585375

Epoch: 5| Step: 3
Training loss: 3.05696439743042
Validation loss: 2.9024420451092463

Epoch: 5| Step: 4
Training loss: 3.068406581878662
Validation loss: 2.900280060306672

Epoch: 5| Step: 5
Training loss: 3.7331433296203613
Validation loss: 2.8986874652165238

Epoch: 5| Step: 6
Training loss: 3.048305034637451
Validation loss: 2.899638004200433

Epoch: 5| Step: 7
Training loss: 2.2064945697784424
Validation loss: 2.8974164019348803

Epoch: 5| Step: 8
Training loss: 2.832571029663086
Validation loss: 2.89716233745698

Epoch: 5| Step: 9
Training loss: 2.065467357635498
Validation loss: 2.897424505602929

Epoch: 5| Step: 10
Training loss: 3.9779295921325684
Validation loss: 2.8955332386878228

Epoch: 31| Step: 0
Training loss: 2.751920223236084
Validation loss: 2.8957657506389003

Epoch: 5| Step: 1
Training loss: 2.9855613708496094
Validation loss: 2.894119275513516

Epoch: 5| Step: 2
Training loss: 2.897037982940674
Validation loss: 2.8920169415012484

Epoch: 5| Step: 3
Training loss: 3.266461133956909
Validation loss: 2.8917436189548944

Epoch: 5| Step: 4
Training loss: 3.414107084274292
Validation loss: 2.891322571744201

Epoch: 5| Step: 5
Training loss: 2.8121588230133057
Validation loss: 2.8909961792730514

Epoch: 5| Step: 6
Training loss: 2.429183006286621
Validation loss: 2.8913578602575485

Epoch: 5| Step: 7
Training loss: 2.7761149406433105
Validation loss: 2.891796119751469

Epoch: 5| Step: 8
Training loss: 2.9837093353271484
Validation loss: 2.8895593843152447

Epoch: 5| Step: 9
Training loss: 3.021972894668579
Validation loss: 2.8874114303178686

Epoch: 5| Step: 10
Training loss: 3.5974738597869873
Validation loss: 2.8902258001348025

Epoch: 32| Step: 0
Training loss: 3.2207932472229004
Validation loss: 2.886331945337275

Epoch: 5| Step: 1
Training loss: 2.474771499633789
Validation loss: 2.8849499712708178

Epoch: 5| Step: 2
Training loss: 2.6271555423736572
Validation loss: 2.8827652521030878

Epoch: 5| Step: 3
Training loss: 3.427475690841675
Validation loss: 2.8841557323291735

Epoch: 5| Step: 4
Training loss: 2.653195381164551
Validation loss: 2.881736406715967

Epoch: 5| Step: 5
Training loss: 2.8987298011779785
Validation loss: 2.8808476053258425

Epoch: 5| Step: 6
Training loss: 3.218426465988159
Validation loss: 2.8795106052070536

Epoch: 5| Step: 7
Training loss: 2.9112913608551025
Validation loss: 2.8789059577449674

Epoch: 5| Step: 8
Training loss: 3.6589057445526123
Validation loss: 2.876374098562425

Epoch: 5| Step: 9
Training loss: 2.9454920291900635
Validation loss: 2.876494376890121

Epoch: 5| Step: 10
Training loss: 2.687131881713867
Validation loss: 2.874944322852678

Epoch: 33| Step: 0
Training loss: 2.6622815132141113
Validation loss: 2.876539263674008

Epoch: 5| Step: 1
Training loss: 2.6731934547424316
Validation loss: 2.87411839987642

Epoch: 5| Step: 2
Training loss: 3.0823757648468018
Validation loss: 2.872676170000466

Epoch: 5| Step: 3
Training loss: 2.7536706924438477
Validation loss: 2.871123751004537

Epoch: 5| Step: 4
Training loss: 2.87699294090271
Validation loss: 2.8709355246636177

Epoch: 5| Step: 5
Training loss: 3.508801221847534
Validation loss: 2.8704346533744567

Epoch: 5| Step: 6
Training loss: 3.194413423538208
Validation loss: 2.8726190649053103

Epoch: 5| Step: 7
Training loss: 2.7463138103485107
Validation loss: 2.8702093606354087

Epoch: 5| Step: 8
Training loss: 3.2882308959960938
Validation loss: 2.8694411682826217

Epoch: 5| Step: 9
Training loss: 3.2574386596679688
Validation loss: 2.869842113987092

Epoch: 5| Step: 10
Training loss: 2.5898935794830322
Validation loss: 2.86699761626541

Epoch: 34| Step: 0
Training loss: 3.1182467937469482
Validation loss: 2.862703484873618

Epoch: 5| Step: 1
Training loss: 2.212984085083008
Validation loss: 2.8639641705379693

Epoch: 5| Step: 2
Training loss: 3.0260684490203857
Validation loss: 2.8648732041799896

Epoch: 5| Step: 3
Training loss: 2.635158061981201
Validation loss: 2.8630077351805983

Epoch: 5| Step: 4
Training loss: 3.158425807952881
Validation loss: 2.8596862977550876

Epoch: 5| Step: 5
Training loss: 2.6325860023498535
Validation loss: 2.8610482395336194

Epoch: 5| Step: 6
Training loss: 4.471835136413574
Validation loss: 2.860453751779372

Epoch: 5| Step: 7
Training loss: 2.6975908279418945
Validation loss: 2.861330750168011

Epoch: 5| Step: 8
Training loss: 3.242422580718994
Validation loss: 2.8602768298118346

Epoch: 5| Step: 9
Training loss: 2.881260633468628
Validation loss: 2.860600351005472

Epoch: 5| Step: 10
Training loss: 2.4729695320129395
Validation loss: 2.861008298012518

Epoch: 35| Step: 0
Training loss: 2.350379228591919
Validation loss: 2.858991830579696

Epoch: 5| Step: 1
Training loss: 3.187272310256958
Validation loss: 2.8571454940303678

Epoch: 5| Step: 2
Training loss: 3.409912109375
Validation loss: 2.857480741316272

Epoch: 5| Step: 3
Training loss: 3.2465310096740723
Validation loss: 2.854821382030364

Epoch: 5| Step: 4
Training loss: 2.2783827781677246
Validation loss: 2.854641701585503

Epoch: 5| Step: 5
Training loss: 3.7144839763641357
Validation loss: 2.8502650466016544

Epoch: 5| Step: 6
Training loss: 2.517451524734497
Validation loss: 2.8565236804305867

Epoch: 5| Step: 7
Training loss: 2.43827486038208
Validation loss: 2.8541217260463263

Epoch: 5| Step: 8
Training loss: 3.7185111045837402
Validation loss: 2.852079547861571

Epoch: 5| Step: 9
Training loss: 3.280017137527466
Validation loss: 2.8495537234890844

Epoch: 5| Step: 10
Training loss: 2.3573811054229736
Validation loss: 2.8499585966910086

Epoch: 36| Step: 0
Training loss: 3.263235569000244
Validation loss: 2.8491436384057485

Epoch: 5| Step: 1
Training loss: 4.04306173324585
Validation loss: 2.8516053179258942

Epoch: 5| Step: 2
Training loss: 2.3899288177490234
Validation loss: 2.8479366353763047

Epoch: 5| Step: 3
Training loss: 3.5153632164001465
Validation loss: 2.850804405827676

Epoch: 5| Step: 4
Training loss: 2.5974185466766357
Validation loss: 2.8475456827430317

Epoch: 5| Step: 5
Training loss: 2.694389820098877
Validation loss: 2.847819889745405

Epoch: 5| Step: 6
Training loss: 2.1505026817321777
Validation loss: 2.84576262709915

Epoch: 5| Step: 7
Training loss: 2.959773302078247
Validation loss: 2.8441652944011073

Epoch: 5| Step: 8
Training loss: 3.2056491374969482
Validation loss: 2.8433869115767942

Epoch: 5| Step: 9
Training loss: 2.5763916969299316
Validation loss: 2.8463034809276624

Epoch: 5| Step: 10
Training loss: 3.158531427383423
Validation loss: 2.8462821796376216

Epoch: 37| Step: 0
Training loss: 2.8437020778656006
Validation loss: 2.866173426310221

Epoch: 5| Step: 1
Training loss: 2.857647657394409
Validation loss: 2.852472013042819

Epoch: 5| Step: 2
Training loss: 3.2201690673828125
Validation loss: 2.8444178027491414

Epoch: 5| Step: 3
Training loss: 2.954474925994873
Validation loss: 2.845270103023898

Epoch: 5| Step: 4
Training loss: 3.210475444793701
Validation loss: 2.843745687956451

Epoch: 5| Step: 5
Training loss: 2.6350343227386475
Validation loss: 2.8426636726625505

Epoch: 5| Step: 6
Training loss: 2.8356614112854004
Validation loss: 2.843098155913814

Epoch: 5| Step: 7
Training loss: 3.451542377471924
Validation loss: 2.843913544890701

Epoch: 5| Step: 8
Training loss: 2.7120723724365234
Validation loss: 2.843286201518069

Epoch: 5| Step: 9
Training loss: 3.287290096282959
Validation loss: 2.8430746011836554

Epoch: 5| Step: 10
Training loss: 2.445802688598633
Validation loss: 2.8427346009080128

Epoch: 38| Step: 0
Training loss: 1.9375098943710327
Validation loss: 2.8431950384570706

Epoch: 5| Step: 1
Training loss: 3.003096580505371
Validation loss: 2.840942931431596

Epoch: 5| Step: 2
Training loss: 3.3408894538879395
Validation loss: 2.842680085089899

Epoch: 5| Step: 3
Training loss: 3.719210147857666
Validation loss: 2.8379240702557307

Epoch: 5| Step: 4
Training loss: 2.580909252166748
Validation loss: 2.836902274880358

Epoch: 5| Step: 5
Training loss: 3.152890205383301
Validation loss: 2.8379423233770553

Epoch: 5| Step: 6
Training loss: 2.6751224994659424
Validation loss: 2.834573176599318

Epoch: 5| Step: 7
Training loss: 3.117084503173828
Validation loss: 2.8334873594263548

Epoch: 5| Step: 8
Training loss: 3.233036756515503
Validation loss: 2.833110829835297

Epoch: 5| Step: 9
Training loss: 2.166980028152466
Validation loss: 2.834278283580657

Epoch: 5| Step: 10
Training loss: 3.5951178073883057
Validation loss: 2.834594277925389

Epoch: 39| Step: 0
Training loss: 3.4291186332702637
Validation loss: 2.8337879642363517

Epoch: 5| Step: 1
Training loss: 3.146404981613159
Validation loss: 2.834504714576147

Epoch: 5| Step: 2
Training loss: 3.00285267829895
Validation loss: 2.8311518135891167

Epoch: 5| Step: 3
Training loss: 2.010230541229248
Validation loss: 2.8283871912187144

Epoch: 5| Step: 4
Training loss: 3.042464017868042
Validation loss: 2.8290372843383462

Epoch: 5| Step: 5
Training loss: 3.060537338256836
Validation loss: 2.832124815192274

Epoch: 5| Step: 6
Training loss: 3.8947417736053467
Validation loss: 2.830647219893753

Epoch: 5| Step: 7
Training loss: 2.771064043045044
Validation loss: 2.832280315378661

Epoch: 5| Step: 8
Training loss: 2.566352367401123
Validation loss: 2.8334532860786683

Epoch: 5| Step: 9
Training loss: 2.714397430419922
Validation loss: 2.8298056869096655

Epoch: 5| Step: 10
Training loss: 2.747265577316284
Validation loss: 2.830477233855955

Epoch: 40| Step: 0
Training loss: 3.132338523864746
Validation loss: 2.8239151406031784

Epoch: 5| Step: 1
Training loss: 2.9760851860046387
Validation loss: 2.826339352515436

Epoch: 5| Step: 2
Training loss: 3.2981479167938232
Validation loss: 2.825024876543271

Epoch: 5| Step: 3
Training loss: 2.954939365386963
Validation loss: 2.8265361478251796

Epoch: 5| Step: 4
Training loss: 2.1997339725494385
Validation loss: 2.82451658351447

Epoch: 5| Step: 5
Training loss: 2.9697322845458984
Validation loss: 2.8239074599358345

Epoch: 5| Step: 6
Training loss: 3.4939494132995605
Validation loss: 2.8249181355199506

Epoch: 5| Step: 7
Training loss: 2.7196991443634033
Validation loss: 2.8236275667785318

Epoch: 5| Step: 8
Training loss: 2.495758533477783
Validation loss: 2.8231603150726645

Epoch: 5| Step: 9
Training loss: 3.1422905921936035
Validation loss: 2.8256275705111924

Epoch: 5| Step: 10
Training loss: 2.958972692489624
Validation loss: 2.8235278437214513

Epoch: 41| Step: 0
Training loss: 2.7114195823669434
Validation loss: 2.823292629693144

Epoch: 5| Step: 1
Training loss: 3.456294536590576
Validation loss: 2.8222215662720385

Epoch: 5| Step: 2
Training loss: 2.3724074363708496
Validation loss: 2.8227749255395707

Epoch: 5| Step: 3
Training loss: 3.3015503883361816
Validation loss: 2.8225289262751097

Epoch: 5| Step: 4
Training loss: 2.854339122772217
Validation loss: 2.8223420573819067

Epoch: 5| Step: 5
Training loss: 1.6616493463516235
Validation loss: 2.8199803393374205

Epoch: 5| Step: 6
Training loss: 3.398141860961914
Validation loss: 2.8190731258802515

Epoch: 5| Step: 7
Training loss: 3.4851067066192627
Validation loss: 2.8193827623962076

Epoch: 5| Step: 8
Training loss: 3.3886406421661377
Validation loss: 2.8180553990025676

Epoch: 5| Step: 9
Training loss: 3.450331926345825
Validation loss: 2.8193534420382593

Epoch: 5| Step: 10
Training loss: 2.111326217651367
Validation loss: 2.818154147876206

Epoch: 42| Step: 0
Training loss: 2.6649136543273926
Validation loss: 2.817500193913778

Epoch: 5| Step: 1
Training loss: 3.28509259223938
Validation loss: 2.8172295324264036

Epoch: 5| Step: 2
Training loss: 2.107325315475464
Validation loss: 2.81650528600139

Epoch: 5| Step: 3
Training loss: 3.5120551586151123
Validation loss: 2.81613822906248

Epoch: 5| Step: 4
Training loss: 2.6208879947662354
Validation loss: 2.8145242967913227

Epoch: 5| Step: 5
Training loss: 2.4348537921905518
Validation loss: 2.815597190651842

Epoch: 5| Step: 6
Training loss: 2.9190125465393066
Validation loss: 2.814750907241657

Epoch: 5| Step: 7
Training loss: 3.5718986988067627
Validation loss: 2.8159821648751535

Epoch: 5| Step: 8
Training loss: 3.21826171875
Validation loss: 2.8177721961852042

Epoch: 5| Step: 9
Training loss: 2.993422269821167
Validation loss: 2.820608272347399

Epoch: 5| Step: 10
Training loss: 3.007239818572998
Validation loss: 2.816891772772676

Epoch: 43| Step: 0
Training loss: 3.552898406982422
Validation loss: 2.8136356825469644

Epoch: 5| Step: 1
Training loss: 2.507558822631836
Validation loss: 2.813270668829641

Epoch: 5| Step: 2
Training loss: 3.263211488723755
Validation loss: 2.810547718437769

Epoch: 5| Step: 3
Training loss: 1.9980205297470093
Validation loss: 2.8112647123234247

Epoch: 5| Step: 4
Training loss: 3.3895301818847656
Validation loss: 2.8101054365916918

Epoch: 5| Step: 5
Training loss: 2.6814303398132324
Validation loss: 2.8131384029183337

Epoch: 5| Step: 6
Training loss: 2.735281467437744
Validation loss: 2.8122089447513705

Epoch: 5| Step: 7
Training loss: 3.0300002098083496
Validation loss: 2.809871565911078

Epoch: 5| Step: 8
Training loss: 2.631981372833252
Validation loss: 2.809585835344048

Epoch: 5| Step: 9
Training loss: 3.274033784866333
Validation loss: 2.811921424763177

Epoch: 5| Step: 10
Training loss: 3.2298569679260254
Validation loss: 2.808910159654515

Epoch: 44| Step: 0
Training loss: 3.0564489364624023
Validation loss: 2.8093280535872265

Epoch: 5| Step: 1
Training loss: 2.9033753871917725
Validation loss: 2.806490321313181

Epoch: 5| Step: 2
Training loss: 3.075915575027466
Validation loss: 2.805891162605696

Epoch: 5| Step: 3
Training loss: 3.2865536212921143
Validation loss: 2.8063302706646662

Epoch: 5| Step: 4
Training loss: 3.3108906745910645
Validation loss: 2.805718768027521

Epoch: 5| Step: 5
Training loss: 2.6923069953918457
Validation loss: 2.806043158295334

Epoch: 5| Step: 6
Training loss: 2.861825704574585
Validation loss: 2.8040351072947183

Epoch: 5| Step: 7
Training loss: 2.2286641597747803
Validation loss: 2.8034718087924424

Epoch: 5| Step: 8
Training loss: 3.385845184326172
Validation loss: 2.8056096569184334

Epoch: 5| Step: 9
Training loss: 3.1880228519439697
Validation loss: 2.8043687010324128

Epoch: 5| Step: 10
Training loss: 2.1168580055236816
Validation loss: 2.8038355765804166

Epoch: 45| Step: 0
Training loss: 2.959658145904541
Validation loss: 2.806231116735807

Epoch: 5| Step: 1
Training loss: 3.1799843311309814
Validation loss: 2.804874448366063

Epoch: 5| Step: 2
Training loss: 2.9785571098327637
Validation loss: 2.8022680128774335

Epoch: 5| Step: 3
Training loss: 2.615226984024048
Validation loss: 2.803625770794448

Epoch: 5| Step: 4
Training loss: 3.534080982208252
Validation loss: 2.799855991076398

Epoch: 5| Step: 5
Training loss: 2.466801166534424
Validation loss: 2.8019629498963714

Epoch: 5| Step: 6
Training loss: 2.376304864883423
Validation loss: 2.8021896834014566

Epoch: 5| Step: 7
Training loss: 2.5949087142944336
Validation loss: 2.8016054476461103

Epoch: 5| Step: 8
Training loss: 3.4995925426483154
Validation loss: 2.799282604648221

Epoch: 5| Step: 9
Training loss: 3.026332139968872
Validation loss: 2.7992003348565873

Epoch: 5| Step: 10
Training loss: 2.9798028469085693
Validation loss: 2.8016676697679745

Epoch: 46| Step: 0
Training loss: 3.7199597358703613
Validation loss: 2.8034008779833393

Epoch: 5| Step: 1
Training loss: 2.249384641647339
Validation loss: 2.802034967689104

Epoch: 5| Step: 2
Training loss: 3.018312454223633
Validation loss: 2.802739897081929

Epoch: 5| Step: 3
Training loss: 2.7917847633361816
Validation loss: 2.801132599512736

Epoch: 5| Step: 4
Training loss: 2.857430934906006
Validation loss: 2.7970096629153014

Epoch: 5| Step: 5
Training loss: 2.639786958694458
Validation loss: 2.7974412774526947

Epoch: 5| Step: 6
Training loss: 2.877340793609619
Validation loss: 2.7965804710183093

Epoch: 5| Step: 7
Training loss: 3.5925605297088623
Validation loss: 2.794878687909854

Epoch: 5| Step: 8
Training loss: 2.7943918704986572
Validation loss: 2.79709287869033

Epoch: 5| Step: 9
Training loss: 2.8229165077209473
Validation loss: 2.796295827434909

Epoch: 5| Step: 10
Training loss: 2.7855236530303955
Validation loss: 2.795799065661687

Epoch: 47| Step: 0
Training loss: 2.6740641593933105
Validation loss: 2.7940836670578166

Epoch: 5| Step: 1
Training loss: 3.0462260246276855
Validation loss: 2.7960230842713387

Epoch: 5| Step: 2
Training loss: 2.2214858531951904
Validation loss: 2.794795100406934

Epoch: 5| Step: 3
Training loss: 3.587625026702881
Validation loss: 2.796618212935745

Epoch: 5| Step: 4
Training loss: 3.161123752593994
Validation loss: 2.794310659490606

Epoch: 5| Step: 5
Training loss: 3.309622287750244
Validation loss: 2.796134287311185

Epoch: 5| Step: 6
Training loss: 2.975142240524292
Validation loss: 2.801199807915636

Epoch: 5| Step: 7
Training loss: 2.82828950881958
Validation loss: 2.7976193402403142

Epoch: 5| Step: 8
Training loss: 2.792255401611328
Validation loss: 2.7936797654756935

Epoch: 5| Step: 9
Training loss: 3.077849864959717
Validation loss: 2.7918006373989965

Epoch: 5| Step: 10
Training loss: 2.394719123840332
Validation loss: 2.7936420748310704

Epoch: 48| Step: 0
Training loss: 2.571141481399536
Validation loss: 2.7938523343814317

Epoch: 5| Step: 1
Training loss: 2.8866846561431885
Validation loss: 2.796955103515297

Epoch: 5| Step: 2
Training loss: 2.6228480339050293
Validation loss: 2.802376949658958

Epoch: 5| Step: 3
Training loss: 3.40116810798645
Validation loss: 2.795713875883369

Epoch: 5| Step: 4
Training loss: 3.0563747882843018
Validation loss: 2.792844469829272

Epoch: 5| Step: 5
Training loss: 3.1963694095611572
Validation loss: 2.791480300247028

Epoch: 5| Step: 6
Training loss: 3.164464235305786
Validation loss: 2.7927473745038434

Epoch: 5| Step: 7
Training loss: 2.382725954055786
Validation loss: 2.794839261680521

Epoch: 5| Step: 8
Training loss: 3.4040350914001465
Validation loss: 2.796535712416454

Epoch: 5| Step: 9
Training loss: 2.4567253589630127
Validation loss: 2.8028447807476087

Epoch: 5| Step: 10
Training loss: 3.005871057510376
Validation loss: 2.8015681569294264

Epoch: 49| Step: 0
Training loss: 2.4409701824188232
Validation loss: 2.8043482944529545

Epoch: 5| Step: 1
Training loss: 3.683584690093994
Validation loss: 2.802762449428599

Epoch: 5| Step: 2
Training loss: 2.28041410446167
Validation loss: 2.7953098076646046

Epoch: 5| Step: 3
Training loss: 2.6323370933532715
Validation loss: 2.7936216785061743

Epoch: 5| Step: 4
Training loss: 2.759854793548584
Validation loss: 2.7937101266717397

Epoch: 5| Step: 5
Training loss: 2.3971505165100098
Validation loss: 2.7937860258163942

Epoch: 5| Step: 6
Training loss: 2.8284192085266113
Validation loss: 2.7920066361786215

Epoch: 5| Step: 7
Training loss: 4.093265533447266
Validation loss: 2.792732172114875

Epoch: 5| Step: 8
Training loss: 3.190030574798584
Validation loss: 2.7873602631271526

Epoch: 5| Step: 9
Training loss: 2.7074878215789795
Validation loss: 2.787837879632109

Epoch: 5| Step: 10
Training loss: 3.1527023315429688
Validation loss: 2.7878557969165105

Epoch: 50| Step: 0
Training loss: 2.3711469173431396
Validation loss: 2.790151967797228

Epoch: 5| Step: 1
Training loss: 3.2848830223083496
Validation loss: 2.786260694585821

Epoch: 5| Step: 2
Training loss: 2.3249905109405518
Validation loss: 2.7902171304149013

Epoch: 5| Step: 3
Training loss: 3.369061231613159
Validation loss: 2.7858640352884927

Epoch: 5| Step: 4
Training loss: 3.5017807483673096
Validation loss: 2.7867650126898162

Epoch: 5| Step: 5
Training loss: 3.1084814071655273
Validation loss: 2.785895985941733

Epoch: 5| Step: 6
Training loss: 3.4132957458496094
Validation loss: 2.7842803770496

Epoch: 5| Step: 7
Training loss: 2.220268487930298
Validation loss: 2.784029442776916

Epoch: 5| Step: 8
Training loss: 2.6602258682250977
Validation loss: 2.7867363755420973

Epoch: 5| Step: 9
Training loss: 2.9522957801818848
Validation loss: 2.785714608366771

Epoch: 5| Step: 10
Training loss: 2.874891996383667
Validation loss: 2.7851733187193513

Epoch: 51| Step: 0
Training loss: 3.55818247795105
Validation loss: 2.782929097452471

Epoch: 5| Step: 1
Training loss: 2.8034591674804688
Validation loss: 2.7806204877873903

Epoch: 5| Step: 2
Training loss: 3.2029852867126465
Validation loss: 2.7826747381558983

Epoch: 5| Step: 3
Training loss: 2.6610920429229736
Validation loss: 2.780448539282686

Epoch: 5| Step: 4
Training loss: 2.5076651573181152
Validation loss: 2.7815017315649215

Epoch: 5| Step: 5
Training loss: 3.304063320159912
Validation loss: 2.7820641661203034

Epoch: 5| Step: 6
Training loss: 3.1331112384796143
Validation loss: 2.780857765546409

Epoch: 5| Step: 7
Training loss: 2.622948408126831
Validation loss: 2.7788071119657127

Epoch: 5| Step: 8
Training loss: 2.823796510696411
Validation loss: 2.7808106458315285

Epoch: 5| Step: 9
Training loss: 2.4508912563323975
Validation loss: 2.780331260414534

Epoch: 5| Step: 10
Training loss: 2.9961812496185303
Validation loss: 2.780824404890819

Epoch: 52| Step: 0
Training loss: 2.800440549850464
Validation loss: 2.7820039128744476

Epoch: 5| Step: 1
Training loss: 2.4499573707580566
Validation loss: 2.7816108221648843

Epoch: 5| Step: 2
Training loss: 2.8636350631713867
Validation loss: 2.7796629500645462

Epoch: 5| Step: 3
Training loss: 2.654449939727783
Validation loss: 2.778485914712311

Epoch: 5| Step: 4
Training loss: 2.7307517528533936
Validation loss: 2.777389326403218

Epoch: 5| Step: 5
Training loss: 3.4167675971984863
Validation loss: 2.779449524418

Epoch: 5| Step: 6
Training loss: 2.3837664127349854
Validation loss: 2.7796440791058283

Epoch: 5| Step: 7
Training loss: 3.3206114768981934
Validation loss: 2.782542713226811

Epoch: 5| Step: 8
Training loss: 3.4661953449249268
Validation loss: 2.7807620648414857

Epoch: 5| Step: 9
Training loss: 2.914022922515869
Validation loss: 2.7832105851942495

Epoch: 5| Step: 10
Training loss: 3.0629823207855225
Validation loss: 2.783750318711804

Epoch: 53| Step: 0
Training loss: 1.8454679250717163
Validation loss: 2.7821014901643157

Epoch: 5| Step: 1
Training loss: 2.550067901611328
Validation loss: 2.7793782603356147

Epoch: 5| Step: 2
Training loss: 3.2906196117401123
Validation loss: 2.7775251147567586

Epoch: 5| Step: 3
Training loss: 3.3938591480255127
Validation loss: 2.7732478444294264

Epoch: 5| Step: 4
Training loss: 2.6354336738586426
Validation loss: 2.7744930636498237

Epoch: 5| Step: 5
Training loss: 2.9186041355133057
Validation loss: 2.775188205062702

Epoch: 5| Step: 6
Training loss: 3.258380174636841
Validation loss: 2.7783938120770197

Epoch: 5| Step: 7
Training loss: 2.985746383666992
Validation loss: 2.7754488734788794

Epoch: 5| Step: 8
Training loss: 2.6729931831359863
Validation loss: 2.7756436665852866

Epoch: 5| Step: 9
Training loss: 3.550410747528076
Validation loss: 2.7749654964734147

Epoch: 5| Step: 10
Training loss: 2.940633773803711
Validation loss: 2.7750015386971096

Epoch: 54| Step: 0
Training loss: 3.29533052444458
Validation loss: 2.7724578534403155

Epoch: 5| Step: 1
Training loss: 2.833582639694214
Validation loss: 2.7711396089164158

Epoch: 5| Step: 2
Training loss: 2.971771717071533
Validation loss: 2.7723979411586637

Epoch: 5| Step: 3
Training loss: 2.6593635082244873
Validation loss: 2.7756817802306144

Epoch: 5| Step: 4
Training loss: 3.0377888679504395
Validation loss: 2.773509940793437

Epoch: 5| Step: 5
Training loss: 2.651759147644043
Validation loss: 2.775620170818862

Epoch: 5| Step: 6
Training loss: 3.4761111736297607
Validation loss: 2.7741863035386607

Epoch: 5| Step: 7
Training loss: 2.7624144554138184
Validation loss: 2.773933636244907

Epoch: 5| Step: 8
Training loss: 2.4668729305267334
Validation loss: 2.7743674785860124

Epoch: 5| Step: 9
Training loss: 2.6068108081817627
Validation loss: 2.7763660402708155

Epoch: 5| Step: 10
Training loss: 3.259953260421753
Validation loss: 2.7764445248470513

Epoch: 55| Step: 0
Training loss: 3.1083874702453613
Validation loss: 2.7741012675787813

Epoch: 5| Step: 1
Training loss: 3.2270827293395996
Validation loss: 2.772697469239594

Epoch: 5| Step: 2
Training loss: 2.6498970985412598
Validation loss: 2.7778587802763908

Epoch: 5| Step: 3
Training loss: 3.235118865966797
Validation loss: 2.774060010910034

Epoch: 5| Step: 4
Training loss: 2.9539687633514404
Validation loss: 2.775649837268296

Epoch: 5| Step: 5
Training loss: 2.6607065200805664
Validation loss: 2.77556110453862

Epoch: 5| Step: 6
Training loss: 3.0648887157440186
Validation loss: 2.773100837584465

Epoch: 5| Step: 7
Training loss: 3.002668857574463
Validation loss: 2.7709928609991588

Epoch: 5| Step: 8
Training loss: 2.2603259086608887
Validation loss: 2.7725804441718647

Epoch: 5| Step: 9
Training loss: 2.762180805206299
Validation loss: 2.773465766701647

Epoch: 5| Step: 10
Training loss: 3.0288825035095215
Validation loss: 2.772222859885103

Epoch: 56| Step: 0
Training loss: 3.9338295459747314
Validation loss: 2.767565614433699

Epoch: 5| Step: 1
Training loss: 2.7567992210388184
Validation loss: 2.7664342644394084

Epoch: 5| Step: 2
Training loss: 2.886387586593628
Validation loss: 2.7682039712065007

Epoch: 5| Step: 3
Training loss: 2.4284942150115967
Validation loss: 2.766961049008113

Epoch: 5| Step: 4
Training loss: 3.03692889213562
Validation loss: 2.7682515293039303

Epoch: 5| Step: 5
Training loss: 2.912277936935425
Validation loss: 2.768261114756266

Epoch: 5| Step: 6
Training loss: 2.3573384284973145
Validation loss: 2.7679587307796685

Epoch: 5| Step: 7
Training loss: 3.0030975341796875
Validation loss: 2.769058076284265

Epoch: 5| Step: 8
Training loss: 2.993485927581787
Validation loss: 2.766493102555634

Epoch: 5| Step: 9
Training loss: 2.9686946868896484
Validation loss: 2.7637420931170062

Epoch: 5| Step: 10
Training loss: 2.5887980461120605
Validation loss: 2.7630911027231524

Epoch: 57| Step: 0
Training loss: 3.3647143840789795
Validation loss: 2.7635880362602974

Epoch: 5| Step: 1
Training loss: 3.379354476928711
Validation loss: 2.768178504000428

Epoch: 5| Step: 2
Training loss: 3.589055299758911
Validation loss: 2.7676333355647262

Epoch: 5| Step: 3
Training loss: 2.909621000289917
Validation loss: 2.766901526399838

Epoch: 5| Step: 4
Training loss: 2.607896566390991
Validation loss: 2.766500416622367

Epoch: 5| Step: 5
Training loss: 2.442934513092041
Validation loss: 2.7701506640321467

Epoch: 5| Step: 6
Training loss: 2.7914602756500244
Validation loss: 2.766338158679265

Epoch: 5| Step: 7
Training loss: 2.7643144130706787
Validation loss: 2.764816863562471

Epoch: 5| Step: 8
Training loss: 2.0791678428649902
Validation loss: 2.762990126045801

Epoch: 5| Step: 9
Training loss: 2.9436957836151123
Validation loss: 2.762372680889663

Epoch: 5| Step: 10
Training loss: 3.0613293647766113
Validation loss: 2.763418302741102

Epoch: 58| Step: 0
Training loss: 3.366300582885742
Validation loss: 2.7665878726590063

Epoch: 5| Step: 1
Training loss: 3.350295305252075
Validation loss: 2.7687173299891974

Epoch: 5| Step: 2
Training loss: 2.531731605529785
Validation loss: 2.7641951012355026

Epoch: 5| Step: 3
Training loss: 3.0998587608337402
Validation loss: 2.7659415455274683

Epoch: 5| Step: 4
Training loss: 2.7166295051574707
Validation loss: 2.7680497887314006

Epoch: 5| Step: 5
Training loss: 1.912057876586914
Validation loss: 2.7679234012480705

Epoch: 5| Step: 6
Training loss: 3.2813682556152344
Validation loss: 2.765210554163943

Epoch: 5| Step: 7
Training loss: 2.4852466583251953
Validation loss: 2.7650697923475698

Epoch: 5| Step: 8
Training loss: 3.3169777393341064
Validation loss: 2.7653048269210325

Epoch: 5| Step: 9
Training loss: 3.413339614868164
Validation loss: 2.7648192016027306

Epoch: 5| Step: 10
Training loss: 2.3247931003570557
Validation loss: 2.7642952396023657

Epoch: 59| Step: 0
Training loss: 2.453533887863159
Validation loss: 2.767519299701978

Epoch: 5| Step: 1
Training loss: 3.269176483154297
Validation loss: 2.7669505021905385

Epoch: 5| Step: 2
Training loss: 2.4811198711395264
Validation loss: 2.7684997871357906

Epoch: 5| Step: 3
Training loss: 2.6110618114471436
Validation loss: 2.765631070701025

Epoch: 5| Step: 4
Training loss: 2.8157694339752197
Validation loss: 2.764461730116157

Epoch: 5| Step: 5
Training loss: 3.3927783966064453
Validation loss: 2.7638962704648256

Epoch: 5| Step: 6
Training loss: 3.135024309158325
Validation loss: 2.7608412414468746

Epoch: 5| Step: 7
Training loss: 2.882560968399048
Validation loss: 2.764663662961734

Epoch: 5| Step: 8
Training loss: 3.191868305206299
Validation loss: 2.7601991443223852

Epoch: 5| Step: 9
Training loss: 3.0750370025634766
Validation loss: 2.7603632788504324

Epoch: 5| Step: 10
Training loss: 2.5334343910217285
Validation loss: 2.759799208692325

Epoch: 60| Step: 0
Training loss: 2.6206207275390625
Validation loss: 2.7603468894958496

Epoch: 5| Step: 1
Training loss: 3.1705260276794434
Validation loss: 2.75743531668058

Epoch: 5| Step: 2
Training loss: 3.0688586235046387
Validation loss: 2.759146777532434

Epoch: 5| Step: 3
Training loss: 2.84230375289917
Validation loss: 2.7570575052692043

Epoch: 5| Step: 4
Training loss: 3.4198334217071533
Validation loss: 2.758543601600073

Epoch: 5| Step: 5
Training loss: 2.2647030353546143
Validation loss: 2.7586235589878534

Epoch: 5| Step: 6
Training loss: 3.200592041015625
Validation loss: 2.7582506697664977

Epoch: 5| Step: 7
Training loss: 3.6877388954162598
Validation loss: 2.7614270051320395

Epoch: 5| Step: 8
Training loss: 2.7242090702056885
Validation loss: 2.7607614840230634

Epoch: 5| Step: 9
Training loss: 2.4990711212158203
Validation loss: 2.758104206413351

Epoch: 5| Step: 10
Training loss: 2.2567312717437744
Validation loss: 2.7560525196854786

Epoch: 61| Step: 0
Training loss: 2.3004870414733887
Validation loss: 2.7569264878508863

Epoch: 5| Step: 1
Training loss: 2.3452000617980957
Validation loss: 2.7569237550099692

Epoch: 5| Step: 2
Training loss: 2.7332561016082764
Validation loss: 2.756817010141188

Epoch: 5| Step: 3
Training loss: 3.231881618499756
Validation loss: 2.7551710272348053

Epoch: 5| Step: 4
Training loss: 2.337127685546875
Validation loss: 2.7529385961512083

Epoch: 5| Step: 5
Training loss: 2.8757903575897217
Validation loss: 2.75459639487728

Epoch: 5| Step: 6
Training loss: 3.097200393676758
Validation loss: 2.753119191815776

Epoch: 5| Step: 7
Training loss: 3.5452628135681152
Validation loss: 2.753187741002729

Epoch: 5| Step: 8
Training loss: 2.439047336578369
Validation loss: 2.753021773471627

Epoch: 5| Step: 9
Training loss: 3.8952155113220215
Validation loss: 2.753996738823511

Epoch: 5| Step: 10
Training loss: 3.0530526638031006
Validation loss: 2.752904535621725

Epoch: 62| Step: 0
Training loss: 3.089014768600464
Validation loss: 2.7530968035421064

Epoch: 5| Step: 1
Training loss: 3.01568341255188
Validation loss: 2.7499961468481247

Epoch: 5| Step: 2
Training loss: 3.926271438598633
Validation loss: 2.750475965520387

Epoch: 5| Step: 3
Training loss: 2.320061206817627
Validation loss: 2.753074158904373

Epoch: 5| Step: 4
Training loss: 3.0403285026550293
Validation loss: 2.751269586624638

Epoch: 5| Step: 5
Training loss: 2.5067782402038574
Validation loss: 2.747745203715499

Epoch: 5| Step: 6
Training loss: 2.585519313812256
Validation loss: 2.7510122329958024

Epoch: 5| Step: 7
Training loss: 2.7593834400177
Validation loss: 2.755792961325697

Epoch: 5| Step: 8
Training loss: 3.1433587074279785
Validation loss: 2.75422421322074

Epoch: 5| Step: 9
Training loss: 2.7229931354522705
Validation loss: 2.755154796825942

Epoch: 5| Step: 10
Training loss: 2.6606688499450684
Validation loss: 2.7534290308593423

Epoch: 63| Step: 0
Training loss: 3.286177158355713
Validation loss: 2.7551998784465175

Epoch: 5| Step: 1
Training loss: 3.047001361846924
Validation loss: 2.757358540770828

Epoch: 5| Step: 2
Training loss: 2.580207109451294
Validation loss: 2.7519068999957015

Epoch: 5| Step: 3
Training loss: 3.7040023803710938
Validation loss: 2.753019702049994

Epoch: 5| Step: 4
Training loss: 2.92101788520813
Validation loss: 2.749966090725314

Epoch: 5| Step: 5
Training loss: 2.865208148956299
Validation loss: 2.748510652972806

Epoch: 5| Step: 6
Training loss: 1.9700829982757568
Validation loss: 2.749624054919007

Epoch: 5| Step: 7
Training loss: 2.8213467597961426
Validation loss: 2.749889450688516

Epoch: 5| Step: 8
Training loss: 2.151869773864746
Validation loss: 2.7483206923289965

Epoch: 5| Step: 9
Training loss: 3.5382285118103027
Validation loss: 2.7474767700318368

Epoch: 5| Step: 10
Training loss: 2.912590265274048
Validation loss: 2.7480843374806065

Epoch: 64| Step: 0
Training loss: 2.926952600479126
Validation loss: 2.7468202037196003

Epoch: 5| Step: 1
Training loss: 2.4117958545684814
Validation loss: 2.7479360001061552

Epoch: 5| Step: 2
Training loss: 2.875124454498291
Validation loss: 2.7467335270297144

Epoch: 5| Step: 3
Training loss: 2.414712905883789
Validation loss: 2.747059319608955

Epoch: 5| Step: 4
Training loss: 2.3117592334747314
Validation loss: 2.745875202199464

Epoch: 5| Step: 5
Training loss: 3.4422619342803955
Validation loss: 2.7471361775552072

Epoch: 5| Step: 6
Training loss: 4.093784332275391
Validation loss: 2.7485753156805552

Epoch: 5| Step: 7
Training loss: 2.914680004119873
Validation loss: 2.7452397961770334

Epoch: 5| Step: 8
Training loss: 3.3311843872070312
Validation loss: 2.745820091616723

Epoch: 5| Step: 9
Training loss: 2.495969295501709
Validation loss: 2.7440792616977485

Epoch: 5| Step: 10
Training loss: 2.455228328704834
Validation loss: 2.7444763132320937

Epoch: 65| Step: 0
Training loss: 3.41919207572937
Validation loss: 2.743582423015307

Epoch: 5| Step: 1
Training loss: 2.383265972137451
Validation loss: 2.744201273046514

Epoch: 5| Step: 2
Training loss: 2.7510199546813965
Validation loss: 2.743022088081606

Epoch: 5| Step: 3
Training loss: 2.6861960887908936
Validation loss: 2.743247911494265

Epoch: 5| Step: 4
Training loss: 3.1433911323547363
Validation loss: 2.7419829881319435

Epoch: 5| Step: 5
Training loss: 2.7730915546417236
Validation loss: 2.7429310301298737

Epoch: 5| Step: 6
Training loss: 2.3183350563049316
Validation loss: 2.7409790869682067

Epoch: 5| Step: 7
Training loss: 2.9631025791168213
Validation loss: 2.742774683942077

Epoch: 5| Step: 8
Training loss: 3.1552510261535645
Validation loss: 2.74413590533759

Epoch: 5| Step: 9
Training loss: 2.8430657386779785
Validation loss: 2.744050992432461

Epoch: 5| Step: 10
Training loss: 3.332888603210449
Validation loss: 2.739884025307112

Epoch: 66| Step: 0
Training loss: 2.5881056785583496
Validation loss: 2.7391874072372273

Epoch: 5| Step: 1
Training loss: 3.2792751789093018
Validation loss: 2.7393874763160624

Epoch: 5| Step: 2
Training loss: 3.294477939605713
Validation loss: 2.740363654269967

Epoch: 5| Step: 3
Training loss: 2.2584614753723145
Validation loss: 2.740266671744726

Epoch: 5| Step: 4
Training loss: 3.0710506439208984
Validation loss: 2.741933987986657

Epoch: 5| Step: 5
Training loss: 3.278125047683716
Validation loss: 2.7374241172626452

Epoch: 5| Step: 6
Training loss: 2.5920112133026123
Validation loss: 2.738237370726883

Epoch: 5| Step: 7
Training loss: 2.7747433185577393
Validation loss: 2.7410013265507196

Epoch: 5| Step: 8
Training loss: 2.4879558086395264
Validation loss: 2.736767466350268

Epoch: 5| Step: 9
Training loss: 2.5618205070495605
Validation loss: 2.7384448974363265

Epoch: 5| Step: 10
Training loss: 3.6145694255828857
Validation loss: 2.7391125925125612

Epoch: 67| Step: 0
Training loss: 3.611762285232544
Validation loss: 2.735160578963577

Epoch: 5| Step: 1
Training loss: 2.6560864448547363
Validation loss: 2.7358214047647293

Epoch: 5| Step: 2
Training loss: 2.4114577770233154
Validation loss: 2.7341556907981954

Epoch: 5| Step: 3
Training loss: 3.652283191680908
Validation loss: 2.736931081741087

Epoch: 5| Step: 4
Training loss: 2.2888197898864746
Validation loss: 2.734591084141885

Epoch: 5| Step: 5
Training loss: 3.3774046897888184
Validation loss: 2.7377561343613492

Epoch: 5| Step: 6
Training loss: 2.1130356788635254
Validation loss: 2.736566223124022

Epoch: 5| Step: 7
Training loss: 2.36407732963562
Validation loss: 2.7351805625423307

Epoch: 5| Step: 8
Training loss: 3.2442946434020996
Validation loss: 2.7350890918444564

Epoch: 5| Step: 9
Training loss: 2.5239553451538086
Validation loss: 2.737525798941171

Epoch: 5| Step: 10
Training loss: 3.5177745819091797
Validation loss: 2.7356947211809057

Epoch: 68| Step: 0
Training loss: 2.9752745628356934
Validation loss: 2.7340386221485753

Epoch: 5| Step: 1
Training loss: 3.697006940841675
Validation loss: 2.7298489155307895

Epoch: 5| Step: 2
Training loss: 3.2833099365234375
Validation loss: 2.7290693162589945

Epoch: 5| Step: 3
Training loss: 2.290417432785034
Validation loss: 2.727775809585407

Epoch: 5| Step: 4
Training loss: 2.9719173908233643
Validation loss: 2.729123415485505

Epoch: 5| Step: 5
Training loss: 1.9541587829589844
Validation loss: 2.7287330063440467

Epoch: 5| Step: 6
Training loss: 2.4226436614990234
Validation loss: 2.72692733938976

Epoch: 5| Step: 7
Training loss: 2.968259334564209
Validation loss: 2.726659200524771

Epoch: 5| Step: 8
Training loss: 2.79282808303833
Validation loss: 2.7268661606696343

Epoch: 5| Step: 9
Training loss: 3.1515281200408936
Validation loss: 2.7244799880571264

Epoch: 5| Step: 10
Training loss: 3.1447811126708984
Validation loss: 2.7229312132763606

Epoch: 69| Step: 0
Training loss: 2.926389217376709
Validation loss: 2.723979849969187

Epoch: 5| Step: 1
Training loss: 2.5814003944396973
Validation loss: 2.7245004946185696

Epoch: 5| Step: 2
Training loss: 3.341702938079834
Validation loss: 2.7241584767577467

Epoch: 5| Step: 3
Training loss: 2.5672507286071777
Validation loss: 2.721352356736378

Epoch: 5| Step: 4
Training loss: 1.784785509109497
Validation loss: 2.723717369059081

Epoch: 5| Step: 5
Training loss: 2.8882415294647217
Validation loss: 2.72091184636598

Epoch: 5| Step: 6
Training loss: 2.7341725826263428
Validation loss: 2.718821535828293

Epoch: 5| Step: 7
Training loss: 2.932547092437744
Validation loss: 2.7191507329222975

Epoch: 5| Step: 8
Training loss: 2.8394081592559814
Validation loss: 2.71760477301895

Epoch: 5| Step: 9
Training loss: 3.459973096847534
Validation loss: 2.714239243538149

Epoch: 5| Step: 10
Training loss: 3.5383853912353516
Validation loss: 2.7145243152495353

Epoch: 70| Step: 0
Training loss: 3.311387538909912
Validation loss: 2.7154846524679535

Epoch: 5| Step: 1
Training loss: 2.8287060260772705
Validation loss: 2.7185821738294376

Epoch: 5| Step: 2
Training loss: 3.4427146911621094
Validation loss: 2.7149131938975346

Epoch: 5| Step: 3
Training loss: 2.1487209796905518
Validation loss: 2.7206169200199906

Epoch: 5| Step: 4
Training loss: 1.9331836700439453
Validation loss: 2.7196497045537478

Epoch: 5| Step: 5
Training loss: 2.97590970993042
Validation loss: 2.719576512613604

Epoch: 5| Step: 6
Training loss: 3.9436066150665283
Validation loss: 2.7169580536503948

Epoch: 5| Step: 7
Training loss: 3.5366859436035156
Validation loss: 2.7136697948619886

Epoch: 5| Step: 8
Training loss: 2.7269949913024902
Validation loss: 2.712715136107578

Epoch: 5| Step: 9
Training loss: 1.9099986553192139
Validation loss: 2.7085961885349725

Epoch: 5| Step: 10
Training loss: 2.65529465675354
Validation loss: 2.7098285716067076

Epoch: 71| Step: 0
Training loss: 2.4184131622314453
Validation loss: 2.7132627194927585

Epoch: 5| Step: 1
Training loss: 3.0628435611724854
Validation loss: 2.707674423853556

Epoch: 5| Step: 2
Training loss: 2.737016201019287
Validation loss: 2.7127892714674755

Epoch: 5| Step: 3
Training loss: 2.0191521644592285
Validation loss: 2.712669828886627

Epoch: 5| Step: 4
Training loss: 3.2918906211853027
Validation loss: 2.7107873296224945

Epoch: 5| Step: 5
Training loss: 3.2472243309020996
Validation loss: 2.7049518374986548

Epoch: 5| Step: 6
Training loss: 3.0049705505371094
Validation loss: 2.707589554530318

Epoch: 5| Step: 7
Training loss: 3.2578697204589844
Validation loss: 2.7084362481230047

Epoch: 5| Step: 8
Training loss: 2.9886717796325684
Validation loss: 2.703912717039867

Epoch: 5| Step: 9
Training loss: 2.261218786239624
Validation loss: 2.703700183540262

Epoch: 5| Step: 10
Training loss: 3.1147844791412354
Validation loss: 2.7038267043329056

Epoch: 72| Step: 0
Training loss: 2.447509765625
Validation loss: 2.70077189835169

Epoch: 5| Step: 1
Training loss: 2.674595594406128
Validation loss: 2.703054476809758

Epoch: 5| Step: 2
Training loss: 2.695415496826172
Validation loss: 2.6997029858250774

Epoch: 5| Step: 3
Training loss: 3.1470706462860107
Validation loss: 2.704279168959587

Epoch: 5| Step: 4
Training loss: 2.792113780975342
Validation loss: 2.7043219971400436

Epoch: 5| Step: 5
Training loss: 2.7676260471343994
Validation loss: 2.7053455486092517

Epoch: 5| Step: 6
Training loss: 2.25921630859375
Validation loss: 2.70603846990934

Epoch: 5| Step: 7
Training loss: 2.9100193977355957
Validation loss: 2.706537026231007

Epoch: 5| Step: 8
Training loss: 2.997624158859253
Validation loss: 2.701402412947788

Epoch: 5| Step: 9
Training loss: 3.8160033226013184
Validation loss: 2.6999167126993977

Epoch: 5| Step: 10
Training loss: 2.824280261993408
Validation loss: 2.7018886099579515

Epoch: 73| Step: 0
Training loss: 2.895476818084717
Validation loss: 2.696138917758901

Epoch: 5| Step: 1
Training loss: 2.511099338531494
Validation loss: 2.6942612048118346

Epoch: 5| Step: 2
Training loss: 2.7909417152404785
Validation loss: 2.69430794254426

Epoch: 5| Step: 3
Training loss: 3.0439229011535645
Validation loss: 2.6970252708722184

Epoch: 5| Step: 4
Training loss: 2.935805082321167
Validation loss: 2.697840770085653

Epoch: 5| Step: 5
Training loss: 2.615781545639038
Validation loss: 2.702692236951602

Epoch: 5| Step: 6
Training loss: 2.610198497772217
Validation loss: 2.700846646421699

Epoch: 5| Step: 7
Training loss: 3.478307008743286
Validation loss: 2.695467533603791

Epoch: 5| Step: 8
Training loss: 3.3354294300079346
Validation loss: 2.6980980826962377

Epoch: 5| Step: 9
Training loss: 2.6555304527282715
Validation loss: 2.694610811048938

Epoch: 5| Step: 10
Training loss: 2.303401470184326
Validation loss: 2.694631104828209

Epoch: 74| Step: 0
Training loss: 3.0085253715515137
Validation loss: 2.6949593379933345

Epoch: 5| Step: 1
Training loss: 3.0689616203308105
Validation loss: 2.6931576216092674

Epoch: 5| Step: 2
Training loss: 2.7235143184661865
Validation loss: 2.691589909215127

Epoch: 5| Step: 3
Training loss: 3.23150897026062
Validation loss: 2.6920338907549457

Epoch: 5| Step: 4
Training loss: 2.8948259353637695
Validation loss: 2.6880353804557555

Epoch: 5| Step: 5
Training loss: 2.8321268558502197
Validation loss: 2.697740670173399

Epoch: 5| Step: 6
Training loss: 2.5086722373962402
Validation loss: 2.711189793002221

Epoch: 5| Step: 7
Training loss: 3.4784603118896484
Validation loss: 2.7111358796396563

Epoch: 5| Step: 8
Training loss: 2.4810876846313477
Validation loss: 2.693049556465559

Epoch: 5| Step: 9
Training loss: 2.8747665882110596
Validation loss: 2.6903786300331034

Epoch: 5| Step: 10
Training loss: 2.042179584503174
Validation loss: 2.689395458467545

Epoch: 75| Step: 0
Training loss: 2.440743923187256
Validation loss: 2.690556659493395

Epoch: 5| Step: 1
Training loss: 3.636875867843628
Validation loss: 2.6901027310279106

Epoch: 5| Step: 2
Training loss: 2.928499698638916
Validation loss: 2.6905426671428065

Epoch: 5| Step: 3
Training loss: 2.987360954284668
Validation loss: 2.688495533440703

Epoch: 5| Step: 4
Training loss: 2.520819902420044
Validation loss: 2.6902907407411965

Epoch: 5| Step: 5
Training loss: 3.202908754348755
Validation loss: 2.6926116456267652

Epoch: 5| Step: 6
Training loss: 3.699659824371338
Validation loss: 2.6921321833005516

Epoch: 5| Step: 7
Training loss: 3.174527645111084
Validation loss: 2.689964017560405

Epoch: 5| Step: 8
Training loss: 2.3612515926361084
Validation loss: 2.6902353173942974

Epoch: 5| Step: 9
Training loss: 2.0757040977478027
Validation loss: 2.6895129706269953

Epoch: 5| Step: 10
Training loss: 2.0979647636413574
Validation loss: 2.6899401423751668

Epoch: 76| Step: 0
Training loss: 2.950469970703125
Validation loss: 2.692065904217382

Epoch: 5| Step: 1
Training loss: 3.003415584564209
Validation loss: 2.697027391003024

Epoch: 5| Step: 2
Training loss: 2.527759075164795
Validation loss: 2.6922162296951457

Epoch: 5| Step: 3
Training loss: 3.533964157104492
Validation loss: 2.693873933566514

Epoch: 5| Step: 4
Training loss: 2.956321954727173
Validation loss: 2.6990034580230713

Epoch: 5| Step: 5
Training loss: 2.644810199737549
Validation loss: 2.6987325094079457

Epoch: 5| Step: 6
Training loss: 3.070936679840088
Validation loss: 2.7000033701619794

Epoch: 5| Step: 7
Training loss: 2.5714235305786133
Validation loss: 2.7006958069339877

Epoch: 5| Step: 8
Training loss: 2.9686293601989746
Validation loss: 2.698712231010519

Epoch: 5| Step: 9
Training loss: 2.499451160430908
Validation loss: 2.687351970262425

Epoch: 5| Step: 10
Training loss: 2.450287103652954
Validation loss: 2.692742522044848

Epoch: 77| Step: 0
Training loss: 2.160759449005127
Validation loss: 2.6897535785551994

Epoch: 5| Step: 1
Training loss: 3.137028455734253
Validation loss: 2.6879087032810336

Epoch: 5| Step: 2
Training loss: 3.408998966217041
Validation loss: 2.6842819285649124

Epoch: 5| Step: 3
Training loss: 3.56876802444458
Validation loss: 2.6860199564246723

Epoch: 5| Step: 4
Training loss: 3.414180040359497
Validation loss: 2.6833366860625563

Epoch: 5| Step: 5
Training loss: 2.251187801361084
Validation loss: 2.6835749251868135

Epoch: 5| Step: 6
Training loss: 2.827524185180664
Validation loss: 2.6832908097133843

Epoch: 5| Step: 7
Training loss: 3.451976776123047
Validation loss: 2.6834912915383615

Epoch: 5| Step: 8
Training loss: 2.3010470867156982
Validation loss: 2.686790668836204

Epoch: 5| Step: 9
Training loss: 2.2361104488372803
Validation loss: 2.6897176875863025

Epoch: 5| Step: 10
Training loss: 2.3659780025482178
Validation loss: 2.68865494830634

Epoch: 78| Step: 0
Training loss: 3.208012342453003
Validation loss: 2.683965177946193

Epoch: 5| Step: 1
Training loss: 2.738711357116699
Validation loss: 2.6866336253381546

Epoch: 5| Step: 2
Training loss: 2.623781681060791
Validation loss: 2.683950944613385

Epoch: 5| Step: 3
Training loss: 3.0696635246276855
Validation loss: 2.6862848215205695

Epoch: 5| Step: 4
Training loss: 3.349992275238037
Validation loss: 2.6826110578352407

Epoch: 5| Step: 5
Training loss: 3.1658859252929688
Validation loss: 2.6851266378997476

Epoch: 5| Step: 6
Training loss: 3.141742706298828
Validation loss: 2.680496725984799

Epoch: 5| Step: 7
Training loss: 2.93383526802063
Validation loss: 2.6848209083721204

Epoch: 5| Step: 8
Training loss: 2.00003981590271
Validation loss: 2.680849095826508

Epoch: 5| Step: 9
Training loss: 2.144038200378418
Validation loss: 2.6823303289310907

Epoch: 5| Step: 10
Training loss: 2.742997884750366
Validation loss: 2.6792333356795774

Epoch: 79| Step: 0
Training loss: 2.5739197731018066
Validation loss: 2.68804689632949

Epoch: 5| Step: 1
Training loss: 3.2435154914855957
Validation loss: 2.6868260829679427

Epoch: 5| Step: 2
Training loss: 2.333153247833252
Validation loss: 2.6915059730570805

Epoch: 5| Step: 3
Training loss: 2.8784775733947754
Validation loss: 2.690289079502065

Epoch: 5| Step: 4
Training loss: 3.4720425605773926
Validation loss: 2.697543631317795

Epoch: 5| Step: 5
Training loss: 3.410489320755005
Validation loss: 2.6977619714634393

Epoch: 5| Step: 6
Training loss: 2.701878070831299
Validation loss: 2.699435164851527

Epoch: 5| Step: 7
Training loss: 3.1405136585235596
Validation loss: 2.6878554410831903

Epoch: 5| Step: 8
Training loss: 2.210663080215454
Validation loss: 2.6870671164604927

Epoch: 5| Step: 9
Training loss: 2.5469794273376465
Validation loss: 2.680073010024204

Epoch: 5| Step: 10
Training loss: 2.5511667728424072
Validation loss: 2.6737104205675024

Epoch: 80| Step: 0
Training loss: 3.053546667098999
Validation loss: 2.6805634165322907

Epoch: 5| Step: 1
Training loss: 2.2921793460845947
Validation loss: 2.6953152456591205

Epoch: 5| Step: 2
Training loss: 2.1735312938690186
Validation loss: 2.7152274500939155

Epoch: 5| Step: 3
Training loss: 2.8886702060699463
Validation loss: 2.7120402807830484

Epoch: 5| Step: 4
Training loss: 2.608715295791626
Validation loss: 2.6954245721140215

Epoch: 5| Step: 5
Training loss: 2.834404706954956
Validation loss: 2.6889446755891204

Epoch: 5| Step: 6
Training loss: 2.6373977661132812
Validation loss: 2.685073724357031

Epoch: 5| Step: 7
Training loss: 2.420896053314209
Validation loss: 2.6817442627363306

Epoch: 5| Step: 8
Training loss: 4.057207107543945
Validation loss: 2.6822887492436234

Epoch: 5| Step: 9
Training loss: 3.4635722637176514
Validation loss: 2.679870136322514

Epoch: 5| Step: 10
Training loss: 2.7037103176116943
Validation loss: 2.680946191151937

Epoch: 81| Step: 0
Training loss: 3.484441041946411
Validation loss: 2.678698544861168

Epoch: 5| Step: 1
Training loss: 2.234304189682007
Validation loss: 2.6852699402839906

Epoch: 5| Step: 2
Training loss: 2.5936119556427
Validation loss: 2.686730218190019

Epoch: 5| Step: 3
Training loss: 2.5192043781280518
Validation loss: 2.6892243226369223

Epoch: 5| Step: 4
Training loss: 2.4513254165649414
Validation loss: 2.6905848851767917

Epoch: 5| Step: 5
Training loss: 3.5418388843536377
Validation loss: 2.692081300161218

Epoch: 5| Step: 6
Training loss: 3.4118847846984863
Validation loss: 2.690146717973935

Epoch: 5| Step: 7
Training loss: 3.1134753227233887
Validation loss: 2.683958327898415

Epoch: 5| Step: 8
Training loss: 2.5768110752105713
Validation loss: 2.6763697388351604

Epoch: 5| Step: 9
Training loss: 2.4554553031921387
Validation loss: 2.676066929294217

Epoch: 5| Step: 10
Training loss: 2.7354629039764404
Validation loss: 2.6764153639475503

Epoch: 82| Step: 0
Training loss: 2.9768333435058594
Validation loss: 2.672225252274544

Epoch: 5| Step: 1
Training loss: 2.605916976928711
Validation loss: 2.6747720369728665

Epoch: 5| Step: 2
Training loss: 2.8984267711639404
Validation loss: 2.6733311453173236

Epoch: 5| Step: 3
Training loss: 2.741089344024658
Validation loss: 2.67359451324709

Epoch: 5| Step: 4
Training loss: 2.461103916168213
Validation loss: 2.673872358055525

Epoch: 5| Step: 5
Training loss: 3.604782819747925
Validation loss: 2.6738184857112106

Epoch: 5| Step: 6
Training loss: 2.2713074684143066
Validation loss: 2.6733755142458024

Epoch: 5| Step: 7
Training loss: 2.9070873260498047
Validation loss: 2.672090063812912

Epoch: 5| Step: 8
Training loss: 3.1431782245635986
Validation loss: 2.6700124535509335

Epoch: 5| Step: 9
Training loss: 2.4658451080322266
Validation loss: 2.672772676714005

Epoch: 5| Step: 10
Training loss: 2.992607831954956
Validation loss: 2.6722480943126063

Epoch: 83| Step: 0
Training loss: 3.1953699588775635
Validation loss: 2.6741051417525097

Epoch: 5| Step: 1
Training loss: 2.3553404808044434
Validation loss: 2.676131107473886

Epoch: 5| Step: 2
Training loss: 3.19610595703125
Validation loss: 2.6728526905018795

Epoch: 5| Step: 3
Training loss: 2.989952802658081
Validation loss: 2.6700150761553036

Epoch: 5| Step: 4
Training loss: 2.6694653034210205
Validation loss: 2.671452368459394

Epoch: 5| Step: 5
Training loss: 2.5394885540008545
Validation loss: 2.670949584694319

Epoch: 5| Step: 6
Training loss: 2.378495693206787
Validation loss: 2.6679985625769502

Epoch: 5| Step: 7
Training loss: 3.3152618408203125
Validation loss: 2.6670905492639028

Epoch: 5| Step: 8
Training loss: 2.6855506896972656
Validation loss: 2.6700983355122228

Epoch: 5| Step: 9
Training loss: 2.6980783939361572
Validation loss: 2.6696818105636106

Epoch: 5| Step: 10
Training loss: 3.0544655323028564
Validation loss: 2.6701651952599965

Epoch: 84| Step: 0
Training loss: 3.351278781890869
Validation loss: 2.6716943299898537

Epoch: 5| Step: 1
Training loss: 1.9107348918914795
Validation loss: 2.6695325912967807

Epoch: 5| Step: 2
Training loss: 3.019334316253662
Validation loss: 2.6676699089747604

Epoch: 5| Step: 3
Training loss: 2.8388726711273193
Validation loss: 2.6650353272755942

Epoch: 5| Step: 4
Training loss: 3.214245557785034
Validation loss: 2.6688327635488203

Epoch: 5| Step: 5
Training loss: 1.9974489212036133
Validation loss: 2.6674721035906064

Epoch: 5| Step: 6
Training loss: 3.3022994995117188
Validation loss: 2.668463571097261

Epoch: 5| Step: 7
Training loss: 2.5996267795562744
Validation loss: 2.667219287605696

Epoch: 5| Step: 8
Training loss: 3.1098551750183105
Validation loss: 2.6678807299624205

Epoch: 5| Step: 9
Training loss: 2.206198215484619
Validation loss: 2.6652732254356466

Epoch: 5| Step: 10
Training loss: 3.5357818603515625
Validation loss: 2.667003862319454

Epoch: 85| Step: 0
Training loss: 2.819059371948242
Validation loss: 2.669797920411633

Epoch: 5| Step: 1
Training loss: 3.3777337074279785
Validation loss: 2.668331351331485

Epoch: 5| Step: 2
Training loss: 3.1797988414764404
Validation loss: 2.6719274572146836

Epoch: 5| Step: 3
Training loss: 2.612360954284668
Validation loss: 2.675893255459365

Epoch: 5| Step: 4
Training loss: 2.2462680339813232
Validation loss: 2.6783804970402874

Epoch: 5| Step: 5
Training loss: 2.3839261531829834
Validation loss: 2.6740691226015807

Epoch: 5| Step: 6
Training loss: 3.3123199939727783
Validation loss: 2.6749606568326234

Epoch: 5| Step: 7
Training loss: 2.6227145195007324
Validation loss: 2.6707716372705277

Epoch: 5| Step: 8
Training loss: 2.7657957077026367
Validation loss: 2.670479223292361

Epoch: 5| Step: 9
Training loss: 3.364468812942505
Validation loss: 2.6699721813201904

Epoch: 5| Step: 10
Training loss: 2.2180511951446533
Validation loss: 2.665617630045901

Epoch: 86| Step: 0
Training loss: 3.128446578979492
Validation loss: 2.6676800507371143

Epoch: 5| Step: 1
Training loss: 3.0088553428649902
Validation loss: 2.667136074394308

Epoch: 5| Step: 2
Training loss: 2.5934276580810547
Validation loss: 2.669532637442312

Epoch: 5| Step: 3
Training loss: 3.2778427600860596
Validation loss: 2.673113005135649

Epoch: 5| Step: 4
Training loss: 2.3398618698120117
Validation loss: 2.6736941029948573

Epoch: 5| Step: 5
Training loss: 2.6953108310699463
Validation loss: 2.6797315164278914

Epoch: 5| Step: 6
Training loss: 2.6507346630096436
Validation loss: 2.6814108074352307

Epoch: 5| Step: 7
Training loss: 2.572242021560669
Validation loss: 2.6717130035482426

Epoch: 5| Step: 8
Training loss: 2.9259724617004395
Validation loss: 2.66652504603068

Epoch: 5| Step: 9
Training loss: 2.5213537216186523
Validation loss: 2.6654311867170435

Epoch: 5| Step: 10
Training loss: 3.2922956943511963
Validation loss: 2.6622804159759195

Epoch: 87| Step: 0
Training loss: 3.0673511028289795
Validation loss: 2.664157405976326

Epoch: 5| Step: 1
Training loss: 2.958548069000244
Validation loss: 2.6638808276063655

Epoch: 5| Step: 2
Training loss: 1.936979055404663
Validation loss: 2.6653546133349018

Epoch: 5| Step: 3
Training loss: 2.511176824569702
Validation loss: 2.6687033202058528

Epoch: 5| Step: 4
Training loss: 3.3592522144317627
Validation loss: 2.6741858502869964

Epoch: 5| Step: 5
Training loss: 2.8192615509033203
Validation loss: 2.6723241447120585

Epoch: 5| Step: 6
Training loss: 3.1287198066711426
Validation loss: 2.6696964233152327

Epoch: 5| Step: 7
Training loss: 2.7769973278045654
Validation loss: 2.6677111630798667

Epoch: 5| Step: 8
Training loss: 2.807081937789917
Validation loss: 2.663015937292448

Epoch: 5| Step: 9
Training loss: 3.123112916946411
Validation loss: 2.662881241049818

Epoch: 5| Step: 10
Training loss: 2.330411672592163
Validation loss: 2.6607860031948296

Epoch: 88| Step: 0
Training loss: 3.656928300857544
Validation loss: 2.660635689253448

Epoch: 5| Step: 1
Training loss: 3.0639190673828125
Validation loss: 2.6631828533705844

Epoch: 5| Step: 2
Training loss: 2.845646381378174
Validation loss: 2.6670962969462075

Epoch: 5| Step: 3
Training loss: 2.590538740158081
Validation loss: 2.667398997532424

Epoch: 5| Step: 4
Training loss: 2.1682772636413574
Validation loss: 2.664983252043365

Epoch: 5| Step: 5
Training loss: 2.5491890907287598
Validation loss: 2.6701124457902807

Epoch: 5| Step: 6
Training loss: 2.819438934326172
Validation loss: 2.6643016210166355

Epoch: 5| Step: 7
Training loss: 3.6886017322540283
Validation loss: 2.6645712724295993

Epoch: 5| Step: 8
Training loss: 2.2851593494415283
Validation loss: 2.6655738020455964

Epoch: 5| Step: 9
Training loss: 2.788856029510498
Validation loss: 2.6657151304265505

Epoch: 5| Step: 10
Training loss: 2.3461363315582275
Validation loss: 2.6684208505897113

Epoch: 89| Step: 0
Training loss: 3.040731906890869
Validation loss: 2.67066050857626

Epoch: 5| Step: 1
Training loss: 2.5585875511169434
Validation loss: 2.67180529204748

Epoch: 5| Step: 2
Training loss: 2.4450173377990723
Validation loss: 2.665245179207094

Epoch: 5| Step: 3
Training loss: 2.6314926147460938
Validation loss: 2.669717140095208

Epoch: 5| Step: 4
Training loss: 2.5901408195495605
Validation loss: 2.668490735433435

Epoch: 5| Step: 5
Training loss: 3.3640575408935547
Validation loss: 2.667220405353013

Epoch: 5| Step: 6
Training loss: 2.373016357421875
Validation loss: 2.669587230169645

Epoch: 5| Step: 7
Training loss: 2.9362874031066895
Validation loss: 2.6675091763978362

Epoch: 5| Step: 8
Training loss: 2.6310815811157227
Validation loss: 2.6636350308695147

Epoch: 5| Step: 9
Training loss: 3.003628969192505
Validation loss: 2.6633551325849307

Epoch: 5| Step: 10
Training loss: 3.3249852657318115
Validation loss: 2.664897329063826

Epoch: 90| Step: 0
Training loss: 3.495129108428955
Validation loss: 2.6591631622724634

Epoch: 5| Step: 1
Training loss: 2.807631254196167
Validation loss: 2.6644393269733717

Epoch: 5| Step: 2
Training loss: 3.1929514408111572
Validation loss: 2.6662592234150058

Epoch: 5| Step: 3
Training loss: 2.5382676124572754
Validation loss: 2.665186237263423

Epoch: 5| Step: 4
Training loss: 2.465564012527466
Validation loss: 2.661047420194072

Epoch: 5| Step: 5
Training loss: 2.509164333343506
Validation loss: 2.656767616989792

Epoch: 5| Step: 6
Training loss: 2.590376138687134
Validation loss: 2.6569581544527443

Epoch: 5| Step: 7
Training loss: 3.211134672164917
Validation loss: 2.65507839059317

Epoch: 5| Step: 8
Training loss: 3.0919365882873535
Validation loss: 2.6575835904767438

Epoch: 5| Step: 9
Training loss: 2.495573043823242
Validation loss: 2.6622593787408646

Epoch: 5| Step: 10
Training loss: 2.3898353576660156
Validation loss: 2.6587281098929783

Epoch: 91| Step: 0
Training loss: 2.703974723815918
Validation loss: 2.661236583545644

Epoch: 5| Step: 1
Training loss: 2.7548019886016846
Validation loss: 2.664091064083961

Epoch: 5| Step: 2
Training loss: 2.866821765899658
Validation loss: 2.6690367421796246

Epoch: 5| Step: 3
Training loss: 2.4075064659118652
Validation loss: 2.6790270984813733

Epoch: 5| Step: 4
Training loss: 3.1421163082122803
Validation loss: 2.6695281741439656

Epoch: 5| Step: 5
Training loss: 2.877840518951416
Validation loss: 2.6713573112282702

Epoch: 5| Step: 6
Training loss: 2.90674090385437
Validation loss: 2.6701756549137894

Epoch: 5| Step: 7
Training loss: 2.8688323497772217
Validation loss: 2.6645847007792485

Epoch: 5| Step: 8
Training loss: 2.7920289039611816
Validation loss: 2.6619888710719284

Epoch: 5| Step: 9
Training loss: 2.403712034225464
Validation loss: 2.6600562859607

Epoch: 5| Step: 10
Training loss: 3.215214490890503
Validation loss: 2.6545396902227916

Epoch: 92| Step: 0
Training loss: 2.2809059619903564
Validation loss: 2.652214209238688

Epoch: 5| Step: 1
Training loss: 2.412039279937744
Validation loss: 2.653615128609442

Epoch: 5| Step: 2
Training loss: 2.180030107498169
Validation loss: 2.658264983084894

Epoch: 5| Step: 3
Training loss: 3.2726292610168457
Validation loss: 2.661261958460654

Epoch: 5| Step: 4
Training loss: 4.092897415161133
Validation loss: 2.6574447488272064

Epoch: 5| Step: 5
Training loss: 2.314460277557373
Validation loss: 2.6574085117668234

Epoch: 5| Step: 6
Training loss: 2.8516223430633545
Validation loss: 2.658975052577193

Epoch: 5| Step: 7
Training loss: 2.952420711517334
Validation loss: 2.6578044173538045

Epoch: 5| Step: 8
Training loss: 3.1542439460754395
Validation loss: 2.6579155716844785

Epoch: 5| Step: 9
Training loss: 2.5429320335388184
Validation loss: 2.654800348384406

Epoch: 5| Step: 10
Training loss: 2.7958340644836426
Validation loss: 2.6527214639930317

Epoch: 93| Step: 0
Training loss: 2.5592079162597656
Validation loss: 2.6539032869441535

Epoch: 5| Step: 1
Training loss: 2.510477304458618
Validation loss: 2.6517976586536696

Epoch: 5| Step: 2
Training loss: 2.4297077655792236
Validation loss: 2.6519848544110536

Epoch: 5| Step: 3
Training loss: 2.643502950668335
Validation loss: 2.651044394380303

Epoch: 5| Step: 4
Training loss: 3.0984950065612793
Validation loss: 2.649902407841016

Epoch: 5| Step: 5
Training loss: 2.8701670169830322
Validation loss: 2.650552221523818

Epoch: 5| Step: 6
Training loss: 2.686065435409546
Validation loss: 2.6555411995098157

Epoch: 5| Step: 7
Training loss: 2.7647628784179688
Validation loss: 2.653889876539989

Epoch: 5| Step: 8
Training loss: 3.3529605865478516
Validation loss: 2.656865099424957

Epoch: 5| Step: 9
Training loss: 2.9481425285339355
Validation loss: 2.654727061589559

Epoch: 5| Step: 10
Training loss: 2.9589948654174805
Validation loss: 2.656913662469515

Epoch: 94| Step: 0
Training loss: 3.043612003326416
Validation loss: 2.655376888090564

Epoch: 5| Step: 1
Training loss: 2.600276470184326
Validation loss: 2.652363041395782

Epoch: 5| Step: 2
Training loss: 3.146756410598755
Validation loss: 2.650915648347588

Epoch: 5| Step: 3
Training loss: 3.3767504692077637
Validation loss: 2.6510256516036166

Epoch: 5| Step: 4
Training loss: 3.399890899658203
Validation loss: 2.646116743805588

Epoch: 5| Step: 5
Training loss: 1.5974705219268799
Validation loss: 2.6465347197748

Epoch: 5| Step: 6
Training loss: 2.4363138675689697
Validation loss: 2.6489005140078965

Epoch: 5| Step: 7
Training loss: 2.7309322357177734
Validation loss: 2.6474545899257866

Epoch: 5| Step: 8
Training loss: 2.4460482597351074
Validation loss: 2.645639701556134

Epoch: 5| Step: 9
Training loss: 3.436122179031372
Validation loss: 2.64786599784769

Epoch: 5| Step: 10
Training loss: 2.5053677558898926
Validation loss: 2.6483923901793776

Epoch: 95| Step: 0
Training loss: 2.653796672821045
Validation loss: 2.650475755814583

Epoch: 5| Step: 1
Training loss: 2.7491278648376465
Validation loss: 2.64674808133033

Epoch: 5| Step: 2
Training loss: 2.9002366065979004
Validation loss: 2.6462914969331477

Epoch: 5| Step: 3
Training loss: 3.0145692825317383
Validation loss: 2.645999426482826

Epoch: 5| Step: 4
Training loss: 2.8884878158569336
Validation loss: 2.6473235032891713

Epoch: 5| Step: 5
Training loss: 2.8460168838500977
Validation loss: 2.6456436059808217

Epoch: 5| Step: 6
Training loss: 3.0757155418395996
Validation loss: 2.6467727230441187

Epoch: 5| Step: 7
Training loss: 2.664846181869507
Validation loss: 2.645682739955123

Epoch: 5| Step: 8
Training loss: 2.5472114086151123
Validation loss: 2.6436865663015716

Epoch: 5| Step: 9
Training loss: 2.6499805450439453
Validation loss: 2.6452546786236506

Epoch: 5| Step: 10
Training loss: 2.717489719390869
Validation loss: 2.642961138038225

Epoch: 96| Step: 0
Training loss: 2.8274309635162354
Validation loss: 2.6430326456664712

Epoch: 5| Step: 1
Training loss: 2.886718988418579
Validation loss: 2.6471552182269353

Epoch: 5| Step: 2
Training loss: 3.0700156688690186
Validation loss: 2.644089088645033

Epoch: 5| Step: 3
Training loss: 3.228381633758545
Validation loss: 2.6449839940635105

Epoch: 5| Step: 4
Training loss: 2.292332172393799
Validation loss: 2.6420998214393534

Epoch: 5| Step: 5
Training loss: 2.695862054824829
Validation loss: 2.6420035054606776

Epoch: 5| Step: 6
Training loss: 2.9833977222442627
Validation loss: 2.6439323784202657

Epoch: 5| Step: 7
Training loss: 2.5504767894744873
Validation loss: 2.6423009339199273

Epoch: 5| Step: 8
Training loss: 2.458434581756592
Validation loss: 2.639045597404562

Epoch: 5| Step: 9
Training loss: 2.5725581645965576
Validation loss: 2.638838739805324

Epoch: 5| Step: 10
Training loss: 3.1731791496276855
Validation loss: 2.6378137475700787

Epoch: 97| Step: 0
Training loss: 3.344287395477295
Validation loss: 2.6443320705044653

Epoch: 5| Step: 1
Training loss: 2.7778682708740234
Validation loss: 2.6409582630280526

Epoch: 5| Step: 2
Training loss: 3.230839490890503
Validation loss: 2.6498602949162966

Epoch: 5| Step: 3
Training loss: 3.6415505409240723
Validation loss: 2.6555042702664613

Epoch: 5| Step: 4
Training loss: 2.051306962966919
Validation loss: 2.653406389297978

Epoch: 5| Step: 5
Training loss: 2.1522364616394043
Validation loss: 2.6512715175587642

Epoch: 5| Step: 6
Training loss: 2.697831630706787
Validation loss: 2.6532278471095587

Epoch: 5| Step: 7
Training loss: 3.295525312423706
Validation loss: 2.652887777615619

Epoch: 5| Step: 8
Training loss: 2.2514376640319824
Validation loss: 2.6516462090194866

Epoch: 5| Step: 9
Training loss: 2.515770435333252
Validation loss: 2.644947616002893

Epoch: 5| Step: 10
Training loss: 2.7308225631713867
Validation loss: 2.6430358220172185

Epoch: 98| Step: 0
Training loss: 2.729016065597534
Validation loss: 2.637610968723092

Epoch: 5| Step: 1
Training loss: 2.4473533630371094
Validation loss: 2.6374309139866985

Epoch: 5| Step: 2
Training loss: 3.263725996017456
Validation loss: 2.6410522922392814

Epoch: 5| Step: 3
Training loss: 2.9747426509857178
Validation loss: 2.6440415613112913

Epoch: 5| Step: 4
Training loss: 3.390049695968628
Validation loss: 2.6464613740162184

Epoch: 5| Step: 5
Training loss: 2.2220816612243652
Validation loss: 2.6447105023168747

Epoch: 5| Step: 6
Training loss: 2.8491568565368652
Validation loss: 2.643127672133907

Epoch: 5| Step: 7
Training loss: 2.0344343185424805
Validation loss: 2.6398062962357716

Epoch: 5| Step: 8
Training loss: 2.2974047660827637
Validation loss: 2.6329701254444737

Epoch: 5| Step: 9
Training loss: 3.71855092048645
Validation loss: 2.634957654501802

Epoch: 5| Step: 10
Training loss: 2.7784202098846436
Validation loss: 2.6377681224576888

Epoch: 99| Step: 0
Training loss: 3.010288715362549
Validation loss: 2.6366441660029913

Epoch: 5| Step: 1
Training loss: 2.7449123859405518
Validation loss: 2.637794276719452

Epoch: 5| Step: 2
Training loss: 2.8562357425689697
Validation loss: 2.636480790312572

Epoch: 5| Step: 3
Training loss: 3.2137179374694824
Validation loss: 2.6378452162588797

Epoch: 5| Step: 4
Training loss: 2.539475917816162
Validation loss: 2.6411305858242895

Epoch: 5| Step: 5
Training loss: 2.8526902198791504
Validation loss: 2.647332150449035

Epoch: 5| Step: 6
Training loss: 2.5345747470855713
Validation loss: 2.6495466463027464

Epoch: 5| Step: 7
Training loss: 2.0415894985198975
Validation loss: 2.6514727684759323

Epoch: 5| Step: 8
Training loss: 2.788449764251709
Validation loss: 2.652474526436098

Epoch: 5| Step: 9
Training loss: 2.897961139678955
Validation loss: 2.643624851780553

Epoch: 5| Step: 10
Training loss: 3.311572551727295
Validation loss: 2.637532144464472

Epoch: 100| Step: 0
Training loss: 2.303705930709839
Validation loss: 2.6334938464626187

Epoch: 5| Step: 1
Training loss: 2.944622039794922
Validation loss: 2.6344433753721175

Epoch: 5| Step: 2
Training loss: 2.9477314949035645
Validation loss: 2.634952868184736

Epoch: 5| Step: 3
Training loss: 2.653115749359131
Validation loss: 2.6380685657583256

Epoch: 5| Step: 4
Training loss: 2.590069532394409
Validation loss: 2.641549209112762

Epoch: 5| Step: 5
Training loss: 2.688532590866089
Validation loss: 2.6403281611780964

Epoch: 5| Step: 6
Training loss: 2.964545488357544
Validation loss: 2.638862294535483

Epoch: 5| Step: 7
Training loss: 3.468060255050659
Validation loss: 2.636919190806727

Epoch: 5| Step: 8
Training loss: 3.0434699058532715
Validation loss: 2.634378610118743

Epoch: 5| Step: 9
Training loss: 2.6798765659332275
Validation loss: 2.6340788026009836

Epoch: 5| Step: 10
Training loss: 2.3508689403533936
Validation loss: 2.6321541775939283

Epoch: 101| Step: 0
Training loss: 3.2066028118133545
Validation loss: 2.6319150514500116

Epoch: 5| Step: 1
Training loss: 3.1766269207000732
Validation loss: 2.6342430191655315

Epoch: 5| Step: 2
Training loss: 2.68623948097229
Validation loss: 2.6366705304832867

Epoch: 5| Step: 3
Training loss: 2.5349817276000977
Validation loss: 2.638513275372085

Epoch: 5| Step: 4
Training loss: 3.0762035846710205
Validation loss: 2.640478454610353

Epoch: 5| Step: 5
Training loss: 2.7336559295654297
Validation loss: 2.6441910446331067

Epoch: 5| Step: 6
Training loss: 3.0962746143341064
Validation loss: 2.647433534745247

Epoch: 5| Step: 7
Training loss: 2.6947903633117676
Validation loss: 2.648323307755173

Epoch: 5| Step: 8
Training loss: 2.3592324256896973
Validation loss: 2.6484155167815504

Epoch: 5| Step: 9
Training loss: 2.798992872238159
Validation loss: 2.65172912228492

Epoch: 5| Step: 10
Training loss: 2.2209126949310303
Validation loss: 2.644745962594145

Epoch: 102| Step: 0
Training loss: 2.4397292137145996
Validation loss: 2.640643876085999

Epoch: 5| Step: 1
Training loss: 2.8957717418670654
Validation loss: 2.6402620859043573

Epoch: 5| Step: 2
Training loss: 2.932072162628174
Validation loss: 2.6417492589642926

Epoch: 5| Step: 3
Training loss: 2.640434980392456
Validation loss: 2.636303706835675

Epoch: 5| Step: 4
Training loss: 2.464844226837158
Validation loss: 2.6334109331971858

Epoch: 5| Step: 5
Training loss: 3.3288216590881348
Validation loss: 2.630540609359741

Epoch: 5| Step: 6
Training loss: 3.1298794746398926
Validation loss: 2.629359491409794

Epoch: 5| Step: 7
Training loss: 2.441312074661255
Validation loss: 2.629230317249093

Epoch: 5| Step: 8
Training loss: 2.409395933151245
Validation loss: 2.632230866339899

Epoch: 5| Step: 9
Training loss: 3.642608165740967
Validation loss: 2.6281824137574885

Epoch: 5| Step: 10
Training loss: 2.1548337936401367
Validation loss: 2.6279146594385945

Epoch: 103| Step: 0
Training loss: 3.3851351737976074
Validation loss: 2.6275609462491927

Epoch: 5| Step: 1
Training loss: 3.2762560844421387
Validation loss: 2.6265285861107612

Epoch: 5| Step: 2
Training loss: 2.074414014816284
Validation loss: 2.6266491618207706

Epoch: 5| Step: 3
Training loss: 2.393177032470703
Validation loss: 2.630244537066388

Epoch: 5| Step: 4
Training loss: 2.4953858852386475
Validation loss: 2.632814375303125

Epoch: 5| Step: 5
Training loss: 3.3485946655273438
Validation loss: 2.6314772790478123

Epoch: 5| Step: 6
Training loss: 2.886017084121704
Validation loss: 2.6296133636146464

Epoch: 5| Step: 7
Training loss: 2.5632076263427734
Validation loss: 2.6268520227042575

Epoch: 5| Step: 8
Training loss: 2.941666841506958
Validation loss: 2.6253877865370883

Epoch: 5| Step: 9
Training loss: 2.994903564453125
Validation loss: 2.6264811869590514

Epoch: 5| Step: 10
Training loss: 2.1845285892486572
Validation loss: 2.62803082055943

Epoch: 104| Step: 0
Training loss: 2.898874282836914
Validation loss: 2.6265013423017276

Epoch: 5| Step: 1
Training loss: 2.6122164726257324
Validation loss: 2.6308416910068964

Epoch: 5| Step: 2
Training loss: 3.376488447189331
Validation loss: 2.629543947917159

Epoch: 5| Step: 3
Training loss: 3.0209920406341553
Validation loss: 2.628260848342731

Epoch: 5| Step: 4
Training loss: 3.1495399475097656
Validation loss: 2.6300014244612826

Epoch: 5| Step: 5
Training loss: 2.0539066791534424
Validation loss: 2.6280113189451155

Epoch: 5| Step: 6
Training loss: 2.5534305572509766
Validation loss: 2.6315651503942346

Epoch: 5| Step: 7
Training loss: 2.702348232269287
Validation loss: 2.6303149295109574

Epoch: 5| Step: 8
Training loss: 2.6741549968719482
Validation loss: 2.6252472477574504

Epoch: 5| Step: 9
Training loss: 2.5959701538085938
Validation loss: 2.6241635814789803

Epoch: 5| Step: 10
Training loss: 2.9480254650115967
Validation loss: 2.625871753179899

Epoch: 105| Step: 0
Training loss: 3.4482293128967285
Validation loss: 2.6231691657855944

Epoch: 5| Step: 1
Training loss: 3.5635242462158203
Validation loss: 2.6230079717533563

Epoch: 5| Step: 2
Training loss: 2.826378345489502
Validation loss: 2.6225087347851006

Epoch: 5| Step: 3
Training loss: 2.338291645050049
Validation loss: 2.624477663347798

Epoch: 5| Step: 4
Training loss: 2.462721347808838
Validation loss: 2.6251234444238807

Epoch: 5| Step: 5
Training loss: 3.1675708293914795
Validation loss: 2.6247741355690906

Epoch: 5| Step: 6
Training loss: 2.5342462062835693
Validation loss: 2.623580748035062

Epoch: 5| Step: 7
Training loss: 2.240055561065674
Validation loss: 2.621115320472307

Epoch: 5| Step: 8
Training loss: 2.9070560932159424
Validation loss: 2.6230077974257933

Epoch: 5| Step: 9
Training loss: 2.2364859580993652
Validation loss: 2.621440646468952

Epoch: 5| Step: 10
Training loss: 2.792217254638672
Validation loss: 2.6244285081022527

Epoch: 106| Step: 0
Training loss: 3.7976622581481934
Validation loss: 2.621762983260616

Epoch: 5| Step: 1
Training loss: 2.3040215969085693
Validation loss: 2.6229221538830827

Epoch: 5| Step: 2
Training loss: 3.275909423828125
Validation loss: 2.621184869479108

Epoch: 5| Step: 3
Training loss: 2.271300792694092
Validation loss: 2.62294259378987

Epoch: 5| Step: 4
Training loss: 2.5711491107940674
Validation loss: 2.6222785262651342

Epoch: 5| Step: 5
Training loss: 2.4507956504821777
Validation loss: 2.6227037035008913

Epoch: 5| Step: 6
Training loss: 3.0895111560821533
Validation loss: 2.623137084386682

Epoch: 5| Step: 7
Training loss: 2.647117853164673
Validation loss: 2.6190261687001875

Epoch: 5| Step: 8
Training loss: 2.7436206340789795
Validation loss: 2.6208770916026127

Epoch: 5| Step: 9
Training loss: 3.2534592151641846
Validation loss: 2.618990913514168

Epoch: 5| Step: 10
Training loss: 1.9747377634048462
Validation loss: 2.6215487475036294

Epoch: 107| Step: 0
Training loss: 2.9902420043945312
Validation loss: 2.6226573708236858

Epoch: 5| Step: 1
Training loss: 3.029481887817383
Validation loss: 2.6238224788378646

Epoch: 5| Step: 2
Training loss: 1.911383032798767
Validation loss: 2.6291396028252056

Epoch: 5| Step: 3
Training loss: 2.7532193660736084
Validation loss: 2.628486802501063

Epoch: 5| Step: 4
Training loss: 3.428351879119873
Validation loss: 2.6265275683454288

Epoch: 5| Step: 5
Training loss: 3.037374973297119
Validation loss: 2.625420662664598

Epoch: 5| Step: 6
Training loss: 2.5686869621276855
Validation loss: 2.6251482425197477

Epoch: 5| Step: 7
Training loss: 3.228471040725708
Validation loss: 2.6189886780195337

Epoch: 5| Step: 8
Training loss: 2.4148178100585938
Validation loss: 2.6155002117156982

Epoch: 5| Step: 9
Training loss: 3.081528425216675
Validation loss: 2.6143634652578704

Epoch: 5| Step: 10
Training loss: 1.9196802377700806
Validation loss: 2.6139846976085375

Epoch: 108| Step: 0
Training loss: 2.1885132789611816
Validation loss: 2.615024751232516

Epoch: 5| Step: 1
Training loss: 3.128990888595581
Validation loss: 2.6173631452745005

Epoch: 5| Step: 2
Training loss: 2.2841734886169434
Validation loss: 2.6167944887632966

Epoch: 5| Step: 3
Training loss: 2.9120028018951416
Validation loss: 2.616432556542017

Epoch: 5| Step: 4
Training loss: 3.1037399768829346
Validation loss: 2.6149780981002317

Epoch: 5| Step: 5
Training loss: 3.0394606590270996
Validation loss: 2.6187914084362727

Epoch: 5| Step: 6
Training loss: 2.046165943145752
Validation loss: 2.6171329059908466

Epoch: 5| Step: 7
Training loss: 2.8321304321289062
Validation loss: 2.6163955683349283

Epoch: 5| Step: 8
Training loss: 2.431591272354126
Validation loss: 2.616189182445567

Epoch: 5| Step: 9
Training loss: 3.5945587158203125
Validation loss: 2.615102473125663

Epoch: 5| Step: 10
Training loss: 2.933840751647949
Validation loss: 2.6159542606722925

Epoch: 109| Step: 0
Training loss: 2.5865962505340576
Validation loss: 2.6153414018692507

Epoch: 5| Step: 1
Training loss: 3.521144390106201
Validation loss: 2.6227396021607103

Epoch: 5| Step: 2
Training loss: 2.0074448585510254
Validation loss: 2.61971035311299

Epoch: 5| Step: 3
Training loss: 2.278467893600464
Validation loss: 2.622145181061119

Epoch: 5| Step: 4
Training loss: 2.9991507530212402
Validation loss: 2.619929628987466

Epoch: 5| Step: 5
Training loss: 2.9700582027435303
Validation loss: 2.616044564913678

Epoch: 5| Step: 6
Training loss: 2.678650140762329
Validation loss: 2.6132754100266324

Epoch: 5| Step: 7
Training loss: 2.6399378776550293
Validation loss: 2.6120562963588263

Epoch: 5| Step: 8
Training loss: 2.3889994621276855
Validation loss: 2.613882498074603

Epoch: 5| Step: 9
Training loss: 2.6492950916290283
Validation loss: 2.6124007573691745

Epoch: 5| Step: 10
Training loss: 3.9191720485687256
Validation loss: 2.611978725720477

Epoch: 110| Step: 0
Training loss: 3.0064306259155273
Validation loss: 2.611168412752049

Epoch: 5| Step: 1
Training loss: 3.4804561138153076
Validation loss: 2.6100953958367787

Epoch: 5| Step: 2
Training loss: 2.388298511505127
Validation loss: 2.610612154006958

Epoch: 5| Step: 3
Training loss: 2.01568865776062
Validation loss: 2.6109543449135235

Epoch: 5| Step: 4
Training loss: 2.9020605087280273
Validation loss: 2.61038149556806

Epoch: 5| Step: 5
Training loss: 2.2428245544433594
Validation loss: 2.6099771838034354

Epoch: 5| Step: 6
Training loss: 3.1383044719696045
Validation loss: 2.609579268322196

Epoch: 5| Step: 7
Training loss: 3.502676486968994
Validation loss: 2.6121043723116637

Epoch: 5| Step: 8
Training loss: 2.699237823486328
Validation loss: 2.6100097522940686

Epoch: 5| Step: 9
Training loss: 2.804140329360962
Validation loss: 2.611827322231826

Epoch: 5| Step: 10
Training loss: 2.157052993774414
Validation loss: 2.611805767141363

Epoch: 111| Step: 0
Training loss: 2.613539218902588
Validation loss: 2.6100855104384886

Epoch: 5| Step: 1
Training loss: 2.842294216156006
Validation loss: 2.609405086886498

Epoch: 5| Step: 2
Training loss: 2.463440418243408
Validation loss: 2.6064979055876374

Epoch: 5| Step: 3
Training loss: 2.3552584648132324
Validation loss: 2.6083170854917137

Epoch: 5| Step: 4
Training loss: 3.42423677444458
Validation loss: 2.609386351800734

Epoch: 5| Step: 5
Training loss: 2.8483870029449463
Validation loss: 2.609728387607041

Epoch: 5| Step: 6
Training loss: 3.436183452606201
Validation loss: 2.6111351418238815

Epoch: 5| Step: 7
Training loss: 2.2877135276794434
Validation loss: 2.6102221345388763

Epoch: 5| Step: 8
Training loss: 2.397019863128662
Validation loss: 2.607105529436501

Epoch: 5| Step: 9
Training loss: 2.8568248748779297
Validation loss: 2.6108914575269146

Epoch: 5| Step: 10
Training loss: 2.8854451179504395
Validation loss: 2.607635405755812

Epoch: 112| Step: 0
Training loss: 3.4313156604766846
Validation loss: 2.6096196225894395

Epoch: 5| Step: 1
Training loss: 2.2346653938293457
Validation loss: 2.608264487276795

Epoch: 5| Step: 2
Training loss: 2.3218514919281006
Validation loss: 2.608448156746485

Epoch: 5| Step: 3
Training loss: 2.759336233139038
Validation loss: 2.6113696303418887

Epoch: 5| Step: 4
Training loss: 2.774660587310791
Validation loss: 2.618294346717096

Epoch: 5| Step: 5
Training loss: 1.6958404779434204
Validation loss: 2.616057006261682

Epoch: 5| Step: 6
Training loss: 3.344722270965576
Validation loss: 2.612743846831783

Epoch: 5| Step: 7
Training loss: 2.802955389022827
Validation loss: 2.608073957504765

Epoch: 5| Step: 8
Training loss: 2.905857563018799
Validation loss: 2.6095041331424507

Epoch: 5| Step: 9
Training loss: 3.309749126434326
Validation loss: 2.609846450949228

Epoch: 5| Step: 10
Training loss: 2.818573236465454
Validation loss: 2.608906412637362

Epoch: 113| Step: 0
Training loss: 2.500777006149292
Validation loss: 2.614754922928349

Epoch: 5| Step: 1
Training loss: 2.3404247760772705
Validation loss: 2.6161973796864992

Epoch: 5| Step: 2
Training loss: 2.8195724487304688
Validation loss: 2.6094624432184363

Epoch: 5| Step: 3
Training loss: 3.09718918800354
Validation loss: 2.6086737750678934

Epoch: 5| Step: 4
Training loss: 3.033762216567993
Validation loss: 2.6080491312088503

Epoch: 5| Step: 5
Training loss: 2.684767961502075
Validation loss: 2.606085067154259

Epoch: 5| Step: 6
Training loss: 2.7127037048339844
Validation loss: 2.601136848490725

Epoch: 5| Step: 7
Training loss: 1.873064637184143
Validation loss: 2.606116356388215

Epoch: 5| Step: 8
Training loss: 2.7016310691833496
Validation loss: 2.606055898051108

Epoch: 5| Step: 9
Training loss: 3.348987579345703
Validation loss: 2.611655742891373

Epoch: 5| Step: 10
Training loss: 3.3019773960113525
Validation loss: 2.6098401085022958

Epoch: 114| Step: 0
Training loss: 2.6090779304504395
Validation loss: 2.608871985507268

Epoch: 5| Step: 1
Training loss: 2.9491825103759766
Validation loss: 2.606240241758285

Epoch: 5| Step: 2
Training loss: 2.9648852348327637
Validation loss: 2.6071530798430085

Epoch: 5| Step: 3
Training loss: 2.520610809326172
Validation loss: 2.609631830646146

Epoch: 5| Step: 4
Training loss: 2.481870174407959
Validation loss: 2.605940441931448

Epoch: 5| Step: 5
Training loss: 3.1884212493896484
Validation loss: 2.605088641566615

Epoch: 5| Step: 6
Training loss: 2.6366565227508545
Validation loss: 2.6043203056499524

Epoch: 5| Step: 7
Training loss: 2.5822508335113525
Validation loss: 2.6051429010206655

Epoch: 5| Step: 8
Training loss: 2.919753074645996
Validation loss: 2.605912034229566

Epoch: 5| Step: 9
Training loss: 2.708003282546997
Validation loss: 2.6111432711283364

Epoch: 5| Step: 10
Training loss: 2.8049917221069336
Validation loss: 2.608216160087175

Epoch: 115| Step: 0
Training loss: 3.5217742919921875
Validation loss: 2.60937399248923

Epoch: 5| Step: 1
Training loss: 2.3281071186065674
Validation loss: 2.6048142730548816

Epoch: 5| Step: 2
Training loss: 2.714676856994629
Validation loss: 2.6069004407493015

Epoch: 5| Step: 3
Training loss: 2.8112945556640625
Validation loss: 2.6032295303960002

Epoch: 5| Step: 4
Training loss: 2.7818121910095215
Validation loss: 2.6051509559795423

Epoch: 5| Step: 5
Training loss: 2.8968143463134766
Validation loss: 2.6038147095711

Epoch: 5| Step: 6
Training loss: 2.0541388988494873
Validation loss: 2.6056506223576044

Epoch: 5| Step: 7
Training loss: 3.051215171813965
Validation loss: 2.6013660866727113

Epoch: 5| Step: 8
Training loss: 2.355477809906006
Validation loss: 2.599234850175919

Epoch: 5| Step: 9
Training loss: 2.9094626903533936
Validation loss: 2.6020264215366815

Epoch: 5| Step: 10
Training loss: 2.962334156036377
Validation loss: 2.599252659787414

Epoch: 116| Step: 0
Training loss: 2.816568374633789
Validation loss: 2.6003835393536474

Epoch: 5| Step: 1
Training loss: 2.381904125213623
Validation loss: 2.601215011330061

Epoch: 5| Step: 2
Training loss: 2.501155376434326
Validation loss: 2.6034288034644177

Epoch: 5| Step: 3
Training loss: 3.3341705799102783
Validation loss: 2.607208805699502

Epoch: 5| Step: 4
Training loss: 2.1314234733581543
Validation loss: 2.6055575724570983

Epoch: 5| Step: 5
Training loss: 3.2068142890930176
Validation loss: 2.6073664080712105

Epoch: 5| Step: 6
Training loss: 2.6105408668518066
Validation loss: 2.602307294004707

Epoch: 5| Step: 7
Training loss: 2.3578033447265625
Validation loss: 2.602714712901782

Epoch: 5| Step: 8
Training loss: 3.184908390045166
Validation loss: 2.5965152043168263

Epoch: 5| Step: 9
Training loss: 2.823647975921631
Validation loss: 2.5977052898817163

Epoch: 5| Step: 10
Training loss: 3.026280164718628
Validation loss: 2.5994316454856627

Epoch: 117| Step: 0
Training loss: 2.271402597427368
Validation loss: 2.6012649305405153

Epoch: 5| Step: 1
Training loss: 3.1494250297546387
Validation loss: 2.5990580384449293

Epoch: 5| Step: 2
Training loss: 2.627107858657837
Validation loss: 2.6017514608239614

Epoch: 5| Step: 3
Training loss: 2.8293309211730957
Validation loss: 2.601266486670381

Epoch: 5| Step: 4
Training loss: 2.837345600128174
Validation loss: 2.6000303042832242

Epoch: 5| Step: 5
Training loss: 2.7497875690460205
Validation loss: 2.601908673522293

Epoch: 5| Step: 6
Training loss: 2.9048879146575928
Validation loss: 2.595456705298475

Epoch: 5| Step: 7
Training loss: 2.931697130203247
Validation loss: 2.5964939260995514

Epoch: 5| Step: 8
Training loss: 2.821190595626831
Validation loss: 2.596951897426318

Epoch: 5| Step: 9
Training loss: 2.296351909637451
Validation loss: 2.5967144709761425

Epoch: 5| Step: 10
Training loss: 2.892270803451538
Validation loss: 2.595006383875365

Epoch: 118| Step: 0
Training loss: 2.6288857460021973
Validation loss: 2.5970273325520177

Epoch: 5| Step: 1
Training loss: 2.5762877464294434
Validation loss: 2.5973603110159598

Epoch: 5| Step: 2
Training loss: 2.6077847480773926
Validation loss: 2.594512947144047

Epoch: 5| Step: 3
Training loss: 2.5822432041168213
Validation loss: 2.5965496493924047

Epoch: 5| Step: 4
Training loss: 2.087090492248535
Validation loss: 2.597163951525124

Epoch: 5| Step: 5
Training loss: 2.964047908782959
Validation loss: 2.595689296722412

Epoch: 5| Step: 6
Training loss: 3.380962371826172
Validation loss: 2.593425748168781

Epoch: 5| Step: 7
Training loss: 2.7892093658447266
Validation loss: 2.597338209870041

Epoch: 5| Step: 8
Training loss: 3.3131346702575684
Validation loss: 2.603179407376115

Epoch: 5| Step: 9
Training loss: 2.321014642715454
Validation loss: 2.6013686580042683

Epoch: 5| Step: 10
Training loss: 3.1009438037872314
Validation loss: 2.5989367602973856

Epoch: 119| Step: 0
Training loss: 2.5540060997009277
Validation loss: 2.5966671359154487

Epoch: 5| Step: 1
Training loss: 3.1353344917297363
Validation loss: 2.5928161785166752

Epoch: 5| Step: 2
Training loss: 2.9655909538269043
Validation loss: 2.590974912848524

Epoch: 5| Step: 3
Training loss: 3.4510884284973145
Validation loss: 2.5926385310388382

Epoch: 5| Step: 4
Training loss: 2.6711158752441406
Validation loss: 2.5918913028573476

Epoch: 5| Step: 5
Training loss: 2.550285816192627
Validation loss: 2.592343763638568

Epoch: 5| Step: 6
Training loss: 2.1532089710235596
Validation loss: 2.5918128669902845

Epoch: 5| Step: 7
Training loss: 2.9360930919647217
Validation loss: 2.5935829813762377

Epoch: 5| Step: 8
Training loss: 2.6011745929718018
Validation loss: 2.59126526822326

Epoch: 5| Step: 9
Training loss: 2.16763973236084
Validation loss: 2.5990975236379974

Epoch: 5| Step: 10
Training loss: 3.0607028007507324
Validation loss: 2.601164746028121

Epoch: 120| Step: 0
Training loss: 2.393955707550049
Validation loss: 2.619579145985265

Epoch: 5| Step: 1
Training loss: 2.9514098167419434
Validation loss: 2.656807621320089

Epoch: 5| Step: 2
Training loss: 2.598342180252075
Validation loss: 2.6743419093470417

Epoch: 5| Step: 3
Training loss: 3.116034984588623
Validation loss: 2.6608517144315984

Epoch: 5| Step: 4
Training loss: 2.3750884532928467
Validation loss: 2.637964669094291

Epoch: 5| Step: 5
Training loss: 2.659184217453003
Validation loss: 2.595614828089232

Epoch: 5| Step: 6
Training loss: 2.8085827827453613
Validation loss: 2.5936445266969743

Epoch: 5| Step: 7
Training loss: 2.7680752277374268
Validation loss: 2.594034694856213

Epoch: 5| Step: 8
Training loss: 2.6220273971557617
Validation loss: 2.5911225657309256

Epoch: 5| Step: 9
Training loss: 2.5484633445739746
Validation loss: 2.595432263548656

Epoch: 5| Step: 10
Training loss: 3.555482864379883
Validation loss: 2.596797655987483

Epoch: 121| Step: 0
Training loss: 2.6934008598327637
Validation loss: 2.597202144643312

Epoch: 5| Step: 1
Training loss: 2.761802911758423
Validation loss: 2.5919566257025606

Epoch: 5| Step: 2
Training loss: 2.9183952808380127
Validation loss: 2.5943423163506294

Epoch: 5| Step: 3
Training loss: 3.2438933849334717
Validation loss: 2.5926109360110376

Epoch: 5| Step: 4
Training loss: 2.6786320209503174
Validation loss: 2.5896777952871015

Epoch: 5| Step: 5
Training loss: 2.7930076122283936
Validation loss: 2.590276005447552

Epoch: 5| Step: 6
Training loss: 3.0625922679901123
Validation loss: 2.5857374770666963

Epoch: 5| Step: 7
Training loss: 2.171799898147583
Validation loss: 2.586721138287616

Epoch: 5| Step: 8
Training loss: 2.7279210090637207
Validation loss: 2.589575477825698

Epoch: 5| Step: 9
Training loss: 2.387500762939453
Validation loss: 2.590387528942477

Epoch: 5| Step: 10
Training loss: 2.829139232635498
Validation loss: 2.5933420094110633

Epoch: 122| Step: 0
Training loss: 3.399930477142334
Validation loss: 2.5946530270320114

Epoch: 5| Step: 1
Training loss: 2.151467800140381
Validation loss: 2.594777307202739

Epoch: 5| Step: 2
Training loss: 2.9916317462921143
Validation loss: 2.5886968028160835

Epoch: 5| Step: 3
Training loss: 3.4496216773986816
Validation loss: 2.595354651892057

Epoch: 5| Step: 4
Training loss: 1.8224579095840454
Validation loss: 2.5921490089867705

Epoch: 5| Step: 5
Training loss: 2.972053050994873
Validation loss: 2.5835764895203295

Epoch: 5| Step: 6
Training loss: 2.744321346282959
Validation loss: 2.5849467426218014

Epoch: 5| Step: 7
Training loss: 2.9004414081573486
Validation loss: 2.5844771554393153

Epoch: 5| Step: 8
Training loss: 2.8078906536102295
Validation loss: 2.5892980124360774

Epoch: 5| Step: 9
Training loss: 2.2792556285858154
Validation loss: 2.5872748077556653

Epoch: 5| Step: 10
Training loss: 2.6977787017822266
Validation loss: 2.5914538060465167

Epoch: 123| Step: 0
Training loss: 2.1804723739624023
Validation loss: 2.592124626200686

Epoch: 5| Step: 1
Training loss: 2.8261735439300537
Validation loss: 2.5909549395243325

Epoch: 5| Step: 2
Training loss: 2.6148951053619385
Validation loss: 2.590165533045287

Epoch: 5| Step: 3
Training loss: 3.0993459224700928
Validation loss: 2.5888537001866165

Epoch: 5| Step: 4
Training loss: 2.8897125720977783
Validation loss: 2.5852934211812992

Epoch: 5| Step: 5
Training loss: 2.935831069946289
Validation loss: 2.5849092929593978

Epoch: 5| Step: 6
Training loss: 2.349865436553955
Validation loss: 2.586179389748522

Epoch: 5| Step: 7
Training loss: 3.4945743083953857
Validation loss: 2.584239338033943

Epoch: 5| Step: 8
Training loss: 2.43285870552063
Validation loss: 2.584725615798786

Epoch: 5| Step: 9
Training loss: 3.131816864013672
Validation loss: 2.586946661754321

Epoch: 5| Step: 10
Training loss: 2.1702840328216553
Validation loss: 2.5862073154859644

Epoch: 124| Step: 0
Training loss: 2.5993363857269287
Validation loss: 2.585544788709251

Epoch: 5| Step: 1
Training loss: 3.13881254196167
Validation loss: 2.5899530867094636

Epoch: 5| Step: 2
Training loss: 2.936656951904297
Validation loss: 2.5895016321571926

Epoch: 5| Step: 3
Training loss: 2.264524459838867
Validation loss: 2.5884541721754175

Epoch: 5| Step: 4
Training loss: 2.896979808807373
Validation loss: 2.5809275616881666

Epoch: 5| Step: 5
Training loss: 2.207705020904541
Validation loss: 2.5846031147946595

Epoch: 5| Step: 6
Training loss: 2.9028804302215576
Validation loss: 2.5810642588523125

Epoch: 5| Step: 7
Training loss: 3.5719008445739746
Validation loss: 2.5829255093810377

Epoch: 5| Step: 8
Training loss: 2.6357932090759277
Validation loss: 2.582140394436416

Epoch: 5| Step: 9
Training loss: 2.4808335304260254
Validation loss: 2.581570217686315

Epoch: 5| Step: 10
Training loss: 2.4553589820861816
Validation loss: 2.584928999664963

Epoch: 125| Step: 0
Training loss: 2.5278689861297607
Validation loss: 2.587523785970544

Epoch: 5| Step: 1
Training loss: 2.3340680599212646
Validation loss: 2.5920799009261595

Epoch: 5| Step: 2
Training loss: 2.5559990406036377
Validation loss: 2.592505306325933

Epoch: 5| Step: 3
Training loss: 2.1829686164855957
Validation loss: 2.5956027712873233

Epoch: 5| Step: 4
Training loss: 3.1558053493499756
Validation loss: 2.5923668158951627

Epoch: 5| Step: 5
Training loss: 2.548994541168213
Validation loss: 2.5967791644475793

Epoch: 5| Step: 6
Training loss: 3.304622173309326
Validation loss: 2.594021184470064

Epoch: 5| Step: 7
Training loss: 3.1855876445770264
Validation loss: 2.5928547407991145

Epoch: 5| Step: 8
Training loss: 2.6624598503112793
Validation loss: 2.5900472364118023

Epoch: 5| Step: 9
Training loss: 2.7396888732910156
Validation loss: 2.584105453183574

Epoch: 5| Step: 10
Training loss: 2.921035051345825
Validation loss: 2.586566586648264

Epoch: 126| Step: 0
Training loss: 3.3837294578552246
Validation loss: 2.583662002317367

Epoch: 5| Step: 1
Training loss: 2.689650297164917
Validation loss: 2.5885593788598174

Epoch: 5| Step: 2
Training loss: 2.920314073562622
Validation loss: 2.5881669905877884

Epoch: 5| Step: 3
Training loss: 2.953930377960205
Validation loss: 2.587394170863654

Epoch: 5| Step: 4
Training loss: 2.6116394996643066
Validation loss: 2.5920910091810327

Epoch: 5| Step: 5
Training loss: 2.0579590797424316
Validation loss: 2.587827872204524

Epoch: 5| Step: 6
Training loss: 2.9237983226776123
Validation loss: 2.5859292553317164

Epoch: 5| Step: 7
Training loss: 2.9864001274108887
Validation loss: 2.5822038676149104

Epoch: 5| Step: 8
Training loss: 1.8002345561981201
Validation loss: 2.57720697054299

Epoch: 5| Step: 9
Training loss: 2.11830735206604
Validation loss: 2.5781266407300065

Epoch: 5| Step: 10
Training loss: 3.879446029663086
Validation loss: 2.5796528964914303

Epoch: 127| Step: 0
Training loss: 2.275815963745117
Validation loss: 2.5786395534392326

Epoch: 5| Step: 1
Training loss: 2.170767307281494
Validation loss: 2.5795691551700717

Epoch: 5| Step: 2
Training loss: 3.0275001525878906
Validation loss: 2.58075778202344

Epoch: 5| Step: 3
Training loss: 3.124992847442627
Validation loss: 2.57913476164623

Epoch: 5| Step: 4
Training loss: 3.5301334857940674
Validation loss: 2.583387724814876

Epoch: 5| Step: 5
Training loss: 3.38822865486145
Validation loss: 2.5807040660612044

Epoch: 5| Step: 6
Training loss: 3.145108938217163
Validation loss: 2.5764872181800103

Epoch: 5| Step: 7
Training loss: 2.6581242084503174
Validation loss: 2.578298925071634

Epoch: 5| Step: 8
Training loss: 2.4317381381988525
Validation loss: 2.575210066251857

Epoch: 5| Step: 9
Training loss: 2.0964865684509277
Validation loss: 2.574192423974314

Epoch: 5| Step: 10
Training loss: 2.2114195823669434
Validation loss: 2.5752655152351625

Epoch: 128| Step: 0
Training loss: 2.4031927585601807
Validation loss: 2.5764986725263697

Epoch: 5| Step: 1
Training loss: 2.274517297744751
Validation loss: 2.57440943871775

Epoch: 5| Step: 2
Training loss: 2.575955629348755
Validation loss: 2.5760116448966404

Epoch: 5| Step: 3
Training loss: 2.781888484954834
Validation loss: 2.5749387971816526

Epoch: 5| Step: 4
Training loss: 2.9195828437805176
Validation loss: 2.580123555275702

Epoch: 5| Step: 5
Training loss: 2.720986843109131
Validation loss: 2.576336117200954

Epoch: 5| Step: 6
Training loss: 3.018602132797241
Validation loss: 2.5793127116336616

Epoch: 5| Step: 7
Training loss: 2.874601364135742
Validation loss: 2.5774889505037697

Epoch: 5| Step: 8
Training loss: 2.577552318572998
Validation loss: 2.576910290666806

Epoch: 5| Step: 9
Training loss: 2.864180326461792
Validation loss: 2.5733219115964827

Epoch: 5| Step: 10
Training loss: 3.1581380367279053
Validation loss: 2.572089118342246

Epoch: 129| Step: 0
Training loss: 2.196242570877075
Validation loss: 2.5726761741022908

Epoch: 5| Step: 1
Training loss: 2.567699432373047
Validation loss: 2.576018894872358

Epoch: 5| Step: 2
Training loss: 2.7346267700195312
Validation loss: 2.5744642647363807

Epoch: 5| Step: 3
Training loss: 2.1823654174804688
Validation loss: 2.580665470451437

Epoch: 5| Step: 4
Training loss: 3.4890823364257812
Validation loss: 2.580157292786465

Epoch: 5| Step: 5
Training loss: 2.876181125640869
Validation loss: 2.5917217987839893

Epoch: 5| Step: 6
Training loss: 3.0013623237609863
Validation loss: 2.583445774611606

Epoch: 5| Step: 7
Training loss: 2.9708149433135986
Validation loss: 2.585458891366118

Epoch: 5| Step: 8
Training loss: 2.582350254058838
Validation loss: 2.5776099184507966

Epoch: 5| Step: 9
Training loss: 2.987107992172241
Validation loss: 2.5743858224602154

Epoch: 5| Step: 10
Training loss: 2.4355409145355225
Validation loss: 2.573731460878926

Epoch: 130| Step: 0
Training loss: 3.4101836681365967
Validation loss: 2.5749336288821314

Epoch: 5| Step: 1
Training loss: 2.7820353507995605
Validation loss: 2.571747940073731

Epoch: 5| Step: 2
Training loss: 2.272054433822632
Validation loss: 2.5748845915640555

Epoch: 5| Step: 3
Training loss: 2.4282326698303223
Validation loss: 2.576877486321234

Epoch: 5| Step: 4
Training loss: 1.897310495376587
Validation loss: 2.581296997685586

Epoch: 5| Step: 5
Training loss: 3.0517661571502686
Validation loss: 2.579974425736294

Epoch: 5| Step: 6
Training loss: 3.1416046619415283
Validation loss: 2.577376063152026

Epoch: 5| Step: 7
Training loss: 3.3624167442321777
Validation loss: 2.577530391754643

Epoch: 5| Step: 8
Training loss: 2.3176255226135254
Validation loss: 2.5710130096763693

Epoch: 5| Step: 9
Training loss: 2.7984859943389893
Validation loss: 2.5710250895510436

Epoch: 5| Step: 10
Training loss: 2.5617871284484863
Validation loss: 2.5715453675998154

Epoch: 131| Step: 0
Training loss: 2.9585766792297363
Validation loss: 2.5713574732503583

Epoch: 5| Step: 1
Training loss: 2.6925365924835205
Validation loss: 2.569769021003477

Epoch: 5| Step: 2
Training loss: 3.4446768760681152
Validation loss: 2.5724536911133797

Epoch: 5| Step: 3
Training loss: 2.7174410820007324
Validation loss: 2.571896478693972

Epoch: 5| Step: 4
Training loss: 3.8840794563293457
Validation loss: 2.573226039127637

Epoch: 5| Step: 5
Training loss: 2.2360243797302246
Validation loss: 2.5744978484287055

Epoch: 5| Step: 6
Training loss: 2.418485164642334
Validation loss: 2.5756323440100557

Epoch: 5| Step: 7
Training loss: 2.2688891887664795
Validation loss: 2.5723839934154222

Epoch: 5| Step: 8
Training loss: 2.146261692047119
Validation loss: 2.573529945906772

Epoch: 5| Step: 9
Training loss: 2.3640875816345215
Validation loss: 2.56929959020307

Epoch: 5| Step: 10
Training loss: 2.9283528327941895
Validation loss: 2.5716079922132593

Epoch: 132| Step: 0
Training loss: 2.9763927459716797
Validation loss: 2.5711853837454193

Epoch: 5| Step: 1
Training loss: 2.8559038639068604
Validation loss: 2.56999175266553

Epoch: 5| Step: 2
Training loss: 2.3420231342315674
Validation loss: 2.573706057763869

Epoch: 5| Step: 3
Training loss: 2.30724835395813
Validation loss: 2.5693204172195925

Epoch: 5| Step: 4
Training loss: 2.706592082977295
Validation loss: 2.5668556254397155

Epoch: 5| Step: 5
Training loss: 2.3782291412353516
Validation loss: 2.567256881344703

Epoch: 5| Step: 6
Training loss: 2.932677984237671
Validation loss: 2.5679289807555494

Epoch: 5| Step: 7
Training loss: 2.610027551651001
Validation loss: 2.57064768960399

Epoch: 5| Step: 8
Training loss: 2.252450942993164
Validation loss: 2.5670836010286884

Epoch: 5| Step: 9
Training loss: 3.4243385791778564
Validation loss: 2.569672910116052

Epoch: 5| Step: 10
Training loss: 3.288027048110962
Validation loss: 2.5699796343362458

Epoch: 133| Step: 0
Training loss: 2.0768914222717285
Validation loss: 2.571731605837422

Epoch: 5| Step: 1
Training loss: 3.07216215133667
Validation loss: 2.574609976942821

Epoch: 5| Step: 2
Training loss: 3.1280174255371094
Validation loss: 2.5717785525065597

Epoch: 5| Step: 3
Training loss: 3.2593891620635986
Validation loss: 2.57057890840756

Epoch: 5| Step: 4
Training loss: 2.4435317516326904
Validation loss: 2.5698626682322514

Epoch: 5| Step: 5
Training loss: 2.371570587158203
Validation loss: 2.569637883094049

Epoch: 5| Step: 6
Training loss: 2.6062064170837402
Validation loss: 2.5680821070107083

Epoch: 5| Step: 7
Training loss: 3.249513626098633
Validation loss: 2.567903321276429

Epoch: 5| Step: 8
Training loss: 2.3691582679748535
Validation loss: 2.5641060695853284

Epoch: 5| Step: 9
Training loss: 2.6314873695373535
Validation loss: 2.5647217099384596

Epoch: 5| Step: 10
Training loss: 2.7653651237487793
Validation loss: 2.5661060886998333

Epoch: 134| Step: 0
Training loss: 3.0620455741882324
Validation loss: 2.5620380268302014

Epoch: 5| Step: 1
Training loss: 2.693277359008789
Validation loss: 2.563454025535173

Epoch: 5| Step: 2
Training loss: 2.1902644634246826
Validation loss: 2.564624196739607

Epoch: 5| Step: 3
Training loss: 2.4255824089050293
Validation loss: 2.5647261040185088

Epoch: 5| Step: 4
Training loss: 2.9007034301757812
Validation loss: 2.5620130467158493

Epoch: 5| Step: 5
Training loss: 3.2376277446746826
Validation loss: 2.5602190315082507

Epoch: 5| Step: 6
Training loss: 2.346062421798706
Validation loss: 2.5630566279093423

Epoch: 5| Step: 7
Training loss: 2.1518959999084473
Validation loss: 2.560353409859442

Epoch: 5| Step: 8
Training loss: 3.1612930297851562
Validation loss: 2.5617984853765017

Epoch: 5| Step: 9
Training loss: 3.1370010375976562
Validation loss: 2.5583559210582445

Epoch: 5| Step: 10
Training loss: 2.624751329421997
Validation loss: 2.559480374859225

Epoch: 135| Step: 0
Training loss: 2.6023147106170654
Validation loss: 2.561228298371838

Epoch: 5| Step: 1
Training loss: 2.9381213188171387
Validation loss: 2.5623144359998804

Epoch: 5| Step: 2
Training loss: 2.784142255783081
Validation loss: 2.5607631027057605

Epoch: 5| Step: 3
Training loss: 2.596402883529663
Validation loss: 2.5601067543029785

Epoch: 5| Step: 4
Training loss: 3.2315526008605957
Validation loss: 2.5582208530877226

Epoch: 5| Step: 5
Training loss: 2.005783796310425
Validation loss: 2.5594127844738703

Epoch: 5| Step: 6
Training loss: 2.830779790878296
Validation loss: 2.558225690677602

Epoch: 5| Step: 7
Training loss: 2.9135143756866455
Validation loss: 2.5582185201747443

Epoch: 5| Step: 8
Training loss: 2.72330904006958
Validation loss: 2.559418668029129

Epoch: 5| Step: 9
Training loss: 2.4595370292663574
Validation loss: 2.5598382206373316

Epoch: 5| Step: 10
Training loss: 2.8652994632720947
Validation loss: 2.5643283244102233

Epoch: 136| Step: 0
Training loss: 2.467020273208618
Validation loss: 2.563558583618492

Epoch: 5| Step: 1
Training loss: 3.373217821121216
Validation loss: 2.5602712785044024

Epoch: 5| Step: 2
Training loss: 3.114755153656006
Validation loss: 2.5609180183820826

Epoch: 5| Step: 3
Training loss: 2.719621181488037
Validation loss: 2.5668479088814027

Epoch: 5| Step: 4
Training loss: 2.340895414352417
Validation loss: 2.569557048941171

Epoch: 5| Step: 5
Training loss: 2.761310577392578
Validation loss: 2.5629978000476794

Epoch: 5| Step: 6
Training loss: 2.508117914199829
Validation loss: 2.563937282049528

Epoch: 5| Step: 7
Training loss: 2.969151735305786
Validation loss: 2.5629383825486705

Epoch: 5| Step: 8
Training loss: 2.4603238105773926
Validation loss: 2.558012705977245

Epoch: 5| Step: 9
Training loss: 2.7434000968933105
Validation loss: 2.5633112153699322

Epoch: 5| Step: 10
Training loss: 2.432429552078247
Validation loss: 2.5601234179671093

Epoch: 137| Step: 0
Training loss: 2.717921733856201
Validation loss: 2.569780321531398

Epoch: 5| Step: 1
Training loss: 2.6091244220733643
Validation loss: 2.571204964832593

Epoch: 5| Step: 2
Training loss: 3.099275588989258
Validation loss: 2.5668378389009865

Epoch: 5| Step: 3
Training loss: 2.7234013080596924
Validation loss: 2.564022228281985

Epoch: 5| Step: 4
Training loss: 2.8330025672912598
Validation loss: 2.566617246597044

Epoch: 5| Step: 5
Training loss: 2.7246463298797607
Validation loss: 2.568678617477417

Epoch: 5| Step: 6
Training loss: 1.6087703704833984
Validation loss: 2.5752643128877044

Epoch: 5| Step: 7
Training loss: 3.231694459915161
Validation loss: 2.569578029776132

Epoch: 5| Step: 8
Training loss: 2.32844877243042
Validation loss: 2.5662565564596527

Epoch: 5| Step: 9
Training loss: 3.541726589202881
Validation loss: 2.562341151698943

Epoch: 5| Step: 10
Training loss: 2.613722801208496
Validation loss: 2.5559170489670127

Epoch: 138| Step: 0
Training loss: 2.4195923805236816
Validation loss: 2.5561485931437504

Epoch: 5| Step: 1
Training loss: 3.6332359313964844
Validation loss: 2.5560471345019597

Epoch: 5| Step: 2
Training loss: 2.751923084259033
Validation loss: 2.5573207793697232

Epoch: 5| Step: 3
Training loss: 2.0628185272216797
Validation loss: 2.556711471208962

Epoch: 5| Step: 4
Training loss: 2.6649327278137207
Validation loss: 2.5600576400756836

Epoch: 5| Step: 5
Training loss: 2.535947799682617
Validation loss: 2.557383216837401

Epoch: 5| Step: 6
Training loss: 3.1098527908325195
Validation loss: 2.5545632172656316

Epoch: 5| Step: 7
Training loss: 3.3355910778045654
Validation loss: 2.5516124028031544

Epoch: 5| Step: 8
Training loss: 2.3259947299957275
Validation loss: 2.558445748462472

Epoch: 5| Step: 9
Training loss: 2.629997730255127
Validation loss: 2.5524639596221266

Epoch: 5| Step: 10
Training loss: 2.3839499950408936
Validation loss: 2.5487047292852916

Epoch: 139| Step: 0
Training loss: 3.180938482284546
Validation loss: 2.554811713516071

Epoch: 5| Step: 1
Training loss: 3.632833957672119
Validation loss: 2.5535336309863674

Epoch: 5| Step: 2
Training loss: 3.1493613719940186
Validation loss: 2.5515905913486274

Epoch: 5| Step: 3
Training loss: 2.6838932037353516
Validation loss: 2.554495511516448

Epoch: 5| Step: 4
Training loss: 1.8368698358535767
Validation loss: 2.549245539531913

Epoch: 5| Step: 5
Training loss: 3.2117702960968018
Validation loss: 2.553561455460005

Epoch: 5| Step: 6
Training loss: 2.030625820159912
Validation loss: 2.5513963801886446

Epoch: 5| Step: 7
Training loss: 2.6480770111083984
Validation loss: 2.5516105313454904

Epoch: 5| Step: 8
Training loss: 2.643556594848633
Validation loss: 2.554420935210361

Epoch: 5| Step: 9
Training loss: 1.9688698053359985
Validation loss: 2.5500973552785893

Epoch: 5| Step: 10
Training loss: 2.911005735397339
Validation loss: 2.5541265959380777

Epoch: 140| Step: 0
Training loss: 1.7688674926757812
Validation loss: 2.5493799922286824

Epoch: 5| Step: 1
Training loss: 2.427861452102661
Validation loss: 2.5513950368409515

Epoch: 5| Step: 2
Training loss: 3.343292236328125
Validation loss: 2.556145878248317

Epoch: 5| Step: 3
Training loss: 2.956103563308716
Validation loss: 2.5528305115238314

Epoch: 5| Step: 4
Training loss: 3.103752613067627
Validation loss: 2.558361412376486

Epoch: 5| Step: 5
Training loss: 2.689451217651367
Validation loss: 2.5591539900789977

Epoch: 5| Step: 6
Training loss: 2.861345052719116
Validation loss: 2.559370917658652

Epoch: 5| Step: 7
Training loss: 2.885324001312256
Validation loss: 2.556258950182187

Epoch: 5| Step: 8
Training loss: 2.4128518104553223
Validation loss: 2.5540143969238445

Epoch: 5| Step: 9
Training loss: 2.3837087154388428
Validation loss: 2.550447630625899

Epoch: 5| Step: 10
Training loss: 3.0842862129211426
Validation loss: 2.5514707129488707

Epoch: 141| Step: 0
Training loss: 3.1017351150512695
Validation loss: 2.551835959957492

Epoch: 5| Step: 1
Training loss: 2.397437810897827
Validation loss: 2.5589485142820623

Epoch: 5| Step: 2
Training loss: 2.4522643089294434
Validation loss: 2.5515645345052085

Epoch: 5| Step: 3
Training loss: 2.509007215499878
Validation loss: 2.548692423810241

Epoch: 5| Step: 4
Training loss: 3.1367619037628174
Validation loss: 2.550367368164883

Epoch: 5| Step: 5
Training loss: 2.189793348312378
Validation loss: 2.5494759826249975

Epoch: 5| Step: 6
Training loss: 2.6523876190185547
Validation loss: 2.5430993597994567

Epoch: 5| Step: 7
Training loss: 2.6994025707244873
Validation loss: 2.5464315952793246

Epoch: 5| Step: 8
Training loss: 2.1333765983581543
Validation loss: 2.548438226023028

Epoch: 5| Step: 9
Training loss: 3.721250057220459
Validation loss: 2.550021320260981

Epoch: 5| Step: 10
Training loss: 2.901883840560913
Validation loss: 2.5525144197607554

Epoch: 142| Step: 0
Training loss: 2.832336664199829
Validation loss: 2.5520907986548638

Epoch: 5| Step: 1
Training loss: 2.567700147628784
Validation loss: 2.54931217111567

Epoch: 5| Step: 2
Training loss: 3.480705976486206
Validation loss: 2.5472076503179406

Epoch: 5| Step: 3
Training loss: 2.7465384006500244
Validation loss: 2.5452035191238567

Epoch: 5| Step: 4
Training loss: 2.6310644149780273
Validation loss: 2.5446018916304394

Epoch: 5| Step: 5
Training loss: 2.390960216522217
Validation loss: 2.545492741369432

Epoch: 5| Step: 6
Training loss: 2.763404130935669
Validation loss: 2.5444856536003853

Epoch: 5| Step: 7
Training loss: 2.55190110206604
Validation loss: 2.5452558968656804

Epoch: 5| Step: 8
Training loss: 2.9524178504943848
Validation loss: 2.5434772558109735

Epoch: 5| Step: 9
Training loss: 1.898411750793457
Validation loss: 2.5463969502397763

Epoch: 5| Step: 10
Training loss: 3.0495998859405518
Validation loss: 2.547770430964808

Epoch: 143| Step: 0
Training loss: 3.316134214401245
Validation loss: 2.5498130475321124

Epoch: 5| Step: 1
Training loss: 2.7711782455444336
Validation loss: 2.5506731182016353

Epoch: 5| Step: 2
Training loss: 2.602084159851074
Validation loss: 2.5521423380862

Epoch: 5| Step: 3
Training loss: 2.7179834842681885
Validation loss: 2.5499436675861316

Epoch: 5| Step: 4
Training loss: 2.1371848583221436
Validation loss: 2.55346998348031

Epoch: 5| Step: 5
Training loss: 2.435307741165161
Validation loss: 2.548921723519602

Epoch: 5| Step: 6
Training loss: 2.251542329788208
Validation loss: 2.5485246976216636

Epoch: 5| Step: 7
Training loss: 3.1169636249542236
Validation loss: 2.5478994871980403

Epoch: 5| Step: 8
Training loss: 2.870573043823242
Validation loss: 2.5475988849516837

Epoch: 5| Step: 9
Training loss: 3.0260612964630127
Validation loss: 2.551442766702303

Epoch: 5| Step: 10
Training loss: 2.505762815475464
Validation loss: 2.555530757032415

Epoch: 144| Step: 0
Training loss: 2.4474241733551025
Validation loss: 2.5627568293643255

Epoch: 5| Step: 1
Training loss: 3.0850536823272705
Validation loss: 2.5663015432255243

Epoch: 5| Step: 2
Training loss: 2.645120859146118
Validation loss: 2.5693496914320093

Epoch: 5| Step: 3
Training loss: 2.5141396522521973
Validation loss: 2.5650123396227436

Epoch: 5| Step: 4
Training loss: 2.7413244247436523
Validation loss: 2.5559906164805093

Epoch: 5| Step: 5
Training loss: 3.0578436851501465
Validation loss: 2.5497887185824815

Epoch: 5| Step: 6
Training loss: 2.845808506011963
Validation loss: 2.5452872117360434

Epoch: 5| Step: 7
Training loss: 2.960768461227417
Validation loss: 2.5436947038096767

Epoch: 5| Step: 8
Training loss: 2.487818717956543
Validation loss: 2.546296901600335

Epoch: 5| Step: 9
Training loss: 2.513017416000366
Validation loss: 2.555995466888592

Epoch: 5| Step: 10
Training loss: 2.6116209030151367
Validation loss: 2.55831314158696

Epoch: 145| Step: 0
Training loss: 2.6438345909118652
Validation loss: 2.5577079557603404

Epoch: 5| Step: 1
Training loss: 3.2196297645568848
Validation loss: 2.5513297332230436

Epoch: 5| Step: 2
Training loss: 2.9363715648651123
Validation loss: 2.5548339377167406

Epoch: 5| Step: 3
Training loss: 3.0117311477661133
Validation loss: 2.55003095698613

Epoch: 5| Step: 4
Training loss: 2.7212436199188232
Validation loss: 2.54936138532495

Epoch: 5| Step: 5
Training loss: 3.2661471366882324
Validation loss: 2.539144544191258

Epoch: 5| Step: 6
Training loss: 1.7666218280792236
Validation loss: 2.536536212890379

Epoch: 5| Step: 7
Training loss: 2.0955023765563965
Validation loss: 2.5385284372555312

Epoch: 5| Step: 8
Training loss: 2.9665026664733887
Validation loss: 2.5378033473927486

Epoch: 5| Step: 9
Training loss: 2.440046787261963
Validation loss: 2.5420727614433534

Epoch: 5| Step: 10
Training loss: 2.749555826187134
Validation loss: 2.5452091693878174

Epoch: 146| Step: 0
Training loss: 2.9558563232421875
Validation loss: 2.546656944418466

Epoch: 5| Step: 1
Training loss: 2.320582866668701
Validation loss: 2.5435595871299825

Epoch: 5| Step: 2
Training loss: 2.790231943130493
Validation loss: 2.536188058955695

Epoch: 5| Step: 3
Training loss: 3.1634745597839355
Validation loss: 2.5352724803391324

Epoch: 5| Step: 4
Training loss: 2.592480421066284
Validation loss: 2.5334598018277075

Epoch: 5| Step: 5
Training loss: 2.8206095695495605
Validation loss: 2.532978685953284

Epoch: 5| Step: 6
Training loss: 2.331073045730591
Validation loss: 2.536666544534827

Epoch: 5| Step: 7
Training loss: 2.344635009765625
Validation loss: 2.537422764685846

Epoch: 5| Step: 8
Training loss: 3.122093915939331
Validation loss: 2.5385624490758425

Epoch: 5| Step: 9
Training loss: 2.407376289367676
Validation loss: 2.5454887907992125

Epoch: 5| Step: 10
Training loss: 2.974853515625
Validation loss: 2.541620426280524

Epoch: 147| Step: 0
Training loss: 2.650899887084961
Validation loss: 2.546814698044972

Epoch: 5| Step: 1
Training loss: 2.5166046619415283
Validation loss: 2.549928257542272

Epoch: 5| Step: 2
Training loss: 2.4533917903900146
Validation loss: 2.5554298841825096

Epoch: 5| Step: 3
Training loss: 2.9062254428863525
Validation loss: 2.5534375482989895

Epoch: 5| Step: 4
Training loss: 3.429685592651367
Validation loss: 2.551277709263627

Epoch: 5| Step: 5
Training loss: 2.007887601852417
Validation loss: 2.5503244694843086

Epoch: 5| Step: 6
Training loss: 2.6737959384918213
Validation loss: 2.5423773847600466

Epoch: 5| Step: 7
Training loss: 2.5477490425109863
Validation loss: 2.531765922423332

Epoch: 5| Step: 8
Training loss: 3.5069801807403564
Validation loss: 2.531497965576828

Epoch: 5| Step: 9
Training loss: 2.967045545578003
Validation loss: 2.537186966147474

Epoch: 5| Step: 10
Training loss: 2.0754621028900146
Validation loss: 2.5384043057759604

Epoch: 148| Step: 0
Training loss: 2.2531418800354004
Validation loss: 2.546274318489977

Epoch: 5| Step: 1
Training loss: 2.932208299636841
Validation loss: 2.5482237185201337

Epoch: 5| Step: 2
Training loss: 2.5644071102142334
Validation loss: 2.5539749027580343

Epoch: 5| Step: 3
Training loss: 2.9062674045562744
Validation loss: 2.55270984865004

Epoch: 5| Step: 4
Training loss: 3.1474850177764893
Validation loss: 2.549803303134057

Epoch: 5| Step: 5
Training loss: 2.607968807220459
Validation loss: 2.544178798634519

Epoch: 5| Step: 6
Training loss: 2.6131529808044434
Validation loss: 2.5390920869765745

Epoch: 5| Step: 7
Training loss: 2.466248035430908
Validation loss: 2.53852427646678

Epoch: 5| Step: 8
Training loss: 3.097078800201416
Validation loss: 2.535273003321822

Epoch: 5| Step: 9
Training loss: 2.4374184608459473
Validation loss: 2.536561927487773

Epoch: 5| Step: 10
Training loss: 2.776735544204712
Validation loss: 2.533216130348944

Epoch: 149| Step: 0
Training loss: 2.215318202972412
Validation loss: 2.533311859253914

Epoch: 5| Step: 1
Training loss: 2.6871912479400635
Validation loss: 2.5349241328495804

Epoch: 5| Step: 2
Training loss: 2.827685594558716
Validation loss: 2.5352323080903743

Epoch: 5| Step: 3
Training loss: 2.871983289718628
Validation loss: 2.535865819582375

Epoch: 5| Step: 4
Training loss: 2.0467770099639893
Validation loss: 2.5378185984908894

Epoch: 5| Step: 5
Training loss: 3.1528446674346924
Validation loss: 2.5413857352349067

Epoch: 5| Step: 6
Training loss: 3.534140110015869
Validation loss: 2.5433971881866455

Epoch: 5| Step: 7
Training loss: 3.0179126262664795
Validation loss: 2.5448508185725056

Epoch: 5| Step: 8
Training loss: 2.03568959236145
Validation loss: 2.538157929656326

Epoch: 5| Step: 9
Training loss: 2.738223075866699
Validation loss: 2.5354093402944584

Epoch: 5| Step: 10
Training loss: 2.556960105895996
Validation loss: 2.539726834143362

Epoch: 150| Step: 0
Training loss: 2.767368793487549
Validation loss: 2.5394221941630044

Epoch: 5| Step: 1
Training loss: 2.787518262863159
Validation loss: 2.540087058979978

Epoch: 5| Step: 2
Training loss: 2.3079023361206055
Validation loss: 2.54272521439419

Epoch: 5| Step: 3
Training loss: 2.8561909198760986
Validation loss: 2.5401057017746793

Epoch: 5| Step: 4
Training loss: 2.462627410888672
Validation loss: 2.537568356401177

Epoch: 5| Step: 5
Training loss: 2.757737874984741
Validation loss: 2.529931655494116

Epoch: 5| Step: 6
Training loss: 2.907829523086548
Validation loss: 2.5336222238438104

Epoch: 5| Step: 7
Training loss: 3.0497164726257324
Validation loss: 2.5325719771846646

Epoch: 5| Step: 8
Training loss: 2.8212730884552
Validation loss: 2.542938447767688

Epoch: 5| Step: 9
Training loss: 2.566422700881958
Validation loss: 2.5459788665976575

Epoch: 5| Step: 10
Training loss: 2.4213054180145264
Validation loss: 2.5531099534803823

Testing loss: 2.678418609831068

Epoch: 1| Step: 0
Training loss: 4.948785781860352
Validation loss: 5.164468103839505

Epoch: 6| Step: 1
Training loss: 4.245697975158691
Validation loss: 5.160625765400548

Epoch: 6| Step: 2
Training loss: 4.4005537033081055
Validation loss: 5.156777176805722

Epoch: 6| Step: 3
Training loss: 5.652658462524414
Validation loss: 5.153000200948408

Epoch: 6| Step: 4
Training loss: 4.262897968292236
Validation loss: 5.149237489187589

Epoch: 6| Step: 5
Training loss: 4.909692287445068
Validation loss: 5.145359777635144

Epoch: 6| Step: 6
Training loss: 4.183093070983887
Validation loss: 5.141521474366547

Epoch: 6| Step: 7
Training loss: 4.812522888183594
Validation loss: 5.13771854933872

Epoch: 6| Step: 8
Training loss: 4.762466907501221
Validation loss: 5.133856117084462

Epoch: 6| Step: 9
Training loss: 5.227866172790527
Validation loss: 5.129623782250189

Epoch: 6| Step: 10
Training loss: 5.177652359008789
Validation loss: 5.1253839615852606

Epoch: 6| Step: 11
Training loss: 6.059392929077148
Validation loss: 5.121050450109666

Epoch: 6| Step: 12
Training loss: 5.247622489929199
Validation loss: 5.1165131497126755

Epoch: 6| Step: 13
Training loss: 5.414609909057617
Validation loss: 5.111778515641407

Epoch: 2| Step: 0
Training loss: 5.384929656982422
Validation loss: 5.106835585768505

Epoch: 6| Step: 1
Training loss: 4.321285724639893
Validation loss: 5.101655098699754

Epoch: 6| Step: 2
Training loss: 4.944170951843262
Validation loss: 5.096293044346635

Epoch: 6| Step: 3
Training loss: 4.082545757293701
Validation loss: 5.090758615924466

Epoch: 6| Step: 4
Training loss: 4.544478416442871
Validation loss: 5.084618794020786

Epoch: 6| Step: 5
Training loss: 5.447985649108887
Validation loss: 5.07851142268027

Epoch: 6| Step: 6
Training loss: 4.580624103546143
Validation loss: 5.071854560605941

Epoch: 6| Step: 7
Training loss: 4.525352478027344
Validation loss: 5.064960915555236

Epoch: 6| Step: 8
Training loss: 5.43875789642334
Validation loss: 5.057895752691453

Epoch: 6| Step: 9
Training loss: 4.86939811706543
Validation loss: 5.0504794530971076

Epoch: 6| Step: 10
Training loss: 4.737961769104004
Validation loss: 5.042342667938561

Epoch: 6| Step: 11
Training loss: 5.938144683837891
Validation loss: 5.0337345369400515

Epoch: 6| Step: 12
Training loss: 4.572324752807617
Validation loss: 5.025175827805714

Epoch: 6| Step: 13
Training loss: 4.542636871337891
Validation loss: 5.015490219157229

Epoch: 3| Step: 0
Training loss: 3.200657367706299
Validation loss: 5.0057786408291065

Epoch: 6| Step: 1
Training loss: 6.233466625213623
Validation loss: 4.995383047288464

Epoch: 6| Step: 2
Training loss: 4.732540607452393
Validation loss: 4.984586536243397

Epoch: 6| Step: 3
Training loss: 3.4737582206726074
Validation loss: 4.973143844194309

Epoch: 6| Step: 4
Training loss: 3.7625913619995117
Validation loss: 4.961702782620666

Epoch: 6| Step: 5
Training loss: 4.961441993713379
Validation loss: 4.948982669461158

Epoch: 6| Step: 6
Training loss: 4.308284759521484
Validation loss: 4.9363145725701445

Epoch: 6| Step: 7
Training loss: 4.674600124359131
Validation loss: 4.923111941224786

Epoch: 6| Step: 8
Training loss: 5.330484390258789
Validation loss: 4.908914858295072

Epoch: 6| Step: 9
Training loss: 5.365510940551758
Validation loss: 4.893112833781909

Epoch: 6| Step: 10
Training loss: 5.337160110473633
Validation loss: 4.877626506231165

Epoch: 6| Step: 11
Training loss: 4.345588207244873
Validation loss: 4.8608798878167265

Epoch: 6| Step: 12
Training loss: 5.808016777038574
Validation loss: 4.845080050089026

Epoch: 6| Step: 13
Training loss: 4.350527763366699
Validation loss: 4.826567088404009

Epoch: 4| Step: 0
Training loss: 5.1616129875183105
Validation loss: 4.808368626461234

Epoch: 6| Step: 1
Training loss: 3.9766221046447754
Validation loss: 4.789947930202689

Epoch: 6| Step: 2
Training loss: 3.9009323120117188
Validation loss: 4.770001754965834

Epoch: 6| Step: 3
Training loss: 4.865869522094727
Validation loss: 4.751552115204514

Epoch: 6| Step: 4
Training loss: 3.6834540367126465
Validation loss: 4.732369961277131

Epoch: 6| Step: 5
Training loss: 5.777831077575684
Validation loss: 4.711490733649141

Epoch: 6| Step: 6
Training loss: 4.830646514892578
Validation loss: 4.6916508623348765

Epoch: 6| Step: 7
Training loss: 3.6601505279541016
Validation loss: 4.67082893207509

Epoch: 6| Step: 8
Training loss: 4.187557220458984
Validation loss: 4.650328513114683

Epoch: 6| Step: 9
Training loss: 4.3798828125
Validation loss: 4.6294817873226695

Epoch: 6| Step: 10
Training loss: 4.605971336364746
Validation loss: 4.6089331719183155

Epoch: 6| Step: 11
Training loss: 4.351615905761719
Validation loss: 4.588672955830892

Epoch: 6| Step: 12
Training loss: 5.399719715118408
Validation loss: 4.56802152305521

Epoch: 6| Step: 13
Training loss: 3.2637739181518555
Validation loss: 4.547075627952494

Epoch: 5| Step: 0
Training loss: 4.533220291137695
Validation loss: 4.527705218202325

Epoch: 6| Step: 1
Training loss: 4.61585807800293
Validation loss: 4.507010426572574

Epoch: 6| Step: 2
Training loss: 3.470026969909668
Validation loss: 4.488542556762695

Epoch: 6| Step: 3
Training loss: 4.006058692932129
Validation loss: 4.4690162238254345

Epoch: 6| Step: 4
Training loss: 4.371153831481934
Validation loss: 4.4506757746460615

Epoch: 6| Step: 5
Training loss: 4.48422384262085
Validation loss: 4.430807805830432

Epoch: 6| Step: 6
Training loss: 2.3781166076660156
Validation loss: 4.411920819231259

Epoch: 6| Step: 7
Training loss: 4.264028549194336
Validation loss: 4.394614824684718

Epoch: 6| Step: 8
Training loss: 4.729145050048828
Validation loss: 4.3782329046598045

Epoch: 6| Step: 9
Training loss: 5.863218307495117
Validation loss: 4.360128515510149

Epoch: 6| Step: 10
Training loss: 3.8866066932678223
Validation loss: 4.342872332501155

Epoch: 6| Step: 11
Training loss: 4.207292556762695
Validation loss: 4.325182507115025

Epoch: 6| Step: 12
Training loss: 3.447500228881836
Validation loss: 4.308653780209121

Epoch: 6| Step: 13
Training loss: 4.908663272857666
Validation loss: 4.2911462373630975

Epoch: 6| Step: 0
Training loss: 4.297550201416016
Validation loss: 4.273986108841434

Epoch: 6| Step: 1
Training loss: 3.4956507682800293
Validation loss: 4.258799675972231

Epoch: 6| Step: 2
Training loss: 2.5509068965911865
Validation loss: 4.240391136497579

Epoch: 6| Step: 3
Training loss: 4.7611846923828125
Validation loss: 4.226887303013956

Epoch: 6| Step: 4
Training loss: 4.7745771408081055
Validation loss: 4.212340800992904

Epoch: 6| Step: 5
Training loss: 4.002829551696777
Validation loss: 4.19840007699946

Epoch: 6| Step: 6
Training loss: 5.587771415710449
Validation loss: 4.184444581308672

Epoch: 6| Step: 7
Training loss: 3.898974895477295
Validation loss: 4.171928313470656

Epoch: 6| Step: 8
Training loss: 2.150087356567383
Validation loss: 4.159606138865153

Epoch: 6| Step: 9
Training loss: 3.895861864089966
Validation loss: 4.147482569499682

Epoch: 6| Step: 10
Training loss: 5.306520462036133
Validation loss: 4.1356323329351286

Epoch: 6| Step: 11
Training loss: 4.222114086151123
Validation loss: 4.123403964504119

Epoch: 6| Step: 12
Training loss: 3.158987522125244
Validation loss: 4.111747649408156

Epoch: 6| Step: 13
Training loss: 3.944462299346924
Validation loss: 4.099262314458048

Epoch: 7| Step: 0
Training loss: 4.590743064880371
Validation loss: 4.08805497487386

Epoch: 6| Step: 1
Training loss: 4.062445640563965
Validation loss: 4.078379533624136

Epoch: 6| Step: 2
Training loss: 4.147716045379639
Validation loss: 4.065749204286965

Epoch: 6| Step: 3
Training loss: 3.63162899017334
Validation loss: 4.055137449695218

Epoch: 6| Step: 4
Training loss: 5.042812347412109
Validation loss: 4.044786043064569

Epoch: 6| Step: 5
Training loss: 2.932302951812744
Validation loss: 4.0343934130925

Epoch: 6| Step: 6
Training loss: 4.200288772583008
Validation loss: 4.024065991883637

Epoch: 6| Step: 7
Training loss: 3.088174343109131
Validation loss: 4.011400520160634

Epoch: 6| Step: 8
Training loss: 3.554492950439453
Validation loss: 3.9994197404512795

Epoch: 6| Step: 9
Training loss: 4.32928466796875
Validation loss: 3.987613808724188

Epoch: 6| Step: 10
Training loss: 3.6858034133911133
Validation loss: 3.9779165021834837

Epoch: 6| Step: 11
Training loss: 3.182931423187256
Validation loss: 3.9640311374459216

Epoch: 6| Step: 12
Training loss: 3.69834041595459
Validation loss: 3.9527467040605444

Epoch: 6| Step: 13
Training loss: 4.001689910888672
Validation loss: 3.9408102445704962

Epoch: 8| Step: 0
Training loss: 2.166250705718994
Validation loss: 3.9309441069121003

Epoch: 6| Step: 1
Training loss: 4.813852310180664
Validation loss: 3.9212741262169293

Epoch: 6| Step: 2
Training loss: 3.937577486038208
Validation loss: 3.9100076870251725

Epoch: 6| Step: 3
Training loss: 3.7099742889404297
Validation loss: 3.900165355333718

Epoch: 6| Step: 4
Training loss: 1.899871826171875
Validation loss: 3.890083113024312

Epoch: 6| Step: 5
Training loss: 3.6628806591033936
Validation loss: 3.879172530225528

Epoch: 6| Step: 6
Training loss: 5.748225212097168
Validation loss: 3.8710237728652133

Epoch: 6| Step: 7
Training loss: 3.752004623413086
Validation loss: 3.8618524920555855

Epoch: 6| Step: 8
Training loss: 2.880612850189209
Validation loss: 3.851919568994994

Epoch: 6| Step: 9
Training loss: 4.149353981018066
Validation loss: 3.840913329073178

Epoch: 6| Step: 10
Training loss: 4.297403335571289
Validation loss: 3.8332211432918424

Epoch: 6| Step: 11
Training loss: 3.7979369163513184
Validation loss: 3.824339187273415

Epoch: 6| Step: 12
Training loss: 4.210986137390137
Validation loss: 3.815555621218938

Epoch: 6| Step: 13
Training loss: 3.1141138076782227
Validation loss: 3.806795381730603

Epoch: 9| Step: 0
Training loss: 3.9520509243011475
Validation loss: 3.7988183293291318

Epoch: 6| Step: 1
Training loss: 4.02345085144043
Validation loss: 3.7903611506185224

Epoch: 6| Step: 2
Training loss: 3.0046186447143555
Validation loss: 3.781817123454104

Epoch: 6| Step: 3
Training loss: 3.441962242126465
Validation loss: 3.7742698423324095

Epoch: 6| Step: 4
Training loss: 3.205568790435791
Validation loss: 3.7656304810636785

Epoch: 6| Step: 5
Training loss: 4.858650207519531
Validation loss: 3.759190967006068

Epoch: 6| Step: 6
Training loss: 3.283174991607666
Validation loss: 3.752005943688013

Epoch: 6| Step: 7
Training loss: 4.306735038757324
Validation loss: 3.7445438497809955

Epoch: 6| Step: 8
Training loss: 3.403080463409424
Validation loss: 3.7381189048931165

Epoch: 6| Step: 9
Training loss: 3.896867275238037
Validation loss: 3.7320670748269684

Epoch: 6| Step: 10
Training loss: 3.936443567276001
Validation loss: 3.7223020856098463

Epoch: 6| Step: 11
Training loss: 2.9588639736175537
Validation loss: 3.71671921976151

Epoch: 6| Step: 12
Training loss: 3.7531208992004395
Validation loss: 3.7093630452309885

Epoch: 6| Step: 13
Training loss: 2.460017204284668
Validation loss: 3.7034362131549465

Epoch: 10| Step: 0
Training loss: 4.556143283843994
Validation loss: 3.6982411774255897

Epoch: 6| Step: 1
Training loss: 3.333475112915039
Validation loss: 3.689904705170662

Epoch: 6| Step: 2
Training loss: 3.5918872356414795
Validation loss: 3.6859284011266564

Epoch: 6| Step: 3
Training loss: 3.803584098815918
Validation loss: 3.6789719058621313

Epoch: 6| Step: 4
Training loss: 4.158360481262207
Validation loss: 3.671062602791735

Epoch: 6| Step: 5
Training loss: 3.0309391021728516
Validation loss: 3.6636518201520367

Epoch: 6| Step: 6
Training loss: 2.458042621612549
Validation loss: 3.6552941517163346

Epoch: 6| Step: 7
Training loss: 4.708332061767578
Validation loss: 3.6504864000505015

Epoch: 6| Step: 8
Training loss: 2.7225775718688965
Validation loss: 3.640079523927422

Epoch: 6| Step: 9
Training loss: 4.121769905090332
Validation loss: 3.6341314213250273

Epoch: 6| Step: 10
Training loss: 3.2064361572265625
Validation loss: 3.625497500101725

Epoch: 6| Step: 11
Training loss: 3.6656417846679688
Validation loss: 3.616920291736562

Epoch: 6| Step: 12
Training loss: 3.0250649452209473
Validation loss: 3.608452868717973

Epoch: 6| Step: 13
Training loss: 3.382809638977051
Validation loss: 3.6009941639438754

Epoch: 11| Step: 0
Training loss: 4.231784820556641
Validation loss: 3.5944578442522275

Epoch: 6| Step: 1
Training loss: 3.966557264328003
Validation loss: 3.587158505634595

Epoch: 6| Step: 2
Training loss: 2.505082368850708
Validation loss: 3.5773191195662304

Epoch: 6| Step: 3
Training loss: 2.220816135406494
Validation loss: 3.5728682805133123

Epoch: 6| Step: 4
Training loss: 4.371394157409668
Validation loss: 3.564553055711972

Epoch: 6| Step: 5
Training loss: 3.6684179306030273
Validation loss: 3.559466346617668

Epoch: 6| Step: 6
Training loss: 3.9734935760498047
Validation loss: 3.553670116650161

Epoch: 6| Step: 7
Training loss: 3.451096773147583
Validation loss: 3.5452533050249984

Epoch: 6| Step: 8
Training loss: 3.5036396980285645
Validation loss: 3.5422152549989763

Epoch: 6| Step: 9
Training loss: 2.522890090942383
Validation loss: 3.533864167428786

Epoch: 6| Step: 10
Training loss: 3.5949573516845703
Validation loss: 3.5284598488961496

Epoch: 6| Step: 11
Training loss: 3.5868587493896484
Validation loss: 3.5229189934269076

Epoch: 6| Step: 12
Training loss: 3.3705430030822754
Validation loss: 3.5172352278104393

Epoch: 6| Step: 13
Training loss: 3.91943359375
Validation loss: 3.5136763793165966

Epoch: 12| Step: 0
Training loss: 3.6878976821899414
Validation loss: 3.5083308117364043

Epoch: 6| Step: 1
Training loss: 3.191457986831665
Validation loss: 3.5029395036799933

Epoch: 6| Step: 2
Training loss: 3.00464129447937
Validation loss: 3.5000454046392955

Epoch: 6| Step: 3
Training loss: 3.1081809997558594
Validation loss: 3.4964575946971936

Epoch: 6| Step: 4
Training loss: 3.203381061553955
Validation loss: 3.491231446625084

Epoch: 6| Step: 5
Training loss: 3.8960318565368652
Validation loss: 3.482338910461754

Epoch: 6| Step: 6
Training loss: 3.971315383911133
Validation loss: 3.480255814008815

Epoch: 6| Step: 7
Training loss: 4.536785125732422
Validation loss: 3.4772772378818964

Epoch: 6| Step: 8
Training loss: 2.5044684410095215
Validation loss: 3.4717430248055408

Epoch: 6| Step: 9
Training loss: 2.940023422241211
Validation loss: 3.466860117450837

Epoch: 6| Step: 10
Training loss: 3.1249401569366455
Validation loss: 3.460590160021218

Epoch: 6| Step: 11
Training loss: 3.537548542022705
Validation loss: 3.4542052822728313

Epoch: 6| Step: 12
Training loss: 4.079545974731445
Validation loss: 3.4498049751404793

Epoch: 6| Step: 13
Training loss: 2.6627163887023926
Validation loss: 3.444774330303233

Epoch: 13| Step: 0
Training loss: 3.594654083251953
Validation loss: 3.4404091809385564

Epoch: 6| Step: 1
Training loss: 2.1457598209381104
Validation loss: 3.4347949797107327

Epoch: 6| Step: 2
Training loss: 3.9998655319213867
Validation loss: 3.433008599024947

Epoch: 6| Step: 3
Training loss: 4.059348106384277
Validation loss: 3.425907927174722

Epoch: 6| Step: 4
Training loss: 2.834038019180298
Validation loss: 3.4209128451603714

Epoch: 6| Step: 5
Training loss: 3.122872829437256
Validation loss: 3.4169107662734164

Epoch: 6| Step: 6
Training loss: 3.2116916179656982
Validation loss: 3.4123848125498784

Epoch: 6| Step: 7
Training loss: 2.834425449371338
Validation loss: 3.407436304194953

Epoch: 6| Step: 8
Training loss: 3.309293270111084
Validation loss: 3.4026960403688493

Epoch: 6| Step: 9
Training loss: 2.7632880210876465
Validation loss: 3.3976424945298063

Epoch: 6| Step: 10
Training loss: 4.790805816650391
Validation loss: 3.3942866863742953

Epoch: 6| Step: 11
Training loss: 3.995814561843872
Validation loss: 3.389404289184078

Epoch: 6| Step: 12
Training loss: 3.2515270709991455
Validation loss: 3.3834958384113927

Epoch: 6| Step: 13
Training loss: 2.8620245456695557
Validation loss: 3.3787049298645346

Epoch: 14| Step: 0
Training loss: 3.730868101119995
Validation loss: 3.3736454081791702

Epoch: 6| Step: 1
Training loss: 3.503355026245117
Validation loss: 3.370862009704754

Epoch: 6| Step: 2
Training loss: 3.4957528114318848
Validation loss: 3.3650743064060005

Epoch: 6| Step: 3
Training loss: 3.375389575958252
Validation loss: 3.3597108651232976

Epoch: 6| Step: 4
Training loss: 4.218131065368652
Validation loss: 3.3541437810467136

Epoch: 6| Step: 5
Training loss: 2.6258974075317383
Validation loss: 3.353725569222563

Epoch: 6| Step: 6
Training loss: 2.5983262062072754
Validation loss: 3.3505160706017607

Epoch: 6| Step: 7
Training loss: 2.477445125579834
Validation loss: 3.344520035610404

Epoch: 6| Step: 8
Training loss: 2.468902826309204
Validation loss: 3.3400797356841383

Epoch: 6| Step: 9
Training loss: 3.9590115547180176
Validation loss: 3.3340349043569257

Epoch: 6| Step: 10
Training loss: 3.771418333053589
Validation loss: 3.331432029765139

Epoch: 6| Step: 11
Training loss: 2.4575769901275635
Validation loss: 3.324472801659697

Epoch: 6| Step: 12
Training loss: 3.8502957820892334
Validation loss: 3.325546092884515

Epoch: 6| Step: 13
Training loss: 4.05090856552124
Validation loss: 3.320013210337649

Epoch: 15| Step: 0
Training loss: 3.2888565063476562
Validation loss: 3.3151054279778593

Epoch: 6| Step: 1
Training loss: 3.837800979614258
Validation loss: 3.313696474157354

Epoch: 6| Step: 2
Training loss: 4.309329032897949
Validation loss: 3.3105809278385614

Epoch: 6| Step: 3
Training loss: 2.7698984146118164
Validation loss: 3.310431106116182

Epoch: 6| Step: 4
Training loss: 2.664778232574463
Validation loss: 3.3055099851341656

Epoch: 6| Step: 5
Training loss: 2.7487668991088867
Validation loss: 3.299316408813641

Epoch: 6| Step: 6
Training loss: 2.2011048793792725
Validation loss: 3.2968550292394494

Epoch: 6| Step: 7
Training loss: 3.890180826187134
Validation loss: 3.2928822373831146

Epoch: 6| Step: 8
Training loss: 3.9509329795837402
Validation loss: 3.2901155666638444

Epoch: 6| Step: 9
Training loss: 3.4781246185302734
Validation loss: 3.288475121221235

Epoch: 6| Step: 10
Training loss: 3.4117276668548584
Validation loss: 3.28324398943173

Epoch: 6| Step: 11
Training loss: 2.69996976852417
Validation loss: 3.2763457682824906

Epoch: 6| Step: 12
Training loss: 3.654158115386963
Validation loss: 3.274847217785415

Epoch: 6| Step: 13
Training loss: 2.3935816287994385
Validation loss: 3.273888408496816

Epoch: 16| Step: 0
Training loss: 2.9600882530212402
Validation loss: 3.2741385147135746

Epoch: 6| Step: 1
Training loss: 3.613788366317749
Validation loss: 3.2711715211150465

Epoch: 6| Step: 2
Training loss: 2.6187195777893066
Validation loss: 3.2615267845892135

Epoch: 6| Step: 3
Training loss: 3.2021260261535645
Validation loss: 3.2617359648468676

Epoch: 6| Step: 4
Training loss: 2.7192630767822266
Validation loss: 3.254761057515298

Epoch: 6| Step: 5
Training loss: 3.3946051597595215
Validation loss: 3.2517342490534626

Epoch: 6| Step: 6
Training loss: 2.856675148010254
Validation loss: 3.251019949554115

Epoch: 6| Step: 7
Training loss: 4.0210418701171875
Validation loss: 3.246840043734479

Epoch: 6| Step: 8
Training loss: 3.5116498470306396
Validation loss: 3.2440700069550545

Epoch: 6| Step: 9
Training loss: 3.4067575931549072
Validation loss: 3.2386542853488716

Epoch: 6| Step: 10
Training loss: 2.844665288925171
Validation loss: 3.2374082226907053

Epoch: 6| Step: 11
Training loss: 3.3978190422058105
Validation loss: 3.234790858402047

Epoch: 6| Step: 12
Training loss: 3.5226364135742188
Validation loss: 3.231845745476343

Epoch: 6| Step: 13
Training loss: 3.0406577587127686
Validation loss: 3.2298649152119956

Epoch: 17| Step: 0
Training loss: 2.2719764709472656
Validation loss: 3.22809257558597

Epoch: 6| Step: 1
Training loss: 3.543520450592041
Validation loss: 3.2231819886033253

Epoch: 6| Step: 2
Training loss: 3.713827610015869
Validation loss: 3.2225171622409614

Epoch: 6| Step: 3
Training loss: 3.140883445739746
Validation loss: 3.2162899253188924

Epoch: 6| Step: 4
Training loss: 4.0161614418029785
Validation loss: 3.212587615495087

Epoch: 6| Step: 5
Training loss: 2.7185440063476562
Validation loss: 3.2115503434211976

Epoch: 6| Step: 6
Training loss: 3.374377727508545
Validation loss: 3.2064463605162916

Epoch: 6| Step: 7
Training loss: 3.026587963104248
Validation loss: 3.2094376420462005

Epoch: 6| Step: 8
Training loss: 2.2177748680114746
Validation loss: 3.202500243340769

Epoch: 6| Step: 9
Training loss: 2.5357794761657715
Validation loss: 3.2050199483030584

Epoch: 6| Step: 10
Training loss: 3.021982192993164
Validation loss: 3.200555791137039

Epoch: 6| Step: 11
Training loss: 3.9527595043182373
Validation loss: 3.2018341992491033

Epoch: 6| Step: 12
Training loss: 4.0521955490112305
Validation loss: 3.1941116343262377

Epoch: 6| Step: 13
Training loss: 3.1756608486175537
Validation loss: 3.188990162264916

Epoch: 18| Step: 0
Training loss: 2.69828462600708
Validation loss: 3.193527331916235

Epoch: 6| Step: 1
Training loss: 3.2588553428649902
Validation loss: 3.2004719703428206

Epoch: 6| Step: 2
Training loss: 3.7642078399658203
Validation loss: 3.192100530029625

Epoch: 6| Step: 3
Training loss: 3.1894822120666504
Validation loss: 3.1810607243609685

Epoch: 6| Step: 4
Training loss: 3.8919246196746826
Validation loss: 3.1788829629139235

Epoch: 6| Step: 5
Training loss: 3.6060357093811035
Validation loss: 3.1745092202258367

Epoch: 6| Step: 6
Training loss: 3.9473724365234375
Validation loss: 3.175848430202853

Epoch: 6| Step: 7
Training loss: 2.697655200958252
Validation loss: 3.1732880453909598

Epoch: 6| Step: 8
Training loss: 2.0676016807556152
Validation loss: 3.1671930897620415

Epoch: 6| Step: 9
Training loss: 2.826460361480713
Validation loss: 3.1606822193309827

Epoch: 6| Step: 10
Training loss: 2.5488054752349854
Validation loss: 3.1616669188263598

Epoch: 6| Step: 11
Training loss: 2.9253315925598145
Validation loss: 3.1576121186697357

Epoch: 6| Step: 12
Training loss: 3.226588249206543
Validation loss: 3.1573994518608175

Epoch: 6| Step: 13
Training loss: 4.205070972442627
Validation loss: 3.1553989148909047

Epoch: 19| Step: 0
Training loss: 2.9498343467712402
Validation loss: 3.151532724339475

Epoch: 6| Step: 1
Training loss: 2.862791061401367
Validation loss: 3.149291776841687

Epoch: 6| Step: 2
Training loss: 2.8761444091796875
Validation loss: 3.1428735768923195

Epoch: 6| Step: 3
Training loss: 3.1069881916046143
Validation loss: 3.1425063328076432

Epoch: 6| Step: 4
Training loss: 2.9565417766571045
Validation loss: 3.1391042329931773

Epoch: 6| Step: 5
Training loss: 3.1650586128234863
Validation loss: 3.135225942057948

Epoch: 6| Step: 6
Training loss: 3.6358792781829834
Validation loss: 3.135203835784748

Epoch: 6| Step: 7
Training loss: 4.0844011306762695
Validation loss: 3.1287017791501937

Epoch: 6| Step: 8
Training loss: 2.399331569671631
Validation loss: 3.1270784203724196

Epoch: 6| Step: 9
Training loss: 3.791077136993408
Validation loss: 3.126798560542445

Epoch: 6| Step: 10
Training loss: 3.164699077606201
Validation loss: 3.129192767604705

Epoch: 6| Step: 11
Training loss: 2.728060483932495
Validation loss: 3.125589545055102

Epoch: 6| Step: 12
Training loss: 2.905834674835205
Validation loss: 3.125560465679374

Epoch: 6| Step: 13
Training loss: 3.607405424118042
Validation loss: 3.1180551077729914

Epoch: 20| Step: 0
Training loss: 3.0985212326049805
Validation loss: 3.1144723123119724

Epoch: 6| Step: 1
Training loss: 3.356123924255371
Validation loss: 3.108158201299688

Epoch: 6| Step: 2
Training loss: 2.8752102851867676
Validation loss: 3.10560594835589

Epoch: 6| Step: 3
Training loss: 2.4297618865966797
Validation loss: 3.0985073786909862

Epoch: 6| Step: 4
Training loss: 2.98075008392334
Validation loss: 3.098206966154037

Epoch: 6| Step: 5
Training loss: 4.13308048248291
Validation loss: 3.0971731062858336

Epoch: 6| Step: 6
Training loss: 2.6551480293273926
Validation loss: 3.0959298636323664

Epoch: 6| Step: 7
Training loss: 3.517949104309082
Validation loss: 3.093588480385401

Epoch: 6| Step: 8
Training loss: 2.0614326000213623
Validation loss: 3.0910004928547847

Epoch: 6| Step: 9
Training loss: 3.7309792041778564
Validation loss: 3.087473233540853

Epoch: 6| Step: 10
Training loss: 3.2650070190429688
Validation loss: 3.088563273029943

Epoch: 6| Step: 11
Training loss: 2.724781036376953
Validation loss: 3.0848012637066584

Epoch: 6| Step: 12
Training loss: 3.2819807529449463
Validation loss: 3.081383764102895

Epoch: 6| Step: 13
Training loss: 3.866236686706543
Validation loss: 3.075390354279549

Epoch: 21| Step: 0
Training loss: 3.686079978942871
Validation loss: 3.0748245613549345

Epoch: 6| Step: 1
Training loss: 4.575137138366699
Validation loss: 3.0743305990772862

Epoch: 6| Step: 2
Training loss: 2.4126157760620117
Validation loss: 3.0697572641475226

Epoch: 6| Step: 3
Training loss: 3.8920469284057617
Validation loss: 3.0683578470701813

Epoch: 6| Step: 4
Training loss: 3.747818946838379
Validation loss: 3.069522965338922

Epoch: 6| Step: 5
Training loss: 2.0818419456481934
Validation loss: 3.069049294276904

Epoch: 6| Step: 6
Training loss: 1.9659676551818848
Validation loss: 3.065305607293242

Epoch: 6| Step: 7
Training loss: 2.9559593200683594
Validation loss: 3.0615318180412374

Epoch: 6| Step: 8
Training loss: 3.518179416656494
Validation loss: 3.0614902998811457

Epoch: 6| Step: 9
Training loss: 3.253904342651367
Validation loss: 3.0535109632758686

Epoch: 6| Step: 10
Training loss: 3.023329257965088
Validation loss: 3.048932585664975

Epoch: 6| Step: 11
Training loss: 3.095592498779297
Validation loss: 3.052112333236202

Epoch: 6| Step: 12
Training loss: 2.7294106483459473
Validation loss: 3.0479025584395214

Epoch: 6| Step: 13
Training loss: 1.9204703569412231
Validation loss: 3.0507231425213557

Epoch: 22| Step: 0
Training loss: 3.0486245155334473
Validation loss: 3.0489599781651653

Epoch: 6| Step: 1
Training loss: 2.7946853637695312
Validation loss: 3.0494492976896224

Epoch: 6| Step: 2
Training loss: 3.5343098640441895
Validation loss: 3.0475376344496206

Epoch: 6| Step: 3
Training loss: 4.4804582595825195
Validation loss: 3.048403355383104

Epoch: 6| Step: 4
Training loss: 3.2632992267608643
Validation loss: 3.0414541485489055

Epoch: 6| Step: 5
Training loss: 2.555311918258667
Validation loss: 3.0359321409656155

Epoch: 6| Step: 6
Training loss: 3.1783175468444824
Validation loss: 3.030060309235768

Epoch: 6| Step: 7
Training loss: 2.5704855918884277
Validation loss: 3.029241946435744

Epoch: 6| Step: 8
Training loss: 2.6572518348693848
Validation loss: 3.043153134725427

Epoch: 6| Step: 9
Training loss: 3.0513253211975098
Validation loss: 3.064481435283538

Epoch: 6| Step: 10
Training loss: 2.5875864028930664
Validation loss: 3.030487181037985

Epoch: 6| Step: 11
Training loss: 3.04577898979187
Validation loss: 3.0212612869918987

Epoch: 6| Step: 12
Training loss: 4.203284740447998
Validation loss: 3.029980738957723

Epoch: 6| Step: 13
Training loss: 1.5022759437561035
Validation loss: 3.038065512975057

Epoch: 23| Step: 0
Training loss: 3.8122687339782715
Validation loss: 3.0666598350771013

Epoch: 6| Step: 1
Training loss: 3.3598077297210693
Validation loss: 3.0679897621113765

Epoch: 6| Step: 2
Training loss: 2.7703046798706055
Validation loss: 3.0488415072041173

Epoch: 6| Step: 3
Training loss: 2.2666397094726562
Validation loss: 3.0299061934153237

Epoch: 6| Step: 4
Training loss: 2.6763625144958496
Validation loss: 3.0167526609154156

Epoch: 6| Step: 5
Training loss: 2.568925380706787
Validation loss: 3.0128676865690496

Epoch: 6| Step: 6
Training loss: 2.954664707183838
Validation loss: 3.0154646032599994

Epoch: 6| Step: 7
Training loss: 3.2628021240234375
Validation loss: 3.0196808512492845

Epoch: 6| Step: 8
Training loss: 2.6914544105529785
Validation loss: 3.0284007646704234

Epoch: 6| Step: 9
Training loss: 2.6289865970611572
Validation loss: 3.028728787617017

Epoch: 6| Step: 10
Training loss: 2.7891597747802734
Validation loss: 3.027060203654792

Epoch: 6| Step: 11
Training loss: 3.899099588394165
Validation loss: 3.0235229717787875

Epoch: 6| Step: 12
Training loss: 3.7514452934265137
Validation loss: 3.0201486823379353

Epoch: 6| Step: 13
Training loss: 4.13743782043457
Validation loss: 3.0073441818196285

Epoch: 24| Step: 0
Training loss: 1.6361982822418213
Validation loss: 2.9999044556771555

Epoch: 6| Step: 1
Training loss: 2.983065128326416
Validation loss: 2.9962860461204284

Epoch: 6| Step: 2
Training loss: 2.9175329208374023
Validation loss: 2.99418302248883

Epoch: 6| Step: 3
Training loss: 3.4942784309387207
Validation loss: 2.9956207352299846

Epoch: 6| Step: 4
Training loss: 2.966582775115967
Validation loss: 2.9965474015922955

Epoch: 6| Step: 5
Training loss: 3.3213162422180176
Validation loss: 2.998328642178607

Epoch: 6| Step: 6
Training loss: 3.494429588317871
Validation loss: 2.990539079071373

Epoch: 6| Step: 7
Training loss: 3.052225112915039
Validation loss: 2.992292042701475

Epoch: 6| Step: 8
Training loss: 2.8355672359466553
Validation loss: 2.986687119289111

Epoch: 6| Step: 9
Training loss: 2.206305980682373
Validation loss: 2.978582715475431

Epoch: 6| Step: 10
Training loss: 3.336007595062256
Validation loss: 2.976817620697842

Epoch: 6| Step: 11
Training loss: 3.500849962234497
Validation loss: 2.9774955498274935

Epoch: 6| Step: 12
Training loss: 4.102262496948242
Validation loss: 2.9828432606112574

Epoch: 6| Step: 13
Training loss: 2.7770895957946777
Validation loss: 2.976345016110328

Epoch: 25| Step: 0
Training loss: 2.831632137298584
Validation loss: 2.976967898748254

Epoch: 6| Step: 1
Training loss: 3.548675298690796
Validation loss: 2.973612626393636

Epoch: 6| Step: 2
Training loss: 2.8491854667663574
Validation loss: 2.9677482548580376

Epoch: 6| Step: 3
Training loss: 3.321901321411133
Validation loss: 2.9697685267335627

Epoch: 6| Step: 4
Training loss: 2.9708566665649414
Validation loss: 2.966566716471026

Epoch: 6| Step: 5
Training loss: 4.037436485290527
Validation loss: 2.959286159084689

Epoch: 6| Step: 6
Training loss: 2.6861226558685303
Validation loss: 2.9612644180174796

Epoch: 6| Step: 7
Training loss: 3.3837099075317383
Validation loss: 2.965661177071192

Epoch: 6| Step: 8
Training loss: 2.5467214584350586
Validation loss: 2.9659584286392375

Epoch: 6| Step: 9
Training loss: 3.879976272583008
Validation loss: 2.9787751987416256

Epoch: 6| Step: 10
Training loss: 2.704317808151245
Validation loss: 2.9692983268409647

Epoch: 6| Step: 11
Training loss: 2.040592670440674
Validation loss: 2.9740177226322952

Epoch: 6| Step: 12
Training loss: 2.9837489128112793
Validation loss: 2.9540937151960147

Epoch: 6| Step: 13
Training loss: 2.4880764484405518
Validation loss: 2.9481419542784333

Epoch: 26| Step: 0
Training loss: 2.5765676498413086
Validation loss: 2.9447596483333136

Epoch: 6| Step: 1
Training loss: 3.1577486991882324
Validation loss: 2.944772658809539

Epoch: 6| Step: 2
Training loss: 2.8199148178100586
Validation loss: 2.9434211869393625

Epoch: 6| Step: 3
Training loss: 3.230670928955078
Validation loss: 2.945235662562873

Epoch: 6| Step: 4
Training loss: 2.5947980880737305
Validation loss: 2.9441427159053024

Epoch: 6| Step: 5
Training loss: 3.3872714042663574
Validation loss: 2.943746425772226

Epoch: 6| Step: 6
Training loss: 3.920039653778076
Validation loss: 2.939752899190431

Epoch: 6| Step: 7
Training loss: 3.700406312942505
Validation loss: 2.939521230677123

Epoch: 6| Step: 8
Training loss: 3.803802967071533
Validation loss: 2.9396528813146774

Epoch: 6| Step: 9
Training loss: 2.4954874515533447
Validation loss: 2.9333837519409838

Epoch: 6| Step: 10
Training loss: 2.476205825805664
Validation loss: 2.936004554071734

Epoch: 6| Step: 11
Training loss: 2.3077380657196045
Validation loss: 2.9333142849706833

Epoch: 6| Step: 12
Training loss: 2.54439640045166
Validation loss: 2.9371989568074546

Epoch: 6| Step: 13
Training loss: 3.426117181777954
Validation loss: 2.94157455044408

Epoch: 27| Step: 0
Training loss: 2.733518600463867
Validation loss: 2.933494980617236

Epoch: 6| Step: 1
Training loss: 2.487447500228882
Validation loss: 2.929613967095652

Epoch: 6| Step: 2
Training loss: 3.3234572410583496
Validation loss: 2.9255864235662643

Epoch: 6| Step: 3
Training loss: 2.708228588104248
Validation loss: 2.9193640575614026

Epoch: 6| Step: 4
Training loss: 3.212533950805664
Validation loss: 2.9221888870321293

Epoch: 6| Step: 5
Training loss: 2.9918465614318848
Validation loss: 2.9225814650135655

Epoch: 6| Step: 6
Training loss: 2.8584213256835938
Validation loss: 2.9202426069526264

Epoch: 6| Step: 7
Training loss: 3.0104565620422363
Validation loss: 2.921083334953554

Epoch: 6| Step: 8
Training loss: 3.3007264137268066
Validation loss: 2.9183532012406217

Epoch: 6| Step: 9
Training loss: 2.0923266410827637
Validation loss: 2.922665757517661

Epoch: 6| Step: 10
Training loss: 2.6054604053497314
Validation loss: 2.9136934946942072

Epoch: 6| Step: 11
Training loss: 3.9919466972351074
Validation loss: 2.914675586967058

Epoch: 6| Step: 12
Training loss: 3.3141820430755615
Validation loss: 2.910308967354477

Epoch: 6| Step: 13
Training loss: 3.7437663078308105
Validation loss: 2.907535268414405

Epoch: 28| Step: 0
Training loss: 3.237692356109619
Validation loss: 2.9055267944130847

Epoch: 6| Step: 1
Training loss: 2.337831497192383
Validation loss: 2.900741046474826

Epoch: 6| Step: 2
Training loss: 3.3665387630462646
Validation loss: 2.9023145937150523

Epoch: 6| Step: 3
Training loss: 2.1997294425964355
Validation loss: 2.9022451472538773

Epoch: 6| Step: 4
Training loss: 3.4834604263305664
Validation loss: 2.899053153171334

Epoch: 6| Step: 5
Training loss: 3.803960084915161
Validation loss: 2.90070758070997

Epoch: 6| Step: 6
Training loss: 3.2482824325561523
Validation loss: 2.8983284658001316

Epoch: 6| Step: 7
Training loss: 3.2320303916931152
Validation loss: 2.902076926282657

Epoch: 6| Step: 8
Training loss: 2.0059046745300293
Validation loss: 2.892717966469385

Epoch: 6| Step: 9
Training loss: 2.860574245452881
Validation loss: 2.9012479628286054

Epoch: 6| Step: 10
Training loss: 2.43829083442688
Validation loss: 2.8981864631816907

Epoch: 6| Step: 11
Training loss: 3.988576650619507
Validation loss: 2.9013900782472346

Epoch: 6| Step: 12
Training loss: 2.752990245819092
Validation loss: 2.899154370830905

Epoch: 6| Step: 13
Training loss: 2.7653744220733643
Validation loss: 2.895607086919969

Epoch: 29| Step: 0
Training loss: 2.709836006164551
Validation loss: 2.8913170035167406

Epoch: 6| Step: 1
Training loss: 2.6768836975097656
Validation loss: 2.8889638095773678

Epoch: 6| Step: 2
Training loss: 2.45881986618042
Validation loss: 2.8896672341131393

Epoch: 6| Step: 3
Training loss: 2.7537543773651123
Validation loss: 2.8911757443540838

Epoch: 6| Step: 4
Training loss: 3.3224222660064697
Validation loss: 2.9007654754064416

Epoch: 6| Step: 5
Training loss: 3.519515037536621
Validation loss: 2.8962421212145077

Epoch: 6| Step: 6
Training loss: 2.8472955226898193
Validation loss: 2.8879270656134493

Epoch: 6| Step: 7
Training loss: 3.291457176208496
Validation loss: 2.8830526131455616

Epoch: 6| Step: 8
Training loss: 2.6613564491271973
Validation loss: 2.883143214769261

Epoch: 6| Step: 9
Training loss: 2.890232563018799
Validation loss: 2.8834858620038597

Epoch: 6| Step: 10
Training loss: 3.154439926147461
Validation loss: 2.8801529586956067

Epoch: 6| Step: 11
Training loss: 3.5141069889068604
Validation loss: 2.8807892414831344

Epoch: 6| Step: 12
Training loss: 3.0134096145629883
Validation loss: 2.873799136889878

Epoch: 6| Step: 13
Training loss: 2.8551366329193115
Validation loss: 2.8768856525421143

Epoch: 30| Step: 0
Training loss: 2.6357929706573486
Validation loss: 2.8774473667144775

Epoch: 6| Step: 1
Training loss: 2.5070815086364746
Validation loss: 2.87484299495656

Epoch: 6| Step: 2
Training loss: 3.5631799697875977
Validation loss: 2.8750789601315736

Epoch: 6| Step: 3
Training loss: 2.7496514320373535
Validation loss: 2.8730664304507676

Epoch: 6| Step: 4
Training loss: 3.0500810146331787
Validation loss: 2.876706959098898

Epoch: 6| Step: 5
Training loss: 3.0657644271850586
Validation loss: 2.872773862654163

Epoch: 6| Step: 6
Training loss: 2.5405216217041016
Validation loss: 2.8719249848396546

Epoch: 6| Step: 7
Training loss: 4.064268112182617
Validation loss: 2.8694048081674883

Epoch: 6| Step: 8
Training loss: 3.2019054889678955
Validation loss: 2.865481533030028

Epoch: 6| Step: 9
Training loss: 3.9613335132598877
Validation loss: 2.869503564732049

Epoch: 6| Step: 10
Training loss: 3.1216416358947754
Validation loss: 2.868317157991471

Epoch: 6| Step: 11
Training loss: 1.989889144897461
Validation loss: 2.866656018841651

Epoch: 6| Step: 12
Training loss: 2.635671854019165
Validation loss: 2.872515259250518

Epoch: 6| Step: 13
Training loss: 2.211573839187622
Validation loss: 2.8788444918970906

Epoch: 31| Step: 0
Training loss: 3.7137255668640137
Validation loss: 2.879687434883528

Epoch: 6| Step: 1
Training loss: 2.6632301807403564
Validation loss: 2.8981125559858096

Epoch: 6| Step: 2
Training loss: 3.0027120113372803
Validation loss: 2.9045326863565752

Epoch: 6| Step: 3
Training loss: 3.597215414047241
Validation loss: 2.9070500507149646

Epoch: 6| Step: 4
Training loss: 2.5723445415496826
Validation loss: 2.884751094284878

Epoch: 6| Step: 5
Training loss: 3.285977840423584
Validation loss: 2.866255224391978

Epoch: 6| Step: 6
Training loss: 3.1943788528442383
Validation loss: 2.8636423849290416

Epoch: 6| Step: 7
Training loss: 3.4097297191619873
Validation loss: 2.8607717226910334

Epoch: 6| Step: 8
Training loss: 2.985320568084717
Validation loss: 2.8628374274059007

Epoch: 6| Step: 9
Training loss: 1.8174834251403809
Validation loss: 2.8613582221410607

Epoch: 6| Step: 10
Training loss: 3.1845433712005615
Validation loss: 2.866319594844695

Epoch: 6| Step: 11
Training loss: 1.6202741861343384
Validation loss: 2.8799353696966685

Epoch: 6| Step: 12
Training loss: 3.4642229080200195
Validation loss: 2.878437585728143

Epoch: 6| Step: 13
Training loss: 3.177157163619995
Validation loss: 2.8639925064579135

Epoch: 32| Step: 0
Training loss: 3.380404472351074
Validation loss: 2.8569464324623026

Epoch: 6| Step: 1
Training loss: 2.2052314281463623
Validation loss: 2.8595036845053396

Epoch: 6| Step: 2
Training loss: 2.4434280395507812
Validation loss: 2.8609983639050554

Epoch: 6| Step: 3
Training loss: 3.316049098968506
Validation loss: 2.864913599465483

Epoch: 6| Step: 4
Training loss: 2.5168938636779785
Validation loss: 2.867926328412948

Epoch: 6| Step: 5
Training loss: 3.4737601280212402
Validation loss: 2.8629244040417414

Epoch: 6| Step: 6
Training loss: 3.1729495525360107
Validation loss: 2.861096377013832

Epoch: 6| Step: 7
Training loss: 2.960019111633301
Validation loss: 2.8581803357729347

Epoch: 6| Step: 8
Training loss: 3.8093483448028564
Validation loss: 2.8562210400899253

Epoch: 6| Step: 9
Training loss: 2.9707422256469727
Validation loss: 2.8539720068695726

Epoch: 6| Step: 10
Training loss: 3.7823333740234375
Validation loss: 2.849962547261228

Epoch: 6| Step: 11
Training loss: 2.446974992752075
Validation loss: 2.8521956961642028

Epoch: 6| Step: 12
Training loss: 2.170323371887207
Validation loss: 2.84521698182629

Epoch: 6| Step: 13
Training loss: 2.7448019981384277
Validation loss: 2.8490470711902907

Epoch: 33| Step: 0
Training loss: 3.452090263366699
Validation loss: 2.8485556520441526

Epoch: 6| Step: 1
Training loss: 2.9548110961914062
Validation loss: 2.846352292645362

Epoch: 6| Step: 2
Training loss: 2.113084316253662
Validation loss: 2.8460039784831386

Epoch: 6| Step: 3
Training loss: 3.4847121238708496
Validation loss: 2.8442328181318057

Epoch: 6| Step: 4
Training loss: 2.9703564643859863
Validation loss: 2.843282268893334

Epoch: 6| Step: 5
Training loss: 2.9962284564971924
Validation loss: 2.843652143273302

Epoch: 6| Step: 6
Training loss: 2.8909711837768555
Validation loss: 2.8437598777073685

Epoch: 6| Step: 7
Training loss: 2.9596617221832275
Validation loss: 2.843269460944719

Epoch: 6| Step: 8
Training loss: 2.7106449604034424
Validation loss: 2.841852713656682

Epoch: 6| Step: 9
Training loss: 3.109577178955078
Validation loss: 2.84375193554868

Epoch: 6| Step: 10
Training loss: 2.545689105987549
Validation loss: 2.8411592924466698

Epoch: 6| Step: 11
Training loss: 2.636500358581543
Validation loss: 2.8424569047907347

Epoch: 6| Step: 12
Training loss: 4.050870895385742
Validation loss: 2.8453319944361204

Epoch: 6| Step: 13
Training loss: 2.013913154602051
Validation loss: 2.841833701697729

Epoch: 34| Step: 0
Training loss: 3.1568801403045654
Validation loss: 2.8472406171983287

Epoch: 6| Step: 1
Training loss: 2.039632797241211
Validation loss: 2.8474620260218138

Epoch: 6| Step: 2
Training loss: 2.961751937866211
Validation loss: 2.8480745592424945

Epoch: 6| Step: 3
Training loss: 2.8725221157073975
Validation loss: 2.839754794233589

Epoch: 6| Step: 4
Training loss: 2.5812559127807617
Validation loss: 2.836586131844469

Epoch: 6| Step: 5
Training loss: 2.8073744773864746
Validation loss: 2.8330456467084986

Epoch: 6| Step: 6
Training loss: 2.694877862930298
Validation loss: 2.827953951333159

Epoch: 6| Step: 7
Training loss: 3.140744686126709
Validation loss: 2.8324572963099324

Epoch: 6| Step: 8
Training loss: 3.145005226135254
Validation loss: 2.8367177030091644

Epoch: 6| Step: 9
Training loss: 3.069094657897949
Validation loss: 2.837162994569348

Epoch: 6| Step: 10
Training loss: 2.476686954498291
Validation loss: 2.8340247292672434

Epoch: 6| Step: 11
Training loss: 3.8165159225463867
Validation loss: 2.8303969983131654

Epoch: 6| Step: 12
Training loss: 3.0124387741088867
Validation loss: 2.8277986100924912

Epoch: 6| Step: 13
Training loss: 3.9867210388183594
Validation loss: 2.8283301245781685

Epoch: 35| Step: 0
Training loss: 3.547344923019409
Validation loss: 2.829058593319308

Epoch: 6| Step: 1
Training loss: 3.0589404106140137
Validation loss: 2.8252655844534598

Epoch: 6| Step: 2
Training loss: 3.743177890777588
Validation loss: 2.825959236391129

Epoch: 6| Step: 3
Training loss: 2.2106053829193115
Validation loss: 2.826475771524573

Epoch: 6| Step: 4
Training loss: 1.6534336805343628
Validation loss: 2.831247527112243

Epoch: 6| Step: 5
Training loss: 2.511859655380249
Validation loss: 2.8302853184361614

Epoch: 6| Step: 6
Training loss: 2.662679433822632
Validation loss: 2.8268436821558143

Epoch: 6| Step: 7
Training loss: 3.3753604888916016
Validation loss: 2.8278571815900904

Epoch: 6| Step: 8
Training loss: 3.1442959308624268
Validation loss: 2.8258503303732923

Epoch: 6| Step: 9
Training loss: 2.896780252456665
Validation loss: 2.8263848468821537

Epoch: 6| Step: 10
Training loss: 4.154281139373779
Validation loss: 2.82269819577535

Epoch: 6| Step: 11
Training loss: 2.0850536823272705
Validation loss: 2.823048489068144

Epoch: 6| Step: 12
Training loss: 2.979149341583252
Validation loss: 2.8245514951726443

Epoch: 6| Step: 13
Training loss: 3.2977800369262695
Validation loss: 2.823249719476187

Epoch: 36| Step: 0
Training loss: 2.821713447570801
Validation loss: 2.8202894477434057

Epoch: 6| Step: 1
Training loss: 2.5340123176574707
Validation loss: 2.820713427758986

Epoch: 6| Step: 2
Training loss: 2.821687698364258
Validation loss: 2.8216536250165714

Epoch: 6| Step: 3
Training loss: 3.4760565757751465
Validation loss: 2.8181808584479877

Epoch: 6| Step: 4
Training loss: 2.5138206481933594
Validation loss: 2.8193844159444175

Epoch: 6| Step: 5
Training loss: 3.40114688873291
Validation loss: 2.8175744574557067

Epoch: 6| Step: 6
Training loss: 3.1177048683166504
Validation loss: 2.813664438903973

Epoch: 6| Step: 7
Training loss: 2.5106008052825928
Validation loss: 2.817984493829871

Epoch: 6| Step: 8
Training loss: 3.0211405754089355
Validation loss: 2.8195471430337555

Epoch: 6| Step: 9
Training loss: 2.58451509475708
Validation loss: 2.8187099477296234

Epoch: 6| Step: 10
Training loss: 3.579847574234009
Validation loss: 2.818855436899329

Epoch: 6| Step: 11
Training loss: 2.111598491668701
Validation loss: 2.8208588502740346

Epoch: 6| Step: 12
Training loss: 3.3048319816589355
Validation loss: 2.82586318959472

Epoch: 6| Step: 13
Training loss: 3.59975528717041
Validation loss: 2.8216561091843473

Epoch: 37| Step: 0
Training loss: 2.2769534587860107
Validation loss: 2.821888005861672

Epoch: 6| Step: 1
Training loss: 3.1771903038024902
Validation loss: 2.8184843729901057

Epoch: 6| Step: 2
Training loss: 3.1809539794921875
Validation loss: 2.8202478526740946

Epoch: 6| Step: 3
Training loss: 2.144883394241333
Validation loss: 2.81591365157917

Epoch: 6| Step: 4
Training loss: 3.019961357116699
Validation loss: 2.817079551758305

Epoch: 6| Step: 5
Training loss: 3.1441569328308105
Validation loss: 2.8169830229974564

Epoch: 6| Step: 6
Training loss: 2.132689952850342
Validation loss: 2.8147693475087485

Epoch: 6| Step: 7
Training loss: 2.5797362327575684
Validation loss: 2.814646492722214

Epoch: 6| Step: 8
Training loss: 3.385840892791748
Validation loss: 2.814514867721065

Epoch: 6| Step: 9
Training loss: 4.179745674133301
Validation loss: 2.8123082883896364

Epoch: 6| Step: 10
Training loss: 2.1195602416992188
Validation loss: 2.810424784178375

Epoch: 6| Step: 11
Training loss: 3.3541927337646484
Validation loss: 2.8137110715271323

Epoch: 6| Step: 12
Training loss: 2.6285037994384766
Validation loss: 2.8093953952994397

Epoch: 6| Step: 13
Training loss: 4.377350330352783
Validation loss: 2.812005732649116

Epoch: 38| Step: 0
Training loss: 3.3679592609405518
Validation loss: 2.8095446350753948

Epoch: 6| Step: 1
Training loss: 1.958237648010254
Validation loss: 2.808634570849839

Epoch: 6| Step: 2
Training loss: 2.955866575241089
Validation loss: 2.811269196130896

Epoch: 6| Step: 3
Training loss: 3.103102445602417
Validation loss: 2.8118451795270367

Epoch: 6| Step: 4
Training loss: 3.121721029281616
Validation loss: 2.8150092940176688

Epoch: 6| Step: 5
Training loss: 2.8139376640319824
Validation loss: 2.809130714785668

Epoch: 6| Step: 6
Training loss: 3.536099433898926
Validation loss: 2.812811169573056

Epoch: 6| Step: 7
Training loss: 2.9617879390716553
Validation loss: 2.8085376267792075

Epoch: 6| Step: 8
Training loss: 2.6068997383117676
Validation loss: 2.809431296522899

Epoch: 6| Step: 9
Training loss: 2.7467126846313477
Validation loss: 2.8085671240283596

Epoch: 6| Step: 10
Training loss: 3.352647304534912
Validation loss: 2.809204137453469

Epoch: 6| Step: 11
Training loss: 2.915019989013672
Validation loss: 2.8075758026492212

Epoch: 6| Step: 12
Training loss: 2.817349672317505
Validation loss: 2.8091693027045137

Epoch: 6| Step: 13
Training loss: 2.5672543048858643
Validation loss: 2.8090952391265542

Epoch: 39| Step: 0
Training loss: 2.3446898460388184
Validation loss: 2.8117659502131964

Epoch: 6| Step: 1
Training loss: 3.006667137145996
Validation loss: 2.8099780877431235

Epoch: 6| Step: 2
Training loss: 2.480801582336426
Validation loss: 2.8058153788248696

Epoch: 6| Step: 3
Training loss: 3.421360969543457
Validation loss: 2.803979291710802

Epoch: 6| Step: 4
Training loss: 3.6156511306762695
Validation loss: 2.803866422304543

Epoch: 6| Step: 5
Training loss: 3.258847713470459
Validation loss: 2.801478708944013

Epoch: 6| Step: 6
Training loss: 2.9749755859375
Validation loss: 2.7998306238523094

Epoch: 6| Step: 7
Training loss: 3.31139874458313
Validation loss: 2.8029808408470562

Epoch: 6| Step: 8
Training loss: 2.332045555114746
Validation loss: 2.8014956904995825

Epoch: 6| Step: 9
Training loss: 2.142914295196533
Validation loss: 2.7994376561974965

Epoch: 6| Step: 10
Training loss: 2.1537790298461914
Validation loss: 2.7997227227816017

Epoch: 6| Step: 11
Training loss: 2.6903347969055176
Validation loss: 2.8045602075515257

Epoch: 6| Step: 12
Training loss: 4.284555435180664
Validation loss: 2.8013774759025982

Epoch: 6| Step: 13
Training loss: 2.8503782749176025
Validation loss: 2.7983455452867734

Epoch: 40| Step: 0
Training loss: 1.8066933155059814
Validation loss: 2.798382866767145

Epoch: 6| Step: 1
Training loss: 3.0351858139038086
Validation loss: 2.796097122212892

Epoch: 6| Step: 2
Training loss: 2.0077967643737793
Validation loss: 2.7981087469285533

Epoch: 6| Step: 3
Training loss: 2.8686084747314453
Validation loss: 2.7977863434822328

Epoch: 6| Step: 4
Training loss: 2.3905699253082275
Validation loss: 2.798274581150342

Epoch: 6| Step: 5
Training loss: 3.1082825660705566
Validation loss: 2.8010065299208446

Epoch: 6| Step: 6
Training loss: 2.217334270477295
Validation loss: 2.795983176077566

Epoch: 6| Step: 7
Training loss: 3.484030246734619
Validation loss: 2.7997030801670526

Epoch: 6| Step: 8
Training loss: 3.300790786743164
Validation loss: 2.7994563348831667

Epoch: 6| Step: 9
Training loss: 3.624441146850586
Validation loss: 2.7943059705918833

Epoch: 6| Step: 10
Training loss: 3.273455858230591
Validation loss: 2.799670519367341

Epoch: 6| Step: 11
Training loss: 3.4928224086761475
Validation loss: 2.8001611463485228

Epoch: 6| Step: 12
Training loss: 2.9994726181030273
Validation loss: 2.7986005224207395

Epoch: 6| Step: 13
Training loss: 3.4827277660369873
Validation loss: 2.8066059030512327

Epoch: 41| Step: 0
Training loss: 2.1911230087280273
Validation loss: 2.8012973800782235

Epoch: 6| Step: 1
Training loss: 2.382018804550171
Validation loss: 2.7995298575329524

Epoch: 6| Step: 2
Training loss: 3.3422346115112305
Validation loss: 2.7993917157573085

Epoch: 6| Step: 3
Training loss: 3.9886112213134766
Validation loss: 2.793386003022553

Epoch: 6| Step: 4
Training loss: 2.4006519317626953
Validation loss: 2.7943322761084444

Epoch: 6| Step: 5
Training loss: 2.406322956085205
Validation loss: 2.794972332574988

Epoch: 6| Step: 6
Training loss: 2.856700897216797
Validation loss: 2.795339230568178

Epoch: 6| Step: 7
Training loss: 3.0165305137634277
Validation loss: 2.803160518728277

Epoch: 6| Step: 8
Training loss: 3.715127468109131
Validation loss: 2.803371144879249

Epoch: 6| Step: 9
Training loss: 2.8232929706573486
Validation loss: 2.7962338950044368

Epoch: 6| Step: 10
Training loss: 2.4655861854553223
Validation loss: 2.793461263820689

Epoch: 6| Step: 11
Training loss: 3.302800178527832
Validation loss: 2.79238215825891

Epoch: 6| Step: 12
Training loss: 2.587360382080078
Validation loss: 2.787151182851484

Epoch: 6| Step: 13
Training loss: 3.894292116165161
Validation loss: 2.789114647014167

Epoch: 42| Step: 0
Training loss: 2.4472827911376953
Validation loss: 2.789008596891998

Epoch: 6| Step: 1
Training loss: 3.0971291065216064
Validation loss: 2.7910593837820072

Epoch: 6| Step: 2
Training loss: 2.7468326091766357
Validation loss: 2.7899178868980816

Epoch: 6| Step: 3
Training loss: 3.107419729232788
Validation loss: 2.792872328912058

Epoch: 6| Step: 4
Training loss: 3.31807279586792
Validation loss: 2.793333209970946

Epoch: 6| Step: 5
Training loss: 3.4123544692993164
Validation loss: 2.790719650124991

Epoch: 6| Step: 6
Training loss: 2.304941177368164
Validation loss: 2.7957578243747836

Epoch: 6| Step: 7
Training loss: 3.066629409790039
Validation loss: 2.7953793053985923

Epoch: 6| Step: 8
Training loss: 2.895225763320923
Validation loss: 2.7958119966650523

Epoch: 6| Step: 9
Training loss: 2.42417049407959
Validation loss: 2.7987983303685344

Epoch: 6| Step: 10
Training loss: 2.4177041053771973
Validation loss: 2.7953596320203555

Epoch: 6| Step: 11
Training loss: 3.247251033782959
Validation loss: 2.7905012689610964

Epoch: 6| Step: 12
Training loss: 3.438171148300171
Validation loss: 2.7838214853758454

Epoch: 6| Step: 13
Training loss: 2.924635171890259
Validation loss: 2.781099339967133

Epoch: 43| Step: 0
Training loss: 2.332822799682617
Validation loss: 2.7819793583244405

Epoch: 6| Step: 1
Training loss: 2.182072639465332
Validation loss: 2.7815309750136508

Epoch: 6| Step: 2
Training loss: 3.8816120624542236
Validation loss: 2.780399812165127

Epoch: 6| Step: 3
Training loss: 3.32804274559021
Validation loss: 2.783380123876756

Epoch: 6| Step: 4
Training loss: 2.9174492359161377
Validation loss: 2.7801164452747633

Epoch: 6| Step: 5
Training loss: 3.2476272583007812
Validation loss: 2.783623082663423

Epoch: 6| Step: 6
Training loss: 3.0765540599823
Validation loss: 2.775421227178266

Epoch: 6| Step: 7
Training loss: 2.1285324096679688
Validation loss: 2.7791935577187488

Epoch: 6| Step: 8
Training loss: 2.00297474861145
Validation loss: 2.776536795400804

Epoch: 6| Step: 9
Training loss: 4.046266555786133
Validation loss: 2.7779367816063667

Epoch: 6| Step: 10
Training loss: 3.0556349754333496
Validation loss: 2.777265315414757

Epoch: 6| Step: 11
Training loss: 2.8713040351867676
Validation loss: 2.7785707853173696

Epoch: 6| Step: 12
Training loss: 2.7888543605804443
Validation loss: 2.779228533467939

Epoch: 6| Step: 13
Training loss: 2.8626410961151123
Validation loss: 2.7789147797451226

Epoch: 44| Step: 0
Training loss: 3.601074695587158
Validation loss: 2.7777996755415395

Epoch: 6| Step: 1
Training loss: 2.770174026489258
Validation loss: 2.7752745741157123

Epoch: 6| Step: 2
Training loss: 3.2001402378082275
Validation loss: 2.775990596381567

Epoch: 6| Step: 3
Training loss: 2.817272663116455
Validation loss: 2.775601020423315

Epoch: 6| Step: 4
Training loss: 2.438145399093628
Validation loss: 2.7760211601052234

Epoch: 6| Step: 5
Training loss: 1.7435567378997803
Validation loss: 2.7752823419468378

Epoch: 6| Step: 6
Training loss: 3.541144847869873
Validation loss: 2.7732393305788756

Epoch: 6| Step: 7
Training loss: 2.3215761184692383
Validation loss: 2.775120117331064

Epoch: 6| Step: 8
Training loss: 2.866349697113037
Validation loss: 2.7706674504023727

Epoch: 6| Step: 9
Training loss: 2.903107166290283
Validation loss: 2.7712958217948995

Epoch: 6| Step: 10
Training loss: 3.246758460998535
Validation loss: 2.7737546223466114

Epoch: 6| Step: 11
Training loss: 3.4838457107543945
Validation loss: 2.7724255695137927

Epoch: 6| Step: 12
Training loss: 3.047358274459839
Validation loss: 2.772906567460747

Epoch: 6| Step: 13
Training loss: 2.5469815731048584
Validation loss: 2.770201575371527

Epoch: 45| Step: 0
Training loss: 3.0408735275268555
Validation loss: 2.772286356136363

Epoch: 6| Step: 1
Training loss: 2.613325595855713
Validation loss: 2.7693644672311764

Epoch: 6| Step: 2
Training loss: 2.9200124740600586
Validation loss: 2.7713736052154214

Epoch: 6| Step: 3
Training loss: 3.7664597034454346
Validation loss: 2.767940000821185

Epoch: 6| Step: 4
Training loss: 1.9456647634506226
Validation loss: 2.7692757473197034

Epoch: 6| Step: 5
Training loss: 2.6122426986694336
Validation loss: 2.7706832757560154

Epoch: 6| Step: 6
Training loss: 3.433128595352173
Validation loss: 2.769947664712065

Epoch: 6| Step: 7
Training loss: 2.780884265899658
Validation loss: 2.7674875336308635

Epoch: 6| Step: 8
Training loss: 2.6392807960510254
Validation loss: 2.766494225430232

Epoch: 6| Step: 9
Training loss: 2.949315071105957
Validation loss: 2.7667462620683896

Epoch: 6| Step: 10
Training loss: 2.6757497787475586
Validation loss: 2.765073930063555

Epoch: 6| Step: 11
Training loss: 2.8373711109161377
Validation loss: 2.7687827797346216

Epoch: 6| Step: 12
Training loss: 2.4100570678710938
Validation loss: 2.7685110184454147

Epoch: 6| Step: 13
Training loss: 4.8098039627075195
Validation loss: 2.768238139408891

Epoch: 46| Step: 0
Training loss: 2.9342641830444336
Validation loss: 2.767578096799953

Epoch: 6| Step: 1
Training loss: 3.5917327404022217
Validation loss: 2.7631816530740387

Epoch: 6| Step: 2
Training loss: 3.185298442840576
Validation loss: 2.7673749769887617

Epoch: 6| Step: 3
Training loss: 2.898559093475342
Validation loss: 2.7663308933217037

Epoch: 6| Step: 4
Training loss: 2.93796443939209
Validation loss: 2.767052940143052

Epoch: 6| Step: 5
Training loss: 2.4208691120147705
Validation loss: 2.7690149609760573

Epoch: 6| Step: 6
Training loss: 2.3032407760620117
Validation loss: 2.7665541197663996

Epoch: 6| Step: 7
Training loss: 3.3573970794677734
Validation loss: 2.7691957591682352

Epoch: 6| Step: 8
Training loss: 2.7892208099365234
Validation loss: 2.7708975807312997

Epoch: 6| Step: 9
Training loss: 2.7996795177459717
Validation loss: 2.7644422105563584

Epoch: 6| Step: 10
Training loss: 3.0857632160186768
Validation loss: 2.7648181351282264

Epoch: 6| Step: 11
Training loss: 2.9014644622802734
Validation loss: 2.7644942088793685

Epoch: 6| Step: 12
Training loss: 2.4493837356567383
Validation loss: 2.7603024590399956

Epoch: 6| Step: 13
Training loss: 2.9857490062713623
Validation loss: 2.7637478331083893

Epoch: 47| Step: 0
Training loss: 3.4613444805145264
Validation loss: 2.760600577118576

Epoch: 6| Step: 1
Training loss: 3.123523235321045
Validation loss: 2.7641190508360505

Epoch: 6| Step: 2
Training loss: 2.5665388107299805
Validation loss: 2.7637804810718825

Epoch: 6| Step: 3
Training loss: 2.999222993850708
Validation loss: 2.761021752511301

Epoch: 6| Step: 4
Training loss: 3.2167251110076904
Validation loss: 2.767016692828107

Epoch: 6| Step: 5
Training loss: 3.023440361022949
Validation loss: 2.765164788051318

Epoch: 6| Step: 6
Training loss: 3.240957736968994
Validation loss: 2.7632475719656995

Epoch: 6| Step: 7
Training loss: 3.0965890884399414
Validation loss: 2.7626302934462026

Epoch: 6| Step: 8
Training loss: 2.3694212436676025
Validation loss: 2.7626065336247927

Epoch: 6| Step: 9
Training loss: 2.618734359741211
Validation loss: 2.7614186681726927

Epoch: 6| Step: 10
Training loss: 2.711794137954712
Validation loss: 2.7625031137979157

Epoch: 6| Step: 11
Training loss: 2.593971014022827
Validation loss: 2.7606697928520942

Epoch: 6| Step: 12
Training loss: 2.822455883026123
Validation loss: 2.7551811228516283

Epoch: 6| Step: 13
Training loss: 2.476195812225342
Validation loss: 2.755774636422434

Epoch: 48| Step: 0
Training loss: 2.614522933959961
Validation loss: 2.772173289329775

Epoch: 6| Step: 1
Training loss: 3.202332019805908
Validation loss: 2.780348700861777

Epoch: 6| Step: 2
Training loss: 2.2570114135742188
Validation loss: 2.7878523667653403

Epoch: 6| Step: 3
Training loss: 3.8988566398620605
Validation loss: 2.794875567959201

Epoch: 6| Step: 4
Training loss: 2.877882242202759
Validation loss: 2.762784855340117

Epoch: 6| Step: 5
Training loss: 2.1214237213134766
Validation loss: 2.7689683257892566

Epoch: 6| Step: 6
Training loss: 2.3394477367401123
Validation loss: 2.7734199108616

Epoch: 6| Step: 7
Training loss: 3.259016990661621
Validation loss: 2.779851923706711

Epoch: 6| Step: 8
Training loss: 3.332252264022827
Validation loss: 2.781013863061064

Epoch: 6| Step: 9
Training loss: 3.047720432281494
Validation loss: 2.7714516988364597

Epoch: 6| Step: 10
Training loss: 3.55126953125
Validation loss: 2.7643679598326325

Epoch: 6| Step: 11
Training loss: 2.9825632572174072
Validation loss: 2.7600719005830827

Epoch: 6| Step: 12
Training loss: 2.3406944274902344
Validation loss: 2.754346501442694

Epoch: 6| Step: 13
Training loss: 2.7562036514282227
Validation loss: 2.753770484719225

Epoch: 49| Step: 0
Training loss: 2.032158374786377
Validation loss: 2.755023605080061

Epoch: 6| Step: 1
Training loss: 2.856391429901123
Validation loss: 2.7520261477398615

Epoch: 6| Step: 2
Training loss: 2.463135004043579
Validation loss: 2.7547676178716842

Epoch: 6| Step: 3
Training loss: 3.172966480255127
Validation loss: 2.757765959667903

Epoch: 6| Step: 4
Training loss: 2.1317660808563232
Validation loss: 2.7596120859986994

Epoch: 6| Step: 5
Training loss: 3.5194530487060547
Validation loss: 2.755922584123509

Epoch: 6| Step: 6
Training loss: 3.1696295738220215
Validation loss: 2.754837615515596

Epoch: 6| Step: 7
Training loss: 3.0060579776763916
Validation loss: 2.7516834351324264

Epoch: 6| Step: 8
Training loss: 4.29155158996582
Validation loss: 2.750261752836166

Epoch: 6| Step: 9
Training loss: 2.434727191925049
Validation loss: 2.7530315819606987

Epoch: 6| Step: 10
Training loss: 2.9177067279815674
Validation loss: 2.749390620057301

Epoch: 6| Step: 11
Training loss: 2.9694738388061523
Validation loss: 2.7517584113664526

Epoch: 6| Step: 12
Training loss: 2.5045266151428223
Validation loss: 2.7491816295090543

Epoch: 6| Step: 13
Training loss: 3.0567080974578857
Validation loss: 2.7474710659314225

Epoch: 50| Step: 0
Training loss: 3.013385772705078
Validation loss: 2.760368375368016

Epoch: 6| Step: 1
Training loss: 2.480670690536499
Validation loss: 2.7779375840258855

Epoch: 6| Step: 2
Training loss: 3.4560306072235107
Validation loss: 2.802202516986478

Epoch: 6| Step: 3
Training loss: 2.91111421585083
Validation loss: 2.779887960803124

Epoch: 6| Step: 4
Training loss: 2.8284475803375244
Validation loss: 2.756831007619058

Epoch: 6| Step: 5
Training loss: 2.1415457725524902
Validation loss: 2.748218495358703

Epoch: 6| Step: 6
Training loss: 2.8915576934814453
Validation loss: 2.747070081772343

Epoch: 6| Step: 7
Training loss: 3.594019889831543
Validation loss: 2.7482415322334535

Epoch: 6| Step: 8
Training loss: 2.4663965702056885
Validation loss: 2.749495506286621

Epoch: 6| Step: 9
Training loss: 2.865917444229126
Validation loss: 2.7446128399141374

Epoch: 6| Step: 10
Training loss: 2.636995792388916
Validation loss: 2.742903919630153

Epoch: 6| Step: 11
Training loss: 3.4257025718688965
Validation loss: 2.741703284684048

Epoch: 6| Step: 12
Training loss: 2.95668888092041
Validation loss: 2.7439607215184036

Epoch: 6| Step: 13
Training loss: 2.7444944381713867
Validation loss: 2.743253187466693

Epoch: 51| Step: 0
Training loss: 3.0275464057922363
Validation loss: 2.7443033367074947

Epoch: 6| Step: 1
Training loss: 2.2346293926239014
Validation loss: 2.7475960126487156

Epoch: 6| Step: 2
Training loss: 1.8071529865264893
Validation loss: 2.7475388126988567

Epoch: 6| Step: 3
Training loss: 3.2729811668395996
Validation loss: 2.7469483114057973

Epoch: 6| Step: 4
Training loss: 2.9633431434631348
Validation loss: 2.7492801143277075

Epoch: 6| Step: 5
Training loss: 3.432150363922119
Validation loss: 2.7518282269918792

Epoch: 6| Step: 6
Training loss: 3.34364914894104
Validation loss: 2.7493000338154454

Epoch: 6| Step: 7
Training loss: 2.4459753036499023
Validation loss: 2.747364326189923

Epoch: 6| Step: 8
Training loss: 2.8489444255828857
Validation loss: 2.7473466063058503

Epoch: 6| Step: 9
Training loss: 2.787919044494629
Validation loss: 2.741854385663104

Epoch: 6| Step: 10
Training loss: 3.238943338394165
Validation loss: 2.7423009923709336

Epoch: 6| Step: 11
Training loss: 2.5150814056396484
Validation loss: 2.738927620713429

Epoch: 6| Step: 12
Training loss: 3.6134543418884277
Validation loss: 2.73640949239013

Epoch: 6| Step: 13
Training loss: 2.86704683303833
Validation loss: 2.738047440846761

Epoch: 52| Step: 0
Training loss: 3.396470785140991
Validation loss: 2.7357985358084402

Epoch: 6| Step: 1
Training loss: 2.6690406799316406
Validation loss: 2.737448835885653

Epoch: 6| Step: 2
Training loss: 1.9831050634384155
Validation loss: 2.7378534629780757

Epoch: 6| Step: 3
Training loss: 3.2784664630889893
Validation loss: 2.7359959643374205

Epoch: 6| Step: 4
Training loss: 3.5310258865356445
Validation loss: 2.740833992599159

Epoch: 6| Step: 5
Training loss: 3.0469555854797363
Validation loss: 2.745561468985773

Epoch: 6| Step: 6
Training loss: 2.6314685344696045
Validation loss: 2.7854100299137894

Epoch: 6| Step: 7
Training loss: 3.2500247955322266
Validation loss: 2.8025213595359557

Epoch: 6| Step: 8
Training loss: 2.2659943103790283
Validation loss: 2.8045691264572965

Epoch: 6| Step: 9
Training loss: 2.833055019378662
Validation loss: 2.7986477933904177

Epoch: 6| Step: 10
Training loss: 2.760071277618408
Validation loss: 2.7758191682959117

Epoch: 6| Step: 11
Training loss: 2.985664129257202
Validation loss: 2.732584681562198

Epoch: 6| Step: 12
Training loss: 3.0335354804992676
Validation loss: 2.7293254867676766

Epoch: 6| Step: 13
Training loss: 2.741046905517578
Validation loss: 2.7288885706214496

Epoch: 53| Step: 0
Training loss: 3.1449170112609863
Validation loss: 2.729782253183344

Epoch: 6| Step: 1
Training loss: 3.8957581520080566
Validation loss: 2.731704306858842

Epoch: 6| Step: 2
Training loss: 2.784609317779541
Validation loss: 2.735982797479117

Epoch: 6| Step: 3
Training loss: 2.8072948455810547
Validation loss: 2.734517010309363

Epoch: 6| Step: 4
Training loss: 2.5477986335754395
Validation loss: 2.733181038210469

Epoch: 6| Step: 5
Training loss: 2.3112688064575195
Validation loss: 2.7272937297821045

Epoch: 6| Step: 6
Training loss: 3.4215247631073
Validation loss: 2.725398991697578

Epoch: 6| Step: 7
Training loss: 2.671308994293213
Validation loss: 2.7234239039882535

Epoch: 6| Step: 8
Training loss: 2.5820229053497314
Validation loss: 2.72058964801091

Epoch: 6| Step: 9
Training loss: 3.581338405609131
Validation loss: 2.7203889175127913

Epoch: 6| Step: 10
Training loss: 2.143368721008301
Validation loss: 2.725469366196663

Epoch: 6| Step: 11
Training loss: 3.2687160968780518
Validation loss: 2.7284912217047905

Epoch: 6| Step: 12
Training loss: 2.305122137069702
Validation loss: 2.7240807164099907

Epoch: 6| Step: 13
Training loss: 2.6578073501586914
Validation loss: 2.723252001629081

Epoch: 54| Step: 0
Training loss: 2.2992095947265625
Validation loss: 2.7235132494280414

Epoch: 6| Step: 1
Training loss: 2.7652809619903564
Validation loss: 2.7208017072369977

Epoch: 6| Step: 2
Training loss: 2.907804250717163
Validation loss: 2.7207737840631956

Epoch: 6| Step: 3
Training loss: 2.14882755279541
Validation loss: 2.7211245029203353

Epoch: 6| Step: 4
Training loss: 2.6469647884368896
Validation loss: 2.718057576046195

Epoch: 6| Step: 5
Training loss: 3.0113003253936768
Validation loss: 2.7191348819322485

Epoch: 6| Step: 6
Training loss: 3.4618351459503174
Validation loss: 2.719619622794531

Epoch: 6| Step: 7
Training loss: 2.3902201652526855
Validation loss: 2.7225950020615772

Epoch: 6| Step: 8
Training loss: 2.794513702392578
Validation loss: 2.7199404316563762

Epoch: 6| Step: 9
Training loss: 3.5202462673187256
Validation loss: 2.715932220541021

Epoch: 6| Step: 10
Training loss: 3.57539439201355
Validation loss: 2.717514148322485

Epoch: 6| Step: 11
Training loss: 2.620659112930298
Validation loss: 2.71666294528592

Epoch: 6| Step: 12
Training loss: 2.586026191711426
Validation loss: 2.7176480062546267

Epoch: 6| Step: 13
Training loss: 3.860119342803955
Validation loss: 2.719229159816619

Epoch: 55| Step: 0
Training loss: 2.821948528289795
Validation loss: 2.721691362319454

Epoch: 6| Step: 1
Training loss: 2.9977495670318604
Validation loss: 2.718094256616408

Epoch: 6| Step: 2
Training loss: 2.8496928215026855
Validation loss: 2.7159652376687653

Epoch: 6| Step: 3
Training loss: 2.232069253921509
Validation loss: 2.717253041523759

Epoch: 6| Step: 4
Training loss: 2.8208184242248535
Validation loss: 2.716611933964555

Epoch: 6| Step: 5
Training loss: 1.9086847305297852
Validation loss: 2.716442600373299

Epoch: 6| Step: 6
Training loss: 2.90437388420105
Validation loss: 2.716044577219153

Epoch: 6| Step: 7
Training loss: 2.7150330543518066
Validation loss: 2.7152554399223736

Epoch: 6| Step: 8
Training loss: 3.9606428146362305
Validation loss: 2.714403211429555

Epoch: 6| Step: 9
Training loss: 2.7067906856536865
Validation loss: 2.712835040143741

Epoch: 6| Step: 10
Training loss: 3.1320667266845703
Validation loss: 2.713369269524851

Epoch: 6| Step: 11
Training loss: 2.601915121078491
Validation loss: 2.71076383385607

Epoch: 6| Step: 12
Training loss: 3.029386520385742
Validation loss: 2.7122081172081733

Epoch: 6| Step: 13
Training loss: 3.8620758056640625
Validation loss: 2.711119831249278

Epoch: 56| Step: 0
Training loss: 3.338550329208374
Validation loss: 2.710674193597609

Epoch: 6| Step: 1
Training loss: 3.086894989013672
Validation loss: 2.7145892753395984

Epoch: 6| Step: 2
Training loss: 3.379115581512451
Validation loss: 2.717297762952825

Epoch: 6| Step: 3
Training loss: 3.7759077548980713
Validation loss: 2.720361171230193

Epoch: 6| Step: 4
Training loss: 2.5812149047851562
Validation loss: 2.7181172396547053

Epoch: 6| Step: 5
Training loss: 2.1789090633392334
Validation loss: 2.7164928426024733

Epoch: 6| Step: 6
Training loss: 3.3212664127349854
Validation loss: 2.713100012912545

Epoch: 6| Step: 7
Training loss: 2.301410436630249
Validation loss: 2.716303984324137

Epoch: 6| Step: 8
Training loss: 3.632021427154541
Validation loss: 2.7118471078975226

Epoch: 6| Step: 9
Training loss: 2.381382703781128
Validation loss: 2.7098143485284623

Epoch: 6| Step: 10
Training loss: 2.6124184131622314
Validation loss: 2.7136063524471816

Epoch: 6| Step: 11
Training loss: 2.2509543895721436
Validation loss: 2.714582456055508

Epoch: 6| Step: 12
Training loss: 2.5543055534362793
Validation loss: 2.7125013618059057

Epoch: 6| Step: 13
Training loss: 2.6967835426330566
Validation loss: 2.713407337024648

Epoch: 57| Step: 0
Training loss: 4.058546543121338
Validation loss: 2.7169321378072104

Epoch: 6| Step: 1
Training loss: 2.450026273727417
Validation loss: 2.7112221000015095

Epoch: 6| Step: 2
Training loss: 2.795578956604004
Validation loss: 2.7140263818925425

Epoch: 6| Step: 3
Training loss: 2.028373956680298
Validation loss: 2.7140177167871946

Epoch: 6| Step: 4
Training loss: 2.6938858032226562
Validation loss: 2.7129585037949266

Epoch: 6| Step: 5
Training loss: 2.933966636657715
Validation loss: 2.7085126728139897

Epoch: 6| Step: 6
Training loss: 2.805828094482422
Validation loss: 2.707850581856184

Epoch: 6| Step: 7
Training loss: 2.9348387718200684
Validation loss: 2.7076133040971655

Epoch: 6| Step: 8
Training loss: 3.2990708351135254
Validation loss: 2.7067599988752797

Epoch: 6| Step: 9
Training loss: 3.1229004859924316
Validation loss: 2.70559980792384

Epoch: 6| Step: 10
Training loss: 3.011336326599121
Validation loss: 2.703053541080926

Epoch: 6| Step: 11
Training loss: 2.1631197929382324
Validation loss: 2.704905325366605

Epoch: 6| Step: 12
Training loss: 3.206005096435547
Validation loss: 2.704437614769064

Epoch: 6| Step: 13
Training loss: 2.3528401851654053
Validation loss: 2.704808294132192

Epoch: 58| Step: 0
Training loss: 2.6206560134887695
Validation loss: 2.7050699546772945

Epoch: 6| Step: 1
Training loss: 2.7012593746185303
Validation loss: 2.704439745154432

Epoch: 6| Step: 2
Training loss: 2.8312149047851562
Validation loss: 2.703759742039506

Epoch: 6| Step: 3
Training loss: 2.921924591064453
Validation loss: 2.7033890293490503

Epoch: 6| Step: 4
Training loss: 3.005159378051758
Validation loss: 2.7030850046424457

Epoch: 6| Step: 5
Training loss: 3.105828285217285
Validation loss: 2.702841725400699

Epoch: 6| Step: 6
Training loss: 2.1025502681732178
Validation loss: 2.70199187596639

Epoch: 6| Step: 7
Training loss: 2.792933702468872
Validation loss: 2.7012742616797007

Epoch: 6| Step: 8
Training loss: 2.9291207790374756
Validation loss: 2.7030464551782094

Epoch: 6| Step: 9
Training loss: 3.4795548915863037
Validation loss: 2.701983226242886

Epoch: 6| Step: 10
Training loss: 2.044442653656006
Validation loss: 2.6998631031282487

Epoch: 6| Step: 11
Training loss: 3.3098220825195312
Validation loss: 2.7001976095220095

Epoch: 6| Step: 12
Training loss: 2.8551478385925293
Validation loss: 2.7000763108653407

Epoch: 6| Step: 13
Training loss: 3.5870518684387207
Validation loss: 2.7014660886538926

Epoch: 59| Step: 0
Training loss: 2.834545373916626
Validation loss: 2.6987448789740123

Epoch: 6| Step: 1
Training loss: 3.440186023712158
Validation loss: 2.6976281801859536

Epoch: 6| Step: 2
Training loss: 2.8917927742004395
Validation loss: 2.7009625460511897

Epoch: 6| Step: 3
Training loss: 2.8745245933532715
Validation loss: 2.7008974372699694

Epoch: 6| Step: 4
Training loss: 3.3463196754455566
Validation loss: 2.6996438682720227

Epoch: 6| Step: 5
Training loss: 2.6515989303588867
Validation loss: 2.702236121700656

Epoch: 6| Step: 6
Training loss: 2.049190044403076
Validation loss: 2.7016167640686035

Epoch: 6| Step: 7
Training loss: 2.770752429962158
Validation loss: 2.6998820868871545

Epoch: 6| Step: 8
Training loss: 3.1628012657165527
Validation loss: 2.700059098582114

Epoch: 6| Step: 9
Training loss: 2.2804057598114014
Validation loss: 2.7000166088022213

Epoch: 6| Step: 10
Training loss: 3.034306764602661
Validation loss: 2.6973881772769395

Epoch: 6| Step: 11
Training loss: 3.2445430755615234
Validation loss: 2.698604873431626

Epoch: 6| Step: 12
Training loss: 2.649569034576416
Validation loss: 2.6989727097172893

Epoch: 6| Step: 13
Training loss: 2.6199185848236084
Validation loss: 2.6996265201158423

Epoch: 60| Step: 0
Training loss: 2.04414963722229
Validation loss: 2.6997545662746636

Epoch: 6| Step: 1
Training loss: 3.6194372177124023
Validation loss: 2.697632194847189

Epoch: 6| Step: 2
Training loss: 3.184951066970825
Validation loss: 2.6977614997535624

Epoch: 6| Step: 3
Training loss: 3.43756103515625
Validation loss: 2.7014938682638188

Epoch: 6| Step: 4
Training loss: 2.7422661781311035
Validation loss: 2.697083560369348

Epoch: 6| Step: 5
Training loss: 2.5694096088409424
Validation loss: 2.6991035835717314

Epoch: 6| Step: 6
Training loss: 3.152709484100342
Validation loss: 2.6999975942796275

Epoch: 6| Step: 7
Training loss: 2.0263805389404297
Validation loss: 2.6992306119652203

Epoch: 6| Step: 8
Training loss: 3.2313714027404785
Validation loss: 2.6968675351911977

Epoch: 6| Step: 9
Training loss: 3.0595083236694336
Validation loss: 2.6973580032266598

Epoch: 6| Step: 10
Training loss: 2.7051775455474854
Validation loss: 2.6978352864583335

Epoch: 6| Step: 11
Training loss: 2.382875919342041
Validation loss: 2.696486137246573

Epoch: 6| Step: 12
Training loss: 3.128265619277954
Validation loss: 2.6940684215996855

Epoch: 6| Step: 13
Training loss: 2.4924612045288086
Validation loss: 2.6934564318708194

Epoch: 61| Step: 0
Training loss: 3.123721122741699
Validation loss: 2.6971125551449355

Epoch: 6| Step: 1
Training loss: 3.093111515045166
Validation loss: 2.7049912509097847

Epoch: 6| Step: 2
Training loss: 2.8942887783050537
Validation loss: 2.705225352318056

Epoch: 6| Step: 3
Training loss: 2.5247929096221924
Validation loss: 2.7107583297196256

Epoch: 6| Step: 4
Training loss: 2.4575307369232178
Validation loss: 2.707034735269444

Epoch: 6| Step: 5
Training loss: 3.0745835304260254
Validation loss: 2.7151196002960205

Epoch: 6| Step: 6
Training loss: 2.7068371772766113
Validation loss: 2.699585171156032

Epoch: 6| Step: 7
Training loss: 3.3070006370544434
Validation loss: 2.702573437844553

Epoch: 6| Step: 8
Training loss: 2.1976661682128906
Validation loss: 2.696509712485857

Epoch: 6| Step: 9
Training loss: 3.1636886596679688
Validation loss: 2.6963924336177048

Epoch: 6| Step: 10
Training loss: 3.820115089416504
Validation loss: 2.693626726827314

Epoch: 6| Step: 11
Training loss: 2.231459617614746
Validation loss: 2.7007082764820387

Epoch: 6| Step: 12
Training loss: 2.8378701210021973
Validation loss: 2.69970118102207

Epoch: 6| Step: 13
Training loss: 2.3200254440307617
Validation loss: 2.703042443080615

Epoch: 62| Step: 0
Training loss: 3.4010238647460938
Validation loss: 2.7005661687543316

Epoch: 6| Step: 1
Training loss: 2.174107074737549
Validation loss: 2.7035154014505367

Epoch: 6| Step: 2
Training loss: 2.430142641067505
Validation loss: 2.699142074072233

Epoch: 6| Step: 3
Training loss: 3.5454468727111816
Validation loss: 2.701259664309922

Epoch: 6| Step: 4
Training loss: 2.8429007530212402
Validation loss: 2.698634088680308

Epoch: 6| Step: 5
Training loss: 2.366392135620117
Validation loss: 2.696121718293877

Epoch: 6| Step: 6
Training loss: 2.6619644165039062
Validation loss: 2.692422684802804

Epoch: 6| Step: 7
Training loss: 2.968966484069824
Validation loss: 2.692504375211654

Epoch: 6| Step: 8
Training loss: 3.697862386703491
Validation loss: 2.6954892784036617

Epoch: 6| Step: 9
Training loss: 3.1867799758911133
Validation loss: 2.6922125816345215

Epoch: 6| Step: 10
Training loss: 3.350353240966797
Validation loss: 2.6909996847952566

Epoch: 6| Step: 11
Training loss: 2.281221866607666
Validation loss: 2.6958032474722913

Epoch: 6| Step: 12
Training loss: 2.7984704971313477
Validation loss: 2.690990771016767

Epoch: 6| Step: 13
Training loss: 1.7162926197052002
Validation loss: 2.691239826140865

Epoch: 63| Step: 0
Training loss: 2.2132887840270996
Validation loss: 2.695622036533971

Epoch: 6| Step: 1
Training loss: 3.097074508666992
Validation loss: 2.692259433448956

Epoch: 6| Step: 2
Training loss: 2.5427048206329346
Validation loss: 2.693214375485656

Epoch: 6| Step: 3
Training loss: 3.532423734664917
Validation loss: 2.691712979347475

Epoch: 6| Step: 4
Training loss: 2.9109647274017334
Validation loss: 2.691126726006949

Epoch: 6| Step: 5
Training loss: 2.4655635356903076
Validation loss: 2.692174096261301

Epoch: 6| Step: 6
Training loss: 2.681550979614258
Validation loss: 2.690843366807507

Epoch: 6| Step: 7
Training loss: 2.759622573852539
Validation loss: 2.689154996666857

Epoch: 6| Step: 8
Training loss: 2.7110376358032227
Validation loss: 2.686768803545224

Epoch: 6| Step: 9
Training loss: 2.9286680221557617
Validation loss: 2.6890908261781097

Epoch: 6| Step: 10
Training loss: 2.806727170944214
Validation loss: 2.687258264069916

Epoch: 6| Step: 11
Training loss: 3.32000732421875
Validation loss: 2.6869641862889773

Epoch: 6| Step: 12
Training loss: 3.5075223445892334
Validation loss: 2.6858967632375736

Epoch: 6| Step: 13
Training loss: 2.086702346801758
Validation loss: 2.6853802024677234

Epoch: 64| Step: 0
Training loss: 2.9391186237335205
Validation loss: 2.6875059040643836

Epoch: 6| Step: 1
Training loss: 3.1054491996765137
Validation loss: 2.6870969444192867

Epoch: 6| Step: 2
Training loss: 3.421245574951172
Validation loss: 2.693061613267468

Epoch: 6| Step: 3
Training loss: 1.9424186944961548
Validation loss: 2.7058931063580256

Epoch: 6| Step: 4
Training loss: 3.1882123947143555
Validation loss: 2.70224949108657

Epoch: 6| Step: 5
Training loss: 2.3157663345336914
Validation loss: 2.6994943541865193

Epoch: 6| Step: 6
Training loss: 2.651651382446289
Validation loss: 2.689370278389223

Epoch: 6| Step: 7
Training loss: 2.8535690307617188
Validation loss: 2.6858563602611585

Epoch: 6| Step: 8
Training loss: 2.558619976043701
Validation loss: 2.6828178026342906

Epoch: 6| Step: 9
Training loss: 2.580561399459839
Validation loss: 2.6868214068874234

Epoch: 6| Step: 10
Training loss: 3.7993905544281006
Validation loss: 2.6888378538111204

Epoch: 6| Step: 11
Training loss: 2.7521910667419434
Validation loss: 2.6913225702060166

Epoch: 6| Step: 12
Training loss: 3.265462875366211
Validation loss: 2.697694076004849

Epoch: 6| Step: 13
Training loss: 2.373621940612793
Validation loss: 2.697091576873615

Epoch: 65| Step: 0
Training loss: 3.7555387020111084
Validation loss: 2.6923934439177155

Epoch: 6| Step: 1
Training loss: 2.741091251373291
Validation loss: 2.690099531604398

Epoch: 6| Step: 2
Training loss: 2.9781551361083984
Validation loss: 2.6888771467311408

Epoch: 6| Step: 3
Training loss: 1.731791615486145
Validation loss: 2.6835544955345894

Epoch: 6| Step: 4
Training loss: 2.2231788635253906
Validation loss: 2.6845164683557328

Epoch: 6| Step: 5
Training loss: 2.759491205215454
Validation loss: 2.683815735642628

Epoch: 6| Step: 6
Training loss: 2.3842520713806152
Validation loss: 2.686608173513925

Epoch: 6| Step: 7
Training loss: 1.7980095148086548
Validation loss: 2.691119170957996

Epoch: 6| Step: 8
Training loss: 3.6305296421051025
Validation loss: 2.6938805246865876

Epoch: 6| Step: 9
Training loss: 3.724642038345337
Validation loss: 2.697253647670951

Epoch: 6| Step: 10
Training loss: 2.2819433212280273
Validation loss: 2.6958025424711165

Epoch: 6| Step: 11
Training loss: 3.3470101356506348
Validation loss: 2.68956849908316

Epoch: 6| Step: 12
Training loss: 3.55961275100708
Validation loss: 2.687918383588073

Epoch: 6| Step: 13
Training loss: 3.0440454483032227
Validation loss: 2.6847211878786803

Epoch: 66| Step: 0
Training loss: 3.2785072326660156
Validation loss: 2.6839063218844834

Epoch: 6| Step: 1
Training loss: 2.4018638134002686
Validation loss: 2.680573283985097

Epoch: 6| Step: 2
Training loss: 3.378262996673584
Validation loss: 2.6834912120655017

Epoch: 6| Step: 3
Training loss: 2.2170050144195557
Validation loss: 2.682015380551738

Epoch: 6| Step: 4
Training loss: 3.4461143016815186
Validation loss: 2.681851061441565

Epoch: 6| Step: 5
Training loss: 2.899463176727295
Validation loss: 2.678741006441014

Epoch: 6| Step: 6
Training loss: 2.518925666809082
Validation loss: 2.6797983979666107

Epoch: 6| Step: 7
Training loss: 1.5044645071029663
Validation loss: 2.681707743675478

Epoch: 6| Step: 8
Training loss: 3.4073519706726074
Validation loss: 2.6811966152601343

Epoch: 6| Step: 9
Training loss: 2.5359010696411133
Validation loss: 2.679137952866093

Epoch: 6| Step: 10
Training loss: 2.881882905960083
Validation loss: 2.6805981013082687

Epoch: 6| Step: 11
Training loss: 3.0584466457366943
Validation loss: 2.6769029248145317

Epoch: 6| Step: 12
Training loss: 3.3145134449005127
Validation loss: 2.6757628379329557

Epoch: 6| Step: 13
Training loss: 3.0424139499664307
Validation loss: 2.682842831457815

Epoch: 67| Step: 0
Training loss: 2.596954822540283
Validation loss: 2.6786439341883503

Epoch: 6| Step: 1
Training loss: 2.8730387687683105
Validation loss: 2.6784973605986564

Epoch: 6| Step: 2
Training loss: 3.0506796836853027
Validation loss: 2.6812009170491207

Epoch: 6| Step: 3
Training loss: 3.1534547805786133
Validation loss: 2.6770569303984284

Epoch: 6| Step: 4
Training loss: 3.51088809967041
Validation loss: 2.677506959566506

Epoch: 6| Step: 5
Training loss: 2.3503570556640625
Validation loss: 2.6783563347272974

Epoch: 6| Step: 6
Training loss: 2.7681398391723633
Validation loss: 2.6752196768278718

Epoch: 6| Step: 7
Training loss: 3.4077107906341553
Validation loss: 2.6765073601917555

Epoch: 6| Step: 8
Training loss: 2.3466618061065674
Validation loss: 2.678678809955556

Epoch: 6| Step: 9
Training loss: 2.3568267822265625
Validation loss: 2.677958139809229

Epoch: 6| Step: 10
Training loss: 3.245016098022461
Validation loss: 2.675671869708646

Epoch: 6| Step: 11
Training loss: 2.890984535217285
Validation loss: 2.6741207850876676

Epoch: 6| Step: 12
Training loss: 2.2521135807037354
Validation loss: 2.6785259939009145

Epoch: 6| Step: 13
Training loss: 3.0573103427886963
Validation loss: 2.6772320603811615

Epoch: 68| Step: 0
Training loss: 2.0137693881988525
Validation loss: 2.6783884776535856

Epoch: 6| Step: 1
Training loss: 1.9966583251953125
Validation loss: 2.6814516180305072

Epoch: 6| Step: 2
Training loss: 3.614013671875
Validation loss: 2.6770007969230734

Epoch: 6| Step: 3
Training loss: 3.450638771057129
Validation loss: 2.6756075530923824

Epoch: 6| Step: 4
Training loss: 3.464691162109375
Validation loss: 2.6747572755300872

Epoch: 6| Step: 5
Training loss: 2.615941286087036
Validation loss: 2.6748813377913607

Epoch: 6| Step: 6
Training loss: 2.137709617614746
Validation loss: 2.6709328466846096

Epoch: 6| Step: 7
Training loss: 3.4759678840637207
Validation loss: 2.6740582091833955

Epoch: 6| Step: 8
Training loss: 3.0124945640563965
Validation loss: 2.673910915210683

Epoch: 6| Step: 9
Training loss: 2.463937759399414
Validation loss: 2.6741536945425053

Epoch: 6| Step: 10
Training loss: 1.8284255266189575
Validation loss: 2.669399371711157

Epoch: 6| Step: 11
Training loss: 3.5293116569519043
Validation loss: 2.6713348178453344

Epoch: 6| Step: 12
Training loss: 3.0920610427856445
Validation loss: 2.6735661388725362

Epoch: 6| Step: 13
Training loss: 3.19583797454834
Validation loss: 2.6718825986308437

Epoch: 69| Step: 0
Training loss: 3.390381336212158
Validation loss: 2.6687434540000012

Epoch: 6| Step: 1
Training loss: 2.3043532371520996
Validation loss: 2.6694640395461873

Epoch: 6| Step: 2
Training loss: 3.216653347015381
Validation loss: 2.670471978443925

Epoch: 6| Step: 3
Training loss: 2.3510072231292725
Validation loss: 2.6682048613025295

Epoch: 6| Step: 4
Training loss: 3.532607078552246
Validation loss: 2.6664642954385407

Epoch: 6| Step: 5
Training loss: 3.0745599269866943
Validation loss: 2.6718475510997157

Epoch: 6| Step: 6
Training loss: 2.8102400302886963
Validation loss: 2.668571005585373

Epoch: 6| Step: 7
Training loss: 2.4968371391296387
Validation loss: 2.665899540788384

Epoch: 6| Step: 8
Training loss: 2.754361867904663
Validation loss: 2.66999751265331

Epoch: 6| Step: 9
Training loss: 2.9362332820892334
Validation loss: 2.664450409591839

Epoch: 6| Step: 10
Training loss: 2.3794808387756348
Validation loss: 2.6691768041221042

Epoch: 6| Step: 11
Training loss: 2.9177212715148926
Validation loss: 2.6683384167250765

Epoch: 6| Step: 12
Training loss: 2.768850088119507
Validation loss: 2.6710370663673646

Epoch: 6| Step: 13
Training loss: 2.54938006401062
Validation loss: 2.6716376478954027

Epoch: 70| Step: 0
Training loss: 3.0837578773498535
Validation loss: 2.673450746843892

Epoch: 6| Step: 1
Training loss: 2.2248079776763916
Validation loss: 2.6811326934445288

Epoch: 6| Step: 2
Training loss: 2.668454647064209
Validation loss: 2.68330241018726

Epoch: 6| Step: 3
Training loss: 2.981750011444092
Validation loss: 2.6793284698199202

Epoch: 6| Step: 4
Training loss: 2.9144537448883057
Validation loss: 2.666535287775019

Epoch: 6| Step: 5
Training loss: 3.5925679206848145
Validation loss: 2.6615968186368226

Epoch: 6| Step: 6
Training loss: 2.5674185752868652
Validation loss: 2.652690197831841

Epoch: 6| Step: 7
Training loss: 2.389622449874878
Validation loss: 2.6556178549284577

Epoch: 6| Step: 8
Training loss: 2.468095302581787
Validation loss: 2.6535085119226927

Epoch: 6| Step: 9
Training loss: 3.174431324005127
Validation loss: 2.6546004254330873

Epoch: 6| Step: 10
Training loss: 2.2844643592834473
Validation loss: 2.654717158245784

Epoch: 6| Step: 11
Training loss: 3.2560606002807617
Validation loss: 2.6510377853147444

Epoch: 6| Step: 12
Training loss: 3.8733246326446533
Validation loss: 2.6452835939263784

Epoch: 6| Step: 13
Training loss: 1.478209376335144
Validation loss: 2.647681120903261

Epoch: 71| Step: 0
Training loss: 2.0863118171691895
Validation loss: 2.650824377613683

Epoch: 6| Step: 1
Training loss: 2.7374374866485596
Validation loss: 2.6471131194022393

Epoch: 6| Step: 2
Training loss: 2.953230857849121
Validation loss: 2.647173317529822

Epoch: 6| Step: 3
Training loss: 2.3863987922668457
Validation loss: 2.646695001150972

Epoch: 6| Step: 4
Training loss: 2.5830724239349365
Validation loss: 2.6461443593425136

Epoch: 6| Step: 5
Training loss: 3.362672805786133
Validation loss: 2.643481287904965

Epoch: 6| Step: 6
Training loss: 2.947199821472168
Validation loss: 2.6463783787142847

Epoch: 6| Step: 7
Training loss: 2.9159531593322754
Validation loss: 2.644277972559775

Epoch: 6| Step: 8
Training loss: 2.7844443321228027
Validation loss: 2.6415232176421792

Epoch: 6| Step: 9
Training loss: 2.73972487449646
Validation loss: 2.6436393748047533

Epoch: 6| Step: 10
Training loss: 3.3412036895751953
Validation loss: 2.6448247817254837

Epoch: 6| Step: 11
Training loss: 3.3004589080810547
Validation loss: 2.641109953644455

Epoch: 6| Step: 12
Training loss: 2.159855365753174
Validation loss: 2.6417107223182597

Epoch: 6| Step: 13
Training loss: 3.2330780029296875
Validation loss: 2.6443094566304195

Epoch: 72| Step: 0
Training loss: 2.560565948486328
Validation loss: 2.6393812318002023

Epoch: 6| Step: 1
Training loss: 3.7204926013946533
Validation loss: 2.6504669471453597

Epoch: 6| Step: 2
Training loss: 2.815730571746826
Validation loss: 2.649919658578852

Epoch: 6| Step: 3
Training loss: 2.207472801208496
Validation loss: 2.6497723030787643

Epoch: 6| Step: 4
Training loss: 2.0310099124908447
Validation loss: 2.6546033274742866

Epoch: 6| Step: 5
Training loss: 2.7126264572143555
Validation loss: 2.642479701708722

Epoch: 6| Step: 6
Training loss: 2.252462387084961
Validation loss: 2.6433510190697125

Epoch: 6| Step: 7
Training loss: 3.3698835372924805
Validation loss: 2.6404407819112143

Epoch: 6| Step: 8
Training loss: 2.7258450984954834
Validation loss: 2.645637760880173

Epoch: 6| Step: 9
Training loss: 3.474998950958252
Validation loss: 2.643176119814637

Epoch: 6| Step: 10
Training loss: 3.759887933731079
Validation loss: 2.6389743102494108

Epoch: 6| Step: 11
Training loss: 2.58355712890625
Validation loss: 2.6408751779986965

Epoch: 6| Step: 12
Training loss: 2.6477861404418945
Validation loss: 2.642332657690971

Epoch: 6| Step: 13
Training loss: 2.315164804458618
Validation loss: 2.6390954294512348

Epoch: 73| Step: 0
Training loss: 2.881925582885742
Validation loss: 2.6369266868919454

Epoch: 6| Step: 1
Training loss: 3.220799207687378
Validation loss: 2.6381327272743307

Epoch: 6| Step: 2
Training loss: 3.41131854057312
Validation loss: 2.63633898509446

Epoch: 6| Step: 3
Training loss: 3.367128372192383
Validation loss: 2.6385721750156854

Epoch: 6| Step: 4
Training loss: 2.3984620571136475
Validation loss: 2.634063600212015

Epoch: 6| Step: 5
Training loss: 2.822091579437256
Validation loss: 2.6349092837302917

Epoch: 6| Step: 6
Training loss: 3.2199344635009766
Validation loss: 2.63187486638305

Epoch: 6| Step: 7
Training loss: 2.6218738555908203
Validation loss: 2.628547801766344

Epoch: 6| Step: 8
Training loss: 2.3001694679260254
Validation loss: 2.6292915472420315

Epoch: 6| Step: 9
Training loss: 2.0125937461853027
Validation loss: 2.626019142007315

Epoch: 6| Step: 10
Training loss: 2.861682891845703
Validation loss: 2.6339518716258388

Epoch: 6| Step: 11
Training loss: 2.9993772506713867
Validation loss: 2.6283362219410558

Epoch: 6| Step: 12
Training loss: 2.4212279319763184
Validation loss: 2.6310503893001105

Epoch: 6| Step: 13
Training loss: 2.7299740314483643
Validation loss: 2.6285949240448656

Epoch: 74| Step: 0
Training loss: 1.8397321701049805
Validation loss: 2.629393272502448

Epoch: 6| Step: 1
Training loss: 3.536085367202759
Validation loss: 2.62752257111252

Epoch: 6| Step: 2
Training loss: 2.352786064147949
Validation loss: 2.6235071856488466

Epoch: 6| Step: 3
Training loss: 2.312753200531006
Validation loss: 2.623947922901441

Epoch: 6| Step: 4
Training loss: 3.9609715938568115
Validation loss: 2.6266451189594884

Epoch: 6| Step: 5
Training loss: 2.0517783164978027
Validation loss: 2.6258880899798487

Epoch: 6| Step: 6
Training loss: 2.0608084201812744
Validation loss: 2.6213601558439192

Epoch: 6| Step: 7
Training loss: 2.7133498191833496
Validation loss: 2.6250843719769548

Epoch: 6| Step: 8
Training loss: 2.541016101837158
Validation loss: 2.625177908969182

Epoch: 6| Step: 9
Training loss: 2.8286843299865723
Validation loss: 2.6218587275474303

Epoch: 6| Step: 10
Training loss: 3.8847978115081787
Validation loss: 2.6215461505356656

Epoch: 6| Step: 11
Training loss: 2.6963162422180176
Validation loss: 2.6226751317260084

Epoch: 6| Step: 12
Training loss: 3.6036601066589355
Validation loss: 2.6237201434309765

Epoch: 6| Step: 13
Training loss: 2.921053886413574
Validation loss: 2.6233778179332776

Epoch: 75| Step: 0
Training loss: 2.5336833000183105
Validation loss: 2.6181277741668043

Epoch: 6| Step: 1
Training loss: 2.479530096054077
Validation loss: 2.619873539094002

Epoch: 6| Step: 2
Training loss: 2.175449848175049
Validation loss: 2.6272795482348372

Epoch: 6| Step: 3
Training loss: 3.0933830738067627
Validation loss: 2.6272769999760452

Epoch: 6| Step: 4
Training loss: 3.425349235534668
Validation loss: 2.6255478474401657

Epoch: 6| Step: 5
Training loss: 2.6919045448303223
Validation loss: 2.6267867626682406

Epoch: 6| Step: 6
Training loss: 3.277751922607422
Validation loss: 2.6288070832529375

Epoch: 6| Step: 7
Training loss: 3.224522590637207
Validation loss: 2.621670804997926

Epoch: 6| Step: 8
Training loss: 3.0046253204345703
Validation loss: 2.6229508820400445

Epoch: 6| Step: 9
Training loss: 1.961449384689331
Validation loss: 2.6209077988901446

Epoch: 6| Step: 10
Training loss: 2.9413633346557617
Validation loss: 2.619855914064633

Epoch: 6| Step: 11
Training loss: 3.011899948120117
Validation loss: 2.6189942770106818

Epoch: 6| Step: 12
Training loss: 3.0061898231506348
Validation loss: 2.6189031344588085

Epoch: 6| Step: 13
Training loss: 2.0461831092834473
Validation loss: 2.619983064231052

Epoch: 76| Step: 0
Training loss: 2.725750207901001
Validation loss: 2.628724205878473

Epoch: 6| Step: 1
Training loss: 3.515143632888794
Validation loss: 2.626763141283425

Epoch: 6| Step: 2
Training loss: 2.812587261199951
Validation loss: 2.6203466794824086

Epoch: 6| Step: 3
Training loss: 2.5036673545837402
Validation loss: 2.6252997895722747

Epoch: 6| Step: 4
Training loss: 2.5256612300872803
Validation loss: 2.624913207946285

Epoch: 6| Step: 5
Training loss: 2.457493782043457
Validation loss: 2.6217725558947493

Epoch: 6| Step: 6
Training loss: 3.089672803878784
Validation loss: 2.615640112148818

Epoch: 6| Step: 7
Training loss: 3.7159762382507324
Validation loss: 2.6129967474168345

Epoch: 6| Step: 8
Training loss: 2.514683961868286
Validation loss: 2.611555399433259

Epoch: 6| Step: 9
Training loss: 2.2854812145233154
Validation loss: 2.615372488575597

Epoch: 6| Step: 10
Training loss: 2.319288730621338
Validation loss: 2.6150310090793076

Epoch: 6| Step: 11
Training loss: 2.7772436141967773
Validation loss: 2.6223488571823284

Epoch: 6| Step: 12
Training loss: 3.4969897270202637
Validation loss: 2.6221743834915983

Epoch: 6| Step: 13
Training loss: 2.1266908645629883
Validation loss: 2.623927625276709

Epoch: 77| Step: 0
Training loss: 2.778872013092041
Validation loss: 2.6327812902389036

Epoch: 6| Step: 1
Training loss: 2.4184932708740234
Validation loss: 2.635152042552989

Epoch: 6| Step: 2
Training loss: 3.4027462005615234
Validation loss: 2.6380544298438617

Epoch: 6| Step: 3
Training loss: 2.832904100418091
Validation loss: 2.631226267865909

Epoch: 6| Step: 4
Training loss: 3.009542465209961
Validation loss: 2.6208982852197464

Epoch: 6| Step: 5
Training loss: 2.7506580352783203
Validation loss: 2.6189345364929526

Epoch: 6| Step: 6
Training loss: 2.852856159210205
Validation loss: 2.6226360695336455

Epoch: 6| Step: 7
Training loss: 2.8212618827819824
Validation loss: 2.613258433598344

Epoch: 6| Step: 8
Training loss: 2.1318814754486084
Validation loss: 2.615523733118529

Epoch: 6| Step: 9
Training loss: 2.934319019317627
Validation loss: 2.6181964258993826

Epoch: 6| Step: 10
Training loss: 2.857877254486084
Validation loss: 2.615731836647116

Epoch: 6| Step: 11
Training loss: 2.658402919769287
Validation loss: 2.6159076844492266

Epoch: 6| Step: 12
Training loss: 2.708613872528076
Validation loss: 2.6166083505076747

Epoch: 6| Step: 13
Training loss: 3.181913137435913
Validation loss: 2.621539156924012

Epoch: 78| Step: 0
Training loss: 2.940916061401367
Validation loss: 2.6147839894858738

Epoch: 6| Step: 1
Training loss: 2.5997283458709717
Validation loss: 2.6129629047968055

Epoch: 6| Step: 2
Training loss: 3.463231086730957
Validation loss: 2.619605948848109

Epoch: 6| Step: 3
Training loss: 3.8219492435455322
Validation loss: 2.6259530334062475

Epoch: 6| Step: 4
Training loss: 3.740661144256592
Validation loss: 2.623438928716926

Epoch: 6| Step: 5
Training loss: 2.917273759841919
Validation loss: 2.622605654501146

Epoch: 6| Step: 6
Training loss: 2.26739239692688
Validation loss: 2.623328701142342

Epoch: 6| Step: 7
Training loss: 2.3824315071105957
Validation loss: 2.6203627381273495

Epoch: 6| Step: 8
Training loss: 2.0035440921783447
Validation loss: 2.615235185110441

Epoch: 6| Step: 9
Training loss: 2.6925806999206543
Validation loss: 2.617788040509788

Epoch: 6| Step: 10
Training loss: 2.8462066650390625
Validation loss: 2.6153478904436995

Epoch: 6| Step: 11
Training loss: 2.1001858711242676
Validation loss: 2.6126679194870817

Epoch: 6| Step: 12
Training loss: 2.772571086883545
Validation loss: 2.616637501665341

Epoch: 6| Step: 13
Training loss: 2.3752975463867188
Validation loss: 2.621427692392821

Epoch: 79| Step: 0
Training loss: 2.4725112915039062
Validation loss: 2.616338578603601

Epoch: 6| Step: 1
Training loss: 2.873211145401001
Validation loss: 2.6160682221894622

Epoch: 6| Step: 2
Training loss: 2.9114108085632324
Validation loss: 2.613348937803699

Epoch: 6| Step: 3
Training loss: 2.107163190841675
Validation loss: 2.6082700888315835

Epoch: 6| Step: 4
Training loss: 3.2055909633636475
Validation loss: 2.609832191980013

Epoch: 6| Step: 5
Training loss: 2.5878796577453613
Validation loss: 2.606681252038607

Epoch: 6| Step: 6
Training loss: 3.214256763458252
Validation loss: 2.6090553037581907

Epoch: 6| Step: 7
Training loss: 3.9470386505126953
Validation loss: 2.6117246074061238

Epoch: 6| Step: 8
Training loss: 2.7871806621551514
Validation loss: 2.610195844404159

Epoch: 6| Step: 9
Training loss: 2.0382208824157715
Validation loss: 2.61301947921835

Epoch: 6| Step: 10
Training loss: 2.594613552093506
Validation loss: 2.6180169838731007

Epoch: 6| Step: 11
Training loss: 2.5420193672180176
Validation loss: 2.624215474692724

Epoch: 6| Step: 12
Training loss: 3.0414633750915527
Validation loss: 2.6269877187667356

Epoch: 6| Step: 13
Training loss: 2.6917810440063477
Validation loss: 2.6248557798324095

Epoch: 80| Step: 0
Training loss: 2.3363702297210693
Validation loss: 2.6284974044369114

Epoch: 6| Step: 1
Training loss: 3.293332099914551
Validation loss: 2.6332788416134414

Epoch: 6| Step: 2
Training loss: 2.462942361831665
Validation loss: 2.636832365425684

Epoch: 6| Step: 3
Training loss: 2.6178557872772217
Validation loss: 2.6315128290525047

Epoch: 6| Step: 4
Training loss: 2.921764612197876
Validation loss: 2.6305358743154876

Epoch: 6| Step: 5
Training loss: 2.794297695159912
Validation loss: 2.6285692286747757

Epoch: 6| Step: 6
Training loss: 2.258894205093384
Validation loss: 2.6231070539002777

Epoch: 6| Step: 7
Training loss: 3.217660903930664
Validation loss: 2.6123615772493425

Epoch: 6| Step: 8
Training loss: 2.2509288787841797
Validation loss: 2.6122553681814544

Epoch: 6| Step: 9
Training loss: 2.4240336418151855
Validation loss: 2.6129369992081837

Epoch: 6| Step: 10
Training loss: 2.700705051422119
Validation loss: 2.6086760951626684

Epoch: 6| Step: 11
Training loss: 3.3939385414123535
Validation loss: 2.6073602194427163

Epoch: 6| Step: 12
Training loss: 3.884293556213379
Validation loss: 2.612150779334448

Epoch: 6| Step: 13
Training loss: 2.331648349761963
Validation loss: 2.605593419844104

Epoch: 81| Step: 0
Training loss: 2.2447075843811035
Validation loss: 2.603524120905066

Epoch: 6| Step: 1
Training loss: 3.3711061477661133
Validation loss: 2.605072593176237

Epoch: 6| Step: 2
Training loss: 3.51063871383667
Validation loss: 2.6047613390030397

Epoch: 6| Step: 3
Training loss: 3.2918734550476074
Validation loss: 2.6040553469811716

Epoch: 6| Step: 4
Training loss: 3.003387689590454
Validation loss: 2.600720892670334

Epoch: 6| Step: 5
Training loss: 3.3164968490600586
Validation loss: 2.60482834487833

Epoch: 6| Step: 6
Training loss: 2.7270865440368652
Validation loss: 2.5992744584237375

Epoch: 6| Step: 7
Training loss: 2.0166549682617188
Validation loss: 2.6026540315279396

Epoch: 6| Step: 8
Training loss: 2.7062809467315674
Validation loss: 2.6046137348298104

Epoch: 6| Step: 9
Training loss: 3.372048854827881
Validation loss: 2.5994568627367736

Epoch: 6| Step: 10
Training loss: 1.8986575603485107
Validation loss: 2.602271996518617

Epoch: 6| Step: 11
Training loss: 2.3718152046203613
Validation loss: 2.5970603266069965

Epoch: 6| Step: 12
Training loss: 2.473463535308838
Validation loss: 2.6000277047516196

Epoch: 6| Step: 13
Training loss: 2.6175363063812256
Validation loss: 2.597937935142107

Epoch: 82| Step: 0
Training loss: 3.1650497913360596
Validation loss: 2.6042701916028093

Epoch: 6| Step: 1
Training loss: 2.534148693084717
Validation loss: 2.598764245228101

Epoch: 6| Step: 2
Training loss: 2.585615873336792
Validation loss: 2.601296512029504

Epoch: 6| Step: 3
Training loss: 2.847435712814331
Validation loss: 2.5994969875581804

Epoch: 6| Step: 4
Training loss: 3.1868622303009033
Validation loss: 2.602677749049279

Epoch: 6| Step: 5
Training loss: 2.564901828765869
Validation loss: 2.5978583546094995

Epoch: 6| Step: 6
Training loss: 2.7519938945770264
Validation loss: 2.59944430217948

Epoch: 6| Step: 7
Training loss: 3.134284734725952
Validation loss: 2.603780723387195

Epoch: 6| Step: 8
Training loss: 2.2408971786499023
Validation loss: 2.5994333682521695

Epoch: 6| Step: 9
Training loss: 2.3048598766326904
Validation loss: 2.606040240615927

Epoch: 6| Step: 10
Training loss: 3.187626838684082
Validation loss: 2.6082529867849042

Epoch: 6| Step: 11
Training loss: 2.98799991607666
Validation loss: 2.604532831458635

Epoch: 6| Step: 12
Training loss: 2.4219069480895996
Validation loss: 2.6112249641008276

Epoch: 6| Step: 13
Training loss: 3.1190741062164307
Validation loss: 2.6081058056123796

Epoch: 83| Step: 0
Training loss: 2.949751377105713
Validation loss: 2.6123126886224233

Epoch: 6| Step: 1
Training loss: 2.542685031890869
Validation loss: 2.607256994452528

Epoch: 6| Step: 2
Training loss: 2.919160842895508
Validation loss: 2.612293084462484

Epoch: 6| Step: 3
Training loss: 2.394603967666626
Validation loss: 2.613462555793024

Epoch: 6| Step: 4
Training loss: 3.0496737957000732
Validation loss: 2.6138944356672225

Epoch: 6| Step: 5
Training loss: 2.4627318382263184
Validation loss: 2.613635111880559

Epoch: 6| Step: 6
Training loss: 2.5470213890075684
Validation loss: 2.6161408065467753

Epoch: 6| Step: 7
Training loss: 2.8802928924560547
Validation loss: 2.615118497161455

Epoch: 6| Step: 8
Training loss: 2.549468994140625
Validation loss: 2.6073078391372517

Epoch: 6| Step: 9
Training loss: 2.4413719177246094
Validation loss: 2.6023135313423733

Epoch: 6| Step: 10
Training loss: 3.0414702892303467
Validation loss: 2.601170893638365

Epoch: 6| Step: 11
Training loss: 3.1647605895996094
Validation loss: 2.602043887620331

Epoch: 6| Step: 12
Training loss: 3.081773042678833
Validation loss: 2.59875706062522

Epoch: 6| Step: 13
Training loss: 2.9104952812194824
Validation loss: 2.601759005618352

Epoch: 84| Step: 0
Training loss: 2.237725257873535
Validation loss: 2.6015771537698726

Epoch: 6| Step: 1
Training loss: 2.7475638389587402
Validation loss: 2.599460158296811

Epoch: 6| Step: 2
Training loss: 2.171691417694092
Validation loss: 2.5987704723112044

Epoch: 6| Step: 3
Training loss: 2.5512094497680664
Validation loss: 2.6052010648994037

Epoch: 6| Step: 4
Training loss: 2.759946823120117
Validation loss: 2.6129768586927846

Epoch: 6| Step: 5
Training loss: 2.4561758041381836
Validation loss: 2.606502584231797

Epoch: 6| Step: 6
Training loss: 2.8408584594726562
Validation loss: 2.605628469938873

Epoch: 6| Step: 7
Training loss: 2.05783748626709
Validation loss: 2.603568095032887

Epoch: 6| Step: 8
Training loss: 2.244696855545044
Validation loss: 2.5995752785795476

Epoch: 6| Step: 9
Training loss: 3.9661099910736084
Validation loss: 2.5973511460006877

Epoch: 6| Step: 10
Training loss: 2.7657952308654785
Validation loss: 2.59663978956079

Epoch: 6| Step: 11
Training loss: 4.128532409667969
Validation loss: 2.593270283873363

Epoch: 6| Step: 12
Training loss: 3.294044017791748
Validation loss: 2.5962544846278366

Epoch: 6| Step: 13
Training loss: 2.610804557800293
Validation loss: 2.596376757467947

Epoch: 85| Step: 0
Training loss: 3.395381450653076
Validation loss: 2.6059730437494095

Epoch: 6| Step: 1
Training loss: 2.2842578887939453
Validation loss: 2.6156399198757705

Epoch: 6| Step: 2
Training loss: 2.7341866493225098
Validation loss: 2.6254533208826536

Epoch: 6| Step: 3
Training loss: 2.7738518714904785
Validation loss: 2.6170042509673745

Epoch: 6| Step: 4
Training loss: 3.2065277099609375
Validation loss: 2.614354641206803

Epoch: 6| Step: 5
Training loss: 2.9890987873077393
Validation loss: 2.607901409108152

Epoch: 6| Step: 6
Training loss: 2.1670827865600586
Validation loss: 2.6063487683573077

Epoch: 6| Step: 7
Training loss: 2.031022548675537
Validation loss: 2.6039235258615143

Epoch: 6| Step: 8
Training loss: 3.272724151611328
Validation loss: 2.5950594820002073

Epoch: 6| Step: 9
Training loss: 2.138853073120117
Validation loss: 2.5975163393123175

Epoch: 6| Step: 10
Training loss: 2.964991569519043
Validation loss: 2.5923489652654177

Epoch: 6| Step: 11
Training loss: 2.9203925132751465
Validation loss: 2.5970639362130115

Epoch: 6| Step: 12
Training loss: 3.0492773056030273
Validation loss: 2.6001851840685775

Epoch: 6| Step: 13
Training loss: 3.0239601135253906
Validation loss: 2.5983957987959667

Epoch: 86| Step: 0
Training loss: 2.575803756713867
Validation loss: 2.608718051705309

Epoch: 6| Step: 1
Training loss: 2.7751240730285645
Validation loss: 2.6166266677200154

Epoch: 6| Step: 2
Training loss: 2.04978609085083
Validation loss: 2.6287152972272647

Epoch: 6| Step: 3
Training loss: 2.828456401824951
Validation loss: 2.627981621731994

Epoch: 6| Step: 4
Training loss: 3.6549572944641113
Validation loss: 2.6277820705085673

Epoch: 6| Step: 5
Training loss: 2.158620834350586
Validation loss: 2.63151039103026

Epoch: 6| Step: 6
Training loss: 2.9628372192382812
Validation loss: 2.629856183964719

Epoch: 6| Step: 7
Training loss: 2.772413492202759
Validation loss: 2.6181507341323362

Epoch: 6| Step: 8
Training loss: 2.845554828643799
Validation loss: 2.604761779949229

Epoch: 6| Step: 9
Training loss: 3.0966997146606445
Validation loss: 2.590600877679804

Epoch: 6| Step: 10
Training loss: 2.786379814147949
Validation loss: 2.5854344073162285

Epoch: 6| Step: 11
Training loss: 3.1212987899780273
Validation loss: 2.5970609239352647

Epoch: 6| Step: 12
Training loss: 2.441286563873291
Validation loss: 2.597316598379484

Epoch: 6| Step: 13
Training loss: 2.742790699005127
Validation loss: 2.5993389109129548

Epoch: 87| Step: 0
Training loss: 1.7393488883972168
Validation loss: 2.6038895704412974

Epoch: 6| Step: 1
Training loss: 2.689432382583618
Validation loss: 2.602425326583206

Epoch: 6| Step: 2
Training loss: 3.994328498840332
Validation loss: 2.6083263274162047

Epoch: 6| Step: 3
Training loss: 3.9953179359436035
Validation loss: 2.603694700425671

Epoch: 6| Step: 4
Training loss: 3.1257076263427734
Validation loss: 2.6031401106106338

Epoch: 6| Step: 5
Training loss: 2.603625774383545
Validation loss: 2.6014017853685605

Epoch: 6| Step: 6
Training loss: 1.8434113264083862
Validation loss: 2.6023228976034347

Epoch: 6| Step: 7
Training loss: 2.7659993171691895
Validation loss: 2.596957750217889

Epoch: 6| Step: 8
Training loss: 2.4585113525390625
Validation loss: 2.597572229241812

Epoch: 6| Step: 9
Training loss: 2.0450048446655273
Validation loss: 2.5902977784474692

Epoch: 6| Step: 10
Training loss: 3.3540923595428467
Validation loss: 2.5896150271097818

Epoch: 6| Step: 11
Training loss: 2.6667046546936035
Validation loss: 2.588247947795417

Epoch: 6| Step: 12
Training loss: 2.971170425415039
Validation loss: 2.58882354023636

Epoch: 6| Step: 13
Training loss: 2.3152294158935547
Validation loss: 2.5857061929600214

Epoch: 88| Step: 0
Training loss: 2.9016799926757812
Validation loss: 2.5847021482324086

Epoch: 6| Step: 1
Training loss: 2.546538829803467
Validation loss: 2.587640082964333

Epoch: 6| Step: 2
Training loss: 2.5737650394439697
Validation loss: 2.5791470055939048

Epoch: 6| Step: 3
Training loss: 3.100494384765625
Validation loss: 2.5865591777268278

Epoch: 6| Step: 4
Training loss: 2.9449386596679688
Validation loss: 2.5856883064393075

Epoch: 6| Step: 5
Training loss: 2.8198227882385254
Validation loss: 2.588075327616866

Epoch: 6| Step: 6
Training loss: 3.4945807456970215
Validation loss: 2.582122416906459

Epoch: 6| Step: 7
Training loss: 2.837204933166504
Validation loss: 2.585921379827684

Epoch: 6| Step: 8
Training loss: 2.2092525959014893
Validation loss: 2.5869737440539944

Epoch: 6| Step: 9
Training loss: 1.9333858489990234
Validation loss: 2.5865946764587076

Epoch: 6| Step: 10
Training loss: 2.9418892860412598
Validation loss: 2.5857512258714244

Epoch: 6| Step: 11
Training loss: 2.460706949234009
Validation loss: 2.5856669487491732

Epoch: 6| Step: 12
Training loss: 3.145144462585449
Validation loss: 2.585630001560334

Epoch: 6| Step: 13
Training loss: 2.8771581649780273
Validation loss: 2.5807022920218845

Epoch: 89| Step: 0
Training loss: 2.5000967979431152
Validation loss: 2.5832561100682905

Epoch: 6| Step: 1
Training loss: 2.893113613128662
Validation loss: 2.588352046987062

Epoch: 6| Step: 2
Training loss: 2.5569729804992676
Validation loss: 2.58187674706982

Epoch: 6| Step: 3
Training loss: 3.1910810470581055
Validation loss: 2.582597904307868

Epoch: 6| Step: 4
Training loss: 3.2593817710876465
Validation loss: 2.585743675949753

Epoch: 6| Step: 5
Training loss: 2.3518428802490234
Validation loss: 2.583912459752893

Epoch: 6| Step: 6
Training loss: 3.0702741146087646
Validation loss: 2.5837502171916347

Epoch: 6| Step: 7
Training loss: 2.9016361236572266
Validation loss: 2.5782364260765815

Epoch: 6| Step: 8
Training loss: 2.3282573223114014
Validation loss: 2.584460581502607

Epoch: 6| Step: 9
Training loss: 3.0502665042877197
Validation loss: 2.5802135185528825

Epoch: 6| Step: 10
Training loss: 2.925820827484131
Validation loss: 2.582080110426872

Epoch: 6| Step: 11
Training loss: 1.7831186056137085
Validation loss: 2.5861231409093386

Epoch: 6| Step: 12
Training loss: 2.9673104286193848
Validation loss: 2.583773115629791

Epoch: 6| Step: 13
Training loss: 2.982478141784668
Validation loss: 2.5799750743373746

Epoch: 90| Step: 0
Training loss: 2.642199754714966
Validation loss: 2.586766258362801

Epoch: 6| Step: 1
Training loss: 2.8304734230041504
Validation loss: 2.5919709615809943

Epoch: 6| Step: 2
Training loss: 2.4985668659210205
Validation loss: 2.5992580152327016

Epoch: 6| Step: 3
Training loss: 2.868793249130249
Validation loss: 2.5981818040211997

Epoch: 6| Step: 4
Training loss: 3.35563325881958
Validation loss: 2.60248541062878

Epoch: 6| Step: 5
Training loss: 3.258950710296631
Validation loss: 2.5954134823173605

Epoch: 6| Step: 6
Training loss: 2.8754231929779053
Validation loss: 2.5953168458836053

Epoch: 6| Step: 7
Training loss: 1.713071346282959
Validation loss: 2.591249768451978

Epoch: 6| Step: 8
Training loss: 2.3283400535583496
Validation loss: 2.5874828651387203

Epoch: 6| Step: 9
Training loss: 2.429718494415283
Validation loss: 2.580305912161386

Epoch: 6| Step: 10
Training loss: 2.9942970275878906
Validation loss: 2.575976751183951

Epoch: 6| Step: 11
Training loss: 3.414130687713623
Validation loss: 2.5833261448849916

Epoch: 6| Step: 12
Training loss: 3.039769172668457
Validation loss: 2.5795192590323825

Epoch: 6| Step: 13
Training loss: 2.1807665824890137
Validation loss: 2.583742641633557

Epoch: 91| Step: 0
Training loss: 2.18190598487854
Validation loss: 2.57947342113782

Epoch: 6| Step: 1
Training loss: 2.91243839263916
Validation loss: 2.578928247574837

Epoch: 6| Step: 2
Training loss: 2.4393486976623535
Validation loss: 2.572232643763224

Epoch: 6| Step: 3
Training loss: 3.601245641708374
Validation loss: 2.574360793636691

Epoch: 6| Step: 4
Training loss: 3.490138530731201
Validation loss: 2.5759217354559127

Epoch: 6| Step: 5
Training loss: 2.2293014526367188
Validation loss: 2.5782096334683

Epoch: 6| Step: 6
Training loss: 2.462470293045044
Validation loss: 2.5767361707584833

Epoch: 6| Step: 7
Training loss: 3.351850748062134
Validation loss: 2.5798592746898694

Epoch: 6| Step: 8
Training loss: 2.308526039123535
Validation loss: 2.5868264923813524

Epoch: 6| Step: 9
Training loss: 3.4436633586883545
Validation loss: 2.5898719782470376

Epoch: 6| Step: 10
Training loss: 2.5615365505218506
Validation loss: 2.5895074952033257

Epoch: 6| Step: 11
Training loss: 2.816427230834961
Validation loss: 2.5846278641813543

Epoch: 6| Step: 12
Training loss: 2.802215576171875
Validation loss: 2.5777442660383

Epoch: 6| Step: 13
Training loss: 1.5627857446670532
Validation loss: 2.5802196097630326

Epoch: 92| Step: 0
Training loss: 2.3924779891967773
Validation loss: 2.5730402136361725

Epoch: 6| Step: 1
Training loss: 3.236501693725586
Validation loss: 2.5746181831564954

Epoch: 6| Step: 2
Training loss: 2.9334630966186523
Validation loss: 2.57334743520265

Epoch: 6| Step: 3
Training loss: 2.6395883560180664
Validation loss: 2.565143226295389

Epoch: 6| Step: 4
Training loss: 2.603682041168213
Validation loss: 2.5738532825182845

Epoch: 6| Step: 5
Training loss: 2.6152658462524414
Validation loss: 2.5657267929405294

Epoch: 6| Step: 6
Training loss: 2.4091238975524902
Validation loss: 2.578862064628191

Epoch: 6| Step: 7
Training loss: 3.103214979171753
Validation loss: 2.5786005194469164

Epoch: 6| Step: 8
Training loss: 2.6769771575927734
Validation loss: 2.5884580407091367

Epoch: 6| Step: 9
Training loss: 3.0724802017211914
Validation loss: 2.5908139956894742

Epoch: 6| Step: 10
Training loss: 2.881932497024536
Validation loss: 2.5827545119870092

Epoch: 6| Step: 11
Training loss: 2.2638375759124756
Validation loss: 2.5817926186387257

Epoch: 6| Step: 12
Training loss: 3.0166053771972656
Validation loss: 2.575140250626431

Epoch: 6| Step: 13
Training loss: 2.434852123260498
Validation loss: 2.5738422870635986

Epoch: 93| Step: 0
Training loss: 3.074263095855713
Validation loss: 2.566873865742837

Epoch: 6| Step: 1
Training loss: 2.9723565578460693
Validation loss: 2.5610876647374963

Epoch: 6| Step: 2
Training loss: 2.9472923278808594
Validation loss: 2.560242191437752

Epoch: 6| Step: 3
Training loss: 2.4476590156555176
Validation loss: 2.557662561375608

Epoch: 6| Step: 4
Training loss: 3.000213623046875
Validation loss: 2.5569791229822303

Epoch: 6| Step: 5
Training loss: 3.1061761379241943
Validation loss: 2.5576178925011748

Epoch: 6| Step: 6
Training loss: 3.0570783615112305
Validation loss: 2.560916862180156

Epoch: 6| Step: 7
Training loss: 3.2718396186828613
Validation loss: 2.5599155554207425

Epoch: 6| Step: 8
Training loss: 2.919612407684326
Validation loss: 2.573555436185611

Epoch: 6| Step: 9
Training loss: 2.0515694618225098
Validation loss: 2.583933089369087

Epoch: 6| Step: 10
Training loss: 1.5939898490905762
Validation loss: 2.5747842506695817

Epoch: 6| Step: 11
Training loss: 2.2684895992279053
Validation loss: 2.584889704181302

Epoch: 6| Step: 12
Training loss: 2.9092907905578613
Validation loss: 2.5752586369873374

Epoch: 6| Step: 13
Training loss: 2.6544251441955566
Validation loss: 2.565743064367643

Epoch: 94| Step: 0
Training loss: 2.027867555618286
Validation loss: 2.5609949634921167

Epoch: 6| Step: 1
Training loss: 3.8056752681732178
Validation loss: 2.5594135022932485

Epoch: 6| Step: 2
Training loss: 2.409634590148926
Validation loss: 2.552497956060594

Epoch: 6| Step: 3
Training loss: 2.8383052349090576
Validation loss: 2.559923610379619

Epoch: 6| Step: 4
Training loss: 2.9309144020080566
Validation loss: 2.5737182017295592

Epoch: 6| Step: 5
Training loss: 2.519482135772705
Validation loss: 2.5953510756133706

Epoch: 6| Step: 6
Training loss: 3.2998759746551514
Validation loss: 2.578348398208618

Epoch: 6| Step: 7
Training loss: 2.929502010345459
Validation loss: 2.5584691698833177

Epoch: 6| Step: 8
Training loss: 2.613377809524536
Validation loss: 2.553143778154927

Epoch: 6| Step: 9
Training loss: 3.086945056915283
Validation loss: 2.554563894066759

Epoch: 6| Step: 10
Training loss: 2.016981601715088
Validation loss: 2.5577648993461364

Epoch: 6| Step: 11
Training loss: 2.221987247467041
Validation loss: 2.5553357242256083

Epoch: 6| Step: 12
Training loss: 2.5712552070617676
Validation loss: 2.5605192440812305

Epoch: 6| Step: 13
Training loss: 3.560173511505127
Validation loss: 2.561404025682839

Epoch: 95| Step: 0
Training loss: 2.631821393966675
Validation loss: 2.563558109344975

Epoch: 6| Step: 1
Training loss: 2.095843553543091
Validation loss: 2.5657789937911497

Epoch: 6| Step: 2
Training loss: 2.8458166122436523
Validation loss: 2.5740296251030377

Epoch: 6| Step: 3
Training loss: 2.930341958999634
Validation loss: 2.575663835771622

Epoch: 6| Step: 4
Training loss: 2.0622622966766357
Validation loss: 2.5791729829644643

Epoch: 6| Step: 5
Training loss: 2.1389570236206055
Validation loss: 2.5683691757981495

Epoch: 6| Step: 6
Training loss: 3.0990352630615234
Validation loss: 2.5640192852225354

Epoch: 6| Step: 7
Training loss: 2.536505699157715
Validation loss: 2.5684416986280874

Epoch: 6| Step: 8
Training loss: 3.5726237297058105
Validation loss: 2.564194658751129

Epoch: 6| Step: 9
Training loss: 2.748732089996338
Validation loss: 2.568211599062848

Epoch: 6| Step: 10
Training loss: 3.16701340675354
Validation loss: 2.562997769283992

Epoch: 6| Step: 11
Training loss: 3.2404589653015137
Validation loss: 2.550419804870441

Epoch: 6| Step: 12
Training loss: 2.2549092769622803
Validation loss: 2.549842983163813

Epoch: 6| Step: 13
Training loss: 3.241147041320801
Validation loss: 2.547521188694944

Epoch: 96| Step: 0
Training loss: 2.025015115737915
Validation loss: 2.548290432140391

Epoch: 6| Step: 1
Training loss: 2.1000213623046875
Validation loss: 2.5501180130948304

Epoch: 6| Step: 2
Training loss: 2.4862918853759766
Validation loss: 2.544566333934825

Epoch: 6| Step: 3
Training loss: 3.1214044094085693
Validation loss: 2.545881612326509

Epoch: 6| Step: 4
Training loss: 2.527838706970215
Validation loss: 2.548899583919074

Epoch: 6| Step: 5
Training loss: 3.4970319271087646
Validation loss: 2.5454401918636855

Epoch: 6| Step: 6
Training loss: 2.976970672607422
Validation loss: 2.5501422343715543

Epoch: 6| Step: 7
Training loss: 2.745675563812256
Validation loss: 2.548137905777142

Epoch: 6| Step: 8
Training loss: 3.2175049781799316
Validation loss: 2.5447441890675533

Epoch: 6| Step: 9
Training loss: 2.9620275497436523
Validation loss: 2.5541215404387443

Epoch: 6| Step: 10
Training loss: 2.4556405544281006
Validation loss: 2.548844496409098

Epoch: 6| Step: 11
Training loss: 2.756577968597412
Validation loss: 2.548452385010258

Epoch: 6| Step: 12
Training loss: 2.844313859939575
Validation loss: 2.5469138417192685

Epoch: 6| Step: 13
Training loss: 2.537257671356201
Validation loss: 2.5415863478055565

Epoch: 97| Step: 0
Training loss: 2.9945762157440186
Validation loss: 2.546891709809662

Epoch: 6| Step: 1
Training loss: 1.9961276054382324
Validation loss: 2.543620909413984

Epoch: 6| Step: 2
Training loss: 3.0807175636291504
Validation loss: 2.544307372903311

Epoch: 6| Step: 3
Training loss: 2.8776111602783203
Validation loss: 2.5423045440386702

Epoch: 6| Step: 4
Training loss: 2.682786464691162
Validation loss: 2.54081904503607

Epoch: 6| Step: 5
Training loss: 2.9733870029449463
Validation loss: 2.5418840582652757

Epoch: 6| Step: 6
Training loss: 2.119378089904785
Validation loss: 2.54335287309462

Epoch: 6| Step: 7
Training loss: 3.362818717956543
Validation loss: 2.5471598153473227

Epoch: 6| Step: 8
Training loss: 2.6759426593780518
Validation loss: 2.5451901805016304

Epoch: 6| Step: 9
Training loss: 2.630289316177368
Validation loss: 2.5511594921030025

Epoch: 6| Step: 10
Training loss: 2.451061248779297
Validation loss: 2.5473618302294003

Epoch: 6| Step: 11
Training loss: 2.057239532470703
Validation loss: 2.5564211004523822

Epoch: 6| Step: 12
Training loss: 3.3853750228881836
Validation loss: 2.567645057555168

Epoch: 6| Step: 13
Training loss: 3.008439540863037
Validation loss: 2.565897698043495

Epoch: 98| Step: 0
Training loss: 3.7614216804504395
Validation loss: 2.572702543709868

Epoch: 6| Step: 1
Training loss: 2.3233187198638916
Validation loss: 2.5681297625264814

Epoch: 6| Step: 2
Training loss: 2.927560806274414
Validation loss: 2.5731322534622683

Epoch: 6| Step: 3
Training loss: 3.456460475921631
Validation loss: 2.562228451492966

Epoch: 6| Step: 4
Training loss: 2.2106571197509766
Validation loss: 2.5602983120949037

Epoch: 6| Step: 5
Training loss: 3.6856935024261475
Validation loss: 2.5561302708041285

Epoch: 6| Step: 6
Training loss: 1.8813706636428833
Validation loss: 2.5640206695884786

Epoch: 6| Step: 7
Training loss: 2.8122220039367676
Validation loss: 2.573493149972731

Epoch: 6| Step: 8
Training loss: 2.556751251220703
Validation loss: 2.585217491272957

Epoch: 6| Step: 9
Training loss: 2.257080078125
Validation loss: 2.586189172601187

Epoch: 6| Step: 10
Training loss: 1.9449176788330078
Validation loss: 2.5833616256713867

Epoch: 6| Step: 11
Training loss: 3.1970763206481934
Validation loss: 2.5747964638535694

Epoch: 6| Step: 12
Training loss: 2.8641531467437744
Validation loss: 2.5552948123665264

Epoch: 6| Step: 13
Training loss: 2.1615450382232666
Validation loss: 2.5507420647528862

Epoch: 99| Step: 0
Training loss: 3.3736624717712402
Validation loss: 2.5494025138116654

Epoch: 6| Step: 1
Training loss: 2.081320285797119
Validation loss: 2.554039891048144

Epoch: 6| Step: 2
Training loss: 2.7997779846191406
Validation loss: 2.5490547200684905

Epoch: 6| Step: 3
Training loss: 2.2301573753356934
Validation loss: 2.544008134513773

Epoch: 6| Step: 4
Training loss: 2.957440137863159
Validation loss: 2.543719159659519

Epoch: 6| Step: 5
Training loss: 2.476797580718994
Validation loss: 2.534337123235067

Epoch: 6| Step: 6
Training loss: 3.1874146461486816
Validation loss: 2.5345630838025

Epoch: 6| Step: 7
Training loss: 2.1318066120147705
Validation loss: 2.529543653611214

Epoch: 6| Step: 8
Training loss: 3.009652853012085
Validation loss: 2.5332291638979347

Epoch: 6| Step: 9
Training loss: 3.2428603172302246
Validation loss: 2.536905498914821

Epoch: 6| Step: 10
Training loss: 3.1012377738952637
Validation loss: 2.531706151141915

Epoch: 6| Step: 11
Training loss: 2.03399658203125
Validation loss: 2.534225647167493

Epoch: 6| Step: 12
Training loss: 2.532949924468994
Validation loss: 2.5361753612436275

Epoch: 6| Step: 13
Training loss: 3.352020263671875
Validation loss: 2.537819118909938

Epoch: 100| Step: 0
Training loss: 3.172201633453369
Validation loss: 2.5360999414997716

Epoch: 6| Step: 1
Training loss: 2.5765116214752197
Validation loss: 2.5363607380979802

Epoch: 6| Step: 2
Training loss: 2.458425521850586
Validation loss: 2.5352262553348335

Epoch: 6| Step: 3
Training loss: 3.0580406188964844
Validation loss: 2.535272749521399

Epoch: 6| Step: 4
Training loss: 2.31837797164917
Validation loss: 2.5325159795822634

Epoch: 6| Step: 5
Training loss: 2.5630831718444824
Validation loss: 2.534304080470916

Epoch: 6| Step: 6
Training loss: 3.438962459564209
Validation loss: 2.5360301181834233

Epoch: 6| Step: 7
Training loss: 2.5677871704101562
Validation loss: 2.5337563330127346

Epoch: 6| Step: 8
Training loss: 2.7628560066223145
Validation loss: 2.5334959055787776

Epoch: 6| Step: 9
Training loss: 3.168236017227173
Validation loss: 2.5350041286919707

Epoch: 6| Step: 10
Training loss: 2.038645029067993
Validation loss: 2.537049578082177

Epoch: 6| Step: 11
Training loss: 2.368027448654175
Validation loss: 2.5390442468786754

Epoch: 6| Step: 12
Training loss: 2.475593090057373
Validation loss: 2.537583328062488

Epoch: 6| Step: 13
Training loss: 3.5762104988098145
Validation loss: 2.5401646603820143

Epoch: 101| Step: 0
Training loss: 2.2651119232177734
Validation loss: 2.5337102079904206

Epoch: 6| Step: 1
Training loss: 2.3914496898651123
Validation loss: 2.53330300956644

Epoch: 6| Step: 2
Training loss: 3.2418136596679688
Validation loss: 2.538809414832823

Epoch: 6| Step: 3
Training loss: 2.8530282974243164
Validation loss: 2.539955662142846

Epoch: 6| Step: 4
Training loss: 2.7717037200927734
Validation loss: 2.536437255080028

Epoch: 6| Step: 5
Training loss: 2.3214616775512695
Validation loss: 2.5308647258307344

Epoch: 6| Step: 6
Training loss: 1.6512150764465332
Validation loss: 2.536966554580196

Epoch: 6| Step: 7
Training loss: 2.7003636360168457
Validation loss: 2.5452110818637315

Epoch: 6| Step: 8
Training loss: 2.986292600631714
Validation loss: 2.541849636262463

Epoch: 6| Step: 9
Training loss: 3.8033971786499023
Validation loss: 2.5524765727340535

Epoch: 6| Step: 10
Training loss: 2.5202536582946777
Validation loss: 2.5487500518880863

Epoch: 6| Step: 11
Training loss: 3.2321534156799316
Validation loss: 2.550418958869032

Epoch: 6| Step: 12
Training loss: 2.944375991821289
Validation loss: 2.544365261190681

Epoch: 6| Step: 13
Training loss: 2.1882059574127197
Validation loss: 2.5370093648151686

Epoch: 102| Step: 0
Training loss: 2.166560649871826
Validation loss: 2.5289759917925765

Epoch: 6| Step: 1
Training loss: 2.5839102268218994
Validation loss: 2.5323987084050334

Epoch: 6| Step: 2
Training loss: 2.8003182411193848
Validation loss: 2.5307107792105725

Epoch: 6| Step: 3
Training loss: 2.623622417449951
Validation loss: 2.5305480059757026

Epoch: 6| Step: 4
Training loss: 2.597512722015381
Validation loss: 2.5324721515819593

Epoch: 6| Step: 5
Training loss: 2.502624988555908
Validation loss: 2.530058253195978

Epoch: 6| Step: 6
Training loss: 3.496126651763916
Validation loss: 2.5282784610666256

Epoch: 6| Step: 7
Training loss: 2.739781379699707
Validation loss: 2.531279576722012

Epoch: 6| Step: 8
Training loss: 2.352081537246704
Validation loss: 2.5307207261362383

Epoch: 6| Step: 9
Training loss: 2.0810546875
Validation loss: 2.528748784013974

Epoch: 6| Step: 10
Training loss: 2.9846625328063965
Validation loss: 2.5318969475325717

Epoch: 6| Step: 11
Training loss: 3.198794364929199
Validation loss: 2.5328541032729612

Epoch: 6| Step: 12
Training loss: 2.803173065185547
Validation loss: 2.54099033212149

Epoch: 6| Step: 13
Training loss: 3.4521684646606445
Validation loss: 2.5414986712958223

Epoch: 103| Step: 0
Training loss: 2.482478141784668
Validation loss: 2.546054196614091

Epoch: 6| Step: 1
Training loss: 2.852855682373047
Validation loss: 2.544415545719926

Epoch: 6| Step: 2
Training loss: 2.884610891342163
Validation loss: 2.552114309803132

Epoch: 6| Step: 3
Training loss: 2.231846332550049
Validation loss: 2.547127849312239

Epoch: 6| Step: 4
Training loss: 2.2332892417907715
Validation loss: 2.548746955010199

Epoch: 6| Step: 5
Training loss: 3.384467601776123
Validation loss: 2.552437884833223

Epoch: 6| Step: 6
Training loss: 2.212393045425415
Validation loss: 2.5483270742559947

Epoch: 6| Step: 7
Training loss: 2.824273109436035
Validation loss: 2.5412971358145438

Epoch: 6| Step: 8
Training loss: 3.259823799133301
Validation loss: 2.5413272944829797

Epoch: 6| Step: 9
Training loss: 2.5753090381622314
Validation loss: 2.5380562531050814

Epoch: 6| Step: 10
Training loss: 2.9162211418151855
Validation loss: 2.529518295359868

Epoch: 6| Step: 11
Training loss: 2.4922542572021484
Validation loss: 2.5258421513342086

Epoch: 6| Step: 12
Training loss: 3.330869436264038
Validation loss: 2.5229653773769254

Epoch: 6| Step: 13
Training loss: 2.0763635635375977
Validation loss: 2.52662032393999

Epoch: 104| Step: 0
Training loss: 2.326195478439331
Validation loss: 2.529206116994222

Epoch: 6| Step: 1
Training loss: 2.026491641998291
Validation loss: 2.5268416917452248

Epoch: 6| Step: 2
Training loss: 3.007417678833008
Validation loss: 2.5418057954439552

Epoch: 6| Step: 3
Training loss: 2.7340431213378906
Validation loss: 2.5520397386243268

Epoch: 6| Step: 4
Training loss: 3.2433581352233887
Validation loss: 2.56276878874789

Epoch: 6| Step: 5
Training loss: 2.5784616470336914
Validation loss: 2.5611292559613466

Epoch: 6| Step: 6
Training loss: 2.6623148918151855
Validation loss: 2.5467190921947522

Epoch: 6| Step: 7
Training loss: 3.44974946975708
Validation loss: 2.549149021025627

Epoch: 6| Step: 8
Training loss: 2.3590707778930664
Validation loss: 2.5327855233223207

Epoch: 6| Step: 9
Training loss: 3.286820888519287
Validation loss: 2.527896001774778

Epoch: 6| Step: 10
Training loss: 2.7624120712280273
Validation loss: 2.5272147834941907

Epoch: 6| Step: 11
Training loss: 2.9211981296539307
Validation loss: 2.521660504802581

Epoch: 6| Step: 12
Training loss: 1.6669384241104126
Validation loss: 2.5174167207492295

Epoch: 6| Step: 13
Training loss: 3.285741090774536
Validation loss: 2.522292362746372

Epoch: 105| Step: 0
Training loss: 2.3308701515197754
Validation loss: 2.5220408978000766

Epoch: 6| Step: 1
Training loss: 2.506563186645508
Validation loss: 2.523463258179285

Epoch: 6| Step: 2
Training loss: 2.856311321258545
Validation loss: 2.5236241843110774

Epoch: 6| Step: 3
Training loss: 3.0737485885620117
Validation loss: 2.529090230182935

Epoch: 6| Step: 4
Training loss: 3.5059943199157715
Validation loss: 2.5307690379440144

Epoch: 6| Step: 5
Training loss: 2.7325730323791504
Validation loss: 2.5283556035769883

Epoch: 6| Step: 6
Training loss: 2.578524589538574
Validation loss: 2.533788732303086

Epoch: 6| Step: 7
Training loss: 2.8153529167175293
Validation loss: 2.5249299105777534

Epoch: 6| Step: 8
Training loss: 2.3955564498901367
Validation loss: 2.529435198794129

Epoch: 6| Step: 9
Training loss: 3.4564454555511475
Validation loss: 2.5350500665685183

Epoch: 6| Step: 10
Training loss: 3.1176586151123047
Validation loss: 2.528340034587409

Epoch: 6| Step: 11
Training loss: 2.453162431716919
Validation loss: 2.527201052634947

Epoch: 6| Step: 12
Training loss: 1.9462436437606812
Validation loss: 2.523649815590151

Epoch: 6| Step: 13
Training loss: 1.9574909210205078
Validation loss: 2.5241187310987905

Epoch: 106| Step: 0
Training loss: 3.056056499481201
Validation loss: 2.5214543111862673

Epoch: 6| Step: 1
Training loss: 2.7856051921844482
Validation loss: 2.5219294896689792

Epoch: 6| Step: 2
Training loss: 2.963007926940918
Validation loss: 2.5203638230600665

Epoch: 6| Step: 3
Training loss: 3.0277862548828125
Validation loss: 2.5153341831699496

Epoch: 6| Step: 4
Training loss: 3.0010080337524414
Validation loss: 2.521078714760401

Epoch: 6| Step: 5
Training loss: 2.490739345550537
Validation loss: 2.5226236825348227

Epoch: 6| Step: 6
Training loss: 2.617422103881836
Validation loss: 2.5193269304049912

Epoch: 6| Step: 7
Training loss: 2.663167715072632
Validation loss: 2.5213165283203125

Epoch: 6| Step: 8
Training loss: 2.6156418323516846
Validation loss: 2.522266945531291

Epoch: 6| Step: 9
Training loss: 3.092853546142578
Validation loss: 2.523729112840468

Epoch: 6| Step: 10
Training loss: 1.997734785079956
Validation loss: 2.5260705332602225

Epoch: 6| Step: 11
Training loss: 2.59468936920166
Validation loss: 2.5278572100465015

Epoch: 6| Step: 12
Training loss: 2.227949619293213
Validation loss: 2.5263761038421304

Epoch: 6| Step: 13
Training loss: 2.9146952629089355
Validation loss: 2.527787272648145

Epoch: 107| Step: 0
Training loss: 1.7817074060440063
Validation loss: 2.526772242720409

Epoch: 6| Step: 1
Training loss: 3.1350414752960205
Validation loss: 2.5252679368501068

Epoch: 6| Step: 2
Training loss: 3.1242127418518066
Validation loss: 2.5213194867616058

Epoch: 6| Step: 3
Training loss: 2.3322246074676514
Validation loss: 2.523531324119978

Epoch: 6| Step: 4
Training loss: 3.0558316707611084
Validation loss: 2.5289123699229252

Epoch: 6| Step: 5
Training loss: 2.6329522132873535
Validation loss: 2.5317013263702393

Epoch: 6| Step: 6
Training loss: 2.378011465072632
Validation loss: 2.537593521097655

Epoch: 6| Step: 7
Training loss: 2.738232135772705
Validation loss: 2.53919622975011

Epoch: 6| Step: 8
Training loss: 3.634923219680786
Validation loss: 2.5426340462059103

Epoch: 6| Step: 9
Training loss: 2.2655553817749023
Validation loss: 2.539523555386451

Epoch: 6| Step: 10
Training loss: 3.242264747619629
Validation loss: 2.5328850002699

Epoch: 6| Step: 11
Training loss: 2.5862019062042236
Validation loss: 2.533002394501881

Epoch: 6| Step: 12
Training loss: 1.8538155555725098
Validation loss: 2.5382826635914464

Epoch: 6| Step: 13
Training loss: 3.4258644580841064
Validation loss: 2.5476719153824674

Epoch: 108| Step: 0
Training loss: 3.5068836212158203
Validation loss: 2.5404200041165916

Epoch: 6| Step: 1
Training loss: 2.8379292488098145
Validation loss: 2.543385946622459

Epoch: 6| Step: 2
Training loss: 2.6701431274414062
Validation loss: 2.549590536343154

Epoch: 6| Step: 3
Training loss: 2.100674629211426
Validation loss: 2.551460048203827

Epoch: 6| Step: 4
Training loss: 3.123297691345215
Validation loss: 2.5504203381076938

Epoch: 6| Step: 5
Training loss: 2.730196952819824
Validation loss: 2.5530212002415813

Epoch: 6| Step: 6
Training loss: 2.5241947174072266
Validation loss: 2.5529809100653535

Epoch: 6| Step: 7
Training loss: 2.8816280364990234
Validation loss: 2.5504048947365052

Epoch: 6| Step: 8
Training loss: 2.1519293785095215
Validation loss: 2.540178550186978

Epoch: 6| Step: 9
Training loss: 2.859175682067871
Validation loss: 2.5302167887328775

Epoch: 6| Step: 10
Training loss: 2.3671622276306152
Validation loss: 2.5239313289683354

Epoch: 6| Step: 11
Training loss: 2.4124200344085693
Validation loss: 2.526862773843991

Epoch: 6| Step: 12
Training loss: 2.6804726123809814
Validation loss: 2.5278580111842

Epoch: 6| Step: 13
Training loss: 3.4357755184173584
Validation loss: 2.5369113440154702

Epoch: 109| Step: 0
Training loss: 2.9519104957580566
Validation loss: 2.5352495229372414

Epoch: 6| Step: 1
Training loss: 2.548464298248291
Validation loss: 2.535471152233821

Epoch: 6| Step: 2
Training loss: 3.100569009780884
Validation loss: 2.530988544546148

Epoch: 6| Step: 3
Training loss: 2.877833604812622
Validation loss: 2.535057403708017

Epoch: 6| Step: 4
Training loss: 2.626358985900879
Validation loss: 2.541809576813893

Epoch: 6| Step: 5
Training loss: 2.8367104530334473
Validation loss: 2.5330720050360567

Epoch: 6| Step: 6
Training loss: 3.270395278930664
Validation loss: 2.525594772831086

Epoch: 6| Step: 7
Training loss: 2.2661309242248535
Validation loss: 2.5260058705524733

Epoch: 6| Step: 8
Training loss: 2.5068469047546387
Validation loss: 2.524074057097076

Epoch: 6| Step: 9
Training loss: 2.190685987472534
Validation loss: 2.5223524929374777

Epoch: 6| Step: 10
Training loss: 1.8664264678955078
Validation loss: 2.5370842205580844

Epoch: 6| Step: 11
Training loss: 2.9990217685699463
Validation loss: 2.5439631862025105

Epoch: 6| Step: 12
Training loss: 3.137946128845215
Validation loss: 2.542655752551171

Epoch: 6| Step: 13
Training loss: 2.9334793090820312
Validation loss: 2.538756831999748

Epoch: 110| Step: 0
Training loss: 1.8114862442016602
Validation loss: 2.531224968612835

Epoch: 6| Step: 1
Training loss: 2.219273567199707
Validation loss: 2.5343036010701168

Epoch: 6| Step: 2
Training loss: 3.0933876037597656
Validation loss: 2.5352081355228218

Epoch: 6| Step: 3
Training loss: 2.7448372840881348
Validation loss: 2.5236012115273425

Epoch: 6| Step: 4
Training loss: 2.84065842628479
Validation loss: 2.5196571580825315

Epoch: 6| Step: 5
Training loss: 2.7604260444641113
Validation loss: 2.5356469667086037

Epoch: 6| Step: 6
Training loss: 3.133160352706909
Validation loss: 2.5231744089434223

Epoch: 6| Step: 7
Training loss: 2.3062639236450195
Validation loss: 2.525704937596475

Epoch: 6| Step: 8
Training loss: 2.313103437423706
Validation loss: 2.521852393304148

Epoch: 6| Step: 9
Training loss: 2.938192844390869
Validation loss: 2.5149245415964434

Epoch: 6| Step: 10
Training loss: 3.434922218322754
Validation loss: 2.5274419374363397

Epoch: 6| Step: 11
Training loss: 2.762484312057495
Validation loss: 2.5247041974016415

Epoch: 6| Step: 12
Training loss: 2.995969772338867
Validation loss: 2.5144740625094344

Epoch: 6| Step: 13
Training loss: 2.4769623279571533
Validation loss: 2.512871306429627

Epoch: 111| Step: 0
Training loss: 2.4930732250213623
Validation loss: 2.5165665636780443

Epoch: 6| Step: 1
Training loss: 2.806668758392334
Validation loss: 2.5086899547166723

Epoch: 6| Step: 2
Training loss: 2.726677417755127
Validation loss: 2.5117262794125463

Epoch: 6| Step: 3
Training loss: 2.2179791927337646
Validation loss: 2.511046496770715

Epoch: 6| Step: 4
Training loss: 2.981825351715088
Validation loss: 2.5064204405712824

Epoch: 6| Step: 5
Training loss: 2.8542189598083496
Validation loss: 2.508370735312021

Epoch: 6| Step: 6
Training loss: 2.5844502449035645
Validation loss: 2.509649843297979

Epoch: 6| Step: 7
Training loss: 3.2847518920898438
Validation loss: 2.513721889065158

Epoch: 6| Step: 8
Training loss: 2.2977991104125977
Validation loss: 2.5187827720437

Epoch: 6| Step: 9
Training loss: 2.999307632446289
Validation loss: 2.5129262888303368

Epoch: 6| Step: 10
Training loss: 2.768097400665283
Validation loss: 2.5163588767410605

Epoch: 6| Step: 11
Training loss: 3.0723624229431152
Validation loss: 2.5228578172704226

Epoch: 6| Step: 12
Training loss: 2.4254343509674072
Validation loss: 2.522570994592482

Epoch: 6| Step: 13
Training loss: 1.9469544887542725
Validation loss: 2.5243170697201966

Epoch: 112| Step: 0
Training loss: 2.200188159942627
Validation loss: 2.5327374499331237

Epoch: 6| Step: 1
Training loss: 3.7380595207214355
Validation loss: 2.53217940433051

Epoch: 6| Step: 2
Training loss: 2.3703975677490234
Validation loss: 2.5273729575577604

Epoch: 6| Step: 3
Training loss: 2.4260659217834473
Validation loss: 2.5288137261585524

Epoch: 6| Step: 4
Training loss: 3.1919846534729004
Validation loss: 2.516278928326022

Epoch: 6| Step: 5
Training loss: 3.2261786460876465
Validation loss: 2.521472848871703

Epoch: 6| Step: 6
Training loss: 3.053431987762451
Validation loss: 2.5131408552969656

Epoch: 6| Step: 7
Training loss: 3.1004326343536377
Validation loss: 2.5122528153081096

Epoch: 6| Step: 8
Training loss: 2.1761369705200195
Validation loss: 2.512574031788816

Epoch: 6| Step: 9
Training loss: 3.010714530944824
Validation loss: 2.5134401141956286

Epoch: 6| Step: 10
Training loss: 2.326645851135254
Validation loss: 2.5155900011780443

Epoch: 6| Step: 11
Training loss: 2.213589668273926
Validation loss: 2.5117971051123833

Epoch: 6| Step: 12
Training loss: 2.0858235359191895
Validation loss: 2.5140044253359557

Epoch: 6| Step: 13
Training loss: 2.6851394176483154
Validation loss: 2.514400492432297

Epoch: 113| Step: 0
Training loss: 3.012122631072998
Validation loss: 2.517388279720019

Epoch: 6| Step: 1
Training loss: 3.155343770980835
Validation loss: 2.522247158071046

Epoch: 6| Step: 2
Training loss: 3.151994228363037
Validation loss: 2.5199868730319444

Epoch: 6| Step: 3
Training loss: 2.843381404876709
Validation loss: 2.5387143037652455

Epoch: 6| Step: 4
Training loss: 2.7227702140808105
Validation loss: 2.543479081123106

Epoch: 6| Step: 5
Training loss: 2.366778612136841
Validation loss: 2.537784545652328

Epoch: 6| Step: 6
Training loss: 3.3260254859924316
Validation loss: 2.561352568288003

Epoch: 6| Step: 7
Training loss: 1.9410408735275269
Validation loss: 2.538922045820503

Epoch: 6| Step: 8
Training loss: 2.330644130706787
Validation loss: 2.5450082568712133

Epoch: 6| Step: 9
Training loss: 2.8812026977539062
Validation loss: 2.5451846020196074

Epoch: 6| Step: 10
Training loss: 2.9077401161193848
Validation loss: 2.53958309337657

Epoch: 6| Step: 11
Training loss: 2.3885247707366943
Validation loss: 2.5421932948532926

Epoch: 6| Step: 12
Training loss: 2.872563600540161
Validation loss: 2.530259983513945

Epoch: 6| Step: 13
Training loss: 1.4714852571487427
Validation loss: 2.5182995847476426

Epoch: 114| Step: 0
Training loss: 2.530600070953369
Validation loss: 2.5193151350944274

Epoch: 6| Step: 1
Training loss: 3.041408061981201
Validation loss: 2.517386867154029

Epoch: 6| Step: 2
Training loss: 2.3039252758026123
Validation loss: 2.5127439165628083

Epoch: 6| Step: 3
Training loss: 1.916123628616333
Validation loss: 2.5209222121905257

Epoch: 6| Step: 4
Training loss: 3.3413939476013184
Validation loss: 2.5290407826823573

Epoch: 6| Step: 5
Training loss: 4.212463855743408
Validation loss: 2.5366132028641237

Epoch: 6| Step: 6
Training loss: 2.9224298000335693
Validation loss: 2.5451918917317546

Epoch: 6| Step: 7
Training loss: 2.4011802673339844
Validation loss: 2.547806211697158

Epoch: 6| Step: 8
Training loss: 2.630826950073242
Validation loss: 2.5564312960511897

Epoch: 6| Step: 9
Training loss: 2.4015164375305176
Validation loss: 2.5356298364618772

Epoch: 6| Step: 10
Training loss: 2.4066083431243896
Validation loss: 2.526926391868181

Epoch: 6| Step: 11
Training loss: 2.352673053741455
Validation loss: 2.519503919027185

Epoch: 6| Step: 12
Training loss: 2.633239269256592
Validation loss: 2.519071655888711

Epoch: 6| Step: 13
Training loss: 2.845583915710449
Validation loss: 2.5081328832975

Epoch: 115| Step: 0
Training loss: 3.1290218830108643
Validation loss: 2.511468228473458

Epoch: 6| Step: 1
Training loss: 2.2050793170928955
Validation loss: 2.5073287999758156

Epoch: 6| Step: 2
Training loss: 2.8428049087524414
Validation loss: 2.5149946571678243

Epoch: 6| Step: 3
Training loss: 3.13731050491333
Validation loss: 2.5194536165524553

Epoch: 6| Step: 4
Training loss: 2.211611032485962
Validation loss: 2.5188706485174035

Epoch: 6| Step: 5
Training loss: 2.1545886993408203
Validation loss: 2.531799793243408

Epoch: 6| Step: 6
Training loss: 2.53281307220459
Validation loss: 2.5284523656291347

Epoch: 6| Step: 7
Training loss: 3.048332452774048
Validation loss: 2.5293930486966203

Epoch: 6| Step: 8
Training loss: 3.443342447280884
Validation loss: 2.5297017328200804

Epoch: 6| Step: 9
Training loss: 2.5135409832000732
Validation loss: 2.52011707521254

Epoch: 6| Step: 10
Training loss: 2.8791561126708984
Validation loss: 2.509960333506266

Epoch: 6| Step: 11
Training loss: 2.3779189586639404
Validation loss: 2.5046067084035566

Epoch: 6| Step: 12
Training loss: 2.5249979496002197
Validation loss: 2.499590217426259

Epoch: 6| Step: 13
Training loss: 2.941946268081665
Validation loss: 2.4997761967361614

Epoch: 116| Step: 0
Training loss: 2.5708746910095215
Validation loss: 2.499289189615557

Epoch: 6| Step: 1
Training loss: 2.6845130920410156
Validation loss: 2.501077728886758

Epoch: 6| Step: 2
Training loss: 3.0412330627441406
Validation loss: 2.5030397702288885

Epoch: 6| Step: 3
Training loss: 3.177042245864868
Validation loss: 2.495547386907762

Epoch: 6| Step: 4
Training loss: 2.3289060592651367
Validation loss: 2.5033962265137704

Epoch: 6| Step: 5
Training loss: 3.04990816116333
Validation loss: 2.4973854480251187

Epoch: 6| Step: 6
Training loss: 2.7517054080963135
Validation loss: 2.5008197881842174

Epoch: 6| Step: 7
Training loss: 2.3806281089782715
Validation loss: 2.501267143475112

Epoch: 6| Step: 8
Training loss: 2.168236255645752
Validation loss: 2.499090007556382

Epoch: 6| Step: 9
Training loss: 2.620352268218994
Validation loss: 2.501775792849961

Epoch: 6| Step: 10
Training loss: 3.013580083847046
Validation loss: 2.5009712557638846

Epoch: 6| Step: 11
Training loss: 2.1978507041931152
Validation loss: 2.5098421727457354

Epoch: 6| Step: 12
Training loss: 3.102989673614502
Validation loss: 2.513173818588257

Epoch: 6| Step: 13
Training loss: 2.7330920696258545
Validation loss: 2.513483057739914

Epoch: 117| Step: 0
Training loss: 2.744159460067749
Validation loss: 2.514945258376419

Epoch: 6| Step: 1
Training loss: 2.688015937805176
Validation loss: 2.517869762195054

Epoch: 6| Step: 2
Training loss: 2.193723201751709
Validation loss: 2.5218464777033818

Epoch: 6| Step: 3
Training loss: 2.631049871444702
Validation loss: 2.5176477355341755

Epoch: 6| Step: 4
Training loss: 1.8930460214614868
Validation loss: 2.5317810479030816

Epoch: 6| Step: 5
Training loss: 3.3104934692382812
Validation loss: 2.526953307531213

Epoch: 6| Step: 6
Training loss: 3.6837446689605713
Validation loss: 2.5210728119778376

Epoch: 6| Step: 7
Training loss: 3.2186827659606934
Validation loss: 2.5213148183720087

Epoch: 6| Step: 8
Training loss: 2.448716163635254
Validation loss: 2.5187688002022366

Epoch: 6| Step: 9
Training loss: 2.5341711044311523
Validation loss: 2.5193533794854277

Epoch: 6| Step: 10
Training loss: 2.0913233757019043
Validation loss: 2.51370265406947

Epoch: 6| Step: 11
Training loss: 2.886834144592285
Validation loss: 2.5135771228421118

Epoch: 6| Step: 12
Training loss: 2.757744789123535
Validation loss: 2.5112454250294673

Epoch: 6| Step: 13
Training loss: 2.5224382877349854
Validation loss: 2.5126301473186863

Epoch: 118| Step: 0
Training loss: 2.223897695541382
Validation loss: 2.506131408035114

Epoch: 6| Step: 1
Training loss: 2.7258710861206055
Validation loss: 2.5039066935098298

Epoch: 6| Step: 2
Training loss: 2.875272750854492
Validation loss: 2.5005142996388097

Epoch: 6| Step: 3
Training loss: 2.05539870262146
Validation loss: 2.5053641462838776

Epoch: 6| Step: 4
Training loss: 2.0162456035614014
Validation loss: 2.506884262125979

Epoch: 6| Step: 5
Training loss: 2.528985023498535
Validation loss: 2.5037951495057795

Epoch: 6| Step: 6
Training loss: 2.342923164367676
Validation loss: 2.5106889996477353

Epoch: 6| Step: 7
Training loss: 3.6973588466644287
Validation loss: 2.5167568511860345

Epoch: 6| Step: 8
Training loss: 3.5657804012298584
Validation loss: 2.5213812089735463

Epoch: 6| Step: 9
Training loss: 2.3163294792175293
Validation loss: 2.5107742714625534

Epoch: 6| Step: 10
Training loss: 3.3530259132385254
Validation loss: 2.5061119064208

Epoch: 6| Step: 11
Training loss: 3.079036235809326
Validation loss: 2.5086529383095364

Epoch: 6| Step: 12
Training loss: 2.7966291904449463
Validation loss: 2.511070587301767

Epoch: 6| Step: 13
Training loss: 1.7044557332992554
Validation loss: 2.5190794237198366

Epoch: 119| Step: 0
Training loss: 1.839016079902649
Validation loss: 2.51243854594487

Epoch: 6| Step: 1
Training loss: 2.632601737976074
Validation loss: 2.5191893398120837

Epoch: 6| Step: 2
Training loss: 3.924879550933838
Validation loss: 2.519363646866173

Epoch: 6| Step: 3
Training loss: 2.5389790534973145
Validation loss: 2.510206725007744

Epoch: 6| Step: 4
Training loss: 2.564913272857666
Validation loss: 2.5091827633560344

Epoch: 6| Step: 5
Training loss: 2.3102328777313232
Validation loss: 2.5044832332159883

Epoch: 6| Step: 6
Training loss: 2.9889559745788574
Validation loss: 2.5047303194640786

Epoch: 6| Step: 7
Training loss: 2.4451241493225098
Validation loss: 2.50409302660214

Epoch: 6| Step: 8
Training loss: 2.3585939407348633
Validation loss: 2.5048964613227436

Epoch: 6| Step: 9
Training loss: 2.748621702194214
Validation loss: 2.503149829885011

Epoch: 6| Step: 10
Training loss: 2.9143035411834717
Validation loss: 2.511864831370692

Epoch: 6| Step: 11
Training loss: 3.7799606323242188
Validation loss: 2.5179100062257502

Epoch: 6| Step: 12
Training loss: 2.5342230796813965
Validation loss: 2.516820033391317

Epoch: 6| Step: 13
Training loss: 1.7732040882110596
Validation loss: 2.5210409702793246

Epoch: 120| Step: 0
Training loss: 2.732306957244873
Validation loss: 2.54103607772499

Epoch: 6| Step: 1
Training loss: 2.513810157775879
Validation loss: 2.5722066458835395

Epoch: 6| Step: 2
Training loss: 2.8584187030792236
Validation loss: 2.6095426902976087

Epoch: 6| Step: 3
Training loss: 2.8011996746063232
Validation loss: 2.5769865897393998

Epoch: 6| Step: 4
Training loss: 2.5970568656921387
Validation loss: 2.559251226404662

Epoch: 6| Step: 5
Training loss: 2.3297979831695557
Validation loss: 2.529193583355155

Epoch: 6| Step: 6
Training loss: 3.027381420135498
Validation loss: 2.5048464754576325

Epoch: 6| Step: 7
Training loss: 3.046764850616455
Validation loss: 2.4939703736253964

Epoch: 6| Step: 8
Training loss: 2.3443384170532227
Validation loss: 2.4930648162800777

Epoch: 6| Step: 9
Training loss: 2.6766817569732666
Validation loss: 2.4932986831152313

Epoch: 6| Step: 10
Training loss: 2.7296152114868164
Validation loss: 2.4941724064529582

Epoch: 6| Step: 11
Training loss: 2.4463133811950684
Validation loss: 2.5008033373022593

Epoch: 6| Step: 12
Training loss: 3.1616663932800293
Validation loss: 2.49772556366459

Epoch: 6| Step: 13
Training loss: 2.46753191947937
Validation loss: 2.5131617566590667

Epoch: 121| Step: 0
Training loss: 3.4860000610351562
Validation loss: 2.5063111423164286

Epoch: 6| Step: 1
Training loss: 1.956528663635254
Validation loss: 2.50133486716978

Epoch: 6| Step: 2
Training loss: 3.0913357734680176
Validation loss: 2.500354097735497

Epoch: 6| Step: 3
Training loss: 3.651247262954712
Validation loss: 2.4974010375238236

Epoch: 6| Step: 4
Training loss: 3.160733222961426
Validation loss: 2.4922779375506985

Epoch: 6| Step: 5
Training loss: 2.651089906692505
Validation loss: 2.4920828060437272

Epoch: 6| Step: 6
Training loss: 1.828782320022583
Validation loss: 2.4888288051851335

Epoch: 6| Step: 7
Training loss: 2.123493194580078
Validation loss: 2.483633813037667

Epoch: 6| Step: 8
Training loss: 2.4806342124938965
Validation loss: 2.488431489595803

Epoch: 6| Step: 9
Training loss: 3.1664514541625977
Validation loss: 2.4874849242548787

Epoch: 6| Step: 10
Training loss: 2.3692703247070312
Validation loss: 2.4850642629849014

Epoch: 6| Step: 11
Training loss: 2.3512628078460693
Validation loss: 2.4880100309207873

Epoch: 6| Step: 12
Training loss: 2.910508871078491
Validation loss: 2.485113905322167

Epoch: 6| Step: 13
Training loss: 2.398728847503662
Validation loss: 2.4921644041615147

Epoch: 122| Step: 0
Training loss: 3.3560690879821777
Validation loss: 2.484615086227335

Epoch: 6| Step: 1
Training loss: 2.1179027557373047
Validation loss: 2.4877433802491877

Epoch: 6| Step: 2
Training loss: 2.4315407276153564
Validation loss: 2.493484317615468

Epoch: 6| Step: 3
Training loss: 2.7585909366607666
Validation loss: 2.4957003824172483

Epoch: 6| Step: 4
Training loss: 2.414785623550415
Validation loss: 2.499503066462855

Epoch: 6| Step: 5
Training loss: 2.2169036865234375
Validation loss: 2.502740716421476

Epoch: 6| Step: 6
Training loss: 2.976353645324707
Validation loss: 2.503205845432897

Epoch: 6| Step: 7
Training loss: 3.063838481903076
Validation loss: 2.5070079757321264

Epoch: 6| Step: 8
Training loss: 1.5067718029022217
Validation loss: 2.5020016444626676

Epoch: 6| Step: 9
Training loss: 3.2301759719848633
Validation loss: 2.506106279229605

Epoch: 6| Step: 10
Training loss: 3.112168788909912
Validation loss: 2.501801798420568

Epoch: 6| Step: 11
Training loss: 3.0082225799560547
Validation loss: 2.493384043375651

Epoch: 6| Step: 12
Training loss: 3.1489343643188477
Validation loss: 2.4895827693323933

Epoch: 6| Step: 13
Training loss: 2.054385185241699
Validation loss: 2.49108233246752

Epoch: 123| Step: 0
Training loss: 2.8297059535980225
Validation loss: 2.4900882397928545

Epoch: 6| Step: 1
Training loss: 2.591512680053711
Validation loss: 2.4858131767601095

Epoch: 6| Step: 2
Training loss: 3.004011631011963
Validation loss: 2.489890465172388

Epoch: 6| Step: 3
Training loss: 2.422057628631592
Validation loss: 2.496348688679357

Epoch: 6| Step: 4
Training loss: 3.1063666343688965
Validation loss: 2.4970165683377172

Epoch: 6| Step: 5
Training loss: 2.6938223838806152
Validation loss: 2.49923191788376

Epoch: 6| Step: 6
Training loss: 2.752852439880371
Validation loss: 2.501998580912108

Epoch: 6| Step: 7
Training loss: 2.4575507640838623
Validation loss: 2.497521956761678

Epoch: 6| Step: 8
Training loss: 2.7663488388061523
Validation loss: 2.4952851956890476

Epoch: 6| Step: 9
Training loss: 2.699080228805542
Validation loss: 2.501713050309048

Epoch: 6| Step: 10
Training loss: 2.550035238265991
Validation loss: 2.50420835966705

Epoch: 6| Step: 11
Training loss: 2.4773266315460205
Validation loss: 2.524334164075954

Epoch: 6| Step: 12
Training loss: 2.9303340911865234
Validation loss: 2.5350297317709973

Epoch: 6| Step: 13
Training loss: 2.022543430328369
Validation loss: 2.5468137623161398

Epoch: 124| Step: 0
Training loss: 1.9792101383209229
Validation loss: 2.56134017052189

Epoch: 6| Step: 1
Training loss: 3.3131892681121826
Validation loss: 2.5675315087841404

Epoch: 6| Step: 2
Training loss: 2.7194864749908447
Validation loss: 2.5638845351434525

Epoch: 6| Step: 3
Training loss: 2.9029526710510254
Validation loss: 2.5406099237421507

Epoch: 6| Step: 4
Training loss: 3.5855231285095215
Validation loss: 2.5242071536279496

Epoch: 6| Step: 5
Training loss: 1.807242751121521
Validation loss: 2.507581777470086

Epoch: 6| Step: 6
Training loss: 2.5390944480895996
Validation loss: 2.4997527932608

Epoch: 6| Step: 7
Training loss: 2.1564993858337402
Validation loss: 2.4926974004314792

Epoch: 6| Step: 8
Training loss: 2.8992090225219727
Validation loss: 2.502090384883265

Epoch: 6| Step: 9
Training loss: 3.167332172393799
Validation loss: 2.483811440006379

Epoch: 6| Step: 10
Training loss: 2.1967687606811523
Validation loss: 2.4921204069609284

Epoch: 6| Step: 11
Training loss: 2.977905750274658
Validation loss: 2.4862150043569584

Epoch: 6| Step: 12
Training loss: 2.494441032409668
Validation loss: 2.484045631142073

Epoch: 6| Step: 13
Training loss: 2.863497734069824
Validation loss: 2.481332307220787

Epoch: 125| Step: 0
Training loss: 2.998786687850952
Validation loss: 2.4838153085400982

Epoch: 6| Step: 1
Training loss: 2.0796046257019043
Validation loss: 2.4815539390810075

Epoch: 6| Step: 2
Training loss: 3.2241950035095215
Validation loss: 2.4828169550946964

Epoch: 6| Step: 3
Training loss: 2.945833683013916
Validation loss: 2.4926511549180552

Epoch: 6| Step: 4
Training loss: 1.646432638168335
Validation loss: 2.491560541173463

Epoch: 6| Step: 5
Training loss: 2.1613855361938477
Validation loss: 2.494794258507349

Epoch: 6| Step: 6
Training loss: 2.897376775741577
Validation loss: 2.494517416082403

Epoch: 6| Step: 7
Training loss: 2.9786617755889893
Validation loss: 2.49155246057818

Epoch: 6| Step: 8
Training loss: 2.8714699745178223
Validation loss: 2.482929665555236

Epoch: 6| Step: 9
Training loss: 3.2991857528686523
Validation loss: 2.4860709944079

Epoch: 6| Step: 10
Training loss: 2.9968101978302
Validation loss: 2.4817780897181523

Epoch: 6| Step: 11
Training loss: 2.350156307220459
Validation loss: 2.481614922964445

Epoch: 6| Step: 12
Training loss: 2.548038959503174
Validation loss: 2.486498253319853

Epoch: 6| Step: 13
Training loss: 2.5865299701690674
Validation loss: 2.4869891059014106

Testing loss: 2.6253994835747614

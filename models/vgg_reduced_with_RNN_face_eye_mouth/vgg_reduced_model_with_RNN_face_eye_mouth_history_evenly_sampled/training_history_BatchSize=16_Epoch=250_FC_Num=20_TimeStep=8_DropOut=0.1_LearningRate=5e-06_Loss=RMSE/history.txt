Epoch: 1| Step: 0
Training loss: 5.63170613972262
Validation loss: 5.795172229396672

Epoch: 6| Step: 1
Training loss: 5.498970802419691
Validation loss: 5.790545515798994

Epoch: 6| Step: 2
Training loss: 5.021885657403031
Validation loss: 5.785349481118317

Epoch: 6| Step: 3
Training loss: 6.597468335652806
Validation loss: 5.7797745985717635

Epoch: 6| Step: 4
Training loss: 4.748689169457714
Validation loss: 5.77493298424263

Epoch: 6| Step: 5
Training loss: 6.6195667947971675
Validation loss: 5.769587869459589

Epoch: 6| Step: 6
Training loss: 4.839653399328795
Validation loss: 5.764567883458868

Epoch: 6| Step: 7
Training loss: 7.002820128069387
Validation loss: 5.759685731743757

Epoch: 6| Step: 8
Training loss: 5.579808427263515
Validation loss: 5.7540167198589645

Epoch: 6| Step: 9
Training loss: 4.550227970658277
Validation loss: 5.748848488147997

Epoch: 6| Step: 10
Training loss: 6.629831819314316
Validation loss: 5.743312597504395

Epoch: 6| Step: 11
Training loss: 6.4874330961860585
Validation loss: 5.737659768131546

Epoch: 6| Step: 12
Training loss: 5.599488793609275
Validation loss: 5.7320037022357635

Epoch: 6| Step: 13
Training loss: 5.530750316743842
Validation loss: 5.726328224928083

Epoch: 2| Step: 0
Training loss: 4.9838313939337056
Validation loss: 5.720011741774716

Epoch: 6| Step: 1
Training loss: 6.249581284802332
Validation loss: 5.713842366125853

Epoch: 6| Step: 2
Training loss: 6.223962487468104
Validation loss: 5.707168249423502

Epoch: 6| Step: 3
Training loss: 5.887394269572948
Validation loss: 5.700227248881401

Epoch: 6| Step: 4
Training loss: 5.070931653981886
Validation loss: 5.693247830833738

Epoch: 6| Step: 5
Training loss: 3.7254455299967355
Validation loss: 5.685083650599892

Epoch: 6| Step: 6
Training loss: 5.999357189076918
Validation loss: 5.677272402492011

Epoch: 6| Step: 7
Training loss: 7.2502036230497415
Validation loss: 5.668595144520567

Epoch: 6| Step: 8
Training loss: 5.628773080811099
Validation loss: 5.660118182218265

Epoch: 6| Step: 9
Training loss: 5.290512539816909
Validation loss: 5.651479297891795

Epoch: 6| Step: 10
Training loss: 5.042829563338703
Validation loss: 5.641817552845071

Epoch: 6| Step: 11
Training loss: 6.151678411444476
Validation loss: 5.631564088805471

Epoch: 6| Step: 12
Training loss: 5.547086641814061
Validation loss: 5.621262958595435

Epoch: 6| Step: 13
Training loss: 6.364508878664681
Validation loss: 5.6101657084932075

Epoch: 3| Step: 0
Training loss: 6.098360635564795
Validation loss: 5.598317434532463

Epoch: 6| Step: 1
Training loss: 5.63886248243608
Validation loss: 5.5856559427170565

Epoch: 6| Step: 2
Training loss: 6.485902610911041
Validation loss: 5.57351244307512

Epoch: 6| Step: 3
Training loss: 6.020296101432701
Validation loss: 5.560878870004376

Epoch: 6| Step: 4
Training loss: 6.027636142049178
Validation loss: 5.546563968029136

Epoch: 6| Step: 5
Training loss: 5.018282842687226
Validation loss: 5.532231990779433

Epoch: 6| Step: 6
Training loss: 5.212950630743059
Validation loss: 5.5165971560824

Epoch: 6| Step: 7
Training loss: 5.49958730363051
Validation loss: 5.50301944189125

Epoch: 6| Step: 8
Training loss: 4.407764147601796
Validation loss: 5.486988629473429

Epoch: 6| Step: 9
Training loss: 5.310523888558197
Validation loss: 5.472005853615522

Epoch: 6| Step: 10
Training loss: 5.519940209272237
Validation loss: 5.454563383562062

Epoch: 6| Step: 11
Training loss: 5.227289214880954
Validation loss: 5.438139869684348

Epoch: 6| Step: 12
Training loss: 5.603629992951032
Validation loss: 5.419744554534814

Epoch: 6| Step: 13
Training loss: 5.19517831270551
Validation loss: 5.400769298617954

Epoch: 4| Step: 0
Training loss: 5.504774102367037
Validation loss: 5.38217432154595

Epoch: 6| Step: 1
Training loss: 4.975448411832215
Validation loss: 5.361024115230624

Epoch: 6| Step: 2
Training loss: 5.133141912053691
Validation loss: 5.33975399423287

Epoch: 6| Step: 3
Training loss: 5.682819158987025
Validation loss: 5.318030346335573

Epoch: 6| Step: 4
Training loss: 5.783881500971917
Validation loss: 5.2948818746865385

Epoch: 6| Step: 5
Training loss: 5.199793026546431
Validation loss: 5.273181281266882

Epoch: 6| Step: 6
Training loss: 4.9127073132593555
Validation loss: 5.249175069898176

Epoch: 6| Step: 7
Training loss: 5.431609723258015
Validation loss: 5.226332810410103

Epoch: 6| Step: 8
Training loss: 3.7691822265960555
Validation loss: 5.198613831525205

Epoch: 6| Step: 9
Training loss: 5.480310395631866
Validation loss: 5.174675252564495

Epoch: 6| Step: 10
Training loss: 5.816670552045659
Validation loss: 5.149636803163436

Epoch: 6| Step: 11
Training loss: 5.528829788643082
Validation loss: 5.124571907658363

Epoch: 6| Step: 12
Training loss: 5.563854952601996
Validation loss: 5.095351994416908

Epoch: 6| Step: 13
Training loss: 4.542351324055358
Validation loss: 5.069175063882979

Epoch: 5| Step: 0
Training loss: 4.909652987646525
Validation loss: 5.04178193883563

Epoch: 6| Step: 1
Training loss: 5.050378675710011
Validation loss: 5.013389670189214

Epoch: 6| Step: 2
Training loss: 4.697893141231574
Validation loss: 4.985094013752535

Epoch: 6| Step: 3
Training loss: 4.749012091448764
Validation loss: 4.9605561063051065

Epoch: 6| Step: 4
Training loss: 5.37150619865258
Validation loss: 4.932054853917343

Epoch: 6| Step: 5
Training loss: 5.503161128812055
Validation loss: 4.905122080614551

Epoch: 6| Step: 6
Training loss: 4.33866463891818
Validation loss: 4.876490634045211

Epoch: 6| Step: 7
Training loss: 4.717069819354391
Validation loss: 4.84999733457417

Epoch: 6| Step: 8
Training loss: 5.377038258724873
Validation loss: 4.827457156520756

Epoch: 6| Step: 9
Training loss: 5.014609926367458
Validation loss: 4.798915087042636

Epoch: 6| Step: 10
Training loss: 4.747522410680859
Validation loss: 4.773307432109108

Epoch: 6| Step: 11
Training loss: 4.833475746599826
Validation loss: 4.747162284837437

Epoch: 6| Step: 12
Training loss: 5.445349803999284
Validation loss: 4.7220214182565

Epoch: 6| Step: 13
Training loss: 3.479634297327759
Validation loss: 4.694416974595653

Epoch: 6| Step: 0
Training loss: 4.000289906486466
Validation loss: 4.668130277417714

Epoch: 6| Step: 1
Training loss: 4.652111492877518
Validation loss: 4.641335285611823

Epoch: 6| Step: 2
Training loss: 5.113619380113547
Validation loss: 4.614924808094473

Epoch: 6| Step: 3
Training loss: 4.685891651159494
Validation loss: 4.588308900583824

Epoch: 6| Step: 4
Training loss: 3.7812167079500307
Validation loss: 4.561254994791274

Epoch: 6| Step: 5
Training loss: 4.993559695020615
Validation loss: 4.537451486021102

Epoch: 6| Step: 6
Training loss: 4.646203794662702
Validation loss: 4.516357759830424

Epoch: 6| Step: 7
Training loss: 5.085275539558542
Validation loss: 4.493725103644522

Epoch: 6| Step: 8
Training loss: 5.1782572571112375
Validation loss: 4.471989471820275

Epoch: 6| Step: 9
Training loss: 4.114885587897828
Validation loss: 4.4484368652385715

Epoch: 6| Step: 10
Training loss: 4.641761306720161
Validation loss: 4.426552611454571

Epoch: 6| Step: 11
Training loss: 4.145486072712431
Validation loss: 4.403746121061067

Epoch: 6| Step: 12
Training loss: 5.186598182330694
Validation loss: 4.379790362296497

Epoch: 6| Step: 13
Training loss: 3.2938101022871975
Validation loss: 4.358804736690454

Epoch: 7| Step: 0
Training loss: 4.379921025648518
Validation loss: 4.340610212473336

Epoch: 6| Step: 1
Training loss: 4.809648536007282
Validation loss: 4.31902451831492

Epoch: 6| Step: 2
Training loss: 4.641485166625221
Validation loss: 4.303750582298519

Epoch: 6| Step: 3
Training loss: 4.050810443180726
Validation loss: 4.285602562752405

Epoch: 6| Step: 4
Training loss: 4.477224996652982
Validation loss: 4.270054829632504

Epoch: 6| Step: 5
Training loss: 4.939375122950737
Validation loss: 4.25262223575776

Epoch: 6| Step: 6
Training loss: 4.334251722193002
Validation loss: 4.236843235400276

Epoch: 6| Step: 7
Training loss: 2.8201484949501037
Validation loss: 4.220266532484358

Epoch: 6| Step: 8
Training loss: 4.58254801640221
Validation loss: 4.205549646044335

Epoch: 6| Step: 9
Training loss: 4.589375183861859
Validation loss: 4.190346174389912

Epoch: 6| Step: 10
Training loss: 3.9811397806717452
Validation loss: 4.178590504418215

Epoch: 6| Step: 11
Training loss: 4.944645985890319
Validation loss: 4.162061680684167

Epoch: 6| Step: 12
Training loss: 3.901548073481851
Validation loss: 4.150476877859287

Epoch: 6| Step: 13
Training loss: 4.077991702652704
Validation loss: 4.138424336181813

Epoch: 8| Step: 0
Training loss: 4.47846749933257
Validation loss: 4.129592802260583

Epoch: 6| Step: 1
Training loss: 4.252760944018916
Validation loss: 4.116805804297915

Epoch: 6| Step: 2
Training loss: 4.05292003528048
Validation loss: 4.106298274238455

Epoch: 6| Step: 3
Training loss: 3.045139698901782
Validation loss: 4.095350864131784

Epoch: 6| Step: 4
Training loss: 4.64668469119487
Validation loss: 4.08650081946259

Epoch: 6| Step: 5
Training loss: 4.465277756683893
Validation loss: 4.074587429734281

Epoch: 6| Step: 6
Training loss: 3.7487908003247674
Validation loss: 4.064597944398832

Epoch: 6| Step: 7
Training loss: 5.082557785824007
Validation loss: 4.057513520154804

Epoch: 6| Step: 8
Training loss: 4.613125997930072
Validation loss: 4.045254275301326

Epoch: 6| Step: 9
Training loss: 4.807820608921298
Validation loss: 4.037653322190433

Epoch: 6| Step: 10
Training loss: 4.562897965306355
Validation loss: 4.028408589727999

Epoch: 6| Step: 11
Training loss: 1.962677324387044
Validation loss: 4.019000331470559

Epoch: 6| Step: 12
Training loss: 3.301461451040663
Validation loss: 4.011651024183492

Epoch: 6| Step: 13
Training loss: 5.1398437277345534
Validation loss: 4.002190659141557

Epoch: 9| Step: 0
Training loss: 4.326052393242832
Validation loss: 3.9930150395436783

Epoch: 6| Step: 1
Training loss: 4.1761952864925815
Validation loss: 3.985892599855189

Epoch: 6| Step: 2
Training loss: 4.003608982867247
Validation loss: 3.9763565969141954

Epoch: 6| Step: 3
Training loss: 4.5104475600291
Validation loss: 3.9669598698970585

Epoch: 6| Step: 4
Training loss: 4.753932329042066
Validation loss: 3.96016801414291

Epoch: 6| Step: 5
Training loss: 2.8968336906903294
Validation loss: 3.9528746348310024

Epoch: 6| Step: 6
Training loss: 3.97768591579966
Validation loss: 3.942779535604392

Epoch: 6| Step: 7
Training loss: 4.487894517988345
Validation loss: 3.935254920734604

Epoch: 6| Step: 8
Training loss: 3.11526376357611
Validation loss: 3.9269189766200814

Epoch: 6| Step: 9
Training loss: 3.8988447874825543
Validation loss: 3.919481185628047

Epoch: 6| Step: 10
Training loss: 4.058270409401068
Validation loss: 3.9120263674339997

Epoch: 6| Step: 11
Training loss: 4.962995159514982
Validation loss: 3.9060111785403255

Epoch: 6| Step: 12
Training loss: 3.799171914673466
Validation loss: 3.8989317762283187

Epoch: 6| Step: 13
Training loss: 3.7942543561791906
Validation loss: 3.8919480784728555

Epoch: 10| Step: 0
Training loss: 3.724294552152642
Validation loss: 3.885673039207215

Epoch: 6| Step: 1
Training loss: 3.9395785219568937
Validation loss: 3.8793816855828225

Epoch: 6| Step: 2
Training loss: 3.2106872609587973
Validation loss: 3.873376718676707

Epoch: 6| Step: 3
Training loss: 4.745710745219826
Validation loss: 3.8692249952042483

Epoch: 6| Step: 4
Training loss: 4.384400214451062
Validation loss: 3.8626413182690804

Epoch: 6| Step: 5
Training loss: 4.263539010326764
Validation loss: 3.856033489466008

Epoch: 6| Step: 6
Training loss: 3.915028953396465
Validation loss: 3.850122629723559

Epoch: 6| Step: 7
Training loss: 3.9371070060189197
Validation loss: 3.846913619578278

Epoch: 6| Step: 8
Training loss: 3.717296155870449
Validation loss: 3.8398825905396228

Epoch: 6| Step: 9
Training loss: 3.793455994225708
Validation loss: 3.8373536143230855

Epoch: 6| Step: 10
Training loss: 3.749026362861909
Validation loss: 3.832766510162326

Epoch: 6| Step: 11
Training loss: 3.8208561609904286
Validation loss: 3.825774056271658

Epoch: 6| Step: 12
Training loss: 4.714342092201698
Validation loss: 3.8206279097741085

Epoch: 6| Step: 13
Training loss: 4.055837949446201
Validation loss: 3.814943900377195

Epoch: 11| Step: 0
Training loss: 4.273472759652299
Validation loss: 3.8123019399155105

Epoch: 6| Step: 1
Training loss: 3.6145662569344394
Validation loss: 3.8086475588329076

Epoch: 6| Step: 2
Training loss: 3.9999861716985095
Validation loss: 3.8074531658398776

Epoch: 6| Step: 3
Training loss: 3.959531636139291
Validation loss: 3.797413522319411

Epoch: 6| Step: 4
Training loss: 3.8457884306816013
Validation loss: 3.7932196520697605

Epoch: 6| Step: 5
Training loss: 4.089898080838386
Validation loss: 3.7868433964717254

Epoch: 6| Step: 6
Training loss: 4.448910080381521
Validation loss: 3.7834243224814452

Epoch: 6| Step: 7
Training loss: 3.24521695815068
Validation loss: 3.780829237820938

Epoch: 6| Step: 8
Training loss: 3.6523131996549836
Validation loss: 3.7760923474580794

Epoch: 6| Step: 9
Training loss: 3.776866294879846
Validation loss: 3.772143456178497

Epoch: 6| Step: 10
Training loss: 5.214374101042713
Validation loss: 3.7661178934151667

Epoch: 6| Step: 11
Training loss: 3.469676538391856
Validation loss: 3.763439891275964

Epoch: 6| Step: 12
Training loss: 3.3662246779074803
Validation loss: 3.75619468395027

Epoch: 6| Step: 13
Training loss: 4.0926130040808735
Validation loss: 3.750849620048675

Epoch: 12| Step: 0
Training loss: 2.8722265383279897
Validation loss: 3.747356850571407

Epoch: 6| Step: 1
Training loss: 3.4707924268371957
Validation loss: 3.743122636256598

Epoch: 6| Step: 2
Training loss: 4.109193515033594
Validation loss: 3.739709637704076

Epoch: 6| Step: 3
Training loss: 3.431941584252233
Validation loss: 3.7364034362355576

Epoch: 6| Step: 4
Training loss: 4.632236695812324
Validation loss: 3.7286938832607355

Epoch: 6| Step: 5
Training loss: 3.7216254905772366
Validation loss: 3.725545475975165

Epoch: 6| Step: 6
Training loss: 2.936343026082152
Validation loss: 3.7227982529041017

Epoch: 6| Step: 7
Training loss: 3.905317271454735
Validation loss: 3.7188253390769757

Epoch: 6| Step: 8
Training loss: 3.1873782265533603
Validation loss: 3.715225138901668

Epoch: 6| Step: 9
Training loss: 4.868393358284765
Validation loss: 3.70995008439416

Epoch: 6| Step: 10
Training loss: 4.421799082828872
Validation loss: 3.703355556233916

Epoch: 6| Step: 11
Training loss: 4.322424484484635
Validation loss: 3.6997227451529002

Epoch: 6| Step: 12
Training loss: 3.881221575794167
Validation loss: 3.698281329816464

Epoch: 6| Step: 13
Training loss: 4.419815068928821
Validation loss: 3.69299056976336

Epoch: 13| Step: 0
Training loss: 3.3079059340769006
Validation loss: 3.687220358947415

Epoch: 6| Step: 1
Training loss: 4.343322952464988
Validation loss: 3.6850004531419946

Epoch: 6| Step: 2
Training loss: 3.6941925351744294
Validation loss: 3.6751024572496616

Epoch: 6| Step: 3
Training loss: 3.523999222260648
Validation loss: 3.6742667516901033

Epoch: 6| Step: 4
Training loss: 4.226092017988585
Validation loss: 3.6704048182531634

Epoch: 6| Step: 5
Training loss: 3.5931097372503586
Validation loss: 3.6680487279887224

Epoch: 6| Step: 6
Training loss: 3.9257296278866542
Validation loss: 3.6653225472980906

Epoch: 6| Step: 7
Training loss: 4.43087661095453
Validation loss: 3.6583875030163022

Epoch: 6| Step: 8
Training loss: 3.6742309803711497
Validation loss: 3.652622102662879

Epoch: 6| Step: 9
Training loss: 3.8002693482358096
Validation loss: 3.6498368172122886

Epoch: 6| Step: 10
Training loss: 2.841421662300103
Validation loss: 3.64862746699579

Epoch: 6| Step: 11
Training loss: 4.1227278954210185
Validation loss: 3.646525611136553

Epoch: 6| Step: 12
Training loss: 3.7767791799522903
Validation loss: 3.644046054015828

Epoch: 6| Step: 13
Training loss: 4.659839449887782
Validation loss: 3.6386393596900146

Epoch: 14| Step: 0
Training loss: 3.5058993621500907
Validation loss: 3.63239384626181

Epoch: 6| Step: 1
Training loss: 4.731323467374608
Validation loss: 3.629571564547585

Epoch: 6| Step: 2
Training loss: 4.477671008313519
Validation loss: 3.6277587910438354

Epoch: 6| Step: 3
Training loss: 3.473057452305764
Validation loss: 3.623424941735959

Epoch: 6| Step: 4
Training loss: 3.327467159392919
Validation loss: 3.6169226899497335

Epoch: 6| Step: 5
Training loss: 3.4859738637411675
Validation loss: 3.6148853895090842

Epoch: 6| Step: 6
Training loss: 3.976995238495284
Validation loss: 3.6091372738322844

Epoch: 6| Step: 7
Training loss: 4.470799969598737
Validation loss: 3.6048198769291195

Epoch: 6| Step: 8
Training loss: 3.5806749078512183
Validation loss: 3.6022371505465056

Epoch: 6| Step: 9
Training loss: 3.7662469500641067
Validation loss: 3.5968402503965136

Epoch: 6| Step: 10
Training loss: 3.846832528023182
Validation loss: 3.590094549048195

Epoch: 6| Step: 11
Training loss: 3.1303253575103134
Validation loss: 3.586222558895532

Epoch: 6| Step: 12
Training loss: 3.801430312561924
Validation loss: 3.5797355965132702

Epoch: 6| Step: 13
Training loss: 2.8483010230585784
Validation loss: 3.5761459214425075

Epoch: 15| Step: 0
Training loss: 3.7476669683535753
Validation loss: 3.5726281295350697

Epoch: 6| Step: 1
Training loss: 3.8266778195669446
Validation loss: 3.566877130350574

Epoch: 6| Step: 2
Training loss: 3.134291302218587
Validation loss: 3.5625244698208514

Epoch: 6| Step: 3
Training loss: 4.2380782821503855
Validation loss: 3.5576883351109356

Epoch: 6| Step: 4
Training loss: 3.420031604063116
Validation loss: 3.5540128096217054

Epoch: 6| Step: 5
Training loss: 4.616824373014396
Validation loss: 3.5468190253674927

Epoch: 6| Step: 6
Training loss: 2.5973400411101455
Validation loss: 3.5396496776139124

Epoch: 6| Step: 7
Training loss: 3.6351203396907237
Validation loss: 3.5353449814887448

Epoch: 6| Step: 8
Training loss: 4.105593954367583
Validation loss: 3.5341488222370483

Epoch: 6| Step: 9
Training loss: 3.71437636201934
Validation loss: 3.528169100163618

Epoch: 6| Step: 10
Training loss: 3.588682366627066
Validation loss: 3.522451628949065

Epoch: 6| Step: 11
Training loss: 3.0650512033887023
Validation loss: 3.515923891840946

Epoch: 6| Step: 12
Training loss: 4.174637583461236
Validation loss: 3.5104330685729175

Epoch: 6| Step: 13
Training loss: 4.342155108857799
Validation loss: 3.5075488529173073

Epoch: 16| Step: 0
Training loss: 4.439791020810741
Validation loss: 3.4991431571805007

Epoch: 6| Step: 1
Training loss: 3.9793752379287786
Validation loss: 3.4963479068886962

Epoch: 6| Step: 2
Training loss: 2.8433160136307642
Validation loss: 3.4905079943360002

Epoch: 6| Step: 3
Training loss: 4.076237150217044
Validation loss: 3.4879928958596946

Epoch: 6| Step: 4
Training loss: 4.235475291634331
Validation loss: 3.4835989247975165

Epoch: 6| Step: 5
Training loss: 2.7419283926508373
Validation loss: 3.474695257807855

Epoch: 6| Step: 6
Training loss: 4.270026203584338
Validation loss: 3.4731839749781037

Epoch: 6| Step: 7
Training loss: 3.547290122320708
Validation loss: 3.465155807604059

Epoch: 6| Step: 8
Training loss: 3.1607083096574717
Validation loss: 3.460822623289466

Epoch: 6| Step: 9
Training loss: 3.3781497527423388
Validation loss: 3.4553407149256206

Epoch: 6| Step: 10
Training loss: 4.156813504445705
Validation loss: 3.4516197409038347

Epoch: 6| Step: 11
Training loss: 3.3041111508901335
Validation loss: 3.447428078169993

Epoch: 6| Step: 12
Training loss: 3.687730684985072
Validation loss: 3.4401248147136507

Epoch: 6| Step: 13
Training loss: 2.8461369149146605
Validation loss: 3.4398675173173827

Epoch: 17| Step: 0
Training loss: 3.1578440653767803
Validation loss: 3.4362261995274896

Epoch: 6| Step: 1
Training loss: 3.8250814990168633
Validation loss: 3.4291999091508014

Epoch: 6| Step: 2
Training loss: 2.9407055186032407
Validation loss: 3.430032230322515

Epoch: 6| Step: 3
Training loss: 2.998578688580863
Validation loss: 3.4293460242878444

Epoch: 6| Step: 4
Training loss: 3.5604302266359156
Validation loss: 3.424090143580808

Epoch: 6| Step: 5
Training loss: 3.9307334524154633
Validation loss: 3.4188848858622336

Epoch: 6| Step: 6
Training loss: 4.0409084801829955
Validation loss: 3.416626354993171

Epoch: 6| Step: 7
Training loss: 2.84144381397874
Validation loss: 3.4131918784730524

Epoch: 6| Step: 8
Training loss: 4.402820957318404
Validation loss: 3.4123502297965764

Epoch: 6| Step: 9
Training loss: 4.218704336413749
Validation loss: 3.406905003746715

Epoch: 6| Step: 10
Training loss: 3.3353262347976242
Validation loss: 3.4035823762848247

Epoch: 6| Step: 11
Training loss: 3.690825562998991
Validation loss: 3.396256803502083

Epoch: 6| Step: 12
Training loss: 3.82036117803535
Validation loss: 3.3973284812699163

Epoch: 6| Step: 13
Training loss: 3.80204590382725
Validation loss: 3.3925805400932574

Epoch: 18| Step: 0
Training loss: 2.7881988128420976
Validation loss: 3.3882256971882247

Epoch: 6| Step: 1
Training loss: 4.1380542764178525
Validation loss: 3.3856105827787193

Epoch: 6| Step: 2
Training loss: 3.206206537439186
Validation loss: 3.383356211659854

Epoch: 6| Step: 3
Training loss: 4.037946240592976
Validation loss: 3.379737483836653

Epoch: 6| Step: 4
Training loss: 4.174738783288292
Validation loss: 3.375884547474589

Epoch: 6| Step: 5
Training loss: 3.4003813698000123
Validation loss: 3.3714617112470724

Epoch: 6| Step: 6
Training loss: 3.5297271685758544
Validation loss: 3.3704823269920867

Epoch: 6| Step: 7
Training loss: 2.8098306492176204
Validation loss: 3.3661491110650115

Epoch: 6| Step: 8
Training loss: 3.53907314408409
Validation loss: 3.3656110632417984

Epoch: 6| Step: 9
Training loss: 3.88917017933619
Validation loss: 3.3595021030413026

Epoch: 6| Step: 10
Training loss: 2.682659847335541
Validation loss: 3.35883206680289

Epoch: 6| Step: 11
Training loss: 4.710307874720951
Validation loss: 3.35970278202273

Epoch: 6| Step: 12
Training loss: 3.1914386724274872
Validation loss: 3.354785413975418

Epoch: 6| Step: 13
Training loss: 3.7544623210183223
Validation loss: 3.351844225596147

Epoch: 19| Step: 0
Training loss: 3.8876507892373278
Validation loss: 3.346607432168657

Epoch: 6| Step: 1
Training loss: 3.078115240555991
Validation loss: 3.344545412570534

Epoch: 6| Step: 2
Training loss: 3.7004090985103106
Validation loss: 3.3389813431178332

Epoch: 6| Step: 3
Training loss: 3.716213868161521
Validation loss: 3.341344447068838

Epoch: 6| Step: 4
Training loss: 3.5291786532928464
Validation loss: 3.338810710974426

Epoch: 6| Step: 5
Training loss: 3.934364402459789
Validation loss: 3.335732512345418

Epoch: 6| Step: 6
Training loss: 3.6502652215618716
Validation loss: 3.329766576118665

Epoch: 6| Step: 7
Training loss: 2.571461151310234
Validation loss: 3.327432259453431

Epoch: 6| Step: 8
Training loss: 3.8666739074595387
Validation loss: 3.3255749186523844

Epoch: 6| Step: 9
Training loss: 2.996532820816089
Validation loss: 3.320271144080979

Epoch: 6| Step: 10
Training loss: 3.5647899562598324
Validation loss: 3.3177140678986867

Epoch: 6| Step: 11
Training loss: 3.8193697712081702
Validation loss: 3.3184949353509006

Epoch: 6| Step: 12
Training loss: 3.9157887685615367
Validation loss: 3.3164761578892867

Epoch: 6| Step: 13
Training loss: 3.330253242967039
Validation loss: 3.309760759899858

Epoch: 20| Step: 0
Training loss: 3.805468469320971
Validation loss: 3.308734061790078

Epoch: 6| Step: 1
Training loss: 3.5779228319916307
Validation loss: 3.305514785174158

Epoch: 6| Step: 2
Training loss: 3.007838657810365
Validation loss: 3.302746216954871

Epoch: 6| Step: 3
Training loss: 3.0140096656831648
Validation loss: 3.298996616383089

Epoch: 6| Step: 4
Training loss: 3.299378515734121
Validation loss: 3.298764315236906

Epoch: 6| Step: 5
Training loss: 2.720330754393583
Validation loss: 3.2950190250136857

Epoch: 6| Step: 6
Training loss: 3.503072343850815
Validation loss: 3.2926167687163646

Epoch: 6| Step: 7
Training loss: 3.618389679622165
Validation loss: 3.292764228113096

Epoch: 6| Step: 8
Training loss: 4.008325257693039
Validation loss: 3.2883224155451494

Epoch: 6| Step: 9
Training loss: 3.9549852901913405
Validation loss: 3.2862682590162953

Epoch: 6| Step: 10
Training loss: 3.6679814900104892
Validation loss: 3.2789679721729366

Epoch: 6| Step: 11
Training loss: 3.5662139807325177
Validation loss: 3.2783305597886057

Epoch: 6| Step: 12
Training loss: 4.136853843550723
Validation loss: 3.2727908765189238

Epoch: 6| Step: 13
Training loss: 3.0833225421888515
Validation loss: 3.2735457660277607

Epoch: 21| Step: 0
Training loss: 3.8188110396631085
Validation loss: 3.2719098533702065

Epoch: 6| Step: 1
Training loss: 3.1722698999382444
Validation loss: 3.2729774477025657

Epoch: 6| Step: 2
Training loss: 3.704551969098024
Validation loss: 3.2702417057397444

Epoch: 6| Step: 3
Training loss: 3.9389907194843192
Validation loss: 3.264319192004544

Epoch: 6| Step: 4
Training loss: 3.0895688866663167
Validation loss: 3.2630154596435124

Epoch: 6| Step: 5
Training loss: 2.808879811199156
Validation loss: 3.2647757325012248

Epoch: 6| Step: 6
Training loss: 2.831238608095269
Validation loss: 3.257155696391414

Epoch: 6| Step: 7
Training loss: 3.9554913943036403
Validation loss: 3.2576731317596295

Epoch: 6| Step: 8
Training loss: 3.437370714010508
Validation loss: 3.2505256773556925

Epoch: 6| Step: 9
Training loss: 3.563464920806895
Validation loss: 3.244046193440012

Epoch: 6| Step: 10
Training loss: 3.4985135873048487
Validation loss: 3.2394076090215473

Epoch: 6| Step: 11
Training loss: 3.1400033515863695
Validation loss: 3.2429343628361433

Epoch: 6| Step: 12
Training loss: 4.358294609486166
Validation loss: 3.240250947220777

Epoch: 6| Step: 13
Training loss: 3.204850787945992
Validation loss: 3.247426600493499

Epoch: 22| Step: 0
Training loss: 3.6703850503103217
Validation loss: 3.2359321392529066

Epoch: 6| Step: 1
Training loss: 3.7405026013482945
Validation loss: 3.234766197712004

Epoch: 6| Step: 2
Training loss: 4.153049010764914
Validation loss: 3.2286444922910253

Epoch: 6| Step: 3
Training loss: 3.0726055095487883
Validation loss: 3.2330453565141126

Epoch: 6| Step: 4
Training loss: 3.4076805260599166
Validation loss: 3.2315051822775436

Epoch: 6| Step: 5
Training loss: 3.336310837020541
Validation loss: 3.227272838037757

Epoch: 6| Step: 6
Training loss: 3.237052362197045
Validation loss: 3.2203629989331723

Epoch: 6| Step: 7
Training loss: 3.150116503544096
Validation loss: 3.215744436038223

Epoch: 6| Step: 8
Training loss: 3.4639462036378075
Validation loss: 3.2149320188664197

Epoch: 6| Step: 9
Training loss: 3.418838477515332
Validation loss: 3.2129009509102238

Epoch: 6| Step: 10
Training loss: 3.72524559696276
Validation loss: 3.2090743979863485

Epoch: 6| Step: 11
Training loss: 3.8479084377617343
Validation loss: 3.2053255083426997

Epoch: 6| Step: 12
Training loss: 2.912913896350819
Validation loss: 3.2023115284142945

Epoch: 6| Step: 13
Training loss: 3.1640365787904265
Validation loss: 3.2011494272521848

Epoch: 23| Step: 0
Training loss: 2.9222849186052704
Validation loss: 3.202605896591722

Epoch: 6| Step: 1
Training loss: 3.3295837930382954
Validation loss: 3.2054836774012223

Epoch: 6| Step: 2
Training loss: 3.391809212753243
Validation loss: 3.1983693809739098

Epoch: 6| Step: 3
Training loss: 2.7003265183425618
Validation loss: 3.1944108602486216

Epoch: 6| Step: 4
Training loss: 3.7018337654368962
Validation loss: 3.19492746606696

Epoch: 6| Step: 5
Training loss: 2.911241408417711
Validation loss: 3.1926780399070456

Epoch: 6| Step: 6
Training loss: 3.2717321501319496
Validation loss: 3.1901629522087704

Epoch: 6| Step: 7
Training loss: 3.645960095336298
Validation loss: 3.189149496122402

Epoch: 6| Step: 8
Training loss: 3.056686800769429
Validation loss: 3.186135167345481

Epoch: 6| Step: 9
Training loss: 3.3859052726081518
Validation loss: 3.185989089348009

Epoch: 6| Step: 10
Training loss: 4.060391392375536
Validation loss: 3.1798365151056127

Epoch: 6| Step: 11
Training loss: 4.620538982972015
Validation loss: 3.177459136722597

Epoch: 6| Step: 12
Training loss: 3.7426223661489204
Validation loss: 3.174025271417838

Epoch: 6| Step: 13
Training loss: 2.6810590164706527
Validation loss: 3.1764322096133513

Epoch: 24| Step: 0
Training loss: 3.4569552483259343
Validation loss: 3.1708245972209594

Epoch: 6| Step: 1
Training loss: 3.4621728625497283
Validation loss: 3.173591683061125

Epoch: 6| Step: 2
Training loss: 3.8306940916916212
Validation loss: 3.1726636921906697

Epoch: 6| Step: 3
Training loss: 3.615613953516644
Validation loss: 3.1674290215950176

Epoch: 6| Step: 4
Training loss: 2.8575663865755634
Validation loss: 3.1615875266937987

Epoch: 6| Step: 5
Training loss: 3.733381961324159
Validation loss: 3.1647365729632675

Epoch: 6| Step: 6
Training loss: 3.507628574094776
Validation loss: 3.160931038745101

Epoch: 6| Step: 7
Training loss: 3.1290327372802733
Validation loss: 3.161775018908541

Epoch: 6| Step: 8
Training loss: 2.92487886618163
Validation loss: 3.162045681199384

Epoch: 6| Step: 9
Training loss: 3.690486539073211
Validation loss: 3.154187090676424

Epoch: 6| Step: 10
Training loss: 3.681388445648781
Validation loss: 3.15441722080898

Epoch: 6| Step: 11
Training loss: 3.0533864404260456
Validation loss: 3.1685803178907856

Epoch: 6| Step: 12
Training loss: 3.8480555293305634
Validation loss: 3.1725050343271417

Epoch: 6| Step: 13
Training loss: 2.501658557045957
Validation loss: 3.149953600236878

Epoch: 25| Step: 0
Training loss: 3.6131664360981612
Validation loss: 3.144815106714478

Epoch: 6| Step: 1
Training loss: 2.965920475187546
Validation loss: 3.1428387449195077

Epoch: 6| Step: 2
Training loss: 4.453257347951056
Validation loss: 3.1480645705175756

Epoch: 6| Step: 3
Training loss: 4.27591790946761
Validation loss: 3.151190789495156

Epoch: 6| Step: 4
Training loss: 3.463687536088706
Validation loss: 3.1442270549493125

Epoch: 6| Step: 5
Training loss: 3.023165747285299
Validation loss: 3.141371030506755

Epoch: 6| Step: 6
Training loss: 3.1434553307826687
Validation loss: 3.136625672028554

Epoch: 6| Step: 7
Training loss: 2.5697964321216507
Validation loss: 3.1366985609872695

Epoch: 6| Step: 8
Training loss: 3.27259120152552
Validation loss: 3.1380792090108787

Epoch: 6| Step: 9
Training loss: 3.326964843270721
Validation loss: 3.133706571092831

Epoch: 6| Step: 10
Training loss: 2.4655261659334187
Validation loss: 3.1394772963774025

Epoch: 6| Step: 11
Training loss: 3.760850467475209
Validation loss: 3.1396342762584797

Epoch: 6| Step: 12
Training loss: 3.1552451772082466
Validation loss: 3.1381882432389263

Epoch: 6| Step: 13
Training loss: 3.778842380213426
Validation loss: 3.1335004410684077

Epoch: 26| Step: 0
Training loss: 3.806299514323172
Validation loss: 3.124026183298157

Epoch: 6| Step: 1
Training loss: 3.4778315253229666
Validation loss: 3.120476368688075

Epoch: 6| Step: 2
Training loss: 3.8363744172236913
Validation loss: 3.114001720082643

Epoch: 6| Step: 3
Training loss: 2.640713662007745
Validation loss: 3.1130068024441018

Epoch: 6| Step: 4
Training loss: 2.3277528196507657
Validation loss: 3.1111432256105616

Epoch: 6| Step: 5
Training loss: 3.6438819822091597
Validation loss: 3.1065917061125057

Epoch: 6| Step: 6
Training loss: 3.7612456502537475
Validation loss: 3.1080223402543683

Epoch: 6| Step: 7
Training loss: 3.5378638252286954
Validation loss: 3.10604585524891

Epoch: 6| Step: 8
Training loss: 3.6445916958914264
Validation loss: 3.102395001304894

Epoch: 6| Step: 9
Training loss: 3.607404162534345
Validation loss: 3.101025279296446

Epoch: 6| Step: 10
Training loss: 2.894870094718485
Validation loss: 3.10155129176872

Epoch: 6| Step: 11
Training loss: 3.8585190538126484
Validation loss: 3.0929638247061186

Epoch: 6| Step: 12
Training loss: 3.1294024165746537
Validation loss: 3.0922709161419437

Epoch: 6| Step: 13
Training loss: 1.9533092564454066
Validation loss: 3.091907937678108

Epoch: 27| Step: 0
Training loss: 2.4977140465887224
Validation loss: 3.0882474512251754

Epoch: 6| Step: 1
Training loss: 3.378484128482579
Validation loss: 3.095789229313294

Epoch: 6| Step: 2
Training loss: 3.4760390669862793
Validation loss: 3.102458865348887

Epoch: 6| Step: 3
Training loss: 4.0127622144352895
Validation loss: 3.087678967103593

Epoch: 6| Step: 4
Training loss: 3.1562116261783895
Validation loss: 3.0871631087286557

Epoch: 6| Step: 5
Training loss: 3.0727546433660526
Validation loss: 3.083009424021785

Epoch: 6| Step: 6
Training loss: 2.419550610896868
Validation loss: 3.0824044338014738

Epoch: 6| Step: 7
Training loss: 3.7569516915004466
Validation loss: 3.082434887236064

Epoch: 6| Step: 8
Training loss: 3.831381646487977
Validation loss: 3.079782672057762

Epoch: 6| Step: 9
Training loss: 3.051284181996452
Validation loss: 3.0797257929851485

Epoch: 6| Step: 10
Training loss: 3.9778966557967816
Validation loss: 3.0857593373315035

Epoch: 6| Step: 11
Training loss: 3.894290040408295
Validation loss: 3.093654343961808

Epoch: 6| Step: 12
Training loss: 3.1990512156606563
Validation loss: 3.0714485033036047

Epoch: 6| Step: 13
Training loss: 2.3691437446844157
Validation loss: 3.064986536474564

Epoch: 28| Step: 0
Training loss: 3.7756462503993005
Validation loss: 3.0713408826289528

Epoch: 6| Step: 1
Training loss: 3.3338992751020355
Validation loss: 3.070284422931568

Epoch: 6| Step: 2
Training loss: 2.8547566122073187
Validation loss: 3.085414929353563

Epoch: 6| Step: 3
Training loss: 3.2187116305832757
Validation loss: 3.081627706311167

Epoch: 6| Step: 4
Training loss: 3.821577677854524
Validation loss: 3.077551124087934

Epoch: 6| Step: 5
Training loss: 3.436208863967894
Validation loss: 3.0673763603133213

Epoch: 6| Step: 6
Training loss: 3.3781078299369236
Validation loss: 3.063059524041323

Epoch: 6| Step: 7
Training loss: 3.4626738797263115
Validation loss: 3.061621564456463

Epoch: 6| Step: 8
Training loss: 2.007221773280508
Validation loss: 3.066228527663795

Epoch: 6| Step: 9
Training loss: 2.9842404639791655
Validation loss: 3.065592430215773

Epoch: 6| Step: 10
Training loss: 4.048259483554804
Validation loss: 3.070878091453868

Epoch: 6| Step: 11
Training loss: 3.173526728477689
Validation loss: 3.067021646765572

Epoch: 6| Step: 12
Training loss: 3.3847082873912284
Validation loss: 3.0674736464814276

Epoch: 6| Step: 13
Training loss: 3.706181419728719
Validation loss: 3.0757327088801865

Epoch: 29| Step: 0
Training loss: 2.49019933339316
Validation loss: 3.0763982078261534

Epoch: 6| Step: 1
Training loss: 3.162190653632294
Validation loss: 3.074372176734309

Epoch: 6| Step: 2
Training loss: 2.520085520731254
Validation loss: 3.0516587651937446

Epoch: 6| Step: 3
Training loss: 3.100877047111625
Validation loss: 3.0422891751568524

Epoch: 6| Step: 4
Training loss: 3.9726806159758543
Validation loss: 3.041878634854971

Epoch: 6| Step: 5
Training loss: 3.3560290329578724
Validation loss: 3.0503945779108173

Epoch: 6| Step: 6
Training loss: 3.8451850507278347
Validation loss: 3.049537597952821

Epoch: 6| Step: 7
Training loss: 3.0456130324264987
Validation loss: 3.0442733254454013

Epoch: 6| Step: 8
Training loss: 4.0471179550472
Validation loss: 3.0406838055555547

Epoch: 6| Step: 9
Training loss: 2.653980137984258
Validation loss: 3.035513544067834

Epoch: 6| Step: 10
Training loss: 3.745008834604352
Validation loss: 3.0427070088185

Epoch: 6| Step: 11
Training loss: 3.466341299291492
Validation loss: 3.040551051112704

Epoch: 6| Step: 12
Training loss: 3.2579380807068334
Validation loss: 3.0363508244960586

Epoch: 6| Step: 13
Training loss: 3.5983212795529793
Validation loss: 3.02918510103595

Epoch: 30| Step: 0
Training loss: 3.3754605579493124
Validation loss: 3.0259871004648664

Epoch: 6| Step: 1
Training loss: 3.2129743292667845
Validation loss: 3.026793586737279

Epoch: 6| Step: 2
Training loss: 3.2080819828278893
Validation loss: 3.022681341793154

Epoch: 6| Step: 3
Training loss: 2.7707258696021198
Validation loss: 3.0225609500814317

Epoch: 6| Step: 4
Training loss: 3.849710696361314
Validation loss: 3.025474497023251

Epoch: 6| Step: 5
Training loss: 3.4301765805715263
Validation loss: 3.018809325700349

Epoch: 6| Step: 6
Training loss: 3.588899075113439
Validation loss: 3.005444747256728

Epoch: 6| Step: 7
Training loss: 3.546313539674642
Validation loss: 3.002912771979523

Epoch: 6| Step: 8
Training loss: 4.169275967247126
Validation loss: 3.0072729540116816

Epoch: 6| Step: 9
Training loss: 3.026005563613608
Validation loss: 3.0016515041750305

Epoch: 6| Step: 10
Training loss: 2.8823039942566115
Validation loss: 3.0050757088070856

Epoch: 6| Step: 11
Training loss: 3.025778167510291
Validation loss: 3.005404258544971

Epoch: 6| Step: 12
Training loss: 2.9327568557004864
Validation loss: 3.0046207535087555

Epoch: 6| Step: 13
Training loss: 2.5831163581871572
Validation loss: 3.0001702388534706

Epoch: 31| Step: 0
Training loss: 3.34608720043538
Validation loss: 3.0124892131680547

Epoch: 6| Step: 1
Training loss: 3.8684130643631693
Validation loss: 3.007078092890324

Epoch: 6| Step: 2
Training loss: 3.4892636888157207
Validation loss: 3.0005186463228837

Epoch: 6| Step: 3
Training loss: 3.1312198538480693
Validation loss: 2.9944777406120893

Epoch: 6| Step: 4
Training loss: 3.624996711466383
Validation loss: 2.9942410646751085

Epoch: 6| Step: 5
Training loss: 3.0308759006227204
Validation loss: 2.9929981421439895

Epoch: 6| Step: 6
Training loss: 2.7947098860195454
Validation loss: 2.9944645006438844

Epoch: 6| Step: 7
Training loss: 3.8096516709782025
Validation loss: 2.9936863602705874

Epoch: 6| Step: 8
Training loss: 2.446248810640649
Validation loss: 2.9984138766780144

Epoch: 6| Step: 9
Training loss: 3.3526266760507775
Validation loss: 3.004613463479844

Epoch: 6| Step: 10
Training loss: 2.655007733838443
Validation loss: 2.992619010864368

Epoch: 6| Step: 11
Training loss: 3.620360200359445
Validation loss: 2.989122027081707

Epoch: 6| Step: 12
Training loss: 3.2318589011259617
Validation loss: 2.9896913977476993

Epoch: 6| Step: 13
Training loss: 3.3376991926893154
Validation loss: 2.984388063961952

Epoch: 32| Step: 0
Training loss: 3.447127852239818
Validation loss: 2.986038700366371

Epoch: 6| Step: 1
Training loss: 3.2759161330260147
Validation loss: 2.9856123272465567

Epoch: 6| Step: 2
Training loss: 2.638528386527003
Validation loss: 2.98421966263576

Epoch: 6| Step: 3
Training loss: 3.2290399013781763
Validation loss: 2.9812951711218765

Epoch: 6| Step: 4
Training loss: 3.3727813775252398
Validation loss: 2.9876334169974137

Epoch: 6| Step: 5
Training loss: 3.189916554857372
Validation loss: 2.98019899747895

Epoch: 6| Step: 6
Training loss: 4.056269401421604
Validation loss: 2.9779837563448406

Epoch: 6| Step: 7
Training loss: 2.683540443657248
Validation loss: 2.978813224999591

Epoch: 6| Step: 8
Training loss: 2.659646588991199
Validation loss: 2.9748940567975297

Epoch: 6| Step: 9
Training loss: 3.5566449152225634
Validation loss: 2.9734268956404315

Epoch: 6| Step: 10
Training loss: 3.539820171034359
Validation loss: 2.98156198530634

Epoch: 6| Step: 11
Training loss: 3.3251673491390283
Validation loss: 2.991545023110242

Epoch: 6| Step: 12
Training loss: 3.4122167001581083
Validation loss: 2.9744379205600384

Epoch: 6| Step: 13
Training loss: 3.213639581920753
Validation loss: 2.979206829860195

Epoch: 33| Step: 0
Training loss: 3.754358429126802
Validation loss: 2.9853974549273796

Epoch: 6| Step: 1
Training loss: 3.227822858882624
Validation loss: 2.987872807712966

Epoch: 6| Step: 2
Training loss: 3.355542787172503
Validation loss: 2.985073602919494

Epoch: 6| Step: 3
Training loss: 2.839035821767283
Validation loss: 2.98262173522063

Epoch: 6| Step: 4
Training loss: 3.638762165286434
Validation loss: 2.980840566545862

Epoch: 6| Step: 5
Training loss: 3.2025305219590945
Validation loss: 2.9850928764936366

Epoch: 6| Step: 6
Training loss: 2.774644186216932
Validation loss: 2.981037827279383

Epoch: 6| Step: 7
Training loss: 3.3293978983594896
Validation loss: 2.9776928508408265

Epoch: 6| Step: 8
Training loss: 2.6281354434500996
Validation loss: 2.9777004159869906

Epoch: 6| Step: 9
Training loss: 3.005850808650074
Validation loss: 2.980325427175131

Epoch: 6| Step: 10
Training loss: 3.6948196695430067
Validation loss: 2.9743993506973214

Epoch: 6| Step: 11
Training loss: 3.6122937311860746
Validation loss: 2.978422721867713

Epoch: 6| Step: 12
Training loss: 3.015370728836542
Validation loss: 2.974316096697881

Epoch: 6| Step: 13
Training loss: 3.8251916974469133
Validation loss: 2.9741764924248097

Epoch: 34| Step: 0
Training loss: 2.873999214135091
Validation loss: 2.9760674062266284

Epoch: 6| Step: 1
Training loss: 3.7325212674004526
Validation loss: 2.9708463597575507

Epoch: 6| Step: 2
Training loss: 3.168156139697109
Validation loss: 2.967884843082037

Epoch: 6| Step: 3
Training loss: 2.918067123613684
Validation loss: 2.9694054700875423

Epoch: 6| Step: 4
Training loss: 3.063112937157834
Validation loss: 2.971629450866498

Epoch: 6| Step: 5
Training loss: 2.4617209504413364
Validation loss: 2.9724711750683857

Epoch: 6| Step: 6
Training loss: 3.5358919674263323
Validation loss: 2.971974458629617

Epoch: 6| Step: 7
Training loss: 3.330861287582276
Validation loss: 2.9655706318295376

Epoch: 6| Step: 8
Training loss: 3.79105662933453
Validation loss: 2.9632911445760017

Epoch: 6| Step: 9
Training loss: 3.2373282545333733
Validation loss: 2.9621045987343826

Epoch: 6| Step: 10
Training loss: 3.3961448916561365
Validation loss: 2.956144195094465

Epoch: 6| Step: 11
Training loss: 3.2741825102664435
Validation loss: 2.9606787763110654

Epoch: 6| Step: 12
Training loss: 3.4501689150826427
Validation loss: 2.957591179655646

Epoch: 6| Step: 13
Training loss: 3.2854680152674907
Validation loss: 2.9592903688619434

Epoch: 35| Step: 0
Training loss: 3.300336855254975
Validation loss: 2.956018786570077

Epoch: 6| Step: 1
Training loss: 3.126033154410027
Validation loss: 2.9548727804508745

Epoch: 6| Step: 2
Training loss: 2.4793780474479377
Validation loss: 2.956848786524159

Epoch: 6| Step: 3
Training loss: 2.5514857208791306
Validation loss: 2.950389572013051

Epoch: 6| Step: 4
Training loss: 3.7092967692789167
Validation loss: 2.9503899743211206

Epoch: 6| Step: 5
Training loss: 3.0016438430971824
Validation loss: 2.959231295105063

Epoch: 6| Step: 6
Training loss: 3.869977957689859
Validation loss: 2.958623417507839

Epoch: 6| Step: 7
Training loss: 3.219026720318278
Validation loss: 2.956114449144875

Epoch: 6| Step: 8
Training loss: 4.20670707679197
Validation loss: 2.9560964106389815

Epoch: 6| Step: 9
Training loss: 3.3000351643856223
Validation loss: 2.950006584603685

Epoch: 6| Step: 10
Training loss: 3.0385507971663808
Validation loss: 2.956511742742716

Epoch: 6| Step: 11
Training loss: 2.976422003686769
Validation loss: 2.951401617773517

Epoch: 6| Step: 12
Training loss: 3.331629094924503
Validation loss: 2.9498551767853796

Epoch: 6| Step: 13
Training loss: 2.828068685234381
Validation loss: 2.9505465861654705

Epoch: 36| Step: 0
Training loss: 3.1255251633920773
Validation loss: 2.952345177864589

Epoch: 6| Step: 1
Training loss: 2.579735270380777
Validation loss: 2.945997406429436

Epoch: 6| Step: 2
Training loss: 3.5156167263357503
Validation loss: 2.9471957118932166

Epoch: 6| Step: 3
Training loss: 3.349785891850438
Validation loss: 2.949862424848539

Epoch: 6| Step: 4
Training loss: 3.0203519941926045
Validation loss: 2.94293410189899

Epoch: 6| Step: 5
Training loss: 3.2467036035879193
Validation loss: 2.949056893519572

Epoch: 6| Step: 6
Training loss: 3.5074651669372043
Validation loss: 2.953166686401503

Epoch: 6| Step: 7
Training loss: 3.3546789658699523
Validation loss: 2.9533673475806688

Epoch: 6| Step: 8
Training loss: 3.5701269857010356
Validation loss: 2.944040839896196

Epoch: 6| Step: 9
Training loss: 3.543089606969134
Validation loss: 2.947159586485024

Epoch: 6| Step: 10
Training loss: 2.3964348300578413
Validation loss: 2.9461882863455022

Epoch: 6| Step: 11
Training loss: 3.3523423503587444
Validation loss: 2.9458752714629894

Epoch: 6| Step: 12
Training loss: 3.406477474351498
Validation loss: 2.9442865517112278

Epoch: 6| Step: 13
Training loss: 3.2297347686807645
Validation loss: 2.9475767405691857

Epoch: 37| Step: 0
Training loss: 2.8139041362557307
Validation loss: 2.947111720724439

Epoch: 6| Step: 1
Training loss: 3.4272037627946585
Validation loss: 2.9433177038645595

Epoch: 6| Step: 2
Training loss: 3.2498446207317246
Validation loss: 2.9391165338975065

Epoch: 6| Step: 3
Training loss: 3.3828209575168526
Validation loss: 2.9411567626087143

Epoch: 6| Step: 4
Training loss: 3.3732314951103626
Validation loss: 2.9395198977011425

Epoch: 6| Step: 5
Training loss: 2.8843953405719005
Validation loss: 2.936887319812494

Epoch: 6| Step: 6
Training loss: 3.2367885264417238
Validation loss: 2.936889731664021

Epoch: 6| Step: 7
Training loss: 3.2746920491807736
Validation loss: 2.9380027063317455

Epoch: 6| Step: 8
Training loss: 3.416902875101008
Validation loss: 2.937091401883574

Epoch: 6| Step: 9
Training loss: 3.003001301229413
Validation loss: 2.934322072053836

Epoch: 6| Step: 10
Training loss: 2.9858511424689125
Validation loss: 2.9387990278083227

Epoch: 6| Step: 11
Training loss: 3.5704615455418867
Validation loss: 2.935894555555779

Epoch: 6| Step: 12
Training loss: 3.106020550917158
Validation loss: 2.934224322041373

Epoch: 6| Step: 13
Training loss: 3.707251072949241
Validation loss: 2.9321629414322286

Epoch: 38| Step: 0
Training loss: 3.218335763886566
Validation loss: 2.940649223038253

Epoch: 6| Step: 1
Training loss: 2.4325580873052166
Validation loss: 2.938358233003184

Epoch: 6| Step: 2
Training loss: 3.593651944356907
Validation loss: 2.943665110192258

Epoch: 6| Step: 3
Training loss: 3.2719198637385
Validation loss: 2.949095933912152

Epoch: 6| Step: 4
Training loss: 3.958246223428149
Validation loss: 2.9645188347972176

Epoch: 6| Step: 5
Training loss: 3.0532104355289773
Validation loss: 2.9530266656806323

Epoch: 6| Step: 6
Training loss: 3.10600796222833
Validation loss: 2.940088031372508

Epoch: 6| Step: 7
Training loss: 3.0165018180640617
Validation loss: 2.940177245529662

Epoch: 6| Step: 8
Training loss: 3.5351236078430825
Validation loss: 2.931433180417212

Epoch: 6| Step: 9
Training loss: 2.171413454920991
Validation loss: 2.9334421837048548

Epoch: 6| Step: 10
Training loss: 3.3222514157268885
Validation loss: 2.934165739669444

Epoch: 6| Step: 11
Training loss: 3.42841400625225
Validation loss: 2.9424444950245485

Epoch: 6| Step: 12
Training loss: 3.344116190885797
Validation loss: 2.9446781332976055

Epoch: 6| Step: 13
Training loss: 3.76607045431576
Validation loss: 2.9364311230242097

Epoch: 39| Step: 0
Training loss: 3.353266214061771
Validation loss: 2.9288598408772826

Epoch: 6| Step: 1
Training loss: 3.5653584038769095
Validation loss: 2.931568229982562

Epoch: 6| Step: 2
Training loss: 2.761375481451327
Validation loss: 2.9312964358247218

Epoch: 6| Step: 3
Training loss: 3.156736732948673
Validation loss: 2.932962105376312

Epoch: 6| Step: 4
Training loss: 2.958961169463462
Validation loss: 2.9417485181781764

Epoch: 6| Step: 5
Training loss: 3.507345937443631
Validation loss: 2.9450207211019155

Epoch: 6| Step: 6
Training loss: 3.64977271012474
Validation loss: 2.935896022541432

Epoch: 6| Step: 7
Training loss: 3.1299969204531015
Validation loss: 2.9300577600510196

Epoch: 6| Step: 8
Training loss: 3.1230371032523796
Validation loss: 2.929019319419577

Epoch: 6| Step: 9
Training loss: 2.9897814606377247
Validation loss: 2.9294761120954482

Epoch: 6| Step: 10
Training loss: 2.943947230249657
Validation loss: 2.9345633385428846

Epoch: 6| Step: 11
Training loss: 4.321996874411396
Validation loss: 2.9352209273621215

Epoch: 6| Step: 12
Training loss: 2.78749694481926
Validation loss: 2.934842387850136

Epoch: 6| Step: 13
Training loss: 2.185202891097861
Validation loss: 2.933214937568017

Epoch: 40| Step: 0
Training loss: 3.6241683005858985
Validation loss: 2.929084674903221

Epoch: 6| Step: 1
Training loss: 3.2734337126134117
Validation loss: 2.926693496490957

Epoch: 6| Step: 2
Training loss: 2.8374957382909556
Validation loss: 2.9263665723734684

Epoch: 6| Step: 3
Training loss: 2.4901910995014283
Validation loss: 2.9262843382464556

Epoch: 6| Step: 4
Training loss: 3.3198138412311144
Validation loss: 2.926400843211361

Epoch: 6| Step: 5
Training loss: 3.304562541502013
Validation loss: 2.9287475740770907

Epoch: 6| Step: 6
Training loss: 3.27647823139878
Validation loss: 2.9304956716356543

Epoch: 6| Step: 7
Training loss: 3.3506393050344
Validation loss: 2.931986970188069

Epoch: 6| Step: 8
Training loss: 3.697948072416521
Validation loss: 2.9295792693241958

Epoch: 6| Step: 9
Training loss: 2.4414079101556854
Validation loss: 2.9249537634422715

Epoch: 6| Step: 10
Training loss: 3.7812300752477372
Validation loss: 2.9337961474915195

Epoch: 6| Step: 11
Training loss: 3.289174565016705
Validation loss: 2.9212868996710677

Epoch: 6| Step: 12
Training loss: 2.519972178593206
Validation loss: 2.927720051249711

Epoch: 6| Step: 13
Training loss: 3.854730320860023
Validation loss: 2.9264334097719384

Epoch: 41| Step: 0
Training loss: 3.067391051554917
Validation loss: 2.924650472563222

Epoch: 6| Step: 1
Training loss: 2.878951963892484
Validation loss: 2.9176170933537153

Epoch: 6| Step: 2
Training loss: 3.8223606852161063
Validation loss: 2.915757898774677

Epoch: 6| Step: 3
Training loss: 3.3203002929463103
Validation loss: 2.9169691277643044

Epoch: 6| Step: 4
Training loss: 3.317966053932196
Validation loss: 2.914444072747974

Epoch: 6| Step: 5
Training loss: 3.106808317117669
Validation loss: 2.9152853034125434

Epoch: 6| Step: 6
Training loss: 3.2012228179099353
Validation loss: 2.9139880318998883

Epoch: 6| Step: 7
Training loss: 3.484328060090564
Validation loss: 2.9150496950706426

Epoch: 6| Step: 8
Training loss: 3.6085539705570224
Validation loss: 2.917800799428117

Epoch: 6| Step: 9
Training loss: 3.3562729818740045
Validation loss: 2.91431141230434

Epoch: 6| Step: 10
Training loss: 2.712322134347916
Validation loss: 2.9125532797902243

Epoch: 6| Step: 11
Training loss: 2.6565031043245804
Validation loss: 2.9150274730417705

Epoch: 6| Step: 12
Training loss: 3.647131349106681
Validation loss: 2.913798639314518

Epoch: 6| Step: 13
Training loss: 2.168919040567193
Validation loss: 2.915529511678094

Epoch: 42| Step: 0
Training loss: 3.607906819098216
Validation loss: 2.9202783914380994

Epoch: 6| Step: 1
Training loss: 2.9139628457218802
Validation loss: 2.9123979918461567

Epoch: 6| Step: 2
Training loss: 3.0782718526596953
Validation loss: 2.921520214027356

Epoch: 6| Step: 3
Training loss: 2.8727379898007466
Validation loss: 2.9285646842414965

Epoch: 6| Step: 4
Training loss: 2.719143192315804
Validation loss: 2.9251663791036977

Epoch: 6| Step: 5
Training loss: 2.964448724095758
Validation loss: 2.912069079888826

Epoch: 6| Step: 6
Training loss: 3.5115358434447685
Validation loss: 2.909038333881519

Epoch: 6| Step: 7
Training loss: 3.577327589442983
Validation loss: 2.910066551559907

Epoch: 6| Step: 8
Training loss: 3.0647188828756686
Validation loss: 2.910541065378661

Epoch: 6| Step: 9
Training loss: 2.983262099227211
Validation loss: 2.9097506306318137

Epoch: 6| Step: 10
Training loss: 2.8066657695028847
Validation loss: 2.9105100870420033

Epoch: 6| Step: 11
Training loss: 3.441506842226639
Validation loss: 2.9102257156973184

Epoch: 6| Step: 12
Training loss: 3.4597506242530027
Validation loss: 2.9092592451293005

Epoch: 6| Step: 13
Training loss: 4.244545297162357
Validation loss: 2.9176304948919984

Epoch: 43| Step: 0
Training loss: 3.426651638557145
Validation loss: 2.9099297887343014

Epoch: 6| Step: 1
Training loss: 3.2017978803233147
Validation loss: 2.907716797967612

Epoch: 6| Step: 2
Training loss: 3.130608979966872
Validation loss: 2.9072400401536354

Epoch: 6| Step: 3
Training loss: 2.624693171416345
Validation loss: 2.908807515065231

Epoch: 6| Step: 4
Training loss: 3.0682421976949312
Validation loss: 2.9082758618718687

Epoch: 6| Step: 5
Training loss: 3.4633839659772887
Validation loss: 2.910115179044748

Epoch: 6| Step: 6
Training loss: 2.9658063248250164
Validation loss: 2.908250076165188

Epoch: 6| Step: 7
Training loss: 3.1890256633808485
Validation loss: 2.9118703886315176

Epoch: 6| Step: 8
Training loss: 3.2767863112137645
Validation loss: 2.909092229933616

Epoch: 6| Step: 9
Training loss: 3.6803944550450787
Validation loss: 2.9091531563507225

Epoch: 6| Step: 10
Training loss: 3.5370105580050173
Validation loss: 2.903501665062513

Epoch: 6| Step: 11
Training loss: 2.8899791099030923
Validation loss: 2.9080902793730914

Epoch: 6| Step: 12
Training loss: 3.3081553054940684
Validation loss: 2.9060306111214973

Epoch: 6| Step: 13
Training loss: 2.9662465030152823
Validation loss: 2.9034857136823224

Epoch: 44| Step: 0
Training loss: 2.820508532698691
Validation loss: 2.9045363786041376

Epoch: 6| Step: 1
Training loss: 3.618849568158429
Validation loss: 2.9048399047624542

Epoch: 6| Step: 2
Training loss: 3.4620994528992286
Validation loss: 2.905819856089367

Epoch: 6| Step: 3
Training loss: 2.6115776405018836
Validation loss: 2.9031252920817168

Epoch: 6| Step: 4
Training loss: 3.893171220261004
Validation loss: 2.9053492054788665

Epoch: 6| Step: 5
Training loss: 3.1823985846123635
Validation loss: 2.901949742245188

Epoch: 6| Step: 6
Training loss: 2.2817389604341756
Validation loss: 2.9030106374177707

Epoch: 6| Step: 7
Training loss: 2.393411147234894
Validation loss: 2.9018934996450296

Epoch: 6| Step: 8
Training loss: 3.852223430511468
Validation loss: 2.903348919891255

Epoch: 6| Step: 9
Training loss: 3.458813706705007
Validation loss: 2.8989284621302915

Epoch: 6| Step: 10
Training loss: 3.7768106173100446
Validation loss: 2.901806800474488

Epoch: 6| Step: 11
Training loss: 3.1375559524471157
Validation loss: 2.904258385227152

Epoch: 6| Step: 12
Training loss: 2.439996956526312
Validation loss: 2.908162347339421

Epoch: 6| Step: 13
Training loss: 3.485520520955508
Validation loss: 2.914345490631431

Epoch: 45| Step: 0
Training loss: 3.4087817381334964
Validation loss: 2.918570888060235

Epoch: 6| Step: 1
Training loss: 2.7552358893955664
Validation loss: 2.9075676538414483

Epoch: 6| Step: 2
Training loss: 2.847215992577
Validation loss: 2.896961958659543

Epoch: 6| Step: 3
Training loss: 3.223227340496694
Validation loss: 2.9056413436351405

Epoch: 6| Step: 4
Training loss: 3.0561189151416093
Validation loss: 2.895933950828464

Epoch: 6| Step: 5
Training loss: 3.6866146253208893
Validation loss: 2.8971632368248277

Epoch: 6| Step: 6
Training loss: 2.9016986607499153
Validation loss: 2.8993395877640586

Epoch: 6| Step: 7
Training loss: 3.308623294630854
Validation loss: 2.9017481644040077

Epoch: 6| Step: 8
Training loss: 3.619749805382197
Validation loss: 2.900843750172283

Epoch: 6| Step: 9
Training loss: 3.153088185120964
Validation loss: 2.9046346881719787

Epoch: 6| Step: 10
Training loss: 3.1643156727415507
Validation loss: 2.901761308811043

Epoch: 6| Step: 11
Training loss: 3.478733576394178
Validation loss: 2.8995373340203017

Epoch: 6| Step: 12
Training loss: 2.931313025604251
Validation loss: 2.902903242860955

Epoch: 6| Step: 13
Training loss: 3.234289490679751
Validation loss: 2.897113258394143

Epoch: 46| Step: 0
Training loss: 3.2600081475278926
Validation loss: 2.8969157413581907

Epoch: 6| Step: 1
Training loss: 3.158445746468701
Validation loss: 2.8978816646712264

Epoch: 6| Step: 2
Training loss: 3.2668294191364433
Validation loss: 2.9018677049313477

Epoch: 6| Step: 3
Training loss: 3.1948335843631828
Validation loss: 2.897428065644529

Epoch: 6| Step: 4
Training loss: 3.1481997113794247
Validation loss: 2.896162324390616

Epoch: 6| Step: 5
Training loss: 3.193238422308003
Validation loss: 2.896513189748702

Epoch: 6| Step: 6
Training loss: 3.3810782623394644
Validation loss: 2.8950500585740944

Epoch: 6| Step: 7
Training loss: 3.2655778361295877
Validation loss: 2.896035025734654

Epoch: 6| Step: 8
Training loss: 2.8495477953421755
Validation loss: 2.8953067822849237

Epoch: 6| Step: 9
Training loss: 3.757950809970925
Validation loss: 2.899137802794103

Epoch: 6| Step: 10
Training loss: 3.0559600754970977
Validation loss: 2.8945206332838547

Epoch: 6| Step: 11
Training loss: 2.676457630069387
Validation loss: 2.8947715066711566

Epoch: 6| Step: 12
Training loss: 3.5800730637068696
Validation loss: 2.890965174264702

Epoch: 6| Step: 13
Training loss: 2.589153951665448
Validation loss: 2.8966575211627537

Epoch: 47| Step: 0
Training loss: 3.090468081973707
Validation loss: 2.894709426466267

Epoch: 6| Step: 1
Training loss: 3.7245399856704986
Validation loss: 2.897434917517693

Epoch: 6| Step: 2
Training loss: 2.9344616049436856
Validation loss: 2.893505796765945

Epoch: 6| Step: 3
Training loss: 3.4805149199466436
Validation loss: 2.893356319709693

Epoch: 6| Step: 4
Training loss: 2.159924789638128
Validation loss: 2.888889828495896

Epoch: 6| Step: 5
Training loss: 3.5560010558084953
Validation loss: 2.8895759716629663

Epoch: 6| Step: 6
Training loss: 3.3123669867530743
Validation loss: 2.8910550237467914

Epoch: 6| Step: 7
Training loss: 3.9922684812450644
Validation loss: 2.8887233510863544

Epoch: 6| Step: 8
Training loss: 3.449682254256828
Validation loss: 2.886379766378424

Epoch: 6| Step: 9
Training loss: 2.8539819773671016
Validation loss: 2.886630476043837

Epoch: 6| Step: 10
Training loss: 2.5547501209295214
Validation loss: 2.885230824315737

Epoch: 6| Step: 11
Training loss: 2.4559844050580173
Validation loss: 2.8841831414773664

Epoch: 6| Step: 12
Training loss: 3.224254604812224
Validation loss: 2.8859840206367062

Epoch: 6| Step: 13
Training loss: 3.6021010519591745
Validation loss: 2.8857856663510635

Epoch: 48| Step: 0
Training loss: 3.119010220299622
Validation loss: 2.8852919382819424

Epoch: 6| Step: 1
Training loss: 3.2468119904294257
Validation loss: 2.8874164079998708

Epoch: 6| Step: 2
Training loss: 2.7466505633811114
Validation loss: 2.8864750253651814

Epoch: 6| Step: 3
Training loss: 3.1825457199420537
Validation loss: 2.887556596959617

Epoch: 6| Step: 4
Training loss: 2.8250736193221306
Validation loss: 2.8878477582058113

Epoch: 6| Step: 5
Training loss: 3.4576178273985456
Validation loss: 2.887645891092029

Epoch: 6| Step: 6
Training loss: 3.3264736323135122
Validation loss: 2.884798809989851

Epoch: 6| Step: 7
Training loss: 2.913553065547164
Validation loss: 2.8871038487159177

Epoch: 6| Step: 8
Training loss: 3.366833165901839
Validation loss: 2.8847628052091507

Epoch: 6| Step: 9
Training loss: 3.6145415876251685
Validation loss: 2.884665980646011

Epoch: 6| Step: 10
Training loss: 3.158082637717904
Validation loss: 2.885294060067785

Epoch: 6| Step: 11
Training loss: 3.23971603138134
Validation loss: 2.88898169832126

Epoch: 6| Step: 12
Training loss: 3.082907123147899
Validation loss: 2.888080230403605

Epoch: 6| Step: 13
Training loss: 3.4082099805178188
Validation loss: 2.888643690249355

Epoch: 49| Step: 0
Training loss: 3.6057480593328775
Validation loss: 2.8883486488578654

Epoch: 6| Step: 1
Training loss: 2.8564635082219607
Validation loss: 2.8925570881281266

Epoch: 6| Step: 2
Training loss: 3.1452954234509116
Validation loss: 2.8930251418591206

Epoch: 6| Step: 3
Training loss: 2.321928649528968
Validation loss: 2.896641804684856

Epoch: 6| Step: 4
Training loss: 3.1558617079185893
Validation loss: 2.895487035969291

Epoch: 6| Step: 5
Training loss: 2.3752391092500225
Validation loss: 2.887357799329722

Epoch: 6| Step: 6
Training loss: 3.4038172442531915
Validation loss: 2.886326107041278

Epoch: 6| Step: 7
Training loss: 3.2789734708754628
Validation loss: 2.883051170537953

Epoch: 6| Step: 8
Training loss: 3.3348283911709116
Validation loss: 2.8890824671803754

Epoch: 6| Step: 9
Training loss: 3.2691009858205917
Validation loss: 2.884990845670316

Epoch: 6| Step: 10
Training loss: 3.892151559903441
Validation loss: 2.879827492049759

Epoch: 6| Step: 11
Training loss: 3.2520191082832905
Validation loss: 2.8796361696737125

Epoch: 6| Step: 12
Training loss: 2.5934578604656817
Validation loss: 2.8851966322368874

Epoch: 6| Step: 13
Training loss: 4.024347117292318
Validation loss: 2.8795352545343946

Epoch: 50| Step: 0
Training loss: 2.8412808608507567
Validation loss: 2.8889414746286812

Epoch: 6| Step: 1
Training loss: 3.2837564250649396
Validation loss: 2.8792403331125116

Epoch: 6| Step: 2
Training loss: 2.477814367002298
Validation loss: 2.880831382564021

Epoch: 6| Step: 3
Training loss: 3.153501314631379
Validation loss: 2.8737303064422277

Epoch: 6| Step: 4
Training loss: 3.663680131884421
Validation loss: 2.879382854860079

Epoch: 6| Step: 5
Training loss: 3.628420958494008
Validation loss: 2.8744142924630673

Epoch: 6| Step: 6
Training loss: 2.968412199377404
Validation loss: 2.87355692431942

Epoch: 6| Step: 7
Training loss: 3.051234486379102
Validation loss: 2.870560387755284

Epoch: 6| Step: 8
Training loss: 3.053485292320879
Validation loss: 2.872639608692646

Epoch: 6| Step: 9
Training loss: 2.865419097125733
Validation loss: 2.8706040715476346

Epoch: 6| Step: 10
Training loss: 3.3874192794810623
Validation loss: 2.8733385161853766

Epoch: 6| Step: 11
Training loss: 3.4164166397784648
Validation loss: 2.874355580111335

Epoch: 6| Step: 12
Training loss: 3.1981463189352883
Validation loss: 2.8771329825948975

Epoch: 6| Step: 13
Training loss: 3.503279648020741
Validation loss: 2.880543654104801

Epoch: 51| Step: 0
Training loss: 3.4230287601244545
Validation loss: 2.8827327425219593

Epoch: 6| Step: 1
Training loss: 3.2830817060014286
Validation loss: 2.8773399396904895

Epoch: 6| Step: 2
Training loss: 3.001389658457724
Validation loss: 2.8745673408263976

Epoch: 6| Step: 3
Training loss: 3.241373497684162
Validation loss: 2.8770869858502577

Epoch: 6| Step: 4
Training loss: 3.6821599538394825
Validation loss: 2.8749659769435714

Epoch: 6| Step: 5
Training loss: 2.452511850497578
Validation loss: 2.8747951778804803

Epoch: 6| Step: 6
Training loss: 3.3095626763429906
Validation loss: 2.8706426598032535

Epoch: 6| Step: 7
Training loss: 3.578223614395716
Validation loss: 2.873769178329854

Epoch: 6| Step: 8
Training loss: 3.379184248408539
Validation loss: 2.8765022879117255

Epoch: 6| Step: 9
Training loss: 3.306904367112103
Validation loss: 2.8587198989155587

Epoch: 6| Step: 10
Training loss: 3.5007203586955007
Validation loss: 2.851230238659948

Epoch: 6| Step: 11
Training loss: 2.4187024410017677
Validation loss: 2.8527628095293345

Epoch: 6| Step: 12
Training loss: 2.247579650316175
Validation loss: 2.8615901388176264

Epoch: 6| Step: 13
Training loss: 3.4082158566649734
Validation loss: 2.866106553194724

Epoch: 52| Step: 0
Training loss: 2.676508405130986
Validation loss: 2.867306833577175

Epoch: 6| Step: 1
Training loss: 3.182750379567599
Validation loss: 2.870428098095959

Epoch: 6| Step: 2
Training loss: 3.814061704867607
Validation loss: 2.869093403754012

Epoch: 6| Step: 3
Training loss: 2.8070843584072858
Validation loss: 2.861274142793633

Epoch: 6| Step: 4
Training loss: 3.4138580191195755
Validation loss: 2.8564938951961696

Epoch: 6| Step: 5
Training loss: 3.147982808261252
Validation loss: 2.855040670980563

Epoch: 6| Step: 6
Training loss: 2.598202113112268
Validation loss: 2.8527705352290056

Epoch: 6| Step: 7
Training loss: 3.277223859919756
Validation loss: 2.8559470058643313

Epoch: 6| Step: 8
Training loss: 2.6291942693729413
Validation loss: 2.858792442951079

Epoch: 6| Step: 9
Training loss: 3.235116477414148
Validation loss: 2.864084569443214

Epoch: 6| Step: 10
Training loss: 2.9901098303892666
Validation loss: 2.864507494208737

Epoch: 6| Step: 11
Training loss: 4.225913063119143
Validation loss: 2.8624711510923673

Epoch: 6| Step: 12
Training loss: 2.9401308007240443
Validation loss: 2.8656136938270986

Epoch: 6| Step: 13
Training loss: 2.9974079060028287
Validation loss: 2.8645131602413256

Epoch: 53| Step: 0
Training loss: 2.6281211100038098
Validation loss: 2.8616704318089248

Epoch: 6| Step: 1
Training loss: 2.785314782808409
Validation loss: 2.857031818032606

Epoch: 6| Step: 2
Training loss: 3.2923376308947514
Validation loss: 2.857473512108567

Epoch: 6| Step: 3
Training loss: 2.5903252661757796
Validation loss: 2.8550792486966783

Epoch: 6| Step: 4
Training loss: 3.518143357991247
Validation loss: 2.8572666575012966

Epoch: 6| Step: 5
Training loss: 2.964259556246193
Validation loss: 2.8506710064233545

Epoch: 6| Step: 6
Training loss: 2.764682415181086
Validation loss: 2.8443962479820617

Epoch: 6| Step: 7
Training loss: 3.6100068303093793
Validation loss: 2.8487206233315328

Epoch: 6| Step: 8
Training loss: 2.7326139173991852
Validation loss: 2.8469883869177965

Epoch: 6| Step: 9
Training loss: 3.068097196240467
Validation loss: 2.8447744937168618

Epoch: 6| Step: 10
Training loss: 3.7161530474793474
Validation loss: 2.845292776428807

Epoch: 6| Step: 11
Training loss: 3.7253647644866943
Validation loss: 2.8490332867131065

Epoch: 6| Step: 12
Training loss: 3.4664458449043014
Validation loss: 2.8450348665043084

Epoch: 6| Step: 13
Training loss: 2.99794221875557
Validation loss: 2.850349290230165

Epoch: 54| Step: 0
Training loss: 3.1155643682979512
Validation loss: 2.8588838272526367

Epoch: 6| Step: 1
Training loss: 3.342055702553404
Validation loss: 2.8516598489180645

Epoch: 6| Step: 2
Training loss: 3.254276396545368
Validation loss: 2.8612754948251444

Epoch: 6| Step: 3
Training loss: 2.7595379756437084
Validation loss: 2.856202944911892

Epoch: 6| Step: 4
Training loss: 3.2848212290156367
Validation loss: 2.85287508937775

Epoch: 6| Step: 5
Training loss: 2.4347559815633066
Validation loss: 2.855093672891439

Epoch: 6| Step: 6
Training loss: 3.738280357113595
Validation loss: 2.8637338551511715

Epoch: 6| Step: 7
Training loss: 2.9886841346570048
Validation loss: 2.8587675982049654

Epoch: 6| Step: 8
Training loss: 2.9872949505909476
Validation loss: 2.859777960896413

Epoch: 6| Step: 9
Training loss: 3.7594452481195146
Validation loss: 2.8561223492146617

Epoch: 6| Step: 10
Training loss: 2.9825302733861796
Validation loss: 2.8423481753680093

Epoch: 6| Step: 11
Training loss: 2.9853807921471414
Validation loss: 2.842281156485102

Epoch: 6| Step: 12
Training loss: 3.417144013221024
Validation loss: 2.83439246606884

Epoch: 6| Step: 13
Training loss: 2.591917930598876
Validation loss: 2.839131209148703

Epoch: 55| Step: 0
Training loss: 3.278956456376031
Validation loss: 2.839479796061678

Epoch: 6| Step: 1
Training loss: 2.585311214520939
Validation loss: 2.84154878308728

Epoch: 6| Step: 2
Training loss: 3.417770641580161
Validation loss: 2.8422127307255716

Epoch: 6| Step: 3
Training loss: 3.0302178896878647
Validation loss: 2.8418382946170677

Epoch: 6| Step: 4
Training loss: 3.6376363250250967
Validation loss: 2.84231282613478

Epoch: 6| Step: 5
Training loss: 3.179657807082643
Validation loss: 2.8434554953500375

Epoch: 6| Step: 6
Training loss: 3.6266722932472715
Validation loss: 2.8410397325161583

Epoch: 6| Step: 7
Training loss: 3.253978787972608
Validation loss: 2.8367426975621104

Epoch: 6| Step: 8
Training loss: 2.7224567619372166
Validation loss: 2.8388054208781317

Epoch: 6| Step: 9
Training loss: 3.101411983959965
Validation loss: 2.8385984323467803

Epoch: 6| Step: 10
Training loss: 2.841769859998312
Validation loss: 2.8404982351292887

Epoch: 6| Step: 11
Training loss: 2.9577061879377133
Validation loss: 2.839953510233793

Epoch: 6| Step: 12
Training loss: 3.4640226025449636
Validation loss: 2.839872980571775

Epoch: 6| Step: 13
Training loss: 2.6413782614144923
Validation loss: 2.8404219376569877

Epoch: 56| Step: 0
Training loss: 2.9549138539726356
Validation loss: 2.83899828935473

Epoch: 6| Step: 1
Training loss: 3.266506678367852
Validation loss: 2.8387980057588726

Epoch: 6| Step: 2
Training loss: 3.215417711047595
Validation loss: 2.8433179142821965

Epoch: 6| Step: 3
Training loss: 3.51859100687264
Validation loss: 2.838366537269558

Epoch: 6| Step: 4
Training loss: 3.152190061696565
Validation loss: 2.8368842339281417

Epoch: 6| Step: 5
Training loss: 3.058316545847485
Validation loss: 2.834934896400995

Epoch: 6| Step: 6
Training loss: 2.606489036081841
Validation loss: 2.837424127907754

Epoch: 6| Step: 7
Training loss: 2.9673706062471092
Validation loss: 2.834948917587092

Epoch: 6| Step: 8
Training loss: 3.2399563753464258
Validation loss: 2.8305900045870644

Epoch: 6| Step: 9
Training loss: 2.6210515435312707
Validation loss: 2.8339860675182225

Epoch: 6| Step: 10
Training loss: 3.4750298368421757
Validation loss: 2.836463723237755

Epoch: 6| Step: 11
Training loss: 3.872671227409824
Validation loss: 2.837376446582047

Epoch: 6| Step: 12
Training loss: 3.067410638653235
Validation loss: 2.835592654537218

Epoch: 6| Step: 13
Training loss: 2.6184549024281103
Validation loss: 2.8360502254029325

Epoch: 57| Step: 0
Training loss: 2.680758693115688
Validation loss: 2.8428642879638018

Epoch: 6| Step: 1
Training loss: 2.7525070206970983
Validation loss: 2.8522478768719166

Epoch: 6| Step: 2
Training loss: 3.3276639090621525
Validation loss: 2.868247880735724

Epoch: 6| Step: 3
Training loss: 3.498511951739603
Validation loss: 2.873355459330731

Epoch: 6| Step: 4
Training loss: 3.190364672346049
Validation loss: 2.8414465522505794

Epoch: 6| Step: 5
Training loss: 2.443824095712478
Validation loss: 2.832770171890327

Epoch: 6| Step: 6
Training loss: 2.471251174393926
Validation loss: 2.827540152986276

Epoch: 6| Step: 7
Training loss: 2.7554714219091454
Validation loss: 2.837477743482451

Epoch: 6| Step: 8
Training loss: 3.4956583615066124
Validation loss: 2.8506242598389346

Epoch: 6| Step: 9
Training loss: 3.3347776303941536
Validation loss: 2.8407050771089457

Epoch: 6| Step: 10
Training loss: 3.784434215230427
Validation loss: 2.83765411514972

Epoch: 6| Step: 11
Training loss: 3.0669408247207617
Validation loss: 2.833439571323974

Epoch: 6| Step: 12
Training loss: 3.36086917738588
Validation loss: 2.8374620009806995

Epoch: 6| Step: 13
Training loss: 4.0345540072723844
Validation loss: 2.843314322158118

Epoch: 58| Step: 0
Training loss: 3.677318677133958
Validation loss: 2.8488363911896073

Epoch: 6| Step: 1
Training loss: 2.7137357577701793
Validation loss: 2.849299719698614

Epoch: 6| Step: 2
Training loss: 3.556068906547683
Validation loss: 2.8461043384221694

Epoch: 6| Step: 3
Training loss: 3.4399586987788386
Validation loss: 2.8353113011749507

Epoch: 6| Step: 4
Training loss: 2.9989630178503504
Validation loss: 2.8263587800563235

Epoch: 6| Step: 5
Training loss: 3.2095414071182113
Validation loss: 2.827016217526601

Epoch: 6| Step: 6
Training loss: 3.1133662538877704
Validation loss: 2.827660939912362

Epoch: 6| Step: 7
Training loss: 2.6203870659206325
Validation loss: 2.83175072604178

Epoch: 6| Step: 8
Training loss: 2.2987447381594635
Validation loss: 2.8293127324853544

Epoch: 6| Step: 9
Training loss: 2.7584011757182116
Validation loss: 2.829984659141554

Epoch: 6| Step: 10
Training loss: 3.762793905242172
Validation loss: 2.8298315604363693

Epoch: 6| Step: 11
Training loss: 3.201225350138694
Validation loss: 2.830445185377977

Epoch: 6| Step: 12
Training loss: 3.2932251890344393
Validation loss: 2.830636421789386

Epoch: 6| Step: 13
Training loss: 3.1057041294936023
Validation loss: 2.831338926676885

Epoch: 59| Step: 0
Training loss: 3.2368299225303288
Validation loss: 2.8308500455859726

Epoch: 6| Step: 1
Training loss: 3.6682796976159557
Validation loss: 2.832133781775071

Epoch: 6| Step: 2
Training loss: 3.1395431856871983
Validation loss: 2.8306992922419663

Epoch: 6| Step: 3
Training loss: 1.9659333440554496
Validation loss: 2.8286305643463034

Epoch: 6| Step: 4
Training loss: 3.7798282693272545
Validation loss: 2.828991365822982

Epoch: 6| Step: 5
Training loss: 3.296303794517355
Validation loss: 2.8272639933113326

Epoch: 6| Step: 6
Training loss: 3.013443706426546
Validation loss: 2.8326338692414677

Epoch: 6| Step: 7
Training loss: 3.0264982753353604
Validation loss: 2.829069861855525

Epoch: 6| Step: 8
Training loss: 3.073539765542111
Validation loss: 2.8304782572875733

Epoch: 6| Step: 9
Training loss: 2.6718848155772736
Validation loss: 2.830037537872275

Epoch: 6| Step: 10
Training loss: 3.062054037204081
Validation loss: 2.833127367784152

Epoch: 6| Step: 11
Training loss: 3.411350456259926
Validation loss: 2.837743096840806

Epoch: 6| Step: 12
Training loss: 3.3149422333576264
Validation loss: 2.823818043051311

Epoch: 6| Step: 13
Training loss: 2.9674186532135876
Validation loss: 2.8318333479631295

Epoch: 60| Step: 0
Training loss: 3.506827099203914
Validation loss: 2.8255462238563496

Epoch: 6| Step: 1
Training loss: 2.2070690590649704
Validation loss: 2.82351960173486

Epoch: 6| Step: 2
Training loss: 3.4756062424772147
Validation loss: 2.824135996992541

Epoch: 6| Step: 3
Training loss: 3.2015850552257277
Validation loss: 2.825524021035548

Epoch: 6| Step: 4
Training loss: 3.1480388530965544
Validation loss: 2.8255487597810145

Epoch: 6| Step: 5
Training loss: 2.9907545039758374
Validation loss: 2.8252517861686512

Epoch: 6| Step: 6
Training loss: 3.1878898232637454
Validation loss: 2.822066870982399

Epoch: 6| Step: 7
Training loss: 3.091033976372764
Validation loss: 2.8239144737317137

Epoch: 6| Step: 8
Training loss: 3.505460294573461
Validation loss: 2.8244159629460013

Epoch: 6| Step: 9
Training loss: 2.91842790741962
Validation loss: 2.8207136404124236

Epoch: 6| Step: 10
Training loss: 3.862226660479225
Validation loss: 2.81835178328769

Epoch: 6| Step: 11
Training loss: 3.2055360233351635
Validation loss: 2.8215343092297958

Epoch: 6| Step: 12
Training loss: 2.4267456299996026
Validation loss: 2.824388394118299

Epoch: 6| Step: 13
Training loss: 2.580090507575793
Validation loss: 2.818602912029571

Epoch: 61| Step: 0
Training loss: 2.9273203327351
Validation loss: 2.821047236815586

Epoch: 6| Step: 1
Training loss: 2.9156223243954598
Validation loss: 2.822417655375889

Epoch: 6| Step: 2
Training loss: 3.066434551521465
Validation loss: 2.818515117021176

Epoch: 6| Step: 3
Training loss: 2.572709284982071
Validation loss: 2.8212978584801767

Epoch: 6| Step: 4
Training loss: 2.71901754180985
Validation loss: 2.817884121303781

Epoch: 6| Step: 5
Training loss: 2.8246934842614864
Validation loss: 2.8170011663866243

Epoch: 6| Step: 6
Training loss: 3.6440835005136503
Validation loss: 2.820433747795716

Epoch: 6| Step: 7
Training loss: 2.9486954989051806
Validation loss: 2.8254935750057126

Epoch: 6| Step: 8
Training loss: 3.2748126143799063
Validation loss: 2.8197444266053506

Epoch: 6| Step: 9
Training loss: 3.152067075402958
Validation loss: 2.8172105507153917

Epoch: 6| Step: 10
Training loss: 3.3541588319169624
Validation loss: 2.8177646416540516

Epoch: 6| Step: 11
Training loss: 3.5617189052989247
Validation loss: 2.8147190250163776

Epoch: 6| Step: 12
Training loss: 3.325694454781946
Validation loss: 2.81683660904579

Epoch: 6| Step: 13
Training loss: 3.5910825698440156
Validation loss: 2.818236817194429

Epoch: 62| Step: 0
Training loss: 3.276127622575448
Validation loss: 2.8126294670249705

Epoch: 6| Step: 1
Training loss: 3.2984002368291288
Validation loss: 2.8126960248554096

Epoch: 6| Step: 2
Training loss: 3.2896342380894197
Validation loss: 2.8148489273249226

Epoch: 6| Step: 3
Training loss: 2.873039572347959
Validation loss: 2.814048878391475

Epoch: 6| Step: 4
Training loss: 3.146120946675191
Validation loss: 2.8168236444511185

Epoch: 6| Step: 5
Training loss: 2.822819237711034
Validation loss: 2.820489491490932

Epoch: 6| Step: 6
Training loss: 2.8309111807151677
Validation loss: 2.8218847073996356

Epoch: 6| Step: 7
Training loss: 2.99011796342229
Validation loss: 2.824626487587418

Epoch: 6| Step: 8
Training loss: 2.950188685703001
Validation loss: 2.8325949333440406

Epoch: 6| Step: 9
Training loss: 3.4904103376180817
Validation loss: 2.8366561427031494

Epoch: 6| Step: 10
Training loss: 3.7747847085154524
Validation loss: 2.842359638159029

Epoch: 6| Step: 11
Training loss: 3.2575263393973772
Validation loss: 2.8324569894727887

Epoch: 6| Step: 12
Training loss: 2.797757824564104
Validation loss: 2.823120180384727

Epoch: 6| Step: 13
Training loss: 2.716527183634025
Validation loss: 2.816278312935528

Epoch: 63| Step: 0
Training loss: 3.6496790339818137
Validation loss: 2.8205704345960028

Epoch: 6| Step: 1
Training loss: 2.8494276442437125
Validation loss: 2.817302811530139

Epoch: 6| Step: 2
Training loss: 2.8440801407887584
Validation loss: 2.8264343741677704

Epoch: 6| Step: 3
Training loss: 3.5922743711715395
Validation loss: 2.8168709792672058

Epoch: 6| Step: 4
Training loss: 3.027481725686422
Validation loss: 2.8214330968579637

Epoch: 6| Step: 5
Training loss: 2.996239053119751
Validation loss: 2.8217793747453195

Epoch: 6| Step: 6
Training loss: 3.2774060212722325
Validation loss: 2.8149077569959493

Epoch: 6| Step: 7
Training loss: 2.9374334449533106
Validation loss: 2.814664137968364

Epoch: 6| Step: 8
Training loss: 2.9266701584304915
Validation loss: 2.8156246461413152

Epoch: 6| Step: 9
Training loss: 3.0049383525903908
Validation loss: 2.812354417904722

Epoch: 6| Step: 10
Training loss: 2.9577416558101635
Validation loss: 2.8133011455101036

Epoch: 6| Step: 11
Training loss: 2.560039734830046
Validation loss: 2.8120332182422567

Epoch: 6| Step: 12
Training loss: 3.2242065399877844
Validation loss: 2.81162454157978

Epoch: 6| Step: 13
Training loss: 4.102409696729986
Validation loss: 2.810619597050828

Epoch: 64| Step: 0
Training loss: 3.217684374952958
Validation loss: 2.808057877990039

Epoch: 6| Step: 1
Training loss: 3.088572166939848
Validation loss: 2.8105821950545478

Epoch: 6| Step: 2
Training loss: 3.7453934668302225
Validation loss: 2.8073709916455307

Epoch: 6| Step: 3
Training loss: 3.1399614382763215
Validation loss: 2.805029390842185

Epoch: 6| Step: 4
Training loss: 2.7407175672637814
Validation loss: 2.8078458590841566

Epoch: 6| Step: 5
Training loss: 3.3208731144365484
Validation loss: 2.8086739104971876

Epoch: 6| Step: 6
Training loss: 3.593622620025126
Validation loss: 2.808212210316034

Epoch: 6| Step: 7
Training loss: 2.458612803290487
Validation loss: 2.8104039684580333

Epoch: 6| Step: 8
Training loss: 3.2263189413219537
Validation loss: 2.813868896163705

Epoch: 6| Step: 9
Training loss: 3.281234595852979
Validation loss: 2.8087812899036284

Epoch: 6| Step: 10
Training loss: 2.7434977908646117
Validation loss: 2.8151958256718714

Epoch: 6| Step: 11
Training loss: 2.0675209658757847
Validation loss: 2.814443291077947

Epoch: 6| Step: 12
Training loss: 3.752830200306044
Validation loss: 2.8218620015364184

Epoch: 6| Step: 13
Training loss: 2.7456581345401245
Validation loss: 2.8149215655564186

Epoch: 65| Step: 0
Training loss: 3.1033664329011987
Validation loss: 2.8241251065623216

Epoch: 6| Step: 1
Training loss: 3.1432784528853066
Validation loss: 2.8261053760087576

Epoch: 6| Step: 2
Training loss: 3.28055616036572
Validation loss: 2.839938893594526

Epoch: 6| Step: 3
Training loss: 3.1427401731448366
Validation loss: 2.8180063555719514

Epoch: 6| Step: 4
Training loss: 2.9818201763395926
Validation loss: 2.810256921729624

Epoch: 6| Step: 5
Training loss: 3.2070713946748977
Validation loss: 2.808257750384439

Epoch: 6| Step: 6
Training loss: 3.141103224143021
Validation loss: 2.8107228323693274

Epoch: 6| Step: 7
Training loss: 3.0173889693444855
Validation loss: 2.8091502852453876

Epoch: 6| Step: 8
Training loss: 3.1096018032762593
Validation loss: 2.80782454894019

Epoch: 6| Step: 9
Training loss: 3.4879683368103143
Validation loss: 2.8074063114764507

Epoch: 6| Step: 10
Training loss: 1.7651394201656367
Validation loss: 2.8057804885242814

Epoch: 6| Step: 11
Training loss: 3.4169185049393516
Validation loss: 2.8068512242623487

Epoch: 6| Step: 12
Training loss: 3.6646230956076953
Validation loss: 2.8076589780375074

Epoch: 6| Step: 13
Training loss: 2.8183445708691663
Validation loss: 2.809531951206363

Epoch: 66| Step: 0
Training loss: 3.8569427115369663
Validation loss: 2.8044941211908867

Epoch: 6| Step: 1
Training loss: 2.582476001934505
Validation loss: 2.803870109052013

Epoch: 6| Step: 2
Training loss: 2.3835290050434255
Validation loss: 2.807054125180996

Epoch: 6| Step: 3
Training loss: 3.4693061451512435
Validation loss: 2.808773041609422

Epoch: 6| Step: 4
Training loss: 2.9922407260075063
Validation loss: 2.8102417460499156

Epoch: 6| Step: 5
Training loss: 3.2636137248896486
Validation loss: 2.81583026528697

Epoch: 6| Step: 6
Training loss: 3.5153355458097866
Validation loss: 2.820968817443648

Epoch: 6| Step: 7
Training loss: 2.5178525551673965
Validation loss: 2.8236818339501935

Epoch: 6| Step: 8
Training loss: 2.7063616300117586
Validation loss: 2.8301075270913314

Epoch: 6| Step: 9
Training loss: 4.162227147412808
Validation loss: 2.8365580077315498

Epoch: 6| Step: 10
Training loss: 2.5433911794514477
Validation loss: 2.821465253114009

Epoch: 6| Step: 11
Training loss: 3.3693229117353716
Validation loss: 2.8182558217717744

Epoch: 6| Step: 12
Training loss: 2.84568988741988
Validation loss: 2.8055847544412855

Epoch: 6| Step: 13
Training loss: 2.9323475879240184
Validation loss: 2.8054469189857167

Epoch: 67| Step: 0
Training loss: 3.0077324398334477
Validation loss: 2.805931001884482

Epoch: 6| Step: 1
Training loss: 3.466594679402922
Validation loss: 2.8076191433636244

Epoch: 6| Step: 2
Training loss: 2.4921054168863948
Validation loss: 2.807539894185011

Epoch: 6| Step: 3
Training loss: 3.1289819905880436
Validation loss: 2.807273762561928

Epoch: 6| Step: 4
Training loss: 3.2150671175629553
Validation loss: 2.808940170488112

Epoch: 6| Step: 5
Training loss: 3.122730193743135
Validation loss: 2.804309755663149

Epoch: 6| Step: 6
Training loss: 3.038750561392829
Validation loss: 2.8119887777238843

Epoch: 6| Step: 7
Training loss: 1.7896162616985078
Validation loss: 2.8069748143847097

Epoch: 6| Step: 8
Training loss: 3.3753212669900785
Validation loss: 2.806400555266556

Epoch: 6| Step: 9
Training loss: 3.3703911005934417
Validation loss: 2.8070179022346613

Epoch: 6| Step: 10
Training loss: 3.3919452962063015
Validation loss: 2.809056854986535

Epoch: 6| Step: 11
Training loss: 3.3621815867495664
Validation loss: 2.805110686837883

Epoch: 6| Step: 12
Training loss: 3.00295271205604
Validation loss: 2.8031651401996514

Epoch: 6| Step: 13
Training loss: 3.8306074540204293
Validation loss: 2.80812047703871

Epoch: 68| Step: 0
Training loss: 3.010056013175438
Validation loss: 2.803719216230783

Epoch: 6| Step: 1
Training loss: 2.457500564559608
Validation loss: 2.811414119330228

Epoch: 6| Step: 2
Training loss: 3.1596318624429256
Validation loss: 2.808678269827805

Epoch: 6| Step: 3
Training loss: 2.9931384136849672
Validation loss: 2.8103154876544556

Epoch: 6| Step: 4
Training loss: 3.2153445997405266
Validation loss: 2.805304597045879

Epoch: 6| Step: 5
Training loss: 3.073627885314091
Validation loss: 2.802069433448998

Epoch: 6| Step: 6
Training loss: 3.283706472142483
Validation loss: 2.8034532916184305

Epoch: 6| Step: 7
Training loss: 2.913611328543311
Validation loss: 2.8064573139627345

Epoch: 6| Step: 8
Training loss: 3.337396592956366
Validation loss: 2.805590518454827

Epoch: 6| Step: 9
Training loss: 3.1463858685503716
Validation loss: 2.804730797570914

Epoch: 6| Step: 10
Training loss: 3.525855340734935
Validation loss: 2.8069402362218465

Epoch: 6| Step: 11
Training loss: 3.487324103838415
Validation loss: 2.803146349748714

Epoch: 6| Step: 12
Training loss: 2.737066894828765
Validation loss: 2.796071515347342

Epoch: 6| Step: 13
Training loss: 3.2056374721392316
Validation loss: 2.8029430397337936

Epoch: 69| Step: 0
Training loss: 3.475273802145369
Validation loss: 2.8018790672009413

Epoch: 6| Step: 1
Training loss: 3.483566669393928
Validation loss: 2.7971468933272625

Epoch: 6| Step: 2
Training loss: 3.4843702102422935
Validation loss: 2.795162492202023

Epoch: 6| Step: 3
Training loss: 3.437095202106675
Validation loss: 2.7981138361033406

Epoch: 6| Step: 4
Training loss: 3.0674120377269003
Validation loss: 2.797529839513039

Epoch: 6| Step: 5
Training loss: 3.4793870745682853
Validation loss: 2.7987527836791717

Epoch: 6| Step: 6
Training loss: 3.0111693043108074
Validation loss: 2.796044038382023

Epoch: 6| Step: 7
Training loss: 2.756207136655619
Validation loss: 2.798169964892315

Epoch: 6| Step: 8
Training loss: 3.0033496594029376
Validation loss: 2.798227984438328

Epoch: 6| Step: 9
Training loss: 2.745468220340816
Validation loss: 2.7948710323161503

Epoch: 6| Step: 10
Training loss: 2.4496599739942884
Validation loss: 2.7966833891696132

Epoch: 6| Step: 11
Training loss: 3.316544277163025
Validation loss: 2.796555592125421

Epoch: 6| Step: 12
Training loss: 2.9733135283741428
Validation loss: 2.7970365916552673

Epoch: 6| Step: 13
Training loss: 2.30908425117849
Validation loss: 2.7976826239438957

Epoch: 70| Step: 0
Training loss: 3.479016481611044
Validation loss: 2.797229268917904

Epoch: 6| Step: 1
Training loss: 2.214673340177424
Validation loss: 2.8009435864906713

Epoch: 6| Step: 2
Training loss: 3.455823991399691
Validation loss: 2.802559778509875

Epoch: 6| Step: 3
Training loss: 3.7251421702859573
Validation loss: 2.802167466143296

Epoch: 6| Step: 4
Training loss: 2.717057501483844
Validation loss: 2.8063105249637026

Epoch: 6| Step: 5
Training loss: 3.0198773855618914
Validation loss: 2.829766964792782

Epoch: 6| Step: 6
Training loss: 3.0500402979455528
Validation loss: 2.8121873697786532

Epoch: 6| Step: 7
Training loss: 2.4487487244614865
Validation loss: 2.825427490132152

Epoch: 6| Step: 8
Training loss: 3.927574241708653
Validation loss: 2.8336904739203015

Epoch: 6| Step: 9
Training loss: 2.526210524955124
Validation loss: 2.813810016528857

Epoch: 6| Step: 10
Training loss: 2.428785837550079
Validation loss: 2.800754611923725

Epoch: 6| Step: 11
Training loss: 2.9159381138182896
Validation loss: 2.8014483531586714

Epoch: 6| Step: 12
Training loss: 3.2789926666149842
Validation loss: 2.7990398817634987

Epoch: 6| Step: 13
Training loss: 4.167688219091339
Validation loss: 2.7913986224915535

Epoch: 71| Step: 0
Training loss: 3.655316225454215
Validation loss: 2.7971703872982885

Epoch: 6| Step: 1
Training loss: 3.2332696914963988
Validation loss: 2.7942659481433916

Epoch: 6| Step: 2
Training loss: 3.394703604972427
Validation loss: 2.794603442875769

Epoch: 6| Step: 3
Training loss: 3.0661294411286453
Validation loss: 2.7915912701253314

Epoch: 6| Step: 4
Training loss: 2.411285877583796
Validation loss: 2.7926235925104286

Epoch: 6| Step: 5
Training loss: 2.8674519180483804
Validation loss: 2.7903151940902333

Epoch: 6| Step: 6
Training loss: 3.2976174625880166
Validation loss: 2.79064012906975

Epoch: 6| Step: 7
Training loss: 3.436486944561495
Validation loss: 2.7936689872857987

Epoch: 6| Step: 8
Training loss: 2.81391362585969
Validation loss: 2.7934920456600616

Epoch: 6| Step: 9
Training loss: 3.6474851227026157
Validation loss: 2.7930452735170848

Epoch: 6| Step: 10
Training loss: 3.0706532218904896
Validation loss: 2.7886283033057646

Epoch: 6| Step: 11
Training loss: 3.301178063010042
Validation loss: 2.7918959590640906

Epoch: 6| Step: 12
Training loss: 1.9168904491755978
Validation loss: 2.7928989839119276

Epoch: 6| Step: 13
Training loss: 2.8775045018032466
Validation loss: 2.7948174597895643

Epoch: 72| Step: 0
Training loss: 3.2502759669586765
Validation loss: 2.7891971138228135

Epoch: 6| Step: 1
Training loss: 3.438492510302764
Validation loss: 2.794384572447233

Epoch: 6| Step: 2
Training loss: 3.1928244606459377
Validation loss: 2.7939832666111686

Epoch: 6| Step: 3
Training loss: 2.1187441125298454
Validation loss: 2.793531202618526

Epoch: 6| Step: 4
Training loss: 2.997427155004863
Validation loss: 2.7950168472320303

Epoch: 6| Step: 5
Training loss: 3.438237475268445
Validation loss: 2.7898005231237617

Epoch: 6| Step: 6
Training loss: 3.0465959470068964
Validation loss: 2.794367152769652

Epoch: 6| Step: 7
Training loss: 3.5768075708208906
Validation loss: 2.7950092930118013

Epoch: 6| Step: 8
Training loss: 2.5464059596738307
Validation loss: 2.792406195107938

Epoch: 6| Step: 9
Training loss: 3.2735387381328103
Validation loss: 2.792242620418624

Epoch: 6| Step: 10
Training loss: 2.4372173047581933
Validation loss: 2.794010027968541

Epoch: 6| Step: 11
Training loss: 3.2573786384886887
Validation loss: 2.7940409042336545

Epoch: 6| Step: 12
Training loss: 2.8016310164864793
Validation loss: 2.7939673084034737

Epoch: 6| Step: 13
Training loss: 4.11438425604567
Validation loss: 2.7950058965411473

Epoch: 73| Step: 0
Training loss: 2.721786732342823
Validation loss: 2.7900073599934228

Epoch: 6| Step: 1
Training loss: 3.468487153963098
Validation loss: 2.789317524883509

Epoch: 6| Step: 2
Training loss: 2.7717589550821558
Validation loss: 2.790284457596246

Epoch: 6| Step: 3
Training loss: 3.5742390282894703
Validation loss: 2.789363296230314

Epoch: 6| Step: 4
Training loss: 2.4218060883593173
Validation loss: 2.7914942261270705

Epoch: 6| Step: 5
Training loss: 3.3917820797590617
Validation loss: 2.791648077378684

Epoch: 6| Step: 6
Training loss: 3.3399328387240814
Validation loss: 2.791919747900051

Epoch: 6| Step: 7
Training loss: 3.388590822157459
Validation loss: 2.791448356554427

Epoch: 6| Step: 8
Training loss: 3.6307837287036446
Validation loss: 2.794291546211388

Epoch: 6| Step: 9
Training loss: 2.993341208562991
Validation loss: 2.7941003518506

Epoch: 6| Step: 10
Training loss: 3.0279241190161983
Validation loss: 2.7916795904466185

Epoch: 6| Step: 11
Training loss: 2.6654167424875257
Validation loss: 2.790786513197483

Epoch: 6| Step: 12
Training loss: 2.7014002065631235
Validation loss: 2.793887288435643

Epoch: 6| Step: 13
Training loss: 3.1635618214011796
Validation loss: 2.7945954646366773

Epoch: 74| Step: 0
Training loss: 2.60523484765773
Validation loss: 2.7933972546950994

Epoch: 6| Step: 1
Training loss: 3.4888197941292702
Validation loss: 2.7891215215352707

Epoch: 6| Step: 2
Training loss: 3.2989489644460295
Validation loss: 2.7909190478252905

Epoch: 6| Step: 3
Training loss: 2.980172278781591
Validation loss: 2.792506908046457

Epoch: 6| Step: 4
Training loss: 2.8338975531583483
Validation loss: 2.789575409380364

Epoch: 6| Step: 5
Training loss: 2.5662056715342403
Validation loss: 2.790188708683942

Epoch: 6| Step: 6
Training loss: 3.1474113969398587
Validation loss: 2.7881136455279356

Epoch: 6| Step: 7
Training loss: 3.3360061102429244
Validation loss: 2.7890384781385706

Epoch: 6| Step: 8
Training loss: 2.6836952956531577
Validation loss: 2.7885699139491953

Epoch: 6| Step: 9
Training loss: 3.3064863210637716
Validation loss: 2.7858522614930843

Epoch: 6| Step: 10
Training loss: 3.742025670512485
Validation loss: 2.785868934273147

Epoch: 6| Step: 11
Training loss: 3.7981487382527064
Validation loss: 2.785110362568805

Epoch: 6| Step: 12
Training loss: 2.735973043771875
Validation loss: 2.785841366801628

Epoch: 6| Step: 13
Training loss: 2.012156140823194
Validation loss: 2.7860645232495305

Epoch: 75| Step: 0
Training loss: 3.085662288707034
Validation loss: 2.788964876234834

Epoch: 6| Step: 1
Training loss: 1.832506520778821
Validation loss: 2.7879431504498737

Epoch: 6| Step: 2
Training loss: 2.9076829884511355
Validation loss: 2.785332084682971

Epoch: 6| Step: 3
Training loss: 2.841830937027394
Validation loss: 2.7828960334053674

Epoch: 6| Step: 4
Training loss: 3.092505773170405
Validation loss: 2.790826948220178

Epoch: 6| Step: 5
Training loss: 2.859750024768353
Validation loss: 2.7888108457033596

Epoch: 6| Step: 6
Training loss: 2.9645164419648906
Validation loss: 2.7872665550320384

Epoch: 6| Step: 7
Training loss: 3.289742515019148
Validation loss: 2.79002975081046

Epoch: 6| Step: 8
Training loss: 4.020713105981977
Validation loss: 2.791014491529747

Epoch: 6| Step: 9
Training loss: 3.5534376021993985
Validation loss: 2.792167663310944

Epoch: 6| Step: 10
Training loss: 2.6386290461341906
Validation loss: 2.790084690615902

Epoch: 6| Step: 11
Training loss: 3.136416828071355
Validation loss: 2.793703481943894

Epoch: 6| Step: 12
Training loss: 3.7135725122626875
Validation loss: 2.7833861622749976

Epoch: 6| Step: 13
Training loss: 2.717544025110148
Validation loss: 2.783952808860876

Epoch: 76| Step: 0
Training loss: 2.874561773985046
Validation loss: 2.7809781490917556

Epoch: 6| Step: 1
Training loss: 3.229403152830371
Validation loss: 2.7829710596840855

Epoch: 6| Step: 2
Training loss: 3.830921506256317
Validation loss: 2.78600845791772

Epoch: 6| Step: 3
Training loss: 3.2185227397206058
Validation loss: 2.782134632186229

Epoch: 6| Step: 4
Training loss: 3.428408999228536
Validation loss: 2.782443885489105

Epoch: 6| Step: 5
Training loss: 2.8494453827469326
Validation loss: 2.7821545606457687

Epoch: 6| Step: 6
Training loss: 3.245389602619465
Validation loss: 2.782219900517746

Epoch: 6| Step: 7
Training loss: 2.989805224399441
Validation loss: 2.782070778512712

Epoch: 6| Step: 8
Training loss: 1.949909156737359
Validation loss: 2.7812652218770704

Epoch: 6| Step: 9
Training loss: 2.784145776494641
Validation loss: 2.7834926678129177

Epoch: 6| Step: 10
Training loss: 3.396064859622342
Validation loss: 2.7830543633459737

Epoch: 6| Step: 11
Training loss: 3.217832119628037
Validation loss: 2.7810945226597426

Epoch: 6| Step: 12
Training loss: 3.3339823726744133
Validation loss: 2.783348627351159

Epoch: 6| Step: 13
Training loss: 2.220908927943026
Validation loss: 2.7803156390376396

Epoch: 77| Step: 0
Training loss: 3.421418181283285
Validation loss: 2.784587086346884

Epoch: 6| Step: 1
Training loss: 2.773586903161177
Validation loss: 2.78492642786109

Epoch: 6| Step: 2
Training loss: 3.3254658996457973
Validation loss: 2.787055645001012

Epoch: 6| Step: 3
Training loss: 2.629060783208833
Validation loss: 2.7867778390573017

Epoch: 6| Step: 4
Training loss: 3.4955927484512412
Validation loss: 2.7906310894718285

Epoch: 6| Step: 5
Training loss: 3.0673531206279425
Validation loss: 2.787546240832628

Epoch: 6| Step: 6
Training loss: 2.8674469292542875
Validation loss: 2.78636993246509

Epoch: 6| Step: 7
Training loss: 3.113848511291057
Validation loss: 2.7913311706362816

Epoch: 6| Step: 8
Training loss: 3.4279346896596885
Validation loss: 2.79124347440428

Epoch: 6| Step: 9
Training loss: 2.8388532461710483
Validation loss: 2.783162254978996

Epoch: 6| Step: 10
Training loss: 3.204400529664886
Validation loss: 2.7888607426582057

Epoch: 6| Step: 11
Training loss: 3.1807402196708456
Validation loss: 2.7839892057543247

Epoch: 6| Step: 12
Training loss: 3.1530459920551643
Validation loss: 2.7829611393886107

Epoch: 6| Step: 13
Training loss: 2.282568563578436
Validation loss: 2.7811728938384395

Epoch: 78| Step: 0
Training loss: 3.2084135730832495
Validation loss: 2.782370289730845

Epoch: 6| Step: 1
Training loss: 3.338399581543864
Validation loss: 2.781902180078866

Epoch: 6| Step: 2
Training loss: 3.5009491859546773
Validation loss: 2.7793574469094957

Epoch: 6| Step: 3
Training loss: 3.07522384325047
Validation loss: 2.7837444704234793

Epoch: 6| Step: 4
Training loss: 2.9748276989191886
Validation loss: 2.7801210555529607

Epoch: 6| Step: 5
Training loss: 3.042018364905425
Validation loss: 2.785801068312134

Epoch: 6| Step: 6
Training loss: 2.959940156073947
Validation loss: 2.7861936940172947

Epoch: 6| Step: 7
Training loss: 3.2358648582125977
Validation loss: 2.7895242358181105

Epoch: 6| Step: 8
Training loss: 2.7687630818835105
Validation loss: 2.7845309728540717

Epoch: 6| Step: 9
Training loss: 2.6951854731162883
Validation loss: 2.7869343074042545

Epoch: 6| Step: 10
Training loss: 2.6141840311952595
Validation loss: 2.7846274465955885

Epoch: 6| Step: 11
Training loss: 3.1752810676700123
Validation loss: 2.773936388645964

Epoch: 6| Step: 12
Training loss: 3.125592747739097
Validation loss: 2.781307085757284

Epoch: 6| Step: 13
Training loss: 3.7189873371201023
Validation loss: 2.779172343685086

Epoch: 79| Step: 0
Training loss: 3.3427906977625526
Validation loss: 2.775279375502714

Epoch: 6| Step: 1
Training loss: 3.2010288730261816
Validation loss: 2.7797126018936935

Epoch: 6| Step: 2
Training loss: 2.754582228788509
Validation loss: 2.775521398145883

Epoch: 6| Step: 3
Training loss: 3.5987209273048766
Validation loss: 2.777472729613889

Epoch: 6| Step: 4
Training loss: 3.630615882855521
Validation loss: 2.7771956695923623

Epoch: 6| Step: 5
Training loss: 3.5351613756005658
Validation loss: 2.7774078129603863

Epoch: 6| Step: 6
Training loss: 2.518619532081333
Validation loss: 2.7796832680754298

Epoch: 6| Step: 7
Training loss: 2.3727973963833167
Validation loss: 2.777907610050152

Epoch: 6| Step: 8
Training loss: 3.963578466711588
Validation loss: 2.776516526733934

Epoch: 6| Step: 9
Training loss: 2.913900825985957
Validation loss: 2.7744946856807085

Epoch: 6| Step: 10
Training loss: 2.8148036423493044
Validation loss: 2.774723729108778

Epoch: 6| Step: 11
Training loss: 2.8936161545846475
Validation loss: 2.778426037861258

Epoch: 6| Step: 12
Training loss: 2.6274243013781873
Validation loss: 2.7765914728694017

Epoch: 6| Step: 13
Training loss: 2.2872716779316384
Validation loss: 2.773636734776631

Epoch: 80| Step: 0
Training loss: 2.546312515772272
Validation loss: 2.77653974330471

Epoch: 6| Step: 1
Training loss: 2.5354048925882333
Validation loss: 2.7771815885271276

Epoch: 6| Step: 2
Training loss: 2.7043276189819485
Validation loss: 2.7745115884109732

Epoch: 6| Step: 3
Training loss: 3.488544518253273
Validation loss: 2.7761118068344497

Epoch: 6| Step: 4
Training loss: 3.09787898567712
Validation loss: 2.7729907741224897

Epoch: 6| Step: 5
Training loss: 3.612738557218402
Validation loss: 2.7765816913595915

Epoch: 6| Step: 6
Training loss: 3.2047290786285236
Validation loss: 2.782546416551693

Epoch: 6| Step: 7
Training loss: 2.791122402447068
Validation loss: 2.790433638607497

Epoch: 6| Step: 8
Training loss: 3.031898822364489
Validation loss: 2.7808710521977513

Epoch: 6| Step: 9
Training loss: 3.038645423963323
Validation loss: 2.785123144335968

Epoch: 6| Step: 10
Training loss: 3.194087682142715
Validation loss: 2.7782168448127265

Epoch: 6| Step: 11
Training loss: 3.322525112955612
Validation loss: 2.7825890977104093

Epoch: 6| Step: 12
Training loss: 3.0860217119146602
Validation loss: 2.776537739694929

Epoch: 6| Step: 13
Training loss: 3.5583260160158012
Validation loss: 2.7767903825618703

Epoch: 81| Step: 0
Training loss: 3.1679479784114037
Validation loss: 2.773180704159707

Epoch: 6| Step: 1
Training loss: 3.432609269416543
Validation loss: 2.7748366223086194

Epoch: 6| Step: 2
Training loss: 2.9839702551435634
Validation loss: 2.775991830507566

Epoch: 6| Step: 3
Training loss: 3.230462550534408
Validation loss: 2.7774001333105067

Epoch: 6| Step: 4
Training loss: 3.0306609849766066
Validation loss: 2.7785495148155093

Epoch: 6| Step: 5
Training loss: 3.287371347905241
Validation loss: 2.773904568646883

Epoch: 6| Step: 6
Training loss: 3.3555602659595207
Validation loss: 2.7787177815649335

Epoch: 6| Step: 7
Training loss: 2.717782034605069
Validation loss: 2.773571863751442

Epoch: 6| Step: 8
Training loss: 2.268874580598891
Validation loss: 2.775955521771611

Epoch: 6| Step: 9
Training loss: 3.411720572528566
Validation loss: 2.7741430942285756

Epoch: 6| Step: 10
Training loss: 3.472384087074606
Validation loss: 2.7779572432663295

Epoch: 6| Step: 11
Training loss: 2.463298911997119
Validation loss: 2.775579364652303

Epoch: 6| Step: 12
Training loss: 3.1170674159886036
Validation loss: 2.7829961656392777

Epoch: 6| Step: 13
Training loss: 3.172784233376884
Validation loss: 2.7911305329804454

Epoch: 82| Step: 0
Training loss: 3.315139726366669
Validation loss: 2.7925336954864823

Epoch: 6| Step: 1
Training loss: 3.2278515178276987
Validation loss: 2.797163262323788

Epoch: 6| Step: 2
Training loss: 3.0498573770164406
Validation loss: 2.804508237891077

Epoch: 6| Step: 3
Training loss: 2.0939443697312146
Validation loss: 2.8100457689425204

Epoch: 6| Step: 4
Training loss: 3.3540554778504386
Validation loss: 2.821536635239903

Epoch: 6| Step: 5
Training loss: 2.0413130110051534
Validation loss: 2.817719925779686

Epoch: 6| Step: 6
Training loss: 2.609392360241184
Validation loss: 2.8329694183658725

Epoch: 6| Step: 7
Training loss: 3.1609994638038863
Validation loss: 2.8340189334707233

Epoch: 6| Step: 8
Training loss: 2.7902753313998083
Validation loss: 2.823303142316982

Epoch: 6| Step: 9
Training loss: 3.9961694018529377
Validation loss: 2.8078446611917505

Epoch: 6| Step: 10
Training loss: 3.601498551535378
Validation loss: 2.782312987143499

Epoch: 6| Step: 11
Training loss: 2.975181599692271
Validation loss: 2.7724533097998556

Epoch: 6| Step: 12
Training loss: 3.438712721095302
Validation loss: 2.7749635247801896

Epoch: 6| Step: 13
Training loss: 3.198832394253165
Validation loss: 2.781851899353041

Epoch: 83| Step: 0
Training loss: 2.707178769626519
Validation loss: 2.7879273434175644

Epoch: 6| Step: 1
Training loss: 3.577465546320107
Validation loss: 2.7912157184575443

Epoch: 6| Step: 2
Training loss: 3.5199980939513162
Validation loss: 2.7989244805637186

Epoch: 6| Step: 3
Training loss: 2.7723083776601634
Validation loss: 2.8019551021647366

Epoch: 6| Step: 4
Training loss: 2.425294590783004
Validation loss: 2.7879006680426994

Epoch: 6| Step: 5
Training loss: 2.996810330040231
Validation loss: 2.774600993852962

Epoch: 6| Step: 6
Training loss: 2.5505614952775844
Validation loss: 2.772164614262843

Epoch: 6| Step: 7
Training loss: 2.4473358655684563
Validation loss: 2.7714936657481695

Epoch: 6| Step: 8
Training loss: 2.8348635486953655
Validation loss: 2.773668218638326

Epoch: 6| Step: 9
Training loss: 3.592052025416099
Validation loss: 2.7822188058502566

Epoch: 6| Step: 10
Training loss: 3.431380216979961
Validation loss: 2.777082563962555

Epoch: 6| Step: 11
Training loss: 3.5998373471709004
Validation loss: 2.775199045782376

Epoch: 6| Step: 12
Training loss: 3.0855721945568324
Validation loss: 2.7723153057392316

Epoch: 6| Step: 13
Training loss: 3.73888058377295
Validation loss: 2.773428744496251

Epoch: 84| Step: 0
Training loss: 2.383823267946417
Validation loss: 2.784339782235054

Epoch: 6| Step: 1
Training loss: 3.486466810529372
Validation loss: 2.7844093116331172

Epoch: 6| Step: 2
Training loss: 3.84051302939848
Validation loss: 2.780347626321545

Epoch: 6| Step: 3
Training loss: 2.83745254947462
Validation loss: 2.77706201844555

Epoch: 6| Step: 4
Training loss: 2.950912050110988
Validation loss: 2.7746079078917796

Epoch: 6| Step: 5
Training loss: 2.9732139355018177
Validation loss: 2.7784652484438785

Epoch: 6| Step: 6
Training loss: 3.2508278305944596
Validation loss: 2.7804980554871053

Epoch: 6| Step: 7
Training loss: 2.749731917752499
Validation loss: 2.7776718766182413

Epoch: 6| Step: 8
Training loss: 2.835202797123668
Validation loss: 2.77129946283777

Epoch: 6| Step: 9
Training loss: 2.917812848809654
Validation loss: 2.780164011852473

Epoch: 6| Step: 10
Training loss: 2.514434437951232
Validation loss: 2.7724779987278727

Epoch: 6| Step: 11
Training loss: 3.405579947266204
Validation loss: 2.7691896997647842

Epoch: 6| Step: 12
Training loss: 3.5073777504985664
Validation loss: 2.7721550344568335

Epoch: 6| Step: 13
Training loss: 3.3110107817017767
Validation loss: 2.7706572126406313

Epoch: 85| Step: 0
Training loss: 3.2053280581252643
Validation loss: 2.775025401488329

Epoch: 6| Step: 1
Training loss: 2.8732588097923437
Validation loss: 2.7778642958494886

Epoch: 6| Step: 2
Training loss: 3.176679008062299
Validation loss: 2.772835609376339

Epoch: 6| Step: 3
Training loss: 3.26741176074547
Validation loss: 2.7663582127151485

Epoch: 6| Step: 4
Training loss: 3.4433892865972524
Validation loss: 2.765843393861131

Epoch: 6| Step: 5
Training loss: 2.697940153709501
Validation loss: 2.7686757426011277

Epoch: 6| Step: 6
Training loss: 2.758986442054213
Validation loss: 2.761890524216216

Epoch: 6| Step: 7
Training loss: 2.746835187985853
Validation loss: 2.7650802814656843

Epoch: 6| Step: 8
Training loss: 3.208992993464537
Validation loss: 2.7668085139986887

Epoch: 6| Step: 9
Training loss: 3.257110300475016
Validation loss: 2.7636477344803225

Epoch: 6| Step: 10
Training loss: 3.2876482389845694
Validation loss: 2.7617913449667704

Epoch: 6| Step: 11
Training loss: 2.9640567021371043
Validation loss: 2.7636320417307654

Epoch: 6| Step: 12
Training loss: 2.8224658301069976
Validation loss: 2.7633800503691757

Epoch: 6| Step: 13
Training loss: 3.619779444978056
Validation loss: 2.762916983223865

Epoch: 86| Step: 0
Training loss: 2.965000073077505
Validation loss: 2.7691374653165024

Epoch: 6| Step: 1
Training loss: 2.7092663453874417
Validation loss: 2.774637407165977

Epoch: 6| Step: 2
Training loss: 3.773301274148827
Validation loss: 2.7737924979769355

Epoch: 6| Step: 3
Training loss: 3.455609424684983
Validation loss: 2.7700485782369957

Epoch: 6| Step: 4
Training loss: 3.335043722663929
Validation loss: 2.76694575686249

Epoch: 6| Step: 5
Training loss: 3.0427952767165096
Validation loss: 2.7648814305844507

Epoch: 6| Step: 6
Training loss: 3.2887420860667977
Validation loss: 2.7651301175779897

Epoch: 6| Step: 7
Training loss: 3.1766242190630893
Validation loss: 2.7603971722487772

Epoch: 6| Step: 8
Training loss: 2.2754245739151955
Validation loss: 2.761086800313904

Epoch: 6| Step: 9
Training loss: 3.1467410834720946
Validation loss: 2.760423895620739

Epoch: 6| Step: 10
Training loss: 2.3257930264544817
Validation loss: 2.7617357420624495

Epoch: 6| Step: 11
Training loss: 3.6996332760090054
Validation loss: 2.763370291674038

Epoch: 6| Step: 12
Training loss: 2.9509747462702896
Validation loss: 2.761283429788135

Epoch: 6| Step: 13
Training loss: 2.2201696877452437
Validation loss: 2.7624245700830743

Epoch: 87| Step: 0
Training loss: 2.731070565411677
Validation loss: 2.762980367470071

Epoch: 6| Step: 1
Training loss: 3.0282911823917398
Validation loss: 2.7585974278720573

Epoch: 6| Step: 2
Training loss: 3.209749990691889
Validation loss: 2.766977407655298

Epoch: 6| Step: 3
Training loss: 2.282299897530039
Validation loss: 2.774121279370174

Epoch: 6| Step: 4
Training loss: 3.360778657288899
Validation loss: 2.7816467610515767

Epoch: 6| Step: 5
Training loss: 3.182955026033812
Validation loss: 2.7818516348655975

Epoch: 6| Step: 6
Training loss: 2.8976431115316186
Validation loss: 2.7904473955590032

Epoch: 6| Step: 7
Training loss: 2.6133208450031966
Validation loss: 2.7866865694640697

Epoch: 6| Step: 8
Training loss: 3.709790889882511
Validation loss: 2.7806457438303083

Epoch: 6| Step: 9
Training loss: 3.234822265364973
Validation loss: 2.7634641932164827

Epoch: 6| Step: 10
Training loss: 3.0805185374436057
Validation loss: 2.7624199011096384

Epoch: 6| Step: 11
Training loss: 3.2551281891175794
Validation loss: 2.7606401077143845

Epoch: 6| Step: 12
Training loss: 3.7790711485803907
Validation loss: 2.7668613696561914

Epoch: 6| Step: 13
Training loss: 1.814172170709833
Validation loss: 2.765948261463967

Epoch: 88| Step: 0
Training loss: 3.2600847914264497
Validation loss: 2.7670644966570164

Epoch: 6| Step: 1
Training loss: 3.9435273314668278
Validation loss: 2.7739800636112215

Epoch: 6| Step: 2
Training loss: 2.502009347228864
Validation loss: 2.773303775894098

Epoch: 6| Step: 3
Training loss: 3.179557928871068
Validation loss: 2.7670990941280453

Epoch: 6| Step: 4
Training loss: 3.1202020051558383
Validation loss: 2.764550341085265

Epoch: 6| Step: 5
Training loss: 3.3098872695119756
Validation loss: 2.7631690270212514

Epoch: 6| Step: 6
Training loss: 3.0839267022295003
Validation loss: 2.764182257470139

Epoch: 6| Step: 7
Training loss: 3.4353367327537523
Validation loss: 2.7617691002263354

Epoch: 6| Step: 8
Training loss: 2.5210328351567974
Validation loss: 2.7637904614059043

Epoch: 6| Step: 9
Training loss: 2.5056022815430876
Validation loss: 2.765476187346534

Epoch: 6| Step: 10
Training loss: 2.868524473546369
Validation loss: 2.764620546224223

Epoch: 6| Step: 11
Training loss: 2.796329551659964
Validation loss: 2.7717841024741814

Epoch: 6| Step: 12
Training loss: 3.4851849217658177
Validation loss: 2.769781355384412

Epoch: 6| Step: 13
Training loss: 2.6952469527525715
Validation loss: 2.777268312317373

Epoch: 89| Step: 0
Training loss: 2.9472627127843465
Validation loss: 2.776692229463254

Epoch: 6| Step: 1
Training loss: 2.934918507057122
Validation loss: 2.774697665945238

Epoch: 6| Step: 2
Training loss: 2.8653665107781356
Validation loss: 2.7711263827481405

Epoch: 6| Step: 3
Training loss: 3.0587264186883933
Validation loss: 2.7679427047728122

Epoch: 6| Step: 4
Training loss: 3.4781567293623774
Validation loss: 2.76633929921351

Epoch: 6| Step: 5
Training loss: 2.6309422038858985
Validation loss: 2.7606517491199978

Epoch: 6| Step: 6
Training loss: 2.762653974842071
Validation loss: 2.760119097120982

Epoch: 6| Step: 7
Training loss: 2.6609977925896047
Validation loss: 2.7621625747369936

Epoch: 6| Step: 8
Training loss: 2.9114793880204055
Validation loss: 2.768137142255612

Epoch: 6| Step: 9
Training loss: 4.002784475096948
Validation loss: 2.761395549513416

Epoch: 6| Step: 10
Training loss: 2.8228789511175205
Validation loss: 2.7626012456259974

Epoch: 6| Step: 11
Training loss: 3.2636528813593033
Validation loss: 2.7671277655174764

Epoch: 6| Step: 12
Training loss: 3.9103062436110094
Validation loss: 2.766292935612276

Epoch: 6| Step: 13
Training loss: 1.877513726358232
Validation loss: 2.771181955425523

Epoch: 90| Step: 0
Training loss: 3.4480456801572643
Validation loss: 2.766979040170784

Epoch: 6| Step: 1
Training loss: 3.0214062553027388
Validation loss: 2.772155113988133

Epoch: 6| Step: 2
Training loss: 3.08925726300218
Validation loss: 2.762232202955778

Epoch: 6| Step: 3
Training loss: 2.8745451857684223
Validation loss: 2.7659098401620064

Epoch: 6| Step: 4
Training loss: 2.8605537694722663
Validation loss: 2.7604011620341318

Epoch: 6| Step: 5
Training loss: 3.2373631628862842
Validation loss: 2.7560934519975016

Epoch: 6| Step: 6
Training loss: 2.8490256534598397
Validation loss: 2.753790293329168

Epoch: 6| Step: 7
Training loss: 3.728112752019775
Validation loss: 2.752513979988562

Epoch: 6| Step: 8
Training loss: 2.5040035615718974
Validation loss: 2.7514100845763205

Epoch: 6| Step: 9
Training loss: 2.647853820759299
Validation loss: 2.7552463319440395

Epoch: 6| Step: 10
Training loss: 2.791545542656188
Validation loss: 2.7549368749829153

Epoch: 6| Step: 11
Training loss: 3.586148124941779
Validation loss: 2.7524229694707825

Epoch: 6| Step: 12
Training loss: 3.309372198951984
Validation loss: 2.7529184855807918

Epoch: 6| Step: 13
Training loss: 2.9221471975902866
Validation loss: 2.753711684664534

Epoch: 91| Step: 0
Training loss: 3.301287550158357
Validation loss: 2.7535581242018234

Epoch: 6| Step: 1
Training loss: 3.3731209504387865
Validation loss: 2.7543486688433108

Epoch: 6| Step: 2
Training loss: 2.4958318295312427
Validation loss: 2.7547125202837646

Epoch: 6| Step: 3
Training loss: 3.2555802196426247
Validation loss: 2.752686099964666

Epoch: 6| Step: 4
Training loss: 3.068724398690705
Validation loss: 2.7532340672384654

Epoch: 6| Step: 5
Training loss: 3.2313560365142058
Validation loss: 2.7508598877557815

Epoch: 6| Step: 6
Training loss: 3.1203673547498965
Validation loss: 2.751987976725948

Epoch: 6| Step: 7
Training loss: 2.978335675916585
Validation loss: 2.7494514809504653

Epoch: 6| Step: 8
Training loss: 1.8423547799840834
Validation loss: 2.759133028205106

Epoch: 6| Step: 9
Training loss: 2.8141254390475092
Validation loss: 2.7572419707817493

Epoch: 6| Step: 10
Training loss: 3.2787098089293347
Validation loss: 2.7657923503243964

Epoch: 6| Step: 11
Training loss: 2.5905362177269833
Validation loss: 2.7639323684417305

Epoch: 6| Step: 12
Training loss: 3.8337037833518552
Validation loss: 2.7700062813618587

Epoch: 6| Step: 13
Training loss: 3.69186388842304
Validation loss: 2.7729205183362358

Epoch: 92| Step: 0
Training loss: 3.5155246296436125
Validation loss: 2.769711368065107

Epoch: 6| Step: 1
Training loss: 2.911550958259866
Validation loss: 2.7526365004997797

Epoch: 6| Step: 2
Training loss: 3.05025885062008
Validation loss: 2.7537736013479317

Epoch: 6| Step: 3
Training loss: 3.386547115073089
Validation loss: 2.7487610461959324

Epoch: 6| Step: 4
Training loss: 2.8940758803832383
Validation loss: 2.748005959763421

Epoch: 6| Step: 5
Training loss: 2.8652453588107467
Validation loss: 2.7503743956273308

Epoch: 6| Step: 6
Training loss: 3.4060314703320183
Validation loss: 2.749566192661045

Epoch: 6| Step: 7
Training loss: 2.699303544254228
Validation loss: 2.7518278863583743

Epoch: 6| Step: 8
Training loss: 2.913057619341491
Validation loss: 2.750208862591693

Epoch: 6| Step: 9
Training loss: 3.3040196530771304
Validation loss: 2.7466016954381196

Epoch: 6| Step: 10
Training loss: 3.237943146176457
Validation loss: 2.748140935380363

Epoch: 6| Step: 11
Training loss: 2.760939945550169
Validation loss: 2.7463723742971813

Epoch: 6| Step: 12
Training loss: 2.9117769579932777
Validation loss: 2.75286372691901

Epoch: 6| Step: 13
Training loss: 3.1501053020496737
Validation loss: 2.7691369746475942

Epoch: 93| Step: 0
Training loss: 3.1492416074392207
Validation loss: 2.774920630250725

Epoch: 6| Step: 1
Training loss: 3.364719273957562
Validation loss: 2.7797821400488276

Epoch: 6| Step: 2
Training loss: 2.3964675616705944
Validation loss: 2.792722579166306

Epoch: 6| Step: 3
Training loss: 3.0931377383031675
Validation loss: 2.799625573342284

Epoch: 6| Step: 4
Training loss: 2.756681907531724
Validation loss: 2.77045284906107

Epoch: 6| Step: 5
Training loss: 2.4804711529577435
Validation loss: 2.762265312891807

Epoch: 6| Step: 6
Training loss: 2.630211153438051
Validation loss: 2.760068550533696

Epoch: 6| Step: 7
Training loss: 3.651575213676653
Validation loss: 2.749290717901032

Epoch: 6| Step: 8
Training loss: 2.9904909428732163
Validation loss: 2.746340188258642

Epoch: 6| Step: 9
Training loss: 3.091682745248229
Validation loss: 2.7486905926744365

Epoch: 6| Step: 10
Training loss: 3.419679259491008
Validation loss: 2.744723163621718

Epoch: 6| Step: 11
Training loss: 3.486077547017329
Validation loss: 2.749401573706169

Epoch: 6| Step: 12
Training loss: 2.9744227771003895
Validation loss: 2.746407090549485

Epoch: 6| Step: 13
Training loss: 3.281540485421214
Validation loss: 2.7475722625977386

Epoch: 94| Step: 0
Training loss: 2.958865444701505
Validation loss: 2.7493742541320194

Epoch: 6| Step: 1
Training loss: 2.855963000146032
Validation loss: 2.750271158283349

Epoch: 6| Step: 2
Training loss: 3.307006599375851
Validation loss: 2.7488236162506587

Epoch: 6| Step: 3
Training loss: 3.4195625469334723
Validation loss: 2.7485101846280986

Epoch: 6| Step: 4
Training loss: 3.765087801655468
Validation loss: 2.7476072184105926

Epoch: 6| Step: 5
Training loss: 2.55123658957501
Validation loss: 2.7506339227219287

Epoch: 6| Step: 6
Training loss: 3.210001491177887
Validation loss: 2.752078180020366

Epoch: 6| Step: 7
Training loss: 2.880354000958459
Validation loss: 2.7533945749074547

Epoch: 6| Step: 8
Training loss: 2.6773936961323703
Validation loss: 2.7596460695095204

Epoch: 6| Step: 9
Training loss: 3.3065301614190705
Validation loss: 2.7692838279709013

Epoch: 6| Step: 10
Training loss: 3.168037686615088
Validation loss: 2.76858476055765

Epoch: 6| Step: 11
Training loss: 2.8985520003495657
Validation loss: 2.767330990616496

Epoch: 6| Step: 12
Training loss: 3.009020594943793
Validation loss: 2.7709681400798885

Epoch: 6| Step: 13
Training loss: 2.537768130864018
Validation loss: 2.7742341869207983

Epoch: 95| Step: 0
Training loss: 3.187648096571082
Validation loss: 2.7817883433110553

Epoch: 6| Step: 1
Training loss: 2.864003588647565
Validation loss: 2.774972135017204

Epoch: 6| Step: 2
Training loss: 3.2758724652106337
Validation loss: 2.790811010545483

Epoch: 6| Step: 3
Training loss: 2.209377857513954
Validation loss: 2.7886919553485843

Epoch: 6| Step: 4
Training loss: 2.6044678374025763
Validation loss: 2.7703412337806936

Epoch: 6| Step: 5
Training loss: 3.079523224032174
Validation loss: 2.765547619494848

Epoch: 6| Step: 6
Training loss: 3.3442790245752323
Validation loss: 2.7568824331761146

Epoch: 6| Step: 7
Training loss: 3.2756900730085468
Validation loss: 2.7441991291882784

Epoch: 6| Step: 8
Training loss: 3.643700344473648
Validation loss: 2.7451510520518463

Epoch: 6| Step: 9
Training loss: 3.4381793651294226
Validation loss: 2.7439763144584783

Epoch: 6| Step: 10
Training loss: 3.1652823651566813
Validation loss: 2.7417858367054384

Epoch: 6| Step: 11
Training loss: 3.2534173458487508
Validation loss: 2.7411440355814776

Epoch: 6| Step: 12
Training loss: 2.5219964320787547
Validation loss: 2.745708415143546

Epoch: 6| Step: 13
Training loss: 2.5220843487034883
Validation loss: 2.7434078988372432

Epoch: 96| Step: 0
Training loss: 3.1802481642043743
Validation loss: 2.748038558223799

Epoch: 6| Step: 1
Training loss: 3.148283015169303
Validation loss: 2.746071782334445

Epoch: 6| Step: 2
Training loss: 3.5127297920187144
Validation loss: 2.7450174293029734

Epoch: 6| Step: 3
Training loss: 2.98948096832225
Validation loss: 2.74620553861859

Epoch: 6| Step: 4
Training loss: 2.3949736891159406
Validation loss: 2.746705824849831

Epoch: 6| Step: 5
Training loss: 2.860416410316035
Validation loss: 2.744934789697897

Epoch: 6| Step: 6
Training loss: 2.7528599692695037
Validation loss: 2.7404265958582275

Epoch: 6| Step: 7
Training loss: 2.7709504487567678
Validation loss: 2.742218929849566

Epoch: 6| Step: 8
Training loss: 3.272067345446112
Validation loss: 2.7404274256373946

Epoch: 6| Step: 9
Training loss: 3.2031653424955673
Validation loss: 2.7426102662381058

Epoch: 6| Step: 10
Training loss: 2.887681606906518
Validation loss: 2.7445913863133318

Epoch: 6| Step: 11
Training loss: 3.4586271751000184
Validation loss: 2.7435276124969996

Epoch: 6| Step: 12
Training loss: 3.1840035204077193
Validation loss: 2.7600340349120964

Epoch: 6| Step: 13
Training loss: 3.3119955578465547
Validation loss: 2.7580657836166584

Epoch: 97| Step: 0
Training loss: 2.7143678975391157
Validation loss: 2.754140587474834

Epoch: 6| Step: 1
Training loss: 2.8797344533038824
Validation loss: 2.7522253967890857

Epoch: 6| Step: 2
Training loss: 2.9510419654150337
Validation loss: 2.744777172164223

Epoch: 6| Step: 3
Training loss: 3.348917310749228
Validation loss: 2.7387689841254304

Epoch: 6| Step: 4
Training loss: 3.2956076260591103
Validation loss: 2.7398891230746085

Epoch: 6| Step: 5
Training loss: 3.25185458747107
Validation loss: 2.738416181622017

Epoch: 6| Step: 6
Training loss: 3.270448180116622
Validation loss: 2.7361896883878454

Epoch: 6| Step: 7
Training loss: 2.929259083259019
Validation loss: 2.7381488405893006

Epoch: 6| Step: 8
Training loss: 2.483362147358222
Validation loss: 2.741901698920754

Epoch: 6| Step: 9
Training loss: 2.811921378060785
Validation loss: 2.7405764880672474

Epoch: 6| Step: 10
Training loss: 3.481227941802732
Validation loss: 2.74045491953109

Epoch: 6| Step: 11
Training loss: 3.490955382910763
Validation loss: 2.741202895514664

Epoch: 6| Step: 12
Training loss: 2.804349661113883
Validation loss: 2.7415299586814394

Epoch: 6| Step: 13
Training loss: 3.0427167638766153
Validation loss: 2.7405805179331484

Epoch: 98| Step: 0
Training loss: 3.5221540392972717
Validation loss: 2.74195445782239

Epoch: 6| Step: 1
Training loss: 2.84775169432386
Validation loss: 2.7412911297386957

Epoch: 6| Step: 2
Training loss: 2.9063381058658524
Validation loss: 2.7409883250621374

Epoch: 6| Step: 3
Training loss: 4.053917607769188
Validation loss: 2.7413969524456174

Epoch: 6| Step: 4
Training loss: 2.9136026546317444
Validation loss: 2.7419179152758897

Epoch: 6| Step: 5
Training loss: 2.756308688622357
Validation loss: 2.7391556302578204

Epoch: 6| Step: 6
Training loss: 2.9348566052112077
Validation loss: 2.7408139650223484

Epoch: 6| Step: 7
Training loss: 2.7155114539741936
Validation loss: 2.7385595542224825

Epoch: 6| Step: 8
Training loss: 2.804992999171804
Validation loss: 2.736230983627295

Epoch: 6| Step: 9
Training loss: 2.4109865610071695
Validation loss: 2.7396045934933873

Epoch: 6| Step: 10
Training loss: 3.2168698050703632
Validation loss: 2.739426046443431

Epoch: 6| Step: 11
Training loss: 3.0485096776757676
Validation loss: 2.7397755683659164

Epoch: 6| Step: 12
Training loss: 3.1436051989476326
Validation loss: 2.7382733950815976

Epoch: 6| Step: 13
Training loss: 3.483248130693251
Validation loss: 2.740843264001074

Epoch: 99| Step: 0
Training loss: 3.847232264671271
Validation loss: 2.746639323719209

Epoch: 6| Step: 1
Training loss: 3.0044413116101416
Validation loss: 2.7424461715563235

Epoch: 6| Step: 2
Training loss: 3.3165014318459933
Validation loss: 2.7431646475320397

Epoch: 6| Step: 3
Training loss: 3.1654405395148557
Validation loss: 2.7427685018133006

Epoch: 6| Step: 4
Training loss: 2.949172508433013
Validation loss: 2.742169776319095

Epoch: 6| Step: 5
Training loss: 3.1253440667522505
Validation loss: 2.7410837518461606

Epoch: 6| Step: 6
Training loss: 2.6251441371037263
Validation loss: 2.7456903127011247

Epoch: 6| Step: 7
Training loss: 2.8587303894197666
Validation loss: 2.7421015075713266

Epoch: 6| Step: 8
Training loss: 3.1483869146254198
Validation loss: 2.742864507838878

Epoch: 6| Step: 9
Training loss: 2.925414201150158
Validation loss: 2.7423044352792396

Epoch: 6| Step: 10
Training loss: 2.382734653889763
Validation loss: 2.7437510097962536

Epoch: 6| Step: 11
Training loss: 3.1834035243206644
Validation loss: 2.7446360438394932

Epoch: 6| Step: 12
Training loss: 3.1571270319043316
Validation loss: 2.739336910805377

Epoch: 6| Step: 13
Training loss: 2.9206855515240506
Validation loss: 2.7412432005969327

Epoch: 100| Step: 0
Training loss: 2.817494344900256
Validation loss: 2.73986355007291

Epoch: 6| Step: 1
Training loss: 3.1289272807696773
Validation loss: 2.734725713080709

Epoch: 6| Step: 2
Training loss: 3.2387019161422583
Validation loss: 2.736751030120997

Epoch: 6| Step: 3
Training loss: 2.997094018468889
Validation loss: 2.735898362865302

Epoch: 6| Step: 4
Training loss: 3.4205532918056574
Validation loss: 2.7350751026860185

Epoch: 6| Step: 5
Training loss: 2.7400329472308176
Validation loss: 2.733289299049207

Epoch: 6| Step: 6
Training loss: 3.1182770701120077
Validation loss: 2.7320722868406255

Epoch: 6| Step: 7
Training loss: 3.4930047565977143
Validation loss: 2.734035388113635

Epoch: 6| Step: 8
Training loss: 2.728808857610234
Validation loss: 2.733898094766954

Epoch: 6| Step: 9
Training loss: 2.486385467659152
Validation loss: 2.7338038116019865

Epoch: 6| Step: 10
Training loss: 3.01933984779042
Validation loss: 2.732534278291577

Epoch: 6| Step: 11
Training loss: 3.2141996644627144
Validation loss: 2.731728917286966

Epoch: 6| Step: 12
Training loss: 3.3594361765414402
Validation loss: 2.7328036423325632

Epoch: 6| Step: 13
Training loss: 2.7798660778473847
Validation loss: 2.731696642237499

Epoch: 101| Step: 0
Training loss: 3.2738998195861955
Validation loss: 2.735093389738371

Epoch: 6| Step: 1
Training loss: 3.0256714761305012
Validation loss: 2.7325506553146157

Epoch: 6| Step: 2
Training loss: 2.4562610577438755
Validation loss: 2.7360335019542044

Epoch: 6| Step: 3
Training loss: 3.717709948852108
Validation loss: 2.7385500646825864

Epoch: 6| Step: 4
Training loss: 3.4620395395543215
Validation loss: 2.737133518946225

Epoch: 6| Step: 5
Training loss: 2.893970430024038
Validation loss: 2.735165692227894

Epoch: 6| Step: 6
Training loss: 2.64695946064376
Validation loss: 2.7323399568960567

Epoch: 6| Step: 7
Training loss: 2.9156015540322033
Validation loss: 2.7383344962032248

Epoch: 6| Step: 8
Training loss: 3.314682439492902
Validation loss: 2.7330443367045634

Epoch: 6| Step: 9
Training loss: 2.2376811502958605
Validation loss: 2.7401995608971594

Epoch: 6| Step: 10
Training loss: 3.2589056801785174
Validation loss: 2.7368783730960216

Epoch: 6| Step: 11
Training loss: 2.802651604151274
Validation loss: 2.7361746411190135

Epoch: 6| Step: 12
Training loss: 3.207064109197147
Validation loss: 2.7335462204285403

Epoch: 6| Step: 13
Training loss: 3.378683975935116
Validation loss: 2.7363007863899993

Epoch: 102| Step: 0
Training loss: 3.2645402046540544
Validation loss: 2.737565953704841

Epoch: 6| Step: 1
Training loss: 2.3010329414871133
Validation loss: 2.7412168125579877

Epoch: 6| Step: 2
Training loss: 2.742369469150422
Validation loss: 2.750047264737048

Epoch: 6| Step: 3
Training loss: 3.489751525165862
Validation loss: 2.745528509584978

Epoch: 6| Step: 4
Training loss: 2.7552050835074073
Validation loss: 2.7431794433807393

Epoch: 6| Step: 5
Training loss: 3.4432984431648053
Validation loss: 2.737964095608966

Epoch: 6| Step: 6
Training loss: 3.590833591813221
Validation loss: 2.7396474916757763

Epoch: 6| Step: 7
Training loss: 3.3162732496427956
Validation loss: 2.7325963502174853

Epoch: 6| Step: 8
Training loss: 3.090997878242685
Validation loss: 2.737918409766085

Epoch: 6| Step: 9
Training loss: 3.1113134121591868
Validation loss: 2.73286881505226

Epoch: 6| Step: 10
Training loss: 2.5572859588580066
Validation loss: 2.7299845588629337

Epoch: 6| Step: 11
Training loss: 2.2513120852164197
Validation loss: 2.728465071725793

Epoch: 6| Step: 12
Training loss: 3.3873089162838306
Validation loss: 2.729807423327672

Epoch: 6| Step: 13
Training loss: 2.9933142868602816
Validation loss: 2.7305661649046553

Epoch: 103| Step: 0
Training loss: 3.351852440019263
Validation loss: 2.7289855760432222

Epoch: 6| Step: 1
Training loss: 3.306593757743391
Validation loss: 2.7308291234792477

Epoch: 6| Step: 2
Training loss: 3.590261341666487
Validation loss: 2.7305925206742137

Epoch: 6| Step: 3
Training loss: 1.860250635488161
Validation loss: 2.728595445288568

Epoch: 6| Step: 4
Training loss: 3.3769123169797917
Validation loss: 2.7264954257562035

Epoch: 6| Step: 5
Training loss: 3.193865385416031
Validation loss: 2.7303745315076426

Epoch: 6| Step: 6
Training loss: 3.3256983260322825
Validation loss: 2.7423445792723657

Epoch: 6| Step: 7
Training loss: 2.8400072892861106
Validation loss: 2.7370528733232304

Epoch: 6| Step: 8
Training loss: 2.8075978260201975
Validation loss: 2.745101440182333

Epoch: 6| Step: 9
Training loss: 2.4794835334257206
Validation loss: 2.746365973524844

Epoch: 6| Step: 10
Training loss: 3.4682916561801864
Validation loss: 2.756499849012122

Epoch: 6| Step: 11
Training loss: 3.5154649486137166
Validation loss: 2.7378223894823477

Epoch: 6| Step: 12
Training loss: 2.4324291988156563
Validation loss: 2.732638830236786

Epoch: 6| Step: 13
Training loss: 2.3761485484789198
Validation loss: 2.727895411653637

Epoch: 104| Step: 0
Training loss: 3.4909769644063284
Validation loss: 2.7271563589042964

Epoch: 6| Step: 1
Training loss: 3.2770200076151594
Validation loss: 2.724375117141872

Epoch: 6| Step: 2
Training loss: 2.9119317085621343
Validation loss: 2.728815973164995

Epoch: 6| Step: 3
Training loss: 3.418416962799473
Validation loss: 2.7288628504344583

Epoch: 6| Step: 4
Training loss: 2.0953463262207133
Validation loss: 2.7297660440621616

Epoch: 6| Step: 5
Training loss: 3.21268699487684
Validation loss: 2.7343932504821287

Epoch: 6| Step: 6
Training loss: 2.8185554589511104
Validation loss: 2.7317264828994507

Epoch: 6| Step: 7
Training loss: 3.379522049196581
Validation loss: 2.732195042812201

Epoch: 6| Step: 8
Training loss: 2.5600718647585405
Validation loss: 2.7313398888990914

Epoch: 6| Step: 9
Training loss: 2.8037770103075337
Validation loss: 2.7311404343579446

Epoch: 6| Step: 10
Training loss: 3.0627019387443464
Validation loss: 2.728103034652704

Epoch: 6| Step: 11
Training loss: 3.3237330152429827
Validation loss: 2.7289360507550615

Epoch: 6| Step: 12
Training loss: 3.0785395154351525
Validation loss: 2.7286961759022765

Epoch: 6| Step: 13
Training loss: 3.158937122401478
Validation loss: 2.727964485123365

Epoch: 105| Step: 0
Training loss: 3.077052515315076
Validation loss: 2.7288897412281754

Epoch: 6| Step: 1
Training loss: 3.2123733614292433
Validation loss: 2.7298207523494917

Epoch: 6| Step: 2
Training loss: 2.7742741571993563
Validation loss: 2.7269563276203628

Epoch: 6| Step: 3
Training loss: 2.863769489623295
Validation loss: 2.7402952892603523

Epoch: 6| Step: 4
Training loss: 3.2715307249515106
Validation loss: 2.733165208711693

Epoch: 6| Step: 5
Training loss: 3.243639811443218
Validation loss: 2.734132954421889

Epoch: 6| Step: 6
Training loss: 2.722102148517596
Validation loss: 2.7369299586863534

Epoch: 6| Step: 7
Training loss: 2.9258459514395447
Validation loss: 2.7403901498757572

Epoch: 6| Step: 8
Training loss: 3.1400077554886403
Validation loss: 2.7289077343506065

Epoch: 6| Step: 9
Training loss: 3.559249465778362
Validation loss: 2.7303484299652174

Epoch: 6| Step: 10
Training loss: 2.8278654200677393
Validation loss: 2.730967196305927

Epoch: 6| Step: 11
Training loss: 3.1630444808085203
Validation loss: 2.724702039032265

Epoch: 6| Step: 12
Training loss: 2.810972180240049
Validation loss: 2.7259852075029247

Epoch: 6| Step: 13
Training loss: 3.145175654565463
Validation loss: 2.7275417337222816

Epoch: 106| Step: 0
Training loss: 3.4371827412633125
Validation loss: 2.7270567671905415

Epoch: 6| Step: 1
Training loss: 2.906860123446382
Validation loss: 2.72936373251759

Epoch: 6| Step: 2
Training loss: 3.0025852508257884
Validation loss: 2.7267618550413792

Epoch: 6| Step: 3
Training loss: 2.329580683981023
Validation loss: 2.726996011281598

Epoch: 6| Step: 4
Training loss: 3.0212471689472182
Validation loss: 2.7296850323860347

Epoch: 6| Step: 5
Training loss: 3.8464898989658973
Validation loss: 2.7307768368332055

Epoch: 6| Step: 6
Training loss: 3.681169280379943
Validation loss: 2.7310661685635855

Epoch: 6| Step: 7
Training loss: 2.8346653967226807
Validation loss: 2.7279528047815393

Epoch: 6| Step: 8
Training loss: 1.8735473091580368
Validation loss: 2.7306476358199294

Epoch: 6| Step: 9
Training loss: 3.0097928748360845
Validation loss: 2.7287674857500663

Epoch: 6| Step: 10
Training loss: 3.6537036342856797
Validation loss: 2.732466354253792

Epoch: 6| Step: 11
Training loss: 2.4321250341109257
Validation loss: 2.7266406201951026

Epoch: 6| Step: 12
Training loss: 3.059476021179104
Validation loss: 2.731579276315758

Epoch: 6| Step: 13
Training loss: 3.0797825721686096
Validation loss: 2.725588319055894

Epoch: 107| Step: 0
Training loss: 2.7273211366518573
Validation loss: 2.7272871588066914

Epoch: 6| Step: 1
Training loss: 2.866590058241973
Validation loss: 2.7310931662631392

Epoch: 6| Step: 2
Training loss: 2.462668836198535
Validation loss: 2.7355376534960953

Epoch: 6| Step: 3
Training loss: 2.706520726007399
Validation loss: 2.731576314346357

Epoch: 6| Step: 4
Training loss: 3.3152566631489804
Validation loss: 2.7315667554973433

Epoch: 6| Step: 5
Training loss: 3.5084717264915817
Validation loss: 2.735696244743633

Epoch: 6| Step: 6
Training loss: 3.5239574107996656
Validation loss: 2.741987251641163

Epoch: 6| Step: 7
Training loss: 3.5324807511957315
Validation loss: 2.740960422177742

Epoch: 6| Step: 8
Training loss: 2.7341339877042836
Validation loss: 2.7391231263712585

Epoch: 6| Step: 9
Training loss: 3.1537426346470983
Validation loss: 2.739830671886281

Epoch: 6| Step: 10
Training loss: 2.704378928703344
Validation loss: 2.7315079263098716

Epoch: 6| Step: 11
Training loss: 3.110293152169364
Validation loss: 2.7298394145611136

Epoch: 6| Step: 12
Training loss: 3.30042238711193
Validation loss: 2.7315640140677084

Epoch: 6| Step: 13
Training loss: 2.6440238973082963
Validation loss: 2.7241846602824245

Epoch: 108| Step: 0
Training loss: 2.5882847208770685
Validation loss: 2.7291104717116275

Epoch: 6| Step: 1
Training loss: 2.3298229196080453
Validation loss: 2.720035074352903

Epoch: 6| Step: 2
Training loss: 3.3414997836051117
Validation loss: 2.723367849659332

Epoch: 6| Step: 3
Training loss: 3.0685076274059093
Validation loss: 2.7213918259484817

Epoch: 6| Step: 4
Training loss: 3.3540278972723456
Validation loss: 2.7212486567324294

Epoch: 6| Step: 5
Training loss: 3.48542284070958
Validation loss: 2.7217178666132638

Epoch: 6| Step: 6
Training loss: 3.3452909252427836
Validation loss: 2.719929791926657

Epoch: 6| Step: 7
Training loss: 2.5538423872717515
Validation loss: 2.7204477951711508

Epoch: 6| Step: 8
Training loss: 2.7963446428621452
Validation loss: 2.7230983439997143

Epoch: 6| Step: 9
Training loss: 2.896325671014592
Validation loss: 2.724311563052087

Epoch: 6| Step: 10
Training loss: 3.015265092472113
Validation loss: 2.722014769051606

Epoch: 6| Step: 11
Training loss: 3.6797595492879926
Validation loss: 2.717843905792825

Epoch: 6| Step: 12
Training loss: 2.586664840887677
Validation loss: 2.720611921026324

Epoch: 6| Step: 13
Training loss: 3.521284507119259
Validation loss: 2.717808761485915

Epoch: 109| Step: 0
Training loss: 2.778531063513713
Validation loss: 2.7214909090492765

Epoch: 6| Step: 1
Training loss: 2.9245028994450157
Validation loss: 2.7199239246133846

Epoch: 6| Step: 2
Training loss: 2.4842344040344164
Validation loss: 2.7181445126335637

Epoch: 6| Step: 3
Training loss: 3.405074730499771
Validation loss: 2.7175515918441993

Epoch: 6| Step: 4
Training loss: 2.435405907608515
Validation loss: 2.7186852846893426

Epoch: 6| Step: 5
Training loss: 2.7749669975174105
Validation loss: 2.720160659949953

Epoch: 6| Step: 6
Training loss: 3.6603411095509997
Validation loss: 2.7234772219061587

Epoch: 6| Step: 7
Training loss: 2.955868854106728
Validation loss: 2.72435063594102

Epoch: 6| Step: 8
Training loss: 3.546780085028901
Validation loss: 2.7270257633076023

Epoch: 6| Step: 9
Training loss: 2.660326892012198
Validation loss: 2.7255958361807515

Epoch: 6| Step: 10
Training loss: 2.979851455058434
Validation loss: 2.722593610586198

Epoch: 6| Step: 11
Training loss: 3.3273873384750594
Validation loss: 2.7249906036434717

Epoch: 6| Step: 12
Training loss: 3.3327043575692303
Validation loss: 2.724048791268587

Epoch: 6| Step: 13
Training loss: 3.0789330675631277
Validation loss: 2.7170729631586665

Epoch: 110| Step: 0
Training loss: 2.540723802960906
Validation loss: 2.7218755083813697

Epoch: 6| Step: 1
Training loss: 3.331870393794591
Validation loss: 2.717633231234615

Epoch: 6| Step: 2
Training loss: 3.166737271241231
Validation loss: 2.7179060585444645

Epoch: 6| Step: 3
Training loss: 3.543330770248199
Validation loss: 2.715586680433132

Epoch: 6| Step: 4
Training loss: 2.018113367183612
Validation loss: 2.715186096594294

Epoch: 6| Step: 5
Training loss: 2.7468062842478704
Validation loss: 2.717976621493384

Epoch: 6| Step: 6
Training loss: 2.8247394846747405
Validation loss: 2.7126412187946154

Epoch: 6| Step: 7
Training loss: 2.788387697795368
Validation loss: 2.7183525144717984

Epoch: 6| Step: 8
Training loss: 3.262460297675466
Validation loss: 2.715634735760989

Epoch: 6| Step: 9
Training loss: 3.4406070885464892
Validation loss: 2.7150255169694155

Epoch: 6| Step: 10
Training loss: 3.6771044563933097
Validation loss: 2.71604878595914

Epoch: 6| Step: 11
Training loss: 2.8386580603764577
Validation loss: 2.7138967263833944

Epoch: 6| Step: 12
Training loss: 3.225554449636273
Validation loss: 2.717394831339013

Epoch: 6| Step: 13
Training loss: 2.655649453883479
Validation loss: 2.7172763948260883

Epoch: 111| Step: 0
Training loss: 3.1106469701259747
Validation loss: 2.7177795235815814

Epoch: 6| Step: 1
Training loss: 2.766479877309802
Validation loss: 2.7202862058222106

Epoch: 6| Step: 2
Training loss: 3.2007024768822263
Validation loss: 2.716992893055092

Epoch: 6| Step: 3
Training loss: 3.1630610635503147
Validation loss: 2.7262762382324115

Epoch: 6| Step: 4
Training loss: 3.3006718905238133
Validation loss: 2.720173092844257

Epoch: 6| Step: 5
Training loss: 2.1426391036502648
Validation loss: 2.7293164317741

Epoch: 6| Step: 6
Training loss: 2.9086698896641012
Validation loss: 2.7354055780084905

Epoch: 6| Step: 7
Training loss: 2.8914869518298953
Validation loss: 2.7631844858152537

Epoch: 6| Step: 8
Training loss: 3.4466501696970857
Validation loss: 2.784162652860656

Epoch: 6| Step: 9
Training loss: 3.130693970307186
Validation loss: 2.7934296978908075

Epoch: 6| Step: 10
Training loss: 3.401518869536374
Validation loss: 2.792553111836241

Epoch: 6| Step: 11
Training loss: 2.352500551002754
Validation loss: 2.7369069245740065

Epoch: 6| Step: 12
Training loss: 2.7862846153987593
Validation loss: 2.7244236130962913

Epoch: 6| Step: 13
Training loss: 4.133371585750978
Validation loss: 2.7203310041296622

Epoch: 112| Step: 0
Training loss: 3.0687592049827734
Validation loss: 2.7152711862453778

Epoch: 6| Step: 1
Training loss: 3.4842243247533067
Validation loss: 2.7113603214487334

Epoch: 6| Step: 2
Training loss: 2.7985945375854686
Validation loss: 2.717137551756853

Epoch: 6| Step: 3
Training loss: 3.0782314224048153
Validation loss: 2.7253795976917528

Epoch: 6| Step: 4
Training loss: 3.414235963932081
Validation loss: 2.7322789422159977

Epoch: 6| Step: 5
Training loss: 2.683858577982019
Validation loss: 2.730325774123619

Epoch: 6| Step: 6
Training loss: 2.924519856487764
Validation loss: 2.7243380263805754

Epoch: 6| Step: 7
Training loss: 2.911156071695073
Validation loss: 2.720596625521864

Epoch: 6| Step: 8
Training loss: 3.2803895821299753
Validation loss: 2.7216660963109782

Epoch: 6| Step: 9
Training loss: 2.873146786693249
Validation loss: 2.7199718524162906

Epoch: 6| Step: 10
Training loss: 2.8823243428262084
Validation loss: 2.721944751568411

Epoch: 6| Step: 11
Training loss: 2.936652365601697
Validation loss: 2.720731214906525

Epoch: 6| Step: 12
Training loss: 3.1436552545365086
Validation loss: 2.719049130114074

Epoch: 6| Step: 13
Training loss: 3.390544837672059
Validation loss: 2.7217448892234994

Epoch: 113| Step: 0
Training loss: 3.639787438442786
Validation loss: 2.7172396572024433

Epoch: 6| Step: 1
Training loss: 2.556912168066216
Validation loss: 2.7151327014081286

Epoch: 6| Step: 2
Training loss: 2.733217528456403
Validation loss: 2.7162699898680756

Epoch: 6| Step: 3
Training loss: 2.8900363632079205
Validation loss: 2.7236610199677544

Epoch: 6| Step: 4
Training loss: 3.5087718034208843
Validation loss: 2.7300391200359115

Epoch: 6| Step: 5
Training loss: 2.4318467140954696
Validation loss: 2.730224851985303

Epoch: 6| Step: 6
Training loss: 2.62627271044143
Validation loss: 2.7355376394386726

Epoch: 6| Step: 7
Training loss: 3.1045533180518037
Validation loss: 2.7362395705228084

Epoch: 6| Step: 8
Training loss: 2.6168032235961016
Validation loss: 2.756748249170398

Epoch: 6| Step: 9
Training loss: 3.5005559479724435
Validation loss: 2.7457954411468437

Epoch: 6| Step: 10
Training loss: 3.258543815221046
Validation loss: 2.7432999734920185

Epoch: 6| Step: 11
Training loss: 2.6341627150830473
Validation loss: 2.733950691175112

Epoch: 6| Step: 12
Training loss: 3.3615340990421974
Validation loss: 2.72832827891203

Epoch: 6| Step: 13
Training loss: 3.64070699050982
Validation loss: 2.7272885791411725

Epoch: 114| Step: 0
Training loss: 2.466150001165191
Validation loss: 2.7217120898114517

Epoch: 6| Step: 1
Training loss: 3.4620458752674055
Validation loss: 2.723157062079626

Epoch: 6| Step: 2
Training loss: 3.628911768782806
Validation loss: 2.7145473872948305

Epoch: 6| Step: 3
Training loss: 2.8043919994705333
Validation loss: 2.7111478391008488

Epoch: 6| Step: 4
Training loss: 3.2728160186256208
Validation loss: 2.7159979345645544

Epoch: 6| Step: 5
Training loss: 3.1710173852131778
Validation loss: 2.714415827581415

Epoch: 6| Step: 6
Training loss: 3.476158821586417
Validation loss: 2.7093594395520113

Epoch: 6| Step: 7
Training loss: 2.4165840682898967
Validation loss: 2.7108847067520254

Epoch: 6| Step: 8
Training loss: 2.9785582092242726
Validation loss: 2.7107414025029644

Epoch: 6| Step: 9
Training loss: 3.3960502570828974
Validation loss: 2.710177259738257

Epoch: 6| Step: 10
Training loss: 2.876888484189797
Validation loss: 2.7126887498177488

Epoch: 6| Step: 11
Training loss: 2.2876088601146396
Validation loss: 2.7139900357809816

Epoch: 6| Step: 12
Training loss: 3.1968002772990767
Validation loss: 2.7098550375722756

Epoch: 6| Step: 13
Training loss: 2.5863001819755227
Validation loss: 2.708317501189068

Epoch: 115| Step: 0
Training loss: 2.585040256622116
Validation loss: 2.7114501744491495

Epoch: 6| Step: 1
Training loss: 3.1424080261512546
Validation loss: 2.715109591903556

Epoch: 6| Step: 2
Training loss: 3.3118309928877196
Validation loss: 2.7180712518231065

Epoch: 6| Step: 3
Training loss: 2.5663103756349885
Validation loss: 2.725054485276461

Epoch: 6| Step: 4
Training loss: 3.4561199474182582
Validation loss: 2.726413464988433

Epoch: 6| Step: 5
Training loss: 2.6052460124877834
Validation loss: 2.7349927747554177

Epoch: 6| Step: 6
Training loss: 3.650863984156428
Validation loss: 2.72862903102879

Epoch: 6| Step: 7
Training loss: 3.258114441708731
Validation loss: 2.722768009432174

Epoch: 6| Step: 8
Training loss: 2.5299546494773115
Validation loss: 2.7240145297955327

Epoch: 6| Step: 9
Training loss: 3.1327790141991567
Validation loss: 2.7180482710538936

Epoch: 6| Step: 10
Training loss: 2.668216543014502
Validation loss: 2.714569364527576

Epoch: 6| Step: 11
Training loss: 4.125303777433065
Validation loss: 2.7138455588255535

Epoch: 6| Step: 12
Training loss: 1.9680078712321476
Validation loss: 2.7124204032390296

Epoch: 6| Step: 13
Training loss: 2.6781543316119563
Validation loss: 2.710368033113863

Epoch: 116| Step: 0
Training loss: 2.994489695486854
Validation loss: 2.708382274920653

Epoch: 6| Step: 1
Training loss: 2.6252710111681954
Validation loss: 2.7101984494553384

Epoch: 6| Step: 2
Training loss: 3.854514455138998
Validation loss: 2.7064918397693307

Epoch: 6| Step: 3
Training loss: 3.022292755821661
Validation loss: 2.7146687636924076

Epoch: 6| Step: 4
Training loss: 3.3197325435592853
Validation loss: 2.7200308896373144

Epoch: 6| Step: 5
Training loss: 3.211420992719651
Validation loss: 2.717221096206418

Epoch: 6| Step: 6
Training loss: 3.1775092834769234
Validation loss: 2.718153556563731

Epoch: 6| Step: 7
Training loss: 3.3700720519561562
Validation loss: 2.7161317730430583

Epoch: 6| Step: 8
Training loss: 2.9167489358107637
Validation loss: 2.724010863166545

Epoch: 6| Step: 9
Training loss: 2.6720881683799185
Validation loss: 2.715800883732914

Epoch: 6| Step: 10
Training loss: 3.425200079204216
Validation loss: 2.7255981283729827

Epoch: 6| Step: 11
Training loss: 2.5890305567205676
Validation loss: 2.709709668419269

Epoch: 6| Step: 12
Training loss: 2.652997347902654
Validation loss: 2.70139840819922

Epoch: 6| Step: 13
Training loss: 1.5073288848577355
Validation loss: 2.704281899556227

Epoch: 117| Step: 0
Training loss: 3.0571578785587104
Validation loss: 2.7017287855161736

Epoch: 6| Step: 1
Training loss: 2.8903072698056866
Validation loss: 2.7042351156910733

Epoch: 6| Step: 2
Training loss: 2.613512516479997
Validation loss: 2.7008512039363595

Epoch: 6| Step: 3
Training loss: 3.31780839626421
Validation loss: 2.704332700134395

Epoch: 6| Step: 4
Training loss: 3.069133658423667
Validation loss: 2.706315415417895

Epoch: 6| Step: 5
Training loss: 2.358941651014417
Validation loss: 2.703436362274218

Epoch: 6| Step: 6
Training loss: 2.9886029238631613
Validation loss: 2.7016446049433784

Epoch: 6| Step: 7
Training loss: 3.4564049794663934
Validation loss: 2.699400820725759

Epoch: 6| Step: 8
Training loss: 2.6043504777252307
Validation loss: 2.701659065474205

Epoch: 6| Step: 9
Training loss: 2.482533570184602
Validation loss: 2.7022211120268476

Epoch: 6| Step: 10
Training loss: 3.319339959820305
Validation loss: 2.7028005674271114

Epoch: 6| Step: 11
Training loss: 3.6234935392625096
Validation loss: 2.70205280834688

Epoch: 6| Step: 12
Training loss: 3.1649436948085814
Validation loss: 2.7025733568383203

Epoch: 6| Step: 13
Training loss: 3.354454659997509
Validation loss: 2.700663142422444

Epoch: 118| Step: 0
Training loss: 3.041463104821948
Validation loss: 2.7049442968792907

Epoch: 6| Step: 1
Training loss: 3.2026268548431154
Validation loss: 2.705443835464091

Epoch: 6| Step: 2
Training loss: 3.315291470103086
Validation loss: 2.7112676987910715

Epoch: 6| Step: 3
Training loss: 3.0186591030044063
Validation loss: 2.720996212143068

Epoch: 6| Step: 4
Training loss: 2.3318554989324727
Validation loss: 2.723120411323557

Epoch: 6| Step: 5
Training loss: 3.5769560791083412
Validation loss: 2.73446643119807

Epoch: 6| Step: 6
Training loss: 2.5363627473861317
Validation loss: 2.7255489177707837

Epoch: 6| Step: 7
Training loss: 2.8178482067954844
Validation loss: 2.7141532269423925

Epoch: 6| Step: 8
Training loss: 3.2108020614683936
Validation loss: 2.715012148360815

Epoch: 6| Step: 9
Training loss: 3.0800018010815395
Validation loss: 2.703724770125893

Epoch: 6| Step: 10
Training loss: 3.0911842264535125
Validation loss: 2.7047097196583505

Epoch: 6| Step: 11
Training loss: 3.4031722136445954
Validation loss: 2.704509323970698

Epoch: 6| Step: 12
Training loss: 2.7887700459141547
Validation loss: 2.70216061267354

Epoch: 6| Step: 13
Training loss: 2.6224809094253274
Validation loss: 2.705229038571824

Epoch: 119| Step: 0
Training loss: 3.3313339594983615
Validation loss: 2.7020632144929997

Epoch: 6| Step: 1
Training loss: 3.4783505759903197
Validation loss: 2.7081853922406305

Epoch: 6| Step: 2
Training loss: 2.9839060150256658
Validation loss: 2.7074085797659

Epoch: 6| Step: 3
Training loss: 2.928253555325656
Validation loss: 2.70130285404135

Epoch: 6| Step: 4
Training loss: 2.9357973399262702
Validation loss: 2.708465411718039

Epoch: 6| Step: 5
Training loss: 3.0236242271150156
Validation loss: 2.7112316645661023

Epoch: 6| Step: 6
Training loss: 2.622374720458446
Validation loss: 2.70861096595914

Epoch: 6| Step: 7
Training loss: 3.2434103256867584
Validation loss: 2.7069789241757642

Epoch: 6| Step: 8
Training loss: 3.6784926497321293
Validation loss: 2.7086252946513785

Epoch: 6| Step: 9
Training loss: 2.315513194238299
Validation loss: 2.712453042808076

Epoch: 6| Step: 10
Training loss: 2.9594073607015825
Validation loss: 2.7122678273726817

Epoch: 6| Step: 11
Training loss: 3.077603523502658
Validation loss: 2.7159580354599737

Epoch: 6| Step: 12
Training loss: 3.0109000868321325
Validation loss: 2.71137019736875

Epoch: 6| Step: 13
Training loss: 2.417477756622772
Validation loss: 2.7117228656340893

Epoch: 120| Step: 0
Training loss: 2.742062818512987
Validation loss: 2.7123630793642457

Epoch: 6| Step: 1
Training loss: 2.836538708225523
Validation loss: 2.718219992546606

Epoch: 6| Step: 2
Training loss: 2.9157405654557036
Validation loss: 2.7171999895590853

Epoch: 6| Step: 3
Training loss: 3.539502653325457
Validation loss: 2.7260546655746354

Epoch: 6| Step: 4
Training loss: 3.6708715488091443
Validation loss: 2.7047484179224663

Epoch: 6| Step: 5
Training loss: 3.180840810304415
Validation loss: 2.6973031393594615

Epoch: 6| Step: 6
Training loss: 3.3344218543066844
Validation loss: 2.7042049688202807

Epoch: 6| Step: 7
Training loss: 3.2728492372116076
Validation loss: 2.7067384095486076

Epoch: 6| Step: 8
Training loss: 2.81824711544986
Validation loss: 2.703781523350729

Epoch: 6| Step: 9
Training loss: 2.825970076373526
Validation loss: 2.705628213303189

Epoch: 6| Step: 10
Training loss: 2.8921250858507195
Validation loss: 2.705084218794692

Epoch: 6| Step: 11
Training loss: 2.7575174233239763
Validation loss: 2.7110186624359294

Epoch: 6| Step: 12
Training loss: 2.6906749689688003
Validation loss: 2.714925455708373

Epoch: 6| Step: 13
Training loss: 2.779619917924492
Validation loss: 2.7128101394434223

Epoch: 121| Step: 0
Training loss: 3.1744016909617874
Validation loss: 2.722254303153065

Epoch: 6| Step: 1
Training loss: 3.480008113018532
Validation loss: 2.716120407562338

Epoch: 6| Step: 2
Training loss: 3.523318946349461
Validation loss: 2.710209160126936

Epoch: 6| Step: 3
Training loss: 3.2493044769212127
Validation loss: 2.7055213629950146

Epoch: 6| Step: 4
Training loss: 3.1285447424977875
Validation loss: 2.703751286086251

Epoch: 6| Step: 5
Training loss: 2.9691974503467278
Validation loss: 2.703281805614226

Epoch: 6| Step: 6
Training loss: 3.1782973341227527
Validation loss: 2.7016294952713236

Epoch: 6| Step: 7
Training loss: 2.061854868475768
Validation loss: 2.7044697814740366

Epoch: 6| Step: 8
Training loss: 3.3450905082557707
Validation loss: 2.7004982123658308

Epoch: 6| Step: 9
Training loss: 2.653545214261871
Validation loss: 2.7003800800292557

Epoch: 6| Step: 10
Training loss: 2.5086981139545723
Validation loss: 2.7018830842149595

Epoch: 6| Step: 11
Training loss: 3.3406665881488125
Validation loss: 2.700097228175443

Epoch: 6| Step: 12
Training loss: 2.580060013065686
Validation loss: 2.705629041436767

Epoch: 6| Step: 13
Training loss: 2.999470982002493
Validation loss: 2.7057517644303775

Epoch: 122| Step: 0
Training loss: 3.1654760899970125
Validation loss: 2.7088460429052583

Epoch: 6| Step: 1
Training loss: 3.415508601237443
Validation loss: 2.7087829762448927

Epoch: 6| Step: 2
Training loss: 3.037428389596013
Validation loss: 2.720215478483602

Epoch: 6| Step: 3
Training loss: 3.542040517186474
Validation loss: 2.72537611256453

Epoch: 6| Step: 4
Training loss: 3.087229166237214
Validation loss: 2.726991950059746

Epoch: 6| Step: 5
Training loss: 3.071181467958359
Validation loss: 2.7250977291582665

Epoch: 6| Step: 6
Training loss: 2.7337383291992228
Validation loss: 2.731534901811058

Epoch: 6| Step: 7
Training loss: 3.0466673168902325
Validation loss: 2.7420086069146774

Epoch: 6| Step: 8
Training loss: 2.905881632768215
Validation loss: 2.7292728282874585

Epoch: 6| Step: 9
Training loss: 2.858111159547436
Validation loss: 2.71628400162348

Epoch: 6| Step: 10
Training loss: 3.16602737264554
Validation loss: 2.713450723981797

Epoch: 6| Step: 11
Training loss: 2.3267614480672827
Validation loss: 2.714912724941121

Epoch: 6| Step: 12
Training loss: 2.7906944138116048
Validation loss: 2.7083974056064024

Epoch: 6| Step: 13
Training loss: 3.203946040924269
Validation loss: 2.7036253801325305

Epoch: 123| Step: 0
Training loss: 3.4632563346794396
Validation loss: 2.704266871927534

Epoch: 6| Step: 1
Training loss: 2.925597242367503
Validation loss: 2.6997755814978404

Epoch: 6| Step: 2
Training loss: 3.126251580422119
Validation loss: 2.696865635502969

Epoch: 6| Step: 3
Training loss: 2.450406262715625
Validation loss: 2.700211204926046

Epoch: 6| Step: 4
Training loss: 2.6496302076755462
Validation loss: 2.7000396683064154

Epoch: 6| Step: 5
Training loss: 2.959957393391148
Validation loss: 2.6975377875857918

Epoch: 6| Step: 6
Training loss: 3.2629810197882008
Validation loss: 2.6991230094504552

Epoch: 6| Step: 7
Training loss: 3.6362876103863426
Validation loss: 2.69868219932989

Epoch: 6| Step: 8
Training loss: 2.8682239124884252
Validation loss: 2.6960393043709425

Epoch: 6| Step: 9
Training loss: 2.5002836066551524
Validation loss: 2.69794850424278

Epoch: 6| Step: 10
Training loss: 2.8754607121730906
Validation loss: 2.697655853373805

Epoch: 6| Step: 11
Training loss: 2.9004014756741476
Validation loss: 2.6954002904661203

Epoch: 6| Step: 12
Training loss: 3.416410917309012
Validation loss: 2.6920630990907237

Epoch: 6| Step: 13
Training loss: 3.242462220100536
Validation loss: 2.695510462737148

Epoch: 124| Step: 0
Training loss: 3.2617045693460454
Validation loss: 2.6929936227618123

Epoch: 6| Step: 1
Training loss: 2.789705517585577
Validation loss: 2.697238290608482

Epoch: 6| Step: 2
Training loss: 3.4810024755612115
Validation loss: 2.6955592516447036

Epoch: 6| Step: 3
Training loss: 3.4612753124144975
Validation loss: 2.698933530941534

Epoch: 6| Step: 4
Training loss: 2.1706194644750854
Validation loss: 2.696736423995611

Epoch: 6| Step: 5
Training loss: 2.976533023558118
Validation loss: 2.6943094506710366

Epoch: 6| Step: 6
Training loss: 2.7295061633139515
Validation loss: 2.693465464725853

Epoch: 6| Step: 7
Training loss: 2.6023966050656795
Validation loss: 2.6966392624616318

Epoch: 6| Step: 8
Training loss: 2.8406437267865425
Validation loss: 2.693848436139387

Epoch: 6| Step: 9
Training loss: 3.1540578651224087
Validation loss: 2.6928838168597005

Epoch: 6| Step: 10
Training loss: 3.6425553471975496
Validation loss: 2.6971347150925156

Epoch: 6| Step: 11
Training loss: 2.795772910756177
Validation loss: 2.697807199247517

Epoch: 6| Step: 12
Training loss: 2.5349206565162477
Validation loss: 2.700555443496214

Epoch: 6| Step: 13
Training loss: 3.7985069001800342
Validation loss: 2.705286611220943

Epoch: 125| Step: 0
Training loss: 3.492926943875853
Validation loss: 2.706011500252115

Epoch: 6| Step: 1
Training loss: 2.586212604560253
Validation loss: 2.7147042990046284

Epoch: 6| Step: 2
Training loss: 2.803147597205897
Validation loss: 2.719831942337541

Epoch: 6| Step: 3
Training loss: 3.652079494116695
Validation loss: 2.721739194425446

Epoch: 6| Step: 4
Training loss: 3.1065768586085847
Validation loss: 2.712756376180554

Epoch: 6| Step: 5
Training loss: 3.005043717255166
Validation loss: 2.709717727237789

Epoch: 6| Step: 6
Training loss: 2.940902038561475
Validation loss: 2.719838202901379

Epoch: 6| Step: 7
Training loss: 3.156410439823063
Validation loss: 2.703178566932963

Epoch: 6| Step: 8
Training loss: 2.653094852487405
Validation loss: 2.68975966881666

Epoch: 6| Step: 9
Training loss: 3.2470839696611287
Validation loss: 2.6880420539765204

Epoch: 6| Step: 10
Training loss: 2.967761387099178
Validation loss: 2.689477578795925

Epoch: 6| Step: 11
Training loss: 2.708391384578552
Validation loss: 2.6882013053121723

Epoch: 6| Step: 12
Training loss: 3.368457103445354
Validation loss: 2.691477155273081

Epoch: 6| Step: 13
Training loss: 1.708557494118538
Validation loss: 2.691385181896473

Epoch: 126| Step: 0
Training loss: 3.091949247074469
Validation loss: 2.693725160375212

Epoch: 6| Step: 1
Training loss: 3.3256009698869597
Validation loss: 2.691483018876467

Epoch: 6| Step: 2
Training loss: 2.8482186556149216
Validation loss: 2.691208196837681

Epoch: 6| Step: 3
Training loss: 2.7383682302463646
Validation loss: 2.6917349855431416

Epoch: 6| Step: 4
Training loss: 3.6352810255977186
Validation loss: 2.691999276415663

Epoch: 6| Step: 5
Training loss: 3.442994875773069
Validation loss: 2.693079893375051

Epoch: 6| Step: 6
Training loss: 3.457806895682874
Validation loss: 2.6952975555238647

Epoch: 6| Step: 7
Training loss: 3.626965088791186
Validation loss: 2.6897033317173364

Epoch: 6| Step: 8
Training loss: 2.1535695318861223
Validation loss: 2.6907097702485414

Epoch: 6| Step: 9
Training loss: 3.282994269922318
Validation loss: 2.689731981740758

Epoch: 6| Step: 10
Training loss: 2.673868160909406
Validation loss: 2.6921235396803285

Epoch: 6| Step: 11
Training loss: 2.2491921988083132
Validation loss: 2.690726030255241

Epoch: 6| Step: 12
Training loss: 2.468787856958555
Validation loss: 2.693462525565613

Epoch: 6| Step: 13
Training loss: 2.8655019685881253
Validation loss: 2.698120151957347

Epoch: 127| Step: 0
Training loss: 2.6479109969379784
Validation loss: 2.6934831538721897

Epoch: 6| Step: 1
Training loss: 3.2597057961012723
Validation loss: 2.6907885759618364

Epoch: 6| Step: 2
Training loss: 2.207086775089706
Validation loss: 2.6957036948831736

Epoch: 6| Step: 3
Training loss: 3.314982797339267
Validation loss: 2.693277072155409

Epoch: 6| Step: 4
Training loss: 3.173271285286591
Validation loss: 2.691665899549084

Epoch: 6| Step: 5
Training loss: 3.109447229207794
Validation loss: 2.6927044158892453

Epoch: 6| Step: 6
Training loss: 2.822754032944665
Validation loss: 2.696262820200221

Epoch: 6| Step: 7
Training loss: 3.167622840143293
Validation loss: 2.6921718928890925

Epoch: 6| Step: 8
Training loss: 2.679064085931107
Validation loss: 2.69527557048136

Epoch: 6| Step: 9
Training loss: 2.597765101396524
Validation loss: 2.6905548369728582

Epoch: 6| Step: 10
Training loss: 3.2362537179795146
Validation loss: 2.6991956096716434

Epoch: 6| Step: 11
Training loss: 3.534995194449389
Validation loss: 2.6974597901506034

Epoch: 6| Step: 12
Training loss: 3.096617015636309
Validation loss: 2.7028499361132114

Epoch: 6| Step: 13
Training loss: 3.121710610094842
Validation loss: 2.6962181867861927

Epoch: 128| Step: 0
Training loss: 3.4685784374802777
Validation loss: 2.699203002745086

Epoch: 6| Step: 1
Training loss: 3.2542705354871235
Validation loss: 2.700246710237022

Epoch: 6| Step: 2
Training loss: 3.344119327861895
Validation loss: 2.700101397261364

Epoch: 6| Step: 3
Training loss: 3.17575977876544
Validation loss: 2.68841008189663

Epoch: 6| Step: 4
Training loss: 2.4799083166710494
Validation loss: 2.692942145614781

Epoch: 6| Step: 5
Training loss: 2.5495298625411342
Validation loss: 2.6877061259727957

Epoch: 6| Step: 6
Training loss: 2.4859178658602588
Validation loss: 2.6859369180037014

Epoch: 6| Step: 7
Training loss: 3.5264076181997543
Validation loss: 2.685187737548518

Epoch: 6| Step: 8
Training loss: 3.0227897009201237
Validation loss: 2.6925141191165194

Epoch: 6| Step: 9
Training loss: 3.4619104813098542
Validation loss: 2.6851658444487376

Epoch: 6| Step: 10
Training loss: 2.9793719952301885
Validation loss: 2.689703164919341

Epoch: 6| Step: 11
Training loss: 2.3521762186456208
Validation loss: 2.686025239195439

Epoch: 6| Step: 12
Training loss: 2.953076074588518
Validation loss: 2.692994177758424

Epoch: 6| Step: 13
Training loss: 2.930083306596205
Validation loss: 2.69456433551468

Epoch: 129| Step: 0
Training loss: 3.004924229636164
Validation loss: 2.6913743820243328

Epoch: 6| Step: 1
Training loss: 3.133514706117102
Validation loss: 2.6932085768637215

Epoch: 6| Step: 2
Training loss: 2.8510266780401206
Validation loss: 2.69827296535289

Epoch: 6| Step: 3
Training loss: 3.750048319187399
Validation loss: 2.6952544451158214

Epoch: 6| Step: 4
Training loss: 3.347206112166417
Validation loss: 2.701327020251756

Epoch: 6| Step: 5
Training loss: 2.6456751638478444
Validation loss: 2.6966215407801375

Epoch: 6| Step: 6
Training loss: 2.9727805634219417
Validation loss: 2.6983345257059868

Epoch: 6| Step: 7
Training loss: 2.896566028434373
Validation loss: 2.688610579088264

Epoch: 6| Step: 8
Training loss: 2.6413246446813528
Validation loss: 2.691053227477725

Epoch: 6| Step: 9
Training loss: 2.847493484931707
Validation loss: 2.6875007650379525

Epoch: 6| Step: 10
Training loss: 2.8510808668599705
Validation loss: 2.6827295790333943

Epoch: 6| Step: 11
Training loss: 2.9834537382354225
Validation loss: 2.684278331910161

Epoch: 6| Step: 12
Training loss: 3.0595484932792045
Validation loss: 2.6850459366771657

Epoch: 6| Step: 13
Training loss: 3.0828938213900337
Validation loss: 2.693054440380727

Epoch: 130| Step: 0
Training loss: 3.3447760317965773
Validation loss: 2.6917918153658125

Epoch: 6| Step: 1
Training loss: 3.4067792568774298
Validation loss: 2.691694844888716

Epoch: 6| Step: 2
Training loss: 3.2649132349890766
Validation loss: 2.692163424450621

Epoch: 6| Step: 3
Training loss: 2.7783716287549987
Validation loss: 2.696141027864835

Epoch: 6| Step: 4
Training loss: 2.9902295910518557
Validation loss: 2.693330132487509

Epoch: 6| Step: 5
Training loss: 2.972866857940581
Validation loss: 2.690356226577871

Epoch: 6| Step: 6
Training loss: 3.0621750328323265
Validation loss: 2.6896686336989455

Epoch: 6| Step: 7
Training loss: 3.065403865572989
Validation loss: 2.688971322525277

Epoch: 6| Step: 8
Training loss: 3.1611441257617297
Validation loss: 2.68356093615455

Epoch: 6| Step: 9
Training loss: 2.7576162468347785
Validation loss: 2.6804187477343366

Epoch: 6| Step: 10
Training loss: 2.721933101905478
Validation loss: 2.6849801569379754

Epoch: 6| Step: 11
Training loss: 3.264657639317349
Validation loss: 2.6822571225465244

Epoch: 6| Step: 12
Training loss: 2.8755111239992037
Validation loss: 2.679521553437843

Epoch: 6| Step: 13
Training loss: 1.6241419801096717
Validation loss: 2.682997739151313

Epoch: 131| Step: 0
Training loss: 3.5201662022193725
Validation loss: 2.6867263407247934

Epoch: 6| Step: 1
Training loss: 3.5770261984727685
Validation loss: 2.6852081697379457

Epoch: 6| Step: 2
Training loss: 2.4298834522382307
Validation loss: 2.6783422296246644

Epoch: 6| Step: 3
Training loss: 3.0244396037426524
Validation loss: 2.685008653987448

Epoch: 6| Step: 4
Training loss: 3.0495890731399613
Validation loss: 2.6805975765461407

Epoch: 6| Step: 5
Training loss: 3.1971096217476935
Validation loss: 2.679058088928673

Epoch: 6| Step: 6
Training loss: 2.8724533079915844
Validation loss: 2.6835713614650154

Epoch: 6| Step: 7
Training loss: 2.6408723794304176
Validation loss: 2.682839212862678

Epoch: 6| Step: 8
Training loss: 3.3635748581101685
Validation loss: 2.6857746191922036

Epoch: 6| Step: 9
Training loss: 3.1927693513118465
Validation loss: 2.6811261163239775

Epoch: 6| Step: 10
Training loss: 2.4530346118772703
Validation loss: 2.6883745747431327

Epoch: 6| Step: 11
Training loss: 3.0698482560420657
Validation loss: 2.6869675399052024

Epoch: 6| Step: 12
Training loss: 2.5768048229742053
Validation loss: 2.688539828972148

Epoch: 6| Step: 13
Training loss: 2.727731099045496
Validation loss: 2.6949388794065037

Epoch: 132| Step: 0
Training loss: 2.974650091542576
Validation loss: 2.710272833973493

Epoch: 6| Step: 1
Training loss: 3.1472845877333815
Validation loss: 2.7129961418502186

Epoch: 6| Step: 2
Training loss: 3.163946305013456
Validation loss: 2.7276639943263694

Epoch: 6| Step: 3
Training loss: 2.713486586077002
Validation loss: 2.752105879827316

Epoch: 6| Step: 4
Training loss: 2.952073005724609
Validation loss: 2.7712392744805663

Epoch: 6| Step: 5
Training loss: 2.8210836185853365
Validation loss: 2.785882517757531

Epoch: 6| Step: 6
Training loss: 2.905490570930092
Validation loss: 2.763861147770416

Epoch: 6| Step: 7
Training loss: 2.706733015622133
Validation loss: 2.730407579895675

Epoch: 6| Step: 8
Training loss: 3.176839616943579
Validation loss: 2.6981649705412547

Epoch: 6| Step: 9
Training loss: 3.0343021089420343
Validation loss: 2.6890199507389023

Epoch: 6| Step: 10
Training loss: 3.1079543045396347
Validation loss: 2.679064139518431

Epoch: 6| Step: 11
Training loss: 3.528606539386652
Validation loss: 2.6797640669442324

Epoch: 6| Step: 12
Training loss: 2.867723295369292
Validation loss: 2.6803112568692593

Epoch: 6| Step: 13
Training loss: 2.9753465146128852
Validation loss: 2.68928651794603

Epoch: 133| Step: 0
Training loss: 3.499255509895663
Validation loss: 2.6907648553162207

Epoch: 6| Step: 1
Training loss: 2.102488260678643
Validation loss: 2.6897253079776764

Epoch: 6| Step: 2
Training loss: 3.3997348457614125
Validation loss: 2.69027686598378

Epoch: 6| Step: 3
Training loss: 3.5776692062658464
Validation loss: 2.6926900415117987

Epoch: 6| Step: 4
Training loss: 2.6461028214629434
Validation loss: 2.6902340296152336

Epoch: 6| Step: 5
Training loss: 2.929442372557573
Validation loss: 2.6887187774528516

Epoch: 6| Step: 6
Training loss: 3.3334193854350724
Validation loss: 2.684300315337815

Epoch: 6| Step: 7
Training loss: 2.8994392247455654
Validation loss: 2.6809272090493073

Epoch: 6| Step: 8
Training loss: 3.2207845535302093
Validation loss: 2.6866237092218133

Epoch: 6| Step: 9
Training loss: 2.617880020755357
Validation loss: 2.679089307260648

Epoch: 6| Step: 10
Training loss: 2.6640028065904375
Validation loss: 2.6800778294629235

Epoch: 6| Step: 11
Training loss: 3.020181484706097
Validation loss: 2.6840539756649293

Epoch: 6| Step: 12
Training loss: 3.1387921958891463
Validation loss: 2.6824643939744663

Epoch: 6| Step: 13
Training loss: 3.0203592564199706
Validation loss: 2.6771119549811195

Epoch: 134| Step: 0
Training loss: 2.8842349794401247
Validation loss: 2.676711927694382

Epoch: 6| Step: 1
Training loss: 2.898163075578475
Validation loss: 2.677983043072704

Epoch: 6| Step: 2
Training loss: 3.0684414275311584
Validation loss: 2.6855439319463956

Epoch: 6| Step: 3
Training loss: 3.207389263284154
Validation loss: 2.684698769812532

Epoch: 6| Step: 4
Training loss: 2.832042025841568
Validation loss: 2.683423598754719

Epoch: 6| Step: 5
Training loss: 2.499880501752188
Validation loss: 2.695696772473941

Epoch: 6| Step: 6
Training loss: 3.195909365098121
Validation loss: 2.692082916295336

Epoch: 6| Step: 7
Training loss: 3.4525927655617843
Validation loss: 2.691892632728587

Epoch: 6| Step: 8
Training loss: 2.9749257951986765
Validation loss: 2.686833220001094

Epoch: 6| Step: 9
Training loss: 3.146543477389521
Validation loss: 2.7049639571614414

Epoch: 6| Step: 10
Training loss: 2.914043845763852
Validation loss: 2.7198578507230664

Epoch: 6| Step: 11
Training loss: 3.2869361655011042
Validation loss: 2.714534289266806

Epoch: 6| Step: 12
Training loss: 2.551638869423413
Validation loss: 2.6996062991719634

Epoch: 6| Step: 13
Training loss: 3.062072879823421
Validation loss: 2.7232999897611556

Epoch: 135| Step: 0
Training loss: 2.9325809279622366
Validation loss: 2.7153955354034043

Epoch: 6| Step: 1
Training loss: 3.3636674293751834
Validation loss: 2.6957214834628167

Epoch: 6| Step: 2
Training loss: 2.5064330302921234
Validation loss: 2.6755986700423815

Epoch: 6| Step: 3
Training loss: 3.2706606064076915
Validation loss: 2.671666469470974

Epoch: 6| Step: 4
Training loss: 3.4670103367380993
Validation loss: 2.6707739862793987

Epoch: 6| Step: 5
Training loss: 3.3336564066407055
Validation loss: 2.675773602510582

Epoch: 6| Step: 6
Training loss: 3.049696647692084
Validation loss: 2.6751013504960373

Epoch: 6| Step: 7
Training loss: 3.6007851115409024
Validation loss: 2.673323252636689

Epoch: 6| Step: 8
Training loss: 2.5723683214360906
Validation loss: 2.67397821078977

Epoch: 6| Step: 9
Training loss: 3.3627875449739366
Validation loss: 2.6732270895670114

Epoch: 6| Step: 10
Training loss: 3.040973915397167
Validation loss: 2.6712458086248434

Epoch: 6| Step: 11
Training loss: 2.390301651373338
Validation loss: 2.6728953258121657

Epoch: 6| Step: 12
Training loss: 2.108351621686702
Validation loss: 2.6775535271768818

Epoch: 6| Step: 13
Training loss: 2.485135519834063
Validation loss: 2.6766693559975163

Epoch: 136| Step: 0
Training loss: 2.7749286779912437
Validation loss: 2.677192961220276

Epoch: 6| Step: 1
Training loss: 2.660127211088749
Validation loss: 2.67400164604322

Epoch: 6| Step: 2
Training loss: 3.1723987165776246
Validation loss: 2.6836381166921766

Epoch: 6| Step: 3
Training loss: 2.894649858399868
Validation loss: 2.683327235403019

Epoch: 6| Step: 4
Training loss: 3.04319956226085
Validation loss: 2.67943660102392

Epoch: 6| Step: 5
Training loss: 3.2431983199316115
Validation loss: 2.6832252038371354

Epoch: 6| Step: 6
Training loss: 2.9136926656651987
Validation loss: 2.681279414456112

Epoch: 6| Step: 7
Training loss: 2.622824539453278
Validation loss: 2.6788618588756243

Epoch: 6| Step: 8
Training loss: 3.28405307749185
Validation loss: 2.6762753127440395

Epoch: 6| Step: 9
Training loss: 3.0625277148179935
Validation loss: 2.680050992234169

Epoch: 6| Step: 10
Training loss: 2.4557890792016073
Validation loss: 2.6830818430893624

Epoch: 6| Step: 11
Training loss: 3.7615088288389797
Validation loss: 2.685007279078114

Epoch: 6| Step: 12
Training loss: 3.2312807771556202
Validation loss: 2.679551691948724

Epoch: 6| Step: 13
Training loss: 2.436850632464749
Validation loss: 2.6778463319583956

Epoch: 137| Step: 0
Training loss: 2.9625917509011135
Validation loss: 2.6763802309688396

Epoch: 6| Step: 1
Training loss: 2.9527778219576097
Validation loss: 2.673444952451886

Epoch: 6| Step: 2
Training loss: 2.569777319942445
Validation loss: 2.680258624609493

Epoch: 6| Step: 3
Training loss: 3.038373619003632
Validation loss: 2.6763934726151737

Epoch: 6| Step: 4
Training loss: 2.7009327548884743
Validation loss: 2.6809997655617033

Epoch: 6| Step: 5
Training loss: 2.944182566227232
Validation loss: 2.6819345760824707

Epoch: 6| Step: 6
Training loss: 2.8372275200643235
Validation loss: 2.6813464275253795

Epoch: 6| Step: 7
Training loss: 3.405167293836124
Validation loss: 2.681697817872049

Epoch: 6| Step: 8
Training loss: 3.738790160549803
Validation loss: 2.683610221254707

Epoch: 6| Step: 9
Training loss: 2.8456186715333622
Validation loss: 2.679802048250302

Epoch: 6| Step: 10
Training loss: 2.686335644108915
Validation loss: 2.6748710682522256

Epoch: 6| Step: 11
Training loss: 2.5187173164462138
Validation loss: 2.6736284839705116

Epoch: 6| Step: 12
Training loss: 3.2451082241544134
Validation loss: 2.6749938520089267

Epoch: 6| Step: 13
Training loss: 3.535778686355281
Validation loss: 2.66914411762758

Epoch: 138| Step: 0
Training loss: 3.14370834292651
Validation loss: 2.6684334930203497

Epoch: 6| Step: 1
Training loss: 2.621346337872467
Validation loss: 2.6663398500749587

Epoch: 6| Step: 2
Training loss: 2.8308715971646947
Validation loss: 2.6697844985553862

Epoch: 6| Step: 3
Training loss: 3.230688380532758
Validation loss: 2.6652074142381124

Epoch: 6| Step: 4
Training loss: 3.3325989073078905
Validation loss: 2.668192343063537

Epoch: 6| Step: 5
Training loss: 2.809190625684834
Validation loss: 2.6665770431680755

Epoch: 6| Step: 6
Training loss: 3.456882831383226
Validation loss: 2.666246058635926

Epoch: 6| Step: 7
Training loss: 2.740960523190942
Validation loss: 2.668754322532363

Epoch: 6| Step: 8
Training loss: 3.08983526216636
Validation loss: 2.6641389242037494

Epoch: 6| Step: 9
Training loss: 2.994537625802386
Validation loss: 2.6679147374862486

Epoch: 6| Step: 10
Training loss: 2.9378363234161964
Validation loss: 2.6634144377144766

Epoch: 6| Step: 11
Training loss: 2.906218497813138
Validation loss: 2.664673839953887

Epoch: 6| Step: 12
Training loss: 2.9018859913026485
Validation loss: 2.6643748102880362

Epoch: 6| Step: 13
Training loss: 2.815017421255189
Validation loss: 2.6648122543010224

Epoch: 139| Step: 0
Training loss: 2.7580542093748965
Validation loss: 2.6680455744549683

Epoch: 6| Step: 1
Training loss: 3.3593246101326195
Validation loss: 2.6684193510564405

Epoch: 6| Step: 2
Training loss: 2.9674409891638827
Validation loss: 2.6643703870868642

Epoch: 6| Step: 3
Training loss: 2.8979725426214937
Validation loss: 2.6728082953142294

Epoch: 6| Step: 4
Training loss: 3.104087436014779
Validation loss: 2.6742105912407013

Epoch: 6| Step: 5
Training loss: 3.5004223160042933
Validation loss: 2.6926221444384377

Epoch: 6| Step: 6
Training loss: 2.6771379358880627
Validation loss: 2.674449827313644

Epoch: 6| Step: 7
Training loss: 3.1820533442602392
Validation loss: 2.667629130423554

Epoch: 6| Step: 8
Training loss: 2.6048625372350505
Validation loss: 2.6620894194110747

Epoch: 6| Step: 9
Training loss: 3.022803582657764
Validation loss: 2.6656927472854255

Epoch: 6| Step: 10
Training loss: 2.1048962552562647
Validation loss: 2.6630275064142594

Epoch: 6| Step: 11
Training loss: 2.6284174698558647
Validation loss: 2.6648635658273476

Epoch: 6| Step: 12
Training loss: 3.8305389889574095
Validation loss: 2.666779506767283

Epoch: 6| Step: 13
Training loss: 3.053286023613279
Validation loss: 2.66692538570193

Epoch: 140| Step: 0
Training loss: 2.8918614397953264
Validation loss: 2.6679843577888995

Epoch: 6| Step: 1
Training loss: 3.0210481413838135
Validation loss: 2.6659701457399008

Epoch: 6| Step: 2
Training loss: 2.881256427881845
Validation loss: 2.6693876219420662

Epoch: 6| Step: 3
Training loss: 2.9399942400609897
Validation loss: 2.6623428804836333

Epoch: 6| Step: 4
Training loss: 2.8429307700794184
Validation loss: 2.6695579878975684

Epoch: 6| Step: 5
Training loss: 3.095495174896517
Validation loss: 2.670311400777873

Epoch: 6| Step: 6
Training loss: 3.4375577054815567
Validation loss: 2.676657325388536

Epoch: 6| Step: 7
Training loss: 3.0562694775169383
Validation loss: 2.672919846688542

Epoch: 6| Step: 8
Training loss: 3.0000155766400627
Validation loss: 2.6730777451556227

Epoch: 6| Step: 9
Training loss: 2.9956092492462902
Validation loss: 2.6730309522325135

Epoch: 6| Step: 10
Training loss: 3.0793973355405564
Validation loss: 2.6714377835617364

Epoch: 6| Step: 11
Training loss: 3.015765409215042
Validation loss: 2.6759784819510375

Epoch: 6| Step: 12
Training loss: 2.9033428130960433
Validation loss: 2.6775821272028626

Epoch: 6| Step: 13
Training loss: 2.6695674278659096
Validation loss: 2.687517876266318

Epoch: 141| Step: 0
Training loss: 2.7270394593962606
Validation loss: 2.706359214485886

Epoch: 6| Step: 1
Training loss: 2.735716136645751
Validation loss: 2.7174966459895753

Epoch: 6| Step: 2
Training loss: 2.297178702484833
Validation loss: 2.729420565026617

Epoch: 6| Step: 3
Training loss: 2.879885627276939
Validation loss: 2.726387506062956

Epoch: 6| Step: 4
Training loss: 3.3329146758064563
Validation loss: 2.725323759867479

Epoch: 6| Step: 5
Training loss: 3.1361708299079045
Validation loss: 2.727857725934767

Epoch: 6| Step: 6
Training loss: 3.4807587747866693
Validation loss: 2.7005868091763885

Epoch: 6| Step: 7
Training loss: 3.3152219996620826
Validation loss: 2.681007391450095

Epoch: 6| Step: 8
Training loss: 2.860520097085507
Validation loss: 2.6703932723678427

Epoch: 6| Step: 9
Training loss: 2.985771132245469
Validation loss: 2.6680330619975514

Epoch: 6| Step: 10
Training loss: 3.381205469991117
Validation loss: 2.6723447150648787

Epoch: 6| Step: 11
Training loss: 3.1097166554208684
Validation loss: 2.660584408971966

Epoch: 6| Step: 12
Training loss: 2.670809706307188
Validation loss: 2.6678023704654166

Epoch: 6| Step: 13
Training loss: 2.8982045370494847
Validation loss: 2.665892039226224

Epoch: 142| Step: 0
Training loss: 3.1544332279932896
Validation loss: 2.6634736543889965

Epoch: 6| Step: 1
Training loss: 3.274431391851325
Validation loss: 2.6677429913737716

Epoch: 6| Step: 2
Training loss: 3.104826240530494
Validation loss: 2.659631903892408

Epoch: 6| Step: 3
Training loss: 2.8560484833862896
Validation loss: 2.6624560864620994

Epoch: 6| Step: 4
Training loss: 2.872451647956608
Validation loss: 2.6643653644305565

Epoch: 6| Step: 5
Training loss: 3.0321400702237593
Validation loss: 2.6584010834581946

Epoch: 6| Step: 6
Training loss: 2.4420686601363837
Validation loss: 2.666806663086504

Epoch: 6| Step: 7
Training loss: 2.5448413054446952
Validation loss: 2.6606178588504643

Epoch: 6| Step: 8
Training loss: 2.6374422112231204
Validation loss: 2.6600980138072767

Epoch: 6| Step: 9
Training loss: 2.9649955700588744
Validation loss: 2.661123574782662

Epoch: 6| Step: 10
Training loss: 3.1062254937354643
Validation loss: 2.6613711979057046

Epoch: 6| Step: 11
Training loss: 3.042822700883782
Validation loss: 2.6635098515475844

Epoch: 6| Step: 12
Training loss: 3.1561102505706806
Validation loss: 2.663673629192201

Epoch: 6| Step: 13
Training loss: 3.940811339169295
Validation loss: 2.6597450315201283

Epoch: 143| Step: 0
Training loss: 2.26939951108882
Validation loss: 2.662102747549986

Epoch: 6| Step: 1
Training loss: 3.0893524975413444
Validation loss: 2.665461518496394

Epoch: 6| Step: 2
Training loss: 3.107092244655262
Validation loss: 2.6600274324694952

Epoch: 6| Step: 3
Training loss: 3.5625186049661526
Validation loss: 2.66374849484884

Epoch: 6| Step: 4
Training loss: 3.0544786308666327
Validation loss: 2.674031591749123

Epoch: 6| Step: 5
Training loss: 2.7705911133714185
Validation loss: 2.6709337210607575

Epoch: 6| Step: 6
Training loss: 3.2471275840813902
Validation loss: 2.674052928878618

Epoch: 6| Step: 7
Training loss: 3.44348982089485
Validation loss: 2.660892027940273

Epoch: 6| Step: 8
Training loss: 2.7947014402502317
Validation loss: 2.660479858407547

Epoch: 6| Step: 9
Training loss: 3.3134764185915824
Validation loss: 2.6611958100063955

Epoch: 6| Step: 10
Training loss: 2.639333375227233
Validation loss: 2.663188252588882

Epoch: 6| Step: 11
Training loss: 3.0303631301179763
Validation loss: 2.6713557832378823

Epoch: 6| Step: 12
Training loss: 2.5159306310906517
Validation loss: 2.665039462672155

Epoch: 6| Step: 13
Training loss: 2.6351940578685533
Validation loss: 2.6675784016248065

Epoch: 144| Step: 0
Training loss: 3.6654363793432814
Validation loss: 2.658142037545033

Epoch: 6| Step: 1
Training loss: 3.3377877672127503
Validation loss: 2.6631000134461713

Epoch: 6| Step: 2
Training loss: 3.303178735907662
Validation loss: 2.6685273468778075

Epoch: 6| Step: 3
Training loss: 2.7631944223728664
Validation loss: 2.665155772663243

Epoch: 6| Step: 4
Training loss: 2.96396291160288
Validation loss: 2.6586857321204707

Epoch: 6| Step: 5
Training loss: 3.025526956105594
Validation loss: 2.6607200096613957

Epoch: 6| Step: 6
Training loss: 3.3404998670217405
Validation loss: 2.6581008032015263

Epoch: 6| Step: 7
Training loss: 2.782961372450095
Validation loss: 2.6618766484019027

Epoch: 6| Step: 8
Training loss: 2.574033507655865
Validation loss: 2.666802999513397

Epoch: 6| Step: 9
Training loss: 2.646507617806145
Validation loss: 2.6597803762905343

Epoch: 6| Step: 10
Training loss: 2.4070420385983775
Validation loss: 2.6589894953260766

Epoch: 6| Step: 11
Training loss: 2.9035641965238104
Validation loss: 2.662371898116976

Epoch: 6| Step: 12
Training loss: 3.0586080928383375
Validation loss: 2.653114228324746

Epoch: 6| Step: 13
Training loss: 2.726280656910686
Validation loss: 2.6645723958634298

Epoch: 145| Step: 0
Training loss: 3.0081425473343195
Validation loss: 2.6604823464237612

Epoch: 6| Step: 1
Training loss: 3.080326125974066
Validation loss: 2.662865350715936

Epoch: 6| Step: 2
Training loss: 3.2571641748012774
Validation loss: 2.6705716623198823

Epoch: 6| Step: 3
Training loss: 3.460312487652962
Validation loss: 2.6756201767231405

Epoch: 6| Step: 4
Training loss: 3.350921071524647
Validation loss: 2.681591426070411

Epoch: 6| Step: 5
Training loss: 3.474234978423701
Validation loss: 2.697539881235102

Epoch: 6| Step: 6
Training loss: 2.7527879108080358
Validation loss: 2.6907577306022143

Epoch: 6| Step: 7
Training loss: 2.1521408068456473
Validation loss: 2.6732270540837915

Epoch: 6| Step: 8
Training loss: 3.166481280337919
Validation loss: 2.6563012636318177

Epoch: 6| Step: 9
Training loss: 3.05978927534984
Validation loss: 2.654438010247536

Epoch: 6| Step: 10
Training loss: 2.694535649341101
Validation loss: 2.6517859997364175

Epoch: 6| Step: 11
Training loss: 2.8601555097756655
Validation loss: 2.655546181248425

Epoch: 6| Step: 12
Training loss: 2.346411859196095
Validation loss: 2.656384915732416

Epoch: 6| Step: 13
Training loss: 3.0271898587397743
Validation loss: 2.655853002463613

Epoch: 146| Step: 0
Training loss: 2.7589860963930812
Validation loss: 2.662603104442918

Epoch: 6| Step: 1
Training loss: 3.3690293799568884
Validation loss: 2.6619444022503633

Epoch: 6| Step: 2
Training loss: 3.210630378978546
Validation loss: 2.658458691433867

Epoch: 6| Step: 3
Training loss: 3.133214149508017
Validation loss: 2.656981061770836

Epoch: 6| Step: 4
Training loss: 3.0616167312838725
Validation loss: 2.6570583418890674

Epoch: 6| Step: 5
Training loss: 2.9249949398160022
Validation loss: 2.650569331591898

Epoch: 6| Step: 6
Training loss: 2.8566571435877184
Validation loss: 2.65612475819575

Epoch: 6| Step: 7
Training loss: 2.9667084751683546
Validation loss: 2.6515905639339756

Epoch: 6| Step: 8
Training loss: 3.222413875828769
Validation loss: 2.6549005872593927

Epoch: 6| Step: 9
Training loss: 2.9173859572165384
Validation loss: 2.6540363978397146

Epoch: 6| Step: 10
Training loss: 3.656183486724734
Validation loss: 2.6521069098356342

Epoch: 6| Step: 11
Training loss: 2.8512131568438157
Validation loss: 2.652594858244244

Epoch: 6| Step: 12
Training loss: 2.0195078746984967
Validation loss: 2.6597780167742693

Epoch: 6| Step: 13
Training loss: 2.443931896728042
Validation loss: 2.670607662478374

Epoch: 147| Step: 0
Training loss: 2.7375017000654682
Validation loss: 2.6747160609730907

Epoch: 6| Step: 1
Training loss: 2.4082936117555027
Validation loss: 2.6751585538005167

Epoch: 6| Step: 2
Training loss: 3.405609350590306
Validation loss: 2.6813512147237333

Epoch: 6| Step: 3
Training loss: 2.71413854866896
Validation loss: 2.692561957831609

Epoch: 6| Step: 4
Training loss: 3.0297939303483954
Validation loss: 2.699701235362804

Epoch: 6| Step: 5
Training loss: 2.840599578642953
Validation loss: 2.6986558321062457

Epoch: 6| Step: 6
Training loss: 3.3774780783234797
Validation loss: 2.6864068778934533

Epoch: 6| Step: 7
Training loss: 2.2484789051222576
Validation loss: 2.679786116104087

Epoch: 6| Step: 8
Training loss: 2.93008737505414
Validation loss: 2.6704658534877934

Epoch: 6| Step: 9
Training loss: 3.511988000445189
Validation loss: 2.6644022393596285

Epoch: 6| Step: 10
Training loss: 3.4390212420944217
Validation loss: 2.6484296428759766

Epoch: 6| Step: 11
Training loss: 2.9005278994765917
Validation loss: 2.6546740649463625

Epoch: 6| Step: 12
Training loss: 3.2721633798619036
Validation loss: 2.649632809407012

Epoch: 6| Step: 13
Training loss: 2.5159284515275004
Validation loss: 2.6521620417473004

Epoch: 148| Step: 0
Training loss: 3.15289642140623
Validation loss: 2.65212919376358

Epoch: 6| Step: 1
Training loss: 2.9092832753670734
Validation loss: 2.653464759476605

Epoch: 6| Step: 2
Training loss: 2.3296970461899904
Validation loss: 2.652869015758442

Epoch: 6| Step: 3
Training loss: 3.2432340472537766
Validation loss: 2.6478759139243935

Epoch: 6| Step: 4
Training loss: 2.9764218434819427
Validation loss: 2.6560672038407165

Epoch: 6| Step: 5
Training loss: 3.145758689011633
Validation loss: 2.653207882007952

Epoch: 6| Step: 6
Training loss: 3.2827980386160385
Validation loss: 2.6608265355495124

Epoch: 6| Step: 7
Training loss: 2.954180493521962
Validation loss: 2.6576577410298805

Epoch: 6| Step: 8
Training loss: 3.3434279812631758
Validation loss: 2.6607707025337692

Epoch: 6| Step: 9
Training loss: 2.8185270369286775
Validation loss: 2.6636519375077263

Epoch: 6| Step: 10
Training loss: 2.544303110335076
Validation loss: 2.6550325975841935

Epoch: 6| Step: 11
Training loss: 2.335225178482004
Validation loss: 2.653194242502177

Epoch: 6| Step: 12
Training loss: 3.4905951707840983
Validation loss: 2.66784201248784

Epoch: 6| Step: 13
Training loss: 3.1012005726920386
Validation loss: 2.656821222430522

Epoch: 149| Step: 0
Training loss: 2.6932108328441213
Validation loss: 2.660451052367986

Epoch: 6| Step: 1
Training loss: 2.804498777707888
Validation loss: 2.6469471923367953

Epoch: 6| Step: 2
Training loss: 2.6659817312692113
Validation loss: 2.6581949620844383

Epoch: 6| Step: 3
Training loss: 3.4097182788985774
Validation loss: 2.652610410556743

Epoch: 6| Step: 4
Training loss: 3.0010412316596207
Validation loss: 2.650039260598194

Epoch: 6| Step: 5
Training loss: 2.567472332263956
Validation loss: 2.649384998740745

Epoch: 6| Step: 6
Training loss: 2.763937052483347
Validation loss: 2.652461370577105

Epoch: 6| Step: 7
Training loss: 2.8750529077056886
Validation loss: 2.6533847061140543

Epoch: 6| Step: 8
Training loss: 3.318347591852976
Validation loss: 2.6462198203998177

Epoch: 6| Step: 9
Training loss: 2.553414123674654
Validation loss: 2.6466230170499037

Epoch: 6| Step: 10
Training loss: 3.383650255314956
Validation loss: 2.6510780222866424

Epoch: 6| Step: 11
Training loss: 2.7110372299626673
Validation loss: 2.652029812167575

Epoch: 6| Step: 12
Training loss: 3.1726434086941553
Validation loss: 2.6494837490051926

Epoch: 6| Step: 13
Training loss: 3.983366595545692
Validation loss: 2.650270006715399

Epoch: 150| Step: 0
Training loss: 2.796486172207264
Validation loss: 2.6485759681849093

Epoch: 6| Step: 1
Training loss: 3.4148079693311053
Validation loss: 2.646424344185712

Epoch: 6| Step: 2
Training loss: 2.947747557050165
Validation loss: 2.6492913966772083

Epoch: 6| Step: 3
Training loss: 3.060154386378118
Validation loss: 2.653609111158692

Epoch: 6| Step: 4
Training loss: 2.7603480108588205
Validation loss: 2.65156238452045

Epoch: 6| Step: 5
Training loss: 3.1143613159932446
Validation loss: 2.656298419431462

Epoch: 6| Step: 6
Training loss: 2.7697723671101007
Validation loss: 2.6557840391001566

Epoch: 6| Step: 7
Training loss: 3.4407247502889526
Validation loss: 2.6625210448211414

Epoch: 6| Step: 8
Training loss: 2.782715572016377
Validation loss: 2.6637985324377107

Epoch: 6| Step: 9
Training loss: 3.2126313357094256
Validation loss: 2.6593722220941913

Epoch: 6| Step: 10
Training loss: 3.1125226981319702
Validation loss: 2.6902430911383126

Epoch: 6| Step: 11
Training loss: 2.822895758482207
Validation loss: 2.668789133006553

Epoch: 6| Step: 12
Training loss: 2.753611447402371
Validation loss: 2.6577419639170508

Epoch: 6| Step: 13
Training loss: 2.371426152114816
Validation loss: 2.648479879862193

Epoch: 151| Step: 0
Training loss: 3.3778318604383757
Validation loss: 2.6468782158288597

Epoch: 6| Step: 1
Training loss: 2.6501819188276228
Validation loss: 2.649409326936518

Epoch: 6| Step: 2
Training loss: 2.8977095931494703
Validation loss: 2.645489287024704

Epoch: 6| Step: 3
Training loss: 3.2460508561771904
Validation loss: 2.644417131251646

Epoch: 6| Step: 4
Training loss: 3.334876148492062
Validation loss: 2.649256003899092

Epoch: 6| Step: 5
Training loss: 3.2541834975298647
Validation loss: 2.6505256648838507

Epoch: 6| Step: 6
Training loss: 3.0818415632969596
Validation loss: 2.6432489293877777

Epoch: 6| Step: 7
Training loss: 3.0857548526842273
Validation loss: 2.651040931077984

Epoch: 6| Step: 8
Training loss: 2.8020200663140096
Validation loss: 2.665775172806832

Epoch: 6| Step: 9
Training loss: 2.973533871763266
Validation loss: 2.6655088020839375

Epoch: 6| Step: 10
Training loss: 3.2407195295416775
Validation loss: 2.6560100024751936

Epoch: 6| Step: 11
Training loss: 2.6431109221871227
Validation loss: 2.6544163869947988

Epoch: 6| Step: 12
Training loss: 2.496132433954203
Validation loss: 2.6478740182123874

Epoch: 6| Step: 13
Training loss: 2.015751679453954
Validation loss: 2.644267523794309

Epoch: 152| Step: 0
Training loss: 2.8482707214914234
Validation loss: 2.6474126178932207

Epoch: 6| Step: 1
Training loss: 2.831491732014557
Validation loss: 2.646342169412414

Epoch: 6| Step: 2
Training loss: 3.4329590368096565
Validation loss: 2.649852919797077

Epoch: 6| Step: 3
Training loss: 2.6450504035690234
Validation loss: 2.6498888637899807

Epoch: 6| Step: 4
Training loss: 3.27426056995595
Validation loss: 2.651658766234147

Epoch: 6| Step: 5
Training loss: 2.8168780689548645
Validation loss: 2.655140567268271

Epoch: 6| Step: 6
Training loss: 3.4672119579853606
Validation loss: 2.6512021245606703

Epoch: 6| Step: 7
Training loss: 2.648517258604976
Validation loss: 2.645225188231117

Epoch: 6| Step: 8
Training loss: 2.7270818614640038
Validation loss: 2.649768523556346

Epoch: 6| Step: 9
Training loss: 3.279059123723975
Validation loss: 2.653742781155296

Epoch: 6| Step: 10
Training loss: 3.1334602276152914
Validation loss: 2.6517254721355026

Epoch: 6| Step: 11
Training loss: 1.9553819967085095
Validation loss: 2.6521325798834696

Epoch: 6| Step: 12
Training loss: 3.3758202015334073
Validation loss: 2.658817897769138

Epoch: 6| Step: 13
Training loss: 2.9411658281245945
Validation loss: 2.659138491420973

Epoch: 153| Step: 0
Training loss: 2.8743919061178933
Validation loss: 2.6687368594878262

Epoch: 6| Step: 1
Training loss: 3.048440540806883
Validation loss: 2.683641799315332

Epoch: 6| Step: 2
Training loss: 3.374297174812882
Validation loss: 2.6948110586302865

Epoch: 6| Step: 3
Training loss: 3.2248471482596925
Validation loss: 2.680586058968579

Epoch: 6| Step: 4
Training loss: 2.671247442063074
Validation loss: 2.6656139430725476

Epoch: 6| Step: 5
Training loss: 2.756430563342927
Validation loss: 2.6401229869541916

Epoch: 6| Step: 6
Training loss: 3.1500570140704647
Validation loss: 2.6394722501169103

Epoch: 6| Step: 7
Training loss: 2.950096878788607
Validation loss: 2.6414147982428835

Epoch: 6| Step: 8
Training loss: 2.629913273240786
Validation loss: 2.6447128010857215

Epoch: 6| Step: 9
Training loss: 3.3707483143373964
Validation loss: 2.645042155478252

Epoch: 6| Step: 10
Training loss: 3.4625122070097323
Validation loss: 2.645667124100954

Epoch: 6| Step: 11
Training loss: 2.934464042376975
Validation loss: 2.6456977655123355

Epoch: 6| Step: 12
Training loss: 2.714014599102338
Validation loss: 2.6421994896631116

Epoch: 6| Step: 13
Training loss: 2.4613396210092486
Validation loss: 2.6404337014271895

Epoch: 154| Step: 0
Training loss: 2.4122769069793883
Validation loss: 2.642199614827633

Epoch: 6| Step: 1
Training loss: 3.6745611227165726
Validation loss: 2.645333741369416

Epoch: 6| Step: 2
Training loss: 3.073565053692898
Validation loss: 2.64553000956393

Epoch: 6| Step: 3
Training loss: 3.329745618938603
Validation loss: 2.643928597668695

Epoch: 6| Step: 4
Training loss: 1.9376639019950612
Validation loss: 2.645432670964956

Epoch: 6| Step: 5
Training loss: 1.9394443968915513
Validation loss: 2.643484576061766

Epoch: 6| Step: 6
Training loss: 3.396506135595545
Validation loss: 2.64858429625615

Epoch: 6| Step: 7
Training loss: 2.9859713933229775
Validation loss: 2.663836722134579

Epoch: 6| Step: 8
Training loss: 3.135826278409609
Validation loss: 2.6630775257822497

Epoch: 6| Step: 9
Training loss: 2.997966713097695
Validation loss: 2.6563136373638496

Epoch: 6| Step: 10
Training loss: 3.064444566839946
Validation loss: 2.6614437934075843

Epoch: 6| Step: 11
Training loss: 2.991039085620003
Validation loss: 2.661988884187796

Epoch: 6| Step: 12
Training loss: 3.117993855214013
Validation loss: 2.6667641707124825

Epoch: 6| Step: 13
Training loss: 3.126183552727939
Validation loss: 2.6711793809706266

Epoch: 155| Step: 0
Training loss: 2.849075361471549
Validation loss: 2.664080581649386

Epoch: 6| Step: 1
Training loss: 3.4122851740809894
Validation loss: 2.668564413112037

Epoch: 6| Step: 2
Training loss: 2.566853802121828
Validation loss: 2.666757282773505

Epoch: 6| Step: 3
Training loss: 3.014877780189436
Validation loss: 2.6546530201488454

Epoch: 6| Step: 4
Training loss: 3.085432798423726
Validation loss: 2.659211227891959

Epoch: 6| Step: 5
Training loss: 2.3954008181937874
Validation loss: 2.663758789818856

Epoch: 6| Step: 6
Training loss: 3.0125156639609947
Validation loss: 2.6521426502538716

Epoch: 6| Step: 7
Training loss: 2.5239315907416455
Validation loss: 2.6472473248390243

Epoch: 6| Step: 8
Training loss: 3.434667495337479
Validation loss: 2.645384053622834

Epoch: 6| Step: 9
Training loss: 3.238846788156377
Validation loss: 2.648182740939967

Epoch: 6| Step: 10
Training loss: 2.961546500592368
Validation loss: 2.6406725739457215

Epoch: 6| Step: 11
Training loss: 3.0793010188530565
Validation loss: 2.637747666636196

Epoch: 6| Step: 12
Training loss: 3.0454506699370487
Validation loss: 2.6378245480754856

Epoch: 6| Step: 13
Training loss: 2.890237978208492
Validation loss: 2.6378758704493657

Epoch: 156| Step: 0
Training loss: 2.6620066476705935
Validation loss: 2.637759629789997

Epoch: 6| Step: 1
Training loss: 2.554977913777843
Validation loss: 2.6330776033052854

Epoch: 6| Step: 2
Training loss: 3.0735697079311084
Validation loss: 2.6348001859177836

Epoch: 6| Step: 3
Training loss: 3.3009394319993253
Validation loss: 2.643675418227362

Epoch: 6| Step: 4
Training loss: 3.0315907080391535
Validation loss: 2.645160597920671

Epoch: 6| Step: 5
Training loss: 2.6903546276110792
Validation loss: 2.6626487865988353

Epoch: 6| Step: 6
Training loss: 2.8761252813309666
Validation loss: 2.6715016281979325

Epoch: 6| Step: 7
Training loss: 2.6601388625418356
Validation loss: 2.672313891852518

Epoch: 6| Step: 8
Training loss: 2.8967214269989383
Validation loss: 2.669207756843901

Epoch: 6| Step: 9
Training loss: 3.37889806492867
Validation loss: 2.6760227536004018

Epoch: 6| Step: 10
Training loss: 3.436810233530334
Validation loss: 2.686799795935403

Epoch: 6| Step: 11
Training loss: 3.2436223175850394
Validation loss: 2.666758526737562

Epoch: 6| Step: 12
Training loss: 2.752758636519264
Validation loss: 2.657919500103252

Epoch: 6| Step: 13
Training loss: 2.95857454489616
Validation loss: 2.645121457237439

Epoch: 157| Step: 0
Training loss: 2.586797934114862
Validation loss: 2.6454666055689855

Epoch: 6| Step: 1
Training loss: 3.2261435026544056
Validation loss: 2.6389603857887574

Epoch: 6| Step: 2
Training loss: 2.982450813597734
Validation loss: 2.63846743139237

Epoch: 6| Step: 3
Training loss: 2.5114580793366104
Validation loss: 2.638364231484057

Epoch: 6| Step: 4
Training loss: 3.0363349344208346
Validation loss: 2.641481405681827

Epoch: 6| Step: 5
Training loss: 3.573815961978223
Validation loss: 2.65273888567728

Epoch: 6| Step: 6
Training loss: 2.8341425319841558
Validation loss: 2.6583657222778916

Epoch: 6| Step: 7
Training loss: 3.314324794031513
Validation loss: 2.664471394351734

Epoch: 6| Step: 8
Training loss: 2.8591305153974957
Validation loss: 2.675363213950659

Epoch: 6| Step: 9
Training loss: 3.172637697422516
Validation loss: 2.6659062119063988

Epoch: 6| Step: 10
Training loss: 3.281464633279678
Validation loss: 2.674867698457599

Epoch: 6| Step: 11
Training loss: 2.621301315889511
Validation loss: 2.6650674803453263

Epoch: 6| Step: 12
Training loss: 2.2791083747434397
Validation loss: 2.6658817082584627

Epoch: 6| Step: 13
Training loss: 3.367243719848604
Validation loss: 2.6685888277616336

Epoch: 158| Step: 0
Training loss: 3.3545987974325393
Validation loss: 2.675949611865642

Epoch: 6| Step: 1
Training loss: 3.078666678078267
Validation loss: 2.6715958809353335

Epoch: 6| Step: 2
Training loss: 3.0827278539344567
Validation loss: 2.6698928001937885

Epoch: 6| Step: 3
Training loss: 2.8115683814031027
Validation loss: 2.6669673679637733

Epoch: 6| Step: 4
Training loss: 3.320400676678422
Validation loss: 2.6750868163627826

Epoch: 6| Step: 5
Training loss: 2.989091431736452
Validation loss: 2.673478727592833

Epoch: 6| Step: 6
Training loss: 2.6677564142718637
Validation loss: 2.6777216071939987

Epoch: 6| Step: 7
Training loss: 3.1321787966169894
Validation loss: 2.670183967974234

Epoch: 6| Step: 8
Training loss: 2.9030910264112277
Validation loss: 2.666673596498755

Epoch: 6| Step: 9
Training loss: 2.103126029344508
Validation loss: 2.6575897775717943

Epoch: 6| Step: 10
Training loss: 3.2653909261673095
Validation loss: 2.658268259616069

Epoch: 6| Step: 11
Training loss: 2.6769141256290667
Validation loss: 2.648930789146417

Epoch: 6| Step: 12
Training loss: 3.1857456540525098
Validation loss: 2.646018853020695

Epoch: 6| Step: 13
Training loss: 3.0649838398294253
Validation loss: 2.6388311446355295

Epoch: 159| Step: 0
Training loss: 3.368774323384476
Validation loss: 2.6380436019676945

Epoch: 6| Step: 1
Training loss: 3.3772990555357194
Validation loss: 2.65040986971375

Epoch: 6| Step: 2
Training loss: 3.5835884062596413
Validation loss: 2.648897616555803

Epoch: 6| Step: 3
Training loss: 2.8239047771587846
Validation loss: 2.6512306191652786

Epoch: 6| Step: 4
Training loss: 2.3552048568553707
Validation loss: 2.6487343151414797

Epoch: 6| Step: 5
Training loss: 3.8615634906295515
Validation loss: 2.6442857185033732

Epoch: 6| Step: 6
Training loss: 2.524950453772578
Validation loss: 2.6472087495837013

Epoch: 6| Step: 7
Training loss: 2.056598543350637
Validation loss: 2.639735424985821

Epoch: 6| Step: 8
Training loss: 2.759617028687048
Validation loss: 2.6394769248360523

Epoch: 6| Step: 9
Training loss: 2.9181177798288274
Validation loss: 2.6342375425934494

Epoch: 6| Step: 10
Training loss: 2.9400484109642613
Validation loss: 2.6402276639372406

Epoch: 6| Step: 11
Training loss: 2.907322675393115
Validation loss: 2.6463028456714945

Epoch: 6| Step: 12
Training loss: 2.8996883685290253
Validation loss: 2.66325610590813

Epoch: 6| Step: 13
Training loss: 2.7533761021461243
Validation loss: 2.6644499930006877

Epoch: 160| Step: 0
Training loss: 3.0454285929802074
Validation loss: 2.6580902017999444

Epoch: 6| Step: 1
Training loss: 3.2997158159366364
Validation loss: 2.667067763583949

Epoch: 6| Step: 2
Training loss: 3.128666976942785
Validation loss: 2.671907160993047

Epoch: 6| Step: 3
Training loss: 2.890447662687367
Validation loss: 2.6749848404168133

Epoch: 6| Step: 4
Training loss: 1.9965406540902337
Validation loss: 2.6637628146341203

Epoch: 6| Step: 5
Training loss: 3.206956757994102
Validation loss: 2.659186652428923

Epoch: 6| Step: 6
Training loss: 3.121023776495807
Validation loss: 2.6720041117776208

Epoch: 6| Step: 7
Training loss: 3.3074411593715585
Validation loss: 2.652762244736595

Epoch: 6| Step: 8
Training loss: 3.0569502701088314
Validation loss: 2.649697519035858

Epoch: 6| Step: 9
Training loss: 2.887875790933936
Validation loss: 2.6444271563559485

Epoch: 6| Step: 10
Training loss: 3.368549398985528
Validation loss: 2.6393193648521063

Epoch: 6| Step: 11
Training loss: 1.938242862172135
Validation loss: 2.6503177576915533

Epoch: 6| Step: 12
Training loss: 3.1437899456178635
Validation loss: 2.6367810697798157

Epoch: 6| Step: 13
Training loss: 2.655962760204621
Validation loss: 2.6412035104557714

Epoch: 161| Step: 0
Training loss: 2.6641919057074346
Validation loss: 2.6520546168325794

Epoch: 6| Step: 1
Training loss: 1.9968153513325926
Validation loss: 2.6524178683987647

Epoch: 6| Step: 2
Training loss: 2.0625577976335845
Validation loss: 2.672497894345524

Epoch: 6| Step: 3
Training loss: 2.79888076892439
Validation loss: 2.6845930078362037

Epoch: 6| Step: 4
Training loss: 3.095598689583041
Validation loss: 2.7188371534266844

Epoch: 6| Step: 5
Training loss: 3.1560399958175744
Validation loss: 2.7502833133757343

Epoch: 6| Step: 6
Training loss: 2.732318300870477
Validation loss: 2.7451006510396745

Epoch: 6| Step: 7
Training loss: 3.724840322054717
Validation loss: 2.741244322849833

Epoch: 6| Step: 8
Training loss: 2.969829965213132
Validation loss: 2.722321533686386

Epoch: 6| Step: 9
Training loss: 3.2683231578346237
Validation loss: 2.684924162591528

Epoch: 6| Step: 10
Training loss: 3.30719000392265
Validation loss: 2.6578016194974277

Epoch: 6| Step: 11
Training loss: 2.6138552276285267
Validation loss: 2.647476959594604

Epoch: 6| Step: 12
Training loss: 3.0778648924941057
Validation loss: 2.6336629122230484

Epoch: 6| Step: 13
Training loss: 4.069228015015083
Validation loss: 2.640070628456328

Epoch: 162| Step: 0
Training loss: 2.809793059714831
Validation loss: 2.6318563786639224

Epoch: 6| Step: 1
Training loss: 2.6397639569328035
Validation loss: 2.6348084728812777

Epoch: 6| Step: 2
Training loss: 3.0141303751388993
Validation loss: 2.6379253948937387

Epoch: 6| Step: 3
Training loss: 2.6160890921782674
Validation loss: 2.638714743004165

Epoch: 6| Step: 4
Training loss: 3.225740120113486
Validation loss: 2.631902262337689

Epoch: 6| Step: 5
Training loss: 3.3482552952752673
Validation loss: 2.6332926195106623

Epoch: 6| Step: 6
Training loss: 2.497000611622544
Validation loss: 2.6295972683038946

Epoch: 6| Step: 7
Training loss: 3.0810077935714237
Validation loss: 2.632722245644193

Epoch: 6| Step: 8
Training loss: 3.202507741116199
Validation loss: 2.631713300820135

Epoch: 6| Step: 9
Training loss: 2.8477826711799743
Validation loss: 2.634490614081808

Epoch: 6| Step: 10
Training loss: 2.6734920315556856
Validation loss: 2.6328843386510283

Epoch: 6| Step: 11
Training loss: 3.2596763932327137
Validation loss: 2.6364346759619424

Epoch: 6| Step: 12
Training loss: 3.0548673220728833
Validation loss: 2.6306332129990024

Epoch: 6| Step: 13
Training loss: 3.3819586048356323
Validation loss: 2.6254581055587844

Epoch: 163| Step: 0
Training loss: 1.9982980997173225
Validation loss: 2.624397380546564

Epoch: 6| Step: 1
Training loss: 3.151106316189629
Validation loss: 2.627341774332003

Epoch: 6| Step: 2
Training loss: 3.1997168296455536
Validation loss: 2.627566179508489

Epoch: 6| Step: 3
Training loss: 3.4142298188225855
Validation loss: 2.625964148325463

Epoch: 6| Step: 4
Training loss: 2.3032741544917124
Validation loss: 2.6242010870692885

Epoch: 6| Step: 5
Training loss: 2.6052784085271004
Validation loss: 2.624362020318885

Epoch: 6| Step: 6
Training loss: 2.513781138359556
Validation loss: 2.629065957172065

Epoch: 6| Step: 7
Training loss: 2.7022299635334277
Validation loss: 2.6212633514928836

Epoch: 6| Step: 8
Training loss: 2.9883147593096693
Validation loss: 2.6283209147455047

Epoch: 6| Step: 9
Training loss: 3.2302216478007972
Validation loss: 2.626075396409614

Epoch: 6| Step: 10
Training loss: 3.1794177041655027
Validation loss: 2.6297883067031864

Epoch: 6| Step: 11
Training loss: 3.2486543070198395
Validation loss: 2.625410332082994

Epoch: 6| Step: 12
Training loss: 3.058051946793833
Validation loss: 2.636233093430472

Epoch: 6| Step: 13
Training loss: 3.845972922544732
Validation loss: 2.6366651804718506

Epoch: 164| Step: 0
Training loss: 3.368706805176967
Validation loss: 2.6310567999107177

Epoch: 6| Step: 1
Training loss: 3.1792732737520906
Validation loss: 2.634203826828217

Epoch: 6| Step: 2
Training loss: 2.516252143354872
Validation loss: 2.6363779461650036

Epoch: 6| Step: 3
Training loss: 2.89460686341648
Validation loss: 2.6305850247913387

Epoch: 6| Step: 4
Training loss: 2.5004247304613343
Validation loss: 2.625851663427406

Epoch: 6| Step: 5
Training loss: 3.023474562464617
Validation loss: 2.6318917044393277

Epoch: 6| Step: 6
Training loss: 3.2038875509524285
Validation loss: 2.6337877766832447

Epoch: 6| Step: 7
Training loss: 3.058522970225892
Validation loss: 2.6245802442643518

Epoch: 6| Step: 8
Training loss: 3.03689584257389
Validation loss: 2.6299199827837074

Epoch: 6| Step: 9
Training loss: 3.146366015338652
Validation loss: 2.624772657246609

Epoch: 6| Step: 10
Training loss: 2.7882366079581566
Validation loss: 2.6272832879303163

Epoch: 6| Step: 11
Training loss: 3.154135269418629
Validation loss: 2.6242676781348875

Epoch: 6| Step: 12
Training loss: 2.3639879415369136
Validation loss: 2.623094922171485

Epoch: 6| Step: 13
Training loss: 3.1652596175274623
Validation loss: 2.6265859368988247

Epoch: 165| Step: 0
Training loss: 3.730821778534654
Validation loss: 2.623849500432371

Epoch: 6| Step: 1
Training loss: 2.9327217360339284
Validation loss: 2.618679066678577

Epoch: 6| Step: 2
Training loss: 2.874854042660525
Validation loss: 2.6219876452917132

Epoch: 6| Step: 3
Training loss: 2.851755464896558
Validation loss: 2.621504194767529

Epoch: 6| Step: 4
Training loss: 2.380884910490369
Validation loss: 2.621959976860245

Epoch: 6| Step: 5
Training loss: 3.3522407894640973
Validation loss: 2.620529639085505

Epoch: 6| Step: 6
Training loss: 2.9836020542218
Validation loss: 2.618084012745832

Epoch: 6| Step: 7
Training loss: 2.4787047353854326
Validation loss: 2.619631415938775

Epoch: 6| Step: 8
Training loss: 2.898652019942174
Validation loss: 2.6251531615843473

Epoch: 6| Step: 9
Training loss: 2.9271346299279837
Validation loss: 2.6348218738297886

Epoch: 6| Step: 10
Training loss: 2.4577558022256856
Validation loss: 2.6513004165019645

Epoch: 6| Step: 11
Training loss: 3.2456236497746564
Validation loss: 2.6676398265367225

Epoch: 6| Step: 12
Training loss: 3.1411918773806957
Validation loss: 2.663859333310631

Epoch: 6| Step: 13
Training loss: 3.1165016586223406
Validation loss: 2.674350663586407

Epoch: 166| Step: 0
Training loss: 3.8143124884068307
Validation loss: 2.6712807526604445

Epoch: 6| Step: 1
Training loss: 3.1639221913817153
Validation loss: 2.644286877058164

Epoch: 6| Step: 2
Training loss: 2.9980570541358142
Validation loss: 2.626447454333558

Epoch: 6| Step: 3
Training loss: 2.420669649491941
Validation loss: 2.62009896683333

Epoch: 6| Step: 4
Training loss: 3.4132897661329444
Validation loss: 2.6196559685831264

Epoch: 6| Step: 5
Training loss: 2.902355743424496
Validation loss: 2.6185740716516137

Epoch: 6| Step: 6
Training loss: 2.7703396541428646
Validation loss: 2.6226088048006897

Epoch: 6| Step: 7
Training loss: 3.0684526163426793
Validation loss: 2.626388830395403

Epoch: 6| Step: 8
Training loss: 2.6462201313825218
Validation loss: 2.623966482362216

Epoch: 6| Step: 9
Training loss: 3.014741758354747
Validation loss: 2.6264427866824342

Epoch: 6| Step: 10
Training loss: 2.562485113333283
Validation loss: 2.6232290568970513

Epoch: 6| Step: 11
Training loss: 3.2258554196486493
Validation loss: 2.6228869625999276

Epoch: 6| Step: 12
Training loss: 2.4595801106756614
Validation loss: 2.6213609450341737

Epoch: 6| Step: 13
Training loss: 2.887963136464121
Validation loss: 2.6175323459945807

Epoch: 167| Step: 0
Training loss: 3.1947568676154794
Validation loss: 2.6175936407507865

Epoch: 6| Step: 1
Training loss: 1.9230196265708355
Validation loss: 2.6187309257980957

Epoch: 6| Step: 2
Training loss: 2.64332595923583
Validation loss: 2.625677738116172

Epoch: 6| Step: 3
Training loss: 2.787859146861925
Validation loss: 2.630330339735461

Epoch: 6| Step: 4
Training loss: 3.5410358241115834
Validation loss: 2.6362382304642553

Epoch: 6| Step: 5
Training loss: 3.147428213547695
Validation loss: 2.6363916162712453

Epoch: 6| Step: 6
Training loss: 3.1705406652085717
Validation loss: 2.649464192768199

Epoch: 6| Step: 7
Training loss: 3.093242873425723
Validation loss: 2.645179738216264

Epoch: 6| Step: 8
Training loss: 2.934542851629002
Validation loss: 2.647127993745138

Epoch: 6| Step: 9
Training loss: 2.8906942823200605
Validation loss: 2.6374658078203854

Epoch: 6| Step: 10
Training loss: 3.159812201219031
Validation loss: 2.647290124672856

Epoch: 6| Step: 11
Training loss: 3.077411704512782
Validation loss: 2.637900030727358

Epoch: 6| Step: 12
Training loss: 2.601654509329224
Validation loss: 2.6384869972355762

Epoch: 6| Step: 13
Training loss: 2.9841174269165336
Validation loss: 2.6257023631020697

Epoch: 168| Step: 0
Training loss: 2.076066453182505
Validation loss: 2.6260741771042198

Epoch: 6| Step: 1
Training loss: 3.803405169634102
Validation loss: 2.6250061107355376

Epoch: 6| Step: 2
Training loss: 3.0767754244223124
Validation loss: 2.6398528119805134

Epoch: 6| Step: 3
Training loss: 3.1469565569434006
Validation loss: 2.625973402338615

Epoch: 6| Step: 4
Training loss: 2.852810186961199
Validation loss: 2.625674552209407

Epoch: 6| Step: 5
Training loss: 2.8627352955158782
Validation loss: 2.6248818745479334

Epoch: 6| Step: 6
Training loss: 2.9092801612286423
Validation loss: 2.628167920210063

Epoch: 6| Step: 7
Training loss: 2.8335798567690835
Validation loss: 2.6231652443409668

Epoch: 6| Step: 8
Training loss: 2.716489795083577
Validation loss: 2.6298788157688366

Epoch: 6| Step: 9
Training loss: 2.633797752697009
Validation loss: 2.6245154750574833

Epoch: 6| Step: 10
Training loss: 3.29815186276176
Validation loss: 2.632629853637387

Epoch: 6| Step: 11
Training loss: 2.686385788713982
Validation loss: 2.640931496305226

Epoch: 6| Step: 12
Training loss: 2.981506886800706
Validation loss: 2.6571946859578

Epoch: 6| Step: 13
Training loss: 3.482170605112264
Validation loss: 2.6796372606632537

Epoch: 169| Step: 0
Training loss: 2.850507818120658
Validation loss: 2.6632055392786507

Epoch: 6| Step: 1
Training loss: 2.5075943040847286
Validation loss: 2.640841818908657

Epoch: 6| Step: 2
Training loss: 3.1140390045918083
Validation loss: 2.630136093435814

Epoch: 6| Step: 3
Training loss: 2.421194854722794
Validation loss: 2.6286285136171927

Epoch: 6| Step: 4
Training loss: 3.421901615683805
Validation loss: 2.6245848302480703

Epoch: 6| Step: 5
Training loss: 3.2826988291317867
Validation loss: 2.614962163847637

Epoch: 6| Step: 6
Training loss: 3.133343658700799
Validation loss: 2.616577053736721

Epoch: 6| Step: 7
Training loss: 3.237966708562909
Validation loss: 2.6172699319160055

Epoch: 6| Step: 8
Training loss: 2.9911052449801647
Validation loss: 2.615448506618949

Epoch: 6| Step: 9
Training loss: 3.3303351428950005
Validation loss: 2.617844616563458

Epoch: 6| Step: 10
Training loss: 2.6315434054434035
Validation loss: 2.6132219010070443

Epoch: 6| Step: 11
Training loss: 2.3264556634786917
Validation loss: 2.6178706774169465

Epoch: 6| Step: 12
Training loss: 2.891350402540058
Validation loss: 2.616279691696782

Epoch: 6| Step: 13
Training loss: 2.995937298565515
Validation loss: 2.6160762087231406

Epoch: 170| Step: 0
Training loss: 2.916878574484398
Validation loss: 2.615198764422106

Epoch: 6| Step: 1
Training loss: 3.3097009889091686
Validation loss: 2.6132588780968824

Epoch: 6| Step: 2
Training loss: 3.2623490689614907
Validation loss: 2.6204741938549163

Epoch: 6| Step: 3
Training loss: 2.7908966274000124
Validation loss: 2.611122664632075

Epoch: 6| Step: 4
Training loss: 3.40495219573803
Validation loss: 2.6206503202445304

Epoch: 6| Step: 5
Training loss: 2.5405362121908204
Validation loss: 2.6185858218427605

Epoch: 6| Step: 6
Training loss: 1.9809325388051757
Validation loss: 2.613951643300542

Epoch: 6| Step: 7
Training loss: 3.340668015519799
Validation loss: 2.6203202157832113

Epoch: 6| Step: 8
Training loss: 2.9709422800882908
Validation loss: 2.6252882291861686

Epoch: 6| Step: 9
Training loss: 3.2287932805591604
Validation loss: 2.640995998330784

Epoch: 6| Step: 10
Training loss: 2.89609813622965
Validation loss: 2.647026526125191

Epoch: 6| Step: 11
Training loss: 2.423924815349587
Validation loss: 2.6771868786315154

Epoch: 6| Step: 12
Training loss: 3.0641952804904555
Validation loss: 2.695064911030667

Epoch: 6| Step: 13
Training loss: 2.965312694509755
Validation loss: 2.6596602609525277

Epoch: 171| Step: 0
Training loss: 3.2818864477623015
Validation loss: 2.6368496220725834

Epoch: 6| Step: 1
Training loss: 2.0535347028966853
Validation loss: 2.638553776716132

Epoch: 6| Step: 2
Training loss: 3.1862275818926182
Validation loss: 2.627410979786273

Epoch: 6| Step: 3
Training loss: 3.004271168859842
Validation loss: 2.622683155527205

Epoch: 6| Step: 4
Training loss: 2.9462717452155145
Validation loss: 2.626901006602835

Epoch: 6| Step: 5
Training loss: 3.6874852584285582
Validation loss: 2.6230833553945607

Epoch: 6| Step: 6
Training loss: 3.272025083580963
Validation loss: 2.624939673180298

Epoch: 6| Step: 7
Training loss: 2.85081325773344
Validation loss: 2.6242238258148824

Epoch: 6| Step: 8
Training loss: 2.900496170743727
Validation loss: 2.6201837160250676

Epoch: 6| Step: 9
Training loss: 2.8070121630713274
Validation loss: 2.6141925894508904

Epoch: 6| Step: 10
Training loss: 2.5792692477317916
Validation loss: 2.6189879771136293

Epoch: 6| Step: 11
Training loss: 2.7717897490215164
Validation loss: 2.621224903398863

Epoch: 6| Step: 12
Training loss: 2.8383106569320256
Validation loss: 2.61121481620632

Epoch: 6| Step: 13
Training loss: 2.7685263549862
Validation loss: 2.616787387933625

Epoch: 172| Step: 0
Training loss: 2.446022783661452
Validation loss: 2.6128639424975457

Epoch: 6| Step: 1
Training loss: 2.918062548170306
Validation loss: 2.619077472406089

Epoch: 6| Step: 2
Training loss: 3.1858610539715007
Validation loss: 2.6184536629300346

Epoch: 6| Step: 3
Training loss: 3.5272926504292865
Validation loss: 2.614358257323941

Epoch: 6| Step: 4
Training loss: 3.063728553201977
Validation loss: 2.608954869540493

Epoch: 6| Step: 5
Training loss: 2.974404341105494
Validation loss: 2.6140133652743844

Epoch: 6| Step: 6
Training loss: 2.7936095029590646
Validation loss: 2.61586874372591

Epoch: 6| Step: 7
Training loss: 2.884529904786297
Validation loss: 2.6233774758005115

Epoch: 6| Step: 8
Training loss: 2.9948602198618937
Validation loss: 2.6383793672697675

Epoch: 6| Step: 9
Training loss: 2.8606817876018633
Validation loss: 2.6657849098672917

Epoch: 6| Step: 10
Training loss: 2.5183386530673757
Validation loss: 2.6775000661102353

Epoch: 6| Step: 11
Training loss: 3.106075817727834
Validation loss: 2.6747776996015844

Epoch: 6| Step: 12
Training loss: 3.4996728744221666
Validation loss: 2.6666417675112055

Epoch: 6| Step: 13
Training loss: 2.1280205520340143
Validation loss: 2.6553396422726006

Epoch: 173| Step: 0
Training loss: 2.2795886953694415
Validation loss: 2.647231688723896

Epoch: 6| Step: 1
Training loss: 3.1923596605060602
Validation loss: 2.640109001161084

Epoch: 6| Step: 2
Training loss: 2.6133887207558555
Validation loss: 2.6512171609467936

Epoch: 6| Step: 3
Training loss: 2.6255926643892105
Validation loss: 2.6396740946777517

Epoch: 6| Step: 4
Training loss: 3.1962787687448815
Validation loss: 2.6277209251843976

Epoch: 6| Step: 5
Training loss: 3.3617436065215873
Validation loss: 2.6225915906970574

Epoch: 6| Step: 6
Training loss: 2.9577497166309508
Validation loss: 2.6157321626046204

Epoch: 6| Step: 7
Training loss: 2.936897378346322
Validation loss: 2.6208635897329233

Epoch: 6| Step: 8
Training loss: 3.4342934651714656
Validation loss: 2.6221525508312205

Epoch: 6| Step: 9
Training loss: 2.4324905564692356
Validation loss: 2.616675766523517

Epoch: 6| Step: 10
Training loss: 2.6626607389688135
Validation loss: 2.6106488053417243

Epoch: 6| Step: 11
Training loss: 2.9428078603652232
Validation loss: 2.6162876394929127

Epoch: 6| Step: 12
Training loss: 3.2064266401566175
Validation loss: 2.6101778507374758

Epoch: 6| Step: 13
Training loss: 3.3803163265302887
Validation loss: 2.6085274773853158

Epoch: 174| Step: 0
Training loss: 2.4649892241143774
Validation loss: 2.611565401330493

Epoch: 6| Step: 1
Training loss: 2.7138354727534697
Validation loss: 2.6174599292835943

Epoch: 6| Step: 2
Training loss: 2.7410634228432795
Validation loss: 2.6131483352267932

Epoch: 6| Step: 3
Training loss: 3.3419308571457416
Validation loss: 2.61102574586435

Epoch: 6| Step: 4
Training loss: 3.3749416487559194
Validation loss: 2.606552469085005

Epoch: 6| Step: 5
Training loss: 3.160365678899395
Validation loss: 2.610870827618515

Epoch: 6| Step: 6
Training loss: 2.72060234721152
Validation loss: 2.6143128865130176

Epoch: 6| Step: 7
Training loss: 3.399513091957455
Validation loss: 2.6133575828064215

Epoch: 6| Step: 8
Training loss: 2.8598435648570186
Validation loss: 2.615695303297042

Epoch: 6| Step: 9
Training loss: 2.7925818542122602
Validation loss: 2.620457718055319

Epoch: 6| Step: 10
Training loss: 3.3024318924262115
Validation loss: 2.619548791160463

Epoch: 6| Step: 11
Training loss: 2.9748597568826867
Validation loss: 2.6220699429232486

Epoch: 6| Step: 12
Training loss: 2.500534095932926
Validation loss: 2.6264328617956765

Epoch: 6| Step: 13
Training loss: 2.8517467700521606
Validation loss: 2.621296423913779

Epoch: 175| Step: 0
Training loss: 3.15364964697828
Validation loss: 2.623367761148701

Epoch: 6| Step: 1
Training loss: 2.702278489766531
Validation loss: 2.626445229830847

Epoch: 6| Step: 2
Training loss: 2.9740631744201225
Validation loss: 2.6313482520154694

Epoch: 6| Step: 3
Training loss: 2.9859324281051576
Validation loss: 2.6195835459878007

Epoch: 6| Step: 4
Training loss: 2.6717886213650694
Validation loss: 2.618557923619187

Epoch: 6| Step: 5
Training loss: 2.9299654816036096
Validation loss: 2.618406906206012

Epoch: 6| Step: 6
Training loss: 3.003977046772192
Validation loss: 2.628245357919772

Epoch: 6| Step: 7
Training loss: 2.9882418361881973
Validation loss: 2.6307412134758357

Epoch: 6| Step: 8
Training loss: 2.7730449559900565
Validation loss: 2.621934392736638

Epoch: 6| Step: 9
Training loss: 3.152533732270833
Validation loss: 2.6214029088758015

Epoch: 6| Step: 10
Training loss: 3.014549102975516
Validation loss: 2.6105665826978313

Epoch: 6| Step: 11
Training loss: 2.963674281973947
Validation loss: 2.6068141761097263

Epoch: 6| Step: 12
Training loss: 3.2008471321325627
Validation loss: 2.6085591938645702

Epoch: 6| Step: 13
Training loss: 2.6431106515757885
Validation loss: 2.6058728390745585

Epoch: 176| Step: 0
Training loss: 3.3652117037106324
Validation loss: 2.6053656894544073

Epoch: 6| Step: 1
Training loss: 2.3822216864881023
Validation loss: 2.6092538720151826

Epoch: 6| Step: 2
Training loss: 2.6331734339136963
Validation loss: 2.6075660822080153

Epoch: 6| Step: 3
Training loss: 2.9019272352648633
Validation loss: 2.6099193619962633

Epoch: 6| Step: 4
Training loss: 3.321305572998142
Validation loss: 2.6114283192809515

Epoch: 6| Step: 5
Training loss: 2.6131016964661398
Validation loss: 2.6114896041642055

Epoch: 6| Step: 6
Training loss: 2.6430157554927587
Validation loss: 2.6058320801930894

Epoch: 6| Step: 7
Training loss: 3.070691267339321
Validation loss: 2.608438485010216

Epoch: 6| Step: 8
Training loss: 2.589052105216856
Validation loss: 2.6142973358364636

Epoch: 6| Step: 9
Training loss: 2.749252737994564
Validation loss: 2.614723134826898

Epoch: 6| Step: 10
Training loss: 2.8062158527293253
Validation loss: 2.6114955207279102

Epoch: 6| Step: 11
Training loss: 3.605240737136021
Validation loss: 2.6181839631758224

Epoch: 6| Step: 12
Training loss: 3.1412840194469074
Validation loss: 2.6160480524642056

Epoch: 6| Step: 13
Training loss: 3.4748462341099366
Validation loss: 2.619627517577346

Epoch: 177| Step: 0
Training loss: 3.0931557749344365
Validation loss: 2.6145032203584244

Epoch: 6| Step: 1
Training loss: 2.6937241449022475
Validation loss: 2.6137589310031077

Epoch: 6| Step: 2
Training loss: 3.0107385447898007
Validation loss: 2.6127711730434653

Epoch: 6| Step: 3
Training loss: 3.068796497000484
Validation loss: 2.61551360817388

Epoch: 6| Step: 4
Training loss: 2.758098295710519
Validation loss: 2.622592032536905

Epoch: 6| Step: 5
Training loss: 2.4532666985474
Validation loss: 2.61910198521744

Epoch: 6| Step: 6
Training loss: 3.059301769404937
Validation loss: 2.6179823066348993

Epoch: 6| Step: 7
Training loss: 2.753907375470736
Validation loss: 2.6270432525957865

Epoch: 6| Step: 8
Training loss: 3.369439241748248
Validation loss: 2.6331934840226503

Epoch: 6| Step: 9
Training loss: 3.712217090243734
Validation loss: 2.635743007439344

Epoch: 6| Step: 10
Training loss: 2.403517720013714
Validation loss: 2.63061209476964

Epoch: 6| Step: 11
Training loss: 2.8752709965902357
Validation loss: 2.637478476955921

Epoch: 6| Step: 12
Training loss: 2.8577392568739226
Validation loss: 2.64112977913454

Epoch: 6| Step: 13
Training loss: 2.9195269773726094
Validation loss: 2.634965813213361

Epoch: 178| Step: 0
Training loss: 3.1868765726214647
Validation loss: 2.6391527829417223

Epoch: 6| Step: 1
Training loss: 3.0546206884077813
Validation loss: 2.633847228261127

Epoch: 6| Step: 2
Training loss: 2.7149846102171225
Validation loss: 2.634896415057839

Epoch: 6| Step: 3
Training loss: 3.195722856729971
Validation loss: 2.6285041141065797

Epoch: 6| Step: 4
Training loss: 2.6651054421583895
Validation loss: 2.61803106366505

Epoch: 6| Step: 5
Training loss: 3.2565458840573247
Validation loss: 2.6214638950498355

Epoch: 6| Step: 6
Training loss: 2.679944440351169
Validation loss: 2.61665725148518

Epoch: 6| Step: 7
Training loss: 3.2683400817936294
Validation loss: 2.6103183533732612

Epoch: 6| Step: 8
Training loss: 3.1895601683277293
Validation loss: 2.606338787910024

Epoch: 6| Step: 9
Training loss: 3.274759030394957
Validation loss: 2.6115078809722982

Epoch: 6| Step: 10
Training loss: 2.600153911143204
Validation loss: 2.609392060588878

Epoch: 6| Step: 11
Training loss: 2.6238240832464776
Validation loss: 2.6056241507857165

Epoch: 6| Step: 12
Training loss: 2.5291335123100165
Validation loss: 2.6012460263057866

Epoch: 6| Step: 13
Training loss: 2.6461989583177514
Validation loss: 2.6156016825547423

Epoch: 179| Step: 0
Training loss: 3.2743505693031407
Validation loss: 2.610489865807899

Epoch: 6| Step: 1
Training loss: 3.4064261014720456
Validation loss: 2.6214974431484492

Epoch: 6| Step: 2
Training loss: 3.3776689502127715
Validation loss: 2.617553704908049

Epoch: 6| Step: 3
Training loss: 2.952626665646128
Validation loss: 2.6266584963140764

Epoch: 6| Step: 4
Training loss: 2.9084467635309994
Validation loss: 2.6425833947333395

Epoch: 6| Step: 5
Training loss: 2.532483118420865
Validation loss: 2.645823382674705

Epoch: 6| Step: 6
Training loss: 3.077799978378904
Validation loss: 2.6383990902041443

Epoch: 6| Step: 7
Training loss: 2.725351321470216
Validation loss: 2.615733149550329

Epoch: 6| Step: 8
Training loss: 2.495897073414752
Validation loss: 2.605651568630189

Epoch: 6| Step: 9
Training loss: 2.559261229132429
Validation loss: 2.6077926275675956

Epoch: 6| Step: 10
Training loss: 2.7516872258595706
Validation loss: 2.603139696644899

Epoch: 6| Step: 11
Training loss: 3.0611164315283808
Validation loss: 2.607102629126826

Epoch: 6| Step: 12
Training loss: 3.252058111105614
Validation loss: 2.6081378792638987

Epoch: 6| Step: 13
Training loss: 2.497953435538658
Validation loss: 2.6104456573144836

Epoch: 180| Step: 0
Training loss: 2.556695644640506
Validation loss: 2.6108717555234673

Epoch: 6| Step: 1
Training loss: 3.1083511890651914
Validation loss: 2.6075573704603925

Epoch: 6| Step: 2
Training loss: 3.0984353700785356
Validation loss: 2.6138302870243004

Epoch: 6| Step: 3
Training loss: 2.808625668064462
Validation loss: 2.606815959082569

Epoch: 6| Step: 4
Training loss: 3.022727547813085
Validation loss: 2.6052959781077836

Epoch: 6| Step: 5
Training loss: 3.038669119432884
Validation loss: 2.6070971234597633

Epoch: 6| Step: 6
Training loss: 2.4144115998024236
Validation loss: 2.606820915602984

Epoch: 6| Step: 7
Training loss: 2.6270934340433816
Validation loss: 2.6059576593817493

Epoch: 6| Step: 8
Training loss: 2.91123911533311
Validation loss: 2.606896035236608

Epoch: 6| Step: 9
Training loss: 2.9887114171593407
Validation loss: 2.6036441713609384

Epoch: 6| Step: 10
Training loss: 3.570480509671813
Validation loss: 2.6144898937239778

Epoch: 6| Step: 11
Training loss: 2.9949227920795516
Validation loss: 2.6130630233069003

Epoch: 6| Step: 12
Training loss: 3.1887094035867336
Validation loss: 2.609655190202839

Epoch: 6| Step: 13
Training loss: 2.7951171294971955
Validation loss: 2.615237885351134

Epoch: 181| Step: 0
Training loss: 3.153750043302786
Validation loss: 2.6127264742771343

Epoch: 6| Step: 1
Training loss: 2.7680891018897813
Validation loss: 2.6121905505524237

Epoch: 6| Step: 2
Training loss: 3.3894246169532654
Validation loss: 2.6336835650369834

Epoch: 6| Step: 3
Training loss: 3.4100673178910745
Validation loss: 2.6353203133598933

Epoch: 6| Step: 4
Training loss: 3.046329390751763
Validation loss: 2.651735190211042

Epoch: 6| Step: 5
Training loss: 2.8931594872288806
Validation loss: 2.639437133610663

Epoch: 6| Step: 6
Training loss: 2.957725211667638
Validation loss: 2.644258821468995

Epoch: 6| Step: 7
Training loss: 2.8414874455679566
Validation loss: 2.632108307721837

Epoch: 6| Step: 8
Training loss: 2.6730429790222594
Validation loss: 2.616242482441814

Epoch: 6| Step: 9
Training loss: 3.0948786362623912
Validation loss: 2.6131895774466716

Epoch: 6| Step: 10
Training loss: 2.733637421448009
Validation loss: 2.6075390433495227

Epoch: 6| Step: 11
Training loss: 2.6461639998655735
Validation loss: 2.60679555464094

Epoch: 6| Step: 12
Training loss: 2.253386174697834
Validation loss: 2.6033356500220637

Epoch: 6| Step: 13
Training loss: 3.2884709422002367
Validation loss: 2.602338665318067

Epoch: 182| Step: 0
Training loss: 3.698649990100115
Validation loss: 2.600366270764199

Epoch: 6| Step: 1
Training loss: 2.3507702275401585
Validation loss: 2.5981441696171705

Epoch: 6| Step: 2
Training loss: 2.8247503727363106
Validation loss: 2.601784149732792

Epoch: 6| Step: 3
Training loss: 2.628886842018349
Validation loss: 2.595244387217695

Epoch: 6| Step: 4
Training loss: 2.8994138980401605
Validation loss: 2.599567050477887

Epoch: 6| Step: 5
Training loss: 3.675294493209545
Validation loss: 2.60688132538846

Epoch: 6| Step: 6
Training loss: 3.006228339420569
Validation loss: 2.6030492624071693

Epoch: 6| Step: 7
Training loss: 2.8207272042426212
Validation loss: 2.60641837316009

Epoch: 6| Step: 8
Training loss: 2.8314274006016698
Validation loss: 2.6086646656816477

Epoch: 6| Step: 9
Training loss: 3.2282459217921087
Validation loss: 2.6106508626176796

Epoch: 6| Step: 10
Training loss: 2.561488440358522
Validation loss: 2.608611996151115

Epoch: 6| Step: 11
Training loss: 2.450145296731942
Validation loss: 2.606704253921355

Epoch: 6| Step: 12
Training loss: 3.036440151851567
Validation loss: 2.609644598294172

Epoch: 6| Step: 13
Training loss: 2.636521350394342
Validation loss: 2.6035729221569768

Epoch: 183| Step: 0
Training loss: 2.875510129037501
Validation loss: 2.6159104614858886

Epoch: 6| Step: 1
Training loss: 3.0157002652119806
Validation loss: 2.6160566448003273

Epoch: 6| Step: 2
Training loss: 3.4553446130535947
Validation loss: 2.6118498922642677

Epoch: 6| Step: 3
Training loss: 2.585911407800058
Validation loss: 2.607943217625577

Epoch: 6| Step: 4
Training loss: 3.045706970126308
Validation loss: 2.6043643336597144

Epoch: 6| Step: 5
Training loss: 2.632718500559077
Validation loss: 2.603182629796747

Epoch: 6| Step: 6
Training loss: 3.206378754229789
Validation loss: 2.6097498592487174

Epoch: 6| Step: 7
Training loss: 2.8120705170668363
Validation loss: 2.6046853157317416

Epoch: 6| Step: 8
Training loss: 3.181791991584296
Validation loss: 2.6061380041569664

Epoch: 6| Step: 9
Training loss: 2.5371692347935064
Validation loss: 2.5986642896621284

Epoch: 6| Step: 10
Training loss: 3.34322231799342
Validation loss: 2.6015457861795577

Epoch: 6| Step: 11
Training loss: 2.6194823267617755
Validation loss: 2.606074748955541

Epoch: 6| Step: 12
Training loss: 2.766868354922084
Validation loss: 2.596847359139848

Epoch: 6| Step: 13
Training loss: 2.9001951415078406
Validation loss: 2.605505745124173

Epoch: 184| Step: 0
Training loss: 2.8411682482335516
Validation loss: 2.604093666940339

Epoch: 6| Step: 1
Training loss: 2.9814064480659686
Validation loss: 2.6015288051985417

Epoch: 6| Step: 2
Training loss: 2.745210985924173
Validation loss: 2.602022877667915

Epoch: 6| Step: 3
Training loss: 2.348502059120216
Validation loss: 2.610529431460383

Epoch: 6| Step: 4
Training loss: 3.2871598564528317
Validation loss: 2.6153438043781256

Epoch: 6| Step: 5
Training loss: 3.295863715094877
Validation loss: 2.6168504096576415

Epoch: 6| Step: 6
Training loss: 3.3545312782020003
Validation loss: 2.626428856886318

Epoch: 6| Step: 7
Training loss: 3.2576179720755696
Validation loss: 2.6278877200837956

Epoch: 6| Step: 8
Training loss: 3.088079322487934
Validation loss: 2.609873040766559

Epoch: 6| Step: 9
Training loss: 2.47074075903897
Validation loss: 2.597525955498937

Epoch: 6| Step: 10
Training loss: 3.108558585942811
Validation loss: 2.5996307125383695

Epoch: 6| Step: 11
Training loss: 2.7925165410520956
Validation loss: 2.5988332310710587

Epoch: 6| Step: 12
Training loss: 2.4758919852119
Validation loss: 2.598534430468735

Epoch: 6| Step: 13
Training loss: 3.001345809430437
Validation loss: 2.5967794567768943

Epoch: 185| Step: 0
Training loss: 2.6924985487853434
Validation loss: 2.5925242689033112

Epoch: 6| Step: 1
Training loss: 2.7990552193829235
Validation loss: 2.5916263086411746

Epoch: 6| Step: 2
Training loss: 3.2550297477054286
Validation loss: 2.5895328627457173

Epoch: 6| Step: 3
Training loss: 2.7425493398312257
Validation loss: 2.5903368347339306

Epoch: 6| Step: 4
Training loss: 3.5822474290682944
Validation loss: 2.5930267296160707

Epoch: 6| Step: 5
Training loss: 2.68205116946843
Validation loss: 2.5957002282272117

Epoch: 6| Step: 6
Training loss: 2.5973975949115387
Validation loss: 2.597872880489918

Epoch: 6| Step: 7
Training loss: 3.1469892858002244
Validation loss: 2.5947508607516374

Epoch: 6| Step: 8
Training loss: 2.322315109662531
Validation loss: 2.59611333187831

Epoch: 6| Step: 9
Training loss: 3.0326311563305355
Validation loss: 2.593784123994493

Epoch: 6| Step: 10
Training loss: 2.669097795537806
Validation loss: 2.595013504075378

Epoch: 6| Step: 11
Training loss: 3.1462703847546822
Validation loss: 2.606672109673679

Epoch: 6| Step: 12
Training loss: 2.887653204746683
Validation loss: 2.605807864609791

Epoch: 6| Step: 13
Training loss: 3.618549263612886
Validation loss: 2.613659573483188

Epoch: 186| Step: 0
Training loss: 3.3231934020625715
Validation loss: 2.6135361515846367

Epoch: 6| Step: 1
Training loss: 2.7523691202727436
Validation loss: 2.6204982015147356

Epoch: 6| Step: 2
Training loss: 3.24301864817219
Validation loss: 2.6207821370422897

Epoch: 6| Step: 3
Training loss: 2.1937847862854727
Validation loss: 2.6148998279914397

Epoch: 6| Step: 4
Training loss: 2.6253651864479988
Validation loss: 2.6145851221206353

Epoch: 6| Step: 5
Training loss: 2.9075668796971996
Validation loss: 2.612597639823688

Epoch: 6| Step: 6
Training loss: 2.4162390319606857
Validation loss: 2.615765369643896

Epoch: 6| Step: 7
Training loss: 3.2453797584583732
Validation loss: 2.6153443283114535

Epoch: 6| Step: 8
Training loss: 2.836653017518511
Validation loss: 2.636560725706934

Epoch: 6| Step: 9
Training loss: 2.966948112967516
Validation loss: 2.6320581741589097

Epoch: 6| Step: 10
Training loss: 2.97178863817078
Validation loss: 2.6230096630283604

Epoch: 6| Step: 11
Training loss: 3.1746657548518864
Validation loss: 2.61203292185947

Epoch: 6| Step: 12
Training loss: 3.356316598128454
Validation loss: 2.6125293411905024

Epoch: 6| Step: 13
Training loss: 2.8014380032162514
Validation loss: 2.613129086868849

Epoch: 187| Step: 0
Training loss: 2.9526518589020836
Validation loss: 2.6080169395678254

Epoch: 6| Step: 1
Training loss: 2.149913378012934
Validation loss: 2.6070611037793525

Epoch: 6| Step: 2
Training loss: 2.3958152328028604
Validation loss: 2.602374397690632

Epoch: 6| Step: 3
Training loss: 2.7420473416148794
Validation loss: 2.608497155610637

Epoch: 6| Step: 4
Training loss: 2.6336300992536565
Validation loss: 2.6044322499034953

Epoch: 6| Step: 5
Training loss: 3.242066015702163
Validation loss: 2.6019097789611694

Epoch: 6| Step: 6
Training loss: 2.9258200384403117
Validation loss: 2.6022340148754037

Epoch: 6| Step: 7
Training loss: 2.4841729807738795
Validation loss: 2.60117488832371

Epoch: 6| Step: 8
Training loss: 3.2983937313441243
Validation loss: 2.601757540504123

Epoch: 6| Step: 9
Training loss: 3.4801292382281592
Validation loss: 2.605055698040794

Epoch: 6| Step: 10
Training loss: 3.347393439536245
Validation loss: 2.602149170726287

Epoch: 6| Step: 11
Training loss: 2.965516909849367
Validation loss: 2.6000248229630074

Epoch: 6| Step: 12
Training loss: 3.4171896433463784
Validation loss: 2.6022773747633328

Epoch: 6| Step: 13
Training loss: 2.713650711611822
Validation loss: 2.6026534109273953

Epoch: 188| Step: 0
Training loss: 3.13243728635807
Validation loss: 2.6103941207129604

Epoch: 6| Step: 1
Training loss: 2.4504718404014736
Validation loss: 2.6057698141877

Epoch: 6| Step: 2
Training loss: 3.3639546250810954
Validation loss: 2.620248860387982

Epoch: 6| Step: 3
Training loss: 3.217477491369707
Validation loss: 2.6192052207593184

Epoch: 6| Step: 4
Training loss: 3.117971986039178
Validation loss: 2.628517833940043

Epoch: 6| Step: 5
Training loss: 2.3757576988630964
Validation loss: 2.6443866931307753

Epoch: 6| Step: 6
Training loss: 2.898540813733171
Validation loss: 2.6450491193497654

Epoch: 6| Step: 7
Training loss: 2.926443516501461
Validation loss: 2.647952503233479

Epoch: 6| Step: 8
Training loss: 3.3191353899220957
Validation loss: 2.646767516674939

Epoch: 6| Step: 9
Training loss: 2.3406107923741133
Validation loss: 2.634688596890931

Epoch: 6| Step: 10
Training loss: 2.9830886704977857
Validation loss: 2.636720387326503

Epoch: 6| Step: 11
Training loss: 2.379292774588334
Validation loss: 2.6292288771991053

Epoch: 6| Step: 12
Training loss: 3.0995906067287806
Validation loss: 2.6205851105345763

Epoch: 6| Step: 13
Training loss: 3.389835107444618
Validation loss: 2.622849341750409

Epoch: 189| Step: 0
Training loss: 3.460831685741341
Validation loss: 2.604956891337728

Epoch: 6| Step: 1
Training loss: 3.237451242204938
Validation loss: 2.6024477740059324

Epoch: 6| Step: 2
Training loss: 2.963660766854265
Validation loss: 2.5985283413408187

Epoch: 6| Step: 3
Training loss: 3.0138710732429104
Validation loss: 2.5934401227088517

Epoch: 6| Step: 4
Training loss: 2.3493094058570154
Validation loss: 2.587807773371328

Epoch: 6| Step: 5
Training loss: 3.2531587948908043
Validation loss: 2.5899751565188094

Epoch: 6| Step: 6
Training loss: 3.0056402908605735
Validation loss: 2.58771252450176

Epoch: 6| Step: 7
Training loss: 2.3557741084265404
Validation loss: 2.5834934278957196

Epoch: 6| Step: 8
Training loss: 1.9218882738600929
Validation loss: 2.587398138226438

Epoch: 6| Step: 9
Training loss: 3.3879017947037133
Validation loss: 2.585496567026651

Epoch: 6| Step: 10
Training loss: 3.231814933105053
Validation loss: 2.583762217303959

Epoch: 6| Step: 11
Training loss: 2.287685253375552
Validation loss: 2.601582249781368

Epoch: 6| Step: 12
Training loss: 3.073256461974317
Validation loss: 2.5867027045522937

Epoch: 6| Step: 13
Training loss: 3.1997663054963934
Validation loss: 2.5891728980413657

Epoch: 190| Step: 0
Training loss: 2.9943017885115712
Validation loss: 2.603161629620736

Epoch: 6| Step: 1
Training loss: 3.7477033575968854
Validation loss: 2.599283428419284

Epoch: 6| Step: 2
Training loss: 2.69944150587942
Validation loss: 2.5992515129476135

Epoch: 6| Step: 3
Training loss: 2.7834553065575736
Validation loss: 2.591064242750414

Epoch: 6| Step: 4
Training loss: 2.831613486159006
Validation loss: 2.597553677923574

Epoch: 6| Step: 5
Training loss: 2.3456038200709006
Validation loss: 2.602384871422779

Epoch: 6| Step: 6
Training loss: 3.2303605528566988
Validation loss: 2.6038713109689815

Epoch: 6| Step: 7
Training loss: 2.4858158180096317
Validation loss: 2.6049881976071276

Epoch: 6| Step: 8
Training loss: 3.0020998917034096
Validation loss: 2.6187324764749995

Epoch: 6| Step: 9
Training loss: 3.0857295099014053
Validation loss: 2.618279673090851

Epoch: 6| Step: 10
Training loss: 2.8702042427891272
Validation loss: 2.627677354889231

Epoch: 6| Step: 11
Training loss: 3.0193193171299297
Validation loss: 2.6355152743165764

Epoch: 6| Step: 12
Training loss: 2.6462086889387693
Validation loss: 2.630914788458461

Epoch: 6| Step: 13
Training loss: 3.1196383447604323
Validation loss: 2.627310453400686

Epoch: 191| Step: 0
Training loss: 2.7499488479018406
Validation loss: 2.641590864797384

Epoch: 6| Step: 1
Training loss: 2.5020670928131556
Validation loss: 2.6330065576144777

Epoch: 6| Step: 2
Training loss: 2.544187098634628
Validation loss: 2.6254079485128696

Epoch: 6| Step: 3
Training loss: 2.6537671321939387
Validation loss: 2.6215633802181544

Epoch: 6| Step: 4
Training loss: 3.363158468559511
Validation loss: 2.618019246354918

Epoch: 6| Step: 5
Training loss: 2.611125884003028
Validation loss: 2.6181200761234935

Epoch: 6| Step: 6
Training loss: 2.734349365114211
Validation loss: 2.614737569170924

Epoch: 6| Step: 7
Training loss: 3.659608260488991
Validation loss: 2.6081084833222516

Epoch: 6| Step: 8
Training loss: 2.773013488164454
Validation loss: 2.598587957353784

Epoch: 6| Step: 9
Training loss: 3.211299235440543
Validation loss: 2.6079243732089474

Epoch: 6| Step: 10
Training loss: 3.476616539695874
Validation loss: 2.5970620482913263

Epoch: 6| Step: 11
Training loss: 2.864487044132156
Validation loss: 2.589418515082726

Epoch: 6| Step: 12
Training loss: 2.720653350002331
Validation loss: 2.5932045274181044

Epoch: 6| Step: 13
Training loss: 2.796422740663638
Validation loss: 2.5876911362424826

Epoch: 192| Step: 0
Training loss: 2.790976671570366
Validation loss: 2.5892663872425516

Epoch: 6| Step: 1
Training loss: 2.6076681070825627
Validation loss: 2.583021310493067

Epoch: 6| Step: 2
Training loss: 3.1043286035955604
Validation loss: 2.58949678883081

Epoch: 6| Step: 3
Training loss: 3.360073141255619
Validation loss: 2.587671861989225

Epoch: 6| Step: 4
Training loss: 2.7844953996891237
Validation loss: 2.587450950268226

Epoch: 6| Step: 5
Training loss: 2.5059743068440468
Validation loss: 2.585792550577239

Epoch: 6| Step: 6
Training loss: 2.6901201295086703
Validation loss: 2.588360428982833

Epoch: 6| Step: 7
Training loss: 2.8798553268864717
Validation loss: 2.583923015185757

Epoch: 6| Step: 8
Training loss: 3.263583626686913
Validation loss: 2.5872893758308293

Epoch: 6| Step: 9
Training loss: 3.3420855220807644
Validation loss: 2.596774386315048

Epoch: 6| Step: 10
Training loss: 2.9486866047744495
Validation loss: 2.6287076044570963

Epoch: 6| Step: 11
Training loss: 3.131063758047416
Validation loss: 2.646906968123583

Epoch: 6| Step: 12
Training loss: 2.7038735472325306
Validation loss: 2.661272702822213

Epoch: 6| Step: 13
Training loss: 2.70420031024555
Validation loss: 2.661382635845484

Epoch: 193| Step: 0
Training loss: 2.861521931096457
Validation loss: 2.631827081074132

Epoch: 6| Step: 1
Training loss: 3.037244395109653
Validation loss: 2.625216946326492

Epoch: 6| Step: 2
Training loss: 2.9044323230961675
Validation loss: 2.6065936179786355

Epoch: 6| Step: 3
Training loss: 2.836974067512992
Validation loss: 2.5922938011427865

Epoch: 6| Step: 4
Training loss: 3.286873204342659
Validation loss: 2.587811662704097

Epoch: 6| Step: 5
Training loss: 3.0909435759879873
Validation loss: 2.58406806451683

Epoch: 6| Step: 6
Training loss: 2.971206612340403
Validation loss: 2.589063015052526

Epoch: 6| Step: 7
Training loss: 2.7822264768065534
Validation loss: 2.5903008482230603

Epoch: 6| Step: 8
Training loss: 2.991642914857502
Validation loss: 2.5844953978519656

Epoch: 6| Step: 9
Training loss: 3.135891968111935
Validation loss: 2.5866894200232067

Epoch: 6| Step: 10
Training loss: 2.9247862648105105
Validation loss: 2.5920631338754223

Epoch: 6| Step: 11
Training loss: 2.536577622717621
Validation loss: 2.587978327900552

Epoch: 6| Step: 12
Training loss: 2.7802062058300705
Validation loss: 2.600681101303044

Epoch: 6| Step: 13
Training loss: 2.8075956181236688
Validation loss: 2.599458434305161

Epoch: 194| Step: 0
Training loss: 2.188366309694909
Validation loss: 2.6003207711684664

Epoch: 6| Step: 1
Training loss: 2.9718422295240736
Validation loss: 2.603759204776809

Epoch: 6| Step: 2
Training loss: 2.6000883490883893
Validation loss: 2.60148839214647

Epoch: 6| Step: 3
Training loss: 2.889673024724309
Validation loss: 2.591570628858141

Epoch: 6| Step: 4
Training loss: 3.408989041280323
Validation loss: 2.597613002464279

Epoch: 6| Step: 5
Training loss: 2.393551101588501
Validation loss: 2.5947565052484722

Epoch: 6| Step: 6
Training loss: 2.941517780920039
Validation loss: 2.5939801298492986

Epoch: 6| Step: 7
Training loss: 3.319274309197308
Validation loss: 2.5980274123711675

Epoch: 6| Step: 8
Training loss: 2.4582163465002775
Validation loss: 2.6027412270616606

Epoch: 6| Step: 9
Training loss: 3.5879411273323765
Validation loss: 2.6115364376153356

Epoch: 6| Step: 10
Training loss: 2.778964185606798
Validation loss: 2.6053590062200342

Epoch: 6| Step: 11
Training loss: 3.346575532510779
Validation loss: 2.613987079623862

Epoch: 6| Step: 12
Training loss: 2.8546341747529542
Validation loss: 2.616588309296818

Epoch: 6| Step: 13
Training loss: 2.926156889531195
Validation loss: 2.6149951189083778

Epoch: 195| Step: 0
Training loss: 3.165574454292836
Validation loss: 2.609968829533232

Epoch: 6| Step: 1
Training loss: 2.4482032337597106
Validation loss: 2.6261320529727814

Epoch: 6| Step: 2
Training loss: 3.29108835926673
Validation loss: 2.6207047150476215

Epoch: 6| Step: 3
Training loss: 2.086964329501073
Validation loss: 2.622081692114218

Epoch: 6| Step: 4
Training loss: 2.846970296326265
Validation loss: 2.627176904698122

Epoch: 6| Step: 5
Training loss: 3.029710201593487
Validation loss: 2.637269627713663

Epoch: 6| Step: 6
Training loss: 2.937700305359014
Validation loss: 2.618057569184565

Epoch: 6| Step: 7
Training loss: 2.522294674428842
Validation loss: 2.614035589496439

Epoch: 6| Step: 8
Training loss: 3.0927074486763444
Validation loss: 2.597296180291384

Epoch: 6| Step: 9
Training loss: 3.3112663994999
Validation loss: 2.590747463805654

Epoch: 6| Step: 10
Training loss: 2.7674676823301296
Validation loss: 2.589144584866589

Epoch: 6| Step: 11
Training loss: 3.052727658391923
Validation loss: 2.59292069881645

Epoch: 6| Step: 12
Training loss: 3.2969370470038912
Validation loss: 2.5931648714241313

Epoch: 6| Step: 13
Training loss: 2.797760381094924
Validation loss: 2.5930256213198337

Epoch: 196| Step: 0
Training loss: 2.3180821035226242
Validation loss: 2.596083750405319

Epoch: 6| Step: 1
Training loss: 2.883662566276189
Validation loss: 2.587785833105739

Epoch: 6| Step: 2
Training loss: 3.336289970127073
Validation loss: 2.5846859308197074

Epoch: 6| Step: 3
Training loss: 3.0143834854313427
Validation loss: 2.58527367358146

Epoch: 6| Step: 4
Training loss: 2.4771267702375743
Validation loss: 2.5882801706143646

Epoch: 6| Step: 5
Training loss: 3.460071463976956
Validation loss: 2.5909990522728243

Epoch: 6| Step: 6
Training loss: 2.700443856525685
Validation loss: 2.5830473733300936

Epoch: 6| Step: 7
Training loss: 2.4602129157122166
Validation loss: 2.5849724675850876

Epoch: 6| Step: 8
Training loss: 3.4449324279270557
Validation loss: 2.5851759900139286

Epoch: 6| Step: 9
Training loss: 2.663295601906549
Validation loss: 2.580087773120259

Epoch: 6| Step: 10
Training loss: 2.98398591546015
Validation loss: 2.587414309811806

Epoch: 6| Step: 11
Training loss: 2.943530446670888
Validation loss: 2.5824726813292287

Epoch: 6| Step: 12
Training loss: 3.119468676490982
Validation loss: 2.587164978338598

Epoch: 6| Step: 13
Training loss: 3.088870429327654
Validation loss: 2.5891943730734877

Epoch: 197| Step: 0
Training loss: 3.119992936933299
Validation loss: 2.585179248638429

Epoch: 6| Step: 1
Training loss: 2.7979795523190143
Validation loss: 2.5959270388155464

Epoch: 6| Step: 2
Training loss: 2.9435605775831024
Validation loss: 2.6061136704208536

Epoch: 6| Step: 3
Training loss: 3.2531092516016806
Validation loss: 2.6291389449504416

Epoch: 6| Step: 4
Training loss: 2.7935442138253634
Validation loss: 2.626176108690123

Epoch: 6| Step: 5
Training loss: 2.380463690106054
Validation loss: 2.6129938077246586

Epoch: 6| Step: 6
Training loss: 3.0849049832690696
Validation loss: 2.6155190892630884

Epoch: 6| Step: 7
Training loss: 3.5676013494262127
Validation loss: 2.6216019710096403

Epoch: 6| Step: 8
Training loss: 3.029925499431379
Validation loss: 2.6097678672956643

Epoch: 6| Step: 9
Training loss: 2.296998105026347
Validation loss: 2.6026647109419887

Epoch: 6| Step: 10
Training loss: 2.8868475899444985
Validation loss: 2.5958731035076266

Epoch: 6| Step: 11
Training loss: 2.9721276594035295
Validation loss: 2.59489379325631

Epoch: 6| Step: 12
Training loss: 2.676299508575382
Validation loss: 2.596884205604432

Epoch: 6| Step: 13
Training loss: 3.0080713730420263
Validation loss: 2.59709444671805

Epoch: 198| Step: 0
Training loss: 3.1371419390742328
Validation loss: 2.593262271859149

Epoch: 6| Step: 1
Training loss: 2.9173110976595114
Validation loss: 2.584188203419092

Epoch: 6| Step: 2
Training loss: 2.690335219813825
Validation loss: 2.5874717846798854

Epoch: 6| Step: 3
Training loss: 2.757136880824391
Validation loss: 2.5824611817770915

Epoch: 6| Step: 4
Training loss: 3.0542986297895105
Validation loss: 2.575923926995304

Epoch: 6| Step: 5
Training loss: 2.567225495310923
Validation loss: 2.5794908343067333

Epoch: 6| Step: 6
Training loss: 3.140204558057145
Validation loss: 2.5730496463900105

Epoch: 6| Step: 7
Training loss: 3.4883478207446075
Validation loss: 2.5747504111262827

Epoch: 6| Step: 8
Training loss: 2.0676495393966876
Validation loss: 2.569604125673853

Epoch: 6| Step: 9
Training loss: 2.8911660177829313
Validation loss: 2.5733842923390906

Epoch: 6| Step: 10
Training loss: 3.195569165654292
Validation loss: 2.573849108568846

Epoch: 6| Step: 11
Training loss: 3.1016587403401297
Validation loss: 2.576000356157397

Epoch: 6| Step: 12
Training loss: 2.569895980154823
Validation loss: 2.5768999099431786

Epoch: 6| Step: 13
Training loss: 3.2101798915665554
Validation loss: 2.5741250728200153

Epoch: 199| Step: 0
Training loss: 2.8688823461650874
Validation loss: 2.59314484992409

Epoch: 6| Step: 1
Training loss: 2.930405836413937
Validation loss: 2.5897578347053978

Epoch: 6| Step: 2
Training loss: 2.889597117183257
Validation loss: 2.5830933607929216

Epoch: 6| Step: 3
Training loss: 3.39943580434192
Validation loss: 2.581891856170379

Epoch: 6| Step: 4
Training loss: 2.9519723731818375
Validation loss: 2.5842170431069102

Epoch: 6| Step: 5
Training loss: 3.258514401867473
Validation loss: 2.5981905450400347

Epoch: 6| Step: 6
Training loss: 2.767128918036824
Validation loss: 2.5914223910726406

Epoch: 6| Step: 7
Training loss: 2.9776200946254847
Validation loss: 2.5984873439326677

Epoch: 6| Step: 8
Training loss: 2.7834541073786903
Validation loss: 2.5905103361245527

Epoch: 6| Step: 9
Training loss: 3.0733013020112976
Validation loss: 2.591496592391218

Epoch: 6| Step: 10
Training loss: 2.2888870953220555
Validation loss: 2.5849763859774884

Epoch: 6| Step: 11
Training loss: 2.772100335802987
Validation loss: 2.6030196159648296

Epoch: 6| Step: 12
Training loss: 2.7363085204898625
Validation loss: 2.598026123656705

Epoch: 6| Step: 13
Training loss: 3.188391018664874
Validation loss: 2.6005306476996934

Epoch: 200| Step: 0
Training loss: 2.6461344469993735
Validation loss: 2.591434346980537

Epoch: 6| Step: 1
Training loss: 3.0108335705702585
Validation loss: 2.5925779149311756

Epoch: 6| Step: 2
Training loss: 3.0118849890282644
Validation loss: 2.5831667980046813

Epoch: 6| Step: 3
Training loss: 2.7934127774773163
Validation loss: 2.589590887055831

Epoch: 6| Step: 4
Training loss: 2.7767815859285996
Validation loss: 2.5887186726037625

Epoch: 6| Step: 5
Training loss: 2.9285359064798904
Validation loss: 2.586274851739664

Epoch: 6| Step: 6
Training loss: 3.1725700631607907
Validation loss: 2.5986999571361693

Epoch: 6| Step: 7
Training loss: 2.676673016601986
Validation loss: 2.5909968557129224

Epoch: 6| Step: 8
Training loss: 2.916725230764349
Validation loss: 2.590661075779784

Epoch: 6| Step: 9
Training loss: 3.2639260869352666
Validation loss: 2.588348304860487

Epoch: 6| Step: 10
Training loss: 3.604247310963673
Validation loss: 2.595053531917234

Epoch: 6| Step: 11
Training loss: 2.2561077294972347
Validation loss: 2.5828456375309696

Epoch: 6| Step: 12
Training loss: 2.830610500306137
Validation loss: 2.5910088101213717

Epoch: 6| Step: 13
Training loss: 2.762153040988946
Validation loss: 2.584561242372879

Epoch: 201| Step: 0
Training loss: 2.9907503586089446
Validation loss: 2.579554525731135

Epoch: 6| Step: 1
Training loss: 3.2079708010136643
Validation loss: 2.575777564092675

Epoch: 6| Step: 2
Training loss: 2.8976431115316186
Validation loss: 2.5802249205768426

Epoch: 6| Step: 3
Training loss: 3.5383623453736623
Validation loss: 2.5763612363958592

Epoch: 6| Step: 4
Training loss: 2.657924438487526
Validation loss: 2.579552588754256

Epoch: 6| Step: 5
Training loss: 3.172891237921373
Validation loss: 2.5805811801664875

Epoch: 6| Step: 6
Training loss: 2.8944862763775863
Validation loss: 2.5785137322217517

Epoch: 6| Step: 7
Training loss: 2.7187487722810624
Validation loss: 2.5945826653150443

Epoch: 6| Step: 8
Training loss: 2.352016772625603
Validation loss: 2.5764183140452612

Epoch: 6| Step: 9
Training loss: 2.6769027253219684
Validation loss: 2.578592547757715

Epoch: 6| Step: 10
Training loss: 2.6380875714902023
Validation loss: 2.57762755625586

Epoch: 6| Step: 11
Training loss: 3.226379537128659
Validation loss: 2.5746302062273108

Epoch: 6| Step: 12
Training loss: 2.587522821740389
Validation loss: 2.582756971719899

Epoch: 6| Step: 13
Training loss: 3.2912554101722025
Validation loss: 2.5766730751086637

Epoch: 202| Step: 0
Training loss: 2.6936523632224785
Validation loss: 2.5766846989915213

Epoch: 6| Step: 1
Training loss: 2.6146424630053313
Validation loss: 2.581710353906875

Epoch: 6| Step: 2
Training loss: 2.9451446080608905
Validation loss: 2.5852679865645047

Epoch: 6| Step: 3
Training loss: 2.718623103545631
Validation loss: 2.579622482983371

Epoch: 6| Step: 4
Training loss: 3.2160673350691416
Validation loss: 2.5856270592140915

Epoch: 6| Step: 5
Training loss: 3.157712691899008
Validation loss: 2.578283615925817

Epoch: 6| Step: 6
Training loss: 2.9852616356160566
Validation loss: 2.5838840134533694

Epoch: 6| Step: 7
Training loss: 3.198116648299491
Validation loss: 2.586120794769744

Epoch: 6| Step: 8
Training loss: 3.1484477987191983
Validation loss: 2.5835366537906563

Epoch: 6| Step: 9
Training loss: 2.423310082318381
Validation loss: 2.5921148272416996

Epoch: 6| Step: 10
Training loss: 3.2391462286705193
Validation loss: 2.5927781143487727

Epoch: 6| Step: 11
Training loss: 2.618010707463224
Validation loss: 2.6010386710612288

Epoch: 6| Step: 12
Training loss: 3.1584673354039103
Validation loss: 2.59931850944101

Epoch: 6| Step: 13
Training loss: 2.200305297649067
Validation loss: 2.599193934113886

Epoch: 203| Step: 0
Training loss: 3.4592735820125533
Validation loss: 2.5956873225861448

Epoch: 6| Step: 1
Training loss: 2.9657027818229365
Validation loss: 2.591685720391556

Epoch: 6| Step: 2
Training loss: 3.031408404114599
Validation loss: 2.591608864013987

Epoch: 6| Step: 3
Training loss: 1.8463348760704588
Validation loss: 2.5869773620678322

Epoch: 6| Step: 4
Training loss: 2.889857834747881
Validation loss: 2.6002135756624716

Epoch: 6| Step: 5
Training loss: 3.287016533522518
Validation loss: 2.591841733925891

Epoch: 6| Step: 6
Training loss: 2.4829504379260707
Validation loss: 2.5977623184387957

Epoch: 6| Step: 7
Training loss: 2.722275388505549
Validation loss: 2.5892506584171846

Epoch: 6| Step: 8
Training loss: 2.959508384913125
Validation loss: 2.588547739011203

Epoch: 6| Step: 9
Training loss: 2.7651265875206197
Validation loss: 2.585458102660249

Epoch: 6| Step: 10
Training loss: 3.3065318919474858
Validation loss: 2.579678832081358

Epoch: 6| Step: 11
Training loss: 2.994501479090696
Validation loss: 2.579239288197753

Epoch: 6| Step: 12
Training loss: 3.1298817460509105
Validation loss: 2.586237292076344

Epoch: 6| Step: 13
Training loss: 2.4223630351900955
Validation loss: 2.5883176986520806

Epoch: 204| Step: 0
Training loss: 2.3995495929875137
Validation loss: 2.580295384772991

Epoch: 6| Step: 1
Training loss: 3.327471315187188
Validation loss: 2.5901445459018198

Epoch: 6| Step: 2
Training loss: 3.2409951097074274
Validation loss: 2.585915600375034

Epoch: 6| Step: 3
Training loss: 2.284676160673361
Validation loss: 2.589310516751057

Epoch: 6| Step: 4
Training loss: 3.08201670673454
Validation loss: 2.5793909051900323

Epoch: 6| Step: 5
Training loss: 3.1158517829598416
Validation loss: 2.5774351726595244

Epoch: 6| Step: 6
Training loss: 2.9129808479200685
Validation loss: 2.5867892019771612

Epoch: 6| Step: 7
Training loss: 3.032985701570348
Validation loss: 2.5835186097640537

Epoch: 6| Step: 8
Training loss: 2.8733880873181397
Validation loss: 2.58273580496695

Epoch: 6| Step: 9
Training loss: 3.1890264110027755
Validation loss: 2.5827678302281076

Epoch: 6| Step: 10
Training loss: 2.642709665344488
Validation loss: 2.57894223642044

Epoch: 6| Step: 11
Training loss: 2.6218861776302353
Validation loss: 2.577416592568669

Epoch: 6| Step: 12
Training loss: 3.353795870233723
Validation loss: 2.585870355102964

Epoch: 6| Step: 13
Training loss: 2.155069110762313
Validation loss: 2.5889811555069695

Epoch: 205| Step: 0
Training loss: 3.170074703648005
Validation loss: 2.5936414327977553

Epoch: 6| Step: 1
Training loss: 3.1028218439157227
Validation loss: 2.6077563471460667

Epoch: 6| Step: 2
Training loss: 3.0777397108267777
Validation loss: 2.6049194711376686

Epoch: 6| Step: 3
Training loss: 3.0561885024048245
Validation loss: 2.614412867367956

Epoch: 6| Step: 4
Training loss: 3.340978882390328
Validation loss: 2.6110047989515164

Epoch: 6| Step: 5
Training loss: 2.6871558567867497
Validation loss: 2.628565547036618

Epoch: 6| Step: 6
Training loss: 1.9136618058546524
Validation loss: 2.6410620513929355

Epoch: 6| Step: 7
Training loss: 3.406953511513513
Validation loss: 2.63628278925392

Epoch: 6| Step: 8
Training loss: 2.439280128911447
Validation loss: 2.623808452150889

Epoch: 6| Step: 9
Training loss: 3.2589116792185053
Validation loss: 2.608476121069806

Epoch: 6| Step: 10
Training loss: 2.2786577729411897
Validation loss: 2.5804848588485365

Epoch: 6| Step: 11
Training loss: 3.1088478753389515
Validation loss: 2.5723220545301486

Epoch: 6| Step: 12
Training loss: 3.2370008046947913
Validation loss: 2.572973301452553

Epoch: 6| Step: 13
Training loss: 1.9328526781820232
Validation loss: 2.5756386519745593

Epoch: 206| Step: 0
Training loss: 2.3661809109346406
Validation loss: 2.5748050935716216

Epoch: 6| Step: 1
Training loss: 2.8405817849005834
Validation loss: 2.5767693120752426

Epoch: 6| Step: 2
Training loss: 3.241766991979165
Validation loss: 2.570203419560773

Epoch: 6| Step: 3
Training loss: 2.76919618201868
Validation loss: 2.571530774974291

Epoch: 6| Step: 4
Training loss: 3.213225132187608
Validation loss: 2.5715322454481786

Epoch: 6| Step: 5
Training loss: 2.602376174839274
Validation loss: 2.5705051553762117

Epoch: 6| Step: 6
Training loss: 2.783192502720958
Validation loss: 2.5734095711370575

Epoch: 6| Step: 7
Training loss: 2.985097747684726
Validation loss: 2.573806350475933

Epoch: 6| Step: 8
Training loss: 2.6493591685360913
Validation loss: 2.5735800149826313

Epoch: 6| Step: 9
Training loss: 3.186628035413607
Validation loss: 2.5705182821936456

Epoch: 6| Step: 10
Training loss: 3.2616161216077293
Validation loss: 2.5739324852054373

Epoch: 6| Step: 11
Training loss: 3.0450073778285454
Validation loss: 2.578813312685745

Epoch: 6| Step: 12
Training loss: 2.598226705454983
Validation loss: 2.577746858580513

Epoch: 6| Step: 13
Training loss: 3.3752456858184754
Validation loss: 2.583086794608261

Epoch: 207| Step: 0
Training loss: 2.862135924794969
Validation loss: 2.581801021238518

Epoch: 6| Step: 1
Training loss: 2.778434614300798
Validation loss: 2.594482591212782

Epoch: 6| Step: 2
Training loss: 2.8031940362396552
Validation loss: 2.6039539301127697

Epoch: 6| Step: 3
Training loss: 3.0148744587991283
Validation loss: 2.6017298579464465

Epoch: 6| Step: 4
Training loss: 3.3691462862271275
Validation loss: 2.6069262050151054

Epoch: 6| Step: 5
Training loss: 2.376045097494652
Validation loss: 2.615944504166865

Epoch: 6| Step: 6
Training loss: 3.2572953433479728
Validation loss: 2.6098755357707217

Epoch: 6| Step: 7
Training loss: 3.258543815221046
Validation loss: 2.5968178393385695

Epoch: 6| Step: 8
Training loss: 2.2895282362700256
Validation loss: 2.597350842106819

Epoch: 6| Step: 9
Training loss: 2.8926374698503823
Validation loss: 2.617713052886005

Epoch: 6| Step: 10
Training loss: 2.6603965258651208
Validation loss: 2.585346518848649

Epoch: 6| Step: 11
Training loss: 2.6658440950428264
Validation loss: 2.580673075276323

Epoch: 6| Step: 12
Training loss: 2.9557333432250688
Validation loss: 2.574136545873371

Epoch: 6| Step: 13
Training loss: 3.7204187319543296
Validation loss: 2.5648608953363743

Epoch: 208| Step: 0
Training loss: 3.026604622472431
Validation loss: 2.5678569984709423

Epoch: 6| Step: 1
Training loss: 2.2775075863356964
Validation loss: 2.5714013749641085

Epoch: 6| Step: 2
Training loss: 2.782875357618893
Validation loss: 2.5658902127559506

Epoch: 6| Step: 3
Training loss: 2.845984618455681
Validation loss: 2.573756640112629

Epoch: 6| Step: 4
Training loss: 2.978126255287314
Validation loss: 2.5749752021182504

Epoch: 6| Step: 5
Training loss: 2.7945595645723187
Validation loss: 2.569193334336456

Epoch: 6| Step: 6
Training loss: 2.258868858026342
Validation loss: 2.571895615141674

Epoch: 6| Step: 7
Training loss: 3.3516037589425762
Validation loss: 2.5635228100426946

Epoch: 6| Step: 8
Training loss: 3.28574056792558
Validation loss: 2.57461192054741

Epoch: 6| Step: 9
Training loss: 3.0034973104282945
Validation loss: 2.5659348382090195

Epoch: 6| Step: 10
Training loss: 3.2561598869638844
Validation loss: 2.5749889572660236

Epoch: 6| Step: 11
Training loss: 2.239095543073851
Validation loss: 2.571414998685719

Epoch: 6| Step: 12
Training loss: 3.379958713211618
Validation loss: 2.582606897885023

Epoch: 6| Step: 13
Training loss: 3.1524762547662806
Validation loss: 2.5862857990630848

Epoch: 209| Step: 0
Training loss: 2.882109434825736
Validation loss: 2.5954962262559116

Epoch: 6| Step: 1
Training loss: 2.7673769644221062
Validation loss: 2.6018204148995308

Epoch: 6| Step: 2
Training loss: 3.2343811919664462
Validation loss: 2.6108023708892674

Epoch: 6| Step: 3
Training loss: 2.7891899528553323
Validation loss: 2.5981115968013495

Epoch: 6| Step: 4
Training loss: 2.41996621005846
Validation loss: 2.5948871117019494

Epoch: 6| Step: 5
Training loss: 2.4609844869336293
Validation loss: 2.6043688223524337

Epoch: 6| Step: 6
Training loss: 2.8509564317406877
Validation loss: 2.593584757410422

Epoch: 6| Step: 7
Training loss: 3.0853063783031414
Validation loss: 2.597595544747697

Epoch: 6| Step: 8
Training loss: 3.039760636894665
Validation loss: 2.6025778546091782

Epoch: 6| Step: 9
Training loss: 3.3586904848573482
Validation loss: 2.599351844307021

Epoch: 6| Step: 10
Training loss: 2.383334043364875
Validation loss: 2.5915221970487603

Epoch: 6| Step: 11
Training loss: 3.4911212018624833
Validation loss: 2.596206047803101

Epoch: 6| Step: 12
Training loss: 2.662599491893164
Validation loss: 2.593576779581537

Epoch: 6| Step: 13
Training loss: 3.2472093778961164
Validation loss: 2.583072612148218

Epoch: 210| Step: 0
Training loss: 3.0179178484940223
Validation loss: 2.587339711011225

Epoch: 6| Step: 1
Training loss: 2.8038803256694584
Validation loss: 2.5856490911678707

Epoch: 6| Step: 2
Training loss: 3.2794286123218828
Validation loss: 2.575760958653281

Epoch: 6| Step: 3
Training loss: 2.920327659147135
Validation loss: 2.570347594426686

Epoch: 6| Step: 4
Training loss: 2.3709176766444355
Validation loss: 2.575890226245775

Epoch: 6| Step: 5
Training loss: 2.7463621873821062
Validation loss: 2.5852244693148

Epoch: 6| Step: 6
Training loss: 3.027654343946154
Validation loss: 2.599641805780129

Epoch: 6| Step: 7
Training loss: 2.629330469057691
Validation loss: 2.5916554859644156

Epoch: 6| Step: 8
Training loss: 3.3382920257658695
Validation loss: 2.6108132222406004

Epoch: 6| Step: 9
Training loss: 3.1034890444405314
Validation loss: 2.595463981721108

Epoch: 6| Step: 10
Training loss: 2.633296571144998
Validation loss: 2.589306112843851

Epoch: 6| Step: 11
Training loss: 2.7126016483510287
Validation loss: 2.586685773802492

Epoch: 6| Step: 12
Training loss: 3.1800793305584407
Validation loss: 2.5910604038131653

Epoch: 6| Step: 13
Training loss: 2.830563163393541
Validation loss: 2.5924623646097165

Epoch: 211| Step: 0
Training loss: 3.087094633233023
Validation loss: 2.5933755900177995

Epoch: 6| Step: 1
Training loss: 2.923361007692828
Validation loss: 2.587607373682934

Epoch: 6| Step: 2
Training loss: 2.613568801911329
Validation loss: 2.5969702543381255

Epoch: 6| Step: 3
Training loss: 2.5964536249260455
Validation loss: 2.5885362307936974

Epoch: 6| Step: 4
Training loss: 3.3557120291610616
Validation loss: 2.576354162486331

Epoch: 6| Step: 5
Training loss: 2.0747073185418428
Validation loss: 2.585770023133477

Epoch: 6| Step: 6
Training loss: 2.9405972000089022
Validation loss: 2.591060069390012

Epoch: 6| Step: 7
Training loss: 3.3742725859982237
Validation loss: 2.5987985371007385

Epoch: 6| Step: 8
Training loss: 3.108563187787609
Validation loss: 2.5928222780189216

Epoch: 6| Step: 9
Training loss: 2.5316879458743813
Validation loss: 2.5939410411478967

Epoch: 6| Step: 10
Training loss: 3.5207097560405765
Validation loss: 2.5960735830539514

Epoch: 6| Step: 11
Training loss: 3.3000468684249427
Validation loss: 2.59062987939582

Epoch: 6| Step: 12
Training loss: 2.34365478322212
Validation loss: 2.5810916292991295

Epoch: 6| Step: 13
Training loss: 2.140920020051445
Validation loss: 2.578008313613208

Epoch: 212| Step: 0
Training loss: 2.56711534896755
Validation loss: 2.5777547909211846

Epoch: 6| Step: 1
Training loss: 3.2421867646365885
Validation loss: 2.5767640321090743

Epoch: 6| Step: 2
Training loss: 3.5359492808713378
Validation loss: 2.584178068629352

Epoch: 6| Step: 3
Training loss: 3.5959354555577203
Validation loss: 2.5733065167617974

Epoch: 6| Step: 4
Training loss: 2.418058674115424
Validation loss: 2.570893082295274

Epoch: 6| Step: 5
Training loss: 2.7283195365070663
Validation loss: 2.569048310348141

Epoch: 6| Step: 6
Training loss: 3.091733641468818
Validation loss: 2.570635196873032

Epoch: 6| Step: 7
Training loss: 1.9340520142323225
Validation loss: 2.57332974009598

Epoch: 6| Step: 8
Training loss: 3.2275104007804525
Validation loss: 2.575560048638617

Epoch: 6| Step: 9
Training loss: 3.225185146487869
Validation loss: 2.5782980106656535

Epoch: 6| Step: 10
Training loss: 2.3842057954238896
Validation loss: 2.5761593064052857

Epoch: 6| Step: 11
Training loss: 2.9555494256621286
Validation loss: 2.590134571006745

Epoch: 6| Step: 12
Training loss: 2.335920965333935
Validation loss: 2.5873603947212653

Epoch: 6| Step: 13
Training loss: 2.922983866920804
Validation loss: 2.5946654814229158

Epoch: 213| Step: 0
Training loss: 3.2484721480554355
Validation loss: 2.610661736202519

Epoch: 6| Step: 1
Training loss: 2.9526118079841552
Validation loss: 2.608308969925324

Epoch: 6| Step: 2
Training loss: 2.8182071002262457
Validation loss: 2.6013343802419007

Epoch: 6| Step: 3
Training loss: 2.8984980409782386
Validation loss: 2.60056508788453

Epoch: 6| Step: 4
Training loss: 2.08043570870073
Validation loss: 2.6103428433527784

Epoch: 6| Step: 5
Training loss: 2.8871594248807484
Validation loss: 2.607763749258151

Epoch: 6| Step: 6
Training loss: 3.544646382201406
Validation loss: 2.628907129606668

Epoch: 6| Step: 7
Training loss: 3.0277513586480187
Validation loss: 2.6254488975757906

Epoch: 6| Step: 8
Training loss: 2.8657096359417324
Validation loss: 2.627461078105311

Epoch: 6| Step: 9
Training loss: 2.7722177321559682
Validation loss: 2.599634754787426

Epoch: 6| Step: 10
Training loss: 3.1066941248890463
Validation loss: 2.598272027613228

Epoch: 6| Step: 11
Training loss: 2.894509998814954
Validation loss: 2.590837619948288

Epoch: 6| Step: 12
Training loss: 2.575987883758483
Validation loss: 2.580092284175438

Epoch: 6| Step: 13
Training loss: 2.7383696233009047
Validation loss: 2.575435516614249

Epoch: 214| Step: 0
Training loss: 2.605211328146056
Validation loss: 2.5731253693551186

Epoch: 6| Step: 1
Training loss: 2.529479832457424
Validation loss: 2.567115462813243

Epoch: 6| Step: 2
Training loss: 2.6972349525602595
Validation loss: 2.5666511888127905

Epoch: 6| Step: 3
Training loss: 2.3598187768182175
Validation loss: 2.567033990947818

Epoch: 6| Step: 4
Training loss: 3.3510717641806727
Validation loss: 2.5596332596691793

Epoch: 6| Step: 5
Training loss: 3.056068674024922
Validation loss: 2.565270604581142

Epoch: 6| Step: 6
Training loss: 2.955239805247377
Validation loss: 2.569208206081971

Epoch: 6| Step: 7
Training loss: 2.467420292333135
Validation loss: 2.566502650192554

Epoch: 6| Step: 8
Training loss: 3.5265752857197885
Validation loss: 2.575776569799719

Epoch: 6| Step: 9
Training loss: 2.891006691966168
Validation loss: 2.5782539482099556

Epoch: 6| Step: 10
Training loss: 3.0482082482132182
Validation loss: 2.568723561795313

Epoch: 6| Step: 11
Training loss: 2.7607406330138002
Validation loss: 2.56594643180769

Epoch: 6| Step: 12
Training loss: 3.0044716252120107
Validation loss: 2.580118820744993

Epoch: 6| Step: 13
Training loss: 3.344232827381394
Validation loss: 2.5845856094097064

Epoch: 215| Step: 0
Training loss: 3.2015385862828527
Validation loss: 2.5920982354717315

Epoch: 6| Step: 1
Training loss: 3.122924878646372
Validation loss: 2.5829040224868915

Epoch: 6| Step: 2
Training loss: 3.456597976688746
Validation loss: 2.584327560683587

Epoch: 6| Step: 3
Training loss: 2.685594104698331
Validation loss: 2.579502606992818

Epoch: 6| Step: 4
Training loss: 2.680006892351995
Validation loss: 2.575803998807703

Epoch: 6| Step: 5
Training loss: 2.7822133656781616
Validation loss: 2.5755415665029977

Epoch: 6| Step: 6
Training loss: 2.935071875034936
Validation loss: 2.5766807192378227

Epoch: 6| Step: 7
Training loss: 2.4872713300177804
Validation loss: 2.582012242283896

Epoch: 6| Step: 8
Training loss: 3.22822435637878
Validation loss: 2.578206102494528

Epoch: 6| Step: 9
Training loss: 3.1469564054201236
Validation loss: 2.5800185299176372

Epoch: 6| Step: 10
Training loss: 2.4523631085096276
Validation loss: 2.577536927115804

Epoch: 6| Step: 11
Training loss: 2.7954161691392754
Validation loss: 2.570668981473513

Epoch: 6| Step: 12
Training loss: 2.7910082785614474
Validation loss: 2.5772227468874704

Epoch: 6| Step: 13
Training loss: 2.555509475854641
Validation loss: 2.5852452948952376

Epoch: 216| Step: 0
Training loss: 3.2658199270378336
Validation loss: 2.5850712160061766

Epoch: 6| Step: 1
Training loss: 2.647541626052767
Validation loss: 2.5828786103099355

Epoch: 6| Step: 2
Training loss: 2.6792842461477044
Validation loss: 2.6167434709980553

Epoch: 6| Step: 3
Training loss: 2.2049330198187898
Validation loss: 2.61317226891999

Epoch: 6| Step: 4
Training loss: 2.837711504477085
Validation loss: 2.607826064486237

Epoch: 6| Step: 5
Training loss: 3.1368333693449726
Validation loss: 2.589287228828592

Epoch: 6| Step: 6
Training loss: 3.0730401655477633
Validation loss: 2.585774024589719

Epoch: 6| Step: 7
Training loss: 3.1163867505147977
Validation loss: 2.5801986871476346

Epoch: 6| Step: 8
Training loss: 3.3222789730496847
Validation loss: 2.5731873313646876

Epoch: 6| Step: 9
Training loss: 2.219418921845755
Validation loss: 2.5665901909685527

Epoch: 6| Step: 10
Training loss: 3.063806371960095
Validation loss: 2.569709689938911

Epoch: 6| Step: 11
Training loss: 3.0952232783176012
Validation loss: 2.568695124021117

Epoch: 6| Step: 12
Training loss: 2.7108092703710436
Validation loss: 2.5618680989722225

Epoch: 6| Step: 13
Training loss: 2.982975656674145
Validation loss: 2.56529974888546

Epoch: 217| Step: 0
Training loss: 2.75820185260557
Validation loss: 2.563257444112227

Epoch: 6| Step: 1
Training loss: 3.2112282577259705
Validation loss: 2.5741426030672003

Epoch: 6| Step: 2
Training loss: 3.0540255636707903
Validation loss: 2.565901151134262

Epoch: 6| Step: 3
Training loss: 3.2501913161087295
Validation loss: 2.579574324751559

Epoch: 6| Step: 4
Training loss: 2.5814475180215384
Validation loss: 2.585004880673674

Epoch: 6| Step: 5
Training loss: 2.9432414334366195
Validation loss: 2.58037328053797

Epoch: 6| Step: 6
Training loss: 3.279619229475312
Validation loss: 2.5979348917053433

Epoch: 6| Step: 7
Training loss: 3.039360600538775
Validation loss: 2.6039743587900617

Epoch: 6| Step: 8
Training loss: 2.832309724994055
Validation loss: 2.605975392604964

Epoch: 6| Step: 9
Training loss: 2.4279904832732013
Validation loss: 2.5979115419526164

Epoch: 6| Step: 10
Training loss: 2.793415337984719
Validation loss: 2.593572132838276

Epoch: 6| Step: 11
Training loss: 2.6631271196852446
Validation loss: 2.5898809376333984

Epoch: 6| Step: 12
Training loss: 2.8068613962350475
Validation loss: 2.587223149887936

Epoch: 6| Step: 13
Training loss: 2.8380905679522304
Validation loss: 2.5853997872810126

Epoch: 218| Step: 0
Training loss: 3.39608578045886
Validation loss: 2.5833396640293804

Epoch: 6| Step: 1
Training loss: 2.627782527640604
Validation loss: 2.5886291765100164

Epoch: 6| Step: 2
Training loss: 3.0426736671451917
Validation loss: 2.5799716017898606

Epoch: 6| Step: 3
Training loss: 2.6671188587011283
Validation loss: 2.585190898732843

Epoch: 6| Step: 4
Training loss: 2.7165185825576645
Validation loss: 2.576119775138057

Epoch: 6| Step: 5
Training loss: 3.036804458668755
Validation loss: 2.576477723747487

Epoch: 6| Step: 6
Training loss: 2.8150527389491633
Validation loss: 2.578003346446406

Epoch: 6| Step: 7
Training loss: 2.9319229752440075
Validation loss: 2.5698476496268996

Epoch: 6| Step: 8
Training loss: 2.9350429566861154
Validation loss: 2.5710451007334165

Epoch: 6| Step: 9
Training loss: 3.1098997353830784
Validation loss: 2.5742324076094825

Epoch: 6| Step: 10
Training loss: 2.2275255763922095
Validation loss: 2.569140783619695

Epoch: 6| Step: 11
Training loss: 2.85331007826173
Validation loss: 2.578502202088501

Epoch: 6| Step: 12
Training loss: 2.826640714340312
Validation loss: 2.574330725520261

Epoch: 6| Step: 13
Training loss: 3.4605313099322603
Validation loss: 2.5733739765257595

Epoch: 219| Step: 0
Training loss: 3.1039255206424192
Validation loss: 2.5872141328186364

Epoch: 6| Step: 1
Training loss: 2.9353049985078443
Validation loss: 2.5829188559839737

Epoch: 6| Step: 2
Training loss: 2.766438682385431
Validation loss: 2.6049561758683075

Epoch: 6| Step: 3
Training loss: 2.479878416926531
Validation loss: 2.5972049602082374

Epoch: 6| Step: 4
Training loss: 2.4958384208648576
Validation loss: 2.608164475493794

Epoch: 6| Step: 5
Training loss: 2.780934005102398
Validation loss: 2.600414611795194

Epoch: 6| Step: 6
Training loss: 2.901199382190857
Validation loss: 2.598962978173359

Epoch: 6| Step: 7
Training loss: 3.5451730013924916
Validation loss: 2.592210096818439

Epoch: 6| Step: 8
Training loss: 2.9179442377860996
Validation loss: 2.6004595645787965

Epoch: 6| Step: 9
Training loss: 2.623518934684286
Validation loss: 2.606342381062545

Epoch: 6| Step: 10
Training loss: 2.7176202641101908
Validation loss: 2.5949321612280274

Epoch: 6| Step: 11
Training loss: 2.755740849185324
Validation loss: 2.5986641387241907

Epoch: 6| Step: 12
Training loss: 3.4228734340866835
Validation loss: 2.5995940874439665

Epoch: 6| Step: 13
Training loss: 2.936950469864782
Validation loss: 2.5742998580482808

Epoch: 220| Step: 0
Training loss: 2.8150619705994804
Validation loss: 2.574325021299866

Epoch: 6| Step: 1
Training loss: 3.107607238343149
Validation loss: 2.5882113986694204

Epoch: 6| Step: 2
Training loss: 2.9082537890671674
Validation loss: 2.5814130700633933

Epoch: 6| Step: 3
Training loss: 3.1913708389540183
Validation loss: 2.580414073958533

Epoch: 6| Step: 4
Training loss: 3.0356316114435735
Validation loss: 2.5691804482146723

Epoch: 6| Step: 5
Training loss: 2.6416889281994544
Validation loss: 2.589443362120764

Epoch: 6| Step: 6
Training loss: 2.091181531880909
Validation loss: 2.5809849482693856

Epoch: 6| Step: 7
Training loss: 3.2898206403360435
Validation loss: 2.5790504035215687

Epoch: 6| Step: 8
Training loss: 3.1863495489636966
Validation loss: 2.590444955549331

Epoch: 6| Step: 9
Training loss: 2.409728867090782
Validation loss: 2.582127753900171

Epoch: 6| Step: 10
Training loss: 2.877976245328783
Validation loss: 2.581643246035685

Epoch: 6| Step: 11
Training loss: 2.619747902193057
Validation loss: 2.584751971828919

Epoch: 6| Step: 12
Training loss: 3.051941400727832
Validation loss: 2.5783181633652714

Epoch: 6| Step: 13
Training loss: 3.1861728074311157
Validation loss: 2.5692281846158678

Epoch: 221| Step: 0
Training loss: 3.0732451354946937
Validation loss: 2.574362175185825

Epoch: 6| Step: 1
Training loss: 2.812791427772401
Validation loss: 2.5744880665975236

Epoch: 6| Step: 2
Training loss: 3.1211456943815743
Validation loss: 2.5907102044987065

Epoch: 6| Step: 3
Training loss: 2.770782661452687
Validation loss: 2.5786144916803364

Epoch: 6| Step: 4
Training loss: 2.7106495465202234
Validation loss: 2.581231075281948

Epoch: 6| Step: 5
Training loss: 2.553904375441439
Validation loss: 2.575049474662827

Epoch: 6| Step: 6
Training loss: 3.303941286216526
Validation loss: 2.5731408918792456

Epoch: 6| Step: 7
Training loss: 2.6379805646849945
Validation loss: 2.5715196043299446

Epoch: 6| Step: 8
Training loss: 2.940791943601961
Validation loss: 2.5727207195079576

Epoch: 6| Step: 9
Training loss: 3.0108335705702585
Validation loss: 2.5755912513744565

Epoch: 6| Step: 10
Training loss: 3.064194191179757
Validation loss: 2.5823404145254814

Epoch: 6| Step: 11
Training loss: 2.775130150294616
Validation loss: 2.588857163121568

Epoch: 6| Step: 12
Training loss: 2.757882525281741
Validation loss: 2.5843781737690383

Epoch: 6| Step: 13
Training loss: 2.9314610517121116
Validation loss: 2.5741534346725885

Epoch: 222| Step: 0
Training loss: 3.4014274686016335
Validation loss: 2.571341184468217

Epoch: 6| Step: 1
Training loss: 3.538286473495054
Validation loss: 2.5750457671594593

Epoch: 6| Step: 2
Training loss: 2.560272178784038
Validation loss: 2.569611621226888

Epoch: 6| Step: 3
Training loss: 2.532169504696061
Validation loss: 2.5745637094170974

Epoch: 6| Step: 4
Training loss: 2.674255114081611
Validation loss: 2.5717979437850307

Epoch: 6| Step: 5
Training loss: 2.67980253901324
Validation loss: 2.5676438371906243

Epoch: 6| Step: 6
Training loss: 2.582623804749331
Validation loss: 2.5690199140891825

Epoch: 6| Step: 7
Training loss: 2.6212658434795166
Validation loss: 2.5661870071408504

Epoch: 6| Step: 8
Training loss: 3.502854273303067
Validation loss: 2.5616472392531415

Epoch: 6| Step: 9
Training loss: 2.6769231211496174
Validation loss: 2.5659549930931806

Epoch: 6| Step: 10
Training loss: 2.847846130822181
Validation loss: 2.5691175254133776

Epoch: 6| Step: 11
Training loss: 2.973984290085982
Validation loss: 2.5777205830249565

Epoch: 6| Step: 12
Training loss: 3.2311535700976344
Validation loss: 2.581892134191063

Epoch: 6| Step: 13
Training loss: 1.9186343652132678
Validation loss: 2.589114086097071

Epoch: 223| Step: 0
Training loss: 3.224931133629118
Validation loss: 2.5911799731545715

Epoch: 6| Step: 1
Training loss: 1.6053951645668654
Validation loss: 2.5841098660955013

Epoch: 6| Step: 2
Training loss: 2.8313792352885967
Validation loss: 2.5955999394265787

Epoch: 6| Step: 3
Training loss: 2.8892634303860243
Validation loss: 2.6314772625082763

Epoch: 6| Step: 4
Training loss: 3.4184410945944226
Validation loss: 2.6055662159029285

Epoch: 6| Step: 5
Training loss: 2.661001555679251
Validation loss: 2.613260927431999

Epoch: 6| Step: 6
Training loss: 3.377277312374235
Validation loss: 2.5890901221017684

Epoch: 6| Step: 7
Training loss: 2.8463753120402426
Validation loss: 2.5589288517936732

Epoch: 6| Step: 8
Training loss: 2.3275284995066388
Validation loss: 2.555627428253576

Epoch: 6| Step: 9
Training loss: 3.133528706026045
Validation loss: 2.553103145302443

Epoch: 6| Step: 10
Training loss: 2.742034820914287
Validation loss: 2.5581518459637373

Epoch: 6| Step: 11
Training loss: 3.2407785318583113
Validation loss: 2.5618359385343283

Epoch: 6| Step: 12
Training loss: 2.817846514591837
Validation loss: 2.562320992104703

Epoch: 6| Step: 13
Training loss: 3.27947717640131
Validation loss: 2.5655950347792027

Epoch: 224| Step: 0
Training loss: 2.515859652172337
Validation loss: 2.5597777935241663

Epoch: 6| Step: 1
Training loss: 2.7361511565331518
Validation loss: 2.5606795636927466

Epoch: 6| Step: 2
Training loss: 3.4845418163105317
Validation loss: 2.5641359539003536

Epoch: 6| Step: 3
Training loss: 2.8402255508690804
Validation loss: 2.5714596917622434

Epoch: 6| Step: 4
Training loss: 2.7672590180799
Validation loss: 2.57469912486707

Epoch: 6| Step: 5
Training loss: 2.481053465162948
Validation loss: 2.5777601116224758

Epoch: 6| Step: 6
Training loss: 2.8174890984067913
Validation loss: 2.583386860933662

Epoch: 6| Step: 7
Training loss: 2.3418530735020706
Validation loss: 2.590124966249088

Epoch: 6| Step: 8
Training loss: 2.8214222423351414
Validation loss: 2.6020462614644084

Epoch: 6| Step: 9
Training loss: 3.2901652972313515
Validation loss: 2.6080722692212555

Epoch: 6| Step: 10
Training loss: 3.238651709999339
Validation loss: 2.6058857621532985

Epoch: 6| Step: 11
Training loss: 2.7370722083705115
Validation loss: 2.603678994733894

Epoch: 6| Step: 12
Training loss: 3.2183819903016557
Validation loss: 2.611016506621459

Epoch: 6| Step: 13
Training loss: 3.343225598436123
Validation loss: 2.6154328342886286

Epoch: 225| Step: 0
Training loss: 2.410902108843093
Validation loss: 2.621281855512938

Epoch: 6| Step: 1
Training loss: 3.4281251168403153
Validation loss: 2.601841915603748

Epoch: 6| Step: 2
Training loss: 2.56943967893233
Validation loss: 2.593832828856238

Epoch: 6| Step: 3
Training loss: 2.9427911707378684
Validation loss: 2.595841336712426

Epoch: 6| Step: 4
Training loss: 2.40326783310109
Validation loss: 2.6083760207687106

Epoch: 6| Step: 5
Training loss: 2.667163196669765
Validation loss: 2.598714847458842

Epoch: 6| Step: 6
Training loss: 3.2684646744496106
Validation loss: 2.5988982220776746

Epoch: 6| Step: 7
Training loss: 2.8518942444014295
Validation loss: 2.606293587339701

Epoch: 6| Step: 8
Training loss: 3.0505008348839917
Validation loss: 2.6095610914459906

Epoch: 6| Step: 9
Training loss: 2.8553954400750974
Validation loss: 2.625268707058369

Epoch: 6| Step: 10
Training loss: 2.6249099897665484
Validation loss: 2.6231868945579153

Epoch: 6| Step: 11
Training loss: 2.8201517075117337
Validation loss: 2.6199857369621538

Epoch: 6| Step: 12
Training loss: 3.429917312604091
Validation loss: 2.6246541865362416

Epoch: 6| Step: 13
Training loss: 3.060972494535795
Validation loss: 2.6123559302769825

Epoch: 226| Step: 0
Training loss: 2.5485022091675407
Validation loss: 2.597802485506317

Epoch: 6| Step: 1
Training loss: 2.9108718709053694
Validation loss: 2.6035668162576235

Epoch: 6| Step: 2
Training loss: 2.6340111061447145
Validation loss: 2.59736197370932

Epoch: 6| Step: 3
Training loss: 3.3569827056066566
Validation loss: 2.5874042277866693

Epoch: 6| Step: 4
Training loss: 3.5024511745842624
Validation loss: 2.5730668930337894

Epoch: 6| Step: 5
Training loss: 3.0230395624447186
Validation loss: 2.575937752714208

Epoch: 6| Step: 6
Training loss: 2.7957799035626962
Validation loss: 2.5602250073698936

Epoch: 6| Step: 7
Training loss: 2.8793514326756924
Validation loss: 2.5612349350049897

Epoch: 6| Step: 8
Training loss: 2.4054840652359863
Validation loss: 2.562709998833407

Epoch: 6| Step: 9
Training loss: 3.1413284956220715
Validation loss: 2.554584074716254

Epoch: 6| Step: 10
Training loss: 2.1992809767922723
Validation loss: 2.557897716272866

Epoch: 6| Step: 11
Training loss: 2.7532316639704515
Validation loss: 2.5647046009661714

Epoch: 6| Step: 12
Training loss: 3.3167373617743303
Validation loss: 2.5716345953432036

Epoch: 6| Step: 13
Training loss: 2.645285291807848
Validation loss: 2.572447691997432

Epoch: 227| Step: 0
Training loss: 2.618050595207698
Validation loss: 2.571847735866896

Epoch: 6| Step: 1
Training loss: 3.4094964751395853
Validation loss: 2.575735005223955

Epoch: 6| Step: 2
Training loss: 2.426096725221275
Validation loss: 2.572101370348497

Epoch: 6| Step: 3
Training loss: 3.4566167378168733
Validation loss: 2.585362863410089

Epoch: 6| Step: 4
Training loss: 2.8980643554028678
Validation loss: 2.593436275426369

Epoch: 6| Step: 5
Training loss: 2.7585043755213636
Validation loss: 2.576238376075963

Epoch: 6| Step: 6
Training loss: 2.447861388103136
Validation loss: 2.592483255657003

Epoch: 6| Step: 7
Training loss: 3.093792847616007
Validation loss: 2.5786178162597904

Epoch: 6| Step: 8
Training loss: 2.7828434012323564
Validation loss: 2.5871304885895956

Epoch: 6| Step: 9
Training loss: 2.726801383585255
Validation loss: 2.5935343378310236

Epoch: 6| Step: 10
Training loss: 3.240290001500366
Validation loss: 2.590763367596511

Epoch: 6| Step: 11
Training loss: 2.7765050981949835
Validation loss: 2.592456356152751

Epoch: 6| Step: 12
Training loss: 2.57298571124596
Validation loss: 2.596214270340671

Epoch: 6| Step: 13
Training loss: 3.0722929413121283
Validation loss: 2.5916677509067267

Epoch: 228| Step: 0
Training loss: 2.769853366041704
Validation loss: 2.5765889503821118

Epoch: 6| Step: 1
Training loss: 2.051495876232803
Validation loss: 2.572238893793823

Epoch: 6| Step: 2
Training loss: 2.5838007555342415
Validation loss: 2.5629157172581123

Epoch: 6| Step: 3
Training loss: 2.5513649896166757
Validation loss: 2.5689244186882654

Epoch: 6| Step: 4
Training loss: 3.149551535274832
Validation loss: 2.5803343642456302

Epoch: 6| Step: 5
Training loss: 3.020333680672144
Validation loss: 2.581350622222365

Epoch: 6| Step: 6
Training loss: 2.7414308900723223
Validation loss: 2.580068429149288

Epoch: 6| Step: 7
Training loss: 3.0163397856026553
Validation loss: 2.591126483200442

Epoch: 6| Step: 8
Training loss: 2.835217260953573
Validation loss: 2.5980011722030585

Epoch: 6| Step: 9
Training loss: 3.2367930932959856
Validation loss: 2.6158455961974956

Epoch: 6| Step: 10
Training loss: 3.2292234518329135
Validation loss: 2.599411034116727

Epoch: 6| Step: 11
Training loss: 3.0658340989409396
Validation loss: 2.59560097057173

Epoch: 6| Step: 12
Training loss: 3.2086538840009613
Validation loss: 2.593404484713428

Epoch: 6| Step: 13
Training loss: 2.5597709451960964
Validation loss: 2.5883939218109084

Epoch: 229| Step: 0
Training loss: 3.053495598969287
Validation loss: 2.5938007652710415

Epoch: 6| Step: 1
Training loss: 1.950757842864927
Validation loss: 2.580455481700734

Epoch: 6| Step: 2
Training loss: 3.7534029461473306
Validation loss: 2.578541837016246

Epoch: 6| Step: 3
Training loss: 2.80468537481966
Validation loss: 2.579983754346543

Epoch: 6| Step: 4
Training loss: 2.7809916172662543
Validation loss: 2.5774596607759594

Epoch: 6| Step: 5
Training loss: 3.020625262932319
Validation loss: 2.5755511718948587

Epoch: 6| Step: 6
Training loss: 2.860594609211433
Validation loss: 2.587797745879698

Epoch: 6| Step: 7
Training loss: 3.131330867492125
Validation loss: 2.5852168712730643

Epoch: 6| Step: 8
Training loss: 2.792542837253224
Validation loss: 2.5926341663718286

Epoch: 6| Step: 9
Training loss: 3.0477813155253815
Validation loss: 2.5843707299718304

Epoch: 6| Step: 10
Training loss: 2.6386783805896856
Validation loss: 2.5827583906390315

Epoch: 6| Step: 11
Training loss: 2.780180736265189
Validation loss: 2.5958940619134383

Epoch: 6| Step: 12
Training loss: 2.566845349701826
Validation loss: 2.581685747227975

Epoch: 6| Step: 13
Training loss: 2.91913267971955
Validation loss: 2.576516780379108

Epoch: 230| Step: 0
Training loss: 3.073017510952326
Validation loss: 2.563858807466857

Epoch: 6| Step: 1
Training loss: 3.305059173100432
Validation loss: 2.5856767089249653

Epoch: 6| Step: 2
Training loss: 2.6919481592840806
Validation loss: 2.5770602705349446

Epoch: 6| Step: 3
Training loss: 2.9729343840604203
Validation loss: 2.577305717965871

Epoch: 6| Step: 4
Training loss: 2.8769687048754995
Validation loss: 2.576276588017544

Epoch: 6| Step: 5
Training loss: 3.0994995328481623
Validation loss: 2.581750178892725

Epoch: 6| Step: 6
Training loss: 2.8093232757932682
Validation loss: 2.5834872814860503

Epoch: 6| Step: 7
Training loss: 2.280040263439502
Validation loss: 2.579595503035572

Epoch: 6| Step: 8
Training loss: 2.848437794629373
Validation loss: 2.5941838626707945

Epoch: 6| Step: 9
Training loss: 3.248923636974123
Validation loss: 2.5878065053247403

Epoch: 6| Step: 10
Training loss: 2.920043044622166
Validation loss: 2.582139945950945

Epoch: 6| Step: 11
Training loss: 2.689566771212249
Validation loss: 2.579413411817004

Epoch: 6| Step: 12
Training loss: 2.640799973689336
Validation loss: 2.578172821367079

Epoch: 6| Step: 13
Training loss: 2.4983684938216
Validation loss: 2.582769144422631

Epoch: 231| Step: 0
Training loss: 2.8547419133087253
Validation loss: 2.5821262427967087

Epoch: 6| Step: 1
Training loss: 3.4127916986903704
Validation loss: 2.5854403873623437

Epoch: 6| Step: 2
Training loss: 3.1775672085525057
Validation loss: 2.5880440243751868

Epoch: 6| Step: 3
Training loss: 2.338471228226189
Validation loss: 2.593816085983666

Epoch: 6| Step: 4
Training loss: 2.5323917027596767
Validation loss: 2.593077920324236

Epoch: 6| Step: 5
Training loss: 2.8584820129458506
Validation loss: 2.604175697721183

Epoch: 6| Step: 6
Training loss: 3.0522123099059164
Validation loss: 2.6118102376622843

Epoch: 6| Step: 7
Training loss: 3.0428988604177176
Validation loss: 2.6046270853567517

Epoch: 6| Step: 8
Training loss: 2.5188433040631137
Validation loss: 2.594149848687908

Epoch: 6| Step: 9
Training loss: 2.6612350360240016
Validation loss: 2.57206683808211

Epoch: 6| Step: 10
Training loss: 2.722080689814384
Validation loss: 2.573727595556544

Epoch: 6| Step: 11
Training loss: 2.8732310948544457
Validation loss: 2.574720559318074

Epoch: 6| Step: 12
Training loss: 3.1918971829215885
Validation loss: 2.5733899777242812

Epoch: 6| Step: 13
Training loss: 3.0786547519634464
Validation loss: 2.5605848375315

Epoch: 232| Step: 0
Training loss: 3.311241486633117
Validation loss: 2.563915785984657

Epoch: 6| Step: 1
Training loss: 2.6912098495954058
Validation loss: 2.5632369289637107

Epoch: 6| Step: 2
Training loss: 2.156464828625403
Validation loss: 2.5746333716560974

Epoch: 6| Step: 3
Training loss: 2.759282571318155
Validation loss: 2.573247102088778

Epoch: 6| Step: 4
Training loss: 2.707145391262463
Validation loss: 2.57205080074365

Epoch: 6| Step: 5
Training loss: 2.805070006233029
Validation loss: 2.56918229622062

Epoch: 6| Step: 6
Training loss: 2.916973388529564
Validation loss: 2.5867666049192337

Epoch: 6| Step: 7
Training loss: 3.256392135007465
Validation loss: 2.5955091940859094

Epoch: 6| Step: 8
Training loss: 2.721077635128944
Validation loss: 2.618660654892443

Epoch: 6| Step: 9
Training loss: 2.8860310108213256
Validation loss: 2.636856635775145

Epoch: 6| Step: 10
Training loss: 2.971830356074048
Validation loss: 2.649510259179019

Epoch: 6| Step: 11
Training loss: 3.1646281925828252
Validation loss: 2.623672927293262

Epoch: 6| Step: 12
Training loss: 3.2911355922125307
Validation loss: 2.613390553196238

Epoch: 6| Step: 13
Training loss: 2.4284130814163682
Validation loss: 2.603439097175235

Epoch: 233| Step: 0
Training loss: 2.66593066628595
Validation loss: 2.5997072045519087

Epoch: 6| Step: 1
Training loss: 2.5157989062741684
Validation loss: 2.594441850027113

Epoch: 6| Step: 2
Training loss: 2.1145498589432767
Validation loss: 2.576223806628885

Epoch: 6| Step: 3
Training loss: 2.8520925368909857
Validation loss: 2.5637925772476975

Epoch: 6| Step: 4
Training loss: 3.3641041669378047
Validation loss: 2.5605472314301094

Epoch: 6| Step: 5
Training loss: 2.768007276128566
Validation loss: 2.555972270940274

Epoch: 6| Step: 6
Training loss: 2.9693082585719908
Validation loss: 2.5502318382434717

Epoch: 6| Step: 7
Training loss: 2.943436488082425
Validation loss: 2.5557022831356924

Epoch: 6| Step: 8
Training loss: 2.7124020361020302
Validation loss: 2.55452063481749

Epoch: 6| Step: 9
Training loss: 2.537211615049288
Validation loss: 2.5556644256298253

Epoch: 6| Step: 10
Training loss: 3.0535696184221477
Validation loss: 2.557166051831858

Epoch: 6| Step: 11
Training loss: 3.174444952079683
Validation loss: 2.5557560711267624

Epoch: 6| Step: 12
Training loss: 3.0707184423713185
Validation loss: 2.5642974456122714

Epoch: 6| Step: 13
Training loss: 3.59144584761725
Validation loss: 2.5716158048669175

Epoch: 234| Step: 0
Training loss: 3.1838465677989505
Validation loss: 2.59091496326681

Epoch: 6| Step: 1
Training loss: 2.9289514049224956
Validation loss: 2.5875948269480435

Epoch: 6| Step: 2
Training loss: 2.4464497709499384
Validation loss: 2.6011657776961843

Epoch: 6| Step: 3
Training loss: 2.6055003197171445
Validation loss: 2.6090636651888466

Epoch: 6| Step: 4
Training loss: 2.7236172308737694
Validation loss: 2.6160671147119

Epoch: 6| Step: 5
Training loss: 3.0637195260980934
Validation loss: 2.614615794419382

Epoch: 6| Step: 6
Training loss: 2.741318437284725
Validation loss: 2.6124500029456104

Epoch: 6| Step: 7
Training loss: 3.216837787194296
Validation loss: 2.5899279787099667

Epoch: 6| Step: 8
Training loss: 3.0498609730036677
Validation loss: 2.5924062656880347

Epoch: 6| Step: 9
Training loss: 2.5962046762256077
Validation loss: 2.5814523415133377

Epoch: 6| Step: 10
Training loss: 3.2656632562614134
Validation loss: 2.565475529099308

Epoch: 6| Step: 11
Training loss: 3.103523614487828
Validation loss: 2.566956152796385

Epoch: 6| Step: 12
Training loss: 2.2620004007489216
Validation loss: 2.559519839867146

Epoch: 6| Step: 13
Training loss: 2.747592652609566
Validation loss: 2.5665809156233728

Epoch: 235| Step: 0
Training loss: 3.246227128539893
Validation loss: 2.5529315291538093

Epoch: 6| Step: 1
Training loss: 3.193412831643548
Validation loss: 2.5542207454384536

Epoch: 6| Step: 2
Training loss: 2.87361858185238
Validation loss: 2.550090825817683

Epoch: 6| Step: 3
Training loss: 2.886226131956753
Validation loss: 2.5589495257074524

Epoch: 6| Step: 4
Training loss: 3.4335159796368906
Validation loss: 2.5517448413441137

Epoch: 6| Step: 5
Training loss: 2.607897677210478
Validation loss: 2.551118315583974

Epoch: 6| Step: 6
Training loss: 2.286622188550747
Validation loss: 2.5701246739360077

Epoch: 6| Step: 7
Training loss: 2.972448995937065
Validation loss: 2.5704916943846303

Epoch: 6| Step: 8
Training loss: 2.7472575992080266
Validation loss: 2.5762767611641144

Epoch: 6| Step: 9
Training loss: 2.3943175676348907
Validation loss: 2.587212728729303

Epoch: 6| Step: 10
Training loss: 2.6001868290899974
Validation loss: 2.602842354178496

Epoch: 6| Step: 11
Training loss: 3.01009989373098
Validation loss: 2.615042458177812

Epoch: 6| Step: 12
Training loss: 2.7463970857870907
Validation loss: 2.6373573614571226

Epoch: 6| Step: 13
Training loss: 3.2321619396052865
Validation loss: 2.6565260934876154

Epoch: 236| Step: 0
Training loss: 3.1577051415330497
Validation loss: 2.651237339546262

Epoch: 6| Step: 1
Training loss: 2.385432769410089
Validation loss: 2.610030301913396

Epoch: 6| Step: 2
Training loss: 2.4446002544491265
Validation loss: 2.60738404477636

Epoch: 6| Step: 3
Training loss: 3.1209748857802264
Validation loss: 2.568455585106509

Epoch: 6| Step: 4
Training loss: 3.411022517585968
Validation loss: 2.5444389574295916

Epoch: 6| Step: 5
Training loss: 3.1457591437545145
Validation loss: 2.552953227201327

Epoch: 6| Step: 6
Training loss: 2.657545783437127
Validation loss: 2.545788784277435

Epoch: 6| Step: 7
Training loss: 2.7395982826511585
Validation loss: 2.5496966662343072

Epoch: 6| Step: 8
Training loss: 3.3654276415411486
Validation loss: 2.547118531071634

Epoch: 6| Step: 9
Training loss: 2.428938477291025
Validation loss: 2.5510434297038103

Epoch: 6| Step: 10
Training loss: 2.7113991819698584
Validation loss: 2.5567399923965253

Epoch: 6| Step: 11
Training loss: 3.1177722506732066
Validation loss: 2.556181405974867

Epoch: 6| Step: 12
Training loss: 2.3384043448612384
Validation loss: 2.557469062190225

Epoch: 6| Step: 13
Training loss: 3.2796100696366794
Validation loss: 2.5577227693976865

Epoch: 237| Step: 0
Training loss: 2.8824537100470433
Validation loss: 2.569921551632271

Epoch: 6| Step: 1
Training loss: 3.0360340238531816
Validation loss: 2.56114923014762

Epoch: 6| Step: 2
Training loss: 3.161702498047093
Validation loss: 2.579881300439351

Epoch: 6| Step: 3
Training loss: 2.8655668662527876
Validation loss: 2.564660182016169

Epoch: 6| Step: 4
Training loss: 3.1874652935457344
Validation loss: 2.5679032869234635

Epoch: 6| Step: 5
Training loss: 1.6283022266073077
Validation loss: 2.5722545701685093

Epoch: 6| Step: 6
Training loss: 2.4118409042142916
Validation loss: 2.56877315398928

Epoch: 6| Step: 7
Training loss: 2.7030750777204573
Validation loss: 2.572968800832639

Epoch: 6| Step: 8
Training loss: 3.1040186153325586
Validation loss: 2.5652433667543235

Epoch: 6| Step: 9
Training loss: 2.6218759020689
Validation loss: 2.5862568248346545

Epoch: 6| Step: 10
Training loss: 3.009466652747058
Validation loss: 2.57654153144489

Epoch: 6| Step: 11
Training loss: 2.865017352892133
Validation loss: 2.6059616780440953

Epoch: 6| Step: 12
Training loss: 3.230819148072778
Validation loss: 2.607315291983587

Epoch: 6| Step: 13
Training loss: 3.224112922442034
Validation loss: 2.621414028315742

Epoch: 238| Step: 0
Training loss: 2.915406926903769
Validation loss: 2.630374886522953

Epoch: 6| Step: 1
Training loss: 3.02217331905941
Validation loss: 2.642392842373007

Epoch: 6| Step: 2
Training loss: 3.1141343999927593
Validation loss: 2.600186977967655

Epoch: 6| Step: 3
Training loss: 2.588814693510137
Validation loss: 2.601510035527709

Epoch: 6| Step: 4
Training loss: 2.97275217229847
Validation loss: 2.6171574965085536

Epoch: 6| Step: 5
Training loss: 3.3385077051745387
Validation loss: 2.5852079523216585

Epoch: 6| Step: 6
Training loss: 2.261261203584177
Validation loss: 2.5713092621562814

Epoch: 6| Step: 7
Training loss: 3.01853873261919
Validation loss: 2.571233280315028

Epoch: 6| Step: 8
Training loss: 2.209191161712643
Validation loss: 2.560530417086025

Epoch: 6| Step: 9
Training loss: 2.8574205399860175
Validation loss: 2.5430157838252323

Epoch: 6| Step: 10
Training loss: 3.3375976148181707
Validation loss: 2.547062740909157

Epoch: 6| Step: 11
Training loss: 2.858532390577115
Validation loss: 2.5534695953542714

Epoch: 6| Step: 12
Training loss: 2.659852132344255
Validation loss: 2.5471590357957608

Epoch: 6| Step: 13
Training loss: 2.8166653623709585
Validation loss: 2.5496971750015067

Epoch: 239| Step: 0
Training loss: 2.569399964460217
Validation loss: 2.5486223256474085

Epoch: 6| Step: 1
Training loss: 3.062864632211
Validation loss: 2.547546730965809

Epoch: 6| Step: 2
Training loss: 2.957041731580251
Validation loss: 2.559888824418912

Epoch: 6| Step: 3
Training loss: 2.2935965879105575
Validation loss: 2.5609243707627445

Epoch: 6| Step: 4
Training loss: 2.7076533319762537
Validation loss: 2.5931686459519026

Epoch: 6| Step: 5
Training loss: 2.4755712986500122
Validation loss: 2.607608818605894

Epoch: 6| Step: 6
Training loss: 2.3240297897845785
Validation loss: 2.6276921336896297

Epoch: 6| Step: 7
Training loss: 3.080882120514014
Validation loss: 2.647471862282143

Epoch: 6| Step: 8
Training loss: 2.930868251644228
Validation loss: 2.6300003575465487

Epoch: 6| Step: 9
Training loss: 2.977794802634439
Validation loss: 2.6483069469447442

Epoch: 6| Step: 10
Training loss: 2.9744115552040618
Validation loss: 2.62671827809747

Epoch: 6| Step: 11
Training loss: 3.515739066074884
Validation loss: 2.602254364449505

Epoch: 6| Step: 12
Training loss: 3.379916812843648
Validation loss: 2.5780298229758145

Epoch: 6| Step: 13
Training loss: 2.8494232932732277
Validation loss: 2.5712813794104012

Epoch: 240| Step: 0
Training loss: 2.5995745237380237
Validation loss: 2.569712140137244

Epoch: 6| Step: 1
Training loss: 3.0996256817663372
Validation loss: 2.548535527698622

Epoch: 6| Step: 2
Training loss: 2.863990102665598
Validation loss: 2.5476142640322683

Epoch: 6| Step: 3
Training loss: 2.9387201250732846
Validation loss: 2.5464035036615353

Epoch: 6| Step: 4
Training loss: 3.0052355539729794
Validation loss: 2.551633424929056

Epoch: 6| Step: 5
Training loss: 3.1145513186303155
Validation loss: 2.552188502629479

Epoch: 6| Step: 6
Training loss: 3.0627639033993965
Validation loss: 2.5513681351768347

Epoch: 6| Step: 7
Training loss: 2.641345676290579
Validation loss: 2.5588381975750245

Epoch: 6| Step: 8
Training loss: 2.646016412443323
Validation loss: 2.584609919679257

Epoch: 6| Step: 9
Training loss: 2.639749325353054
Validation loss: 2.5760193181683895

Epoch: 6| Step: 10
Training loss: 2.536871989291393
Validation loss: 2.5779011290779916

Epoch: 6| Step: 11
Training loss: 3.1560221674809035
Validation loss: 2.5738258510751977

Epoch: 6| Step: 12
Training loss: 3.0291599264861357
Validation loss: 2.5788617097949675

Epoch: 6| Step: 13
Training loss: 2.6065758407684934
Validation loss: 2.569695035589115

Epoch: 241| Step: 0
Training loss: 3.3077725919353185
Validation loss: 2.5717215128431032

Epoch: 6| Step: 1
Training loss: 2.8531220652302984
Validation loss: 2.5563692777270477

Epoch: 6| Step: 2
Training loss: 3.083371411337581
Validation loss: 2.557549595797654

Epoch: 6| Step: 3
Training loss: 2.844252154271424
Validation loss: 2.549705104115134

Epoch: 6| Step: 4
Training loss: 3.061665635403823
Validation loss: 2.5365097109086836

Epoch: 6| Step: 5
Training loss: 2.479421607835064
Validation loss: 2.5591015594001116

Epoch: 6| Step: 6
Training loss: 2.37804258105989
Validation loss: 2.5622001270398247

Epoch: 6| Step: 7
Training loss: 3.1794408004247794
Validation loss: 2.5560226651842823

Epoch: 6| Step: 8
Training loss: 2.7884802116962795
Validation loss: 2.563133626614992

Epoch: 6| Step: 9
Training loss: 2.4693807448057465
Validation loss: 2.570133529522891

Epoch: 6| Step: 10
Training loss: 3.2727382158809313
Validation loss: 2.5589544607331205

Epoch: 6| Step: 11
Training loss: 2.6344146834130213
Validation loss: 2.568880656510642

Epoch: 6| Step: 12
Training loss: 2.7906249555735143
Validation loss: 2.5566065437146466

Epoch: 6| Step: 13
Training loss: 2.7670695524749
Validation loss: 2.5714934745029243

Epoch: 242| Step: 0
Training loss: 2.8722939402589085
Validation loss: 2.5840802642784815

Epoch: 6| Step: 1
Training loss: 2.7242411265481206
Validation loss: 2.5818327165187185

Epoch: 6| Step: 2
Training loss: 3.060997574953242
Validation loss: 2.5908951064447416

Epoch: 6| Step: 3
Training loss: 2.8913418267655766
Validation loss: 2.578876034703828

Epoch: 6| Step: 4
Training loss: 2.686326946365344
Validation loss: 2.581132344820617

Epoch: 6| Step: 5
Training loss: 2.75464870103158
Validation loss: 2.5823202048597302

Epoch: 6| Step: 6
Training loss: 2.5511921994111266
Validation loss: 2.5669853059273247

Epoch: 6| Step: 7
Training loss: 3.166439416327195
Validation loss: 2.561743003427351

Epoch: 6| Step: 8
Training loss: 2.675943589680808
Validation loss: 2.559231490157554

Epoch: 6| Step: 9
Training loss: 3.122524654391884
Validation loss: 2.559929109057758

Epoch: 6| Step: 10
Training loss: 2.899311437746783
Validation loss: 2.5653559898710303

Epoch: 6| Step: 11
Training loss: 2.756645236579721
Validation loss: 2.576211079068244

Epoch: 6| Step: 12
Training loss: 3.0539108030412625
Validation loss: 2.563132989488909

Epoch: 6| Step: 13
Training loss: 2.8084338146612975
Validation loss: 2.582524958775436

Epoch: 243| Step: 0
Training loss: 3.0276058354294673
Validation loss: 2.5641076832066276

Epoch: 6| Step: 1
Training loss: 2.846967449006167
Validation loss: 2.5665058815830353

Epoch: 6| Step: 2
Training loss: 2.6332802739008865
Validation loss: 2.571428137807033

Epoch: 6| Step: 3
Training loss: 3.186776771026564
Validation loss: 2.569597440216714

Epoch: 6| Step: 4
Training loss: 3.171284136680522
Validation loss: 2.557348803866065

Epoch: 6| Step: 5
Training loss: 2.50304865917134
Validation loss: 2.558217660027752

Epoch: 6| Step: 6
Training loss: 2.8134823249246783
Validation loss: 2.54997938639613

Epoch: 6| Step: 7
Training loss: 3.557933892753641
Validation loss: 2.5480782536335997

Epoch: 6| Step: 8
Training loss: 2.9182828149466076
Validation loss: 2.5494131122666337

Epoch: 6| Step: 9
Training loss: 2.50420331457385
Validation loss: 2.553686824992109

Epoch: 6| Step: 10
Training loss: 2.3179050890669806
Validation loss: 2.547340234314219

Epoch: 6| Step: 11
Training loss: 2.722910886922324
Validation loss: 2.546993730722925

Epoch: 6| Step: 12
Training loss: 2.715305119013932
Validation loss: 2.5581655883495396

Epoch: 6| Step: 13
Training loss: 3.0773707981087903
Validation loss: 2.5684596459762945

Epoch: 244| Step: 0
Training loss: 2.983898024864544
Validation loss: 2.580116441039801

Epoch: 6| Step: 1
Training loss: 2.873972170000181
Validation loss: 2.6025568830447976

Epoch: 6| Step: 2
Training loss: 2.9650872373050174
Validation loss: 2.598845496664733

Epoch: 6| Step: 3
Training loss: 2.855581800568628
Validation loss: 2.5917177488148226

Epoch: 6| Step: 4
Training loss: 2.788005981367747
Validation loss: 2.604254933163701

Epoch: 6| Step: 5
Training loss: 2.8248429617655693
Validation loss: 2.6338548047635872

Epoch: 6| Step: 6
Training loss: 3.0459602743502616
Validation loss: 2.6345947646131194

Epoch: 6| Step: 7
Training loss: 3.0315614520650054
Validation loss: 2.6785604976542348

Epoch: 6| Step: 8
Training loss: 2.964173654614708
Validation loss: 2.6733154451593504

Epoch: 6| Step: 9
Training loss: 2.7874555473033356
Validation loss: 2.653374651085628

Epoch: 6| Step: 10
Training loss: 2.9845934083917576
Validation loss: 2.621686912168323

Epoch: 6| Step: 11
Training loss: 2.265724337800683
Validation loss: 2.593039682113513

Epoch: 6| Step: 12
Training loss: 3.0400781184750274
Validation loss: 2.597462890297378

Epoch: 6| Step: 13
Training loss: 2.406988847923039
Validation loss: 2.578570009110711

Epoch: 245| Step: 0
Training loss: 2.728268152700007
Validation loss: 2.5703511800435406

Epoch: 6| Step: 1
Training loss: 2.7385069227620416
Validation loss: 2.5736353359034116

Epoch: 6| Step: 2
Training loss: 3.435030518330985
Validation loss: 2.574374060452261

Epoch: 6| Step: 3
Training loss: 3.2825561830080674
Validation loss: 2.5856597070105347

Epoch: 6| Step: 4
Training loss: 2.331817157061695
Validation loss: 2.5791095045901593

Epoch: 6| Step: 5
Training loss: 2.7061894857146966
Validation loss: 2.590076641029386

Epoch: 6| Step: 6
Training loss: 2.5872366139984115
Validation loss: 2.5989632124455007

Epoch: 6| Step: 7
Training loss: 3.182437242007516
Validation loss: 2.604153696991813

Epoch: 6| Step: 8
Training loss: 3.0401828926684695
Validation loss: 2.603572334313395

Epoch: 6| Step: 9
Training loss: 3.0168487753794193
Validation loss: 2.601068893995093

Epoch: 6| Step: 10
Training loss: 2.7054299466608005
Validation loss: 2.5947033478882173

Epoch: 6| Step: 11
Training loss: 2.6733546036264917
Validation loss: 2.581938981241681

Epoch: 6| Step: 12
Training loss: 2.767173893710373
Validation loss: 2.587242399739403

Epoch: 6| Step: 13
Training loss: 2.7523979655937545
Validation loss: 2.5775253747099405

Epoch: 246| Step: 0
Training loss: 2.924808600268668
Validation loss: 2.5774006924600474

Epoch: 6| Step: 1
Training loss: 3.2051528094398796
Validation loss: 2.5752805628476474

Epoch: 6| Step: 2
Training loss: 3.070152220291645
Validation loss: 2.591933570046003

Epoch: 6| Step: 3
Training loss: 2.804693280482379
Validation loss: 2.583010936894669

Epoch: 6| Step: 4
Training loss: 2.243910815073139
Validation loss: 2.5814941010357044

Epoch: 6| Step: 5
Training loss: 2.7769203346152738
Validation loss: 2.5860340901030376

Epoch: 6| Step: 6
Training loss: 2.948692264678928
Validation loss: 2.6067660535759316

Epoch: 6| Step: 7
Training loss: 2.6526436053348417
Validation loss: 2.6115290103658624

Epoch: 6| Step: 8
Training loss: 2.5040105122537515
Validation loss: 2.601327811810507

Epoch: 6| Step: 9
Training loss: 2.9369064705399146
Validation loss: 2.5985326497061334

Epoch: 6| Step: 10
Training loss: 3.401845061131722
Validation loss: 2.5875653769542706

Epoch: 6| Step: 11
Training loss: 2.8845932172265867
Validation loss: 2.563544986966029

Epoch: 6| Step: 12
Training loss: 2.6900262713656176
Validation loss: 2.564480713108114

Epoch: 6| Step: 13
Training loss: 2.7713319334617434
Validation loss: 2.5684163814378578

Epoch: 247| Step: 0
Training loss: 2.693966205886227
Validation loss: 2.558251007351844

Epoch: 6| Step: 1
Training loss: 3.449366945429081
Validation loss: 2.5635990224126037

Epoch: 6| Step: 2
Training loss: 2.2647133933453483
Validation loss: 2.5746720534988645

Epoch: 6| Step: 3
Training loss: 2.6694784738058925
Validation loss: 2.5809601817079457

Epoch: 6| Step: 4
Training loss: 2.6452365312297395
Validation loss: 2.5776731013625604

Epoch: 6| Step: 5
Training loss: 2.683412504114179
Validation loss: 2.5838507151716708

Epoch: 6| Step: 6
Training loss: 2.7356578324500154
Validation loss: 2.575329603713946

Epoch: 6| Step: 7
Training loss: 3.1282249594769755
Validation loss: 2.6181948113500373

Epoch: 6| Step: 8
Training loss: 3.2665476978462578
Validation loss: 2.6331075042548147

Epoch: 6| Step: 9
Training loss: 2.637288259531196
Validation loss: 2.659761199377126

Epoch: 6| Step: 10
Training loss: 2.8717570051292274
Validation loss: 2.645158683786576

Epoch: 6| Step: 11
Training loss: 3.2754460911839844
Validation loss: 2.6043567835823604

Epoch: 6| Step: 12
Training loss: 3.032148405041434
Validation loss: 2.5714964020337603

Epoch: 6| Step: 13
Training loss: 2.198258673166748
Validation loss: 2.5568197919538673

Epoch: 248| Step: 0
Training loss: 2.2896660018959176
Validation loss: 2.5525135149015474

Epoch: 6| Step: 1
Training loss: 3.0278008097512417
Validation loss: 2.544356417879316

Epoch: 6| Step: 2
Training loss: 3.2846409306179605
Validation loss: 2.542564635577313

Epoch: 6| Step: 3
Training loss: 2.8221677136184313
Validation loss: 2.5397485807893694

Epoch: 6| Step: 4
Training loss: 2.765200393754646
Validation loss: 2.541738696383038

Epoch: 6| Step: 5
Training loss: 2.8056129905017055
Validation loss: 2.538910861728897

Epoch: 6| Step: 6
Training loss: 3.2611470892194383
Validation loss: 2.544553662001163

Epoch: 6| Step: 7
Training loss: 2.9642724251900647
Validation loss: 2.5485040490313686

Epoch: 6| Step: 8
Training loss: 2.8796632514218317
Validation loss: 2.543792110942226

Epoch: 6| Step: 9
Training loss: 3.2259643591409612
Validation loss: 2.5411307887732235

Epoch: 6| Step: 10
Training loss: 3.029691000312251
Validation loss: 2.5468495957359325

Epoch: 6| Step: 11
Training loss: 2.815543668294152
Validation loss: 2.5457312852394236

Epoch: 6| Step: 12
Training loss: 2.6062792842692595
Validation loss: 2.5413648648738847

Epoch: 6| Step: 13
Training loss: 2.5317197764059745
Validation loss: 2.5399432405462474

Epoch: 249| Step: 0
Training loss: 2.516836313083445
Validation loss: 2.5518284509456177

Epoch: 6| Step: 1
Training loss: 2.880816008677835
Validation loss: 2.5560528988466564

Epoch: 6| Step: 2
Training loss: 3.069429925929313
Validation loss: 2.551444591543887

Epoch: 6| Step: 3
Training loss: 2.623420058210055
Validation loss: 2.56032178598471

Epoch: 6| Step: 4
Training loss: 2.82840419676767
Validation loss: 2.555125278498127

Epoch: 6| Step: 5
Training loss: 2.8954055433562407
Validation loss: 2.5790470844698827

Epoch: 6| Step: 6
Training loss: 2.830961206793572
Validation loss: 2.5736698341751074

Epoch: 6| Step: 7
Training loss: 2.775504532046321
Validation loss: 2.5767620840803844

Epoch: 6| Step: 8
Training loss: 2.7526811187776192
Validation loss: 2.5830027169892773

Epoch: 6| Step: 9
Training loss: 2.2130926297569475
Validation loss: 2.6263669991485323

Epoch: 6| Step: 10
Training loss: 3.228008694321506
Validation loss: 2.6132386956408307

Epoch: 6| Step: 11
Training loss: 3.2930153767643486
Validation loss: 2.6189401477719794

Epoch: 6| Step: 12
Training loss: 2.97438879065574
Validation loss: 2.653539475508388

Epoch: 6| Step: 13
Training loss: 2.9345306647696057
Validation loss: 2.6574467008775753

Epoch: 250| Step: 0
Training loss: 2.981530556583123
Validation loss: 2.6608365421851268

Epoch: 6| Step: 1
Training loss: 1.9765205228064786
Validation loss: 2.634380943639925

Epoch: 6| Step: 2
Training loss: 2.9894876675246924
Validation loss: 2.6003752313824977

Epoch: 6| Step: 3
Training loss: 2.902922006661187
Validation loss: 2.5927961928112935

Epoch: 6| Step: 4
Training loss: 3.303583488295058
Validation loss: 2.5621239831238007

Epoch: 6| Step: 5
Training loss: 3.1123599959295674
Validation loss: 2.5613070876971613

Epoch: 6| Step: 6
Training loss: 3.2373282545333733
Validation loss: 2.5609303120508575

Epoch: 6| Step: 7
Training loss: 2.717724398390891
Validation loss: 2.5542701374496

Epoch: 6| Step: 8
Training loss: 2.7993694242510037
Validation loss: 2.553694297996544

Epoch: 6| Step: 9
Training loss: 2.5037572283742837
Validation loss: 2.554671245180918

Epoch: 6| Step: 10
Training loss: 3.0195730978766537
Validation loss: 2.559527331898889

Epoch: 6| Step: 11
Training loss: 2.795316123213747
Validation loss: 2.551134706594856

Epoch: 6| Step: 12
Training loss: 2.648275634860503
Validation loss: 2.5566044038423934

Epoch: 6| Step: 13
Training loss: 3.140442344705975
Validation loss: 2.559598623293065

Testing loss: 2.7726331577824506

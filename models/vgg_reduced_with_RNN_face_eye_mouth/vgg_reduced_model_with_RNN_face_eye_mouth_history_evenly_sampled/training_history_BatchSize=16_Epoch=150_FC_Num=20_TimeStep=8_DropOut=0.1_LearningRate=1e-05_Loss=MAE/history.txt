Epoch: 1| Step: 0
Training loss: 4.705821514129639
Validation loss: 5.137729024374357

Epoch: 6| Step: 1
Training loss: 5.966273307800293
Validation loss: 5.129549088016633

Epoch: 6| Step: 2
Training loss: 5.239224910736084
Validation loss: 5.121745509486044

Epoch: 6| Step: 3
Training loss: 6.10215425491333
Validation loss: 5.114472419984879

Epoch: 6| Step: 4
Training loss: 4.237180709838867
Validation loss: 5.1070006790981495

Epoch: 6| Step: 5
Training loss: 4.319806098937988
Validation loss: 5.099266785447315

Epoch: 6| Step: 6
Training loss: 4.523712158203125
Validation loss: 5.091193640103904

Epoch: 6| Step: 7
Training loss: 5.336325168609619
Validation loss: 5.082168322737499

Epoch: 6| Step: 8
Training loss: 4.991980075836182
Validation loss: 5.072951675743185

Epoch: 6| Step: 9
Training loss: 4.5540313720703125
Validation loss: 5.062737669996036

Epoch: 6| Step: 10
Training loss: 4.22824764251709
Validation loss: 5.0518469041393645

Epoch: 6| Step: 11
Training loss: 4.190736293792725
Validation loss: 5.040275389148343

Epoch: 6| Step: 12
Training loss: 5.581038951873779
Validation loss: 5.02750208557293

Epoch: 6| Step: 13
Training loss: 3.9264423847198486
Validation loss: 5.013577133096675

Epoch: 2| Step: 0
Training loss: 5.700935363769531
Validation loss: 4.998503238924088

Epoch: 6| Step: 1
Training loss: 4.558375358581543
Validation loss: 4.9821789700497865

Epoch: 6| Step: 2
Training loss: 4.18670654296875
Validation loss: 4.964670832439135

Epoch: 6| Step: 3
Training loss: 4.457530975341797
Validation loss: 4.9455320194203365

Epoch: 6| Step: 4
Training loss: 4.044068336486816
Validation loss: 4.9258271135309695

Epoch: 6| Step: 5
Training loss: 4.960092544555664
Validation loss: 4.903923014158844

Epoch: 6| Step: 6
Training loss: 5.646646499633789
Validation loss: 4.880580466280701

Epoch: 6| Step: 7
Training loss: 4.104831695556641
Validation loss: 4.855691499607538

Epoch: 6| Step: 8
Training loss: 4.784313201904297
Validation loss: 4.828178621107532

Epoch: 6| Step: 9
Training loss: 3.541900396347046
Validation loss: 4.800875033101728

Epoch: 6| Step: 10
Training loss: 4.773362636566162
Validation loss: 4.77096757068429

Epoch: 6| Step: 11
Training loss: 4.189121246337891
Validation loss: 4.738364168392715

Epoch: 6| Step: 12
Training loss: 4.285773277282715
Validation loss: 4.706716706675868

Epoch: 6| Step: 13
Training loss: 6.735820293426514
Validation loss: 4.674662461844823

Epoch: 3| Step: 0
Training loss: 3.8013551235198975
Validation loss: 4.639064886236704

Epoch: 6| Step: 1
Training loss: 4.226169109344482
Validation loss: 4.603664787866736

Epoch: 6| Step: 2
Training loss: 3.6827192306518555
Validation loss: 4.567840053189185

Epoch: 6| Step: 3
Training loss: 4.33973503112793
Validation loss: 4.531314149979623

Epoch: 6| Step: 4
Training loss: 4.46073055267334
Validation loss: 4.494511468436128

Epoch: 6| Step: 5
Training loss: 4.629549026489258
Validation loss: 4.454602133843206

Epoch: 6| Step: 6
Training loss: 4.653470993041992
Validation loss: 4.412528314898091

Epoch: 6| Step: 7
Training loss: 4.819371223449707
Validation loss: 4.372604682881345

Epoch: 6| Step: 8
Training loss: 3.197929859161377
Validation loss: 4.329282391455866

Epoch: 6| Step: 9
Training loss: 3.1761631965637207
Validation loss: 4.286889271069598

Epoch: 6| Step: 10
Training loss: 3.846738815307617
Validation loss: 4.24498539586221

Epoch: 6| Step: 11
Training loss: 4.958083152770996
Validation loss: 4.204712565227221

Epoch: 6| Step: 12
Training loss: 4.104898452758789
Validation loss: 4.165169028825657

Epoch: 6| Step: 13
Training loss: 5.230865955352783
Validation loss: 4.127744587518835

Epoch: 4| Step: 0
Training loss: 3.907045841217041
Validation loss: 4.0905381607752975

Epoch: 6| Step: 1
Training loss: 4.2099127769470215
Validation loss: 4.055551946804088

Epoch: 6| Step: 2
Training loss: 3.3105111122131348
Validation loss: 4.019248729111046

Epoch: 6| Step: 3
Training loss: 3.825329303741455
Validation loss: 3.9875749054775445

Epoch: 6| Step: 4
Training loss: 4.363417148590088
Validation loss: 3.9583159108315744

Epoch: 6| Step: 5
Training loss: 4.183497428894043
Validation loss: 3.9324872929562806

Epoch: 6| Step: 6
Training loss: 3.42811918258667
Validation loss: 3.907389279334776

Epoch: 6| Step: 7
Training loss: 4.0670928955078125
Validation loss: 3.88499629112982

Epoch: 6| Step: 8
Training loss: 3.328794479370117
Validation loss: 3.8635604971198627

Epoch: 6| Step: 9
Training loss: 3.4027578830718994
Validation loss: 3.8439838117168796

Epoch: 6| Step: 10
Training loss: 4.095383167266846
Validation loss: 3.8263524937373337

Epoch: 6| Step: 11
Training loss: 3.665975570678711
Validation loss: 3.803290864472748

Epoch: 6| Step: 12
Training loss: 3.657334566116333
Validation loss: 3.773660485462476

Epoch: 6| Step: 13
Training loss: 3.3509604930877686
Validation loss: 3.7544018709531395

Epoch: 5| Step: 0
Training loss: 4.020061492919922
Validation loss: 3.742171264463855

Epoch: 6| Step: 1
Training loss: 3.659207344055176
Validation loss: 3.7302459311741654

Epoch: 6| Step: 2
Training loss: 3.4368319511413574
Validation loss: 3.718882683784731

Epoch: 6| Step: 3
Training loss: 4.751062393188477
Validation loss: 3.709728394785235

Epoch: 6| Step: 4
Training loss: 3.3851640224456787
Validation loss: 3.6965387277705695

Epoch: 6| Step: 5
Training loss: 3.7319440841674805
Validation loss: 3.6829497583450808

Epoch: 6| Step: 6
Training loss: 3.7128710746765137
Validation loss: 3.669042697516821

Epoch: 6| Step: 7
Training loss: 2.3705966472625732
Validation loss: 3.654240131378174

Epoch: 6| Step: 8
Training loss: 3.9601759910583496
Validation loss: 3.643574427532893

Epoch: 6| Step: 9
Training loss: 3.503354072570801
Validation loss: 3.6330809952110372

Epoch: 6| Step: 10
Training loss: 3.155933380126953
Validation loss: 3.6237354970747426

Epoch: 6| Step: 11
Training loss: 3.318650722503662
Validation loss: 3.612820807323661

Epoch: 6| Step: 12
Training loss: 3.2137961387634277
Validation loss: 3.6023898124694824

Epoch: 6| Step: 13
Training loss: 4.1396284103393555
Validation loss: 3.5929623342329458

Epoch: 6| Step: 0
Training loss: 3.8877146244049072
Validation loss: 3.5826905081349034

Epoch: 6| Step: 1
Training loss: 4.0234456062316895
Validation loss: 3.5711753727287374

Epoch: 6| Step: 2
Training loss: 3.594970226287842
Validation loss: 3.5587689594555925

Epoch: 6| Step: 3
Training loss: 3.1178946495056152
Validation loss: 3.5477367062722482

Epoch: 6| Step: 4
Training loss: 1.4724392890930176
Validation loss: 3.533452695415866

Epoch: 6| Step: 5
Training loss: 3.2662601470947266
Validation loss: 3.5253350350164596

Epoch: 6| Step: 6
Training loss: 4.115786552429199
Validation loss: 3.515914194045528

Epoch: 6| Step: 7
Training loss: 4.376883506774902
Validation loss: 3.506119199978408

Epoch: 6| Step: 8
Training loss: 3.1840603351593018
Validation loss: 3.492248155737436

Epoch: 6| Step: 9
Training loss: 4.31443977355957
Validation loss: 3.4794160499367663

Epoch: 6| Step: 10
Training loss: 3.488574266433716
Validation loss: 3.468934876944429

Epoch: 6| Step: 11
Training loss: 3.1363167762756348
Validation loss: 3.4631154332109677

Epoch: 6| Step: 12
Training loss: 2.732144832611084
Validation loss: 3.4781794932580765

Epoch: 6| Step: 13
Training loss: 3.754037618637085
Validation loss: 3.4822277304946736

Epoch: 7| Step: 0
Training loss: 4.138532638549805
Validation loss: 3.4570009170040006

Epoch: 6| Step: 1
Training loss: 3.84287691116333
Validation loss: 3.417762787111344

Epoch: 6| Step: 2
Training loss: 3.4622764587402344
Validation loss: 3.408171715274934

Epoch: 6| Step: 3
Training loss: 4.038515090942383
Validation loss: 3.4098246661565637

Epoch: 6| Step: 4
Training loss: 4.222774505615234
Validation loss: 3.4125218955419396

Epoch: 6| Step: 5
Training loss: 3.038782835006714
Validation loss: 3.4031105426050003

Epoch: 6| Step: 6
Training loss: 3.221550941467285
Validation loss: 3.391234720906904

Epoch: 6| Step: 7
Training loss: 3.1106057167053223
Validation loss: 3.3777598719443045

Epoch: 6| Step: 8
Training loss: 2.2707834243774414
Validation loss: 3.3640344271095852

Epoch: 6| Step: 9
Training loss: 3.694761037826538
Validation loss: 3.3548180441702566

Epoch: 6| Step: 10
Training loss: 3.319199800491333
Validation loss: 3.3458452840005197

Epoch: 6| Step: 11
Training loss: 2.611069679260254
Validation loss: 3.333790453531409

Epoch: 6| Step: 12
Training loss: 2.9127354621887207
Validation loss: 3.3240223059090237

Epoch: 6| Step: 13
Training loss: 2.588958263397217
Validation loss: 3.3122937089653424

Epoch: 8| Step: 0
Training loss: 3.4991636276245117
Validation loss: 3.3028126762759302

Epoch: 6| Step: 1
Training loss: 2.7497456073760986
Validation loss: 3.297496388035436

Epoch: 6| Step: 2
Training loss: 3.965747594833374
Validation loss: 3.295273357822049

Epoch: 6| Step: 3
Training loss: 3.485964298248291
Validation loss: 3.2823996723339124

Epoch: 6| Step: 4
Training loss: 3.2065184116363525
Validation loss: 3.270243352459323

Epoch: 6| Step: 5
Training loss: 4.071826934814453
Validation loss: 3.2607189660431235

Epoch: 6| Step: 6
Training loss: 1.8622369766235352
Validation loss: 3.250477208886095

Epoch: 6| Step: 7
Training loss: 3.245256185531616
Validation loss: 3.2442519741673626

Epoch: 6| Step: 8
Training loss: 2.652332305908203
Validation loss: 3.2375655225528184

Epoch: 6| Step: 9
Training loss: 3.4520106315612793
Validation loss: 3.2308866439327115

Epoch: 6| Step: 10
Training loss: 3.1232666969299316
Validation loss: 3.22046398603788

Epoch: 6| Step: 11
Training loss: 3.500499725341797
Validation loss: 3.2027727327039166

Epoch: 6| Step: 12
Training loss: 2.6745777130126953
Validation loss: 3.1959000941245788

Epoch: 6| Step: 13
Training loss: 4.099536895751953
Validation loss: 3.1840294971260974

Epoch: 9| Step: 0
Training loss: 3.8861804008483887
Validation loss: 3.1834504758158038

Epoch: 6| Step: 1
Training loss: 2.9279112815856934
Validation loss: 3.1712491102116083

Epoch: 6| Step: 2
Training loss: 3.289064407348633
Validation loss: 3.1634980222230316

Epoch: 6| Step: 3
Training loss: 3.286590576171875
Validation loss: 3.1684306052423294

Epoch: 6| Step: 4
Training loss: 3.953674793243408
Validation loss: 3.157344251550654

Epoch: 6| Step: 5
Training loss: 3.483959197998047
Validation loss: 3.1384627639606433

Epoch: 6| Step: 6
Training loss: 4.238207817077637
Validation loss: 3.1390942424856205

Epoch: 6| Step: 7
Training loss: 2.414902687072754
Validation loss: 3.1385253629376813

Epoch: 6| Step: 8
Training loss: 2.2869174480438232
Validation loss: 3.1303438909592165

Epoch: 6| Step: 9
Training loss: 3.036524772644043
Validation loss: 3.128402981706845

Epoch: 6| Step: 10
Training loss: 3.2507052421569824
Validation loss: 3.1194230869252193

Epoch: 6| Step: 11
Training loss: 2.861588478088379
Validation loss: 3.1146604527709303

Epoch: 6| Step: 12
Training loss: 2.2819571495056152
Validation loss: 3.1067364882397395

Epoch: 6| Step: 13
Training loss: 2.759291648864746
Validation loss: 3.0970360386756157

Epoch: 10| Step: 0
Training loss: 2.944345235824585
Validation loss: 3.0940278037901847

Epoch: 6| Step: 1
Training loss: 3.210057258605957
Validation loss: 3.084732755537956

Epoch: 6| Step: 2
Training loss: 3.3847475051879883
Validation loss: 3.0784423325651433

Epoch: 6| Step: 3
Training loss: 2.8886733055114746
Validation loss: 3.0721045924771215

Epoch: 6| Step: 4
Training loss: 3.2382616996765137
Validation loss: 3.0653701802735687

Epoch: 6| Step: 5
Training loss: 3.021440029144287
Validation loss: 3.0579278263994443

Epoch: 6| Step: 6
Training loss: 3.359457015991211
Validation loss: 3.0524268368239045

Epoch: 6| Step: 7
Training loss: 3.3714849948883057
Validation loss: 3.0465367942728023

Epoch: 6| Step: 8
Training loss: 3.1698737144470215
Validation loss: 3.041661834204069

Epoch: 6| Step: 9
Training loss: 2.2726142406463623
Validation loss: 3.0327547878347416

Epoch: 6| Step: 10
Training loss: 3.70090651512146
Validation loss: 3.031506133335893

Epoch: 6| Step: 11
Training loss: 3.241337776184082
Validation loss: 3.0258379546544885

Epoch: 6| Step: 12
Training loss: 2.1757190227508545
Validation loss: 3.0245468821576846

Epoch: 6| Step: 13
Training loss: 3.577314615249634
Validation loss: 3.027223353744835

Epoch: 11| Step: 0
Training loss: 3.792259693145752
Validation loss: 3.0332670673247306

Epoch: 6| Step: 1
Training loss: 3.1748828887939453
Validation loss: 3.0037922884828303

Epoch: 6| Step: 2
Training loss: 3.404099225997925
Validation loss: 3.000951131184896

Epoch: 6| Step: 3
Training loss: 3.3962807655334473
Validation loss: 3.0065120291966263

Epoch: 6| Step: 4
Training loss: 3.4769043922424316
Validation loss: 3.0033146053232174

Epoch: 6| Step: 5
Training loss: 3.308952808380127
Validation loss: 2.996149657874979

Epoch: 6| Step: 6
Training loss: 2.556314468383789
Validation loss: 2.9879620536681144

Epoch: 6| Step: 7
Training loss: 2.5843257904052734
Validation loss: 2.985349803842524

Epoch: 6| Step: 8
Training loss: 3.5742788314819336
Validation loss: 2.9795714296320432

Epoch: 6| Step: 9
Training loss: 3.182633399963379
Validation loss: 2.981343618003271

Epoch: 6| Step: 10
Training loss: 2.886702060699463
Validation loss: 2.9768889693803686

Epoch: 6| Step: 11
Training loss: 2.4964065551757812
Validation loss: 2.9769292518656743

Epoch: 6| Step: 12
Training loss: 2.179807662963867
Validation loss: 2.9706228574117026

Epoch: 6| Step: 13
Training loss: 2.603142738342285
Validation loss: 2.9667568540060394

Epoch: 12| Step: 0
Training loss: 2.059767723083496
Validation loss: 2.964383291941817

Epoch: 6| Step: 1
Training loss: 3.6003289222717285
Validation loss: 2.9579410194068827

Epoch: 6| Step: 2
Training loss: 3.2081873416900635
Validation loss: 2.9524284639666156

Epoch: 6| Step: 3
Training loss: 3.6678507328033447
Validation loss: 2.950074541953302

Epoch: 6| Step: 4
Training loss: 3.206453323364258
Validation loss: 2.9486762323687152

Epoch: 6| Step: 5
Training loss: 2.9168224334716797
Validation loss: 2.9445807831261748

Epoch: 6| Step: 6
Training loss: 3.5894908905029297
Validation loss: 2.940348030418478

Epoch: 6| Step: 7
Training loss: 2.6554040908813477
Validation loss: 2.9361658173222698

Epoch: 6| Step: 8
Training loss: 2.9944381713867188
Validation loss: 2.934708595275879

Epoch: 6| Step: 9
Training loss: 3.3801939487457275
Validation loss: 2.931608356455321

Epoch: 6| Step: 10
Training loss: 2.9458136558532715
Validation loss: 2.928582510640544

Epoch: 6| Step: 11
Training loss: 2.6135828495025635
Validation loss: 2.9297076527790358

Epoch: 6| Step: 12
Training loss: 3.5151093006134033
Validation loss: 2.9316553954155213

Epoch: 6| Step: 13
Training loss: 1.1770100593566895
Validation loss: 2.925891696765859

Epoch: 13| Step: 0
Training loss: 3.306529998779297
Validation loss: 2.922647912015197

Epoch: 6| Step: 1
Training loss: 2.7605433464050293
Validation loss: 2.9119546669785694

Epoch: 6| Step: 2
Training loss: 3.7328195571899414
Validation loss: 2.913005962166735

Epoch: 6| Step: 3
Training loss: 2.057600975036621
Validation loss: 2.9105653583362536

Epoch: 6| Step: 4
Training loss: 2.1137917041778564
Validation loss: 2.9046530441571305

Epoch: 6| Step: 5
Training loss: 4.075128555297852
Validation loss: 2.903247146196263

Epoch: 6| Step: 6
Training loss: 2.920435905456543
Validation loss: 2.9059358130219164

Epoch: 6| Step: 7
Training loss: 2.6187143325805664
Validation loss: 2.903863396695865

Epoch: 6| Step: 8
Training loss: 3.410020351409912
Validation loss: 2.8986292449376916

Epoch: 6| Step: 9
Training loss: 3.424771785736084
Validation loss: 2.895356547447943

Epoch: 6| Step: 10
Training loss: 2.941005229949951
Validation loss: 2.8943441555064213

Epoch: 6| Step: 11
Training loss: 3.0273406505584717
Validation loss: 2.8944406150489725

Epoch: 6| Step: 12
Training loss: 2.5420825481414795
Validation loss: 2.8892248163941088

Epoch: 6| Step: 13
Training loss: 3.02823805809021
Validation loss: 2.885737442201184

Epoch: 14| Step: 0
Training loss: 2.7799971103668213
Validation loss: 2.882950823794129

Epoch: 6| Step: 1
Training loss: 3.34674334526062
Validation loss: 2.882595880057222

Epoch: 6| Step: 2
Training loss: 2.5533361434936523
Validation loss: 2.882202317637782

Epoch: 6| Step: 3
Training loss: 3.5927629470825195
Validation loss: 2.8819294898740706

Epoch: 6| Step: 4
Training loss: 3.214618682861328
Validation loss: 2.8766898416703746

Epoch: 6| Step: 5
Training loss: 3.481818437576294
Validation loss: 2.8756175579563266

Epoch: 6| Step: 6
Training loss: 3.889249086380005
Validation loss: 2.8765319880618843

Epoch: 6| Step: 7
Training loss: 2.828589916229248
Validation loss: 2.8741032923421552

Epoch: 6| Step: 8
Training loss: 1.642432689666748
Validation loss: 2.87540021763053

Epoch: 6| Step: 9
Training loss: 2.421999454498291
Validation loss: 2.8817831777757212

Epoch: 6| Step: 10
Training loss: 2.2385332584381104
Validation loss: 2.8726735704688617

Epoch: 6| Step: 11
Training loss: 3.286553382873535
Validation loss: 2.8734773871719197

Epoch: 6| Step: 12
Training loss: 3.7026147842407227
Validation loss: 2.8699912050718903

Epoch: 6| Step: 13
Training loss: 2.4893527030944824
Validation loss: 2.8577638877335416

Epoch: 15| Step: 0
Training loss: 2.916637659072876
Validation loss: 2.8577171782011628

Epoch: 6| Step: 1
Training loss: 2.254855155944824
Validation loss: 2.8527717872332503

Epoch: 6| Step: 2
Training loss: 2.895106315612793
Validation loss: 2.852539257336688

Epoch: 6| Step: 3
Training loss: 2.416510581970215
Validation loss: 2.8510205207332486

Epoch: 6| Step: 4
Training loss: 3.03587007522583
Validation loss: 2.850299350676998

Epoch: 6| Step: 5
Training loss: 3.106066942214966
Validation loss: 2.850616293568765

Epoch: 6| Step: 6
Training loss: 2.4532814025878906
Validation loss: 2.8502701508101596

Epoch: 6| Step: 7
Training loss: 3.5311179161071777
Validation loss: 2.8423916498819985

Epoch: 6| Step: 8
Training loss: 2.935914993286133
Validation loss: 2.839245711603472

Epoch: 6| Step: 9
Training loss: 3.314668655395508
Validation loss: 2.840032797987743

Epoch: 6| Step: 10
Training loss: 2.945812225341797
Validation loss: 2.8376610073992

Epoch: 6| Step: 11
Training loss: 2.8218555450439453
Validation loss: 2.832210556153328

Epoch: 6| Step: 12
Training loss: 3.769996404647827
Validation loss: 2.8318071365356445

Epoch: 6| Step: 13
Training loss: 3.0850448608398438
Validation loss: 2.826358356783467

Epoch: 16| Step: 0
Training loss: 2.507258415222168
Validation loss: 2.832516006244126

Epoch: 6| Step: 1
Training loss: 4.1853742599487305
Validation loss: 2.831202245527698

Epoch: 6| Step: 2
Training loss: 3.59712815284729
Validation loss: 2.8237418282416558

Epoch: 6| Step: 3
Training loss: 3.5642001628875732
Validation loss: 2.8220833245144097

Epoch: 6| Step: 4
Training loss: 2.4383745193481445
Validation loss: 2.8228355787133657

Epoch: 6| Step: 5
Training loss: 2.415680170059204
Validation loss: 2.8239307865019767

Epoch: 6| Step: 6
Training loss: 3.2947487831115723
Validation loss: 2.825968257842525

Epoch: 6| Step: 7
Training loss: 3.4931509494781494
Validation loss: 2.8221249247110016

Epoch: 6| Step: 8
Training loss: 2.489672899246216
Validation loss: 2.8197207630321546

Epoch: 6| Step: 9
Training loss: 2.1597533226013184
Validation loss: 2.813635256982619

Epoch: 6| Step: 10
Training loss: 3.228273630142212
Validation loss: 2.813660401169972

Epoch: 6| Step: 11
Training loss: 2.677966833114624
Validation loss: 2.8093662108144453

Epoch: 6| Step: 12
Training loss: 2.812281847000122
Validation loss: 2.805452887729932

Epoch: 6| Step: 13
Training loss: 1.9250550270080566
Validation loss: 2.803881511893324

Epoch: 17| Step: 0
Training loss: 2.6639087200164795
Validation loss: 2.803476446418352

Epoch: 6| Step: 1
Training loss: 3.0379865169525146
Validation loss: 2.8017170454866145

Epoch: 6| Step: 2
Training loss: 2.9500739574432373
Validation loss: 2.7994841760204685

Epoch: 6| Step: 3
Training loss: 2.3894639015197754
Validation loss: 2.7974319470826017

Epoch: 6| Step: 4
Training loss: 3.3975844383239746
Validation loss: 2.7970035435051046

Epoch: 6| Step: 5
Training loss: 2.6171369552612305
Validation loss: 2.7975132593544583

Epoch: 6| Step: 6
Training loss: 3.009389877319336
Validation loss: 2.7917226540145053

Epoch: 6| Step: 7
Training loss: 2.9205973148345947
Validation loss: 2.789850213194406

Epoch: 6| Step: 8
Training loss: 2.345259189605713
Validation loss: 2.788355770931449

Epoch: 6| Step: 9
Training loss: 3.754491090774536
Validation loss: 2.7888552245273384

Epoch: 6| Step: 10
Training loss: 3.121486186981201
Validation loss: 2.7846889675304456

Epoch: 6| Step: 11
Training loss: 3.28235125541687
Validation loss: 2.7829727793252594

Epoch: 6| Step: 12
Training loss: 3.026073455810547
Validation loss: 2.7797861714516916

Epoch: 6| Step: 13
Training loss: 2.0883169174194336
Validation loss: 2.7765209085197857

Epoch: 18| Step: 0
Training loss: 3.6015729904174805
Validation loss: 2.7756636117094304

Epoch: 6| Step: 1
Training loss: 3.4602489471435547
Validation loss: 2.773011158871394

Epoch: 6| Step: 2
Training loss: 3.683366298675537
Validation loss: 2.7732782235709568

Epoch: 6| Step: 3
Training loss: 3.449420928955078
Validation loss: 2.769086271203974

Epoch: 6| Step: 4
Training loss: 2.177013397216797
Validation loss: 2.766008371947914

Epoch: 6| Step: 5
Training loss: 2.715519905090332
Validation loss: 2.763923850110782

Epoch: 6| Step: 6
Training loss: 2.437486410140991
Validation loss: 2.761769769012287

Epoch: 6| Step: 7
Training loss: 1.7956537008285522
Validation loss: 2.760559812668831

Epoch: 6| Step: 8
Training loss: 2.629925012588501
Validation loss: 2.75790044312836

Epoch: 6| Step: 9
Training loss: 3.232433795928955
Validation loss: 2.754327174155943

Epoch: 6| Step: 10
Training loss: 3.489532470703125
Validation loss: 2.7529778839439474

Epoch: 6| Step: 11
Training loss: 2.374250888824463
Validation loss: 2.7512473137147966

Epoch: 6| Step: 12
Training loss: 2.7356443405151367
Validation loss: 2.7476051930458314

Epoch: 6| Step: 13
Training loss: 2.8941965103149414
Validation loss: 2.747885424603698

Epoch: 19| Step: 0
Training loss: 2.841134786605835
Validation loss: 2.7446928229383243

Epoch: 6| Step: 1
Training loss: 3.907066822052002
Validation loss: 2.745307566017233

Epoch: 6| Step: 2
Training loss: 3.333451747894287
Validation loss: 2.740020257170482

Epoch: 6| Step: 3
Training loss: 2.134918451309204
Validation loss: 2.7393083931297384

Epoch: 6| Step: 4
Training loss: 2.912992000579834
Validation loss: 2.741873066912415

Epoch: 6| Step: 5
Training loss: 2.4906530380249023
Validation loss: 2.738162463711154

Epoch: 6| Step: 6
Training loss: 2.528013229370117
Validation loss: 2.7363509285834526

Epoch: 6| Step: 7
Training loss: 3.151548385620117
Validation loss: 2.7341043103125786

Epoch: 6| Step: 8
Training loss: 2.8034744262695312
Validation loss: 2.7327736552043627

Epoch: 6| Step: 9
Training loss: 2.6559743881225586
Validation loss: 2.7291694687258814

Epoch: 6| Step: 10
Training loss: 2.928103446960449
Validation loss: 2.730770318738876

Epoch: 6| Step: 11
Training loss: 2.900808811187744
Validation loss: 2.7292959613184773

Epoch: 6| Step: 12
Training loss: 2.8009262084960938
Validation loss: 2.7263183132294686

Epoch: 6| Step: 13
Training loss: 3.171419858932495
Validation loss: 2.726639865547098

Epoch: 20| Step: 0
Training loss: 2.5229640007019043
Validation loss: 2.723413352043398

Epoch: 6| Step: 1
Training loss: 2.7842600345611572
Validation loss: 2.723535463374148

Epoch: 6| Step: 2
Training loss: 3.2414450645446777
Validation loss: 2.7243128386876916

Epoch: 6| Step: 3
Training loss: 2.4538164138793945
Validation loss: 2.72714558211706

Epoch: 6| Step: 4
Training loss: 2.0076541900634766
Validation loss: 2.7175112872995357

Epoch: 6| Step: 5
Training loss: 3.968055009841919
Validation loss: 2.714728340025871

Epoch: 6| Step: 6
Training loss: 2.423543930053711
Validation loss: 2.718934625707647

Epoch: 6| Step: 7
Training loss: 3.017731189727783
Validation loss: 2.7201462176538285

Epoch: 6| Step: 8
Training loss: 2.855438232421875
Validation loss: 2.722137192244171

Epoch: 6| Step: 9
Training loss: 2.8207473754882812
Validation loss: 2.73837923747237

Epoch: 6| Step: 10
Training loss: 3.1267144680023193
Validation loss: 2.7246728327966507

Epoch: 6| Step: 11
Training loss: 2.9504289627075195
Validation loss: 2.7043632973906813

Epoch: 6| Step: 12
Training loss: 2.9783546924591064
Validation loss: 2.7153749030123473

Epoch: 6| Step: 13
Training loss: 3.46651291847229
Validation loss: 2.7457838725018244

Epoch: 21| Step: 0
Training loss: 3.4152865409851074
Validation loss: 2.7290749626774944

Epoch: 6| Step: 1
Training loss: 2.6875290870666504
Validation loss: 2.7163993825194654

Epoch: 6| Step: 2
Training loss: 3.167851448059082
Validation loss: 2.712805809513215

Epoch: 6| Step: 3
Training loss: 2.2633914947509766
Validation loss: 2.711994227542672

Epoch: 6| Step: 4
Training loss: 3.570894241333008
Validation loss: 2.7200258777987574

Epoch: 6| Step: 5
Training loss: 3.16206693649292
Validation loss: 2.7245727918481313

Epoch: 6| Step: 6
Training loss: 2.409245014190674
Validation loss: 2.7238257213305404

Epoch: 6| Step: 7
Training loss: 2.008223533630371
Validation loss: 2.7159049357137373

Epoch: 6| Step: 8
Training loss: 2.5932693481445312
Validation loss: 2.707937245727867

Epoch: 6| Step: 9
Training loss: 2.773347854614258
Validation loss: 2.699652833323325

Epoch: 6| Step: 10
Training loss: 3.3438467979431152
Validation loss: 2.7008682617577175

Epoch: 6| Step: 11
Training loss: 2.80706787109375
Validation loss: 2.696085701706589

Epoch: 6| Step: 12
Training loss: 2.5578408241271973
Validation loss: 2.6940846596994708

Epoch: 6| Step: 13
Training loss: 3.85022234916687
Validation loss: 2.69410438435052

Epoch: 22| Step: 0
Training loss: 2.874528646469116
Validation loss: 2.6912837477140528

Epoch: 6| Step: 1
Training loss: 2.3500313758850098
Validation loss: 2.691388831343702

Epoch: 6| Step: 2
Training loss: 3.460926055908203
Validation loss: 2.6903894896148355

Epoch: 6| Step: 3
Training loss: 3.309490919113159
Validation loss: 2.6901538525858233

Epoch: 6| Step: 4
Training loss: 2.445685863494873
Validation loss: 2.6863729492310555

Epoch: 6| Step: 5
Training loss: 3.9771461486816406
Validation loss: 2.6878974206985964

Epoch: 6| Step: 6
Training loss: 3.32454514503479
Validation loss: 2.6842731250229703

Epoch: 6| Step: 7
Training loss: 1.9603271484375
Validation loss: 2.6844986818170034

Epoch: 6| Step: 8
Training loss: 3.4637811183929443
Validation loss: 2.6833954421422814

Epoch: 6| Step: 9
Training loss: 3.1223363876342773
Validation loss: 2.685298478731545

Epoch: 6| Step: 10
Training loss: 2.6566720008850098
Validation loss: 2.686386182744016

Epoch: 6| Step: 11
Training loss: 2.241886615753174
Validation loss: 2.6834426131299747

Epoch: 6| Step: 12
Training loss: 2.702536106109619
Validation loss: 2.680330086779851

Epoch: 6| Step: 13
Training loss: 1.5057518482208252
Validation loss: 2.6795963702663297

Epoch: 23| Step: 0
Training loss: 3.0360076427459717
Validation loss: 2.680438054505215

Epoch: 6| Step: 1
Training loss: 3.2691893577575684
Validation loss: 2.6794431517201085

Epoch: 6| Step: 2
Training loss: 3.395296573638916
Validation loss: 2.6815441603301675

Epoch: 6| Step: 3
Training loss: 3.3790931701660156
Validation loss: 2.6780675765006774

Epoch: 6| Step: 4
Training loss: 3.547455310821533
Validation loss: 2.6764974773571057

Epoch: 6| Step: 5
Training loss: 3.507516622543335
Validation loss: 2.6733178682224725

Epoch: 6| Step: 6
Training loss: 2.042936325073242
Validation loss: 2.6854450574485202

Epoch: 6| Step: 7
Training loss: 2.603933334350586
Validation loss: 2.6727213039193103

Epoch: 6| Step: 8
Training loss: 2.9012293815612793
Validation loss: 2.6769718124020483

Epoch: 6| Step: 9
Training loss: 2.6763124465942383
Validation loss: 2.684343934059143

Epoch: 6| Step: 10
Training loss: 2.430781364440918
Validation loss: 2.6818965634992047

Epoch: 6| Step: 11
Training loss: 2.4804816246032715
Validation loss: 2.6815966841995076

Epoch: 6| Step: 12
Training loss: 2.0001256465911865
Validation loss: 2.67229978499874

Epoch: 6| Step: 13
Training loss: 2.5378994941711426
Validation loss: 2.6732935597819667

Epoch: 24| Step: 0
Training loss: 3.3374552726745605
Validation loss: 2.6731370469575286

Epoch: 6| Step: 1
Training loss: 2.0977628231048584
Validation loss: 2.670690426262476

Epoch: 6| Step: 2
Training loss: 2.7474703788757324
Validation loss: 2.6690900966685307

Epoch: 6| Step: 3
Training loss: 3.6284475326538086
Validation loss: 2.6716108373416367

Epoch: 6| Step: 4
Training loss: 3.333857774734497
Validation loss: 2.674871903593822

Epoch: 6| Step: 5
Training loss: 3.3039116859436035
Validation loss: 2.6753433365975656

Epoch: 6| Step: 6
Training loss: 2.7980058193206787
Validation loss: 2.6725385983784995

Epoch: 6| Step: 7
Training loss: 3.149294376373291
Validation loss: 2.6713858496758247

Epoch: 6| Step: 8
Training loss: 3.366445302963257
Validation loss: 2.668365140115061

Epoch: 6| Step: 9
Training loss: 2.1574010848999023
Validation loss: 2.663280092259889

Epoch: 6| Step: 10
Training loss: 2.7274487018585205
Validation loss: 2.6692228612079414

Epoch: 6| Step: 11
Training loss: 2.134697198867798
Validation loss: 2.669073704750307

Epoch: 6| Step: 12
Training loss: 2.5968337059020996
Validation loss: 2.6654461199237454

Epoch: 6| Step: 13
Training loss: 2.2074098587036133
Validation loss: 2.666082336056617

Epoch: 25| Step: 0
Training loss: 3.4598498344421387
Validation loss: 2.664129885294104

Epoch: 6| Step: 1
Training loss: 3.4274754524230957
Validation loss: 2.659972404920927

Epoch: 6| Step: 2
Training loss: 2.7154648303985596
Validation loss: 2.6621299584706626

Epoch: 6| Step: 3
Training loss: 2.757838487625122
Validation loss: 2.6720095911333637

Epoch: 6| Step: 4
Training loss: 3.755293607711792
Validation loss: 2.6865129035006285

Epoch: 6| Step: 5
Training loss: 1.9807908535003662
Validation loss: 2.685568824891121

Epoch: 6| Step: 6
Training loss: 2.4054276943206787
Validation loss: 2.6837279053144556

Epoch: 6| Step: 7
Training loss: 1.945518970489502
Validation loss: 2.681402280766477

Epoch: 6| Step: 8
Training loss: 2.79744029045105
Validation loss: 2.6718328050387803

Epoch: 6| Step: 9
Training loss: 2.540895938873291
Validation loss: 2.660882724228726

Epoch: 6| Step: 10
Training loss: 3.6102476119995117
Validation loss: 2.6620469529141664

Epoch: 6| Step: 11
Training loss: 3.2352006435394287
Validation loss: 2.6640323644043296

Epoch: 6| Step: 12
Training loss: 2.433640480041504
Validation loss: 2.668341685366887

Epoch: 6| Step: 13
Training loss: 2.704726219177246
Validation loss: 2.675709944899364

Epoch: 26| Step: 0
Training loss: 2.4790077209472656
Validation loss: 2.68466535434928

Epoch: 6| Step: 1
Training loss: 3.7471537590026855
Validation loss: 2.7223487618148967

Epoch: 6| Step: 2
Training loss: 3.006585121154785
Validation loss: 2.6664723734701834

Epoch: 6| Step: 3
Training loss: 2.2541422843933105
Validation loss: 2.665196734090005

Epoch: 6| Step: 4
Training loss: 2.3010144233703613
Validation loss: 2.680759814477736

Epoch: 6| Step: 5
Training loss: 3.051870346069336
Validation loss: 2.7105823691173265

Epoch: 6| Step: 6
Training loss: 2.677668571472168
Validation loss: 2.7164725078049528

Epoch: 6| Step: 7
Training loss: 2.584685802459717
Validation loss: 2.7623919799763668

Epoch: 6| Step: 8
Training loss: 3.0114762783050537
Validation loss: 2.7125560314424577

Epoch: 6| Step: 9
Training loss: 3.1775147914886475
Validation loss: 2.689184332406649

Epoch: 6| Step: 10
Training loss: 2.9176998138427734
Validation loss: 2.6812913238361316

Epoch: 6| Step: 11
Training loss: 2.7425389289855957
Validation loss: 2.672752385498375

Epoch: 6| Step: 12
Training loss: 2.820317506790161
Validation loss: 2.6659833590189614

Epoch: 6| Step: 13
Training loss: 3.3590142726898193
Validation loss: 2.6664203238743607

Epoch: 27| Step: 0
Training loss: 2.9984335899353027
Validation loss: 2.6675474079706336

Epoch: 6| Step: 1
Training loss: 3.3428759574890137
Validation loss: 2.6644399166107178

Epoch: 6| Step: 2
Training loss: 2.420001268386841
Validation loss: 2.6657239416594147

Epoch: 6| Step: 3
Training loss: 2.6244592666625977
Validation loss: 2.6719043229215886

Epoch: 6| Step: 4
Training loss: 2.389054298400879
Validation loss: 2.6686838185915382

Epoch: 6| Step: 5
Training loss: 2.1889944076538086
Validation loss: 2.665753874727475

Epoch: 6| Step: 6
Training loss: 2.3108348846435547
Validation loss: 2.6681954835050847

Epoch: 6| Step: 7
Training loss: 2.781987428665161
Validation loss: 2.662200063787481

Epoch: 6| Step: 8
Training loss: 3.2951815128326416
Validation loss: 2.6593941027118313

Epoch: 6| Step: 9
Training loss: 2.9352049827575684
Validation loss: 2.656899139445315

Epoch: 6| Step: 10
Training loss: 2.838627338409424
Validation loss: 2.65499851652371

Epoch: 6| Step: 11
Training loss: 2.905104160308838
Validation loss: 2.654878298441569

Epoch: 6| Step: 12
Training loss: 3.9514524936676025
Validation loss: 2.652933218145883

Epoch: 6| Step: 13
Training loss: 2.637399673461914
Validation loss: 2.6505881624837078

Epoch: 28| Step: 0
Training loss: 2.4748592376708984
Validation loss: 2.6510968233949397

Epoch: 6| Step: 1
Training loss: 3.9792428016662598
Validation loss: 2.650493862808392

Epoch: 6| Step: 2
Training loss: 1.967401385307312
Validation loss: 2.6481297733963176

Epoch: 6| Step: 3
Training loss: 2.8290963172912598
Validation loss: 2.651651664446759

Epoch: 6| Step: 4
Training loss: 2.490631103515625
Validation loss: 2.6518333240221907

Epoch: 6| Step: 5
Training loss: 3.433473825454712
Validation loss: 2.667118062255203

Epoch: 6| Step: 6
Training loss: 2.129554271697998
Validation loss: 2.6619371034765757

Epoch: 6| Step: 7
Training loss: 2.7681589126586914
Validation loss: 2.664392827659525

Epoch: 6| Step: 8
Training loss: 3.2994236946105957
Validation loss: 2.6546737019733717

Epoch: 6| Step: 9
Training loss: 2.631040334701538
Validation loss: 2.650179309229697

Epoch: 6| Step: 10
Training loss: 2.479691505432129
Validation loss: 2.6435699257799374

Epoch: 6| Step: 11
Training loss: 3.0442543029785156
Validation loss: 2.6428025819922007

Epoch: 6| Step: 12
Training loss: 2.990342378616333
Validation loss: 2.640653625611336

Epoch: 6| Step: 13
Training loss: 3.2894222736358643
Validation loss: 2.6397024841718775

Epoch: 29| Step: 0
Training loss: 2.9763734340667725
Validation loss: 2.6450945792659635

Epoch: 6| Step: 1
Training loss: 2.4405064582824707
Validation loss: 2.642407053260393

Epoch: 6| Step: 2
Training loss: 2.4528956413269043
Validation loss: 2.6415219563309864

Epoch: 6| Step: 3
Training loss: 2.02291202545166
Validation loss: 2.641201865288519

Epoch: 6| Step: 4
Training loss: 3.7515883445739746
Validation loss: 2.6434438459334837

Epoch: 6| Step: 5
Training loss: 3.3790881633758545
Validation loss: 2.6415814968847458

Epoch: 6| Step: 6
Training loss: 3.6436104774475098
Validation loss: 2.639532853198308

Epoch: 6| Step: 7
Training loss: 2.8004815578460693
Validation loss: 2.638965896380845

Epoch: 6| Step: 8
Training loss: 2.8611626625061035
Validation loss: 2.638303590077226

Epoch: 6| Step: 9
Training loss: 1.941343069076538
Validation loss: 2.6317365400252806

Epoch: 6| Step: 10
Training loss: 2.421077013015747
Validation loss: 2.6314696393987185

Epoch: 6| Step: 11
Training loss: 3.3576340675354004
Validation loss: 2.6355121007529636

Epoch: 6| Step: 12
Training loss: 2.776556968688965
Validation loss: 2.636657045733544

Epoch: 6| Step: 13
Training loss: 2.5173003673553467
Validation loss: 2.6397206962749524

Epoch: 30| Step: 0
Training loss: 2.2504146099090576
Validation loss: 2.6420691679882746

Epoch: 6| Step: 1
Training loss: 1.9657485485076904
Validation loss: 2.6426581234060307

Epoch: 6| Step: 2
Training loss: 2.5223655700683594
Validation loss: 2.63800323137673

Epoch: 6| Step: 3
Training loss: 2.883686065673828
Validation loss: 2.6386564136833273

Epoch: 6| Step: 4
Training loss: 2.900592803955078
Validation loss: 2.632590168265886

Epoch: 6| Step: 5
Training loss: 2.279047966003418
Validation loss: 2.627360156787339

Epoch: 6| Step: 6
Training loss: 3.2628066539764404
Validation loss: 2.6295500211818243

Epoch: 6| Step: 7
Training loss: 3.0693442821502686
Validation loss: 2.629417903961674

Epoch: 6| Step: 8
Training loss: 2.7770633697509766
Validation loss: 2.627384075554468

Epoch: 6| Step: 9
Training loss: 3.1863389015197754
Validation loss: 2.628047422696185

Epoch: 6| Step: 10
Training loss: 2.7692959308624268
Validation loss: 2.6248889943604827

Epoch: 6| Step: 11
Training loss: 3.2998924255371094
Validation loss: 2.6265109585177515

Epoch: 6| Step: 12
Training loss: 2.9143104553222656
Validation loss: 2.6276382502689155

Epoch: 6| Step: 13
Training loss: 3.6459639072418213
Validation loss: 2.625980592543079

Epoch: 31| Step: 0
Training loss: 3.3051180839538574
Validation loss: 2.627505217829058

Epoch: 6| Step: 1
Training loss: 2.41538667678833
Validation loss: 2.6251317942014305

Epoch: 6| Step: 2
Training loss: 2.763807773590088
Validation loss: 2.6291340499795894

Epoch: 6| Step: 3
Training loss: 3.5606372356414795
Validation loss: 2.626056748051797

Epoch: 6| Step: 4
Training loss: 2.3660330772399902
Validation loss: 2.6251627886167137

Epoch: 6| Step: 5
Training loss: 2.7116739749908447
Validation loss: 2.622503903604323

Epoch: 6| Step: 6
Training loss: 3.308465003967285
Validation loss: 2.6222459526472193

Epoch: 6| Step: 7
Training loss: 2.795867919921875
Validation loss: 2.62189043465481

Epoch: 6| Step: 8
Training loss: 2.8027052879333496
Validation loss: 2.6214192323787238

Epoch: 6| Step: 9
Training loss: 2.415163516998291
Validation loss: 2.619110735513831

Epoch: 6| Step: 10
Training loss: 2.5533056259155273
Validation loss: 2.622144627314742

Epoch: 6| Step: 11
Training loss: 3.492940902709961
Validation loss: 2.620125501386581

Epoch: 6| Step: 12
Training loss: 2.788935661315918
Validation loss: 2.620829295086604

Epoch: 6| Step: 13
Training loss: 1.247274398803711
Validation loss: 2.6222258485773557

Epoch: 32| Step: 0
Training loss: 2.5068135261535645
Validation loss: 2.6208118930939706

Epoch: 6| Step: 1
Training loss: 2.589231252670288
Validation loss: 2.6209826187420915

Epoch: 6| Step: 2
Training loss: 2.265404224395752
Validation loss: 2.6179226636886597

Epoch: 6| Step: 3
Training loss: 2.2543203830718994
Validation loss: 2.6181424202457553

Epoch: 6| Step: 4
Training loss: 2.7905025482177734
Validation loss: 2.615910686472411

Epoch: 6| Step: 5
Training loss: 2.2647829055786133
Validation loss: 2.61685109907581

Epoch: 6| Step: 6
Training loss: 3.470743417739868
Validation loss: 2.617819929635653

Epoch: 6| Step: 7
Training loss: 3.0421440601348877
Validation loss: 2.6169073966241654

Epoch: 6| Step: 8
Training loss: 2.7632665634155273
Validation loss: 2.6161373840865267

Epoch: 6| Step: 9
Training loss: 3.2312350273132324
Validation loss: 2.6186977842802643

Epoch: 6| Step: 10
Training loss: 2.450277328491211
Validation loss: 2.6231993372722338

Epoch: 6| Step: 11
Training loss: 3.7120954990386963
Validation loss: 2.624890399235551

Epoch: 6| Step: 12
Training loss: 2.699399709701538
Validation loss: 2.6265575475590204

Epoch: 6| Step: 13
Training loss: 3.34470272064209
Validation loss: 2.625839764072049

Epoch: 33| Step: 0
Training loss: 3.8986940383911133
Validation loss: 2.628786592073338

Epoch: 6| Step: 1
Training loss: 2.5168871879577637
Validation loss: 2.625947347251318

Epoch: 6| Step: 2
Training loss: 3.283989429473877
Validation loss: 2.618678267284106

Epoch: 6| Step: 3
Training loss: 2.487149715423584
Validation loss: 2.6212773938332834

Epoch: 6| Step: 4
Training loss: 2.763866901397705
Validation loss: 2.6157471364544285

Epoch: 6| Step: 5
Training loss: 2.3817241191864014
Validation loss: 2.6106638446930917

Epoch: 6| Step: 6
Training loss: 2.373546600341797
Validation loss: 2.61341259812796

Epoch: 6| Step: 7
Training loss: 2.688915729522705
Validation loss: 2.612649320274271

Epoch: 6| Step: 8
Training loss: 2.5482683181762695
Validation loss: 2.6102446151036087

Epoch: 6| Step: 9
Training loss: 3.0541841983795166
Validation loss: 2.6193311342629055

Epoch: 6| Step: 10
Training loss: 3.0267481803894043
Validation loss: 2.623157573002641

Epoch: 6| Step: 11
Training loss: 3.0483927726745605
Validation loss: 2.623739952682167

Epoch: 6| Step: 12
Training loss: 1.976395845413208
Validation loss: 2.6238023747680006

Epoch: 6| Step: 13
Training loss: 3.2619619369506836
Validation loss: 2.619318982606293

Epoch: 34| Step: 0
Training loss: 2.560150623321533
Validation loss: 2.6141795830060075

Epoch: 6| Step: 1
Training loss: 3.0458803176879883
Validation loss: 2.6104408566669752

Epoch: 6| Step: 2
Training loss: 2.5124354362487793
Validation loss: 2.619302398415022

Epoch: 6| Step: 3
Training loss: 3.0529143810272217
Validation loss: 2.6262640517245055

Epoch: 6| Step: 4
Training loss: 2.539397716522217
Validation loss: 2.64028412295926

Epoch: 6| Step: 5
Training loss: 3.3901264667510986
Validation loss: 2.6508510446035736

Epoch: 6| Step: 6
Training loss: 3.126955032348633
Validation loss: 2.646221371107204

Epoch: 6| Step: 7
Training loss: 2.8952534198760986
Validation loss: 2.627070708941388

Epoch: 6| Step: 8
Training loss: 2.2717885971069336
Validation loss: 2.611361011382072

Epoch: 6| Step: 9
Training loss: 2.6410531997680664
Validation loss: 2.6100659895968694

Epoch: 6| Step: 10
Training loss: 2.5697972774505615
Validation loss: 2.60821799309023

Epoch: 6| Step: 11
Training loss: 3.5263772010803223
Validation loss: 2.615433613459269

Epoch: 6| Step: 12
Training loss: 2.3283607959747314
Validation loss: 2.608653235179122

Epoch: 6| Step: 13
Training loss: 2.549449920654297
Validation loss: 2.6047586189803256

Epoch: 35| Step: 0
Training loss: 3.7451415061950684
Validation loss: 2.605393550729239

Epoch: 6| Step: 1
Training loss: 2.209862232208252
Validation loss: 2.6191430604586037

Epoch: 6| Step: 2
Training loss: 2.976679801940918
Validation loss: 2.6130571801175355

Epoch: 6| Step: 3
Training loss: 2.35634183883667
Validation loss: 2.6176731407001452

Epoch: 6| Step: 4
Training loss: 2.9109463691711426
Validation loss: 2.617045012853479

Epoch: 6| Step: 5
Training loss: 3.391108989715576
Validation loss: 2.620674266610094

Epoch: 6| Step: 6
Training loss: 3.5078344345092773
Validation loss: 2.60979111220247

Epoch: 6| Step: 7
Training loss: 2.6147959232330322
Validation loss: 2.6067481886956

Epoch: 6| Step: 8
Training loss: 2.8048696517944336
Validation loss: 2.6016434802803943

Epoch: 6| Step: 9
Training loss: 3.013976573944092
Validation loss: 2.6049319723600983

Epoch: 6| Step: 10
Training loss: 2.6998291015625
Validation loss: 2.606300018166983

Epoch: 6| Step: 11
Training loss: 2.2489871978759766
Validation loss: 2.605575056486232

Epoch: 6| Step: 12
Training loss: 2.047246217727661
Validation loss: 2.606748042568084

Epoch: 6| Step: 13
Training loss: 2.2280900478363037
Validation loss: 2.6119049723430345

Epoch: 36| Step: 0
Training loss: 2.56943941116333
Validation loss: 2.609702907582765

Epoch: 6| Step: 1
Training loss: 2.749873638153076
Validation loss: 2.608402033005991

Epoch: 6| Step: 2
Training loss: 3.454216718673706
Validation loss: 2.607005326978622

Epoch: 6| Step: 3
Training loss: 2.9878458976745605
Validation loss: 2.603094047115695

Epoch: 6| Step: 4
Training loss: 1.9058526754379272
Validation loss: 2.60860320573212

Epoch: 6| Step: 5
Training loss: 3.0739970207214355
Validation loss: 2.6168757356623167

Epoch: 6| Step: 6
Training loss: 3.4993791580200195
Validation loss: 2.612107297425629

Epoch: 6| Step: 7
Training loss: 2.6734261512756348
Validation loss: 2.6025349991295927

Epoch: 6| Step: 8
Training loss: 2.1194746494293213
Validation loss: 2.5961331449529177

Epoch: 6| Step: 9
Training loss: 3.0063118934631348
Validation loss: 2.5994602787879204

Epoch: 6| Step: 10
Training loss: 2.029582977294922
Validation loss: 2.596409856632192

Epoch: 6| Step: 11
Training loss: 3.534437417984009
Validation loss: 2.5973389892167944

Epoch: 6| Step: 12
Training loss: 2.864753007888794
Validation loss: 2.6000911881846767

Epoch: 6| Step: 13
Training loss: 2.133211135864258
Validation loss: 2.606880793007471

Epoch: 37| Step: 0
Training loss: 2.8922882080078125
Validation loss: 2.620546212760351

Epoch: 6| Step: 1
Training loss: 2.565377712249756
Validation loss: 2.6250657214913318

Epoch: 6| Step: 2
Training loss: 3.011000633239746
Validation loss: 2.6422680988106677

Epoch: 6| Step: 3
Training loss: 3.6890945434570312
Validation loss: 2.6453666712648127

Epoch: 6| Step: 4
Training loss: 2.3584444522857666
Validation loss: 2.663900923985307

Epoch: 6| Step: 5
Training loss: 2.0495481491088867
Validation loss: 2.634233956695885

Epoch: 6| Step: 6
Training loss: 2.601292133331299
Validation loss: 2.6083396224565405

Epoch: 6| Step: 7
Training loss: 3.1436662673950195
Validation loss: 2.5951810895755725

Epoch: 6| Step: 8
Training loss: 2.695359230041504
Validation loss: 2.5938533993177515

Epoch: 6| Step: 9
Training loss: 3.0462396144866943
Validation loss: 2.5983090336604784

Epoch: 6| Step: 10
Training loss: 3.073483943939209
Validation loss: 2.6119934897268973

Epoch: 6| Step: 11
Training loss: 3.244894027709961
Validation loss: 2.610506596103791

Epoch: 6| Step: 12
Training loss: 1.7794994115829468
Validation loss: 2.6038651004914315

Epoch: 6| Step: 13
Training loss: 3.0166962146759033
Validation loss: 2.591908965059506

Epoch: 38| Step: 0
Training loss: 2.4301319122314453
Validation loss: 2.5887881299500823

Epoch: 6| Step: 1
Training loss: 3.402890682220459
Validation loss: 2.589422730989354

Epoch: 6| Step: 2
Training loss: 2.828683376312256
Validation loss: 2.5989814573718655

Epoch: 6| Step: 3
Training loss: 3.5316925048828125
Validation loss: 2.612896406522361

Epoch: 6| Step: 4
Training loss: 2.1450300216674805
Validation loss: 2.6319498528716383

Epoch: 6| Step: 5
Training loss: 2.4587349891662598
Validation loss: 2.645388695501512

Epoch: 6| Step: 6
Training loss: 2.126652717590332
Validation loss: 2.6590057137191936

Epoch: 6| Step: 7
Training loss: 3.08888578414917
Validation loss: 2.650397389165817

Epoch: 6| Step: 8
Training loss: 3.897031307220459
Validation loss: 2.617255292912965

Epoch: 6| Step: 9
Training loss: 2.411945104598999
Validation loss: 2.596807227339796

Epoch: 6| Step: 10
Training loss: 2.2236719131469727
Validation loss: 2.5875471612458587

Epoch: 6| Step: 11
Training loss: 3.1221346855163574
Validation loss: 2.587552885855398

Epoch: 6| Step: 12
Training loss: 2.1612401008605957
Validation loss: 2.5877941962211364

Epoch: 6| Step: 13
Training loss: 3.346775770187378
Validation loss: 2.5969113252496205

Epoch: 39| Step: 0
Training loss: 2.4070467948913574
Validation loss: 2.607429826131431

Epoch: 6| Step: 1
Training loss: 1.9488661289215088
Validation loss: 2.608618720885246

Epoch: 6| Step: 2
Training loss: 3.29082989692688
Validation loss: 2.60663164559231

Epoch: 6| Step: 3
Training loss: 2.3805060386657715
Validation loss: 2.6039709352677867

Epoch: 6| Step: 4
Training loss: 2.9461169242858887
Validation loss: 2.600438094908191

Epoch: 6| Step: 5
Training loss: 3.1348884105682373
Validation loss: 2.5925273741445234

Epoch: 6| Step: 6
Training loss: 2.795353412628174
Validation loss: 2.5930209570033576

Epoch: 6| Step: 7
Training loss: 2.932910919189453
Validation loss: 2.592405714014525

Epoch: 6| Step: 8
Training loss: 3.1493587493896484
Validation loss: 2.5965075441586074

Epoch: 6| Step: 9
Training loss: 2.824802875518799
Validation loss: 2.6012491692778883

Epoch: 6| Step: 10
Training loss: 2.7113325595855713
Validation loss: 2.5912626558734524

Epoch: 6| Step: 11
Training loss: 2.4808802604675293
Validation loss: 2.5866764924859487

Epoch: 6| Step: 12
Training loss: 2.546931266784668
Validation loss: 2.5822646335888932

Epoch: 6| Step: 13
Training loss: 3.446320056915283
Validation loss: 2.5863039570470012

Epoch: 40| Step: 0
Training loss: 3.4068474769592285
Validation loss: 2.5863418040737027

Epoch: 6| Step: 1
Training loss: 2.8858113288879395
Validation loss: 2.5860522254820792

Epoch: 6| Step: 2
Training loss: 2.4783248901367188
Validation loss: 2.588329981732112

Epoch: 6| Step: 3
Training loss: 2.7586021423339844
Validation loss: 2.6031348910383

Epoch: 6| Step: 4
Training loss: 2.1170763969421387
Validation loss: 2.5994636269025904

Epoch: 6| Step: 5
Training loss: 2.8468031883239746
Validation loss: 2.5925004636087725

Epoch: 6| Step: 6
Training loss: 2.90956711769104
Validation loss: 2.5985346096818165

Epoch: 6| Step: 7
Training loss: 2.562899589538574
Validation loss: 2.60559816257928

Epoch: 6| Step: 8
Training loss: 3.162111282348633
Validation loss: 2.605170924176452

Epoch: 6| Step: 9
Training loss: 2.683120012283325
Validation loss: 2.6035819053649902

Epoch: 6| Step: 10
Training loss: 2.9617667198181152
Validation loss: 2.596080405737764

Epoch: 6| Step: 11
Training loss: 2.934736728668213
Validation loss: 2.586415321596207

Epoch: 6| Step: 12
Training loss: 2.8547401428222656
Validation loss: 2.575202598366686

Epoch: 6| Step: 13
Training loss: 1.6237157583236694
Validation loss: 2.572508035167571

Epoch: 41| Step: 0
Training loss: 2.5777955055236816
Validation loss: 2.5760664837334746

Epoch: 6| Step: 1
Training loss: 3.214005470275879
Validation loss: 2.575478951136271

Epoch: 6| Step: 2
Training loss: 3.7472362518310547
Validation loss: 2.579824821923369

Epoch: 6| Step: 3
Training loss: 2.579012632369995
Validation loss: 2.5871018440492692

Epoch: 6| Step: 4
Training loss: 2.542682647705078
Validation loss: 2.6003799130839687

Epoch: 6| Step: 5
Training loss: 2.611598491668701
Validation loss: 2.607312412672145

Epoch: 6| Step: 6
Training loss: 2.786935806274414
Validation loss: 2.6093242168426514

Epoch: 6| Step: 7
Training loss: 2.6034464836120605
Validation loss: 2.6003995992804088

Epoch: 6| Step: 8
Training loss: 3.28631329536438
Validation loss: 2.597014360530402

Epoch: 6| Step: 9
Training loss: 2.7524750232696533
Validation loss: 2.6013457211115028

Epoch: 6| Step: 10
Training loss: 2.714956760406494
Validation loss: 2.591156462187408

Epoch: 6| Step: 11
Training loss: 2.2228336334228516
Validation loss: 2.582728955053514

Epoch: 6| Step: 12
Training loss: 2.4922633171081543
Validation loss: 2.5766397163432133

Epoch: 6| Step: 13
Training loss: 2.2433440685272217
Validation loss: 2.573775235042777

Epoch: 42| Step: 0
Training loss: 3.543306827545166
Validation loss: 2.5741396770682385

Epoch: 6| Step: 1
Training loss: 2.6895570755004883
Validation loss: 2.5729827906495784

Epoch: 6| Step: 2
Training loss: 2.6564245223999023
Validation loss: 2.571656991076726

Epoch: 6| Step: 3
Training loss: 2.57798171043396
Validation loss: 2.5704972077441472

Epoch: 6| Step: 4
Training loss: 3.3875246047973633
Validation loss: 2.5677212463912142

Epoch: 6| Step: 5
Training loss: 3.2884464263916016
Validation loss: 2.5729925401749147

Epoch: 6| Step: 6
Training loss: 2.295980215072632
Validation loss: 2.572233884565292

Epoch: 6| Step: 7
Training loss: 2.652263641357422
Validation loss: 2.5706479498135146

Epoch: 6| Step: 8
Training loss: 1.6662952899932861
Validation loss: 2.574342922497821

Epoch: 6| Step: 9
Training loss: 3.718367576599121
Validation loss: 2.5766890459163214

Epoch: 6| Step: 10
Training loss: 3.444729804992676
Validation loss: 2.570479154586792

Epoch: 6| Step: 11
Training loss: 1.790344476699829
Validation loss: 2.5671947463866203

Epoch: 6| Step: 12
Training loss: 2.0904510021209717
Validation loss: 2.5631587889886673

Epoch: 6| Step: 13
Training loss: 2.574458360671997
Validation loss: 2.559603306554979

Epoch: 43| Step: 0
Training loss: 2.288107395172119
Validation loss: 2.5631188884858163

Epoch: 6| Step: 1
Training loss: 2.4579102993011475
Validation loss: 2.565649993958012

Epoch: 6| Step: 2
Training loss: 2.601492166519165
Validation loss: 2.5745215108317714

Epoch: 6| Step: 3
Training loss: 2.6328210830688477
Validation loss: 2.5659949574419247

Epoch: 6| Step: 4
Training loss: 3.0362424850463867
Validation loss: 2.563504239564301

Epoch: 6| Step: 5
Training loss: 3.5191941261291504
Validation loss: 2.560606692426948

Epoch: 6| Step: 6
Training loss: 2.9037749767303467
Validation loss: 2.563317665489771

Epoch: 6| Step: 7
Training loss: 2.615199565887451
Validation loss: 2.5650496175212245

Epoch: 6| Step: 8
Training loss: 2.4726967811584473
Validation loss: 2.5582994312368412

Epoch: 6| Step: 9
Training loss: 2.8867197036743164
Validation loss: 2.56187577145074

Epoch: 6| Step: 10
Training loss: 2.1658272743225098
Validation loss: 2.5597905651215584

Epoch: 6| Step: 11
Training loss: 2.8200631141662598
Validation loss: 2.5611076252434843

Epoch: 6| Step: 12
Training loss: 2.7300422191619873
Validation loss: 2.5633773675528904

Epoch: 6| Step: 13
Training loss: 3.771815061569214
Validation loss: 2.5703177144450526

Epoch: 44| Step: 0
Training loss: 2.5554420948028564
Validation loss: 2.574968958413729

Epoch: 6| Step: 1
Training loss: 2.4154727458953857
Validation loss: 2.5682355383391022

Epoch: 6| Step: 2
Training loss: 3.7965965270996094
Validation loss: 2.5678339055789414

Epoch: 6| Step: 3
Training loss: 3.13746976852417
Validation loss: 2.562387979158791

Epoch: 6| Step: 4
Training loss: 2.3120527267456055
Validation loss: 2.556318570208806

Epoch: 6| Step: 5
Training loss: 2.671346664428711
Validation loss: 2.5577062817030054

Epoch: 6| Step: 6
Training loss: 3.0673398971557617
Validation loss: 2.5542130137002594

Epoch: 6| Step: 7
Training loss: 2.3095264434814453
Validation loss: 2.553975559050037

Epoch: 6| Step: 8
Training loss: 2.3480124473571777
Validation loss: 2.554872525635586

Epoch: 6| Step: 9
Training loss: 3.2901535034179688
Validation loss: 2.556805287638018

Epoch: 6| Step: 10
Training loss: 2.3796191215515137
Validation loss: 2.5536667967355378

Epoch: 6| Step: 11
Training loss: 2.3184397220611572
Validation loss: 2.5550964006813626

Epoch: 6| Step: 12
Training loss: 3.156219959259033
Validation loss: 2.554692037643925

Epoch: 6| Step: 13
Training loss: 2.499589681625366
Validation loss: 2.553593932941396

Epoch: 45| Step: 0
Training loss: 2.3961760997772217
Validation loss: 2.555167487872544

Epoch: 6| Step: 1
Training loss: 2.671738862991333
Validation loss: 2.5546393189378964

Epoch: 6| Step: 2
Training loss: 2.2392983436584473
Validation loss: 2.556150628674415

Epoch: 6| Step: 3
Training loss: 2.6788136959075928
Validation loss: 2.554976501772481

Epoch: 6| Step: 4
Training loss: 3.0563578605651855
Validation loss: 2.5539292622638006

Epoch: 6| Step: 5
Training loss: 2.58303165435791
Validation loss: 2.556734777265979

Epoch: 6| Step: 6
Training loss: 2.6118881702423096
Validation loss: 2.5656221425661476

Epoch: 6| Step: 7
Training loss: 2.63966703414917
Validation loss: 2.5614608795412126

Epoch: 6| Step: 8
Training loss: 3.1691930294036865
Validation loss: 2.562885502333282

Epoch: 6| Step: 9
Training loss: 3.231837272644043
Validation loss: 2.5591999651283346

Epoch: 6| Step: 10
Training loss: 2.5107250213623047
Validation loss: 2.554843412932529

Epoch: 6| Step: 11
Training loss: 2.8814125061035156
Validation loss: 2.5519754912263606

Epoch: 6| Step: 12
Training loss: 2.3359813690185547
Validation loss: 2.5491342595828477

Epoch: 6| Step: 13
Training loss: 3.7287402153015137
Validation loss: 2.5507419211890108

Epoch: 46| Step: 0
Training loss: 2.6950643062591553
Validation loss: 2.546650307152861

Epoch: 6| Step: 1
Training loss: 2.9863834381103516
Validation loss: 2.5476999769928637

Epoch: 6| Step: 2
Training loss: 3.099435329437256
Validation loss: 2.5433787812468824

Epoch: 6| Step: 3
Training loss: 2.5819201469421387
Validation loss: 2.548370038309405

Epoch: 6| Step: 4
Training loss: 2.203361749649048
Validation loss: 2.5468688062442246

Epoch: 6| Step: 5
Training loss: 2.2389800548553467
Validation loss: 2.5489541305008756

Epoch: 6| Step: 6
Training loss: 2.259262800216675
Validation loss: 2.5534314327342535

Epoch: 6| Step: 7
Training loss: 2.1275599002838135
Validation loss: 2.557949399435392

Epoch: 6| Step: 8
Training loss: 2.2569189071655273
Validation loss: 2.5780232875577864

Epoch: 6| Step: 9
Training loss: 2.7953526973724365
Validation loss: 2.5916849413225727

Epoch: 6| Step: 10
Training loss: 3.315861225128174
Validation loss: 2.589352530817832

Epoch: 6| Step: 11
Training loss: 3.8527207374572754
Validation loss: 2.581189460651849

Epoch: 6| Step: 12
Training loss: 2.98598051071167
Validation loss: 2.568534884401547

Epoch: 6| Step: 13
Training loss: 3.060453176498413
Validation loss: 2.5510477583895446

Epoch: 47| Step: 0
Training loss: 2.4181065559387207
Validation loss: 2.548611041038267

Epoch: 6| Step: 1
Training loss: 2.5286808013916016
Validation loss: 2.54013386336706

Epoch: 6| Step: 2
Training loss: 2.756757974624634
Validation loss: 2.542053266238141

Epoch: 6| Step: 3
Training loss: 2.472499370574951
Validation loss: 2.541135390599569

Epoch: 6| Step: 4
Training loss: 2.4259085655212402
Validation loss: 2.540719950070945

Epoch: 6| Step: 5
Training loss: 2.737600088119507
Validation loss: 2.542546772187756

Epoch: 6| Step: 6
Training loss: 2.8550620079040527
Validation loss: 2.549662636172387

Epoch: 6| Step: 7
Training loss: 3.161882162094116
Validation loss: 2.557358277741299

Epoch: 6| Step: 8
Training loss: 2.2410669326782227
Validation loss: 2.5460751466853644

Epoch: 6| Step: 9
Training loss: 3.264427423477173
Validation loss: 2.5419784463861936

Epoch: 6| Step: 10
Training loss: 2.3242697715759277
Validation loss: 2.538146218945903

Epoch: 6| Step: 11
Training loss: 3.680048942565918
Validation loss: 2.5400447691640546

Epoch: 6| Step: 12
Training loss: 2.9568822383880615
Validation loss: 2.54135956815494

Epoch: 6| Step: 13
Training loss: 2.131502151489258
Validation loss: 2.5465887259411555

Epoch: 48| Step: 0
Training loss: 2.311516761779785
Validation loss: 2.5581477201113136

Epoch: 6| Step: 1
Training loss: 3.247680187225342
Validation loss: 2.558851782993604

Epoch: 6| Step: 2
Training loss: 2.7845540046691895
Validation loss: 2.5594442531626713

Epoch: 6| Step: 3
Training loss: 3.059068441390991
Validation loss: 2.5591178837642876

Epoch: 6| Step: 4
Training loss: 3.246568202972412
Validation loss: 2.5483557306310183

Epoch: 6| Step: 5
Training loss: 2.35398006439209
Validation loss: 2.541431712847884

Epoch: 6| Step: 6
Training loss: 2.3077893257141113
Validation loss: 2.5392631715343845

Epoch: 6| Step: 7
Training loss: 3.649035930633545
Validation loss: 2.5333469349850892

Epoch: 6| Step: 8
Training loss: 2.4921534061431885
Validation loss: 2.5336258308861845

Epoch: 6| Step: 9
Training loss: 3.0031239986419678
Validation loss: 2.5325772685389363

Epoch: 6| Step: 10
Training loss: 2.8116469383239746
Validation loss: 2.5329470352459977

Epoch: 6| Step: 11
Training loss: 2.4581339359283447
Validation loss: 2.5320642866114134

Epoch: 6| Step: 12
Training loss: 1.7315871715545654
Validation loss: 2.531495540372787

Epoch: 6| Step: 13
Training loss: 2.5743913650512695
Validation loss: 2.5318350689385527

Epoch: 49| Step: 0
Training loss: 2.012016773223877
Validation loss: 2.530028776455951

Epoch: 6| Step: 1
Training loss: 2.9746127128601074
Validation loss: 2.5300859302602787

Epoch: 6| Step: 2
Training loss: 2.1746938228607178
Validation loss: 2.5308462214726273

Epoch: 6| Step: 3
Training loss: 2.812422275543213
Validation loss: 2.529087366596345

Epoch: 6| Step: 4
Training loss: 2.8923499584198
Validation loss: 2.529707674057253

Epoch: 6| Step: 5
Training loss: 3.638212203979492
Validation loss: 2.531353278826642

Epoch: 6| Step: 6
Training loss: 2.412753105163574
Validation loss: 2.5324655732800885

Epoch: 6| Step: 7
Training loss: 3.1387484073638916
Validation loss: 2.5312060566358667

Epoch: 6| Step: 8
Training loss: 2.3411967754364014
Validation loss: 2.531271401272025

Epoch: 6| Step: 9
Training loss: 2.8168771266937256
Validation loss: 2.5321682781301518

Epoch: 6| Step: 10
Training loss: 2.090177536010742
Validation loss: 2.5319973089361705

Epoch: 6| Step: 11
Training loss: 3.085048198699951
Validation loss: 2.5363984159244004

Epoch: 6| Step: 12
Training loss: 3.0313143730163574
Validation loss: 2.537328049700747

Epoch: 6| Step: 13
Training loss: 2.4740445613861084
Validation loss: 2.5423915437472764

Epoch: 50| Step: 0
Training loss: 3.4066152572631836
Validation loss: 2.543547466237058

Epoch: 6| Step: 1
Training loss: 3.4364097118377686
Validation loss: 2.542309658501738

Epoch: 6| Step: 2
Training loss: 2.336268186569214
Validation loss: 2.5435134851804344

Epoch: 6| Step: 3
Training loss: 2.8447930812835693
Validation loss: 2.537945278229252

Epoch: 6| Step: 4
Training loss: 2.107541561126709
Validation loss: 2.540294519034765

Epoch: 6| Step: 5
Training loss: 2.5054073333740234
Validation loss: 2.5355380863271733

Epoch: 6| Step: 6
Training loss: 2.3938426971435547
Validation loss: 2.52978608428791

Epoch: 6| Step: 7
Training loss: 3.244856357574463
Validation loss: 2.5280273652845815

Epoch: 6| Step: 8
Training loss: 2.829679489135742
Validation loss: 2.526373960638559

Epoch: 6| Step: 9
Training loss: 2.7485527992248535
Validation loss: 2.525828999857749

Epoch: 6| Step: 10
Training loss: 2.583833694458008
Validation loss: 2.5279742056323635

Epoch: 6| Step: 11
Training loss: 2.52970552444458
Validation loss: 2.5275106558235745

Epoch: 6| Step: 12
Training loss: 2.6615965366363525
Validation loss: 2.530572142652286

Epoch: 6| Step: 13
Training loss: 2.186901807785034
Validation loss: 2.528362322879094

Epoch: 51| Step: 0
Training loss: 2.728717803955078
Validation loss: 2.528287769645773

Epoch: 6| Step: 1
Training loss: 3.1826109886169434
Validation loss: 2.5234870167188745

Epoch: 6| Step: 2
Training loss: 2.5529348850250244
Validation loss: 2.5233776492457234

Epoch: 6| Step: 3
Training loss: 2.3555960655212402
Validation loss: 2.5214413417282926

Epoch: 6| Step: 4
Training loss: 2.2956862449645996
Validation loss: 2.5234224565567507

Epoch: 6| Step: 5
Training loss: 2.5240180492401123
Validation loss: 2.527277374780306

Epoch: 6| Step: 6
Training loss: 2.4378857612609863
Validation loss: 2.5310150115720687

Epoch: 6| Step: 7
Training loss: 3.0792651176452637
Validation loss: 2.5286144056627826

Epoch: 6| Step: 8
Training loss: 2.5318996906280518
Validation loss: 2.5359505120144097

Epoch: 6| Step: 9
Training loss: 2.3116912841796875
Validation loss: 2.53234181609205

Epoch: 6| Step: 10
Training loss: 3.393007755279541
Validation loss: 2.5362345044330885

Epoch: 6| Step: 11
Training loss: 2.8411312103271484
Validation loss: 2.533847931892641

Epoch: 6| Step: 12
Training loss: 2.7948436737060547
Validation loss: 2.537541671465802

Epoch: 6| Step: 13
Training loss: 2.9214582443237305
Validation loss: 2.537312064119565

Epoch: 52| Step: 0
Training loss: 3.1893763542175293
Validation loss: 2.5315010214364655

Epoch: 6| Step: 1
Training loss: 2.0908126831054688
Validation loss: 2.5234167883473058

Epoch: 6| Step: 2
Training loss: 3.1869492530822754
Validation loss: 2.5177691751910793

Epoch: 6| Step: 3
Training loss: 2.6362013816833496
Validation loss: 2.518756625472858

Epoch: 6| Step: 4
Training loss: 2.282933235168457
Validation loss: 2.52086381758413

Epoch: 6| Step: 5
Training loss: 3.1923980712890625
Validation loss: 2.5211341611800657

Epoch: 6| Step: 6
Training loss: 3.2846994400024414
Validation loss: 2.5212710416445168

Epoch: 6| Step: 7
Training loss: 2.5878360271453857
Validation loss: 2.5218762479802614

Epoch: 6| Step: 8
Training loss: 2.8843913078308105
Validation loss: 2.520009348469396

Epoch: 6| Step: 9
Training loss: 2.532179355621338
Validation loss: 2.5221942676010953

Epoch: 6| Step: 10
Training loss: 1.7152149677276611
Validation loss: 2.5205728315537974

Epoch: 6| Step: 11
Training loss: 2.975339412689209
Validation loss: 2.520681245352632

Epoch: 6| Step: 12
Training loss: 2.509192943572998
Validation loss: 2.5161364309249388

Epoch: 6| Step: 13
Training loss: 3.104074239730835
Validation loss: 2.5182272144543227

Epoch: 53| Step: 0
Training loss: 2.4810590744018555
Validation loss: 2.518700227942518

Epoch: 6| Step: 1
Training loss: 3.6027934551239014
Validation loss: 2.5196667230257423

Epoch: 6| Step: 2
Training loss: 2.6658878326416016
Validation loss: 2.521832691725864

Epoch: 6| Step: 3
Training loss: 2.5485363006591797
Validation loss: 2.5221588047601844

Epoch: 6| Step: 4
Training loss: 2.9085397720336914
Validation loss: 2.5275214846416185

Epoch: 6| Step: 5
Training loss: 2.5337438583374023
Validation loss: 2.5374700125827583

Epoch: 6| Step: 6
Training loss: 2.7544193267822266
Validation loss: 2.541241597103816

Epoch: 6| Step: 7
Training loss: 2.843014717102051
Validation loss: 2.5359046818107687

Epoch: 6| Step: 8
Training loss: 2.8283469676971436
Validation loss: 2.531757544445735

Epoch: 6| Step: 9
Training loss: 2.6100621223449707
Validation loss: 2.5296143203653316

Epoch: 6| Step: 10
Training loss: 2.6312029361724854
Validation loss: 2.5219216603104786

Epoch: 6| Step: 11
Training loss: 3.1300346851348877
Validation loss: 2.5225355420061337

Epoch: 6| Step: 12
Training loss: 1.9001904726028442
Validation loss: 2.5195269994838263

Epoch: 6| Step: 13
Training loss: 2.266026496887207
Validation loss: 2.513032941408055

Epoch: 54| Step: 0
Training loss: 2.550151824951172
Validation loss: 2.518401858627155

Epoch: 6| Step: 1
Training loss: 3.12744140625
Validation loss: 2.5159863400202926

Epoch: 6| Step: 2
Training loss: 2.9928436279296875
Validation loss: 2.5155192523874264

Epoch: 6| Step: 3
Training loss: 3.6321539878845215
Validation loss: 2.5145112852896414

Epoch: 6| Step: 4
Training loss: 1.9548804759979248
Validation loss: 2.515530901570474

Epoch: 6| Step: 5
Training loss: 2.9716334342956543
Validation loss: 2.512012179179858

Epoch: 6| Step: 6
Training loss: 2.8902177810668945
Validation loss: 2.514328590003393

Epoch: 6| Step: 7
Training loss: 2.2293906211853027
Validation loss: 2.5180676060338176

Epoch: 6| Step: 8
Training loss: 1.3790135383605957
Validation loss: 2.516540442743609

Epoch: 6| Step: 9
Training loss: 3.2434494495391846
Validation loss: 2.5163775669631137

Epoch: 6| Step: 10
Training loss: 2.4144325256347656
Validation loss: 2.5211478048755276

Epoch: 6| Step: 11
Training loss: 3.0798938274383545
Validation loss: 2.520340588784987

Epoch: 6| Step: 12
Training loss: 2.9123334884643555
Validation loss: 2.516351253755631

Epoch: 6| Step: 13
Training loss: 2.1470956802368164
Validation loss: 2.52003752287998

Epoch: 55| Step: 0
Training loss: 2.199652671813965
Validation loss: 2.519660295978669

Epoch: 6| Step: 1
Training loss: 2.8445827960968018
Validation loss: 2.5182556670199157

Epoch: 6| Step: 2
Training loss: 2.3858542442321777
Validation loss: 2.5176727617940595

Epoch: 6| Step: 3
Training loss: 3.2551045417785645
Validation loss: 2.5206074714660645

Epoch: 6| Step: 4
Training loss: 2.8967103958129883
Validation loss: 2.513390556458504

Epoch: 6| Step: 5
Training loss: 2.9542183876037598
Validation loss: 2.5110247904254543

Epoch: 6| Step: 6
Training loss: 1.9501433372497559
Validation loss: 2.508835531050159

Epoch: 6| Step: 7
Training loss: 2.550999402999878
Validation loss: 2.509809440182101

Epoch: 6| Step: 8
Training loss: 3.4621098041534424
Validation loss: 2.507334839913153

Epoch: 6| Step: 9
Training loss: 2.5372612476348877
Validation loss: 2.505616357249598

Epoch: 6| Step: 10
Training loss: 2.5154998302459717
Validation loss: 2.507723923652403

Epoch: 6| Step: 11
Training loss: 2.801508665084839
Validation loss: 2.5092074486517135

Epoch: 6| Step: 12
Training loss: 2.9734911918640137
Validation loss: 2.5073320993813137

Epoch: 6| Step: 13
Training loss: 2.2192769050598145
Validation loss: 2.5097503764655

Epoch: 56| Step: 0
Training loss: 2.755234479904175
Validation loss: 2.509101431856873

Epoch: 6| Step: 1
Training loss: 2.6660544872283936
Validation loss: 2.507655989739203

Epoch: 6| Step: 2
Training loss: 2.6792654991149902
Validation loss: 2.507018514858779

Epoch: 6| Step: 3
Training loss: 2.504927635192871
Validation loss: 2.509201516387283

Epoch: 6| Step: 4
Training loss: 3.186105966567993
Validation loss: 2.515848949391355

Epoch: 6| Step: 5
Training loss: 2.7822940349578857
Validation loss: 2.5146656728559926

Epoch: 6| Step: 6
Training loss: 2.8516952991485596
Validation loss: 2.517080096788304

Epoch: 6| Step: 7
Training loss: 2.940310478210449
Validation loss: 2.5188025633494058

Epoch: 6| Step: 8
Training loss: 1.9614890813827515
Validation loss: 2.5123095384208103

Epoch: 6| Step: 9
Training loss: 2.4362120628356934
Validation loss: 2.5128531225265993

Epoch: 6| Step: 10
Training loss: 3.270030975341797
Validation loss: 2.509299942242202

Epoch: 6| Step: 11
Training loss: 2.5998241901397705
Validation loss: 2.5098232043686735

Epoch: 6| Step: 12
Training loss: 2.4065053462982178
Validation loss: 2.516318872410764

Epoch: 6| Step: 13
Training loss: 2.7132248878479004
Validation loss: 2.5159720425964682

Epoch: 57| Step: 0
Training loss: 2.2995245456695557
Validation loss: 2.536708693350515

Epoch: 6| Step: 1
Training loss: 2.4159300327301025
Validation loss: 2.5393759563405025

Epoch: 6| Step: 2
Training loss: 2.9144649505615234
Validation loss: 2.5372253284659436

Epoch: 6| Step: 3
Training loss: 2.311016082763672
Validation loss: 2.5317842191265476

Epoch: 6| Step: 4
Training loss: 2.81129789352417
Validation loss: 2.537589996091781

Epoch: 6| Step: 5
Training loss: 2.5116522312164307
Validation loss: 2.519729814221782

Epoch: 6| Step: 6
Training loss: 2.596369981765747
Validation loss: 2.504750144097113

Epoch: 6| Step: 7
Training loss: 2.7826123237609863
Validation loss: 2.501132260086716

Epoch: 6| Step: 8
Training loss: 2.639199733734131
Validation loss: 2.5050844453996226

Epoch: 6| Step: 9
Training loss: 2.6476056575775146
Validation loss: 2.506308450493761

Epoch: 6| Step: 10
Training loss: 3.4768524169921875
Validation loss: 2.5071756557751725

Epoch: 6| Step: 11
Training loss: 2.467698574066162
Validation loss: 2.5159816485579296

Epoch: 6| Step: 12
Training loss: 2.725794792175293
Validation loss: 2.5072053119700444

Epoch: 6| Step: 13
Training loss: 3.733548402786255
Validation loss: 2.5025710187932497

Epoch: 58| Step: 0
Training loss: 2.8908934593200684
Validation loss: 2.503103099843507

Epoch: 6| Step: 1
Training loss: 2.2286672592163086
Validation loss: 2.504208941613474

Epoch: 6| Step: 2
Training loss: 2.6030502319335938
Validation loss: 2.5139277365899857

Epoch: 6| Step: 3
Training loss: 2.365849733352661
Validation loss: 2.5312839323474514

Epoch: 6| Step: 4
Training loss: 3.2005581855773926
Validation loss: 2.548659268245902

Epoch: 6| Step: 5
Training loss: 3.0352730751037598
Validation loss: 2.5482510648747927

Epoch: 6| Step: 6
Training loss: 2.9345898628234863
Validation loss: 2.54249099762209

Epoch: 6| Step: 7
Training loss: 3.1129653453826904
Validation loss: 2.5172055229063957

Epoch: 6| Step: 8
Training loss: 2.5335912704467773
Validation loss: 2.5051232281551568

Epoch: 6| Step: 9
Training loss: 3.2999348640441895
Validation loss: 2.5011612933169127

Epoch: 6| Step: 10
Training loss: 2.3586442470550537
Validation loss: 2.5026705290681575

Epoch: 6| Step: 11
Training loss: 2.465399742126465
Validation loss: 2.498693479004727

Epoch: 6| Step: 12
Training loss: 2.3973641395568848
Validation loss: 2.500860029651273

Epoch: 6| Step: 13
Training loss: 2.0270893573760986
Validation loss: 2.508586727162843

Epoch: 59| Step: 0
Training loss: 1.8300966024398804
Validation loss: 2.4958135056239303

Epoch: 6| Step: 1
Training loss: 3.113379955291748
Validation loss: 2.494783091288741

Epoch: 6| Step: 2
Training loss: 2.9811134338378906
Validation loss: 2.4930809518342376

Epoch: 6| Step: 3
Training loss: 2.681563377380371
Validation loss: 2.494893140690301

Epoch: 6| Step: 4
Training loss: 3.039111614227295
Validation loss: 2.494561315864645

Epoch: 6| Step: 5
Training loss: 2.4327189922332764
Validation loss: 2.5046322422642864

Epoch: 6| Step: 6
Training loss: 3.2710111141204834
Validation loss: 2.5071323610121206

Epoch: 6| Step: 7
Training loss: 3.2180140018463135
Validation loss: 2.5097087839598298

Epoch: 6| Step: 8
Training loss: 2.802133560180664
Validation loss: 2.5023495843333583

Epoch: 6| Step: 9
Training loss: 2.3771519660949707
Validation loss: 2.511945968033165

Epoch: 6| Step: 10
Training loss: 2.8658270835876465
Validation loss: 2.526284340889223

Epoch: 6| Step: 11
Training loss: 1.768698811531067
Validation loss: 2.54959931424869

Epoch: 6| Step: 12
Training loss: 2.4489221572875977
Validation loss: 2.573590063279675

Epoch: 6| Step: 13
Training loss: 3.314678430557251
Validation loss: 2.585449357186594

Epoch: 60| Step: 0
Training loss: 2.113064765930176
Validation loss: 2.552363895600842

Epoch: 6| Step: 1
Training loss: 2.907406806945801
Validation loss: 2.5309040520780828

Epoch: 6| Step: 2
Training loss: 3.3303921222686768
Validation loss: 2.5013191366708405

Epoch: 6| Step: 3
Training loss: 3.2075881958007812
Validation loss: 2.4925444382493214

Epoch: 6| Step: 4
Training loss: 2.1309943199157715
Validation loss: 2.4851084819404026

Epoch: 6| Step: 5
Training loss: 3.5288867950439453
Validation loss: 2.4866113098718787

Epoch: 6| Step: 6
Training loss: 2.7505898475646973
Validation loss: 2.4904829763597056

Epoch: 6| Step: 7
Training loss: 2.465095043182373
Validation loss: 2.501618121259956

Epoch: 6| Step: 8
Training loss: 2.6709814071655273
Validation loss: 2.513964896560997

Epoch: 6| Step: 9
Training loss: 2.5105581283569336
Validation loss: 2.530162852297547

Epoch: 6| Step: 10
Training loss: 2.8425674438476562
Validation loss: 2.526519952281829

Epoch: 6| Step: 11
Training loss: 2.4702577590942383
Validation loss: 2.5232175165607083

Epoch: 6| Step: 12
Training loss: 2.294419288635254
Validation loss: 2.518385071908274

Epoch: 6| Step: 13
Training loss: 2.6524674892425537
Validation loss: 2.510065278699321

Epoch: 61| Step: 0
Training loss: 2.503779411315918
Validation loss: 2.506457656942388

Epoch: 6| Step: 1
Training loss: 2.2355828285217285
Validation loss: 2.498359336647936

Epoch: 6| Step: 2
Training loss: 2.765956401824951
Validation loss: 2.4862870606043006

Epoch: 6| Step: 3
Training loss: 2.669334888458252
Validation loss: 2.4901274045308432

Epoch: 6| Step: 4
Training loss: 2.506251811981201
Validation loss: 2.4943202310992825

Epoch: 6| Step: 5
Training loss: 2.881622314453125
Validation loss: 2.50350248172719

Epoch: 6| Step: 6
Training loss: 3.018840789794922
Validation loss: 2.5015867422985774

Epoch: 6| Step: 7
Training loss: 1.7592136859893799
Validation loss: 2.4972731836380495

Epoch: 6| Step: 8
Training loss: 2.654539108276367
Validation loss: 2.4986651789757515

Epoch: 6| Step: 9
Training loss: 3.079456090927124
Validation loss: 2.4927172840282483

Epoch: 6| Step: 10
Training loss: 3.451005458831787
Validation loss: 2.486118898596815

Epoch: 6| Step: 11
Training loss: 2.277914524078369
Validation loss: 2.4838771922613985

Epoch: 6| Step: 12
Training loss: 2.817172050476074
Validation loss: 2.4803624870956584

Epoch: 6| Step: 13
Training loss: 3.3025004863739014
Validation loss: 2.4783000074407107

Epoch: 62| Step: 0
Training loss: 2.4406018257141113
Validation loss: 2.4782800418074413

Epoch: 6| Step: 1
Training loss: 3.145827054977417
Validation loss: 2.475572322004585

Epoch: 6| Step: 2
Training loss: 1.8770384788513184
Validation loss: 2.4808524424029934

Epoch: 6| Step: 3
Training loss: 2.874030113220215
Validation loss: 2.4811357990387948

Epoch: 6| Step: 4
Training loss: 3.147697925567627
Validation loss: 2.4879430852910525

Epoch: 6| Step: 5
Training loss: 2.2900404930114746
Validation loss: 2.4922608739586285

Epoch: 6| Step: 6
Training loss: 2.780214786529541
Validation loss: 2.4897314886892996

Epoch: 6| Step: 7
Training loss: 2.998056411743164
Validation loss: 2.4850765197507796

Epoch: 6| Step: 8
Training loss: 2.543248176574707
Validation loss: 2.4799237610191427

Epoch: 6| Step: 9
Training loss: 2.3561289310455322
Validation loss: 2.4774803807658534

Epoch: 6| Step: 10
Training loss: 3.27059006690979
Validation loss: 2.476589423353954

Epoch: 6| Step: 11
Training loss: 2.7342581748962402
Validation loss: 2.4724113325918875

Epoch: 6| Step: 12
Training loss: 2.6449742317199707
Validation loss: 2.472916099332994

Epoch: 6| Step: 13
Training loss: 2.4454753398895264
Validation loss: 2.480109865947436

Epoch: 63| Step: 0
Training loss: 3.117330551147461
Validation loss: 2.4899011965720885

Epoch: 6| Step: 1
Training loss: 2.8576650619506836
Validation loss: 2.500162457907072

Epoch: 6| Step: 2
Training loss: 3.3568649291992188
Validation loss: 2.5007310528909006

Epoch: 6| Step: 3
Training loss: 3.0070602893829346
Validation loss: 2.5059986781048518

Epoch: 6| Step: 4
Training loss: 2.2711341381073
Validation loss: 2.4970724044307584

Epoch: 6| Step: 5
Training loss: 2.7367711067199707
Validation loss: 2.4911949121823875

Epoch: 6| Step: 6
Training loss: 2.661121129989624
Validation loss: 2.469794004194198

Epoch: 6| Step: 7
Training loss: 2.593613386154175
Validation loss: 2.4675410332218295

Epoch: 6| Step: 8
Training loss: 2.875704288482666
Validation loss: 2.466301397610736

Epoch: 6| Step: 9
Training loss: 2.6772537231445312
Validation loss: 2.466138752557898

Epoch: 6| Step: 10
Training loss: 2.405998945236206
Validation loss: 2.4665026382733415

Epoch: 6| Step: 11
Training loss: 2.0908303260803223
Validation loss: 2.4736908815240346

Epoch: 6| Step: 12
Training loss: 2.0363097190856934
Validation loss: 2.470154869940973

Epoch: 6| Step: 13
Training loss: 2.9742507934570312
Validation loss: 2.4711408230566208

Epoch: 64| Step: 0
Training loss: 3.0338263511657715
Validation loss: 2.4647585807308072

Epoch: 6| Step: 1
Training loss: 2.7042219638824463
Validation loss: 2.4685933051570768

Epoch: 6| Step: 2
Training loss: 1.8777960538864136
Validation loss: 2.462280906656737

Epoch: 6| Step: 3
Training loss: 2.404681921005249
Validation loss: 2.4656123063897573

Epoch: 6| Step: 4
Training loss: 2.4194655418395996
Validation loss: 2.468519041615148

Epoch: 6| Step: 5
Training loss: 2.9204635620117188
Validation loss: 2.4787147557863625

Epoch: 6| Step: 6
Training loss: 3.657926321029663
Validation loss: 2.473639319019933

Epoch: 6| Step: 7
Training loss: 2.6999688148498535
Validation loss: 2.4706703667999594

Epoch: 6| Step: 8
Training loss: 2.5241682529449463
Validation loss: 2.4619457747346614

Epoch: 6| Step: 9
Training loss: 2.5937719345092773
Validation loss: 2.4575041955517185

Epoch: 6| Step: 10
Training loss: 2.5133519172668457
Validation loss: 2.4577638641480477

Epoch: 6| Step: 11
Training loss: 3.3436479568481445
Validation loss: 2.4653987602521013

Epoch: 6| Step: 12
Training loss: 2.6144542694091797
Validation loss: 2.473482842086464

Epoch: 6| Step: 13
Training loss: 2.0052456855773926
Validation loss: 2.4916435108389905

Epoch: 65| Step: 0
Training loss: 3.9391908645629883
Validation loss: 2.485456830711775

Epoch: 6| Step: 1
Training loss: 3.238800287246704
Validation loss: 2.4894947826221423

Epoch: 6| Step: 2
Training loss: 2.9034624099731445
Validation loss: 2.4844685600649927

Epoch: 6| Step: 3
Training loss: 2.3055052757263184
Validation loss: 2.474891616452125

Epoch: 6| Step: 4
Training loss: 2.487339973449707
Validation loss: 2.4635817081697526

Epoch: 6| Step: 5
Training loss: 2.8149712085723877
Validation loss: 2.463154464639643

Epoch: 6| Step: 6
Training loss: 2.5569026470184326
Validation loss: 2.4585189691153904

Epoch: 6| Step: 7
Training loss: 2.9428813457489014
Validation loss: 2.4582643329456286

Epoch: 6| Step: 8
Training loss: 2.604684591293335
Validation loss: 2.4592719898428967

Epoch: 6| Step: 9
Training loss: 2.85943865776062
Validation loss: 2.466494634587278

Epoch: 6| Step: 10
Training loss: 2.2843337059020996
Validation loss: 2.4686400582713466

Epoch: 6| Step: 11
Training loss: 2.414860725402832
Validation loss: 2.470534670737482

Epoch: 6| Step: 12
Training loss: 1.7282812595367432
Validation loss: 2.4653900502830424

Epoch: 6| Step: 13
Training loss: 2.133302927017212
Validation loss: 2.4649704861384567

Epoch: 66| Step: 0
Training loss: 2.6951096057891846
Validation loss: 2.4677276688237346

Epoch: 6| Step: 1
Training loss: 2.68733549118042
Validation loss: 2.463609585198023

Epoch: 6| Step: 2
Training loss: 2.507312297821045
Validation loss: 2.4604007274873796

Epoch: 6| Step: 3
Training loss: 2.914177417755127
Validation loss: 2.45785923927061

Epoch: 6| Step: 4
Training loss: 2.4470183849334717
Validation loss: 2.4589267905040453

Epoch: 6| Step: 5
Training loss: 3.040541648864746
Validation loss: 2.4530923828001945

Epoch: 6| Step: 6
Training loss: 2.5862441062927246
Validation loss: 2.456460750231179

Epoch: 6| Step: 7
Training loss: 2.047178268432617
Validation loss: 2.455894503542172

Epoch: 6| Step: 8
Training loss: 2.33915114402771
Validation loss: 2.45417445705783

Epoch: 6| Step: 9
Training loss: 2.9874379634857178
Validation loss: 2.456305924282279

Epoch: 6| Step: 10
Training loss: 2.9327101707458496
Validation loss: 2.4558539364927556

Epoch: 6| Step: 11
Training loss: 2.294987678527832
Validation loss: 2.4546326642395346

Epoch: 6| Step: 12
Training loss: 2.851041555404663
Validation loss: 2.4553756021684214

Epoch: 6| Step: 13
Training loss: 3.428412437438965
Validation loss: 2.4586549189782914

Epoch: 67| Step: 0
Training loss: 2.9458348751068115
Validation loss: 2.4595695080295688

Epoch: 6| Step: 1
Training loss: 2.260415554046631
Validation loss: 2.459789283813969

Epoch: 6| Step: 2
Training loss: 3.5553231239318848
Validation loss: 2.4710839666346067

Epoch: 6| Step: 3
Training loss: 1.6283001899719238
Validation loss: 2.483152151107788

Epoch: 6| Step: 4
Training loss: 1.77023446559906
Validation loss: 2.4881391679086993

Epoch: 6| Step: 5
Training loss: 2.729877471923828
Validation loss: 2.496217263642178

Epoch: 6| Step: 6
Training loss: 2.9786453247070312
Validation loss: 2.483614952333512

Epoch: 6| Step: 7
Training loss: 3.611894130706787
Validation loss: 2.4663669729745514

Epoch: 6| Step: 8
Training loss: 3.354865074157715
Validation loss: 2.4596334964998308

Epoch: 6| Step: 9
Training loss: 2.301894426345825
Validation loss: 2.459236273201563

Epoch: 6| Step: 10
Training loss: 2.4801206588745117
Validation loss: 2.4536601189644105

Epoch: 6| Step: 11
Training loss: 2.1040961742401123
Validation loss: 2.453401998807025

Epoch: 6| Step: 12
Training loss: 2.78212833404541
Validation loss: 2.4562364803847445

Epoch: 6| Step: 13
Training loss: 2.989003896713257
Validation loss: 2.4557235497300343

Epoch: 68| Step: 0
Training loss: 2.8685708045959473
Validation loss: 2.4533014400030977

Epoch: 6| Step: 1
Training loss: 2.85304594039917
Validation loss: 2.450872057227678

Epoch: 6| Step: 2
Training loss: 2.992662191390991
Validation loss: 2.456779267198296

Epoch: 6| Step: 3
Training loss: 2.6040711402893066
Validation loss: 2.4651908413056405

Epoch: 6| Step: 4
Training loss: 2.59375
Validation loss: 2.4624205917440434

Epoch: 6| Step: 5
Training loss: 2.5711159706115723
Validation loss: 2.4560436689725487

Epoch: 6| Step: 6
Training loss: 2.277387857437134
Validation loss: 2.4609280222205707

Epoch: 6| Step: 7
Training loss: 2.0108203887939453
Validation loss: 2.4571655104237218

Epoch: 6| Step: 8
Training loss: 2.3663086891174316
Validation loss: 2.447515144143053

Epoch: 6| Step: 9
Training loss: 2.820169448852539
Validation loss: 2.4469429651896157

Epoch: 6| Step: 10
Training loss: 2.9524624347686768
Validation loss: 2.448919270628242

Epoch: 6| Step: 11
Training loss: 2.569239377975464
Validation loss: 2.447721609505274

Epoch: 6| Step: 12
Training loss: 2.7387897968292236
Validation loss: 2.4448547619645313

Epoch: 6| Step: 13
Training loss: 3.347611665725708
Validation loss: 2.4500012884857836

Epoch: 69| Step: 0
Training loss: 2.1320104598999023
Validation loss: 2.447178820128082

Epoch: 6| Step: 1
Training loss: 2.7939224243164062
Validation loss: 2.4465369537312496

Epoch: 6| Step: 2
Training loss: 2.255016326904297
Validation loss: 2.4508831757371143

Epoch: 6| Step: 3
Training loss: 2.8260116577148438
Validation loss: 2.4516637325286865

Epoch: 6| Step: 4
Training loss: 2.9057791233062744
Validation loss: 2.451867352249802

Epoch: 6| Step: 5
Training loss: 3.1172749996185303
Validation loss: 2.4489798545837402

Epoch: 6| Step: 6
Training loss: 2.356433868408203
Validation loss: 2.4489578354743218

Epoch: 6| Step: 7
Training loss: 1.9362106323242188
Validation loss: 2.4507287086979037

Epoch: 6| Step: 8
Training loss: 2.9827117919921875
Validation loss: 2.445355438417004

Epoch: 6| Step: 9
Training loss: 3.2069315910339355
Validation loss: 2.4461137479351414

Epoch: 6| Step: 10
Training loss: 3.05711030960083
Validation loss: 2.4445210169720393

Epoch: 6| Step: 11
Training loss: 2.826127052307129
Validation loss: 2.445824028343283

Epoch: 6| Step: 12
Training loss: 2.091959238052368
Validation loss: 2.442185887726404

Epoch: 6| Step: 13
Training loss: 2.8444807529449463
Validation loss: 2.4426083462212675

Epoch: 70| Step: 0
Training loss: 2.4877963066101074
Validation loss: 2.4444078066015757

Epoch: 6| Step: 1
Training loss: 2.677034854888916
Validation loss: 2.4431836220525924

Epoch: 6| Step: 2
Training loss: 2.311800479888916
Validation loss: 2.4377223137886292

Epoch: 6| Step: 3
Training loss: 3.879373073577881
Validation loss: 2.441488166009226

Epoch: 6| Step: 4
Training loss: 2.5759129524230957
Validation loss: 2.4448400902491745

Epoch: 6| Step: 5
Training loss: 2.08794903755188
Validation loss: 2.4555894072337816

Epoch: 6| Step: 6
Training loss: 2.2443556785583496
Validation loss: 2.4526309403040076

Epoch: 6| Step: 7
Training loss: 2.8448379039764404
Validation loss: 2.459350016809279

Epoch: 6| Step: 8
Training loss: 2.6900012493133545
Validation loss: 2.4594445331122285

Epoch: 6| Step: 9
Training loss: 2.330143451690674
Validation loss: 2.4593569950390886

Epoch: 6| Step: 10
Training loss: 3.031416416168213
Validation loss: 2.4500970737908476

Epoch: 6| Step: 11
Training loss: 2.3066658973693848
Validation loss: 2.452722131565053

Epoch: 6| Step: 12
Training loss: 3.3601582050323486
Validation loss: 2.4594705207373506

Epoch: 6| Step: 13
Training loss: 2.1854212284088135
Validation loss: 2.449834031443442

Epoch: 71| Step: 0
Training loss: 1.642927646636963
Validation loss: 2.4371452690452657

Epoch: 6| Step: 1
Training loss: 3.5053606033325195
Validation loss: 2.4335712386715795

Epoch: 6| Step: 2
Training loss: 3.6894195079803467
Validation loss: 2.437542808953152

Epoch: 6| Step: 3
Training loss: 2.496297836303711
Validation loss: 2.441603458055886

Epoch: 6| Step: 4
Training loss: 3.110701084136963
Validation loss: 2.447892130062144

Epoch: 6| Step: 5
Training loss: 2.395540952682495
Validation loss: 2.4596095777327016

Epoch: 6| Step: 6
Training loss: 3.0122036933898926
Validation loss: 2.4621666567299956

Epoch: 6| Step: 7
Training loss: 2.6163244247436523
Validation loss: 2.4539327262550272

Epoch: 6| Step: 8
Training loss: 2.110626220703125
Validation loss: 2.4420049575067337

Epoch: 6| Step: 9
Training loss: 2.409173011779785
Validation loss: 2.4361889439244426

Epoch: 6| Step: 10
Training loss: 2.443711280822754
Validation loss: 2.431160778127691

Epoch: 6| Step: 11
Training loss: 3.10168194770813
Validation loss: 2.42837207804444

Epoch: 6| Step: 12
Training loss: 2.4177968502044678
Validation loss: 2.431794520347349

Epoch: 6| Step: 13
Training loss: 2.310415506362915
Validation loss: 2.4332131442203315

Epoch: 72| Step: 0
Training loss: 2.5823445320129395
Validation loss: 2.4343493253953996

Epoch: 6| Step: 1
Training loss: 2.6394782066345215
Validation loss: 2.4329162489983345

Epoch: 6| Step: 2
Training loss: 2.613607406616211
Validation loss: 2.4399500329007386

Epoch: 6| Step: 3
Training loss: 2.3472518920898438
Validation loss: 2.439025125195903

Epoch: 6| Step: 4
Training loss: 2.3680238723754883
Validation loss: 2.4397242043607976

Epoch: 6| Step: 5
Training loss: 2.514526605606079
Validation loss: 2.4406188585424937

Epoch: 6| Step: 6
Training loss: 3.1064882278442383
Validation loss: 2.4544360317209715

Epoch: 6| Step: 7
Training loss: 2.042337417602539
Validation loss: 2.4540873291671916

Epoch: 6| Step: 8
Training loss: 2.9870479106903076
Validation loss: 2.4559273617241972

Epoch: 6| Step: 9
Training loss: 3.465639114379883
Validation loss: 2.4539217308003414

Epoch: 6| Step: 10
Training loss: 2.409457206726074
Validation loss: 2.4429630669214393

Epoch: 6| Step: 11
Training loss: 2.4631600379943848
Validation loss: 2.428641644857263

Epoch: 6| Step: 12
Training loss: 2.6763548851013184
Validation loss: 2.4295378628597466

Epoch: 6| Step: 13
Training loss: 3.27527117729187
Validation loss: 2.4321361254620295

Epoch: 73| Step: 0
Training loss: 3.2497572898864746
Validation loss: 2.429206181597966

Epoch: 6| Step: 1
Training loss: 2.6995766162872314
Validation loss: 2.4225479415667954

Epoch: 6| Step: 2
Training loss: 2.6036417484283447
Validation loss: 2.4243293128987795

Epoch: 6| Step: 3
Training loss: 3.0056304931640625
Validation loss: 2.424178595183998

Epoch: 6| Step: 4
Training loss: 2.9339919090270996
Validation loss: 2.4240657488505044

Epoch: 6| Step: 5
Training loss: 2.184138536453247
Validation loss: 2.4209423706095707

Epoch: 6| Step: 6
Training loss: 2.0442068576812744
Validation loss: 2.422986404870146

Epoch: 6| Step: 7
Training loss: 2.4588887691497803
Validation loss: 2.423159086576072

Epoch: 6| Step: 8
Training loss: 2.759251117706299
Validation loss: 2.420104185740153

Epoch: 6| Step: 9
Training loss: 2.580810546875
Validation loss: 2.4245892673410396

Epoch: 6| Step: 10
Training loss: 2.442197322845459
Validation loss: 2.4295158309321248

Epoch: 6| Step: 11
Training loss: 2.3662829399108887
Validation loss: 2.43429906393892

Epoch: 6| Step: 12
Training loss: 3.223423480987549
Validation loss: 2.438936132256703

Epoch: 6| Step: 13
Training loss: 2.4991064071655273
Validation loss: 2.4399034720595165

Epoch: 74| Step: 0
Training loss: 2.6700820922851562
Validation loss: 2.444038406495125

Epoch: 6| Step: 1
Training loss: 2.897625684738159
Validation loss: 2.4399232633652224

Epoch: 6| Step: 2
Training loss: 2.2995753288269043
Validation loss: 2.4410834837985296

Epoch: 6| Step: 3
Training loss: 3.010114908218384
Validation loss: 2.4462508668181715

Epoch: 6| Step: 4
Training loss: 2.4182467460632324
Validation loss: 2.446402371570628

Epoch: 6| Step: 5
Training loss: 3.095254421234131
Validation loss: 2.4576747494359172

Epoch: 6| Step: 6
Training loss: 3.2167716026306152
Validation loss: 2.440432015285697

Epoch: 6| Step: 7
Training loss: 2.5711307525634766
Validation loss: 2.4321845731427594

Epoch: 6| Step: 8
Training loss: 2.3482556343078613
Validation loss: 2.4264668700515584

Epoch: 6| Step: 9
Training loss: 2.798745632171631
Validation loss: 2.425212514015936

Epoch: 6| Step: 10
Training loss: 1.8775155544281006
Validation loss: 2.420723775381683

Epoch: 6| Step: 11
Training loss: 2.5823071002960205
Validation loss: 2.420671083593881

Epoch: 6| Step: 12
Training loss: 2.5478858947753906
Validation loss: 2.420361436823363

Epoch: 6| Step: 13
Training loss: 3.0529868602752686
Validation loss: 2.418177104765369

Epoch: 75| Step: 0
Training loss: 2.459559440612793
Validation loss: 2.4229225574001187

Epoch: 6| Step: 1
Training loss: 2.967357635498047
Validation loss: 2.4213637792935936

Epoch: 6| Step: 2
Training loss: 3.1568870544433594
Validation loss: 2.4262506256821337

Epoch: 6| Step: 3
Training loss: 3.109320640563965
Validation loss: 2.4307064881888767

Epoch: 6| Step: 4
Training loss: 2.5295262336730957
Validation loss: 2.436661066547517

Epoch: 6| Step: 5
Training loss: 2.3170862197875977
Validation loss: 2.436161051514328

Epoch: 6| Step: 6
Training loss: 3.374098777770996
Validation loss: 2.4263738214328723

Epoch: 6| Step: 7
Training loss: 2.9820637702941895
Validation loss: 2.424359142139394

Epoch: 6| Step: 8
Training loss: 2.8648486137390137
Validation loss: 2.425751873241958

Epoch: 6| Step: 9
Training loss: 2.703779935836792
Validation loss: 2.419593072706653

Epoch: 6| Step: 10
Training loss: 2.629603624343872
Validation loss: 2.4190167970554803

Epoch: 6| Step: 11
Training loss: 1.9558749198913574
Validation loss: 2.4186905955755584

Epoch: 6| Step: 12
Training loss: 2.0137624740600586
Validation loss: 2.41817819687628

Epoch: 6| Step: 13
Training loss: 1.5495831966400146
Validation loss: 2.4181288185939995

Epoch: 76| Step: 0
Training loss: 2.146956443786621
Validation loss: 2.4194406950345604

Epoch: 6| Step: 1
Training loss: 2.5270166397094727
Validation loss: 2.4201283198530956

Epoch: 6| Step: 2
Training loss: 2.743992328643799
Validation loss: 2.4184744563153995

Epoch: 6| Step: 3
Training loss: 2.151341676712036
Validation loss: 2.4167381281493814

Epoch: 6| Step: 4
Training loss: 3.077519655227661
Validation loss: 2.418093358316729

Epoch: 6| Step: 5
Training loss: 3.156651735305786
Validation loss: 2.4141198255682506

Epoch: 6| Step: 6
Training loss: 2.4032883644104004
Validation loss: 2.4186642708316928

Epoch: 6| Step: 7
Training loss: 3.3066859245300293
Validation loss: 2.4138196283771145

Epoch: 6| Step: 8
Training loss: 1.7529278993606567
Validation loss: 2.4169814868639876

Epoch: 6| Step: 9
Training loss: 3.3647711277008057
Validation loss: 2.422957007602979

Epoch: 6| Step: 10
Training loss: 2.1487984657287598
Validation loss: 2.424002811472903

Epoch: 6| Step: 11
Training loss: 2.9173808097839355
Validation loss: 2.432785980163082

Epoch: 6| Step: 12
Training loss: 2.6619279384613037
Validation loss: 2.4464857757732434

Epoch: 6| Step: 13
Training loss: 2.7505922317504883
Validation loss: 2.4506878314479703

Epoch: 77| Step: 0
Training loss: 3.4316494464874268
Validation loss: 2.4569767854546987

Epoch: 6| Step: 1
Training loss: 2.7832510471343994
Validation loss: 2.4656864699497016

Epoch: 6| Step: 2
Training loss: 2.6987881660461426
Validation loss: 2.469139640049268

Epoch: 6| Step: 3
Training loss: 2.79110050201416
Validation loss: 2.4542048361993607

Epoch: 6| Step: 4
Training loss: 2.3219926357269287
Validation loss: 2.443413652399535

Epoch: 6| Step: 5
Training loss: 2.8429481983184814
Validation loss: 2.4309662516399095

Epoch: 6| Step: 6
Training loss: 2.4805548191070557
Validation loss: 2.4297501092316

Epoch: 6| Step: 7
Training loss: 3.2044713497161865
Validation loss: 2.417280617580619

Epoch: 6| Step: 8
Training loss: 2.834672212600708
Validation loss: 2.411286343810379

Epoch: 6| Step: 9
Training loss: 2.105374813079834
Validation loss: 2.410845156638853

Epoch: 6| Step: 10
Training loss: 2.5370328426361084
Validation loss: 2.4105825860013246

Epoch: 6| Step: 11
Training loss: 2.5351743698120117
Validation loss: 2.411890393944197

Epoch: 6| Step: 12
Training loss: 2.271027088165283
Validation loss: 2.4080373523055867

Epoch: 6| Step: 13
Training loss: 2.2133631706237793
Validation loss: 2.41343032416477

Epoch: 78| Step: 0
Training loss: 2.585555076599121
Validation loss: 2.4118363088177097

Epoch: 6| Step: 1
Training loss: 2.413702964782715
Validation loss: 2.413272656420226

Epoch: 6| Step: 2
Training loss: 2.7793874740600586
Validation loss: 2.416094762022777

Epoch: 6| Step: 3
Training loss: 1.9252804517745972
Validation loss: 2.416790818655363

Epoch: 6| Step: 4
Training loss: 3.557830333709717
Validation loss: 2.418313544283631

Epoch: 6| Step: 5
Training loss: 2.5018272399902344
Validation loss: 2.4220885769013436

Epoch: 6| Step: 6
Training loss: 3.1114706993103027
Validation loss: 2.4231581893018497

Epoch: 6| Step: 7
Training loss: 2.8407983779907227
Validation loss: 2.4235367877509004

Epoch: 6| Step: 8
Training loss: 2.5313880443573
Validation loss: 2.427928340050482

Epoch: 6| Step: 9
Training loss: 2.891543388366699
Validation loss: 2.4289673605272846

Epoch: 6| Step: 10
Training loss: 2.1143808364868164
Validation loss: 2.4202254843968216

Epoch: 6| Step: 11
Training loss: 2.658926010131836
Validation loss: 2.422107752933297

Epoch: 6| Step: 12
Training loss: 2.3566367626190186
Validation loss: 2.413952189107095

Epoch: 6| Step: 13
Training loss: 2.8088362216949463
Validation loss: 2.4148346993231002

Epoch: 79| Step: 0
Training loss: 2.2770333290100098
Validation loss: 2.420127227742185

Epoch: 6| Step: 1
Training loss: 2.5842719078063965
Validation loss: 2.4260009719479467

Epoch: 6| Step: 2
Training loss: 2.5228724479675293
Validation loss: 2.434311371977611

Epoch: 6| Step: 3
Training loss: 2.3747668266296387
Validation loss: 2.445099566572456

Epoch: 6| Step: 4
Training loss: 3.07332706451416
Validation loss: 2.4486602813966813

Epoch: 6| Step: 5
Training loss: 2.5514888763427734
Validation loss: 2.453323787258517

Epoch: 6| Step: 6
Training loss: 3.0235435962677
Validation loss: 2.4356401479372414

Epoch: 6| Step: 7
Training loss: 2.7207932472229004
Validation loss: 2.4211646305617465

Epoch: 6| Step: 8
Training loss: 2.812532424926758
Validation loss: 2.411521286092779

Epoch: 6| Step: 9
Training loss: 2.770153045654297
Validation loss: 2.4076064735330562

Epoch: 6| Step: 10
Training loss: 2.0678348541259766
Validation loss: 2.404122047526862

Epoch: 6| Step: 11
Training loss: 2.2700462341308594
Validation loss: 2.402610389135217

Epoch: 6| Step: 12
Training loss: 3.0666089057922363
Validation loss: 2.403973638370473

Epoch: 6| Step: 13
Training loss: 3.1116111278533936
Validation loss: 2.4010691335124354

Epoch: 80| Step: 0
Training loss: 1.8675832748413086
Validation loss: 2.404099118325018

Epoch: 6| Step: 1
Training loss: 3.391815662384033
Validation loss: 2.4052626317547214

Epoch: 6| Step: 2
Training loss: 2.688880443572998
Validation loss: 2.4043598405776487

Epoch: 6| Step: 3
Training loss: 2.5168654918670654
Validation loss: 2.404795692813012

Epoch: 6| Step: 4
Training loss: 2.683201313018799
Validation loss: 2.405239882007722

Epoch: 6| Step: 5
Training loss: 2.458890914916992
Validation loss: 2.405030091603597

Epoch: 6| Step: 6
Training loss: 2.7890748977661133
Validation loss: 2.4023431834354194

Epoch: 6| Step: 7
Training loss: 2.478925943374634
Validation loss: 2.399963350706203

Epoch: 6| Step: 8
Training loss: 2.225275993347168
Validation loss: 2.4002211427175872

Epoch: 6| Step: 9
Training loss: 2.6474380493164062
Validation loss: 2.401557960817891

Epoch: 6| Step: 10
Training loss: 2.785338878631592
Validation loss: 2.4034169066336846

Epoch: 6| Step: 11
Training loss: 3.0565309524536133
Validation loss: 2.4022096485219975

Epoch: 6| Step: 12
Training loss: 2.48024845123291
Validation loss: 2.402648115670809

Epoch: 6| Step: 13
Training loss: 3.2226006984710693
Validation loss: 2.4003694980375228

Epoch: 81| Step: 0
Training loss: 3.1153523921966553
Validation loss: 2.402583295299161

Epoch: 6| Step: 1
Training loss: 2.4468271732330322
Validation loss: 2.4018110870033182

Epoch: 6| Step: 2
Training loss: 2.61260986328125
Validation loss: 2.400966718632688

Epoch: 6| Step: 3
Training loss: 2.313400983810425
Validation loss: 2.400669974665488

Epoch: 6| Step: 4
Training loss: 2.3627190589904785
Validation loss: 2.4059972583606677

Epoch: 6| Step: 5
Training loss: 1.9135642051696777
Validation loss: 2.410224578713858

Epoch: 6| Step: 6
Training loss: 2.1301746368408203
Validation loss: 2.412818454927014

Epoch: 6| Step: 7
Training loss: 3.046583652496338
Validation loss: 2.410605920258389

Epoch: 6| Step: 8
Training loss: 2.7362637519836426
Validation loss: 2.4123727916389384

Epoch: 6| Step: 9
Training loss: 3.532836675643921
Validation loss: 2.405201071052141

Epoch: 6| Step: 10
Training loss: 2.040097236633301
Validation loss: 2.4084092750344226

Epoch: 6| Step: 11
Training loss: 2.5405325889587402
Validation loss: 2.410084924390239

Epoch: 6| Step: 12
Training loss: 3.336750030517578
Validation loss: 2.4088925341124177

Epoch: 6| Step: 13
Training loss: 2.9003729820251465
Validation loss: 2.4062196106039067

Epoch: 82| Step: 0
Training loss: 3.044684886932373
Validation loss: 2.406043957638484

Epoch: 6| Step: 1
Training loss: 2.727937698364258
Validation loss: 2.403458733712473

Epoch: 6| Step: 2
Training loss: 2.1022496223449707
Validation loss: 2.4040026895461546

Epoch: 6| Step: 3
Training loss: 2.1557703018188477
Validation loss: 2.399632315481863

Epoch: 6| Step: 4
Training loss: 2.810525417327881
Validation loss: 2.4020716810739167

Epoch: 6| Step: 5
Training loss: 2.132016181945801
Validation loss: 2.4033057612757527

Epoch: 6| Step: 6
Training loss: 2.8994390964508057
Validation loss: 2.4050090236048542

Epoch: 6| Step: 7
Training loss: 3.3727121353149414
Validation loss: 2.40121131045844

Epoch: 6| Step: 8
Training loss: 1.9447808265686035
Validation loss: 2.405921519443553

Epoch: 6| Step: 9
Training loss: 3.0535354614257812
Validation loss: 2.409618059794108

Epoch: 6| Step: 10
Training loss: 2.1327271461486816
Validation loss: 2.41073428943593

Epoch: 6| Step: 11
Training loss: 2.6007766723632812
Validation loss: 2.4156159893158944

Epoch: 6| Step: 12
Training loss: 2.8407530784606934
Validation loss: 2.4145944528682257

Epoch: 6| Step: 13
Training loss: 3.400570869445801
Validation loss: 2.4081104391364643

Epoch: 83| Step: 0
Training loss: 2.286532402038574
Validation loss: 2.4125369825670795

Epoch: 6| Step: 1
Training loss: 2.254755973815918
Validation loss: 2.409835412938108

Epoch: 6| Step: 2
Training loss: 3.415172576904297
Validation loss: 2.41403337447874

Epoch: 6| Step: 3
Training loss: 2.1698930263519287
Validation loss: 2.4192607812984015

Epoch: 6| Step: 4
Training loss: 3.501983642578125
Validation loss: 2.4161902704546527

Epoch: 6| Step: 5
Training loss: 1.799710750579834
Validation loss: 2.4179877645225933

Epoch: 6| Step: 6
Training loss: 2.605485439300537
Validation loss: 2.42163485352711

Epoch: 6| Step: 7
Training loss: 1.8853785991668701
Validation loss: 2.4309133175880677

Epoch: 6| Step: 8
Training loss: 2.3741393089294434
Validation loss: 2.4300077371699835

Epoch: 6| Step: 9
Training loss: 2.788243055343628
Validation loss: 2.433666495866673

Epoch: 6| Step: 10
Training loss: 3.323559284210205
Validation loss: 2.429219335638067

Epoch: 6| Step: 11
Training loss: 2.642808675765991
Validation loss: 2.4234741067373626

Epoch: 6| Step: 12
Training loss: 2.385580062866211
Validation loss: 2.413672424131824

Epoch: 6| Step: 13
Training loss: 3.935213327407837
Validation loss: 2.405653361351259

Epoch: 84| Step: 0
Training loss: 2.8626866340637207
Validation loss: 2.406753199074858

Epoch: 6| Step: 1
Training loss: 2.112264633178711
Validation loss: 2.3985979377582507

Epoch: 6| Step: 2
Training loss: 2.0359435081481934
Validation loss: 2.396377004602904

Epoch: 6| Step: 3
Training loss: 2.3881282806396484
Validation loss: 2.402455601640927

Epoch: 6| Step: 4
Training loss: 1.9541124105453491
Validation loss: 2.3955619027537685

Epoch: 6| Step: 5
Training loss: 2.4828219413757324
Validation loss: 2.394515473355529

Epoch: 6| Step: 6
Training loss: 2.0411975383758545
Validation loss: 2.3964101114580707

Epoch: 6| Step: 7
Training loss: 3.0671205520629883
Validation loss: 2.4018021655339066

Epoch: 6| Step: 8
Training loss: 3.1546716690063477
Validation loss: 2.396696859790433

Epoch: 6| Step: 9
Training loss: 2.321270704269409
Validation loss: 2.3976976794581257

Epoch: 6| Step: 10
Training loss: 2.999513626098633
Validation loss: 2.3975392285213677

Epoch: 6| Step: 11
Training loss: 3.6574482917785645
Validation loss: 2.396892709116782

Epoch: 6| Step: 12
Training loss: 2.721625328063965
Validation loss: 2.3958148469207106

Epoch: 6| Step: 13
Training loss: 3.1937103271484375
Validation loss: 2.3975458760415354

Epoch: 85| Step: 0
Training loss: 2.243746519088745
Validation loss: 2.3922275150975874

Epoch: 6| Step: 1
Training loss: 2.7945408821105957
Validation loss: 2.3925520617474794

Epoch: 6| Step: 2
Training loss: 2.2721080780029297
Validation loss: 2.3892118994907667

Epoch: 6| Step: 3
Training loss: 2.1223721504211426
Validation loss: 2.3895646320876254

Epoch: 6| Step: 4
Training loss: 2.834202527999878
Validation loss: 2.3907515131017214

Epoch: 6| Step: 5
Training loss: 2.860574245452881
Validation loss: 2.386621798238447

Epoch: 6| Step: 6
Training loss: 2.5007309913635254
Validation loss: 2.391642887105224

Epoch: 6| Step: 7
Training loss: 1.8739159107208252
Validation loss: 2.3957068381770963

Epoch: 6| Step: 8
Training loss: 2.848707914352417
Validation loss: 2.405182161638814

Epoch: 6| Step: 9
Training loss: 3.5966596603393555
Validation loss: 2.4076323201579433

Epoch: 6| Step: 10
Training loss: 2.7183752059936523
Validation loss: 2.4093478495074856

Epoch: 6| Step: 11
Training loss: 2.6622509956359863
Validation loss: 2.4256978265700804

Epoch: 6| Step: 12
Training loss: 2.500372886657715
Validation loss: 2.418809390837146

Epoch: 6| Step: 13
Training loss: 3.185055732727051
Validation loss: 2.421060639043008

Epoch: 86| Step: 0
Training loss: 2.7112655639648438
Validation loss: 2.3902669363124396

Epoch: 6| Step: 1
Training loss: 2.4233181476593018
Validation loss: 2.387183132991996

Epoch: 6| Step: 2
Training loss: 3.4567742347717285
Validation loss: 2.3873503541433685

Epoch: 6| Step: 3
Training loss: 2.382014751434326
Validation loss: 2.3884954401241836

Epoch: 6| Step: 4
Training loss: 2.235562324523926
Validation loss: 2.3889872643255416

Epoch: 6| Step: 5
Training loss: 2.248171329498291
Validation loss: 2.3914039365706907

Epoch: 6| Step: 6
Training loss: 3.1041855812072754
Validation loss: 2.392494373424079

Epoch: 6| Step: 7
Training loss: 2.881816864013672
Validation loss: 2.392265837679627

Epoch: 6| Step: 8
Training loss: 2.502033233642578
Validation loss: 2.3947557146831224

Epoch: 6| Step: 9
Training loss: 2.6874990463256836
Validation loss: 2.391513750117312

Epoch: 6| Step: 10
Training loss: 2.813377618789673
Validation loss: 2.39256779865552

Epoch: 6| Step: 11
Training loss: 1.7491941452026367
Validation loss: 2.3902097850717525

Epoch: 6| Step: 12
Training loss: 2.810072183609009
Validation loss: 2.3894176252426638

Epoch: 6| Step: 13
Training loss: 2.9350013732910156
Validation loss: 2.388044552136493

Epoch: 87| Step: 0
Training loss: 2.563835620880127
Validation loss: 2.3857728537692817

Epoch: 6| Step: 1
Training loss: 2.9263978004455566
Validation loss: 2.385219730356688

Epoch: 6| Step: 2
Training loss: 2.3067822456359863
Validation loss: 2.38609085031735

Epoch: 6| Step: 3
Training loss: 2.377075672149658
Validation loss: 2.385782721222088

Epoch: 6| Step: 4
Training loss: 2.6523194313049316
Validation loss: 2.381525085818383

Epoch: 6| Step: 5
Training loss: 2.565356731414795
Validation loss: 2.383682963668659

Epoch: 6| Step: 6
Training loss: 2.2650089263916016
Validation loss: 2.384976707479005

Epoch: 6| Step: 7
Training loss: 3.2400918006896973
Validation loss: 2.380732254315448

Epoch: 6| Step: 8
Training loss: 2.9377079010009766
Validation loss: 2.38213788565769

Epoch: 6| Step: 9
Training loss: 2.434783935546875
Validation loss: 2.3871706736985074

Epoch: 6| Step: 10
Training loss: 2.6261401176452637
Validation loss: 2.396048856037919

Epoch: 6| Step: 11
Training loss: 2.8060944080352783
Validation loss: 2.3973108453135334

Epoch: 6| Step: 12
Training loss: 2.006687879562378
Validation loss: 2.405130560680102

Epoch: 6| Step: 13
Training loss: 3.2930123805999756
Validation loss: 2.4222991210158153

Epoch: 88| Step: 0
Training loss: 2.1769630908966064
Validation loss: 2.427339946070025

Epoch: 6| Step: 1
Training loss: 2.056881904602051
Validation loss: 2.433422329605267

Epoch: 6| Step: 2
Training loss: 2.4471185207366943
Validation loss: 2.421963973711896

Epoch: 6| Step: 3
Training loss: 2.6343255043029785
Validation loss: 2.399682060364754

Epoch: 6| Step: 4
Training loss: 3.0075113773345947
Validation loss: 2.37810061054845

Epoch: 6| Step: 5
Training loss: 2.6810367107391357
Validation loss: 2.3735441495013494

Epoch: 6| Step: 6
Training loss: 2.718550682067871
Validation loss: 2.371094219146236

Epoch: 6| Step: 7
Training loss: 2.9859418869018555
Validation loss: 2.375118891398112

Epoch: 6| Step: 8
Training loss: 2.8570189476013184
Validation loss: 2.3776861211305023

Epoch: 6| Step: 9
Training loss: 2.9805428981781006
Validation loss: 2.377576676748132

Epoch: 6| Step: 10
Training loss: 3.102034091949463
Validation loss: 2.380002789599921

Epoch: 6| Step: 11
Training loss: 2.707184076309204
Validation loss: 2.380856280685753

Epoch: 6| Step: 12
Training loss: 1.7998303174972534
Validation loss: 2.3816822139165734

Epoch: 6| Step: 13
Training loss: 2.492208242416382
Validation loss: 2.3820262083443264

Epoch: 89| Step: 0
Training loss: 2.6833863258361816
Validation loss: 2.3844065563653105

Epoch: 6| Step: 1
Training loss: 2.9395341873168945
Validation loss: 2.3832114870830248

Epoch: 6| Step: 2
Training loss: 2.245943069458008
Validation loss: 2.384906430398264

Epoch: 6| Step: 3
Training loss: 2.6768717765808105
Validation loss: 2.389120109619633

Epoch: 6| Step: 4
Training loss: 2.967331886291504
Validation loss: 2.391301239690473

Epoch: 6| Step: 5
Training loss: 2.6875603199005127
Validation loss: 2.3974951928661716

Epoch: 6| Step: 6
Training loss: 2.5388216972351074
Validation loss: 2.3987420887075444

Epoch: 6| Step: 7
Training loss: 3.067915916442871
Validation loss: 2.398776259473575

Epoch: 6| Step: 8
Training loss: 2.3972184658050537
Validation loss: 2.4035837317025788

Epoch: 6| Step: 9
Training loss: 2.5488195419311523
Validation loss: 2.4009089854455765

Epoch: 6| Step: 10
Training loss: 2.623520851135254
Validation loss: 2.397939076987646

Epoch: 6| Step: 11
Training loss: 2.0185492038726807
Validation loss: 2.3932442767645723

Epoch: 6| Step: 12
Training loss: 2.243072986602783
Validation loss: 2.4015444529953824

Epoch: 6| Step: 13
Training loss: 3.2548561096191406
Validation loss: 2.392504176785869

Epoch: 90| Step: 0
Training loss: 3.216864824295044
Validation loss: 2.388960684499433

Epoch: 6| Step: 1
Training loss: 2.6307010650634766
Validation loss: 2.3903034066641204

Epoch: 6| Step: 2
Training loss: 2.4560368061065674
Validation loss: 2.397733913954868

Epoch: 6| Step: 3
Training loss: 3.2263288497924805
Validation loss: 2.398609830487159

Epoch: 6| Step: 4
Training loss: 2.8539624214172363
Validation loss: 2.385318745848953

Epoch: 6| Step: 5
Training loss: 3.0482873916625977
Validation loss: 2.3830727633609565

Epoch: 6| Step: 6
Training loss: 2.1448535919189453
Validation loss: 2.3765163729267735

Epoch: 6| Step: 7
Training loss: 2.262436628341675
Validation loss: 2.3762273173178396

Epoch: 6| Step: 8
Training loss: 2.3807077407836914
Validation loss: 2.37744410832723

Epoch: 6| Step: 9
Training loss: 2.1215813159942627
Validation loss: 2.3812432263487127

Epoch: 6| Step: 10
Training loss: 2.4121785163879395
Validation loss: 2.379125141328381

Epoch: 6| Step: 11
Training loss: 2.8131229877471924
Validation loss: 2.3837831045991633

Epoch: 6| Step: 12
Training loss: 2.470315456390381
Validation loss: 2.377258103380921

Epoch: 6| Step: 13
Training loss: 2.3977205753326416
Validation loss: 2.385215856695688

Epoch: 91| Step: 0
Training loss: 2.1897971630096436
Validation loss: 2.376812263201642

Epoch: 6| Step: 1
Training loss: 2.8731868267059326
Validation loss: 2.382761150278071

Epoch: 6| Step: 2
Training loss: 2.573530435562134
Validation loss: 2.3783270364166587

Epoch: 6| Step: 3
Training loss: 2.6664481163024902
Validation loss: 2.369907590650743

Epoch: 6| Step: 4
Training loss: 3.075897693634033
Validation loss: 2.3738207253076697

Epoch: 6| Step: 5
Training loss: 1.865058422088623
Validation loss: 2.3706590590938443

Epoch: 6| Step: 6
Training loss: 3.1020264625549316
Validation loss: 2.3706325920679236

Epoch: 6| Step: 7
Training loss: 2.9333653450012207
Validation loss: 2.3731355026204097

Epoch: 6| Step: 8
Training loss: 1.887035846710205
Validation loss: 2.3725165577344995

Epoch: 6| Step: 9
Training loss: 2.7495038509368896
Validation loss: 2.3744728565216064

Epoch: 6| Step: 10
Training loss: 2.696192979812622
Validation loss: 2.3709820367956675

Epoch: 6| Step: 11
Training loss: 3.034799575805664
Validation loss: 2.3797187676993747

Epoch: 6| Step: 12
Training loss: 2.4305331707000732
Validation loss: 2.3770530813483783

Epoch: 6| Step: 13
Training loss: 2.358339309692383
Validation loss: 2.379827958281322

Epoch: 92| Step: 0
Training loss: 3.0315589904785156
Validation loss: 2.3787036659897014

Epoch: 6| Step: 1
Training loss: 2.2053956985473633
Validation loss: 2.3735777383209555

Epoch: 6| Step: 2
Training loss: 3.2890048027038574
Validation loss: 2.375162075924617

Epoch: 6| Step: 3
Training loss: 2.6994071006774902
Validation loss: 2.371613912684943

Epoch: 6| Step: 4
Training loss: 2.1656694412231445
Validation loss: 2.369134013370801

Epoch: 6| Step: 5
Training loss: 2.9167888164520264
Validation loss: 2.3740930480341755

Epoch: 6| Step: 6
Training loss: 2.2669529914855957
Validation loss: 2.3731151857683734

Epoch: 6| Step: 7
Training loss: 3.0310776233673096
Validation loss: 2.369205754290345

Epoch: 6| Step: 8
Training loss: 3.1785776615142822
Validation loss: 2.373811903820243

Epoch: 6| Step: 9
Training loss: 2.1179914474487305
Validation loss: 2.3749146102577128

Epoch: 6| Step: 10
Training loss: 2.0887646675109863
Validation loss: 2.3795908343407417

Epoch: 6| Step: 11
Training loss: 2.3166792392730713
Validation loss: 2.3795974933972923

Epoch: 6| Step: 12
Training loss: 2.984729528427124
Validation loss: 2.3933832183960946

Epoch: 6| Step: 13
Training loss: 1.9126442670822144
Validation loss: 2.3877059926268873

Epoch: 93| Step: 0
Training loss: 2.051626205444336
Validation loss: 2.3867142892652944

Epoch: 6| Step: 1
Training loss: 2.5473732948303223
Validation loss: 2.385131146318169

Epoch: 6| Step: 2
Training loss: 1.8999857902526855
Validation loss: 2.3903253129733506

Epoch: 6| Step: 3
Training loss: 2.2606089115142822
Validation loss: 2.3855624609096076

Epoch: 6| Step: 4
Training loss: 1.9184061288833618
Validation loss: 2.3927176229415403

Epoch: 6| Step: 5
Training loss: 2.4938721656799316
Validation loss: 2.3876227025062806

Epoch: 6| Step: 6
Training loss: 2.8760180473327637
Validation loss: 2.389822998354512

Epoch: 6| Step: 7
Training loss: 2.749384880065918
Validation loss: 2.395362833494781

Epoch: 6| Step: 8
Training loss: 3.358281135559082
Validation loss: 2.3908546919463785

Epoch: 6| Step: 9
Training loss: 3.5105481147766113
Validation loss: 2.386884609858195

Epoch: 6| Step: 10
Training loss: 2.806230068206787
Validation loss: 2.3847855111604095

Epoch: 6| Step: 11
Training loss: 2.6857595443725586
Validation loss: 2.377197088733796

Epoch: 6| Step: 12
Training loss: 2.7958874702453613
Validation loss: 2.3688680792367585

Epoch: 6| Step: 13
Training loss: 2.27370023727417
Validation loss: 2.3711056170925016

Epoch: 94| Step: 0
Training loss: 2.989617347717285
Validation loss: 2.3606584610477572

Epoch: 6| Step: 1
Training loss: 2.4515490531921387
Validation loss: 2.36563277757296

Epoch: 6| Step: 2
Training loss: 2.663381338119507
Validation loss: 2.3614817793651293

Epoch: 6| Step: 3
Training loss: 2.231100559234619
Validation loss: 2.3595124983018443

Epoch: 6| Step: 4
Training loss: 2.6386423110961914
Validation loss: 2.3600022638997724

Epoch: 6| Step: 5
Training loss: 3.1039459705352783
Validation loss: 2.3589199332780737

Epoch: 6| Step: 6
Training loss: 3.10026216506958
Validation loss: 2.37004243686635

Epoch: 6| Step: 7
Training loss: 1.82367742061615
Validation loss: 2.3779893216266426

Epoch: 6| Step: 8
Training loss: 2.5003583431243896
Validation loss: 2.3905931339469007

Epoch: 6| Step: 9
Training loss: 2.8092401027679443
Validation loss: 2.397746434775732

Epoch: 6| Step: 10
Training loss: 2.276744842529297
Validation loss: 2.3945376257742605

Epoch: 6| Step: 11
Training loss: 2.484720468521118
Validation loss: 2.3880484104156494

Epoch: 6| Step: 12
Training loss: 2.8186099529266357
Validation loss: 2.3857992990042574

Epoch: 6| Step: 13
Training loss: 2.8257827758789062
Validation loss: 2.377753920452569

Epoch: 95| Step: 0
Training loss: 2.4639101028442383
Validation loss: 2.3679533825125745

Epoch: 6| Step: 1
Training loss: 3.0982301235198975
Validation loss: 2.3568339732385453

Epoch: 6| Step: 2
Training loss: 2.6387808322906494
Validation loss: 2.359383708687239

Epoch: 6| Step: 3
Training loss: 2.740476608276367
Validation loss: 2.3615392202972085

Epoch: 6| Step: 4
Training loss: 2.772150993347168
Validation loss: 2.360999681616342

Epoch: 6| Step: 5
Training loss: 3.039415121078491
Validation loss: 2.3622816480616087

Epoch: 6| Step: 6
Training loss: 1.6291639804840088
Validation loss: 2.35869530195831

Epoch: 6| Step: 7
Training loss: 2.1084682941436768
Validation loss: 2.3590229544588315

Epoch: 6| Step: 8
Training loss: 2.126960277557373
Validation loss: 2.3690430836011003

Epoch: 6| Step: 9
Training loss: 3.5404114723205566
Validation loss: 2.374917773790257

Epoch: 6| Step: 10
Training loss: 2.8238210678100586
Validation loss: 2.3650099590260494

Epoch: 6| Step: 11
Training loss: 2.812356472015381
Validation loss: 2.3740604462162143

Epoch: 6| Step: 12
Training loss: 2.486041307449341
Validation loss: 2.3658376765507523

Epoch: 6| Step: 13
Training loss: 1.7283142805099487
Validation loss: 2.360312905362857

Epoch: 96| Step: 0
Training loss: 1.8076128959655762
Validation loss: 2.3735305160604496

Epoch: 6| Step: 1
Training loss: 2.5782718658447266
Validation loss: 2.3853839341030327

Epoch: 6| Step: 2
Training loss: 3.214585781097412
Validation loss: 2.3930634913905973

Epoch: 6| Step: 3
Training loss: 2.4643988609313965
Validation loss: 2.3849964475118988

Epoch: 6| Step: 4
Training loss: 2.6172828674316406
Validation loss: 2.402068937978437

Epoch: 6| Step: 5
Training loss: 3.1022415161132812
Validation loss: 2.3967047686217935

Epoch: 6| Step: 6
Training loss: 2.9740684032440186
Validation loss: 2.3959681269943074

Epoch: 6| Step: 7
Training loss: 3.052762269973755
Validation loss: 2.3752840718915387

Epoch: 6| Step: 8
Training loss: 2.7712554931640625
Validation loss: 2.3621702835124028

Epoch: 6| Step: 9
Training loss: 3.1128461360931396
Validation loss: 2.357415509480302

Epoch: 6| Step: 10
Training loss: 2.539621591567993
Validation loss: 2.3547527482432704

Epoch: 6| Step: 11
Training loss: 2.0344533920288086
Validation loss: 2.3496449044955674

Epoch: 6| Step: 12
Training loss: 1.8229191303253174
Validation loss: 2.349769066738826

Epoch: 6| Step: 13
Training loss: 2.1156513690948486
Validation loss: 2.349427321905731

Epoch: 97| Step: 0
Training loss: 2.378422737121582
Validation loss: 2.353118460665467

Epoch: 6| Step: 1
Training loss: 3.388479232788086
Validation loss: 2.3545768158410185

Epoch: 6| Step: 2
Training loss: 2.586430311203003
Validation loss: 2.3628466360030638

Epoch: 6| Step: 3
Training loss: 1.7569248676300049
Validation loss: 2.3710623838568248

Epoch: 6| Step: 4
Training loss: 2.3285927772521973
Validation loss: 2.4027947302787536

Epoch: 6| Step: 5
Training loss: 3.0222384929656982
Validation loss: 2.4199400383939027

Epoch: 6| Step: 6
Training loss: 2.4918224811553955
Validation loss: 2.4142723647497033

Epoch: 6| Step: 7
Training loss: 2.663447380065918
Validation loss: 2.3966148463628625

Epoch: 6| Step: 8
Training loss: 2.476855993270874
Validation loss: 2.3831924520513064

Epoch: 6| Step: 9
Training loss: 2.4922924041748047
Validation loss: 2.3690837019233295

Epoch: 6| Step: 10
Training loss: 2.752838134765625
Validation loss: 2.3657474722913516

Epoch: 6| Step: 11
Training loss: 2.3818933963775635
Validation loss: 2.365217101189398

Epoch: 6| Step: 12
Training loss: 2.5484471321105957
Validation loss: 2.357464531416534

Epoch: 6| Step: 13
Training loss: 3.5972280502319336
Validation loss: 2.355633556201894

Epoch: 98| Step: 0
Training loss: 1.9978735446929932
Validation loss: 2.3587602210301224

Epoch: 6| Step: 1
Training loss: 2.8219404220581055
Validation loss: 2.354983672018974

Epoch: 6| Step: 2
Training loss: 3.1780407428741455
Validation loss: 2.358652958305933

Epoch: 6| Step: 3
Training loss: 2.62955379486084
Validation loss: 2.3510441395544235

Epoch: 6| Step: 4
Training loss: 2.415098190307617
Validation loss: 2.3573044089860815

Epoch: 6| Step: 5
Training loss: 2.584670066833496
Validation loss: 2.3587603415212324

Epoch: 6| Step: 6
Training loss: 2.7977776527404785
Validation loss: 2.361613740203201

Epoch: 6| Step: 7
Training loss: 2.587038993835449
Validation loss: 2.366156865191716

Epoch: 6| Step: 8
Training loss: 2.3388075828552246
Validation loss: 2.3771913102878037

Epoch: 6| Step: 9
Training loss: 2.7322661876678467
Validation loss: 2.3811718315206547

Epoch: 6| Step: 10
Training loss: 3.1777026653289795
Validation loss: 2.388724457833075

Epoch: 6| Step: 11
Training loss: 1.9657493829727173
Validation loss: 2.3841528431061776

Epoch: 6| Step: 12
Training loss: 2.5421276092529297
Validation loss: 2.385876332559893

Epoch: 6| Step: 13
Training loss: 2.736544609069824
Validation loss: 2.3706323459584224

Epoch: 99| Step: 0
Training loss: 2.5097625255584717
Validation loss: 2.375156210314843

Epoch: 6| Step: 1
Training loss: 2.064809799194336
Validation loss: 2.3677452610385035

Epoch: 6| Step: 2
Training loss: 2.8293442726135254
Validation loss: 2.3733357306449645

Epoch: 6| Step: 3
Training loss: 2.6914291381835938
Validation loss: 2.3729918849083687

Epoch: 6| Step: 4
Training loss: 2.6651830673217773
Validation loss: 2.366800026227069

Epoch: 6| Step: 5
Training loss: 2.2863807678222656
Validation loss: 2.3583264171436267

Epoch: 6| Step: 6
Training loss: 3.096613883972168
Validation loss: 2.3541830355121243

Epoch: 6| Step: 7
Training loss: 1.8876497745513916
Validation loss: 2.3578765341030654

Epoch: 6| Step: 8
Training loss: 2.9754247665405273
Validation loss: 2.3498719815284974

Epoch: 6| Step: 9
Training loss: 2.9832849502563477
Validation loss: 2.3605492602112474

Epoch: 6| Step: 10
Training loss: 1.8499128818511963
Validation loss: 2.3566740687175463

Epoch: 6| Step: 11
Training loss: 2.791635036468506
Validation loss: 2.35900802253395

Epoch: 6| Step: 12
Training loss: 2.771313190460205
Validation loss: 2.3552328181523148

Epoch: 6| Step: 13
Training loss: 3.133622884750366
Validation loss: 2.3476776974175566

Epoch: 100| Step: 0
Training loss: 3.1777987480163574
Validation loss: 2.347066238362302

Epoch: 6| Step: 1
Training loss: 2.9858007431030273
Validation loss: 2.3466212326480496

Epoch: 6| Step: 2
Training loss: 3.1311697959899902
Validation loss: 2.345270518333681

Epoch: 6| Step: 3
Training loss: 3.19181227684021
Validation loss: 2.350436725924092

Epoch: 6| Step: 4
Training loss: 2.0249111652374268
Validation loss: 2.357072977609532

Epoch: 6| Step: 5
Training loss: 2.487971305847168
Validation loss: 2.361070725225633

Epoch: 6| Step: 6
Training loss: 2.038939952850342
Validation loss: 2.358856280644735

Epoch: 6| Step: 7
Training loss: 2.3420867919921875
Validation loss: 2.3571731121309343

Epoch: 6| Step: 8
Training loss: 2.264940023422241
Validation loss: 2.352596289368086

Epoch: 6| Step: 9
Training loss: 1.9572118520736694
Validation loss: 2.3511517099154893

Epoch: 6| Step: 10
Training loss: 2.530555248260498
Validation loss: 2.3492827569284747

Epoch: 6| Step: 11
Training loss: 3.1234989166259766
Validation loss: 2.338755753732497

Epoch: 6| Step: 12
Training loss: 2.544710636138916
Validation loss: 2.3377441795923377

Epoch: 6| Step: 13
Training loss: 2.322176933288574
Validation loss: 2.3395302346957627

Epoch: 101| Step: 0
Training loss: 2.890225410461426
Validation loss: 2.339629375806419

Epoch: 6| Step: 1
Training loss: 2.741844654083252
Validation loss: 2.345705250258087

Epoch: 6| Step: 2
Training loss: 3.043505907058716
Validation loss: 2.360031673985143

Epoch: 6| Step: 3
Training loss: 2.7315940856933594
Validation loss: 2.3675519548436648

Epoch: 6| Step: 4
Training loss: 1.8762569427490234
Validation loss: 2.371371053880261

Epoch: 6| Step: 5
Training loss: 2.1568171977996826
Validation loss: 2.360968110381916

Epoch: 6| Step: 6
Training loss: 2.2603328227996826
Validation loss: 2.346609451437509

Epoch: 6| Step: 7
Training loss: 2.082159996032715
Validation loss: 2.3415203632846957

Epoch: 6| Step: 8
Training loss: 3.0698671340942383
Validation loss: 2.3366553116870183

Epoch: 6| Step: 9
Training loss: 2.8636484146118164
Validation loss: 2.3365853345522316

Epoch: 6| Step: 10
Training loss: 2.4210023880004883
Validation loss: 2.3290984784403155

Epoch: 6| Step: 11
Training loss: 2.769681692123413
Validation loss: 2.3314277561762

Epoch: 6| Step: 12
Training loss: 2.7461955547332764
Validation loss: 2.3344079884149695

Epoch: 6| Step: 13
Training loss: 2.581516981124878
Validation loss: 2.3441014161673923

Epoch: 102| Step: 0
Training loss: 2.980497360229492
Validation loss: 2.348520576312978

Epoch: 6| Step: 1
Training loss: 2.5508148670196533
Validation loss: 2.3510183518932712

Epoch: 6| Step: 2
Training loss: 2.105011463165283
Validation loss: 2.3590689269445275

Epoch: 6| Step: 3
Training loss: 2.268664836883545
Validation loss: 2.3562477788617535

Epoch: 6| Step: 4
Training loss: 3.0719494819641113
Validation loss: 2.360115038451328

Epoch: 6| Step: 5
Training loss: 3.494983673095703
Validation loss: 2.3581997168961393

Epoch: 6| Step: 6
Training loss: 2.994095802307129
Validation loss: 2.3615428991215204

Epoch: 6| Step: 7
Training loss: 2.425797462463379
Validation loss: 2.3525929143351894

Epoch: 6| Step: 8
Training loss: 1.5794649124145508
Validation loss: 2.3473844989653556

Epoch: 6| Step: 9
Training loss: 3.615225315093994
Validation loss: 2.3593061354852494

Epoch: 6| Step: 10
Training loss: 2.339931011199951
Validation loss: 2.361950046272688

Epoch: 6| Step: 11
Training loss: 3.0352540016174316
Validation loss: 2.3770837578722226

Epoch: 6| Step: 12
Training loss: 1.394363284111023
Validation loss: 2.387600524451143

Epoch: 6| Step: 13
Training loss: 2.2672643661499023
Validation loss: 2.4005524983970066

Epoch: 103| Step: 0
Training loss: 2.706923246383667
Validation loss: 2.4010170659711285

Epoch: 6| Step: 1
Training loss: 1.901221513748169
Validation loss: 2.401544722177649

Epoch: 6| Step: 2
Training loss: 2.822125196456909
Validation loss: 2.397596207998132

Epoch: 6| Step: 3
Training loss: 3.063603162765503
Validation loss: 2.400164804150981

Epoch: 6| Step: 4
Training loss: 2.8140206336975098
Validation loss: 2.3893264083452124

Epoch: 6| Step: 5
Training loss: 2.9806325435638428
Validation loss: 2.3794351034266974

Epoch: 6| Step: 6
Training loss: 2.891094207763672
Validation loss: 2.363233614993352

Epoch: 6| Step: 7
Training loss: 2.0597755908966064
Validation loss: 2.3455127362282044

Epoch: 6| Step: 8
Training loss: 2.246823787689209
Validation loss: 2.3299110243397374

Epoch: 6| Step: 9
Training loss: 1.9633458852767944
Validation loss: 2.3330078612091723

Epoch: 6| Step: 10
Training loss: 2.3555707931518555
Validation loss: 2.3385054193517214

Epoch: 6| Step: 11
Training loss: 2.864405393600464
Validation loss: 2.347247374955044

Epoch: 6| Step: 12
Training loss: 2.7711262702941895
Validation loss: 2.343424145893384

Epoch: 6| Step: 13
Training loss: 2.951796770095825
Validation loss: 2.341658810133575

Epoch: 104| Step: 0
Training loss: 2.845801830291748
Validation loss: 2.3473431243691394

Epoch: 6| Step: 1
Training loss: 2.5554966926574707
Validation loss: 2.358914829069568

Epoch: 6| Step: 2
Training loss: 2.5841288566589355
Validation loss: 2.323686827895462

Epoch: 6| Step: 3
Training loss: 2.9944117069244385
Validation loss: 2.3293910667460453

Epoch: 6| Step: 4
Training loss: 2.3398287296295166
Validation loss: 2.3233051197503203

Epoch: 6| Step: 5
Training loss: 2.4587488174438477
Validation loss: 2.363856025921401

Epoch: 6| Step: 6
Training loss: 2.520066261291504
Validation loss: 2.3542033933824107

Epoch: 6| Step: 7
Training loss: 2.5054850578308105
Validation loss: 2.34002992158295

Epoch: 6| Step: 8
Training loss: 2.938102960586548
Validation loss: 2.367195552395236

Epoch: 6| Step: 9
Training loss: 2.453432559967041
Validation loss: 2.3623460262052474

Epoch: 6| Step: 10
Training loss: 2.776369571685791
Validation loss: 2.3169179552344867

Epoch: 6| Step: 11
Training loss: 2.693957805633545
Validation loss: 2.29464913183643

Epoch: 6| Step: 12
Training loss: 2.5318493843078613
Validation loss: 2.2936049263964415

Epoch: 6| Step: 13
Training loss: 1.702404499053955
Validation loss: 2.2949147557699554

Epoch: 105| Step: 0
Training loss: 2.8641257286071777
Validation loss: 2.305039210986066

Epoch: 6| Step: 1
Training loss: 2.4367361068725586
Validation loss: 2.3049089677872194

Epoch: 6| Step: 2
Training loss: 2.490298271179199
Validation loss: 2.306668604573896

Epoch: 6| Step: 3
Training loss: 2.215028762817383
Validation loss: 2.312922070103307

Epoch: 6| Step: 4
Training loss: 3.1710214614868164
Validation loss: 2.308706619406259

Epoch: 6| Step: 5
Training loss: 2.5765066146850586
Validation loss: 2.328409296210094

Epoch: 6| Step: 6
Training loss: 2.5126311779022217
Validation loss: 2.338547916822536

Epoch: 6| Step: 7
Training loss: 2.8068833351135254
Validation loss: 2.355500118706816

Epoch: 6| Step: 8
Training loss: 1.8602759838104248
Validation loss: 2.3802571373601116

Epoch: 6| Step: 9
Training loss: 2.372598886489868
Validation loss: 2.387608920374224

Epoch: 6| Step: 10
Training loss: 2.125434398651123
Validation loss: 2.3784327199382167

Epoch: 6| Step: 11
Training loss: 3.0735507011413574
Validation loss: 2.39132793231677

Epoch: 6| Step: 12
Training loss: 2.808896780014038
Validation loss: 2.3692673816475818

Epoch: 6| Step: 13
Training loss: 2.677608013153076
Validation loss: 2.3319502363922777

Epoch: 106| Step: 0
Training loss: 1.7498772144317627
Validation loss: 2.3158858642783215

Epoch: 6| Step: 1
Training loss: 2.326931953430176
Validation loss: 2.306218630524092

Epoch: 6| Step: 2
Training loss: 2.5394365787506104
Validation loss: 2.306449656845421

Epoch: 6| Step: 3
Training loss: 2.988938331604004
Validation loss: 2.3060986098422798

Epoch: 6| Step: 4
Training loss: 2.6274526119232178
Validation loss: 2.2969361607746412

Epoch: 6| Step: 5
Training loss: 2.9746830463409424
Validation loss: 2.300705143200454

Epoch: 6| Step: 6
Training loss: 2.0779805183410645
Validation loss: 2.3030191775291198

Epoch: 6| Step: 7
Training loss: 1.6074117422103882
Validation loss: 2.3012782296826764

Epoch: 6| Step: 8
Training loss: 2.42862606048584
Validation loss: 2.3024975792054208

Epoch: 6| Step: 9
Training loss: 2.6729164123535156
Validation loss: 2.302451774638186

Epoch: 6| Step: 10
Training loss: 3.073596239089966
Validation loss: 2.3014231907424105

Epoch: 6| Step: 11
Training loss: 3.2221837043762207
Validation loss: 2.3111535297927035

Epoch: 6| Step: 12
Training loss: 3.0549354553222656
Validation loss: 2.3177697837993665

Epoch: 6| Step: 13
Training loss: 2.558877944946289
Validation loss: 2.3225903946866273

Epoch: 107| Step: 0
Training loss: 3.0303735733032227
Validation loss: 2.3348910372744323

Epoch: 6| Step: 1
Training loss: 2.5220370292663574
Validation loss: 2.340955759889336

Epoch: 6| Step: 2
Training loss: 2.548929452896118
Validation loss: 2.3394196520569506

Epoch: 6| Step: 3
Training loss: 2.050367593765259
Validation loss: 2.3397182982455016

Epoch: 6| Step: 4
Training loss: 2.8782219886779785
Validation loss: 2.346557117277576

Epoch: 6| Step: 5
Training loss: 3.092439651489258
Validation loss: 2.3347131590689383

Epoch: 6| Step: 6
Training loss: 2.1146106719970703
Validation loss: 2.319873382968287

Epoch: 6| Step: 7
Training loss: 2.430666208267212
Validation loss: 2.3131883426379134

Epoch: 6| Step: 8
Training loss: 2.144627094268799
Validation loss: 2.318094845741026

Epoch: 6| Step: 9
Training loss: 2.7479777336120605
Validation loss: 2.318161687543315

Epoch: 6| Step: 10
Training loss: 2.685037612915039
Validation loss: 2.3204795955329813

Epoch: 6| Step: 11
Training loss: 2.464385986328125
Validation loss: 2.3211254842819704

Epoch: 6| Step: 12
Training loss: 2.359016180038452
Validation loss: 2.31620523493777

Epoch: 6| Step: 13
Training loss: 2.982617139816284
Validation loss: 2.3210503747386317

Epoch: 108| Step: 0
Training loss: 2.579587936401367
Validation loss: 2.3184950249169463

Epoch: 6| Step: 1
Training loss: 3.6884400844573975
Validation loss: 2.3200508215094127

Epoch: 6| Step: 2
Training loss: 2.475187063217163
Validation loss: 2.312573253467519

Epoch: 6| Step: 3
Training loss: 2.692115306854248
Validation loss: 2.298748113775766

Epoch: 6| Step: 4
Training loss: 2.262256622314453
Validation loss: 2.2956196825991393

Epoch: 6| Step: 5
Training loss: 2.9831366539001465
Validation loss: 2.2957093305485223

Epoch: 6| Step: 6
Training loss: 2.8861608505249023
Validation loss: 2.298030348234279

Epoch: 6| Step: 7
Training loss: 2.6383144855499268
Validation loss: 2.3023032834452968

Epoch: 6| Step: 8
Training loss: 1.6677992343902588
Validation loss: 2.296318123417516

Epoch: 6| Step: 9
Training loss: 2.2135417461395264
Validation loss: 2.298077126984955

Epoch: 6| Step: 10
Training loss: 2.2891812324523926
Validation loss: 2.2960308815843318

Epoch: 6| Step: 11
Training loss: 2.143916606903076
Validation loss: 2.2947571610891693

Epoch: 6| Step: 12
Training loss: 2.907785654067993
Validation loss: 2.293687584579632

Epoch: 6| Step: 13
Training loss: 1.915414571762085
Validation loss: 2.2976450381740445

Epoch: 109| Step: 0
Training loss: 2.8468470573425293
Validation loss: 2.2979978335800992

Epoch: 6| Step: 1
Training loss: 2.287651300430298
Validation loss: 2.2979129437477357

Epoch: 6| Step: 2
Training loss: 2.9251835346221924
Validation loss: 2.2995949150413595

Epoch: 6| Step: 3
Training loss: 2.396684408187866
Validation loss: 2.300909742232292

Epoch: 6| Step: 4
Training loss: 2.5869271755218506
Validation loss: 2.305038954622002

Epoch: 6| Step: 5
Training loss: 2.263861656188965
Validation loss: 2.3143079947399836

Epoch: 6| Step: 6
Training loss: 2.1293129920959473
Validation loss: 2.315626644319104

Epoch: 6| Step: 7
Training loss: 2.8094115257263184
Validation loss: 2.3277247439148607

Epoch: 6| Step: 8
Training loss: 1.9490747451782227
Validation loss: 2.325909132598549

Epoch: 6| Step: 9
Training loss: 2.9877138137817383
Validation loss: 2.3343339684189006

Epoch: 6| Step: 10
Training loss: 2.732931137084961
Validation loss: 2.3298394936387257

Epoch: 6| Step: 11
Training loss: 3.074636459350586
Validation loss: 2.327963606003792

Epoch: 6| Step: 12
Training loss: 1.9719345569610596
Validation loss: 2.3163429562763502

Epoch: 6| Step: 13
Training loss: 2.8591902256011963
Validation loss: 2.3093453145796254

Epoch: 110| Step: 0
Training loss: 2.4809627532958984
Validation loss: 2.3000474617045414

Epoch: 6| Step: 1
Training loss: 2.3123397827148438
Validation loss: 2.303366297034807

Epoch: 6| Step: 2
Training loss: 2.0943405628204346
Validation loss: 2.314394650920745

Epoch: 6| Step: 3
Training loss: 2.9362945556640625
Validation loss: 2.3158679751939673

Epoch: 6| Step: 4
Training loss: 2.2672066688537598
Validation loss: 2.3230866514226443

Epoch: 6| Step: 5
Training loss: 3.2824411392211914
Validation loss: 2.3249767134266515

Epoch: 6| Step: 6
Training loss: 2.20587420463562
Validation loss: 2.320848249620007

Epoch: 6| Step: 7
Training loss: 2.5374062061309814
Validation loss: 2.324445532214257

Epoch: 6| Step: 8
Training loss: 2.763248920440674
Validation loss: 2.3193677189529582

Epoch: 6| Step: 9
Training loss: 2.4666290283203125
Validation loss: 2.311099918939734

Epoch: 6| Step: 10
Training loss: 1.9998130798339844
Validation loss: 2.3100951243472356

Epoch: 6| Step: 11
Training loss: 2.67283296585083
Validation loss: 2.304980818943311

Epoch: 6| Step: 12
Training loss: 2.8583502769470215
Validation loss: 2.297150069667447

Epoch: 6| Step: 13
Training loss: 2.9583401679992676
Validation loss: 2.288886113833356

Epoch: 111| Step: 0
Training loss: 3.109020471572876
Validation loss: 2.2852116246377268

Epoch: 6| Step: 1
Training loss: 2.5248758792877197
Validation loss: 2.2836018621280627

Epoch: 6| Step: 2
Training loss: 3.0727362632751465
Validation loss: 2.284606597756827

Epoch: 6| Step: 3
Training loss: 1.9140870571136475
Validation loss: 2.2819342254310526

Epoch: 6| Step: 4
Training loss: 2.7238473892211914
Validation loss: 2.2895558136765675

Epoch: 6| Step: 5
Training loss: 2.8778817653656006
Validation loss: 2.2882366077874297

Epoch: 6| Step: 6
Training loss: 3.0927538871765137
Validation loss: 2.291554835534865

Epoch: 6| Step: 7
Training loss: 1.7797253131866455
Validation loss: 2.3099810615662606

Epoch: 6| Step: 8
Training loss: 2.186697006225586
Validation loss: 2.3136694764578216

Epoch: 6| Step: 9
Training loss: 2.1853272914886475
Validation loss: 2.3215565014910955

Epoch: 6| Step: 10
Training loss: 2.487144708633423
Validation loss: 2.3146628410585466

Epoch: 6| Step: 11
Training loss: 2.497239112854004
Validation loss: 2.3416124518199632

Epoch: 6| Step: 12
Training loss: 2.196115493774414
Validation loss: 2.3120709132122736

Epoch: 6| Step: 13
Training loss: 3.4049928188323975
Validation loss: 2.2937490427365868

Epoch: 112| Step: 0
Training loss: 2.3346638679504395
Validation loss: 2.2841650978211434

Epoch: 6| Step: 1
Training loss: 3.0505590438842773
Validation loss: 2.2828345350039903

Epoch: 6| Step: 2
Training loss: 2.2597436904907227
Validation loss: 2.2888777666194464

Epoch: 6| Step: 3
Training loss: 2.7691423892974854
Validation loss: 2.289754602216905

Epoch: 6| Step: 4
Training loss: 2.0614070892333984
Validation loss: 2.2939300255108903

Epoch: 6| Step: 5
Training loss: 2.7304112911224365
Validation loss: 2.2900432437978764

Epoch: 6| Step: 6
Training loss: 2.105529308319092
Validation loss: 2.2888402169750584

Epoch: 6| Step: 7
Training loss: 3.149989604949951
Validation loss: 2.291071868711902

Epoch: 6| Step: 8
Training loss: 2.4484479427337646
Validation loss: 2.302010169593237

Epoch: 6| Step: 9
Training loss: 2.9744670391082764
Validation loss: 2.3006663168630292

Epoch: 6| Step: 10
Training loss: 2.667429208755493
Validation loss: 2.3073652764802337

Epoch: 6| Step: 11
Training loss: 2.243478298187256
Validation loss: 2.3095706739733295

Epoch: 6| Step: 12
Training loss: 2.4065699577331543
Validation loss: 2.3082769634903118

Epoch: 6| Step: 13
Training loss: 2.322434425354004
Validation loss: 2.307564053484189

Epoch: 113| Step: 0
Training loss: 2.9638328552246094
Validation loss: 2.3179209283603135

Epoch: 6| Step: 1
Training loss: 2.9419894218444824
Validation loss: 2.320988885817989

Epoch: 6| Step: 2
Training loss: 2.5829997062683105
Validation loss: 2.308433328905413

Epoch: 6| Step: 3
Training loss: 3.190455913543701
Validation loss: 2.308124044890045

Epoch: 6| Step: 4
Training loss: 3.073915481567383
Validation loss: 2.3012431744606263

Epoch: 6| Step: 5
Training loss: 2.4901485443115234
Validation loss: 2.292411160725419

Epoch: 6| Step: 6
Training loss: 1.808009386062622
Validation loss: 2.2847263313108876

Epoch: 6| Step: 7
Training loss: 2.267075777053833
Validation loss: 2.282259877010058

Epoch: 6| Step: 8
Training loss: 2.4774322509765625
Validation loss: 2.2767141660054526

Epoch: 6| Step: 9
Training loss: 1.985887885093689
Validation loss: 2.270969542123938

Epoch: 6| Step: 10
Training loss: 2.880087375640869
Validation loss: 2.2701872087294057

Epoch: 6| Step: 11
Training loss: 2.786188840866089
Validation loss: 2.270805602432579

Epoch: 6| Step: 12
Training loss: 2.0441079139709473
Validation loss: 2.270316803327171

Epoch: 6| Step: 13
Training loss: 1.4750455617904663
Validation loss: 2.2689046705922773

Epoch: 114| Step: 0
Training loss: 2.1026225090026855
Validation loss: 2.269771506709437

Epoch: 6| Step: 1
Training loss: 2.481426239013672
Validation loss: 2.272425592586558

Epoch: 6| Step: 2
Training loss: 2.862428665161133
Validation loss: 2.2791528535145584

Epoch: 6| Step: 3
Training loss: 2.395888090133667
Validation loss: 2.279883551341231

Epoch: 6| Step: 4
Training loss: 3.1672003269195557
Validation loss: 2.281563825504754

Epoch: 6| Step: 5
Training loss: 2.1025774478912354
Validation loss: 2.287736641463413

Epoch: 6| Step: 6
Training loss: 3.098266124725342
Validation loss: 2.2965328334480204

Epoch: 6| Step: 7
Training loss: 2.2931220531463623
Validation loss: 2.304874009983514

Epoch: 6| Step: 8
Training loss: 2.9424681663513184
Validation loss: 2.309889301176994

Epoch: 6| Step: 9
Training loss: 2.0932703018188477
Validation loss: 2.2942871842333066

Epoch: 6| Step: 10
Training loss: 2.6388015747070312
Validation loss: 2.2800912831419256

Epoch: 6| Step: 11
Training loss: 1.9296369552612305
Validation loss: 2.2849532468344576

Epoch: 6| Step: 12
Training loss: 2.58693265914917
Validation loss: 2.29453585609313

Epoch: 6| Step: 13
Training loss: 2.8572142124176025
Validation loss: 2.286344241070491

Epoch: 115| Step: 0
Training loss: 2.7157704830169678
Validation loss: 2.295609943328365

Epoch: 6| Step: 1
Training loss: 1.973314881324768
Validation loss: 2.296603079765074

Epoch: 6| Step: 2
Training loss: 2.864598274230957
Validation loss: 2.293362686710973

Epoch: 6| Step: 3
Training loss: 2.5441834926605225
Validation loss: 2.2815570164752264

Epoch: 6| Step: 4
Training loss: 2.2412948608398438
Validation loss: 2.2808144297651065

Epoch: 6| Step: 5
Training loss: 2.546412467956543
Validation loss: 2.285280625025431

Epoch: 6| Step: 6
Training loss: 2.606447696685791
Validation loss: 2.2857011389988724

Epoch: 6| Step: 7
Training loss: 2.0717904567718506
Validation loss: 2.2900537842063495

Epoch: 6| Step: 8
Training loss: 2.3379201889038086
Validation loss: 2.2966172259341002

Epoch: 6| Step: 9
Training loss: 2.569258689880371
Validation loss: 2.301276212097496

Epoch: 6| Step: 10
Training loss: 2.1358821392059326
Validation loss: 2.2877870246928227

Epoch: 6| Step: 11
Training loss: 3.8526811599731445
Validation loss: 2.2883204183270855

Epoch: 6| Step: 12
Training loss: 2.2753982543945312
Validation loss: 2.2962736621979745

Epoch: 6| Step: 13
Training loss: 2.7374982833862305
Validation loss: 2.285129729137626

Epoch: 116| Step: 0
Training loss: 2.894049644470215
Validation loss: 2.288012622505106

Epoch: 6| Step: 1
Training loss: 2.3227758407592773
Validation loss: 2.2849934511287238

Epoch: 6| Step: 2
Training loss: 2.4150543212890625
Validation loss: 2.2753435821943384

Epoch: 6| Step: 3
Training loss: 2.5768001079559326
Validation loss: 2.268825295150921

Epoch: 6| Step: 4
Training loss: 2.816248893737793
Validation loss: 2.2779806839522494

Epoch: 6| Step: 5
Training loss: 2.2980618476867676
Validation loss: 2.281055893949283

Epoch: 6| Step: 6
Training loss: 2.268345832824707
Validation loss: 2.2930880784988403

Epoch: 6| Step: 7
Training loss: 2.9119949340820312
Validation loss: 2.2989478700904438

Epoch: 6| Step: 8
Training loss: 1.94964599609375
Validation loss: 2.293450524730067

Epoch: 6| Step: 9
Training loss: 3.083041191101074
Validation loss: 2.2892119230762606

Epoch: 6| Step: 10
Training loss: 2.3123273849487305
Validation loss: 2.2911739503183672

Epoch: 6| Step: 11
Training loss: 2.1540541648864746
Validation loss: 2.287132986130253

Epoch: 6| Step: 12
Training loss: 2.520599842071533
Validation loss: 2.277258247457525

Epoch: 6| Step: 13
Training loss: 3.115527391433716
Validation loss: 2.2778806071127615

Epoch: 117| Step: 0
Training loss: 2.7740979194641113
Validation loss: 2.2771954895347677

Epoch: 6| Step: 1
Training loss: 2.5347914695739746
Validation loss: 2.283834124124178

Epoch: 6| Step: 2
Training loss: 2.18344783782959
Validation loss: 2.288791764167047

Epoch: 6| Step: 3
Training loss: 2.2138712406158447
Validation loss: 2.29677096233573

Epoch: 6| Step: 4
Training loss: 2.335515022277832
Validation loss: 2.302548421326504

Epoch: 6| Step: 5
Training loss: 2.4851560592651367
Validation loss: 2.3099130302347164

Epoch: 6| Step: 6
Training loss: 2.4491467475891113
Validation loss: 2.308735744927519

Epoch: 6| Step: 7
Training loss: 2.6917309761047363
Validation loss: 2.313094601836256

Epoch: 6| Step: 8
Training loss: 2.093050479888916
Validation loss: 2.3016834899943364

Epoch: 6| Step: 9
Training loss: 3.529073715209961
Validation loss: 2.2885252096319713

Epoch: 6| Step: 10
Training loss: 3.050929069519043
Validation loss: 2.272669323029057

Epoch: 6| Step: 11
Training loss: 2.0962581634521484
Validation loss: 2.2621113510542017

Epoch: 6| Step: 12
Training loss: 2.1005749702453613
Validation loss: 2.2606254495600218

Epoch: 6| Step: 13
Training loss: 3.0144379138946533
Validation loss: 2.2647327146222516

Epoch: 118| Step: 0
Training loss: 3.3286972045898438
Validation loss: 2.2697331777182956

Epoch: 6| Step: 1
Training loss: 2.606645107269287
Validation loss: 2.2681463251831713

Epoch: 6| Step: 2
Training loss: 1.9430270195007324
Validation loss: 2.267458067145399

Epoch: 6| Step: 3
Training loss: 2.1668457984924316
Validation loss: 2.2751985467890257

Epoch: 6| Step: 4
Training loss: 3.194423198699951
Validation loss: 2.2740089047339653

Epoch: 6| Step: 5
Training loss: 3.4355838298797607
Validation loss: 2.2713968869178527

Epoch: 6| Step: 6
Training loss: 2.52337908744812
Validation loss: 2.275938460903783

Epoch: 6| Step: 7
Training loss: 2.4047183990478516
Validation loss: 2.2672820475793656

Epoch: 6| Step: 8
Training loss: 2.081178665161133
Validation loss: 2.2680737587713424

Epoch: 6| Step: 9
Training loss: 2.2916769981384277
Validation loss: 2.2714765840961086

Epoch: 6| Step: 10
Training loss: 2.1707005500793457
Validation loss: 2.269688411425519

Epoch: 6| Step: 11
Training loss: 2.545091152191162
Validation loss: 2.2726856841835925

Epoch: 6| Step: 12
Training loss: 1.9855601787567139
Validation loss: 2.267403584654613

Epoch: 6| Step: 13
Training loss: 2.4326019287109375
Validation loss: 2.2720092342745875

Epoch: 119| Step: 0
Training loss: 2.325601100921631
Validation loss: 2.288856614020563

Epoch: 6| Step: 1
Training loss: 3.3127636909484863
Validation loss: 2.282574507497972

Epoch: 6| Step: 2
Training loss: 2.194706916809082
Validation loss: 2.282262800842203

Epoch: 6| Step: 3
Training loss: 3.3459384441375732
Validation loss: 2.2869497627340336

Epoch: 6| Step: 4
Training loss: 2.435457944869995
Validation loss: 2.2783280316219536

Epoch: 6| Step: 5
Training loss: 2.4042232036590576
Validation loss: 2.264010365291308

Epoch: 6| Step: 6
Training loss: 2.328914165496826
Validation loss: 2.2573940087390203

Epoch: 6| Step: 7
Training loss: 2.277411937713623
Validation loss: 2.257822880180933

Epoch: 6| Step: 8
Training loss: 2.6861891746520996
Validation loss: 2.2566398036095405

Epoch: 6| Step: 9
Training loss: 2.4234886169433594
Validation loss: 2.257781487639232

Epoch: 6| Step: 10
Training loss: 2.4569950103759766
Validation loss: 2.255248362018216

Epoch: 6| Step: 11
Training loss: 2.8898918628692627
Validation loss: 2.266422915202315

Epoch: 6| Step: 12
Training loss: 2.1396474838256836
Validation loss: 2.2743872340007494

Epoch: 6| Step: 13
Training loss: 1.7142078876495361
Validation loss: 2.2767074210669405

Epoch: 120| Step: 0
Training loss: 2.5970242023468018
Validation loss: 2.2759975823022986

Epoch: 6| Step: 1
Training loss: 2.8252649307250977
Validation loss: 2.2734291309951455

Epoch: 6| Step: 2
Training loss: 2.7640492916107178
Validation loss: 2.2748313744862876

Epoch: 6| Step: 3
Training loss: 2.71746563911438
Validation loss: 2.2725940109581075

Epoch: 6| Step: 4
Training loss: 2.299034595489502
Validation loss: 2.2718895891661286

Epoch: 6| Step: 5
Training loss: 1.5771965980529785
Validation loss: 2.265038808186849

Epoch: 6| Step: 6
Training loss: 2.1753175258636475
Validation loss: 2.269236413381433

Epoch: 6| Step: 7
Training loss: 3.1040186882019043
Validation loss: 2.260109829646285

Epoch: 6| Step: 8
Training loss: 2.5697264671325684
Validation loss: 2.2619836676505303

Epoch: 6| Step: 9
Training loss: 2.5643064975738525
Validation loss: 2.2638955193181194

Epoch: 6| Step: 10
Training loss: 3.6085498332977295
Validation loss: 2.2900747586322088

Epoch: 6| Step: 11
Training loss: 1.6843392848968506
Validation loss: 2.294316391791067

Epoch: 6| Step: 12
Training loss: 2.2832741737365723
Validation loss: 2.2881471303201493

Epoch: 6| Step: 13
Training loss: 2.1698293685913086
Validation loss: 2.2988925979983423

Epoch: 121| Step: 0
Training loss: 2.5743043422698975
Validation loss: 2.3012446716267574

Epoch: 6| Step: 1
Training loss: 2.6369595527648926
Validation loss: 2.3083104087460424

Epoch: 6| Step: 2
Training loss: 2.1841797828674316
Validation loss: 2.301455400323355

Epoch: 6| Step: 3
Training loss: 3.416956663131714
Validation loss: 2.2864012564382246

Epoch: 6| Step: 4
Training loss: 1.9575750827789307
Validation loss: 2.2810033085525676

Epoch: 6| Step: 5
Training loss: 1.970245122909546
Validation loss: 2.281285244931457

Epoch: 6| Step: 6
Training loss: 1.7765955924987793
Validation loss: 2.27949615704116

Epoch: 6| Step: 7
Training loss: 2.183596611022949
Validation loss: 2.284481120365922

Epoch: 6| Step: 8
Training loss: 3.630479574203491
Validation loss: 2.293973010073426

Epoch: 6| Step: 9
Training loss: 2.324666738510132
Validation loss: 2.291978823241367

Epoch: 6| Step: 10
Training loss: 2.4509871006011963
Validation loss: 2.2850737930625997

Epoch: 6| Step: 11
Training loss: 3.657722234725952
Validation loss: 2.286787848318777

Epoch: 6| Step: 12
Training loss: 2.168679714202881
Validation loss: 2.2622726886503157

Epoch: 6| Step: 13
Training loss: 1.950744867324829
Validation loss: 2.252258308472172

Epoch: 122| Step: 0
Training loss: 2.293583869934082
Validation loss: 2.2518894569848174

Epoch: 6| Step: 1
Training loss: 2.6076784133911133
Validation loss: 2.248198793780419

Epoch: 6| Step: 2
Training loss: 2.582819938659668
Validation loss: 2.249828302732078

Epoch: 6| Step: 3
Training loss: 2.6084327697753906
Validation loss: 2.2568736332719044

Epoch: 6| Step: 4
Training loss: 2.545952558517456
Validation loss: 2.266691853923182

Epoch: 6| Step: 5
Training loss: 2.5248372554779053
Validation loss: 2.272225704244388

Epoch: 6| Step: 6
Training loss: 2.380526542663574
Validation loss: 2.289989843163439

Epoch: 6| Step: 7
Training loss: 2.354275941848755
Validation loss: 2.2832768245409896

Epoch: 6| Step: 8
Training loss: 2.0496506690979004
Validation loss: 2.2723898426178963

Epoch: 6| Step: 9
Training loss: 2.6790571212768555
Validation loss: 2.267827195505942

Epoch: 6| Step: 10
Training loss: 2.7454872131347656
Validation loss: 2.26236903795632

Epoch: 6| Step: 11
Training loss: 2.9650657176971436
Validation loss: 2.259888356731784

Epoch: 6| Step: 12
Training loss: 2.1912689208984375
Validation loss: 2.2624736985852643

Epoch: 6| Step: 13
Training loss: 2.5759527683258057
Validation loss: 2.2435285865619616

Epoch: 123| Step: 0
Training loss: 2.1525864601135254
Validation loss: 2.2544227876970844

Epoch: 6| Step: 1
Training loss: 2.7191476821899414
Validation loss: 2.243931793397473

Epoch: 6| Step: 2
Training loss: 2.8912692070007324
Validation loss: 2.246397896479535

Epoch: 6| Step: 3
Training loss: 2.331530809402466
Validation loss: 2.2591861409525715

Epoch: 6| Step: 4
Training loss: 1.7893261909484863
Validation loss: 2.2507978972568305

Epoch: 6| Step: 5
Training loss: 2.4052398204803467
Validation loss: 2.257418709416543

Epoch: 6| Step: 6
Training loss: 3.1231799125671387
Validation loss: 2.262061590789467

Epoch: 6| Step: 7
Training loss: 3.0242953300476074
Validation loss: 2.261210209579878

Epoch: 6| Step: 8
Training loss: 2.198262929916382
Validation loss: 2.2702568115726596

Epoch: 6| Step: 9
Training loss: 2.1799025535583496
Validation loss: 2.2815618233014177

Epoch: 6| Step: 10
Training loss: 2.307161569595337
Validation loss: 2.2977207245365268

Epoch: 6| Step: 11
Training loss: 2.3023548126220703
Validation loss: 2.309734006081858

Epoch: 6| Step: 12
Training loss: 2.890120267868042
Validation loss: 2.3249737780581237

Epoch: 6| Step: 13
Training loss: 2.960860252380371
Validation loss: 2.313898140384305

Epoch: 124| Step: 0
Training loss: 1.8501770496368408
Validation loss: 2.28695224946545

Epoch: 6| Step: 1
Training loss: 2.6160855293273926
Validation loss: 2.2581303940024426

Epoch: 6| Step: 2
Training loss: 2.3514740467071533
Validation loss: 2.2450388272603354

Epoch: 6| Step: 3
Training loss: 2.9862990379333496
Validation loss: 2.2410858010733

Epoch: 6| Step: 4
Training loss: 2.539480209350586
Validation loss: 2.2447661353695776

Epoch: 6| Step: 5
Training loss: 2.3990330696105957
Validation loss: 2.2447918461215113

Epoch: 6| Step: 6
Training loss: 2.200037956237793
Validation loss: 2.251208136158605

Epoch: 6| Step: 7
Training loss: 2.346989631652832
Validation loss: 2.252857663298166

Epoch: 6| Step: 8
Training loss: 3.0994439125061035
Validation loss: 2.259200921622656

Epoch: 6| Step: 9
Training loss: 2.281863212585449
Validation loss: 2.2596928945151706

Epoch: 6| Step: 10
Training loss: 2.764871597290039
Validation loss: 2.257966028746738

Epoch: 6| Step: 11
Training loss: 2.476029872894287
Validation loss: 2.253592409113402

Epoch: 6| Step: 12
Training loss: 2.3748137950897217
Validation loss: 2.257761163096274

Epoch: 6| Step: 13
Training loss: 2.9238078594207764
Validation loss: 2.2506082032316472

Epoch: 125| Step: 0
Training loss: 2.85438871383667
Validation loss: 2.2504322862112396

Epoch: 6| Step: 1
Training loss: 2.7706894874572754
Validation loss: 2.258536269587855

Epoch: 6| Step: 2
Training loss: 2.5435917377471924
Validation loss: 2.253327021034815

Epoch: 6| Step: 3
Training loss: 2.9187679290771484
Validation loss: 2.2602544035962833

Epoch: 6| Step: 4
Training loss: 2.7882862091064453
Validation loss: 2.2451463040485176

Epoch: 6| Step: 5
Training loss: 2.436204433441162
Validation loss: 2.2396046448779363

Epoch: 6| Step: 6
Training loss: 1.9790575504302979
Validation loss: 2.2318618592395576

Epoch: 6| Step: 7
Training loss: 2.063159704208374
Validation loss: 2.225554766193513

Epoch: 6| Step: 8
Training loss: 2.0444297790527344
Validation loss: 2.2271271803045787

Epoch: 6| Step: 9
Training loss: 2.23124623298645
Validation loss: 2.2291107626371485

Epoch: 6| Step: 10
Training loss: 2.1934409141540527
Validation loss: 2.2432933135699202

Epoch: 6| Step: 11
Training loss: 3.463731288909912
Validation loss: 2.239004491477884

Epoch: 6| Step: 12
Training loss: 2.525881767272949
Validation loss: 2.2501446700865224

Epoch: 6| Step: 13
Training loss: 1.9955304861068726
Validation loss: 2.252972459280363

Epoch: 126| Step: 0
Training loss: 2.4993484020233154
Validation loss: 2.25630517928831

Epoch: 6| Step: 1
Training loss: 3.01358699798584
Validation loss: 2.2588330673915085

Epoch: 6| Step: 2
Training loss: 2.563814640045166
Validation loss: 2.257078286140196

Epoch: 6| Step: 3
Training loss: 2.6317644119262695
Validation loss: 2.257200594871275

Epoch: 6| Step: 4
Training loss: 2.4123497009277344
Validation loss: 2.252982801006686

Epoch: 6| Step: 5
Training loss: 2.287930488586426
Validation loss: 2.2582477369616107

Epoch: 6| Step: 6
Training loss: 1.5838937759399414
Validation loss: 2.2629856448019705

Epoch: 6| Step: 7
Training loss: 2.962477207183838
Validation loss: 2.2671614539238716

Epoch: 6| Step: 8
Training loss: 2.2839441299438477
Validation loss: 2.2638107089586157

Epoch: 6| Step: 9
Training loss: 2.3948349952697754
Validation loss: 2.2747572134899836

Epoch: 6| Step: 10
Training loss: 2.2795844078063965
Validation loss: 2.2812828889457126

Epoch: 6| Step: 11
Training loss: 2.356325626373291
Validation loss: 2.2929823629317747

Epoch: 6| Step: 12
Training loss: 2.5603106021881104
Validation loss: 2.2997693297683552

Epoch: 6| Step: 13
Training loss: 3.644098997116089
Validation loss: 2.3048719385618806

Epoch: 127| Step: 0
Training loss: 2.0943915843963623
Validation loss: 2.308459835667764

Epoch: 6| Step: 1
Training loss: 2.9347662925720215
Validation loss: 2.2927123526091218

Epoch: 6| Step: 2
Training loss: 2.238675594329834
Validation loss: 2.2811654601045834

Epoch: 6| Step: 3
Training loss: 2.922783851623535
Validation loss: 2.2607002540301253

Epoch: 6| Step: 4
Training loss: 2.6263675689697266
Validation loss: 2.2574716896139164

Epoch: 6| Step: 5
Training loss: 2.450460433959961
Validation loss: 2.2502682901197866

Epoch: 6| Step: 6
Training loss: 2.799090623855591
Validation loss: 2.246828144596469

Epoch: 6| Step: 7
Training loss: 2.243513584136963
Validation loss: 2.245873305105394

Epoch: 6| Step: 8
Training loss: 2.743051290512085
Validation loss: 2.23890668858764

Epoch: 6| Step: 9
Training loss: 2.426243305206299
Validation loss: 2.237699063875342

Epoch: 6| Step: 10
Training loss: 2.279552698135376
Validation loss: 2.238054096057851

Epoch: 6| Step: 11
Training loss: 2.2768216133117676
Validation loss: 2.2493283415353424

Epoch: 6| Step: 12
Training loss: 2.2274954319000244
Validation loss: 2.2490115268256075

Epoch: 6| Step: 13
Training loss: 2.676525354385376
Validation loss: 2.2565506068609094

Epoch: 128| Step: 0
Training loss: 2.6368894577026367
Validation loss: 2.2496625018376175

Epoch: 6| Step: 1
Training loss: 2.5303614139556885
Validation loss: 2.2547076235535326

Epoch: 6| Step: 2
Training loss: 2.201456069946289
Validation loss: 2.258502011658043

Epoch: 6| Step: 3
Training loss: 2.5611109733581543
Validation loss: 2.2577266462387575

Epoch: 6| Step: 4
Training loss: 2.559918165206909
Validation loss: 2.2676367016248804

Epoch: 6| Step: 5
Training loss: 3.0198583602905273
Validation loss: 2.2714978302678754

Epoch: 6| Step: 6
Training loss: 2.2918248176574707
Validation loss: 2.287873355291223

Epoch: 6| Step: 7
Training loss: 1.857774257659912
Validation loss: 2.282103600040559

Epoch: 6| Step: 8
Training loss: 2.5546064376831055
Validation loss: 2.287259042903941

Epoch: 6| Step: 9
Training loss: 2.697587490081787
Validation loss: 2.264902796796573

Epoch: 6| Step: 10
Training loss: 2.558187961578369
Validation loss: 2.2488296724134877

Epoch: 6| Step: 11
Training loss: 2.722311496734619
Validation loss: 2.245002349217733

Epoch: 6| Step: 12
Training loss: 2.4366211891174316
Validation loss: 2.242490282622717

Epoch: 6| Step: 13
Training loss: 1.907012701034546
Validation loss: 2.2304983139038086

Epoch: 129| Step: 0
Training loss: 2.4719016551971436
Validation loss: 2.230616210609354

Epoch: 6| Step: 1
Training loss: 3.130021095275879
Validation loss: 2.2308574517567954

Epoch: 6| Step: 2
Training loss: 1.5782568454742432
Validation loss: 2.2374973835483676

Epoch: 6| Step: 3
Training loss: 2.9860215187072754
Validation loss: 2.2438539407586537

Epoch: 6| Step: 4
Training loss: 2.577836036682129
Validation loss: 2.258956109323809

Epoch: 6| Step: 5
Training loss: 2.053666353225708
Validation loss: 2.2749142134061424

Epoch: 6| Step: 6
Training loss: 2.0434975624084473
Validation loss: 2.286089310082056

Epoch: 6| Step: 7
Training loss: 2.052119493484497
Validation loss: 2.289706112236105

Epoch: 6| Step: 8
Training loss: 2.3332412242889404
Validation loss: 2.259460918364986

Epoch: 6| Step: 9
Training loss: 2.6927103996276855
Validation loss: 2.2504793264532603

Epoch: 6| Step: 10
Training loss: 3.4042110443115234
Validation loss: 2.245372756834953

Epoch: 6| Step: 11
Training loss: 2.2387239933013916
Validation loss: 2.244572498465097

Epoch: 6| Step: 12
Training loss: 2.3362627029418945
Validation loss: 2.234418293481232

Epoch: 6| Step: 13
Training loss: 3.1849334239959717
Validation loss: 2.2271556802975234

Epoch: 130| Step: 0
Training loss: 2.2944207191467285
Validation loss: 2.222817108195315

Epoch: 6| Step: 1
Training loss: 2.5336153507232666
Validation loss: 2.2270080069059968

Epoch: 6| Step: 2
Training loss: 2.6401357650756836
Validation loss: 2.2284090672769854

Epoch: 6| Step: 3
Training loss: 2.0164477825164795
Validation loss: 2.236768853279852

Epoch: 6| Step: 4
Training loss: 2.3509812355041504
Validation loss: 2.236390131776051

Epoch: 6| Step: 5
Training loss: 2.899601459503174
Validation loss: 2.24048258924997

Epoch: 6| Step: 6
Training loss: 2.556093215942383
Validation loss: 2.239854674185476

Epoch: 6| Step: 7
Training loss: 2.300523281097412
Validation loss: 2.226220732094139

Epoch: 6| Step: 8
Training loss: 2.3424265384674072
Validation loss: 2.2401750972194057

Epoch: 6| Step: 9
Training loss: 2.6446306705474854
Validation loss: 2.240050185111261

Epoch: 6| Step: 10
Training loss: 2.5865890979766846
Validation loss: 2.245095142754175

Epoch: 6| Step: 11
Training loss: 2.276108741760254
Validation loss: 2.246190804307179

Epoch: 6| Step: 12
Training loss: 2.343029499053955
Validation loss: 2.2403920568445677

Epoch: 6| Step: 13
Training loss: 3.7363619804382324
Validation loss: 2.2433756346343667

Epoch: 131| Step: 0
Training loss: 2.9789061546325684
Validation loss: 2.2362418661835375

Epoch: 6| Step: 1
Training loss: 1.631245732307434
Validation loss: 2.2400901727778937

Epoch: 6| Step: 2
Training loss: 2.7369327545166016
Validation loss: 2.2398461270075973

Epoch: 6| Step: 3
Training loss: 2.187018394470215
Validation loss: 2.2344787197728313

Epoch: 6| Step: 4
Training loss: 2.4509637355804443
Validation loss: 2.2281918423150175

Epoch: 6| Step: 5
Training loss: 2.5342905521392822
Validation loss: 2.23539508927253

Epoch: 6| Step: 6
Training loss: 2.708134174346924
Validation loss: 2.2350219834235405

Epoch: 6| Step: 7
Training loss: 2.485004186630249
Validation loss: 2.2344311975663707

Epoch: 6| Step: 8
Training loss: 2.2729740142822266
Validation loss: 2.222104395589521

Epoch: 6| Step: 9
Training loss: 2.766845703125
Validation loss: 2.2320491677971295

Epoch: 6| Step: 10
Training loss: 2.4164438247680664
Validation loss: 2.2303352202138593

Epoch: 6| Step: 11
Training loss: 2.9385876655578613
Validation loss: 2.2355707383924917

Epoch: 6| Step: 12
Training loss: 1.9999908208847046
Validation loss: 2.230683552321567

Epoch: 6| Step: 13
Training loss: 2.6959240436553955
Validation loss: 2.2368816355223298

Epoch: 132| Step: 0
Training loss: 3.003525972366333
Validation loss: 2.245855913367323

Epoch: 6| Step: 1
Training loss: 1.5062105655670166
Validation loss: 2.252274049225674

Epoch: 6| Step: 2
Training loss: 2.4676170349121094
Validation loss: 2.255957349654167

Epoch: 6| Step: 3
Training loss: 2.4190328121185303
Validation loss: 2.2696311243118776

Epoch: 6| Step: 4
Training loss: 2.8463988304138184
Validation loss: 2.2724846229758313

Epoch: 6| Step: 5
Training loss: 2.8528189659118652
Validation loss: 2.27038287860091

Epoch: 6| Step: 6
Training loss: 2.2631068229675293
Validation loss: 2.262252902471891

Epoch: 6| Step: 7
Training loss: 2.4015517234802246
Validation loss: 2.2544499161422893

Epoch: 6| Step: 8
Training loss: 2.09487247467041
Validation loss: 2.2368863372392553

Epoch: 6| Step: 9
Training loss: 1.9389772415161133
Validation loss: 2.2297131476863736

Epoch: 6| Step: 10
Training loss: 3.511300802230835
Validation loss: 2.2198799143555346

Epoch: 6| Step: 11
Training loss: 2.5351784229278564
Validation loss: 2.21480740270307

Epoch: 6| Step: 12
Training loss: 2.2876150608062744
Validation loss: 2.215599593295846

Epoch: 6| Step: 13
Training loss: 2.3855981826782227
Validation loss: 2.2166772081005957

Epoch: 133| Step: 0
Training loss: 2.3164520263671875
Validation loss: 2.2145509360938944

Epoch: 6| Step: 1
Training loss: 2.1038336753845215
Validation loss: 2.220256810547203

Epoch: 6| Step: 2
Training loss: 2.2957468032836914
Validation loss: 2.2171492884235997

Epoch: 6| Step: 3
Training loss: 2.8261914253234863
Validation loss: 2.2170179505502023

Epoch: 6| Step: 4
Training loss: 1.9419913291931152
Validation loss: 2.228300250986571

Epoch: 6| Step: 5
Training loss: 2.283515691757202
Validation loss: 2.2336636256146174

Epoch: 6| Step: 6
Training loss: 2.6284360885620117
Validation loss: 2.2348646220340522

Epoch: 6| Step: 7
Training loss: 2.901458501815796
Validation loss: 2.246052316440049

Epoch: 6| Step: 8
Training loss: 2.5261735916137695
Validation loss: 2.2431187655336116

Epoch: 6| Step: 9
Training loss: 2.4915902614593506
Validation loss: 2.2336249402774278

Epoch: 6| Step: 10
Training loss: 2.5850167274475098
Validation loss: 2.235926947286052

Epoch: 6| Step: 11
Training loss: 2.4770336151123047
Validation loss: 2.2347786580362627

Epoch: 6| Step: 12
Training loss: 2.1653246879577637
Validation loss: 2.2395523363544094

Epoch: 6| Step: 13
Training loss: 3.3498687744140625
Validation loss: 2.24345552280385

Epoch: 134| Step: 0
Training loss: 2.7437429428100586
Validation loss: 2.2475452705096175

Epoch: 6| Step: 1
Training loss: 2.4414610862731934
Validation loss: 2.246762519241661

Epoch: 6| Step: 2
Training loss: 2.92063570022583
Validation loss: 2.24886675547528

Epoch: 6| Step: 3
Training loss: 2.3982791900634766
Validation loss: 2.250974319314444

Epoch: 6| Step: 4
Training loss: 2.390082597732544
Validation loss: 2.2565205891927085

Epoch: 6| Step: 5
Training loss: 2.131441354751587
Validation loss: 2.2525863506460704

Epoch: 6| Step: 6
Training loss: 2.2474396228790283
Validation loss: 2.264755810460737

Epoch: 6| Step: 7
Training loss: 2.267719268798828
Validation loss: 2.2637593643639677

Epoch: 6| Step: 8
Training loss: 3.198737859725952
Validation loss: 2.2517266273498535

Epoch: 6| Step: 9
Training loss: 2.75032901763916
Validation loss: 2.2381358890123266

Epoch: 6| Step: 10
Training loss: 2.1796364784240723
Validation loss: 2.2427189375764582

Epoch: 6| Step: 11
Training loss: 2.3658204078674316
Validation loss: 2.244602272587438

Epoch: 6| Step: 12
Training loss: 2.2145702838897705
Validation loss: 2.2424239240666872

Epoch: 6| Step: 13
Training loss: 1.89150071144104
Validation loss: 2.243521213531494

Epoch: 135| Step: 0
Training loss: 2.67535662651062
Validation loss: 2.2284594863973637

Epoch: 6| Step: 1
Training loss: 2.9150989055633545
Validation loss: 2.2222170611863494

Epoch: 6| Step: 2
Training loss: 2.187560558319092
Validation loss: 2.2166739356133247

Epoch: 6| Step: 3
Training loss: 1.9947190284729004
Validation loss: 2.2096454251197075

Epoch: 6| Step: 4
Training loss: 2.2125496864318848
Validation loss: 2.214544609028806

Epoch: 6| Step: 5
Training loss: 2.0633320808410645
Validation loss: 2.214599629884125

Epoch: 6| Step: 6
Training loss: 2.660271167755127
Validation loss: 2.2111492144164218

Epoch: 6| Step: 7
Training loss: 2.5073399543762207
Validation loss: 2.215622294333673

Epoch: 6| Step: 8
Training loss: 2.040877103805542
Validation loss: 2.219045226291944

Epoch: 6| Step: 9
Training loss: 2.8483009338378906
Validation loss: 2.2296436396978234

Epoch: 6| Step: 10
Training loss: 2.6209402084350586
Validation loss: 2.225680394839215

Epoch: 6| Step: 11
Training loss: 2.758110284805298
Validation loss: 2.2322184680610575

Epoch: 6| Step: 12
Training loss: 2.215221881866455
Validation loss: 2.2357163275441816

Epoch: 6| Step: 13
Training loss: 2.759063959121704
Validation loss: 2.2317692156760924

Epoch: 136| Step: 0
Training loss: 2.4455373287200928
Validation loss: 2.2220450216724026

Epoch: 6| Step: 1
Training loss: 2.058241367340088
Validation loss: 2.2256183983177267

Epoch: 6| Step: 2
Training loss: 2.279137134552002
Validation loss: 2.21518111741671

Epoch: 6| Step: 3
Training loss: 2.6921000480651855
Validation loss: 2.216528795098746

Epoch: 6| Step: 4
Training loss: 2.210648536682129
Validation loss: 2.216323073192309

Epoch: 6| Step: 5
Training loss: 3.0616798400878906
Validation loss: 2.215522366185342

Epoch: 6| Step: 6
Training loss: 2.798879384994507
Validation loss: 2.2230619076759583

Epoch: 6| Step: 7
Training loss: 2.8200395107269287
Validation loss: 2.2075611570829987

Epoch: 6| Step: 8
Training loss: 2.9056711196899414
Validation loss: 2.21215671108615

Epoch: 6| Step: 9
Training loss: 2.31484317779541
Validation loss: 2.204011622295585

Epoch: 6| Step: 10
Training loss: 1.7241190671920776
Validation loss: 2.21295823589448

Epoch: 6| Step: 11
Training loss: 2.678661346435547
Validation loss: 2.215218138951127

Epoch: 6| Step: 12
Training loss: 2.179727792739868
Validation loss: 2.237731602884108

Epoch: 6| Step: 13
Training loss: 2.059605121612549
Validation loss: 2.2385533522534113

Epoch: 137| Step: 0
Training loss: 3.4571268558502197
Validation loss: 2.225450546510758

Epoch: 6| Step: 1
Training loss: 2.0009140968322754
Validation loss: 2.2214024989835677

Epoch: 6| Step: 2
Training loss: 1.737440824508667
Validation loss: 2.211946919400205

Epoch: 6| Step: 3
Training loss: 1.9342617988586426
Validation loss: 2.201180200422964

Epoch: 6| Step: 4
Training loss: 2.481064796447754
Validation loss: 2.2010940121066187

Epoch: 6| Step: 5
Training loss: 2.156620740890503
Validation loss: 2.2068518387374056

Epoch: 6| Step: 6
Training loss: 2.227393865585327
Validation loss: 2.210636260688946

Epoch: 6| Step: 7
Training loss: 2.3302276134490967
Validation loss: 2.2212386849106

Epoch: 6| Step: 8
Training loss: 2.888725757598877
Validation loss: 2.21896795816319

Epoch: 6| Step: 9
Training loss: 2.5813708305358887
Validation loss: 2.229955668090492

Epoch: 6| Step: 10
Training loss: 2.238420009613037
Validation loss: 2.2311591089412732

Epoch: 6| Step: 11
Training loss: 2.7311410903930664
Validation loss: 2.231320893892678

Epoch: 6| Step: 12
Training loss: 2.9466216564178467
Validation loss: 2.257685069114931

Epoch: 6| Step: 13
Training loss: 2.630034923553467
Validation loss: 2.2505194781928934

Epoch: 138| Step: 0
Training loss: 2.143199920654297
Validation loss: 2.2482850474696003

Epoch: 6| Step: 1
Training loss: 2.1318254470825195
Validation loss: 2.25971423041436

Epoch: 6| Step: 2
Training loss: 1.9732115268707275
Validation loss: 2.2683388238312094

Epoch: 6| Step: 3
Training loss: 2.694955587387085
Validation loss: 2.2617833345167098

Epoch: 6| Step: 4
Training loss: 1.918038010597229
Validation loss: 2.2569241805743148

Epoch: 6| Step: 5
Training loss: 2.4481000900268555
Validation loss: 2.2608275105876308

Epoch: 6| Step: 6
Training loss: 2.585155963897705
Validation loss: 2.245624465327109

Epoch: 6| Step: 7
Training loss: 2.877808094024658
Validation loss: 2.255117775291525

Epoch: 6| Step: 8
Training loss: 1.970955491065979
Validation loss: 2.2329429670046737

Epoch: 6| Step: 9
Training loss: 2.58526873588562
Validation loss: 2.2326875271335727

Epoch: 6| Step: 10
Training loss: 3.1425108909606934
Validation loss: 2.2190674376744095

Epoch: 6| Step: 11
Training loss: 2.671196937561035
Validation loss: 2.2118381108007124

Epoch: 6| Step: 12
Training loss: 2.52750563621521
Validation loss: 2.205113107158292

Epoch: 6| Step: 13
Training loss: 2.4894542694091797
Validation loss: 2.2103540051367974

Epoch: 139| Step: 0
Training loss: 3.085998058319092
Validation loss: 2.2095350783358336

Epoch: 6| Step: 1
Training loss: 1.6674364805221558
Validation loss: 2.205111457455543

Epoch: 6| Step: 2
Training loss: 2.2414441108703613
Validation loss: 2.1988336655401413

Epoch: 6| Step: 3
Training loss: 2.550081253051758
Validation loss: 2.198254664738973

Epoch: 6| Step: 4
Training loss: 2.8248629570007324
Validation loss: 2.1928423450839136

Epoch: 6| Step: 5
Training loss: 2.1358399391174316
Validation loss: 2.1886734859917754

Epoch: 6| Step: 6
Training loss: 2.4648990631103516
Validation loss: 2.194014136509229

Epoch: 6| Step: 7
Training loss: 2.7277262210845947
Validation loss: 2.198949847170102

Epoch: 6| Step: 8
Training loss: 2.697808265686035
Validation loss: 2.2085695523087696

Epoch: 6| Step: 9
Training loss: 2.4269766807556152
Validation loss: 2.218869058034753

Epoch: 6| Step: 10
Training loss: 2.70979642868042
Validation loss: 2.2299754004324637

Epoch: 6| Step: 11
Training loss: 2.831422805786133
Validation loss: 2.2375807351963495

Epoch: 6| Step: 12
Training loss: 2.1652417182922363
Validation loss: 2.258162611274309

Epoch: 6| Step: 13
Training loss: 1.5726505517959595
Validation loss: 2.252981465349915

Epoch: 140| Step: 0
Training loss: 2.2373769283294678
Validation loss: 2.2460827622362363

Epoch: 6| Step: 1
Training loss: 2.37276029586792
Validation loss: 2.218794068982524

Epoch: 6| Step: 2
Training loss: 2.4974424839019775
Validation loss: 2.2206370522899013

Epoch: 6| Step: 3
Training loss: 2.657088279724121
Validation loss: 2.220477772015397

Epoch: 6| Step: 4
Training loss: 2.6783134937286377
Validation loss: 2.22416514478704

Epoch: 6| Step: 5
Training loss: 2.052485466003418
Validation loss: 2.2275466047307497

Epoch: 6| Step: 6
Training loss: 2.646733283996582
Validation loss: 2.2349120391312467

Epoch: 6| Step: 7
Training loss: 2.7754158973693848
Validation loss: 2.229445726640763

Epoch: 6| Step: 8
Training loss: 2.428335189819336
Validation loss: 2.221779597702847

Epoch: 6| Step: 9
Training loss: 1.8625084161758423
Validation loss: 2.215760538654943

Epoch: 6| Step: 10
Training loss: 3.043513059616089
Validation loss: 2.199155786985992

Epoch: 6| Step: 11
Training loss: 1.7557929754257202
Validation loss: 2.1996028218218076

Epoch: 6| Step: 12
Training loss: 2.233999252319336
Validation loss: 2.1998194545827885

Epoch: 6| Step: 13
Training loss: 3.599782705307007
Validation loss: 2.206849733988444

Epoch: 141| Step: 0
Training loss: 2.1897072792053223
Validation loss: 2.2065118307708413

Epoch: 6| Step: 1
Training loss: 1.8923242092132568
Validation loss: 2.220104355965891

Epoch: 6| Step: 2
Training loss: 2.2661385536193848
Validation loss: 2.222300111606557

Epoch: 6| Step: 3
Training loss: 1.743435263633728
Validation loss: 2.224145161208286

Epoch: 6| Step: 4
Training loss: 3.1054444313049316
Validation loss: 2.2220904160571355

Epoch: 6| Step: 5
Training loss: 2.6732535362243652
Validation loss: 2.217863605868432

Epoch: 6| Step: 6
Training loss: 2.6015660762786865
Validation loss: 2.2187285320733183

Epoch: 6| Step: 7
Training loss: 3.426659345626831
Validation loss: 2.2128734819350706

Epoch: 6| Step: 8
Training loss: 1.9334571361541748
Validation loss: 2.217483371816656

Epoch: 6| Step: 9
Training loss: 2.4209930896759033
Validation loss: 2.2039743982335573

Epoch: 6| Step: 10
Training loss: 2.6704599857330322
Validation loss: 2.196521148886732

Epoch: 6| Step: 11
Training loss: 2.28702974319458
Validation loss: 2.204180625177199

Epoch: 6| Step: 12
Training loss: 2.7983384132385254
Validation loss: 2.19742509113845

Epoch: 6| Step: 13
Training loss: 1.9396836757659912
Validation loss: 2.198722626573296

Epoch: 142| Step: 0
Training loss: 2.704293727874756
Validation loss: 2.2025112464863765

Epoch: 6| Step: 1
Training loss: 2.769465684890747
Validation loss: 2.204622141776546

Epoch: 6| Step: 2
Training loss: 1.7802269458770752
Validation loss: 2.2106790375965897

Epoch: 6| Step: 3
Training loss: 3.0155601501464844
Validation loss: 2.2118536503084245

Epoch: 6| Step: 4
Training loss: 2.9814305305480957
Validation loss: 2.224070031155822

Epoch: 6| Step: 5
Training loss: 2.2598931789398193
Validation loss: 2.2413695627643215

Epoch: 6| Step: 6
Training loss: 3.15303897857666
Validation loss: 2.235707656029732

Epoch: 6| Step: 7
Training loss: 2.5873541831970215
Validation loss: 2.2366314421417894

Epoch: 6| Step: 8
Training loss: 1.33726167678833
Validation loss: 2.223211430734204

Epoch: 6| Step: 9
Training loss: 2.6657378673553467
Validation loss: 2.209259808704417

Epoch: 6| Step: 10
Training loss: 2.0802693367004395
Validation loss: 2.212226708730062

Epoch: 6| Step: 11
Training loss: 1.9351673126220703
Validation loss: 2.213932755172894

Epoch: 6| Step: 12
Training loss: 2.487433433532715
Validation loss: 2.221459727133474

Epoch: 6| Step: 13
Training loss: 2.188624382019043
Validation loss: 2.220675928618318

Epoch: 143| Step: 0
Training loss: 2.7307021617889404
Validation loss: 2.221691782756518

Epoch: 6| Step: 1
Training loss: 2.3897132873535156
Validation loss: 2.217048170746014

Epoch: 6| Step: 2
Training loss: 1.8421697616577148
Validation loss: 2.1927897161053074

Epoch: 6| Step: 3
Training loss: 2.5576467514038086
Validation loss: 2.177129445537444

Epoch: 6| Step: 4
Training loss: 2.726522922515869
Validation loss: 2.176056710622644

Epoch: 6| Step: 5
Training loss: 1.9705169200897217
Validation loss: 2.1720061366276076

Epoch: 6| Step: 6
Training loss: 2.8558924198150635
Validation loss: 2.1635792563038487

Epoch: 6| Step: 7
Training loss: 2.348583698272705
Validation loss: 2.169274112229706

Epoch: 6| Step: 8
Training loss: 2.181649684906006
Validation loss: 2.1778061671923568

Epoch: 6| Step: 9
Training loss: 2.6409058570861816
Validation loss: 2.174226253263412

Epoch: 6| Step: 10
Training loss: 1.9966434240341187
Validation loss: 2.1753445363813833

Epoch: 6| Step: 11
Training loss: 2.6872365474700928
Validation loss: 2.1883886065534366

Epoch: 6| Step: 12
Training loss: 2.8443448543548584
Validation loss: 2.2024119515572824

Epoch: 6| Step: 13
Training loss: 2.174675226211548
Validation loss: 2.2240332531672653

Epoch: 144| Step: 0
Training loss: 3.0058717727661133
Validation loss: 2.2334364306542183

Epoch: 6| Step: 1
Training loss: 2.3648085594177246
Validation loss: 2.234433586879443

Epoch: 6| Step: 2
Training loss: 2.3792896270751953
Validation loss: 2.21685032690725

Epoch: 6| Step: 3
Training loss: 2.5907204151153564
Validation loss: 2.193572113590856

Epoch: 6| Step: 4
Training loss: 2.0660853385925293
Validation loss: 2.196649028408912

Epoch: 6| Step: 5
Training loss: 2.8910179138183594
Validation loss: 2.1926766621169222

Epoch: 6| Step: 6
Training loss: 1.85298490524292
Validation loss: 2.2037955099536526

Epoch: 6| Step: 7
Training loss: 2.70112681388855
Validation loss: 2.201028429051881

Epoch: 6| Step: 8
Training loss: 2.5962038040161133
Validation loss: 2.1975864479618687

Epoch: 6| Step: 9
Training loss: 2.671008586883545
Validation loss: 2.195767077066565

Epoch: 6| Step: 10
Training loss: 2.3579092025756836
Validation loss: 2.1888649232925905

Epoch: 6| Step: 11
Training loss: 2.101454973220825
Validation loss: 2.1855739085905013

Epoch: 6| Step: 12
Training loss: 2.333939552307129
Validation loss: 2.179377343064995

Epoch: 6| Step: 13
Training loss: 1.9000909328460693
Validation loss: 2.1803450558775213

Epoch: 145| Step: 0
Training loss: 3.106301784515381
Validation loss: 2.1965156806412565

Epoch: 6| Step: 1
Training loss: 2.488424301147461
Validation loss: 2.2050298311377086

Epoch: 6| Step: 2
Training loss: 2.0784122943878174
Validation loss: 2.22505864789409

Epoch: 6| Step: 3
Training loss: 1.5300874710083008
Validation loss: 2.2830061451081307

Epoch: 6| Step: 4
Training loss: 2.3088393211364746
Validation loss: 2.2980913564723027

Epoch: 6| Step: 5
Training loss: 2.01790714263916
Validation loss: 2.2781575315742084

Epoch: 6| Step: 6
Training loss: 2.493224620819092
Validation loss: 2.240482529004415

Epoch: 6| Step: 7
Training loss: 2.5489494800567627
Validation loss: 2.214458209212108

Epoch: 6| Step: 8
Training loss: 2.2586519718170166
Validation loss: 2.1969346897576445

Epoch: 6| Step: 9
Training loss: 2.624481201171875
Validation loss: 2.2007632076099353

Epoch: 6| Step: 10
Training loss: 3.1530823707580566
Validation loss: 2.2007854343742452

Epoch: 6| Step: 11
Training loss: 2.143489360809326
Validation loss: 2.2031936978781097

Epoch: 6| Step: 12
Training loss: 2.633274555206299
Validation loss: 2.1947083498841975

Epoch: 6| Step: 13
Training loss: 2.6718926429748535
Validation loss: 2.195527994504539

Epoch: 146| Step: 0
Training loss: 1.8011515140533447
Validation loss: 2.2050246961655153

Epoch: 6| Step: 1
Training loss: 1.9635566473007202
Validation loss: 2.206332160580543

Epoch: 6| Step: 2
Training loss: 2.1527349948883057
Validation loss: 2.2049553099498955

Epoch: 6| Step: 3
Training loss: 2.1234405040740967
Validation loss: 2.2104411240546935

Epoch: 6| Step: 4
Training loss: 2.807002305984497
Validation loss: 2.2063892323483705

Epoch: 6| Step: 5
Training loss: 2.1614224910736084
Validation loss: 2.22732045573573

Epoch: 6| Step: 6
Training loss: 2.6852643489837646
Validation loss: 2.2414877440339778

Epoch: 6| Step: 7
Training loss: 2.9867653846740723
Validation loss: 2.236322218371976

Epoch: 6| Step: 8
Training loss: 3.3879945278167725
Validation loss: 2.2361486342645462

Epoch: 6| Step: 9
Training loss: 2.316380500793457
Validation loss: 2.2272114343540643

Epoch: 6| Step: 10
Training loss: 1.9500062465667725
Validation loss: 2.2078054745992026

Epoch: 6| Step: 11
Training loss: 3.068589448928833
Validation loss: 2.19767734055878

Epoch: 6| Step: 12
Training loss: 2.5261712074279785
Validation loss: 2.18017719381599

Epoch: 6| Step: 13
Training loss: 1.4819259643554688
Validation loss: 2.177847279015408

Epoch: 147| Step: 0
Training loss: 2.9675817489624023
Validation loss: 2.1746000218135055

Epoch: 6| Step: 1
Training loss: 2.3893256187438965
Validation loss: 2.175880209092171

Epoch: 6| Step: 2
Training loss: 2.868920087814331
Validation loss: 2.1784608530741867

Epoch: 6| Step: 3
Training loss: 2.3379087448120117
Validation loss: 2.186000681692554

Epoch: 6| Step: 4
Training loss: 2.2299838066101074
Validation loss: 2.192013668757613

Epoch: 6| Step: 5
Training loss: 2.7768821716308594
Validation loss: 2.184794522100879

Epoch: 6| Step: 6
Training loss: 2.177945137023926
Validation loss: 2.1789477102218138

Epoch: 6| Step: 7
Training loss: 2.571479320526123
Validation loss: 2.182679009693925

Epoch: 6| Step: 8
Training loss: 2.271064519882202
Validation loss: 2.180700553360806

Epoch: 6| Step: 9
Training loss: 2.2246148586273193
Validation loss: 2.1816514871453725

Epoch: 6| Step: 10
Training loss: 1.5036060810089111
Validation loss: 2.1874663496530182

Epoch: 6| Step: 11
Training loss: 2.1854164600372314
Validation loss: 2.1833891048226306

Epoch: 6| Step: 12
Training loss: 2.468871593475342
Validation loss: 2.1790785174215994

Epoch: 6| Step: 13
Training loss: 3.256807327270508
Validation loss: 2.1810694715028167

Epoch: 148| Step: 0
Training loss: 2.1956794261932373
Validation loss: 2.1893867805439937

Epoch: 6| Step: 1
Training loss: 2.838240385055542
Validation loss: 2.2064212547835482

Epoch: 6| Step: 2
Training loss: 1.683990478515625
Validation loss: 2.1972425317251556

Epoch: 6| Step: 3
Training loss: 2.141266107559204
Validation loss: 2.2051690701515443

Epoch: 6| Step: 4
Training loss: 2.481553554534912
Validation loss: 2.1833926657194733

Epoch: 6| Step: 5
Training loss: 2.254032850265503
Validation loss: 2.1919722377613025

Epoch: 6| Step: 6
Training loss: 2.7195334434509277
Validation loss: 2.1862634420394897

Epoch: 6| Step: 7
Training loss: 2.4516780376434326
Validation loss: 2.1848203123256726

Epoch: 6| Step: 8
Training loss: 3.0692155361175537
Validation loss: 2.197701361871535

Epoch: 6| Step: 9
Training loss: 2.1050398349761963
Validation loss: 2.217884839221995

Epoch: 6| Step: 10
Training loss: 2.5004212856292725
Validation loss: 2.212467021839593

Epoch: 6| Step: 11
Training loss: 2.7827839851379395
Validation loss: 2.2115853858250443

Epoch: 6| Step: 12
Training loss: 1.8194712400436401
Validation loss: 2.2084401140930834

Epoch: 6| Step: 13
Training loss: 2.8776330947875977
Validation loss: 2.2050290210272676

Epoch: 149| Step: 0
Training loss: 2.389026165008545
Validation loss: 2.2229940839993056

Epoch: 6| Step: 1
Training loss: 2.411384105682373
Validation loss: 2.23446592720606

Epoch: 6| Step: 2
Training loss: 2.3633360862731934
Validation loss: 2.2420165141423545

Epoch: 6| Step: 3
Training loss: 2.0054876804351807
Validation loss: 2.261499950962682

Epoch: 6| Step: 4
Training loss: 2.7929792404174805
Validation loss: 2.2545561175192557

Epoch: 6| Step: 5
Training loss: 2.280329704284668
Validation loss: 2.2248469770595594

Epoch: 6| Step: 6
Training loss: 2.2193286418914795
Validation loss: 2.2148619082666214

Epoch: 6| Step: 7
Training loss: 2.1293511390686035
Validation loss: 2.2106324549644225

Epoch: 6| Step: 8
Training loss: 2.827944040298462
Validation loss: 2.1996893523841776

Epoch: 6| Step: 9
Training loss: 1.8888881206512451
Validation loss: 2.1989946442265667

Epoch: 6| Step: 10
Training loss: 2.3949947357177734
Validation loss: 2.2013371118935208

Epoch: 6| Step: 11
Training loss: 2.2310781478881836
Validation loss: 2.2136473450609433

Epoch: 6| Step: 12
Training loss: 2.9592971801757812
Validation loss: 2.23339307180015

Epoch: 6| Step: 13
Training loss: 3.088250160217285
Validation loss: 2.2338824810520297

Epoch: 150| Step: 0
Training loss: 2.74227237701416
Validation loss: 2.227075251199866

Epoch: 6| Step: 1
Training loss: 3.065934181213379
Validation loss: 2.207335300343011

Epoch: 6| Step: 2
Training loss: 2.2737655639648438
Validation loss: 2.193398601265364

Epoch: 6| Step: 3
Training loss: 2.201312303543091
Validation loss: 2.1763489272004817

Epoch: 6| Step: 4
Training loss: 2.8623409271240234
Validation loss: 2.18092482320724

Epoch: 6| Step: 5
Training loss: 2.040961742401123
Validation loss: 2.1757512464318225

Epoch: 6| Step: 6
Training loss: 2.3215761184692383
Validation loss: 2.196430462662892

Epoch: 6| Step: 7
Training loss: 2.5801873207092285
Validation loss: 2.188615108049044

Epoch: 6| Step: 8
Training loss: 2.573246717453003
Validation loss: 2.187596285214988

Epoch: 6| Step: 9
Training loss: 1.921614170074463
Validation loss: 2.1879552974495837

Epoch: 6| Step: 10
Training loss: 2.4311280250549316
Validation loss: 2.174959276312141

Epoch: 6| Step: 11
Training loss: 1.959669828414917
Validation loss: 2.1695973373228505

Epoch: 6| Step: 12
Training loss: 2.318483352661133
Validation loss: 2.1748013086216424

Epoch: 6| Step: 13
Training loss: 2.6361169815063477
Validation loss: 2.178063920749131

Testing loss: 2.4002071327633328

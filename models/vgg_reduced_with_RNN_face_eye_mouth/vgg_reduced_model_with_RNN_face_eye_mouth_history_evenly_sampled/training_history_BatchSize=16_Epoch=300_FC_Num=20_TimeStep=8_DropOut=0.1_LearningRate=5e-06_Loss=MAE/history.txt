Epoch: 1| Step: 0
Training loss: 4.381899356842041
Validation loss: 5.14387462985131

Epoch: 6| Step: 1
Training loss: 5.102329730987549
Validation loss: 5.138604117978003

Epoch: 6| Step: 2
Training loss: 4.708293914794922
Validation loss: 5.133445970473751

Epoch: 6| Step: 3
Training loss: 6.024064064025879
Validation loss: 5.128522267905614

Epoch: 6| Step: 4
Training loss: 4.696283340454102
Validation loss: 5.123873444013698

Epoch: 6| Step: 5
Training loss: 4.056540489196777
Validation loss: 5.11977114728702

Epoch: 6| Step: 6
Training loss: 4.07208251953125
Validation loss: 5.115384814559772

Epoch: 6| Step: 7
Training loss: 6.422886848449707
Validation loss: 5.110701130282495

Epoch: 6| Step: 8
Training loss: 5.112827301025391
Validation loss: 5.1069365368094495

Epoch: 6| Step: 9
Training loss: 3.8006534576416016
Validation loss: 5.102653887964064

Epoch: 6| Step: 10
Training loss: 5.027828216552734
Validation loss: 5.098266381089405

Epoch: 6| Step: 11
Training loss: 5.803596019744873
Validation loss: 5.09372967032976

Epoch: 6| Step: 12
Training loss: 5.394428253173828
Validation loss: 5.089121203268728

Epoch: 6| Step: 13
Training loss: 3.544314384460449
Validation loss: 5.084089022810741

Epoch: 2| Step: 0
Training loss: 4.858503341674805
Validation loss: 5.079199852481965

Epoch: 6| Step: 1
Training loss: 4.27225923538208
Validation loss: 5.073745635248

Epoch: 6| Step: 2
Training loss: 4.632791042327881
Validation loss: 5.068140014525382

Epoch: 6| Step: 3
Training loss: 6.043608665466309
Validation loss: 5.0624152614224345

Epoch: 6| Step: 4
Training loss: 3.9478940963745117
Validation loss: 5.0563477546938005

Epoch: 6| Step: 5
Training loss: 4.708965301513672
Validation loss: 5.050309360668224

Epoch: 6| Step: 6
Training loss: 5.011560916900635
Validation loss: 5.043804835247737

Epoch: 6| Step: 7
Training loss: 4.874373912811279
Validation loss: 5.037260809252339

Epoch: 6| Step: 8
Training loss: 4.628702163696289
Validation loss: 5.029964493167016

Epoch: 6| Step: 9
Training loss: 5.3820085525512695
Validation loss: 5.022630399273288

Epoch: 6| Step: 10
Training loss: 5.996952056884766
Validation loss: 5.01465714875088

Epoch: 6| Step: 11
Training loss: 5.387582302093506
Validation loss: 5.0071713129679365

Epoch: 6| Step: 12
Training loss: 2.827106475830078
Validation loss: 4.998533556538243

Epoch: 6| Step: 13
Training loss: 5.348933696746826
Validation loss: 4.989772842776391

Epoch: 3| Step: 0
Training loss: 4.071148872375488
Validation loss: 4.980508809448571

Epoch: 6| Step: 1
Training loss: 3.9496817588806152
Validation loss: 4.971075345111149

Epoch: 6| Step: 2
Training loss: 5.865661144256592
Validation loss: 4.961630282863494

Epoch: 6| Step: 3
Training loss: 5.471811294555664
Validation loss: 4.951290469015798

Epoch: 6| Step: 4
Training loss: 4.677061080932617
Validation loss: 4.941135068093577

Epoch: 6| Step: 5
Training loss: 5.137514591217041
Validation loss: 4.929376714973039

Epoch: 6| Step: 6
Training loss: 4.257912635803223
Validation loss: 4.917798119206583

Epoch: 6| Step: 7
Training loss: 5.234590530395508
Validation loss: 4.906107892272293

Epoch: 6| Step: 8
Training loss: 3.017876386642456
Validation loss: 4.893114184820524

Epoch: 6| Step: 9
Training loss: 4.510740280151367
Validation loss: 4.880396704519948

Epoch: 6| Step: 10
Training loss: 4.953606605529785
Validation loss: 4.866383896079115

Epoch: 6| Step: 11
Training loss: 5.366250038146973
Validation loss: 4.85256169431953

Epoch: 6| Step: 12
Training loss: 4.858931541442871
Validation loss: 4.837533130440661

Epoch: 6| Step: 13
Training loss: 4.311152935028076
Validation loss: 4.82245196065595

Epoch: 4| Step: 0
Training loss: 5.246318817138672
Validation loss: 4.807304859161377

Epoch: 6| Step: 1
Training loss: 3.704023838043213
Validation loss: 4.792531721053585

Epoch: 6| Step: 2
Training loss: 4.957793712615967
Validation loss: 4.776241989545925

Epoch: 6| Step: 3
Training loss: 5.071956157684326
Validation loss: 4.759030188283613

Epoch: 6| Step: 4
Training loss: 4.892786502838135
Validation loss: 4.741437271077146

Epoch: 6| Step: 5
Training loss: 4.3355607986450195
Validation loss: 4.725411394590973

Epoch: 6| Step: 6
Training loss: 4.880599021911621
Validation loss: 4.708568598634454

Epoch: 6| Step: 7
Training loss: 3.4759812355041504
Validation loss: 4.691097392830797

Epoch: 6| Step: 8
Training loss: 5.476846218109131
Validation loss: 4.673348426818848

Epoch: 6| Step: 9
Training loss: 4.256170272827148
Validation loss: 4.655380854042628

Epoch: 6| Step: 10
Training loss: 4.14985466003418
Validation loss: 4.637297122709213

Epoch: 6| Step: 11
Training loss: 4.117451190948486
Validation loss: 4.618855655834239

Epoch: 6| Step: 12
Training loss: 4.266096115112305
Validation loss: 4.60048182036287

Epoch: 6| Step: 13
Training loss: 3.519779920578003
Validation loss: 4.582023266823061

Epoch: 5| Step: 0
Training loss: 4.893243789672852
Validation loss: 4.562949721531202

Epoch: 6| Step: 1
Training loss: 4.030998229980469
Validation loss: 4.543840398070633

Epoch: 6| Step: 2
Training loss: 4.865286350250244
Validation loss: 4.524944546402142

Epoch: 6| Step: 3
Training loss: 5.457371711730957
Validation loss: 4.5057693860864125

Epoch: 6| Step: 4
Training loss: 4.113168716430664
Validation loss: 4.487785928992815

Epoch: 6| Step: 5
Training loss: 3.9901962280273438
Validation loss: 4.4682602215838685

Epoch: 6| Step: 6
Training loss: 3.5953850746154785
Validation loss: 4.450063646480602

Epoch: 6| Step: 7
Training loss: 3.370185375213623
Validation loss: 4.43116400318761

Epoch: 6| Step: 8
Training loss: 5.01866340637207
Validation loss: 4.412262537146128

Epoch: 6| Step: 9
Training loss: 3.841120481491089
Validation loss: 4.3940559971717095

Epoch: 6| Step: 10
Training loss: 4.206640243530273
Validation loss: 4.375963159786758

Epoch: 6| Step: 11
Training loss: 3.4312210083007812
Validation loss: 4.355363158769505

Epoch: 6| Step: 12
Training loss: 3.8762357234954834
Validation loss: 4.336767576074087

Epoch: 6| Step: 13
Training loss: 5.046707630157471
Validation loss: 4.3167561305466515

Epoch: 6| Step: 0
Training loss: 4.832126617431641
Validation loss: 4.298318006659067

Epoch: 6| Step: 1
Training loss: 4.863145351409912
Validation loss: 4.278903345907888

Epoch: 6| Step: 2
Training loss: 3.6421446800231934
Validation loss: 4.259688720908216

Epoch: 6| Step: 3
Training loss: 3.6031196117401123
Validation loss: 4.240966120073872

Epoch: 6| Step: 4
Training loss: 3.4560861587524414
Validation loss: 4.220500289752919

Epoch: 6| Step: 5
Training loss: 4.510365962982178
Validation loss: 4.202912997174007

Epoch: 6| Step: 6
Training loss: 2.8210043907165527
Validation loss: 4.183214977223386

Epoch: 6| Step: 7
Training loss: 3.5631394386291504
Validation loss: 4.165564408866308

Epoch: 6| Step: 8
Training loss: 4.422172546386719
Validation loss: 4.147337867367652

Epoch: 6| Step: 9
Training loss: 3.376030206680298
Validation loss: 4.129931880581763

Epoch: 6| Step: 10
Training loss: 3.7423808574676514
Validation loss: 4.110474135286065

Epoch: 6| Step: 11
Training loss: 3.855499267578125
Validation loss: 4.095531166240733

Epoch: 6| Step: 12
Training loss: 5.303544521331787
Validation loss: 4.079066086840886

Epoch: 6| Step: 13
Training loss: 3.857393503189087
Validation loss: 4.064233564561413

Epoch: 7| Step: 0
Training loss: 3.985431671142578
Validation loss: 4.05012825996645

Epoch: 6| Step: 1
Training loss: 2.413478136062622
Validation loss: 4.038129157917474

Epoch: 6| Step: 2
Training loss: 5.330594062805176
Validation loss: 4.025012549533639

Epoch: 6| Step: 3
Training loss: 4.710824012756348
Validation loss: 4.011729112235448

Epoch: 6| Step: 4
Training loss: 3.9361178874969482
Validation loss: 4.000909213096865

Epoch: 6| Step: 5
Training loss: 4.524182319641113
Validation loss: 3.9878686730579664

Epoch: 6| Step: 6
Training loss: 3.201279640197754
Validation loss: 3.9773657629566808

Epoch: 6| Step: 7
Training loss: 4.029849052429199
Validation loss: 3.9661671500052176

Epoch: 6| Step: 8
Training loss: 3.5275216102600098
Validation loss: 3.9541168828164377

Epoch: 6| Step: 9
Training loss: 3.5952706336975098
Validation loss: 3.9427903134335756

Epoch: 6| Step: 10
Training loss: 4.290945053100586
Validation loss: 3.9336122953763573

Epoch: 6| Step: 11
Training loss: 3.1661322116851807
Validation loss: 3.9209336567950506

Epoch: 6| Step: 12
Training loss: 3.3157622814178467
Validation loss: 3.9108223146007908

Epoch: 6| Step: 13
Training loss: 3.28794527053833
Validation loss: 3.90016310445724

Epoch: 8| Step: 0
Training loss: 4.020506858825684
Validation loss: 3.8912002501949186

Epoch: 6| Step: 1
Training loss: 3.694228172302246
Validation loss: 3.881940487892397

Epoch: 6| Step: 2
Training loss: 3.4392354488372803
Validation loss: 3.8724146196919103

Epoch: 6| Step: 3
Training loss: 3.8561959266662598
Validation loss: 3.8614345981228735

Epoch: 6| Step: 4
Training loss: 4.951385974884033
Validation loss: 3.855846845975486

Epoch: 6| Step: 5
Training loss: 3.774822235107422
Validation loss: 3.8454925449945594

Epoch: 6| Step: 6
Training loss: 2.860607862472534
Validation loss: 3.834441041433683

Epoch: 6| Step: 7
Training loss: 4.054512977600098
Validation loss: 3.8262250218340146

Epoch: 6| Step: 8
Training loss: 2.7027029991149902
Validation loss: 3.8182592853423087

Epoch: 6| Step: 9
Training loss: 5.4832611083984375
Validation loss: 3.807993083871821

Epoch: 6| Step: 10
Training loss: 3.574514627456665
Validation loss: 3.7998374969728532

Epoch: 6| Step: 11
Training loss: 2.8158249855041504
Validation loss: 3.791110295121388

Epoch: 6| Step: 12
Training loss: 4.11995792388916
Validation loss: 3.783078924302132

Epoch: 6| Step: 13
Training loss: 1.7894924879074097
Validation loss: 3.7744018903342624

Epoch: 9| Step: 0
Training loss: 3.7984020709991455
Validation loss: 3.765646211562618

Epoch: 6| Step: 1
Training loss: 3.8256611824035645
Validation loss: 3.7575351243378012

Epoch: 6| Step: 2
Training loss: 4.4927544593811035
Validation loss: 3.7485918896172636

Epoch: 6| Step: 3
Training loss: 3.680959701538086
Validation loss: 3.74091326549489

Epoch: 6| Step: 4
Training loss: 3.228778839111328
Validation loss: 3.730352109478366

Epoch: 6| Step: 5
Training loss: 4.721396446228027
Validation loss: 3.720114149073119

Epoch: 6| Step: 6
Training loss: 3.257317066192627
Validation loss: 3.708096901575724

Epoch: 6| Step: 7
Training loss: 3.854825019836426
Validation loss: 3.697573789986231

Epoch: 6| Step: 8
Training loss: 4.163089275360107
Validation loss: 3.6914044323787896

Epoch: 6| Step: 9
Training loss: 4.724668979644775
Validation loss: 3.6814712298813688

Epoch: 6| Step: 10
Training loss: 1.6686931848526
Validation loss: 3.671438373545165

Epoch: 6| Step: 11
Training loss: 2.8496081829071045
Validation loss: 3.669376306636359

Epoch: 6| Step: 12
Training loss: 2.466257095336914
Validation loss: 3.664459930953159

Epoch: 6| Step: 13
Training loss: 3.9852163791656494
Validation loss: 3.658312628346105

Epoch: 10| Step: 0
Training loss: 2.5889406204223633
Validation loss: 3.6544122593377226

Epoch: 6| Step: 1
Training loss: 3.2998905181884766
Validation loss: 3.647797276896815

Epoch: 6| Step: 2
Training loss: 3.504495143890381
Validation loss: 3.641254376339656

Epoch: 6| Step: 3
Training loss: 3.1251790523529053
Validation loss: 3.638839849861719

Epoch: 6| Step: 4
Training loss: 4.823145866394043
Validation loss: 3.6313762869886173

Epoch: 6| Step: 5
Training loss: 4.396693229675293
Validation loss: 3.6246556158988708

Epoch: 6| Step: 6
Training loss: 3.5307376384735107
Validation loss: 3.6197528172564764

Epoch: 6| Step: 7
Training loss: 3.4937362670898438
Validation loss: 3.6147680256956365

Epoch: 6| Step: 8
Training loss: 3.659960985183716
Validation loss: 3.6083194619865826

Epoch: 6| Step: 9
Training loss: 4.066056251525879
Validation loss: 3.6044489440097602

Epoch: 6| Step: 10
Training loss: 2.389763832092285
Validation loss: 3.5978626948530956

Epoch: 6| Step: 11
Training loss: 4.127859592437744
Validation loss: 3.592425548902122

Epoch: 6| Step: 12
Training loss: 3.7426600456237793
Validation loss: 3.5859138478514967

Epoch: 6| Step: 13
Training loss: 2.0284440517425537
Validation loss: 3.5825316880338933

Epoch: 11| Step: 0
Training loss: 2.949756622314453
Validation loss: 3.57495278440496

Epoch: 6| Step: 1
Training loss: 2.95131778717041
Validation loss: 3.5702174966053297

Epoch: 6| Step: 2
Training loss: 4.524476528167725
Validation loss: 3.5646038106692735

Epoch: 6| Step: 3
Training loss: 3.972464084625244
Validation loss: 3.559499438090991

Epoch: 6| Step: 4
Training loss: 4.034753799438477
Validation loss: 3.5532331235947145

Epoch: 6| Step: 5
Training loss: 3.6510705947875977
Validation loss: 3.546699405998312

Epoch: 6| Step: 6
Training loss: 4.743755340576172
Validation loss: 3.5402575615913636

Epoch: 6| Step: 7
Training loss: 3.262357234954834
Validation loss: 3.5312986732811056

Epoch: 6| Step: 8
Training loss: 3.709167242050171
Validation loss: 3.523986006295809

Epoch: 6| Step: 9
Training loss: 3.2117862701416016
Validation loss: 3.5192496956035657

Epoch: 6| Step: 10
Training loss: 3.1874780654907227
Validation loss: 3.510545017898724

Epoch: 6| Step: 11
Training loss: 2.439871072769165
Validation loss: 3.5028659964120514

Epoch: 6| Step: 12
Training loss: 2.7361857891082764
Validation loss: 3.4993396958997174

Epoch: 6| Step: 13
Training loss: 2.911778211593628
Validation loss: 3.4935539153314408

Epoch: 12| Step: 0
Training loss: 3.803225517272949
Validation loss: 3.483185029798938

Epoch: 6| Step: 1
Training loss: 3.7478623390197754
Validation loss: 3.479949907589984

Epoch: 6| Step: 2
Training loss: 3.359891891479492
Validation loss: 3.4699985160622546

Epoch: 6| Step: 3
Training loss: 2.2594995498657227
Validation loss: 3.463863065165858

Epoch: 6| Step: 4
Training loss: 3.254716396331787
Validation loss: 3.4591916837999896

Epoch: 6| Step: 5
Training loss: 3.465928554534912
Validation loss: 3.4538911363129974

Epoch: 6| Step: 6
Training loss: 2.570923328399658
Validation loss: 3.4483200760297876

Epoch: 6| Step: 7
Training loss: 3.6063637733459473
Validation loss: 3.443866781009141

Epoch: 6| Step: 8
Training loss: 3.5558323860168457
Validation loss: 3.439943616108228

Epoch: 6| Step: 9
Training loss: 3.53806209564209
Validation loss: 3.43276293816105

Epoch: 6| Step: 10
Training loss: 4.161919593811035
Validation loss: 3.4272354443868003

Epoch: 6| Step: 11
Training loss: 3.306779384613037
Validation loss: 3.422008281112999

Epoch: 6| Step: 12
Training loss: 3.0216169357299805
Validation loss: 3.415833680860458

Epoch: 6| Step: 13
Training loss: 4.073483943939209
Validation loss: 3.410209363506686

Epoch: 13| Step: 0
Training loss: 3.2836668491363525
Validation loss: 3.407111642181232

Epoch: 6| Step: 1
Training loss: 3.2970080375671387
Validation loss: 3.4023619672303558

Epoch: 6| Step: 2
Training loss: 2.729097843170166
Validation loss: 3.3984722270760486

Epoch: 6| Step: 3
Training loss: 4.049402236938477
Validation loss: 3.3954129449782835

Epoch: 6| Step: 4
Training loss: 1.9454197883605957
Validation loss: 3.399946810096823

Epoch: 6| Step: 5
Training loss: 3.870530128479004
Validation loss: 3.432244511060817

Epoch: 6| Step: 6
Training loss: 3.7746188640594482
Validation loss: 3.4156233597827215

Epoch: 6| Step: 7
Training loss: 3.4085705280303955
Validation loss: 3.3809797379278366

Epoch: 6| Step: 8
Training loss: 2.849655866622925
Validation loss: 3.3719331192713913

Epoch: 6| Step: 9
Training loss: 2.997406005859375
Validation loss: 3.369412288870863

Epoch: 6| Step: 10
Training loss: 2.984365463256836
Validation loss: 3.367377324770856

Epoch: 6| Step: 11
Training loss: 4.157055854797363
Validation loss: 3.3640042838229927

Epoch: 6| Step: 12
Training loss: 3.2157766819000244
Validation loss: 3.35931134223938

Epoch: 6| Step: 13
Training loss: 4.756644248962402
Validation loss: 3.3544453472219486

Epoch: 14| Step: 0
Training loss: 3.598562240600586
Validation loss: 3.3492699566707818

Epoch: 6| Step: 1
Training loss: 2.7041878700256348
Validation loss: 3.350468243322065

Epoch: 6| Step: 2
Training loss: 3.919706344604492
Validation loss: 3.350409153969057

Epoch: 6| Step: 3
Training loss: 3.609199285507202
Validation loss: 3.346365995304559

Epoch: 6| Step: 4
Training loss: 3.2235846519470215
Validation loss: 3.3440829861548638

Epoch: 6| Step: 5
Training loss: 3.0802574157714844
Validation loss: 3.336947259082589

Epoch: 6| Step: 6
Training loss: 2.7516205310821533
Validation loss: 3.333533130666261

Epoch: 6| Step: 7
Training loss: 3.7502100467681885
Validation loss: 3.329883908712736

Epoch: 6| Step: 8
Training loss: 3.574301242828369
Validation loss: 3.3233754224674676

Epoch: 6| Step: 9
Training loss: 3.306758403778076
Validation loss: 3.3220695090550247

Epoch: 6| Step: 10
Training loss: 2.480226993560791
Validation loss: 3.3136967869215113

Epoch: 6| Step: 11
Training loss: 3.355794906616211
Validation loss: 3.3100428581237793

Epoch: 6| Step: 12
Training loss: 2.868769407272339
Validation loss: 3.301075889218238

Epoch: 6| Step: 13
Training loss: 4.29250431060791
Validation loss: 3.29457926493819

Epoch: 15| Step: 0
Training loss: 3.612246513366699
Validation loss: 3.2902987362236105

Epoch: 6| Step: 1
Training loss: 2.753113269805908
Validation loss: 3.2857261268041467

Epoch: 6| Step: 2
Training loss: 3.3927927017211914
Validation loss: 3.2783899050886913

Epoch: 6| Step: 3
Training loss: 3.3468613624572754
Validation loss: 3.2753058043859338

Epoch: 6| Step: 4
Training loss: 3.677865505218506
Validation loss: 3.2719323378737255

Epoch: 6| Step: 5
Training loss: 2.34399676322937
Validation loss: 3.2681955265742477

Epoch: 6| Step: 6
Training loss: 3.4262642860412598
Validation loss: 3.2653836973251833

Epoch: 6| Step: 7
Training loss: 3.509828805923462
Validation loss: 3.261842466169788

Epoch: 6| Step: 8
Training loss: 4.461252212524414
Validation loss: 3.2585785722219818

Epoch: 6| Step: 9
Training loss: 2.7426869869232178
Validation loss: 3.252656393153693

Epoch: 6| Step: 10
Training loss: 3.126150369644165
Validation loss: 3.2441930027418238

Epoch: 6| Step: 11
Training loss: 3.1579790115356445
Validation loss: 3.2425446843588226

Epoch: 6| Step: 12
Training loss: 2.725883722305298
Validation loss: 3.241054404166437

Epoch: 6| Step: 13
Training loss: 2.986156940460205
Validation loss: 3.240750889624319

Epoch: 16| Step: 0
Training loss: 3.4594597816467285
Validation loss: 3.2367417581619753

Epoch: 6| Step: 1
Training loss: 2.4176430702209473
Validation loss: 3.2344679858094905

Epoch: 6| Step: 2
Training loss: 2.514528274536133
Validation loss: 3.233815295721895

Epoch: 6| Step: 3
Training loss: 3.8801450729370117
Validation loss: 3.2331799742996052

Epoch: 6| Step: 4
Training loss: 3.697843551635742
Validation loss: 3.229063626258604

Epoch: 6| Step: 5
Training loss: 3.4278712272644043
Validation loss: 3.226429162486907

Epoch: 6| Step: 6
Training loss: 2.4550251960754395
Validation loss: 3.2249016402870097

Epoch: 6| Step: 7
Training loss: 3.3746719360351562
Validation loss: 3.2209187912684616

Epoch: 6| Step: 8
Training loss: 3.1231398582458496
Validation loss: 3.217751218426612

Epoch: 6| Step: 9
Training loss: 2.887526035308838
Validation loss: 3.2161339329135035

Epoch: 6| Step: 10
Training loss: 3.3562252521514893
Validation loss: 3.2132995615723314

Epoch: 6| Step: 11
Training loss: 3.7551634311676025
Validation loss: 3.2072324137533865

Epoch: 6| Step: 12
Training loss: 3.691627025604248
Validation loss: 3.203725327727615

Epoch: 6| Step: 13
Training loss: 2.597524642944336
Validation loss: 3.2009020031139417

Epoch: 17| Step: 0
Training loss: 2.781445026397705
Validation loss: 3.1960746037062777

Epoch: 6| Step: 1
Training loss: 4.129944801330566
Validation loss: 3.19146974625126

Epoch: 6| Step: 2
Training loss: 3.274940013885498
Validation loss: 3.1828895640629593

Epoch: 6| Step: 3
Training loss: 3.498453378677368
Validation loss: 3.178448669372066

Epoch: 6| Step: 4
Training loss: 3.3569979667663574
Validation loss: 3.1749689091918287

Epoch: 6| Step: 5
Training loss: 2.968740940093994
Validation loss: 3.1721288593866492

Epoch: 6| Step: 6
Training loss: 2.962096929550171
Validation loss: 3.162935164666945

Epoch: 6| Step: 7
Training loss: 1.8783178329467773
Validation loss: 3.1523714219370196

Epoch: 6| Step: 8
Training loss: 2.4776968955993652
Validation loss: 3.149570890652236

Epoch: 6| Step: 9
Training loss: 3.261303424835205
Validation loss: 3.1508521059507966

Epoch: 6| Step: 10
Training loss: 4.104101181030273
Validation loss: 3.1375631593888804

Epoch: 6| Step: 11
Training loss: 3.1570708751678467
Validation loss: 3.1332917521076817

Epoch: 6| Step: 12
Training loss: 3.1311888694763184
Validation loss: 3.1244667319841284

Epoch: 6| Step: 13
Training loss: 3.433762311935425
Validation loss: 3.1228766082435526

Epoch: 18| Step: 0
Training loss: 2.6540277004241943
Validation loss: 3.1205790222332044

Epoch: 6| Step: 1
Training loss: 2.963013172149658
Validation loss: 3.118143468774775

Epoch: 6| Step: 2
Training loss: 3.6573853492736816
Validation loss: 3.1106616989258797

Epoch: 6| Step: 3
Training loss: 2.9808430671691895
Validation loss: 3.1057800349368843

Epoch: 6| Step: 4
Training loss: 3.1437907218933105
Validation loss: 3.1013042593515046

Epoch: 6| Step: 5
Training loss: 3.006309986114502
Validation loss: 3.1009443934245775

Epoch: 6| Step: 6
Training loss: 3.4669713973999023
Validation loss: 3.0964033808759464

Epoch: 6| Step: 7
Training loss: 3.237631320953369
Validation loss: 3.0890335934136504

Epoch: 6| Step: 8
Training loss: 3.500880479812622
Validation loss: 3.089851922886346

Epoch: 6| Step: 9
Training loss: 2.7141973972320557
Validation loss: 3.0858263969421387

Epoch: 6| Step: 10
Training loss: 2.894198417663574
Validation loss: 3.079256203866774

Epoch: 6| Step: 11
Training loss: 3.020528554916382
Validation loss: 3.08119656578187

Epoch: 6| Step: 12
Training loss: 3.2863268852233887
Validation loss: 3.0738359933258383

Epoch: 6| Step: 13
Training loss: 3.2548937797546387
Validation loss: 3.0705865942021853

Epoch: 19| Step: 0
Training loss: 3.836470127105713
Validation loss: 3.0574996625223467

Epoch: 6| Step: 1
Training loss: 3.445084810256958
Validation loss: 3.0554606119791665

Epoch: 6| Step: 2
Training loss: 2.0782108306884766
Validation loss: 3.050629169710221

Epoch: 6| Step: 3
Training loss: 3.2146825790405273
Validation loss: 3.047680711233488

Epoch: 6| Step: 4
Training loss: 3.092586040496826
Validation loss: 3.041710981758692

Epoch: 6| Step: 5
Training loss: 3.6446733474731445
Validation loss: 3.037842155784689

Epoch: 6| Step: 6
Training loss: 2.5281319618225098
Validation loss: 3.0356437877942155

Epoch: 6| Step: 7
Training loss: 3.7676103115081787
Validation loss: 3.0361556468471402

Epoch: 6| Step: 8
Training loss: 2.5159974098205566
Validation loss: 3.0273931769914526

Epoch: 6| Step: 9
Training loss: 2.5350375175476074
Validation loss: 3.0207542142560406

Epoch: 6| Step: 10
Training loss: 3.790799379348755
Validation loss: 3.0194010503830446

Epoch: 6| Step: 11
Training loss: 3.01737117767334
Validation loss: 3.0175652452694472

Epoch: 6| Step: 12
Training loss: 2.6701154708862305
Validation loss: 3.0131003625931276

Epoch: 6| Step: 13
Training loss: 3.029308319091797
Validation loss: 3.004464070002238

Epoch: 20| Step: 0
Training loss: 3.155965566635132
Validation loss: 3.0060357098938315

Epoch: 6| Step: 1
Training loss: 3.261679172515869
Validation loss: 3.008543270890431

Epoch: 6| Step: 2
Training loss: 2.5842156410217285
Validation loss: 3.0077166403493574

Epoch: 6| Step: 3
Training loss: 3.05228853225708
Validation loss: 3.0027969088605655

Epoch: 6| Step: 4
Training loss: 2.618800401687622
Validation loss: 3.000057492204892

Epoch: 6| Step: 5
Training loss: 3.609856367111206
Validation loss: 2.9995653808757825

Epoch: 6| Step: 6
Training loss: 3.9067225456237793
Validation loss: 3.004192377931328

Epoch: 6| Step: 7
Training loss: 3.361994743347168
Validation loss: 3.0019015727504605

Epoch: 6| Step: 8
Training loss: 2.569377899169922
Validation loss: 2.9948798225771998

Epoch: 6| Step: 9
Training loss: 2.1383109092712402
Validation loss: 2.9906679045769478

Epoch: 6| Step: 10
Training loss: 2.91843843460083
Validation loss: 2.984789340726791

Epoch: 6| Step: 11
Training loss: 3.405360460281372
Validation loss: 2.986515057984219

Epoch: 6| Step: 12
Training loss: 3.6436538696289062
Validation loss: 2.978843440291702

Epoch: 6| Step: 13
Training loss: 2.245652675628662
Validation loss: 2.9838589955401678

Epoch: 21| Step: 0
Training loss: 2.7779746055603027
Validation loss: 2.978144138090072

Epoch: 6| Step: 1
Training loss: 3.00160551071167
Validation loss: 2.9793755213419595

Epoch: 6| Step: 2
Training loss: 3.3902523517608643
Validation loss: 2.978845550167945

Epoch: 6| Step: 3
Training loss: 2.5020570755004883
Validation loss: 2.9769553728001092

Epoch: 6| Step: 4
Training loss: 3.8076038360595703
Validation loss: 2.981359625375399

Epoch: 6| Step: 5
Training loss: 3.102123498916626
Validation loss: 2.9844614408349477

Epoch: 6| Step: 6
Training loss: 2.1553196907043457
Validation loss: 2.9761797510167605

Epoch: 6| Step: 7
Training loss: 2.4208359718322754
Validation loss: 2.979842319283434

Epoch: 6| Step: 8
Training loss: 3.1280837059020996
Validation loss: 2.9886708131400486

Epoch: 6| Step: 9
Training loss: 2.8125851154327393
Validation loss: 2.972157916715068

Epoch: 6| Step: 10
Training loss: 3.372612953186035
Validation loss: 2.9615832887670046

Epoch: 6| Step: 11
Training loss: 2.955688714981079
Validation loss: 2.963791557537612

Epoch: 6| Step: 12
Training loss: 3.438642740249634
Validation loss: 2.9615535454083513

Epoch: 6| Step: 13
Training loss: 4.251720428466797
Validation loss: 2.963273955929664

Epoch: 22| Step: 0
Training loss: 2.7703237533569336
Validation loss: 2.965845582305744

Epoch: 6| Step: 1
Training loss: 2.756624698638916
Validation loss: 2.9622669707062426

Epoch: 6| Step: 2
Training loss: 2.815247058868408
Validation loss: 2.961197935124879

Epoch: 6| Step: 3
Training loss: 2.850705146789551
Validation loss: 2.953958608770883

Epoch: 6| Step: 4
Training loss: 3.679136276245117
Validation loss: 2.954953319282942

Epoch: 6| Step: 5
Training loss: 2.934889078140259
Validation loss: 2.951235161032728

Epoch: 6| Step: 6
Training loss: 2.595407247543335
Validation loss: 2.9474875696243776

Epoch: 6| Step: 7
Training loss: 2.7952544689178467
Validation loss: 2.9469373072347333

Epoch: 6| Step: 8
Training loss: 3.023463726043701
Validation loss: 2.9494790082336753

Epoch: 6| Step: 9
Training loss: 3.9489552974700928
Validation loss: 2.950238258607926

Epoch: 6| Step: 10
Training loss: 3.3319103717803955
Validation loss: 2.947535435358683

Epoch: 6| Step: 11
Training loss: 3.0963268280029297
Validation loss: 2.943929123622115

Epoch: 6| Step: 12
Training loss: 2.7202651500701904
Validation loss: 2.9420098591876287

Epoch: 6| Step: 13
Training loss: 3.0326147079467773
Validation loss: 2.9404149927118772

Epoch: 23| Step: 0
Training loss: 3.4768927097320557
Validation loss: 2.9306299327522196

Epoch: 6| Step: 1
Training loss: 3.1470775604248047
Validation loss: 2.9362678066376717

Epoch: 6| Step: 2
Training loss: 2.8054561614990234
Validation loss: 2.932723142767465

Epoch: 6| Step: 3
Training loss: 3.2028377056121826
Validation loss: 2.932020959033761

Epoch: 6| Step: 4
Training loss: 3.1372995376586914
Validation loss: 2.93139624339278

Epoch: 6| Step: 5
Training loss: 3.2332961559295654
Validation loss: 2.937333901723226

Epoch: 6| Step: 6
Training loss: 2.624598503112793
Validation loss: 2.9260984492558304

Epoch: 6| Step: 7
Training loss: 2.411449432373047
Validation loss: 2.931953727558095

Epoch: 6| Step: 8
Training loss: 2.4396066665649414
Validation loss: 2.9224459740423385

Epoch: 6| Step: 9
Training loss: 2.8655145168304443
Validation loss: 2.924809666090114

Epoch: 6| Step: 10
Training loss: 4.429983139038086
Validation loss: 2.928037853651149

Epoch: 6| Step: 11
Training loss: 1.939194679260254
Validation loss: 2.932756736714353

Epoch: 6| Step: 12
Training loss: 3.478238582611084
Validation loss: 2.9265667623089207

Epoch: 6| Step: 13
Training loss: 2.97017240524292
Validation loss: 2.9166009631208194

Epoch: 24| Step: 0
Training loss: 2.4665403366088867
Validation loss: 2.9180501148264897

Epoch: 6| Step: 1
Training loss: 2.753631830215454
Validation loss: 2.9167599960040023

Epoch: 6| Step: 2
Training loss: 2.395031213760376
Validation loss: 2.92348317433429

Epoch: 6| Step: 3
Training loss: 3.64123272895813
Validation loss: 2.9426738395485827

Epoch: 6| Step: 4
Training loss: 4.16574764251709
Validation loss: 2.9305958876045803

Epoch: 6| Step: 5
Training loss: 2.901003837585449
Validation loss: 2.9073184638895015

Epoch: 6| Step: 6
Training loss: 3.138822078704834
Validation loss: 2.907286931109685

Epoch: 6| Step: 7
Training loss: 3.0717544555664062
Validation loss: 2.9050825770183275

Epoch: 6| Step: 8
Training loss: 2.353029727935791
Validation loss: 2.9025112300790767

Epoch: 6| Step: 9
Training loss: 3.147876739501953
Validation loss: 2.898203880556168

Epoch: 6| Step: 10
Training loss: 3.312319278717041
Validation loss: 2.896131625739477

Epoch: 6| Step: 11
Training loss: 2.4404120445251465
Validation loss: 2.901579608199417

Epoch: 6| Step: 12
Training loss: 3.4846913814544678
Validation loss: 2.903239837256811

Epoch: 6| Step: 13
Training loss: 2.4271953105926514
Validation loss: 2.899657100759527

Epoch: 25| Step: 0
Training loss: 2.260601043701172
Validation loss: 2.8953120221373854

Epoch: 6| Step: 1
Training loss: 3.1548972129821777
Validation loss: 2.8980215313614055

Epoch: 6| Step: 2
Training loss: 3.7340683937072754
Validation loss: 2.8975120590579126

Epoch: 6| Step: 3
Training loss: 2.703130006790161
Validation loss: 2.898329634820261

Epoch: 6| Step: 4
Training loss: 3.689417839050293
Validation loss: 2.8892942705462055

Epoch: 6| Step: 5
Training loss: 3.136641502380371
Validation loss: 2.8876481389486663

Epoch: 6| Step: 6
Training loss: 3.9037728309631348
Validation loss: 2.891343339796989

Epoch: 6| Step: 7
Training loss: 2.9587669372558594
Validation loss: 2.8975809722818355

Epoch: 6| Step: 8
Training loss: 2.5675230026245117
Validation loss: 2.89983614542151

Epoch: 6| Step: 9
Training loss: 2.65340256690979
Validation loss: 2.8949811484224055

Epoch: 6| Step: 10
Training loss: 3.1007699966430664
Validation loss: 2.8854119418769755

Epoch: 6| Step: 11
Training loss: 1.9821827411651611
Validation loss: 2.88736730493525

Epoch: 6| Step: 12
Training loss: 3.3777031898498535
Validation loss: 2.875437762147637

Epoch: 6| Step: 13
Training loss: 2.0305089950561523
Validation loss: 2.8778209404278825

Epoch: 26| Step: 0
Training loss: 3.1524605751037598
Validation loss: 2.8730160574759207

Epoch: 6| Step: 1
Training loss: 1.9656531810760498
Validation loss: 2.87286550511596

Epoch: 6| Step: 2
Training loss: 3.0659523010253906
Validation loss: 2.8760638134453886

Epoch: 6| Step: 3
Training loss: 3.3964924812316895
Validation loss: 2.8701256321322535

Epoch: 6| Step: 4
Training loss: 2.6754682064056396
Validation loss: 2.8706955473910094

Epoch: 6| Step: 5
Training loss: 3.2553811073303223
Validation loss: 2.8708669190765708

Epoch: 6| Step: 6
Training loss: 2.727832794189453
Validation loss: 2.869615354845601

Epoch: 6| Step: 7
Training loss: 2.755812406539917
Validation loss: 2.867153893234909

Epoch: 6| Step: 8
Training loss: 2.877281665802002
Validation loss: 2.858752122489355

Epoch: 6| Step: 9
Training loss: 3.768880844116211
Validation loss: 2.859660453693841

Epoch: 6| Step: 10
Training loss: 3.106566905975342
Validation loss: 2.8597710927327475

Epoch: 6| Step: 11
Training loss: 2.796273946762085
Validation loss: 2.854457378387451

Epoch: 6| Step: 12
Training loss: 2.6990299224853516
Validation loss: 2.8568273872457524

Epoch: 6| Step: 13
Training loss: 3.6480700969696045
Validation loss: 2.8567807546225925

Epoch: 27| Step: 0
Training loss: 3.3265042304992676
Validation loss: 2.853849959629838

Epoch: 6| Step: 1
Training loss: 3.281888484954834
Validation loss: 2.855174272291122

Epoch: 6| Step: 2
Training loss: 3.0357282161712646
Validation loss: 2.857543942748859

Epoch: 6| Step: 3
Training loss: 2.910526990890503
Validation loss: 2.8554669118696645

Epoch: 6| Step: 4
Training loss: 2.8374671936035156
Validation loss: 2.85502000265224

Epoch: 6| Step: 5
Training loss: 4.289173126220703
Validation loss: 2.8553899462505052

Epoch: 6| Step: 6
Training loss: 2.559950828552246
Validation loss: 2.8492777296291885

Epoch: 6| Step: 7
Training loss: 3.0253243446350098
Validation loss: 2.8483126804392827

Epoch: 6| Step: 8
Training loss: 3.8920860290527344
Validation loss: 2.84736737128227

Epoch: 6| Step: 9
Training loss: 2.6763288974761963
Validation loss: 2.8485784915185746

Epoch: 6| Step: 10
Training loss: 2.471534013748169
Validation loss: 2.8435699196271997

Epoch: 6| Step: 11
Training loss: 2.373196601867676
Validation loss: 2.841565814069522

Epoch: 6| Step: 12
Training loss: 2.0280399322509766
Validation loss: 2.839009408027895

Epoch: 6| Step: 13
Training loss: 2.4315340518951416
Validation loss: 2.833954195822439

Epoch: 28| Step: 0
Training loss: 3.186607837677002
Validation loss: 2.842700996706563

Epoch: 6| Step: 1
Training loss: 3.469809055328369
Validation loss: 2.840376797542777

Epoch: 6| Step: 2
Training loss: 2.885012626647949
Validation loss: 2.8434907210770475

Epoch: 6| Step: 3
Training loss: 2.4218759536743164
Validation loss: 2.8386359548056

Epoch: 6| Step: 4
Training loss: 2.6843862533569336
Validation loss: 2.8413205249335176

Epoch: 6| Step: 5
Training loss: 2.490612506866455
Validation loss: 2.835989629068682

Epoch: 6| Step: 6
Training loss: 3.2087385654449463
Validation loss: 2.837654480370142

Epoch: 6| Step: 7
Training loss: 3.600680351257324
Validation loss: 2.8418113877696376

Epoch: 6| Step: 8
Training loss: 3.0821633338928223
Validation loss: 2.8367520865573677

Epoch: 6| Step: 9
Training loss: 4.068376541137695
Validation loss: 2.8401440420458393

Epoch: 6| Step: 10
Training loss: 2.5375962257385254
Validation loss: 2.834252567701442

Epoch: 6| Step: 11
Training loss: 2.7669782638549805
Validation loss: 2.826805850510956

Epoch: 6| Step: 12
Training loss: 2.621802806854248
Validation loss: 2.8327356820465415

Epoch: 6| Step: 13
Training loss: 1.5404789447784424
Validation loss: 2.8357429401848906

Epoch: 29| Step: 0
Training loss: 2.618035078048706
Validation loss: 2.835013715169763

Epoch: 6| Step: 1
Training loss: 2.388538122177124
Validation loss: 2.8341578860436716

Epoch: 6| Step: 2
Training loss: 2.0790982246398926
Validation loss: 2.8314848561440744

Epoch: 6| Step: 3
Training loss: 4.077948093414307
Validation loss: 2.8311911116364183

Epoch: 6| Step: 4
Training loss: 2.2152187824249268
Validation loss: 2.8297885387174544

Epoch: 6| Step: 5
Training loss: 3.7766666412353516
Validation loss: 2.827446599160471

Epoch: 6| Step: 6
Training loss: 2.056865692138672
Validation loss: 2.825447174810594

Epoch: 6| Step: 7
Training loss: 3.023447036743164
Validation loss: 2.822635776253157

Epoch: 6| Step: 8
Training loss: 2.283053159713745
Validation loss: 2.827964634023687

Epoch: 6| Step: 9
Training loss: 3.531277894973755
Validation loss: 2.8228172871374313

Epoch: 6| Step: 10
Training loss: 2.9913699626922607
Validation loss: 2.8279028784844185

Epoch: 6| Step: 11
Training loss: 3.6557509899139404
Validation loss: 2.8304147489609255

Epoch: 6| Step: 12
Training loss: 3.25518798828125
Validation loss: 2.8331226584731892

Epoch: 6| Step: 13
Training loss: 3.267085552215576
Validation loss: 2.8381264696839037

Epoch: 30| Step: 0
Training loss: 3.0571608543395996
Validation loss: 2.8259028747517574

Epoch: 6| Step: 1
Training loss: 2.433021068572998
Validation loss: 2.8216851347236225

Epoch: 6| Step: 2
Training loss: 2.7644968032836914
Validation loss: 2.8190744077005694

Epoch: 6| Step: 3
Training loss: 3.137624740600586
Validation loss: 2.825321125727828

Epoch: 6| Step: 4
Training loss: 2.7188491821289062
Validation loss: 2.8221587852765153

Epoch: 6| Step: 5
Training loss: 3.439175605773926
Validation loss: 2.8228860516701975

Epoch: 6| Step: 6
Training loss: 2.4455604553222656
Validation loss: 2.82833517495022

Epoch: 6| Step: 7
Training loss: 2.8836145401000977
Validation loss: 2.8209170372255388

Epoch: 6| Step: 8
Training loss: 2.387894630432129
Validation loss: 2.8193375577208815

Epoch: 6| Step: 9
Training loss: 3.596897840499878
Validation loss: 2.8222592928076304

Epoch: 6| Step: 10
Training loss: 2.6647181510925293
Validation loss: 2.820996771576584

Epoch: 6| Step: 11
Training loss: 3.087223768234253
Validation loss: 2.8191164590979136

Epoch: 6| Step: 12
Training loss: 2.8218259811401367
Validation loss: 2.823536293480986

Epoch: 6| Step: 13
Training loss: 4.159835338592529
Validation loss: 2.8182741006215415

Epoch: 31| Step: 0
Training loss: 3.948668956756592
Validation loss: 2.815672648850308

Epoch: 6| Step: 1
Training loss: 2.3552393913269043
Validation loss: 2.820619034510787

Epoch: 6| Step: 2
Training loss: 2.884545087814331
Validation loss: 2.816980351683914

Epoch: 6| Step: 3
Training loss: 2.4358983039855957
Validation loss: 2.8184350587988414

Epoch: 6| Step: 4
Training loss: 3.2824668884277344
Validation loss: 2.8147667505407847

Epoch: 6| Step: 5
Training loss: 3.2271194458007812
Validation loss: 2.813701532220328

Epoch: 6| Step: 6
Training loss: 3.8280835151672363
Validation loss: 2.824058768569782

Epoch: 6| Step: 7
Training loss: 3.7382917404174805
Validation loss: 2.8125219140001523

Epoch: 6| Step: 8
Training loss: 2.829976797103882
Validation loss: 2.8142819302056425

Epoch: 6| Step: 9
Training loss: 2.314349889755249
Validation loss: 2.814279951075072

Epoch: 6| Step: 10
Training loss: 2.389810800552368
Validation loss: 2.80757321593582

Epoch: 6| Step: 11
Training loss: 1.9904839992523193
Validation loss: 2.8074424907725346

Epoch: 6| Step: 12
Training loss: 2.6445775032043457
Validation loss: 2.814480120135892

Epoch: 6| Step: 13
Training loss: 3.1640031337738037
Validation loss: 2.8092493139287478

Epoch: 32| Step: 0
Training loss: 2.2521114349365234
Validation loss: 2.81525836195997

Epoch: 6| Step: 1
Training loss: 3.0498788356781006
Validation loss: 2.8055518750221498

Epoch: 6| Step: 2
Training loss: 2.4279775619506836
Validation loss: 2.807980601505567

Epoch: 6| Step: 3
Training loss: 3.300632953643799
Validation loss: 2.8169106206586285

Epoch: 6| Step: 4
Training loss: 3.3878376483917236
Validation loss: 2.8257942481707503

Epoch: 6| Step: 5
Training loss: 2.112516164779663
Validation loss: 2.8229096935641382

Epoch: 6| Step: 6
Training loss: 2.6951112747192383
Validation loss: 2.812873669849929

Epoch: 6| Step: 7
Training loss: 3.687753200531006
Validation loss: 2.803463446196689

Epoch: 6| Step: 8
Training loss: 2.557668924331665
Validation loss: 2.810843870203982

Epoch: 6| Step: 9
Training loss: 3.0394084453582764
Validation loss: 2.816538901739223

Epoch: 6| Step: 10
Training loss: 2.7555766105651855
Validation loss: 2.812511310782484

Epoch: 6| Step: 11
Training loss: 3.080354690551758
Validation loss: 2.814414449917373

Epoch: 6| Step: 12
Training loss: 3.4935503005981445
Validation loss: 2.818072772795154

Epoch: 6| Step: 13
Training loss: 3.375417709350586
Validation loss: 2.815202646358039

Epoch: 33| Step: 0
Training loss: 1.7904369831085205
Validation loss: 2.814822781470514

Epoch: 6| Step: 1
Training loss: 2.886221408843994
Validation loss: 2.8081235988165743

Epoch: 6| Step: 2
Training loss: 3.359394073486328
Validation loss: 2.8124199990303285

Epoch: 6| Step: 3
Training loss: 3.1633858680725098
Validation loss: 2.80755849294765

Epoch: 6| Step: 4
Training loss: 3.3277993202209473
Validation loss: 2.821242101730839

Epoch: 6| Step: 5
Training loss: 2.5853044986724854
Validation loss: 2.840776556281633

Epoch: 6| Step: 6
Training loss: 3.7017455101013184
Validation loss: 2.8468618136580273

Epoch: 6| Step: 7
Training loss: 2.692896842956543
Validation loss: 2.830979178028722

Epoch: 6| Step: 8
Training loss: 2.98472261428833
Validation loss: 2.8166669158525366

Epoch: 6| Step: 9
Training loss: 2.7944109439849854
Validation loss: 2.8235794805711314

Epoch: 6| Step: 10
Training loss: 3.023510456085205
Validation loss: 2.81693067601932

Epoch: 6| Step: 11
Training loss: 2.2037062644958496
Validation loss: 2.8163845846729894

Epoch: 6| Step: 12
Training loss: 3.2978317737579346
Validation loss: 2.807930992495629

Epoch: 6| Step: 13
Training loss: 3.260343313217163
Validation loss: 2.8014008306687876

Epoch: 34| Step: 0
Training loss: 3.4131271839141846
Validation loss: 2.8031620825490644

Epoch: 6| Step: 1
Training loss: 3.188925266265869
Validation loss: 2.7998771129115934

Epoch: 6| Step: 2
Training loss: 2.3373327255249023
Validation loss: 2.8058845817401843

Epoch: 6| Step: 3
Training loss: 3.2915852069854736
Validation loss: 2.8040117679103727

Epoch: 6| Step: 4
Training loss: 2.817931652069092
Validation loss: 2.7970478893608175

Epoch: 6| Step: 5
Training loss: 2.402653455734253
Validation loss: 2.801918839895597

Epoch: 6| Step: 6
Training loss: 3.1602258682250977
Validation loss: 2.795140527909802

Epoch: 6| Step: 7
Training loss: 2.705007553100586
Validation loss: 2.797835096236198

Epoch: 6| Step: 8
Training loss: 3.151535987854004
Validation loss: 2.7973689058775544

Epoch: 6| Step: 9
Training loss: 2.576997756958008
Validation loss: 2.7959346053420857

Epoch: 6| Step: 10
Training loss: 3.8977227210998535
Validation loss: 2.7979056373719247

Epoch: 6| Step: 11
Training loss: 2.61271333694458
Validation loss: 2.7937865257263184

Epoch: 6| Step: 12
Training loss: 3.001707077026367
Validation loss: 2.7929349509618615

Epoch: 6| Step: 13
Training loss: 1.9669690132141113
Validation loss: 2.7951124022083897

Epoch: 35| Step: 0
Training loss: 2.572254180908203
Validation loss: 2.7980988640939035

Epoch: 6| Step: 1
Training loss: 2.303603172302246
Validation loss: 2.794424605625932

Epoch: 6| Step: 2
Training loss: 3.0211853981018066
Validation loss: 2.7980711306295087

Epoch: 6| Step: 3
Training loss: 3.8036155700683594
Validation loss: 2.8017443892776326

Epoch: 6| Step: 4
Training loss: 3.086998462677002
Validation loss: 2.801169782556513

Epoch: 6| Step: 5
Training loss: 2.603318452835083
Validation loss: 2.7966022029999764

Epoch: 6| Step: 6
Training loss: 3.167620897293091
Validation loss: 2.7944003228218324

Epoch: 6| Step: 7
Training loss: 3.1324830055236816
Validation loss: 2.7952806693251415

Epoch: 6| Step: 8
Training loss: 3.0444836616516113
Validation loss: 2.795318265115061

Epoch: 6| Step: 9
Training loss: 2.228090763092041
Validation loss: 2.7910201370075183

Epoch: 6| Step: 10
Training loss: 2.9032061100006104
Validation loss: 2.793092627679148

Epoch: 6| Step: 11
Training loss: 2.2520763874053955
Validation loss: 2.788183207152992

Epoch: 6| Step: 12
Training loss: 3.580061197280884
Validation loss: 2.7932556726599254

Epoch: 6| Step: 13
Training loss: 3.27420711517334
Validation loss: 2.7849403017310688

Epoch: 36| Step: 0
Training loss: 2.012794256210327
Validation loss: 2.79507807249664

Epoch: 6| Step: 1
Training loss: 3.2386908531188965
Validation loss: 2.797020355860392

Epoch: 6| Step: 2
Training loss: 3.5234596729278564
Validation loss: 2.7979851230498283

Epoch: 6| Step: 3
Training loss: 2.873873710632324
Validation loss: 2.80258697079074

Epoch: 6| Step: 4
Training loss: 3.1569290161132812
Validation loss: 2.7986214853102163

Epoch: 6| Step: 5
Training loss: 2.397303342819214
Validation loss: 2.812482446752569

Epoch: 6| Step: 6
Training loss: 4.1920552253723145
Validation loss: 2.8324156550950903

Epoch: 6| Step: 7
Training loss: 3.2028262615203857
Validation loss: 2.831878051962904

Epoch: 6| Step: 8
Training loss: 2.428083896636963
Validation loss: 2.8287352643987185

Epoch: 6| Step: 9
Training loss: 3.7760140895843506
Validation loss: 2.7880796219712947

Epoch: 6| Step: 10
Training loss: 2.7552318572998047
Validation loss: 2.7894789121484243

Epoch: 6| Step: 11
Training loss: 2.5123343467712402
Validation loss: 2.7923760388487127

Epoch: 6| Step: 12
Training loss: 2.1657395362854004
Validation loss: 2.8009657782893025

Epoch: 6| Step: 13
Training loss: 2.42708683013916
Validation loss: 2.8059751167092273

Epoch: 37| Step: 0
Training loss: 2.2863471508026123
Validation loss: 2.80135832037977

Epoch: 6| Step: 1
Training loss: 2.8517184257507324
Validation loss: 2.8030395635994534

Epoch: 6| Step: 2
Training loss: 3.2004427909851074
Validation loss: 2.8073229994825137

Epoch: 6| Step: 3
Training loss: 2.2408294677734375
Validation loss: 2.8044917147646666

Epoch: 6| Step: 4
Training loss: 3.327911615371704
Validation loss: 2.800227303658762

Epoch: 6| Step: 5
Training loss: 3.093593120574951
Validation loss: 2.7979993204916678

Epoch: 6| Step: 6
Training loss: 3.6764748096466064
Validation loss: 2.793164619835474

Epoch: 6| Step: 7
Training loss: 3.6411800384521484
Validation loss: 2.7882574912040465

Epoch: 6| Step: 8
Training loss: 2.8991923332214355
Validation loss: 2.7964592902891097

Epoch: 6| Step: 9
Training loss: 2.3680479526519775
Validation loss: 2.8021606373530563

Epoch: 6| Step: 10
Training loss: 3.248544692993164
Validation loss: 2.8120413057265745

Epoch: 6| Step: 11
Training loss: 2.337554693222046
Validation loss: 2.8018840102739233

Epoch: 6| Step: 12
Training loss: 3.2929909229278564
Validation loss: 2.8081982571591615

Epoch: 6| Step: 13
Training loss: 1.7955678701400757
Validation loss: 2.7988082157668246

Epoch: 38| Step: 0
Training loss: 1.764420986175537
Validation loss: 2.800356493201307

Epoch: 6| Step: 1
Training loss: 2.9850735664367676
Validation loss: 2.7918911287861485

Epoch: 6| Step: 2
Training loss: 2.2824106216430664
Validation loss: 2.7988642236237884

Epoch: 6| Step: 3
Training loss: 3.352464199066162
Validation loss: 2.80255885278025

Epoch: 6| Step: 4
Training loss: 3.201385974884033
Validation loss: 2.8003245246025825

Epoch: 6| Step: 5
Training loss: 2.9291398525238037
Validation loss: 2.799948343666651

Epoch: 6| Step: 6
Training loss: 2.826435089111328
Validation loss: 2.797081475616783

Epoch: 6| Step: 7
Training loss: 2.9719715118408203
Validation loss: 2.7952043420525006

Epoch: 6| Step: 8
Training loss: 2.815028667449951
Validation loss: 2.795265254154

Epoch: 6| Step: 9
Training loss: 3.2818336486816406
Validation loss: 2.7984281944972214

Epoch: 6| Step: 10
Training loss: 2.719273328781128
Validation loss: 2.7961936919919905

Epoch: 6| Step: 11
Training loss: 3.1340911388397217
Validation loss: 2.7914057188136603

Epoch: 6| Step: 12
Training loss: 3.1706838607788086
Validation loss: 2.7880165807662474

Epoch: 6| Step: 13
Training loss: 3.5101144313812256
Validation loss: 2.787165364911479

Epoch: 39| Step: 0
Training loss: 3.706742286682129
Validation loss: 2.786970579495994

Epoch: 6| Step: 1
Training loss: 3.095527172088623
Validation loss: 2.7862859182460333

Epoch: 6| Step: 2
Training loss: 2.126157760620117
Validation loss: 2.7790488607139996

Epoch: 6| Step: 3
Training loss: 2.763002872467041
Validation loss: 2.780136000725531

Epoch: 6| Step: 4
Training loss: 3.563504219055176
Validation loss: 2.7763192012745845

Epoch: 6| Step: 5
Training loss: 2.7330322265625
Validation loss: 2.782505371237314

Epoch: 6| Step: 6
Training loss: 2.6348583698272705
Validation loss: 2.7745288713004

Epoch: 6| Step: 7
Training loss: 2.729830265045166
Validation loss: 2.7786810269919773

Epoch: 6| Step: 8
Training loss: 3.5248074531555176
Validation loss: 2.7753220168493127

Epoch: 6| Step: 9
Training loss: 2.9173712730407715
Validation loss: 2.778644984768283

Epoch: 6| Step: 10
Training loss: 2.6067957878112793
Validation loss: 2.7787790119007068

Epoch: 6| Step: 11
Training loss: 2.6799910068511963
Validation loss: 2.7811991758244012

Epoch: 6| Step: 12
Training loss: 2.909019947052002
Validation loss: 2.781382288984073

Epoch: 6| Step: 13
Training loss: 2.3974435329437256
Validation loss: 2.7790553979976202

Epoch: 40| Step: 0
Training loss: 3.0015647411346436
Validation loss: 2.7761380903182493

Epoch: 6| Step: 1
Training loss: 3.1273622512817383
Validation loss: 2.7805238846809632

Epoch: 6| Step: 2
Training loss: 3.5662989616394043
Validation loss: 2.781769901193598

Epoch: 6| Step: 3
Training loss: 2.744534730911255
Validation loss: 2.7919541289729457

Epoch: 6| Step: 4
Training loss: 2.2466063499450684
Validation loss: 2.789816282128775

Epoch: 6| Step: 5
Training loss: 3.583472728729248
Validation loss: 2.8002501098058556

Epoch: 6| Step: 6
Training loss: 2.7616629600524902
Validation loss: 2.789183942220544

Epoch: 6| Step: 7
Training loss: 2.9172918796539307
Validation loss: 2.78710501937456

Epoch: 6| Step: 8
Training loss: 3.0545949935913086
Validation loss: 2.7827183713195143

Epoch: 6| Step: 9
Training loss: 2.797046661376953
Validation loss: 2.7810441370933288

Epoch: 6| Step: 10
Training loss: 2.5079500675201416
Validation loss: 2.78517642072452

Epoch: 6| Step: 11
Training loss: 2.703352451324463
Validation loss: 2.7758064244383123

Epoch: 6| Step: 12
Training loss: 2.2917442321777344
Validation loss: 2.777051341149115

Epoch: 6| Step: 13
Training loss: 3.4570329189300537
Validation loss: 2.778659177082841

Epoch: 41| Step: 0
Training loss: 2.807887077331543
Validation loss: 2.777527052869079

Epoch: 6| Step: 1
Training loss: 3.979931592941284
Validation loss: 2.7723800110560592

Epoch: 6| Step: 2
Training loss: 2.796876907348633
Validation loss: 2.7753839364615818

Epoch: 6| Step: 3
Training loss: 2.2230992317199707
Validation loss: 2.770098001726212

Epoch: 6| Step: 4
Training loss: 2.5692455768585205
Validation loss: 2.770072062810262

Epoch: 6| Step: 5
Training loss: 3.613635540008545
Validation loss: 2.7749561981488298

Epoch: 6| Step: 6
Training loss: 3.25970196723938
Validation loss: 2.7716444282121557

Epoch: 6| Step: 7
Training loss: 2.1170058250427246
Validation loss: 2.772755040917345

Epoch: 6| Step: 8
Training loss: 2.955576181411743
Validation loss: 2.7642305281854447

Epoch: 6| Step: 9
Training loss: 2.841298818588257
Validation loss: 2.7691334627007924

Epoch: 6| Step: 10
Training loss: 3.056980609893799
Validation loss: 2.7757504447813957

Epoch: 6| Step: 11
Training loss: 2.6300549507141113
Validation loss: 2.7737297524688063

Epoch: 6| Step: 12
Training loss: 2.6904616355895996
Validation loss: 2.7773422400156655

Epoch: 6| Step: 13
Training loss: 3.1013360023498535
Validation loss: 2.7726485601035495

Epoch: 42| Step: 0
Training loss: 2.9460458755493164
Validation loss: 2.772899273903139

Epoch: 6| Step: 1
Training loss: 3.4337515830993652
Validation loss: 2.772889539759646

Epoch: 6| Step: 2
Training loss: 2.817241907119751
Validation loss: 2.7703528070962555

Epoch: 6| Step: 3
Training loss: 2.6284658908843994
Validation loss: 2.7737244354781283

Epoch: 6| Step: 4
Training loss: 2.7231032848358154
Validation loss: 2.772532788656091

Epoch: 6| Step: 5
Training loss: 3.937631130218506
Validation loss: 2.772671861033286

Epoch: 6| Step: 6
Training loss: 2.6757984161376953
Validation loss: 2.7640518014149

Epoch: 6| Step: 7
Training loss: 2.4208526611328125
Validation loss: 2.7619149043995845

Epoch: 6| Step: 8
Training loss: 2.6868746280670166
Validation loss: 2.7650037375829553

Epoch: 6| Step: 9
Training loss: 2.579587936401367
Validation loss: 2.7705336437430432

Epoch: 6| Step: 10
Training loss: 2.954451322555542
Validation loss: 2.7672883977172194

Epoch: 6| Step: 11
Training loss: 2.817309617996216
Validation loss: 2.7730624957751204

Epoch: 6| Step: 12
Training loss: 2.50212025642395
Validation loss: 2.768100295015561

Epoch: 6| Step: 13
Training loss: 3.624476432800293
Validation loss: 2.768392393665929

Epoch: 43| Step: 0
Training loss: 2.977639675140381
Validation loss: 2.7672851060026433

Epoch: 6| Step: 1
Training loss: 2.1296091079711914
Validation loss: 2.768657986835767

Epoch: 6| Step: 2
Training loss: 3.8723692893981934
Validation loss: 2.7612670826655563

Epoch: 6| Step: 3
Training loss: 3.3076682090759277
Validation loss: 2.76374420555689

Epoch: 6| Step: 4
Training loss: 3.106210470199585
Validation loss: 2.7628565219140824

Epoch: 6| Step: 5
Training loss: 3.319972276687622
Validation loss: 2.765906515941825

Epoch: 6| Step: 6
Training loss: 2.671658515930176
Validation loss: 2.768270036225678

Epoch: 6| Step: 7
Training loss: 2.363044261932373
Validation loss: 2.7683282180499007

Epoch: 6| Step: 8
Training loss: 2.4958128929138184
Validation loss: 2.7686555975226947

Epoch: 6| Step: 9
Training loss: 3.977820873260498
Validation loss: 2.76641014570831

Epoch: 6| Step: 10
Training loss: 2.741471529006958
Validation loss: 2.7645472211222493

Epoch: 6| Step: 11
Training loss: 2.4642601013183594
Validation loss: 2.767398226645685

Epoch: 6| Step: 12
Training loss: 1.8926326036453247
Validation loss: 2.767678891458819

Epoch: 6| Step: 13
Training loss: 3.2711591720581055
Validation loss: 2.762712429928523

Epoch: 44| Step: 0
Training loss: 2.396134376525879
Validation loss: 2.7619848071887927

Epoch: 6| Step: 1
Training loss: 2.3309168815612793
Validation loss: 2.7665353898079164

Epoch: 6| Step: 2
Training loss: 3.703124761581421
Validation loss: 2.758975713483749

Epoch: 6| Step: 3
Training loss: 1.9794154167175293
Validation loss: 2.764251906384704

Epoch: 6| Step: 4
Training loss: 2.8202033042907715
Validation loss: 2.7587583449579056

Epoch: 6| Step: 5
Training loss: 2.559828281402588
Validation loss: 2.76643084069734

Epoch: 6| Step: 6
Training loss: 2.3086299896240234
Validation loss: 2.7648123054094214

Epoch: 6| Step: 7
Training loss: 2.441473960876465
Validation loss: 2.7643410005877094

Epoch: 6| Step: 8
Training loss: 2.673839569091797
Validation loss: 2.7602151568217943

Epoch: 6| Step: 9
Training loss: 3.804797649383545
Validation loss: 2.7626336774518414

Epoch: 6| Step: 10
Training loss: 2.8939051628112793
Validation loss: 2.7618017760656213

Epoch: 6| Step: 11
Training loss: 3.0359411239624023
Validation loss: 2.7640132545143046

Epoch: 6| Step: 12
Training loss: 4.449028491973877
Validation loss: 2.7665512792525755

Epoch: 6| Step: 13
Training loss: 2.959681987762451
Validation loss: 2.769280646436958

Epoch: 45| Step: 0
Training loss: 3.493488311767578
Validation loss: 2.7628564757685505

Epoch: 6| Step: 1
Training loss: 3.5066428184509277
Validation loss: 2.7570106367911063

Epoch: 6| Step: 2
Training loss: 2.635056495666504
Validation loss: 2.7501293613064672

Epoch: 6| Step: 3
Training loss: 3.2028892040252686
Validation loss: 2.7542201626685356

Epoch: 6| Step: 4
Training loss: 2.0504310131073
Validation loss: 2.7539851819315264

Epoch: 6| Step: 5
Training loss: 2.3980560302734375
Validation loss: 2.752211942467638

Epoch: 6| Step: 6
Training loss: 3.048635959625244
Validation loss: 2.7505740760475077

Epoch: 6| Step: 7
Training loss: 2.9760642051696777
Validation loss: 2.754090662925474

Epoch: 6| Step: 8
Training loss: 2.611337900161743
Validation loss: 2.757904165534563

Epoch: 6| Step: 9
Training loss: 2.7289066314697266
Validation loss: 2.752927662223898

Epoch: 6| Step: 10
Training loss: 3.193385124206543
Validation loss: 2.7629029186823035

Epoch: 6| Step: 11
Training loss: 2.4003429412841797
Validation loss: 2.762452733132147

Epoch: 6| Step: 12
Training loss: 3.245494842529297
Validation loss: 2.7606839056937926

Epoch: 6| Step: 13
Training loss: 2.872295379638672
Validation loss: 2.765533616465907

Epoch: 46| Step: 0
Training loss: 2.047597646713257
Validation loss: 2.771038216929282

Epoch: 6| Step: 1
Training loss: 2.8657002449035645
Validation loss: 2.7677517398711173

Epoch: 6| Step: 2
Training loss: 3.4255642890930176
Validation loss: 2.7723060243873188

Epoch: 6| Step: 3
Training loss: 2.554868221282959
Validation loss: 2.7633375224246772

Epoch: 6| Step: 4
Training loss: 2.2468743324279785
Validation loss: 2.7568971392928914

Epoch: 6| Step: 5
Training loss: 3.349564552307129
Validation loss: 2.7475015348003757

Epoch: 6| Step: 6
Training loss: 2.6919708251953125
Validation loss: 2.745609606465986

Epoch: 6| Step: 7
Training loss: 2.535606622695923
Validation loss: 2.7443349669056554

Epoch: 6| Step: 8
Training loss: 2.827594757080078
Validation loss: 2.7436713275089057

Epoch: 6| Step: 9
Training loss: 3.8011512756347656
Validation loss: 2.7457061352268344

Epoch: 6| Step: 10
Training loss: 3.1462557315826416
Validation loss: 2.743455412567303

Epoch: 6| Step: 11
Training loss: 2.602091073989868
Validation loss: 2.747428704333562

Epoch: 6| Step: 12
Training loss: 2.9559943675994873
Validation loss: 2.7475322215787825

Epoch: 6| Step: 13
Training loss: 3.6033990383148193
Validation loss: 2.7450536092122397

Epoch: 47| Step: 0
Training loss: 2.387460708618164
Validation loss: 2.7361794646068285

Epoch: 6| Step: 1
Training loss: 2.0369789600372314
Validation loss: 2.7355879532393588

Epoch: 6| Step: 2
Training loss: 2.437526226043701
Validation loss: 2.7344315308396534

Epoch: 6| Step: 3
Training loss: 2.615208625793457
Validation loss: 2.7290864324056976

Epoch: 6| Step: 4
Training loss: 4.029058456420898
Validation loss: 2.7312069708301174

Epoch: 6| Step: 5
Training loss: 2.8837101459503174
Validation loss: 2.7271310924201884

Epoch: 6| Step: 6
Training loss: 3.318593978881836
Validation loss: 2.730292715052123

Epoch: 6| Step: 7
Training loss: 3.3981080055236816
Validation loss: 2.7252339445134646

Epoch: 6| Step: 8
Training loss: 1.9515557289123535
Validation loss: 2.72966924149503

Epoch: 6| Step: 9
Training loss: 2.1792335510253906
Validation loss: 2.7253004145878617

Epoch: 6| Step: 10
Training loss: 3.2438950538635254
Validation loss: 2.723879668020433

Epoch: 6| Step: 11
Training loss: 3.825576066970825
Validation loss: 2.7279253441800355

Epoch: 6| Step: 12
Training loss: 2.7478456497192383
Validation loss: 2.732528309668264

Epoch: 6| Step: 13
Training loss: 3.1925506591796875
Validation loss: 2.7332597573598227

Epoch: 48| Step: 0
Training loss: 2.861172676086426
Validation loss: 2.7321002432095107

Epoch: 6| Step: 1
Training loss: 2.6467442512512207
Validation loss: 2.734659525655931

Epoch: 6| Step: 2
Training loss: 2.6867523193359375
Validation loss: 2.726884444554647

Epoch: 6| Step: 3
Training loss: 3.0157310962677
Validation loss: 2.728301743025421

Epoch: 6| Step: 4
Training loss: 2.104588508605957
Validation loss: 2.7212476550891833

Epoch: 6| Step: 5
Training loss: 3.0172767639160156
Validation loss: 2.722500624195222

Epoch: 6| Step: 6
Training loss: 2.8199462890625
Validation loss: 2.7241321532957015

Epoch: 6| Step: 7
Training loss: 3.197099447250366
Validation loss: 2.7227955479775705

Epoch: 6| Step: 8
Training loss: 2.694660186767578
Validation loss: 2.7253545791872087

Epoch: 6| Step: 9
Training loss: 3.8452391624450684
Validation loss: 2.7202255905315442

Epoch: 6| Step: 10
Training loss: 2.8720345497131348
Validation loss: 2.720139072787377

Epoch: 6| Step: 11
Training loss: 3.593440055847168
Validation loss: 2.723463258435649

Epoch: 6| Step: 12
Training loss: 1.4681470394134521
Validation loss: 2.7258795743347495

Epoch: 6| Step: 13
Training loss: 3.474047899246216
Validation loss: 2.723289989656018

Epoch: 49| Step: 0
Training loss: 2.698011636734009
Validation loss: 2.717213261512018

Epoch: 6| Step: 1
Training loss: 2.053755283355713
Validation loss: 2.722575726047639

Epoch: 6| Step: 2
Training loss: 3.0628440380096436
Validation loss: 2.7235576721929733

Epoch: 6| Step: 3
Training loss: 2.258984327316284
Validation loss: 2.722937512141402

Epoch: 6| Step: 4
Training loss: 2.681547164916992
Validation loss: 2.7264171185032016

Epoch: 6| Step: 5
Training loss: 3.452061176300049
Validation loss: 2.7337059795215564

Epoch: 6| Step: 6
Training loss: 2.727969169616699
Validation loss: 2.729794689404067

Epoch: 6| Step: 7
Training loss: 3.042905330657959
Validation loss: 2.7478559606818744

Epoch: 6| Step: 8
Training loss: 3.022404432296753
Validation loss: 2.734034433159777

Epoch: 6| Step: 9
Training loss: 3.3185408115386963
Validation loss: 2.7382039562348397

Epoch: 6| Step: 10
Training loss: 2.8921267986297607
Validation loss: 2.738627044103479

Epoch: 6| Step: 11
Training loss: 3.7025132179260254
Validation loss: 2.736680333332349

Epoch: 6| Step: 12
Training loss: 2.4292845726013184
Validation loss: 2.732317198989212

Epoch: 6| Step: 13
Training loss: 2.5485048294067383
Validation loss: 2.7336385070636706

Epoch: 50| Step: 0
Training loss: 3.0476183891296387
Validation loss: 2.723899510598952

Epoch: 6| Step: 1
Training loss: 2.868623733520508
Validation loss: 2.7242502858561854

Epoch: 6| Step: 2
Training loss: 3.0107898712158203
Validation loss: 2.7276445947667605

Epoch: 6| Step: 3
Training loss: 2.9805846214294434
Validation loss: 2.7319197552178496

Epoch: 6| Step: 4
Training loss: 1.8354971408843994
Validation loss: 2.743730932153681

Epoch: 6| Step: 5
Training loss: 2.4000039100646973
Validation loss: 2.743692203234601

Epoch: 6| Step: 6
Training loss: 3.099543333053589
Validation loss: 2.7323692690941597

Epoch: 6| Step: 7
Training loss: 2.429873466491699
Validation loss: 2.7404174394505

Epoch: 6| Step: 8
Training loss: 2.4510440826416016
Validation loss: 2.7393778242090696

Epoch: 6| Step: 9
Training loss: 3.4442501068115234
Validation loss: 2.7340242016700005

Epoch: 6| Step: 10
Training loss: 3.0784356594085693
Validation loss: 2.7346249549619612

Epoch: 6| Step: 11
Training loss: 3.544851779937744
Validation loss: 2.725619641683435

Epoch: 6| Step: 12
Training loss: 3.016052722930908
Validation loss: 2.7198138454908967

Epoch: 6| Step: 13
Training loss: 2.92543888092041
Validation loss: 2.7187264555244037

Epoch: 51| Step: 0
Training loss: 2.7625882625579834
Validation loss: 2.7152757542107695

Epoch: 6| Step: 1
Training loss: 3.5889086723327637
Validation loss: 2.7140750321008826

Epoch: 6| Step: 2
Training loss: 3.15663480758667
Validation loss: 2.7141455424729215

Epoch: 6| Step: 3
Training loss: 2.7365188598632812
Validation loss: 2.721600409476988

Epoch: 6| Step: 4
Training loss: 3.721649646759033
Validation loss: 2.7168593791223343

Epoch: 6| Step: 5
Training loss: 2.7226245403289795
Validation loss: 2.7214256255857405

Epoch: 6| Step: 6
Training loss: 3.2670974731445312
Validation loss: 2.7148813739899667

Epoch: 6| Step: 7
Training loss: 2.4921624660491943
Validation loss: 2.717416404395975

Epoch: 6| Step: 8
Training loss: 2.412123680114746
Validation loss: 2.716108904089979

Epoch: 6| Step: 9
Training loss: 2.558284044265747
Validation loss: 2.7152186004064416

Epoch: 6| Step: 10
Training loss: 2.26800274848938
Validation loss: 2.7189201949745097

Epoch: 6| Step: 11
Training loss: 3.2486765384674072
Validation loss: 2.71606646942836

Epoch: 6| Step: 12
Training loss: 2.849146604537964
Validation loss: 2.7154036209147465

Epoch: 6| Step: 13
Training loss: 1.5515968799591064
Validation loss: 2.7182675048869145

Epoch: 52| Step: 0
Training loss: 2.9332449436187744
Validation loss: 2.719234779316892

Epoch: 6| Step: 1
Training loss: 3.114649772644043
Validation loss: 2.7127554134656022

Epoch: 6| Step: 2
Training loss: 2.9613664150238037
Validation loss: 2.70910128214026

Epoch: 6| Step: 3
Training loss: 2.331590175628662
Validation loss: 2.7135840103190434

Epoch: 6| Step: 4
Training loss: 2.027564525604248
Validation loss: 2.712987769034601

Epoch: 6| Step: 5
Training loss: 2.7681939601898193
Validation loss: 2.7157244579766386

Epoch: 6| Step: 6
Training loss: 2.7243924140930176
Validation loss: 2.715559559483682

Epoch: 6| Step: 7
Training loss: 2.8182740211486816
Validation loss: 2.714700504015851

Epoch: 6| Step: 8
Training loss: 3.3317439556121826
Validation loss: 2.7111398840463288

Epoch: 6| Step: 9
Training loss: 2.970414876937866
Validation loss: 2.7111864987240044

Epoch: 6| Step: 10
Training loss: 2.5654656887054443
Validation loss: 2.70850371801725

Epoch: 6| Step: 11
Training loss: 3.05952787399292
Validation loss: 2.707334282577679

Epoch: 6| Step: 12
Training loss: 2.942786693572998
Validation loss: 2.7119911845012377

Epoch: 6| Step: 13
Training loss: 3.8491575717926025
Validation loss: 2.7168779014259257

Epoch: 53| Step: 0
Training loss: 3.499617338180542
Validation loss: 2.7141397050631944

Epoch: 6| Step: 1
Training loss: 2.8413186073303223
Validation loss: 2.7189550092143397

Epoch: 6| Step: 2
Training loss: 2.4934916496276855
Validation loss: 2.720828776718468

Epoch: 6| Step: 3
Training loss: 2.9143362045288086
Validation loss: 2.7228986652948524

Epoch: 6| Step: 4
Training loss: 2.122196912765503
Validation loss: 2.7230696729434434

Epoch: 6| Step: 5
Training loss: 3.29154109954834
Validation loss: 2.7306493251554427

Epoch: 6| Step: 6
Training loss: 2.149043321609497
Validation loss: 2.723023722248693

Epoch: 6| Step: 7
Training loss: 3.0618715286254883
Validation loss: 2.718712288846252

Epoch: 6| Step: 8
Training loss: 3.8121533393859863
Validation loss: 2.7213680205806607

Epoch: 6| Step: 9
Training loss: 2.310286521911621
Validation loss: 2.7124235758217434

Epoch: 6| Step: 10
Training loss: 2.740774154663086
Validation loss: 2.7041222920981784

Epoch: 6| Step: 11
Training loss: 2.6436257362365723
Validation loss: 2.7122325820307576

Epoch: 6| Step: 12
Training loss: 3.2043240070343018
Validation loss: 2.714882796810519

Epoch: 6| Step: 13
Training loss: 2.832892417907715
Validation loss: 2.714857191167852

Epoch: 54| Step: 0
Training loss: 2.3844492435455322
Validation loss: 2.713529145845803

Epoch: 6| Step: 1
Training loss: 2.8167083263397217
Validation loss: 2.717183930899507

Epoch: 6| Step: 2
Training loss: 2.4553608894348145
Validation loss: 2.7095785422991683

Epoch: 6| Step: 3
Training loss: 2.50748872756958
Validation loss: 2.7086386680603027

Epoch: 6| Step: 4
Training loss: 3.1072769165039062
Validation loss: 2.7044424574862242

Epoch: 6| Step: 5
Training loss: 3.208348274230957
Validation loss: 2.707054115110828

Epoch: 6| Step: 6
Training loss: 3.934567451477051
Validation loss: 2.703654240536433

Epoch: 6| Step: 7
Training loss: 3.151639699935913
Validation loss: 2.7057677520218717

Epoch: 6| Step: 8
Training loss: 2.9318809509277344
Validation loss: 2.7011479459783083

Epoch: 6| Step: 9
Training loss: 2.323253631591797
Validation loss: 2.7026247811573807

Epoch: 6| Step: 10
Training loss: 2.946909189224243
Validation loss: 2.7031562815430346

Epoch: 6| Step: 11
Training loss: 2.6493005752563477
Validation loss: 2.7028116692778883

Epoch: 6| Step: 12
Training loss: 2.9105701446533203
Validation loss: 2.710971757929812

Epoch: 6| Step: 13
Training loss: 2.3904638290405273
Validation loss: 2.715311288833618

Epoch: 55| Step: 0
Training loss: 2.1080117225646973
Validation loss: 2.7153316543948267

Epoch: 6| Step: 1
Training loss: 2.8712034225463867
Validation loss: 2.7126682137930267

Epoch: 6| Step: 2
Training loss: 3.6441969871520996
Validation loss: 2.717852997523482

Epoch: 6| Step: 3
Training loss: 2.8255505561828613
Validation loss: 2.7221870499272502

Epoch: 6| Step: 4
Training loss: 3.254094123840332
Validation loss: 2.7251504723743727

Epoch: 6| Step: 5
Training loss: 2.584730625152588
Validation loss: 2.7298635129005677

Epoch: 6| Step: 6
Training loss: 1.948573112487793
Validation loss: 2.721494182463615

Epoch: 6| Step: 7
Training loss: 2.198479652404785
Validation loss: 2.723455993078088

Epoch: 6| Step: 8
Training loss: 4.3236799240112305
Validation loss: 2.7253459038272982

Epoch: 6| Step: 9
Training loss: 3.1970226764678955
Validation loss: 2.7158062637493177

Epoch: 6| Step: 10
Training loss: 2.5142741203308105
Validation loss: 2.707871513981973

Epoch: 6| Step: 11
Training loss: 3.37986421585083
Validation loss: 2.708620250865977

Epoch: 6| Step: 12
Training loss: 2.0304837226867676
Validation loss: 2.7040194234540387

Epoch: 6| Step: 13
Training loss: 2.858804225921631
Validation loss: 2.6997093231447282

Epoch: 56| Step: 0
Training loss: 2.060603380203247
Validation loss: 2.6987650394439697

Epoch: 6| Step: 1
Training loss: 2.7702016830444336
Validation loss: 2.698868848944223

Epoch: 6| Step: 2
Training loss: 3.6931679248809814
Validation loss: 2.6983061887884654

Epoch: 6| Step: 3
Training loss: 2.819862127304077
Validation loss: 2.699802452518094

Epoch: 6| Step: 4
Training loss: 2.5370419025421143
Validation loss: 2.696894968709638

Epoch: 6| Step: 5
Training loss: 2.6800122261047363
Validation loss: 2.6916144278741654

Epoch: 6| Step: 6
Training loss: 3.4137675762176514
Validation loss: 2.694587322973436

Epoch: 6| Step: 7
Training loss: 2.7854437828063965
Validation loss: 2.6966930461186234

Epoch: 6| Step: 8
Training loss: 3.7022128105163574
Validation loss: 2.6960444963106545

Epoch: 6| Step: 9
Training loss: 2.7532143592834473
Validation loss: 2.6967031391718055

Epoch: 6| Step: 10
Training loss: 2.670747995376587
Validation loss: 2.699796517690023

Epoch: 6| Step: 11
Training loss: 2.5031163692474365
Validation loss: 2.6996219158172607

Epoch: 6| Step: 12
Training loss: 2.9916231632232666
Validation loss: 2.6967118401681223

Epoch: 6| Step: 13
Training loss: 2.0432026386260986
Validation loss: 2.692138579583937

Epoch: 57| Step: 0
Training loss: 3.3311960697174072
Validation loss: 2.6922811820942867

Epoch: 6| Step: 1
Training loss: 3.0501632690429688
Validation loss: 2.69209135219615

Epoch: 6| Step: 2
Training loss: 1.9406795501708984
Validation loss: 2.6894150908275316

Epoch: 6| Step: 3
Training loss: 2.322481155395508
Validation loss: 2.686835863256967

Epoch: 6| Step: 4
Training loss: 2.177654504776001
Validation loss: 2.6918284790490263

Epoch: 6| Step: 5
Training loss: 3.3478660583496094
Validation loss: 2.6865439363705215

Epoch: 6| Step: 6
Training loss: 2.5584378242492676
Validation loss: 2.6946461739078647

Epoch: 6| Step: 7
Training loss: 2.9249227046966553
Validation loss: 2.692447454698624

Epoch: 6| Step: 8
Training loss: 3.085867166519165
Validation loss: 2.6890591703435427

Epoch: 6| Step: 9
Training loss: 3.4470131397247314
Validation loss: 2.6976555573043

Epoch: 6| Step: 10
Training loss: 3.2301082611083984
Validation loss: 2.6995300836460565

Epoch: 6| Step: 11
Training loss: 3.013582229614258
Validation loss: 2.6904170256789013

Epoch: 6| Step: 12
Training loss: 2.4898478984832764
Validation loss: 2.6947294640284714

Epoch: 6| Step: 13
Training loss: 2.6883633136749268
Validation loss: 2.6978858978517595

Epoch: 58| Step: 0
Training loss: 3.7820682525634766
Validation loss: 2.6910402428719307

Epoch: 6| Step: 1
Training loss: 2.131986618041992
Validation loss: 2.6892435063597975

Epoch: 6| Step: 2
Training loss: 3.0357439517974854
Validation loss: 2.693226804015457

Epoch: 6| Step: 3
Training loss: 3.549271583557129
Validation loss: 2.6857491923916723

Epoch: 6| Step: 4
Training loss: 1.8416402339935303
Validation loss: 2.6886343212537867

Epoch: 6| Step: 5
Training loss: 2.870256185531616
Validation loss: 2.6909725742955364

Epoch: 6| Step: 6
Training loss: 2.84336519241333
Validation loss: 2.6872461072860228

Epoch: 6| Step: 7
Training loss: 2.6221189498901367
Validation loss: 2.690552467940956

Epoch: 6| Step: 8
Training loss: 3.309770107269287
Validation loss: 2.6937279803778535

Epoch: 6| Step: 9
Training loss: 2.068469762802124
Validation loss: 2.6893455802753405

Epoch: 6| Step: 10
Training loss: 3.118192195892334
Validation loss: 2.690297549770724

Epoch: 6| Step: 11
Training loss: 2.8087315559387207
Validation loss: 2.6867706903847317

Epoch: 6| Step: 12
Training loss: 3.145157814025879
Validation loss: 2.686893058079545

Epoch: 6| Step: 13
Training loss: 2.3196628093719482
Validation loss: 2.6885571979707286

Epoch: 59| Step: 0
Training loss: 2.2381391525268555
Validation loss: 2.6876004254946144

Epoch: 6| Step: 1
Training loss: 2.8241701126098633
Validation loss: 2.6906448384766937

Epoch: 6| Step: 2
Training loss: 3.1617798805236816
Validation loss: 2.6955772164047405

Epoch: 6| Step: 3
Training loss: 3.184246063232422
Validation loss: 2.6861238505250666

Epoch: 6| Step: 4
Training loss: 2.863217353820801
Validation loss: 2.684744768245246

Epoch: 6| Step: 5
Training loss: 2.306972026824951
Validation loss: 2.688437241379933

Epoch: 6| Step: 6
Training loss: 2.948815107345581
Validation loss: 2.688359006758659

Epoch: 6| Step: 7
Training loss: 1.802073359489441
Validation loss: 2.6908539854070193

Epoch: 6| Step: 8
Training loss: 3.2552719116210938
Validation loss: 2.689012437738398

Epoch: 6| Step: 9
Training loss: 2.9123992919921875
Validation loss: 2.690179640246976

Epoch: 6| Step: 10
Training loss: 3.2927160263061523
Validation loss: 2.6928310932651645

Epoch: 6| Step: 11
Training loss: 2.9138505458831787
Validation loss: 2.686531674477362

Epoch: 6| Step: 12
Training loss: 2.7084832191467285
Validation loss: 2.6868427338138705

Epoch: 6| Step: 13
Training loss: 3.3753552436828613
Validation loss: 2.6919221032050347

Epoch: 60| Step: 0
Training loss: 3.0522289276123047
Validation loss: 2.692254509977115

Epoch: 6| Step: 1
Training loss: 2.4597620964050293
Validation loss: 2.692415411754321

Epoch: 6| Step: 2
Training loss: 2.594982385635376
Validation loss: 2.6925329136592087

Epoch: 6| Step: 3
Training loss: 3.0078048706054688
Validation loss: 2.6987512060391006

Epoch: 6| Step: 4
Training loss: 3.1109776496887207
Validation loss: 2.6968680351011214

Epoch: 6| Step: 5
Training loss: 2.6185977458953857
Validation loss: 2.694740690210814

Epoch: 6| Step: 6
Training loss: 3.2948098182678223
Validation loss: 2.6967995858961538

Epoch: 6| Step: 7
Training loss: 3.064113140106201
Validation loss: 2.7011950567204464

Epoch: 6| Step: 8
Training loss: 2.6244750022888184
Validation loss: 2.6905665372007634

Epoch: 6| Step: 9
Training loss: 2.248716115951538
Validation loss: 2.6988516853701685

Epoch: 6| Step: 10
Training loss: 2.4418296813964844
Validation loss: 2.6993992149188952

Epoch: 6| Step: 11
Training loss: 2.8858532905578613
Validation loss: 2.6925548866230953

Epoch: 6| Step: 12
Training loss: 3.410128593444824
Validation loss: 2.6905683727674585

Epoch: 6| Step: 13
Training loss: 2.790184259414673
Validation loss: 2.6844779676006687

Epoch: 61| Step: 0
Training loss: 3.3292734622955322
Validation loss: 2.6822761258771344

Epoch: 6| Step: 1
Training loss: 3.4429426193237305
Validation loss: 2.6835097394963747

Epoch: 6| Step: 2
Training loss: 3.23994779586792
Validation loss: 2.678874688763772

Epoch: 6| Step: 3
Training loss: 1.9813207387924194
Validation loss: 2.6811684946860037

Epoch: 6| Step: 4
Training loss: 2.9288148880004883
Validation loss: 2.682213739682269

Epoch: 6| Step: 5
Training loss: 2.2027087211608887
Validation loss: 2.6844070649916127

Epoch: 6| Step: 6
Training loss: 2.3138675689697266
Validation loss: 2.6832149387687765

Epoch: 6| Step: 7
Training loss: 3.244816780090332
Validation loss: 2.682675664142896

Epoch: 6| Step: 8
Training loss: 3.6404573917388916
Validation loss: 2.684582248810799

Epoch: 6| Step: 9
Training loss: 2.1341869831085205
Validation loss: 2.681224428197389

Epoch: 6| Step: 10
Training loss: 2.467947483062744
Validation loss: 2.6839479323356383

Epoch: 6| Step: 11
Training loss: 3.495541572570801
Validation loss: 2.6801118773798787

Epoch: 6| Step: 12
Training loss: 2.158565044403076
Validation loss: 2.682513395945231

Epoch: 6| Step: 13
Training loss: 2.934656858444214
Validation loss: 2.6855288167153635

Epoch: 62| Step: 0
Training loss: 2.736548662185669
Validation loss: 2.690196285965622

Epoch: 6| Step: 1
Training loss: 2.5420680046081543
Validation loss: 2.696663648851456

Epoch: 6| Step: 2
Training loss: 3.123460292816162
Validation loss: 2.6955332038223103

Epoch: 6| Step: 3
Training loss: 2.7533020973205566
Validation loss: 2.6939966883710635

Epoch: 6| Step: 4
Training loss: 2.778393268585205
Validation loss: 2.6878431125353743

Epoch: 6| Step: 5
Training loss: 3.539523124694824
Validation loss: 2.690558828333373

Epoch: 6| Step: 6
Training loss: 2.8510615825653076
Validation loss: 2.699358840142527

Epoch: 6| Step: 7
Training loss: 2.979763984680176
Validation loss: 2.6986719485252135

Epoch: 6| Step: 8
Training loss: 2.0678608417510986
Validation loss: 2.6931008754238004

Epoch: 6| Step: 9
Training loss: 2.5441784858703613
Validation loss: 2.68766204772457

Epoch: 6| Step: 10
Training loss: 2.9111263751983643
Validation loss: 2.6832879666359193

Epoch: 6| Step: 11
Training loss: 2.551286220550537
Validation loss: 2.675421307163854

Epoch: 6| Step: 12
Training loss: 3.603827714920044
Validation loss: 2.676291360650011

Epoch: 6| Step: 13
Training loss: 2.3914804458618164
Validation loss: 2.677825112496653

Epoch: 63| Step: 0
Training loss: 1.9045344591140747
Validation loss: 2.676484288707856

Epoch: 6| Step: 1
Training loss: 3.4911117553710938
Validation loss: 2.677455840572234

Epoch: 6| Step: 2
Training loss: 2.8503284454345703
Validation loss: 2.677679156744352

Epoch: 6| Step: 3
Training loss: 3.0880954265594482
Validation loss: 2.6783967889765257

Epoch: 6| Step: 4
Training loss: 3.0409038066864014
Validation loss: 2.678410589054067

Epoch: 6| Step: 5
Training loss: 2.8936662673950195
Validation loss: 2.677829401467436

Epoch: 6| Step: 6
Training loss: 2.8106770515441895
Validation loss: 2.6822715523422405

Epoch: 6| Step: 7
Training loss: 2.9924721717834473
Validation loss: 2.6830361581617788

Epoch: 6| Step: 8
Training loss: 2.527967929840088
Validation loss: 2.683011890739523

Epoch: 6| Step: 9
Training loss: 2.0186681747436523
Validation loss: 2.67656006351594

Epoch: 6| Step: 10
Training loss: 3.0410280227661133
Validation loss: 2.6807253976022043

Epoch: 6| Step: 11
Training loss: 2.712338447570801
Validation loss: 2.6836958751883557

Epoch: 6| Step: 12
Training loss: 2.9637722969055176
Validation loss: 2.68340975751159

Epoch: 6| Step: 13
Training loss: 3.3021087646484375
Validation loss: 2.6875347680942987

Epoch: 64| Step: 0
Training loss: 3.385868549346924
Validation loss: 2.681896722444924

Epoch: 6| Step: 1
Training loss: 2.6557984352111816
Validation loss: 2.6859252119577057

Epoch: 6| Step: 2
Training loss: 2.488554000854492
Validation loss: 2.6845342266944145

Epoch: 6| Step: 3
Training loss: 3.6045498847961426
Validation loss: 2.6835126594830583

Epoch: 6| Step: 4
Training loss: 2.786275863647461
Validation loss: 2.683189836881494

Epoch: 6| Step: 5
Training loss: 3.6250197887420654
Validation loss: 2.6815287143953386

Epoch: 6| Step: 6
Training loss: 2.222115993499756
Validation loss: 2.6791559060414634

Epoch: 6| Step: 7
Training loss: 3.8100826740264893
Validation loss: 2.676598056670158

Epoch: 6| Step: 8
Training loss: 2.3310422897338867
Validation loss: 2.675842454356532

Epoch: 6| Step: 9
Training loss: 1.7097136974334717
Validation loss: 2.6788365046183267

Epoch: 6| Step: 10
Training loss: 2.55424165725708
Validation loss: 2.685494745931318

Epoch: 6| Step: 11
Training loss: 2.8863253593444824
Validation loss: 2.688897643038022

Epoch: 6| Step: 12
Training loss: 2.360485076904297
Validation loss: 2.691953464220929

Epoch: 6| Step: 13
Training loss: 3.1111860275268555
Validation loss: 2.698858314944852

Epoch: 65| Step: 0
Training loss: 2.597264528274536
Validation loss: 2.6932166032893683

Epoch: 6| Step: 1
Training loss: 3.404120683670044
Validation loss: 2.6992824923607612

Epoch: 6| Step: 2
Training loss: 2.4970626831054688
Validation loss: 2.689547769484981

Epoch: 6| Step: 3
Training loss: 2.6500940322875977
Validation loss: 2.6836413568066013

Epoch: 6| Step: 4
Training loss: 2.7434659004211426
Validation loss: 2.6850492826072117

Epoch: 6| Step: 5
Training loss: 2.3916492462158203
Validation loss: 2.679495180806806

Epoch: 6| Step: 6
Training loss: 3.809415817260742
Validation loss: 2.680826590907189

Epoch: 6| Step: 7
Training loss: 2.353506326675415
Validation loss: 2.6866779045392106

Epoch: 6| Step: 8
Training loss: 3.2859795093536377
Validation loss: 2.6803579099716677

Epoch: 6| Step: 9
Training loss: 2.2885828018188477
Validation loss: 2.679500097869545

Epoch: 6| Step: 10
Training loss: 2.5292816162109375
Validation loss: 2.6875565923670286

Epoch: 6| Step: 11
Training loss: 2.7705821990966797
Validation loss: 2.679418038296443

Epoch: 6| Step: 12
Training loss: 2.9851419925689697
Validation loss: 2.683723108742827

Epoch: 6| Step: 13
Training loss: 3.207373857498169
Validation loss: 2.684557055914274

Epoch: 66| Step: 0
Training loss: 1.9816679954528809
Validation loss: 2.68255203770053

Epoch: 6| Step: 1
Training loss: 2.7467563152313232
Validation loss: 2.689224586691908

Epoch: 6| Step: 2
Training loss: 3.8068950176239014
Validation loss: 2.7027068035576933

Epoch: 6| Step: 3
Training loss: 2.640451669692993
Validation loss: 2.703406515941825

Epoch: 6| Step: 4
Training loss: 2.806793689727783
Validation loss: 2.693629075122136

Epoch: 6| Step: 5
Training loss: 2.2711596488952637
Validation loss: 2.6883720761986187

Epoch: 6| Step: 6
Training loss: 2.3514387607574463
Validation loss: 2.6740112407233125

Epoch: 6| Step: 7
Training loss: 3.091966152191162
Validation loss: 2.670525412405691

Epoch: 6| Step: 8
Training loss: 2.529825210571289
Validation loss: 2.6672704706909838

Epoch: 6| Step: 9
Training loss: 3.097057342529297
Validation loss: 2.6716952580277638

Epoch: 6| Step: 10
Training loss: 2.875180959701538
Validation loss: 2.6712465183709257

Epoch: 6| Step: 11
Training loss: 2.7556700706481934
Validation loss: 2.6740026704726683

Epoch: 6| Step: 12
Training loss: 3.0113377571105957
Validation loss: 2.670741101746918

Epoch: 6| Step: 13
Training loss: 3.8208563327789307
Validation loss: 2.670692354120234

Epoch: 67| Step: 0
Training loss: 3.2358481884002686
Validation loss: 2.674625792810994

Epoch: 6| Step: 1
Training loss: 2.9273617267608643
Validation loss: 2.672325570096252

Epoch: 6| Step: 2
Training loss: 2.8805794715881348
Validation loss: 2.6764621016799763

Epoch: 6| Step: 3
Training loss: 3.0924313068389893
Validation loss: 2.679400054357385

Epoch: 6| Step: 4
Training loss: 3.683581590652466
Validation loss: 2.692654558407363

Epoch: 6| Step: 5
Training loss: 2.5950188636779785
Validation loss: 2.6816875344963482

Epoch: 6| Step: 6
Training loss: 2.9865756034851074
Validation loss: 2.6708213360078874

Epoch: 6| Step: 7
Training loss: 2.2495687007904053
Validation loss: 2.6725064195612425

Epoch: 6| Step: 8
Training loss: 2.7877607345581055
Validation loss: 2.6705358387321554

Epoch: 6| Step: 9
Training loss: 2.4522695541381836
Validation loss: 2.6722854542475876

Epoch: 6| Step: 10
Training loss: 2.4347498416900635
Validation loss: 2.668588448596257

Epoch: 6| Step: 11
Training loss: 2.716257095336914
Validation loss: 2.6853487517244075

Epoch: 6| Step: 12
Training loss: 2.580056667327881
Validation loss: 2.6844101516149377

Epoch: 6| Step: 13
Training loss: 2.8432068824768066
Validation loss: 2.6681106731455815

Epoch: 68| Step: 0
Training loss: 2.7186825275421143
Validation loss: 2.6675170993292205

Epoch: 6| Step: 1
Training loss: 2.5314812660217285
Validation loss: 2.6634575551556003

Epoch: 6| Step: 2
Training loss: 2.884838581085205
Validation loss: 2.659215675887241

Epoch: 6| Step: 3
Training loss: 2.518096446990967
Validation loss: 2.6631122045619513

Epoch: 6| Step: 4
Training loss: 3.4492359161376953
Validation loss: 2.6637674480356197

Epoch: 6| Step: 5
Training loss: 3.3313183784484863
Validation loss: 2.669212354126797

Epoch: 6| Step: 6
Training loss: 2.9056663513183594
Validation loss: 2.669471650995234

Epoch: 6| Step: 7
Training loss: 2.384819746017456
Validation loss: 2.672038934564078

Epoch: 6| Step: 8
Training loss: 2.283348798751831
Validation loss: 2.6610034665753766

Epoch: 6| Step: 9
Training loss: 3.355060577392578
Validation loss: 2.662945193629111

Epoch: 6| Step: 10
Training loss: 2.720951795578003
Validation loss: 2.659937745781355

Epoch: 6| Step: 11
Training loss: 2.6662650108337402
Validation loss: 2.6588328730675483

Epoch: 6| Step: 12
Training loss: 2.74802303314209
Validation loss: 2.6573344712616294

Epoch: 6| Step: 13
Training loss: 2.785224199295044
Validation loss: 2.6586503187815347

Epoch: 69| Step: 0
Training loss: 3.1719908714294434
Validation loss: 2.6558112764871247

Epoch: 6| Step: 1
Training loss: 2.8085951805114746
Validation loss: 2.658658173776442

Epoch: 6| Step: 2
Training loss: 2.61847186088562
Validation loss: 2.657929169234409

Epoch: 6| Step: 3
Training loss: 2.038318395614624
Validation loss: 2.6559192775398173

Epoch: 6| Step: 4
Training loss: 3.3351736068725586
Validation loss: 2.658678752119823

Epoch: 6| Step: 5
Training loss: 3.066708564758301
Validation loss: 2.6506953752169045

Epoch: 6| Step: 6
Training loss: 2.521611452102661
Validation loss: 2.6625088491747455

Epoch: 6| Step: 7
Training loss: 2.7498602867126465
Validation loss: 2.6629788516670145

Epoch: 6| Step: 8
Training loss: 2.9759294986724854
Validation loss: 2.6557702454187537

Epoch: 6| Step: 9
Training loss: 2.385392189025879
Validation loss: 2.658733542247485

Epoch: 6| Step: 10
Training loss: 2.810697555541992
Validation loss: 2.6653639552413777

Epoch: 6| Step: 11
Training loss: 2.4122066497802734
Validation loss: 2.6663750858717066

Epoch: 6| Step: 12
Training loss: 3.439014434814453
Validation loss: 2.663842329414942

Epoch: 6| Step: 13
Training loss: 2.7887187004089355
Validation loss: 2.6566896412962224

Epoch: 70| Step: 0
Training loss: 3.197608232498169
Validation loss: 2.6633492567205943

Epoch: 6| Step: 1
Training loss: 2.7766120433807373
Validation loss: 2.6637147600932787

Epoch: 6| Step: 2
Training loss: 2.56508731842041
Validation loss: 2.6722065607706704

Epoch: 6| Step: 3
Training loss: 2.2202892303466797
Validation loss: 2.67449418703715

Epoch: 6| Step: 4
Training loss: 2.7645716667175293
Validation loss: 2.6801406645005748

Epoch: 6| Step: 5
Training loss: 2.7464427947998047
Validation loss: 2.6803467863349506

Epoch: 6| Step: 6
Training loss: 3.041477918624878
Validation loss: 2.6821631052160777

Epoch: 6| Step: 7
Training loss: 2.6377508640289307
Validation loss: 2.67531995619497

Epoch: 6| Step: 8
Training loss: 2.4703445434570312
Validation loss: 2.6687926553910777

Epoch: 6| Step: 9
Training loss: 3.4894683361053467
Validation loss: 2.6574720977455057

Epoch: 6| Step: 10
Training loss: 2.651141405105591
Validation loss: 2.6522289065904516

Epoch: 6| Step: 11
Training loss: 3.0104024410247803
Validation loss: 2.652361536538729

Epoch: 6| Step: 12
Training loss: 2.853609561920166
Validation loss: 2.651483581912133

Epoch: 6| Step: 13
Training loss: 2.9023139476776123
Validation loss: 2.652317977720691

Epoch: 71| Step: 0
Training loss: 2.96826171875
Validation loss: 2.6582164200403358

Epoch: 6| Step: 1
Training loss: 2.521400213241577
Validation loss: 2.64972621651106

Epoch: 6| Step: 2
Training loss: 2.928253650665283
Validation loss: 2.6538158283438733

Epoch: 6| Step: 3
Training loss: 3.662701368331909
Validation loss: 2.6615110699848463

Epoch: 6| Step: 4
Training loss: 2.520078182220459
Validation loss: 2.664985728520219

Epoch: 6| Step: 5
Training loss: 3.5112247467041016
Validation loss: 2.664270826565322

Epoch: 6| Step: 6
Training loss: 2.3969547748565674
Validation loss: 2.6624257244089597

Epoch: 6| Step: 7
Training loss: 3.0448966026306152
Validation loss: 2.6616925142144643

Epoch: 6| Step: 8
Training loss: 2.2584147453308105
Validation loss: 2.671097614431894

Epoch: 6| Step: 9
Training loss: 2.6502175331115723
Validation loss: 2.6602403015218754

Epoch: 6| Step: 10
Training loss: 2.5304040908813477
Validation loss: 2.6643558215069514

Epoch: 6| Step: 11
Training loss: 2.5618414878845215
Validation loss: 2.662211710406888

Epoch: 6| Step: 12
Training loss: 2.364318370819092
Validation loss: 2.658177496284567

Epoch: 6| Step: 13
Training loss: 3.703599691390991
Validation loss: 2.652257078437395

Epoch: 72| Step: 0
Training loss: 3.1004374027252197
Validation loss: 2.64694950016596

Epoch: 6| Step: 1
Training loss: 2.465529680252075
Validation loss: 2.6516918828410487

Epoch: 6| Step: 2
Training loss: 2.9381630420684814
Validation loss: 2.649252709522042

Epoch: 6| Step: 3
Training loss: 3.183490753173828
Validation loss: 2.6530328591664634

Epoch: 6| Step: 4
Training loss: 2.428903341293335
Validation loss: 2.6555031679009877

Epoch: 6| Step: 5
Training loss: 3.800827741622925
Validation loss: 2.6503787937984673

Epoch: 6| Step: 6
Training loss: 2.8341023921966553
Validation loss: 2.653408635047174

Epoch: 6| Step: 7
Training loss: 2.8554587364196777
Validation loss: 2.6542607917580554

Epoch: 6| Step: 8
Training loss: 2.513261318206787
Validation loss: 2.6482864759301625

Epoch: 6| Step: 9
Training loss: 2.4181652069091797
Validation loss: 2.64775421286142

Epoch: 6| Step: 10
Training loss: 2.7294840812683105
Validation loss: 2.6513697562679166

Epoch: 6| Step: 11
Training loss: 2.7909042835235596
Validation loss: 2.646749306750554

Epoch: 6| Step: 12
Training loss: 2.685713291168213
Validation loss: 2.650027701931615

Epoch: 6| Step: 13
Training loss: 2.0431272983551025
Validation loss: 2.666795625481554

Epoch: 73| Step: 0
Training loss: 2.874814748764038
Validation loss: 2.6619761451598136

Epoch: 6| Step: 1
Training loss: 2.539463520050049
Validation loss: 2.6578050018638693

Epoch: 6| Step: 2
Training loss: 2.9025769233703613
Validation loss: 2.662340628203525

Epoch: 6| Step: 3
Training loss: 2.4772143363952637
Validation loss: 2.658552969655683

Epoch: 6| Step: 4
Training loss: 2.5184364318847656
Validation loss: 2.6578307972159436

Epoch: 6| Step: 5
Training loss: 2.341834545135498
Validation loss: 2.6531771511159916

Epoch: 6| Step: 6
Training loss: 2.8321521282196045
Validation loss: 2.6548469835712063

Epoch: 6| Step: 7
Training loss: 3.051161050796509
Validation loss: 2.6540512038815405

Epoch: 6| Step: 8
Training loss: 3.4003968238830566
Validation loss: 2.6589171168624715

Epoch: 6| Step: 9
Training loss: 2.7253270149230957
Validation loss: 2.6495396603820143

Epoch: 6| Step: 10
Training loss: 2.9833619594573975
Validation loss: 2.647457948295019

Epoch: 6| Step: 11
Training loss: 3.013780117034912
Validation loss: 2.653423358035344

Epoch: 6| Step: 12
Training loss: 2.588212490081787
Validation loss: 2.6533817937297206

Epoch: 6| Step: 13
Training loss: 2.7766714096069336
Validation loss: 2.6538730359846547

Epoch: 74| Step: 0
Training loss: 2.2224435806274414
Validation loss: 2.6421986087676017

Epoch: 6| Step: 1
Training loss: 3.034095287322998
Validation loss: 2.647088953243789

Epoch: 6| Step: 2
Training loss: 2.2780869007110596
Validation loss: 2.6451920873375347

Epoch: 6| Step: 3
Training loss: 2.5876264572143555
Validation loss: 2.64653601184968

Epoch: 6| Step: 4
Training loss: 3.196446657180786
Validation loss: 2.64276788567984

Epoch: 6| Step: 5
Training loss: 3.044445514678955
Validation loss: 2.6443476574395293

Epoch: 6| Step: 6
Training loss: 2.3570775985717773
Validation loss: 2.641721663936492

Epoch: 6| Step: 7
Training loss: 2.547541856765747
Validation loss: 2.6425265317322104

Epoch: 6| Step: 8
Training loss: 3.474883794784546
Validation loss: 2.6426689342785905

Epoch: 6| Step: 9
Training loss: 2.501920223236084
Validation loss: 2.6489477157592773

Epoch: 6| Step: 10
Training loss: 2.1503472328186035
Validation loss: 2.6482249562458327

Epoch: 6| Step: 11
Training loss: 3.2834298610687256
Validation loss: 2.6506662009864725

Epoch: 6| Step: 12
Training loss: 3.2015061378479004
Validation loss: 2.6573110165134555

Epoch: 6| Step: 13
Training loss: 3.214056968688965
Validation loss: 2.658629104655276

Epoch: 75| Step: 0
Training loss: 3.178853988647461
Validation loss: 2.6581450175213557

Epoch: 6| Step: 1
Training loss: 2.6347107887268066
Validation loss: 2.660152199447796

Epoch: 6| Step: 2
Training loss: 2.756946563720703
Validation loss: 2.65305672666078

Epoch: 6| Step: 3
Training loss: 3.168029308319092
Validation loss: 2.665680213641095

Epoch: 6| Step: 4
Training loss: 2.527524948120117
Validation loss: 2.6666181677131244

Epoch: 6| Step: 5
Training loss: 2.816150665283203
Validation loss: 2.659134434115502

Epoch: 6| Step: 6
Training loss: 2.648045301437378
Validation loss: 2.6501619533825944

Epoch: 6| Step: 7
Training loss: 2.8793325424194336
Validation loss: 2.643205186372162

Epoch: 6| Step: 8
Training loss: 3.1843199729919434
Validation loss: 2.637662495336225

Epoch: 6| Step: 9
Training loss: 1.9355759620666504
Validation loss: 2.638657100739018

Epoch: 6| Step: 10
Training loss: 1.6956173181533813
Validation loss: 2.6370393742797194

Epoch: 6| Step: 11
Training loss: 2.399016857147217
Validation loss: 2.635731453536659

Epoch: 6| Step: 12
Training loss: 4.021439552307129
Validation loss: 2.6370698713487193

Epoch: 6| Step: 13
Training loss: 3.4498777389526367
Validation loss: 2.6389061225357877

Epoch: 76| Step: 0
Training loss: 2.341785430908203
Validation loss: 2.635391994189191

Epoch: 6| Step: 1
Training loss: 2.451615333557129
Validation loss: 2.6331578762300554

Epoch: 6| Step: 2
Training loss: 3.246161699295044
Validation loss: 2.635842357912371

Epoch: 6| Step: 3
Training loss: 3.354788303375244
Validation loss: 2.636634580550655

Epoch: 6| Step: 4
Training loss: 3.0931825637817383
Validation loss: 2.6318104600393646

Epoch: 6| Step: 5
Training loss: 3.161878824234009
Validation loss: 2.6304007217448246

Epoch: 6| Step: 6
Training loss: 2.0253143310546875
Validation loss: 2.631404276817076

Epoch: 6| Step: 7
Training loss: 2.696033477783203
Validation loss: 2.6331656953339935

Epoch: 6| Step: 8
Training loss: 3.385446786880493
Validation loss: 2.6342645614377913

Epoch: 6| Step: 9
Training loss: 2.3119606971740723
Validation loss: 2.6342170007767214

Epoch: 6| Step: 10
Training loss: 2.807504177093506
Validation loss: 2.630095986909764

Epoch: 6| Step: 11
Training loss: 2.482804775238037
Validation loss: 2.6329936109563357

Epoch: 6| Step: 12
Training loss: 3.029810905456543
Validation loss: 2.628761032576202

Epoch: 6| Step: 13
Training loss: 2.445124626159668
Validation loss: 2.6328331167979906

Epoch: 77| Step: 0
Training loss: 3.0180752277374268
Validation loss: 2.6300946307438675

Epoch: 6| Step: 1
Training loss: 2.751708984375
Validation loss: 2.6312971525294806

Epoch: 6| Step: 2
Training loss: 2.138777017593384
Validation loss: 2.630390400527626

Epoch: 6| Step: 3
Training loss: 3.5350327491760254
Validation loss: 2.629478959627049

Epoch: 6| Step: 4
Training loss: 3.0566463470458984
Validation loss: 2.6306133372809297

Epoch: 6| Step: 5
Training loss: 2.5308241844177246
Validation loss: 2.6305680223690566

Epoch: 6| Step: 6
Training loss: 2.6657910346984863
Validation loss: 2.628314141304262

Epoch: 6| Step: 7
Training loss: 2.9004170894622803
Validation loss: 2.6339183033153577

Epoch: 6| Step: 8
Training loss: 3.1456284523010254
Validation loss: 2.633718731582806

Epoch: 6| Step: 9
Training loss: 3.432818651199341
Validation loss: 2.6304348386744016

Epoch: 6| Step: 10
Training loss: 3.139192819595337
Validation loss: 2.628706947449715

Epoch: 6| Step: 11
Training loss: 1.664841890335083
Validation loss: 2.6288201091110066

Epoch: 6| Step: 12
Training loss: 1.9152424335479736
Validation loss: 2.6303673200709845

Epoch: 6| Step: 13
Training loss: 3.186605930328369
Validation loss: 2.6318897662624234

Epoch: 78| Step: 0
Training loss: 2.614285469055176
Validation loss: 2.6310822784259753

Epoch: 6| Step: 1
Training loss: 2.8406124114990234
Validation loss: 2.6353425313067693

Epoch: 6| Step: 2
Training loss: 2.996286392211914
Validation loss: 2.6351660272126556

Epoch: 6| Step: 3
Training loss: 3.1908769607543945
Validation loss: 2.6328660262528287

Epoch: 6| Step: 4
Training loss: 2.9245762825012207
Validation loss: 2.6358408030643257

Epoch: 6| Step: 5
Training loss: 2.340251922607422
Validation loss: 2.639286818042878

Epoch: 6| Step: 6
Training loss: 2.6053080558776855
Validation loss: 2.6446083591830347

Epoch: 6| Step: 7
Training loss: 2.7362968921661377
Validation loss: 2.6417246146868636

Epoch: 6| Step: 8
Training loss: 2.2118518352508545
Validation loss: 2.6539387908033145

Epoch: 6| Step: 9
Training loss: 3.7147533893585205
Validation loss: 2.649361200230096

Epoch: 6| Step: 10
Training loss: 3.3380064964294434
Validation loss: 2.6514922008719495

Epoch: 6| Step: 11
Training loss: 2.5526909828186035
Validation loss: 2.6457347228962886

Epoch: 6| Step: 12
Training loss: 2.578001022338867
Validation loss: 2.6376145603836223

Epoch: 6| Step: 13
Training loss: 1.8102641105651855
Validation loss: 2.6325179120545745

Epoch: 79| Step: 0
Training loss: 3.23396897315979
Validation loss: 2.626789516018283

Epoch: 6| Step: 1
Training loss: 2.5026493072509766
Validation loss: 2.624863983482443

Epoch: 6| Step: 2
Training loss: 3.4246561527252197
Validation loss: 2.6222007556628157

Epoch: 6| Step: 3
Training loss: 3.0803754329681396
Validation loss: 2.624977760417487

Epoch: 6| Step: 4
Training loss: 2.860950469970703
Validation loss: 2.6224964254645893

Epoch: 6| Step: 5
Training loss: 2.4791059494018555
Validation loss: 2.627738283526513

Epoch: 6| Step: 6
Training loss: 3.0604987144470215
Validation loss: 2.624786989663237

Epoch: 6| Step: 7
Training loss: 3.4999589920043945
Validation loss: 2.6255949312640774

Epoch: 6| Step: 8
Training loss: 2.899263620376587
Validation loss: 2.621524005807856

Epoch: 6| Step: 9
Training loss: 1.6851894855499268
Validation loss: 2.621432258236793

Epoch: 6| Step: 10
Training loss: 2.292005777359009
Validation loss: 2.6251834554056965

Epoch: 6| Step: 11
Training loss: 2.750875473022461
Validation loss: 2.62480430705573

Epoch: 6| Step: 12
Training loss: 2.7398781776428223
Validation loss: 2.6206794708005843

Epoch: 6| Step: 13
Training loss: 1.9956135749816895
Validation loss: 2.6242086605359147

Epoch: 80| Step: 0
Training loss: 3.177043914794922
Validation loss: 2.623824252877184

Epoch: 6| Step: 1
Training loss: 3.8684659004211426
Validation loss: 2.6254649469929356

Epoch: 6| Step: 2
Training loss: 1.8718665838241577
Validation loss: 2.6258007941707486

Epoch: 6| Step: 3
Training loss: 2.8712615966796875
Validation loss: 2.623904412792575

Epoch: 6| Step: 4
Training loss: 2.946749210357666
Validation loss: 2.62428718228494

Epoch: 6| Step: 5
Training loss: 2.482907295227051
Validation loss: 2.62460760660069

Epoch: 6| Step: 6
Training loss: 2.407557487487793
Validation loss: 2.6246618391365133

Epoch: 6| Step: 7
Training loss: 3.5886006355285645
Validation loss: 2.6267819096965175

Epoch: 6| Step: 8
Training loss: 2.1791653633117676
Validation loss: 2.628304261033253

Epoch: 6| Step: 9
Training loss: 2.6433396339416504
Validation loss: 2.622909909935408

Epoch: 6| Step: 10
Training loss: 2.0201520919799805
Validation loss: 2.621023183227867

Epoch: 6| Step: 11
Training loss: 2.2542226314544678
Validation loss: 2.6243782351093907

Epoch: 6| Step: 12
Training loss: 3.1342849731445312
Validation loss: 2.6226164679373465

Epoch: 6| Step: 13
Training loss: 3.781881093978882
Validation loss: 2.622584109665245

Epoch: 81| Step: 0
Training loss: 3.5347118377685547
Validation loss: 2.627772975993413

Epoch: 6| Step: 1
Training loss: 2.2793116569519043
Validation loss: 2.6330029579900924

Epoch: 6| Step: 2
Training loss: 2.7554850578308105
Validation loss: 2.6405960821336314

Epoch: 6| Step: 3
Training loss: 2.1053903102874756
Validation loss: 2.6446198878749723

Epoch: 6| Step: 4
Training loss: 2.6633381843566895
Validation loss: 2.6491720189330397

Epoch: 6| Step: 5
Training loss: 2.6054413318634033
Validation loss: 2.645743147019417

Epoch: 6| Step: 6
Training loss: 3.751549243927002
Validation loss: 2.6399700974905365

Epoch: 6| Step: 7
Training loss: 3.0510334968566895
Validation loss: 2.6371952949031705

Epoch: 6| Step: 8
Training loss: 3.21260404586792
Validation loss: 2.621719709006689

Epoch: 6| Step: 9
Training loss: 2.2420225143432617
Validation loss: 2.624412185402327

Epoch: 6| Step: 10
Training loss: 2.9650719165802
Validation loss: 2.6215302328909598

Epoch: 6| Step: 11
Training loss: 2.5941085815429688
Validation loss: 2.619091903009722

Epoch: 6| Step: 12
Training loss: 2.704832077026367
Validation loss: 2.621398530980592

Epoch: 6| Step: 13
Training loss: 1.9275155067443848
Validation loss: 2.6157069103692168

Epoch: 82| Step: 0
Training loss: 2.071606159210205
Validation loss: 2.6214973798362156

Epoch: 6| Step: 1
Training loss: 2.4833383560180664
Validation loss: 2.6238353829230032

Epoch: 6| Step: 2
Training loss: 3.129849910736084
Validation loss: 2.6202655248744513

Epoch: 6| Step: 3
Training loss: 2.245682716369629
Validation loss: 2.624082806289837

Epoch: 6| Step: 4
Training loss: 3.520258903503418
Validation loss: 2.6191287168892483

Epoch: 6| Step: 5
Training loss: 2.5034260749816895
Validation loss: 2.6260470062173824

Epoch: 6| Step: 6
Training loss: 2.681307554244995
Validation loss: 2.6363448609587965

Epoch: 6| Step: 7
Training loss: 2.8150827884674072
Validation loss: 2.6422607232165594

Epoch: 6| Step: 8
Training loss: 2.721453905105591
Validation loss: 2.640581653964135

Epoch: 6| Step: 9
Training loss: 2.514343738555908
Validation loss: 2.637000714578936

Epoch: 6| Step: 10
Training loss: 3.2999563217163086
Validation loss: 2.634255971959842

Epoch: 6| Step: 11
Training loss: 2.760150671005249
Validation loss: 2.6287362806258665

Epoch: 6| Step: 12
Training loss: 3.0240108966827393
Validation loss: 2.625467026105491

Epoch: 6| Step: 13
Training loss: 3.2420034408569336
Validation loss: 2.617573404824862

Epoch: 83| Step: 0
Training loss: 3.142674446105957
Validation loss: 2.6175976363561486

Epoch: 6| Step: 1
Training loss: 2.2955472469329834
Validation loss: 2.61531691141026

Epoch: 6| Step: 2
Training loss: 2.6166603565216064
Validation loss: 2.6125343358644875

Epoch: 6| Step: 3
Training loss: 2.663447856903076
Validation loss: 2.6185464397553475

Epoch: 6| Step: 4
Training loss: 2.4188971519470215
Validation loss: 2.6190291707233717

Epoch: 6| Step: 5
Training loss: 2.939272165298462
Validation loss: 2.6172111777849096

Epoch: 6| Step: 6
Training loss: 3.2864131927490234
Validation loss: 2.614408023895756

Epoch: 6| Step: 7
Training loss: 2.061220169067383
Validation loss: 2.6195840733025664

Epoch: 6| Step: 8
Training loss: 3.2066502571105957
Validation loss: 2.621985561104231

Epoch: 6| Step: 9
Training loss: 2.9118919372558594
Validation loss: 2.6291524697375555

Epoch: 6| Step: 10
Training loss: 3.047896385192871
Validation loss: 2.6365565382024294

Epoch: 6| Step: 11
Training loss: 3.049086570739746
Validation loss: 2.635022468464349

Epoch: 6| Step: 12
Training loss: 2.3841629028320312
Validation loss: 2.646628459294637

Epoch: 6| Step: 13
Training loss: 2.664354085922241
Validation loss: 2.6625890783084336

Epoch: 84| Step: 0
Training loss: 3.2861382961273193
Validation loss: 2.6636783281962075

Epoch: 6| Step: 1
Training loss: 2.526505470275879
Validation loss: 2.6603966451460317

Epoch: 6| Step: 2
Training loss: 3.2435855865478516
Validation loss: 2.6412517075897544

Epoch: 6| Step: 3
Training loss: 2.642451286315918
Validation loss: 2.6250043633163616

Epoch: 6| Step: 4
Training loss: 2.040436267852783
Validation loss: 2.6139209629386984

Epoch: 6| Step: 5
Training loss: 3.5943374633789062
Validation loss: 2.6122264221150386

Epoch: 6| Step: 6
Training loss: 2.4012346267700195
Validation loss: 2.6174073629481818

Epoch: 6| Step: 7
Training loss: 2.5872623920440674
Validation loss: 2.6151848505902033

Epoch: 6| Step: 8
Training loss: 3.0911173820495605
Validation loss: 2.618841799356604

Epoch: 6| Step: 9
Training loss: 2.963711977005005
Validation loss: 2.6222034423582015

Epoch: 6| Step: 10
Training loss: 2.6679983139038086
Validation loss: 2.6235334001561648

Epoch: 6| Step: 11
Training loss: 2.2462010383605957
Validation loss: 2.6161885312808457

Epoch: 6| Step: 12
Training loss: 2.785064458847046
Validation loss: 2.6167764150968162

Epoch: 6| Step: 13
Training loss: 2.773937463760376
Validation loss: 2.6200539040309128

Epoch: 85| Step: 0
Training loss: 2.6791443824768066
Validation loss: 2.624803425163351

Epoch: 6| Step: 1
Training loss: 2.282083034515381
Validation loss: 2.6212805804385932

Epoch: 6| Step: 2
Training loss: 2.2651309967041016
Validation loss: 2.6285252801833616

Epoch: 6| Step: 3
Training loss: 2.8508872985839844
Validation loss: 2.62544128458987

Epoch: 6| Step: 4
Training loss: 3.059049129486084
Validation loss: 2.637814667917067

Epoch: 6| Step: 5
Training loss: 3.4051294326782227
Validation loss: 2.6303194517730386

Epoch: 6| Step: 6
Training loss: 3.98451828956604
Validation loss: 2.618209238975279

Epoch: 6| Step: 7
Training loss: 2.9277663230895996
Validation loss: 2.61954072213942

Epoch: 6| Step: 8
Training loss: 2.515580654144287
Validation loss: 2.6151775237052672

Epoch: 6| Step: 9
Training loss: 1.7002226114273071
Validation loss: 2.6166820295395388

Epoch: 6| Step: 10
Training loss: 2.6204700469970703
Validation loss: 2.6219371185507825

Epoch: 6| Step: 11
Training loss: 2.610722064971924
Validation loss: 2.6205368580356723

Epoch: 6| Step: 12
Training loss: 2.923814058303833
Validation loss: 2.616407812282603

Epoch: 6| Step: 13
Training loss: 3.0911760330200195
Validation loss: 2.6139412874816568

Epoch: 86| Step: 0
Training loss: 3.502340316772461
Validation loss: 2.615124502489644

Epoch: 6| Step: 1
Training loss: 2.3308658599853516
Validation loss: 2.611418298495713

Epoch: 6| Step: 2
Training loss: 2.403573513031006
Validation loss: 2.6127999085252003

Epoch: 6| Step: 3
Training loss: 1.9248632192611694
Validation loss: 2.6146104592148975

Epoch: 6| Step: 4
Training loss: 3.198629379272461
Validation loss: 2.6151218593761487

Epoch: 6| Step: 5
Training loss: 2.973926067352295
Validation loss: 2.6192820405447357

Epoch: 6| Step: 6
Training loss: 2.883260726928711
Validation loss: 2.621793152183615

Epoch: 6| Step: 7
Training loss: 2.786294460296631
Validation loss: 2.6257596528658302

Epoch: 6| Step: 8
Training loss: 3.3744754791259766
Validation loss: 2.6170538958682807

Epoch: 6| Step: 9
Training loss: 3.047473907470703
Validation loss: 2.6079385152427097

Epoch: 6| Step: 10
Training loss: 2.4246766567230225
Validation loss: 2.6038395204851703

Epoch: 6| Step: 11
Training loss: 2.501034736633301
Validation loss: 2.607666313007314

Epoch: 6| Step: 12
Training loss: 2.842374086380005
Validation loss: 2.611942909097159

Epoch: 6| Step: 13
Training loss: 2.5070762634277344
Validation loss: 2.6120200951894126

Epoch: 87| Step: 0
Training loss: 2.7603909969329834
Validation loss: 2.608488703286776

Epoch: 6| Step: 1
Training loss: 2.4126877784729004
Validation loss: 2.6118696812660462

Epoch: 6| Step: 2
Training loss: 3.173083782196045
Validation loss: 2.609007199605306

Epoch: 6| Step: 3
Training loss: 2.8013949394226074
Validation loss: 2.611838791960029

Epoch: 6| Step: 4
Training loss: 1.9816287755966187
Validation loss: 2.6159143627330823

Epoch: 6| Step: 5
Training loss: 3.744442939758301
Validation loss: 2.6174051428353913

Epoch: 6| Step: 6
Training loss: 2.6397550106048584
Validation loss: 2.614700835238221

Epoch: 6| Step: 7
Training loss: 2.538527011871338
Validation loss: 2.613117474381642

Epoch: 6| Step: 8
Training loss: 3.3417506217956543
Validation loss: 2.6139683390176423

Epoch: 6| Step: 9
Training loss: 2.748809337615967
Validation loss: 2.6144635600428425

Epoch: 6| Step: 10
Training loss: 2.240328073501587
Validation loss: 2.6126953145509124

Epoch: 6| Step: 11
Training loss: 3.3293075561523438
Validation loss: 2.6106916781394713

Epoch: 6| Step: 12
Training loss: 2.569803237915039
Validation loss: 2.604821397412208

Epoch: 6| Step: 13
Training loss: 2.203301429748535
Validation loss: 2.6088365175390757

Epoch: 88| Step: 0
Training loss: 3.122281074523926
Validation loss: 2.6067339425445883

Epoch: 6| Step: 1
Training loss: 2.496462106704712
Validation loss: 2.6008527868537494

Epoch: 6| Step: 2
Training loss: 2.662135124206543
Validation loss: 2.600314650484311

Epoch: 6| Step: 3
Training loss: 1.9258179664611816
Validation loss: 2.604006805727559

Epoch: 6| Step: 4
Training loss: 2.677190065383911
Validation loss: 2.6064204964586484

Epoch: 6| Step: 5
Training loss: 3.3608362674713135
Validation loss: 2.600581507528982

Epoch: 6| Step: 6
Training loss: 2.4438066482543945
Validation loss: 2.6054833063515286

Epoch: 6| Step: 7
Training loss: 3.2392594814300537
Validation loss: 2.6033452787706928

Epoch: 6| Step: 8
Training loss: 2.939608573913574
Validation loss: 2.596584984051284

Epoch: 6| Step: 9
Training loss: 3.0549440383911133
Validation loss: 2.6049817044247865

Epoch: 6| Step: 10
Training loss: 1.9948546886444092
Validation loss: 2.6122315929782007

Epoch: 6| Step: 11
Training loss: 2.9880106449127197
Validation loss: 2.608653317215622

Epoch: 6| Step: 12
Training loss: 3.3494489192962646
Validation loss: 2.6076737629470004

Epoch: 6| Step: 13
Training loss: 1.9983876943588257
Validation loss: 2.6183547871087187

Epoch: 89| Step: 0
Training loss: 2.4707536697387695
Validation loss: 2.609222158308952

Epoch: 6| Step: 1
Training loss: 3.4556939601898193
Validation loss: 2.6151275711674846

Epoch: 6| Step: 2
Training loss: 2.50950288772583
Validation loss: 2.6091258243847917

Epoch: 6| Step: 3
Training loss: 2.142772674560547
Validation loss: 2.6022691060138006

Epoch: 6| Step: 4
Training loss: 3.165806770324707
Validation loss: 2.6016810171065794

Epoch: 6| Step: 5
Training loss: 2.621544361114502
Validation loss: 2.6034988434084

Epoch: 6| Step: 6
Training loss: 2.5602121353149414
Validation loss: 2.598613590322515

Epoch: 6| Step: 7
Training loss: 2.566099166870117
Validation loss: 2.5976204385039625

Epoch: 6| Step: 8
Training loss: 2.395432233810425
Validation loss: 2.601232187722319

Epoch: 6| Step: 9
Training loss: 2.6582224369049072
Validation loss: 2.5995764911815686

Epoch: 6| Step: 10
Training loss: 3.2701289653778076
Validation loss: 2.6025973853244575

Epoch: 6| Step: 11
Training loss: 2.977987766265869
Validation loss: 2.5982785122368925

Epoch: 6| Step: 12
Training loss: 3.1792221069335938
Validation loss: 2.5981764537031933

Epoch: 6| Step: 13
Training loss: 2.641822338104248
Validation loss: 2.597127676010132

Epoch: 90| Step: 0
Training loss: 1.9398654699325562
Validation loss: 2.5954166176498576

Epoch: 6| Step: 1
Training loss: 2.8403337001800537
Validation loss: 2.596884935132919

Epoch: 6| Step: 2
Training loss: 3.096681594848633
Validation loss: 2.5971148090977825

Epoch: 6| Step: 3
Training loss: 3.754800319671631
Validation loss: 2.6003594193407285

Epoch: 6| Step: 4
Training loss: 3.199720859527588
Validation loss: 2.602343811783739

Epoch: 6| Step: 5
Training loss: 3.194392681121826
Validation loss: 2.600381456395631

Epoch: 6| Step: 6
Training loss: 2.459481716156006
Validation loss: 2.5966930056131012

Epoch: 6| Step: 7
Training loss: 2.556586742401123
Validation loss: 2.5969038804372153

Epoch: 6| Step: 8
Training loss: 2.3192830085754395
Validation loss: 2.59310979740594

Epoch: 6| Step: 9
Training loss: 2.4883790016174316
Validation loss: 2.5969567606526036

Epoch: 6| Step: 10
Training loss: 2.5188512802124023
Validation loss: 2.5930479764938354

Epoch: 6| Step: 11
Training loss: 2.3476722240448
Validation loss: 2.597017934245448

Epoch: 6| Step: 12
Training loss: 3.1137115955352783
Validation loss: 2.594937252741988

Epoch: 6| Step: 13
Training loss: 2.7890138626098633
Validation loss: 2.5959564947312876

Epoch: 91| Step: 0
Training loss: 1.950067400932312
Validation loss: 2.5929571044060493

Epoch: 6| Step: 1
Training loss: 1.8960392475128174
Validation loss: 2.59178170593836

Epoch: 6| Step: 2
Training loss: 3.473616123199463
Validation loss: 2.598586749005061

Epoch: 6| Step: 3
Training loss: 2.7463550567626953
Validation loss: 2.599894533875168

Epoch: 6| Step: 4
Training loss: 2.9112844467163086
Validation loss: 2.5990339607320805

Epoch: 6| Step: 5
Training loss: 3.601350784301758
Validation loss: 2.6000317014673704

Epoch: 6| Step: 6
Training loss: 2.9303245544433594
Validation loss: 2.6014185182509886

Epoch: 6| Step: 7
Training loss: 3.570638656616211
Validation loss: 2.604539801997523

Epoch: 6| Step: 8
Training loss: 2.510075330734253
Validation loss: 2.606503330251222

Epoch: 6| Step: 9
Training loss: 2.457350730895996
Validation loss: 2.6044113405289187

Epoch: 6| Step: 10
Training loss: 1.8952388763427734
Validation loss: 2.597527288621472

Epoch: 6| Step: 11
Training loss: 2.8022890090942383
Validation loss: 2.5928560892740884

Epoch: 6| Step: 12
Training loss: 2.618717908859253
Validation loss: 2.5975530609007804

Epoch: 6| Step: 13
Training loss: 3.4533159732818604
Validation loss: 2.5969257226554294

Epoch: 92| Step: 0
Training loss: 2.8861238956451416
Validation loss: 2.594066648073094

Epoch: 6| Step: 1
Training loss: 2.364192008972168
Validation loss: 2.5925095312057005

Epoch: 6| Step: 2
Training loss: 3.16672682762146
Validation loss: 2.594931689641809

Epoch: 6| Step: 3
Training loss: 3.0932514667510986
Validation loss: 2.5930764546958347

Epoch: 6| Step: 4
Training loss: 3.4383316040039062
Validation loss: 2.590543923839446

Epoch: 6| Step: 5
Training loss: 3.0003044605255127
Validation loss: 2.591960181472122

Epoch: 6| Step: 6
Training loss: 2.6273059844970703
Validation loss: 2.592125754202566

Epoch: 6| Step: 7
Training loss: 2.4313488006591797
Validation loss: 2.589596071550923

Epoch: 6| Step: 8
Training loss: 2.606969118118286
Validation loss: 2.594872881007451

Epoch: 6| Step: 9
Training loss: 2.3720526695251465
Validation loss: 2.590046900574879

Epoch: 6| Step: 10
Training loss: 3.186657428741455
Validation loss: 2.5994298688827024

Epoch: 6| Step: 11
Training loss: 2.985879421234131
Validation loss: 2.595971125428395

Epoch: 6| Step: 12
Training loss: 2.138601303100586
Validation loss: 2.6033980564404557

Epoch: 6| Step: 13
Training loss: 1.8735334873199463
Validation loss: 2.597272232014646

Epoch: 93| Step: 0
Training loss: 2.3632001876831055
Validation loss: 2.5854377003126245

Epoch: 6| Step: 1
Training loss: 2.880274772644043
Validation loss: 2.5916161511534

Epoch: 6| Step: 2
Training loss: 1.8873727321624756
Validation loss: 2.5926380208743516

Epoch: 6| Step: 3
Training loss: 2.155348300933838
Validation loss: 2.5895560069750716

Epoch: 6| Step: 4
Training loss: 2.907932996749878
Validation loss: 2.5904002035817792

Epoch: 6| Step: 5
Training loss: 2.541325569152832
Validation loss: 2.5865127501949186

Epoch: 6| Step: 6
Training loss: 2.5833845138549805
Validation loss: 2.589619815990489

Epoch: 6| Step: 7
Training loss: 2.8473262786865234
Validation loss: 2.589342991511027

Epoch: 6| Step: 8
Training loss: 3.2796568870544434
Validation loss: 2.584663170640187

Epoch: 6| Step: 9
Training loss: 3.63090181350708
Validation loss: 2.5901894056668846

Epoch: 6| Step: 10
Training loss: 2.4810256958007812
Validation loss: 2.586296317397907

Epoch: 6| Step: 11
Training loss: 3.092154026031494
Validation loss: 2.5822121430468816

Epoch: 6| Step: 12
Training loss: 3.209868907928467
Validation loss: 2.5867273807525635

Epoch: 6| Step: 13
Training loss: 2.4582102298736572
Validation loss: 2.587826539111394

Epoch: 94| Step: 0
Training loss: 3.0229830741882324
Validation loss: 2.5900640154397614

Epoch: 6| Step: 1
Training loss: 2.478092670440674
Validation loss: 2.5885374135868524

Epoch: 6| Step: 2
Training loss: 2.5268898010253906
Validation loss: 2.5882263670685473

Epoch: 6| Step: 3
Training loss: 3.4224939346313477
Validation loss: 2.5870146879585842

Epoch: 6| Step: 4
Training loss: 2.571780204772949
Validation loss: 2.6006152142760572

Epoch: 6| Step: 5
Training loss: 1.9916505813598633
Validation loss: 2.6044806203534527

Epoch: 6| Step: 6
Training loss: 2.259814739227295
Validation loss: 2.5988248343108804

Epoch: 6| Step: 7
Training loss: 2.8946733474731445
Validation loss: 2.5995622193941506

Epoch: 6| Step: 8
Training loss: 2.7651515007019043
Validation loss: 2.5953053992281676

Epoch: 6| Step: 9
Training loss: 2.9125564098358154
Validation loss: 2.5865897440141246

Epoch: 6| Step: 10
Training loss: 2.5849666595458984
Validation loss: 2.5870072559643815

Epoch: 6| Step: 11
Training loss: 2.6970255374908447
Validation loss: 2.584124944543326

Epoch: 6| Step: 12
Training loss: 3.3055381774902344
Validation loss: 2.5840394266190065

Epoch: 6| Step: 13
Training loss: 3.2576539516448975
Validation loss: 2.5852611808366674

Epoch: 95| Step: 0
Training loss: 2.7373318672180176
Validation loss: 2.5853164965106594

Epoch: 6| Step: 1
Training loss: 3.290817975997925
Validation loss: 2.5859213747004026

Epoch: 6| Step: 2
Training loss: 2.0008206367492676
Validation loss: 2.580726977317564

Epoch: 6| Step: 3
Training loss: 3.674561023712158
Validation loss: 2.5815949286184003

Epoch: 6| Step: 4
Training loss: 2.546684741973877
Validation loss: 2.5828522174589095

Epoch: 6| Step: 5
Training loss: 2.4504125118255615
Validation loss: 2.585392157236735

Epoch: 6| Step: 6
Training loss: 3.0605716705322266
Validation loss: 2.578977559202461

Epoch: 6| Step: 7
Training loss: 2.7335152626037598
Validation loss: 2.581594623545165

Epoch: 6| Step: 8
Training loss: 2.891899585723877
Validation loss: 2.5861612571183072

Epoch: 6| Step: 9
Training loss: 2.960405111312866
Validation loss: 2.5822842659488803

Epoch: 6| Step: 10
Training loss: 2.5860376358032227
Validation loss: 2.581843965797014

Epoch: 6| Step: 11
Training loss: 2.2575511932373047
Validation loss: 2.5786125224123717

Epoch: 6| Step: 12
Training loss: 3.004983901977539
Validation loss: 2.581247229729929

Epoch: 6| Step: 13
Training loss: 1.788785457611084
Validation loss: 2.580886370392256

Epoch: 96| Step: 0
Training loss: 2.581028461456299
Validation loss: 2.581706731550155

Epoch: 6| Step: 1
Training loss: 2.9073567390441895
Validation loss: 2.585601273403373

Epoch: 6| Step: 2
Training loss: 2.490304946899414
Validation loss: 2.5940861317419235

Epoch: 6| Step: 3
Training loss: 3.390641450881958
Validation loss: 2.602507793775169

Epoch: 6| Step: 4
Training loss: 3.032149076461792
Validation loss: 2.610190858123123

Epoch: 6| Step: 5
Training loss: 1.8772614002227783
Validation loss: 2.6102277284027426

Epoch: 6| Step: 6
Training loss: 3.5897059440612793
Validation loss: 2.613894098548479

Epoch: 6| Step: 7
Training loss: 2.336033582687378
Validation loss: 2.6025233973738966

Epoch: 6| Step: 8
Training loss: 2.4371423721313477
Validation loss: 2.5973371741592244

Epoch: 6| Step: 9
Training loss: 3.2995967864990234
Validation loss: 2.5859170011294785

Epoch: 6| Step: 10
Training loss: 2.318909168243408
Validation loss: 2.5869204562197448

Epoch: 6| Step: 11
Training loss: 2.3504605293273926
Validation loss: 2.581268505383563

Epoch: 6| Step: 12
Training loss: 3.0894174575805664
Validation loss: 2.5825625875944733

Epoch: 6| Step: 13
Training loss: 2.7115886211395264
Validation loss: 2.578667597104144

Epoch: 97| Step: 0
Training loss: 2.9852821826934814
Validation loss: 2.5783506260123303

Epoch: 6| Step: 1
Training loss: 1.7002623081207275
Validation loss: 2.5826316828368814

Epoch: 6| Step: 2
Training loss: 2.3791913986206055
Validation loss: 2.584593931833903

Epoch: 6| Step: 3
Training loss: 2.6428170204162598
Validation loss: 2.5825646564524662

Epoch: 6| Step: 4
Training loss: 2.578139305114746
Validation loss: 2.578578784901609

Epoch: 6| Step: 5
Training loss: 2.510570526123047
Validation loss: 2.5797816450877855

Epoch: 6| Step: 6
Training loss: 3.189182758331299
Validation loss: 2.574806423597438

Epoch: 6| Step: 7
Training loss: 2.48819637298584
Validation loss: 2.5807354578407864

Epoch: 6| Step: 8
Training loss: 2.7571773529052734
Validation loss: 2.5814237261331208

Epoch: 6| Step: 9
Training loss: 2.8056280612945557
Validation loss: 2.5774026199053695

Epoch: 6| Step: 10
Training loss: 2.028165340423584
Validation loss: 2.5800665091442805

Epoch: 6| Step: 11
Training loss: 3.5141825675964355
Validation loss: 2.5747977584920902

Epoch: 6| Step: 12
Training loss: 3.3655521869659424
Validation loss: 2.5809906862115346

Epoch: 6| Step: 13
Training loss: 3.902284860610962
Validation loss: 2.5761545652984292

Epoch: 98| Step: 0
Training loss: 3.31280255317688
Validation loss: 2.576387684832337

Epoch: 6| Step: 1
Training loss: 1.9557385444641113
Validation loss: 2.5772465146997923

Epoch: 6| Step: 2
Training loss: 3.2325024604797363
Validation loss: 2.576050304597424

Epoch: 6| Step: 3
Training loss: 2.7407302856445312
Validation loss: 2.574660303772137

Epoch: 6| Step: 4
Training loss: 2.428061008453369
Validation loss: 2.5762088606434483

Epoch: 6| Step: 5
Training loss: 2.7209885120391846
Validation loss: 2.5801098782529115

Epoch: 6| Step: 6
Training loss: 3.275587797164917
Validation loss: 2.5743401204386065

Epoch: 6| Step: 7
Training loss: 2.306820869445801
Validation loss: 2.5759526016891643

Epoch: 6| Step: 8
Training loss: 4.105252742767334
Validation loss: 2.5767929066893873

Epoch: 6| Step: 9
Training loss: 2.2701516151428223
Validation loss: 2.5785202595495407

Epoch: 6| Step: 10
Training loss: 2.2693369388580322
Validation loss: 2.576219576661305

Epoch: 6| Step: 11
Training loss: 2.368778944015503
Validation loss: 2.577682195171233

Epoch: 6| Step: 12
Training loss: 3.0796287059783936
Validation loss: 2.576116361925679

Epoch: 6| Step: 13
Training loss: 1.94636869430542
Validation loss: 2.584669984796996

Epoch: 99| Step: 0
Training loss: 2.7260901927948
Validation loss: 2.5778052601762997

Epoch: 6| Step: 1
Training loss: 2.718653678894043
Validation loss: 2.5774047631089405

Epoch: 6| Step: 2
Training loss: 3.9202167987823486
Validation loss: 2.5772715640324417

Epoch: 6| Step: 3
Training loss: 2.6665701866149902
Validation loss: 2.5802276698491906

Epoch: 6| Step: 4
Training loss: 1.9552302360534668
Validation loss: 2.582145688354328

Epoch: 6| Step: 5
Training loss: 2.3808770179748535
Validation loss: 2.58058274433177

Epoch: 6| Step: 6
Training loss: 2.2676007747650146
Validation loss: 2.5810246634226974

Epoch: 6| Step: 7
Training loss: 3.0883278846740723
Validation loss: 2.581759370783324

Epoch: 6| Step: 8
Training loss: 2.5719141960144043
Validation loss: 2.5741860712728193

Epoch: 6| Step: 9
Training loss: 2.5873875617980957
Validation loss: 2.573039731671733

Epoch: 6| Step: 10
Training loss: 2.854938268661499
Validation loss: 2.575415336957542

Epoch: 6| Step: 11
Training loss: 2.724170684814453
Validation loss: 2.573421150125483

Epoch: 6| Step: 12
Training loss: 3.4376235008239746
Validation loss: 2.579900049394177

Epoch: 6| Step: 13
Training loss: 2.1300430297851562
Validation loss: 2.582011148493777

Epoch: 100| Step: 0
Training loss: 2.686328411102295
Validation loss: 2.585423490052582

Epoch: 6| Step: 1
Training loss: 2.1011805534362793
Validation loss: 2.5886991716200307

Epoch: 6| Step: 2
Training loss: 2.781524658203125
Validation loss: 2.584057366976174

Epoch: 6| Step: 3
Training loss: 2.815746545791626
Validation loss: 2.5843197273951706

Epoch: 6| Step: 4
Training loss: 2.798673629760742
Validation loss: 2.586668486236244

Epoch: 6| Step: 5
Training loss: 3.1518852710723877
Validation loss: 2.5842273260957453

Epoch: 6| Step: 6
Training loss: 3.416354179382324
Validation loss: 2.5824386048060592

Epoch: 6| Step: 7
Training loss: 2.9184844493865967
Validation loss: 2.586100765453872

Epoch: 6| Step: 8
Training loss: 2.9668684005737305
Validation loss: 2.5786143759245514

Epoch: 6| Step: 9
Training loss: 2.216629981994629
Validation loss: 2.5762138264153593

Epoch: 6| Step: 10
Training loss: 2.7489147186279297
Validation loss: 2.5716060233372513

Epoch: 6| Step: 11
Training loss: 2.816344976425171
Validation loss: 2.5660785577630483

Epoch: 6| Step: 12
Training loss: 2.2430801391601562
Validation loss: 2.5709708095878683

Epoch: 6| Step: 13
Training loss: 2.6320128440856934
Validation loss: 2.5686251681338073

Epoch: 101| Step: 0
Training loss: 3.1821069717407227
Validation loss: 2.581800248033257

Epoch: 6| Step: 1
Training loss: 1.7463515996932983
Validation loss: 2.593043263240527

Epoch: 6| Step: 2
Training loss: 2.557155132293701
Validation loss: 2.6027588382844002

Epoch: 6| Step: 3
Training loss: 3.081613540649414
Validation loss: 2.6210650346612416

Epoch: 6| Step: 4
Training loss: 2.35994553565979
Validation loss: 2.6123509996680805

Epoch: 6| Step: 5
Training loss: 3.1765079498291016
Validation loss: 2.6113930312536096

Epoch: 6| Step: 6
Training loss: 3.568250894546509
Validation loss: 2.5819820280997985

Epoch: 6| Step: 7
Training loss: 2.167926788330078
Validation loss: 2.567854783868277

Epoch: 6| Step: 8
Training loss: 2.827393054962158
Validation loss: 2.564712280868202

Epoch: 6| Step: 9
Training loss: 3.012320041656494
Validation loss: 2.5638705043382544

Epoch: 6| Step: 10
Training loss: 3.5777511596679688
Validation loss: 2.569057518436063

Epoch: 6| Step: 11
Training loss: 2.0534002780914307
Validation loss: 2.5795700319351687

Epoch: 6| Step: 12
Training loss: 2.6689703464508057
Validation loss: 2.583097505313094

Epoch: 6| Step: 13
Training loss: 2.4690027236938477
Validation loss: 2.579719264020202

Epoch: 102| Step: 0
Training loss: 3.1110446453094482
Validation loss: 2.5808450355324695

Epoch: 6| Step: 1
Training loss: 2.8488571643829346
Validation loss: 2.5774873123374036

Epoch: 6| Step: 2
Training loss: 1.933964729309082
Validation loss: 2.572154373250982

Epoch: 6| Step: 3
Training loss: 3.4519362449645996
Validation loss: 2.5664803699780534

Epoch: 6| Step: 4
Training loss: 2.815476894378662
Validation loss: 2.5669905524100027

Epoch: 6| Step: 5
Training loss: 2.3950419425964355
Validation loss: 2.5631493265910814

Epoch: 6| Step: 6
Training loss: 2.1397390365600586
Validation loss: 2.5635958102441605

Epoch: 6| Step: 7
Training loss: 2.85835337638855
Validation loss: 2.570253587538196

Epoch: 6| Step: 8
Training loss: 2.6600522994995117
Validation loss: 2.5794343102362847

Epoch: 6| Step: 9
Training loss: 2.9995055198669434
Validation loss: 2.5889244720500004

Epoch: 6| Step: 10
Training loss: 2.9201464653015137
Validation loss: 2.5950655655194352

Epoch: 6| Step: 11
Training loss: 3.1989779472351074
Validation loss: 2.605231826023389

Epoch: 6| Step: 12
Training loss: 1.9820592403411865
Validation loss: 2.6263107484386814

Epoch: 6| Step: 13
Training loss: 3.2136173248291016
Validation loss: 2.62688156097166

Epoch: 103| Step: 0
Training loss: 3.0035815238952637
Validation loss: 2.6232561911306074

Epoch: 6| Step: 1
Training loss: 2.5750370025634766
Validation loss: 2.593740786275556

Epoch: 6| Step: 2
Training loss: 2.958803653717041
Validation loss: 2.5893160579025105

Epoch: 6| Step: 3
Training loss: 2.4555106163024902
Validation loss: 2.5882898838289323

Epoch: 6| Step: 4
Training loss: 2.558791160583496
Validation loss: 2.567335218511602

Epoch: 6| Step: 5
Training loss: 2.731032371520996
Validation loss: 2.566700553381315

Epoch: 6| Step: 6
Training loss: 3.057417869567871
Validation loss: 2.562715276595085

Epoch: 6| Step: 7
Training loss: 2.8922152519226074
Validation loss: 2.5664006638270553

Epoch: 6| Step: 8
Training loss: 2.854094982147217
Validation loss: 2.570557754526856

Epoch: 6| Step: 9
Training loss: 3.1856980323791504
Validation loss: 2.5717740494717836

Epoch: 6| Step: 10
Training loss: 2.6748476028442383
Validation loss: 2.569245030803065

Epoch: 6| Step: 11
Training loss: 2.912065029144287
Validation loss: 2.5737702718345066

Epoch: 6| Step: 12
Training loss: 2.305406093597412
Validation loss: 2.5688972114234843

Epoch: 6| Step: 13
Training loss: 1.9891738891601562
Validation loss: 2.569463681149226

Epoch: 104| Step: 0
Training loss: 3.173247814178467
Validation loss: 2.571526406913675

Epoch: 6| Step: 1
Training loss: 3.3538808822631836
Validation loss: 2.5673097538691696

Epoch: 6| Step: 2
Training loss: 2.851261854171753
Validation loss: 2.562923518560266

Epoch: 6| Step: 3
Training loss: 2.924525260925293
Validation loss: 2.5665622757327173

Epoch: 6| Step: 4
Training loss: 2.186269998550415
Validation loss: 2.559521626400691

Epoch: 6| Step: 5
Training loss: 2.105302095413208
Validation loss: 2.5607994192390033

Epoch: 6| Step: 6
Training loss: 2.9727869033813477
Validation loss: 2.5626216152662873

Epoch: 6| Step: 7
Training loss: 2.734858989715576
Validation loss: 2.5602117815325336

Epoch: 6| Step: 8
Training loss: 2.758737802505493
Validation loss: 2.563271463558238

Epoch: 6| Step: 9
Training loss: 2.184020519256592
Validation loss: 2.5645122810076644

Epoch: 6| Step: 10
Training loss: 3.5931243896484375
Validation loss: 2.57674120062141

Epoch: 6| Step: 11
Training loss: 2.2940449714660645
Validation loss: 2.573256013213947

Epoch: 6| Step: 12
Training loss: 2.6174561977386475
Validation loss: 2.5767401495287494

Epoch: 6| Step: 13
Training loss: 2.391172170639038
Validation loss: 2.569932499239522

Epoch: 105| Step: 0
Training loss: 2.1022777557373047
Validation loss: 2.5648121654346423

Epoch: 6| Step: 1
Training loss: 2.2129321098327637
Validation loss: 2.5628609452196347

Epoch: 6| Step: 2
Training loss: 3.094738483428955
Validation loss: 2.5641716308491205

Epoch: 6| Step: 3
Training loss: 2.439836025238037
Validation loss: 2.5661446279095066

Epoch: 6| Step: 4
Training loss: 2.3589062690734863
Validation loss: 2.5680925923009075

Epoch: 6| Step: 5
Training loss: 2.065873622894287
Validation loss: 2.5687068867427048

Epoch: 6| Step: 6
Training loss: 2.767699718475342
Validation loss: 2.5735576768075266

Epoch: 6| Step: 7
Training loss: 2.120142936706543
Validation loss: 2.5780059701652935

Epoch: 6| Step: 8
Training loss: 3.51114821434021
Validation loss: 2.5848371803119616

Epoch: 6| Step: 9
Training loss: 3.6265244483947754
Validation loss: 2.578872655027656

Epoch: 6| Step: 10
Training loss: 2.1282544136047363
Validation loss: 2.5850658339838826

Epoch: 6| Step: 11
Training loss: 3.081974744796753
Validation loss: 2.585240484565817

Epoch: 6| Step: 12
Training loss: 3.2434768676757812
Validation loss: 2.572704335694672

Epoch: 6| Step: 13
Training loss: 4.033050060272217
Validation loss: 2.568739132214618

Epoch: 106| Step: 0
Training loss: 3.364297389984131
Validation loss: 2.5668617551044752

Epoch: 6| Step: 1
Training loss: 1.9649720191955566
Validation loss: 2.56479230747428

Epoch: 6| Step: 2
Training loss: 2.2056984901428223
Validation loss: 2.568746889791181

Epoch: 6| Step: 3
Training loss: 2.863767147064209
Validation loss: 2.566338175086565

Epoch: 6| Step: 4
Training loss: 2.6093809604644775
Validation loss: 2.5693093884375786

Epoch: 6| Step: 5
Training loss: 3.1351113319396973
Validation loss: 2.5708172859684115

Epoch: 6| Step: 6
Training loss: 3.069655656814575
Validation loss: 2.5720761488842707

Epoch: 6| Step: 7
Training loss: 2.3291783332824707
Validation loss: 2.5675248510094097

Epoch: 6| Step: 8
Training loss: 3.166584014892578
Validation loss: 2.565538555063227

Epoch: 6| Step: 9
Training loss: 2.064647912979126
Validation loss: 2.5641206849005913

Epoch: 6| Step: 10
Training loss: 3.1449522972106934
Validation loss: 2.5705103771660918

Epoch: 6| Step: 11
Training loss: 3.327526092529297
Validation loss: 2.5733179482080604

Epoch: 6| Step: 12
Training loss: 2.906862258911133
Validation loss: 2.5786192776054464

Epoch: 6| Step: 13
Training loss: 1.58681058883667
Validation loss: 2.5762663349028556

Epoch: 107| Step: 0
Training loss: 2.960111379623413
Validation loss: 2.578409856365573

Epoch: 6| Step: 1
Training loss: 2.2057175636291504
Validation loss: 2.5826827223582933

Epoch: 6| Step: 2
Training loss: 2.924929618835449
Validation loss: 2.572191958786339

Epoch: 6| Step: 3
Training loss: 2.3280882835388184
Validation loss: 2.584828663897771

Epoch: 6| Step: 4
Training loss: 3.618861675262451
Validation loss: 2.5914133107790382

Epoch: 6| Step: 5
Training loss: 2.4456663131713867
Validation loss: 2.5834807221607496

Epoch: 6| Step: 6
Training loss: 2.487356424331665
Validation loss: 2.5868764846555647

Epoch: 6| Step: 7
Training loss: 3.4192609786987305
Validation loss: 2.5878251265454035

Epoch: 6| Step: 8
Training loss: 3.124636173248291
Validation loss: 2.584187869102724

Epoch: 6| Step: 9
Training loss: 1.7581274509429932
Validation loss: 2.5792428575536257

Epoch: 6| Step: 10
Training loss: 2.5044384002685547
Validation loss: 2.584557610173379

Epoch: 6| Step: 11
Training loss: 3.0629143714904785
Validation loss: 2.5666852099921114

Epoch: 6| Step: 12
Training loss: 2.5004680156707764
Validation loss: 2.569682308422622

Epoch: 6| Step: 13
Training loss: 2.8428924083709717
Validation loss: 2.568186254911525

Epoch: 108| Step: 0
Training loss: 2.087575912475586
Validation loss: 2.562383985006681

Epoch: 6| Step: 1
Training loss: 3.4416933059692383
Validation loss: 2.5633933287794872

Epoch: 6| Step: 2
Training loss: 2.4476852416992188
Validation loss: 2.561823075817477

Epoch: 6| Step: 3
Training loss: 3.173159122467041
Validation loss: 2.569570387563398

Epoch: 6| Step: 4
Training loss: 2.8814289569854736
Validation loss: 2.5799211891748572

Epoch: 6| Step: 5
Training loss: 1.861436367034912
Validation loss: 2.578149708368445

Epoch: 6| Step: 6
Training loss: 3.0169215202331543
Validation loss: 2.577303163466915

Epoch: 6| Step: 7
Training loss: 3.1575989723205566
Validation loss: 2.5682198232220066

Epoch: 6| Step: 8
Training loss: 2.9725921154022217
Validation loss: 2.5687277214501494

Epoch: 6| Step: 9
Training loss: 2.550816059112549
Validation loss: 2.561653765298987

Epoch: 6| Step: 10
Training loss: 2.424276828765869
Validation loss: 2.553254476157568

Epoch: 6| Step: 11
Training loss: 2.951810359954834
Validation loss: 2.5517777217331754

Epoch: 6| Step: 12
Training loss: 2.6859865188598633
Validation loss: 2.5590580432645735

Epoch: 6| Step: 13
Training loss: 2.3501741886138916
Validation loss: 2.5624024791102253

Epoch: 109| Step: 0
Training loss: 2.451296329498291
Validation loss: 2.5581361145101567

Epoch: 6| Step: 1
Training loss: 2.9716179370880127
Validation loss: 2.5577804888448408

Epoch: 6| Step: 2
Training loss: 3.0226423740386963
Validation loss: 2.5650028464614705

Epoch: 6| Step: 3
Training loss: 2.7181684970855713
Validation loss: 2.5555688924686883

Epoch: 6| Step: 4
Training loss: 3.1232709884643555
Validation loss: 2.5548451203171925

Epoch: 6| Step: 5
Training loss: 2.728665828704834
Validation loss: 2.5572992012064946

Epoch: 6| Step: 6
Training loss: 2.1528775691986084
Validation loss: 2.551351870259931

Epoch: 6| Step: 7
Training loss: 3.266500949859619
Validation loss: 2.550727503274077

Epoch: 6| Step: 8
Training loss: 3.0914649963378906
Validation loss: 2.5475277746877363

Epoch: 6| Step: 9
Training loss: 2.7230851650238037
Validation loss: 2.54771525116377

Epoch: 6| Step: 10
Training loss: 1.7572252750396729
Validation loss: 2.5481789163363877

Epoch: 6| Step: 11
Training loss: 3.3500607013702393
Validation loss: 2.5481454531351724

Epoch: 6| Step: 12
Training loss: 2.0426077842712402
Validation loss: 2.549594686877343

Epoch: 6| Step: 13
Training loss: 2.7122697830200195
Validation loss: 2.555066657322709

Epoch: 110| Step: 0
Training loss: 3.593148708343506
Validation loss: 2.5590917500116492

Epoch: 6| Step: 1
Training loss: 3.2392139434814453
Validation loss: 2.5493930949959704

Epoch: 6| Step: 2
Training loss: 2.482598304748535
Validation loss: 2.5535263861379316

Epoch: 6| Step: 3
Training loss: 2.1093344688415527
Validation loss: 2.5494115711540304

Epoch: 6| Step: 4
Training loss: 3.3499813079833984
Validation loss: 2.5471748921179

Epoch: 6| Step: 5
Training loss: 2.823528289794922
Validation loss: 2.5491636465954524

Epoch: 6| Step: 6
Training loss: 1.6178840398788452
Validation loss: 2.550362471611269

Epoch: 6| Step: 7
Training loss: 2.435213327407837
Validation loss: 2.551039893140075

Epoch: 6| Step: 8
Training loss: 2.4001824855804443
Validation loss: 2.5524554380806546

Epoch: 6| Step: 9
Training loss: 3.1651487350463867
Validation loss: 2.567207444098688

Epoch: 6| Step: 10
Training loss: 2.6710500717163086
Validation loss: 2.579597293689687

Epoch: 6| Step: 11
Training loss: 2.6447606086730957
Validation loss: 2.5817803439273628

Epoch: 6| Step: 12
Training loss: 2.9323031902313232
Validation loss: 2.5873914046954085

Epoch: 6| Step: 13
Training loss: 2.415848731994629
Validation loss: 2.5898715706281763

Epoch: 111| Step: 0
Training loss: 2.907702922821045
Validation loss: 2.591365985972907

Epoch: 6| Step: 1
Training loss: 2.569368839263916
Validation loss: 2.599879933941749

Epoch: 6| Step: 2
Training loss: 2.7669808864593506
Validation loss: 2.5897206901222147

Epoch: 6| Step: 3
Training loss: 3.3988196849823
Validation loss: 2.588601691748506

Epoch: 6| Step: 4
Training loss: 2.340014934539795
Validation loss: 2.5944898102873113

Epoch: 6| Step: 5
Training loss: 3.284529209136963
Validation loss: 2.604595343271891

Epoch: 6| Step: 6
Training loss: 2.3532252311706543
Validation loss: 2.59239960229525

Epoch: 6| Step: 7
Training loss: 2.8642115592956543
Validation loss: 2.5897667100352626

Epoch: 6| Step: 8
Training loss: 3.277592658996582
Validation loss: 2.582056119877805

Epoch: 6| Step: 9
Training loss: 2.064181089401245
Validation loss: 2.5688930531983734

Epoch: 6| Step: 10
Training loss: 1.755506992340088
Validation loss: 2.5547485659199376

Epoch: 6| Step: 11
Training loss: 3.054630756378174
Validation loss: 2.552943798803514

Epoch: 6| Step: 12
Training loss: 2.751720905303955
Validation loss: 2.5469455385720856

Epoch: 6| Step: 13
Training loss: 2.6740903854370117
Validation loss: 2.560641986067577

Epoch: 112| Step: 0
Training loss: 2.7536940574645996
Validation loss: 2.5624523316660235

Epoch: 6| Step: 1
Training loss: 3.360375165939331
Validation loss: 2.5732900096524145

Epoch: 6| Step: 2
Training loss: 2.989957094192505
Validation loss: 2.573004840522684

Epoch: 6| Step: 3
Training loss: 2.6256096363067627
Validation loss: 2.5790823454497964

Epoch: 6| Step: 4
Training loss: 2.6028988361358643
Validation loss: 2.572630807917605

Epoch: 6| Step: 5
Training loss: 3.035989284515381
Validation loss: 2.566306714088686

Epoch: 6| Step: 6
Training loss: 3.1033401489257812
Validation loss: 2.5659098855910765

Epoch: 6| Step: 7
Training loss: 2.540858507156372
Validation loss: 2.552703365202873

Epoch: 6| Step: 8
Training loss: 2.5237414836883545
Validation loss: 2.54909251582238

Epoch: 6| Step: 9
Training loss: 2.0351595878601074
Validation loss: 2.54418150583903

Epoch: 6| Step: 10
Training loss: 2.782048225402832
Validation loss: 2.5400902250761628

Epoch: 6| Step: 11
Training loss: 1.8928087949752808
Validation loss: 2.537096077396024

Epoch: 6| Step: 12
Training loss: 3.2300350666046143
Validation loss: 2.5389368252087663

Epoch: 6| Step: 13
Training loss: 2.5654900074005127
Validation loss: 2.548398079410676

Epoch: 113| Step: 0
Training loss: 2.482041835784912
Validation loss: 2.5477931986572924

Epoch: 6| Step: 1
Training loss: 2.0491561889648438
Validation loss: 2.553271858922897

Epoch: 6| Step: 2
Training loss: 3.3109731674194336
Validation loss: 2.5625428076713317

Epoch: 6| Step: 3
Training loss: 2.9983291625976562
Validation loss: 2.567468512442804

Epoch: 6| Step: 4
Training loss: 2.5242104530334473
Validation loss: 2.5670001558078233

Epoch: 6| Step: 5
Training loss: 2.134977340698242
Validation loss: 2.5622667125476304

Epoch: 6| Step: 6
Training loss: 2.7559447288513184
Validation loss: 2.5512009461720786

Epoch: 6| Step: 7
Training loss: 4.253120422363281
Validation loss: 2.5429197408819713

Epoch: 6| Step: 8
Training loss: 2.449084520339966
Validation loss: 2.5422199592795423

Epoch: 6| Step: 9
Training loss: 2.207954168319702
Validation loss: 2.534264067167877

Epoch: 6| Step: 10
Training loss: 2.4197163581848145
Validation loss: 2.5334364560342606

Epoch: 6| Step: 11
Training loss: 2.560793876647949
Validation loss: 2.53268769479567

Epoch: 6| Step: 12
Training loss: 3.2750585079193115
Validation loss: 2.5429980703579482

Epoch: 6| Step: 13
Training loss: 2.688373565673828
Validation loss: 2.535262051449027

Epoch: 114| Step: 0
Training loss: 2.223479747772217
Validation loss: 2.5353067433962257

Epoch: 6| Step: 1
Training loss: 3.4900574684143066
Validation loss: 2.5341428992568806

Epoch: 6| Step: 2
Training loss: 2.6861581802368164
Validation loss: 2.5342166936525734

Epoch: 6| Step: 3
Training loss: 2.886678695678711
Validation loss: 2.5298781753868185

Epoch: 6| Step: 4
Training loss: 3.381688356399536
Validation loss: 2.5318985421170472

Epoch: 6| Step: 5
Training loss: 2.3232247829437256
Validation loss: 2.532025711510771

Epoch: 6| Step: 6
Training loss: 3.2074742317199707
Validation loss: 2.5301997610317764

Epoch: 6| Step: 7
Training loss: 2.263148784637451
Validation loss: 2.530666241081812

Epoch: 6| Step: 8
Training loss: 2.761265754699707
Validation loss: 2.5335224366957143

Epoch: 6| Step: 9
Training loss: 2.3711729049682617
Validation loss: 2.5273879881828063

Epoch: 6| Step: 10
Training loss: 2.028806209564209
Validation loss: 2.5314983091046734

Epoch: 6| Step: 11
Training loss: 3.0600247383117676
Validation loss: 2.5280772280949417

Epoch: 6| Step: 12
Training loss: 2.2789571285247803
Validation loss: 2.5351421345946608

Epoch: 6| Step: 13
Training loss: 3.204089641571045
Validation loss: 2.534012604785222

Epoch: 115| Step: 0
Training loss: 3.0438060760498047
Validation loss: 2.5282467949774956

Epoch: 6| Step: 1
Training loss: 2.6450235843658447
Validation loss: 2.5278077151185725

Epoch: 6| Step: 2
Training loss: 3.397625207901001
Validation loss: 2.5246922175089517

Epoch: 6| Step: 3
Training loss: 2.6361594200134277
Validation loss: 2.529049322169314

Epoch: 6| Step: 4
Training loss: 3.1340084075927734
Validation loss: 2.5253317227927585

Epoch: 6| Step: 5
Training loss: 2.756589889526367
Validation loss: 2.526848477701987

Epoch: 6| Step: 6
Training loss: 2.3979406356811523
Validation loss: 2.524673254259171

Epoch: 6| Step: 7
Training loss: 2.370938539505005
Validation loss: 2.53403962555752

Epoch: 6| Step: 8
Training loss: 2.2591071128845215
Validation loss: 2.5269011066805933

Epoch: 6| Step: 9
Training loss: 3.3969664573669434
Validation loss: 2.5364226730920936

Epoch: 6| Step: 10
Training loss: 2.477221965789795
Validation loss: 2.5357428391774497

Epoch: 6| Step: 11
Training loss: 2.3086163997650146
Validation loss: 2.534269432867727

Epoch: 6| Step: 12
Training loss: 2.492069721221924
Validation loss: 2.5407246389696674

Epoch: 6| Step: 13
Training loss: 2.4479665756225586
Validation loss: 2.538602490578928

Epoch: 116| Step: 0
Training loss: 2.788034677505493
Validation loss: 2.5343645259898198

Epoch: 6| Step: 1
Training loss: 2.632244110107422
Validation loss: 2.531826549960721

Epoch: 6| Step: 2
Training loss: 2.735412120819092
Validation loss: 2.5328424874172417

Epoch: 6| Step: 3
Training loss: 2.0697240829467773
Validation loss: 2.543371259525258

Epoch: 6| Step: 4
Training loss: 2.4697866439819336
Validation loss: 2.549795719885057

Epoch: 6| Step: 5
Training loss: 2.2524492740631104
Validation loss: 2.553508504744499

Epoch: 6| Step: 6
Training loss: 2.5372366905212402
Validation loss: 2.5457777489898024

Epoch: 6| Step: 7
Training loss: 2.670431613922119
Validation loss: 2.535211650274133

Epoch: 6| Step: 8
Training loss: 2.427856683731079
Validation loss: 2.5334413256696475

Epoch: 6| Step: 9
Training loss: 2.5072715282440186
Validation loss: 2.521444495006274

Epoch: 6| Step: 10
Training loss: 3.923374891281128
Validation loss: 2.5199270761141213

Epoch: 6| Step: 11
Training loss: 2.8712058067321777
Validation loss: 2.520732702747468

Epoch: 6| Step: 12
Training loss: 2.9799935817718506
Validation loss: 2.518335232170679

Epoch: 6| Step: 13
Training loss: 3.131758689880371
Validation loss: 2.5201607083761566

Epoch: 117| Step: 0
Training loss: 2.890918254852295
Validation loss: 2.519217347586027

Epoch: 6| Step: 1
Training loss: 2.533510208129883
Validation loss: 2.5236934590083298

Epoch: 6| Step: 2
Training loss: 2.2080721855163574
Validation loss: 2.5251191918567946

Epoch: 6| Step: 3
Training loss: 2.7579398155212402
Validation loss: 2.5264316399892173

Epoch: 6| Step: 4
Training loss: 2.625218391418457
Validation loss: 2.5273482338074715

Epoch: 6| Step: 5
Training loss: 2.671560287475586
Validation loss: 2.5247905946546987

Epoch: 6| Step: 6
Training loss: 2.496492862701416
Validation loss: 2.524815010768111

Epoch: 6| Step: 7
Training loss: 2.2209815979003906
Validation loss: 2.515836541370679

Epoch: 6| Step: 8
Training loss: 3.2946739196777344
Validation loss: 2.5222189170058056

Epoch: 6| Step: 9
Training loss: 2.9226267337799072
Validation loss: 2.5244697088836343

Epoch: 6| Step: 10
Training loss: 2.260852813720703
Validation loss: 2.5192601603846394

Epoch: 6| Step: 11
Training loss: 2.666846752166748
Validation loss: 2.5242825272262737

Epoch: 6| Step: 12
Training loss: 3.4869472980499268
Validation loss: 2.5271646207378757

Epoch: 6| Step: 13
Training loss: 2.7977867126464844
Validation loss: 2.5255421028342298

Epoch: 118| Step: 0
Training loss: 3.1327900886535645
Validation loss: 2.525946763253981

Epoch: 6| Step: 1
Training loss: 2.4140126705169678
Validation loss: 2.528634314895958

Epoch: 6| Step: 2
Training loss: 2.926449775695801
Validation loss: 2.5250394421239055

Epoch: 6| Step: 3
Training loss: 2.3633718490600586
Validation loss: 2.525062196998186

Epoch: 6| Step: 4
Training loss: 3.406400442123413
Validation loss: 2.526195946560111

Epoch: 6| Step: 5
Training loss: 3.013692855834961
Validation loss: 2.526793679883403

Epoch: 6| Step: 6
Training loss: 1.8944040536880493
Validation loss: 2.5295728944963023

Epoch: 6| Step: 7
Training loss: 2.9470877647399902
Validation loss: 2.529564916446645

Epoch: 6| Step: 8
Training loss: 2.2439823150634766
Validation loss: 2.5442113927615586

Epoch: 6| Step: 9
Training loss: 2.760493755340576
Validation loss: 2.539385280301494

Epoch: 6| Step: 10
Training loss: 2.644946336746216
Validation loss: 2.5323270879765993

Epoch: 6| Step: 11
Training loss: 2.835956335067749
Validation loss: 2.530012184573758

Epoch: 6| Step: 12
Training loss: 2.6236701011657715
Validation loss: 2.535313521662066

Epoch: 6| Step: 13
Training loss: 2.3778908252716064
Validation loss: 2.5374205958458687

Epoch: 119| Step: 0
Training loss: 2.3306660652160645
Validation loss: 2.53479068509994

Epoch: 6| Step: 1
Training loss: 3.045767068862915
Validation loss: 2.540097403269942

Epoch: 6| Step: 2
Training loss: 2.6890454292297363
Validation loss: 2.543116613100934

Epoch: 6| Step: 3
Training loss: 2.6551952362060547
Validation loss: 2.5435361041817615

Epoch: 6| Step: 4
Training loss: 2.3824477195739746
Validation loss: 2.5409661146902267

Epoch: 6| Step: 5
Training loss: 2.0053796768188477
Validation loss: 2.534877433571764

Epoch: 6| Step: 6
Training loss: 2.3239922523498535
Validation loss: 2.547530525474138

Epoch: 6| Step: 7
Training loss: 2.973111629486084
Validation loss: 2.5379364464872625

Epoch: 6| Step: 8
Training loss: 3.2891407012939453
Validation loss: 2.545252938424387

Epoch: 6| Step: 9
Training loss: 2.4140028953552246
Validation loss: 2.5549040814881683

Epoch: 6| Step: 10
Training loss: 2.7255213260650635
Validation loss: 2.534043847873647

Epoch: 6| Step: 11
Training loss: 2.9211792945861816
Validation loss: 2.5194965793240454

Epoch: 6| Step: 12
Training loss: 3.7908639907836914
Validation loss: 2.5208217687504266

Epoch: 6| Step: 13
Training loss: 1.8395930528640747
Validation loss: 2.5308218002319336

Epoch: 120| Step: 0
Training loss: 3.2929468154907227
Validation loss: 2.5243551269654305

Epoch: 6| Step: 1
Training loss: 2.3309741020202637
Validation loss: 2.5229104590672318

Epoch: 6| Step: 2
Training loss: 2.830843925476074
Validation loss: 2.5330516881840204

Epoch: 6| Step: 3
Training loss: 2.1367945671081543
Validation loss: 2.5350950020615772

Epoch: 6| Step: 4
Training loss: 2.6195359230041504
Validation loss: 2.54704906863551

Epoch: 6| Step: 5
Training loss: 2.248760223388672
Validation loss: 2.5383783642963698

Epoch: 6| Step: 6
Training loss: 2.6708478927612305
Validation loss: 2.5477903683980307

Epoch: 6| Step: 7
Training loss: 2.750837802886963
Validation loss: 2.554329901613215

Epoch: 6| Step: 8
Training loss: 2.9172921180725098
Validation loss: 2.5538305133901615

Epoch: 6| Step: 9
Training loss: 2.378918170928955
Validation loss: 2.5523466576812086

Epoch: 6| Step: 10
Training loss: 3.161499500274658
Validation loss: 2.533061706891624

Epoch: 6| Step: 11
Training loss: 3.38970685005188
Validation loss: 2.534465107866513

Epoch: 6| Step: 12
Training loss: 2.003861904144287
Validation loss: 2.535151950774654

Epoch: 6| Step: 13
Training loss: 3.0091419219970703
Validation loss: 2.538883824502268

Epoch: 121| Step: 0
Training loss: 2.363755702972412
Validation loss: 2.5442157765870452

Epoch: 6| Step: 1
Training loss: 3.1735000610351562
Validation loss: 2.5406219754167783

Epoch: 6| Step: 2
Training loss: 2.6784329414367676
Validation loss: 2.541215265950849

Epoch: 6| Step: 3
Training loss: 1.7082483768463135
Validation loss: 2.5304196316708802

Epoch: 6| Step: 4
Training loss: 2.9041717052459717
Validation loss: 2.5340356006417224

Epoch: 6| Step: 5
Training loss: 3.0899713039398193
Validation loss: 2.5340003480193434

Epoch: 6| Step: 6
Training loss: 3.7481770515441895
Validation loss: 2.5367323557535806

Epoch: 6| Step: 7
Training loss: 3.2966983318328857
Validation loss: 2.5305160527588217

Epoch: 6| Step: 8
Training loss: 2.0664772987365723
Validation loss: 2.5308984556505756

Epoch: 6| Step: 9
Training loss: 1.9352024793624878
Validation loss: 2.5254522600481586

Epoch: 6| Step: 10
Training loss: 2.260540008544922
Validation loss: 2.526351531346639

Epoch: 6| Step: 11
Training loss: 3.017425537109375
Validation loss: 2.5319590376269434

Epoch: 6| Step: 12
Training loss: 2.93707275390625
Validation loss: 2.5235208542116228

Epoch: 6| Step: 13
Training loss: 2.4045395851135254
Validation loss: 2.5196255714662614

Epoch: 122| Step: 0
Training loss: 2.81904935836792
Validation loss: 2.5163704938786005

Epoch: 6| Step: 1
Training loss: 2.9534990787506104
Validation loss: 2.5232292964894283

Epoch: 6| Step: 2
Training loss: 2.0309500694274902
Validation loss: 2.521185364774478

Epoch: 6| Step: 3
Training loss: 2.1586010456085205
Validation loss: 2.5164797639334076

Epoch: 6| Step: 4
Training loss: 2.4917359352111816
Validation loss: 2.5221205629328245

Epoch: 6| Step: 5
Training loss: 2.6453280448913574
Validation loss: 2.531555744909471

Epoch: 6| Step: 6
Training loss: 2.738844156265259
Validation loss: 2.530573001471899

Epoch: 6| Step: 7
Training loss: 2.934762716293335
Validation loss: 2.539153652806436

Epoch: 6| Step: 8
Training loss: 3.0801327228546143
Validation loss: 2.5214702544673795

Epoch: 6| Step: 9
Training loss: 2.593825101852417
Validation loss: 2.522389755454115

Epoch: 6| Step: 10
Training loss: 2.72104811668396
Validation loss: 2.5118953784306846

Epoch: 6| Step: 11
Training loss: 3.196932554244995
Validation loss: 2.5174103270294848

Epoch: 6| Step: 12
Training loss: 2.8354744911193848
Validation loss: 2.518493765143938

Epoch: 6| Step: 13
Training loss: 2.2923836708068848
Validation loss: 2.5233332290444324

Epoch: 123| Step: 0
Training loss: 3.0502002239227295
Validation loss: 2.5243170492110716

Epoch: 6| Step: 1
Training loss: 3.0249130725860596
Validation loss: 2.5289722052953576

Epoch: 6| Step: 2
Training loss: 2.905738353729248
Validation loss: 2.5289513603333504

Epoch: 6| Step: 3
Training loss: 3.081291437149048
Validation loss: 2.527719710462837

Epoch: 6| Step: 4
Training loss: 2.2544620037078857
Validation loss: 2.5275488130507933

Epoch: 6| Step: 5
Training loss: 2.2849886417388916
Validation loss: 2.5238915745930006

Epoch: 6| Step: 6
Training loss: 2.7184066772460938
Validation loss: 2.518648857711464

Epoch: 6| Step: 7
Training loss: 3.192401647567749
Validation loss: 2.5158905188242593

Epoch: 6| Step: 8
Training loss: 2.6727542877197266
Validation loss: 2.5207676477329706

Epoch: 6| Step: 9
Training loss: 2.051037311553955
Validation loss: 2.5232552636054253

Epoch: 6| Step: 10
Training loss: 3.471323013305664
Validation loss: 2.529126690280053

Epoch: 6| Step: 11
Training loss: 1.9276680946350098
Validation loss: 2.529344933007353

Epoch: 6| Step: 12
Training loss: 2.6334710121154785
Validation loss: 2.537394005765197

Epoch: 6| Step: 13
Training loss: 2.2191107273101807
Validation loss: 2.5319380183373728

Epoch: 124| Step: 0
Training loss: 3.006052255630493
Validation loss: 2.5371956286891812

Epoch: 6| Step: 1
Training loss: 2.2401881217956543
Validation loss: 2.5336073906190935

Epoch: 6| Step: 2
Training loss: 2.472486734390259
Validation loss: 2.516728365293113

Epoch: 6| Step: 3
Training loss: 3.1471662521362305
Validation loss: 2.519694220635199

Epoch: 6| Step: 4
Training loss: 2.217684507369995
Validation loss: 2.5146519137967016

Epoch: 6| Step: 5
Training loss: 2.3966755867004395
Validation loss: 2.5156721684240524

Epoch: 6| Step: 6
Training loss: 2.610821008682251
Validation loss: 2.5132964170107277

Epoch: 6| Step: 7
Training loss: 2.0007176399230957
Validation loss: 2.5112400311295704

Epoch: 6| Step: 8
Training loss: 2.742544174194336
Validation loss: 2.513404628281952

Epoch: 6| Step: 9
Training loss: 2.6943607330322266
Validation loss: 2.5099510300544

Epoch: 6| Step: 10
Training loss: 3.3871288299560547
Validation loss: 2.511510569562194

Epoch: 6| Step: 11
Training loss: 2.056603193283081
Validation loss: 2.510719668480658

Epoch: 6| Step: 12
Training loss: 4.018569469451904
Validation loss: 2.5219802420626403

Epoch: 6| Step: 13
Training loss: 2.5736398696899414
Validation loss: 2.5210055638385076

Epoch: 125| Step: 0
Training loss: 2.4595720767974854
Validation loss: 2.5342807641593357

Epoch: 6| Step: 1
Training loss: 3.230177879333496
Validation loss: 2.536611439079367

Epoch: 6| Step: 2
Training loss: 2.2607970237731934
Validation loss: 2.54725125528151

Epoch: 6| Step: 3
Training loss: 2.4839911460876465
Validation loss: 2.5369457275636735

Epoch: 6| Step: 4
Training loss: 3.773618221282959
Validation loss: 2.53258240607477

Epoch: 6| Step: 5
Training loss: 2.8627398014068604
Validation loss: 2.5305369797573296

Epoch: 6| Step: 6
Training loss: 2.7964954376220703
Validation loss: 2.525687474076466

Epoch: 6| Step: 7
Training loss: 2.6553139686584473
Validation loss: 2.511763095855713

Epoch: 6| Step: 8
Training loss: 2.8388986587524414
Validation loss: 2.5050491466317126

Epoch: 6| Step: 9
Training loss: 2.7220351696014404
Validation loss: 2.5098076738337034

Epoch: 6| Step: 10
Training loss: 2.5816097259521484
Validation loss: 2.511968920307775

Epoch: 6| Step: 11
Training loss: 1.8018815517425537
Validation loss: 2.5102924582778767

Epoch: 6| Step: 12
Training loss: 2.528273105621338
Validation loss: 2.5115556127281597

Epoch: 6| Step: 13
Training loss: 2.440922260284424
Validation loss: 2.511637089073017

Epoch: 126| Step: 0
Training loss: 2.160449504852295
Validation loss: 2.512380083401998

Epoch: 6| Step: 1
Training loss: 2.938441753387451
Validation loss: 2.517262786947271

Epoch: 6| Step: 2
Training loss: 2.79563045501709
Validation loss: 2.51197616259257

Epoch: 6| Step: 3
Training loss: 2.7230186462402344
Validation loss: 2.5148035121220413

Epoch: 6| Step: 4
Training loss: 2.225252866744995
Validation loss: 2.5154558343272053

Epoch: 6| Step: 5
Training loss: 2.413066864013672
Validation loss: 2.511919134406633

Epoch: 6| Step: 6
Training loss: 2.725245714187622
Validation loss: 2.5161279862926853

Epoch: 6| Step: 7
Training loss: 2.6313586235046387
Validation loss: 2.512441283913069

Epoch: 6| Step: 8
Training loss: 2.286803722381592
Validation loss: 2.516282381549958

Epoch: 6| Step: 9
Training loss: 2.74615478515625
Validation loss: 2.5181272055513118

Epoch: 6| Step: 10
Training loss: 3.2721004486083984
Validation loss: 2.511302504488217

Epoch: 6| Step: 11
Training loss: 2.554844856262207
Validation loss: 2.5106599484720538

Epoch: 6| Step: 12
Training loss: 3.3978772163391113
Validation loss: 2.50875445078778

Epoch: 6| Step: 13
Training loss: 2.655358076095581
Validation loss: 2.506696113976099

Epoch: 127| Step: 0
Training loss: 2.853849411010742
Validation loss: 2.5046230003397953

Epoch: 6| Step: 1
Training loss: 1.8585178852081299
Validation loss: 2.5060319310875347

Epoch: 6| Step: 2
Training loss: 2.2321982383728027
Validation loss: 2.5121058802450857

Epoch: 6| Step: 3
Training loss: 3.4175596237182617
Validation loss: 2.520695601740191

Epoch: 6| Step: 4
Training loss: 2.1239354610443115
Validation loss: 2.519605957051759

Epoch: 6| Step: 5
Training loss: 2.4587173461914062
Validation loss: 2.5132517353180917

Epoch: 6| Step: 6
Training loss: 2.9080393314361572
Validation loss: 2.524358782716977

Epoch: 6| Step: 7
Training loss: 2.731396198272705
Validation loss: 2.523984843684781

Epoch: 6| Step: 8
Training loss: 2.8168134689331055
Validation loss: 2.5195344263507473

Epoch: 6| Step: 9
Training loss: 2.9720516204833984
Validation loss: 2.51279023385817

Epoch: 6| Step: 10
Training loss: 2.7771944999694824
Validation loss: 2.5097670632023967

Epoch: 6| Step: 11
Training loss: 2.630352020263672
Validation loss: 2.514672174248644

Epoch: 6| Step: 12
Training loss: 2.9005205631256104
Validation loss: 2.5035386162419475

Epoch: 6| Step: 13
Training loss: 3.192582130432129
Validation loss: 2.5077156405295096

Epoch: 128| Step: 0
Training loss: 2.8940024375915527
Validation loss: 2.5066043894778014

Epoch: 6| Step: 1
Training loss: 2.4953370094299316
Validation loss: 2.5035588023483113

Epoch: 6| Step: 2
Training loss: 3.0050148963928223
Validation loss: 2.5017071257355394

Epoch: 6| Step: 3
Training loss: 3.053940534591675
Validation loss: 2.4944497718605945

Epoch: 6| Step: 4
Training loss: 2.537055730819702
Validation loss: 2.494648600137362

Epoch: 6| Step: 5
Training loss: 2.7044899463653564
Validation loss: 2.500653612998224

Epoch: 6| Step: 6
Training loss: 1.946748971939087
Validation loss: 2.499533022603681

Epoch: 6| Step: 7
Training loss: 2.3450169563293457
Validation loss: 2.5016130529424196

Epoch: 6| Step: 8
Training loss: 2.5587449073791504
Validation loss: 2.5035158690585884

Epoch: 6| Step: 9
Training loss: 3.4219741821289062
Validation loss: 2.5035840824086177

Epoch: 6| Step: 10
Training loss: 2.197805404663086
Validation loss: 2.5049309089619625

Epoch: 6| Step: 11
Training loss: 2.9454474449157715
Validation loss: 2.5025024362789687

Epoch: 6| Step: 12
Training loss: 2.6790106296539307
Validation loss: 2.5022103555740847

Epoch: 6| Step: 13
Training loss: 3.020205497741699
Validation loss: 2.5013679048066497

Epoch: 129| Step: 0
Training loss: 3.411078929901123
Validation loss: 2.5035734817545903

Epoch: 6| Step: 1
Training loss: 3.4179697036743164
Validation loss: 2.4994150861617057

Epoch: 6| Step: 2
Training loss: 2.54655122756958
Validation loss: 2.5008769560885686

Epoch: 6| Step: 3
Training loss: 3.118927478790283
Validation loss: 2.501092269856443

Epoch: 6| Step: 4
Training loss: 2.0264415740966797
Validation loss: 2.4972024451019945

Epoch: 6| Step: 5
Training loss: 1.9466629028320312
Validation loss: 2.500412607705721

Epoch: 6| Step: 6
Training loss: 2.568918228149414
Validation loss: 2.5073680262411795

Epoch: 6| Step: 7
Training loss: 2.4428327083587646
Validation loss: 2.508116683652324

Epoch: 6| Step: 8
Training loss: 2.5416908264160156
Validation loss: 2.5042089262316303

Epoch: 6| Step: 9
Training loss: 2.846803665161133
Validation loss: 2.5046079902238745

Epoch: 6| Step: 10
Training loss: 2.529682159423828
Validation loss: 2.5091004679279942

Epoch: 6| Step: 11
Training loss: 2.7192304134368896
Validation loss: 2.5072932756075295

Epoch: 6| Step: 12
Training loss: 2.8208365440368652
Validation loss: 2.514464173265683

Epoch: 6| Step: 13
Training loss: 2.3553454875946045
Validation loss: 2.5100497122733825

Epoch: 130| Step: 0
Training loss: 2.296964645385742
Validation loss: 2.5092177801234747

Epoch: 6| Step: 1
Training loss: 2.77135968208313
Validation loss: 2.510076504881664

Epoch: 6| Step: 2
Training loss: 3.2234325408935547
Validation loss: 2.5138346354166665

Epoch: 6| Step: 3
Training loss: 2.2466342449188232
Validation loss: 2.507117376532606

Epoch: 6| Step: 4
Training loss: 2.485513210296631
Validation loss: 2.5118898063577633

Epoch: 6| Step: 5
Training loss: 2.846303939819336
Validation loss: 2.5042347779837986

Epoch: 6| Step: 6
Training loss: 3.413442373275757
Validation loss: 2.506061456536734

Epoch: 6| Step: 7
Training loss: 2.2589197158813477
Validation loss: 2.5018895979850524

Epoch: 6| Step: 8
Training loss: 2.185129404067993
Validation loss: 2.5004533362644974

Epoch: 6| Step: 9
Training loss: 1.8653490543365479
Validation loss: 2.5040868559191303

Epoch: 6| Step: 10
Training loss: 3.2809486389160156
Validation loss: 2.500173891744306

Epoch: 6| Step: 11
Training loss: 2.837923288345337
Validation loss: 2.5000768835826586

Epoch: 6| Step: 12
Training loss: 3.0925281047821045
Validation loss: 2.499949300160972

Epoch: 6| Step: 13
Training loss: 2.6838035583496094
Validation loss: 2.5059363149827525

Epoch: 131| Step: 0
Training loss: 2.9801950454711914
Validation loss: 2.499859450965799

Epoch: 6| Step: 1
Training loss: 2.5370912551879883
Validation loss: 2.5024445390188568

Epoch: 6| Step: 2
Training loss: 3.0600786209106445
Validation loss: 2.4968512699168217

Epoch: 6| Step: 3
Training loss: 2.3458917140960693
Validation loss: 2.500597459013744

Epoch: 6| Step: 4
Training loss: 3.486177921295166
Validation loss: 2.4997044276165705

Epoch: 6| Step: 5
Training loss: 1.9585151672363281
Validation loss: 2.500694490248157

Epoch: 6| Step: 6
Training loss: 3.8585519790649414
Validation loss: 2.493981408816512

Epoch: 6| Step: 7
Training loss: 2.3976991176605225
Validation loss: 2.4932293071541736

Epoch: 6| Step: 8
Training loss: 3.6118521690368652
Validation loss: 2.4949056076747116

Epoch: 6| Step: 9
Training loss: 2.5813443660736084
Validation loss: 2.4952369556632092

Epoch: 6| Step: 10
Training loss: 2.2725672721862793
Validation loss: 2.497122326204854

Epoch: 6| Step: 11
Training loss: 1.59916353225708
Validation loss: 2.496398277180169

Epoch: 6| Step: 12
Training loss: 2.1247644424438477
Validation loss: 2.4972678692110124

Epoch: 6| Step: 13
Training loss: 2.729161262512207
Validation loss: 2.495213093296174

Epoch: 132| Step: 0
Training loss: 2.3115320205688477
Validation loss: 2.502172311147054

Epoch: 6| Step: 1
Training loss: 3.128696918487549
Validation loss: 2.503531297047933

Epoch: 6| Step: 2
Training loss: 2.895775318145752
Validation loss: 2.497023336348995

Epoch: 6| Step: 3
Training loss: 2.5070900917053223
Validation loss: 2.506614528676515

Epoch: 6| Step: 4
Training loss: 2.761441230773926
Validation loss: 2.504938446065431

Epoch: 6| Step: 5
Training loss: 2.6563470363616943
Validation loss: 2.5098547602212555

Epoch: 6| Step: 6
Training loss: 2.6012539863586426
Validation loss: 2.507975778272075

Epoch: 6| Step: 7
Training loss: 2.326915740966797
Validation loss: 2.510008845278012

Epoch: 6| Step: 8
Training loss: 3.423527956008911
Validation loss: 2.513769745826721

Epoch: 6| Step: 9
Training loss: 3.026252269744873
Validation loss: 2.512486668043239

Epoch: 6| Step: 10
Training loss: 2.1898787021636963
Validation loss: 2.5037163380653626

Epoch: 6| Step: 11
Training loss: 2.430663824081421
Validation loss: 2.504032006827734

Epoch: 6| Step: 12
Training loss: 2.581935405731201
Validation loss: 2.4972119536451114

Epoch: 6| Step: 13
Training loss: 2.7898213863372803
Validation loss: 2.492900574079124

Epoch: 133| Step: 0
Training loss: 2.7171638011932373
Validation loss: 2.4961547313197965

Epoch: 6| Step: 1
Training loss: 2.595315933227539
Validation loss: 2.4986202357917704

Epoch: 6| Step: 2
Training loss: 3.1115736961364746
Validation loss: 2.4976874884738716

Epoch: 6| Step: 3
Training loss: 2.642796039581299
Validation loss: 2.502939014024632

Epoch: 6| Step: 4
Training loss: 3.064432382583618
Validation loss: 2.5220622349810857

Epoch: 6| Step: 5
Training loss: 2.6899847984313965
Validation loss: 2.517650168429139

Epoch: 6| Step: 6
Training loss: 3.390188217163086
Validation loss: 2.5135177361067904

Epoch: 6| Step: 7
Training loss: 2.5437397956848145
Validation loss: 2.5011181318631737

Epoch: 6| Step: 8
Training loss: 3.0538952350616455
Validation loss: 2.502685728893485

Epoch: 6| Step: 9
Training loss: 2.9335567951202393
Validation loss: 2.4952682090061966

Epoch: 6| Step: 10
Training loss: 1.948486089706421
Validation loss: 2.4899541331875708

Epoch: 6| Step: 11
Training loss: 2.27127742767334
Validation loss: 2.489602745220225

Epoch: 6| Step: 12
Training loss: 2.259695291519165
Validation loss: 2.4853791818823865

Epoch: 6| Step: 13
Training loss: 2.165179967880249
Validation loss: 2.483655014345723

Epoch: 134| Step: 0
Training loss: 1.9890148639678955
Validation loss: 2.491041362926524

Epoch: 6| Step: 1
Training loss: 2.8091139793395996
Validation loss: 2.488519596797164

Epoch: 6| Step: 2
Training loss: 2.2658073902130127
Validation loss: 2.493351459503174

Epoch: 6| Step: 3
Training loss: 2.720942497253418
Validation loss: 2.4918142467416744

Epoch: 6| Step: 4
Training loss: 1.76382577419281
Validation loss: 2.4940307242895967

Epoch: 6| Step: 5
Training loss: 2.688178777694702
Validation loss: 2.4937070518411617

Epoch: 6| Step: 6
Training loss: 2.4000751972198486
Validation loss: 2.491477691999046

Epoch: 6| Step: 7
Training loss: 2.9481489658355713
Validation loss: 2.4900889858122794

Epoch: 6| Step: 8
Training loss: 2.7336933612823486
Validation loss: 2.4924061375279583

Epoch: 6| Step: 9
Training loss: 2.629699230194092
Validation loss: 2.4912389683467087

Epoch: 6| Step: 10
Training loss: 4.035491943359375
Validation loss: 2.487637350636144

Epoch: 6| Step: 11
Training loss: 2.6140942573547363
Validation loss: 2.4946512253053728

Epoch: 6| Step: 12
Training loss: 3.214271306991577
Validation loss: 2.496515650903025

Epoch: 6| Step: 13
Training loss: 2.791249990463257
Validation loss: 2.4963892070196008

Epoch: 135| Step: 0
Training loss: 2.716432571411133
Validation loss: 2.498863879070487

Epoch: 6| Step: 1
Training loss: 3.252901077270508
Validation loss: 2.493089988667478

Epoch: 6| Step: 2
Training loss: 2.239738702774048
Validation loss: 2.494999806086222

Epoch: 6| Step: 3
Training loss: 2.7874951362609863
Validation loss: 2.4928829669952393

Epoch: 6| Step: 4
Training loss: 2.2987868785858154
Validation loss: 2.4878829961181967

Epoch: 6| Step: 5
Training loss: 2.8498568534851074
Validation loss: 2.487842821305798

Epoch: 6| Step: 6
Training loss: 3.371483564376831
Validation loss: 2.4903919594262236

Epoch: 6| Step: 7
Training loss: 3.776528835296631
Validation loss: 2.4948968707874255

Epoch: 6| Step: 8
Training loss: 2.089125156402588
Validation loss: 2.4915658684187036

Epoch: 6| Step: 9
Training loss: 2.9379148483276367
Validation loss: 2.4906254096697737

Epoch: 6| Step: 10
Training loss: 2.735598564147949
Validation loss: 2.4933400333568616

Epoch: 6| Step: 11
Training loss: 2.3667056560516357
Validation loss: 2.4894894489678006

Epoch: 6| Step: 12
Training loss: 1.9126560688018799
Validation loss: 2.49652785383245

Epoch: 6| Step: 13
Training loss: 1.7180001735687256
Validation loss: 2.49576203028361

Epoch: 136| Step: 0
Training loss: 2.8551228046417236
Validation loss: 2.497930508787914

Epoch: 6| Step: 1
Training loss: 2.8892369270324707
Validation loss: 2.507381928864346

Epoch: 6| Step: 2
Training loss: 3.276885509490967
Validation loss: 2.5117293788540747

Epoch: 6| Step: 3
Training loss: 2.3605494499206543
Validation loss: 2.505117339472617

Epoch: 6| Step: 4
Training loss: 2.893991708755493
Validation loss: 2.4942909338141

Epoch: 6| Step: 5
Training loss: 2.848862648010254
Validation loss: 2.48720541051639

Epoch: 6| Step: 6
Training loss: 2.3946707248687744
Validation loss: 2.4820308275120233

Epoch: 6| Step: 7
Training loss: 2.656841516494751
Validation loss: 2.4867883472032446

Epoch: 6| Step: 8
Training loss: 2.7315833568573
Validation loss: 2.490455150604248

Epoch: 6| Step: 9
Training loss: 2.4396462440490723
Validation loss: 2.4877101400847077

Epoch: 6| Step: 10
Training loss: 2.9410080909729004
Validation loss: 2.4916713109580417

Epoch: 6| Step: 11
Training loss: 3.0281925201416016
Validation loss: 2.4891979412365983

Epoch: 6| Step: 12
Training loss: 2.0154991149902344
Validation loss: 2.4908274424973356

Epoch: 6| Step: 13
Training loss: 2.0391900539398193
Validation loss: 2.487718971826697

Epoch: 137| Step: 0
Training loss: 2.447153091430664
Validation loss: 2.481913712716872

Epoch: 6| Step: 1
Training loss: 2.7164306640625
Validation loss: 2.4802353894838722

Epoch: 6| Step: 2
Training loss: 2.589420795440674
Validation loss: 2.4905942537451304

Epoch: 6| Step: 3
Training loss: 2.681675434112549
Validation loss: 2.495367468044322

Epoch: 6| Step: 4
Training loss: 2.808753490447998
Validation loss: 2.515526371617471

Epoch: 6| Step: 5
Training loss: 2.1181626319885254
Validation loss: 2.5080663234956804

Epoch: 6| Step: 6
Training loss: 2.775524139404297
Validation loss: 2.502637755486273

Epoch: 6| Step: 7
Training loss: 2.604930877685547
Validation loss: 2.5192146352542344

Epoch: 6| Step: 8
Training loss: 3.3235549926757812
Validation loss: 2.518647381054458

Epoch: 6| Step: 9
Training loss: 3.338972806930542
Validation loss: 2.526735136585851

Epoch: 6| Step: 10
Training loss: 2.6995577812194824
Validation loss: 2.530476603456723

Epoch: 6| Step: 11
Training loss: 2.5090274810791016
Validation loss: 2.517493212094871

Epoch: 6| Step: 12
Training loss: 2.769975185394287
Validation loss: 2.5070860155167116

Epoch: 6| Step: 13
Training loss: 1.8927911520004272
Validation loss: 2.50238198618735

Epoch: 138| Step: 0
Training loss: 2.2220420837402344
Validation loss: 2.4924068168927263

Epoch: 6| Step: 1
Training loss: 2.7478983402252197
Validation loss: 2.4925597560021187

Epoch: 6| Step: 2
Training loss: 2.870211601257324
Validation loss: 2.4856051232225154

Epoch: 6| Step: 3
Training loss: 3.2556653022766113
Validation loss: 2.486474467862037

Epoch: 6| Step: 4
Training loss: 3.1858153343200684
Validation loss: 2.4868792872275076

Epoch: 6| Step: 5
Training loss: 2.604292869567871
Validation loss: 2.484895242157803

Epoch: 6| Step: 6
Training loss: 2.3877782821655273
Validation loss: 2.4853059925058836

Epoch: 6| Step: 7
Training loss: 2.683199882507324
Validation loss: 2.487841690740278

Epoch: 6| Step: 8
Training loss: 2.448894739151001
Validation loss: 2.482914763112222

Epoch: 6| Step: 9
Training loss: 2.126729726791382
Validation loss: 2.4879275906470513

Epoch: 6| Step: 10
Training loss: 3.0156259536743164
Validation loss: 2.4831798461175736

Epoch: 6| Step: 11
Training loss: 2.3539254665374756
Validation loss: 2.4841624716276764

Epoch: 6| Step: 12
Training loss: 3.164194107055664
Validation loss: 2.4860979997983543

Epoch: 6| Step: 13
Training loss: 2.2759649753570557
Validation loss: 2.4844724696169616

Epoch: 139| Step: 0
Training loss: 2.8236021995544434
Validation loss: 2.4829708581329673

Epoch: 6| Step: 1
Training loss: 3.3973875045776367
Validation loss: 2.478185387067897

Epoch: 6| Step: 2
Training loss: 1.7860795259475708
Validation loss: 2.4886770632959183

Epoch: 6| Step: 3
Training loss: 2.5371453762054443
Validation loss: 2.4827875475729666

Epoch: 6| Step: 4
Training loss: 3.3082821369171143
Validation loss: 2.483577028397591

Epoch: 6| Step: 5
Training loss: 3.018151044845581
Validation loss: 2.4839156494345715

Epoch: 6| Step: 6
Training loss: 2.7240216732025146
Validation loss: 2.479447011024721

Epoch: 6| Step: 7
Training loss: 3.1423115730285645
Validation loss: 2.481976934658584

Epoch: 6| Step: 8
Training loss: 2.420888662338257
Validation loss: 2.4864869117736816

Epoch: 6| Step: 9
Training loss: 2.151827335357666
Validation loss: 2.4824619036848827

Epoch: 6| Step: 10
Training loss: 2.91068696975708
Validation loss: 2.4867390099392144

Epoch: 6| Step: 11
Training loss: 2.9619040489196777
Validation loss: 2.4838476745031213

Epoch: 6| Step: 12
Training loss: 2.003709554672241
Validation loss: 2.4837519866164013

Epoch: 6| Step: 13
Training loss: 2.014564275741577
Validation loss: 2.478877741803405

Epoch: 140| Step: 0
Training loss: 3.161376476287842
Validation loss: 2.477970028436312

Epoch: 6| Step: 1
Training loss: 3.482400894165039
Validation loss: 2.479198673720001

Epoch: 6| Step: 2
Training loss: 2.2263684272766113
Validation loss: 2.474190911939067

Epoch: 6| Step: 3
Training loss: 2.703578472137451
Validation loss: 2.474916140238444

Epoch: 6| Step: 4
Training loss: 2.6920089721679688
Validation loss: 2.476535266445529

Epoch: 6| Step: 5
Training loss: 1.4312883615493774
Validation loss: 2.476423112295007

Epoch: 6| Step: 6
Training loss: 2.8520355224609375
Validation loss: 2.4779325915921118

Epoch: 6| Step: 7
Training loss: 3.368405342102051
Validation loss: 2.480722447877289

Epoch: 6| Step: 8
Training loss: 2.2108030319213867
Validation loss: 2.476712626795615

Epoch: 6| Step: 9
Training loss: 2.6769535541534424
Validation loss: 2.4827072902392318

Epoch: 6| Step: 10
Training loss: 2.820322036743164
Validation loss: 2.485107993566862

Epoch: 6| Step: 11
Training loss: 3.2372117042541504
Validation loss: 2.488703299594182

Epoch: 6| Step: 12
Training loss: 2.277831792831421
Validation loss: 2.4869365179410545

Epoch: 6| Step: 13
Training loss: 1.9664276838302612
Validation loss: 2.4782958081973496

Epoch: 141| Step: 0
Training loss: 2.6393585205078125
Validation loss: 2.478886631227309

Epoch: 6| Step: 1
Training loss: 2.6940391063690186
Validation loss: 2.4797046030721357

Epoch: 6| Step: 2
Training loss: 2.785900592803955
Validation loss: 2.4783522441822994

Epoch: 6| Step: 3
Training loss: 2.951094627380371
Validation loss: 2.4738255931485083

Epoch: 6| Step: 4
Training loss: 2.892820358276367
Validation loss: 2.476194889314713

Epoch: 6| Step: 5
Training loss: 3.0183942317962646
Validation loss: 2.4797917412173365

Epoch: 6| Step: 6
Training loss: 2.076782464981079
Validation loss: 2.4781425165873703

Epoch: 6| Step: 7
Training loss: 2.1284172534942627
Validation loss: 2.4791852479339926

Epoch: 6| Step: 8
Training loss: 2.673475742340088
Validation loss: 2.476878990409195

Epoch: 6| Step: 9
Training loss: 2.465198278427124
Validation loss: 2.479816859768283

Epoch: 6| Step: 10
Training loss: 3.402200698852539
Validation loss: 2.48541913237623

Epoch: 6| Step: 11
Training loss: 2.7315518856048584
Validation loss: 2.486911189171576

Epoch: 6| Step: 12
Training loss: 2.3684470653533936
Validation loss: 2.4792335264144407

Epoch: 6| Step: 13
Training loss: 2.4842960834503174
Validation loss: 2.476110554510547

Epoch: 142| Step: 0
Training loss: 2.849377393722534
Validation loss: 2.478215281681348

Epoch: 6| Step: 1
Training loss: 2.3529787063598633
Validation loss: 2.476640878185149

Epoch: 6| Step: 2
Training loss: 3.661778688430786
Validation loss: 2.473187159466487

Epoch: 6| Step: 3
Training loss: 2.939696788787842
Validation loss: 2.4712122717211322

Epoch: 6| Step: 4
Training loss: 2.2643234729766846
Validation loss: 2.4679294401599514

Epoch: 6| Step: 5
Training loss: 2.8085832595825195
Validation loss: 2.46898716239519

Epoch: 6| Step: 6
Training loss: 2.716733455657959
Validation loss: 2.4733713724279918

Epoch: 6| Step: 7
Training loss: 2.842603921890259
Validation loss: 2.4727035312242407

Epoch: 6| Step: 8
Training loss: 2.2373147010803223
Validation loss: 2.4696968627232376

Epoch: 6| Step: 9
Training loss: 2.8353171348571777
Validation loss: 2.471902378143803

Epoch: 6| Step: 10
Training loss: 2.060839891433716
Validation loss: 2.471727458379602

Epoch: 6| Step: 11
Training loss: 2.258256435394287
Validation loss: 2.478121913889403

Epoch: 6| Step: 12
Training loss: 2.8289191722869873
Validation loss: 2.4757029138585573

Epoch: 6| Step: 13
Training loss: 2.8934123516082764
Validation loss: 2.4782868021277973

Epoch: 143| Step: 0
Training loss: 1.9417979717254639
Validation loss: 2.4735326126057613

Epoch: 6| Step: 1
Training loss: 2.6266367435455322
Validation loss: 2.473692588908698

Epoch: 6| Step: 2
Training loss: 2.7746427059173584
Validation loss: 2.468824983924948

Epoch: 6| Step: 3
Training loss: 3.123164653778076
Validation loss: 2.473156585488268

Epoch: 6| Step: 4
Training loss: 3.5368785858154297
Validation loss: 2.47706849087951

Epoch: 6| Step: 5
Training loss: 2.989124298095703
Validation loss: 2.4861038525899253

Epoch: 6| Step: 6
Training loss: 2.4731171131134033
Validation loss: 2.4796798383035967

Epoch: 6| Step: 7
Training loss: 3.2123374938964844
Validation loss: 2.482443522381526

Epoch: 6| Step: 8
Training loss: 2.7063045501708984
Validation loss: 2.4806322820724978

Epoch: 6| Step: 9
Training loss: 1.9555660486221313
Validation loss: 2.482432016762354

Epoch: 6| Step: 10
Training loss: 2.5082969665527344
Validation loss: 2.4789962409645

Epoch: 6| Step: 11
Training loss: 2.4439573287963867
Validation loss: 2.4713309016278995

Epoch: 6| Step: 12
Training loss: 3.1582412719726562
Validation loss: 2.474908833862633

Epoch: 6| Step: 13
Training loss: 1.3555880784988403
Validation loss: 2.4712551819380892

Epoch: 144| Step: 0
Training loss: 2.060851573944092
Validation loss: 2.4674599785958566

Epoch: 6| Step: 1
Training loss: 3.077610969543457
Validation loss: 2.4670032378165954

Epoch: 6| Step: 2
Training loss: 2.1927504539489746
Validation loss: 2.470667249412947

Epoch: 6| Step: 3
Training loss: 2.9770193099975586
Validation loss: 2.471297202571746

Epoch: 6| Step: 4
Training loss: 2.7581517696380615
Validation loss: 2.469010768398162

Epoch: 6| Step: 5
Training loss: 2.303809404373169
Validation loss: 2.4669100469158542

Epoch: 6| Step: 6
Training loss: 2.659928321838379
Validation loss: 2.470685297443021

Epoch: 6| Step: 7
Training loss: 2.387406349182129
Validation loss: 2.467475998786188

Epoch: 6| Step: 8
Training loss: 2.536464214324951
Validation loss: 2.474828771365586

Epoch: 6| Step: 9
Training loss: 2.554851770401001
Validation loss: 2.4779691773076213

Epoch: 6| Step: 10
Training loss: 3.1180458068847656
Validation loss: 2.4749109309206725

Epoch: 6| Step: 11
Training loss: 2.762092351913452
Validation loss: 2.4728334642225698

Epoch: 6| Step: 12
Training loss: 3.264634132385254
Validation loss: 2.4745372520980013

Epoch: 6| Step: 13
Training loss: 2.685276985168457
Validation loss: 2.4761180749503513

Epoch: 145| Step: 0
Training loss: 2.1539764404296875
Validation loss: 2.4723844605107463

Epoch: 6| Step: 1
Training loss: 3.184094190597534
Validation loss: 2.4701713567139

Epoch: 6| Step: 2
Training loss: 3.2869436740875244
Validation loss: 2.468672831853231

Epoch: 6| Step: 3
Training loss: 2.7088546752929688
Validation loss: 2.4747321862046436

Epoch: 6| Step: 4
Training loss: 2.828341007232666
Validation loss: 2.473848624895978

Epoch: 6| Step: 5
Training loss: 1.970956563949585
Validation loss: 2.475919476119421

Epoch: 6| Step: 6
Training loss: 2.9653077125549316
Validation loss: 2.4792726860251477

Epoch: 6| Step: 7
Training loss: 3.2781808376312256
Validation loss: 2.47688752348705

Epoch: 6| Step: 8
Training loss: 2.5582351684570312
Validation loss: 2.476327621808616

Epoch: 6| Step: 9
Training loss: 2.115783929824829
Validation loss: 2.4722530662372546

Epoch: 6| Step: 10
Training loss: 2.6033520698547363
Validation loss: 2.472990071901711

Epoch: 6| Step: 11
Training loss: 2.6274776458740234
Validation loss: 2.4747026505008822

Epoch: 6| Step: 12
Training loss: 3.119412899017334
Validation loss: 2.472566819960071

Epoch: 6| Step: 13
Training loss: 1.3687701225280762
Validation loss: 2.474827430581534

Epoch: 146| Step: 0
Training loss: 2.48573637008667
Validation loss: 2.472647043966478

Epoch: 6| Step: 1
Training loss: 2.3377838134765625
Validation loss: 2.4700871231735393

Epoch: 6| Step: 2
Training loss: 2.316300392150879
Validation loss: 2.466880485575686

Epoch: 6| Step: 3
Training loss: 2.7822017669677734
Validation loss: 2.469426057671988

Epoch: 6| Step: 4
Training loss: 2.7602996826171875
Validation loss: 2.4751538922709804

Epoch: 6| Step: 5
Training loss: 3.131295680999756
Validation loss: 2.4715494263556694

Epoch: 6| Step: 6
Training loss: 2.627810478210449
Validation loss: 2.4777547826049147

Epoch: 6| Step: 7
Training loss: 2.630110740661621
Validation loss: 2.468398924796812

Epoch: 6| Step: 8
Training loss: 2.6626100540161133
Validation loss: 2.4746124180414344

Epoch: 6| Step: 9
Training loss: 2.816507339477539
Validation loss: 2.475674424120175

Epoch: 6| Step: 10
Training loss: 2.821300506591797
Validation loss: 2.4719066286599762

Epoch: 6| Step: 11
Training loss: 2.3218448162078857
Validation loss: 2.4706188594141314

Epoch: 6| Step: 12
Training loss: 3.412940263748169
Validation loss: 2.472080943404987

Epoch: 6| Step: 13
Training loss: 1.905517578125
Validation loss: 2.474112105625932

Epoch: 147| Step: 0
Training loss: 1.9443280696868896
Validation loss: 2.4730831628204673

Epoch: 6| Step: 1
Training loss: 2.203639030456543
Validation loss: 2.46425437670882

Epoch: 6| Step: 2
Training loss: 3.6442723274230957
Validation loss: 2.4681854171137654

Epoch: 6| Step: 3
Training loss: 2.025782346725464
Validation loss: 2.4691605952478226

Epoch: 6| Step: 4
Training loss: 3.241162061691284
Validation loss: 2.4697096142717587

Epoch: 6| Step: 5
Training loss: 3.1942853927612305
Validation loss: 2.4686164984139065

Epoch: 6| Step: 6
Training loss: 2.7607879638671875
Validation loss: 2.467068613216441

Epoch: 6| Step: 7
Training loss: 2.661470890045166
Validation loss: 2.4677950130995883

Epoch: 6| Step: 8
Training loss: 2.7106058597564697
Validation loss: 2.4668401107993176

Epoch: 6| Step: 9
Training loss: 2.9581422805786133
Validation loss: 2.469446418105915

Epoch: 6| Step: 10
Training loss: 1.906966209411621
Validation loss: 2.4654482910709996

Epoch: 6| Step: 11
Training loss: 2.7274599075317383
Validation loss: 2.464479095192366

Epoch: 6| Step: 12
Training loss: 2.271017551422119
Validation loss: 2.466245194917084

Epoch: 6| Step: 13
Training loss: 3.5100667476654053
Validation loss: 2.4737462997436523

Epoch: 148| Step: 0
Training loss: 2.4610209465026855
Validation loss: 2.4702874742528445

Epoch: 6| Step: 1
Training loss: 3.5264902114868164
Validation loss: 2.4705536057872157

Epoch: 6| Step: 2
Training loss: 2.4900832176208496
Validation loss: 2.468593707648657

Epoch: 6| Step: 3
Training loss: 2.202529191970825
Validation loss: 2.4664483890738538

Epoch: 6| Step: 4
Training loss: 2.1750712394714355
Validation loss: 2.466351814167474

Epoch: 6| Step: 5
Training loss: 2.5310959815979004
Validation loss: 2.4639010224291074

Epoch: 6| Step: 6
Training loss: 2.8679962158203125
Validation loss: 2.463165798494893

Epoch: 6| Step: 7
Training loss: 3.5321364402770996
Validation loss: 2.463612330857144

Epoch: 6| Step: 8
Training loss: 3.240098237991333
Validation loss: 2.462906799008769

Epoch: 6| Step: 9
Training loss: 2.2728419303894043
Validation loss: 2.4656092723210654

Epoch: 6| Step: 10
Training loss: 2.6130599975585938
Validation loss: 2.4640831562780563

Epoch: 6| Step: 11
Training loss: 2.659327507019043
Validation loss: 2.470740715662638

Epoch: 6| Step: 12
Training loss: 2.0852255821228027
Validation loss: 2.4656300262738298

Epoch: 6| Step: 13
Training loss: 2.6388750076293945
Validation loss: 2.4608609753270305

Epoch: 149| Step: 0
Training loss: 2.1884031295776367
Validation loss: 2.4693630485124487

Epoch: 6| Step: 1
Training loss: 3.0369503498077393
Validation loss: 2.47617514928182

Epoch: 6| Step: 2
Training loss: 2.374206066131592
Validation loss: 2.4761042928182952

Epoch: 6| Step: 3
Training loss: 2.4875731468200684
Validation loss: 2.4742878713915424

Epoch: 6| Step: 4
Training loss: 3.080780029296875
Validation loss: 2.4742287717839724

Epoch: 6| Step: 5
Training loss: 2.338642120361328
Validation loss: 2.4706010613390195

Epoch: 6| Step: 6
Training loss: 2.6492342948913574
Validation loss: 2.4699633300945325

Epoch: 6| Step: 7
Training loss: 2.2266483306884766
Validation loss: 2.4706329350830405

Epoch: 6| Step: 8
Training loss: 2.2607407569885254
Validation loss: 2.4683263058303506

Epoch: 6| Step: 9
Training loss: 2.99407696723938
Validation loss: 2.4698877360231135

Epoch: 6| Step: 10
Training loss: 3.3493621349334717
Validation loss: 2.4666856309419036

Epoch: 6| Step: 11
Training loss: 2.238020896911621
Validation loss: 2.4673347960236254

Epoch: 6| Step: 12
Training loss: 3.2895255088806152
Validation loss: 2.4680196418557117

Epoch: 6| Step: 13
Training loss: 2.830775260925293
Validation loss: 2.4690413808309906

Epoch: 150| Step: 0
Training loss: 2.5310957431793213
Validation loss: 2.4680679331543627

Epoch: 6| Step: 1
Training loss: 2.891785144805908
Validation loss: 2.4669942830198552

Epoch: 6| Step: 2
Training loss: 2.5917649269104004
Validation loss: 2.482104445016512

Epoch: 6| Step: 3
Training loss: 2.7712759971618652
Validation loss: 2.495475810061219

Epoch: 6| Step: 4
Training loss: 1.9178532361984253
Validation loss: 2.492750075555617

Epoch: 6| Step: 5
Training loss: 1.689898133277893
Validation loss: 2.5046896498690367

Epoch: 6| Step: 6
Training loss: 3.099705696105957
Validation loss: 2.508586960454141

Epoch: 6| Step: 7
Training loss: 3.0385522842407227
Validation loss: 2.501825912024385

Epoch: 6| Step: 8
Training loss: 2.376899242401123
Validation loss: 2.489770012517129

Epoch: 6| Step: 9
Training loss: 3.148258924484253
Validation loss: 2.4870073026226414

Epoch: 6| Step: 10
Training loss: 2.486527442932129
Validation loss: 2.48138685892987

Epoch: 6| Step: 11
Training loss: 2.831829309463501
Validation loss: 2.472111189237205

Epoch: 6| Step: 12
Training loss: 2.9507904052734375
Validation loss: 2.4694160594735095

Epoch: 6| Step: 13
Training loss: 3.2269375324249268
Validation loss: 2.4604941696249027

Epoch: 151| Step: 0
Training loss: 2.0817084312438965
Validation loss: 2.467435206136396

Epoch: 6| Step: 1
Training loss: 3.471919536590576
Validation loss: 2.4654965246877363

Epoch: 6| Step: 2
Training loss: 3.3090925216674805
Validation loss: 2.4690991191453833

Epoch: 6| Step: 3
Training loss: 1.9938879013061523
Validation loss: 2.4694277701839322

Epoch: 6| Step: 4
Training loss: 2.873352289199829
Validation loss: 2.4750857942847797

Epoch: 6| Step: 5
Training loss: 2.215282440185547
Validation loss: 2.4751976215711204

Epoch: 6| Step: 6
Training loss: 2.7719221115112305
Validation loss: 2.4777105187857025

Epoch: 6| Step: 7
Training loss: 2.6824710369110107
Validation loss: 2.4756321086678454

Epoch: 6| Step: 8
Training loss: 2.535109519958496
Validation loss: 2.4868906390282417

Epoch: 6| Step: 9
Training loss: 2.632840156555176
Validation loss: 2.4817847077564528

Epoch: 6| Step: 10
Training loss: 2.334590435028076
Validation loss: 2.4773595922736713

Epoch: 6| Step: 11
Training loss: 3.386770725250244
Validation loss: 2.4820491677971295

Epoch: 6| Step: 12
Training loss: 1.9443564414978027
Validation loss: 2.481976245039253

Epoch: 6| Step: 13
Training loss: 3.1464807987213135
Validation loss: 2.475546803525699

Epoch: 152| Step: 0
Training loss: 2.978463649749756
Validation loss: 2.4745700359344482

Epoch: 6| Step: 1
Training loss: 2.113548755645752
Validation loss: 2.47870538567984

Epoch: 6| Step: 2
Training loss: 2.660071849822998
Validation loss: 2.472137112771311

Epoch: 6| Step: 3
Training loss: 2.639472484588623
Validation loss: 2.4731710982579056

Epoch: 6| Step: 4
Training loss: 2.38101863861084
Validation loss: 2.4753065647617465

Epoch: 6| Step: 5
Training loss: 2.150195360183716
Validation loss: 2.4787605988082064

Epoch: 6| Step: 6
Training loss: 2.7118868827819824
Validation loss: 2.480120620419902

Epoch: 6| Step: 7
Training loss: 2.1860575675964355
Validation loss: 2.4795920259209088

Epoch: 6| Step: 8
Training loss: 2.3230209350585938
Validation loss: 2.466780995809904

Epoch: 6| Step: 9
Training loss: 3.632863759994507
Validation loss: 2.4789658028592347

Epoch: 6| Step: 10
Training loss: 1.83406400680542
Validation loss: 2.4854831439192577

Epoch: 6| Step: 11
Training loss: 3.3847148418426514
Validation loss: 2.4934033565623785

Epoch: 6| Step: 12
Training loss: 3.151552677154541
Validation loss: 2.498345764734412

Epoch: 6| Step: 13
Training loss: 3.096663475036621
Validation loss: 2.5034249572343725

Epoch: 153| Step: 0
Training loss: 2.9546477794647217
Validation loss: 2.504824241002401

Epoch: 6| Step: 1
Training loss: 2.2663583755493164
Validation loss: 2.499148989236483

Epoch: 6| Step: 2
Training loss: 2.904493570327759
Validation loss: 2.4882865721179592

Epoch: 6| Step: 3
Training loss: 2.267808198928833
Validation loss: 2.4775482531516784

Epoch: 6| Step: 4
Training loss: 3.843597412109375
Validation loss: 2.4735698110313824

Epoch: 6| Step: 5
Training loss: 3.4636385440826416
Validation loss: 2.4669489450352167

Epoch: 6| Step: 6
Training loss: 2.702335834503174
Validation loss: 2.47631319620276

Epoch: 6| Step: 7
Training loss: 2.4002630710601807
Validation loss: 2.472802692844022

Epoch: 6| Step: 8
Training loss: 2.2664198875427246
Validation loss: 2.466427828675957

Epoch: 6| Step: 9
Training loss: 2.5366811752319336
Validation loss: 2.4704027560449417

Epoch: 6| Step: 10
Training loss: 2.242136240005493
Validation loss: 2.4721544686184136

Epoch: 6| Step: 11
Training loss: 2.482882499694824
Validation loss: 2.4669204963150846

Epoch: 6| Step: 12
Training loss: 2.3805432319641113
Validation loss: 2.4800797482972503

Epoch: 6| Step: 13
Training loss: 2.355020761489868
Validation loss: 2.479028349281639

Epoch: 154| Step: 0
Training loss: 2.438124895095825
Validation loss: 2.5006552229645433

Epoch: 6| Step: 1
Training loss: 2.0341851711273193
Validation loss: 2.504608449115548

Epoch: 6| Step: 2
Training loss: 2.9311208724975586
Validation loss: 2.5197551609367452

Epoch: 6| Step: 3
Training loss: 2.4008660316467285
Validation loss: 2.5179077374037875

Epoch: 6| Step: 4
Training loss: 2.7100820541381836
Validation loss: 2.5133381223165863

Epoch: 6| Step: 5
Training loss: 2.61613130569458
Validation loss: 2.5087447243352092

Epoch: 6| Step: 6
Training loss: 2.7238850593566895
Validation loss: 2.5180816240208124

Epoch: 6| Step: 7
Training loss: 2.4357709884643555
Validation loss: 2.502346090091172

Epoch: 6| Step: 8
Training loss: 3.165562868118286
Validation loss: 2.483910393971269

Epoch: 6| Step: 9
Training loss: 3.136746644973755
Validation loss: 2.4863820204170803

Epoch: 6| Step: 10
Training loss: 3.0212061405181885
Validation loss: 2.4764644330547703

Epoch: 6| Step: 11
Training loss: 2.6724371910095215
Validation loss: 2.4750516183914675

Epoch: 6| Step: 12
Training loss: 2.234785556793213
Validation loss: 2.469647387022613

Epoch: 6| Step: 13
Training loss: 2.537062168121338
Validation loss: 2.47454422520053

Epoch: 155| Step: 0
Training loss: 3.613013982772827
Validation loss: 2.4771745435653196

Epoch: 6| Step: 1
Training loss: 1.9457135200500488
Validation loss: 2.465848325401224

Epoch: 6| Step: 2
Training loss: 2.7328004837036133
Validation loss: 2.4741399980360463

Epoch: 6| Step: 3
Training loss: 2.6832284927368164
Validation loss: 2.4704528880375687

Epoch: 6| Step: 4
Training loss: 2.713707208633423
Validation loss: 2.4769363890412035

Epoch: 6| Step: 5
Training loss: 3.2833807468414307
Validation loss: 2.4755968150272163

Epoch: 6| Step: 6
Training loss: 2.3957176208496094
Validation loss: 2.4854361882773777

Epoch: 6| Step: 7
Training loss: 2.302846908569336
Validation loss: 2.480904127961846

Epoch: 6| Step: 8
Training loss: 2.6350936889648438
Validation loss: 2.478487358298353

Epoch: 6| Step: 9
Training loss: 2.429920196533203
Validation loss: 2.4763715215908584

Epoch: 6| Step: 10
Training loss: 2.8058998584747314
Validation loss: 2.477534524856075

Epoch: 6| Step: 11
Training loss: 2.3121635913848877
Validation loss: 2.473683367493332

Epoch: 6| Step: 12
Training loss: 2.747499465942383
Validation loss: 2.4668549594058784

Epoch: 6| Step: 13
Training loss: 2.5930120944976807
Validation loss: 2.469058136786184

Epoch: 156| Step: 0
Training loss: 2.5184109210968018
Validation loss: 2.461742034522436

Epoch: 6| Step: 1
Training loss: 2.406891345977783
Validation loss: 2.4585910202354513

Epoch: 6| Step: 2
Training loss: 1.7968250513076782
Validation loss: 2.4586250218012

Epoch: 6| Step: 3
Training loss: 2.6693620681762695
Validation loss: 2.4540160881575717

Epoch: 6| Step: 4
Training loss: 2.7279632091522217
Validation loss: 2.454384139789048

Epoch: 6| Step: 5
Training loss: 2.6392343044281006
Validation loss: 2.4505935843272875

Epoch: 6| Step: 6
Training loss: 3.353729248046875
Validation loss: 2.4505014496464885

Epoch: 6| Step: 7
Training loss: 3.6178817749023438
Validation loss: 2.445527135684926

Epoch: 6| Step: 8
Training loss: 2.2192647457122803
Validation loss: 2.449699822292533

Epoch: 6| Step: 9
Training loss: 2.6667697429656982
Validation loss: 2.4522982182041293

Epoch: 6| Step: 10
Training loss: 2.723632335662842
Validation loss: 2.450549925527265

Epoch: 6| Step: 11
Training loss: 3.0758090019226074
Validation loss: 2.446934082174814

Epoch: 6| Step: 12
Training loss: 2.6610729694366455
Validation loss: 2.4486187734911518

Epoch: 6| Step: 13
Training loss: 1.5888906717300415
Validation loss: 2.4514940361822806

Epoch: 157| Step: 0
Training loss: 2.6562933921813965
Validation loss: 2.4507627589728243

Epoch: 6| Step: 1
Training loss: 2.34122896194458
Validation loss: 2.450379635698052

Epoch: 6| Step: 2
Training loss: 2.2662482261657715
Validation loss: 2.456487645385086

Epoch: 6| Step: 3
Training loss: 2.6575827598571777
Validation loss: 2.453418994462618

Epoch: 6| Step: 4
Training loss: 2.597219944000244
Validation loss: 2.4618635382703555

Epoch: 6| Step: 5
Training loss: 2.601497173309326
Validation loss: 2.460819452039657

Epoch: 6| Step: 6
Training loss: 3.012681245803833
Validation loss: 2.467036347235403

Epoch: 6| Step: 7
Training loss: 3.3296170234680176
Validation loss: 2.4665173176796205

Epoch: 6| Step: 8
Training loss: 2.9012176990509033
Validation loss: 2.4741896455005934

Epoch: 6| Step: 9
Training loss: 2.3734264373779297
Validation loss: 2.459741392443257

Epoch: 6| Step: 10
Training loss: 2.9690709114074707
Validation loss: 2.4587568442026773

Epoch: 6| Step: 11
Training loss: 2.9521024227142334
Validation loss: 2.453601880740094

Epoch: 6| Step: 12
Training loss: 2.142577648162842
Validation loss: 2.4461245049712477

Epoch: 6| Step: 13
Training loss: 2.3209736347198486
Validation loss: 2.4445001258645007

Epoch: 158| Step: 0
Training loss: 2.431368827819824
Validation loss: 2.4450454993914534

Epoch: 6| Step: 1
Training loss: 2.665834665298462
Validation loss: 2.4490050961894374

Epoch: 6| Step: 2
Training loss: 2.616328001022339
Validation loss: 2.4435728673012025

Epoch: 6| Step: 3
Training loss: 3.1512231826782227
Validation loss: 2.4419163529590895

Epoch: 6| Step: 4
Training loss: 2.5200142860412598
Validation loss: 2.442907879429479

Epoch: 6| Step: 5
Training loss: 2.6319174766540527
Validation loss: 2.448332537886917

Epoch: 6| Step: 6
Training loss: 3.656679153442383
Validation loss: 2.448305801678729

Epoch: 6| Step: 7
Training loss: 2.699364185333252
Validation loss: 2.44279819919217

Epoch: 6| Step: 8
Training loss: 2.026067018508911
Validation loss: 2.44213169364519

Epoch: 6| Step: 9
Training loss: 2.512507677078247
Validation loss: 2.4496557148553992

Epoch: 6| Step: 10
Training loss: 3.0795044898986816
Validation loss: 2.4430994756760134

Epoch: 6| Step: 11
Training loss: 1.5438997745513916
Validation loss: 2.4434294418622087

Epoch: 6| Step: 12
Training loss: 2.5299785137176514
Validation loss: 2.4482061427126647

Epoch: 6| Step: 13
Training loss: 3.477689504623413
Validation loss: 2.446788905769266

Epoch: 159| Step: 0
Training loss: 2.066741466522217
Validation loss: 2.451869046816262

Epoch: 6| Step: 1
Training loss: 2.684931755065918
Validation loss: 2.4447820007160144

Epoch: 6| Step: 2
Training loss: 2.3414230346679688
Validation loss: 2.45034344221956

Epoch: 6| Step: 3
Training loss: 3.212846279144287
Validation loss: 2.4501846246821906

Epoch: 6| Step: 4
Training loss: 2.5479722023010254
Validation loss: 2.450313009241576

Epoch: 6| Step: 5
Training loss: 3.3021364212036133
Validation loss: 2.4490933777183614

Epoch: 6| Step: 6
Training loss: 2.620577812194824
Validation loss: 2.455032143541562

Epoch: 6| Step: 7
Training loss: 2.9321489334106445
Validation loss: 2.4600552512753393

Epoch: 6| Step: 8
Training loss: 2.678173542022705
Validation loss: 2.463133668386808

Epoch: 6| Step: 9
Training loss: 1.8171796798706055
Validation loss: 2.4676452144499748

Epoch: 6| Step: 10
Training loss: 2.672135591506958
Validation loss: 2.46607344893999

Epoch: 6| Step: 11
Training loss: 3.030527353286743
Validation loss: 2.4643463780803065

Epoch: 6| Step: 12
Training loss: 2.63087797164917
Validation loss: 2.4592049301311536

Epoch: 6| Step: 13
Training loss: 2.724525213241577
Validation loss: 2.465109173969556

Epoch: 160| Step: 0
Training loss: 2.322906732559204
Validation loss: 2.471285327788322

Epoch: 6| Step: 1
Training loss: 2.353610038757324
Validation loss: 2.461153919978808

Epoch: 6| Step: 2
Training loss: 2.853719711303711
Validation loss: 2.4583340588436333

Epoch: 6| Step: 3
Training loss: 2.6092305183410645
Validation loss: 2.473049786783034

Epoch: 6| Step: 4
Training loss: 3.4506804943084717
Validation loss: 2.480089159422023

Epoch: 6| Step: 5
Training loss: 2.9549145698547363
Validation loss: 2.474706898453415

Epoch: 6| Step: 6
Training loss: 2.0962438583374023
Validation loss: 2.476918646084365

Epoch: 6| Step: 7
Training loss: 1.9647856950759888
Validation loss: 2.473513575010402

Epoch: 6| Step: 8
Training loss: 2.2527098655700684
Validation loss: 2.4777041942842546

Epoch: 6| Step: 9
Training loss: 3.4815003871917725
Validation loss: 2.461230662561232

Epoch: 6| Step: 10
Training loss: 3.42120623588562
Validation loss: 2.463657714987314

Epoch: 6| Step: 11
Training loss: 2.242767810821533
Validation loss: 2.4556183276637906

Epoch: 6| Step: 12
Training loss: 2.510836124420166
Validation loss: 2.4454608835199827

Epoch: 6| Step: 13
Training loss: 2.6298117637634277
Validation loss: 2.448627071995889

Epoch: 161| Step: 0
Training loss: 2.8301620483398438
Validation loss: 2.452981000305504

Epoch: 6| Step: 1
Training loss: 3.3844995498657227
Validation loss: 2.44508239787112

Epoch: 6| Step: 2
Training loss: 1.7696523666381836
Validation loss: 2.4506753336998726

Epoch: 6| Step: 3
Training loss: 3.4598779678344727
Validation loss: 2.4521958802336004

Epoch: 6| Step: 4
Training loss: 2.150273561477661
Validation loss: 2.442584829945718

Epoch: 6| Step: 5
Training loss: 2.086132764816284
Validation loss: 2.4467758183838217

Epoch: 6| Step: 6
Training loss: 2.004101276397705
Validation loss: 2.4478969035610074

Epoch: 6| Step: 7
Training loss: 3.48931884765625
Validation loss: 2.4484866947256108

Epoch: 6| Step: 8
Training loss: 3.1817047595977783
Validation loss: 2.452501371342649

Epoch: 6| Step: 9
Training loss: 2.92689847946167
Validation loss: 2.453289572910596

Epoch: 6| Step: 10
Training loss: 1.8155914545059204
Validation loss: 2.4510699292664886

Epoch: 6| Step: 11
Training loss: 2.7173233032226562
Validation loss: 2.4530116127383326

Epoch: 6| Step: 12
Training loss: 2.547393321990967
Validation loss: 2.4607679561902116

Epoch: 6| Step: 13
Training loss: 2.6889703273773193
Validation loss: 2.462058932550492

Epoch: 162| Step: 0
Training loss: 2.7176475524902344
Validation loss: 2.4630358680602042

Epoch: 6| Step: 1
Training loss: 2.67048978805542
Validation loss: 2.460861590600783

Epoch: 6| Step: 2
Training loss: 2.290639638900757
Validation loss: 2.46227583577556

Epoch: 6| Step: 3
Training loss: 3.159865617752075
Validation loss: 2.4690781998377975

Epoch: 6| Step: 4
Training loss: 3.5655324459075928
Validation loss: 2.4621303978786675

Epoch: 6| Step: 5
Training loss: 2.4962527751922607
Validation loss: 2.468678620553786

Epoch: 6| Step: 6
Training loss: 3.0500078201293945
Validation loss: 2.469646059056764

Epoch: 6| Step: 7
Training loss: 2.3375022411346436
Validation loss: 2.4692328335136495

Epoch: 6| Step: 8
Training loss: 2.998521327972412
Validation loss: 2.47737015703673

Epoch: 6| Step: 9
Training loss: 2.028010845184326
Validation loss: 2.467861008900468

Epoch: 6| Step: 10
Training loss: 2.3274168968200684
Validation loss: 2.4632867382418726

Epoch: 6| Step: 11
Training loss: 2.2427849769592285
Validation loss: 2.456097969444849

Epoch: 6| Step: 12
Training loss: 2.3503992557525635
Validation loss: 2.457012232913766

Epoch: 6| Step: 13
Training loss: 3.221656084060669
Validation loss: 2.470067952268867

Epoch: 163| Step: 0
Training loss: 3.0113754272460938
Validation loss: 2.477246774140225

Epoch: 6| Step: 1
Training loss: 2.791893482208252
Validation loss: 2.4822609040044967

Epoch: 6| Step: 2
Training loss: 2.4661898612976074
Validation loss: 2.4821299173498668

Epoch: 6| Step: 3
Training loss: 2.311154842376709
Validation loss: 2.492229546270063

Epoch: 6| Step: 4
Training loss: 2.5846171379089355
Validation loss: 2.504198287122993

Epoch: 6| Step: 5
Training loss: 3.165249824523926
Validation loss: 2.5051754315694175

Epoch: 6| Step: 6
Training loss: 3.269106388092041
Validation loss: 2.514375455917851

Epoch: 6| Step: 7
Training loss: 2.5211570262908936
Validation loss: 2.513290907747002

Epoch: 6| Step: 8
Training loss: 2.7255048751831055
Validation loss: 2.5016917977281796

Epoch: 6| Step: 9
Training loss: 2.322549819946289
Validation loss: 2.4830301884681947

Epoch: 6| Step: 10
Training loss: 2.5741677284240723
Validation loss: 2.4555272620211364

Epoch: 6| Step: 11
Training loss: 2.3023581504821777
Validation loss: 2.447381252883583

Epoch: 6| Step: 12
Training loss: 3.3080921173095703
Validation loss: 2.443323619904057

Epoch: 6| Step: 13
Training loss: 1.6179836988449097
Validation loss: 2.445243953376688

Epoch: 164| Step: 0
Training loss: 2.808825969696045
Validation loss: 2.461391415647281

Epoch: 6| Step: 1
Training loss: 2.4312829971313477
Validation loss: 2.4631412798358547

Epoch: 6| Step: 2
Training loss: 3.423366069793701
Validation loss: 2.457538227881155

Epoch: 6| Step: 3
Training loss: 1.7596569061279297
Validation loss: 2.4621781623491676

Epoch: 6| Step: 4
Training loss: 2.956062078475952
Validation loss: 2.470504876106016

Epoch: 6| Step: 5
Training loss: 3.2208364009857178
Validation loss: 2.467227963991063

Epoch: 6| Step: 6
Training loss: 2.6653225421905518
Validation loss: 2.4568305861565376

Epoch: 6| Step: 7
Training loss: 2.5162227153778076
Validation loss: 2.4361479820743686

Epoch: 6| Step: 8
Training loss: 2.2278640270233154
Validation loss: 2.4382912574275846

Epoch: 6| Step: 9
Training loss: 2.955155372619629
Validation loss: 2.435483391566943

Epoch: 6| Step: 10
Training loss: 2.8256638050079346
Validation loss: 2.437032284275178

Epoch: 6| Step: 11
Training loss: 2.3890395164489746
Validation loss: 2.4367212608296382

Epoch: 6| Step: 12
Training loss: 2.280831813812256
Validation loss: 2.4437647840028167

Epoch: 6| Step: 13
Training loss: 2.6894001960754395
Validation loss: 2.4470315915282055

Epoch: 165| Step: 0
Training loss: 2.0532844066619873
Validation loss: 2.453824166328676

Epoch: 6| Step: 1
Training loss: 3.5440943241119385
Validation loss: 2.4582118859855075

Epoch: 6| Step: 2
Training loss: 2.2546637058258057
Validation loss: 2.472915087976763

Epoch: 6| Step: 3
Training loss: 2.9335267543792725
Validation loss: 2.473187969576928

Epoch: 6| Step: 4
Training loss: 2.918431282043457
Validation loss: 2.4759104892771733

Epoch: 6| Step: 5
Training loss: 1.290352702140808
Validation loss: 2.4790744473857265

Epoch: 6| Step: 6
Training loss: 2.453383445739746
Validation loss: 2.479368232911633

Epoch: 6| Step: 7
Training loss: 3.2397685050964355
Validation loss: 2.4662213351136897

Epoch: 6| Step: 8
Training loss: 2.406306743621826
Validation loss: 2.4551159181902484

Epoch: 6| Step: 9
Training loss: 3.3260536193847656
Validation loss: 2.452443958610617

Epoch: 6| Step: 10
Training loss: 3.024118661880493
Validation loss: 2.4425204953839703

Epoch: 6| Step: 11
Training loss: 2.9531776905059814
Validation loss: 2.433365414219518

Epoch: 6| Step: 12
Training loss: 2.595053195953369
Validation loss: 2.4375174532654467

Epoch: 6| Step: 13
Training loss: 1.9823600053787231
Validation loss: 2.4372839671309277

Epoch: 166| Step: 0
Training loss: 3.266172409057617
Validation loss: 2.4484281745008243

Epoch: 6| Step: 1
Training loss: 1.9945974349975586
Validation loss: 2.4590284542370866

Epoch: 6| Step: 2
Training loss: 3.076751708984375
Validation loss: 2.474496792721492

Epoch: 6| Step: 3
Training loss: 2.4610955715179443
Validation loss: 2.478444419881349

Epoch: 6| Step: 4
Training loss: 2.896792411804199
Validation loss: 2.4735266880322526

Epoch: 6| Step: 5
Training loss: 2.9278295040130615
Validation loss: 2.4689454647802536

Epoch: 6| Step: 6
Training loss: 2.196641206741333
Validation loss: 2.4681147913778982

Epoch: 6| Step: 7
Training loss: 1.998376727104187
Validation loss: 2.471415763260216

Epoch: 6| Step: 8
Training loss: 2.5282063484191895
Validation loss: 2.4560490449269614

Epoch: 6| Step: 9
Training loss: 2.5197696685791016
Validation loss: 2.4534492902858283

Epoch: 6| Step: 10
Training loss: 3.338811159133911
Validation loss: 2.4506346641048307

Epoch: 6| Step: 11
Training loss: 2.169079303741455
Validation loss: 2.4598750427205074

Epoch: 6| Step: 12
Training loss: 3.3090524673461914
Validation loss: 2.4645730962035475

Epoch: 6| Step: 13
Training loss: 2.2049622535705566
Validation loss: 2.4771525629105104

Epoch: 167| Step: 0
Training loss: 2.8280107975006104
Validation loss: 2.4680470241013395

Epoch: 6| Step: 1
Training loss: 2.5896763801574707
Validation loss: 2.468039607488981

Epoch: 6| Step: 2
Training loss: 2.326294422149658
Validation loss: 2.45494342875737

Epoch: 6| Step: 3
Training loss: 2.7159929275512695
Validation loss: 2.457545167656355

Epoch: 6| Step: 4
Training loss: 3.1989200115203857
Validation loss: 2.4587399293017644

Epoch: 6| Step: 5
Training loss: 2.7908127307891846
Validation loss: 2.453335059586392

Epoch: 6| Step: 6
Training loss: 2.878166437149048
Validation loss: 2.4579870521381335

Epoch: 6| Step: 7
Training loss: 2.703859806060791
Validation loss: 2.457785975548529

Epoch: 6| Step: 8
Training loss: 2.70125675201416
Validation loss: 2.4546331513312554

Epoch: 6| Step: 9
Training loss: 2.3638579845428467
Validation loss: 2.457127332687378

Epoch: 6| Step: 10
Training loss: 2.8957977294921875
Validation loss: 2.4726800585305817

Epoch: 6| Step: 11
Training loss: 2.2420167922973633
Validation loss: 2.4576248097163376

Epoch: 6| Step: 12
Training loss: 2.284687042236328
Validation loss: 2.4656777715170257

Epoch: 6| Step: 13
Training loss: 2.1464996337890625
Validation loss: 2.4559197220751035

Epoch: 168| Step: 0
Training loss: 2.9682512283325195
Validation loss: 2.439200044960104

Epoch: 6| Step: 1
Training loss: 2.0707592964172363
Validation loss: 2.4404451283075477

Epoch: 6| Step: 2
Training loss: 2.864985466003418
Validation loss: 2.452968000083841

Epoch: 6| Step: 3
Training loss: 3.202486038208008
Validation loss: 2.457239294564852

Epoch: 6| Step: 4
Training loss: 2.792896270751953
Validation loss: 2.458387046731928

Epoch: 6| Step: 5
Training loss: 2.3732316493988037
Validation loss: 2.4659112089423725

Epoch: 6| Step: 6
Training loss: 3.347435474395752
Validation loss: 2.473673943550356

Epoch: 6| Step: 7
Training loss: 2.030369758605957
Validation loss: 2.4752706250836773

Epoch: 6| Step: 8
Training loss: 2.6480472087860107
Validation loss: 2.4642357723687285

Epoch: 6| Step: 9
Training loss: 2.434230327606201
Validation loss: 2.4568297939915813

Epoch: 6| Step: 10
Training loss: 1.756208896636963
Validation loss: 2.4543316851380053

Epoch: 6| Step: 11
Training loss: 3.046607494354248
Validation loss: 2.4520676289835284

Epoch: 6| Step: 12
Training loss: 3.091268539428711
Validation loss: 2.4508316568149033

Epoch: 6| Step: 13
Training loss: 2.677701473236084
Validation loss: 2.452844532587195

Epoch: 169| Step: 0
Training loss: 3.0979690551757812
Validation loss: 2.4524421794440157

Epoch: 6| Step: 1
Training loss: 3.0655722618103027
Validation loss: 2.4503265555186937

Epoch: 6| Step: 2
Training loss: 2.5293030738830566
Validation loss: 2.441198400271836

Epoch: 6| Step: 3
Training loss: 2.4475526809692383
Validation loss: 2.4365095784587245

Epoch: 6| Step: 4
Training loss: 2.994483709335327
Validation loss: 2.448680685412499

Epoch: 6| Step: 5
Training loss: 1.8930946588516235
Validation loss: 2.448565608711653

Epoch: 6| Step: 6
Training loss: 1.873582363128662
Validation loss: 2.4544331591616393

Epoch: 6| Step: 7
Training loss: 2.406066656112671
Validation loss: 2.4609242818688832

Epoch: 6| Step: 8
Training loss: 2.7519025802612305
Validation loss: 2.4694040770171792

Epoch: 6| Step: 9
Training loss: 3.090384006500244
Validation loss: 2.4700174511119886

Epoch: 6| Step: 10
Training loss: 2.5152499675750732
Validation loss: 2.45800369785678

Epoch: 6| Step: 11
Training loss: 2.5773160457611084
Validation loss: 2.4734355916259108

Epoch: 6| Step: 12
Training loss: 3.1552793979644775
Validation loss: 2.4618994215483307

Epoch: 6| Step: 13
Training loss: 2.7752747535705566
Validation loss: 2.453759826639647

Epoch: 170| Step: 0
Training loss: 2.792860984802246
Validation loss: 2.4532524206305064

Epoch: 6| Step: 1
Training loss: 2.7189836502075195
Validation loss: 2.4412682517882316

Epoch: 6| Step: 2
Training loss: 2.896859645843506
Validation loss: 2.438082197661041

Epoch: 6| Step: 3
Training loss: 2.037230968475342
Validation loss: 2.4368098807591263

Epoch: 6| Step: 4
Training loss: 2.746443748474121
Validation loss: 2.4357077819044872

Epoch: 6| Step: 5
Training loss: 3.482112407684326
Validation loss: 2.4403492763478267

Epoch: 6| Step: 6
Training loss: 1.7713336944580078
Validation loss: 2.4333892201864593

Epoch: 6| Step: 7
Training loss: 2.59881854057312
Validation loss: 2.440207267320284

Epoch: 6| Step: 8
Training loss: 2.4661903381347656
Validation loss: 2.4398169953336

Epoch: 6| Step: 9
Training loss: 2.6088151931762695
Validation loss: 2.434576306291806

Epoch: 6| Step: 10
Training loss: 2.5621747970581055
Validation loss: 2.4443795386181084

Epoch: 6| Step: 11
Training loss: 2.247986316680908
Validation loss: 2.44724267016175

Epoch: 6| Step: 12
Training loss: 3.2405290603637695
Validation loss: 2.4485240085150606

Epoch: 6| Step: 13
Training loss: 2.746840238571167
Validation loss: 2.4477569544187157

Epoch: 171| Step: 0
Training loss: 3.1880953311920166
Validation loss: 2.4571899957554315

Epoch: 6| Step: 1
Training loss: 2.566009998321533
Validation loss: 2.457390618580644

Epoch: 6| Step: 2
Training loss: 2.2789297103881836
Validation loss: 2.4562892298544607

Epoch: 6| Step: 3
Training loss: 2.098055124282837
Validation loss: 2.473745587051556

Epoch: 6| Step: 4
Training loss: 2.7625832557678223
Validation loss: 2.4830254175329722

Epoch: 6| Step: 5
Training loss: 1.9073493480682373
Validation loss: 2.4856812953948975

Epoch: 6| Step: 6
Training loss: 2.6397314071655273
Validation loss: 2.4840276190029678

Epoch: 6| Step: 7
Training loss: 3.527090311050415
Validation loss: 2.485141569568265

Epoch: 6| Step: 8
Training loss: 2.8952226638793945
Validation loss: 2.4765106247317408

Epoch: 6| Step: 9
Training loss: 2.4195895195007324
Validation loss: 2.470933662947788

Epoch: 6| Step: 10
Training loss: 2.9534473419189453
Validation loss: 2.4618638561617945

Epoch: 6| Step: 11
Training loss: 2.510953187942505
Validation loss: 2.457893704855314

Epoch: 6| Step: 12
Training loss: 2.812025547027588
Validation loss: 2.4487069165834816

Epoch: 6| Step: 13
Training loss: 2.173865556716919
Validation loss: 2.4478336482919674

Epoch: 172| Step: 0
Training loss: 2.887050151824951
Validation loss: 2.4466222640006774

Epoch: 6| Step: 1
Training loss: 2.0028233528137207
Validation loss: 2.4437771997144146

Epoch: 6| Step: 2
Training loss: 3.029122829437256
Validation loss: 2.4565193486470047

Epoch: 6| Step: 3
Training loss: 2.4912426471710205
Validation loss: 2.45612959964301

Epoch: 6| Step: 4
Training loss: 2.0904927253723145
Validation loss: 2.4577774950253066

Epoch: 6| Step: 5
Training loss: 3.3923566341400146
Validation loss: 2.453232011487407

Epoch: 6| Step: 6
Training loss: 3.1066577434539795
Validation loss: 2.4579190131156676

Epoch: 6| Step: 7
Training loss: 2.4226229190826416
Validation loss: 2.4628954651535198

Epoch: 6| Step: 8
Training loss: 2.5913820266723633
Validation loss: 2.4543823760042907

Epoch: 6| Step: 9
Training loss: 3.068235397338867
Validation loss: 2.4537352028713433

Epoch: 6| Step: 10
Training loss: 2.269423246383667
Validation loss: 2.446272334744853

Epoch: 6| Step: 11
Training loss: 1.9992964267730713
Validation loss: 2.450708394409508

Epoch: 6| Step: 12
Training loss: 3.133596181869507
Validation loss: 2.443213439756824

Epoch: 6| Step: 13
Training loss: 2.5264782905578613
Validation loss: 2.4383925955782653

Epoch: 173| Step: 0
Training loss: 2.700949192047119
Validation loss: 2.4507172722970285

Epoch: 6| Step: 1
Training loss: 2.682248115539551
Validation loss: 2.4494397614591863

Epoch: 6| Step: 2
Training loss: 3.0631566047668457
Validation loss: 2.4439583721981255

Epoch: 6| Step: 3
Training loss: 2.8700411319732666
Validation loss: 2.4463191545137795

Epoch: 6| Step: 4
Training loss: 1.739905595779419
Validation loss: 2.4453627012109243

Epoch: 6| Step: 5
Training loss: 2.3802883625030518
Validation loss: 2.442655042935443

Epoch: 6| Step: 6
Training loss: 2.1656806468963623
Validation loss: 2.442855114577919

Epoch: 6| Step: 7
Training loss: 2.8958897590637207
Validation loss: 2.4442773378023537

Epoch: 6| Step: 8
Training loss: 3.136025905609131
Validation loss: 2.4507577816645303

Epoch: 6| Step: 9
Training loss: 3.47027587890625
Validation loss: 2.450857852094917

Epoch: 6| Step: 10
Training loss: 3.2540180683135986
Validation loss: 2.434617688578944

Epoch: 6| Step: 11
Training loss: 2.043753147125244
Validation loss: 2.4422164873410295

Epoch: 6| Step: 12
Training loss: 2.066258430480957
Validation loss: 2.440702494754586

Epoch: 6| Step: 13
Training loss: 2.3383989334106445
Validation loss: 2.4383302914199008

Epoch: 174| Step: 0
Training loss: 3.5309605598449707
Validation loss: 2.4418259551448207

Epoch: 6| Step: 1
Training loss: 2.7492928504943848
Validation loss: 2.437905352602723

Epoch: 6| Step: 2
Training loss: 2.604926586151123
Validation loss: 2.440366739867836

Epoch: 6| Step: 3
Training loss: 2.6717984676361084
Validation loss: 2.4408184994933424

Epoch: 6| Step: 4
Training loss: 1.9792239665985107
Validation loss: 2.446217126743768

Epoch: 6| Step: 5
Training loss: 2.2860732078552246
Validation loss: 2.4437241964442755

Epoch: 6| Step: 6
Training loss: 3.0670955181121826
Validation loss: 2.445067080118323

Epoch: 6| Step: 7
Training loss: 2.5021092891693115
Validation loss: 2.4492281354883665

Epoch: 6| Step: 8
Training loss: 2.9414753913879395
Validation loss: 2.449504934331422

Epoch: 6| Step: 9
Training loss: 2.434750556945801
Validation loss: 2.462080805532394

Epoch: 6| Step: 10
Training loss: 2.1209728717803955
Validation loss: 2.466995208494125

Epoch: 6| Step: 11
Training loss: 2.4627909660339355
Validation loss: 2.467056997360722

Epoch: 6| Step: 12
Training loss: 2.3341753482818604
Validation loss: 2.4750758627409577

Epoch: 6| Step: 13
Training loss: 3.4598708152770996
Validation loss: 2.478612592143397

Epoch: 175| Step: 0
Training loss: 2.436941623687744
Validation loss: 2.469458882526685

Epoch: 6| Step: 1
Training loss: 2.1278555393218994
Validation loss: 2.4545822451191563

Epoch: 6| Step: 2
Training loss: 3.02219557762146
Validation loss: 2.4465090126119633

Epoch: 6| Step: 3
Training loss: 2.291496753692627
Validation loss: 2.4452989793592885

Epoch: 6| Step: 4
Training loss: 2.680401563644409
Validation loss: 2.4437406537353352

Epoch: 6| Step: 5
Training loss: 3.02664852142334
Validation loss: 2.44498549994602

Epoch: 6| Step: 6
Training loss: 2.2269959449768066
Validation loss: 2.451364953030822

Epoch: 6| Step: 7
Training loss: 3.1852409839630127
Validation loss: 2.4451568485588155

Epoch: 6| Step: 8
Training loss: 2.4330129623413086
Validation loss: 2.442911583890197

Epoch: 6| Step: 9
Training loss: 3.2909209728240967
Validation loss: 2.4555517883710962

Epoch: 6| Step: 10
Training loss: 3.0310182571411133
Validation loss: 2.4459328702701035

Epoch: 6| Step: 11
Training loss: 2.2340381145477295
Validation loss: 2.444403591976371

Epoch: 6| Step: 12
Training loss: 2.4882047176361084
Validation loss: 2.4448924731182795

Epoch: 6| Step: 13
Training loss: 2.1663129329681396
Validation loss: 2.44843961346534

Epoch: 176| Step: 0
Training loss: 2.556386947631836
Validation loss: 2.4445609790022655

Epoch: 6| Step: 1
Training loss: 2.5603246688842773
Validation loss: 2.446133480277113

Epoch: 6| Step: 2
Training loss: 2.4530084133148193
Validation loss: 2.4479509117782756

Epoch: 6| Step: 3
Training loss: 3.3782100677490234
Validation loss: 2.442200476123441

Epoch: 6| Step: 4
Training loss: 3.0286197662353516
Validation loss: 2.443416546749812

Epoch: 6| Step: 5
Training loss: 2.6144490242004395
Validation loss: 2.4464450651599514

Epoch: 6| Step: 6
Training loss: 2.8624825477600098
Validation loss: 2.4423186573930966

Epoch: 6| Step: 7
Training loss: 2.9285836219787598
Validation loss: 2.435260654777609

Epoch: 6| Step: 8
Training loss: 2.4219179153442383
Validation loss: 2.442821148903139

Epoch: 6| Step: 9
Training loss: 1.9749174118041992
Validation loss: 2.4400718289036907

Epoch: 6| Step: 10
Training loss: 2.1378302574157715
Validation loss: 2.4466201784790202

Epoch: 6| Step: 11
Training loss: 1.8433705568313599
Validation loss: 2.442570827340567

Epoch: 6| Step: 12
Training loss: 3.068861246109009
Validation loss: 2.451400577381093

Epoch: 6| Step: 13
Training loss: 3.109369993209839
Validation loss: 2.458025311910978

Epoch: 177| Step: 0
Training loss: 2.6669297218322754
Validation loss: 2.4631781142245055

Epoch: 6| Step: 1
Training loss: 2.400876760482788
Validation loss: 2.4691182900500555

Epoch: 6| Step: 2
Training loss: 2.0521674156188965
Validation loss: 2.474208306240779

Epoch: 6| Step: 3
Training loss: 2.4873616695404053
Validation loss: 2.4719472956913773

Epoch: 6| Step: 4
Training loss: 2.759917974472046
Validation loss: 2.4779395570037184

Epoch: 6| Step: 5
Training loss: 2.5854015350341797
Validation loss: 2.4814412081113426

Epoch: 6| Step: 6
Training loss: 2.738593101501465
Validation loss: 2.484463502002019

Epoch: 6| Step: 7
Training loss: 2.7060396671295166
Validation loss: 2.4828265764380015

Epoch: 6| Step: 8
Training loss: 3.341453790664673
Validation loss: 2.484314441680908

Epoch: 6| Step: 9
Training loss: 2.555055618286133
Validation loss: 2.483229519218527

Epoch: 6| Step: 10
Training loss: 2.432356834411621
Validation loss: 2.4684353208029144

Epoch: 6| Step: 11
Training loss: 2.7854018211364746
Validation loss: 2.4705860691685833

Epoch: 6| Step: 12
Training loss: 2.623668909072876
Validation loss: 2.466395749840685

Epoch: 6| Step: 13
Training loss: 3.0042874813079834
Validation loss: 2.4611114353261967

Epoch: 178| Step: 0
Training loss: 2.128849983215332
Validation loss: 2.458889592078424

Epoch: 6| Step: 1
Training loss: 2.3340182304382324
Validation loss: 2.458170610089456

Epoch: 6| Step: 2
Training loss: 2.818664073944092
Validation loss: 2.4480657731333086

Epoch: 6| Step: 3
Training loss: 3.1963798999786377
Validation loss: 2.4454599734275573

Epoch: 6| Step: 4
Training loss: 2.0610790252685547
Validation loss: 2.438611771470757

Epoch: 6| Step: 5
Training loss: 2.908844470977783
Validation loss: 2.4297336506587204

Epoch: 6| Step: 6
Training loss: 2.159824848175049
Validation loss: 2.4247385994080575

Epoch: 6| Step: 7
Training loss: 2.550480842590332
Validation loss: 2.4261074245616956

Epoch: 6| Step: 8
Training loss: 2.716297149658203
Validation loss: 2.4277409251018236

Epoch: 6| Step: 9
Training loss: 3.0834407806396484
Validation loss: 2.431696827693652

Epoch: 6| Step: 10
Training loss: 3.0290865898132324
Validation loss: 2.427252213160197

Epoch: 6| Step: 11
Training loss: 2.3023974895477295
Validation loss: 2.4327723313403387

Epoch: 6| Step: 12
Training loss: 2.9182400703430176
Validation loss: 2.4249983244044806

Epoch: 6| Step: 13
Training loss: 2.4817264080047607
Validation loss: 2.4339500268300376

Epoch: 179| Step: 0
Training loss: 3.1646533012390137
Validation loss: 2.433676673519996

Epoch: 6| Step: 1
Training loss: 2.4166884422302246
Validation loss: 2.4337556144242645

Epoch: 6| Step: 2
Training loss: 2.708785057067871
Validation loss: 2.437509354724679

Epoch: 6| Step: 3
Training loss: 3.2986743450164795
Validation loss: 2.4375140359324794

Epoch: 6| Step: 4
Training loss: 2.9454236030578613
Validation loss: 2.4313201827387654

Epoch: 6| Step: 5
Training loss: 3.212852716445923
Validation loss: 2.4362385247343328

Epoch: 6| Step: 6
Training loss: 2.3020825386047363
Validation loss: 2.436731298764547

Epoch: 6| Step: 7
Training loss: 2.485595226287842
Validation loss: 2.434047609247187

Epoch: 6| Step: 8
Training loss: 2.092597007751465
Validation loss: 2.440018138577861

Epoch: 6| Step: 9
Training loss: 2.162297248840332
Validation loss: 2.4419138636640323

Epoch: 6| Step: 10
Training loss: 2.5512373447418213
Validation loss: 2.4499666767735637

Epoch: 6| Step: 11
Training loss: 2.564081907272339
Validation loss: 2.445454864091771

Epoch: 6| Step: 12
Training loss: 2.702585220336914
Validation loss: 2.446542509140507

Epoch: 6| Step: 13
Training loss: 1.7985727787017822
Validation loss: 2.446357701414375

Epoch: 180| Step: 0
Training loss: 3.2845964431762695
Validation loss: 2.4434160186398413

Epoch: 6| Step: 1
Training loss: 2.0673470497131348
Validation loss: 2.4435544770251036

Epoch: 6| Step: 2
Training loss: 2.980955123901367
Validation loss: 2.443898052297613

Epoch: 6| Step: 3
Training loss: 2.7200675010681152
Validation loss: 2.44257612382212

Epoch: 6| Step: 4
Training loss: 2.08842134475708
Validation loss: 2.4522549593320457

Epoch: 6| Step: 5
Training loss: 3.250098705291748
Validation loss: 2.4519010231059086

Epoch: 6| Step: 6
Training loss: 2.5970141887664795
Validation loss: 2.4533947706222534

Epoch: 6| Step: 7
Training loss: 2.8913800716400146
Validation loss: 2.458665294031943

Epoch: 6| Step: 8
Training loss: 2.165097713470459
Validation loss: 2.449317096382059

Epoch: 6| Step: 9
Training loss: 2.1379408836364746
Validation loss: 2.4513580811920987

Epoch: 6| Step: 10
Training loss: 3.5020673274993896
Validation loss: 2.4458498647136073

Epoch: 6| Step: 11
Training loss: 1.7983827590942383
Validation loss: 2.44418405717419

Epoch: 6| Step: 12
Training loss: 2.415207862854004
Validation loss: 2.444938895522907

Epoch: 6| Step: 13
Training loss: 2.86676287651062
Validation loss: 2.4475581402419717

Epoch: 181| Step: 0
Training loss: 2.0015809535980225
Validation loss: 2.4457625573681248

Epoch: 6| Step: 1
Training loss: 1.8960422277450562
Validation loss: 2.440740164890084

Epoch: 6| Step: 2
Training loss: 2.5711824893951416
Validation loss: 2.442847546710763

Epoch: 6| Step: 3
Training loss: 3.0087854862213135
Validation loss: 2.4394457237694853

Epoch: 6| Step: 4
Training loss: 2.800816297531128
Validation loss: 2.4442604703287922

Epoch: 6| Step: 5
Training loss: 2.584153652191162
Validation loss: 2.4472966886335805

Epoch: 6| Step: 6
Training loss: 2.785794734954834
Validation loss: 2.434662590744675

Epoch: 6| Step: 7
Training loss: 2.8589956760406494
Validation loss: 2.4325407807544996

Epoch: 6| Step: 8
Training loss: 2.1564183235168457
Validation loss: 2.434290519324682

Epoch: 6| Step: 9
Training loss: 2.7224297523498535
Validation loss: 2.4360824708015687

Epoch: 6| Step: 10
Training loss: 2.671375274658203
Validation loss: 2.4293609947286625

Epoch: 6| Step: 11
Training loss: 3.153996229171753
Validation loss: 2.4286742184751775

Epoch: 6| Step: 12
Training loss: 3.6132571697235107
Validation loss: 2.434206247329712

Epoch: 6| Step: 13
Training loss: 1.201802134513855
Validation loss: 2.4291171514859764

Epoch: 182| Step: 0
Training loss: 2.696460247039795
Validation loss: 2.430576632099767

Epoch: 6| Step: 1
Training loss: 1.7117034196853638
Validation loss: 2.418396444730861

Epoch: 6| Step: 2
Training loss: 2.9979608058929443
Validation loss: 2.427625815073649

Epoch: 6| Step: 3
Training loss: 2.8792974948883057
Validation loss: 2.4272958437601724

Epoch: 6| Step: 4
Training loss: 2.2955386638641357
Validation loss: 2.4337488733312136

Epoch: 6| Step: 5
Training loss: 2.1817140579223633
Validation loss: 2.42651968361229

Epoch: 6| Step: 6
Training loss: 2.8484244346618652
Validation loss: 2.431639245761338

Epoch: 6| Step: 7
Training loss: 3.1545591354370117
Validation loss: 2.434738887253628

Epoch: 6| Step: 8
Training loss: 2.7950143814086914
Validation loss: 2.433233138053648

Epoch: 6| Step: 9
Training loss: 2.1822521686553955
Validation loss: 2.4386457858547086

Epoch: 6| Step: 10
Training loss: 2.264425277709961
Validation loss: 2.4344501802998204

Epoch: 6| Step: 11
Training loss: 3.0884451866149902
Validation loss: 2.441563775462489

Epoch: 6| Step: 12
Training loss: 2.4591784477233887
Validation loss: 2.433892185970019

Epoch: 6| Step: 13
Training loss: 3.465752124786377
Validation loss: 2.4421767765475857

Epoch: 183| Step: 0
Training loss: 2.308407783508301
Validation loss: 2.4443300334356164

Epoch: 6| Step: 1
Training loss: 2.7561886310577393
Validation loss: 2.437329405097551

Epoch: 6| Step: 2
Training loss: 2.5646767616271973
Validation loss: 2.43325464699858

Epoch: 6| Step: 3
Training loss: 2.80617094039917
Validation loss: 2.4325025773817495

Epoch: 6| Step: 4
Training loss: 3.1266846656799316
Validation loss: 2.4269514314589964

Epoch: 6| Step: 5
Training loss: 1.9032433032989502
Validation loss: 2.4229972875246437

Epoch: 6| Step: 6
Training loss: 2.5135669708251953
Validation loss: 2.421690417874244

Epoch: 6| Step: 7
Training loss: 3.0824875831604004
Validation loss: 2.4173939304967083

Epoch: 6| Step: 8
Training loss: 1.8363261222839355
Validation loss: 2.421680946503916

Epoch: 6| Step: 9
Training loss: 2.837466239929199
Validation loss: 2.4313319729220484

Epoch: 6| Step: 10
Training loss: 2.203094482421875
Validation loss: 2.429531071775703

Epoch: 6| Step: 11
Training loss: 2.626760959625244
Validation loss: 2.4317764018171575

Epoch: 6| Step: 12
Training loss: 2.9117064476013184
Validation loss: 2.445339887372909

Epoch: 6| Step: 13
Training loss: 3.659799337387085
Validation loss: 2.4538826839898222

Epoch: 184| Step: 0
Training loss: 3.414483070373535
Validation loss: 2.463720121691304

Epoch: 6| Step: 1
Training loss: 3.2568464279174805
Validation loss: 2.474617055667344

Epoch: 6| Step: 2
Training loss: 1.6782793998718262
Validation loss: 2.4843098758369364

Epoch: 6| Step: 3
Training loss: 1.7092454433441162
Validation loss: 2.4726641024312666

Epoch: 6| Step: 4
Training loss: 2.5185320377349854
Validation loss: 2.4718543355182936

Epoch: 6| Step: 5
Training loss: 3.3892412185668945
Validation loss: 2.4642111562913462

Epoch: 6| Step: 6
Training loss: 2.321333408355713
Validation loss: 2.453364767054076

Epoch: 6| Step: 7
Training loss: 1.5692133903503418
Validation loss: 2.4524144331614175

Epoch: 6| Step: 8
Training loss: 2.102587938308716
Validation loss: 2.450333995203818

Epoch: 6| Step: 9
Training loss: 2.505408763885498
Validation loss: 2.44527869455276

Epoch: 6| Step: 10
Training loss: 3.3954434394836426
Validation loss: 2.4453790418563353

Epoch: 6| Step: 11
Training loss: 3.232360363006592
Validation loss: 2.437473581683251

Epoch: 6| Step: 12
Training loss: 3.3448522090911865
Validation loss: 2.4421034013071368

Epoch: 6| Step: 13
Training loss: 2.133927583694458
Validation loss: 2.441088622616183

Epoch: 185| Step: 0
Training loss: 2.477391242980957
Validation loss: 2.4452445558322373

Epoch: 6| Step: 1
Training loss: 2.3355979919433594
Validation loss: 2.4464544480846775

Epoch: 6| Step: 2
Training loss: 2.4551758766174316
Validation loss: 2.444873017649497

Epoch: 6| Step: 3
Training loss: 2.919517755508423
Validation loss: 2.4411261671332904

Epoch: 6| Step: 4
Training loss: 3.0625553131103516
Validation loss: 2.441946775682511

Epoch: 6| Step: 5
Training loss: 2.8529441356658936
Validation loss: 2.4380076623732045

Epoch: 6| Step: 6
Training loss: 2.872870445251465
Validation loss: 2.4341049501972813

Epoch: 6| Step: 7
Training loss: 2.8234407901763916
Validation loss: 2.424753581323931

Epoch: 6| Step: 8
Training loss: 3.455709934234619
Validation loss: 2.4245383483107372

Epoch: 6| Step: 9
Training loss: 2.1481921672821045
Validation loss: 2.4315573169339086

Epoch: 6| Step: 10
Training loss: 1.9385688304901123
Validation loss: 2.4263886097938783

Epoch: 6| Step: 11
Training loss: 2.519650459289551
Validation loss: 2.4338861562872447

Epoch: 6| Step: 12
Training loss: 2.122927188873291
Validation loss: 2.4256283724179832

Epoch: 6| Step: 13
Training loss: 2.509387969970703
Validation loss: 2.4300987566671064

Epoch: 186| Step: 0
Training loss: 2.0197372436523438
Validation loss: 2.4129962792960544

Epoch: 6| Step: 1
Training loss: 2.5876457691192627
Validation loss: 2.4138246761855258

Epoch: 6| Step: 2
Training loss: 2.045933246612549
Validation loss: 2.4126882501827773

Epoch: 6| Step: 3
Training loss: 2.9252452850341797
Validation loss: 2.404545214868361

Epoch: 6| Step: 4
Training loss: 3.1114859580993652
Validation loss: 2.411430319150289

Epoch: 6| Step: 5
Training loss: 2.7403640747070312
Validation loss: 2.4050743041499967

Epoch: 6| Step: 6
Training loss: 3.1210060119628906
Validation loss: 2.404612454034949

Epoch: 6| Step: 7
Training loss: 2.558349132537842
Validation loss: 2.4038857183148785

Epoch: 6| Step: 8
Training loss: 2.844290018081665
Validation loss: 2.4077596920792774

Epoch: 6| Step: 9
Training loss: 2.771437406539917
Validation loss: 2.410629521134079

Epoch: 6| Step: 10
Training loss: 2.74021315574646
Validation loss: 2.405171307184363

Epoch: 6| Step: 11
Training loss: 3.0594286918640137
Validation loss: 2.4068761461524555

Epoch: 6| Step: 12
Training loss: 2.1685996055603027
Validation loss: 2.4051297710787867

Epoch: 6| Step: 13
Training loss: 1.4636321067810059
Validation loss: 2.4041658473271195

Epoch: 187| Step: 0
Training loss: 2.565027952194214
Validation loss: 2.4078216604007188

Epoch: 6| Step: 1
Training loss: 2.804387092590332
Validation loss: 2.407643251521613

Epoch: 6| Step: 2
Training loss: 2.899505376815796
Validation loss: 2.4084913089711177

Epoch: 6| Step: 3
Training loss: 2.6022164821624756
Validation loss: 2.403753683131228

Epoch: 6| Step: 4
Training loss: 2.5167758464813232
Validation loss: 2.4009707384212042

Epoch: 6| Step: 5
Training loss: 3.5335428714752197
Validation loss: 2.402388075346588

Epoch: 6| Step: 6
Training loss: 2.456918954849243
Validation loss: 2.4066216766193347

Epoch: 6| Step: 7
Training loss: 2.433624029159546
Validation loss: 2.406737199393652

Epoch: 6| Step: 8
Training loss: 2.767411231994629
Validation loss: 2.402966853111021

Epoch: 6| Step: 9
Training loss: 3.1708579063415527
Validation loss: 2.3970173123062297

Epoch: 6| Step: 10
Training loss: 1.2093887329101562
Validation loss: 2.409958900943879

Epoch: 6| Step: 11
Training loss: 2.9739246368408203
Validation loss: 2.40800920865869

Epoch: 6| Step: 12
Training loss: 2.159280776977539
Validation loss: 2.402877389743764

Epoch: 6| Step: 13
Training loss: 2.5899064540863037
Validation loss: 2.3988645474116006

Epoch: 188| Step: 0
Training loss: 3.167534351348877
Validation loss: 2.408603145230201

Epoch: 6| Step: 1
Training loss: 3.0779292583465576
Validation loss: 2.4148421800264748

Epoch: 6| Step: 2
Training loss: 3.0145230293273926
Validation loss: 2.417501644421649

Epoch: 6| Step: 3
Training loss: 1.9700698852539062
Validation loss: 2.4213826117977018

Epoch: 6| Step: 4
Training loss: 1.9096083641052246
Validation loss: 2.4311026270671556

Epoch: 6| Step: 5
Training loss: 2.3903965950012207
Validation loss: 2.434724982066821

Epoch: 6| Step: 6
Training loss: 2.8894567489624023
Validation loss: 2.439721030573691

Epoch: 6| Step: 7
Training loss: 2.183196544647217
Validation loss: 2.445746848660131

Epoch: 6| Step: 8
Training loss: 2.967799186706543
Validation loss: 2.440480693694084

Epoch: 6| Step: 9
Training loss: 2.2908668518066406
Validation loss: 2.4240506131161927

Epoch: 6| Step: 10
Training loss: 3.0361227989196777
Validation loss: 2.41984724485746

Epoch: 6| Step: 11
Training loss: 2.825894355773926
Validation loss: 2.4050716815456266

Epoch: 6| Step: 12
Training loss: 2.495490074157715
Validation loss: 2.4071451438370572

Epoch: 6| Step: 13
Training loss: 2.547762632369995
Validation loss: 2.4004068066996913

Epoch: 189| Step: 0
Training loss: 2.4505460262298584
Validation loss: 2.405290206273397

Epoch: 6| Step: 1
Training loss: 2.5362939834594727
Validation loss: 2.4050430174796813

Epoch: 6| Step: 2
Training loss: 2.4413416385650635
Validation loss: 2.413443047513244

Epoch: 6| Step: 3
Training loss: 2.4287848472595215
Validation loss: 2.4109223658038723

Epoch: 6| Step: 4
Training loss: 3.1576294898986816
Validation loss: 2.411108893732871

Epoch: 6| Step: 5
Training loss: 2.7028493881225586
Validation loss: 2.4223914454060216

Epoch: 6| Step: 6
Training loss: 2.4411725997924805
Validation loss: 2.4151335608574653

Epoch: 6| Step: 7
Training loss: 2.687070846557617
Validation loss: 2.40801933247556

Epoch: 6| Step: 8
Training loss: 1.9669902324676514
Validation loss: 2.4022040444035686

Epoch: 6| Step: 9
Training loss: 2.861206531524658
Validation loss: 2.4007019919733845

Epoch: 6| Step: 10
Training loss: 1.5913746356964111
Validation loss: 2.393815907098914

Epoch: 6| Step: 11
Training loss: 3.5520455837249756
Validation loss: 2.3980391666453373

Epoch: 6| Step: 12
Training loss: 3.3503804206848145
Validation loss: 2.4074312102410103

Epoch: 6| Step: 13
Training loss: 2.580684185028076
Validation loss: 2.4015447196140083

Epoch: 190| Step: 0
Training loss: 2.4139490127563477
Validation loss: 2.4031262038856425

Epoch: 6| Step: 1
Training loss: 2.823390483856201
Validation loss: 2.40259983206308

Epoch: 6| Step: 2
Training loss: 2.503591537475586
Validation loss: 2.4079764940405406

Epoch: 6| Step: 3
Training loss: 2.5869131088256836
Validation loss: 2.4079045967389177

Epoch: 6| Step: 4
Training loss: 1.9579427242279053
Validation loss: 2.4091060341045423

Epoch: 6| Step: 5
Training loss: 2.1154544353485107
Validation loss: 2.4053040858237975

Epoch: 6| Step: 6
Training loss: 2.482827663421631
Validation loss: 2.40505047511029

Epoch: 6| Step: 7
Training loss: 3.154604434967041
Validation loss: 2.4079520394725185

Epoch: 6| Step: 8
Training loss: 2.518233299255371
Validation loss: 2.407567908686976

Epoch: 6| Step: 9
Training loss: 3.0132365226745605
Validation loss: 2.404904737267443

Epoch: 6| Step: 10
Training loss: 2.2719762325286865
Validation loss: 2.412049639609552

Epoch: 6| Step: 11
Training loss: 3.257997751235962
Validation loss: 2.411823841833299

Epoch: 6| Step: 12
Training loss: 3.1502902507781982
Validation loss: 2.42170823261302

Epoch: 6| Step: 13
Training loss: 2.0828843116760254
Validation loss: 2.4415863329364407

Epoch: 191| Step: 0
Training loss: 2.637968063354492
Validation loss: 2.4506319492093978

Epoch: 6| Step: 1
Training loss: 3.3427467346191406
Validation loss: 2.470804947678761

Epoch: 6| Step: 2
Training loss: 3.204775810241699
Validation loss: 2.48112855931764

Epoch: 6| Step: 3
Training loss: 2.3920764923095703
Validation loss: 2.479044275899087

Epoch: 6| Step: 4
Training loss: 2.8357362747192383
Validation loss: 2.47103811079456

Epoch: 6| Step: 5
Training loss: 2.2654242515563965
Validation loss: 2.474299248828683

Epoch: 6| Step: 6
Training loss: 2.084324836730957
Validation loss: 2.482893318258306

Epoch: 6| Step: 7
Training loss: 2.4574973583221436
Validation loss: 2.4821314375887633

Epoch: 6| Step: 8
Training loss: 2.498500108718872
Validation loss: 2.4858980050650974

Epoch: 6| Step: 9
Training loss: 2.3725829124450684
Validation loss: 2.4900553405925794

Epoch: 6| Step: 10
Training loss: 3.1320927143096924
Validation loss: 2.4869673021378054

Epoch: 6| Step: 11
Training loss: 2.756958484649658
Validation loss: 2.4831309497997327

Epoch: 6| Step: 12
Training loss: 2.6440248489379883
Validation loss: 2.479213063434888

Epoch: 6| Step: 13
Training loss: 2.333862781524658
Validation loss: 2.485351267681327

Epoch: 192| Step: 0
Training loss: 1.9025542736053467
Validation loss: 2.486540586717667

Epoch: 6| Step: 1
Training loss: 2.9332258701324463
Validation loss: 2.4823010249804427

Epoch: 6| Step: 2
Training loss: 2.9929614067077637
Validation loss: 2.4815951278132777

Epoch: 6| Step: 3
Training loss: 1.7460980415344238
Validation loss: 2.4809102909539336

Epoch: 6| Step: 4
Training loss: 3.203507900238037
Validation loss: 2.480840262546334

Epoch: 6| Step: 5
Training loss: 3.1480965614318848
Validation loss: 2.4839746336783133

Epoch: 6| Step: 6
Training loss: 3.617384910583496
Validation loss: 2.4779915117448374

Epoch: 6| Step: 7
Training loss: 3.4287161827087402
Validation loss: 2.4709413730969993

Epoch: 6| Step: 8
Training loss: 2.6526458263397217
Validation loss: 2.474497725886683

Epoch: 6| Step: 9
Training loss: 2.5611350536346436
Validation loss: 2.4821327296636437

Epoch: 6| Step: 10
Training loss: 2.1768174171447754
Validation loss: 2.483782688776652

Epoch: 6| Step: 11
Training loss: 1.7760392427444458
Validation loss: 2.4783250875370477

Epoch: 6| Step: 12
Training loss: 2.1811161041259766
Validation loss: 2.4811509091367006

Epoch: 6| Step: 13
Training loss: 2.847553253173828
Validation loss: 2.486677167236164

Epoch: 193| Step: 0
Training loss: 3.186082124710083
Validation loss: 2.482602204045942

Epoch: 6| Step: 1
Training loss: 3.160094976425171
Validation loss: 2.48505836661144

Epoch: 6| Step: 2
Training loss: 2.5629520416259766
Validation loss: 2.478221244709466

Epoch: 6| Step: 3
Training loss: 1.8360052108764648
Validation loss: 2.4858103208644415

Epoch: 6| Step: 4
Training loss: 2.4515421390533447
Validation loss: 2.4761409913339922

Epoch: 6| Step: 5
Training loss: 1.7882453203201294
Validation loss: 2.4755893445784047

Epoch: 6| Step: 6
Training loss: 1.9305508136749268
Validation loss: 2.4783084956548547

Epoch: 6| Step: 7
Training loss: 2.66428279876709
Validation loss: 2.478905926468552

Epoch: 6| Step: 8
Training loss: 2.6351239681243896
Validation loss: 2.4797511075132634

Epoch: 6| Step: 9
Training loss: 3.3430192470550537
Validation loss: 2.483574769830191

Epoch: 6| Step: 10
Training loss: 3.055988311767578
Validation loss: 2.4789352929720314

Epoch: 6| Step: 11
Training loss: 2.400570869445801
Validation loss: 2.477574045940112

Epoch: 6| Step: 12
Training loss: 2.874532699584961
Validation loss: 2.4750045781494467

Epoch: 6| Step: 13
Training loss: 3.5789408683776855
Validation loss: 2.4795096612745717

Epoch: 194| Step: 0
Training loss: 2.256014823913574
Validation loss: 2.4828521615715435

Epoch: 6| Step: 1
Training loss: 2.4409589767456055
Validation loss: 2.483188554804812

Epoch: 6| Step: 2
Training loss: 3.0311179161071777
Validation loss: 2.4858197114800893

Epoch: 6| Step: 3
Training loss: 3.2367827892303467
Validation loss: 2.479960641553325

Epoch: 6| Step: 4
Training loss: 2.5468456745147705
Validation loss: 2.4851381035261255

Epoch: 6| Step: 5
Training loss: 2.777055263519287
Validation loss: 2.4832008192616124

Epoch: 6| Step: 6
Training loss: 3.075822114944458
Validation loss: 2.4797819455464682

Epoch: 6| Step: 7
Training loss: 2.100247859954834
Validation loss: 2.4741591663770777

Epoch: 6| Step: 8
Training loss: 2.797623634338379
Validation loss: 2.471130278802687

Epoch: 6| Step: 9
Training loss: 2.744384527206421
Validation loss: 2.470462704217562

Epoch: 6| Step: 10
Training loss: 2.0208611488342285
Validation loss: 2.4771606665785595

Epoch: 6| Step: 11
Training loss: 2.3333330154418945
Validation loss: 2.4703425207445697

Epoch: 6| Step: 12
Training loss: 2.7373969554901123
Validation loss: 2.4720917183865785

Epoch: 6| Step: 13
Training loss: 3.066157579421997
Validation loss: 2.4661450232228925

Epoch: 195| Step: 0
Training loss: 2.599020481109619
Validation loss: 2.470604068489485

Epoch: 6| Step: 1
Training loss: 2.632624626159668
Validation loss: 2.4680540510403213

Epoch: 6| Step: 2
Training loss: 2.599086284637451
Validation loss: 2.4534986301134993

Epoch: 6| Step: 3
Training loss: 2.911825656890869
Validation loss: 2.4471265513409852

Epoch: 6| Step: 4
Training loss: 2.7382893562316895
Validation loss: 2.437324082979592

Epoch: 6| Step: 5
Training loss: 2.413776159286499
Validation loss: 2.426805470579414

Epoch: 6| Step: 6
Training loss: 2.5005078315734863
Validation loss: 2.4243769863600373

Epoch: 6| Step: 7
Training loss: 2.7851555347442627
Validation loss: 2.4198613717991817

Epoch: 6| Step: 8
Training loss: 2.506434440612793
Validation loss: 2.4135295037300355

Epoch: 6| Step: 9
Training loss: 2.3444974422454834
Validation loss: 2.4105636714607157

Epoch: 6| Step: 10
Training loss: 2.509944438934326
Validation loss: 2.3995817797158354

Epoch: 6| Step: 11
Training loss: 2.4677369594573975
Validation loss: 2.39464408095165

Epoch: 6| Step: 12
Training loss: 2.9851813316345215
Validation loss: 2.3910547943525415

Epoch: 6| Step: 13
Training loss: 2.7448222637176514
Validation loss: 2.3849005083883963

Epoch: 196| Step: 0
Training loss: 1.6596158742904663
Validation loss: 2.3794399922893894

Epoch: 6| Step: 1
Training loss: 2.9048049449920654
Validation loss: 2.394405129135296

Epoch: 6| Step: 2
Training loss: 2.283130645751953
Validation loss: 2.3893145335617887

Epoch: 6| Step: 3
Training loss: 2.4087142944335938
Validation loss: 2.3873002759871946

Epoch: 6| Step: 4
Training loss: 2.267055034637451
Validation loss: 2.3868152351789576

Epoch: 6| Step: 5
Training loss: 3.0533406734466553
Validation loss: 2.385903304623019

Epoch: 6| Step: 6
Training loss: 2.9358272552490234
Validation loss: 2.394408356758856

Epoch: 6| Step: 7
Training loss: 2.4996957778930664
Validation loss: 2.4002496324559695

Epoch: 6| Step: 8
Training loss: 2.8282876014709473
Validation loss: 2.4021151591372747

Epoch: 6| Step: 9
Training loss: 2.580667734146118
Validation loss: 2.401308275038196

Epoch: 6| Step: 10
Training loss: 3.328415870666504
Validation loss: 2.4167964637920423

Epoch: 6| Step: 11
Training loss: 2.5435638427734375
Validation loss: 2.4196390900560605

Epoch: 6| Step: 12
Training loss: 2.4402358531951904
Validation loss: 2.4184664654475387

Epoch: 6| Step: 13
Training loss: 3.1673810482025146
Validation loss: 2.4192109518153693

Epoch: 197| Step: 0
Training loss: 3.151005268096924
Validation loss: 2.4120873020541285

Epoch: 6| Step: 1
Training loss: 2.7597503662109375
Validation loss: 2.415638067389047

Epoch: 6| Step: 2
Training loss: 2.4487404823303223
Validation loss: 2.4151319047456146

Epoch: 6| Step: 3
Training loss: 1.7734967470169067
Validation loss: 2.4182572749353226

Epoch: 6| Step: 4
Training loss: 2.0360679626464844
Validation loss: 2.4219970395488124

Epoch: 6| Step: 5
Training loss: 3.1581544876098633
Validation loss: 2.4197516518254436

Epoch: 6| Step: 6
Training loss: 2.6648106575012207
Validation loss: 2.4268001715342202

Epoch: 6| Step: 7
Training loss: 3.1434760093688965
Validation loss: 2.425227503622732

Epoch: 6| Step: 8
Training loss: 2.456192970275879
Validation loss: 2.4206312600002495

Epoch: 6| Step: 9
Training loss: 2.707766056060791
Validation loss: 2.4249724521431872

Epoch: 6| Step: 10
Training loss: 3.121583938598633
Validation loss: 2.420609389581988

Epoch: 6| Step: 11
Training loss: 2.4043116569519043
Validation loss: 2.4195666108080136

Epoch: 6| Step: 12
Training loss: 2.6337506771087646
Validation loss: 2.4142210637369463

Epoch: 6| Step: 13
Training loss: 1.914745569229126
Validation loss: 2.409098115018619

Epoch: 198| Step: 0
Training loss: 2.8296236991882324
Validation loss: 2.412985991406184

Epoch: 6| Step: 1
Training loss: 3.3924970626831055
Validation loss: 2.422245479399158

Epoch: 6| Step: 2
Training loss: 2.1360912322998047
Validation loss: 2.4239375681005497

Epoch: 6| Step: 3
Training loss: 2.3032772541046143
Validation loss: 2.4250295521110616

Epoch: 6| Step: 4
Training loss: 2.2269375324249268
Validation loss: 2.4205417966329925

Epoch: 6| Step: 5
Training loss: 1.6342957019805908
Validation loss: 2.412676047253352

Epoch: 6| Step: 6
Training loss: 2.770341634750366
Validation loss: 2.4144767151083997

Epoch: 6| Step: 7
Training loss: 2.2746729850769043
Validation loss: 2.4069302005152546

Epoch: 6| Step: 8
Training loss: 2.985976219177246
Validation loss: 2.400756037363442

Epoch: 6| Step: 9
Training loss: 2.7907001972198486
Validation loss: 2.385405630193731

Epoch: 6| Step: 10
Training loss: 2.5655856132507324
Validation loss: 2.3813524758943947

Epoch: 6| Step: 11
Training loss: 3.2440810203552246
Validation loss: 2.3827854817913425

Epoch: 6| Step: 12
Training loss: 2.9938697814941406
Validation loss: 2.37799241722271

Epoch: 6| Step: 13
Training loss: 2.3267016410827637
Validation loss: 2.390210751564272

Epoch: 199| Step: 0
Training loss: 2.444936752319336
Validation loss: 2.397473294247863

Epoch: 6| Step: 1
Training loss: 2.1995129585266113
Validation loss: 2.4163530077985538

Epoch: 6| Step: 2
Training loss: 2.50252628326416
Validation loss: 2.4316351721363683

Epoch: 6| Step: 3
Training loss: 2.220822334289551
Validation loss: 2.445364946960121

Epoch: 6| Step: 4
Training loss: 2.5698652267456055
Validation loss: 2.4626796425029798

Epoch: 6| Step: 5
Training loss: 2.6064529418945312
Validation loss: 2.463867184936359

Epoch: 6| Step: 6
Training loss: 2.769948959350586
Validation loss: 2.485349985861009

Epoch: 6| Step: 7
Training loss: 2.1612768173217773
Validation loss: 2.4792190726085375

Epoch: 6| Step: 8
Training loss: 2.5523622035980225
Validation loss: 2.477868103211926

Epoch: 6| Step: 9
Training loss: 2.960623264312744
Validation loss: 2.4480838288543043

Epoch: 6| Step: 10
Training loss: 2.731755256652832
Validation loss: 2.422753828828053

Epoch: 6| Step: 11
Training loss: 3.2052247524261475
Validation loss: 2.401078595910021

Epoch: 6| Step: 12
Training loss: 3.3339085578918457
Validation loss: 2.3982968817475023

Epoch: 6| Step: 13
Training loss: 3.151279926300049
Validation loss: 2.3974493370261243

Epoch: 200| Step: 0
Training loss: 2.815540313720703
Validation loss: 2.4085280972142376

Epoch: 6| Step: 1
Training loss: 2.9641058444976807
Validation loss: 2.4173606954595095

Epoch: 6| Step: 2
Training loss: 3.1336545944213867
Validation loss: 2.419601930085049

Epoch: 6| Step: 3
Training loss: 2.1530354022979736
Validation loss: 2.4382765011120866

Epoch: 6| Step: 4
Training loss: 2.7918310165405273
Validation loss: 2.431506549158404

Epoch: 6| Step: 5
Training loss: 2.0514678955078125
Validation loss: 2.4079418374646093

Epoch: 6| Step: 6
Training loss: 3.238610029220581
Validation loss: 2.398372906510548

Epoch: 6| Step: 7
Training loss: 2.8835365772247314
Validation loss: 2.3846485922413487

Epoch: 6| Step: 8
Training loss: 2.5688889026641846
Validation loss: 2.3812042872111

Epoch: 6| Step: 9
Training loss: 2.126052141189575
Validation loss: 2.393561919530233

Epoch: 6| Step: 10
Training loss: 2.560257911682129
Validation loss: 2.3935827427012946

Epoch: 6| Step: 11
Training loss: 1.6073205471038818
Validation loss: 2.3988267349940475

Epoch: 6| Step: 12
Training loss: 2.8878936767578125
Validation loss: 2.405305754753851

Epoch: 6| Step: 13
Training loss: 3.030038595199585
Validation loss: 2.4121263232282413

Epoch: 201| Step: 0
Training loss: 2.2657647132873535
Validation loss: 2.4207316316584104

Epoch: 6| Step: 1
Training loss: 3.0070605278015137
Validation loss: 2.424595889224801

Epoch: 6| Step: 2
Training loss: 2.0603060722351074
Validation loss: 2.4228861844667824

Epoch: 6| Step: 3
Training loss: 2.782762289047241
Validation loss: 2.4307790828007523

Epoch: 6| Step: 4
Training loss: 2.470670700073242
Validation loss: 2.43439071665528

Epoch: 6| Step: 5
Training loss: 3.4068498611450195
Validation loss: 2.4338742430492113

Epoch: 6| Step: 6
Training loss: 2.5913991928100586
Validation loss: 2.4333265443002023

Epoch: 6| Step: 7
Training loss: 2.5599358081817627
Validation loss: 2.410132218432683

Epoch: 6| Step: 8
Training loss: 2.12538480758667
Validation loss: 2.412046540168024

Epoch: 6| Step: 9
Training loss: 3.406332015991211
Validation loss: 2.401830521962976

Epoch: 6| Step: 10
Training loss: 2.886706829071045
Validation loss: 2.3926833829572125

Epoch: 6| Step: 11
Training loss: 2.0841665267944336
Validation loss: 2.3874623134572017

Epoch: 6| Step: 12
Training loss: 2.8505196571350098
Validation loss: 2.389847227322158

Epoch: 6| Step: 13
Training loss: 2.215242862701416
Validation loss: 2.3921854290910947

Epoch: 202| Step: 0
Training loss: 3.0096287727355957
Validation loss: 2.394417514083206

Epoch: 6| Step: 1
Training loss: 2.1597115993499756
Validation loss: 2.395201849681075

Epoch: 6| Step: 2
Training loss: 3.047497510910034
Validation loss: 2.3953002473359466

Epoch: 6| Step: 3
Training loss: 3.216869831085205
Validation loss: 2.411415087279453

Epoch: 6| Step: 4
Training loss: 2.1911792755126953
Validation loss: 2.4066871314920406

Epoch: 6| Step: 5
Training loss: 1.9820556640625
Validation loss: 2.410946615280644

Epoch: 6| Step: 6
Training loss: 2.9009628295898438
Validation loss: 2.4177561011365665

Epoch: 6| Step: 7
Training loss: 2.192091464996338
Validation loss: 2.418674684339954

Epoch: 6| Step: 8
Training loss: 2.526700019836426
Validation loss: 2.40286168231759

Epoch: 6| Step: 9
Training loss: 2.535207748413086
Validation loss: 2.4030173414496967

Epoch: 6| Step: 10
Training loss: 2.6964573860168457
Validation loss: 2.4045675621237805

Epoch: 6| Step: 11
Training loss: 2.551994800567627
Validation loss: 2.405724517760738

Epoch: 6| Step: 12
Training loss: 2.606553554534912
Validation loss: 2.4092702891236994

Epoch: 6| Step: 13
Training loss: 2.970883369445801
Validation loss: 2.4071364146406933

Epoch: 203| Step: 0
Training loss: 2.0349526405334473
Validation loss: 2.4068216457161853

Epoch: 6| Step: 1
Training loss: 2.1907968521118164
Validation loss: 2.4066234788587018

Epoch: 6| Step: 2
Training loss: 2.275120258331299
Validation loss: 2.406122748569776

Epoch: 6| Step: 3
Training loss: 2.4224727153778076
Validation loss: 2.399630400442308

Epoch: 6| Step: 4
Training loss: 2.740265130996704
Validation loss: 2.4074719387997865

Epoch: 6| Step: 5
Training loss: 2.659672260284424
Validation loss: 2.404860919521701

Epoch: 6| Step: 6
Training loss: 2.8018107414245605
Validation loss: 2.4010952467559488

Epoch: 6| Step: 7
Training loss: 2.8268518447875977
Validation loss: 2.400355572341591

Epoch: 6| Step: 8
Training loss: 3.1265902519226074
Validation loss: 2.404209970146097

Epoch: 6| Step: 9
Training loss: 2.201949119567871
Validation loss: 2.4053382822262344

Epoch: 6| Step: 10
Training loss: 2.24782133102417
Validation loss: 2.4050586197965886

Epoch: 6| Step: 11
Training loss: 2.7545337677001953
Validation loss: 2.4066311390169206

Epoch: 6| Step: 12
Training loss: 2.9438211917877197
Validation loss: 2.400450416790542

Epoch: 6| Step: 13
Training loss: 3.363945960998535
Validation loss: 2.4022885548171176

Epoch: 204| Step: 0
Training loss: 2.2743473052978516
Validation loss: 2.395883531980617

Epoch: 6| Step: 1
Training loss: 2.6772806644439697
Validation loss: 2.3871718888641684

Epoch: 6| Step: 2
Training loss: 3.055912971496582
Validation loss: 2.390819828997376

Epoch: 6| Step: 3
Training loss: 3.272587299346924
Validation loss: 2.382013605486962

Epoch: 6| Step: 4
Training loss: 3.271451711654663
Validation loss: 2.3924757306293776

Epoch: 6| Step: 5
Training loss: 2.8248181343078613
Validation loss: 2.3784665420491207

Epoch: 6| Step: 6
Training loss: 2.4076411724090576
Validation loss: 2.383826212216449

Epoch: 6| Step: 7
Training loss: 1.6793718338012695
Validation loss: 2.388336117549609

Epoch: 6| Step: 8
Training loss: 3.2317352294921875
Validation loss: 2.394943819251112

Epoch: 6| Step: 9
Training loss: 1.959964632987976
Validation loss: 2.3900782844071746

Epoch: 6| Step: 10
Training loss: 2.1491827964782715
Validation loss: 2.3944065673376924

Epoch: 6| Step: 11
Training loss: 3.13651704788208
Validation loss: 2.390205060282061

Epoch: 6| Step: 12
Training loss: 1.7966654300689697
Validation loss: 2.382305201663766

Epoch: 6| Step: 13
Training loss: 2.321500539779663
Validation loss: 2.3889501812637493

Epoch: 205| Step: 0
Training loss: 3.181340456008911
Validation loss: 2.390133488562799

Epoch: 6| Step: 1
Training loss: 1.9997684955596924
Validation loss: 2.3858270260595504

Epoch: 6| Step: 2
Training loss: 2.454188823699951
Validation loss: 2.395621858617311

Epoch: 6| Step: 3
Training loss: 1.7087490558624268
Validation loss: 2.3917287780392553

Epoch: 6| Step: 4
Training loss: 2.732973337173462
Validation loss: 2.3859197529413367

Epoch: 6| Step: 5
Training loss: 2.6869192123413086
Validation loss: 2.394010384877523

Epoch: 6| Step: 6
Training loss: 2.575157403945923
Validation loss: 2.3990931946744203

Epoch: 6| Step: 7
Training loss: 2.182849407196045
Validation loss: 2.40141878333143

Epoch: 6| Step: 8
Training loss: 2.6931090354919434
Validation loss: 2.4044886019922074

Epoch: 6| Step: 9
Training loss: 2.2970752716064453
Validation loss: 2.402596830039896

Epoch: 6| Step: 10
Training loss: 3.464465379714966
Validation loss: 2.3998631149209957

Epoch: 6| Step: 11
Training loss: 2.9771106243133545
Validation loss: 2.3998367478770595

Epoch: 6| Step: 12
Training loss: 2.700291633605957
Validation loss: 2.3974841974114858

Epoch: 6| Step: 13
Training loss: 2.6216166019439697
Validation loss: 2.398363339003696

Epoch: 206| Step: 0
Training loss: 3.0582892894744873
Validation loss: 2.3899813262365197

Epoch: 6| Step: 1
Training loss: 2.3299508094787598
Validation loss: 2.389697454308951

Epoch: 6| Step: 2
Training loss: 3.0305557250976562
Validation loss: 2.3889971497238323

Epoch: 6| Step: 3
Training loss: 3.246917724609375
Validation loss: 2.3925008107257146

Epoch: 6| Step: 4
Training loss: 2.315037965774536
Validation loss: 2.3945078747246855

Epoch: 6| Step: 5
Training loss: 2.676666736602783
Validation loss: 2.4018972586559992

Epoch: 6| Step: 6
Training loss: 2.2026047706604004
Validation loss: 2.387079620874056

Epoch: 6| Step: 7
Training loss: 2.7021069526672363
Validation loss: 2.3816201533040693

Epoch: 6| Step: 8
Training loss: 2.5727620124816895
Validation loss: 2.384953070712346

Epoch: 6| Step: 9
Training loss: 2.7194664478302
Validation loss: 2.3726064107751332

Epoch: 6| Step: 10
Training loss: 2.401700496673584
Validation loss: 2.375407462478966

Epoch: 6| Step: 11
Training loss: 2.2862253189086914
Validation loss: 2.3698319799156597

Epoch: 6| Step: 12
Training loss: 2.5744893550872803
Validation loss: 2.3806809071571595

Epoch: 6| Step: 13
Training loss: 1.966307520866394
Validation loss: 2.38006410291118

Epoch: 207| Step: 0
Training loss: 2.87514591217041
Validation loss: 2.3855181329993793

Epoch: 6| Step: 1
Training loss: 3.0078577995300293
Validation loss: 2.383950971787976

Epoch: 6| Step: 2
Training loss: 2.007941961288452
Validation loss: 2.3846936866801274

Epoch: 6| Step: 3
Training loss: 2.695004940032959
Validation loss: 2.3877146013321413

Epoch: 6| Step: 4
Training loss: 3.088700532913208
Validation loss: 2.3931389752254693

Epoch: 6| Step: 5
Training loss: 2.1072516441345215
Validation loss: 2.385220161048315

Epoch: 6| Step: 6
Training loss: 2.2614450454711914
Validation loss: 2.378062243102699

Epoch: 6| Step: 7
Training loss: 2.6083261966705322
Validation loss: 2.380783106691094

Epoch: 6| Step: 8
Training loss: 1.5052011013031006
Validation loss: 2.3836394215142853

Epoch: 6| Step: 9
Training loss: 2.8042409420013428
Validation loss: 2.384708255849859

Epoch: 6| Step: 10
Training loss: 2.3884475231170654
Validation loss: 2.3914433243454143

Epoch: 6| Step: 11
Training loss: 3.252267360687256
Validation loss: 2.401420788098407

Epoch: 6| Step: 12
Training loss: 3.115431785583496
Validation loss: 2.412163330662635

Epoch: 6| Step: 13
Training loss: 2.68123722076416
Validation loss: 2.4090285737027406

Epoch: 208| Step: 0
Training loss: 3.3002185821533203
Validation loss: 2.408282328677434

Epoch: 6| Step: 1
Training loss: 2.6723074913024902
Validation loss: 2.4058810434033795

Epoch: 6| Step: 2
Training loss: 1.6032239198684692
Validation loss: 2.4008458045221146

Epoch: 6| Step: 3
Training loss: 3.0213823318481445
Validation loss: 2.3970969594934934

Epoch: 6| Step: 4
Training loss: 1.7644963264465332
Validation loss: 2.3939156968106508

Epoch: 6| Step: 5
Training loss: 3.431211471557617
Validation loss: 2.3998420366676907

Epoch: 6| Step: 6
Training loss: 2.28717303276062
Validation loss: 2.3943216134143133

Epoch: 6| Step: 7
Training loss: 2.2871768474578857
Validation loss: 2.3907106717427573

Epoch: 6| Step: 8
Training loss: 3.038684844970703
Validation loss: 2.3950872164900585

Epoch: 6| Step: 9
Training loss: 2.385188579559326
Validation loss: 2.3969338260671145

Epoch: 6| Step: 10
Training loss: 2.532888889312744
Validation loss: 2.395844990207303

Epoch: 6| Step: 11
Training loss: 2.6690123081207275
Validation loss: 2.392628961993802

Epoch: 6| Step: 12
Training loss: 2.184633731842041
Validation loss: 2.3895786898110503

Epoch: 6| Step: 13
Training loss: 3.3965559005737305
Validation loss: 2.3919482948959514

Epoch: 209| Step: 0
Training loss: 2.5581166744232178
Validation loss: 2.4053014196375364

Epoch: 6| Step: 1
Training loss: 2.35499906539917
Validation loss: 2.3973662571240495

Epoch: 6| Step: 2
Training loss: 3.1965839862823486
Validation loss: 2.3855289054173294

Epoch: 6| Step: 3
Training loss: 3.2910518646240234
Validation loss: 2.3984650719550347

Epoch: 6| Step: 4
Training loss: 2.390411138534546
Validation loss: 2.389838992908437

Epoch: 6| Step: 5
Training loss: 2.496109962463379
Validation loss: 2.391240014824816

Epoch: 6| Step: 6
Training loss: 2.520040512084961
Validation loss: 2.397593059847432

Epoch: 6| Step: 7
Training loss: 2.378837823867798
Validation loss: 2.4040226872249315

Epoch: 6| Step: 8
Training loss: 2.2820801734924316
Validation loss: 2.4017324601450274

Epoch: 6| Step: 9
Training loss: 2.361454963684082
Validation loss: 2.4046351448182137

Epoch: 6| Step: 10
Training loss: 2.6766772270202637
Validation loss: 2.4290032668780257

Epoch: 6| Step: 11
Training loss: 3.072077751159668
Validation loss: 2.433905698919809

Epoch: 6| Step: 12
Training loss: 2.0249276161193848
Validation loss: 2.4268430894420994

Epoch: 6| Step: 13
Training loss: 2.5802812576293945
Validation loss: 2.4213970117671515

Epoch: 210| Step: 0
Training loss: 2.9740042686462402
Validation loss: 2.425228939261488

Epoch: 6| Step: 1
Training loss: 2.7707443237304688
Validation loss: 2.4108399396301596

Epoch: 6| Step: 2
Training loss: 1.7524104118347168
Validation loss: 2.4178913947074645

Epoch: 6| Step: 3
Training loss: 2.015174150466919
Validation loss: 2.4138227906278384

Epoch: 6| Step: 4
Training loss: 2.986907720565796
Validation loss: 2.405382743445776

Epoch: 6| Step: 5
Training loss: 2.60062837600708
Validation loss: 2.4007196323845976

Epoch: 6| Step: 6
Training loss: 2.5970587730407715
Validation loss: 2.4006544569487214

Epoch: 6| Step: 7
Training loss: 3.1952266693115234
Validation loss: 2.3964933118512555

Epoch: 6| Step: 8
Training loss: 2.1497037410736084
Validation loss: 2.385589050990279

Epoch: 6| Step: 9
Training loss: 2.1107981204986572
Validation loss: 2.378905024579776

Epoch: 6| Step: 10
Training loss: 3.397592544555664
Validation loss: 2.3702446670942408

Epoch: 6| Step: 11
Training loss: 2.1551878452301025
Validation loss: 2.366368665490099

Epoch: 6| Step: 12
Training loss: 2.539459705352783
Validation loss: 2.368813050690518

Epoch: 6| Step: 13
Training loss: 3.302830457687378
Validation loss: 2.365119106026106

Epoch: 211| Step: 0
Training loss: 3.205613613128662
Validation loss: 2.3633437156677246

Epoch: 6| Step: 1
Training loss: 2.2692928314208984
Validation loss: 2.3675993104134836

Epoch: 6| Step: 2
Training loss: 3.6337599754333496
Validation loss: 2.3667597873236543

Epoch: 6| Step: 3
Training loss: 2.825092315673828
Validation loss: 2.3730629080085346

Epoch: 6| Step: 4
Training loss: 2.842376708984375
Validation loss: 2.367726855380561

Epoch: 6| Step: 5
Training loss: 2.1147232055664062
Validation loss: 2.3733441342589674

Epoch: 6| Step: 6
Training loss: 2.5219149589538574
Validation loss: 2.379833952073128

Epoch: 6| Step: 7
Training loss: 2.687222480773926
Validation loss: 2.38518080916456

Epoch: 6| Step: 8
Training loss: 1.383711814880371
Validation loss: 2.38068570885607

Epoch: 6| Step: 9
Training loss: 2.799643039703369
Validation loss: 2.3974180247194026

Epoch: 6| Step: 10
Training loss: 2.4008171558380127
Validation loss: 2.387301360407183

Epoch: 6| Step: 11
Training loss: 2.759532928466797
Validation loss: 2.402396409742294

Epoch: 6| Step: 12
Training loss: 2.3360354900360107
Validation loss: 2.383589049821259

Epoch: 6| Step: 13
Training loss: 2.0796267986297607
Validation loss: 2.385202237354812

Epoch: 212| Step: 0
Training loss: 2.6320860385894775
Validation loss: 2.3807732059109594

Epoch: 6| Step: 1
Training loss: 2.4477787017822266
Validation loss: 2.3838238664852676

Epoch: 6| Step: 2
Training loss: 2.2053799629211426
Validation loss: 2.3842829119774605

Epoch: 6| Step: 3
Training loss: 1.9864212274551392
Validation loss: 2.383478567164431

Epoch: 6| Step: 4
Training loss: 2.3003125190734863
Validation loss: 2.3820203799073414

Epoch: 6| Step: 5
Training loss: 2.457017660140991
Validation loss: 2.37465920499576

Epoch: 6| Step: 6
Training loss: 2.5638017654418945
Validation loss: 2.3755960285022693

Epoch: 6| Step: 7
Training loss: 3.0144662857055664
Validation loss: 2.366689843516196

Epoch: 6| Step: 8
Training loss: 2.9567360877990723
Validation loss: 2.378150893795875

Epoch: 6| Step: 9
Training loss: 2.428779125213623
Validation loss: 2.371849675332346

Epoch: 6| Step: 10
Training loss: 2.8744282722473145
Validation loss: 2.3726095819985993

Epoch: 6| Step: 11
Training loss: 2.3033711910247803
Validation loss: 2.3863447430313274

Epoch: 6| Step: 12
Training loss: 3.2111904621124268
Validation loss: 2.385970566862373

Epoch: 6| Step: 13
Training loss: 2.7160122394561768
Validation loss: 2.3862313660242225

Epoch: 213| Step: 0
Training loss: 3.4484143257141113
Validation loss: 2.375704521773964

Epoch: 6| Step: 1
Training loss: 2.752309560775757
Validation loss: 2.366555372873942

Epoch: 6| Step: 2
Training loss: 3.066075325012207
Validation loss: 2.35754757542764

Epoch: 6| Step: 3
Training loss: 2.485736608505249
Validation loss: 2.357664405658681

Epoch: 6| Step: 4
Training loss: 2.0726120471954346
Validation loss: 2.359178181617491

Epoch: 6| Step: 5
Training loss: 2.426163911819458
Validation loss: 2.3698226277546217

Epoch: 6| Step: 6
Training loss: 2.3377130031585693
Validation loss: 2.358893471379434

Epoch: 6| Step: 7
Training loss: 2.4615139961242676
Validation loss: 2.3602135924882788

Epoch: 6| Step: 8
Training loss: 2.8739616870880127
Validation loss: 2.3596848928800194

Epoch: 6| Step: 9
Training loss: 2.2550723552703857
Validation loss: 2.3602841349058252

Epoch: 6| Step: 10
Training loss: 2.1666364669799805
Validation loss: 2.361748044208814

Epoch: 6| Step: 11
Training loss: 2.4927139282226562
Validation loss: 2.3615627596455235

Epoch: 6| Step: 12
Training loss: 2.552797317504883
Validation loss: 2.3608657006294496

Epoch: 6| Step: 13
Training loss: 2.459601879119873
Validation loss: 2.352422211759834

Epoch: 214| Step: 0
Training loss: 2.2373061180114746
Validation loss: 2.365796660864225

Epoch: 6| Step: 1
Training loss: 3.2012882232666016
Validation loss: 2.357928491407825

Epoch: 6| Step: 2
Training loss: 2.5786983966827393
Validation loss: 2.359945043440788

Epoch: 6| Step: 3
Training loss: 1.8694236278533936
Validation loss: 2.359990571134834

Epoch: 6| Step: 4
Training loss: 2.8448681831359863
Validation loss: 2.3613894037021104

Epoch: 6| Step: 5
Training loss: 2.4968457221984863
Validation loss: 2.36608556009108

Epoch: 6| Step: 6
Training loss: 2.955660343170166
Validation loss: 2.366515339061778

Epoch: 6| Step: 7
Training loss: 2.471785068511963
Validation loss: 2.365354614873086

Epoch: 6| Step: 8
Training loss: 2.541975975036621
Validation loss: 2.3728692967404603

Epoch: 6| Step: 9
Training loss: 1.9937623739242554
Validation loss: 2.369585665323401

Epoch: 6| Step: 10
Training loss: 2.4404854774475098
Validation loss: 2.3673674726998932

Epoch: 6| Step: 11
Training loss: 3.1511635780334473
Validation loss: 2.3588662429522445

Epoch: 6| Step: 12
Training loss: 2.5811846256256104
Validation loss: 2.361778815587362

Epoch: 6| Step: 13
Training loss: 2.264439582824707
Validation loss: 2.3571197063692155

Epoch: 215| Step: 0
Training loss: 2.141633987426758
Validation loss: 2.3551344051155993

Epoch: 6| Step: 1
Training loss: 2.181448221206665
Validation loss: 2.352418745717695

Epoch: 6| Step: 2
Training loss: 2.382002115249634
Validation loss: 2.3557450976423038

Epoch: 6| Step: 3
Training loss: 2.1401143074035645
Validation loss: 2.3505882704129784

Epoch: 6| Step: 4
Training loss: 2.8085761070251465
Validation loss: 2.3533443379145798

Epoch: 6| Step: 5
Training loss: 2.818795919418335
Validation loss: 2.3501216083444576

Epoch: 6| Step: 6
Training loss: 2.6470999717712402
Validation loss: 2.35434066864752

Epoch: 6| Step: 7
Training loss: 2.0018582344055176
Validation loss: 2.349617541477244

Epoch: 6| Step: 8
Training loss: 2.976905345916748
Validation loss: 2.354657814066897

Epoch: 6| Step: 9
Training loss: 2.699430227279663
Validation loss: 2.3513902682130055

Epoch: 6| Step: 10
Training loss: 2.8320627212524414
Validation loss: 2.3454370652475665

Epoch: 6| Step: 11
Training loss: 2.6293721199035645
Validation loss: 2.342702063181067

Epoch: 6| Step: 12
Training loss: 2.737791061401367
Validation loss: 2.353716068370368

Epoch: 6| Step: 13
Training loss: 2.89247989654541
Validation loss: 2.361144019711402

Epoch: 216| Step: 0
Training loss: 2.9477951526641846
Validation loss: 2.366064574128838

Epoch: 6| Step: 1
Training loss: 2.8382608890533447
Validation loss: 2.3813112192256476

Epoch: 6| Step: 2
Training loss: 2.3400073051452637
Validation loss: 2.387157263294343

Epoch: 6| Step: 3
Training loss: 2.2892954349517822
Validation loss: 2.3982763726224183

Epoch: 6| Step: 4
Training loss: 1.9672560691833496
Validation loss: 2.4049186039996404

Epoch: 6| Step: 5
Training loss: 3.033590316772461
Validation loss: 2.415361440309914

Epoch: 6| Step: 6
Training loss: 2.15643310546875
Validation loss: 2.418894913888747

Epoch: 6| Step: 7
Training loss: 2.3529458045959473
Validation loss: 2.402143934721588

Epoch: 6| Step: 8
Training loss: 2.722656488418579
Validation loss: 2.3917361536333637

Epoch: 6| Step: 9
Training loss: 3.3844199180603027
Validation loss: 2.375553436176751

Epoch: 6| Step: 10
Training loss: 2.148494005203247
Validation loss: 2.373878881495486

Epoch: 6| Step: 11
Training loss: 2.9280543327331543
Validation loss: 2.3799788580145886

Epoch: 6| Step: 12
Training loss: 2.8031015396118164
Validation loss: 2.4017632968964113

Epoch: 6| Step: 13
Training loss: 1.8659940958023071
Validation loss: 2.4111492249273483

Epoch: 217| Step: 0
Training loss: 2.2828545570373535
Validation loss: 2.422884997501168

Epoch: 6| Step: 1
Training loss: 3.4659061431884766
Validation loss: 2.4099746314428185

Epoch: 6| Step: 2
Training loss: 3.0513625144958496
Validation loss: 2.3950188313761065

Epoch: 6| Step: 3
Training loss: 2.7567391395568848
Validation loss: 2.3886427622969433

Epoch: 6| Step: 4
Training loss: 2.410435438156128
Validation loss: 2.3768794100771666

Epoch: 6| Step: 5
Training loss: 2.780672550201416
Validation loss: 2.3686979868078746

Epoch: 6| Step: 6
Training loss: 2.155704975128174
Validation loss: 2.359765588596303

Epoch: 6| Step: 7
Training loss: 2.8811845779418945
Validation loss: 2.346920321064611

Epoch: 6| Step: 8
Training loss: 1.8896921873092651
Validation loss: 2.342947795826902

Epoch: 6| Step: 9
Training loss: 2.4825387001037598
Validation loss: 2.334465608801893

Epoch: 6| Step: 10
Training loss: 2.2319869995117188
Validation loss: 2.3284461600806123

Epoch: 6| Step: 11
Training loss: 2.7966108322143555
Validation loss: 2.325454842659735

Epoch: 6| Step: 12
Training loss: 2.4281058311462402
Validation loss: 2.323567698078771

Epoch: 6| Step: 13
Training loss: 2.8244261741638184
Validation loss: 2.3261427674242245

Epoch: 218| Step: 0
Training loss: 2.773937702178955
Validation loss: 2.323724674922164

Epoch: 6| Step: 1
Training loss: 2.713271379470825
Validation loss: 2.321680591952416

Epoch: 6| Step: 2
Training loss: 2.1709632873535156
Validation loss: 2.3216353718952467

Epoch: 6| Step: 3
Training loss: 2.709345817565918
Validation loss: 2.323979910983834

Epoch: 6| Step: 4
Training loss: 2.1732406616210938
Validation loss: 2.3289660933197185

Epoch: 6| Step: 5
Training loss: 2.7720751762390137
Validation loss: 2.3247168346117904

Epoch: 6| Step: 6
Training loss: 3.0531163215637207
Validation loss: 2.326127424035021

Epoch: 6| Step: 7
Training loss: 2.409224033355713
Validation loss: 2.32982208395517

Epoch: 6| Step: 8
Training loss: 2.8441662788391113
Validation loss: 2.3221280805526243

Epoch: 6| Step: 9
Training loss: 2.17042875289917
Validation loss: 2.317507251616447

Epoch: 6| Step: 10
Training loss: 2.530339002609253
Validation loss: 2.3239299712642545

Epoch: 6| Step: 11
Training loss: 2.314707040786743
Validation loss: 2.322929190051171

Epoch: 6| Step: 12
Training loss: 2.6510305404663086
Validation loss: 2.3303161718512095

Epoch: 6| Step: 13
Training loss: 3.0419962406158447
Validation loss: 2.330860491721861

Epoch: 219| Step: 0
Training loss: 2.088904857635498
Validation loss: 2.325452655874273

Epoch: 6| Step: 1
Training loss: 2.5783848762512207
Validation loss: 2.3258076867749615

Epoch: 6| Step: 2
Training loss: 2.5748207569122314
Validation loss: 2.32927115501896

Epoch: 6| Step: 3
Training loss: 3.4933934211730957
Validation loss: 2.332328668204687

Epoch: 6| Step: 4
Training loss: 3.4927737712860107
Validation loss: 2.332974442871668

Epoch: 6| Step: 5
Training loss: 2.66201114654541
Validation loss: 2.346924376744096

Epoch: 6| Step: 6
Training loss: 1.6660442352294922
Validation loss: 2.3426346804506037

Epoch: 6| Step: 7
Training loss: 2.700744867324829
Validation loss: 2.343953753030428

Epoch: 6| Step: 8
Training loss: 2.111769199371338
Validation loss: 2.3516584775781118

Epoch: 6| Step: 9
Training loss: 2.355747699737549
Validation loss: 2.359090507671397

Epoch: 6| Step: 10
Training loss: 2.5856428146362305
Validation loss: 2.3538643185810377

Epoch: 6| Step: 11
Training loss: 2.5717334747314453
Validation loss: 2.35710060468284

Epoch: 6| Step: 12
Training loss: 2.44960618019104
Validation loss: 2.3400879688160394

Epoch: 6| Step: 13
Training loss: 2.7505831718444824
Validation loss: 2.346080780029297

Epoch: 220| Step: 0
Training loss: 2.866580009460449
Validation loss: 2.3470781234002884

Epoch: 6| Step: 1
Training loss: 1.9977185726165771
Validation loss: 2.346067190170288

Epoch: 6| Step: 2
Training loss: 3.213942527770996
Validation loss: 2.3405514250519457

Epoch: 6| Step: 3
Training loss: 2.650381088256836
Validation loss: 2.341113939080187

Epoch: 6| Step: 4
Training loss: 2.877772331237793
Validation loss: 2.341071992792109

Epoch: 6| Step: 5
Training loss: 2.1164212226867676
Validation loss: 2.3451137491451797

Epoch: 6| Step: 6
Training loss: 3.0323479175567627
Validation loss: 2.3426271125834477

Epoch: 6| Step: 7
Training loss: 2.3846120834350586
Validation loss: 2.342202127620738

Epoch: 6| Step: 8
Training loss: 2.5801312923431396
Validation loss: 2.34444490555794

Epoch: 6| Step: 9
Training loss: 2.2109527587890625
Validation loss: 2.3418463378824215

Epoch: 6| Step: 10
Training loss: 2.4007692337036133
Validation loss: 2.3538384232469785

Epoch: 6| Step: 11
Training loss: 2.5399298667907715
Validation loss: 2.357526002391692

Epoch: 6| Step: 12
Training loss: 2.4370503425598145
Validation loss: 2.361268376791349

Epoch: 6| Step: 13
Training loss: 2.3604605197906494
Validation loss: 2.3683418971236034

Epoch: 221| Step: 0
Training loss: 2.1228981018066406
Validation loss: 2.3657406735163864

Epoch: 6| Step: 1
Training loss: 2.7331583499908447
Validation loss: 2.369620769254623

Epoch: 6| Step: 2
Training loss: 2.676804304122925
Validation loss: 2.370114177785894

Epoch: 6| Step: 3
Training loss: 2.0040242671966553
Validation loss: 2.3602555849218882

Epoch: 6| Step: 4
Training loss: 2.504532814025879
Validation loss: 2.355474877101119

Epoch: 6| Step: 5
Training loss: 2.8087050914764404
Validation loss: 2.3538435274554836

Epoch: 6| Step: 6
Training loss: 2.831620216369629
Validation loss: 2.3434599061166086

Epoch: 6| Step: 7
Training loss: 2.5498263835906982
Validation loss: 2.34618886568213

Epoch: 6| Step: 8
Training loss: 2.6735947132110596
Validation loss: 2.34698946117073

Epoch: 6| Step: 9
Training loss: 2.6960463523864746
Validation loss: 2.3450649887002926

Epoch: 6| Step: 10
Training loss: 2.718125343322754
Validation loss: 2.333691886676255

Epoch: 6| Step: 11
Training loss: 1.872431755065918
Validation loss: 2.3394216696421304

Epoch: 6| Step: 12
Training loss: 2.3446130752563477
Validation loss: 2.346078021551973

Epoch: 6| Step: 13
Training loss: 3.581378936767578
Validation loss: 2.3365300547692085

Epoch: 222| Step: 0
Training loss: 2.9022459983825684
Validation loss: 2.340099234734812

Epoch: 6| Step: 1
Training loss: 3.1748549938201904
Validation loss: 2.342854848472021

Epoch: 6| Step: 2
Training loss: 1.822798728942871
Validation loss: 2.342917852504279

Epoch: 6| Step: 3
Training loss: 3.0234928131103516
Validation loss: 2.34702363065494

Epoch: 6| Step: 4
Training loss: 3.1390557289123535
Validation loss: 2.3631969164776545

Epoch: 6| Step: 5
Training loss: 1.7960467338562012
Validation loss: 2.358783011795372

Epoch: 6| Step: 6
Training loss: 2.7016003131866455
Validation loss: 2.3621562860345326

Epoch: 6| Step: 7
Training loss: 2.107581377029419
Validation loss: 2.3574668771477154

Epoch: 6| Step: 8
Training loss: 2.810974359512329
Validation loss: 2.3586822555911158

Epoch: 6| Step: 9
Training loss: 3.0621144771575928
Validation loss: 2.362758764656641

Epoch: 6| Step: 10
Training loss: 2.532050609588623
Validation loss: 2.360737159687986

Epoch: 6| Step: 11
Training loss: 2.0716166496276855
Validation loss: 2.359119951084096

Epoch: 6| Step: 12
Training loss: 2.161083698272705
Validation loss: 2.3553100247536936

Epoch: 6| Step: 13
Training loss: 2.2682101726531982
Validation loss: 2.353412248755014

Epoch: 223| Step: 0
Training loss: 3.2360010147094727
Validation loss: 2.358687041908182

Epoch: 6| Step: 1
Training loss: 2.2578976154327393
Validation loss: 2.34924792474316

Epoch: 6| Step: 2
Training loss: 2.6027779579162598
Validation loss: 2.350990705592658

Epoch: 6| Step: 3
Training loss: 2.062570095062256
Validation loss: 2.342677764995124

Epoch: 6| Step: 4
Training loss: 2.7372164726257324
Validation loss: 2.338060676410634

Epoch: 6| Step: 5
Training loss: 2.811379909515381
Validation loss: 2.3493175455318984

Epoch: 6| Step: 6
Training loss: 2.244452714920044
Validation loss: 2.3512896260907574

Epoch: 6| Step: 7
Training loss: 3.2258386611938477
Validation loss: 2.340711970483103

Epoch: 6| Step: 8
Training loss: 2.3844971656799316
Validation loss: 2.352376002137379

Epoch: 6| Step: 9
Training loss: 3.0364367961883545
Validation loss: 2.3431355594306864

Epoch: 6| Step: 10
Training loss: 2.2640671730041504
Validation loss: 2.3403461312734954

Epoch: 6| Step: 11
Training loss: 1.840306282043457
Validation loss: 2.3547722088393344

Epoch: 6| Step: 12
Training loss: 2.7066562175750732
Validation loss: 2.347929589210018

Epoch: 6| Step: 13
Training loss: 1.8562164306640625
Validation loss: 2.3545276221408638

Epoch: 224| Step: 0
Training loss: 2.3480753898620605
Validation loss: 2.3482930275701706

Epoch: 6| Step: 1
Training loss: 2.833378791809082
Validation loss: 2.3375983802221154

Epoch: 6| Step: 2
Training loss: 2.4693291187286377
Validation loss: 2.337720799189742

Epoch: 6| Step: 3
Training loss: 2.9369289875030518
Validation loss: 2.3462900205325057

Epoch: 6| Step: 4
Training loss: 1.8779805898666382
Validation loss: 2.3526487709373556

Epoch: 6| Step: 5
Training loss: 2.9943222999572754
Validation loss: 2.3639306112002303

Epoch: 6| Step: 6
Training loss: 2.8139078617095947
Validation loss: 2.3518992265065513

Epoch: 6| Step: 7
Training loss: 2.3959596157073975
Validation loss: 2.346634470006471

Epoch: 6| Step: 8
Training loss: 2.0927646160125732
Validation loss: 2.3550889825308197

Epoch: 6| Step: 9
Training loss: 2.5830297470092773
Validation loss: 2.3454985208408807

Epoch: 6| Step: 10
Training loss: 3.02167010307312
Validation loss: 2.3502659387485956

Epoch: 6| Step: 11
Training loss: 2.359299659729004
Validation loss: 2.3417568193968905

Epoch: 6| Step: 12
Training loss: 2.684962272644043
Validation loss: 2.3423567407874653

Epoch: 6| Step: 13
Training loss: 1.5836974382400513
Validation loss: 2.3499855918269

Epoch: 225| Step: 0
Training loss: 1.9948710203170776
Validation loss: 2.3455662086445797

Epoch: 6| Step: 1
Training loss: 2.442196846008301
Validation loss: 2.3424783778446976

Epoch: 6| Step: 2
Training loss: 2.8247952461242676
Validation loss: 2.348529267054732

Epoch: 6| Step: 3
Training loss: 2.4341678619384766
Validation loss: 2.3616580758043515

Epoch: 6| Step: 4
Training loss: 2.378701686859131
Validation loss: 2.3665200253968597

Epoch: 6| Step: 5
Training loss: 2.668935775756836
Validation loss: 2.3629096964354157

Epoch: 6| Step: 6
Training loss: 2.702411651611328
Validation loss: 2.3544391457752516

Epoch: 6| Step: 7
Training loss: 2.2165651321411133
Validation loss: 2.3498519646224154

Epoch: 6| Step: 8
Training loss: 2.706763982772827
Validation loss: 2.3544636772524927

Epoch: 6| Step: 9
Training loss: 2.6899242401123047
Validation loss: 2.3533943263433312

Epoch: 6| Step: 10
Training loss: 3.2453417778015137
Validation loss: 2.3646417869034635

Epoch: 6| Step: 11
Training loss: 2.6568808555603027
Validation loss: 2.354331242140903

Epoch: 6| Step: 12
Training loss: 2.487211227416992
Validation loss: 2.352071059647427

Epoch: 6| Step: 13
Training loss: 1.902158260345459
Validation loss: 2.3529024893237698

Epoch: 226| Step: 0
Training loss: 2.1009082794189453
Validation loss: 2.350530127043365

Epoch: 6| Step: 1
Training loss: 3.2002696990966797
Validation loss: 2.3642161738487983

Epoch: 6| Step: 2
Training loss: 2.9332263469696045
Validation loss: 2.355691586771319

Epoch: 6| Step: 3
Training loss: 2.930657386779785
Validation loss: 2.3601997808743547

Epoch: 6| Step: 4
Training loss: 2.0417516231536865
Validation loss: 2.3591495508788736

Epoch: 6| Step: 5
Training loss: 2.531005620956421
Validation loss: 2.350717782974243

Epoch: 6| Step: 6
Training loss: 2.7580621242523193
Validation loss: 2.342644883740333

Epoch: 6| Step: 7
Training loss: 1.9870030879974365
Validation loss: 2.335571768463299

Epoch: 6| Step: 8
Training loss: 2.4926702976226807
Validation loss: 2.3365000217191634

Epoch: 6| Step: 9
Training loss: 2.8424153327941895
Validation loss: 2.333509211899132

Epoch: 6| Step: 10
Training loss: 2.2673821449279785
Validation loss: 2.3360916004386

Epoch: 6| Step: 11
Training loss: 2.824660301208496
Validation loss: 2.340940788228025

Epoch: 6| Step: 12
Training loss: 2.2469239234924316
Validation loss: 2.342453800221925

Epoch: 6| Step: 13
Training loss: 2.529170036315918
Validation loss: 2.329139463363155

Epoch: 227| Step: 0
Training loss: 2.7772326469421387
Validation loss: 2.328425539437161

Epoch: 6| Step: 1
Training loss: 2.5906453132629395
Validation loss: 2.325503683859302

Epoch: 6| Step: 2
Training loss: 2.56854510307312
Validation loss: 2.3199246365536927

Epoch: 6| Step: 3
Training loss: 2.551784038543701
Validation loss: 2.3305398648785007

Epoch: 6| Step: 4
Training loss: 2.9648704528808594
Validation loss: 2.3240754322339128

Epoch: 6| Step: 5
Training loss: 2.3458902835845947
Validation loss: 2.3265685727519374

Epoch: 6| Step: 6
Training loss: 2.7735776901245117
Validation loss: 2.3289866267993884

Epoch: 6| Step: 7
Training loss: 3.6057567596435547
Validation loss: 2.3376283209810973

Epoch: 6| Step: 8
Training loss: 1.994666576385498
Validation loss: 2.3194441462075837

Epoch: 6| Step: 9
Training loss: 1.8475048542022705
Validation loss: 2.324599234006738

Epoch: 6| Step: 10
Training loss: 2.4933464527130127
Validation loss: 2.32522629409708

Epoch: 6| Step: 11
Training loss: 2.504274845123291
Validation loss: 2.32787392216344

Epoch: 6| Step: 12
Training loss: 2.485323667526245
Validation loss: 2.325615447054627

Epoch: 6| Step: 13
Training loss: 1.693380355834961
Validation loss: 2.3340105215708413

Epoch: 228| Step: 0
Training loss: 3.0102500915527344
Validation loss: 2.3400992654984996

Epoch: 6| Step: 1
Training loss: 2.872734546661377
Validation loss: 2.3215205310493388

Epoch: 6| Step: 2
Training loss: 3.275383472442627
Validation loss: 2.3331124859471477

Epoch: 6| Step: 3
Training loss: 2.5049116611480713
Validation loss: 2.330773171558175

Epoch: 6| Step: 4
Training loss: 1.6450433731079102
Validation loss: 2.32719006846028

Epoch: 6| Step: 5
Training loss: 1.7998900413513184
Validation loss: 2.3185910242860035

Epoch: 6| Step: 6
Training loss: 2.647369623184204
Validation loss: 2.3191382551705964

Epoch: 6| Step: 7
Training loss: 2.493065357208252
Validation loss: 2.3152322922983477

Epoch: 6| Step: 8
Training loss: 3.1731152534484863
Validation loss: 2.3243086748225714

Epoch: 6| Step: 9
Training loss: 2.2545437812805176
Validation loss: 2.321758649682486

Epoch: 6| Step: 10
Training loss: 2.039278984069824
Validation loss: 2.334409518908429

Epoch: 6| Step: 11
Training loss: 2.695110559463501
Validation loss: 2.3389438557368454

Epoch: 6| Step: 12
Training loss: 2.5088300704956055
Validation loss: 2.3480645892440632

Epoch: 6| Step: 13
Training loss: 2.6589059829711914
Validation loss: 2.35868432957639

Epoch: 229| Step: 0
Training loss: 2.5350301265716553
Validation loss: 2.351530340410048

Epoch: 6| Step: 1
Training loss: 2.1240859031677246
Validation loss: 2.3499357392711024

Epoch: 6| Step: 2
Training loss: 2.846092700958252
Validation loss: 2.3517689089621268

Epoch: 6| Step: 3
Training loss: 2.5041005611419678
Validation loss: 2.3478651508208244

Epoch: 6| Step: 4
Training loss: 3.205395221710205
Validation loss: 2.3334471025774555

Epoch: 6| Step: 5
Training loss: 2.5359888076782227
Validation loss: 2.3386523646693074

Epoch: 6| Step: 6
Training loss: 2.9841456413269043
Validation loss: 2.344216154467675

Epoch: 6| Step: 7
Training loss: 2.7746520042419434
Validation loss: 2.3297963655123146

Epoch: 6| Step: 8
Training loss: 1.8677016496658325
Validation loss: 2.3274922306819628

Epoch: 6| Step: 9
Training loss: 3.0108017921447754
Validation loss: 2.338963900842974

Epoch: 6| Step: 10
Training loss: 2.684049606323242
Validation loss: 2.3355792004575013

Epoch: 6| Step: 11
Training loss: 2.229790687561035
Validation loss: 2.330626590277559

Epoch: 6| Step: 12
Training loss: 1.7609729766845703
Validation loss: 2.3289809380808184

Epoch: 6| Step: 13
Training loss: 2.3440825939178467
Validation loss: 2.3388924265420563

Epoch: 230| Step: 0
Training loss: 2.6210179328918457
Validation loss: 2.331866246397777

Epoch: 6| Step: 1
Training loss: 3.1115708351135254
Validation loss: 2.3378863027018886

Epoch: 6| Step: 2
Training loss: 2.79893159866333
Validation loss: 2.3369905064182896

Epoch: 6| Step: 3
Training loss: 2.047671318054199
Validation loss: 2.340528941923572

Epoch: 6| Step: 4
Training loss: 2.1817803382873535
Validation loss: 2.3360510692801526

Epoch: 6| Step: 5
Training loss: 2.549929618835449
Validation loss: 2.33747176970205

Epoch: 6| Step: 6
Training loss: 2.1050310134887695
Validation loss: 2.341627005607851

Epoch: 6| Step: 7
Training loss: 2.4453954696655273
Validation loss: 2.3355680665662213

Epoch: 6| Step: 8
Training loss: 2.7280797958374023
Validation loss: 2.3402229150136313

Epoch: 6| Step: 9
Training loss: 2.2785720825195312
Validation loss: 2.3267586128686064

Epoch: 6| Step: 10
Training loss: 2.4082603454589844
Validation loss: 2.340063384784165

Epoch: 6| Step: 11
Training loss: 3.027468204498291
Validation loss: 2.3386770166376585

Epoch: 6| Step: 12
Training loss: 2.2233362197875977
Validation loss: 2.3370024914382608

Epoch: 6| Step: 13
Training loss: 3.157912254333496
Validation loss: 2.3458929446435746

Epoch: 231| Step: 0
Training loss: 3.07308030128479
Validation loss: 2.3453010666754937

Epoch: 6| Step: 1
Training loss: 2.152340888977051
Validation loss: 2.3483737104682514

Epoch: 6| Step: 2
Training loss: 2.6387228965759277
Validation loss: 2.355927450682527

Epoch: 6| Step: 3
Training loss: 2.0763068199157715
Validation loss: 2.35382830455739

Epoch: 6| Step: 4
Training loss: 2.4359636306762695
Validation loss: 2.348111255194551

Epoch: 6| Step: 5
Training loss: 2.4554405212402344
Validation loss: 2.3496096185458604

Epoch: 6| Step: 6
Training loss: 2.9868526458740234
Validation loss: 2.3685093464389926

Epoch: 6| Step: 7
Training loss: 1.8690288066864014
Validation loss: 2.3587084893257386

Epoch: 6| Step: 8
Training loss: 2.5806243419647217
Validation loss: 2.3741494994009695

Epoch: 6| Step: 9
Training loss: 3.0797877311706543
Validation loss: 2.3827158174207135

Epoch: 6| Step: 10
Training loss: 2.505990743637085
Validation loss: 2.3811181053038566

Epoch: 6| Step: 11
Training loss: 1.9903534650802612
Validation loss: 2.388633376808577

Epoch: 6| Step: 12
Training loss: 2.948793888092041
Validation loss: 2.360767682393392

Epoch: 6| Step: 13
Training loss: 3.0486273765563965
Validation loss: 2.348964624507453

Epoch: 232| Step: 0
Training loss: 2.6347815990448
Validation loss: 2.3386963234152844

Epoch: 6| Step: 1
Training loss: 2.1996591091156006
Validation loss: 2.331841384210894

Epoch: 6| Step: 2
Training loss: 2.9734060764312744
Validation loss: 2.329379017635058

Epoch: 6| Step: 3
Training loss: 2.870680093765259
Validation loss: 2.322548863708332

Epoch: 6| Step: 4
Training loss: 2.865696907043457
Validation loss: 2.3237310276236585

Epoch: 6| Step: 5
Training loss: 2.270303249359131
Validation loss: 2.3244088772804505

Epoch: 6| Step: 6
Training loss: 2.491384983062744
Validation loss: 2.316991895757696

Epoch: 6| Step: 7
Training loss: 2.6324992179870605
Validation loss: 2.315452770520282

Epoch: 6| Step: 8
Training loss: 2.3053300380706787
Validation loss: 2.3179635847768476

Epoch: 6| Step: 9
Training loss: 2.86148738861084
Validation loss: 2.323310516213858

Epoch: 6| Step: 10
Training loss: 2.1863183975219727
Validation loss: 2.3206350393192743

Epoch: 6| Step: 11
Training loss: 2.1030685901641846
Validation loss: 2.3298924763997397

Epoch: 6| Step: 12
Training loss: 2.526308536529541
Validation loss: 2.339378754297892

Epoch: 6| Step: 13
Training loss: 2.38084077835083
Validation loss: 2.3351477346112652

Epoch: 233| Step: 0
Training loss: 3.387406349182129
Validation loss: 2.3324211874315814

Epoch: 6| Step: 1
Training loss: 2.1695663928985596
Validation loss: 2.3295855009427635

Epoch: 6| Step: 2
Training loss: 2.721874713897705
Validation loss: 2.3283183343948854

Epoch: 6| Step: 3
Training loss: 2.8034069538116455
Validation loss: 2.3333567137359292

Epoch: 6| Step: 4
Training loss: 3.091385841369629
Validation loss: 2.3253279937210904

Epoch: 6| Step: 5
Training loss: 2.0605571269989014
Validation loss: 2.33304859745887

Epoch: 6| Step: 6
Training loss: 2.4386351108551025
Validation loss: 2.3388742477663103

Epoch: 6| Step: 7
Training loss: 2.9874749183654785
Validation loss: 2.3286419453159457

Epoch: 6| Step: 8
Training loss: 2.4705846309661865
Validation loss: 2.3292198437516407

Epoch: 6| Step: 9
Training loss: 1.9499231576919556
Validation loss: 2.337513098152735

Epoch: 6| Step: 10
Training loss: 2.8360395431518555
Validation loss: 2.3319140198410198

Epoch: 6| Step: 11
Training loss: 2.1959824562072754
Validation loss: 2.3225508659116683

Epoch: 6| Step: 12
Training loss: 1.8327535390853882
Validation loss: 2.333667321871686

Epoch: 6| Step: 13
Training loss: 2.30562686920166
Validation loss: 2.3309727548271097

Epoch: 234| Step: 0
Training loss: 2.7073917388916016
Validation loss: 2.3353350803416264

Epoch: 6| Step: 1
Training loss: 3.139643430709839
Validation loss: 2.3386456402399207

Epoch: 6| Step: 2
Training loss: 2.481536865234375
Validation loss: 2.3312726354086273

Epoch: 6| Step: 3
Training loss: 1.6779810190200806
Validation loss: 2.3340006054088636

Epoch: 6| Step: 4
Training loss: 2.787173271179199
Validation loss: 2.3383228830111924

Epoch: 6| Step: 5
Training loss: 2.6008591651916504
Validation loss: 2.341901021618997

Epoch: 6| Step: 6
Training loss: 1.6938594579696655
Validation loss: 2.3444104681732836

Epoch: 6| Step: 7
Training loss: 2.5551581382751465
Validation loss: 2.358680793034133

Epoch: 6| Step: 8
Training loss: 2.287075996398926
Validation loss: 2.352066332294095

Epoch: 6| Step: 9
Training loss: 2.355215549468994
Validation loss: 2.3554459233437814

Epoch: 6| Step: 10
Training loss: 3.6614670753479004
Validation loss: 2.354193784857309

Epoch: 6| Step: 11
Training loss: 2.5002644062042236
Validation loss: 2.3473608750168995

Epoch: 6| Step: 12
Training loss: 2.652614116668701
Validation loss: 2.3445866159213486

Epoch: 6| Step: 13
Training loss: 2.218703269958496
Validation loss: 2.3260128754441456

Epoch: 235| Step: 0
Training loss: 2.4595160484313965
Validation loss: 2.32855720930202

Epoch: 6| Step: 1
Training loss: 2.4567790031433105
Validation loss: 2.3237597660351823

Epoch: 6| Step: 2
Training loss: 2.9470443725585938
Validation loss: 2.3256688758891118

Epoch: 6| Step: 3
Training loss: 3.394935131072998
Validation loss: 2.32045833013391

Epoch: 6| Step: 4
Training loss: 1.8943912982940674
Validation loss: 2.32677694289915

Epoch: 6| Step: 5
Training loss: 2.392199993133545
Validation loss: 2.323111134190713

Epoch: 6| Step: 6
Training loss: 2.100006341934204
Validation loss: 2.329634153714744

Epoch: 6| Step: 7
Training loss: 1.9290707111358643
Validation loss: 2.3240723815015567

Epoch: 6| Step: 8
Training loss: 2.5388147830963135
Validation loss: 2.3355240411655878

Epoch: 6| Step: 9
Training loss: 2.7178878784179688
Validation loss: 2.34163615908674

Epoch: 6| Step: 10
Training loss: 2.5253870487213135
Validation loss: 2.331867858927737

Epoch: 6| Step: 11
Training loss: 2.860015869140625
Validation loss: 2.342684450969901

Epoch: 6| Step: 12
Training loss: 2.299799680709839
Validation loss: 2.3446436235981603

Epoch: 6| Step: 13
Training loss: 3.3279025554656982
Validation loss: 2.3486309718060236

Epoch: 236| Step: 0
Training loss: 2.6609129905700684
Validation loss: 2.350099843035462

Epoch: 6| Step: 1
Training loss: 2.8992114067077637
Validation loss: 2.3303607740709857

Epoch: 6| Step: 2
Training loss: 2.4889559745788574
Validation loss: 2.332298130117437

Epoch: 6| Step: 3
Training loss: 1.8320648670196533
Validation loss: 2.341208370782996

Epoch: 6| Step: 4
Training loss: 2.828460693359375
Validation loss: 2.327704675735966

Epoch: 6| Step: 5
Training loss: 2.960831642150879
Validation loss: 2.3377762507366877

Epoch: 6| Step: 6
Training loss: 2.2646560668945312
Validation loss: 2.329981424475229

Epoch: 6| Step: 7
Training loss: 2.810387134552002
Validation loss: 2.323852813372048

Epoch: 6| Step: 8
Training loss: 2.6262996196746826
Validation loss: 2.3207828050018637

Epoch: 6| Step: 9
Training loss: 2.249357223510742
Validation loss: 2.315741818438294

Epoch: 6| Step: 10
Training loss: 2.1464271545410156
Validation loss: 2.323623637999258

Epoch: 6| Step: 11
Training loss: 2.1034116744995117
Validation loss: 2.3214658537218646

Epoch: 6| Step: 12
Training loss: 3.139334201812744
Validation loss: 2.3154053124048377

Epoch: 6| Step: 13
Training loss: 2.0491456985473633
Validation loss: 2.3155441207270466

Epoch: 237| Step: 0
Training loss: 2.209109306335449
Validation loss: 2.322764200548972

Epoch: 6| Step: 1
Training loss: 3.1238832473754883
Validation loss: 2.323722888064641

Epoch: 6| Step: 2
Training loss: 2.368724822998047
Validation loss: 2.32920322110576

Epoch: 6| Step: 3
Training loss: 3.253448009490967
Validation loss: 2.327990693430747

Epoch: 6| Step: 4
Training loss: 1.7305586338043213
Validation loss: 2.3313686065776373

Epoch: 6| Step: 5
Training loss: 2.8706963062286377
Validation loss: 2.3211560890238774

Epoch: 6| Step: 6
Training loss: 3.2439627647399902
Validation loss: 2.320530914491223

Epoch: 6| Step: 7
Training loss: 2.1827447414398193
Validation loss: 2.326704150886946

Epoch: 6| Step: 8
Training loss: 2.267021656036377
Validation loss: 2.3190844481991184

Epoch: 6| Step: 9
Training loss: 3.133307695388794
Validation loss: 2.3209519181200253

Epoch: 6| Step: 10
Training loss: 1.8147695064544678
Validation loss: 2.318691192134734

Epoch: 6| Step: 11
Training loss: 2.192145824432373
Validation loss: 2.3228988391096874

Epoch: 6| Step: 12
Training loss: 2.685105562210083
Validation loss: 2.33100999042552

Epoch: 6| Step: 13
Training loss: 2.2716128826141357
Validation loss: 2.3263920507123395

Epoch: 238| Step: 0
Training loss: 2.1606063842773438
Validation loss: 2.32216408688535

Epoch: 6| Step: 1
Training loss: 2.125092029571533
Validation loss: 2.3276747580497497

Epoch: 6| Step: 2
Training loss: 2.1306490898132324
Validation loss: 2.3170251359221754

Epoch: 6| Step: 3
Training loss: 2.951955795288086
Validation loss: 2.3212599062150523

Epoch: 6| Step: 4
Training loss: 2.4495067596435547
Validation loss: 2.311196978374194

Epoch: 6| Step: 5
Training loss: 3.193178176879883
Validation loss: 2.31440717686889

Epoch: 6| Step: 6
Training loss: 2.591048240661621
Validation loss: 2.303417698029549

Epoch: 6| Step: 7
Training loss: 2.830099582672119
Validation loss: 2.3125746839789936

Epoch: 6| Step: 8
Training loss: 2.6426138877868652
Validation loss: 2.302915629520211

Epoch: 6| Step: 9
Training loss: 2.5650925636291504
Validation loss: 2.2990292118441675

Epoch: 6| Step: 10
Training loss: 2.037515640258789
Validation loss: 2.2940527751881588

Epoch: 6| Step: 11
Training loss: 3.146007537841797
Validation loss: 2.299941457727904

Epoch: 6| Step: 12
Training loss: 2.1248369216918945
Validation loss: 2.3039619743183093

Epoch: 6| Step: 13
Training loss: 2.823091745376587
Validation loss: 2.303575256819366

Epoch: 239| Step: 0
Training loss: 3.0348246097564697
Validation loss: 2.316030756119759

Epoch: 6| Step: 1
Training loss: 2.6244473457336426
Validation loss: 2.3004167079925537

Epoch: 6| Step: 2
Training loss: 2.037970781326294
Validation loss: 2.306539673959055

Epoch: 6| Step: 3
Training loss: 2.592559814453125
Validation loss: 2.3033418360576836

Epoch: 6| Step: 4
Training loss: 2.1979713439941406
Validation loss: 2.300550191633163

Epoch: 6| Step: 5
Training loss: 2.754347562789917
Validation loss: 2.3073490345349876

Epoch: 6| Step: 6
Training loss: 2.702427387237549
Validation loss: 2.3082401547380673

Epoch: 6| Step: 7
Training loss: 2.0810861587524414
Validation loss: 2.30565474366629

Epoch: 6| Step: 8
Training loss: 2.552276611328125
Validation loss: 2.3076238760384182

Epoch: 6| Step: 9
Training loss: 2.0061464309692383
Validation loss: 2.3142646333222747

Epoch: 6| Step: 10
Training loss: 2.8404674530029297
Validation loss: 2.3119763417910506

Epoch: 6| Step: 11
Training loss: 2.780916929244995
Validation loss: 2.3130279740979596

Epoch: 6| Step: 12
Training loss: 2.6363391876220703
Validation loss: 2.3171606474025275

Epoch: 6| Step: 13
Training loss: 3.036048173904419
Validation loss: 2.309541368997225

Epoch: 240| Step: 0
Training loss: 2.987161159515381
Validation loss: 2.325445939135808

Epoch: 6| Step: 1
Training loss: 2.9748756885528564
Validation loss: 2.32139063906926

Epoch: 6| Step: 2
Training loss: 2.84200382232666
Validation loss: 2.3139195621654554

Epoch: 6| Step: 3
Training loss: 1.8676118850708008
Validation loss: 2.307500341887115

Epoch: 6| Step: 4
Training loss: 2.6155500411987305
Validation loss: 2.306990344037292

Epoch: 6| Step: 5
Training loss: 2.4620611667633057
Validation loss: 2.2965443416308333

Epoch: 6| Step: 6
Training loss: 2.004650115966797
Validation loss: 2.304883728745163

Epoch: 6| Step: 7
Training loss: 2.632422685623169
Validation loss: 2.3019469220151185

Epoch: 6| Step: 8
Training loss: 2.945446491241455
Validation loss: 2.2971332765394643

Epoch: 6| Step: 9
Training loss: 2.2755613327026367
Validation loss: 2.3104147705980527

Epoch: 6| Step: 10
Training loss: 2.7254483699798584
Validation loss: 2.3195282669477564

Epoch: 6| Step: 11
Training loss: 2.324127674102783
Validation loss: 2.326165094170519

Epoch: 6| Step: 12
Training loss: 2.2544987201690674
Validation loss: 2.3323694557271977

Epoch: 6| Step: 13
Training loss: 2.712266683578491
Validation loss: 2.3230418518025386

Epoch: 241| Step: 0
Training loss: 2.9401700496673584
Validation loss: 2.3150206406911216

Epoch: 6| Step: 1
Training loss: 2.9077200889587402
Validation loss: 2.310685101375785

Epoch: 6| Step: 2
Training loss: 2.8851068019866943
Validation loss: 2.3080229067033335

Epoch: 6| Step: 3
Training loss: 2.545797824859619
Validation loss: 2.2972995324801375

Epoch: 6| Step: 4
Training loss: 2.9336752891540527
Validation loss: 2.3012336787357124

Epoch: 6| Step: 5
Training loss: 1.7217185497283936
Validation loss: 2.3039343895450717

Epoch: 6| Step: 6
Training loss: 1.7317336797714233
Validation loss: 2.3032567654886553

Epoch: 6| Step: 7
Training loss: 2.92366361618042
Validation loss: 2.303249510385657

Epoch: 6| Step: 8
Training loss: 2.290523052215576
Validation loss: 2.300344855554642

Epoch: 6| Step: 9
Training loss: 2.474439859390259
Validation loss: 2.3121043251406763

Epoch: 6| Step: 10
Training loss: 2.652047634124756
Validation loss: 2.304649519663985

Epoch: 6| Step: 11
Training loss: 2.069953441619873
Validation loss: 2.31088928509784

Epoch: 6| Step: 12
Training loss: 2.589236259460449
Validation loss: 2.306235844089139

Epoch: 6| Step: 13
Training loss: 2.9301681518554688
Validation loss: 2.314381742990145

Epoch: 242| Step: 0
Training loss: 2.7481110095977783
Validation loss: 2.332671429521294

Epoch: 6| Step: 1
Training loss: 2.672590494155884
Validation loss: 2.3202786727618148

Epoch: 6| Step: 2
Training loss: 2.7824199199676514
Validation loss: 2.3203373186049925

Epoch: 6| Step: 3
Training loss: 1.8143396377563477
Validation loss: 2.324099853474607

Epoch: 6| Step: 4
Training loss: 2.112318515777588
Validation loss: 2.3248631031282487

Epoch: 6| Step: 5
Training loss: 2.8622772693634033
Validation loss: 2.315012785696214

Epoch: 6| Step: 6
Training loss: 2.1595709323883057
Validation loss: 2.323097934005081

Epoch: 6| Step: 7
Training loss: 2.0158963203430176
Validation loss: 2.3303457229368147

Epoch: 6| Step: 8
Training loss: 2.604386806488037
Validation loss: 2.329637683847899

Epoch: 6| Step: 9
Training loss: 2.6231741905212402
Validation loss: 2.335006017838755

Epoch: 6| Step: 10
Training loss: 2.38238525390625
Validation loss: 2.31851875910195

Epoch: 6| Step: 11
Training loss: 2.8020336627960205
Validation loss: 2.32080481642036

Epoch: 6| Step: 12
Training loss: 3.3667900562286377
Validation loss: 2.314598653906135

Epoch: 6| Step: 13
Training loss: 2.074413299560547
Validation loss: 2.316884817615632

Epoch: 243| Step: 0
Training loss: 2.678576946258545
Validation loss: 2.315505576390092

Epoch: 6| Step: 1
Training loss: 2.6397366523742676
Validation loss: 2.3157794526828233

Epoch: 6| Step: 2
Training loss: 2.79062557220459
Validation loss: 2.325315075535928

Epoch: 6| Step: 3
Training loss: 2.708048105239868
Validation loss: 2.329958531164354

Epoch: 6| Step: 4
Training loss: 2.119717597961426
Validation loss: 2.3381779834788334

Epoch: 6| Step: 5
Training loss: 2.105017900466919
Validation loss: 2.3267557851729856

Epoch: 6| Step: 6
Training loss: 2.5356898307800293
Validation loss: 2.3222796929779874

Epoch: 6| Step: 7
Training loss: 2.0352442264556885
Validation loss: 2.3323808049642913

Epoch: 6| Step: 8
Training loss: 2.412656784057617
Validation loss: 2.3301502479019987

Epoch: 6| Step: 9
Training loss: 2.472477436065674
Validation loss: 2.326040070544007

Epoch: 6| Step: 10
Training loss: 2.511620283126831
Validation loss: 2.323738985164191

Epoch: 6| Step: 11
Training loss: 2.60434627532959
Validation loss: 2.3213403609491166

Epoch: 6| Step: 12
Training loss: 2.942134141921997
Validation loss: 2.317523137215645

Epoch: 6| Step: 13
Training loss: 2.508120536804199
Validation loss: 2.3145272116507254

Epoch: 244| Step: 0
Training loss: 2.4277520179748535
Validation loss: 2.309380805620583

Epoch: 6| Step: 1
Training loss: 2.773777484893799
Validation loss: 2.31450375177527

Epoch: 6| Step: 2
Training loss: 2.5699377059936523
Validation loss: 2.3239081213551183

Epoch: 6| Step: 3
Training loss: 2.622284412384033
Validation loss: 2.3112943544182727

Epoch: 6| Step: 4
Training loss: 2.431887149810791
Validation loss: 2.30183135309527

Epoch: 6| Step: 5
Training loss: 1.6932120323181152
Validation loss: 2.3086302921336186

Epoch: 6| Step: 6
Training loss: 2.8440637588500977
Validation loss: 2.3082052123162056

Epoch: 6| Step: 7
Training loss: 2.258943557739258
Validation loss: 2.309029702217348

Epoch: 6| Step: 8
Training loss: 2.859165668487549
Validation loss: 2.3095191012146654

Epoch: 6| Step: 9
Training loss: 2.1426680088043213
Validation loss: 2.310471532165363

Epoch: 6| Step: 10
Training loss: 2.8095808029174805
Validation loss: 2.3226360172353764

Epoch: 6| Step: 11
Training loss: 2.566288471221924
Validation loss: 2.31204749948235

Epoch: 6| Step: 12
Training loss: 2.501466989517212
Validation loss: 2.3141293576968613

Epoch: 6| Step: 13
Training loss: 2.647714614868164
Validation loss: 2.3118820613430393

Epoch: 245| Step: 0
Training loss: 2.3182029724121094
Validation loss: 2.3243621703117125

Epoch: 6| Step: 1
Training loss: 2.4884753227233887
Validation loss: 2.3238429395101403

Epoch: 6| Step: 2
Training loss: 2.2378365993499756
Validation loss: 2.3256937278214322

Epoch: 6| Step: 3
Training loss: 2.763803005218506
Validation loss: 2.334094939693328

Epoch: 6| Step: 4
Training loss: 2.1375937461853027
Validation loss: 2.3312336860164518

Epoch: 6| Step: 5
Training loss: 2.402096748352051
Validation loss: 2.338922826192712

Epoch: 6| Step: 6
Training loss: 2.451232433319092
Validation loss: 2.3458792137843307

Epoch: 6| Step: 7
Training loss: 2.759028434753418
Validation loss: 2.346669638028709

Epoch: 6| Step: 8
Training loss: 2.1355698108673096
Validation loss: 2.3330777845075055

Epoch: 6| Step: 9
Training loss: 2.6445047855377197
Validation loss: 2.3470849939571914

Epoch: 6| Step: 10
Training loss: 2.964268207550049
Validation loss: 2.3411489455930647

Epoch: 6| Step: 11
Training loss: 2.9879207611083984
Validation loss: 2.338555815399334

Epoch: 6| Step: 12
Training loss: 1.94390869140625
Validation loss: 2.3443418600225963

Epoch: 6| Step: 13
Training loss: 3.2637007236480713
Validation loss: 2.3433086179917857

Epoch: 246| Step: 0
Training loss: 2.7457430362701416
Validation loss: 2.326515861736831

Epoch: 6| Step: 1
Training loss: 3.9207587242126465
Validation loss: 2.3255603723628546

Epoch: 6| Step: 2
Training loss: 2.6308178901672363
Validation loss: 2.319413931139054

Epoch: 6| Step: 3
Training loss: 2.5033905506134033
Validation loss: 2.320451118612802

Epoch: 6| Step: 4
Training loss: 3.0140857696533203
Validation loss: 2.315178991645895

Epoch: 6| Step: 5
Training loss: 2.12168025970459
Validation loss: 2.31871102445869

Epoch: 6| Step: 6
Training loss: 2.2260427474975586
Validation loss: 2.30772231471154

Epoch: 6| Step: 7
Training loss: 2.253650665283203
Validation loss: 2.295152915421353

Epoch: 6| Step: 8
Training loss: 1.5253392457962036
Validation loss: 2.2893978113769204

Epoch: 6| Step: 9
Training loss: 2.849825859069824
Validation loss: 2.283348169378055

Epoch: 6| Step: 10
Training loss: 2.971747398376465
Validation loss: 2.2923139859271306

Epoch: 6| Step: 11
Training loss: 2.11482834815979
Validation loss: 2.292989651362101

Epoch: 6| Step: 12
Training loss: 2.153852939605713
Validation loss: 2.297598751642371

Epoch: 6| Step: 13
Training loss: 2.0398592948913574
Validation loss: 2.305035465507097

Epoch: 247| Step: 0
Training loss: 1.993752360343933
Validation loss: 2.30558475243148

Epoch: 6| Step: 1
Training loss: 3.3214941024780273
Validation loss: 2.3125684235685613

Epoch: 6| Step: 2
Training loss: 2.759300708770752
Validation loss: 2.316433575845534

Epoch: 6| Step: 3
Training loss: 1.9306957721710205
Validation loss: 2.3178383560590845

Epoch: 6| Step: 4
Training loss: 2.732325553894043
Validation loss: 2.329272216366183

Epoch: 6| Step: 5
Training loss: 1.9411213397979736
Validation loss: 2.3288783065734373

Epoch: 6| Step: 6
Training loss: 2.6249356269836426
Validation loss: 2.3458995255090858

Epoch: 6| Step: 7
Training loss: 2.6830012798309326
Validation loss: 2.341559056312807

Epoch: 6| Step: 8
Training loss: 2.583380699157715
Validation loss: 2.3499344164325344

Epoch: 6| Step: 9
Training loss: 3.0783205032348633
Validation loss: 2.360363369346947

Epoch: 6| Step: 10
Training loss: 2.4317805767059326
Validation loss: 2.335876203352405

Epoch: 6| Step: 11
Training loss: 2.1465344429016113
Validation loss: 2.339609443500478

Epoch: 6| Step: 12
Training loss: 2.145477533340454
Validation loss: 2.3402036902725056

Epoch: 6| Step: 13
Training loss: 3.276294231414795
Validation loss: 2.3249492747809297

Epoch: 248| Step: 0
Training loss: 2.615058422088623
Validation loss: 2.3180112633653867

Epoch: 6| Step: 1
Training loss: 2.527538776397705
Validation loss: 2.309976362412976

Epoch: 6| Step: 2
Training loss: 2.6464641094207764
Validation loss: 2.309203770852858

Epoch: 6| Step: 3
Training loss: 1.6390855312347412
Validation loss: 2.3046577028048936

Epoch: 6| Step: 4
Training loss: 1.8568445444107056
Validation loss: 2.3139346902088453

Epoch: 6| Step: 5
Training loss: 3.0976343154907227
Validation loss: 2.3148065459343696

Epoch: 6| Step: 6
Training loss: 2.5886054039001465
Validation loss: 2.317499481221681

Epoch: 6| Step: 7
Training loss: 2.9558823108673096
Validation loss: 2.3238970925731044

Epoch: 6| Step: 8
Training loss: 2.0452709197998047
Validation loss: 2.3310597199265675

Epoch: 6| Step: 9
Training loss: 2.9388999938964844
Validation loss: 2.3311510470605667

Epoch: 6| Step: 10
Training loss: 2.6122303009033203
Validation loss: 2.328582348362092

Epoch: 6| Step: 11
Training loss: 2.611940860748291
Validation loss: 2.3208286018781763

Epoch: 6| Step: 12
Training loss: 2.9330592155456543
Validation loss: 2.308561950601557

Epoch: 6| Step: 13
Training loss: 2.43349027633667
Validation loss: 2.3052062590916953

Epoch: 249| Step: 0
Training loss: 3.8059799671173096
Validation loss: 2.303979783929804

Epoch: 6| Step: 1
Training loss: 2.7100067138671875
Validation loss: 2.287781189846736

Epoch: 6| Step: 2
Training loss: 2.300853967666626
Validation loss: 2.285159341750606

Epoch: 6| Step: 3
Training loss: 2.087144613265991
Validation loss: 2.271792073403635

Epoch: 6| Step: 4
Training loss: 2.494302749633789
Validation loss: 2.276551229979402

Epoch: 6| Step: 5
Training loss: 2.5344789028167725
Validation loss: 2.273557133572076

Epoch: 6| Step: 6
Training loss: 1.7716821432113647
Validation loss: 2.2778978168323474

Epoch: 6| Step: 7
Training loss: 3.5174906253814697
Validation loss: 2.278078056150867

Epoch: 6| Step: 8
Training loss: 3.1806044578552246
Validation loss: 2.2881170344609085

Epoch: 6| Step: 9
Training loss: 2.6642260551452637
Validation loss: 2.2899476328203754

Epoch: 6| Step: 10
Training loss: 2.053419828414917
Validation loss: 2.2983019095595165

Epoch: 6| Step: 11
Training loss: 1.9967952966690063
Validation loss: 2.302753602304766

Epoch: 6| Step: 12
Training loss: 2.220909595489502
Validation loss: 2.301661632394278

Epoch: 6| Step: 13
Training loss: 1.7454115152359009
Validation loss: 2.294460427376532

Epoch: 250| Step: 0
Training loss: 2.2639403343200684
Validation loss: 2.3062869861561763

Epoch: 6| Step: 1
Training loss: 2.6765990257263184
Validation loss: 2.3020764050945157

Epoch: 6| Step: 2
Training loss: 2.1837944984436035
Validation loss: 2.3102267314028997

Epoch: 6| Step: 3
Training loss: 2.397735118865967
Validation loss: 2.316445924902475

Epoch: 6| Step: 4
Training loss: 3.2951226234436035
Validation loss: 2.327148481081891

Epoch: 6| Step: 5
Training loss: 1.5019469261169434
Validation loss: 2.3313076393578642

Epoch: 6| Step: 6
Training loss: 2.8961989879608154
Validation loss: 2.3452757327787337

Epoch: 6| Step: 7
Training loss: 3.1208043098449707
Validation loss: 2.3610773342911915

Epoch: 6| Step: 8
Training loss: 2.7046093940734863
Validation loss: 2.3657377458387807

Epoch: 6| Step: 9
Training loss: 3.04764461517334
Validation loss: 2.3554107014850905

Epoch: 6| Step: 10
Training loss: 2.0433242321014404
Validation loss: 2.3450057762925343

Epoch: 6| Step: 11
Training loss: 2.2390902042388916
Validation loss: 2.339779359038158

Epoch: 6| Step: 12
Training loss: 2.636744499206543
Validation loss: 2.337106148401896

Epoch: 6| Step: 13
Training loss: 2.3060925006866455
Validation loss: 2.322761138280233

Epoch: 251| Step: 0
Training loss: 1.6735754013061523
Validation loss: 2.321734397642074

Epoch: 6| Step: 1
Training loss: 3.20304274559021
Validation loss: 2.3230807037763697

Epoch: 6| Step: 2
Training loss: 2.7377095222473145
Validation loss: 2.3139217899691675

Epoch: 6| Step: 3
Training loss: 2.7942702770233154
Validation loss: 2.3303406264192317

Epoch: 6| Step: 4
Training loss: 3.1964709758758545
Validation loss: 2.314062623567479

Epoch: 6| Step: 5
Training loss: 2.9843573570251465
Validation loss: 2.304777878586964

Epoch: 6| Step: 6
Training loss: 2.951848030090332
Validation loss: 2.3051532686397596

Epoch: 6| Step: 7
Training loss: 2.100811004638672
Validation loss: 2.3053232392957135

Epoch: 6| Step: 8
Training loss: 2.088351249694824
Validation loss: 2.288880148241597

Epoch: 6| Step: 9
Training loss: 1.9306180477142334
Validation loss: 2.293487997465236

Epoch: 6| Step: 10
Training loss: 2.8512415885925293
Validation loss: 2.2899319151396393

Epoch: 6| Step: 11
Training loss: 2.146054744720459
Validation loss: 2.288583519638226

Epoch: 6| Step: 12
Training loss: 2.1561315059661865
Validation loss: 2.2869497217157835

Epoch: 6| Step: 13
Training loss: 2.4788215160369873
Validation loss: 2.2975215834956013

Epoch: 252| Step: 0
Training loss: 2.1816835403442383
Validation loss: 2.291206159899312

Epoch: 6| Step: 1
Training loss: 2.9727659225463867
Validation loss: 2.3017811441934235

Epoch: 6| Step: 2
Training loss: 2.2083230018615723
Validation loss: 2.2974602548024987

Epoch: 6| Step: 3
Training loss: 2.962186813354492
Validation loss: 2.290204542939381

Epoch: 6| Step: 4
Training loss: 2.690033435821533
Validation loss: 2.286717971165975

Epoch: 6| Step: 5
Training loss: 2.8403759002685547
Validation loss: 2.2823911200287523

Epoch: 6| Step: 6
Training loss: 2.9217326641082764
Validation loss: 2.2842928235248854

Epoch: 6| Step: 7
Training loss: 2.322077989578247
Validation loss: 2.2774295653066328

Epoch: 6| Step: 8
Training loss: 2.6900157928466797
Validation loss: 2.2707921266555786

Epoch: 6| Step: 9
Training loss: 2.3665597438812256
Validation loss: 2.276480649107246

Epoch: 6| Step: 10
Training loss: 2.219542980194092
Validation loss: 2.2769966202397502

Epoch: 6| Step: 11
Training loss: 2.4205915927886963
Validation loss: 2.2695046765829927

Epoch: 6| Step: 12
Training loss: 2.1462392807006836
Validation loss: 2.282038352822745

Epoch: 6| Step: 13
Training loss: 2.0938758850097656
Validation loss: 2.283863913628363

Epoch: 253| Step: 0
Training loss: 1.9138526916503906
Validation loss: 2.296946660164864

Epoch: 6| Step: 1
Training loss: 2.4189696311950684
Validation loss: 2.3096552587324575

Epoch: 6| Step: 2
Training loss: 3.1341567039489746
Validation loss: 2.322266286419284

Epoch: 6| Step: 3
Training loss: 2.9912326335906982
Validation loss: 2.3255449648826354

Epoch: 6| Step: 4
Training loss: 2.404695987701416
Validation loss: 2.3368430740089825

Epoch: 6| Step: 5
Training loss: 1.983229398727417
Validation loss: 2.353526843491421

Epoch: 6| Step: 6
Training loss: 2.1874685287475586
Validation loss: 2.3630147787832443

Epoch: 6| Step: 7
Training loss: 2.686244010925293
Validation loss: 2.370616228349747

Epoch: 6| Step: 8
Training loss: 2.4824466705322266
Validation loss: 2.3954925050017652

Epoch: 6| Step: 9
Training loss: 2.619114875793457
Validation loss: 2.4155058245505057

Epoch: 6| Step: 10
Training loss: 2.2841224670410156
Validation loss: 2.410221525417861

Epoch: 6| Step: 11
Training loss: 3.175701379776001
Validation loss: 2.39501005090693

Epoch: 6| Step: 12
Training loss: 2.148498058319092
Validation loss: 2.3776744847656577

Epoch: 6| Step: 13
Training loss: 3.080486536026001
Validation loss: 2.3385749542584984

Epoch: 254| Step: 0
Training loss: 2.36358642578125
Validation loss: 2.3171989353754188

Epoch: 6| Step: 1
Training loss: 2.293948173522949
Validation loss: 2.3173621316109934

Epoch: 6| Step: 2
Training loss: 2.202887773513794
Validation loss: 2.326044669715307

Epoch: 6| Step: 3
Training loss: 3.2758100032806396
Validation loss: 2.322568393522693

Epoch: 6| Step: 4
Training loss: 3.008495807647705
Validation loss: 2.3177716270569833

Epoch: 6| Step: 5
Training loss: 2.7320497035980225
Validation loss: 2.3095492650103826

Epoch: 6| Step: 6
Training loss: 2.079697608947754
Validation loss: 2.319150404263568

Epoch: 6| Step: 7
Training loss: 2.4043092727661133
Validation loss: 2.305367628733317

Epoch: 6| Step: 8
Training loss: 2.712332248687744
Validation loss: 2.301456089942686

Epoch: 6| Step: 9
Training loss: 2.219010829925537
Validation loss: 2.2985098797787904

Epoch: 6| Step: 10
Training loss: 2.74777889251709
Validation loss: 2.279783853920557

Epoch: 6| Step: 11
Training loss: 2.206474781036377
Validation loss: 2.2752077925589775

Epoch: 6| Step: 12
Training loss: 2.278287172317505
Validation loss: 2.278457191682631

Epoch: 6| Step: 13
Training loss: 2.9798905849456787
Validation loss: 2.2830252698672715

Epoch: 255| Step: 0
Training loss: 2.0868096351623535
Validation loss: 2.285189364546089

Epoch: 6| Step: 1
Training loss: 2.5015861988067627
Validation loss: 2.2827525754128732

Epoch: 6| Step: 2
Training loss: 2.642576217651367
Validation loss: 2.2812417963499665

Epoch: 6| Step: 3
Training loss: 2.3672866821289062
Validation loss: 2.2798145637717298

Epoch: 6| Step: 4
Training loss: 2.352484941482544
Validation loss: 2.2841172474686817

Epoch: 6| Step: 5
Training loss: 2.625678539276123
Validation loss: 2.286456267038981

Epoch: 6| Step: 6
Training loss: 2.6186037063598633
Validation loss: 2.2912743553038566

Epoch: 6| Step: 7
Training loss: 2.510807514190674
Validation loss: 2.3013607225110455

Epoch: 6| Step: 8
Training loss: 2.520080804824829
Validation loss: 2.2903148692141295

Epoch: 6| Step: 9
Training loss: 3.022294521331787
Validation loss: 2.286687627915413

Epoch: 6| Step: 10
Training loss: 2.4300992488861084
Validation loss: 2.286530463926254

Epoch: 6| Step: 11
Training loss: 2.9457480907440186
Validation loss: 2.291193410914431

Epoch: 6| Step: 12
Training loss: 2.0946457386016846
Validation loss: 2.283996189794233

Epoch: 6| Step: 13
Training loss: 2.2782278060913086
Validation loss: 2.28382549875526

Epoch: 256| Step: 0
Training loss: 1.8491060733795166
Validation loss: 2.2836438558434926

Epoch: 6| Step: 1
Training loss: 2.4486441612243652
Validation loss: 2.2989437272471767

Epoch: 6| Step: 2
Training loss: 1.9523108005523682
Validation loss: 2.3102233896973314

Epoch: 6| Step: 3
Training loss: 2.6518542766571045
Validation loss: 2.3121854105303363

Epoch: 6| Step: 4
Training loss: 2.8309149742126465
Validation loss: 2.305428776689755

Epoch: 6| Step: 5
Training loss: 2.7857370376586914
Validation loss: 2.309560737302226

Epoch: 6| Step: 6
Training loss: 2.441286563873291
Validation loss: 2.31090425163187

Epoch: 6| Step: 7
Training loss: 2.6035537719726562
Validation loss: 2.308104207438807

Epoch: 6| Step: 8
Training loss: 2.333285093307495
Validation loss: 2.3047084705803984

Epoch: 6| Step: 9
Training loss: 2.9390063285827637
Validation loss: 2.32233896434948

Epoch: 6| Step: 10
Training loss: 2.485431671142578
Validation loss: 2.32978880277244

Epoch: 6| Step: 11
Training loss: 2.205310821533203
Validation loss: 2.3222530605972453

Epoch: 6| Step: 12
Training loss: 2.4370949268341064
Validation loss: 2.3339107216045423

Epoch: 6| Step: 13
Training loss: 3.637556314468384
Validation loss: 2.3285654398702804

Epoch: 257| Step: 0
Training loss: 3.203242063522339
Validation loss: 2.3192067248846895

Epoch: 6| Step: 1
Training loss: 2.5312206745147705
Validation loss: 2.31781828788019

Epoch: 6| Step: 2
Training loss: 2.0486016273498535
Validation loss: 2.3118753381954726

Epoch: 6| Step: 3
Training loss: 2.7351598739624023
Validation loss: 2.3117795118721585

Epoch: 6| Step: 4
Training loss: 2.8130574226379395
Validation loss: 2.306654396877494

Epoch: 6| Step: 5
Training loss: 2.5861761569976807
Validation loss: 2.296174574923772

Epoch: 6| Step: 6
Training loss: 1.4826539754867554
Validation loss: 2.2964123167017454

Epoch: 6| Step: 7
Training loss: 2.169828176498413
Validation loss: 2.286931340412427

Epoch: 6| Step: 8
Training loss: 2.838151216506958
Validation loss: 2.2838413433362077

Epoch: 6| Step: 9
Training loss: 2.119663953781128
Validation loss: 2.2819973473907798

Epoch: 6| Step: 10
Training loss: 3.015775680541992
Validation loss: 2.28397282220984

Epoch: 6| Step: 11
Training loss: 2.537616491317749
Validation loss: 2.2772081923741165

Epoch: 6| Step: 12
Training loss: 2.4887075424194336
Validation loss: 2.282650870661582

Epoch: 6| Step: 13
Training loss: 2.131802797317505
Validation loss: 2.287386830135058

Epoch: 258| Step: 0
Training loss: 1.9358272552490234
Validation loss: 2.2976000001353603

Epoch: 6| Step: 1
Training loss: 2.3527863025665283
Validation loss: 2.3002474756651026

Epoch: 6| Step: 2
Training loss: 2.06646728515625
Validation loss: 2.2896263727577786

Epoch: 6| Step: 3
Training loss: 2.9962875843048096
Validation loss: 2.2934805475255495

Epoch: 6| Step: 4
Training loss: 2.725645065307617
Validation loss: 2.2871667569683445

Epoch: 6| Step: 5
Training loss: 3.5446600914001465
Validation loss: 2.2741104915577877

Epoch: 6| Step: 6
Training loss: 2.199551820755005
Validation loss: 2.2918191725207913

Epoch: 6| Step: 7
Training loss: 2.7563765048980713
Validation loss: 2.2846036162427676

Epoch: 6| Step: 8
Training loss: 1.9185553789138794
Validation loss: 2.295838176563222

Epoch: 6| Step: 9
Training loss: 2.5151174068450928
Validation loss: 2.2922847065874326

Epoch: 6| Step: 10
Training loss: 1.9272820949554443
Validation loss: 2.299324622718237

Epoch: 6| Step: 11
Training loss: 2.738884687423706
Validation loss: 2.304120368854974

Epoch: 6| Step: 12
Training loss: 2.345076322555542
Validation loss: 2.3132195677808536

Epoch: 6| Step: 13
Training loss: 3.2574005126953125
Validation loss: 2.3111057742949455

Epoch: 259| Step: 0
Training loss: 2.20699405670166
Validation loss: 2.318858269722231

Epoch: 6| Step: 1
Training loss: 2.522615909576416
Validation loss: 2.3171540485915316

Epoch: 6| Step: 2
Training loss: 2.3322558403015137
Validation loss: 2.3200994909450574

Epoch: 6| Step: 3
Training loss: 2.737792491912842
Validation loss: 2.3240491523537585

Epoch: 6| Step: 4
Training loss: 2.670294761657715
Validation loss: 2.311163240863431

Epoch: 6| Step: 5
Training loss: 1.9443135261535645
Validation loss: 2.30840285875464

Epoch: 6| Step: 6
Training loss: 2.4784913063049316
Validation loss: 2.2925791484053417

Epoch: 6| Step: 7
Training loss: 3.0895867347717285
Validation loss: 2.287844842480075

Epoch: 6| Step: 8
Training loss: 2.776515007019043
Validation loss: 2.2867854474693217

Epoch: 6| Step: 9
Training loss: 2.73494815826416
Validation loss: 2.2836078469471266

Epoch: 6| Step: 10
Training loss: 2.9438724517822266
Validation loss: 2.284924566104848

Epoch: 6| Step: 11
Training loss: 2.646965742111206
Validation loss: 2.276473891350531

Epoch: 6| Step: 12
Training loss: 2.1040902137756348
Validation loss: 2.281240540166055

Epoch: 6| Step: 13
Training loss: 0.9897356033325195
Validation loss: 2.282588384484732

Epoch: 260| Step: 0
Training loss: 1.7875949144363403
Validation loss: 2.280884201808642

Epoch: 6| Step: 1
Training loss: 2.267780303955078
Validation loss: 2.2861083733138217

Epoch: 6| Step: 2
Training loss: 2.677773952484131
Validation loss: 2.294809259394164

Epoch: 6| Step: 3
Training loss: 2.529472589492798
Validation loss: 2.299308784546391

Epoch: 6| Step: 4
Training loss: 2.482541084289551
Validation loss: 2.293992926997523

Epoch: 6| Step: 5
Training loss: 2.556283950805664
Validation loss: 2.298277667773667

Epoch: 6| Step: 6
Training loss: 3.2674248218536377
Validation loss: 2.3022447042567755

Epoch: 6| Step: 7
Training loss: 1.3496087789535522
Validation loss: 2.3012754917144775

Epoch: 6| Step: 8
Training loss: 3.3253211975097656
Validation loss: 2.300434922659269

Epoch: 6| Step: 9
Training loss: 2.6018431186676025
Validation loss: 2.305523810848113

Epoch: 6| Step: 10
Training loss: 2.0999221801757812
Validation loss: 2.3050987925580753

Epoch: 6| Step: 11
Training loss: 2.367845058441162
Validation loss: 2.297694262637887

Epoch: 6| Step: 12
Training loss: 3.1779141426086426
Validation loss: 2.303933569180068

Epoch: 6| Step: 13
Training loss: 2.252931833267212
Validation loss: 2.2985073238290767

Epoch: 261| Step: 0
Training loss: 2.8998618125915527
Validation loss: 2.2832086137546006

Epoch: 6| Step: 1
Training loss: 2.1991419792175293
Validation loss: 2.278723680844871

Epoch: 6| Step: 2
Training loss: 3.474411964416504
Validation loss: 2.2781263371949554

Epoch: 6| Step: 3
Training loss: 2.67795991897583
Validation loss: 2.2836193115480485

Epoch: 6| Step: 4
Training loss: 2.0415964126586914
Validation loss: 2.280084994531447

Epoch: 6| Step: 5
Training loss: 2.7319562435150146
Validation loss: 2.276167354276103

Epoch: 6| Step: 6
Training loss: 2.7014167308807373
Validation loss: 2.2833635935219387

Epoch: 6| Step: 7
Training loss: 2.53725266456604
Validation loss: 2.2706705344620572

Epoch: 6| Step: 8
Training loss: 2.9409422874450684
Validation loss: 2.280686011878393

Epoch: 6| Step: 9
Training loss: 1.4159663915634155
Validation loss: 2.2672132753556773

Epoch: 6| Step: 10
Training loss: 1.7312761545181274
Validation loss: 2.267903656087896

Epoch: 6| Step: 11
Training loss: 2.446969509124756
Validation loss: 2.261310128755467

Epoch: 6| Step: 12
Training loss: 2.2389206886291504
Validation loss: 2.2692717890585623

Epoch: 6| Step: 13
Training loss: 3.0062975883483887
Validation loss: 2.265416863144085

Epoch: 262| Step: 0
Training loss: 2.599151134490967
Validation loss: 2.268056474706178

Epoch: 6| Step: 1
Training loss: 2.547335624694824
Validation loss: 2.2639513195201917

Epoch: 6| Step: 2
Training loss: 1.8112238645553589
Validation loss: 2.264890101648146

Epoch: 6| Step: 3
Training loss: 2.5740370750427246
Validation loss: 2.2701001141660955

Epoch: 6| Step: 4
Training loss: 2.473414182662964
Validation loss: 2.2758733610953055

Epoch: 6| Step: 5
Training loss: 2.7882213592529297
Validation loss: 2.2736550210624613

Epoch: 6| Step: 6
Training loss: 2.0991692543029785
Validation loss: 2.278959079455304

Epoch: 6| Step: 7
Training loss: 2.8105931282043457
Validation loss: 2.291777226232713

Epoch: 6| Step: 8
Training loss: 2.8950157165527344
Validation loss: 2.292975305229105

Epoch: 6| Step: 9
Training loss: 2.50704288482666
Validation loss: 2.285618564134003

Epoch: 6| Step: 10
Training loss: 2.0311057567596436
Validation loss: 2.296415171315593

Epoch: 6| Step: 11
Training loss: 2.531550884246826
Validation loss: 2.28682739503922

Epoch: 6| Step: 12
Training loss: 2.986090898513794
Validation loss: 2.2919495977381223

Epoch: 6| Step: 13
Training loss: 2.0508968830108643
Validation loss: 2.29237695406842

Epoch: 263| Step: 0
Training loss: 3.3264570236206055
Validation loss: 2.307101203549293

Epoch: 6| Step: 1
Training loss: 2.5811727046966553
Validation loss: 2.296973571982435

Epoch: 6| Step: 2
Training loss: 2.3120951652526855
Validation loss: 2.301321929500949

Epoch: 6| Step: 3
Training loss: 2.5349955558776855
Validation loss: 2.299131654923962

Epoch: 6| Step: 4
Training loss: 2.1760075092315674
Validation loss: 2.2930541858878186

Epoch: 6| Step: 5
Training loss: 2.4557442665100098
Validation loss: 2.3042981034965924

Epoch: 6| Step: 6
Training loss: 3.041090726852417
Validation loss: 2.3094496111715994

Epoch: 6| Step: 7
Training loss: 2.985935688018799
Validation loss: 2.2965745977176133

Epoch: 6| Step: 8
Training loss: 2.450136661529541
Validation loss: 2.3107617824308333

Epoch: 6| Step: 9
Training loss: 2.3094468116760254
Validation loss: 2.2948999789453324

Epoch: 6| Step: 10
Training loss: 1.6441235542297363
Validation loss: 2.2908386620142127

Epoch: 6| Step: 11
Training loss: 2.038410186767578
Validation loss: 2.306959782877276

Epoch: 6| Step: 12
Training loss: 2.7988622188568115
Validation loss: 2.3083276133383475

Epoch: 6| Step: 13
Training loss: 1.8231574296951294
Validation loss: 2.313842793946625

Epoch: 264| Step: 0
Training loss: 2.8810110092163086
Validation loss: 2.3173131430020897

Epoch: 6| Step: 1
Training loss: 2.4767298698425293
Validation loss: 2.3195827955840738

Epoch: 6| Step: 2
Training loss: 2.369372844696045
Validation loss: 2.3143644614886214

Epoch: 6| Step: 3
Training loss: 2.9405174255371094
Validation loss: 2.3055251888049546

Epoch: 6| Step: 4
Training loss: 2.5647175312042236
Validation loss: 2.2942659162705943

Epoch: 6| Step: 5
Training loss: 2.4357399940490723
Validation loss: 2.285320774201424

Epoch: 6| Step: 6
Training loss: 2.864617347717285
Validation loss: 2.2797442072181293

Epoch: 6| Step: 7
Training loss: 2.142239809036255
Validation loss: 2.2814906976556264

Epoch: 6| Step: 8
Training loss: 2.2580106258392334
Validation loss: 2.2722207320633756

Epoch: 6| Step: 9
Training loss: 2.6448721885681152
Validation loss: 2.280016453035416

Epoch: 6| Step: 10
Training loss: 2.0422520637512207
Validation loss: 2.284533699353536

Epoch: 6| Step: 11
Training loss: 2.234362840652466
Validation loss: 2.2802191242094962

Epoch: 6| Step: 12
Training loss: 2.6981141567230225
Validation loss: 2.2869836848269225

Epoch: 6| Step: 13
Training loss: 1.847154140472412
Validation loss: 2.2858815782813617

Epoch: 265| Step: 0
Training loss: 2.7746896743774414
Validation loss: 2.2871744696811964

Epoch: 6| Step: 1
Training loss: 2.0407493114471436
Validation loss: 2.290457697324855

Epoch: 6| Step: 2
Training loss: 2.243227005004883
Validation loss: 2.2934943860577

Epoch: 6| Step: 3
Training loss: 3.176283121109009
Validation loss: 2.3087732407354538

Epoch: 6| Step: 4
Training loss: 2.4980010986328125
Validation loss: 2.304082411591725

Epoch: 6| Step: 5
Training loss: 2.1541080474853516
Validation loss: 2.3039257475124892

Epoch: 6| Step: 6
Training loss: 2.5036654472351074
Validation loss: 2.3069588433029833

Epoch: 6| Step: 7
Training loss: 2.742600440979004
Validation loss: 2.294558290512331

Epoch: 6| Step: 8
Training loss: 1.5619831085205078
Validation loss: 2.3002814118580153

Epoch: 6| Step: 9
Training loss: 2.0239243507385254
Validation loss: 2.2971181177323863

Epoch: 6| Step: 10
Training loss: 3.318666458129883
Validation loss: 2.2875031130288237

Epoch: 6| Step: 11
Training loss: 2.916576623916626
Validation loss: 2.2810228973306637

Epoch: 6| Step: 12
Training loss: 2.23604154586792
Validation loss: 2.2733470624493015

Epoch: 6| Step: 13
Training loss: 2.5479626655578613
Validation loss: 2.2716770223391953

Epoch: 266| Step: 0
Training loss: 3.0508389472961426
Validation loss: 2.2609010178555726

Epoch: 6| Step: 1
Training loss: 2.2433972358703613
Validation loss: 2.2625682507791827

Epoch: 6| Step: 2
Training loss: 2.6919498443603516
Validation loss: 2.2597098555616153

Epoch: 6| Step: 3
Training loss: 1.8932759761810303
Validation loss: 2.2608822276515346

Epoch: 6| Step: 4
Training loss: 2.6806845664978027
Validation loss: 2.266338268915812

Epoch: 6| Step: 5
Training loss: 2.334552526473999
Validation loss: 2.269027658688125

Epoch: 6| Step: 6
Training loss: 2.863236904144287
Validation loss: 2.2703064603190266

Epoch: 6| Step: 7
Training loss: 3.2425060272216797
Validation loss: 2.270430220070706

Epoch: 6| Step: 8
Training loss: 2.625112533569336
Validation loss: 2.268547893852316

Epoch: 6| Step: 9
Training loss: 1.3242393732070923
Validation loss: 2.2792182942872405

Epoch: 6| Step: 10
Training loss: 2.254173994064331
Validation loss: 2.291210420670048

Epoch: 6| Step: 11
Training loss: 1.9733483791351318
Validation loss: 2.2846996220209266

Epoch: 6| Step: 12
Training loss: 2.607404947280884
Validation loss: 2.2834734403958885

Epoch: 6| Step: 13
Training loss: 3.2117419242858887
Validation loss: 2.273340791784307

Epoch: 267| Step: 0
Training loss: 2.3940837383270264
Validation loss: 2.278894414183914

Epoch: 6| Step: 1
Training loss: 2.546104669570923
Validation loss: 2.2840961769062984

Epoch: 6| Step: 2
Training loss: 2.6735243797302246
Validation loss: 2.2776247916683072

Epoch: 6| Step: 3
Training loss: 2.329336404800415
Validation loss: 2.2785108961084837

Epoch: 6| Step: 4
Training loss: 3.247983455657959
Validation loss: 2.279931132511426

Epoch: 6| Step: 5
Training loss: 2.2898082733154297
Validation loss: 2.282383841852988

Epoch: 6| Step: 6
Training loss: 2.5481433868408203
Validation loss: 2.2736245150207193

Epoch: 6| Step: 7
Training loss: 1.5068784952163696
Validation loss: 2.2680755199924594

Epoch: 6| Step: 8
Training loss: 2.638357162475586
Validation loss: 2.274597152586906

Epoch: 6| Step: 9
Training loss: 2.47456431388855
Validation loss: 2.264887200888767

Epoch: 6| Step: 10
Training loss: 2.7631750106811523
Validation loss: 2.2691616166022515

Epoch: 6| Step: 11
Training loss: 2.2235803604125977
Validation loss: 2.2803844367304156

Epoch: 6| Step: 12
Training loss: 2.7128329277038574
Validation loss: 2.2846426963806152

Epoch: 6| Step: 13
Training loss: 2.2847061157226562
Validation loss: 2.2878938208344164

Epoch: 268| Step: 0
Training loss: 2.5392370223999023
Validation loss: 2.297602140775291

Epoch: 6| Step: 1
Training loss: 2.579585552215576
Validation loss: 2.2926524352001887

Epoch: 6| Step: 2
Training loss: 2.7763099670410156
Validation loss: 2.3014364575826995

Epoch: 6| Step: 3
Training loss: 2.4114623069763184
Validation loss: 2.301582546644313

Epoch: 6| Step: 4
Training loss: 2.7926383018493652
Validation loss: 2.309984596826697

Epoch: 6| Step: 5
Training loss: 2.363402843475342
Validation loss: 2.3129202986276276

Epoch: 6| Step: 6
Training loss: 2.1845316886901855
Validation loss: 2.314654952736311

Epoch: 6| Step: 7
Training loss: 1.8713794946670532
Validation loss: 2.316647793657036

Epoch: 6| Step: 8
Training loss: 1.9488835334777832
Validation loss: 2.3230608996524604

Epoch: 6| Step: 9
Training loss: 2.8921046257019043
Validation loss: 2.3087400544074272

Epoch: 6| Step: 10
Training loss: 2.4960246086120605
Validation loss: 2.295549031226866

Epoch: 6| Step: 11
Training loss: 2.0622568130493164
Validation loss: 2.297451961425043

Epoch: 6| Step: 12
Training loss: 3.3710618019104004
Validation loss: 2.28252326801259

Epoch: 6| Step: 13
Training loss: 2.288714647293091
Validation loss: 2.2834977539636756

Epoch: 269| Step: 0
Training loss: 2.429568290710449
Validation loss: 2.276319573002477

Epoch: 6| Step: 1
Training loss: 2.6787681579589844
Validation loss: 2.276570604693505

Epoch: 6| Step: 2
Training loss: 2.695568084716797
Validation loss: 2.2620402330993326

Epoch: 6| Step: 3
Training loss: 2.219907760620117
Validation loss: 2.2558256554347214

Epoch: 6| Step: 4
Training loss: 1.9766786098480225
Validation loss: 2.257614166505875

Epoch: 6| Step: 5
Training loss: 2.086373805999756
Validation loss: 2.2431000842843005

Epoch: 6| Step: 6
Training loss: 2.8238067626953125
Validation loss: 2.253539626316358

Epoch: 6| Step: 7
Training loss: 2.570262908935547
Validation loss: 2.2486782740521174

Epoch: 6| Step: 8
Training loss: 2.7442831993103027
Validation loss: 2.2552542737735215

Epoch: 6| Step: 9
Training loss: 2.3015127182006836
Validation loss: 2.253727653975128

Epoch: 6| Step: 10
Training loss: 2.3954885005950928
Validation loss: 2.2641154617391606

Epoch: 6| Step: 11
Training loss: 2.2288410663604736
Validation loss: 2.263096683768816

Epoch: 6| Step: 12
Training loss: 3.0093047618865967
Validation loss: 2.2531600101019746

Epoch: 6| Step: 13
Training loss: 2.773953437805176
Validation loss: 2.2632131294537614

Epoch: 270| Step: 0
Training loss: 3.2747390270233154
Validation loss: 2.265124192801855

Epoch: 6| Step: 1
Training loss: 2.9128971099853516
Validation loss: 2.262698383741481

Epoch: 6| Step: 2
Training loss: 2.3789734840393066
Validation loss: 2.2678715964799285

Epoch: 6| Step: 3
Training loss: 2.5599045753479004
Validation loss: 2.2667343539576374

Epoch: 6| Step: 4
Training loss: 2.511457920074463
Validation loss: 2.266187411482616

Epoch: 6| Step: 5
Training loss: 2.117828369140625
Validation loss: 2.283811274395194

Epoch: 6| Step: 6
Training loss: 1.6002541780471802
Validation loss: 2.2792019920964397

Epoch: 6| Step: 7
Training loss: 2.159311294555664
Validation loss: 2.2781155942588724

Epoch: 6| Step: 8
Training loss: 2.1917805671691895
Validation loss: 2.2860464178105837

Epoch: 6| Step: 9
Training loss: 2.3675312995910645
Validation loss: 2.2951318986954226

Epoch: 6| Step: 10
Training loss: 2.914668083190918
Validation loss: 2.2897753638605916

Epoch: 6| Step: 11
Training loss: 3.1455507278442383
Validation loss: 2.2910732569233065

Epoch: 6| Step: 12
Training loss: 2.3684298992156982
Validation loss: 2.2825533343899633

Epoch: 6| Step: 13
Training loss: 1.8937339782714844
Validation loss: 2.2762397822513374

Epoch: 271| Step: 0
Training loss: 1.9819505214691162
Validation loss: 2.284451210370628

Epoch: 6| Step: 1
Training loss: 2.1642088890075684
Validation loss: 2.272687271077146

Epoch: 6| Step: 2
Training loss: 2.3945112228393555
Validation loss: 2.260119625317153

Epoch: 6| Step: 3
Training loss: 2.240774631500244
Validation loss: 2.2651495215713338

Epoch: 6| Step: 4
Training loss: 2.5061895847320557
Validation loss: 2.2667581189063286

Epoch: 6| Step: 5
Training loss: 3.2937896251678467
Validation loss: 2.25254104727058

Epoch: 6| Step: 6
Training loss: 2.8479065895080566
Validation loss: 2.2611055169054257

Epoch: 6| Step: 7
Training loss: 1.4672781229019165
Validation loss: 2.2563737464207474

Epoch: 6| Step: 8
Training loss: 2.4551820755004883
Validation loss: 2.2630792997216664

Epoch: 6| Step: 9
Training loss: 2.2812728881835938
Validation loss: 2.2596763872331187

Epoch: 6| Step: 10
Training loss: 2.9588704109191895
Validation loss: 2.2692105206110145

Epoch: 6| Step: 11
Training loss: 2.4818601608276367
Validation loss: 2.274148505221131

Epoch: 6| Step: 12
Training loss: 2.7847113609313965
Validation loss: 2.2656105718305035

Epoch: 6| Step: 13
Training loss: 2.814607858657837
Validation loss: 2.269113638067758

Epoch: 272| Step: 0
Training loss: 2.1069517135620117
Validation loss: 2.2753879716319423

Epoch: 6| Step: 1
Training loss: 2.0042622089385986
Validation loss: 2.287118720751937

Epoch: 6| Step: 2
Training loss: 2.1497843265533447
Validation loss: 2.297009350151144

Epoch: 6| Step: 3
Training loss: 2.6012134552001953
Validation loss: 2.2934055610369612

Epoch: 6| Step: 4
Training loss: 2.0411696434020996
Validation loss: 2.294515440540929

Epoch: 6| Step: 5
Training loss: 2.8743176460266113
Validation loss: 2.2975343401714037

Epoch: 6| Step: 6
Training loss: 2.0187978744506836
Validation loss: 2.291462359889861

Epoch: 6| Step: 7
Training loss: 2.5768213272094727
Validation loss: 2.293264127546741

Epoch: 6| Step: 8
Training loss: 2.060215950012207
Validation loss: 2.2989211595186623

Epoch: 6| Step: 9
Training loss: 2.663361072540283
Validation loss: 2.310251174434539

Epoch: 6| Step: 10
Training loss: 2.8726587295532227
Validation loss: 2.294679190522881

Epoch: 6| Step: 11
Training loss: 3.3325562477111816
Validation loss: 2.2845300089928413

Epoch: 6| Step: 12
Training loss: 2.919515371322632
Validation loss: 2.2820807656934186

Epoch: 6| Step: 13
Training loss: 2.1039695739746094
Validation loss: 2.269998941370236

Epoch: 273| Step: 0
Training loss: 3.0273375511169434
Validation loss: 2.263835483981717

Epoch: 6| Step: 1
Training loss: 1.9030373096466064
Validation loss: 2.2692200163359284

Epoch: 6| Step: 2
Training loss: 2.625885486602783
Validation loss: 2.26832555442728

Epoch: 6| Step: 3
Training loss: 2.732189655303955
Validation loss: 2.276097341250348

Epoch: 6| Step: 4
Training loss: 2.2795867919921875
Validation loss: 2.2713433978378132

Epoch: 6| Step: 5
Training loss: 2.5433907508850098
Validation loss: 2.2718749738508657

Epoch: 6| Step: 6
Training loss: 2.3264970779418945
Validation loss: 2.268726764186736

Epoch: 6| Step: 7
Training loss: 2.2244551181793213
Validation loss: 2.2736310164133706

Epoch: 6| Step: 8
Training loss: 2.3802602291107178
Validation loss: 2.276765101699419

Epoch: 6| Step: 9
Training loss: 2.8286290168762207
Validation loss: 2.271775569967044

Epoch: 6| Step: 10
Training loss: 1.8623690605163574
Validation loss: 2.271588235773066

Epoch: 6| Step: 11
Training loss: 2.4666340351104736
Validation loss: 2.262628778334587

Epoch: 6| Step: 12
Training loss: 2.7412266731262207
Validation loss: 2.286364821977513

Epoch: 6| Step: 13
Training loss: 2.6695187091827393
Validation loss: 2.2656919187115085

Epoch: 274| Step: 0
Training loss: 2.8647525310516357
Validation loss: 2.2707034913442468

Epoch: 6| Step: 1
Training loss: 2.5770270824432373
Validation loss: 2.256829943708194

Epoch: 6| Step: 2
Training loss: 3.0094821453094482
Validation loss: 2.2726799852104596

Epoch: 6| Step: 3
Training loss: 2.897592067718506
Validation loss: 2.265991166073789

Epoch: 6| Step: 4
Training loss: 1.8929387331008911
Validation loss: 2.268917852832425

Epoch: 6| Step: 5
Training loss: 2.2349023818969727
Validation loss: 2.260676460881387

Epoch: 6| Step: 6
Training loss: 2.5337579250335693
Validation loss: 2.2742405963200394

Epoch: 6| Step: 7
Training loss: 2.4851179122924805
Validation loss: 2.2844721066054476

Epoch: 6| Step: 8
Training loss: 2.2525548934936523
Validation loss: 2.2909394489821566

Epoch: 6| Step: 9
Training loss: 1.733660340309143
Validation loss: 2.2987091823290755

Epoch: 6| Step: 10
Training loss: 1.6737842559814453
Validation loss: 2.2819783508136706

Epoch: 6| Step: 11
Training loss: 3.0192294120788574
Validation loss: 2.289560466684321

Epoch: 6| Step: 12
Training loss: 2.1501312255859375
Validation loss: 2.2925859215439006

Epoch: 6| Step: 13
Training loss: 3.441976547241211
Validation loss: 2.2932886897876696

Epoch: 275| Step: 0
Training loss: 2.7119245529174805
Validation loss: 2.315118876836633

Epoch: 6| Step: 1
Training loss: 2.4121627807617188
Validation loss: 2.308668064814742

Epoch: 6| Step: 2
Training loss: 2.265874147415161
Validation loss: 2.2936296104103007

Epoch: 6| Step: 3
Training loss: 2.9025001525878906
Validation loss: 2.2971653246110484

Epoch: 6| Step: 4
Training loss: 2.333333969116211
Validation loss: 2.296659791341392

Epoch: 6| Step: 5
Training loss: 2.889619827270508
Validation loss: 2.292690433481688

Epoch: 6| Step: 6
Training loss: 1.8824292421340942
Validation loss: 2.287634870057465

Epoch: 6| Step: 7
Training loss: 2.4367690086364746
Validation loss: 2.287959978144656

Epoch: 6| Step: 8
Training loss: 3.117910146713257
Validation loss: 2.2929942236151746

Epoch: 6| Step: 9
Training loss: 2.1157586574554443
Validation loss: 2.2898940450401715

Epoch: 6| Step: 10
Training loss: 2.1544971466064453
Validation loss: 2.296012836117898

Epoch: 6| Step: 11
Training loss: 2.470134735107422
Validation loss: 2.2827051967702885

Epoch: 6| Step: 12
Training loss: 2.1644818782806396
Validation loss: 2.279612669380762

Epoch: 6| Step: 13
Training loss: 2.8751795291900635
Validation loss: 2.2778267962958223

Epoch: 276| Step: 0
Training loss: 1.8068721294403076
Validation loss: 2.2786220324936735

Epoch: 6| Step: 1
Training loss: 2.0006937980651855
Validation loss: 2.262724999458559

Epoch: 6| Step: 2
Training loss: 2.477708339691162
Validation loss: 2.262657268072969

Epoch: 6| Step: 3
Training loss: 1.818037509918213
Validation loss: 2.257126364656674

Epoch: 6| Step: 4
Training loss: 3.1637063026428223
Validation loss: 2.2581008224077124

Epoch: 6| Step: 5
Training loss: 3.1437487602233887
Validation loss: 2.260530574347383

Epoch: 6| Step: 6
Training loss: 2.1567673683166504
Validation loss: 2.2536228113276984

Epoch: 6| Step: 7
Training loss: 2.0052523612976074
Validation loss: 2.256971736108103

Epoch: 6| Step: 8
Training loss: 2.4042911529541016
Validation loss: 2.25373742529141

Epoch: 6| Step: 9
Training loss: 2.6237540245056152
Validation loss: 2.2541292816080074

Epoch: 6| Step: 10
Training loss: 3.0940895080566406
Validation loss: 2.2599816501781507

Epoch: 6| Step: 11
Training loss: 3.0322577953338623
Validation loss: 2.2587070772724767

Epoch: 6| Step: 12
Training loss: 2.2651925086975098
Validation loss: 2.2601699559919295

Epoch: 6| Step: 13
Training loss: 2.2274675369262695
Validation loss: 2.2633872057801936

Epoch: 277| Step: 0
Training loss: 2.1219539642333984
Validation loss: 2.2672586902495353

Epoch: 6| Step: 1
Training loss: 2.655722141265869
Validation loss: 2.27108121174638

Epoch: 6| Step: 2
Training loss: 2.3231358528137207
Validation loss: 2.269254587029898

Epoch: 6| Step: 3
Training loss: 2.2804927825927734
Validation loss: 2.2751130416829097

Epoch: 6| Step: 4
Training loss: 3.1187758445739746
Validation loss: 2.2763802518126783

Epoch: 6| Step: 5
Training loss: 3.2011537551879883
Validation loss: 2.2712725285560853

Epoch: 6| Step: 6
Training loss: 2.866406202316284
Validation loss: 2.276805149611606

Epoch: 6| Step: 7
Training loss: 2.3125040531158447
Validation loss: 2.26741430323611

Epoch: 6| Step: 8
Training loss: 2.2936933040618896
Validation loss: 2.2651523313214703

Epoch: 6| Step: 9
Training loss: 1.4634549617767334
Validation loss: 2.2629834733983523

Epoch: 6| Step: 10
Training loss: 3.0785765647888184
Validation loss: 2.272358707202378

Epoch: 6| Step: 11
Training loss: 2.000868320465088
Validation loss: 2.2658730732497347

Epoch: 6| Step: 12
Training loss: 2.1084256172180176
Validation loss: 2.255114960414107

Epoch: 6| Step: 13
Training loss: 2.6055541038513184
Validation loss: 2.2538570050270326

Epoch: 278| Step: 0
Training loss: 2.905026912689209
Validation loss: 2.250356140957084

Epoch: 6| Step: 1
Training loss: 2.5246214866638184
Validation loss: 2.252080335411974

Epoch: 6| Step: 2
Training loss: 2.786783456802368
Validation loss: 2.2460420195774367

Epoch: 6| Step: 3
Training loss: 2.0996479988098145
Validation loss: 2.254551104319993

Epoch: 6| Step: 4
Training loss: 2.8236260414123535
Validation loss: 2.247535997821439

Epoch: 6| Step: 5
Training loss: 2.615748405456543
Validation loss: 2.234678171014273

Epoch: 6| Step: 6
Training loss: 2.439471483230591
Validation loss: 2.2462012229427213

Epoch: 6| Step: 7
Training loss: 2.215160369873047
Validation loss: 2.2509495212185766

Epoch: 6| Step: 8
Training loss: 2.941391944885254
Validation loss: 2.2566076094104397

Epoch: 6| Step: 9
Training loss: 2.8319225311279297
Validation loss: 2.255253953318442

Epoch: 6| Step: 10
Training loss: 2.4276771545410156
Validation loss: 2.2573475453161422

Epoch: 6| Step: 11
Training loss: 1.508777379989624
Validation loss: 2.263042064123256

Epoch: 6| Step: 12
Training loss: 1.669053316116333
Validation loss: 2.269057350773965

Epoch: 6| Step: 13
Training loss: 2.7123682498931885
Validation loss: 2.291102904145436

Epoch: 279| Step: 0
Training loss: 2.4057600498199463
Validation loss: 2.286959760932512

Epoch: 6| Step: 1
Training loss: 2.3239173889160156
Validation loss: 2.2991130428929485

Epoch: 6| Step: 2
Training loss: 2.104811191558838
Validation loss: 2.299704190223448

Epoch: 6| Step: 3
Training loss: 2.64351224899292
Validation loss: 2.303347323530464

Epoch: 6| Step: 4
Training loss: 3.5064377784729004
Validation loss: 2.308025539562266

Epoch: 6| Step: 5
Training loss: 2.4441399574279785
Validation loss: 2.290583106779283

Epoch: 6| Step: 6
Training loss: 2.746490001678467
Validation loss: 2.2769489031966015

Epoch: 6| Step: 7
Training loss: 3.0808606147766113
Validation loss: 2.2587973302410496

Epoch: 6| Step: 8
Training loss: 2.814030170440674
Validation loss: 2.2389888558336484

Epoch: 6| Step: 9
Training loss: 2.477949857711792
Validation loss: 2.246568118372271

Epoch: 6| Step: 10
Training loss: 2.1498143672943115
Validation loss: 2.2494136287320043

Epoch: 6| Step: 11
Training loss: 1.3645548820495605
Validation loss: 2.253681936571675

Epoch: 6| Step: 12
Training loss: 2.0839438438415527
Validation loss: 2.2518704706622708

Epoch: 6| Step: 13
Training loss: 1.9123183488845825
Validation loss: 2.2486758488480763

Epoch: 280| Step: 0
Training loss: 2.888320207595825
Validation loss: 2.2490452220363

Epoch: 6| Step: 1
Training loss: 2.8183112144470215
Validation loss: 2.2511882987073673

Epoch: 6| Step: 2
Training loss: 2.481832981109619
Validation loss: 2.264438752205141

Epoch: 6| Step: 3
Training loss: 2.1635406017303467
Validation loss: 2.2661036701612574

Epoch: 6| Step: 4
Training loss: 2.486164093017578
Validation loss: 2.2729741206733127

Epoch: 6| Step: 5
Training loss: 2.5550975799560547
Validation loss: 2.269618159981184

Epoch: 6| Step: 6
Training loss: 2.7260587215423584
Validation loss: 2.268604040145874

Epoch: 6| Step: 7
Training loss: 3.247490882873535
Validation loss: 2.2557416321128927

Epoch: 6| Step: 8
Training loss: 1.758684515953064
Validation loss: 2.2526936325975644

Epoch: 6| Step: 9
Training loss: 2.0744528770446777
Validation loss: 2.2440845838157077

Epoch: 6| Step: 10
Training loss: 2.0823192596435547
Validation loss: 2.2510085695533344

Epoch: 6| Step: 11
Training loss: 2.301614999771118
Validation loss: 2.2550571400632142

Epoch: 6| Step: 12
Training loss: 2.829324722290039
Validation loss: 2.259811701313142

Epoch: 6| Step: 13
Training loss: 1.7556222677230835
Validation loss: 2.2656991891963507

Epoch: 281| Step: 0
Training loss: 2.57500958442688
Validation loss: 2.2688045860618673

Epoch: 6| Step: 1
Training loss: 3.1313118934631348
Validation loss: 2.277459675265897

Epoch: 6| Step: 2
Training loss: 2.7315638065338135
Validation loss: 2.2869449610351236

Epoch: 6| Step: 3
Training loss: 1.9836280345916748
Validation loss: 2.3024557482811714

Epoch: 6| Step: 4
Training loss: 2.485279083251953
Validation loss: 2.308828807646228

Epoch: 6| Step: 5
Training loss: 2.5330312252044678
Validation loss: 2.3028811716264292

Epoch: 6| Step: 6
Training loss: 2.2266321182250977
Validation loss: 2.301695654469152

Epoch: 6| Step: 7
Training loss: 3.3270692825317383
Validation loss: 2.2981818670867593

Epoch: 6| Step: 8
Training loss: 1.7026679515838623
Validation loss: 2.2979091136686263

Epoch: 6| Step: 9
Training loss: 2.1372289657592773
Validation loss: 2.286089133190852

Epoch: 6| Step: 10
Training loss: 1.6910440921783447
Validation loss: 2.279359907232305

Epoch: 6| Step: 11
Training loss: 3.3649749755859375
Validation loss: 2.2758215627362652

Epoch: 6| Step: 12
Training loss: 2.3726038932800293
Validation loss: 2.278259059434296

Epoch: 6| Step: 13
Training loss: 1.9269133806228638
Validation loss: 2.270072075628465

Epoch: 282| Step: 0
Training loss: 2.070178985595703
Validation loss: 2.2606172100190194

Epoch: 6| Step: 1
Training loss: 2.123824119567871
Validation loss: 2.2534475967448246

Epoch: 6| Step: 2
Training loss: 2.546208143234253
Validation loss: 2.256464545444776

Epoch: 6| Step: 3
Training loss: 2.9048304557800293
Validation loss: 2.256687648834721

Epoch: 6| Step: 4
Training loss: 2.7268199920654297
Validation loss: 2.2546111204290904

Epoch: 6| Step: 5
Training loss: 2.707615852355957
Validation loss: 2.2479978479364866

Epoch: 6| Step: 6
Training loss: 1.7857365608215332
Validation loss: 2.248067504616194

Epoch: 6| Step: 7
Training loss: 2.5808348655700684
Validation loss: 2.24440404932986

Epoch: 6| Step: 8
Training loss: 2.588700771331787
Validation loss: 2.2538664802428214

Epoch: 6| Step: 9
Training loss: 3.1649789810180664
Validation loss: 2.2464090008889475

Epoch: 6| Step: 10
Training loss: 2.608365058898926
Validation loss: 2.246551203471358

Epoch: 6| Step: 11
Training loss: 2.176867961883545
Validation loss: 2.247591082767774

Epoch: 6| Step: 12
Training loss: 2.382932186126709
Validation loss: 2.239009811032203

Epoch: 6| Step: 13
Training loss: 1.5001044273376465
Validation loss: 2.248452603176076

Epoch: 283| Step: 0
Training loss: 2.9181413650512695
Validation loss: 2.2465935701965005

Epoch: 6| Step: 1
Training loss: 2.1436767578125
Validation loss: 2.2541810876579693

Epoch: 6| Step: 2
Training loss: 2.567572593688965
Validation loss: 2.2533488504348265

Epoch: 6| Step: 3
Training loss: 1.6133110523223877
Validation loss: 2.2649147882256457

Epoch: 6| Step: 4
Training loss: 2.707204818725586
Validation loss: 2.2775515946008826

Epoch: 6| Step: 5
Training loss: 1.7348517179489136
Validation loss: 2.280138743821011

Epoch: 6| Step: 6
Training loss: 2.945552349090576
Validation loss: 2.2986005378025833

Epoch: 6| Step: 7
Training loss: 2.412679672241211
Validation loss: 2.301805652597899

Epoch: 6| Step: 8
Training loss: 3.471348285675049
Validation loss: 2.2958089613145396

Epoch: 6| Step: 9
Training loss: 2.5017971992492676
Validation loss: 2.29990291595459

Epoch: 6| Step: 10
Training loss: 2.261319160461426
Validation loss: 2.292590469442388

Epoch: 6| Step: 11
Training loss: 1.8916890621185303
Validation loss: 2.2889370110727127

Epoch: 6| Step: 12
Training loss: 2.7744266986846924
Validation loss: 2.2707019980235765

Epoch: 6| Step: 13
Training loss: 2.579540729522705
Validation loss: 2.2849910489974485

Epoch: 284| Step: 0
Training loss: 2.888657569885254
Validation loss: 2.2934905764877156

Epoch: 6| Step: 1
Training loss: 2.4711356163024902
Validation loss: 2.305018565988028

Epoch: 6| Step: 2
Training loss: 3.470418930053711
Validation loss: 2.3160010794157624

Epoch: 6| Step: 3
Training loss: 2.431349039077759
Validation loss: 2.3171362543618805

Epoch: 6| Step: 4
Training loss: 2.2812464237213135
Validation loss: 2.311434389442526

Epoch: 6| Step: 5
Training loss: 1.7365124225616455
Validation loss: 2.305272335647255

Epoch: 6| Step: 6
Training loss: 2.754283905029297
Validation loss: 2.309461606446133

Epoch: 6| Step: 7
Training loss: 1.6984128952026367
Validation loss: 2.302031783647435

Epoch: 6| Step: 8
Training loss: 2.836470603942871
Validation loss: 2.2886381046746367

Epoch: 6| Step: 9
Training loss: 2.7057909965515137
Validation loss: 2.2947874812669653

Epoch: 6| Step: 10
Training loss: 2.4487087726593018
Validation loss: 2.2818442775357153

Epoch: 6| Step: 11
Training loss: 2.5244293212890625
Validation loss: 2.2643189942964943

Epoch: 6| Step: 12
Training loss: 1.9906225204467773
Validation loss: 2.2637933223478255

Epoch: 6| Step: 13
Training loss: 2.3204469680786133
Validation loss: 2.258606428741127

Epoch: 285| Step: 0
Training loss: 2.1043903827667236
Validation loss: 2.2556992243695

Epoch: 6| Step: 1
Training loss: 2.7858195304870605
Validation loss: 2.2451807991150887

Epoch: 6| Step: 2
Training loss: 2.800347089767456
Validation loss: 2.259221443565943

Epoch: 6| Step: 3
Training loss: 1.719010829925537
Validation loss: 2.252118659275834

Epoch: 6| Step: 4
Training loss: 2.179565906524658
Validation loss: 2.2656321307664276

Epoch: 6| Step: 5
Training loss: 2.2728965282440186
Validation loss: 2.248959555420824

Epoch: 6| Step: 6
Training loss: 2.5130820274353027
Validation loss: 2.2536194888494347

Epoch: 6| Step: 7
Training loss: 2.9990475177764893
Validation loss: 2.243936433587023

Epoch: 6| Step: 8
Training loss: 3.052424907684326
Validation loss: 2.2482114889288463

Epoch: 6| Step: 9
Training loss: 2.1290364265441895
Validation loss: 2.2395999457246516

Epoch: 6| Step: 10
Training loss: 2.3727121353149414
Validation loss: 2.253754977257021

Epoch: 6| Step: 11
Training loss: 2.6612815856933594
Validation loss: 2.2487938096446376

Epoch: 6| Step: 12
Training loss: 2.481555700302124
Validation loss: 2.2537723459223264

Epoch: 6| Step: 13
Training loss: 2.024348258972168
Validation loss: 2.26159191900684

Epoch: 286| Step: 0
Training loss: 2.059122085571289
Validation loss: 2.2637010966577837

Epoch: 6| Step: 1
Training loss: 1.7336972951889038
Validation loss: 2.257652840306682

Epoch: 6| Step: 2
Training loss: 2.00705885887146
Validation loss: 2.2532173266974826

Epoch: 6| Step: 3
Training loss: 3.433180570602417
Validation loss: 2.263829738863053

Epoch: 6| Step: 4
Training loss: 1.7070308923721313
Validation loss: 2.2724401591926493

Epoch: 6| Step: 5
Training loss: 2.4056613445281982
Validation loss: 2.267913523540702

Epoch: 6| Step: 6
Training loss: 2.5911664962768555
Validation loss: 2.2716720052944717

Epoch: 6| Step: 7
Training loss: 2.991793155670166
Validation loss: 2.2685865740622244

Epoch: 6| Step: 8
Training loss: 2.848653793334961
Validation loss: 2.259557834235571

Epoch: 6| Step: 9
Training loss: 2.5811243057250977
Validation loss: 2.263584344617782

Epoch: 6| Step: 10
Training loss: 3.3374695777893066
Validation loss: 2.2682728869940645

Epoch: 6| Step: 11
Training loss: 2.0378544330596924
Validation loss: 2.251617544440813

Epoch: 6| Step: 12
Training loss: 2.102479934692383
Validation loss: 2.268590416959537

Epoch: 6| Step: 13
Training loss: 2.2487146854400635
Validation loss: 2.265993454123056

Epoch: 287| Step: 0
Training loss: 2.3358888626098633
Validation loss: 2.2620749550481

Epoch: 6| Step: 1
Training loss: 2.0717668533325195
Validation loss: 2.264989511941069

Epoch: 6| Step: 2
Training loss: 2.5673775672912598
Validation loss: 2.2644110200225667

Epoch: 6| Step: 3
Training loss: 2.8488776683807373
Validation loss: 2.2717101830308155

Epoch: 6| Step: 4
Training loss: 2.5392956733703613
Validation loss: 2.255380766366118

Epoch: 6| Step: 5
Training loss: 1.9171998500823975
Validation loss: 2.2532989209698093

Epoch: 6| Step: 6
Training loss: 2.0641746520996094
Validation loss: 2.2590754301317277

Epoch: 6| Step: 7
Training loss: 3.0136091709136963
Validation loss: 2.248537325089978

Epoch: 6| Step: 8
Training loss: 2.7403926849365234
Validation loss: 2.243003711905531

Epoch: 6| Step: 9
Training loss: 2.5443034172058105
Validation loss: 2.241520740652597

Epoch: 6| Step: 10
Training loss: 2.3964180946350098
Validation loss: 2.2350414273559407

Epoch: 6| Step: 11
Training loss: 2.9038360118865967
Validation loss: 2.2432493932785524

Epoch: 6| Step: 12
Training loss: 2.0608723163604736
Validation loss: 2.2326921288685133

Epoch: 6| Step: 13
Training loss: 1.8289228677749634
Validation loss: 2.2366248561489965

Epoch: 288| Step: 0
Training loss: 2.0899877548217773
Validation loss: 2.242792198734899

Epoch: 6| Step: 1
Training loss: 2.487771987915039
Validation loss: 2.243956786330028

Epoch: 6| Step: 2
Training loss: 2.781039237976074
Validation loss: 2.2572718589536604

Epoch: 6| Step: 3
Training loss: 2.251995086669922
Validation loss: 2.2515084922954602

Epoch: 6| Step: 4
Training loss: 2.1838676929473877
Validation loss: 2.2633972706333285

Epoch: 6| Step: 5
Training loss: 2.427018642425537
Validation loss: 2.269246591034756

Epoch: 6| Step: 6
Training loss: 2.1349072456359863
Validation loss: 2.276028894609021

Epoch: 6| Step: 7
Training loss: 2.4330694675445557
Validation loss: 2.2787827343069096

Epoch: 6| Step: 8
Training loss: 2.475127696990967
Validation loss: 2.2845740536207795

Epoch: 6| Step: 9
Training loss: 2.5397424697875977
Validation loss: 2.2790866539042485

Epoch: 6| Step: 10
Training loss: 2.274667263031006
Validation loss: 2.283621841861356

Epoch: 6| Step: 11
Training loss: 2.9310765266418457
Validation loss: 2.272150989501707

Epoch: 6| Step: 12
Training loss: 2.5800085067749023
Validation loss: 2.2568395496696554

Epoch: 6| Step: 13
Training loss: 2.6067118644714355
Validation loss: 2.2680781989969234

Epoch: 289| Step: 0
Training loss: 2.22247314453125
Validation loss: 2.2494302641960884

Epoch: 6| Step: 1
Training loss: 2.4301037788391113
Validation loss: 2.2392383711312407

Epoch: 6| Step: 2
Training loss: 2.3981354236602783
Validation loss: 2.227818876184443

Epoch: 6| Step: 3
Training loss: 2.4743399620056152
Validation loss: 2.2432223161061606

Epoch: 6| Step: 4
Training loss: 1.2918808460235596
Validation loss: 2.234160410460605

Epoch: 6| Step: 5
Training loss: 2.576691150665283
Validation loss: 2.234914589953679

Epoch: 6| Step: 6
Training loss: 1.6599029302597046
Validation loss: 2.2233524604510237

Epoch: 6| Step: 7
Training loss: 2.853365898132324
Validation loss: 2.23291435292972

Epoch: 6| Step: 8
Training loss: 3.1297147274017334
Validation loss: 2.234225716642154

Epoch: 6| Step: 9
Training loss: 2.568723678588867
Validation loss: 2.232227404912313

Epoch: 6| Step: 10
Training loss: 2.5777440071105957
Validation loss: 2.235773468530306

Epoch: 6| Step: 11
Training loss: 2.6046807765960693
Validation loss: 2.2300798098246255

Epoch: 6| Step: 12
Training loss: 2.833212375640869
Validation loss: 2.2294701747996832

Epoch: 6| Step: 13
Training loss: 2.327960252761841
Validation loss: 2.2424558747199272

Epoch: 290| Step: 0
Training loss: 1.8733195066452026
Validation loss: 2.2436360364319174

Epoch: 6| Step: 1
Training loss: 2.610598564147949
Validation loss: 2.240922489473897

Epoch: 6| Step: 2
Training loss: 2.8315563201904297
Validation loss: 2.236831439438687

Epoch: 6| Step: 3
Training loss: 2.823087692260742
Validation loss: 2.2453122305613693

Epoch: 6| Step: 4
Training loss: 2.8180811405181885
Validation loss: 2.2511346981089604

Epoch: 6| Step: 5
Training loss: 2.0195915699005127
Validation loss: 2.2511780364539034

Epoch: 6| Step: 6
Training loss: 2.181582450866699
Validation loss: 2.245770626170661

Epoch: 6| Step: 7
Training loss: 2.222022294998169
Validation loss: 2.2671029042172175

Epoch: 6| Step: 8
Training loss: 2.756103515625
Validation loss: 2.2545376746885237

Epoch: 6| Step: 9
Training loss: 2.4598965644836426
Validation loss: 2.2620215749227874

Epoch: 6| Step: 10
Training loss: 2.4086318016052246
Validation loss: 2.25053890802527

Epoch: 6| Step: 11
Training loss: 2.783353805541992
Validation loss: 2.2685357627048286

Epoch: 6| Step: 12
Training loss: 1.776881217956543
Validation loss: 2.2669551731437765

Epoch: 6| Step: 13
Training loss: 2.698996067047119
Validation loss: 2.259347143993583

Epoch: 291| Step: 0
Training loss: 2.990882158279419
Validation loss: 2.258392236566031

Epoch: 6| Step: 1
Training loss: 2.2594473361968994
Validation loss: 2.2546115613752797

Epoch: 6| Step: 2
Training loss: 2.617201328277588
Validation loss: 2.253780044535155

Epoch: 6| Step: 3
Training loss: 2.7709150314331055
Validation loss: 2.251037874529439

Epoch: 6| Step: 4
Training loss: 2.615554094314575
Validation loss: 2.2644004309049217

Epoch: 6| Step: 5
Training loss: 1.8600016832351685
Validation loss: 2.2469548358712146

Epoch: 6| Step: 6
Training loss: 2.47275972366333
Validation loss: 2.2508056291969876

Epoch: 6| Step: 7
Training loss: 2.733213186264038
Validation loss: 2.2578310594763806

Epoch: 6| Step: 8
Training loss: 2.1815946102142334
Validation loss: 2.2469620807196504

Epoch: 6| Step: 9
Training loss: 2.2786011695861816
Validation loss: 2.2394694500072028

Epoch: 6| Step: 10
Training loss: 1.8581641912460327
Validation loss: 2.2480324788760115

Epoch: 6| Step: 11
Training loss: 2.1927928924560547
Validation loss: 2.229905523279662

Epoch: 6| Step: 12
Training loss: 2.6697280406951904
Validation loss: 2.230470416366413

Epoch: 6| Step: 13
Training loss: 2.6661183834075928
Validation loss: 2.22364576144885

Epoch: 292| Step: 0
Training loss: 2.381028890609741
Validation loss: 2.2136397413028184

Epoch: 6| Step: 1
Training loss: 2.3371193408966064
Validation loss: 2.2205737713844544

Epoch: 6| Step: 2
Training loss: 2.311063289642334
Validation loss: 2.2304854726278656

Epoch: 6| Step: 3
Training loss: 2.422407865524292
Validation loss: 2.2227662776106145

Epoch: 6| Step: 4
Training loss: 2.4510111808776855
Validation loss: 2.2318601557003555

Epoch: 6| Step: 5
Training loss: 2.5103983879089355
Validation loss: 2.2413730082973355

Epoch: 6| Step: 6
Training loss: 2.608628988265991
Validation loss: 2.250455148758427

Epoch: 6| Step: 7
Training loss: 1.8523449897766113
Validation loss: 2.249354661151927

Epoch: 6| Step: 8
Training loss: 2.3035764694213867
Validation loss: 2.2519101814557145

Epoch: 6| Step: 9
Training loss: 2.1737852096557617
Validation loss: 2.2523169466244277

Epoch: 6| Step: 10
Training loss: 2.3688275814056396
Validation loss: 2.2569238652465162

Epoch: 6| Step: 11
Training loss: 2.853649616241455
Validation loss: 2.247249400743874

Epoch: 6| Step: 12
Training loss: 2.4614267349243164
Validation loss: 2.2481694272769395

Epoch: 6| Step: 13
Training loss: 3.4346702098846436
Validation loss: 2.2405555273896907

Epoch: 293| Step: 0
Training loss: 2.9673407077789307
Validation loss: 2.240782178858275

Epoch: 6| Step: 1
Training loss: 2.130859851837158
Validation loss: 2.238687825459306

Epoch: 6| Step: 2
Training loss: 2.5764455795288086
Validation loss: 2.2479074411494757

Epoch: 6| Step: 3
Training loss: 2.280487060546875
Validation loss: 2.245966011478055

Epoch: 6| Step: 4
Training loss: 2.99739933013916
Validation loss: 2.23608555845035

Epoch: 6| Step: 5
Training loss: 1.8418958187103271
Validation loss: 2.231628164168327

Epoch: 6| Step: 6
Training loss: 2.1738386154174805
Validation loss: 2.232360537334155

Epoch: 6| Step: 7
Training loss: 3.075263023376465
Validation loss: 2.243481118191955

Epoch: 6| Step: 8
Training loss: 2.3309106826782227
Validation loss: 2.2307028424355293

Epoch: 6| Step: 9
Training loss: 3.0737056732177734
Validation loss: 2.2382435132098455

Epoch: 6| Step: 10
Training loss: 2.3590991497039795
Validation loss: 2.2348171344367405

Epoch: 6| Step: 11
Training loss: 2.2692151069641113
Validation loss: 2.242734024601598

Epoch: 6| Step: 12
Training loss: 1.5967897176742554
Validation loss: 2.239876034439251

Epoch: 6| Step: 13
Training loss: 2.3534669876098633
Validation loss: 2.25057384275621

Epoch: 294| Step: 0
Training loss: 2.737456798553467
Validation loss: 2.260547617430328

Epoch: 6| Step: 1
Training loss: 2.014896869659424
Validation loss: 2.269180941325362

Epoch: 6| Step: 2
Training loss: 2.1665027141571045
Validation loss: 2.2815937431909705

Epoch: 6| Step: 3
Training loss: 2.4514148235321045
Validation loss: 2.2856252783088276

Epoch: 6| Step: 4
Training loss: 2.4835257530212402
Validation loss: 2.2884654870597263

Epoch: 6| Step: 5
Training loss: 2.1392130851745605
Validation loss: 2.2855872313181558

Epoch: 6| Step: 6
Training loss: 2.563237190246582
Validation loss: 2.286460066354403

Epoch: 6| Step: 7
Training loss: 1.977067232131958
Validation loss: 2.2715743331499

Epoch: 6| Step: 8
Training loss: 1.8948465585708618
Validation loss: 2.268269576052184

Epoch: 6| Step: 9
Training loss: 2.960587978363037
Validation loss: 2.2734909826709377

Epoch: 6| Step: 10
Training loss: 2.3390936851501465
Validation loss: 2.2639986879082135

Epoch: 6| Step: 11
Training loss: 3.2107510566711426
Validation loss: 2.264909498153194

Epoch: 6| Step: 12
Training loss: 2.788884162902832
Validation loss: 2.2439919812704927

Epoch: 6| Step: 13
Training loss: 2.241997241973877
Validation loss: 2.23980543433979

Epoch: 295| Step: 0
Training loss: 3.4274415969848633
Validation loss: 2.2365467445824736

Epoch: 6| Step: 1
Training loss: 1.7959222793579102
Validation loss: 2.2273419928807083

Epoch: 6| Step: 2
Training loss: 2.2678141593933105
Validation loss: 2.2435829562525593

Epoch: 6| Step: 3
Training loss: 1.8576421737670898
Validation loss: 2.2355000972747803

Epoch: 6| Step: 4
Training loss: 3.089020013809204
Validation loss: 2.2295224589686238

Epoch: 6| Step: 5
Training loss: 3.4176292419433594
Validation loss: 2.2340089377536567

Epoch: 6| Step: 6
Training loss: 2.580049991607666
Validation loss: 2.2244443919069026

Epoch: 6| Step: 7
Training loss: 2.573324680328369
Validation loss: 2.2387330826892646

Epoch: 6| Step: 8
Training loss: 2.5818676948547363
Validation loss: 2.23690152424638

Epoch: 6| Step: 9
Training loss: 2.4985389709472656
Validation loss: 2.228733447290236

Epoch: 6| Step: 10
Training loss: 1.3298786878585815
Validation loss: 2.2329593332864905

Epoch: 6| Step: 11
Training loss: 2.5905206203460693
Validation loss: 2.243381514344164

Epoch: 6| Step: 12
Training loss: 1.4089124202728271
Validation loss: 2.2440137555522304

Epoch: 6| Step: 13
Training loss: 2.593045234680176
Validation loss: 2.2471186050804715

Epoch: 296| Step: 0
Training loss: 1.7299137115478516
Validation loss: 2.256156152294528

Epoch: 6| Step: 1
Training loss: 2.1062936782836914
Validation loss: 2.2593034185389036

Epoch: 6| Step: 2
Training loss: 2.4625861644744873
Validation loss: 2.260024878286546

Epoch: 6| Step: 3
Training loss: 3.244202136993408
Validation loss: 2.258443461951389

Epoch: 6| Step: 4
Training loss: 2.5498151779174805
Validation loss: 2.27271520450551

Epoch: 6| Step: 5
Training loss: 2.333993673324585
Validation loss: 2.254715536230354

Epoch: 6| Step: 6
Training loss: 2.3768961429595947
Validation loss: 2.2550827623695455

Epoch: 6| Step: 7
Training loss: 2.97806978225708
Validation loss: 2.248645010814872

Epoch: 6| Step: 8
Training loss: 2.6768457889556885
Validation loss: 2.243435130324415

Epoch: 6| Step: 9
Training loss: 2.7226860523223877
Validation loss: 2.2523811363404795

Epoch: 6| Step: 10
Training loss: 1.9876078367233276
Validation loss: 2.2361969883723924

Epoch: 6| Step: 11
Training loss: 3.0214877128601074
Validation loss: 2.229603951977145

Epoch: 6| Step: 12
Training loss: 1.8752655982971191
Validation loss: 2.2234777096779115

Epoch: 6| Step: 13
Training loss: 1.4212265014648438
Validation loss: 2.2193828064908265

Epoch: 297| Step: 0
Training loss: 2.1365036964416504
Validation loss: 2.2216540126390356

Epoch: 6| Step: 1
Training loss: 2.37020206451416
Validation loss: 2.2311582514034805

Epoch: 6| Step: 2
Training loss: 2.7223856449127197
Validation loss: 2.2308299131290887

Epoch: 6| Step: 3
Training loss: 2.6198372840881348
Validation loss: 2.236198727802564

Epoch: 6| Step: 4
Training loss: 3.015272617340088
Validation loss: 2.2362837329987557

Epoch: 6| Step: 5
Training loss: 2.5360662937164307
Validation loss: 2.2372269553522908

Epoch: 6| Step: 6
Training loss: 2.8640451431274414
Validation loss: 2.237937122262934

Epoch: 6| Step: 7
Training loss: 1.9583039283752441
Validation loss: 2.2329222412519556

Epoch: 6| Step: 8
Training loss: 2.690673828125
Validation loss: 2.232143373899562

Epoch: 6| Step: 9
Training loss: 2.495483875274658
Validation loss: 2.2285934955843034

Epoch: 6| Step: 10
Training loss: 2.4741697311401367
Validation loss: 2.2393199577126452

Epoch: 6| Step: 11
Training loss: 2.0683515071868896
Validation loss: 2.233170719556911

Epoch: 6| Step: 12
Training loss: 1.5196022987365723
Validation loss: 2.2501039787005355

Epoch: 6| Step: 13
Training loss: 2.47177791595459
Validation loss: 2.260355576392143

Epoch: 298| Step: 0
Training loss: 3.299266815185547
Validation loss: 2.260115300455401

Epoch: 6| Step: 1
Training loss: 1.8636066913604736
Validation loss: 2.260500810479605

Epoch: 6| Step: 2
Training loss: 2.2210755348205566
Validation loss: 2.25616055662914

Epoch: 6| Step: 3
Training loss: 2.011385679244995
Validation loss: 2.2513325188749578

Epoch: 6| Step: 4
Training loss: 1.9973496198654175
Validation loss: 2.257708667426981

Epoch: 6| Step: 5
Training loss: 2.4300620555877686
Validation loss: 2.231683285005631

Epoch: 6| Step: 6
Training loss: 3.1428780555725098
Validation loss: 2.240513996411395

Epoch: 6| Step: 7
Training loss: 2.2738170623779297
Validation loss: 2.2316191683533373

Epoch: 6| Step: 8
Training loss: 2.446428060531616
Validation loss: 2.231283403212024

Epoch: 6| Step: 9
Training loss: 2.8631811141967773
Validation loss: 2.215538755539925

Epoch: 6| Step: 10
Training loss: 2.3649237155914307
Validation loss: 2.227371745212104

Epoch: 6| Step: 11
Training loss: 2.2766222953796387
Validation loss: 2.2277129850079938

Epoch: 6| Step: 12
Training loss: 1.9939544200897217
Validation loss: 2.2260173725825485

Epoch: 6| Step: 13
Training loss: 2.851877212524414
Validation loss: 2.2236191893136628

Epoch: 299| Step: 0
Training loss: 2.7048351764678955
Validation loss: 2.214719900520899

Epoch: 6| Step: 1
Training loss: 2.2816109657287598
Validation loss: 2.20791684427569

Epoch: 6| Step: 2
Training loss: 2.5684127807617188
Validation loss: 2.2318700846805366

Epoch: 6| Step: 3
Training loss: 2.8903441429138184
Validation loss: 2.2336149830971994

Epoch: 6| Step: 4
Training loss: 1.5685478448867798
Validation loss: 2.2507969000006236

Epoch: 6| Step: 5
Training loss: 1.870128870010376
Validation loss: 2.2533471456138034

Epoch: 6| Step: 6
Training loss: 3.101370334625244
Validation loss: 2.270654111780146

Epoch: 6| Step: 7
Training loss: 1.9722455739974976
Validation loss: 2.2746093888436594

Epoch: 6| Step: 8
Training loss: 2.410521984100342
Validation loss: 2.2676295695766324

Epoch: 6| Step: 9
Training loss: 2.3349668979644775
Validation loss: 2.257413423189553

Epoch: 6| Step: 10
Training loss: 2.1262099742889404
Validation loss: 2.255828163957083

Epoch: 6| Step: 11
Training loss: 3.0478172302246094
Validation loss: 2.247420626301919

Epoch: 6| Step: 12
Training loss: 2.680586338043213
Validation loss: 2.2558339488121772

Epoch: 6| Step: 13
Training loss: 2.320187568664551
Validation loss: 2.267597806069159

Epoch: 300| Step: 0
Training loss: 2.9403915405273438
Validation loss: 2.256310870570521

Epoch: 6| Step: 1
Training loss: 2.462437152862549
Validation loss: 2.245543154337073

Epoch: 6| Step: 2
Training loss: 2.573788642883301
Validation loss: 2.2304406678804787

Epoch: 6| Step: 3
Training loss: 2.9632911682128906
Validation loss: 2.2381261625597553

Epoch: 6| Step: 4
Training loss: 3.103224277496338
Validation loss: 2.22279832952766

Epoch: 6| Step: 5
Training loss: 1.9971082210540771
Validation loss: 2.21219055626982

Epoch: 6| Step: 6
Training loss: 2.556394577026367
Validation loss: 2.2074287988806285

Epoch: 6| Step: 7
Training loss: 1.957349419593811
Validation loss: 2.198110931663103

Epoch: 6| Step: 8
Training loss: 1.3243560791015625
Validation loss: 2.205596170117778

Epoch: 6| Step: 9
Training loss: 2.9352259635925293
Validation loss: 2.1995338111795406

Epoch: 6| Step: 10
Training loss: 1.957837462425232
Validation loss: 2.2021018920406217

Epoch: 6| Step: 11
Training loss: 2.1309425830841064
Validation loss: 2.208155629455402

Epoch: 6| Step: 12
Training loss: 3.1395344734191895
Validation loss: 2.2079170570578626

Epoch: 6| Step: 13
Training loss: 1.8910346031188965
Validation loss: 2.204945461724394

Testing loss: 2.42515057987637

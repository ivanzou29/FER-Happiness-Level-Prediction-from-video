Epoch: 1| Step: 0
Training loss: 5.642053604125977
Validation loss: 5.341073354085286

Epoch: 6| Step: 1
Training loss: 4.690206527709961
Validation loss: 5.339016040166219

Epoch: 6| Step: 2
Training loss: 6.419107913970947
Validation loss: 5.337094863255818

Epoch: 6| Step: 3
Training loss: 5.805056095123291
Validation loss: 5.335306008656819

Epoch: 6| Step: 4
Training loss: 5.003644943237305
Validation loss: 5.333455483118693

Epoch: 6| Step: 5
Training loss: 4.570428848266602
Validation loss: 5.331753492355347

Epoch: 6| Step: 6
Training loss: 4.975191116333008
Validation loss: 5.330066363016765

Epoch: 6| Step: 7
Training loss: 5.810514450073242
Validation loss: 5.328437328338623

Epoch: 6| Step: 8
Training loss: 5.252326965332031
Validation loss: 5.326666911443074

Epoch: 6| Step: 9
Training loss: 4.832995414733887
Validation loss: 5.324857711791992

Epoch: 6| Step: 10
Training loss: 6.247102737426758
Validation loss: 5.323038617769877

Epoch: 6| Step: 11
Training loss: 4.811408519744873
Validation loss: 5.3211502234141035

Epoch: 6| Step: 12
Training loss: 5.739227294921875
Validation loss: 5.3191258907318115

Epoch: 6| Step: 13
Training loss: 5.774785041809082
Validation loss: 5.317158222198486

Epoch: 2| Step: 0
Training loss: 5.985057830810547
Validation loss: 5.315000454584758

Epoch: 6| Step: 1
Training loss: 6.3433685302734375
Validation loss: 5.312728643417358

Epoch: 6| Step: 2
Training loss: 5.04679012298584
Validation loss: 5.310428222020467

Epoch: 6| Step: 3
Training loss: 6.276246070861816
Validation loss: 5.308063109715779

Epoch: 6| Step: 4
Training loss: 5.246881484985352
Validation loss: 5.305538813273112

Epoch: 6| Step: 5
Training loss: 5.6698455810546875
Validation loss: 5.302906354268392

Epoch: 6| Step: 6
Training loss: 4.714717388153076
Validation loss: 5.300310134887695

Epoch: 6| Step: 7
Training loss: 5.203799247741699
Validation loss: 5.297314365704854

Epoch: 6| Step: 8
Training loss: 5.038948059082031
Validation loss: 5.294427871704102

Epoch: 6| Step: 9
Training loss: 6.015810012817383
Validation loss: 5.291228612263997

Epoch: 6| Step: 10
Training loss: 4.654805660247803
Validation loss: 5.288024346033732

Epoch: 6| Step: 11
Training loss: 4.812189102172852
Validation loss: 5.284524758656819

Epoch: 6| Step: 12
Training loss: 4.934563636779785
Validation loss: 5.280956586201985

Epoch: 6| Step: 13
Training loss: 5.194170951843262
Validation loss: 5.276933828989665

Epoch: 3| Step: 0
Training loss: 5.6623454093933105
Validation loss: 5.273072878519694

Epoch: 6| Step: 1
Training loss: 5.354193687438965
Validation loss: 5.269023259480794

Epoch: 6| Step: 2
Training loss: 5.001336097717285
Validation loss: 5.2645933628082275

Epoch: 6| Step: 3
Training loss: 5.767030715942383
Validation loss: 5.259929736455281

Epoch: 6| Step: 4
Training loss: 5.540750026702881
Validation loss: 5.255220015843709

Epoch: 6| Step: 5
Training loss: 4.701011657714844
Validation loss: 5.250054041544597

Epoch: 6| Step: 6
Training loss: 5.090569972991943
Validation loss: 5.2448194821675616

Epoch: 6| Step: 7
Training loss: 5.047151565551758
Validation loss: 5.239153782526652

Epoch: 6| Step: 8
Training loss: 5.213448524475098
Validation loss: 5.233143965403239

Epoch: 6| Step: 9
Training loss: 5.7846574783325195
Validation loss: 5.2270980676015215

Epoch: 6| Step: 10
Training loss: 5.742556571960449
Validation loss: 5.220547278722127

Epoch: 6| Step: 11
Training loss: 4.529482841491699
Validation loss: 5.213993946711223

Epoch: 6| Step: 12
Training loss: 5.170605182647705
Validation loss: 5.206753810246785

Epoch: 6| Step: 13
Training loss: 5.72382926940918
Validation loss: 5.199407974878947

Epoch: 4| Step: 0
Training loss: 5.143061637878418
Validation loss: 5.19140362739563

Epoch: 6| Step: 1
Training loss: 5.025105953216553
Validation loss: 5.183736324310303

Epoch: 6| Step: 2
Training loss: 5.074843883514404
Validation loss: 5.175127983093262

Epoch: 6| Step: 3
Training loss: 6.324580192565918
Validation loss: 5.166331052780151

Epoch: 6| Step: 4
Training loss: 5.16572380065918
Validation loss: 5.157424767812093

Epoch: 6| Step: 5
Training loss: 4.973301410675049
Validation loss: 5.147813638051351

Epoch: 6| Step: 6
Training loss: 5.269774913787842
Validation loss: 5.137747844060262

Epoch: 6| Step: 7
Training loss: 4.584062576293945
Validation loss: 5.127982298533122

Epoch: 6| Step: 8
Training loss: 5.3614888191223145
Validation loss: 5.11751914024353

Epoch: 6| Step: 9
Training loss: 5.193512916564941
Validation loss: 5.106476068496704

Epoch: 6| Step: 10
Training loss: 5.204216480255127
Validation loss: 5.095323006312053

Epoch: 6| Step: 11
Training loss: 5.1018147468566895
Validation loss: 5.084154367446899

Epoch: 6| Step: 12
Training loss: 5.183719635009766
Validation loss: 5.071826299031575

Epoch: 6| Step: 13
Training loss: 5.249781608581543
Validation loss: 5.059991280237834

Epoch: 5| Step: 0
Training loss: 5.442731857299805
Validation loss: 5.047757466634114

Epoch: 6| Step: 1
Training loss: 5.037939071655273
Validation loss: 5.035750230153401

Epoch: 6| Step: 2
Training loss: 3.8605918884277344
Validation loss: 5.023106654485066

Epoch: 6| Step: 3
Training loss: 5.901247978210449
Validation loss: 5.010052124659221

Epoch: 6| Step: 4
Training loss: 4.49127197265625
Validation loss: 4.997961759567261

Epoch: 6| Step: 5
Training loss: 5.113434791564941
Validation loss: 4.984825054804484

Epoch: 6| Step: 6
Training loss: 4.46224308013916
Validation loss: 4.972183704376221

Epoch: 6| Step: 7
Training loss: 5.249074935913086
Validation loss: 4.959715525309245

Epoch: 6| Step: 8
Training loss: 6.0695905685424805
Validation loss: 4.947276910146077

Epoch: 6| Step: 9
Training loss: 5.36012077331543
Validation loss: 4.934750477472941

Epoch: 6| Step: 10
Training loss: 4.949062347412109
Validation loss: 4.922335068384807

Epoch: 6| Step: 11
Training loss: 4.7171783447265625
Validation loss: 4.910097757975261

Epoch: 6| Step: 12
Training loss: 4.586100101470947
Validation loss: 4.897887786229451

Epoch: 6| Step: 13
Training loss: 5.447170257568359
Validation loss: 4.8857167561848955

Epoch: 6| Step: 0
Training loss: 5.300784111022949
Validation loss: 4.873323281606038

Epoch: 6| Step: 1
Training loss: 4.709421634674072
Validation loss: 4.861557563145955

Epoch: 6| Step: 2
Training loss: 5.67464542388916
Validation loss: 4.849701960881551

Epoch: 6| Step: 3
Training loss: 4.257312774658203
Validation loss: 4.8379136721293134

Epoch: 6| Step: 4
Training loss: 5.477390289306641
Validation loss: 4.826896588007609

Epoch: 6| Step: 5
Training loss: 5.217887878417969
Validation loss: 4.8161516189575195

Epoch: 6| Step: 6
Training loss: 4.728496551513672
Validation loss: 4.805344621340434

Epoch: 6| Step: 7
Training loss: 4.932234287261963
Validation loss: 4.7946271896362305

Epoch: 6| Step: 8
Training loss: 4.209700584411621
Validation loss: 4.784204721450806

Epoch: 6| Step: 9
Training loss: 4.352591037750244
Validation loss: 4.7739174365997314

Epoch: 6| Step: 10
Training loss: 5.433980941772461
Validation loss: 4.763421376546224

Epoch: 6| Step: 11
Training loss: 4.846673965454102
Validation loss: 4.75365948677063

Epoch: 6| Step: 12
Training loss: 4.443287372589111
Validation loss: 4.74342946211497

Epoch: 6| Step: 13
Training loss: 4.9052276611328125
Validation loss: 4.733885049819946

Epoch: 7| Step: 0
Training loss: 4.469921112060547
Validation loss: 4.724346995353699

Epoch: 6| Step: 1
Training loss: 4.005720615386963
Validation loss: 4.714841524759929

Epoch: 6| Step: 2
Training loss: 4.773271560668945
Validation loss: 4.706171989440918

Epoch: 6| Step: 3
Training loss: 5.377203941345215
Validation loss: 4.697328090667725

Epoch: 6| Step: 4
Training loss: 4.315834999084473
Validation loss: 4.688498059908549

Epoch: 6| Step: 5
Training loss: 4.089673042297363
Validation loss: 4.680309534072876

Epoch: 6| Step: 6
Training loss: 3.762869358062744
Validation loss: 4.671698490778605

Epoch: 6| Step: 7
Training loss: 5.76928186416626
Validation loss: 4.663057724634807

Epoch: 6| Step: 8
Training loss: 5.010652542114258
Validation loss: 4.654726028442383

Epoch: 6| Step: 9
Training loss: 4.693497657775879
Validation loss: 4.646308660507202

Epoch: 6| Step: 10
Training loss: 4.957531929016113
Validation loss: 4.637598991394043

Epoch: 6| Step: 11
Training loss: 5.722447872161865
Validation loss: 4.628920793533325

Epoch: 6| Step: 12
Training loss: 5.281158924102783
Validation loss: 4.620254278182983

Epoch: 6| Step: 13
Training loss: 4.429831027984619
Validation loss: 4.612080415089925

Epoch: 8| Step: 0
Training loss: 5.110036849975586
Validation loss: 4.602572004000346

Epoch: 6| Step: 1
Training loss: 5.400354385375977
Validation loss: 4.592851082483928

Epoch: 6| Step: 2
Training loss: 4.350605010986328
Validation loss: 4.58299978574117

Epoch: 6| Step: 3
Training loss: 4.938200950622559
Validation loss: 4.572921474774678

Epoch: 6| Step: 4
Training loss: 4.431196212768555
Validation loss: 4.5646369854609175

Epoch: 6| Step: 5
Training loss: 5.228991508483887
Validation loss: 4.556591113408406

Epoch: 6| Step: 6
Training loss: 4.853512763977051
Validation loss: 4.549531618754069

Epoch: 6| Step: 7
Training loss: 3.7427139282226562
Validation loss: 4.5419935782750445

Epoch: 6| Step: 8
Training loss: 4.064146995544434
Validation loss: 4.533389091491699

Epoch: 6| Step: 9
Training loss: 4.824535846710205
Validation loss: 4.525868892669678

Epoch: 6| Step: 10
Training loss: 4.762626647949219
Validation loss: 4.517876386642456

Epoch: 6| Step: 11
Training loss: 4.226921081542969
Validation loss: 4.51029904683431

Epoch: 6| Step: 12
Training loss: 4.275304794311523
Validation loss: 4.502779006958008

Epoch: 6| Step: 13
Training loss: 4.854389190673828
Validation loss: 4.494861125946045

Epoch: 9| Step: 0
Training loss: 5.107349395751953
Validation loss: 4.4872918128967285

Epoch: 6| Step: 1
Training loss: 4.0790228843688965
Validation loss: 4.480401595433553

Epoch: 6| Step: 2
Training loss: 4.281593322753906
Validation loss: 4.472655375798543

Epoch: 6| Step: 3
Training loss: 5.284962177276611
Validation loss: 4.465266704559326

Epoch: 6| Step: 4
Training loss: 3.522768974304199
Validation loss: 4.457969148953755

Epoch: 6| Step: 5
Training loss: 5.603815078735352
Validation loss: 4.4513205687205

Epoch: 6| Step: 6
Training loss: 4.6531782150268555
Validation loss: 4.4438637892405195

Epoch: 6| Step: 7
Training loss: 4.249876976013184
Validation loss: 4.436494032541911

Epoch: 6| Step: 8
Training loss: 3.903046131134033
Validation loss: 4.4287006457646685

Epoch: 6| Step: 9
Training loss: 5.0099382400512695
Validation loss: 4.421059608459473

Epoch: 6| Step: 10
Training loss: 4.049403667449951
Validation loss: 4.413253744443257

Epoch: 6| Step: 11
Training loss: 3.6337857246398926
Validation loss: 4.406237363815308

Epoch: 6| Step: 12
Training loss: 4.560945510864258
Validation loss: 4.39862338701884

Epoch: 6| Step: 13
Training loss: 5.776431083679199
Validation loss: 4.390608509381612

Epoch: 10| Step: 0
Training loss: 4.248546600341797
Validation loss: 4.383286555608113

Epoch: 6| Step: 1
Training loss: 3.600188970565796
Validation loss: 4.37604558467865

Epoch: 6| Step: 2
Training loss: 5.339003086090088
Validation loss: 4.368376572926839

Epoch: 6| Step: 3
Training loss: 4.153939723968506
Validation loss: 4.360987583796184

Epoch: 6| Step: 4
Training loss: 4.767961502075195
Validation loss: 4.352477471033732

Epoch: 6| Step: 5
Training loss: 4.795841217041016
Validation loss: 4.3444929122924805

Epoch: 6| Step: 6
Training loss: 3.782606840133667
Validation loss: 4.335348804791768

Epoch: 6| Step: 7
Training loss: 3.986264705657959
Validation loss: 4.3273301521937055

Epoch: 6| Step: 8
Training loss: 4.543869495391846
Validation loss: 4.319812615712483

Epoch: 6| Step: 9
Training loss: 4.545864582061768
Validation loss: 4.311369260152181

Epoch: 6| Step: 10
Training loss: 4.746855735778809
Validation loss: 4.302996397018433

Epoch: 6| Step: 11
Training loss: 3.8026680946350098
Validation loss: 4.2958773374557495

Epoch: 6| Step: 12
Training loss: 5.344281196594238
Validation loss: 4.288163701693217

Epoch: 6| Step: 13
Training loss: 4.684366226196289
Validation loss: 4.280720790227254

Epoch: 11| Step: 0
Training loss: 5.014341354370117
Validation loss: 4.272717873255412

Epoch: 6| Step: 1
Training loss: 5.156023979187012
Validation loss: 4.266175031661987

Epoch: 6| Step: 2
Training loss: 3.9853320121765137
Validation loss: 4.258919795354207

Epoch: 6| Step: 3
Training loss: 4.815241813659668
Validation loss: 4.2518532276153564

Epoch: 6| Step: 4
Training loss: 4.897432327270508
Validation loss: 4.244645555814107

Epoch: 6| Step: 5
Training loss: 4.052177429199219
Validation loss: 4.2376822630564375

Epoch: 6| Step: 6
Training loss: 3.8440723419189453
Validation loss: 4.230798800786336

Epoch: 6| Step: 7
Training loss: 4.483168601989746
Validation loss: 4.224425236384074

Epoch: 6| Step: 8
Training loss: 5.673079967498779
Validation loss: 4.217928171157837

Epoch: 6| Step: 9
Training loss: 3.813204050064087
Validation loss: 4.211351990699768

Epoch: 6| Step: 10
Training loss: 4.537059783935547
Validation loss: 4.2043940623601275

Epoch: 6| Step: 11
Training loss: 3.813539505004883
Validation loss: 4.1974243720372515

Epoch: 6| Step: 12
Training loss: 3.4235472679138184
Validation loss: 4.190924088160197

Epoch: 6| Step: 13
Training loss: 3.533968925476074
Validation loss: 4.184342424074809

Epoch: 12| Step: 0
Training loss: 4.258525848388672
Validation loss: 4.17847200234731

Epoch: 6| Step: 1
Training loss: 4.7463507652282715
Validation loss: 4.171804269154866

Epoch: 6| Step: 2
Training loss: 4.455892562866211
Validation loss: 4.167203346888225

Epoch: 6| Step: 3
Training loss: 4.113069534301758
Validation loss: 4.160586714744568

Epoch: 6| Step: 4
Training loss: 3.908761739730835
Validation loss: 4.154174288113912

Epoch: 6| Step: 5
Training loss: 4.396108627319336
Validation loss: 4.1483341455459595

Epoch: 6| Step: 6
Training loss: 4.602663993835449
Validation loss: 4.143524964650472

Epoch: 6| Step: 7
Training loss: 5.029964447021484
Validation loss: 4.139034231503804

Epoch: 6| Step: 8
Training loss: 4.598189830780029
Validation loss: 4.1321009794871015

Epoch: 6| Step: 9
Training loss: 4.217343330383301
Validation loss: 4.126445889472961

Epoch: 6| Step: 10
Training loss: 3.7225334644317627
Validation loss: 4.120702902475993

Epoch: 6| Step: 11
Training loss: 4.546849250793457
Validation loss: 4.114842255910237

Epoch: 6| Step: 12
Training loss: 3.4879531860351562
Validation loss: 4.1097027858098345

Epoch: 6| Step: 13
Training loss: 3.835343599319458
Validation loss: 4.104068915049235

Epoch: 13| Step: 0
Training loss: 3.4979019165039062
Validation loss: 4.0990768273671465

Epoch: 6| Step: 1
Training loss: 4.664791107177734
Validation loss: 4.093323548634847

Epoch: 6| Step: 2
Training loss: 4.7715864181518555
Validation loss: 4.087686896324158

Epoch: 6| Step: 3
Training loss: 3.5694546699523926
Validation loss: 4.08159859975179

Epoch: 6| Step: 4
Training loss: 3.721367359161377
Validation loss: 4.076793750127156

Epoch: 6| Step: 5
Training loss: 4.048741340637207
Validation loss: 4.071674664815267

Epoch: 6| Step: 6
Training loss: 3.720066547393799
Validation loss: 4.066747347513835

Epoch: 6| Step: 7
Training loss: 4.740992546081543
Validation loss: 4.062149365743001

Epoch: 6| Step: 8
Training loss: 3.783538818359375
Validation loss: 4.056671142578125

Epoch: 6| Step: 9
Training loss: 4.368213653564453
Validation loss: 4.051573991775513

Epoch: 6| Step: 10
Training loss: 4.105444431304932
Validation loss: 4.047005772590637

Epoch: 6| Step: 11
Training loss: 4.369298458099365
Validation loss: 4.041662096977234

Epoch: 6| Step: 12
Training loss: 4.383810997009277
Validation loss: 4.036961952845256

Epoch: 6| Step: 13
Training loss: 5.1570024490356445
Validation loss: 4.032633741696675

Epoch: 14| Step: 0
Training loss: 3.7365341186523438
Validation loss: 4.027050773302714

Epoch: 6| Step: 1
Training loss: 4.161357879638672
Validation loss: 4.022035876909892

Epoch: 6| Step: 2
Training loss: 4.263926029205322
Validation loss: 4.017600893974304

Epoch: 6| Step: 3
Training loss: 4.465651512145996
Validation loss: 4.012744665145874

Epoch: 6| Step: 4
Training loss: 4.350623607635498
Validation loss: 4.007734576861064

Epoch: 6| Step: 5
Training loss: 3.9778003692626953
Validation loss: 4.0034617980321245

Epoch: 6| Step: 6
Training loss: 4.051877498626709
Validation loss: 3.9989938338597617

Epoch: 6| Step: 7
Training loss: 4.809325695037842
Validation loss: 3.9938822587331138

Epoch: 6| Step: 8
Training loss: 4.190474510192871
Validation loss: 3.988397479057312

Epoch: 6| Step: 9
Training loss: 3.526683807373047
Validation loss: 3.98422118028005

Epoch: 6| Step: 10
Training loss: 5.20539665222168
Validation loss: 3.9784783124923706

Epoch: 6| Step: 11
Training loss: 4.281590461730957
Validation loss: 3.9734069108963013

Epoch: 6| Step: 12
Training loss: 3.180042266845703
Validation loss: 3.968418995539347

Epoch: 6| Step: 13
Training loss: 3.7804975509643555
Validation loss: 3.962795297304789

Epoch: 15| Step: 0
Training loss: 3.185366153717041
Validation loss: 3.9585499366124473

Epoch: 6| Step: 1
Training loss: 4.844164848327637
Validation loss: 3.9544041951497397

Epoch: 6| Step: 2
Training loss: 3.6244606971740723
Validation loss: 3.9497332175572715

Epoch: 6| Step: 3
Training loss: 3.913663864135742
Validation loss: 3.9447606007258096

Epoch: 6| Step: 4
Training loss: 4.0424041748046875
Validation loss: 3.939511775970459

Epoch: 6| Step: 5
Training loss: 4.144618988037109
Validation loss: 3.9344672362009683

Epoch: 6| Step: 6
Training loss: 5.3457231521606445
Validation loss: 3.930022398630778

Epoch: 6| Step: 7
Training loss: 3.6519651412963867
Validation loss: 3.9245594342549643

Epoch: 6| Step: 8
Training loss: 3.2980425357818604
Validation loss: 3.9197850227355957

Epoch: 6| Step: 9
Training loss: 4.052847862243652
Validation loss: 3.9154776334762573

Epoch: 6| Step: 10
Training loss: 4.063381195068359
Validation loss: 3.911116361618042

Epoch: 6| Step: 11
Training loss: 4.545028209686279
Validation loss: 3.9062556823094687

Epoch: 6| Step: 12
Training loss: 3.7605228424072266
Validation loss: 3.9012825886408486

Epoch: 6| Step: 13
Training loss: 4.591984748840332
Validation loss: 3.89738663037618

Epoch: 16| Step: 0
Training loss: 3.6428651809692383
Validation loss: 3.892569820086161

Epoch: 6| Step: 1
Training loss: 3.520707368850708
Validation loss: 3.8885147968928018

Epoch: 6| Step: 2
Training loss: 3.6330432891845703
Validation loss: 3.8835220336914062

Epoch: 6| Step: 3
Training loss: 4.258145809173584
Validation loss: 3.8789554437001548

Epoch: 6| Step: 4
Training loss: 4.047613143920898
Validation loss: 3.8743335803349814

Epoch: 6| Step: 5
Training loss: 4.028928279876709
Validation loss: 3.8696428140004477

Epoch: 6| Step: 6
Training loss: 5.67905330657959
Validation loss: 3.8647193113962808

Epoch: 6| Step: 7
Training loss: 3.962585210800171
Validation loss: 3.8598791360855103

Epoch: 6| Step: 8
Training loss: 4.640625476837158
Validation loss: 3.8553682963053384

Epoch: 6| Step: 9
Training loss: 3.933511257171631
Validation loss: 3.8506835301717124

Epoch: 6| Step: 10
Training loss: 4.224234104156494
Validation loss: 3.8460758129755654

Epoch: 6| Step: 11
Training loss: 3.794729709625244
Validation loss: 3.8412640492121377

Epoch: 6| Step: 12
Training loss: 4.424873352050781
Validation loss: 3.8366070985794067

Epoch: 6| Step: 13
Training loss: 2.421070098876953
Validation loss: 3.832477251688639

Epoch: 17| Step: 0
Training loss: 2.6881515979766846
Validation loss: 3.827264189720154

Epoch: 6| Step: 1
Training loss: 4.057787895202637
Validation loss: 3.8234722216924033

Epoch: 6| Step: 2
Training loss: 4.966042995452881
Validation loss: 3.8193016846974692

Epoch: 6| Step: 3
Training loss: 4.420819282531738
Validation loss: 3.815682331720988

Epoch: 6| Step: 4
Training loss: 4.736501216888428
Validation loss: 3.810149629910787

Epoch: 6| Step: 5
Training loss: 4.516718864440918
Validation loss: 3.8051804304122925

Epoch: 6| Step: 6
Training loss: 3.8552777767181396
Validation loss: 3.8008437156677246

Epoch: 6| Step: 7
Training loss: 4.079780101776123
Validation loss: 3.7969866196314492

Epoch: 6| Step: 8
Training loss: 3.220493793487549
Validation loss: 3.7939149936040244

Epoch: 6| Step: 9
Training loss: 4.057901382446289
Validation loss: 3.788634697596232

Epoch: 6| Step: 10
Training loss: 3.4794130325317383
Validation loss: 3.7836942275365195

Epoch: 6| Step: 11
Training loss: 3.798126697540283
Validation loss: 3.779500206311544

Epoch: 6| Step: 12
Training loss: 3.218873977661133
Validation loss: 3.7745747566223145

Epoch: 6| Step: 13
Training loss: 4.282810688018799
Validation loss: 3.769725958506266

Epoch: 18| Step: 0
Training loss: 3.9479925632476807
Validation loss: 3.7652390797932944

Epoch: 6| Step: 1
Training loss: 4.373968124389648
Validation loss: 3.761574069658915

Epoch: 6| Step: 2
Training loss: 3.2128193378448486
Validation loss: 3.756771524747213

Epoch: 6| Step: 3
Training loss: 3.5885655879974365
Validation loss: 3.7525874376296997

Epoch: 6| Step: 4
Training loss: 3.9618771076202393
Validation loss: 3.7483015855153403

Epoch: 6| Step: 5
Training loss: 3.534877300262451
Validation loss: 3.7438440720240274

Epoch: 6| Step: 6
Training loss: 4.565830707550049
Validation loss: 3.740246891975403

Epoch: 6| Step: 7
Training loss: 3.520493268966675
Validation loss: 3.7361428340276084

Epoch: 6| Step: 8
Training loss: 4.5360565185546875
Validation loss: 3.7322434981664023

Epoch: 6| Step: 9
Training loss: 3.4113516807556152
Validation loss: 3.7276620467503867

Epoch: 6| Step: 10
Training loss: 3.425601005554199
Validation loss: 3.7228153149286904

Epoch: 6| Step: 11
Training loss: 3.6515913009643555
Validation loss: 3.719431519508362

Epoch: 6| Step: 12
Training loss: 4.548392295837402
Validation loss: 3.7159504493077598

Epoch: 6| Step: 13
Training loss: 4.265653610229492
Validation loss: 3.7114513317743936

Epoch: 19| Step: 0
Training loss: 2.956021785736084
Validation loss: 3.707345207532247

Epoch: 6| Step: 1
Training loss: 4.880536079406738
Validation loss: 3.7021919091542563

Epoch: 6| Step: 2
Training loss: 3.4423844814300537
Validation loss: 3.697557012240092

Epoch: 6| Step: 3
Training loss: 3.7831900119781494
Validation loss: 3.6935837666193643

Epoch: 6| Step: 4
Training loss: 3.615485191345215
Validation loss: 3.689277489980062

Epoch: 6| Step: 5
Training loss: 3.855372905731201
Validation loss: 3.68532665570577

Epoch: 6| Step: 6
Training loss: 3.4000556468963623
Validation loss: 3.6807940006256104

Epoch: 6| Step: 7
Training loss: 4.748287677764893
Validation loss: 3.6763914426167807

Epoch: 6| Step: 8
Training loss: 3.980380058288574
Validation loss: 3.6719489892323813

Epoch: 6| Step: 9
Training loss: 4.496021270751953
Validation loss: 3.6673229138056436

Epoch: 6| Step: 10
Training loss: 3.395826578140259
Validation loss: 3.663162191708883

Epoch: 6| Step: 11
Training loss: 3.867422580718994
Validation loss: 3.658347209294637

Epoch: 6| Step: 12
Training loss: 3.710575819015503
Validation loss: 3.6540319522221885

Epoch: 6| Step: 13
Training loss: 3.6338202953338623
Validation loss: 3.6498461167017617

Epoch: 20| Step: 0
Training loss: 3.318190574645996
Validation loss: 3.6460511287053428

Epoch: 6| Step: 1
Training loss: 3.53751540184021
Validation loss: 3.6418465773264566

Epoch: 6| Step: 2
Training loss: 4.114390850067139
Validation loss: 3.6376362641652427

Epoch: 6| Step: 3
Training loss: 4.267806053161621
Validation loss: 3.6331220070521035

Epoch: 6| Step: 4
Training loss: 3.357644557952881
Validation loss: 3.6279796361923218

Epoch: 6| Step: 5
Training loss: 3.300783157348633
Validation loss: 3.6238465706507363

Epoch: 6| Step: 6
Training loss: 3.3888795375823975
Validation loss: 3.6203485329945884

Epoch: 6| Step: 7
Training loss: 2.7279906272888184
Validation loss: 3.6161656777064004

Epoch: 6| Step: 8
Training loss: 3.48004150390625
Validation loss: 3.612263560295105

Epoch: 6| Step: 9
Training loss: 4.332537651062012
Validation loss: 3.6073522567749023

Epoch: 6| Step: 10
Training loss: 4.666779041290283
Validation loss: 3.6031493743260703

Epoch: 6| Step: 11
Training loss: 3.8834621906280518
Validation loss: 3.598582625389099

Epoch: 6| Step: 12
Training loss: 3.901392936706543
Validation loss: 3.595004995663961

Epoch: 6| Step: 13
Training loss: 4.639808654785156
Validation loss: 3.590502460797628

Epoch: 21| Step: 0
Training loss: 4.557720184326172
Validation loss: 3.5857088565826416

Epoch: 6| Step: 1
Training loss: 3.6840639114379883
Validation loss: 3.580804467201233

Epoch: 6| Step: 2
Training loss: 3.5222575664520264
Validation loss: 3.5756291151046753

Epoch: 6| Step: 3
Training loss: 4.670436859130859
Validation loss: 3.571108818054199

Epoch: 6| Step: 4
Training loss: 3.37570858001709
Validation loss: 3.5666922330856323

Epoch: 6| Step: 5
Training loss: 3.325515031814575
Validation loss: 3.562537352244059

Epoch: 6| Step: 6
Training loss: 3.5148844718933105
Validation loss: 3.5578835805257163

Epoch: 6| Step: 7
Training loss: 3.659269094467163
Validation loss: 3.553274313608805

Epoch: 6| Step: 8
Training loss: 3.6418752670288086
Validation loss: 3.54798416296641

Epoch: 6| Step: 9
Training loss: 3.935401439666748
Validation loss: 3.543230652809143

Epoch: 6| Step: 10
Training loss: 4.382417678833008
Validation loss: 3.5389122565587363

Epoch: 6| Step: 11
Training loss: 3.49068546295166
Validation loss: 3.534154534339905

Epoch: 6| Step: 12
Training loss: 3.05711030960083
Validation loss: 3.5294145345687866

Epoch: 6| Step: 13
Training loss: 3.287804126739502
Validation loss: 3.525403618812561

Epoch: 22| Step: 0
Training loss: 4.289654731750488
Validation loss: 3.5206493139266968

Epoch: 6| Step: 1
Training loss: 3.6017656326293945
Validation loss: 3.5148744583129883

Epoch: 6| Step: 2
Training loss: 3.782061815261841
Validation loss: 3.5113184054692588

Epoch: 6| Step: 3
Training loss: 4.228879451751709
Validation loss: 3.506918986638387

Epoch: 6| Step: 4
Training loss: 3.553567409515381
Validation loss: 3.5024522145589194

Epoch: 6| Step: 5
Training loss: 3.2060582637786865
Validation loss: 3.49833083152771

Epoch: 6| Step: 6
Training loss: 3.479832172393799
Validation loss: 3.493743658065796

Epoch: 6| Step: 7
Training loss: 2.790304183959961
Validation loss: 3.48927104473114

Epoch: 6| Step: 8
Training loss: 3.7952017784118652
Validation loss: 3.484970529874166

Epoch: 6| Step: 9
Training loss: 3.9218478202819824
Validation loss: 3.480715751647949

Epoch: 6| Step: 10
Training loss: 3.2250657081604004
Validation loss: 3.4759724934895835

Epoch: 6| Step: 11
Training loss: 3.472445249557495
Validation loss: 3.4717796246210733

Epoch: 6| Step: 12
Training loss: 3.8324742317199707
Validation loss: 3.4674174388249717

Epoch: 6| Step: 13
Training loss: 4.039313793182373
Validation loss: 3.463249842325846

Epoch: 23| Step: 0
Training loss: 4.5661492347717285
Validation loss: 3.458135406176249

Epoch: 6| Step: 1
Training loss: 3.363302230834961
Validation loss: 3.4536457459131875

Epoch: 6| Step: 2
Training loss: 2.658702850341797
Validation loss: 3.4488309224446616

Epoch: 6| Step: 3
Training loss: 3.159954309463501
Validation loss: 3.4450318415959678

Epoch: 6| Step: 4
Training loss: 4.03515625
Validation loss: 3.440296490987142

Epoch: 6| Step: 5
Training loss: 4.019492149353027
Validation loss: 3.4358773231506348

Epoch: 6| Step: 6
Training loss: 4.028530120849609
Validation loss: 3.431639075279236

Epoch: 6| Step: 7
Training loss: 3.4454174041748047
Validation loss: 3.4273502429326377

Epoch: 6| Step: 8
Training loss: 3.232220411300659
Validation loss: 3.422533551851908

Epoch: 6| Step: 9
Training loss: 4.4178667068481445
Validation loss: 3.4181950092315674

Epoch: 6| Step: 10
Training loss: 3.1463088989257812
Validation loss: 3.4139324426651

Epoch: 6| Step: 11
Training loss: 3.784776210784912
Validation loss: 3.4084537426630654

Epoch: 6| Step: 12
Training loss: 3.3770530223846436
Validation loss: 3.4039666255315146

Epoch: 6| Step: 13
Training loss: 3.105865478515625
Validation loss: 3.39873472849528

Epoch: 24| Step: 0
Training loss: 2.913416862487793
Validation loss: 3.3944784800211587

Epoch: 6| Step: 1
Training loss: 3.645615577697754
Validation loss: 3.3895721435546875

Epoch: 6| Step: 2
Training loss: 3.5660529136657715
Validation loss: 3.384969194730123

Epoch: 6| Step: 3
Training loss: 3.6544156074523926
Validation loss: 3.3798996607462564

Epoch: 6| Step: 4
Training loss: 3.611898899078369
Validation loss: 3.3751558462778726

Epoch: 6| Step: 5
Training loss: 3.458052158355713
Validation loss: 3.370765050252279

Epoch: 6| Step: 6
Training loss: 4.249669075012207
Validation loss: 3.365877906481425

Epoch: 6| Step: 7
Training loss: 3.086141586303711
Validation loss: 3.361294428507487

Epoch: 6| Step: 8
Training loss: 3.6104929447174072
Validation loss: 3.356528321901957

Epoch: 6| Step: 9
Training loss: 3.8066024780273438
Validation loss: 3.352423310279846

Epoch: 6| Step: 10
Training loss: 4.091887474060059
Validation loss: 3.347788174947103

Epoch: 6| Step: 11
Training loss: 2.722090244293213
Validation loss: 3.3429778814315796

Epoch: 6| Step: 12
Training loss: 3.324146270751953
Validation loss: 3.339064280192057

Epoch: 6| Step: 13
Training loss: 3.755112886428833
Validation loss: 3.3339321613311768

Epoch: 25| Step: 0
Training loss: 2.871103525161743
Validation loss: 3.3289211988449097

Epoch: 6| Step: 1
Training loss: 3.188239574432373
Validation loss: 3.324820955594381

Epoch: 6| Step: 2
Training loss: 4.2147111892700195
Validation loss: 3.320474624633789

Epoch: 6| Step: 3
Training loss: 2.5656208992004395
Validation loss: 3.315632422765096

Epoch: 6| Step: 4
Training loss: 3.1094422340393066
Validation loss: 3.311149557431539

Epoch: 6| Step: 5
Training loss: 3.987898111343384
Validation loss: 3.306733330090841

Epoch: 6| Step: 6
Training loss: 3.6630001068115234
Validation loss: 3.302202582359314

Epoch: 6| Step: 7
Training loss: 4.458404541015625
Validation loss: 3.2981920639673867

Epoch: 6| Step: 8
Training loss: 3.0778050422668457
Validation loss: 3.293378472328186

Epoch: 6| Step: 9
Training loss: 3.7902169227600098
Validation loss: 3.2896356185277305

Epoch: 6| Step: 10
Training loss: 2.693751573562622
Validation loss: 3.285310943921407

Epoch: 6| Step: 11
Training loss: 4.231895446777344
Validation loss: 3.280858596165975

Epoch: 6| Step: 12
Training loss: 3.0901503562927246
Validation loss: 3.276897350947062

Epoch: 6| Step: 13
Training loss: 3.7036638259887695
Validation loss: 3.2721975247065225

Epoch: 26| Step: 0
Training loss: 3.1264748573303223
Validation loss: 3.26829198996226

Epoch: 6| Step: 1
Training loss: 3.8010754585266113
Validation loss: 3.26406991481781

Epoch: 6| Step: 2
Training loss: 3.3394267559051514
Validation loss: 3.2603010336558023

Epoch: 6| Step: 3
Training loss: 3.438924551010132
Validation loss: 3.255590796470642

Epoch: 6| Step: 4
Training loss: 3.9793472290039062
Validation loss: 3.2514206568400064

Epoch: 6| Step: 5
Training loss: 3.8574295043945312
Validation loss: 3.2466472387313843

Epoch: 6| Step: 6
Training loss: 3.233262538909912
Validation loss: 3.242005546887716

Epoch: 6| Step: 7
Training loss: 3.1923365592956543
Validation loss: 3.2378236452738443

Epoch: 6| Step: 8
Training loss: 3.016511917114258
Validation loss: 3.233190337816874

Epoch: 6| Step: 9
Training loss: 2.8202428817749023
Validation loss: 3.228490114212036

Epoch: 6| Step: 10
Training loss: 3.704488515853882
Validation loss: 3.224298636118571

Epoch: 6| Step: 11
Training loss: 3.0929813385009766
Validation loss: 3.2198277711868286

Epoch: 6| Step: 12
Training loss: 3.5856118202209473
Validation loss: 3.2151746352513633

Epoch: 6| Step: 13
Training loss: 3.6648006439208984
Validation loss: 3.211002230644226

Epoch: 27| Step: 0
Training loss: 3.775252342224121
Validation loss: 3.2065929969151816

Epoch: 6| Step: 1
Training loss: 2.573782444000244
Validation loss: 3.202138582865397

Epoch: 6| Step: 2
Training loss: 3.425295114517212
Validation loss: 3.1977827548980713

Epoch: 6| Step: 3
Training loss: 2.919491767883301
Validation loss: 3.1932026147842407

Epoch: 6| Step: 4
Training loss: 3.3627896308898926
Validation loss: 3.18807582060496

Epoch: 6| Step: 5
Training loss: 3.6642308235168457
Validation loss: 3.1840809186299643

Epoch: 6| Step: 6
Training loss: 3.5776772499084473
Validation loss: 3.179381847381592

Epoch: 6| Step: 7
Training loss: 3.3240716457366943
Validation loss: 3.1750711599985757

Epoch: 6| Step: 8
Training loss: 3.1634912490844727
Validation loss: 3.1709936062494912

Epoch: 6| Step: 9
Training loss: 3.6520447731018066
Validation loss: 3.166679541269938

Epoch: 6| Step: 10
Training loss: 3.5902905464172363
Validation loss: 3.1625904639561973

Epoch: 6| Step: 11
Training loss: 3.48482608795166
Validation loss: 3.158465266227722

Epoch: 6| Step: 12
Training loss: 3.606328010559082
Validation loss: 3.154029885927836

Epoch: 6| Step: 13
Training loss: 2.9547863006591797
Validation loss: 3.1499693393707275

Epoch: 28| Step: 0
Training loss: 3.5101475715637207
Validation loss: 3.1461186011632285

Epoch: 6| Step: 1
Training loss: 4.110010623931885
Validation loss: 3.141512115796407

Epoch: 6| Step: 2
Training loss: 2.3328962326049805
Validation loss: 3.137176513671875

Epoch: 6| Step: 3
Training loss: 3.6170151233673096
Validation loss: 3.132812023162842

Epoch: 6| Step: 4
Training loss: 3.4214062690734863
Validation loss: 3.128585934638977

Epoch: 6| Step: 5
Training loss: 3.23374080657959
Validation loss: 3.124205390612284

Epoch: 6| Step: 6
Training loss: 3.6462090015411377
Validation loss: 3.1199866930643716

Epoch: 6| Step: 7
Training loss: 3.3451666831970215
Validation loss: 3.115716060002645

Epoch: 6| Step: 8
Training loss: 3.338937282562256
Validation loss: 3.1111008326212564

Epoch: 6| Step: 9
Training loss: 4.092209815979004
Validation loss: 3.107542872428894

Epoch: 6| Step: 10
Training loss: 2.559253454208374
Validation loss: 3.10259211063385

Epoch: 6| Step: 11
Training loss: 2.6901350021362305
Validation loss: 3.09880801041921

Epoch: 6| Step: 12
Training loss: 2.281040906906128
Validation loss: 3.0952099561691284

Epoch: 6| Step: 13
Training loss: 4.123415946960449
Validation loss: 3.0913486083348594

Epoch: 29| Step: 0
Training loss: 3.273777961730957
Validation loss: 3.087377905845642

Epoch: 6| Step: 1
Training loss: 3.739967107772827
Validation loss: 3.0832523504892984

Epoch: 6| Step: 2
Training loss: 4.252396583557129
Validation loss: 3.0797059535980225

Epoch: 6| Step: 3
Training loss: 2.9202022552490234
Validation loss: 3.0755141178766885

Epoch: 6| Step: 4
Training loss: 2.584545373916626
Validation loss: 3.071731964747111

Epoch: 6| Step: 5
Training loss: 3.2342529296875
Validation loss: 3.0678037802378335

Epoch: 6| Step: 6
Training loss: 3.358854293823242
Validation loss: 3.0640916426976523

Epoch: 6| Step: 7
Training loss: 3.649595260620117
Validation loss: 3.060259540875753

Epoch: 6| Step: 8
Training loss: 3.391326427459717
Validation loss: 3.056879679361979

Epoch: 6| Step: 9
Training loss: 3.3926281929016113
Validation loss: 3.052651127179464

Epoch: 6| Step: 10
Training loss: 3.298320770263672
Validation loss: 3.0488098859786987

Epoch: 6| Step: 11
Training loss: 2.5315990447998047
Validation loss: 3.0451846520105996

Epoch: 6| Step: 12
Training loss: 3.3203482627868652
Validation loss: 3.0412298838297525

Epoch: 6| Step: 13
Training loss: 2.6065773963928223
Validation loss: 3.0372051000595093

Epoch: 30| Step: 0
Training loss: 3.2974472045898438
Validation loss: 3.033390680948893

Epoch: 6| Step: 1
Training loss: 3.442643165588379
Validation loss: 3.029619812965393

Epoch: 6| Step: 2
Training loss: 3.403179168701172
Validation loss: 3.025925079981486

Epoch: 6| Step: 3
Training loss: 3.9029746055603027
Validation loss: 3.022097865740458

Epoch: 6| Step: 4
Training loss: 3.0473480224609375
Validation loss: 3.018192728360494

Epoch: 6| Step: 5
Training loss: 3.143045425415039
Validation loss: 3.0142940282821655

Epoch: 6| Step: 6
Training loss: 3.4891152381896973
Validation loss: 3.0105908314387

Epoch: 6| Step: 7
Training loss: 2.649562358856201
Validation loss: 3.006500482559204

Epoch: 6| Step: 8
Training loss: 2.346229076385498
Validation loss: 3.0030327240626016

Epoch: 6| Step: 9
Training loss: 3.0428333282470703
Validation loss: 2.998947540918986

Epoch: 6| Step: 10
Training loss: 2.955535411834717
Validation loss: 2.9953153133392334

Epoch: 6| Step: 11
Training loss: 2.1434640884399414
Validation loss: 2.9912173748016357

Epoch: 6| Step: 12
Training loss: 3.83088755607605
Validation loss: 2.987796147664388

Epoch: 6| Step: 13
Training loss: 4.14786434173584
Validation loss: 2.984313170115153

Epoch: 31| Step: 0
Training loss: 2.667431354522705
Validation loss: 2.980238358179728

Epoch: 6| Step: 1
Training loss: 2.6298253536224365
Validation loss: 2.9767580032348633

Epoch: 6| Step: 2
Training loss: 3.5415456295013428
Validation loss: 2.973277727762858

Epoch: 6| Step: 3
Training loss: 2.289486885070801
Validation loss: 2.9695538679758706

Epoch: 6| Step: 4
Training loss: 2.9100184440612793
Validation loss: 2.9661178986231485

Epoch: 6| Step: 5
Training loss: 3.9477357864379883
Validation loss: 2.9623725414276123

Epoch: 6| Step: 6
Training loss: 3.868978500366211
Validation loss: 2.9586936632792153

Epoch: 6| Step: 7
Training loss: 3.157743453979492
Validation loss: 2.954835613568624

Epoch: 6| Step: 8
Training loss: 3.7710344791412354
Validation loss: 2.9507880210876465

Epoch: 6| Step: 9
Training loss: 2.970702648162842
Validation loss: 2.9468966325124106

Epoch: 6| Step: 10
Training loss: 3.0748064517974854
Validation loss: 2.943156282107035

Epoch: 6| Step: 11
Training loss: 2.7773475646972656
Validation loss: 2.9394112825393677

Epoch: 6| Step: 12
Training loss: 3.0942115783691406
Validation loss: 2.9355385303497314

Epoch: 6| Step: 13
Training loss: 3.472133159637451
Validation loss: 2.9315330187479653

Epoch: 32| Step: 0
Training loss: 3.194739818572998
Validation loss: 2.927510658899943

Epoch: 6| Step: 1
Training loss: 2.5026984214782715
Validation loss: 2.9240523179372153

Epoch: 6| Step: 2
Training loss: 3.6631903648376465
Validation loss: 2.9200985034306846

Epoch: 6| Step: 3
Training loss: 3.0680646896362305
Validation loss: 2.9169671138127646

Epoch: 6| Step: 4
Training loss: 3.65694522857666
Validation loss: 2.9132845799128213

Epoch: 6| Step: 5
Training loss: 3.496692419052124
Validation loss: 2.910246253013611

Epoch: 6| Step: 6
Training loss: 3.5129642486572266
Validation loss: 2.906573534011841

Epoch: 6| Step: 7
Training loss: 3.1278653144836426
Validation loss: 2.9027405182520547

Epoch: 6| Step: 8
Training loss: 3.6258769035339355
Validation loss: 2.8993947505950928

Epoch: 6| Step: 9
Training loss: 2.0994582176208496
Validation loss: 2.8953508933385215

Epoch: 6| Step: 10
Training loss: 3.263256788253784
Validation loss: 2.89225697517395

Epoch: 6| Step: 11
Training loss: 3.0373129844665527
Validation loss: 2.887840151786804

Epoch: 6| Step: 12
Training loss: 2.59157657623291
Validation loss: 2.8843551874160767

Epoch: 6| Step: 13
Training loss: 2.6983084678649902
Validation loss: 2.88038698832194

Epoch: 33| Step: 0
Training loss: 3.4304287433624268
Validation loss: 2.876588304837545

Epoch: 6| Step: 1
Training loss: 2.730499267578125
Validation loss: 2.872827132542928

Epoch: 6| Step: 2
Training loss: 2.7363622188568115
Validation loss: 2.869555711746216

Epoch: 6| Step: 3
Training loss: 3.7160844802856445
Validation loss: 2.8657747507095337

Epoch: 6| Step: 4
Training loss: 3.4828474521636963
Validation loss: 2.861984650293986

Epoch: 6| Step: 5
Training loss: 2.8000402450561523
Validation loss: 2.8581764300664267

Epoch: 6| Step: 6
Training loss: 2.4734110832214355
Validation loss: 2.8540984789530435

Epoch: 6| Step: 7
Training loss: 3.421902656555176
Validation loss: 2.8509411017100015

Epoch: 6| Step: 8
Training loss: 3.597118616104126
Validation loss: 2.8471275568008423

Epoch: 6| Step: 9
Training loss: 2.825145721435547
Validation loss: 2.843352754910787

Epoch: 6| Step: 10
Training loss: 2.9040491580963135
Validation loss: 2.839525739351908

Epoch: 6| Step: 11
Training loss: 2.7384259700775146
Validation loss: 2.8361941973368325

Epoch: 6| Step: 12
Training loss: 3.4634687900543213
Validation loss: 2.832581082979838

Epoch: 6| Step: 13
Training loss: 2.595310688018799
Validation loss: 2.829285422960917

Epoch: 34| Step: 0
Training loss: 3.9542765617370605
Validation loss: 2.8259073893229165

Epoch: 6| Step: 1
Training loss: 2.5583455562591553
Validation loss: 2.822213292121887

Epoch: 6| Step: 2
Training loss: 3.144446849822998
Validation loss: 2.818838596343994

Epoch: 6| Step: 3
Training loss: 3.5580358505249023
Validation loss: 2.8156560262044272

Epoch: 6| Step: 4
Training loss: 2.6198620796203613
Validation loss: 2.8128063678741455

Epoch: 6| Step: 5
Training loss: 3.1895322799682617
Validation loss: 2.8096307118733725

Epoch: 6| Step: 6
Training loss: 3.0340263843536377
Validation loss: 2.806531528631846

Epoch: 6| Step: 7
Training loss: 2.7366557121276855
Validation loss: 2.8031064669291177

Epoch: 6| Step: 8
Training loss: 2.8274590969085693
Validation loss: 2.800100406010946

Epoch: 6| Step: 9
Training loss: 2.6189370155334473
Validation loss: 2.796855608622233

Epoch: 6| Step: 10
Training loss: 3.112931728363037
Validation loss: 2.7935970226923623

Epoch: 6| Step: 11
Training loss: 3.2709603309631348
Validation loss: 2.7905290126800537

Epoch: 6| Step: 12
Training loss: 3.2857866287231445
Validation loss: 2.7872830629348755

Epoch: 6| Step: 13
Training loss: 2.3394641876220703
Validation loss: 2.784175157546997

Epoch: 35| Step: 0
Training loss: 3.3968658447265625
Validation loss: 2.780932585398356

Epoch: 6| Step: 1
Training loss: 3.1365137100219727
Validation loss: 2.77718993028005

Epoch: 6| Step: 2
Training loss: 3.2664380073547363
Validation loss: 2.773782173792521

Epoch: 6| Step: 3
Training loss: 2.9395089149475098
Validation loss: 2.770294745763143

Epoch: 6| Step: 4
Training loss: 2.4542927742004395
Validation loss: 2.76686426003774

Epoch: 6| Step: 5
Training loss: 3.3814210891723633
Validation loss: 2.76422111193339

Epoch: 6| Step: 6
Training loss: 3.189277172088623
Validation loss: 2.760704457759857

Epoch: 6| Step: 7
Training loss: 3.00520658493042
Validation loss: 2.757787843545278

Epoch: 6| Step: 8
Training loss: 2.4385337829589844
Validation loss: 2.754323403040568

Epoch: 6| Step: 9
Training loss: 3.0179543495178223
Validation loss: 2.7517396608988443

Epoch: 6| Step: 10
Training loss: 2.994635581970215
Validation loss: 2.7493184407552085

Epoch: 6| Step: 11
Training loss: 2.783759832382202
Validation loss: 2.7457822958628335

Epoch: 6| Step: 12
Training loss: 2.956101179122925
Validation loss: 2.7423890829086304

Epoch: 6| Step: 13
Training loss: 2.6457433700561523
Validation loss: 2.7391805251439414

Epoch: 36| Step: 0
Training loss: 3.0970048904418945
Validation loss: 2.7360097567240396

Epoch: 6| Step: 1
Training loss: 2.705024242401123
Validation loss: 2.7330540815989175

Epoch: 6| Step: 2
Training loss: 3.627896785736084
Validation loss: 2.7303936878840127

Epoch: 6| Step: 3
Training loss: 2.12849760055542
Validation loss: 2.7278587023417153

Epoch: 6| Step: 4
Training loss: 2.763383388519287
Validation loss: 2.7245315313339233

Epoch: 6| Step: 5
Training loss: 2.7832112312316895
Validation loss: 2.7217745582262673

Epoch: 6| Step: 6
Training loss: 3.276350259780884
Validation loss: 2.7179245154062905

Epoch: 6| Step: 7
Training loss: 2.7181808948516846
Validation loss: 2.714671492576599

Epoch: 6| Step: 8
Training loss: 2.508031129837036
Validation loss: 2.7108881076176963

Epoch: 6| Step: 9
Training loss: 2.3235344886779785
Validation loss: 2.7076414028803506

Epoch: 6| Step: 10
Training loss: 3.177520990371704
Validation loss: 2.704475442568461

Epoch: 6| Step: 11
Training loss: 3.1194841861724854
Validation loss: 2.701002597808838

Epoch: 6| Step: 12
Training loss: 3.4144675731658936
Validation loss: 2.6981964906056723

Epoch: 6| Step: 13
Training loss: 3.3067336082458496
Validation loss: 2.7208602031071982

Epoch: 37| Step: 0
Training loss: 2.597393035888672
Validation loss: 2.6912821531295776

Epoch: 6| Step: 1
Training loss: 3.632354974746704
Validation loss: 2.6884679198265076

Epoch: 6| Step: 2
Training loss: 2.955742120742798
Validation loss: 2.6886435747146606

Epoch: 6| Step: 3
Training loss: 2.6551170349121094
Validation loss: 2.6969246864318848

Epoch: 6| Step: 4
Training loss: 3.0254294872283936
Validation loss: 2.696270982424418

Epoch: 6| Step: 5
Training loss: 2.305872917175293
Validation loss: 2.683394273122152

Epoch: 6| Step: 6
Training loss: 1.8621762990951538
Validation loss: 2.6773096720377603

Epoch: 6| Step: 7
Training loss: 3.3534085750579834
Validation loss: 2.673694610595703

Epoch: 6| Step: 8
Training loss: 3.4436697959899902
Validation loss: 2.6700746218363443

Epoch: 6| Step: 9
Training loss: 2.8500869274139404
Validation loss: 2.6652112007141113

Epoch: 6| Step: 10
Training loss: 2.7950730323791504
Validation loss: 2.6616161664326987

Epoch: 6| Step: 11
Training loss: 2.3672094345092773
Validation loss: 2.6588094234466553

Epoch: 6| Step: 12
Training loss: 3.266986846923828
Validation loss: 2.655763804912567

Epoch: 6| Step: 13
Training loss: 3.2169902324676514
Validation loss: 2.6534140507380166

Epoch: 38| Step: 0
Training loss: 3.1510567665100098
Validation loss: 2.6497344970703125

Epoch: 6| Step: 1
Training loss: 2.9699697494506836
Validation loss: 2.645376960436503

Epoch: 6| Step: 2
Training loss: 2.881510019302368
Validation loss: 2.640868902206421

Epoch: 6| Step: 3
Training loss: 3.751617431640625
Validation loss: 2.636891722679138

Epoch: 6| Step: 4
Training loss: 2.663257598876953
Validation loss: 2.6340847412745156

Epoch: 6| Step: 5
Training loss: 2.4610540866851807
Validation loss: 2.630249500274658

Epoch: 6| Step: 6
Training loss: 2.7465474605560303
Validation loss: 2.6264222462972007

Epoch: 6| Step: 7
Training loss: 2.4836621284484863
Validation loss: 2.6229436794916787

Epoch: 6| Step: 8
Training loss: 3.0216708183288574
Validation loss: 2.6196388006210327

Epoch: 6| Step: 9
Training loss: 2.8464221954345703
Validation loss: 2.6158623894055686

Epoch: 6| Step: 10
Training loss: 2.4949228763580322
Validation loss: 2.6132278045018515

Epoch: 6| Step: 11
Training loss: 2.8519392013549805
Validation loss: 2.6100213130315146

Epoch: 6| Step: 12
Training loss: 2.363463878631592
Validation loss: 2.6059017976125083

Epoch: 6| Step: 13
Training loss: 2.9391727447509766
Validation loss: 2.6028290589650473

Epoch: 39| Step: 0
Training loss: 2.749748706817627
Validation loss: 2.5996907154719033

Epoch: 6| Step: 1
Training loss: 2.8302745819091797
Validation loss: 2.5955649614334106

Epoch: 6| Step: 2
Training loss: 2.8433117866516113
Validation loss: 2.591831843058268

Epoch: 6| Step: 3
Training loss: 2.662010669708252
Validation loss: 2.5890949765841165

Epoch: 6| Step: 4
Training loss: 2.5588278770446777
Validation loss: 2.5856306552886963

Epoch: 6| Step: 5
Training loss: 3.089240074157715
Validation loss: 2.5827616453170776

Epoch: 6| Step: 6
Training loss: 2.4951367378234863
Validation loss: 2.5791175365448

Epoch: 6| Step: 7
Training loss: 2.493499279022217
Validation loss: 2.5773048400878906

Epoch: 6| Step: 8
Training loss: 2.6378116607666016
Validation loss: 2.574897368748983

Epoch: 6| Step: 9
Training loss: 3.0390465259552
Validation loss: 2.571824073791504

Epoch: 6| Step: 10
Training loss: 2.399141311645508
Validation loss: 2.568562070528666

Epoch: 6| Step: 11
Training loss: 2.8963921070098877
Validation loss: 2.565992752710978

Epoch: 6| Step: 12
Training loss: 2.5476698875427246
Validation loss: 2.562312682469686

Epoch: 6| Step: 13
Training loss: 3.6685166358947754
Validation loss: 2.5600061813990274

Epoch: 40| Step: 0
Training loss: 2.5280487537384033
Validation loss: 2.5564201871554055

Epoch: 6| Step: 1
Training loss: 2.908055067062378
Validation loss: 2.5535730918248496

Epoch: 6| Step: 2
Training loss: 3.1916604042053223
Validation loss: 2.5509841243426004

Epoch: 6| Step: 3
Training loss: 2.500821590423584
Validation loss: 2.547742486000061

Epoch: 6| Step: 4
Training loss: 2.1317126750946045
Validation loss: 2.544246951738993

Epoch: 6| Step: 5
Training loss: 3.1238605976104736
Validation loss: 2.5418095191319785

Epoch: 6| Step: 6
Training loss: 3.3487019538879395
Validation loss: 2.5379521449406943

Epoch: 6| Step: 7
Training loss: 1.7525489330291748
Validation loss: 2.5354586442311606

Epoch: 6| Step: 8
Training loss: 3.226642608642578
Validation loss: 2.532184878985087

Epoch: 6| Step: 9
Training loss: 2.9568722248077393
Validation loss: 2.5288668870925903

Epoch: 6| Step: 10
Training loss: 2.7212657928466797
Validation loss: 2.5262120167414346

Epoch: 6| Step: 11
Training loss: 2.8031983375549316
Validation loss: 2.5236313740412393

Epoch: 6| Step: 12
Training loss: 2.12666392326355
Validation loss: 2.5201438268025718

Epoch: 6| Step: 13
Training loss: 2.9615087509155273
Validation loss: 2.5174649755160012

Epoch: 41| Step: 0
Training loss: 2.456447124481201
Validation loss: 2.513774871826172

Epoch: 6| Step: 1
Training loss: 2.9052233695983887
Validation loss: 2.5088916619618735

Epoch: 6| Step: 2
Training loss: 2.631032705307007
Validation loss: 2.5079688231150308

Epoch: 6| Step: 3
Training loss: 2.511183261871338
Validation loss: 2.503411034742991

Epoch: 6| Step: 4
Training loss: 2.2825734615325928
Validation loss: 2.5013795693715415

Epoch: 6| Step: 5
Training loss: 2.3784801959991455
Validation loss: 2.4984957774480185

Epoch: 6| Step: 6
Training loss: 3.2275772094726562
Validation loss: 2.4954041242599487

Epoch: 6| Step: 7
Training loss: 2.6272051334381104
Validation loss: 2.493442714214325

Epoch: 6| Step: 8
Training loss: 2.8420891761779785
Validation loss: 2.4902357856432595

Epoch: 6| Step: 9
Training loss: 3.0126447677612305
Validation loss: 2.487720171610514

Epoch: 6| Step: 10
Training loss: 3.1033132076263428
Validation loss: 2.48525333404541

Epoch: 6| Step: 11
Training loss: 2.629978895187378
Validation loss: 2.4810076157251992

Epoch: 6| Step: 12
Training loss: 2.2220048904418945
Validation loss: 2.476010243097941

Epoch: 6| Step: 13
Training loss: 2.819639205932617
Validation loss: 2.4715375502904258

Epoch: 42| Step: 0
Training loss: 2.2719695568084717
Validation loss: 2.468141039212545

Epoch: 6| Step: 1
Training loss: 2.3507344722747803
Validation loss: 2.4664065837860107

Epoch: 6| Step: 2
Training loss: 2.181443691253662
Validation loss: 2.4592873056729636

Epoch: 6| Step: 3
Training loss: 2.745051145553589
Validation loss: 2.4594072898228965

Epoch: 6| Step: 4
Training loss: 2.4787354469299316
Validation loss: 2.457994520664215

Epoch: 6| Step: 5
Training loss: 2.5196337699890137
Validation loss: 2.4510088364283242

Epoch: 6| Step: 6
Training loss: 2.6366970539093018
Validation loss: 2.4473329385121665

Epoch: 6| Step: 7
Training loss: 3.06209397315979
Validation loss: 2.447994033495585

Epoch: 6| Step: 8
Training loss: 2.092311143875122
Validation loss: 2.4485884308815002

Epoch: 6| Step: 9
Training loss: 2.8109846115112305
Validation loss: 2.4484750429789224

Epoch: 6| Step: 10
Training loss: 3.6755154132843018
Validation loss: 2.4499348402023315

Epoch: 6| Step: 11
Training loss: 2.299020290374756
Validation loss: 2.448579748471578

Epoch: 6| Step: 12
Training loss: 2.6219255924224854
Validation loss: 2.44837216536204

Epoch: 6| Step: 13
Training loss: 3.288863182067871
Validation loss: 2.4461272160212197

Epoch: 43| Step: 0
Training loss: 3.416259288787842
Validation loss: 2.439069151878357

Epoch: 6| Step: 1
Training loss: 3.089186668395996
Validation loss: 2.432274083296458

Epoch: 6| Step: 2
Training loss: 1.9746909141540527
Validation loss: 2.428385575612386

Epoch: 6| Step: 3
Training loss: 2.4977967739105225
Validation loss: 2.4244407415390015

Epoch: 6| Step: 4
Training loss: 2.443740129470825
Validation loss: 2.416920324166616

Epoch: 6| Step: 5
Training loss: 2.31797456741333
Validation loss: 2.4131948153177896

Epoch: 6| Step: 6
Training loss: 2.595181465148926
Validation loss: 2.4079803625742593

Epoch: 6| Step: 7
Training loss: 2.480820894241333
Validation loss: 2.4042219320933023

Epoch: 6| Step: 8
Training loss: 2.4630126953125
Validation loss: 2.40374364455541

Epoch: 6| Step: 9
Training loss: 2.629258632659912
Validation loss: 2.400053024291992

Epoch: 6| Step: 10
Training loss: 2.716977596282959
Validation loss: 2.393775145212809

Epoch: 6| Step: 11
Training loss: 2.3157460689544678
Validation loss: 2.3906767765680947

Epoch: 6| Step: 12
Training loss: 2.804236888885498
Validation loss: 2.3863124450047812

Epoch: 6| Step: 13
Training loss: 2.5586886405944824
Validation loss: 2.3852572043736777

Epoch: 44| Step: 0
Training loss: 2.2037479877471924
Validation loss: 2.3841509421666465

Epoch: 6| Step: 1
Training loss: 3.315558671951294
Validation loss: 2.379423956076304

Epoch: 6| Step: 2
Training loss: 2.6487646102905273
Validation loss: 2.3771747748057046

Epoch: 6| Step: 3
Training loss: 2.531421184539795
Validation loss: 2.37768284479777

Epoch: 6| Step: 4
Training loss: 2.151604175567627
Validation loss: 2.3762471278508506

Epoch: 6| Step: 5
Training loss: 2.0586676597595215
Validation loss: 2.3720179398854575

Epoch: 6| Step: 6
Training loss: 3.0474557876586914
Validation loss: 2.3717079162597656

Epoch: 6| Step: 7
Training loss: 2.694598436355591
Validation loss: 2.367141286532084

Epoch: 6| Step: 8
Training loss: 2.4098000526428223
Validation loss: 2.366745193799337

Epoch: 6| Step: 9
Training loss: 2.621586799621582
Validation loss: 2.363737861315409

Epoch: 6| Step: 10
Training loss: 1.8489179611206055
Validation loss: 2.3608248233795166

Epoch: 6| Step: 11
Training loss: 2.919804096221924
Validation loss: 2.35485577583313

Epoch: 6| Step: 12
Training loss: 2.678009033203125
Validation loss: 2.3517884413401284

Epoch: 6| Step: 13
Training loss: 2.5665454864501953
Validation loss: 2.3454819122950235

Epoch: 45| Step: 0
Training loss: 2.496584415435791
Validation loss: 2.3412474195162454

Epoch: 6| Step: 1
Training loss: 2.478658437728882
Validation loss: 2.336748560269674

Epoch: 6| Step: 2
Training loss: 2.022674083709717
Validation loss: 2.3439455231030784

Epoch: 6| Step: 3
Training loss: 2.3784923553466797
Validation loss: 2.3509501814842224

Epoch: 6| Step: 4
Training loss: 2.414876937866211
Validation loss: 2.3379615545272827

Epoch: 6| Step: 5
Training loss: 2.2133047580718994
Validation loss: 2.326208233833313

Epoch: 6| Step: 6
Training loss: 2.689765691757202
Validation loss: 2.3209723432858786

Epoch: 6| Step: 7
Training loss: 2.8138010501861572
Validation loss: 2.3218690554300943

Epoch: 6| Step: 8
Training loss: 2.6644182205200195
Validation loss: 2.316965421040853

Epoch: 6| Step: 9
Training loss: 2.7690815925598145
Validation loss: 2.3204299211502075

Epoch: 6| Step: 10
Training loss: 2.5067834854125977
Validation loss: 2.321202794710795

Epoch: 6| Step: 11
Training loss: 2.4688374996185303
Validation loss: 2.318763514359792

Epoch: 6| Step: 12
Training loss: 2.4199905395507812
Validation loss: 2.3164449334144592

Epoch: 6| Step: 13
Training loss: 2.740410804748535
Validation loss: 2.316041668256124

Epoch: 46| Step: 0
Training loss: 3.099858045578003
Validation loss: 2.3102904756863913

Epoch: 6| Step: 1
Training loss: 2.586061954498291
Validation loss: 2.3103752930959067

Epoch: 6| Step: 2
Training loss: 2.155830144882202
Validation loss: 2.3057138125101724

Epoch: 6| Step: 3
Training loss: 2.0052294731140137
Validation loss: 2.301809092362722

Epoch: 6| Step: 4
Training loss: 2.0574190616607666
Validation loss: 2.2966968615849814

Epoch: 6| Step: 5
Training loss: 2.462707996368408
Validation loss: 2.298067649205526

Epoch: 6| Step: 6
Training loss: 2.591750144958496
Validation loss: 2.287086069583893

Epoch: 6| Step: 7
Training loss: 2.1747920513153076
Validation loss: 2.2907944917678833

Epoch: 6| Step: 8
Training loss: 2.2298007011413574
Validation loss: 2.2962313493092856

Epoch: 6| Step: 9
Training loss: 2.862945079803467
Validation loss: 2.299870252609253

Epoch: 6| Step: 10
Training loss: 2.445862293243408
Validation loss: 2.290325323740641

Epoch: 6| Step: 11
Training loss: 3.0003252029418945
Validation loss: 2.2793776392936707

Epoch: 6| Step: 12
Training loss: 2.5777673721313477
Validation loss: 2.272881289323171

Epoch: 6| Step: 13
Training loss: 2.309555768966675
Validation loss: 2.2703130642573037

Epoch: 47| Step: 0
Training loss: 1.799911618232727
Validation loss: 2.2672340472539267

Epoch: 6| Step: 1
Training loss: 2.969235420227051
Validation loss: 2.2650131781895957

Epoch: 6| Step: 2
Training loss: 2.4415667057037354
Validation loss: 2.268335223197937

Epoch: 6| Step: 3
Training loss: 2.3563427925109863
Validation loss: 2.2686781088511148

Epoch: 6| Step: 4
Training loss: 2.4430222511291504
Validation loss: 2.260902444521586

Epoch: 6| Step: 5
Training loss: 3.3302054405212402
Validation loss: 2.257718781630198

Epoch: 6| Step: 6
Training loss: 2.299793004989624
Validation loss: 2.255851666132609

Epoch: 6| Step: 7
Training loss: 2.242504358291626
Validation loss: 2.252526581287384

Epoch: 6| Step: 8
Training loss: 2.369762659072876
Validation loss: 2.252649744351705

Epoch: 6| Step: 9
Training loss: 2.2219901084899902
Validation loss: 2.24552059173584

Epoch: 6| Step: 10
Training loss: 2.544285774230957
Validation loss: 2.244645893573761

Epoch: 6| Step: 11
Training loss: 2.0354576110839844
Validation loss: 2.241327087084452

Epoch: 6| Step: 12
Training loss: 2.534337043762207
Validation loss: 2.236970822016398

Epoch: 6| Step: 13
Training loss: 2.370713710784912
Validation loss: 2.2384166320165

Epoch: 48| Step: 0
Training loss: 2.6842150688171387
Validation loss: 2.233765482902527

Epoch: 6| Step: 1
Training loss: 1.9188288450241089
Validation loss: 2.2303348382314048

Epoch: 6| Step: 2
Training loss: 2.898789405822754
Validation loss: 2.22383322318395

Epoch: 6| Step: 3
Training loss: 2.2808780670166016
Validation loss: 2.2201930483182273

Epoch: 6| Step: 4
Training loss: 2.680266857147217
Validation loss: 2.2256397207578025

Epoch: 6| Step: 5
Training loss: 2.3446784019470215
Validation loss: 2.2201627691586814

Epoch: 6| Step: 6
Training loss: 2.4806087017059326
Validation loss: 2.2149717013041177

Epoch: 6| Step: 7
Training loss: 2.0440874099731445
Validation loss: 2.2131400903066

Epoch: 6| Step: 8
Training loss: 2.50736403465271
Validation loss: 2.213352998097738

Epoch: 6| Step: 9
Training loss: 2.4467639923095703
Validation loss: 2.2133268117904663

Epoch: 6| Step: 10
Training loss: 2.3361682891845703
Validation loss: 2.2138914267222085

Epoch: 6| Step: 11
Training loss: 2.671236038208008
Validation loss: 2.2110661268234253

Epoch: 6| Step: 12
Training loss: 2.045400619506836
Validation loss: 2.2072019577026367

Epoch: 6| Step: 13
Training loss: 2.0888190269470215
Validation loss: 2.2108686168988547

Epoch: 49| Step: 0
Training loss: 2.8969082832336426
Validation loss: 2.207882364590963

Epoch: 6| Step: 1
Training loss: 2.2032742500305176
Validation loss: 2.2014350295066833

Epoch: 6| Step: 2
Training loss: 2.049210548400879
Validation loss: 2.201882322629293

Epoch: 6| Step: 3
Training loss: 2.1395328044891357
Validation loss: 2.1986436446507773

Epoch: 6| Step: 4
Training loss: 2.4220595359802246
Validation loss: 2.1956740021705627

Epoch: 6| Step: 5
Training loss: 2.175877809524536
Validation loss: 2.1976804733276367

Epoch: 6| Step: 6
Training loss: 2.739076852798462
Validation loss: 2.192580838998159

Epoch: 6| Step: 7
Training loss: 2.3134546279907227
Validation loss: 2.188773453235626

Epoch: 6| Step: 8
Training loss: 2.182684898376465
Validation loss: 2.1852457523345947

Epoch: 6| Step: 9
Training loss: 2.1538853645324707
Validation loss: 2.184467335542043

Epoch: 6| Step: 10
Training loss: 2.517526865005493
Validation loss: 2.1765233278274536

Epoch: 6| Step: 11
Training loss: 2.188223361968994
Validation loss: 2.1784335573514304

Epoch: 6| Step: 12
Training loss: 2.845036268234253
Validation loss: 2.182887236277262

Epoch: 6| Step: 13
Training loss: 2.179973602294922
Validation loss: 2.196109672387441

Epoch: 50| Step: 0
Training loss: 1.9490313529968262
Validation loss: 2.193138281504313

Epoch: 6| Step: 1
Training loss: 2.8018345832824707
Validation loss: 2.1848873694737754

Epoch: 6| Step: 2
Training loss: 2.7518105506896973
Validation loss: 2.167657415072123

Epoch: 6| Step: 3
Training loss: 2.895207405090332
Validation loss: 2.163881162802378

Epoch: 6| Step: 4
Training loss: 2.1888163089752197
Validation loss: 2.164795915285746

Epoch: 6| Step: 5
Training loss: 2.122257947921753
Validation loss: 2.174004932244619

Epoch: 6| Step: 6
Training loss: 1.8693290948867798
Validation loss: 2.1705410877863565

Epoch: 6| Step: 7
Training loss: 2.805286407470703
Validation loss: 2.1744867960611978

Epoch: 6| Step: 8
Training loss: 2.2156243324279785
Validation loss: 2.1753673950831094

Epoch: 6| Step: 9
Training loss: 2.4236671924591064
Validation loss: 2.183697283267975

Epoch: 6| Step: 10
Training loss: 2.2113895416259766
Validation loss: 2.1768799821535745

Epoch: 6| Step: 11
Training loss: 2.3172075748443604
Validation loss: 2.176365594069163

Epoch: 6| Step: 12
Training loss: 1.9620883464813232
Validation loss: 2.1759766936302185

Epoch: 6| Step: 13
Training loss: 2.4271130561828613
Validation loss: 2.168425997098287

Epoch: 51| Step: 0
Training loss: 2.4899845123291016
Validation loss: 2.1685991088549295

Epoch: 6| Step: 1
Training loss: 2.3921990394592285
Validation loss: 2.1657350262006125

Epoch: 6| Step: 2
Training loss: 2.470170021057129
Validation loss: 2.1644442478815713

Epoch: 6| Step: 3
Training loss: 2.1590356826782227
Validation loss: 2.1635824839274087

Epoch: 6| Step: 4
Training loss: 2.045027732849121
Validation loss: 2.1598199804623923

Epoch: 6| Step: 5
Training loss: 1.9714248180389404
Validation loss: 2.1560223499933877

Epoch: 6| Step: 6
Training loss: 2.265955686569214
Validation loss: 2.1496363083521524

Epoch: 6| Step: 7
Training loss: 2.37339448928833
Validation loss: 2.1473851005236306

Epoch: 6| Step: 8
Training loss: 2.5677309036254883
Validation loss: 2.1406095822652182

Epoch: 6| Step: 9
Training loss: 2.312185287475586
Validation loss: 2.1409593423207602

Epoch: 6| Step: 10
Training loss: 2.1351561546325684
Validation loss: 2.14847997824351

Epoch: 6| Step: 11
Training loss: 2.5355844497680664
Validation loss: 2.1397927006085715

Epoch: 6| Step: 12
Training loss: 2.3230228424072266
Validation loss: 2.1384257674217224

Epoch: 6| Step: 13
Training loss: 2.368533134460449
Validation loss: 2.1444393595059714

Epoch: 52| Step: 0
Training loss: 2.1795735359191895
Validation loss: 2.142761508623759

Epoch: 6| Step: 1
Training loss: 1.9168041944503784
Validation loss: 2.135641554991404

Epoch: 6| Step: 2
Training loss: 2.4241695404052734
Validation loss: 2.1369575460751853

Epoch: 6| Step: 3
Training loss: 2.9679954051971436
Validation loss: 2.1287299195925393

Epoch: 6| Step: 4
Training loss: 2.2603793144226074
Validation loss: 2.1346843242645264

Epoch: 6| Step: 5
Training loss: 2.7545905113220215
Validation loss: 2.1355069080988565

Epoch: 6| Step: 6
Training loss: 2.185323476791382
Validation loss: 2.138484477996826

Epoch: 6| Step: 7
Training loss: 2.0172698497772217
Validation loss: 2.136750340461731

Epoch: 6| Step: 8
Training loss: 1.7266383171081543
Validation loss: 2.1330591440200806

Epoch: 6| Step: 9
Training loss: 2.052205801010132
Validation loss: 2.138017495473226

Epoch: 6| Step: 10
Training loss: 1.9721028804779053
Validation loss: 2.1382156213124595

Epoch: 6| Step: 11
Training loss: 2.5949325561523438
Validation loss: 2.1316813826560974

Epoch: 6| Step: 12
Training loss: 3.137420892715454
Validation loss: 2.1304892698923745

Epoch: 6| Step: 13
Training loss: 2.1804370880126953
Validation loss: 2.1222991744677224

Epoch: 53| Step: 0
Training loss: 2.637857675552368
Validation loss: 2.117100179195404

Epoch: 6| Step: 1
Training loss: 2.4196057319641113
Validation loss: 2.121447662512461

Epoch: 6| Step: 2
Training loss: 2.520033597946167
Validation loss: 2.118992726008097

Epoch: 6| Step: 3
Training loss: 1.5865002870559692
Validation loss: 2.1123473048210144

Epoch: 6| Step: 4
Training loss: 2.162492275238037
Validation loss: 2.1057621041933694

Epoch: 6| Step: 5
Training loss: 2.745976686477661
Validation loss: 2.1111047069231668

Epoch: 6| Step: 6
Training loss: 2.445500612258911
Validation loss: 2.107480009396871

Epoch: 6| Step: 7
Training loss: 3.126476287841797
Validation loss: 2.105960965156555

Epoch: 6| Step: 8
Training loss: 1.4917776584625244
Validation loss: 2.1066407759984336

Epoch: 6| Step: 9
Training loss: 2.4320592880249023
Validation loss: 2.103009521961212

Epoch: 6| Step: 10
Training loss: 1.6872668266296387
Validation loss: 2.1031126578648887

Epoch: 6| Step: 11
Training loss: 2.529747486114502
Validation loss: 2.10302205880483

Epoch: 6| Step: 12
Training loss: 1.7159204483032227
Validation loss: 2.103825648625692

Epoch: 6| Step: 13
Training loss: 2.5183565616607666
Validation loss: 2.1019318103790283

Epoch: 54| Step: 0
Training loss: 2.438027858734131
Validation loss: 2.098904768625895

Epoch: 6| Step: 1
Training loss: 2.3790245056152344
Validation loss: 2.09824146827062

Epoch: 6| Step: 2
Training loss: 2.1039977073669434
Validation loss: 2.1026230851809182

Epoch: 6| Step: 3
Training loss: 1.8500804901123047
Validation loss: 2.102226734161377

Epoch: 6| Step: 4
Training loss: 2.804734230041504
Validation loss: 2.1002318064371743

Epoch: 6| Step: 5
Training loss: 1.866227626800537
Validation loss: 2.1016010642051697

Epoch: 6| Step: 6
Training loss: 2.0880932807922363
Validation loss: 2.0993025501569114

Epoch: 6| Step: 7
Training loss: 2.8692290782928467
Validation loss: 2.1004889607429504

Epoch: 6| Step: 8
Training loss: 2.6144938468933105
Validation loss: 2.09870982170105

Epoch: 6| Step: 9
Training loss: 2.652008295059204
Validation loss: 2.0941338340441384

Epoch: 6| Step: 10
Training loss: 1.672790288925171
Validation loss: 2.090988516807556

Epoch: 6| Step: 11
Training loss: 2.2135684490203857
Validation loss: 2.0937355359395347

Epoch: 6| Step: 12
Training loss: 2.00009822845459
Validation loss: 2.093032737572988

Epoch: 6| Step: 13
Training loss: 2.2115108966827393
Validation loss: 2.0768436789512634

Epoch: 55| Step: 0
Training loss: 1.5250006914138794
Validation loss: 2.0858708222707114

Epoch: 6| Step: 1
Training loss: 1.8287582397460938
Validation loss: 2.0952258904774985

Epoch: 6| Step: 2
Training loss: 1.9790289402008057
Validation loss: 2.08012326558431

Epoch: 6| Step: 3
Training loss: 2.987173080444336
Validation loss: 2.076408882935842

Epoch: 6| Step: 4
Training loss: 2.6785128116607666
Validation loss: 2.0756003061930337

Epoch: 6| Step: 5
Training loss: 1.8715016841888428
Validation loss: 2.0774617989857993

Epoch: 6| Step: 6
Training loss: 1.9123973846435547
Validation loss: 2.0774844884872437

Epoch: 6| Step: 7
Training loss: 2.3219668865203857
Validation loss: 2.0758675734202066

Epoch: 6| Step: 8
Training loss: 2.2487926483154297
Validation loss: 2.0788795550664267

Epoch: 6| Step: 9
Training loss: 2.507910966873169
Validation loss: 2.080610533555349

Epoch: 6| Step: 10
Training loss: 2.1157102584838867
Validation loss: 2.0824585358301797

Epoch: 6| Step: 11
Training loss: 2.72003436088562
Validation loss: 2.084854543209076

Epoch: 6| Step: 12
Training loss: 2.583527088165283
Validation loss: 2.0893820921579995

Epoch: 6| Step: 13
Training loss: 2.3649680614471436
Validation loss: 2.079197188218435

Epoch: 56| Step: 0
Training loss: 2.1713433265686035
Validation loss: 2.0806567668914795

Epoch: 6| Step: 1
Training loss: 2.1875219345092773
Validation loss: 2.071871042251587

Epoch: 6| Step: 2
Training loss: 2.441358804702759
Validation loss: 2.0745529731114707

Epoch: 6| Step: 3
Training loss: 2.027430534362793
Validation loss: 2.0679999788602195

Epoch: 6| Step: 4
Training loss: 2.2880029678344727
Validation loss: 2.067083239555359

Epoch: 6| Step: 5
Training loss: 2.4931435585021973
Validation loss: 2.0632837216059365

Epoch: 6| Step: 6
Training loss: 2.1922237873077393
Validation loss: 2.0620349248250327

Epoch: 6| Step: 7
Training loss: 2.0782008171081543
Validation loss: 2.061718205610911

Epoch: 6| Step: 8
Training loss: 2.20877742767334
Validation loss: 2.0607714851697287

Epoch: 6| Step: 9
Training loss: 1.5027492046356201
Validation loss: 2.058825453122457

Epoch: 6| Step: 10
Training loss: 2.5968971252441406
Validation loss: 2.06560226281484

Epoch: 6| Step: 11
Training loss: 2.004828929901123
Validation loss: 2.0828675429026284

Epoch: 6| Step: 12
Training loss: 2.3165714740753174
Validation loss: 2.0772854884465537

Epoch: 6| Step: 13
Training loss: 2.9031901359558105
Validation loss: 2.072406589984894

Epoch: 57| Step: 0
Training loss: 2.83217716217041
Validation loss: 2.059940675894419

Epoch: 6| Step: 1
Training loss: 2.3147294521331787
Validation loss: 2.055817504723867

Epoch: 6| Step: 2
Training loss: 2.7210254669189453
Validation loss: 2.0543664693832397

Epoch: 6| Step: 3
Training loss: 1.9538393020629883
Validation loss: 2.0558581948280334

Epoch: 6| Step: 4
Training loss: 2.675645351409912
Validation loss: 2.0641372402509055

Epoch: 6| Step: 5
Training loss: 2.017110824584961
Validation loss: 2.0692509015401206

Epoch: 6| Step: 6
Training loss: 2.231605052947998
Validation loss: 2.0694750944773355

Epoch: 6| Step: 7
Training loss: 1.7486878633499146
Validation loss: 2.073784093062083

Epoch: 6| Step: 8
Training loss: 2.457627058029175
Validation loss: 2.0731340249379477

Epoch: 6| Step: 9
Training loss: 2.0415804386138916
Validation loss: 2.0732510685920715

Epoch: 6| Step: 10
Training loss: 1.601456642150879
Validation loss: 2.0778274734814963

Epoch: 6| Step: 11
Training loss: 2.5567116737365723
Validation loss: 2.0744497776031494

Epoch: 6| Step: 12
Training loss: 2.1247854232788086
Validation loss: 2.0810473362604776

Epoch: 6| Step: 13
Training loss: 2.2595109939575195
Validation loss: 2.0734005769093833

Epoch: 58| Step: 0
Training loss: 2.8618993759155273
Validation loss: 2.065569758415222

Epoch: 6| Step: 1
Training loss: 1.9255077838897705
Validation loss: 2.058106283346812

Epoch: 6| Step: 2
Training loss: 2.399020195007324
Validation loss: 2.0621995528539023

Epoch: 6| Step: 3
Training loss: 1.4542152881622314
Validation loss: 2.057728588581085

Epoch: 6| Step: 4
Training loss: 2.0549111366271973
Validation loss: 2.058572689692179

Epoch: 6| Step: 5
Training loss: 2.0229568481445312
Validation loss: 2.051758329073588

Epoch: 6| Step: 6
Training loss: 2.054318904876709
Validation loss: 2.050203522046407

Epoch: 6| Step: 7
Training loss: 2.570672035217285
Validation loss: 2.0436978141466775

Epoch: 6| Step: 8
Training loss: 2.32383394241333
Validation loss: 2.0461233854293823

Epoch: 6| Step: 9
Training loss: 2.710221767425537
Validation loss: 2.058698832988739

Epoch: 6| Step: 10
Training loss: 1.8122057914733887
Validation loss: 2.056722561518351

Epoch: 6| Step: 11
Training loss: 1.5189787149429321
Validation loss: 2.093476891517639

Epoch: 6| Step: 12
Training loss: 2.898728370666504
Validation loss: 2.1091989278793335

Epoch: 6| Step: 13
Training loss: 2.4926962852478027
Validation loss: 2.127476374308268

Epoch: 59| Step: 0
Training loss: 1.6672968864440918
Validation loss: 2.158054749170939

Epoch: 6| Step: 1
Training loss: 2.1590962409973145
Validation loss: 2.129092574119568

Epoch: 6| Step: 2
Training loss: 1.866148829460144
Validation loss: 2.11673895517985

Epoch: 6| Step: 3
Training loss: 2.9225375652313232
Validation loss: 2.0778916676839194

Epoch: 6| Step: 4
Training loss: 1.8232728242874146
Validation loss: 2.0633095701535544

Epoch: 6| Step: 5
Training loss: 2.4665889739990234
Validation loss: 2.091417054335276

Epoch: 6| Step: 6
Training loss: 2.5498571395874023
Validation loss: 2.112752318382263

Epoch: 6| Step: 7
Training loss: 2.1031670570373535
Validation loss: 2.132134755452474

Epoch: 6| Step: 8
Training loss: 2.8874337673187256
Validation loss: 2.1441863775253296

Epoch: 6| Step: 9
Training loss: 2.4828543663024902
Validation loss: 2.1635164817174277

Epoch: 6| Step: 10
Training loss: 2.094458818435669
Validation loss: 2.157531976699829

Epoch: 6| Step: 11
Training loss: 2.09505558013916
Validation loss: 2.1507246096928916

Epoch: 6| Step: 12
Training loss: 2.550130844116211
Validation loss: 2.1101392904917398

Epoch: 6| Step: 13
Training loss: 2.63124680519104
Validation loss: 2.076475520928701

Epoch: 60| Step: 0
Training loss: 2.1619796752929688
Validation loss: 2.059300720691681

Epoch: 6| Step: 1
Training loss: 1.8165850639343262
Validation loss: 2.047545870145162

Epoch: 6| Step: 2
Training loss: 2.8295304775238037
Validation loss: 2.043426275253296

Epoch: 6| Step: 3
Training loss: 2.162494659423828
Validation loss: 2.0360753933588662

Epoch: 6| Step: 4
Training loss: 1.9571822881698608
Validation loss: 2.0328721602757773

Epoch: 6| Step: 5
Training loss: 2.183039903640747
Validation loss: 2.0345375140508017

Epoch: 6| Step: 6
Training loss: 2.0214109420776367
Validation loss: 2.0342486103375754

Epoch: 6| Step: 7
Training loss: 2.183061122894287
Validation loss: 2.0459529757499695

Epoch: 6| Step: 8
Training loss: 2.920962333679199
Validation loss: 2.0574699441591897

Epoch: 6| Step: 9
Training loss: 1.8451831340789795
Validation loss: 2.04995067914327

Epoch: 6| Step: 10
Training loss: 2.07108736038208
Validation loss: 2.058754324913025

Epoch: 6| Step: 11
Training loss: 2.53117036819458
Validation loss: 2.0542608300844827

Epoch: 6| Step: 12
Training loss: 2.3922581672668457
Validation loss: 2.055935780207316

Epoch: 6| Step: 13
Training loss: 2.077261209487915
Validation loss: 2.0787754456202188

Epoch: 61| Step: 0
Training loss: 1.9914076328277588
Validation loss: 2.073978364467621

Epoch: 6| Step: 1
Training loss: 2.436990737915039
Validation loss: 2.0613907972971597

Epoch: 6| Step: 2
Training loss: 2.0548319816589355
Validation loss: 2.052597006162008

Epoch: 6| Step: 3
Training loss: 2.27522611618042
Validation loss: 2.039519190788269

Epoch: 6| Step: 4
Training loss: 2.4211037158966064
Validation loss: 2.024273912111918

Epoch: 6| Step: 5
Training loss: 2.2346763610839844
Validation loss: 2.024857302506765

Epoch: 6| Step: 6
Training loss: 2.360555648803711
Validation loss: 2.0242532094319663

Epoch: 6| Step: 7
Training loss: 2.341358184814453
Validation loss: 2.0321319897969565

Epoch: 6| Step: 8
Training loss: 2.517940044403076
Validation loss: 2.039251446723938

Epoch: 6| Step: 9
Training loss: 2.1600682735443115
Validation loss: 2.040866712729136

Epoch: 6| Step: 10
Training loss: 1.9694263935089111
Validation loss: 2.0377995371818542

Epoch: 6| Step: 11
Training loss: 2.0718019008636475
Validation loss: 2.044558862845103

Epoch: 6| Step: 12
Training loss: 1.9422487020492554
Validation loss: 2.034035563468933

Epoch: 6| Step: 13
Training loss: 2.534398078918457
Validation loss: 2.035456875960032

Epoch: 62| Step: 0
Training loss: 2.7601675987243652
Validation loss: 2.031129459540049

Epoch: 6| Step: 1
Training loss: 2.0505709648132324
Validation loss: 2.026787062486013

Epoch: 6| Step: 2
Training loss: 2.350205898284912
Validation loss: 2.0301795999209085

Epoch: 6| Step: 3
Training loss: 2.2348742485046387
Validation loss: 2.024225433667501

Epoch: 6| Step: 4
Training loss: 2.437671661376953
Validation loss: 2.0241075356801352

Epoch: 6| Step: 5
Training loss: 1.451829433441162
Validation loss: 2.022518773873647

Epoch: 6| Step: 6
Training loss: 2.4723920822143555
Validation loss: 2.027721881866455

Epoch: 6| Step: 7
Training loss: 1.9719316959381104
Validation loss: 2.0338494578997293

Epoch: 6| Step: 8
Training loss: 1.804657220840454
Validation loss: 2.030657152334849

Epoch: 6| Step: 9
Training loss: 2.0150949954986572
Validation loss: 2.033195714155833

Epoch: 6| Step: 10
Training loss: 2.061587333679199
Validation loss: 2.038350601991018

Epoch: 6| Step: 11
Training loss: 2.499384880065918
Validation loss: 2.0432185928026834

Epoch: 6| Step: 12
Training loss: 2.356680393218994
Validation loss: 2.0246885220209756

Epoch: 6| Step: 13
Training loss: 2.5817065238952637
Validation loss: 2.013792256514231

Epoch: 63| Step: 0
Training loss: 2.6970763206481934
Validation loss: 2.023302416006724

Epoch: 6| Step: 1
Training loss: 2.4410810470581055
Validation loss: 2.0289005041122437

Epoch: 6| Step: 2
Training loss: 1.5986278057098389
Validation loss: 2.036788761615753

Epoch: 6| Step: 3
Training loss: 1.7309246063232422
Validation loss: 2.0381291111310325

Epoch: 6| Step: 4
Training loss: 2.7994184494018555
Validation loss: 2.040729522705078

Epoch: 6| Step: 5
Training loss: 2.197784900665283
Validation loss: 2.0410210291544595

Epoch: 6| Step: 6
Training loss: 1.4870768785476685
Validation loss: 2.043077826499939

Epoch: 6| Step: 7
Training loss: 2.677332878112793
Validation loss: 2.040029207865397

Epoch: 6| Step: 8
Training loss: 2.4065089225769043
Validation loss: 2.036138395468394

Epoch: 6| Step: 9
Training loss: 2.5296096801757812
Validation loss: 2.033204217751821

Epoch: 6| Step: 10
Training loss: 1.8830002546310425
Validation loss: 2.032371719678243

Epoch: 6| Step: 11
Training loss: 2.14016056060791
Validation loss: 2.0360233187675476

Epoch: 6| Step: 12
Training loss: 2.1073784828186035
Validation loss: 2.0291412274042764

Epoch: 6| Step: 13
Training loss: 2.3101086616516113
Validation loss: 2.0328328808148703

Epoch: 64| Step: 0
Training loss: 1.6425954103469849
Validation loss: 2.0288759072621665

Epoch: 6| Step: 1
Training loss: 2.482602834701538
Validation loss: 2.025401453177134

Epoch: 6| Step: 2
Training loss: 1.8497966527938843
Validation loss: 2.0307221015294394

Epoch: 6| Step: 3
Training loss: 1.6814169883728027
Validation loss: 2.031875809033712

Epoch: 6| Step: 4
Training loss: 2.028297185897827
Validation loss: 2.0262819131215415

Epoch: 6| Step: 5
Training loss: 1.5911591053009033
Validation loss: 2.0280178984006247

Epoch: 6| Step: 6
Training loss: 2.893711566925049
Validation loss: 2.024210492769877

Epoch: 6| Step: 7
Training loss: 2.315821886062622
Validation loss: 2.0289293328921

Epoch: 6| Step: 8
Training loss: 2.617241144180298
Validation loss: 2.0290164947509766

Epoch: 6| Step: 9
Training loss: 2.0701112747192383
Validation loss: 2.0223188996315002

Epoch: 6| Step: 10
Training loss: 2.171232223510742
Validation loss: 2.0224582751592

Epoch: 6| Step: 11
Training loss: 2.429230213165283
Validation loss: 2.0196327567100525

Epoch: 6| Step: 12
Training loss: 2.2482821941375732
Validation loss: 2.021238923072815

Epoch: 6| Step: 13
Training loss: 2.7393670082092285
Validation loss: 2.0280370712280273

Epoch: 65| Step: 0
Training loss: 2.275761127471924
Validation loss: 2.0339617133140564

Epoch: 6| Step: 1
Training loss: 2.7814688682556152
Validation loss: 2.0374613404273987

Epoch: 6| Step: 2
Training loss: 2.3036038875579834
Validation loss: 2.0276434222857156

Epoch: 6| Step: 3
Training loss: 1.363553524017334
Validation loss: 2.0356234510739646

Epoch: 6| Step: 4
Training loss: 2.0230517387390137
Validation loss: 2.0393019715944924

Epoch: 6| Step: 5
Training loss: 2.197824478149414
Validation loss: 2.041113634904226

Epoch: 6| Step: 6
Training loss: 2.6159987449645996
Validation loss: 2.0673522551854453

Epoch: 6| Step: 7
Training loss: 1.1052639484405518
Validation loss: 2.0732603073120117

Epoch: 6| Step: 8
Training loss: 2.538405656814575
Validation loss: 2.0667486786842346

Epoch: 6| Step: 9
Training loss: 2.766845703125
Validation loss: 2.082007646560669

Epoch: 6| Step: 10
Training loss: 1.6835904121398926
Validation loss: 2.042741894721985

Epoch: 6| Step: 11
Training loss: 2.5355958938598633
Validation loss: 2.039605716864268

Epoch: 6| Step: 12
Training loss: 2.6097750663757324
Validation loss: 2.0302091042200723

Epoch: 6| Step: 13
Training loss: 2.292402744293213
Validation loss: 2.0278555154800415

Epoch: 66| Step: 0
Training loss: 2.8178882598876953
Validation loss: 2.021065334479014

Epoch: 6| Step: 1
Training loss: 1.9087083339691162
Validation loss: 2.0264328718185425

Epoch: 6| Step: 2
Training loss: 2.4194226264953613
Validation loss: 2.0123711427052817

Epoch: 6| Step: 3
Training loss: 2.101914644241333
Validation loss: 2.017787973086039

Epoch: 6| Step: 4
Training loss: 2.5500755310058594
Validation loss: 2.0272504488627114

Epoch: 6| Step: 5
Training loss: 1.6546251773834229
Validation loss: 2.0149205525716147

Epoch: 6| Step: 6
Training loss: 1.6993741989135742
Validation loss: 2.0238148172696433

Epoch: 6| Step: 7
Training loss: 2.2991185188293457
Validation loss: 2.0155624945958457

Epoch: 6| Step: 8
Training loss: 2.3316988945007324
Validation loss: 2.0225358804066977

Epoch: 6| Step: 9
Training loss: 2.041475772857666
Validation loss: 2.016405244668325

Epoch: 6| Step: 10
Training loss: 2.4502997398376465
Validation loss: 2.018923759460449

Epoch: 6| Step: 11
Training loss: 2.007594347000122
Validation loss: 2.0168063839276633

Epoch: 6| Step: 12
Training loss: 2.55085825920105
Validation loss: 2.012683113416036

Epoch: 6| Step: 13
Training loss: 1.9473410844802856
Validation loss: 2.013772209485372

Epoch: 67| Step: 0
Training loss: 2.4688124656677246
Validation loss: 2.0150431593259177

Epoch: 6| Step: 1
Training loss: 1.8206067085266113
Validation loss: 2.018479347229004

Epoch: 6| Step: 2
Training loss: 2.1024646759033203
Validation loss: 2.012392977873484

Epoch: 6| Step: 3
Training loss: 2.2950901985168457
Validation loss: 2.0147718588511148

Epoch: 6| Step: 4
Training loss: 1.6819612979888916
Validation loss: 2.02293731768926

Epoch: 6| Step: 5
Training loss: 2.2214438915252686
Validation loss: 2.0213780601819358

Epoch: 6| Step: 6
Training loss: 2.2193548679351807
Validation loss: 2.013236959775289

Epoch: 6| Step: 7
Training loss: 2.195474147796631
Validation loss: 2.017206291357676

Epoch: 6| Step: 8
Training loss: 2.5800602436065674
Validation loss: 2.016775051752726

Epoch: 6| Step: 9
Training loss: 2.4667117595672607
Validation loss: 2.0181140899658203

Epoch: 6| Step: 10
Training loss: 2.631884813308716
Validation loss: 2.0183274149894714

Epoch: 6| Step: 11
Training loss: 1.6792633533477783
Validation loss: 2.019691308339437

Epoch: 6| Step: 12
Training loss: 1.3910598754882812
Validation loss: 2.0194267630577087

Epoch: 6| Step: 13
Training loss: 2.8166894912719727
Validation loss: 2.0253915588061013

Epoch: 68| Step: 0
Training loss: 1.7341275215148926
Validation loss: 2.0250301361083984

Epoch: 6| Step: 1
Training loss: 2.594358444213867
Validation loss: 2.0277722676595054

Epoch: 6| Step: 2
Training loss: 2.0306003093719482
Validation loss: 2.023736814657847

Epoch: 6| Step: 3
Training loss: 2.122718095779419
Validation loss: 2.029215653737386

Epoch: 6| Step: 4
Training loss: 2.705155849456787
Validation loss: 2.0219920674959817

Epoch: 6| Step: 5
Training loss: 2.2124156951904297
Validation loss: 2.0264440973599753

Epoch: 6| Step: 6
Training loss: 2.2707929611206055
Validation loss: 2.022211730480194

Epoch: 6| Step: 7
Training loss: 2.252868175506592
Validation loss: 2.022186835606893

Epoch: 6| Step: 8
Training loss: 2.757905960083008
Validation loss: 2.0254094203313193

Epoch: 6| Step: 9
Training loss: 2.232670307159424
Validation loss: 2.0238210161527

Epoch: 6| Step: 10
Training loss: 2.319232702255249
Validation loss: 2.024239957332611

Epoch: 6| Step: 11
Training loss: 1.510339379310608
Validation loss: 2.0219525496164956

Epoch: 6| Step: 12
Training loss: 1.7412314414978027
Validation loss: 2.025759836037954

Epoch: 6| Step: 13
Training loss: 2.0989224910736084
Validation loss: 2.020349125067393

Epoch: 69| Step: 0
Training loss: 1.8674378395080566
Validation loss: 2.0272576411565146

Epoch: 6| Step: 1
Training loss: 2.0779294967651367
Validation loss: 2.0287906924883523

Epoch: 6| Step: 2
Training loss: 2.067246437072754
Validation loss: 2.035070061683655

Epoch: 6| Step: 3
Training loss: 2.6702682971954346
Validation loss: 2.036324401696523

Epoch: 6| Step: 4
Training loss: 1.7230720520019531
Validation loss: 2.0267173846562705

Epoch: 6| Step: 5
Training loss: 2.1411566734313965
Validation loss: 2.039464076360067

Epoch: 6| Step: 6
Training loss: 1.6408958435058594
Validation loss: 2.0411157608032227

Epoch: 6| Step: 7
Training loss: 2.5081844329833984
Validation loss: 2.0345895886421204

Epoch: 6| Step: 8
Training loss: 2.175534248352051
Validation loss: 2.038691063721975

Epoch: 6| Step: 9
Training loss: 1.863981008529663
Validation loss: 2.0314488410949707

Epoch: 6| Step: 10
Training loss: 2.535810708999634
Validation loss: 2.028485377629598

Epoch: 6| Step: 11
Training loss: 2.2703988552093506
Validation loss: 2.0341585874557495

Epoch: 6| Step: 12
Training loss: 2.504782199859619
Validation loss: 2.0299169421195984

Epoch: 6| Step: 13
Training loss: 2.4252874851226807
Validation loss: 2.0259511272112527

Epoch: 70| Step: 0
Training loss: 2.030771493911743
Validation loss: 2.024311045805613

Epoch: 6| Step: 1
Training loss: 2.3236441612243652
Validation loss: 2.011420210202535

Epoch: 6| Step: 2
Training loss: 2.5740089416503906
Validation loss: 2.0191309452056885

Epoch: 6| Step: 3
Training loss: 2.867537498474121
Validation loss: 2.0240402221679688

Epoch: 6| Step: 4
Training loss: 2.8766350746154785
Validation loss: 2.0285465319951377

Epoch: 6| Step: 5
Training loss: 1.7408783435821533
Validation loss: 2.034833828608195

Epoch: 6| Step: 6
Training loss: 1.7996212244033813
Validation loss: 2.0346179207166037

Epoch: 6| Step: 7
Training loss: 2.5010061264038086
Validation loss: 2.0326191584269204

Epoch: 6| Step: 8
Training loss: 1.868438482284546
Validation loss: 2.0244807799657187

Epoch: 6| Step: 9
Training loss: 1.6592400074005127
Validation loss: 2.0295238296190896

Epoch: 6| Step: 10
Training loss: 1.8495646715164185
Validation loss: 2.0270145535469055

Epoch: 6| Step: 11
Training loss: 1.8568687438964844
Validation loss: 2.016830325126648

Epoch: 6| Step: 12
Training loss: 2.384124279022217
Validation loss: 2.0248207251230874

Epoch: 6| Step: 13
Training loss: 2.331791639328003
Validation loss: 2.0157938599586487

Epoch: 71| Step: 0
Training loss: 2.5726592540740967
Validation loss: 2.0185053944587708

Epoch: 6| Step: 1
Training loss: 1.8045625686645508
Validation loss: 2.0239797234535217

Epoch: 6| Step: 2
Training loss: 1.883493185043335
Validation loss: 2.011755367120107

Epoch: 6| Step: 3
Training loss: 2.0328621864318848
Validation loss: 2.0199413498242698

Epoch: 6| Step: 4
Training loss: 2.4965858459472656
Validation loss: 2.029959797859192

Epoch: 6| Step: 5
Training loss: 2.3734829425811768
Validation loss: 2.0335957209269204

Epoch: 6| Step: 6
Training loss: 2.113970994949341
Validation loss: 2.035985827445984

Epoch: 6| Step: 7
Training loss: 2.6839990615844727
Validation loss: 2.0446839531262717

Epoch: 6| Step: 8
Training loss: 1.9423012733459473
Validation loss: 2.042621076107025

Epoch: 6| Step: 9
Training loss: 2.0403170585632324
Validation loss: 2.042288283507029

Epoch: 6| Step: 10
Training loss: 2.4573986530303955
Validation loss: 2.048565904299418

Epoch: 6| Step: 11
Training loss: 2.3838205337524414
Validation loss: 2.0333438316980996

Epoch: 6| Step: 12
Training loss: 1.5192780494689941
Validation loss: 2.0174167156219482

Epoch: 6| Step: 13
Training loss: 2.0968425273895264
Validation loss: 2.0257895588874817

Epoch: 72| Step: 0
Training loss: 1.8509200811386108
Validation loss: 2.0154934922854104

Epoch: 6| Step: 1
Training loss: 2.0966153144836426
Validation loss: 2.009700874487559

Epoch: 6| Step: 2
Training loss: 1.9917678833007812
Validation loss: 2.0041654308636985

Epoch: 6| Step: 3
Training loss: 1.9343235492706299
Validation loss: 2.0099472800890603

Epoch: 6| Step: 4
Training loss: 2.3444600105285645
Validation loss: 2.014231105645498

Epoch: 6| Step: 5
Training loss: 2.4326210021972656
Validation loss: 2.013035694758097

Epoch: 6| Step: 6
Training loss: 1.7145731449127197
Validation loss: 2.013053556283315

Epoch: 6| Step: 7
Training loss: 1.9042023420333862
Validation loss: 2.016530613104502

Epoch: 6| Step: 8
Training loss: 2.553943395614624
Validation loss: 2.018569310506185

Epoch: 6| Step: 9
Training loss: 2.4939205646514893
Validation loss: 2.0233590602874756

Epoch: 6| Step: 10
Training loss: 2.090655565261841
Validation loss: 2.0133636196454368

Epoch: 6| Step: 11
Training loss: 2.584928035736084
Validation loss: 2.007103900114695

Epoch: 6| Step: 12
Training loss: 2.940192222595215
Validation loss: 2.016724427541097

Epoch: 6| Step: 13
Training loss: 1.4674196243286133
Validation loss: 2.0177342096964517

Epoch: 73| Step: 0
Training loss: 1.814316987991333
Validation loss: 2.012276311715444

Epoch: 6| Step: 1
Training loss: 2.5605154037475586
Validation loss: 2.015730837980906

Epoch: 6| Step: 2
Training loss: 2.4956448078155518
Validation loss: 2.016040484110514

Epoch: 6| Step: 3
Training loss: 2.4022059440612793
Validation loss: 2.013467252254486

Epoch: 6| Step: 4
Training loss: 1.9938983917236328
Validation loss: 2.008124570051829

Epoch: 6| Step: 5
Training loss: 2.0419132709503174
Validation loss: 2.0122809410095215

Epoch: 6| Step: 6
Training loss: 2.3923416137695312
Validation loss: 2.0159197052319846

Epoch: 6| Step: 7
Training loss: 1.7437021732330322
Validation loss: 2.0094582041104636

Epoch: 6| Step: 8
Training loss: 2.893371343612671
Validation loss: 2.0131605863571167

Epoch: 6| Step: 9
Training loss: 2.1747395992279053
Validation loss: 2.0160440603892007

Epoch: 6| Step: 10
Training loss: 1.3986660242080688
Validation loss: 2.014599084854126

Epoch: 6| Step: 11
Training loss: 2.0809903144836426
Validation loss: 2.014310439427694

Epoch: 6| Step: 12
Training loss: 2.053166151046753
Validation loss: 2.0094146132469177

Epoch: 6| Step: 13
Training loss: 2.2576661109924316
Validation loss: 2.008506417274475

Epoch: 74| Step: 0
Training loss: 2.1034438610076904
Validation loss: 2.009099284807841

Epoch: 6| Step: 1
Training loss: 2.0746326446533203
Validation loss: 2.0205573042233786

Epoch: 6| Step: 2
Training loss: 2.503505229949951
Validation loss: 2.0192616979281106

Epoch: 6| Step: 3
Training loss: 2.4022216796875
Validation loss: 2.0136146346728006

Epoch: 6| Step: 4
Training loss: 2.563107967376709
Validation loss: 2.0189029773076377

Epoch: 6| Step: 5
Training loss: 2.368724822998047
Validation loss: 2.0153462886810303

Epoch: 6| Step: 6
Training loss: 1.7878507375717163
Validation loss: 2.0154630144437156

Epoch: 6| Step: 7
Training loss: 2.1172375679016113
Validation loss: 2.0164279341697693

Epoch: 6| Step: 8
Training loss: 1.7731837034225464
Validation loss: 2.022313912709554

Epoch: 6| Step: 9
Training loss: 1.5607993602752686
Validation loss: 2.0209277272224426

Epoch: 6| Step: 10
Training loss: 2.2460379600524902
Validation loss: 2.0286444624265036

Epoch: 6| Step: 11
Training loss: 2.5579793453216553
Validation loss: 2.02491287390391

Epoch: 6| Step: 12
Training loss: 1.9439688920974731
Validation loss: 2.0237805048624673

Epoch: 6| Step: 13
Training loss: 2.168015480041504
Validation loss: 2.026448925336202

Epoch: 75| Step: 0
Training loss: 2.004901885986328
Validation loss: 2.0314618945121765

Epoch: 6| Step: 1
Training loss: 2.065145969390869
Validation loss: 2.033050537109375

Epoch: 6| Step: 2
Training loss: 2.454822063446045
Validation loss: 2.029405117034912

Epoch: 6| Step: 3
Training loss: 1.846544623374939
Validation loss: 2.021045366923014

Epoch: 6| Step: 4
Training loss: 2.1075878143310547
Validation loss: 2.0105340282122293

Epoch: 6| Step: 5
Training loss: 2.1322364807128906
Validation loss: 2.020352323849996

Epoch: 6| Step: 6
Training loss: 1.6549742221832275
Validation loss: 2.013838311036428

Epoch: 6| Step: 7
Training loss: 1.8234070539474487
Validation loss: 2.0144880612691245

Epoch: 6| Step: 8
Training loss: 2.4683425426483154
Validation loss: 2.0183133085568747

Epoch: 6| Step: 9
Training loss: 1.9958893060684204
Validation loss: 2.0178295771280923

Epoch: 6| Step: 10
Training loss: 2.4742331504821777
Validation loss: 2.019109070301056

Epoch: 6| Step: 11
Training loss: 2.49733304977417
Validation loss: 2.0207302967707315

Epoch: 6| Step: 12
Training loss: 2.850465774536133
Validation loss: 2.0181456804275513

Epoch: 6| Step: 13
Training loss: 1.8895986080169678
Validation loss: 2.0238352020581565

Epoch: 76| Step: 0
Training loss: 2.304779529571533
Validation loss: 2.0166833798090615

Epoch: 6| Step: 1
Training loss: 1.7212042808532715
Validation loss: 2.019645094871521

Epoch: 6| Step: 2
Training loss: 1.870190143585205
Validation loss: 2.0125200351079306

Epoch: 6| Step: 3
Training loss: 2.138639450073242
Validation loss: 2.0191601514816284

Epoch: 6| Step: 4
Training loss: 1.7914395332336426
Validation loss: 2.023266394933065

Epoch: 6| Step: 5
Training loss: 2.6777169704437256
Validation loss: 2.026373287041982

Epoch: 6| Step: 6
Training loss: 1.9179933071136475
Validation loss: 2.0237494508425393

Epoch: 6| Step: 7
Training loss: 2.261806011199951
Validation loss: 2.0316967964172363

Epoch: 6| Step: 8
Training loss: 1.4343209266662598
Validation loss: 2.026854316393534

Epoch: 6| Step: 9
Training loss: 2.6604504585266113
Validation loss: 2.029609203338623

Epoch: 6| Step: 10
Training loss: 2.0652735233306885
Validation loss: 2.0338886777559915

Epoch: 6| Step: 11
Training loss: 2.1771438121795654
Validation loss: 2.0284803907076516

Epoch: 6| Step: 12
Training loss: 2.526982307434082
Validation loss: 2.0231388211250305

Epoch: 6| Step: 13
Training loss: 2.6548423767089844
Validation loss: 2.024225374062856

Epoch: 77| Step: 0
Training loss: 1.8967753648757935
Validation loss: 2.022050976753235

Epoch: 6| Step: 1
Training loss: 2.3846962451934814
Validation loss: 2.024117370446523

Epoch: 6| Step: 2
Training loss: 2.1881370544433594
Validation loss: 2.0165207187334695

Epoch: 6| Step: 3
Training loss: 2.5224783420562744
Validation loss: 2.020244300365448

Epoch: 6| Step: 4
Training loss: 2.122079849243164
Validation loss: 2.020650625228882

Epoch: 6| Step: 5
Training loss: 2.0693655014038086
Validation loss: 2.0148927370707193

Epoch: 6| Step: 6
Training loss: 2.1769766807556152
Validation loss: 2.024000863234202

Epoch: 6| Step: 7
Training loss: 2.81567120552063
Validation loss: 2.0238329966863

Epoch: 6| Step: 8
Training loss: 1.4995288848876953
Validation loss: 2.0248374342918396

Epoch: 6| Step: 9
Training loss: 2.1705501079559326
Validation loss: 2.0316619078318277

Epoch: 6| Step: 10
Training loss: 2.3365230560302734
Validation loss: 2.0206528504689536

Epoch: 6| Step: 11
Training loss: 1.953626036643982
Validation loss: 2.0282177329063416

Epoch: 6| Step: 12
Training loss: 2.0395116806030273
Validation loss: 2.0296921928723655

Epoch: 6| Step: 13
Training loss: 1.7998193502426147
Validation loss: 2.0358837246894836

Epoch: 78| Step: 0
Training loss: 1.5912797451019287
Validation loss: 2.031232714653015

Epoch: 6| Step: 1
Training loss: 2.154660701751709
Validation loss: 2.0311818520228067

Epoch: 6| Step: 2
Training loss: 2.5758819580078125
Validation loss: 2.0347115993499756

Epoch: 6| Step: 3
Training loss: 1.7503938674926758
Validation loss: 2.030407210191091

Epoch: 6| Step: 4
Training loss: 2.08642315864563
Validation loss: 2.0360991954803467

Epoch: 6| Step: 5
Training loss: 2.7028884887695312
Validation loss: 2.035092373689016

Epoch: 6| Step: 6
Training loss: 2.656510591506958
Validation loss: 2.026652753353119

Epoch: 6| Step: 7
Training loss: 2.3984997272491455
Validation loss: 2.0309528708457947

Epoch: 6| Step: 8
Training loss: 1.8131434917449951
Validation loss: 2.0320298870404563

Epoch: 6| Step: 9
Training loss: 2.198328733444214
Validation loss: 2.033026397228241

Epoch: 6| Step: 10
Training loss: 1.7442655563354492
Validation loss: 2.021762947241465

Epoch: 6| Step: 11
Training loss: 1.8107423782348633
Validation loss: 2.0233522852261863

Epoch: 6| Step: 12
Training loss: 2.0474562644958496
Validation loss: 2.028276026248932

Epoch: 6| Step: 13
Training loss: 2.413848876953125
Validation loss: 2.0236111680666604

Epoch: 79| Step: 0
Training loss: 1.8291670083999634
Validation loss: 2.030825436115265

Epoch: 6| Step: 1
Training loss: 1.3735933303833008
Validation loss: 2.0168376167615256

Epoch: 6| Step: 2
Training loss: 1.9136910438537598
Validation loss: 2.0238596399625144

Epoch: 6| Step: 3
Training loss: 2.3712494373321533
Validation loss: 2.0200008153915405

Epoch: 6| Step: 4
Training loss: 2.0933496952056885
Validation loss: 2.0285235246022544

Epoch: 6| Step: 5
Training loss: 3.2250728607177734
Validation loss: 2.026878774166107

Epoch: 6| Step: 6
Training loss: 2.4792959690093994
Validation loss: 2.0280922849973044

Epoch: 6| Step: 7
Training loss: 1.9146904945373535
Validation loss: 2.025513549645742

Epoch: 6| Step: 8
Training loss: 2.7119898796081543
Validation loss: 2.020508289337158

Epoch: 6| Step: 9
Training loss: 2.0784831047058105
Validation loss: 2.018556237220764

Epoch: 6| Step: 10
Training loss: 1.9358190298080444
Validation loss: 2.0167986353238425

Epoch: 6| Step: 11
Training loss: 2.5518839359283447
Validation loss: 2.023102104663849

Epoch: 6| Step: 12
Training loss: 1.8463926315307617
Validation loss: 2.0078529119491577

Epoch: 6| Step: 13
Training loss: 1.9654815196990967
Validation loss: 2.0121134916941323

Epoch: 80| Step: 0
Training loss: 2.202167510986328
Validation loss: 2.0180504520734153

Epoch: 6| Step: 1
Training loss: 2.4615108966827393
Validation loss: 2.0085476636886597

Epoch: 6| Step: 2
Training loss: 1.6136099100112915
Validation loss: 2.008745471636454

Epoch: 6| Step: 3
Training loss: 1.9133812189102173
Validation loss: 2.0081360141436257

Epoch: 6| Step: 4
Training loss: 2.1969776153564453
Validation loss: 2.010289947191874

Epoch: 6| Step: 5
Training loss: 2.077237844467163
Validation loss: 2.0087567567825317

Epoch: 6| Step: 6
Training loss: 2.3686270713806152
Validation loss: 2.0170525709788003

Epoch: 6| Step: 7
Training loss: 2.462268352508545
Validation loss: 2.0103584925333657

Epoch: 6| Step: 8
Training loss: 2.203082323074341
Validation loss: 2.0225667158762612

Epoch: 6| Step: 9
Training loss: 1.6634321212768555
Validation loss: 2.0178677241007485

Epoch: 6| Step: 10
Training loss: 1.9310739040374756
Validation loss: 2.0120880802472434

Epoch: 6| Step: 11
Training loss: 2.2835211753845215
Validation loss: 2.0151882568995156

Epoch: 6| Step: 12
Training loss: 2.4618780612945557
Validation loss: 2.009723703066508

Epoch: 6| Step: 13
Training loss: 2.2828545570373535
Validation loss: 2.0181137522061667

Epoch: 81| Step: 0
Training loss: 2.324796676635742
Validation loss: 2.0201613108317056

Epoch: 6| Step: 1
Training loss: 1.5259311199188232
Validation loss: 2.0163639783859253

Epoch: 6| Step: 2
Training loss: 1.9475631713867188
Validation loss: 2.0095713138580322

Epoch: 6| Step: 3
Training loss: 2.4694628715515137
Validation loss: 2.008673906326294

Epoch: 6| Step: 4
Training loss: 2.860823631286621
Validation loss: 2.0160078605016074

Epoch: 6| Step: 5
Training loss: 2.356729507446289
Validation loss: 2.0149840116500854

Epoch: 6| Step: 6
Training loss: 1.4208157062530518
Validation loss: 2.015522221724192

Epoch: 6| Step: 7
Training loss: 1.979350209236145
Validation loss: 2.016793429851532

Epoch: 6| Step: 8
Training loss: 2.0517067909240723
Validation loss: 2.00844011704127

Epoch: 6| Step: 9
Training loss: 2.3000130653381348
Validation loss: 2.0170553723971048

Epoch: 6| Step: 10
Training loss: 2.6240200996398926
Validation loss: 2.0207528869311013

Epoch: 6| Step: 11
Training loss: 2.184169292449951
Validation loss: 2.017329235871633

Epoch: 6| Step: 12
Training loss: 2.209285259246826
Validation loss: 2.023915390173594

Epoch: 6| Step: 13
Training loss: 1.755153775215149
Validation loss: 2.0206954876581826

Epoch: 82| Step: 0
Training loss: 1.5908782482147217
Validation loss: 2.0180524388949075

Epoch: 6| Step: 1
Training loss: 1.9744631052017212
Validation loss: 2.030905822912852

Epoch: 6| Step: 2
Training loss: 2.3871259689331055
Validation loss: 2.0232645869255066

Epoch: 6| Step: 3
Training loss: 2.172550678253174
Validation loss: 2.0214701096216836

Epoch: 6| Step: 4
Training loss: 2.2396059036254883
Validation loss: 2.024528920650482

Epoch: 6| Step: 5
Training loss: 1.9798599481582642
Validation loss: 2.0224973360697427

Epoch: 6| Step: 6
Training loss: 2.3094635009765625
Validation loss: 2.018652002016703

Epoch: 6| Step: 7
Training loss: 1.865368366241455
Validation loss: 2.02793550491333

Epoch: 6| Step: 8
Training loss: 2.262342691421509
Validation loss: 2.0242472688357034

Epoch: 6| Step: 9
Training loss: 2.2286157608032227
Validation loss: 2.024278382460276

Epoch: 6| Step: 10
Training loss: 2.6958391666412354
Validation loss: 2.026079078515371

Epoch: 6| Step: 11
Training loss: 2.1537933349609375
Validation loss: 2.022530496120453

Epoch: 6| Step: 12
Training loss: 2.2946701049804688
Validation loss: 2.022987484931946

Epoch: 6| Step: 13
Training loss: 1.8848860263824463
Validation loss: 2.0183130701382956

Epoch: 83| Step: 0
Training loss: 2.029550075531006
Validation loss: 2.0169884165128074

Epoch: 6| Step: 1
Training loss: 1.1804254055023193
Validation loss: 2.0162726243336997

Epoch: 6| Step: 2
Training loss: 2.297192096710205
Validation loss: 2.0138614177703857

Epoch: 6| Step: 3
Training loss: 2.3168182373046875
Validation loss: 2.0166741013526917

Epoch: 6| Step: 4
Training loss: 2.073363780975342
Validation loss: 2.015321513017019

Epoch: 6| Step: 5
Training loss: 2.285609722137451
Validation loss: 2.0190336505572

Epoch: 6| Step: 6
Training loss: 2.283721923828125
Validation loss: 2.0166648825009665

Epoch: 6| Step: 7
Training loss: 2.013819694519043
Validation loss: 2.0069734851519265

Epoch: 6| Step: 8
Training loss: 2.1696407794952393
Validation loss: 2.0193055669466653

Epoch: 6| Step: 9
Training loss: 1.7726609706878662
Validation loss: 2.021177093187968

Epoch: 6| Step: 10
Training loss: 2.465333938598633
Validation loss: 2.0196668903032937

Epoch: 6| Step: 11
Training loss: 2.617802619934082
Validation loss: 2.024475951989492

Epoch: 6| Step: 12
Training loss: 2.452324867248535
Validation loss: 2.025821586449941

Epoch: 6| Step: 13
Training loss: 2.0381510257720947
Validation loss: 2.0254555344581604

Epoch: 84| Step: 0
Training loss: 2.175182819366455
Validation loss: 2.022450049718221

Epoch: 6| Step: 1
Training loss: 2.9366884231567383
Validation loss: 2.023119270801544

Epoch: 6| Step: 2
Training loss: 1.9590903520584106
Validation loss: 2.0291554729143777

Epoch: 6| Step: 3
Training loss: 1.3237956762313843
Validation loss: 2.02298774321874

Epoch: 6| Step: 4
Training loss: 1.756273865699768
Validation loss: 2.012127031882604

Epoch: 6| Step: 5
Training loss: 2.136946201324463
Validation loss: 2.022940754890442

Epoch: 6| Step: 6
Training loss: 2.9525954723358154
Validation loss: 2.0253400802612305

Epoch: 6| Step: 7
Training loss: 1.5861244201660156
Validation loss: 2.035758634408315

Epoch: 6| Step: 8
Training loss: 2.3813319206237793
Validation loss: 2.0268704096476235

Epoch: 6| Step: 9
Training loss: 1.902998447418213
Validation loss: 2.0309205055236816

Epoch: 6| Step: 10
Training loss: 2.2115418910980225
Validation loss: 2.023410737514496

Epoch: 6| Step: 11
Training loss: 1.8560724258422852
Validation loss: 2.0215199987093606

Epoch: 6| Step: 12
Training loss: 2.5273704528808594
Validation loss: 2.022773563861847

Epoch: 6| Step: 13
Training loss: 2.0332388877868652
Validation loss: 2.025794565677643

Epoch: 85| Step: 0
Training loss: 2.264907121658325
Validation loss: 2.016931653022766

Epoch: 6| Step: 1
Training loss: 2.5504941940307617
Validation loss: 2.0108179648717246

Epoch: 6| Step: 2
Training loss: 2.055121898651123
Validation loss: 2.0214138428370156

Epoch: 6| Step: 3
Training loss: 1.462849497795105
Validation loss: 2.015745004018148

Epoch: 6| Step: 4
Training loss: 2.105431079864502
Validation loss: 2.021884640057882

Epoch: 6| Step: 5
Training loss: 2.357698917388916
Validation loss: 2.0211174686749778

Epoch: 6| Step: 6
Training loss: 2.2107837200164795
Validation loss: 2.021623353163401

Epoch: 6| Step: 7
Training loss: 2.0298099517822266
Validation loss: 2.028875152269999

Epoch: 6| Step: 8
Training loss: 1.992255449295044
Validation loss: 2.0211660861968994

Epoch: 6| Step: 9
Training loss: 2.399293899536133
Validation loss: 2.0271819829940796

Epoch: 6| Step: 10
Training loss: 2.056194543838501
Validation loss: 2.0341781775156655

Epoch: 6| Step: 11
Training loss: 1.6156718730926514
Validation loss: 2.029507557551066

Epoch: 6| Step: 12
Training loss: 2.4868695735931396
Validation loss: 2.023390531539917

Epoch: 6| Step: 13
Training loss: 2.2082200050354004
Validation loss: 2.0249523719151816

Epoch: 86| Step: 0
Training loss: 2.3580174446105957
Validation loss: 2.016808191935221

Epoch: 6| Step: 1
Training loss: 2.5659093856811523
Validation loss: 2.0300806760787964

Epoch: 6| Step: 2
Training loss: 2.0235815048217773
Validation loss: 2.019146203994751

Epoch: 6| Step: 3
Training loss: 2.0217907428741455
Validation loss: 2.0108473698298135

Epoch: 6| Step: 4
Training loss: 2.123190402984619
Validation loss: 2.0100829799969993

Epoch: 6| Step: 5
Training loss: 2.1041500568389893
Validation loss: 2.0121090610822043

Epoch: 6| Step: 6
Training loss: 1.9243072271347046
Validation loss: 2.011107087135315

Epoch: 6| Step: 7
Training loss: 1.8768824338912964
Validation loss: 2.0030636191368103

Epoch: 6| Step: 8
Training loss: 2.151827096939087
Validation loss: 2.0116355617841086

Epoch: 6| Step: 9
Training loss: 2.0108962059020996
Validation loss: 2.007099707921346

Epoch: 6| Step: 10
Training loss: 1.7981613874435425
Validation loss: 2.014009495576223

Epoch: 6| Step: 11
Training loss: 2.038785219192505
Validation loss: 2.0247859160105386

Epoch: 6| Step: 12
Training loss: 2.628638744354248
Validation loss: 2.02748050292333

Epoch: 6| Step: 13
Training loss: 2.155097484588623
Validation loss: 2.0260509252548218

Epoch: 87| Step: 0
Training loss: 2.3044369220733643
Validation loss: 2.0250429113705954

Epoch: 6| Step: 1
Training loss: 1.9843969345092773
Validation loss: 2.022823770840963

Epoch: 6| Step: 2
Training loss: 1.926184892654419
Validation loss: 2.027924954891205

Epoch: 6| Step: 3
Training loss: 2.1546213626861572
Validation loss: 2.0246370236078897

Epoch: 6| Step: 4
Training loss: 1.7782036066055298
Validation loss: 2.0248759984970093

Epoch: 6| Step: 5
Training loss: 2.420579433441162
Validation loss: 2.0292811393737793

Epoch: 6| Step: 6
Training loss: 2.413311004638672
Validation loss: 2.016993761062622

Epoch: 6| Step: 7
Training loss: 2.159708261489868
Validation loss: 2.02033931016922

Epoch: 6| Step: 8
Training loss: 2.623356342315674
Validation loss: 2.0229201912879944

Epoch: 6| Step: 9
Training loss: 1.8718401193618774
Validation loss: 2.0189693371454873

Epoch: 6| Step: 10
Training loss: 2.412137985229492
Validation loss: 2.0184786518414817

Epoch: 6| Step: 11
Training loss: 2.6462130546569824
Validation loss: 2.0128363768259683

Epoch: 6| Step: 12
Training loss: 1.7086946964263916
Validation loss: 2.013160308202108

Epoch: 6| Step: 13
Training loss: 1.4346753358840942
Validation loss: 2.0116982062657676

Epoch: 88| Step: 0
Training loss: 1.7600513696670532
Validation loss: 2.014147937297821

Epoch: 6| Step: 1
Training loss: 1.8280200958251953
Validation loss: 2.0192885597546897

Epoch: 6| Step: 2
Training loss: 2.215087413787842
Validation loss: 2.022748510042826

Epoch: 6| Step: 3
Training loss: 2.041332244873047
Validation loss: 2.025727848211924

Epoch: 6| Step: 4
Training loss: 2.4275882244110107
Validation loss: 2.027202069759369

Epoch: 6| Step: 5
Training loss: 2.17250394821167
Validation loss: 2.0337149103482566

Epoch: 6| Step: 6
Training loss: 2.399326801300049
Validation loss: 2.0382506450017295

Epoch: 6| Step: 7
Training loss: 2.089305877685547
Validation loss: 2.035069386164347

Epoch: 6| Step: 8
Training loss: 2.278496503829956
Validation loss: 2.0277939637502036

Epoch: 6| Step: 9
Training loss: 1.925274133682251
Validation loss: 2.0349841316541037

Epoch: 6| Step: 10
Training loss: 2.146493911743164
Validation loss: 2.024024784564972

Epoch: 6| Step: 11
Training loss: 2.9695401191711426
Validation loss: 2.038300653298696

Epoch: 6| Step: 12
Training loss: 2.30301570892334
Validation loss: 2.0319899916648865

Epoch: 6| Step: 13
Training loss: 1.0795016288757324
Validation loss: 2.034453491369883

Epoch: 89| Step: 0
Training loss: 2.2093453407287598
Validation loss: 2.0348800818125405

Epoch: 6| Step: 1
Training loss: 2.295542001724243
Validation loss: 2.0418985883394876

Epoch: 6| Step: 2
Training loss: 1.9111889600753784
Validation loss: 2.0410528977711997

Epoch: 6| Step: 3
Training loss: 2.167675495147705
Validation loss: 2.0417622526486716

Epoch: 6| Step: 4
Training loss: 1.9076097011566162
Validation loss: 2.0425104896227517

Epoch: 6| Step: 5
Training loss: 1.9034881591796875
Validation loss: 2.041957120100657

Epoch: 6| Step: 6
Training loss: 2.4978864192962646
Validation loss: 2.027271091938019

Epoch: 6| Step: 7
Training loss: 2.4398064613342285
Validation loss: 2.0227203965187073

Epoch: 6| Step: 8
Training loss: 2.1356892585754395
Validation loss: 2.010531465212504

Epoch: 6| Step: 9
Training loss: 2.5660226345062256
Validation loss: 2.0085908571879068

Epoch: 6| Step: 10
Training loss: 1.8574590682983398
Validation loss: 2.0141957998275757

Epoch: 6| Step: 11
Training loss: 2.6937103271484375
Validation loss: 2.0192903677622476

Epoch: 6| Step: 12
Training loss: 1.9500510692596436
Validation loss: 2.0208826859792075

Epoch: 6| Step: 13
Training loss: 1.5476670265197754
Validation loss: 2.021186133225759

Epoch: 90| Step: 0
Training loss: 1.977171540260315
Validation loss: 2.016011397043864

Epoch: 6| Step: 1
Training loss: 2.2468373775482178
Validation loss: 2.0115288297335305

Epoch: 6| Step: 2
Training loss: 2.156212329864502
Validation loss: 1.9995898008346558

Epoch: 6| Step: 3
Training loss: 1.9075145721435547
Validation loss: 1.9974193771680195

Epoch: 6| Step: 4
Training loss: 1.7679710388183594
Validation loss: 2.0142290790875754

Epoch: 6| Step: 5
Training loss: 2.1180295944213867
Validation loss: 2.008494714895884

Epoch: 6| Step: 6
Training loss: 1.6648361682891846
Validation loss: 2.0146268606185913

Epoch: 6| Step: 7
Training loss: 2.381293535232544
Validation loss: 2.023490289847056

Epoch: 6| Step: 8
Training loss: 2.1788346767425537
Validation loss: 2.0350642005602517

Epoch: 6| Step: 9
Training loss: 2.3533859252929688
Validation loss: 2.0458149909973145

Epoch: 6| Step: 10
Training loss: 2.867610454559326
Validation loss: 2.053003211816152

Epoch: 6| Step: 11
Training loss: 2.4428207874298096
Validation loss: 2.043726066748301

Epoch: 6| Step: 12
Training loss: 2.073638677597046
Validation loss: 2.0400038957595825

Epoch: 6| Step: 13
Training loss: 1.908332109451294
Validation loss: 2.029864013195038

Epoch: 91| Step: 0
Training loss: 1.8619202375411987
Validation loss: 2.0205024679501853

Epoch: 6| Step: 1
Training loss: 2.8456926345825195
Validation loss: 2.0163269440333047

Epoch: 6| Step: 2
Training loss: 2.0706682205200195
Validation loss: 2.01639993985494

Epoch: 6| Step: 3
Training loss: 1.7000682353973389
Validation loss: 2.0219155152638755

Epoch: 6| Step: 4
Training loss: 2.598705530166626
Validation loss: 2.0203815499941506

Epoch: 6| Step: 5
Training loss: 1.6159672737121582
Validation loss: 2.0177194078763327

Epoch: 6| Step: 6
Training loss: 2.4895553588867188
Validation loss: 2.0127970973650613

Epoch: 6| Step: 7
Training loss: 2.0879857540130615
Validation loss: 2.0092211961746216

Epoch: 6| Step: 8
Training loss: 1.941856026649475
Validation loss: 2.0166184504826865

Epoch: 6| Step: 9
Training loss: 1.8720659017562866
Validation loss: 2.012274225552877

Epoch: 6| Step: 10
Training loss: 2.2115731239318848
Validation loss: 2.012023170789083

Epoch: 6| Step: 11
Training loss: 2.79172420501709
Validation loss: 2.0228115717569985

Epoch: 6| Step: 12
Training loss: 2.1934070587158203
Validation loss: 2.0163843631744385

Epoch: 6| Step: 13
Training loss: 1.312491536140442
Validation loss: 2.023220499356588

Epoch: 92| Step: 0
Training loss: 2.20229434967041
Validation loss: 2.0157044529914856

Epoch: 6| Step: 1
Training loss: 1.7480107545852661
Validation loss: 2.0210644404093423

Epoch: 6| Step: 2
Training loss: 1.8806604146957397
Validation loss: 2.018351197242737

Epoch: 6| Step: 3
Training loss: 1.9795114994049072
Validation loss: 2.014547049999237

Epoch: 6| Step: 4
Training loss: 1.9697279930114746
Validation loss: 2.0173775951067605

Epoch: 6| Step: 5
Training loss: 2.438051700592041
Validation loss: 2.0105138222376504

Epoch: 6| Step: 6
Training loss: 1.9838755130767822
Validation loss: 2.018358459075292

Epoch: 6| Step: 7
Training loss: 2.383739471435547
Validation loss: 2.0248903234799704

Epoch: 6| Step: 8
Training loss: 2.513587236404419
Validation loss: 2.028367002805074

Epoch: 6| Step: 9
Training loss: 2.0021543502807617
Validation loss: 2.029727061589559

Epoch: 6| Step: 10
Training loss: 2.00270414352417
Validation loss: 2.03488035996755

Epoch: 6| Step: 11
Training loss: 2.089972496032715
Validation loss: 2.0272743900616965

Epoch: 6| Step: 12
Training loss: 2.3167858123779297
Validation loss: 2.036141117413839

Epoch: 6| Step: 13
Training loss: 2.1005783081054688
Validation loss: 2.0531710982322693

Epoch: 93| Step: 0
Training loss: 2.3485608100891113
Validation loss: 2.037341852982839

Epoch: 6| Step: 1
Training loss: 2.363708019256592
Validation loss: 2.0484100580215454

Epoch: 6| Step: 2
Training loss: 2.11454701423645
Validation loss: 2.05021999279658

Epoch: 6| Step: 3
Training loss: 1.799390435218811
Validation loss: 2.0528231461842856

Epoch: 6| Step: 4
Training loss: 1.9041130542755127
Validation loss: 2.045068323612213

Epoch: 6| Step: 5
Training loss: 2.3055062294006348
Validation loss: 2.053320328394572

Epoch: 6| Step: 6
Training loss: 2.037757396697998
Validation loss: 2.0414358973503113

Epoch: 6| Step: 7
Training loss: 2.103304386138916
Validation loss: 2.039093017578125

Epoch: 6| Step: 8
Training loss: 1.290649652481079
Validation loss: 2.0446860591570535

Epoch: 6| Step: 9
Training loss: 2.8756368160247803
Validation loss: 2.054099917411804

Epoch: 6| Step: 10
Training loss: 1.9126360416412354
Validation loss: 2.0330877105394998

Epoch: 6| Step: 11
Training loss: 2.1554460525512695
Validation loss: 2.0295563340187073

Epoch: 6| Step: 12
Training loss: 2.1258201599121094
Validation loss: 2.041341543197632

Epoch: 6| Step: 13
Training loss: 2.3672900199890137
Validation loss: 2.027138272921244

Epoch: 94| Step: 0
Training loss: 1.9595770835876465
Validation loss: 2.0298705101013184

Epoch: 6| Step: 1
Training loss: 2.236056089401245
Validation loss: 2.0278597672780356

Epoch: 6| Step: 2
Training loss: 1.7392076253890991
Validation loss: 2.0258887807528176

Epoch: 6| Step: 3
Training loss: 2.4786453247070312
Validation loss: 2.022463877995809

Epoch: 6| Step: 4
Training loss: 2.4437150955200195
Validation loss: 2.022550384203593

Epoch: 6| Step: 5
Training loss: 2.1257681846618652
Validation loss: 2.024633447329203

Epoch: 6| Step: 6
Training loss: 2.5680675506591797
Validation loss: 2.019384423891703

Epoch: 6| Step: 7
Training loss: 2.1108527183532715
Validation loss: 2.023315151532491

Epoch: 6| Step: 8
Training loss: 2.229745864868164
Validation loss: 2.0314292510350547

Epoch: 6| Step: 9
Training loss: 2.147430658340454
Validation loss: 2.021774192651113

Epoch: 6| Step: 10
Training loss: 2.158054828643799
Validation loss: 2.0239992141723633

Epoch: 6| Step: 11
Training loss: 2.009150505065918
Validation loss: 2.026080012321472

Epoch: 6| Step: 12
Training loss: 1.83380126953125
Validation loss: 2.0302279790242515

Epoch: 6| Step: 13
Training loss: 1.6286756992340088
Validation loss: 2.0408902764320374

Epoch: 95| Step: 0
Training loss: 2.6054935455322266
Validation loss: 2.0340104897816977

Epoch: 6| Step: 1
Training loss: 1.0676209926605225
Validation loss: 2.0274662176767984

Epoch: 6| Step: 2
Training loss: 1.771630048751831
Validation loss: 2.029019296169281

Epoch: 6| Step: 3
Training loss: 1.5719729661941528
Validation loss: 2.0430779258410134

Epoch: 6| Step: 4
Training loss: 2.288545608520508
Validation loss: 2.039290487766266

Epoch: 6| Step: 5
Training loss: 2.454270601272583
Validation loss: 2.0459949175516763

Epoch: 6| Step: 6
Training loss: 1.7668852806091309
Validation loss: 2.039454778035482

Epoch: 6| Step: 7
Training loss: 2.8505403995513916
Validation loss: 2.0304217537244162

Epoch: 6| Step: 8
Training loss: 2.320722818374634
Validation loss: 2.041733423868815

Epoch: 6| Step: 9
Training loss: 2.2576904296875
Validation loss: 2.0325812101364136

Epoch: 6| Step: 10
Training loss: 2.1058454513549805
Validation loss: 2.033747434616089

Epoch: 6| Step: 11
Training loss: 2.0211551189422607
Validation loss: 2.0370362997055054

Epoch: 6| Step: 12
Training loss: 2.6155080795288086
Validation loss: 2.048500418663025

Epoch: 6| Step: 13
Training loss: 1.8155003786087036
Validation loss: 2.031373123327891

Epoch: 96| Step: 0
Training loss: 1.3706297874450684
Validation loss: 2.036021610101064

Epoch: 6| Step: 1
Training loss: 1.2985222339630127
Validation loss: 2.0258986552556357

Epoch: 6| Step: 2
Training loss: 2.177974224090576
Validation loss: 2.0310794909795127

Epoch: 6| Step: 3
Training loss: 2.6622095108032227
Validation loss: 2.038243889808655

Epoch: 6| Step: 4
Training loss: 2.599822998046875
Validation loss: 2.0247262120246887

Epoch: 6| Step: 5
Training loss: 2.730159282684326
Validation loss: 2.026875694592794

Epoch: 6| Step: 6
Training loss: 2.210739850997925
Validation loss: 2.0232383807500205

Epoch: 6| Step: 7
Training loss: 2.070983409881592
Validation loss: 2.023099939028422

Epoch: 6| Step: 8
Training loss: 1.913011908531189
Validation loss: 2.027683655420939

Epoch: 6| Step: 9
Training loss: 2.4037156105041504
Validation loss: 2.021209458510081

Epoch: 6| Step: 10
Training loss: 1.755718469619751
Validation loss: 2.0227483908335366

Epoch: 6| Step: 11
Training loss: 1.9186341762542725
Validation loss: 2.0228532950083413

Epoch: 6| Step: 12
Training loss: 2.697988986968994
Validation loss: 2.032012661298116

Epoch: 6| Step: 13
Training loss: 1.7206690311431885
Validation loss: 2.0319042404492698

Epoch: 97| Step: 0
Training loss: 2.2451391220092773
Validation loss: 2.016691486040751

Epoch: 6| Step: 1
Training loss: 2.462675094604492
Validation loss: 2.020502964655558

Epoch: 6| Step: 2
Training loss: 2.255594253540039
Validation loss: 2.034262239933014

Epoch: 6| Step: 3
Training loss: 2.032806396484375
Validation loss: 2.03115584452947

Epoch: 6| Step: 4
Training loss: 1.95215904712677
Validation loss: 2.0401482780774436

Epoch: 6| Step: 5
Training loss: 1.6182373762130737
Validation loss: 2.0355932116508484

Epoch: 6| Step: 6
Training loss: 2.338995933532715
Validation loss: 2.0306273102760315

Epoch: 6| Step: 7
Training loss: 1.9087200164794922
Validation loss: 2.024777432282766

Epoch: 6| Step: 8
Training loss: 2.2581450939178467
Validation loss: 2.015595257282257

Epoch: 6| Step: 9
Training loss: 2.1994099617004395
Validation loss: 2.027718742688497

Epoch: 6| Step: 10
Training loss: 1.9005708694458008
Validation loss: 2.0423208276430764

Epoch: 6| Step: 11
Training loss: 2.3351643085479736
Validation loss: 2.0335174004236856

Epoch: 6| Step: 12
Training loss: 1.7418174743652344
Validation loss: 2.0384374260902405

Epoch: 6| Step: 13
Training loss: 2.2488856315612793
Validation loss: 2.017703394095103

Epoch: 98| Step: 0
Training loss: 2.4412684440612793
Validation loss: 2.0420093536376953

Epoch: 6| Step: 1
Training loss: 2.082564115524292
Validation loss: 2.0289039810498557

Epoch: 6| Step: 2
Training loss: 2.3663692474365234
Validation loss: 2.030542174975077

Epoch: 6| Step: 3
Training loss: 1.8883980512619019
Validation loss: 2.0160170197486877

Epoch: 6| Step: 4
Training loss: 1.8078184127807617
Validation loss: 2.0157713095347085

Epoch: 6| Step: 5
Training loss: 1.2928495407104492
Validation loss: 2.0208125710487366

Epoch: 6| Step: 6
Training loss: 2.759643077850342
Validation loss: 2.019054631392161

Epoch: 6| Step: 7
Training loss: 2.2653541564941406
Validation loss: 2.0141746203104653

Epoch: 6| Step: 8
Training loss: 2.4666903018951416
Validation loss: 2.0165264010429382

Epoch: 6| Step: 9
Training loss: 1.8199644088745117
Validation loss: 2.0226442019144693

Epoch: 6| Step: 10
Training loss: 1.7402015924453735
Validation loss: 2.0188655853271484

Epoch: 6| Step: 11
Training loss: 2.1723620891571045
Validation loss: 2.0165173411369324

Epoch: 6| Step: 12
Training loss: 2.6544198989868164
Validation loss: 2.0207812388738

Epoch: 6| Step: 13
Training loss: 1.922149419784546
Validation loss: 2.0198556979497275

Epoch: 99| Step: 0
Training loss: 2.4500107765197754
Validation loss: 2.0283358097076416

Epoch: 6| Step: 1
Training loss: 1.66324782371521
Validation loss: 2.0294132828712463

Epoch: 6| Step: 2
Training loss: 2.295320987701416
Validation loss: 2.0334888299306235

Epoch: 6| Step: 3
Training loss: 1.951037883758545
Validation loss: 2.0412158171335855

Epoch: 6| Step: 4
Training loss: 1.796325445175171
Validation loss: 2.0396804412206015

Epoch: 6| Step: 5
Training loss: 1.972654104232788
Validation loss: 2.0452829798062644

Epoch: 6| Step: 6
Training loss: 2.325040817260742
Validation loss: 2.036680301030477

Epoch: 6| Step: 7
Training loss: 2.0135977268218994
Validation loss: 2.035753150780996

Epoch: 6| Step: 8
Training loss: 2.587838888168335
Validation loss: 2.041996339956919

Epoch: 6| Step: 9
Training loss: 2.1784791946411133
Validation loss: 2.032489021619161

Epoch: 6| Step: 10
Training loss: 1.7542500495910645
Validation loss: 2.0324806769688926

Epoch: 6| Step: 11
Training loss: 2.102025032043457
Validation loss: 2.038627644379934

Epoch: 6| Step: 12
Training loss: 1.9764989614486694
Validation loss: 2.037932833035787

Epoch: 6| Step: 13
Training loss: 2.3829102516174316
Validation loss: 2.0331639647483826

Epoch: 100| Step: 0
Training loss: 2.33121657371521
Validation loss: 2.0355474948883057

Epoch: 6| Step: 1
Training loss: 2.566220283508301
Validation loss: 2.029320021470388

Epoch: 6| Step: 2
Training loss: 1.9042084217071533
Validation loss: 2.0404619773228965

Epoch: 6| Step: 3
Training loss: 1.9212318658828735
Validation loss: 2.035473028818766

Epoch: 6| Step: 4
Training loss: 2.086522102355957
Validation loss: 2.03151927391688

Epoch: 6| Step: 5
Training loss: 1.5845282077789307
Validation loss: 2.03193990389506

Epoch: 6| Step: 6
Training loss: 1.741511344909668
Validation loss: 2.0387441515922546

Epoch: 6| Step: 7
Training loss: 2.02601957321167
Validation loss: 2.0394705732663474

Epoch: 6| Step: 8
Training loss: 2.160210609436035
Validation loss: 2.0413280526796975

Epoch: 6| Step: 9
Training loss: 2.434317111968994
Validation loss: 2.0299083391825357

Epoch: 6| Step: 10
Training loss: 2.3299121856689453
Validation loss: 2.036065419514974

Epoch: 6| Step: 11
Training loss: 1.9895575046539307
Validation loss: 2.0361886620521545

Epoch: 6| Step: 12
Training loss: 1.8055751323699951
Validation loss: 2.0313498775164285

Epoch: 6| Step: 13
Training loss: 2.3574211597442627
Validation loss: 2.0328017274538674

Epoch: 101| Step: 0
Training loss: 2.680086135864258
Validation loss: 2.039299249649048

Epoch: 6| Step: 1
Training loss: 2.011408805847168
Validation loss: 2.0315136512120566

Epoch: 6| Step: 2
Training loss: 2.274719476699829
Validation loss: 2.044957439104716

Epoch: 6| Step: 3
Training loss: 2.2021098136901855
Validation loss: 2.0228806535402932

Epoch: 6| Step: 4
Training loss: 1.9954373836517334
Validation loss: 2.0218882958094277

Epoch: 6| Step: 5
Training loss: 1.5632424354553223
Validation loss: 2.028406023979187

Epoch: 6| Step: 6
Training loss: 2.206460475921631
Validation loss: 2.021769185860952

Epoch: 6| Step: 7
Training loss: 1.7785109281539917
Validation loss: 2.0274725755055747

Epoch: 6| Step: 8
Training loss: 1.8219175338745117
Validation loss: 2.025393803914388

Epoch: 6| Step: 9
Training loss: 2.2958335876464844
Validation loss: 2.0264023542404175

Epoch: 6| Step: 10
Training loss: 2.3464114665985107
Validation loss: 2.0222419102986655

Epoch: 6| Step: 11
Training loss: 2.373875141143799
Validation loss: 2.019409398237864

Epoch: 6| Step: 12
Training loss: 2.171692132949829
Validation loss: 2.02808811267217

Epoch: 6| Step: 13
Training loss: 2.002960205078125
Validation loss: 2.031824807325999

Epoch: 102| Step: 0
Training loss: 2.1618857383728027
Validation loss: 2.0317562421162925

Epoch: 6| Step: 1
Training loss: 2.0566678047180176
Validation loss: 2.030594766139984

Epoch: 6| Step: 2
Training loss: 1.9570302963256836
Validation loss: 2.031562864780426

Epoch: 6| Step: 3
Training loss: 1.9325106143951416
Validation loss: 2.029217084248861

Epoch: 6| Step: 4
Training loss: 2.565483808517456
Validation loss: 2.0223259329795837

Epoch: 6| Step: 5
Training loss: 2.3443691730499268
Validation loss: 2.026848018169403

Epoch: 6| Step: 6
Training loss: 2.319577932357788
Validation loss: 2.025415360927582

Epoch: 6| Step: 7
Training loss: 2.066713333129883
Validation loss: 2.025613566239675

Epoch: 6| Step: 8
Training loss: 2.2029027938842773
Validation loss: 2.029550095399221

Epoch: 6| Step: 9
Training loss: 1.9900124073028564
Validation loss: 2.01772932211558

Epoch: 6| Step: 10
Training loss: 1.5025379657745361
Validation loss: 2.0372711022694907

Epoch: 6| Step: 11
Training loss: 1.9266858100891113
Validation loss: 2.023702919483185

Epoch: 6| Step: 12
Training loss: 2.360896110534668
Validation loss: 2.0110466678937278

Epoch: 6| Step: 13
Training loss: 1.956512451171875
Validation loss: 2.0159796675046286

Epoch: 103| Step: 0
Training loss: 2.649606227874756
Validation loss: 2.0122934182484946

Epoch: 6| Step: 1
Training loss: 2.079690456390381
Validation loss: 2.018912603457769

Epoch: 6| Step: 2
Training loss: 1.7176265716552734
Validation loss: 2.0339243610699973

Epoch: 6| Step: 3
Training loss: 1.8129645586013794
Validation loss: 2.0309781432151794

Epoch: 6| Step: 4
Training loss: 1.8080604076385498
Validation loss: 2.0388091007868447

Epoch: 6| Step: 5
Training loss: 2.4958834648132324
Validation loss: 2.0221343835194907

Epoch: 6| Step: 6
Training loss: 2.19350528717041
Validation loss: 2.016385475794474

Epoch: 6| Step: 7
Training loss: 2.000514030456543
Validation loss: 2.003831764062246

Epoch: 6| Step: 8
Training loss: 2.3666696548461914
Validation loss: 2.0061084826787314

Epoch: 6| Step: 9
Training loss: 2.3362083435058594
Validation loss: 2.006860693295797

Epoch: 6| Step: 10
Training loss: 2.7505722045898438
Validation loss: 2.016550898551941

Epoch: 6| Step: 11
Training loss: 2.028843402862549
Validation loss: 2.0182732741038003

Epoch: 6| Step: 12
Training loss: 1.6723852157592773
Validation loss: 2.0177020033200583

Epoch: 6| Step: 13
Training loss: 1.9500353336334229
Validation loss: 2.0278947949409485

Epoch: 104| Step: 0
Training loss: 1.4176158905029297
Validation loss: 2.026998003323873

Epoch: 6| Step: 1
Training loss: 2.1554460525512695
Validation loss: 2.027186393737793

Epoch: 6| Step: 2
Training loss: 2.1359753608703613
Validation loss: 2.0328755180040994

Epoch: 6| Step: 3
Training loss: 2.806661605834961
Validation loss: 2.048010289669037

Epoch: 6| Step: 4
Training loss: 2.8881006240844727
Validation loss: 2.033175845940908

Epoch: 6| Step: 5
Training loss: 2.3187026977539062
Validation loss: 2.035106877485911

Epoch: 6| Step: 6
Training loss: 2.3980627059936523
Validation loss: 2.035957674185435

Epoch: 6| Step: 7
Training loss: 1.8217188119888306
Validation loss: 2.0313015580177307

Epoch: 6| Step: 8
Training loss: 2.0743961334228516
Validation loss: 2.0407845775286355

Epoch: 6| Step: 9
Training loss: 2.1528072357177734
Validation loss: 2.0486771861712136

Epoch: 6| Step: 10
Training loss: 1.9762762784957886
Validation loss: 2.040995935599009

Epoch: 6| Step: 11
Training loss: 1.4845168590545654
Validation loss: 2.053674578666687

Epoch: 6| Step: 12
Training loss: 2.0382614135742188
Validation loss: 2.031465689341227

Epoch: 6| Step: 13
Training loss: 1.7165849208831787
Validation loss: 2.0521766940752664

Epoch: 105| Step: 0
Training loss: 1.7646900415420532
Validation loss: 2.024691164493561

Epoch: 6| Step: 1
Training loss: 2.3848493099212646
Validation loss: 2.02148308356603

Epoch: 6| Step: 2
Training loss: 2.23042631149292
Validation loss: 2.03314737478892

Epoch: 6| Step: 3
Training loss: 2.3564252853393555
Validation loss: 2.0420438249905906

Epoch: 6| Step: 4
Training loss: 2.6250290870666504
Validation loss: 2.047735611597697

Epoch: 6| Step: 5
Training loss: 2.348851442337036
Validation loss: 2.0514358282089233

Epoch: 6| Step: 6
Training loss: 2.243405342102051
Validation loss: 2.055720031261444

Epoch: 6| Step: 7
Training loss: 1.9819772243499756
Validation loss: 2.0622103214263916

Epoch: 6| Step: 8
Training loss: 1.723222255706787
Validation loss: 2.068403899669647

Epoch: 6| Step: 9
Training loss: 2.2012481689453125
Validation loss: 2.072300434112549

Epoch: 6| Step: 10
Training loss: 2.393982172012329
Validation loss: 2.066387434800466

Epoch: 6| Step: 11
Training loss: 1.7396745681762695
Validation loss: 2.0724544525146484

Epoch: 6| Step: 12
Training loss: 1.939805030822754
Validation loss: 2.063989738623301

Epoch: 6| Step: 13
Training loss: 2.4110639095306396
Validation loss: 2.055554131666819

Epoch: 106| Step: 0
Training loss: 2.2756505012512207
Validation loss: 2.0515191555023193

Epoch: 6| Step: 1
Training loss: 1.6881098747253418
Validation loss: 2.0390666723251343

Epoch: 6| Step: 2
Training loss: 1.9606388807296753
Validation loss: 2.0314159194628396

Epoch: 6| Step: 3
Training loss: 2.2390193939208984
Validation loss: 2.0262784560521445

Epoch: 6| Step: 4
Training loss: 2.4863994121551514
Validation loss: 2.0124072233835855

Epoch: 6| Step: 5
Training loss: 2.461393356323242
Validation loss: 2.0195466677347818

Epoch: 6| Step: 6
Training loss: 2.374917984008789
Validation loss: 2.012957990169525

Epoch: 6| Step: 7
Training loss: 1.9499011039733887
Validation loss: 2.019041915734609

Epoch: 6| Step: 8
Training loss: 2.0889902114868164
Validation loss: 2.0260008772214255

Epoch: 6| Step: 9
Training loss: 1.5442837476730347
Validation loss: 2.025914986928304

Epoch: 6| Step: 10
Training loss: 2.2736220359802246
Validation loss: 2.0239049792289734

Epoch: 6| Step: 11
Training loss: 2.6810450553894043
Validation loss: 2.038453916708628

Epoch: 6| Step: 12
Training loss: 1.9300448894500732
Validation loss: 2.0416435599327087

Epoch: 6| Step: 13
Training loss: 2.5012502670288086
Validation loss: 2.0463919043540955

Epoch: 107| Step: 0
Training loss: 2.2211761474609375
Validation loss: 2.0477784474690757

Epoch: 6| Step: 1
Training loss: 2.0656566619873047
Validation loss: 2.0358428756395974

Epoch: 6| Step: 2
Training loss: 2.2185277938842773
Validation loss: 2.02221949895223

Epoch: 6| Step: 3
Training loss: 2.1341166496276855
Validation loss: 2.0305022597312927

Epoch: 6| Step: 4
Training loss: 2.000152587890625
Validation loss: 2.0141957799593606

Epoch: 6| Step: 5
Training loss: 2.003274440765381
Validation loss: 2.014661947886149

Epoch: 6| Step: 6
Training loss: 1.9825356006622314
Validation loss: 2.016562799612681

Epoch: 6| Step: 7
Training loss: 2.266993999481201
Validation loss: 2.0108721057573953

Epoch: 6| Step: 8
Training loss: 2.3033342361450195
Validation loss: 2.0077131589253745

Epoch: 6| Step: 9
Training loss: 2.089508533477783
Validation loss: 2.0082298517227173

Epoch: 6| Step: 10
Training loss: 2.156407356262207
Validation loss: 2.0099640091260276

Epoch: 6| Step: 11
Training loss: 2.121371030807495
Validation loss: 2.0128401716550193

Epoch: 6| Step: 12
Training loss: 2.4123435020446777
Validation loss: 2.0112457474072776

Epoch: 6| Step: 13
Training loss: 1.7831101417541504
Validation loss: 2.0121642549832663

Epoch: 108| Step: 0
Training loss: 2.2213311195373535
Validation loss: 2.0160059928894043

Epoch: 6| Step: 1
Training loss: 1.8910131454467773
Validation loss: 2.017862876256307

Epoch: 6| Step: 2
Training loss: 1.5471880435943604
Validation loss: 2.0193092624346414

Epoch: 6| Step: 3
Training loss: 2.2248988151550293
Validation loss: 2.01470015446345

Epoch: 6| Step: 4
Training loss: 1.9955554008483887
Validation loss: 2.0150180657704673

Epoch: 6| Step: 5
Training loss: 2.340207099914551
Validation loss: 2.028221905231476

Epoch: 6| Step: 6
Training loss: 2.174121379852295
Validation loss: 2.0209514697392783

Epoch: 6| Step: 7
Training loss: 1.8582165241241455
Validation loss: 2.0268425742785134

Epoch: 6| Step: 8
Training loss: 1.9666857719421387
Validation loss: 2.0265868107477822

Epoch: 6| Step: 9
Training loss: 1.8821032047271729
Validation loss: 2.0343963503837585

Epoch: 6| Step: 10
Training loss: 2.339576005935669
Validation loss: 2.0379950801531472

Epoch: 6| Step: 11
Training loss: 1.9757115840911865
Validation loss: 2.0379329323768616

Epoch: 6| Step: 12
Training loss: 2.259922981262207
Validation loss: 2.0341712633768716

Epoch: 6| Step: 13
Training loss: 2.6477155685424805
Validation loss: 2.049677054087321

Epoch: 109| Step: 0
Training loss: 1.6417642831802368
Validation loss: 2.036636233329773

Epoch: 6| Step: 1
Training loss: 2.0674877166748047
Validation loss: 2.0404051740964255

Epoch: 6| Step: 2
Training loss: 1.6908284425735474
Validation loss: 2.0384387175242105

Epoch: 6| Step: 3
Training loss: 2.8428709506988525
Validation loss: 2.0370093981424966

Epoch: 6| Step: 4
Training loss: 2.346885919570923
Validation loss: 2.0447317957878113

Epoch: 6| Step: 5
Training loss: 2.002687931060791
Validation loss: 2.0409271717071533

Epoch: 6| Step: 6
Training loss: 1.598099708557129
Validation loss: 2.0354489286740622

Epoch: 6| Step: 7
Training loss: 2.452815055847168
Validation loss: 2.0474092165629068

Epoch: 6| Step: 8
Training loss: 2.6015419960021973
Validation loss: 2.0407302180926004

Epoch: 6| Step: 9
Training loss: 1.5885487794876099
Validation loss: 2.037441909313202

Epoch: 6| Step: 10
Training loss: 2.2977566719055176
Validation loss: 2.033772865931193

Epoch: 6| Step: 11
Training loss: 1.9397896528244019
Validation loss: 2.0246766010920205

Epoch: 6| Step: 12
Training loss: 1.9347107410430908
Validation loss: 2.024438718954722

Epoch: 6| Step: 13
Training loss: 2.1588587760925293
Validation loss: 2.0185999075571694

Epoch: 110| Step: 0
Training loss: 2.222916841506958
Validation loss: 2.024721086025238

Epoch: 6| Step: 1
Training loss: 2.208228588104248
Validation loss: 2.0248323480288186

Epoch: 6| Step: 2
Training loss: 2.1329240798950195
Validation loss: 2.0234373211860657

Epoch: 6| Step: 3
Training loss: 2.402559757232666
Validation loss: 2.024008830388387

Epoch: 6| Step: 4
Training loss: 2.06034779548645
Validation loss: 2.0277730425198874

Epoch: 6| Step: 5
Training loss: 2.7539877891540527
Validation loss: 2.0240028500556946

Epoch: 6| Step: 6
Training loss: 2.08208966255188
Validation loss: 2.027221123377482

Epoch: 6| Step: 7
Training loss: 2.0352213382720947
Validation loss: 2.022854526837667

Epoch: 6| Step: 8
Training loss: 1.7910109758377075
Validation loss: 2.014848828315735

Epoch: 6| Step: 9
Training loss: 2.367809772491455
Validation loss: 2.013185660044352

Epoch: 6| Step: 10
Training loss: 1.7300097942352295
Validation loss: 2.013222873210907

Epoch: 6| Step: 11
Training loss: 1.7161026000976562
Validation loss: 2.009375274181366

Epoch: 6| Step: 12
Training loss: 2.0651869773864746
Validation loss: 2.0226822098096213

Epoch: 6| Step: 13
Training loss: 1.9415333271026611
Validation loss: 2.021356165409088

Epoch: 111| Step: 0
Training loss: 1.9682289361953735
Validation loss: 2.0304758151372275

Epoch: 6| Step: 1
Training loss: 1.8611278533935547
Validation loss: 2.022195299466451

Epoch: 6| Step: 2
Training loss: 2.119847297668457
Validation loss: 2.028094172477722

Epoch: 6| Step: 3
Training loss: 1.9439547061920166
Validation loss: 2.028220057487488

Epoch: 6| Step: 4
Training loss: 2.417069435119629
Validation loss: 2.0292861461639404

Epoch: 6| Step: 5
Training loss: 2.739193916320801
Validation loss: 2.0200511614481607

Epoch: 6| Step: 6
Training loss: 2.261387825012207
Validation loss: 2.030800978342692

Epoch: 6| Step: 7
Training loss: 1.6232966184616089
Validation loss: 2.0300625761349997

Epoch: 6| Step: 8
Training loss: 1.376499056816101
Validation loss: 2.0273316899935403

Epoch: 6| Step: 9
Training loss: 2.151491641998291
Validation loss: 2.021405577659607

Epoch: 6| Step: 10
Training loss: 1.6666972637176514
Validation loss: 2.035022715727488

Epoch: 6| Step: 11
Training loss: 2.747973918914795
Validation loss: 2.017873386542002

Epoch: 6| Step: 12
Training loss: 2.2352566719055176
Validation loss: 2.035814901192983

Epoch: 6| Step: 13
Training loss: 2.1419806480407715
Validation loss: 2.0220935344696045

Epoch: 112| Step: 0
Training loss: 2.4265570640563965
Validation loss: 2.020466427008311

Epoch: 6| Step: 1
Training loss: 1.1058447360992432
Validation loss: 2.0326244831085205

Epoch: 6| Step: 2
Training loss: 1.9028781652450562
Validation loss: 2.0222956935564675

Epoch: 6| Step: 3
Training loss: 2.494873523712158
Validation loss: 2.0023800134658813

Epoch: 6| Step: 4
Training loss: 2.0653445720672607
Validation loss: 2.0096582969029746

Epoch: 6| Step: 5
Training loss: 1.9913506507873535
Validation loss: 2.0188791950543723

Epoch: 6| Step: 6
Training loss: 2.179910659790039
Validation loss: 2.005902568499247

Epoch: 6| Step: 7
Training loss: 2.747694253921509
Validation loss: 2.0142931938171387

Epoch: 6| Step: 8
Training loss: 2.2972209453582764
Validation loss: 2.012680490811666

Epoch: 6| Step: 9
Training loss: 1.7743269205093384
Validation loss: 2.0183191100756326

Epoch: 6| Step: 10
Training loss: 1.893310308456421
Validation loss: 2.0156518618265786

Epoch: 6| Step: 11
Training loss: 2.272179126739502
Validation loss: 2.0198708375295005

Epoch: 6| Step: 12
Training loss: 2.0532174110412598
Validation loss: 2.028312385082245

Epoch: 6| Step: 13
Training loss: 2.0373101234436035
Validation loss: 2.0215306083361306

Epoch: 113| Step: 0
Training loss: 2.2909631729125977
Validation loss: 2.029339293638865

Epoch: 6| Step: 1
Training loss: 2.523449659347534
Validation loss: 2.0364314119021096

Epoch: 6| Step: 2
Training loss: 2.267198085784912
Validation loss: 2.0338205099105835

Epoch: 6| Step: 3
Training loss: 1.783036708831787
Validation loss: 2.034517288208008

Epoch: 6| Step: 4
Training loss: 2.0338006019592285
Validation loss: 2.0374661684036255

Epoch: 6| Step: 5
Training loss: 1.917450189590454
Validation loss: 2.0476430654525757

Epoch: 6| Step: 6
Training loss: 1.576486587524414
Validation loss: 2.0431270798047385

Epoch: 6| Step: 7
Training loss: 1.5564441680908203
Validation loss: 2.0397233168284097

Epoch: 6| Step: 8
Training loss: 2.8843560218811035
Validation loss: 2.043642063935598

Epoch: 6| Step: 9
Training loss: 1.861575961112976
Validation loss: 2.0448734561602273

Epoch: 6| Step: 10
Training loss: 2.52683687210083
Validation loss: 2.0384552280108132

Epoch: 6| Step: 11
Training loss: 2.4201066493988037
Validation loss: 2.030740658442179

Epoch: 6| Step: 12
Training loss: 2.151108980178833
Validation loss: 2.03364888827006

Epoch: 6| Step: 13
Training loss: 1.578261375427246
Validation loss: 2.028063098589579

Epoch: 114| Step: 0
Training loss: 2.0547924041748047
Validation loss: 2.0222063859303794

Epoch: 6| Step: 1
Training loss: 1.493199110031128
Validation loss: 2.0316147605578103

Epoch: 6| Step: 2
Training loss: 2.054347038269043
Validation loss: 2.024364789326986

Epoch: 6| Step: 3
Training loss: 2.515212059020996
Validation loss: 2.023858606815338

Epoch: 6| Step: 4
Training loss: 2.9420931339263916
Validation loss: 2.0240230560302734

Epoch: 6| Step: 5
Training loss: 2.256943941116333
Validation loss: 2.022578775882721

Epoch: 6| Step: 6
Training loss: 2.1830239295959473
Validation loss: 2.0263640880584717

Epoch: 6| Step: 7
Training loss: 2.1797773838043213
Validation loss: 2.0316004554430642

Epoch: 6| Step: 8
Training loss: 2.4977521896362305
Validation loss: 2.032070259253184

Epoch: 6| Step: 9
Training loss: 2.2295432090759277
Validation loss: 2.0401593248049417

Epoch: 6| Step: 10
Training loss: 2.1214394569396973
Validation loss: 2.0310482581456504

Epoch: 6| Step: 11
Training loss: 2.176797389984131
Validation loss: 2.0365461707115173

Epoch: 6| Step: 12
Training loss: 1.48133385181427
Validation loss: 2.03029465675354

Epoch: 6| Step: 13
Training loss: 1.1320165395736694
Validation loss: 2.0352344314257302

Epoch: 115| Step: 0
Training loss: 2.382397174835205
Validation loss: 2.0367050170898438

Epoch: 6| Step: 1
Training loss: 1.7714205980300903
Validation loss: 2.041778008143107

Epoch: 6| Step: 2
Training loss: 2.6039681434631348
Validation loss: 2.042044003804525

Epoch: 6| Step: 3
Training loss: 2.434253215789795
Validation loss: 2.045448581377665

Epoch: 6| Step: 4
Training loss: 2.1468653678894043
Validation loss: 2.0441569487253823

Epoch: 6| Step: 5
Training loss: 1.865429401397705
Validation loss: 2.0367825627326965

Epoch: 6| Step: 6
Training loss: 1.7759008407592773
Validation loss: 2.0389077067375183

Epoch: 6| Step: 7
Training loss: 1.7872393131256104
Validation loss: 2.0545008182525635

Epoch: 6| Step: 8
Training loss: 2.170135974884033
Validation loss: 2.0418758193651834

Epoch: 6| Step: 9
Training loss: 1.7328927516937256
Validation loss: 2.0520162185033164

Epoch: 6| Step: 10
Training loss: 2.171780586242676
Validation loss: 2.030634105205536

Epoch: 6| Step: 11
Training loss: 2.1398816108703613
Validation loss: 2.02997096379598

Epoch: 6| Step: 12
Training loss: 2.048327922821045
Validation loss: 2.0285101731618247

Epoch: 6| Step: 13
Training loss: 2.23736572265625
Validation loss: 2.0278744300206504

Epoch: 116| Step: 0
Training loss: 2.5311222076416016
Validation loss: 2.033771197001139

Epoch: 6| Step: 1
Training loss: 2.108128547668457
Validation loss: 2.0309343139330545

Epoch: 6| Step: 2
Training loss: 2.180990219116211
Validation loss: 2.032046377658844

Epoch: 6| Step: 3
Training loss: 1.9089903831481934
Validation loss: 2.0295262734095254

Epoch: 6| Step: 4
Training loss: 2.0375733375549316
Validation loss: 2.0341699520746865

Epoch: 6| Step: 5
Training loss: 2.4776015281677246
Validation loss: 2.0248897870381675

Epoch: 6| Step: 6
Training loss: 2.201676845550537
Validation loss: 2.022602677345276

Epoch: 6| Step: 7
Training loss: 2.7458243370056152
Validation loss: 2.0206631620724997

Epoch: 6| Step: 8
Training loss: 2.341654062271118
Validation loss: 2.0185601909955344

Epoch: 6| Step: 9
Training loss: 1.7046432495117188
Validation loss: 2.0168155233065286

Epoch: 6| Step: 10
Training loss: 1.617389440536499
Validation loss: 2.011683980623881

Epoch: 6| Step: 11
Training loss: 1.9978384971618652
Validation loss: 2.0113570292790732

Epoch: 6| Step: 12
Training loss: 2.122075319290161
Validation loss: 2.008945365746816

Epoch: 6| Step: 13
Training loss: 2.1258890628814697
Validation loss: 2.0177230834960938

Epoch: 117| Step: 0
Training loss: 1.8658652305603027
Validation loss: 2.0263598561286926

Epoch: 6| Step: 1
Training loss: 2.0788707733154297
Validation loss: 2.0241777896881104

Epoch: 6| Step: 2
Training loss: 1.915418267250061
Validation loss: 2.0336018006006875

Epoch: 6| Step: 3
Training loss: 1.6157593727111816
Validation loss: 2.0315219362576804

Epoch: 6| Step: 4
Training loss: 2.218630790710449
Validation loss: 2.0370922287305198

Epoch: 6| Step: 5
Training loss: 2.3658108711242676
Validation loss: 2.027110437552134

Epoch: 6| Step: 6
Training loss: 2.551361083984375
Validation loss: 2.0393060445785522

Epoch: 6| Step: 7
Training loss: 2.1983656883239746
Validation loss: 2.0349583427111306

Epoch: 6| Step: 8
Training loss: 2.276338577270508
Validation loss: 2.0291896065076194

Epoch: 6| Step: 9
Training loss: 2.421922206878662
Validation loss: 2.033059597015381

Epoch: 6| Step: 10
Training loss: 2.5232582092285156
Validation loss: 2.0379589398701987

Epoch: 6| Step: 11
Training loss: 1.9843140840530396
Validation loss: 2.0362033247947693

Epoch: 6| Step: 12
Training loss: 1.9873626232147217
Validation loss: 2.032481590906779

Epoch: 6| Step: 13
Training loss: 1.7076835632324219
Validation loss: 2.0333510438601174

Epoch: 118| Step: 0
Training loss: 1.7948195934295654
Validation loss: 2.0412443478902182

Epoch: 6| Step: 1
Training loss: 1.8463605642318726
Validation loss: 2.031877358754476

Epoch: 6| Step: 2
Training loss: 2.3308019638061523
Validation loss: 2.0435743729273477

Epoch: 6| Step: 3
Training loss: 2.1341922283172607
Validation loss: 2.029087801774343

Epoch: 6| Step: 4
Training loss: 2.1513872146606445
Validation loss: 2.043326258659363

Epoch: 6| Step: 5
Training loss: 1.9044376611709595
Validation loss: 2.0409286618232727

Epoch: 6| Step: 6
Training loss: 2.5290677547454834
Validation loss: 2.043060759703318

Epoch: 6| Step: 7
Training loss: 2.527160882949829
Validation loss: 2.049314479033152

Epoch: 6| Step: 8
Training loss: 1.997873067855835
Validation loss: 2.0486441055933633

Epoch: 6| Step: 9
Training loss: 1.8168880939483643
Validation loss: 2.0406321088473

Epoch: 6| Step: 10
Training loss: 1.9494810104370117
Validation loss: 2.038422962029775

Epoch: 6| Step: 11
Training loss: 2.3794095516204834
Validation loss: 2.033555726210276

Epoch: 6| Step: 12
Training loss: 1.7720450162887573
Validation loss: 2.042142311731974

Epoch: 6| Step: 13
Training loss: 2.0200421810150146
Validation loss: 2.0363523761431375

Epoch: 119| Step: 0
Training loss: 2.18162202835083
Validation loss: 2.029268125693003

Epoch: 6| Step: 1
Training loss: 1.8724290132522583
Validation loss: 2.042547285556793

Epoch: 6| Step: 2
Training loss: 2.3508353233337402
Validation loss: 2.0430198907852173

Epoch: 6| Step: 3
Training loss: 2.404346227645874
Validation loss: 2.0351823767026267

Epoch: 6| Step: 4
Training loss: 2.348641872406006
Validation loss: 2.0327779054641724

Epoch: 6| Step: 5
Training loss: 2.094897747039795
Validation loss: 2.033652047316233

Epoch: 6| Step: 6
Training loss: 1.5849014520645142
Validation loss: 2.040706992149353

Epoch: 6| Step: 7
Training loss: 2.1833109855651855
Validation loss: 2.0296994845072427

Epoch: 6| Step: 8
Training loss: 1.5934163331985474
Validation loss: 2.0240288178126016

Epoch: 6| Step: 9
Training loss: 1.8800759315490723
Validation loss: 2.0329518715540567

Epoch: 6| Step: 10
Training loss: 1.5931217670440674
Validation loss: 2.0257696906725564

Epoch: 6| Step: 11
Training loss: 2.3720178604125977
Validation loss: 2.0427055756251016

Epoch: 6| Step: 12
Training loss: 1.8695714473724365
Validation loss: 2.0394074122111

Epoch: 6| Step: 13
Training loss: 2.7447757720947266
Validation loss: 2.0422722895940146

Epoch: 120| Step: 0
Training loss: 1.6816270351409912
Validation loss: 2.037485202153524

Epoch: 6| Step: 1
Training loss: 2.1509509086608887
Validation loss: 2.0397913455963135

Epoch: 6| Step: 2
Training loss: 1.81560480594635
Validation loss: 2.0413041512171426

Epoch: 6| Step: 3
Training loss: 2.1876983642578125
Validation loss: 2.041120946407318

Epoch: 6| Step: 4
Training loss: 1.9859223365783691
Validation loss: 2.049681822458903

Epoch: 6| Step: 5
Training loss: 2.2785277366638184
Validation loss: 2.0450644493103027

Epoch: 6| Step: 6
Training loss: 1.8709272146224976
Validation loss: 2.041106025377909

Epoch: 6| Step: 7
Training loss: 2.4233927726745605
Validation loss: 2.0471091667811074

Epoch: 6| Step: 8
Training loss: 2.3489606380462646
Validation loss: 2.0457900961240134

Epoch: 6| Step: 9
Training loss: 1.9690548181533813
Validation loss: 2.0503265460332236

Epoch: 6| Step: 10
Training loss: 1.7535516023635864
Validation loss: 2.049376904964447

Epoch: 6| Step: 11
Training loss: 2.142815113067627
Validation loss: 2.0420618057250977

Epoch: 6| Step: 12
Training loss: 1.7073942422866821
Validation loss: 2.0578909317652383

Epoch: 6| Step: 13
Training loss: 2.6480746269226074
Validation loss: 2.0472043553988137

Epoch: 121| Step: 0
Training loss: 2.2912497520446777
Validation loss: 2.0399502515792847

Epoch: 6| Step: 1
Training loss: 1.610329508781433
Validation loss: 2.0372382005055747

Epoch: 6| Step: 2
Training loss: 1.827659010887146
Validation loss: 2.038682301839193

Epoch: 6| Step: 3
Training loss: 1.910840392112732
Validation loss: 2.0302732785542807

Epoch: 6| Step: 4
Training loss: 1.6722601652145386
Validation loss: 2.0323251684506736

Epoch: 6| Step: 5
Training loss: 2.415956974029541
Validation loss: 2.0348521073659263

Epoch: 6| Step: 6
Training loss: 1.6598763465881348
Validation loss: 2.040046811103821

Epoch: 6| Step: 7
Training loss: 1.9095128774642944
Validation loss: 2.033389468987783

Epoch: 6| Step: 8
Training loss: 2.3311359882354736
Validation loss: 2.041383981704712

Epoch: 6| Step: 9
Training loss: 2.3259873390197754
Validation loss: 2.033340315024058

Epoch: 6| Step: 10
Training loss: 2.261664628982544
Validation loss: 2.042599101861318

Epoch: 6| Step: 11
Training loss: 2.311722755432129
Validation loss: 2.0499000946680703

Epoch: 6| Step: 12
Training loss: 1.4433821439743042
Validation loss: 2.045329829057058

Epoch: 6| Step: 13
Training loss: 2.7662439346313477
Validation loss: 2.0520280400911965

Epoch: 122| Step: 0
Training loss: 1.7770860195159912
Validation loss: 2.0388927261034646

Epoch: 6| Step: 1
Training loss: 2.17153263092041
Validation loss: 2.0458390514055886

Epoch: 6| Step: 2
Training loss: 2.131685256958008
Validation loss: 2.028484026590983

Epoch: 6| Step: 3
Training loss: 1.5900046825408936
Validation loss: 2.045024593671163

Epoch: 6| Step: 4
Training loss: 2.2588441371917725
Validation loss: 2.0429784059524536

Epoch: 6| Step: 5
Training loss: 2.0435595512390137
Validation loss: 2.045088748137156

Epoch: 6| Step: 6
Training loss: 2.4667558670043945
Validation loss: 2.0392856995264688

Epoch: 6| Step: 7
Training loss: 2.1317763328552246
Validation loss: 2.0550378561019897

Epoch: 6| Step: 8
Training loss: 2.4697561264038086
Validation loss: 2.044065455595652

Epoch: 6| Step: 9
Training loss: 1.7336313724517822
Validation loss: 2.041843076546987

Epoch: 6| Step: 10
Training loss: 1.9318523406982422
Validation loss: 2.0501555601755777

Epoch: 6| Step: 11
Training loss: 1.7970833778381348
Validation loss: 2.0613551338513694

Epoch: 6| Step: 12
Training loss: 1.8812912702560425
Validation loss: 2.0412305196126304

Epoch: 6| Step: 13
Training loss: 2.5832290649414062
Validation loss: 2.0392253398895264

Epoch: 123| Step: 0
Training loss: 1.7296676635742188
Validation loss: 2.0496448477109275

Epoch: 6| Step: 1
Training loss: 2.3945908546447754
Validation loss: 2.0327855944633484

Epoch: 6| Step: 2
Training loss: 1.6422888040542603
Validation loss: 2.047447164853414

Epoch: 6| Step: 3
Training loss: 1.9630532264709473
Validation loss: 2.0393624107042947

Epoch: 6| Step: 4
Training loss: 2.123133897781372
Validation loss: 2.0329445203145347

Epoch: 6| Step: 5
Training loss: 1.4711735248565674
Validation loss: 2.0421502391497293

Epoch: 6| Step: 6
Training loss: 2.2778372764587402
Validation loss: 2.045477271080017

Epoch: 6| Step: 7
Training loss: 2.1625638008117676
Validation loss: 2.0529163678487143

Epoch: 6| Step: 8
Training loss: 2.1564645767211914
Validation loss: 2.0545961459477744

Epoch: 6| Step: 9
Training loss: 2.5928497314453125
Validation loss: 2.0400510430336

Epoch: 6| Step: 10
Training loss: 2.3165550231933594
Validation loss: 2.0547765096028647

Epoch: 6| Step: 11
Training loss: 1.8873801231384277
Validation loss: 2.0514794985453286

Epoch: 6| Step: 12
Training loss: 2.169642210006714
Validation loss: 2.058708926041921

Epoch: 6| Step: 13
Training loss: 1.7678937911987305
Validation loss: 2.055172781149546

Epoch: 124| Step: 0
Training loss: 1.9374990463256836
Validation loss: 2.0658907691637673

Epoch: 6| Step: 1
Training loss: 2.3515465259552
Validation loss: 2.0546185771624246

Epoch: 6| Step: 2
Training loss: 2.456845283508301
Validation loss: 2.05527796347936

Epoch: 6| Step: 3
Training loss: 1.9055449962615967
Validation loss: 2.05023060242335

Epoch: 6| Step: 4
Training loss: 1.7500289678573608
Validation loss: 2.047094146410624

Epoch: 6| Step: 5
Training loss: 1.823323130607605
Validation loss: 2.0408188303311667

Epoch: 6| Step: 6
Training loss: 1.7912564277648926
Validation loss: 2.049522817134857

Epoch: 6| Step: 7
Training loss: 1.6877442598342896
Validation loss: 2.0419949094454446

Epoch: 6| Step: 8
Training loss: 2.324268341064453
Validation loss: 2.041555404663086

Epoch: 6| Step: 9
Training loss: 2.1730337142944336
Validation loss: 2.046273708343506

Epoch: 6| Step: 10
Training loss: 1.4912172555923462
Validation loss: 2.0545175075531006

Epoch: 6| Step: 11
Training loss: 3.0289251804351807
Validation loss: 2.0639134645462036

Epoch: 6| Step: 12
Training loss: 2.3999202251434326
Validation loss: 2.060051520665487

Epoch: 6| Step: 13
Training loss: 1.958391547203064
Validation loss: 2.054575204849243

Epoch: 125| Step: 0
Training loss: 2.239652633666992
Validation loss: 2.0665072997411094

Epoch: 6| Step: 1
Training loss: 2.0211267471313477
Validation loss: 2.05413955450058

Epoch: 6| Step: 2
Training loss: 1.8901575803756714
Validation loss: 2.0517710049947104

Epoch: 6| Step: 3
Training loss: 2.111236810684204
Validation loss: 2.063058535257975

Epoch: 6| Step: 4
Training loss: 1.672260046005249
Validation loss: 2.059012214342753

Epoch: 6| Step: 5
Training loss: 2.0270307064056396
Validation loss: 2.05572517712911

Epoch: 6| Step: 6
Training loss: 2.2650692462921143
Validation loss: 2.043861210346222

Epoch: 6| Step: 7
Training loss: 2.3756723403930664
Validation loss: 2.046350638071696

Epoch: 6| Step: 8
Training loss: 1.7564541101455688
Validation loss: 2.0388609170913696

Epoch: 6| Step: 9
Training loss: 2.33073091506958
Validation loss: 2.029084881146749

Epoch: 6| Step: 10
Training loss: 1.89249587059021
Validation loss: 2.0240487257639566

Epoch: 6| Step: 11
Training loss: 1.9040513038635254
Validation loss: 2.0245327949523926

Epoch: 6| Step: 12
Training loss: 2.121462345123291
Validation loss: 2.027360995610555

Epoch: 6| Step: 13
Training loss: 2.220219612121582
Validation loss: 2.0173670450846353

Epoch: 126| Step: 0
Training loss: 2.0824062824249268
Validation loss: 2.0261298616727195

Epoch: 6| Step: 1
Training loss: 2.3452677726745605
Validation loss: 2.0224192142486572

Epoch: 6| Step: 2
Training loss: 1.6859374046325684
Validation loss: 2.0361668864885965

Epoch: 6| Step: 3
Training loss: 2.335653305053711
Validation loss: 2.0395777225494385

Epoch: 6| Step: 4
Training loss: 2.3192481994628906
Validation loss: 2.047610580921173

Epoch: 6| Step: 5
Training loss: 1.8441734313964844
Validation loss: 2.0622480710347495

Epoch: 6| Step: 6
Training loss: 1.6670479774475098
Validation loss: 2.0622374018033347

Epoch: 6| Step: 7
Training loss: 2.2437052726745605
Validation loss: 2.051732818285624

Epoch: 6| Step: 8
Training loss: 2.6456656455993652
Validation loss: 2.04010401169459

Epoch: 6| Step: 9
Training loss: 2.0341739654541016
Validation loss: 2.04276180267334

Epoch: 6| Step: 10
Training loss: 2.2503247261047363
Validation loss: 2.04238231976827

Epoch: 6| Step: 11
Training loss: 1.9333741664886475
Validation loss: 2.0429518620173135

Epoch: 6| Step: 12
Training loss: 2.0267412662506104
Validation loss: 2.0406688253084817

Epoch: 6| Step: 13
Training loss: 1.7010972499847412
Validation loss: 2.0338255961736045

Epoch: 127| Step: 0
Training loss: 1.5567317008972168
Validation loss: 2.04204785823822

Epoch: 6| Step: 1
Training loss: 1.7433161735534668
Validation loss: 2.0343255201975503

Epoch: 6| Step: 2
Training loss: 1.8946690559387207
Validation loss: 2.0313441356023154

Epoch: 6| Step: 3
Training loss: 2.0097219944000244
Validation loss: 2.0239731669425964

Epoch: 6| Step: 4
Training loss: 2.045414447784424
Validation loss: 2.035853902498881

Epoch: 6| Step: 5
Training loss: 1.4656956195831299
Validation loss: 2.0198028286298118

Epoch: 6| Step: 6
Training loss: 2.2910757064819336
Validation loss: 2.0270268321037292

Epoch: 6| Step: 7
Training loss: 2.4860541820526123
Validation loss: 2.028580685456594

Epoch: 6| Step: 8
Training loss: 2.5779688358306885
Validation loss: 2.0235948165257773

Epoch: 6| Step: 9
Training loss: 1.9294426441192627
Validation loss: 2.0173632701238

Epoch: 6| Step: 10
Training loss: 2.3812921047210693
Validation loss: 2.020580271879832

Epoch: 6| Step: 11
Training loss: 1.7564735412597656
Validation loss: 2.0249056220054626

Epoch: 6| Step: 12
Training loss: 1.6278533935546875
Validation loss: 2.0208695928255715

Epoch: 6| Step: 13
Training loss: 3.242367744445801
Validation loss: 2.052255709966024

Epoch: 128| Step: 0
Training loss: 1.7885887622833252
Validation loss: 2.063712775707245

Epoch: 6| Step: 1
Training loss: 1.4262843132019043
Validation loss: 2.0611873865127563

Epoch: 6| Step: 2
Training loss: 1.8682842254638672
Validation loss: 2.050411065419515

Epoch: 6| Step: 3
Training loss: 2.579298734664917
Validation loss: 2.040414114793142

Epoch: 6| Step: 4
Training loss: 2.0906031131744385
Validation loss: 2.033386985460917

Epoch: 6| Step: 5
Training loss: 2.172287940979004
Validation loss: 2.0414287447929382

Epoch: 6| Step: 6
Training loss: 1.8349993228912354
Validation loss: 2.042950610319773

Epoch: 6| Step: 7
Training loss: 1.8377646207809448
Validation loss: 2.038661082585653

Epoch: 6| Step: 8
Training loss: 2.126645565032959
Validation loss: 2.0475276708602905

Epoch: 6| Step: 9
Training loss: 2.7250537872314453
Validation loss: 2.0500681400299072

Epoch: 6| Step: 10
Training loss: 2.300060749053955
Validation loss: 2.0580214460690818

Epoch: 6| Step: 11
Training loss: 2.142340660095215
Validation loss: 2.0576502084732056

Epoch: 6| Step: 12
Training loss: 2.161930561065674
Validation loss: 2.0650871992111206

Epoch: 6| Step: 13
Training loss: 1.7667492628097534
Validation loss: 2.059359093507131

Epoch: 129| Step: 0
Training loss: 2.2596452236175537
Validation loss: 2.062014897664388

Epoch: 6| Step: 1
Training loss: 1.9644116163253784
Validation loss: 2.0877802769343057

Epoch: 6| Step: 2
Training loss: 2.0086886882781982
Validation loss: 2.0758626461029053

Epoch: 6| Step: 3
Training loss: 2.1064367294311523
Validation loss: 2.0727227528889975

Epoch: 6| Step: 4
Training loss: 2.1298975944519043
Validation loss: 2.0571404298146567

Epoch: 6| Step: 5
Training loss: 1.7382639646530151
Validation loss: 2.0625062584877014

Epoch: 6| Step: 6
Training loss: 2.205150604248047
Validation loss: 2.0656742453575134

Epoch: 6| Step: 7
Training loss: 1.6229023933410645
Validation loss: 2.0574148694674173

Epoch: 6| Step: 8
Training loss: 1.795725703239441
Validation loss: 2.0559412638346353

Epoch: 6| Step: 9
Training loss: 1.8543822765350342
Validation loss: 2.0503718058268228

Epoch: 6| Step: 10
Training loss: 2.150305986404419
Validation loss: 2.0520795981089273

Epoch: 6| Step: 11
Training loss: 2.669365882873535
Validation loss: 2.051651736100515

Epoch: 6| Step: 12
Training loss: 2.5268940925598145
Validation loss: 2.063188850879669

Epoch: 6| Step: 13
Training loss: 1.5852222442626953
Validation loss: 2.0524797836939492

Epoch: 130| Step: 0
Training loss: 1.9423813819885254
Validation loss: 2.073056717713674

Epoch: 6| Step: 1
Training loss: 1.9243916273117065
Validation loss: 2.049607038497925

Epoch: 6| Step: 2
Training loss: 1.8308119773864746
Validation loss: 2.0574856400489807

Epoch: 6| Step: 3
Training loss: 1.9716899394989014
Validation loss: 2.0545544823010764

Epoch: 6| Step: 4
Training loss: 1.6045665740966797
Validation loss: 2.0479915738105774

Epoch: 6| Step: 5
Training loss: 1.3163745403289795
Validation loss: 2.0460775891939798

Epoch: 6| Step: 6
Training loss: 1.8689792156219482
Validation loss: 2.0457650423049927

Epoch: 6| Step: 7
Training loss: 2.12479567527771
Validation loss: 2.044592638810476

Epoch: 6| Step: 8
Training loss: 2.1790568828582764
Validation loss: 2.0524562994639077

Epoch: 6| Step: 9
Training loss: 3.399681568145752
Validation loss: 2.0485716462135315

Epoch: 6| Step: 10
Training loss: 2.044992685317993
Validation loss: 2.0511385599772134

Epoch: 6| Step: 11
Training loss: 1.8942694664001465
Validation loss: 2.063256025314331

Epoch: 6| Step: 12
Training loss: 1.6995059251785278
Validation loss: 2.0483017961184182

Epoch: 6| Step: 13
Training loss: 3.0750744342803955
Validation loss: 2.050187567869822

Epoch: 131| Step: 0
Training loss: 2.3218994140625
Validation loss: 2.063496172428131

Epoch: 6| Step: 1
Training loss: 1.90200674533844
Validation loss: 2.0681022008260093

Epoch: 6| Step: 2
Training loss: 2.4057059288024902
Validation loss: 2.0531963109970093

Epoch: 6| Step: 3
Training loss: 2.2946605682373047
Validation loss: 2.068449079990387

Epoch: 6| Step: 4
Training loss: 2.133892059326172
Validation loss: 2.074359655380249

Epoch: 6| Step: 5
Training loss: 2.0169715881347656
Validation loss: 2.0652079383532205

Epoch: 6| Step: 6
Training loss: 2.426698684692383
Validation loss: 2.068907002607981

Epoch: 6| Step: 7
Training loss: 1.5605332851409912
Validation loss: 2.064553916454315

Epoch: 6| Step: 8
Training loss: 1.6503926515579224
Validation loss: 2.0490166743596396

Epoch: 6| Step: 9
Training loss: 1.5805087089538574
Validation loss: 2.061999420324961

Epoch: 6| Step: 10
Training loss: 1.9068437814712524
Validation loss: 2.0656391382217407

Epoch: 6| Step: 11
Training loss: 1.9175612926483154
Validation loss: 2.0536726117134094

Epoch: 6| Step: 12
Training loss: 2.0361297130584717
Validation loss: 2.0515588323275247

Epoch: 6| Step: 13
Training loss: 2.538113594055176
Validation loss: 2.0526687701543174

Epoch: 132| Step: 0
Training loss: 1.430419921875
Validation loss: 2.0457099080085754

Epoch: 6| Step: 1
Training loss: 1.7274152040481567
Validation loss: 2.0505951642990112

Epoch: 6| Step: 2
Training loss: 2.0429296493530273
Validation loss: 2.042179743448893

Epoch: 6| Step: 3
Training loss: 2.223754405975342
Validation loss: 2.0533268451690674

Epoch: 6| Step: 4
Training loss: 1.9921315908432007
Validation loss: 2.056262036164602

Epoch: 6| Step: 5
Training loss: 2.2794129848480225
Validation loss: 2.063002586364746

Epoch: 6| Step: 6
Training loss: 2.3185842037200928
Validation loss: 2.0662864645322165

Epoch: 6| Step: 7
Training loss: 1.8788458108901978
Validation loss: 2.0599846641222634

Epoch: 6| Step: 8
Training loss: 2.1259207725524902
Validation loss: 2.0643766125043235

Epoch: 6| Step: 9
Training loss: 2.522888660430908
Validation loss: 2.074565052986145

Epoch: 6| Step: 10
Training loss: 1.915229320526123
Validation loss: 2.0697827537854514

Epoch: 6| Step: 11
Training loss: 2.676091194152832
Validation loss: 2.0741085012753806

Epoch: 6| Step: 12
Training loss: 1.6632404327392578
Validation loss: 2.056970020135244

Epoch: 6| Step: 13
Training loss: 1.9608083963394165
Validation loss: 2.0543997486432395

Epoch: 133| Step: 0
Training loss: 2.3153204917907715
Validation loss: 2.0496091842651367

Epoch: 6| Step: 1
Training loss: 2.172013282775879
Validation loss: 2.0715227127075195

Epoch: 6| Step: 2
Training loss: 2.5900447368621826
Validation loss: 2.061043918132782

Epoch: 6| Step: 3
Training loss: 1.843592643737793
Validation loss: 2.0576884150505066

Epoch: 6| Step: 4
Training loss: 2.4832963943481445
Validation loss: 2.064775546391805

Epoch: 6| Step: 5
Training loss: 1.9807360172271729
Validation loss: 2.060701370239258

Epoch: 6| Step: 6
Training loss: 1.9114160537719727
Validation loss: 2.070569157600403

Epoch: 6| Step: 7
Training loss: 1.7342290878295898
Validation loss: 2.0559997161229453

Epoch: 6| Step: 8
Training loss: 1.883407473564148
Validation loss: 2.0712838967641196

Epoch: 6| Step: 9
Training loss: 1.9889817237854004
Validation loss: 2.062226394812266

Epoch: 6| Step: 10
Training loss: 2.030651569366455
Validation loss: 2.072901666164398

Epoch: 6| Step: 11
Training loss: 2.3410964012145996
Validation loss: 2.072932759920756

Epoch: 6| Step: 12
Training loss: 1.5471017360687256
Validation loss: 2.0706658363342285

Epoch: 6| Step: 13
Training loss: 1.828514814376831
Validation loss: 2.0688207546869912

Epoch: 134| Step: 0
Training loss: 1.5557600259780884
Validation loss: 2.0689749320348105

Epoch: 6| Step: 1
Training loss: 2.4542396068573
Validation loss: 2.0789058208465576

Epoch: 6| Step: 2
Training loss: 2.1649327278137207
Validation loss: 2.079243024190267

Epoch: 6| Step: 3
Training loss: 2.301255464553833
Validation loss: 2.0784136056900024

Epoch: 6| Step: 4
Training loss: 1.87296462059021
Validation loss: 2.0728830893834433

Epoch: 6| Step: 5
Training loss: 1.4050188064575195
Validation loss: 2.090208113193512

Epoch: 6| Step: 6
Training loss: 2.1187572479248047
Validation loss: 2.082870920499166

Epoch: 6| Step: 7
Training loss: 2.6561050415039062
Validation loss: 2.0811063249905906

Epoch: 6| Step: 8
Training loss: 2.0017549991607666
Validation loss: 2.064947704474131

Epoch: 6| Step: 9
Training loss: 1.8867088556289673
Validation loss: 2.068716605504354

Epoch: 6| Step: 10
Training loss: 1.9563772678375244
Validation loss: 2.0785590410232544

Epoch: 6| Step: 11
Training loss: 1.9336763620376587
Validation loss: 2.0655155777931213

Epoch: 6| Step: 12
Training loss: 2.3641600608825684
Validation loss: 2.0776997804641724

Epoch: 6| Step: 13
Training loss: 1.812544345855713
Validation loss: 2.0841737588246665

Epoch: 135| Step: 0
Training loss: 1.963932991027832
Validation loss: 2.064896543820699

Epoch: 6| Step: 1
Training loss: 1.9412482976913452
Validation loss: 2.0742902755737305

Epoch: 6| Step: 2
Training loss: 1.7744073867797852
Validation loss: 2.0669161478678384

Epoch: 6| Step: 3
Training loss: 1.9278957843780518
Validation loss: 2.059096395969391

Epoch: 6| Step: 4
Training loss: 2.458528518676758
Validation loss: 2.0657471815745034

Epoch: 6| Step: 5
Training loss: 2.4203290939331055
Validation loss: 2.0696887373924255

Epoch: 6| Step: 6
Training loss: 1.3679172992706299
Validation loss: 2.0621240735054016

Epoch: 6| Step: 7
Training loss: 2.2247314453125
Validation loss: 2.0688621401786804

Epoch: 6| Step: 8
Training loss: 1.5207017660140991
Validation loss: 2.062343716621399

Epoch: 6| Step: 9
Training loss: 2.4071204662323
Validation loss: 2.0556710958480835

Epoch: 6| Step: 10
Training loss: 1.7114088535308838
Validation loss: 2.067192852497101

Epoch: 6| Step: 11
Training loss: 2.601630210876465
Validation loss: 2.0541534225145974

Epoch: 6| Step: 12
Training loss: 2.0505928993225098
Validation loss: 2.0506113171577454

Epoch: 6| Step: 13
Training loss: 2.135952949523926
Validation loss: 2.0567051768302917

Epoch: 136| Step: 0
Training loss: 2.0263664722442627
Validation loss: 2.042295734087626

Epoch: 6| Step: 1
Training loss: 1.8272819519042969
Validation loss: 2.0382325053215027

Epoch: 6| Step: 2
Training loss: 1.8619911670684814
Validation loss: 2.052131632963816

Epoch: 6| Step: 3
Training loss: 1.9968926906585693
Validation loss: 2.050788144270579

Epoch: 6| Step: 4
Training loss: 2.375211715698242
Validation loss: 2.0530288219451904

Epoch: 6| Step: 5
Training loss: 2.257133722305298
Validation loss: 2.0560702284177146

Epoch: 6| Step: 6
Training loss: 2.027373790740967
Validation loss: 2.0557871659596763

Epoch: 6| Step: 7
Training loss: 2.2785513401031494
Validation loss: 2.064905067284902

Epoch: 6| Step: 8
Training loss: 2.223374843597412
Validation loss: 2.063331683476766

Epoch: 6| Step: 9
Training loss: 1.96311354637146
Validation loss: 2.073323984940847

Epoch: 6| Step: 10
Training loss: 1.9107760190963745
Validation loss: 2.0918884873390198

Epoch: 6| Step: 11
Training loss: 2.447309732437134
Validation loss: 2.094355742136637

Epoch: 6| Step: 12
Training loss: 2.2249460220336914
Validation loss: 2.10523655017217

Epoch: 6| Step: 13
Training loss: 2.016322374343872
Validation loss: 2.096642037232717

Epoch: 137| Step: 0
Training loss: 2.078716993331909
Validation loss: 2.0902550419171653

Epoch: 6| Step: 1
Training loss: 1.8420668840408325
Validation loss: 2.0810811519622803

Epoch: 6| Step: 2
Training loss: 2.2582216262817383
Validation loss: 2.078716814517975

Epoch: 6| Step: 3
Training loss: 2.2773380279541016
Validation loss: 2.059838076432546

Epoch: 6| Step: 4
Training loss: 1.5068094730377197
Validation loss: 2.0604642430941262

Epoch: 6| Step: 5
Training loss: 1.8104534149169922
Validation loss: 2.052747110525767

Epoch: 6| Step: 6
Training loss: 1.5287672281265259
Validation loss: 2.0512704451878867

Epoch: 6| Step: 7
Training loss: 2.357631206512451
Validation loss: 2.0498953461647034

Epoch: 6| Step: 8
Training loss: 1.8220181465148926
Validation loss: 2.050818979740143

Epoch: 6| Step: 9
Training loss: 2.479584217071533
Validation loss: 2.041032552719116

Epoch: 6| Step: 10
Training loss: 1.6841365098953247
Validation loss: 2.045993765195211

Epoch: 6| Step: 11
Training loss: 2.9330010414123535
Validation loss: 2.0364151199658713

Epoch: 6| Step: 12
Training loss: 2.2493577003479004
Validation loss: 2.0545268456141152

Epoch: 6| Step: 13
Training loss: 2.0376524925231934
Validation loss: 2.0497212409973145

Epoch: 138| Step: 0
Training loss: 1.7519633769989014
Validation loss: 2.031019449234009

Epoch: 6| Step: 1
Training loss: 1.791142225265503
Validation loss: 2.0487703680992126

Epoch: 6| Step: 2
Training loss: 1.8554296493530273
Validation loss: 2.04618908961614

Epoch: 6| Step: 3
Training loss: 2.406661033630371
Validation loss: 2.0451048016548157

Epoch: 6| Step: 4
Training loss: 2.117784023284912
Validation loss: 2.0543046991030374

Epoch: 6| Step: 5
Training loss: 1.8192297220230103
Validation loss: 2.0595357418060303

Epoch: 6| Step: 6
Training loss: 2.397268056869507
Validation loss: 2.0643480817476907

Epoch: 6| Step: 7
Training loss: 2.3871240615844727
Validation loss: 2.049004077911377

Epoch: 6| Step: 8
Training loss: 1.6990727186203003
Validation loss: 2.058881143728892

Epoch: 6| Step: 9
Training loss: 2.273858070373535
Validation loss: 2.054757297039032

Epoch: 6| Step: 10
Training loss: 1.7858343124389648
Validation loss: 2.062609632809957

Epoch: 6| Step: 11
Training loss: 1.741654872894287
Validation loss: 2.066808740297953

Epoch: 6| Step: 12
Training loss: 2.2920708656311035
Validation loss: 2.054919938246409

Epoch: 6| Step: 13
Training loss: 2.0650506019592285
Validation loss: 2.0600898464520774

Epoch: 139| Step: 0
Training loss: 2.0030109882354736
Validation loss: 2.0591168999671936

Epoch: 6| Step: 1
Training loss: 2.095188856124878
Validation loss: 2.0687642693519592

Epoch: 6| Step: 2
Training loss: 1.3207650184631348
Validation loss: 2.064395248889923

Epoch: 6| Step: 3
Training loss: 1.9303319454193115
Validation loss: 2.0659956534703574

Epoch: 6| Step: 4
Training loss: 1.3541436195373535
Validation loss: 2.0751867294311523

Epoch: 6| Step: 5
Training loss: 2.064711570739746
Validation loss: 2.0777421991030374

Epoch: 6| Step: 6
Training loss: 2.3017377853393555
Validation loss: 2.069717983404795

Epoch: 6| Step: 7
Training loss: 2.0764148235321045
Validation loss: 2.07350891828537

Epoch: 6| Step: 8
Training loss: 1.603799819946289
Validation loss: 2.0809549689292908

Epoch: 6| Step: 9
Training loss: 2.9943666458129883
Validation loss: 2.0707874496777854

Epoch: 6| Step: 10
Training loss: 2.250490188598633
Validation loss: 2.071005324522654

Epoch: 6| Step: 11
Training loss: 1.8127150535583496
Validation loss: 2.0392550230026245

Epoch: 6| Step: 12
Training loss: 1.7274197340011597
Validation loss: 2.0462642709414163

Epoch: 6| Step: 13
Training loss: 2.8965187072753906
Validation loss: 2.0419657230377197

Epoch: 140| Step: 0
Training loss: 1.9245097637176514
Validation loss: 2.041735053062439

Epoch: 6| Step: 1
Training loss: 2.3130784034729004
Validation loss: 2.0346665581067405

Epoch: 6| Step: 2
Training loss: 2.1512556076049805
Validation loss: 2.045590817928314

Epoch: 6| Step: 3
Training loss: 1.6114795207977295
Validation loss: 2.041318694750468

Epoch: 6| Step: 4
Training loss: 2.494147777557373
Validation loss: 2.0442283352216086

Epoch: 6| Step: 5
Training loss: 1.9541542530059814
Validation loss: 2.044405917326609

Epoch: 6| Step: 6
Training loss: 2.044808864593506
Validation loss: 2.042265792687734

Epoch: 6| Step: 7
Training loss: 1.609347939491272
Validation loss: 2.0329992175102234

Epoch: 6| Step: 8
Training loss: 2.2406132221221924
Validation loss: 2.0465377966562905

Epoch: 6| Step: 9
Training loss: 2.319645881652832
Validation loss: 2.038634419441223

Epoch: 6| Step: 10
Training loss: 1.6821917295455933
Validation loss: 2.0521647930145264

Epoch: 6| Step: 11
Training loss: 1.9346944093704224
Validation loss: 2.0683735807736716

Epoch: 6| Step: 12
Training loss: 2.777313232421875
Validation loss: 2.0669040083885193

Epoch: 6| Step: 13
Training loss: 1.5829030275344849
Validation loss: 2.0687849720319114

Epoch: 141| Step: 0
Training loss: 2.289536237716675
Validation loss: 2.0764202078183494

Epoch: 6| Step: 1
Training loss: 2.0343077182769775
Validation loss: 2.076717416445414

Epoch: 6| Step: 2
Training loss: 2.261012554168701
Validation loss: 2.0689991116523743

Epoch: 6| Step: 3
Training loss: 2.5689592361450195
Validation loss: 2.0736427903175354

Epoch: 6| Step: 4
Training loss: 1.8757028579711914
Validation loss: 2.0757452050844827

Epoch: 6| Step: 5
Training loss: 2.26967453956604
Validation loss: 2.0716930627822876

Epoch: 6| Step: 6
Training loss: 2.016348361968994
Validation loss: 2.068593660990397

Epoch: 6| Step: 7
Training loss: 2.7538065910339355
Validation loss: 2.0620071490605674

Epoch: 6| Step: 8
Training loss: 2.2883310317993164
Validation loss: 2.0520431796709695

Epoch: 6| Step: 9
Training loss: 1.4849655628204346
Validation loss: 2.046765923500061

Epoch: 6| Step: 10
Training loss: 1.495715618133545
Validation loss: 2.0560883283615112

Epoch: 6| Step: 11
Training loss: 2.020566463470459
Validation loss: 2.0493065118789673

Epoch: 6| Step: 12
Training loss: 2.0784647464752197
Validation loss: 2.0504289269447327

Epoch: 6| Step: 13
Training loss: 1.4544544219970703
Validation loss: 2.054373780886332

Epoch: 142| Step: 0
Training loss: 2.674201488494873
Validation loss: 2.0512933929761252

Epoch: 6| Step: 1
Training loss: 1.608854055404663
Validation loss: 2.0494712193806968

Epoch: 6| Step: 2
Training loss: 1.6709038019180298
Validation loss: 2.052019019921621

Epoch: 6| Step: 3
Training loss: 1.9020395278930664
Validation loss: 2.0650972922643027

Epoch: 6| Step: 4
Training loss: 1.6194908618927002
Validation loss: 2.0540493925412497

Epoch: 6| Step: 5
Training loss: 1.670552372932434
Validation loss: 2.078172028064728

Epoch: 6| Step: 6
Training loss: 1.5428736209869385
Validation loss: 2.066070278485616

Epoch: 6| Step: 7
Training loss: 2.2565407752990723
Validation loss: 2.0763185620307922

Epoch: 6| Step: 8
Training loss: 1.9645752906799316
Validation loss: 2.062855899333954

Epoch: 6| Step: 9
Training loss: 2.719048500061035
Validation loss: 2.0676122903823853

Epoch: 6| Step: 10
Training loss: 2.422318458557129
Validation loss: 2.0578487714131675

Epoch: 6| Step: 11
Training loss: 1.9947994947433472
Validation loss: 2.0630611181259155

Epoch: 6| Step: 12
Training loss: 2.223963737487793
Validation loss: 2.045479158560435

Epoch: 6| Step: 13
Training loss: 2.285351276397705
Validation loss: 2.047005216280619

Epoch: 143| Step: 0
Training loss: 2.2053451538085938
Validation loss: 2.0384886264801025

Epoch: 6| Step: 1
Training loss: 1.7773116827011108
Validation loss: 2.042860289414724

Epoch: 6| Step: 2
Training loss: 2.4949042797088623
Validation loss: 2.0453490018844604

Epoch: 6| Step: 3
Training loss: 2.7719802856445312
Validation loss: 2.033566157023112

Epoch: 6| Step: 4
Training loss: 1.6810555458068848
Validation loss: 2.036920507748922

Epoch: 6| Step: 5
Training loss: 1.823532223701477
Validation loss: 2.0725655555725098

Epoch: 6| Step: 6
Training loss: 2.3675832748413086
Validation loss: 2.068292419115702

Epoch: 6| Step: 7
Training loss: 1.8905818462371826
Validation loss: 2.0775635838508606

Epoch: 6| Step: 8
Training loss: 2.028881788253784
Validation loss: 2.0915895104408264

Epoch: 6| Step: 9
Training loss: 1.5854768753051758
Validation loss: 2.0762725869814553

Epoch: 6| Step: 10
Training loss: 2.0613346099853516
Validation loss: 2.0765949885050454

Epoch: 6| Step: 11
Training loss: 1.9689116477966309
Validation loss: 2.06986931959788

Epoch: 6| Step: 12
Training loss: 2.315911293029785
Validation loss: 2.0730366309483848

Epoch: 6| Step: 13
Training loss: 1.535575032234192
Validation loss: 2.0550697644551597

Epoch: 144| Step: 0
Training loss: 2.3414618968963623
Validation loss: 2.053784271081289

Epoch: 6| Step: 1
Training loss: 1.402778148651123
Validation loss: 2.038498640060425

Epoch: 6| Step: 2
Training loss: 2.457106828689575
Validation loss: 2.0619208216667175

Epoch: 6| Step: 3
Training loss: 2.226938486099243
Validation loss: 2.0532228151957193

Epoch: 6| Step: 4
Training loss: 2.2328543663024902
Validation loss: 2.0817846854527793

Epoch: 6| Step: 5
Training loss: 1.690109133720398
Validation loss: 2.0699408054351807

Epoch: 6| Step: 6
Training loss: 2.2368216514587402
Validation loss: 2.081770976384481

Epoch: 6| Step: 7
Training loss: 1.8910794258117676
Validation loss: 2.08742618560791

Epoch: 6| Step: 8
Training loss: 1.7856515645980835
Validation loss: 2.0782026449839273

Epoch: 6| Step: 9
Training loss: 2.112574577331543
Validation loss: 2.085923433303833

Epoch: 6| Step: 10
Training loss: 2.27724552154541
Validation loss: 2.0724374453226724

Epoch: 6| Step: 11
Training loss: 1.8775781393051147
Validation loss: 2.08412899573644

Epoch: 6| Step: 12
Training loss: 1.926217794418335
Validation loss: 2.082446336746216

Epoch: 6| Step: 13
Training loss: 1.8973857164382935
Validation loss: 2.063133100668589

Epoch: 145| Step: 0
Training loss: 1.7147904634475708
Validation loss: 2.0763620932896933

Epoch: 6| Step: 1
Training loss: 2.063344955444336
Validation loss: 2.082187016805013

Epoch: 6| Step: 2
Training loss: 1.9422663450241089
Validation loss: 2.085236350695292

Epoch: 6| Step: 3
Training loss: 2.4784932136535645
Validation loss: 2.0882776776949563

Epoch: 6| Step: 4
Training loss: 1.5220866203308105
Validation loss: 2.0928358832995095

Epoch: 6| Step: 5
Training loss: 2.3562426567077637
Validation loss: 2.1001749436060586

Epoch: 6| Step: 6
Training loss: 2.102431535720825
Validation loss: 2.10221004486084

Epoch: 6| Step: 7
Training loss: 1.403033971786499
Validation loss: 2.0941114028294883

Epoch: 6| Step: 8
Training loss: 2.3547720909118652
Validation loss: 2.086332639058431

Epoch: 6| Step: 9
Training loss: 1.6801174879074097
Validation loss: 2.0837626258532205

Epoch: 6| Step: 10
Training loss: 1.981240153312683
Validation loss: 2.0849770307540894

Epoch: 6| Step: 11
Training loss: 2.395913600921631
Validation loss: 2.0667102932929993

Epoch: 6| Step: 12
Training loss: 1.7787368297576904
Validation loss: 2.0493789116541543

Epoch: 6| Step: 13
Training loss: 2.2260851860046387
Validation loss: 2.0575360655784607

Epoch: 146| Step: 0
Training loss: 2.1466078758239746
Validation loss: 2.044323146343231

Epoch: 6| Step: 1
Training loss: 1.952701449394226
Validation loss: 2.0560491482416787

Epoch: 6| Step: 2
Training loss: 1.4166874885559082
Validation loss: 2.053563912709554

Epoch: 6| Step: 3
Training loss: 1.9206286668777466
Validation loss: 2.040302097797394

Epoch: 6| Step: 4
Training loss: 1.9342080354690552
Validation loss: 2.0378229220708213

Epoch: 6| Step: 5
Training loss: 1.7829580307006836
Validation loss: 2.0512800017992654

Epoch: 6| Step: 6
Training loss: 2.1766908168792725
Validation loss: 2.0631465713183084

Epoch: 6| Step: 7
Training loss: 2.2108120918273926
Validation loss: 2.0600128173828125

Epoch: 6| Step: 8
Training loss: 2.415271759033203
Validation loss: 2.0569947560628257

Epoch: 6| Step: 9
Training loss: 2.235957622528076
Validation loss: 2.061520298322042

Epoch: 6| Step: 10
Training loss: 2.384474039077759
Validation loss: 2.0628920992215476

Epoch: 6| Step: 11
Training loss: 2.2929601669311523
Validation loss: 2.0824759801228843

Epoch: 6| Step: 12
Training loss: 1.2562003135681152
Validation loss: 2.068681259950002

Epoch: 6| Step: 13
Training loss: 2.006171941757202
Validation loss: 2.0818727612495422

Epoch: 147| Step: 0
Training loss: 2.3201515674591064
Validation loss: 2.080021242300669

Epoch: 6| Step: 1
Training loss: 2.8007545471191406
Validation loss: 2.0765217343966165

Epoch: 6| Step: 2
Training loss: 2.0131194591522217
Validation loss: 2.0846135020256042

Epoch: 6| Step: 3
Training loss: 1.877307415008545
Validation loss: 2.0931243300437927

Epoch: 6| Step: 4
Training loss: 1.6641701459884644
Validation loss: 2.0565289656321206

Epoch: 6| Step: 5
Training loss: 2.0590980052948
Validation loss: 2.0660850008328757

Epoch: 6| Step: 6
Training loss: 1.6170761585235596
Validation loss: 2.085776070753733

Epoch: 6| Step: 7
Training loss: 2.2617619037628174
Validation loss: 2.070570468902588

Epoch: 6| Step: 8
Training loss: 1.3017966747283936
Validation loss: 2.061551292737325

Epoch: 6| Step: 9
Training loss: 2.096662998199463
Validation loss: 2.0478408535321555

Epoch: 6| Step: 10
Training loss: 1.9077186584472656
Validation loss: 2.0345844427744546

Epoch: 6| Step: 11
Training loss: 1.923039197921753
Validation loss: 2.0499847133954368

Epoch: 6| Step: 12
Training loss: 1.858476161956787
Validation loss: 2.035025715827942

Epoch: 6| Step: 13
Training loss: 2.537120819091797
Validation loss: 2.0458067258199057

Epoch: 148| Step: 0
Training loss: 2.2978832721710205
Validation loss: 2.0472179651260376

Epoch: 6| Step: 1
Training loss: 2.002781867980957
Validation loss: 2.0500268737475076

Epoch: 6| Step: 2
Training loss: 1.8322107791900635
Validation loss: 2.063933809598287

Epoch: 6| Step: 3
Training loss: 2.253659963607788
Validation loss: 2.0600805282592773

Epoch: 6| Step: 4
Training loss: 2.065707206726074
Validation loss: 2.06722484032313

Epoch: 6| Step: 5
Training loss: 1.877282977104187
Validation loss: 2.071758449077606

Epoch: 6| Step: 6
Training loss: 1.554281234741211
Validation loss: 2.0633941094080606

Epoch: 6| Step: 7
Training loss: 1.9158730506896973
Validation loss: 2.068611999352773

Epoch: 6| Step: 8
Training loss: 2.0965418815612793
Validation loss: 2.059094488620758

Epoch: 6| Step: 9
Training loss: 1.9572360515594482
Validation loss: 2.0551276008288064

Epoch: 6| Step: 10
Training loss: 2.637268304824829
Validation loss: 2.04385115702947

Epoch: 6| Step: 11
Training loss: 1.5913728475570679
Validation loss: 2.0448522567749023

Epoch: 6| Step: 12
Training loss: 1.6424694061279297
Validation loss: 2.0415085752805076

Epoch: 6| Step: 13
Training loss: 2.4928436279296875
Validation loss: 2.042268991470337

Epoch: 149| Step: 0
Training loss: 1.9561243057250977
Validation loss: 2.0534695982933044

Epoch: 6| Step: 1
Training loss: 2.105685234069824
Validation loss: 2.038594047228495

Epoch: 6| Step: 2
Training loss: 2.6176300048828125
Validation loss: 2.0591260393460593

Epoch: 6| Step: 3
Training loss: 1.9556454420089722
Validation loss: 2.059241155783335

Epoch: 6| Step: 4
Training loss: 1.861539602279663
Validation loss: 2.0590471625328064

Epoch: 6| Step: 5
Training loss: 2.013030529022217
Validation loss: 2.0598509907722473

Epoch: 6| Step: 6
Training loss: 2.0743212699890137
Validation loss: 2.054285685221354

Epoch: 6| Step: 7
Training loss: 2.2154574394226074
Validation loss: 2.0540012319882712

Epoch: 6| Step: 8
Training loss: 2.1594130992889404
Validation loss: 2.0517151753107705

Epoch: 6| Step: 9
Training loss: 1.815244197845459
Validation loss: 2.0425322453180947

Epoch: 6| Step: 10
Training loss: 1.8931995630264282
Validation loss: 2.053398311138153

Epoch: 6| Step: 11
Training loss: 1.9955724477767944
Validation loss: 2.068168123563131

Epoch: 6| Step: 12
Training loss: 1.4924471378326416
Validation loss: 2.052310605843862

Epoch: 6| Step: 13
Training loss: 2.0719497203826904
Validation loss: 2.0599369208017984

Epoch: 150| Step: 0
Training loss: 2.3054442405700684
Validation loss: 2.0615219473838806

Epoch: 6| Step: 1
Training loss: 1.6578285694122314
Validation loss: 2.071393668651581

Epoch: 6| Step: 2
Training loss: 2.0000131130218506
Validation loss: 2.0876145164171853

Epoch: 6| Step: 3
Training loss: 1.7921645641326904
Validation loss: 2.0873978535334268

Epoch: 6| Step: 4
Training loss: 1.900089979171753
Validation loss: 2.1014325817426047

Epoch: 6| Step: 5
Training loss: 1.933654546737671
Validation loss: 2.101304372151693

Epoch: 6| Step: 6
Training loss: 1.9205714464187622
Validation loss: 2.1223727067311606

Epoch: 6| Step: 7
Training loss: 2.5273261070251465
Validation loss: 2.119617303212484

Epoch: 6| Step: 8
Training loss: 2.266697406768799
Validation loss: 2.1456231276194253

Epoch: 6| Step: 9
Training loss: 2.193089485168457
Validation loss: 2.1389946341514587

Epoch: 6| Step: 10
Training loss: 2.3565926551818848
Validation loss: 2.166092038154602

Epoch: 6| Step: 11
Training loss: 1.5857555866241455
Validation loss: 2.143819530804952

Epoch: 6| Step: 12
Training loss: 2.1574387550354004
Validation loss: 2.1067582170168557

Epoch: 6| Step: 13
Training loss: 1.8914536237716675
Validation loss: 2.096846024195353

Epoch: 151| Step: 0
Training loss: 1.6949081420898438
Validation loss: 2.070189634958903

Epoch: 6| Step: 1
Training loss: 2.1205215454101562
Validation loss: 2.060968577861786

Epoch: 6| Step: 2
Training loss: 2.3212451934814453
Validation loss: 2.0723496874173484

Epoch: 6| Step: 3
Training loss: 1.8365570306777954
Validation loss: 2.07350621620814

Epoch: 6| Step: 4
Training loss: 2.1042089462280273
Validation loss: 2.069006383419037

Epoch: 6| Step: 5
Training loss: 1.6477378606796265
Validation loss: 2.088765263557434

Epoch: 6| Step: 6
Training loss: 2.4983530044555664
Validation loss: 2.0839696129163108

Epoch: 6| Step: 7
Training loss: 2.412198066711426
Validation loss: 2.075848857561747

Epoch: 6| Step: 8
Training loss: 2.12272310256958
Validation loss: 2.0653512279192605

Epoch: 6| Step: 9
Training loss: 2.6469171047210693
Validation loss: 2.0762340823809304

Epoch: 6| Step: 10
Training loss: 2.294107675552368
Validation loss: 2.075529674688975

Epoch: 6| Step: 11
Training loss: 1.9793223142623901
Validation loss: 2.0760703682899475

Epoch: 6| Step: 12
Training loss: 2.106570243835449
Validation loss: 2.0716571609179177

Epoch: 6| Step: 13
Training loss: 2.0305662155151367
Validation loss: 2.0685850977897644

Epoch: 152| Step: 0
Training loss: 1.5526750087738037
Validation loss: 2.0706916650136313

Epoch: 6| Step: 1
Training loss: 1.923953652381897
Validation loss: 2.071764568487803

Epoch: 6| Step: 2
Training loss: 2.5378332138061523
Validation loss: 2.0664918422698975

Epoch: 6| Step: 3
Training loss: 1.776918888092041
Validation loss: 2.046065390110016

Epoch: 6| Step: 4
Training loss: 2.2421233654022217
Validation loss: 2.053264935811361

Epoch: 6| Step: 5
Training loss: 2.3443422317504883
Validation loss: 2.0440683563550315

Epoch: 6| Step: 6
Training loss: 2.221860647201538
Validation loss: 2.0398199558258057

Epoch: 6| Step: 7
Training loss: 2.4547581672668457
Validation loss: 2.034637928009033

Epoch: 6| Step: 8
Training loss: 2.2458248138427734
Validation loss: 2.0353889067967734

Epoch: 6| Step: 9
Training loss: 1.8284034729003906
Validation loss: 2.036114732424418

Epoch: 6| Step: 10
Training loss: 1.7500085830688477
Validation loss: 2.020896077156067

Epoch: 6| Step: 11
Training loss: 2.071925640106201
Validation loss: 2.0407280921936035

Epoch: 6| Step: 12
Training loss: 2.4308228492736816
Validation loss: 2.0272133350372314

Epoch: 6| Step: 13
Training loss: 2.157761335372925
Validation loss: 2.0392409364382424

Epoch: 153| Step: 0
Training loss: 1.4437484741210938
Validation loss: 2.0187822580337524

Epoch: 6| Step: 1
Training loss: 1.7457365989685059
Validation loss: 2.0294520060221353

Epoch: 6| Step: 2
Training loss: 2.672635078430176
Validation loss: 2.023120363553365

Epoch: 6| Step: 3
Training loss: 2.2456159591674805
Validation loss: 2.0390690763791404

Epoch: 6| Step: 4
Training loss: 1.834910273551941
Validation loss: 2.040659268697103

Epoch: 6| Step: 5
Training loss: 2.2622859477996826
Validation loss: 2.040638506412506

Epoch: 6| Step: 6
Training loss: 1.8508656024932861
Validation loss: 2.063913941383362

Epoch: 6| Step: 7
Training loss: 2.3911266326904297
Validation loss: 2.052985648314158

Epoch: 6| Step: 8
Training loss: 1.9850926399230957
Validation loss: 2.0683939456939697

Epoch: 6| Step: 9
Training loss: 2.156672477722168
Validation loss: 2.058724502722422

Epoch: 6| Step: 10
Training loss: 2.8280763626098633
Validation loss: 2.0636123021443686

Epoch: 6| Step: 11
Training loss: 1.8661704063415527
Validation loss: 2.0769699811935425

Epoch: 6| Step: 12
Training loss: 1.3147286176681519
Validation loss: 2.0748837192853293

Epoch: 6| Step: 13
Training loss: 1.5382843017578125
Validation loss: 2.0663647651672363

Epoch: 154| Step: 0
Training loss: 1.6307507753372192
Validation loss: 2.0813860098520913

Epoch: 6| Step: 1
Training loss: 1.8434796333312988
Validation loss: 2.093567669391632

Epoch: 6| Step: 2
Training loss: 2.163097858428955
Validation loss: 2.0800182024637857

Epoch: 6| Step: 3
Training loss: 2.1193995475769043
Validation loss: 2.089400053024292

Epoch: 6| Step: 4
Training loss: 2.1112332344055176
Validation loss: 2.0932303071022034

Epoch: 6| Step: 5
Training loss: 2.412790298461914
Validation loss: 2.084658662478129

Epoch: 6| Step: 6
Training loss: 2.0001726150512695
Validation loss: 2.091219981511434

Epoch: 6| Step: 7
Training loss: 1.7055366039276123
Validation loss: 2.091729720433553

Epoch: 6| Step: 8
Training loss: 2.084824562072754
Validation loss: 2.0855143070220947

Epoch: 6| Step: 9
Training loss: 2.382303237915039
Validation loss: 2.0864368081092834

Epoch: 6| Step: 10
Training loss: 2.261521339416504
Validation loss: 2.0874767104784646

Epoch: 6| Step: 11
Training loss: 1.8590036630630493
Validation loss: 2.0903039375940957

Epoch: 6| Step: 12
Training loss: 1.657982349395752
Validation loss: 2.100380460421244

Epoch: 6| Step: 13
Training loss: 1.8805227279663086
Validation loss: 2.0922282934188843

Epoch: 155| Step: 0
Training loss: 2.0692577362060547
Validation loss: 2.1035463213920593

Epoch: 6| Step: 1
Training loss: 2.0543341636657715
Validation loss: 2.0868658820788064

Epoch: 6| Step: 2
Training loss: 2.5506043434143066
Validation loss: 2.096048136552175

Epoch: 6| Step: 3
Training loss: 2.4637551307678223
Validation loss: 2.0941566626230874

Epoch: 6| Step: 4
Training loss: 1.9441263675689697
Validation loss: 2.088975667953491

Epoch: 6| Step: 5
Training loss: 2.7217931747436523
Validation loss: 2.092031399408976

Epoch: 6| Step: 6
Training loss: 1.6196783781051636
Validation loss: 2.0774345795313516

Epoch: 6| Step: 7
Training loss: 1.8251898288726807
Validation loss: 2.08303302526474

Epoch: 6| Step: 8
Training loss: 1.4614976644515991
Validation loss: 2.089013636112213

Epoch: 6| Step: 9
Training loss: 1.6428512334823608
Validation loss: 2.077653964360555

Epoch: 6| Step: 10
Training loss: 1.7322263717651367
Validation loss: 2.092057724793752

Epoch: 6| Step: 11
Training loss: 1.7291533946990967
Validation loss: 2.0723493496576944

Epoch: 6| Step: 12
Training loss: 1.7832505702972412
Validation loss: 2.0849626461664834

Epoch: 6| Step: 13
Training loss: 2.299814462661743
Validation loss: 2.084597925345103

Epoch: 156| Step: 0
Training loss: 2.200550079345703
Validation loss: 2.069037119547526

Epoch: 6| Step: 1
Training loss: 2.0077688694000244
Validation loss: 2.0859320362408957

Epoch: 6| Step: 2
Training loss: 1.9329789876937866
Validation loss: 2.085026184717814

Epoch: 6| Step: 3
Training loss: 2.0743587017059326
Validation loss: 2.094197670618693

Epoch: 6| Step: 4
Training loss: 2.394489288330078
Validation loss: 2.0864664117495217

Epoch: 6| Step: 5
Training loss: 1.5963716506958008
Validation loss: 2.0637774070103965

Epoch: 6| Step: 6
Training loss: 1.7695398330688477
Validation loss: 2.0750553011894226

Epoch: 6| Step: 7
Training loss: 1.6090459823608398
Validation loss: 2.07489005724589

Epoch: 6| Step: 8
Training loss: 1.9099061489105225
Validation loss: 2.073189655939738

Epoch: 6| Step: 9
Training loss: 1.9255774021148682
Validation loss: 2.0717666149139404

Epoch: 6| Step: 10
Training loss: 2.598435640335083
Validation loss: 2.066438694794973

Epoch: 6| Step: 11
Training loss: 2.2017018795013428
Validation loss: 2.073213001092275

Epoch: 6| Step: 12
Training loss: 1.6282641887664795
Validation loss: 2.0859471956888833

Epoch: 6| Step: 13
Training loss: 2.415602684020996
Validation loss: 2.080673019091288

Epoch: 157| Step: 0
Training loss: 2.011174440383911
Validation loss: 2.0979692141215005

Epoch: 6| Step: 1
Training loss: 2.1706924438476562
Validation loss: 2.0927829146385193

Epoch: 6| Step: 2
Training loss: 1.7873660326004028
Validation loss: 2.103382170200348

Epoch: 6| Step: 3
Training loss: 1.72383451461792
Validation loss: 2.0990086793899536

Epoch: 6| Step: 4
Training loss: 2.2752318382263184
Validation loss: 2.096410095691681

Epoch: 6| Step: 5
Training loss: 1.7322256565093994
Validation loss: 2.1067309776941934

Epoch: 6| Step: 6
Training loss: 2.105776786804199
Validation loss: 2.1009983817736306

Epoch: 6| Step: 7
Training loss: 2.4800667762756348
Validation loss: 2.1084461212158203

Epoch: 6| Step: 8
Training loss: 1.5521759986877441
Validation loss: 2.102901816368103

Epoch: 6| Step: 9
Training loss: 2.022610664367676
Validation loss: 2.1030510862668357

Epoch: 6| Step: 10
Training loss: 1.7892155647277832
Validation loss: 2.093586007754008

Epoch: 6| Step: 11
Training loss: 2.2152178287506104
Validation loss: 2.097662568092346

Epoch: 6| Step: 12
Training loss: 1.8958661556243896
Validation loss: 2.0984737078348794

Epoch: 6| Step: 13
Training loss: 2.3140182495117188
Validation loss: 2.106369952360789

Epoch: 158| Step: 0
Training loss: 1.9562042951583862
Validation loss: 2.0978859066963196

Epoch: 6| Step: 1
Training loss: 1.3502782583236694
Validation loss: 2.0862126549084983

Epoch: 6| Step: 2
Training loss: 1.924818754196167
Validation loss: 2.0983654657999673

Epoch: 6| Step: 3
Training loss: 2.139338970184326
Validation loss: 2.094776729742686

Epoch: 6| Step: 4
Training loss: 2.0062644481658936
Validation loss: 2.074851870536804

Epoch: 6| Step: 5
Training loss: 2.0970020294189453
Validation loss: 2.0856897036234536

Epoch: 6| Step: 6
Training loss: 1.8417046070098877
Validation loss: 2.080096960067749

Epoch: 6| Step: 7
Training loss: 2.7530579566955566
Validation loss: 2.0730021595954895

Epoch: 6| Step: 8
Training loss: 1.9165985584259033
Validation loss: 2.0841583609580994

Epoch: 6| Step: 9
Training loss: 1.9517695903778076
Validation loss: 2.0738062262535095

Epoch: 6| Step: 10
Training loss: 1.6963809728622437
Validation loss: 2.077136675516764

Epoch: 6| Step: 11
Training loss: 2.2549171447753906
Validation loss: 2.0737716952959695

Epoch: 6| Step: 12
Training loss: 2.103215217590332
Validation loss: 2.066050032774607

Epoch: 6| Step: 13
Training loss: 2.174136161804199
Validation loss: 2.0686752001444497

Epoch: 159| Step: 0
Training loss: 1.8975911140441895
Validation loss: 2.0656997561454773

Epoch: 6| Step: 1
Training loss: 1.8264997005462646
Validation loss: 2.0864696304003396

Epoch: 6| Step: 2
Training loss: 2.2063074111938477
Validation loss: 2.0829924742380777

Epoch: 6| Step: 3
Training loss: 2.0163421630859375
Validation loss: 2.083031098047892

Epoch: 6| Step: 4
Training loss: 2.405984878540039
Validation loss: 2.0714582999547324

Epoch: 6| Step: 5
Training loss: 2.5722007751464844
Validation loss: 2.0834469397862754

Epoch: 6| Step: 6
Training loss: 2.3103296756744385
Validation loss: 2.0818109114964805

Epoch: 6| Step: 7
Training loss: 1.4345908164978027
Validation loss: 2.0796717405319214

Epoch: 6| Step: 8
Training loss: 1.9948720932006836
Validation loss: 2.0800243814786277

Epoch: 6| Step: 9
Training loss: 1.6101981401443481
Validation loss: 2.103484332561493

Epoch: 6| Step: 10
Training loss: 2.0882906913757324
Validation loss: 2.0960530440012612

Epoch: 6| Step: 11
Training loss: 1.032594084739685
Validation loss: 2.0894283850987754

Epoch: 6| Step: 12
Training loss: 2.2176055908203125
Validation loss: 2.0981401205062866

Epoch: 6| Step: 13
Training loss: 2.03855562210083
Validation loss: 2.0980167388916016

Epoch: 160| Step: 0
Training loss: 2.5950889587402344
Validation loss: 2.1139442125956216

Epoch: 6| Step: 1
Training loss: 2.032729148864746
Validation loss: 2.1025962034861245

Epoch: 6| Step: 2
Training loss: 2.1830697059631348
Validation loss: 2.108989695707957

Epoch: 6| Step: 3
Training loss: 1.6288936138153076
Validation loss: 2.1059170365333557

Epoch: 6| Step: 4
Training loss: 2.578705072402954
Validation loss: 2.122170627117157

Epoch: 6| Step: 5
Training loss: 1.7324392795562744
Validation loss: 2.120830694834391

Epoch: 6| Step: 6
Training loss: 1.9071877002716064
Validation loss: 2.106444795926412

Epoch: 6| Step: 7
Training loss: 1.5315747261047363
Validation loss: 2.0922475258509317

Epoch: 6| Step: 8
Training loss: 2.1909990310668945
Validation loss: 2.1063985427220664

Epoch: 6| Step: 9
Training loss: 2.0357141494750977
Validation loss: 2.0993876655896506

Epoch: 6| Step: 10
Training loss: 1.6413530111312866
Validation loss: 2.0997503201166787

Epoch: 6| Step: 11
Training loss: 2.0010948181152344
Validation loss: 2.0905988613764444

Epoch: 6| Step: 12
Training loss: 1.5864064693450928
Validation loss: 2.0934815406799316

Epoch: 6| Step: 13
Training loss: 2.165419101715088
Validation loss: 2.1021064718564353

Epoch: 161| Step: 0
Training loss: 1.8028827905654907
Validation loss: 2.101755420366923

Epoch: 6| Step: 1
Training loss: 1.8785927295684814
Validation loss: 2.0966568191846213

Epoch: 6| Step: 2
Training loss: 2.0493791103363037
Validation loss: 2.0905080437660217

Epoch: 6| Step: 3
Training loss: 1.662913203239441
Validation loss: 2.081822395324707

Epoch: 6| Step: 4
Training loss: 2.2764408588409424
Validation loss: 2.088659942150116

Epoch: 6| Step: 5
Training loss: 1.9448858499526978
Validation loss: 2.0818630854288735

Epoch: 6| Step: 6
Training loss: 2.303353786468506
Validation loss: 2.0842941403388977

Epoch: 6| Step: 7
Training loss: 1.9578518867492676
Validation loss: 2.0895197987556458

Epoch: 6| Step: 8
Training loss: 1.8867707252502441
Validation loss: 2.097845176855723

Epoch: 6| Step: 9
Training loss: 1.9611661434173584
Validation loss: 2.09769876797994

Epoch: 6| Step: 10
Training loss: 2.1778340339660645
Validation loss: 2.0955101450284324

Epoch: 6| Step: 11
Training loss: 1.2435075044631958
Validation loss: 2.080253799756368

Epoch: 6| Step: 12
Training loss: 2.3928909301757812
Validation loss: 2.0848629673322043

Epoch: 6| Step: 13
Training loss: 2.1215574741363525
Validation loss: 2.078088084856669

Epoch: 162| Step: 0
Training loss: 2.0435259342193604
Validation loss: 2.0908937454223633

Epoch: 6| Step: 1
Training loss: 1.7564839124679565
Validation loss: 2.086829900741577

Epoch: 6| Step: 2
Training loss: 1.7541618347167969
Validation loss: 2.0787237882614136

Epoch: 6| Step: 3
Training loss: 2.1037826538085938
Validation loss: 2.0632675687472024

Epoch: 6| Step: 4
Training loss: 1.9530762434005737
Validation loss: 2.097323556741079

Epoch: 6| Step: 5
Training loss: 1.9722657203674316
Validation loss: 2.092404365539551

Epoch: 6| Step: 6
Training loss: 1.1887671947479248
Validation loss: 2.0978161493937173

Epoch: 6| Step: 7
Training loss: 2.057969331741333
Validation loss: 2.096448083718618

Epoch: 6| Step: 8
Training loss: 1.8285804986953735
Validation loss: 2.099834938844045

Epoch: 6| Step: 9
Training loss: 2.369377613067627
Validation loss: 2.0945201913515725

Epoch: 6| Step: 10
Training loss: 2.664511203765869
Validation loss: 2.1083462834358215

Epoch: 6| Step: 11
Training loss: 2.1728436946868896
Validation loss: 2.097604751586914

Epoch: 6| Step: 12
Training loss: 1.9532229900360107
Validation loss: 2.1007142265637717

Epoch: 6| Step: 13
Training loss: 1.9362902641296387
Validation loss: 2.0956888993581138

Epoch: 163| Step: 0
Training loss: 1.2522534132003784
Validation loss: 2.090029239654541

Epoch: 6| Step: 1
Training loss: 2.05975079536438
Validation loss: 2.084252337614695

Epoch: 6| Step: 2
Training loss: 2.3161935806274414
Validation loss: 2.088599363962809

Epoch: 6| Step: 3
Training loss: 2.1044039726257324
Validation loss: 2.0719906091690063

Epoch: 6| Step: 4
Training loss: 2.3184139728546143
Validation loss: 2.089080353577932

Epoch: 6| Step: 5
Training loss: 1.9959609508514404
Validation loss: 2.0782850980758667

Epoch: 6| Step: 6
Training loss: 2.4012985229492188
Validation loss: 2.0796121954917908

Epoch: 6| Step: 7
Training loss: 2.1127655506134033
Validation loss: 2.084084769090017

Epoch: 6| Step: 8
Training loss: 2.2477548122406006
Validation loss: 2.093676726023356

Epoch: 6| Step: 9
Training loss: 1.7701911926269531
Validation loss: 2.0957753658294678

Epoch: 6| Step: 10
Training loss: 1.204772710800171
Validation loss: 2.0980852842330933

Epoch: 6| Step: 11
Training loss: 2.2174696922302246
Validation loss: 2.099177519480387

Epoch: 6| Step: 12
Training loss: 2.2482621669769287
Validation loss: 2.126941978931427

Epoch: 6| Step: 13
Training loss: 2.277376174926758
Validation loss: 2.1542696356773376

Epoch: 164| Step: 0
Training loss: 2.241445541381836
Validation loss: 2.1482673486073813

Epoch: 6| Step: 1
Training loss: 2.186599016189575
Validation loss: 2.144749124844869

Epoch: 6| Step: 2
Training loss: 1.9099750518798828
Validation loss: 2.0954087575276694

Epoch: 6| Step: 3
Training loss: 2.217604160308838
Validation loss: 2.117304484049479

Epoch: 6| Step: 4
Training loss: 2.5693254470825195
Validation loss: 2.1046910087267556

Epoch: 6| Step: 5
Training loss: 2.1848666667938232
Validation loss: 2.0930033127466836

Epoch: 6| Step: 6
Training loss: 2.8718879222869873
Validation loss: 2.081839640935262

Epoch: 6| Step: 7
Training loss: 1.7619881629943848
Validation loss: 2.0947584907213845

Epoch: 6| Step: 8
Training loss: 1.9398702383041382
Validation loss: 2.103436529636383

Epoch: 6| Step: 9
Training loss: 1.8719773292541504
Validation loss: 2.1021785338719687

Epoch: 6| Step: 10
Training loss: 2.2779626846313477
Validation loss: 2.1048940420150757

Epoch: 6| Step: 11
Training loss: 1.4285178184509277
Validation loss: 2.09785924355189

Epoch: 6| Step: 12
Training loss: 1.2189514636993408
Validation loss: 2.0936374068260193

Epoch: 6| Step: 13
Training loss: 1.710229516029358
Validation loss: 2.104006846745809

Epoch: 165| Step: 0
Training loss: 2.006819725036621
Validation loss: 2.0978797674179077

Epoch: 6| Step: 1
Training loss: 1.9777461290359497
Validation loss: 2.0955581068992615

Epoch: 6| Step: 2
Training loss: 1.882944107055664
Validation loss: 2.1010499795277915

Epoch: 6| Step: 3
Training loss: 2.407224178314209
Validation loss: 2.0977823535601297

Epoch: 6| Step: 4
Training loss: 2.237907648086548
Validation loss: 2.093419134616852

Epoch: 6| Step: 5
Training loss: 1.8113505840301514
Validation loss: 2.1059706807136536

Epoch: 6| Step: 6
Training loss: 1.912245750427246
Validation loss: 2.100497921307882

Epoch: 6| Step: 7
Training loss: 2.354569673538208
Validation loss: 2.079455534617106

Epoch: 6| Step: 8
Training loss: 1.7494194507598877
Validation loss: 2.108883539835612

Epoch: 6| Step: 9
Training loss: 1.7710988521575928
Validation loss: 2.100858906904856

Epoch: 6| Step: 10
Training loss: 1.7869927883148193
Validation loss: 2.0774654348691306

Epoch: 6| Step: 11
Training loss: 2.425224781036377
Validation loss: 2.0776780049006143

Epoch: 6| Step: 12
Training loss: 1.803125023841858
Validation loss: 2.080701013406118

Epoch: 6| Step: 13
Training loss: 1.6713910102844238
Validation loss: 2.0727192560831704

Epoch: 166| Step: 0
Training loss: 2.144383430480957
Validation loss: 2.0719485680262246

Epoch: 6| Step: 1
Training loss: 2.4148831367492676
Validation loss: 2.071106572945913

Epoch: 6| Step: 2
Training loss: 1.6050219535827637
Validation loss: 2.085268974304199

Epoch: 6| Step: 3
Training loss: 2.301723003387451
Validation loss: 2.1003185709317527

Epoch: 6| Step: 4
Training loss: 1.9141701459884644
Validation loss: 2.0790050427118936

Epoch: 6| Step: 5
Training loss: 2.063370943069458
Validation loss: 2.0725073417027793

Epoch: 6| Step: 6
Training loss: 2.349557399749756
Validation loss: 2.0855486392974854

Epoch: 6| Step: 7
Training loss: 1.56145179271698
Validation loss: 2.0730318824450173

Epoch: 6| Step: 8
Training loss: 1.4199990034103394
Validation loss: 2.0928061803181968

Epoch: 6| Step: 9
Training loss: 2.247580051422119
Validation loss: 2.0730785131454468

Epoch: 6| Step: 10
Training loss: 2.2536518573760986
Validation loss: 2.0763513843218484

Epoch: 6| Step: 11
Training loss: 1.326125144958496
Validation loss: 2.0769065221150718

Epoch: 6| Step: 12
Training loss: 2.441599130630493
Validation loss: 2.0824219981829324

Epoch: 6| Step: 13
Training loss: 2.107600688934326
Validation loss: 2.09089465936025

Epoch: 167| Step: 0
Training loss: 1.7776166200637817
Validation loss: 2.0876566966374717

Epoch: 6| Step: 1
Training loss: 2.564052104949951
Validation loss: 2.0880566835403442

Epoch: 6| Step: 2
Training loss: 2.595897674560547
Validation loss: 2.0988591512044272

Epoch: 6| Step: 3
Training loss: 1.7502553462982178
Validation loss: 2.114527106285095

Epoch: 6| Step: 4
Training loss: 1.9728330373764038
Validation loss: 2.0879236857096353

Epoch: 6| Step: 5
Training loss: 1.8210080862045288
Validation loss: 2.098960220813751

Epoch: 6| Step: 6
Training loss: 1.7567124366760254
Validation loss: 2.108861764272054

Epoch: 6| Step: 7
Training loss: 2.1781344413757324
Validation loss: 2.1236348946889243

Epoch: 6| Step: 8
Training loss: 1.60660719871521
Validation loss: 2.1155371268590293

Epoch: 6| Step: 9
Training loss: 2.1210622787475586
Validation loss: 2.107194264729818

Epoch: 6| Step: 10
Training loss: 2.1593785285949707
Validation loss: 2.0919156670570374

Epoch: 6| Step: 11
Training loss: 2.074921131134033
Validation loss: 2.0973458886146545

Epoch: 6| Step: 12
Training loss: 1.6451458930969238
Validation loss: 2.0945345958073935

Epoch: 6| Step: 13
Training loss: 1.9900068044662476
Validation loss: 2.1089834769566855

Epoch: 168| Step: 0
Training loss: 2.2377212047576904
Validation loss: 2.104444603125254

Epoch: 6| Step: 1
Training loss: 2.1022732257843018
Validation loss: 2.1083776156107583

Epoch: 6| Step: 2
Training loss: 2.335268974304199
Validation loss: 2.105105996131897

Epoch: 6| Step: 3
Training loss: 1.3538894653320312
Validation loss: 2.0959346691767373

Epoch: 6| Step: 4
Training loss: 2.375706195831299
Validation loss: 2.0893765687942505

Epoch: 6| Step: 5
Training loss: 1.8599733114242554
Validation loss: 2.0905014673868814

Epoch: 6| Step: 6
Training loss: 2.427199363708496
Validation loss: 2.08629701534907

Epoch: 6| Step: 7
Training loss: 2.0387966632843018
Validation loss: 2.093733231226603

Epoch: 6| Step: 8
Training loss: 2.4372551441192627
Validation loss: 2.080892006556193

Epoch: 6| Step: 9
Training loss: 2.3312623500823975
Validation loss: 2.086919625600179

Epoch: 6| Step: 10
Training loss: 1.6767807006835938
Validation loss: 2.0975369612375894

Epoch: 6| Step: 11
Training loss: 1.642499327659607
Validation loss: 2.0949032505353293

Epoch: 6| Step: 12
Training loss: 1.6128917932510376
Validation loss: 2.0946209033330283

Epoch: 6| Step: 13
Training loss: 1.9074680805206299
Validation loss: 2.088089426358541

Epoch: 169| Step: 0
Training loss: 2.3971757888793945
Validation loss: 2.096057871977488

Epoch: 6| Step: 1
Training loss: 2.0840957164764404
Validation loss: 2.1015336910883584

Epoch: 6| Step: 2
Training loss: 1.8545966148376465
Validation loss: 2.0945324897766113

Epoch: 6| Step: 3
Training loss: 1.59488844871521
Validation loss: 2.087126632531484

Epoch: 6| Step: 4
Training loss: 1.8855223655700684
Validation loss: 2.0968265930811563

Epoch: 6| Step: 5
Training loss: 1.9897429943084717
Validation loss: 2.0953247348467507

Epoch: 6| Step: 6
Training loss: 2.0155282020568848
Validation loss: 2.0906205574671426

Epoch: 6| Step: 7
Training loss: 1.65781831741333
Validation loss: 2.083504855632782

Epoch: 6| Step: 8
Training loss: 2.4999399185180664
Validation loss: 2.0880009134610495

Epoch: 6| Step: 9
Training loss: 1.410262107849121
Validation loss: 2.0665773153305054

Epoch: 6| Step: 10
Training loss: 2.1160500049591064
Validation loss: 2.0776851773262024

Epoch: 6| Step: 11
Training loss: 2.4356276988983154
Validation loss: 2.070942481358846

Epoch: 6| Step: 12
Training loss: 1.9068471193313599
Validation loss: 2.0704489946365356

Epoch: 6| Step: 13
Training loss: 1.7239274978637695
Validation loss: 2.0681601961453757

Epoch: 170| Step: 0
Training loss: 2.2448573112487793
Validation loss: 2.069179097811381

Epoch: 6| Step: 1
Training loss: 2.3896420001983643
Validation loss: 2.0745506286621094

Epoch: 6| Step: 2
Training loss: 2.1979551315307617
Validation loss: 2.0763731002807617

Epoch: 6| Step: 3
Training loss: 1.4190661907196045
Validation loss: 2.0852602124214172

Epoch: 6| Step: 4
Training loss: 2.1279115676879883
Validation loss: 2.0849636793136597

Epoch: 6| Step: 5
Training loss: 1.9294015169143677
Validation loss: 2.0917407671610513

Epoch: 6| Step: 6
Training loss: 1.7847729921340942
Validation loss: 2.0957703987757363

Epoch: 6| Step: 7
Training loss: 1.7526307106018066
Validation loss: 2.090853691101074

Epoch: 6| Step: 8
Training loss: 2.4064841270446777
Validation loss: 2.093785365422567

Epoch: 6| Step: 9
Training loss: 1.5676555633544922
Validation loss: 2.0945884784062705

Epoch: 6| Step: 10
Training loss: 1.9365476369857788
Validation loss: 2.095981935660044

Epoch: 6| Step: 11
Training loss: 2.1146697998046875
Validation loss: 2.109992265701294

Epoch: 6| Step: 12
Training loss: 2.1827268600463867
Validation loss: 2.0950952768325806

Epoch: 6| Step: 13
Training loss: 1.4427196979522705
Validation loss: 2.0907123684883118

Epoch: 171| Step: 0
Training loss: 1.2880358695983887
Validation loss: 2.105120539665222

Epoch: 6| Step: 1
Training loss: 1.5166738033294678
Validation loss: 2.100820302963257

Epoch: 6| Step: 2
Training loss: 1.8612947463989258
Validation loss: 2.094945967197418

Epoch: 6| Step: 3
Training loss: 1.58150315284729
Validation loss: 2.0960738261540732

Epoch: 6| Step: 4
Training loss: 2.2085907459259033
Validation loss: 2.1048139929771423

Epoch: 6| Step: 5
Training loss: 2.079230546951294
Validation loss: 2.1126135985056558

Epoch: 6| Step: 6
Training loss: 2.1962993144989014
Validation loss: 2.124123692512512

Epoch: 6| Step: 7
Training loss: 1.911514401435852
Validation loss: 2.1094877123832703

Epoch: 6| Step: 8
Training loss: 2.421584129333496
Validation loss: 2.121693730354309

Epoch: 6| Step: 9
Training loss: 2.8069920539855957
Validation loss: 2.126133402188619

Epoch: 6| Step: 10
Training loss: 2.0471534729003906
Validation loss: 2.103592117627462

Epoch: 6| Step: 11
Training loss: 1.9317407608032227
Validation loss: 2.115280489126841

Epoch: 6| Step: 12
Training loss: 1.789495587348938
Validation loss: 2.0984851717948914

Epoch: 6| Step: 13
Training loss: 2.290868043899536
Validation loss: 2.113749146461487

Epoch: 172| Step: 0
Training loss: 2.3737921714782715
Validation loss: 2.107384999593099

Epoch: 6| Step: 1
Training loss: 1.6823369264602661
Validation loss: 2.093973914782206

Epoch: 6| Step: 2
Training loss: 2.1264169216156006
Validation loss: 2.0955419341723123

Epoch: 6| Step: 3
Training loss: 1.8034045696258545
Validation loss: 2.093418995539347

Epoch: 6| Step: 4
Training loss: 1.7538070678710938
Validation loss: 2.0850773652394614

Epoch: 6| Step: 5
Training loss: 1.8639349937438965
Validation loss: 2.091387848059336

Epoch: 6| Step: 6
Training loss: 1.8383574485778809
Validation loss: 2.073268254597982

Epoch: 6| Step: 7
Training loss: 2.033115863800049
Validation loss: 2.089795966943105

Epoch: 6| Step: 8
Training loss: 2.158094882965088
Validation loss: 2.077185352643331

Epoch: 6| Step: 9
Training loss: 2.0554041862487793
Validation loss: 2.083092828591665

Epoch: 6| Step: 10
Training loss: 2.5328948497772217
Validation loss: 2.072667121887207

Epoch: 6| Step: 11
Training loss: 1.7642714977264404
Validation loss: 2.0861462155977883

Epoch: 6| Step: 12
Training loss: 1.8194013833999634
Validation loss: 2.094659765561422

Epoch: 6| Step: 13
Training loss: 2.0080623626708984
Validation loss: 2.076684276262919

Epoch: 173| Step: 0
Training loss: 1.922820806503296
Validation loss: 2.06935586531957

Epoch: 6| Step: 1
Training loss: 1.2407639026641846
Validation loss: 2.080585300922394

Epoch: 6| Step: 2
Training loss: 1.591062068939209
Validation loss: 2.085722784201304

Epoch: 6| Step: 3
Training loss: 1.3696386814117432
Validation loss: 2.0705681443214417

Epoch: 6| Step: 4
Training loss: 2.356043815612793
Validation loss: 2.0814869006474814

Epoch: 6| Step: 5
Training loss: 2.6139957904815674
Validation loss: 2.0816242496172586

Epoch: 6| Step: 6
Training loss: 1.8053324222564697
Validation loss: 2.0926276246706643

Epoch: 6| Step: 7
Training loss: 2.2894139289855957
Validation loss: 2.0986849268277488

Epoch: 6| Step: 8
Training loss: 2.549668312072754
Validation loss: 2.098013997077942

Epoch: 6| Step: 9
Training loss: 1.9627199172973633
Validation loss: 2.099090258280436

Epoch: 6| Step: 10
Training loss: 2.0375945568084717
Validation loss: 2.09897118806839

Epoch: 6| Step: 11
Training loss: 1.832990050315857
Validation loss: 2.0859662294387817

Epoch: 6| Step: 12
Training loss: 1.9274519681930542
Validation loss: 2.099410057067871

Epoch: 6| Step: 13
Training loss: 2.076554298400879
Validation loss: 2.099320332209269

Epoch: 174| Step: 0
Training loss: 1.2956467866897583
Validation loss: 2.097567915916443

Epoch: 6| Step: 1
Training loss: 1.7869107723236084
Validation loss: 2.1017006635665894

Epoch: 6| Step: 2
Training loss: 1.7130496501922607
Validation loss: 2.1141310930252075

Epoch: 6| Step: 3
Training loss: 2.090608596801758
Validation loss: 2.0961912870407104

Epoch: 6| Step: 4
Training loss: 2.0200693607330322
Validation loss: 2.1123408476511636

Epoch: 6| Step: 5
Training loss: 2.4218809604644775
Validation loss: 2.0972670316696167

Epoch: 6| Step: 6
Training loss: 2.1042768955230713
Validation loss: 2.114561458428701

Epoch: 6| Step: 7
Training loss: 1.8337746858596802
Validation loss: 2.1118738849957785

Epoch: 6| Step: 8
Training loss: 2.3177642822265625
Validation loss: 2.124156355857849

Epoch: 6| Step: 9
Training loss: 1.914681315422058
Validation loss: 2.1219372550646463

Epoch: 6| Step: 10
Training loss: 1.8542587757110596
Validation loss: 2.1300936937332153

Epoch: 6| Step: 11
Training loss: 1.6440268754959106
Validation loss: 2.128612240155538

Epoch: 6| Step: 12
Training loss: 2.416389226913452
Validation loss: 2.1180657943089805

Epoch: 6| Step: 13
Training loss: 2.3115410804748535
Validation loss: 2.1261528531710305

Epoch: 175| Step: 0
Training loss: 2.657613754272461
Validation loss: 2.121823271115621

Epoch: 6| Step: 1
Training loss: 2.1538033485412598
Validation loss: 2.121389071146647

Epoch: 6| Step: 2
Training loss: 2.520540952682495
Validation loss: 2.102181533972422

Epoch: 6| Step: 3
Training loss: 2.599062442779541
Validation loss: 2.1011210878690085

Epoch: 6| Step: 4
Training loss: 1.5886560678482056
Validation loss: 2.1124468644460044

Epoch: 6| Step: 5
Training loss: 1.6323370933532715
Validation loss: 2.1213114062945047

Epoch: 6| Step: 6
Training loss: 2.204066514968872
Validation loss: 2.1001001397768655

Epoch: 6| Step: 7
Training loss: 1.458704948425293
Validation loss: 2.0988072752952576

Epoch: 6| Step: 8
Training loss: 1.8840694427490234
Validation loss: 2.1013343135515847

Epoch: 6| Step: 9
Training loss: 2.408066749572754
Validation loss: 2.101197342077891

Epoch: 6| Step: 10
Training loss: 1.5682929754257202
Validation loss: 2.1104350884755454

Epoch: 6| Step: 11
Training loss: 1.4903302192687988
Validation loss: 2.1120429635047913

Epoch: 6| Step: 12
Training loss: 1.5362735986709595
Validation loss: 2.123126824696859

Epoch: 6| Step: 13
Training loss: 1.984735131263733
Validation loss: 2.109230558077494

Epoch: 176| Step: 0
Training loss: 2.351254463195801
Validation loss: 2.1067872842152915

Epoch: 6| Step: 1
Training loss: 2.383582353591919
Validation loss: 2.1185373663902283

Epoch: 6| Step: 2
Training loss: 1.8420507907867432
Validation loss: 2.137043754259745

Epoch: 6| Step: 3
Training loss: 1.418162226676941
Validation loss: 2.1193009416262307

Epoch: 6| Step: 4
Training loss: 1.9105250835418701
Validation loss: 2.1417293548583984

Epoch: 6| Step: 5
Training loss: 1.8116258382797241
Validation loss: 2.137298345565796

Epoch: 6| Step: 6
Training loss: 2.300703287124634
Validation loss: 2.1426451007525125

Epoch: 6| Step: 7
Training loss: 1.924682378768921
Validation loss: 2.1394987305005393

Epoch: 6| Step: 8
Training loss: 1.8155897855758667
Validation loss: 2.147739827632904

Epoch: 6| Step: 9
Training loss: 1.737345814704895
Validation loss: 2.12979527314504

Epoch: 6| Step: 10
Training loss: 2.008378505706787
Validation loss: 2.1425251960754395

Epoch: 6| Step: 11
Training loss: 2.1123995780944824
Validation loss: 2.117349167664846

Epoch: 6| Step: 12
Training loss: 2.401106119155884
Validation loss: 2.094439148902893

Epoch: 6| Step: 13
Training loss: 1.7233905792236328
Validation loss: 2.094722112019857

Epoch: 177| Step: 0
Training loss: 2.6269800662994385
Validation loss: 2.0954230626424155

Epoch: 6| Step: 1
Training loss: 1.9736359119415283
Validation loss: 2.0955061117808023

Epoch: 6| Step: 2
Training loss: 1.698098063468933
Validation loss: 2.085254669189453

Epoch: 6| Step: 3
Training loss: 1.7776517868041992
Validation loss: 2.093892514705658

Epoch: 6| Step: 4
Training loss: 2.020664691925049
Validation loss: 2.1112340092658997

Epoch: 6| Step: 5
Training loss: 2.1003494262695312
Validation loss: 2.095574935277303

Epoch: 6| Step: 6
Training loss: 2.1482162475585938
Validation loss: 2.1305681467056274

Epoch: 6| Step: 7
Training loss: 1.9396908283233643
Validation loss: 2.115699032942454

Epoch: 6| Step: 8
Training loss: 1.9188125133514404
Validation loss: 2.12942765156428

Epoch: 6| Step: 9
Training loss: 1.614264726638794
Validation loss: 2.117062250773112

Epoch: 6| Step: 10
Training loss: 2.3023922443389893
Validation loss: 2.110735754172007

Epoch: 6| Step: 11
Training loss: 2.4220731258392334
Validation loss: 2.11745548248291

Epoch: 6| Step: 12
Training loss: 1.6191303730010986
Validation loss: 2.1088800628980002

Epoch: 6| Step: 13
Training loss: 1.632404088973999
Validation loss: 2.1015155712763467

Epoch: 178| Step: 0
Training loss: 1.676041841506958
Validation loss: 2.1137775778770447

Epoch: 6| Step: 1
Training loss: 2.3370065689086914
Validation loss: 2.1026159127553306

Epoch: 6| Step: 2
Training loss: 1.0337564945220947
Validation loss: 2.109654188156128

Epoch: 6| Step: 3
Training loss: 2.1913392543792725
Validation loss: 2.099367161591848

Epoch: 6| Step: 4
Training loss: 1.3393774032592773
Validation loss: 2.1147676507631936

Epoch: 6| Step: 5
Training loss: 1.6075729131698608
Validation loss: 2.1193692684173584

Epoch: 6| Step: 6
Training loss: 2.672571897506714
Validation loss: 2.1255889534950256

Epoch: 6| Step: 7
Training loss: 1.5429555177688599
Validation loss: 2.112974683443705

Epoch: 6| Step: 8
Training loss: 2.3894472122192383
Validation loss: 2.120203375816345

Epoch: 6| Step: 9
Training loss: 1.944217324256897
Validation loss: 2.1087571382522583

Epoch: 6| Step: 10
Training loss: 1.7801792621612549
Validation loss: 2.135003646214803

Epoch: 6| Step: 11
Training loss: 2.4516897201538086
Validation loss: 2.1130721966425576

Epoch: 6| Step: 12
Training loss: 2.136580228805542
Validation loss: 2.1206437746683755

Epoch: 6| Step: 13
Training loss: 2.2529540061950684
Validation loss: 2.1263638536135354

Epoch: 179| Step: 0
Training loss: 2.2208895683288574
Validation loss: 2.1278727849324546

Epoch: 6| Step: 1
Training loss: 2.14680552482605
Validation loss: 2.1283066272735596

Epoch: 6| Step: 2
Training loss: 1.7028263807296753
Validation loss: 2.1152189572652182

Epoch: 6| Step: 3
Training loss: 2.280137538909912
Validation loss: 2.1330760717391968

Epoch: 6| Step: 4
Training loss: 2.29744815826416
Validation loss: 2.1378352840741477

Epoch: 6| Step: 5
Training loss: 1.7378597259521484
Validation loss: 2.1311043898264566

Epoch: 6| Step: 6
Training loss: 2.238293409347534
Validation loss: 2.1387967069943747

Epoch: 6| Step: 7
Training loss: 1.174694538116455
Validation loss: 2.1078543861707053

Epoch: 6| Step: 8
Training loss: 1.562415361404419
Validation loss: 2.112574338912964

Epoch: 6| Step: 9
Training loss: 1.5823447704315186
Validation loss: 2.121221343676249

Epoch: 6| Step: 10
Training loss: 2.2055349349975586
Validation loss: 2.110876719156901

Epoch: 6| Step: 11
Training loss: 1.9464236497879028
Validation loss: 2.1256055434544883

Epoch: 6| Step: 12
Training loss: 1.6450932025909424
Validation loss: 2.1253480513890586

Epoch: 6| Step: 13
Training loss: 2.6033568382263184
Validation loss: 2.101428727308909

Epoch: 180| Step: 0
Training loss: 1.5515249967575073
Validation loss: 2.1152865489323935

Epoch: 6| Step: 1
Training loss: 1.8360679149627686
Validation loss: 2.1060556968053183

Epoch: 6| Step: 2
Training loss: 1.3151473999023438
Validation loss: 2.11849311987559

Epoch: 6| Step: 3
Training loss: 1.7013500928878784
Validation loss: 2.1076348423957825

Epoch: 6| Step: 4
Training loss: 1.9458632469177246
Validation loss: 2.113464812437693

Epoch: 6| Step: 5
Training loss: 2.1587250232696533
Validation loss: 2.119171977043152

Epoch: 6| Step: 6
Training loss: 2.056002616882324
Validation loss: 2.118979036808014

Epoch: 6| Step: 7
Training loss: 1.574019432067871
Validation loss: 2.1161898374557495

Epoch: 6| Step: 8
Training loss: 2.3928935527801514
Validation loss: 2.1138153870900473

Epoch: 6| Step: 9
Training loss: 2.0649538040161133
Validation loss: 2.1143972078959146

Epoch: 6| Step: 10
Training loss: 1.8014360666275024
Validation loss: 2.1124190290768943

Epoch: 6| Step: 11
Training loss: 2.1507434844970703
Validation loss: 2.121676027774811

Epoch: 6| Step: 12
Training loss: 1.9599354267120361
Validation loss: 2.1101273894309998

Epoch: 6| Step: 13
Training loss: 2.6521592140197754
Validation loss: 2.126707136631012

Epoch: 181| Step: 0
Training loss: 2.2892074584960938
Validation loss: 2.1244733730951944

Epoch: 6| Step: 1
Training loss: 1.5783902406692505
Validation loss: 2.1098522345225015

Epoch: 6| Step: 2
Training loss: 2.3676724433898926
Validation loss: 2.121351877848307

Epoch: 6| Step: 3
Training loss: 1.9926989078521729
Validation loss: 2.1194794178009033

Epoch: 6| Step: 4
Training loss: 1.9014217853546143
Validation loss: 2.1146056056022644

Epoch: 6| Step: 5
Training loss: 1.3042727708816528
Validation loss: 2.1005094051361084

Epoch: 6| Step: 6
Training loss: 2.402014970779419
Validation loss: 2.1096537113189697

Epoch: 6| Step: 7
Training loss: 2.254666328430176
Validation loss: 2.0964727799097695

Epoch: 6| Step: 8
Training loss: 1.8525108098983765
Validation loss: 2.1023625334103904

Epoch: 6| Step: 9
Training loss: 2.2125959396362305
Validation loss: 2.1241936683654785

Epoch: 6| Step: 10
Training loss: 2.2056188583374023
Validation loss: 2.0869097113609314

Epoch: 6| Step: 11
Training loss: 1.6783103942871094
Validation loss: 2.1241360108057656

Epoch: 6| Step: 12
Training loss: 1.6481504440307617
Validation loss: 2.1052052974700928

Epoch: 6| Step: 13
Training loss: 1.1941606998443604
Validation loss: 2.106367071469625

Epoch: 182| Step: 0
Training loss: 1.6062045097351074
Validation loss: 2.113900363445282

Epoch: 6| Step: 1
Training loss: 1.6587419509887695
Validation loss: 2.119373381137848

Epoch: 6| Step: 2
Training loss: 1.7961111068725586
Validation loss: 2.12357497215271

Epoch: 6| Step: 3
Training loss: 1.7681715488433838
Validation loss: 2.110979517300924

Epoch: 6| Step: 4
Training loss: 1.8368511199951172
Validation loss: 2.1359680692354837

Epoch: 6| Step: 5
Training loss: 2.0449490547180176
Validation loss: 2.1245131889979043

Epoch: 6| Step: 6
Training loss: 2.2464325428009033
Validation loss: 2.104648550351461

Epoch: 6| Step: 7
Training loss: 2.2141971588134766
Validation loss: 2.1024543046951294

Epoch: 6| Step: 8
Training loss: 2.1205716133117676
Validation loss: 2.1216144363085427

Epoch: 6| Step: 9
Training loss: 2.2871758937835693
Validation loss: 2.1085774898529053

Epoch: 6| Step: 10
Training loss: 1.4852745532989502
Validation loss: 2.1033897201220193

Epoch: 6| Step: 11
Training loss: 1.9325487613677979
Validation loss: 2.117545465628306

Epoch: 6| Step: 12
Training loss: 1.943403720855713
Validation loss: 2.105698903401693

Epoch: 6| Step: 13
Training loss: 2.4066147804260254
Validation loss: 2.105951189994812

Epoch: 183| Step: 0
Training loss: 2.0455784797668457
Validation loss: 2.10808402299881

Epoch: 6| Step: 1
Training loss: 2.0927090644836426
Validation loss: 2.1224653323491416

Epoch: 6| Step: 2
Training loss: 1.5486552715301514
Validation loss: 2.1243484020233154

Epoch: 6| Step: 3
Training loss: 2.0576353073120117
Validation loss: 2.1187564531962075

Epoch: 6| Step: 4
Training loss: 1.7682729959487915
Validation loss: 2.125641942024231

Epoch: 6| Step: 5
Training loss: 2.2543976306915283
Validation loss: 2.1194164554278054

Epoch: 6| Step: 6
Training loss: 2.2285513877868652
Validation loss: 2.105431318283081

Epoch: 6| Step: 7
Training loss: 2.083484411239624
Validation loss: 2.0937612056732178

Epoch: 6| Step: 8
Training loss: 2.7622623443603516
Validation loss: 2.1132742961247764

Epoch: 6| Step: 9
Training loss: 1.3494296073913574
Validation loss: 2.1077574491500854

Epoch: 6| Step: 10
Training loss: 2.1097209453582764
Validation loss: 2.101072589556376

Epoch: 6| Step: 11
Training loss: 1.7082693576812744
Validation loss: 2.095913032690684

Epoch: 6| Step: 12
Training loss: 1.8863258361816406
Validation loss: 2.0868872801462808

Epoch: 6| Step: 13
Training loss: 2.021307945251465
Validation loss: 2.1047775745391846

Epoch: 184| Step: 0
Training loss: 1.7632834911346436
Validation loss: 2.091066598892212

Epoch: 6| Step: 1
Training loss: 1.7384912967681885
Validation loss: 2.106787403424581

Epoch: 6| Step: 2
Training loss: 1.7234865427017212
Validation loss: 2.1021516720453897

Epoch: 6| Step: 3
Training loss: 2.0207338333129883
Validation loss: 2.0963216423988342

Epoch: 6| Step: 4
Training loss: 2.041691303253174
Validation loss: 2.126578172047933

Epoch: 6| Step: 5
Training loss: 2.256742477416992
Validation loss: 2.1052400271097818

Epoch: 6| Step: 6
Training loss: 2.186814785003662
Validation loss: 2.1005881627400718

Epoch: 6| Step: 7
Training loss: 2.0484018325805664
Validation loss: 2.0984679659207663

Epoch: 6| Step: 8
Training loss: 1.9863578081130981
Validation loss: 2.1062867840131125

Epoch: 6| Step: 9
Training loss: 1.6292932033538818
Validation loss: 2.1040809949239097

Epoch: 6| Step: 10
Training loss: 2.2688560485839844
Validation loss: 2.109459161758423

Epoch: 6| Step: 11
Training loss: 2.658818006515503
Validation loss: 2.1031539837519326

Epoch: 6| Step: 12
Training loss: 1.557552695274353
Validation loss: 2.1054628094037375

Epoch: 6| Step: 13
Training loss: 1.7944214344024658
Validation loss: 2.1104211608568826

Epoch: 185| Step: 0
Training loss: 1.7091732025146484
Validation loss: 2.103622535864512

Epoch: 6| Step: 1
Training loss: 2.4121673107147217
Validation loss: 2.0966528256734214

Epoch: 6| Step: 2
Training loss: 2.0162296295166016
Validation loss: 2.1209412018458047

Epoch: 6| Step: 3
Training loss: 1.83623206615448
Validation loss: 2.1054813265800476

Epoch: 6| Step: 4
Training loss: 1.7721238136291504
Validation loss: 2.1195755203564963

Epoch: 6| Step: 5
Training loss: 2.2804155349731445
Validation loss: 2.1185938715934753

Epoch: 6| Step: 6
Training loss: 2.030663013458252
Validation loss: 2.1251056790351868

Epoch: 6| Step: 7
Training loss: 1.0538465976715088
Validation loss: 2.126435418923696

Epoch: 6| Step: 8
Training loss: 1.8254330158233643
Validation loss: 2.141324043273926

Epoch: 6| Step: 9
Training loss: 2.190483570098877
Validation loss: 2.1350410183270774

Epoch: 6| Step: 10
Training loss: 2.050410270690918
Validation loss: 2.1452016830444336

Epoch: 6| Step: 11
Training loss: 1.5865771770477295
Validation loss: 2.1386025746663413

Epoch: 6| Step: 12
Training loss: 2.0065159797668457
Validation loss: 2.1367607514063516

Epoch: 6| Step: 13
Training loss: 2.61303448677063
Validation loss: 2.1357194979985556

Epoch: 186| Step: 0
Training loss: 2.7809534072875977
Validation loss: 2.1194302837053933

Epoch: 6| Step: 1
Training loss: 2.228091239929199
Validation loss: 2.1165708104769387

Epoch: 6| Step: 2
Training loss: 1.4401578903198242
Validation loss: 2.1144374012947083

Epoch: 6| Step: 3
Training loss: 2.1388092041015625
Validation loss: 2.1179295778274536

Epoch: 6| Step: 4
Training loss: 1.624488353729248
Validation loss: 2.107068181037903

Epoch: 6| Step: 5
Training loss: 2.063539505004883
Validation loss: 2.1091544230779014

Epoch: 6| Step: 6
Training loss: 2.1891355514526367
Validation loss: 2.1088064114252725

Epoch: 6| Step: 7
Training loss: 1.9279320240020752
Validation loss: 2.109915534655253

Epoch: 6| Step: 8
Training loss: 2.2954516410827637
Validation loss: 2.0981823801994324

Epoch: 6| Step: 9
Training loss: 1.4771945476531982
Validation loss: 2.1234105825424194

Epoch: 6| Step: 10
Training loss: 1.8729867935180664
Validation loss: 2.1281604766845703

Epoch: 6| Step: 11
Training loss: 1.7497881650924683
Validation loss: 2.1250926653544107

Epoch: 6| Step: 12
Training loss: 2.2157907485961914
Validation loss: 2.1316524346669516

Epoch: 6| Step: 13
Training loss: 2.387481689453125
Validation loss: 2.1124727725982666

Epoch: 187| Step: 0
Training loss: 1.832862377166748
Validation loss: 2.116635799407959

Epoch: 6| Step: 1
Training loss: 2.1777849197387695
Validation loss: 2.1063295801480613

Epoch: 6| Step: 2
Training loss: 1.6176176071166992
Validation loss: 2.1043059825897217

Epoch: 6| Step: 3
Training loss: 2.2398085594177246
Validation loss: 2.092701772848765

Epoch: 6| Step: 4
Training loss: 2.522425651550293
Validation loss: 2.0834412376085916

Epoch: 6| Step: 5
Training loss: 2.4749789237976074
Validation loss: 2.091020405292511

Epoch: 6| Step: 6
Training loss: 2.07568359375
Validation loss: 2.106648882230123

Epoch: 6| Step: 7
Training loss: 1.7440550327301025
Validation loss: 2.096028486887614

Epoch: 6| Step: 8
Training loss: 2.0938923358917236
Validation loss: 2.1069118976593018

Epoch: 6| Step: 9
Training loss: 1.5953000783920288
Validation loss: 2.098021904627482

Epoch: 6| Step: 10
Training loss: 2.0730464458465576
Validation loss: 2.1018803119659424

Epoch: 6| Step: 11
Training loss: 2.1275734901428223
Validation loss: 2.1106186707814536

Epoch: 6| Step: 12
Training loss: 1.4073407649993896
Validation loss: 2.103702942530314

Epoch: 6| Step: 13
Training loss: 1.9531720876693726
Validation loss: 2.1056707302729287

Epoch: 188| Step: 0
Training loss: 1.3540698289871216
Validation loss: 2.1077253818511963

Epoch: 6| Step: 1
Training loss: 1.8502757549285889
Validation loss: 2.108214179674784

Epoch: 6| Step: 2
Training loss: 1.8712866306304932
Validation loss: 2.1103276014328003

Epoch: 6| Step: 3
Training loss: 2.2168869972229004
Validation loss: 2.0945314168930054

Epoch: 6| Step: 4
Training loss: 2.559354782104492
Validation loss: 2.1039533019065857

Epoch: 6| Step: 5
Training loss: 2.2705836296081543
Validation loss: 2.10832279920578

Epoch: 6| Step: 6
Training loss: 1.8696775436401367
Validation loss: 2.121114949385325

Epoch: 6| Step: 7
Training loss: 1.7888914346694946
Validation loss: 2.135278344154358

Epoch: 6| Step: 8
Training loss: 1.7101736068725586
Validation loss: 2.1185102264086404

Epoch: 6| Step: 9
Training loss: 2.4351625442504883
Validation loss: 2.1181304852167764

Epoch: 6| Step: 10
Training loss: 2.2828218936920166
Validation loss: 2.1217421690622964

Epoch: 6| Step: 11
Training loss: 2.1724119186401367
Validation loss: 2.118746042251587

Epoch: 6| Step: 12
Training loss: 1.9395501613616943
Validation loss: 2.124191164970398

Epoch: 6| Step: 13
Training loss: 1.8766026496887207
Validation loss: 2.143796424070994

Epoch: 189| Step: 0
Training loss: 1.967994213104248
Validation loss: 2.1302466988563538

Epoch: 6| Step: 1
Training loss: 1.5219099521636963
Validation loss: 2.1222955783208213

Epoch: 6| Step: 2
Training loss: 2.2984862327575684
Validation loss: 2.1025034189224243

Epoch: 6| Step: 3
Training loss: 1.5558549165725708
Validation loss: 2.1220619281133017

Epoch: 6| Step: 4
Training loss: 2.3095502853393555
Validation loss: 2.1199413736661277

Epoch: 6| Step: 5
Training loss: 1.9308836460113525
Validation loss: 2.1106956203778586

Epoch: 6| Step: 6
Training loss: 2.12137508392334
Validation loss: 2.121526559193929

Epoch: 6| Step: 7
Training loss: 2.197462558746338
Validation loss: 2.1006970604260764

Epoch: 6| Step: 8
Training loss: 1.967208981513977
Validation loss: 2.106321394443512

Epoch: 6| Step: 9
Training loss: 1.977520227432251
Validation loss: 2.098395804564158

Epoch: 6| Step: 10
Training loss: 1.881195068359375
Validation loss: 2.105303188165029

Epoch: 6| Step: 11
Training loss: 2.112473964691162
Validation loss: 2.0964786211649575

Epoch: 6| Step: 12
Training loss: 1.7357990741729736
Validation loss: 2.0950585206349692

Epoch: 6| Step: 13
Training loss: 1.8833943605422974
Validation loss: 2.0957258145014444

Epoch: 190| Step: 0
Training loss: 2.3197994232177734
Validation loss: 2.102505366007487

Epoch: 6| Step: 1
Training loss: 2.034799337387085
Validation loss: 2.0886959036191306

Epoch: 6| Step: 2
Training loss: 2.410224437713623
Validation loss: 2.0913657347361245

Epoch: 6| Step: 3
Training loss: 1.9883278608322144
Validation loss: 2.101097802321116

Epoch: 6| Step: 4
Training loss: 2.2312686443328857
Validation loss: 2.0909094413121543

Epoch: 6| Step: 5
Training loss: 1.2782975435256958
Validation loss: 2.1012641390164695

Epoch: 6| Step: 6
Training loss: 1.7552305459976196
Validation loss: 2.100106179714203

Epoch: 6| Step: 7
Training loss: 1.3233146667480469
Validation loss: 2.120649496714274

Epoch: 6| Step: 8
Training loss: 2.0443596839904785
Validation loss: 2.1054245034853616

Epoch: 6| Step: 9
Training loss: 1.8838919401168823
Validation loss: 2.107799530029297

Epoch: 6| Step: 10
Training loss: 1.831315517425537
Validation loss: 2.1167260805765786

Epoch: 6| Step: 11
Training loss: 2.0336785316467285
Validation loss: 2.120563487211863

Epoch: 6| Step: 12
Training loss: 1.9527170658111572
Validation loss: 2.1282246510187783

Epoch: 6| Step: 13
Training loss: 1.9915186166763306
Validation loss: 2.151026427745819

Epoch: 191| Step: 0
Training loss: 1.6018702983856201
Validation loss: 2.155715823173523

Epoch: 6| Step: 1
Training loss: 2.086489200592041
Validation loss: 2.132029175758362

Epoch: 6| Step: 2
Training loss: 2.03558349609375
Validation loss: 2.115563372770945

Epoch: 6| Step: 3
Training loss: 1.6506978273391724
Validation loss: 2.1368449131647744

Epoch: 6| Step: 4
Training loss: 2.1456515789031982
Validation loss: 2.1402440468470254

Epoch: 6| Step: 5
Training loss: 2.4291181564331055
Validation loss: 2.1534170111020408

Epoch: 6| Step: 6
Training loss: 1.485050082206726
Validation loss: 2.1215874354044595

Epoch: 6| Step: 7
Training loss: 1.758849024772644
Validation loss: 2.136238177617391

Epoch: 6| Step: 8
Training loss: 1.8780633211135864
Validation loss: 2.1336110830307007

Epoch: 6| Step: 9
Training loss: 1.678087592124939
Validation loss: 2.128304898738861

Epoch: 6| Step: 10
Training loss: 2.2712249755859375
Validation loss: 2.13089511791865

Epoch: 6| Step: 11
Training loss: 2.487436294555664
Validation loss: 2.144976794719696

Epoch: 6| Step: 12
Training loss: 1.6461769342422485
Validation loss: 2.142174184322357

Epoch: 6| Step: 13
Training loss: 1.709561824798584
Validation loss: 2.144811232884725

Epoch: 192| Step: 0
Training loss: 2.290679454803467
Validation loss: 2.144101361433665

Epoch: 6| Step: 1
Training loss: 2.107760429382324
Validation loss: 2.129007796446482

Epoch: 6| Step: 2
Training loss: 1.5133121013641357
Validation loss: 2.128130614757538

Epoch: 6| Step: 3
Training loss: 2.309605836868286
Validation loss: 2.123828033606211

Epoch: 6| Step: 4
Training loss: 2.113618850708008
Validation loss: 2.1502084732055664

Epoch: 6| Step: 5
Training loss: 2.032193660736084
Validation loss: 2.1436070799827576

Epoch: 6| Step: 6
Training loss: 1.9328296184539795
Validation loss: 2.143747548262278

Epoch: 6| Step: 7
Training loss: 1.4120562076568604
Validation loss: 2.138998885949453

Epoch: 6| Step: 8
Training loss: 2.492366313934326
Validation loss: 2.1455907225608826

Epoch: 6| Step: 9
Training loss: 2.1749348640441895
Validation loss: 2.1471908489863076

Epoch: 6| Step: 10
Training loss: 1.553567886352539
Validation loss: 2.146886189778646

Epoch: 6| Step: 11
Training loss: 1.0996826887130737
Validation loss: 2.149706860383352

Epoch: 6| Step: 12
Training loss: 2.127610206604004
Validation loss: 2.1282814741134644

Epoch: 6| Step: 13
Training loss: 1.6771457195281982
Validation loss: 2.141531844933828

Epoch: 193| Step: 0
Training loss: 2.2312939167022705
Validation loss: 2.116843501726786

Epoch: 6| Step: 1
Training loss: 1.919647455215454
Validation loss: 2.143318295478821

Epoch: 6| Step: 2
Training loss: 1.4365817308425903
Validation loss: 2.1583512226740518

Epoch: 6| Step: 3
Training loss: 2.2967305183410645
Validation loss: 2.1503801345825195

Epoch: 6| Step: 4
Training loss: 1.9120838642120361
Validation loss: 2.1504571040471396

Epoch: 6| Step: 5
Training loss: 2.008593797683716
Validation loss: 2.1276719768842063

Epoch: 6| Step: 6
Training loss: 1.5915167331695557
Validation loss: 2.1520219643910727

Epoch: 6| Step: 7
Training loss: 1.9113322496414185
Validation loss: 2.13803231716156

Epoch: 6| Step: 8
Training loss: 2.364975690841675
Validation loss: 2.1289581855138144

Epoch: 6| Step: 9
Training loss: 1.5341079235076904
Validation loss: 2.118070105711619

Epoch: 6| Step: 10
Training loss: 1.8628485202789307
Validation loss: 2.1494417786598206

Epoch: 6| Step: 11
Training loss: 2.474508285522461
Validation loss: 2.1459630926450095

Epoch: 6| Step: 12
Training loss: 1.7667515277862549
Validation loss: 2.149469276269277

Epoch: 6| Step: 13
Training loss: 1.9000684022903442
Validation loss: 2.120816389719645

Epoch: 194| Step: 0
Training loss: 2.1661603450775146
Validation loss: 2.1196696360905967

Epoch: 6| Step: 1
Training loss: 1.357781171798706
Validation loss: 2.123759150505066

Epoch: 6| Step: 2
Training loss: 1.8048067092895508
Validation loss: 2.1453151305516562

Epoch: 6| Step: 3
Training loss: 1.7092878818511963
Validation loss: 2.126325229803721

Epoch: 6| Step: 4
Training loss: 2.815633535385132
Validation loss: 2.1141289273897805

Epoch: 6| Step: 5
Training loss: 1.7130485773086548
Validation loss: 2.126192490259806

Epoch: 6| Step: 6
Training loss: 1.8896596431732178
Validation loss: 2.131575127442678

Epoch: 6| Step: 7
Training loss: 1.2495191097259521
Validation loss: 2.130584716796875

Epoch: 6| Step: 8
Training loss: 2.042074680328369
Validation loss: 2.124645551045736

Epoch: 6| Step: 9
Training loss: 2.5117971897125244
Validation loss: 2.1416385968526206

Epoch: 6| Step: 10
Training loss: 1.632433533668518
Validation loss: 2.1391294399897256

Epoch: 6| Step: 11
Training loss: 2.164016008377075
Validation loss: 2.1458085775375366

Epoch: 6| Step: 12
Training loss: 2.453650951385498
Validation loss: 2.15129562218984

Epoch: 6| Step: 13
Training loss: 1.6851036548614502
Validation loss: 2.1399512887001038

Epoch: 195| Step: 0
Training loss: 1.688896656036377
Validation loss: 2.164553960164388

Epoch: 6| Step: 1
Training loss: 1.0906319618225098
Validation loss: 2.1209623614947

Epoch: 6| Step: 2
Training loss: 2.4234628677368164
Validation loss: 2.1386688152949014

Epoch: 6| Step: 3
Training loss: 2.016516923904419
Validation loss: 2.124793589115143

Epoch: 6| Step: 4
Training loss: 1.7553179264068604
Validation loss: 2.136276880900065

Epoch: 6| Step: 5
Training loss: 2.1468183994293213
Validation loss: 2.1358810861905417

Epoch: 6| Step: 6
Training loss: 2.0092196464538574
Validation loss: 2.137323876221975

Epoch: 6| Step: 7
Training loss: 2.4280900955200195
Validation loss: 2.144568920135498

Epoch: 6| Step: 8
Training loss: 1.9828670024871826
Validation loss: 2.1483561595280967

Epoch: 6| Step: 9
Training loss: 1.4428303241729736
Validation loss: 2.150710701942444

Epoch: 6| Step: 10
Training loss: 1.5484139919281006
Validation loss: 2.149359126885732

Epoch: 6| Step: 11
Training loss: 2.0601987838745117
Validation loss: 2.152344584465027

Epoch: 6| Step: 12
Training loss: 2.270815372467041
Validation loss: 2.1558417876561484

Epoch: 6| Step: 13
Training loss: 2.0527122020721436
Validation loss: 2.152204751968384

Epoch: 196| Step: 0
Training loss: 1.1541911363601685
Validation loss: 2.1540980140368142

Epoch: 6| Step: 1
Training loss: 1.248604416847229
Validation loss: 2.1609568993250527

Epoch: 6| Step: 2
Training loss: 1.720474362373352
Validation loss: 2.159917493661245

Epoch: 6| Step: 3
Training loss: 2.1688790321350098
Validation loss: 2.1676478187243142

Epoch: 6| Step: 4
Training loss: 1.683668613433838
Validation loss: 2.1618019342422485

Epoch: 6| Step: 5
Training loss: 2.396975040435791
Validation loss: 2.135009547074636

Epoch: 6| Step: 6
Training loss: 1.5810515880584717
Validation loss: 2.1234222650527954

Epoch: 6| Step: 7
Training loss: 2.5186972618103027
Validation loss: 2.141314129034678

Epoch: 6| Step: 8
Training loss: 2.708432674407959
Validation loss: 2.147923767566681

Epoch: 6| Step: 9
Training loss: 2.1910388469696045
Validation loss: 2.150820235411326

Epoch: 6| Step: 10
Training loss: 1.5998120307922363
Validation loss: 2.1429079373677573

Epoch: 6| Step: 11
Training loss: 1.591399908065796
Validation loss: 2.143111785252889

Epoch: 6| Step: 12
Training loss: 1.9514867067337036
Validation loss: 2.1432467699050903

Epoch: 6| Step: 13
Training loss: 2.1149463653564453
Validation loss: 2.137791653474172

Epoch: 197| Step: 0
Training loss: 1.9207205772399902
Validation loss: 2.1355575720469155

Epoch: 6| Step: 1
Training loss: 1.8126524686813354
Validation loss: 2.1220645109812417

Epoch: 6| Step: 2
Training loss: 2.1927313804626465
Validation loss: 2.1584858099619546

Epoch: 6| Step: 3
Training loss: 2.000092029571533
Validation loss: 2.1420220732688904

Epoch: 6| Step: 4
Training loss: 1.6512529850006104
Validation loss: 2.1548367937405906

Epoch: 6| Step: 5
Training loss: 1.995164394378662
Validation loss: 2.1302467385927835

Epoch: 6| Step: 6
Training loss: 2.39442777633667
Validation loss: 2.140804708003998

Epoch: 6| Step: 7
Training loss: 1.4850836992263794
Validation loss: 2.142277101675669

Epoch: 6| Step: 8
Training loss: 1.4781650304794312
Validation loss: 2.1409327189127603

Epoch: 6| Step: 9
Training loss: 2.261716365814209
Validation loss: 2.155968209107717

Epoch: 6| Step: 10
Training loss: 1.4266892671585083
Validation loss: 2.1436816851298013

Epoch: 6| Step: 11
Training loss: 2.130511999130249
Validation loss: 2.1499629418055215

Epoch: 6| Step: 12
Training loss: 2.1042332649230957
Validation loss: 2.155655105908712

Epoch: 6| Step: 13
Training loss: 1.7436800003051758
Validation loss: 2.1501157879829407

Epoch: 198| Step: 0
Training loss: 2.0880661010742188
Validation loss: 2.163255055745443

Epoch: 6| Step: 1
Training loss: 1.9310448169708252
Validation loss: 2.1575916608174643

Epoch: 6| Step: 2
Training loss: 2.614891290664673
Validation loss: 2.138440211613973

Epoch: 6| Step: 3
Training loss: 1.6936591863632202
Validation loss: 2.14992618560791

Epoch: 6| Step: 4
Training loss: 1.9505308866500854
Validation loss: 2.166796922683716

Epoch: 6| Step: 5
Training loss: 1.8918275833129883
Validation loss: 2.1576264103253684

Epoch: 6| Step: 6
Training loss: 1.5612329244613647
Validation loss: 2.1413979729016623

Epoch: 6| Step: 7
Training loss: 1.8338559865951538
Validation loss: 2.162622650464376

Epoch: 6| Step: 8
Training loss: 1.6511411666870117
Validation loss: 2.167704224586487

Epoch: 6| Step: 9
Training loss: 2.328810930252075
Validation loss: 2.152120371659597

Epoch: 6| Step: 10
Training loss: 1.814264178276062
Validation loss: 2.150939921538035

Epoch: 6| Step: 11
Training loss: 1.7834725379943848
Validation loss: 2.144333561261495

Epoch: 6| Step: 12
Training loss: 1.5347769260406494
Validation loss: 2.135540803273519

Epoch: 6| Step: 13
Training loss: 2.1974072456359863
Validation loss: 2.150115112463633

Epoch: 199| Step: 0
Training loss: 2.4840927124023438
Validation loss: 2.1622835795084634

Epoch: 6| Step: 1
Training loss: 2.1476261615753174
Validation loss: 2.1561843752861023

Epoch: 6| Step: 2
Training loss: 1.6488168239593506
Validation loss: 2.155131240685781

Epoch: 6| Step: 3
Training loss: 2.172274112701416
Validation loss: 2.1448166569073996

Epoch: 6| Step: 4
Training loss: 1.2854673862457275
Validation loss: 2.162505288918813

Epoch: 6| Step: 5
Training loss: 2.0059428215026855
Validation loss: 2.14708944161733

Epoch: 6| Step: 6
Training loss: 1.7960736751556396
Validation loss: 2.1444573998451233

Epoch: 6| Step: 7
Training loss: 1.3389897346496582
Validation loss: 2.1557214657465615

Epoch: 6| Step: 8
Training loss: 2.0164222717285156
Validation loss: 2.145051638285319

Epoch: 6| Step: 9
Training loss: 1.89058518409729
Validation loss: 2.1340357859929404

Epoch: 6| Step: 10
Training loss: 2.271310329437256
Validation loss: 2.1457719206809998

Epoch: 6| Step: 11
Training loss: 1.7730257511138916
Validation loss: 2.1608675718307495

Epoch: 6| Step: 12
Training loss: 1.9131474494934082
Validation loss: 2.1716458400090537

Epoch: 6| Step: 13
Training loss: 2.3612709045410156
Validation loss: 2.1649193366368613

Epoch: 200| Step: 0
Training loss: 1.8593697547912598
Validation loss: 2.158912261327108

Epoch: 6| Step: 1
Training loss: 1.730489730834961
Validation loss: 2.1496697664260864

Epoch: 6| Step: 2
Training loss: 1.5915255546569824
Validation loss: 2.1510589917500815

Epoch: 6| Step: 3
Training loss: 2.4454641342163086
Validation loss: 2.157611866792043

Epoch: 6| Step: 4
Training loss: 1.8442144393920898
Validation loss: 2.1232515374819436

Epoch: 6| Step: 5
Training loss: 1.7910981178283691
Validation loss: 2.1489583452542624

Epoch: 6| Step: 6
Training loss: 1.9574064016342163
Validation loss: 2.1375333865483603

Epoch: 6| Step: 7
Training loss: 1.8286049365997314
Validation loss: 2.1301485101381936

Epoch: 6| Step: 8
Training loss: 1.6669038534164429
Validation loss: 2.1309929688771567

Epoch: 6| Step: 9
Training loss: 2.2588047981262207
Validation loss: 2.1251874566078186

Epoch: 6| Step: 10
Training loss: 2.514108657836914
Validation loss: 2.1204925179481506

Epoch: 6| Step: 11
Training loss: 1.9985544681549072
Validation loss: 2.1343893011411033

Epoch: 6| Step: 12
Training loss: 1.8505322933197021
Validation loss: 2.1331957379976907

Epoch: 6| Step: 13
Training loss: 2.33809757232666
Validation loss: 2.1476520895957947

Epoch: 201| Step: 0
Training loss: 1.685997724533081
Validation loss: 2.157498836517334

Epoch: 6| Step: 1
Training loss: 2.9741005897521973
Validation loss: 2.15903502702713

Epoch: 6| Step: 2
Training loss: 1.9858813285827637
Validation loss: 2.1394337018330893

Epoch: 6| Step: 3
Training loss: 1.2561743259429932
Validation loss: 2.148783028125763

Epoch: 6| Step: 4
Training loss: 1.7390336990356445
Validation loss: 2.1532461841901145

Epoch: 6| Step: 5
Training loss: 1.5962278842926025
Validation loss: 2.162363568941752

Epoch: 6| Step: 6
Training loss: 2.649286985397339
Validation loss: 2.1511980295181274

Epoch: 6| Step: 7
Training loss: 1.3521922826766968
Validation loss: 2.1400652329126992

Epoch: 6| Step: 8
Training loss: 1.885201096534729
Validation loss: 2.1499513586362204

Epoch: 6| Step: 9
Training loss: 1.9414031505584717
Validation loss: 2.1602104902267456

Epoch: 6| Step: 10
Training loss: 1.8937941789627075
Validation loss: 2.1469381848971048

Epoch: 6| Step: 11
Training loss: 1.9337786436080933
Validation loss: 2.1613957484563193

Epoch: 6| Step: 12
Training loss: 1.2616384029388428
Validation loss: 2.147714893023173

Epoch: 6| Step: 13
Training loss: 2.509364604949951
Validation loss: 2.156511147816976

Epoch: 202| Step: 0
Training loss: 1.9173719882965088
Validation loss: 2.1640735467274985

Epoch: 6| Step: 1
Training loss: 2.378458023071289
Validation loss: 2.1525025367736816

Epoch: 6| Step: 2
Training loss: 1.9698736667633057
Validation loss: 2.158017575740814

Epoch: 6| Step: 3
Training loss: 1.8555166721343994
Validation loss: 2.1563318371772766

Epoch: 6| Step: 4
Training loss: 2.1488423347473145
Validation loss: 2.1394572854042053

Epoch: 6| Step: 5
Training loss: 1.5455880165100098
Validation loss: 2.1550390919049582

Epoch: 6| Step: 6
Training loss: 1.8245155811309814
Validation loss: 2.1588831543922424

Epoch: 6| Step: 7
Training loss: 1.9107863903045654
Validation loss: 2.145184596379598

Epoch: 6| Step: 8
Training loss: 2.142598867416382
Validation loss: 2.1566075881322226

Epoch: 6| Step: 9
Training loss: 2.413668155670166
Validation loss: 2.1524800856908164

Epoch: 6| Step: 10
Training loss: 2.080138921737671
Validation loss: 2.1578577558199563

Epoch: 6| Step: 11
Training loss: 1.8555588722229004
Validation loss: 2.139083822568258

Epoch: 6| Step: 12
Training loss: 1.3630366325378418
Validation loss: 2.1678197185198465

Epoch: 6| Step: 13
Training loss: 1.1911070346832275
Validation loss: 2.183034876982371

Epoch: 203| Step: 0
Training loss: 1.8796296119689941
Validation loss: 2.167306145032247

Epoch: 6| Step: 1
Training loss: 1.691749930381775
Validation loss: 2.192350765069326

Epoch: 6| Step: 2
Training loss: 1.7345633506774902
Validation loss: 2.178842226664225

Epoch: 6| Step: 3
Training loss: 2.055527448654175
Validation loss: 2.1852298180262246

Epoch: 6| Step: 4
Training loss: 1.8902634382247925
Validation loss: 2.1852688988049827

Epoch: 6| Step: 5
Training loss: 1.8278766870498657
Validation loss: 2.1892903447151184

Epoch: 6| Step: 6
Training loss: 2.485330581665039
Validation loss: 2.174607515335083

Epoch: 6| Step: 7
Training loss: 1.9060138463974
Validation loss: 2.1689185301462808

Epoch: 6| Step: 8
Training loss: 2.010236978530884
Validation loss: 2.1635955770810447

Epoch: 6| Step: 9
Training loss: 1.980851173400879
Validation loss: 2.1448932687441506

Epoch: 6| Step: 10
Training loss: 1.699479341506958
Validation loss: 2.151439050833384

Epoch: 6| Step: 11
Training loss: 2.182086229324341
Validation loss: 2.156540354092916

Epoch: 6| Step: 12
Training loss: 1.6725969314575195
Validation loss: 2.154463450113932

Epoch: 6| Step: 13
Training loss: 1.8524636030197144
Validation loss: 2.1503587563832602

Epoch: 204| Step: 0
Training loss: 1.5989400148391724
Validation loss: 2.1380112568537393

Epoch: 6| Step: 1
Training loss: 1.4462249279022217
Validation loss: 2.1454400618871055

Epoch: 6| Step: 2
Training loss: 1.612313985824585
Validation loss: 2.148554046948751

Epoch: 6| Step: 3
Training loss: 1.360135555267334
Validation loss: 2.1422696113586426

Epoch: 6| Step: 4
Training loss: 2.2252655029296875
Validation loss: 2.141915738582611

Epoch: 6| Step: 5
Training loss: 2.575518846511841
Validation loss: 2.1458454728126526

Epoch: 6| Step: 6
Training loss: 1.563720941543579
Validation loss: 2.1501829822858176

Epoch: 6| Step: 7
Training loss: 2.1688334941864014
Validation loss: 2.1484413345654807

Epoch: 6| Step: 8
Training loss: 1.3987538814544678
Validation loss: 2.147457718849182

Epoch: 6| Step: 9
Training loss: 1.631983995437622
Validation loss: 2.123957177003225

Epoch: 6| Step: 10
Training loss: 2.3476145267486572
Validation loss: 2.1479803125063577

Epoch: 6| Step: 11
Training loss: 2.589355230331421
Validation loss: 2.135277012983958

Epoch: 6| Step: 12
Training loss: 2.4852871894836426
Validation loss: 2.13901956876119

Epoch: 6| Step: 13
Training loss: 1.7252094745635986
Validation loss: 2.138733386993408

Epoch: 205| Step: 0
Training loss: 1.3899320363998413
Validation loss: 2.1478648583094277

Epoch: 6| Step: 1
Training loss: 1.894012689590454
Validation loss: 2.1320186456044516

Epoch: 6| Step: 2
Training loss: 2.315931797027588
Validation loss: 2.1410496830940247

Epoch: 6| Step: 3
Training loss: 2.1660401821136475
Validation loss: 2.1369513869285583

Epoch: 6| Step: 4
Training loss: 1.9542890787124634
Validation loss: 2.1470916668574014

Epoch: 6| Step: 5
Training loss: 1.4735485315322876
Validation loss: 2.1399532556533813

Epoch: 6| Step: 6
Training loss: 1.4917564392089844
Validation loss: 2.1456849177678428

Epoch: 6| Step: 7
Training loss: 2.030898094177246
Validation loss: 2.1434261202812195

Epoch: 6| Step: 8
Training loss: 1.7532949447631836
Validation loss: 2.1388213634490967

Epoch: 6| Step: 9
Training loss: 2.401710033416748
Validation loss: 2.1459005872408548

Epoch: 6| Step: 10
Training loss: 2.0790159702301025
Validation loss: 2.1426563262939453

Epoch: 6| Step: 11
Training loss: 1.1652710437774658
Validation loss: 2.14448744058609

Epoch: 6| Step: 12
Training loss: 1.732415795326233
Validation loss: 2.1436351339022317

Epoch: 6| Step: 13
Training loss: 2.5729897022247314
Validation loss: 2.115744948387146

Epoch: 206| Step: 0
Training loss: 1.8559592962265015
Validation loss: 2.1293472250302634

Epoch: 6| Step: 1
Training loss: 2.330556869506836
Validation loss: 2.130987008412679

Epoch: 6| Step: 2
Training loss: 1.8998548984527588
Validation loss: 2.141957998275757

Epoch: 6| Step: 3
Training loss: 2.003427743911743
Validation loss: 2.1354727745056152

Epoch: 6| Step: 4
Training loss: 1.2539498805999756
Validation loss: 2.135283946990967

Epoch: 6| Step: 5
Training loss: 1.787113904953003
Validation loss: 2.139416217803955

Epoch: 6| Step: 6
Training loss: 2.0398874282836914
Validation loss: 2.151081144809723

Epoch: 6| Step: 7
Training loss: 1.840683937072754
Validation loss: 2.1508262356122336

Epoch: 6| Step: 8
Training loss: 1.9894258975982666
Validation loss: 2.147940238316854

Epoch: 6| Step: 9
Training loss: 1.894564151763916
Validation loss: 2.1507763266563416

Epoch: 6| Step: 10
Training loss: 1.6665124893188477
Validation loss: 2.1541531682014465

Epoch: 6| Step: 11
Training loss: 1.5899356603622437
Validation loss: 2.147636353969574

Epoch: 6| Step: 12
Training loss: 2.6952362060546875
Validation loss: 2.1648911039034524

Epoch: 6| Step: 13
Training loss: 1.8181922435760498
Validation loss: 2.1379496653874717

Epoch: 207| Step: 0
Training loss: 1.9112358093261719
Validation loss: 2.156931718190511

Epoch: 6| Step: 1
Training loss: 2.1838650703430176
Validation loss: 2.153494874636332

Epoch: 6| Step: 2
Training loss: 1.773789882659912
Validation loss: 2.17427921295166

Epoch: 6| Step: 3
Training loss: 1.8750892877578735
Validation loss: 2.1605677604675293

Epoch: 6| Step: 4
Training loss: 2.2271242141723633
Validation loss: 2.144927700360616

Epoch: 6| Step: 5
Training loss: 1.5880249738693237
Validation loss: 2.1640527049700418

Epoch: 6| Step: 6
Training loss: 1.6670812368392944
Validation loss: 2.151590585708618

Epoch: 6| Step: 7
Training loss: 1.8869400024414062
Validation loss: 2.1732614437739053

Epoch: 6| Step: 8
Training loss: 1.9020864963531494
Validation loss: 2.1578661600748696

Epoch: 6| Step: 9
Training loss: 1.549487829208374
Validation loss: 2.153108378251394

Epoch: 6| Step: 10
Training loss: 2.2297096252441406
Validation loss: 2.1427813172340393

Epoch: 6| Step: 11
Training loss: 1.2202156782150269
Validation loss: 2.1533183256785073

Epoch: 6| Step: 12
Training loss: 2.198399782180786
Validation loss: 2.1718801657358804

Epoch: 6| Step: 13
Training loss: 2.09037446975708
Validation loss: 2.159770449002584

Epoch: 208| Step: 0
Training loss: 1.8680338859558105
Validation loss: 2.142079015572866

Epoch: 6| Step: 1
Training loss: 2.162282705307007
Validation loss: 2.1651673515637717

Epoch: 6| Step: 2
Training loss: 1.8127812147140503
Validation loss: 2.1553674936294556

Epoch: 6| Step: 3
Training loss: 1.858438491821289
Validation loss: 2.160650054613749

Epoch: 6| Step: 4
Training loss: 2.315521001815796
Validation loss: 2.1490521828333535

Epoch: 6| Step: 5
Training loss: 2.1165249347686768
Validation loss: 2.153074324131012

Epoch: 6| Step: 6
Training loss: 1.544135332107544
Validation loss: 2.1352309783299765

Epoch: 6| Step: 7
Training loss: 2.2349791526794434
Validation loss: 2.1538869539896646

Epoch: 6| Step: 8
Training loss: 1.826528549194336
Validation loss: 2.1459262371063232

Epoch: 6| Step: 9
Training loss: 2.2635297775268555
Validation loss: 2.15714160601298

Epoch: 6| Step: 10
Training loss: 2.103595495223999
Validation loss: 2.163973331451416

Epoch: 6| Step: 11
Training loss: 1.2558742761611938
Validation loss: 2.1705860098203025

Epoch: 6| Step: 12
Training loss: 1.4251055717468262
Validation loss: 2.1861642599105835

Epoch: 6| Step: 13
Training loss: 1.9526671171188354
Validation loss: 2.1482532024383545

Epoch: 209| Step: 0
Training loss: 2.025850772857666
Validation loss: 2.1634953022003174

Epoch: 6| Step: 1
Training loss: 1.740932583808899
Validation loss: 2.160711963971456

Epoch: 6| Step: 2
Training loss: 2.3782026767730713
Validation loss: 2.167247176170349

Epoch: 6| Step: 3
Training loss: 1.8649052381515503
Validation loss: 2.1494051615397134

Epoch: 6| Step: 4
Training loss: 1.707956075668335
Validation loss: 2.177839001019796

Epoch: 6| Step: 5
Training loss: 2.3333897590637207
Validation loss: 2.1710179249445596

Epoch: 6| Step: 6
Training loss: 1.5168540477752686
Validation loss: 2.194722135861715

Epoch: 6| Step: 7
Training loss: 2.720792055130005
Validation loss: 2.166516681512197

Epoch: 6| Step: 8
Training loss: 1.748558521270752
Validation loss: 2.172725240389506

Epoch: 6| Step: 9
Training loss: 2.0550904273986816
Validation loss: 2.151097615559896

Epoch: 6| Step: 10
Training loss: 1.5053350925445557
Validation loss: 2.167099674542745

Epoch: 6| Step: 11
Training loss: 1.3597332239151
Validation loss: 2.1596699953079224

Epoch: 6| Step: 12
Training loss: 1.4931023120880127
Validation loss: 2.153912603855133

Epoch: 6| Step: 13
Training loss: 2.5598714351654053
Validation loss: 2.156731208165487

Epoch: 210| Step: 0
Training loss: 1.4882290363311768
Validation loss: 2.148427883783976

Epoch: 6| Step: 1
Training loss: 1.9591424465179443
Validation loss: 2.145934224128723

Epoch: 6| Step: 2
Training loss: 2.2430150508880615
Validation loss: 2.150632083415985

Epoch: 6| Step: 3
Training loss: 2.7839813232421875
Validation loss: 2.151902437210083

Epoch: 6| Step: 4
Training loss: 1.8903236389160156
Validation loss: 2.1531416177749634

Epoch: 6| Step: 5
Training loss: 1.646888017654419
Validation loss: 2.1571112871170044

Epoch: 6| Step: 6
Training loss: 1.863364815711975
Validation loss: 2.1424883604049683

Epoch: 6| Step: 7
Training loss: 2.117870807647705
Validation loss: 2.162711183230082

Epoch: 6| Step: 8
Training loss: 1.6790225505828857
Validation loss: 2.1468457778294883

Epoch: 6| Step: 9
Training loss: 1.892076849937439
Validation loss: 2.155367990334829

Epoch: 6| Step: 10
Training loss: 2.0127458572387695
Validation loss: 2.1533174912134805

Epoch: 6| Step: 11
Training loss: 1.7692073583602905
Validation loss: 2.146413048108419

Epoch: 6| Step: 12
Training loss: 1.4428679943084717
Validation loss: 2.1764761010805764

Epoch: 6| Step: 13
Training loss: 1.6208131313323975
Validation loss: 2.1423755288124084

Epoch: 211| Step: 0
Training loss: 1.4305477142333984
Validation loss: 2.148380915323893

Epoch: 6| Step: 1
Training loss: 2.0289430618286133
Validation loss: 2.1563785672187805

Epoch: 6| Step: 2
Training loss: 1.2005425691604614
Validation loss: 2.1609182159105935

Epoch: 6| Step: 3
Training loss: 2.0708279609680176
Validation loss: 2.1511587103207908

Epoch: 6| Step: 4
Training loss: 2.4286000728607178
Validation loss: 2.145881175994873

Epoch: 6| Step: 5
Training loss: 1.8333029747009277
Validation loss: 2.1519434054692588

Epoch: 6| Step: 6
Training loss: 1.8633649349212646
Validation loss: 2.144879460334778

Epoch: 6| Step: 7
Training loss: 1.6892454624176025
Validation loss: 2.127281586329142

Epoch: 6| Step: 8
Training loss: 1.7750134468078613
Validation loss: 2.1473016937573752

Epoch: 6| Step: 9
Training loss: 1.3523389101028442
Validation loss: 2.115197320779165

Epoch: 6| Step: 10
Training loss: 2.158825397491455
Validation loss: 2.1477487881978354

Epoch: 6| Step: 11
Training loss: 2.509960889816284
Validation loss: 2.155145009358724

Epoch: 6| Step: 12
Training loss: 2.1519582271575928
Validation loss: 2.154901683330536

Epoch: 6| Step: 13
Training loss: 2.473466396331787
Validation loss: 2.1382078727086387

Epoch: 212| Step: 0
Training loss: 1.854464054107666
Validation loss: 2.1447325150171914

Epoch: 6| Step: 1
Training loss: 1.7476087808609009
Validation loss: 2.1482189297676086

Epoch: 6| Step: 2
Training loss: 1.160251259803772
Validation loss: 2.1511627038319907

Epoch: 6| Step: 3
Training loss: 1.7164249420166016
Validation loss: 2.1584650675455728

Epoch: 6| Step: 4
Training loss: 1.6511986255645752
Validation loss: 2.1621721982955933

Epoch: 6| Step: 5
Training loss: 2.11910343170166
Validation loss: 2.165535092353821

Epoch: 6| Step: 6
Training loss: 1.4008874893188477
Validation loss: 2.168237566947937

Epoch: 6| Step: 7
Training loss: 1.7404587268829346
Validation loss: 2.175949494043986

Epoch: 6| Step: 8
Training loss: 2.449408531188965
Validation loss: 2.1819210847218833

Epoch: 6| Step: 9
Training loss: 2.6481261253356934
Validation loss: 2.1701043049494424

Epoch: 6| Step: 10
Training loss: 1.3714133501052856
Validation loss: 2.170360247294108

Epoch: 6| Step: 11
Training loss: 2.31489896774292
Validation loss: 2.165088713169098

Epoch: 6| Step: 12
Training loss: 2.7082157135009766
Validation loss: 2.1739392081896463

Epoch: 6| Step: 13
Training loss: 1.3094340562820435
Validation loss: 2.190092364947001

Epoch: 213| Step: 0
Training loss: 1.99188232421875
Validation loss: 2.1682302157084146

Epoch: 6| Step: 1
Training loss: 1.6253712177276611
Validation loss: 2.1826847394307456

Epoch: 6| Step: 2
Training loss: 2.010849952697754
Validation loss: 2.1619261105855307

Epoch: 6| Step: 3
Training loss: 1.4424636363983154
Validation loss: 2.1710930267969766

Epoch: 6| Step: 4
Training loss: 1.6704108715057373
Validation loss: 2.173170030117035

Epoch: 6| Step: 5
Training loss: 1.6491475105285645
Validation loss: 2.177284280459086

Epoch: 6| Step: 6
Training loss: 2.125589609146118
Validation loss: 2.181412100791931

Epoch: 6| Step: 7
Training loss: 1.7240324020385742
Validation loss: 2.1745236118634543

Epoch: 6| Step: 8
Training loss: 2.3793258666992188
Validation loss: 2.16354238986969

Epoch: 6| Step: 9
Training loss: 2.164773941040039
Validation loss: 2.1663140455881753

Epoch: 6| Step: 10
Training loss: 2.367696762084961
Validation loss: 2.1568865378697715

Epoch: 6| Step: 11
Training loss: 2.0123608112335205
Validation loss: 2.1599106589953103

Epoch: 6| Step: 12
Training loss: 1.7567821741104126
Validation loss: 2.1363315979639688

Epoch: 6| Step: 13
Training loss: 1.5314637422561646
Validation loss: 2.1456588308016458

Epoch: 214| Step: 0
Training loss: 1.3270752429962158
Validation loss: 2.1502676804860434

Epoch: 6| Step: 1
Training loss: 2.0287981033325195
Validation loss: 2.126671473185221

Epoch: 6| Step: 2
Training loss: 1.5617542266845703
Validation loss: 2.1414532462755838

Epoch: 6| Step: 3
Training loss: 1.8624013662338257
Validation loss: 2.14375634988149

Epoch: 6| Step: 4
Training loss: 2.1797122955322266
Validation loss: 2.1412514050801597

Epoch: 6| Step: 5
Training loss: 1.572932481765747
Validation loss: 2.1541441679000854

Epoch: 6| Step: 6
Training loss: 1.3782191276550293
Validation loss: 2.1327367226282754

Epoch: 6| Step: 7
Training loss: 2.945399522781372
Validation loss: 2.1689164638519287

Epoch: 6| Step: 8
Training loss: 2.5742409229278564
Validation loss: 2.1641212900479636

Epoch: 6| Step: 9
Training loss: 1.5367300510406494
Validation loss: 2.1487479408582053

Epoch: 6| Step: 10
Training loss: 1.9370992183685303
Validation loss: 2.157991429169973

Epoch: 6| Step: 11
Training loss: 1.8264981508255005
Validation loss: 2.1584572792053223

Epoch: 6| Step: 12
Training loss: 2.1815128326416016
Validation loss: 2.162052015463511

Epoch: 6| Step: 13
Training loss: 1.4299871921539307
Validation loss: 2.156626582145691

Epoch: 215| Step: 0
Training loss: 2.0809788703918457
Validation loss: 2.1623608668645224

Epoch: 6| Step: 1
Training loss: 2.3253085613250732
Validation loss: 2.170065442721049

Epoch: 6| Step: 2
Training loss: 1.311397910118103
Validation loss: 2.166918476422628

Epoch: 6| Step: 3
Training loss: 1.941880226135254
Validation loss: 2.156373381614685

Epoch: 6| Step: 4
Training loss: 1.7536489963531494
Validation loss: 2.162623882293701

Epoch: 6| Step: 5
Training loss: 1.5126265287399292
Validation loss: 2.1648235519727073

Epoch: 6| Step: 6
Training loss: 2.5233500003814697
Validation loss: 2.1755517721176147

Epoch: 6| Step: 7
Training loss: 2.139591693878174
Validation loss: 2.183662176132202

Epoch: 6| Step: 8
Training loss: 1.8149216175079346
Validation loss: 2.1696754495302835

Epoch: 6| Step: 9
Training loss: 1.906158447265625
Validation loss: 2.1607825557390847

Epoch: 6| Step: 10
Training loss: 1.8452731370925903
Validation loss: 2.1575605869293213

Epoch: 6| Step: 11
Training loss: 1.7940444946289062
Validation loss: 2.1842231154441833

Epoch: 6| Step: 12
Training loss: 1.6445293426513672
Validation loss: 2.164136052131653

Epoch: 6| Step: 13
Training loss: 1.5299299955368042
Validation loss: 2.164582908153534

Epoch: 216| Step: 0
Training loss: 1.8541641235351562
Validation loss: 2.156369666258494

Epoch: 6| Step: 1
Training loss: 2.0006439685821533
Validation loss: 2.168982287247976

Epoch: 6| Step: 2
Training loss: 1.5116245746612549
Validation loss: 2.1390129129091897

Epoch: 6| Step: 3
Training loss: 1.5386624336242676
Validation loss: 2.1614983677864075

Epoch: 6| Step: 4
Training loss: 1.9873547554016113
Validation loss: 2.162847717603048

Epoch: 6| Step: 5
Training loss: 1.9899119138717651
Validation loss: 2.1690481106440225

Epoch: 6| Step: 6
Training loss: 2.314955949783325
Validation loss: 2.15900049606959

Epoch: 6| Step: 7
Training loss: 2.3039541244506836
Validation loss: 2.1563326716423035

Epoch: 6| Step: 8
Training loss: 2.332704544067383
Validation loss: 2.167274216810862

Epoch: 6| Step: 9
Training loss: 1.897395372390747
Validation loss: 2.17194664478302

Epoch: 6| Step: 10
Training loss: 2.6767334938049316
Validation loss: 2.158609708150228

Epoch: 6| Step: 11
Training loss: 1.0210394859313965
Validation loss: 2.1578436891237893

Epoch: 6| Step: 12
Training loss: 1.6535563468933105
Validation loss: 2.1581307450930276

Epoch: 6| Step: 13
Training loss: 1.5551577806472778
Validation loss: 2.1508065462112427

Epoch: 217| Step: 0
Training loss: 2.3623509407043457
Validation loss: 2.1577619314193726

Epoch: 6| Step: 1
Training loss: 2.2074906826019287
Validation loss: 2.1418798367182412

Epoch: 6| Step: 2
Training loss: 1.797156572341919
Validation loss: 2.145341952641805

Epoch: 6| Step: 3
Training loss: 2.373444080352783
Validation loss: 2.127832293510437

Epoch: 6| Step: 4
Training loss: 2.0231401920318604
Validation loss: 2.1455409129460654

Epoch: 6| Step: 5
Training loss: 1.8604846000671387
Validation loss: 2.1125171184539795

Epoch: 6| Step: 6
Training loss: 1.6901596784591675
Validation loss: 2.124237378438314

Epoch: 6| Step: 7
Training loss: 2.6171860694885254
Validation loss: 2.111923416455587

Epoch: 6| Step: 8
Training loss: 1.8516006469726562
Validation loss: 2.1419352094332376

Epoch: 6| Step: 9
Training loss: 1.1804075241088867
Validation loss: 2.1348201433817544

Epoch: 6| Step: 10
Training loss: 1.5481960773468018
Validation loss: 2.16890017191569

Epoch: 6| Step: 11
Training loss: 1.7312110662460327
Validation loss: 2.154960592587789

Epoch: 6| Step: 12
Training loss: 1.662928581237793
Validation loss: 2.170011281967163

Epoch: 6| Step: 13
Training loss: 1.6990869045257568
Validation loss: 2.1413275996843972

Epoch: 218| Step: 0
Training loss: 1.1784391403198242
Validation loss: 2.167445441087087

Epoch: 6| Step: 1
Training loss: 1.963758111000061
Validation loss: 2.150392452875773

Epoch: 6| Step: 2
Training loss: 2.4396495819091797
Validation loss: 2.1523730754852295

Epoch: 6| Step: 3
Training loss: 2.1071600914001465
Validation loss: 2.1541737715403237

Epoch: 6| Step: 4
Training loss: 1.5141538381576538
Validation loss: 2.15989488363266

Epoch: 6| Step: 5
Training loss: 2.528951644897461
Validation loss: 2.168256938457489

Epoch: 6| Step: 6
Training loss: 2.4038538932800293
Validation loss: 2.1650489370028176

Epoch: 6| Step: 7
Training loss: 1.9240984916687012
Validation loss: 2.160359263420105

Epoch: 6| Step: 8
Training loss: 1.7751073837280273
Validation loss: 2.1643446683883667

Epoch: 6| Step: 9
Training loss: 1.804459810256958
Validation loss: 2.1665075421333313

Epoch: 6| Step: 10
Training loss: 1.394031047821045
Validation loss: 2.1592871944109597

Epoch: 6| Step: 11
Training loss: 2.4957823753356934
Validation loss: 2.1699758768081665

Epoch: 6| Step: 12
Training loss: 1.42201566696167
Validation loss: 2.16159725189209

Epoch: 6| Step: 13
Training loss: 1.535760521888733
Validation loss: 2.1527310609817505

Epoch: 219| Step: 0
Training loss: 1.6765364408493042
Validation loss: 2.153548220793406

Epoch: 6| Step: 1
Training loss: 1.9566452503204346
Validation loss: 2.159119407335917

Epoch: 6| Step: 2
Training loss: 1.7251588106155396
Validation loss: 2.1608097155888877

Epoch: 6| Step: 3
Training loss: 2.918107509613037
Validation loss: 2.181970993677775

Epoch: 6| Step: 4
Training loss: 2.3034987449645996
Validation loss: 2.1649040977160134

Epoch: 6| Step: 5
Training loss: 1.1617109775543213
Validation loss: 2.1580369075139365

Epoch: 6| Step: 6
Training loss: 1.48563814163208
Validation loss: 2.180899222691854

Epoch: 6| Step: 7
Training loss: 1.6570217609405518
Validation loss: 2.183900157610575

Epoch: 6| Step: 8
Training loss: 1.8820654153823853
Validation loss: 2.1690853436787925

Epoch: 6| Step: 9
Training loss: 1.6049611568450928
Validation loss: 2.1730043292045593

Epoch: 6| Step: 10
Training loss: 1.888343334197998
Validation loss: 2.152381658554077

Epoch: 6| Step: 11
Training loss: 1.3323278427124023
Validation loss: 2.1723848779996238

Epoch: 6| Step: 12
Training loss: 2.2394399642944336
Validation loss: 2.1872595945994058

Epoch: 6| Step: 13
Training loss: 2.168736696243286
Validation loss: 2.1919115781784058

Epoch: 220| Step: 0
Training loss: 1.741687297821045
Validation loss: 2.1897090673446655

Epoch: 6| Step: 1
Training loss: 2.0280144214630127
Validation loss: 2.189404229323069

Epoch: 6| Step: 2
Training loss: 1.9353914260864258
Validation loss: 2.1905205647150674

Epoch: 6| Step: 3
Training loss: 1.0952757596969604
Validation loss: 2.1891377369562783

Epoch: 6| Step: 4
Training loss: 1.3311305046081543
Validation loss: 2.2014183600743613

Epoch: 6| Step: 5
Training loss: 2.153923511505127
Validation loss: 2.193044126033783

Epoch: 6| Step: 6
Training loss: 2.213412284851074
Validation loss: 2.1943838000297546

Epoch: 6| Step: 7
Training loss: 1.4648244380950928
Validation loss: 2.179847558339437

Epoch: 6| Step: 8
Training loss: 2.5061802864074707
Validation loss: 2.1774654388427734

Epoch: 6| Step: 9
Training loss: 1.315163016319275
Validation loss: 2.1749112208684287

Epoch: 6| Step: 10
Training loss: 1.600799560546875
Validation loss: 2.1903938253720603

Epoch: 6| Step: 11
Training loss: 2.2683396339416504
Validation loss: 2.180470367272695

Epoch: 6| Step: 12
Training loss: 2.9120898246765137
Validation loss: 2.1868523160616555

Epoch: 6| Step: 13
Training loss: 1.979653239250183
Validation loss: 2.1666552424430847

Epoch: 221| Step: 0
Training loss: 2.3669731616973877
Validation loss: 2.181169311205546

Epoch: 6| Step: 1
Training loss: 1.8589653968811035
Validation loss: 2.16271710395813

Epoch: 6| Step: 2
Training loss: 1.155985713005066
Validation loss: 2.1971876422564187

Epoch: 6| Step: 3
Training loss: 1.8394594192504883
Validation loss: 2.1799813310305276

Epoch: 6| Step: 4
Training loss: 1.631481647491455
Validation loss: 2.1841920415560403

Epoch: 6| Step: 5
Training loss: 1.7238556146621704
Validation loss: 2.1757110357284546

Epoch: 6| Step: 6
Training loss: 2.0184829235076904
Validation loss: 2.174874722957611

Epoch: 6| Step: 7
Training loss: 1.7679705619812012
Validation loss: 2.1809735099474588

Epoch: 6| Step: 8
Training loss: 2.603822946548462
Validation loss: 2.163440446058909

Epoch: 6| Step: 9
Training loss: 2.0614819526672363
Validation loss: 2.194693128267924

Epoch: 6| Step: 10
Training loss: 1.904856562614441
Validation loss: 2.180157701174418

Epoch: 6| Step: 11
Training loss: 1.3741395473480225
Validation loss: 2.16279935836792

Epoch: 6| Step: 12
Training loss: 1.7060014009475708
Validation loss: 2.1759654879570007

Epoch: 6| Step: 13
Training loss: 2.006946563720703
Validation loss: 2.166580239931742

Epoch: 222| Step: 0
Training loss: 1.4182798862457275
Validation loss: 2.1724754571914673

Epoch: 6| Step: 1
Training loss: 2.2100462913513184
Validation loss: 2.1571292082468667

Epoch: 6| Step: 2
Training loss: 1.710195779800415
Validation loss: 2.175081173578898

Epoch: 6| Step: 3
Training loss: 2.156980037689209
Validation loss: 2.171836038430532

Epoch: 6| Step: 4
Training loss: 2.020874500274658
Validation loss: 2.180749793847402

Epoch: 6| Step: 5
Training loss: 1.6179324388504028
Validation loss: 2.171195864677429

Epoch: 6| Step: 6
Training loss: 2.0908286571502686
Validation loss: 2.1584993998209634

Epoch: 6| Step: 7
Training loss: 2.2377982139587402
Validation loss: 2.1677114367485046

Epoch: 6| Step: 8
Training loss: 2.0149009227752686
Validation loss: 2.1562756299972534

Epoch: 6| Step: 9
Training loss: 1.5988025665283203
Validation loss: 2.1535513401031494

Epoch: 6| Step: 10
Training loss: 1.5678670406341553
Validation loss: 2.1578196088473

Epoch: 6| Step: 11
Training loss: 2.224303960800171
Validation loss: 2.1427436470985413

Epoch: 6| Step: 12
Training loss: 1.8875279426574707
Validation loss: 2.1465518474578857

Epoch: 6| Step: 13
Training loss: 1.5667414665222168
Validation loss: 2.154768228530884

Epoch: 223| Step: 0
Training loss: 2.036818742752075
Validation loss: 2.1381530364354453

Epoch: 6| Step: 1
Training loss: 1.852400779724121
Validation loss: 2.111508925755819

Epoch: 6| Step: 2
Training loss: 1.7248469591140747
Validation loss: 2.129534204800924

Epoch: 6| Step: 3
Training loss: 1.4588195085525513
Validation loss: 2.1092538237571716

Epoch: 6| Step: 4
Training loss: 2.1257660388946533
Validation loss: 2.130766749382019

Epoch: 6| Step: 5
Training loss: 1.9778834581375122
Validation loss: 2.1475750207901

Epoch: 6| Step: 6
Training loss: 2.0031492710113525
Validation loss: 2.1455817818641663

Epoch: 6| Step: 7
Training loss: 1.8495467901229858
Validation loss: 2.152998427549998

Epoch: 6| Step: 8
Training loss: 2.2324512004852295
Validation loss: 2.1828089157740274

Epoch: 6| Step: 9
Training loss: 1.0804831981658936
Validation loss: 2.1758690675099692

Epoch: 6| Step: 10
Training loss: 1.7299224138259888
Validation loss: 2.175890068213145

Epoch: 6| Step: 11
Training loss: 1.9367469549179077
Validation loss: 2.164182742436727

Epoch: 6| Step: 12
Training loss: 1.6907048225402832
Validation loss: 2.1712811986605325

Epoch: 6| Step: 13
Training loss: 2.612621545791626
Validation loss: 2.156064967314402

Epoch: 224| Step: 0
Training loss: 1.5023523569107056
Validation loss: 2.164427101612091

Epoch: 6| Step: 1
Training loss: 1.27455735206604
Validation loss: 2.184825519720713

Epoch: 6| Step: 2
Training loss: 1.7758111953735352
Validation loss: 2.179958701133728

Epoch: 6| Step: 3
Training loss: 2.5820116996765137
Validation loss: 2.167542338371277

Epoch: 6| Step: 4
Training loss: 1.627634048461914
Validation loss: 2.1814687649408975

Epoch: 6| Step: 5
Training loss: 2.198185682296753
Validation loss: 2.177466332912445

Epoch: 6| Step: 6
Training loss: 2.460019588470459
Validation loss: 2.187056581179301

Epoch: 6| Step: 7
Training loss: 1.532607913017273
Validation loss: 2.1832470297813416

Epoch: 6| Step: 8
Training loss: 2.4405288696289062
Validation loss: 2.1725366512934365

Epoch: 6| Step: 9
Training loss: 1.5395610332489014
Validation loss: 2.1687920292218528

Epoch: 6| Step: 10
Training loss: 1.933306097984314
Validation loss: 2.2023955980936685

Epoch: 6| Step: 11
Training loss: 1.5006860494613647
Validation loss: 2.2072928746541343

Epoch: 6| Step: 12
Training loss: 2.35623836517334
Validation loss: 2.1880677342414856

Epoch: 6| Step: 13
Training loss: 2.3714256286621094
Validation loss: 2.169467806816101

Epoch: 225| Step: 0
Training loss: 1.8921680450439453
Validation loss: 2.1452879707018533

Epoch: 6| Step: 1
Training loss: 2.091029167175293
Validation loss: 2.1466947396596274

Epoch: 6| Step: 2
Training loss: 2.3493289947509766
Validation loss: 2.1431621313095093

Epoch: 6| Step: 3
Training loss: 1.9193395376205444
Validation loss: 2.105254312356313

Epoch: 6| Step: 4
Training loss: 2.3437628746032715
Validation loss: 2.1219846606254578

Epoch: 6| Step: 5
Training loss: 1.4408777952194214
Validation loss: 2.1080684065818787

Epoch: 6| Step: 6
Training loss: 1.8267128467559814
Validation loss: 2.1156888008117676

Epoch: 6| Step: 7
Training loss: 2.3999648094177246
Validation loss: 2.152004818121592

Epoch: 6| Step: 8
Training loss: 1.6471383571624756
Validation loss: 2.150654355684916

Epoch: 6| Step: 9
Training loss: 1.7544984817504883
Validation loss: 2.1443260510762534

Epoch: 6| Step: 10
Training loss: 1.3482856750488281
Validation loss: 2.1473114093144736

Epoch: 6| Step: 11
Training loss: 2.67446231842041
Validation loss: 2.14638880888621

Epoch: 6| Step: 12
Training loss: 1.9367727041244507
Validation loss: 2.148458937803904

Epoch: 6| Step: 13
Training loss: 1.8636785745620728
Validation loss: 2.153870383898417

Epoch: 226| Step: 0
Training loss: 1.6578283309936523
Validation loss: 2.1513598759969077

Epoch: 6| Step: 1
Training loss: 1.6913371086120605
Validation loss: 2.1544610261917114

Epoch: 6| Step: 2
Training loss: 2.210841655731201
Validation loss: 2.1562968691190085

Epoch: 6| Step: 3
Training loss: 2.010925054550171
Validation loss: 2.1509451071421304

Epoch: 6| Step: 4
Training loss: 1.9501783847808838
Validation loss: 2.139129380385081

Epoch: 6| Step: 5
Training loss: 1.7867257595062256
Validation loss: 2.159210483233134

Epoch: 6| Step: 6
Training loss: 1.8431105613708496
Validation loss: 2.136814057826996

Epoch: 6| Step: 7
Training loss: 2.287877082824707
Validation loss: 2.1526325941085815

Epoch: 6| Step: 8
Training loss: 2.009244441986084
Validation loss: 2.141897658507029

Epoch: 6| Step: 9
Training loss: 1.5616828203201294
Validation loss: 2.137348473072052

Epoch: 6| Step: 10
Training loss: 1.5755494832992554
Validation loss: 2.153106908003489

Epoch: 6| Step: 11
Training loss: 1.821123719215393
Validation loss: 2.1341520150502524

Epoch: 6| Step: 12
Training loss: 1.857290267944336
Validation loss: 2.1498336593310037

Epoch: 6| Step: 13
Training loss: 1.9218181371688843
Validation loss: 2.1460242668787637

Epoch: 227| Step: 0
Training loss: 1.623110055923462
Validation loss: 2.169405698776245

Epoch: 6| Step: 1
Training loss: 2.0016140937805176
Validation loss: 2.1708964506785073

Epoch: 6| Step: 2
Training loss: 1.945338487625122
Validation loss: 2.196907083193461

Epoch: 6| Step: 3
Training loss: 1.313920497894287
Validation loss: 2.1567803223927817

Epoch: 6| Step: 4
Training loss: 1.2716670036315918
Validation loss: 2.1730451385180154

Epoch: 6| Step: 5
Training loss: 1.7262346744537354
Validation loss: 2.1978139877319336

Epoch: 6| Step: 6
Training loss: 1.5695974826812744
Validation loss: 2.175805668036143

Epoch: 6| Step: 7
Training loss: 2.2841591835021973
Validation loss: 2.199835558732351

Epoch: 6| Step: 8
Training loss: 2.6951241493225098
Validation loss: 2.164378503958384

Epoch: 6| Step: 9
Training loss: 1.529109001159668
Validation loss: 2.2027896841367087

Epoch: 6| Step: 10
Training loss: 2.270218849182129
Validation loss: 2.191967705885569

Epoch: 6| Step: 11
Training loss: 1.6671562194824219
Validation loss: 2.157595932483673

Epoch: 6| Step: 12
Training loss: 1.672507643699646
Validation loss: 2.182460288206736

Epoch: 6| Step: 13
Training loss: 2.4054746627807617
Validation loss: 2.1865047415097556

Epoch: 228| Step: 0
Training loss: 1.951028823852539
Validation loss: 2.1661441922187805

Epoch: 6| Step: 1
Training loss: 1.1530717611312866
Validation loss: 2.1831032037734985

Epoch: 6| Step: 2
Training loss: 2.3544719219207764
Validation loss: 2.1985005537668862

Epoch: 6| Step: 3
Training loss: 1.6665781736373901
Validation loss: 2.203871746857961

Epoch: 6| Step: 4
Training loss: 1.826399326324463
Validation loss: 2.1997463504473367

Epoch: 6| Step: 5
Training loss: 1.8357529640197754
Validation loss: 2.1777531703313193

Epoch: 6| Step: 6
Training loss: 2.3717727661132812
Validation loss: 2.1786473989486694

Epoch: 6| Step: 7
Training loss: 1.9544774293899536
Validation loss: 2.192377785841624

Epoch: 6| Step: 8
Training loss: 1.8429994583129883
Validation loss: 2.1780981024106345

Epoch: 6| Step: 9
Training loss: 1.4692598581314087
Validation loss: 2.161523083845774

Epoch: 6| Step: 10
Training loss: 2.322937488555908
Validation loss: 2.1889043052991233

Epoch: 6| Step: 11
Training loss: 1.6652946472167969
Validation loss: 2.1785144209861755

Epoch: 6| Step: 12
Training loss: 1.5377941131591797
Validation loss: 2.1759987672170005

Epoch: 6| Step: 13
Training loss: 1.5237908363342285
Validation loss: 2.215344170729319

Epoch: 229| Step: 0
Training loss: 1.9403032064437866
Validation loss: 2.192625641822815

Epoch: 6| Step: 1
Training loss: 1.3575719594955444
Validation loss: 2.1896137992540994

Epoch: 6| Step: 2
Training loss: 1.344506025314331
Validation loss: 2.1856472889582315

Epoch: 6| Step: 3
Training loss: 2.4865715503692627
Validation loss: 2.1744034687678018

Epoch: 6| Step: 4
Training loss: 1.658638596534729
Validation loss: 2.1805479327837625

Epoch: 6| Step: 5
Training loss: 2.8455452919006348
Validation loss: 2.1753235856691995

Epoch: 6| Step: 6
Training loss: 1.6298298835754395
Validation loss: 2.1825767358144126

Epoch: 6| Step: 7
Training loss: 2.0669171810150146
Validation loss: 2.1853240331014

Epoch: 6| Step: 8
Training loss: 2.108900308609009
Validation loss: 2.164482374986013

Epoch: 6| Step: 9
Training loss: 1.302208423614502
Validation loss: 2.2025596698125205

Epoch: 6| Step: 10
Training loss: 1.5906665325164795
Validation loss: 2.1799891789754233

Epoch: 6| Step: 11
Training loss: 2.1791844367980957
Validation loss: 2.1687912742296853

Epoch: 6| Step: 12
Training loss: 2.1151833534240723
Validation loss: 2.182681997617086

Epoch: 6| Step: 13
Training loss: 1.2520203590393066
Validation loss: 2.1726905703544617

Epoch: 230| Step: 0
Training loss: 1.548563003540039
Validation loss: 2.171193778514862

Epoch: 6| Step: 1
Training loss: 1.6444988250732422
Validation loss: 2.1674267252286277

Epoch: 6| Step: 2
Training loss: 1.697566270828247
Validation loss: 2.1684294939041138

Epoch: 6| Step: 3
Training loss: 1.7174441814422607
Validation loss: 2.1627163688341775

Epoch: 6| Step: 4
Training loss: 1.8609066009521484
Validation loss: 2.1824910243352256

Epoch: 6| Step: 5
Training loss: 1.7617120742797852
Validation loss: 2.174890637397766

Epoch: 6| Step: 6
Training loss: 2.746016025543213
Validation loss: 2.174155036608378

Epoch: 6| Step: 7
Training loss: 2.216583251953125
Validation loss: 2.169985771179199

Epoch: 6| Step: 8
Training loss: 2.0216822624206543
Validation loss: 2.1778708497683206

Epoch: 6| Step: 9
Training loss: 1.4729163646697998
Validation loss: 2.1810586055119834

Epoch: 6| Step: 10
Training loss: 2.0411529541015625
Validation loss: 2.1804553667704263

Epoch: 6| Step: 11
Training loss: 1.4394055604934692
Validation loss: 2.1920214096705117

Epoch: 6| Step: 12
Training loss: 1.427249550819397
Validation loss: 2.173699458440145

Epoch: 6| Step: 13
Training loss: 1.8490009307861328
Validation loss: 2.183616797129313

Epoch: 231| Step: 0
Training loss: 2.119935989379883
Validation loss: 2.181833287080129

Epoch: 6| Step: 1
Training loss: 1.9237120151519775
Validation loss: 2.188934942086538

Epoch: 6| Step: 2
Training loss: 1.7509030103683472
Validation loss: 2.163244386514028

Epoch: 6| Step: 3
Training loss: 2.204394578933716
Validation loss: 2.1845725774765015

Epoch: 6| Step: 4
Training loss: 1.4303983449935913
Validation loss: 2.1990637381871543

Epoch: 6| Step: 5
Training loss: 1.5351271629333496
Validation loss: 2.18092010418574

Epoch: 6| Step: 6
Training loss: 2.4887495040893555
Validation loss: 2.1764533122380576

Epoch: 6| Step: 7
Training loss: 1.7035601139068604
Validation loss: 2.1694419582684836

Epoch: 6| Step: 8
Training loss: 1.6172168254852295
Validation loss: 2.1895458896954856

Epoch: 6| Step: 9
Training loss: 1.8337228298187256
Validation loss: 2.2131534020105996

Epoch: 6| Step: 10
Training loss: 1.5624891519546509
Validation loss: 2.219841261704763

Epoch: 6| Step: 11
Training loss: 1.7562965154647827
Validation loss: 2.193751315275828

Epoch: 6| Step: 12
Training loss: 1.5620207786560059
Validation loss: 2.195796032746633

Epoch: 6| Step: 13
Training loss: 2.364996910095215
Validation loss: 2.2018584410349527

Epoch: 232| Step: 0
Training loss: 1.219600796699524
Validation loss: 2.171969175338745

Epoch: 6| Step: 1
Training loss: 1.2236371040344238
Validation loss: 2.1689273913701377

Epoch: 6| Step: 2
Training loss: 2.0015053749084473
Validation loss: 2.1677475968996682

Epoch: 6| Step: 3
Training loss: 1.7159833908081055
Validation loss: 2.205328047275543

Epoch: 6| Step: 4
Training loss: 1.2272486686706543
Validation loss: 2.1955365339914956

Epoch: 6| Step: 5
Training loss: 2.1361749172210693
Validation loss: 2.1772151390711465

Epoch: 6| Step: 6
Training loss: 1.4477938413619995
Validation loss: 2.198865830898285

Epoch: 6| Step: 7
Training loss: 1.58660888671875
Validation loss: 2.2069859306017556

Epoch: 6| Step: 8
Training loss: 1.9684886932373047
Validation loss: 2.173075000445048

Epoch: 6| Step: 9
Training loss: 2.224557399749756
Validation loss: 2.204824964205424

Epoch: 6| Step: 10
Training loss: 1.8883205652236938
Validation loss: 2.1691524585088096

Epoch: 6| Step: 11
Training loss: 1.752422571182251
Validation loss: 2.1842437982559204

Epoch: 6| Step: 12
Training loss: 2.4039242267608643
Validation loss: 2.212369978427887

Epoch: 6| Step: 13
Training loss: 2.3252062797546387
Validation loss: 2.1878161827723184

Epoch: 233| Step: 0
Training loss: 1.5073204040527344
Validation loss: 2.1934475898742676

Epoch: 6| Step: 1
Training loss: 1.5280606746673584
Validation loss: 2.159339706103007

Epoch: 6| Step: 2
Training loss: 1.895088791847229
Validation loss: 2.137398382027944

Epoch: 6| Step: 3
Training loss: 2.424029588699341
Validation loss: 2.184407591819763

Epoch: 6| Step: 4
Training loss: 2.4115705490112305
Validation loss: 2.160849134127299

Epoch: 6| Step: 5
Training loss: 1.5763345956802368
Validation loss: 2.1737149953842163

Epoch: 6| Step: 6
Training loss: 2.1800081729888916
Validation loss: 2.18380073706309

Epoch: 6| Step: 7
Training loss: 1.8201574087142944
Validation loss: 2.204948345820109

Epoch: 6| Step: 8
Training loss: 1.639731764793396
Validation loss: 2.1908864180246987

Epoch: 6| Step: 9
Training loss: 1.7686783075332642
Validation loss: 2.178391973177592

Epoch: 6| Step: 10
Training loss: 1.3417112827301025
Validation loss: 2.171556909879049

Epoch: 6| Step: 11
Training loss: 2.4071173667907715
Validation loss: 2.1883042057355246

Epoch: 6| Step: 12
Training loss: 1.6279101371765137
Validation loss: 2.1856868267059326

Epoch: 6| Step: 13
Training loss: 2.393885612487793
Validation loss: 2.161220649878184

Epoch: 234| Step: 0
Training loss: 1.6601731777191162
Validation loss: 2.1760576168696084

Epoch: 6| Step: 1
Training loss: 1.0669121742248535
Validation loss: 2.1666883627573648

Epoch: 6| Step: 2
Training loss: 1.50398850440979
Validation loss: 2.1674249370892844

Epoch: 6| Step: 3
Training loss: 1.386998176574707
Validation loss: 2.1780933340390525

Epoch: 6| Step: 4
Training loss: 2.49936580657959
Validation loss: 2.1911211212476096

Epoch: 6| Step: 5
Training loss: 1.7311081886291504
Validation loss: 2.192524234453837

Epoch: 6| Step: 6
Training loss: 2.2981433868408203
Validation loss: 2.1935847798983255

Epoch: 6| Step: 7
Training loss: 2.0064940452575684
Validation loss: 2.1814075112342834

Epoch: 6| Step: 8
Training loss: 1.2130612134933472
Validation loss: 2.187954227129618

Epoch: 6| Step: 9
Training loss: 1.4949109554290771
Validation loss: 2.198072691758474

Epoch: 6| Step: 10
Training loss: 2.137299060821533
Validation loss: 2.174180785814921

Epoch: 6| Step: 11
Training loss: 1.8620532751083374
Validation loss: 2.175429940223694

Epoch: 6| Step: 12
Training loss: 2.3149185180664062
Validation loss: 2.186708648999532

Epoch: 6| Step: 13
Training loss: 2.2114570140838623
Validation loss: 2.189660827318827

Epoch: 235| Step: 0
Training loss: 1.8826560974121094
Validation loss: 2.1946759819984436

Epoch: 6| Step: 1
Training loss: 1.331695318222046
Validation loss: 2.20997154712677

Epoch: 6| Step: 2
Training loss: 1.3050483465194702
Validation loss: 2.220645229021708

Epoch: 6| Step: 3
Training loss: 1.5473268032073975
Validation loss: 2.2298555771509805

Epoch: 6| Step: 4
Training loss: 2.192537784576416
Validation loss: 2.2044482231140137

Epoch: 6| Step: 5
Training loss: 1.6889076232910156
Validation loss: 2.2270795504252114

Epoch: 6| Step: 6
Training loss: 1.3624839782714844
Validation loss: 2.213661313056946

Epoch: 6| Step: 7
Training loss: 1.4718985557556152
Validation loss: 2.1879102190335593

Epoch: 6| Step: 8
Training loss: 2.2979469299316406
Validation loss: 2.2108560601870217

Epoch: 6| Step: 9
Training loss: 2.495906352996826
Validation loss: 2.2120191852251687

Epoch: 6| Step: 10
Training loss: 2.3814101219177246
Validation loss: 2.211806535720825

Epoch: 6| Step: 11
Training loss: 1.422688364982605
Validation loss: 2.197329521179199

Epoch: 6| Step: 12
Training loss: 2.0103917121887207
Validation loss: 2.186681588490804

Epoch: 6| Step: 13
Training loss: 2.1002912521362305
Validation loss: 2.2077037493387857

Epoch: 236| Step: 0
Training loss: 2.3406577110290527
Validation loss: 2.1918843189875283

Epoch: 6| Step: 1
Training loss: 1.2017321586608887
Validation loss: 2.2188722689946494

Epoch: 6| Step: 2
Training loss: 2.1027941703796387
Validation loss: 2.1936411261558533

Epoch: 6| Step: 3
Training loss: 1.8415321111679077
Validation loss: 2.2094832261403403

Epoch: 6| Step: 4
Training loss: 1.4504413604736328
Validation loss: 2.1871738831202188

Epoch: 6| Step: 5
Training loss: 2.160102605819702
Validation loss: 2.2032913168271384

Epoch: 6| Step: 6
Training loss: 2.075517177581787
Validation loss: 2.2191239992777505

Epoch: 6| Step: 7
Training loss: 1.767566442489624
Validation loss: 2.1824945410092673

Epoch: 6| Step: 8
Training loss: 2.3970634937286377
Validation loss: 2.20082821448644

Epoch: 6| Step: 9
Training loss: 1.8770283460617065
Validation loss: 2.193426191806793

Epoch: 6| Step: 10
Training loss: 1.7196974754333496
Validation loss: 2.1978386640548706

Epoch: 6| Step: 11
Training loss: 1.8642711639404297
Validation loss: 2.199870983759562

Epoch: 6| Step: 12
Training loss: 2.0294575691223145
Validation loss: 2.1801181634267173

Epoch: 6| Step: 13
Training loss: 1.429561734199524
Validation loss: 2.19235106309255

Epoch: 237| Step: 0
Training loss: 1.9764505624771118
Validation loss: 2.191507418950399

Epoch: 6| Step: 1
Training loss: 1.0485503673553467
Validation loss: 2.1953901449839273

Epoch: 6| Step: 2
Training loss: 1.701653242111206
Validation loss: 2.1980891823768616

Epoch: 6| Step: 3
Training loss: 2.2191529273986816
Validation loss: 2.190407872200012

Epoch: 6| Step: 4
Training loss: 1.5713222026824951
Validation loss: 2.20278791586558

Epoch: 6| Step: 5
Training loss: 2.405954122543335
Validation loss: 2.2055821816126504

Epoch: 6| Step: 6
Training loss: 2.078361749649048
Validation loss: 2.177932838598887

Epoch: 6| Step: 7
Training loss: 2.1179003715515137
Validation loss: 2.1968634128570557

Epoch: 6| Step: 8
Training loss: 2.001683235168457
Validation loss: 2.174565772215525

Epoch: 6| Step: 9
Training loss: 1.457705020904541
Validation loss: 2.174753208955129

Epoch: 6| Step: 10
Training loss: 1.706636667251587
Validation loss: 2.193429430325826

Epoch: 6| Step: 11
Training loss: 1.3039772510528564
Validation loss: 2.1837605237960815

Epoch: 6| Step: 12
Training loss: 1.8416163921356201
Validation loss: 2.163467446962992

Epoch: 6| Step: 13
Training loss: 2.1112282276153564
Validation loss: 2.183617035547892

Epoch: 238| Step: 0
Training loss: 1.4687786102294922
Validation loss: 2.161309858163198

Epoch: 6| Step: 1
Training loss: 1.8071529865264893
Validation loss: 2.190390706062317

Epoch: 6| Step: 2
Training loss: 2.453761577606201
Validation loss: 2.183071235815684

Epoch: 6| Step: 3
Training loss: 1.69380784034729
Validation loss: 2.183003544807434

Epoch: 6| Step: 4
Training loss: 1.695686936378479
Validation loss: 2.1954016288121543

Epoch: 6| Step: 5
Training loss: 1.9979314804077148
Validation loss: 2.1849793791770935

Epoch: 6| Step: 6
Training loss: 1.0541038513183594
Validation loss: 2.2073076566060386

Epoch: 6| Step: 7
Training loss: 2.3705077171325684
Validation loss: 2.204704523086548

Epoch: 6| Step: 8
Training loss: 1.711647629737854
Validation loss: 2.1859689354896545

Epoch: 6| Step: 9
Training loss: 1.4600610733032227
Validation loss: 2.191824515660604

Epoch: 6| Step: 10
Training loss: 1.790635347366333
Validation loss: 2.2005433440208435

Epoch: 6| Step: 11
Training loss: 2.4566097259521484
Validation loss: 2.209496637185415

Epoch: 6| Step: 12
Training loss: 2.1882505416870117
Validation loss: 2.2104876240094504

Epoch: 6| Step: 13
Training loss: 1.5548293590545654
Validation loss: 2.168632209300995

Epoch: 239| Step: 0
Training loss: 2.042340040206909
Validation loss: 2.185264070828756

Epoch: 6| Step: 1
Training loss: 1.8601622581481934
Validation loss: 2.173290411631266

Epoch: 6| Step: 2
Training loss: 2.2064476013183594
Validation loss: 2.1712113420168557

Epoch: 6| Step: 3
Training loss: 1.3721776008605957
Validation loss: 2.1890262365341187

Epoch: 6| Step: 4
Training loss: 1.6837449073791504
Validation loss: 2.1660540103912354

Epoch: 6| Step: 5
Training loss: 2.480377197265625
Validation loss: 2.1957308848698935

Epoch: 6| Step: 6
Training loss: 1.8487880229949951
Validation loss: 2.1561723351478577

Epoch: 6| Step: 7
Training loss: 1.6861531734466553
Validation loss: 2.166777014732361

Epoch: 6| Step: 8
Training loss: 2.4219183921813965
Validation loss: 2.1580002109209695

Epoch: 6| Step: 9
Training loss: 1.784644603729248
Validation loss: 2.1729220747947693

Epoch: 6| Step: 10
Training loss: 1.7184981107711792
Validation loss: 2.1594231128692627

Epoch: 6| Step: 11
Training loss: 1.6000487804412842
Validation loss: 2.188510298728943

Epoch: 6| Step: 12
Training loss: 1.593721628189087
Validation loss: 2.205609997113546

Epoch: 6| Step: 13
Training loss: 1.340732455253601
Validation loss: 2.1933091282844543

Epoch: 240| Step: 0
Training loss: 1.2278459072113037
Validation loss: 2.2172307769457498

Epoch: 6| Step: 1
Training loss: 2.2769620418548584
Validation loss: 2.2190675338109336

Epoch: 6| Step: 2
Training loss: 2.007077932357788
Validation loss: 2.2133659521738687

Epoch: 6| Step: 3
Training loss: 1.6063562631607056
Validation loss: 2.2042638063430786

Epoch: 6| Step: 4
Training loss: 2.1594598293304443
Validation loss: 2.207244078318278

Epoch: 6| Step: 5
Training loss: 1.9398608207702637
Validation loss: 2.2134907046953836

Epoch: 6| Step: 6
Training loss: 1.7014483213424683
Validation loss: 2.2331372102101645

Epoch: 6| Step: 7
Training loss: 1.7002652883529663
Validation loss: 2.226745367050171

Epoch: 6| Step: 8
Training loss: 1.9116640090942383
Validation loss: 2.1955884297688804

Epoch: 6| Step: 9
Training loss: 1.7483981847763062
Validation loss: 2.1831204295158386

Epoch: 6| Step: 10
Training loss: 1.9839868545532227
Validation loss: 2.1772589882214866

Epoch: 6| Step: 11
Training loss: 1.0704560279846191
Validation loss: 2.1968066294988

Epoch: 6| Step: 12
Training loss: 2.052654504776001
Validation loss: 2.188855012257894

Epoch: 6| Step: 13
Training loss: 1.9237737655639648
Validation loss: 2.1991174022356668

Epoch: 241| Step: 0
Training loss: 1.6976996660232544
Validation loss: 2.182281176249186

Epoch: 6| Step: 1
Training loss: 1.7209265232086182
Validation loss: 2.2053945064544678

Epoch: 6| Step: 2
Training loss: 1.8946484327316284
Validation loss: 2.1919211546579995

Epoch: 6| Step: 3
Training loss: 1.6730995178222656
Validation loss: 2.199965516726176

Epoch: 6| Step: 4
Training loss: 1.9487862586975098
Validation loss: 2.178372581799825

Epoch: 6| Step: 5
Training loss: 1.4130897521972656
Validation loss: 2.1887994408607483

Epoch: 6| Step: 6
Training loss: 1.9545495510101318
Validation loss: 2.1885568698247275

Epoch: 6| Step: 7
Training loss: 2.2816102504730225
Validation loss: 2.212587316830953

Epoch: 6| Step: 8
Training loss: 2.089578151702881
Validation loss: 2.18471089998881

Epoch: 6| Step: 9
Training loss: 1.510655164718628
Validation loss: 2.2068825165430703

Epoch: 6| Step: 10
Training loss: 2.144272804260254
Validation loss: 2.1907951633135476

Epoch: 6| Step: 11
Training loss: 1.6299078464508057
Validation loss: 2.2103156248728433

Epoch: 6| Step: 12
Training loss: 1.4667716026306152
Validation loss: 2.2128010789553323

Epoch: 6| Step: 13
Training loss: 1.711045265197754
Validation loss: 2.216549058755239

Epoch: 242| Step: 0
Training loss: 2.076742172241211
Validation loss: 2.2102161248524985

Epoch: 6| Step: 1
Training loss: 1.3597097396850586
Validation loss: 2.206985135873159

Epoch: 6| Step: 2
Training loss: 2.374312400817871
Validation loss: 2.2052290638287864

Epoch: 6| Step: 3
Training loss: 2.379852533340454
Validation loss: 2.174320717652639

Epoch: 6| Step: 4
Training loss: 1.915948510169983
Validation loss: 2.2059587836265564

Epoch: 6| Step: 5
Training loss: 1.4765244722366333
Validation loss: 2.196998735268911

Epoch: 6| Step: 6
Training loss: 1.7966575622558594
Validation loss: 2.199538230895996

Epoch: 6| Step: 7
Training loss: 1.4248970746994019
Validation loss: 2.164316991964976

Epoch: 6| Step: 8
Training loss: 1.8738933801651
Validation loss: 2.1758933464686074

Epoch: 6| Step: 9
Training loss: 1.720963478088379
Validation loss: 2.164908250172933

Epoch: 6| Step: 10
Training loss: 1.9945440292358398
Validation loss: 2.1722650130589805

Epoch: 6| Step: 11
Training loss: 2.0628981590270996
Validation loss: 2.1811408599217734

Epoch: 6| Step: 12
Training loss: 1.226029396057129
Validation loss: 2.1646913091341653

Epoch: 6| Step: 13
Training loss: 1.5917129516601562
Validation loss: 2.1850584745407104

Epoch: 243| Step: 0
Training loss: 2.0971121788024902
Validation loss: 2.158876677354177

Epoch: 6| Step: 1
Training loss: 2.0505292415618896
Validation loss: 2.146393974622091

Epoch: 6| Step: 2
Training loss: 1.522850751876831
Validation loss: 2.185317635536194

Epoch: 6| Step: 3
Training loss: 1.808861494064331
Validation loss: 2.1764264901479087

Epoch: 6| Step: 4
Training loss: 1.5841223001480103
Validation loss: 2.2088772853215537

Epoch: 6| Step: 5
Training loss: 2.5917744636535645
Validation loss: 2.1991469462712607

Epoch: 6| Step: 6
Training loss: 1.5734468698501587
Validation loss: 2.2042171955108643

Epoch: 6| Step: 7
Training loss: 1.9804517030715942
Validation loss: 2.2224740187327066

Epoch: 6| Step: 8
Training loss: 1.518841028213501
Validation loss: 2.2051807840665183

Epoch: 6| Step: 9
Training loss: 1.6161370277404785
Validation loss: 2.1817304690678916

Epoch: 6| Step: 10
Training loss: 2.240757942199707
Validation loss: 2.2000369230906167

Epoch: 6| Step: 11
Training loss: 1.7939934730529785
Validation loss: 2.2177916765213013

Epoch: 6| Step: 12
Training loss: 1.7345051765441895
Validation loss: 2.182963192462921

Epoch: 6| Step: 13
Training loss: 1.4208965301513672
Validation loss: 2.2053059140841165

Epoch: 244| Step: 0
Training loss: 1.1026561260223389
Validation loss: 2.1644946138064065

Epoch: 6| Step: 1
Training loss: 1.4779760837554932
Validation loss: 2.1736037333806357

Epoch: 6| Step: 2
Training loss: 2.48695707321167
Validation loss: 2.174038589000702

Epoch: 6| Step: 3
Training loss: 2.012350559234619
Validation loss: 2.1735470294952393

Epoch: 6| Step: 4
Training loss: 1.3278226852416992
Validation loss: 2.1648613015810647

Epoch: 6| Step: 5
Training loss: 1.7171202898025513
Validation loss: 2.1708905498186746

Epoch: 6| Step: 6
Training loss: 1.8369847536087036
Validation loss: 2.183341066042582

Epoch: 6| Step: 7
Training loss: 2.3042995929718018
Validation loss: 2.1749368707338967

Epoch: 6| Step: 8
Training loss: 1.6625007390975952
Validation loss: 2.1754852732022605

Epoch: 6| Step: 9
Training loss: 1.6875362396240234
Validation loss: 2.1854251424471536

Epoch: 6| Step: 10
Training loss: 1.5885155200958252
Validation loss: 2.188213268915812

Epoch: 6| Step: 11
Training loss: 1.9572550058364868
Validation loss: 2.2125521103541055

Epoch: 6| Step: 12
Training loss: 1.7295029163360596
Validation loss: 2.2092164357503257

Epoch: 6| Step: 13
Training loss: 2.155210494995117
Validation loss: 2.199458122253418

Epoch: 245| Step: 0
Training loss: 1.828871488571167
Validation loss: 2.163659950097402

Epoch: 6| Step: 1
Training loss: 1.154995083808899
Validation loss: 2.176160295804342

Epoch: 6| Step: 2
Training loss: 2.055833339691162
Validation loss: 2.176739056905111

Epoch: 6| Step: 3
Training loss: 1.5430676937103271
Validation loss: 2.1782915194829306

Epoch: 6| Step: 4
Training loss: 1.8699281215667725
Validation loss: 2.1919479171435037

Epoch: 6| Step: 5
Training loss: 1.7765402793884277
Validation loss: 2.1906277736028037

Epoch: 6| Step: 6
Training loss: 1.5413835048675537
Validation loss: 2.195610245068868

Epoch: 6| Step: 7
Training loss: 1.423376202583313
Validation loss: 2.195208966732025

Epoch: 6| Step: 8
Training loss: 1.638488531112671
Validation loss: 2.2166654467582703

Epoch: 6| Step: 9
Training loss: 2.5148448944091797
Validation loss: 2.198975702126821

Epoch: 6| Step: 10
Training loss: 1.892667531967163
Validation loss: 2.198663592338562

Epoch: 6| Step: 11
Training loss: 1.8701187372207642
Validation loss: 2.20160973072052

Epoch: 6| Step: 12
Training loss: 2.0189123153686523
Validation loss: 2.200248916943868

Epoch: 6| Step: 13
Training loss: 2.162536144256592
Validation loss: 2.212923844655355

Epoch: 246| Step: 0
Training loss: 1.5187290906906128
Validation loss: 2.21440060933431

Epoch: 6| Step: 1
Training loss: 1.5623698234558105
Validation loss: 2.2067025899887085

Epoch: 6| Step: 2
Training loss: 1.6904921531677246
Validation loss: 2.2141817808151245

Epoch: 6| Step: 3
Training loss: 1.5489710569381714
Validation loss: 2.2404162883758545

Epoch: 6| Step: 4
Training loss: 2.7860662937164307
Validation loss: 2.2249860366185508

Epoch: 6| Step: 5
Training loss: 1.9830818176269531
Validation loss: 2.2182565728823342

Epoch: 6| Step: 6
Training loss: 1.7569599151611328
Validation loss: 2.212876836458842

Epoch: 6| Step: 7
Training loss: 1.5209434032440186
Validation loss: 2.2089850306510925

Epoch: 6| Step: 8
Training loss: 1.381866216659546
Validation loss: 2.199877142906189

Epoch: 6| Step: 9
Training loss: 2.075331211090088
Validation loss: 2.2289079229036965

Epoch: 6| Step: 10
Training loss: 1.7369391918182373
Validation loss: 2.216866691907247

Epoch: 6| Step: 11
Training loss: 1.6674973964691162
Validation loss: 2.201344927151998

Epoch: 6| Step: 12
Training loss: 1.8990641832351685
Validation loss: 2.1955160101254783

Epoch: 6| Step: 13
Training loss: 1.5331001281738281
Validation loss: 2.239385505517324

Epoch: 247| Step: 0
Training loss: 1.9796640872955322
Validation loss: 2.2171026269594827

Epoch: 6| Step: 1
Training loss: 2.2984752655029297
Validation loss: 2.228554884592692

Epoch: 6| Step: 2
Training loss: 2.1369402408599854
Validation loss: 2.2108195622762046

Epoch: 6| Step: 3
Training loss: 1.4407715797424316
Validation loss: 2.1992467443148294

Epoch: 6| Step: 4
Training loss: 2.383652687072754
Validation loss: 2.208601693312327

Epoch: 6| Step: 5
Training loss: 1.5844569206237793
Validation loss: 2.1884814898173013

Epoch: 6| Step: 6
Training loss: 2.079860210418701
Validation loss: 2.2043996254603067

Epoch: 6| Step: 7
Training loss: 1.7750056982040405
Validation loss: 2.205193122227987

Epoch: 6| Step: 8
Training loss: 1.0425729751586914
Validation loss: 2.1991011699040732

Epoch: 6| Step: 9
Training loss: 2.430687189102173
Validation loss: 2.2125175396601358

Epoch: 6| Step: 10
Training loss: 1.286527156829834
Validation loss: 2.2158191005388894

Epoch: 6| Step: 11
Training loss: 1.328531265258789
Validation loss: 2.189394990603129

Epoch: 6| Step: 12
Training loss: 2.1281909942626953
Validation loss: 2.223164518674215

Epoch: 6| Step: 13
Training loss: 1.1457345485687256
Validation loss: 2.201741794745127

Epoch: 248| Step: 0
Training loss: 1.5651044845581055
Validation loss: 2.1902297536532083

Epoch: 6| Step: 1
Training loss: 1.7248497009277344
Validation loss: 2.1924921671549478

Epoch: 6| Step: 2
Training loss: 1.2789063453674316
Validation loss: 2.189921220143636

Epoch: 6| Step: 3
Training loss: 1.7572219371795654
Validation loss: 2.1868083675702414

Epoch: 6| Step: 4
Training loss: 1.9050178527832031
Validation loss: 2.1959877014160156

Epoch: 6| Step: 5
Training loss: 2.321988105773926
Validation loss: 2.18248051404953

Epoch: 6| Step: 6
Training loss: 1.2899807691574097
Validation loss: 2.2030765811602273

Epoch: 6| Step: 7
Training loss: 1.9533109664916992
Validation loss: 2.19683047135671

Epoch: 6| Step: 8
Training loss: 1.191577434539795
Validation loss: 2.200324018796285

Epoch: 6| Step: 9
Training loss: 1.5175623893737793
Validation loss: 2.2071937719980874

Epoch: 6| Step: 10
Training loss: 2.7899856567382812
Validation loss: 2.212718188762665

Epoch: 6| Step: 11
Training loss: 1.7192035913467407
Validation loss: 2.1999741593996682

Epoch: 6| Step: 12
Training loss: 1.7987215518951416
Validation loss: 2.2002702355384827

Epoch: 6| Step: 13
Training loss: 1.7935861349105835
Validation loss: 2.1970050732294717

Epoch: 249| Step: 0
Training loss: 1.6962672472000122
Validation loss: 2.1849000056584678

Epoch: 6| Step: 1
Training loss: 1.6088035106658936
Validation loss: 2.1827449202537537

Epoch: 6| Step: 2
Training loss: 1.767122745513916
Validation loss: 2.200056811173757

Epoch: 6| Step: 3
Training loss: 1.694314956665039
Validation loss: 2.2088379661242166

Epoch: 6| Step: 4
Training loss: 2.2216291427612305
Validation loss: 2.192537486553192

Epoch: 6| Step: 5
Training loss: 1.7532132863998413
Validation loss: 2.2294692595799765

Epoch: 6| Step: 6
Training loss: 1.6559025049209595
Validation loss: 2.2196525732676187

Epoch: 6| Step: 7
Training loss: 1.955512523651123
Validation loss: 2.2372713883717856

Epoch: 6| Step: 8
Training loss: 1.8294750452041626
Validation loss: 2.2272733648618064

Epoch: 6| Step: 9
Training loss: 1.5403337478637695
Validation loss: 2.244571010271708

Epoch: 6| Step: 10
Training loss: 1.613979458808899
Validation loss: 2.2478705644607544

Epoch: 6| Step: 11
Training loss: 1.6819614171981812
Validation loss: 2.2385457356770835

Epoch: 6| Step: 12
Training loss: 1.7365585565567017
Validation loss: 2.216310143470764

Epoch: 6| Step: 13
Training loss: 2.3935599327087402
Validation loss: 2.212844133377075

Epoch: 250| Step: 0
Training loss: 1.4016799926757812
Validation loss: 2.205467243989309

Epoch: 6| Step: 1
Training loss: 1.540358066558838
Validation loss: 2.21599280834198

Epoch: 6| Step: 2
Training loss: 2.084115982055664
Validation loss: 2.233769496281942

Epoch: 6| Step: 3
Training loss: 2.4004578590393066
Validation loss: 2.2246179978052774

Epoch: 6| Step: 4
Training loss: 2.417009115219116
Validation loss: 2.2213627099990845

Epoch: 6| Step: 5
Training loss: 1.8703585863113403
Validation loss: 2.235635280609131

Epoch: 6| Step: 6
Training loss: 2.374678611755371
Validation loss: 2.209223906199137

Epoch: 6| Step: 7
Training loss: 1.4161701202392578
Validation loss: 2.2167456348737082

Epoch: 6| Step: 8
Training loss: 1.012312889099121
Validation loss: 2.1994208693504333

Epoch: 6| Step: 9
Training loss: 1.9077250957489014
Validation loss: 2.2065617044766745

Epoch: 6| Step: 10
Training loss: 1.6255407333374023
Validation loss: 2.2414541244506836

Epoch: 6| Step: 11
Training loss: 1.5749471187591553
Validation loss: 2.2490727504094443

Epoch: 6| Step: 12
Training loss: 1.487720251083374
Validation loss: 2.2140624721844993

Epoch: 6| Step: 13
Training loss: 1.775138020515442
Validation loss: 2.2263839840888977

Testing loss: 1.8767415979783313

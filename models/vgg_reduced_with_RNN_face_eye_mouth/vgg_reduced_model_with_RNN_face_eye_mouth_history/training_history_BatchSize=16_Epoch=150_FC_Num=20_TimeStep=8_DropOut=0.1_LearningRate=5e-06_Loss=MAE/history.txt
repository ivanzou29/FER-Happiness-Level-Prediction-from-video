Epoch: 1| Step: 0
Training loss: 5.24489688873291
Validation loss: 5.3188239733378095

Epoch: 6| Step: 1
Training loss: 4.756094455718994
Validation loss: 5.317521731058757

Epoch: 6| Step: 2
Training loss: 4.998345375061035
Validation loss: 5.316194613774617

Epoch: 6| Step: 3
Training loss: 5.240941047668457
Validation loss: 5.314717372258504

Epoch: 6| Step: 4
Training loss: 5.795105934143066
Validation loss: 5.3132641315460205

Epoch: 6| Step: 5
Training loss: 5.025295257568359
Validation loss: 5.311727523803711

Epoch: 6| Step: 6
Training loss: 5.691587448120117
Validation loss: 5.310095548629761

Epoch: 6| Step: 7
Training loss: 6.359739780426025
Validation loss: 5.308489084243774

Epoch: 6| Step: 8
Training loss: 6.532509803771973
Validation loss: 5.3068539301554365

Epoch: 6| Step: 9
Training loss: 5.613823890686035
Validation loss: 5.305080016454061

Epoch: 6| Step: 10
Training loss: 4.911099910736084
Validation loss: 5.303364117940267

Epoch: 6| Step: 11
Training loss: 5.477555274963379
Validation loss: 5.301469723383586

Epoch: 6| Step: 12
Training loss: 4.675744533538818
Validation loss: 5.299475749333699

Epoch: 6| Step: 13
Training loss: 4.967674255371094
Validation loss: 5.297667662302653

Epoch: 2| Step: 0
Training loss: 4.584799766540527
Validation loss: 5.295541127522786

Epoch: 6| Step: 1
Training loss: 5.28117561340332
Validation loss: 5.293349027633667

Epoch: 6| Step: 2
Training loss: 5.7531914710998535
Validation loss: 5.291165987650554

Epoch: 6| Step: 3
Training loss: 5.186283111572266
Validation loss: 5.288731416066487

Epoch: 6| Step: 4
Training loss: 5.362330436706543
Validation loss: 5.286226987838745

Epoch: 6| Step: 5
Training loss: 5.4313273429870605
Validation loss: 5.283703804016113

Epoch: 6| Step: 6
Training loss: 5.321941375732422
Validation loss: 5.281051397323608

Epoch: 6| Step: 7
Training loss: 4.612929344177246
Validation loss: 5.278191645940145

Epoch: 6| Step: 8
Training loss: 5.237448692321777
Validation loss: 5.275104999542236

Epoch: 6| Step: 9
Training loss: 5.943656921386719
Validation loss: 5.272021929423015

Epoch: 6| Step: 10
Training loss: 5.512596607208252
Validation loss: 5.268786350886027

Epoch: 6| Step: 11
Training loss: 6.421813488006592
Validation loss: 5.265209595362346

Epoch: 6| Step: 12
Training loss: 6.1775994300842285
Validation loss: 5.26153024037679

Epoch: 6| Step: 13
Training loss: 4.041419982910156
Validation loss: 5.2577113310496015

Epoch: 3| Step: 0
Training loss: 5.374922275543213
Validation loss: 5.253541946411133

Epoch: 6| Step: 1
Training loss: 4.766347885131836
Validation loss: 5.2492995262146

Epoch: 6| Step: 2
Training loss: 4.805759429931641
Validation loss: 5.244712909062703

Epoch: 6| Step: 3
Training loss: 5.149722576141357
Validation loss: 5.24007002512614

Epoch: 6| Step: 4
Training loss: 5.896965980529785
Validation loss: 5.234975655873616

Epoch: 6| Step: 5
Training loss: 5.860342979431152
Validation loss: 5.229722817738851

Epoch: 6| Step: 6
Training loss: 4.942234516143799
Validation loss: 5.224212169647217

Epoch: 6| Step: 7
Training loss: 5.483736991882324
Validation loss: 5.218366543451945

Epoch: 6| Step: 8
Training loss: 5.997720718383789
Validation loss: 5.212318817774455

Epoch: 6| Step: 9
Training loss: 4.123869895935059
Validation loss: 5.2060418128967285

Epoch: 6| Step: 10
Training loss: 5.738564968109131
Validation loss: 5.199613094329834

Epoch: 6| Step: 11
Training loss: 5.6262359619140625
Validation loss: 5.192454814910889

Epoch: 6| Step: 12
Training loss: 5.590836048126221
Validation loss: 5.185451825459798

Epoch: 6| Step: 13
Training loss: 4.7062273025512695
Validation loss: 5.17746631304423

Epoch: 4| Step: 0
Training loss: 5.409726142883301
Validation loss: 5.169976313908895

Epoch: 6| Step: 1
Training loss: 6.091562747955322
Validation loss: 5.161663055419922

Epoch: 6| Step: 2
Training loss: 5.998098373413086
Validation loss: 5.153143088022868

Epoch: 6| Step: 3
Training loss: 5.147211074829102
Validation loss: 5.14393949508667

Epoch: 6| Step: 4
Training loss: 4.996806621551514
Validation loss: 5.134821732838948

Epoch: 6| Step: 5
Training loss: 5.278224468231201
Validation loss: 5.1255167325337725

Epoch: 6| Step: 6
Training loss: 4.257981300354004
Validation loss: 5.115328470865886

Epoch: 6| Step: 7
Training loss: 5.28080940246582
Validation loss: 5.1048743724823

Epoch: 6| Step: 8
Training loss: 5.324060440063477
Validation loss: 5.094660679499309

Epoch: 6| Step: 9
Training loss: 4.426556587219238
Validation loss: 5.083018938700358

Epoch: 6| Step: 10
Training loss: 5.997311592102051
Validation loss: 5.07200010617574

Epoch: 6| Step: 11
Training loss: 5.941740036010742
Validation loss: 5.0602279504140215

Epoch: 6| Step: 12
Training loss: 3.9472830295562744
Validation loss: 5.048597574234009

Epoch: 6| Step: 13
Training loss: 4.454291820526123
Validation loss: 5.036693175633748

Epoch: 5| Step: 0
Training loss: 5.358337879180908
Validation loss: 5.0247964064280195

Epoch: 6| Step: 1
Training loss: 5.008090019226074
Validation loss: 5.012712796529134

Epoch: 6| Step: 2
Training loss: 4.961153030395508
Validation loss: 5.000494559605916

Epoch: 6| Step: 3
Training loss: 5.475697040557861
Validation loss: 4.988245407740275

Epoch: 6| Step: 4
Training loss: 4.597214698791504
Validation loss: 4.9765206178029375

Epoch: 6| Step: 5
Training loss: 4.304558753967285
Validation loss: 4.964011112848918

Epoch: 6| Step: 6
Training loss: 4.2641191482543945
Validation loss: 4.952344338099162

Epoch: 6| Step: 7
Training loss: 6.154146671295166
Validation loss: 4.9408369064331055

Epoch: 6| Step: 8
Training loss: 4.639142990112305
Validation loss: 4.929396231969197

Epoch: 6| Step: 9
Training loss: 4.626492500305176
Validation loss: 4.917142629623413

Epoch: 6| Step: 10
Training loss: 4.42443323135376
Validation loss: 4.905231475830078

Epoch: 6| Step: 11
Training loss: 6.082767009735107
Validation loss: 4.894413948059082

Epoch: 6| Step: 12
Training loss: 5.037483215332031
Validation loss: 4.8833088874816895

Epoch: 6| Step: 13
Training loss: 5.478481292724609
Validation loss: 4.872515241305034

Epoch: 6| Step: 0
Training loss: 4.097561836242676
Validation loss: 4.862372676531474

Epoch: 6| Step: 1
Training loss: 4.447660446166992
Validation loss: 4.852115790049235

Epoch: 6| Step: 2
Training loss: 3.8209216594696045
Validation loss: 4.841799736022949

Epoch: 6| Step: 3
Training loss: 5.335541248321533
Validation loss: 4.832666635513306

Epoch: 6| Step: 4
Training loss: 4.866246223449707
Validation loss: 4.823591868082683

Epoch: 6| Step: 5
Training loss: 5.261744499206543
Validation loss: 4.81423020362854

Epoch: 6| Step: 6
Training loss: 4.708014488220215
Validation loss: 4.805904547373454

Epoch: 6| Step: 7
Training loss: 4.722251892089844
Validation loss: 4.7971421877543134

Epoch: 6| Step: 8
Training loss: 4.916360855102539
Validation loss: 4.788704951604207

Epoch: 6| Step: 9
Training loss: 4.983170509338379
Validation loss: 4.780682802200317

Epoch: 6| Step: 10
Training loss: 5.94413423538208
Validation loss: 4.772947072982788

Epoch: 6| Step: 11
Training loss: 5.348531723022461
Validation loss: 4.765323638916016

Epoch: 6| Step: 12
Training loss: 4.766203880310059
Validation loss: 4.7575944264729815

Epoch: 6| Step: 13
Training loss: 5.262463569641113
Validation loss: 4.75027855237325

Epoch: 7| Step: 0
Training loss: 5.229434013366699
Validation loss: 4.742841919263204

Epoch: 6| Step: 1
Training loss: 5.721207618713379
Validation loss: 4.735661347707112

Epoch: 6| Step: 2
Training loss: 4.773683071136475
Validation loss: 4.7289084593455

Epoch: 6| Step: 3
Training loss: 4.107675552368164
Validation loss: 4.721832354863484

Epoch: 6| Step: 4
Training loss: 3.4929890632629395
Validation loss: 4.714986324310303

Epoch: 6| Step: 5
Training loss: 4.957828044891357
Validation loss: 4.707888921101888

Epoch: 6| Step: 6
Training loss: 5.40346097946167
Validation loss: 4.700900713602702

Epoch: 6| Step: 7
Training loss: 4.720075607299805
Validation loss: 4.694435675938924

Epoch: 6| Step: 8
Training loss: 4.816891193389893
Validation loss: 4.687608083089192

Epoch: 6| Step: 9
Training loss: 6.235208034515381
Validation loss: 4.681143045425415

Epoch: 6| Step: 10
Training loss: 3.7432658672332764
Validation loss: 4.674524664878845

Epoch: 6| Step: 11
Training loss: 4.415587902069092
Validation loss: 4.668479760487874

Epoch: 6| Step: 12
Training loss: 4.895142555236816
Validation loss: 4.662295341491699

Epoch: 6| Step: 13
Training loss: 4.611885070800781
Validation loss: 4.656323671340942

Epoch: 8| Step: 0
Training loss: 5.032660961151123
Validation loss: 4.650344173113505

Epoch: 6| Step: 1
Training loss: 4.326867580413818
Validation loss: 4.644459168116252

Epoch: 6| Step: 2
Training loss: 3.872281551361084
Validation loss: 4.638652642567952

Epoch: 6| Step: 3
Training loss: 5.4873762130737305
Validation loss: 4.632986346880595

Epoch: 6| Step: 4
Training loss: 5.614898681640625
Validation loss: 4.627161820729573

Epoch: 6| Step: 5
Training loss: 4.641176223754883
Validation loss: 4.62139077981313

Epoch: 6| Step: 6
Training loss: 5.02862548828125
Validation loss: 4.615753173828125

Epoch: 6| Step: 7
Training loss: 4.602980613708496
Validation loss: 4.610140244166057

Epoch: 6| Step: 8
Training loss: 4.555347919464111
Validation loss: 4.604926506678264

Epoch: 6| Step: 9
Training loss: 4.490844249725342
Validation loss: 4.598744710286458

Epoch: 6| Step: 10
Training loss: 4.731470584869385
Validation loss: 4.593482732772827

Epoch: 6| Step: 11
Training loss: 4.4865217208862305
Validation loss: 4.587544282277425

Epoch: 6| Step: 12
Training loss: 4.336044788360596
Validation loss: 4.58230455716451

Epoch: 6| Step: 13
Training loss: 4.765921592712402
Validation loss: 4.576668898264567

Epoch: 9| Step: 0
Training loss: 4.07804012298584
Validation loss: 4.570637941360474

Epoch: 6| Step: 1
Training loss: 4.042719841003418
Validation loss: 4.565431276957194

Epoch: 6| Step: 2
Training loss: 4.966326713562012
Validation loss: 4.560466965039571

Epoch: 6| Step: 3
Training loss: 4.960850715637207
Validation loss: 4.554315567016602

Epoch: 6| Step: 4
Training loss: 4.365514755249023
Validation loss: 4.548929373423259

Epoch: 6| Step: 5
Training loss: 3.9454100131988525
Validation loss: 4.5435434977213545

Epoch: 6| Step: 6
Training loss: 4.244273662567139
Validation loss: 4.538006067276001

Epoch: 6| Step: 7
Training loss: 5.064060211181641
Validation loss: 4.532998164494832

Epoch: 6| Step: 8
Training loss: 4.465414524078369
Validation loss: 4.527788321177165

Epoch: 6| Step: 9
Training loss: 5.535155773162842
Validation loss: 4.5220522085825605

Epoch: 6| Step: 10
Training loss: 4.52620792388916
Validation loss: 4.516674240430196

Epoch: 6| Step: 11
Training loss: 5.507375717163086
Validation loss: 4.5120194753011065

Epoch: 6| Step: 12
Training loss: 4.581836700439453
Validation loss: 4.506790955861409

Epoch: 6| Step: 13
Training loss: 4.6629133224487305
Validation loss: 4.5013424555460615

Epoch: 10| Step: 0
Training loss: 4.446294784545898
Validation loss: 4.49584166208903

Epoch: 6| Step: 1
Training loss: 3.797283887863159
Validation loss: 4.490691343943278

Epoch: 6| Step: 2
Training loss: 4.526614189147949
Validation loss: 4.485380093256633

Epoch: 6| Step: 3
Training loss: 4.655692100524902
Validation loss: 4.479771256446838

Epoch: 6| Step: 4
Training loss: 5.0320940017700195
Validation loss: 4.475020249684651

Epoch: 6| Step: 5
Training loss: 3.3133866786956787
Validation loss: 4.468958616256714

Epoch: 6| Step: 6
Training loss: 5.04831600189209
Validation loss: 4.4635186195373535

Epoch: 6| Step: 7
Training loss: 4.761075019836426
Validation loss: 4.457957903544108

Epoch: 6| Step: 8
Training loss: 5.477566719055176
Validation loss: 4.452772219975789

Epoch: 6| Step: 9
Training loss: 4.4498186111450195
Validation loss: 4.447230180104573

Epoch: 6| Step: 10
Training loss: 4.072508335113525
Validation loss: 4.443114201227824

Epoch: 6| Step: 11
Training loss: 4.416787147521973
Validation loss: 4.43721866607666

Epoch: 6| Step: 12
Training loss: 4.240053653717041
Validation loss: 4.431258360544841

Epoch: 6| Step: 13
Training loss: 5.774518966674805
Validation loss: 4.425936222076416

Epoch: 11| Step: 0
Training loss: 5.4841790199279785
Validation loss: 4.419994990030925

Epoch: 6| Step: 1
Training loss: 6.112209320068359
Validation loss: 4.41482416788737

Epoch: 6| Step: 2
Training loss: 4.586428642272949
Validation loss: 4.408888816833496

Epoch: 6| Step: 3
Training loss: 4.2033233642578125
Validation loss: 4.403373916943868

Epoch: 6| Step: 4
Training loss: 4.835179328918457
Validation loss: 4.398253679275513

Epoch: 6| Step: 5
Training loss: 4.934903144836426
Validation loss: 4.392246286074321

Epoch: 6| Step: 6
Training loss: 2.8866405487060547
Validation loss: 4.387298742930095

Epoch: 6| Step: 7
Training loss: 3.801936149597168
Validation loss: 4.381864865620931

Epoch: 6| Step: 8
Training loss: 3.677332878112793
Validation loss: 4.376285115877788

Epoch: 6| Step: 9
Training loss: 4.344058036804199
Validation loss: 4.370435953140259

Epoch: 6| Step: 10
Training loss: 4.07313346862793
Validation loss: 4.36569881439209

Epoch: 6| Step: 11
Training loss: 5.050236701965332
Validation loss: 4.359752535820007

Epoch: 6| Step: 12
Training loss: 3.810454845428467
Validation loss: 4.353960275650024

Epoch: 6| Step: 13
Training loss: 5.239354133605957
Validation loss: 4.348528107007344

Epoch: 12| Step: 0
Training loss: 4.412711143493652
Validation loss: 4.34357476234436

Epoch: 6| Step: 1
Training loss: 5.322587013244629
Validation loss: 4.338196317354838

Epoch: 6| Step: 2
Training loss: 3.79757022857666
Validation loss: 4.333113034566243

Epoch: 6| Step: 3
Training loss: 4.4535136222839355
Validation loss: 4.326886296272278

Epoch: 6| Step: 4
Training loss: 3.890599012374878
Validation loss: 4.320910930633545

Epoch: 6| Step: 5
Training loss: 5.239372253417969
Validation loss: 4.3160858154296875

Epoch: 6| Step: 6
Training loss: 4.546076774597168
Validation loss: 4.310556093851726

Epoch: 6| Step: 7
Training loss: 4.488033294677734
Validation loss: 4.304927428563436

Epoch: 6| Step: 8
Training loss: 4.127729892730713
Validation loss: 4.298811078071594

Epoch: 6| Step: 9
Training loss: 4.869636535644531
Validation loss: 4.292932510375977

Epoch: 6| Step: 10
Training loss: 2.926541805267334
Validation loss: 4.2872053782145185

Epoch: 6| Step: 11
Training loss: 4.560436248779297
Validation loss: 4.281575361887614

Epoch: 6| Step: 12
Training loss: 4.22865104675293
Validation loss: 4.276448408762614

Epoch: 6| Step: 13
Training loss: 5.199702739715576
Validation loss: 4.2706687450408936

Epoch: 13| Step: 0
Training loss: 3.9084272384643555
Validation loss: 4.2646015882492065

Epoch: 6| Step: 1
Training loss: 3.6263182163238525
Validation loss: 4.259257157643636

Epoch: 6| Step: 2
Training loss: 4.376034259796143
Validation loss: 4.2536940574646

Epoch: 6| Step: 3
Training loss: 4.15528678894043
Validation loss: 4.247319459915161

Epoch: 6| Step: 4
Training loss: 4.496020793914795
Validation loss: 4.242267966270447

Epoch: 6| Step: 5
Training loss: 4.302431106567383
Validation loss: 4.2373660405476885

Epoch: 6| Step: 6
Training loss: 4.001025199890137
Validation loss: 4.232832670211792

Epoch: 6| Step: 7
Training loss: 4.368494033813477
Validation loss: 4.226613442103068

Epoch: 6| Step: 8
Training loss: 5.126560688018799
Validation loss: 4.219044089317322

Epoch: 6| Step: 9
Training loss: 4.8054728507995605
Validation loss: 4.212775627772014

Epoch: 6| Step: 10
Training loss: 5.054649353027344
Validation loss: 4.2088208595911665

Epoch: 6| Step: 11
Training loss: 4.244894981384277
Validation loss: 4.203491290410359

Epoch: 6| Step: 12
Training loss: 4.110483169555664
Validation loss: 4.195850610733032

Epoch: 6| Step: 13
Training loss: 4.44189453125
Validation loss: 4.189355333646138

Epoch: 14| Step: 0
Training loss: 5.00691032409668
Validation loss: 4.18442698319753

Epoch: 6| Step: 1
Training loss: 5.247257232666016
Validation loss: 4.179407596588135

Epoch: 6| Step: 2
Training loss: 4.582813262939453
Validation loss: 4.172539790471395

Epoch: 6| Step: 3
Training loss: 3.8856582641601562
Validation loss: 4.165338555971782

Epoch: 6| Step: 4
Training loss: 5.183506965637207
Validation loss: 4.1594507694244385

Epoch: 6| Step: 5
Training loss: 4.763250350952148
Validation loss: 4.1541978518168134

Epoch: 6| Step: 6
Training loss: 3.3069214820861816
Validation loss: 4.148480534553528

Epoch: 6| Step: 7
Training loss: 4.621057510375977
Validation loss: 4.142297347386678

Epoch: 6| Step: 8
Training loss: 4.11867618560791
Validation loss: 4.136689106623332

Epoch: 6| Step: 9
Training loss: 3.2261805534362793
Validation loss: 4.129735549290975

Epoch: 6| Step: 10
Training loss: 4.103721618652344
Validation loss: 4.1246864795684814

Epoch: 6| Step: 11
Training loss: 3.9596216678619385
Validation loss: 4.120682756106059

Epoch: 6| Step: 12
Training loss: 4.679283142089844
Validation loss: 4.114642898241679

Epoch: 6| Step: 13
Training loss: 3.2888576984405518
Validation loss: 4.108731110890706

Epoch: 15| Step: 0
Training loss: 4.247553825378418
Validation loss: 4.101638674736023

Epoch: 6| Step: 1
Training loss: 5.247035980224609
Validation loss: 4.095728278160095

Epoch: 6| Step: 2
Training loss: 4.508730888366699
Validation loss: 4.0909576416015625

Epoch: 6| Step: 3
Training loss: 2.609358310699463
Validation loss: 4.085070371627808

Epoch: 6| Step: 4
Training loss: 3.933819055557251
Validation loss: 4.079822540283203

Epoch: 6| Step: 5
Training loss: 5.347169876098633
Validation loss: 4.073509216308594

Epoch: 6| Step: 6
Training loss: 3.27730655670166
Validation loss: 4.067026098569234

Epoch: 6| Step: 7
Training loss: 4.58212947845459
Validation loss: 4.063115318616231

Epoch: 6| Step: 8
Training loss: 4.163984298706055
Validation loss: 4.058488567670186

Epoch: 6| Step: 9
Training loss: 4.424142837524414
Validation loss: 4.053662657737732

Epoch: 6| Step: 10
Training loss: 4.643346786499023
Validation loss: 4.047489682833354

Epoch: 6| Step: 11
Training loss: 3.3293190002441406
Validation loss: 4.040893634160359

Epoch: 6| Step: 12
Training loss: 4.539211273193359
Validation loss: 4.035690665245056

Epoch: 6| Step: 13
Training loss: 4.050655364990234
Validation loss: 4.029471596082051

Epoch: 16| Step: 0
Training loss: 4.58318567276001
Validation loss: 4.024813890457153

Epoch: 6| Step: 1
Training loss: 5.494792461395264
Validation loss: 4.018309712409973

Epoch: 6| Step: 2
Training loss: 4.487581253051758
Validation loss: 4.013296445210774

Epoch: 6| Step: 3
Training loss: 4.394527435302734
Validation loss: 4.006704330444336

Epoch: 6| Step: 4
Training loss: 4.405013084411621
Validation loss: 4.001184105873108

Epoch: 6| Step: 5
Training loss: 3.7034599781036377
Validation loss: 3.9961323738098145

Epoch: 6| Step: 6
Training loss: 3.9466817378997803
Validation loss: 3.9917662541071572

Epoch: 6| Step: 7
Training loss: 3.2630257606506348
Validation loss: 3.9857486486434937

Epoch: 6| Step: 8
Training loss: 4.7275004386901855
Validation loss: 3.9798027674357095

Epoch: 6| Step: 9
Training loss: 4.00441837310791
Validation loss: 3.973713437716166

Epoch: 6| Step: 10
Training loss: 3.953402042388916
Validation loss: 3.96829084555308

Epoch: 6| Step: 11
Training loss: 3.7166762351989746
Validation loss: 3.962586283683777

Epoch: 6| Step: 12
Training loss: 3.4422593116760254
Validation loss: 3.9573609431584678

Epoch: 6| Step: 13
Training loss: 3.746094226837158
Validation loss: 3.951757272084554

Epoch: 17| Step: 0
Training loss: 3.583630084991455
Validation loss: 3.9469851652781167

Epoch: 6| Step: 1
Training loss: 4.086494445800781
Validation loss: 3.9407807191212973

Epoch: 6| Step: 2
Training loss: 4.032477855682373
Validation loss: 3.936020811398824

Epoch: 6| Step: 3
Training loss: 3.4398746490478516
Validation loss: 3.930907368659973

Epoch: 6| Step: 4
Training loss: 3.5022265911102295
Validation loss: 3.924954096476237

Epoch: 6| Step: 5
Training loss: 4.453863143920898
Validation loss: 3.919800639152527

Epoch: 6| Step: 6
Training loss: 4.240262031555176
Validation loss: 3.914648731549581

Epoch: 6| Step: 7
Training loss: 4.773153305053711
Validation loss: 3.9095224142074585

Epoch: 6| Step: 8
Training loss: 3.7674882411956787
Validation loss: 3.9049528439839682

Epoch: 6| Step: 9
Training loss: 4.892953872680664
Validation loss: 3.900310516357422

Epoch: 6| Step: 10
Training loss: 3.838200092315674
Validation loss: 3.8951804637908936

Epoch: 6| Step: 11
Training loss: 3.8651223182678223
Validation loss: 3.8892489274342856

Epoch: 6| Step: 12
Training loss: 4.538413047790527
Validation loss: 3.883718729019165

Epoch: 6| Step: 13
Training loss: 3.809030294418335
Validation loss: 3.87894868850708

Epoch: 18| Step: 0
Training loss: 4.0596418380737305
Validation loss: 3.873632629712423

Epoch: 6| Step: 1
Training loss: 4.450164794921875
Validation loss: 3.8685343662897744

Epoch: 6| Step: 2
Training loss: 3.75608491897583
Validation loss: 3.862671693166097

Epoch: 6| Step: 3
Training loss: 3.584933280944824
Validation loss: 3.858782728513082

Epoch: 6| Step: 4
Training loss: 4.143030166625977
Validation loss: 3.8545349836349487

Epoch: 6| Step: 5
Training loss: 4.4236297607421875
Validation loss: 3.849956472714742

Epoch: 6| Step: 6
Training loss: 3.544687032699585
Validation loss: 3.844473679860433

Epoch: 6| Step: 7
Training loss: 4.42121696472168
Validation loss: 3.837414542833964

Epoch: 6| Step: 8
Training loss: 3.381369113922119
Validation loss: 3.8314472436904907

Epoch: 6| Step: 9
Training loss: 4.465015888214111
Validation loss: 3.828605135281881

Epoch: 6| Step: 10
Training loss: 4.101011753082275
Validation loss: 3.8244667847951255

Epoch: 6| Step: 11
Training loss: 4.806884765625
Validation loss: 3.817155639330546

Epoch: 6| Step: 12
Training loss: 3.9209227561950684
Validation loss: 3.810941537221273

Epoch: 6| Step: 13
Training loss: 2.8500139713287354
Validation loss: 3.80566414197286

Epoch: 19| Step: 0
Training loss: 3.392754554748535
Validation loss: 3.8014609813690186

Epoch: 6| Step: 1
Training loss: 3.3842220306396484
Validation loss: 3.798287789026896

Epoch: 6| Step: 2
Training loss: 4.131608486175537
Validation loss: 3.79327126344045

Epoch: 6| Step: 3
Training loss: 3.9197423458099365
Validation loss: 3.787936806678772

Epoch: 6| Step: 4
Training loss: 4.989497184753418
Validation loss: 3.7833823362986245

Epoch: 6| Step: 5
Training loss: 3.6877033710479736
Validation loss: 3.7776999870936074

Epoch: 6| Step: 6
Training loss: 3.6663107872009277
Validation loss: 3.7728418509165444

Epoch: 6| Step: 7
Training loss: 4.427948951721191
Validation loss: 3.7675056060155234

Epoch: 6| Step: 8
Training loss: 3.0118069648742676
Validation loss: 3.762678384780884

Epoch: 6| Step: 9
Training loss: 4.076411724090576
Validation loss: 3.7576365868250527

Epoch: 6| Step: 10
Training loss: 3.2811031341552734
Validation loss: 3.752488692601522

Epoch: 6| Step: 11
Training loss: 4.216709136962891
Validation loss: 3.747940977414449

Epoch: 6| Step: 12
Training loss: 4.23815393447876
Validation loss: 3.7437679767608643

Epoch: 6| Step: 13
Training loss: 4.530648708343506
Validation loss: 3.7408478260040283

Epoch: 20| Step: 0
Training loss: 4.6098313331604
Validation loss: 3.7373985846837363

Epoch: 6| Step: 1
Training loss: 3.1619479656219482
Validation loss: 3.7321434020996094

Epoch: 6| Step: 2
Training loss: 4.386200428009033
Validation loss: 3.7245099544525146

Epoch: 6| Step: 3
Training loss: 3.7849106788635254
Validation loss: 3.7180228233337402

Epoch: 6| Step: 4
Training loss: 3.3303542137145996
Validation loss: 3.712823232014974

Epoch: 6| Step: 5
Training loss: 4.671632766723633
Validation loss: 3.708781878153483

Epoch: 6| Step: 6
Training loss: 3.065638542175293
Validation loss: 3.7038485209147134

Epoch: 6| Step: 7
Training loss: 3.3812479972839355
Validation loss: 3.7000550429026284

Epoch: 6| Step: 8
Training loss: 4.272283554077148
Validation loss: 3.6960145235061646

Epoch: 6| Step: 9
Training loss: 4.224709987640381
Validation loss: 3.687870661417643

Epoch: 6| Step: 10
Training loss: 3.7156496047973633
Validation loss: 3.685863812764486

Epoch: 6| Step: 11
Training loss: 4.0417070388793945
Validation loss: 3.6834900776545205

Epoch: 6| Step: 12
Training loss: 3.9530396461486816
Validation loss: 3.672239104906718

Epoch: 6| Step: 13
Training loss: 3.4580607414245605
Validation loss: 3.668079932530721

Epoch: 21| Step: 0
Training loss: 3.5141468048095703
Validation loss: 3.6642563343048096

Epoch: 6| Step: 1
Training loss: 3.599750518798828
Validation loss: 3.659892201423645

Epoch: 6| Step: 2
Training loss: 3.2937660217285156
Validation loss: 3.654161254564921

Epoch: 6| Step: 3
Training loss: 4.539540767669678
Validation loss: 3.6493525902430215

Epoch: 6| Step: 4
Training loss: 3.2542245388031006
Validation loss: 3.6458601554234824

Epoch: 6| Step: 5
Training loss: 4.236066818237305
Validation loss: 3.6424084901809692

Epoch: 6| Step: 6
Training loss: 3.151221752166748
Validation loss: 3.6360278924306235

Epoch: 6| Step: 7
Training loss: 3.6024868488311768
Validation loss: 3.6307105223337808

Epoch: 6| Step: 8
Training loss: 4.065636157989502
Validation loss: 3.626359502474467

Epoch: 6| Step: 9
Training loss: 3.5448458194732666
Validation loss: 3.6216475566228232

Epoch: 6| Step: 10
Training loss: 3.07340407371521
Validation loss: 3.616484800974528

Epoch: 6| Step: 11
Training loss: 3.785353422164917
Validation loss: 3.611494263013204

Epoch: 6| Step: 12
Training loss: 4.210805892944336
Validation loss: 3.6079008976618447

Epoch: 6| Step: 13
Training loss: 5.254445552825928
Validation loss: 3.6023755073547363

Epoch: 22| Step: 0
Training loss: 2.88663387298584
Validation loss: 3.597946286201477

Epoch: 6| Step: 1
Training loss: 4.008881568908691
Validation loss: 3.592238942782084

Epoch: 6| Step: 2
Training loss: 3.6344094276428223
Validation loss: 3.588546395301819

Epoch: 6| Step: 3
Training loss: 3.781858205795288
Validation loss: 3.5827192465464273

Epoch: 6| Step: 4
Training loss: 3.9917523860931396
Validation loss: 3.578782796859741

Epoch: 6| Step: 5
Training loss: 4.059440612792969
Validation loss: 3.5753175814946494

Epoch: 6| Step: 6
Training loss: 4.42300271987915
Validation loss: 3.5690191189448037

Epoch: 6| Step: 7
Training loss: 4.7180399894714355
Validation loss: 3.564448118209839

Epoch: 6| Step: 8
Training loss: 3.202261209487915
Validation loss: 3.5603637297948203

Epoch: 6| Step: 9
Training loss: 3.8274989128112793
Validation loss: 3.5542015631993613

Epoch: 6| Step: 10
Training loss: 3.484753131866455
Validation loss: 3.549753427505493

Epoch: 6| Step: 11
Training loss: 2.754734516143799
Validation loss: 3.544589877128601

Epoch: 6| Step: 12
Training loss: 3.3133230209350586
Validation loss: 3.5402228037516275

Epoch: 6| Step: 13
Training loss: 4.148045539855957
Validation loss: 3.5362029472986856

Epoch: 23| Step: 0
Training loss: 4.310541152954102
Validation loss: 3.5331924756368003

Epoch: 6| Step: 1
Training loss: 3.6365866661071777
Validation loss: 3.528058409690857

Epoch: 6| Step: 2
Training loss: 5.057651519775391
Validation loss: 3.5232659578323364

Epoch: 6| Step: 3
Training loss: 4.198354721069336
Validation loss: 3.5171865622202554

Epoch: 6| Step: 4
Training loss: 3.29673433303833
Validation loss: 3.5129942099253335

Epoch: 6| Step: 5
Training loss: 3.7158255577087402
Validation loss: 3.5077993472417197

Epoch: 6| Step: 6
Training loss: 3.8606576919555664
Validation loss: 3.5040965477625527

Epoch: 6| Step: 7
Training loss: 3.1437296867370605
Validation loss: 3.498544176419576

Epoch: 6| Step: 8
Training loss: 4.115941047668457
Validation loss: 3.4929482539494834

Epoch: 6| Step: 9
Training loss: 2.5107603073120117
Validation loss: 3.4890307188034058

Epoch: 6| Step: 10
Training loss: 2.79410982131958
Validation loss: 3.484349171320597

Epoch: 6| Step: 11
Training loss: 3.483381509780884
Validation loss: 3.479022741317749

Epoch: 6| Step: 12
Training loss: 3.5977108478546143
Validation loss: 3.4748468001683555

Epoch: 6| Step: 13
Training loss: 3.6267528533935547
Validation loss: 3.4709722995758057

Epoch: 24| Step: 0
Training loss: 2.6514499187469482
Validation loss: 3.4677160580952964

Epoch: 6| Step: 1
Training loss: 3.7038044929504395
Validation loss: 3.4655681848526

Epoch: 6| Step: 2
Training loss: 3.2736873626708984
Validation loss: 3.4628722270329795

Epoch: 6| Step: 3
Training loss: 4.520009994506836
Validation loss: 3.458365559577942

Epoch: 6| Step: 4
Training loss: 3.0942416191101074
Validation loss: 3.449365496635437

Epoch: 6| Step: 5
Training loss: 4.498668670654297
Validation loss: 3.443886478741964

Epoch: 6| Step: 6
Training loss: 3.1854729652404785
Validation loss: 3.4412440061569214

Epoch: 6| Step: 7
Training loss: 4.249326705932617
Validation loss: 3.438305695851644

Epoch: 6| Step: 8
Training loss: 3.0853681564331055
Validation loss: 3.4336798191070557

Epoch: 6| Step: 9
Training loss: 4.119042873382568
Validation loss: 3.4291563034057617

Epoch: 6| Step: 10
Training loss: 2.691317558288574
Validation loss: 3.426293214162191

Epoch: 6| Step: 11
Training loss: 3.4992287158966064
Validation loss: 3.420830766359965

Epoch: 6| Step: 12
Training loss: 4.231260299682617
Validation loss: 3.415651480356852

Epoch: 6| Step: 13
Training loss: 3.6508095264434814
Validation loss: 3.412278175354004

Epoch: 25| Step: 0
Training loss: 4.079122066497803
Validation loss: 3.4074524641036987

Epoch: 6| Step: 1
Training loss: 3.6298255920410156
Validation loss: 3.401679754257202

Epoch: 6| Step: 2
Training loss: 3.2542872428894043
Validation loss: 3.398239016532898

Epoch: 6| Step: 3
Training loss: 3.9736428260803223
Validation loss: 3.392721255620321

Epoch: 6| Step: 4
Training loss: 3.427529811859131
Validation loss: 3.3867775201797485

Epoch: 6| Step: 5
Training loss: 3.560243606567383
Validation loss: 3.382602334022522

Epoch: 6| Step: 6
Training loss: 3.4077796936035156
Validation loss: 3.378377993901571

Epoch: 6| Step: 7
Training loss: 3.020674705505371
Validation loss: 3.37309992313385

Epoch: 6| Step: 8
Training loss: 3.9118611812591553
Validation loss: 3.372610608736674

Epoch: 6| Step: 9
Training loss: 3.6110730171203613
Validation loss: 3.3689659039179483

Epoch: 6| Step: 10
Training loss: 3.348021984100342
Validation loss: 3.362509767214457

Epoch: 6| Step: 11
Training loss: 3.7671072483062744
Validation loss: 3.3614519834518433

Epoch: 6| Step: 12
Training loss: 2.876887798309326
Validation loss: 3.352139155069987

Epoch: 6| Step: 13
Training loss: 3.763706684112549
Validation loss: 3.3482664028803506

Epoch: 26| Step: 0
Training loss: 4.124006748199463
Validation loss: 3.3440001010894775

Epoch: 6| Step: 1
Training loss: 3.5579051971435547
Validation loss: 3.3397567669550576

Epoch: 6| Step: 2
Training loss: 2.9459400177001953
Validation loss: 3.3338112036387124

Epoch: 6| Step: 3
Training loss: 3.6446471214294434
Validation loss: 3.328489661216736

Epoch: 6| Step: 4
Training loss: 2.842299461364746
Validation loss: 3.323310216267904

Epoch: 6| Step: 5
Training loss: 3.586045265197754
Validation loss: 3.3170493841171265

Epoch: 6| Step: 6
Training loss: 3.4590821266174316
Validation loss: 3.313239415486654

Epoch: 6| Step: 7
Training loss: 3.211740493774414
Validation loss: 3.309411803881327

Epoch: 6| Step: 8
Training loss: 4.013126373291016
Validation loss: 3.3063296476999917

Epoch: 6| Step: 9
Training loss: 2.8512561321258545
Validation loss: 3.300140659014384

Epoch: 6| Step: 10
Training loss: 3.677090644836426
Validation loss: 3.29609751701355

Epoch: 6| Step: 11
Training loss: 3.144702911376953
Validation loss: 3.2920289436976113

Epoch: 6| Step: 12
Training loss: 4.903769493103027
Validation loss: 3.2890493869781494

Epoch: 6| Step: 13
Training loss: 2.867509365081787
Validation loss: 3.28382400671641

Epoch: 27| Step: 0
Training loss: 2.864497184753418
Validation loss: 3.2790298064549765

Epoch: 6| Step: 1
Training loss: 3.515500068664551
Validation loss: 3.2758158842722573

Epoch: 6| Step: 2
Training loss: 3.5981709957122803
Validation loss: 3.272104024887085

Epoch: 6| Step: 3
Training loss: 3.850351333618164
Validation loss: 3.2670141061147056

Epoch: 6| Step: 4
Training loss: 3.6336848735809326
Validation loss: 3.2626723845799765

Epoch: 6| Step: 5
Training loss: 3.438645839691162
Validation loss: 3.2593584060668945

Epoch: 6| Step: 6
Training loss: 4.147890090942383
Validation loss: 3.2551238536834717

Epoch: 6| Step: 7
Training loss: 3.408501148223877
Validation loss: 3.250281731287638

Epoch: 6| Step: 8
Training loss: 4.197517395019531
Validation loss: 3.2477506001790366

Epoch: 6| Step: 9
Training loss: 2.802645683288574
Validation loss: 3.2434107462565103

Epoch: 6| Step: 10
Training loss: 2.7949130535125732
Validation loss: 3.238728404045105

Epoch: 6| Step: 11
Training loss: 2.676135540008545
Validation loss: 3.234334150950114

Epoch: 6| Step: 12
Training loss: 3.7109413146972656
Validation loss: 3.2308311065038047

Epoch: 6| Step: 13
Training loss: 3.3768372535705566
Validation loss: 3.226660887400309

Epoch: 28| Step: 0
Training loss: 2.7335665225982666
Validation loss: 3.222844938437144

Epoch: 6| Step: 1
Training loss: 3.233969211578369
Validation loss: 3.219558080037435

Epoch: 6| Step: 2
Training loss: 3.6663737297058105
Validation loss: 3.215385635693868

Epoch: 6| Step: 3
Training loss: 2.7516884803771973
Validation loss: 3.2111394008000693

Epoch: 6| Step: 4
Training loss: 3.894270420074463
Validation loss: 3.2061901092529297

Epoch: 6| Step: 5
Training loss: 2.5249133110046387
Validation loss: 3.2024502754211426

Epoch: 6| Step: 6
Training loss: 3.4324874877929688
Validation loss: 3.1981430451075235

Epoch: 6| Step: 7
Training loss: 2.9813804626464844
Validation loss: 3.193670868873596

Epoch: 6| Step: 8
Training loss: 3.409701347351074
Validation loss: 3.188821236292521

Epoch: 6| Step: 9
Training loss: 4.252115249633789
Validation loss: 3.185787796974182

Epoch: 6| Step: 10
Training loss: 4.35725212097168
Validation loss: 3.182239294052124

Epoch: 6| Step: 11
Training loss: 3.7682087421417236
Validation loss: 3.1782319148381553

Epoch: 6| Step: 12
Training loss: 2.6060168743133545
Validation loss: 3.1735101540883384

Epoch: 6| Step: 13
Training loss: 3.6611196994781494
Validation loss: 3.1709063053131104

Epoch: 29| Step: 0
Training loss: 3.1423940658569336
Validation loss: 3.165317098299662

Epoch: 6| Step: 1
Training loss: 3.0735864639282227
Validation loss: 3.159959634145101

Epoch: 6| Step: 2
Training loss: 3.4815549850463867
Validation loss: 3.1550820668538413

Epoch: 6| Step: 3
Training loss: 3.322298765182495
Validation loss: 3.152257045110067

Epoch: 6| Step: 4
Training loss: 3.358402729034424
Validation loss: 3.147969961166382

Epoch: 6| Step: 5
Training loss: 3.2212367057800293
Validation loss: 3.144550840059916

Epoch: 6| Step: 6
Training loss: 4.072979927062988
Validation loss: 3.140811244646708

Epoch: 6| Step: 7
Training loss: 3.623514413833618
Validation loss: 3.1366512775421143

Epoch: 6| Step: 8
Training loss: 3.5082156658172607
Validation loss: 3.1378944714864097

Epoch: 6| Step: 9
Training loss: 3.76302433013916
Validation loss: 3.1310872634251914

Epoch: 6| Step: 10
Training loss: 3.4974966049194336
Validation loss: 3.126678943634033

Epoch: 6| Step: 11
Training loss: 2.7119901180267334
Validation loss: 3.1200042963027954

Epoch: 6| Step: 12
Training loss: 2.757443428039551
Validation loss: 3.114919066429138

Epoch: 6| Step: 13
Training loss: 2.9794859886169434
Validation loss: 3.111328363418579

Epoch: 30| Step: 0
Training loss: 3.6237218379974365
Validation loss: 3.1076605319976807

Epoch: 6| Step: 1
Training loss: 2.824883222579956
Validation loss: 3.104696273803711

Epoch: 6| Step: 2
Training loss: 3.9397153854370117
Validation loss: 3.1011911233266196

Epoch: 6| Step: 3
Training loss: 2.5888280868530273
Validation loss: 3.096063216527303

Epoch: 6| Step: 4
Training loss: 3.0056605339050293
Validation loss: 3.093660831451416

Epoch: 6| Step: 5
Training loss: 3.8163371086120605
Validation loss: 3.088530500729879

Epoch: 6| Step: 6
Training loss: 2.432556629180908
Validation loss: 3.0840046803156533

Epoch: 6| Step: 7
Training loss: 2.614065647125244
Validation loss: 3.080110947291056

Epoch: 6| Step: 8
Training loss: 3.5328147411346436
Validation loss: 3.077447017033895

Epoch: 6| Step: 9
Training loss: 3.7980661392211914
Validation loss: 3.0730662743250527

Epoch: 6| Step: 10
Training loss: 3.7043380737304688
Validation loss: 3.070074995358785

Epoch: 6| Step: 11
Training loss: 2.9117276668548584
Validation loss: 3.0652331113815308

Epoch: 6| Step: 12
Training loss: 3.5984671115875244
Validation loss: 3.062225262324015

Epoch: 6| Step: 13
Training loss: 3.4008400440216064
Validation loss: 3.0579121112823486

Epoch: 31| Step: 0
Training loss: 3.426301956176758
Validation loss: 3.053435722986857

Epoch: 6| Step: 1
Training loss: 3.541637659072876
Validation loss: 3.053080916404724

Epoch: 6| Step: 2
Training loss: 3.682034730911255
Validation loss: 3.0492328802744546

Epoch: 6| Step: 3
Training loss: 2.5242795944213867
Validation loss: 3.0466415087381997

Epoch: 6| Step: 4
Training loss: 2.7924060821533203
Validation loss: 3.044765830039978

Epoch: 6| Step: 5
Training loss: 3.3433494567871094
Validation loss: 3.0349040826161704

Epoch: 6| Step: 6
Training loss: 2.8879120349884033
Validation loss: 3.02967635790507

Epoch: 6| Step: 7
Training loss: 2.484055995941162
Validation loss: 3.024846911430359

Epoch: 6| Step: 8
Training loss: 3.4983034133911133
Validation loss: 3.022006869316101

Epoch: 6| Step: 9
Training loss: 3.319265365600586
Validation loss: 3.019578774770101

Epoch: 6| Step: 10
Training loss: 3.1676459312438965
Validation loss: 3.0164144039154053

Epoch: 6| Step: 11
Training loss: 3.7730135917663574
Validation loss: 3.0116575161616006

Epoch: 6| Step: 12
Training loss: 3.291769504547119
Validation loss: 3.0061285495758057

Epoch: 6| Step: 13
Training loss: 3.3677453994750977
Validation loss: 3.0009573698043823

Epoch: 32| Step: 0
Training loss: 2.6504316329956055
Validation loss: 2.996187925338745

Epoch: 6| Step: 1
Training loss: 3.135314702987671
Validation loss: 2.990169405937195

Epoch: 6| Step: 2
Training loss: 3.11895489692688
Validation loss: 2.986156384150187

Epoch: 6| Step: 3
Training loss: 2.4146616458892822
Validation loss: 2.982182780901591

Epoch: 6| Step: 4
Training loss: 4.243696212768555
Validation loss: 2.9841983715693154

Epoch: 6| Step: 5
Training loss: 3.001697301864624
Validation loss: 2.9855047464370728

Epoch: 6| Step: 6
Training loss: 3.8434319496154785
Validation loss: 2.9704230229059854

Epoch: 6| Step: 7
Training loss: 3.7838246822357178
Validation loss: 2.968558192253113

Epoch: 6| Step: 8
Training loss: 3.428621768951416
Validation loss: 2.9620862007141113

Epoch: 6| Step: 9
Training loss: 2.874936103820801
Validation loss: 2.9573781490325928

Epoch: 6| Step: 10
Training loss: 2.8117072582244873
Validation loss: 2.95410684744517

Epoch: 6| Step: 11
Training loss: 2.7590041160583496
Validation loss: 2.9511359135309854

Epoch: 6| Step: 12
Training loss: 2.8834409713745117
Validation loss: 2.9475303888320923

Epoch: 6| Step: 13
Training loss: 3.3321144580841064
Validation loss: 2.9425501028696694

Epoch: 33| Step: 0
Training loss: 3.0734310150146484
Validation loss: 2.9389611085255942

Epoch: 6| Step: 1
Training loss: 2.93117094039917
Validation loss: 2.9348271687825522

Epoch: 6| Step: 2
Training loss: 3.2378017902374268
Validation loss: 2.9308961232503257

Epoch: 6| Step: 3
Training loss: 3.1184606552124023
Validation loss: 2.927103598912557

Epoch: 6| Step: 4
Training loss: 3.711337089538574
Validation loss: 2.9234703381856284

Epoch: 6| Step: 5
Training loss: 3.31911563873291
Validation loss: 2.9198832909266152

Epoch: 6| Step: 6
Training loss: 2.4222121238708496
Validation loss: 2.9173999230066934

Epoch: 6| Step: 7
Training loss: 3.841508388519287
Validation loss: 2.915487051010132

Epoch: 6| Step: 8
Training loss: 2.4131922721862793
Validation loss: 2.914146661758423

Epoch: 6| Step: 9
Training loss: 2.534555196762085
Validation loss: 2.913585146268209

Epoch: 6| Step: 10
Training loss: 2.5597567558288574
Validation loss: 2.9277687072753906

Epoch: 6| Step: 11
Training loss: 4.316951274871826
Validation loss: 2.9111051162083945

Epoch: 6| Step: 12
Training loss: 2.8148155212402344
Validation loss: 2.894468605518341

Epoch: 6| Step: 13
Training loss: 3.3748624324798584
Validation loss: 2.8909212350845337

Epoch: 34| Step: 0
Training loss: 2.9072351455688477
Validation loss: 2.890517751375834

Epoch: 6| Step: 1
Training loss: 3.002840280532837
Validation loss: 2.8900291522343955

Epoch: 6| Step: 2
Training loss: 2.7249562740325928
Validation loss: 2.88977313041687

Epoch: 6| Step: 3
Training loss: 3.061959743499756
Validation loss: 2.8844149907430015

Epoch: 6| Step: 4
Training loss: 4.162494659423828
Validation loss: 2.8788499434789023

Epoch: 6| Step: 5
Training loss: 2.8818206787109375
Validation loss: 2.8737776279449463

Epoch: 6| Step: 6
Training loss: 3.156002998352051
Validation loss: 2.868990898132324

Epoch: 6| Step: 7
Training loss: 3.7681121826171875
Validation loss: 2.8654504219690957

Epoch: 6| Step: 8
Training loss: 3.9123382568359375
Validation loss: 2.863207538922628

Epoch: 6| Step: 9
Training loss: 3.3154871463775635
Validation loss: 2.8629834254582724

Epoch: 6| Step: 10
Training loss: 2.911746025085449
Validation loss: 2.8599778413772583

Epoch: 6| Step: 11
Training loss: 2.3179399967193604
Validation loss: 2.8537851174672446

Epoch: 6| Step: 12
Training loss: 2.469064235687256
Validation loss: 2.850523511568705

Epoch: 6| Step: 13
Training loss: 2.480494976043701
Validation loss: 2.847496589024862

Epoch: 35| Step: 0
Training loss: 3.487013101577759
Validation loss: 2.845693508783976

Epoch: 6| Step: 1
Training loss: 2.357628107070923
Validation loss: 2.843375245730082

Epoch: 6| Step: 2
Training loss: 3.3677587509155273
Validation loss: 2.8405174016952515

Epoch: 6| Step: 3
Training loss: 3.107835054397583
Validation loss: 2.8361765146255493

Epoch: 6| Step: 4
Training loss: 3.530932903289795
Validation loss: 2.8327470620473227

Epoch: 6| Step: 5
Training loss: 3.1366207599639893
Validation loss: 2.8285285234451294

Epoch: 6| Step: 6
Training loss: 2.9693689346313477
Validation loss: 2.824045459429423

Epoch: 6| Step: 7
Training loss: 2.4371237754821777
Validation loss: 2.821224888165792

Epoch: 6| Step: 8
Training loss: 3.2895619869232178
Validation loss: 2.8170659144719443

Epoch: 6| Step: 9
Training loss: 3.3276257514953613
Validation loss: 2.815821131070455

Epoch: 6| Step: 10
Training loss: 3.3086514472961426
Validation loss: 2.813739538192749

Epoch: 6| Step: 11
Training loss: 1.7456984519958496
Validation loss: 2.811928709348043

Epoch: 6| Step: 12
Training loss: 2.9760777950286865
Validation loss: 2.8084917465845742

Epoch: 6| Step: 13
Training loss: 3.44138503074646
Validation loss: 2.8081040382385254

Epoch: 36| Step: 0
Training loss: 2.6389780044555664
Validation loss: 2.8046894868214927

Epoch: 6| Step: 1
Training loss: 2.602252960205078
Validation loss: 2.803389390309652

Epoch: 6| Step: 2
Training loss: 3.409942388534546
Validation loss: 2.8036654790242515

Epoch: 6| Step: 3
Training loss: 3.840078830718994
Validation loss: 2.801849365234375

Epoch: 6| Step: 4
Training loss: 3.435685157775879
Validation loss: 2.792409658432007

Epoch: 6| Step: 5
Training loss: 2.667703628540039
Validation loss: 2.7867486079533896

Epoch: 6| Step: 6
Training loss: 3.6036930084228516
Validation loss: 2.781355857849121

Epoch: 6| Step: 7
Training loss: 2.6464719772338867
Validation loss: 2.778696894645691

Epoch: 6| Step: 8
Training loss: 2.745067596435547
Validation loss: 2.776809573173523

Epoch: 6| Step: 9
Training loss: 3.1191039085388184
Validation loss: 2.772935708363851

Epoch: 6| Step: 10
Training loss: 3.509049892425537
Validation loss: 2.769370198249817

Epoch: 6| Step: 11
Training loss: 2.878909111022949
Validation loss: 2.7667946020762124

Epoch: 6| Step: 12
Training loss: 2.4583282470703125
Validation loss: 2.762695868810018

Epoch: 6| Step: 13
Training loss: 2.30244779586792
Validation loss: 2.7602659861246743

Epoch: 37| Step: 0
Training loss: 2.9715840816497803
Validation loss: 2.7575062910715737

Epoch: 6| Step: 1
Training loss: 3.107548475265503
Validation loss: 2.75430691242218

Epoch: 6| Step: 2
Training loss: 3.191561698913574
Validation loss: 2.7516892353693643

Epoch: 6| Step: 3
Training loss: 3.124405860900879
Validation loss: 2.7507090171178183

Epoch: 6| Step: 4
Training loss: 3.1595537662506104
Validation loss: 2.746256470680237

Epoch: 6| Step: 5
Training loss: 1.9975097179412842
Validation loss: 2.744634429613749

Epoch: 6| Step: 6
Training loss: 4.303081512451172
Validation loss: 2.740840792655945

Epoch: 6| Step: 7
Training loss: 3.2655832767486572
Validation loss: 2.7375530203183494

Epoch: 6| Step: 8
Training loss: 2.6341257095336914
Validation loss: 2.7350913683573403

Epoch: 6| Step: 9
Training loss: 2.214223623275757
Validation loss: 2.731898864110311

Epoch: 6| Step: 10
Training loss: 2.835153818130493
Validation loss: 2.7294450203577676

Epoch: 6| Step: 11
Training loss: 2.64931058883667
Validation loss: 2.7260692914326987

Epoch: 6| Step: 12
Training loss: 3.0197291374206543
Validation loss: 2.7238561312357583

Epoch: 6| Step: 13
Training loss: 2.8129029273986816
Validation loss: 2.720846732457479

Epoch: 38| Step: 0
Training loss: 1.809800386428833
Validation loss: 2.718234101931254

Epoch: 6| Step: 1
Training loss: 3.0842931270599365
Validation loss: 2.7148388624191284

Epoch: 6| Step: 2
Training loss: 3.310363292694092
Validation loss: 2.712948441505432

Epoch: 6| Step: 3
Training loss: 2.992602825164795
Validation loss: 2.7095525662104287

Epoch: 6| Step: 4
Training loss: 2.3893816471099854
Validation loss: 2.706123868624369

Epoch: 6| Step: 5
Training loss: 3.282057285308838
Validation loss: 2.704447070757548

Epoch: 6| Step: 6
Training loss: 2.9166617393493652
Validation loss: 2.7055830160776773

Epoch: 6| Step: 7
Training loss: 2.0198895931243896
Validation loss: 2.705856362978617

Epoch: 6| Step: 8
Training loss: 2.8934006690979004
Validation loss: 2.7006914615631104

Epoch: 6| Step: 9
Training loss: 3.6394338607788086
Validation loss: 2.6969619194666543

Epoch: 6| Step: 10
Training loss: 3.5646989345550537
Validation loss: 2.6916690667470298

Epoch: 6| Step: 11
Training loss: 2.899693012237549
Validation loss: 2.687087059020996

Epoch: 6| Step: 12
Training loss: 2.902238130569458
Validation loss: 2.6836415926615396

Epoch: 6| Step: 13
Training loss: 2.912435531616211
Validation loss: 2.681489189465841

Epoch: 39| Step: 0
Training loss: 2.836226463317871
Validation loss: 2.6792792876561484

Epoch: 6| Step: 1
Training loss: 2.775052070617676
Validation loss: 2.6763280431429544

Epoch: 6| Step: 2
Training loss: 3.3472938537597656
Validation loss: 2.673214872678121

Epoch: 6| Step: 3
Training loss: 2.6682450771331787
Validation loss: 2.6703351934750876

Epoch: 6| Step: 4
Training loss: 2.86332106590271
Validation loss: 2.6653037468592324

Epoch: 6| Step: 5
Training loss: 2.5160651206970215
Validation loss: 2.6638333797454834

Epoch: 6| Step: 6
Training loss: 2.650161027908325
Validation loss: 2.6598581870396933

Epoch: 6| Step: 7
Training loss: 3.226271629333496
Validation loss: 2.658259630203247

Epoch: 6| Step: 8
Training loss: 2.9790406227111816
Validation loss: 2.652224818865458

Epoch: 6| Step: 9
Training loss: 3.2218074798583984
Validation loss: 2.6508440176645913

Epoch: 6| Step: 10
Training loss: 2.6078338623046875
Validation loss: 2.6467316150665283

Epoch: 6| Step: 11
Training loss: 2.6426830291748047
Validation loss: 2.6423758268356323

Epoch: 6| Step: 12
Training loss: 3.2991905212402344
Validation loss: 2.64472488562266

Epoch: 6| Step: 13
Training loss: 2.412802219390869
Validation loss: 2.638595461845398

Epoch: 40| Step: 0
Training loss: 2.9023256301879883
Validation loss: 2.6477109591166177

Epoch: 6| Step: 1
Training loss: 2.9452743530273438
Validation loss: 2.6571813821792603

Epoch: 6| Step: 2
Training loss: 2.7123730182647705
Validation loss: 2.625604430834452

Epoch: 6| Step: 3
Training loss: 3.0384297370910645
Validation loss: 2.624481717745463

Epoch: 6| Step: 4
Training loss: 2.680224657058716
Validation loss: 2.625196119149526

Epoch: 6| Step: 5
Training loss: 2.7520856857299805
Validation loss: 2.629356841246287

Epoch: 6| Step: 6
Training loss: 3.7137932777404785
Validation loss: 2.6276039679845176

Epoch: 6| Step: 7
Training loss: 2.3822991847991943
Validation loss: 2.6238266229629517

Epoch: 6| Step: 8
Training loss: 2.749992847442627
Validation loss: 2.6190143823623657

Epoch: 6| Step: 9
Training loss: 2.8350000381469727
Validation loss: 2.616557757059733

Epoch: 6| Step: 10
Training loss: 2.5640034675598145
Validation loss: 2.6153615713119507

Epoch: 6| Step: 11
Training loss: 3.1314237117767334
Validation loss: 2.6114308834075928

Epoch: 6| Step: 12
Training loss: 2.4065613746643066
Validation loss: 2.608400503794352

Epoch: 6| Step: 13
Training loss: 2.7499375343322754
Validation loss: 2.6029417514801025

Epoch: 41| Step: 0
Training loss: 2.9502830505371094
Validation loss: 2.5985618034998574

Epoch: 6| Step: 1
Training loss: 2.9342143535614014
Validation loss: 2.593256632486979

Epoch: 6| Step: 2
Training loss: 2.0152201652526855
Validation loss: 2.590332349141439

Epoch: 6| Step: 3
Training loss: 3.012305974960327
Validation loss: 2.5859049956003823

Epoch: 6| Step: 4
Training loss: 2.4756855964660645
Validation loss: 2.5822255611419678

Epoch: 6| Step: 5
Training loss: 3.1626317501068115
Validation loss: 2.581409494082133

Epoch: 6| Step: 6
Training loss: 2.359367609024048
Validation loss: 2.5779608488082886

Epoch: 6| Step: 7
Training loss: 2.6417064666748047
Validation loss: 2.5781619548797607

Epoch: 6| Step: 8
Training loss: 2.5236716270446777
Validation loss: 2.5845505793889365

Epoch: 6| Step: 9
Training loss: 2.773200511932373
Validation loss: 2.579871495564779

Epoch: 6| Step: 10
Training loss: 2.936999559402466
Validation loss: 2.5701480309168496

Epoch: 6| Step: 11
Training loss: 2.7510085105895996
Validation loss: 2.566572388013204

Epoch: 6| Step: 12
Training loss: 3.2501354217529297
Validation loss: 2.560638904571533

Epoch: 6| Step: 13
Training loss: 3.0966238975524902
Validation loss: 2.558462659517924

Epoch: 42| Step: 0
Training loss: 2.208932638168335
Validation loss: 2.5544894536336265

Epoch: 6| Step: 1
Training loss: 2.6335558891296387
Validation loss: 2.554372866948446

Epoch: 6| Step: 2
Training loss: 2.8826375007629395
Validation loss: 2.552865823109945

Epoch: 6| Step: 3
Training loss: 3.026995897293091
Validation loss: 2.549557606379191

Epoch: 6| Step: 4
Training loss: 2.9914050102233887
Validation loss: 2.5500261783599854

Epoch: 6| Step: 5
Training loss: 2.993135690689087
Validation loss: 2.5473064184188843

Epoch: 6| Step: 6
Training loss: 2.3373372554779053
Validation loss: 2.545186678568522

Epoch: 6| Step: 7
Training loss: 2.8924195766448975
Validation loss: 2.542841116587321

Epoch: 6| Step: 8
Training loss: 2.117992639541626
Validation loss: 2.537754774093628

Epoch: 6| Step: 9
Training loss: 3.0847721099853516
Validation loss: 2.5331046183904014

Epoch: 6| Step: 10
Training loss: 2.550910472869873
Validation loss: 2.5328278144200644

Epoch: 6| Step: 11
Training loss: 3.2528321743011475
Validation loss: 2.527885993321737

Epoch: 6| Step: 12
Training loss: 3.056140899658203
Validation loss: 2.5252530574798584

Epoch: 6| Step: 13
Training loss: 2.3502354621887207
Validation loss: 2.5242135524749756

Epoch: 43| Step: 0
Training loss: 2.963733434677124
Validation loss: 2.5199045141537986

Epoch: 6| Step: 1
Training loss: 2.4485831260681152
Validation loss: 2.5183066924413047

Epoch: 6| Step: 2
Training loss: 3.005362033843994
Validation loss: 2.5178736448287964

Epoch: 6| Step: 3
Training loss: 2.8314743041992188
Validation loss: 2.513019243876139

Epoch: 6| Step: 4
Training loss: 3.391505241394043
Validation loss: 2.5096266667048135

Epoch: 6| Step: 5
Training loss: 3.0356979370117188
Validation loss: 2.507657527923584

Epoch: 6| Step: 6
Training loss: 3.063293218612671
Validation loss: 2.5010494788487754

Epoch: 6| Step: 7
Training loss: 2.7082955837249756
Validation loss: 2.5015933911005654

Epoch: 6| Step: 8
Training loss: 2.634896993637085
Validation loss: 2.5047517816225686

Epoch: 6| Step: 9
Training loss: 2.3797435760498047
Validation loss: 2.510936419169108

Epoch: 6| Step: 10
Training loss: 2.276219367980957
Validation loss: 2.50389031569163

Epoch: 6| Step: 11
Training loss: 2.191892385482788
Validation loss: 2.501332859198252

Epoch: 6| Step: 12
Training loss: 3.0241456031799316
Validation loss: 2.4842302799224854

Epoch: 6| Step: 13
Training loss: 1.8128248453140259
Validation loss: 2.4839466412862143

Epoch: 44| Step: 0
Training loss: 2.8263564109802246
Validation loss: 2.481285015741984

Epoch: 6| Step: 1
Training loss: 2.6960062980651855
Validation loss: 2.4799119432767234

Epoch: 6| Step: 2
Training loss: 3.5983691215515137
Validation loss: 2.4787404537200928

Epoch: 6| Step: 3
Training loss: 1.8944978713989258
Validation loss: 2.4809120496114097

Epoch: 6| Step: 4
Training loss: 2.7497315406799316
Validation loss: 2.4840651551882424

Epoch: 6| Step: 5
Training loss: 3.012685775756836
Validation loss: 2.4877378940582275

Epoch: 6| Step: 6
Training loss: 2.731382369995117
Validation loss: 2.4885980685551963

Epoch: 6| Step: 7
Training loss: 2.912876605987549
Validation loss: 2.4912514289220176

Epoch: 6| Step: 8
Training loss: 3.2354767322540283
Validation loss: 2.4857352574666343

Epoch: 6| Step: 9
Training loss: 2.6695494651794434
Validation loss: 2.475820779800415

Epoch: 6| Step: 10
Training loss: 2.1696152687072754
Validation loss: 2.466577967007955

Epoch: 6| Step: 11
Training loss: 2.2809932231903076
Validation loss: 2.4594631592432656

Epoch: 6| Step: 12
Training loss: 2.0023090839385986
Validation loss: 2.4544003009796143

Epoch: 6| Step: 13
Training loss: 2.5924854278564453
Validation loss: 2.4480642875035605

Epoch: 45| Step: 0
Training loss: 2.6217827796936035
Validation loss: 2.445193807284037

Epoch: 6| Step: 1
Training loss: 2.443166494369507
Validation loss: 2.4415780504544577

Epoch: 6| Step: 2
Training loss: 1.970505952835083
Validation loss: 2.439688245455424

Epoch: 6| Step: 3
Training loss: 2.5721054077148438
Validation loss: 2.437934637069702

Epoch: 6| Step: 4
Training loss: 2.743544340133667
Validation loss: 2.437088350454966

Epoch: 6| Step: 5
Training loss: 2.1926212310791016
Validation loss: 2.4316320419311523

Epoch: 6| Step: 6
Training loss: 2.570493221282959
Validation loss: 2.433645725250244

Epoch: 6| Step: 7
Training loss: 2.19718337059021
Validation loss: 2.429470698038737

Epoch: 6| Step: 8
Training loss: 2.7604494094848633
Validation loss: 2.431019405523936

Epoch: 6| Step: 9
Training loss: 2.700746536254883
Validation loss: 2.4393078883488974

Epoch: 6| Step: 10
Training loss: 3.5296027660369873
Validation loss: 2.4311081965764365

Epoch: 6| Step: 11
Training loss: 3.0366337299346924
Validation loss: 2.420357346534729

Epoch: 6| Step: 12
Training loss: 2.654029369354248
Validation loss: 2.418203512827555

Epoch: 6| Step: 13
Training loss: 2.676792860031128
Validation loss: 2.4145347674687705

Epoch: 46| Step: 0
Training loss: 2.7010176181793213
Validation loss: 2.4079386591911316

Epoch: 6| Step: 1
Training loss: 2.350346088409424
Validation loss: 2.409086982409159

Epoch: 6| Step: 2
Training loss: 2.1956787109375
Validation loss: 2.4069308042526245

Epoch: 6| Step: 3
Training loss: 3.5320663452148438
Validation loss: 2.406529406706492

Epoch: 6| Step: 4
Training loss: 3.1995186805725098
Validation loss: 2.4037360747655234

Epoch: 6| Step: 5
Training loss: 2.6151974201202393
Validation loss: 2.402998685836792

Epoch: 6| Step: 6
Training loss: 2.351078987121582
Validation loss: 2.400451421737671

Epoch: 6| Step: 7
Training loss: 1.9006431102752686
Validation loss: 2.398961822191874

Epoch: 6| Step: 8
Training loss: 2.8996458053588867
Validation loss: 2.398137350877126

Epoch: 6| Step: 9
Training loss: 2.4154577255249023
Validation loss: 2.3964240550994873

Epoch: 6| Step: 10
Training loss: 2.429746627807617
Validation loss: 2.393430312474569

Epoch: 6| Step: 11
Training loss: 2.493964672088623
Validation loss: 2.388782024383545

Epoch: 6| Step: 12
Training loss: 2.8145546913146973
Validation loss: 2.38551793495814

Epoch: 6| Step: 13
Training loss: 2.2786314487457275
Validation loss: 2.3848664363225303

Epoch: 47| Step: 0
Training loss: 2.61472225189209
Validation loss: 2.3797380924224854

Epoch: 6| Step: 1
Training loss: 2.3304786682128906
Validation loss: 2.380338668823242

Epoch: 6| Step: 2
Training loss: 2.462923049926758
Validation loss: 2.3765756289164224

Epoch: 6| Step: 3
Training loss: 2.5890190601348877
Validation loss: 2.375166336695353

Epoch: 6| Step: 4
Training loss: 1.9706001281738281
Validation loss: 2.375968317190806

Epoch: 6| Step: 5
Training loss: 2.963846206665039
Validation loss: 2.3787096540133157

Epoch: 6| Step: 6
Training loss: 2.4635918140411377
Validation loss: 2.38265589872996

Epoch: 6| Step: 7
Training loss: 2.5525729656219482
Validation loss: 2.3756611744562783

Epoch: 6| Step: 8
Training loss: 2.060570240020752
Validation loss: 2.370929797490438

Epoch: 6| Step: 9
Training loss: 2.9180378913879395
Validation loss: 2.364934424559275

Epoch: 6| Step: 10
Training loss: 2.9563159942626953
Validation loss: 2.356098175048828

Epoch: 6| Step: 11
Training loss: 2.3506345748901367
Validation loss: 2.3538902600606284

Epoch: 6| Step: 12
Training loss: 3.016941547393799
Validation loss: 2.3565791050593057

Epoch: 6| Step: 13
Training loss: 2.4480998516082764
Validation loss: 2.353928526242574

Epoch: 48| Step: 0
Training loss: 2.0829660892486572
Validation loss: 2.3523898323376975

Epoch: 6| Step: 1
Training loss: 2.1560163497924805
Validation loss: 2.3543042739232383

Epoch: 6| Step: 2
Training loss: 2.5793323516845703
Validation loss: 2.3522598147392273

Epoch: 6| Step: 3
Training loss: 2.891573905944824
Validation loss: 2.3511842091878257

Epoch: 6| Step: 4
Training loss: 2.643003225326538
Validation loss: 2.347949723402659

Epoch: 6| Step: 5
Training loss: 2.3681845664978027
Validation loss: 2.344910442829132

Epoch: 6| Step: 6
Training loss: 2.555676221847534
Validation loss: 2.3401883244514465

Epoch: 6| Step: 7
Training loss: 2.6373820304870605
Validation loss: 2.338482062021891

Epoch: 6| Step: 8
Training loss: 3.1995701789855957
Validation loss: 2.334502935409546

Epoch: 6| Step: 9
Training loss: 2.2675981521606445
Validation loss: 2.3282756010691323

Epoch: 6| Step: 10
Training loss: 2.2937889099121094
Validation loss: 2.326929966608683

Epoch: 6| Step: 11
Training loss: 3.1592752933502197
Validation loss: 2.3254082004229226

Epoch: 6| Step: 12
Training loss: 2.3481149673461914
Validation loss: 2.3219590981801352

Epoch: 6| Step: 13
Training loss: 2.0309085845947266
Validation loss: 2.3181936740875244

Epoch: 49| Step: 0
Training loss: 2.700314998626709
Validation loss: 2.3234145641326904

Epoch: 6| Step: 1
Training loss: 2.94292950630188
Validation loss: 2.3205746014912925

Epoch: 6| Step: 2
Training loss: 2.3870668411254883
Validation loss: 2.314524292945862

Epoch: 6| Step: 3
Training loss: 2.599567413330078
Validation loss: 2.3124693234761557

Epoch: 6| Step: 4
Training loss: 2.301539421081543
Validation loss: 2.3124330639839172

Epoch: 6| Step: 5
Training loss: 2.058825969696045
Validation loss: 2.3094212412834167

Epoch: 6| Step: 6
Training loss: 1.6637136936187744
Validation loss: 2.304282029469808

Epoch: 6| Step: 7
Training loss: 3.0846598148345947
Validation loss: 2.303965389728546

Epoch: 6| Step: 8
Training loss: 1.786043405532837
Validation loss: 2.305777629216512

Epoch: 6| Step: 9
Training loss: 2.2050747871398926
Validation loss: 2.303031404813131

Epoch: 6| Step: 10
Training loss: 3.1441779136657715
Validation loss: 2.298961599667867

Epoch: 6| Step: 11
Training loss: 2.6537766456604004
Validation loss: 2.298027197519938

Epoch: 6| Step: 12
Training loss: 2.3995919227600098
Validation loss: 2.2976197401682534

Epoch: 6| Step: 13
Training loss: 2.7872447967529297
Validation loss: 2.2953189412752786

Epoch: 50| Step: 0
Training loss: 2.849553108215332
Validation loss: 2.293483853340149

Epoch: 6| Step: 1
Training loss: 2.6048731803894043
Validation loss: 2.293242653210958

Epoch: 6| Step: 2
Training loss: 1.971880555152893
Validation loss: 2.2936574618021646

Epoch: 6| Step: 3
Training loss: 1.6554532051086426
Validation loss: 2.2926417787869773

Epoch: 6| Step: 4
Training loss: 2.4799671173095703
Validation loss: 2.2917945782343545

Epoch: 6| Step: 5
Training loss: 2.3686771392822266
Validation loss: 2.289435068766276

Epoch: 6| Step: 6
Training loss: 2.218038320541382
Validation loss: 2.2869226137797036

Epoch: 6| Step: 7
Training loss: 2.6690587997436523
Validation loss: 2.2860347429911294

Epoch: 6| Step: 8
Training loss: 2.3462090492248535
Validation loss: 2.2808008193969727

Epoch: 6| Step: 9
Training loss: 2.6841394901275635
Validation loss: 2.278432250022888

Epoch: 6| Step: 10
Training loss: 2.541609525680542
Validation loss: 2.274993817011515

Epoch: 6| Step: 11
Training loss: 2.4464917182922363
Validation loss: 2.271741191546122

Epoch: 6| Step: 12
Training loss: 2.759536027908325
Validation loss: 2.2674953937530518

Epoch: 6| Step: 13
Training loss: 2.751983404159546
Validation loss: 2.2642404635747275

Epoch: 51| Step: 0
Training loss: 2.2509634494781494
Validation loss: 2.2596933444341025

Epoch: 6| Step: 1
Training loss: 2.463301658630371
Validation loss: 2.2552090883255005

Epoch: 6| Step: 2
Training loss: 2.5633177757263184
Validation loss: 2.253359874089559

Epoch: 6| Step: 3
Training loss: 2.5189316272735596
Validation loss: 2.249605357646942

Epoch: 6| Step: 4
Training loss: 2.3016693592071533
Validation loss: 2.25498894850413

Epoch: 6| Step: 5
Training loss: 2.660315990447998
Validation loss: 2.2460657954216003

Epoch: 6| Step: 6
Training loss: 2.2991271018981934
Validation loss: 2.248408297697703

Epoch: 6| Step: 7
Training loss: 2.1490495204925537
Validation loss: 2.247653861840566

Epoch: 6| Step: 8
Training loss: 1.9418439865112305
Validation loss: 2.2422854900360107

Epoch: 6| Step: 9
Training loss: 2.468672275543213
Validation loss: 2.2430721521377563

Epoch: 6| Step: 10
Training loss: 2.218205451965332
Validation loss: 2.2386791706085205

Epoch: 6| Step: 11
Training loss: 2.669511079788208
Validation loss: 2.238313853740692

Epoch: 6| Step: 12
Training loss: 2.6871535778045654
Validation loss: 2.236052413781484

Epoch: 6| Step: 13
Training loss: 2.5206968784332275
Validation loss: 2.233258366584778

Epoch: 52| Step: 0
Training loss: 2.400819778442383
Validation loss: 2.2334411342938743

Epoch: 6| Step: 1
Training loss: 2.154494285583496
Validation loss: 2.227053244908651

Epoch: 6| Step: 2
Training loss: 2.1751632690429688
Validation loss: 2.226892371972402

Epoch: 6| Step: 3
Training loss: 2.6291937828063965
Validation loss: 2.22798091173172

Epoch: 6| Step: 4
Training loss: 2.8360087871551514
Validation loss: 2.224078138669332

Epoch: 6| Step: 5
Training loss: 1.6519378423690796
Validation loss: 2.2267033656438193

Epoch: 6| Step: 6
Training loss: 2.0605812072753906
Validation loss: 2.2237616777420044

Epoch: 6| Step: 7
Training loss: 2.906614303588867
Validation loss: 2.223301629225413

Epoch: 6| Step: 8
Training loss: 2.798893928527832
Validation loss: 2.2191214164098105

Epoch: 6| Step: 9
Training loss: 1.9306139945983887
Validation loss: 2.21750670671463

Epoch: 6| Step: 10
Training loss: 2.545534610748291
Validation loss: 2.220275561014811

Epoch: 6| Step: 11
Training loss: 2.6073355674743652
Validation loss: 2.2137920459111533

Epoch: 6| Step: 12
Training loss: 2.343414783477783
Validation loss: 2.2141345540682473

Epoch: 6| Step: 13
Training loss: 2.2960314750671387
Validation loss: 2.2101852695147195

Epoch: 53| Step: 0
Training loss: 2.338275194168091
Validation loss: 2.2145684560139975

Epoch: 6| Step: 1
Training loss: 2.336470127105713
Validation loss: 2.2036170164744058

Epoch: 6| Step: 2
Training loss: 2.586162567138672
Validation loss: 2.2061562140782676

Epoch: 6| Step: 3
Training loss: 1.7920336723327637
Validation loss: 2.199100693066915

Epoch: 6| Step: 4
Training loss: 2.336228370666504
Validation loss: 2.201797087987264

Epoch: 6| Step: 5
Training loss: 2.4074935913085938
Validation loss: 2.1944840947786965

Epoch: 6| Step: 6
Training loss: 2.2637434005737305
Validation loss: 2.1953216592470803

Epoch: 6| Step: 7
Training loss: 2.017845869064331
Validation loss: 2.193124751249949

Epoch: 6| Step: 8
Training loss: 2.009979248046875
Validation loss: 2.1898491978645325

Epoch: 6| Step: 9
Training loss: 1.8824927806854248
Validation loss: 2.192073166370392

Epoch: 6| Step: 10
Training loss: 2.1177310943603516
Validation loss: 2.184771021207174

Epoch: 6| Step: 11
Training loss: 2.961789131164551
Validation loss: 2.1851614713668823

Epoch: 6| Step: 12
Training loss: 3.0520706176757812
Validation loss: 2.1894236405690513

Epoch: 6| Step: 13
Training loss: 2.8445346355438232
Validation loss: 2.185558040936788

Epoch: 54| Step: 0
Training loss: 2.4782724380493164
Validation loss: 2.181432863076528

Epoch: 6| Step: 1
Training loss: 2.740949869155884
Validation loss: 2.1820117235183716

Epoch: 6| Step: 2
Training loss: 2.0313560962677
Validation loss: 2.1873938838640847

Epoch: 6| Step: 3
Training loss: 2.2052195072174072
Validation loss: 2.176722764968872

Epoch: 6| Step: 4
Training loss: 2.1138954162597656
Validation loss: 2.1846161484718323

Epoch: 6| Step: 5
Training loss: 2.017286777496338
Validation loss: 2.1835731665293374

Epoch: 6| Step: 6
Training loss: 2.044832706451416
Validation loss: 2.1840154925982156

Epoch: 6| Step: 7
Training loss: 2.602748394012451
Validation loss: 2.183804670969645

Epoch: 6| Step: 8
Training loss: 2.3133442401885986
Validation loss: 2.192265431086222

Epoch: 6| Step: 9
Training loss: 2.560464859008789
Validation loss: 2.192622403303782

Epoch: 6| Step: 10
Training loss: 2.7257418632507324
Validation loss: 2.1857205430666604

Epoch: 6| Step: 11
Training loss: 2.027848958969116
Validation loss: 2.178971290588379

Epoch: 6| Step: 12
Training loss: 2.196772813796997
Validation loss: 2.1732871532440186

Epoch: 6| Step: 13
Training loss: 2.7539169788360596
Validation loss: 2.175729513168335

Epoch: 55| Step: 0
Training loss: 2.207385540008545
Validation loss: 2.1714423100153604

Epoch: 6| Step: 1
Training loss: 2.2208094596862793
Validation loss: 2.172598044077555

Epoch: 6| Step: 2
Training loss: 2.81764817237854
Validation loss: 2.166496435801188

Epoch: 6| Step: 3
Training loss: 2.093067169189453
Validation loss: 2.1649295488993325

Epoch: 6| Step: 4
Training loss: 2.5566306114196777
Validation loss: 2.1636246045430503

Epoch: 6| Step: 5
Training loss: 2.496431350708008
Validation loss: 2.1569000681241355

Epoch: 6| Step: 6
Training loss: 2.1571693420410156
Validation loss: 2.156820774078369

Epoch: 6| Step: 7
Training loss: 2.743652820587158
Validation loss: 2.1476075251897178

Epoch: 6| Step: 8
Training loss: 2.2651214599609375
Validation loss: 2.149550437927246

Epoch: 6| Step: 9
Training loss: 1.3493521213531494
Validation loss: 2.152630349000295

Epoch: 6| Step: 10
Training loss: 3.3032114505767822
Validation loss: 2.1525514324506125

Epoch: 6| Step: 11
Training loss: 1.8179278373718262
Validation loss: 2.1516221165657043

Epoch: 6| Step: 12
Training loss: 2.2914512157440186
Validation loss: 2.1520906488100686

Epoch: 6| Step: 13
Training loss: 2.2554962635040283
Validation loss: 2.1525675654411316

Epoch: 56| Step: 0
Training loss: 2.4890570640563965
Validation loss: 2.1505601604779563

Epoch: 6| Step: 1
Training loss: 1.8580315113067627
Validation loss: 2.1574153502782187

Epoch: 6| Step: 2
Training loss: 2.9584734439849854
Validation loss: 2.1491169134775796

Epoch: 6| Step: 3
Training loss: 2.5489659309387207
Validation loss: 2.148123860359192

Epoch: 6| Step: 4
Training loss: 2.2742667198181152
Validation loss: 2.1483789682388306

Epoch: 6| Step: 5
Training loss: 2.7904105186462402
Validation loss: 2.1427476604779563

Epoch: 6| Step: 6
Training loss: 2.0844101905822754
Validation loss: 2.1431572834650674

Epoch: 6| Step: 7
Training loss: 2.085965633392334
Validation loss: 2.136640707651774

Epoch: 6| Step: 8
Training loss: 2.419811248779297
Validation loss: 2.139023224512736

Epoch: 6| Step: 9
Training loss: 2.2101902961730957
Validation loss: 2.1312479972839355

Epoch: 6| Step: 10
Training loss: 2.5714614391326904
Validation loss: 2.1393521626790366

Epoch: 6| Step: 11
Training loss: 1.9170280694961548
Validation loss: 2.1331051786740622

Epoch: 6| Step: 12
Training loss: 1.9972026348114014
Validation loss: 2.1302780310312905

Epoch: 6| Step: 13
Training loss: 2.1521544456481934
Validation loss: 2.13342148065567

Epoch: 57| Step: 0
Training loss: 1.8100674152374268
Validation loss: 2.1303048729896545

Epoch: 6| Step: 1
Training loss: 2.3638253211975098
Validation loss: 2.134492059548696

Epoch: 6| Step: 2
Training loss: 1.8196176290512085
Validation loss: 2.1313739021619162

Epoch: 6| Step: 3
Training loss: 1.9536079168319702
Validation loss: 2.133997698624929

Epoch: 6| Step: 4
Training loss: 2.1870288848876953
Validation loss: 2.129911800225576

Epoch: 6| Step: 5
Training loss: 1.6605870723724365
Validation loss: 2.1332129637400308

Epoch: 6| Step: 6
Training loss: 2.0190391540527344
Validation loss: 2.1395520766576133

Epoch: 6| Step: 7
Training loss: 2.887582302093506
Validation loss: 2.140082041422526

Epoch: 6| Step: 8
Training loss: 2.4650509357452393
Validation loss: 2.1342145601908364

Epoch: 6| Step: 9
Training loss: 2.7105958461761475
Validation loss: 2.1272125840187073

Epoch: 6| Step: 10
Training loss: 2.2520523071289062
Validation loss: 2.1278371612230935

Epoch: 6| Step: 11
Training loss: 3.069159507751465
Validation loss: 2.128542959690094

Epoch: 6| Step: 12
Training loss: 2.772752523422241
Validation loss: 2.1274789571762085

Epoch: 6| Step: 13
Training loss: 2.223917007446289
Validation loss: 2.1137587626775107

Epoch: 58| Step: 0
Training loss: 2.628175735473633
Validation loss: 2.117357869942983

Epoch: 6| Step: 1
Training loss: 2.6290597915649414
Validation loss: 2.1172507206598916

Epoch: 6| Step: 2
Training loss: 2.028357744216919
Validation loss: 2.1172096331914267

Epoch: 6| Step: 3
Training loss: 2.4666075706481934
Validation loss: 2.1170222957928977

Epoch: 6| Step: 4
Training loss: 2.3745768070220947
Validation loss: 2.118025084336599

Epoch: 6| Step: 5
Training loss: 2.3451452255249023
Validation loss: 2.1221697330474854

Epoch: 6| Step: 6
Training loss: 2.2285890579223633
Validation loss: 2.119784692923228

Epoch: 6| Step: 7
Training loss: 2.3690953254699707
Validation loss: 2.1189200282096863

Epoch: 6| Step: 8
Training loss: 2.490260362625122
Validation loss: 2.1219462951024375

Epoch: 6| Step: 9
Training loss: 1.9710668325424194
Validation loss: 2.120631913344065

Epoch: 6| Step: 10
Training loss: 2.0505149364471436
Validation loss: 2.1171679298082986

Epoch: 6| Step: 11
Training loss: 2.3903849124908447
Validation loss: 2.1146899461746216

Epoch: 6| Step: 12
Training loss: 1.958711862564087
Validation loss: 2.1114778916041055

Epoch: 6| Step: 13
Training loss: 2.212728261947632
Validation loss: 2.114989976088206

Epoch: 59| Step: 0
Training loss: 2.249375581741333
Validation loss: 2.105361541112264

Epoch: 6| Step: 1
Training loss: 2.198032855987549
Validation loss: 2.106042802333832

Epoch: 6| Step: 2
Training loss: 2.488187313079834
Validation loss: 2.109460632006327

Epoch: 6| Step: 3
Training loss: 2.567216396331787
Validation loss: 2.1119625171025596

Epoch: 6| Step: 4
Training loss: 1.6928822994232178
Validation loss: 2.1147615710894265

Epoch: 6| Step: 5
Training loss: 2.412412643432617
Validation loss: 2.109502096970876

Epoch: 6| Step: 6
Training loss: 2.461941957473755
Validation loss: 2.103319823741913

Epoch: 6| Step: 7
Training loss: 2.0189297199249268
Validation loss: 2.10408753156662

Epoch: 6| Step: 8
Training loss: 2.230093002319336
Validation loss: 2.101225713888804

Epoch: 6| Step: 9
Training loss: 2.2123184204101562
Validation loss: 2.1011576851209006

Epoch: 6| Step: 10
Training loss: 2.5698647499084473
Validation loss: 2.10074390967687

Epoch: 6| Step: 11
Training loss: 1.8235578536987305
Validation loss: 2.1024379332860312

Epoch: 6| Step: 12
Training loss: 2.6738576889038086
Validation loss: 2.102620700995127

Epoch: 6| Step: 13
Training loss: 2.450305223464966
Validation loss: 2.1032737096150718

Epoch: 60| Step: 0
Training loss: 2.1804988384246826
Validation loss: 2.0990975499153137

Epoch: 6| Step: 1
Training loss: 2.328277349472046
Validation loss: 2.10022242863973

Epoch: 6| Step: 2
Training loss: 2.196558952331543
Validation loss: 2.0946046113967896

Epoch: 6| Step: 3
Training loss: 2.528928756713867
Validation loss: 2.1028836568196616

Epoch: 6| Step: 4
Training loss: 2.724280834197998
Validation loss: 2.092182477315267

Epoch: 6| Step: 5
Training loss: 1.9880274534225464
Validation loss: 2.095228354136149

Epoch: 6| Step: 6
Training loss: 2.470003604888916
Validation loss: 2.095801075299581

Epoch: 6| Step: 7
Training loss: 2.288912773132324
Validation loss: 2.0957128206888833

Epoch: 6| Step: 8
Training loss: 1.4679919481277466
Validation loss: 2.094710409641266

Epoch: 6| Step: 9
Training loss: 2.443002700805664
Validation loss: 2.0909565687179565

Epoch: 6| Step: 10
Training loss: 2.580965280532837
Validation loss: 2.0873839457829795

Epoch: 6| Step: 11
Training loss: 2.504549980163574
Validation loss: 2.0879135529200235

Epoch: 6| Step: 12
Training loss: 2.0209665298461914
Validation loss: 2.103459676106771

Epoch: 6| Step: 13
Training loss: 2.151732921600342
Validation loss: 2.113536218802134

Epoch: 61| Step: 0
Training loss: 2.4069604873657227
Validation loss: 2.087626576423645

Epoch: 6| Step: 1
Training loss: 2.0451700687408447
Validation loss: 2.079606612523397

Epoch: 6| Step: 2
Training loss: 2.6980438232421875
Validation loss: 2.0825547774632773

Epoch: 6| Step: 3
Training loss: 2.3517813682556152
Validation loss: 2.0833382407824197

Epoch: 6| Step: 4
Training loss: 2.084745407104492
Validation loss: 2.0790478189786277

Epoch: 6| Step: 5
Training loss: 2.3376502990722656
Validation loss: 2.0883579651514688

Epoch: 6| Step: 6
Training loss: 2.0831432342529297
Validation loss: 2.0853368043899536

Epoch: 6| Step: 7
Training loss: 1.44126296043396
Validation loss: 2.0864216685295105

Epoch: 6| Step: 8
Training loss: 2.37526798248291
Validation loss: 2.08164914449056

Epoch: 6| Step: 9
Training loss: 2.2352828979492188
Validation loss: 2.0823606053988137

Epoch: 6| Step: 10
Training loss: 2.4055395126342773
Validation loss: 2.079412281513214

Epoch: 6| Step: 11
Training loss: 2.1779472827911377
Validation loss: 2.0765389601389566

Epoch: 6| Step: 12
Training loss: 2.6002349853515625
Validation loss: 2.0794637401898703

Epoch: 6| Step: 13
Training loss: 2.3730616569519043
Validation loss: 2.080067217350006

Epoch: 62| Step: 0
Training loss: 2.404452323913574
Validation loss: 2.0749616424242654

Epoch: 6| Step: 1
Training loss: 2.4616026878356934
Validation loss: 2.07617857058843

Epoch: 6| Step: 2
Training loss: 2.1381258964538574
Validation loss: 2.073830803235372

Epoch: 6| Step: 3
Training loss: 2.309788942337036
Validation loss: 2.0732579231262207

Epoch: 6| Step: 4
Training loss: 2.0092406272888184
Validation loss: 2.0805745124816895

Epoch: 6| Step: 5
Training loss: 2.629835605621338
Validation loss: 2.0843804081281028

Epoch: 6| Step: 6
Training loss: 2.043637990951538
Validation loss: 2.0787238478660583

Epoch: 6| Step: 7
Training loss: 1.7788002490997314
Validation loss: 2.0794593691825867

Epoch: 6| Step: 8
Training loss: 1.9154293537139893
Validation loss: 2.0706339279810586

Epoch: 6| Step: 9
Training loss: 2.2487552165985107
Validation loss: 2.06721168756485

Epoch: 6| Step: 10
Training loss: 2.5628693103790283
Validation loss: 2.061394433180491

Epoch: 6| Step: 11
Training loss: 2.8286006450653076
Validation loss: 2.062135895093282

Epoch: 6| Step: 12
Training loss: 2.6614561080932617
Validation loss: 2.0712380409240723

Epoch: 6| Step: 13
Training loss: 1.4606456756591797
Validation loss: 2.0800994237264

Epoch: 63| Step: 0
Training loss: 2.5524559020996094
Validation loss: 2.0800394217173257

Epoch: 6| Step: 1
Training loss: 2.1288328170776367
Validation loss: 2.071081042289734

Epoch: 6| Step: 2
Training loss: 2.394822120666504
Validation loss: 2.0666999022165933

Epoch: 6| Step: 3
Training loss: 2.1809749603271484
Validation loss: 2.060671011606852

Epoch: 6| Step: 4
Training loss: 2.0750138759613037
Validation loss: 2.0713669657707214

Epoch: 6| Step: 5
Training loss: 2.4875969886779785
Validation loss: 2.078573703765869

Epoch: 6| Step: 6
Training loss: 2.6064295768737793
Validation loss: 2.064436415831248

Epoch: 6| Step: 7
Training loss: 2.06294584274292
Validation loss: 2.0540838837623596

Epoch: 6| Step: 8
Training loss: 2.1274538040161133
Validation loss: 2.047803541024526

Epoch: 6| Step: 9
Training loss: 1.8108675479888916
Validation loss: 2.047944704691569

Epoch: 6| Step: 10
Training loss: 1.7318884134292603
Validation loss: 2.0469518899917603

Epoch: 6| Step: 11
Training loss: 2.1907572746276855
Validation loss: 2.0506104230880737

Epoch: 6| Step: 12
Training loss: 2.610673189163208
Validation loss: 2.0479582150777182

Epoch: 6| Step: 13
Training loss: 2.3767173290252686
Validation loss: 2.0500882466634116

Epoch: 64| Step: 0
Training loss: 2.0530834197998047
Validation loss: 2.0445147355397544

Epoch: 6| Step: 1
Training loss: 2.8774890899658203
Validation loss: 2.0524133443832397

Epoch: 6| Step: 2
Training loss: 2.1819210052490234
Validation loss: 2.0517816742261252

Epoch: 6| Step: 3
Training loss: 1.7543398141860962
Validation loss: 2.049723466237386

Epoch: 6| Step: 4
Training loss: 2.1175003051757812
Validation loss: 2.0481337706247964

Epoch: 6| Step: 5
Training loss: 1.6552445888519287
Validation loss: 2.0643009742101035

Epoch: 6| Step: 6
Training loss: 2.408344268798828
Validation loss: 2.0677165389060974

Epoch: 6| Step: 7
Training loss: 2.180741310119629
Validation loss: 2.06099671125412

Epoch: 6| Step: 8
Training loss: 2.1786575317382812
Validation loss: 2.054622213045756

Epoch: 6| Step: 9
Training loss: 1.8783254623413086
Validation loss: 2.054150144259135

Epoch: 6| Step: 10
Training loss: 2.49626088142395
Validation loss: 2.0427329540252686

Epoch: 6| Step: 11
Training loss: 2.58321213722229
Validation loss: 2.0410165786743164

Epoch: 6| Step: 12
Training loss: 1.968951940536499
Validation loss: 2.058291792869568

Epoch: 6| Step: 13
Training loss: 2.8418335914611816
Validation loss: 2.0674537817637124

Epoch: 65| Step: 0
Training loss: 2.2456846237182617
Validation loss: 2.071709235509237

Epoch: 6| Step: 1
Training loss: 1.5607304573059082
Validation loss: 2.0786835153897605

Epoch: 6| Step: 2
Training loss: 2.5743165016174316
Validation loss: 2.0835154255231223

Epoch: 6| Step: 3
Training loss: 2.2792086601257324
Validation loss: 2.0787851015726724

Epoch: 6| Step: 4
Training loss: 2.1161110401153564
Validation loss: 2.0798980792363486

Epoch: 6| Step: 5
Training loss: 1.7112443447113037
Validation loss: 2.0738225976626077

Epoch: 6| Step: 6
Training loss: 2.5583927631378174
Validation loss: 2.072959582010905

Epoch: 6| Step: 7
Training loss: 2.6261680126190186
Validation loss: 2.0733821590741477

Epoch: 6| Step: 8
Training loss: 2.079742908477783
Validation loss: 2.0596720774968467

Epoch: 6| Step: 9
Training loss: 1.7858206033706665
Validation loss: 2.0639834801355996

Epoch: 6| Step: 10
Training loss: 3.1085658073425293
Validation loss: 2.0490230719248452

Epoch: 6| Step: 11
Training loss: 2.103168249130249
Validation loss: 2.0444977283477783

Epoch: 6| Step: 12
Training loss: 1.991562008857727
Validation loss: 2.036632776260376

Epoch: 6| Step: 13
Training loss: 2.679453134536743
Validation loss: 2.0381190180778503

Epoch: 66| Step: 0
Training loss: 2.0715246200561523
Validation loss: 2.032689650853475

Epoch: 6| Step: 1
Training loss: 2.367331027984619
Validation loss: 2.0349674423535666

Epoch: 6| Step: 2
Training loss: 2.2361221313476562
Validation loss: 2.0458050767580667

Epoch: 6| Step: 3
Training loss: 2.426636219024658
Validation loss: 2.0601619283358255

Epoch: 6| Step: 4
Training loss: 2.2960715293884277
Validation loss: 2.04125847419103

Epoch: 6| Step: 5
Training loss: 1.98822021484375
Validation loss: 2.041523814201355

Epoch: 6| Step: 6
Training loss: 2.4001522064208984
Validation loss: 2.0395056207974753

Epoch: 6| Step: 7
Training loss: 2.012036085128784
Validation loss: 2.0303183595339456

Epoch: 6| Step: 8
Training loss: 1.9006121158599854
Validation loss: 2.033900956312815

Epoch: 6| Step: 9
Training loss: 2.0774927139282227
Validation loss: 2.0321324666341147

Epoch: 6| Step: 10
Training loss: 1.924436330795288
Validation loss: 2.038269559542338

Epoch: 6| Step: 11
Training loss: 2.590010404586792
Validation loss: 2.0334384640057883

Epoch: 6| Step: 12
Training loss: 2.8377463817596436
Validation loss: 2.035625239213308

Epoch: 6| Step: 13
Training loss: 2.212160587310791
Validation loss: 2.0315924882888794

Epoch: 67| Step: 0
Training loss: 1.8175817728042603
Validation loss: 2.0313369830449424

Epoch: 6| Step: 1
Training loss: 3.284727096557617
Validation loss: 2.0326831142107644

Epoch: 6| Step: 2
Training loss: 1.9329479932785034
Validation loss: 2.030235012372335

Epoch: 6| Step: 3
Training loss: 2.112215995788574
Validation loss: 2.031294325987498

Epoch: 6| Step: 4
Training loss: 2.074479818344116
Validation loss: 2.0336081981658936

Epoch: 6| Step: 5
Training loss: 1.770882248878479
Validation loss: 2.0359145204226174

Epoch: 6| Step: 6
Training loss: 2.2043731212615967
Validation loss: 2.0339803099632263

Epoch: 6| Step: 7
Training loss: 2.0322651863098145
Validation loss: 2.0354542334874473

Epoch: 6| Step: 8
Training loss: 2.384594440460205
Validation loss: 2.0345121224721274

Epoch: 6| Step: 9
Training loss: 2.4028306007385254
Validation loss: 2.031050900618235

Epoch: 6| Step: 10
Training loss: 2.3708508014678955
Validation loss: 2.029419998327891

Epoch: 6| Step: 11
Training loss: 1.6073685884475708
Validation loss: 2.0360949635505676

Epoch: 6| Step: 12
Training loss: 2.3831024169921875
Validation loss: 2.048184812068939

Epoch: 6| Step: 13
Training loss: 2.631953716278076
Validation loss: 2.0570173462231955

Epoch: 68| Step: 0
Training loss: 1.9718663692474365
Validation loss: 2.0606961647669473

Epoch: 6| Step: 1
Training loss: 1.7522776126861572
Validation loss: 2.053617278734843

Epoch: 6| Step: 2
Training loss: 2.0335893630981445
Validation loss: 2.039726138114929

Epoch: 6| Step: 3
Training loss: 1.9194166660308838
Validation loss: 2.0412193735440574

Epoch: 6| Step: 4
Training loss: 1.7799601554870605
Validation loss: 2.029800554116567

Epoch: 6| Step: 5
Training loss: 2.5426950454711914
Validation loss: 2.0313591957092285

Epoch: 6| Step: 6
Training loss: 2.419870615005493
Validation loss: 2.0297503074010215

Epoch: 6| Step: 7
Training loss: 2.009418487548828
Validation loss: 2.0310386617978415

Epoch: 6| Step: 8
Training loss: 2.008023738861084
Validation loss: 2.0346696376800537

Epoch: 6| Step: 9
Training loss: 2.5609734058380127
Validation loss: 2.040946344534556

Epoch: 6| Step: 10
Training loss: 2.5091161727905273
Validation loss: 2.0540366967519126

Epoch: 6| Step: 11
Training loss: 2.998889923095703
Validation loss: 2.05415278673172

Epoch: 6| Step: 12
Training loss: 2.4924135208129883
Validation loss: 2.0334285497665405

Epoch: 6| Step: 13
Training loss: 1.7954862117767334
Validation loss: 2.03942608833313

Epoch: 69| Step: 0
Training loss: 1.6550242900848389
Validation loss: 2.0341756343841553

Epoch: 6| Step: 1
Training loss: 2.3546128273010254
Validation loss: 2.0275604923566184

Epoch: 6| Step: 2
Training loss: 2.036283493041992
Validation loss: 2.035627822081248

Epoch: 6| Step: 3
Training loss: 2.2490639686584473
Validation loss: 2.025622089703878

Epoch: 6| Step: 4
Training loss: 2.0650978088378906
Validation loss: 2.0261961023012796

Epoch: 6| Step: 5
Training loss: 1.8179047107696533
Validation loss: 2.0182772278785706

Epoch: 6| Step: 6
Training loss: 2.4366002082824707
Validation loss: 2.0186587969462075

Epoch: 6| Step: 7
Training loss: 2.6645236015319824
Validation loss: 2.013468404610952

Epoch: 6| Step: 8
Training loss: 2.726869583129883
Validation loss: 2.0166112383206687

Epoch: 6| Step: 9
Training loss: 2.2429356575012207
Validation loss: 2.0253028869628906

Epoch: 6| Step: 10
Training loss: 1.9316136837005615
Validation loss: 2.029301941394806

Epoch: 6| Step: 11
Training loss: 2.192676305770874
Validation loss: 2.022962510585785

Epoch: 6| Step: 12
Training loss: 2.279264450073242
Validation loss: 2.0245150526364646

Epoch: 6| Step: 13
Training loss: 2.0759410858154297
Validation loss: 2.0321096380551658

Epoch: 70| Step: 0
Training loss: 2.3207974433898926
Validation loss: 2.0372432669003806

Epoch: 6| Step: 1
Training loss: 2.3309695720672607
Validation loss: 2.0472062826156616

Epoch: 6| Step: 2
Training loss: 2.3211755752563477
Validation loss: 2.0468010306358337

Epoch: 6| Step: 3
Training loss: 2.708378791809082
Validation loss: 2.0534347891807556

Epoch: 6| Step: 4
Training loss: 1.6998034715652466
Validation loss: 2.0426114201545715

Epoch: 6| Step: 5
Training loss: 1.6965999603271484
Validation loss: 2.03525048494339

Epoch: 6| Step: 6
Training loss: 2.5078258514404297
Validation loss: 2.030918002128601

Epoch: 6| Step: 7
Training loss: 1.6934254169464111
Validation loss: 2.0291407108306885

Epoch: 6| Step: 8
Training loss: 2.0511229038238525
Validation loss: 2.02998294432958

Epoch: 6| Step: 9
Training loss: 2.2732338905334473
Validation loss: 2.027735392252604

Epoch: 6| Step: 10
Training loss: 2.832291603088379
Validation loss: 2.035255034764608

Epoch: 6| Step: 11
Training loss: 1.6763310432434082
Validation loss: 2.0253411332766214

Epoch: 6| Step: 12
Training loss: 2.373528480529785
Validation loss: 2.0193695227305093

Epoch: 6| Step: 13
Training loss: 2.1202118396759033
Validation loss: 2.0214515328407288

Epoch: 71| Step: 0
Training loss: 2.1517772674560547
Validation loss: 2.023144006729126

Epoch: 6| Step: 1
Training loss: 2.5281596183776855
Validation loss: 2.034788429737091

Epoch: 6| Step: 2
Training loss: 2.353999376296997
Validation loss: 2.035092055797577

Epoch: 6| Step: 3
Training loss: 2.306034564971924
Validation loss: 2.032778044541677

Epoch: 6| Step: 4
Training loss: 2.2134366035461426
Validation loss: 2.0246927539507547

Epoch: 6| Step: 5
Training loss: 1.5657408237457275
Validation loss: 2.0227531790733337

Epoch: 6| Step: 6
Training loss: 1.6840816736221313
Validation loss: 2.0255625247955322

Epoch: 6| Step: 7
Training loss: 2.2511157989501953
Validation loss: 2.0321913957595825

Epoch: 6| Step: 8
Training loss: 1.8482980728149414
Validation loss: 2.0321292877197266

Epoch: 6| Step: 9
Training loss: 2.469738483428955
Validation loss: 2.019977311293284

Epoch: 6| Step: 10
Training loss: 1.8039672374725342
Validation loss: 2.024668117364248

Epoch: 6| Step: 11
Training loss: 2.475832223892212
Validation loss: 2.0327195723851523

Epoch: 6| Step: 12
Training loss: 2.784167766571045
Validation loss: 2.041227082411448

Epoch: 6| Step: 13
Training loss: 2.4061737060546875
Validation loss: 2.0288859407107034

Epoch: 72| Step: 0
Training loss: 1.7315927743911743
Validation loss: 2.024102032184601

Epoch: 6| Step: 1
Training loss: 1.621629238128662
Validation loss: 2.022675355275472

Epoch: 6| Step: 2
Training loss: 2.3978543281555176
Validation loss: 2.0327709913253784

Epoch: 6| Step: 3
Training loss: 1.89519464969635
Validation loss: 2.025817394256592

Epoch: 6| Step: 4
Training loss: 2.5696828365325928
Validation loss: 2.03023354212443

Epoch: 6| Step: 5
Training loss: 2.262049913406372
Validation loss: 2.0357122222582498

Epoch: 6| Step: 6
Training loss: 2.706716537475586
Validation loss: 2.0225237806638083

Epoch: 6| Step: 7
Training loss: 1.8653488159179688
Validation loss: 2.0214344461758933

Epoch: 6| Step: 8
Training loss: 2.7710626125335693
Validation loss: 2.026745617389679

Epoch: 6| Step: 9
Training loss: 2.134873390197754
Validation loss: 2.021016776561737

Epoch: 6| Step: 10
Training loss: 2.2822577953338623
Validation loss: 2.030471920967102

Epoch: 6| Step: 11
Training loss: 2.571340799331665
Validation loss: 2.0259371201197305

Epoch: 6| Step: 12
Training loss: 2.2629318237304688
Validation loss: 2.0292430321375527

Epoch: 6| Step: 13
Training loss: 1.6595618724822998
Validation loss: 2.024530271689097

Epoch: 73| Step: 0
Training loss: 2.4047598838806152
Validation loss: 2.031895081202189

Epoch: 6| Step: 1
Training loss: 2.238097667694092
Validation loss: 2.0253275632858276

Epoch: 6| Step: 2
Training loss: 1.9290426969528198
Validation loss: 2.0314923326174417

Epoch: 6| Step: 3
Training loss: 2.6618335247039795
Validation loss: 2.032909095287323

Epoch: 6| Step: 4
Training loss: 2.522815465927124
Validation loss: 2.027491867542267

Epoch: 6| Step: 5
Training loss: 1.960329294204712
Validation loss: 2.0338066021601358

Epoch: 6| Step: 6
Training loss: 2.143752098083496
Validation loss: 2.027003745237986

Epoch: 6| Step: 7
Training loss: 1.7386629581451416
Validation loss: 2.02671355009079

Epoch: 6| Step: 8
Training loss: 2.5516116619110107
Validation loss: 2.030048449834188

Epoch: 6| Step: 9
Training loss: 2.4887142181396484
Validation loss: 2.039508104324341

Epoch: 6| Step: 10
Training loss: 2.1247735023498535
Validation loss: 2.049510359764099

Epoch: 6| Step: 11
Training loss: 2.199134349822998
Validation loss: 2.0588517983754477

Epoch: 6| Step: 12
Training loss: 1.457024335861206
Validation loss: 2.0548969507217407

Epoch: 6| Step: 13
Training loss: 1.944350004196167
Validation loss: 2.0355963110923767

Epoch: 74| Step: 0
Training loss: 1.5315481424331665
Validation loss: 2.0386086901028952

Epoch: 6| Step: 1
Training loss: 2.405592918395996
Validation loss: 2.0405415097872415

Epoch: 6| Step: 2
Training loss: 2.1018247604370117
Validation loss: 2.021845579147339

Epoch: 6| Step: 3
Training loss: 1.8371944427490234
Validation loss: 2.0290621320406594

Epoch: 6| Step: 4
Training loss: 2.8039660453796387
Validation loss: 2.016275485356649

Epoch: 6| Step: 5
Training loss: 2.4441885948181152
Validation loss: 2.0222569505373635

Epoch: 6| Step: 6
Training loss: 2.430692672729492
Validation loss: 2.0282638669013977

Epoch: 6| Step: 7
Training loss: 2.159536123275757
Validation loss: 2.028629243373871

Epoch: 6| Step: 8
Training loss: 2.1613283157348633
Validation loss: 2.034537136554718

Epoch: 6| Step: 9
Training loss: 2.037245273590088
Validation loss: 2.0385109980901084

Epoch: 6| Step: 10
Training loss: 2.389956474304199
Validation loss: 2.030475298563639

Epoch: 6| Step: 11
Training loss: 1.91786789894104
Validation loss: 2.0343194802602134

Epoch: 6| Step: 12
Training loss: 1.9071424007415771
Validation loss: 2.035294532775879

Epoch: 6| Step: 13
Training loss: 2.431960344314575
Validation loss: 2.027494708697001

Epoch: 75| Step: 0
Training loss: 2.0549235343933105
Validation loss: 2.023437043031057

Epoch: 6| Step: 1
Training loss: 1.9168107509613037
Validation loss: 2.042124847571055

Epoch: 6| Step: 2
Training loss: 1.8682862520217896
Validation loss: 2.0542455315589905

Epoch: 6| Step: 3
Training loss: 2.4251163005828857
Validation loss: 2.0488873521486917

Epoch: 6| Step: 4
Training loss: 2.3358826637268066
Validation loss: 2.0624232292175293

Epoch: 6| Step: 5
Training loss: 2.002394199371338
Validation loss: 2.0699326395988464

Epoch: 6| Step: 6
Training loss: 2.506822109222412
Validation loss: 2.044519583384196

Epoch: 6| Step: 7
Training loss: 2.030376434326172
Validation loss: 2.0339869260787964

Epoch: 6| Step: 8
Training loss: 2.5028843879699707
Validation loss: 2.0380093256632485

Epoch: 6| Step: 9
Training loss: 2.234895706176758
Validation loss: 2.034853676954905

Epoch: 6| Step: 10
Training loss: 2.044501781463623
Validation loss: 2.038152833779653

Epoch: 6| Step: 11
Training loss: 2.4631729125976562
Validation loss: 2.0333693424860635

Epoch: 6| Step: 12
Training loss: 2.217127561569214
Validation loss: 2.0476536949475608

Epoch: 6| Step: 13
Training loss: 1.8961377143859863
Validation loss: 2.0501268108685813

Epoch: 76| Step: 0
Training loss: 2.1516494750976562
Validation loss: 2.0479401449362435

Epoch: 6| Step: 1
Training loss: 2.233069658279419
Validation loss: 2.040887951850891

Epoch: 6| Step: 2
Training loss: 2.384706497192383
Validation loss: 2.0501474936803183

Epoch: 6| Step: 3
Training loss: 2.2702953815460205
Validation loss: 2.053488572438558

Epoch: 6| Step: 4
Training loss: 1.9196157455444336
Validation loss: 2.0499210556348166

Epoch: 6| Step: 5
Training loss: 2.129931926727295
Validation loss: 2.041477084159851

Epoch: 6| Step: 6
Training loss: 2.593609094619751
Validation loss: 2.044452687104543

Epoch: 6| Step: 7
Training loss: 2.35867977142334
Validation loss: 2.0375834902127585

Epoch: 6| Step: 8
Training loss: 1.8974987268447876
Validation loss: 2.0343608260154724

Epoch: 6| Step: 9
Training loss: 2.1250696182250977
Validation loss: 2.0281740029652915

Epoch: 6| Step: 10
Training loss: 1.7988438606262207
Validation loss: 2.024146854877472

Epoch: 6| Step: 11
Training loss: 2.0412511825561523
Validation loss: 2.0187028845151267

Epoch: 6| Step: 12
Training loss: 1.9695649147033691
Validation loss: 2.0122910936673484

Epoch: 6| Step: 13
Training loss: 3.1294426918029785
Validation loss: 2.023222327232361

Epoch: 77| Step: 0
Training loss: 2.675842761993408
Validation loss: 2.0350520809491477

Epoch: 6| Step: 1
Training loss: 1.5654712915420532
Validation loss: 2.0372339288393655

Epoch: 6| Step: 2
Training loss: 2.407057285308838
Validation loss: 2.053445279598236

Epoch: 6| Step: 3
Training loss: 2.198197841644287
Validation loss: 2.0569827556610107

Epoch: 6| Step: 4
Training loss: 2.0744948387145996
Validation loss: 2.043389538923899

Epoch: 6| Step: 5
Training loss: 1.7837393283843994
Validation loss: 2.0537462631861367

Epoch: 6| Step: 6
Training loss: 2.5530335903167725
Validation loss: 2.042170306046804

Epoch: 6| Step: 7
Training loss: 2.2901387214660645
Validation loss: 2.058160205682119

Epoch: 6| Step: 8
Training loss: 2.073056697845459
Validation loss: 2.03773832321167

Epoch: 6| Step: 9
Training loss: 1.589678168296814
Validation loss: 2.040867288907369

Epoch: 6| Step: 10
Training loss: 2.140592098236084
Validation loss: 2.038469831148783

Epoch: 6| Step: 11
Training loss: 2.632223129272461
Validation loss: 2.0394516785939536

Epoch: 6| Step: 12
Training loss: 1.9302220344543457
Validation loss: 2.0253954927126565

Epoch: 6| Step: 13
Training loss: 2.4295198917388916
Validation loss: 2.0178162256876626

Epoch: 78| Step: 0
Training loss: 1.7630202770233154
Validation loss: 2.019346018632253

Epoch: 6| Step: 1
Training loss: 2.0575060844421387
Validation loss: 2.014862378438314

Epoch: 6| Step: 2
Training loss: 2.235436201095581
Validation loss: 2.0308854977289834

Epoch: 6| Step: 3
Training loss: 1.7001290321350098
Validation loss: 2.0321033795674643

Epoch: 6| Step: 4
Training loss: 2.149850368499756
Validation loss: 2.019142826398214

Epoch: 6| Step: 5
Training loss: 2.9435982704162598
Validation loss: 2.0436227917671204

Epoch: 6| Step: 6
Training loss: 1.7099158763885498
Validation loss: 2.03334371248881

Epoch: 6| Step: 7
Training loss: 2.2517471313476562
Validation loss: 2.032042602698008

Epoch: 6| Step: 8
Training loss: 2.1099026203155518
Validation loss: 2.028931717077891

Epoch: 6| Step: 9
Training loss: 2.4203414916992188
Validation loss: 2.0231390595436096

Epoch: 6| Step: 10
Training loss: 2.4741687774658203
Validation loss: 2.0181026260058084

Epoch: 6| Step: 11
Training loss: 2.5201523303985596
Validation loss: 2.0143465797106423

Epoch: 6| Step: 12
Training loss: 1.9524171352386475
Validation loss: 2.014537831147512

Epoch: 6| Step: 13
Training loss: 1.918796181678772
Validation loss: 2.0248208045959473

Epoch: 79| Step: 0
Training loss: 1.886573314666748
Validation loss: 2.002664089202881

Epoch: 6| Step: 1
Training loss: 2.839487075805664
Validation loss: 2.023351013660431

Epoch: 6| Step: 2
Training loss: 2.064995765686035
Validation loss: 2.019623319307963

Epoch: 6| Step: 3
Training loss: 2.4236133098602295
Validation loss: 2.027576208114624

Epoch: 6| Step: 4
Training loss: 2.2812609672546387
Validation loss: 2.0291948318481445

Epoch: 6| Step: 5
Training loss: 2.015164852142334
Validation loss: 2.0443596243858337

Epoch: 6| Step: 6
Training loss: 1.6020382642745972
Validation loss: 2.045724312464396

Epoch: 6| Step: 7
Training loss: 1.3048561811447144
Validation loss: 2.057745575904846

Epoch: 6| Step: 8
Training loss: 2.1006407737731934
Validation loss: 2.045486032962799

Epoch: 6| Step: 9
Training loss: 2.521233558654785
Validation loss: 2.045499801635742

Epoch: 6| Step: 10
Training loss: 2.28953218460083
Validation loss: 2.01768030722936

Epoch: 6| Step: 11
Training loss: 2.1458864212036133
Validation loss: 2.014267603556315

Epoch: 6| Step: 12
Training loss: 2.5583136081695557
Validation loss: 2.00451789299647

Epoch: 6| Step: 13
Training loss: 2.3982272148132324
Validation loss: 2.0251194636027017

Epoch: 80| Step: 0
Training loss: 2.448458433151245
Validation loss: 2.0234667460123696

Epoch: 6| Step: 1
Training loss: 2.3347671031951904
Validation loss: 2.0228633284568787

Epoch: 6| Step: 2
Training loss: 1.9293546676635742
Validation loss: 2.03572686513265

Epoch: 6| Step: 3
Training loss: 2.206134796142578
Validation loss: 2.0271790424982705

Epoch: 6| Step: 4
Training loss: 1.9682564735412598
Validation loss: 2.035268187522888

Epoch: 6| Step: 5
Training loss: 2.4673075675964355
Validation loss: 2.031378964583079

Epoch: 6| Step: 6
Training loss: 2.4804372787475586
Validation loss: 2.0356765588124595

Epoch: 6| Step: 7
Training loss: 2.4722585678100586
Validation loss: 2.037722190221151

Epoch: 6| Step: 8
Training loss: 2.0627803802490234
Validation loss: 2.031939208507538

Epoch: 6| Step: 9
Training loss: 1.5940186977386475
Validation loss: 2.0300021171569824

Epoch: 6| Step: 10
Training loss: 1.8550111055374146
Validation loss: 2.03055610259374

Epoch: 6| Step: 11
Training loss: 2.1968398094177246
Validation loss: 2.0201966961224875

Epoch: 6| Step: 12
Training loss: 2.072885036468506
Validation loss: 2.0176395575205484

Epoch: 6| Step: 13
Training loss: 2.4924731254577637
Validation loss: 2.01087095340093

Epoch: 81| Step: 0
Training loss: 1.6736156940460205
Validation loss: 2.0001374085744223

Epoch: 6| Step: 1
Training loss: 2.1520180702209473
Validation loss: 2.008199632167816

Epoch: 6| Step: 2
Training loss: 1.7133209705352783
Validation loss: 2.0107512871424356

Epoch: 6| Step: 3
Training loss: 2.5989465713500977
Validation loss: 2.028786520163218

Epoch: 6| Step: 4
Training loss: 2.2421212196350098
Validation loss: 2.0279020071029663

Epoch: 6| Step: 5
Training loss: 1.7002787590026855
Validation loss: 2.039518435796102

Epoch: 6| Step: 6
Training loss: 2.3420801162719727
Validation loss: 2.0409109393755593

Epoch: 6| Step: 7
Training loss: 1.6425535678863525
Validation loss: 2.0624155004819236

Epoch: 6| Step: 8
Training loss: 2.315847873687744
Validation loss: 2.062627593676249

Epoch: 6| Step: 9
Training loss: 2.533209800720215
Validation loss: 2.052146792411804

Epoch: 6| Step: 10
Training loss: 2.142518997192383
Validation loss: 2.049510975678762

Epoch: 6| Step: 11
Training loss: 2.5413150787353516
Validation loss: 2.0517035325368247

Epoch: 6| Step: 12
Training loss: 1.9258604049682617
Validation loss: 2.0421663721402488

Epoch: 6| Step: 13
Training loss: 2.851646900177002
Validation loss: 2.0319135387738547

Epoch: 82| Step: 0
Training loss: 2.8411574363708496
Validation loss: 2.0234712759653726

Epoch: 6| Step: 1
Training loss: 1.9606400728225708
Validation loss: 2.018280486265818

Epoch: 6| Step: 2
Training loss: 2.368940591812134
Validation loss: 2.01605353752772

Epoch: 6| Step: 3
Training loss: 2.2984459400177
Validation loss: 2.0234023133913674

Epoch: 6| Step: 4
Training loss: 2.2292115688323975
Validation loss: 2.020316561063131

Epoch: 6| Step: 5
Training loss: 1.5445408821105957
Validation loss: 2.0277193784713745

Epoch: 6| Step: 6
Training loss: 2.604151487350464
Validation loss: 2.03251705567042

Epoch: 6| Step: 7
Training loss: 1.808597445487976
Validation loss: 2.0252909064292908

Epoch: 6| Step: 8
Training loss: 2.577442169189453
Validation loss: 2.028306782245636

Epoch: 6| Step: 9
Training loss: 2.109344482421875
Validation loss: 2.0205755829811096

Epoch: 6| Step: 10
Training loss: 2.8072192668914795
Validation loss: 2.017125348250071

Epoch: 6| Step: 11
Training loss: 1.4865556955337524
Validation loss: 2.0221338073412576

Epoch: 6| Step: 12
Training loss: 2.026867389678955
Validation loss: 2.0277114311854043

Epoch: 6| Step: 13
Training loss: 1.986136555671692
Validation loss: 2.024894197781881

Epoch: 83| Step: 0
Training loss: 2.333620548248291
Validation loss: 2.0250762701034546

Epoch: 6| Step: 1
Training loss: 2.6477279663085938
Validation loss: 2.0156004826227822

Epoch: 6| Step: 2
Training loss: 2.4001243114471436
Validation loss: 2.021952450275421

Epoch: 6| Step: 3
Training loss: 2.3352932929992676
Validation loss: 2.021609286467234

Epoch: 6| Step: 4
Training loss: 1.9065240621566772
Validation loss: 2.0175721844037375

Epoch: 6| Step: 5
Training loss: 2.2128231525421143
Validation loss: 2.025455892086029

Epoch: 6| Step: 6
Training loss: 2.3436875343322754
Validation loss: 2.0193692247072854

Epoch: 6| Step: 7
Training loss: 1.790719985961914
Validation loss: 2.0294907689094543

Epoch: 6| Step: 8
Training loss: 2.0272490978240967
Validation loss: 2.02605531613032

Epoch: 6| Step: 9
Training loss: 1.805269479751587
Validation loss: 2.0163139502207437

Epoch: 6| Step: 10
Training loss: 1.679978847503662
Validation loss: 2.0257662733395896

Epoch: 6| Step: 11
Training loss: 1.92230224609375
Validation loss: 2.0393352707227073

Epoch: 6| Step: 12
Training loss: 2.821171522140503
Validation loss: 2.033941845099131

Epoch: 6| Step: 13
Training loss: 1.925515055656433
Validation loss: 2.056274930636088

Epoch: 84| Step: 0
Training loss: 2.2327446937561035
Validation loss: 2.0715107321739197

Epoch: 6| Step: 1
Training loss: 2.0823042392730713
Validation loss: 2.088843524456024

Epoch: 6| Step: 2
Training loss: 1.7571492195129395
Validation loss: 2.080954829851786

Epoch: 6| Step: 3
Training loss: 2.463071823120117
Validation loss: 2.0965053836504617

Epoch: 6| Step: 4
Training loss: 2.0112881660461426
Validation loss: 2.0945454239845276

Epoch: 6| Step: 5
Training loss: 1.8626680374145508
Validation loss: 2.0723193486531577

Epoch: 6| Step: 6
Training loss: 2.2324790954589844
Validation loss: 2.062584718068441

Epoch: 6| Step: 7
Training loss: 1.992549180984497
Validation loss: 2.021806240081787

Epoch: 6| Step: 8
Training loss: 2.5079824924468994
Validation loss: 2.024687707424164

Epoch: 6| Step: 9
Training loss: 1.8480374813079834
Validation loss: 2.0207581718762717

Epoch: 6| Step: 10
Training loss: 2.0573596954345703
Validation loss: 2.016151269276937

Epoch: 6| Step: 11
Training loss: 2.5217087268829346
Validation loss: 2.023281196753184

Epoch: 6| Step: 12
Training loss: 2.4749391078948975
Validation loss: 2.0254851579666138

Epoch: 6| Step: 13
Training loss: 2.323239326477051
Validation loss: 2.0237016081809998

Epoch: 85| Step: 0
Training loss: 1.6399552822113037
Validation loss: 2.0285441080729165

Epoch: 6| Step: 1
Training loss: 2.1361823081970215
Validation loss: 2.0157408316930137

Epoch: 6| Step: 2
Training loss: 2.5937716960906982
Validation loss: 2.0234200159708657

Epoch: 6| Step: 3
Training loss: 2.151062488555908
Validation loss: 2.030061960220337

Epoch: 6| Step: 4
Training loss: 3.0493040084838867
Validation loss: 2.0185267329216003

Epoch: 6| Step: 5
Training loss: 2.072608470916748
Validation loss: 2.0219576557477317

Epoch: 6| Step: 6
Training loss: 1.870650291442871
Validation loss: 2.0252270499865213

Epoch: 6| Step: 7
Training loss: 1.9021055698394775
Validation loss: 2.019785304864248

Epoch: 6| Step: 8
Training loss: 1.9806326627731323
Validation loss: 2.0260958075523376

Epoch: 6| Step: 9
Training loss: 2.3917717933654785
Validation loss: 2.0188368956247964

Epoch: 6| Step: 10
Training loss: 1.9600642919540405
Validation loss: 2.0252007643381753

Epoch: 6| Step: 11
Training loss: 1.8976240158081055
Validation loss: 2.0192575653394065

Epoch: 6| Step: 12
Training loss: 2.2012782096862793
Validation loss: 2.029792368412018

Epoch: 6| Step: 13
Training loss: 2.334646224975586
Validation loss: 2.039372503757477

Epoch: 86| Step: 0
Training loss: 2.462554931640625
Validation loss: 2.042475700378418

Epoch: 6| Step: 1
Training loss: 2.458758592605591
Validation loss: 2.053120950857798

Epoch: 6| Step: 2
Training loss: 2.5948615074157715
Validation loss: 2.051350990931193

Epoch: 6| Step: 3
Training loss: 1.8747323751449585
Validation loss: 2.0519712964693704

Epoch: 6| Step: 4
Training loss: 1.3177859783172607
Validation loss: 2.0459948579470315

Epoch: 6| Step: 5
Training loss: 1.8904926776885986
Validation loss: 2.0512662331263223

Epoch: 6| Step: 6
Training loss: 2.2257847785949707
Validation loss: 2.0507448514302573

Epoch: 6| Step: 7
Training loss: 2.403937816619873
Validation loss: 2.053841531276703

Epoch: 6| Step: 8
Training loss: 2.036851167678833
Validation loss: 2.041426738103231

Epoch: 6| Step: 9
Training loss: 2.6183414459228516
Validation loss: 2.0449469089508057

Epoch: 6| Step: 10
Training loss: 2.0293984413146973
Validation loss: 2.0470047990481057

Epoch: 6| Step: 11
Training loss: 2.260896682739258
Validation loss: 2.0487798055013022

Epoch: 6| Step: 12
Training loss: 1.411345362663269
Validation loss: 2.031774580478668

Epoch: 6| Step: 13
Training loss: 2.415956974029541
Validation loss: 2.016282339890798

Epoch: 87| Step: 0
Training loss: 2.060039520263672
Validation loss: 2.010990560054779

Epoch: 6| Step: 1
Training loss: 1.3919897079467773
Validation loss: 2.0082515875498452

Epoch: 6| Step: 2
Training loss: 2.051851272583008
Validation loss: 2.0138582785924277

Epoch: 6| Step: 3
Training loss: 2.4740703105926514
Validation loss: 2.008419672648112

Epoch: 6| Step: 4
Training loss: 2.0796031951904297
Validation loss: 2.0176804264386496

Epoch: 6| Step: 5
Training loss: 1.755448818206787
Validation loss: 2.0026623209317527

Epoch: 6| Step: 6
Training loss: 2.526984691619873
Validation loss: 2.0145761569341025

Epoch: 6| Step: 7
Training loss: 2.4608445167541504
Validation loss: 2.0198248823483786

Epoch: 6| Step: 8
Training loss: 2.070188283920288
Validation loss: 2.0244162877400718

Epoch: 6| Step: 9
Training loss: 1.9416139125823975
Validation loss: 2.0374351143836975

Epoch: 6| Step: 10
Training loss: 2.1785197257995605
Validation loss: 2.0293290615081787

Epoch: 6| Step: 11
Training loss: 2.464250087738037
Validation loss: 2.0472271045049033

Epoch: 6| Step: 12
Training loss: 2.193570375442505
Validation loss: 2.045973539352417

Epoch: 6| Step: 13
Training loss: 2.513195276260376
Validation loss: 2.030543267726898

Epoch: 88| Step: 0
Training loss: 2.2066760063171387
Validation loss: 2.0225822925567627

Epoch: 6| Step: 1
Training loss: 1.8695504665374756
Validation loss: 2.028206547101339

Epoch: 6| Step: 2
Training loss: 1.5298070907592773
Validation loss: 2.0212761958440146

Epoch: 6| Step: 3
Training loss: 1.9500292539596558
Validation loss: 2.019936958948771

Epoch: 6| Step: 4
Training loss: 2.3900091648101807
Validation loss: 2.0090622305870056

Epoch: 6| Step: 5
Training loss: 2.111133575439453
Validation loss: 2.0172436237335205

Epoch: 6| Step: 6
Training loss: 2.1167571544647217
Validation loss: 2.019910236199697

Epoch: 6| Step: 7
Training loss: 2.3425865173339844
Validation loss: 2.011821130911509

Epoch: 6| Step: 8
Training loss: 1.7764161825180054
Validation loss: 2.013823648293813

Epoch: 6| Step: 9
Training loss: 2.4461278915405273
Validation loss: 2.0227643052736917

Epoch: 6| Step: 10
Training loss: 2.5728049278259277
Validation loss: 2.02343620856603

Epoch: 6| Step: 11
Training loss: 2.5192840099334717
Validation loss: 2.028928260008494

Epoch: 6| Step: 12
Training loss: 1.8477163314819336
Validation loss: 2.0224117239316306

Epoch: 6| Step: 13
Training loss: 2.218205451965332
Validation loss: 2.0288912256558738

Epoch: 89| Step: 0
Training loss: 1.7163898944854736
Validation loss: 2.0398115515708923

Epoch: 6| Step: 1
Training loss: 1.967291235923767
Validation loss: 2.04191255569458

Epoch: 6| Step: 2
Training loss: 2.250941514968872
Validation loss: 2.060135463873545

Epoch: 6| Step: 3
Training loss: 2.3083863258361816
Validation loss: 2.0623307625452676

Epoch: 6| Step: 4
Training loss: 1.8124101161956787
Validation loss: 2.068926751613617

Epoch: 6| Step: 5
Training loss: 1.7188953161239624
Validation loss: 2.0538450678189597

Epoch: 6| Step: 6
Training loss: 2.059295415878296
Validation loss: 2.05399888753891

Epoch: 6| Step: 7
Training loss: 1.9103883504867554
Validation loss: 2.084566613038381

Epoch: 6| Step: 8
Training loss: 2.104504108428955
Validation loss: 2.0617589354515076

Epoch: 6| Step: 9
Training loss: 2.1558432579040527
Validation loss: 2.059944669405619

Epoch: 6| Step: 10
Training loss: 2.6168336868286133
Validation loss: 2.0530805389086404

Epoch: 6| Step: 11
Training loss: 2.4624669551849365
Validation loss: 2.042572498321533

Epoch: 6| Step: 12
Training loss: 2.5202670097351074
Validation loss: 2.026120960712433

Epoch: 6| Step: 13
Training loss: 2.222139835357666
Validation loss: 2.013785978158315

Epoch: 90| Step: 0
Training loss: 2.4356865882873535
Validation loss: 2.027583340803782

Epoch: 6| Step: 1
Training loss: 2.1103134155273438
Validation loss: 2.0316883524258933

Epoch: 6| Step: 2
Training loss: 2.061288356781006
Validation loss: 2.0368561347325644

Epoch: 6| Step: 3
Training loss: 2.312098979949951
Validation loss: 2.042303820451101

Epoch: 6| Step: 4
Training loss: 1.7823638916015625
Validation loss: 2.042425751686096

Epoch: 6| Step: 5
Training loss: 2.603166341781616
Validation loss: 2.0435058077176413

Epoch: 6| Step: 6
Training loss: 2.4645934104919434
Validation loss: 2.0440776546796164

Epoch: 6| Step: 7
Training loss: 1.3951224088668823
Validation loss: 2.0473936994870505

Epoch: 6| Step: 8
Training loss: 2.476642608642578
Validation loss: 2.0465457439422607

Epoch: 6| Step: 9
Training loss: 2.4195494651794434
Validation loss: 2.0446468393007913

Epoch: 6| Step: 10
Training loss: 1.8376274108886719
Validation loss: 2.042280356089274

Epoch: 6| Step: 11
Training loss: 2.0621659755706787
Validation loss: 2.0455485184987388

Epoch: 6| Step: 12
Training loss: 2.646486282348633
Validation loss: 2.039807677268982

Epoch: 6| Step: 13
Training loss: 2.0728657245635986
Validation loss: 2.0470808148384094

Epoch: 91| Step: 0
Training loss: 1.5544862747192383
Validation loss: 2.045150399208069

Epoch: 6| Step: 1
Training loss: 1.6032315492630005
Validation loss: 2.041423817475637

Epoch: 6| Step: 2
Training loss: 2.6036081314086914
Validation loss: 2.045215884844462

Epoch: 6| Step: 3
Training loss: 2.349029541015625
Validation loss: 2.0425632198651633

Epoch: 6| Step: 4
Training loss: 1.6143691539764404
Validation loss: 2.0425209204355874

Epoch: 6| Step: 5
Training loss: 2.1241049766540527
Validation loss: 2.0328574577967324

Epoch: 6| Step: 6
Training loss: 2.353729724884033
Validation loss: 2.033571799596151

Epoch: 6| Step: 7
Training loss: 2.360151767730713
Validation loss: 2.0263489882151284

Epoch: 6| Step: 8
Training loss: 2.2250702381134033
Validation loss: 2.024132251739502

Epoch: 6| Step: 9
Training loss: 2.5379815101623535
Validation loss: 2.020346919695536

Epoch: 6| Step: 10
Training loss: 2.680753707885742
Validation loss: 2.022851546605428

Epoch: 6| Step: 11
Training loss: 1.922460913658142
Validation loss: 2.033448259035746

Epoch: 6| Step: 12
Training loss: 1.823147177696228
Validation loss: 2.04187548160553

Epoch: 6| Step: 13
Training loss: 2.336698055267334
Validation loss: 2.0535546938578286

Epoch: 92| Step: 0
Training loss: 2.4487099647521973
Validation loss: 2.0511393547058105

Epoch: 6| Step: 1
Training loss: 2.286304473876953
Validation loss: 2.057872215906779

Epoch: 6| Step: 2
Training loss: 2.235541343688965
Validation loss: 2.0466995437939963

Epoch: 6| Step: 3
Training loss: 2.3428378105163574
Validation loss: 2.069291571776072

Epoch: 6| Step: 4
Training loss: 1.7180590629577637
Validation loss: 2.058863421281179

Epoch: 6| Step: 5
Training loss: 1.951002597808838
Validation loss: 2.072412073612213

Epoch: 6| Step: 6
Training loss: 2.300964832305908
Validation loss: 2.0733052690823874

Epoch: 6| Step: 7
Training loss: 1.7055764198303223
Validation loss: 2.0835400223731995

Epoch: 6| Step: 8
Training loss: 2.01534366607666
Validation loss: 2.068089187145233

Epoch: 6| Step: 9
Training loss: 1.3988763093948364
Validation loss: 2.0549007256825766

Epoch: 6| Step: 10
Training loss: 2.2742295265197754
Validation loss: 2.049983183542887

Epoch: 6| Step: 11
Training loss: 2.4285073280334473
Validation loss: 2.0630255142847695

Epoch: 6| Step: 12
Training loss: 2.507340431213379
Validation loss: 2.036563456058502

Epoch: 6| Step: 13
Training loss: 2.2783989906311035
Validation loss: 2.044205069541931

Epoch: 93| Step: 0
Training loss: 1.6689373254776
Validation loss: 2.0335112611452737

Epoch: 6| Step: 1
Training loss: 1.540876865386963
Validation loss: 2.022730509440104

Epoch: 6| Step: 2
Training loss: 1.9030914306640625
Validation loss: 2.025128205617269

Epoch: 6| Step: 3
Training loss: 2.179224729537964
Validation loss: 2.0281536976496377

Epoch: 6| Step: 4
Training loss: 2.6892781257629395
Validation loss: 2.0263830622037253

Epoch: 6| Step: 5
Training loss: 2.234494686126709
Validation loss: 2.0171244939168296

Epoch: 6| Step: 6
Training loss: 1.8201611042022705
Validation loss: 2.012735605239868

Epoch: 6| Step: 7
Training loss: 2.301464557647705
Validation loss: 2.0070950587590537

Epoch: 6| Step: 8
Training loss: 1.7433574199676514
Validation loss: 2.0074775218963623

Epoch: 6| Step: 9
Training loss: 2.175489664077759
Validation loss: 2.009981373945872

Epoch: 6| Step: 10
Training loss: 2.5807154178619385
Validation loss: 2.014676113923391

Epoch: 6| Step: 11
Training loss: 2.652398109436035
Validation loss: 2.005421817302704

Epoch: 6| Step: 12
Training loss: 1.8015494346618652
Validation loss: 2.016148885091146

Epoch: 6| Step: 13
Training loss: 2.8628878593444824
Validation loss: 2.020936608314514

Epoch: 94| Step: 0
Training loss: 2.2588050365448
Validation loss: 2.0122952858606973

Epoch: 6| Step: 1
Training loss: 2.326667308807373
Validation loss: 2.010676622390747

Epoch: 6| Step: 2
Training loss: 2.0208096504211426
Validation loss: 2.0151370763778687

Epoch: 6| Step: 3
Training loss: 2.8328351974487305
Validation loss: 2.015316347281138

Epoch: 6| Step: 4
Training loss: 2.0828640460968018
Validation loss: 2.0235374172528586

Epoch: 6| Step: 5
Training loss: 2.219480514526367
Validation loss: 2.023232400417328

Epoch: 6| Step: 6
Training loss: 1.9974405765533447
Validation loss: 2.031230866909027

Epoch: 6| Step: 7
Training loss: 1.7470111846923828
Validation loss: 2.0244248708089194

Epoch: 6| Step: 8
Training loss: 2.6698741912841797
Validation loss: 2.032000104586283

Epoch: 6| Step: 9
Training loss: 1.8315260410308838
Validation loss: 2.0269649624824524

Epoch: 6| Step: 10
Training loss: 1.7678413391113281
Validation loss: 2.021658639113108

Epoch: 6| Step: 11
Training loss: 2.1916627883911133
Validation loss: 2.037863870461782

Epoch: 6| Step: 12
Training loss: 1.8449605703353882
Validation loss: 2.0362565517425537

Epoch: 6| Step: 13
Training loss: 2.196470022201538
Validation loss: 2.0420971512794495

Epoch: 95| Step: 0
Training loss: 2.2061147689819336
Validation loss: 2.0460540056228638

Epoch: 6| Step: 1
Training loss: 2.057283401489258
Validation loss: 2.0327155391375222

Epoch: 6| Step: 2
Training loss: 1.927309513092041
Validation loss: 2.0354615251223245

Epoch: 6| Step: 3
Training loss: 1.626265287399292
Validation loss: 2.0394575198491416

Epoch: 6| Step: 4
Training loss: 1.714145541191101
Validation loss: 2.0435067415237427

Epoch: 6| Step: 5
Training loss: 2.027522563934326
Validation loss: 2.0436187982559204

Epoch: 6| Step: 6
Training loss: 2.0526397228240967
Validation loss: 2.045236865679423

Epoch: 6| Step: 7
Training loss: 2.5711944103240967
Validation loss: 2.0387361447016397

Epoch: 6| Step: 8
Training loss: 2.221712112426758
Validation loss: 2.0408234198888144

Epoch: 6| Step: 9
Training loss: 2.204422950744629
Validation loss: 2.0348668297131858

Epoch: 6| Step: 10
Training loss: 2.7375259399414062
Validation loss: 2.0218636790911355

Epoch: 6| Step: 11
Training loss: 1.406531572341919
Validation loss: 2.018670837084452

Epoch: 6| Step: 12
Training loss: 2.302964687347412
Validation loss: 2.02374404668808

Epoch: 6| Step: 13
Training loss: 2.7328226566314697
Validation loss: 2.0124390522638955

Epoch: 96| Step: 0
Training loss: 2.1583478450775146
Validation loss: 2.017106056213379

Epoch: 6| Step: 1
Training loss: 2.065861225128174
Validation loss: 2.0225072701772056

Epoch: 6| Step: 2
Training loss: 2.2725136280059814
Validation loss: 2.0177870194117227

Epoch: 6| Step: 3
Training loss: 2.1201000213623047
Validation loss: 2.0213362773259482

Epoch: 6| Step: 4
Training loss: 2.894080638885498
Validation loss: 2.020274579524994

Epoch: 6| Step: 5
Training loss: 2.0230486392974854
Validation loss: 2.0233104626337686

Epoch: 6| Step: 6
Training loss: 2.617053270339966
Validation loss: 2.022255539894104

Epoch: 6| Step: 7
Training loss: 2.315779685974121
Validation loss: 2.036015729109446

Epoch: 6| Step: 8
Training loss: 2.4769935607910156
Validation loss: 2.0326903065045676

Epoch: 6| Step: 9
Training loss: 1.235162615776062
Validation loss: 2.036993443965912

Epoch: 6| Step: 10
Training loss: 1.446761965751648
Validation loss: 2.032599608103434

Epoch: 6| Step: 11
Training loss: 2.033905029296875
Validation loss: 2.033547262350718

Epoch: 6| Step: 12
Training loss: 1.9788103103637695
Validation loss: 2.0425137678782144

Epoch: 6| Step: 13
Training loss: 2.352437973022461
Validation loss: 2.0371093352635703

Epoch: 97| Step: 0
Training loss: 1.6837042570114136
Validation loss: 2.036528249581655

Epoch: 6| Step: 1
Training loss: 2.1245131492614746
Validation loss: 2.03727134068807

Epoch: 6| Step: 2
Training loss: 2.41386079788208
Validation loss: 2.0350736180941262

Epoch: 6| Step: 3
Training loss: 2.261064052581787
Validation loss: 2.0372406244277954

Epoch: 6| Step: 4
Training loss: 1.9369745254516602
Validation loss: 2.0317710240681968

Epoch: 6| Step: 5
Training loss: 1.975320816040039
Validation loss: 2.030664046605428

Epoch: 6| Step: 6
Training loss: 2.0666165351867676
Validation loss: 2.033925731976827

Epoch: 6| Step: 7
Training loss: 2.104729652404785
Validation loss: 2.0164810021718345

Epoch: 6| Step: 8
Training loss: 2.124819278717041
Validation loss: 2.021353463331858

Epoch: 6| Step: 9
Training loss: 2.5031001567840576
Validation loss: 2.022280474503835

Epoch: 6| Step: 10
Training loss: 2.027534008026123
Validation loss: 2.011990706125895

Epoch: 6| Step: 11
Training loss: 2.352269172668457
Validation loss: 2.0149768193562827

Epoch: 6| Step: 12
Training loss: 2.3869752883911133
Validation loss: 2.0120020310084024

Epoch: 6| Step: 13
Training loss: 1.713728666305542
Validation loss: 2.0177884896596274

Epoch: 98| Step: 0
Training loss: 2.771273136138916
Validation loss: 2.0140499671300254

Epoch: 6| Step: 1
Training loss: 2.274540662765503
Validation loss: 2.0163532296816506

Epoch: 6| Step: 2
Training loss: 1.934394121170044
Validation loss: 2.013041297594706

Epoch: 6| Step: 3
Training loss: 1.8082630634307861
Validation loss: 2.0200208822886148

Epoch: 6| Step: 4
Training loss: 2.3926496505737305
Validation loss: 2.011453648408254

Epoch: 6| Step: 5
Training loss: 1.807211995124817
Validation loss: 2.0101866324742637

Epoch: 6| Step: 6
Training loss: 2.617644786834717
Validation loss: 2.0164926846822104

Epoch: 6| Step: 7
Training loss: 2.693819046020508
Validation loss: 2.0117554465929666

Epoch: 6| Step: 8
Training loss: 1.8943390846252441
Validation loss: 2.0088219046592712

Epoch: 6| Step: 9
Training loss: 1.9886658191680908
Validation loss: 2.021819233894348

Epoch: 6| Step: 10
Training loss: 1.599888801574707
Validation loss: 2.0273833672205606

Epoch: 6| Step: 11
Training loss: 1.7484970092773438
Validation loss: 2.0205472310384116

Epoch: 6| Step: 12
Training loss: 2.0677971839904785
Validation loss: 2.02557235956192

Epoch: 6| Step: 13
Training loss: 1.9437251091003418
Validation loss: 2.0237894852956138

Epoch: 99| Step: 0
Training loss: 1.8209967613220215
Validation loss: 2.0329115192095437

Epoch: 6| Step: 1
Training loss: 2.6110832691192627
Validation loss: 2.031848748524984

Epoch: 6| Step: 2
Training loss: 2.6698336601257324
Validation loss: 2.0317848324775696

Epoch: 6| Step: 3
Training loss: 1.9209156036376953
Validation loss: 2.023857037226359

Epoch: 6| Step: 4
Training loss: 1.845733642578125
Validation loss: 2.032203455766042

Epoch: 6| Step: 5
Training loss: 2.179250717163086
Validation loss: 2.0192381938298545

Epoch: 6| Step: 6
Training loss: 2.255664110183716
Validation loss: 2.031951665878296

Epoch: 6| Step: 7
Training loss: 2.2899842262268066
Validation loss: 2.031205117702484

Epoch: 6| Step: 8
Training loss: 1.5148932933807373
Validation loss: 2.0263577500979104

Epoch: 6| Step: 9
Training loss: 1.5905067920684814
Validation loss: 2.0217756628990173

Epoch: 6| Step: 10
Training loss: 2.2333784103393555
Validation loss: 2.028615335623423

Epoch: 6| Step: 11
Training loss: 2.1413931846618652
Validation loss: 2.0300808946291604

Epoch: 6| Step: 12
Training loss: 1.865790605545044
Validation loss: 2.037730058034261

Epoch: 6| Step: 13
Training loss: 2.6846423149108887
Validation loss: 2.0397882064183555

Epoch: 100| Step: 0
Training loss: 2.397645950317383
Validation loss: 2.047395129998525

Epoch: 6| Step: 1
Training loss: 2.0561532974243164
Validation loss: 2.0489336252212524

Epoch: 6| Step: 2
Training loss: 1.9374679327011108
Validation loss: 2.0309558709462485

Epoch: 6| Step: 3
Training loss: 1.877455711364746
Validation loss: 2.035165866216024

Epoch: 6| Step: 4
Training loss: 2.091221332550049
Validation loss: 2.035232186317444

Epoch: 6| Step: 5
Training loss: 2.4110593795776367
Validation loss: 2.0406464536984763

Epoch: 6| Step: 6
Training loss: 2.3078932762145996
Validation loss: 2.0500736435254416

Epoch: 6| Step: 7
Training loss: 1.425654411315918
Validation loss: 2.0579433043797812

Epoch: 6| Step: 8
Training loss: 2.213008403778076
Validation loss: 2.0757206678390503

Epoch: 6| Step: 9
Training loss: 2.0862302780151367
Validation loss: 2.0943061312039695

Epoch: 6| Step: 10
Training loss: 2.056865692138672
Validation loss: 2.067192335923513

Epoch: 6| Step: 11
Training loss: 2.3305089473724365
Validation loss: 2.0568349758783975

Epoch: 6| Step: 12
Training loss: 2.324828624725342
Validation loss: 2.0524067282676697

Epoch: 6| Step: 13
Training loss: 2.3218042850494385
Validation loss: 2.040359079837799

Epoch: 101| Step: 0
Training loss: 1.9069225788116455
Validation loss: 2.021631638209025

Epoch: 6| Step: 1
Training loss: 2.2439379692077637
Validation loss: 2.0231310526529946

Epoch: 6| Step: 2
Training loss: 2.020247220993042
Validation loss: 2.0152546763420105

Epoch: 6| Step: 3
Training loss: 2.365117073059082
Validation loss: 2.0269691546758017

Epoch: 6| Step: 4
Training loss: 2.4230446815490723
Validation loss: 2.020626664161682

Epoch: 6| Step: 5
Training loss: 1.9937100410461426
Validation loss: 2.020161052544912

Epoch: 6| Step: 6
Training loss: 2.256608486175537
Validation loss: 2.027489960193634

Epoch: 6| Step: 7
Training loss: 1.5219008922576904
Validation loss: 2.016431212425232

Epoch: 6| Step: 8
Training loss: 2.214914560317993
Validation loss: 2.0254199107488

Epoch: 6| Step: 9
Training loss: 1.8004382848739624
Validation loss: 2.0192180077234902

Epoch: 6| Step: 10
Training loss: 1.9402278661727905
Validation loss: 2.024427890777588

Epoch: 6| Step: 11
Training loss: 3.1000964641571045
Validation loss: 2.0325825214385986

Epoch: 6| Step: 12
Training loss: 1.9262433052062988
Validation loss: 2.0339162349700928

Epoch: 6| Step: 13
Training loss: 1.8944765329360962
Validation loss: 2.048732856909434

Epoch: 102| Step: 0
Training loss: 1.6237502098083496
Validation loss: 2.0473799308141074

Epoch: 6| Step: 1
Training loss: 2.034766435623169
Validation loss: 2.0408628582954407

Epoch: 6| Step: 2
Training loss: 2.0396366119384766
Validation loss: 2.050479253133138

Epoch: 6| Step: 3
Training loss: 1.9586164951324463
Validation loss: 2.031793177127838

Epoch: 6| Step: 4
Training loss: 2.4923107624053955
Validation loss: 2.0363077918688455

Epoch: 6| Step: 5
Training loss: 2.1686317920684814
Validation loss: 2.036081075668335

Epoch: 6| Step: 6
Training loss: 1.8070780038833618
Validation loss: 2.038858195145925

Epoch: 6| Step: 7
Training loss: 2.7565834522247314
Validation loss: 2.039445459842682

Epoch: 6| Step: 8
Training loss: 2.114380359649658
Validation loss: 2.0263033310572305

Epoch: 6| Step: 9
Training loss: 2.134650707244873
Validation loss: 2.033523201942444

Epoch: 6| Step: 10
Training loss: 2.1664505004882812
Validation loss: 2.0388322869936624

Epoch: 6| Step: 11
Training loss: 1.367777705192566
Validation loss: 2.035518447558085

Epoch: 6| Step: 12
Training loss: 2.9158618450164795
Validation loss: 2.0568057099978128

Epoch: 6| Step: 13
Training loss: 2.0737009048461914
Validation loss: 2.040363053480784

Epoch: 103| Step: 0
Training loss: 2.5636212825775146
Validation loss: 2.043267468611399

Epoch: 6| Step: 1
Training loss: 1.7335580587387085
Validation loss: 2.0487718184789023

Epoch: 6| Step: 2
Training loss: 2.3042924404144287
Validation loss: 2.037240445613861

Epoch: 6| Step: 3
Training loss: 1.83115816116333
Validation loss: 2.02188773949941

Epoch: 6| Step: 4
Training loss: 2.3505165576934814
Validation loss: 2.037987550099691

Epoch: 6| Step: 5
Training loss: 2.3865697383880615
Validation loss: 2.037157972653707

Epoch: 6| Step: 6
Training loss: 2.262542724609375
Validation loss: 2.0209267338116965

Epoch: 6| Step: 7
Training loss: 2.7407326698303223
Validation loss: 2.019726594289144

Epoch: 6| Step: 8
Training loss: 1.835038661956787
Validation loss: 2.0280343890190125

Epoch: 6| Step: 9
Training loss: 1.6281863451004028
Validation loss: 2.0306409796079

Epoch: 6| Step: 10
Training loss: 2.390463352203369
Validation loss: 2.0340308944384256

Epoch: 6| Step: 11
Training loss: 2.1010403633117676
Validation loss: 2.0280060370763144

Epoch: 6| Step: 12
Training loss: 1.8675563335418701
Validation loss: 2.023880203564962

Epoch: 6| Step: 13
Training loss: 1.990867257118225
Validation loss: 2.024733523527781

Epoch: 104| Step: 0
Training loss: 2.502133846282959
Validation loss: 2.0307747522989907

Epoch: 6| Step: 1
Training loss: 1.915861964225769
Validation loss: 2.0291465520858765

Epoch: 6| Step: 2
Training loss: 1.7521945238113403
Validation loss: 2.027153511842092

Epoch: 6| Step: 3
Training loss: 2.3389620780944824
Validation loss: 2.0387277603149414

Epoch: 6| Step: 4
Training loss: 2.11726713180542
Validation loss: 2.0412758588790894

Epoch: 6| Step: 5
Training loss: 1.8936538696289062
Validation loss: 2.040348927179972

Epoch: 6| Step: 6
Training loss: 2.2368547916412354
Validation loss: 2.0386075576146445

Epoch: 6| Step: 7
Training loss: 1.9703328609466553
Validation loss: 2.0512628157933555

Epoch: 6| Step: 8
Training loss: 2.012336015701294
Validation loss: 2.0425190528233848

Epoch: 6| Step: 9
Training loss: 2.4933021068573
Validation loss: 2.0405160387357077

Epoch: 6| Step: 10
Training loss: 2.3593313694000244
Validation loss: 2.046044905980428

Epoch: 6| Step: 11
Training loss: 2.465123176574707
Validation loss: 2.039264957110087

Epoch: 6| Step: 12
Training loss: 2.0178821086883545
Validation loss: 2.031574547290802

Epoch: 6| Step: 13
Training loss: 1.5267972946166992
Validation loss: 2.0320956707000732

Epoch: 105| Step: 0
Training loss: 1.9387249946594238
Validation loss: 2.0286612709363303

Epoch: 6| Step: 1
Training loss: 2.363135814666748
Validation loss: 2.016216238339742

Epoch: 6| Step: 2
Training loss: 2.52138352394104
Validation loss: 2.0281610091527305

Epoch: 6| Step: 3
Training loss: 2.068401336669922
Validation loss: 2.026563505331675

Epoch: 6| Step: 4
Training loss: 1.820139765739441
Validation loss: 2.0236470897992453

Epoch: 6| Step: 5
Training loss: 1.8108234405517578
Validation loss: 2.02388596534729

Epoch: 6| Step: 6
Training loss: 1.8381484746932983
Validation loss: 2.035319228967031

Epoch: 6| Step: 7
Training loss: 2.710373640060425
Validation loss: 2.032214124997457

Epoch: 6| Step: 8
Training loss: 2.0279908180236816
Validation loss: 2.0198455452919006

Epoch: 6| Step: 9
Training loss: 1.968808889389038
Validation loss: 2.0338034629821777

Epoch: 6| Step: 10
Training loss: 2.0008063316345215
Validation loss: 2.0342450936635337

Epoch: 6| Step: 11
Training loss: 2.353933095932007
Validation loss: 2.031440794467926

Epoch: 6| Step: 12
Training loss: 1.8709385395050049
Validation loss: 2.026349445184072

Epoch: 6| Step: 13
Training loss: 2.174072265625
Validation loss: 2.02827654282252

Epoch: 106| Step: 0
Training loss: 1.727206826210022
Validation loss: 2.040861427783966

Epoch: 6| Step: 1
Training loss: 1.9631705284118652
Validation loss: 2.032158931096395

Epoch: 6| Step: 2
Training loss: 2.2424864768981934
Validation loss: 2.029396335283915

Epoch: 6| Step: 3
Training loss: 2.1217076778411865
Validation loss: 2.0467170675595603

Epoch: 6| Step: 4
Training loss: 1.7010550498962402
Validation loss: 2.038024922211965

Epoch: 6| Step: 5
Training loss: 1.6381516456604004
Validation loss: 2.048343777656555

Epoch: 6| Step: 6
Training loss: 1.8927276134490967
Validation loss: 2.0465627710024514

Epoch: 6| Step: 7
Training loss: 2.083972454071045
Validation loss: 2.04427703221639

Epoch: 6| Step: 8
Training loss: 2.681652069091797
Validation loss: 2.040679315725962

Epoch: 6| Step: 9
Training loss: 2.1154139041900635
Validation loss: 2.0439544121424356

Epoch: 6| Step: 10
Training loss: 2.0618557929992676
Validation loss: 2.0502904256184897

Epoch: 6| Step: 11
Training loss: 2.0188655853271484
Validation loss: 2.0350452065467834

Epoch: 6| Step: 12
Training loss: 2.5017478466033936
Validation loss: 2.038636247316996

Epoch: 6| Step: 13
Training loss: 2.661637783050537
Validation loss: 2.0423008799552917

Epoch: 107| Step: 0
Training loss: 2.1745405197143555
Validation loss: 2.043432136376699

Epoch: 6| Step: 1
Training loss: 2.432392120361328
Validation loss: 2.0457005302111306

Epoch: 6| Step: 2
Training loss: 1.764695405960083
Validation loss: 2.0514071583747864

Epoch: 6| Step: 3
Training loss: 2.334507465362549
Validation loss: 2.0662380854288735

Epoch: 6| Step: 4
Training loss: 2.4070630073547363
Validation loss: 2.0485828717549643

Epoch: 6| Step: 5
Training loss: 2.9130074977874756
Validation loss: 2.041657487551371

Epoch: 6| Step: 6
Training loss: 1.9714986085891724
Validation loss: 2.0469062129656472

Epoch: 6| Step: 7
Training loss: 2.2572343349456787
Validation loss: 2.049634416898092

Epoch: 6| Step: 8
Training loss: 2.1018826961517334
Validation loss: 2.03685732682546

Epoch: 6| Step: 9
Training loss: 1.9872910976409912
Validation loss: 2.039061745007833

Epoch: 6| Step: 10
Training loss: 1.7673931121826172
Validation loss: 2.033896724383036

Epoch: 6| Step: 11
Training loss: 1.8772083520889282
Validation loss: 2.0322745641072593

Epoch: 6| Step: 12
Training loss: 1.7675443887710571
Validation loss: 2.03167587518692

Epoch: 6| Step: 13
Training loss: 1.5251693725585938
Validation loss: 2.019515593846639

Epoch: 108| Step: 0
Training loss: 1.6299118995666504
Validation loss: 2.02327823638916

Epoch: 6| Step: 1
Training loss: 2.267106533050537
Validation loss: 2.0174180070559182

Epoch: 6| Step: 2
Training loss: 1.8994852304458618
Validation loss: 2.0186363061269126

Epoch: 6| Step: 3
Training loss: 1.8045648336410522
Validation loss: 2.0174558957417807

Epoch: 6| Step: 4
Training loss: 2.285541534423828
Validation loss: 2.0188211798667908

Epoch: 6| Step: 5
Training loss: 2.575854539871216
Validation loss: 2.0181601643562317

Epoch: 6| Step: 6
Training loss: 2.0649666786193848
Validation loss: 2.0111202398935952

Epoch: 6| Step: 7
Training loss: 3.0119006633758545
Validation loss: 2.000454326470693

Epoch: 6| Step: 8
Training loss: 1.8258171081542969
Validation loss: 2.000136931737264

Epoch: 6| Step: 9
Training loss: 1.707785964012146
Validation loss: 2.0039202173550925

Epoch: 6| Step: 10
Training loss: 1.7255306243896484
Validation loss: 1.9951014320055644

Epoch: 6| Step: 11
Training loss: 2.2724807262420654
Validation loss: 2.009518285592397

Epoch: 6| Step: 12
Training loss: 2.281583309173584
Validation loss: 2.0086000164349875

Epoch: 6| Step: 13
Training loss: 2.1484789848327637
Validation loss: 2.0126742323239646

Epoch: 109| Step: 0
Training loss: 2.1831555366516113
Validation loss: 2.0118927558263144

Epoch: 6| Step: 1
Training loss: 2.288224697113037
Validation loss: 2.020778020222982

Epoch: 6| Step: 2
Training loss: 2.3049519062042236
Validation loss: 2.0158421397209167

Epoch: 6| Step: 3
Training loss: 2.375674247741699
Validation loss: 2.0185572504997253

Epoch: 6| Step: 4
Training loss: 1.7389479875564575
Validation loss: 2.022082289059957

Epoch: 6| Step: 5
Training loss: 2.2511558532714844
Validation loss: 2.0253568490346274

Epoch: 6| Step: 6
Training loss: 2.5215744972229004
Validation loss: 2.026553750038147

Epoch: 6| Step: 7
Training loss: 1.945255994796753
Validation loss: 2.0314303437868753

Epoch: 6| Step: 8
Training loss: 2.1011319160461426
Validation loss: 2.028992772102356

Epoch: 6| Step: 9
Training loss: 2.226200580596924
Validation loss: 2.0270920594533286

Epoch: 6| Step: 10
Training loss: 2.1949715614318848
Validation loss: 2.0217780073483786

Epoch: 6| Step: 11
Training loss: 1.4350703954696655
Validation loss: 2.0139392813046775

Epoch: 6| Step: 12
Training loss: 1.4448797702789307
Validation loss: 2.017382562160492

Epoch: 6| Step: 13
Training loss: 2.355177402496338
Validation loss: 2.01768426100413

Epoch: 110| Step: 0
Training loss: 2.053213119506836
Validation loss: 2.0108032822608948

Epoch: 6| Step: 1
Training loss: 1.70833420753479
Validation loss: 2.0125327905019126

Epoch: 6| Step: 2
Training loss: 1.7613722085952759
Validation loss: 2.0161933302879333

Epoch: 6| Step: 3
Training loss: 1.9445083141326904
Validation loss: 2.0065499544143677

Epoch: 6| Step: 4
Training loss: 3.0218310356140137
Validation loss: 2.0205941994984946

Epoch: 6| Step: 5
Training loss: 2.0663681030273438
Validation loss: 2.005136728286743

Epoch: 6| Step: 6
Training loss: 2.163234233856201
Validation loss: 2.017676591873169

Epoch: 6| Step: 7
Training loss: 2.4249229431152344
Validation loss: 2.027074615160624

Epoch: 6| Step: 8
Training loss: 1.6258351802825928
Validation loss: 2.021032392978668

Epoch: 6| Step: 9
Training loss: 2.138310432434082
Validation loss: 2.026587963104248

Epoch: 6| Step: 10
Training loss: 2.493114471435547
Validation loss: 2.035336752732595

Epoch: 6| Step: 11
Training loss: 2.2775392532348633
Validation loss: 2.0250708063443503

Epoch: 6| Step: 12
Training loss: 1.4329711198806763
Validation loss: 2.058190186818441

Epoch: 6| Step: 13
Training loss: 2.1262238025665283
Validation loss: 2.048099458217621

Epoch: 111| Step: 0
Training loss: 1.7956680059432983
Validation loss: 2.054944475491842

Epoch: 6| Step: 1
Training loss: 2.136678695678711
Validation loss: 2.063790738582611

Epoch: 6| Step: 2
Training loss: 2.089033603668213
Validation loss: 2.063820024331411

Epoch: 6| Step: 3
Training loss: 1.8285326957702637
Validation loss: 2.0471263925234475

Epoch: 6| Step: 4
Training loss: 2.0365500450134277
Validation loss: 2.0176623860994973

Epoch: 6| Step: 5
Training loss: 2.220613479614258
Validation loss: 2.0361730655034385

Epoch: 6| Step: 6
Training loss: 2.132112979888916
Validation loss: 2.0425343910853067

Epoch: 6| Step: 7
Training loss: 1.797346830368042
Validation loss: 2.0203650991121926

Epoch: 6| Step: 8
Training loss: 2.2178902626037598
Validation loss: 2.0207980076471963

Epoch: 6| Step: 9
Training loss: 2.0843708515167236
Validation loss: 2.013744374116262

Epoch: 6| Step: 10
Training loss: 2.1869945526123047
Validation loss: 2.0113556583722434

Epoch: 6| Step: 11
Training loss: 2.1617512702941895
Validation loss: 2.0040516455968223

Epoch: 6| Step: 12
Training loss: 2.7621288299560547
Validation loss: 2.0070420304934182

Epoch: 6| Step: 13
Training loss: 2.5033469200134277
Validation loss: 2.016494711240133

Epoch: 112| Step: 0
Training loss: 2.1702723503112793
Validation loss: 2.00882097085317

Epoch: 6| Step: 1
Training loss: 1.4135022163391113
Validation loss: 2.014152685801188

Epoch: 6| Step: 2
Training loss: 2.5212037563323975
Validation loss: 2.014194448788961

Epoch: 6| Step: 3
Training loss: 1.8671112060546875
Validation loss: 2.01628714799881

Epoch: 6| Step: 4
Training loss: 2.06776762008667
Validation loss: 2.012667953968048

Epoch: 6| Step: 5
Training loss: 2.2962183952331543
Validation loss: 2.009596844514211

Epoch: 6| Step: 6
Training loss: 2.344606637954712
Validation loss: 2.015762686729431

Epoch: 6| Step: 7
Training loss: 2.148562431335449
Validation loss: 2.006558815638224

Epoch: 6| Step: 8
Training loss: 1.7216812372207642
Validation loss: 2.0306857426961265

Epoch: 6| Step: 9
Training loss: 1.8353427648544312
Validation loss: 2.026325762271881

Epoch: 6| Step: 10
Training loss: 1.6959757804870605
Validation loss: 2.029528319835663

Epoch: 6| Step: 11
Training loss: 2.9499123096466064
Validation loss: 2.016579588254293

Epoch: 6| Step: 12
Training loss: 2.149312973022461
Validation loss: 2.024779955546061

Epoch: 6| Step: 13
Training loss: 2.3159494400024414
Validation loss: 2.0370208024978638

Epoch: 113| Step: 0
Training loss: 2.393066167831421
Validation loss: 2.040384292602539

Epoch: 6| Step: 1
Training loss: 2.2346248626708984
Validation loss: 2.024456560611725

Epoch: 6| Step: 2
Training loss: 2.613056182861328
Validation loss: 2.046651601791382

Epoch: 6| Step: 3
Training loss: 1.8594250679016113
Validation loss: 2.027872006098429

Epoch: 6| Step: 4
Training loss: 1.8644808530807495
Validation loss: 2.037112812201182

Epoch: 6| Step: 5
Training loss: 2.546330451965332
Validation loss: 2.0334236224492392

Epoch: 6| Step: 6
Training loss: 2.315842628479004
Validation loss: 2.040025254090627

Epoch: 6| Step: 7
Training loss: 2.043637275695801
Validation loss: 2.0302254358927407

Epoch: 6| Step: 8
Training loss: 1.6750974655151367
Validation loss: 2.032353142897288

Epoch: 6| Step: 9
Training loss: 2.184786558151245
Validation loss: 2.035722851753235

Epoch: 6| Step: 10
Training loss: 2.1662967205047607
Validation loss: 2.0366065899531045

Epoch: 6| Step: 11
Training loss: 1.9180923700332642
Validation loss: 2.0373794436454773

Epoch: 6| Step: 12
Training loss: 1.7633497714996338
Validation loss: 2.026075800259908

Epoch: 6| Step: 13
Training loss: 1.753545880317688
Validation loss: 2.0297992825508118

Epoch: 114| Step: 0
Training loss: 2.0193026065826416
Validation loss: 2.0305272936820984

Epoch: 6| Step: 1
Training loss: 2.2974047660827637
Validation loss: 2.0214202404022217

Epoch: 6| Step: 2
Training loss: 1.913198709487915
Validation loss: 2.024579962094625

Epoch: 6| Step: 3
Training loss: 1.8836383819580078
Validation loss: 2.0342268149058023

Epoch: 6| Step: 4
Training loss: 1.798803687095642
Validation loss: 2.0141722758611045

Epoch: 6| Step: 5
Training loss: 2.3617632389068604
Validation loss: 2.018159508705139

Epoch: 6| Step: 6
Training loss: 2.5766549110412598
Validation loss: 2.024182220300039

Epoch: 6| Step: 7
Training loss: 1.694764494895935
Validation loss: 2.0155736605326333

Epoch: 6| Step: 8
Training loss: 1.8840259313583374
Validation loss: 2.0256463487943015

Epoch: 6| Step: 9
Training loss: 2.069108486175537
Validation loss: 2.030954976876577

Epoch: 6| Step: 10
Training loss: 2.567967176437378
Validation loss: 2.0245158871014914

Epoch: 6| Step: 11
Training loss: 1.7232778072357178
Validation loss: 2.0313822627067566

Epoch: 6| Step: 12
Training loss: 2.108125925064087
Validation loss: 2.020916481812795

Epoch: 6| Step: 13
Training loss: 2.445920467376709
Validation loss: 2.0176559487978616

Epoch: 115| Step: 0
Training loss: 2.414675712585449
Validation loss: 2.0234830180803933

Epoch: 6| Step: 1
Training loss: 2.068974494934082
Validation loss: 2.031177361806234

Epoch: 6| Step: 2
Training loss: 2.3552939891815186
Validation loss: 2.0152894457181296

Epoch: 6| Step: 3
Training loss: 2.2316784858703613
Validation loss: 2.028647323449453

Epoch: 6| Step: 4
Training loss: 2.611072063446045
Validation loss: 2.022119402885437

Epoch: 6| Step: 5
Training loss: 1.7582099437713623
Validation loss: 2.035246511300405

Epoch: 6| Step: 6
Training loss: 2.046481132507324
Validation loss: 2.03458438316981

Epoch: 6| Step: 7
Training loss: 1.8650898933410645
Validation loss: 2.0292759935061135

Epoch: 6| Step: 8
Training loss: 1.9630763530731201
Validation loss: 2.0321675141652427

Epoch: 6| Step: 9
Training loss: 1.5635987520217896
Validation loss: 2.0335466464360556

Epoch: 6| Step: 10
Training loss: 2.0643107891082764
Validation loss: 2.0457438826560974

Epoch: 6| Step: 11
Training loss: 2.5333218574523926
Validation loss: 2.0345537265141806

Epoch: 6| Step: 12
Training loss: 1.9972012042999268
Validation loss: 2.038554628690084

Epoch: 6| Step: 13
Training loss: 1.5680018663406372
Validation loss: 2.035392920176188

Epoch: 116| Step: 0
Training loss: 1.5631957054138184
Validation loss: 2.0255127350489297

Epoch: 6| Step: 1
Training loss: 2.370903491973877
Validation loss: 2.032464027404785

Epoch: 6| Step: 2
Training loss: 2.5290637016296387
Validation loss: 2.0399822294712067

Epoch: 6| Step: 3
Training loss: 2.038661241531372
Validation loss: 2.027255594730377

Epoch: 6| Step: 4
Training loss: 2.396552562713623
Validation loss: 2.0363094210624695

Epoch: 6| Step: 5
Training loss: 2.2546305656433105
Validation loss: 2.0250067512194314

Epoch: 6| Step: 6
Training loss: 2.0628437995910645
Validation loss: 2.0242266257603965

Epoch: 6| Step: 7
Training loss: 1.8550622463226318
Validation loss: 2.020279328028361

Epoch: 6| Step: 8
Training loss: 1.979277491569519
Validation loss: 2.0220937728881836

Epoch: 6| Step: 9
Training loss: 2.023855686187744
Validation loss: 2.0108187993367515

Epoch: 6| Step: 10
Training loss: 1.988721489906311
Validation loss: 2.025405248006185

Epoch: 6| Step: 11
Training loss: 1.622455358505249
Validation loss: 2.0234526991844177

Epoch: 6| Step: 12
Training loss: 1.921809434890747
Validation loss: 2.029727339744568

Epoch: 6| Step: 13
Training loss: 2.4576590061187744
Validation loss: 2.027264634768168

Epoch: 117| Step: 0
Training loss: 1.875035285949707
Validation loss: 2.0355137983957925

Epoch: 6| Step: 1
Training loss: 2.6112382411956787
Validation loss: 2.0248465140660605

Epoch: 6| Step: 2
Training loss: 2.1144394874572754
Validation loss: 2.030362864335378

Epoch: 6| Step: 3
Training loss: 2.451871871948242
Validation loss: 2.025924722353617

Epoch: 6| Step: 4
Training loss: 1.528268814086914
Validation loss: 2.0318541526794434

Epoch: 6| Step: 5
Training loss: 2.1101698875427246
Validation loss: 2.0356366435686746

Epoch: 6| Step: 6
Training loss: 2.6040852069854736
Validation loss: 2.0468424956003823

Epoch: 6| Step: 7
Training loss: 1.763747215270996
Validation loss: 2.0496636231740317

Epoch: 6| Step: 8
Training loss: 1.7876710891723633
Validation loss: 2.049006760120392

Epoch: 6| Step: 9
Training loss: 2.031735420227051
Validation loss: 2.0572884678840637

Epoch: 6| Step: 10
Training loss: 1.8201441764831543
Validation loss: 2.043826103210449

Epoch: 6| Step: 11
Training loss: 2.4143104553222656
Validation loss: 2.0481420358022056

Epoch: 6| Step: 12
Training loss: 1.9551949501037598
Validation loss: 2.04682864745458

Epoch: 6| Step: 13
Training loss: 2.1119821071624756
Validation loss: 2.035440961519877

Epoch: 118| Step: 0
Training loss: 2.291023015975952
Validation loss: 2.0359971721967063

Epoch: 6| Step: 1
Training loss: 2.3064064979553223
Validation loss: 2.0383947690327964

Epoch: 6| Step: 2
Training loss: 1.9087566137313843
Validation loss: 2.0423514445622764

Epoch: 6| Step: 3
Training loss: 2.0031275749206543
Validation loss: 2.043520470460256

Epoch: 6| Step: 4
Training loss: 2.213170289993286
Validation loss: 2.0392645398775735

Epoch: 6| Step: 5
Training loss: 1.4729020595550537
Validation loss: 2.044228216012319

Epoch: 6| Step: 6
Training loss: 2.1284384727478027
Validation loss: 2.0349183281262717

Epoch: 6| Step: 7
Training loss: 1.8149480819702148
Validation loss: 2.0505452950795493

Epoch: 6| Step: 8
Training loss: 2.3727288246154785
Validation loss: 2.044039309024811

Epoch: 6| Step: 9
Training loss: 2.2138006687164307
Validation loss: 2.0424055457115173

Epoch: 6| Step: 10
Training loss: 1.731323480606079
Validation loss: 2.0418107509613037

Epoch: 6| Step: 11
Training loss: 2.0056557655334473
Validation loss: 2.047808051109314

Epoch: 6| Step: 12
Training loss: 2.197726011276245
Validation loss: 2.0522738099098206

Epoch: 6| Step: 13
Training loss: 2.691082715988159
Validation loss: 2.060062885284424

Epoch: 119| Step: 0
Training loss: 1.8426357507705688
Validation loss: 2.042988101641337

Epoch: 6| Step: 1
Training loss: 2.5344104766845703
Validation loss: 2.038701673348745

Epoch: 6| Step: 2
Training loss: 1.7693310976028442
Validation loss: 2.0550698240598044

Epoch: 6| Step: 3
Training loss: 2.1708295345306396
Validation loss: 2.053105970223745

Epoch: 6| Step: 4
Training loss: 2.0385050773620605
Validation loss: 2.0373604695002236

Epoch: 6| Step: 5
Training loss: 1.793749451637268
Validation loss: 2.0410264333089194

Epoch: 6| Step: 6
Training loss: 2.4307210445404053
Validation loss: 2.039231061935425

Epoch: 6| Step: 7
Training loss: 1.655727505683899
Validation loss: 2.039200802644094

Epoch: 6| Step: 8
Training loss: 2.273796796798706
Validation loss: 2.04113898674647

Epoch: 6| Step: 9
Training loss: 1.8873478174209595
Validation loss: 2.0434213678042092

Epoch: 6| Step: 10
Training loss: 2.4959936141967773
Validation loss: 2.0456231037775674

Epoch: 6| Step: 11
Training loss: 1.9675836563110352
Validation loss: 2.033052404721578

Epoch: 6| Step: 12
Training loss: 1.9749794006347656
Validation loss: 2.036878546079

Epoch: 6| Step: 13
Training loss: 2.1456551551818848
Validation loss: 2.0323044657707214

Epoch: 120| Step: 0
Training loss: 2.591099739074707
Validation loss: 2.033014237880707

Epoch: 6| Step: 1
Training loss: 2.240464448928833
Validation loss: 2.025070150693258

Epoch: 6| Step: 2
Training loss: 1.8312280178070068
Validation loss: 2.024932305018107

Epoch: 6| Step: 3
Training loss: 1.590725064277649
Validation loss: 2.0344663858413696

Epoch: 6| Step: 4
Training loss: 2.0102310180664062
Validation loss: 2.0265889962514243

Epoch: 6| Step: 5
Training loss: 2.067049026489258
Validation loss: 2.03456978003184

Epoch: 6| Step: 6
Training loss: 1.9134875535964966
Validation loss: 2.0348596572875977

Epoch: 6| Step: 7
Training loss: 3.1328189373016357
Validation loss: 2.040970742702484

Epoch: 6| Step: 8
Training loss: 1.6581480503082275
Validation loss: 2.0341323018074036

Epoch: 6| Step: 9
Training loss: 1.919445276260376
Validation loss: 2.045118749141693

Epoch: 6| Step: 10
Training loss: 2.616446018218994
Validation loss: 2.0448503692944846

Epoch: 6| Step: 11
Training loss: 1.8937793970108032
Validation loss: 2.052574892838796

Epoch: 6| Step: 12
Training loss: 2.456801414489746
Validation loss: 2.051827828089396

Epoch: 6| Step: 13
Training loss: 1.6037229299545288
Validation loss: 2.056230306625366

Epoch: 121| Step: 0
Training loss: 1.9902410507202148
Validation loss: 2.043783684571584

Epoch: 6| Step: 1
Training loss: 2.4039456844329834
Validation loss: 2.052541414896647

Epoch: 6| Step: 2
Training loss: 2.2744712829589844
Validation loss: 2.045812487602234

Epoch: 6| Step: 3
Training loss: 1.3519055843353271
Validation loss: 2.038742462793986

Epoch: 6| Step: 4
Training loss: 2.2372913360595703
Validation loss: 2.0402674476305642

Epoch: 6| Step: 5
Training loss: 2.5718648433685303
Validation loss: 2.0383149782816568

Epoch: 6| Step: 6
Training loss: 2.254138946533203
Validation loss: 2.0429242849349976

Epoch: 6| Step: 7
Training loss: 1.997726321220398
Validation loss: 2.036876142024994

Epoch: 6| Step: 8
Training loss: 2.431344509124756
Validation loss: 2.0255070130030313

Epoch: 6| Step: 9
Training loss: 2.621438503265381
Validation loss: 2.013139287630717

Epoch: 6| Step: 10
Training loss: 1.9342589378356934
Validation loss: 2.0172466039657593

Epoch: 6| Step: 11
Training loss: 1.9902293682098389
Validation loss: 2.028849164644877

Epoch: 6| Step: 12
Training loss: 1.4811936616897583
Validation loss: 2.0286775827407837

Epoch: 6| Step: 13
Training loss: 1.7762030363082886
Validation loss: 2.025384783744812

Epoch: 122| Step: 0
Training loss: 2.20153546333313
Validation loss: 2.0320667028427124

Epoch: 6| Step: 1
Training loss: 1.985397458076477
Validation loss: 2.0415170391400657

Epoch: 6| Step: 2
Training loss: 2.0262339115142822
Validation loss: 2.032528281211853

Epoch: 6| Step: 3
Training loss: 1.6521458625793457
Validation loss: 2.0328370134035745

Epoch: 6| Step: 4
Training loss: 2.601050853729248
Validation loss: 2.0471064249674478

Epoch: 6| Step: 5
Training loss: 2.0569772720336914
Validation loss: 2.0465299487113953

Epoch: 6| Step: 6
Training loss: 2.0555577278137207
Validation loss: 2.0426153540611267

Epoch: 6| Step: 7
Training loss: 2.2813940048217773
Validation loss: 2.046338895956675

Epoch: 6| Step: 8
Training loss: 1.9941349029541016
Validation loss: 2.0394676327705383

Epoch: 6| Step: 9
Training loss: 2.1567723751068115
Validation loss: 2.0385756691296897

Epoch: 6| Step: 10
Training loss: 2.3293299674987793
Validation loss: 2.028119365374247

Epoch: 6| Step: 11
Training loss: 2.2205209732055664
Validation loss: 2.044627328713735

Epoch: 6| Step: 12
Training loss: 1.5753364562988281
Validation loss: 2.03874534368515

Epoch: 6| Step: 13
Training loss: 1.6432931423187256
Validation loss: 2.040555755297343

Epoch: 123| Step: 0
Training loss: 2.2779059410095215
Validation loss: 2.043689171473185

Epoch: 6| Step: 1
Training loss: 1.5065722465515137
Validation loss: 2.0445714592933655

Epoch: 6| Step: 2
Training loss: 2.4126203060150146
Validation loss: 2.0651522080103555

Epoch: 6| Step: 3
Training loss: 1.7306954860687256
Validation loss: 2.065200944741567

Epoch: 6| Step: 4
Training loss: 1.829370379447937
Validation loss: 2.061897019545237

Epoch: 6| Step: 5
Training loss: 2.471184015274048
Validation loss: 2.0504536628723145

Epoch: 6| Step: 6
Training loss: 2.217947006225586
Validation loss: 2.04415625333786

Epoch: 6| Step: 7
Training loss: 2.3074843883514404
Validation loss: 2.0204593539237976

Epoch: 6| Step: 8
Training loss: 1.8323333263397217
Validation loss: 2.0207255681355796

Epoch: 6| Step: 9
Training loss: 2.2169437408447266
Validation loss: 2.0236250162124634

Epoch: 6| Step: 10
Training loss: 2.630004644393921
Validation loss: 2.024340351422628

Epoch: 6| Step: 11
Training loss: 1.6631016731262207
Validation loss: 2.018415172894796

Epoch: 6| Step: 12
Training loss: 2.0982613563537598
Validation loss: 2.030212720235189

Epoch: 6| Step: 13
Training loss: 1.9806909561157227
Validation loss: 2.0289989511171975

Epoch: 124| Step: 0
Training loss: 1.7110059261322021
Validation loss: 2.0376833081245422

Epoch: 6| Step: 1
Training loss: 2.275230884552002
Validation loss: 2.0406589110692344

Epoch: 6| Step: 2
Training loss: 2.167464256286621
Validation loss: 2.035035490989685

Epoch: 6| Step: 3
Training loss: 2.535438060760498
Validation loss: 2.0302842060724893

Epoch: 6| Step: 4
Training loss: 2.4569435119628906
Validation loss: 2.029272476832072

Epoch: 6| Step: 5
Training loss: 2.23405385017395
Validation loss: 2.0232041676839194

Epoch: 6| Step: 6
Training loss: 1.9109563827514648
Validation loss: 2.029377539952596

Epoch: 6| Step: 7
Training loss: 1.9733788967132568
Validation loss: 2.0162037213643393

Epoch: 6| Step: 8
Training loss: 2.1082444190979004
Validation loss: 2.0204341808954873

Epoch: 6| Step: 9
Training loss: 2.22270131111145
Validation loss: 2.0233652194341025

Epoch: 6| Step: 10
Training loss: 1.8789929151535034
Validation loss: 2.0221243500709534

Epoch: 6| Step: 11
Training loss: 2.3701658248901367
Validation loss: 2.021971662839254

Epoch: 6| Step: 12
Training loss: 1.403317928314209
Validation loss: 2.0338807503382363

Epoch: 6| Step: 13
Training loss: 1.8207967281341553
Validation loss: 2.0427284240722656

Epoch: 125| Step: 0
Training loss: 1.9526804685592651
Validation loss: 2.0532984336217246

Epoch: 6| Step: 1
Training loss: 2.483725070953369
Validation loss: 2.066999614238739

Epoch: 6| Step: 2
Training loss: 2.149995803833008
Validation loss: 2.055504322052002

Epoch: 6| Step: 3
Training loss: 2.3340277671813965
Validation loss: 2.051612436771393

Epoch: 6| Step: 4
Training loss: 2.7185070514678955
Validation loss: 2.0557246605555215

Epoch: 6| Step: 5
Training loss: 1.918924331665039
Validation loss: 2.050935943921407

Epoch: 6| Step: 6
Training loss: 1.4246677160263062
Validation loss: 2.0245035688082376

Epoch: 6| Step: 7
Training loss: 1.457237720489502
Validation loss: 2.0297635793685913

Epoch: 6| Step: 8
Training loss: 1.8315598964691162
Validation loss: 2.0323340694109597

Epoch: 6| Step: 9
Training loss: 2.403468608856201
Validation loss: 2.0448372761408486

Epoch: 6| Step: 10
Training loss: 2.1874959468841553
Validation loss: 2.0473812222480774

Epoch: 6| Step: 11
Training loss: 2.3241994380950928
Validation loss: 2.0463804403940835

Epoch: 6| Step: 12
Training loss: 1.9798911809921265
Validation loss: 2.053347170352936

Epoch: 6| Step: 13
Training loss: 1.8826212882995605
Validation loss: 2.0475627779960632

Epoch: 126| Step: 0
Training loss: 2.1585826873779297
Validation loss: 2.0384337306022644

Epoch: 6| Step: 1
Training loss: 1.6616626977920532
Validation loss: 2.0386173327763877

Epoch: 6| Step: 2
Training loss: 1.96088445186615
Validation loss: 2.03508061170578

Epoch: 6| Step: 3
Training loss: 2.1116342544555664
Validation loss: 2.029510021209717

Epoch: 6| Step: 4
Training loss: 2.5235908031463623
Validation loss: 2.04154102007548

Epoch: 6| Step: 5
Training loss: 2.7785801887512207
Validation loss: 2.039646585782369

Epoch: 6| Step: 6
Training loss: 2.230863094329834
Validation loss: 2.0355883836746216

Epoch: 6| Step: 7
Training loss: 1.8769135475158691
Validation loss: 2.0320180853207908

Epoch: 6| Step: 8
Training loss: 2.076474189758301
Validation loss: 2.0433204571406045

Epoch: 6| Step: 9
Training loss: 1.6333409547805786
Validation loss: 2.0424071550369263

Epoch: 6| Step: 10
Training loss: 1.6481597423553467
Validation loss: 2.035819947719574

Epoch: 6| Step: 11
Training loss: 2.2447893619537354
Validation loss: 2.045147657394409

Epoch: 6| Step: 12
Training loss: 2.484147071838379
Validation loss: 2.0497552355130515

Epoch: 6| Step: 13
Training loss: 1.644940733909607
Validation loss: 2.0483840505282083

Epoch: 127| Step: 0
Training loss: 2.6656360626220703
Validation loss: 2.0665759642918906

Epoch: 6| Step: 1
Training loss: 1.8677736520767212
Validation loss: 2.04996520280838

Epoch: 6| Step: 2
Training loss: 2.097559928894043
Validation loss: 2.051862955093384

Epoch: 6| Step: 3
Training loss: 1.75641667842865
Validation loss: 2.050250748793284

Epoch: 6| Step: 4
Training loss: 2.040226936340332
Validation loss: 2.049336632092794

Epoch: 6| Step: 5
Training loss: 2.063295841217041
Validation loss: 2.0387295285860696

Epoch: 6| Step: 6
Training loss: 2.0591163635253906
Validation loss: 2.0312973062197366

Epoch: 6| Step: 7
Training loss: 2.333869218826294
Validation loss: 2.0246522227923074

Epoch: 6| Step: 8
Training loss: 1.738556146621704
Validation loss: 2.032362182935079

Epoch: 6| Step: 9
Training loss: 2.6375796794891357
Validation loss: 2.033541222413381

Epoch: 6| Step: 10
Training loss: 1.9485304355621338
Validation loss: 2.0156418283780417

Epoch: 6| Step: 11
Training loss: 2.123171806335449
Validation loss: 2.02192630370458

Epoch: 6| Step: 12
Training loss: 1.746184229850769
Validation loss: 2.0206420024236045

Epoch: 6| Step: 13
Training loss: 2.1096413135528564
Validation loss: 2.019291639328003

Epoch: 128| Step: 0
Training loss: 2.5079054832458496
Validation loss: 2.0149041016896567

Epoch: 6| Step: 1
Training loss: 1.9442487955093384
Validation loss: 2.014791111151377

Epoch: 6| Step: 2
Training loss: 2.1707763671875
Validation loss: 2.015635887781779

Epoch: 6| Step: 3
Training loss: 2.381941318511963
Validation loss: 2.03613011042277

Epoch: 6| Step: 4
Training loss: 1.705337405204773
Validation loss: 2.0114315946896872

Epoch: 6| Step: 5
Training loss: 2.8199033737182617
Validation loss: 2.022143840789795

Epoch: 6| Step: 6
Training loss: 1.5650303363800049
Validation loss: 2.025249461332957

Epoch: 6| Step: 7
Training loss: 1.8320796489715576
Validation loss: 2.008765002091726

Epoch: 6| Step: 8
Training loss: 1.979720950126648
Validation loss: 2.035054643948873

Epoch: 6| Step: 9
Training loss: 2.6616296768188477
Validation loss: 2.034949759642283

Epoch: 6| Step: 10
Training loss: 1.7301867008209229
Validation loss: 2.0316632986068726

Epoch: 6| Step: 11
Training loss: 1.6923105716705322
Validation loss: 2.02945484717687

Epoch: 6| Step: 12
Training loss: 1.8627772331237793
Validation loss: 2.0328657825787864

Epoch: 6| Step: 13
Training loss: 2.5352725982666016
Validation loss: 2.0289661288261414

Epoch: 129| Step: 0
Training loss: 2.1330337524414062
Validation loss: 2.025339444478353

Epoch: 6| Step: 1
Training loss: 1.334334135055542
Validation loss: 2.036644438902537

Epoch: 6| Step: 2
Training loss: 2.5444958209991455
Validation loss: 2.0351057052612305

Epoch: 6| Step: 3
Training loss: 2.4571926593780518
Validation loss: 2.0312612056732178

Epoch: 6| Step: 4
Training loss: 1.9667534828186035
Validation loss: 2.0606359243392944

Epoch: 6| Step: 5
Training loss: 2.4778404235839844
Validation loss: 2.0371787746747336

Epoch: 6| Step: 6
Training loss: 2.1253623962402344
Validation loss: 2.0575472513834634

Epoch: 6| Step: 7
Training loss: 1.4009056091308594
Validation loss: 2.049004296461741

Epoch: 6| Step: 8
Training loss: 2.032970666885376
Validation loss: 2.0565894842147827

Epoch: 6| Step: 9
Training loss: 2.172295093536377
Validation loss: 2.0556767781575522

Epoch: 6| Step: 10
Training loss: 1.8080205917358398
Validation loss: 2.038676142692566

Epoch: 6| Step: 11
Training loss: 2.211019992828369
Validation loss: 2.0417511463165283

Epoch: 6| Step: 12
Training loss: 2.5869336128234863
Validation loss: 2.0393484036127725

Epoch: 6| Step: 13
Training loss: 1.739806890487671
Validation loss: 2.0301974614461265

Epoch: 130| Step: 0
Training loss: 1.9387993812561035
Validation loss: 2.0461466312408447

Epoch: 6| Step: 1
Training loss: 2.148676633834839
Validation loss: 2.0592840711275735

Epoch: 6| Step: 2
Training loss: 1.4226189851760864
Validation loss: 2.049303491910299

Epoch: 6| Step: 3
Training loss: 2.0040664672851562
Validation loss: 2.0570210417111716

Epoch: 6| Step: 4
Training loss: 2.077974319458008
Validation loss: 2.052436908086141

Epoch: 6| Step: 5
Training loss: 1.9052428007125854
Validation loss: 2.0488770405451455

Epoch: 6| Step: 6
Training loss: 1.702718734741211
Validation loss: 2.0484664837519326

Epoch: 6| Step: 7
Training loss: 2.664496421813965
Validation loss: 2.047870457172394

Epoch: 6| Step: 8
Training loss: 1.6412527561187744
Validation loss: 2.059311548868815

Epoch: 6| Step: 9
Training loss: 2.8898563385009766
Validation loss: 2.062307854493459

Epoch: 6| Step: 10
Training loss: 1.9331384897232056
Validation loss: 2.043492595354716

Epoch: 6| Step: 11
Training loss: 2.042405366897583
Validation loss: 2.047248125076294

Epoch: 6| Step: 12
Training loss: 1.8737642765045166
Validation loss: 2.0397339860598245

Epoch: 6| Step: 13
Training loss: 2.377840280532837
Validation loss: 2.052437742551168

Epoch: 131| Step: 0
Training loss: 1.2884273529052734
Validation loss: 2.0593291322390237

Epoch: 6| Step: 1
Training loss: 1.9519619941711426
Validation loss: 2.0584840973218284

Epoch: 6| Step: 2
Training loss: 1.9913703203201294
Validation loss: 2.066362818082174

Epoch: 6| Step: 3
Training loss: 1.7144041061401367
Validation loss: 2.0623027086257935

Epoch: 6| Step: 4
Training loss: 1.7069587707519531
Validation loss: 2.0535687804222107

Epoch: 6| Step: 5
Training loss: 2.302645206451416
Validation loss: 2.0565430720647178

Epoch: 6| Step: 6
Training loss: 1.619087815284729
Validation loss: 2.0505561232566833

Epoch: 6| Step: 7
Training loss: 1.78507661819458
Validation loss: 2.049148142337799

Epoch: 6| Step: 8
Training loss: 2.222207546234131
Validation loss: 2.03374836842219

Epoch: 6| Step: 9
Training loss: 2.8391761779785156
Validation loss: 2.042304595311483

Epoch: 6| Step: 10
Training loss: 2.0731301307678223
Validation loss: 2.0416105588277182

Epoch: 6| Step: 11
Training loss: 2.1020588874816895
Validation loss: 2.0337747931480408

Epoch: 6| Step: 12
Training loss: 2.4822840690612793
Validation loss: 2.0542914072672525

Epoch: 6| Step: 13
Training loss: 2.6381592750549316
Validation loss: 2.0635730425516763

Epoch: 132| Step: 0
Training loss: 1.7304844856262207
Validation loss: 2.064465880393982

Epoch: 6| Step: 1
Training loss: 2.4083828926086426
Validation loss: 2.05362602074941

Epoch: 6| Step: 2
Training loss: 1.6058759689331055
Validation loss: 2.051237463951111

Epoch: 6| Step: 3
Training loss: 2.397134304046631
Validation loss: 2.0515424609184265

Epoch: 6| Step: 4
Training loss: 2.0561275482177734
Validation loss: 2.0478505293528237

Epoch: 6| Step: 5
Training loss: 2.507960319519043
Validation loss: 2.0459313790003457

Epoch: 6| Step: 6
Training loss: 2.043516159057617
Validation loss: 2.0430386066436768

Epoch: 6| Step: 7
Training loss: 2.181819438934326
Validation loss: 2.0408639907836914

Epoch: 6| Step: 8
Training loss: 2.078514814376831
Validation loss: 2.0369531512260437

Epoch: 6| Step: 9
Training loss: 2.599881887435913
Validation loss: 2.0216326316197715

Epoch: 6| Step: 10
Training loss: 1.781527042388916
Validation loss: 2.026922345161438

Epoch: 6| Step: 11
Training loss: 1.743800401687622
Validation loss: 2.029320696989695

Epoch: 6| Step: 12
Training loss: 2.0721418857574463
Validation loss: 2.0359419186909995

Epoch: 6| Step: 13
Training loss: 1.510153889656067
Validation loss: 2.02290540933609

Epoch: 133| Step: 0
Training loss: 2.0476651191711426
Validation loss: 2.0224292476971946

Epoch: 6| Step: 1
Training loss: 1.7986619472503662
Validation loss: 2.0363815228144326

Epoch: 6| Step: 2
Training loss: 2.5566275119781494
Validation loss: 2.028774837652842

Epoch: 6| Step: 3
Training loss: 1.7392185926437378
Validation loss: 2.03277325630188

Epoch: 6| Step: 4
Training loss: 2.5374765396118164
Validation loss: 2.036360263824463

Epoch: 6| Step: 5
Training loss: 2.439469575881958
Validation loss: 2.041278680165609

Epoch: 6| Step: 6
Training loss: 1.4838064908981323
Validation loss: 2.0407340923945108

Epoch: 6| Step: 7
Training loss: 2.2301182746887207
Validation loss: 2.060911993185679

Epoch: 6| Step: 8
Training loss: 2.1010046005249023
Validation loss: 2.044718106587728

Epoch: 6| Step: 9
Training loss: 1.8861818313598633
Validation loss: 2.049161990483602

Epoch: 6| Step: 10
Training loss: 1.8283203840255737
Validation loss: 2.044987658659617

Epoch: 6| Step: 11
Training loss: 2.2656421661376953
Validation loss: 2.0370805859565735

Epoch: 6| Step: 12
Training loss: 1.9033619165420532
Validation loss: 2.0585241516431174

Epoch: 6| Step: 13
Training loss: 1.8950693607330322
Validation loss: 2.0681318243344626

Epoch: 134| Step: 0
Training loss: 2.287337303161621
Validation loss: 2.066032330195109

Epoch: 6| Step: 1
Training loss: 1.8341877460479736
Validation loss: 2.067527194817861

Epoch: 6| Step: 2
Training loss: 1.7684414386749268
Validation loss: 2.0614362359046936

Epoch: 6| Step: 3
Training loss: 1.9922046661376953
Validation loss: 2.077668766180674

Epoch: 6| Step: 4
Training loss: 1.8778352737426758
Validation loss: 2.0539370576540628

Epoch: 6| Step: 5
Training loss: 2.2755393981933594
Validation loss: 2.044493039449056

Epoch: 6| Step: 6
Training loss: 1.4304008483886719
Validation loss: 2.0484966039657593

Epoch: 6| Step: 7
Training loss: 2.156156063079834
Validation loss: 2.061779260635376

Epoch: 6| Step: 8
Training loss: 2.0619006156921387
Validation loss: 2.0587286353111267

Epoch: 6| Step: 9
Training loss: 2.522150993347168
Validation loss: 2.0556374192237854

Epoch: 6| Step: 10
Training loss: 1.8099069595336914
Validation loss: 2.070602297782898

Epoch: 6| Step: 11
Training loss: 1.7713606357574463
Validation loss: 2.0730986992518106

Epoch: 6| Step: 12
Training loss: 2.6183552742004395
Validation loss: 2.0691317518552146

Epoch: 6| Step: 13
Training loss: 2.107868194580078
Validation loss: 2.0681111812591553

Epoch: 135| Step: 0
Training loss: 2.1285128593444824
Validation loss: 2.076428492863973

Epoch: 6| Step: 1
Training loss: 2.215402603149414
Validation loss: 2.0694180925687156

Epoch: 6| Step: 2
Training loss: 1.7946194410324097
Validation loss: 2.0849300424257913

Epoch: 6| Step: 3
Training loss: 2.2018630504608154
Validation loss: 2.0727210640907288

Epoch: 6| Step: 4
Training loss: 2.2657530307769775
Validation loss: 2.077914218107859

Epoch: 6| Step: 5
Training loss: 1.9220860004425049
Validation loss: 2.085318148136139

Epoch: 6| Step: 6
Training loss: 2.0009617805480957
Validation loss: 2.0655745466550193

Epoch: 6| Step: 7
Training loss: 1.6434144973754883
Validation loss: 2.058258056640625

Epoch: 6| Step: 8
Training loss: 1.8813940286636353
Validation loss: 2.058716654777527

Epoch: 6| Step: 9
Training loss: 2.2190840244293213
Validation loss: 2.0637860695521035

Epoch: 6| Step: 10
Training loss: 1.520613431930542
Validation loss: 2.062790791193644

Epoch: 6| Step: 11
Training loss: 2.467183828353882
Validation loss: 2.0621161659558616

Epoch: 6| Step: 12
Training loss: 2.1112353801727295
Validation loss: 2.0597264766693115

Epoch: 6| Step: 13
Training loss: 2.147109031677246
Validation loss: 2.0537181893984475

Epoch: 136| Step: 0
Training loss: 1.8603527545928955
Validation loss: 2.054391026496887

Epoch: 6| Step: 1
Training loss: 2.0363845825195312
Validation loss: 2.0697002013524375

Epoch: 6| Step: 2
Training loss: 2.062009334564209
Validation loss: 2.0690606037775674

Epoch: 6| Step: 3
Training loss: 2.387829542160034
Validation loss: 2.0581132570902505

Epoch: 6| Step: 4
Training loss: 1.7340917587280273
Validation loss: 2.0630245407422385

Epoch: 6| Step: 5
Training loss: 3.476468324661255
Validation loss: 2.049393435319265

Epoch: 6| Step: 6
Training loss: 1.3158493041992188
Validation loss: 2.0491570234298706

Epoch: 6| Step: 7
Training loss: 2.878976821899414
Validation loss: 2.038478990395864

Epoch: 6| Step: 8
Training loss: 1.7084743976593018
Validation loss: 2.0376230279604592

Epoch: 6| Step: 9
Training loss: 2.0049753189086914
Validation loss: 2.0524231592814126

Epoch: 6| Step: 10
Training loss: 1.6544194221496582
Validation loss: 2.0404454270998635

Epoch: 6| Step: 11
Training loss: 2.4064958095550537
Validation loss: 2.046772301197052

Epoch: 6| Step: 12
Training loss: 1.709014892578125
Validation loss: 2.0496625304222107

Epoch: 6| Step: 13
Training loss: 1.72764253616333
Validation loss: 2.0435861547787986

Epoch: 137| Step: 0
Training loss: 1.8979904651641846
Validation loss: 2.046150883038839

Epoch: 6| Step: 1
Training loss: 1.9845536947250366
Validation loss: 2.045820474624634

Epoch: 6| Step: 2
Training loss: 2.3352956771850586
Validation loss: 2.050290048122406

Epoch: 6| Step: 3
Training loss: 2.067117691040039
Validation loss: 2.0362996260325112

Epoch: 6| Step: 4
Training loss: 1.6379420757293701
Validation loss: 2.054624875386556

Epoch: 6| Step: 5
Training loss: 2.16477108001709
Validation loss: 2.0395894050598145

Epoch: 6| Step: 6
Training loss: 1.8323616981506348
Validation loss: 2.044414520263672

Epoch: 6| Step: 7
Training loss: 1.5472071170806885
Validation loss: 2.0466599067052207

Epoch: 6| Step: 8
Training loss: 2.1499404907226562
Validation loss: 2.0453483859697976

Epoch: 6| Step: 9
Training loss: 2.441739797592163
Validation loss: 2.0548812548319497

Epoch: 6| Step: 10
Training loss: 2.2241482734680176
Validation loss: 2.038068115711212

Epoch: 6| Step: 11
Training loss: 2.0165929794311523
Validation loss: 2.0608097314834595

Epoch: 6| Step: 12
Training loss: 1.3994179964065552
Validation loss: 2.050998032093048

Epoch: 6| Step: 13
Training loss: 2.7092092037200928
Validation loss: 2.0571788549423218

Epoch: 138| Step: 0
Training loss: 1.953162670135498
Validation loss: 2.0565446416536965

Epoch: 6| Step: 1
Training loss: 2.0800061225891113
Validation loss: 2.0699217120806375

Epoch: 6| Step: 2
Training loss: 2.5102031230926514
Validation loss: 2.077451229095459

Epoch: 6| Step: 3
Training loss: 2.102780342102051
Validation loss: 2.0762998263041177

Epoch: 6| Step: 4
Training loss: 2.1672754287719727
Validation loss: 2.059943656126658

Epoch: 6| Step: 5
Training loss: 1.793351411819458
Validation loss: 2.0465689500172934

Epoch: 6| Step: 6
Training loss: 2.427555561065674
Validation loss: 2.052510142326355

Epoch: 6| Step: 7
Training loss: 1.686453104019165
Validation loss: 2.0619406501452127

Epoch: 6| Step: 8
Training loss: 1.6113814115524292
Validation loss: 2.0520653128623962

Epoch: 6| Step: 9
Training loss: 2.0352771282196045
Validation loss: 2.0497769117355347

Epoch: 6| Step: 10
Training loss: 2.3860325813293457
Validation loss: 2.0536374052365622

Epoch: 6| Step: 11
Training loss: 1.8664367198944092
Validation loss: 2.0669947067896524

Epoch: 6| Step: 12
Training loss: 1.5085492134094238
Validation loss: 2.063490708669027

Epoch: 6| Step: 13
Training loss: 2.3711795806884766
Validation loss: 2.06013031800588

Epoch: 139| Step: 0
Training loss: 1.866098403930664
Validation loss: 2.0578061739603677

Epoch: 6| Step: 1
Training loss: 2.781975030899048
Validation loss: 2.0589102506637573

Epoch: 6| Step: 2
Training loss: 1.7439409494400024
Validation loss: 2.0470588207244873

Epoch: 6| Step: 3
Training loss: 2.0718069076538086
Validation loss: 2.045738955338796

Epoch: 6| Step: 4
Training loss: 1.4893566370010376
Validation loss: 2.0492621461550393

Epoch: 6| Step: 5
Training loss: 1.6645551919937134
Validation loss: 2.0551969011624656

Epoch: 6| Step: 6
Training loss: 2.927267551422119
Validation loss: 2.0720510681470237

Epoch: 6| Step: 7
Training loss: 2.1529088020324707
Validation loss: 2.0574690103530884

Epoch: 6| Step: 8
Training loss: 2.034212112426758
Validation loss: 2.0613784988721213

Epoch: 6| Step: 9
Training loss: 2.009653091430664
Validation loss: 2.057892839113871

Epoch: 6| Step: 10
Training loss: 1.7154064178466797
Validation loss: 2.0555568734804788

Epoch: 6| Step: 11
Training loss: 2.3746964931488037
Validation loss: 2.059328258037567

Epoch: 6| Step: 12
Training loss: 1.7950778007507324
Validation loss: 2.0572586059570312

Epoch: 6| Step: 13
Training loss: 2.0404257774353027
Validation loss: 2.050019840399424

Epoch: 140| Step: 0
Training loss: 1.6381750106811523
Validation loss: 2.0390211741129556

Epoch: 6| Step: 1
Training loss: 2.1643693447113037
Validation loss: 2.04988823334376

Epoch: 6| Step: 2
Training loss: 1.489397644996643
Validation loss: 2.043957750002543

Epoch: 6| Step: 3
Training loss: 1.4646718502044678
Validation loss: 2.0406800707181296

Epoch: 6| Step: 4
Training loss: 1.9718272686004639
Validation loss: 2.038990596930186

Epoch: 6| Step: 5
Training loss: 2.30242919921875
Validation loss: 2.0312926967938743

Epoch: 6| Step: 6
Training loss: 2.017791271209717
Validation loss: 2.038064122200012

Epoch: 6| Step: 7
Training loss: 2.2298951148986816
Validation loss: 2.0374486446380615

Epoch: 6| Step: 8
Training loss: 2.6028966903686523
Validation loss: 2.0392361084620156

Epoch: 6| Step: 9
Training loss: 1.8247873783111572
Validation loss: 2.034979820251465

Epoch: 6| Step: 10
Training loss: 2.5424742698669434
Validation loss: 2.04954202969869

Epoch: 6| Step: 11
Training loss: 2.152496099472046
Validation loss: 2.0569908022880554

Epoch: 6| Step: 12
Training loss: 2.125054359436035
Validation loss: 2.050054748853048

Epoch: 6| Step: 13
Training loss: 1.9616692066192627
Validation loss: 2.027578830718994

Epoch: 141| Step: 0
Training loss: 2.691779613494873
Validation loss: 2.0583519339561462

Epoch: 6| Step: 1
Training loss: 1.9734748601913452
Validation loss: 2.04716428120931

Epoch: 6| Step: 2
Training loss: 1.5718269348144531
Validation loss: 2.042685866355896

Epoch: 6| Step: 3
Training loss: 2.6139092445373535
Validation loss: 2.0445087552070618

Epoch: 6| Step: 4
Training loss: 1.871750831604004
Validation loss: 2.049715300401052

Epoch: 6| Step: 5
Training loss: 1.7112841606140137
Validation loss: 2.0563161770502725

Epoch: 6| Step: 6
Training loss: 1.635936975479126
Validation loss: 2.041405200958252

Epoch: 6| Step: 7
Training loss: 1.961992859840393
Validation loss: 2.0489176710446677

Epoch: 6| Step: 8
Training loss: 1.5427478551864624
Validation loss: 2.055131475130717

Epoch: 6| Step: 9
Training loss: 1.9616258144378662
Validation loss: 2.0516934792200723

Epoch: 6| Step: 10
Training loss: 2.5307705402374268
Validation loss: 2.058886011441549

Epoch: 6| Step: 11
Training loss: 1.7232413291931152
Validation loss: 2.055794874827067

Epoch: 6| Step: 12
Training loss: 2.346524953842163
Validation loss: 2.0439816315968833

Epoch: 6| Step: 13
Training loss: 2.4759440422058105
Validation loss: 2.0395136078198752

Epoch: 142| Step: 0
Training loss: 1.7780299186706543
Validation loss: 2.050269603729248

Epoch: 6| Step: 1
Training loss: 2.39495587348938
Validation loss: 2.0495651165644326

Epoch: 6| Step: 2
Training loss: 1.8286864757537842
Validation loss: 2.0426180362701416

Epoch: 6| Step: 3
Training loss: 1.9804766178131104
Validation loss: 2.049272119998932

Epoch: 6| Step: 4
Training loss: 2.225034236907959
Validation loss: 2.06327094634374

Epoch: 6| Step: 5
Training loss: 1.7748873233795166
Validation loss: 2.047803362210592

Epoch: 6| Step: 6
Training loss: 2.1866376399993896
Validation loss: 2.061016241709391

Epoch: 6| Step: 7
Training loss: 2.4521477222442627
Validation loss: 2.050144155820211

Epoch: 6| Step: 8
Training loss: 2.468526840209961
Validation loss: 2.0519243478775024

Epoch: 6| Step: 9
Training loss: 1.407232403755188
Validation loss: 2.0686450401941934

Epoch: 6| Step: 10
Training loss: 2.0203254222869873
Validation loss: 2.0576239625612893

Epoch: 6| Step: 11
Training loss: 2.9015910625457764
Validation loss: 2.067988634109497

Epoch: 6| Step: 12
Training loss: 1.6875697374343872
Validation loss: 2.0610799193382263

Epoch: 6| Step: 13
Training loss: 1.4932061433792114
Validation loss: 2.065955698490143

Epoch: 143| Step: 0
Training loss: 2.2597761154174805
Validation loss: 2.075174252192179

Epoch: 6| Step: 1
Training loss: 1.9933547973632812
Validation loss: 2.069969197114309

Epoch: 6| Step: 2
Training loss: 2.5611391067504883
Validation loss: 2.0755592385927835

Epoch: 6| Step: 3
Training loss: 1.9432677030563354
Validation loss: 2.086985727151235

Epoch: 6| Step: 4
Training loss: 1.737982988357544
Validation loss: 2.0684174497922263

Epoch: 6| Step: 5
Training loss: 1.9460747241973877
Validation loss: 2.0766552488009133

Epoch: 6| Step: 6
Training loss: 2.060393810272217
Validation loss: 2.0766104261080423

Epoch: 6| Step: 7
Training loss: 2.1752872467041016
Validation loss: 2.057434697945913

Epoch: 6| Step: 8
Training loss: 1.61601984500885
Validation loss: 2.0604602495829263

Epoch: 6| Step: 9
Training loss: 1.7312202453613281
Validation loss: 2.065194527308146

Epoch: 6| Step: 10
Training loss: 2.1415417194366455
Validation loss: 2.0681480964024863

Epoch: 6| Step: 11
Training loss: 1.7897098064422607
Validation loss: 2.0636035402615867

Epoch: 6| Step: 12
Training loss: 1.9187555313110352
Validation loss: 2.062019646167755

Epoch: 6| Step: 13
Training loss: 2.6222238540649414
Validation loss: 2.062837243080139

Epoch: 144| Step: 0
Training loss: 2.08864164352417
Validation loss: 2.061272660891215

Epoch: 6| Step: 1
Training loss: 1.8337239027023315
Validation loss: 2.071221351623535

Epoch: 6| Step: 2
Training loss: 1.792074203491211
Validation loss: 2.0697713096936545

Epoch: 6| Step: 3
Training loss: 2.007866859436035
Validation loss: 2.0848288933436074

Epoch: 6| Step: 4
Training loss: 2.6910908222198486
Validation loss: 2.0895965496699014

Epoch: 6| Step: 5
Training loss: 1.791074514389038
Validation loss: 2.0818750460942588

Epoch: 6| Step: 6
Training loss: 1.8464337587356567
Validation loss: 2.090932309627533

Epoch: 6| Step: 7
Training loss: 1.8814780712127686
Validation loss: 2.0868865251541138

Epoch: 6| Step: 8
Training loss: 1.9513115882873535
Validation loss: 2.088265518347422

Epoch: 6| Step: 9
Training loss: 1.8544888496398926
Validation loss: 2.0891692638397217

Epoch: 6| Step: 10
Training loss: 2.7730231285095215
Validation loss: 2.0781275828679404

Epoch: 6| Step: 11
Training loss: 2.184556484222412
Validation loss: 2.0811527172724404

Epoch: 6| Step: 12
Training loss: 1.4386662244796753
Validation loss: 2.069818437099457

Epoch: 6| Step: 13
Training loss: 2.0917646884918213
Validation loss: 2.0640719334284463

Epoch: 145| Step: 0
Training loss: 1.9652589559555054
Validation loss: 2.070473869641622

Epoch: 6| Step: 1
Training loss: 2.0638833045959473
Validation loss: 2.0554113388061523

Epoch: 6| Step: 2
Training loss: 1.58143949508667
Validation loss: 2.054255406061808

Epoch: 6| Step: 3
Training loss: 2.0984156131744385
Validation loss: 2.073940714200338

Epoch: 6| Step: 4
Training loss: 1.6731364727020264
Validation loss: 2.0588696599006653

Epoch: 6| Step: 5
Training loss: 2.349043130874634
Validation loss: 2.0641902089118958

Epoch: 6| Step: 6
Training loss: 2.1390738487243652
Validation loss: 2.0487483938535056

Epoch: 6| Step: 7
Training loss: 1.941277027130127
Validation loss: 2.069427172342936

Epoch: 6| Step: 8
Training loss: 2.222036838531494
Validation loss: 2.0612045526504517

Epoch: 6| Step: 9
Training loss: 2.1181540489196777
Validation loss: 2.0495707392692566

Epoch: 6| Step: 10
Training loss: 1.8051414489746094
Validation loss: 2.0492790738741555

Epoch: 6| Step: 11
Training loss: 2.4851276874542236
Validation loss: 2.0545854369799295

Epoch: 6| Step: 12
Training loss: 1.9391915798187256
Validation loss: 2.0674323240915933

Epoch: 6| Step: 13
Training loss: 1.8570139408111572
Validation loss: 2.0591482718785605

Epoch: 146| Step: 0
Training loss: 1.7494664192199707
Validation loss: 2.0596272150675454

Epoch: 6| Step: 1
Training loss: 1.9211480617523193
Validation loss: 2.054876228173574

Epoch: 6| Step: 2
Training loss: 2.10705828666687
Validation loss: 2.079076409339905

Epoch: 6| Step: 3
Training loss: 1.9722788333892822
Validation loss: 2.061123549938202

Epoch: 6| Step: 4
Training loss: 1.9447568655014038
Validation loss: 2.0660752058029175

Epoch: 6| Step: 5
Training loss: 1.9594154357910156
Validation loss: 2.078773041566213

Epoch: 6| Step: 6
Training loss: 2.314380168914795
Validation loss: 2.0661804477373757

Epoch: 6| Step: 7
Training loss: 1.9004567861557007
Validation loss: 2.067251980304718

Epoch: 6| Step: 8
Training loss: 2.2505669593811035
Validation loss: 2.08015384276708

Epoch: 6| Step: 9
Training loss: 2.075584650039673
Validation loss: 2.06315807501475

Epoch: 6| Step: 10
Training loss: 1.9059051275253296
Validation loss: 2.074666957060496

Epoch: 6| Step: 11
Training loss: 1.8853425979614258
Validation loss: 2.076531151930491

Epoch: 6| Step: 12
Training loss: 2.125856399536133
Validation loss: 2.0565027793248496

Epoch: 6| Step: 13
Training loss: 2.3039236068725586
Validation loss: 2.073165774345398

Epoch: 147| Step: 0
Training loss: 2.365743637084961
Validation loss: 2.0709045926729837

Epoch: 6| Step: 1
Training loss: 2.1568281650543213
Validation loss: 2.0880202651023865

Epoch: 6| Step: 2
Training loss: 2.153563976287842
Validation loss: 2.066164751847585

Epoch: 6| Step: 3
Training loss: 1.2607076168060303
Validation loss: 2.0668229262034097

Epoch: 6| Step: 4
Training loss: 1.2668931484222412
Validation loss: 2.0695579648017883

Epoch: 6| Step: 5
Training loss: 2.0022552013397217
Validation loss: 2.0642773111661277

Epoch: 6| Step: 6
Training loss: 2.2835693359375
Validation loss: 2.0726542870203652

Epoch: 6| Step: 7
Training loss: 2.0520071983337402
Validation loss: 2.0752982099850974

Epoch: 6| Step: 8
Training loss: 1.9976812601089478
Validation loss: 2.0943604111671448

Epoch: 6| Step: 9
Training loss: 1.9024336338043213
Validation loss: 2.0843409299850464

Epoch: 6| Step: 10
Training loss: 2.733375072479248
Validation loss: 2.096185823281606

Epoch: 6| Step: 11
Training loss: 1.675086259841919
Validation loss: 2.0814943512280784

Epoch: 6| Step: 12
Training loss: 2.479095935821533
Validation loss: 2.1037318309148154

Epoch: 6| Step: 13
Training loss: 1.604268193244934
Validation loss: 2.073191146055857

Epoch: 148| Step: 0
Training loss: 2.4124374389648438
Validation loss: 2.0915141701698303

Epoch: 6| Step: 1
Training loss: 2.1250967979431152
Validation loss: 2.0744961500167847

Epoch: 6| Step: 2
Training loss: 2.32061767578125
Validation loss: 2.084499935309092

Epoch: 6| Step: 3
Training loss: 2.6131277084350586
Validation loss: 2.0792445739110312

Epoch: 6| Step: 4
Training loss: 1.465320110321045
Validation loss: 2.0947689215342202

Epoch: 6| Step: 5
Training loss: 1.9862478971481323
Validation loss: 2.081416666507721

Epoch: 6| Step: 6
Training loss: 2.15604829788208
Validation loss: 2.0886518955230713

Epoch: 6| Step: 7
Training loss: 1.9392856359481812
Validation loss: 2.0887990991274514

Epoch: 6| Step: 8
Training loss: 1.8865892887115479
Validation loss: 2.084710677464803

Epoch: 6| Step: 9
Training loss: 1.6594698429107666
Validation loss: 2.0871965885162354

Epoch: 6| Step: 10
Training loss: 2.1528964042663574
Validation loss: 2.074247201283773

Epoch: 6| Step: 11
Training loss: 2.32220458984375
Validation loss: 2.072836776574453

Epoch: 6| Step: 12
Training loss: 1.288326621055603
Validation loss: 2.066933274269104

Epoch: 6| Step: 13
Training loss: 1.6523703336715698
Validation loss: 2.0657087564468384

Epoch: 149| Step: 0
Training loss: 2.106308937072754
Validation loss: 2.0724802811940513

Epoch: 6| Step: 1
Training loss: 2.3061327934265137
Validation loss: 2.0762759844462075

Epoch: 6| Step: 2
Training loss: 2.1705095767974854
Validation loss: 2.0779441793759665

Epoch: 6| Step: 3
Training loss: 2.4521074295043945
Validation loss: 2.0684162179629006

Epoch: 6| Step: 4
Training loss: 1.9635409116744995
Validation loss: 2.078929920991262

Epoch: 6| Step: 5
Training loss: 1.844016194343567
Validation loss: 2.0845876336097717

Epoch: 6| Step: 6
Training loss: 1.848510503768921
Validation loss: 2.0815054774284363

Epoch: 6| Step: 7
Training loss: 1.8649170398712158
Validation loss: 2.0939916570981345

Epoch: 6| Step: 8
Training loss: 1.497753620147705
Validation loss: 2.0791823466618857

Epoch: 6| Step: 9
Training loss: 2.523101329803467
Validation loss: 2.0874180992444358

Epoch: 6| Step: 10
Training loss: 1.7686811685562134
Validation loss: 2.1057435472806296

Epoch: 6| Step: 11
Training loss: 2.3179192543029785
Validation loss: 2.093187093734741

Epoch: 6| Step: 12
Training loss: 1.527146816253662
Validation loss: 2.0951735575993857

Epoch: 6| Step: 13
Training loss: 1.984142541885376
Validation loss: 2.089054604371389

Epoch: 150| Step: 0
Training loss: 1.5070503950119019
Validation loss: 2.0993811090787253

Epoch: 6| Step: 1
Training loss: 2.170409679412842
Validation loss: 2.099718451499939

Epoch: 6| Step: 2
Training loss: 2.1657092571258545
Validation loss: 2.1058265765508017

Epoch: 6| Step: 3
Training loss: 2.274662971496582
Validation loss: 2.0978525082270303

Epoch: 6| Step: 4
Training loss: 2.5195062160491943
Validation loss: 2.0736684997876487

Epoch: 6| Step: 5
Training loss: 1.8621684312820435
Validation loss: 2.0677124659220376

Epoch: 6| Step: 6
Training loss: 2.688962936401367
Validation loss: 2.0798743963241577

Epoch: 6| Step: 7
Training loss: 1.7838369607925415
Validation loss: 2.067128837108612

Epoch: 6| Step: 8
Training loss: 2.2169852256774902
Validation loss: 2.0744099020957947

Epoch: 6| Step: 9
Training loss: 1.851905107498169
Validation loss: 2.089489301045736

Epoch: 6| Step: 10
Training loss: 1.671147346496582
Validation loss: 2.0772550106048584

Epoch: 6| Step: 11
Training loss: 2.08674955368042
Validation loss: 2.098332862059275

Epoch: 6| Step: 12
Training loss: 1.820626974105835
Validation loss: 2.1057454148928323

Epoch: 6| Step: 13
Training loss: 1.827972650527954
Validation loss: 2.0987290342648826

Testing loss: 1.6892786351896876

Epoch: 1| Step: 0
Training loss: 5.515057499465632
Validation loss: 5.955214486667492

Epoch: 6| Step: 1
Training loss: 5.78823710746682
Validation loss: 5.95315020431563

Epoch: 6| Step: 2
Training loss: 5.306163464526915
Validation loss: 5.951051793244801

Epoch: 6| Step: 3
Training loss: 5.844200728878486
Validation loss: 5.948901690030463

Epoch: 6| Step: 4
Training loss: 6.115460187263257
Validation loss: 5.946979357548304

Epoch: 6| Step: 5
Training loss: 6.539329678308134
Validation loss: 5.944872164366462

Epoch: 6| Step: 6
Training loss: 6.89199282678905
Validation loss: 5.942708966330099

Epoch: 6| Step: 7
Training loss: 6.200496087685116
Validation loss: 5.940797418871161

Epoch: 6| Step: 8
Training loss: 5.802440182608857
Validation loss: 5.938610173889537

Epoch: 6| Step: 9
Training loss: 6.840928981225567
Validation loss: 5.93639312599445

Epoch: 6| Step: 10
Training loss: 5.779968779588483
Validation loss: 5.93417661582021

Epoch: 6| Step: 11
Training loss: 6.26332929225681
Validation loss: 5.932027148298895

Epoch: 6| Step: 12
Training loss: 5.2219645028516775
Validation loss: 5.929631745181444

Epoch: 6| Step: 13
Training loss: 6.402231733309267
Validation loss: 5.92725138322654

Epoch: 2| Step: 0
Training loss: 5.014861336918063
Validation loss: 5.924933268711821

Epoch: 6| Step: 1
Training loss: 6.247917133402089
Validation loss: 5.92244704946795

Epoch: 6| Step: 2
Training loss: 6.100266947924287
Validation loss: 5.919936480997598

Epoch: 6| Step: 3
Training loss: 6.878534691484308
Validation loss: 5.9172895488697845

Epoch: 6| Step: 4
Training loss: 7.363412838678306
Validation loss: 5.914462507285119

Epoch: 6| Step: 5
Training loss: 5.464627526827241
Validation loss: 5.911441196859339

Epoch: 6| Step: 6
Training loss: 5.726784986725073
Validation loss: 5.908417722729757

Epoch: 6| Step: 7
Training loss: 6.322062586039217
Validation loss: 5.90532576176794

Epoch: 6| Step: 8
Training loss: 6.131573322259957
Validation loss: 5.901872415361259

Epoch: 6| Step: 9
Training loss: 5.585407877536543
Validation loss: 5.8983219766988055

Epoch: 6| Step: 10
Training loss: 5.57310880050538
Validation loss: 5.894421070199144

Epoch: 6| Step: 11
Training loss: 6.421506699444891
Validation loss: 5.890714204552932

Epoch: 6| Step: 12
Training loss: 4.974520038091614
Validation loss: 5.8866401503424015

Epoch: 6| Step: 13
Training loss: 6.023740214102214
Validation loss: 5.882370674256168

Epoch: 3| Step: 0
Training loss: 6.417959834328865
Validation loss: 5.877696507418281

Epoch: 6| Step: 1
Training loss: 6.860400981249941
Validation loss: 5.8732158305480064

Epoch: 6| Step: 2
Training loss: 6.066332991649582
Validation loss: 5.868057965259948

Epoch: 6| Step: 3
Training loss: 5.815527670905234
Validation loss: 5.862830388479855

Epoch: 6| Step: 4
Training loss: 5.495591043718574
Validation loss: 5.857392188379951

Epoch: 6| Step: 5
Training loss: 6.815076988048436
Validation loss: 5.851509694980367

Epoch: 6| Step: 6
Training loss: 5.701584699424958
Validation loss: 5.845531851547963

Epoch: 6| Step: 7
Training loss: 5.836382296156136
Validation loss: 5.839406185704492

Epoch: 6| Step: 8
Training loss: 6.242337221491355
Validation loss: 5.833185366615848

Epoch: 6| Step: 9
Training loss: 6.048678343621013
Validation loss: 5.826062666811222

Epoch: 6| Step: 10
Training loss: 4.995482120253213
Validation loss: 5.819267134695284

Epoch: 6| Step: 11
Training loss: 5.696409520118654
Validation loss: 5.81200562702492

Epoch: 6| Step: 12
Training loss: 5.592462316871255
Validation loss: 5.804780051450649

Epoch: 6| Step: 13
Training loss: 5.531888935665032
Validation loss: 5.797251014818725

Epoch: 4| Step: 0
Training loss: 5.377351290935708
Validation loss: 5.789283957559141

Epoch: 6| Step: 1
Training loss: 6.300216961712385
Validation loss: 5.781201061694614

Epoch: 6| Step: 2
Training loss: 6.104488359986202
Validation loss: 5.7725249564257926

Epoch: 6| Step: 3
Training loss: 5.92295259922581
Validation loss: 5.763931232383672

Epoch: 6| Step: 4
Training loss: 6.227963470645859
Validation loss: 5.754967299257156

Epoch: 6| Step: 5
Training loss: 6.559396645781489
Validation loss: 5.74567286447355

Epoch: 6| Step: 6
Training loss: 5.274440189686029
Validation loss: 5.736437268204321

Epoch: 6| Step: 7
Training loss: 5.538740872405944
Validation loss: 5.726772802358939

Epoch: 6| Step: 8
Training loss: 5.375264715728906
Validation loss: 5.717261448976437

Epoch: 6| Step: 9
Training loss: 5.838510377671327
Validation loss: 5.707706871834073

Epoch: 6| Step: 10
Training loss: 5.995572045707852
Validation loss: 5.6981760927195415

Epoch: 6| Step: 11
Training loss: 5.6413320166967145
Validation loss: 5.6885197602079405

Epoch: 6| Step: 12
Training loss: 5.411205512251886
Validation loss: 5.679114759305465

Epoch: 6| Step: 13
Training loss: 6.106702917788211
Validation loss: 5.66977374594797

Epoch: 5| Step: 0
Training loss: 5.691002071484135
Validation loss: 5.660445594456277

Epoch: 6| Step: 1
Training loss: 5.180403301784584
Validation loss: 5.6511748923727865

Epoch: 6| Step: 2
Training loss: 6.000560098572635
Validation loss: 5.642587617941204

Epoch: 6| Step: 3
Training loss: 5.78839296904266
Validation loss: 5.633403942786722

Epoch: 6| Step: 4
Training loss: 6.4823053573062515
Validation loss: 5.625133201999977

Epoch: 6| Step: 5
Training loss: 4.9036986891264265
Validation loss: 5.616456077468917

Epoch: 6| Step: 6
Training loss: 5.870784464574746
Validation loss: 5.608196279920975

Epoch: 6| Step: 7
Training loss: 5.588232974592265
Validation loss: 5.600212356537366

Epoch: 6| Step: 8
Training loss: 5.871254234125642
Validation loss: 5.592300767187584

Epoch: 6| Step: 9
Training loss: 5.576895613058853
Validation loss: 5.584303169094486

Epoch: 6| Step: 10
Training loss: 6.04616524722715
Validation loss: 5.576217481935501

Epoch: 6| Step: 11
Training loss: 6.042146159470354
Validation loss: 5.568259497096559

Epoch: 6| Step: 12
Training loss: 5.540287199873314
Validation loss: 5.560107941940467

Epoch: 6| Step: 13
Training loss: 5.366693510063438
Validation loss: 5.551800871553189

Epoch: 6| Step: 0
Training loss: 4.950687229731935
Validation loss: 5.543725494135638

Epoch: 6| Step: 1
Training loss: 5.739302594287814
Validation loss: 5.536053139999334

Epoch: 6| Step: 2
Training loss: 5.630361079126955
Validation loss: 5.5279196503205785

Epoch: 6| Step: 3
Training loss: 6.187141793411344
Validation loss: 5.519616689918376

Epoch: 6| Step: 4
Training loss: 5.862361868916734
Validation loss: 5.51146514474951

Epoch: 6| Step: 5
Training loss: 4.988351222468319
Validation loss: 5.503067288375029

Epoch: 6| Step: 6
Training loss: 5.962237415089782
Validation loss: 5.494763077455123

Epoch: 6| Step: 7
Training loss: 6.396621837740495
Validation loss: 5.4863170869471105

Epoch: 6| Step: 8
Training loss: 5.563561670376541
Validation loss: 5.477853083313215

Epoch: 6| Step: 9
Training loss: 5.998109519678563
Validation loss: 5.470097577412005

Epoch: 6| Step: 10
Training loss: 4.128457672090649
Validation loss: 5.461950292796206

Epoch: 6| Step: 11
Training loss: 4.922304358752648
Validation loss: 5.454359641667111

Epoch: 6| Step: 12
Training loss: 6.085461422830628
Validation loss: 5.447125447319406

Epoch: 6| Step: 13
Training loss: 5.7057102980157355
Validation loss: 5.439769022984079

Epoch: 7| Step: 0
Training loss: 6.093056350745443
Validation loss: 5.431848534138546

Epoch: 6| Step: 1
Training loss: 4.313844775184286
Validation loss: 5.424830031772105

Epoch: 6| Step: 2
Training loss: 5.866880609483842
Validation loss: 5.417801615826267

Epoch: 6| Step: 3
Training loss: 5.825806184571824
Validation loss: 5.410766097124524

Epoch: 6| Step: 4
Training loss: 5.2148046142024
Validation loss: 5.403603407605163

Epoch: 6| Step: 5
Training loss: 5.368426894744749
Validation loss: 5.396653347474113

Epoch: 6| Step: 6
Training loss: 6.721278934483877
Validation loss: 5.389972002219745

Epoch: 6| Step: 7
Training loss: 6.016343428082285
Validation loss: 5.383735507472915

Epoch: 6| Step: 8
Training loss: 4.7209364518128885
Validation loss: 5.376943547371601

Epoch: 6| Step: 9
Training loss: 5.176901780238478
Validation loss: 5.371012398768392

Epoch: 6| Step: 10
Training loss: 5.154086127097799
Validation loss: 5.365097890329968

Epoch: 6| Step: 11
Training loss: 4.443704699836679
Validation loss: 5.3595703504602294

Epoch: 6| Step: 12
Training loss: 5.438375819035704
Validation loss: 5.353512537242208

Epoch: 6| Step: 13
Training loss: 6.222689353725506
Validation loss: 5.347835235907005

Epoch: 8| Step: 0
Training loss: 5.282517501144918
Validation loss: 5.341796666714271

Epoch: 6| Step: 1
Training loss: 5.647734581143828
Validation loss: 5.335474279952334

Epoch: 6| Step: 2
Training loss: 5.193820274419279
Validation loss: 5.3295744623838015

Epoch: 6| Step: 3
Training loss: 5.784432849563843
Validation loss: 5.323776155819412

Epoch: 6| Step: 4
Training loss: 6.090617935074789
Validation loss: 5.318202888389726

Epoch: 6| Step: 5
Training loss: 4.8957714753267485
Validation loss: 5.312369281432144

Epoch: 6| Step: 6
Training loss: 6.05775879764048
Validation loss: 5.307260210607248

Epoch: 6| Step: 7
Training loss: 4.653169368125024
Validation loss: 5.301614027356602

Epoch: 6| Step: 8
Training loss: 5.886697041370888
Validation loss: 5.296576627506582

Epoch: 6| Step: 9
Training loss: 4.248907060899097
Validation loss: 5.2911073684483325

Epoch: 6| Step: 10
Training loss: 5.893450719489953
Validation loss: 5.2858900043437185

Epoch: 6| Step: 11
Training loss: 5.331050841355775
Validation loss: 5.28060057369661

Epoch: 6| Step: 12
Training loss: 5.265460184387414
Validation loss: 5.275313522639768

Epoch: 6| Step: 13
Training loss: 5.420065278226754
Validation loss: 5.269799092062535

Epoch: 9| Step: 0
Training loss: 5.153040771536282
Validation loss: 5.264798606071062

Epoch: 6| Step: 1
Training loss: 5.62667957238138
Validation loss: 5.259391196232211

Epoch: 6| Step: 2
Training loss: 5.826384665490571
Validation loss: 5.254290023835924

Epoch: 6| Step: 3
Training loss: 6.263172459237443
Validation loss: 5.248601515261707

Epoch: 6| Step: 4
Training loss: 3.9870896609605575
Validation loss: 5.243212141627825

Epoch: 6| Step: 5
Training loss: 5.465013197370755
Validation loss: 5.238086352609141

Epoch: 6| Step: 6
Training loss: 4.618714881938972
Validation loss: 5.232751395232401

Epoch: 6| Step: 7
Training loss: 5.673566245569985
Validation loss: 5.227713738937825

Epoch: 6| Step: 8
Training loss: 4.576576361565193
Validation loss: 5.222390190200849

Epoch: 6| Step: 9
Training loss: 5.4315667063358415
Validation loss: 5.216860613807862

Epoch: 6| Step: 10
Training loss: 5.533037317163603
Validation loss: 5.2120942638633405

Epoch: 6| Step: 11
Training loss: 4.685753865855721
Validation loss: 5.206412955699449

Epoch: 6| Step: 12
Training loss: 5.72663744035634
Validation loss: 5.200733142913177

Epoch: 6| Step: 13
Training loss: 5.901339311043087
Validation loss: 5.19537910284885

Epoch: 10| Step: 0
Training loss: 5.599705940427827
Validation loss: 5.189984994130378

Epoch: 6| Step: 1
Training loss: 5.65471008582979
Validation loss: 5.184268910321057

Epoch: 6| Step: 2
Training loss: 5.200145888116066
Validation loss: 5.178316712653076

Epoch: 6| Step: 3
Training loss: 5.117865830517609
Validation loss: 5.1725734150763545

Epoch: 6| Step: 4
Training loss: 5.5208017408318755
Validation loss: 5.167115386838041

Epoch: 6| Step: 5
Training loss: 5.135689914493198
Validation loss: 5.161569937363267

Epoch: 6| Step: 6
Training loss: 5.0378971142108515
Validation loss: 5.156016735618939

Epoch: 6| Step: 7
Training loss: 6.064715423071539
Validation loss: 5.149931301658713

Epoch: 6| Step: 8
Training loss: 5.147091216297959
Validation loss: 5.144525874083657

Epoch: 6| Step: 9
Training loss: 4.537733188658747
Validation loss: 5.138310120899597

Epoch: 6| Step: 10
Training loss: 5.014657090577978
Validation loss: 5.131911726614874

Epoch: 6| Step: 11
Training loss: 5.087322275117648
Validation loss: 5.126111786126394

Epoch: 6| Step: 12
Training loss: 5.487488474214654
Validation loss: 5.120551993595633

Epoch: 6| Step: 13
Training loss: 5.101836543295165
Validation loss: 5.114751135824384

Epoch: 11| Step: 0
Training loss: 4.869350583369385
Validation loss: 5.1083967884265

Epoch: 6| Step: 1
Training loss: 5.600621951851293
Validation loss: 5.102384648032353

Epoch: 6| Step: 2
Training loss: 4.103675748378172
Validation loss: 5.0963749275181005

Epoch: 6| Step: 3
Training loss: 5.389272857480223
Validation loss: 5.0900091933371625

Epoch: 6| Step: 4
Training loss: 5.464347418713633
Validation loss: 5.084015889020653

Epoch: 6| Step: 5
Training loss: 5.667645351193412
Validation loss: 5.077759527112873

Epoch: 6| Step: 6
Training loss: 5.0425619586245425
Validation loss: 5.072649980724571

Epoch: 6| Step: 7
Training loss: 4.31084598390198
Validation loss: 5.066097315788671

Epoch: 6| Step: 8
Training loss: 6.197063415094534
Validation loss: 5.059459762381573

Epoch: 6| Step: 9
Training loss: 5.241558736509709
Validation loss: 5.054307721909088

Epoch: 6| Step: 10
Training loss: 4.281000951498203
Validation loss: 5.047710306271471

Epoch: 6| Step: 11
Training loss: 5.7996441172891835
Validation loss: 5.041373909050882

Epoch: 6| Step: 12
Training loss: 5.4145993713791105
Validation loss: 5.034587314011345

Epoch: 6| Step: 13
Training loss: 4.829819267793144
Validation loss: 5.027065108332485

Epoch: 12| Step: 0
Training loss: 5.446370048119188
Validation loss: 5.020357808536807

Epoch: 6| Step: 1
Training loss: 5.425867375238879
Validation loss: 5.013518591274935

Epoch: 6| Step: 2
Training loss: 5.970764139878789
Validation loss: 5.006719365780174

Epoch: 6| Step: 3
Training loss: 4.42856116667231
Validation loss: 5.000970110559774

Epoch: 6| Step: 4
Training loss: 4.1663351817197265
Validation loss: 4.991706550366632

Epoch: 6| Step: 5
Training loss: 4.786559504068275
Validation loss: 4.985248708333378

Epoch: 6| Step: 6
Training loss: 5.662230643966962
Validation loss: 4.978612935494645

Epoch: 6| Step: 7
Training loss: 5.186699127514094
Validation loss: 4.9721485087485195

Epoch: 6| Step: 8
Training loss: 5.616939958362526
Validation loss: 4.964520491654403

Epoch: 6| Step: 9
Training loss: 4.166242247264389
Validation loss: 4.956576070686575

Epoch: 6| Step: 10
Training loss: 4.232844565302826
Validation loss: 4.949344030258387

Epoch: 6| Step: 11
Training loss: 5.328705607799125
Validation loss: 4.9432746501139455

Epoch: 6| Step: 12
Training loss: 6.067192856881809
Validation loss: 4.935593120125319

Epoch: 6| Step: 13
Training loss: 4.308231618677874
Validation loss: 4.928101556045285

Epoch: 13| Step: 0
Training loss: 4.382235619969636
Validation loss: 4.920011234115552

Epoch: 6| Step: 1
Training loss: 4.9598762398323135
Validation loss: 4.913572285939345

Epoch: 6| Step: 2
Training loss: 5.23003405587248
Validation loss: 4.907314403753605

Epoch: 6| Step: 3
Training loss: 4.82966900182987
Validation loss: 4.899493738866951

Epoch: 6| Step: 4
Training loss: 4.968986841471096
Validation loss: 4.892006838319896

Epoch: 6| Step: 5
Training loss: 4.838832401647164
Validation loss: 4.882448749471834

Epoch: 6| Step: 6
Training loss: 4.900048874591831
Validation loss: 4.875383476107617

Epoch: 6| Step: 7
Training loss: 4.85093343345188
Validation loss: 4.868455259438992

Epoch: 6| Step: 8
Training loss: 5.965261028810533
Validation loss: 4.860131567703427

Epoch: 6| Step: 9
Training loss: 5.116894709240566
Validation loss: 4.851792842456535

Epoch: 6| Step: 10
Training loss: 5.216317346514291
Validation loss: 4.844799018394433

Epoch: 6| Step: 11
Training loss: 4.327794251732302
Validation loss: 4.83810982421261

Epoch: 6| Step: 12
Training loss: 4.712567656256504
Validation loss: 4.831299759582649

Epoch: 6| Step: 13
Training loss: 5.421526719046216
Validation loss: 4.823976052958114

Epoch: 14| Step: 0
Training loss: 5.283161246368215
Validation loss: 4.817584831785549

Epoch: 6| Step: 1
Training loss: 4.811508274693228
Validation loss: 4.81004918433492

Epoch: 6| Step: 2
Training loss: 4.5827477976994695
Validation loss: 4.80413352026289

Epoch: 6| Step: 3
Training loss: 5.320426144275424
Validation loss: 4.7960119144036115

Epoch: 6| Step: 4
Training loss: 5.2768559883423105
Validation loss: 4.789940675223424

Epoch: 6| Step: 5
Training loss: 4.973516708927345
Validation loss: 4.7836211398176065

Epoch: 6| Step: 6
Training loss: 4.338245664148687
Validation loss: 4.777046067494127

Epoch: 6| Step: 7
Training loss: 4.725743448210626
Validation loss: 4.770382900157959

Epoch: 6| Step: 8
Training loss: 5.188066198863413
Validation loss: 4.76361718745302

Epoch: 6| Step: 9
Training loss: 5.086988208718842
Validation loss: 4.756820500128923

Epoch: 6| Step: 10
Training loss: 4.777991686820538
Validation loss: 4.749312049839095

Epoch: 6| Step: 11
Training loss: 4.951966544338724
Validation loss: 4.742311479182739

Epoch: 6| Step: 12
Training loss: 4.2755773232563055
Validation loss: 4.735600714652938

Epoch: 6| Step: 13
Training loss: 4.87814425281975
Validation loss: 4.730237203864282

Epoch: 15| Step: 0
Training loss: 5.151607758748719
Validation loss: 4.724667373601911

Epoch: 6| Step: 1
Training loss: 4.461455467069413
Validation loss: 4.7175889428882725

Epoch: 6| Step: 2
Training loss: 5.074867870696664
Validation loss: 4.711315403463482

Epoch: 6| Step: 3
Training loss: 5.015117679025354
Validation loss: 4.706453094220588

Epoch: 6| Step: 4
Training loss: 4.452061147936518
Validation loss: 4.698651421965666

Epoch: 6| Step: 5
Training loss: 4.165378956493189
Validation loss: 4.690892530483293

Epoch: 6| Step: 6
Training loss: 5.267778720225791
Validation loss: 4.6842791872615175

Epoch: 6| Step: 7
Training loss: 4.983561003894807
Validation loss: 4.678564430447901

Epoch: 6| Step: 8
Training loss: 4.9818765243855285
Validation loss: 4.672365943178932

Epoch: 6| Step: 9
Training loss: 4.413999411883227
Validation loss: 4.665726816812787

Epoch: 6| Step: 10
Training loss: 4.941430760499982
Validation loss: 4.660480633999943

Epoch: 6| Step: 11
Training loss: 5.117360072829567
Validation loss: 4.653923187216501

Epoch: 6| Step: 12
Training loss: 4.769591235034341
Validation loss: 4.64667716581088

Epoch: 6| Step: 13
Training loss: 4.367876848056735
Validation loss: 4.640600801655265

Epoch: 16| Step: 0
Training loss: 4.206371769269059
Validation loss: 4.63470732444193

Epoch: 6| Step: 1
Training loss: 5.214372455002357
Validation loss: 4.628537517728382

Epoch: 6| Step: 2
Training loss: 4.52171511627761
Validation loss: 4.621995276347043

Epoch: 6| Step: 3
Training loss: 4.622149026935118
Validation loss: 4.616546121577404

Epoch: 6| Step: 4
Training loss: 3.943188630063952
Validation loss: 4.609601307286549

Epoch: 6| Step: 5
Training loss: 4.7039763735093505
Validation loss: 4.604449900665226

Epoch: 6| Step: 6
Training loss: 4.994367478715515
Validation loss: 4.598316059935036

Epoch: 6| Step: 7
Training loss: 4.828532278913113
Validation loss: 4.591063858082226

Epoch: 6| Step: 8
Training loss: 5.1951028652489235
Validation loss: 4.587210673980039

Epoch: 6| Step: 9
Training loss: 4.816160717078945
Validation loss: 4.5797879580591605

Epoch: 6| Step: 10
Training loss: 5.308585845514719
Validation loss: 4.57510123358005

Epoch: 6| Step: 11
Training loss: 4.40108452349079
Validation loss: 4.567314325242883

Epoch: 6| Step: 12
Training loss: 4.614829350351032
Validation loss: 4.56165699595517

Epoch: 6| Step: 13
Training loss: 4.535472086735905
Validation loss: 4.555206391410958

Epoch: 17| Step: 0
Training loss: 4.4721837223099135
Validation loss: 4.549171051556222

Epoch: 6| Step: 1
Training loss: 4.897939458956543
Validation loss: 4.543965355808202

Epoch: 6| Step: 2
Training loss: 4.476809190274145
Validation loss: 4.537675392817456

Epoch: 6| Step: 3
Training loss: 4.385763823600884
Validation loss: 4.532068270759117

Epoch: 6| Step: 4
Training loss: 4.9937424603573595
Validation loss: 4.525800282224137

Epoch: 6| Step: 5
Training loss: 4.322650628540421
Validation loss: 4.520120954273661

Epoch: 6| Step: 6
Training loss: 4.717332639732619
Validation loss: 4.514115605977773

Epoch: 6| Step: 7
Training loss: 5.600364823038142
Validation loss: 4.508036783634143

Epoch: 6| Step: 8
Training loss: 4.487544730555923
Validation loss: 4.501936178113439

Epoch: 6| Step: 9
Training loss: 4.261686078443739
Validation loss: 4.496397748889331

Epoch: 6| Step: 10
Training loss: 5.040819249250392
Validation loss: 4.493505559859885

Epoch: 6| Step: 11
Training loss: 4.681591735508182
Validation loss: 4.48423637503732

Epoch: 6| Step: 12
Training loss: 4.605805233480339
Validation loss: 4.4786908392299765

Epoch: 6| Step: 13
Training loss: 3.759768415177536
Validation loss: 4.474717288614888

Epoch: 18| Step: 0
Training loss: 4.621830034327413
Validation loss: 4.46900069593935

Epoch: 6| Step: 1
Training loss: 4.091574054195622
Validation loss: 4.460456354545811

Epoch: 6| Step: 2
Training loss: 4.478422780328498
Validation loss: 4.456151993125434

Epoch: 6| Step: 3
Training loss: 5.453829044920706
Validation loss: 4.45202739194296

Epoch: 6| Step: 4
Training loss: 3.89888930521643
Validation loss: 4.4462576041926525

Epoch: 6| Step: 5
Training loss: 4.129860528618351
Validation loss: 4.439421761465323

Epoch: 6| Step: 6
Training loss: 4.274487354259413
Validation loss: 4.433287506057871

Epoch: 6| Step: 7
Training loss: 3.819852149858335
Validation loss: 4.427734231408982

Epoch: 6| Step: 8
Training loss: 4.585805861811169
Validation loss: 4.421846261621912

Epoch: 6| Step: 9
Training loss: 5.388234899541927
Validation loss: 4.4171720731412085

Epoch: 6| Step: 10
Training loss: 4.956544868770262
Validation loss: 4.410839711500628

Epoch: 6| Step: 11
Training loss: 3.9491028358359843
Validation loss: 4.40556995744397

Epoch: 6| Step: 12
Training loss: 5.298745694276825
Validation loss: 4.399736572818529

Epoch: 6| Step: 13
Training loss: 4.4525668681208925
Validation loss: 4.39341885877573

Epoch: 19| Step: 0
Training loss: 5.009633130557885
Validation loss: 4.3890348838045075

Epoch: 6| Step: 1
Training loss: 4.612558074006655
Validation loss: 4.384334850603265

Epoch: 6| Step: 2
Training loss: 4.150573295612424
Validation loss: 4.378985070487044

Epoch: 6| Step: 3
Training loss: 4.30200976046382
Validation loss: 4.373321828962717

Epoch: 6| Step: 4
Training loss: 4.673059829965638
Validation loss: 4.366685432413172

Epoch: 6| Step: 5
Training loss: 4.6136161301506995
Validation loss: 4.360344071504623

Epoch: 6| Step: 6
Training loss: 4.796615345043893
Validation loss: 4.355398755159732

Epoch: 6| Step: 7
Training loss: 4.199360235625161
Validation loss: 4.348341292352924

Epoch: 6| Step: 8
Training loss: 3.8700203432243367
Validation loss: 4.34266868689723

Epoch: 6| Step: 9
Training loss: 4.83414417349511
Validation loss: 4.338076923417043

Epoch: 6| Step: 10
Training loss: 4.070437614289915
Validation loss: 4.331803412446982

Epoch: 6| Step: 11
Training loss: 4.543344496901957
Validation loss: 4.327395160711941

Epoch: 6| Step: 12
Training loss: 4.049108882873036
Validation loss: 4.321953938228296

Epoch: 6| Step: 13
Training loss: 4.8560775902973665
Validation loss: 4.317313893718752

Epoch: 20| Step: 0
Training loss: 3.958330187879952
Validation loss: 4.311679725219235

Epoch: 6| Step: 1
Training loss: 4.9537907583180445
Validation loss: 4.305616921155045

Epoch: 6| Step: 2
Training loss: 4.493249704580733
Validation loss: 4.299900316960749

Epoch: 6| Step: 3
Training loss: 4.477058849146469
Validation loss: 4.2944371746627334

Epoch: 6| Step: 4
Training loss: 3.830198263708923
Validation loss: 4.288652781480983

Epoch: 6| Step: 5
Training loss: 4.931404697326955
Validation loss: 4.283294449467682

Epoch: 6| Step: 6
Training loss: 4.912415730382096
Validation loss: 4.278144545472152

Epoch: 6| Step: 7
Training loss: 4.275337313009892
Validation loss: 4.272912159952546

Epoch: 6| Step: 8
Training loss: 4.539492141953806
Validation loss: 4.2672719791802916

Epoch: 6| Step: 9
Training loss: 4.279550479563117
Validation loss: 4.263277817027396

Epoch: 6| Step: 10
Training loss: 4.199047053175893
Validation loss: 4.257151234457905

Epoch: 6| Step: 11
Training loss: 4.423470037590615
Validation loss: 4.252189913505317

Epoch: 6| Step: 12
Training loss: 4.03751353920004
Validation loss: 4.2483768729583655

Epoch: 6| Step: 13
Training loss: 4.193740521802581
Validation loss: 4.242064266393666

Epoch: 21| Step: 0
Training loss: 4.198555997845094
Validation loss: 4.236213800960946

Epoch: 6| Step: 1
Training loss: 5.037783722188059
Validation loss: 4.2310165844259195

Epoch: 6| Step: 2
Training loss: 4.451308351695052
Validation loss: 4.225654866557296

Epoch: 6| Step: 3
Training loss: 3.8126667173861093
Validation loss: 4.221048327158108

Epoch: 6| Step: 4
Training loss: 4.351431880613679
Validation loss: 4.215918751806575

Epoch: 6| Step: 5
Training loss: 3.8178507044094414
Validation loss: 4.210609022173133

Epoch: 6| Step: 6
Training loss: 4.407852422566569
Validation loss: 4.204687553678881

Epoch: 6| Step: 7
Training loss: 4.137298975552812
Validation loss: 4.199539527173831

Epoch: 6| Step: 8
Training loss: 3.440374386397373
Validation loss: 4.195416213019868

Epoch: 6| Step: 9
Training loss: 5.025609046883199
Validation loss: 4.191256987090619

Epoch: 6| Step: 10
Training loss: 5.105698875838859
Validation loss: 4.185829474657303

Epoch: 6| Step: 11
Training loss: 4.617427090336166
Validation loss: 4.1793197327432745

Epoch: 6| Step: 12
Training loss: 3.739509404690481
Validation loss: 4.173817233671057

Epoch: 6| Step: 13
Training loss: 4.138440516775452
Validation loss: 4.1690532461554115

Epoch: 22| Step: 0
Training loss: 3.989984370992162
Validation loss: 4.163763019988425

Epoch: 6| Step: 1
Training loss: 4.774625290153873
Validation loss: 4.159306962746261

Epoch: 6| Step: 2
Training loss: 3.6675685871474393
Validation loss: 4.154069371488594

Epoch: 6| Step: 3
Training loss: 4.23644968705256
Validation loss: 4.148941919343957

Epoch: 6| Step: 4
Training loss: 4.510789440276692
Validation loss: 4.1437308125461625

Epoch: 6| Step: 5
Training loss: 4.487849042913813
Validation loss: 4.138738987204354

Epoch: 6| Step: 6
Training loss: 4.942842514526746
Validation loss: 4.134449010423477

Epoch: 6| Step: 7
Training loss: 4.764956367989404
Validation loss: 4.129056535029055

Epoch: 6| Step: 8
Training loss: 3.410555297295894
Validation loss: 4.12382648614088

Epoch: 6| Step: 9
Training loss: 4.451441824854994
Validation loss: 4.118544642606722

Epoch: 6| Step: 10
Training loss: 3.7688212778616728
Validation loss: 4.113087841871404

Epoch: 6| Step: 11
Training loss: 4.160602327170132
Validation loss: 4.109153316112257

Epoch: 6| Step: 12
Training loss: 3.9877408041636064
Validation loss: 4.1046500849147

Epoch: 6| Step: 13
Training loss: 4.246590761577277
Validation loss: 4.100806757011309

Epoch: 23| Step: 0
Training loss: 4.887748676812827
Validation loss: 4.095525807774977

Epoch: 6| Step: 1
Training loss: 4.167895072421665
Validation loss: 4.088468488617444

Epoch: 6| Step: 2
Training loss: 4.051624236131192
Validation loss: 4.083979704684082

Epoch: 6| Step: 3
Training loss: 3.9542661685838545
Validation loss: 4.078218227602173

Epoch: 6| Step: 4
Training loss: 3.8642734838654302
Validation loss: 4.0741868961170375

Epoch: 6| Step: 5
Training loss: 3.9456582701591745
Validation loss: 4.0691266129242605

Epoch: 6| Step: 6
Training loss: 4.462996827579357
Validation loss: 4.063951223322248

Epoch: 6| Step: 7
Training loss: 4.400202287012039
Validation loss: 4.058920491307844

Epoch: 6| Step: 8
Training loss: 4.181413924991088
Validation loss: 4.053520862911345

Epoch: 6| Step: 9
Training loss: 2.983157883356369
Validation loss: 4.048024607089795

Epoch: 6| Step: 10
Training loss: 4.399739101651103
Validation loss: 4.043883308992238

Epoch: 6| Step: 11
Training loss: 4.8468808792049085
Validation loss: 4.0392385182496975

Epoch: 6| Step: 12
Training loss: 4.382243236760462
Validation loss: 4.033720535944606

Epoch: 6| Step: 13
Training loss: 3.857649073609738
Validation loss: 4.028738752730725

Epoch: 24| Step: 0
Training loss: 3.8406965331329084
Validation loss: 4.0238298243378425

Epoch: 6| Step: 1
Training loss: 4.348228725365733
Validation loss: 4.019755217278278

Epoch: 6| Step: 2
Training loss: 3.887401302348144
Validation loss: 4.0151740685587995

Epoch: 6| Step: 3
Training loss: 3.8256457961697743
Validation loss: 4.009619966448162

Epoch: 6| Step: 4
Training loss: 4.513196879756018
Validation loss: 4.005042493914631

Epoch: 6| Step: 5
Training loss: 3.72493825259651
Validation loss: 3.999452334899886

Epoch: 6| Step: 6
Training loss: 4.673067584968842
Validation loss: 3.9947123546593564

Epoch: 6| Step: 7
Training loss: 4.3956252978425265
Validation loss: 3.9899183818210298

Epoch: 6| Step: 8
Training loss: 4.048764763357929
Validation loss: 3.98490020681246

Epoch: 6| Step: 9
Training loss: 4.174418270676714
Validation loss: 3.980310079390107

Epoch: 6| Step: 10
Training loss: 4.136246579521289
Validation loss: 3.975337591170797

Epoch: 6| Step: 11
Training loss: 4.482495276155552
Validation loss: 3.9701011740590717

Epoch: 6| Step: 12
Training loss: 2.8891048248805284
Validation loss: 3.9648408836517244

Epoch: 6| Step: 13
Training loss: 4.487196191050979
Validation loss: 3.9602844482430153

Epoch: 25| Step: 0
Training loss: 4.314454009166953
Validation loss: 3.955399232238797

Epoch: 6| Step: 1
Training loss: 3.684085622979114
Validation loss: 3.9493091647108147

Epoch: 6| Step: 2
Training loss: 4.053782808663342
Validation loss: 3.944533469459603

Epoch: 6| Step: 3
Training loss: 4.174376005951369
Validation loss: 3.939430006013285

Epoch: 6| Step: 4
Training loss: 4.205201497551546
Validation loss: 3.9352078098486136

Epoch: 6| Step: 5
Training loss: 3.291036778956494
Validation loss: 3.9299882555034666

Epoch: 6| Step: 6
Training loss: 4.057160845089187
Validation loss: 3.9250474639420085

Epoch: 6| Step: 7
Training loss: 4.59767989652479
Validation loss: 3.920606775389937

Epoch: 6| Step: 8
Training loss: 3.5120303617220787
Validation loss: 3.9153632910468055

Epoch: 6| Step: 9
Training loss: 3.6048870505999226
Validation loss: 3.9113013400671233

Epoch: 6| Step: 10
Training loss: 4.471080892392071
Validation loss: 3.9058950440941613

Epoch: 6| Step: 11
Training loss: 4.116353306961089
Validation loss: 3.900637406647954

Epoch: 6| Step: 12
Training loss: 4.536946261327527
Validation loss: 3.8957829531643697

Epoch: 6| Step: 13
Training loss: 3.9218864288771473
Validation loss: 3.8901465145980643

Epoch: 26| Step: 0
Training loss: 3.512297416085102
Validation loss: 3.8860690239957183

Epoch: 6| Step: 1
Training loss: 3.5635603280439754
Validation loss: 3.8811265853122925

Epoch: 6| Step: 2
Training loss: 4.20302198773046
Validation loss: 3.877039136981451

Epoch: 6| Step: 3
Training loss: 4.469756539902479
Validation loss: 3.8717757521484533

Epoch: 6| Step: 4
Training loss: 4.045820061884182
Validation loss: 3.866043301736662

Epoch: 6| Step: 5
Training loss: 3.65934035938784
Validation loss: 3.8602101069835806

Epoch: 6| Step: 6
Training loss: 3.0627137712878794
Validation loss: 3.8543456714747926

Epoch: 6| Step: 7
Training loss: 3.745714409071542
Validation loss: 3.850027938014797

Epoch: 6| Step: 8
Training loss: 4.210906246260408
Validation loss: 3.8446063403278403

Epoch: 6| Step: 9
Training loss: 3.9378263550190624
Validation loss: 3.839404867864274

Epoch: 6| Step: 10
Training loss: 4.108114186929517
Validation loss: 3.8340149080029433

Epoch: 6| Step: 11
Training loss: 4.493655925079049
Validation loss: 3.8288283759371073

Epoch: 6| Step: 12
Training loss: 4.5418251546568404
Validation loss: 3.8239455471311716

Epoch: 6| Step: 13
Training loss: 3.9694254142768366
Validation loss: 3.818755619858656

Epoch: 27| Step: 0
Training loss: 4.035389748507978
Validation loss: 3.8133708605442527

Epoch: 6| Step: 1
Training loss: 4.277614414238887
Validation loss: 3.8089314211621255

Epoch: 6| Step: 2
Training loss: 4.047216216913214
Validation loss: 3.8038165736026786

Epoch: 6| Step: 3
Training loss: 3.8988692478388094
Validation loss: 3.7989555278684386

Epoch: 6| Step: 4
Training loss: 3.5790269539461077
Validation loss: 3.793692962619927

Epoch: 6| Step: 5
Training loss: 3.9339365502008112
Validation loss: 3.788611840783497

Epoch: 6| Step: 6
Training loss: 4.079679346386912
Validation loss: 3.783914620252819

Epoch: 6| Step: 7
Training loss: 2.989287961043246
Validation loss: 3.7789125390007365

Epoch: 6| Step: 8
Training loss: 3.834749305817051
Validation loss: 3.7736808113965457

Epoch: 6| Step: 9
Training loss: 3.9064072233984426
Validation loss: 3.768256970531261

Epoch: 6| Step: 10
Training loss: 3.9065361223336534
Validation loss: 3.7636592477435857

Epoch: 6| Step: 11
Training loss: 4.115240400298152
Validation loss: 3.759017053371106

Epoch: 6| Step: 12
Training loss: 3.4923985906035147
Validation loss: 3.7539801350499675

Epoch: 6| Step: 13
Training loss: 4.527684354435865
Validation loss: 3.7492578831791357

Epoch: 28| Step: 0
Training loss: 3.804430817830211
Validation loss: 3.7443238427285186

Epoch: 6| Step: 1
Training loss: 4.1590864785062305
Validation loss: 3.739806626364537

Epoch: 6| Step: 2
Training loss: 3.3741851105440683
Validation loss: 3.7348748249014037

Epoch: 6| Step: 3
Training loss: 3.719716435361313
Validation loss: 3.730273624130349

Epoch: 6| Step: 4
Training loss: 3.8440092666149086
Validation loss: 3.7258175599178056

Epoch: 6| Step: 5
Training loss: 2.971719320975023
Validation loss: 3.7209992645503522

Epoch: 6| Step: 6
Training loss: 4.143493119644818
Validation loss: 3.7164814758920297

Epoch: 6| Step: 7
Training loss: 3.9838832177736947
Validation loss: 3.712056801625818

Epoch: 6| Step: 8
Training loss: 4.282658254858222
Validation loss: 3.7074243238652222

Epoch: 6| Step: 9
Training loss: 4.250392671284316
Validation loss: 3.702838570077539

Epoch: 6| Step: 10
Training loss: 3.5356310109381908
Validation loss: 3.6978878539489677

Epoch: 6| Step: 11
Training loss: 3.7322965448198406
Validation loss: 3.692904977803994

Epoch: 6| Step: 12
Training loss: 3.9119081335924166
Validation loss: 3.688722203183469

Epoch: 6| Step: 13
Training loss: 3.9715275694298295
Validation loss: 3.684028456904886

Epoch: 29| Step: 0
Training loss: 3.9175274220999747
Validation loss: 3.679154148850939

Epoch: 6| Step: 1
Training loss: 3.8643423384030076
Validation loss: 3.6745218895857845

Epoch: 6| Step: 2
Training loss: 3.9157243501399965
Validation loss: 3.669604721021446

Epoch: 6| Step: 3
Training loss: 3.027564886076646
Validation loss: 3.6649192345269315

Epoch: 6| Step: 4
Training loss: 3.3529992925254652
Validation loss: 3.6601830219930376

Epoch: 6| Step: 5
Training loss: 4.271860783740975
Validation loss: 3.6553694705372344

Epoch: 6| Step: 6
Training loss: 3.047630803193399
Validation loss: 3.65148315082305

Epoch: 6| Step: 7
Training loss: 4.00412894769396
Validation loss: 3.6466462600481075

Epoch: 6| Step: 8
Training loss: 3.4217721583967995
Validation loss: 3.642449256178715

Epoch: 6| Step: 9
Training loss: 3.8460111092711
Validation loss: 3.638187012350623

Epoch: 6| Step: 10
Training loss: 4.180745156777434
Validation loss: 3.634003081115189

Epoch: 6| Step: 11
Training loss: 4.610574029695303
Validation loss: 3.629204876359514

Epoch: 6| Step: 12
Training loss: 4.1082272395323685
Validation loss: 3.62475530302747

Epoch: 6| Step: 13
Training loss: 2.992578386317457
Validation loss: 3.619730308982373

Epoch: 30| Step: 0
Training loss: 3.3979281339718908
Validation loss: 3.6150665106839037

Epoch: 6| Step: 1
Training loss: 3.8948854459436832
Validation loss: 3.6101489978027588

Epoch: 6| Step: 2
Training loss: 3.503525729233395
Validation loss: 3.6055075552778373

Epoch: 6| Step: 3
Training loss: 3.0980530685499494
Validation loss: 3.6009827186468537

Epoch: 6| Step: 4
Training loss: 3.595751926271222
Validation loss: 3.596494096947449

Epoch: 6| Step: 5
Training loss: 4.016296093151167
Validation loss: 3.5921663637384285

Epoch: 6| Step: 6
Training loss: 3.188209305175618
Validation loss: 3.5880722528764633

Epoch: 6| Step: 7
Training loss: 4.038034806394845
Validation loss: 3.5837239932236598

Epoch: 6| Step: 8
Training loss: 3.6604729416671344
Validation loss: 3.5790044822554665

Epoch: 6| Step: 9
Training loss: 3.6424554635429716
Validation loss: 3.5744446729955683

Epoch: 6| Step: 10
Training loss: 4.239825467530142
Validation loss: 3.5701314823219157

Epoch: 6| Step: 11
Training loss: 4.246880676849591
Validation loss: 3.5656240050358767

Epoch: 6| Step: 12
Training loss: 3.4738274606343538
Validation loss: 3.5609290468395423

Epoch: 6| Step: 13
Training loss: 3.896582764265316
Validation loss: 3.556656959100958

Epoch: 31| Step: 0
Training loss: 3.854943453204571
Validation loss: 3.552529128991223

Epoch: 6| Step: 1
Training loss: 2.9880281627915997
Validation loss: 3.5471798041813067

Epoch: 6| Step: 2
Training loss: 2.7630456653069957
Validation loss: 3.543070406508582

Epoch: 6| Step: 3
Training loss: 4.328951209357533
Validation loss: 3.538395317015085

Epoch: 6| Step: 4
Training loss: 3.90169949793266
Validation loss: 3.5343535072129626

Epoch: 6| Step: 5
Training loss: 3.977578623514285
Validation loss: 3.529975728716092

Epoch: 6| Step: 6
Training loss: 3.779074555395707
Validation loss: 3.525925856491059

Epoch: 6| Step: 7
Training loss: 3.8094875754040993
Validation loss: 3.521261666932762

Epoch: 6| Step: 8
Training loss: 3.330051777755009
Validation loss: 3.516657111228749

Epoch: 6| Step: 9
Training loss: 3.2031520935401407
Validation loss: 3.5125721766793108

Epoch: 6| Step: 10
Training loss: 3.3087492526824187
Validation loss: 3.5079493393293797

Epoch: 6| Step: 11
Training loss: 4.249018387472579
Validation loss: 3.5040475171153647

Epoch: 6| Step: 12
Training loss: 3.6990197197645527
Validation loss: 3.499571524232734

Epoch: 6| Step: 13
Training loss: 3.691387002884677
Validation loss: 3.495382157690193

Epoch: 32| Step: 0
Training loss: 3.546835878131229
Validation loss: 3.4909095559381766

Epoch: 6| Step: 1
Training loss: 4.0747854070971155
Validation loss: 3.486343557571177

Epoch: 6| Step: 2
Training loss: 4.430983796079945
Validation loss: 3.481824571550966

Epoch: 6| Step: 3
Training loss: 2.9285020388048526
Validation loss: 3.4773449649435855

Epoch: 6| Step: 4
Training loss: 3.5068303625773702
Validation loss: 3.4734970007596515

Epoch: 6| Step: 5
Training loss: 4.411937316808929
Validation loss: 3.4689165825695154

Epoch: 6| Step: 6
Training loss: 2.7985638682056675
Validation loss: 3.4641093465286916

Epoch: 6| Step: 7
Training loss: 3.171064451786695
Validation loss: 3.4598850461206045

Epoch: 6| Step: 8
Training loss: 3.8041363890753197
Validation loss: 3.4555889102134203

Epoch: 6| Step: 9
Training loss: 2.981348710255329
Validation loss: 3.4519048177681535

Epoch: 6| Step: 10
Training loss: 2.9100648187189075
Validation loss: 3.447595533338705

Epoch: 6| Step: 11
Training loss: 4.168581687812279
Validation loss: 3.443565196613244

Epoch: 6| Step: 12
Training loss: 3.196493588065667
Validation loss: 3.439125624543424

Epoch: 6| Step: 13
Training loss: 3.903703759976453
Validation loss: 3.4351594990991168

Epoch: 33| Step: 0
Training loss: 3.719804373898999
Validation loss: 3.431149992791134

Epoch: 6| Step: 1
Training loss: 3.401362701421819
Validation loss: 3.427110704739768

Epoch: 6| Step: 2
Training loss: 4.060022625789643
Validation loss: 3.4230189508784075

Epoch: 6| Step: 3
Training loss: 3.0203268920113486
Validation loss: 3.4188257854088677

Epoch: 6| Step: 4
Training loss: 3.768617319583516
Validation loss: 3.415033989323393

Epoch: 6| Step: 5
Training loss: 3.2807515083582186
Validation loss: 3.411465271610261

Epoch: 6| Step: 6
Training loss: 3.691081877489385
Validation loss: 3.4066931876614017

Epoch: 6| Step: 7
Training loss: 3.7693931119442885
Validation loss: 3.4029851309643617

Epoch: 6| Step: 8
Training loss: 3.8474611804136556
Validation loss: 3.3986390035822174

Epoch: 6| Step: 9
Training loss: 3.5998311215102237
Validation loss: 3.394505145488725

Epoch: 6| Step: 10
Training loss: 3.2569063605111057
Validation loss: 3.390392582628784

Epoch: 6| Step: 11
Training loss: 3.495738841535187
Validation loss: 3.386789780942152

Epoch: 6| Step: 12
Training loss: 3.5701148314400153
Validation loss: 3.382766782056708

Epoch: 6| Step: 13
Training loss: 2.9570188333107046
Validation loss: 3.378559848911877

Epoch: 34| Step: 0
Training loss: 2.99395142198191
Validation loss: 3.3746486528032125

Epoch: 6| Step: 1
Training loss: 3.727444526367033
Validation loss: 3.370572270460091

Epoch: 6| Step: 2
Training loss: 3.250324233094178
Validation loss: 3.3665964739315237

Epoch: 6| Step: 3
Training loss: 3.4714136292653484
Validation loss: 3.362494813520362

Epoch: 6| Step: 4
Training loss: 2.8980255245456292
Validation loss: 3.358672336162685

Epoch: 6| Step: 5
Training loss: 3.499354030398456
Validation loss: 3.3548767491312663

Epoch: 6| Step: 6
Training loss: 3.7967034446357264
Validation loss: 3.3509634293204007

Epoch: 6| Step: 7
Training loss: 3.7502588818517855
Validation loss: 3.3467731535131957

Epoch: 6| Step: 8
Training loss: 3.4162364471758533
Validation loss: 3.343034186350172

Epoch: 6| Step: 9
Training loss: 3.5355108431020112
Validation loss: 3.339144413772918

Epoch: 6| Step: 10
Training loss: 4.067907871459676
Validation loss: 3.335176379578745

Epoch: 6| Step: 11
Training loss: 3.3976982628107844
Validation loss: 3.3311214896983334

Epoch: 6| Step: 12
Training loss: 2.7546529420447365
Validation loss: 3.3274747186355254

Epoch: 6| Step: 13
Training loss: 3.998890126746242
Validation loss: 3.323357643515523

Epoch: 35| Step: 0
Training loss: 3.7226005301853737
Validation loss: 3.3195531717332063

Epoch: 6| Step: 1
Training loss: 2.941998547499847
Validation loss: 3.3157881725067586

Epoch: 6| Step: 2
Training loss: 3.0330437141183655
Validation loss: 3.3116908254703725

Epoch: 6| Step: 3
Training loss: 4.175528423195685
Validation loss: 3.307878929724086

Epoch: 6| Step: 4
Training loss: 3.3271202039178505
Validation loss: 3.304253203199674

Epoch: 6| Step: 5
Training loss: 3.5190648861209555
Validation loss: 3.3005270354060317

Epoch: 6| Step: 6
Training loss: 3.774813257107906
Validation loss: 3.2961318879514185

Epoch: 6| Step: 7
Training loss: 3.025212676083232
Validation loss: 3.29250741836612

Epoch: 6| Step: 8
Training loss: 3.457567628101158
Validation loss: 3.288757962514821

Epoch: 6| Step: 9
Training loss: 3.9020754060326968
Validation loss: 3.2850223231222433

Epoch: 6| Step: 10
Training loss: 3.3701730755018935
Validation loss: 3.280935484329672

Epoch: 6| Step: 11
Training loss: 3.193605447323421
Validation loss: 3.27717036386123

Epoch: 6| Step: 12
Training loss: 3.330070392677973
Validation loss: 3.2732514758729123

Epoch: 6| Step: 13
Training loss: 3.0845831494501135
Validation loss: 3.269883805639961

Epoch: 36| Step: 0
Training loss: 3.994632218754356
Validation loss: 3.2659911174057696

Epoch: 6| Step: 1
Training loss: 2.79362819329288
Validation loss: 3.2624043427495657

Epoch: 6| Step: 2
Training loss: 3.766528009433835
Validation loss: 3.2588757579726937

Epoch: 6| Step: 3
Training loss: 3.470678532244398
Validation loss: 3.255263455750737

Epoch: 6| Step: 4
Training loss: 3.33949508185495
Validation loss: 3.2514632916717714

Epoch: 6| Step: 5
Training loss: 3.881495538337735
Validation loss: 3.247847675784014

Epoch: 6| Step: 6
Training loss: 2.2584132678910707
Validation loss: 3.2439465021628915

Epoch: 6| Step: 7
Training loss: 2.6986321834627804
Validation loss: 3.240350409620868

Epoch: 6| Step: 8
Training loss: 3.340397660573022
Validation loss: 3.237005690417131

Epoch: 6| Step: 9
Training loss: 4.025334949654399
Validation loss: 3.2333946646612

Epoch: 6| Step: 10
Training loss: 3.2105742385947598
Validation loss: 3.2298831185923214

Epoch: 6| Step: 11
Training loss: 3.0683312466386545
Validation loss: 3.2263366521652794

Epoch: 6| Step: 12
Training loss: 3.4778937716195304
Validation loss: 3.223002972687382

Epoch: 6| Step: 13
Training loss: 3.549323761696619
Validation loss: 3.219479993489161

Epoch: 37| Step: 0
Training loss: 4.09018371393275
Validation loss: 3.2159722573497636

Epoch: 6| Step: 1
Training loss: 3.4323574092668045
Validation loss: 3.2123045351007495

Epoch: 6| Step: 2
Training loss: 2.277916235967046
Validation loss: 3.208534052791709

Epoch: 6| Step: 3
Training loss: 3.28949029817903
Validation loss: 3.2051936843003395

Epoch: 6| Step: 4
Training loss: 3.216153180606908
Validation loss: 3.2019054913612512

Epoch: 6| Step: 5
Training loss: 3.7330666009667466
Validation loss: 3.1985452181305396

Epoch: 6| Step: 6
Training loss: 2.9553285481300686
Validation loss: 3.1949948228327476

Epoch: 6| Step: 7
Training loss: 3.226637573846648
Validation loss: 3.191733858249519

Epoch: 6| Step: 8
Training loss: 3.7839891585099963
Validation loss: 3.188400041753427

Epoch: 6| Step: 9
Training loss: 1.968098123802406
Validation loss: 3.185149753748881

Epoch: 6| Step: 10
Training loss: 4.0122455076517785
Validation loss: 3.181955239617925

Epoch: 6| Step: 11
Training loss: 3.75914310999019
Validation loss: 3.1784141171462

Epoch: 6| Step: 12
Training loss: 3.5982730432017824
Validation loss: 3.174976279703278

Epoch: 6| Step: 13
Training loss: 2.5433785244621117
Validation loss: 3.1713154742446603

Epoch: 38| Step: 0
Training loss: 3.278007140100045
Validation loss: 3.1680335725360447

Epoch: 6| Step: 1
Training loss: 3.180693446191415
Validation loss: 3.164707724006023

Epoch: 6| Step: 2
Training loss: 3.0533587988023343
Validation loss: 3.1617844280927208

Epoch: 6| Step: 3
Training loss: 3.749185473673371
Validation loss: 3.1586617046616197

Epoch: 6| Step: 4
Training loss: 3.4879597241237428
Validation loss: 3.1554105290665055

Epoch: 6| Step: 5
Training loss: 3.4012558749371435
Validation loss: 3.1524097887926996

Epoch: 6| Step: 6
Training loss: 3.6375364373862964
Validation loss: 3.1489791214930083

Epoch: 6| Step: 7
Training loss: 2.513002912757239
Validation loss: 3.145886671275155

Epoch: 6| Step: 8
Training loss: 3.460655321453763
Validation loss: 3.142565960936617

Epoch: 6| Step: 9
Training loss: 3.264000301398469
Validation loss: 3.13908576254155

Epoch: 6| Step: 10
Training loss: 3.145033442041992
Validation loss: 3.136122251351363

Epoch: 6| Step: 11
Training loss: 3.599285870823159
Validation loss: 3.133015588787546

Epoch: 6| Step: 12
Training loss: 2.880535601509292
Validation loss: 3.1295043399088134

Epoch: 6| Step: 13
Training loss: 3.17630522324537
Validation loss: 3.1265442402370165

Epoch: 39| Step: 0
Training loss: 3.144263138802978
Validation loss: 3.123401086571777

Epoch: 6| Step: 1
Training loss: 2.8472953745637644
Validation loss: 3.1201194544282975

Epoch: 6| Step: 2
Training loss: 3.28062360324928
Validation loss: 3.11724268355319

Epoch: 6| Step: 3
Training loss: 3.386183400504259
Validation loss: 3.1141961706565744

Epoch: 6| Step: 4
Training loss: 3.252891428071688
Validation loss: 3.1114045616861405

Epoch: 6| Step: 5
Training loss: 2.8705629026680164
Validation loss: 3.108233767969623

Epoch: 6| Step: 6
Training loss: 3.678924805470234
Validation loss: 3.105579417434066

Epoch: 6| Step: 7
Training loss: 3.708698954756468
Validation loss: 3.102232469125376

Epoch: 6| Step: 8
Training loss: 2.726321846471336
Validation loss: 3.099032891068489

Epoch: 6| Step: 9
Training loss: 3.3560274700363633
Validation loss: 3.095934934390734

Epoch: 6| Step: 10
Training loss: 2.8335334108805164
Validation loss: 3.0928795999756074

Epoch: 6| Step: 11
Training loss: 3.546152452990231
Validation loss: 3.0897109127305447

Epoch: 6| Step: 12
Training loss: 3.326009301755816
Validation loss: 3.086668341645129

Epoch: 6| Step: 13
Training loss: 3.2617865823784937
Validation loss: 3.0840480251077578

Epoch: 40| Step: 0
Training loss: 3.2919876972214257
Validation loss: 3.0804219978197294

Epoch: 6| Step: 1
Training loss: 3.5214459189178644
Validation loss: 3.077229713327825

Epoch: 6| Step: 2
Training loss: 3.5550161356499417
Validation loss: 3.073958583423721

Epoch: 6| Step: 3
Training loss: 2.7846116741389153
Validation loss: 3.0704732113516333

Epoch: 6| Step: 4
Training loss: 3.235410073186211
Validation loss: 3.0673551156405696

Epoch: 6| Step: 5
Training loss: 3.173014919857867
Validation loss: 3.063914134706541

Epoch: 6| Step: 6
Training loss: 3.758842216149569
Validation loss: 3.060568661136496

Epoch: 6| Step: 7
Training loss: 2.809468309594751
Validation loss: 3.0581272203042493

Epoch: 6| Step: 8
Training loss: 2.8601470071851134
Validation loss: 3.0542727137565104

Epoch: 6| Step: 9
Training loss: 3.796612765828554
Validation loss: 3.051115583464526

Epoch: 6| Step: 10
Training loss: 3.36579260731975
Validation loss: 3.04813901325337

Epoch: 6| Step: 11
Training loss: 3.019826383589614
Validation loss: 3.044984540769591

Epoch: 6| Step: 12
Training loss: 2.434854685933797
Validation loss: 3.041868952116876

Epoch: 6| Step: 13
Training loss: 2.9363368551985425
Validation loss: 3.0389538429272234

Epoch: 41| Step: 0
Training loss: 3.090663719127567
Validation loss: 3.036117224838866

Epoch: 6| Step: 1
Training loss: 3.531754989299705
Validation loss: 3.0336215649067335

Epoch: 6| Step: 2
Training loss: 3.0154728826392416
Validation loss: 3.0307841778736275

Epoch: 6| Step: 3
Training loss: 2.9184215352665084
Validation loss: 3.0278247082035294

Epoch: 6| Step: 4
Training loss: 2.650822558588481
Validation loss: 3.0249924058004427

Epoch: 6| Step: 5
Training loss: 3.197680502920028
Validation loss: 3.0223545760347847

Epoch: 6| Step: 6
Training loss: 3.531379089696614
Validation loss: 3.019779354691888

Epoch: 6| Step: 7
Training loss: 3.160719926165581
Validation loss: 3.0171663104116124

Epoch: 6| Step: 8
Training loss: 3.5255868788431517
Validation loss: 3.014388283775357

Epoch: 6| Step: 9
Training loss: 3.3185877011372114
Validation loss: 3.0114460849325946

Epoch: 6| Step: 10
Training loss: 3.1033194151613026
Validation loss: 3.008633297647342

Epoch: 6| Step: 11
Training loss: 3.0354280288752435
Validation loss: 3.005684976225922

Epoch: 6| Step: 12
Training loss: 2.931919722515837
Validation loss: 3.002997000748776

Epoch: 6| Step: 13
Training loss: 3.1225655800962695
Validation loss: 3.00010839902224

Epoch: 42| Step: 0
Training loss: 3.6239909379016337
Validation loss: 2.997578889022386

Epoch: 6| Step: 1
Training loss: 3.1304991592822846
Validation loss: 2.994879020854336

Epoch: 6| Step: 2
Training loss: 2.748717442450238
Validation loss: 2.9924203435142993

Epoch: 6| Step: 3
Training loss: 3.612019152466953
Validation loss: 2.989872793258086

Epoch: 6| Step: 4
Training loss: 2.843367666176605
Validation loss: 2.986925243809991

Epoch: 6| Step: 5
Training loss: 3.2249370480093167
Validation loss: 2.984424064725554

Epoch: 6| Step: 6
Training loss: 3.3731134581496502
Validation loss: 2.9817568227558167

Epoch: 6| Step: 7
Training loss: 2.4437553152041183
Validation loss: 2.9791654617911796

Epoch: 6| Step: 8
Training loss: 3.1348273820824812
Validation loss: 2.9765800415483366

Epoch: 6| Step: 9
Training loss: 3.2949589228059453
Validation loss: 2.973940718421055

Epoch: 6| Step: 10
Training loss: 3.1582135428573537
Validation loss: 2.9715827677877305

Epoch: 6| Step: 11
Training loss: 3.440651575968121
Validation loss: 2.9688242183576716

Epoch: 6| Step: 12
Training loss: 2.7968748295107315
Validation loss: 2.966533958131392

Epoch: 6| Step: 13
Training loss: 2.6385229648951887
Validation loss: 2.9641422049866395

Epoch: 43| Step: 0
Training loss: 2.8491853185375846
Validation loss: 2.961651476933975

Epoch: 6| Step: 1
Training loss: 3.1535995988215593
Validation loss: 2.959495710081624

Epoch: 6| Step: 2
Training loss: 3.221225268232374
Validation loss: 2.956980655695906

Epoch: 6| Step: 3
Training loss: 2.2299150892095034
Validation loss: 2.9542321782096046

Epoch: 6| Step: 4
Training loss: 3.111553866428168
Validation loss: 2.951830491083044

Epoch: 6| Step: 5
Training loss: 3.521083544506233
Validation loss: 2.9496504781313777

Epoch: 6| Step: 6
Training loss: 3.2453274516687354
Validation loss: 2.9471834481801036

Epoch: 6| Step: 7
Training loss: 3.2871717513949505
Validation loss: 2.944340202197713

Epoch: 6| Step: 8
Training loss: 3.084089358320375
Validation loss: 2.941993793168945

Epoch: 6| Step: 9
Training loss: 3.2056416371241596
Validation loss: 2.939503723587729

Epoch: 6| Step: 10
Training loss: 2.983371745737799
Validation loss: 2.9372017756374693

Epoch: 6| Step: 11
Training loss: 3.1320385821269254
Validation loss: 2.9345728649397005

Epoch: 6| Step: 12
Training loss: 2.5536338196410333
Validation loss: 2.9322469559959767

Epoch: 6| Step: 13
Training loss: 3.4236869021252776
Validation loss: 2.930058692305223

Epoch: 44| Step: 0
Training loss: 3.2938662716576648
Validation loss: 2.9274654116434604

Epoch: 6| Step: 1
Training loss: 2.9978372407367684
Validation loss: 2.92517922985309

Epoch: 6| Step: 2
Training loss: 2.750912601592708
Validation loss: 2.9227226238230837

Epoch: 6| Step: 3
Training loss: 2.8617210561048925
Validation loss: 2.920205630460634

Epoch: 6| Step: 4
Training loss: 3.648258572424259
Validation loss: 2.9176734730659177

Epoch: 6| Step: 5
Training loss: 3.2546599798554467
Validation loss: 2.9154681377184337

Epoch: 6| Step: 6
Training loss: 2.9984862959222434
Validation loss: 2.9130596722808306

Epoch: 6| Step: 7
Training loss: 3.2190930822541923
Validation loss: 2.9101276165878485

Epoch: 6| Step: 8
Training loss: 2.58166963061989
Validation loss: 2.9077679898017075

Epoch: 6| Step: 9
Training loss: 3.130658938725999
Validation loss: 2.9053030488030536

Epoch: 6| Step: 10
Training loss: 2.645107640345873
Validation loss: 2.9032406899211423

Epoch: 6| Step: 11
Training loss: 3.2310794866496013
Validation loss: 2.9005460652528954

Epoch: 6| Step: 12
Training loss: 2.774811482255046
Validation loss: 2.8985777732720415

Epoch: 6| Step: 13
Training loss: 3.2136186603903676
Validation loss: 2.8965627360017168

Epoch: 45| Step: 0
Training loss: 2.6742032263770783
Validation loss: 2.894189894261081

Epoch: 6| Step: 1
Training loss: 3.248691588900818
Validation loss: 2.892473774018743

Epoch: 6| Step: 2
Training loss: 2.987211626891039
Validation loss: 2.8906499913140533

Epoch: 6| Step: 3
Training loss: 3.2542737590704642
Validation loss: 2.8885657046161937

Epoch: 6| Step: 4
Training loss: 2.879319636195817
Validation loss: 2.886470799514978

Epoch: 6| Step: 5
Training loss: 3.3604955889016344
Validation loss: 2.8841562283898434

Epoch: 6| Step: 6
Training loss: 2.766685498031811
Validation loss: 2.8822939853546297

Epoch: 6| Step: 7
Training loss: 2.814718939401358
Validation loss: 2.880138352678118

Epoch: 6| Step: 8
Training loss: 2.2576533594772332
Validation loss: 2.8783385753602264

Epoch: 6| Step: 9
Training loss: 3.470655313225826
Validation loss: 2.876162846679002

Epoch: 6| Step: 10
Training loss: 3.519599261881017
Validation loss: 2.8740140495877156

Epoch: 6| Step: 11
Training loss: 2.8758039594322033
Validation loss: 2.8715515602128434

Epoch: 6| Step: 12
Training loss: 2.796809616603293
Validation loss: 2.869807114648982

Epoch: 6| Step: 13
Training loss: 3.1532194488262397
Validation loss: 2.8673938673488943

Epoch: 46| Step: 0
Training loss: 2.889470875226421
Validation loss: 2.865148943923489

Epoch: 6| Step: 1
Training loss: 2.9007881277482093
Validation loss: 2.863293906648357

Epoch: 6| Step: 2
Training loss: 3.3674425351593005
Validation loss: 2.8610679048117342

Epoch: 6| Step: 3
Training loss: 3.0218239749360065
Validation loss: 2.8588688303373955

Epoch: 6| Step: 4
Training loss: 3.232311530405113
Validation loss: 2.856718764733276

Epoch: 6| Step: 5
Training loss: 3.142204987995607
Validation loss: 2.8548809512827518

Epoch: 6| Step: 6
Training loss: 3.2855752684523187
Validation loss: 2.852783638406355

Epoch: 6| Step: 7
Training loss: 3.0643809244999964
Validation loss: 2.8508992299040283

Epoch: 6| Step: 8
Training loss: 2.8129969687609946
Validation loss: 2.8485721184549817

Epoch: 6| Step: 9
Training loss: 2.956874183101537
Validation loss: 2.846690756861876

Epoch: 6| Step: 10
Training loss: 2.601125869538914
Validation loss: 2.8445749728069387

Epoch: 6| Step: 11
Training loss: 2.0364901514974227
Validation loss: 2.8423550017276433

Epoch: 6| Step: 12
Training loss: 2.7192695384353414
Validation loss: 2.8408208440310463

Epoch: 6| Step: 13
Training loss: 3.5835937287102135
Validation loss: 2.8385887538725747

Epoch: 47| Step: 0
Training loss: 3.1825488663463015
Validation loss: 2.836542574644526

Epoch: 6| Step: 1
Training loss: 2.574566043471176
Validation loss: 2.8347405183600296

Epoch: 6| Step: 2
Training loss: 2.9971062691310024
Validation loss: 2.8325744108036894

Epoch: 6| Step: 3
Training loss: 2.7423462563664582
Validation loss: 2.8308429478738555

Epoch: 6| Step: 4
Training loss: 3.045290647519322
Validation loss: 2.828749038105013

Epoch: 6| Step: 5
Training loss: 2.840515141308944
Validation loss: 2.8270279809812737

Epoch: 6| Step: 6
Training loss: 2.7846859057292073
Validation loss: 2.825053252221805

Epoch: 6| Step: 7
Training loss: 3.525017969613232
Validation loss: 2.823166154673097

Epoch: 6| Step: 8
Training loss: 2.791473543309925
Validation loss: 2.821537966788967

Epoch: 6| Step: 9
Training loss: 3.2994458629168832
Validation loss: 2.8195262316621

Epoch: 6| Step: 10
Training loss: 3.369763019956885
Validation loss: 2.817955899365719

Epoch: 6| Step: 11
Training loss: 2.114819317758644
Validation loss: 2.8163770443993257

Epoch: 6| Step: 12
Training loss: 2.746082984176087
Validation loss: 2.8146116171055047

Epoch: 6| Step: 13
Training loss: 3.2201675700780426
Validation loss: 2.812689725339949

Epoch: 48| Step: 0
Training loss: 2.8209320824907995
Validation loss: 2.8107520711827214

Epoch: 6| Step: 1
Training loss: 2.5558303936308215
Validation loss: 2.808816829305681

Epoch: 6| Step: 2
Training loss: 1.9677020720231975
Validation loss: 2.8067947164880747

Epoch: 6| Step: 3
Training loss: 3.5048200931714093
Validation loss: 2.804815347886949

Epoch: 6| Step: 4
Training loss: 2.7613029544669048
Validation loss: 2.803282957187603

Epoch: 6| Step: 5
Training loss: 3.1862084259038643
Validation loss: 2.8013894783145066

Epoch: 6| Step: 6
Training loss: 3.505058175772801
Validation loss: 2.7997749255181534

Epoch: 6| Step: 7
Training loss: 2.65744566093004
Validation loss: 2.7981870759643854

Epoch: 6| Step: 8
Training loss: 3.1056689695676125
Validation loss: 2.796744743169685

Epoch: 6| Step: 9
Training loss: 3.3695357558151535
Validation loss: 2.7948724976484662

Epoch: 6| Step: 10
Training loss: 2.964911941327131
Validation loss: 2.792942671220577

Epoch: 6| Step: 11
Training loss: 2.4681702911015813
Validation loss: 2.7908628407244764

Epoch: 6| Step: 12
Training loss: 2.883014620179873
Validation loss: 2.788713007666916

Epoch: 6| Step: 13
Training loss: 3.0429433642835524
Validation loss: 2.786807618873285

Epoch: 49| Step: 0
Training loss: 2.5794650207738807
Validation loss: 2.785294538705582

Epoch: 6| Step: 1
Training loss: 3.0228477513988703
Validation loss: 2.783466684455321

Epoch: 6| Step: 2
Training loss: 3.127951334611326
Validation loss: 2.78202834650074

Epoch: 6| Step: 3
Training loss: 2.7217029889453737
Validation loss: 2.780242080073943

Epoch: 6| Step: 4
Training loss: 3.062173787084451
Validation loss: 2.778902098622482

Epoch: 6| Step: 5
Training loss: 2.917508212799568
Validation loss: 2.7780758250132065

Epoch: 6| Step: 6
Training loss: 3.1443435139028386
Validation loss: 2.7759512671279882

Epoch: 6| Step: 7
Training loss: 2.6239759173276678
Validation loss: 2.7746909161289706

Epoch: 6| Step: 8
Training loss: 3.130682090075754
Validation loss: 2.7723255776110367

Epoch: 6| Step: 9
Training loss: 2.384836206789752
Validation loss: 2.770605082659386

Epoch: 6| Step: 10
Training loss: 2.8488288213011628
Validation loss: 2.769242731228829

Epoch: 6| Step: 11
Training loss: 3.209551509768906
Validation loss: 2.768036231240245

Epoch: 6| Step: 12
Training loss: 3.0055027403767425
Validation loss: 2.7662526943160914

Epoch: 6| Step: 13
Training loss: 2.917497262307838
Validation loss: 2.7649882817494307

Epoch: 50| Step: 0
Training loss: 3.230536943336315
Validation loss: 2.7639234089146676

Epoch: 6| Step: 1
Training loss: 3.14465074015985
Validation loss: 2.762254805924682

Epoch: 6| Step: 2
Training loss: 2.706468047416598
Validation loss: 2.7609611741932607

Epoch: 6| Step: 3
Training loss: 2.8478622048078877
Validation loss: 2.759707612914801

Epoch: 6| Step: 4
Training loss: 2.779621032983744
Validation loss: 2.757715816081791

Epoch: 6| Step: 5
Training loss: 2.7010026659480992
Validation loss: 2.7558383810179556

Epoch: 6| Step: 6
Training loss: 2.900510637773781
Validation loss: 2.7544871914673634

Epoch: 6| Step: 7
Training loss: 2.8091761975922145
Validation loss: 2.7527874488887365

Epoch: 6| Step: 8
Training loss: 3.060134752834484
Validation loss: 2.7511293086504764

Epoch: 6| Step: 9
Training loss: 2.717294237476402
Validation loss: 2.749216069123259

Epoch: 6| Step: 10
Training loss: 2.668738057380059
Validation loss: 2.7481344281826448

Epoch: 6| Step: 11
Training loss: 3.128671244393243
Validation loss: 2.74603421909565

Epoch: 6| Step: 12
Training loss: 2.562267013750881
Validation loss: 2.7446147613169947

Epoch: 6| Step: 13
Training loss: 3.177629634394762
Validation loss: 2.7430931689638487

Epoch: 51| Step: 0
Training loss: 2.9852057294381424
Validation loss: 2.7415307717621777

Epoch: 6| Step: 1
Training loss: 2.701325804543932
Validation loss: 2.7395796733488478

Epoch: 6| Step: 2
Training loss: 2.5437567408516912
Validation loss: 2.7392129587273955

Epoch: 6| Step: 3
Training loss: 2.5405680257079344
Validation loss: 2.737975186427094

Epoch: 6| Step: 4
Training loss: 2.6251444095671275
Validation loss: 2.7368013933389896

Epoch: 6| Step: 5
Training loss: 3.3719197097939437
Validation loss: 2.735884752823521

Epoch: 6| Step: 6
Training loss: 2.1379919634847306
Validation loss: 2.734788540040933

Epoch: 6| Step: 7
Training loss: 3.168613956096734
Validation loss: 2.733816558488127

Epoch: 6| Step: 8
Training loss: 2.744558151899448
Validation loss: 2.7328284113198844

Epoch: 6| Step: 9
Training loss: 2.797983642443055
Validation loss: 2.7313074980269576

Epoch: 6| Step: 10
Training loss: 3.3769625856124708
Validation loss: 2.729953511695814

Epoch: 6| Step: 11
Training loss: 2.9362525726216147
Validation loss: 2.728548290482221

Epoch: 6| Step: 12
Training loss: 2.5168148094010725
Validation loss: 2.726922915883829

Epoch: 6| Step: 13
Training loss: 3.460379734259441
Validation loss: 2.725409423331724

Epoch: 52| Step: 0
Training loss: 2.7689006824694675
Validation loss: 2.7235346526024955

Epoch: 6| Step: 1
Training loss: 2.872131243243938
Validation loss: 2.7218666190402687

Epoch: 6| Step: 2
Training loss: 2.867091205861273
Validation loss: 2.720438319726877

Epoch: 6| Step: 3
Training loss: 2.882235999213731
Validation loss: 2.718098383311441

Epoch: 6| Step: 4
Training loss: 3.2295664550067222
Validation loss: 2.716861039406556

Epoch: 6| Step: 5
Training loss: 3.069725388164263
Validation loss: 2.717313701323188

Epoch: 6| Step: 6
Training loss: 2.4321694408041843
Validation loss: 2.7152173414168486

Epoch: 6| Step: 7
Training loss: 3.0227598865181085
Validation loss: 2.713224313132891

Epoch: 6| Step: 8
Training loss: 2.7320286738509267
Validation loss: 2.711605652949399

Epoch: 6| Step: 9
Training loss: 2.8705595804106734
Validation loss: 2.7101893496551663

Epoch: 6| Step: 10
Training loss: 2.5819847114281735
Validation loss: 2.7090818153391254

Epoch: 6| Step: 11
Training loss: 2.711547958723669
Validation loss: 2.708536922921221

Epoch: 6| Step: 12
Training loss: 2.8949331810179304
Validation loss: 2.707798337670073

Epoch: 6| Step: 13
Training loss: 2.9537402929920606
Validation loss: 2.706732956899784

Epoch: 53| Step: 0
Training loss: 2.9288524200466446
Validation loss: 2.705178379445794

Epoch: 6| Step: 1
Training loss: 2.808534836449651
Validation loss: 2.7044915069316957

Epoch: 6| Step: 2
Training loss: 3.261040202389281
Validation loss: 2.703948525758954

Epoch: 6| Step: 3
Training loss: 3.2548776850300896
Validation loss: 2.702366658231567

Epoch: 6| Step: 4
Training loss: 2.6395678689609543
Validation loss: 2.700324075582291

Epoch: 6| Step: 5
Training loss: 2.162079381002432
Validation loss: 2.6991932523179147

Epoch: 6| Step: 6
Training loss: 2.8763056568395826
Validation loss: 2.6978805470841065

Epoch: 6| Step: 7
Training loss: 3.212789108388608
Validation loss: 2.695528104700333

Epoch: 6| Step: 8
Training loss: 2.0965594899975715
Validation loss: 2.6940181259455507

Epoch: 6| Step: 9
Training loss: 2.4040486558569865
Validation loss: 2.692709317136446

Epoch: 6| Step: 10
Training loss: 3.0178502228658344
Validation loss: 2.690556805994772

Epoch: 6| Step: 11
Training loss: 2.606602823722795
Validation loss: 2.690286522464154

Epoch: 6| Step: 12
Training loss: 2.9899661431580533
Validation loss: 2.6881085379595433

Epoch: 6| Step: 13
Training loss: 3.1357058437145637
Validation loss: 2.6871839086073326

Epoch: 54| Step: 0
Training loss: 2.750123107928925
Validation loss: 2.6854126386905723

Epoch: 6| Step: 1
Training loss: 2.161806879704554
Validation loss: 2.6846410640951084

Epoch: 6| Step: 2
Training loss: 3.124486346945843
Validation loss: 2.6831395460330287

Epoch: 6| Step: 3
Training loss: 2.7228859321554455
Validation loss: 2.681826628337151

Epoch: 6| Step: 4
Training loss: 3.2432841824619696
Validation loss: 2.6782274782270097

Epoch: 6| Step: 5
Training loss: 2.6215864420566124
Validation loss: 2.6768796870531792

Epoch: 6| Step: 6
Training loss: 2.681107214480303
Validation loss: 2.674749289555593

Epoch: 6| Step: 7
Training loss: 2.4092790438571936
Validation loss: 2.679622178537353

Epoch: 6| Step: 8
Training loss: 2.5750605085356013
Validation loss: 2.6897630773791867

Epoch: 6| Step: 9
Training loss: 3.082676963707557
Validation loss: 2.6774440082070328

Epoch: 6| Step: 10
Training loss: 3.1837886071981454
Validation loss: 2.6705024867424467

Epoch: 6| Step: 11
Training loss: 2.995679605401381
Validation loss: 2.6717370650173846

Epoch: 6| Step: 12
Training loss: 2.9065671358070526
Validation loss: 2.6720195976526573

Epoch: 6| Step: 13
Training loss: 2.7967039940411365
Validation loss: 2.6700748826716634

Epoch: 55| Step: 0
Training loss: 2.9340504622225168
Validation loss: 2.6681721034076706

Epoch: 6| Step: 1
Training loss: 2.759149589274502
Validation loss: 2.6687163557065556

Epoch: 6| Step: 2
Training loss: 2.942090282971326
Validation loss: 2.665508426026819

Epoch: 6| Step: 3
Training loss: 2.856597385107931
Validation loss: 2.6646352023717523

Epoch: 6| Step: 4
Training loss: 2.5815697054040325
Validation loss: 2.6625035890366986

Epoch: 6| Step: 5
Training loss: 3.2788969776457826
Validation loss: 2.6633776756847647

Epoch: 6| Step: 6
Training loss: 2.960865352023374
Validation loss: 2.662436666914427

Epoch: 6| Step: 7
Training loss: 2.598927305924406
Validation loss: 2.6632684622476517

Epoch: 6| Step: 8
Training loss: 2.7771092998629006
Validation loss: 2.6662721838835255

Epoch: 6| Step: 9
Training loss: 2.6567651585693617
Validation loss: 2.6743927483245717

Epoch: 6| Step: 10
Training loss: 2.608952379551668
Validation loss: 2.6625498246141146

Epoch: 6| Step: 11
Training loss: 2.898563351430895
Validation loss: 2.6603514776604893

Epoch: 6| Step: 12
Training loss: 2.863566510613219
Validation loss: 2.6579195348262985

Epoch: 6| Step: 13
Training loss: 2.429296333250362
Validation loss: 2.6535177053279315

Epoch: 56| Step: 0
Training loss: 2.569941346059532
Validation loss: 2.6637747302459545

Epoch: 6| Step: 1
Training loss: 2.713575854729799
Validation loss: 2.702805649079069

Epoch: 6| Step: 2
Training loss: 2.7510315087585995
Validation loss: 2.6666603883033595

Epoch: 6| Step: 3
Training loss: 3.223300125071983
Validation loss: 2.6516449896907255

Epoch: 6| Step: 4
Training loss: 2.806595432286639
Validation loss: 2.648854420721782

Epoch: 6| Step: 5
Training loss: 3.0862610466266145
Validation loss: 2.6499518282128744

Epoch: 6| Step: 6
Training loss: 3.00910030390764
Validation loss: 2.657163634905074

Epoch: 6| Step: 7
Training loss: 2.5610718818118663
Validation loss: 2.6693837559082363

Epoch: 6| Step: 8
Training loss: 2.7528784166252587
Validation loss: 2.6783018245952355

Epoch: 6| Step: 9
Training loss: 2.4318973022787302
Validation loss: 2.677868521363275

Epoch: 6| Step: 10
Training loss: 2.817256296579865
Validation loss: 2.673242676244978

Epoch: 6| Step: 11
Training loss: 2.2782154541477366
Validation loss: 2.671903018916045

Epoch: 6| Step: 12
Training loss: 2.8277517673101427
Validation loss: 2.668925749019271

Epoch: 6| Step: 13
Training loss: 3.35909862712171
Validation loss: 2.6591708561039313

Epoch: 57| Step: 0
Training loss: 2.8527958123283867
Validation loss: 2.6476852714063885

Epoch: 6| Step: 1
Training loss: 2.8367367293434977
Validation loss: 2.639580755275159

Epoch: 6| Step: 2
Training loss: 2.7797592968645035
Validation loss: 2.6370686315411076

Epoch: 6| Step: 3
Training loss: 2.8293061714067034
Validation loss: 2.6442904041030144

Epoch: 6| Step: 4
Training loss: 2.746211737191264
Validation loss: 2.6691531321309165

Epoch: 6| Step: 5
Training loss: 3.0043119595577616
Validation loss: 2.6765705809997318

Epoch: 6| Step: 6
Training loss: 2.6769407558447638
Validation loss: 2.6634950751351054

Epoch: 6| Step: 7
Training loss: 2.8786804856037427
Validation loss: 2.657509449092398

Epoch: 6| Step: 8
Training loss: 2.7683851186772626
Validation loss: 2.6513506116635783

Epoch: 6| Step: 9
Training loss: 2.65697227082806
Validation loss: 2.642846236028467

Epoch: 6| Step: 10
Training loss: 2.249795374572432
Validation loss: 2.6371973728887586

Epoch: 6| Step: 11
Training loss: 2.7058872816767767
Validation loss: 2.6329998549632645

Epoch: 6| Step: 12
Training loss: 2.7529428514894554
Validation loss: 2.641575959951792

Epoch: 6| Step: 13
Training loss: 3.275373155141246
Validation loss: 2.6613157250306645

Epoch: 58| Step: 0
Training loss: 2.355577052016308
Validation loss: 2.646516821780839

Epoch: 6| Step: 1
Training loss: 2.358570694407954
Validation loss: 2.643171057352071

Epoch: 6| Step: 2
Training loss: 2.6296743826672158
Validation loss: 2.637758117026003

Epoch: 6| Step: 3
Training loss: 2.6867759083987455
Validation loss: 2.6347155536769873

Epoch: 6| Step: 4
Training loss: 2.917235091995538
Validation loss: 2.633196130228403

Epoch: 6| Step: 5
Training loss: 2.5279787377296286
Validation loss: 2.632754829899536

Epoch: 6| Step: 6
Training loss: 2.37195713468935
Validation loss: 2.6318493448174265

Epoch: 6| Step: 7
Training loss: 3.1111488075091844
Validation loss: 2.6362360307550374

Epoch: 6| Step: 8
Training loss: 2.6683463727198404
Validation loss: 2.6371740178909002

Epoch: 6| Step: 9
Training loss: 2.858990419300381
Validation loss: 2.6375649382444832

Epoch: 6| Step: 10
Training loss: 2.7804977599836196
Validation loss: 2.6363531165276832

Epoch: 6| Step: 11
Training loss: 3.247309671628899
Validation loss: 2.631764581641291

Epoch: 6| Step: 12
Training loss: 3.278282349394804
Validation loss: 2.6269528829770645

Epoch: 6| Step: 13
Training loss: 2.9420711581443713
Validation loss: 2.6255615330100395

Epoch: 59| Step: 0
Training loss: 2.9461807873396793
Validation loss: 2.6240122162749695

Epoch: 6| Step: 1
Training loss: 3.123134208636614
Validation loss: 2.624040579683648

Epoch: 6| Step: 2
Training loss: 2.8835544200066083
Validation loss: 2.6234753586091895

Epoch: 6| Step: 3
Training loss: 2.690037261534647
Validation loss: 2.6224227924091323

Epoch: 6| Step: 4
Training loss: 2.7835104682277376
Validation loss: 2.620889184798381

Epoch: 6| Step: 5
Training loss: 2.852332776690324
Validation loss: 2.6200946963768263

Epoch: 6| Step: 6
Training loss: 2.8586394817971796
Validation loss: 2.6201807621728648

Epoch: 6| Step: 7
Training loss: 2.505265603365879
Validation loss: 2.61857329920344

Epoch: 6| Step: 8
Training loss: 2.5082230751754375
Validation loss: 2.6152533676981027

Epoch: 6| Step: 9
Training loss: 3.0554379373126745
Validation loss: 2.61466957547838

Epoch: 6| Step: 10
Training loss: 2.3104497739198737
Validation loss: 2.6137394752819603

Epoch: 6| Step: 11
Training loss: 2.8472307303283437
Validation loss: 2.6123411181990486

Epoch: 6| Step: 12
Training loss: 2.513687999231987
Validation loss: 2.6114323894082885

Epoch: 6| Step: 13
Training loss: 2.589120340938609
Validation loss: 2.61033465009739

Epoch: 60| Step: 0
Training loss: 2.74919654206317
Validation loss: 2.6095165891931966

Epoch: 6| Step: 1
Training loss: 2.743204216342377
Validation loss: 2.606263748120487

Epoch: 6| Step: 2
Training loss: 2.5979885720522167
Validation loss: 2.6060437774182055

Epoch: 6| Step: 3
Training loss: 2.464028587906204
Validation loss: 2.606132136911349

Epoch: 6| Step: 4
Training loss: 2.523761645777201
Validation loss: 2.606130917128558

Epoch: 6| Step: 5
Training loss: 2.95962745048949
Validation loss: 2.6035136713858638

Epoch: 6| Step: 6
Training loss: 2.965730114954173
Validation loss: 2.604115986648782

Epoch: 6| Step: 7
Training loss: 3.1225654273894143
Validation loss: 2.601177794276026

Epoch: 6| Step: 8
Training loss: 2.6204933354322657
Validation loss: 2.6002665548135933

Epoch: 6| Step: 9
Training loss: 2.8483994590742694
Validation loss: 2.597575832259882

Epoch: 6| Step: 10
Training loss: 2.986655437527846
Validation loss: 2.594482287862232

Epoch: 6| Step: 11
Training loss: 2.8225879735895467
Validation loss: 2.5939519259739017

Epoch: 6| Step: 12
Training loss: 2.4606720311924493
Validation loss: 2.5946252564052767

Epoch: 6| Step: 13
Training loss: 2.3211504004520163
Validation loss: 2.5942537056102806

Epoch: 61| Step: 0
Training loss: 2.890928180363282
Validation loss: 2.593511945838302

Epoch: 6| Step: 1
Training loss: 2.739865006929186
Validation loss: 2.5911588479997953

Epoch: 6| Step: 2
Training loss: 2.706478177994459
Validation loss: 2.588535244867865

Epoch: 6| Step: 3
Training loss: 2.5220159063480465
Validation loss: 2.587233665138289

Epoch: 6| Step: 4
Training loss: 2.4400387771839562
Validation loss: 2.588946417640955

Epoch: 6| Step: 5
Training loss: 2.99317409893301
Validation loss: 2.5892711637429495

Epoch: 6| Step: 6
Training loss: 2.698708868380288
Validation loss: 2.588820280640376

Epoch: 6| Step: 7
Training loss: 2.665479355977749
Validation loss: 2.586404287690529

Epoch: 6| Step: 8
Training loss: 2.400741712875864
Validation loss: 2.5847343496354367

Epoch: 6| Step: 9
Training loss: 2.6877088243837743
Validation loss: 2.5824280249778657

Epoch: 6| Step: 10
Training loss: 2.849806822537494
Validation loss: 2.5848989254678503

Epoch: 6| Step: 11
Training loss: 2.6995169313411718
Validation loss: 2.5816722318253973

Epoch: 6| Step: 12
Training loss: 2.7288660849720516
Validation loss: 2.583676020647851

Epoch: 6| Step: 13
Training loss: 3.126369176375328
Validation loss: 2.580769178976577

Epoch: 62| Step: 0
Training loss: 2.076147414859147
Validation loss: 2.5768218320767495

Epoch: 6| Step: 1
Training loss: 2.7980008549828685
Validation loss: 2.579502816446858

Epoch: 6| Step: 2
Training loss: 2.3833808596302863
Validation loss: 2.5769243315172847

Epoch: 6| Step: 3
Training loss: 2.9479036544314923
Validation loss: 2.576991916927539

Epoch: 6| Step: 4
Training loss: 2.995442902895877
Validation loss: 2.5823879870790405

Epoch: 6| Step: 5
Training loss: 2.9057360779226005
Validation loss: 2.57594668137022

Epoch: 6| Step: 6
Training loss: 2.950037720002657
Validation loss: 2.5729298044299154

Epoch: 6| Step: 7
Training loss: 2.6596422861258855
Validation loss: 2.568352056959433

Epoch: 6| Step: 8
Training loss: 2.3961871894799014
Validation loss: 2.572693530663158

Epoch: 6| Step: 9
Training loss: 2.690331852236577
Validation loss: 2.575161673893083

Epoch: 6| Step: 10
Training loss: 3.1502088386698883
Validation loss: 2.5745305445104965

Epoch: 6| Step: 11
Training loss: 2.6132084447187363
Validation loss: 2.577528096467392

Epoch: 6| Step: 12
Training loss: 2.6689235157360955
Validation loss: 2.578945453218322

Epoch: 6| Step: 13
Training loss: 2.571065311280401
Validation loss: 2.577750408047404

Epoch: 63| Step: 0
Training loss: 2.7311629257463634
Validation loss: 2.5819763854907216

Epoch: 6| Step: 1
Training loss: 3.346077367514034
Validation loss: 2.5822588521624885

Epoch: 6| Step: 2
Training loss: 2.8875919409972735
Validation loss: 2.581558376608037

Epoch: 6| Step: 3
Training loss: 1.947111531151475
Validation loss: 2.582807892512377

Epoch: 6| Step: 4
Training loss: 2.8466229022530953
Validation loss: 2.584991205103342

Epoch: 6| Step: 5
Training loss: 2.546512788630741
Validation loss: 2.585696651925002

Epoch: 6| Step: 6
Training loss: 2.8410930587850793
Validation loss: 2.583041210475444

Epoch: 6| Step: 7
Training loss: 2.6449337629018124
Validation loss: 2.5815817421953056

Epoch: 6| Step: 8
Training loss: 2.7602334783457123
Validation loss: 2.5813138410479888

Epoch: 6| Step: 9
Training loss: 3.151418027653896
Validation loss: 2.578457481446479

Epoch: 6| Step: 10
Training loss: 2.5822598677864073
Validation loss: 2.576874832522171

Epoch: 6| Step: 11
Training loss: 2.3432846624638626
Validation loss: 2.5742693342796903

Epoch: 6| Step: 12
Training loss: 2.8829818717182825
Validation loss: 2.5748934751531434

Epoch: 6| Step: 13
Training loss: 2.1973029510718503
Validation loss: 2.57291411323781

Epoch: 64| Step: 0
Training loss: 2.846645515982724
Validation loss: 2.568482834782199

Epoch: 6| Step: 1
Training loss: 2.2066582018448817
Validation loss: 2.5664097956725334

Epoch: 6| Step: 2
Training loss: 2.7621968029602257
Validation loss: 2.564668513189368

Epoch: 6| Step: 3
Training loss: 2.8297010213949103
Validation loss: 2.5619237918741233

Epoch: 6| Step: 4
Training loss: 2.8311746077783693
Validation loss: 2.559707686429612

Epoch: 6| Step: 5
Training loss: 2.3688868098143487
Validation loss: 2.559348610915446

Epoch: 6| Step: 6
Training loss: 2.925626417068826
Validation loss: 2.558557967307036

Epoch: 6| Step: 7
Training loss: 2.118476053267783
Validation loss: 2.5547314405794017

Epoch: 6| Step: 8
Training loss: 2.336098859948647
Validation loss: 2.556707316734038

Epoch: 6| Step: 9
Training loss: 2.9366158311350246
Validation loss: 2.554884760062458

Epoch: 6| Step: 10
Training loss: 2.5310156442919096
Validation loss: 2.5577409793686114

Epoch: 6| Step: 11
Training loss: 2.797914706011947
Validation loss: 2.5591228374859067

Epoch: 6| Step: 12
Training loss: 2.9829411282588536
Validation loss: 2.56456583380184

Epoch: 6| Step: 13
Training loss: 3.1593878227901877
Validation loss: 2.558831139343205

Epoch: 65| Step: 0
Training loss: 2.6213438821478805
Validation loss: 2.550517794452955

Epoch: 6| Step: 1
Training loss: 2.6853432983919405
Validation loss: 2.5501998230923784

Epoch: 6| Step: 2
Training loss: 2.727386787225785
Validation loss: 2.552875245421705

Epoch: 6| Step: 3
Training loss: 2.7334781592853523
Validation loss: 2.5514855962884093

Epoch: 6| Step: 4
Training loss: 2.895664420564551
Validation loss: 2.551440852252174

Epoch: 6| Step: 5
Training loss: 2.7233710643655655
Validation loss: 2.5542654452947633

Epoch: 6| Step: 6
Training loss: 1.8694279847436814
Validation loss: 2.5557582683835594

Epoch: 6| Step: 7
Training loss: 2.7510438151834444
Validation loss: 2.5539250378303215

Epoch: 6| Step: 8
Training loss: 2.906831088475681
Validation loss: 2.5554948594537894

Epoch: 6| Step: 9
Training loss: 3.1019286893202316
Validation loss: 2.553562099186956

Epoch: 6| Step: 10
Training loss: 2.3496117088452144
Validation loss: 2.552478375643732

Epoch: 6| Step: 11
Training loss: 2.1894300393240993
Validation loss: 2.551762110417831

Epoch: 6| Step: 12
Training loss: 2.3471922963458685
Validation loss: 2.5498772149038897

Epoch: 6| Step: 13
Training loss: 3.3975305509000675
Validation loss: 2.5491037582694394

Epoch: 66| Step: 0
Training loss: 2.4688619274407304
Validation loss: 2.5443141208637803

Epoch: 6| Step: 1
Training loss: 2.6343322352222405
Validation loss: 2.542927622035246

Epoch: 6| Step: 2
Training loss: 2.968886362256836
Validation loss: 2.544584293376749

Epoch: 6| Step: 3
Training loss: 2.9972222501911694
Validation loss: 2.549171239527275

Epoch: 6| Step: 4
Training loss: 2.4762869600390567
Validation loss: 2.552066076311167

Epoch: 6| Step: 5
Training loss: 2.856001568096596
Validation loss: 2.544674677637909

Epoch: 6| Step: 6
Training loss: 2.6934365641132785
Validation loss: 2.5369026270168202

Epoch: 6| Step: 7
Training loss: 2.994268027703685
Validation loss: 2.535100355350133

Epoch: 6| Step: 8
Training loss: 2.8237991548931545
Validation loss: 2.535473757132161

Epoch: 6| Step: 9
Training loss: 2.3750024092812616
Validation loss: 2.5411583178422608

Epoch: 6| Step: 10
Training loss: 2.9366660050194033
Validation loss: 2.537373032548811

Epoch: 6| Step: 11
Training loss: 2.0565137978018324
Validation loss: 2.5402619800478474

Epoch: 6| Step: 12
Training loss: 2.407078587934181
Validation loss: 2.53782476529854

Epoch: 6| Step: 13
Training loss: 2.632108056433046
Validation loss: 2.5401210593237478

Epoch: 67| Step: 0
Training loss: 2.979850494935605
Validation loss: 2.5355105551703887

Epoch: 6| Step: 1
Training loss: 2.647798624316366
Validation loss: 2.535457755774314

Epoch: 6| Step: 2
Training loss: 2.162172890327232
Validation loss: 2.537211787325306

Epoch: 6| Step: 3
Training loss: 2.9083562622332377
Validation loss: 2.532496926198175

Epoch: 6| Step: 4
Training loss: 2.481198373252659
Validation loss: 2.5381306508181485

Epoch: 6| Step: 5
Training loss: 2.5962815397327637
Validation loss: 2.536859129481465

Epoch: 6| Step: 6
Training loss: 2.967644736689876
Validation loss: 2.544889225900389

Epoch: 6| Step: 7
Training loss: 2.783825142937795
Validation loss: 2.548295933299517

Epoch: 6| Step: 8
Training loss: 2.40785243194956
Validation loss: 2.538893348732594

Epoch: 6| Step: 9
Training loss: 2.594920985612052
Validation loss: 2.532079450867864

Epoch: 6| Step: 10
Training loss: 2.5657172127250947
Validation loss: 2.5321997128519738

Epoch: 6| Step: 11
Training loss: 2.685040346407241
Validation loss: 2.5317294918583366

Epoch: 6| Step: 12
Training loss: 2.7544748138699946
Validation loss: 2.5360683217036235

Epoch: 6| Step: 13
Training loss: 2.878931757116481
Validation loss: 2.5343557621248847

Epoch: 68| Step: 0
Training loss: 1.8486441618444536
Validation loss: 2.5341633800454755

Epoch: 6| Step: 1
Training loss: 2.7510070690756145
Validation loss: 2.537750311972144

Epoch: 6| Step: 2
Training loss: 2.5211083968818406
Validation loss: 2.537046193351012

Epoch: 6| Step: 3
Training loss: 2.7888918698899885
Validation loss: 2.536150094376003

Epoch: 6| Step: 4
Training loss: 2.959278940577338
Validation loss: 2.5360552384581023

Epoch: 6| Step: 5
Training loss: 2.284940686614253
Validation loss: 2.5330069609851473

Epoch: 6| Step: 6
Training loss: 3.0724627315784443
Validation loss: 2.530049294354642

Epoch: 6| Step: 7
Training loss: 2.4413683590809643
Validation loss: 2.5288579336537103

Epoch: 6| Step: 8
Training loss: 2.4170696207608966
Validation loss: 2.5313004853665233

Epoch: 6| Step: 9
Training loss: 2.7778303575837593
Validation loss: 2.5289132120584403

Epoch: 6| Step: 10
Training loss: 2.5594144713323934
Validation loss: 2.5318730219438796

Epoch: 6| Step: 11
Training loss: 2.6727946522416866
Validation loss: 2.5307736203615896

Epoch: 6| Step: 12
Training loss: 3.115714047612402
Validation loss: 2.5279098261995343

Epoch: 6| Step: 13
Training loss: 2.9052719603462633
Validation loss: 2.524864006157166

Epoch: 69| Step: 0
Training loss: 2.790727049188747
Validation loss: 2.524643002820814

Epoch: 6| Step: 1
Training loss: 2.831537200979748
Validation loss: 2.5241922169153432

Epoch: 6| Step: 2
Training loss: 2.6728238211304705
Validation loss: 2.5260918733516644

Epoch: 6| Step: 3
Training loss: 2.8059973248273913
Validation loss: 2.5262835095180933

Epoch: 6| Step: 4
Training loss: 2.982546420891215
Validation loss: 2.5219204715377828

Epoch: 6| Step: 5
Training loss: 2.7409372114903987
Validation loss: 2.527213311895137

Epoch: 6| Step: 6
Training loss: 2.463527128299024
Validation loss: 2.5227898080454914

Epoch: 6| Step: 7
Training loss: 2.200616477076573
Validation loss: 2.525067978752768

Epoch: 6| Step: 8
Training loss: 2.246988612500524
Validation loss: 2.523962700420595

Epoch: 6| Step: 9
Training loss: 2.3927899698943187
Validation loss: 2.5298395269436935

Epoch: 6| Step: 10
Training loss: 2.3433062324023974
Validation loss: 2.546885016486007

Epoch: 6| Step: 11
Training loss: 2.8355742923405853
Validation loss: 2.5453795584815317

Epoch: 6| Step: 12
Training loss: 2.841740159979589
Validation loss: 2.5627011320993276

Epoch: 6| Step: 13
Training loss: 3.15950433634252
Validation loss: 2.574991174948177

Epoch: 70| Step: 0
Training loss: 3.2930481019872686
Validation loss: 2.560713432087908

Epoch: 6| Step: 1
Training loss: 2.4675516044685533
Validation loss: 2.542987969907446

Epoch: 6| Step: 2
Training loss: 2.97576076493507
Validation loss: 2.5291188691685407

Epoch: 6| Step: 3
Training loss: 2.2315536506113047
Validation loss: 2.5237745408352428

Epoch: 6| Step: 4
Training loss: 2.5219326197058045
Validation loss: 2.5236369271696666

Epoch: 6| Step: 5
Training loss: 2.8159028554437717
Validation loss: 2.5235742742518217

Epoch: 6| Step: 6
Training loss: 2.375174465798451
Validation loss: 2.5191350581030276

Epoch: 6| Step: 7
Training loss: 2.1312282393343014
Validation loss: 2.521109705085338

Epoch: 6| Step: 8
Training loss: 2.5672929181437927
Validation loss: 2.5219617214993666

Epoch: 6| Step: 9
Training loss: 2.2343894051040785
Validation loss: 2.523840353414155

Epoch: 6| Step: 10
Training loss: 2.8320930421599293
Validation loss: 2.522463741539689

Epoch: 6| Step: 11
Training loss: 3.2136854307557177
Validation loss: 2.521728503806788

Epoch: 6| Step: 12
Training loss: 2.658772785212883
Validation loss: 2.52253015541121

Epoch: 6| Step: 13
Training loss: 2.793705001571072
Validation loss: 2.524727144386113

Epoch: 71| Step: 0
Training loss: 2.6510723139721994
Validation loss: 2.5266857833524954

Epoch: 6| Step: 1
Training loss: 2.751039048616973
Validation loss: 2.523831819918223

Epoch: 6| Step: 2
Training loss: 2.972281513830905
Validation loss: 2.5301019082132674

Epoch: 6| Step: 3
Training loss: 2.737662121575038
Validation loss: 2.5296708035561104

Epoch: 6| Step: 4
Training loss: 3.0631263248150415
Validation loss: 2.5308192793232225

Epoch: 6| Step: 5
Training loss: 2.4392881436905354
Validation loss: 2.531465843008924

Epoch: 6| Step: 6
Training loss: 2.640587620216664
Validation loss: 2.526603672916494

Epoch: 6| Step: 7
Training loss: 2.2437283740688314
Validation loss: 2.519604767113695

Epoch: 6| Step: 8
Training loss: 2.221473955962082
Validation loss: 2.51615070986619

Epoch: 6| Step: 9
Training loss: 2.7286870598098156
Validation loss: 2.5170320955022834

Epoch: 6| Step: 10
Training loss: 2.4562387325426402
Validation loss: 2.514341544278702

Epoch: 6| Step: 11
Training loss: 2.5596357014849604
Validation loss: 2.5114249635555077

Epoch: 6| Step: 12
Training loss: 2.6622904592092835
Validation loss: 2.5066926541486234

Epoch: 6| Step: 13
Training loss: 2.831283912708217
Validation loss: 2.5066086522788353

Epoch: 72| Step: 0
Training loss: 2.577344926670219
Validation loss: 2.5036237044467793

Epoch: 6| Step: 1
Training loss: 2.363773717090515
Validation loss: 2.5035873424241144

Epoch: 6| Step: 2
Training loss: 2.8755819312230124
Validation loss: 2.5060771352968483

Epoch: 6| Step: 3
Training loss: 2.388756514240447
Validation loss: 2.510848492301137

Epoch: 6| Step: 4
Training loss: 2.4250611051745823
Validation loss: 2.506087964940743

Epoch: 6| Step: 5
Training loss: 2.987600291336419
Validation loss: 2.507044784590199

Epoch: 6| Step: 6
Training loss: 2.8414233404636215
Validation loss: 2.5052343566769544

Epoch: 6| Step: 7
Training loss: 2.360732237642689
Validation loss: 2.506153211365678

Epoch: 6| Step: 8
Training loss: 2.740793770581597
Validation loss: 2.5012378727410636

Epoch: 6| Step: 9
Training loss: 2.6140902739495564
Validation loss: 2.505815211734721

Epoch: 6| Step: 10
Training loss: 2.790111697022245
Validation loss: 2.5047288040496394

Epoch: 6| Step: 11
Training loss: 2.8259413070847113
Validation loss: 2.505961177479545

Epoch: 6| Step: 12
Training loss: 2.2951862588733714
Validation loss: 2.500909258477929

Epoch: 6| Step: 13
Training loss: 2.6537027150135732
Validation loss: 2.5083875779108897

Epoch: 73| Step: 0
Training loss: 2.720793383676481
Validation loss: 2.501651758670121

Epoch: 6| Step: 1
Training loss: 2.5165304130233817
Validation loss: 2.5069957048688485

Epoch: 6| Step: 2
Training loss: 2.6893361051611673
Validation loss: 2.522713737506157

Epoch: 6| Step: 3
Training loss: 2.8678041049767478
Validation loss: 2.519447423836674

Epoch: 6| Step: 4
Training loss: 2.9464670847567898
Validation loss: 2.5030645978341397

Epoch: 6| Step: 5
Training loss: 3.077188572066496
Validation loss: 2.50434464590719

Epoch: 6| Step: 6
Training loss: 2.5059651733803707
Validation loss: 2.502505509214533

Epoch: 6| Step: 7
Training loss: 2.4918767561952992
Validation loss: 2.5125064040018583

Epoch: 6| Step: 8
Training loss: 2.4326886350720556
Validation loss: 2.514429120126045

Epoch: 6| Step: 9
Training loss: 2.5568120212673375
Validation loss: 2.516394045465705

Epoch: 6| Step: 10
Training loss: 2.7302654665475785
Validation loss: 2.5215461500791863

Epoch: 6| Step: 11
Training loss: 2.6272649530305943
Validation loss: 2.526510880665841

Epoch: 6| Step: 12
Training loss: 2.815113379327813
Validation loss: 2.52602067656825

Epoch: 6| Step: 13
Training loss: 2.294634290491539
Validation loss: 2.5279147619874576

Epoch: 74| Step: 0
Training loss: 2.6670541382462916
Validation loss: 2.5273121630872835

Epoch: 6| Step: 1
Training loss: 2.8750368198856586
Validation loss: 2.523063419356898

Epoch: 6| Step: 2
Training loss: 2.4718812809802864
Validation loss: 2.52399780855522

Epoch: 6| Step: 3
Training loss: 2.260613627722888
Validation loss: 2.5178428019508474

Epoch: 6| Step: 4
Training loss: 2.189808417697649
Validation loss: 2.512044945750093

Epoch: 6| Step: 5
Training loss: 3.0884426330505224
Validation loss: 2.508560433567968

Epoch: 6| Step: 6
Training loss: 2.73546234309655
Validation loss: 2.504454776314236

Epoch: 6| Step: 7
Training loss: 2.499949550119624
Validation loss: 2.5013117368883506

Epoch: 6| Step: 8
Training loss: 2.6360730546196915
Validation loss: 2.502232889559593

Epoch: 6| Step: 9
Training loss: 2.4120706284843476
Validation loss: 2.499533514370077

Epoch: 6| Step: 10
Training loss: 2.7102672616848587
Validation loss: 2.50015816188234

Epoch: 6| Step: 11
Training loss: 2.6076976387414352
Validation loss: 2.494649310951611

Epoch: 6| Step: 12
Training loss: 3.0582250224240797
Validation loss: 2.497532341931926

Epoch: 6| Step: 13
Training loss: 2.704278424211216
Validation loss: 2.4957527480916726

Epoch: 75| Step: 0
Training loss: 3.182659587829329
Validation loss: 2.495273047230663

Epoch: 6| Step: 1
Training loss: 2.688613306150545
Validation loss: 2.4954491202277613

Epoch: 6| Step: 2
Training loss: 2.7161956714265036
Validation loss: 2.4957439434196216

Epoch: 6| Step: 3
Training loss: 2.4231938862546842
Validation loss: 2.4936511326872752

Epoch: 6| Step: 4
Training loss: 2.7824538122355458
Validation loss: 2.4888484194659437

Epoch: 6| Step: 5
Training loss: 2.7417586553692126
Validation loss: 2.492629327942109

Epoch: 6| Step: 6
Training loss: 2.6808871150370686
Validation loss: 2.4959033143215557

Epoch: 6| Step: 7
Training loss: 2.873634677612305
Validation loss: 2.4941096531700713

Epoch: 6| Step: 8
Training loss: 2.87249447655209
Validation loss: 2.4961351083782635

Epoch: 6| Step: 9
Training loss: 2.405790210738654
Validation loss: 2.496460809518326

Epoch: 6| Step: 10
Training loss: 2.323836710329367
Validation loss: 2.4952627757896146

Epoch: 6| Step: 11
Training loss: 1.996987935724622
Validation loss: 2.496783842190064

Epoch: 6| Step: 12
Training loss: 2.3770708793115523
Validation loss: 2.4933395194505596

Epoch: 6| Step: 13
Training loss: 2.566487814711764
Validation loss: 2.4907535266141

Epoch: 76| Step: 0
Training loss: 2.638347478729454
Validation loss: 2.494999955839209

Epoch: 6| Step: 1
Training loss: 2.3577531709784187
Validation loss: 2.4901459243125332

Epoch: 6| Step: 2
Training loss: 2.945507579383818
Validation loss: 2.4925634246123263

Epoch: 6| Step: 3
Training loss: 3.088963205905659
Validation loss: 2.4927374572795427

Epoch: 6| Step: 4
Training loss: 2.4889239045737797
Validation loss: 2.4931994290102786

Epoch: 6| Step: 5
Training loss: 2.436127104852309
Validation loss: 2.4957603267718547

Epoch: 6| Step: 6
Training loss: 2.4299717580464932
Validation loss: 2.4904659148638744

Epoch: 6| Step: 7
Training loss: 2.7270193509687206
Validation loss: 2.4902044077547374

Epoch: 6| Step: 8
Training loss: 2.6130438498647353
Validation loss: 2.493460088145389

Epoch: 6| Step: 9
Training loss: 2.771393358431055
Validation loss: 2.494017930325048

Epoch: 6| Step: 10
Training loss: 2.3221344136738633
Validation loss: 2.490067061245074

Epoch: 6| Step: 11
Training loss: 2.6173361664865364
Validation loss: 2.494310086994804

Epoch: 6| Step: 12
Training loss: 2.172383310676235
Validation loss: 2.4917160752244754

Epoch: 6| Step: 13
Training loss: 2.9604501448976985
Validation loss: 2.4900216444318257

Epoch: 77| Step: 0
Training loss: 3.1268597981466364
Validation loss: 2.489139666322009

Epoch: 6| Step: 1
Training loss: 2.315890327110105
Validation loss: 2.492223263028527

Epoch: 6| Step: 2
Training loss: 2.707165118894123
Validation loss: 2.4896602272917074

Epoch: 6| Step: 3
Training loss: 2.7604660797494884
Validation loss: 2.4897593245822125

Epoch: 6| Step: 4
Training loss: 2.6624149362649465
Validation loss: 2.4946528789697218

Epoch: 6| Step: 5
Training loss: 2.978463114291221
Validation loss: 2.492721564171048

Epoch: 6| Step: 6
Training loss: 2.3978808623777517
Validation loss: 2.4962073804196647

Epoch: 6| Step: 7
Training loss: 2.8676009126980757
Validation loss: 2.495155966221607

Epoch: 6| Step: 8
Training loss: 2.963353442140656
Validation loss: 2.4997583272470787

Epoch: 6| Step: 9
Training loss: 2.0637019729802244
Validation loss: 2.5019687450595343

Epoch: 6| Step: 10
Training loss: 2.8967936910763368
Validation loss: 2.4983374153529696

Epoch: 6| Step: 11
Training loss: 2.4781714186103208
Validation loss: 2.4966805832359977

Epoch: 6| Step: 12
Training loss: 2.401064549224668
Validation loss: 2.4930913675251474

Epoch: 6| Step: 13
Training loss: 1.897455408831369
Validation loss: 2.4923087223112135

Epoch: 78| Step: 0
Training loss: 2.6345167671693765
Validation loss: 2.4929996470632045

Epoch: 6| Step: 1
Training loss: 2.247241766736838
Validation loss: 2.491121979145357

Epoch: 6| Step: 2
Training loss: 2.812529245860313
Validation loss: 2.4847316206082706

Epoch: 6| Step: 3
Training loss: 2.727191229527348
Validation loss: 2.48439097849188

Epoch: 6| Step: 4
Training loss: 3.3231517903882226
Validation loss: 2.4872434998220054

Epoch: 6| Step: 5
Training loss: 2.6789142171319082
Validation loss: 2.486833088098132

Epoch: 6| Step: 6
Training loss: 2.5459729803049505
Validation loss: 2.483606207553259

Epoch: 6| Step: 7
Training loss: 2.5250218381031777
Validation loss: 2.4834473835487767

Epoch: 6| Step: 8
Training loss: 2.424190472638616
Validation loss: 2.4868394795811755

Epoch: 6| Step: 9
Training loss: 2.463521127971389
Validation loss: 2.4837986179528064

Epoch: 6| Step: 10
Training loss: 1.5448910098844757
Validation loss: 2.483845252412134

Epoch: 6| Step: 11
Training loss: 2.927276677344924
Validation loss: 2.4860734711774723

Epoch: 6| Step: 12
Training loss: 2.6977055195745545
Validation loss: 2.4877902374402328

Epoch: 6| Step: 13
Training loss: 2.668739129430711
Validation loss: 2.481493784653618

Epoch: 79| Step: 0
Training loss: 2.66863906951416
Validation loss: 2.486241772898415

Epoch: 6| Step: 1
Training loss: 2.5700030386940176
Validation loss: 2.482798125903815

Epoch: 6| Step: 2
Training loss: 2.13444774944935
Validation loss: 2.4877170819459153

Epoch: 6| Step: 3
Training loss: 2.253869861328875
Validation loss: 2.4862784205660367

Epoch: 6| Step: 4
Training loss: 2.9114402447424634
Validation loss: 2.4864088086833616

Epoch: 6| Step: 5
Training loss: 2.7319925446750686
Validation loss: 2.4908309640763586

Epoch: 6| Step: 6
Training loss: 3.176051504875369
Validation loss: 2.491770598839183

Epoch: 6| Step: 7
Training loss: 2.4387187356279294
Validation loss: 2.4893124218838665

Epoch: 6| Step: 8
Training loss: 2.8101826763845974
Validation loss: 2.486733570588027

Epoch: 6| Step: 9
Training loss: 2.3902287372959643
Validation loss: 2.488736672396877

Epoch: 6| Step: 10
Training loss: 2.8956341206114455
Validation loss: 2.487551723434486

Epoch: 6| Step: 11
Training loss: 2.329229822366005
Validation loss: 2.4856101430542132

Epoch: 6| Step: 12
Training loss: 2.2097305939736445
Validation loss: 2.485350930963922

Epoch: 6| Step: 13
Training loss: 2.8819783976034468
Validation loss: 2.483204299872873

Epoch: 80| Step: 0
Training loss: 3.231733339857088
Validation loss: 2.478468088626547

Epoch: 6| Step: 1
Training loss: 2.4080967937671973
Validation loss: 2.4858536548489347

Epoch: 6| Step: 2
Training loss: 2.355376335089814
Validation loss: 2.4831886977974342

Epoch: 6| Step: 3
Training loss: 2.460366900460497
Validation loss: 2.4814925836701707

Epoch: 6| Step: 4
Training loss: 2.5473679141616152
Validation loss: 2.4824348887560883

Epoch: 6| Step: 5
Training loss: 2.195613093855599
Validation loss: 2.4844910456587597

Epoch: 6| Step: 6
Training loss: 2.7426496589045524
Validation loss: 2.485950090626998

Epoch: 6| Step: 7
Training loss: 3.211563680436453
Validation loss: 2.4830180687023526

Epoch: 6| Step: 8
Training loss: 2.4254925689677798
Validation loss: 2.4811913106208703

Epoch: 6| Step: 9
Training loss: 3.115782609968643
Validation loss: 2.4820014123320884

Epoch: 6| Step: 10
Training loss: 2.296157517250212
Validation loss: 2.478768730817398

Epoch: 6| Step: 11
Training loss: 2.511963827893237
Validation loss: 2.4790521857065

Epoch: 6| Step: 12
Training loss: 2.3902936718289345
Validation loss: 2.4843167472103813

Epoch: 6| Step: 13
Training loss: 2.349367860223547
Validation loss: 2.4851573296114573

Epoch: 81| Step: 0
Training loss: 2.1368586687935047
Validation loss: 2.4813643633336957

Epoch: 6| Step: 1
Training loss: 2.960244935701386
Validation loss: 2.4836468379228744

Epoch: 6| Step: 2
Training loss: 2.7416794352441842
Validation loss: 2.485013579824634

Epoch: 6| Step: 3
Training loss: 2.642350665648427
Validation loss: 2.475754847836678

Epoch: 6| Step: 4
Training loss: 2.887023661903319
Validation loss: 2.4813055754490367

Epoch: 6| Step: 5
Training loss: 2.2148316936400256
Validation loss: 2.483481624408569

Epoch: 6| Step: 6
Training loss: 2.6977637602337
Validation loss: 2.480791550505366

Epoch: 6| Step: 7
Training loss: 2.520874233281862
Validation loss: 2.476442336126556

Epoch: 6| Step: 8
Training loss: 2.377604010680501
Validation loss: 2.482555130714225

Epoch: 6| Step: 9
Training loss: 2.666327176736441
Validation loss: 2.484427013692442

Epoch: 6| Step: 10
Training loss: 2.5313163089308643
Validation loss: 2.4825402768579043

Epoch: 6| Step: 11
Training loss: 2.666922725861046
Validation loss: 2.4825022294520123

Epoch: 6| Step: 12
Training loss: 1.9329663424376287
Validation loss: 2.487536460123638

Epoch: 6| Step: 13
Training loss: 3.180752812413174
Validation loss: 2.485839779809549

Epoch: 82| Step: 0
Training loss: 2.6736041268481783
Validation loss: 2.4808353183248477

Epoch: 6| Step: 1
Training loss: 2.4622852328604514
Validation loss: 2.488483510562714

Epoch: 6| Step: 2
Training loss: 2.0912587161888623
Validation loss: 2.481083014415926

Epoch: 6| Step: 3
Training loss: 2.785485803225919
Validation loss: 2.4844279573502366

Epoch: 6| Step: 4
Training loss: 2.91678457475937
Validation loss: 2.482457954790832

Epoch: 6| Step: 5
Training loss: 3.066096160177001
Validation loss: 2.4845150602194783

Epoch: 6| Step: 6
Training loss: 2.346003656883674
Validation loss: 2.4858790229522736

Epoch: 6| Step: 7
Training loss: 2.6626746178617973
Validation loss: 2.489155103386081

Epoch: 6| Step: 8
Training loss: 2.8426868840954502
Validation loss: 2.475543882676156

Epoch: 6| Step: 9
Training loss: 2.643769057184888
Validation loss: 2.4809074914945457

Epoch: 6| Step: 10
Training loss: 2.9638914808003576
Validation loss: 2.483568136477722

Epoch: 6| Step: 11
Training loss: 1.652966465406194
Validation loss: 2.4802551337746115

Epoch: 6| Step: 12
Training loss: 2.6759579342613513
Validation loss: 2.481151192569079

Epoch: 6| Step: 13
Training loss: 2.3267228173126933
Validation loss: 2.482787962900897

Epoch: 83| Step: 0
Training loss: 2.7934425645682692
Validation loss: 2.4827867945530135

Epoch: 6| Step: 1
Training loss: 2.7350129173573516
Validation loss: 2.478921442699633

Epoch: 6| Step: 2
Training loss: 1.855471962373864
Validation loss: 2.4795175245326906

Epoch: 6| Step: 3
Training loss: 1.4454574924633068
Validation loss: 2.4773487884588943

Epoch: 6| Step: 4
Training loss: 2.6280298458477414
Validation loss: 2.474884234716898

Epoch: 6| Step: 5
Training loss: 2.7360749981901877
Validation loss: 2.48701771544827

Epoch: 6| Step: 6
Training loss: 2.5158642009481165
Validation loss: 2.47518202115561

Epoch: 6| Step: 7
Training loss: 3.057445793014437
Validation loss: 2.477668747786297

Epoch: 6| Step: 8
Training loss: 2.436585474848998
Validation loss: 2.480307890714842

Epoch: 6| Step: 9
Training loss: 2.45459717718531
Validation loss: 2.484459929544047

Epoch: 6| Step: 10
Training loss: 3.0853635616398787
Validation loss: 2.486154123167439

Epoch: 6| Step: 11
Training loss: 2.6563383199930826
Validation loss: 2.4832069162113104

Epoch: 6| Step: 12
Training loss: 2.195759575007376
Validation loss: 2.4756729501935815

Epoch: 6| Step: 13
Training loss: 3.1572813860554816
Validation loss: 2.4803554158589067

Epoch: 84| Step: 0
Training loss: 3.021046878677003
Validation loss: 2.479073760485019

Epoch: 6| Step: 1
Training loss: 2.127998480487941
Validation loss: 2.47989330275008

Epoch: 6| Step: 2
Training loss: 2.423634142494498
Validation loss: 2.4824162404866557

Epoch: 6| Step: 3
Training loss: 2.7003476201676095
Validation loss: 2.478045271384247

Epoch: 6| Step: 4
Training loss: 2.273254072517314
Validation loss: 2.483963250481667

Epoch: 6| Step: 5
Training loss: 2.7098250951313467
Validation loss: 2.4797898532420994

Epoch: 6| Step: 6
Training loss: 1.910023316046319
Validation loss: 2.48431828272154

Epoch: 6| Step: 7
Training loss: 2.6176473185153992
Validation loss: 2.480977372214554

Epoch: 6| Step: 8
Training loss: 2.787721541286746
Validation loss: 2.4821433674680127

Epoch: 6| Step: 9
Training loss: 2.68886957091581
Validation loss: 2.479039410656272

Epoch: 6| Step: 10
Training loss: 2.763073450007567
Validation loss: 2.482210620021808

Epoch: 6| Step: 11
Training loss: 2.444859666912102
Validation loss: 2.4817966060894663

Epoch: 6| Step: 12
Training loss: 2.8986314569853984
Validation loss: 2.4737504945420072

Epoch: 6| Step: 13
Training loss: 2.666910388855666
Validation loss: 2.477337841202125

Epoch: 85| Step: 0
Training loss: 2.4110938525229226
Validation loss: 2.4783902167860017

Epoch: 6| Step: 1
Training loss: 2.7289348435890144
Validation loss: 2.48162686603194

Epoch: 6| Step: 2
Training loss: 2.4392549115079034
Validation loss: 2.4809476775536297

Epoch: 6| Step: 3
Training loss: 2.6797631255841856
Validation loss: 2.4819494598780114

Epoch: 6| Step: 4
Training loss: 2.6644436292335096
Validation loss: 2.479981157733751

Epoch: 6| Step: 5
Training loss: 2.4785134125347197
Validation loss: 2.480663958700346

Epoch: 6| Step: 6
Training loss: 2.5436866321215907
Validation loss: 2.4816925314005234

Epoch: 6| Step: 7
Training loss: 2.5603533801913985
Validation loss: 2.4833671396843173

Epoch: 6| Step: 8
Training loss: 2.3084947077488276
Validation loss: 2.481794668738969

Epoch: 6| Step: 9
Training loss: 2.343827513366662
Validation loss: 2.4783971751681917

Epoch: 6| Step: 10
Training loss: 2.0898199062235
Validation loss: 2.483763965502793

Epoch: 6| Step: 11
Training loss: 3.3013168712979972
Validation loss: 2.4729416749284776

Epoch: 6| Step: 12
Training loss: 2.727092877151035
Validation loss: 2.471065224309909

Epoch: 6| Step: 13
Training loss: 2.898931168211792
Validation loss: 2.4782664697585277

Epoch: 86| Step: 0
Training loss: 3.3182498763386232
Validation loss: 2.478683253582468

Epoch: 6| Step: 1
Training loss: 2.4002527580678468
Validation loss: 2.4797121671312268

Epoch: 6| Step: 2
Training loss: 2.2180538966008734
Validation loss: 2.4771174020779823

Epoch: 6| Step: 3
Training loss: 2.941688473398216
Validation loss: 2.4777503469209874

Epoch: 6| Step: 4
Training loss: 2.8101912453078843
Validation loss: 2.479016489133871

Epoch: 6| Step: 5
Training loss: 1.8232482027384704
Validation loss: 2.4777377094950204

Epoch: 6| Step: 6
Training loss: 2.4701769604014694
Validation loss: 2.474062623619895

Epoch: 6| Step: 7
Training loss: 2.0997293979139577
Validation loss: 2.478229591372051

Epoch: 6| Step: 8
Training loss: 2.534893098642507
Validation loss: 2.4729365812130197

Epoch: 6| Step: 9
Training loss: 2.2345273692857095
Validation loss: 2.4712088689241725

Epoch: 6| Step: 10
Training loss: 2.6338746958988555
Validation loss: 2.477813934006302

Epoch: 6| Step: 11
Training loss: 2.7575902227891853
Validation loss: 2.4769123203373913

Epoch: 6| Step: 12
Training loss: 2.6999010915653634
Validation loss: 2.482556859390006

Epoch: 6| Step: 13
Training loss: 2.9762968810906028
Validation loss: 2.4724205029252437

Epoch: 87| Step: 0
Training loss: 2.681063907452093
Validation loss: 2.4733331365490994

Epoch: 6| Step: 1
Training loss: 2.3955599642712837
Validation loss: 2.4725454907271183

Epoch: 6| Step: 2
Training loss: 2.3316473205541848
Validation loss: 2.4760564854721014

Epoch: 6| Step: 3
Training loss: 2.20487354772336
Validation loss: 2.469501187926874

Epoch: 6| Step: 4
Training loss: 2.1003467773037907
Validation loss: 2.4745473434915057

Epoch: 6| Step: 5
Training loss: 2.9157446539266525
Validation loss: 2.475114015720948

Epoch: 6| Step: 6
Training loss: 2.5622480315097493
Validation loss: 2.4892452175320097

Epoch: 6| Step: 7
Training loss: 3.2456943160628597
Validation loss: 2.488553130638529

Epoch: 6| Step: 8
Training loss: 2.2663823407558015
Validation loss: 2.4907995682137236

Epoch: 6| Step: 9
Training loss: 2.9003577439213615
Validation loss: 2.4899176023375063

Epoch: 6| Step: 10
Training loss: 2.788944188448255
Validation loss: 2.4765184881910516

Epoch: 6| Step: 11
Training loss: 2.920455506212949
Validation loss: 2.47908546143793

Epoch: 6| Step: 12
Training loss: 2.310818447427806
Validation loss: 2.481034277979881

Epoch: 6| Step: 13
Training loss: 2.6640433481372625
Validation loss: 2.490222901959886

Epoch: 88| Step: 0
Training loss: 2.4217817719266495
Validation loss: 2.4955585843292716

Epoch: 6| Step: 1
Training loss: 2.2313008535023537
Validation loss: 2.514583285083365

Epoch: 6| Step: 2
Training loss: 2.368547206441245
Validation loss: 2.5315453627830027

Epoch: 6| Step: 3
Training loss: 2.7809924745811085
Validation loss: 2.5355648892870075

Epoch: 6| Step: 4
Training loss: 3.058852846125967
Validation loss: 2.5405747043062132

Epoch: 6| Step: 5
Training loss: 3.3323852780306407
Validation loss: 2.537836696417183

Epoch: 6| Step: 6
Training loss: 2.225301177825431
Validation loss: 2.5483372397067474

Epoch: 6| Step: 7
Training loss: 2.641577012940454
Validation loss: 2.545890336925524

Epoch: 6| Step: 8
Training loss: 2.4430805782125575
Validation loss: 2.555160043236535

Epoch: 6| Step: 9
Training loss: 2.8762920627560473
Validation loss: 2.5514570181309484

Epoch: 6| Step: 10
Training loss: 2.7270718074250966
Validation loss: 2.549183974874979

Epoch: 6| Step: 11
Training loss: 3.0887015410156833
Validation loss: 2.5326247230186842

Epoch: 6| Step: 12
Training loss: 2.422719500570048
Validation loss: 2.5206621337849193

Epoch: 6| Step: 13
Training loss: 2.3499704846091016
Validation loss: 2.5185890506028668

Epoch: 89| Step: 0
Training loss: 2.4406684432028034
Validation loss: 2.5091434801091004

Epoch: 6| Step: 1
Training loss: 3.2784877230955556
Validation loss: 2.5018886425832148

Epoch: 6| Step: 2
Training loss: 2.8682189250370898
Validation loss: 2.4988228728733284

Epoch: 6| Step: 3
Training loss: 2.5846722322367497
Validation loss: 2.49384859188162

Epoch: 6| Step: 4
Training loss: 2.3509579511911824
Validation loss: 2.4861993229404775

Epoch: 6| Step: 5
Training loss: 2.4284711043688865
Validation loss: 2.4843198022368567

Epoch: 6| Step: 6
Training loss: 2.298196840426466
Validation loss: 2.4829605042420666

Epoch: 6| Step: 7
Training loss: 2.232152727377731
Validation loss: 2.4804883420475847

Epoch: 6| Step: 8
Training loss: 2.624428823182145
Validation loss: 2.4772191505853196

Epoch: 6| Step: 9
Training loss: 2.2185888097533297
Validation loss: 2.4718686778505723

Epoch: 6| Step: 10
Training loss: 2.3112979548478783
Validation loss: 2.4808515358478016

Epoch: 6| Step: 11
Training loss: 2.6084042873624034
Validation loss: 2.472979403446911

Epoch: 6| Step: 12
Training loss: 3.2037355910984444
Validation loss: 2.4700711413291585

Epoch: 6| Step: 13
Training loss: 3.009226280966354
Validation loss: 2.478045976941439

Epoch: 90| Step: 0
Training loss: 2.6073158956985165
Validation loss: 2.471884142396194

Epoch: 6| Step: 1
Training loss: 2.397369246273614
Validation loss: 2.4737572973117046

Epoch: 6| Step: 2
Training loss: 2.488638141135804
Validation loss: 2.470298040300645

Epoch: 6| Step: 3
Training loss: 2.808675836416699
Validation loss: 2.4701381917023113

Epoch: 6| Step: 4
Training loss: 2.6751233259632254
Validation loss: 2.472995487704173

Epoch: 6| Step: 5
Training loss: 2.185597927939241
Validation loss: 2.4710815783158795

Epoch: 6| Step: 6
Training loss: 3.127918711442653
Validation loss: 2.4741409609127256

Epoch: 6| Step: 7
Training loss: 3.155500266287739
Validation loss: 2.4740585681620155

Epoch: 6| Step: 8
Training loss: 2.704062767756202
Validation loss: 2.475669507303385

Epoch: 6| Step: 9
Training loss: 1.842788186795108
Validation loss: 2.4730957267442037

Epoch: 6| Step: 10
Training loss: 2.262395938566682
Validation loss: 2.4736935256312367

Epoch: 6| Step: 11
Training loss: 3.245587801838576
Validation loss: 2.4720141404219693

Epoch: 6| Step: 12
Training loss: 2.022570685812406
Validation loss: 2.4709407566470913

Epoch: 6| Step: 13
Training loss: 2.1494035248279286
Validation loss: 2.471444200993432

Epoch: 91| Step: 0
Training loss: 2.61375169838805
Validation loss: 2.4755690193475983

Epoch: 6| Step: 1
Training loss: 3.052002333953885
Validation loss: 2.473768565612796

Epoch: 6| Step: 2
Training loss: 2.4238564538030256
Validation loss: 2.4870116279973415

Epoch: 6| Step: 3
Training loss: 2.5987442835625205
Validation loss: 2.4750871244823442

Epoch: 6| Step: 4
Training loss: 2.6185595204128127
Validation loss: 2.4724917806572964

Epoch: 6| Step: 5
Training loss: 1.8530601528716013
Validation loss: 2.47021148172685

Epoch: 6| Step: 6
Training loss: 2.625419401542653
Validation loss: 2.4734370328220385

Epoch: 6| Step: 7
Training loss: 2.927156784565983
Validation loss: 2.474018238078526

Epoch: 6| Step: 8
Training loss: 2.628124375859208
Validation loss: 2.468698307896506

Epoch: 6| Step: 9
Training loss: 2.6506453681924187
Validation loss: 2.4764133733381817

Epoch: 6| Step: 10
Training loss: 2.456071772430636
Validation loss: 2.47056832932174

Epoch: 6| Step: 11
Training loss: 2.466528557012587
Validation loss: 2.4800350420793675

Epoch: 6| Step: 12
Training loss: 2.458495657351625
Validation loss: 2.4770979437839666

Epoch: 6| Step: 13
Training loss: 2.5009048731671797
Validation loss: 2.4763845385802488

Epoch: 92| Step: 0
Training loss: 2.557240368444459
Validation loss: 2.4756574772135664

Epoch: 6| Step: 1
Training loss: 2.343852536819556
Validation loss: 2.4722214447572703

Epoch: 6| Step: 2
Training loss: 1.806213922371301
Validation loss: 2.4680698600683195

Epoch: 6| Step: 3
Training loss: 2.892444264922592
Validation loss: 2.474172279074875

Epoch: 6| Step: 4
Training loss: 2.8051480310991423
Validation loss: 2.4736120737930567

Epoch: 6| Step: 5
Training loss: 2.259969769672321
Validation loss: 2.4734651950772513

Epoch: 6| Step: 6
Training loss: 2.7972764734005353
Validation loss: 2.4714917759960113

Epoch: 6| Step: 7
Training loss: 2.7702539359247984
Validation loss: 2.4713506561243643

Epoch: 6| Step: 8
Training loss: 2.632902149519209
Validation loss: 2.4687236832773305

Epoch: 6| Step: 9
Training loss: 2.673869320069346
Validation loss: 2.465034304193788

Epoch: 6| Step: 10
Training loss: 2.2386798071067595
Validation loss: 2.467315828651226

Epoch: 6| Step: 11
Training loss: 3.1153373868646366
Validation loss: 2.4734803926014495

Epoch: 6| Step: 12
Training loss: 2.8148898673322837
Validation loss: 2.470208940098656

Epoch: 6| Step: 13
Training loss: 2.2337251798662927
Validation loss: 2.4731572564146957

Epoch: 93| Step: 0
Training loss: 2.5999939001451975
Validation loss: 2.474414700360419

Epoch: 6| Step: 1
Training loss: 2.731326600141965
Validation loss: 2.47743906739794

Epoch: 6| Step: 2
Training loss: 2.418987201425001
Validation loss: 2.4767620756358624

Epoch: 6| Step: 3
Training loss: 3.082675571561885
Validation loss: 2.4714125266920646

Epoch: 6| Step: 4
Training loss: 2.360884227946926
Validation loss: 2.4758448157551927

Epoch: 6| Step: 5
Training loss: 2.7315159264480244
Validation loss: 2.4740213218835114

Epoch: 6| Step: 6
Training loss: 2.239253873027226
Validation loss: 2.4738176379435033

Epoch: 6| Step: 7
Training loss: 2.634717846119791
Validation loss: 2.469914286538108

Epoch: 6| Step: 8
Training loss: 2.375414159702418
Validation loss: 2.476203475263469

Epoch: 6| Step: 9
Training loss: 2.4814614539764266
Validation loss: 2.474782165420045

Epoch: 6| Step: 10
Training loss: 2.8707080815586887
Validation loss: 2.471775985001413

Epoch: 6| Step: 11
Training loss: 2.2350595132640274
Validation loss: 2.4731713793427947

Epoch: 6| Step: 12
Training loss: 2.6553344944019095
Validation loss: 2.472085293241241

Epoch: 6| Step: 13
Training loss: 2.541206839401814
Validation loss: 2.470967274895673

Epoch: 94| Step: 0
Training loss: 2.511583197037209
Validation loss: 2.469622920425769

Epoch: 6| Step: 1
Training loss: 2.2789339824319086
Validation loss: 2.4784865902441804

Epoch: 6| Step: 2
Training loss: 2.857864230914637
Validation loss: 2.4761438186322993

Epoch: 6| Step: 3
Training loss: 2.8784868408453637
Validation loss: 2.476285130705563

Epoch: 6| Step: 4
Training loss: 2.256987212990373
Validation loss: 2.4764367682373067

Epoch: 6| Step: 5
Training loss: 2.7004080110903765
Validation loss: 2.478126922260323

Epoch: 6| Step: 6
Training loss: 2.2083638986835457
Validation loss: 2.47683914837562

Epoch: 6| Step: 7
Training loss: 2.12646680584367
Validation loss: 2.4732325295661877

Epoch: 6| Step: 8
Training loss: 2.606869619798836
Validation loss: 2.478786797364988

Epoch: 6| Step: 9
Training loss: 2.785922294228606
Validation loss: 2.476746665613244

Epoch: 6| Step: 10
Training loss: 2.5070854868242187
Validation loss: 2.4755481845000995

Epoch: 6| Step: 11
Training loss: 2.568734006066198
Validation loss: 2.4738859199500207

Epoch: 6| Step: 12
Training loss: 2.360884328933909
Validation loss: 2.4682310661528093

Epoch: 6| Step: 13
Training loss: 3.275797646333876
Validation loss: 2.4687658945207733

Epoch: 95| Step: 0
Training loss: 2.581783588668551
Validation loss: 2.469543522650717

Epoch: 6| Step: 1
Training loss: 2.801699521085624
Validation loss: 2.4710821732973405

Epoch: 6| Step: 2
Training loss: 2.6710985628292776
Validation loss: 2.4694660934836166

Epoch: 6| Step: 3
Training loss: 2.9778189823121313
Validation loss: 2.471255483690377

Epoch: 6| Step: 4
Training loss: 2.7179623043710848
Validation loss: 2.4621456026767623

Epoch: 6| Step: 5
Training loss: 2.459366748219162
Validation loss: 2.4790820793873523

Epoch: 6| Step: 6
Training loss: 2.778216218255375
Validation loss: 2.4659976505176564

Epoch: 6| Step: 7
Training loss: 2.07898527996509
Validation loss: 2.470710732268633

Epoch: 6| Step: 8
Training loss: 2.346943523356485
Validation loss: 2.466413953976727

Epoch: 6| Step: 9
Training loss: 2.941574841615988
Validation loss: 2.4707518078979684

Epoch: 6| Step: 10
Training loss: 2.437721046792996
Validation loss: 2.4703686393521176

Epoch: 6| Step: 11
Training loss: 2.68087839961649
Validation loss: 2.471597341318507

Epoch: 6| Step: 12
Training loss: 2.191401027734419
Validation loss: 2.4749203601701146

Epoch: 6| Step: 13
Training loss: 2.3473338891227056
Validation loss: 2.4728963613814687

Epoch: 96| Step: 0
Training loss: 2.7424085044377327
Validation loss: 2.4736332622918726

Epoch: 6| Step: 1
Training loss: 2.020066210728633
Validation loss: 2.47798432010578

Epoch: 6| Step: 2
Training loss: 3.0729380633800005
Validation loss: 2.479348149322111

Epoch: 6| Step: 3
Training loss: 2.4655904711878955
Validation loss: 2.478999594393573

Epoch: 6| Step: 4
Training loss: 2.7376703078712032
Validation loss: 2.473609808749466

Epoch: 6| Step: 5
Training loss: 2.3667342252446377
Validation loss: 2.4764630671776

Epoch: 6| Step: 6
Training loss: 2.56644824035759
Validation loss: 2.4739022392472214

Epoch: 6| Step: 7
Training loss: 2.1620179581721195
Validation loss: 2.4703444953147775

Epoch: 6| Step: 8
Training loss: 2.538022435719651
Validation loss: 2.4770316913976274

Epoch: 6| Step: 9
Training loss: 2.6303285376146066
Validation loss: 2.47638988194137

Epoch: 6| Step: 10
Training loss: 2.315242893337839
Validation loss: 2.4684703523656957

Epoch: 6| Step: 11
Training loss: 3.007344315987097
Validation loss: 2.47088082017559

Epoch: 6| Step: 12
Training loss: 2.726171602138993
Validation loss: 2.469549476164469

Epoch: 6| Step: 13
Training loss: 2.499312878595296
Validation loss: 2.4695579397836376

Epoch: 97| Step: 0
Training loss: 2.6685004088566058
Validation loss: 2.4687721798701205

Epoch: 6| Step: 1
Training loss: 2.7047810856204926
Validation loss: 2.469576942594766

Epoch: 6| Step: 2
Training loss: 2.950483806183616
Validation loss: 2.4768894192377537

Epoch: 6| Step: 3
Training loss: 2.171680688742012
Validation loss: 2.470350446898195

Epoch: 6| Step: 4
Training loss: 2.498865060680888
Validation loss: 2.4776611137675184

Epoch: 6| Step: 5
Training loss: 2.4944107518779806
Validation loss: 2.476344150099385

Epoch: 6| Step: 6
Training loss: 2.5012291747545956
Validation loss: 2.4733572674781694

Epoch: 6| Step: 7
Training loss: 2.5484185719211045
Validation loss: 2.477340046697307

Epoch: 6| Step: 8
Training loss: 2.6019424298520057
Validation loss: 2.461848595916554

Epoch: 6| Step: 9
Training loss: 3.1843906271179514
Validation loss: 2.46968070727127

Epoch: 6| Step: 10
Training loss: 1.7405315791867617
Validation loss: 2.4732963934639276

Epoch: 6| Step: 11
Training loss: 2.907581639541801
Validation loss: 2.4717295086623676

Epoch: 6| Step: 12
Training loss: 2.3701950456541248
Validation loss: 2.4726234020042024

Epoch: 6| Step: 13
Training loss: 2.3839451833669716
Validation loss: 2.4697735270254095

Epoch: 98| Step: 0
Training loss: 2.4935137529255886
Validation loss: 2.477722457896375

Epoch: 6| Step: 1
Training loss: 2.7387862018819833
Validation loss: 2.4734491942003345

Epoch: 6| Step: 2
Training loss: 2.7493063311923565
Validation loss: 2.4761519066632873

Epoch: 6| Step: 3
Training loss: 2.107030851698609
Validation loss: 2.468973173846287

Epoch: 6| Step: 4
Training loss: 2.5991912207422607
Validation loss: 2.4773016948613003

Epoch: 6| Step: 5
Training loss: 2.612303044684063
Validation loss: 2.467970310495271

Epoch: 6| Step: 6
Training loss: 2.3552118417499184
Validation loss: 2.4740207115474133

Epoch: 6| Step: 7
Training loss: 2.4639117962234547
Validation loss: 2.4718770370764362

Epoch: 6| Step: 8
Training loss: 2.4469856168996915
Validation loss: 2.4774149521678877

Epoch: 6| Step: 9
Training loss: 2.3146110384221776
Validation loss: 2.4736124272039266

Epoch: 6| Step: 10
Training loss: 2.6034074719365967
Validation loss: 2.4678977750816937

Epoch: 6| Step: 11
Training loss: 3.1780482760220248
Validation loss: 2.462392112705132

Epoch: 6| Step: 12
Training loss: 2.584234777703354
Validation loss: 2.4727373878219843

Epoch: 6| Step: 13
Training loss: 2.8947793336879877
Validation loss: 2.467369385663164

Epoch: 99| Step: 0
Training loss: 2.4602978071745283
Validation loss: 2.4696881246133575

Epoch: 6| Step: 1
Training loss: 2.3999785899160986
Validation loss: 2.4722894172489998

Epoch: 6| Step: 2
Training loss: 2.264063224502774
Validation loss: 2.4775950047471467

Epoch: 6| Step: 3
Training loss: 2.6917035252869126
Validation loss: 2.4715560868114483

Epoch: 6| Step: 4
Training loss: 2.490047001988725
Validation loss: 2.4754392401113283

Epoch: 6| Step: 5
Training loss: 2.5606759254879434
Validation loss: 2.4730364771815023

Epoch: 6| Step: 6
Training loss: 2.3944496028851465
Validation loss: 2.4779572034408406

Epoch: 6| Step: 7
Training loss: 2.629756342258198
Validation loss: 2.477015312507865

Epoch: 6| Step: 8
Training loss: 2.9678687142796925
Validation loss: 2.4739132418622347

Epoch: 6| Step: 9
Training loss: 2.696803468394305
Validation loss: 2.4761898430275022

Epoch: 6| Step: 10
Training loss: 2.467430244858936
Validation loss: 2.4772435965534023

Epoch: 6| Step: 11
Training loss: 2.772688643673916
Validation loss: 2.474899696475422

Epoch: 6| Step: 12
Training loss: 2.5199890194002728
Validation loss: 2.4725609349686772

Epoch: 6| Step: 13
Training loss: 2.533167171884898
Validation loss: 2.4752901902566156

Epoch: 100| Step: 0
Training loss: 2.384128494819348
Validation loss: 2.4779524487732707

Epoch: 6| Step: 1
Training loss: 2.3917584692939737
Validation loss: 2.4703196111066412

Epoch: 6| Step: 2
Training loss: 2.707956219362922
Validation loss: 2.471089682915897

Epoch: 6| Step: 3
Training loss: 2.423586431436542
Validation loss: 2.4678436901306045

Epoch: 6| Step: 4
Training loss: 2.7847290567476453
Validation loss: 2.472575205940679

Epoch: 6| Step: 5
Training loss: 2.475043182766975
Validation loss: 2.478203744106252

Epoch: 6| Step: 6
Training loss: 2.5768170362305525
Validation loss: 2.4727708610179486

Epoch: 6| Step: 7
Training loss: 2.7528574576481266
Validation loss: 2.4699995846046585

Epoch: 6| Step: 8
Training loss: 3.006634687209935
Validation loss: 2.4657854235103964

Epoch: 6| Step: 9
Training loss: 2.570533997983695
Validation loss: 2.473597856995747

Epoch: 6| Step: 10
Training loss: 3.032186619100843
Validation loss: 2.4745952361921586

Epoch: 6| Step: 11
Training loss: 2.923689824300913
Validation loss: 2.479854525714003

Epoch: 6| Step: 12
Training loss: 2.0805786103806287
Validation loss: 2.4842018770880983

Epoch: 6| Step: 13
Training loss: 1.5659276369020607
Validation loss: 2.470602764760097

Epoch: 101| Step: 0
Training loss: 1.9763708813328646
Validation loss: 2.470194542851614

Epoch: 6| Step: 1
Training loss: 2.1479367314119
Validation loss: 2.470376842802102

Epoch: 6| Step: 2
Training loss: 2.8792820430296486
Validation loss: 2.4724905753012307

Epoch: 6| Step: 3
Training loss: 2.7538108430392825
Validation loss: 2.471423813723167

Epoch: 6| Step: 4
Training loss: 2.9515888719809302
Validation loss: 2.469987776258855

Epoch: 6| Step: 5
Training loss: 2.8297308477728764
Validation loss: 2.4748282793628706

Epoch: 6| Step: 6
Training loss: 3.207490802136239
Validation loss: 2.473773657615125

Epoch: 6| Step: 7
Training loss: 2.5408165140451273
Validation loss: 2.4731407554625258

Epoch: 6| Step: 8
Training loss: 2.5040307929001178
Validation loss: 2.4712500005917253

Epoch: 6| Step: 9
Training loss: 2.877146044320928
Validation loss: 2.4708182125904736

Epoch: 6| Step: 10
Training loss: 1.9641646620371056
Validation loss: 2.467892059103878

Epoch: 6| Step: 11
Training loss: 2.528692389759254
Validation loss: 2.468880324002157

Epoch: 6| Step: 12
Training loss: 2.7432774825531157
Validation loss: 2.4655119911536096

Epoch: 6| Step: 13
Training loss: 1.9587745440028055
Validation loss: 2.470283482715127

Epoch: 102| Step: 0
Training loss: 2.816955174299018
Validation loss: 2.4707662983936114

Epoch: 6| Step: 1
Training loss: 2.260381273499432
Validation loss: 2.4616504262217584

Epoch: 6| Step: 2
Training loss: 2.4325426994567176
Validation loss: 2.4704476728298803

Epoch: 6| Step: 3
Training loss: 2.479704491329329
Validation loss: 2.462865189520038

Epoch: 6| Step: 4
Training loss: 2.7808827361315074
Validation loss: 2.46933234062477

Epoch: 6| Step: 5
Training loss: 2.861735552530951
Validation loss: 2.4694855475488438

Epoch: 6| Step: 6
Training loss: 2.7847783713785472
Validation loss: 2.4775867289668003

Epoch: 6| Step: 7
Training loss: 2.119104397371788
Validation loss: 2.471803828588083

Epoch: 6| Step: 8
Training loss: 2.0211917388955554
Validation loss: 2.4692712386465607

Epoch: 6| Step: 9
Training loss: 2.723022786538574
Validation loss: 2.4731543161362453

Epoch: 6| Step: 10
Training loss: 2.556847082417846
Validation loss: 2.473606724431454

Epoch: 6| Step: 11
Training loss: 2.9027324431639716
Validation loss: 2.4699795554289223

Epoch: 6| Step: 12
Training loss: 2.275354475135316
Validation loss: 2.4765521509574038

Epoch: 6| Step: 13
Training loss: 2.865233709313147
Validation loss: 2.4762423335442447

Epoch: 103| Step: 0
Training loss: 2.598788962390929
Validation loss: 2.471469025644352

Epoch: 6| Step: 1
Training loss: 2.6119930815904486
Validation loss: 2.468666767375153

Epoch: 6| Step: 2
Training loss: 1.9353639456552698
Validation loss: 2.4699649155270462

Epoch: 6| Step: 3
Training loss: 2.7005417668785947
Validation loss: 2.4669910071613304

Epoch: 6| Step: 4
Training loss: 3.0497848309772264
Validation loss: 2.469696008531717

Epoch: 6| Step: 5
Training loss: 2.7227707870232103
Validation loss: 2.4674884614993533

Epoch: 6| Step: 6
Training loss: 2.195251029569217
Validation loss: 2.4698042891903964

Epoch: 6| Step: 7
Training loss: 2.0423458163502164
Validation loss: 2.4693828849953023

Epoch: 6| Step: 8
Training loss: 2.8085601337739616
Validation loss: 2.462696306759542

Epoch: 6| Step: 9
Training loss: 2.3700121654729425
Validation loss: 2.4705641957501676

Epoch: 6| Step: 10
Training loss: 2.5515331895012583
Validation loss: 2.4699325385128197

Epoch: 6| Step: 11
Training loss: 2.6690617077791328
Validation loss: 2.4631077799001124

Epoch: 6| Step: 12
Training loss: 2.6523730539043027
Validation loss: 2.469546901673797

Epoch: 6| Step: 13
Training loss: 2.6561970424982895
Validation loss: 2.4687742320613846

Epoch: 104| Step: 0
Training loss: 2.926254824928109
Validation loss: 2.4683940485978817

Epoch: 6| Step: 1
Training loss: 3.049955092552898
Validation loss: 2.470757581590657

Epoch: 6| Step: 2
Training loss: 2.257302196873031
Validation loss: 2.46598000591595

Epoch: 6| Step: 3
Training loss: 2.5698729721915603
Validation loss: 2.4631524669061453

Epoch: 6| Step: 4
Training loss: 2.231838573969356
Validation loss: 2.462015809868573

Epoch: 6| Step: 5
Training loss: 2.257646811982563
Validation loss: 2.4552316873049587

Epoch: 6| Step: 6
Training loss: 3.2458349094868573
Validation loss: 2.468832248009789

Epoch: 6| Step: 7
Training loss: 2.151411072521081
Validation loss: 2.471712869566876

Epoch: 6| Step: 8
Training loss: 2.710454276603662
Validation loss: 2.463454550774788

Epoch: 6| Step: 9
Training loss: 2.6461712078316526
Validation loss: 2.4677352630597214

Epoch: 6| Step: 10
Training loss: 2.225919717730789
Validation loss: 2.4699534448619804

Epoch: 6| Step: 11
Training loss: 2.5781113132922684
Validation loss: 2.462222083944224

Epoch: 6| Step: 12
Training loss: 2.13181099765674
Validation loss: 2.4605294025465128

Epoch: 6| Step: 13
Training loss: 2.6479256734587295
Validation loss: 2.4613965126138324

Epoch: 105| Step: 0
Training loss: 2.6749046059114554
Validation loss: 2.471773509286259

Epoch: 6| Step: 1
Training loss: 2.889040951109468
Validation loss: 2.4684721874905597

Epoch: 6| Step: 2
Training loss: 2.5291402053915144
Validation loss: 2.46914474132106

Epoch: 6| Step: 3
Training loss: 2.585253299425877
Validation loss: 2.4793313930719627

Epoch: 6| Step: 4
Training loss: 2.2356002755913504
Validation loss: 2.476816126274293

Epoch: 6| Step: 5
Training loss: 2.976420401638115
Validation loss: 2.4752348701701226

Epoch: 6| Step: 6
Training loss: 2.0531406163938297
Validation loss: 2.4805689834007256

Epoch: 6| Step: 7
Training loss: 2.4112312973507395
Validation loss: 2.4750798035930592

Epoch: 6| Step: 8
Training loss: 2.5652237349525224
Validation loss: 2.472953348656346

Epoch: 6| Step: 9
Training loss: 2.293222442577819
Validation loss: 2.4748840420464

Epoch: 6| Step: 10
Training loss: 3.176541808624353
Validation loss: 2.4749906989285675

Epoch: 6| Step: 11
Training loss: 2.922812571522004
Validation loss: 2.472258187745568

Epoch: 6| Step: 12
Training loss: 2.146348416133201
Validation loss: 2.4701953954283575

Epoch: 6| Step: 13
Training loss: 2.472749584780716
Validation loss: 2.468518950519088

Epoch: 106| Step: 0
Training loss: 2.039125171151356
Validation loss: 2.4673147415550485

Epoch: 6| Step: 1
Training loss: 2.79771172139073
Validation loss: 2.463120460122116

Epoch: 6| Step: 2
Training loss: 2.581952023120264
Validation loss: 2.4648530842019425

Epoch: 6| Step: 3
Training loss: 2.577757283198708
Validation loss: 2.466803463373072

Epoch: 6| Step: 4
Training loss: 2.798616517100912
Validation loss: 2.455068154771256

Epoch: 6| Step: 5
Training loss: 2.7878427269140067
Validation loss: 2.4605635424474017

Epoch: 6| Step: 6
Training loss: 2.401037738876826
Validation loss: 2.4563927399475483

Epoch: 6| Step: 7
Training loss: 3.008513450690124
Validation loss: 2.458041922759863

Epoch: 6| Step: 8
Training loss: 2.594967659691772
Validation loss: 2.464172658834524

Epoch: 6| Step: 9
Training loss: 2.0911704727437788
Validation loss: 2.4571125480006564

Epoch: 6| Step: 10
Training loss: 2.428613848676373
Validation loss: 2.455988110144887

Epoch: 6| Step: 11
Training loss: 2.7123121134997716
Validation loss: 2.45502103841848

Epoch: 6| Step: 12
Training loss: 2.5354995847719364
Validation loss: 2.4494059205721554

Epoch: 6| Step: 13
Training loss: 2.3548938562077755
Validation loss: 2.4505351137453912

Epoch: 107| Step: 0
Training loss: 2.405022840807781
Validation loss: 2.45847230184408

Epoch: 6| Step: 1
Training loss: 2.666235253087148
Validation loss: 2.4743905877732604

Epoch: 6| Step: 2
Training loss: 2.415331658124365
Validation loss: 2.4750542365017343

Epoch: 6| Step: 3
Training loss: 2.487848121398588
Validation loss: 2.4757233972150905

Epoch: 6| Step: 4
Training loss: 3.2554482496223525
Validation loss: 2.4761846436617003

Epoch: 6| Step: 5
Training loss: 3.0036012968480432
Validation loss: 2.4752423912701706

Epoch: 6| Step: 6
Training loss: 2.8522202660676457
Validation loss: 2.4750994061887597

Epoch: 6| Step: 7
Training loss: 2.1646354274949084
Validation loss: 2.4730308694883414

Epoch: 6| Step: 8
Training loss: 2.7242209974651073
Validation loss: 2.4703647788956404

Epoch: 6| Step: 9
Training loss: 2.038595679621318
Validation loss: 2.474979444219339

Epoch: 6| Step: 10
Training loss: 2.8104128404670914
Validation loss: 2.481240460355855

Epoch: 6| Step: 11
Training loss: 1.7794169313228745
Validation loss: 2.472245104346898

Epoch: 6| Step: 12
Training loss: 2.4898332818837345
Validation loss: 2.473785576393478

Epoch: 6| Step: 13
Training loss: 2.5327375773903804
Validation loss: 2.46787663394838

Epoch: 108| Step: 0
Training loss: 2.638992617615221
Validation loss: 2.47232585384984

Epoch: 6| Step: 1
Training loss: 3.5486064163512054
Validation loss: 2.472345285412561

Epoch: 6| Step: 2
Training loss: 2.216366844647174
Validation loss: 2.4763018834986568

Epoch: 6| Step: 3
Training loss: 2.4541248786447096
Validation loss: 2.471160854313297

Epoch: 6| Step: 4
Training loss: 2.3696183920312017
Validation loss: 2.4664185858889156

Epoch: 6| Step: 5
Training loss: 2.361981297624854
Validation loss: 2.4589569841697005

Epoch: 6| Step: 6
Training loss: 2.238866173708115
Validation loss: 2.4564687047500073

Epoch: 6| Step: 7
Training loss: 2.2210475147975295
Validation loss: 2.4628646812917863

Epoch: 6| Step: 8
Training loss: 2.262226581386088
Validation loss: 2.461385429832019

Epoch: 6| Step: 9
Training loss: 2.1802311701215715
Validation loss: 2.4588556516050706

Epoch: 6| Step: 10
Training loss: 2.3289814852105133
Validation loss: 2.4559507354316263

Epoch: 6| Step: 11
Training loss: 2.875941412667205
Validation loss: 2.4641972987318264

Epoch: 6| Step: 12
Training loss: 2.9352655231522498
Validation loss: 2.4593710621822877

Epoch: 6| Step: 13
Training loss: 2.8926470308512884
Validation loss: 2.457640377807262

Epoch: 109| Step: 0
Training loss: 2.863963962988855
Validation loss: 2.4665431367597495

Epoch: 6| Step: 1
Training loss: 2.3918128958026443
Validation loss: 2.465034320313819

Epoch: 6| Step: 2
Training loss: 2.9260439582907174
Validation loss: 2.4661576949857804

Epoch: 6| Step: 3
Training loss: 2.296033847958091
Validation loss: 2.4678787915437796

Epoch: 6| Step: 4
Training loss: 2.5091866504486173
Validation loss: 2.470603778032891

Epoch: 6| Step: 5
Training loss: 2.699173701968971
Validation loss: 2.4669648810707545

Epoch: 6| Step: 6
Training loss: 2.2541061656492727
Validation loss: 2.4670982629777076

Epoch: 6| Step: 7
Training loss: 2.363479378962354
Validation loss: 2.4646777274939753

Epoch: 6| Step: 8
Training loss: 3.1981075532253556
Validation loss: 2.466179487300209

Epoch: 6| Step: 9
Training loss: 2.5008758917897653
Validation loss: 2.4676856148735316

Epoch: 6| Step: 10
Training loss: 2.5632530594530953
Validation loss: 2.466721824295642

Epoch: 6| Step: 11
Training loss: 2.7841959577852378
Validation loss: 2.467748012092219

Epoch: 6| Step: 12
Training loss: 1.8434841158259907
Validation loss: 2.467778823659474

Epoch: 6| Step: 13
Training loss: 2.1511712454744174
Validation loss: 2.471355367217394

Epoch: 110| Step: 0
Training loss: 2.8576124418468387
Validation loss: 2.4736130537048897

Epoch: 6| Step: 1
Training loss: 2.388178751863958
Validation loss: 2.474647784618711

Epoch: 6| Step: 2
Training loss: 2.279501362366132
Validation loss: 2.4734817500924615

Epoch: 6| Step: 3
Training loss: 2.768906107133438
Validation loss: 2.476108425162128

Epoch: 6| Step: 4
Training loss: 2.441491404764927
Validation loss: 2.478679325919257

Epoch: 6| Step: 5
Training loss: 2.505302908547032
Validation loss: 2.4738871728132668

Epoch: 6| Step: 6
Training loss: 2.637326409290637
Validation loss: 2.469320368167684

Epoch: 6| Step: 7
Training loss: 2.8970275904559974
Validation loss: 2.469209813300936

Epoch: 6| Step: 8
Training loss: 2.5864105252965914
Validation loss: 2.4659566890659788

Epoch: 6| Step: 9
Training loss: 2.291697785137325
Validation loss: 2.452705865840005

Epoch: 6| Step: 10
Training loss: 2.570778013239041
Validation loss: 2.4555522306150284

Epoch: 6| Step: 11
Training loss: 2.5215513189457694
Validation loss: 2.4594939180906294

Epoch: 6| Step: 12
Training loss: 3.1837146197818047
Validation loss: 2.4626707401885644

Epoch: 6| Step: 13
Training loss: 1.7926934057837394
Validation loss: 2.4661019202850727

Epoch: 111| Step: 0
Training loss: 2.3560438070968615
Validation loss: 2.466317036267557

Epoch: 6| Step: 1
Training loss: 2.753387012635768
Validation loss: 2.4649638667111717

Epoch: 6| Step: 2
Training loss: 2.72075605375203
Validation loss: 2.4621014138517237

Epoch: 6| Step: 3
Training loss: 2.8387922730483544
Validation loss: 2.4670686187030437

Epoch: 6| Step: 4
Training loss: 2.707116239900723
Validation loss: 2.4671883452407775

Epoch: 6| Step: 5
Training loss: 2.216441498221657
Validation loss: 2.470971230898659

Epoch: 6| Step: 6
Training loss: 2.3000851988561357
Validation loss: 2.4678080004984007

Epoch: 6| Step: 7
Training loss: 2.344277894651373
Validation loss: 2.4700455304375444

Epoch: 6| Step: 8
Training loss: 2.5541524997668854
Validation loss: 2.4665950431821146

Epoch: 6| Step: 9
Training loss: 2.823537995112687
Validation loss: 2.4689358347551487

Epoch: 6| Step: 10
Training loss: 2.6367509401970226
Validation loss: 2.4671249191661184

Epoch: 6| Step: 11
Training loss: 2.5164787786548906
Validation loss: 2.464202015437998

Epoch: 6| Step: 12
Training loss: 2.2241420506181986
Validation loss: 2.4680266386904712

Epoch: 6| Step: 13
Training loss: 2.732957308963755
Validation loss: 2.465581317058563

Epoch: 112| Step: 0
Training loss: 2.1161940379891675
Validation loss: 2.468114046684452

Epoch: 6| Step: 1
Training loss: 1.9900969063823506
Validation loss: 2.4669053956659077

Epoch: 6| Step: 2
Training loss: 2.7033195646086674
Validation loss: 2.462525032290595

Epoch: 6| Step: 3
Training loss: 2.739203413424861
Validation loss: 2.4645326058290853

Epoch: 6| Step: 4
Training loss: 2.681614930543099
Validation loss: 2.4603114628621046

Epoch: 6| Step: 5
Training loss: 2.3201873437284237
Validation loss: 2.456716027024898

Epoch: 6| Step: 6
Training loss: 2.408339150849634
Validation loss: 2.455775552084345

Epoch: 6| Step: 7
Training loss: 2.7486527350522394
Validation loss: 2.4589283971901534

Epoch: 6| Step: 8
Training loss: 3.069935083852325
Validation loss: 2.463704712024259

Epoch: 6| Step: 9
Training loss: 2.1939338889580635
Validation loss: 2.4560321824525997

Epoch: 6| Step: 10
Training loss: 2.1209327819331087
Validation loss: 2.4597548453619966

Epoch: 6| Step: 11
Training loss: 2.411414413107113
Validation loss: 2.462045506911815

Epoch: 6| Step: 12
Training loss: 2.7295292232505246
Validation loss: 2.456629750126656

Epoch: 6| Step: 13
Training loss: 3.0115103841264057
Validation loss: 2.4564502152369263

Epoch: 113| Step: 0
Training loss: 2.157332217874384
Validation loss: 2.4655259080643606

Epoch: 6| Step: 1
Training loss: 2.0379476605433853
Validation loss: 2.4571159602907784

Epoch: 6| Step: 2
Training loss: 2.6919163634357504
Validation loss: 2.458497079686785

Epoch: 6| Step: 3
Training loss: 2.7003941495506254
Validation loss: 2.459216982762914

Epoch: 6| Step: 4
Training loss: 3.139383706519911
Validation loss: 2.4635552102832956

Epoch: 6| Step: 5
Training loss: 2.92690753521982
Validation loss: 2.4589757295496435

Epoch: 6| Step: 6
Training loss: 1.7978076670445942
Validation loss: 2.455442293845666

Epoch: 6| Step: 7
Training loss: 2.93041804042744
Validation loss: 2.4606371338015203

Epoch: 6| Step: 8
Training loss: 1.7560230556486451
Validation loss: 2.460652523595775

Epoch: 6| Step: 9
Training loss: 2.4079837250523908
Validation loss: 2.458594774363785

Epoch: 6| Step: 10
Training loss: 2.8819535792766455
Validation loss: 2.460880743108939

Epoch: 6| Step: 11
Training loss: 2.5730605812338947
Validation loss: 2.4602355278795542

Epoch: 6| Step: 12
Training loss: 2.3472331295725586
Validation loss: 2.458396415116617

Epoch: 6| Step: 13
Training loss: 2.7144971008734995
Validation loss: 2.467843738435645

Epoch: 114| Step: 0
Training loss: 3.104031826564004
Validation loss: 2.460422102631182

Epoch: 6| Step: 1
Training loss: 2.5386502444767256
Validation loss: 2.4689951424873438

Epoch: 6| Step: 2
Training loss: 2.6503875664937215
Validation loss: 2.472656716039854

Epoch: 6| Step: 3
Training loss: 2.2605063659402824
Validation loss: 2.4749379249777324

Epoch: 6| Step: 4
Training loss: 2.4479059963129033
Validation loss: 2.47096869005194

Epoch: 6| Step: 5
Training loss: 2.387656369506042
Validation loss: 2.4771926350991698

Epoch: 6| Step: 6
Training loss: 2.7931372531665635
Validation loss: 2.467725919631593

Epoch: 6| Step: 7
Training loss: 2.7998831997079514
Validation loss: 2.465685225711335

Epoch: 6| Step: 8
Training loss: 2.3006597982322696
Validation loss: 2.4599204893624487

Epoch: 6| Step: 9
Training loss: 2.1178905262954624
Validation loss: 2.459209695420422

Epoch: 6| Step: 10
Training loss: 2.2330175157501504
Validation loss: 2.4555229728813583

Epoch: 6| Step: 11
Training loss: 1.9404959741721182
Validation loss: 2.4509474369152273

Epoch: 6| Step: 12
Training loss: 3.5572304820453478
Validation loss: 2.457696643644284

Epoch: 6| Step: 13
Training loss: 2.106697361014282
Validation loss: 2.452288345240899

Epoch: 115| Step: 0
Training loss: 2.518287339799304
Validation loss: 2.456745885178198

Epoch: 6| Step: 1
Training loss: 2.0250871333461293
Validation loss: 2.4600116421131992

Epoch: 6| Step: 2
Training loss: 2.4857052294718067
Validation loss: 2.4563450908939437

Epoch: 6| Step: 3
Training loss: 2.3747421927817207
Validation loss: 2.4517069968306386

Epoch: 6| Step: 4
Training loss: 2.642569824566602
Validation loss: 2.453040597358619

Epoch: 6| Step: 5
Training loss: 2.6722461821851526
Validation loss: 2.459469748081217

Epoch: 6| Step: 6
Training loss: 2.5765772942279654
Validation loss: 2.45983648950794

Epoch: 6| Step: 7
Training loss: 2.8547150208097167
Validation loss: 2.448227157921973

Epoch: 6| Step: 8
Training loss: 2.3772949374199284
Validation loss: 2.4562913905725847

Epoch: 6| Step: 9
Training loss: 2.4565691248017427
Validation loss: 2.4542042490267924

Epoch: 6| Step: 10
Training loss: 2.8411853669816725
Validation loss: 2.4542519315492

Epoch: 6| Step: 11
Training loss: 2.469090884853868
Validation loss: 2.457745648845326

Epoch: 6| Step: 12
Training loss: 2.111689223500454
Validation loss: 2.4600188140098007

Epoch: 6| Step: 13
Training loss: 3.141087284511282
Validation loss: 2.457474935849431

Epoch: 116| Step: 0
Training loss: 2.3891868504058746
Validation loss: 2.457509053505629

Epoch: 6| Step: 1
Training loss: 2.823655954849135
Validation loss: 2.4589277023074856

Epoch: 6| Step: 2
Training loss: 2.6071072163543905
Validation loss: 2.4567208308801245

Epoch: 6| Step: 3
Training loss: 3.239730161097315
Validation loss: 2.461532795269876

Epoch: 6| Step: 4
Training loss: 2.25160139104543
Validation loss: 2.4558358733914067

Epoch: 6| Step: 5
Training loss: 2.684110411281766
Validation loss: 2.4557412970773576

Epoch: 6| Step: 6
Training loss: 2.745613848452778
Validation loss: 2.4608759635215125

Epoch: 6| Step: 7
Training loss: 2.308404027175365
Validation loss: 2.461494826730926

Epoch: 6| Step: 8
Training loss: 2.5830118738153325
Validation loss: 2.4597467034149125

Epoch: 6| Step: 9
Training loss: 2.787703666602332
Validation loss: 2.4590735102449153

Epoch: 6| Step: 10
Training loss: 2.043328627331507
Validation loss: 2.4568300715708338

Epoch: 6| Step: 11
Training loss: 1.6799713604620337
Validation loss: 2.460636915792201

Epoch: 6| Step: 12
Training loss: 2.106433541199145
Validation loss: 2.4616487877888886

Epoch: 6| Step: 13
Training loss: 3.04085803496417
Validation loss: 2.454683574088179

Epoch: 117| Step: 0
Training loss: 2.489517934623883
Validation loss: 2.451653737886191

Epoch: 6| Step: 1
Training loss: 2.8696006162065832
Validation loss: 2.457387505172061

Epoch: 6| Step: 2
Training loss: 2.8131063019716525
Validation loss: 2.4540779222937847

Epoch: 6| Step: 3
Training loss: 2.747102251080776
Validation loss: 2.4473908177468022

Epoch: 6| Step: 4
Training loss: 2.511668440580427
Validation loss: 2.4525313418346513

Epoch: 6| Step: 5
Training loss: 2.531191978260548
Validation loss: 2.446425829406199

Epoch: 6| Step: 6
Training loss: 2.670660981217781
Validation loss: 2.4489936299270396

Epoch: 6| Step: 7
Training loss: 2.8704137295236194
Validation loss: 2.4494074617457176

Epoch: 6| Step: 8
Training loss: 2.2125695341573888
Validation loss: 2.4562556220613936

Epoch: 6| Step: 9
Training loss: 2.6611492975406827
Validation loss: 2.4548488804286053

Epoch: 6| Step: 10
Training loss: 2.110898972053102
Validation loss: 2.4551112239268518

Epoch: 6| Step: 11
Training loss: 2.3297683752952
Validation loss: 2.451713406951212

Epoch: 6| Step: 12
Training loss: 2.4741152636630788
Validation loss: 2.4544763588786402

Epoch: 6| Step: 13
Training loss: 2.2004600260682907
Validation loss: 2.463888467864878

Epoch: 118| Step: 0
Training loss: 2.7231550813265017
Validation loss: 2.4602155968800616

Epoch: 6| Step: 1
Training loss: 2.996402809088178
Validation loss: 2.4598066850239673

Epoch: 6| Step: 2
Training loss: 2.129184305773664
Validation loss: 2.454641776303534

Epoch: 6| Step: 3
Training loss: 2.3208874333740432
Validation loss: 2.452446635213651

Epoch: 6| Step: 4
Training loss: 2.7998490531288285
Validation loss: 2.451271085517141

Epoch: 6| Step: 5
Training loss: 2.3532053129515846
Validation loss: 2.451436443666078

Epoch: 6| Step: 6
Training loss: 3.003396654495273
Validation loss: 2.4545651478812243

Epoch: 6| Step: 7
Training loss: 2.679990879157189
Validation loss: 2.4626748708737365

Epoch: 6| Step: 8
Training loss: 2.37254507779709
Validation loss: 2.4617028232141167

Epoch: 6| Step: 9
Training loss: 2.6673688758720084
Validation loss: 2.461237409774845

Epoch: 6| Step: 10
Training loss: 2.5443460277012635
Validation loss: 2.468229214749507

Epoch: 6| Step: 11
Training loss: 2.2590151425040292
Validation loss: 2.467479620388479

Epoch: 6| Step: 12
Training loss: 2.229165359449152
Validation loss: 2.4629652277724396

Epoch: 6| Step: 13
Training loss: 2.2529870444719813
Validation loss: 2.464203974682523

Epoch: 119| Step: 0
Training loss: 2.3910201001760316
Validation loss: 2.4539578880618182

Epoch: 6| Step: 1
Training loss: 2.6470388134343406
Validation loss: 2.466278029661346

Epoch: 6| Step: 2
Training loss: 2.3484292685643022
Validation loss: 2.462937477870167

Epoch: 6| Step: 3
Training loss: 1.7972984810080892
Validation loss: 2.4576569181568235

Epoch: 6| Step: 4
Training loss: 2.6470388134343406
Validation loss: 2.4568582462763997

Epoch: 6| Step: 5
Training loss: 2.6033142424049105
Validation loss: 2.448562128448958

Epoch: 6| Step: 6
Training loss: 2.8266494864115645
Validation loss: 2.4565646279864493

Epoch: 6| Step: 7
Training loss: 2.6394031112875713
Validation loss: 2.447530486034226

Epoch: 6| Step: 8
Training loss: 2.4589063224494314
Validation loss: 2.4551186852858917

Epoch: 6| Step: 9
Training loss: 2.889191803171381
Validation loss: 2.4511025873652565

Epoch: 6| Step: 10
Training loss: 2.394176562676413
Validation loss: 2.446908870509693

Epoch: 6| Step: 11
Training loss: 2.5504919475165813
Validation loss: 2.456511781702168

Epoch: 6| Step: 12
Training loss: 2.9625590773216817
Validation loss: 2.4616672866818785

Epoch: 6| Step: 13
Training loss: 2.1201441766188633
Validation loss: 2.4593825498898867

Epoch: 120| Step: 0
Training loss: 2.560561307376136
Validation loss: 2.460820933008347

Epoch: 6| Step: 1
Training loss: 2.4586273491681565
Validation loss: 2.4643410695547643

Epoch: 6| Step: 2
Training loss: 2.518380592840226
Validation loss: 2.46571735230074

Epoch: 6| Step: 3
Training loss: 2.8633705114838275
Validation loss: 2.4658143660974092

Epoch: 6| Step: 4
Training loss: 2.4564220843809643
Validation loss: 2.4643610316864346

Epoch: 6| Step: 5
Training loss: 2.6492519870429145
Validation loss: 2.4579745989982853

Epoch: 6| Step: 6
Training loss: 2.361851888898365
Validation loss: 2.4597089657844218

Epoch: 6| Step: 7
Training loss: 2.055922452584844
Validation loss: 2.4587136367774876

Epoch: 6| Step: 8
Training loss: 2.828022907585159
Validation loss: 2.4556434403333363

Epoch: 6| Step: 9
Training loss: 2.7023058405321048
Validation loss: 2.454896598989859

Epoch: 6| Step: 10
Training loss: 2.2460741125160575
Validation loss: 2.4546298940768136

Epoch: 6| Step: 11
Training loss: 2.7797155540727463
Validation loss: 2.4578654172600243

Epoch: 6| Step: 12
Training loss: 2.1793123968446495
Validation loss: 2.4504353545168787

Epoch: 6| Step: 13
Training loss: 2.571508622058801
Validation loss: 2.4578082984133385

Epoch: 121| Step: 0
Training loss: 1.7880949377461568
Validation loss: 2.4581254504827608

Epoch: 6| Step: 1
Training loss: 2.7493469156373145
Validation loss: 2.4612602062669207

Epoch: 6| Step: 2
Training loss: 2.3731075577374954
Validation loss: 2.458983300380255

Epoch: 6| Step: 3
Training loss: 2.6587095654080266
Validation loss: 2.457820731131256

Epoch: 6| Step: 4
Training loss: 2.5143122120488943
Validation loss: 2.462723438175849

Epoch: 6| Step: 5
Training loss: 2.478086273544987
Validation loss: 2.4602212257078677

Epoch: 6| Step: 6
Training loss: 2.9399321207008526
Validation loss: 2.456306524491854

Epoch: 6| Step: 7
Training loss: 2.495986577988263
Validation loss: 2.461903022399383

Epoch: 6| Step: 8
Training loss: 2.9230321540956323
Validation loss: 2.4579140634280994

Epoch: 6| Step: 9
Training loss: 2.6570846143981357
Validation loss: 2.4597356858959896

Epoch: 6| Step: 10
Training loss: 2.6412305871093924
Validation loss: 2.458957485125678

Epoch: 6| Step: 11
Training loss: 2.1400049801349623
Validation loss: 2.458616771091209

Epoch: 6| Step: 12
Training loss: 2.2950853914485463
Validation loss: 2.4564394579321185

Epoch: 6| Step: 13
Training loss: 2.6360025068858706
Validation loss: 2.4572253532134813

Epoch: 122| Step: 0
Training loss: 3.0783211117928504
Validation loss: 2.4559790496605673

Epoch: 6| Step: 1
Training loss: 2.6532689139758534
Validation loss: 2.4531116687712093

Epoch: 6| Step: 2
Training loss: 2.7138858120537606
Validation loss: 2.4523466134720495

Epoch: 6| Step: 3
Training loss: 2.014799081899628
Validation loss: 2.455456219280205

Epoch: 6| Step: 4
Training loss: 2.6015824783978574
Validation loss: 2.452568258294171

Epoch: 6| Step: 5
Training loss: 2.4390197320056255
Validation loss: 2.4536247635251645

Epoch: 6| Step: 6
Training loss: 2.5150278460741387
Validation loss: 2.458895066841

Epoch: 6| Step: 7
Training loss: 2.0768942953218468
Validation loss: 2.454112394819781

Epoch: 6| Step: 8
Training loss: 2.42223882095918
Validation loss: 2.458878954990238

Epoch: 6| Step: 9
Training loss: 2.7892806451366603
Validation loss: 2.463097293662119

Epoch: 6| Step: 10
Training loss: 2.848145493916454
Validation loss: 2.4520960308082187

Epoch: 6| Step: 11
Training loss: 2.1602357156495606
Validation loss: 2.4550738763279294

Epoch: 6| Step: 12
Training loss: 2.352329268548708
Validation loss: 2.446230341345498

Epoch: 6| Step: 13
Training loss: 2.5580462386819005
Validation loss: 2.453721057049415

Epoch: 123| Step: 0
Training loss: 2.6893307859566002
Validation loss: 2.459949048659573

Epoch: 6| Step: 1
Training loss: 2.751064094664845
Validation loss: 2.46249476011077

Epoch: 6| Step: 2
Training loss: 2.6169244890291576
Validation loss: 2.467926097153921

Epoch: 6| Step: 3
Training loss: 2.648184615133329
Validation loss: 2.4648821828257

Epoch: 6| Step: 4
Training loss: 2.441440234138472
Validation loss: 2.468289972251129

Epoch: 6| Step: 5
Training loss: 2.3536169259837814
Validation loss: 2.4657258371314525

Epoch: 6| Step: 6
Training loss: 2.999448089694742
Validation loss: 2.4695721476570633

Epoch: 6| Step: 7
Training loss: 2.4749609163839685
Validation loss: 2.4703936276192864

Epoch: 6| Step: 8
Training loss: 2.3384678637123666
Validation loss: 2.4705586950333025

Epoch: 6| Step: 9
Training loss: 2.213793631780096
Validation loss: 2.4712901587817857

Epoch: 6| Step: 10
Training loss: 1.9272882782151666
Validation loss: 2.472269020875851

Epoch: 6| Step: 11
Training loss: 2.697666897988868
Validation loss: 2.4694044155208767

Epoch: 6| Step: 12
Training loss: 2.5985058305831656
Validation loss: 2.462133530717102

Epoch: 6| Step: 13
Training loss: 2.632923882276288
Validation loss: 2.4622592827734953

Epoch: 124| Step: 0
Training loss: 2.270627202002216
Validation loss: 2.460684384847119

Epoch: 6| Step: 1
Training loss: 2.3042368302748564
Validation loss: 2.4583136886415224

Epoch: 6| Step: 2
Training loss: 2.283289167807732
Validation loss: 2.4488367069686485

Epoch: 6| Step: 3
Training loss: 2.462819182337619
Validation loss: 2.4504404139253184

Epoch: 6| Step: 4
Training loss: 2.667009709069137
Validation loss: 2.458081900675208

Epoch: 6| Step: 5
Training loss: 2.8103410699818028
Validation loss: 2.454134431723359

Epoch: 6| Step: 6
Training loss: 2.545490941957877
Validation loss: 2.4499146498520834

Epoch: 6| Step: 7
Training loss: 2.052547370094521
Validation loss: 2.4589914206155683

Epoch: 6| Step: 8
Training loss: 2.7197277897775507
Validation loss: 2.449169217247125

Epoch: 6| Step: 9
Training loss: 2.215856680439209
Validation loss: 2.4514517777552682

Epoch: 6| Step: 10
Training loss: 2.6751587080889903
Validation loss: 2.4511251376738192

Epoch: 6| Step: 11
Training loss: 2.539890940570133
Validation loss: 2.4611844055000818

Epoch: 6| Step: 12
Training loss: 2.8915997485535514
Validation loss: 2.4518311929723455

Epoch: 6| Step: 13
Training loss: 2.7149027646463466
Validation loss: 2.453607353848562

Epoch: 125| Step: 0
Training loss: 2.745180762337467
Validation loss: 2.4522902896980834

Epoch: 6| Step: 1
Training loss: 2.335658538389737
Validation loss: 2.4556912971777844

Epoch: 6| Step: 2
Training loss: 2.399188591964123
Validation loss: 2.454554892263882

Epoch: 6| Step: 3
Training loss: 2.3563110465077157
Validation loss: 2.450618305490643

Epoch: 6| Step: 4
Training loss: 2.3997617166167364
Validation loss: 2.4467324713537755

Epoch: 6| Step: 5
Training loss: 2.2496274003825074
Validation loss: 2.448767677802835

Epoch: 6| Step: 6
Training loss: 2.799034009922422
Validation loss: 2.454308485543721

Epoch: 6| Step: 7
Training loss: 2.626547493514491
Validation loss: 2.4540051057892494

Epoch: 6| Step: 8
Training loss: 3.180923559081655
Validation loss: 2.451887154543769

Epoch: 6| Step: 9
Training loss: 2.9183453951298977
Validation loss: 2.450118812612516

Epoch: 6| Step: 10
Training loss: 2.396966837450286
Validation loss: 2.4543079350673755

Epoch: 6| Step: 11
Training loss: 2.7138008584519824
Validation loss: 2.460609656400661

Epoch: 6| Step: 12
Training loss: 1.7721961293879005
Validation loss: 2.453048469985733

Epoch: 6| Step: 13
Training loss: 2.1298514691354917
Validation loss: 2.4581423270381646

Epoch: 126| Step: 0
Training loss: 2.434182574951399
Validation loss: 2.459216586887879

Epoch: 6| Step: 1
Training loss: 2.3964447789235988
Validation loss: 2.4513890954132522

Epoch: 6| Step: 2
Training loss: 2.253215399633598
Validation loss: 2.4557145173224795

Epoch: 6| Step: 3
Training loss: 2.3950384950032864
Validation loss: 2.457746247055204

Epoch: 6| Step: 4
Training loss: 2.0187921997950578
Validation loss: 2.4646789044262185

Epoch: 6| Step: 5
Training loss: 3.1020736465319936
Validation loss: 2.4572437964661966

Epoch: 6| Step: 6
Training loss: 2.390751255665734
Validation loss: 2.4580169625024206

Epoch: 6| Step: 7
Training loss: 2.6767134553169862
Validation loss: 2.457785599244664

Epoch: 6| Step: 8
Training loss: 1.8092237656396384
Validation loss: 2.463888306589594

Epoch: 6| Step: 9
Training loss: 2.4822745410767584
Validation loss: 2.4598877620679027

Epoch: 6| Step: 10
Training loss: 2.352119455885142
Validation loss: 2.462101252459383

Epoch: 6| Step: 11
Training loss: 2.975738972157995
Validation loss: 2.4554059060939455

Epoch: 6| Step: 12
Training loss: 2.846400440580989
Validation loss: 2.4564084798663908

Epoch: 6| Step: 13
Training loss: 2.755997619838268
Validation loss: 2.46427055581948

Epoch: 127| Step: 0
Training loss: 1.746916506688729
Validation loss: 2.4562147569918635

Epoch: 6| Step: 1
Training loss: 2.7287871021608994
Validation loss: 2.456522409283169

Epoch: 6| Step: 2
Training loss: 2.287813125608182
Validation loss: 2.451729022939656

Epoch: 6| Step: 3
Training loss: 3.3053384775498977
Validation loss: 2.4475318254473954

Epoch: 6| Step: 4
Training loss: 2.9922037547494194
Validation loss: 2.4531962552422955

Epoch: 6| Step: 5
Training loss: 2.4120530341977795
Validation loss: 2.4476189750222015

Epoch: 6| Step: 6
Training loss: 2.265109088956973
Validation loss: 2.451761616031522

Epoch: 6| Step: 7
Training loss: 2.179299925114654
Validation loss: 2.4512022140672003

Epoch: 6| Step: 8
Training loss: 3.0456249313620645
Validation loss: 2.4636919541652347

Epoch: 6| Step: 9
Training loss: 2.5826575923373607
Validation loss: 2.48011409597046

Epoch: 6| Step: 10
Training loss: 2.317681873184779
Validation loss: 2.458012048017722

Epoch: 6| Step: 11
Training loss: 2.5011960029778755
Validation loss: 2.4554518175244247

Epoch: 6| Step: 12
Training loss: 2.079263017010761
Validation loss: 2.4521404971641485

Epoch: 6| Step: 13
Training loss: 2.5824200543772418
Validation loss: 2.462445994654702

Epoch: 128| Step: 0
Training loss: 2.6025288937711757
Validation loss: 2.4595688177610255

Epoch: 6| Step: 1
Training loss: 2.4793473720141117
Validation loss: 2.455393987106725

Epoch: 6| Step: 2
Training loss: 2.22406572593336
Validation loss: 2.4597269946219025

Epoch: 6| Step: 3
Training loss: 2.385318026661827
Validation loss: 2.4547065447597256

Epoch: 6| Step: 4
Training loss: 2.6379827337849577
Validation loss: 2.4578969428027713

Epoch: 6| Step: 5
Training loss: 2.715161992363511
Validation loss: 2.4552410418651895

Epoch: 6| Step: 6
Training loss: 3.1492134444339457
Validation loss: 2.4587394707536236

Epoch: 6| Step: 7
Training loss: 2.3818623633989158
Validation loss: 2.452926951713463

Epoch: 6| Step: 8
Training loss: 2.1742951884456265
Validation loss: 2.4582538160836767

Epoch: 6| Step: 9
Training loss: 2.674354785576237
Validation loss: 2.463324326867384

Epoch: 6| Step: 10
Training loss: 2.3146082572613706
Validation loss: 2.4622569508026615

Epoch: 6| Step: 11
Training loss: 2.5839746868617985
Validation loss: 2.45277502718126

Epoch: 6| Step: 12
Training loss: 2.635636261447135
Validation loss: 2.454207406302247

Epoch: 6| Step: 13
Training loss: 2.05793291230582
Validation loss: 2.4516399286222414

Epoch: 129| Step: 0
Training loss: 2.5242098639003294
Validation loss: 2.4513577050112847

Epoch: 6| Step: 1
Training loss: 2.4051388243264102
Validation loss: 2.4518187136756717

Epoch: 6| Step: 2
Training loss: 2.9520550762610798
Validation loss: 2.4471216466411465

Epoch: 6| Step: 3
Training loss: 2.3877658076888872
Validation loss: 2.450054750025384

Epoch: 6| Step: 4
Training loss: 2.3896029413067876
Validation loss: 2.4520772490402964

Epoch: 6| Step: 5
Training loss: 2.5886374954098152
Validation loss: 2.4482266547703566

Epoch: 6| Step: 6
Training loss: 3.085915403649135
Validation loss: 2.4558838316565965

Epoch: 6| Step: 7
Training loss: 2.4043611321124954
Validation loss: 2.449613434922991

Epoch: 6| Step: 8
Training loss: 2.418305553834954
Validation loss: 2.452466580742957

Epoch: 6| Step: 9
Training loss: 2.3740444268727114
Validation loss: 2.45302324023242

Epoch: 6| Step: 10
Training loss: 2.7547555898818596
Validation loss: 2.4563940502632433

Epoch: 6| Step: 11
Training loss: 2.518238771111925
Validation loss: 2.4536709997875357

Epoch: 6| Step: 12
Training loss: 2.227876616727316
Validation loss: 2.4479318712993283

Epoch: 6| Step: 13
Training loss: 1.7843638190939655
Validation loss: 2.449726415137623

Epoch: 130| Step: 0
Training loss: 2.891225061766739
Validation loss: 2.4514614141924724

Epoch: 6| Step: 1
Training loss: 3.029410679227154
Validation loss: 2.4516775149224896

Epoch: 6| Step: 2
Training loss: 2.320472377830517
Validation loss: 2.471451500495684

Epoch: 6| Step: 3
Training loss: 2.5629145589303115
Validation loss: 2.464052664818383

Epoch: 6| Step: 4
Training loss: 2.1507642319210714
Validation loss: 2.4507270966711063

Epoch: 6| Step: 5
Training loss: 2.1264360848550403
Validation loss: 2.4626963390302117

Epoch: 6| Step: 6
Training loss: 2.3976325756406958
Validation loss: 2.46033576995997

Epoch: 6| Step: 7
Training loss: 2.6143691645189384
Validation loss: 2.4560264712179722

Epoch: 6| Step: 8
Training loss: 2.8049962290910204
Validation loss: 2.4534189732477683

Epoch: 6| Step: 9
Training loss: 2.3122371833537625
Validation loss: 2.4569578741020575

Epoch: 6| Step: 10
Training loss: 2.6150359918221757
Validation loss: 2.4504595163412133

Epoch: 6| Step: 11
Training loss: 1.831510750495587
Validation loss: 2.4567974810233038

Epoch: 6| Step: 12
Training loss: 2.32392350573556
Validation loss: 2.4536656393408216

Epoch: 6| Step: 13
Training loss: 2.925111660904548
Validation loss: 2.457672973345676

Epoch: 131| Step: 0
Training loss: 2.5443701098363927
Validation loss: 2.454707281306948

Epoch: 6| Step: 1
Training loss: 2.4127440571251793
Validation loss: 2.4627717463627476

Epoch: 6| Step: 2
Training loss: 2.5990954549593748
Validation loss: 2.453170217109456

Epoch: 6| Step: 3
Training loss: 2.378308050301497
Validation loss: 2.457597142780277

Epoch: 6| Step: 4
Training loss: 2.080540106964591
Validation loss: 2.4584855635972147

Epoch: 6| Step: 5
Training loss: 2.9272639715493476
Validation loss: 2.454557514856767

Epoch: 6| Step: 6
Training loss: 2.5329837277594174
Validation loss: 2.4582219233294964

Epoch: 6| Step: 7
Training loss: 2.8901708297662845
Validation loss: 2.458678040879303

Epoch: 6| Step: 8
Training loss: 2.262515966875419
Validation loss: 2.4605351840767633

Epoch: 6| Step: 9
Training loss: 2.4685098072496183
Validation loss: 2.461015778764955

Epoch: 6| Step: 10
Training loss: 2.449543859876765
Validation loss: 2.4546283076171225

Epoch: 6| Step: 11
Training loss: 2.6791523657654888
Validation loss: 2.4486071539395065

Epoch: 6| Step: 12
Training loss: 2.5145284027783235
Validation loss: 2.453702222942276

Epoch: 6| Step: 13
Training loss: 2.380100996294795
Validation loss: 2.4560376833459165

Epoch: 132| Step: 0
Training loss: 2.0252696105060326
Validation loss: 2.4529421711883037

Epoch: 6| Step: 1
Training loss: 2.724610425067002
Validation loss: 2.4522740696371907

Epoch: 6| Step: 2
Training loss: 2.6562726188145422
Validation loss: 2.4509071156449904

Epoch: 6| Step: 3
Training loss: 2.8434461494177574
Validation loss: 2.4556799216539478

Epoch: 6| Step: 4
Training loss: 2.355005323076003
Validation loss: 2.4517334718979202

Epoch: 6| Step: 5
Training loss: 2.3681854058316554
Validation loss: 2.4445168822806953

Epoch: 6| Step: 6
Training loss: 2.395452872764815
Validation loss: 2.4506416142393506

Epoch: 6| Step: 7
Training loss: 2.6479337770242863
Validation loss: 2.441105645556151

Epoch: 6| Step: 8
Training loss: 2.3164799438899393
Validation loss: 2.449041089393529

Epoch: 6| Step: 9
Training loss: 2.771213037006027
Validation loss: 2.448906934927058

Epoch: 6| Step: 10
Training loss: 2.0161794446979804
Validation loss: 2.444998041475912

Epoch: 6| Step: 11
Training loss: 2.282226249097379
Validation loss: 2.4517121265492006

Epoch: 6| Step: 12
Training loss: 2.825934135808759
Validation loss: 2.449604513076015

Epoch: 6| Step: 13
Training loss: 2.687022854343294
Validation loss: 2.4607706972136887

Epoch: 133| Step: 0
Training loss: 2.821602734920949
Validation loss: 2.458422260507925

Epoch: 6| Step: 1
Training loss: 2.4290481167672566
Validation loss: 2.4598876489915993

Epoch: 6| Step: 2
Training loss: 2.3669944155667064
Validation loss: 2.456195343418955

Epoch: 6| Step: 3
Training loss: 2.763560499771043
Validation loss: 2.4595578721469273

Epoch: 6| Step: 4
Training loss: 2.633836315100316
Validation loss: 2.465591051378046

Epoch: 6| Step: 5
Training loss: 2.6938259282220294
Validation loss: 2.4612967415642077

Epoch: 6| Step: 6
Training loss: 2.5053204189507032
Validation loss: 2.4701484228372057

Epoch: 6| Step: 7
Training loss: 2.6086053684063852
Validation loss: 2.461985442657466

Epoch: 6| Step: 8
Training loss: 2.766934962841109
Validation loss: 2.462359934662315

Epoch: 6| Step: 9
Training loss: 2.307957905377211
Validation loss: 2.465033345051746

Epoch: 6| Step: 10
Training loss: 2.1331202246598253
Validation loss: 2.4583749983106995

Epoch: 6| Step: 11
Training loss: 2.417259790267947
Validation loss: 2.464020056914363

Epoch: 6| Step: 12
Training loss: 2.0679441328784836
Validation loss: 2.458086556370755

Epoch: 6| Step: 13
Training loss: 2.7145213422823367
Validation loss: 2.4613317103244534

Epoch: 134| Step: 0
Training loss: 2.578390489259957
Validation loss: 2.45715603413011

Epoch: 6| Step: 1
Training loss: 2.3543451517236855
Validation loss: 2.4595691085663445

Epoch: 6| Step: 2
Training loss: 2.102818337198683
Validation loss: 2.4494569572357348

Epoch: 6| Step: 3
Training loss: 2.905417866683122
Validation loss: 2.4633170758727374

Epoch: 6| Step: 4
Training loss: 2.1854678933170644
Validation loss: 2.4579623368059766

Epoch: 6| Step: 5
Training loss: 2.688680345243995
Validation loss: 2.4601875575344816

Epoch: 6| Step: 6
Training loss: 2.6645483206048515
Validation loss: 2.4538375564736112

Epoch: 6| Step: 7
Training loss: 2.7142118088978964
Validation loss: 2.4591773464686693

Epoch: 6| Step: 8
Training loss: 2.3173313705836955
Validation loss: 2.461746906203031

Epoch: 6| Step: 9
Training loss: 2.550377713073026
Validation loss: 2.4618659150316793

Epoch: 6| Step: 10
Training loss: 2.8513728666611535
Validation loss: 2.4630018910447093

Epoch: 6| Step: 11
Training loss: 2.2603709367149034
Validation loss: 2.4600233045109925

Epoch: 6| Step: 12
Training loss: 2.508127353190866
Validation loss: 2.460365172344464

Epoch: 6| Step: 13
Training loss: 2.365517407972699
Validation loss: 2.4546856947159115

Epoch: 135| Step: 0
Training loss: 1.9151102740456936
Validation loss: 2.4554074435015267

Epoch: 6| Step: 1
Training loss: 2.4338324905781015
Validation loss: 2.4605510751402044

Epoch: 6| Step: 2
Training loss: 3.0478939602245743
Validation loss: 2.459497601734307

Epoch: 6| Step: 3
Training loss: 2.843652073777525
Validation loss: 2.460496360355965

Epoch: 6| Step: 4
Training loss: 2.4658794856761386
Validation loss: 2.460769792927693

Epoch: 6| Step: 5
Training loss: 2.4955471437281695
Validation loss: 2.4568542352014173

Epoch: 6| Step: 6
Training loss: 2.301137278135998
Validation loss: 2.4603825826542427

Epoch: 6| Step: 7
Training loss: 2.6085713685187457
Validation loss: 2.4589755437125262

Epoch: 6| Step: 8
Training loss: 2.97186950618906
Validation loss: 2.4541876854074727

Epoch: 6| Step: 9
Training loss: 2.176788918575413
Validation loss: 2.45572249464556

Epoch: 6| Step: 10
Training loss: 2.0358489572823606
Validation loss: 2.4534549045848344

Epoch: 6| Step: 11
Training loss: 2.8743731603183154
Validation loss: 2.4619464400534414

Epoch: 6| Step: 12
Training loss: 2.394948801596407
Validation loss: 2.458088415413388

Epoch: 6| Step: 13
Training loss: 2.015506829634659
Validation loss: 2.4555685584796607

Epoch: 136| Step: 0
Training loss: 2.438422590887496
Validation loss: 2.4594341388912824

Epoch: 6| Step: 1
Training loss: 2.26154776076716
Validation loss: 2.4577275327177417

Epoch: 6| Step: 2
Training loss: 2.3675550924495274
Validation loss: 2.4619155151411323

Epoch: 6| Step: 3
Training loss: 2.1743673391007095
Validation loss: 2.458744084800867

Epoch: 6| Step: 4
Training loss: 2.6292616446660575
Validation loss: 2.4518738571010044

Epoch: 6| Step: 5
Training loss: 3.209086903508919
Validation loss: 2.454808250936054

Epoch: 6| Step: 6
Training loss: 2.468070890481202
Validation loss: 2.4590301549809577

Epoch: 6| Step: 7
Training loss: 2.474680862658714
Validation loss: 2.4550419341712324

Epoch: 6| Step: 8
Training loss: 2.3524978146355395
Validation loss: 2.459140262606359

Epoch: 6| Step: 9
Training loss: 2.3572236414685896
Validation loss: 2.462091536620952

Epoch: 6| Step: 10
Training loss: 2.6103505578058765
Validation loss: 2.462967325136066

Epoch: 6| Step: 11
Training loss: 2.233419787693098
Validation loss: 2.4568224698440644

Epoch: 6| Step: 12
Training loss: 2.32790317374678
Validation loss: 2.456560584080775

Epoch: 6| Step: 13
Training loss: 2.9478615979194616
Validation loss: 2.4533365891355525

Epoch: 137| Step: 0
Training loss: 1.8780040835335143
Validation loss: 2.464974054840947

Epoch: 6| Step: 1
Training loss: 2.415606649904025
Validation loss: 2.460592449561725

Epoch: 6| Step: 2
Training loss: 2.4379555692183787
Validation loss: 2.455724161304107

Epoch: 6| Step: 3
Training loss: 2.532873410169679
Validation loss: 2.459408837311441

Epoch: 6| Step: 4
Training loss: 2.4282538302499463
Validation loss: 2.454807328266462

Epoch: 6| Step: 5
Training loss: 2.658678179130894
Validation loss: 2.466203309584064

Epoch: 6| Step: 6
Training loss: 3.0344806248110263
Validation loss: 2.4604262613168113

Epoch: 6| Step: 7
Training loss: 2.727422889997099
Validation loss: 2.4645712531065933

Epoch: 6| Step: 8
Training loss: 2.0368196861151793
Validation loss: 2.4661671611718967

Epoch: 6| Step: 9
Training loss: 2.5671166492049013
Validation loss: 2.466185674510021

Epoch: 6| Step: 10
Training loss: 2.634909680318758
Validation loss: 2.4558417306926246

Epoch: 6| Step: 11
Training loss: 2.718397358791916
Validation loss: 2.453139618913053

Epoch: 6| Step: 12
Training loss: 2.573928654580789
Validation loss: 2.452371242563934

Epoch: 6| Step: 13
Training loss: 2.2926603301547597
Validation loss: 2.4533468417473014

Epoch: 138| Step: 0
Training loss: 2.4501505513547936
Validation loss: 2.4515131293618295

Epoch: 6| Step: 1
Training loss: 2.865892996367097
Validation loss: 2.457565855927011

Epoch: 6| Step: 2
Training loss: 2.5860343062152835
Validation loss: 2.452201701904541

Epoch: 6| Step: 3
Training loss: 3.1944366970981877
Validation loss: 2.4513454583499015

Epoch: 6| Step: 4
Training loss: 2.6670578033968737
Validation loss: 2.457934772920329

Epoch: 6| Step: 5
Training loss: 1.8795900746098275
Validation loss: 2.458608342537426

Epoch: 6| Step: 6
Training loss: 2.0851505238013823
Validation loss: 2.455798366916011

Epoch: 6| Step: 7
Training loss: 3.4210905107725447
Validation loss: 2.458490986267866

Epoch: 6| Step: 8
Training loss: 2.153938491906513
Validation loss: 2.4509968850945953

Epoch: 6| Step: 9
Training loss: 1.8965854480340019
Validation loss: 2.4538372730858375

Epoch: 6| Step: 10
Training loss: 1.7030967220093647
Validation loss: 2.45363673159399

Epoch: 6| Step: 11
Training loss: 2.7467461322504776
Validation loss: 2.4685524487223063

Epoch: 6| Step: 12
Training loss: 2.3216026136111503
Validation loss: 2.464138633440424

Epoch: 6| Step: 13
Training loss: 2.3885228507466323
Validation loss: 2.4544165141369683

Epoch: 139| Step: 0
Training loss: 2.463129720192108
Validation loss: 2.455251545472522

Epoch: 6| Step: 1
Training loss: 2.7413969720839457
Validation loss: 2.457111723228399

Epoch: 6| Step: 2
Training loss: 3.300112179092142
Validation loss: 2.4490953624253873

Epoch: 6| Step: 3
Training loss: 2.392180391845508
Validation loss: 2.4549044171053196

Epoch: 6| Step: 4
Training loss: 2.5963213939887697
Validation loss: 2.461402122597257

Epoch: 6| Step: 5
Training loss: 2.408228667557668
Validation loss: 2.4648306111238973

Epoch: 6| Step: 6
Training loss: 2.44406281247251
Validation loss: 2.45065719652093

Epoch: 6| Step: 7
Training loss: 2.3320756201939985
Validation loss: 2.4724074525294246

Epoch: 6| Step: 8
Training loss: 2.525129949417275
Validation loss: 2.4597162516477376

Epoch: 6| Step: 9
Training loss: 2.5995149086541014
Validation loss: 2.4657315339577233

Epoch: 6| Step: 10
Training loss: 2.3891642976058782
Validation loss: 2.4588000344850203

Epoch: 6| Step: 11
Training loss: 2.3279288804572706
Validation loss: 2.4518673906768154

Epoch: 6| Step: 12
Training loss: 2.01302743954251
Validation loss: 2.4549663460594373

Epoch: 6| Step: 13
Training loss: 2.3376886954617477
Validation loss: 2.454239059788088

Epoch: 140| Step: 0
Training loss: 2.3773222161056444
Validation loss: 2.4541858557926175

Epoch: 6| Step: 1
Training loss: 2.8365205527962507
Validation loss: 2.4496754003090433

Epoch: 6| Step: 2
Training loss: 3.1235879378087965
Validation loss: 2.4581149591557208

Epoch: 6| Step: 3
Training loss: 2.665631768954807
Validation loss: 2.4622320090664394

Epoch: 6| Step: 4
Training loss: 2.256526805350781
Validation loss: 2.4587913399036156

Epoch: 6| Step: 5
Training loss: 2.2974836101092655
Validation loss: 2.455768335433891

Epoch: 6| Step: 6
Training loss: 2.3237898230462055
Validation loss: 2.4600250813257474

Epoch: 6| Step: 7
Training loss: 1.8050971949184391
Validation loss: 2.451599051448402

Epoch: 6| Step: 8
Training loss: 2.3809973635284427
Validation loss: 2.4629164877654683

Epoch: 6| Step: 9
Training loss: 2.228602388449865
Validation loss: 2.451080004425619

Epoch: 6| Step: 10
Training loss: 2.6574445843234438
Validation loss: 2.4559196379423334

Epoch: 6| Step: 11
Training loss: 2.8072470883785705
Validation loss: 2.461092061028139

Epoch: 6| Step: 12
Training loss: 2.7754629556571455
Validation loss: 2.459972406299767

Epoch: 6| Step: 13
Training loss: 2.385097121334526
Validation loss: 2.4526291931348765

Epoch: 141| Step: 0
Training loss: 2.5396529699536097
Validation loss: 2.4551082620361733

Epoch: 6| Step: 1
Training loss: 2.811462889294621
Validation loss: 2.4609736041291748

Epoch: 6| Step: 2
Training loss: 2.94729377626963
Validation loss: 2.459491171510613

Epoch: 6| Step: 3
Training loss: 2.5242068414103183
Validation loss: 2.454051925900006

Epoch: 6| Step: 4
Training loss: 2.549539307510671
Validation loss: 2.462045587609814

Epoch: 6| Step: 5
Training loss: 2.7540740266379298
Validation loss: 2.4583616389784053

Epoch: 6| Step: 6
Training loss: 2.0423447657114133
Validation loss: 2.4565510727883866

Epoch: 6| Step: 7
Training loss: 2.103501570432388
Validation loss: 2.46216043432157

Epoch: 6| Step: 8
Training loss: 2.448132531137693
Validation loss: 2.46322294812099

Epoch: 6| Step: 9
Training loss: 2.2360289527642743
Validation loss: 2.463194983339234

Epoch: 6| Step: 10
Training loss: 2.7141703477162444
Validation loss: 2.469903314373811

Epoch: 6| Step: 11
Training loss: 2.1751847331202616
Validation loss: 2.451055881141811

Epoch: 6| Step: 12
Training loss: 2.310874058057022
Validation loss: 2.461163771788073

Epoch: 6| Step: 13
Training loss: 2.601979448511474
Validation loss: 2.452902360600861

Epoch: 142| Step: 0
Training loss: 2.6553537090776627
Validation loss: 2.4514669658653374

Epoch: 6| Step: 1
Training loss: 2.2384369747720965
Validation loss: 2.445101906642623

Epoch: 6| Step: 2
Training loss: 2.756391553639043
Validation loss: 2.4577060454106987

Epoch: 6| Step: 3
Training loss: 1.936003014392713
Validation loss: 2.453882153204591

Epoch: 6| Step: 4
Training loss: 2.427740856600861
Validation loss: 2.46151143803878

Epoch: 6| Step: 5
Training loss: 2.6800107177121517
Validation loss: 2.455472385955232

Epoch: 6| Step: 6
Training loss: 2.702912779839994
Validation loss: 2.4525095821689864

Epoch: 6| Step: 7
Training loss: 1.8157364461446244
Validation loss: 2.4564518166963203

Epoch: 6| Step: 8
Training loss: 2.3911071490598057
Validation loss: 2.4570231718042757

Epoch: 6| Step: 9
Training loss: 2.3194269102224396
Validation loss: 2.463436476630505

Epoch: 6| Step: 10
Training loss: 2.924786916943093
Validation loss: 2.452753350681179

Epoch: 6| Step: 11
Training loss: 2.7509566290434186
Validation loss: 2.4602911690778506

Epoch: 6| Step: 12
Training loss: 2.621568798785635
Validation loss: 2.4551277327602525

Epoch: 6| Step: 13
Training loss: 2.3766618235120966
Validation loss: 2.450832924071581

Epoch: 143| Step: 0
Training loss: 2.8174058301702725
Validation loss: 2.457221835961933

Epoch: 6| Step: 1
Training loss: 2.6214628457174536
Validation loss: 2.444439066775023

Epoch: 6| Step: 2
Training loss: 2.813371141941996
Validation loss: 2.4503022656208553

Epoch: 6| Step: 3
Training loss: 2.697382035667917
Validation loss: 2.453079012623994

Epoch: 6| Step: 4
Training loss: 1.9577245645343102
Validation loss: 2.4539491763242163

Epoch: 6| Step: 5
Training loss: 2.4880285688704182
Validation loss: 2.4533298836127506

Epoch: 6| Step: 6
Training loss: 2.6157420251843786
Validation loss: 2.448331519070605

Epoch: 6| Step: 7
Training loss: 2.7728024897485506
Validation loss: 2.4557025270173383

Epoch: 6| Step: 8
Training loss: 1.9035398886058121
Validation loss: 2.460722252853101

Epoch: 6| Step: 9
Training loss: 2.5972140057638375
Validation loss: 2.4477913249506673

Epoch: 6| Step: 10
Training loss: 2.091036504323765
Validation loss: 2.454723606697983

Epoch: 6| Step: 11
Training loss: 2.5755013727752103
Validation loss: 2.4543518108829616

Epoch: 6| Step: 12
Training loss: 2.3532207130126173
Validation loss: 2.449487568990053

Epoch: 6| Step: 13
Training loss: 2.345739308760265
Validation loss: 2.4545725866171098

Epoch: 144| Step: 0
Training loss: 2.0532955197572216
Validation loss: 2.4613997413828326

Epoch: 6| Step: 1
Training loss: 2.4560009080220384
Validation loss: 2.463608058709579

Epoch: 6| Step: 2
Training loss: 2.52508982130376
Validation loss: 2.4583103426704143

Epoch: 6| Step: 3
Training loss: 2.554212613545574
Validation loss: 2.4616454140665405

Epoch: 6| Step: 4
Training loss: 2.3644372051868343
Validation loss: 2.4547016560231105

Epoch: 6| Step: 5
Training loss: 2.346858493630056
Validation loss: 2.466805927967767

Epoch: 6| Step: 6
Training loss: 2.4656020749649636
Validation loss: 2.4583012907450743

Epoch: 6| Step: 7
Training loss: 2.7661934823297067
Validation loss: 2.461227044727743

Epoch: 6| Step: 8
Training loss: 2.1318008203274594
Validation loss: 2.459071603469652

Epoch: 6| Step: 9
Training loss: 2.626486856709086
Validation loss: 2.4550780117021738

Epoch: 6| Step: 10
Training loss: 2.4708065689807364
Validation loss: 2.4613745891603056

Epoch: 6| Step: 11
Training loss: 2.5830421488738597
Validation loss: 2.4634094659820867

Epoch: 6| Step: 12
Training loss: 2.770506349549094
Validation loss: 2.462389788928765

Epoch: 6| Step: 13
Training loss: 2.7176341255167773
Validation loss: 2.4564677988803427

Epoch: 145| Step: 0
Training loss: 2.5724173510502957
Validation loss: 2.4570852171647166

Epoch: 6| Step: 1
Training loss: 2.0067145169320475
Validation loss: 2.45811489449409

Epoch: 6| Step: 2
Training loss: 2.548964035678394
Validation loss: 2.459091737649084

Epoch: 6| Step: 3
Training loss: 2.222604224697311
Validation loss: 2.4632388299107815

Epoch: 6| Step: 4
Training loss: 2.1424514886088972
Validation loss: 2.463858349522562

Epoch: 6| Step: 5
Training loss: 2.8502239273172996
Validation loss: 2.4607276948171575

Epoch: 6| Step: 6
Training loss: 2.622410359700695
Validation loss: 2.4594588747308026

Epoch: 6| Step: 7
Training loss: 2.028715929837051
Validation loss: 2.4674746764383504

Epoch: 6| Step: 8
Training loss: 2.882643449163438
Validation loss: 2.4587974972218762

Epoch: 6| Step: 9
Training loss: 2.708915550056226
Validation loss: 2.4687738538144957

Epoch: 6| Step: 10
Training loss: 2.139623031569842
Validation loss: 2.461730248059073

Epoch: 6| Step: 11
Training loss: 2.245708505514895
Validation loss: 2.4609824443931383

Epoch: 6| Step: 12
Training loss: 2.7709428770357953
Validation loss: 2.458980141157081

Epoch: 6| Step: 13
Training loss: 2.736765051299509
Validation loss: 2.4525760190240584

Epoch: 146| Step: 0
Training loss: 3.0311021277156893
Validation loss: 2.458698323735755

Epoch: 6| Step: 1
Training loss: 2.2153046412684394
Validation loss: 2.457656093567253

Epoch: 6| Step: 2
Training loss: 2.5966006321115085
Validation loss: 2.45171480890961

Epoch: 6| Step: 3
Training loss: 2.338216327106239
Validation loss: 2.4600325600865105

Epoch: 6| Step: 4
Training loss: 2.409542951655015
Validation loss: 2.455802137006086

Epoch: 6| Step: 5
Training loss: 2.1630131900859597
Validation loss: 2.460833027554691

Epoch: 6| Step: 6
Training loss: 2.66008526543507
Validation loss: 2.4576718900683585

Epoch: 6| Step: 7
Training loss: 1.82812956458932
Validation loss: 2.451457451019772

Epoch: 6| Step: 8
Training loss: 2.0631483966155995
Validation loss: 2.465546553620488

Epoch: 6| Step: 9
Training loss: 2.7025509263285703
Validation loss: 2.467860162094464

Epoch: 6| Step: 10
Training loss: 2.915505386957286
Validation loss: 2.4589449611956202

Epoch: 6| Step: 11
Training loss: 2.80989046894701
Validation loss: 2.460414843083502

Epoch: 6| Step: 12
Training loss: 2.4205672147922073
Validation loss: 2.460818753077116

Epoch: 6| Step: 13
Training loss: 2.4065179056420463
Validation loss: 2.455696005949613

Epoch: 147| Step: 0
Training loss: 2.4591468714924876
Validation loss: 2.467426492535962

Epoch: 6| Step: 1
Training loss: 2.6883401666386084
Validation loss: 2.4561077296733647

Epoch: 6| Step: 2
Training loss: 2.938914993048412
Validation loss: 2.458112259531196

Epoch: 6| Step: 3
Training loss: 2.7254354776412604
Validation loss: 2.4583126137285953

Epoch: 6| Step: 4
Training loss: 2.2318839745940657
Validation loss: 2.4671548929409215

Epoch: 6| Step: 5
Training loss: 1.79056815544827
Validation loss: 2.4593111992089254

Epoch: 6| Step: 6
Training loss: 1.6257541080465774
Validation loss: 2.4552328687652243

Epoch: 6| Step: 7
Training loss: 2.188838004582051
Validation loss: 2.463832964352672

Epoch: 6| Step: 8
Training loss: 1.8747395652463303
Validation loss: 2.4645379265165315

Epoch: 6| Step: 9
Training loss: 1.8218122206482936
Validation loss: 2.4607771402417904

Epoch: 6| Step: 10
Training loss: 2.3924127015516987
Validation loss: 2.459059710329553

Epoch: 6| Step: 11
Training loss: 2.665152228983554
Validation loss: 2.4609940698974087

Epoch: 6| Step: 12
Training loss: 3.2430877538730414
Validation loss: 2.452801077609521

Epoch: 6| Step: 13
Training loss: 3.261772255829853
Validation loss: 2.4565025856531926

Epoch: 148| Step: 0
Training loss: 2.6316456911758235
Validation loss: 2.4570791040722346

Epoch: 6| Step: 1
Training loss: 2.3274199166948644
Validation loss: 2.46052591424236

Epoch: 6| Step: 2
Training loss: 2.7428994843927352
Validation loss: 2.4632509367828206

Epoch: 6| Step: 3
Training loss: 2.2842000411595844
Validation loss: 2.4610506223483726

Epoch: 6| Step: 4
Training loss: 2.374907040031568
Validation loss: 2.4643020800461057

Epoch: 6| Step: 5
Training loss: 2.0022284490554596
Validation loss: 2.4562471934942334

Epoch: 6| Step: 6
Training loss: 3.1962665355435163
Validation loss: 2.459656930199652

Epoch: 6| Step: 7
Training loss: 2.129109168224994
Validation loss: 2.458386862436047

Epoch: 6| Step: 8
Training loss: 2.8268180061109085
Validation loss: 2.46186714173046

Epoch: 6| Step: 9
Training loss: 1.9477954629599443
Validation loss: 2.460916089161572

Epoch: 6| Step: 10
Training loss: 2.303290923519897
Validation loss: 2.459322735668556

Epoch: 6| Step: 11
Training loss: 2.752288299790259
Validation loss: 2.4640738065077605

Epoch: 6| Step: 12
Training loss: 2.4734675084484823
Validation loss: 2.4658311013970864

Epoch: 6| Step: 13
Training loss: 2.7144290305427083
Validation loss: 2.4559372415642375

Epoch: 149| Step: 0
Training loss: 2.435389265086294
Validation loss: 2.456381302964834

Epoch: 6| Step: 1
Training loss: 1.9678058479612224
Validation loss: 2.4584743222254715

Epoch: 6| Step: 2
Training loss: 2.252860687879235
Validation loss: 2.4543284968941106

Epoch: 6| Step: 3
Training loss: 2.353807562611923
Validation loss: 2.466656936377158

Epoch: 6| Step: 4
Training loss: 2.7309030343004608
Validation loss: 2.4577039354731287

Epoch: 6| Step: 5
Training loss: 2.925730726470479
Validation loss: 2.452971840388008

Epoch: 6| Step: 6
Training loss: 2.6441082975395376
Validation loss: 2.4578347481773344

Epoch: 6| Step: 7
Training loss: 3.4661079861327466
Validation loss: 2.455621870062686

Epoch: 6| Step: 8
Training loss: 2.424016584006003
Validation loss: 2.4574826972535986

Epoch: 6| Step: 9
Training loss: 1.9230758512934119
Validation loss: 2.4564023489130777

Epoch: 6| Step: 10
Training loss: 2.1936292609655372
Validation loss: 2.4568344465965337

Epoch: 6| Step: 11
Training loss: 2.4283615370216363
Validation loss: 2.4599312960585564

Epoch: 6| Step: 12
Training loss: 2.608241217533442
Validation loss: 2.4544260256062596

Epoch: 6| Step: 13
Training loss: 2.4617790599760814
Validation loss: 2.4495225116696915

Epoch: 150| Step: 0
Training loss: 2.438250890638755
Validation loss: 2.4571333451076383

Epoch: 6| Step: 1
Training loss: 2.8687541953328357
Validation loss: 2.457560649497269

Epoch: 6| Step: 2
Training loss: 2.2117424567813937
Validation loss: 2.4505679497292117

Epoch: 6| Step: 3
Training loss: 2.1067586991974867
Validation loss: 2.4528633350682743

Epoch: 6| Step: 4
Training loss: 2.2020561926356246
Validation loss: 2.4594249133773154

Epoch: 6| Step: 5
Training loss: 2.427920174029608
Validation loss: 2.4500893278121167

Epoch: 6| Step: 6
Training loss: 2.9122485531169615
Validation loss: 2.460476391094605

Epoch: 6| Step: 7
Training loss: 3.2127380520384206
Validation loss: 2.4614167730692134

Epoch: 6| Step: 8
Training loss: 2.018433971883286
Validation loss: 2.459382873031378

Epoch: 6| Step: 9
Training loss: 2.171798155817952
Validation loss: 2.4521937050256173

Epoch: 6| Step: 10
Training loss: 2.3015823394479695
Validation loss: 2.4535612459500933

Epoch: 6| Step: 11
Training loss: 2.3808569716456285
Validation loss: 2.452155907874133

Epoch: 6| Step: 12
Training loss: 2.912417359059661
Validation loss: 2.4579186547747645

Epoch: 6| Step: 13
Training loss: 2.2728535010223108
Validation loss: 2.4610924888931978

Epoch: 151| Step: 0
Training loss: 2.637370796112394
Validation loss: 2.4527271377368405

Epoch: 6| Step: 1
Training loss: 1.578937585699996
Validation loss: 2.460485136245992

Epoch: 6| Step: 2
Training loss: 1.829819700708302
Validation loss: 2.463394633792088

Epoch: 6| Step: 3
Training loss: 2.6432872646818297
Validation loss: 2.4520924089685683

Epoch: 6| Step: 4
Training loss: 2.9959821817049015
Validation loss: 2.461326075964065

Epoch: 6| Step: 5
Training loss: 2.581920442497545
Validation loss: 2.4641177906387663

Epoch: 6| Step: 6
Training loss: 2.207164983157901
Validation loss: 2.4568767165452408

Epoch: 6| Step: 7
Training loss: 2.519325709275985
Validation loss: 2.457078004360862

Epoch: 6| Step: 8
Training loss: 2.1865655265461137
Validation loss: 2.462532035503073

Epoch: 6| Step: 9
Training loss: 2.538222986963335
Validation loss: 2.464786712260605

Epoch: 6| Step: 10
Training loss: 2.3577956414552204
Validation loss: 2.455323952784949

Epoch: 6| Step: 11
Training loss: 2.7544474617792494
Validation loss: 2.4585242978510884

Epoch: 6| Step: 12
Training loss: 2.813757890633459
Validation loss: 2.451326930193941

Epoch: 6| Step: 13
Training loss: 2.75605687786148
Validation loss: 2.4570448914974508

Epoch: 152| Step: 0
Training loss: 2.0913257513560417
Validation loss: 2.4631416582100654

Epoch: 6| Step: 1
Training loss: 2.3863559078735443
Validation loss: 2.462004786364078

Epoch: 6| Step: 2
Training loss: 2.579265735143029
Validation loss: 2.461269295744189

Epoch: 6| Step: 3
Training loss: 2.126820065318286
Validation loss: 2.470914157705556

Epoch: 6| Step: 4
Training loss: 2.7301335163094533
Validation loss: 2.459273842682595

Epoch: 6| Step: 5
Training loss: 2.5187229959635316
Validation loss: 2.457695528041067

Epoch: 6| Step: 6
Training loss: 2.9810265735259507
Validation loss: 2.457977905010659

Epoch: 6| Step: 7
Training loss: 2.4982785019849616
Validation loss: 2.4587284568019956

Epoch: 6| Step: 8
Training loss: 2.8481039733981155
Validation loss: 2.4660414150304835

Epoch: 6| Step: 9
Training loss: 2.0589922350995984
Validation loss: 2.4666078343549427

Epoch: 6| Step: 10
Training loss: 2.058231213259066
Validation loss: 2.458817641726238

Epoch: 6| Step: 11
Training loss: 2.100158222232469
Validation loss: 2.4665601007106814

Epoch: 6| Step: 12
Training loss: 2.392296699009918
Validation loss: 2.4517041929041077

Epoch: 6| Step: 13
Training loss: 2.9284214386760588
Validation loss: 2.461432658441097

Epoch: 153| Step: 0
Training loss: 2.8475045371752086
Validation loss: 2.4557044444992187

Epoch: 6| Step: 1
Training loss: 2.4295080186107425
Validation loss: 2.4580530125338544

Epoch: 6| Step: 2
Training loss: 2.517355755913545
Validation loss: 2.4625620974538953

Epoch: 6| Step: 3
Training loss: 1.7143033160146153
Validation loss: 2.46026141855594

Epoch: 6| Step: 4
Training loss: 2.285026246635898
Validation loss: 2.4643315237932937

Epoch: 6| Step: 5
Training loss: 2.089301892917765
Validation loss: 2.454451880426724

Epoch: 6| Step: 6
Training loss: 2.5247235401752772
Validation loss: 2.4569130178204768

Epoch: 6| Step: 7
Training loss: 2.049155675489629
Validation loss: 2.458039481709598

Epoch: 6| Step: 8
Training loss: 2.3480858941232152
Validation loss: 2.458075903221073

Epoch: 6| Step: 9
Training loss: 2.1549970884369833
Validation loss: 2.4686330614682848

Epoch: 6| Step: 10
Training loss: 2.6326321956122674
Validation loss: 2.4672165466106257

Epoch: 6| Step: 11
Training loss: 2.7562724452067164
Validation loss: 2.4635447259498187

Epoch: 6| Step: 12
Training loss: 3.4750066468977994
Validation loss: 2.466396481520108

Epoch: 6| Step: 13
Training loss: 2.5251258894253747
Validation loss: 2.465586111699204

Epoch: 154| Step: 0
Training loss: 3.014588331013201
Validation loss: 2.4627447848963273

Epoch: 6| Step: 1
Training loss: 2.3795425741607676
Validation loss: 2.4614252242935417

Epoch: 6| Step: 2
Training loss: 2.390101556256292
Validation loss: 2.4508868007368356

Epoch: 6| Step: 3
Training loss: 2.3291394372575627
Validation loss: 2.449603701997406

Epoch: 6| Step: 4
Training loss: 2.3740880118673027
Validation loss: 2.4623781861514535

Epoch: 6| Step: 5
Training loss: 2.7418807420568294
Validation loss: 2.4581025440914543

Epoch: 6| Step: 6
Training loss: 2.752894005843608
Validation loss: 2.4548640960814296

Epoch: 6| Step: 7
Training loss: 2.520485677995119
Validation loss: 2.4561784211293998

Epoch: 6| Step: 8
Training loss: 2.4221039694560966
Validation loss: 2.4518766203171674

Epoch: 6| Step: 9
Training loss: 2.1275921887014295
Validation loss: 2.4520145016594554

Epoch: 6| Step: 10
Training loss: 2.2705127478104963
Validation loss: 2.44983867646117

Epoch: 6| Step: 11
Training loss: 2.16202391306509
Validation loss: 2.4511740472994195

Epoch: 6| Step: 12
Training loss: 2.910607137889415
Validation loss: 2.45485357462943

Epoch: 6| Step: 13
Training loss: 2.4362215455195693
Validation loss: 2.4457236889812046

Epoch: 155| Step: 0
Training loss: 2.7837988500330026
Validation loss: 2.447292529681559

Epoch: 6| Step: 1
Training loss: 2.5212616884928014
Validation loss: 2.4450122295456036

Epoch: 6| Step: 2
Training loss: 2.282501922305782
Validation loss: 2.4445628031580586

Epoch: 6| Step: 3
Training loss: 2.8015084699195985
Validation loss: 2.450127181171086

Epoch: 6| Step: 4
Training loss: 2.70526426480513
Validation loss: 2.4511780676693853

Epoch: 6| Step: 5
Training loss: 2.800878632154658
Validation loss: 2.4613111262805805

Epoch: 6| Step: 6
Training loss: 2.3733459535363224
Validation loss: 2.472554779783031

Epoch: 6| Step: 7
Training loss: 1.71758362467937
Validation loss: 2.4636918735211624

Epoch: 6| Step: 8
Training loss: 3.144881367591335
Validation loss: 2.472673011309164

Epoch: 6| Step: 9
Training loss: 2.473846775685162
Validation loss: 2.467938237385531

Epoch: 6| Step: 10
Training loss: 1.7999782137082156
Validation loss: 2.4626630274062986

Epoch: 6| Step: 11
Training loss: 2.0518932996823325
Validation loss: 2.4670591157088473

Epoch: 6| Step: 12
Training loss: 2.276870599868632
Validation loss: 2.468957948578171

Epoch: 6| Step: 13
Training loss: 2.20330788144153
Validation loss: 2.4655924051545326

Epoch: 156| Step: 0
Training loss: 2.866355172318489
Validation loss: 2.4662449517047746

Epoch: 6| Step: 1
Training loss: 2.210195140422297
Validation loss: 2.4657698723698314

Epoch: 6| Step: 2
Training loss: 3.0167291230819484
Validation loss: 2.4560774835599224

Epoch: 6| Step: 3
Training loss: 2.151460275874168
Validation loss: 2.4575501234208375

Epoch: 6| Step: 4
Training loss: 2.5532192478970606
Validation loss: 2.4636376639783624

Epoch: 6| Step: 5
Training loss: 2.1299640792181838
Validation loss: 2.455119195117623

Epoch: 6| Step: 6
Training loss: 2.161251625073662
Validation loss: 2.4646571069049967

Epoch: 6| Step: 7
Training loss: 2.170914032571628
Validation loss: 2.4540749186759365

Epoch: 6| Step: 8
Training loss: 2.531126231240576
Validation loss: 2.457481921114285

Epoch: 6| Step: 9
Training loss: 2.641838923312418
Validation loss: 2.4605602318344224

Epoch: 6| Step: 10
Training loss: 1.6887395332966448
Validation loss: 2.456570014458469

Epoch: 6| Step: 11
Training loss: 2.443717948527773
Validation loss: 2.4545613111359486

Epoch: 6| Step: 12
Training loss: 2.641495781153744
Validation loss: 2.4585926490266456

Epoch: 6| Step: 13
Training loss: 2.8491160310178882
Validation loss: 2.458834553932382

Epoch: 157| Step: 0
Training loss: 1.9874862550464125
Validation loss: 2.4538771332789855

Epoch: 6| Step: 1
Training loss: 2.276227255324762
Validation loss: 2.4548662974872206

Epoch: 6| Step: 2
Training loss: 2.874410651795218
Validation loss: 2.4586294502322623

Epoch: 6| Step: 3
Training loss: 2.43976233716336
Validation loss: 2.452225587057342

Epoch: 6| Step: 4
Training loss: 1.9674439109359212
Validation loss: 2.4667967461318914

Epoch: 6| Step: 5
Training loss: 2.1068227515564355
Validation loss: 2.463860107443958

Epoch: 6| Step: 6
Training loss: 1.8239406071412814
Validation loss: 2.464793193155572

Epoch: 6| Step: 7
Training loss: 3.0848235232235885
Validation loss: 2.464664224977688

Epoch: 6| Step: 8
Training loss: 2.527424311348842
Validation loss: 2.4639662416403842

Epoch: 6| Step: 9
Training loss: 3.0702031628114614
Validation loss: 2.4628967398018773

Epoch: 6| Step: 10
Training loss: 2.0406191208011353
Validation loss: 2.468184651857984

Epoch: 6| Step: 11
Training loss: 1.894790600200882
Validation loss: 2.4619127712627185

Epoch: 6| Step: 12
Training loss: 3.0600287918531444
Validation loss: 2.466372669157711

Epoch: 6| Step: 13
Training loss: 2.762046870011422
Validation loss: 2.4665413163098395

Epoch: 158| Step: 0
Training loss: 2.9220701754248513
Validation loss: 2.455149533961326

Epoch: 6| Step: 1
Training loss: 2.2061270963243977
Validation loss: 2.4705722779215895

Epoch: 6| Step: 2
Training loss: 2.5264021049360283
Validation loss: 2.4669076668652976

Epoch: 6| Step: 3
Training loss: 2.202582867521574
Validation loss: 2.461148998718986

Epoch: 6| Step: 4
Training loss: 2.183282219188969
Validation loss: 2.459651195070956

Epoch: 6| Step: 5
Training loss: 2.685836632095601
Validation loss: 2.4564857706250125

Epoch: 6| Step: 6
Training loss: 2.8666007041734933
Validation loss: 2.452516678790023

Epoch: 6| Step: 7
Training loss: 2.3013494596273616
Validation loss: 2.4686103974478875

Epoch: 6| Step: 8
Training loss: 2.552349455575722
Validation loss: 2.46287358737168

Epoch: 6| Step: 9
Training loss: 2.1170631738173507
Validation loss: 2.4552135769045327

Epoch: 6| Step: 10
Training loss: 2.093862957540664
Validation loss: 2.4578267292080476

Epoch: 6| Step: 11
Training loss: 2.6020162833874947
Validation loss: 2.4644886371631536

Epoch: 6| Step: 12
Training loss: 2.7540128213809774
Validation loss: 2.470432287758616

Epoch: 6| Step: 13
Training loss: 2.318349193881657
Validation loss: 2.4844547954676184

Epoch: 159| Step: 0
Training loss: 3.1177369209573764
Validation loss: 2.4730491386310423

Epoch: 6| Step: 1
Training loss: 2.8286813431118643
Validation loss: 2.4750086646137066

Epoch: 6| Step: 2
Training loss: 2.6916473679298503
Validation loss: 2.4820862229771037

Epoch: 6| Step: 3
Training loss: 2.4453442848747953
Validation loss: 2.489441980687857

Epoch: 6| Step: 4
Training loss: 2.0310337831888936
Validation loss: 2.4836427261181004

Epoch: 6| Step: 5
Training loss: 1.8435707651725035
Validation loss: 2.4932450429393502

Epoch: 6| Step: 6
Training loss: 2.3734978392554815
Validation loss: 2.4738472415007364

Epoch: 6| Step: 7
Training loss: 2.1471784751990017
Validation loss: 2.475196565984782

Epoch: 6| Step: 8
Training loss: 2.3434912983851017
Validation loss: 2.4594914946378332

Epoch: 6| Step: 9
Training loss: 2.208087067698001
Validation loss: 2.460850854387786

Epoch: 6| Step: 10
Training loss: 2.419612295131689
Validation loss: 2.462775150811786

Epoch: 6| Step: 11
Training loss: 2.8349748979698464
Validation loss: 2.4607664341482276

Epoch: 6| Step: 12
Training loss: 2.168862758200692
Validation loss: 2.4666574035504967

Epoch: 6| Step: 13
Training loss: 2.654258250558287
Validation loss: 2.4589047145085243

Epoch: 160| Step: 0
Training loss: 2.642417344575341
Validation loss: 2.467423013982528

Epoch: 6| Step: 1
Training loss: 2.0881205932607223
Validation loss: 2.4643029669110996

Epoch: 6| Step: 2
Training loss: 2.1186508245522035
Validation loss: 2.4694519332735956

Epoch: 6| Step: 3
Training loss: 2.2624549524027238
Validation loss: 2.4559875438655965

Epoch: 6| Step: 4
Training loss: 2.400731682506223
Validation loss: 2.4603906417447923

Epoch: 6| Step: 5
Training loss: 2.5569703521143525
Validation loss: 2.4690951174530635

Epoch: 6| Step: 6
Training loss: 2.6153924508214756
Validation loss: 2.465086202087862

Epoch: 6| Step: 7
Training loss: 2.695817894476224
Validation loss: 2.4659897225478513

Epoch: 6| Step: 8
Training loss: 2.666257966034583
Validation loss: 2.4542644956182103

Epoch: 6| Step: 9
Training loss: 2.6333219222131388
Validation loss: 2.4675099602433517

Epoch: 6| Step: 10
Training loss: 2.4339253550221254
Validation loss: 2.463392262569094

Epoch: 6| Step: 11
Training loss: 2.569799308213984
Validation loss: 2.464502842014124

Epoch: 6| Step: 12
Training loss: 2.5249833134770223
Validation loss: 2.465661785204273

Epoch: 6| Step: 13
Training loss: 2.1201545223566685
Validation loss: 2.465296458380998

Epoch: 161| Step: 0
Training loss: 2.6123956796360694
Validation loss: 2.469814465397561

Epoch: 6| Step: 1
Training loss: 2.333204424794025
Validation loss: 2.462019102384501

Epoch: 6| Step: 2
Training loss: 2.8198079426613605
Validation loss: 2.4639380675775

Epoch: 6| Step: 3
Training loss: 2.445131532840437
Validation loss: 2.46614284709896

Epoch: 6| Step: 4
Training loss: 2.288791679532002
Validation loss: 2.4614693364691025

Epoch: 6| Step: 5
Training loss: 2.561823220549847
Validation loss: 2.4819677194083813

Epoch: 6| Step: 6
Training loss: 2.5085178227132685
Validation loss: 2.475795125556279

Epoch: 6| Step: 7
Training loss: 2.3520672532087477
Validation loss: 2.467764999964965

Epoch: 6| Step: 8
Training loss: 2.38428519343565
Validation loss: 2.464174803547416

Epoch: 6| Step: 9
Training loss: 2.1043100087910394
Validation loss: 2.4647767813039105

Epoch: 6| Step: 10
Training loss: 2.352912489982651
Validation loss: 2.4632055740193435

Epoch: 6| Step: 11
Training loss: 2.619196516403557
Validation loss: 2.465074305726604

Epoch: 6| Step: 12
Training loss: 2.958784060102874
Validation loss: 2.464320204278224

Epoch: 6| Step: 13
Training loss: 1.9764710657736257
Validation loss: 2.4686468722536845

Epoch: 162| Step: 0
Training loss: 2.5080129002630236
Validation loss: 2.465890201799366

Epoch: 6| Step: 1
Training loss: 2.6296314979252076
Validation loss: 2.467474233575885

Epoch: 6| Step: 2
Training loss: 2.344585015641115
Validation loss: 2.473855963479608

Epoch: 6| Step: 3
Training loss: 2.1613516785072333
Validation loss: 2.4652015922009554

Epoch: 6| Step: 4
Training loss: 2.432723819044927
Validation loss: 2.470032105526782

Epoch: 6| Step: 5
Training loss: 3.030628101223779
Validation loss: 2.464974490092389

Epoch: 6| Step: 6
Training loss: 2.0602919579061467
Validation loss: 2.4600937946137167

Epoch: 6| Step: 7
Training loss: 2.8363813573110304
Validation loss: 2.46311069990613

Epoch: 6| Step: 8
Training loss: 3.030463677107284
Validation loss: 2.460825357455427

Epoch: 6| Step: 9
Training loss: 2.085035150355157
Validation loss: 2.454434832789214

Epoch: 6| Step: 10
Training loss: 2.5756990983686183
Validation loss: 2.462806936198943

Epoch: 6| Step: 11
Training loss: 1.600544154498675
Validation loss: 2.4617144453163813

Epoch: 6| Step: 12
Training loss: 2.4742605780970597
Validation loss: 2.4621902667990017

Epoch: 6| Step: 13
Training loss: 2.8221901009034114
Validation loss: 2.4618925229595185

Epoch: 163| Step: 0
Training loss: 2.1723926393870623
Validation loss: 2.4634412593152324

Epoch: 6| Step: 1
Training loss: 2.324277117340912
Validation loss: 2.461043372727776

Epoch: 6| Step: 2
Training loss: 2.814065624618702
Validation loss: 2.4630614142578096

Epoch: 6| Step: 3
Training loss: 2.477358620898829
Validation loss: 2.461763515809922

Epoch: 6| Step: 4
Training loss: 2.778127567732046
Validation loss: 2.4579646324352953

Epoch: 6| Step: 5
Training loss: 1.988533948267536
Validation loss: 2.453322408714884

Epoch: 6| Step: 6
Training loss: 2.505816559637636
Validation loss: 2.4537394942533055

Epoch: 6| Step: 7
Training loss: 2.0949516833430772
Validation loss: 2.4437831528440053

Epoch: 6| Step: 8
Training loss: 1.9214208422954606
Validation loss: 2.45089609081549

Epoch: 6| Step: 9
Training loss: 2.967949367780224
Validation loss: 2.4477914385857633

Epoch: 6| Step: 10
Training loss: 2.5342053715280826
Validation loss: 2.445141771102902

Epoch: 6| Step: 11
Training loss: 2.526830795093908
Validation loss: 2.4478263892442813

Epoch: 6| Step: 12
Training loss: 2.839070588742261
Validation loss: 2.4430531554589536

Epoch: 6| Step: 13
Training loss: 2.3206677919403536
Validation loss: 2.4663070470260937

Epoch: 164| Step: 0
Training loss: 2.2591864289540746
Validation loss: 2.456406085722855

Epoch: 6| Step: 1
Training loss: 2.589217672722454
Validation loss: 2.465805261144245

Epoch: 6| Step: 2
Training loss: 2.7772316353403887
Validation loss: 2.4681980787676476

Epoch: 6| Step: 3
Training loss: 2.4343309587013224
Validation loss: 2.4649518246743676

Epoch: 6| Step: 4
Training loss: 2.451886003884296
Validation loss: 2.4561290368214523

Epoch: 6| Step: 5
Training loss: 2.396664240397718
Validation loss: 2.457607854595874

Epoch: 6| Step: 6
Training loss: 1.805419113024878
Validation loss: 2.4584449053085033

Epoch: 6| Step: 7
Training loss: 1.8324263091837738
Validation loss: 2.468912465347003

Epoch: 6| Step: 8
Training loss: 3.0503304474471364
Validation loss: 2.469715863022782

Epoch: 6| Step: 9
Training loss: 2.8735377698187987
Validation loss: 2.4711279542790727

Epoch: 6| Step: 10
Training loss: 2.3310679837675883
Validation loss: 2.4703809203891645

Epoch: 6| Step: 11
Training loss: 2.4174242038146647
Validation loss: 2.4629967929061842

Epoch: 6| Step: 12
Training loss: 2.1204129245170584
Validation loss: 2.462090624750427

Epoch: 6| Step: 13
Training loss: 2.7487799799222272
Validation loss: 2.4621606441265684

Epoch: 165| Step: 0
Training loss: 1.9671297217891328
Validation loss: 2.460171034199046

Epoch: 6| Step: 1
Training loss: 2.4805123873499175
Validation loss: 2.453497240855721

Epoch: 6| Step: 2
Training loss: 1.7756649598536598
Validation loss: 2.465347697978295

Epoch: 6| Step: 3
Training loss: 2.692422661055764
Validation loss: 2.465348681175002

Epoch: 6| Step: 4
Training loss: 2.0802934848534664
Validation loss: 2.469138537386274

Epoch: 6| Step: 5
Training loss: 2.621171793792006
Validation loss: 2.466756474518236

Epoch: 6| Step: 6
Training loss: 2.859473555070233
Validation loss: 2.468001626457779

Epoch: 6| Step: 7
Training loss: 2.8036980969152583
Validation loss: 2.4738265849050993

Epoch: 6| Step: 8
Training loss: 2.2748796535980738
Validation loss: 2.474346802311456

Epoch: 6| Step: 9
Training loss: 2.340003719815941
Validation loss: 2.467997440285285

Epoch: 6| Step: 10
Training loss: 2.2399312540131846
Validation loss: 2.4615269837967726

Epoch: 6| Step: 11
Training loss: 2.5150663336012733
Validation loss: 2.4661068025351645

Epoch: 6| Step: 12
Training loss: 2.977540183512826
Validation loss: 2.4586932813201376

Epoch: 6| Step: 13
Training loss: 2.2326032109340757
Validation loss: 2.4576976460699314

Epoch: 166| Step: 0
Training loss: 2.728923747970087
Validation loss: 2.460992124243923

Epoch: 6| Step: 1
Training loss: 2.2898682097446716
Validation loss: 2.4720743226757738

Epoch: 6| Step: 2
Training loss: 2.2461286724526963
Validation loss: 2.474071915010211

Epoch: 6| Step: 3
Training loss: 2.8875021071653397
Validation loss: 2.4612478232127892

Epoch: 6| Step: 4
Training loss: 2.2037052986514425
Validation loss: 2.4742247802737976

Epoch: 6| Step: 5
Training loss: 2.8771904603394542
Validation loss: 2.4759782408819717

Epoch: 6| Step: 6
Training loss: 2.045432128205424
Validation loss: 2.4694007949299297

Epoch: 6| Step: 7
Training loss: 2.1175553681432038
Validation loss: 2.4674625661342966

Epoch: 6| Step: 8
Training loss: 2.5645493476985397
Validation loss: 2.471284812433778

Epoch: 6| Step: 9
Training loss: 2.253248941978038
Validation loss: 2.4703104101594837

Epoch: 6| Step: 10
Training loss: 2.263274347981472
Validation loss: 2.473303366177061

Epoch: 6| Step: 11
Training loss: 2.8704200421300223
Validation loss: 2.4738804266190684

Epoch: 6| Step: 12
Training loss: 2.0465912367416257
Validation loss: 2.470090445845814

Epoch: 6| Step: 13
Training loss: 2.7387064604995963
Validation loss: 2.4855064358824563

Epoch: 167| Step: 0
Training loss: 2.4542056090844095
Validation loss: 2.4816488187296275

Epoch: 6| Step: 1
Training loss: 2.3866942758756937
Validation loss: 2.4762960745928146

Epoch: 6| Step: 2
Training loss: 2.0560026999875163
Validation loss: 2.4732575290644636

Epoch: 6| Step: 3
Training loss: 2.6223489908064757
Validation loss: 2.4734779025272426

Epoch: 6| Step: 4
Training loss: 2.3894102712907133
Validation loss: 2.472828951926773

Epoch: 6| Step: 5
Training loss: 1.7949194840673683
Validation loss: 2.4765855886383896

Epoch: 6| Step: 6
Training loss: 2.4986872087215737
Validation loss: 2.4679801480975088

Epoch: 6| Step: 7
Training loss: 2.3658828413422164
Validation loss: 2.471651054748014

Epoch: 6| Step: 8
Training loss: 2.339900504874338
Validation loss: 2.4683347022752318

Epoch: 6| Step: 9
Training loss: 2.862504424379086
Validation loss: 2.4671666021027705

Epoch: 6| Step: 10
Training loss: 1.947532887995219
Validation loss: 2.465929633508995

Epoch: 6| Step: 11
Training loss: 3.4414020932331595
Validation loss: 2.4660622657439126

Epoch: 6| Step: 12
Training loss: 2.3850473398432697
Validation loss: 2.465635967351118

Epoch: 6| Step: 13
Training loss: 2.169586952381792
Validation loss: 2.4651922834971

Epoch: 168| Step: 0
Training loss: 3.0017537712860753
Validation loss: 2.4849973814686335

Epoch: 6| Step: 1
Training loss: 2.5989253794403506
Validation loss: 2.4703078847186934

Epoch: 6| Step: 2
Training loss: 1.9757611003314357
Validation loss: 2.4829195825728556

Epoch: 6| Step: 3
Training loss: 2.6105069200927806
Validation loss: 2.4809064343793823

Epoch: 6| Step: 4
Training loss: 2.6895077325156955
Validation loss: 2.483173407668396

Epoch: 6| Step: 5
Training loss: 2.1766526619118425
Validation loss: 2.491869197596637

Epoch: 6| Step: 6
Training loss: 2.1847311752770726
Validation loss: 2.484038012219873

Epoch: 6| Step: 7
Training loss: 2.7436085901337517
Validation loss: 2.4739431976390756

Epoch: 6| Step: 8
Training loss: 2.0487328171722266
Validation loss: 2.463753371924279

Epoch: 6| Step: 9
Training loss: 2.2505038015269507
Validation loss: 2.466132148186411

Epoch: 6| Step: 10
Training loss: 1.849414593632765
Validation loss: 2.469597522104213

Epoch: 6| Step: 11
Training loss: 2.840958451275731
Validation loss: 2.4642464890987967

Epoch: 6| Step: 12
Training loss: 2.450401203236672
Validation loss: 2.465161834521554

Epoch: 6| Step: 13
Training loss: 2.783677317060578
Validation loss: 2.462344781466426

Epoch: 169| Step: 0
Training loss: 2.705944024600291
Validation loss: 2.468581310577057

Epoch: 6| Step: 1
Training loss: 2.3539931196762045
Validation loss: 2.4722638293371433

Epoch: 6| Step: 2
Training loss: 2.550381545899179
Validation loss: 2.467288812156116

Epoch: 6| Step: 3
Training loss: 2.6099056229756363
Validation loss: 2.466528790611425

Epoch: 6| Step: 4
Training loss: 2.905959248098866
Validation loss: 2.464884971761627

Epoch: 6| Step: 5
Training loss: 2.2114274523377713
Validation loss: 2.461674510252003

Epoch: 6| Step: 6
Training loss: 1.7346384346712418
Validation loss: 2.4693192578159

Epoch: 6| Step: 7
Training loss: 2.4855505123588424
Validation loss: 2.4664439122253983

Epoch: 6| Step: 8
Training loss: 2.2556386864453852
Validation loss: 2.4631219039854946

Epoch: 6| Step: 9
Training loss: 2.5194216682201818
Validation loss: 2.4657038151884385

Epoch: 6| Step: 10
Training loss: 2.349262113586417
Validation loss: 2.457987572444314

Epoch: 6| Step: 11
Training loss: 2.5888842247942314
Validation loss: 2.465538575849973

Epoch: 6| Step: 12
Training loss: 2.249891384470113
Validation loss: 2.4627486814999364

Epoch: 6| Step: 13
Training loss: 2.641876646374816
Validation loss: 2.463367808190643

Epoch: 170| Step: 0
Training loss: 2.390035818462036
Validation loss: 2.455173422805695

Epoch: 6| Step: 1
Training loss: 2.52131776384818
Validation loss: 2.4571214264138947

Epoch: 6| Step: 2
Training loss: 2.7280203957620266
Validation loss: 2.4672976458980957

Epoch: 6| Step: 3
Training loss: 1.5472769166938392
Validation loss: 2.459720298442762

Epoch: 6| Step: 4
Training loss: 2.5155065283511484
Validation loss: 2.4511334541622833

Epoch: 6| Step: 5
Training loss: 2.926625026965688
Validation loss: 2.464880135454543

Epoch: 6| Step: 6
Training loss: 2.525422534903291
Validation loss: 2.4680374018423015

Epoch: 6| Step: 7
Training loss: 1.9937927958998591
Validation loss: 2.4653781768938763

Epoch: 6| Step: 8
Training loss: 3.1263894616112413
Validation loss: 2.4691762435509355

Epoch: 6| Step: 9
Training loss: 2.368337421167628
Validation loss: 2.477671201573052

Epoch: 6| Step: 10
Training loss: 2.6530335643095944
Validation loss: 2.4867649538679624

Epoch: 6| Step: 11
Training loss: 1.9939753270550202
Validation loss: 2.4774980031662075

Epoch: 6| Step: 12
Training loss: 2.2698339893958113
Validation loss: 2.475751934718577

Epoch: 6| Step: 13
Training loss: 2.374190343076193
Validation loss: 2.482666163676314

Epoch: 171| Step: 0
Training loss: 2.670491892333973
Validation loss: 2.474742617854973

Epoch: 6| Step: 1
Training loss: 2.2278490064125833
Validation loss: 2.4759655221912262

Epoch: 6| Step: 2
Training loss: 2.499397777502234
Validation loss: 2.481118881450971

Epoch: 6| Step: 3
Training loss: 2.377080808922923
Validation loss: 2.4795685904471685

Epoch: 6| Step: 4
Training loss: 3.0115455350034828
Validation loss: 2.4761153979805575

Epoch: 6| Step: 5
Training loss: 2.5956930213427327
Validation loss: 2.4828623839277104

Epoch: 6| Step: 6
Training loss: 2.3266977121018013
Validation loss: 2.476008067359785

Epoch: 6| Step: 7
Training loss: 2.021863641957568
Validation loss: 2.490844460346906

Epoch: 6| Step: 8
Training loss: 2.7057858639535843
Validation loss: 2.4810735731075013

Epoch: 6| Step: 9
Training loss: 2.373585932537136
Validation loss: 2.4922689745429403

Epoch: 6| Step: 10
Training loss: 2.3698848049319743
Validation loss: 2.485636936434371

Epoch: 6| Step: 11
Training loss: 2.174946431344351
Validation loss: 2.4862205320273807

Epoch: 6| Step: 12
Training loss: 2.7210610750543665
Validation loss: 2.4870218216731583

Epoch: 6| Step: 13
Training loss: 1.9715952824460885
Validation loss: 2.477331762045241

Epoch: 172| Step: 0
Training loss: 2.2690220062883375
Validation loss: 2.4772622356399427

Epoch: 6| Step: 1
Training loss: 2.3059802551482935
Validation loss: 2.46859885610186

Epoch: 6| Step: 2
Training loss: 2.8259328702875846
Validation loss: 2.4652399630325323

Epoch: 6| Step: 3
Training loss: 2.195672382437447
Validation loss: 2.464695784474776

Epoch: 6| Step: 4
Training loss: 2.3084112569645203
Validation loss: 2.455176764959493

Epoch: 6| Step: 5
Training loss: 2.816856316571583
Validation loss: 2.4568832829996547

Epoch: 6| Step: 6
Training loss: 2.4086738367713925
Validation loss: 2.463610744247135

Epoch: 6| Step: 7
Training loss: 2.731533645112636
Validation loss: 2.464517732028242

Epoch: 6| Step: 8
Training loss: 2.0948756593937294
Validation loss: 2.4687052533565104

Epoch: 6| Step: 9
Training loss: 2.766488495417228
Validation loss: 2.4758438206770155

Epoch: 6| Step: 10
Training loss: 2.152877824434289
Validation loss: 2.4743900658531914

Epoch: 6| Step: 11
Training loss: 2.736985361156961
Validation loss: 2.4585088462460964

Epoch: 6| Step: 12
Training loss: 2.410704712621664
Validation loss: 2.460517274208467

Epoch: 6| Step: 13
Training loss: 2.100188078859427
Validation loss: 2.4525037169096167

Epoch: 173| Step: 0
Training loss: 2.324062823025895
Validation loss: 2.451561707074483

Epoch: 6| Step: 1
Training loss: 2.49995708428741
Validation loss: 2.4705122922998015

Epoch: 6| Step: 2
Training loss: 2.5912666842139274
Validation loss: 2.4589309666383135

Epoch: 6| Step: 3
Training loss: 2.4217964405731447
Validation loss: 2.4645503252860372

Epoch: 6| Step: 4
Training loss: 2.816249213423875
Validation loss: 2.465502828674406

Epoch: 6| Step: 5
Training loss: 2.012965377168762
Validation loss: 2.4740544886056

Epoch: 6| Step: 6
Training loss: 2.4627463822625275
Validation loss: 2.4714792834260075

Epoch: 6| Step: 7
Training loss: 2.593557051870063
Validation loss: 2.474975366180824

Epoch: 6| Step: 8
Training loss: 3.1281371582282764
Validation loss: 2.469205661362342

Epoch: 6| Step: 9
Training loss: 2.866913078126747
Validation loss: 2.475420956503308

Epoch: 6| Step: 10
Training loss: 2.192066657500751
Validation loss: 2.463192394140902

Epoch: 6| Step: 11
Training loss: 2.6730606393154495
Validation loss: 2.4575296047182857

Epoch: 6| Step: 12
Training loss: 1.8816672993409158
Validation loss: 2.4625872859556717

Epoch: 6| Step: 13
Training loss: 2.441583587309343
Validation loss: 2.4599720832357166

Epoch: 174| Step: 0
Training loss: 2.5284566654643026
Validation loss: 2.4643463745312184

Epoch: 6| Step: 1
Training loss: 2.565431360332058
Validation loss: 2.4632304655680275

Epoch: 6| Step: 2
Training loss: 2.7120318663646352
Validation loss: 2.4751814592675583

Epoch: 6| Step: 3
Training loss: 2.465767809620503
Validation loss: 2.467699720808603

Epoch: 6| Step: 4
Training loss: 2.1768480627039968
Validation loss: 2.476442881682754

Epoch: 6| Step: 5
Training loss: 2.856478198261551
Validation loss: 2.470235723580794

Epoch: 6| Step: 6
Training loss: 2.3210232349368454
Validation loss: 2.457279453567959

Epoch: 6| Step: 7
Training loss: 2.6977581041398335
Validation loss: 2.4443681821221657

Epoch: 6| Step: 8
Training loss: 1.9889395772464422
Validation loss: 2.447960473067697

Epoch: 6| Step: 9
Training loss: 2.127986044135258
Validation loss: 2.450435792350714

Epoch: 6| Step: 10
Training loss: 2.3976128866053656
Validation loss: 2.4432279743058034

Epoch: 6| Step: 11
Training loss: 3.076271544764289
Validation loss: 2.448404033653702

Epoch: 6| Step: 12
Training loss: 1.8100632038989037
Validation loss: 2.443562475555964

Epoch: 6| Step: 13
Training loss: 3.008551489474517
Validation loss: 2.4485542170545025

Epoch: 175| Step: 0
Training loss: 2.4145894385966367
Validation loss: 2.4431185076349093

Epoch: 6| Step: 1
Training loss: 2.079871913377338
Validation loss: 2.4483861486539107

Epoch: 6| Step: 2
Training loss: 3.0549678429941807
Validation loss: 2.449052763470072

Epoch: 6| Step: 3
Training loss: 2.8189129090863507
Validation loss: 2.4466952638314767

Epoch: 6| Step: 4
Training loss: 2.6547290711181883
Validation loss: 2.4587109216488114

Epoch: 6| Step: 5
Training loss: 2.8539407088410367
Validation loss: 2.4559364002175323

Epoch: 6| Step: 6
Training loss: 1.7288192568550724
Validation loss: 2.4546584987404336

Epoch: 6| Step: 7
Training loss: 2.3499939208256433
Validation loss: 2.4652173161741264

Epoch: 6| Step: 8
Training loss: 1.984329163029447
Validation loss: 2.4614522082854875

Epoch: 6| Step: 9
Training loss: 2.5193159617625676
Validation loss: 2.450322269147293

Epoch: 6| Step: 10
Training loss: 2.3749901620761498
Validation loss: 2.466266533792496

Epoch: 6| Step: 11
Training loss: 2.463204831947278
Validation loss: 2.4732422819712183

Epoch: 6| Step: 12
Training loss: 2.471321408557481
Validation loss: 2.470579346784173

Epoch: 6| Step: 13
Training loss: 2.2896188313790073
Validation loss: 2.4665645309730504

Epoch: 176| Step: 0
Training loss: 2.8542686685494734
Validation loss: 2.472165935449469

Epoch: 6| Step: 1
Training loss: 2.2108263183244725
Validation loss: 2.470934299917044

Epoch: 6| Step: 2
Training loss: 2.5561997723483123
Validation loss: 2.4622891221223053

Epoch: 6| Step: 3
Training loss: 2.4226037682510806
Validation loss: 2.4730772651344006

Epoch: 6| Step: 4
Training loss: 2.3763614566997555
Validation loss: 2.463786394626874

Epoch: 6| Step: 5
Training loss: 2.359907380727382
Validation loss: 2.4729574059266994

Epoch: 6| Step: 6
Training loss: 2.007968881654694
Validation loss: 2.4650524311022037

Epoch: 6| Step: 7
Training loss: 2.167169867060848
Validation loss: 2.4701417468656244

Epoch: 6| Step: 8
Training loss: 2.658831430326138
Validation loss: 2.467639480173455

Epoch: 6| Step: 9
Training loss: 1.8481052490252305
Validation loss: 2.484273304647261

Epoch: 6| Step: 10
Training loss: 2.4460208342219314
Validation loss: 2.468079665073478

Epoch: 6| Step: 11
Training loss: 2.8832716330686905
Validation loss: 2.4592070131558863

Epoch: 6| Step: 12
Training loss: 2.4187223526510575
Validation loss: 2.474677699389542

Epoch: 6| Step: 13
Training loss: 2.5263722835883082
Validation loss: 2.4667547508784997

Epoch: 177| Step: 0
Training loss: 2.0800852314285883
Validation loss: 2.4830113953262205

Epoch: 6| Step: 1
Training loss: 3.0739504007521297
Validation loss: 2.489492539754675

Epoch: 6| Step: 2
Training loss: 2.8331628261946835
Validation loss: 2.5001386842407998

Epoch: 6| Step: 3
Training loss: 1.8418905288978837
Validation loss: 2.487150692974758

Epoch: 6| Step: 4
Training loss: 2.0541440295764035
Validation loss: 2.4836023836724777

Epoch: 6| Step: 5
Training loss: 2.511888940245213
Validation loss: 2.4870368644585636

Epoch: 6| Step: 6
Training loss: 2.401991605923459
Validation loss: 2.49645589112791

Epoch: 6| Step: 7
Training loss: 2.3166163125729207
Validation loss: 2.4799463558035217

Epoch: 6| Step: 8
Training loss: 2.3788244674981733
Validation loss: 2.4757169048068643

Epoch: 6| Step: 9
Training loss: 2.4679162271338124
Validation loss: 2.4735323622877776

Epoch: 6| Step: 10
Training loss: 2.6866199915628077
Validation loss: 2.4835097207614587

Epoch: 6| Step: 11
Training loss: 2.3120698270977837
Validation loss: 2.4748669424800087

Epoch: 6| Step: 12
Training loss: 2.4482166728701396
Validation loss: 2.459183874463234

Epoch: 6| Step: 13
Training loss: 2.4980726442059247
Validation loss: 2.4802338576548855

Epoch: 178| Step: 0
Training loss: 2.7144927971224906
Validation loss: 2.4820379785228988

Epoch: 6| Step: 1
Training loss: 2.5319297725466066
Validation loss: 2.48155013414354

Epoch: 6| Step: 2
Training loss: 2.3168531118696283
Validation loss: 2.4778425356347933

Epoch: 6| Step: 3
Training loss: 2.8527673971434457
Validation loss: 2.476652077580127

Epoch: 6| Step: 4
Training loss: 1.9743288710646423
Validation loss: 2.472916431207386

Epoch: 6| Step: 5
Training loss: 2.276024045083089
Validation loss: 2.483736247994285

Epoch: 6| Step: 6
Training loss: 2.2761057503507574
Validation loss: 2.468685567795668

Epoch: 6| Step: 7
Training loss: 2.8926048303333403
Validation loss: 2.4765271044908332

Epoch: 6| Step: 8
Training loss: 2.588563812795628
Validation loss: 2.46552708459172

Epoch: 6| Step: 9
Training loss: 2.2488318695890634
Validation loss: 2.482198837749367

Epoch: 6| Step: 10
Training loss: 1.7226767333320192
Validation loss: 2.474312258383673

Epoch: 6| Step: 11
Training loss: 2.792753454059913
Validation loss: 2.46415510598254

Epoch: 6| Step: 12
Training loss: 1.9620172939607712
Validation loss: 2.469285061954898

Epoch: 6| Step: 13
Training loss: 2.4556088841182047
Validation loss: 2.4665548971559916

Epoch: 179| Step: 0
Training loss: 2.2945489848318177
Validation loss: 2.4550158913351185

Epoch: 6| Step: 1
Training loss: 2.5650794839720343
Validation loss: 2.4607477992981095

Epoch: 6| Step: 2
Training loss: 2.503792746793649
Validation loss: 2.4707236790461744

Epoch: 6| Step: 3
Training loss: 2.074297025248867
Validation loss: 2.4616990056643875

Epoch: 6| Step: 4
Training loss: 2.172621564372198
Validation loss: 2.46452605169376

Epoch: 6| Step: 5
Training loss: 3.064632840840435
Validation loss: 2.4758269684858587

Epoch: 6| Step: 6
Training loss: 2.377870732072041
Validation loss: 2.4755424540837154

Epoch: 6| Step: 7
Training loss: 2.054828479412203
Validation loss: 2.476189602316363

Epoch: 6| Step: 8
Training loss: 2.391658783832102
Validation loss: 2.476752385221584

Epoch: 6| Step: 9
Training loss: 2.828336086088275
Validation loss: 2.4683774997057624

Epoch: 6| Step: 10
Training loss: 1.8549924861622114
Validation loss: 2.46668939665898

Epoch: 6| Step: 11
Training loss: 2.4374265415175467
Validation loss: 2.4734404707870907

Epoch: 6| Step: 12
Training loss: 2.9037794872138223
Validation loss: 2.47767854688097

Epoch: 6| Step: 13
Training loss: 1.9518295264222292
Validation loss: 2.4799490797291215

Epoch: 180| Step: 0
Training loss: 1.8725498084865624
Validation loss: 2.4765001162785625

Epoch: 6| Step: 1
Training loss: 2.432908845046033
Validation loss: 2.4890631660353573

Epoch: 6| Step: 2
Training loss: 2.0699846116014595
Validation loss: 2.4795198803347187

Epoch: 6| Step: 3
Training loss: 2.5836401931468793
Validation loss: 2.485656367848905

Epoch: 6| Step: 4
Training loss: 2.9630016690834333
Validation loss: 2.492021194378084

Epoch: 6| Step: 5
Training loss: 2.9098442576047994
Validation loss: 2.4713588563146334

Epoch: 6| Step: 6
Training loss: 2.5054766748515336
Validation loss: 2.4846060783355486

Epoch: 6| Step: 7
Training loss: 2.182587366092414
Validation loss: 2.4812210824403973

Epoch: 6| Step: 8
Training loss: 1.7851857656630654
Validation loss: 2.48121917667029

Epoch: 6| Step: 9
Training loss: 2.5248655642233953
Validation loss: 2.4769001278343437

Epoch: 6| Step: 10
Training loss: 2.097212021601489
Validation loss: 2.487716075643549

Epoch: 6| Step: 11
Training loss: 2.656002077986582
Validation loss: 2.487681940987264

Epoch: 6| Step: 12
Training loss: 2.644319015844373
Validation loss: 2.4912193514528425

Epoch: 6| Step: 13
Training loss: 2.183275994671469
Validation loss: 2.4845827763520716

Epoch: 181| Step: 0
Training loss: 2.462512090817894
Validation loss: 2.4758892407734066

Epoch: 6| Step: 1
Training loss: 2.1335024433859546
Validation loss: 2.477570498092515

Epoch: 6| Step: 2
Training loss: 2.6262665372506686
Validation loss: 2.4772454251791123

Epoch: 6| Step: 3
Training loss: 2.79391501875944
Validation loss: 2.4748837530406256

Epoch: 6| Step: 4
Training loss: 2.6924138058721176
Validation loss: 2.472526205378492

Epoch: 6| Step: 5
Training loss: 2.3845636655627525
Validation loss: 2.4600039856059137

Epoch: 6| Step: 6
Training loss: 2.331119531657273
Validation loss: 2.4778795881647278

Epoch: 6| Step: 7
Training loss: 2.4587146226270384
Validation loss: 2.4763533286280444

Epoch: 6| Step: 8
Training loss: 1.6168564904579763
Validation loss: 2.4831856493804776

Epoch: 6| Step: 9
Training loss: 2.7527587231300448
Validation loss: 2.4734926501560044

Epoch: 6| Step: 10
Training loss: 2.2777143278304464
Validation loss: 2.4840608474150714

Epoch: 6| Step: 11
Training loss: 2.709093769617764
Validation loss: 2.477454457074531

Epoch: 6| Step: 12
Training loss: 2.411335512882535
Validation loss: 2.480641773026526

Epoch: 6| Step: 13
Training loss: 1.8981540295614014
Validation loss: 2.4827540485790935

Epoch: 182| Step: 0
Training loss: 1.8980048043659532
Validation loss: 2.4831701271993496

Epoch: 6| Step: 1
Training loss: 2.398968963886603
Validation loss: 2.4752566066487205

Epoch: 6| Step: 2
Training loss: 2.6222077914377895
Validation loss: 2.472302428086282

Epoch: 6| Step: 3
Training loss: 1.9840160856287046
Validation loss: 2.4768489828386917

Epoch: 6| Step: 4
Training loss: 2.576562211295182
Validation loss: 2.4718659450206006

Epoch: 6| Step: 5
Training loss: 2.3131725261251304
Validation loss: 2.4790981240507763

Epoch: 6| Step: 6
Training loss: 2.666288726568316
Validation loss: 2.466181952518479

Epoch: 6| Step: 7
Training loss: 2.9509052633334307
Validation loss: 2.4607062014112735

Epoch: 6| Step: 8
Training loss: 2.083094341557203
Validation loss: 2.462181487357108

Epoch: 6| Step: 9
Training loss: 2.1269993072063857
Validation loss: 2.4761214801237523

Epoch: 6| Step: 10
Training loss: 2.7611773228611143
Validation loss: 2.4675217804467637

Epoch: 6| Step: 11
Training loss: 2.1625962781697177
Validation loss: 2.494583046830671

Epoch: 6| Step: 12
Training loss: 1.7813796949854062
Validation loss: 2.475744005883905

Epoch: 6| Step: 13
Training loss: 2.8941869286390736
Validation loss: 2.47806042478432

Epoch: 183| Step: 0
Training loss: 1.963205128236491
Validation loss: 2.482984349565452

Epoch: 6| Step: 1
Training loss: 1.9561044169602169
Validation loss: 2.4819829529251902

Epoch: 6| Step: 2
Training loss: 2.30040547487088
Validation loss: 2.4797864561280707

Epoch: 6| Step: 3
Training loss: 2.630866171779221
Validation loss: 2.4844233350230422

Epoch: 6| Step: 4
Training loss: 2.639660179346341
Validation loss: 2.4930977429640198

Epoch: 6| Step: 5
Training loss: 2.224760376960584
Validation loss: 2.4764277023389774

Epoch: 6| Step: 6
Training loss: 2.5363343591700103
Validation loss: 2.4872102054243834

Epoch: 6| Step: 7
Training loss: 2.8033166792197592
Validation loss: 2.4891802382974544

Epoch: 6| Step: 8
Training loss: 2.462852871005735
Validation loss: 2.4814219808351097

Epoch: 6| Step: 9
Training loss: 2.611866657931116
Validation loss: 2.4856354816699517

Epoch: 6| Step: 10
Training loss: 2.562614159250884
Validation loss: 2.4790592544324763

Epoch: 6| Step: 11
Training loss: 2.123487607520518
Validation loss: 2.4764345619376718

Epoch: 6| Step: 12
Training loss: 2.2426353299782384
Validation loss: 2.474258458186556

Epoch: 6| Step: 13
Training loss: 2.4587196649988883
Validation loss: 2.471684799882356

Epoch: 184| Step: 0
Training loss: 3.0331099005212683
Validation loss: 2.472715066749618

Epoch: 6| Step: 1
Training loss: 2.5511908910577366
Validation loss: 2.477296257225452

Epoch: 6| Step: 2
Training loss: 2.803950305846641
Validation loss: 2.4721596587285855

Epoch: 6| Step: 3
Training loss: 2.6961691559267207
Validation loss: 2.474951588192522

Epoch: 6| Step: 4
Training loss: 2.4677017497378366
Validation loss: 2.4715334737117263

Epoch: 6| Step: 5
Training loss: 2.1505108071263868
Validation loss: 2.4871931666363056

Epoch: 6| Step: 6
Training loss: 2.339576667737651
Validation loss: 2.486329339629678

Epoch: 6| Step: 7
Training loss: 2.310964744876691
Validation loss: 2.4750146210363835

Epoch: 6| Step: 8
Training loss: 2.193195122863979
Validation loss: 2.4777024590753443

Epoch: 6| Step: 9
Training loss: 1.6604545056073203
Validation loss: 2.4758971530936713

Epoch: 6| Step: 10
Training loss: 2.3386093730230075
Validation loss: 2.4865785339306425

Epoch: 6| Step: 11
Training loss: 2.6426543614569042
Validation loss: 2.481079074535862

Epoch: 6| Step: 12
Training loss: 2.5770075370314203
Validation loss: 2.4840298298904386

Epoch: 6| Step: 13
Training loss: 2.1114197039791303
Validation loss: 2.4772621714780954

Epoch: 185| Step: 0
Training loss: 2.465290494596489
Validation loss: 2.482100735353032

Epoch: 6| Step: 1
Training loss: 2.01502461326087
Validation loss: 2.4791828977764094

Epoch: 6| Step: 2
Training loss: 1.6651874733390701
Validation loss: 2.4663545277855143

Epoch: 6| Step: 3
Training loss: 2.007094199090675
Validation loss: 2.4622265704271884

Epoch: 6| Step: 4
Training loss: 2.779373907762831
Validation loss: 2.473969129572283

Epoch: 6| Step: 5
Training loss: 2.1887944342581496
Validation loss: 2.4652472486680166

Epoch: 6| Step: 6
Training loss: 2.4352901907767497
Validation loss: 2.474627504028938

Epoch: 6| Step: 7
Training loss: 2.6973580821748753
Validation loss: 2.46960119067741

Epoch: 6| Step: 8
Training loss: 2.220037811919549
Validation loss: 2.4724230744236855

Epoch: 6| Step: 9
Training loss: 2.361030250610804
Validation loss: 2.476376451309054

Epoch: 6| Step: 10
Training loss: 2.5053967400418258
Validation loss: 2.479512885038478

Epoch: 6| Step: 11
Training loss: 2.609625341775803
Validation loss: 2.4829223032366343

Epoch: 6| Step: 12
Training loss: 2.470056695017165
Validation loss: 2.479466705972599

Epoch: 6| Step: 13
Training loss: 2.977213150158969
Validation loss: 2.4821330096763994

Epoch: 186| Step: 0
Training loss: 2.061118616991294
Validation loss: 2.490091397039218

Epoch: 6| Step: 1
Training loss: 2.8577127263008806
Validation loss: 2.493437769312481

Epoch: 6| Step: 2
Training loss: 2.4955902308127778
Validation loss: 2.5043706200471485

Epoch: 6| Step: 3
Training loss: 2.001237129012778
Validation loss: 2.5047288992370205

Epoch: 6| Step: 4
Training loss: 2.379048360454111
Validation loss: 2.5073565012070995

Epoch: 6| Step: 5
Training loss: 2.240463817124643
Validation loss: 2.516717787932866

Epoch: 6| Step: 6
Training loss: 2.829647602745808
Validation loss: 2.506400149343344

Epoch: 6| Step: 7
Training loss: 2.0786054242312453
Validation loss: 2.5127264189669565

Epoch: 6| Step: 8
Training loss: 2.40749317108802
Validation loss: 2.5156841863457937

Epoch: 6| Step: 9
Training loss: 1.747279709559437
Validation loss: 2.4932875801247203

Epoch: 6| Step: 10
Training loss: 2.972065569907746
Validation loss: 2.4881882902194614

Epoch: 6| Step: 11
Training loss: 2.456656859579745
Validation loss: 2.471526118179664

Epoch: 6| Step: 12
Training loss: 2.150831740313589
Validation loss: 2.462940817555188

Epoch: 6| Step: 13
Training loss: 2.7347211346210543
Validation loss: 2.4790716767478855

Epoch: 187| Step: 0
Training loss: 1.58012158083012
Validation loss: 2.458388252506932

Epoch: 6| Step: 1
Training loss: 2.8403264492085643
Validation loss: 2.4697293459654226

Epoch: 6| Step: 2
Training loss: 2.5017850225289533
Validation loss: 2.4761350886644427

Epoch: 6| Step: 3
Training loss: 2.289488977276172
Validation loss: 2.4846673949335845

Epoch: 6| Step: 4
Training loss: 1.6811422036934611
Validation loss: 2.4857697000862045

Epoch: 6| Step: 5
Training loss: 2.9197958009989757
Validation loss: 2.4851123667230968

Epoch: 6| Step: 6
Training loss: 2.988911959839954
Validation loss: 2.4789633039738885

Epoch: 6| Step: 7
Training loss: 2.604230549346657
Validation loss: 2.483374228130089

Epoch: 6| Step: 8
Training loss: 2.1340224638152514
Validation loss: 2.4834936885990344

Epoch: 6| Step: 9
Training loss: 2.377185318074404
Validation loss: 2.48138684680915

Epoch: 6| Step: 10
Training loss: 2.4246160930861897
Validation loss: 2.479069496836394

Epoch: 6| Step: 11
Training loss: 2.3267485370641636
Validation loss: 2.476551589380212

Epoch: 6| Step: 12
Training loss: 2.047341330019605
Validation loss: 2.4796596058408555

Epoch: 6| Step: 13
Training loss: 2.7323503246305716
Validation loss: 2.4744082526952926

Epoch: 188| Step: 0
Training loss: 2.4556982066096396
Validation loss: 2.4802872078209885

Epoch: 6| Step: 1
Training loss: 2.2264950390684866
Validation loss: 2.4800434699145524

Epoch: 6| Step: 2
Training loss: 2.7958213483740204
Validation loss: 2.481673317134529

Epoch: 6| Step: 3
Training loss: 1.9679986034520165
Validation loss: 2.4880346698042186

Epoch: 6| Step: 4
Training loss: 2.2535187439377036
Validation loss: 2.4767220944106128

Epoch: 6| Step: 5
Training loss: 2.8771415695347735
Validation loss: 2.4822218979734076

Epoch: 6| Step: 6
Training loss: 1.489731129959058
Validation loss: 2.4867003173017133

Epoch: 6| Step: 7
Training loss: 2.094927783895848
Validation loss: 2.486478159239064

Epoch: 6| Step: 8
Training loss: 2.6378488789004306
Validation loss: 2.473909097817361

Epoch: 6| Step: 9
Training loss: 2.9008739338478886
Validation loss: 2.4769110048333722

Epoch: 6| Step: 10
Training loss: 2.4313714671253184
Validation loss: 2.477592518808281

Epoch: 6| Step: 11
Training loss: 1.9018893159295567
Validation loss: 2.48125656312832

Epoch: 6| Step: 12
Training loss: 2.451812004001593
Validation loss: 2.4894853649788233

Epoch: 6| Step: 13
Training loss: 2.8287984983945114
Validation loss: 2.490245991598722

Epoch: 189| Step: 0
Training loss: 2.45910702400267
Validation loss: 2.484913653633175

Epoch: 6| Step: 1
Training loss: 1.8110001376282057
Validation loss: 2.486644404118198

Epoch: 6| Step: 2
Training loss: 2.4212387725932136
Validation loss: 2.4963964717492266

Epoch: 6| Step: 3
Training loss: 2.1184647989884615
Validation loss: 2.48279374061317

Epoch: 6| Step: 4
Training loss: 2.3090769202437356
Validation loss: 2.4850714485084926

Epoch: 6| Step: 5
Training loss: 2.3728455257943417
Validation loss: 2.488829675558542

Epoch: 6| Step: 6
Training loss: 2.20799906617988
Validation loss: 2.488645342304844

Epoch: 6| Step: 7
Training loss: 1.9285135134321663
Validation loss: 2.482249056095135

Epoch: 6| Step: 8
Training loss: 2.7382492085322228
Validation loss: 2.4896389836644857

Epoch: 6| Step: 9
Training loss: 2.546257459042485
Validation loss: 2.47035118682378

Epoch: 6| Step: 10
Training loss: 2.8370991158986
Validation loss: 2.4878269900765004

Epoch: 6| Step: 11
Training loss: 2.112107943659065
Validation loss: 2.4862146264256535

Epoch: 6| Step: 12
Training loss: 2.604903724633948
Validation loss: 2.482677527588642

Epoch: 6| Step: 13
Training loss: 2.758325285962247
Validation loss: 2.481647441687925

Epoch: 190| Step: 0
Training loss: 2.986362614254732
Validation loss: 2.4856420360963263

Epoch: 6| Step: 1
Training loss: 2.212275878416917
Validation loss: 2.490949715985687

Epoch: 6| Step: 2
Training loss: 2.581502286021528
Validation loss: 2.4949562294560095

Epoch: 6| Step: 3
Training loss: 2.144150330112737
Validation loss: 2.497457307941344

Epoch: 6| Step: 4
Training loss: 2.4117719035814282
Validation loss: 2.4954867791592683

Epoch: 6| Step: 5
Training loss: 1.9931632849819343
Validation loss: 2.487136361844615

Epoch: 6| Step: 6
Training loss: 3.0574707464033577
Validation loss: 2.4919409236283756

Epoch: 6| Step: 7
Training loss: 2.480614653373779
Validation loss: 2.491226680740264

Epoch: 6| Step: 8
Training loss: 1.8875085666285252
Validation loss: 2.488453314644427

Epoch: 6| Step: 9
Training loss: 2.1708779001572975
Validation loss: 2.478071537218814

Epoch: 6| Step: 10
Training loss: 2.6700726057031785
Validation loss: 2.4812926998996465

Epoch: 6| Step: 11
Training loss: 2.2636066786138196
Validation loss: 2.4830774241531395

Epoch: 6| Step: 12
Training loss: 2.3055747183293187
Validation loss: 2.4796306485808217

Epoch: 6| Step: 13
Training loss: 2.347757600700709
Validation loss: 2.484053552972733

Epoch: 191| Step: 0
Training loss: 1.9564415810469369
Validation loss: 2.4760989167417167

Epoch: 6| Step: 1
Training loss: 2.216635004931274
Validation loss: 2.4841738445490096

Epoch: 6| Step: 2
Training loss: 2.5905828787735077
Validation loss: 2.4757949811065725

Epoch: 6| Step: 3
Training loss: 1.945945177260668
Validation loss: 2.4775925027699572

Epoch: 6| Step: 4
Training loss: 2.3614432257152336
Validation loss: 2.4766513716258354

Epoch: 6| Step: 5
Training loss: 2.5192804729242284
Validation loss: 2.482589191807812

Epoch: 6| Step: 6
Training loss: 2.7631254808537227
Validation loss: 2.4704594870197774

Epoch: 6| Step: 7
Training loss: 2.216224199845199
Validation loss: 2.474268865002508

Epoch: 6| Step: 8
Training loss: 2.4679426007698355
Validation loss: 2.481502159495365

Epoch: 6| Step: 9
Training loss: 2.2636499675385124
Validation loss: 2.4739935514386078

Epoch: 6| Step: 10
Training loss: 2.726267189276025
Validation loss: 2.4950641225806978

Epoch: 6| Step: 11
Training loss: 2.0362527130464825
Validation loss: 2.4897500199150255

Epoch: 6| Step: 12
Training loss: 2.632565631102668
Validation loss: 2.4738591759870974

Epoch: 6| Step: 13
Training loss: 2.817363856630338
Validation loss: 2.501484922643323

Epoch: 192| Step: 0
Training loss: 2.5566471528158643
Validation loss: 2.5061449189009157

Epoch: 6| Step: 1
Training loss: 2.6537091837550326
Validation loss: 2.5153027914558694

Epoch: 6| Step: 2
Training loss: 2.9404345524702364
Validation loss: 2.525865078026738

Epoch: 6| Step: 3
Training loss: 2.4894592275504306
Validation loss: 2.5198137859263583

Epoch: 6| Step: 4
Training loss: 2.5129437106986
Validation loss: 2.5132953605852593

Epoch: 6| Step: 5
Training loss: 2.5629366991052676
Validation loss: 2.5118281062294536

Epoch: 6| Step: 6
Training loss: 2.3849125847301402
Validation loss: 2.517697761796488

Epoch: 6| Step: 7
Training loss: 2.7360011034009615
Validation loss: 2.5098432674841695

Epoch: 6| Step: 8
Training loss: 2.3709867600964656
Validation loss: 2.499340494904527

Epoch: 6| Step: 9
Training loss: 2.824801014331602
Validation loss: 2.4900956258578795

Epoch: 6| Step: 10
Training loss: 2.139691225753132
Validation loss: 2.4649879909091306

Epoch: 6| Step: 11
Training loss: 1.810561491774534
Validation loss: 2.4852307203452106

Epoch: 6| Step: 12
Training loss: 1.669312340214894
Validation loss: 2.469223154172651

Epoch: 6| Step: 13
Training loss: 1.2326778865370294
Validation loss: 2.471345912864631

Epoch: 193| Step: 0
Training loss: 2.2569553107627702
Validation loss: 2.471453646931121

Epoch: 6| Step: 1
Training loss: 2.2784867992307523
Validation loss: 2.4849765137369118

Epoch: 6| Step: 2
Training loss: 3.1349414621634333
Validation loss: 2.4818259863847714

Epoch: 6| Step: 3
Training loss: 2.1157971986817894
Validation loss: 2.487835862698073

Epoch: 6| Step: 4
Training loss: 2.1849385114524678
Validation loss: 2.483388756980534

Epoch: 6| Step: 5
Training loss: 2.835938340704507
Validation loss: 2.4867042163164896

Epoch: 6| Step: 6
Training loss: 2.7352131458195292
Validation loss: 2.4822897967226

Epoch: 6| Step: 7
Training loss: 2.4147515656059095
Validation loss: 2.4873673274251953

Epoch: 6| Step: 8
Training loss: 2.544580186344606
Validation loss: 2.468977084761524

Epoch: 6| Step: 9
Training loss: 2.7104058087830523
Validation loss: 2.4814729835381617

Epoch: 6| Step: 10
Training loss: 2.690665310547748
Validation loss: 2.4667105801857563

Epoch: 6| Step: 11
Training loss: 1.9276361102226862
Validation loss: 2.460200850420756

Epoch: 6| Step: 12
Training loss: 2.209581478242085
Validation loss: 2.471853599021661

Epoch: 6| Step: 13
Training loss: 1.8287574782515215
Validation loss: 2.472601160256342

Epoch: 194| Step: 0
Training loss: 2.5455033991200327
Validation loss: 2.486908410681889

Epoch: 6| Step: 1
Training loss: 2.5700329103848896
Validation loss: 2.487221500646658

Epoch: 6| Step: 2
Training loss: 1.9705543726572872
Validation loss: 2.4949726737239004

Epoch: 6| Step: 3
Training loss: 3.064887848088785
Validation loss: 2.4920745154333592

Epoch: 6| Step: 4
Training loss: 2.6873332127115495
Validation loss: 2.4884910475089734

Epoch: 6| Step: 5
Training loss: 2.615761257254961
Validation loss: 2.49171497485261

Epoch: 6| Step: 6
Training loss: 2.460891480999539
Validation loss: 2.4816367135461674

Epoch: 6| Step: 7
Training loss: 1.876045190051334
Validation loss: 2.484588973709435

Epoch: 6| Step: 8
Training loss: 2.5743730466964436
Validation loss: 2.479115442874128

Epoch: 6| Step: 9
Training loss: 2.3336818866158047
Validation loss: 2.4718331669009657

Epoch: 6| Step: 10
Training loss: 2.113078165784881
Validation loss: 2.469548526821346

Epoch: 6| Step: 11
Training loss: 2.0802370968814934
Validation loss: 2.4685307336966984

Epoch: 6| Step: 12
Training loss: 2.156069319521869
Validation loss: 2.4837742684978052

Epoch: 6| Step: 13
Training loss: 2.3981655103310544
Validation loss: 2.4857413575202085

Epoch: 195| Step: 0
Training loss: 2.335056610891504
Validation loss: 2.478524731358041

Epoch: 6| Step: 1
Training loss: 2.4288838029365927
Validation loss: 2.473187598920108

Epoch: 6| Step: 2
Training loss: 2.07575038532344
Validation loss: 2.467254636469948

Epoch: 6| Step: 3
Training loss: 2.3093791213147217
Validation loss: 2.477816548018016

Epoch: 6| Step: 4
Training loss: 1.8194152192283424
Validation loss: 2.476667897321242

Epoch: 6| Step: 5
Training loss: 2.2390463488382935
Validation loss: 2.480972214914913

Epoch: 6| Step: 6
Training loss: 2.688188331636691
Validation loss: 2.489744944627358

Epoch: 6| Step: 7
Training loss: 2.3294931782272355
Validation loss: 2.4876153155615417

Epoch: 6| Step: 8
Training loss: 1.9037037086995332
Validation loss: 2.4791336591644337

Epoch: 6| Step: 9
Training loss: 2.7488284649997126
Validation loss: 2.4747422084067185

Epoch: 6| Step: 10
Training loss: 2.696644506406639
Validation loss: 2.4831405387729566

Epoch: 6| Step: 11
Training loss: 2.93020958889689
Validation loss: 2.4768280624984875

Epoch: 6| Step: 12
Training loss: 2.525943232450407
Validation loss: 2.469073568136725

Epoch: 6| Step: 13
Training loss: 2.2363219415171063
Validation loss: 2.479691863862404

Epoch: 196| Step: 0
Training loss: 2.2117260716194824
Validation loss: 2.4724565518749615

Epoch: 6| Step: 1
Training loss: 2.1813366848135636
Validation loss: 2.4745365203353598

Epoch: 6| Step: 2
Training loss: 1.878271808694153
Validation loss: 2.492128329635916

Epoch: 6| Step: 3
Training loss: 2.915104656663592
Validation loss: 2.5043858997605044

Epoch: 6| Step: 4
Training loss: 2.3328111836352408
Validation loss: 2.4908810083908954

Epoch: 6| Step: 5
Training loss: 2.8094299514708814
Validation loss: 2.4893845807362904

Epoch: 6| Step: 6
Training loss: 2.535012545281843
Validation loss: 2.4899068220544773

Epoch: 6| Step: 7
Training loss: 2.296391858000442
Validation loss: 2.4953263465721705

Epoch: 6| Step: 8
Training loss: 2.7558485608121894
Validation loss: 2.494020989400831

Epoch: 6| Step: 9
Training loss: 2.166999228948677
Validation loss: 2.4929822493613747

Epoch: 6| Step: 10
Training loss: 2.6817662486792044
Validation loss: 2.495941237047285

Epoch: 6| Step: 11
Training loss: 2.6963273501276936
Validation loss: 2.4754036038057343

Epoch: 6| Step: 12
Training loss: 1.5842075778602234
Validation loss: 2.472857297866534

Epoch: 6| Step: 13
Training loss: 2.0000181197299307
Validation loss: 2.4763648819724002

Epoch: 197| Step: 0
Training loss: 2.953353953694962
Validation loss: 2.47734383212446

Epoch: 6| Step: 1
Training loss: 2.5180811294441825
Validation loss: 2.479127648524708

Epoch: 6| Step: 2
Training loss: 2.762018470741071
Validation loss: 2.482255283291308

Epoch: 6| Step: 3
Training loss: 2.3182765878423646
Validation loss: 2.4748394063102275

Epoch: 6| Step: 4
Training loss: 2.2080858799707985
Validation loss: 2.486902522691937

Epoch: 6| Step: 5
Training loss: 2.181231208436135
Validation loss: 2.4804604677878053

Epoch: 6| Step: 6
Training loss: 2.136093913046143
Validation loss: 2.469239970970971

Epoch: 6| Step: 7
Training loss: 1.41525210085957
Validation loss: 2.493952302686186

Epoch: 6| Step: 8
Training loss: 2.001011711768821
Validation loss: 2.494526832450016

Epoch: 6| Step: 9
Training loss: 2.059393074292069
Validation loss: 2.4856288073277732

Epoch: 6| Step: 10
Training loss: 2.3754625371967033
Validation loss: 2.4890868970206528

Epoch: 6| Step: 11
Training loss: 2.331318348555077
Validation loss: 2.492101829276059

Epoch: 6| Step: 12
Training loss: 2.7882737185012285
Validation loss: 2.4894847344914144

Epoch: 6| Step: 13
Training loss: 2.925313629551162
Validation loss: 2.477883068075238

Epoch: 198| Step: 0
Training loss: 2.2995981694695775
Validation loss: 2.4905729734314845

Epoch: 6| Step: 1
Training loss: 2.5551962622943023
Validation loss: 2.497172353939887

Epoch: 6| Step: 2
Training loss: 1.6755494711051804
Validation loss: 2.5032036440746683

Epoch: 6| Step: 3
Training loss: 2.7681505988171837
Validation loss: 2.4913377259780316

Epoch: 6| Step: 4
Training loss: 2.266452191060467
Validation loss: 2.4816581377592084

Epoch: 6| Step: 5
Training loss: 1.8231156013568215
Validation loss: 2.5083693126831275

Epoch: 6| Step: 6
Training loss: 2.541927845360782
Validation loss: 2.515659466303988

Epoch: 6| Step: 7
Training loss: 2.1328922019291197
Validation loss: 2.5268880206236606

Epoch: 6| Step: 8
Training loss: 2.391471962086693
Validation loss: 2.506492576370493

Epoch: 6| Step: 9
Training loss: 2.6024822636860168
Validation loss: 2.4901582753639833

Epoch: 6| Step: 10
Training loss: 2.7463731257358606
Validation loss: 2.5015824872500514

Epoch: 6| Step: 11
Training loss: 2.935527545106435
Validation loss: 2.487470413875511

Epoch: 6| Step: 12
Training loss: 2.218475432264417
Validation loss: 2.4789882777384307

Epoch: 6| Step: 13
Training loss: 1.997859405344062
Validation loss: 2.4817416151687812

Epoch: 199| Step: 0
Training loss: 2.463566033294588
Validation loss: 2.4827157163551563

Epoch: 6| Step: 1
Training loss: 2.3013232488127615
Validation loss: 2.487117101762058

Epoch: 6| Step: 2
Training loss: 2.5971119245839054
Validation loss: 2.4881882263393593

Epoch: 6| Step: 3
Training loss: 2.8096636563395574
Validation loss: 2.4866522182968693

Epoch: 6| Step: 4
Training loss: 2.0193545118430127
Validation loss: 2.4830243739861038

Epoch: 6| Step: 5
Training loss: 2.65472395199991
Validation loss: 2.475728581499174

Epoch: 6| Step: 6
Training loss: 1.9683694623085692
Validation loss: 2.4815758024412475

Epoch: 6| Step: 7
Training loss: 2.1523341128484907
Validation loss: 2.4757259813332815

Epoch: 6| Step: 8
Training loss: 1.7177242339234733
Validation loss: 2.474487879780662

Epoch: 6| Step: 9
Training loss: 2.467406378055012
Validation loss: 2.480798141756113

Epoch: 6| Step: 10
Training loss: 2.2525787410881435
Validation loss: 2.4814915748436257

Epoch: 6| Step: 11
Training loss: 2.354495225332445
Validation loss: 2.4685123345299536

Epoch: 6| Step: 12
Training loss: 2.784746265598255
Validation loss: 2.4851015016356786

Epoch: 6| Step: 13
Training loss: 2.344979129202229
Validation loss: 2.4841617676657717

Epoch: 200| Step: 0
Training loss: 1.9435124169284972
Validation loss: 2.4841702134922747

Epoch: 6| Step: 1
Training loss: 2.4327494961635456
Validation loss: 2.498742693403285

Epoch: 6| Step: 2
Training loss: 2.4097071001920174
Validation loss: 2.501582248982283

Epoch: 6| Step: 3
Training loss: 2.0371238876220827
Validation loss: 2.4834543597622436

Epoch: 6| Step: 4
Training loss: 2.971641176734526
Validation loss: 2.502434879945758

Epoch: 6| Step: 5
Training loss: 2.255157493623211
Validation loss: 2.50881654282078

Epoch: 6| Step: 6
Training loss: 3.0530227066130258
Validation loss: 2.5162223438423985

Epoch: 6| Step: 7
Training loss: 2.9383459293515983
Validation loss: 2.5353973697209073

Epoch: 6| Step: 8
Training loss: 1.934729163759949
Validation loss: 2.558070797657056

Epoch: 6| Step: 9
Training loss: 2.2579026781704643
Validation loss: 2.5265366741080157

Epoch: 6| Step: 10
Training loss: 2.050887622799289
Validation loss: 2.5254137707343145

Epoch: 6| Step: 11
Training loss: 2.6324020654442726
Validation loss: 2.512265064395909

Epoch: 6| Step: 12
Training loss: 2.223099541999619
Validation loss: 2.490782960790989

Epoch: 6| Step: 13
Training loss: 2.046340989202874
Validation loss: 2.4840807790496005

Epoch: 201| Step: 0
Training loss: 2.173956772115671
Validation loss: 2.483629030719251

Epoch: 6| Step: 1
Training loss: 2.1139950489863195
Validation loss: 2.478305431979373

Epoch: 6| Step: 2
Training loss: 2.0106430585084034
Validation loss: 2.474134906022313

Epoch: 6| Step: 3
Training loss: 2.684261588741759
Validation loss: 2.475959359420414

Epoch: 6| Step: 4
Training loss: 1.3815044962629999
Validation loss: 2.4706396604888354

Epoch: 6| Step: 5
Training loss: 2.328915045925397
Validation loss: 2.4625313416372876

Epoch: 6| Step: 6
Training loss: 2.9297727852169775
Validation loss: 2.4672223285763883

Epoch: 6| Step: 7
Training loss: 3.155131226725971
Validation loss: 2.467904746946572

Epoch: 6| Step: 8
Training loss: 2.5789739887039613
Validation loss: 2.4763967897830295

Epoch: 6| Step: 9
Training loss: 2.4610858206481665
Validation loss: 2.4713861900856906

Epoch: 6| Step: 10
Training loss: 2.6432340474702216
Validation loss: 2.478361453127905

Epoch: 6| Step: 11
Training loss: 2.3126235104131254
Validation loss: 2.4738793665011647

Epoch: 6| Step: 12
Training loss: 2.6726416664651174
Validation loss: 2.453918539261592

Epoch: 6| Step: 13
Training loss: 2.3455908094923443
Validation loss: 2.4656472324775933

Epoch: 202| Step: 0
Training loss: 2.856855105487501
Validation loss: 2.485796188024107

Epoch: 6| Step: 1
Training loss: 2.303055731923503
Validation loss: 2.488938848054622

Epoch: 6| Step: 2
Training loss: 2.7725897554500136
Validation loss: 2.4942415755196774

Epoch: 6| Step: 3
Training loss: 2.6008118242349956
Validation loss: 2.4934296257971056

Epoch: 6| Step: 4
Training loss: 2.245258633810955
Validation loss: 2.4896185059769573

Epoch: 6| Step: 5
Training loss: 2.6260087935859007
Validation loss: 2.4970980331757997

Epoch: 6| Step: 6
Training loss: 1.6191517423425252
Validation loss: 2.498094197874627

Epoch: 6| Step: 7
Training loss: 1.7039800474897122
Validation loss: 2.490761662925424

Epoch: 6| Step: 8
Training loss: 1.9318543905036938
Validation loss: 2.4965140357513986

Epoch: 6| Step: 9
Training loss: 2.423824092053202
Validation loss: 2.4993499705511404

Epoch: 6| Step: 10
Training loss: 2.5806363907403496
Validation loss: 2.4899827378733015

Epoch: 6| Step: 11
Training loss: 2.1417282491456464
Validation loss: 2.4981072095196613

Epoch: 6| Step: 12
Training loss: 2.572737271828341
Validation loss: 2.501825826217078

Epoch: 6| Step: 13
Training loss: 2.4863887279043877
Validation loss: 2.508530352597341

Epoch: 203| Step: 0
Training loss: 2.2955225900956417
Validation loss: 2.5331052801941305

Epoch: 6| Step: 1
Training loss: 2.3684437254327984
Validation loss: 2.5158554192763765

Epoch: 6| Step: 2
Training loss: 2.3825467321049683
Validation loss: 2.5080188892085142

Epoch: 6| Step: 3
Training loss: 2.238411198916448
Validation loss: 2.5281725256201035

Epoch: 6| Step: 4
Training loss: 2.9746084131851696
Validation loss: 2.5054883000787007

Epoch: 6| Step: 5
Training loss: 2.4656069098559574
Validation loss: 2.49842711878519

Epoch: 6| Step: 6
Training loss: 2.2772585295080763
Validation loss: 2.4853988473087685

Epoch: 6| Step: 7
Training loss: 2.0673055440656114
Validation loss: 2.4906061829893567

Epoch: 6| Step: 8
Training loss: 2.1152817147576553
Validation loss: 2.495921941425087

Epoch: 6| Step: 9
Training loss: 2.154983590897753
Validation loss: 2.492787766190635

Epoch: 6| Step: 10
Training loss: 2.3503940251943787
Validation loss: 2.491694801282297

Epoch: 6| Step: 11
Training loss: 2.6969176889093136
Validation loss: 2.4788762304470233

Epoch: 6| Step: 12
Training loss: 2.5544108669748757
Validation loss: 2.486869870858289

Epoch: 6| Step: 13
Training loss: 2.2947324763932966
Validation loss: 2.482373789139737

Epoch: 204| Step: 0
Training loss: 2.5803210528909832
Validation loss: 2.4900824287283285

Epoch: 6| Step: 1
Training loss: 2.27502721413904
Validation loss: 2.4907178940434895

Epoch: 6| Step: 2
Training loss: 2.607004516574238
Validation loss: 2.492679718741789

Epoch: 6| Step: 3
Training loss: 2.6334206985113298
Validation loss: 2.493860279280403

Epoch: 6| Step: 4
Training loss: 2.5781947270992323
Validation loss: 2.5010024445135532

Epoch: 6| Step: 5
Training loss: 2.440616083067798
Validation loss: 2.5042048378910993

Epoch: 6| Step: 6
Training loss: 1.7539526034816362
Validation loss: 2.487369140620655

Epoch: 6| Step: 7
Training loss: 2.3540707008809427
Validation loss: 2.4934418968375147

Epoch: 6| Step: 8
Training loss: 2.0665688117251193
Validation loss: 2.4823086220283197

Epoch: 6| Step: 9
Training loss: 2.1577017499496636
Validation loss: 2.4799536783496685

Epoch: 6| Step: 10
Training loss: 1.9949675665874471
Validation loss: 2.4900970780165936

Epoch: 6| Step: 11
Training loss: 2.2440471314293227
Validation loss: 2.484605526575254

Epoch: 6| Step: 12
Training loss: 2.061321152846426
Validation loss: 2.497883679763651

Epoch: 6| Step: 13
Training loss: 3.112814223273024
Validation loss: 2.497414929287445

Epoch: 205| Step: 0
Training loss: 1.8471948156339404
Validation loss: 2.5002591475639915

Epoch: 6| Step: 1
Training loss: 2.3332151541981783
Validation loss: 2.4826821691716736

Epoch: 6| Step: 2
Training loss: 2.8331480152635162
Validation loss: 2.487187103579479

Epoch: 6| Step: 3
Training loss: 2.2246177344964755
Validation loss: 2.487426738842289

Epoch: 6| Step: 4
Training loss: 2.342984087093433
Validation loss: 2.489704150429944

Epoch: 6| Step: 5
Training loss: 2.4685590646039306
Validation loss: 2.4907584801950415

Epoch: 6| Step: 6
Training loss: 2.4264667912145814
Validation loss: 2.494504116956951

Epoch: 6| Step: 7
Training loss: 2.0737638675163943
Validation loss: 2.4948029859653724

Epoch: 6| Step: 8
Training loss: 2.269723486842852
Validation loss: 2.48110329028511

Epoch: 6| Step: 9
Training loss: 2.4416525266409326
Validation loss: 2.4945189871856472

Epoch: 6| Step: 10
Training loss: 2.501833815339299
Validation loss: 2.4957935787552303

Epoch: 6| Step: 11
Training loss: 2.027269659728176
Validation loss: 2.4916657606227424

Epoch: 6| Step: 12
Training loss: 2.8143950965294295
Validation loss: 2.4888997329842764

Epoch: 6| Step: 13
Training loss: 2.1566754349889474
Validation loss: 2.4867968320996856

Epoch: 206| Step: 0
Training loss: 2.8657159589103243
Validation loss: 2.4896207085734114

Epoch: 6| Step: 1
Training loss: 2.1008417531497567
Validation loss: 2.488118100968453

Epoch: 6| Step: 2
Training loss: 2.7501671480186465
Validation loss: 2.490411538198636

Epoch: 6| Step: 3
Training loss: 2.7231527174124985
Validation loss: 2.4944362559770603

Epoch: 6| Step: 4
Training loss: 1.9476085425944218
Validation loss: 2.503684698133462

Epoch: 6| Step: 5
Training loss: 1.6592999572452274
Validation loss: 2.492600393821034

Epoch: 6| Step: 6
Training loss: 2.3062353076828646
Validation loss: 2.4981649657714127

Epoch: 6| Step: 7
Training loss: 1.5136043804256532
Validation loss: 2.499546295958214

Epoch: 6| Step: 8
Training loss: 3.088385969915341
Validation loss: 2.5099660592994977

Epoch: 6| Step: 9
Training loss: 2.1266633423965606
Validation loss: 2.504583274178342

Epoch: 6| Step: 10
Training loss: 2.597126061965087
Validation loss: 2.5236144894474837

Epoch: 6| Step: 11
Training loss: 2.073389839596239
Validation loss: 2.49457979729485

Epoch: 6| Step: 12
Training loss: 2.27118542476007
Validation loss: 2.507185860654354

Epoch: 6| Step: 13
Training loss: 2.35757913515316
Validation loss: 2.494985701648052

Epoch: 207| Step: 0
Training loss: 2.1788400622874566
Validation loss: 2.4830238058710923

Epoch: 6| Step: 1
Training loss: 1.9320279033059622
Validation loss: 2.4776679298568385

Epoch: 6| Step: 2
Training loss: 2.6232174770295376
Validation loss: 2.477788900294332

Epoch: 6| Step: 3
Training loss: 3.567730059406742
Validation loss: 2.486780006196722

Epoch: 6| Step: 4
Training loss: 1.772934560838609
Validation loss: 2.4821736561562324

Epoch: 6| Step: 5
Training loss: 2.0745487273941685
Validation loss: 2.4929106250196447

Epoch: 6| Step: 6
Training loss: 1.9300743055122473
Validation loss: 2.474557684860572

Epoch: 6| Step: 7
Training loss: 2.177350503933618
Validation loss: 2.4857996728374143

Epoch: 6| Step: 8
Training loss: 1.9693046272994814
Validation loss: 2.4899138280468334

Epoch: 6| Step: 9
Training loss: 1.922275966592438
Validation loss: 2.4862216827789934

Epoch: 6| Step: 10
Training loss: 2.7431740575589982
Validation loss: 2.4751369733830186

Epoch: 6| Step: 11
Training loss: 1.813183688424016
Validation loss: 2.4882806271922053

Epoch: 6| Step: 12
Training loss: 2.7050452495463926
Validation loss: 2.4857442189670875

Epoch: 6| Step: 13
Training loss: 2.786128961698837
Validation loss: 2.4919455399841155

Epoch: 208| Step: 0
Training loss: 2.6789284568082024
Validation loss: 2.480975346133831

Epoch: 6| Step: 1
Training loss: 2.104054728133447
Validation loss: 2.4792500302913965

Epoch: 6| Step: 2
Training loss: 1.774538614187758
Validation loss: 2.4797450173719535

Epoch: 6| Step: 3
Training loss: 2.3637144085863855
Validation loss: 2.479196056758087

Epoch: 6| Step: 4
Training loss: 2.136805670328447
Validation loss: 2.4832067401886273

Epoch: 6| Step: 5
Training loss: 2.201691722664731
Validation loss: 2.4828760995595323

Epoch: 6| Step: 6
Training loss: 2.8258703528360245
Validation loss: 2.483080656736033

Epoch: 6| Step: 7
Training loss: 1.6273831718457064
Validation loss: 2.4806754198858805

Epoch: 6| Step: 8
Training loss: 2.1317878469581526
Validation loss: 2.4602193925042433

Epoch: 6| Step: 9
Training loss: 2.505427477181151
Validation loss: 2.4748422482498005

Epoch: 6| Step: 10
Training loss: 2.224759841130725
Validation loss: 2.467665663533168

Epoch: 6| Step: 11
Training loss: 2.446991268041049
Validation loss: 2.476453889055667

Epoch: 6| Step: 12
Training loss: 2.53296452606698
Validation loss: 2.4748269627510715

Epoch: 6| Step: 13
Training loss: 2.8351370362224393
Validation loss: 2.4831537567939206

Epoch: 209| Step: 0
Training loss: 2.7208373727422837
Validation loss: 2.4690344602338676

Epoch: 6| Step: 1
Training loss: 2.47550638586433
Validation loss: 2.483869649164654

Epoch: 6| Step: 2
Training loss: 2.0824253646829933
Validation loss: 2.4823417740691096

Epoch: 6| Step: 3
Training loss: 2.663663325230999
Validation loss: 2.4783974076483655

Epoch: 6| Step: 4
Training loss: 2.6530838890170556
Validation loss: 2.47493416798183

Epoch: 6| Step: 5
Training loss: 2.0753214575630636
Validation loss: 2.478852505907297

Epoch: 6| Step: 6
Training loss: 2.2686552632206207
Validation loss: 2.483424950729529

Epoch: 6| Step: 7
Training loss: 1.9638328909665324
Validation loss: 2.459474433462564

Epoch: 6| Step: 8
Training loss: 2.485742444550696
Validation loss: 2.4766157046277923

Epoch: 6| Step: 9
Training loss: 2.016793555479183
Validation loss: 2.48883258135241

Epoch: 6| Step: 10
Training loss: 2.039333398589305
Validation loss: 2.5174844160610954

Epoch: 6| Step: 11
Training loss: 2.313363712983452
Validation loss: 2.5063737364655085

Epoch: 6| Step: 12
Training loss: 2.991730896695046
Validation loss: 2.527108874798602

Epoch: 6| Step: 13
Training loss: 2.1123773753674686
Validation loss: 2.4981586350769485

Epoch: 210| Step: 0
Training loss: 2.2346528420730056
Validation loss: 2.498539752470163

Epoch: 6| Step: 1
Training loss: 2.5352004507293735
Validation loss: 2.466790351043555

Epoch: 6| Step: 2
Training loss: 2.2530604846622264
Validation loss: 2.4763527188667056

Epoch: 6| Step: 3
Training loss: 1.9991898087750473
Validation loss: 2.47238008183268

Epoch: 6| Step: 4
Training loss: 2.507402142852527
Validation loss: 2.4759686356685644

Epoch: 6| Step: 5
Training loss: 2.2683598290586335
Validation loss: 2.473260453153796

Epoch: 6| Step: 6
Training loss: 2.4907074841633814
Validation loss: 2.4749992964644028

Epoch: 6| Step: 7
Training loss: 2.3429368706671636
Validation loss: 2.4708979151639303

Epoch: 6| Step: 8
Training loss: 2.7750285997721216
Validation loss: 2.47616296347243

Epoch: 6| Step: 9
Training loss: 2.080560275509478
Validation loss: 2.4666598521817216

Epoch: 6| Step: 10
Training loss: 2.321022207723381
Validation loss: 2.469612952601221

Epoch: 6| Step: 11
Training loss: 2.5174355951156766
Validation loss: 2.4685270474354315

Epoch: 6| Step: 12
Training loss: 1.4643653190055081
Validation loss: 2.4779873027604498

Epoch: 6| Step: 13
Training loss: 2.789861399034722
Validation loss: 2.4899141153080584

Epoch: 211| Step: 0
Training loss: 2.2764179841031806
Validation loss: 2.490688243718728

Epoch: 6| Step: 1
Training loss: 1.9147668904785895
Validation loss: 2.4842424097407467

Epoch: 6| Step: 2
Training loss: 2.418166834986245
Validation loss: 2.4928306299846024

Epoch: 6| Step: 3
Training loss: 2.043378566388561
Validation loss: 2.4926592500931184

Epoch: 6| Step: 4
Training loss: 2.149050094659659
Validation loss: 2.499227499819837

Epoch: 6| Step: 5
Training loss: 2.5285867879922925
Validation loss: 2.499932622001587

Epoch: 6| Step: 6
Training loss: 2.387899702876424
Validation loss: 2.496568327250062

Epoch: 6| Step: 7
Training loss: 2.8300016299401434
Validation loss: 2.5373376005098094

Epoch: 6| Step: 8
Training loss: 1.8936902883451157
Validation loss: 2.482256643987621

Epoch: 6| Step: 9
Training loss: 2.1277449213827233
Validation loss: 2.488683168059009

Epoch: 6| Step: 10
Training loss: 2.6540069983188146
Validation loss: 2.4879345215622206

Epoch: 6| Step: 11
Training loss: 2.6767600392468394
Validation loss: 2.4945817565743074

Epoch: 6| Step: 12
Training loss: 2.5155333508210576
Validation loss: 2.4795018832325284

Epoch: 6| Step: 13
Training loss: 2.41919289016133
Validation loss: 2.47532517002149

Epoch: 212| Step: 0
Training loss: 2.5425608786887244
Validation loss: 2.4792590297729302

Epoch: 6| Step: 1
Training loss: 2.490026320199621
Validation loss: 2.4866751653135704

Epoch: 6| Step: 2
Training loss: 1.69017607815603
Validation loss: 2.477344786500308

Epoch: 6| Step: 3
Training loss: 1.8629405991835666
Validation loss: 2.492148180803908

Epoch: 6| Step: 4
Training loss: 2.0092321935426876
Validation loss: 2.483660341233857

Epoch: 6| Step: 5
Training loss: 2.3505485097443484
Validation loss: 2.489957539248723

Epoch: 6| Step: 6
Training loss: 2.464323300219135
Validation loss: 2.4921309764674513

Epoch: 6| Step: 7
Training loss: 1.697699909377333
Validation loss: 2.4934216097471893

Epoch: 6| Step: 8
Training loss: 2.1238642630730866
Validation loss: 2.474526580332746

Epoch: 6| Step: 9
Training loss: 2.524023785111792
Validation loss: 2.487689831773015

Epoch: 6| Step: 10
Training loss: 3.0605693103038574
Validation loss: 2.4647803038962137

Epoch: 6| Step: 11
Training loss: 2.484821783233879
Validation loss: 2.468189771479637

Epoch: 6| Step: 12
Training loss: 2.3172009087635685
Validation loss: 2.4753364231587933

Epoch: 6| Step: 13
Training loss: 2.5947789368890835
Validation loss: 2.4831388745135134

Epoch: 213| Step: 0
Training loss: 2.5342595611509315
Validation loss: 2.479386685863689

Epoch: 6| Step: 1
Training loss: 1.8510574746588384
Validation loss: 2.489510281074922

Epoch: 6| Step: 2
Training loss: 1.2421327494905174
Validation loss: 2.4778723236353315

Epoch: 6| Step: 3
Training loss: 2.6509644822511724
Validation loss: 2.5037434842930644

Epoch: 6| Step: 4
Training loss: 2.6365791340351037
Validation loss: 2.493372349518806

Epoch: 6| Step: 5
Training loss: 2.5752310491808377
Validation loss: 2.4939636151724662

Epoch: 6| Step: 6
Training loss: 1.836030608717807
Validation loss: 2.506147265528527

Epoch: 6| Step: 7
Training loss: 3.254354933707431
Validation loss: 2.5030813142708426

Epoch: 6| Step: 8
Training loss: 2.316584511084599
Validation loss: 2.517485126349143

Epoch: 6| Step: 9
Training loss: 1.8711928180119668
Validation loss: 2.487434917985142

Epoch: 6| Step: 10
Training loss: 2.7644507722519838
Validation loss: 2.488229173148261

Epoch: 6| Step: 11
Training loss: 1.5101089777999974
Validation loss: 2.5119440067706202

Epoch: 6| Step: 12
Training loss: 2.240471904637224
Validation loss: 2.496837252513374

Epoch: 6| Step: 13
Training loss: 2.595626887342142
Validation loss: 2.5005815465371573

Epoch: 214| Step: 0
Training loss: 3.0038776768390734
Validation loss: 2.502537933224709

Epoch: 6| Step: 1
Training loss: 2.1759684018196985
Validation loss: 2.4844097318960574

Epoch: 6| Step: 2
Training loss: 2.7453132920326477
Validation loss: 2.496444820734744

Epoch: 6| Step: 3
Training loss: 1.9407968466531054
Validation loss: 2.4917037318948365

Epoch: 6| Step: 4
Training loss: 2.3111982032166782
Validation loss: 2.48981923750689

Epoch: 6| Step: 5
Training loss: 2.1016701596276675
Validation loss: 2.4923692754545215

Epoch: 6| Step: 6
Training loss: 2.6904442206957744
Validation loss: 2.496842377043324

Epoch: 6| Step: 7
Training loss: 2.843382423874502
Validation loss: 2.5008279224232557

Epoch: 6| Step: 8
Training loss: 1.7280376225985263
Validation loss: 2.493658526534744

Epoch: 6| Step: 9
Training loss: 1.7735430312161697
Validation loss: 2.5040132734781744

Epoch: 6| Step: 10
Training loss: 2.1346537149732554
Validation loss: 2.497265050895852

Epoch: 6| Step: 11
Training loss: 2.5124347431571326
Validation loss: 2.5009624694800525

Epoch: 6| Step: 12
Training loss: 2.0786627740503403
Validation loss: 2.4903819879401494

Epoch: 6| Step: 13
Training loss: 2.0975962369208547
Validation loss: 2.500154426891437

Epoch: 215| Step: 0
Training loss: 1.7777910612855128
Validation loss: 2.4808113722576124

Epoch: 6| Step: 1
Training loss: 2.7171634999861047
Validation loss: 2.472616941632738

Epoch: 6| Step: 2
Training loss: 2.6926883030238034
Validation loss: 2.4809490389671094

Epoch: 6| Step: 3
Training loss: 2.57703723501321
Validation loss: 2.4845978738872265

Epoch: 6| Step: 4
Training loss: 1.4442307006381112
Validation loss: 2.488801591364933

Epoch: 6| Step: 5
Training loss: 2.3938985116172615
Validation loss: 2.4859850083116743

Epoch: 6| Step: 6
Training loss: 2.5000667563108676
Validation loss: 2.4732504517899834

Epoch: 6| Step: 7
Training loss: 2.285894395339712
Validation loss: 2.4860710896183784

Epoch: 6| Step: 8
Training loss: 2.3821047873201207
Validation loss: 2.486174917083053

Epoch: 6| Step: 9
Training loss: 2.099852933729464
Validation loss: 2.4921570938471755

Epoch: 6| Step: 10
Training loss: 2.8005050884814247
Validation loss: 2.4822453741980177

Epoch: 6| Step: 11
Training loss: 1.7408608487396926
Validation loss: 2.48078710560585

Epoch: 6| Step: 12
Training loss: 2.052633557080591
Validation loss: 2.4919882508353512

Epoch: 6| Step: 13
Training loss: 2.4878513797270654
Validation loss: 2.5034711581748317

Epoch: 216| Step: 0
Training loss: 2.5815527121913964
Validation loss: 2.490917029465182

Epoch: 6| Step: 1
Training loss: 2.348461857083748
Validation loss: 2.4806635101832737

Epoch: 6| Step: 2
Training loss: 1.9953492450243242
Validation loss: 2.4989608194313124

Epoch: 6| Step: 3
Training loss: 1.6278876543482488
Validation loss: 2.4981717895422033

Epoch: 6| Step: 4
Training loss: 2.6564772564779413
Validation loss: 2.4893454327632356

Epoch: 6| Step: 5
Training loss: 2.206765596048783
Validation loss: 2.4858366147551543

Epoch: 6| Step: 6
Training loss: 2.1631020297793055
Validation loss: 2.4913684611237317

Epoch: 6| Step: 7
Training loss: 2.842027247159357
Validation loss: 2.495877315747197

Epoch: 6| Step: 8
Training loss: 2.5218123644456476
Validation loss: 2.4897059379831137

Epoch: 6| Step: 9
Training loss: 1.5852922902923614
Validation loss: 2.4945037824355216

Epoch: 6| Step: 10
Training loss: 2.6798622363537574
Validation loss: 2.489373909881938

Epoch: 6| Step: 11
Training loss: 2.369872732487649
Validation loss: 2.482281568610467

Epoch: 6| Step: 12
Training loss: 2.326372139587333
Validation loss: 2.4835395767881088

Epoch: 6| Step: 13
Training loss: 2.2489181672809306
Validation loss: 2.4829036906360424

Epoch: 217| Step: 0
Training loss: 1.8994701073230638
Validation loss: 2.4707758192945413

Epoch: 6| Step: 1
Training loss: 2.5815021936650018
Validation loss: 2.4795737747044897

Epoch: 6| Step: 2
Training loss: 2.1028796751506307
Validation loss: 2.4948987572819696

Epoch: 6| Step: 3
Training loss: 2.494730067969284
Validation loss: 2.5017186853344753

Epoch: 6| Step: 4
Training loss: 2.0996834107586406
Validation loss: 2.5060286788060533

Epoch: 6| Step: 5
Training loss: 2.9684963720298523
Validation loss: 2.508479171349136

Epoch: 6| Step: 6
Training loss: 2.367119010715066
Validation loss: 2.499129183421248

Epoch: 6| Step: 7
Training loss: 2.607453786818997
Validation loss: 2.509985452764335

Epoch: 6| Step: 8
Training loss: 2.2975400623568603
Validation loss: 2.5169397713304713

Epoch: 6| Step: 9
Training loss: 1.6926741220000305
Validation loss: 2.512508064623996

Epoch: 6| Step: 10
Training loss: 2.3364954253432955
Validation loss: 2.521316203589561

Epoch: 6| Step: 11
Training loss: 1.9272113310774142
Validation loss: 2.498892872758517

Epoch: 6| Step: 12
Training loss: 1.9977578469624635
Validation loss: 2.5076738994638936

Epoch: 6| Step: 13
Training loss: 2.819627166072113
Validation loss: 2.5126829694357538

Epoch: 218| Step: 0
Training loss: 1.8311489559067176
Validation loss: 2.501801128073519

Epoch: 6| Step: 1
Training loss: 2.1001616279528084
Validation loss: 2.500805883853326

Epoch: 6| Step: 2
Training loss: 2.4465876657826637
Validation loss: 2.4870801229440764

Epoch: 6| Step: 3
Training loss: 2.6158968803543194
Validation loss: 2.4813601997047736

Epoch: 6| Step: 4
Training loss: 2.1565611241657825
Validation loss: 2.4828443790451367

Epoch: 6| Step: 5
Training loss: 2.4744345972833695
Validation loss: 2.4787837515498667

Epoch: 6| Step: 6
Training loss: 2.189890835623603
Validation loss: 2.4864029035288215

Epoch: 6| Step: 7
Training loss: 2.010425692786739
Validation loss: 2.4764832444851628

Epoch: 6| Step: 8
Training loss: 2.245737913389442
Validation loss: 2.472768707691087

Epoch: 6| Step: 9
Training loss: 2.7195861232254854
Validation loss: 2.466379033106573

Epoch: 6| Step: 10
Training loss: 2.355214676193971
Validation loss: 2.483152788648284

Epoch: 6| Step: 11
Training loss: 2.470504330564686
Validation loss: 2.4692781020334227

Epoch: 6| Step: 12
Training loss: 2.5550646955507226
Validation loss: 2.4803484870011294

Epoch: 6| Step: 13
Training loss: 2.2599651278278174
Validation loss: 2.4736783133527616

Epoch: 219| Step: 0
Training loss: 2.4534701936667047
Validation loss: 2.4788580523402994

Epoch: 6| Step: 1
Training loss: 1.9558262570218885
Validation loss: 2.483735744036722

Epoch: 6| Step: 2
Training loss: 2.799008115423444
Validation loss: 2.481037777489693

Epoch: 6| Step: 3
Training loss: 2.5531996381068223
Validation loss: 2.5047619924971776

Epoch: 6| Step: 4
Training loss: 1.9731799950606925
Validation loss: 2.4838183917104044

Epoch: 6| Step: 5
Training loss: 2.6684928144758837
Validation loss: 2.487652510052431

Epoch: 6| Step: 6
Training loss: 2.426441047611035
Validation loss: 2.4916938284814973

Epoch: 6| Step: 7
Training loss: 2.254879851936125
Validation loss: 2.500686281103739

Epoch: 6| Step: 8
Training loss: 1.973421397864583
Validation loss: 2.4970778473984176

Epoch: 6| Step: 9
Training loss: 2.456946536788591
Validation loss: 2.503818679047913

Epoch: 6| Step: 10
Training loss: 2.374616491574095
Validation loss: 2.4825331700245243

Epoch: 6| Step: 11
Training loss: 2.381074064869167
Validation loss: 2.4809833463401767

Epoch: 6| Step: 12
Training loss: 2.06605408521467
Validation loss: 2.4709876820138756

Epoch: 6| Step: 13
Training loss: 2.0785967069202074
Validation loss: 2.4773268297368074

Epoch: 220| Step: 0
Training loss: 1.6751024698826438
Validation loss: 2.4594219082111777

Epoch: 6| Step: 1
Training loss: 3.177778190962521
Validation loss: 2.4636346720155746

Epoch: 6| Step: 2
Training loss: 2.0885495180155247
Validation loss: 2.4681378019967988

Epoch: 6| Step: 3
Training loss: 1.8768972334940601
Validation loss: 2.4698055280352493

Epoch: 6| Step: 4
Training loss: 2.5911379611152925
Validation loss: 2.4595397936016234

Epoch: 6| Step: 5
Training loss: 2.2203673794196908
Validation loss: 2.47840698741587

Epoch: 6| Step: 6
Training loss: 2.782572742481265
Validation loss: 2.4708827097967663

Epoch: 6| Step: 7
Training loss: 2.242546876823532
Validation loss: 2.469624979955124

Epoch: 6| Step: 8
Training loss: 1.9805090785387884
Validation loss: 2.4662368472960696

Epoch: 6| Step: 9
Training loss: 2.1164124819303667
Validation loss: 2.4850395481434484

Epoch: 6| Step: 10
Training loss: 2.319665272670582
Validation loss: 2.481533777070471

Epoch: 6| Step: 11
Training loss: 2.3132105586244185
Validation loss: 2.4842203120091493

Epoch: 6| Step: 12
Training loss: 2.5708947725131517
Validation loss: 2.463686301009361

Epoch: 6| Step: 13
Training loss: 2.316961778281861
Validation loss: 2.4551614136731486

Epoch: 221| Step: 0
Training loss: 2.014025030978745
Validation loss: 2.463240652801383

Epoch: 6| Step: 1
Training loss: 1.8263113040021344
Validation loss: 2.4821072190677507

Epoch: 6| Step: 2
Training loss: 2.5866235474141956
Validation loss: 2.4770979758670175

Epoch: 6| Step: 3
Training loss: 2.2082018423220116
Validation loss: 2.4739262361284764

Epoch: 6| Step: 4
Training loss: 2.860861803404476
Validation loss: 2.4721217730352043

Epoch: 6| Step: 5
Training loss: 2.7247481553477195
Validation loss: 2.4709897243245447

Epoch: 6| Step: 6
Training loss: 2.4106105580291666
Validation loss: 2.477671514310405

Epoch: 6| Step: 7
Training loss: 2.1450824967105793
Validation loss: 2.4686821875936324

Epoch: 6| Step: 8
Training loss: 2.0991212641202828
Validation loss: 2.4847463973961137

Epoch: 6| Step: 9
Training loss: 2.249993748126351
Validation loss: 2.465465163032805

Epoch: 6| Step: 10
Training loss: 2.1066574110157767
Validation loss: 2.4755884575564506

Epoch: 6| Step: 11
Training loss: 1.8597433542778201
Validation loss: 2.4778387429511515

Epoch: 6| Step: 12
Training loss: 2.5844375547428564
Validation loss: 2.499455392645898

Epoch: 6| Step: 13
Training loss: 2.573671783548239
Validation loss: 2.4954241360395306

Epoch: 222| Step: 0
Training loss: 2.3921041462913153
Validation loss: 2.5107021697060232

Epoch: 6| Step: 1
Training loss: 2.438578244901391
Validation loss: 2.5052450789349434

Epoch: 6| Step: 2
Training loss: 2.0209755077359675
Validation loss: 2.5265326635596006

Epoch: 6| Step: 3
Training loss: 2.533794678123354
Validation loss: 2.5291173058667757

Epoch: 6| Step: 4
Training loss: 1.9688393708788825
Validation loss: 2.5623343964696055

Epoch: 6| Step: 5
Training loss: 2.409546019029781
Validation loss: 2.579780848335462

Epoch: 6| Step: 6
Training loss: 2.4815058424952157
Validation loss: 2.5851993486876084

Epoch: 6| Step: 7
Training loss: 2.3170323675575983
Validation loss: 2.567487530503146

Epoch: 6| Step: 8
Training loss: 2.5116207880877037
Validation loss: 2.4990707577827376

Epoch: 6| Step: 9
Training loss: 1.6763592770178835
Validation loss: 2.4858199102346448

Epoch: 6| Step: 10
Training loss: 1.9285466454947044
Validation loss: 2.4896300775669906

Epoch: 6| Step: 11
Training loss: 2.5592857298693783
Validation loss: 2.4877831775496317

Epoch: 6| Step: 12
Training loss: 2.7071670564216226
Validation loss: 2.4863295633771862

Epoch: 6| Step: 13
Training loss: 2.4967116664129714
Validation loss: 2.4860485685877083

Epoch: 223| Step: 0
Training loss: 2.940402605685233
Validation loss: 2.4692376697316494

Epoch: 6| Step: 1
Training loss: 1.875920133521434
Validation loss: 2.4736150777838395

Epoch: 6| Step: 2
Training loss: 2.694064440154065
Validation loss: 2.47695292414835

Epoch: 6| Step: 3
Training loss: 2.3550027920975705
Validation loss: 2.47510632565926

Epoch: 6| Step: 4
Training loss: 2.5653201146244546
Validation loss: 2.4842002695245067

Epoch: 6| Step: 5
Training loss: 2.296597314773926
Validation loss: 2.4642503188271934

Epoch: 6| Step: 6
Training loss: 1.9304614040307546
Validation loss: 2.467830961719592

Epoch: 6| Step: 7
Training loss: 1.8773285711582068
Validation loss: 2.478340000416404

Epoch: 6| Step: 8
Training loss: 2.8966751704586855
Validation loss: 2.470918708814406

Epoch: 6| Step: 9
Training loss: 2.2742996940504905
Validation loss: 2.468421399101473

Epoch: 6| Step: 10
Training loss: 2.105163438844606
Validation loss: 2.478976319856891

Epoch: 6| Step: 11
Training loss: 2.034596195882085
Validation loss: 2.4806155664442886

Epoch: 6| Step: 12
Training loss: 2.5892960327393837
Validation loss: 2.4793721335615433

Epoch: 6| Step: 13
Training loss: 2.8937130491068546
Validation loss: 2.4758787364470902

Epoch: 224| Step: 0
Training loss: 2.2548917999022384
Validation loss: 2.4648365115358173

Epoch: 6| Step: 1
Training loss: 2.0712057238066888
Validation loss: 2.47070725833914

Epoch: 6| Step: 2
Training loss: 2.37494789869481
Validation loss: 2.463483069101341

Epoch: 6| Step: 3
Training loss: 2.3238108557519364
Validation loss: 2.455441751715008

Epoch: 6| Step: 4
Training loss: 2.5669643312063273
Validation loss: 2.4603620068203584

Epoch: 6| Step: 5
Training loss: 2.7520084849326607
Validation loss: 2.457003174314621

Epoch: 6| Step: 6
Training loss: 2.1459483760870883
Validation loss: 2.4632818933242713

Epoch: 6| Step: 7
Training loss: 2.8855286908122206
Validation loss: 2.472460360850538

Epoch: 6| Step: 8
Training loss: 2.5080644237390493
Validation loss: 2.4611346130550182

Epoch: 6| Step: 9
Training loss: 2.9824087646036332
Validation loss: 2.47967696080475

Epoch: 6| Step: 10
Training loss: 1.9048074986473242
Validation loss: 2.4814249993916184

Epoch: 6| Step: 11
Training loss: 1.9251974277072559
Validation loss: 2.493282942347701

Epoch: 6| Step: 12
Training loss: 2.2597328124305847
Validation loss: 2.488862102109896

Epoch: 6| Step: 13
Training loss: 1.91464423856554
Validation loss: 2.504516225738777

Epoch: 225| Step: 0
Training loss: 1.9271122354625478
Validation loss: 2.5197947440746913

Epoch: 6| Step: 1
Training loss: 2.7670606776878506
Validation loss: 2.4940886784587946

Epoch: 6| Step: 2
Training loss: 2.696609140952731
Validation loss: 2.4946332229494277

Epoch: 6| Step: 3
Training loss: 2.113360673184049
Validation loss: 2.4916769479350225

Epoch: 6| Step: 4
Training loss: 2.2145804327739143
Validation loss: 2.485294963439521

Epoch: 6| Step: 5
Training loss: 2.0022965597187454
Validation loss: 2.468571781212245

Epoch: 6| Step: 6
Training loss: 2.8150311418508043
Validation loss: 2.462590093626649

Epoch: 6| Step: 7
Training loss: 2.4003433458779573
Validation loss: 2.47227565895745

Epoch: 6| Step: 8
Training loss: 2.0510714080446926
Validation loss: 2.4772210113105326

Epoch: 6| Step: 9
Training loss: 2.046505028288606
Validation loss: 2.485525876310787

Epoch: 6| Step: 10
Training loss: 2.6368036553170042
Validation loss: 2.4861836757287348

Epoch: 6| Step: 11
Training loss: 1.713229033948194
Validation loss: 2.4813802170875667

Epoch: 6| Step: 12
Training loss: 2.5278795196216644
Validation loss: 2.481082581990371

Epoch: 6| Step: 13
Training loss: 2.425443518281961
Validation loss: 2.4729613587460677

Epoch: 226| Step: 0
Training loss: 2.302487114985196
Validation loss: 2.4698006611411816

Epoch: 6| Step: 1
Training loss: 2.850874810056939
Validation loss: 2.4729226819045307

Epoch: 6| Step: 2
Training loss: 2.417180982209821
Validation loss: 2.453639460435069

Epoch: 6| Step: 3
Training loss: 1.5945365311321626
Validation loss: 2.4856054110191175

Epoch: 6| Step: 4
Training loss: 2.3336474570500974
Validation loss: 2.4893977576219672

Epoch: 6| Step: 5
Training loss: 2.7263912813920324
Validation loss: 2.4831484759949536

Epoch: 6| Step: 6
Training loss: 2.1571914165737027
Validation loss: 2.483498232655581

Epoch: 6| Step: 7
Training loss: 2.38504224168009
Validation loss: 2.482762691253459

Epoch: 6| Step: 8
Training loss: 2.522514717807687
Validation loss: 2.474442072639629

Epoch: 6| Step: 9
Training loss: 2.649999287443245
Validation loss: 2.4691589274324306

Epoch: 6| Step: 10
Training loss: 1.8098104186909116
Validation loss: 2.4658094832682997

Epoch: 6| Step: 11
Training loss: 2.1106458438457434
Validation loss: 2.4841600880945784

Epoch: 6| Step: 12
Training loss: 2.2105628461137794
Validation loss: 2.4900501217961675

Epoch: 6| Step: 13
Training loss: 2.3756522236754276
Validation loss: 2.490717925951101

Epoch: 227| Step: 0
Training loss: 2.32268005215189
Validation loss: 2.486230409294723

Epoch: 6| Step: 1
Training loss: 2.0633425292188647
Validation loss: 2.494779277497325

Epoch: 6| Step: 2
Training loss: 2.4927933772610014
Validation loss: 2.4893967679612623

Epoch: 6| Step: 3
Training loss: 2.3185323443399466
Validation loss: 2.4768562022402367

Epoch: 6| Step: 4
Training loss: 3.065218594645247
Validation loss: 2.476501191319991

Epoch: 6| Step: 5
Training loss: 2.0210891116059186
Validation loss: 2.4898618013006693

Epoch: 6| Step: 6
Training loss: 2.416655496593399
Validation loss: 2.4866684857720616

Epoch: 6| Step: 7
Training loss: 2.2959661437273176
Validation loss: 2.4808437434413624

Epoch: 6| Step: 8
Training loss: 1.9419717559921326
Validation loss: 2.490582873324393

Epoch: 6| Step: 9
Training loss: 1.7790068841278872
Validation loss: 2.4847255595410886

Epoch: 6| Step: 10
Training loss: 2.874214894446908
Validation loss: 2.477290177966523

Epoch: 6| Step: 11
Training loss: 2.1737736149317364
Validation loss: 2.4901169613353433

Epoch: 6| Step: 12
Training loss: 2.033035549913157
Validation loss: 2.4807715363814986

Epoch: 6| Step: 13
Training loss: 2.3695410180197096
Validation loss: 2.4819234752379975

Epoch: 228| Step: 0
Training loss: 2.010991292736413
Validation loss: 2.4799879193780825

Epoch: 6| Step: 1
Training loss: 2.7334560048868273
Validation loss: 2.493526182910382

Epoch: 6| Step: 2
Training loss: 2.0595443818009627
Validation loss: 2.5013880452614683

Epoch: 6| Step: 3
Training loss: 2.4313845089769663
Validation loss: 2.491520727682913

Epoch: 6| Step: 4
Training loss: 2.676792371402229
Validation loss: 2.5200971889424215

Epoch: 6| Step: 5
Training loss: 2.06352139237862
Validation loss: 2.503958254713984

Epoch: 6| Step: 6
Training loss: 2.4048606527647935
Validation loss: 2.5307300331840175

Epoch: 6| Step: 7
Training loss: 1.7863549146013538
Validation loss: 2.5002579079155733

Epoch: 6| Step: 8
Training loss: 1.5305535425879473
Validation loss: 2.4807389239858093

Epoch: 6| Step: 9
Training loss: 3.1900539264485643
Validation loss: 2.4712102196231105

Epoch: 6| Step: 10
Training loss: 2.5111323453723706
Validation loss: 2.495056509933683

Epoch: 6| Step: 11
Training loss: 1.5400622843247638
Validation loss: 2.484367258631643

Epoch: 6| Step: 12
Training loss: 2.605679848053198
Validation loss: 2.480813454533504

Epoch: 6| Step: 13
Training loss: 2.0236908153906286
Validation loss: 2.495060483483506

Epoch: 229| Step: 0
Training loss: 2.7681220037771834
Validation loss: 2.481728333585977

Epoch: 6| Step: 1
Training loss: 2.3756917648748828
Validation loss: 2.485944464110525

Epoch: 6| Step: 2
Training loss: 2.4500004982461228
Validation loss: 2.4891189769939626

Epoch: 6| Step: 3
Training loss: 1.6314284011510511
Validation loss: 2.494444921895156

Epoch: 6| Step: 4
Training loss: 3.0582805292905033
Validation loss: 2.488135892014857

Epoch: 6| Step: 5
Training loss: 1.8339657198573638
Validation loss: 2.482664675160001

Epoch: 6| Step: 6
Training loss: 1.6295818876167243
Validation loss: 2.4816644785222164

Epoch: 6| Step: 7
Training loss: 1.9894759929787527
Validation loss: 2.495627903537349

Epoch: 6| Step: 8
Training loss: 2.064870339264636
Validation loss: 2.4896078760275437

Epoch: 6| Step: 9
Training loss: 2.1643211678384775
Validation loss: 2.481309434897966

Epoch: 6| Step: 10
Training loss: 2.8782940113920996
Validation loss: 2.4962328263661298

Epoch: 6| Step: 11
Training loss: 2.40535907848449
Validation loss: 2.4782630384883317

Epoch: 6| Step: 12
Training loss: 2.2401807357898393
Validation loss: 2.4832040678427254

Epoch: 6| Step: 13
Training loss: 2.1952606955283893
Validation loss: 2.4853280756452034

Epoch: 230| Step: 0
Training loss: 2.627175292649987
Validation loss: 2.475733541067285

Epoch: 6| Step: 1
Training loss: 2.253007150813919
Validation loss: 2.5045877323758257

Epoch: 6| Step: 2
Training loss: 2.0007673222102844
Validation loss: 2.512904542330681

Epoch: 6| Step: 3
Training loss: 1.8253322690766054
Validation loss: 2.508114330155933

Epoch: 6| Step: 4
Training loss: 2.660218628811582
Validation loss: 2.5139189358900786

Epoch: 6| Step: 5
Training loss: 1.4872190835303964
Validation loss: 2.5366314013666096

Epoch: 6| Step: 6
Training loss: 2.295255959725254
Validation loss: 2.512596091841606

Epoch: 6| Step: 7
Training loss: 2.376618887629258
Validation loss: 2.509443630041681

Epoch: 6| Step: 8
Training loss: 2.7444495627921386
Validation loss: 2.4931950301386174

Epoch: 6| Step: 9
Training loss: 1.9582866365533214
Validation loss: 2.4947862936982745

Epoch: 6| Step: 10
Training loss: 2.8670494607651316
Validation loss: 2.5021965390867873

Epoch: 6| Step: 11
Training loss: 1.81935611863136
Validation loss: 2.4931209494238367

Epoch: 6| Step: 12
Training loss: 2.1402906170602334
Validation loss: 2.4846346177698018

Epoch: 6| Step: 13
Training loss: 2.6307868749290253
Validation loss: 2.4973132796394637

Epoch: 231| Step: 0
Training loss: 2.290370320191318
Validation loss: 2.4859362480948457

Epoch: 6| Step: 1
Training loss: 2.1300379553623405
Validation loss: 2.5002752867767266

Epoch: 6| Step: 2
Training loss: 1.8606090137229319
Validation loss: 2.4903234448429563

Epoch: 6| Step: 3
Training loss: 2.510670017316503
Validation loss: 2.486323969683455

Epoch: 6| Step: 4
Training loss: 2.1244134654473625
Validation loss: 2.48822054946137

Epoch: 6| Step: 5
Training loss: 2.2181807311562776
Validation loss: 2.4851096804365884

Epoch: 6| Step: 6
Training loss: 2.4551961137556266
Validation loss: 2.4845613133846487

Epoch: 6| Step: 7
Training loss: 3.1009367112113844
Validation loss: 2.5036715408908874

Epoch: 6| Step: 8
Training loss: 2.1850393351979513
Validation loss: 2.5030852671441126

Epoch: 6| Step: 9
Training loss: 2.0771182496998297
Validation loss: 2.5105644251180967

Epoch: 6| Step: 10
Training loss: 2.6569560290722887
Validation loss: 2.5001420457540293

Epoch: 6| Step: 11
Training loss: 2.159006319716137
Validation loss: 2.507996692013422

Epoch: 6| Step: 12
Training loss: 2.3050787124930405
Validation loss: 2.506802744807276

Epoch: 6| Step: 13
Training loss: 1.968193762781234
Validation loss: 2.4995935745483058

Epoch: 232| Step: 0
Training loss: 2.484479410148759
Validation loss: 2.5134222997996027

Epoch: 6| Step: 1
Training loss: 1.7615202053880843
Validation loss: 2.4943167301420486

Epoch: 6| Step: 2
Training loss: 3.008848809401515
Validation loss: 2.498057389504398

Epoch: 6| Step: 3
Training loss: 2.5542602181739427
Validation loss: 2.4882841244954945

Epoch: 6| Step: 4
Training loss: 1.874730408678701
Validation loss: 2.4889176062528944

Epoch: 6| Step: 5
Training loss: 1.680401459592073
Validation loss: 2.49749729134709

Epoch: 6| Step: 6
Training loss: 1.5481103145317108
Validation loss: 2.491289939809039

Epoch: 6| Step: 7
Training loss: 2.429799656870516
Validation loss: 2.4921331130643507

Epoch: 6| Step: 8
Training loss: 2.393296587758466
Validation loss: 2.483277164267769

Epoch: 6| Step: 9
Training loss: 2.339982119434869
Validation loss: 2.488249294968111

Epoch: 6| Step: 10
Training loss: 1.97866986380539
Validation loss: 2.461730151209069

Epoch: 6| Step: 11
Training loss: 2.3255732332130736
Validation loss: 2.480584441763537

Epoch: 6| Step: 12
Training loss: 2.458116446372755
Validation loss: 2.4841364620061204

Epoch: 6| Step: 13
Training loss: 2.827516268979327
Validation loss: 2.474885631577556

Epoch: 233| Step: 0
Training loss: 2.537603340487548
Validation loss: 2.491338770692115

Epoch: 6| Step: 1
Training loss: 2.083733126109178
Validation loss: 2.493645172975298

Epoch: 6| Step: 2
Training loss: 1.783162345294593
Validation loss: 2.4944951326515565

Epoch: 6| Step: 3
Training loss: 2.1302136807425116
Validation loss: 2.5153613060906417

Epoch: 6| Step: 4
Training loss: 2.0757125963739202
Validation loss: 2.5371478094724

Epoch: 6| Step: 5
Training loss: 2.5861189395373123
Validation loss: 2.5108659956823107

Epoch: 6| Step: 6
Training loss: 2.7004558637572345
Validation loss: 2.5343465819923385

Epoch: 6| Step: 7
Training loss: 2.166824616274721
Validation loss: 2.518868828960845

Epoch: 6| Step: 8
Training loss: 2.1238256463223535
Validation loss: 2.531185478989367

Epoch: 6| Step: 9
Training loss: 1.3963462067656662
Validation loss: 2.5028346045258565

Epoch: 6| Step: 10
Training loss: 2.648106377081249
Validation loss: 2.5158116526099645

Epoch: 6| Step: 11
Training loss: 2.9560071010684976
Validation loss: 2.5181920792781964

Epoch: 6| Step: 12
Training loss: 2.495783301495725
Validation loss: 2.495275372233156

Epoch: 6| Step: 13
Training loss: 1.9834070923924902
Validation loss: 2.5196479868890544

Epoch: 234| Step: 0
Training loss: 1.925713156836851
Validation loss: 2.508864501990967

Epoch: 6| Step: 1
Training loss: 2.6861284881559206
Validation loss: 2.5093462919031704

Epoch: 6| Step: 2
Training loss: 2.2482708538508858
Validation loss: 2.5199371405234565

Epoch: 6| Step: 3
Training loss: 1.6579797457250487
Validation loss: 2.544444135205927

Epoch: 6| Step: 4
Training loss: 2.196204930615429
Validation loss: 2.5429822195763823

Epoch: 6| Step: 5
Training loss: 2.542515774418181
Validation loss: 2.560604122946712

Epoch: 6| Step: 6
Training loss: 2.3813143663893404
Validation loss: 2.5580216173347705

Epoch: 6| Step: 7
Training loss: 2.3154927039611177
Validation loss: 2.5220893746688167

Epoch: 6| Step: 8
Training loss: 2.8912382557882355
Validation loss: 2.517793893261218

Epoch: 6| Step: 9
Training loss: 1.6835940332257593
Validation loss: 2.4836039436245536

Epoch: 6| Step: 10
Training loss: 2.6774419601233377
Validation loss: 2.4793936895293203

Epoch: 6| Step: 11
Training loss: 1.5923849505631573
Validation loss: 2.4700914271547147

Epoch: 6| Step: 12
Training loss: 2.4270762060058026
Validation loss: 2.4812030336177275

Epoch: 6| Step: 13
Training loss: 2.7241017952382602
Validation loss: 2.4714448280440315

Epoch: 235| Step: 0
Training loss: 1.6607684274570196
Validation loss: 2.4630665687277626

Epoch: 6| Step: 1
Training loss: 1.727160044010674
Validation loss: 2.465771242163354

Epoch: 6| Step: 2
Training loss: 2.6666459341037974
Validation loss: 2.46386949375532

Epoch: 6| Step: 3
Training loss: 2.2540486256077092
Validation loss: 2.469972798561916

Epoch: 6| Step: 4
Training loss: 2.5909479471423245
Validation loss: 2.4671683979321695

Epoch: 6| Step: 5
Training loss: 1.7538522826729894
Validation loss: 2.484845178925105

Epoch: 6| Step: 6
Training loss: 2.092353696824403
Validation loss: 2.477336365524253

Epoch: 6| Step: 7
Training loss: 2.998396126847834
Validation loss: 2.4838380053306324

Epoch: 6| Step: 8
Training loss: 1.5737629649481937
Validation loss: 2.473870194856091

Epoch: 6| Step: 9
Training loss: 2.443548279068509
Validation loss: 2.4766176941634774

Epoch: 6| Step: 10
Training loss: 2.753487542970186
Validation loss: 2.477082271163965

Epoch: 6| Step: 11
Training loss: 2.147172034980601
Validation loss: 2.4698966860046068

Epoch: 6| Step: 12
Training loss: 2.5149753276608253
Validation loss: 2.481275780621534

Epoch: 6| Step: 13
Training loss: 2.6417346857341277
Validation loss: 2.478928006863466

Epoch: 236| Step: 0
Training loss: 2.547403105295976
Validation loss: 2.4853008072722447

Epoch: 6| Step: 1
Training loss: 2.497093800302617
Validation loss: 2.4892997952663913

Epoch: 6| Step: 2
Training loss: 1.8089908304018982
Validation loss: 2.4993549468410987

Epoch: 6| Step: 3
Training loss: 2.342732323003132
Validation loss: 2.56096237074339

Epoch: 6| Step: 4
Training loss: 2.7727608728433815
Validation loss: 2.5584191027335086

Epoch: 6| Step: 5
Training loss: 2.8586064540917486
Validation loss: 2.611556582180742

Epoch: 6| Step: 6
Training loss: 2.6507569005844527
Validation loss: 2.577244695012835

Epoch: 6| Step: 7
Training loss: 2.8799008601397027
Validation loss: 2.5838473690481543

Epoch: 6| Step: 8
Training loss: 1.7845956268469012
Validation loss: 2.525327999855916

Epoch: 6| Step: 9
Training loss: 1.5339423663218321
Validation loss: 2.4949896673473995

Epoch: 6| Step: 10
Training loss: 2.507817443652643
Validation loss: 2.481153418697534

Epoch: 6| Step: 11
Training loss: 1.8780486435689263
Validation loss: 2.486254798624974

Epoch: 6| Step: 12
Training loss: 2.5345178867234965
Validation loss: 2.498809371988393

Epoch: 6| Step: 13
Training loss: 2.0721525256674824
Validation loss: 2.514535134735074

Epoch: 237| Step: 0
Training loss: 1.6041881345162892
Validation loss: 2.5257569195163216

Epoch: 6| Step: 1
Training loss: 2.583270677196371
Validation loss: 2.5071859398994345

Epoch: 6| Step: 2
Training loss: 2.7204169063263217
Validation loss: 2.518958361252883

Epoch: 6| Step: 3
Training loss: 2.4843076300933307
Validation loss: 2.499205669731754

Epoch: 6| Step: 4
Training loss: 2.287268133867307
Validation loss: 2.5199402627474634

Epoch: 6| Step: 5
Training loss: 1.936344355826259
Validation loss: 2.5130616390624594

Epoch: 6| Step: 6
Training loss: 2.0208430208059185
Validation loss: 2.5074751281392196

Epoch: 6| Step: 7
Training loss: 2.7309365588054613
Validation loss: 2.4936396036593855

Epoch: 6| Step: 8
Training loss: 2.740788029309535
Validation loss: 2.497661418668816

Epoch: 6| Step: 9
Training loss: 2.6106792548242734
Validation loss: 2.498525995592206

Epoch: 6| Step: 10
Training loss: 2.778481294764981
Validation loss: 2.505765877989122

Epoch: 6| Step: 11
Training loss: 1.8001866879607877
Validation loss: 2.490636281012271

Epoch: 6| Step: 12
Training loss: 2.6573842936578984
Validation loss: 2.4852691496829604

Epoch: 6| Step: 13
Training loss: 1.7378564208307334
Validation loss: 2.5001046158836133

Epoch: 238| Step: 0
Training loss: 1.9353786052770554
Validation loss: 2.5005415806660793

Epoch: 6| Step: 1
Training loss: 2.5354583983381325
Validation loss: 2.506096669847516

Epoch: 6| Step: 2
Training loss: 2.605412747125826
Validation loss: 2.5017975464253834

Epoch: 6| Step: 3
Training loss: 2.6267364752960343
Validation loss: 2.497811106237548

Epoch: 6| Step: 4
Training loss: 2.203635082310311
Validation loss: 2.490733744099226

Epoch: 6| Step: 5
Training loss: 1.9042752152213704
Validation loss: 2.49623818295553

Epoch: 6| Step: 6
Training loss: 1.5629913320514586
Validation loss: 2.4943592092252693

Epoch: 6| Step: 7
Training loss: 1.7491236263442318
Validation loss: 2.496327866314174

Epoch: 6| Step: 8
Training loss: 2.8511507756060985
Validation loss: 2.4989158345836144

Epoch: 6| Step: 9
Training loss: 2.447683726380851
Validation loss: 2.494933557689576

Epoch: 6| Step: 10
Training loss: 2.298263856512213
Validation loss: 2.5072646131800616

Epoch: 6| Step: 11
Training loss: 3.04373178966174
Validation loss: 2.527217714448184

Epoch: 6| Step: 12
Training loss: 1.8817111391139911
Validation loss: 2.5341595226899236

Epoch: 6| Step: 13
Training loss: 2.378923488109753
Validation loss: 2.5291765299756084

Epoch: 239| Step: 0
Training loss: 2.047151270269068
Validation loss: 2.5481185373563187

Epoch: 6| Step: 1
Training loss: 2.4222354743703884
Validation loss: 2.5407859704330544

Epoch: 6| Step: 2
Training loss: 2.403367929866552
Validation loss: 2.55743520972207

Epoch: 6| Step: 3
Training loss: 1.7659043622759762
Validation loss: 2.559123070396345

Epoch: 6| Step: 4
Training loss: 2.4153703523277352
Validation loss: 2.5442142590909884

Epoch: 6| Step: 5
Training loss: 3.0496750705425444
Validation loss: 2.548393498923195

Epoch: 6| Step: 6
Training loss: 1.6934834119166233
Validation loss: 2.54200694406813

Epoch: 6| Step: 7
Training loss: 1.9400444550680742
Validation loss: 2.524988428095623

Epoch: 6| Step: 8
Training loss: 2.152594965347375
Validation loss: 2.5125948187426688

Epoch: 6| Step: 9
Training loss: 2.9312499089027506
Validation loss: 2.5132206391012395

Epoch: 6| Step: 10
Training loss: 1.870638160748488
Validation loss: 2.518376900653498

Epoch: 6| Step: 11
Training loss: 2.9932327237740597
Validation loss: 2.518565605488

Epoch: 6| Step: 12
Training loss: 2.090115595454486
Validation loss: 2.5046553659073307

Epoch: 6| Step: 13
Training loss: 1.9304420138969611
Validation loss: 2.5046840497153973

Epoch: 240| Step: 0
Training loss: 2.7529162636301394
Validation loss: 2.5069891032441265

Epoch: 6| Step: 1
Training loss: 2.393967828226906
Validation loss: 2.503288125300007

Epoch: 6| Step: 2
Training loss: 2.772972046384842
Validation loss: 2.503226455238936

Epoch: 6| Step: 3
Training loss: 3.0367264190333603
Validation loss: 2.5027192268292215

Epoch: 6| Step: 4
Training loss: 2.203933242908409
Validation loss: 2.505238813715804

Epoch: 6| Step: 5
Training loss: 1.367779761504997
Validation loss: 2.5171987535291036

Epoch: 6| Step: 6
Training loss: 2.0005366082344582
Validation loss: 2.5188767955786817

Epoch: 6| Step: 7
Training loss: 1.7631255155858068
Validation loss: 2.5370034500888514

Epoch: 6| Step: 8
Training loss: 2.740770631441968
Validation loss: 2.527515592240845

Epoch: 6| Step: 9
Training loss: 2.3738093152100546
Validation loss: 2.51455500654454

Epoch: 6| Step: 10
Training loss: 1.8850925300411692
Validation loss: 2.521204177649348

Epoch: 6| Step: 11
Training loss: 1.9702595569779107
Validation loss: 2.5174721359382763

Epoch: 6| Step: 12
Training loss: 2.2356542379863154
Validation loss: 2.4884868319316755

Epoch: 6| Step: 13
Training loss: 2.2599329511435844
Validation loss: 2.499770249300101

Epoch: 241| Step: 0
Training loss: 2.379857166075395
Validation loss: 2.4821153677123533

Epoch: 6| Step: 1
Training loss: 2.2136709616803745
Validation loss: 2.4827549848702697

Epoch: 6| Step: 2
Training loss: 2.287278453333458
Validation loss: 2.4748994235272512

Epoch: 6| Step: 3
Training loss: 1.9721372015929162
Validation loss: 2.4809668493899326

Epoch: 6| Step: 4
Training loss: 1.9563486580148
Validation loss: 2.4845913007131757

Epoch: 6| Step: 5
Training loss: 1.4015195757056411
Validation loss: 2.4733245492875167

Epoch: 6| Step: 6
Training loss: 2.237162844866664
Validation loss: 2.4904068791098903

Epoch: 6| Step: 7
Training loss: 3.2039881589958847
Validation loss: 2.4718188916448236

Epoch: 6| Step: 8
Training loss: 1.8103447958753611
Validation loss: 2.479431824702481

Epoch: 6| Step: 9
Training loss: 2.320968586549274
Validation loss: 2.4763685726127096

Epoch: 6| Step: 10
Training loss: 1.7276821734903778
Validation loss: 2.478269884990033

Epoch: 6| Step: 11
Training loss: 2.6862633987312714
Validation loss: 2.4888094626347868

Epoch: 6| Step: 12
Training loss: 2.8509085963652185
Validation loss: 2.4865237366590227

Epoch: 6| Step: 13
Training loss: 2.2668786099397003
Validation loss: 2.487642071371271

Epoch: 242| Step: 0
Training loss: 2.7349178102407343
Validation loss: 2.4808637249483554

Epoch: 6| Step: 1
Training loss: 2.2729176979050947
Validation loss: 2.4833067510662787

Epoch: 6| Step: 2
Training loss: 2.5368843947953468
Validation loss: 2.4978942983568073

Epoch: 6| Step: 3
Training loss: 2.510974162604979
Validation loss: 2.4977140624978413

Epoch: 6| Step: 4
Training loss: 1.8007190221928593
Validation loss: 2.506236499169723

Epoch: 6| Step: 5
Training loss: 2.1744722712668243
Validation loss: 2.49023721532722

Epoch: 6| Step: 6
Training loss: 2.94412911923916
Validation loss: 2.487251096441935

Epoch: 6| Step: 7
Training loss: 2.532213192634874
Validation loss: 2.498898645037727

Epoch: 6| Step: 8
Training loss: 2.674996499032555
Validation loss: 2.5063567724656033

Epoch: 6| Step: 9
Training loss: 1.741610033614239
Validation loss: 2.4962949636883214

Epoch: 6| Step: 10
Training loss: 1.89073288034398
Validation loss: 2.4766345570186905

Epoch: 6| Step: 11
Training loss: 1.788956151741569
Validation loss: 2.516200061093453

Epoch: 6| Step: 12
Training loss: 2.1112301051194526
Validation loss: 2.48417993895389

Epoch: 6| Step: 13
Training loss: 1.7451790754362648
Validation loss: 2.4934998886386808

Epoch: 243| Step: 0
Training loss: 2.894924286427109
Validation loss: 2.4857332527337967

Epoch: 6| Step: 1
Training loss: 2.0084577777765418
Validation loss: 2.487928284623438

Epoch: 6| Step: 2
Training loss: 1.59675688484061
Validation loss: 2.4810931203398408

Epoch: 6| Step: 3
Training loss: 2.182186539939419
Validation loss: 2.5011246853456357

Epoch: 6| Step: 4
Training loss: 2.8091281599968183
Validation loss: 2.490870000961361

Epoch: 6| Step: 5
Training loss: 1.4514831784095228
Validation loss: 2.4799874146588223

Epoch: 6| Step: 6
Training loss: 2.4152730233797213
Validation loss: 2.4833144637378757

Epoch: 6| Step: 7
Training loss: 2.2836339330384177
Validation loss: 2.4897178602915178

Epoch: 6| Step: 8
Training loss: 2.5590521714801793
Validation loss: 2.484752866211188

Epoch: 6| Step: 9
Training loss: 2.0716457464502467
Validation loss: 2.4827588860797025

Epoch: 6| Step: 10
Training loss: 2.028060521774552
Validation loss: 2.48779355973476

Epoch: 6| Step: 11
Training loss: 2.6718949880465805
Validation loss: 2.4843223454235694

Epoch: 6| Step: 12
Training loss: 1.9574923704931624
Validation loss: 2.4934342473648683

Epoch: 6| Step: 13
Training loss: 2.4983641040511153
Validation loss: 2.5050324809848643

Epoch: 244| Step: 0
Training loss: 2.2807241382414225
Validation loss: 2.500658838560699

Epoch: 6| Step: 1
Training loss: 2.3878945109583647
Validation loss: 2.5105102859283317

Epoch: 6| Step: 2
Training loss: 1.6855195751609449
Validation loss: 2.516363268578625

Epoch: 6| Step: 3
Training loss: 2.5360156749397347
Validation loss: 2.5117177374933157

Epoch: 6| Step: 4
Training loss: 1.420776949273672
Validation loss: 2.5155832986159274

Epoch: 6| Step: 5
Training loss: 2.109805366976641
Validation loss: 2.5002478079366495

Epoch: 6| Step: 6
Training loss: 2.4540150965293877
Validation loss: 2.508703182577471

Epoch: 6| Step: 7
Training loss: 1.9384006744827564
Validation loss: 2.4931701907122306

Epoch: 6| Step: 8
Training loss: 2.111952499679119
Validation loss: 2.484896047401992

Epoch: 6| Step: 9
Training loss: 2.666709859816105
Validation loss: 2.480841509026367

Epoch: 6| Step: 10
Training loss: 3.01749437333209
Validation loss: 2.487811880192677

Epoch: 6| Step: 11
Training loss: 2.1789434660273566
Validation loss: 2.4817624140326773

Epoch: 6| Step: 12
Training loss: 2.8220387087086007
Validation loss: 2.4900714496845464

Epoch: 6| Step: 13
Training loss: 1.510235830421451
Validation loss: 2.493146211704929

Epoch: 245| Step: 0
Training loss: 2.5135307361308734
Validation loss: 2.4883641698765793

Epoch: 6| Step: 1
Training loss: 2.252743531842355
Validation loss: 2.4846142987497757

Epoch: 6| Step: 2
Training loss: 1.9056964758520574
Validation loss: 2.4857552011376565

Epoch: 6| Step: 3
Training loss: 2.6916037875961343
Validation loss: 2.5016256292265435

Epoch: 6| Step: 4
Training loss: 2.471063423272154
Validation loss: 2.4950173155821442

Epoch: 6| Step: 5
Training loss: 2.6691596080898297
Validation loss: 2.4843222334594293

Epoch: 6| Step: 6
Training loss: 2.446776515493898
Validation loss: 2.485551047922393

Epoch: 6| Step: 7
Training loss: 1.9141053486913076
Validation loss: 2.4881232434563376

Epoch: 6| Step: 8
Training loss: 2.3687521418659925
Validation loss: 2.4843120127116682

Epoch: 6| Step: 9
Training loss: 1.9606378941937335
Validation loss: 2.510204121520723

Epoch: 6| Step: 10
Training loss: 2.050709866689197
Validation loss: 2.51358070791135

Epoch: 6| Step: 11
Training loss: 2.5813401952322614
Validation loss: 2.518609134972525

Epoch: 6| Step: 12
Training loss: 1.9343986916516331
Validation loss: 2.522543293052449

Epoch: 6| Step: 13
Training loss: 1.8663110311944473
Validation loss: 2.5182778328444

Epoch: 246| Step: 0
Training loss: 2.261726761508911
Validation loss: 2.540384099283471

Epoch: 6| Step: 1
Training loss: 2.2851688743307803
Validation loss: 2.508054878033477

Epoch: 6| Step: 2
Training loss: 2.6438403896454132
Validation loss: 2.4930636819927225

Epoch: 6| Step: 3
Training loss: 2.859505405474945
Validation loss: 2.5017159454029465

Epoch: 6| Step: 4
Training loss: 1.9499432702248896
Validation loss: 2.501186359567528

Epoch: 6| Step: 5
Training loss: 1.679798849983789
Validation loss: 2.482301570559354

Epoch: 6| Step: 6
Training loss: 1.9038681408398739
Validation loss: 2.4666162436289

Epoch: 6| Step: 7
Training loss: 1.922559042670079
Validation loss: 2.474372465038101

Epoch: 6| Step: 8
Training loss: 2.829160047775264
Validation loss: 2.47451464906525

Epoch: 6| Step: 9
Training loss: 2.1102960906710493
Validation loss: 2.4910845094523877

Epoch: 6| Step: 10
Training loss: 2.0531464225777825
Validation loss: 2.4825282560535107

Epoch: 6| Step: 11
Training loss: 1.7605806383442222
Validation loss: 2.477015264381683

Epoch: 6| Step: 12
Training loss: 2.601321097506331
Validation loss: 2.472976229967213

Epoch: 6| Step: 13
Training loss: 2.679745865344703
Validation loss: 2.4845314216553733

Epoch: 247| Step: 0
Training loss: 2.8246278163184995
Validation loss: 2.477747099365265

Epoch: 6| Step: 1
Training loss: 2.5031209538459223
Validation loss: 2.490433285816078

Epoch: 6| Step: 2
Training loss: 1.8700842951578664
Validation loss: 2.4642477710506494

Epoch: 6| Step: 3
Training loss: 2.5781509860491987
Validation loss: 2.4761023831068045

Epoch: 6| Step: 4
Training loss: 1.8073174275990904
Validation loss: 2.4622280228839504

Epoch: 6| Step: 5
Training loss: 2.4644964732204655
Validation loss: 2.4799340179853977

Epoch: 6| Step: 6
Training loss: 1.6485360644916343
Validation loss: 2.4781107430290485

Epoch: 6| Step: 7
Training loss: 1.8991024909140881
Validation loss: 2.4751973124886093

Epoch: 6| Step: 8
Training loss: 2.2868293571243914
Validation loss: 2.498636414269776

Epoch: 6| Step: 9
Training loss: 2.692998539666064
Validation loss: 2.4886029014116553

Epoch: 6| Step: 10
Training loss: 2.3231202679232603
Validation loss: 2.4705667370128874

Epoch: 6| Step: 11
Training loss: 1.9832266541115187
Validation loss: 2.50043358221839

Epoch: 6| Step: 12
Training loss: 2.2775532280975623
Validation loss: 2.475158823099985

Epoch: 6| Step: 13
Training loss: 2.3540134773938126
Validation loss: 2.491066452336521

Epoch: 248| Step: 0
Training loss: 1.7273049117699188
Validation loss: 2.483589112040575

Epoch: 6| Step: 1
Training loss: 2.4244409565658183
Validation loss: 2.512889757180436

Epoch: 6| Step: 2
Training loss: 2.24728133940246
Validation loss: 2.516928514748903

Epoch: 6| Step: 3
Training loss: 2.6986622215932994
Validation loss: 2.5142837645163967

Epoch: 6| Step: 4
Training loss: 2.369679363789627
Validation loss: 2.514744780339903

Epoch: 6| Step: 5
Training loss: 2.092205787675629
Validation loss: 2.5148377855239454

Epoch: 6| Step: 6
Training loss: 1.7915514162240747
Validation loss: 2.505631636441644

Epoch: 6| Step: 7
Training loss: 2.2849765804768407
Validation loss: 2.512983147267753

Epoch: 6| Step: 8
Training loss: 2.5845191961465956
Validation loss: 2.506191644916728

Epoch: 6| Step: 9
Training loss: 2.2355820389934404
Validation loss: 2.5321283425611942

Epoch: 6| Step: 10
Training loss: 2.454204540467774
Validation loss: 2.514262823765972

Epoch: 6| Step: 11
Training loss: 2.206326154307683
Validation loss: 2.515132532034186

Epoch: 6| Step: 12
Training loss: 1.8042827994637298
Validation loss: 2.4857634177517367

Epoch: 6| Step: 13
Training loss: 2.367414104832994
Validation loss: 2.4954071453747457

Epoch: 249| Step: 0
Training loss: 2.2939627733211583
Validation loss: 2.5132915897806902

Epoch: 6| Step: 1
Training loss: 2.6073586902749297
Validation loss: 2.4940987715150653

Epoch: 6| Step: 2
Training loss: 2.2314983069473056
Validation loss: 2.485871893694187

Epoch: 6| Step: 3
Training loss: 1.6202577940369778
Validation loss: 2.4891944060064

Epoch: 6| Step: 4
Training loss: 3.2064498392925804
Validation loss: 2.4815097176456793

Epoch: 6| Step: 5
Training loss: 2.199910517520023
Validation loss: 2.498561492954071

Epoch: 6| Step: 6
Training loss: 2.126938104357545
Validation loss: 2.491273224003493

Epoch: 6| Step: 7
Training loss: 1.9942568932842486
Validation loss: 2.4881586656462447

Epoch: 6| Step: 8
Training loss: 2.5925078577494562
Validation loss: 2.4977537872516864

Epoch: 6| Step: 9
Training loss: 2.067634779793347
Validation loss: 2.491166801664802

Epoch: 6| Step: 10
Training loss: 2.1354891012668826
Validation loss: 2.4921556428888163

Epoch: 6| Step: 11
Training loss: 1.7234057398452256
Validation loss: 2.498052569699252

Epoch: 6| Step: 12
Training loss: 2.329267797395753
Validation loss: 2.48906251149438

Epoch: 6| Step: 13
Training loss: 1.9995663888092376
Validation loss: 2.497580708703817

Epoch: 250| Step: 0
Training loss: 2.742758666978494
Validation loss: 2.493333606804953

Epoch: 6| Step: 1
Training loss: 2.0759052094591572
Validation loss: 2.5085653757466515

Epoch: 6| Step: 2
Training loss: 1.8513894302025236
Validation loss: 2.5171933073627613

Epoch: 6| Step: 3
Training loss: 2.551602895759151
Validation loss: 2.5281387878846737

Epoch: 6| Step: 4
Training loss: 1.5000297225550512
Validation loss: 2.518344538562957

Epoch: 6| Step: 5
Training loss: 2.8780418557946748
Validation loss: 2.5104554570368203

Epoch: 6| Step: 6
Training loss: 2.197948027713972
Validation loss: 2.5421716834547143

Epoch: 6| Step: 7
Training loss: 2.977169745872705
Validation loss: 2.5228129933920362

Epoch: 6| Step: 8
Training loss: 1.8942072325811579
Validation loss: 2.541090936544078

Epoch: 6| Step: 9
Training loss: 1.935224858508882
Validation loss: 2.51967638962911

Epoch: 6| Step: 10
Training loss: 1.8489380598076093
Validation loss: 2.5145792633714796

Epoch: 6| Step: 11
Training loss: 2.0248271632904205
Validation loss: 2.506720474490813

Epoch: 6| Step: 12
Training loss: 2.176561527293057
Validation loss: 2.5036811429866765

Epoch: 6| Step: 13
Training loss: 2.401866536697537
Validation loss: 2.4986083210275805

Epoch: 251| Step: 0
Training loss: 3.291106035479194
Validation loss: 2.489605130747948

Epoch: 6| Step: 1
Training loss: 2.563225899203084
Validation loss: 2.5004067725814485

Epoch: 6| Step: 2
Training loss: 2.1746871635567215
Validation loss: 2.4917492775209067

Epoch: 6| Step: 3
Training loss: 1.9297985709035734
Validation loss: 2.5123376474386054

Epoch: 6| Step: 4
Training loss: 2.8607837979474495
Validation loss: 2.4966938409824646

Epoch: 6| Step: 5
Training loss: 1.6593177024015178
Validation loss: 2.507993602452492

Epoch: 6| Step: 6
Training loss: 2.177598177495526
Validation loss: 2.533065113709105

Epoch: 6| Step: 7
Training loss: 1.453666094050258
Validation loss: 2.5033579844527964

Epoch: 6| Step: 8
Training loss: 2.1368428251795883
Validation loss: 2.499464343223698

Epoch: 6| Step: 9
Training loss: 2.131566169085754
Validation loss: 2.5093587859798956

Epoch: 6| Step: 10
Training loss: 1.9997851137115554
Validation loss: 2.5162707777408286

Epoch: 6| Step: 11
Training loss: 2.174580377899674
Validation loss: 2.510825710818924

Epoch: 6| Step: 12
Training loss: 2.341950195994787
Validation loss: 2.508360124573782

Epoch: 6| Step: 13
Training loss: 1.8768573462652884
Validation loss: 2.501450014811945

Epoch: 252| Step: 0
Training loss: 2.599754002744563
Validation loss: 2.505731339031814

Epoch: 6| Step: 1
Training loss: 2.1770835280228185
Validation loss: 2.501458600833392

Epoch: 6| Step: 2
Training loss: 1.6618507264693332
Validation loss: 2.4797155242855484

Epoch: 6| Step: 3
Training loss: 1.4327529811665038
Validation loss: 2.497444022426083

Epoch: 6| Step: 4
Training loss: 1.6009072801044966
Validation loss: 2.491483088548806

Epoch: 6| Step: 5
Training loss: 2.7601885623374
Validation loss: 2.4938007582212975

Epoch: 6| Step: 6
Training loss: 2.1015712354967726
Validation loss: 2.495088537042986

Epoch: 6| Step: 7
Training loss: 2.476166221129568
Validation loss: 2.4845185468297095

Epoch: 6| Step: 8
Training loss: 2.427640488052833
Validation loss: 2.5005449495832446

Epoch: 6| Step: 9
Training loss: 1.4863240345962523
Validation loss: 2.494270116222186

Epoch: 6| Step: 10
Training loss: 2.5344431010737973
Validation loss: 2.4965868220615013

Epoch: 6| Step: 11
Training loss: 2.3071380518171907
Validation loss: 2.5024674398417166

Epoch: 6| Step: 12
Training loss: 2.7662441616625912
Validation loss: 2.498171479371208

Epoch: 6| Step: 13
Training loss: 2.39628837797488
Validation loss: 2.4899466314686247

Epoch: 253| Step: 0
Training loss: 1.7453737417215305
Validation loss: 2.4874031597565094

Epoch: 6| Step: 1
Training loss: 2.448827490210926
Validation loss: 2.502824808662737

Epoch: 6| Step: 2
Training loss: 2.1512391844083574
Validation loss: 2.4752488207089773

Epoch: 6| Step: 3
Training loss: 2.435303701147208
Validation loss: 2.5080892027952393

Epoch: 6| Step: 4
Training loss: 2.100273446036544
Validation loss: 2.5312763730782732

Epoch: 6| Step: 5
Training loss: 2.029081037569753
Validation loss: 2.5181969236553705

Epoch: 6| Step: 6
Training loss: 2.4137582926753427
Validation loss: 2.5150708996111826

Epoch: 6| Step: 7
Training loss: 2.355961028574977
Validation loss: 2.5001917447626747

Epoch: 6| Step: 8
Training loss: 2.1641089919842327
Validation loss: 2.4847786053715937

Epoch: 6| Step: 9
Training loss: 2.3897899091628374
Validation loss: 2.489067907461202

Epoch: 6| Step: 10
Training loss: 1.791832857038872
Validation loss: 2.4964008331446985

Epoch: 6| Step: 11
Training loss: 2.7689096374645987
Validation loss: 2.489843847033193

Epoch: 6| Step: 12
Training loss: 2.3765508707879306
Validation loss: 2.494320824351082

Epoch: 6| Step: 13
Training loss: 2.0187214568869094
Validation loss: 2.4990919689358426

Epoch: 254| Step: 0
Training loss: 1.5211108315972783
Validation loss: 2.4835983117929556

Epoch: 6| Step: 1
Training loss: 1.9730573491898655
Validation loss: 2.516685957129443

Epoch: 6| Step: 2
Training loss: 2.7725093524248834
Validation loss: 2.520985028665432

Epoch: 6| Step: 3
Training loss: 2.3621466316879496
Validation loss: 2.526665307105847

Epoch: 6| Step: 4
Training loss: 1.8882313858664264
Validation loss: 2.5286428894801083

Epoch: 6| Step: 5
Training loss: 2.450563685130001
Validation loss: 2.534022472995502

Epoch: 6| Step: 6
Training loss: 2.3291007435795206
Validation loss: 2.5214487748473253

Epoch: 6| Step: 7
Training loss: 2.4148623428443483
Validation loss: 2.5312004555457053

Epoch: 6| Step: 8
Training loss: 1.514741069585424
Validation loss: 2.5245287005270813

Epoch: 6| Step: 9
Training loss: 2.1514525186634903
Validation loss: 2.5067945971545824

Epoch: 6| Step: 10
Training loss: 2.550694883324421
Validation loss: 2.5315276884498226

Epoch: 6| Step: 11
Training loss: 2.1354408696981744
Validation loss: 2.5155251050745355

Epoch: 6| Step: 12
Training loss: 2.979922823322608
Validation loss: 2.5038877933831682

Epoch: 6| Step: 13
Training loss: 2.1262683449143855
Validation loss: 2.5034812610220256

Epoch: 255| Step: 0
Training loss: 2.993922435278161
Validation loss: 2.5030821715209974

Epoch: 6| Step: 1
Training loss: 2.213367972945504
Validation loss: 2.5269270980437324

Epoch: 6| Step: 2
Training loss: 2.6292767880003582
Validation loss: 2.5104513495702037

Epoch: 6| Step: 3
Training loss: 2.801053213538083
Validation loss: 2.531370167960626

Epoch: 6| Step: 4
Training loss: 2.1012770448268383
Validation loss: 2.5051995248831362

Epoch: 6| Step: 5
Training loss: 1.858946422262429
Validation loss: 2.5271916606565834

Epoch: 6| Step: 6
Training loss: 2.267367199559692
Validation loss: 2.54662184441985

Epoch: 6| Step: 7
Training loss: 1.6988957942239888
Validation loss: 2.5415925473730208

Epoch: 6| Step: 8
Training loss: 2.2830786495688637
Validation loss: 2.513415715054686

Epoch: 6| Step: 9
Training loss: 1.906467175228889
Validation loss: 2.520587299625257

Epoch: 6| Step: 10
Training loss: 2.5486612433394873
Validation loss: 2.514350015160699

Epoch: 6| Step: 11
Training loss: 2.005948518816982
Validation loss: 2.52282223911958

Epoch: 6| Step: 12
Training loss: 2.0288820990181535
Validation loss: 2.501322126465454

Epoch: 6| Step: 13
Training loss: 2.048120367320237
Validation loss: 2.481737228011256

Epoch: 256| Step: 0
Training loss: 2.782687726401584
Validation loss: 2.476055602816942

Epoch: 6| Step: 1
Training loss: 1.3953885560703547
Validation loss: 2.469272695005743

Epoch: 6| Step: 2
Training loss: 2.7292070652735676
Validation loss: 2.478342870408747

Epoch: 6| Step: 3
Training loss: 2.103144394216913
Validation loss: 2.470515750419182

Epoch: 6| Step: 4
Training loss: 1.6283088155649208
Validation loss: 2.46653840036515

Epoch: 6| Step: 5
Training loss: 2.4247216015706403
Validation loss: 2.493455394912215

Epoch: 6| Step: 6
Training loss: 2.4299177937672196
Validation loss: 2.495166946780925

Epoch: 6| Step: 7
Training loss: 2.231334297874447
Validation loss: 2.511110626689992

Epoch: 6| Step: 8
Training loss: 2.103138385973838
Validation loss: 2.5228743106763165

Epoch: 6| Step: 9
Training loss: 2.13283966382513
Validation loss: 2.50225265262476

Epoch: 6| Step: 10
Training loss: 2.476892491446605
Validation loss: 2.5040669104520092

Epoch: 6| Step: 11
Training loss: 2.3139588162145857
Validation loss: 2.4897014212168256

Epoch: 6| Step: 12
Training loss: 2.734806484510549
Validation loss: 2.4868979768665493

Epoch: 6| Step: 13
Training loss: 2.6635571668937454
Validation loss: 2.498580402409148

Epoch: 257| Step: 0
Training loss: 1.6307037461999292
Validation loss: 2.4942920054372673

Epoch: 6| Step: 1
Training loss: 2.5564347104815877
Validation loss: 2.479512909077331

Epoch: 6| Step: 2
Training loss: 2.4663433464470184
Validation loss: 2.482980892807932

Epoch: 6| Step: 3
Training loss: 2.4021791975528726
Validation loss: 2.489781684369237

Epoch: 6| Step: 4
Training loss: 2.6750881216204934
Validation loss: 2.481104659621013

Epoch: 6| Step: 5
Training loss: 2.5329791156014774
Validation loss: 2.491810545918957

Epoch: 6| Step: 6
Training loss: 2.058252758760525
Validation loss: 2.485405746099159

Epoch: 6| Step: 7
Training loss: 2.1043648453352177
Validation loss: 2.476661463544197

Epoch: 6| Step: 8
Training loss: 2.5870683392912306
Validation loss: 2.4823513546222875

Epoch: 6| Step: 9
Training loss: 1.544728649223366
Validation loss: 2.4874949667670414

Epoch: 6| Step: 10
Training loss: 2.3325454426189265
Validation loss: 2.469059630988065

Epoch: 6| Step: 11
Training loss: 1.7568090668977605
Validation loss: 2.464333313626386

Epoch: 6| Step: 12
Training loss: 2.127102428902283
Validation loss: 2.508964115706215

Epoch: 6| Step: 13
Training loss: 2.799506457291882
Validation loss: 2.5114277403613787

Epoch: 258| Step: 0
Training loss: 2.126662781850362
Validation loss: 2.4943983900222

Epoch: 6| Step: 1
Training loss: 1.9147423607245129
Validation loss: 2.5023083123270196

Epoch: 6| Step: 2
Training loss: 2.8609286397216533
Validation loss: 2.504779395632582

Epoch: 6| Step: 3
Training loss: 2.5937924668845596
Validation loss: 2.514606672514213

Epoch: 6| Step: 4
Training loss: 2.1795411419696977
Validation loss: 2.529847042777499

Epoch: 6| Step: 5
Training loss: 1.7869035112388647
Validation loss: 2.533173156262972

Epoch: 6| Step: 6
Training loss: 2.101678440909059
Validation loss: 2.550534854222255

Epoch: 6| Step: 7
Training loss: 2.1302884435785305
Validation loss: 2.526320095382752

Epoch: 6| Step: 8
Training loss: 2.0830760288290784
Validation loss: 2.539984808448357

Epoch: 6| Step: 9
Training loss: 2.118666466610713
Validation loss: 2.513947300615794

Epoch: 6| Step: 10
Training loss: 1.71080678501456
Validation loss: 2.497711739765411

Epoch: 6| Step: 11
Training loss: 2.5725768530276967
Validation loss: 2.5135716495130023

Epoch: 6| Step: 12
Training loss: 1.7087448213801573
Validation loss: 2.507258971101093

Epoch: 6| Step: 13
Training loss: 3.0538628677327657
Validation loss: 2.489978492904375

Epoch: 259| Step: 0
Training loss: 2.3255648265292748
Validation loss: 2.471959390015545

Epoch: 6| Step: 1
Training loss: 1.7274530107922585
Validation loss: 2.4913479338525955

Epoch: 6| Step: 2
Training loss: 1.5808755303064745
Validation loss: 2.487435588929261

Epoch: 6| Step: 3
Training loss: 2.514993149916798
Validation loss: 2.4689690858871125

Epoch: 6| Step: 4
Training loss: 2.302209278188539
Validation loss: 2.4883976244898305

Epoch: 6| Step: 5
Training loss: 1.7729002689211397
Validation loss: 2.493717246290651

Epoch: 6| Step: 6
Training loss: 2.4240006501477067
Validation loss: 2.4927034073263026

Epoch: 6| Step: 7
Training loss: 2.79363143635177
Validation loss: 2.4954869861623954

Epoch: 6| Step: 8
Training loss: 2.6942681542194014
Validation loss: 2.4718306912430843

Epoch: 6| Step: 9
Training loss: 2.415064137872753
Validation loss: 2.4907946865044104

Epoch: 6| Step: 10
Training loss: 2.033938697168706
Validation loss: 2.4877736658327643

Epoch: 6| Step: 11
Training loss: 2.2706879968826654
Validation loss: 2.4806824919956094

Epoch: 6| Step: 12
Training loss: 2.1735217760257552
Validation loss: 2.5043021537278065

Epoch: 6| Step: 13
Training loss: 2.1463132031598624
Validation loss: 2.492013971073222

Epoch: 260| Step: 0
Training loss: 2.7507469723317404
Validation loss: 2.4962332322893546

Epoch: 6| Step: 1
Training loss: 1.9761001588981861
Validation loss: 2.52154388081734

Epoch: 6| Step: 2
Training loss: 2.874552650477749
Validation loss: 2.511090664325542

Epoch: 6| Step: 3
Training loss: 2.6251042663211717
Validation loss: 2.5270808071827244

Epoch: 6| Step: 4
Training loss: 2.4517480180712314
Validation loss: 2.5225035491152177

Epoch: 6| Step: 5
Training loss: 1.6377522339820911
Validation loss: 2.5175231027916305

Epoch: 6| Step: 6
Training loss: 2.2570609455525292
Validation loss: 2.5255467405201273

Epoch: 6| Step: 7
Training loss: 1.8694284311181522
Validation loss: 2.5205120060523845

Epoch: 6| Step: 8
Training loss: 1.8963464594468469
Validation loss: 2.5122953695342765

Epoch: 6| Step: 9
Training loss: 1.7329232265638397
Validation loss: 2.5163558782798114

Epoch: 6| Step: 10
Training loss: 2.751325374612404
Validation loss: 2.513971816206147

Epoch: 6| Step: 11
Training loss: 1.8439941244631686
Validation loss: 2.5091962947846684

Epoch: 6| Step: 12
Training loss: 2.104843018357305
Validation loss: 2.5247922081632894

Epoch: 6| Step: 13
Training loss: 2.156703845926562
Validation loss: 2.5319330526172266

Epoch: 261| Step: 0
Training loss: 2.7486213349348225
Validation loss: 2.530583564718678

Epoch: 6| Step: 1
Training loss: 1.9816421553900803
Validation loss: 2.531719729319685

Epoch: 6| Step: 2
Training loss: 1.5481517416438964
Validation loss: 2.5202027207281303

Epoch: 6| Step: 3
Training loss: 1.9898795128773608
Validation loss: 2.518456455034259

Epoch: 6| Step: 4
Training loss: 1.7644633294756944
Validation loss: 2.521723886821023

Epoch: 6| Step: 5
Training loss: 1.528038549805725
Validation loss: 2.491305140248585

Epoch: 6| Step: 6
Training loss: 2.6660859151104805
Validation loss: 2.5351391337658904

Epoch: 6| Step: 7
Training loss: 2.5843325702546873
Validation loss: 2.5202879013497648

Epoch: 6| Step: 8
Training loss: 2.1933135030404634
Validation loss: 2.509262378868467

Epoch: 6| Step: 9
Training loss: 2.0083672020722143
Validation loss: 2.4873818169766593

Epoch: 6| Step: 10
Training loss: 3.307647029177827
Validation loss: 2.4831112860502476

Epoch: 6| Step: 11
Training loss: 2.33676336992593
Validation loss: 2.4747939508896795

Epoch: 6| Step: 12
Training loss: 1.5523664314444532
Validation loss: 2.487282577034955

Epoch: 6| Step: 13
Training loss: 2.076407159946761
Validation loss: 2.485358661274243

Epoch: 262| Step: 0
Training loss: 2.459936513624287
Validation loss: 2.4775867129284395

Epoch: 6| Step: 1
Training loss: 2.0218086903943706
Validation loss: 2.489283616820307

Epoch: 6| Step: 2
Training loss: 2.4025585047557976
Validation loss: 2.491666223107068

Epoch: 6| Step: 3
Training loss: 2.428800463895791
Validation loss: 2.490925452387038

Epoch: 6| Step: 4
Training loss: 1.9729774742329336
Validation loss: 2.499505963784317

Epoch: 6| Step: 5
Training loss: 2.322824781393847
Validation loss: 2.5159907736535807

Epoch: 6| Step: 6
Training loss: 2.3188430808579046
Validation loss: 2.5302636536389786

Epoch: 6| Step: 7
Training loss: 2.7109637080528013
Validation loss: 2.519134884590797

Epoch: 6| Step: 8
Training loss: 2.5462734705611543
Validation loss: 2.533645250124611

Epoch: 6| Step: 9
Training loss: 1.8096952112185423
Validation loss: 2.5223971211248104

Epoch: 6| Step: 10
Training loss: 2.579110991617088
Validation loss: 2.5364577484652386

Epoch: 6| Step: 11
Training loss: 1.920894900126223
Validation loss: 2.510537446679336

Epoch: 6| Step: 12
Training loss: 1.7946624402331333
Validation loss: 2.5179772446603805

Epoch: 6| Step: 13
Training loss: 2.1312261138203943
Validation loss: 2.4897805991022697

Epoch: 263| Step: 0
Training loss: 2.7616363038270406
Validation loss: 2.5242047791884943

Epoch: 6| Step: 1
Training loss: 1.8062667872298876
Validation loss: 2.5147878150214145

Epoch: 6| Step: 2
Training loss: 1.9003634632204283
Validation loss: 2.5075085654830302

Epoch: 6| Step: 3
Training loss: 1.7581302609926035
Validation loss: 2.484780300516633

Epoch: 6| Step: 4
Training loss: 3.138979352549555
Validation loss: 2.534724319724227

Epoch: 6| Step: 5
Training loss: 2.0839159595875105
Validation loss: 2.5075451162684317

Epoch: 6| Step: 6
Training loss: 2.636383351787404
Validation loss: 2.530543507383186

Epoch: 6| Step: 7
Training loss: 1.9276307299423472
Validation loss: 2.5236066952446716

Epoch: 6| Step: 8
Training loss: 2.6583667175046277
Validation loss: 2.5190929258277643

Epoch: 6| Step: 9
Training loss: 2.2243181662655758
Validation loss: 2.5041119931854245

Epoch: 6| Step: 10
Training loss: 1.7568212130037133
Validation loss: 2.487670999269249

Epoch: 6| Step: 11
Training loss: 2.309979689207414
Validation loss: 2.4664556247625313

Epoch: 6| Step: 12
Training loss: 1.7651547506210714
Validation loss: 2.525077578168047

Epoch: 6| Step: 13
Training loss: 1.8528762212956567
Validation loss: 2.502151723578692

Epoch: 264| Step: 0
Training loss: 2.4884805404886383
Validation loss: 2.495167098071833

Epoch: 6| Step: 1
Training loss: 2.8704810079036998
Validation loss: 2.4913115202585363

Epoch: 6| Step: 2
Training loss: 2.751349031571653
Validation loss: 2.503384532324472

Epoch: 6| Step: 3
Training loss: 1.6267617285941578
Validation loss: 2.5033823339015453

Epoch: 6| Step: 4
Training loss: 2.1018876175242243
Validation loss: 2.511604650584443

Epoch: 6| Step: 5
Training loss: 1.3685562693435807
Validation loss: 2.4962843940057864

Epoch: 6| Step: 6
Training loss: 2.208845355097177
Validation loss: 2.505455692210843

Epoch: 6| Step: 7
Training loss: 1.8174890715678471
Validation loss: 2.5075387696387375

Epoch: 6| Step: 8
Training loss: 2.3661933044960515
Validation loss: 2.5226272368689826

Epoch: 6| Step: 9
Training loss: 2.0700063803017
Validation loss: 2.5477572202243612

Epoch: 6| Step: 10
Training loss: 2.5872399314620322
Validation loss: 2.5283645698698227

Epoch: 6| Step: 11
Training loss: 1.8615153839926515
Validation loss: 2.527236252257056

Epoch: 6| Step: 12
Training loss: 2.3329509921397995
Validation loss: 2.5125511297081267

Epoch: 6| Step: 13
Training loss: 2.224556859495925
Validation loss: 2.503867955939092

Epoch: 265| Step: 0
Training loss: 2.719851303664263
Validation loss: 2.4726505851982035

Epoch: 6| Step: 1
Training loss: 1.7658846503892107
Validation loss: 2.4829976164281073

Epoch: 6| Step: 2
Training loss: 2.7396313526504423
Validation loss: 2.4804339549109735

Epoch: 6| Step: 3
Training loss: 2.3443010826732866
Validation loss: 2.4820384107820024

Epoch: 6| Step: 4
Training loss: 2.2549297580647942
Validation loss: 2.480532940211716

Epoch: 6| Step: 5
Training loss: 2.3073401750986497
Validation loss: 2.474156475497986

Epoch: 6| Step: 6
Training loss: 2.3370866787481233
Validation loss: 2.4858059151169285

Epoch: 6| Step: 7
Training loss: 2.027947896269258
Validation loss: 2.4906458535910834

Epoch: 6| Step: 8
Training loss: 1.9561602392109207
Validation loss: 2.472480048499948

Epoch: 6| Step: 9
Training loss: 2.3909052266954354
Validation loss: 2.4770988260677123

Epoch: 6| Step: 10
Training loss: 2.353041275784263
Validation loss: 2.475543770314983

Epoch: 6| Step: 11
Training loss: 2.1421160551771363
Validation loss: 2.4927635045421437

Epoch: 6| Step: 12
Training loss: 2.4662084897120486
Validation loss: 2.4910943076161756

Epoch: 6| Step: 13
Training loss: 1.618759055093897
Validation loss: 2.4925270925444063

Epoch: 266| Step: 0
Training loss: 2.3519263508807073
Validation loss: 2.500247116591203

Epoch: 6| Step: 1
Training loss: 1.594757360101422
Validation loss: 2.4996456371931397

Epoch: 6| Step: 2
Training loss: 3.170864602000336
Validation loss: 2.504899659574194

Epoch: 6| Step: 3
Training loss: 1.9846423712398618
Validation loss: 2.533550935631095

Epoch: 6| Step: 4
Training loss: 2.2048479201622277
Validation loss: 2.5145547616047033

Epoch: 6| Step: 5
Training loss: 2.351004398057881
Validation loss: 2.532591075972953

Epoch: 6| Step: 6
Training loss: 1.8351983713096236
Validation loss: 2.536438416410002

Epoch: 6| Step: 7
Training loss: 1.4015138343348708
Validation loss: 2.5215229058073434

Epoch: 6| Step: 8
Training loss: 2.9137530533272593
Validation loss: 2.5121565101617485

Epoch: 6| Step: 9
Training loss: 2.042609859766443
Validation loss: 2.532860412395503

Epoch: 6| Step: 10
Training loss: 1.5614092262016004
Validation loss: 2.500353168337506

Epoch: 6| Step: 11
Training loss: 2.795532159933142
Validation loss: 2.4880490756147147

Epoch: 6| Step: 12
Training loss: 2.2967217841914356
Validation loss: 2.478082705726505

Epoch: 6| Step: 13
Training loss: 1.8290707350870896
Validation loss: 2.4928874564517067

Epoch: 267| Step: 0
Training loss: 1.7794795021104897
Validation loss: 2.485774335895351

Epoch: 6| Step: 1
Training loss: 2.621109941094273
Validation loss: 2.4878264310443785

Epoch: 6| Step: 2
Training loss: 1.6993095286006372
Validation loss: 2.48215871196857

Epoch: 6| Step: 3
Training loss: 2.1268768997475354
Validation loss: 2.484995814396015

Epoch: 6| Step: 4
Training loss: 2.1410735836720423
Validation loss: 2.4735345149498427

Epoch: 6| Step: 5
Training loss: 2.461327900271276
Validation loss: 2.4822919898140317

Epoch: 6| Step: 6
Training loss: 2.1958652221371655
Validation loss: 2.4948775104786765

Epoch: 6| Step: 7
Training loss: 1.7297099841688768
Validation loss: 2.4878733573606215

Epoch: 6| Step: 8
Training loss: 2.134150829244009
Validation loss: 2.4947507585477915

Epoch: 6| Step: 9
Training loss: 2.5571226127566002
Validation loss: 2.4958924962091533

Epoch: 6| Step: 10
Training loss: 2.6969759465815057
Validation loss: 2.519062229237402

Epoch: 6| Step: 11
Training loss: 2.003825225084345
Validation loss: 2.5228697587893922

Epoch: 6| Step: 12
Training loss: 2.6437892577230335
Validation loss: 2.5086702046662963

Epoch: 6| Step: 13
Training loss: 2.001439649279551
Validation loss: 2.5326382005394987

Epoch: 268| Step: 0
Training loss: 1.9484481358486403
Validation loss: 2.5472706524474122

Epoch: 6| Step: 1
Training loss: 1.681567537179324
Validation loss: 2.5064824143203803

Epoch: 6| Step: 2
Training loss: 1.7639251898820676
Validation loss: 2.5121004833961154

Epoch: 6| Step: 3
Training loss: 3.051383258264743
Validation loss: 2.493849946252383

Epoch: 6| Step: 4
Training loss: 1.9139143905946123
Validation loss: 2.5090999289406213

Epoch: 6| Step: 5
Training loss: 2.1625907658398074
Validation loss: 2.4940737260028034

Epoch: 6| Step: 6
Training loss: 2.0906030719331254
Validation loss: 2.510805208099508

Epoch: 6| Step: 7
Training loss: 2.1986282667126766
Validation loss: 2.49852011112417

Epoch: 6| Step: 8
Training loss: 2.2304685362166055
Validation loss: 2.4899168602469803

Epoch: 6| Step: 9
Training loss: 1.7831013745528808
Validation loss: 2.496741483935669

Epoch: 6| Step: 10
Training loss: 2.1073907632449207
Validation loss: 2.4984373851694186

Epoch: 6| Step: 11
Training loss: 2.82067378469585
Validation loss: 2.492519440264251

Epoch: 6| Step: 12
Training loss: 2.6712238789960585
Validation loss: 2.506855260073361

Epoch: 6| Step: 13
Training loss: 1.971502650391444
Validation loss: 2.4769535578256248

Epoch: 269| Step: 0
Training loss: 1.9340955295106157
Validation loss: 2.5060400953411337

Epoch: 6| Step: 1
Training loss: 2.389301606931255
Validation loss: 2.490588688795111

Epoch: 6| Step: 2
Training loss: 2.3730113085683477
Validation loss: 2.4993251048515908

Epoch: 6| Step: 3
Training loss: 1.8412173705566783
Validation loss: 2.4957677143707215

Epoch: 6| Step: 4
Training loss: 2.2179458665355933
Validation loss: 2.509753829591891

Epoch: 6| Step: 5
Training loss: 1.554351617002982
Validation loss: 2.502562433557076

Epoch: 6| Step: 6
Training loss: 1.7863081339725406
Validation loss: 2.5022592032246127

Epoch: 6| Step: 7
Training loss: 2.0898711300776496
Validation loss: 2.521895875651364

Epoch: 6| Step: 8
Training loss: 2.8693829272473743
Validation loss: 2.543654115574013

Epoch: 6| Step: 9
Training loss: 2.0870909055863263
Validation loss: 2.570825001988031

Epoch: 6| Step: 10
Training loss: 2.4323181434077927
Validation loss: 2.5923022783808163

Epoch: 6| Step: 11
Training loss: 2.5134468837488404
Validation loss: 2.5579724049982366

Epoch: 6| Step: 12
Training loss: 2.603489067932351
Validation loss: 2.5130371224990546

Epoch: 6| Step: 13
Training loss: 2.0505027945256904
Validation loss: 2.5065467468535245

Epoch: 270| Step: 0
Training loss: 2.4087535170414127
Validation loss: 2.4908891283331

Epoch: 6| Step: 1
Training loss: 2.0429659491496097
Validation loss: 2.505901935916653

Epoch: 6| Step: 2
Training loss: 2.6677623127147427
Validation loss: 2.494963563692512

Epoch: 6| Step: 3
Training loss: 1.1038770235786297
Validation loss: 2.481560990760997

Epoch: 6| Step: 4
Training loss: 1.9372477828724601
Validation loss: 2.5056746611253424

Epoch: 6| Step: 5
Training loss: 2.530074988901098
Validation loss: 2.4885839560614538

Epoch: 6| Step: 6
Training loss: 1.622685324454784
Validation loss: 2.51707833525194

Epoch: 6| Step: 7
Training loss: 3.125198205384262
Validation loss: 2.5019633054464117

Epoch: 6| Step: 8
Training loss: 2.466761500451526
Validation loss: 2.511322306605834

Epoch: 6| Step: 9
Training loss: 2.3502359799341326
Validation loss: 2.5019001256573863

Epoch: 6| Step: 10
Training loss: 1.9853321080115445
Validation loss: 2.492279290209977

Epoch: 6| Step: 11
Training loss: 1.924442901751424
Validation loss: 2.5032695212614358

Epoch: 6| Step: 12
Training loss: 2.1830414157071942
Validation loss: 2.4686641839157684

Epoch: 6| Step: 13
Training loss: 2.4804727869676797
Validation loss: 2.50503492383032

Epoch: 271| Step: 0
Training loss: 2.2601289580695374
Validation loss: 2.479328219709109

Epoch: 6| Step: 1
Training loss: 2.136503832955809
Validation loss: 2.4843075821084155

Epoch: 6| Step: 2
Training loss: 3.085234202146133
Validation loss: 2.508560987979523

Epoch: 6| Step: 3
Training loss: 2.1821573681958717
Validation loss: 2.512071045907558

Epoch: 6| Step: 4
Training loss: 2.7801189051385884
Validation loss: 2.541739546142644

Epoch: 6| Step: 5
Training loss: 2.5560086535096875
Validation loss: 2.5041733082087116

Epoch: 6| Step: 6
Training loss: 2.6264708803117793
Validation loss: 2.5321032965733488

Epoch: 6| Step: 7
Training loss: 2.3694330524003266
Validation loss: 2.51432366999189

Epoch: 6| Step: 8
Training loss: 1.9150737971336815
Validation loss: 2.5009027122865914

Epoch: 6| Step: 9
Training loss: 2.3941968774621096
Validation loss: 2.5067656125372877

Epoch: 6| Step: 10
Training loss: 1.9710007169694144
Validation loss: 2.4910723225086886

Epoch: 6| Step: 11
Training loss: 2.171289708153801
Validation loss: 2.4794757767853275

Epoch: 6| Step: 12
Training loss: 1.4907412885494205
Validation loss: 2.471694204709071

Epoch: 6| Step: 13
Training loss: 1.5005733665725234
Validation loss: 2.4599717117120066

Epoch: 272| Step: 0
Training loss: 2.724902502993065
Validation loss: 2.4855091617082583

Epoch: 6| Step: 1
Training loss: 1.5869927865088589
Validation loss: 2.486313453505171

Epoch: 6| Step: 2
Training loss: 1.6565250672142955
Validation loss: 2.4825708968330242

Epoch: 6| Step: 3
Training loss: 2.395514082715754
Validation loss: 2.4790085226530287

Epoch: 6| Step: 4
Training loss: 2.56764523400907
Validation loss: 2.4813393173989455

Epoch: 6| Step: 5
Training loss: 2.544594802976346
Validation loss: 2.484611316053983

Epoch: 6| Step: 6
Training loss: 2.238623574594002
Validation loss: 2.476970634966061

Epoch: 6| Step: 7
Training loss: 1.9524168638145445
Validation loss: 2.48246789504923

Epoch: 6| Step: 8
Training loss: 2.3124247358291496
Validation loss: 2.465807033792629

Epoch: 6| Step: 9
Training loss: 2.263508827847843
Validation loss: 2.4776610496160005

Epoch: 6| Step: 10
Training loss: 2.805869700739301
Validation loss: 2.5006715349454924

Epoch: 6| Step: 11
Training loss: 1.7111436554193256
Validation loss: 2.5080785877563847

Epoch: 6| Step: 12
Training loss: 1.9228008490249286
Validation loss: 2.4970352872565993

Epoch: 6| Step: 13
Training loss: 2.2545521569292224
Validation loss: 2.5091675517033036

Epoch: 273| Step: 0
Training loss: 2.3657397384735437
Validation loss: 2.5295075985433133

Epoch: 6| Step: 1
Training loss: 3.2273251276487813
Validation loss: 2.524736981179895

Epoch: 6| Step: 2
Training loss: 2.1409945203843743
Validation loss: 2.5312716950165792

Epoch: 6| Step: 3
Training loss: 2.0765456314492505
Validation loss: 2.5969763991921684

Epoch: 6| Step: 4
Training loss: 1.465232695759335
Validation loss: 2.6205188406868856

Epoch: 6| Step: 5
Training loss: 2.0271292335930386
Validation loss: 2.586650761546397

Epoch: 6| Step: 6
Training loss: 2.949114947987011
Validation loss: 2.5572556274940874

Epoch: 6| Step: 7
Training loss: 1.880614204312364
Validation loss: 2.5092213952227755

Epoch: 6| Step: 8
Training loss: 2.1675675304413096
Validation loss: 2.5195555220033636

Epoch: 6| Step: 9
Training loss: 2.0886581908434634
Validation loss: 2.5008064240945647

Epoch: 6| Step: 10
Training loss: 2.2502717807657793
Validation loss: 2.5015385502898457

Epoch: 6| Step: 11
Training loss: 2.3253333232220523
Validation loss: 2.4969635245521364

Epoch: 6| Step: 12
Training loss: 1.3215134705698781
Validation loss: 2.50010313774984

Epoch: 6| Step: 13
Training loss: 2.2044978632701366
Validation loss: 2.5049064491351496

Epoch: 274| Step: 0
Training loss: 2.4082772768705976
Validation loss: 2.513702352873052

Epoch: 6| Step: 1
Training loss: 2.6250139871860423
Validation loss: 2.507693524573764

Epoch: 6| Step: 2
Training loss: 2.30376682318818
Validation loss: 2.493437370902367

Epoch: 6| Step: 3
Training loss: 1.9357115582138738
Validation loss: 2.5178135578566314

Epoch: 6| Step: 4
Training loss: 2.4233521909474134
Validation loss: 2.5257184690332117

Epoch: 6| Step: 5
Training loss: 1.7777773150138783
Validation loss: 2.509663003092545

Epoch: 6| Step: 6
Training loss: 2.563045722723264
Validation loss: 2.5105485103275997

Epoch: 6| Step: 7
Training loss: 1.9055366432071954
Validation loss: 2.5383548161162595

Epoch: 6| Step: 8
Training loss: 1.6265260426652448
Validation loss: 2.531675938691201

Epoch: 6| Step: 9
Training loss: 1.8070358916213558
Validation loss: 2.5378516024078546

Epoch: 6| Step: 10
Training loss: 2.6191114954569548
Validation loss: 2.5377698689038923

Epoch: 6| Step: 11
Training loss: 1.8996846062747523
Validation loss: 2.523486629988064

Epoch: 6| Step: 12
Training loss: 2.705867192281999
Validation loss: 2.529493782302746

Epoch: 6| Step: 13
Training loss: 1.9266463188012075
Validation loss: 2.5195235693368474

Epoch: 275| Step: 0
Training loss: 1.4312023204874211
Validation loss: 2.498499316575427

Epoch: 6| Step: 1
Training loss: 2.0964665795429758
Validation loss: 2.49423556145893

Epoch: 6| Step: 2
Training loss: 3.015228561670536
Validation loss: 2.4862197169113327

Epoch: 6| Step: 3
Training loss: 1.485993476719869
Validation loss: 2.4718072366729897

Epoch: 6| Step: 4
Training loss: 2.8239885291766225
Validation loss: 2.473568796643981

Epoch: 6| Step: 5
Training loss: 2.118069735898214
Validation loss: 2.4763039454964866

Epoch: 6| Step: 6
Training loss: 2.1362967065623724
Validation loss: 2.483243008598784

Epoch: 6| Step: 7
Training loss: 1.872696160506668
Validation loss: 2.493763615595744

Epoch: 6| Step: 8
Training loss: 2.008728886928353
Validation loss: 2.486972818150165

Epoch: 6| Step: 9
Training loss: 1.8804242194196124
Validation loss: 2.53668463061067

Epoch: 6| Step: 10
Training loss: 2.2266250333872204
Validation loss: 2.5069787530027865

Epoch: 6| Step: 11
Training loss: 2.8349834760557737
Validation loss: 2.507706708251278

Epoch: 6| Step: 12
Training loss: 1.3985228698928096
Validation loss: 2.5168465991044355

Epoch: 6| Step: 13
Training loss: 2.698925659343111
Validation loss: 2.4983391410610833

Epoch: 276| Step: 0
Training loss: 1.6240843614079814
Validation loss: 2.5099038092227968

Epoch: 6| Step: 1
Training loss: 1.7146254554787421
Validation loss: 2.5063289005230027

Epoch: 6| Step: 2
Training loss: 1.5184177741735867
Validation loss: 2.512601595417166

Epoch: 6| Step: 3
Training loss: 2.851694098793428
Validation loss: 2.512533084531359

Epoch: 6| Step: 4
Training loss: 2.171805511021615
Validation loss: 2.5001994212403664

Epoch: 6| Step: 5
Training loss: 2.257505825030985
Validation loss: 2.506716178606279

Epoch: 6| Step: 6
Training loss: 2.687442512784503
Validation loss: 2.510418845738986

Epoch: 6| Step: 7
Training loss: 1.9777819825721838
Validation loss: 2.5019894154815447

Epoch: 6| Step: 8
Training loss: 3.106551378700481
Validation loss: 2.4869896906818125

Epoch: 6| Step: 9
Training loss: 1.9653323951863018
Validation loss: 2.515252182465933

Epoch: 6| Step: 10
Training loss: 1.5506986120387896
Validation loss: 2.5311846705039827

Epoch: 6| Step: 11
Training loss: 2.290864931371787
Validation loss: 2.5179690700415995

Epoch: 6| Step: 12
Training loss: 2.134535097342536
Validation loss: 2.5159715212406457

Epoch: 6| Step: 13
Training loss: 1.8618949035215826
Validation loss: 2.507985046725436

Epoch: 277| Step: 0
Training loss: 1.797969716844989
Validation loss: 2.4935321190030533

Epoch: 6| Step: 1
Training loss: 1.4855603755364553
Validation loss: 2.476381297256473

Epoch: 6| Step: 2
Training loss: 2.7366374220158645
Validation loss: 2.500158654582851

Epoch: 6| Step: 3
Training loss: 1.870279983327078
Validation loss: 2.48333766561382

Epoch: 6| Step: 4
Training loss: 1.6983450912702482
Validation loss: 2.486988636151938

Epoch: 6| Step: 5
Training loss: 1.8511408716114215
Validation loss: 2.4847796608393957

Epoch: 6| Step: 6
Training loss: 2.688041898520085
Validation loss: 2.483448335579063

Epoch: 6| Step: 7
Training loss: 2.490087311833993
Validation loss: 2.48746367257076

Epoch: 6| Step: 8
Training loss: 1.8193299093264328
Validation loss: 2.5091882657587177

Epoch: 6| Step: 9
Training loss: 2.468445674633027
Validation loss: 2.4811223408026883

Epoch: 6| Step: 10
Training loss: 1.794115152973439
Validation loss: 2.521019153734557

Epoch: 6| Step: 11
Training loss: 3.169474477920042
Validation loss: 2.5296940357924145

Epoch: 6| Step: 12
Training loss: 1.842634348429036
Validation loss: 2.5202910152559355

Epoch: 6| Step: 13
Training loss: 2.0277624855451943
Validation loss: 2.5091163517791157

Epoch: 278| Step: 0
Training loss: 2.4586292886120105
Validation loss: 2.503872748679979

Epoch: 6| Step: 1
Training loss: 2.220319595701408
Validation loss: 2.513182392099075

Epoch: 6| Step: 2
Training loss: 2.0055942973125442
Validation loss: 2.543508508514917

Epoch: 6| Step: 3
Training loss: 2.2568961531208473
Validation loss: 2.5289333872752744

Epoch: 6| Step: 4
Training loss: 2.4451348480920236
Validation loss: 2.537085145634441

Epoch: 6| Step: 5
Training loss: 2.100988940849412
Validation loss: 2.5581480924246107

Epoch: 6| Step: 6
Training loss: 2.704559033988509
Validation loss: 2.5254985317179885

Epoch: 6| Step: 7
Training loss: 1.6333833271601692
Validation loss: 2.501629838547446

Epoch: 6| Step: 8
Training loss: 2.158286307962846
Validation loss: 2.5096858584620647

Epoch: 6| Step: 9
Training loss: 1.7349735636167414
Validation loss: 2.502425797080383

Epoch: 6| Step: 10
Training loss: 2.8938606916641287
Validation loss: 2.5031574020161798

Epoch: 6| Step: 11
Training loss: 1.9042712713623322
Validation loss: 2.48937370237048

Epoch: 6| Step: 12
Training loss: 1.5176636660707
Validation loss: 2.4910529094137233

Epoch: 6| Step: 13
Training loss: 2.0862593384677157
Validation loss: 2.4833465622756505

Epoch: 279| Step: 0
Training loss: 1.3869469683189322
Validation loss: 2.48368201997413

Epoch: 6| Step: 1
Training loss: 2.4838714089211438
Validation loss: 2.4662891146249915

Epoch: 6| Step: 2
Training loss: 2.2064975329501366
Validation loss: 2.4657039118823616

Epoch: 6| Step: 3
Training loss: 2.0541801261331942
Validation loss: 2.4789666541276456

Epoch: 6| Step: 4
Training loss: 2.381882783210908
Validation loss: 2.4727761961182146

Epoch: 6| Step: 5
Training loss: 2.311906274407132
Validation loss: 2.472517285853879

Epoch: 6| Step: 6
Training loss: 1.8910920378075162
Validation loss: 2.4642788521320322

Epoch: 6| Step: 7
Training loss: 1.7872279727126401
Validation loss: 2.4735996883165954

Epoch: 6| Step: 8
Training loss: 2.606343226972355
Validation loss: 2.476563743486704

Epoch: 6| Step: 9
Training loss: 2.1880017386365047
Validation loss: 2.4661489377242263

Epoch: 6| Step: 10
Training loss: 2.176912352739542
Validation loss: 2.466761532669014

Epoch: 6| Step: 11
Training loss: 1.6709938222746965
Validation loss: 2.472467890390478

Epoch: 6| Step: 12
Training loss: 2.512873026926905
Validation loss: 2.4764718762171842

Epoch: 6| Step: 13
Training loss: 2.3953161151259006
Validation loss: 2.5231298410669853

Epoch: 280| Step: 0
Training loss: 2.3373210981316803
Validation loss: 2.492429540155421

Epoch: 6| Step: 1
Training loss: 1.826592479554415
Validation loss: 2.512308956113847

Epoch: 6| Step: 2
Training loss: 2.650363098316433
Validation loss: 2.509987368355587

Epoch: 6| Step: 3
Training loss: 2.799062374345394
Validation loss: 2.52020421860752

Epoch: 6| Step: 4
Training loss: 2.1623656302805396
Validation loss: 2.521317007359273

Epoch: 6| Step: 5
Training loss: 2.3310584718300347
Validation loss: 2.5300113017567343

Epoch: 6| Step: 6
Training loss: 2.662090836010511
Validation loss: 2.4855897920420396

Epoch: 6| Step: 7
Training loss: 1.5580562726045215
Validation loss: 2.486527659921724

Epoch: 6| Step: 8
Training loss: 1.7496285044226272
Validation loss: 2.4720001153673272

Epoch: 6| Step: 9
Training loss: 1.8014970461492703
Validation loss: 2.471641231756683

Epoch: 6| Step: 10
Training loss: 1.6262088827320091
Validation loss: 2.495267370075062

Epoch: 6| Step: 11
Training loss: 2.6287024590460177
Validation loss: 2.500788230612158

Epoch: 6| Step: 12
Training loss: 1.8723965690374367
Validation loss: 2.4872799250440103

Epoch: 6| Step: 13
Training loss: 1.7182366037877523
Validation loss: 2.4847297015384666

Epoch: 281| Step: 0
Training loss: 1.7864596160201944
Validation loss: 2.4729210268371586

Epoch: 6| Step: 1
Training loss: 2.641010504577846
Validation loss: 2.469144322897933

Epoch: 6| Step: 2
Training loss: 2.0094063811276706
Validation loss: 2.4510016353097974

Epoch: 6| Step: 3
Training loss: 2.3180478536687272
Validation loss: 2.462036839931319

Epoch: 6| Step: 4
Training loss: 1.3890412130537704
Validation loss: 2.4676292386361345

Epoch: 6| Step: 5
Training loss: 2.207452946438992
Validation loss: 2.4684032888843164

Epoch: 6| Step: 6
Training loss: 2.233306415832156
Validation loss: 2.4590551695910543

Epoch: 6| Step: 7
Training loss: 1.954716636629617
Validation loss: 2.4814731837036907

Epoch: 6| Step: 8
Training loss: 1.952007126380857
Validation loss: 2.4884004349762394

Epoch: 6| Step: 9
Training loss: 2.4312597750195124
Validation loss: 2.469835147504421

Epoch: 6| Step: 10
Training loss: 2.485660052686604
Validation loss: 2.4885757567438085

Epoch: 6| Step: 11
Training loss: 2.9445336726202225
Validation loss: 2.504092990618677

Epoch: 6| Step: 12
Training loss: 1.502228353257117
Validation loss: 2.528388882804022

Epoch: 6| Step: 13
Training loss: 2.0022176130544938
Validation loss: 2.545504538681948

Epoch: 282| Step: 0
Training loss: 2.3236643410459767
Validation loss: 2.562523027642221

Epoch: 6| Step: 1
Training loss: 1.4688590800585033
Validation loss: 2.587146334350774

Epoch: 6| Step: 2
Training loss: 1.6034777098992674
Validation loss: 2.5706402879775574

Epoch: 6| Step: 3
Training loss: 2.398422489678447
Validation loss: 2.540983942192549

Epoch: 6| Step: 4
Training loss: 1.9837483295128369
Validation loss: 2.5301332875400258

Epoch: 6| Step: 5
Training loss: 2.176281856016677
Validation loss: 2.518656039950461

Epoch: 6| Step: 6
Training loss: 2.6377958231266487
Validation loss: 2.5347723452229665

Epoch: 6| Step: 7
Training loss: 1.48527716275393
Validation loss: 2.5268816518118102

Epoch: 6| Step: 8
Training loss: 2.2581817430619897
Validation loss: 2.5271873523996238

Epoch: 6| Step: 9
Training loss: 2.295141071646212
Validation loss: 2.5300918488422073

Epoch: 6| Step: 10
Training loss: 2.581708232856139
Validation loss: 2.530464050345136

Epoch: 6| Step: 11
Training loss: 2.4714824186329087
Validation loss: 2.5217336407844275

Epoch: 6| Step: 12
Training loss: 1.743530234978044
Validation loss: 2.5174862154570934

Epoch: 6| Step: 13
Training loss: 2.5950550335173324
Validation loss: 2.5383474115727287

Epoch: 283| Step: 0
Training loss: 2.6063942702359797
Validation loss: 2.5135410277746164

Epoch: 6| Step: 1
Training loss: 2.215751018087934
Validation loss: 2.5405443141961146

Epoch: 6| Step: 2
Training loss: 1.3915456660237844
Validation loss: 2.531120563855418

Epoch: 6| Step: 3
Training loss: 2.17831027366304
Validation loss: 2.496355547481107

Epoch: 6| Step: 4
Training loss: 2.321217986396076
Validation loss: 2.518858764168029

Epoch: 6| Step: 5
Training loss: 1.807911622702033
Validation loss: 2.5105364020405

Epoch: 6| Step: 6
Training loss: 2.1495605862498364
Validation loss: 2.507238605626157

Epoch: 6| Step: 7
Training loss: 2.593744025166076
Validation loss: 2.508380718565148

Epoch: 6| Step: 8
Training loss: 2.0265401375741905
Validation loss: 2.489062750960611

Epoch: 6| Step: 9
Training loss: 1.4665081689908164
Validation loss: 2.519306561212285

Epoch: 6| Step: 10
Training loss: 2.344443053455553
Validation loss: 2.515086596194476

Epoch: 6| Step: 11
Training loss: 2.5060157875726716
Validation loss: 2.481754776594824

Epoch: 6| Step: 12
Training loss: 2.0977010313684823
Validation loss: 2.5084767635395337

Epoch: 6| Step: 13
Training loss: 1.8663141610319676
Validation loss: 2.4886925405820763

Epoch: 284| Step: 0
Training loss: 2.45253187650755
Validation loss: 2.5000601284425636

Epoch: 6| Step: 1
Training loss: 2.58335207860565
Validation loss: 2.4980081252492026

Epoch: 6| Step: 2
Training loss: 2.291217089394286
Validation loss: 2.5612429427568952

Epoch: 6| Step: 3
Training loss: 2.4753788667712295
Validation loss: 2.5222678608436673

Epoch: 6| Step: 4
Training loss: 1.8350298646788574
Validation loss: 2.5640487642635117

Epoch: 6| Step: 5
Training loss: 1.9336762998279933
Validation loss: 2.525430968617072

Epoch: 6| Step: 6
Training loss: 1.49039883287336
Validation loss: 2.5491613879049897

Epoch: 6| Step: 7
Training loss: 2.7928820189767825
Validation loss: 2.5336174274396472

Epoch: 6| Step: 8
Training loss: 1.9551634869913712
Validation loss: 2.509302364199138

Epoch: 6| Step: 9
Training loss: 2.205407657783866
Validation loss: 2.5211517406122748

Epoch: 6| Step: 10
Training loss: 1.7316210077242913
Validation loss: 2.4800783666155066

Epoch: 6| Step: 11
Training loss: 2.0112871910815087
Validation loss: 2.4879003819171346

Epoch: 6| Step: 12
Training loss: 2.2140959161728255
Validation loss: 2.4884631191763105

Epoch: 6| Step: 13
Training loss: 1.7682672495572986
Validation loss: 2.4981866299863578

Epoch: 285| Step: 0
Training loss: 1.9126396028517052
Validation loss: 2.4916972572039358

Epoch: 6| Step: 1
Training loss: 1.885961661069473
Validation loss: 2.4836001517393425

Epoch: 6| Step: 2
Training loss: 1.8864148134055998
Validation loss: 2.476661615965542

Epoch: 6| Step: 3
Training loss: 1.63439514264446
Validation loss: 2.4943563019043644

Epoch: 6| Step: 4
Training loss: 2.237122027476443
Validation loss: 2.4906460131337522

Epoch: 6| Step: 5
Training loss: 2.155487921015291
Validation loss: 2.486768373407122

Epoch: 6| Step: 6
Training loss: 2.358601222195434
Validation loss: 2.470521484452976

Epoch: 6| Step: 7
Training loss: 1.9892620073516767
Validation loss: 2.4887286013148366

Epoch: 6| Step: 8
Training loss: 1.5246425459877107
Validation loss: 2.485078963823113

Epoch: 6| Step: 9
Training loss: 2.6735697050865426
Validation loss: 2.497253681760747

Epoch: 6| Step: 10
Training loss: 2.5323295645870503
Validation loss: 2.5170746411549874

Epoch: 6| Step: 11
Training loss: 2.272287294108417
Validation loss: 2.520488279282482

Epoch: 6| Step: 12
Training loss: 2.172412613668471
Validation loss: 2.54427788745497

Epoch: 6| Step: 13
Training loss: 2.779927657304447
Validation loss: 2.6007543077923034

Epoch: 286| Step: 0
Training loss: 2.1727710220325958
Validation loss: 2.605318796245478

Epoch: 6| Step: 1
Training loss: 1.7619413586231543
Validation loss: 2.6313260916900276

Epoch: 6| Step: 2
Training loss: 2.299001526754502
Validation loss: 2.623701728205914

Epoch: 6| Step: 3
Training loss: 2.1358231196442703
Validation loss: 2.588433727160488

Epoch: 6| Step: 4
Training loss: 2.2206410690400475
Validation loss: 2.57319529693945

Epoch: 6| Step: 5
Training loss: 1.5913649271345924
Validation loss: 2.5392558880053815

Epoch: 6| Step: 6
Training loss: 1.6945291986096893
Validation loss: 2.5034970143983966

Epoch: 6| Step: 7
Training loss: 2.775098018805291
Validation loss: 2.5132604824034632

Epoch: 6| Step: 8
Training loss: 1.518565363708451
Validation loss: 2.4846393276645347

Epoch: 6| Step: 9
Training loss: 2.2491221304808247
Validation loss: 2.485616553668471

Epoch: 6| Step: 10
Training loss: 1.9505619168676558
Validation loss: 2.474256257974492

Epoch: 6| Step: 11
Training loss: 2.178121462234842
Validation loss: 2.486037795503028

Epoch: 6| Step: 12
Training loss: 2.900489594797121
Validation loss: 2.455995131997245

Epoch: 6| Step: 13
Training loss: 2.0562168710005175
Validation loss: 2.4786743241513833

Epoch: 287| Step: 0
Training loss: 1.6129461868778872
Validation loss: 2.47966879613465

Epoch: 6| Step: 1
Training loss: 1.685805847854057
Validation loss: 2.5029518502522157

Epoch: 6| Step: 2
Training loss: 2.073610608162648
Validation loss: 2.467473122393349

Epoch: 6| Step: 3
Training loss: 2.7807357827066377
Validation loss: 2.469613274404095

Epoch: 6| Step: 4
Training loss: 2.080462066605276
Validation loss: 2.4683775158039603

Epoch: 6| Step: 5
Training loss: 2.8259303392435347
Validation loss: 2.471346105810967

Epoch: 6| Step: 6
Training loss: 2.37726334590377
Validation loss: 2.4766196435860057

Epoch: 6| Step: 7
Training loss: 2.7209710881254416
Validation loss: 2.4359744261854015

Epoch: 6| Step: 8
Training loss: 2.0412793732846475
Validation loss: 2.4563658702333133

Epoch: 6| Step: 9
Training loss: 2.068755057060768
Validation loss: 2.472276157214672

Epoch: 6| Step: 10
Training loss: 1.521423338619472
Validation loss: 2.4773104848786693

Epoch: 6| Step: 11
Training loss: 1.753561822933185
Validation loss: 2.482459683534282

Epoch: 6| Step: 12
Training loss: 1.92026011532877
Validation loss: 2.4727911246960446

Epoch: 6| Step: 13
Training loss: 1.8899849683173933
Validation loss: 2.4849975973408043

Epoch: 288| Step: 0
Training loss: 2.2520548656074744
Validation loss: 2.494229619074731

Epoch: 6| Step: 1
Training loss: 1.5450197133408416
Validation loss: 2.4922426829511757

Epoch: 6| Step: 2
Training loss: 2.5690941053403877
Validation loss: 2.5003196035178616

Epoch: 6| Step: 3
Training loss: 1.5075382912297537
Validation loss: 2.509981004154216

Epoch: 6| Step: 4
Training loss: 2.226209967294515
Validation loss: 2.518120169951742

Epoch: 6| Step: 5
Training loss: 2.2631466694837115
Validation loss: 2.508720003996328

Epoch: 6| Step: 6
Training loss: 1.7332628137964112
Validation loss: 2.473405255516977

Epoch: 6| Step: 7
Training loss: 1.6487860288580565
Validation loss: 2.4745388005888103

Epoch: 6| Step: 8
Training loss: 2.1879224096903425
Validation loss: 2.4694229930918126

Epoch: 6| Step: 9
Training loss: 3.123398484887567
Validation loss: 2.4841220894923515

Epoch: 6| Step: 10
Training loss: 2.2535529165477044
Validation loss: 2.4799072511182887

Epoch: 6| Step: 11
Training loss: 1.9385977680739308
Validation loss: 2.4879676784953966

Epoch: 6| Step: 12
Training loss: 2.5461267413576727
Validation loss: 2.463032729717803

Epoch: 6| Step: 13
Training loss: 1.9303533355163154
Validation loss: 2.502462509438457

Epoch: 289| Step: 0
Training loss: 1.9630617589033716
Validation loss: 2.5196245516275932

Epoch: 6| Step: 1
Training loss: 1.3243712390884064
Validation loss: 2.538663079561109

Epoch: 6| Step: 2
Training loss: 3.115013339533035
Validation loss: 2.5344641259417

Epoch: 6| Step: 3
Training loss: 2.627683085733105
Validation loss: 2.5582127566507458

Epoch: 6| Step: 4
Training loss: 2.037543070519294
Validation loss: 2.5796806189023407

Epoch: 6| Step: 5
Training loss: 1.9086973395637186
Validation loss: 2.5712619562262056

Epoch: 6| Step: 6
Training loss: 1.8576477747751194
Validation loss: 2.5577795465824864

Epoch: 6| Step: 7
Training loss: 2.273898048551227
Validation loss: 2.555199108166739

Epoch: 6| Step: 8
Training loss: 2.1459412655645274
Validation loss: 2.5181067409691322

Epoch: 6| Step: 9
Training loss: 1.8524018650599914
Validation loss: 2.5008482288788567

Epoch: 6| Step: 10
Training loss: 2.201108943442396
Validation loss: 2.495524134983983

Epoch: 6| Step: 11
Training loss: 1.8063258541243539
Validation loss: 2.477988553550049

Epoch: 6| Step: 12
Training loss: 1.8755815875562971
Validation loss: 2.4778992167204206

Epoch: 6| Step: 13
Training loss: 2.7926086620475954
Validation loss: 2.47802216427517

Epoch: 290| Step: 0
Training loss: 2.236400832947419
Validation loss: 2.4674289565083436

Epoch: 6| Step: 1
Training loss: 1.4466759101877074
Validation loss: 2.4926810737476464

Epoch: 6| Step: 2
Training loss: 2.573029076801488
Validation loss: 2.5086453046305666

Epoch: 6| Step: 3
Training loss: 2.086262424036588
Validation loss: 2.541535207340069

Epoch: 6| Step: 4
Training loss: 2.7007479938449133
Validation loss: 2.5495095541795534

Epoch: 6| Step: 5
Training loss: 2.250923179274802
Validation loss: 2.575263699358993

Epoch: 6| Step: 6
Training loss: 1.7878763189528963
Validation loss: 2.5786864083665932

Epoch: 6| Step: 7
Training loss: 2.2510644725919393
Validation loss: 2.581320414225443

Epoch: 6| Step: 8
Training loss: 1.9833017285283818
Validation loss: 2.595965561374317

Epoch: 6| Step: 9
Training loss: 2.397727339291356
Validation loss: 2.5885776284455466

Epoch: 6| Step: 10
Training loss: 2.5440991961015382
Validation loss: 2.530054320203328

Epoch: 6| Step: 11
Training loss: 2.5448984538806187
Validation loss: 2.5046205736572276

Epoch: 6| Step: 12
Training loss: 1.0403235677655767
Validation loss: 2.4896893392257375

Epoch: 6| Step: 13
Training loss: 2.1798229107104334
Validation loss: 2.4975613461973674

Epoch: 291| Step: 0
Training loss: 2.5277625655204417
Validation loss: 2.49411889377687

Epoch: 6| Step: 1
Training loss: 2.129633620008087
Validation loss: 2.4877389968742722

Epoch: 6| Step: 2
Training loss: 1.7800067109613673
Validation loss: 2.474843324011386

Epoch: 6| Step: 3
Training loss: 2.2550909938342647
Validation loss: 2.489004703672079

Epoch: 6| Step: 4
Training loss: 2.5137179710589854
Validation loss: 2.4956942513698666

Epoch: 6| Step: 5
Training loss: 1.7537500528565315
Validation loss: 2.490494363215384

Epoch: 6| Step: 6
Training loss: 2.117627538017606
Validation loss: 2.487422585361012

Epoch: 6| Step: 7
Training loss: 1.9884143477522909
Validation loss: 2.4852253480251476

Epoch: 6| Step: 8
Training loss: 2.1528966508538705
Validation loss: 2.47175992496093

Epoch: 6| Step: 9
Training loss: 1.8489343202849415
Validation loss: 2.4894239356010033

Epoch: 6| Step: 10
Training loss: 2.2567655782655276
Validation loss: 2.5100595106742514

Epoch: 6| Step: 11
Training loss: 2.948679651162646
Validation loss: 2.490682931032451

Epoch: 6| Step: 12
Training loss: 1.9849776907178844
Validation loss: 2.529121320172378

Epoch: 6| Step: 13
Training loss: 1.8147132777371493
Validation loss: 2.561257022136236

Epoch: 292| Step: 0
Training loss: 2.5869751660570968
Validation loss: 2.595850710456042

Epoch: 6| Step: 1
Training loss: 1.441346950396693
Validation loss: 2.5928939111262674

Epoch: 6| Step: 2
Training loss: 1.924926568154132
Validation loss: 2.6335610401917733

Epoch: 6| Step: 3
Training loss: 2.785746336672224
Validation loss: 2.6024643076740333

Epoch: 6| Step: 4
Training loss: 1.7053675455077113
Validation loss: 2.548190083549344

Epoch: 6| Step: 5
Training loss: 1.5942263171928563
Validation loss: 2.5244229088437034

Epoch: 6| Step: 6
Training loss: 2.4306194321246632
Validation loss: 2.511317441055927

Epoch: 6| Step: 7
Training loss: 1.5127767955882532
Validation loss: 2.474306187852818

Epoch: 6| Step: 8
Training loss: 2.9080585062052475
Validation loss: 2.48073843543733

Epoch: 6| Step: 9
Training loss: 2.1036937925068457
Validation loss: 2.4664529745440738

Epoch: 6| Step: 10
Training loss: 2.536319412928544
Validation loss: 2.457065292955446

Epoch: 6| Step: 11
Training loss: 1.3624483203711133
Validation loss: 2.471117703079199

Epoch: 6| Step: 12
Training loss: 2.8878239437553934
Validation loss: 2.4440384492128544

Epoch: 6| Step: 13
Training loss: 2.0770554622615514
Validation loss: 2.461750247503218

Epoch: 293| Step: 0
Training loss: 2.369841645660442
Validation loss: 2.476890863096174

Epoch: 6| Step: 1
Training loss: 2.8902656950467143
Validation loss: 2.463336739810899

Epoch: 6| Step: 2
Training loss: 2.100873529316232
Validation loss: 2.4961265597627267

Epoch: 6| Step: 3
Training loss: 1.8082484663402119
Validation loss: 2.5410791927188776

Epoch: 6| Step: 4
Training loss: 2.2319493498712344
Validation loss: 2.576635913254107

Epoch: 6| Step: 5
Training loss: 2.4361304323577326
Validation loss: 2.5677537017796572

Epoch: 6| Step: 6
Training loss: 2.005220870602209
Validation loss: 2.5379452012601464

Epoch: 6| Step: 7
Training loss: 2.775189257517235
Validation loss: 2.496708101337051

Epoch: 6| Step: 8
Training loss: 2.1054988710304845
Validation loss: 2.4512978489738346

Epoch: 6| Step: 9
Training loss: 1.672593648595176
Validation loss: 2.448476805753858

Epoch: 6| Step: 10
Training loss: 1.848027907689239
Validation loss: 2.4703838800536646

Epoch: 6| Step: 11
Training loss: 2.6679402429445633
Validation loss: 2.4585883013730867

Epoch: 6| Step: 12
Training loss: 1.9977051562967822
Validation loss: 2.4691490784335266

Epoch: 6| Step: 13
Training loss: 1.5036419524761664
Validation loss: 2.4730752888181353

Epoch: 294| Step: 0
Training loss: 2.2084125888445616
Validation loss: 2.4970386927311767

Epoch: 6| Step: 1
Training loss: 2.01003620191018
Validation loss: 2.4695217037046584

Epoch: 6| Step: 2
Training loss: 2.1929147461593987
Validation loss: 2.4702810055041886

Epoch: 6| Step: 3
Training loss: 1.4367209272176271
Validation loss: 2.4897313466358213

Epoch: 6| Step: 4
Training loss: 1.9718963065711692
Validation loss: 2.499228016553042

Epoch: 6| Step: 5
Training loss: 2.744492825207373
Validation loss: 2.512196623419484

Epoch: 6| Step: 6
Training loss: 2.0075603876021724
Validation loss: 2.5064685583541526

Epoch: 6| Step: 7
Training loss: 2.366766460983204
Validation loss: 2.5009097669192575

Epoch: 6| Step: 8
Training loss: 2.0635792769410286
Validation loss: 2.524112764599293

Epoch: 6| Step: 9
Training loss: 2.181801703780979
Validation loss: 2.524536259716985

Epoch: 6| Step: 10
Training loss: 2.105966710812006
Validation loss: 2.5604250190501374

Epoch: 6| Step: 11
Training loss: 2.0956184818065196
Validation loss: 2.558359002368729

Epoch: 6| Step: 12
Training loss: 2.3314492475436968
Validation loss: 2.5436784307735794

Epoch: 6| Step: 13
Training loss: 2.454617380443437
Validation loss: 2.5851295953235134

Epoch: 295| Step: 0
Training loss: 2.2782491516694394
Validation loss: 2.6280657864014767

Epoch: 6| Step: 1
Training loss: 1.7730371636735933
Validation loss: 2.6650014287337624

Epoch: 6| Step: 2
Training loss: 2.499391290945927
Validation loss: 2.6358537927107504

Epoch: 6| Step: 3
Training loss: 1.5438526567311406
Validation loss: 2.615109673115466

Epoch: 6| Step: 4
Training loss: 2.278679222225615
Validation loss: 2.501114437935777

Epoch: 6| Step: 5
Training loss: 2.221960082752272
Validation loss: 2.540030426984552

Epoch: 6| Step: 6
Training loss: 1.7975740482745493
Validation loss: 2.516511401593196

Epoch: 6| Step: 7
Training loss: 2.268473761269106
Validation loss: 2.5129927770336504

Epoch: 6| Step: 8
Training loss: 2.435997720087798
Validation loss: 2.5134397062090934

Epoch: 6| Step: 9
Training loss: 2.236241554683562
Validation loss: 2.5071210531886563

Epoch: 6| Step: 10
Training loss: 2.231273178706802
Validation loss: 2.4871837405336805

Epoch: 6| Step: 11
Training loss: 2.0262757650686978
Validation loss: 2.496915440237863

Epoch: 6| Step: 12
Training loss: 1.7043724041573844
Validation loss: 2.4905077336687853

Epoch: 6| Step: 13
Training loss: 2.777917644370295
Validation loss: 2.4855930613242783

Epoch: 296| Step: 0
Training loss: 2.7118947210779494
Validation loss: 2.465134480092062

Epoch: 6| Step: 1
Training loss: 2.9481981949558143
Validation loss: 2.4735971180413365

Epoch: 6| Step: 2
Training loss: 1.7188627032695312
Validation loss: 2.479500472946517

Epoch: 6| Step: 3
Training loss: 2.3932351218486048
Validation loss: 2.4730801814030783

Epoch: 6| Step: 4
Training loss: 1.6314317623906045
Validation loss: 2.5379846016181506

Epoch: 6| Step: 5
Training loss: 1.6529447576265628
Validation loss: 2.5286655811569307

Epoch: 6| Step: 6
Training loss: 2.0866627093309082
Validation loss: 2.545314599448646

Epoch: 6| Step: 7
Training loss: 1.9411757272831112
Validation loss: 2.5137858805853326

Epoch: 6| Step: 8
Training loss: 1.5239957129100263
Validation loss: 2.5025020476646316

Epoch: 6| Step: 9
Training loss: 2.099026195353578
Validation loss: 2.488419541409079

Epoch: 6| Step: 10
Training loss: 3.0082235476537798
Validation loss: 2.4902486563877657

Epoch: 6| Step: 11
Training loss: 1.9856875426637102
Validation loss: 2.4947125469449096

Epoch: 6| Step: 12
Training loss: 2.154358407297803
Validation loss: 2.4857420449071896

Epoch: 6| Step: 13
Training loss: 2.0978219590186207
Validation loss: 2.48758857543791

Epoch: 297| Step: 0
Training loss: 2.915574405145318
Validation loss: 2.4768174418287052

Epoch: 6| Step: 1
Training loss: 2.999866959482898
Validation loss: 2.4928172082287863

Epoch: 6| Step: 2
Training loss: 1.6124261617388675
Validation loss: 2.4888501677192747

Epoch: 6| Step: 3
Training loss: 2.6291493817792486
Validation loss: 2.4733814463687005

Epoch: 6| Step: 4
Training loss: 1.6984189310611686
Validation loss: 2.492971075876383

Epoch: 6| Step: 5
Training loss: 1.7601117503928123
Validation loss: 2.5078942591569335

Epoch: 6| Step: 6
Training loss: 1.731204184466468
Validation loss: 2.513491134251533

Epoch: 6| Step: 7
Training loss: 2.3093317340106725
Validation loss: 2.524477812148394

Epoch: 6| Step: 8
Training loss: 1.8270595738929682
Validation loss: 2.530046341663883

Epoch: 6| Step: 9
Training loss: 2.039282542081403
Validation loss: 2.558190280464419

Epoch: 6| Step: 10
Training loss: 1.4568924825004783
Validation loss: 2.5592973435714916

Epoch: 6| Step: 11
Training loss: 2.489229461759895
Validation loss: 2.583044379491677

Epoch: 6| Step: 12
Training loss: 2.053482688739815
Validation loss: 2.5683803542988226

Epoch: 6| Step: 13
Training loss: 2.494013485098864
Validation loss: 2.5138594708790927

Epoch: 298| Step: 0
Training loss: 2.257783988587822
Validation loss: 2.4920146567299915

Epoch: 6| Step: 1
Training loss: 1.8838385481042388
Validation loss: 2.487799589368542

Epoch: 6| Step: 2
Training loss: 2.466717813150575
Validation loss: 2.4772794149147863

Epoch: 6| Step: 3
Training loss: 1.664959876003741
Validation loss: 2.4819125401541218

Epoch: 6| Step: 4
Training loss: 1.346637860728144
Validation loss: 2.4789295617422185

Epoch: 6| Step: 5
Training loss: 1.8779834535294357
Validation loss: 2.4779770238117504

Epoch: 6| Step: 6
Training loss: 2.008514637763405
Validation loss: 2.4813916669579243

Epoch: 6| Step: 7
Training loss: 2.3700377172374134
Validation loss: 2.4647170093192297

Epoch: 6| Step: 8
Training loss: 3.137170210447831
Validation loss: 2.4878828287663817

Epoch: 6| Step: 9
Training loss: 2.4331790086938505
Validation loss: 2.4849032594154985

Epoch: 6| Step: 10
Training loss: 1.7389047940183247
Validation loss: 2.4709045488881363

Epoch: 6| Step: 11
Training loss: 1.9868463461010608
Validation loss: 2.4682509485268236

Epoch: 6| Step: 12
Training loss: 2.3601422609674176
Validation loss: 2.5042642148060548

Epoch: 6| Step: 13
Training loss: 2.227423892792109
Validation loss: 2.504902816405058

Epoch: 299| Step: 0
Training loss: 2.997675312887328
Validation loss: 2.5012751823910233

Epoch: 6| Step: 1
Training loss: 1.6455064360389273
Validation loss: 2.498495921043765

Epoch: 6| Step: 2
Training loss: 2.20835072732469
Validation loss: 2.502556637971921

Epoch: 6| Step: 3
Training loss: 1.8485889621053868
Validation loss: 2.499837838317785

Epoch: 6| Step: 4
Training loss: 2.0901941879079233
Validation loss: 2.469412075064674

Epoch: 6| Step: 5
Training loss: 2.1679460953912053
Validation loss: 2.43052917980839

Epoch: 6| Step: 6
Training loss: 1.9463896316292928
Validation loss: 2.453550257339936

Epoch: 6| Step: 7
Training loss: 1.9635819920885762
Validation loss: 2.4416170644658353

Epoch: 6| Step: 8
Training loss: 2.0806978976908392
Validation loss: 2.4859025205864187

Epoch: 6| Step: 9
Training loss: 1.9619574458944649
Validation loss: 2.457394498770514

Epoch: 6| Step: 10
Training loss: 1.8566295967761701
Validation loss: 2.473430735228801

Epoch: 6| Step: 11
Training loss: 1.50437590956809
Validation loss: 2.434249503835175

Epoch: 6| Step: 12
Training loss: 2.223370965108236
Validation loss: 2.460609470686946

Epoch: 6| Step: 13
Training loss: 2.7173670616727086
Validation loss: 2.4723713707194173

Epoch: 300| Step: 0
Training loss: 2.4464206317538086
Validation loss: 2.4943916355802735

Epoch: 6| Step: 1
Training loss: 1.5356454627878733
Validation loss: 2.4728350903607925

Epoch: 6| Step: 2
Training loss: 2.2030943739906443
Validation loss: 2.4876929784963844

Epoch: 6| Step: 3
Training loss: 1.9864979599146013
Validation loss: 2.479066194907985

Epoch: 6| Step: 4
Training loss: 2.0442331752021756
Validation loss: 2.482681881073669

Epoch: 6| Step: 5
Training loss: 2.3016915196488723
Validation loss: 2.4780282497560946

Epoch: 6| Step: 6
Training loss: 2.1257628025430733
Validation loss: 2.484880791765651

Epoch: 6| Step: 7
Training loss: 1.9503650543657558
Validation loss: 2.4701427120674504

Epoch: 6| Step: 8
Training loss: 2.5813163656427145
Validation loss: 2.464418820984736

Epoch: 6| Step: 9
Training loss: 2.333094777673566
Validation loss: 2.461098293319383

Epoch: 6| Step: 10
Training loss: 2.016402338638266
Validation loss: 2.4724713698819785

Epoch: 6| Step: 11
Training loss: 2.2596955680043704
Validation loss: 2.5320486056949147

Epoch: 6| Step: 12
Training loss: 2.244131700781864
Validation loss: 2.620134810214139

Epoch: 6| Step: 13
Training loss: 2.6183123093053773
Validation loss: 2.626144432186629

Testing loss: 2.1224372260457915

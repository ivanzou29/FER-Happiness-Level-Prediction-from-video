Epoch: 1| Step: 0
Training loss: 5.323240843853022
Validation loss: 5.905174379446843

Epoch: 5| Step: 1
Training loss: 6.472666269040224
Validation loss: 5.903703116896376

Epoch: 5| Step: 2
Training loss: 5.9524502108949715
Validation loss: 5.902294112442169

Epoch: 5| Step: 3
Training loss: 6.131152118986475
Validation loss: 5.900960730334038

Epoch: 5| Step: 4
Training loss: 5.867650081111282
Validation loss: 5.899579999813066

Epoch: 5| Step: 5
Training loss: 5.559181166256215
Validation loss: 5.898134478939063

Epoch: 5| Step: 6
Training loss: 5.320793410200972
Validation loss: 5.896847778052627

Epoch: 5| Step: 7
Training loss: 5.938092975118556
Validation loss: 5.895461420071817

Epoch: 5| Step: 8
Training loss: 6.279302038780925
Validation loss: 5.894067212195226

Epoch: 5| Step: 9
Training loss: 6.3073135068994635
Validation loss: 5.8926674146233475

Epoch: 5| Step: 10
Training loss: 6.8277126859923545
Validation loss: 5.891221249841612

Epoch: 5| Step: 11
Training loss: 6.000946923874123
Validation loss: 5.889693836713341

Epoch: 2| Step: 0
Training loss: 6.530092273661917
Validation loss: 5.888227215917518

Epoch: 5| Step: 1
Training loss: 5.777534569406192
Validation loss: 5.886589084303369

Epoch: 5| Step: 2
Training loss: 6.5750757249307945
Validation loss: 5.885020374920755

Epoch: 5| Step: 3
Training loss: 5.1207469878070295
Validation loss: 5.883332876668305

Epoch: 5| Step: 4
Training loss: 6.225675014681358
Validation loss: 5.8815639567448965

Epoch: 5| Step: 5
Training loss: 6.006017845978393
Validation loss: 5.879773872032371

Epoch: 5| Step: 6
Training loss: 5.898761826244867
Validation loss: 5.8778270926486975

Epoch: 5| Step: 7
Training loss: 6.507757692690131
Validation loss: 5.875947517389592

Epoch: 5| Step: 8
Training loss: 6.101660655620538
Validation loss: 5.87382737760308

Epoch: 5| Step: 9
Training loss: 5.44900838247611
Validation loss: 5.871652806226252

Epoch: 5| Step: 10
Training loss: 5.4995843556879045
Validation loss: 5.8694411553628765

Epoch: 5| Step: 11
Training loss: 6.359206438759613
Validation loss: 5.867125611162957

Epoch: 3| Step: 0
Training loss: 6.210521679280139
Validation loss: 5.864702920276002

Epoch: 5| Step: 1
Training loss: 5.7882021780813515
Validation loss: 5.862175173189059

Epoch: 5| Step: 2
Training loss: 5.25066317048222
Validation loss: 5.859438367712902

Epoch: 5| Step: 3
Training loss: 5.555102798238663
Validation loss: 5.856683131390035

Epoch: 5| Step: 4
Training loss: 5.278492947136403
Validation loss: 5.85373150123325

Epoch: 5| Step: 5
Training loss: 5.58727721041086
Validation loss: 5.850617186818861

Epoch: 5| Step: 6
Training loss: 6.9419991965022785
Validation loss: 5.847502165118911

Epoch: 5| Step: 7
Training loss: 5.8458140598812305
Validation loss: 5.844032783942029

Epoch: 5| Step: 8
Training loss: 5.942375670830754
Validation loss: 5.8404417822579635

Epoch: 5| Step: 9
Training loss: 6.424915939553005
Validation loss: 5.83674300770022

Epoch: 5| Step: 10
Training loss: 6.378781263127936
Validation loss: 5.832843823557024

Epoch: 5| Step: 11
Training loss: 6.846755669797506
Validation loss: 5.82865349058343

Epoch: 4| Step: 0
Training loss: 5.76933520687159
Validation loss: 5.824193397089842

Epoch: 5| Step: 1
Training loss: 5.20655613396447
Validation loss: 5.8199430480718

Epoch: 5| Step: 2
Training loss: 6.5978959053017245
Validation loss: 5.815102360393849

Epoch: 5| Step: 3
Training loss: 6.6342648195595055
Validation loss: 5.810355963469094

Epoch: 5| Step: 4
Training loss: 6.324131287034175
Validation loss: 5.805022930381449

Epoch: 5| Step: 5
Training loss: 5.736949207737349
Validation loss: 5.799568092225127

Epoch: 5| Step: 6
Training loss: 6.346780889161788
Validation loss: 5.794009443809337

Epoch: 5| Step: 7
Training loss: 5.5484532649579785
Validation loss: 5.788231656629273

Epoch: 5| Step: 8
Training loss: 5.647922518774153
Validation loss: 5.7819915845720224

Epoch: 5| Step: 9
Training loss: 5.295387261685467
Validation loss: 5.776068501342777

Epoch: 5| Step: 10
Training loss: 5.569725230372294
Validation loss: 5.769820322944795

Epoch: 5| Step: 11
Training loss: 6.704443172856091
Validation loss: 5.763316773614577

Epoch: 5| Step: 0
Training loss: 6.0686281044658275
Validation loss: 5.756446424426398

Epoch: 5| Step: 1
Training loss: 5.850034221320846
Validation loss: 5.74942386892147

Epoch: 5| Step: 2
Training loss: 5.469434422852493
Validation loss: 5.7421243953913645

Epoch: 5| Step: 3
Training loss: 5.633858381252565
Validation loss: 5.735186929070315

Epoch: 5| Step: 4
Training loss: 5.385698892760026
Validation loss: 5.727924353260312

Epoch: 5| Step: 5
Training loss: 6.3390548694725455
Validation loss: 5.720552427929803

Epoch: 5| Step: 6
Training loss: 5.771352182284745
Validation loss: 5.713351065401553

Epoch: 5| Step: 7
Training loss: 6.518713203245858
Validation loss: 5.705490840900613

Epoch: 5| Step: 8
Training loss: 6.48470737570595
Validation loss: 5.698156692312423

Epoch: 5| Step: 9
Training loss: 5.212003264325892
Validation loss: 5.690128473598028

Epoch: 5| Step: 10
Training loss: 5.164152620715141
Validation loss: 5.682458585483888

Epoch: 5| Step: 11
Training loss: 6.335597319590008
Validation loss: 5.674765586773026

Epoch: 6| Step: 0
Training loss: 6.121939692561928
Validation loss: 5.6672395692390785

Epoch: 5| Step: 1
Training loss: 5.721862279605945
Validation loss: 5.659459645343765

Epoch: 5| Step: 2
Training loss: 5.80889583948821
Validation loss: 5.651707393499874

Epoch: 5| Step: 3
Training loss: 5.75029521681856
Validation loss: 5.643913085597665

Epoch: 5| Step: 4
Training loss: 5.796423223530119
Validation loss: 5.636172939001312

Epoch: 5| Step: 5
Training loss: 5.375727227273153
Validation loss: 5.628312251851946

Epoch: 5| Step: 6
Training loss: 6.534114519762647
Validation loss: 5.620710977118516

Epoch: 5| Step: 7
Training loss: 5.231343498898423
Validation loss: 5.6126921399255085

Epoch: 5| Step: 8
Training loss: 5.912377141276879
Validation loss: 5.6049515719310925

Epoch: 5| Step: 9
Training loss: 5.673092545906846
Validation loss: 5.597235788729087

Epoch: 5| Step: 10
Training loss: 4.8888696805017915
Validation loss: 5.589258255275201

Epoch: 5| Step: 11
Training loss: 6.838702565779793
Validation loss: 5.5813878522582

Epoch: 7| Step: 0
Training loss: 5.478123941546532
Validation loss: 5.573934731825287

Epoch: 5| Step: 1
Training loss: 5.418821150896685
Validation loss: 5.566490899050262

Epoch: 5| Step: 2
Training loss: 5.162380573655517
Validation loss: 5.559200644229393

Epoch: 5| Step: 3
Training loss: 5.513455145127155
Validation loss: 5.551336186888173

Epoch: 5| Step: 4
Training loss: 6.375067317831614
Validation loss: 5.543935865678971

Epoch: 5| Step: 5
Training loss: 4.763674277367168
Validation loss: 5.5364947409044305

Epoch: 5| Step: 6
Training loss: 6.458330839423241
Validation loss: 5.529254574952447

Epoch: 5| Step: 7
Training loss: 6.102037008397726
Validation loss: 5.522215971163907

Epoch: 5| Step: 8
Training loss: 6.031126663449077
Validation loss: 5.515407331024144

Epoch: 5| Step: 9
Training loss: 4.64864183505726
Validation loss: 5.5089218717833015

Epoch: 5| Step: 10
Training loss: 5.845789262855178
Validation loss: 5.502847967299807

Epoch: 5| Step: 11
Training loss: 6.1462482204593725
Validation loss: 5.496920764032111

Epoch: 8| Step: 0
Training loss: 5.411431448034103
Validation loss: 5.4909694256023664

Epoch: 5| Step: 1
Training loss: 5.465858087209784
Validation loss: 5.485233706987034

Epoch: 5| Step: 2
Training loss: 5.215976549189455
Validation loss: 5.479489584086907

Epoch: 5| Step: 3
Training loss: 5.584689340992961
Validation loss: 5.474025477884691

Epoch: 5| Step: 4
Training loss: 5.170800411458713
Validation loss: 5.4681666689798165

Epoch: 5| Step: 5
Training loss: 5.188699055170943
Validation loss: 5.462321617653193

Epoch: 5| Step: 6
Training loss: 6.611039910169024
Validation loss: 5.456751625799369

Epoch: 5| Step: 7
Training loss: 5.6536209993828805
Validation loss: 5.450939232274245

Epoch: 5| Step: 8
Training loss: 6.242027996827922
Validation loss: 5.444873202832262

Epoch: 5| Step: 9
Training loss: 5.469522301773229
Validation loss: 5.4391446969545285

Epoch: 5| Step: 10
Training loss: 5.306908031770102
Validation loss: 5.4333094237489945

Epoch: 5| Step: 11
Training loss: 5.108955389401872
Validation loss: 5.427546676020358

Epoch: 9| Step: 0
Training loss: 5.344404213996906
Validation loss: 5.421936188718071

Epoch: 5| Step: 1
Training loss: 5.31374209973904
Validation loss: 5.416586063469987

Epoch: 5| Step: 2
Training loss: 6.3725587246966215
Validation loss: 5.4108279108517445

Epoch: 5| Step: 3
Training loss: 4.814580492420203
Validation loss: 5.405230181577524

Epoch: 5| Step: 4
Training loss: 5.390072116970928
Validation loss: 5.399611150905072

Epoch: 5| Step: 5
Training loss: 5.894962241947304
Validation loss: 5.3940233584930635

Epoch: 5| Step: 6
Training loss: 5.558514656796304
Validation loss: 5.387942685248395

Epoch: 5| Step: 7
Training loss: 5.420926497546049
Validation loss: 5.38213359240602

Epoch: 5| Step: 8
Training loss: 6.315993447105289
Validation loss: 5.376561093406416

Epoch: 5| Step: 9
Training loss: 5.0268068775350025
Validation loss: 5.370394337748007

Epoch: 5| Step: 10
Training loss: 4.601328301420272
Validation loss: 5.365130671260498

Epoch: 5| Step: 11
Training loss: 6.965702776262876
Validation loss: 5.35953547438585

Epoch: 10| Step: 0
Training loss: 5.3966570290535785
Validation loss: 5.354505790430184

Epoch: 5| Step: 1
Training loss: 5.303749147550401
Validation loss: 5.349467967962587

Epoch: 5| Step: 2
Training loss: 5.54786969789436
Validation loss: 5.344760018537944

Epoch: 5| Step: 3
Training loss: 5.444072040269
Validation loss: 5.339537940376258

Epoch: 5| Step: 4
Training loss: 5.676208017979865
Validation loss: 5.334538050526422

Epoch: 5| Step: 5
Training loss: 5.7235780774180745
Validation loss: 5.329926925346719

Epoch: 5| Step: 6
Training loss: 5.36335771010229
Validation loss: 5.3249529870813

Epoch: 5| Step: 7
Training loss: 5.440884118042022
Validation loss: 5.320371451030263

Epoch: 5| Step: 8
Training loss: 5.442976725651721
Validation loss: 5.315490075667248

Epoch: 5| Step: 9
Training loss: 5.314395880399615
Validation loss: 5.310890470040083

Epoch: 5| Step: 10
Training loss: 5.479841396236325
Validation loss: 5.305921602964313

Epoch: 5| Step: 11
Training loss: 4.5201228882955125
Validation loss: 5.301284949767556

Epoch: 11| Step: 0
Training loss: 4.793723596452914
Validation loss: 5.296728486591988

Epoch: 5| Step: 1
Training loss: 6.053222479008276
Validation loss: 5.29210538872705

Epoch: 5| Step: 2
Training loss: 5.377410525575604
Validation loss: 5.287343236464623

Epoch: 5| Step: 3
Training loss: 5.820646094676489
Validation loss: 5.282887086564587

Epoch: 5| Step: 4
Training loss: 4.848395107623646
Validation loss: 5.27816661383917

Epoch: 5| Step: 5
Training loss: 5.62425769570399
Validation loss: 5.273911759514139

Epoch: 5| Step: 6
Training loss: 5.475981024375567
Validation loss: 5.269352085318738

Epoch: 5| Step: 7
Training loss: 5.522895667708787
Validation loss: 5.265164937221999

Epoch: 5| Step: 8
Training loss: 5.216664337222683
Validation loss: 5.260495208556763

Epoch: 5| Step: 9
Training loss: 5.278150021102438
Validation loss: 5.256033949431713

Epoch: 5| Step: 10
Training loss: 5.103683241184035
Validation loss: 5.251725367455401

Epoch: 5| Step: 11
Training loss: 5.944363131021461
Validation loss: 5.247255576809516

Epoch: 12| Step: 0
Training loss: 5.101939726314616
Validation loss: 5.242919431130531

Epoch: 5| Step: 1
Training loss: 5.571288121028624
Validation loss: 5.2385348742912505

Epoch: 5| Step: 2
Training loss: 5.489463249886148
Validation loss: 5.234098170916937

Epoch: 5| Step: 3
Training loss: 5.237316978734645
Validation loss: 5.229861014520965

Epoch: 5| Step: 4
Training loss: 5.6598543095793605
Validation loss: 5.2255071269188615

Epoch: 5| Step: 5
Training loss: 5.671265624547988
Validation loss: 5.221189899349825

Epoch: 5| Step: 6
Training loss: 3.530434303259148
Validation loss: 5.216513918000264

Epoch: 5| Step: 7
Training loss: 5.7410610209898065
Validation loss: 5.2121692823944805

Epoch: 5| Step: 8
Training loss: 6.095313864333511
Validation loss: 5.207971425516189

Epoch: 5| Step: 9
Training loss: 5.747972545603342
Validation loss: 5.203243074805254

Epoch: 5| Step: 10
Training loss: 4.411527030376908
Validation loss: 5.198628312539731

Epoch: 5| Step: 11
Training loss: 5.472953009608775
Validation loss: 5.194658862611038

Epoch: 13| Step: 0
Training loss: 4.986827474201037
Validation loss: 5.189763130841615

Epoch: 5| Step: 1
Training loss: 4.6820038171986464
Validation loss: 5.185568867533205

Epoch: 5| Step: 2
Training loss: 5.727563622203312
Validation loss: 5.181606913300077

Epoch: 5| Step: 3
Training loss: 4.289673475096819
Validation loss: 5.177299965740045

Epoch: 5| Step: 4
Training loss: 4.900888806133831
Validation loss: 5.172934893195614

Epoch: 5| Step: 5
Training loss: 5.016937369375594
Validation loss: 5.168476482505517

Epoch: 5| Step: 6
Training loss: 5.541357772746331
Validation loss: 5.164422442520575

Epoch: 5| Step: 7
Training loss: 6.120098410419775
Validation loss: 5.160031767890291

Epoch: 5| Step: 8
Training loss: 5.4361237285980675
Validation loss: 5.155757541451886

Epoch: 5| Step: 9
Training loss: 5.382529906009526
Validation loss: 5.151044518367439

Epoch: 5| Step: 10
Training loss: 5.881580644286376
Validation loss: 5.146436890124272

Epoch: 5| Step: 11
Training loss: 5.059518760443553
Validation loss: 5.142252895233429

Epoch: 14| Step: 0
Training loss: 5.379768917410517
Validation loss: 5.13798650029875

Epoch: 5| Step: 1
Training loss: 5.276792552520327
Validation loss: 5.133207293403926

Epoch: 5| Step: 2
Training loss: 5.352882834956823
Validation loss: 5.128959955697891

Epoch: 5| Step: 3
Training loss: 5.231117806536131
Validation loss: 5.124067376730363

Epoch: 5| Step: 4
Training loss: 5.234123747759538
Validation loss: 5.119310322121932

Epoch: 5| Step: 5
Training loss: 4.737945922156489
Validation loss: 5.114257866888147

Epoch: 5| Step: 6
Training loss: 5.22466103216285
Validation loss: 5.110139884819494

Epoch: 5| Step: 7
Training loss: 5.080986836317819
Validation loss: 5.105352367815121

Epoch: 5| Step: 8
Training loss: 5.142222611322794
Validation loss: 5.100718531138026

Epoch: 5| Step: 9
Training loss: 5.0664794728348825
Validation loss: 5.0962285679496055

Epoch: 5| Step: 10
Training loss: 5.7149729111130405
Validation loss: 5.09127856875867

Epoch: 5| Step: 11
Training loss: 5.839874124607461
Validation loss: 5.086610975077289

Epoch: 15| Step: 0
Training loss: 5.7803446292762795
Validation loss: 5.082007918034289

Epoch: 5| Step: 1
Training loss: 5.434237147946982
Validation loss: 5.077085253569447

Epoch: 5| Step: 2
Training loss: 4.843916957808074
Validation loss: 5.07202453392846

Epoch: 5| Step: 3
Training loss: 4.938076903042946
Validation loss: 5.0670192703519055

Epoch: 5| Step: 4
Training loss: 4.681533678573525
Validation loss: 5.06250683662361

Epoch: 5| Step: 5
Training loss: 5.179432674255786
Validation loss: 5.057384610590635

Epoch: 5| Step: 6
Training loss: 4.64566146920438
Validation loss: 5.051895754238735

Epoch: 5| Step: 7
Training loss: 5.759562957395732
Validation loss: 5.0473151708869075

Epoch: 5| Step: 8
Training loss: 5.479769520054646
Validation loss: 5.041995205959274

Epoch: 5| Step: 9
Training loss: 5.106715452162043
Validation loss: 5.037509207642173

Epoch: 5| Step: 10
Training loss: 5.182057294545775
Validation loss: 5.032450862391328

Epoch: 5| Step: 11
Training loss: 4.156222924165802
Validation loss: 5.027556759921562

Epoch: 16| Step: 0
Training loss: 5.6663010797190605
Validation loss: 5.022273299052205

Epoch: 5| Step: 1
Training loss: 4.533671396403406
Validation loss: 5.0180083699557585

Epoch: 5| Step: 2
Training loss: 5.416349118045724
Validation loss: 5.012678942891135

Epoch: 5| Step: 3
Training loss: 4.970455045074097
Validation loss: 5.0077501155242645

Epoch: 5| Step: 4
Training loss: 5.192455452666849
Validation loss: 5.002934421306483

Epoch: 5| Step: 5
Training loss: 5.387355531671711
Validation loss: 4.9981844307824215

Epoch: 5| Step: 6
Training loss: 5.481028348123159
Validation loss: 4.992953126847391

Epoch: 5| Step: 7
Training loss: 4.243336109321287
Validation loss: 4.987695901093946

Epoch: 5| Step: 8
Training loss: 4.603250242602558
Validation loss: 4.981750666379281

Epoch: 5| Step: 9
Training loss: 4.825655506256479
Validation loss: 4.977018621872171

Epoch: 5| Step: 10
Training loss: 6.078231065310102
Validation loss: 4.972424490003546

Epoch: 5| Step: 11
Training loss: 2.9599979892930084
Validation loss: 4.96773585880285

Epoch: 17| Step: 0
Training loss: 5.003712420316488
Validation loss: 4.962582665338636

Epoch: 5| Step: 1
Training loss: 5.720835487863571
Validation loss: 4.957558020981449

Epoch: 5| Step: 2
Training loss: 5.908973994132152
Validation loss: 4.952969978866923

Epoch: 5| Step: 3
Training loss: 4.657704785319428
Validation loss: 4.947685712811639

Epoch: 5| Step: 4
Training loss: 4.704285335930484
Validation loss: 4.9430189393519495

Epoch: 5| Step: 5
Training loss: 5.007635771062528
Validation loss: 4.937729930755074

Epoch: 5| Step: 6
Training loss: 5.2036772197221906
Validation loss: 4.932640106343281

Epoch: 5| Step: 7
Training loss: 5.47164997049868
Validation loss: 4.926865379991583

Epoch: 5| Step: 8
Training loss: 4.280869961320061
Validation loss: 4.9221714636915

Epoch: 5| Step: 9
Training loss: 4.937658186684278
Validation loss: 4.9180794588030174

Epoch: 5| Step: 10
Training loss: 4.8909884674242035
Validation loss: 4.913378831198986

Epoch: 5| Step: 11
Training loss: 3.1570561956238063
Validation loss: 4.907193532551049

Epoch: 18| Step: 0
Training loss: 5.426131894211015
Validation loss: 4.902549879140355

Epoch: 5| Step: 1
Training loss: 4.764463989915975
Validation loss: 4.897916199254

Epoch: 5| Step: 2
Training loss: 5.569759646422895
Validation loss: 4.893497241115731

Epoch: 5| Step: 3
Training loss: 5.335295117696857
Validation loss: 4.887903010540829

Epoch: 5| Step: 4
Training loss: 4.193138084738048
Validation loss: 4.883306314547702

Epoch: 5| Step: 5
Training loss: 5.202809367186088
Validation loss: 4.877743682522515

Epoch: 5| Step: 6
Training loss: 4.252601949018035
Validation loss: 4.872537863323139

Epoch: 5| Step: 7
Training loss: 4.624790496205105
Validation loss: 4.8678946514568375

Epoch: 5| Step: 8
Training loss: 5.0401670679528054
Validation loss: 4.86281636376717

Epoch: 5| Step: 9
Training loss: 5.498829630392012
Validation loss: 4.858623008969115

Epoch: 5| Step: 10
Training loss: 5.073990579574555
Validation loss: 4.85339893223809

Epoch: 5| Step: 11
Training loss: 4.0832837679183775
Validation loss: 4.848242335684561

Epoch: 19| Step: 0
Training loss: 4.756090175033665
Validation loss: 4.843318476989988

Epoch: 5| Step: 1
Training loss: 4.708392162574374
Validation loss: 4.838835374385626

Epoch: 5| Step: 2
Training loss: 4.617309362155584
Validation loss: 4.834825263562577

Epoch: 5| Step: 3
Training loss: 5.277937894478178
Validation loss: 4.829354772560358

Epoch: 5| Step: 4
Training loss: 5.252568661051705
Validation loss: 4.82399988334333

Epoch: 5| Step: 5
Training loss: 5.0990228801482536
Validation loss: 4.81866864780177

Epoch: 5| Step: 6
Training loss: 5.097720605436684
Validation loss: 4.814172214775373

Epoch: 5| Step: 7
Training loss: 4.788601279024247
Validation loss: 4.809854573016222

Epoch: 5| Step: 8
Training loss: 5.529693901875356
Validation loss: 4.804900216870337

Epoch: 5| Step: 9
Training loss: 5.413911715522942
Validation loss: 4.799520552055387

Epoch: 5| Step: 10
Training loss: 4.03505934887885
Validation loss: 4.793985123817747

Epoch: 5| Step: 11
Training loss: 2.5139554566994704
Validation loss: 4.78983642896131

Epoch: 20| Step: 0
Training loss: 4.5567272715990335
Validation loss: 4.785371894388943

Epoch: 5| Step: 1
Training loss: 3.5650396831171047
Validation loss: 4.779823539222036

Epoch: 5| Step: 2
Training loss: 5.189640936218517
Validation loss: 4.775311083088934

Epoch: 5| Step: 3
Training loss: 5.870280216423739
Validation loss: 4.770560646870829

Epoch: 5| Step: 4
Training loss: 4.320582459001816
Validation loss: 4.766403718289191

Epoch: 5| Step: 5
Training loss: 5.2150271722775114
Validation loss: 4.761615133475099

Epoch: 5| Step: 6
Training loss: 4.708548526853719
Validation loss: 4.756052101820385

Epoch: 5| Step: 7
Training loss: 4.93911233965531
Validation loss: 4.751200223168713

Epoch: 5| Step: 8
Training loss: 5.183485201034696
Validation loss: 4.746519838314242

Epoch: 5| Step: 9
Training loss: 5.54122404850124
Validation loss: 4.741757837948704

Epoch: 5| Step: 10
Training loss: 3.9514210764298956
Validation loss: 4.735799845699545

Epoch: 5| Step: 11
Training loss: 5.922169056539041
Validation loss: 4.731485095412921

Epoch: 21| Step: 0
Training loss: 5.081958063582359
Validation loss: 4.72626925312766

Epoch: 5| Step: 1
Training loss: 4.350720418270862
Validation loss: 4.720822306611981

Epoch: 5| Step: 2
Training loss: 5.5648800333669906
Validation loss: 4.716476920302278

Epoch: 5| Step: 3
Training loss: 4.188814327027231
Validation loss: 4.711470810460903

Epoch: 5| Step: 4
Training loss: 4.462561957946985
Validation loss: 4.706237050613211

Epoch: 5| Step: 5
Training loss: 5.008622364372423
Validation loss: 4.701224288362981

Epoch: 5| Step: 6
Training loss: 4.592903526324499
Validation loss: 4.696193381821029

Epoch: 5| Step: 7
Training loss: 5.218757675073885
Validation loss: 4.69057600608685

Epoch: 5| Step: 8
Training loss: 4.712759497502374
Validation loss: 4.686413787451463

Epoch: 5| Step: 9
Training loss: 4.680625859659192
Validation loss: 4.681861877518074

Epoch: 5| Step: 10
Training loss: 5.116475156167734
Validation loss: 4.676591361607111

Epoch: 5| Step: 11
Training loss: 4.556537232940798
Validation loss: 4.671681352257258

Epoch: 22| Step: 0
Training loss: 5.333736026978947
Validation loss: 4.66642131614584

Epoch: 5| Step: 1
Training loss: 4.559148524243676
Validation loss: 4.661754486507886

Epoch: 5| Step: 2
Training loss: 4.15725168189645
Validation loss: 4.657225598360296

Epoch: 5| Step: 3
Training loss: 4.86874967092726
Validation loss: 4.651677051586358

Epoch: 5| Step: 4
Training loss: 5.050700057956268
Validation loss: 4.646519377935908

Epoch: 5| Step: 5
Training loss: 4.904923375082396
Validation loss: 4.641667198692487

Epoch: 5| Step: 6
Training loss: 4.426636198314309
Validation loss: 4.637403763751902

Epoch: 5| Step: 7
Training loss: 4.572503814945955
Validation loss: 4.633034326089835

Epoch: 5| Step: 8
Training loss: 4.7323160761878835
Validation loss: 4.627256096619363

Epoch: 5| Step: 9
Training loss: 4.9608029324760805
Validation loss: 4.623065045407936

Epoch: 5| Step: 10
Training loss: 4.798146827909473
Validation loss: 4.617250651849077

Epoch: 5| Step: 11
Training loss: 4.712504516923191
Validation loss: 4.612662260652648

Epoch: 23| Step: 0
Training loss: 4.349064930353721
Validation loss: 4.607763079678653

Epoch: 5| Step: 1
Training loss: 5.050045753498354
Validation loss: 4.603009577674689

Epoch: 5| Step: 2
Training loss: 4.4130629226344436
Validation loss: 4.597629794441971

Epoch: 5| Step: 3
Training loss: 4.3435352738912005
Validation loss: 4.592991063405031

Epoch: 5| Step: 4
Training loss: 5.108622924329354
Validation loss: 4.588506902706622

Epoch: 5| Step: 5
Training loss: 5.251295565648333
Validation loss: 4.583751800969511

Epoch: 5| Step: 6
Training loss: 5.3868810948727885
Validation loss: 4.578154545364258

Epoch: 5| Step: 7
Training loss: 4.707185836089691
Validation loss: 4.572559250033546

Epoch: 5| Step: 8
Training loss: 4.117331572876522
Validation loss: 4.567912222974242

Epoch: 5| Step: 9
Training loss: 4.574483523373456
Validation loss: 4.5633119182952635

Epoch: 5| Step: 10
Training loss: 4.53400227061763
Validation loss: 4.5585100547082975

Epoch: 5| Step: 11
Training loss: 3.565496021448809
Validation loss: 4.553338995734995

Epoch: 24| Step: 0
Training loss: 4.670990212647677
Validation loss: 4.549229295314276

Epoch: 5| Step: 1
Training loss: 5.041986418533646
Validation loss: 4.544135606679302

Epoch: 5| Step: 2
Training loss: 4.81841736670868
Validation loss: 4.539423299556369

Epoch: 5| Step: 3
Training loss: 4.888143379031524
Validation loss: 4.534997885274902

Epoch: 5| Step: 4
Training loss: 4.34388568035061
Validation loss: 4.529577950851129

Epoch: 5| Step: 5
Training loss: 3.896738052714995
Validation loss: 4.525540378613374

Epoch: 5| Step: 6
Training loss: 5.2186792791451655
Validation loss: 4.520673672834971

Epoch: 5| Step: 7
Training loss: 4.263899569475601
Validation loss: 4.516397909435962

Epoch: 5| Step: 8
Training loss: 4.297162632418411
Validation loss: 4.5132480423999715

Epoch: 5| Step: 9
Training loss: 5.015612450699838
Validation loss: 4.510333915755989

Epoch: 5| Step: 10
Training loss: 4.598369325941934
Validation loss: 4.504199320380422

Epoch: 5| Step: 11
Training loss: 4.503462201318676
Validation loss: 4.49845232663544

Epoch: 25| Step: 0
Training loss: 4.138017632411377
Validation loss: 4.493891313830729

Epoch: 5| Step: 1
Training loss: 3.645823291583083
Validation loss: 4.490516170350124

Epoch: 5| Step: 2
Training loss: 4.930585437857229
Validation loss: 4.48574138386727

Epoch: 5| Step: 3
Training loss: 5.055963511419816
Validation loss: 4.480617025195067

Epoch: 5| Step: 4
Training loss: 4.582497029155261
Validation loss: 4.475514531701759

Epoch: 5| Step: 5
Training loss: 4.6714671285428775
Validation loss: 4.470660768160772

Epoch: 5| Step: 6
Training loss: 4.7817625880512225
Validation loss: 4.466419741699311

Epoch: 5| Step: 7
Training loss: 4.774266547149669
Validation loss: 4.46190908421571

Epoch: 5| Step: 8
Training loss: 4.645969177905809
Validation loss: 4.457734240034276

Epoch: 5| Step: 9
Training loss: 4.218268472953341
Validation loss: 4.452612177003134

Epoch: 5| Step: 10
Training loss: 4.976999787621688
Validation loss: 4.448210805537284

Epoch: 5| Step: 11
Training loss: 4.4662489928258555
Validation loss: 4.443034320721064

Epoch: 26| Step: 0
Training loss: 4.549833508474127
Validation loss: 4.438547780801817

Epoch: 5| Step: 1
Training loss: 4.719941045748538
Validation loss: 4.433864465366434

Epoch: 5| Step: 2
Training loss: 5.127697304621944
Validation loss: 4.429141962455596

Epoch: 5| Step: 3
Training loss: 4.798576724120446
Validation loss: 4.4246220227897055

Epoch: 5| Step: 4
Training loss: 3.889395278174078
Validation loss: 4.420100796357168

Epoch: 5| Step: 5
Training loss: 4.537595528261701
Validation loss: 4.415491265613532

Epoch: 5| Step: 6
Training loss: 3.7908417918832837
Validation loss: 4.410899115213112

Epoch: 5| Step: 7
Training loss: 4.9544460328015845
Validation loss: 4.406788173121885

Epoch: 5| Step: 8
Training loss: 4.429048899212034
Validation loss: 4.401770499409567

Epoch: 5| Step: 9
Training loss: 4.312897152511816
Validation loss: 4.397036639335276

Epoch: 5| Step: 10
Training loss: 4.594187812961787
Validation loss: 4.392422698355203

Epoch: 5| Step: 11
Training loss: 5.035205873373586
Validation loss: 4.387670181103954

Epoch: 27| Step: 0
Training loss: 4.472064303077425
Validation loss: 4.383092743226232

Epoch: 5| Step: 1
Training loss: 3.886740710325838
Validation loss: 4.378394195354832

Epoch: 5| Step: 2
Training loss: 4.290573993332447
Validation loss: 4.373648911709146

Epoch: 5| Step: 3
Training loss: 5.216541668224108
Validation loss: 4.369254949600476

Epoch: 5| Step: 4
Training loss: 4.665571674673284
Validation loss: 4.364762972440516

Epoch: 5| Step: 5
Training loss: 4.569321223294592
Validation loss: 4.359960405063493

Epoch: 5| Step: 6
Training loss: 3.506772437247068
Validation loss: 4.355147569158424

Epoch: 5| Step: 7
Training loss: 4.0408957358967665
Validation loss: 4.350655813401687

Epoch: 5| Step: 8
Training loss: 4.889973654958829
Validation loss: 4.3462216505812545

Epoch: 5| Step: 9
Training loss: 4.864874864401865
Validation loss: 4.341396261252573

Epoch: 5| Step: 10
Training loss: 4.481340079153701
Validation loss: 4.336396664429809

Epoch: 5| Step: 11
Training loss: 5.486310742229493
Validation loss: 4.331803669296128

Epoch: 28| Step: 0
Training loss: 3.8287265849853056
Validation loss: 4.327235005266884

Epoch: 5| Step: 1
Training loss: 4.687605996522926
Validation loss: 4.322623151749786

Epoch: 5| Step: 2
Training loss: 4.772368515865126
Validation loss: 4.317550769810019

Epoch: 5| Step: 3
Training loss: 4.220595796660014
Validation loss: 4.312784250078105

Epoch: 5| Step: 4
Training loss: 4.419312290528451
Validation loss: 4.307649732080643

Epoch: 5| Step: 5
Training loss: 4.6850231493334205
Validation loss: 4.303144299499183

Epoch: 5| Step: 6
Training loss: 4.654253435592502
Validation loss: 4.298052327093295

Epoch: 5| Step: 7
Training loss: 4.504164993516374
Validation loss: 4.293153050186132

Epoch: 5| Step: 8
Training loss: 4.613367866394215
Validation loss: 4.288455672379063

Epoch: 5| Step: 9
Training loss: 3.980820090948168
Validation loss: 4.283376207520558

Epoch: 5| Step: 10
Training loss: 4.38287299075346
Validation loss: 4.2791137748997965

Epoch: 5| Step: 11
Training loss: 4.0070719192710955
Validation loss: 4.274225486620372

Epoch: 29| Step: 0
Training loss: 4.065103958133136
Validation loss: 4.269257616102753

Epoch: 5| Step: 1
Training loss: 5.069345814109768
Validation loss: 4.264852926693139

Epoch: 5| Step: 2
Training loss: 4.75805442959158
Validation loss: 4.260102307847744

Epoch: 5| Step: 3
Training loss: 4.194611616922511
Validation loss: 4.255155311668135

Epoch: 5| Step: 4
Training loss: 4.099540517227824
Validation loss: 4.250617622863055

Epoch: 5| Step: 5
Training loss: 4.243458369576184
Validation loss: 4.246010261002611

Epoch: 5| Step: 6
Training loss: 4.719616538813032
Validation loss: 4.241718409147106

Epoch: 5| Step: 7
Training loss: 4.0406288046645615
Validation loss: 4.236722157222731

Epoch: 5| Step: 8
Training loss: 4.1727268906531005
Validation loss: 4.232231648713019

Epoch: 5| Step: 9
Training loss: 4.169854635728381
Validation loss: 4.22734416096176

Epoch: 5| Step: 10
Training loss: 4.543203438021243
Validation loss: 4.222854284958985

Epoch: 5| Step: 11
Training loss: 4.054635519822762
Validation loss: 4.218145270627425

Epoch: 30| Step: 0
Training loss: 4.365106786446361
Validation loss: 4.213491826967044

Epoch: 5| Step: 1
Training loss: 4.997590628904714
Validation loss: 4.209124050197786

Epoch: 5| Step: 2
Training loss: 3.3748979906277987
Validation loss: 4.204098259086064

Epoch: 5| Step: 3
Training loss: 4.4835344649983595
Validation loss: 4.199687435993094

Epoch: 5| Step: 4
Training loss: 4.159515933336789
Validation loss: 4.19524465419625

Epoch: 5| Step: 5
Training loss: 4.372597388899993
Validation loss: 4.190661769234367

Epoch: 5| Step: 6
Training loss: 4.545365440188862
Validation loss: 4.18625290648165

Epoch: 5| Step: 7
Training loss: 4.344457335478727
Validation loss: 4.181630694778295

Epoch: 5| Step: 8
Training loss: 4.305482102181142
Validation loss: 4.176727406597202

Epoch: 5| Step: 9
Training loss: 3.610284203625086
Validation loss: 4.172174331080047

Epoch: 5| Step: 10
Training loss: 4.702280570735648
Validation loss: 4.167512715592387

Epoch: 5| Step: 11
Training loss: 4.488720001740844
Validation loss: 4.16296871914223

Epoch: 31| Step: 0
Training loss: 4.277724993711748
Validation loss: 4.158364353306313

Epoch: 5| Step: 1
Training loss: 3.45787336362287
Validation loss: 4.153334003410673

Epoch: 5| Step: 2
Training loss: 4.736079246038421
Validation loss: 4.1491188021476715

Epoch: 5| Step: 3
Training loss: 5.03639290052775
Validation loss: 4.144299168969475

Epoch: 5| Step: 4
Training loss: 4.635541501310929
Validation loss: 4.139520351440826

Epoch: 5| Step: 5
Training loss: 4.263874742868976
Validation loss: 4.135136134371805

Epoch: 5| Step: 6
Training loss: 3.7534794242257328
Validation loss: 4.130345527066824

Epoch: 5| Step: 7
Training loss: 3.502932137753534
Validation loss: 4.125163995006741

Epoch: 5| Step: 8
Training loss: 4.282343147108817
Validation loss: 4.121051353007501

Epoch: 5| Step: 9
Training loss: 4.247173940482411
Validation loss: 4.116430025918327

Epoch: 5| Step: 10
Training loss: 4.660184695467984
Validation loss: 4.111959010004943

Epoch: 5| Step: 11
Training loss: 2.9480144542924624
Validation loss: 4.107110921565776

Epoch: 32| Step: 0
Training loss: 3.4133939808682983
Validation loss: 4.102762508141517

Epoch: 5| Step: 1
Training loss: 4.824542546699336
Validation loss: 4.098614769981409

Epoch: 5| Step: 2
Training loss: 4.557067981861648
Validation loss: 4.0941320634142215

Epoch: 5| Step: 3
Training loss: 4.653398703268091
Validation loss: 4.089589399792212

Epoch: 5| Step: 4
Training loss: 4.12925708619145
Validation loss: 4.085243842116699

Epoch: 5| Step: 5
Training loss: 3.8524930189739055
Validation loss: 4.080581758129514

Epoch: 5| Step: 6
Training loss: 4.281375187066843
Validation loss: 4.07588935461537

Epoch: 5| Step: 7
Training loss: 4.096478666663683
Validation loss: 4.0712276169533785

Epoch: 5| Step: 8
Training loss: 3.583479294280778
Validation loss: 4.066778121395448

Epoch: 5| Step: 9
Training loss: 4.202086139476913
Validation loss: 4.062303132397832

Epoch: 5| Step: 10
Training loss: 4.496956113676236
Validation loss: 4.058002868268419

Epoch: 5| Step: 11
Training loss: 4.297056548011283
Validation loss: 4.05331905452405

Epoch: 33| Step: 0
Training loss: 4.351086027129148
Validation loss: 4.048964374689192

Epoch: 5| Step: 1
Training loss: 3.4305813608644735
Validation loss: 4.04427159643647

Epoch: 5| Step: 2
Training loss: 4.644431867891732
Validation loss: 4.0397902275223085

Epoch: 5| Step: 3
Training loss: 4.472400588012362
Validation loss: 4.035149519189401

Epoch: 5| Step: 4
Training loss: 4.248473847076957
Validation loss: 4.030470878756777

Epoch: 5| Step: 5
Training loss: 4.368691828480961
Validation loss: 4.025790369137119

Epoch: 5| Step: 6
Training loss: 4.179639584275582
Validation loss: 4.021365884817252

Epoch: 5| Step: 7
Training loss: 3.5706796269550773
Validation loss: 4.01680993929981

Epoch: 5| Step: 8
Training loss: 4.039631963905697
Validation loss: 4.012106000465359

Epoch: 5| Step: 9
Training loss: 4.219543608442719
Validation loss: 4.007774665163154

Epoch: 5| Step: 10
Training loss: 4.472147470379993
Validation loss: 4.003091367589706

Epoch: 5| Step: 11
Training loss: 0.22677847006687865
Validation loss: 3.99870645945945

Epoch: 34| Step: 0
Training loss: 3.8938951345860398
Validation loss: 3.99431989122939

Epoch: 5| Step: 1
Training loss: 3.931509153135906
Validation loss: 3.9899964761917626

Epoch: 5| Step: 2
Training loss: 4.7416489120892935
Validation loss: 3.9858102245539557

Epoch: 5| Step: 3
Training loss: 4.880286846798173
Validation loss: 3.981659536093172

Epoch: 5| Step: 4
Training loss: 4.291319428750929
Validation loss: 3.9771323404678114

Epoch: 5| Step: 5
Training loss: 3.1005024287474674
Validation loss: 3.972537078620361

Epoch: 5| Step: 6
Training loss: 4.654915637635752
Validation loss: 3.968078796819508

Epoch: 5| Step: 7
Training loss: 3.5910908024227286
Validation loss: 3.9638448925884053

Epoch: 5| Step: 8
Training loss: 4.3352259146530185
Validation loss: 3.9596539185114827

Epoch: 5| Step: 9
Training loss: 3.7233058686867073
Validation loss: 3.9548239292670275

Epoch: 5| Step: 10
Training loss: 3.7751159094390343
Validation loss: 3.9505587102205397

Epoch: 5| Step: 11
Training loss: 3.530028545491158
Validation loss: 3.946123464096495

Epoch: 35| Step: 0
Training loss: 4.299360520473919
Validation loss: 3.942029774509679

Epoch: 5| Step: 1
Training loss: 3.9780395402741253
Validation loss: 3.93736169965761

Epoch: 5| Step: 2
Training loss: 4.036370625517197
Validation loss: 3.9331395361459105

Epoch: 5| Step: 3
Training loss: 3.787403364805721
Validation loss: 3.928804286954332

Epoch: 5| Step: 4
Training loss: 3.7411649258541657
Validation loss: 3.924302108743033

Epoch: 5| Step: 5
Training loss: 3.671094446208569
Validation loss: 3.920222513061337

Epoch: 5| Step: 6
Training loss: 3.6293368212421298
Validation loss: 3.9159570196495954

Epoch: 5| Step: 7
Training loss: 4.516523753552753
Validation loss: 3.911744183060919

Epoch: 5| Step: 8
Training loss: 4.979402650909815
Validation loss: 3.9073852161072273

Epoch: 5| Step: 9
Training loss: 4.20256452829721
Validation loss: 3.902868251536219

Epoch: 5| Step: 10
Training loss: 3.872101715076004
Validation loss: 3.8984587776454527

Epoch: 5| Step: 11
Training loss: 1.767453285150303
Validation loss: 3.8937485449447893

Epoch: 36| Step: 0
Training loss: 3.791187940844877
Validation loss: 3.8895073834063236

Epoch: 5| Step: 1
Training loss: 4.492626294805776
Validation loss: 3.8854306794323814

Epoch: 5| Step: 2
Training loss: 2.9474542657282354
Validation loss: 3.8808321133535832

Epoch: 5| Step: 3
Training loss: 3.3737017642428286
Validation loss: 3.8766200565532984

Epoch: 5| Step: 4
Training loss: 4.559792956621054
Validation loss: 3.872423443399735

Epoch: 5| Step: 5
Training loss: 4.345181236218569
Validation loss: 3.8683286274086

Epoch: 5| Step: 6
Training loss: 4.106875279400006
Validation loss: 3.863778329517277

Epoch: 5| Step: 7
Training loss: 4.276150304395444
Validation loss: 3.859455849113421

Epoch: 5| Step: 8
Training loss: 4.562531144545349
Validation loss: 3.8548029589154744

Epoch: 5| Step: 9
Training loss: 3.8128565637165104
Validation loss: 3.8500431048056845

Epoch: 5| Step: 10
Training loss: 3.041895156708919
Validation loss: 3.845433732195825

Epoch: 5| Step: 11
Training loss: 5.200637910302184
Validation loss: 3.84107435601752

Epoch: 37| Step: 0
Training loss: 4.177428474658623
Validation loss: 3.836549434293165

Epoch: 5| Step: 1
Training loss: 3.8705576382046116
Validation loss: 3.8320655056468675

Epoch: 5| Step: 2
Training loss: 3.8593509070999623
Validation loss: 3.82736739362671

Epoch: 5| Step: 3
Training loss: 3.9486988000789727
Validation loss: 3.8229810083918547

Epoch: 5| Step: 4
Training loss: 3.5121636877908218
Validation loss: 3.8187524669616355

Epoch: 5| Step: 5
Training loss: 4.002930759598234
Validation loss: 3.8142677700924517

Epoch: 5| Step: 6
Training loss: 3.783530060166897
Validation loss: 3.8100620823655613

Epoch: 5| Step: 7
Training loss: 4.1742818797280306
Validation loss: 3.805754264615486

Epoch: 5| Step: 8
Training loss: 3.679410045230733
Validation loss: 3.8013253836917165

Epoch: 5| Step: 9
Training loss: 4.102112593319628
Validation loss: 3.796493197110081

Epoch: 5| Step: 10
Training loss: 4.292354911258399
Validation loss: 3.7921971257457896

Epoch: 5| Step: 11
Training loss: 3.788841760242208
Validation loss: 3.787734296369224

Epoch: 38| Step: 0
Training loss: 4.070052184349139
Validation loss: 3.783102380464471

Epoch: 5| Step: 1
Training loss: 3.8275210780006246
Validation loss: 3.778687757088171

Epoch: 5| Step: 2
Training loss: 3.8368177472242593
Validation loss: 3.7739900035759457

Epoch: 5| Step: 3
Training loss: 3.962361397710882
Validation loss: 3.769738790171645

Epoch: 5| Step: 4
Training loss: 3.386425177181958
Validation loss: 3.7650078601510253

Epoch: 5| Step: 5
Training loss: 3.8315716855783966
Validation loss: 3.760512800740219

Epoch: 5| Step: 6
Training loss: 4.607522469360737
Validation loss: 3.756107940357153

Epoch: 5| Step: 7
Training loss: 4.095597178007969
Validation loss: 3.7515713684324625

Epoch: 5| Step: 8
Training loss: 3.438492510302764
Validation loss: 3.7472390130344495

Epoch: 5| Step: 9
Training loss: 3.6365022687061774
Validation loss: 3.742741760747314

Epoch: 5| Step: 10
Training loss: 4.051329999520484
Validation loss: 3.7380448296173445

Epoch: 5| Step: 11
Training loss: 3.737709376361559
Validation loss: 3.733534411070366

Epoch: 39| Step: 0
Training loss: 3.9619784520356154
Validation loss: 3.7290984721137588

Epoch: 5| Step: 1
Training loss: 3.9682793525913262
Validation loss: 3.7245515693188915

Epoch: 5| Step: 2
Training loss: 4.522420976495605
Validation loss: 3.720142814251685

Epoch: 5| Step: 3
Training loss: 3.1196394147112296
Validation loss: 3.715583869272237

Epoch: 5| Step: 4
Training loss: 3.501130875042349
Validation loss: 3.710939555920483

Epoch: 5| Step: 5
Training loss: 4.170440541513898
Validation loss: 3.706431033091258

Epoch: 5| Step: 6
Training loss: 3.8426700718705997
Validation loss: 3.7022100678438132

Epoch: 5| Step: 7
Training loss: 4.160189718908796
Validation loss: 3.6976110502223873

Epoch: 5| Step: 8
Training loss: 3.3759199230948056
Validation loss: 3.693146800206412

Epoch: 5| Step: 9
Training loss: 3.526000720884668
Validation loss: 3.688533374152584

Epoch: 5| Step: 10
Training loss: 3.969507340387548
Validation loss: 3.6844166240762166

Epoch: 5| Step: 11
Training loss: 3.5469842969548426
Validation loss: 3.680015905672405

Epoch: 40| Step: 0
Training loss: 3.3591010403371815
Validation loss: 3.675546398761336

Epoch: 5| Step: 1
Training loss: 3.4461116772144074
Validation loss: 3.6710356977340024

Epoch: 5| Step: 2
Training loss: 3.9047506888269488
Validation loss: 3.666620404139466

Epoch: 5| Step: 3
Training loss: 4.033698233079276
Validation loss: 3.662403530937819

Epoch: 5| Step: 4
Training loss: 4.311756456932196
Validation loss: 3.657917723730322

Epoch: 5| Step: 5
Training loss: 3.5257114422930877
Validation loss: 3.6534213511248996

Epoch: 5| Step: 6
Training loss: 4.054501450226365
Validation loss: 3.648903489560544

Epoch: 5| Step: 7
Training loss: 2.9770935065538566
Validation loss: 3.6443635466527633

Epoch: 5| Step: 8
Training loss: 3.6428781503450973
Validation loss: 3.640102960270958

Epoch: 5| Step: 9
Training loss: 4.088697737450449
Validation loss: 3.635820484778392

Epoch: 5| Step: 10
Training loss: 3.74527697365295
Validation loss: 3.631630002972171

Epoch: 5| Step: 11
Training loss: 5.433884745195859
Validation loss: 3.627317290819287

Epoch: 41| Step: 0
Training loss: 4.013882151198021
Validation loss: 3.622727487475316

Epoch: 5| Step: 1
Training loss: 3.729440661261957
Validation loss: 3.617986427439123

Epoch: 5| Step: 2
Training loss: 3.492568436909866
Validation loss: 3.6134667618701113

Epoch: 5| Step: 3
Training loss: 4.213709327684787
Validation loss: 3.6088950428423483

Epoch: 5| Step: 4
Training loss: 4.021891294978402
Validation loss: 3.6043405582763977

Epoch: 5| Step: 5
Training loss: 3.699054009415812
Validation loss: 3.599790025256597

Epoch: 5| Step: 6
Training loss: 3.7286456861237065
Validation loss: 3.5952303283929914

Epoch: 5| Step: 7
Training loss: 3.7957723138778054
Validation loss: 3.590729121022948

Epoch: 5| Step: 8
Training loss: 3.6272618044803697
Validation loss: 3.586181128155138

Epoch: 5| Step: 9
Training loss: 3.5232060730711163
Validation loss: 3.5818319566836547

Epoch: 5| Step: 10
Training loss: 3.2841449863893
Validation loss: 3.5774578488663775

Epoch: 5| Step: 11
Training loss: 3.17163821684893
Validation loss: 3.5731641370449214

Epoch: 42| Step: 0
Training loss: 3.500902059658802
Validation loss: 3.569024424738244

Epoch: 5| Step: 1
Training loss: 3.475359693614461
Validation loss: 3.5648865374432117

Epoch: 5| Step: 2
Training loss: 3.568369530807934
Validation loss: 3.5606123736354465

Epoch: 5| Step: 3
Training loss: 3.636358946016928
Validation loss: 3.556807101950117

Epoch: 5| Step: 4
Training loss: 3.6259760364765308
Validation loss: 3.552339676960297

Epoch: 5| Step: 5
Training loss: 3.519496159690846
Validation loss: 3.548335033949096

Epoch: 5| Step: 6
Training loss: 4.015272548066184
Validation loss: 3.5442520055146995

Epoch: 5| Step: 7
Training loss: 3.5851536644173776
Validation loss: 3.5402291896231683

Epoch: 5| Step: 8
Training loss: 3.766580800678455
Validation loss: 3.5362154109859296

Epoch: 5| Step: 9
Training loss: 3.5417648002642164
Validation loss: 3.531982053422389

Epoch: 5| Step: 10
Training loss: 4.208288261751343
Validation loss: 3.5278337827543953

Epoch: 5| Step: 11
Training loss: 3.791620331086358
Validation loss: 3.523752777813009

Epoch: 43| Step: 0
Training loss: 4.292863894875061
Validation loss: 3.519357652295691

Epoch: 5| Step: 1
Training loss: 3.4067395059770087
Validation loss: 3.515123919346105

Epoch: 5| Step: 2
Training loss: 4.111819856282322
Validation loss: 3.5108307821513685

Epoch: 5| Step: 3
Training loss: 3.504916144441323
Validation loss: 3.5064954203745233

Epoch: 5| Step: 4
Training loss: 3.20050894743055
Validation loss: 3.502436765998047

Epoch: 5| Step: 5
Training loss: 3.2471475554722296
Validation loss: 3.4981238922722615

Epoch: 5| Step: 6
Training loss: 3.576783440974638
Validation loss: 3.494071935607909

Epoch: 5| Step: 7
Training loss: 3.2784155819583556
Validation loss: 3.490173100220207

Epoch: 5| Step: 8
Training loss: 3.4289235717586233
Validation loss: 3.4860789290982206

Epoch: 5| Step: 9
Training loss: 4.214889455411362
Validation loss: 3.4820847275880915

Epoch: 5| Step: 10
Training loss: 3.5459534376328676
Validation loss: 3.478047640186652

Epoch: 5| Step: 11
Training loss: 3.5633150473382105
Validation loss: 3.47397569864595

Epoch: 44| Step: 0
Training loss: 3.5892371992507477
Validation loss: 3.469807666904675

Epoch: 5| Step: 1
Training loss: 3.3675657267906995
Validation loss: 3.4657091954342922

Epoch: 5| Step: 2
Training loss: 2.805957304849267
Validation loss: 3.4615518958898828

Epoch: 5| Step: 3
Training loss: 3.848820017530535
Validation loss: 3.457814057940629

Epoch: 5| Step: 4
Training loss: 3.327803761913797
Validation loss: 3.453763888305029

Epoch: 5| Step: 5
Training loss: 3.672360781737366
Validation loss: 3.4499940443678603

Epoch: 5| Step: 6
Training loss: 4.0653010175485464
Validation loss: 3.4460581508179406

Epoch: 5| Step: 7
Training loss: 4.475748197780081
Validation loss: 3.4420525719345902

Epoch: 5| Step: 8
Training loss: 3.800406238525011
Validation loss: 3.437893908501668

Epoch: 5| Step: 9
Training loss: 3.4948785641247273
Validation loss: 3.4336723923311547

Epoch: 5| Step: 10
Training loss: 3.0372356032852115
Validation loss: 3.429349860948595

Epoch: 5| Step: 11
Training loss: 1.2673820261630226
Validation loss: 3.4251587264785504

Epoch: 45| Step: 0
Training loss: 3.3559643842332694
Validation loss: 3.421707381464418

Epoch: 5| Step: 1
Training loss: 3.4714419255344597
Validation loss: 3.417796077181328

Epoch: 5| Step: 2
Training loss: 3.5446714034274556
Validation loss: 3.4141338727926516

Epoch: 5| Step: 3
Training loss: 3.3309994315264584
Validation loss: 3.4104823145590304

Epoch: 5| Step: 4
Training loss: 3.992718386950509
Validation loss: 3.4067461428233305

Epoch: 5| Step: 5
Training loss: 3.024631313839054
Validation loss: 3.4031669126042514

Epoch: 5| Step: 6
Training loss: 4.14191690120107
Validation loss: 3.3994883699303737

Epoch: 5| Step: 7
Training loss: 3.100166064090594
Validation loss: 3.3956578663775954

Epoch: 5| Step: 8
Training loss: 3.5204662304669814
Validation loss: 3.3920314145532213

Epoch: 5| Step: 9
Training loss: 3.8005008417647264
Validation loss: 3.3883805881744578

Epoch: 5| Step: 10
Training loss: 3.6613125435178024
Validation loss: 3.3847340213439705

Epoch: 5| Step: 11
Training loss: 2.6710612524643653
Validation loss: 3.380690827732889

Epoch: 46| Step: 0
Training loss: 3.520946493343001
Validation loss: 3.3770113885375146

Epoch: 5| Step: 1
Training loss: 3.43620470091379
Validation loss: 3.3730881273743387

Epoch: 5| Step: 2
Training loss: 3.730245054238776
Validation loss: 3.3695325953298623

Epoch: 5| Step: 3
Training loss: 3.7034399486157366
Validation loss: 3.365781521500322

Epoch: 5| Step: 4
Training loss: 3.730153526848768
Validation loss: 3.361924988270456

Epoch: 5| Step: 5
Training loss: 3.347176480717204
Validation loss: 3.3579517868735587

Epoch: 5| Step: 6
Training loss: 3.431644793590951
Validation loss: 3.354302342358091

Epoch: 5| Step: 7
Training loss: 3.3440969411953083
Validation loss: 3.3505604927462906

Epoch: 5| Step: 8
Training loss: 3.44893381533851
Validation loss: 3.346717646509421

Epoch: 5| Step: 9
Training loss: 3.723015270553285
Validation loss: 3.34304017111533

Epoch: 5| Step: 10
Training loss: 3.313131739996873
Validation loss: 3.3393266065554785

Epoch: 5| Step: 11
Training loss: 1.4992567446189462
Validation loss: 3.3354797168294725

Epoch: 47| Step: 0
Training loss: 3.8448117658278718
Validation loss: 3.332101485006687

Epoch: 5| Step: 1
Training loss: 2.871338544517425
Validation loss: 3.3284847112887768

Epoch: 5| Step: 2
Training loss: 2.8050275081162863
Validation loss: 3.325047402772998

Epoch: 5| Step: 3
Training loss: 2.76046288409619
Validation loss: 3.32170861007813

Epoch: 5| Step: 4
Training loss: 3.7066518993449438
Validation loss: 3.3184149132942493

Epoch: 5| Step: 5
Training loss: 3.804079230405004
Validation loss: 3.314928759868565

Epoch: 5| Step: 6
Training loss: 3.7685549405610175
Validation loss: 3.3112449247638867

Epoch: 5| Step: 7
Training loss: 3.987358860878314
Validation loss: 3.30762929119204

Epoch: 5| Step: 8
Training loss: 3.7151213821974176
Validation loss: 3.303877659882412

Epoch: 5| Step: 9
Training loss: 3.2409059496553922
Validation loss: 3.3003284602752605

Epoch: 5| Step: 10
Training loss: 3.1340241407914324
Validation loss: 3.2966733361589124

Epoch: 5| Step: 11
Training loss: 3.716992387625776
Validation loss: 3.293100410987912

Epoch: 48| Step: 0
Training loss: 3.6101358775934598
Validation loss: 3.2896028770959815

Epoch: 5| Step: 1
Training loss: 3.8367421846445016
Validation loss: 3.2858041373153895

Epoch: 5| Step: 2
Training loss: 3.112039469157288
Validation loss: 3.2821795161215315

Epoch: 5| Step: 3
Training loss: 3.1959496494762405
Validation loss: 3.278486944362757

Epoch: 5| Step: 4
Training loss: 3.757013882418204
Validation loss: 3.2748474659475413

Epoch: 5| Step: 5
Training loss: 3.267517271689405
Validation loss: 3.2711475468413336

Epoch: 5| Step: 6
Training loss: 3.2743431422661957
Validation loss: 3.2677725129679764

Epoch: 5| Step: 7
Training loss: 3.5196297448672906
Validation loss: 3.2640106250614935

Epoch: 5| Step: 8
Training loss: 3.4762991473287723
Validation loss: 3.260754157485462

Epoch: 5| Step: 9
Training loss: 2.9408616654834776
Validation loss: 3.256974601176714

Epoch: 5| Step: 10
Training loss: 3.4425617750466304
Validation loss: 3.253456295268021

Epoch: 5| Step: 11
Training loss: 3.3851358306994355
Validation loss: 3.2497968915904707

Epoch: 49| Step: 0
Training loss: 3.6733204695193082
Validation loss: 3.2462072126907775

Epoch: 5| Step: 1
Training loss: 3.299891967161059
Validation loss: 3.242567948536524

Epoch: 5| Step: 2
Training loss: 3.7392062933085755
Validation loss: 3.2390844333002375

Epoch: 5| Step: 3
Training loss: 3.1195999791393434
Validation loss: 3.2354086914913993

Epoch: 5| Step: 4
Training loss: 3.125953528841646
Validation loss: 3.2318661983333006

Epoch: 5| Step: 5
Training loss: 3.4966990026888607
Validation loss: 3.228435893073301

Epoch: 5| Step: 6
Training loss: 2.873607464109892
Validation loss: 3.2247077623434275

Epoch: 5| Step: 7
Training loss: 2.9884191144668675
Validation loss: 3.221651991221839

Epoch: 5| Step: 8
Training loss: 3.3452980522260702
Validation loss: 3.218027061458269

Epoch: 5| Step: 9
Training loss: 3.448425547376417
Validation loss: 3.214709251683493

Epoch: 5| Step: 10
Training loss: 3.809390191164465
Validation loss: 3.211245086614687

Epoch: 5| Step: 11
Training loss: 3.405224146922445
Validation loss: 3.20779615194593

Epoch: 50| Step: 0
Training loss: 3.334997397540448
Validation loss: 3.204643547045601

Epoch: 5| Step: 1
Training loss: 3.3280374578146916
Validation loss: 3.201240360380051

Epoch: 5| Step: 2
Training loss: 3.10739057082755
Validation loss: 3.198197558530858

Epoch: 5| Step: 3
Training loss: 3.4462737042286715
Validation loss: 3.1949697744405383

Epoch: 5| Step: 4
Training loss: 3.155535928757691
Validation loss: 3.1919301356706415

Epoch: 5| Step: 5
Training loss: 3.521123765030782
Validation loss: 3.188636751615086

Epoch: 5| Step: 6
Training loss: 3.147576529108037
Validation loss: 3.185494670989801

Epoch: 5| Step: 7
Training loss: 3.1244115656929607
Validation loss: 3.182634180161531

Epoch: 5| Step: 8
Training loss: 3.2234711327695846
Validation loss: 3.1792109957979613

Epoch: 5| Step: 9
Training loss: 3.279759240967382
Validation loss: 3.176139295225827

Epoch: 5| Step: 10
Training loss: 3.7914802309808957
Validation loss: 3.1728024121304492

Epoch: 5| Step: 11
Training loss: 3.799278973889065
Validation loss: 3.1694554024675106

Epoch: 51| Step: 0
Training loss: 2.9322766879000106
Validation loss: 3.166194476928305

Epoch: 5| Step: 1
Training loss: 3.265433711970051
Validation loss: 3.1628743330865814

Epoch: 5| Step: 2
Training loss: 2.874641561969465
Validation loss: 3.1597307293582926

Epoch: 5| Step: 3
Training loss: 4.0813242322821734
Validation loss: 3.1563347625681515

Epoch: 5| Step: 4
Training loss: 3.7500976549784673
Validation loss: 3.153219921395952

Epoch: 5| Step: 5
Training loss: 2.994018631830641
Validation loss: 3.1497402383026456

Epoch: 5| Step: 6
Training loss: 3.1449771920303515
Validation loss: 3.1462123665663646

Epoch: 5| Step: 7
Training loss: 3.4575325984893763
Validation loss: 3.1428264095457172

Epoch: 5| Step: 8
Training loss: 3.069243344431397
Validation loss: 3.13943610771136

Epoch: 5| Step: 9
Training loss: 3.4486888163034917
Validation loss: 3.1360641182764164

Epoch: 5| Step: 10
Training loss: 3.095991305346441
Validation loss: 3.132834478033255

Epoch: 5| Step: 11
Training loss: 2.6625365420417273
Validation loss: 3.1295466408635275

Epoch: 52| Step: 0
Training loss: 3.54757253367043
Validation loss: 3.126683055327057

Epoch: 5| Step: 1
Training loss: 3.5701611776859266
Validation loss: 3.123726585334012

Epoch: 5| Step: 2
Training loss: 2.2978970076918124
Validation loss: 3.1207599357934335

Epoch: 5| Step: 3
Training loss: 2.965486037251531
Validation loss: 3.117974063362495

Epoch: 5| Step: 4
Training loss: 3.4773669737239827
Validation loss: 3.1150639055859837

Epoch: 5| Step: 5
Training loss: 2.9788322705662686
Validation loss: 3.111930493515506

Epoch: 5| Step: 6
Training loss: 3.4028199450589818
Validation loss: 3.1092068165034217

Epoch: 5| Step: 7
Training loss: 3.218529258491921
Validation loss: 3.1063655558019874

Epoch: 5| Step: 8
Training loss: 3.3837886397456445
Validation loss: 3.10353722473784

Epoch: 5| Step: 9
Training loss: 3.7136749771536954
Validation loss: 3.1007458262631897

Epoch: 5| Step: 10
Training loss: 3.063192172556391
Validation loss: 3.0975749649344357

Epoch: 5| Step: 11
Training loss: 2.8942991258877764
Validation loss: 3.094620450544865

Epoch: 53| Step: 0
Training loss: 3.650136939837385
Validation loss: 3.0914724456484195

Epoch: 5| Step: 1
Training loss: 3.4036533362440418
Validation loss: 3.0883077122218583

Epoch: 5| Step: 2
Training loss: 2.6316866405937818
Validation loss: 3.0852714946593514

Epoch: 5| Step: 3
Training loss: 3.507860077193358
Validation loss: 3.082246019481554

Epoch: 5| Step: 4
Training loss: 3.0071304142399726
Validation loss: 3.0789406110576714

Epoch: 5| Step: 5
Training loss: 2.758819482686266
Validation loss: 3.075996511799447

Epoch: 5| Step: 6
Training loss: 2.881367473731311
Validation loss: 3.0729547250406006

Epoch: 5| Step: 7
Training loss: 3.2918834051572077
Validation loss: 3.0701434774037826

Epoch: 5| Step: 8
Training loss: 3.567467822950285
Validation loss: 3.06721553611541

Epoch: 5| Step: 9
Training loss: 2.775441737714381
Validation loss: 3.0641858462688827

Epoch: 5| Step: 10
Training loss: 3.622153381234359
Validation loss: 3.0613853541074985

Epoch: 5| Step: 11
Training loss: 3.5694009965428504
Validation loss: 3.058438072611198

Epoch: 54| Step: 0
Training loss: 2.3138268247078364
Validation loss: 3.055358273225488

Epoch: 5| Step: 1
Training loss: 3.2134352573148415
Validation loss: 3.0523083292514754

Epoch: 5| Step: 2
Training loss: 3.483718468092564
Validation loss: 3.049237347838608

Epoch: 5| Step: 3
Training loss: 3.2995435572450362
Validation loss: 3.046101460524952

Epoch: 5| Step: 4
Training loss: 3.3415511557638786
Validation loss: 3.0431047049280666

Epoch: 5| Step: 5
Training loss: 3.0187752037496804
Validation loss: 3.0399795299288406

Epoch: 5| Step: 6
Training loss: 3.339212922424112
Validation loss: 3.0369728507443634

Epoch: 5| Step: 7
Training loss: 3.2327067177650832
Validation loss: 3.033915877908392

Epoch: 5| Step: 8
Training loss: 3.431607831907912
Validation loss: 3.0309939784312467

Epoch: 5| Step: 9
Training loss: 2.7197822277460975
Validation loss: 3.028066910539861

Epoch: 5| Step: 10
Training loss: 3.5050183150701297
Validation loss: 3.0254284581543582

Epoch: 5| Step: 11
Training loss: 2.6552159708980017
Validation loss: 3.0226425979081153

Epoch: 55| Step: 0
Training loss: 3.2843046955747
Validation loss: 3.0196562984180613

Epoch: 5| Step: 1
Training loss: 3.826628972664268
Validation loss: 3.016770637724443

Epoch: 5| Step: 2
Training loss: 2.5015393286447343
Validation loss: 3.014001844318866

Epoch: 5| Step: 3
Training loss: 3.1628011567638206
Validation loss: 3.0109882780028703

Epoch: 5| Step: 4
Training loss: 3.4680309753266614
Validation loss: 3.008082944923567

Epoch: 5| Step: 5
Training loss: 3.336944642455725
Validation loss: 3.0052967135668696

Epoch: 5| Step: 6
Training loss: 2.9871201596938994
Validation loss: 3.002196663396839

Epoch: 5| Step: 7
Training loss: 2.6946519125103983
Validation loss: 2.9991245383161838

Epoch: 5| Step: 8
Training loss: 3.2383286641771623
Validation loss: 2.9962072073265538

Epoch: 5| Step: 9
Training loss: 3.221712397293406
Validation loss: 2.993550918744977

Epoch: 5| Step: 10
Training loss: 2.618031744226115
Validation loss: 2.9908611986937115

Epoch: 5| Step: 11
Training loss: 3.3943740580063513
Validation loss: 2.9882936930345427

Epoch: 56| Step: 0
Training loss: 2.709559549384393
Validation loss: 2.985554413813196

Epoch: 5| Step: 1
Training loss: 3.2533506580839955
Validation loss: 2.982834286790869

Epoch: 5| Step: 2
Training loss: 3.1190782515679687
Validation loss: 2.980262459231543

Epoch: 5| Step: 3
Training loss: 3.355644390229936
Validation loss: 2.9774980351419957

Epoch: 5| Step: 4
Training loss: 2.8967434850178164
Validation loss: 2.97463233823673

Epoch: 5| Step: 5
Training loss: 3.334820097913712
Validation loss: 2.971847972352532

Epoch: 5| Step: 6
Training loss: 2.5814149152708317
Validation loss: 2.969318253524859

Epoch: 5| Step: 7
Training loss: 3.457547355101381
Validation loss: 2.9667897662775973

Epoch: 5| Step: 8
Training loss: 3.2990009992077054
Validation loss: 2.9643088902455474

Epoch: 5| Step: 9
Training loss: 2.7817882917025725
Validation loss: 2.9615804734046414

Epoch: 5| Step: 10
Training loss: 3.126963951954632
Validation loss: 2.959125672377554

Epoch: 5| Step: 11
Training loss: 4.040435498625698
Validation loss: 2.9565978444406706

Epoch: 57| Step: 0
Training loss: 3.277818902255194
Validation loss: 2.9539848468517125

Epoch: 5| Step: 1
Training loss: 2.8259613021933156
Validation loss: 2.9513866986783945

Epoch: 5| Step: 2
Training loss: 3.240893884905486
Validation loss: 2.9487848497406377

Epoch: 5| Step: 3
Training loss: 3.706394860458516
Validation loss: 2.946252674505643

Epoch: 5| Step: 4
Training loss: 2.710450582172351
Validation loss: 2.9435616237897917

Epoch: 5| Step: 5
Training loss: 3.290919126459873
Validation loss: 2.9408858110288025

Epoch: 5| Step: 6
Training loss: 3.1292357158196205
Validation loss: 2.938215841498278

Epoch: 5| Step: 7
Training loss: 2.4122525933254018
Validation loss: 2.9357014114336035

Epoch: 5| Step: 8
Training loss: 2.8734609382009526
Validation loss: 2.9331657209156585

Epoch: 5| Step: 9
Training loss: 3.153709522279363
Validation loss: 2.930946035695712

Epoch: 5| Step: 10
Training loss: 3.0648234370343936
Validation loss: 2.928497561078117

Epoch: 5| Step: 11
Training loss: 3.3461084337466747
Validation loss: 2.92623149223944

Epoch: 58| Step: 0
Training loss: 3.117108719291533
Validation loss: 2.9238779543238644

Epoch: 5| Step: 1
Training loss: 2.8024619790866163
Validation loss: 2.9217358180333184

Epoch: 5| Step: 2
Training loss: 3.555970080040613
Validation loss: 2.919329716102486

Epoch: 5| Step: 3
Training loss: 3.1927117020473577
Validation loss: 2.9170984130289432

Epoch: 5| Step: 4
Training loss: 2.616821901207719
Validation loss: 2.914890645485846

Epoch: 5| Step: 5
Training loss: 3.0592243036673405
Validation loss: 2.912767495817615

Epoch: 5| Step: 6
Training loss: 3.094546745278833
Validation loss: 2.9106159913809186

Epoch: 5| Step: 7
Training loss: 2.5489341976850675
Validation loss: 2.9085211510571485

Epoch: 5| Step: 8
Training loss: 2.9181440880323346
Validation loss: 2.906286215471066

Epoch: 5| Step: 9
Training loss: 3.025141745785193
Validation loss: 2.9039700291495207

Epoch: 5| Step: 10
Training loss: 3.568205564427617
Validation loss: 2.901611615711755

Epoch: 5| Step: 11
Training loss: 2.664728433825405
Validation loss: 2.899326160812

Epoch: 59| Step: 0
Training loss: 3.307077251702849
Validation loss: 2.89710875881089

Epoch: 5| Step: 1
Training loss: 3.459685019339829
Validation loss: 2.89489692646697

Epoch: 5| Step: 2
Training loss: 2.8617068928590865
Validation loss: 2.8924943395376252

Epoch: 5| Step: 3
Training loss: 3.349657490999167
Validation loss: 2.8903458297226035

Epoch: 5| Step: 4
Training loss: 2.71292859044709
Validation loss: 2.887996347849281

Epoch: 5| Step: 5
Training loss: 3.0630119051600735
Validation loss: 2.8857884678192787

Epoch: 5| Step: 6
Training loss: 2.8986515264329196
Validation loss: 2.8835748286536393

Epoch: 5| Step: 7
Training loss: 3.525440129067248
Validation loss: 2.881517407109601

Epoch: 5| Step: 8
Training loss: 2.7344131031106356
Validation loss: 2.8793464782993605

Epoch: 5| Step: 9
Training loss: 2.552027726378543
Validation loss: 2.877407140876808

Epoch: 5| Step: 10
Training loss: 2.680116491205766
Validation loss: 2.8753670064284944

Epoch: 5| Step: 11
Training loss: 2.9624505920153057
Validation loss: 2.8731914097511093

Epoch: 60| Step: 0
Training loss: 3.3420819551656638
Validation loss: 2.8711560370161258

Epoch: 5| Step: 1
Training loss: 2.885003474611937
Validation loss: 2.869139246965168

Epoch: 5| Step: 2
Training loss: 3.4958916120858423
Validation loss: 2.8667643110885512

Epoch: 5| Step: 3
Training loss: 2.8328899896862483
Validation loss: 2.864720449922909

Epoch: 5| Step: 4
Training loss: 3.250185154262339
Validation loss: 2.862588025701112

Epoch: 5| Step: 5
Training loss: 3.0870350105319506
Validation loss: 2.8604466978266934

Epoch: 5| Step: 6
Training loss: 2.5776292728895345
Validation loss: 2.858458610121136

Epoch: 5| Step: 7
Training loss: 2.477363432845423
Validation loss: 2.8562621385579345

Epoch: 5| Step: 8
Training loss: 2.9198750061560355
Validation loss: 2.8543504855325046

Epoch: 5| Step: 9
Training loss: 2.800902977164292
Validation loss: 2.852322721826099

Epoch: 5| Step: 10
Training loss: 3.091662540758641
Validation loss: 2.85049667297046

Epoch: 5| Step: 11
Training loss: 3.584967721024977
Validation loss: 2.848496200255215

Epoch: 61| Step: 0
Training loss: 3.3495411857581825
Validation loss: 2.8465154252553746

Epoch: 5| Step: 1
Training loss: 3.086246523293151
Validation loss: 2.8442081878772796

Epoch: 5| Step: 2
Training loss: 3.1991481004676205
Validation loss: 2.842053759872673

Epoch: 5| Step: 3
Training loss: 3.4034412247621018
Validation loss: 2.8398657460784147

Epoch: 5| Step: 4
Training loss: 2.5909622101760856
Validation loss: 2.8379376751683076

Epoch: 5| Step: 5
Training loss: 3.0291067195314443
Validation loss: 2.835616563864744

Epoch: 5| Step: 6
Training loss: 2.734379621229354
Validation loss: 2.833766151542611

Epoch: 5| Step: 7
Training loss: 2.6858758676324874
Validation loss: 2.831639864831964

Epoch: 5| Step: 8
Training loss: 2.5108925986889363
Validation loss: 2.829845565461961

Epoch: 5| Step: 9
Training loss: 2.856347988459522
Validation loss: 2.8278472792131155

Epoch: 5| Step: 10
Training loss: 3.315431125736065
Validation loss: 2.825813620496783

Epoch: 5| Step: 11
Training loss: 2.0635056789237782
Validation loss: 2.8241442349119827

Epoch: 62| Step: 0
Training loss: 2.986896029276793
Validation loss: 2.8222307460438008

Epoch: 5| Step: 1
Training loss: 3.1109943103057716
Validation loss: 2.8204548941582153

Epoch: 5| Step: 2
Training loss: 2.923224152976874
Validation loss: 2.8185569639286836

Epoch: 5| Step: 3
Training loss: 2.6494632861200946
Validation loss: 2.8170016114060252

Epoch: 5| Step: 4
Training loss: 3.1732627200703662
Validation loss: 2.8156188626069

Epoch: 5| Step: 5
Training loss: 3.1033277124612826
Validation loss: 2.813771949240713

Epoch: 5| Step: 6
Training loss: 2.612953975238578
Validation loss: 2.811980637528007

Epoch: 5| Step: 7
Training loss: 3.3813976826737067
Validation loss: 2.810235917113915

Epoch: 5| Step: 8
Training loss: 3.1546034342129974
Validation loss: 2.808546441696209

Epoch: 5| Step: 9
Training loss: 2.541857967577584
Validation loss: 2.8067473955580367

Epoch: 5| Step: 10
Training loss: 2.712519906669735
Validation loss: 2.8049039270173703

Epoch: 5| Step: 11
Training loss: 3.1092967881377347
Validation loss: 2.803158969595354

Epoch: 63| Step: 0
Training loss: 3.241173422453502
Validation loss: 2.801300536271088

Epoch: 5| Step: 1
Training loss: 2.4376672540649444
Validation loss: 2.799060000007197

Epoch: 5| Step: 2
Training loss: 2.898047408101194
Validation loss: 2.797044645879412

Epoch: 5| Step: 3
Training loss: 2.6914322939029276
Validation loss: 2.795228971026188

Epoch: 5| Step: 4
Training loss: 2.895617323761713
Validation loss: 2.793437251564576

Epoch: 5| Step: 5
Training loss: 3.0596784711970852
Validation loss: 2.791854974222135

Epoch: 5| Step: 6
Training loss: 2.808037332694172
Validation loss: 2.790184660768676

Epoch: 5| Step: 7
Training loss: 3.5899219857242617
Validation loss: 2.7884775789686564

Epoch: 5| Step: 8
Training loss: 2.867878261584093
Validation loss: 2.786589084065075

Epoch: 5| Step: 9
Training loss: 2.9775347385894317
Validation loss: 2.7851773511346742

Epoch: 5| Step: 10
Training loss: 2.6932052557144543
Validation loss: 2.7834661526788276

Epoch: 5| Step: 11
Training loss: 2.7458108726435686
Validation loss: 2.781862287952978

Epoch: 64| Step: 0
Training loss: 2.8045848094449095
Validation loss: 2.7801062200113864

Epoch: 5| Step: 1
Training loss: 3.096023186826006
Validation loss: 2.7784211706393087

Epoch: 5| Step: 2
Training loss: 3.1658981461470526
Validation loss: 2.776630544320659

Epoch: 5| Step: 3
Training loss: 2.9151221590961867
Validation loss: 2.774830431668941

Epoch: 5| Step: 4
Training loss: 2.6767074875274286
Validation loss: 2.7729643583794057

Epoch: 5| Step: 5
Training loss: 2.309184610367543
Validation loss: 2.7711433593335926

Epoch: 5| Step: 6
Training loss: 3.3392920322810147
Validation loss: 2.76963881986301

Epoch: 5| Step: 7
Training loss: 2.9730081638156087
Validation loss: 2.7681707565638303

Epoch: 5| Step: 8
Training loss: 3.1416493733248
Validation loss: 2.766282041271895

Epoch: 5| Step: 9
Training loss: 2.947990515405105
Validation loss: 2.7648326255917315

Epoch: 5| Step: 10
Training loss: 2.5045804977168244
Validation loss: 2.7633018901997075

Epoch: 5| Step: 11
Training loss: 3.0435249887753186
Validation loss: 2.761760910985725

Epoch: 65| Step: 0
Training loss: 2.990244102337624
Validation loss: 2.76017266520645

Epoch: 5| Step: 1
Training loss: 2.9644672220045467
Validation loss: 2.7584881697776673

Epoch: 5| Step: 2
Training loss: 2.525026653640119
Validation loss: 2.7569449071352614

Epoch: 5| Step: 3
Training loss: 3.0291046730914455
Validation loss: 2.755680874190069

Epoch: 5| Step: 4
Training loss: 3.520192345652525
Validation loss: 2.754088155451732

Epoch: 5| Step: 5
Training loss: 2.7587815438227192
Validation loss: 2.7524479822018564

Epoch: 5| Step: 6
Training loss: 3.1140788168319387
Validation loss: 2.7508604733644924

Epoch: 5| Step: 7
Training loss: 2.9605413086271724
Validation loss: 2.749061117145514

Epoch: 5| Step: 8
Training loss: 2.7931188156254083
Validation loss: 2.7472938386085004

Epoch: 5| Step: 9
Training loss: 2.5172427644351902
Validation loss: 2.7458121678564575

Epoch: 5| Step: 10
Training loss: 2.6126670856980034
Validation loss: 2.744618558160059

Epoch: 5| Step: 11
Training loss: 2.408126297689762
Validation loss: 2.7431882787269104

Epoch: 66| Step: 0
Training loss: 2.6306063410703793
Validation loss: 2.7419686406712254

Epoch: 5| Step: 1
Training loss: 2.865830102733652
Validation loss: 2.7404844641674027

Epoch: 5| Step: 2
Training loss: 2.547568197638417
Validation loss: 2.738950015549393

Epoch: 5| Step: 3
Training loss: 2.6800144541051725
Validation loss: 2.7373820055900953

Epoch: 5| Step: 4
Training loss: 3.1297176609119113
Validation loss: 2.736043566280484

Epoch: 5| Step: 5
Training loss: 2.5505003606530154
Validation loss: 2.7345946705091193

Epoch: 5| Step: 6
Training loss: 2.9926183803665665
Validation loss: 2.7333179428861327

Epoch: 5| Step: 7
Training loss: 3.162356974266256
Validation loss: 2.7320069731311043

Epoch: 5| Step: 8
Training loss: 2.716064793275962
Validation loss: 2.7307482072970473

Epoch: 5| Step: 9
Training loss: 2.904528035941858
Validation loss: 2.729031094733996

Epoch: 5| Step: 10
Training loss: 3.068445467940025
Validation loss: 2.727593238949017

Epoch: 5| Step: 11
Training loss: 4.108856516572378
Validation loss: 2.726259056191705

Epoch: 67| Step: 0
Training loss: 2.748681619373884
Validation loss: 2.7251851739291344

Epoch: 5| Step: 1
Training loss: 2.978196063799963
Validation loss: 2.7240797578627394

Epoch: 5| Step: 2
Training loss: 3.153186784636094
Validation loss: 2.722575601982353

Epoch: 5| Step: 3
Training loss: 2.6818767535333494
Validation loss: 2.7215939113503613

Epoch: 5| Step: 4
Training loss: 3.0723162220682605
Validation loss: 2.7193392593687533

Epoch: 5| Step: 5
Training loss: 2.8159722828476337
Validation loss: 2.7172134997758373

Epoch: 5| Step: 6
Training loss: 2.607520443771258
Validation loss: 2.7221839272525963

Epoch: 5| Step: 7
Training loss: 2.7095367594546826
Validation loss: 2.726470089487707

Epoch: 5| Step: 8
Training loss: 2.61399934080155
Validation loss: 2.7146794514145016

Epoch: 5| Step: 9
Training loss: 3.091744591758269
Validation loss: 2.7148105317423115

Epoch: 5| Step: 10
Training loss: 2.961714106752081
Validation loss: 2.7145893480864194

Epoch: 5| Step: 11
Training loss: 2.847376931388659
Validation loss: 2.715320880045885

Epoch: 68| Step: 0
Training loss: 2.8782996440506996
Validation loss: 2.725017397647114

Epoch: 5| Step: 1
Training loss: 2.965311247264249
Validation loss: 2.7124489483519163

Epoch: 5| Step: 2
Training loss: 2.59061886334584
Validation loss: 2.7098910485955576

Epoch: 5| Step: 3
Training loss: 2.8256622888458325
Validation loss: 2.708426893281046

Epoch: 5| Step: 4
Training loss: 2.7126674793767505
Validation loss: 2.7069391011989525

Epoch: 5| Step: 5
Training loss: 2.8044873859559147
Validation loss: 2.7060369451760984

Epoch: 5| Step: 6
Training loss: 3.087284923892504
Validation loss: 2.7049323564863332

Epoch: 5| Step: 7
Training loss: 2.833234691772736
Validation loss: 2.7042770760444315

Epoch: 5| Step: 8
Training loss: 3.0181832802383544
Validation loss: 2.700685819161578

Epoch: 5| Step: 9
Training loss: 2.566147603975778
Validation loss: 2.6995850722534054

Epoch: 5| Step: 10
Training loss: 2.8330822066671284
Validation loss: 2.6954375159714816

Epoch: 5| Step: 11
Training loss: 3.7326923234451495
Validation loss: 2.6949013548474934

Epoch: 69| Step: 0
Training loss: 3.0798149310901173
Validation loss: 2.6959779922158256

Epoch: 5| Step: 1
Training loss: 2.8087385668180223
Validation loss: 2.6955531468976432

Epoch: 5| Step: 2
Training loss: 2.767231964667374
Validation loss: 2.694877276088647

Epoch: 5| Step: 3
Training loss: 3.2903589153756747
Validation loss: 2.6939495234047555

Epoch: 5| Step: 4
Training loss: 2.4954673685589888
Validation loss: 2.6931461344789924

Epoch: 5| Step: 5
Training loss: 2.8918775989066523
Validation loss: 2.692809906647993

Epoch: 5| Step: 6
Training loss: 2.6904155973145105
Validation loss: 2.693915505425147

Epoch: 5| Step: 7
Training loss: 2.890999929496054
Validation loss: 2.699559081244826

Epoch: 5| Step: 8
Training loss: 2.988697536619098
Validation loss: 2.6988074159013995

Epoch: 5| Step: 9
Training loss: 2.7302014570696613
Validation loss: 2.6894123828750476

Epoch: 5| Step: 10
Training loss: 2.5177338095530675
Validation loss: 2.6866344048580038

Epoch: 5| Step: 11
Training loss: 2.4938886331443046
Validation loss: 2.6849316219358226

Epoch: 70| Step: 0
Training loss: 3.280942193989076
Validation loss: 2.6830797401886217

Epoch: 5| Step: 1
Training loss: 2.7709273033173885
Validation loss: 2.6815622479866508

Epoch: 5| Step: 2
Training loss: 2.846741664205266
Validation loss: 2.6802619303565374

Epoch: 5| Step: 3
Training loss: 2.708519816703054
Validation loss: 2.6765562990869833

Epoch: 5| Step: 4
Training loss: 2.660179194100759
Validation loss: 2.679944510781002

Epoch: 5| Step: 5
Training loss: 2.6073040996340935
Validation loss: 2.6905378205711297

Epoch: 5| Step: 6
Training loss: 2.9000405867794785
Validation loss: 2.6733995070397003

Epoch: 5| Step: 7
Training loss: 2.15588663328992
Validation loss: 2.675225954376272

Epoch: 5| Step: 8
Training loss: 2.847759731594574
Validation loss: 2.6746438535051458

Epoch: 5| Step: 9
Training loss: 3.0971651589953555
Validation loss: 2.6754905895513694

Epoch: 5| Step: 10
Training loss: 3.1610228454864893
Validation loss: 2.6762668957830975

Epoch: 5| Step: 11
Training loss: 1.8314626498651159
Validation loss: 2.6765944421647063

Epoch: 71| Step: 0
Training loss: 2.558482952485312
Validation loss: 2.677909134926974

Epoch: 5| Step: 1
Training loss: 2.5476891088253
Validation loss: 2.6789716018236236

Epoch: 5| Step: 2
Training loss: 2.517478402280927
Validation loss: 2.6757042384145096

Epoch: 5| Step: 3
Training loss: 2.75097127581472
Validation loss: 2.6726817127134765

Epoch: 5| Step: 4
Training loss: 3.1649988366677797
Validation loss: 2.6662122157907473

Epoch: 5| Step: 5
Training loss: 2.6932051671884936
Validation loss: 2.665252512986071

Epoch: 5| Step: 6
Training loss: 2.827160665587761
Validation loss: 2.661153418784897

Epoch: 5| Step: 7
Training loss: 2.4523502754447866
Validation loss: 2.6624019589827475

Epoch: 5| Step: 8
Training loss: 2.855118214686815
Validation loss: 2.6609200880788015

Epoch: 5| Step: 9
Training loss: 2.886023906246578
Validation loss: 2.6583221767653815

Epoch: 5| Step: 10
Training loss: 3.4738399517853606
Validation loss: 2.660067201562617

Epoch: 5| Step: 11
Training loss: 2.8526161233065066
Validation loss: 2.6591998382152298

Epoch: 72| Step: 0
Training loss: 3.0428261484758923
Validation loss: 2.6597046508463174

Epoch: 5| Step: 1
Training loss: 2.9244228413095374
Validation loss: 2.6598067313088087

Epoch: 5| Step: 2
Training loss: 2.4163802295052204
Validation loss: 2.65927671138049

Epoch: 5| Step: 3
Training loss: 2.8530314803073082
Validation loss: 2.6582089101040327

Epoch: 5| Step: 4
Training loss: 2.6915241542287207
Validation loss: 2.6562079893791726

Epoch: 5| Step: 5
Training loss: 3.247911075361299
Validation loss: 2.656272353283941

Epoch: 5| Step: 6
Training loss: 2.722547137620383
Validation loss: 2.6533897183967112

Epoch: 5| Step: 7
Training loss: 3.0676142748848885
Validation loss: 2.6526373811643804

Epoch: 5| Step: 8
Training loss: 3.043558829881911
Validation loss: 2.65272576526317

Epoch: 5| Step: 9
Training loss: 2.3845030743960467
Validation loss: 2.651530325070021

Epoch: 5| Step: 10
Training loss: 2.3896417528065985
Validation loss: 2.6500087229567137

Epoch: 5| Step: 11
Training loss: 1.5393377532355843
Validation loss: 2.64829685884265

Epoch: 73| Step: 0
Training loss: 2.327615157248179
Validation loss: 2.6470481243961377

Epoch: 5| Step: 1
Training loss: 2.5447792838866654
Validation loss: 2.6467033980648904

Epoch: 5| Step: 2
Training loss: 2.745506603693623
Validation loss: 2.6453468195849603

Epoch: 5| Step: 3
Training loss: 2.8719188931735666
Validation loss: 2.645169194788631

Epoch: 5| Step: 4
Training loss: 2.7412297241741346
Validation loss: 2.6430104182385974

Epoch: 5| Step: 5
Training loss: 3.315646566452083
Validation loss: 2.6426968995416953

Epoch: 5| Step: 6
Training loss: 2.662164543403045
Validation loss: 2.6412967075852367

Epoch: 5| Step: 7
Training loss: 2.4548559217264763
Validation loss: 2.6404566701774534

Epoch: 5| Step: 8
Training loss: 3.0211003854154406
Validation loss: 2.6367281614535933

Epoch: 5| Step: 9
Training loss: 2.367442504442467
Validation loss: 2.638126707572779

Epoch: 5| Step: 10
Training loss: 3.22405021343458
Validation loss: 2.6361419310530643

Epoch: 5| Step: 11
Training loss: 3.4900488395880593
Validation loss: 2.6322307792981348

Epoch: 74| Step: 0
Training loss: 2.8018064870292663
Validation loss: 2.6332762297543204

Epoch: 5| Step: 1
Training loss: 2.6687898726689685
Validation loss: 2.63321285807183

Epoch: 5| Step: 2
Training loss: 3.0287309384818726
Validation loss: 2.633921486827322

Epoch: 5| Step: 3
Training loss: 2.7573008292266556
Validation loss: 2.6356393747628974

Epoch: 5| Step: 4
Training loss: 3.0057032732370716
Validation loss: 2.634295701371585

Epoch: 5| Step: 5
Training loss: 2.5694006140009136
Validation loss: 2.6338470682676247

Epoch: 5| Step: 6
Training loss: 2.4913548719934635
Validation loss: 2.634095657489323

Epoch: 5| Step: 7
Training loss: 2.8127938011148865
Validation loss: 2.634785811618157

Epoch: 5| Step: 8
Training loss: 2.4204574866608723
Validation loss: 2.6320646452037755

Epoch: 5| Step: 9
Training loss: 2.7507836352410844
Validation loss: 2.631862049999459

Epoch: 5| Step: 10
Training loss: 3.124748219837549
Validation loss: 2.631461252353807

Epoch: 5| Step: 11
Training loss: 2.7720289495093167
Validation loss: 2.6308169702531154

Epoch: 75| Step: 0
Training loss: 2.2871824492289177
Validation loss: 2.628098955754897

Epoch: 5| Step: 1
Training loss: 2.50968221681685
Validation loss: 2.6272798583185017

Epoch: 5| Step: 2
Training loss: 2.9483934068642577
Validation loss: 2.626249492814071

Epoch: 5| Step: 3
Training loss: 2.6546229147506133
Validation loss: 2.6244033862089347

Epoch: 5| Step: 4
Training loss: 3.167784510286847
Validation loss: 2.62358735690098

Epoch: 5| Step: 5
Training loss: 3.2317169619406223
Validation loss: 2.6214137443403067

Epoch: 5| Step: 6
Training loss: 2.9163769896025786
Validation loss: 2.621526270284212

Epoch: 5| Step: 7
Training loss: 2.287407286385841
Validation loss: 2.6182653115148167

Epoch: 5| Step: 8
Training loss: 2.901702440345517
Validation loss: 2.616506205081173

Epoch: 5| Step: 9
Training loss: 2.6495177279986266
Validation loss: 2.6181645937023506

Epoch: 5| Step: 10
Training loss: 2.6493934548993523
Validation loss: 2.616152910930141

Epoch: 5| Step: 11
Training loss: 2.685344363813402
Validation loss: 2.611525907328205

Epoch: 76| Step: 0
Training loss: 3.17710753051215
Validation loss: 2.607580820637242

Epoch: 5| Step: 1
Training loss: 2.820569055781368
Validation loss: 2.6067913840922743

Epoch: 5| Step: 2
Training loss: 2.7588299395478257
Validation loss: 2.6117966887050175

Epoch: 5| Step: 3
Training loss: 2.8779288590620626
Validation loss: 2.6620764428292016

Epoch: 5| Step: 4
Training loss: 2.8508952156969554
Validation loss: 2.6603069589489126

Epoch: 5| Step: 5
Training loss: 2.586167616271831
Validation loss: 2.617671120972776

Epoch: 5| Step: 6
Training loss: 2.410150710336016
Validation loss: 2.604796441539676

Epoch: 5| Step: 7
Training loss: 2.7660644526940295
Validation loss: 2.6063450869893554

Epoch: 5| Step: 8
Training loss: 2.734020101131503
Validation loss: 2.6135263332820773

Epoch: 5| Step: 9
Training loss: 2.631501638381161
Validation loss: 2.620164781446935

Epoch: 5| Step: 10
Training loss: 2.735231973688911
Validation loss: 2.6347369509890615

Epoch: 5| Step: 11
Training loss: 3.4037679325882393
Validation loss: 2.6330667442772846

Epoch: 77| Step: 0
Training loss: 2.6687904980197596
Validation loss: 2.6319238387357795

Epoch: 5| Step: 1
Training loss: 3.0812739806567135
Validation loss: 2.6340183398241415

Epoch: 5| Step: 2
Training loss: 2.9907965950681583
Validation loss: 2.633964874996466

Epoch: 5| Step: 3
Training loss: 2.4670096914953796
Validation loss: 2.62042400598269

Epoch: 5| Step: 4
Training loss: 2.365141938448424
Validation loss: 2.613308445019672

Epoch: 5| Step: 5
Training loss: 2.603107257246753
Validation loss: 2.607598234729732

Epoch: 5| Step: 6
Training loss: 2.2791058640909787
Validation loss: 2.605019297574077

Epoch: 5| Step: 7
Training loss: 2.4894429463876233
Validation loss: 2.6102053868573507

Epoch: 5| Step: 8
Training loss: 3.25585146641586
Validation loss: 2.6110144963945014

Epoch: 5| Step: 9
Training loss: 3.178729988662188
Validation loss: 2.599729023622541

Epoch: 5| Step: 10
Training loss: 2.689585209458512
Validation loss: 2.5901232108827386

Epoch: 5| Step: 11
Training loss: 3.025153094744639
Validation loss: 2.5867385545512223

Epoch: 78| Step: 0
Training loss: 2.866193469122663
Validation loss: 2.586292004367845

Epoch: 5| Step: 1
Training loss: 2.876316101030851
Validation loss: 2.583672517895199

Epoch: 5| Step: 2
Training loss: 2.5566313927748223
Validation loss: 2.581941180791144

Epoch: 5| Step: 3
Training loss: 2.532935535000643
Validation loss: 2.5827862226869143

Epoch: 5| Step: 4
Training loss: 2.9422833071043883
Validation loss: 2.5860015500099176

Epoch: 5| Step: 5
Training loss: 2.6845934031837073
Validation loss: 2.5855527631937822

Epoch: 5| Step: 6
Training loss: 2.485464181203367
Validation loss: 2.5866314628157907

Epoch: 5| Step: 7
Training loss: 3.0870033451577497
Validation loss: 2.5777741704039525

Epoch: 5| Step: 8
Training loss: 2.7980531736369842
Validation loss: 2.5764532930893385

Epoch: 5| Step: 9
Training loss: 2.739551722896565
Validation loss: 2.574406753489918

Epoch: 5| Step: 10
Training loss: 2.1900182125780003
Validation loss: 2.575944332767958

Epoch: 5| Step: 11
Training loss: 3.2002029116193724
Validation loss: 2.5749920429798134

Epoch: 79| Step: 0
Training loss: 2.2767885031803266
Validation loss: 2.5724285888028167

Epoch: 5| Step: 1
Training loss: 3.5454359454220605
Validation loss: 2.5746383325044793

Epoch: 5| Step: 2
Training loss: 2.28688648939309
Validation loss: 2.573788168384689

Epoch: 5| Step: 3
Training loss: 2.7850824589738257
Validation loss: 2.5774159043915597

Epoch: 5| Step: 4
Training loss: 2.5528014334033564
Validation loss: 2.5737127251478715

Epoch: 5| Step: 5
Training loss: 2.417857326226631
Validation loss: 2.5792923837095274

Epoch: 5| Step: 6
Training loss: 2.281303196116975
Validation loss: 2.56521053319588

Epoch: 5| Step: 7
Training loss: 2.693160903843604
Validation loss: 2.5682392706305133

Epoch: 5| Step: 8
Training loss: 2.7688323135712407
Validation loss: 2.5666143254623965

Epoch: 5| Step: 9
Training loss: 3.0043492579602247
Validation loss: 2.566877793065265

Epoch: 5| Step: 10
Training loss: 3.0016282749068406
Validation loss: 2.565935692694198

Epoch: 5| Step: 11
Training loss: 2.307332218637595
Validation loss: 2.5646182547225833

Epoch: 80| Step: 0
Training loss: 2.95819818608874
Validation loss: 2.564540143944396

Epoch: 5| Step: 1
Training loss: 2.6198718524489695
Validation loss: 2.5644713626551052

Epoch: 5| Step: 2
Training loss: 2.321584641796081
Validation loss: 2.563297337639778

Epoch: 5| Step: 3
Training loss: 3.0661067354718203
Validation loss: 2.5608842144012085

Epoch: 5| Step: 4
Training loss: 2.0106953508462677
Validation loss: 2.560924115241999

Epoch: 5| Step: 5
Training loss: 2.684684343022002
Validation loss: 2.5627385392851147

Epoch: 5| Step: 6
Training loss: 2.8330342378011797
Validation loss: 2.560461691286431

Epoch: 5| Step: 7
Training loss: 2.7057680647951123
Validation loss: 2.562676850085645

Epoch: 5| Step: 8
Training loss: 2.54437413912079
Validation loss: 2.563211824825049

Epoch: 5| Step: 9
Training loss: 3.1290685490151913
Validation loss: 2.558853644840667

Epoch: 5| Step: 10
Training loss: 2.5822072394755806
Validation loss: 2.5590309254905725

Epoch: 5| Step: 11
Training loss: 2.9608537566488686
Validation loss: 2.569676514761768

Epoch: 81| Step: 0
Training loss: 2.890882655852227
Validation loss: 2.5760511553679954

Epoch: 5| Step: 1
Training loss: 2.4457459476939998
Validation loss: 2.564183658989446

Epoch: 5| Step: 2
Training loss: 2.312023526154512
Validation loss: 2.555105760342563

Epoch: 5| Step: 3
Training loss: 2.3489141918244267
Validation loss: 2.549417922960189

Epoch: 5| Step: 4
Training loss: 2.9145936047307357
Validation loss: 2.5552873752922114

Epoch: 5| Step: 5
Training loss: 2.737411556818336
Validation loss: 2.549857072938571

Epoch: 5| Step: 6
Training loss: 2.5062787365871237
Validation loss: 2.5511692252510656

Epoch: 5| Step: 7
Training loss: 3.0013315106960725
Validation loss: 2.5486166640997

Epoch: 5| Step: 8
Training loss: 2.4531045900698523
Validation loss: 2.5462472138025207

Epoch: 5| Step: 9
Training loss: 3.0849575369955935
Validation loss: 2.5466927539236583

Epoch: 5| Step: 10
Training loss: 2.8912136818748375
Validation loss: 2.5486791730403096

Epoch: 5| Step: 11
Training loss: 1.9309927043688078
Validation loss: 2.5460242272173486

Epoch: 82| Step: 0
Training loss: 2.9723970197416008
Validation loss: 2.5483143879435897

Epoch: 5| Step: 1
Training loss: 1.8218835428080158
Validation loss: 2.5454414444295717

Epoch: 5| Step: 2
Training loss: 2.4412960912647628
Validation loss: 2.5450618156003197

Epoch: 5| Step: 3
Training loss: 2.7922006257724763
Validation loss: 2.5444555823967527

Epoch: 5| Step: 4
Training loss: 2.9027580694556754
Validation loss: 2.5463654334061636

Epoch: 5| Step: 5
Training loss: 2.4036153265827696
Validation loss: 2.544736209868372

Epoch: 5| Step: 6
Training loss: 2.8236947952014493
Validation loss: 2.5440812770755183

Epoch: 5| Step: 7
Training loss: 2.8877707746628833
Validation loss: 2.5452269505220646

Epoch: 5| Step: 8
Training loss: 2.848117869445433
Validation loss: 2.540499897528215

Epoch: 5| Step: 9
Training loss: 2.510898105998341
Validation loss: 2.5418706496781223

Epoch: 5| Step: 10
Training loss: 2.65526104636912
Validation loss: 2.5403717695271757

Epoch: 5| Step: 11
Training loss: 3.6045566120038433
Validation loss: 2.543721792172456

Epoch: 83| Step: 0
Training loss: 2.783648281971152
Validation loss: 2.545074692512547

Epoch: 5| Step: 1
Training loss: 2.4384425370573655
Validation loss: 2.544117571855501

Epoch: 5| Step: 2
Training loss: 2.992623797846059
Validation loss: 2.545237277929219

Epoch: 5| Step: 3
Training loss: 2.65179167944534
Validation loss: 2.5479215956167445

Epoch: 5| Step: 4
Training loss: 2.422765063733302
Validation loss: 2.550135275113673

Epoch: 5| Step: 5
Training loss: 2.5236536647932137
Validation loss: 2.552832747735686

Epoch: 5| Step: 6
Training loss: 2.8782996440506996
Validation loss: 2.564110652913882

Epoch: 5| Step: 7
Training loss: 2.94153026304186
Validation loss: 2.574567150876533

Epoch: 5| Step: 8
Training loss: 2.1318604297078525
Validation loss: 2.568037376960694

Epoch: 5| Step: 9
Training loss: 2.8986080972895185
Validation loss: 2.5658212437230796

Epoch: 5| Step: 10
Training loss: 2.7766772622695877
Validation loss: 2.560880284798096

Epoch: 5| Step: 11
Training loss: 2.926272749521111
Validation loss: 2.5585757810626575

Epoch: 84| Step: 0
Training loss: 2.655405965109398
Validation loss: 2.558533510076524

Epoch: 5| Step: 1
Training loss: 2.810786170130264
Validation loss: 2.5559259770479503

Epoch: 5| Step: 2
Training loss: 2.869348361374216
Validation loss: 2.5571108027024896

Epoch: 5| Step: 3
Training loss: 2.1379986543856346
Validation loss: 2.5565881298748048

Epoch: 5| Step: 4
Training loss: 2.821500322005588
Validation loss: 2.554132025910576

Epoch: 5| Step: 5
Training loss: 2.7629293461980744
Validation loss: 2.556964046575347

Epoch: 5| Step: 6
Training loss: 2.524088961476837
Validation loss: 2.5538682975450193

Epoch: 5| Step: 7
Training loss: 2.7416365632450037
Validation loss: 2.5470405321993645

Epoch: 5| Step: 8
Training loss: 2.715072248983387
Validation loss: 2.5477861362145857

Epoch: 5| Step: 9
Training loss: 2.539877704920244
Validation loss: 2.5415991255640216

Epoch: 5| Step: 10
Training loss: 2.8962724933565775
Validation loss: 2.5399411016491404

Epoch: 5| Step: 11
Training loss: 2.8212651469272356
Validation loss: 2.5375500443884005

Epoch: 85| Step: 0
Training loss: 2.300376869506441
Validation loss: 2.538133112689275

Epoch: 5| Step: 1
Training loss: 2.6220153007206375
Validation loss: 2.5364316446726636

Epoch: 5| Step: 2
Training loss: 2.933579608664734
Validation loss: 2.5355593650321007

Epoch: 5| Step: 3
Training loss: 2.4474594879694056
Validation loss: 2.5365825690537314

Epoch: 5| Step: 4
Training loss: 2.2772631361026874
Validation loss: 2.532866989732534

Epoch: 5| Step: 5
Training loss: 3.356705709594631
Validation loss: 2.5351898159788964

Epoch: 5| Step: 6
Training loss: 2.7810247951583245
Validation loss: 2.5314319529501064

Epoch: 5| Step: 7
Training loss: 2.7947464840583036
Validation loss: 2.5328247779926696

Epoch: 5| Step: 8
Training loss: 2.6563516933945786
Validation loss: 2.534026683385372

Epoch: 5| Step: 9
Training loss: 2.8421012374596595
Validation loss: 2.530288639314941

Epoch: 5| Step: 10
Training loss: 2.401945549442233
Validation loss: 2.530923539761515

Epoch: 5| Step: 11
Training loss: 1.6235887561640898
Validation loss: 2.535957907177195

Epoch: 86| Step: 0
Training loss: 2.5524211011269333
Validation loss: 2.53426497848152

Epoch: 5| Step: 1
Training loss: 2.7135389526900315
Validation loss: 2.532087222913356

Epoch: 5| Step: 2
Training loss: 3.237934015705613
Validation loss: 2.526664685896078

Epoch: 5| Step: 3
Training loss: 2.5400439434305278
Validation loss: 2.5255479166208867

Epoch: 5| Step: 4
Training loss: 2.5601709527812004
Validation loss: 2.5224413721475534

Epoch: 5| Step: 5
Training loss: 2.4136092368457773
Validation loss: 2.523008371088779

Epoch: 5| Step: 6
Training loss: 2.7304762593222027
Validation loss: 2.5265633755479695

Epoch: 5| Step: 7
Training loss: 2.9797844057373166
Validation loss: 2.5251789953347

Epoch: 5| Step: 8
Training loss: 2.5231029182395734
Validation loss: 2.5311748705416797

Epoch: 5| Step: 9
Training loss: 2.831877858184268
Validation loss: 2.529028005559172

Epoch: 5| Step: 10
Training loss: 2.339684992070852
Validation loss: 2.5278760731685734

Epoch: 5| Step: 11
Training loss: 1.2526707727352435
Validation loss: 2.5291159546705018

Epoch: 87| Step: 0
Training loss: 2.876611672498648
Validation loss: 2.528541296943469

Epoch: 5| Step: 1
Training loss: 2.401651422247949
Validation loss: 2.5285616479931576

Epoch: 5| Step: 2
Training loss: 2.8370945779487284
Validation loss: 2.5288585857510872

Epoch: 5| Step: 3
Training loss: 2.858772255994784
Validation loss: 2.5242903875262255

Epoch: 5| Step: 4
Training loss: 2.4630219850290764
Validation loss: 2.52246604541828

Epoch: 5| Step: 5
Training loss: 2.9782340094839057
Validation loss: 2.5246852943022646

Epoch: 5| Step: 6
Training loss: 2.1408703517225236
Validation loss: 2.5213118458924453

Epoch: 5| Step: 7
Training loss: 2.5140196612248573
Validation loss: 2.5200076143273358

Epoch: 5| Step: 8
Training loss: 2.6924639258274796
Validation loss: 2.527690598331761

Epoch: 5| Step: 9
Training loss: 2.8138686346856745
Validation loss: 2.5284104805716208

Epoch: 5| Step: 10
Training loss: 2.7139683910825876
Validation loss: 2.5284611090704834

Epoch: 5| Step: 11
Training loss: 2.1001080439793376
Validation loss: 2.5214349577868025

Epoch: 88| Step: 0
Training loss: 2.7554683069889068
Validation loss: 2.528221952593537

Epoch: 5| Step: 1
Training loss: 2.5960775755413703
Validation loss: 2.522945647436801

Epoch: 5| Step: 2
Training loss: 2.9357720020336737
Validation loss: 2.519860573865002

Epoch: 5| Step: 3
Training loss: 2.975727114256364
Validation loss: 2.5187190321350865

Epoch: 5| Step: 4
Training loss: 2.751525629111728
Validation loss: 2.518325339646931

Epoch: 5| Step: 5
Training loss: 2.6321667521679286
Validation loss: 2.5186851362169986

Epoch: 5| Step: 6
Training loss: 2.5856157085352263
Validation loss: 2.515537197247211

Epoch: 5| Step: 7
Training loss: 2.4806266674325443
Validation loss: 2.520438925309057

Epoch: 5| Step: 8
Training loss: 2.549627957598514
Validation loss: 2.514892001408575

Epoch: 5| Step: 9
Training loss: 2.626932794995682
Validation loss: 2.514989831953179

Epoch: 5| Step: 10
Training loss: 2.248061828828685
Validation loss: 2.5125629791982753

Epoch: 5| Step: 11
Training loss: 2.7154272536435813
Validation loss: 2.5148411550406746

Epoch: 89| Step: 0
Training loss: 2.914898054587868
Validation loss: 2.5149891723098903

Epoch: 5| Step: 1
Training loss: 2.248170956949752
Validation loss: 2.5128543712978186

Epoch: 5| Step: 2
Training loss: 2.6766795189035473
Validation loss: 2.5124183103778357

Epoch: 5| Step: 3
Training loss: 2.669856150891856
Validation loss: 2.513982979334983

Epoch: 5| Step: 4
Training loss: 2.7544265147412923
Validation loss: 2.514120335108288

Epoch: 5| Step: 5
Training loss: 2.6461307528678915
Validation loss: 2.513891697074722

Epoch: 5| Step: 6
Training loss: 2.790650842425224
Validation loss: 2.5131495757388813

Epoch: 5| Step: 7
Training loss: 2.874658481382183
Validation loss: 2.511951805523082

Epoch: 5| Step: 8
Training loss: 2.352643648437996
Validation loss: 2.512006404021679

Epoch: 5| Step: 9
Training loss: 2.550443244259221
Validation loss: 2.5124745534522375

Epoch: 5| Step: 10
Training loss: 2.756490244526573
Validation loss: 2.51527830463612

Epoch: 5| Step: 11
Training loss: 1.602862542215923
Validation loss: 2.513413545167978

Epoch: 90| Step: 0
Training loss: 2.0413760802371415
Validation loss: 2.5024636050844653

Epoch: 5| Step: 1
Training loss: 2.2301495485666805
Validation loss: 2.5131285425272027

Epoch: 5| Step: 2
Training loss: 2.6704342175416502
Validation loss: 2.5131754430669955

Epoch: 5| Step: 3
Training loss: 3.02914465713026
Validation loss: 2.5111757426140193

Epoch: 5| Step: 4
Training loss: 2.7447702791851354
Validation loss: 2.512262399234467

Epoch: 5| Step: 5
Training loss: 2.9909904615428045
Validation loss: 2.5159756572051934

Epoch: 5| Step: 6
Training loss: 2.8041398302657994
Validation loss: 2.5153956456801767

Epoch: 5| Step: 7
Training loss: 2.731145728453769
Validation loss: 2.513574376521175

Epoch: 5| Step: 8
Training loss: 2.5575427044149244
Validation loss: 2.514924846383274

Epoch: 5| Step: 9
Training loss: 2.423128751053759
Validation loss: 2.5103811773321576

Epoch: 5| Step: 10
Training loss: 2.650249570214818
Validation loss: 2.510149859790205

Epoch: 5| Step: 11
Training loss: 2.8002644039383338
Validation loss: 2.5084726449124877

Epoch: 91| Step: 0
Training loss: 2.771150145428703
Validation loss: 2.510234692944053

Epoch: 5| Step: 1
Training loss: 2.4227338683123
Validation loss: 2.5112391637426232

Epoch: 5| Step: 2
Training loss: 2.4366436945836476
Validation loss: 2.5130691853089893

Epoch: 5| Step: 3
Training loss: 2.474580856435906
Validation loss: 2.525539239423133

Epoch: 5| Step: 4
Training loss: 2.6389810534885214
Validation loss: 2.5294961858156233

Epoch: 5| Step: 5
Training loss: 2.887929123206888
Validation loss: 2.547437500416295

Epoch: 5| Step: 6
Training loss: 2.83797715666209
Validation loss: 2.5415695020728593

Epoch: 5| Step: 7
Training loss: 2.5365284643066865
Validation loss: 2.538495150048471

Epoch: 5| Step: 8
Training loss: 2.5671524983467706
Validation loss: 2.5321123436019257

Epoch: 5| Step: 9
Training loss: 3.0927695830513557
Validation loss: 2.515896512024101

Epoch: 5| Step: 10
Training loss: 2.5657556832748756
Validation loss: 2.5117737292911633

Epoch: 5| Step: 11
Training loss: 2.262881070918086
Validation loss: 2.504746556433593

Epoch: 92| Step: 0
Training loss: 2.446286236119874
Validation loss: 2.5028678835807816

Epoch: 5| Step: 1
Training loss: 2.260195837059876
Validation loss: 2.5051994217828453

Epoch: 5| Step: 2
Training loss: 2.7007328981414047
Validation loss: 2.5134288924320933

Epoch: 5| Step: 3
Training loss: 2.9371654238796094
Validation loss: 2.5210616281608784

Epoch: 5| Step: 4
Training loss: 2.771910599025163
Validation loss: 2.5240773471403317

Epoch: 5| Step: 5
Training loss: 2.6775411568122234
Validation loss: 2.525444649740254

Epoch: 5| Step: 6
Training loss: 2.3043454708465982
Validation loss: 2.5282692568267646

Epoch: 5| Step: 7
Training loss: 2.958182066869291
Validation loss: 2.527105364398154

Epoch: 5| Step: 8
Training loss: 2.5114537124454155
Validation loss: 2.525453512141908

Epoch: 5| Step: 9
Training loss: 2.4343117623691284
Validation loss: 2.515740508434798

Epoch: 5| Step: 10
Training loss: 3.01623513171835
Validation loss: 2.5150203610032738

Epoch: 5| Step: 11
Training loss: 2.948907171299878
Validation loss: 2.510423689287633

Epoch: 93| Step: 0
Training loss: 2.678460821865345
Validation loss: 2.5064643294155653

Epoch: 5| Step: 1
Training loss: 2.761754231312697
Validation loss: 2.501688299723019

Epoch: 5| Step: 2
Training loss: 2.5376208159149765
Validation loss: 2.5006822886853195

Epoch: 5| Step: 3
Training loss: 2.983718879834666
Validation loss: 2.5019761421203013

Epoch: 5| Step: 4
Training loss: 2.6472907270187336
Validation loss: 2.501759573492373

Epoch: 5| Step: 5
Training loss: 2.523336780533983
Validation loss: 2.5019317475159286

Epoch: 5| Step: 6
Training loss: 2.2403559098871564
Validation loss: 2.505773327272529

Epoch: 5| Step: 7
Training loss: 2.3875760850217063
Validation loss: 2.4988411800521946

Epoch: 5| Step: 8
Training loss: 2.7281754321913128
Validation loss: 2.513798155004839

Epoch: 5| Step: 9
Training loss: 2.49082924114294
Validation loss: 2.504166572198427

Epoch: 5| Step: 10
Training loss: 2.821894827907179
Validation loss: 2.5065752068935345

Epoch: 5| Step: 11
Training loss: 2.023806033946215
Validation loss: 2.4989845637579604

Epoch: 94| Step: 0
Training loss: 2.516342439881694
Validation loss: 2.5106866356347095

Epoch: 5| Step: 1
Training loss: 2.463449413247042
Validation loss: 2.4989817731255437

Epoch: 5| Step: 2
Training loss: 2.7580664844646243
Validation loss: 2.4947825865026148

Epoch: 5| Step: 3
Training loss: 3.0429859870941516
Validation loss: 2.498742494621156

Epoch: 5| Step: 4
Training loss: 1.7234509077614084
Validation loss: 2.4997364501318833

Epoch: 5| Step: 5
Training loss: 2.9388801092230215
Validation loss: 2.4957581574566112

Epoch: 5| Step: 6
Training loss: 2.7802686352917783
Validation loss: 2.500149034985154

Epoch: 5| Step: 7
Training loss: 2.7316660513963487
Validation loss: 2.5002128192598705

Epoch: 5| Step: 8
Training loss: 2.3907065782777295
Validation loss: 2.4953088536212977

Epoch: 5| Step: 9
Training loss: 2.8783013007128955
Validation loss: 2.4959537305646875

Epoch: 5| Step: 10
Training loss: 2.168860669569142
Validation loss: 2.493446112005642

Epoch: 5| Step: 11
Training loss: 3.402087406701855
Validation loss: 2.4952706903766098

Epoch: 95| Step: 0
Training loss: 2.6359169426135063
Validation loss: 2.500361241626524

Epoch: 5| Step: 1
Training loss: 2.18207946571103
Validation loss: 2.5052319100606204

Epoch: 5| Step: 2
Training loss: 2.513231548639647
Validation loss: 2.50913226377589

Epoch: 5| Step: 3
Training loss: 2.7178689691090425
Validation loss: 2.5062699451112698

Epoch: 5| Step: 4
Training loss: 2.608151360238477
Validation loss: 2.5090429707898423

Epoch: 5| Step: 5
Training loss: 2.6385885658920287
Validation loss: 2.5085339167122243

Epoch: 5| Step: 6
Training loss: 2.9414409417567393
Validation loss: 2.5088948482049958

Epoch: 5| Step: 7
Training loss: 2.4771753750173864
Validation loss: 2.5025765175669794

Epoch: 5| Step: 8
Training loss: 2.797860254404038
Validation loss: 2.512611973894311

Epoch: 5| Step: 9
Training loss: 2.6794221707768426
Validation loss: 2.5171113016698223

Epoch: 5| Step: 10
Training loss: 2.7270582562705297
Validation loss: 2.5121055688810907

Epoch: 5| Step: 11
Training loss: 2.0617149477306023
Validation loss: 2.5168664546373423

Epoch: 96| Step: 0
Training loss: 2.8579071112687733
Validation loss: 2.5141644670740035

Epoch: 5| Step: 1
Training loss: 2.234246843837538
Validation loss: 2.5144596243776265

Epoch: 5| Step: 2
Training loss: 2.6569586313492297
Validation loss: 2.513427852947423

Epoch: 5| Step: 3
Training loss: 2.869048385228068
Validation loss: 2.507888788823678

Epoch: 5| Step: 4
Training loss: 2.6948133808131627
Validation loss: 2.507017792051115

Epoch: 5| Step: 5
Training loss: 2.362927118676382
Validation loss: 2.503635786647543

Epoch: 5| Step: 6
Training loss: 2.307669982435368
Validation loss: 2.508802557196579

Epoch: 5| Step: 7
Training loss: 2.348004561242737
Validation loss: 2.5074022775575426

Epoch: 5| Step: 8
Training loss: 2.383122458387811
Validation loss: 2.5049295263662636

Epoch: 5| Step: 9
Training loss: 2.6104273701230416
Validation loss: 2.504032213173971

Epoch: 5| Step: 10
Training loss: 3.187771355541354
Validation loss: 2.5009462870362005

Epoch: 5| Step: 11
Training loss: 3.1132518427092934
Validation loss: 2.50312654175316

Epoch: 97| Step: 0
Training loss: 2.3013628239198956
Validation loss: 2.493953378171244

Epoch: 5| Step: 1
Training loss: 2.170047347787182
Validation loss: 2.4989208117381008

Epoch: 5| Step: 2
Training loss: 2.0401428400782047
Validation loss: 2.501829082224244

Epoch: 5| Step: 3
Training loss: 2.5577673589396817
Validation loss: 2.4997659216014276

Epoch: 5| Step: 4
Training loss: 2.8601365039501223
Validation loss: 2.495443300160372

Epoch: 5| Step: 5
Training loss: 2.7578597888728473
Validation loss: 2.498697784142874

Epoch: 5| Step: 6
Training loss: 2.68902260874319
Validation loss: 2.4928299325970507

Epoch: 5| Step: 7
Training loss: 2.99441485259612
Validation loss: 2.485544689096971

Epoch: 5| Step: 8
Training loss: 2.6865891088392893
Validation loss: 2.490558071665141

Epoch: 5| Step: 9
Training loss: 2.7549975674544047
Validation loss: 2.4851179111772894

Epoch: 5| Step: 10
Training loss: 2.71374261054707
Validation loss: 2.4919001175420066

Epoch: 5| Step: 11
Training loss: 2.38104522705702
Validation loss: 2.4879487642440856

Epoch: 98| Step: 0
Training loss: 2.4291777735391338
Validation loss: 2.4867607992813476

Epoch: 5| Step: 1
Training loss: 2.5349976852978253
Validation loss: 2.48887664881733

Epoch: 5| Step: 2
Training loss: 2.6073781670610985
Validation loss: 2.4870571157545927

Epoch: 5| Step: 3
Training loss: 2.9918550550673784
Validation loss: 2.4839335755186176

Epoch: 5| Step: 4
Training loss: 2.799468814328
Validation loss: 2.488636241045598

Epoch: 5| Step: 5
Training loss: 2.740457276939554
Validation loss: 2.4889636299275124

Epoch: 5| Step: 6
Training loss: 2.435997230722391
Validation loss: 2.4884232181629624

Epoch: 5| Step: 7
Training loss: 2.9980071124195424
Validation loss: 2.4830600169419688

Epoch: 5| Step: 8
Training loss: 1.9543933259783672
Validation loss: 2.4866604319469574

Epoch: 5| Step: 9
Training loss: 2.4095804525172677
Validation loss: 2.4850909882792322

Epoch: 5| Step: 10
Training loss: 2.4436452623639804
Validation loss: 2.4873360076597844

Epoch: 5| Step: 11
Training loss: 2.9307693315086603
Validation loss: 2.4845289426568606

Epoch: 99| Step: 0
Training loss: 2.654065299581176
Validation loss: 2.488444763592004

Epoch: 5| Step: 1
Training loss: 2.8204137725822744
Validation loss: 2.489062715040678

Epoch: 5| Step: 2
Training loss: 2.49897573469516
Validation loss: 2.482124312780121

Epoch: 5| Step: 3
Training loss: 2.1249284171221077
Validation loss: 2.48241296301882

Epoch: 5| Step: 4
Training loss: 2.660414897427211
Validation loss: 2.4768355907931046

Epoch: 5| Step: 5
Training loss: 3.0500337317430617
Validation loss: 2.4857919399021946

Epoch: 5| Step: 6
Training loss: 2.473295059970467
Validation loss: 2.485149478753227

Epoch: 5| Step: 7
Training loss: 2.2616828032751664
Validation loss: 2.4869407823939182

Epoch: 5| Step: 8
Training loss: 2.6056719790785126
Validation loss: 2.487910126737288

Epoch: 5| Step: 9
Training loss: 2.728811129253577
Validation loss: 2.486827994873321

Epoch: 5| Step: 10
Training loss: 2.546166444286728
Validation loss: 2.4894753529795186

Epoch: 5| Step: 11
Training loss: 2.8398869024511133
Validation loss: 2.488894759738428

Epoch: 100| Step: 0
Training loss: 2.5505817796839914
Validation loss: 2.49101305387158

Epoch: 5| Step: 1
Training loss: 2.5460368456455544
Validation loss: 2.484483244675085

Epoch: 5| Step: 2
Training loss: 2.6948399226219597
Validation loss: 2.4857049696996456

Epoch: 5| Step: 3
Training loss: 2.6090418437323293
Validation loss: 2.4804070011973534

Epoch: 5| Step: 4
Training loss: 2.2366920674791424
Validation loss: 2.4809068668356398

Epoch: 5| Step: 5
Training loss: 2.6865684314150693
Validation loss: 2.4822939167714186

Epoch: 5| Step: 6
Training loss: 2.8145291955476908
Validation loss: 2.479301610581218

Epoch: 5| Step: 7
Training loss: 2.2087422718027314
Validation loss: 2.479814249832307

Epoch: 5| Step: 8
Training loss: 2.5958374080554543
Validation loss: 2.4839746404541967

Epoch: 5| Step: 9
Training loss: 2.6427546834784192
Validation loss: 2.484714812704882

Epoch: 5| Step: 10
Training loss: 2.8216541089767837
Validation loss: 2.4863087307996907

Epoch: 5| Step: 11
Training loss: 2.8190478926613083
Validation loss: 2.4820574621269937

Epoch: 101| Step: 0
Training loss: 2.715217048722548
Validation loss: 2.4911937227031937

Epoch: 5| Step: 1
Training loss: 2.96702172015616
Validation loss: 2.488073622795876

Epoch: 5| Step: 2
Training loss: 2.5617348994158733
Validation loss: 2.484111384035042

Epoch: 5| Step: 3
Training loss: 2.6173593037710012
Validation loss: 2.49079660090136

Epoch: 5| Step: 4
Training loss: 2.617344729127462
Validation loss: 2.4843892550909183

Epoch: 5| Step: 5
Training loss: 2.6663126909242454
Validation loss: 2.4889323841320796

Epoch: 5| Step: 6
Training loss: 2.9430965921388275
Validation loss: 2.4870256522795633

Epoch: 5| Step: 7
Training loss: 2.8326305939078904
Validation loss: 2.4934506020682012

Epoch: 5| Step: 8
Training loss: 2.1488855622976004
Validation loss: 2.4902216094444043

Epoch: 5| Step: 9
Training loss: 2.1833026398498427
Validation loss: 2.4879434177577044

Epoch: 5| Step: 10
Training loss: 2.2610985098177068
Validation loss: 2.495911692555196

Epoch: 5| Step: 11
Training loss: 1.6686477172594536
Validation loss: 2.4906577255319893

Epoch: 102| Step: 0
Training loss: 2.530562507609609
Validation loss: 2.4863413580387244

Epoch: 5| Step: 1
Training loss: 2.747983019425125
Validation loss: 2.4853231032465226

Epoch: 5| Step: 2
Training loss: 2.8202590725981658
Validation loss: 2.481372694574053

Epoch: 5| Step: 3
Training loss: 2.1065247670602596
Validation loss: 2.479724053360036

Epoch: 5| Step: 4
Training loss: 2.8454447297107825
Validation loss: 2.487165726992433

Epoch: 5| Step: 5
Training loss: 2.907999312084594
Validation loss: 2.4932564462912272

Epoch: 5| Step: 6
Training loss: 2.576007782841411
Validation loss: 2.4869059420419375

Epoch: 5| Step: 7
Training loss: 2.414923751922635
Validation loss: 2.4862545708751265

Epoch: 5| Step: 8
Training loss: 2.7011243880835814
Validation loss: 2.488419473542778

Epoch: 5| Step: 9
Training loss: 2.342865433661648
Validation loss: 2.4863990839491326

Epoch: 5| Step: 10
Training loss: 2.289273301140441
Validation loss: 2.494242109217083

Epoch: 5| Step: 11
Training loss: 3.0799044195430008
Validation loss: 2.485811861652872

Epoch: 103| Step: 0
Training loss: 2.0635783526493956
Validation loss: 2.4832123048996815

Epoch: 5| Step: 1
Training loss: 2.562812413965807
Validation loss: 2.4885222529381523

Epoch: 5| Step: 2
Training loss: 2.3662492258561323
Validation loss: 2.482190509281882

Epoch: 5| Step: 3
Training loss: 2.75029449619971
Validation loss: 2.4847307850134754

Epoch: 5| Step: 4
Training loss: 2.8281239040646615
Validation loss: 2.4858800220058654

Epoch: 5| Step: 5
Training loss: 2.2770601230100382
Validation loss: 2.4911305927882057

Epoch: 5| Step: 6
Training loss: 2.761578805876833
Validation loss: 2.4839707451534565

Epoch: 5| Step: 7
Training loss: 2.2660568351924155
Validation loss: 2.4839383587306982

Epoch: 5| Step: 8
Training loss: 3.0017437635394932
Validation loss: 2.4801596020863528

Epoch: 5| Step: 9
Training loss: 2.4782264486667844
Validation loss: 2.4839400344524845

Epoch: 5| Step: 10
Training loss: 2.828777259091846
Validation loss: 2.4854060378783998

Epoch: 5| Step: 11
Training loss: 2.725091838164032
Validation loss: 2.4847048734392487

Epoch: 104| Step: 0
Training loss: 2.6633058071822697
Validation loss: 2.487177465765051

Epoch: 5| Step: 1
Training loss: 2.378854334473028
Validation loss: 2.4868306673196403

Epoch: 5| Step: 2
Training loss: 2.693906555582877
Validation loss: 2.4802421846820653

Epoch: 5| Step: 3
Training loss: 2.7762359684740283
Validation loss: 2.4781619341400143

Epoch: 5| Step: 4
Training loss: 2.5365817583722268
Validation loss: 2.478878837327281

Epoch: 5| Step: 5
Training loss: 2.1562693498268453
Validation loss: 2.476352795086881

Epoch: 5| Step: 6
Training loss: 2.4252920348517177
Validation loss: 2.47753235030778

Epoch: 5| Step: 7
Training loss: 2.8365109707172844
Validation loss: 2.47488378515238

Epoch: 5| Step: 8
Training loss: 2.5915520788866635
Validation loss: 2.478777912391202

Epoch: 5| Step: 9
Training loss: 2.497945322652387
Validation loss: 2.4785630602614854

Epoch: 5| Step: 10
Training loss: 2.841549535520608
Validation loss: 2.479873926325511

Epoch: 5| Step: 11
Training loss: 1.6517112008155839
Validation loss: 2.4813723943135937

Epoch: 105| Step: 0
Training loss: 2.1062662457934764
Validation loss: 2.485483310207806

Epoch: 5| Step: 1
Training loss: 2.1576596503569707
Validation loss: 2.4865266571331706

Epoch: 5| Step: 2
Training loss: 2.709997624329374
Validation loss: 2.483770758844948

Epoch: 5| Step: 3
Training loss: 2.872879075285519
Validation loss: 2.4852346936161203

Epoch: 5| Step: 4
Training loss: 2.700326076880026
Validation loss: 2.4858015950775267

Epoch: 5| Step: 5
Training loss: 2.6189521875375865
Validation loss: 2.483067550346658

Epoch: 5| Step: 6
Training loss: 2.45565461375613
Validation loss: 2.482562253490955

Epoch: 5| Step: 7
Training loss: 2.7639490426661855
Validation loss: 2.485586434828419

Epoch: 5| Step: 8
Training loss: 2.6152473204379936
Validation loss: 2.4828117258572036

Epoch: 5| Step: 9
Training loss: 2.710679715347344
Validation loss: 2.4818396757223837

Epoch: 5| Step: 10
Training loss: 2.649985612074462
Validation loss: 2.4839256048111342

Epoch: 5| Step: 11
Training loss: 2.689447983326149
Validation loss: 2.4807222692337807

Epoch: 106| Step: 0
Training loss: 2.750520656888831
Validation loss: 2.48506154664703

Epoch: 5| Step: 1
Training loss: 2.462059322370718
Validation loss: 2.4824934218025962

Epoch: 5| Step: 2
Training loss: 2.4029154275655378
Validation loss: 2.472367794653888

Epoch: 5| Step: 3
Training loss: 2.484624310345935
Validation loss: 2.48336337543937

Epoch: 5| Step: 4
Training loss: 3.0573422342740084
Validation loss: 2.5111676566214696

Epoch: 5| Step: 5
Training loss: 2.468231629623104
Validation loss: 2.5108494299853645

Epoch: 5| Step: 6
Training loss: 2.637581781502475
Validation loss: 2.509408559768765

Epoch: 5| Step: 7
Training loss: 2.487134540491057
Validation loss: 2.49050603045571

Epoch: 5| Step: 8
Training loss: 2.7186283654375365
Validation loss: 2.4779547659732764

Epoch: 5| Step: 9
Training loss: 3.156569965120708
Validation loss: 2.4808189405212357

Epoch: 5| Step: 10
Training loss: 1.9566597652584368
Validation loss: 2.48287277468608

Epoch: 5| Step: 11
Training loss: 2.7669214345836948
Validation loss: 2.480984723750478

Epoch: 107| Step: 0
Training loss: 2.982371671462691
Validation loss: 2.483128116809583

Epoch: 5| Step: 1
Training loss: 2.4690124758485923
Validation loss: 2.4845048722523706

Epoch: 5| Step: 2
Training loss: 2.2338971247196158
Validation loss: 2.482362840032041

Epoch: 5| Step: 3
Training loss: 2.8023690759871935
Validation loss: 2.488063449400149

Epoch: 5| Step: 4
Training loss: 3.082367273150913
Validation loss: 2.4897631828967004

Epoch: 5| Step: 5
Training loss: 2.355081959814789
Validation loss: 2.4953224331620842

Epoch: 5| Step: 6
Training loss: 2.3912410565621314
Validation loss: 2.4929487247712765

Epoch: 5| Step: 7
Training loss: 2.4768187573824183
Validation loss: 2.4999985098834365

Epoch: 5| Step: 8
Training loss: 2.7126021757087173
Validation loss: 2.492271266474198

Epoch: 5| Step: 9
Training loss: 2.9796371799774883
Validation loss: 2.4939658059649346

Epoch: 5| Step: 10
Training loss: 2.260190984707601
Validation loss: 2.4875646304835017

Epoch: 5| Step: 11
Training loss: 1.808759183300145
Validation loss: 2.4893286081240977

Epoch: 108| Step: 0
Training loss: 2.4986868270513964
Validation loss: 2.487033309480313

Epoch: 5| Step: 1
Training loss: 2.4697010767516585
Validation loss: 2.484486027601851

Epoch: 5| Step: 2
Training loss: 2.687826713397585
Validation loss: 2.4850191404896265

Epoch: 5| Step: 3
Training loss: 2.3726621465072744
Validation loss: 2.487362530837882

Epoch: 5| Step: 4
Training loss: 2.444075689052186
Validation loss: 2.4841255686560673

Epoch: 5| Step: 5
Training loss: 2.808895514011953
Validation loss: 2.4803826705133982

Epoch: 5| Step: 6
Training loss: 2.591179182646308
Validation loss: 2.4783828094544598

Epoch: 5| Step: 7
Training loss: 2.6919083922728344
Validation loss: 2.468333812834208

Epoch: 5| Step: 8
Training loss: 2.6581883258393604
Validation loss: 2.478553264663784

Epoch: 5| Step: 9
Training loss: 2.3294086373830054
Validation loss: 2.4721632833158287

Epoch: 5| Step: 10
Training loss: 2.632705550469405
Validation loss: 2.483366195623421

Epoch: 5| Step: 11
Training loss: 3.2492648540361873
Validation loss: 2.4907073724862707

Epoch: 109| Step: 0
Training loss: 2.3942050431607336
Validation loss: 2.4860604604845418

Epoch: 5| Step: 1
Training loss: 2.8192347953368113
Validation loss: 2.4817329849500678

Epoch: 5| Step: 2
Training loss: 2.4851901078758414
Validation loss: 2.469567594066667

Epoch: 5| Step: 3
Training loss: 2.3635388951112297
Validation loss: 2.4669089474342445

Epoch: 5| Step: 4
Training loss: 2.4713701275121505
Validation loss: 2.473751398098682

Epoch: 5| Step: 5
Training loss: 2.8026549218376635
Validation loss: 2.4722068301863502

Epoch: 5| Step: 6
Training loss: 2.3052544365254697
Validation loss: 2.4728138628841445

Epoch: 5| Step: 7
Training loss: 1.7035724157197416
Validation loss: 2.4730529145583944

Epoch: 5| Step: 8
Training loss: 2.6063792683769185
Validation loss: 2.475116793127752

Epoch: 5| Step: 9
Training loss: 3.1607991284920764
Validation loss: 2.4717305536259793

Epoch: 5| Step: 10
Training loss: 2.6546578516211055
Validation loss: 2.4746086805062144

Epoch: 5| Step: 11
Training loss: 3.8681570359772715
Validation loss: 2.4738205131824462

Epoch: 110| Step: 0
Training loss: 2.2655468105109757
Validation loss: 2.4683830938232454

Epoch: 5| Step: 1
Training loss: 2.619536208532904
Validation loss: 2.47523178386365

Epoch: 5| Step: 2
Training loss: 2.2979166173257646
Validation loss: 2.4779601500472763

Epoch: 5| Step: 3
Training loss: 2.303258420477708
Validation loss: 2.468917460737529

Epoch: 5| Step: 4
Training loss: 2.869243165554048
Validation loss: 2.4694055097428738

Epoch: 5| Step: 5
Training loss: 2.513905002362038
Validation loss: 2.467558311603071

Epoch: 5| Step: 6
Training loss: 2.392325002554352
Validation loss: 2.472078071962415

Epoch: 5| Step: 7
Training loss: 2.3846657471982518
Validation loss: 2.4782662412745617

Epoch: 5| Step: 8
Training loss: 2.9343288430188204
Validation loss: 2.4749016793626994

Epoch: 5| Step: 9
Training loss: 2.642999788801724
Validation loss: 2.474700183401559

Epoch: 5| Step: 10
Training loss: 2.996897364623439
Validation loss: 2.475531378451429

Epoch: 5| Step: 11
Training loss: 2.349270232498845
Validation loss: 2.4760772077175908

Epoch: 111| Step: 0
Training loss: 2.8783804880537924
Validation loss: 2.4798327013468695

Epoch: 5| Step: 1
Training loss: 2.5883270932048865
Validation loss: 2.4778842267079524

Epoch: 5| Step: 2
Training loss: 2.5640698718163186
Validation loss: 2.4821574072494683

Epoch: 5| Step: 3
Training loss: 2.3182821413591097
Validation loss: 2.479776148582612

Epoch: 5| Step: 4
Training loss: 2.552957024528967
Validation loss: 2.4801476658740826

Epoch: 5| Step: 5
Training loss: 2.4392490469580874
Validation loss: 2.4764363009030936

Epoch: 5| Step: 6
Training loss: 3.053219649879077
Validation loss: 2.479105673501837

Epoch: 5| Step: 7
Training loss: 2.3920089605221
Validation loss: 2.4723800175441335

Epoch: 5| Step: 8
Training loss: 2.5975529929841823
Validation loss: 2.4801829856806674

Epoch: 5| Step: 9
Training loss: 2.487875337893644
Validation loss: 2.4781658225367114

Epoch: 5| Step: 10
Training loss: 2.374947195971899
Validation loss: 2.482263415441709

Epoch: 5| Step: 11
Training loss: 2.6425393292722226
Validation loss: 2.485398323703386

Epoch: 112| Step: 0
Training loss: 2.286358899942908
Validation loss: 2.480175543649388

Epoch: 5| Step: 1
Training loss: 2.3676392781941993
Validation loss: 2.4828712582862402

Epoch: 5| Step: 2
Training loss: 2.294734866050706
Validation loss: 2.4781482325038513

Epoch: 5| Step: 3
Training loss: 3.075531617397919
Validation loss: 2.481844086715085

Epoch: 5| Step: 4
Training loss: 2.4855582820157567
Validation loss: 2.4823323015309304

Epoch: 5| Step: 5
Training loss: 2.5386919426113876
Validation loss: 2.479456184749837

Epoch: 5| Step: 6
Training loss: 2.9535988725186857
Validation loss: 2.4783421288607648

Epoch: 5| Step: 7
Training loss: 2.5755390491876415
Validation loss: 2.477840422798005

Epoch: 5| Step: 8
Training loss: 2.35888151351048
Validation loss: 2.479328952948614

Epoch: 5| Step: 9
Training loss: 2.3510507425976153
Validation loss: 2.4746267011518137

Epoch: 5| Step: 10
Training loss: 2.695120188168668
Validation loss: 2.478193686528759

Epoch: 5| Step: 11
Training loss: 3.734075430502002
Validation loss: 2.475750221354126

Epoch: 113| Step: 0
Training loss: 2.4892136579977127
Validation loss: 2.475026144506139

Epoch: 5| Step: 1
Training loss: 2.5575292804492435
Validation loss: 2.473490168126599

Epoch: 5| Step: 2
Training loss: 2.421127696259377
Validation loss: 2.4708046189923882

Epoch: 5| Step: 3
Training loss: 2.300123655063519
Validation loss: 2.469023415760243

Epoch: 5| Step: 4
Training loss: 2.5986107929591324
Validation loss: 2.4705709389368082

Epoch: 5| Step: 5
Training loss: 2.7504240055927327
Validation loss: 2.477219298962166

Epoch: 5| Step: 6
Training loss: 2.655211660852713
Validation loss: 2.472681287464981

Epoch: 5| Step: 7
Training loss: 2.7352650964778995
Validation loss: 2.476449259867626

Epoch: 5| Step: 8
Training loss: 2.515232126584791
Validation loss: 2.47650100278664

Epoch: 5| Step: 9
Training loss: 2.775076798072258
Validation loss: 2.47400893243276

Epoch: 5| Step: 10
Training loss: 2.344309828972858
Validation loss: 2.4733251999603922

Epoch: 5| Step: 11
Training loss: 2.906267719830052
Validation loss: 2.4753022783175362

Epoch: 114| Step: 0
Training loss: 2.670598042836583
Validation loss: 2.480460143387248

Epoch: 5| Step: 1
Training loss: 2.468468468946654
Validation loss: 2.477006810201134

Epoch: 5| Step: 2
Training loss: 2.3417032078327558
Validation loss: 2.476131963360564

Epoch: 5| Step: 3
Training loss: 2.1155107340530215
Validation loss: 2.4803686807626892

Epoch: 5| Step: 4
Training loss: 2.624169627050977
Validation loss: 2.4822701988834006

Epoch: 5| Step: 5
Training loss: 2.676354562640122
Validation loss: 2.4807107722238295

Epoch: 5| Step: 6
Training loss: 2.2379310959818555
Validation loss: 2.4766816352243866

Epoch: 5| Step: 7
Training loss: 2.6461502145757647
Validation loss: 2.475584147776823

Epoch: 5| Step: 8
Training loss: 2.511620598235327
Validation loss: 2.4723041960761813

Epoch: 5| Step: 9
Training loss: 2.5667127080808974
Validation loss: 2.4733176368817675

Epoch: 5| Step: 10
Training loss: 3.1108975923776776
Validation loss: 2.474554091882042

Epoch: 5| Step: 11
Training loss: 3.1283886656409154
Validation loss: 2.4719112372686025

Epoch: 115| Step: 0
Training loss: 3.0703343980314037
Validation loss: 2.4759759378809836

Epoch: 5| Step: 1
Training loss: 2.4627251807722113
Validation loss: 2.473871973772395

Epoch: 5| Step: 2
Training loss: 2.217925549845942
Validation loss: 2.4691601505079834

Epoch: 5| Step: 3
Training loss: 2.256319601829352
Validation loss: 2.4684202460882743

Epoch: 5| Step: 4
Training loss: 2.38993845537183
Validation loss: 2.4601865278577297

Epoch: 5| Step: 5
Training loss: 2.8852016400987157
Validation loss: 2.464567863230926

Epoch: 5| Step: 6
Training loss: 2.5364253507397945
Validation loss: 2.4640116670018024

Epoch: 5| Step: 7
Training loss: 2.8306169016783924
Validation loss: 2.4683607534984358

Epoch: 5| Step: 8
Training loss: 2.736014697403499
Validation loss: 2.4663675860424568

Epoch: 5| Step: 9
Training loss: 2.1328976792314256
Validation loss: 2.4657163450776816

Epoch: 5| Step: 10
Training loss: 2.414251918830279
Validation loss: 2.4706658160743786

Epoch: 5| Step: 11
Training loss: 3.2029710686145507
Validation loss: 2.467194328596616

Epoch: 116| Step: 0
Training loss: 1.8337760376333092
Validation loss: 2.46931585032017

Epoch: 5| Step: 1
Training loss: 2.6626722002533842
Validation loss: 2.466474091631242

Epoch: 5| Step: 2
Training loss: 2.7278840543041425
Validation loss: 2.46526965347408

Epoch: 5| Step: 3
Training loss: 2.5090612231949865
Validation loss: 2.469718280465109

Epoch: 5| Step: 4
Training loss: 2.571014679417643
Validation loss: 2.465888759556922

Epoch: 5| Step: 5
Training loss: 2.7463984747691574
Validation loss: 2.4622387992794432

Epoch: 5| Step: 6
Training loss: 3.2741054681884165
Validation loss: 2.4626502964220585

Epoch: 5| Step: 7
Training loss: 2.3534567671258007
Validation loss: 2.4702158732681356

Epoch: 5| Step: 8
Training loss: 2.617005845783076
Validation loss: 2.4659857706387496

Epoch: 5| Step: 9
Training loss: 2.2831297145557783
Validation loss: 2.462064365962274

Epoch: 5| Step: 10
Training loss: 2.55035854885586
Validation loss: 2.470018970118068

Epoch: 5| Step: 11
Training loss: 2.4196048063947173
Validation loss: 2.468180977154449

Epoch: 117| Step: 0
Training loss: 2.7026011229447886
Validation loss: 2.4676279222090325

Epoch: 5| Step: 1
Training loss: 2.5685879101626807
Validation loss: 2.46913473535628

Epoch: 5| Step: 2
Training loss: 2.1886169851549493
Validation loss: 2.4659557745962224

Epoch: 5| Step: 3
Training loss: 2.9587648820200627
Validation loss: 2.473037919272459

Epoch: 5| Step: 4
Training loss: 2.872913764655234
Validation loss: 2.467232438931033

Epoch: 5| Step: 5
Training loss: 1.9784375370202176
Validation loss: 2.460879709685417

Epoch: 5| Step: 6
Training loss: 2.694073289912221
Validation loss: 2.4649764446928235

Epoch: 5| Step: 7
Training loss: 2.393733976225844
Validation loss: 2.4559138576814283

Epoch: 5| Step: 8
Training loss: 2.6828527857444593
Validation loss: 2.4621661313278715

Epoch: 5| Step: 9
Training loss: 2.4582856922598086
Validation loss: 2.461661423064398

Epoch: 5| Step: 10
Training loss: 2.6523175919469044
Validation loss: 2.460118846865393

Epoch: 5| Step: 11
Training loss: 2.2391890304389412
Validation loss: 2.469538353540988

Epoch: 118| Step: 0
Training loss: 2.2519054821364226
Validation loss: 2.4638548377079452

Epoch: 5| Step: 1
Training loss: 2.449970817392117
Validation loss: 2.464548737150644

Epoch: 5| Step: 2
Training loss: 2.665694914031766
Validation loss: 2.470344282183484

Epoch: 5| Step: 3
Training loss: 2.9273118623371652
Validation loss: 2.46852796497696

Epoch: 5| Step: 4
Training loss: 2.3570785967832246
Validation loss: 2.4663130848829278

Epoch: 5| Step: 5
Training loss: 2.156133455084646
Validation loss: 2.46551963055615

Epoch: 5| Step: 6
Training loss: 2.6158864901236494
Validation loss: 2.464404793015112

Epoch: 5| Step: 7
Training loss: 2.4334706968789166
Validation loss: 2.462572602107031

Epoch: 5| Step: 8
Training loss: 2.4554378030582447
Validation loss: 2.459577146088641

Epoch: 5| Step: 9
Training loss: 2.8678002807050924
Validation loss: 2.4633396635782914

Epoch: 5| Step: 10
Training loss: 2.6328642073174957
Validation loss: 2.465854576552627

Epoch: 5| Step: 11
Training loss: 3.550087704716834
Validation loss: 2.463228453120667

Epoch: 119| Step: 0
Training loss: 2.969711629823111
Validation loss: 2.468121561308998

Epoch: 5| Step: 1
Training loss: 2.126908286947038
Validation loss: 2.467136519752733

Epoch: 5| Step: 2
Training loss: 2.1124123639771186
Validation loss: 2.4635706503993187

Epoch: 5| Step: 3
Training loss: 2.8285471564238205
Validation loss: 2.469697597379564

Epoch: 5| Step: 4
Training loss: 2.1598827333760586
Validation loss: 2.464883093663503

Epoch: 5| Step: 5
Training loss: 2.7303547105454906
Validation loss: 2.4614606392088585

Epoch: 5| Step: 6
Training loss: 2.727340019002792
Validation loss: 2.462442880216526

Epoch: 5| Step: 7
Training loss: 2.298116646783383
Validation loss: 2.463578449052635

Epoch: 5| Step: 8
Training loss: 3.0867211228160962
Validation loss: 2.4584468974267666

Epoch: 5| Step: 9
Training loss: 2.4215409571484057
Validation loss: 2.464222635803842

Epoch: 5| Step: 10
Training loss: 2.2673650965153356
Validation loss: 2.4630830323068036

Epoch: 5| Step: 11
Training loss: 3.073207276821496
Validation loss: 2.4654697604638365

Epoch: 120| Step: 0
Training loss: 2.545516137207768
Validation loss: 2.4600160680158245

Epoch: 5| Step: 1
Training loss: 2.646373563466532
Validation loss: 2.4563544452582367

Epoch: 5| Step: 2
Training loss: 2.5653739258071964
Validation loss: 2.45411910630035

Epoch: 5| Step: 3
Training loss: 2.4725850895047694
Validation loss: 2.4545275734213723

Epoch: 5| Step: 4
Training loss: 2.879436221572126
Validation loss: 2.4519166481116947

Epoch: 5| Step: 5
Training loss: 2.180379559433027
Validation loss: 2.460701994753519

Epoch: 5| Step: 6
Training loss: 2.7422005416690407
Validation loss: 2.458635848365457

Epoch: 5| Step: 7
Training loss: 2.9070300624304717
Validation loss: 2.4578745354585667

Epoch: 5| Step: 8
Training loss: 2.0090456489476947
Validation loss: 2.4670657114349765

Epoch: 5| Step: 9
Training loss: 2.4600346922560403
Validation loss: 2.476650172305299

Epoch: 5| Step: 10
Training loss: 2.726512569637066
Validation loss: 2.4806088305390297

Epoch: 5| Step: 11
Training loss: 2.7581352931110867
Validation loss: 2.4809760788865365

Epoch: 121| Step: 0
Training loss: 3.0389925205832826
Validation loss: 2.4853768397996263

Epoch: 5| Step: 1
Training loss: 2.6450813205960073
Validation loss: 2.4850087587060217

Epoch: 5| Step: 2
Training loss: 2.5559622007986778
Validation loss: 2.479228953906715

Epoch: 5| Step: 3
Training loss: 2.231897113896665
Validation loss: 2.4830553160538558

Epoch: 5| Step: 4
Training loss: 2.9319818490001284
Validation loss: 2.4837172255251896

Epoch: 5| Step: 5
Training loss: 2.2335621882607444
Validation loss: 2.47580384669169

Epoch: 5| Step: 6
Training loss: 2.403063757702651
Validation loss: 2.476804960077003

Epoch: 5| Step: 7
Training loss: 2.4596507750328356
Validation loss: 2.480643378891527

Epoch: 5| Step: 8
Training loss: 2.8532497483188775
Validation loss: 2.4760518154203237

Epoch: 5| Step: 9
Training loss: 2.6067969098281045
Validation loss: 2.4735800818681195

Epoch: 5| Step: 10
Training loss: 2.369865187178716
Validation loss: 2.4752008884796264

Epoch: 5| Step: 11
Training loss: 2.2937667471874574
Validation loss: 2.4815023596585406

Epoch: 122| Step: 0
Training loss: 2.773838927539784
Validation loss: 2.4842101628222513

Epoch: 5| Step: 1
Training loss: 2.0925380558395394
Validation loss: 2.476433747612037

Epoch: 5| Step: 2
Training loss: 2.556795795997725
Validation loss: 2.4788544415497045

Epoch: 5| Step: 3
Training loss: 2.9745152759759295
Validation loss: 2.477714503292428

Epoch: 5| Step: 4
Training loss: 2.4789560586498003
Validation loss: 2.4811041551289263

Epoch: 5| Step: 5
Training loss: 2.60480423307028
Validation loss: 2.4775581324044773

Epoch: 5| Step: 6
Training loss: 2.334305515302042
Validation loss: 2.4732351484200796

Epoch: 5| Step: 7
Training loss: 2.825668195171992
Validation loss: 2.4754258564913987

Epoch: 5| Step: 8
Training loss: 2.5295398727987295
Validation loss: 2.4737568595897783

Epoch: 5| Step: 9
Training loss: 2.4481716807757268
Validation loss: 2.4703397822939523

Epoch: 5| Step: 10
Training loss: 2.536165433313264
Validation loss: 2.473500361299434

Epoch: 5| Step: 11
Training loss: 2.720024073438139
Validation loss: 2.470952069955324

Epoch: 123| Step: 0
Training loss: 2.40098928089601
Validation loss: 2.4615931328857887

Epoch: 5| Step: 1
Training loss: 2.6066341967727102
Validation loss: 2.4679983278349504

Epoch: 5| Step: 2
Training loss: 2.5588410469110685
Validation loss: 2.4606021874684596

Epoch: 5| Step: 3
Training loss: 2.4261887065332477
Validation loss: 2.457654658619057

Epoch: 5| Step: 4
Training loss: 2.4055465066204635
Validation loss: 2.4576753541288503

Epoch: 5| Step: 5
Training loss: 2.577827020244235
Validation loss: 2.4501365430566584

Epoch: 5| Step: 6
Training loss: 2.6583387352899495
Validation loss: 2.4560904589374393

Epoch: 5| Step: 7
Training loss: 2.873913767073774
Validation loss: 2.4617684026287083

Epoch: 5| Step: 8
Training loss: 2.5846930791282468
Validation loss: 2.46121455656395

Epoch: 5| Step: 9
Training loss: 2.5780542537345825
Validation loss: 2.4657426737532395

Epoch: 5| Step: 10
Training loss: 2.4923318564003427
Validation loss: 2.4700520136217676

Epoch: 5| Step: 11
Training loss: 2.701514850502916
Validation loss: 2.48190520337428

Epoch: 124| Step: 0
Training loss: 2.5760800661237373
Validation loss: 2.4849178272991153

Epoch: 5| Step: 1
Training loss: 2.3107034044942374
Validation loss: 2.4840035030826084

Epoch: 5| Step: 2
Training loss: 2.108822785609824
Validation loss: 2.484232284637313

Epoch: 5| Step: 3
Training loss: 2.7343491907264097
Validation loss: 2.4890796292807758

Epoch: 5| Step: 4
Training loss: 2.5157508581676153
Validation loss: 2.483978331785914

Epoch: 5| Step: 5
Training loss: 2.5874532538720194
Validation loss: 2.4800874171661498

Epoch: 5| Step: 6
Training loss: 2.4387396570061637
Validation loss: 2.4892948746993184

Epoch: 5| Step: 7
Training loss: 2.434775957770771
Validation loss: 2.4825419095059424

Epoch: 5| Step: 8
Training loss: 2.89441247199573
Validation loss: 2.4822453221711718

Epoch: 5| Step: 9
Training loss: 2.7853148684068403
Validation loss: 2.480364443370581

Epoch: 5| Step: 10
Training loss: 2.879180853698785
Validation loss: 2.4773460937540244

Epoch: 5| Step: 11
Training loss: 2.147850261541106
Validation loss: 2.4827212381525023

Epoch: 125| Step: 0
Training loss: 2.228625817211604
Validation loss: 2.4715582733497583

Epoch: 5| Step: 1
Training loss: 2.0308999566926573
Validation loss: 2.473211952186731

Epoch: 5| Step: 2
Training loss: 2.2331009013180596
Validation loss: 2.467497536085677

Epoch: 5| Step: 3
Training loss: 3.1791700837269854
Validation loss: 2.466921124886863

Epoch: 5| Step: 4
Training loss: 2.38016230053841
Validation loss: 2.4652871963756855

Epoch: 5| Step: 5
Training loss: 3.24726253542873
Validation loss: 2.4689341086132495

Epoch: 5| Step: 6
Training loss: 2.611274347771569
Validation loss: 2.470639423257815

Epoch: 5| Step: 7
Training loss: 2.3398501693815845
Validation loss: 2.46440568185866

Epoch: 5| Step: 8
Training loss: 2.8302813154930555
Validation loss: 2.466688095839525

Epoch: 5| Step: 9
Training loss: 2.4683765177154857
Validation loss: 2.4634099821632733

Epoch: 5| Step: 10
Training loss: 2.347651984644108
Validation loss: 2.458440177556874

Epoch: 5| Step: 11
Training loss: 2.755893288017772
Validation loss: 2.455743615009176

Epoch: 126| Step: 0
Training loss: 2.4541874425383225
Validation loss: 2.4651321911395914

Epoch: 5| Step: 1
Training loss: 2.2607990298942
Validation loss: 2.4650850978887258

Epoch: 5| Step: 2
Training loss: 2.665051229245771
Validation loss: 2.4648656869306085

Epoch: 5| Step: 3
Training loss: 3.001680221512891
Validation loss: 2.461074988747947

Epoch: 5| Step: 4
Training loss: 2.5516228915728005
Validation loss: 2.461443789440898

Epoch: 5| Step: 5
Training loss: 2.3070692266586756
Validation loss: 2.458017871842673

Epoch: 5| Step: 6
Training loss: 2.5740769481820767
Validation loss: 2.4629273458511864

Epoch: 5| Step: 7
Training loss: 2.430086452688773
Validation loss: 2.46235636826358

Epoch: 5| Step: 8
Training loss: 2.5233164660831964
Validation loss: 2.462489217163334

Epoch: 5| Step: 9
Training loss: 2.42052032977201
Validation loss: 2.4615132460655236

Epoch: 5| Step: 10
Training loss: 2.8395643346033848
Validation loss: 2.462456080243523

Epoch: 5| Step: 11
Training loss: 2.2628897104733934
Validation loss: 2.456847794015894

Epoch: 127| Step: 0
Training loss: 2.105824965880657
Validation loss: 2.468890322950593

Epoch: 5| Step: 1
Training loss: 2.3779834279091694
Validation loss: 2.4675435566882498

Epoch: 5| Step: 2
Training loss: 2.8587133757359826
Validation loss: 2.4701767472557203

Epoch: 5| Step: 3
Training loss: 2.408212431234553
Validation loss: 2.4708846235392934

Epoch: 5| Step: 4
Training loss: 2.846911841948682
Validation loss: 2.4695329913324584

Epoch: 5| Step: 5
Training loss: 2.461155666792843
Validation loss: 2.4739703482620543

Epoch: 5| Step: 6
Training loss: 2.6934055825101746
Validation loss: 2.467493167888352

Epoch: 5| Step: 7
Training loss: 2.75656471445228
Validation loss: 2.4676854739748006

Epoch: 5| Step: 8
Training loss: 2.2005993590178816
Validation loss: 2.468149244875727

Epoch: 5| Step: 9
Training loss: 2.824493437054573
Validation loss: 2.471630335586014

Epoch: 5| Step: 10
Training loss: 2.3505531755675415
Validation loss: 2.466590733798497

Epoch: 5| Step: 11
Training loss: 2.9541281959026673
Validation loss: 2.463908288516865

Epoch: 128| Step: 0
Training loss: 2.494876570773303
Validation loss: 2.4559250642560584

Epoch: 5| Step: 1
Training loss: 2.946844618186604
Validation loss: 2.4550054797883782

Epoch: 5| Step: 2
Training loss: 2.5022144047193593
Validation loss: 2.4533727484323595

Epoch: 5| Step: 3
Training loss: 2.793556247623533
Validation loss: 2.45672533145314

Epoch: 5| Step: 4
Training loss: 2.0872614512940992
Validation loss: 2.4539657516738718

Epoch: 5| Step: 5
Training loss: 2.411386827983761
Validation loss: 2.4560724276741888

Epoch: 5| Step: 6
Training loss: 2.7564185404553525
Validation loss: 2.4538645955650993

Epoch: 5| Step: 7
Training loss: 2.4477612601656187
Validation loss: 2.45962981746286

Epoch: 5| Step: 8
Training loss: 2.501407417860872
Validation loss: 2.4559685289401822

Epoch: 5| Step: 9
Training loss: 2.3404966026302683
Validation loss: 2.462961309327726

Epoch: 5| Step: 10
Training loss: 2.6564885649417924
Validation loss: 2.459328176690084

Epoch: 5| Step: 11
Training loss: 2.842975888370401
Validation loss: 2.456295451098826

Epoch: 129| Step: 0
Training loss: 2.7171699053922422
Validation loss: 2.458372331299163

Epoch: 5| Step: 1
Training loss: 2.5790232625195424
Validation loss: 2.453866235147082

Epoch: 5| Step: 2
Training loss: 2.8169962229352987
Validation loss: 2.4513950281783745

Epoch: 5| Step: 3
Training loss: 2.3931310147209626
Validation loss: 2.4619635002380758

Epoch: 5| Step: 4
Training loss: 2.6577558848940517
Validation loss: 2.467360793750515

Epoch: 5| Step: 5
Training loss: 2.3702104359167278
Validation loss: 2.4732813916210437

Epoch: 5| Step: 6
Training loss: 2.4207163346382456
Validation loss: 2.479735126290383

Epoch: 5| Step: 7
Training loss: 2.5917912636231173
Validation loss: 2.4716257335371274

Epoch: 5| Step: 8
Training loss: 2.622971522995077
Validation loss: 2.472689021226326

Epoch: 5| Step: 9
Training loss: 2.8070390879019698
Validation loss: 2.473667293629038

Epoch: 5| Step: 10
Training loss: 1.8813155305417972
Validation loss: 2.470267955869839

Epoch: 5| Step: 11
Training loss: 2.9163756815761697
Validation loss: 2.4730003724129417

Epoch: 130| Step: 0
Training loss: 2.558452666375038
Validation loss: 2.459167277730204

Epoch: 5| Step: 1
Training loss: 2.667119305660135
Validation loss: 2.4519280410949373

Epoch: 5| Step: 2
Training loss: 2.7835256289300445
Validation loss: 2.455532884624979

Epoch: 5| Step: 3
Training loss: 2.449000395999501
Validation loss: 2.453986545088272

Epoch: 5| Step: 4
Training loss: 2.650368045944031
Validation loss: 2.452352321124079

Epoch: 5| Step: 5
Training loss: 2.7191007541341334
Validation loss: 2.454658191165178

Epoch: 5| Step: 6
Training loss: 2.520457962293988
Validation loss: 2.4519870977928315

Epoch: 5| Step: 7
Training loss: 2.697180589886493
Validation loss: 2.46029189587763

Epoch: 5| Step: 8
Training loss: 2.8897099876745784
Validation loss: 2.455949129600575

Epoch: 5| Step: 9
Training loss: 2.2767701776225517
Validation loss: 2.4436537303320236

Epoch: 5| Step: 10
Training loss: 2.0485326446621097
Validation loss: 2.4453044684037493

Epoch: 5| Step: 11
Training loss: 1.6502225350098456
Validation loss: 2.453344319090486

Epoch: 131| Step: 0
Training loss: 1.8336506987912427
Validation loss: 2.4637978899717488

Epoch: 5| Step: 1
Training loss: 2.4419272881507417
Validation loss: 2.4661639507290176

Epoch: 5| Step: 2
Training loss: 2.7670007936719854
Validation loss: 2.473911539270995

Epoch: 5| Step: 3
Training loss: 2.6661561139437575
Validation loss: 2.473328758575685

Epoch: 5| Step: 4
Training loss: 2.517559752830074
Validation loss: 2.4792388510146113

Epoch: 5| Step: 5
Training loss: 2.9429232268512586
Validation loss: 2.4812346750229746

Epoch: 5| Step: 6
Training loss: 2.2166378014580514
Validation loss: 2.4768782774353033

Epoch: 5| Step: 7
Training loss: 2.704512752571302
Validation loss: 2.4774834197214846

Epoch: 5| Step: 8
Training loss: 2.331084450578477
Validation loss: 2.4803896393262788

Epoch: 5| Step: 9
Training loss: 2.739486189789138
Validation loss: 2.4774967360900053

Epoch: 5| Step: 10
Training loss: 2.7799235406144893
Validation loss: 2.473669430109922

Epoch: 5| Step: 11
Training loss: 2.6796135628067312
Validation loss: 2.4742849047430706

Epoch: 132| Step: 0
Training loss: 2.537890636273232
Validation loss: 2.4659336298219814

Epoch: 5| Step: 1
Training loss: 2.551941495314462
Validation loss: 2.448531095350543

Epoch: 5| Step: 2
Training loss: 2.239865792375245
Validation loss: 2.4597155448714765

Epoch: 5| Step: 3
Training loss: 2.9182150046169295
Validation loss: 2.4650186636839138

Epoch: 5| Step: 4
Training loss: 2.371807663354991
Validation loss: 2.489433245477494

Epoch: 5| Step: 5
Training loss: 2.8512818916942115
Validation loss: 2.5067338019898

Epoch: 5| Step: 6
Training loss: 2.606902452944859
Validation loss: 2.5316539685376847

Epoch: 5| Step: 7
Training loss: 2.8512897517783404
Validation loss: 2.4931201405484624

Epoch: 5| Step: 8
Training loss: 2.8514068142967037
Validation loss: 2.459437047102172

Epoch: 5| Step: 9
Training loss: 2.415505382344703
Validation loss: 2.4536296766755026

Epoch: 5| Step: 10
Training loss: 2.458960426220004
Validation loss: 2.455451510048542

Epoch: 5| Step: 11
Training loss: 2.8686016036071105
Validation loss: 2.4633861368991274

Epoch: 133| Step: 0
Training loss: 2.464849940562749
Validation loss: 2.469233131617051

Epoch: 5| Step: 1
Training loss: 2.252370327552231
Validation loss: 2.4745110600388056

Epoch: 5| Step: 2
Training loss: 2.8116177340690784
Validation loss: 2.480403348606197

Epoch: 5| Step: 3
Training loss: 2.22572187650468
Validation loss: 2.4773182001784315

Epoch: 5| Step: 4
Training loss: 2.415469848861558
Validation loss: 2.480409264038957

Epoch: 5| Step: 5
Training loss: 2.2875262107806646
Validation loss: 2.4794979568660724

Epoch: 5| Step: 6
Training loss: 3.239864537503769
Validation loss: 2.47955621869811

Epoch: 5| Step: 7
Training loss: 2.902254865679251
Validation loss: 2.481276337125891

Epoch: 5| Step: 8
Training loss: 2.473441097331609
Validation loss: 2.479637743701684

Epoch: 5| Step: 9
Training loss: 2.9394526383409065
Validation loss: 2.4864738763249625

Epoch: 5| Step: 10
Training loss: 2.147763899196817
Validation loss: 2.479699385465691

Epoch: 5| Step: 11
Training loss: 2.5265768736063006
Validation loss: 2.4835619965721643

Epoch: 134| Step: 0
Training loss: 2.7274174702410923
Validation loss: 2.4790385891718056

Epoch: 5| Step: 1
Training loss: 2.7317026212823032
Validation loss: 2.4825117773728738

Epoch: 5| Step: 2
Training loss: 3.0024947284077306
Validation loss: 2.4835827201925533

Epoch: 5| Step: 3
Training loss: 1.9388678859579935
Validation loss: 2.4825579158023907

Epoch: 5| Step: 4
Training loss: 2.405110275056149
Validation loss: 2.480768080545044

Epoch: 5| Step: 5
Training loss: 2.683493799738707
Validation loss: 2.480889328271388

Epoch: 5| Step: 6
Training loss: 2.5593855935647194
Validation loss: 2.478228573200118

Epoch: 5| Step: 7
Training loss: 2.4695361853275384
Validation loss: 2.4807255168991795

Epoch: 5| Step: 8
Training loss: 2.693664312216799
Validation loss: 2.475890448487212

Epoch: 5| Step: 9
Training loss: 2.5643171984754485
Validation loss: 2.473639896712492

Epoch: 5| Step: 10
Training loss: 2.4670009936333406
Validation loss: 2.4747013916956537

Epoch: 5| Step: 11
Training loss: 3.2348461452965944
Validation loss: 2.4697674212049354

Epoch: 135| Step: 0
Training loss: 2.5813552502471477
Validation loss: 2.473933062507994

Epoch: 5| Step: 1
Training loss: 2.3994605650255627
Validation loss: 2.464254120330653

Epoch: 5| Step: 2
Training loss: 2.812094849968782
Validation loss: 2.463608048628727

Epoch: 5| Step: 3
Training loss: 2.4478333371095036
Validation loss: 2.4683391474632375

Epoch: 5| Step: 4
Training loss: 2.464112090081653
Validation loss: 2.463068048921757

Epoch: 5| Step: 5
Training loss: 2.8560403024837764
Validation loss: 2.4684649596663153

Epoch: 5| Step: 6
Training loss: 2.858963566764436
Validation loss: 2.458950233373951

Epoch: 5| Step: 7
Training loss: 2.0836762591096294
Validation loss: 2.4525885835774814

Epoch: 5| Step: 8
Training loss: 2.207809122114034
Validation loss: 2.46085834678634

Epoch: 5| Step: 9
Training loss: 2.5831939803243835
Validation loss: 2.456271350725739

Epoch: 5| Step: 10
Training loss: 3.0274722754975394
Validation loss: 2.4558394856651486

Epoch: 5| Step: 11
Training loss: 1.389912669299447
Validation loss: 2.462457193689997

Epoch: 136| Step: 0
Training loss: 2.6399392586280475
Validation loss: 2.454532879371466

Epoch: 5| Step: 1
Training loss: 2.187242111263335
Validation loss: 2.461883314728284

Epoch: 5| Step: 2
Training loss: 2.137611439794778
Validation loss: 2.454392298174513

Epoch: 5| Step: 3
Training loss: 2.5302360293392114
Validation loss: 2.456707733478321

Epoch: 5| Step: 4
Training loss: 2.754080259628802
Validation loss: 2.4547955277774163

Epoch: 5| Step: 5
Training loss: 2.5267337021536562
Validation loss: 2.4575349648214173

Epoch: 5| Step: 6
Training loss: 2.3657924456778985
Validation loss: 2.4538732812887853

Epoch: 5| Step: 7
Training loss: 2.218082488752481
Validation loss: 2.458915105505075

Epoch: 5| Step: 8
Training loss: 3.0523499425547467
Validation loss: 2.4567538065856427

Epoch: 5| Step: 9
Training loss: 2.257875118245264
Validation loss: 2.458880724551726

Epoch: 5| Step: 10
Training loss: 2.900993926627407
Validation loss: 2.4574064242248603

Epoch: 5| Step: 11
Training loss: 3.6902574477287593
Validation loss: 2.4619739065698503

Epoch: 137| Step: 0
Training loss: 2.6718901695133157
Validation loss: 2.458140035616104

Epoch: 5| Step: 1
Training loss: 1.9939374829576788
Validation loss: 2.4594317234579575

Epoch: 5| Step: 2
Training loss: 2.7015277355068728
Validation loss: 2.4582968132625402

Epoch: 5| Step: 3
Training loss: 2.528081252729835
Validation loss: 2.4552603375561297

Epoch: 5| Step: 4
Training loss: 2.4766954452499257
Validation loss: 2.459324981555664

Epoch: 5| Step: 5
Training loss: 2.5894477737148187
Validation loss: 2.46051439149882

Epoch: 5| Step: 6
Training loss: 2.952586452888346
Validation loss: 2.4614457468450137

Epoch: 5| Step: 7
Training loss: 2.5764841117524466
Validation loss: 2.4582322323478802

Epoch: 5| Step: 8
Training loss: 2.0248425881698693
Validation loss: 2.4631983993023403

Epoch: 5| Step: 9
Training loss: 2.5894825771266414
Validation loss: 2.455076340558626

Epoch: 5| Step: 10
Training loss: 2.485403747610286
Validation loss: 2.463020145843425

Epoch: 5| Step: 11
Training loss: 3.602734952064426
Validation loss: 2.4576515461933286

Epoch: 138| Step: 0
Training loss: 3.009558708311638
Validation loss: 2.4561381250647147

Epoch: 5| Step: 1
Training loss: 2.6510610723378547
Validation loss: 2.4483186121260645

Epoch: 5| Step: 2
Training loss: 2.2069930081725584
Validation loss: 2.4543963335102688

Epoch: 5| Step: 3
Training loss: 2.401845790458113
Validation loss: 2.4495133056035265

Epoch: 5| Step: 4
Training loss: 2.1440161136690663
Validation loss: 2.4550680495657042

Epoch: 5| Step: 5
Training loss: 2.8346627052592854
Validation loss: 2.4616914754696584

Epoch: 5| Step: 6
Training loss: 2.4026640887795243
Validation loss: 2.4549533930664444

Epoch: 5| Step: 7
Training loss: 2.4928652998626455
Validation loss: 2.459997120573845

Epoch: 5| Step: 8
Training loss: 2.487255993094218
Validation loss: 2.453539191751678

Epoch: 5| Step: 9
Training loss: 2.4650673742159315
Validation loss: 2.451361502193338

Epoch: 5| Step: 10
Training loss: 2.866526680862602
Validation loss: 2.451951047709988

Epoch: 5| Step: 11
Training loss: 2.9851386406425786
Validation loss: 2.462178332237531

Epoch: 139| Step: 0
Training loss: 2.4142800637386235
Validation loss: 2.4631446225400166

Epoch: 5| Step: 1
Training loss: 2.615938623145795
Validation loss: 2.4666125786710733

Epoch: 5| Step: 2
Training loss: 2.546472342085727
Validation loss: 2.474395802953135

Epoch: 5| Step: 3
Training loss: 2.8312362502145256
Validation loss: 2.47434457407404

Epoch: 5| Step: 4
Training loss: 2.7579418292532787
Validation loss: 2.479969028387178

Epoch: 5| Step: 5
Training loss: 2.457852612924117
Validation loss: 2.484177059709353

Epoch: 5| Step: 6
Training loss: 2.290651778706022
Validation loss: 2.4848531946572336

Epoch: 5| Step: 7
Training loss: 2.9136586254558647
Validation loss: 2.4868236086995945

Epoch: 5| Step: 8
Training loss: 2.196423992457917
Validation loss: 2.484984581014895

Epoch: 5| Step: 9
Training loss: 2.6562344718928017
Validation loss: 2.4866176175985073

Epoch: 5| Step: 10
Training loss: 2.6206029668418975
Validation loss: 2.4852372198776203

Epoch: 5| Step: 11
Training loss: 2.4600640578574793
Validation loss: 2.482695621710713

Epoch: 140| Step: 0
Training loss: 2.7324765835916525
Validation loss: 2.48199443603725

Epoch: 5| Step: 1
Training loss: 2.882902976169563
Validation loss: 2.4833668716670654

Epoch: 5| Step: 2
Training loss: 2.4474735156487895
Validation loss: 2.478899369669576

Epoch: 5| Step: 3
Training loss: 2.76271283119556
Validation loss: 2.478871233083968

Epoch: 5| Step: 4
Training loss: 2.619234838647373
Validation loss: 2.477777946963492

Epoch: 5| Step: 5
Training loss: 2.60115483386256
Validation loss: 2.475361405422343

Epoch: 5| Step: 6
Training loss: 2.3890487361134745
Validation loss: 2.4727506694875716

Epoch: 5| Step: 7
Training loss: 2.677697068058176
Validation loss: 2.476596755824757

Epoch: 5| Step: 8
Training loss: 2.2028568185760444
Validation loss: 2.4660074637620544

Epoch: 5| Step: 9
Training loss: 2.4813744040562415
Validation loss: 2.460729273307447

Epoch: 5| Step: 10
Training loss: 2.2632151448212006
Validation loss: 2.4521287000462633

Epoch: 5| Step: 11
Training loss: 2.206303677432738
Validation loss: 2.4542182583937024

Epoch: 141| Step: 0
Training loss: 2.6530278128536735
Validation loss: 2.4560660410644535

Epoch: 5| Step: 1
Training loss: 2.3913427534524923
Validation loss: 2.4514023509089697

Epoch: 5| Step: 2
Training loss: 2.077258739762027
Validation loss: 2.4601639354299474

Epoch: 5| Step: 3
Training loss: 2.717737294270987
Validation loss: 2.456121623022671

Epoch: 5| Step: 4
Training loss: 2.561248008239478
Validation loss: 2.4590266524304556

Epoch: 5| Step: 5
Training loss: 2.692414337183958
Validation loss: 2.458466111385159

Epoch: 5| Step: 6
Training loss: 3.08722808505389
Validation loss: 2.4473439026755632

Epoch: 5| Step: 7
Training loss: 2.67052858570986
Validation loss: 2.4460279659139554

Epoch: 5| Step: 8
Training loss: 2.1595052942033135
Validation loss: 2.4473925144335142

Epoch: 5| Step: 9
Training loss: 2.4918215491937064
Validation loss: 2.4517901042431505

Epoch: 5| Step: 10
Training loss: 2.220705164872359
Validation loss: 2.4607433626004735

Epoch: 5| Step: 11
Training loss: 3.205049411196409
Validation loss: 2.4608723788248503

Epoch: 142| Step: 0
Training loss: 2.557520331099645
Validation loss: 2.4709279436800125

Epoch: 5| Step: 1
Training loss: 2.025327528850851
Validation loss: 2.4673501081716713

Epoch: 5| Step: 2
Training loss: 3.0599620968833574
Validation loss: 2.462527991311661

Epoch: 5| Step: 3
Training loss: 2.2451467093127544
Validation loss: 2.4693210440337428

Epoch: 5| Step: 4
Training loss: 2.6595259268368823
Validation loss: 2.4649779035894572

Epoch: 5| Step: 5
Training loss: 2.5158845755720427
Validation loss: 2.463130204166934

Epoch: 5| Step: 6
Training loss: 2.4046286532989067
Validation loss: 2.465741154876308

Epoch: 5| Step: 7
Training loss: 2.77448581708843
Validation loss: 2.470884531068575

Epoch: 5| Step: 8
Training loss: 2.7488882244999844
Validation loss: 2.4698522698410774

Epoch: 5| Step: 9
Training loss: 2.3069394246760697
Validation loss: 2.469182513774401

Epoch: 5| Step: 10
Training loss: 2.695882631899621
Validation loss: 2.4613119778983585

Epoch: 5| Step: 11
Training loss: 1.6350770423539631
Validation loss: 2.4666539239124696

Epoch: 143| Step: 0
Training loss: 2.0708329608182616
Validation loss: 2.4618289523738346

Epoch: 5| Step: 1
Training loss: 2.989261162353154
Validation loss: 2.4598994694756335

Epoch: 5| Step: 2
Training loss: 2.539781767864169
Validation loss: 2.461604993607654

Epoch: 5| Step: 3
Training loss: 1.8801958571536994
Validation loss: 2.458401607653724

Epoch: 5| Step: 4
Training loss: 2.566606441220584
Validation loss: 2.444964535445589

Epoch: 5| Step: 5
Training loss: 2.2937849369558636
Validation loss: 2.447868571248593

Epoch: 5| Step: 6
Training loss: 2.541300283404566
Validation loss: 2.446994564534147

Epoch: 5| Step: 7
Training loss: 2.912490543464731
Validation loss: 2.45052492233946

Epoch: 5| Step: 8
Training loss: 2.84720879114803
Validation loss: 2.4440406603695637

Epoch: 5| Step: 9
Training loss: 2.743279046934855
Validation loss: 2.4453486032634055

Epoch: 5| Step: 10
Training loss: 2.6150865919024926
Validation loss: 2.451792648757968

Epoch: 5| Step: 11
Training loss: 1.147222122339137
Validation loss: 2.4512258374276215

Epoch: 144| Step: 0
Training loss: 2.8377318367375532
Validation loss: 2.4431684802136866

Epoch: 5| Step: 1
Training loss: 2.213604292638137
Validation loss: 2.4466594768396006

Epoch: 5| Step: 2
Training loss: 2.489562849867655
Validation loss: 2.4492215729960307

Epoch: 5| Step: 3
Training loss: 3.200501498026081
Validation loss: 2.4473283195938706

Epoch: 5| Step: 4
Training loss: 2.457731259428703
Validation loss: 2.450147291543796

Epoch: 5| Step: 5
Training loss: 2.524086505586349
Validation loss: 2.4444133295682264

Epoch: 5| Step: 6
Training loss: 2.44200812893451
Validation loss: 2.453454556368407

Epoch: 5| Step: 7
Training loss: 2.7636637657235266
Validation loss: 2.4480788576219994

Epoch: 5| Step: 8
Training loss: 1.9809236323865218
Validation loss: 2.4534463084834552

Epoch: 5| Step: 9
Training loss: 2.357830729581564
Validation loss: 2.4491603809886935

Epoch: 5| Step: 10
Training loss: 2.416294189775274
Validation loss: 2.4534681610665023

Epoch: 5| Step: 11
Training loss: 2.4139003267103805
Validation loss: 2.453401788861334

Epoch: 145| Step: 0
Training loss: 2.645860166238403
Validation loss: 2.4552439833653015

Epoch: 5| Step: 1
Training loss: 2.4683222339731734
Validation loss: 2.455131880182569

Epoch: 5| Step: 2
Training loss: 2.439453809140967
Validation loss: 2.4526603809480756

Epoch: 5| Step: 3
Training loss: 2.4389717353975455
Validation loss: 2.4533436914627855

Epoch: 5| Step: 4
Training loss: 2.035615191999682
Validation loss: 2.459465438330321

Epoch: 5| Step: 5
Training loss: 2.5325883694413394
Validation loss: 2.453925324140888

Epoch: 5| Step: 6
Training loss: 2.651566449119131
Validation loss: 2.4497683819453813

Epoch: 5| Step: 7
Training loss: 2.5204394219278816
Validation loss: 2.455790737724613

Epoch: 5| Step: 8
Training loss: 2.5396108181983625
Validation loss: 2.4568606804134685

Epoch: 5| Step: 9
Training loss: 2.687274568551008
Validation loss: 2.4561656626004953

Epoch: 5| Step: 10
Training loss: 2.496768866086807
Validation loss: 2.4488760928017084

Epoch: 5| Step: 11
Training loss: 3.840221739565725
Validation loss: 2.4520370354788037

Epoch: 146| Step: 0
Training loss: 2.6743349942154593
Validation loss: 2.452590147053836

Epoch: 5| Step: 1
Training loss: 2.3150090189813706
Validation loss: 2.4505139606526747

Epoch: 5| Step: 2
Training loss: 2.6011614332835924
Validation loss: 2.4468156056282133

Epoch: 5| Step: 3
Training loss: 2.5428967443722645
Validation loss: 2.448037076924074

Epoch: 5| Step: 4
Training loss: 3.031728805008644
Validation loss: 2.442476921805475

Epoch: 5| Step: 5
Training loss: 2.6192041626917546
Validation loss: 2.4415613435437913

Epoch: 5| Step: 6
Training loss: 2.384457879982092
Validation loss: 2.449001328969657

Epoch: 5| Step: 7
Training loss: 2.860096157831666
Validation loss: 2.4455419235748876

Epoch: 5| Step: 8
Training loss: 2.34628433535719
Validation loss: 2.4489342170857338

Epoch: 5| Step: 9
Training loss: 2.1006332668787584
Validation loss: 2.444830821638954

Epoch: 5| Step: 10
Training loss: 2.3649072523258226
Validation loss: 2.4593795244756147

Epoch: 5| Step: 11
Training loss: 1.4211115202125622
Validation loss: 2.4544397787209316

Epoch: 147| Step: 0
Training loss: 2.139969662781062
Validation loss: 2.452919779331087

Epoch: 5| Step: 1
Training loss: 2.9542734647814703
Validation loss: 2.4544670904459016

Epoch: 5| Step: 2
Training loss: 2.636441500180596
Validation loss: 2.4505712860026976

Epoch: 5| Step: 3
Training loss: 2.0725531203361207
Validation loss: 2.4567488895741247

Epoch: 5| Step: 4
Training loss: 2.705282596062849
Validation loss: 2.4549929802089174

Epoch: 5| Step: 5
Training loss: 2.6945667949133782
Validation loss: 2.4446540978020783

Epoch: 5| Step: 6
Training loss: 2.6443062127294725
Validation loss: 2.449235724431404

Epoch: 5| Step: 7
Training loss: 2.339830401705717
Validation loss: 2.453583830345119

Epoch: 5| Step: 8
Training loss: 2.3316459912612064
Validation loss: 2.4526900495061916

Epoch: 5| Step: 9
Training loss: 2.156530942444136
Validation loss: 2.45048203608211

Epoch: 5| Step: 10
Training loss: 2.5723031634326685
Validation loss: 2.447300299023095

Epoch: 5| Step: 11
Training loss: 3.958205987320711
Validation loss: 2.453227889379586

Epoch: 148| Step: 0
Training loss: 2.5027387875892697
Validation loss: 2.446737246088698

Epoch: 5| Step: 1
Training loss: 2.7038985011164156
Validation loss: 2.461354352573889

Epoch: 5| Step: 2
Training loss: 2.370812187256952
Validation loss: 2.458808922965827

Epoch: 5| Step: 3
Training loss: 2.631430152651769
Validation loss: 2.460730472313461

Epoch: 5| Step: 4
Training loss: 2.7209360388570825
Validation loss: 2.461686733775735

Epoch: 5| Step: 5
Training loss: 2.521551697155103
Validation loss: 2.4717202004285483

Epoch: 5| Step: 6
Training loss: 2.1821379201501845
Validation loss: 2.474573497918602

Epoch: 5| Step: 7
Training loss: 2.3655768728462863
Validation loss: 2.47067947878975

Epoch: 5| Step: 8
Training loss: 2.6476050224762573
Validation loss: 2.478686852599007

Epoch: 5| Step: 9
Training loss: 2.723059735167237
Validation loss: 2.4792331090940727

Epoch: 5| Step: 10
Training loss: 2.429825463011836
Validation loss: 2.4784977027720045

Epoch: 5| Step: 11
Training loss: 3.605559210505214
Validation loss: 2.4715073112306594

Epoch: 149| Step: 0
Training loss: 2.7585116356636594
Validation loss: 2.4675310541941

Epoch: 5| Step: 1
Training loss: 2.304397926889026
Validation loss: 2.465799310676599

Epoch: 5| Step: 2
Training loss: 2.6704771612748126
Validation loss: 2.4619647591684926

Epoch: 5| Step: 3
Training loss: 2.325146196752624
Validation loss: 2.4577857811298616

Epoch: 5| Step: 4
Training loss: 2.119432561076414
Validation loss: 2.451202228251822

Epoch: 5| Step: 5
Training loss: 2.524085466555038
Validation loss: 2.4528304447614904

Epoch: 5| Step: 6
Training loss: 2.806090957450741
Validation loss: 2.4538590371716094

Epoch: 5| Step: 7
Training loss: 3.0396900460691505
Validation loss: 2.4516350074323547

Epoch: 5| Step: 8
Training loss: 2.455360705880887
Validation loss: 2.447333060693046

Epoch: 5| Step: 9
Training loss: 2.5502972223931835
Validation loss: 2.4481043209357907

Epoch: 5| Step: 10
Training loss: 2.3569378928869793
Validation loss: 2.4485212485786

Epoch: 5| Step: 11
Training loss: 2.1018549492293936
Validation loss: 2.4530519365254624

Epoch: 150| Step: 0
Training loss: 2.747380222559446
Validation loss: 2.4425076657789564

Epoch: 5| Step: 1
Training loss: 2.820435751126572
Validation loss: 2.440690727667623

Epoch: 5| Step: 2
Training loss: 2.603216430230743
Validation loss: 2.442130865773361

Epoch: 5| Step: 3
Training loss: 2.1335630109452084
Validation loss: 2.4441038075594848

Epoch: 5| Step: 4
Training loss: 2.774008678667482
Validation loss: 2.4422496549377186

Epoch: 5| Step: 5
Training loss: 2.326520840775921
Validation loss: 2.4419763413167623

Epoch: 5| Step: 6
Training loss: 2.625833742252061
Validation loss: 2.451134605175033

Epoch: 5| Step: 7
Training loss: 2.7812424563187723
Validation loss: 2.4421501430160837

Epoch: 5| Step: 8
Training loss: 2.152879596339621
Validation loss: 2.4436944843045008

Epoch: 5| Step: 9
Training loss: 2.56118172952527
Validation loss: 2.4472643300931103

Epoch: 5| Step: 10
Training loss: 2.3745058449090672
Validation loss: 2.445368212632963

Epoch: 5| Step: 11
Training loss: 2.04975001974207
Validation loss: 2.4471915909472304

Epoch: 151| Step: 0
Training loss: 2.462678130237615
Validation loss: 2.455216081456251

Epoch: 5| Step: 1
Training loss: 2.156150483861833
Validation loss: 2.4543295573616053

Epoch: 5| Step: 2
Training loss: 2.5217856087612156
Validation loss: 2.460101911183542

Epoch: 5| Step: 3
Training loss: 2.24258483130883
Validation loss: 2.4641431567485657

Epoch: 5| Step: 4
Training loss: 2.638851857064954
Validation loss: 2.4644527175507145

Epoch: 5| Step: 5
Training loss: 2.4626681585068133
Validation loss: 2.4720178094274305

Epoch: 5| Step: 6
Training loss: 3.21111391794096
Validation loss: 2.4691753101573095

Epoch: 5| Step: 7
Training loss: 3.030373672760113
Validation loss: 2.4733906399080774

Epoch: 5| Step: 8
Training loss: 2.356841489178843
Validation loss: 2.473132545113271

Epoch: 5| Step: 9
Training loss: 2.375707019171847
Validation loss: 2.467632020441553

Epoch: 5| Step: 10
Training loss: 2.4579255579453334
Validation loss: 2.471566714012588

Epoch: 5| Step: 11
Training loss: 2.689627404200309
Validation loss: 2.467399416860381

Epoch: 152| Step: 0
Training loss: 2.48722081366855
Validation loss: 2.466962114620488

Epoch: 5| Step: 1
Training loss: 2.9317427686652686
Validation loss: 2.464989199933888

Epoch: 5| Step: 2
Training loss: 2.5727240198165666
Validation loss: 2.4617799881033546

Epoch: 5| Step: 3
Training loss: 2.6305667343448165
Validation loss: 2.460247100383367

Epoch: 5| Step: 4
Training loss: 2.6651228866729797
Validation loss: 2.4630465920716627

Epoch: 5| Step: 5
Training loss: 2.2142449432346316
Validation loss: 2.460168623527195

Epoch: 5| Step: 6
Training loss: 2.8210829424802935
Validation loss: 2.4628342640433076

Epoch: 5| Step: 7
Training loss: 2.674628818464132
Validation loss: 2.460176751964041

Epoch: 5| Step: 8
Training loss: 2.4571071303739034
Validation loss: 2.4594262806459084

Epoch: 5| Step: 9
Training loss: 1.8191995779506693
Validation loss: 2.462133972522819

Epoch: 5| Step: 10
Training loss: 2.4397463106943875
Validation loss: 2.4510690857509676

Epoch: 5| Step: 11
Training loss: 2.7579281704340897
Validation loss: 2.45338634548582

Epoch: 153| Step: 0
Training loss: 2.3016501892332406
Validation loss: 2.459057326846867

Epoch: 5| Step: 1
Training loss: 2.845766463455209
Validation loss: 2.462138844484713

Epoch: 5| Step: 2
Training loss: 2.7430482909888365
Validation loss: 2.458667321604916

Epoch: 5| Step: 3
Training loss: 2.2186504932709665
Validation loss: 2.4589711684563427

Epoch: 5| Step: 4
Training loss: 2.2609296881613923
Validation loss: 2.4564049816660125

Epoch: 5| Step: 5
Training loss: 2.6857420028543317
Validation loss: 2.460052807546095

Epoch: 5| Step: 6
Training loss: 2.3921944446824206
Validation loss: 2.468519022956827

Epoch: 5| Step: 7
Training loss: 2.356038342589723
Validation loss: 2.4601728472488484

Epoch: 5| Step: 8
Training loss: 2.4886327761714884
Validation loss: 2.4553113981832517

Epoch: 5| Step: 9
Training loss: 2.876431647727415
Validation loss: 2.4527396124115426

Epoch: 5| Step: 10
Training loss: 2.8724006844159984
Validation loss: 2.4578861877880542

Epoch: 5| Step: 11
Training loss: 1.119323768742608
Validation loss: 2.4475495178089286

Epoch: 154| Step: 0
Training loss: 2.7634820771841397
Validation loss: 2.450133331881229

Epoch: 5| Step: 1
Training loss: 2.7099915538811064
Validation loss: 2.4570148873807747

Epoch: 5| Step: 2
Training loss: 2.543029784423799
Validation loss: 2.462808335875978

Epoch: 5| Step: 3
Training loss: 2.4902295879243206
Validation loss: 2.4565619145264765

Epoch: 5| Step: 4
Training loss: 2.377590774102878
Validation loss: 2.4591046486452934

Epoch: 5| Step: 5
Training loss: 2.572753303891204
Validation loss: 2.459179232963928

Epoch: 5| Step: 6
Training loss: 2.223058466394288
Validation loss: 2.4628478209617635

Epoch: 5| Step: 7
Training loss: 3.0011780333241678
Validation loss: 2.457959483432744

Epoch: 5| Step: 8
Training loss: 2.2610904960827707
Validation loss: 2.463285704388005

Epoch: 5| Step: 9
Training loss: 2.6995183444444653
Validation loss: 2.4595499759264383

Epoch: 5| Step: 10
Training loss: 2.261484400808212
Validation loss: 2.4574575938734196

Epoch: 5| Step: 11
Training loss: 2.3164674901988573
Validation loss: 2.4563686203078747

Epoch: 155| Step: 0
Training loss: 2.5910533077007827
Validation loss: 2.4507779680043034

Epoch: 5| Step: 1
Training loss: 2.5675457844278173
Validation loss: 2.4556298476867386

Epoch: 5| Step: 2
Training loss: 2.5739979396002646
Validation loss: 2.4496101906187597

Epoch: 5| Step: 3
Training loss: 2.265033723693803
Validation loss: 2.4495103633050395

Epoch: 5| Step: 4
Training loss: 2.9804005946294825
Validation loss: 2.44819507368414

Epoch: 5| Step: 5
Training loss: 1.7744757348167286
Validation loss: 2.449099789802394

Epoch: 5| Step: 6
Training loss: 2.997657656178407
Validation loss: 2.448477008616859

Epoch: 5| Step: 7
Training loss: 2.813552828929857
Validation loss: 2.447028905446299

Epoch: 5| Step: 8
Training loss: 2.53076713571736
Validation loss: 2.445784327250555

Epoch: 5| Step: 9
Training loss: 2.0131566510559167
Validation loss: 2.443072209913798

Epoch: 5| Step: 10
Training loss: 2.664912024042488
Validation loss: 2.4471303136732505

Epoch: 5| Step: 11
Training loss: 1.3790079138779756
Validation loss: 2.446302707047776

Epoch: 156| Step: 0
Training loss: 2.3541168837783384
Validation loss: 2.441155158800257

Epoch: 5| Step: 1
Training loss: 2.9072888886290267
Validation loss: 2.4413351959061336

Epoch: 5| Step: 2
Training loss: 2.9088559516393544
Validation loss: 2.4481184828873332

Epoch: 5| Step: 3
Training loss: 2.3661054397976415
Validation loss: 2.4483575599732093

Epoch: 5| Step: 4
Training loss: 2.55944549128114
Validation loss: 2.4466991007311223

Epoch: 5| Step: 5
Training loss: 2.8984286161631037
Validation loss: 2.4457565773764323

Epoch: 5| Step: 6
Training loss: 2.011500078186595
Validation loss: 2.4490100380260977

Epoch: 5| Step: 7
Training loss: 2.334552877612049
Validation loss: 2.445317045944912

Epoch: 5| Step: 8
Training loss: 1.9189705339866054
Validation loss: 2.441556188429636

Epoch: 5| Step: 9
Training loss: 2.5429507487842127
Validation loss: 2.452330551768185

Epoch: 5| Step: 10
Training loss: 2.702930244971129
Validation loss: 2.449506950558811

Epoch: 5| Step: 11
Training loss: 2.3192358119835323
Validation loss: 2.4505074095620154

Epoch: 157| Step: 0
Training loss: 2.8529485808832193
Validation loss: 2.450544895671522

Epoch: 5| Step: 1
Training loss: 2.7678491302233623
Validation loss: 2.4577010454216013

Epoch: 5| Step: 2
Training loss: 2.638186982707082
Validation loss: 2.4591036225548324

Epoch: 5| Step: 3
Training loss: 2.523015604092625
Validation loss: 2.457751590518064

Epoch: 5| Step: 4
Training loss: 2.474219914042491
Validation loss: 2.4616998853962455

Epoch: 5| Step: 5
Training loss: 2.844936584409488
Validation loss: 2.4639774760992297

Epoch: 5| Step: 6
Training loss: 2.1582064391235867
Validation loss: 2.4596394481124406

Epoch: 5| Step: 7
Training loss: 1.982550195008015
Validation loss: 2.4592388648583485

Epoch: 5| Step: 8
Training loss: 2.786185182867424
Validation loss: 2.468320491303602

Epoch: 5| Step: 9
Training loss: 1.8672565782609625
Validation loss: 2.468675262189054

Epoch: 5| Step: 10
Training loss: 2.348603677866331
Validation loss: 2.4691922318852417

Epoch: 5| Step: 11
Training loss: 3.483687807752343
Validation loss: 2.4709920098611557

Epoch: 158| Step: 0
Training loss: 2.294438321729141
Validation loss: 2.463021392133852

Epoch: 5| Step: 1
Training loss: 2.7101937188873575
Validation loss: 2.461129842033417

Epoch: 5| Step: 2
Training loss: 2.5798913626753333
Validation loss: 2.4597844929683546

Epoch: 5| Step: 3
Training loss: 2.5428871809714497
Validation loss: 2.4615741774045667

Epoch: 5| Step: 4
Training loss: 2.983390925471312
Validation loss: 2.458134115088897

Epoch: 5| Step: 5
Training loss: 2.2048477038946217
Validation loss: 2.462335744372304

Epoch: 5| Step: 6
Training loss: 2.5587091085039186
Validation loss: 2.4585401534242943

Epoch: 5| Step: 7
Training loss: 2.422148363006141
Validation loss: 2.4534083889108884

Epoch: 5| Step: 8
Training loss: 2.630954075208405
Validation loss: 2.451487081456456

Epoch: 5| Step: 9
Training loss: 2.918167127941215
Validation loss: 2.457815505032199

Epoch: 5| Step: 10
Training loss: 1.9772587330242233
Validation loss: 2.450008794223806

Epoch: 5| Step: 11
Training loss: 1.7699152043607742
Validation loss: 2.45168091856935

Epoch: 159| Step: 0
Training loss: 3.071709933653226
Validation loss: 2.4537485265703096

Epoch: 5| Step: 1
Training loss: 2.4286858066904475
Validation loss: 2.449746731558143

Epoch: 5| Step: 2
Training loss: 2.436215869396344
Validation loss: 2.4535670762906943

Epoch: 5| Step: 3
Training loss: 2.239757324084146
Validation loss: 2.44784245615268

Epoch: 5| Step: 4
Training loss: 2.115446043122997
Validation loss: 2.4496403665376136

Epoch: 5| Step: 5
Training loss: 2.9972812731227614
Validation loss: 2.455668359990022

Epoch: 5| Step: 6
Training loss: 2.0621109797881023
Validation loss: 2.4481608343435592

Epoch: 5| Step: 7
Training loss: 2.456589311842357
Validation loss: 2.4424992406102706

Epoch: 5| Step: 8
Training loss: 3.0744628809314944
Validation loss: 2.444694229630997

Epoch: 5| Step: 9
Training loss: 2.750448970557777
Validation loss: 2.4481761199675516

Epoch: 5| Step: 10
Training loss: 1.8799467952788513
Validation loss: 2.445068700659924

Epoch: 5| Step: 11
Training loss: 1.7530238049246005
Validation loss: 2.440778162208875

Epoch: 160| Step: 0
Training loss: 2.355045008462115
Validation loss: 2.451086025077122

Epoch: 5| Step: 1
Training loss: 2.443955992775402
Validation loss: 2.4473002949638847

Epoch: 5| Step: 2
Training loss: 2.1995779933191386
Validation loss: 2.44845281505684

Epoch: 5| Step: 3
Training loss: 2.8730838858828722
Validation loss: 2.44487132235381

Epoch: 5| Step: 4
Training loss: 2.271761456660183
Validation loss: 2.4428207124888663

Epoch: 5| Step: 5
Training loss: 2.729543198874846
Validation loss: 2.4455513009823604

Epoch: 5| Step: 6
Training loss: 2.336188771694134
Validation loss: 2.4396471445765653

Epoch: 5| Step: 7
Training loss: 2.241396455420936
Validation loss: 2.4445213077904198

Epoch: 5| Step: 8
Training loss: 2.3614940095979375
Validation loss: 2.4415564488302968

Epoch: 5| Step: 9
Training loss: 2.857426881296879
Validation loss: 2.4411532461679033

Epoch: 5| Step: 10
Training loss: 2.800719277774977
Validation loss: 2.4360657042214533

Epoch: 5| Step: 11
Training loss: 2.888584550900776
Validation loss: 2.4368003957918516

Epoch: 161| Step: 0
Training loss: 2.076563657325439
Validation loss: 2.449699374977654

Epoch: 5| Step: 1
Training loss: 2.7733304714988956
Validation loss: 2.4505153835667093

Epoch: 5| Step: 2
Training loss: 2.880826602038162
Validation loss: 2.444585084632241

Epoch: 5| Step: 3
Training loss: 2.25610054345333
Validation loss: 2.453257069201385

Epoch: 5| Step: 4
Training loss: 1.9823212095362093
Validation loss: 2.461644324466229

Epoch: 5| Step: 5
Training loss: 2.3887666947017467
Validation loss: 2.462432185399487

Epoch: 5| Step: 6
Training loss: 2.9388053104155873
Validation loss: 2.455779136130445

Epoch: 5| Step: 7
Training loss: 2.664359604721458
Validation loss: 2.4574917319822625

Epoch: 5| Step: 8
Training loss: 2.5651365533065436
Validation loss: 2.4605629570344547

Epoch: 5| Step: 9
Training loss: 2.615148860567453
Validation loss: 2.44830350998134

Epoch: 5| Step: 10
Training loss: 2.234061479246755
Validation loss: 2.4482697306664756

Epoch: 5| Step: 11
Training loss: 3.2657856148824354
Validation loss: 2.449401860791893

Epoch: 162| Step: 0
Training loss: 2.307961107762665
Validation loss: 2.4487724201720207

Epoch: 5| Step: 1
Training loss: 2.143826283690811
Validation loss: 2.4501777203322237

Epoch: 5| Step: 2
Training loss: 2.429730675611159
Validation loss: 2.447783532817207

Epoch: 5| Step: 3
Training loss: 2.5570757140883735
Validation loss: 2.451647862475297

Epoch: 5| Step: 4
Training loss: 2.5554613347122332
Validation loss: 2.4510947611762455

Epoch: 5| Step: 5
Training loss: 2.3369247748722817
Validation loss: 2.455237446918383

Epoch: 5| Step: 6
Training loss: 2.302589314868806
Validation loss: 2.454606509880904

Epoch: 5| Step: 7
Training loss: 2.631689902024454
Validation loss: 2.4536964151902647

Epoch: 5| Step: 8
Training loss: 2.5983698502919577
Validation loss: 2.453765336065031

Epoch: 5| Step: 9
Training loss: 2.2675148283989848
Validation loss: 2.4434533613156635

Epoch: 5| Step: 10
Training loss: 3.3448789598036734
Validation loss: 2.4475466401199255

Epoch: 5| Step: 11
Training loss: 2.363725503827864
Validation loss: 2.4385760450866174

Epoch: 163| Step: 0
Training loss: 2.3652437494975764
Validation loss: 2.4350083282649435

Epoch: 5| Step: 1
Training loss: 2.490539962867616
Validation loss: 2.442462344821319

Epoch: 5| Step: 2
Training loss: 2.731181432356513
Validation loss: 2.43641598960828

Epoch: 5| Step: 3
Training loss: 2.205954716187389
Validation loss: 2.4395528487383844

Epoch: 5| Step: 4
Training loss: 2.653420051550584
Validation loss: 2.438524157398648

Epoch: 5| Step: 5
Training loss: 2.7488419955663796
Validation loss: 2.4402427973224583

Epoch: 5| Step: 6
Training loss: 2.5434882926813964
Validation loss: 2.4444012655402587

Epoch: 5| Step: 7
Training loss: 2.325020890500958
Validation loss: 2.440363644564556

Epoch: 5| Step: 8
Training loss: 2.2380426356231884
Validation loss: 2.4420510175613

Epoch: 5| Step: 9
Training loss: 2.574978382056887
Validation loss: 2.4484104402495683

Epoch: 5| Step: 10
Training loss: 2.8309133704273357
Validation loss: 2.4416759047045966

Epoch: 5| Step: 11
Training loss: 1.8495853990493287
Validation loss: 2.4396255631759325

Epoch: 164| Step: 0
Training loss: 2.4125376209231195
Validation loss: 2.44479500735531

Epoch: 5| Step: 1
Training loss: 3.224244696122432
Validation loss: 2.445090404658065

Epoch: 5| Step: 2
Training loss: 2.351029750763918
Validation loss: 2.4468801346518347

Epoch: 5| Step: 3
Training loss: 2.3756779154941237
Validation loss: 2.451591664471654

Epoch: 5| Step: 4
Training loss: 2.843313833470204
Validation loss: 2.449879745205013

Epoch: 5| Step: 5
Training loss: 2.888130555105243
Validation loss: 2.449525305926983

Epoch: 5| Step: 6
Training loss: 2.1575497013524747
Validation loss: 2.4479020801407203

Epoch: 5| Step: 7
Training loss: 1.6357414587218526
Validation loss: 2.445053082762956

Epoch: 5| Step: 8
Training loss: 2.5554686119198506
Validation loss: 2.448860042816537

Epoch: 5| Step: 9
Training loss: 2.6845546817706105
Validation loss: 2.4434695626836223

Epoch: 5| Step: 10
Training loss: 2.316267707374111
Validation loss: 2.442391545156684

Epoch: 5| Step: 11
Training loss: 1.8583894570033934
Validation loss: 2.447299436440678

Epoch: 165| Step: 0
Training loss: 2.2625233432977625
Validation loss: 2.4464469082098397

Epoch: 5| Step: 1
Training loss: 2.2751540163326673
Validation loss: 2.439326653359396

Epoch: 5| Step: 2
Training loss: 2.6162936833685695
Validation loss: 2.442244192137789

Epoch: 5| Step: 3
Training loss: 2.4925673622816045
Validation loss: 2.4405250932643088

Epoch: 5| Step: 4
Training loss: 2.666548299149212
Validation loss: 2.4454076502673794

Epoch: 5| Step: 5
Training loss: 2.5862742778068157
Validation loss: 2.4450765502000227

Epoch: 5| Step: 6
Training loss: 2.583649421140543
Validation loss: 2.4474963063974275

Epoch: 5| Step: 7
Training loss: 3.156395332842891
Validation loss: 2.4476444268856796

Epoch: 5| Step: 8
Training loss: 2.537864237984746
Validation loss: 2.4561534217115306

Epoch: 5| Step: 9
Training loss: 2.0891437249381593
Validation loss: 2.447591639645227

Epoch: 5| Step: 10
Training loss: 2.296139242442586
Validation loss: 2.4482337394602074

Epoch: 5| Step: 11
Training loss: 2.2566245362571813
Validation loss: 2.4445221286829866

Epoch: 166| Step: 0
Training loss: 2.479971864448428
Validation loss: 2.4458743009650905

Epoch: 5| Step: 1
Training loss: 2.772555444711655
Validation loss: 2.4469937119932545

Epoch: 5| Step: 2
Training loss: 2.5680361236099323
Validation loss: 2.445588016121505

Epoch: 5| Step: 3
Training loss: 2.5032225819591676
Validation loss: 2.4439788163258713

Epoch: 5| Step: 4
Training loss: 2.7810645738146405
Validation loss: 2.450467895898141

Epoch: 5| Step: 5
Training loss: 2.4928797415178625
Validation loss: 2.4486020258302625

Epoch: 5| Step: 6
Training loss: 2.1456658257840386
Validation loss: 2.44490293621532

Epoch: 5| Step: 7
Training loss: 2.467323180629838
Validation loss: 2.444385019546973

Epoch: 5| Step: 8
Training loss: 2.4066320338975857
Validation loss: 2.4448066123133905

Epoch: 5| Step: 9
Training loss: 2.445128705122296
Validation loss: 2.4413822366169295

Epoch: 5| Step: 10
Training loss: 2.5489285854842865
Validation loss: 2.4465669982949363

Epoch: 5| Step: 11
Training loss: 1.8288424787520692
Validation loss: 2.443184374451056

Epoch: 167| Step: 0
Training loss: 2.4904202979001098
Validation loss: 2.446774099745332

Epoch: 5| Step: 1
Training loss: 2.5317678039654035
Validation loss: 2.445888183396797

Epoch: 5| Step: 2
Training loss: 2.333921244896745
Validation loss: 2.44842542812907

Epoch: 5| Step: 3
Training loss: 2.60975360978936
Validation loss: 2.453967425597818

Epoch: 5| Step: 4
Training loss: 2.788209587067661
Validation loss: 2.445251724089152

Epoch: 5| Step: 5
Training loss: 2.143069131901177
Validation loss: 2.4478403214798417

Epoch: 5| Step: 6
Training loss: 2.2209434949547866
Validation loss: 2.446536512416734

Epoch: 5| Step: 7
Training loss: 2.932291485982362
Validation loss: 2.4477340765418143

Epoch: 5| Step: 8
Training loss: 2.414310775878278
Validation loss: 2.4515284791801966

Epoch: 5| Step: 9
Training loss: 2.177540915068669
Validation loss: 2.455840325022508

Epoch: 5| Step: 10
Training loss: 2.776384619927465
Validation loss: 2.457288197968673

Epoch: 5| Step: 11
Training loss: 2.5881885515418923
Validation loss: 2.444513775478678

Epoch: 168| Step: 0
Training loss: 2.0789447974380364
Validation loss: 2.4519796147499977

Epoch: 5| Step: 1
Training loss: 2.5297053769418323
Validation loss: 2.4511849188995822

Epoch: 5| Step: 2
Training loss: 2.650507205431617
Validation loss: 2.4457752391202168

Epoch: 5| Step: 3
Training loss: 2.5923183120731177
Validation loss: 2.447731008320466

Epoch: 5| Step: 4
Training loss: 1.7647898901783832
Validation loss: 2.4452922929897793

Epoch: 5| Step: 5
Training loss: 2.687835849802022
Validation loss: 2.4441980944630086

Epoch: 5| Step: 6
Training loss: 2.36426295548155
Validation loss: 2.442310721131543

Epoch: 5| Step: 7
Training loss: 2.711673076116407
Validation loss: 2.4417326564030306

Epoch: 5| Step: 8
Training loss: 2.748021194131959
Validation loss: 2.4341686012741843

Epoch: 5| Step: 9
Training loss: 2.6989709765247203
Validation loss: 2.438182156631226

Epoch: 5| Step: 10
Training loss: 2.6558415715983843
Validation loss: 2.446702450400497

Epoch: 5| Step: 11
Training loss: 1.5663351675443482
Validation loss: 2.4313320714832054

Epoch: 169| Step: 0
Training loss: 2.6557563771865524
Validation loss: 2.4432746024790584

Epoch: 5| Step: 1
Training loss: 2.4855910869660813
Validation loss: 2.441546990979124

Epoch: 5| Step: 2
Training loss: 2.7233785932586088
Validation loss: 2.44067633131608

Epoch: 5| Step: 3
Training loss: 2.6452010192633777
Validation loss: 2.4367523758535095

Epoch: 5| Step: 4
Training loss: 1.842852551854816
Validation loss: 2.438353018380278

Epoch: 5| Step: 5
Training loss: 2.730149934025962
Validation loss: 2.4528812279227457

Epoch: 5| Step: 6
Training loss: 2.808227005784188
Validation loss: 2.4534330842762246

Epoch: 5| Step: 7
Training loss: 2.3735855307507987
Validation loss: 2.4575032284748737

Epoch: 5| Step: 8
Training loss: 2.749758189580635
Validation loss: 2.4520150830362173

Epoch: 5| Step: 9
Training loss: 2.449381407670704
Validation loss: 2.4613541507722294

Epoch: 5| Step: 10
Training loss: 2.3427009778029557
Validation loss: 2.453023191635534

Epoch: 5| Step: 11
Training loss: 1.9138450654759405
Validation loss: 2.44981380293578

Epoch: 170| Step: 0
Training loss: 2.545051811455308
Validation loss: 2.4492026191111274

Epoch: 5| Step: 1
Training loss: 2.9237305975927064
Validation loss: 2.442186057101926

Epoch: 5| Step: 2
Training loss: 2.001466928384649
Validation loss: 2.439278084486956

Epoch: 5| Step: 3
Training loss: 2.214021937309
Validation loss: 2.433614515417014

Epoch: 5| Step: 4
Training loss: 2.364432062584303
Validation loss: 2.434202831335508

Epoch: 5| Step: 5
Training loss: 2.803362690215857
Validation loss: 2.439994778347421

Epoch: 5| Step: 6
Training loss: 2.1890303571956515
Validation loss: 2.4464612543616573

Epoch: 5| Step: 7
Training loss: 2.649793789595781
Validation loss: 2.455198857048132

Epoch: 5| Step: 8
Training loss: 2.621728948042103
Validation loss: 2.4507740766886235

Epoch: 5| Step: 9
Training loss: 3.1114259023203306
Validation loss: 2.4437461443234048

Epoch: 5| Step: 10
Training loss: 2.378763128776623
Validation loss: 2.4377673931665185

Epoch: 5| Step: 11
Training loss: 0.9565940325330544
Validation loss: 2.4412187590767385

Epoch: 171| Step: 0
Training loss: 3.169746473986616
Validation loss: 2.447958930983147

Epoch: 5| Step: 1
Training loss: 2.1794868841789587
Validation loss: 2.447239629308552

Epoch: 5| Step: 2
Training loss: 2.265878597404607
Validation loss: 2.4529391216310588

Epoch: 5| Step: 3
Training loss: 2.446575289673459
Validation loss: 2.4554238532445756

Epoch: 5| Step: 4
Training loss: 2.9140148823710863
Validation loss: 2.4529632223445934

Epoch: 5| Step: 5
Training loss: 2.454013542057095
Validation loss: 2.4483012296386573

Epoch: 5| Step: 6
Training loss: 2.3253063574321358
Validation loss: 2.451297905710107

Epoch: 5| Step: 7
Training loss: 2.472361566662246
Validation loss: 2.4442671673303753

Epoch: 5| Step: 8
Training loss: 2.715444901687221
Validation loss: 2.444227304862882

Epoch: 5| Step: 9
Training loss: 2.4697293620548093
Validation loss: 2.4435820016896255

Epoch: 5| Step: 10
Training loss: 1.8098883393130223
Validation loss: 2.4385926169757433

Epoch: 5| Step: 11
Training loss: 3.0541297032581523
Validation loss: 2.449003539701868

Epoch: 172| Step: 0
Training loss: 2.572999332556877
Validation loss: 2.4468778448628288

Epoch: 5| Step: 1
Training loss: 2.6857308175641994
Validation loss: 2.4413388174281185

Epoch: 5| Step: 2
Training loss: 2.7669143688433366
Validation loss: 2.4448810761503297

Epoch: 5| Step: 3
Training loss: 2.487166941213061
Validation loss: 2.4437632218090215

Epoch: 5| Step: 4
Training loss: 2.262709748045504
Validation loss: 2.441532146005262

Epoch: 5| Step: 5
Training loss: 2.447364896449005
Validation loss: 2.4405267072089134

Epoch: 5| Step: 6
Training loss: 2.299820445142407
Validation loss: 2.439551574170199

Epoch: 5| Step: 7
Training loss: 2.2585272796044076
Validation loss: 2.436127096696653

Epoch: 5| Step: 8
Training loss: 2.7486656593057526
Validation loss: 2.440022121476385

Epoch: 5| Step: 9
Training loss: 2.7250261323008282
Validation loss: 2.4428956272464784

Epoch: 5| Step: 10
Training loss: 2.34426650396011
Validation loss: 2.436891712191095

Epoch: 5| Step: 11
Training loss: 1.7725323623263933
Validation loss: 2.436070616084198

Epoch: 173| Step: 0
Training loss: 1.4757756957580184
Validation loss: 2.443934103916354

Epoch: 5| Step: 1
Training loss: 2.7785517430247313
Validation loss: 2.442511728883953

Epoch: 5| Step: 2
Training loss: 2.359285567653901
Validation loss: 2.4381202862187212

Epoch: 5| Step: 3
Training loss: 3.0493359139813356
Validation loss: 2.437322932112726

Epoch: 5| Step: 4
Training loss: 2.6178955031450286
Validation loss: 2.4362497383666875

Epoch: 5| Step: 5
Training loss: 2.9465470294341953
Validation loss: 2.440122448918193

Epoch: 5| Step: 6
Training loss: 2.432329023729169
Validation loss: 2.4410319577995008

Epoch: 5| Step: 7
Training loss: 2.587885005153096
Validation loss: 2.4480454282511346

Epoch: 5| Step: 8
Training loss: 1.9672406784032619
Validation loss: 2.455986417373905

Epoch: 5| Step: 9
Training loss: 2.6375254661451533
Validation loss: 2.453713109656832

Epoch: 5| Step: 10
Training loss: 2.475286979606343
Validation loss: 2.4489276009214267

Epoch: 5| Step: 11
Training loss: 2.0205960740751405
Validation loss: 2.448797724146633

Epoch: 174| Step: 0
Training loss: 2.8324538343123518
Validation loss: 2.452387429575803

Epoch: 5| Step: 1
Training loss: 2.3195244577988716
Validation loss: 2.452311407214001

Epoch: 5| Step: 2
Training loss: 2.4190233731601314
Validation loss: 2.461155127938877

Epoch: 5| Step: 3
Training loss: 2.337099736668973
Validation loss: 2.4614564540197748

Epoch: 5| Step: 4
Training loss: 2.728650799012743
Validation loss: 2.45693228016395

Epoch: 5| Step: 5
Training loss: 2.130052954145361
Validation loss: 2.4540790091815476

Epoch: 5| Step: 6
Training loss: 2.5880562668097733
Validation loss: 2.4486861189746296

Epoch: 5| Step: 7
Training loss: 2.3652034288141204
Validation loss: 2.44819572494925

Epoch: 5| Step: 8
Training loss: 2.410612239393424
Validation loss: 2.437872568821656

Epoch: 5| Step: 9
Training loss: 3.0892899857356864
Validation loss: 2.436716526410033

Epoch: 5| Step: 10
Training loss: 2.239736992347842
Validation loss: 2.450150539191328

Epoch: 5| Step: 11
Training loss: 2.560388765320149
Validation loss: 2.4493673422485163

Epoch: 175| Step: 0
Training loss: 2.5675529345262267
Validation loss: 2.455214056370609

Epoch: 5| Step: 1
Training loss: 2.840258456545035
Validation loss: 2.44483674187561

Epoch: 5| Step: 2
Training loss: 2.8062127091748623
Validation loss: 2.444123589495591

Epoch: 5| Step: 3
Training loss: 1.9257481941651464
Validation loss: 2.4422525714089254

Epoch: 5| Step: 4
Training loss: 2.0526589943268627
Validation loss: 2.447173971097264

Epoch: 5| Step: 5
Training loss: 2.275314342873687
Validation loss: 2.447871115781504

Epoch: 5| Step: 6
Training loss: 2.7723229116256394
Validation loss: 2.4555945873634575

Epoch: 5| Step: 7
Training loss: 2.7716449801234475
Validation loss: 2.4545879375388897

Epoch: 5| Step: 8
Training loss: 2.4797537186067182
Validation loss: 2.445036319009158

Epoch: 5| Step: 9
Training loss: 2.339207634407976
Validation loss: 2.44227467058175

Epoch: 5| Step: 10
Training loss: 2.4676273545751117
Validation loss: 2.4455866553334635

Epoch: 5| Step: 11
Training loss: 2.5156697334434965
Validation loss: 2.4473816767254246

Epoch: 176| Step: 0
Training loss: 2.0957699042467977
Validation loss: 2.4572312799730915

Epoch: 5| Step: 1
Training loss: 2.4789745245667465
Validation loss: 2.4677855482912547

Epoch: 5| Step: 2
Training loss: 2.871285402298106
Validation loss: 2.4712293906245693

Epoch: 5| Step: 3
Training loss: 2.6816494268507682
Validation loss: 2.475505394663076

Epoch: 5| Step: 4
Training loss: 2.7434324389231888
Validation loss: 2.4829575675706304

Epoch: 5| Step: 5
Training loss: 2.7968338265398587
Validation loss: 2.4870615654255483

Epoch: 5| Step: 6
Training loss: 2.045490757785414
Validation loss: 2.4927490701839146

Epoch: 5| Step: 7
Training loss: 2.7789058450436346
Validation loss: 2.486473073277747

Epoch: 5| Step: 8
Training loss: 2.267969956826399
Validation loss: 2.4863785875687796

Epoch: 5| Step: 9
Training loss: 2.7300378898813817
Validation loss: 2.48687778619107

Epoch: 5| Step: 10
Training loss: 2.6574148877528767
Validation loss: 2.488414373584582

Epoch: 5| Step: 11
Training loss: 2.082778424429972
Validation loss: 2.4827513317321865

Epoch: 177| Step: 0
Training loss: 3.0643546269124893
Validation loss: 2.483010727187455

Epoch: 5| Step: 1
Training loss: 2.462891012217456
Validation loss: 2.4739980246053634

Epoch: 5| Step: 2
Training loss: 2.413697545386888
Validation loss: 2.4690104319023876

Epoch: 5| Step: 3
Training loss: 2.7521025682730587
Validation loss: 2.4729929891076354

Epoch: 5| Step: 4
Training loss: 3.040587682387184
Validation loss: 2.473807164977871

Epoch: 5| Step: 5
Training loss: 2.656322523136795
Validation loss: 2.470673970287934

Epoch: 5| Step: 6
Training loss: 2.5315245491237413
Validation loss: 2.4648709786871397

Epoch: 5| Step: 7
Training loss: 2.2098724711634725
Validation loss: 2.4666264249337138

Epoch: 5| Step: 8
Training loss: 2.5008368998672323
Validation loss: 2.4563392105273807

Epoch: 5| Step: 9
Training loss: 1.8006348788338986
Validation loss: 2.4583133007030766

Epoch: 5| Step: 10
Training loss: 2.1638203289165485
Validation loss: 2.456712207791544

Epoch: 5| Step: 11
Training loss: 2.2870409895839448
Validation loss: 2.4562589142098092

Epoch: 178| Step: 0
Training loss: 2.5098182524273116
Validation loss: 2.455477095146858

Epoch: 5| Step: 1
Training loss: 2.29962564407226
Validation loss: 2.4569914045878294

Epoch: 5| Step: 2
Training loss: 2.2937339012973883
Validation loss: 2.48376072580997

Epoch: 5| Step: 3
Training loss: 2.7352661424541767
Validation loss: 2.506060755966141

Epoch: 5| Step: 4
Training loss: 3.121442830183191
Validation loss: 2.525876986978941

Epoch: 5| Step: 5
Training loss: 2.5693352879429527
Validation loss: 2.529876045676051

Epoch: 5| Step: 6
Training loss: 2.369932490485847
Validation loss: 2.474441378098887

Epoch: 5| Step: 7
Training loss: 2.322619796964721
Validation loss: 2.4556005059225625

Epoch: 5| Step: 8
Training loss: 2.4537113728060187
Validation loss: 2.451805766338237

Epoch: 5| Step: 9
Training loss: 2.3576494184535735
Validation loss: 2.447767376236449

Epoch: 5| Step: 10
Training loss: 2.402008876876246
Validation loss: 2.4533536889455823

Epoch: 5| Step: 11
Training loss: 4.260977983999325
Validation loss: 2.4652685090608633

Epoch: 179| Step: 0
Training loss: 2.876181649892564
Validation loss: 2.4577645368477583

Epoch: 5| Step: 1
Training loss: 2.5016229130194847
Validation loss: 2.460203838483725

Epoch: 5| Step: 2
Training loss: 2.393182520918573
Validation loss: 2.4635785417975877

Epoch: 5| Step: 3
Training loss: 2.004499143223949
Validation loss: 2.4670403028937304

Epoch: 5| Step: 4
Training loss: 2.1764625016988624
Validation loss: 2.4621553566316847

Epoch: 5| Step: 5
Training loss: 2.9031775856798943
Validation loss: 2.457252504608868

Epoch: 5| Step: 6
Training loss: 2.9053681378998486
Validation loss: 2.462127446297887

Epoch: 5| Step: 7
Training loss: 2.7153265434584224
Validation loss: 2.4621572549667134

Epoch: 5| Step: 8
Training loss: 3.02695924347483
Validation loss: 2.4556620977364494

Epoch: 5| Step: 9
Training loss: 1.7640700793715884
Validation loss: 2.461305334464654

Epoch: 5| Step: 10
Training loss: 1.8466298516708841
Validation loss: 2.467549374120863

Epoch: 5| Step: 11
Training loss: 2.6232019351622857
Validation loss: 2.45799851292642

Epoch: 180| Step: 0
Training loss: 2.1311723040516655
Validation loss: 2.4666707743361354

Epoch: 5| Step: 1
Training loss: 2.0856275006657357
Validation loss: 2.4655378304509044

Epoch: 5| Step: 2
Training loss: 2.5616459237298996
Validation loss: 2.460185231675794

Epoch: 5| Step: 3
Training loss: 2.931480083097973
Validation loss: 2.4653144020321127

Epoch: 5| Step: 4
Training loss: 2.4243702493548778
Validation loss: 2.461994093678959

Epoch: 5| Step: 5
Training loss: 2.340321181662338
Validation loss: 2.457777220385304

Epoch: 5| Step: 6
Training loss: 2.354267275750603
Validation loss: 2.4650991058673783

Epoch: 5| Step: 7
Training loss: 2.928149335794683
Validation loss: 2.4494489189506616

Epoch: 5| Step: 8
Training loss: 2.323671112937688
Validation loss: 2.4631084615040364

Epoch: 5| Step: 9
Training loss: 2.588867924250178
Validation loss: 2.452341626861

Epoch: 5| Step: 10
Training loss: 2.668137015849347
Validation loss: 2.453829075068212

Epoch: 5| Step: 11
Training loss: 2.6796412338844884
Validation loss: 2.454976203399125

Epoch: 181| Step: 0
Training loss: 2.261917975155281
Validation loss: 2.4533953143169422

Epoch: 5| Step: 1
Training loss: 2.4873653624629846
Validation loss: 2.454405407932135

Epoch: 5| Step: 2
Training loss: 2.963342821973521
Validation loss: 2.454800928248322

Epoch: 5| Step: 3
Training loss: 2.026984798575763
Validation loss: 2.4532703227027426

Epoch: 5| Step: 4
Training loss: 1.9112022983743189
Validation loss: 2.454969728960441

Epoch: 5| Step: 5
Training loss: 2.6689303049111537
Validation loss: 2.4531574388857837

Epoch: 5| Step: 6
Training loss: 2.8070818953028884
Validation loss: 2.4501725468566837

Epoch: 5| Step: 7
Training loss: 2.2610922886312093
Validation loss: 2.452662114492811

Epoch: 5| Step: 8
Training loss: 2.5896684636193963
Validation loss: 2.4546991104767635

Epoch: 5| Step: 9
Training loss: 2.465549760838127
Validation loss: 2.45012647568324

Epoch: 5| Step: 10
Training loss: 2.6759881378161805
Validation loss: 2.4551433836792156

Epoch: 5| Step: 11
Training loss: 3.5975918344701485
Validation loss: 2.454154916070247

Epoch: 182| Step: 0
Training loss: 2.612904337571704
Validation loss: 2.4538984921816627

Epoch: 5| Step: 1
Training loss: 2.383757357013393
Validation loss: 2.4518841320356892

Epoch: 5| Step: 2
Training loss: 2.523019383988399
Validation loss: 2.4542417778401213

Epoch: 5| Step: 3
Training loss: 2.3793336581422957
Validation loss: 2.457640644587977

Epoch: 5| Step: 4
Training loss: 2.0466509980379595
Validation loss: 2.456287718298052

Epoch: 5| Step: 5
Training loss: 2.58437832851726
Validation loss: 2.4573579499243587

Epoch: 5| Step: 6
Training loss: 2.1378334945274586
Validation loss: 2.4551397541962943

Epoch: 5| Step: 7
Training loss: 2.718704135551763
Validation loss: 2.453687481873216

Epoch: 5| Step: 8
Training loss: 2.441824378257062
Validation loss: 2.4504761132684227

Epoch: 5| Step: 9
Training loss: 2.3369622167782103
Validation loss: 2.4512253835240596

Epoch: 5| Step: 10
Training loss: 3.1562400288943673
Validation loss: 2.4551705621450175

Epoch: 5| Step: 11
Training loss: 1.9731691203653772
Validation loss: 2.4488627790087336

Epoch: 183| Step: 0
Training loss: 2.489004176834053
Validation loss: 2.453217717259095

Epoch: 5| Step: 1
Training loss: 1.9930430650781186
Validation loss: 2.457402393832018

Epoch: 5| Step: 2
Training loss: 2.823751366063193
Validation loss: 2.4563808864117713

Epoch: 5| Step: 3
Training loss: 2.7401670310350044
Validation loss: 2.4530299546929113

Epoch: 5| Step: 4
Training loss: 2.680791510659997
Validation loss: 2.4568491606969705

Epoch: 5| Step: 5
Training loss: 2.0811939953666254
Validation loss: 2.4473409435619256

Epoch: 5| Step: 6
Training loss: 2.6947876350088573
Validation loss: 2.453114165344933

Epoch: 5| Step: 7
Training loss: 2.779411994442543
Validation loss: 2.4449647934516996

Epoch: 5| Step: 8
Training loss: 2.4710665107646452
Validation loss: 2.4448547625489208

Epoch: 5| Step: 9
Training loss: 1.9299744303892155
Validation loss: 2.445135112174371

Epoch: 5| Step: 10
Training loss: 2.5565953961037056
Validation loss: 2.4484209893892617

Epoch: 5| Step: 11
Training loss: 2.1496109410841218
Validation loss: 2.4465947024336354

Epoch: 184| Step: 0
Training loss: 2.6134686367210365
Validation loss: 2.4526054577281635

Epoch: 5| Step: 1
Training loss: 2.437093064989902
Validation loss: 2.4541343629089423

Epoch: 5| Step: 2
Training loss: 2.4155471335194747
Validation loss: 2.451808576226228

Epoch: 5| Step: 3
Training loss: 2.5491087309617897
Validation loss: 2.446130999485885

Epoch: 5| Step: 4
Training loss: 2.3836942448369194
Validation loss: 2.449990154590417

Epoch: 5| Step: 5
Training loss: 1.9722435245646968
Validation loss: 2.4526605146092786

Epoch: 5| Step: 6
Training loss: 2.606279192790735
Validation loss: 2.4439036868160633

Epoch: 5| Step: 7
Training loss: 2.160813409272342
Validation loss: 2.4559115439552657

Epoch: 5| Step: 8
Training loss: 2.696270670097537
Validation loss: 2.451642197755461

Epoch: 5| Step: 9
Training loss: 2.6643732063131718
Validation loss: 2.450793212984652

Epoch: 5| Step: 10
Training loss: 2.7696584825653843
Validation loss: 2.4523150975956804

Epoch: 5| Step: 11
Training loss: 2.7077449942052887
Validation loss: 2.451959470787586

Epoch: 185| Step: 0
Training loss: 2.112974133977256
Validation loss: 2.4492929985801988

Epoch: 5| Step: 1
Training loss: 2.740959740338544
Validation loss: 2.4567283884414297

Epoch: 5| Step: 2
Training loss: 2.8566608158557614
Validation loss: 2.45671726236062

Epoch: 5| Step: 3
Training loss: 2.150837947875488
Validation loss: 2.4549421517069168

Epoch: 5| Step: 4
Training loss: 2.285493639956955
Validation loss: 2.453722344500299

Epoch: 5| Step: 5
Training loss: 2.8474321944391776
Validation loss: 2.45371721898428

Epoch: 5| Step: 6
Training loss: 2.299160294044054
Validation loss: 2.454339392973409

Epoch: 5| Step: 7
Training loss: 2.8979233442531704
Validation loss: 2.453627014627818

Epoch: 5| Step: 8
Training loss: 2.5792551973480395
Validation loss: 2.4560975937479865

Epoch: 5| Step: 9
Training loss: 2.4051047237698
Validation loss: 2.453710530696091

Epoch: 5| Step: 10
Training loss: 2.0842256415299163
Validation loss: 2.450945250228852

Epoch: 5| Step: 11
Training loss: 1.691318712325923
Validation loss: 2.459367164266497

Epoch: 186| Step: 0
Training loss: 2.150301260135049
Validation loss: 2.4578759298609465

Epoch: 5| Step: 1
Training loss: 2.782042144094175
Validation loss: 2.455242328519042

Epoch: 5| Step: 2
Training loss: 2.4767644661461374
Validation loss: 2.4589305545571856

Epoch: 5| Step: 3
Training loss: 2.0270156153795846
Validation loss: 2.450560348846167

Epoch: 5| Step: 4
Training loss: 2.60939162928599
Validation loss: 2.455592588885733

Epoch: 5| Step: 5
Training loss: 2.5890022855581343
Validation loss: 2.4553341849603796

Epoch: 5| Step: 6
Training loss: 2.5932155196323747
Validation loss: 2.4519384636834896

Epoch: 5| Step: 7
Training loss: 2.516141944987007
Validation loss: 2.450963382035488

Epoch: 5| Step: 8
Training loss: 2.3717248318747064
Validation loss: 2.449839276601799

Epoch: 5| Step: 9
Training loss: 2.336183873072955
Validation loss: 2.45377955441094

Epoch: 5| Step: 10
Training loss: 2.6362984329339945
Validation loss: 2.4514959923998476

Epoch: 5| Step: 11
Training loss: 3.211217863368293
Validation loss: 2.453816879201187

Epoch: 187| Step: 0
Training loss: 2.8445416700449426
Validation loss: 2.4571074012555245

Epoch: 5| Step: 1
Training loss: 2.008996043207536
Validation loss: 2.463578505506085

Epoch: 5| Step: 2
Training loss: 2.116831056213636
Validation loss: 2.463367027856463

Epoch: 5| Step: 3
Training loss: 2.146087693135394
Validation loss: 2.4585585706513444

Epoch: 5| Step: 4
Training loss: 2.8432935411263367
Validation loss: 2.46422211173021

Epoch: 5| Step: 5
Training loss: 2.740699473014032
Validation loss: 2.458764159037851

Epoch: 5| Step: 6
Training loss: 2.748571978776592
Validation loss: 2.4612427052960806

Epoch: 5| Step: 7
Training loss: 2.348062641939348
Validation loss: 2.464165603844735

Epoch: 5| Step: 8
Training loss: 2.943782176107161
Validation loss: 2.455908271566115

Epoch: 5| Step: 9
Training loss: 2.2286708553722536
Validation loss: 2.4561843140144957

Epoch: 5| Step: 10
Training loss: 2.3565398101260637
Validation loss: 2.4505271398048993

Epoch: 5| Step: 11
Training loss: 1.2099123152968947
Validation loss: 2.45113072253127

Epoch: 188| Step: 0
Training loss: 2.7764826860264127
Validation loss: 2.444358741246454

Epoch: 5| Step: 1
Training loss: 2.229157658734051
Validation loss: 2.4545207092614434

Epoch: 5| Step: 2
Training loss: 1.9974598250568307
Validation loss: 2.456043014346949

Epoch: 5| Step: 3
Training loss: 3.0148883769816717
Validation loss: 2.450693144028803

Epoch: 5| Step: 4
Training loss: 2.541574591131001
Validation loss: 2.4604969296349988

Epoch: 5| Step: 5
Training loss: 2.5736109199759456
Validation loss: 2.460415012661612

Epoch: 5| Step: 6
Training loss: 2.7839858068893646
Validation loss: 2.459953664476386

Epoch: 5| Step: 7
Training loss: 2.1161900947505545
Validation loss: 2.4590751786720575

Epoch: 5| Step: 8
Training loss: 2.682112594602115
Validation loss: 2.4604873931844486

Epoch: 5| Step: 9
Training loss: 1.8830475086736063
Validation loss: 2.4579921717367763

Epoch: 5| Step: 10
Training loss: 2.3267876797259905
Validation loss: 2.4605060905681415

Epoch: 5| Step: 11
Training loss: 2.579828797507828
Validation loss: 2.4585842971611953

Epoch: 189| Step: 0
Training loss: 1.6881742190177242
Validation loss: 2.459948180416598

Epoch: 5| Step: 1
Training loss: 2.604022080540248
Validation loss: 2.457662290108576

Epoch: 5| Step: 2
Training loss: 2.960479620403543
Validation loss: 2.458681648977357

Epoch: 5| Step: 3
Training loss: 2.173127615891145
Validation loss: 2.461824005146297

Epoch: 5| Step: 4
Training loss: 2.3254133984965923
Validation loss: 2.4616131455507557

Epoch: 5| Step: 5
Training loss: 2.509618946346113
Validation loss: 2.459089527906706

Epoch: 5| Step: 6
Training loss: 2.6591484860912793
Validation loss: 2.4508330335122914

Epoch: 5| Step: 7
Training loss: 2.4565556343311683
Validation loss: 2.4627796483883118

Epoch: 5| Step: 8
Training loss: 2.8603173876095163
Validation loss: 2.4618846342271614

Epoch: 5| Step: 9
Training loss: 2.0846733234365518
Validation loss: 2.4638651876684645

Epoch: 5| Step: 10
Training loss: 2.7402332439134782
Validation loss: 2.4604067113956494

Epoch: 5| Step: 11
Training loss: 2.841536614209822
Validation loss: 2.4598703522943834

Epoch: 190| Step: 0
Training loss: 2.9920906071126714
Validation loss: 2.4656115070226843

Epoch: 5| Step: 1
Training loss: 2.4659549004111603
Validation loss: 2.472315278129522

Epoch: 5| Step: 2
Training loss: 2.4052259569828958
Validation loss: 2.4705557275362238

Epoch: 5| Step: 3
Training loss: 2.6784136444122537
Validation loss: 2.470510181236172

Epoch: 5| Step: 4
Training loss: 2.4160315128380696
Validation loss: 2.4694283716305443

Epoch: 5| Step: 5
Training loss: 2.001502188161759
Validation loss: 2.4735452179899458

Epoch: 5| Step: 6
Training loss: 2.180287924484926
Validation loss: 2.4815790609990387

Epoch: 5| Step: 7
Training loss: 2.128745536687521
Validation loss: 2.4729332349415554

Epoch: 5| Step: 8
Training loss: 2.74191569750051
Validation loss: 2.4702400748638333

Epoch: 5| Step: 9
Training loss: 2.6270973364517065
Validation loss: 2.4659917770546684

Epoch: 5| Step: 10
Training loss: 2.6600155338034557
Validation loss: 2.4688303487715846

Epoch: 5| Step: 11
Training loss: 2.7069570027527585
Validation loss: 2.4729417110825715

Epoch: 191| Step: 0
Training loss: 2.508805028048654
Validation loss: 2.46409461339756

Epoch: 5| Step: 1
Training loss: 2.191889364534315
Validation loss: 2.461282776523442

Epoch: 5| Step: 2
Training loss: 2.498657915366015
Validation loss: 2.4591250976501464

Epoch: 5| Step: 3
Training loss: 2.7171237510323434
Validation loss: 2.4600528923476297

Epoch: 5| Step: 4
Training loss: 2.8262243470314625
Validation loss: 2.4683418942594293

Epoch: 5| Step: 5
Training loss: 2.6869865636741617
Validation loss: 2.469365674922261

Epoch: 5| Step: 6
Training loss: 2.4703968205016436
Validation loss: 2.4708609871069984

Epoch: 5| Step: 7
Training loss: 1.7795181555554689
Validation loss: 2.4753838069742216

Epoch: 5| Step: 8
Training loss: 2.2003732364671094
Validation loss: 2.4652323388820783

Epoch: 5| Step: 9
Training loss: 3.126473346527658
Validation loss: 2.466369378425814

Epoch: 5| Step: 10
Training loss: 1.8043563340757318
Validation loss: 2.472744892413726

Epoch: 5| Step: 11
Training loss: 2.9983277428493436
Validation loss: 2.4632764892661174

Epoch: 192| Step: 0
Training loss: 2.5767562469498486
Validation loss: 2.4617617806035597

Epoch: 5| Step: 1
Training loss: 2.306442781986926
Validation loss: 2.4639471310490197

Epoch: 5| Step: 2
Training loss: 1.9784629641544988
Validation loss: 2.461511094997842

Epoch: 5| Step: 3
Training loss: 2.7189957957037088
Validation loss: 2.4581903857896146

Epoch: 5| Step: 4
Training loss: 1.9949145631971754
Validation loss: 2.4656578247115823

Epoch: 5| Step: 5
Training loss: 2.712136654755771
Validation loss: 2.4637440698450037

Epoch: 5| Step: 6
Training loss: 2.163822312229303
Validation loss: 2.458903959018338

Epoch: 5| Step: 7
Training loss: 2.574126315736599
Validation loss: 2.4560769779718172

Epoch: 5| Step: 8
Training loss: 2.651261975640386
Validation loss: 2.464920788350481

Epoch: 5| Step: 9
Training loss: 2.810436084888545
Validation loss: 2.4590367076052932

Epoch: 5| Step: 10
Training loss: 2.6807867970536794
Validation loss: 2.455180366064425

Epoch: 5| Step: 11
Training loss: 1.9726406587796608
Validation loss: 2.4620128078649186

Epoch: 193| Step: 0
Training loss: 3.0388663652768186
Validation loss: 2.4581917214150577

Epoch: 5| Step: 1
Training loss: 2.4943499615434592
Validation loss: 2.45911286543299

Epoch: 5| Step: 2
Training loss: 2.530155651747598
Validation loss: 2.4650873586754773

Epoch: 5| Step: 3
Training loss: 2.3791900615800103
Validation loss: 2.4543230488363212

Epoch: 5| Step: 4
Training loss: 2.317218194347974
Validation loss: 2.4606567747470534

Epoch: 5| Step: 5
Training loss: 1.8857399117185905
Validation loss: 2.4626896952971613

Epoch: 5| Step: 6
Training loss: 2.420293673201369
Validation loss: 2.4648019149301104

Epoch: 5| Step: 7
Training loss: 2.8126786069318928
Validation loss: 2.4582395629994886

Epoch: 5| Step: 8
Training loss: 2.513898932599072
Validation loss: 2.458472770572711

Epoch: 5| Step: 9
Training loss: 2.5896372532978846
Validation loss: 2.4543689036198493

Epoch: 5| Step: 10
Training loss: 2.0198369929195543
Validation loss: 2.4573469419085305

Epoch: 5| Step: 11
Training loss: 3.571529256218874
Validation loss: 2.464584582810288

Epoch: 194| Step: 0
Training loss: 2.7846513159379542
Validation loss: 2.451147254121087

Epoch: 5| Step: 1
Training loss: 2.3514824026667243
Validation loss: 2.456728004296285

Epoch: 5| Step: 2
Training loss: 2.581904652041331
Validation loss: 2.4656562171445753

Epoch: 5| Step: 3
Training loss: 2.172558354514781
Validation loss: 2.4579879685172896

Epoch: 5| Step: 4
Training loss: 2.3080164773376004
Validation loss: 2.4629854753236105

Epoch: 5| Step: 5
Training loss: 2.6610550447352344
Validation loss: 2.4558656289840295

Epoch: 5| Step: 6
Training loss: 2.13122219839449
Validation loss: 2.4633607629940237

Epoch: 5| Step: 7
Training loss: 2.694269393093615
Validation loss: 2.4584235899461278

Epoch: 5| Step: 8
Training loss: 2.197136606455215
Validation loss: 2.4508558821935233

Epoch: 5| Step: 9
Training loss: 2.584059941231377
Validation loss: 2.45992184222125

Epoch: 5| Step: 10
Training loss: 2.479966192322685
Validation loss: 2.4541421105905514

Epoch: 5| Step: 11
Training loss: 2.970742129459178
Validation loss: 2.4660727474308692

Epoch: 195| Step: 0
Training loss: 2.6650723419963165
Validation loss: 2.4567630987400766

Epoch: 5| Step: 1
Training loss: 2.0946780752692455
Validation loss: 2.4539944106325655

Epoch: 5| Step: 2
Training loss: 2.275443015088928
Validation loss: 2.453139290899616

Epoch: 5| Step: 3
Training loss: 2.6714369208378064
Validation loss: 2.456735096819066

Epoch: 5| Step: 4
Training loss: 2.4306197263935583
Validation loss: 2.454215348046395

Epoch: 5| Step: 5
Training loss: 2.688646205173797
Validation loss: 2.456780558837722

Epoch: 5| Step: 6
Training loss: 2.6906776272437396
Validation loss: 2.4545504322306577

Epoch: 5| Step: 7
Training loss: 2.6266339076205245
Validation loss: 2.453010568561795

Epoch: 5| Step: 8
Training loss: 2.088997985254524
Validation loss: 2.45555490473508

Epoch: 5| Step: 9
Training loss: 2.5883618195737434
Validation loss: 2.460374559876058

Epoch: 5| Step: 10
Training loss: 2.3696635676265463
Validation loss: 2.4577114212970868

Epoch: 5| Step: 11
Training loss: 0.5610279584112496
Validation loss: 2.4573858234714296

Epoch: 196| Step: 0
Training loss: 2.5443799487754317
Validation loss: 2.461306722887275

Epoch: 5| Step: 1
Training loss: 2.5457082310816044
Validation loss: 2.469349707825774

Epoch: 5| Step: 2
Training loss: 2.0251285747635213
Validation loss: 2.458271706072669

Epoch: 5| Step: 3
Training loss: 2.3854864407347702
Validation loss: 2.470151266152153

Epoch: 5| Step: 4
Training loss: 2.6368304193656336
Validation loss: 2.4585580655734605

Epoch: 5| Step: 5
Training loss: 2.613495822188675
Validation loss: 2.465742186262337

Epoch: 5| Step: 6
Training loss: 2.4830566363041817
Validation loss: 2.469688695796043

Epoch: 5| Step: 7
Training loss: 2.9119254859466386
Validation loss: 2.4647401444333226

Epoch: 5| Step: 8
Training loss: 2.3597630061750903
Validation loss: 2.4597463641665325

Epoch: 5| Step: 9
Training loss: 2.309846128483035
Validation loss: 2.4576327543339898

Epoch: 5| Step: 10
Training loss: 2.1901827029655276
Validation loss: 2.4601416416083777

Epoch: 5| Step: 11
Training loss: 2.1850157664273575
Validation loss: 2.459918841700575

Epoch: 197| Step: 0
Training loss: 2.5780859741956315
Validation loss: 2.45749318319509

Epoch: 5| Step: 1
Training loss: 2.2829861240040463
Validation loss: 2.458897252517738

Epoch: 5| Step: 2
Training loss: 2.186587879207537
Validation loss: 2.4613782942053755

Epoch: 5| Step: 3
Training loss: 2.9755575731438553
Validation loss: 2.458679774221582

Epoch: 5| Step: 4
Training loss: 2.595764113514133
Validation loss: 2.4545608396367746

Epoch: 5| Step: 5
Training loss: 2.5609610053165497
Validation loss: 2.459076253251652

Epoch: 5| Step: 6
Training loss: 2.7634636143372893
Validation loss: 2.4645656785523413

Epoch: 5| Step: 7
Training loss: 2.077170475464897
Validation loss: 2.46292902780149

Epoch: 5| Step: 8
Training loss: 1.9811469427315225
Validation loss: 2.4655674165771146

Epoch: 5| Step: 9
Training loss: 2.4460268774793876
Validation loss: 2.460463442884016

Epoch: 5| Step: 10
Training loss: 2.747461968419912
Validation loss: 2.4683466674393872

Epoch: 5| Step: 11
Training loss: 1.3200557391362588
Validation loss: 2.467231637673853

Epoch: 198| Step: 0
Training loss: 2.245364500331598
Validation loss: 2.4673606689380674

Epoch: 5| Step: 1
Training loss: 2.29234999670322
Validation loss: 2.459888274949643

Epoch: 5| Step: 2
Training loss: 2.8442213066151196
Validation loss: 2.464313909589495

Epoch: 5| Step: 3
Training loss: 2.4152207049816323
Validation loss: 2.461418100890048

Epoch: 5| Step: 4
Training loss: 2.797762255882707
Validation loss: 2.459733408074338

Epoch: 5| Step: 5
Training loss: 1.8748652091849975
Validation loss: 2.4566989344436387

Epoch: 5| Step: 6
Training loss: 2.5232867027553088
Validation loss: 2.4577992769739674

Epoch: 5| Step: 7
Training loss: 2.3187638069009107
Validation loss: 2.4588341761765395

Epoch: 5| Step: 8
Training loss: 2.271393896319784
Validation loss: 2.457054000611697

Epoch: 5| Step: 9
Training loss: 2.6818478609097425
Validation loss: 2.4701538158830005

Epoch: 5| Step: 10
Training loss: 2.246889189143469
Validation loss: 2.465302998379452

Epoch: 5| Step: 11
Training loss: 4.562318981185334
Validation loss: 2.4664090844179256

Epoch: 199| Step: 0
Training loss: 2.1060311268809295
Validation loss: 2.464521946280928

Epoch: 5| Step: 1
Training loss: 3.1094555101562578
Validation loss: 2.4740623545946767

Epoch: 5| Step: 2
Training loss: 2.710463424697617
Validation loss: 2.4744431827000044

Epoch: 5| Step: 3
Training loss: 2.663730813238821
Validation loss: 2.4829063032902443

Epoch: 5| Step: 4
Training loss: 2.6142961159409106
Validation loss: 2.496122404838457

Epoch: 5| Step: 5
Training loss: 2.74386762571089
Validation loss: 2.4885386319392784

Epoch: 5| Step: 6
Training loss: 1.8562938903346369
Validation loss: 2.466942157428832

Epoch: 5| Step: 7
Training loss: 2.6135210916482916
Validation loss: 2.466815069498481

Epoch: 5| Step: 8
Training loss: 2.255294187349065
Validation loss: 2.4728404072446977

Epoch: 5| Step: 9
Training loss: 2.174603840495584
Validation loss: 2.4666069483186606

Epoch: 5| Step: 10
Training loss: 2.1456127115059416
Validation loss: 2.4627564706549236

Epoch: 5| Step: 11
Training loss: 2.7956241815584155
Validation loss: 2.4704595191890397

Epoch: 200| Step: 0
Training loss: 1.988300136034291
Validation loss: 2.462479882069365

Epoch: 5| Step: 1
Training loss: 2.0338403470933013
Validation loss: 2.464259006241651

Epoch: 5| Step: 2
Training loss: 2.45543556979874
Validation loss: 2.460412650679741

Epoch: 5| Step: 3
Training loss: 2.5482587741134517
Validation loss: 2.4608921349597837

Epoch: 5| Step: 4
Training loss: 2.4342935453016477
Validation loss: 2.4646756154634724

Epoch: 5| Step: 5
Training loss: 2.697919651653352
Validation loss: 2.4665619775318928

Epoch: 5| Step: 6
Training loss: 2.734468730954894
Validation loss: 2.462657702668045

Epoch: 5| Step: 7
Training loss: 2.801144457985988
Validation loss: 2.461146342786232

Epoch: 5| Step: 8
Training loss: 2.2291100634462344
Validation loss: 2.4631000846190525

Epoch: 5| Step: 9
Training loss: 3.0396800063519227
Validation loss: 2.458211662762195

Epoch: 5| Step: 10
Training loss: 2.27405793942708
Validation loss: 2.456608059113724

Epoch: 5| Step: 11
Training loss: 1.727980156841963
Validation loss: 2.456908966401557

Epoch: 201| Step: 0
Training loss: 2.2837555594708068
Validation loss: 2.4756757951889767

Epoch: 5| Step: 1
Training loss: 2.1099494434464883
Validation loss: 2.4727956562809563

Epoch: 5| Step: 2
Training loss: 2.2834884948173344
Validation loss: 2.490962019182994

Epoch: 5| Step: 3
Training loss: 3.401531486036913
Validation loss: 2.481643730876974

Epoch: 5| Step: 4
Training loss: 2.7765598827343285
Validation loss: 2.484184189832432

Epoch: 5| Step: 5
Training loss: 2.5346734714319723
Validation loss: 2.467717671138497

Epoch: 5| Step: 6
Training loss: 2.4503941977870296
Validation loss: 2.465909621632586

Epoch: 5| Step: 7
Training loss: 2.5183002155515477
Validation loss: 2.465565369774966

Epoch: 5| Step: 8
Training loss: 2.443027977001314
Validation loss: 2.4582425817344524

Epoch: 5| Step: 9
Training loss: 1.8538205827482983
Validation loss: 2.458771615372146

Epoch: 5| Step: 10
Training loss: 2.1324925444675
Validation loss: 2.4597958272980245

Epoch: 5| Step: 11
Training loss: 2.9510885008490937
Validation loss: 2.4574694401945214

Epoch: 202| Step: 0
Training loss: 2.6227419996291386
Validation loss: 2.4590785236024835

Epoch: 5| Step: 1
Training loss: 2.4090752797413146
Validation loss: 2.4598999742780943

Epoch: 5| Step: 2
Training loss: 2.389184954382213
Validation loss: 2.4579296198135694

Epoch: 5| Step: 3
Training loss: 2.1491939808102196
Validation loss: 2.4661166354864172

Epoch: 5| Step: 4
Training loss: 2.6070767635022674
Validation loss: 2.4581925114745475

Epoch: 5| Step: 5
Training loss: 2.535432821037926
Validation loss: 2.476524585389529

Epoch: 5| Step: 6
Training loss: 2.4629465772911785
Validation loss: 2.4799109044401333

Epoch: 5| Step: 7
Training loss: 2.3777190254158422
Validation loss: 2.4724706828234213

Epoch: 5| Step: 8
Training loss: 2.4846472441058554
Validation loss: 2.4770337527879316

Epoch: 5| Step: 9
Training loss: 2.6409562484370026
Validation loss: 2.4745531043138445

Epoch: 5| Step: 10
Training loss: 2.413760663270238
Validation loss: 2.467207692445985

Epoch: 5| Step: 11
Training loss: 2.35562239568075
Validation loss: 2.4675916135039966

Epoch: 203| Step: 0
Training loss: 2.598874556440446
Validation loss: 2.4667789944856224

Epoch: 5| Step: 1
Training loss: 2.6457782549394238
Validation loss: 2.466761448098107

Epoch: 5| Step: 2
Training loss: 2.0261797494259097
Validation loss: 2.456685723701416

Epoch: 5| Step: 3
Training loss: 2.9319950222477935
Validation loss: 2.456675545679849

Epoch: 5| Step: 4
Training loss: 3.126447723737388
Validation loss: 2.460294528506145

Epoch: 5| Step: 5
Training loss: 1.553934192741866
Validation loss: 2.463811723828477

Epoch: 5| Step: 6
Training loss: 2.277315901885371
Validation loss: 2.466100482194713

Epoch: 5| Step: 7
Training loss: 2.686794188323046
Validation loss: 2.4686376529996266

Epoch: 5| Step: 8
Training loss: 2.8545170775407316
Validation loss: 2.4681775600374904

Epoch: 5| Step: 9
Training loss: 1.5678809779546738
Validation loss: 2.4608772108978423

Epoch: 5| Step: 10
Training loss: 2.3772217246939618
Validation loss: 2.463906601187261

Epoch: 5| Step: 11
Training loss: 2.0349997315652657
Validation loss: 2.4630902073598584

Epoch: 204| Step: 0
Training loss: 2.4523035119660506
Validation loss: 2.467326494247209

Epoch: 5| Step: 1
Training loss: 2.1026529085527206
Validation loss: 2.4572425108603464

Epoch: 5| Step: 2
Training loss: 2.904677098723981
Validation loss: 2.4653439807658915

Epoch: 5| Step: 3
Training loss: 2.241223277471475
Validation loss: 2.468148971181036

Epoch: 5| Step: 4
Training loss: 2.815564329994591
Validation loss: 2.464188740104326

Epoch: 5| Step: 5
Training loss: 2.08440236955829
Validation loss: 2.468222119032251

Epoch: 5| Step: 6
Training loss: 2.7961192894893965
Validation loss: 2.4647932898852187

Epoch: 5| Step: 7
Training loss: 2.4933235664312376
Validation loss: 2.46592748226525

Epoch: 5| Step: 8
Training loss: 2.5344015211794364
Validation loss: 2.4614833468633823

Epoch: 5| Step: 9
Training loss: 2.32559753040882
Validation loss: 2.4675352371318944

Epoch: 5| Step: 10
Training loss: 2.1747540389673685
Validation loss: 2.4665796703277616

Epoch: 5| Step: 11
Training loss: 1.6974079173398187
Validation loss: 2.4780240805217635

Epoch: 205| Step: 0
Training loss: 2.264125143359996
Validation loss: 2.4724581992174395

Epoch: 5| Step: 1
Training loss: 2.1494274840917282
Validation loss: 2.475302043540023

Epoch: 5| Step: 2
Training loss: 2.2340928546456413
Validation loss: 2.463747138285092

Epoch: 5| Step: 3
Training loss: 2.429611940834263
Validation loss: 2.4692016582754013

Epoch: 5| Step: 4
Training loss: 2.0390302282826966
Validation loss: 2.4737166771902035

Epoch: 5| Step: 5
Training loss: 2.281785980385633
Validation loss: 2.4562206457453146

Epoch: 5| Step: 6
Training loss: 2.918900061595134
Validation loss: 2.472542875160528

Epoch: 5| Step: 7
Training loss: 2.9629166965934344
Validation loss: 2.4731168915489787

Epoch: 5| Step: 8
Training loss: 2.348372922860988
Validation loss: 2.4734299881918047

Epoch: 5| Step: 9
Training loss: 2.509069680229667
Validation loss: 2.466090057021594

Epoch: 5| Step: 10
Training loss: 2.722605547439706
Validation loss: 2.473818589664026

Epoch: 5| Step: 11
Training loss: 1.7852853941165947
Validation loss: 2.4716659459104235

Epoch: 206| Step: 0
Training loss: 2.488518672138023
Validation loss: 2.473947968042979

Epoch: 5| Step: 1
Training loss: 2.7618502271291803
Validation loss: 2.4676766698010084

Epoch: 5| Step: 2
Training loss: 2.769378356587195
Validation loss: 2.4702373321890954

Epoch: 5| Step: 3
Training loss: 2.236546987959745
Validation loss: 2.4733637741131522

Epoch: 5| Step: 4
Training loss: 2.7942699042487873
Validation loss: 2.468139560895111

Epoch: 5| Step: 5
Training loss: 2.2329605000099066
Validation loss: 2.470653055962818

Epoch: 5| Step: 6
Training loss: 2.843386280987463
Validation loss: 2.479516502883156

Epoch: 5| Step: 7
Training loss: 2.050015086606965
Validation loss: 2.4712059182876143

Epoch: 5| Step: 8
Training loss: 1.7539559338165784
Validation loss: 2.467741438317663

Epoch: 5| Step: 9
Training loss: 2.532074782136488
Validation loss: 2.4731892417589303

Epoch: 5| Step: 10
Training loss: 1.8798306227536326
Validation loss: 2.4699302298747257

Epoch: 5| Step: 11
Training loss: 3.633159268143938
Validation loss: 2.469770427860843

Epoch: 207| Step: 0
Training loss: 2.581216519117855
Validation loss: 2.4649266321239662

Epoch: 5| Step: 1
Training loss: 2.8055243558586187
Validation loss: 2.4754966584308087

Epoch: 5| Step: 2
Training loss: 2.043688674613302
Validation loss: 2.472272982832267

Epoch: 5| Step: 3
Training loss: 2.4645839257990647
Validation loss: 2.4674263314919296

Epoch: 5| Step: 4
Training loss: 2.790138870392773
Validation loss: 2.472016507393257

Epoch: 5| Step: 5
Training loss: 2.171232608776027
Validation loss: 2.479400452775753

Epoch: 5| Step: 6
Training loss: 2.5379589323504357
Validation loss: 2.48540560620499

Epoch: 5| Step: 7
Training loss: 1.7528471945901039
Validation loss: 2.471910891652106

Epoch: 5| Step: 8
Training loss: 2.737495951889041
Validation loss: 2.4843259362708165

Epoch: 5| Step: 9
Training loss: 2.5791108067327704
Validation loss: 2.4835397127874095

Epoch: 5| Step: 10
Training loss: 2.039727230535444
Validation loss: 2.4651252396423873

Epoch: 5| Step: 11
Training loss: 3.046520036414563
Validation loss: 2.474844744979331

Epoch: 208| Step: 0
Training loss: 2.4885012351246276
Validation loss: 2.4816877958983645

Epoch: 5| Step: 1
Training loss: 2.4717595552090197
Validation loss: 2.4783883769800714

Epoch: 5| Step: 2
Training loss: 2.569349670953414
Validation loss: 2.490394291947696

Epoch: 5| Step: 3
Training loss: 2.4119678285898307
Validation loss: 2.476896353761203

Epoch: 5| Step: 4
Training loss: 2.041272365356439
Validation loss: 2.488308549588499

Epoch: 5| Step: 5
Training loss: 2.366483678146098
Validation loss: 2.466968968324205

Epoch: 5| Step: 6
Training loss: 2.391620802578029
Validation loss: 2.472531147263412

Epoch: 5| Step: 7
Training loss: 2.425792061356242
Validation loss: 2.4785010996483847

Epoch: 5| Step: 8
Training loss: 2.1133155467411053
Validation loss: 2.4680663723590213

Epoch: 5| Step: 9
Training loss: 2.4068030180131257
Validation loss: 2.4733574522347075

Epoch: 5| Step: 10
Training loss: 2.9109652425249735
Validation loss: 2.475621779845109

Epoch: 5| Step: 11
Training loss: 3.3334238199032153
Validation loss: 2.4619067912112267

Epoch: 209| Step: 0
Training loss: 1.9581362713082406
Validation loss: 2.4628342317744454

Epoch: 5| Step: 1
Training loss: 2.722394758279862
Validation loss: 2.473332626455348

Epoch: 5| Step: 2
Training loss: 2.6462726578713243
Validation loss: 2.4767046706089237

Epoch: 5| Step: 3
Training loss: 2.1637437496172156
Validation loss: 2.4647277425881895

Epoch: 5| Step: 4
Training loss: 2.5770101275232076
Validation loss: 2.479371031718007

Epoch: 5| Step: 5
Training loss: 2.7415135960232218
Validation loss: 2.4684912187817907

Epoch: 5| Step: 6
Training loss: 1.833255188172377
Validation loss: 2.4667878562297245

Epoch: 5| Step: 7
Training loss: 2.487381944778281
Validation loss: 2.4555679678303552

Epoch: 5| Step: 8
Training loss: 2.3152875852640915
Validation loss: 2.4671292094970276

Epoch: 5| Step: 9
Training loss: 2.8240466982192673
Validation loss: 2.461076500413426

Epoch: 5| Step: 10
Training loss: 2.528588202330809
Validation loss: 2.464465918905551

Epoch: 5| Step: 11
Training loss: 1.3366955842642165
Validation loss: 2.4758549310231586

Epoch: 210| Step: 0
Training loss: 2.71667137925689
Validation loss: 2.4877232435424026

Epoch: 5| Step: 1
Training loss: 2.150088809640956
Validation loss: 2.4842509832557806

Epoch: 5| Step: 2
Training loss: 2.3776028073583078
Validation loss: 2.469764589514954

Epoch: 5| Step: 3
Training loss: 2.3647936308747908
Validation loss: 2.471774196538604

Epoch: 5| Step: 4
Training loss: 2.238978731620878
Validation loss: 2.473612479412346

Epoch: 5| Step: 5
Training loss: 2.88315950284962
Validation loss: 2.463563242865525

Epoch: 5| Step: 6
Training loss: 2.6120579797347245
Validation loss: 2.479942021551032

Epoch: 5| Step: 7
Training loss: 2.2366625406688643
Validation loss: 2.4663546163982253

Epoch: 5| Step: 8
Training loss: 2.411507349909893
Validation loss: 2.477376172428902

Epoch: 5| Step: 9
Training loss: 2.3845165725615236
Validation loss: 2.4669323760909148

Epoch: 5| Step: 10
Training loss: 2.266825916735974
Validation loss: 2.4681018388926406

Epoch: 5| Step: 11
Training loss: 2.691816633631091
Validation loss: 2.475758427034941

Epoch: 211| Step: 0
Training loss: 1.5855330529354106
Validation loss: 2.483646729928089

Epoch: 5| Step: 1
Training loss: 2.272833465349399
Validation loss: 2.4783894792604766

Epoch: 5| Step: 2
Training loss: 2.7294436209901045
Validation loss: 2.4745900655914914

Epoch: 5| Step: 3
Training loss: 2.8456275526706
Validation loss: 2.4763679668666883

Epoch: 5| Step: 4
Training loss: 2.9077029954234885
Validation loss: 2.464513880545668

Epoch: 5| Step: 5
Training loss: 2.718168613051492
Validation loss: 2.454494434245473

Epoch: 5| Step: 6
Training loss: 2.3676129956455423
Validation loss: 2.46595899336712

Epoch: 5| Step: 7
Training loss: 2.4025192072613626
Validation loss: 2.469139869101216

Epoch: 5| Step: 8
Training loss: 1.9271071630155838
Validation loss: 2.469834865952204

Epoch: 5| Step: 9
Training loss: 1.9385445763021036
Validation loss: 2.4736940858493153

Epoch: 5| Step: 10
Training loss: 2.577467031036778
Validation loss: 2.4676395969202938

Epoch: 5| Step: 11
Training loss: 3.5172889671111216
Validation loss: 2.461506578960327

Epoch: 212| Step: 0
Training loss: 2.2684781755015964
Validation loss: 2.4711948573891065

Epoch: 5| Step: 1
Training loss: 1.8223671511776671
Validation loss: 2.473000585315325

Epoch: 5| Step: 2
Training loss: 2.2151692469155644
Validation loss: 2.4676947793122705

Epoch: 5| Step: 3
Training loss: 1.8721085824571948
Validation loss: 2.4631148459860754

Epoch: 5| Step: 4
Training loss: 2.196635977492572
Validation loss: 2.473915954360716

Epoch: 5| Step: 5
Training loss: 2.6417799009809753
Validation loss: 2.4697988853257926

Epoch: 5| Step: 6
Training loss: 2.2808346304879934
Validation loss: 2.468529542503703

Epoch: 5| Step: 7
Training loss: 2.7438916076405024
Validation loss: 2.490996982264825

Epoch: 5| Step: 8
Training loss: 2.6742922907353677
Validation loss: 2.482847763973026

Epoch: 5| Step: 9
Training loss: 2.5495728789508623
Validation loss: 2.4825726014886023

Epoch: 5| Step: 10
Training loss: 3.2684096733591574
Validation loss: 2.4768019639659316

Epoch: 5| Step: 11
Training loss: 2.065734407929539
Validation loss: 2.4778307847113203

Epoch: 213| Step: 0
Training loss: 2.653394712795684
Validation loss: 2.476627906553348

Epoch: 5| Step: 1
Training loss: 2.1958989890584006
Validation loss: 2.4710906919688993

Epoch: 5| Step: 2
Training loss: 2.2809120868563806
Validation loss: 2.4671197772017406

Epoch: 5| Step: 3
Training loss: 2.3544245440822382
Validation loss: 2.465514348250273

Epoch: 5| Step: 4
Training loss: 2.4688227196135664
Validation loss: 2.4641711067385037

Epoch: 5| Step: 5
Training loss: 2.9784949730329116
Validation loss: 2.471925142264311

Epoch: 5| Step: 6
Training loss: 2.1723172403070876
Validation loss: 2.4746539747418645

Epoch: 5| Step: 7
Training loss: 2.6339259297724817
Validation loss: 2.4676272499050125

Epoch: 5| Step: 8
Training loss: 2.49515149117197
Validation loss: 2.4787062102486574

Epoch: 5| Step: 9
Training loss: 2.1618837481782425
Validation loss: 2.4779029371480887

Epoch: 5| Step: 10
Training loss: 2.4093564281415794
Validation loss: 2.4842237910353044

Epoch: 5| Step: 11
Training loss: 1.96497394593028
Validation loss: 2.477699897065557

Epoch: 214| Step: 0
Training loss: 1.8275577284518196
Validation loss: 2.4864205989736448

Epoch: 5| Step: 1
Training loss: 2.7116970790039967
Validation loss: 2.4839877300596727

Epoch: 5| Step: 2
Training loss: 2.2086886234099783
Validation loss: 2.487464810766268

Epoch: 5| Step: 3
Training loss: 2.4125944445213645
Validation loss: 2.4869073840794886

Epoch: 5| Step: 4
Training loss: 2.7737897622351566
Validation loss: 2.4859175141987935

Epoch: 5| Step: 5
Training loss: 2.537046694550254
Validation loss: 2.4948578462735846

Epoch: 5| Step: 6
Training loss: 2.599232039376062
Validation loss: 2.482617218231256

Epoch: 5| Step: 7
Training loss: 2.1904922728115968
Validation loss: 2.4796807606923394

Epoch: 5| Step: 8
Training loss: 2.2575062474769503
Validation loss: 2.482012815309946

Epoch: 5| Step: 9
Training loss: 2.457143226097561
Validation loss: 2.480694946228166

Epoch: 5| Step: 10
Training loss: 2.547018971498704
Validation loss: 2.477544651969967

Epoch: 5| Step: 11
Training loss: 2.1830842271563613
Validation loss: 2.476953261040086

Epoch: 215| Step: 0
Training loss: 2.973980923025159
Validation loss: 2.4648537532323305

Epoch: 5| Step: 1
Training loss: 2.4198944853802065
Validation loss: 2.4693066335637686

Epoch: 5| Step: 2
Training loss: 3.292353127924998
Validation loss: 2.4750684730292862

Epoch: 5| Step: 3
Training loss: 2.25441954270225
Validation loss: 2.4727729058732515

Epoch: 5| Step: 4
Training loss: 2.225571368288206
Validation loss: 2.466951954835467

Epoch: 5| Step: 5
Training loss: 2.533296205432604
Validation loss: 2.4748535718212565

Epoch: 5| Step: 6
Training loss: 2.317161809942195
Validation loss: 2.4708710102041302

Epoch: 5| Step: 7
Training loss: 2.304712017381509
Validation loss: 2.471315786923225

Epoch: 5| Step: 8
Training loss: 1.3534594767760737
Validation loss: 2.4653543023125786

Epoch: 5| Step: 9
Training loss: 2.402118455200522
Validation loss: 2.4713185545295318

Epoch: 5| Step: 10
Training loss: 2.199749680930183
Validation loss: 2.4717303245378406

Epoch: 5| Step: 11
Training loss: 2.066245867845635
Validation loss: 2.4710289178689906

Epoch: 216| Step: 0
Training loss: 2.6342870731545305
Validation loss: 2.480149248026327

Epoch: 5| Step: 1
Training loss: 2.3627269252400698
Validation loss: 2.4757894077489633

Epoch: 5| Step: 2
Training loss: 2.510156409569031
Validation loss: 2.481059375041698

Epoch: 5| Step: 3
Training loss: 2.7510870605637012
Validation loss: 2.4670294186160504

Epoch: 5| Step: 4
Training loss: 2.5388869004783197
Validation loss: 2.4805118186595823

Epoch: 5| Step: 5
Training loss: 2.1456584920916604
Validation loss: 2.481202849445391

Epoch: 5| Step: 6
Training loss: 1.869503484281449
Validation loss: 2.49077671104295

Epoch: 5| Step: 7
Training loss: 2.2277578257330273
Validation loss: 2.482164883360669

Epoch: 5| Step: 8
Training loss: 2.4125080721230927
Validation loss: 2.4897565754796176

Epoch: 5| Step: 9
Training loss: 2.4092087823157025
Validation loss: 2.4838304942602716

Epoch: 5| Step: 10
Training loss: 2.760344987814333
Validation loss: 2.4755214504693566

Epoch: 5| Step: 11
Training loss: 1.8355076063347158
Validation loss: 2.483835107691962

Epoch: 217| Step: 0
Training loss: 2.3535762035123797
Validation loss: 2.474214586070341

Epoch: 5| Step: 1
Training loss: 1.9029870169593113
Validation loss: 2.4830636176162066

Epoch: 5| Step: 2
Training loss: 2.359427748336783
Validation loss: 2.478043707932934

Epoch: 5| Step: 3
Training loss: 2.8274585929287386
Validation loss: 2.485530012977706

Epoch: 5| Step: 4
Training loss: 2.859224242525054
Validation loss: 2.4798160365025512

Epoch: 5| Step: 5
Training loss: 2.466208876408569
Validation loss: 2.489044190263824

Epoch: 5| Step: 6
Training loss: 2.694032049791304
Validation loss: 2.48225522726262

Epoch: 5| Step: 7
Training loss: 2.541678621441185
Validation loss: 2.4953031407170587

Epoch: 5| Step: 8
Training loss: 2.1742336720904794
Validation loss: 2.491790263487178

Epoch: 5| Step: 9
Training loss: 2.231904591514014
Validation loss: 2.48932970156947

Epoch: 5| Step: 10
Training loss: 2.2501484504046934
Validation loss: 2.487397995811038

Epoch: 5| Step: 11
Training loss: 2.0061106315524655
Validation loss: 2.4817405303887643

Epoch: 218| Step: 0
Training loss: 2.807718068839775
Validation loss: 2.476389813745392

Epoch: 5| Step: 1
Training loss: 2.190480408950756
Validation loss: 2.4819785221668065

Epoch: 5| Step: 2
Training loss: 2.820095656240309
Validation loss: 2.46800689941495

Epoch: 5| Step: 3
Training loss: 2.401693414260762
Validation loss: 2.469561117655968

Epoch: 5| Step: 4
Training loss: 2.3487709686821265
Validation loss: 2.4646938538351537

Epoch: 5| Step: 5
Training loss: 1.7728661780633768
Validation loss: 2.474267528018187

Epoch: 5| Step: 6
Training loss: 2.888264119822578
Validation loss: 2.4787109193848718

Epoch: 5| Step: 7
Training loss: 2.0732053878328505
Validation loss: 2.4754826731716553

Epoch: 5| Step: 8
Training loss: 2.6194753184094144
Validation loss: 2.4836798521086174

Epoch: 5| Step: 9
Training loss: 2.15142880358882
Validation loss: 2.482677777674156

Epoch: 5| Step: 10
Training loss: 2.4965492274731003
Validation loss: 2.4892121714012836

Epoch: 5| Step: 11
Training loss: 1.9464165185977638
Validation loss: 2.4738790492688247

Epoch: 219| Step: 0
Training loss: 2.3439870078732548
Validation loss: 2.4857604923844674

Epoch: 5| Step: 1
Training loss: 1.8997334895690976
Validation loss: 2.471549551301218

Epoch: 5| Step: 2
Training loss: 2.3065536960691686
Validation loss: 2.475196535883817

Epoch: 5| Step: 3
Training loss: 2.675009600898412
Validation loss: 2.4820354730195726

Epoch: 5| Step: 4
Training loss: 3.0534724870425762
Validation loss: 2.4751604646304464

Epoch: 5| Step: 5
Training loss: 2.000267726145009
Validation loss: 2.4834457554961396

Epoch: 5| Step: 6
Training loss: 2.7662872556029816
Validation loss: 2.4700866251722013

Epoch: 5| Step: 7
Training loss: 2.29622988821327
Validation loss: 2.4829756256444258

Epoch: 5| Step: 8
Training loss: 1.821390577662659
Validation loss: 2.47885933875832

Epoch: 5| Step: 9
Training loss: 2.869268093822199
Validation loss: 2.4834966086363264

Epoch: 5| Step: 10
Training loss: 2.3469084756345677
Validation loss: 2.4878663016989155

Epoch: 5| Step: 11
Training loss: 2.1806118006920405
Validation loss: 2.511120184515799

Epoch: 220| Step: 0
Training loss: 2.5951697818436155
Validation loss: 2.5088087224431304

Epoch: 5| Step: 1
Training loss: 2.5041570433708364
Validation loss: 2.536429588473537

Epoch: 5| Step: 2
Training loss: 2.530121916898013
Validation loss: 2.5183778355346234

Epoch: 5| Step: 3
Training loss: 2.637728665821382
Validation loss: 2.505711976074759

Epoch: 5| Step: 4
Training loss: 2.007463476815529
Validation loss: 2.492866327996495

Epoch: 5| Step: 5
Training loss: 2.123428549056454
Validation loss: 2.497082454257528

Epoch: 5| Step: 6
Training loss: 2.4328879715244853
Validation loss: 2.482699505007428

Epoch: 5| Step: 7
Training loss: 2.553484992394566
Validation loss: 2.475836132882135

Epoch: 5| Step: 8
Training loss: 2.586083261121608
Validation loss: 2.4756468134009495

Epoch: 5| Step: 9
Training loss: 2.378132310572853
Validation loss: 2.4737171430302767

Epoch: 5| Step: 10
Training loss: 2.2256053272589433
Validation loss: 2.477649277784336

Epoch: 5| Step: 11
Training loss: 3.0045000340167642
Validation loss: 2.472888768870832

Epoch: 221| Step: 0
Training loss: 2.8759919818665596
Validation loss: 2.477864323407372

Epoch: 5| Step: 1
Training loss: 2.4124898880662635
Validation loss: 2.4767263059464932

Epoch: 5| Step: 2
Training loss: 2.5647431766662057
Validation loss: 2.4784954161405035

Epoch: 5| Step: 3
Training loss: 2.5737827609081347
Validation loss: 2.4759649905726038

Epoch: 5| Step: 4
Training loss: 2.377366944449681
Validation loss: 2.4806698935346865

Epoch: 5| Step: 5
Training loss: 2.7259498060815206
Validation loss: 2.482611792237937

Epoch: 5| Step: 6
Training loss: 1.697836127057986
Validation loss: 2.4765909957449734

Epoch: 5| Step: 7
Training loss: 2.0841184535418216
Validation loss: 2.479369957920926

Epoch: 5| Step: 8
Training loss: 2.2855957651915557
Validation loss: 2.474057094543496

Epoch: 5| Step: 9
Training loss: 2.7875325256639645
Validation loss: 2.484343896427225

Epoch: 5| Step: 10
Training loss: 2.207072623886254
Validation loss: 2.4845650198380573

Epoch: 5| Step: 11
Training loss: 1.1909818299810844
Validation loss: 2.487458277117598

Epoch: 222| Step: 0
Training loss: 2.1578580968902235
Validation loss: 2.484884641663846

Epoch: 5| Step: 1
Training loss: 2.148407870001646
Validation loss: 2.492087825587623

Epoch: 5| Step: 2
Training loss: 3.0054394364817707
Validation loss: 2.4875742727881427

Epoch: 5| Step: 3
Training loss: 3.0424782353972906
Validation loss: 2.4983046107669544

Epoch: 5| Step: 4
Training loss: 2.249012412451266
Validation loss: 2.5141364169511076

Epoch: 5| Step: 5
Training loss: 2.1994066175099882
Validation loss: 2.4879013362363853

Epoch: 5| Step: 6
Training loss: 2.554086690358982
Validation loss: 2.480666369478221

Epoch: 5| Step: 7
Training loss: 2.3838070654277175
Validation loss: 2.4962127649213715

Epoch: 5| Step: 8
Training loss: 2.0999177644159235
Validation loss: 2.475179894007314

Epoch: 5| Step: 9
Training loss: 2.3560018110219736
Validation loss: 2.476719896387137

Epoch: 5| Step: 10
Training loss: 2.409189583735248
Validation loss: 2.470842285660023

Epoch: 5| Step: 11
Training loss: 2.5206803414653947
Validation loss: 2.467434355498048

Epoch: 223| Step: 0
Training loss: 2.7161588927577096
Validation loss: 2.4682798420800087

Epoch: 5| Step: 1
Training loss: 2.7809231169551905
Validation loss: 2.472745249965976

Epoch: 5| Step: 2
Training loss: 2.306465626796019
Validation loss: 2.4731590478941143

Epoch: 5| Step: 3
Training loss: 2.223571696088634
Validation loss: 2.4688647521185185

Epoch: 5| Step: 4
Training loss: 2.4089662158096683
Validation loss: 2.4720036095825093

Epoch: 5| Step: 5
Training loss: 2.090555743565707
Validation loss: 2.4785226471593296

Epoch: 5| Step: 6
Training loss: 2.366089216715532
Validation loss: 2.478259374720285

Epoch: 5| Step: 7
Training loss: 2.6403883427316805
Validation loss: 2.4743979829673304

Epoch: 5| Step: 8
Training loss: 2.5836479446637717
Validation loss: 2.4792677447339315

Epoch: 5| Step: 9
Training loss: 2.211482759239891
Validation loss: 2.4876944799771987

Epoch: 5| Step: 10
Training loss: 2.3419731016560834
Validation loss: 2.5010938714958133

Epoch: 5| Step: 11
Training loss: 2.4271106854863564
Validation loss: 2.5016535337151264

Epoch: 224| Step: 0
Training loss: 2.009677599581648
Validation loss: 2.4854711277748653

Epoch: 5| Step: 1
Training loss: 2.546014090290021
Validation loss: 2.4976063474861023

Epoch: 5| Step: 2
Training loss: 2.356327944005186
Validation loss: 2.4865977342637455

Epoch: 5| Step: 3
Training loss: 2.8231313888886262
Validation loss: 2.480833612475531

Epoch: 5| Step: 4
Training loss: 2.2574821679307657
Validation loss: 2.4902303299216535

Epoch: 5| Step: 5
Training loss: 2.5892950198753275
Validation loss: 2.4777662558732

Epoch: 5| Step: 6
Training loss: 2.6935092367039006
Validation loss: 2.475727815092536

Epoch: 5| Step: 7
Training loss: 2.1388630542048115
Validation loss: 2.4775195834594874

Epoch: 5| Step: 8
Training loss: 2.2141450186826797
Validation loss: 2.4653602780311594

Epoch: 5| Step: 9
Training loss: 2.320188165794926
Validation loss: 2.468985920510178

Epoch: 5| Step: 10
Training loss: 2.4516429919515916
Validation loss: 2.4707294688874692

Epoch: 5| Step: 11
Training loss: 3.414101327636046
Validation loss: 2.4685098635902913

Epoch: 225| Step: 0
Training loss: 2.198590529374884
Validation loss: 2.4689593769569456

Epoch: 5| Step: 1
Training loss: 1.4266994080544764
Validation loss: 2.4733818480094047

Epoch: 5| Step: 2
Training loss: 2.82657180202592
Validation loss: 2.4801132307813987

Epoch: 5| Step: 3
Training loss: 2.414796390519502
Validation loss: 2.46231853345757

Epoch: 5| Step: 4
Training loss: 2.426401154311774
Validation loss: 2.4660045310724175

Epoch: 5| Step: 5
Training loss: 2.1543963660942222
Validation loss: 2.4764852421500096

Epoch: 5| Step: 6
Training loss: 2.444322281973442
Validation loss: 2.485435079733939

Epoch: 5| Step: 7
Training loss: 2.7099514357941534
Validation loss: 2.4835441487605006

Epoch: 5| Step: 8
Training loss: 2.529062432577004
Validation loss: 2.4820037817863447

Epoch: 5| Step: 9
Training loss: 2.667627896475664
Validation loss: 2.4975970680751223

Epoch: 5| Step: 10
Training loss: 2.3653969618264545
Validation loss: 2.488673420278237

Epoch: 5| Step: 11
Training loss: 2.634233312088565
Validation loss: 2.490488332128186

Epoch: 226| Step: 0
Training loss: 2.2101592186927745
Validation loss: 2.494369715378593

Epoch: 5| Step: 1
Training loss: 2.20456924175382
Validation loss: 2.4813759293777626

Epoch: 5| Step: 2
Training loss: 2.331558256248965
Validation loss: 2.490715030333683

Epoch: 5| Step: 3
Training loss: 2.0861440263440882
Validation loss: 2.491919803093825

Epoch: 5| Step: 4
Training loss: 2.0308113211324668
Validation loss: 2.4884683487764603

Epoch: 5| Step: 5
Training loss: 2.6841553568214382
Validation loss: 2.4887662801109487

Epoch: 5| Step: 6
Training loss: 2.425531002779964
Validation loss: 2.485520848352741

Epoch: 5| Step: 7
Training loss: 2.4245566995244685
Validation loss: 2.4766419254567227

Epoch: 5| Step: 8
Training loss: 2.9213798225751173
Validation loss: 2.474139668026229

Epoch: 5| Step: 9
Training loss: 2.318510543921021
Validation loss: 2.4765384684940055

Epoch: 5| Step: 10
Training loss: 2.6911243573461374
Validation loss: 2.4803269753777277

Epoch: 5| Step: 11
Training loss: 2.4635066109892407
Validation loss: 2.4823977641553725

Epoch: 227| Step: 0
Training loss: 2.3018181994823173
Validation loss: 2.497463007962603

Epoch: 5| Step: 1
Training loss: 2.356892068740919
Validation loss: 2.4857827802392767

Epoch: 5| Step: 2
Training loss: 2.922390163109966
Validation loss: 2.49228029666284

Epoch: 5| Step: 3
Training loss: 1.9877774362840401
Validation loss: 2.490868425618735

Epoch: 5| Step: 4
Training loss: 2.3443834593174575
Validation loss: 2.4857064324163822

Epoch: 5| Step: 5
Training loss: 2.41041600637556
Validation loss: 2.4952825184173246

Epoch: 5| Step: 6
Training loss: 2.7034333334354814
Validation loss: 2.4728353876405875

Epoch: 5| Step: 7
Training loss: 2.0566013256350293
Validation loss: 2.4802150526493785

Epoch: 5| Step: 8
Training loss: 3.021511992591542
Validation loss: 2.4864642177672502

Epoch: 5| Step: 9
Training loss: 2.4255122282828343
Validation loss: 2.4978767001019895

Epoch: 5| Step: 10
Training loss: 1.7723962356906209
Validation loss: 2.493984383807713

Epoch: 5| Step: 11
Training loss: 2.0716612830773657
Validation loss: 2.496899036614838

Epoch: 228| Step: 0
Training loss: 2.4782572341793347
Validation loss: 2.488570283863881

Epoch: 5| Step: 1
Training loss: 2.1059004813406523
Validation loss: 2.5020414520959657

Epoch: 5| Step: 2
Training loss: 2.6618949596680466
Validation loss: 2.4895133456896352

Epoch: 5| Step: 3
Training loss: 2.5011923807457292
Validation loss: 2.50083684822716

Epoch: 5| Step: 4
Training loss: 2.1135472607965515
Validation loss: 2.485725823323038

Epoch: 5| Step: 5
Training loss: 2.838486715828964
Validation loss: 2.5054039406232502

Epoch: 5| Step: 6
Training loss: 2.197614338648414
Validation loss: 2.4964693052588816

Epoch: 5| Step: 7
Training loss: 2.3460370921058815
Validation loss: 2.517755913011945

Epoch: 5| Step: 8
Training loss: 2.2633889575278663
Validation loss: 2.501310926690157

Epoch: 5| Step: 9
Training loss: 2.442918086586408
Validation loss: 2.4939576641737564

Epoch: 5| Step: 10
Training loss: 2.3493583208949835
Validation loss: 2.486443560113849

Epoch: 5| Step: 11
Training loss: 3.011075872736679
Validation loss: 2.4754719102871388

Epoch: 229| Step: 0
Training loss: 2.9906523030628187
Validation loss: 2.473349949501583

Epoch: 5| Step: 1
Training loss: 2.4574140241452342
Validation loss: 2.4742188540700463

Epoch: 5| Step: 2
Training loss: 2.100192279187338
Validation loss: 2.4770617718850367

Epoch: 5| Step: 3
Training loss: 2.130945751142324
Validation loss: 2.4838514876049587

Epoch: 5| Step: 4
Training loss: 2.4762341976672517
Validation loss: 2.4938410631601466

Epoch: 5| Step: 5
Training loss: 2.2252732141319096
Validation loss: 2.483124796274226

Epoch: 5| Step: 6
Training loss: 2.23936204630251
Validation loss: 2.4999354910475002

Epoch: 5| Step: 7
Training loss: 1.959349152365615
Validation loss: 2.473013086268406

Epoch: 5| Step: 8
Training loss: 2.5472147896298964
Validation loss: 2.4843820955666995

Epoch: 5| Step: 9
Training loss: 3.010828027482104
Validation loss: 2.4968417563710017

Epoch: 5| Step: 10
Training loss: 2.184587774432893
Validation loss: 2.4896634154162123

Epoch: 5| Step: 11
Training loss: 3.2640932131902316
Validation loss: 2.4874164709447055

Epoch: 230| Step: 0
Training loss: 2.333059930678318
Validation loss: 2.4881220556543733

Epoch: 5| Step: 1
Training loss: 2.2365885620389605
Validation loss: 2.473608611970277

Epoch: 5| Step: 2
Training loss: 2.554309222011707
Validation loss: 2.483840153060633

Epoch: 5| Step: 3
Training loss: 2.1119976552458954
Validation loss: 2.473160815271641

Epoch: 5| Step: 4
Training loss: 1.9924936695070108
Validation loss: 2.4748935069668594

Epoch: 5| Step: 5
Training loss: 2.675831325090293
Validation loss: 2.4714776997430605

Epoch: 5| Step: 6
Training loss: 2.9063422075636276
Validation loss: 2.477479957293555

Epoch: 5| Step: 7
Training loss: 2.23892506232855
Validation loss: 2.4779739088489094

Epoch: 5| Step: 8
Training loss: 2.036677577191903
Validation loss: 2.4762120485294017

Epoch: 5| Step: 9
Training loss: 2.7988064878895846
Validation loss: 2.476025941408577

Epoch: 5| Step: 10
Training loss: 2.3856607392781877
Validation loss: 2.4752345812053105

Epoch: 5| Step: 11
Training loss: 2.9016575778714397
Validation loss: 2.4865523060200605

Epoch: 231| Step: 0
Training loss: 2.436100680383177
Validation loss: 2.48468333156375

Epoch: 5| Step: 1
Training loss: 2.216430956514901
Validation loss: 2.4919430962666413

Epoch: 5| Step: 2
Training loss: 1.6504364910185487
Validation loss: 2.489985067815576

Epoch: 5| Step: 3
Training loss: 2.4269963413960687
Validation loss: 2.4898152276631675

Epoch: 5| Step: 4
Training loss: 2.601020183685848
Validation loss: 2.490637748810077

Epoch: 5| Step: 5
Training loss: 2.4005274908219434
Validation loss: 2.494384518704516

Epoch: 5| Step: 6
Training loss: 1.6750996944308565
Validation loss: 2.498734014560798

Epoch: 5| Step: 7
Training loss: 2.8733671776119962
Validation loss: 2.5047336704998497

Epoch: 5| Step: 8
Training loss: 2.933890702295356
Validation loss: 2.496016603172133

Epoch: 5| Step: 9
Training loss: 2.6096632564105144
Validation loss: 2.492582176311616

Epoch: 5| Step: 10
Training loss: 2.135387389439798
Validation loss: 2.4867660324614294

Epoch: 5| Step: 11
Training loss: 2.5101389329006034
Validation loss: 2.490223448486292

Epoch: 232| Step: 0
Training loss: 2.7201123386914223
Validation loss: 2.508843456612639

Epoch: 5| Step: 1
Training loss: 2.0123069717999993
Validation loss: 2.525106919105391

Epoch: 5| Step: 2
Training loss: 1.9739921654156367
Validation loss: 2.511978096473672

Epoch: 5| Step: 3
Training loss: 1.7415487717998297
Validation loss: 2.5142019602627257

Epoch: 5| Step: 4
Training loss: 1.8797179944996603
Validation loss: 2.5001420656211155

Epoch: 5| Step: 5
Training loss: 2.766894377870434
Validation loss: 2.4935323699918954

Epoch: 5| Step: 6
Training loss: 2.4391624454643788
Validation loss: 2.505511276792972

Epoch: 5| Step: 7
Training loss: 2.674369316977614
Validation loss: 2.492385884264571

Epoch: 5| Step: 8
Training loss: 2.9464075294172627
Validation loss: 2.4949103561445236

Epoch: 5| Step: 9
Training loss: 2.372727360820071
Validation loss: 2.492490843644792

Epoch: 5| Step: 10
Training loss: 2.5508817270302107
Validation loss: 2.4787336393384845

Epoch: 5| Step: 11
Training loss: 2.774648138884767
Validation loss: 2.4753545308101446

Epoch: 233| Step: 0
Training loss: 2.3601214510168336
Validation loss: 2.490114168743979

Epoch: 5| Step: 1
Training loss: 2.998233911103895
Validation loss: 2.485698099715261

Epoch: 5| Step: 2
Training loss: 2.4235044842728324
Validation loss: 2.482984913688618

Epoch: 5| Step: 3
Training loss: 2.865240199753367
Validation loss: 2.507702461593492

Epoch: 5| Step: 4
Training loss: 2.322365517228408
Validation loss: 2.5086804766510125

Epoch: 5| Step: 5
Training loss: 2.4873336352972935
Validation loss: 2.5416503981293705

Epoch: 5| Step: 6
Training loss: 2.4496463481465134
Validation loss: 2.540733859336888

Epoch: 5| Step: 7
Training loss: 2.6007600297001994
Validation loss: 2.5548351995641454

Epoch: 5| Step: 8
Training loss: 2.26208366652148
Validation loss: 2.5181231129565775

Epoch: 5| Step: 9
Training loss: 1.8698222831691749
Validation loss: 2.4902679660501232

Epoch: 5| Step: 10
Training loss: 2.2308088534349966
Validation loss: 2.468475728955738

Epoch: 5| Step: 11
Training loss: 1.1530532980028627
Validation loss: 2.469036178258658

Epoch: 234| Step: 0
Training loss: 2.833077325661746
Validation loss: 2.4603828975891386

Epoch: 5| Step: 1
Training loss: 2.846180642095038
Validation loss: 2.4657312398503244

Epoch: 5| Step: 2
Training loss: 2.1696042052363262
Validation loss: 2.4667332415252634

Epoch: 5| Step: 3
Training loss: 2.4122566456180845
Validation loss: 2.472322024576393

Epoch: 5| Step: 4
Training loss: 2.630798928204295
Validation loss: 2.474413817119614

Epoch: 5| Step: 5
Training loss: 2.319514487370752
Validation loss: 2.4746024982986103

Epoch: 5| Step: 6
Training loss: 2.6204540307726405
Validation loss: 2.4730826638439534

Epoch: 5| Step: 7
Training loss: 2.026528608028447
Validation loss: 2.4719603946948174

Epoch: 5| Step: 8
Training loss: 2.540274462852125
Validation loss: 2.463525462886972

Epoch: 5| Step: 9
Training loss: 2.395282870095095
Validation loss: 2.467228823205616

Epoch: 5| Step: 10
Training loss: 2.3082157348350347
Validation loss: 2.4726661332401467

Epoch: 5| Step: 11
Training loss: 2.1529589983503823
Validation loss: 2.4846541209940054

Epoch: 235| Step: 0
Training loss: 2.4918292992971924
Validation loss: 2.4771822365559353

Epoch: 5| Step: 1
Training loss: 2.502728499156615
Validation loss: 2.4865229376230906

Epoch: 5| Step: 2
Training loss: 2.8649179898918837
Validation loss: 2.4822856026299402

Epoch: 5| Step: 3
Training loss: 2.4372216090146077
Validation loss: 2.4993115351343578

Epoch: 5| Step: 4
Training loss: 2.4259397789697386
Validation loss: 2.491786601668387

Epoch: 5| Step: 5
Training loss: 2.539293296301377
Validation loss: 2.497942717774001

Epoch: 5| Step: 6
Training loss: 2.765207981199745
Validation loss: 2.499454287732206

Epoch: 5| Step: 7
Training loss: 2.0489711366094077
Validation loss: 2.5045970017521304

Epoch: 5| Step: 8
Training loss: 2.0611752966488863
Validation loss: 2.5110228638934182

Epoch: 5| Step: 9
Training loss: 2.0669933271148113
Validation loss: 2.4889789443550154

Epoch: 5| Step: 10
Training loss: 2.1655149577753168
Validation loss: 2.489528185873401

Epoch: 5| Step: 11
Training loss: 1.7434091433106012
Validation loss: 2.4999511713981537

Epoch: 236| Step: 0
Training loss: 2.086014078284767
Validation loss: 2.4797631769584947

Epoch: 5| Step: 1
Training loss: 2.5920718175863944
Validation loss: 2.483666536893147

Epoch: 5| Step: 2
Training loss: 2.016028430397655
Validation loss: 2.4871014243761187

Epoch: 5| Step: 3
Training loss: 2.440286657348218
Validation loss: 2.4975357666165707

Epoch: 5| Step: 4
Training loss: 2.7312959610413032
Validation loss: 2.4910217516383444

Epoch: 5| Step: 5
Training loss: 2.2179889716999863
Validation loss: 2.488696723877401

Epoch: 5| Step: 6
Training loss: 2.5274555352560673
Validation loss: 2.480774151281202

Epoch: 5| Step: 7
Training loss: 2.4687200616880314
Validation loss: 2.498829360905156

Epoch: 5| Step: 8
Training loss: 1.7751895803299602
Validation loss: 2.501718177057549

Epoch: 5| Step: 9
Training loss: 2.751966813475094
Validation loss: 2.4963433783112565

Epoch: 5| Step: 10
Training loss: 2.448484855344837
Validation loss: 2.4864338075482726

Epoch: 5| Step: 11
Training loss: 3.311312048601782
Validation loss: 2.493477058213186

Epoch: 237| Step: 0
Training loss: 2.654819417440891
Validation loss: 2.500337689163217

Epoch: 5| Step: 1
Training loss: 2.955441328421146
Validation loss: 2.4967696817376765

Epoch: 5| Step: 2
Training loss: 2.1874631606133454
Validation loss: 2.4999501700203623

Epoch: 5| Step: 3
Training loss: 2.0390639031065576
Validation loss: 2.48543896874278

Epoch: 5| Step: 4
Training loss: 2.4310573629047503
Validation loss: 2.48327694824604

Epoch: 5| Step: 5
Training loss: 2.702267020008259
Validation loss: 2.4889483932358787

Epoch: 5| Step: 6
Training loss: 1.7016258991575068
Validation loss: 2.4877384617817464

Epoch: 5| Step: 7
Training loss: 2.5603045851590824
Validation loss: 2.4961479710444254

Epoch: 5| Step: 8
Training loss: 1.7926563663993005
Validation loss: 2.5021595687326803

Epoch: 5| Step: 9
Training loss: 2.9546764678184276
Validation loss: 2.48431802680308

Epoch: 5| Step: 10
Training loss: 2.001782100164273
Validation loss: 2.489760972446319

Epoch: 5| Step: 11
Training loss: 1.9239539090780193
Validation loss: 2.5153828616922476

Epoch: 238| Step: 0
Training loss: 1.9448753386609432
Validation loss: 2.510301473884518

Epoch: 5| Step: 1
Training loss: 2.5241053025334717
Validation loss: 2.542815848899131

Epoch: 5| Step: 2
Training loss: 2.843414454523447
Validation loss: 2.5289727002027056

Epoch: 5| Step: 3
Training loss: 2.7285507516641023
Validation loss: 2.523033810538552

Epoch: 5| Step: 4
Training loss: 2.246263898956145
Validation loss: 2.523441369808604

Epoch: 5| Step: 5
Training loss: 2.0603529418512543
Validation loss: 2.5184470493034468

Epoch: 5| Step: 6
Training loss: 2.051130573857248
Validation loss: 2.4930316966115584

Epoch: 5| Step: 7
Training loss: 2.6152620890988603
Validation loss: 2.505022733373272

Epoch: 5| Step: 8
Training loss: 2.226692490380364
Validation loss: 2.480446722778878

Epoch: 5| Step: 9
Training loss: 2.2878167730405368
Validation loss: 2.4809605629018727

Epoch: 5| Step: 10
Training loss: 2.7872241717664545
Validation loss: 2.4895459947752157

Epoch: 5| Step: 11
Training loss: 2.9659433047328547
Validation loss: 2.4924489166212056

Epoch: 239| Step: 0
Training loss: 2.0120801878737544
Validation loss: 2.5014364605932213

Epoch: 5| Step: 1
Training loss: 2.4417507569433052
Validation loss: 2.511121877703002

Epoch: 5| Step: 2
Training loss: 2.5646150745583354
Validation loss: 2.496309178567308

Epoch: 5| Step: 3
Training loss: 2.543081067208061
Validation loss: 2.492708970763373

Epoch: 5| Step: 4
Training loss: 2.770784640541642
Validation loss: 2.508183484549354

Epoch: 5| Step: 5
Training loss: 2.1500608657383156
Validation loss: 2.504762167005061

Epoch: 5| Step: 6
Training loss: 2.128415560387345
Validation loss: 2.49311454216737

Epoch: 5| Step: 7
Training loss: 2.2995097301233334
Validation loss: 2.4921208933795644

Epoch: 5| Step: 8
Training loss: 1.9743152252162033
Validation loss: 2.5035020459344963

Epoch: 5| Step: 9
Training loss: 2.360256914443628
Validation loss: 2.507062310565003

Epoch: 5| Step: 10
Training loss: 2.773124053865503
Validation loss: 2.5268842072012094

Epoch: 5| Step: 11
Training loss: 2.7358279482412504
Validation loss: 2.5189973684463496

Epoch: 240| Step: 0
Training loss: 2.9170289677439314
Validation loss: 2.512112271740196

Epoch: 5| Step: 1
Training loss: 2.46374642056883
Validation loss: 2.503753693168914

Epoch: 5| Step: 2
Training loss: 2.54491241289372
Validation loss: 2.496446189614201

Epoch: 5| Step: 3
Training loss: 1.781257361681265
Validation loss: 2.484480757631573

Epoch: 5| Step: 4
Training loss: 2.172799551694933
Validation loss: 2.477776369311009

Epoch: 5| Step: 5
Training loss: 2.1716835431574957
Validation loss: 2.478340429312119

Epoch: 5| Step: 6
Training loss: 2.287867940692883
Validation loss: 2.487993432311275

Epoch: 5| Step: 7
Training loss: 2.678938869523582
Validation loss: 2.480958320583806

Epoch: 5| Step: 8
Training loss: 2.386819241120432
Validation loss: 2.4921905772669497

Epoch: 5| Step: 9
Training loss: 2.3984925701765047
Validation loss: 2.5030646097404734

Epoch: 5| Step: 10
Training loss: 2.278865665549124
Validation loss: 2.5093610147962964

Epoch: 5| Step: 11
Training loss: 3.0464372369313653
Validation loss: 2.51209746610895

Epoch: 241| Step: 0
Training loss: 2.5312545211186475
Validation loss: 2.5191298882221176

Epoch: 5| Step: 1
Training loss: 2.220402706496304
Validation loss: 2.5149100059589524

Epoch: 5| Step: 2
Training loss: 1.8176515309841403
Validation loss: 2.5111638707539736

Epoch: 5| Step: 3
Training loss: 2.6718269923844034
Validation loss: 2.497417662004388

Epoch: 5| Step: 4
Training loss: 2.298616644272254
Validation loss: 2.503841425095727

Epoch: 5| Step: 5
Training loss: 1.8356830826461659
Validation loss: 2.507061827146134

Epoch: 5| Step: 6
Training loss: 2.485921702163918
Validation loss: 2.497640473784593

Epoch: 5| Step: 7
Training loss: 2.6325219783608143
Validation loss: 2.5005523250328676

Epoch: 5| Step: 8
Training loss: 2.2411091301427577
Validation loss: 2.501692824629346

Epoch: 5| Step: 9
Training loss: 2.4903900456857917
Validation loss: 2.4928652799375666

Epoch: 5| Step: 10
Training loss: 2.9156468561299813
Validation loss: 2.497421241978393

Epoch: 5| Step: 11
Training loss: 1.1841535597757487
Validation loss: 2.4927336274800673

Epoch: 242| Step: 0
Training loss: 2.588218581813772
Validation loss: 2.4891453913767174

Epoch: 5| Step: 1
Training loss: 2.1541788962906776
Validation loss: 2.471428741734113

Epoch: 5| Step: 2
Training loss: 2.3148035484322365
Validation loss: 2.4754558461334573

Epoch: 5| Step: 3
Training loss: 2.586631474337464
Validation loss: 2.47972937750674

Epoch: 5| Step: 4
Training loss: 2.7883776082933753
Validation loss: 2.477333177575848

Epoch: 5| Step: 5
Training loss: 1.650596903713867
Validation loss: 2.464517216079068

Epoch: 5| Step: 6
Training loss: 2.2706714070767737
Validation loss: 2.4816491469778277

Epoch: 5| Step: 7
Training loss: 2.305943447508084
Validation loss: 2.483460299928561

Epoch: 5| Step: 8
Training loss: 2.246028680273492
Validation loss: 2.487989762907903

Epoch: 5| Step: 9
Training loss: 2.6736549561003167
Validation loss: 2.4918252887013814

Epoch: 5| Step: 10
Training loss: 2.797527727223185
Validation loss: 2.4905793413566397

Epoch: 5| Step: 11
Training loss: 2.794344647165255
Validation loss: 2.512873687125694

Epoch: 243| Step: 0
Training loss: 2.075596468579193
Validation loss: 2.546483729444768

Epoch: 5| Step: 1
Training loss: 2.2969549255997204
Validation loss: 2.556572544327011

Epoch: 5| Step: 2
Training loss: 2.4227367221687683
Validation loss: 2.614121154321687

Epoch: 5| Step: 3
Training loss: 2.505574115741759
Validation loss: 2.5933675675555548

Epoch: 5| Step: 4
Training loss: 2.5485540366877606
Validation loss: 2.5840552472395566

Epoch: 5| Step: 5
Training loss: 2.4603945178510864
Validation loss: 2.5308273535569783

Epoch: 5| Step: 6
Training loss: 2.131695353494172
Validation loss: 2.508073723835512

Epoch: 5| Step: 7
Training loss: 2.7299168455992793
Validation loss: 2.497284901011164

Epoch: 5| Step: 8
Training loss: 1.6835876606346518
Validation loss: 2.4985929344165894

Epoch: 5| Step: 9
Training loss: 2.9998729996820184
Validation loss: 2.4762205896712333

Epoch: 5| Step: 10
Training loss: 2.575057360556508
Validation loss: 2.4912941506431325

Epoch: 5| Step: 11
Training loss: 4.148349222224577
Validation loss: 2.4845243365111207

Epoch: 244| Step: 0
Training loss: 1.9172743027614538
Validation loss: 2.48497597405187

Epoch: 5| Step: 1
Training loss: 2.1304611267468236
Validation loss: 2.491210989339607

Epoch: 5| Step: 2
Training loss: 1.9425210189173108
Validation loss: 2.4930834101576314

Epoch: 5| Step: 3
Training loss: 2.3109133664328922
Validation loss: 2.5008563124865626

Epoch: 5| Step: 4
Training loss: 2.637081319111331
Validation loss: 2.5016885419516472

Epoch: 5| Step: 5
Training loss: 2.6907625134425293
Validation loss: 2.5056898218772954

Epoch: 5| Step: 6
Training loss: 2.6578423607711334
Validation loss: 2.5018413517741536

Epoch: 5| Step: 7
Training loss: 2.6383860649997035
Validation loss: 2.501082305281466

Epoch: 5| Step: 8
Training loss: 2.1842826070605708
Validation loss: 2.50782316370011

Epoch: 5| Step: 9
Training loss: 2.4667293149707734
Validation loss: 2.5198853591357073

Epoch: 5| Step: 10
Training loss: 2.4574622426668737
Validation loss: 2.5404500994162036

Epoch: 5| Step: 11
Training loss: 2.220572569271356
Validation loss: 2.528773153670156

Epoch: 245| Step: 0
Training loss: 2.0266354302876883
Validation loss: 2.5420205516114462

Epoch: 5| Step: 1
Training loss: 2.0364536243068825
Validation loss: 2.551855152335409

Epoch: 5| Step: 2
Training loss: 2.24065780296415
Validation loss: 2.548645921140854

Epoch: 5| Step: 3
Training loss: 2.3679450808870994
Validation loss: 2.5587915319821932

Epoch: 5| Step: 4
Training loss: 2.297678904079602
Validation loss: 2.543486175789999

Epoch: 5| Step: 5
Training loss: 2.93149309594026
Validation loss: 2.5270120343095486

Epoch: 5| Step: 6
Training loss: 2.6280840966923242
Validation loss: 2.5165018958242436

Epoch: 5| Step: 7
Training loss: 2.3136482481013365
Validation loss: 2.5204791195859726

Epoch: 5| Step: 8
Training loss: 2.6165801752783477
Validation loss: 2.5059210020690017

Epoch: 5| Step: 9
Training loss: 2.405957984544779
Validation loss: 2.494271546036677

Epoch: 5| Step: 10
Training loss: 2.5070290931319117
Validation loss: 2.4987976282264515

Epoch: 5| Step: 11
Training loss: 1.0561640258650904
Validation loss: 2.4887414024698846

Epoch: 246| Step: 0
Training loss: 2.8148030494375273
Validation loss: 2.4915559939191825

Epoch: 5| Step: 1
Training loss: 2.7603341048267644
Validation loss: 2.496541245314684

Epoch: 5| Step: 2
Training loss: 2.0399768869175245
Validation loss: 2.488540132909861

Epoch: 5| Step: 3
Training loss: 2.3573857694198117
Validation loss: 2.489401351124567

Epoch: 5| Step: 4
Training loss: 2.2497221457262655
Validation loss: 2.4891063094848516

Epoch: 5| Step: 5
Training loss: 2.2833449268278776
Validation loss: 2.4881111178681223

Epoch: 5| Step: 6
Training loss: 1.9178579885039986
Validation loss: 2.501442489112112

Epoch: 5| Step: 7
Training loss: 2.700930901161509
Validation loss: 2.500590079922701

Epoch: 5| Step: 8
Training loss: 2.4695712305034894
Validation loss: 2.5433723141202385

Epoch: 5| Step: 9
Training loss: 2.4295205797955584
Validation loss: 2.529795177572154

Epoch: 5| Step: 10
Training loss: 2.020151423075944
Validation loss: 2.562140470574776

Epoch: 5| Step: 11
Training loss: 3.0847255210013405
Validation loss: 2.5424382661897247

Epoch: 247| Step: 0
Training loss: 2.379622127764672
Validation loss: 2.569698606341292

Epoch: 5| Step: 1
Training loss: 3.0243472127944564
Validation loss: 2.582119857952861

Epoch: 5| Step: 2
Training loss: 2.3620577080288174
Validation loss: 2.554986463779642

Epoch: 5| Step: 3
Training loss: 2.1773440434592892
Validation loss: 2.5413783188668857

Epoch: 5| Step: 4
Training loss: 2.029512924427608
Validation loss: 2.538301872372375

Epoch: 5| Step: 5
Training loss: 2.5991438886698846
Validation loss: 2.5232584705758008

Epoch: 5| Step: 6
Training loss: 2.2530357226753788
Validation loss: 2.5188993898740843

Epoch: 5| Step: 7
Training loss: 2.4595361019431032
Validation loss: 2.5321810524927573

Epoch: 5| Step: 8
Training loss: 2.3390075514052975
Validation loss: 2.522347942346369

Epoch: 5| Step: 9
Training loss: 2.596396876640363
Validation loss: 2.48533130929785

Epoch: 5| Step: 10
Training loss: 2.3394656886898617
Validation loss: 2.487638641052942

Epoch: 5| Step: 11
Training loss: 0.4083015104304191
Validation loss: 2.4775171896681085

Epoch: 248| Step: 0
Training loss: 3.013082114046806
Validation loss: 2.4919295222295426

Epoch: 5| Step: 1
Training loss: 2.1238835711851523
Validation loss: 2.4861111122950645

Epoch: 5| Step: 2
Training loss: 2.4742692504392956
Validation loss: 2.491429384015905

Epoch: 5| Step: 3
Training loss: 2.42535298324868
Validation loss: 2.4947398597790773

Epoch: 5| Step: 4
Training loss: 2.3872355449469382
Validation loss: 2.489686282807867

Epoch: 5| Step: 5
Training loss: 2.0148059452410694
Validation loss: 2.4941465277665444

Epoch: 5| Step: 6
Training loss: 2.5161278263606612
Validation loss: 2.5051769815070632

Epoch: 5| Step: 7
Training loss: 2.1363883312207363
Validation loss: 2.506997955598874

Epoch: 5| Step: 8
Training loss: 2.6362417284065667
Validation loss: 2.505688418401496

Epoch: 5| Step: 9
Training loss: 2.471616119447525
Validation loss: 2.523855956096871

Epoch: 5| Step: 10
Training loss: 2.209595181771894
Validation loss: 2.5024416682545594

Epoch: 5| Step: 11
Training loss: 2.6675012494471306
Validation loss: 2.5042268624161133

Epoch: 249| Step: 0
Training loss: 2.0865541611019625
Validation loss: 2.5252765925782548

Epoch: 5| Step: 1
Training loss: 2.050137664080368
Validation loss: 2.510772712805847

Epoch: 5| Step: 2
Training loss: 2.4353014494240033
Validation loss: 2.495325283622124

Epoch: 5| Step: 3
Training loss: 2.7319441098853674
Validation loss: 2.4917361768597646

Epoch: 5| Step: 4
Training loss: 2.362104239441664
Validation loss: 2.490298556828392

Epoch: 5| Step: 5
Training loss: 2.294824840036327
Validation loss: 2.487923013958756

Epoch: 5| Step: 6
Training loss: 2.3680400257465237
Validation loss: 2.501810308505191

Epoch: 5| Step: 7
Training loss: 2.8241020801130623
Validation loss: 2.499364255497471

Epoch: 5| Step: 8
Training loss: 1.9149834110746657
Validation loss: 2.4955481150268906

Epoch: 5| Step: 9
Training loss: 2.5953325706041173
Validation loss: 2.5039750762700463

Epoch: 5| Step: 10
Training loss: 2.6087852314067477
Validation loss: 2.499257291302073

Epoch: 5| Step: 11
Training loss: 1.4601051284878677
Validation loss: 2.501812655227777

Epoch: 250| Step: 0
Training loss: 2.2011989534298273
Validation loss: 2.5361218134568477

Epoch: 5| Step: 1
Training loss: 2.401860283065286
Validation loss: 2.533789416613346

Epoch: 5| Step: 2
Training loss: 2.892521416560934
Validation loss: 2.5504973069992958

Epoch: 5| Step: 3
Training loss: 2.2430916800672165
Validation loss: 2.5364431750231686

Epoch: 5| Step: 4
Training loss: 2.717938356813485
Validation loss: 2.5255224277593316

Epoch: 5| Step: 5
Training loss: 2.3445286792409266
Validation loss: 2.527270288986781

Epoch: 5| Step: 6
Training loss: 1.9462558649718324
Validation loss: 2.5067915061120987

Epoch: 5| Step: 7
Training loss: 1.926024013484463
Validation loss: 2.5006864161705193

Epoch: 5| Step: 8
Training loss: 2.6603163168274957
Validation loss: 2.5022001817011144

Epoch: 5| Step: 9
Training loss: 2.1465876713230863
Validation loss: 2.4830550239983857

Epoch: 5| Step: 10
Training loss: 2.817436632934808
Validation loss: 2.500420241003454

Epoch: 5| Step: 11
Training loss: 1.5855209480039827
Validation loss: 2.482851857088885

Epoch: 251| Step: 0
Training loss: 2.243686719761818
Validation loss: 2.47974443648733

Epoch: 5| Step: 1
Training loss: 2.4473494068609605
Validation loss: 2.4824838978212207

Epoch: 5| Step: 2
Training loss: 1.9865192032602843
Validation loss: 2.5076784512037023

Epoch: 5| Step: 3
Training loss: 3.003101811632072
Validation loss: 2.511548564043659

Epoch: 5| Step: 4
Training loss: 2.1580219451665252
Validation loss: 2.518280030098162

Epoch: 5| Step: 5
Training loss: 1.7865402899831715
Validation loss: 2.5039981462156424

Epoch: 5| Step: 6
Training loss: 2.159109237812127
Validation loss: 2.5137003215532765

Epoch: 5| Step: 7
Training loss: 2.3586753161422536
Validation loss: 2.500016053466117

Epoch: 5| Step: 8
Training loss: 1.8506734782023957
Validation loss: 2.505000941891217

Epoch: 5| Step: 9
Training loss: 2.60196991900591
Validation loss: 2.505459470843729

Epoch: 5| Step: 10
Training loss: 2.9862915595680803
Validation loss: 2.504307294716899

Epoch: 5| Step: 11
Training loss: 3.110199172091023
Validation loss: 2.5133902395180416

Epoch: 252| Step: 0
Training loss: 2.7334800781618145
Validation loss: 2.5152975821024044

Epoch: 5| Step: 1
Training loss: 2.2957675885192748
Validation loss: 2.5051945919260095

Epoch: 5| Step: 2
Training loss: 2.3300227680649233
Validation loss: 2.534802526278975

Epoch: 5| Step: 3
Training loss: 2.400779649142456
Validation loss: 2.5147635760073745

Epoch: 5| Step: 4
Training loss: 2.458646355651956
Validation loss: 2.514512884443164

Epoch: 5| Step: 5
Training loss: 2.3039665516968535
Validation loss: 2.5128411355608984

Epoch: 5| Step: 6
Training loss: 2.4435284721475123
Validation loss: 2.517978597888632

Epoch: 5| Step: 7
Training loss: 1.8984639986174694
Validation loss: 2.4973300146844437

Epoch: 5| Step: 8
Training loss: 2.594665917149769
Validation loss: 2.504526955037654

Epoch: 5| Step: 9
Training loss: 2.185318540316296
Validation loss: 2.4996218832491626

Epoch: 5| Step: 10
Training loss: 2.373671411033931
Validation loss: 2.492429097741299

Epoch: 5| Step: 11
Training loss: 2.3889932486437524
Validation loss: 2.502910533072171

Epoch: 253| Step: 0
Training loss: 2.637094880579705
Validation loss: 2.506064220528563

Epoch: 5| Step: 1
Training loss: 2.3725421635666692
Validation loss: 2.505699515353939

Epoch: 5| Step: 2
Training loss: 2.386501470936582
Validation loss: 2.5074714991305336

Epoch: 5| Step: 3
Training loss: 2.2733352480830895
Validation loss: 2.513991625292386

Epoch: 5| Step: 4
Training loss: 2.1226905165719017
Validation loss: 2.4967820756193264

Epoch: 5| Step: 5
Training loss: 2.367883258964064
Validation loss: 2.513361601710609

Epoch: 5| Step: 6
Training loss: 2.558641739023957
Validation loss: 2.4975399748753597

Epoch: 5| Step: 7
Training loss: 2.6450175031288374
Validation loss: 2.4982122347147566

Epoch: 5| Step: 8
Training loss: 1.7864492729409531
Validation loss: 2.508728525538356

Epoch: 5| Step: 9
Training loss: 2.1648094458118075
Validation loss: 2.5077442227236415

Epoch: 5| Step: 10
Training loss: 2.5793578003628115
Validation loss: 2.516829741217633

Epoch: 5| Step: 11
Training loss: 2.039389280829785
Validation loss: 2.5147744393492717

Epoch: 254| Step: 0
Training loss: 1.7730170604403526
Validation loss: 2.502472747358363

Epoch: 5| Step: 1
Training loss: 2.2740336157697043
Validation loss: 2.5048388699747814

Epoch: 5| Step: 2
Training loss: 2.4404304690625
Validation loss: 2.5034571030069532

Epoch: 5| Step: 3
Training loss: 2.506704400552987
Validation loss: 2.509320927469256

Epoch: 5| Step: 4
Training loss: 2.3784728508891986
Validation loss: 2.5155881559149837

Epoch: 5| Step: 5
Training loss: 2.253395697089966
Validation loss: 2.51934560244585

Epoch: 5| Step: 6
Training loss: 2.5308044418211573
Validation loss: 2.5166935477705348

Epoch: 5| Step: 7
Training loss: 2.3236063687411104
Validation loss: 2.5506619692803376

Epoch: 5| Step: 8
Training loss: 2.0704730277535095
Validation loss: 2.5519456099633056

Epoch: 5| Step: 9
Training loss: 2.3730916839262286
Validation loss: 2.5631855729870914

Epoch: 5| Step: 10
Training loss: 2.8717196450537434
Validation loss: 2.5683769505894207

Epoch: 5| Step: 11
Training loss: 3.267582648821958
Validation loss: 2.5483264570681916

Epoch: 255| Step: 0
Training loss: 2.382828909395342
Validation loss: 2.5085888192821266

Epoch: 5| Step: 1
Training loss: 2.0273562157161313
Validation loss: 2.505565020467451

Epoch: 5| Step: 2
Training loss: 2.213795785715853
Validation loss: 2.4924149284907293

Epoch: 5| Step: 3
Training loss: 1.74848593200749
Validation loss: 2.488472274961411

Epoch: 5| Step: 4
Training loss: 2.471776145762051
Validation loss: 2.4870358219317605

Epoch: 5| Step: 5
Training loss: 2.2773395623353436
Validation loss: 2.4985113480123

Epoch: 5| Step: 6
Training loss: 3.0151660945557857
Validation loss: 2.4993312895020217

Epoch: 5| Step: 7
Training loss: 2.0456157042854213
Validation loss: 2.488881903492637

Epoch: 5| Step: 8
Training loss: 3.038054546707806
Validation loss: 2.493601610075835

Epoch: 5| Step: 9
Training loss: 2.9046839935133826
Validation loss: 2.4937423790390603

Epoch: 5| Step: 10
Training loss: 2.5689328093809247
Validation loss: 2.4793628780606203

Epoch: 5| Step: 11
Training loss: 2.863262764531478
Validation loss: 2.4867187396932464

Epoch: 256| Step: 0
Training loss: 2.18152499938392
Validation loss: 2.479610132309657

Epoch: 5| Step: 1
Training loss: 2.6147289058287786
Validation loss: 2.4994843348514397

Epoch: 5| Step: 2
Training loss: 2.594723897809838
Validation loss: 2.5556791055110506

Epoch: 5| Step: 3
Training loss: 2.493263132089632
Validation loss: 2.660236587095544

Epoch: 5| Step: 4
Training loss: 2.5237106317148594
Validation loss: 2.7402873542795754

Epoch: 5| Step: 5
Training loss: 2.6400798708942363
Validation loss: 2.8242758379768427

Epoch: 5| Step: 6
Training loss: 2.3721459958914783
Validation loss: 2.675524433369461

Epoch: 5| Step: 7
Training loss: 2.877153170817843
Validation loss: 2.6344112971449856

Epoch: 5| Step: 8
Training loss: 2.2732574286705005
Validation loss: 2.5575268722072053

Epoch: 5| Step: 9
Training loss: 1.8865104860468822
Validation loss: 2.5131689367317156

Epoch: 5| Step: 10
Training loss: 2.0592913088196334
Validation loss: 2.480165293796876

Epoch: 5| Step: 11
Training loss: 3.5331302464498684
Validation loss: 2.4908526960670896

Epoch: 257| Step: 0
Training loss: 2.49434604261886
Validation loss: 2.4867419677315237

Epoch: 5| Step: 1
Training loss: 2.724470062522624
Validation loss: 2.487903129077837

Epoch: 5| Step: 2
Training loss: 2.5690163355954265
Validation loss: 2.492584001655468

Epoch: 5| Step: 3
Training loss: 1.9582395801693098
Validation loss: 2.482553297996457

Epoch: 5| Step: 4
Training loss: 2.5320031258381475
Validation loss: 2.4930268989682274

Epoch: 5| Step: 5
Training loss: 2.0741898989062824
Validation loss: 2.4883587923448767

Epoch: 5| Step: 6
Training loss: 2.2156780629334336
Validation loss: 2.4772245081868927

Epoch: 5| Step: 7
Training loss: 2.9490226225659515
Validation loss: 2.48711934252176

Epoch: 5| Step: 8
Training loss: 1.541693360724761
Validation loss: 2.4841008145001773

Epoch: 5| Step: 9
Training loss: 2.6402402518509365
Validation loss: 2.4880969080568462

Epoch: 5| Step: 10
Training loss: 2.628665816718182
Validation loss: 2.491120703047587

Epoch: 5| Step: 11
Training loss: 1.3221015321232616
Validation loss: 2.4805476859441216

Epoch: 258| Step: 0
Training loss: 2.0601374648810555
Validation loss: 2.490930407620135

Epoch: 5| Step: 1
Training loss: 2.612671557182504
Validation loss: 2.5318238428139828

Epoch: 5| Step: 2
Training loss: 2.3586397351649686
Validation loss: 2.51735774482122

Epoch: 5| Step: 3
Training loss: 1.9292558194920826
Validation loss: 2.521618265053886

Epoch: 5| Step: 4
Training loss: 2.4275973734787546
Validation loss: 2.5198562215352647

Epoch: 5| Step: 5
Training loss: 2.24242397217745
Validation loss: 2.512617932109249

Epoch: 5| Step: 6
Training loss: 1.9335388792324975
Validation loss: 2.506145214211437

Epoch: 5| Step: 7
Training loss: 2.6804594034750795
Validation loss: 2.5071670676127114

Epoch: 5| Step: 8
Training loss: 2.9442706706592876
Validation loss: 2.491432625698838

Epoch: 5| Step: 9
Training loss: 2.348135037719294
Validation loss: 2.4905663402418985

Epoch: 5| Step: 10
Training loss: 2.328481045322051
Validation loss: 2.485503382314518

Epoch: 5| Step: 11
Training loss: 4.152698806697927
Validation loss: 2.4999815979916096

Epoch: 259| Step: 0
Training loss: 2.8429019208340502
Validation loss: 2.4920676470629957

Epoch: 5| Step: 1
Training loss: 2.1600605562339914
Validation loss: 2.480389663356634

Epoch: 5| Step: 2
Training loss: 2.104715131027446
Validation loss: 2.4901120742984

Epoch: 5| Step: 3
Training loss: 2.540472583961421
Validation loss: 2.493927470802206

Epoch: 5| Step: 4
Training loss: 2.8246583715153033
Validation loss: 2.4903649548969726

Epoch: 5| Step: 5
Training loss: 2.236347954674639
Validation loss: 2.493577551555769

Epoch: 5| Step: 6
Training loss: 2.399325327616178
Validation loss: 2.497411898235459

Epoch: 5| Step: 7
Training loss: 2.0748399282093186
Validation loss: 2.4895657627863654

Epoch: 5| Step: 8
Training loss: 2.302370516670311
Validation loss: 2.5072061433510635

Epoch: 5| Step: 9
Training loss: 2.47817084136549
Validation loss: 2.514496471171072

Epoch: 5| Step: 10
Training loss: 2.085001989313361
Validation loss: 2.511577584436258

Epoch: 5| Step: 11
Training loss: 2.0610802127456327
Validation loss: 2.51840412251416

Epoch: 260| Step: 0
Training loss: 2.5545976255514096
Validation loss: 2.5240750348931202

Epoch: 5| Step: 1
Training loss: 2.4784373217659477
Validation loss: 2.519305259958853

Epoch: 5| Step: 2
Training loss: 2.232044418344184
Validation loss: 2.508348669092901

Epoch: 5| Step: 3
Training loss: 1.7338380411975156
Validation loss: 2.495231377961628

Epoch: 5| Step: 4
Training loss: 2.375580867205855
Validation loss: 2.4833297449977816

Epoch: 5| Step: 5
Training loss: 2.1612039684470323
Validation loss: 2.4898840085555163

Epoch: 5| Step: 6
Training loss: 2.5656061652696764
Validation loss: 2.4821879318936824

Epoch: 5| Step: 7
Training loss: 2.6683788961713653
Validation loss: 2.4809366260518537

Epoch: 5| Step: 8
Training loss: 2.608437923816625
Validation loss: 2.4852472889228627

Epoch: 5| Step: 9
Training loss: 2.0345316274326337
Validation loss: 2.4976120193251297

Epoch: 5| Step: 10
Training loss: 2.3646938169326184
Validation loss: 2.4960723263135067

Epoch: 5| Step: 11
Training loss: 3.7276011045318187
Validation loss: 2.4845318214936114

Epoch: 261| Step: 0
Training loss: 2.7185705717664046
Validation loss: 2.479763825941919

Epoch: 5| Step: 1
Training loss: 2.6605488715684498
Validation loss: 2.4925781390288715

Epoch: 5| Step: 2
Training loss: 2.1887199269945987
Validation loss: 2.497132339472991

Epoch: 5| Step: 3
Training loss: 1.8308929616078693
Validation loss: 2.5112083395019327

Epoch: 5| Step: 4
Training loss: 2.167105508035773
Validation loss: 2.5122713516349924

Epoch: 5| Step: 5
Training loss: 2.080985602919762
Validation loss: 2.5367001072972295

Epoch: 5| Step: 6
Training loss: 1.9984089363926076
Validation loss: 2.5347400827228563

Epoch: 5| Step: 7
Training loss: 2.627856017873009
Validation loss: 2.5490344708203194

Epoch: 5| Step: 8
Training loss: 3.031364989311794
Validation loss: 2.5471667101223825

Epoch: 5| Step: 9
Training loss: 2.2488084393001286
Validation loss: 2.5526599595913364

Epoch: 5| Step: 10
Training loss: 2.3753488184106257
Validation loss: 2.5202569788701936

Epoch: 5| Step: 11
Training loss: 3.0796619586475744
Validation loss: 2.513312297515043

Epoch: 262| Step: 0
Training loss: 2.2600264205063443
Validation loss: 2.5036986766359934

Epoch: 5| Step: 1
Training loss: 2.091976496431328
Validation loss: 2.5086265740018763

Epoch: 5| Step: 2
Training loss: 1.814825407831297
Validation loss: 2.4995505505115325

Epoch: 5| Step: 3
Training loss: 2.3656591133268354
Validation loss: 2.5020886001616183

Epoch: 5| Step: 4
Training loss: 2.426122963835641
Validation loss: 2.506033447594219

Epoch: 5| Step: 5
Training loss: 2.6518614475138595
Validation loss: 2.512884708861887

Epoch: 5| Step: 6
Training loss: 2.670837290062158
Validation loss: 2.5120561767601806

Epoch: 5| Step: 7
Training loss: 2.067560980196191
Validation loss: 2.5097432473220365

Epoch: 5| Step: 8
Training loss: 2.8038563466407576
Validation loss: 2.514259749807173

Epoch: 5| Step: 9
Training loss: 2.7501380192193734
Validation loss: 2.5195925682945406

Epoch: 5| Step: 10
Training loss: 2.006678398241663
Validation loss: 2.5067427939495315

Epoch: 5| Step: 11
Training loss: 1.4667685921249602
Validation loss: 2.504136208355884

Epoch: 263| Step: 0
Training loss: 2.2648914070286734
Validation loss: 2.5063658450489945

Epoch: 5| Step: 1
Training loss: 2.7411027376668726
Validation loss: 2.5021937679172406

Epoch: 5| Step: 2
Training loss: 2.764102063885119
Validation loss: 2.5040762571325645

Epoch: 5| Step: 3
Training loss: 1.960500356878955
Validation loss: 2.5086430078590953

Epoch: 5| Step: 4
Training loss: 2.4371491204096487
Validation loss: 2.501719741597133

Epoch: 5| Step: 5
Training loss: 2.2090646744357314
Validation loss: 2.5119719310964492

Epoch: 5| Step: 6
Training loss: 2.3822903420584036
Validation loss: 2.501809893559936

Epoch: 5| Step: 7
Training loss: 2.717904935144111
Validation loss: 2.5048558045952007

Epoch: 5| Step: 8
Training loss: 1.4162934971162422
Validation loss: 2.5074481441945995

Epoch: 5| Step: 9
Training loss: 2.6769712154991208
Validation loss: 2.5116507964486092

Epoch: 5| Step: 10
Training loss: 2.132863138457064
Validation loss: 2.538434448816417

Epoch: 5| Step: 11
Training loss: 2.199009616048413
Validation loss: 2.5304883705878938

Epoch: 264| Step: 0
Training loss: 2.517899048136693
Validation loss: 2.527695279090965

Epoch: 5| Step: 1
Training loss: 2.622068675802562
Validation loss: 2.5126581645166213

Epoch: 5| Step: 2
Training loss: 2.0946366439762496
Validation loss: 2.5121314687088017

Epoch: 5| Step: 3
Training loss: 2.5267770122934743
Validation loss: 2.5186033132039314

Epoch: 5| Step: 4
Training loss: 2.0524386438824074
Validation loss: 2.5179890607250823

Epoch: 5| Step: 5
Training loss: 1.9391198923622428
Validation loss: 2.509900884286126

Epoch: 5| Step: 6
Training loss: 1.6386531060363592
Validation loss: 2.50198858167989

Epoch: 5| Step: 7
Training loss: 2.246266340176138
Validation loss: 2.5195868946801183

Epoch: 5| Step: 8
Training loss: 2.760512114020301
Validation loss: 2.5005880737057202

Epoch: 5| Step: 9
Training loss: 2.492038829996917
Validation loss: 2.4950746775251345

Epoch: 5| Step: 10
Training loss: 2.6200924366376617
Validation loss: 2.4989390585695195

Epoch: 5| Step: 11
Training loss: 2.191698894578046
Validation loss: 2.497677865012908

Epoch: 265| Step: 0
Training loss: 2.1239009707869423
Validation loss: 2.502801340905211

Epoch: 5| Step: 1
Training loss: 2.0180353937135895
Validation loss: 2.4897112926547944

Epoch: 5| Step: 2
Training loss: 2.5940509299849834
Validation loss: 2.496964909061123

Epoch: 5| Step: 3
Training loss: 2.174603511582438
Validation loss: 2.50237127537654

Epoch: 5| Step: 4
Training loss: 3.384905090804598
Validation loss: 2.510808911420537

Epoch: 5| Step: 5
Training loss: 1.886043514631585
Validation loss: 2.516119400945785

Epoch: 5| Step: 6
Training loss: 2.920959165930884
Validation loss: 2.508009091809097

Epoch: 5| Step: 7
Training loss: 1.7639380979640689
Validation loss: 2.504633773509878

Epoch: 5| Step: 8
Training loss: 2.2886710499470477
Validation loss: 2.5060953260592362

Epoch: 5| Step: 9
Training loss: 2.002339187241802
Validation loss: 2.5266213974214655

Epoch: 5| Step: 10
Training loss: 2.1761877478077336
Validation loss: 2.5143615993617083

Epoch: 5| Step: 11
Training loss: 2.4886533737393046
Validation loss: 2.512358339311104

Epoch: 266| Step: 0
Training loss: 1.9236270392806998
Validation loss: 2.5078763428798077

Epoch: 5| Step: 1
Training loss: 2.510259937337751
Validation loss: 2.522254434251273

Epoch: 5| Step: 2
Training loss: 3.0105628975768157
Validation loss: 2.5211278819510383

Epoch: 5| Step: 3
Training loss: 2.3032645277722223
Validation loss: 2.5229278222952263

Epoch: 5| Step: 4
Training loss: 2.0140996084649347
Validation loss: 2.52606662986979

Epoch: 5| Step: 5
Training loss: 2.4753781925591714
Validation loss: 2.5147148029742508

Epoch: 5| Step: 6
Training loss: 2.6674385046551574
Validation loss: 2.5294028084466356

Epoch: 5| Step: 7
Training loss: 2.314462087106857
Validation loss: 2.526353711914488

Epoch: 5| Step: 8
Training loss: 2.3675610338861435
Validation loss: 2.5216699379765064

Epoch: 5| Step: 9
Training loss: 2.1716439103607623
Validation loss: 2.510697767868014

Epoch: 5| Step: 10
Training loss: 2.075842844745903
Validation loss: 2.510582486423337

Epoch: 5| Step: 11
Training loss: 1.7873402930985862
Validation loss: 2.4956299694701882

Epoch: 267| Step: 0
Training loss: 1.984536772614335
Validation loss: 2.5078293194706895

Epoch: 5| Step: 1
Training loss: 2.2425175334198197
Validation loss: 2.503399577977576

Epoch: 5| Step: 2
Training loss: 1.8352387092678482
Validation loss: 2.509040420985582

Epoch: 5| Step: 3
Training loss: 2.5374046195007933
Validation loss: 2.499190008553279

Epoch: 5| Step: 4
Training loss: 1.93690549281768
Validation loss: 2.5016206137717307

Epoch: 5| Step: 5
Training loss: 2.276213848197715
Validation loss: 2.5072484040552716

Epoch: 5| Step: 6
Training loss: 2.2982487106327922
Validation loss: 2.5110237223885683

Epoch: 5| Step: 7
Training loss: 2.585057964744951
Validation loss: 2.508910413167923

Epoch: 5| Step: 8
Training loss: 3.0616843246701095
Validation loss: 2.4843109290540215

Epoch: 5| Step: 9
Training loss: 2.217560570200355
Validation loss: 2.5156285580122213

Epoch: 5| Step: 10
Training loss: 2.750150069563632
Validation loss: 2.5135693374820165

Epoch: 5| Step: 11
Training loss: 1.434746302332536
Validation loss: 2.507313041823058

Epoch: 268| Step: 0
Training loss: 2.6567131087560676
Validation loss: 2.5370790060299524

Epoch: 5| Step: 1
Training loss: 2.713545366652023
Validation loss: 2.532158900370937

Epoch: 5| Step: 2
Training loss: 1.7856506063823714
Validation loss: 2.545073634726821

Epoch: 5| Step: 3
Training loss: 2.3217898203036222
Validation loss: 2.5320482290534927

Epoch: 5| Step: 4
Training loss: 2.654331636516995
Validation loss: 2.524178837946978

Epoch: 5| Step: 5
Training loss: 2.069725788401054
Validation loss: 2.5201541219424772

Epoch: 5| Step: 6
Training loss: 2.3145662304246066
Validation loss: 2.5130019244864568

Epoch: 5| Step: 7
Training loss: 2.045810684834942
Validation loss: 2.5110117983789637

Epoch: 5| Step: 8
Training loss: 2.6105708505733065
Validation loss: 2.5177898253853304

Epoch: 5| Step: 9
Training loss: 2.0519152603127035
Validation loss: 2.4898980844851346

Epoch: 5| Step: 10
Training loss: 2.4710810798177882
Validation loss: 2.4979222842978674

Epoch: 5| Step: 11
Training loss: 2.1456147116490243
Validation loss: 2.494254991602274

Epoch: 269| Step: 0
Training loss: 3.073665273397646
Validation loss: 2.4872378722373925

Epoch: 5| Step: 1
Training loss: 2.182480093044806
Validation loss: 2.4900998466904767

Epoch: 5| Step: 2
Training loss: 1.6726121792428421
Validation loss: 2.4814268369461034

Epoch: 5| Step: 3
Training loss: 2.9483186876771725
Validation loss: 2.492667706981189

Epoch: 5| Step: 4
Training loss: 1.8656568438242016
Validation loss: 2.4868741011603963

Epoch: 5| Step: 5
Training loss: 2.187087755867989
Validation loss: 2.492439147696778

Epoch: 5| Step: 6
Training loss: 2.103429482625673
Validation loss: 2.4961350526611246

Epoch: 5| Step: 7
Training loss: 2.4929452539363024
Validation loss: 2.4970506854548455

Epoch: 5| Step: 8
Training loss: 2.342173949260334
Validation loss: 2.5031632974102314

Epoch: 5| Step: 9
Training loss: 2.1781256217353624
Validation loss: 2.515040730613603

Epoch: 5| Step: 10
Training loss: 2.541595510101675
Validation loss: 2.517913590822346

Epoch: 5| Step: 11
Training loss: 2.8260648189111404
Validation loss: 2.5240827036742455

Epoch: 270| Step: 0
Training loss: 2.1665850648429856
Validation loss: 2.5232097259172774

Epoch: 5| Step: 1
Training loss: 2.109167696338263
Validation loss: 2.5197609690740856

Epoch: 5| Step: 2
Training loss: 2.5917240181451553
Validation loss: 2.5170670141695006

Epoch: 5| Step: 3
Training loss: 2.045966026628179
Validation loss: 2.5101120963211927

Epoch: 5| Step: 4
Training loss: 2.1264598544179414
Validation loss: 2.5223499155023354

Epoch: 5| Step: 5
Training loss: 2.22188690225281
Validation loss: 2.5124209160639173

Epoch: 5| Step: 6
Training loss: 2.6359832415995927
Validation loss: 2.4971078594608827

Epoch: 5| Step: 7
Training loss: 2.3310565285261897
Validation loss: 2.5069597285771366

Epoch: 5| Step: 8
Training loss: 2.2882046182342277
Validation loss: 2.5102755254954623

Epoch: 5| Step: 9
Training loss: 2.459631485512646
Validation loss: 2.5211627576817075

Epoch: 5| Step: 10
Training loss: 2.5988538233066842
Validation loss: 2.5218933270248174

Epoch: 5| Step: 11
Training loss: 3.289442316809224
Validation loss: 2.526978424569901

Epoch: 271| Step: 0
Training loss: 2.5717763703393115
Validation loss: 2.5465951231407247

Epoch: 5| Step: 1
Training loss: 2.1290647115266177
Validation loss: 2.5800057381065886

Epoch: 5| Step: 2
Training loss: 2.4274677305109784
Validation loss: 2.638569364667155

Epoch: 5| Step: 3
Training loss: 2.52370203479912
Validation loss: 2.7000692474939534

Epoch: 5| Step: 4
Training loss: 2.0340940079246463
Validation loss: 2.701414860930428

Epoch: 5| Step: 5
Training loss: 2.3562419375172903
Validation loss: 2.640625970603268

Epoch: 5| Step: 6
Training loss: 2.7725355804059033
Validation loss: 2.5550770010608184

Epoch: 5| Step: 7
Training loss: 2.4651774378551057
Validation loss: 2.5229139975846837

Epoch: 5| Step: 8
Training loss: 2.4397935102984203
Validation loss: 2.488218828712862

Epoch: 5| Step: 9
Training loss: 2.2697282137732246
Validation loss: 2.486148832758388

Epoch: 5| Step: 10
Training loss: 2.5511262201824687
Validation loss: 2.4867596967168115

Epoch: 5| Step: 11
Training loss: 2.5506971266516887
Validation loss: 2.4852894033916635

Epoch: 272| Step: 0
Training loss: 2.391295594549933
Validation loss: 2.490681184065569

Epoch: 5| Step: 1
Training loss: 2.25963679863855
Validation loss: 2.4926951458384594

Epoch: 5| Step: 2
Training loss: 2.919917955682988
Validation loss: 2.491200489803365

Epoch: 5| Step: 3
Training loss: 2.5949434039755905
Validation loss: 2.4911889294956047

Epoch: 5| Step: 4
Training loss: 2.5354968578369554
Validation loss: 2.4931575477715207

Epoch: 5| Step: 5
Training loss: 2.4994006392128996
Validation loss: 2.4808655428971162

Epoch: 5| Step: 6
Training loss: 1.7542868288373004
Validation loss: 2.4879279651895323

Epoch: 5| Step: 7
Training loss: 2.5450630529420932
Validation loss: 2.495646472987881

Epoch: 5| Step: 8
Training loss: 2.6190730803767206
Validation loss: 2.4995767433773604

Epoch: 5| Step: 9
Training loss: 2.151634805720692
Validation loss: 2.5089451182132816

Epoch: 5| Step: 10
Training loss: 2.629137139575235
Validation loss: 2.5161998755346513

Epoch: 5| Step: 11
Training loss: 1.295634722233835
Validation loss: 2.534292025762012

Epoch: 273| Step: 0
Training loss: 2.1844146768571093
Validation loss: 2.562426872295455

Epoch: 5| Step: 1
Training loss: 2.10283318999077
Validation loss: 2.5551339555550476

Epoch: 5| Step: 2
Training loss: 2.3018118811930237
Validation loss: 2.545919090726675

Epoch: 5| Step: 3
Training loss: 2.5471721078021283
Validation loss: 2.547579275943588

Epoch: 5| Step: 4
Training loss: 2.2945248783946584
Validation loss: 2.554886054859389

Epoch: 5| Step: 5
Training loss: 2.56938149887744
Validation loss: 2.5626856186171265

Epoch: 5| Step: 6
Training loss: 1.908337185229727
Validation loss: 2.5442007667527404

Epoch: 5| Step: 7
Training loss: 2.086922288092119
Validation loss: 2.534941246347532

Epoch: 5| Step: 8
Training loss: 3.012105202031946
Validation loss: 2.539814912868361

Epoch: 5| Step: 9
Training loss: 2.596795282161702
Validation loss: 2.51848009050053

Epoch: 5| Step: 10
Training loss: 2.4052274438609045
Validation loss: 2.507374068584599

Epoch: 5| Step: 11
Training loss: 0.4600031159129552
Validation loss: 2.5015546137383438

Epoch: 274| Step: 0
Training loss: 2.358901323681112
Validation loss: 2.497186782621038

Epoch: 5| Step: 1
Training loss: 2.3856465480152096
Validation loss: 2.485610494759165

Epoch: 5| Step: 2
Training loss: 2.1786696816115763
Validation loss: 2.4841159909464947

Epoch: 5| Step: 3
Training loss: 2.8806398883549877
Validation loss: 2.488993133165832

Epoch: 5| Step: 4
Training loss: 1.8574360587203256
Validation loss: 2.485774687577071

Epoch: 5| Step: 5
Training loss: 1.9581499081466307
Validation loss: 2.483885084986188

Epoch: 5| Step: 6
Training loss: 2.1572769594328185
Validation loss: 2.4877435251899933

Epoch: 5| Step: 7
Training loss: 2.991976817709315
Validation loss: 2.499893380911543

Epoch: 5| Step: 8
Training loss: 2.6081199140245444
Validation loss: 2.490433991852566

Epoch: 5| Step: 9
Training loss: 2.7432640114512195
Validation loss: 2.4847925463059983

Epoch: 5| Step: 10
Training loss: 2.4472418538101293
Validation loss: 2.493613242858719

Epoch: 5| Step: 11
Training loss: 1.083536801426595
Validation loss: 2.505026936985852

Epoch: 275| Step: 0
Training loss: 2.0478477205123116
Validation loss: 2.5196880952182354

Epoch: 5| Step: 1
Training loss: 2.624810802362613
Validation loss: 2.540512293137387

Epoch: 5| Step: 2
Training loss: 2.374529038965594
Validation loss: 2.536640197270762

Epoch: 5| Step: 3
Training loss: 2.5373343352561224
Validation loss: 2.534908554915754

Epoch: 5| Step: 4
Training loss: 2.281410211659342
Validation loss: 2.5128283662504245

Epoch: 5| Step: 5
Training loss: 2.4290481167672566
Validation loss: 2.529141053809031

Epoch: 5| Step: 6
Training loss: 2.027138760296718
Validation loss: 2.50918040298947

Epoch: 5| Step: 7
Training loss: 2.4071382146566593
Validation loss: 2.494982504397904

Epoch: 5| Step: 8
Training loss: 2.110727851221035
Validation loss: 2.520675651620878

Epoch: 5| Step: 9
Training loss: 2.3024711684949475
Validation loss: 2.5074466109621887

Epoch: 5| Step: 10
Training loss: 2.4526936340107572
Validation loss: 2.5245830034187664

Epoch: 5| Step: 11
Training loss: 3.3368579032532844
Validation loss: 2.5126508740220386

Epoch: 276| Step: 0
Training loss: 2.49403939157441
Validation loss: 2.5286499138644314

Epoch: 5| Step: 1
Training loss: 1.8716497371697143
Validation loss: 2.5518396935974064

Epoch: 5| Step: 2
Training loss: 2.7339071691027184
Validation loss: 2.5694538564480984

Epoch: 5| Step: 3
Training loss: 2.1997069770567417
Validation loss: 2.5776264209497053

Epoch: 5| Step: 4
Training loss: 2.0115313692693046
Validation loss: 2.5712543450916736

Epoch: 5| Step: 5
Training loss: 2.140480426444879
Validation loss: 2.5534710179981026

Epoch: 5| Step: 6
Training loss: 2.582294029449166
Validation loss: 2.5628147087061293

Epoch: 5| Step: 7
Training loss: 2.2500822264163514
Validation loss: 2.5619718658680326

Epoch: 5| Step: 8
Training loss: 2.769022950093504
Validation loss: 2.5398331866342425

Epoch: 5| Step: 9
Training loss: 2.5038249319503065
Validation loss: 2.5225777396543885

Epoch: 5| Step: 10
Training loss: 2.4576369662452637
Validation loss: 2.5091178404399725

Epoch: 5| Step: 11
Training loss: 2.3821587337962407
Validation loss: 2.5113906487851123

Epoch: 277| Step: 0
Training loss: 2.643299351153469
Validation loss: 2.522899905043304

Epoch: 5| Step: 1
Training loss: 2.835298828845478
Validation loss: 2.5120005214721246

Epoch: 5| Step: 2
Training loss: 2.296323022236818
Validation loss: 2.513018416698394

Epoch: 5| Step: 3
Training loss: 1.8512569127027816
Validation loss: 2.5107499504191906

Epoch: 5| Step: 4
Training loss: 2.4921918049837917
Validation loss: 2.5125968153727016

Epoch: 5| Step: 5
Training loss: 2.6563769141900213
Validation loss: 2.5286259138041

Epoch: 5| Step: 6
Training loss: 2.3203114724718894
Validation loss: 2.515839139144666

Epoch: 5| Step: 7
Training loss: 2.524659418987988
Validation loss: 2.5459457332586743

Epoch: 5| Step: 8
Training loss: 1.9094776282067487
Validation loss: 2.56258015778629

Epoch: 5| Step: 9
Training loss: 2.1390205551950348
Validation loss: 2.5516948105783834

Epoch: 5| Step: 10
Training loss: 2.2422708635417536
Validation loss: 2.542171560361379

Epoch: 5| Step: 11
Training loss: 1.8396846111178375
Validation loss: 2.511488726471687

Epoch: 278| Step: 0
Training loss: 2.2681401465575863
Validation loss: 2.4886743862762337

Epoch: 5| Step: 1
Training loss: 1.9765631031141002
Validation loss: 2.4883169893246264

Epoch: 5| Step: 2
Training loss: 2.232602036249274
Validation loss: 2.4816921951521476

Epoch: 5| Step: 3
Training loss: 2.693369023760137
Validation loss: 2.4878256523922855

Epoch: 5| Step: 4
Training loss: 2.016526604614762
Validation loss: 2.4753862991443696

Epoch: 5| Step: 5
Training loss: 2.537215655701899
Validation loss: 2.491958822922011

Epoch: 5| Step: 6
Training loss: 2.7993965929258513
Validation loss: 2.488059001520973

Epoch: 5| Step: 7
Training loss: 2.308697641684639
Validation loss: 2.498877992778929

Epoch: 5| Step: 8
Training loss: 2.794461706254385
Validation loss: 2.4908576534390403

Epoch: 5| Step: 9
Training loss: 1.8910487937421712
Validation loss: 2.4918070176847005

Epoch: 5| Step: 10
Training loss: 2.561190945348026
Validation loss: 2.5055273068417536

Epoch: 5| Step: 11
Training loss: 2.5770729461518154
Validation loss: 2.4958217912469687

Epoch: 279| Step: 0
Training loss: 2.3367532689928834
Validation loss: 2.518279008395125

Epoch: 5| Step: 1
Training loss: 1.8046542077895096
Validation loss: 2.5157816879568706

Epoch: 5| Step: 2
Training loss: 2.566138498863442
Validation loss: 2.5534867041750666

Epoch: 5| Step: 3
Training loss: 2.481168969510386
Validation loss: 2.57101917311306

Epoch: 5| Step: 4
Training loss: 2.3957462681969455
Validation loss: 2.6019437145946265

Epoch: 5| Step: 5
Training loss: 2.609358644719748
Validation loss: 2.5957848521910463

Epoch: 5| Step: 6
Training loss: 3.0014934001635893
Validation loss: 2.595632578453478

Epoch: 5| Step: 7
Training loss: 2.042162763566893
Validation loss: 2.55572080956111

Epoch: 5| Step: 8
Training loss: 2.3908871774958174
Validation loss: 2.5389154323142873

Epoch: 5| Step: 9
Training loss: 2.316463682032019
Validation loss: 2.512311190221323

Epoch: 5| Step: 10
Training loss: 2.0230566424870404
Validation loss: 2.5068107775109283

Epoch: 5| Step: 11
Training loss: 3.1179343645823048
Validation loss: 2.5068637760658943

Epoch: 280| Step: 0
Training loss: 2.083523728889406
Validation loss: 2.5035525591395533

Epoch: 5| Step: 1
Training loss: 2.1032904005832838
Validation loss: 2.5038399372653206

Epoch: 5| Step: 2
Training loss: 2.0480998793252585
Validation loss: 2.502981791882311

Epoch: 5| Step: 3
Training loss: 2.2714703101053164
Validation loss: 2.5027068663188223

Epoch: 5| Step: 4
Training loss: 1.9416920012211518
Validation loss: 2.5070760879635916

Epoch: 5| Step: 5
Training loss: 2.716110000057006
Validation loss: 2.520750565917193

Epoch: 5| Step: 6
Training loss: 2.4549907222657956
Validation loss: 2.50796261156921

Epoch: 5| Step: 7
Training loss: 2.465959734610386
Validation loss: 2.5220329737828227

Epoch: 5| Step: 8
Training loss: 2.5380836830261244
Validation loss: 2.523124037603762

Epoch: 5| Step: 9
Training loss: 2.462652087477051
Validation loss: 2.518128935835366

Epoch: 5| Step: 10
Training loss: 2.788426259877586
Validation loss: 2.5449455535325805

Epoch: 5| Step: 11
Training loss: 1.5859596739473185
Validation loss: 2.5282235439539207

Epoch: 281| Step: 0
Training loss: 2.230237851993559
Validation loss: 2.55192426588027

Epoch: 5| Step: 1
Training loss: 2.4491580061247262
Validation loss: 2.5717175517009987

Epoch: 5| Step: 2
Training loss: 2.004010946948215
Validation loss: 2.546429827268231

Epoch: 5| Step: 3
Training loss: 2.0519988015154635
Validation loss: 2.5842424274899605

Epoch: 5| Step: 4
Training loss: 2.766988558217206
Validation loss: 2.5418950913661287

Epoch: 5| Step: 5
Training loss: 1.9667311110956271
Validation loss: 2.5532795314955394

Epoch: 5| Step: 6
Training loss: 2.651822338062353
Validation loss: 2.5508313799649867

Epoch: 5| Step: 7
Training loss: 2.551361905848615
Validation loss: 2.5040696319423925

Epoch: 5| Step: 8
Training loss: 2.306824705060866
Validation loss: 2.5213802285605023

Epoch: 5| Step: 9
Training loss: 2.442530210028997
Validation loss: 2.528571839171549

Epoch: 5| Step: 10
Training loss: 2.627570664665146
Validation loss: 2.526708410041117

Epoch: 5| Step: 11
Training loss: 2.114583007612031
Validation loss: 2.5072894991928854

Epoch: 282| Step: 0
Training loss: 2.1935428532452783
Validation loss: 2.5099510114506827

Epoch: 5| Step: 1
Training loss: 2.1099256008825917
Validation loss: 2.5154044092105177

Epoch: 5| Step: 2
Training loss: 1.8834133218416247
Validation loss: 2.5157586846065096

Epoch: 5| Step: 3
Training loss: 2.4085996971094072
Validation loss: 2.5091469562516084

Epoch: 5| Step: 4
Training loss: 2.367285395949129
Validation loss: 2.5105476536484224

Epoch: 5| Step: 5
Training loss: 1.9495196215377628
Validation loss: 2.532938276459027

Epoch: 5| Step: 6
Training loss: 2.104115350179346
Validation loss: 2.531153955717501

Epoch: 5| Step: 7
Training loss: 2.6396593664509695
Validation loss: 2.5141294745072558

Epoch: 5| Step: 8
Training loss: 3.0404987618723456
Validation loss: 2.511535301648201

Epoch: 5| Step: 9
Training loss: 2.344674398595871
Validation loss: 2.5226107936940685

Epoch: 5| Step: 10
Training loss: 2.349401349049067
Validation loss: 2.518634421644581

Epoch: 5| Step: 11
Training loss: 2.4706016067335375
Validation loss: 2.5157548622115447

Epoch: 283| Step: 0
Training loss: 2.4489017749931197
Validation loss: 2.5234744184409355

Epoch: 5| Step: 1
Training loss: 2.4330450575262934
Validation loss: 2.501371325451732

Epoch: 5| Step: 2
Training loss: 2.830643517755172
Validation loss: 2.5229831873164343

Epoch: 5| Step: 3
Training loss: 2.3253462420828535
Validation loss: 2.509530308004024

Epoch: 5| Step: 4
Training loss: 2.4954049797334337
Validation loss: 2.5192007079639005

Epoch: 5| Step: 5
Training loss: 2.391735342636963
Validation loss: 2.5187701630712156

Epoch: 5| Step: 6
Training loss: 2.144682220536973
Validation loss: 2.5318783541377567

Epoch: 5| Step: 7
Training loss: 2.470869771893489
Validation loss: 2.5270706021291423

Epoch: 5| Step: 8
Training loss: 2.0440102834349543
Validation loss: 2.5229190140221593

Epoch: 5| Step: 9
Training loss: 1.7736248303928894
Validation loss: 2.5419125802774984

Epoch: 5| Step: 10
Training loss: 2.3618480529620784
Validation loss: 2.531511112764125

Epoch: 5| Step: 11
Training loss: 1.452674303610412
Validation loss: 2.520336363542078

Epoch: 284| Step: 0
Training loss: 1.8438763979242598
Validation loss: 2.506121186920162

Epoch: 5| Step: 1
Training loss: 1.9720112265369816
Validation loss: 2.5196338248257533

Epoch: 5| Step: 2
Training loss: 2.6780418035878224
Validation loss: 2.522935047654957

Epoch: 5| Step: 3
Training loss: 2.2383334435041355
Validation loss: 2.5239291032102744

Epoch: 5| Step: 4
Training loss: 2.449344710863352
Validation loss: 2.5110024616895807

Epoch: 5| Step: 5
Training loss: 2.6959402928858003
Validation loss: 2.5098887451688174

Epoch: 5| Step: 6
Training loss: 2.401500721461362
Validation loss: 2.507077498587417

Epoch: 5| Step: 7
Training loss: 2.186067384538091
Validation loss: 2.510024987263567

Epoch: 5| Step: 8
Training loss: 2.7279950507502253
Validation loss: 2.515938279289163

Epoch: 5| Step: 9
Training loss: 2.0760107543093453
Validation loss: 2.511825958700869

Epoch: 5| Step: 10
Training loss: 2.175020752731061
Validation loss: 2.4991958516947963

Epoch: 5| Step: 11
Training loss: 2.52435323911252
Validation loss: 2.5178320070845985

Epoch: 285| Step: 0
Training loss: 2.5521322647905347
Validation loss: 2.5066016969119613

Epoch: 5| Step: 1
Training loss: 2.4047739034476567
Validation loss: 2.518136837728611

Epoch: 5| Step: 2
Training loss: 2.249112059966905
Validation loss: 2.51924895053014

Epoch: 5| Step: 3
Training loss: 2.8743423041341907
Validation loss: 2.5021217323128537

Epoch: 5| Step: 4
Training loss: 2.248485055172696
Validation loss: 2.513417066786442

Epoch: 5| Step: 5
Training loss: 1.6998850250632977
Validation loss: 2.504730469828282

Epoch: 5| Step: 6
Training loss: 2.0265574317698167
Validation loss: 2.5068663518606358

Epoch: 5| Step: 7
Training loss: 2.277522870146807
Validation loss: 2.5092903231391315

Epoch: 5| Step: 8
Training loss: 2.3545181102108845
Validation loss: 2.509338928444787

Epoch: 5| Step: 9
Training loss: 2.754343936849742
Validation loss: 2.51495417936788

Epoch: 5| Step: 10
Training loss: 1.9202568250986034
Validation loss: 2.5064572230387774

Epoch: 5| Step: 11
Training loss: 2.3871428617021
Validation loss: 2.509770018577441

Epoch: 286| Step: 0
Training loss: 1.9884130288092632
Validation loss: 2.5205438318921844

Epoch: 5| Step: 1
Training loss: 2.272409842604836
Validation loss: 2.5214968916673204

Epoch: 5| Step: 2
Training loss: 2.4282082718914637
Validation loss: 2.526676771937203

Epoch: 5| Step: 3
Training loss: 2.5046843511474766
Validation loss: 2.5162693525310136

Epoch: 5| Step: 4
Training loss: 1.8557284845962434
Validation loss: 2.536106534970354

Epoch: 5| Step: 5
Training loss: 2.2665186730858355
Validation loss: 2.5202249523202584

Epoch: 5| Step: 6
Training loss: 2.678897752411838
Validation loss: 2.5424518929146656

Epoch: 5| Step: 7
Training loss: 2.5290693143891616
Validation loss: 2.5359097317663712

Epoch: 5| Step: 8
Training loss: 2.232733063189041
Validation loss: 2.5348945604086923

Epoch: 5| Step: 9
Training loss: 2.3271294839962007
Validation loss: 2.550459654057868

Epoch: 5| Step: 10
Training loss: 2.4192526125076315
Validation loss: 2.531114743390555

Epoch: 5| Step: 11
Training loss: 2.5056233105329078
Validation loss: 2.5207975216888565

Epoch: 287| Step: 0
Training loss: 1.6019042604105542
Validation loss: 2.5100317115097623

Epoch: 5| Step: 1
Training loss: 1.9816284395877568
Validation loss: 2.505809677396328

Epoch: 5| Step: 2
Training loss: 2.466589163086708
Validation loss: 2.5082976009721762

Epoch: 5| Step: 3
Training loss: 2.21057546502402
Validation loss: 2.5091721878341526

Epoch: 5| Step: 4
Training loss: 3.364808696053188
Validation loss: 2.5107846814732206

Epoch: 5| Step: 5
Training loss: 2.5864214948431776
Validation loss: 2.519820807326447

Epoch: 5| Step: 6
Training loss: 2.4672087675099625
Validation loss: 2.5167720462092027

Epoch: 5| Step: 7
Training loss: 2.1052244819677353
Validation loss: 2.519360424628272

Epoch: 5| Step: 8
Training loss: 1.7463904348495034
Validation loss: 2.5209520577355233

Epoch: 5| Step: 9
Training loss: 2.5803558870049446
Validation loss: 2.5347199106133314

Epoch: 5| Step: 10
Training loss: 2.271027536283222
Validation loss: 2.5495430636687

Epoch: 5| Step: 11
Training loss: 1.575069226151406
Validation loss: 2.512704757552094

Epoch: 288| Step: 0
Training loss: 1.962215296023971
Validation loss: 2.52664644467151

Epoch: 5| Step: 1
Training loss: 2.5359352925044787
Validation loss: 2.5205563256414343

Epoch: 5| Step: 2
Training loss: 2.273844679192392
Validation loss: 2.5000184495563023

Epoch: 5| Step: 3
Training loss: 2.679060793179398
Validation loss: 2.511021643405568

Epoch: 5| Step: 4
Training loss: 2.466641068540525
Validation loss: 2.5098819097156597

Epoch: 5| Step: 5
Training loss: 2.171984992226585
Validation loss: 2.5208150623909944

Epoch: 5| Step: 6
Training loss: 2.3674585168380404
Validation loss: 2.517239060713466

Epoch: 5| Step: 7
Training loss: 2.2642352873459073
Validation loss: 2.52102247164389

Epoch: 5| Step: 8
Training loss: 2.094826265109747
Validation loss: 2.521885448719361

Epoch: 5| Step: 9
Training loss: 3.025523488801008
Validation loss: 2.5131062481583872

Epoch: 5| Step: 10
Training loss: 1.7875695141700398
Validation loss: 2.5221852324054184

Epoch: 5| Step: 11
Training loss: 0.6737036769774528
Validation loss: 2.5336776794779263

Epoch: 289| Step: 0
Training loss: 2.013928786426997
Validation loss: 2.524674347691637

Epoch: 5| Step: 1
Training loss: 2.023186981128514
Validation loss: 2.5196434843708784

Epoch: 5| Step: 2
Training loss: 2.3593459980173734
Validation loss: 2.5280649334436545

Epoch: 5| Step: 3
Training loss: 2.4868936307620997
Validation loss: 2.520673369752197

Epoch: 5| Step: 4
Training loss: 2.6681751713131603
Validation loss: 2.5228521063682146

Epoch: 5| Step: 5
Training loss: 2.1840494461750177
Validation loss: 2.5107093432016763

Epoch: 5| Step: 6
Training loss: 2.2108746307384815
Validation loss: 2.532324966929353

Epoch: 5| Step: 7
Training loss: 2.4947043598058816
Validation loss: 2.541018455478383

Epoch: 5| Step: 8
Training loss: 2.1380462706923202
Validation loss: 2.5193353305710144

Epoch: 5| Step: 9
Training loss: 1.8174020970144902
Validation loss: 2.5331625875123143

Epoch: 5| Step: 10
Training loss: 3.00566741945155
Validation loss: 2.5318762667754866

Epoch: 5| Step: 11
Training loss: 1.6325921452967123
Validation loss: 2.5234911473007275

Epoch: 290| Step: 0
Training loss: 2.759285422714792
Validation loss: 2.532302339427514

Epoch: 5| Step: 1
Training loss: 2.2015356729577245
Validation loss: 2.528388002701885

Epoch: 5| Step: 2
Training loss: 2.4827099864771007
Validation loss: 2.527970190695615

Epoch: 5| Step: 3
Training loss: 2.355057359389708
Validation loss: 2.529313323803295

Epoch: 5| Step: 4
Training loss: 2.155073204124797
Validation loss: 2.535114619104765

Epoch: 5| Step: 5
Training loss: 2.1672421449181614
Validation loss: 2.523092047446475

Epoch: 5| Step: 6
Training loss: 1.8973257946316788
Validation loss: 2.5378279516329587

Epoch: 5| Step: 7
Training loss: 2.3913528232033694
Validation loss: 2.531196411177786

Epoch: 5| Step: 8
Training loss: 2.3797538763095445
Validation loss: 2.5406903765635698

Epoch: 5| Step: 9
Training loss: 2.4920354814712944
Validation loss: 2.550195132998577

Epoch: 5| Step: 10
Training loss: 2.2027261792372244
Validation loss: 2.531218550137403

Epoch: 5| Step: 11
Training loss: 1.519039751207015
Validation loss: 2.541359880307564

Epoch: 291| Step: 0
Training loss: 1.8194034909920498
Validation loss: 2.5379522194692252

Epoch: 5| Step: 1
Training loss: 2.141414079230831
Validation loss: 2.524078458986109

Epoch: 5| Step: 2
Training loss: 2.5970285675510523
Validation loss: 2.533244697113997

Epoch: 5| Step: 3
Training loss: 2.3773805834185504
Validation loss: 2.5288695358551205

Epoch: 5| Step: 4
Training loss: 2.7055760555374087
Validation loss: 2.54217509489622

Epoch: 5| Step: 5
Training loss: 2.247936786305845
Validation loss: 2.533838757542917

Epoch: 5| Step: 6
Training loss: 2.5779017496260592
Validation loss: 2.553034629763228

Epoch: 5| Step: 7
Training loss: 2.2081209716414616
Validation loss: 2.560657191379522

Epoch: 5| Step: 8
Training loss: 2.468910888064931
Validation loss: 2.5428551816020657

Epoch: 5| Step: 9
Training loss: 1.659488031880345
Validation loss: 2.530153829951568

Epoch: 5| Step: 10
Training loss: 2.5048185641431444
Validation loss: 2.5295179862014887

Epoch: 5| Step: 11
Training loss: 2.0488928247350997
Validation loss: 2.528024368578119

Epoch: 292| Step: 0
Training loss: 1.8051454697845657
Validation loss: 2.52832809217191

Epoch: 5| Step: 1
Training loss: 2.4123469803266717
Validation loss: 2.5153903180391026

Epoch: 5| Step: 2
Training loss: 2.700227187453138
Validation loss: 2.524822964930042

Epoch: 5| Step: 3
Training loss: 2.6942663843981087
Validation loss: 2.509820370009031

Epoch: 5| Step: 4
Training loss: 2.2991684861802186
Validation loss: 2.526084657021402

Epoch: 5| Step: 5
Training loss: 1.9909446878801942
Validation loss: 2.5232031509659736

Epoch: 5| Step: 6
Training loss: 2.2382872150335
Validation loss: 2.5245305854078897

Epoch: 5| Step: 7
Training loss: 2.575708632505666
Validation loss: 2.530407239388737

Epoch: 5| Step: 8
Training loss: 2.002261790229072
Validation loss: 2.532889941641857

Epoch: 5| Step: 9
Training loss: 2.525753883145121
Validation loss: 2.532802737399785

Epoch: 5| Step: 10
Training loss: 2.000502761591021
Validation loss: 2.538594720249801

Epoch: 5| Step: 11
Training loss: 2.4248560125332297
Validation loss: 2.543711259439129

Epoch: 293| Step: 0
Training loss: 2.2269449541323088
Validation loss: 2.5503645337940224

Epoch: 5| Step: 1
Training loss: 2.2391666704903552
Validation loss: 2.5520981859566443

Epoch: 5| Step: 2
Training loss: 1.6483135447971269
Validation loss: 2.563352307411463

Epoch: 5| Step: 3
Training loss: 2.3516514149835204
Validation loss: 2.5608496546346737

Epoch: 5| Step: 4
Training loss: 2.649044991197198
Validation loss: 2.5474205329949124

Epoch: 5| Step: 5
Training loss: 2.3495537679153453
Validation loss: 2.5287591703861163

Epoch: 5| Step: 6
Training loss: 2.6858989470923325
Validation loss: 2.5264918185137524

Epoch: 5| Step: 7
Training loss: 2.6072214342433084
Validation loss: 2.498365296924293

Epoch: 5| Step: 8
Training loss: 2.001549478171378
Validation loss: 2.5172967588303776

Epoch: 5| Step: 9
Training loss: 1.9991855751257543
Validation loss: 2.5091831941566007

Epoch: 5| Step: 10
Training loss: 2.5280736137609283
Validation loss: 2.5110013816381027

Epoch: 5| Step: 11
Training loss: 2.2450498593237542
Validation loss: 2.51202340896141

Epoch: 294| Step: 0
Training loss: 2.0376803218875534
Validation loss: 2.5279170962659694

Epoch: 5| Step: 1
Training loss: 2.169186581688624
Validation loss: 2.5097195790540856

Epoch: 5| Step: 2
Training loss: 2.7527374167958043
Validation loss: 2.5089125315158807

Epoch: 5| Step: 3
Training loss: 2.562067041804894
Validation loss: 2.5322814652993144

Epoch: 5| Step: 4
Training loss: 2.41420698505093
Validation loss: 2.5484410719200374

Epoch: 5| Step: 5
Training loss: 1.7567532887295696
Validation loss: 2.547823852060856

Epoch: 5| Step: 6
Training loss: 2.2237710151730066
Validation loss: 2.5474492500261494

Epoch: 5| Step: 7
Training loss: 1.9536780222925216
Validation loss: 2.523773962212255

Epoch: 5| Step: 8
Training loss: 2.469290951722833
Validation loss: 2.5517214356301756

Epoch: 5| Step: 9
Training loss: 2.2706923018124887
Validation loss: 2.5590498190186093

Epoch: 5| Step: 10
Training loss: 2.4180825349969197
Validation loss: 2.5664920260312134

Epoch: 5| Step: 11
Training loss: 3.08914581788482
Validation loss: 2.5618983314471104

Epoch: 295| Step: 0
Training loss: 2.2883985161811005
Validation loss: 2.55510667400918

Epoch: 5| Step: 1
Training loss: 2.65571876150276
Validation loss: 2.5570123070577453

Epoch: 5| Step: 2
Training loss: 2.0138994267798185
Validation loss: 2.5556040411705694

Epoch: 5| Step: 3
Training loss: 2.110327611502217
Validation loss: 2.545127116759101

Epoch: 5| Step: 4
Training loss: 2.1109055229379687
Validation loss: 2.5366280705855035

Epoch: 5| Step: 5
Training loss: 2.1241893343977463
Validation loss: 2.5422367618713917

Epoch: 5| Step: 6
Training loss: 1.8259335938711723
Validation loss: 2.556146576397306

Epoch: 5| Step: 7
Training loss: 2.0555587487868974
Validation loss: 2.548720488827216

Epoch: 5| Step: 8
Training loss: 2.2258395979019645
Validation loss: 2.541507502298929

Epoch: 5| Step: 9
Training loss: 2.4020107627773633
Validation loss: 2.543860139110441

Epoch: 5| Step: 10
Training loss: 3.1358135052521345
Validation loss: 2.5420553693174814

Epoch: 5| Step: 11
Training loss: 3.04520577898919
Validation loss: 2.5427953365047182

Epoch: 296| Step: 0
Training loss: 2.2819294374942087
Validation loss: 2.5403532532726554

Epoch: 5| Step: 1
Training loss: 2.402379279145302
Validation loss: 2.565985472396436

Epoch: 5| Step: 2
Training loss: 1.8498013647511005
Validation loss: 2.540922952457936

Epoch: 5| Step: 3
Training loss: 2.0662280981215013
Validation loss: 2.565579149964827

Epoch: 5| Step: 4
Training loss: 2.7904234912734367
Validation loss: 2.5399988492202157

Epoch: 5| Step: 5
Training loss: 1.9336740188121297
Validation loss: 2.545509534015833

Epoch: 5| Step: 6
Training loss: 2.1069315003067524
Validation loss: 2.53712123117278

Epoch: 5| Step: 7
Training loss: 2.4336923055760797
Validation loss: 2.5355815011583274

Epoch: 5| Step: 8
Training loss: 2.0232732404219935
Validation loss: 2.5117974948750788

Epoch: 5| Step: 9
Training loss: 2.5678604627709443
Validation loss: 2.546204954678067

Epoch: 5| Step: 10
Training loss: 2.733367821623155
Validation loss: 2.5271016653034923

Epoch: 5| Step: 11
Training loss: 1.5991672375260382
Validation loss: 2.5262888613742422

Epoch: 297| Step: 0
Training loss: 2.7922636409169823
Validation loss: 2.526116801976451

Epoch: 5| Step: 1
Training loss: 2.032369569203991
Validation loss: 2.530842807183879

Epoch: 5| Step: 2
Training loss: 1.5433457312418946
Validation loss: 2.5211218413986756

Epoch: 5| Step: 3
Training loss: 2.300562279842969
Validation loss: 2.5323283720213507

Epoch: 5| Step: 4
Training loss: 2.4492594397661698
Validation loss: 2.522074497624836

Epoch: 5| Step: 5
Training loss: 2.0582075824497426
Validation loss: 2.5223199280912403

Epoch: 5| Step: 6
Training loss: 1.9917840287280013
Validation loss: 2.5509042715070342

Epoch: 5| Step: 7
Training loss: 2.1888624035846043
Validation loss: 2.5552683412016606

Epoch: 5| Step: 8
Training loss: 2.969080856358041
Validation loss: 2.5419260593585893

Epoch: 5| Step: 9
Training loss: 2.18894158954509
Validation loss: 2.570551160652618

Epoch: 5| Step: 10
Training loss: 2.2860613733965653
Validation loss: 2.540889251092007

Epoch: 5| Step: 11
Training loss: 2.7946246593584507
Validation loss: 2.5532210960342074

Epoch: 298| Step: 0
Training loss: 2.4264139280996706
Validation loss: 2.5615803301038147

Epoch: 5| Step: 1
Training loss: 1.8602113525739803
Validation loss: 2.561456326259857

Epoch: 5| Step: 2
Training loss: 2.4894374873856693
Validation loss: 2.57181651743033

Epoch: 5| Step: 3
Training loss: 2.303752644910448
Validation loss: 2.533733848598086

Epoch: 5| Step: 4
Training loss: 2.362982411051543
Validation loss: 2.5449287958995193

Epoch: 5| Step: 5
Training loss: 2.4720634726180855
Validation loss: 2.5461074827170536

Epoch: 5| Step: 6
Training loss: 2.0285615001642743
Validation loss: 2.535318100067829

Epoch: 5| Step: 7
Training loss: 2.187841334278949
Validation loss: 2.5533352927515

Epoch: 5| Step: 8
Training loss: 2.396278428459777
Validation loss: 2.526811900477355

Epoch: 5| Step: 9
Training loss: 2.33223264709553
Validation loss: 2.5318190166631194

Epoch: 5| Step: 10
Training loss: 2.3153054000080413
Validation loss: 2.5158280829679884

Epoch: 5| Step: 11
Training loss: 2.925966060897899
Validation loss: 2.523400931393545

Epoch: 299| Step: 0
Training loss: 2.662369713259559
Validation loss: 2.54369679785102

Epoch: 5| Step: 1
Training loss: 2.051201245125439
Validation loss: 2.5385599822608635

Epoch: 5| Step: 2
Training loss: 2.5973406836636124
Validation loss: 2.5009410716116367

Epoch: 5| Step: 3
Training loss: 2.213704887805
Validation loss: 2.5337683508606323

Epoch: 5| Step: 4
Training loss: 2.196119709872468
Validation loss: 2.522829047387067

Epoch: 5| Step: 5
Training loss: 2.7194313861644672
Validation loss: 2.516962363279856

Epoch: 5| Step: 6
Training loss: 2.182602112993245
Validation loss: 2.5539871212351537

Epoch: 5| Step: 7
Training loss: 2.179444002066144
Validation loss: 2.5623646347609403

Epoch: 5| Step: 8
Training loss: 2.09529205016441
Validation loss: 2.5891221174068964

Epoch: 5| Step: 9
Training loss: 2.267018172646855
Validation loss: 2.572207122654969

Epoch: 5| Step: 10
Training loss: 1.877724448349015
Validation loss: 2.5557333801143582

Epoch: 5| Step: 11
Training loss: 3.3139760309617605
Validation loss: 2.5517474608729014

Epoch: 300| Step: 0
Training loss: 2.7135621483164667
Validation loss: 2.5308128576002016

Epoch: 5| Step: 1
Training loss: 2.701587747044406
Validation loss: 2.5397122378306434

Epoch: 5| Step: 2
Training loss: 2.454498003973026
Validation loss: 2.523859333254145

Epoch: 5| Step: 3
Training loss: 2.2564319230808927
Validation loss: 2.5279266101970017

Epoch: 5| Step: 4
Training loss: 2.4967343936522943
Validation loss: 2.515418241624738

Epoch: 5| Step: 5
Training loss: 2.0177394215308038
Validation loss: 2.4940718141213063

Epoch: 5| Step: 6
Training loss: 2.164246809509471
Validation loss: 2.4946755433316707

Epoch: 5| Step: 7
Training loss: 2.13787063151566
Validation loss: 2.5101709337910165

Epoch: 5| Step: 8
Training loss: 1.5640582897270614
Validation loss: 2.5238954071521755

Epoch: 5| Step: 9
Training loss: 2.5837874679912307
Validation loss: 2.49881514446049

Epoch: 5| Step: 10
Training loss: 2.17788391968322
Validation loss: 2.5549806024240573

Epoch: 5| Step: 11
Training loss: 2.427894445836662
Validation loss: 2.5554818212426955

Testing loss: 2.072883748477309

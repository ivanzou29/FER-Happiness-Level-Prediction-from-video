Epoch: 1| Step: 0
Training loss: 5.98044490814209
Validation loss: 5.2482577959696455

Epoch: 6| Step: 1
Training loss: 4.360037326812744
Validation loss: 5.246529420216878

Epoch: 6| Step: 2
Training loss: 5.089711666107178
Validation loss: 5.2448883056640625

Epoch: 6| Step: 3
Training loss: 5.244115829467773
Validation loss: 5.243309895197551

Epoch: 6| Step: 4
Training loss: 5.339095115661621
Validation loss: 5.241727511088054

Epoch: 6| Step: 5
Training loss: 6.19097900390625
Validation loss: 5.240221977233887

Epoch: 6| Step: 6
Training loss: 5.951942443847656
Validation loss: 5.238686879475911

Epoch: 6| Step: 7
Training loss: 4.549773216247559
Validation loss: 5.2371745109558105

Epoch: 6| Step: 8
Training loss: 5.511690616607666
Validation loss: 5.23561708132426

Epoch: 6| Step: 9
Training loss: 4.929320335388184
Validation loss: 5.234002987543742

Epoch: 6| Step: 10
Training loss: 4.833276748657227
Validation loss: 5.232390960057576

Epoch: 6| Step: 11
Training loss: 5.894674301147461
Validation loss: 5.230751673380534

Epoch: 6| Step: 12
Training loss: 4.537407875061035
Validation loss: 5.228955268859863

Epoch: 6| Step: 13
Training loss: 5.8904643058776855
Validation loss: 5.227105061213176

Epoch: 2| Step: 0
Training loss: 4.908514022827148
Validation loss: 5.225175698598226

Epoch: 6| Step: 1
Training loss: 5.595259666442871
Validation loss: 5.223169326782227

Epoch: 6| Step: 2
Training loss: 5.916164398193359
Validation loss: 5.221043984095256

Epoch: 6| Step: 3
Training loss: 6.302467346191406
Validation loss: 5.2188271681467695

Epoch: 6| Step: 4
Training loss: 5.462531089782715
Validation loss: 5.216422398885091

Epoch: 6| Step: 5
Training loss: 5.0530805587768555
Validation loss: 5.213915109634399

Epoch: 6| Step: 6
Training loss: 5.125384330749512
Validation loss: 5.211418787638347

Epoch: 6| Step: 7
Training loss: 4.625990867614746
Validation loss: 5.208781719207764

Epoch: 6| Step: 8
Training loss: 4.903576850891113
Validation loss: 5.205936908721924

Epoch: 6| Step: 9
Training loss: 5.378177642822266
Validation loss: 5.203021208445231

Epoch: 6| Step: 10
Training loss: 3.9402294158935547
Validation loss: 5.2000211874643965

Epoch: 6| Step: 11
Training loss: 5.233256816864014
Validation loss: 5.196849981943767

Epoch: 6| Step: 12
Training loss: 5.061891555786133
Validation loss: 5.193559408187866

Epoch: 6| Step: 13
Training loss: 6.40756368637085
Validation loss: 5.190263271331787

Epoch: 3| Step: 0
Training loss: 5.595190048217773
Validation loss: 5.186883290608724

Epoch: 6| Step: 1
Training loss: 5.930745601654053
Validation loss: 5.183289925257365

Epoch: 6| Step: 2
Training loss: 5.809065341949463
Validation loss: 5.179501493771871

Epoch: 6| Step: 3
Training loss: 4.289494514465332
Validation loss: 5.175835927327474

Epoch: 6| Step: 4
Training loss: 5.31226110458374
Validation loss: 5.171877940495809

Epoch: 6| Step: 5
Training loss: 4.904055118560791
Validation loss: 5.1677811940511065

Epoch: 6| Step: 6
Training loss: 5.133802890777588
Validation loss: 5.16349442799886

Epoch: 6| Step: 7
Training loss: 5.238947868347168
Validation loss: 5.159323374430339

Epoch: 6| Step: 8
Training loss: 5.702170372009277
Validation loss: 5.154890775680542

Epoch: 6| Step: 9
Training loss: 4.983758926391602
Validation loss: 5.15026330947876

Epoch: 6| Step: 10
Training loss: 4.861823558807373
Validation loss: 5.145737965901692

Epoch: 6| Step: 11
Training loss: 5.359140872955322
Validation loss: 5.140893618265788

Epoch: 6| Step: 12
Training loss: 6.093832015991211
Validation loss: 5.13605801264445

Epoch: 6| Step: 13
Training loss: 4.044458866119385
Validation loss: 5.131049633026123

Epoch: 4| Step: 0
Training loss: 5.098213195800781
Validation loss: 5.126303672790527

Epoch: 6| Step: 1
Training loss: 5.278404235839844
Validation loss: 5.121191342671712

Epoch: 6| Step: 2
Training loss: 5.706959247589111
Validation loss: 5.116139570871989

Epoch: 6| Step: 3
Training loss: 5.641275405883789
Validation loss: 5.110779921213786

Epoch: 6| Step: 4
Training loss: 4.612801551818848
Validation loss: 5.105545520782471

Epoch: 6| Step: 5
Training loss: 6.057010650634766
Validation loss: 5.100210189819336

Epoch: 6| Step: 6
Training loss: 5.526073455810547
Validation loss: 5.0944468180338545

Epoch: 6| Step: 7
Training loss: 4.62431001663208
Validation loss: 5.089106400807698

Epoch: 6| Step: 8
Training loss: 4.941818714141846
Validation loss: 5.0835316975911455

Epoch: 6| Step: 9
Training loss: 4.799077033996582
Validation loss: 5.077760616938273

Epoch: 6| Step: 10
Training loss: 4.589372634887695
Validation loss: 5.0719226996103925

Epoch: 6| Step: 11
Training loss: 4.379194259643555
Validation loss: 5.0658806165059405

Epoch: 6| Step: 12
Training loss: 5.441703796386719
Validation loss: 5.059828519821167

Epoch: 6| Step: 13
Training loss: 5.63427734375
Validation loss: 5.053860187530518

Epoch: 5| Step: 0
Training loss: 5.6620683670043945
Validation loss: 5.047739426294963

Epoch: 6| Step: 1
Training loss: 4.535892963409424
Validation loss: 5.0415873527526855

Epoch: 6| Step: 2
Training loss: 4.570099353790283
Validation loss: 5.035476764043172

Epoch: 6| Step: 3
Training loss: 3.949050188064575
Validation loss: 5.029005845387776

Epoch: 6| Step: 4
Training loss: 5.832814693450928
Validation loss: 5.022606531778972

Epoch: 6| Step: 5
Training loss: 4.571551322937012
Validation loss: 5.016105890274048

Epoch: 6| Step: 6
Training loss: 5.590365409851074
Validation loss: 5.009858687718709

Epoch: 6| Step: 7
Training loss: 4.391270637512207
Validation loss: 5.00336496035258

Epoch: 6| Step: 8
Training loss: 6.021956443786621
Validation loss: 4.996702194213867

Epoch: 6| Step: 9
Training loss: 5.8094096183776855
Validation loss: 4.9900805950164795

Epoch: 6| Step: 10
Training loss: 6.027878761291504
Validation loss: 4.982964674631755

Epoch: 6| Step: 11
Training loss: 5.100619792938232
Validation loss: 4.976059754689534

Epoch: 6| Step: 12
Training loss: 4.078695297241211
Validation loss: 4.969273646672566

Epoch: 6| Step: 13
Training loss: 5.0634260177612305
Validation loss: 4.9623035589853925

Epoch: 6| Step: 0
Training loss: 5.220521450042725
Validation loss: 4.955183506011963

Epoch: 6| Step: 1
Training loss: 4.917636871337891
Validation loss: 4.947756290435791

Epoch: 6| Step: 2
Training loss: 5.685891151428223
Validation loss: 4.940586487452189

Epoch: 6| Step: 3
Training loss: 5.532985687255859
Validation loss: 4.93291163444519

Epoch: 6| Step: 4
Training loss: 5.072099685668945
Validation loss: 4.925652901331584

Epoch: 6| Step: 5
Training loss: 3.48604679107666
Validation loss: 4.9182329177856445

Epoch: 6| Step: 6
Training loss: 4.9992475509643555
Validation loss: 4.911121646563212

Epoch: 6| Step: 7
Training loss: 4.872287273406982
Validation loss: 4.903867085774739

Epoch: 6| Step: 8
Training loss: 5.908475875854492
Validation loss: 4.896461049715678

Epoch: 6| Step: 9
Training loss: 4.580933094024658
Validation loss: 4.889680703481038

Epoch: 6| Step: 10
Training loss: 5.056636810302734
Validation loss: 4.882679224014282

Epoch: 6| Step: 11
Training loss: 4.826071262359619
Validation loss: 4.875916083653768

Epoch: 6| Step: 12
Training loss: 4.6951446533203125
Validation loss: 4.869399388631185

Epoch: 6| Step: 13
Training loss: 5.036412715911865
Validation loss: 4.862993001937866

Epoch: 7| Step: 0
Training loss: 4.472141265869141
Validation loss: 4.8566741943359375

Epoch: 6| Step: 1
Training loss: 4.849410533905029
Validation loss: 4.85010548432668

Epoch: 6| Step: 2
Training loss: 5.898661136627197
Validation loss: 4.843782186508179

Epoch: 6| Step: 3
Training loss: 5.315971374511719
Validation loss: 4.8372312386830645

Epoch: 6| Step: 4
Training loss: 5.323936939239502
Validation loss: 4.831088066101074

Epoch: 6| Step: 5
Training loss: 5.149120330810547
Validation loss: 4.825192411740621

Epoch: 6| Step: 6
Training loss: 3.703761577606201
Validation loss: 4.818730354309082

Epoch: 6| Step: 7
Training loss: 4.897861003875732
Validation loss: 4.812891960144043

Epoch: 6| Step: 8
Training loss: 5.624508857727051
Validation loss: 4.8067208131154375

Epoch: 6| Step: 9
Training loss: 4.74016809463501
Validation loss: 4.800489664077759

Epoch: 6| Step: 10
Training loss: 3.6841936111450195
Validation loss: 4.794168472290039

Epoch: 6| Step: 11
Training loss: 5.032725811004639
Validation loss: 4.787791013717651

Epoch: 6| Step: 12
Training loss: 5.507640361785889
Validation loss: 4.781315008799235

Epoch: 6| Step: 13
Training loss: 4.446642875671387
Validation loss: 4.775396903355916

Epoch: 8| Step: 0
Training loss: 5.439504623413086
Validation loss: 4.769100785255432

Epoch: 6| Step: 1
Training loss: 5.070652008056641
Validation loss: 4.762542565663655

Epoch: 6| Step: 2
Training loss: 3.9668965339660645
Validation loss: 4.756720304489136

Epoch: 6| Step: 3
Training loss: 5.217288017272949
Validation loss: 4.750548839569092

Epoch: 6| Step: 4
Training loss: 3.5074310302734375
Validation loss: 4.7442848682403564

Epoch: 6| Step: 5
Training loss: 4.397550582885742
Validation loss: 4.738348563512166

Epoch: 6| Step: 6
Training loss: 4.694911956787109
Validation loss: 4.732279380162557

Epoch: 6| Step: 7
Training loss: 3.859818696975708
Validation loss: 4.726243654886882

Epoch: 6| Step: 8
Training loss: 4.626115798950195
Validation loss: 4.720294078191121

Epoch: 6| Step: 9
Training loss: 5.25278902053833
Validation loss: 4.7145687739054365

Epoch: 6| Step: 10
Training loss: 5.82668399810791
Validation loss: 4.708662788073222

Epoch: 6| Step: 11
Training loss: 3.9843428134918213
Validation loss: 4.702303449312846

Epoch: 6| Step: 12
Training loss: 5.8266167640686035
Validation loss: 4.696536501248677

Epoch: 6| Step: 13
Training loss: 5.811584949493408
Validation loss: 4.69034210840861

Epoch: 9| Step: 0
Training loss: 3.957604169845581
Validation loss: 4.684507369995117

Epoch: 6| Step: 1
Training loss: 5.238331317901611
Validation loss: 4.678485155105591

Epoch: 6| Step: 2
Training loss: 3.9849600791931152
Validation loss: 4.673134883244832

Epoch: 6| Step: 3
Training loss: 5.468104839324951
Validation loss: 4.667151133219401

Epoch: 6| Step: 4
Training loss: 5.017752647399902
Validation loss: 4.661221901575725

Epoch: 6| Step: 5
Training loss: 3.9232256412506104
Validation loss: 4.655584176381429

Epoch: 6| Step: 6
Training loss: 5.321591377258301
Validation loss: 4.648996591567993

Epoch: 6| Step: 7
Training loss: 5.136438369750977
Validation loss: 4.643223285675049

Epoch: 6| Step: 8
Training loss: 5.0521159172058105
Validation loss: 4.637211322784424

Epoch: 6| Step: 9
Training loss: 3.940185785293579
Validation loss: 4.631097594896953

Epoch: 6| Step: 10
Training loss: 5.00815486907959
Validation loss: 4.625112771987915

Epoch: 6| Step: 11
Training loss: 4.743354797363281
Validation loss: 4.618953704833984

Epoch: 6| Step: 12
Training loss: 5.868091583251953
Validation loss: 4.61328383286794

Epoch: 6| Step: 13
Training loss: 3.7252821922302246
Validation loss: 4.607101758321126

Epoch: 10| Step: 0
Training loss: 5.159259796142578
Validation loss: 4.601390441258748

Epoch: 6| Step: 1
Training loss: 3.6645009517669678
Validation loss: 4.595214764277141

Epoch: 6| Step: 2
Training loss: 3.652301549911499
Validation loss: 4.589497804641724

Epoch: 6| Step: 3
Training loss: 4.499046325683594
Validation loss: 4.583516200383504

Epoch: 6| Step: 4
Training loss: 5.417656898498535
Validation loss: 4.578105886777242

Epoch: 6| Step: 5
Training loss: 2.933861255645752
Validation loss: 4.571973204612732

Epoch: 6| Step: 6
Training loss: 4.564652442932129
Validation loss: 4.5660713116327925

Epoch: 6| Step: 7
Training loss: 5.0899200439453125
Validation loss: 4.560313145319621

Epoch: 6| Step: 8
Training loss: 5.35271692276001
Validation loss: 4.554835836092631

Epoch: 6| Step: 9
Training loss: 4.315084457397461
Validation loss: 4.548965692520142

Epoch: 6| Step: 10
Training loss: 6.106184005737305
Validation loss: 4.543577512105306

Epoch: 6| Step: 11
Training loss: 4.448495864868164
Validation loss: 4.537955363591512

Epoch: 6| Step: 12
Training loss: 5.228039741516113
Validation loss: 4.53268547852834

Epoch: 6| Step: 13
Training loss: 4.854907989501953
Validation loss: 4.527358611424764

Epoch: 11| Step: 0
Training loss: 5.139285087585449
Validation loss: 4.521916389465332

Epoch: 6| Step: 1
Training loss: 5.396280288696289
Validation loss: 4.516522725423177

Epoch: 6| Step: 2
Training loss: 4.7573137283325195
Validation loss: 4.511400739351909

Epoch: 6| Step: 3
Training loss: 4.935774803161621
Validation loss: 4.506354610125224

Epoch: 6| Step: 4
Training loss: 2.8385708332061768
Validation loss: 4.500644167264302

Epoch: 6| Step: 5
Training loss: 4.326154708862305
Validation loss: 4.495446999867757

Epoch: 6| Step: 6
Training loss: 4.288191318511963
Validation loss: 4.489872495333354

Epoch: 6| Step: 7
Training loss: 4.388415336608887
Validation loss: 4.484575351079305

Epoch: 6| Step: 8
Training loss: 4.502257347106934
Validation loss: 4.4797855615615845

Epoch: 6| Step: 9
Training loss: 5.026309490203857
Validation loss: 4.474196990331014

Epoch: 6| Step: 10
Training loss: 5.139064311981201
Validation loss: 4.468489170074463

Epoch: 6| Step: 11
Training loss: 4.13350248336792
Validation loss: 4.463347276051839

Epoch: 6| Step: 12
Training loss: 4.918133735656738
Validation loss: 4.458496769269307

Epoch: 6| Step: 13
Training loss: 4.533942222595215
Validation loss: 4.4531136353810625

Epoch: 12| Step: 0
Training loss: 4.024279594421387
Validation loss: 4.448091705640157

Epoch: 6| Step: 1
Training loss: 4.09391450881958
Validation loss: 4.4426320393880205

Epoch: 6| Step: 2
Training loss: 4.494009017944336
Validation loss: 4.438040653864543

Epoch: 6| Step: 3
Training loss: 5.4220170974731445
Validation loss: 4.43288791179657

Epoch: 6| Step: 4
Training loss: 4.171051502227783
Validation loss: 4.4277723630269366

Epoch: 6| Step: 5
Training loss: 4.986331939697266
Validation loss: 4.422873894373576

Epoch: 6| Step: 6
Training loss: 4.666896820068359
Validation loss: 4.417831818262736

Epoch: 6| Step: 7
Training loss: 3.9355759620666504
Validation loss: 4.41261351108551

Epoch: 6| Step: 8
Training loss: 3.653017520904541
Validation loss: 4.4081540902455645

Epoch: 6| Step: 9
Training loss: 5.098917007446289
Validation loss: 4.403570572535197

Epoch: 6| Step: 10
Training loss: 5.448473930358887
Validation loss: 4.398304740587871

Epoch: 6| Step: 11
Training loss: 4.873603820800781
Validation loss: 4.393698692321777

Epoch: 6| Step: 12
Training loss: 3.8184752464294434
Validation loss: 4.388134082158406

Epoch: 6| Step: 13
Training loss: 4.708569526672363
Validation loss: 4.384113629659017

Epoch: 13| Step: 0
Training loss: 3.498199701309204
Validation loss: 4.3794169425964355

Epoch: 6| Step: 1
Training loss: 3.9543135166168213
Validation loss: 4.374507864316304

Epoch: 6| Step: 2
Training loss: 3.526432991027832
Validation loss: 4.369555791219075

Epoch: 6| Step: 3
Training loss: 4.2285003662109375
Validation loss: 4.365276416142781

Epoch: 6| Step: 4
Training loss: 4.817959785461426
Validation loss: 4.360971450805664

Epoch: 6| Step: 5
Training loss: 4.666715621948242
Validation loss: 4.3564151128133135

Epoch: 6| Step: 6
Training loss: 5.151420593261719
Validation loss: 4.351290305455525

Epoch: 6| Step: 7
Training loss: 4.043274402618408
Validation loss: 4.3469367027282715

Epoch: 6| Step: 8
Training loss: 4.372159004211426
Validation loss: 4.342643658320109

Epoch: 6| Step: 9
Training loss: 5.4011125564575195
Validation loss: 4.3382484912872314

Epoch: 6| Step: 10
Training loss: 4.498116493225098
Validation loss: 4.333007733027141

Epoch: 6| Step: 11
Training loss: 4.774784088134766
Validation loss: 4.328609784444173

Epoch: 6| Step: 12
Training loss: 3.9557275772094727
Validation loss: 4.324406623840332

Epoch: 6| Step: 13
Training loss: 5.678167819976807
Validation loss: 4.32038680712382

Epoch: 14| Step: 0
Training loss: 5.647425174713135
Validation loss: 4.31641964117686

Epoch: 6| Step: 1
Training loss: 3.5389065742492676
Validation loss: 4.311926444371541

Epoch: 6| Step: 2
Training loss: 4.294916152954102
Validation loss: 4.307835181554158

Epoch: 6| Step: 3
Training loss: 4.434909820556641
Validation loss: 4.303403854370117

Epoch: 6| Step: 4
Training loss: 4.023920059204102
Validation loss: 4.2990758419036865

Epoch: 6| Step: 5
Training loss: 5.540626525878906
Validation loss: 4.294657985369365

Epoch: 6| Step: 6
Training loss: 3.3703255653381348
Validation loss: 4.2908852100372314

Epoch: 6| Step: 7
Training loss: 3.7339882850646973
Validation loss: 4.287087122599284

Epoch: 6| Step: 8
Training loss: 5.561857223510742
Validation loss: 4.28296434879303

Epoch: 6| Step: 9
Training loss: 4.70999813079834
Validation loss: 4.279076337814331

Epoch: 6| Step: 10
Training loss: 5.0025224685668945
Validation loss: 4.274938543637593

Epoch: 6| Step: 11
Training loss: 3.742368221282959
Validation loss: 4.270831783612569

Epoch: 6| Step: 12
Training loss: 3.7889113426208496
Validation loss: 4.2668488423029585

Epoch: 6| Step: 13
Training loss: 4.410445213317871
Validation loss: 4.262759407361348

Epoch: 15| Step: 0
Training loss: 4.1935319900512695
Validation loss: 4.2587235768636065

Epoch: 6| Step: 1
Training loss: 3.730271577835083
Validation loss: 4.254909475644429

Epoch: 6| Step: 2
Training loss: 4.976690769195557
Validation loss: 4.25183629989624

Epoch: 6| Step: 3
Training loss: 5.174238681793213
Validation loss: 4.247597297032674

Epoch: 6| Step: 4
Training loss: 4.372252464294434
Validation loss: 4.243877291679382

Epoch: 6| Step: 5
Training loss: 4.499650001525879
Validation loss: 4.240145603815715

Epoch: 6| Step: 6
Training loss: 4.869008541107178
Validation loss: 4.235980908075969

Epoch: 6| Step: 7
Training loss: 4.6604413986206055
Validation loss: 4.232339223225911

Epoch: 6| Step: 8
Training loss: 3.97318172454834
Validation loss: 4.228687087694804

Epoch: 6| Step: 9
Training loss: 3.7822210788726807
Validation loss: 4.225094556808472

Epoch: 6| Step: 10
Training loss: 4.373502731323242
Validation loss: 4.221227844556172

Epoch: 6| Step: 11
Training loss: 4.02406120300293
Validation loss: 4.217602690060933

Epoch: 6| Step: 12
Training loss: 4.196157455444336
Validation loss: 4.213400840759277

Epoch: 6| Step: 13
Training loss: 4.248888969421387
Validation loss: 4.209794998168945

Epoch: 16| Step: 0
Training loss: 3.8404364585876465
Validation loss: 4.205832203229268

Epoch: 6| Step: 1
Training loss: 4.756143569946289
Validation loss: 4.201852639516194

Epoch: 6| Step: 2
Training loss: 3.8333592414855957
Validation loss: 4.1979394753774

Epoch: 6| Step: 3
Training loss: 3.6301517486572266
Validation loss: 4.194441080093384

Epoch: 6| Step: 4
Training loss: 4.535879135131836
Validation loss: 4.19058100382487

Epoch: 6| Step: 5
Training loss: 4.794135093688965
Validation loss: 4.186646978060405

Epoch: 6| Step: 6
Training loss: 4.112736225128174
Validation loss: 4.182920734087626

Epoch: 6| Step: 7
Training loss: 3.5658795833587646
Validation loss: 4.178921540578206

Epoch: 6| Step: 8
Training loss: 4.557838439941406
Validation loss: 4.175211509068807

Epoch: 6| Step: 9
Training loss: 4.824897289276123
Validation loss: 4.171499133110046

Epoch: 6| Step: 10
Training loss: 4.805224895477295
Validation loss: 4.167633255322774

Epoch: 6| Step: 11
Training loss: 4.357583045959473
Validation loss: 4.16385285059611

Epoch: 6| Step: 12
Training loss: 4.331516265869141
Validation loss: 4.160264174143474

Epoch: 6| Step: 13
Training loss: 4.469388961791992
Validation loss: 4.156726598739624

Epoch: 17| Step: 0
Training loss: 4.197012424468994
Validation loss: 4.152912378311157

Epoch: 6| Step: 1
Training loss: 4.522729396820068
Validation loss: 4.149246573448181

Epoch: 6| Step: 2
Training loss: 4.337325096130371
Validation loss: 4.145845810572307

Epoch: 6| Step: 3
Training loss: 3.780179977416992
Validation loss: 4.142137368520101

Epoch: 6| Step: 4
Training loss: 4.971948146820068
Validation loss: 4.138672828674316

Epoch: 6| Step: 5
Training loss: 5.6458282470703125
Validation loss: 4.134942293167114

Epoch: 6| Step: 6
Training loss: 3.445542097091675
Validation loss: 4.131040930747986

Epoch: 6| Step: 7
Training loss: 4.05123233795166
Validation loss: 4.127460082372029

Epoch: 6| Step: 8
Training loss: 4.525215148925781
Validation loss: 4.123604456583659

Epoch: 6| Step: 9
Training loss: 4.311708450317383
Validation loss: 4.1202612320582075

Epoch: 6| Step: 10
Training loss: 4.681897163391113
Validation loss: 4.116496324539185

Epoch: 6| Step: 11
Training loss: 4.36369514465332
Validation loss: 4.113010962804158

Epoch: 6| Step: 12
Training loss: 3.925537347793579
Validation loss: 4.108911196390788

Epoch: 6| Step: 13
Training loss: 2.999192714691162
Validation loss: 4.105413158734639

Epoch: 18| Step: 0
Training loss: 3.175346851348877
Validation loss: 4.1018838087717695

Epoch: 6| Step: 1
Training loss: 3.842006206512451
Validation loss: 4.098543604214986

Epoch: 6| Step: 2
Training loss: 4.70941162109375
Validation loss: 4.0950166781743365

Epoch: 6| Step: 3
Training loss: 4.412196159362793
Validation loss: 4.091867804527283

Epoch: 6| Step: 4
Training loss: 3.7492897510528564
Validation loss: 4.088204105695088

Epoch: 6| Step: 5
Training loss: 3.457010269165039
Validation loss: 4.084664066632588

Epoch: 6| Step: 6
Training loss: 4.682234764099121
Validation loss: 4.081320842107137

Epoch: 6| Step: 7
Training loss: 4.924410820007324
Validation loss: 4.077478249867757

Epoch: 6| Step: 8
Training loss: 5.203824043273926
Validation loss: 4.073980609575908

Epoch: 6| Step: 9
Training loss: 4.292092323303223
Validation loss: 4.070217291514079

Epoch: 6| Step: 10
Training loss: 4.196155548095703
Validation loss: 4.066406170527141

Epoch: 6| Step: 11
Training loss: 3.462634563446045
Validation loss: 4.06271489461263

Epoch: 6| Step: 12
Training loss: 4.944696426391602
Validation loss: 4.059352318445842

Epoch: 6| Step: 13
Training loss: 4.034739971160889
Validation loss: 4.055527925491333

Epoch: 19| Step: 0
Training loss: 3.9524006843566895
Validation loss: 4.052170991897583

Epoch: 6| Step: 1
Training loss: 4.28786563873291
Validation loss: 4.048883716265361

Epoch: 6| Step: 2
Training loss: 3.142519474029541
Validation loss: 4.0453068017959595

Epoch: 6| Step: 3
Training loss: 3.760139226913452
Validation loss: 4.041994134585063

Epoch: 6| Step: 4
Training loss: 4.761719703674316
Validation loss: 4.038477977116902

Epoch: 6| Step: 5
Training loss: 5.1412858963012695
Validation loss: 4.034897526105245

Epoch: 6| Step: 6
Training loss: 4.574779033660889
Validation loss: 4.031255761782329

Epoch: 6| Step: 7
Training loss: 4.109198093414307
Validation loss: 4.028038024902344

Epoch: 6| Step: 8
Training loss: 3.115992307662964
Validation loss: 4.024274627367656

Epoch: 6| Step: 9
Training loss: 4.558805465698242
Validation loss: 4.021146575609843

Epoch: 6| Step: 10
Training loss: 4.069403648376465
Validation loss: 4.017171223958333

Epoch: 6| Step: 11
Training loss: 5.206972122192383
Validation loss: 4.013602455457051

Epoch: 6| Step: 12
Training loss: 3.3392224311828613
Validation loss: 4.010040640830994

Epoch: 6| Step: 13
Training loss: 4.4012298583984375
Validation loss: 4.006747563680013

Epoch: 20| Step: 0
Training loss: 3.249129295349121
Validation loss: 4.003075321515401

Epoch: 6| Step: 1
Training loss: 4.841513633728027
Validation loss: 3.999639868736267

Epoch: 6| Step: 2
Training loss: 4.84868049621582
Validation loss: 3.9957741101582847

Epoch: 6| Step: 3
Training loss: 5.170172691345215
Validation loss: 3.9925079345703125

Epoch: 6| Step: 4
Training loss: 3.4339609146118164
Validation loss: 3.98857851823171

Epoch: 6| Step: 5
Training loss: 4.384639263153076
Validation loss: 3.985158085823059

Epoch: 6| Step: 6
Training loss: 4.281744956970215
Validation loss: 3.9813778003056846

Epoch: 6| Step: 7
Training loss: 3.803471088409424
Validation loss: 3.978040417035421

Epoch: 6| Step: 8
Training loss: 3.485621452331543
Validation loss: 3.9743816455205283

Epoch: 6| Step: 9
Training loss: 3.6218767166137695
Validation loss: 3.9710156122843423

Epoch: 6| Step: 10
Training loss: 4.322042465209961
Validation loss: 3.967293381690979

Epoch: 6| Step: 11
Training loss: 3.8657078742980957
Validation loss: 3.9635874032974243

Epoch: 6| Step: 12
Training loss: 5.129535675048828
Validation loss: 3.9598594109217324

Epoch: 6| Step: 13
Training loss: 3.3073902130126953
Validation loss: 3.9561588764190674

Epoch: 21| Step: 0
Training loss: 4.579686164855957
Validation loss: 3.952493747075399

Epoch: 6| Step: 1
Training loss: 3.823871612548828
Validation loss: 3.9488176107406616

Epoch: 6| Step: 2
Training loss: 4.518843650817871
Validation loss: 3.945271650950114

Epoch: 6| Step: 3
Training loss: 4.632691383361816
Validation loss: 3.94142218430837

Epoch: 6| Step: 4
Training loss: 4.83975887298584
Validation loss: 3.937471111615499

Epoch: 6| Step: 5
Training loss: 4.291799068450928
Validation loss: 3.9336803754170737

Epoch: 6| Step: 6
Training loss: 4.021113395690918
Validation loss: 3.9296148220698037

Epoch: 6| Step: 7
Training loss: 3.9913127422332764
Validation loss: 3.9258013566335044

Epoch: 6| Step: 8
Training loss: 4.066454887390137
Validation loss: 3.9222185214360556

Epoch: 6| Step: 9
Training loss: 4.115887641906738
Validation loss: 3.9182982047398887

Epoch: 6| Step: 10
Training loss: 5.134583473205566
Validation loss: 3.914742350578308

Epoch: 6| Step: 11
Training loss: 3.2162325382232666
Validation loss: 3.910775661468506

Epoch: 6| Step: 12
Training loss: 3.4026997089385986
Validation loss: 3.907263477643331

Epoch: 6| Step: 13
Training loss: 2.4606828689575195
Validation loss: 3.903548320134481

Epoch: 22| Step: 0
Training loss: 4.1587419509887695
Validation loss: 3.900142232577006

Epoch: 6| Step: 1
Training loss: 3.2628660202026367
Validation loss: 3.896708846092224

Epoch: 6| Step: 2
Training loss: 4.257596015930176
Validation loss: 3.893205444018046

Epoch: 6| Step: 3
Training loss: 4.133912563323975
Validation loss: 3.889837463696798

Epoch: 6| Step: 4
Training loss: 4.358729362487793
Validation loss: 3.8859291474024453

Epoch: 6| Step: 5
Training loss: 4.048983097076416
Validation loss: 3.8824830452601113

Epoch: 6| Step: 6
Training loss: 5.608683109283447
Validation loss: 3.8787179787953696

Epoch: 6| Step: 7
Training loss: 4.148038387298584
Validation loss: 3.8752127091089883

Epoch: 6| Step: 8
Training loss: 4.314948558807373
Validation loss: 3.8717477321624756

Epoch: 6| Step: 9
Training loss: 3.230940341949463
Validation loss: 3.867939313252767

Epoch: 6| Step: 10
Training loss: 4.063167095184326
Validation loss: 3.8644636074701944

Epoch: 6| Step: 11
Training loss: 3.416688919067383
Validation loss: 3.860576113065084

Epoch: 6| Step: 12
Training loss: 3.216050624847412
Validation loss: 3.856934348742167

Epoch: 6| Step: 13
Training loss: 4.182608127593994
Validation loss: 3.8545462687810264

Epoch: 23| Step: 0
Training loss: 4.52099609375
Validation loss: 3.8500044345855713

Epoch: 6| Step: 1
Training loss: 3.621338129043579
Validation loss: 3.8455929358800254

Epoch: 6| Step: 2
Training loss: 3.5821218490600586
Validation loss: 3.841664671897888

Epoch: 6| Step: 3
Training loss: 3.471809148788452
Validation loss: 3.8386623859405518

Epoch: 6| Step: 4
Training loss: 4.5810160636901855
Validation loss: 3.8349165121714273

Epoch: 6| Step: 5
Training loss: 3.939194679260254
Validation loss: 3.8295907179514566

Epoch: 6| Step: 6
Training loss: 3.485597610473633
Validation loss: 3.8257700204849243

Epoch: 6| Step: 7
Training loss: 3.0087690353393555
Validation loss: 3.822221557299296

Epoch: 6| Step: 8
Training loss: 5.607975006103516
Validation loss: 3.8187080224355063

Epoch: 6| Step: 9
Training loss: 3.4228012561798096
Validation loss: 3.8145474592844644

Epoch: 6| Step: 10
Training loss: 4.206096172332764
Validation loss: 3.8101276556650796

Epoch: 6| Step: 11
Training loss: 3.897779703140259
Validation loss: 3.80621870358785

Epoch: 6| Step: 12
Training loss: 4.556200981140137
Validation loss: 3.8020514249801636

Epoch: 6| Step: 13
Training loss: 3.818687915802002
Validation loss: 3.7984805504480996

Epoch: 24| Step: 0
Training loss: 4.24509334564209
Validation loss: 3.794995983441671

Epoch: 6| Step: 1
Training loss: 4.15805721282959
Validation loss: 3.7911784251530967

Epoch: 6| Step: 2
Training loss: 4.496090888977051
Validation loss: 3.7872690757115683

Epoch: 6| Step: 3
Training loss: 2.755338668823242
Validation loss: 3.782708247502645

Epoch: 6| Step: 4
Training loss: 2.7704505920410156
Validation loss: 3.778971036275228

Epoch: 6| Step: 5
Training loss: 4.158478736877441
Validation loss: 3.7749627431233725

Epoch: 6| Step: 6
Training loss: 4.292405605316162
Validation loss: 3.771324316660563

Epoch: 6| Step: 7
Training loss: 4.527109146118164
Validation loss: 3.7677919467290244

Epoch: 6| Step: 8
Training loss: 3.7321486473083496
Validation loss: 3.764017939567566

Epoch: 6| Step: 9
Training loss: 3.848325729370117
Validation loss: 3.760132670402527

Epoch: 6| Step: 10
Training loss: 3.684633255004883
Validation loss: 3.755402604738871

Epoch: 6| Step: 11
Training loss: 3.7245161533355713
Validation loss: 3.7512056827545166

Epoch: 6| Step: 12
Training loss: 4.861225128173828
Validation loss: 3.7477156718571982

Epoch: 6| Step: 13
Training loss: 3.7154245376586914
Validation loss: 3.7435643672943115

Epoch: 25| Step: 0
Training loss: 3.857377052307129
Validation loss: 3.7396053870519004

Epoch: 6| Step: 1
Training loss: 4.247993469238281
Validation loss: 3.7360079685846963

Epoch: 6| Step: 2
Training loss: 4.272800445556641
Validation loss: 3.732860247294108

Epoch: 6| Step: 3
Training loss: 3.380073070526123
Validation loss: 3.7281753619511924

Epoch: 6| Step: 4
Training loss: 3.651606559753418
Validation loss: 3.723638733228048

Epoch: 6| Step: 5
Training loss: 4.450906753540039
Validation loss: 3.7198214133580527

Epoch: 6| Step: 6
Training loss: 3.392099618911743
Validation loss: 3.7156728506088257

Epoch: 6| Step: 7
Training loss: 3.9646811485290527
Validation loss: 3.711674610773722

Epoch: 6| Step: 8
Training loss: 4.010587692260742
Validation loss: 3.7077276706695557

Epoch: 6| Step: 9
Training loss: 2.2825140953063965
Validation loss: 3.7036750316619873

Epoch: 6| Step: 10
Training loss: 3.7803592681884766
Validation loss: 3.6997180382410684

Epoch: 6| Step: 11
Training loss: 3.645606279373169
Validation loss: 3.6960216760635376

Epoch: 6| Step: 12
Training loss: 5.284002780914307
Validation loss: 3.692043741544088

Epoch: 6| Step: 13
Training loss: 4.032342433929443
Validation loss: 3.687856833140055

Epoch: 26| Step: 0
Training loss: 4.681761741638184
Validation loss: 3.684200127919515

Epoch: 6| Step: 1
Training loss: 3.6913721561431885
Validation loss: 3.679789145787557

Epoch: 6| Step: 2
Training loss: 3.8122739791870117
Validation loss: 3.6757514476776123

Epoch: 6| Step: 3
Training loss: 4.002058982849121
Validation loss: 3.6717191139856973

Epoch: 6| Step: 4
Training loss: 4.059976577758789
Validation loss: 3.6669575770696006

Epoch: 6| Step: 5
Training loss: 4.131017684936523
Validation loss: 3.663031498591105

Epoch: 6| Step: 6
Training loss: 3.3346238136291504
Validation loss: 3.65741495291392

Epoch: 6| Step: 7
Training loss: 3.047137498855591
Validation loss: 3.653490980466207

Epoch: 6| Step: 8
Training loss: 3.9143266677856445
Validation loss: 3.648258646329244

Epoch: 6| Step: 9
Training loss: 3.86631178855896
Validation loss: 3.6445455153783164

Epoch: 6| Step: 10
Training loss: 4.056140422821045
Validation loss: 3.639845689137777

Epoch: 6| Step: 11
Training loss: 3.695098876953125
Validation loss: 3.6359872420628867

Epoch: 6| Step: 12
Training loss: 3.7044894695281982
Validation loss: 3.631088693936666

Epoch: 6| Step: 13
Training loss: 3.458456516265869
Validation loss: 3.6270511150360107

Epoch: 27| Step: 0
Training loss: 3.8503715991973877
Validation loss: 3.6227129697799683

Epoch: 6| Step: 1
Training loss: 3.73030424118042
Validation loss: 3.6187169949213662

Epoch: 6| Step: 2
Training loss: 4.425398826599121
Validation loss: 3.61434276898702

Epoch: 6| Step: 3
Training loss: 4.0322065353393555
Validation loss: 3.610115885734558

Epoch: 6| Step: 4
Training loss: 3.885932445526123
Validation loss: 3.6055414279301963

Epoch: 6| Step: 5
Training loss: 4.387918949127197
Validation loss: 3.601492961247762

Epoch: 6| Step: 6
Training loss: 2.927870273590088
Validation loss: 3.5966286659240723

Epoch: 6| Step: 7
Training loss: 4.29161262512207
Validation loss: 3.591891964276632

Epoch: 6| Step: 8
Training loss: 3.132866621017456
Validation loss: 3.5879791180292764

Epoch: 6| Step: 9
Training loss: 4.193546295166016
Validation loss: 3.583540757497152

Epoch: 6| Step: 10
Training loss: 3.314970016479492
Validation loss: 3.578691919644674

Epoch: 6| Step: 11
Training loss: 4.1480817794799805
Validation loss: 3.57451597849528

Epoch: 6| Step: 12
Training loss: 2.8915185928344727
Validation loss: 3.5703017314275107

Epoch: 6| Step: 13
Training loss: 3.420492172241211
Validation loss: 3.566104292869568

Epoch: 28| Step: 0
Training loss: 4.007320880889893
Validation loss: 3.56176749865214

Epoch: 6| Step: 1
Training loss: 4.276972770690918
Validation loss: 3.557625333468119

Epoch: 6| Step: 2
Training loss: 3.462373733520508
Validation loss: 3.5529466470082602

Epoch: 6| Step: 3
Training loss: 2.831407308578491
Validation loss: 3.5488243897755942

Epoch: 6| Step: 4
Training loss: 3.2122509479522705
Validation loss: 3.5444244543711343

Epoch: 6| Step: 5
Training loss: 4.013429641723633
Validation loss: 3.540230909983317

Epoch: 6| Step: 6
Training loss: 4.032998085021973
Validation loss: 3.535633683204651

Epoch: 6| Step: 7
Training loss: 3.315554141998291
Validation loss: 3.5314088662465415

Epoch: 6| Step: 8
Training loss: 3.6380295753479004
Validation loss: 3.5267982482910156

Epoch: 6| Step: 9
Training loss: 3.2905466556549072
Validation loss: 3.5226706663767495

Epoch: 6| Step: 10
Training loss: 4.753750801086426
Validation loss: 3.518515110015869

Epoch: 6| Step: 11
Training loss: 3.2567451000213623
Validation loss: 3.513878663380941

Epoch: 6| Step: 12
Training loss: 3.5764801502227783
Validation loss: 3.5099549690882363

Epoch: 6| Step: 13
Training loss: 4.12904167175293
Validation loss: 3.5056234995524087

Epoch: 29| Step: 0
Training loss: 4.000062942504883
Validation loss: 3.5011815627415976

Epoch: 6| Step: 1
Training loss: 3.164405345916748
Validation loss: 3.4968814849853516

Epoch: 6| Step: 2
Training loss: 5.012547492980957
Validation loss: 3.4929088751475015

Epoch: 6| Step: 3
Training loss: 3.092963933944702
Validation loss: 3.488975405693054

Epoch: 6| Step: 4
Training loss: 3.216153144836426
Validation loss: 3.4847091833750405

Epoch: 6| Step: 5
Training loss: 4.822028160095215
Validation loss: 3.4800955851872764

Epoch: 6| Step: 6
Training loss: 4.220911026000977
Validation loss: 3.47543211778005

Epoch: 6| Step: 7
Training loss: 3.4076385498046875
Validation loss: 3.4712825616200766

Epoch: 6| Step: 8
Training loss: 4.157573699951172
Validation loss: 3.4667465686798096

Epoch: 6| Step: 9
Training loss: 3.589233875274658
Validation loss: 3.462440013885498

Epoch: 6| Step: 10
Training loss: 2.515754461288452
Validation loss: 3.4579561154047647

Epoch: 6| Step: 11
Training loss: 2.6553969383239746
Validation loss: 3.453592618306478

Epoch: 6| Step: 12
Training loss: 4.146904945373535
Validation loss: 3.449111580848694

Epoch: 6| Step: 13
Training loss: 2.953824996948242
Validation loss: 3.444630185763041

Epoch: 30| Step: 0
Training loss: 3.8926265239715576
Validation loss: 3.4397356510162354

Epoch: 6| Step: 1
Training loss: 3.7087297439575195
Validation loss: 3.435425877571106

Epoch: 6| Step: 2
Training loss: 4.8477067947387695
Validation loss: 3.430804451306661

Epoch: 6| Step: 3
Training loss: 3.353095531463623
Validation loss: 3.4261823495229087

Epoch: 6| Step: 4
Training loss: 3.1113362312316895
Validation loss: 3.4216338396072388

Epoch: 6| Step: 5
Training loss: 3.981081247329712
Validation loss: 3.417076269785563

Epoch: 6| Step: 6
Training loss: 3.5690829753875732
Validation loss: 3.4123822450637817

Epoch: 6| Step: 7
Training loss: 3.9625062942504883
Validation loss: 3.408029476801554

Epoch: 6| Step: 8
Training loss: 3.3790926933288574
Validation loss: 3.403532028198242

Epoch: 6| Step: 9
Training loss: 4.074380397796631
Validation loss: 3.3991053104400635

Epoch: 6| Step: 10
Training loss: 3.7186174392700195
Validation loss: 3.3944632609685264

Epoch: 6| Step: 11
Training loss: 3.3723936080932617
Validation loss: 3.390191674232483

Epoch: 6| Step: 12
Training loss: 2.8321948051452637
Validation loss: 3.385794917742411

Epoch: 6| Step: 13
Training loss: 2.3322372436523438
Validation loss: 3.381375312805176

Epoch: 31| Step: 0
Training loss: 4.059254169464111
Validation loss: 3.3769236405690513

Epoch: 6| Step: 1
Training loss: 3.8029115200042725
Validation loss: 3.37298854192098

Epoch: 6| Step: 2
Training loss: 3.506103515625
Validation loss: 3.368545889854431

Epoch: 6| Step: 3
Training loss: 3.328214645385742
Validation loss: 3.364122231801351

Epoch: 6| Step: 4
Training loss: 3.410104513168335
Validation loss: 3.3599326610565186

Epoch: 6| Step: 5
Training loss: 3.8950414657592773
Validation loss: 3.3556249141693115

Epoch: 6| Step: 6
Training loss: 4.3930864334106445
Validation loss: 3.3514649073282876

Epoch: 6| Step: 7
Training loss: 2.719050168991089
Validation loss: 3.3472288449605307

Epoch: 6| Step: 8
Training loss: 3.136991024017334
Validation loss: 3.342970291773478

Epoch: 6| Step: 9
Training loss: 3.617504596710205
Validation loss: 3.338902711868286

Epoch: 6| Step: 10
Training loss: 3.4887807369232178
Validation loss: 3.3345205783843994

Epoch: 6| Step: 11
Training loss: 3.6926612854003906
Validation loss: 3.3308218717575073

Epoch: 6| Step: 12
Training loss: 3.1906981468200684
Validation loss: 3.3261505762736

Epoch: 6| Step: 13
Training loss: 3.050677537918091
Validation loss: 3.3215337991714478

Epoch: 32| Step: 0
Training loss: 4.336300849914551
Validation loss: 3.317276398340861

Epoch: 6| Step: 1
Training loss: 3.5406625270843506
Validation loss: 3.3132302363713584

Epoch: 6| Step: 2
Training loss: 3.2088470458984375
Validation loss: 3.3089438676834106

Epoch: 6| Step: 3
Training loss: 3.214967727661133
Validation loss: 3.304550369580587

Epoch: 6| Step: 4
Training loss: 3.6892035007476807
Validation loss: 3.300293525060018

Epoch: 6| Step: 5
Training loss: 2.4283461570739746
Validation loss: 3.296020269393921

Epoch: 6| Step: 6
Training loss: 3.2543065547943115
Validation loss: 3.291861812273661

Epoch: 6| Step: 7
Training loss: 4.087258338928223
Validation loss: 3.2873806158701577

Epoch: 6| Step: 8
Training loss: 3.2344207763671875
Validation loss: 3.283775965372721

Epoch: 6| Step: 9
Training loss: 3.5458459854125977
Validation loss: 3.2792283296585083

Epoch: 6| Step: 10
Training loss: 3.128148317337036
Validation loss: 3.274691104888916

Epoch: 6| Step: 11
Training loss: 3.171109199523926
Validation loss: 3.2704264322916665

Epoch: 6| Step: 12
Training loss: 4.074976921081543
Validation loss: 3.2663187980651855

Epoch: 6| Step: 13
Training loss: 3.616248369216919
Validation loss: 3.2620479265848794

Epoch: 33| Step: 0
Training loss: 3.6771671772003174
Validation loss: 3.258077541987101

Epoch: 6| Step: 1
Training loss: 3.7096731662750244
Validation loss: 3.2536809047063193

Epoch: 6| Step: 2
Training loss: 3.831594467163086
Validation loss: 3.2490895986557007

Epoch: 6| Step: 3
Training loss: 2.714108467102051
Validation loss: 3.2446282704671225

Epoch: 6| Step: 4
Training loss: 3.169369697570801
Validation loss: 3.240231235822042

Epoch: 6| Step: 5
Training loss: 3.4067935943603516
Validation loss: 3.2358275651931763

Epoch: 6| Step: 6
Training loss: 3.5745925903320312
Validation loss: 3.2315721114476523

Epoch: 6| Step: 7
Training loss: 2.626520872116089
Validation loss: 3.2274491786956787

Epoch: 6| Step: 8
Training loss: 3.5659732818603516
Validation loss: 3.2232866287231445

Epoch: 6| Step: 9
Training loss: 3.054104804992676
Validation loss: 3.2190061807632446

Epoch: 6| Step: 10
Training loss: 3.664055824279785
Validation loss: 3.214956839879354

Epoch: 6| Step: 11
Training loss: 4.135622978210449
Validation loss: 3.2108373641967773

Epoch: 6| Step: 12
Training loss: 3.486356258392334
Validation loss: 3.206546425819397

Epoch: 6| Step: 13
Training loss: 3.1516928672790527
Validation loss: 3.2023642460505166

Epoch: 34| Step: 0
Training loss: 2.89046049118042
Validation loss: 3.198352495829264

Epoch: 6| Step: 1
Training loss: 3.7566721439361572
Validation loss: 3.194445848464966

Epoch: 6| Step: 2
Training loss: 3.5918703079223633
Validation loss: 3.1905863682428994

Epoch: 6| Step: 3
Training loss: 2.9907569885253906
Validation loss: 3.1863688230514526

Epoch: 6| Step: 4
Training loss: 3.137148380279541
Validation loss: 3.182854334513346

Epoch: 6| Step: 5
Training loss: 3.6762423515319824
Validation loss: 3.178485910097758

Epoch: 6| Step: 6
Training loss: 3.3113696575164795
Validation loss: 3.1746087869008384

Epoch: 6| Step: 7
Training loss: 3.818556785583496
Validation loss: 3.1704495350519815

Epoch: 6| Step: 8
Training loss: 2.681412696838379
Validation loss: 3.1659278074900308

Epoch: 6| Step: 9
Training loss: 3.1346664428710938
Validation loss: 3.1615161101023355

Epoch: 6| Step: 10
Training loss: 4.465177536010742
Validation loss: 3.1579145590464273

Epoch: 6| Step: 11
Training loss: 2.8904857635498047
Validation loss: 3.153638005256653

Epoch: 6| Step: 12
Training loss: 3.543179988861084
Validation loss: 3.149341344833374

Epoch: 6| Step: 13
Training loss: 3.100498676300049
Validation loss: 3.1451104879379272

Epoch: 35| Step: 0
Training loss: 3.6399683952331543
Validation loss: 3.1408804655075073

Epoch: 6| Step: 1
Training loss: 3.0221123695373535
Validation loss: 3.1368300120035806

Epoch: 6| Step: 2
Training loss: 3.5678305625915527
Validation loss: 3.132426102956136

Epoch: 6| Step: 3
Training loss: 3.192833185195923
Validation loss: 3.128079414367676

Epoch: 6| Step: 4
Training loss: 2.5360870361328125
Validation loss: 3.123879869778951

Epoch: 6| Step: 5
Training loss: 3.7901668548583984
Validation loss: 3.1196409861246743

Epoch: 6| Step: 6
Training loss: 3.605634927749634
Validation loss: 3.1154587666193643

Epoch: 6| Step: 7
Training loss: 3.578038215637207
Validation loss: 3.1108343402544656

Epoch: 6| Step: 8
Training loss: 2.704493999481201
Validation loss: 3.1063222885131836

Epoch: 6| Step: 9
Training loss: 3.1768665313720703
Validation loss: 3.1019128561019897

Epoch: 6| Step: 10
Training loss: 2.545565128326416
Validation loss: 3.097792069117228

Epoch: 6| Step: 11
Training loss: 3.697176218032837
Validation loss: 3.0940675338109336

Epoch: 6| Step: 12
Training loss: 3.6979103088378906
Validation loss: 3.0902132590611777

Epoch: 6| Step: 13
Training loss: 3.5055675506591797
Validation loss: 3.0863318045934043

Epoch: 36| Step: 0
Training loss: 2.617262840270996
Validation loss: 3.0828355153401694

Epoch: 6| Step: 1
Training loss: 2.415241241455078
Validation loss: 3.07916525999705

Epoch: 6| Step: 2
Training loss: 3.6136064529418945
Validation loss: 3.0750394264856973

Epoch: 6| Step: 3
Training loss: 3.6366257667541504
Validation loss: 3.0713478724161782

Epoch: 6| Step: 4
Training loss: 3.7207112312316895
Validation loss: 3.0674856901168823

Epoch: 6| Step: 5
Training loss: 4.167301177978516
Validation loss: 3.0637570222218833

Epoch: 6| Step: 6
Training loss: 2.5548837184906006
Validation loss: 3.059359550476074

Epoch: 6| Step: 7
Training loss: 2.707848072052002
Validation loss: 3.0556666453679404

Epoch: 6| Step: 8
Training loss: 3.133869171142578
Validation loss: 3.051311413447062

Epoch: 6| Step: 9
Training loss: 3.5080981254577637
Validation loss: 3.0476123889287314

Epoch: 6| Step: 10
Training loss: 3.3892126083374023
Validation loss: 3.0440762837727866

Epoch: 6| Step: 11
Training loss: 3.8152475357055664
Validation loss: 3.0401344299316406

Epoch: 6| Step: 12
Training loss: 3.448211431503296
Validation loss: 3.0359838008880615

Epoch: 6| Step: 13
Training loss: 2.7873358726501465
Validation loss: 3.0318371852238974

Epoch: 37| Step: 0
Training loss: 2.9701881408691406
Validation loss: 3.0280787150065103

Epoch: 6| Step: 1
Training loss: 2.7780237197875977
Validation loss: 3.0231430530548096

Epoch: 6| Step: 2
Training loss: 2.9949965476989746
Validation loss: 3.018905242284139

Epoch: 6| Step: 3
Training loss: 2.9900853633880615
Validation loss: 3.014880657196045

Epoch: 6| Step: 4
Training loss: 3.033967971801758
Validation loss: 3.010752558708191

Epoch: 6| Step: 5
Training loss: 3.069993495941162
Validation loss: 3.006892999013265

Epoch: 6| Step: 6
Training loss: 4.153543472290039
Validation loss: 3.0024927854537964

Epoch: 6| Step: 7
Training loss: 2.4103596210479736
Validation loss: 2.9999152421951294

Epoch: 6| Step: 8
Training loss: 2.9951233863830566
Validation loss: 2.997843345006307

Epoch: 6| Step: 9
Training loss: 4.0163702964782715
Validation loss: 2.9918723106384277

Epoch: 6| Step: 10
Training loss: 2.4407832622528076
Validation loss: 2.9880597988764444

Epoch: 6| Step: 11
Training loss: 4.19384241104126
Validation loss: 2.9846065441767373

Epoch: 6| Step: 12
Training loss: 3.0879435539245605
Validation loss: 2.980941653251648

Epoch: 6| Step: 13
Training loss: 3.6539762020111084
Validation loss: 2.9774627685546875

Epoch: 38| Step: 0
Training loss: 2.232048988342285
Validation loss: 2.973937710126241

Epoch: 6| Step: 1
Training loss: 2.594717264175415
Validation loss: 2.970415552457174

Epoch: 6| Step: 2
Training loss: 3.3242552280426025
Validation loss: 2.9670719305674234

Epoch: 6| Step: 3
Training loss: 3.517052173614502
Validation loss: 2.963419755299886

Epoch: 6| Step: 4
Training loss: 3.577652931213379
Validation loss: 2.959653615951538

Epoch: 6| Step: 5
Training loss: 3.176589250564575
Validation loss: 2.9561334451039634

Epoch: 6| Step: 6
Training loss: 3.1137571334838867
Validation loss: 2.9518458445866904

Epoch: 6| Step: 7
Training loss: 2.842215061187744
Validation loss: 2.9494313398996987

Epoch: 6| Step: 8
Training loss: 3.729158878326416
Validation loss: 2.9453830321629844

Epoch: 6| Step: 9
Training loss: 3.5727806091308594
Validation loss: 2.940934737523397

Epoch: 6| Step: 10
Training loss: 2.9920196533203125
Validation loss: 2.936384359995524

Epoch: 6| Step: 11
Training loss: 3.018366813659668
Validation loss: 2.932565212249756

Epoch: 6| Step: 12
Training loss: 2.9141392707824707
Validation loss: 2.929296294848124

Epoch: 6| Step: 13
Training loss: 3.505357265472412
Validation loss: 2.926028609275818

Epoch: 39| Step: 0
Training loss: 2.9337830543518066
Validation loss: 2.9221837917963662

Epoch: 6| Step: 1
Training loss: 2.61445689201355
Validation loss: 2.918801744778951

Epoch: 6| Step: 2
Training loss: 3.116647720336914
Validation loss: 2.915704091389974

Epoch: 6| Step: 3
Training loss: 3.134620428085327
Validation loss: 2.912144740422567

Epoch: 6| Step: 4
Training loss: 3.708677291870117
Validation loss: 2.908744772275289

Epoch: 6| Step: 5
Training loss: 2.724331855773926
Validation loss: 2.904609203338623

Epoch: 6| Step: 6
Training loss: 2.9213967323303223
Validation loss: 2.900839865207672

Epoch: 6| Step: 7
Training loss: 4.56085205078125
Validation loss: 2.8974061608314514

Epoch: 6| Step: 8
Training loss: 3.246619701385498
Validation loss: 2.89328134059906

Epoch: 6| Step: 9
Training loss: 2.9304399490356445
Validation loss: 2.8902207612991333

Epoch: 6| Step: 10
Training loss: 2.6024136543273926
Validation loss: 2.886847654978434

Epoch: 6| Step: 11
Training loss: 2.4456419944763184
Validation loss: 2.8836069107055664

Epoch: 6| Step: 12
Training loss: 3.4199419021606445
Validation loss: 2.881144901116689

Epoch: 6| Step: 13
Training loss: 3.122997760772705
Validation loss: 2.8767478466033936

Epoch: 40| Step: 0
Training loss: 2.3588380813598633
Validation loss: 2.8720174630482993

Epoch: 6| Step: 1
Training loss: 2.770714521408081
Validation loss: 2.8692785104115806

Epoch: 6| Step: 2
Training loss: 3.5613627433776855
Validation loss: 2.865081230799357

Epoch: 6| Step: 3
Training loss: 3.1372108459472656
Validation loss: 2.861172874768575

Epoch: 6| Step: 4
Training loss: 3.48569917678833
Validation loss: 2.8577022155125937

Epoch: 6| Step: 5
Training loss: 2.5890395641326904
Validation loss: 2.8545642693837485

Epoch: 6| Step: 6
Training loss: 3.0814414024353027
Validation loss: 2.8507110675175986

Epoch: 6| Step: 7
Training loss: 3.3455634117126465
Validation loss: 2.8489803870519004

Epoch: 6| Step: 8
Training loss: 2.4933390617370605
Validation loss: 2.8445139726003013

Epoch: 6| Step: 9
Training loss: 2.66064453125
Validation loss: 2.8409297863642373

Epoch: 6| Step: 10
Training loss: 3.3526484966278076
Validation loss: 2.840442419052124

Epoch: 6| Step: 11
Training loss: 3.7079195976257324
Validation loss: 2.8346767028172812

Epoch: 6| Step: 12
Training loss: 2.7644474506378174
Validation loss: 2.8318423430124917

Epoch: 6| Step: 13
Training loss: 3.5486364364624023
Validation loss: 2.8267449537913003

Epoch: 41| Step: 0
Training loss: 3.1607260704040527
Validation loss: 2.823729952176412

Epoch: 6| Step: 1
Training loss: 3.2165439128875732
Validation loss: 2.821406523386637

Epoch: 6| Step: 2
Training loss: 3.2524051666259766
Validation loss: 2.817667007446289

Epoch: 6| Step: 3
Training loss: 2.80723237991333
Validation loss: 2.8137559493382773

Epoch: 6| Step: 4
Training loss: 3.50392746925354
Validation loss: 2.811152935028076

Epoch: 6| Step: 5
Training loss: 2.978167772293091
Validation loss: 2.807503322760264

Epoch: 6| Step: 6
Training loss: 3.0661795139312744
Validation loss: 2.8045732180277505

Epoch: 6| Step: 7
Training loss: 2.8046815395355225
Validation loss: 2.799910227457682

Epoch: 6| Step: 8
Training loss: 2.9581992626190186
Validation loss: 2.7951958576838174

Epoch: 6| Step: 9
Training loss: 3.3411359786987305
Validation loss: 2.792531967163086

Epoch: 6| Step: 10
Training loss: 3.1781961917877197
Validation loss: 2.787972847620646

Epoch: 6| Step: 11
Training loss: 2.680187225341797
Validation loss: 2.785710652669271

Epoch: 6| Step: 12
Training loss: 2.9855031967163086
Validation loss: 2.782456715901693

Epoch: 6| Step: 13
Training loss: 2.33148193359375
Validation loss: 2.7816781997680664

Epoch: 42| Step: 0
Training loss: 3.424917697906494
Validation loss: 2.7813138564427695

Epoch: 6| Step: 1
Training loss: 3.7153425216674805
Validation loss: 2.7773840030034385

Epoch: 6| Step: 2
Training loss: 3.041604995727539
Validation loss: 2.772308349609375

Epoch: 6| Step: 3
Training loss: 2.782982349395752
Validation loss: 2.76936407883962

Epoch: 6| Step: 4
Training loss: 2.7434651851654053
Validation loss: 2.7639769315719604

Epoch: 6| Step: 5
Training loss: 2.443344831466675
Validation loss: 2.770485520362854

Epoch: 6| Step: 6
Training loss: 3.231451988220215
Validation loss: 2.760298808415731

Epoch: 6| Step: 7
Training loss: 3.393962860107422
Validation loss: 2.7575196822484336

Epoch: 6| Step: 8
Training loss: 2.4117398262023926
Validation loss: 2.7550034125645957

Epoch: 6| Step: 9
Training loss: 3.180487632751465
Validation loss: 2.762632966041565

Epoch: 6| Step: 10
Training loss: 2.791674852371216
Validation loss: 2.7486617962519326

Epoch: 6| Step: 11
Training loss: 2.1318211555480957
Validation loss: 2.7463356653849282

Epoch: 6| Step: 12
Training loss: 3.2200045585632324
Validation loss: 2.74351433912913

Epoch: 6| Step: 13
Training loss: 3.0966134071350098
Validation loss: 2.7431914806365967

Epoch: 43| Step: 0
Training loss: 3.446434736251831
Validation loss: 2.73471862077713

Epoch: 6| Step: 1
Training loss: 3.7904021739959717
Validation loss: 2.732183794180552

Epoch: 6| Step: 2
Training loss: 2.9619569778442383
Validation loss: 2.729111909866333

Epoch: 6| Step: 3
Training loss: 2.5214614868164062
Validation loss: 2.728156089782715

Epoch: 6| Step: 4
Training loss: 2.667557954788208
Validation loss: 2.728692094484965

Epoch: 6| Step: 5
Training loss: 2.2747302055358887
Validation loss: 2.724420348803202

Epoch: 6| Step: 6
Training loss: 2.761932134628296
Validation loss: 2.7230447133382163

Epoch: 6| Step: 7
Training loss: 2.4804811477661133
Validation loss: 2.7160005370775857

Epoch: 6| Step: 8
Training loss: 3.059755802154541
Validation loss: 2.710647185643514

Epoch: 6| Step: 9
Training loss: 2.572930097579956
Validation loss: 2.708106795946757

Epoch: 6| Step: 10
Training loss: 3.4585163593292236
Validation loss: 2.7043557167053223

Epoch: 6| Step: 11
Training loss: 3.5772757530212402
Validation loss: 2.699734608332316

Epoch: 6| Step: 12
Training loss: 2.720879316329956
Validation loss: 2.6952356497446694

Epoch: 6| Step: 13
Training loss: 2.6056838035583496
Validation loss: 2.6956785519917807

Epoch: 44| Step: 0
Training loss: 2.530886650085449
Validation loss: 2.6955122152964273

Epoch: 6| Step: 1
Training loss: 2.930262565612793
Validation loss: 2.6882327795028687

Epoch: 6| Step: 2
Training loss: 2.996332883834839
Validation loss: 2.6840309500694275

Epoch: 6| Step: 3
Training loss: 3.00009822845459
Validation loss: 2.6798420349756875

Epoch: 6| Step: 4
Training loss: 3.036749839782715
Validation loss: 2.6800562938054404

Epoch: 6| Step: 5
Training loss: 2.4730453491210938
Validation loss: 2.67426606019338

Epoch: 6| Step: 6
Training loss: 3.0707502365112305
Validation loss: 2.6689787904421487

Epoch: 6| Step: 7
Training loss: 2.8620803356170654
Validation loss: 2.663879076639811

Epoch: 6| Step: 8
Training loss: 2.5153677463531494
Validation loss: 2.662603815396627

Epoch: 6| Step: 9
Training loss: 3.1564111709594727
Validation loss: 2.6616012255350747

Epoch: 6| Step: 10
Training loss: 2.678694725036621
Validation loss: 2.656338691711426

Epoch: 6| Step: 11
Training loss: 3.20475435256958
Validation loss: 2.652297019958496

Epoch: 6| Step: 12
Training loss: 3.0722784996032715
Validation loss: 2.65105547507604

Epoch: 6| Step: 13
Training loss: 2.674631118774414
Validation loss: 2.6495002110799155

Epoch: 45| Step: 0
Training loss: 3.4589056968688965
Validation loss: 2.6462491552035012

Epoch: 6| Step: 1
Training loss: 1.8917896747589111
Validation loss: 2.6424361864725747

Epoch: 6| Step: 2
Training loss: 2.506180763244629
Validation loss: 2.63928759098053

Epoch: 6| Step: 3
Training loss: 3.5739989280700684
Validation loss: 2.640327572822571

Epoch: 6| Step: 4
Training loss: 3.29233455657959
Validation loss: 2.646844824155172

Epoch: 6| Step: 5
Training loss: 3.23960018157959
Validation loss: 2.6415890057881675

Epoch: 6| Step: 6
Training loss: 2.5841851234436035
Validation loss: 2.6259700457255044

Epoch: 6| Step: 7
Training loss: 2.115966796875
Validation loss: 2.625832120577494

Epoch: 6| Step: 8
Training loss: 2.989302158355713
Validation loss: 2.6287461519241333

Epoch: 6| Step: 9
Training loss: 2.312786102294922
Validation loss: 2.6458374857902527

Epoch: 6| Step: 10
Training loss: 3.1607298851013184
Validation loss: 2.660780151685079

Epoch: 6| Step: 11
Training loss: 2.937591075897217
Validation loss: 2.6462105909983316

Epoch: 6| Step: 12
Training loss: 2.672945976257324
Validation loss: 2.6301828225453696

Epoch: 6| Step: 13
Training loss: 2.884131669998169
Validation loss: 2.61143966515859

Epoch: 46| Step: 0
Training loss: 2.695775270462036
Validation loss: 2.6061978737513223

Epoch: 6| Step: 1
Training loss: 2.2572951316833496
Validation loss: 2.6004698673884072

Epoch: 6| Step: 2
Training loss: 3.434901714324951
Validation loss: 2.600383480389913

Epoch: 6| Step: 3
Training loss: 2.6728763580322266
Validation loss: 2.5988012552261353

Epoch: 6| Step: 4
Training loss: 2.3779757022857666
Validation loss: 2.5936747193336487

Epoch: 6| Step: 5
Training loss: 2.2547988891601562
Validation loss: 2.5888211329778037

Epoch: 6| Step: 6
Training loss: 2.971989870071411
Validation loss: 2.582220196723938

Epoch: 6| Step: 7
Training loss: 3.2501883506774902
Validation loss: 2.5784592628479004

Epoch: 6| Step: 8
Training loss: 3.208578109741211
Validation loss: 2.571812868118286

Epoch: 6| Step: 9
Training loss: 2.7048399448394775
Validation loss: 2.5701186855634055

Epoch: 6| Step: 10
Training loss: 2.5869665145874023
Validation loss: 2.568586548169454

Epoch: 6| Step: 11
Training loss: 2.3971738815307617
Validation loss: 2.564055562019348

Epoch: 6| Step: 12
Training loss: 2.968653440475464
Validation loss: 2.561698794364929

Epoch: 6| Step: 13
Training loss: 3.196983814239502
Validation loss: 2.5622617403666177

Epoch: 47| Step: 0
Training loss: 2.921218156814575
Validation loss: 2.5620420376459756

Epoch: 6| Step: 1
Training loss: 2.4556097984313965
Validation loss: 2.553741176923116

Epoch: 6| Step: 2
Training loss: 2.588034152984619
Validation loss: 2.550343155860901

Epoch: 6| Step: 3
Training loss: 2.694366216659546
Validation loss: 2.547409097353617

Epoch: 6| Step: 4
Training loss: 2.4908487796783447
Validation loss: 2.542312184969584

Epoch: 6| Step: 5
Training loss: 3.1907365322113037
Validation loss: 2.5454175074895224

Epoch: 6| Step: 6
Training loss: 3.0527114868164062
Validation loss: 2.538463751475016

Epoch: 6| Step: 7
Training loss: 2.7903594970703125
Validation loss: 2.5329111417134604

Epoch: 6| Step: 8
Training loss: 2.6950488090515137
Validation loss: 2.5325251817703247

Epoch: 6| Step: 9
Training loss: 3.367983818054199
Validation loss: 2.5306044816970825

Epoch: 6| Step: 10
Training loss: 2.5789613723754883
Validation loss: 2.5273234446843467

Epoch: 6| Step: 11
Training loss: 2.088771343231201
Validation loss: 2.5261031786600747

Epoch: 6| Step: 12
Training loss: 3.077549457550049
Validation loss: 2.5223265886306763

Epoch: 6| Step: 13
Training loss: 2.2820873260498047
Validation loss: 2.519197861353556

Epoch: 48| Step: 0
Training loss: 2.7499711513519287
Validation loss: 2.517018973827362

Epoch: 6| Step: 1
Training loss: 2.852243423461914
Validation loss: 2.5112202167510986

Epoch: 6| Step: 2
Training loss: 2.2432727813720703
Validation loss: 2.510800282160441

Epoch: 6| Step: 3
Training loss: 2.794610023498535
Validation loss: 2.5095200538635254

Epoch: 6| Step: 4
Training loss: 2.70913028717041
Validation loss: 2.505010485649109

Epoch: 6| Step: 5
Training loss: 2.1247291564941406
Validation loss: 2.507473587989807

Epoch: 6| Step: 6
Training loss: 2.561581611633301
Validation loss: 2.499429623285929

Epoch: 6| Step: 7
Training loss: 2.8996706008911133
Validation loss: 2.4929519494374595

Epoch: 6| Step: 8
Training loss: 2.3338022232055664
Validation loss: 2.4886155923207602

Epoch: 6| Step: 9
Training loss: 3.0093040466308594
Validation loss: 2.4858089685440063

Epoch: 6| Step: 10
Training loss: 3.265902519226074
Validation loss: 2.4796862403551736

Epoch: 6| Step: 11
Training loss: 2.3191699981689453
Validation loss: 2.4787657260894775

Epoch: 6| Step: 12
Training loss: 2.9808788299560547
Validation loss: 2.473471164703369

Epoch: 6| Step: 13
Training loss: 2.7998032569885254
Validation loss: 2.4771879514058432

Epoch: 49| Step: 0
Training loss: 3.2289910316467285
Validation loss: 2.4716877142588296

Epoch: 6| Step: 1
Training loss: 2.330413818359375
Validation loss: 2.4733758767445884

Epoch: 6| Step: 2
Training loss: 2.5015125274658203
Validation loss: 2.4678831497828164

Epoch: 6| Step: 3
Training loss: 2.964663505554199
Validation loss: 2.471548875172933

Epoch: 6| Step: 4
Training loss: 2.5584568977355957
Validation loss: 2.46066677570343

Epoch: 6| Step: 5
Training loss: 2.5393760204315186
Validation loss: 2.460176626841227

Epoch: 6| Step: 6
Training loss: 2.4103307723999023
Validation loss: 2.4544855753580728

Epoch: 6| Step: 7
Training loss: 2.999302625656128
Validation loss: 2.451721429824829

Epoch: 6| Step: 8
Training loss: 2.5344345569610596
Validation loss: 2.447903792063395

Epoch: 6| Step: 9
Training loss: 2.5047268867492676
Validation loss: 2.448893984158834

Epoch: 6| Step: 10
Training loss: 1.5497586727142334
Validation loss: 2.446369787057241

Epoch: 6| Step: 11
Training loss: 3.370661735534668
Validation loss: 2.4394808610280356

Epoch: 6| Step: 12
Training loss: 2.683015823364258
Validation loss: 2.44061815738678

Epoch: 6| Step: 13
Training loss: 2.8202505111694336
Validation loss: 2.4381823539733887

Epoch: 50| Step: 0
Training loss: 2.509547472000122
Validation loss: 2.432726502418518

Epoch: 6| Step: 1
Training loss: 2.3139102458953857
Validation loss: 2.427227477232615

Epoch: 6| Step: 2
Training loss: 2.5228259563446045
Validation loss: 2.4293459256490073

Epoch: 6| Step: 3
Training loss: 2.692554235458374
Validation loss: 2.427588144938151

Epoch: 6| Step: 4
Training loss: 2.9512698650360107
Validation loss: 2.423594315846761

Epoch: 6| Step: 5
Training loss: 2.0521318912506104
Validation loss: 2.421790599822998

Epoch: 6| Step: 6
Training loss: 2.376079797744751
Validation loss: 2.415319859981537

Epoch: 6| Step: 7
Training loss: 3.2987422943115234
Validation loss: 2.4195175170898438

Epoch: 6| Step: 8
Training loss: 2.4688215255737305
Validation loss: 2.420695126056671

Epoch: 6| Step: 9
Training loss: 2.9875495433807373
Validation loss: 2.414397577444712

Epoch: 6| Step: 10
Training loss: 2.463160991668701
Validation loss: 2.409507075945536

Epoch: 6| Step: 11
Training loss: 2.458212375640869
Validation loss: 2.40027783314387

Epoch: 6| Step: 12
Training loss: 2.692387580871582
Validation loss: 2.3958017428716025

Epoch: 6| Step: 13
Training loss: 2.553312301635742
Validation loss: 2.4000926415125527

Epoch: 51| Step: 0
Training loss: 2.1142067909240723
Validation loss: 2.406197110811869

Epoch: 6| Step: 1
Training loss: 2.606311798095703
Validation loss: 2.4135464231173196

Epoch: 6| Step: 2
Training loss: 2.455151081085205
Validation loss: 2.436357080936432

Epoch: 6| Step: 3
Training loss: 2.5697178840637207
Validation loss: 2.475168784459432

Epoch: 6| Step: 4
Training loss: 2.5041189193725586
Validation loss: 2.4677542448043823

Epoch: 6| Step: 5
Training loss: 2.5084710121154785
Validation loss: 2.417493224143982

Epoch: 6| Step: 6
Training loss: 2.489651679992676
Validation loss: 2.3950408697128296

Epoch: 6| Step: 7
Training loss: 2.1572070121765137
Validation loss: 2.384547015031179

Epoch: 6| Step: 8
Training loss: 3.06415057182312
Validation loss: 2.3749313354492188

Epoch: 6| Step: 9
Training loss: 2.689091205596924
Validation loss: 2.3688122828801474

Epoch: 6| Step: 10
Training loss: 2.3034913539886475
Validation loss: 2.3709924618403115

Epoch: 6| Step: 11
Training loss: 2.3679962158203125
Validation loss: 2.3665035367012024

Epoch: 6| Step: 12
Training loss: 3.0529661178588867
Validation loss: 2.3715833822886148

Epoch: 6| Step: 13
Training loss: 3.1258320808410645
Validation loss: 2.3765329321225486

Epoch: 52| Step: 0
Training loss: 2.090338706970215
Validation loss: 2.3865883350372314

Epoch: 6| Step: 1
Training loss: 2.584988832473755
Validation loss: 2.3706709146499634

Epoch: 6| Step: 2
Training loss: 2.8934836387634277
Validation loss: 2.357666492462158

Epoch: 6| Step: 3
Training loss: 2.0138351917266846
Validation loss: 2.35254697004954

Epoch: 6| Step: 4
Training loss: 2.0795035362243652
Validation loss: 2.3494109312693277

Epoch: 6| Step: 5
Training loss: 3.06968355178833
Validation loss: 2.3455315430959067

Epoch: 6| Step: 6
Training loss: 2.8090667724609375
Validation loss: 2.3416000604629517

Epoch: 6| Step: 7
Training loss: 2.2027153968811035
Validation loss: 2.3412678837776184

Epoch: 6| Step: 8
Training loss: 2.900754928588867
Validation loss: 2.3410937388738

Epoch: 6| Step: 9
Training loss: 2.675281286239624
Validation loss: 2.337331453959147

Epoch: 6| Step: 10
Training loss: 3.0694541931152344
Validation loss: 2.334462563196818

Epoch: 6| Step: 11
Training loss: 1.9037142992019653
Validation loss: 2.331157147884369

Epoch: 6| Step: 12
Training loss: 2.6249074935913086
Validation loss: 2.3304020961125693

Epoch: 6| Step: 13
Training loss: 2.3514766693115234
Validation loss: 2.324588179588318

Epoch: 53| Step: 0
Training loss: 2.751218318939209
Validation loss: 2.3257543643315635

Epoch: 6| Step: 1
Training loss: 2.541210651397705
Validation loss: 2.322617491086324

Epoch: 6| Step: 2
Training loss: 2.3742992877960205
Validation loss: 2.3210266629854837

Epoch: 6| Step: 3
Training loss: 1.8640114068984985
Validation loss: 2.319300432999929

Epoch: 6| Step: 4
Training loss: 2.4692420959472656
Validation loss: 2.31477282444636

Epoch: 6| Step: 5
Training loss: 2.3603343963623047
Validation loss: 2.3111986915270486

Epoch: 6| Step: 6
Training loss: 2.5017507076263428
Validation loss: 2.309284806251526

Epoch: 6| Step: 7
Training loss: 2.564005136489868
Validation loss: 2.304320474465688

Epoch: 6| Step: 8
Training loss: 2.8545660972595215
Validation loss: 2.2972706158955893

Epoch: 6| Step: 9
Training loss: 1.9610936641693115
Validation loss: 2.2911643584569297

Epoch: 6| Step: 10
Training loss: 2.4665932655334473
Validation loss: 2.2943328420321145

Epoch: 6| Step: 11
Training loss: 2.9137024879455566
Validation loss: 2.307171026865641

Epoch: 6| Step: 12
Training loss: 2.558215618133545
Validation loss: 2.292060613632202

Epoch: 6| Step: 13
Training loss: 2.5294742584228516
Validation loss: 2.2865135272343955

Epoch: 54| Step: 0
Training loss: 3.0673506259918213
Validation loss: 2.2829072078069053

Epoch: 6| Step: 1
Training loss: 2.360740900039673
Validation loss: 2.283721407254537

Epoch: 6| Step: 2
Training loss: 2.552577495574951
Validation loss: 2.276358644167582

Epoch: 6| Step: 3
Training loss: 2.4715800285339355
Validation loss: 2.276419719060262

Epoch: 6| Step: 4
Training loss: 2.2685205936431885
Validation loss: 2.273121734460195

Epoch: 6| Step: 5
Training loss: 2.395094633102417
Validation loss: 2.274324099222819

Epoch: 6| Step: 6
Training loss: 2.772033214569092
Validation loss: 2.2703529795010886

Epoch: 6| Step: 7
Training loss: 2.883610725402832
Validation loss: 2.2668710152308145

Epoch: 6| Step: 8
Training loss: 2.3542962074279785
Validation loss: 2.264371713002523

Epoch: 6| Step: 9
Training loss: 2.6885671615600586
Validation loss: 2.2667694886525473

Epoch: 6| Step: 10
Training loss: 1.8885068893432617
Validation loss: 2.258015235265096

Epoch: 6| Step: 11
Training loss: 2.8374459743499756
Validation loss: 2.2576198975245156

Epoch: 6| Step: 12
Training loss: 1.6040213108062744
Validation loss: 2.258120814959208

Epoch: 6| Step: 13
Training loss: 1.9220720529556274
Validation loss: 2.2517104943593345

Epoch: 55| Step: 0
Training loss: 2.485687255859375
Validation loss: 2.2468272844950357

Epoch: 6| Step: 1
Training loss: 2.7864341735839844
Validation loss: 2.245697279771169

Epoch: 6| Step: 2
Training loss: 1.6473562717437744
Validation loss: 2.2607187032699585

Epoch: 6| Step: 3
Training loss: 1.508853554725647
Validation loss: 2.2587926387786865

Epoch: 6| Step: 4
Training loss: 2.770139217376709
Validation loss: 2.255490024884542

Epoch: 6| Step: 5
Training loss: 2.14587140083313
Validation loss: 2.240194241205851

Epoch: 6| Step: 6
Training loss: 2.40958571434021
Validation loss: 2.2423052390416465

Epoch: 6| Step: 7
Training loss: 2.88089919090271
Validation loss: 2.230219006538391

Epoch: 6| Step: 8
Training loss: 2.8022642135620117
Validation loss: 2.232136885325114

Epoch: 6| Step: 9
Training loss: 1.8756985664367676
Validation loss: 2.2292751471201577

Epoch: 6| Step: 10
Training loss: 2.365481376647949
Validation loss: 2.2262747486432395

Epoch: 6| Step: 11
Training loss: 2.3791627883911133
Validation loss: 2.223338484764099

Epoch: 6| Step: 12
Training loss: 3.390331268310547
Validation loss: 2.227044701576233

Epoch: 6| Step: 13
Training loss: 2.1139893531799316
Validation loss: 2.2201026678085327

Epoch: 56| Step: 0
Training loss: 2.497720718383789
Validation loss: 2.216978887716929

Epoch: 6| Step: 1
Training loss: 2.0479941368103027
Validation loss: 2.2146360874176025

Epoch: 6| Step: 2
Training loss: 2.6737782955169678
Validation loss: 2.2110975980758667

Epoch: 6| Step: 3
Training loss: 1.5210318565368652
Validation loss: 2.2070652643839517

Epoch: 6| Step: 4
Training loss: 2.2641067504882812
Validation loss: 2.208296080430349

Epoch: 6| Step: 5
Training loss: 2.2870047092437744
Validation loss: 2.2067098220189414

Epoch: 6| Step: 6
Training loss: 2.062016010284424
Validation loss: 2.2093690435091653

Epoch: 6| Step: 7
Training loss: 2.6121606826782227
Validation loss: 2.1972214579582214

Epoch: 6| Step: 8
Training loss: 2.7687935829162598
Validation loss: 2.204793393611908

Epoch: 6| Step: 9
Training loss: 2.4572296142578125
Validation loss: 2.1948952674865723

Epoch: 6| Step: 10
Training loss: 2.1590850353240967
Validation loss: 2.1975250045458474

Epoch: 6| Step: 11
Training loss: 2.7918806076049805
Validation loss: 2.198165317376455

Epoch: 6| Step: 12
Training loss: 2.903073787689209
Validation loss: 2.19842799504598

Epoch: 6| Step: 13
Training loss: 2.039820671081543
Validation loss: 2.1871201594670615

Epoch: 57| Step: 0
Training loss: 2.204655170440674
Validation loss: 2.18457422653834

Epoch: 6| Step: 1
Training loss: 2.5288238525390625
Validation loss: 2.18612011273702

Epoch: 6| Step: 2
Training loss: 3.247805118560791
Validation loss: 2.188701629638672

Epoch: 6| Step: 3
Training loss: 2.4688496589660645
Validation loss: 2.188824554284414

Epoch: 6| Step: 4
Training loss: 2.369439125061035
Validation loss: 2.1877492467562356

Epoch: 6| Step: 5
Training loss: 2.534526824951172
Validation loss: 2.1859585444132485

Epoch: 6| Step: 6
Training loss: 2.402785301208496
Validation loss: 2.1805281241734824

Epoch: 6| Step: 7
Training loss: 2.0116729736328125
Validation loss: 2.1735182801882424

Epoch: 6| Step: 8
Training loss: 2.1395015716552734
Validation loss: 2.189757525920868

Epoch: 6| Step: 9
Training loss: 2.44445538520813
Validation loss: 2.184471766153971

Epoch: 6| Step: 10
Training loss: 1.673893928527832
Validation loss: 2.1879321932792664

Epoch: 6| Step: 11
Training loss: 1.8285398483276367
Validation loss: 2.178542455037435

Epoch: 6| Step: 12
Training loss: 2.4385011196136475
Validation loss: 2.1766979893048606

Epoch: 6| Step: 13
Training loss: 2.455380916595459
Validation loss: 2.167167683442434

Epoch: 58| Step: 0
Training loss: 2.5440726280212402
Validation loss: 2.168442487716675

Epoch: 6| Step: 1
Training loss: 2.0002827644348145
Validation loss: 2.1950173377990723

Epoch: 6| Step: 2
Training loss: 2.2615866661071777
Validation loss: 2.173634926478068

Epoch: 6| Step: 3
Training loss: 2.5207724571228027
Validation loss: 2.1750466426213584

Epoch: 6| Step: 4
Training loss: 2.5355358123779297
Validation loss: 2.1737465858459473

Epoch: 6| Step: 5
Training loss: 2.265803813934326
Validation loss: 2.1767913500467935

Epoch: 6| Step: 6
Training loss: 2.5651350021362305
Validation loss: 2.176079253355662

Epoch: 6| Step: 7
Training loss: 2.1014809608459473
Validation loss: 2.1767592827479043

Epoch: 6| Step: 8
Training loss: 2.608976125717163
Validation loss: 2.175268292427063

Epoch: 6| Step: 9
Training loss: 2.9651193618774414
Validation loss: 2.170819560686747

Epoch: 6| Step: 10
Training loss: 2.550154209136963
Validation loss: 2.170075019200643

Epoch: 6| Step: 11
Training loss: 1.956646203994751
Validation loss: 2.164421300093333

Epoch: 6| Step: 12
Training loss: 1.8945553302764893
Validation loss: 2.16469677289327

Epoch: 6| Step: 13
Training loss: 1.8985711336135864
Validation loss: 2.1534037788709006

Epoch: 59| Step: 0
Training loss: 2.4347877502441406
Validation loss: 2.151837189992269

Epoch: 6| Step: 1
Training loss: 2.269906759262085
Validation loss: 2.1469352642695108

Epoch: 6| Step: 2
Training loss: 2.644186019897461
Validation loss: 2.144731561342875

Epoch: 6| Step: 3
Training loss: 2.4378108978271484
Validation loss: 2.1399133602778115

Epoch: 6| Step: 4
Training loss: 2.625033140182495
Validation loss: 2.1376349528630576

Epoch: 6| Step: 5
Training loss: 2.1616384983062744
Validation loss: 2.140648146470388

Epoch: 6| Step: 6
Training loss: 2.7705588340759277
Validation loss: 2.1405137181282043

Epoch: 6| Step: 7
Training loss: 2.4515159130096436
Validation loss: 2.137882173061371

Epoch: 6| Step: 8
Training loss: 2.042005777359009
Validation loss: 2.137350002924601

Epoch: 6| Step: 9
Training loss: 2.1148831844329834
Validation loss: 2.1381494998931885

Epoch: 6| Step: 10
Training loss: 1.3764979839324951
Validation loss: 2.136863629023234

Epoch: 6| Step: 11
Training loss: 2.1599574089050293
Validation loss: 2.1317323644955954

Epoch: 6| Step: 12
Training loss: 2.484513521194458
Validation loss: 2.1338749527931213

Epoch: 6| Step: 13
Training loss: 2.420846939086914
Validation loss: 2.138032873471578

Epoch: 60| Step: 0
Training loss: 1.835233211517334
Validation loss: 2.1395025054613748

Epoch: 6| Step: 1
Training loss: 1.9813320636749268
Validation loss: 2.137236773967743

Epoch: 6| Step: 2
Training loss: 2.616598129272461
Validation loss: 2.124306082725525

Epoch: 6| Step: 3
Training loss: 2.150088310241699
Validation loss: 2.1185277899106345

Epoch: 6| Step: 4
Training loss: 2.515038013458252
Validation loss: 2.123310705025991

Epoch: 6| Step: 5
Training loss: 2.1156058311462402
Validation loss: 2.1242109139760337

Epoch: 6| Step: 6
Training loss: 2.1561503410339355
Validation loss: 2.1163219213485718

Epoch: 6| Step: 7
Training loss: 2.6777901649475098
Validation loss: 2.1215802828470864

Epoch: 6| Step: 8
Training loss: 2.194826126098633
Validation loss: 2.114970604578654

Epoch: 6| Step: 9
Training loss: 2.3673510551452637
Validation loss: 2.1117315093676248

Epoch: 6| Step: 10
Training loss: 2.3712329864501953
Validation loss: 2.109976053237915

Epoch: 6| Step: 11
Training loss: 2.2721314430236816
Validation loss: 2.1116763750712075

Epoch: 6| Step: 12
Training loss: 2.6531217098236084
Validation loss: 2.1137468616167703

Epoch: 6| Step: 13
Training loss: 2.2066283226013184
Validation loss: 2.118530054887136

Epoch: 61| Step: 0
Training loss: 2.5973336696624756
Validation loss: 2.120446562767029

Epoch: 6| Step: 1
Training loss: 1.7984540462493896
Validation loss: 2.1218210657437644

Epoch: 6| Step: 2
Training loss: 2.3646836280822754
Validation loss: 2.1241868336995444

Epoch: 6| Step: 3
Training loss: 2.050135374069214
Validation loss: 2.115759472052256

Epoch: 6| Step: 4
Training loss: 2.63114857673645
Validation loss: 2.111718793710073

Epoch: 6| Step: 5
Training loss: 2.051344156265259
Validation loss: 2.1128017902374268

Epoch: 6| Step: 6
Training loss: 1.7209551334381104
Validation loss: 2.1172699332237244

Epoch: 6| Step: 7
Training loss: 1.9169844388961792
Validation loss: 2.112413545449575

Epoch: 6| Step: 8
Training loss: 2.047771692276001
Validation loss: 2.112430532773336

Epoch: 6| Step: 9
Training loss: 2.463824987411499
Validation loss: 2.103587488333384

Epoch: 6| Step: 10
Training loss: 2.585174798965454
Validation loss: 2.1134580175081887

Epoch: 6| Step: 11
Training loss: 2.7854933738708496
Validation loss: 2.1150382359822593

Epoch: 6| Step: 12
Training loss: 2.626582145690918
Validation loss: 2.109101176261902

Epoch: 6| Step: 13
Training loss: 2.3417863845825195
Validation loss: 2.111030022303263

Epoch: 62| Step: 0
Training loss: 2.0689496994018555
Validation loss: 2.1022473772366843

Epoch: 6| Step: 1
Training loss: 2.5769448280334473
Validation loss: 2.0934526324272156

Epoch: 6| Step: 2
Training loss: 2.1347103118896484
Validation loss: 2.0940173864364624

Epoch: 6| Step: 3
Training loss: 2.4339189529418945
Validation loss: 2.092524846394857

Epoch: 6| Step: 4
Training loss: 1.9741218090057373
Validation loss: 2.104660948117574

Epoch: 6| Step: 5
Training loss: 2.7548768520355225
Validation loss: 2.1068963209788003

Epoch: 6| Step: 6
Training loss: 2.336211919784546
Validation loss: 2.109841287136078

Epoch: 6| Step: 7
Training loss: 2.2215702533721924
Validation loss: 2.105778932571411

Epoch: 6| Step: 8
Training loss: 2.677450656890869
Validation loss: 2.102764983971914

Epoch: 6| Step: 9
Training loss: 1.2186596393585205
Validation loss: 2.1041833559672036

Epoch: 6| Step: 10
Training loss: 2.3904356956481934
Validation loss: 2.0997879902521768

Epoch: 6| Step: 11
Training loss: 2.195164918899536
Validation loss: 2.098333199818929

Epoch: 6| Step: 12
Training loss: 2.318498134613037
Validation loss: 2.09070618947347

Epoch: 6| Step: 13
Training loss: 2.5074524879455566
Validation loss: 2.0917672316233316

Epoch: 63| Step: 0
Training loss: 2.3695805072784424
Validation loss: 2.0724199215571084

Epoch: 6| Step: 1
Training loss: 2.7892708778381348
Validation loss: 2.0779963533083596

Epoch: 6| Step: 2
Training loss: 2.1190593242645264
Validation loss: 2.079519589742025

Epoch: 6| Step: 3
Training loss: 2.655092716217041
Validation loss: 2.0886412858963013

Epoch: 6| Step: 4
Training loss: 2.123234510421753
Validation loss: 2.090768893559774

Epoch: 6| Step: 5
Training loss: 1.7349921464920044
Validation loss: 2.1171837647755942

Epoch: 6| Step: 6
Training loss: 2.623595714569092
Validation loss: 2.146441022555033

Epoch: 6| Step: 7
Training loss: 2.564743995666504
Validation loss: 2.1448107957839966

Epoch: 6| Step: 8
Training loss: 2.310380697250366
Validation loss: 2.1453447540601096

Epoch: 6| Step: 9
Training loss: 1.8607182502746582
Validation loss: 2.1248385906219482

Epoch: 6| Step: 10
Training loss: 2.124241828918457
Validation loss: 2.1212642590204873

Epoch: 6| Step: 11
Training loss: 2.688774585723877
Validation loss: 2.109541376431783

Epoch: 6| Step: 12
Training loss: 2.168246030807495
Validation loss: 2.108231763044993

Epoch: 6| Step: 13
Training loss: 1.5457847118377686
Validation loss: 2.108789006868998

Epoch: 64| Step: 0
Training loss: 2.276259660720825
Validation loss: 2.1066694060961404

Epoch: 6| Step: 1
Training loss: 2.783592700958252
Validation loss: 2.1036637226740518

Epoch: 6| Step: 2
Training loss: 1.5870482921600342
Validation loss: 2.10582427183787

Epoch: 6| Step: 3
Training loss: 2.0710010528564453
Validation loss: 2.101980904738108

Epoch: 6| Step: 4
Training loss: 1.8666000366210938
Validation loss: 2.1129297614097595

Epoch: 6| Step: 5
Training loss: 1.9645717144012451
Validation loss: 2.107296566168467

Epoch: 6| Step: 6
Training loss: 2.2080235481262207
Validation loss: 2.0964869459470115

Epoch: 6| Step: 7
Training loss: 2.2580554485321045
Validation loss: 2.1002798279126487

Epoch: 6| Step: 8
Training loss: 2.027484893798828
Validation loss: 2.1011027097702026

Epoch: 6| Step: 9
Training loss: 2.1736316680908203
Validation loss: 2.089859406153361

Epoch: 6| Step: 10
Training loss: 2.2843728065490723
Validation loss: 2.0751766363779702

Epoch: 6| Step: 11
Training loss: 2.5475516319274902
Validation loss: 2.067049423853556

Epoch: 6| Step: 12
Training loss: 2.9229514598846436
Validation loss: 2.0616043408711753

Epoch: 6| Step: 13
Training loss: 2.62020206451416
Validation loss: 2.0685428579648337

Epoch: 65| Step: 0
Training loss: 2.870452880859375
Validation loss: 2.0813152392705283

Epoch: 6| Step: 1
Training loss: 1.8166968822479248
Validation loss: 2.076932509740194

Epoch: 6| Step: 2
Training loss: 2.0507888793945312
Validation loss: 2.084043006102244

Epoch: 6| Step: 3
Training loss: 2.0419416427612305
Validation loss: 2.091712931791941

Epoch: 6| Step: 4
Training loss: 2.043131113052368
Validation loss: 2.0811819036801658

Epoch: 6| Step: 5
Training loss: 1.9940471649169922
Validation loss: 2.078472117582957

Epoch: 6| Step: 6
Training loss: 2.348200559616089
Validation loss: 2.05978270371755

Epoch: 6| Step: 7
Training loss: 1.9873279333114624
Validation loss: 2.0657580892244973

Epoch: 6| Step: 8
Training loss: 2.4249067306518555
Validation loss: 2.089630921681722

Epoch: 6| Step: 9
Training loss: 3.0021004676818848
Validation loss: 2.0968902508417764

Epoch: 6| Step: 10
Training loss: 2.41279935836792
Validation loss: 2.1132326126098633

Epoch: 6| Step: 11
Training loss: 2.481548309326172
Validation loss: 2.1150179704030356

Epoch: 6| Step: 12
Training loss: 1.7519330978393555
Validation loss: 2.116427222887675

Epoch: 6| Step: 13
Training loss: 2.626469612121582
Validation loss: 2.1092092990875244

Epoch: 66| Step: 0
Training loss: 2.031318426132202
Validation loss: 2.078141709168752

Epoch: 6| Step: 1
Training loss: 2.4034862518310547
Validation loss: 2.06812850634257

Epoch: 6| Step: 2
Training loss: 2.512205123901367
Validation loss: 2.058860033750534

Epoch: 6| Step: 3
Training loss: 2.194017171859741
Validation loss: 2.045498510201772

Epoch: 6| Step: 4
Training loss: 2.2894530296325684
Validation loss: 2.0363678336143494

Epoch: 6| Step: 5
Training loss: 2.0926599502563477
Validation loss: 2.037478744983673

Epoch: 6| Step: 6
Training loss: 2.6048707962036133
Validation loss: 2.050995409488678

Epoch: 6| Step: 7
Training loss: 2.8467788696289062
Validation loss: 2.0695958534876504

Epoch: 6| Step: 8
Training loss: 2.0141396522521973
Validation loss: 2.081924478212992

Epoch: 6| Step: 9
Training loss: 2.727229595184326
Validation loss: 2.081430812676748

Epoch: 6| Step: 10
Training loss: 1.676038146018982
Validation loss: 2.077948967615763

Epoch: 6| Step: 11
Training loss: 2.1690831184387207
Validation loss: 2.0571542580922446

Epoch: 6| Step: 12
Training loss: 1.666962742805481
Validation loss: 2.0485498507817588

Epoch: 6| Step: 13
Training loss: 1.9312747716903687
Validation loss: 2.0441665649414062

Epoch: 67| Step: 0
Training loss: 2.3924388885498047
Validation loss: 2.033941646416982

Epoch: 6| Step: 1
Training loss: 2.349590301513672
Validation loss: 2.034213682015737

Epoch: 6| Step: 2
Training loss: 2.1041033267974854
Validation loss: 2.036772310733795

Epoch: 6| Step: 3
Training loss: 2.4656479358673096
Validation loss: 2.053075909614563

Epoch: 6| Step: 4
Training loss: 2.0668699741363525
Validation loss: 2.0546591877937317

Epoch: 6| Step: 5
Training loss: 2.618536949157715
Validation loss: 2.0514410734176636

Epoch: 6| Step: 6
Training loss: 1.8430488109588623
Validation loss: 2.056869367758433

Epoch: 6| Step: 7
Training loss: 2.146019458770752
Validation loss: 2.0610764424006143

Epoch: 6| Step: 8
Training loss: 2.5563502311706543
Validation loss: 2.0574923356374106

Epoch: 6| Step: 9
Training loss: 2.376528739929199
Validation loss: 2.05945485830307

Epoch: 6| Step: 10
Training loss: 2.5022177696228027
Validation loss: 2.056057393550873

Epoch: 6| Step: 11
Training loss: 1.8521385192871094
Validation loss: 2.050638755162557

Epoch: 6| Step: 12
Training loss: 2.0164783000946045
Validation loss: 2.0494465033213296

Epoch: 6| Step: 13
Training loss: 1.8060425519943237
Validation loss: 2.040462334950765

Epoch: 68| Step: 0
Training loss: 2.0699729919433594
Validation loss: 2.0441243847211203

Epoch: 6| Step: 1
Training loss: 1.8168082237243652
Validation loss: 2.040547509988149

Epoch: 6| Step: 2
Training loss: 2.6238884925842285
Validation loss: 2.0398876070976257

Epoch: 6| Step: 3
Training loss: 2.110694408416748
Validation loss: 2.0436434745788574

Epoch: 6| Step: 4
Training loss: 2.6098554134368896
Validation loss: 2.0520838697751365

Epoch: 6| Step: 5
Training loss: 2.0080933570861816
Validation loss: 2.0466046134630838

Epoch: 6| Step: 6
Training loss: 2.3329076766967773
Validation loss: 2.046349585056305

Epoch: 6| Step: 7
Training loss: 2.6601462364196777
Validation loss: 2.044317364692688

Epoch: 6| Step: 8
Training loss: 2.0498549938201904
Validation loss: 2.031583070755005

Epoch: 6| Step: 9
Training loss: 2.162994861602783
Validation loss: 2.023983955383301

Epoch: 6| Step: 10
Training loss: 2.319918155670166
Validation loss: 2.0277409156163535

Epoch: 6| Step: 11
Training loss: 1.8895460367202759
Validation loss: 2.025674819946289

Epoch: 6| Step: 12
Training loss: 2.254211664199829
Validation loss: 2.026911954085032

Epoch: 6| Step: 13
Training loss: 2.031773328781128
Validation loss: 2.0376030802726746

Epoch: 69| Step: 0
Training loss: 2.0691559314727783
Validation loss: 2.0322048465410867

Epoch: 6| Step: 1
Training loss: 1.614621877670288
Validation loss: 2.0402747790018716

Epoch: 6| Step: 2
Training loss: 2.1070146560668945
Validation loss: 2.0507917602856955

Epoch: 6| Step: 3
Training loss: 2.043032169342041
Validation loss: 2.0603208541870117

Epoch: 6| Step: 4
Training loss: 2.4762279987335205
Validation loss: 2.0825356245040894

Epoch: 6| Step: 5
Training loss: 2.3500728607177734
Validation loss: 2.1011667052904763

Epoch: 6| Step: 6
Training loss: 2.1208605766296387
Validation loss: 2.123971144358317

Epoch: 6| Step: 7
Training loss: 2.27605938911438
Validation loss: 2.13474178314209

Epoch: 6| Step: 8
Training loss: 2.9281649589538574
Validation loss: 2.126093566417694

Epoch: 6| Step: 9
Training loss: 1.8860113620758057
Validation loss: 2.111166795094808

Epoch: 6| Step: 10
Training loss: 2.1423416137695312
Validation loss: 2.0943551063537598

Epoch: 6| Step: 11
Training loss: 2.7776780128479004
Validation loss: 2.0595651070276895

Epoch: 6| Step: 12
Training loss: 2.193207263946533
Validation loss: 2.0300270716349282

Epoch: 6| Step: 13
Training loss: 2.414163589477539
Validation loss: 2.037456770737966

Epoch: 70| Step: 0
Training loss: 2.4973130226135254
Validation loss: 2.03691832224528

Epoch: 6| Step: 1
Training loss: 2.3653476238250732
Validation loss: 2.0380557576815286

Epoch: 6| Step: 2
Training loss: 2.2743964195251465
Validation loss: 2.0299734671910605

Epoch: 6| Step: 3
Training loss: 1.7715888023376465
Validation loss: 2.0439188877741494

Epoch: 6| Step: 4
Training loss: 2.578251361846924
Validation loss: 2.042698701222738

Epoch: 6| Step: 5
Training loss: 1.9228893518447876
Validation loss: 2.035923480987549

Epoch: 6| Step: 6
Training loss: 2.611978530883789
Validation loss: 2.0268538991610208

Epoch: 6| Step: 7
Training loss: 1.9807798862457275
Validation loss: 2.0320810874303183

Epoch: 6| Step: 8
Training loss: 2.0388760566711426
Validation loss: 2.0318540732065835

Epoch: 6| Step: 9
Training loss: 1.813439965248108
Validation loss: 2.0369504491488137

Epoch: 6| Step: 10
Training loss: 2.366433620452881
Validation loss: 2.033781131108602

Epoch: 6| Step: 11
Training loss: 2.1828980445861816
Validation loss: 2.032252053419749

Epoch: 6| Step: 12
Training loss: 1.6226611137390137
Validation loss: 2.038671374320984

Epoch: 6| Step: 13
Training loss: 2.7515780925750732
Validation loss: 2.0227563778559365

Epoch: 71| Step: 0
Training loss: 2.3890228271484375
Validation loss: 2.020359834035238

Epoch: 6| Step: 1
Training loss: 2.4243736267089844
Validation loss: 2.0223044951756797

Epoch: 6| Step: 2
Training loss: 2.122603416442871
Validation loss: 2.029873470465342

Epoch: 6| Step: 3
Training loss: 1.7039381265640259
Validation loss: 2.032871743043264

Epoch: 6| Step: 4
Training loss: 2.345414876937866
Validation loss: 2.0399745305379233

Epoch: 6| Step: 5
Training loss: 2.1776931285858154
Validation loss: 2.0507470766703286

Epoch: 6| Step: 6
Training loss: 2.0323400497436523
Validation loss: 2.0558997988700867

Epoch: 6| Step: 7
Training loss: 2.555793046951294
Validation loss: 2.053856452306112

Epoch: 6| Step: 8
Training loss: 2.2688183784484863
Validation loss: 2.0513272881507874

Epoch: 6| Step: 9
Training loss: 1.9874210357666016
Validation loss: 2.0458043813705444

Epoch: 6| Step: 10
Training loss: 2.450530529022217
Validation loss: 2.0355446537335715

Epoch: 6| Step: 11
Training loss: 2.22172212600708
Validation loss: 2.0281012058258057

Epoch: 6| Step: 12
Training loss: 1.9440951347351074
Validation loss: 2.0281654596328735

Epoch: 6| Step: 13
Training loss: 2.5018486976623535
Validation loss: 2.029341439406077

Epoch: 72| Step: 0
Training loss: 2.857029438018799
Validation loss: 2.0289020935694375

Epoch: 6| Step: 1
Training loss: 2.1685991287231445
Validation loss: 2.0239339073499045

Epoch: 6| Step: 2
Training loss: 1.8848352432250977
Validation loss: 2.01373827457428

Epoch: 6| Step: 3
Training loss: 2.6071815490722656
Validation loss: 2.016158123811086

Epoch: 6| Step: 4
Training loss: 2.964696168899536
Validation loss: 2.012697776158651

Epoch: 6| Step: 5
Training loss: 2.0818095207214355
Validation loss: 2.0143876671791077

Epoch: 6| Step: 6
Training loss: 1.2353510856628418
Validation loss: 2.021500527858734

Epoch: 6| Step: 7
Training loss: 2.9154441356658936
Validation loss: 2.0234909454981485

Epoch: 6| Step: 8
Training loss: 1.5760040283203125
Validation loss: 2.025579830010732

Epoch: 6| Step: 9
Training loss: 1.6253634691238403
Validation loss: 2.024066607157389

Epoch: 6| Step: 10
Training loss: 2.071887969970703
Validation loss: 2.0224348505338035

Epoch: 6| Step: 11
Training loss: 2.299910068511963
Validation loss: 2.0242419640223184

Epoch: 6| Step: 12
Training loss: 2.2617909908294678
Validation loss: 2.011773427327474

Epoch: 6| Step: 13
Training loss: 2.1138319969177246
Validation loss: 2.016966382662455

Epoch: 73| Step: 0
Training loss: 1.5080845355987549
Validation loss: 2.014261861642202

Epoch: 6| Step: 1
Training loss: 2.1331100463867188
Validation loss: 2.0258482495943704

Epoch: 6| Step: 2
Training loss: 2.3799686431884766
Validation loss: 2.0154969096183777

Epoch: 6| Step: 3
Training loss: 2.1276135444641113
Validation loss: 2.0304376085599265

Epoch: 6| Step: 4
Training loss: 2.4060938358306885
Validation loss: 2.0430372754732766

Epoch: 6| Step: 5
Training loss: 2.610280990600586
Validation loss: 2.04301917552948

Epoch: 6| Step: 6
Training loss: 2.5914902687072754
Validation loss: 2.041300813357035

Epoch: 6| Step: 7
Training loss: 2.7056915760040283
Validation loss: 2.0469772815704346

Epoch: 6| Step: 8
Training loss: 2.19020414352417
Validation loss: 2.02094038327535

Epoch: 6| Step: 9
Training loss: 2.257436990737915
Validation loss: 2.0169864892959595

Epoch: 6| Step: 10
Training loss: 1.7048529386520386
Validation loss: 2.0148527224858603

Epoch: 6| Step: 11
Training loss: 2.052371025085449
Validation loss: 2.0133989453315735

Epoch: 6| Step: 12
Training loss: 1.958837866783142
Validation loss: 2.0138914783795676

Epoch: 6| Step: 13
Training loss: 1.9724112749099731
Validation loss: 2.0063608288764954

Epoch: 74| Step: 0
Training loss: 2.430936098098755
Validation loss: 2.023168385028839

Epoch: 6| Step: 1
Training loss: 2.0442111492156982
Validation loss: 2.012731432914734

Epoch: 6| Step: 2
Training loss: 1.8254483938217163
Validation loss: 2.0112701058387756

Epoch: 6| Step: 3
Training loss: 2.015460968017578
Validation loss: 2.0132665435473123

Epoch: 6| Step: 4
Training loss: 2.4205477237701416
Validation loss: 2.0164208809534707

Epoch: 6| Step: 5
Training loss: 2.3042831420898438
Validation loss: 2.0125705202420554

Epoch: 6| Step: 6
Training loss: 2.2183780670166016
Validation loss: 2.020201802253723

Epoch: 6| Step: 7
Training loss: 1.858163595199585
Validation loss: 2.011369228363037

Epoch: 6| Step: 8
Training loss: 1.5396517515182495
Validation loss: 2.025384704271952

Epoch: 6| Step: 9
Training loss: 2.400848865509033
Validation loss: 2.0186583201090493

Epoch: 6| Step: 10
Training loss: 2.6446750164031982
Validation loss: 2.0252649982770285

Epoch: 6| Step: 11
Training loss: 1.939263105392456
Validation loss: 2.019106149673462

Epoch: 6| Step: 12
Training loss: 2.8847122192382812
Validation loss: 2.007920205593109

Epoch: 6| Step: 13
Training loss: 2.070176362991333
Validation loss: 2.022642970085144

Epoch: 75| Step: 0
Training loss: 2.595090866088867
Validation loss: 2.0165394147237143

Epoch: 6| Step: 1
Training loss: 1.737933874130249
Validation loss: 2.0271253983179727

Epoch: 6| Step: 2
Training loss: 2.4844088554382324
Validation loss: 2.043101191520691

Epoch: 6| Step: 3
Training loss: 2.2501938343048096
Validation loss: 2.0408536195755005

Epoch: 6| Step: 4
Training loss: 2.438689708709717
Validation loss: 2.053371846675873

Epoch: 6| Step: 5
Training loss: 2.1306405067443848
Validation loss: 2.038416862487793

Epoch: 6| Step: 6
Training loss: 1.8483881950378418
Validation loss: 2.03438808520635

Epoch: 6| Step: 7
Training loss: 2.2864222526550293
Validation loss: 2.0434306263923645

Epoch: 6| Step: 8
Training loss: 1.973623514175415
Validation loss: 2.027604560057322

Epoch: 6| Step: 9
Training loss: 1.9623003005981445
Validation loss: 2.030545155207316

Epoch: 6| Step: 10
Training loss: 1.9390442371368408
Validation loss: 2.014954447746277

Epoch: 6| Step: 11
Training loss: 2.074486255645752
Validation loss: 2.0131842692693076

Epoch: 6| Step: 12
Training loss: 2.751613140106201
Validation loss: 2.0107622941335044

Epoch: 6| Step: 13
Training loss: 2.0372207164764404
Validation loss: 2.013836602369944

Epoch: 76| Step: 0
Training loss: 2.1443679332733154
Validation loss: 2.0250392158826194

Epoch: 6| Step: 1
Training loss: 1.6919498443603516
Validation loss: 2.022587478160858

Epoch: 6| Step: 2
Training loss: 2.5295121669769287
Validation loss: 2.025117556254069

Epoch: 6| Step: 3
Training loss: 1.8836236000061035
Validation loss: 2.027843256791433

Epoch: 6| Step: 4
Training loss: 2.933964967727661
Validation loss: 2.0270756681760154

Epoch: 6| Step: 5
Training loss: 2.673726797103882
Validation loss: 2.0280035734176636

Epoch: 6| Step: 6
Training loss: 1.5122276544570923
Validation loss: 2.0302997827529907

Epoch: 6| Step: 7
Training loss: 1.886012315750122
Validation loss: 2.0324602127075195

Epoch: 6| Step: 8
Training loss: 2.055894374847412
Validation loss: 2.028724114100138

Epoch: 6| Step: 9
Training loss: 2.505490303039551
Validation loss: 2.018133501211802

Epoch: 6| Step: 10
Training loss: 2.3884496688842773
Validation loss: 2.0115328232447305

Epoch: 6| Step: 11
Training loss: 2.410167694091797
Validation loss: 2.024471958478292

Epoch: 6| Step: 12
Training loss: 1.839798927307129
Validation loss: 2.0233555833498635

Epoch: 6| Step: 13
Training loss: 2.347236394882202
Validation loss: 2.0295304457346597

Epoch: 77| Step: 0
Training loss: 2.281264305114746
Validation loss: 2.038116236527761

Epoch: 6| Step: 1
Training loss: 2.2780139446258545
Validation loss: 2.0328930616378784

Epoch: 6| Step: 2
Training loss: 1.8226349353790283
Validation loss: 2.035567283630371

Epoch: 6| Step: 3
Training loss: 2.115828514099121
Validation loss: 2.039117753505707

Epoch: 6| Step: 4
Training loss: 2.415699005126953
Validation loss: 2.0333043138186135

Epoch: 6| Step: 5
Training loss: 1.8894591331481934
Validation loss: 2.0267885526021323

Epoch: 6| Step: 6
Training loss: 1.8575581312179565
Validation loss: 2.0227402647336326

Epoch: 6| Step: 7
Training loss: 2.434192657470703
Validation loss: 2.021674315134684

Epoch: 6| Step: 8
Training loss: 2.6783218383789062
Validation loss: 2.013420740763346

Epoch: 6| Step: 9
Training loss: 2.0597314834594727
Validation loss: 2.0110084215799966

Epoch: 6| Step: 10
Training loss: 2.383427619934082
Validation loss: 2.019787073135376

Epoch: 6| Step: 11
Training loss: 1.6552886962890625
Validation loss: 2.024742364883423

Epoch: 6| Step: 12
Training loss: 2.112642288208008
Validation loss: 2.012324253718058

Epoch: 6| Step: 13
Training loss: 2.5545997619628906
Validation loss: 2.0243131518363953

Epoch: 78| Step: 0
Training loss: 2.3794164657592773
Validation loss: 2.014012654622396

Epoch: 6| Step: 1
Training loss: 2.0042366981506348
Validation loss: 2.013329108556112

Epoch: 6| Step: 2
Training loss: 2.2599494457244873
Validation loss: 2.013398051261902

Epoch: 6| Step: 3
Training loss: 1.9890531301498413
Validation loss: 2.012121260166168

Epoch: 6| Step: 4
Training loss: 2.255580425262451
Validation loss: 2.0133318305015564

Epoch: 6| Step: 5
Training loss: 2.534073829650879
Validation loss: 2.01267679532369

Epoch: 6| Step: 6
Training loss: 2.11734676361084
Validation loss: 2.023616313934326

Epoch: 6| Step: 7
Training loss: 2.0333166122436523
Validation loss: 2.0292606949806213

Epoch: 6| Step: 8
Training loss: 2.4204978942871094
Validation loss: 2.0418022076288858

Epoch: 6| Step: 9
Training loss: 1.8577556610107422
Validation loss: 2.0467045108477273

Epoch: 6| Step: 10
Training loss: 1.788748025894165
Validation loss: 2.0522574186325073

Epoch: 6| Step: 11
Training loss: 2.4030141830444336
Validation loss: 2.046156108379364

Epoch: 6| Step: 12
Training loss: 2.388185501098633
Validation loss: 2.0593663454055786

Epoch: 6| Step: 13
Training loss: 1.7920926809310913
Validation loss: 2.0568585793177285

Epoch: 79| Step: 0
Training loss: 1.7622190713882446
Validation loss: 2.0521291693051658

Epoch: 6| Step: 1
Training loss: 1.6229805946350098
Validation loss: 2.0464446345965066

Epoch: 6| Step: 2
Training loss: 2.605926036834717
Validation loss: 2.042538126309713

Epoch: 6| Step: 3
Training loss: 2.0075321197509766
Validation loss: 2.0324514706929526

Epoch: 6| Step: 4
Training loss: 2.08066725730896
Validation loss: 2.025221347808838

Epoch: 6| Step: 5
Training loss: 2.153127431869507
Validation loss: 2.016126294930776

Epoch: 6| Step: 6
Training loss: 2.4323997497558594
Validation loss: 2.0279871622721353

Epoch: 6| Step: 7
Training loss: 2.2682344913482666
Validation loss: 2.029115676879883

Epoch: 6| Step: 8
Training loss: 2.537118434906006
Validation loss: 2.0397579868634543

Epoch: 6| Step: 9
Training loss: 2.499729633331299
Validation loss: 2.0348299940427146

Epoch: 6| Step: 10
Training loss: 2.296715259552002
Validation loss: 2.041233698527018

Epoch: 6| Step: 11
Training loss: 2.3534674644470215
Validation loss: 2.0396982034047446

Epoch: 6| Step: 12
Training loss: 1.97642982006073
Validation loss: 2.035469671090444

Epoch: 6| Step: 13
Training loss: 1.9384558200836182
Validation loss: 2.029925982157389

Epoch: 80| Step: 0
Training loss: 2.180757522583008
Validation loss: 2.023746212323507

Epoch: 6| Step: 1
Training loss: 1.9230237007141113
Validation loss: 2.030116935571035

Epoch: 6| Step: 2
Training loss: 1.9884164333343506
Validation loss: 2.029349207878113

Epoch: 6| Step: 3
Training loss: 1.8415770530700684
Validation loss: 2.015217920144399

Epoch: 6| Step: 4
Training loss: 2.3827133178710938
Validation loss: 2.020690460999807

Epoch: 6| Step: 5
Training loss: 2.314054012298584
Validation loss: 2.0153361757596335

Epoch: 6| Step: 6
Training loss: 2.2437009811401367
Validation loss: 2.016170561313629

Epoch: 6| Step: 7
Training loss: 1.7193468809127808
Validation loss: 2.020960291226705

Epoch: 6| Step: 8
Training loss: 1.6637400388717651
Validation loss: 2.016747852166494

Epoch: 6| Step: 9
Training loss: 2.625936985015869
Validation loss: 2.0119139552116394

Epoch: 6| Step: 10
Training loss: 2.2215542793273926
Validation loss: 2.012851436932882

Epoch: 6| Step: 11
Training loss: 2.651430606842041
Validation loss: 2.0110808610916138

Epoch: 6| Step: 12
Training loss: 2.61492919921875
Validation loss: 2.0188979506492615

Epoch: 6| Step: 13
Training loss: 1.9512630701065063
Validation loss: 2.0228231151898703

Epoch: 81| Step: 0
Training loss: 2.7660655975341797
Validation loss: 2.028375744819641

Epoch: 6| Step: 1
Training loss: 2.2790443897247314
Validation loss: 2.0470216472943625

Epoch: 6| Step: 2
Training loss: 1.9981775283813477
Validation loss: 2.0446919004122415

Epoch: 6| Step: 3
Training loss: 2.527308464050293
Validation loss: 2.04427037636439

Epoch: 6| Step: 4
Training loss: 2.304316997528076
Validation loss: 2.024927298227946

Epoch: 6| Step: 5
Training loss: 2.5233120918273926
Validation loss: 2.0208935936292014

Epoch: 6| Step: 6
Training loss: 1.8045015335083008
Validation loss: 2.015570859114329

Epoch: 6| Step: 7
Training loss: 2.1632442474365234
Validation loss: 2.0130716959635415

Epoch: 6| Step: 8
Training loss: 2.0277369022369385
Validation loss: 2.0261141856511435

Epoch: 6| Step: 9
Training loss: 1.8898136615753174
Validation loss: 2.0183530847231546

Epoch: 6| Step: 10
Training loss: 2.2243356704711914
Validation loss: 2.034075617790222

Epoch: 6| Step: 11
Training loss: 2.1867074966430664
Validation loss: 2.0425991217295327

Epoch: 6| Step: 12
Training loss: 2.070481777191162
Validation loss: 2.0405110716819763

Epoch: 6| Step: 13
Training loss: 2.0153563022613525
Validation loss: 2.0532732407251992

Epoch: 82| Step: 0
Training loss: 1.4243872165679932
Validation loss: 2.044127583503723

Epoch: 6| Step: 1
Training loss: 2.5822582244873047
Validation loss: 2.0281136631965637

Epoch: 6| Step: 2
Training loss: 2.335085868835449
Validation loss: 2.0258546670277915

Epoch: 6| Step: 3
Training loss: 3.2838287353515625
Validation loss: 2.024970988432566

Epoch: 6| Step: 4
Training loss: 2.4448680877685547
Validation loss: 2.0297433535257974

Epoch: 6| Step: 5
Training loss: 2.352005958557129
Validation loss: 2.0227381189664206

Epoch: 6| Step: 6
Training loss: 2.532024383544922
Validation loss: 2.0211398601531982

Epoch: 6| Step: 7
Training loss: 2.4955856800079346
Validation loss: 2.0174371004104614

Epoch: 6| Step: 8
Training loss: 1.8711680173873901
Validation loss: 2.0152584115664163

Epoch: 6| Step: 9
Training loss: 2.0566940307617188
Validation loss: 2.0088149905204773

Epoch: 6| Step: 10
Training loss: 1.6739134788513184
Validation loss: 2.013967434565226

Epoch: 6| Step: 11
Training loss: 1.5645408630371094
Validation loss: 2.0204360286394754

Epoch: 6| Step: 12
Training loss: 2.4199185371398926
Validation loss: 2.019358774026235

Epoch: 6| Step: 13
Training loss: 1.2766551971435547
Validation loss: 2.022468407948812

Epoch: 83| Step: 0
Training loss: 2.068300724029541
Validation loss: 2.015811880429586

Epoch: 6| Step: 1
Training loss: 2.500713586807251
Validation loss: 2.0243173042933145

Epoch: 6| Step: 2
Training loss: 2.5933663845062256
Validation loss: 2.028764565785726

Epoch: 6| Step: 3
Training loss: 1.9897290468215942
Validation loss: 2.0365561644236245

Epoch: 6| Step: 4
Training loss: 1.2343655824661255
Validation loss: 2.0334554513295493

Epoch: 6| Step: 5
Training loss: 2.4414443969726562
Validation loss: 2.0286471446355185

Epoch: 6| Step: 6
Training loss: 2.4667253494262695
Validation loss: 2.0313765605290732

Epoch: 6| Step: 7
Training loss: 1.8323618173599243
Validation loss: 2.02349720398585

Epoch: 6| Step: 8
Training loss: 2.3698887825012207
Validation loss: 2.0239612062772117

Epoch: 6| Step: 9
Training loss: 2.4452710151672363
Validation loss: 2.0251744389533997

Epoch: 6| Step: 10
Training loss: 1.562686562538147
Validation loss: 2.0131946206092834

Epoch: 6| Step: 11
Training loss: 2.0453286170959473
Validation loss: 2.0104203621546426

Epoch: 6| Step: 12
Training loss: 2.3298113346099854
Validation loss: 2.0120076735814414

Epoch: 6| Step: 13
Training loss: 2.1491479873657227
Validation loss: 2.0114665230115256

Epoch: 84| Step: 0
Training loss: 1.9045007228851318
Validation loss: 2.016835312048594

Epoch: 6| Step: 1
Training loss: 2.258544921875
Validation loss: 2.0220404664675393

Epoch: 6| Step: 2
Training loss: 2.211353302001953
Validation loss: 2.0246179699897766

Epoch: 6| Step: 3
Training loss: 2.623046636581421
Validation loss: 2.018285552660624

Epoch: 6| Step: 4
Training loss: 1.8083946704864502
Validation loss: 2.0194838841756186

Epoch: 6| Step: 5
Training loss: 1.429476022720337
Validation loss: 2.0281520883242288

Epoch: 6| Step: 6
Training loss: 1.7948634624481201
Validation loss: 2.015592038631439

Epoch: 6| Step: 7
Training loss: 2.441873550415039
Validation loss: 2.024123469988505

Epoch: 6| Step: 8
Training loss: 2.4531173706054688
Validation loss: 2.023878355820974

Epoch: 6| Step: 9
Training loss: 2.7040882110595703
Validation loss: 2.015465279420217

Epoch: 6| Step: 10
Training loss: 1.8021700382232666
Validation loss: 2.026004989941915

Epoch: 6| Step: 11
Training loss: 2.2617545127868652
Validation loss: 2.025005261103312

Epoch: 6| Step: 12
Training loss: 2.185384750366211
Validation loss: 2.0265361666679382

Epoch: 6| Step: 13
Training loss: 2.3167805671691895
Validation loss: 2.019297023614248

Epoch: 85| Step: 0
Training loss: 1.341247320175171
Validation loss: 2.02095361550649

Epoch: 6| Step: 1
Training loss: 1.879104733467102
Validation loss: 2.024030645688375

Epoch: 6| Step: 2
Training loss: 2.673213243484497
Validation loss: 2.0198368430137634

Epoch: 6| Step: 3
Training loss: 1.661907434463501
Validation loss: 2.015152851740519

Epoch: 6| Step: 4
Training loss: 2.7322449684143066
Validation loss: 2.0102218190828958

Epoch: 6| Step: 5
Training loss: 2.397005558013916
Validation loss: 2.0144493778546653

Epoch: 6| Step: 6
Training loss: 2.580409288406372
Validation loss: 2.01130074262619

Epoch: 6| Step: 7
Training loss: 2.3288660049438477
Validation loss: 2.0163793166478476

Epoch: 6| Step: 8
Training loss: 1.9192392826080322
Validation loss: 2.022400418917338

Epoch: 6| Step: 9
Training loss: 2.2514538764953613
Validation loss: 2.015034019947052

Epoch: 6| Step: 10
Training loss: 2.085024833679199
Validation loss: 2.023456255594889

Epoch: 6| Step: 11
Training loss: 2.1846370697021484
Validation loss: 2.026549279689789

Epoch: 6| Step: 12
Training loss: 2.5743942260742188
Validation loss: 2.0147549510002136

Epoch: 6| Step: 13
Training loss: 1.422609806060791
Validation loss: 2.0248965422312417

Epoch: 86| Step: 0
Training loss: 2.2602670192718506
Validation loss: 2.025540590286255

Epoch: 6| Step: 1
Training loss: 2.7153635025024414
Validation loss: 2.0252756675084433

Epoch: 6| Step: 2
Training loss: 1.9926577806472778
Validation loss: 2.0377158721288047

Epoch: 6| Step: 3
Training loss: 2.7647805213928223
Validation loss: 2.0337818264961243

Epoch: 6| Step: 4
Training loss: 1.3741942644119263
Validation loss: 2.030191441377004

Epoch: 6| Step: 5
Training loss: 1.8433382511138916
Validation loss: 2.0287908911705017

Epoch: 6| Step: 6
Training loss: 2.156179904937744
Validation loss: 2.0293086965878806

Epoch: 6| Step: 7
Training loss: 2.410123825073242
Validation loss: 2.022479752699534

Epoch: 6| Step: 8
Training loss: 1.7542619705200195
Validation loss: 2.0228833158810935

Epoch: 6| Step: 9
Training loss: 2.1012587547302246
Validation loss: 2.0257832407951355

Epoch: 6| Step: 10
Training loss: 2.0219388008117676
Validation loss: 2.027169187863668

Epoch: 6| Step: 11
Training loss: 2.0209431648254395
Validation loss: 2.0253480275472007

Epoch: 6| Step: 12
Training loss: 1.939495325088501
Validation loss: 2.027610500653585

Epoch: 6| Step: 13
Training loss: 2.685519218444824
Validation loss: 2.0320730209350586

Epoch: 87| Step: 0
Training loss: 1.9411143064498901
Validation loss: 2.0208342472712197

Epoch: 6| Step: 1
Training loss: 2.0597727298736572
Validation loss: 2.017104705174764

Epoch: 6| Step: 2
Training loss: 2.112691879272461
Validation loss: 2.014949639638265

Epoch: 6| Step: 3
Training loss: 1.828445315361023
Validation loss: 2.0225530664126077

Epoch: 6| Step: 4
Training loss: 2.5509376525878906
Validation loss: 2.023688554763794

Epoch: 6| Step: 5
Training loss: 1.8506908416748047
Validation loss: 2.0242623885472617

Epoch: 6| Step: 6
Training loss: 2.3797450065612793
Validation loss: 2.0312806169191995

Epoch: 6| Step: 7
Training loss: 3.1257357597351074
Validation loss: 2.045401930809021

Epoch: 6| Step: 8
Training loss: 2.633607864379883
Validation loss: 2.0323663353919983

Epoch: 6| Step: 9
Training loss: 1.9903367757797241
Validation loss: 2.035288234551748

Epoch: 6| Step: 10
Training loss: 1.8842812776565552
Validation loss: 2.021815299987793

Epoch: 6| Step: 11
Training loss: 2.036520004272461
Validation loss: 2.0237196882565818

Epoch: 6| Step: 12
Training loss: 1.4818546772003174
Validation loss: 2.016513486703237

Epoch: 6| Step: 13
Training loss: 2.4526052474975586
Validation loss: 2.009538471698761

Epoch: 88| Step: 0
Training loss: 2.1568892002105713
Validation loss: 2.01516455411911

Epoch: 6| Step: 1
Training loss: 2.237370252609253
Validation loss: 2.006449202696482

Epoch: 6| Step: 2
Training loss: 1.936881184577942
Validation loss: 2.0148999094963074

Epoch: 6| Step: 3
Training loss: 2.2508344650268555
Validation loss: 2.0247145096460977

Epoch: 6| Step: 4
Training loss: 2.706416606903076
Validation loss: 2.023287773132324

Epoch: 6| Step: 5
Training loss: 1.6241496801376343
Validation loss: 2.0236183603604636

Epoch: 6| Step: 6
Training loss: 2.1169095039367676
Validation loss: 2.0222203930219016

Epoch: 6| Step: 7
Training loss: 2.6682865619659424
Validation loss: 2.030275364716848

Epoch: 6| Step: 8
Training loss: 2.442535877227783
Validation loss: 2.023257791996002

Epoch: 6| Step: 9
Training loss: 1.854919195175171
Validation loss: 2.0161486864089966

Epoch: 6| Step: 10
Training loss: 2.338526725769043
Validation loss: 2.0174970030784607

Epoch: 6| Step: 11
Training loss: 2.318734645843506
Validation loss: 2.0114206870396933

Epoch: 6| Step: 12
Training loss: 1.8040070533752441
Validation loss: 2.012123147646586

Epoch: 6| Step: 13
Training loss: 1.8325345516204834
Validation loss: 2.020486533641815

Epoch: 89| Step: 0
Training loss: 2.1188833713531494
Validation loss: 2.019336223602295

Epoch: 6| Step: 1
Training loss: 1.893959641456604
Validation loss: 2.0411388079325357

Epoch: 6| Step: 2
Training loss: 1.9731051921844482
Validation loss: 2.0375996430714927

Epoch: 6| Step: 3
Training loss: 2.4189376831054688
Validation loss: 2.0431628425916037

Epoch: 6| Step: 4
Training loss: 1.8240809440612793
Validation loss: 2.0486642519632974

Epoch: 6| Step: 5
Training loss: 1.6961356401443481
Validation loss: 2.0448887944221497

Epoch: 6| Step: 6
Training loss: 2.3947346210479736
Validation loss: 2.0438146193822226

Epoch: 6| Step: 7
Training loss: 2.5894811153411865
Validation loss: 2.029529651006063

Epoch: 6| Step: 8
Training loss: 2.578874349594116
Validation loss: 2.0304661989212036

Epoch: 6| Step: 9
Training loss: 2.76603102684021
Validation loss: 2.0257877310117087

Epoch: 6| Step: 10
Training loss: 1.8599222898483276
Validation loss: 2.033753196398417

Epoch: 6| Step: 11
Training loss: 2.206996202468872
Validation loss: 2.021255830923716

Epoch: 6| Step: 12
Training loss: 1.9343607425689697
Validation loss: 2.0222936471303306

Epoch: 6| Step: 13
Training loss: 2.03629207611084
Validation loss: 2.0219959815343223

Epoch: 90| Step: 0
Training loss: 2.0279300212860107
Validation loss: 2.0123494068781533

Epoch: 6| Step: 1
Training loss: 2.3278346061706543
Validation loss: 2.0214957197507224

Epoch: 6| Step: 2
Training loss: 2.1569128036499023
Validation loss: 2.0162433783213296

Epoch: 6| Step: 3
Training loss: 2.164196729660034
Validation loss: 2.013918936252594

Epoch: 6| Step: 4
Training loss: 1.7830443382263184
Validation loss: 2.0179970264434814

Epoch: 6| Step: 5
Training loss: 2.3572487831115723
Validation loss: 2.0145249168078103

Epoch: 6| Step: 6
Training loss: 2.698676586151123
Validation loss: 2.018171012401581

Epoch: 6| Step: 7
Training loss: 1.545231819152832
Validation loss: 2.0129294792811074

Epoch: 6| Step: 8
Training loss: 2.300924301147461
Validation loss: 2.0236301819483438

Epoch: 6| Step: 9
Training loss: 1.799067735671997
Validation loss: 2.0175400972366333

Epoch: 6| Step: 10
Training loss: 2.208427906036377
Validation loss: 2.027625580628713

Epoch: 6| Step: 11
Training loss: 2.275883913040161
Validation loss: 2.028439382712046

Epoch: 6| Step: 12
Training loss: 2.171685218811035
Validation loss: 2.022439201672872

Epoch: 6| Step: 13
Training loss: 1.9895753860473633
Validation loss: 2.0213802655537925

Epoch: 91| Step: 0
Training loss: 1.7634515762329102
Validation loss: 2.0362096627553306

Epoch: 6| Step: 1
Training loss: 2.3729639053344727
Validation loss: 2.0418731570243835

Epoch: 6| Step: 2
Training loss: 1.6613094806671143
Validation loss: 2.048341949780782

Epoch: 6| Step: 3
Training loss: 2.0269737243652344
Validation loss: 2.0643606583277383

Epoch: 6| Step: 4
Training loss: 1.8711793422698975
Validation loss: 2.068981091181437

Epoch: 6| Step: 5
Training loss: 1.8949716091156006
Validation loss: 2.051318128903707

Epoch: 6| Step: 6
Training loss: 2.0319395065307617
Validation loss: 2.040276070435842

Epoch: 6| Step: 7
Training loss: 2.776644706726074
Validation loss: 2.037613034248352

Epoch: 6| Step: 8
Training loss: 2.352320432662964
Validation loss: 2.013479689757029

Epoch: 6| Step: 9
Training loss: 2.4913010597229004
Validation loss: 2.010371744632721

Epoch: 6| Step: 10
Training loss: 2.0569264888763428
Validation loss: 2.024891714255015

Epoch: 6| Step: 11
Training loss: 2.612771511077881
Validation loss: 2.015679677327474

Epoch: 6| Step: 12
Training loss: 2.153878927230835
Validation loss: 2.020780881245931

Epoch: 6| Step: 13
Training loss: 2.252296209335327
Validation loss: 2.026621381441752

Epoch: 92| Step: 0
Training loss: 1.8963086605072021
Validation loss: 2.027149498462677

Epoch: 6| Step: 1
Training loss: 1.8354092836380005
Validation loss: 2.025370200475057

Epoch: 6| Step: 2
Training loss: 1.9210171699523926
Validation loss: 2.028418699900309

Epoch: 6| Step: 3
Training loss: 2.079751968383789
Validation loss: 2.026810566584269

Epoch: 6| Step: 4
Training loss: 2.7407419681549072
Validation loss: 2.0247179667154946

Epoch: 6| Step: 5
Training loss: 1.4702789783477783
Validation loss: 2.0199589927991233

Epoch: 6| Step: 6
Training loss: 2.8228540420532227
Validation loss: 2.022378226121267

Epoch: 6| Step: 7
Training loss: 2.0043020248413086
Validation loss: 2.0212101538976035

Epoch: 6| Step: 8
Training loss: 1.8914408683776855
Validation loss: 2.0111426512400308

Epoch: 6| Step: 9
Training loss: 2.5692973136901855
Validation loss: 2.010936141014099

Epoch: 6| Step: 10
Training loss: 1.848745346069336
Validation loss: 2.00724204381307

Epoch: 6| Step: 11
Training loss: 2.5410234928131104
Validation loss: 2.0185988744099936

Epoch: 6| Step: 12
Training loss: 2.7087414264678955
Validation loss: 2.0165926615397134

Epoch: 6| Step: 13
Training loss: 1.7486064434051514
Validation loss: 2.0227035681406655

Epoch: 93| Step: 0
Training loss: 2.025517702102661
Validation loss: 2.02985946337382

Epoch: 6| Step: 1
Training loss: 1.9844619035720825
Validation loss: 2.0256248911221824

Epoch: 6| Step: 2
Training loss: 2.2341461181640625
Validation loss: 2.016131063302358

Epoch: 6| Step: 3
Training loss: 2.431915283203125
Validation loss: 2.019373039404551

Epoch: 6| Step: 4
Training loss: 2.0636239051818848
Validation loss: 2.011598616838455

Epoch: 6| Step: 5
Training loss: 1.908893346786499
Validation loss: 2.0079465508461

Epoch: 6| Step: 6
Training loss: 1.9112062454223633
Validation loss: 2.0114760994911194

Epoch: 6| Step: 7
Training loss: 2.5220344066619873
Validation loss: 2.003582775592804

Epoch: 6| Step: 8
Training loss: 2.0164523124694824
Validation loss: 2.0221983989079795

Epoch: 6| Step: 9
Training loss: 2.1228702068328857
Validation loss: 2.0233409802118936

Epoch: 6| Step: 10
Training loss: 2.5296630859375
Validation loss: 2.0236661434173584

Epoch: 6| Step: 11
Training loss: 1.7120413780212402
Validation loss: 2.0123104453086853

Epoch: 6| Step: 12
Training loss: 2.303760290145874
Validation loss: 2.024185319741567

Epoch: 6| Step: 13
Training loss: 2.1590752601623535
Validation loss: 2.029157598813375

Epoch: 94| Step: 0
Training loss: 2.1583433151245117
Validation loss: 2.0248159368832908

Epoch: 6| Step: 1
Training loss: 2.621967315673828
Validation loss: 2.017614940802256

Epoch: 6| Step: 2
Training loss: 1.5921987295150757
Validation loss: 2.0218412478764853

Epoch: 6| Step: 3
Training loss: 2.245544910430908
Validation loss: 2.0194014509518943

Epoch: 6| Step: 4
Training loss: 2.4228687286376953
Validation loss: 2.011447787284851

Epoch: 6| Step: 5
Training loss: 1.8340198993682861
Validation loss: 2.0187407732009888

Epoch: 6| Step: 6
Training loss: 1.907900094985962
Validation loss: 2.0134440660476685

Epoch: 6| Step: 7
Training loss: 2.2617106437683105
Validation loss: 2.024026115735372

Epoch: 6| Step: 8
Training loss: 1.951017141342163
Validation loss: 2.028998374938965

Epoch: 6| Step: 9
Training loss: 2.3154704570770264
Validation loss: 2.0183879733085632

Epoch: 6| Step: 10
Training loss: 2.13155460357666
Validation loss: 2.0186794797579446

Epoch: 6| Step: 11
Training loss: 2.638430595397949
Validation loss: 2.0171360770861306

Epoch: 6| Step: 12
Training loss: 1.7665294408798218
Validation loss: 1.997197429339091

Epoch: 6| Step: 13
Training loss: 1.8109759092330933
Validation loss: 2.004934549331665

Epoch: 95| Step: 0
Training loss: 2.505462646484375
Validation loss: 2.0086742639541626

Epoch: 6| Step: 1
Training loss: 1.973024845123291
Validation loss: 2.0125213662783303

Epoch: 6| Step: 2
Training loss: 2.3204994201660156
Validation loss: 2.0107144514719644

Epoch: 6| Step: 3
Training loss: 2.200758695602417
Validation loss: 2.0080069104830423

Epoch: 6| Step: 4
Training loss: 2.1344218254089355
Validation loss: 2.0167651971181235

Epoch: 6| Step: 5
Training loss: 2.2355613708496094
Validation loss: 2.0040642817815146

Epoch: 6| Step: 6
Training loss: 1.5688477754592896
Validation loss: 2.0107287168502808

Epoch: 6| Step: 7
Training loss: 1.8947776556015015
Validation loss: 2.009853641192118

Epoch: 6| Step: 8
Training loss: 1.2505464553833008
Validation loss: 2.015111049016317

Epoch: 6| Step: 9
Training loss: 2.511164665222168
Validation loss: 2.0221873124440513

Epoch: 6| Step: 10
Training loss: 1.971515417098999
Validation loss: 2.02053294579188

Epoch: 6| Step: 11
Training loss: 2.707087993621826
Validation loss: 2.033658981323242

Epoch: 6| Step: 12
Training loss: 2.324568271636963
Validation loss: 2.0341426531473794

Epoch: 6| Step: 13
Training loss: 2.1101460456848145
Validation loss: 2.0290629863739014

Epoch: 96| Step: 0
Training loss: 2.344205856323242
Validation loss: 2.0401662786801658

Epoch: 6| Step: 1
Training loss: 1.6224615573883057
Validation loss: 2.0386102000872293

Epoch: 6| Step: 2
Training loss: 1.5385050773620605
Validation loss: 2.03906379143397

Epoch: 6| Step: 3
Training loss: 2.129362106323242
Validation loss: 2.032031496365865

Epoch: 6| Step: 4
Training loss: 2.014772415161133
Validation loss: 2.0412926276524863

Epoch: 6| Step: 5
Training loss: 2.955939292907715
Validation loss: 2.0347312688827515

Epoch: 6| Step: 6
Training loss: 2.0723190307617188
Validation loss: 2.0311109026273093

Epoch: 6| Step: 7
Training loss: 2.4665727615356445
Validation loss: 2.02410626411438

Epoch: 6| Step: 8
Training loss: 2.1277613639831543
Validation loss: 2.018208702405294

Epoch: 6| Step: 9
Training loss: 2.041198253631592
Validation loss: 2.018640875816345

Epoch: 6| Step: 10
Training loss: 2.081666946411133
Validation loss: 2.0162697434425354

Epoch: 6| Step: 11
Training loss: 2.150017261505127
Validation loss: 2.0160836577415466

Epoch: 6| Step: 12
Training loss: 2.220689296722412
Validation loss: 2.0181472301483154

Epoch: 6| Step: 13
Training loss: 1.9660756587982178
Validation loss: 2.0077239672342935

Epoch: 97| Step: 0
Training loss: 2.509162425994873
Validation loss: 2.011934538682302

Epoch: 6| Step: 1
Training loss: 2.248591423034668
Validation loss: 2.016815702120463

Epoch: 6| Step: 2
Training loss: 1.9438459873199463
Validation loss: 2.024673283100128

Epoch: 6| Step: 3
Training loss: 2.598862409591675
Validation loss: 2.019576847553253

Epoch: 6| Step: 4
Training loss: 2.4541473388671875
Validation loss: 2.0201828281084695

Epoch: 6| Step: 5
Training loss: 2.085576057434082
Validation loss: 2.0145572423934937

Epoch: 6| Step: 6
Training loss: 2.317044973373413
Validation loss: 2.0159576535224915

Epoch: 6| Step: 7
Training loss: 1.4407610893249512
Validation loss: 2.0186926325162253

Epoch: 6| Step: 8
Training loss: 2.003931760787964
Validation loss: 2.0093940893809

Epoch: 6| Step: 9
Training loss: 1.7215471267700195
Validation loss: 2.019623418649038

Epoch: 6| Step: 10
Training loss: 2.4411394596099854
Validation loss: 2.021896004676819

Epoch: 6| Step: 11
Training loss: 2.1830718517303467
Validation loss: 2.0197341044743857

Epoch: 6| Step: 12
Training loss: 1.5157814025878906
Validation loss: 2.0353784561157227

Epoch: 6| Step: 13
Training loss: 2.2031373977661133
Validation loss: 2.0245954195658364

Epoch: 98| Step: 0
Training loss: 2.296485424041748
Validation loss: 2.040661573410034

Epoch: 6| Step: 1
Training loss: 2.1202492713928223
Validation loss: 2.031004329522451

Epoch: 6| Step: 2
Training loss: 2.227543354034424
Validation loss: 2.0366867184638977

Epoch: 6| Step: 3
Training loss: 2.356464385986328
Validation loss: 2.0304002364476523

Epoch: 6| Step: 4
Training loss: 2.304980993270874
Validation loss: 2.0327450235684714

Epoch: 6| Step: 5
Training loss: 2.4555587768554688
Validation loss: 2.020437796910604

Epoch: 6| Step: 6
Training loss: 1.9118962287902832
Validation loss: 2.016438980897268

Epoch: 6| Step: 7
Training loss: 2.1153502464294434
Validation loss: 2.0142420132954917

Epoch: 6| Step: 8
Training loss: 2.0656590461730957
Validation loss: 2.0298879941304526

Epoch: 6| Step: 9
Training loss: 1.7455756664276123
Validation loss: 2.0247400800387063

Epoch: 6| Step: 10
Training loss: 1.6554208993911743
Validation loss: 2.017745832602183

Epoch: 6| Step: 11
Training loss: 2.410196304321289
Validation loss: 2.0142940878868103

Epoch: 6| Step: 12
Training loss: 1.9271875619888306
Validation loss: 2.0134143034617105

Epoch: 6| Step: 13
Training loss: 2.077587604522705
Validation loss: 2.014610151449839

Epoch: 99| Step: 0
Training loss: 2.265094518661499
Validation loss: 2.0200851360956826

Epoch: 6| Step: 1
Training loss: 2.097322940826416
Validation loss: 2.0185561378796897

Epoch: 6| Step: 2
Training loss: 2.2955026626586914
Validation loss: 2.012433707714081

Epoch: 6| Step: 3
Training loss: 2.079751491546631
Validation loss: 2.0184214115142822

Epoch: 6| Step: 4
Training loss: 1.7746611833572388
Validation loss: 2.023878196875254

Epoch: 6| Step: 5
Training loss: 2.2116971015930176
Validation loss: 2.0222990910212197

Epoch: 6| Step: 6
Training loss: 1.9078295230865479
Validation loss: 2.0259251793225608

Epoch: 6| Step: 7
Training loss: 2.4124011993408203
Validation loss: 2.037532468636831

Epoch: 6| Step: 8
Training loss: 2.006072759628296
Validation loss: 2.029197911421458

Epoch: 6| Step: 9
Training loss: 2.002490997314453
Validation loss: 2.0201579531033835

Epoch: 6| Step: 10
Training loss: 2.9856791496276855
Validation loss: 2.0285903215408325

Epoch: 6| Step: 11
Training loss: 1.6972343921661377
Validation loss: 2.0293780167897544

Epoch: 6| Step: 12
Training loss: 1.8936247825622559
Validation loss: 2.035785714785258

Epoch: 6| Step: 13
Training loss: 2.2112717628479004
Validation loss: 2.036427597204844

Epoch: 100| Step: 0
Training loss: 2.432051658630371
Validation loss: 2.0286326011021933

Epoch: 6| Step: 1
Training loss: 2.0423693656921387
Validation loss: 2.0270498991012573

Epoch: 6| Step: 2
Training loss: 2.323329210281372
Validation loss: 2.0257116556167603

Epoch: 6| Step: 3
Training loss: 2.16408109664917
Validation loss: 2.0305341879526773

Epoch: 6| Step: 4
Training loss: 1.4811549186706543
Validation loss: 2.0281111796696982

Epoch: 6| Step: 5
Training loss: 2.050992488861084
Validation loss: 2.0233964323997498

Epoch: 6| Step: 6
Training loss: 2.423187732696533
Validation loss: 2.0311357577641806

Epoch: 6| Step: 7
Training loss: 2.5239434242248535
Validation loss: 2.024206201235453

Epoch: 6| Step: 8
Training loss: 2.637312173843384
Validation loss: 2.0263893206914267

Epoch: 6| Step: 9
Training loss: 2.753415584564209
Validation loss: 2.0221097668011985

Epoch: 6| Step: 10
Training loss: 1.4267261028289795
Validation loss: 2.0327789187431335

Epoch: 6| Step: 11
Training loss: 1.8007501363754272
Validation loss: 2.02864670753479

Epoch: 6| Step: 12
Training loss: 1.5355836153030396
Validation loss: 2.033477783203125

Epoch: 6| Step: 13
Training loss: 2.0421786308288574
Validation loss: 2.0324188470840454

Epoch: 101| Step: 0
Training loss: 2.1398873329162598
Validation loss: 2.0286189317703247

Epoch: 6| Step: 1
Training loss: 2.323902130126953
Validation loss: 2.03464146455129

Epoch: 6| Step: 2
Training loss: 2.165241241455078
Validation loss: 2.0276684165000916

Epoch: 6| Step: 3
Training loss: 1.8300786018371582
Validation loss: 2.0317128896713257

Epoch: 6| Step: 4
Training loss: 2.1427316665649414
Validation loss: 2.0284294287363687

Epoch: 6| Step: 5
Training loss: 1.7837908267974854
Validation loss: 2.028855045636495

Epoch: 6| Step: 6
Training loss: 2.1285698413848877
Validation loss: 2.028220315774282

Epoch: 6| Step: 7
Training loss: 2.298612356185913
Validation loss: 2.0237984657287598

Epoch: 6| Step: 8
Training loss: 2.104407787322998
Validation loss: 2.01835827032725

Epoch: 6| Step: 9
Training loss: 2.0378916263580322
Validation loss: 2.024666647116343

Epoch: 6| Step: 10
Training loss: 2.342109203338623
Validation loss: 2.0281885464986167

Epoch: 6| Step: 11
Training loss: 1.9523178339004517
Validation loss: 2.018387258052826

Epoch: 6| Step: 12
Training loss: 2.3255786895751953
Validation loss: 2.019911448160807

Epoch: 6| Step: 13
Training loss: 2.0676746368408203
Validation loss: 2.016963998476664

Epoch: 102| Step: 0
Training loss: 2.070127010345459
Validation loss: 2.0087939699490867

Epoch: 6| Step: 1
Training loss: 2.1450982093811035
Validation loss: 2.009467899799347

Epoch: 6| Step: 2
Training loss: 1.2580070495605469
Validation loss: 2.0185868541399636

Epoch: 6| Step: 3
Training loss: 2.3453145027160645
Validation loss: 2.0037019848823547

Epoch: 6| Step: 4
Training loss: 2.51759672164917
Validation loss: 2.00518669684728

Epoch: 6| Step: 5
Training loss: 1.59470534324646
Validation loss: 2.0105998317400613

Epoch: 6| Step: 6
Training loss: 2.0162782669067383
Validation loss: 2.0117199222246804

Epoch: 6| Step: 7
Training loss: 1.9868299961090088
Validation loss: 2.018526832262675

Epoch: 6| Step: 8
Training loss: 2.3479795455932617
Validation loss: 2.019230385621389

Epoch: 6| Step: 9
Training loss: 2.2315075397491455
Validation loss: 2.027738610903422

Epoch: 6| Step: 10
Training loss: 2.479424476623535
Validation loss: 2.031214972337087

Epoch: 6| Step: 11
Training loss: 2.2749276161193848
Validation loss: 2.018676439921061

Epoch: 6| Step: 12
Training loss: 2.02093505859375
Validation loss: 2.0296720067660012

Epoch: 6| Step: 13
Training loss: 2.5061426162719727
Validation loss: 2.0234145720799765

Epoch: 103| Step: 0
Training loss: 1.597489356994629
Validation loss: 2.029919226964315

Epoch: 6| Step: 1
Training loss: 1.972205638885498
Validation loss: 2.031975209712982

Epoch: 6| Step: 2
Training loss: 2.8723669052124023
Validation loss: 2.023990511894226

Epoch: 6| Step: 3
Training loss: 1.7858476638793945
Validation loss: 2.02208818991979

Epoch: 6| Step: 4
Training loss: 1.760296106338501
Validation loss: 2.0235209663709006

Epoch: 6| Step: 5
Training loss: 2.0915656089782715
Validation loss: 2.0377482175827026

Epoch: 6| Step: 6
Training loss: 1.9864497184753418
Validation loss: 2.025142192840576

Epoch: 6| Step: 7
Training loss: 2.289984703063965
Validation loss: 2.0197322567303977

Epoch: 6| Step: 8
Training loss: 3.2073307037353516
Validation loss: 2.0318449338277182

Epoch: 6| Step: 9
Training loss: 1.922304630279541
Validation loss: 2.033964435259501

Epoch: 6| Step: 10
Training loss: 2.0079338550567627
Validation loss: 2.0307061672210693

Epoch: 6| Step: 11
Training loss: 2.00327205657959
Validation loss: 2.0298686226209006

Epoch: 6| Step: 12
Training loss: 2.188798666000366
Validation loss: 2.0208073457082114

Epoch: 6| Step: 13
Training loss: 2.009528398513794
Validation loss: 2.0246822436650596

Epoch: 104| Step: 0
Training loss: 2.726461410522461
Validation loss: 2.014121949672699

Epoch: 6| Step: 1
Training loss: 2.2110180854797363
Validation loss: 2.022154688835144

Epoch: 6| Step: 2
Training loss: 1.4844086170196533
Validation loss: 2.0228288173675537

Epoch: 6| Step: 3
Training loss: 2.273735523223877
Validation loss: 2.0216761231422424

Epoch: 6| Step: 4
Training loss: 1.9125521183013916
Validation loss: 2.0174898902575173

Epoch: 6| Step: 5
Training loss: 1.744516134262085
Validation loss: 2.021621306737264

Epoch: 6| Step: 6
Training loss: 1.9449200630187988
Validation loss: 2.0154786507288613

Epoch: 6| Step: 7
Training loss: 2.202202796936035
Validation loss: 2.0253254175186157

Epoch: 6| Step: 8
Training loss: 2.549774169921875
Validation loss: 2.025915563106537

Epoch: 6| Step: 9
Training loss: 2.199754238128662
Validation loss: 2.025995930035909

Epoch: 6| Step: 10
Training loss: 2.0374932289123535
Validation loss: 2.031941612561544

Epoch: 6| Step: 11
Training loss: 2.1559572219848633
Validation loss: 2.0272465149561563

Epoch: 6| Step: 12
Training loss: 2.4576516151428223
Validation loss: 2.0250325202941895

Epoch: 6| Step: 13
Training loss: 1.9879765510559082
Validation loss: 2.016991376876831

Epoch: 105| Step: 0
Training loss: 2.1741390228271484
Validation loss: 2.0224468310674033

Epoch: 6| Step: 1
Training loss: 2.098637342453003
Validation loss: 2.0282840927441916

Epoch: 6| Step: 2
Training loss: 1.8436598777770996
Validation loss: 2.0297447045644126

Epoch: 6| Step: 3
Training loss: 2.528244972229004
Validation loss: 2.0285550554593406

Epoch: 6| Step: 4
Training loss: 1.7989742755889893
Validation loss: 2.0338032643000283

Epoch: 6| Step: 5
Training loss: 2.336848735809326
Validation loss: 2.0364548563957214

Epoch: 6| Step: 6
Training loss: 1.8271666765213013
Validation loss: 2.027896225452423

Epoch: 6| Step: 7
Training loss: 1.7287697792053223
Validation loss: 2.024153172969818

Epoch: 6| Step: 8
Training loss: 2.0985374450683594
Validation loss: 2.0201901396115622

Epoch: 6| Step: 9
Training loss: 2.560615062713623
Validation loss: 2.021486739317576

Epoch: 6| Step: 10
Training loss: 1.938907504081726
Validation loss: 2.0243628223737082

Epoch: 6| Step: 11
Training loss: 2.3612141609191895
Validation loss: 2.0129124522209167

Epoch: 6| Step: 12
Training loss: 2.090167999267578
Validation loss: 2.01506237188975

Epoch: 6| Step: 13
Training loss: 2.343474864959717
Validation loss: 2.022722323735555

Epoch: 106| Step: 0
Training loss: 2.4385673999786377
Validation loss: 2.0282389322916665

Epoch: 6| Step: 1
Training loss: 2.7577896118164062
Validation loss: 2.012383202711741

Epoch: 6| Step: 2
Training loss: 2.066946029663086
Validation loss: 2.009045441945394

Epoch: 6| Step: 3
Training loss: 1.8410181999206543
Validation loss: 2.0149287780125937

Epoch: 6| Step: 4
Training loss: 2.124192953109741
Validation loss: 2.004632830619812

Epoch: 6| Step: 5
Training loss: 1.7379460334777832
Validation loss: 2.0061973134676614

Epoch: 6| Step: 6
Training loss: 2.3457961082458496
Validation loss: 2.0040471156438193

Epoch: 6| Step: 7
Training loss: 2.094511032104492
Validation loss: 2.016483207543691

Epoch: 6| Step: 8
Training loss: 1.615685224533081
Validation loss: 2.0071070392926535

Epoch: 6| Step: 9
Training loss: 2.0107059478759766
Validation loss: 2.008675495783488

Epoch: 6| Step: 10
Training loss: 2.3935985565185547
Validation loss: 2.017143468062083

Epoch: 6| Step: 11
Training loss: 1.9369093179702759
Validation loss: 2.0078050096829734

Epoch: 6| Step: 12
Training loss: 2.333888530731201
Validation loss: 2.0197054743766785

Epoch: 6| Step: 13
Training loss: 2.10357403755188
Validation loss: 2.0332123239835105

Epoch: 107| Step: 0
Training loss: 2.0590615272521973
Validation loss: 2.0290720065434775

Epoch: 6| Step: 1
Training loss: 2.827371597290039
Validation loss: 2.018722971280416

Epoch: 6| Step: 2
Training loss: 2.487269401550293
Validation loss: 2.0200840830802917

Epoch: 6| Step: 3
Training loss: 1.9169255495071411
Validation loss: 2.026764194170634

Epoch: 6| Step: 4
Training loss: 1.8118761777877808
Validation loss: 2.020407716433207

Epoch: 6| Step: 5
Training loss: 2.057525634765625
Validation loss: 2.030606289704641

Epoch: 6| Step: 6
Training loss: 2.209202289581299
Validation loss: 2.034607688585917

Epoch: 6| Step: 7
Training loss: 1.7654565572738647
Validation loss: 2.027559240659078

Epoch: 6| Step: 8
Training loss: 2.0210769176483154
Validation loss: 2.032474855581919

Epoch: 6| Step: 9
Training loss: 1.7985424995422363
Validation loss: 2.034042179584503

Epoch: 6| Step: 10
Training loss: 2.1030101776123047
Validation loss: 2.0335254867871604

Epoch: 6| Step: 11
Training loss: 2.076725959777832
Validation loss: 2.0416276454925537

Epoch: 6| Step: 12
Training loss: 2.1189961433410645
Validation loss: 2.026488721370697

Epoch: 6| Step: 13
Training loss: 2.2716593742370605
Validation loss: 2.041777749856313

Epoch: 108| Step: 0
Training loss: 2.2774436473846436
Validation loss: 2.0411843061447144

Epoch: 6| Step: 1
Training loss: 1.6342511177062988
Validation loss: 2.0355993509292603

Epoch: 6| Step: 2
Training loss: 2.493152618408203
Validation loss: 2.0292457342147827

Epoch: 6| Step: 3
Training loss: 2.387359380722046
Validation loss: 2.032888909180959

Epoch: 6| Step: 4
Training loss: 2.32627534866333
Validation loss: 2.0374646385510764

Epoch: 6| Step: 5
Training loss: 2.027243137359619
Validation loss: 2.0227648417154946

Epoch: 6| Step: 6
Training loss: 1.2528780698776245
Validation loss: 2.034918745358785

Epoch: 6| Step: 7
Training loss: 2.6987156867980957
Validation loss: 2.028950651486715

Epoch: 6| Step: 8
Training loss: 2.332953453063965
Validation loss: 2.023535370826721

Epoch: 6| Step: 9
Training loss: 2.0323383808135986
Validation loss: 2.0213624040285745

Epoch: 6| Step: 10
Training loss: 2.0449862480163574
Validation loss: 2.024431049823761

Epoch: 6| Step: 11
Training loss: 1.9190051555633545
Validation loss: 2.0215658148129783

Epoch: 6| Step: 12
Training loss: 1.877551794052124
Validation loss: 2.029606580734253

Epoch: 6| Step: 13
Training loss: 2.5591955184936523
Validation loss: 2.027812739213308

Epoch: 109| Step: 0
Training loss: 2.5068116188049316
Validation loss: 2.0178752740224204

Epoch: 6| Step: 1
Training loss: 2.2794220447540283
Validation loss: 2.0363351702690125

Epoch: 6| Step: 2
Training loss: 1.5946073532104492
Validation loss: 2.0204124252001443

Epoch: 6| Step: 3
Training loss: 2.3328161239624023
Validation loss: 2.0245340863863626

Epoch: 6| Step: 4
Training loss: 1.688971996307373
Validation loss: 2.0343436002731323

Epoch: 6| Step: 5
Training loss: 1.5221960544586182
Validation loss: 2.0258415937423706

Epoch: 6| Step: 6
Training loss: 2.351469039916992
Validation loss: 2.028984228769938

Epoch: 6| Step: 7
Training loss: 2.182140827178955
Validation loss: 2.013977368672689

Epoch: 6| Step: 8
Training loss: 2.7569692134857178
Validation loss: 2.0200111071268716

Epoch: 6| Step: 9
Training loss: 1.6975085735321045
Validation loss: 2.0314129988352456

Epoch: 6| Step: 10
Training loss: 2.104228973388672
Validation loss: 2.025625169277191

Epoch: 6| Step: 11
Training loss: 1.9747692346572876
Validation loss: 2.0383259057998657

Epoch: 6| Step: 12
Training loss: 2.373237133026123
Validation loss: 2.031280279159546

Epoch: 6| Step: 13
Training loss: 2.258434772491455
Validation loss: 2.041033069292704

Epoch: 110| Step: 0
Training loss: 2.309983015060425
Validation loss: 2.0400038361549377

Epoch: 6| Step: 1
Training loss: 1.6797574758529663
Validation loss: 2.0327132741610208

Epoch: 6| Step: 2
Training loss: 1.6761474609375
Validation loss: 2.0322176218032837

Epoch: 6| Step: 3
Training loss: 1.9268001317977905
Validation loss: 2.026343842347463

Epoch: 6| Step: 4
Training loss: 2.4366769790649414
Validation loss: 2.0424293677012124

Epoch: 6| Step: 5
Training loss: 1.6422436237335205
Validation loss: 2.0303534468015036

Epoch: 6| Step: 6
Training loss: 1.9572125673294067
Validation loss: 2.0249493519465127

Epoch: 6| Step: 7
Training loss: 2.3270888328552246
Validation loss: 2.0356417298316956

Epoch: 6| Step: 8
Training loss: 2.5981240272521973
Validation loss: 2.034298837184906

Epoch: 6| Step: 9
Training loss: 2.254903793334961
Validation loss: 2.029502034187317

Epoch: 6| Step: 10
Training loss: 2.386664390563965
Validation loss: 2.033709784348806

Epoch: 6| Step: 11
Training loss: 1.8773330450057983
Validation loss: 2.0384622613588967

Epoch: 6| Step: 12
Training loss: 2.3856606483459473
Validation loss: 2.039719263712565

Epoch: 6| Step: 13
Training loss: 1.9186476469039917
Validation loss: 2.041259209314982

Epoch: 111| Step: 0
Training loss: 2.1907548904418945
Validation loss: 2.043555120627085

Epoch: 6| Step: 1
Training loss: 2.3538589477539062
Validation loss: 2.041256626447042

Epoch: 6| Step: 2
Training loss: 2.1428520679473877
Validation loss: 2.045249581336975

Epoch: 6| Step: 3
Training loss: 2.2457103729248047
Validation loss: 2.0355285008748374

Epoch: 6| Step: 4
Training loss: 2.26981520652771
Validation loss: 2.0354878505071006

Epoch: 6| Step: 5
Training loss: 2.199472427368164
Validation loss: 2.0302933057149253

Epoch: 6| Step: 6
Training loss: 2.0129942893981934
Validation loss: 2.03211373090744

Epoch: 6| Step: 7
Training loss: 2.5659432411193848
Validation loss: 2.024996558825175

Epoch: 6| Step: 8
Training loss: 2.3292477130889893
Validation loss: 2.0520270069440207

Epoch: 6| Step: 9
Training loss: 1.331175684928894
Validation loss: 2.0262813170750937

Epoch: 6| Step: 10
Training loss: 1.8570064306259155
Validation loss: 2.0408616860707602

Epoch: 6| Step: 11
Training loss: 1.6598533391952515
Validation loss: 2.040336767832438

Epoch: 6| Step: 12
Training loss: 2.235125780105591
Validation loss: 2.0361738403638205

Epoch: 6| Step: 13
Training loss: 1.9261507987976074
Validation loss: 2.041874090830485

Epoch: 112| Step: 0
Training loss: 2.4560818672180176
Validation loss: 2.0358985662460327

Epoch: 6| Step: 1
Training loss: 2.3311729431152344
Validation loss: 2.0396992365519204

Epoch: 6| Step: 2
Training loss: 1.720917820930481
Validation loss: 2.0357845425605774

Epoch: 6| Step: 3
Training loss: 1.7329334020614624
Validation loss: 2.038780093193054

Epoch: 6| Step: 4
Training loss: 2.0337822437286377
Validation loss: 2.0452173153559365

Epoch: 6| Step: 5
Training loss: 2.0483620166778564
Validation loss: 2.047054330507914

Epoch: 6| Step: 6
Training loss: 1.4263931512832642
Validation loss: 2.0354713797569275

Epoch: 6| Step: 7
Training loss: 2.4721779823303223
Validation loss: 2.042897045612335

Epoch: 6| Step: 8
Training loss: 1.8560395240783691
Validation loss: 2.0429742336273193

Epoch: 6| Step: 9
Training loss: 2.471710205078125
Validation loss: 2.0534605979919434

Epoch: 6| Step: 10
Training loss: 1.850974202156067
Validation loss: 2.047992388407389

Epoch: 6| Step: 11
Training loss: 1.9898171424865723
Validation loss: 2.0440203746159873

Epoch: 6| Step: 12
Training loss: 2.2482805252075195
Validation loss: 2.03793732325236

Epoch: 6| Step: 13
Training loss: 2.717214822769165
Validation loss: 2.044788122177124

Epoch: 113| Step: 0
Training loss: 2.816262722015381
Validation loss: 2.041602353254954

Epoch: 6| Step: 1
Training loss: 2.1494243144989014
Validation loss: 2.0482760866483054

Epoch: 6| Step: 2
Training loss: 1.8619495630264282
Validation loss: 2.044374704360962

Epoch: 6| Step: 3
Training loss: 2.5258166790008545
Validation loss: 2.0388757983843484

Epoch: 6| Step: 4
Training loss: 2.7852611541748047
Validation loss: 2.0400962829589844

Epoch: 6| Step: 5
Training loss: 1.6886218786239624
Validation loss: 2.036984999974569

Epoch: 6| Step: 6
Training loss: 1.367278814315796
Validation loss: 2.026798744996389

Epoch: 6| Step: 7
Training loss: 1.9766662120819092
Validation loss: 2.025321384270986

Epoch: 6| Step: 8
Training loss: 1.9692864418029785
Validation loss: 2.0334558884302774

Epoch: 6| Step: 9
Training loss: 2.2237813472747803
Validation loss: 2.025354047616323

Epoch: 6| Step: 10
Training loss: 1.761236548423767
Validation loss: 2.0256789525349936

Epoch: 6| Step: 11
Training loss: 1.844956874847412
Validation loss: 2.019955118497213

Epoch: 6| Step: 12
Training loss: 2.42447566986084
Validation loss: 2.0352433919906616

Epoch: 6| Step: 13
Training loss: 2.207512378692627
Validation loss: 2.0345766146977744

Epoch: 114| Step: 0
Training loss: 2.337751865386963
Validation loss: 2.0456568598747253

Epoch: 6| Step: 1
Training loss: 2.110536575317383
Validation loss: 2.0398661891619363

Epoch: 6| Step: 2
Training loss: 1.6975572109222412
Validation loss: 2.046037177244822

Epoch: 6| Step: 3
Training loss: 1.9295835494995117
Validation loss: 2.0419371922810874

Epoch: 6| Step: 4
Training loss: 1.8643624782562256
Validation loss: 2.0435346961021423

Epoch: 6| Step: 5
Training loss: 1.8996009826660156
Validation loss: 2.0533313155174255

Epoch: 6| Step: 6
Training loss: 2.2086610794067383
Validation loss: 2.0416988134384155

Epoch: 6| Step: 7
Training loss: 1.7878773212432861
Validation loss: 2.0517476201057434

Epoch: 6| Step: 8
Training loss: 2.155646800994873
Validation loss: 2.0505367318789163

Epoch: 6| Step: 9
Training loss: 2.3856801986694336
Validation loss: 2.044777035713196

Epoch: 6| Step: 10
Training loss: 2.5456433296203613
Validation loss: 2.0384856263796487

Epoch: 6| Step: 11
Training loss: 1.8401473760604858
Validation loss: 2.040755331516266

Epoch: 6| Step: 12
Training loss: 3.258073329925537
Validation loss: 2.0350786646207175

Epoch: 6| Step: 13
Training loss: 1.2575485706329346
Validation loss: 2.0289809505144754

Epoch: 115| Step: 0
Training loss: 1.9038143157958984
Validation loss: 2.0353968342145285

Epoch: 6| Step: 1
Training loss: 2.2382073402404785
Validation loss: 2.0368719498316445

Epoch: 6| Step: 2
Training loss: 1.7028926610946655
Validation loss: 2.0455175638198853

Epoch: 6| Step: 3
Training loss: 2.100511074066162
Validation loss: 2.0512773791948953

Epoch: 6| Step: 4
Training loss: 2.0391650199890137
Validation loss: 2.0504544178644815

Epoch: 6| Step: 5
Training loss: 2.312737464904785
Validation loss: 2.056353668371836

Epoch: 6| Step: 6
Training loss: 1.735769271850586
Validation loss: 2.057643473148346

Epoch: 6| Step: 7
Training loss: 2.434823513031006
Validation loss: 2.0568933288256326

Epoch: 6| Step: 8
Training loss: 2.1672353744506836
Validation loss: 2.054270307223002

Epoch: 6| Step: 9
Training loss: 1.9843366146087646
Validation loss: 2.052939693133036

Epoch: 6| Step: 10
Training loss: 2.24757719039917
Validation loss: 2.049283266067505

Epoch: 6| Step: 11
Training loss: 2.2439966201782227
Validation loss: 2.0456509788831077

Epoch: 6| Step: 12
Training loss: 2.320096492767334
Validation loss: 2.0420387585957847

Epoch: 6| Step: 13
Training loss: 1.8793749809265137
Validation loss: 2.022497753302256

Epoch: 116| Step: 0
Training loss: 2.47522234916687
Validation loss: 2.039493501186371

Epoch: 6| Step: 1
Training loss: 2.2579288482666016
Validation loss: 2.048958500226339

Epoch: 6| Step: 2
Training loss: 1.6304529905319214
Validation loss: 2.0420759320259094

Epoch: 6| Step: 3
Training loss: 2.3988571166992188
Validation loss: 2.0399325688680015

Epoch: 6| Step: 4
Training loss: 2.4037930965423584
Validation loss: 2.037727336088816

Epoch: 6| Step: 5
Training loss: 1.6110835075378418
Validation loss: 2.0394625862439475

Epoch: 6| Step: 6
Training loss: 1.9787638187408447
Validation loss: 2.0510370333989463

Epoch: 6| Step: 7
Training loss: 2.248704433441162
Validation loss: 2.041188061237335

Epoch: 6| Step: 8
Training loss: 1.8178120851516724
Validation loss: 2.05222749710083

Epoch: 6| Step: 9
Training loss: 2.160271644592285
Validation loss: 2.0620006322860718

Epoch: 6| Step: 10
Training loss: 2.421403408050537
Validation loss: 2.0509636600812278

Epoch: 6| Step: 11
Training loss: 2.1145167350769043
Validation loss: 2.05185999472936

Epoch: 6| Step: 12
Training loss: 1.7483782768249512
Validation loss: 2.040100395679474

Epoch: 6| Step: 13
Training loss: 1.859277367591858
Validation loss: 2.0354813734690347

Epoch: 117| Step: 0
Training loss: 2.4855685234069824
Validation loss: 2.045679271221161

Epoch: 6| Step: 1
Training loss: 1.8854923248291016
Validation loss: 2.028148273626963

Epoch: 6| Step: 2
Training loss: 2.422755718231201
Validation loss: 2.0380986531575522

Epoch: 6| Step: 3
Training loss: 2.356191396713257
Validation loss: 2.039059281349182

Epoch: 6| Step: 4
Training loss: 2.2923948764801025
Validation loss: 2.031647205352783

Epoch: 6| Step: 5
Training loss: 2.099252223968506
Validation loss: 2.0437225699424744

Epoch: 6| Step: 6
Training loss: 1.8650329113006592
Validation loss: 2.040117541948954

Epoch: 6| Step: 7
Training loss: 2.1734652519226074
Validation loss: 2.041388293107351

Epoch: 6| Step: 8
Training loss: 2.067767858505249
Validation loss: 2.0566161076227822

Epoch: 6| Step: 9
Training loss: 1.531801462173462
Validation loss: 2.052243649959564

Epoch: 6| Step: 10
Training loss: 1.8934484720230103
Validation loss: 2.052250027656555

Epoch: 6| Step: 11
Training loss: 1.7164255380630493
Validation loss: 2.0568121671676636

Epoch: 6| Step: 12
Training loss: 1.9018511772155762
Validation loss: 2.057263135910034

Epoch: 6| Step: 13
Training loss: 2.4235525131225586
Validation loss: 2.0474459330240884

Epoch: 118| Step: 0
Training loss: 1.8382349014282227
Validation loss: 2.0446972052256265

Epoch: 6| Step: 1
Training loss: 1.7958507537841797
Validation loss: 2.0352550546328225

Epoch: 6| Step: 2
Training loss: 2.395418643951416
Validation loss: 2.038487414518992

Epoch: 6| Step: 3
Training loss: 2.0132739543914795
Validation loss: 2.0392002065976462

Epoch: 6| Step: 4
Training loss: 2.137678623199463
Validation loss: 2.054587463537852

Epoch: 6| Step: 5
Training loss: 2.4286928176879883
Validation loss: 2.059794843196869

Epoch: 6| Step: 6
Training loss: 2.2505135536193848
Validation loss: 2.061116655667623

Epoch: 6| Step: 7
Training loss: 2.904662609100342
Validation loss: 2.0666555563608804

Epoch: 6| Step: 8
Training loss: 2.047247886657715
Validation loss: 2.0592846671740213

Epoch: 6| Step: 9
Training loss: 1.6596380472183228
Validation loss: 2.0604503750801086

Epoch: 6| Step: 10
Training loss: 2.014940023422241
Validation loss: 2.0567484299341836

Epoch: 6| Step: 11
Training loss: 2.2525763511657715
Validation loss: 2.0511505802472434

Epoch: 6| Step: 12
Training loss: 2.264636516571045
Validation loss: 2.0460867484410605

Epoch: 6| Step: 13
Training loss: 1.5669264793395996
Validation loss: 2.0485909581184387

Epoch: 119| Step: 0
Training loss: 2.1662778854370117
Validation loss: 2.04598335425059

Epoch: 6| Step: 1
Training loss: 1.6165715456008911
Validation loss: 2.0394822557767234

Epoch: 6| Step: 2
Training loss: 2.325106620788574
Validation loss: 2.043125549952189

Epoch: 6| Step: 3
Training loss: 2.42574405670166
Validation loss: 2.037019670009613

Epoch: 6| Step: 4
Training loss: 1.9097731113433838
Validation loss: 2.039719263712565

Epoch: 6| Step: 5
Training loss: 1.803959608078003
Validation loss: 2.0444560249646506

Epoch: 6| Step: 6
Training loss: 1.873640537261963
Validation loss: 2.0526026089986167

Epoch: 6| Step: 7
Training loss: 2.112946033477783
Validation loss: 2.0557236671447754

Epoch: 6| Step: 8
Training loss: 1.9136297702789307
Validation loss: 2.0576820770899453

Epoch: 6| Step: 9
Training loss: 2.0869133472442627
Validation loss: 2.0453463594118753

Epoch: 6| Step: 10
Training loss: 2.451343536376953
Validation loss: 2.054983993371328

Epoch: 6| Step: 11
Training loss: 1.985407829284668
Validation loss: 2.0456717014312744

Epoch: 6| Step: 12
Training loss: 2.5452401638031006
Validation loss: 2.0528579552968345

Epoch: 6| Step: 13
Training loss: 1.9755076169967651
Validation loss: 2.0546809434890747

Epoch: 120| Step: 0
Training loss: 2.1917481422424316
Validation loss: 2.047488570213318

Epoch: 6| Step: 1
Training loss: 2.4393868446350098
Validation loss: 2.0552420020103455

Epoch: 6| Step: 2
Training loss: 1.4797492027282715
Validation loss: 2.05580727259318

Epoch: 6| Step: 3
Training loss: 1.8409584760665894
Validation loss: 2.0626215736071267

Epoch: 6| Step: 4
Training loss: 2.0935916900634766
Validation loss: 2.050604462623596

Epoch: 6| Step: 5
Training loss: 2.438124656677246
Validation loss: 2.048338611920675

Epoch: 6| Step: 6
Training loss: 1.869565486907959
Validation loss: 2.0526826977729797

Epoch: 6| Step: 7
Training loss: 1.785031795501709
Validation loss: 2.049822688102722

Epoch: 6| Step: 8
Training loss: 2.3044633865356445
Validation loss: 2.051077047983805

Epoch: 6| Step: 9
Training loss: 2.050978183746338
Validation loss: 2.05266797542572

Epoch: 6| Step: 10
Training loss: 2.590216875076294
Validation loss: 2.0497311552365622

Epoch: 6| Step: 11
Training loss: 1.881528615951538
Validation loss: 2.0439048210779824

Epoch: 6| Step: 12
Training loss: 2.5562427043914795
Validation loss: 2.034202734629313

Epoch: 6| Step: 13
Training loss: 1.595503807067871
Validation loss: 2.035198966662089

Epoch: 121| Step: 0
Training loss: 1.6783603429794312
Validation loss: 2.028597911198934

Epoch: 6| Step: 1
Training loss: 2.006152629852295
Validation loss: 2.0492191712061563

Epoch: 6| Step: 2
Training loss: 2.113877773284912
Validation loss: 2.041700601577759

Epoch: 6| Step: 3
Training loss: 3.283581256866455
Validation loss: 2.050453782081604

Epoch: 6| Step: 4
Training loss: 2.0063717365264893
Validation loss: 2.0464815497398376

Epoch: 6| Step: 5
Training loss: 1.8873156309127808
Validation loss: 2.0598716338475547

Epoch: 6| Step: 6
Training loss: 2.0639138221740723
Validation loss: 2.055393556753794

Epoch: 6| Step: 7
Training loss: 2.2893712520599365
Validation loss: 2.0471519430478415

Epoch: 6| Step: 8
Training loss: 2.1355979442596436
Validation loss: 2.058732827504476

Epoch: 6| Step: 9
Training loss: 1.6805418729782104
Validation loss: 2.049158732096354

Epoch: 6| Step: 10
Training loss: 1.8891961574554443
Validation loss: 2.0385523239771524

Epoch: 6| Step: 11
Training loss: 1.7634440660476685
Validation loss: 2.0350523392359414

Epoch: 6| Step: 12
Training loss: 2.2814629077911377
Validation loss: 2.053015093008677

Epoch: 6| Step: 13
Training loss: 1.9073641300201416
Validation loss: 2.046372890472412

Epoch: 122| Step: 0
Training loss: 1.7028847932815552
Validation loss: 2.0254732171694436

Epoch: 6| Step: 1
Training loss: 1.9795489311218262
Validation loss: 2.0311619639396667

Epoch: 6| Step: 2
Training loss: 2.061800241470337
Validation loss: 2.035408874352773

Epoch: 6| Step: 3
Training loss: 2.014382839202881
Validation loss: 2.0439637104670205

Epoch: 6| Step: 4
Training loss: 1.6839065551757812
Validation loss: 2.0488783915837607

Epoch: 6| Step: 5
Training loss: 2.050316572189331
Validation loss: 2.046201686064402

Epoch: 6| Step: 6
Training loss: 2.194530963897705
Validation loss: 2.0423498352368674

Epoch: 6| Step: 7
Training loss: 1.8897161483764648
Validation loss: 2.0506953398386636

Epoch: 6| Step: 8
Training loss: 2.4256772994995117
Validation loss: 2.0343985557556152

Epoch: 6| Step: 9
Training loss: 1.8735127449035645
Validation loss: 2.0416380365689597

Epoch: 6| Step: 10
Training loss: 2.0192198753356934
Validation loss: 2.0318604509035745

Epoch: 6| Step: 11
Training loss: 2.3887295722961426
Validation loss: 2.043026069800059

Epoch: 6| Step: 12
Training loss: 2.2234461307525635
Validation loss: 2.0449567238489785

Epoch: 6| Step: 13
Training loss: 2.4496896266937256
Validation loss: 2.0491841236750283

Epoch: 123| Step: 0
Training loss: 2.156581163406372
Validation loss: 2.055883844693502

Epoch: 6| Step: 1
Training loss: 2.3842639923095703
Validation loss: 2.05203777551651

Epoch: 6| Step: 2
Training loss: 2.2003536224365234
Validation loss: 2.064486344655355

Epoch: 6| Step: 3
Training loss: 2.4113197326660156
Validation loss: 2.0487612883249917

Epoch: 6| Step: 4
Training loss: 1.9679149389266968
Validation loss: 2.0490347345670066

Epoch: 6| Step: 5
Training loss: 1.460410237312317
Validation loss: 2.0699202617009482

Epoch: 6| Step: 6
Training loss: 2.137300968170166
Validation loss: 2.072080353895823

Epoch: 6| Step: 7
Training loss: 1.8528037071228027
Validation loss: 2.077236811319987

Epoch: 6| Step: 8
Training loss: 1.8350944519042969
Validation loss: 2.0542481342951455

Epoch: 6| Step: 9
Training loss: 2.982787609100342
Validation loss: 2.054925243059794

Epoch: 6| Step: 10
Training loss: 2.486637592315674
Validation loss: 2.0665797789891562

Epoch: 6| Step: 11
Training loss: 1.4371285438537598
Validation loss: 2.055898447831472

Epoch: 6| Step: 12
Training loss: 1.7714639902114868
Validation loss: 2.0530823270479837

Epoch: 6| Step: 13
Training loss: 2.163004159927368
Validation loss: 2.053479015827179

Epoch: 124| Step: 0
Training loss: 2.6781516075134277
Validation loss: 2.0299838980038962

Epoch: 6| Step: 1
Training loss: 1.3551263809204102
Validation loss: 2.0308656295140586

Epoch: 6| Step: 2
Training loss: 2.687607526779175
Validation loss: 2.0350534319877625

Epoch: 6| Step: 3
Training loss: 1.4295356273651123
Validation loss: 2.0234652757644653

Epoch: 6| Step: 4
Training loss: 1.992478609085083
Validation loss: 2.0422666668891907

Epoch: 6| Step: 5
Training loss: 2.1483843326568604
Validation loss: 2.0403084556261697

Epoch: 6| Step: 6
Training loss: 2.454766035079956
Validation loss: 2.0485086838404336

Epoch: 6| Step: 7
Training loss: 2.449054718017578
Validation loss: 2.0516829093297324

Epoch: 6| Step: 8
Training loss: 1.8196523189544678
Validation loss: 2.0537928342819214

Epoch: 6| Step: 9
Training loss: 2.379559278488159
Validation loss: 2.050076166788737

Epoch: 6| Step: 10
Training loss: 1.88308584690094
Validation loss: 2.0567840337753296

Epoch: 6| Step: 11
Training loss: 1.9887504577636719
Validation loss: 2.0555497209231057

Epoch: 6| Step: 12
Training loss: 1.736992359161377
Validation loss: 2.055143396059672

Epoch: 6| Step: 13
Training loss: 2.082278251647949
Validation loss: 2.066536327203115

Epoch: 125| Step: 0
Training loss: 2.304945945739746
Validation loss: 2.0649811824162803

Epoch: 6| Step: 1
Training loss: 2.5791330337524414
Validation loss: 2.059484342734019

Epoch: 6| Step: 2
Training loss: 2.2990732192993164
Validation loss: 2.052863299846649

Epoch: 6| Step: 3
Training loss: 2.520552396774292
Validation loss: 2.0481977462768555

Epoch: 6| Step: 4
Training loss: 1.656456708908081
Validation loss: 2.0379431446393332

Epoch: 6| Step: 5
Training loss: 2.444988250732422
Validation loss: 2.0407702326774597

Epoch: 6| Step: 6
Training loss: 1.6196471452713013
Validation loss: 2.036970337231954

Epoch: 6| Step: 7
Training loss: 2.170520782470703
Validation loss: 2.0458645820617676

Epoch: 6| Step: 8
Training loss: 2.261652946472168
Validation loss: 2.056138793627421

Epoch: 6| Step: 9
Training loss: 1.7589826583862305
Validation loss: 2.0538121263186135

Epoch: 6| Step: 10
Training loss: 1.6273880004882812
Validation loss: 2.0446078379948935

Epoch: 6| Step: 11
Training loss: 2.1755504608154297
Validation loss: 2.047244151433309

Epoch: 6| Step: 12
Training loss: 2.0689759254455566
Validation loss: 2.06440407037735

Epoch: 6| Step: 13
Training loss: 1.5198934078216553
Validation loss: 2.0562986532847085

Epoch: 126| Step: 0
Training loss: 2.259030818939209
Validation loss: 2.0638720989227295

Epoch: 6| Step: 1
Training loss: 2.115966796875
Validation loss: 2.059739112854004

Epoch: 6| Step: 2
Training loss: 2.114741563796997
Validation loss: 2.0721285343170166

Epoch: 6| Step: 3
Training loss: 2.8570876121520996
Validation loss: 2.0630152821540833

Epoch: 6| Step: 4
Training loss: 1.4063819646835327
Validation loss: 2.065137724081675

Epoch: 6| Step: 5
Training loss: 1.984256386756897
Validation loss: 2.0784379839897156

Epoch: 6| Step: 6
Training loss: 2.2167508602142334
Validation loss: 2.0869659582773843

Epoch: 6| Step: 7
Training loss: 2.208178997039795
Validation loss: 2.0655484000841775

Epoch: 6| Step: 8
Training loss: 2.2177414894104004
Validation loss: 2.069974660873413

Epoch: 6| Step: 9
Training loss: 1.90970778465271
Validation loss: 2.0653929511706033

Epoch: 6| Step: 10
Training loss: 1.5891739130020142
Validation loss: 2.0704685052235923

Epoch: 6| Step: 11
Training loss: 2.078071355819702
Validation loss: 2.066701034704844

Epoch: 6| Step: 12
Training loss: 2.153775930404663
Validation loss: 2.0528111855189004

Epoch: 6| Step: 13
Training loss: 1.7071468830108643
Validation loss: 2.0593141516049704

Epoch: 127| Step: 0
Training loss: 2.0076541900634766
Validation loss: 2.0467680295308432

Epoch: 6| Step: 1
Training loss: 1.8144018650054932
Validation loss: 2.0473192731539407

Epoch: 6| Step: 2
Training loss: 2.3060073852539062
Validation loss: 2.0575676361719766

Epoch: 6| Step: 3
Training loss: 2.0263383388519287
Validation loss: 2.0408891638120017

Epoch: 6| Step: 4
Training loss: 2.091963052749634
Validation loss: 2.0422080357869468

Epoch: 6| Step: 5
Training loss: 2.1253042221069336
Validation loss: 2.0526965657869973

Epoch: 6| Step: 6
Training loss: 2.4105658531188965
Validation loss: 2.0491005182266235

Epoch: 6| Step: 7
Training loss: 2.2153429985046387
Validation loss: 2.0459858179092407

Epoch: 6| Step: 8
Training loss: 1.877861499786377
Validation loss: 2.0278007984161377

Epoch: 6| Step: 9
Training loss: 2.0456786155700684
Validation loss: 2.045339524745941

Epoch: 6| Step: 10
Training loss: 1.877664566040039
Validation loss: 2.057896057764689

Epoch: 6| Step: 11
Training loss: 1.8784079551696777
Validation loss: 2.0524869163831077

Epoch: 6| Step: 12
Training loss: 2.140627861022949
Validation loss: 2.059925854206085

Epoch: 6| Step: 13
Training loss: 2.150930881500244
Validation loss: 2.06621907154719

Epoch: 128| Step: 0
Training loss: 2.1369760036468506
Validation loss: 2.0609543522198996

Epoch: 6| Step: 1
Training loss: 1.6823636293411255
Validation loss: 2.0547879735628762

Epoch: 6| Step: 2
Training loss: 2.2513318061828613
Validation loss: 2.0636427203814187

Epoch: 6| Step: 3
Training loss: 1.8089888095855713
Validation loss: 2.071008046468099

Epoch: 6| Step: 4
Training loss: 1.3687520027160645
Validation loss: 2.0692708492279053

Epoch: 6| Step: 5
Training loss: 1.6805145740509033
Validation loss: 2.074406325817108

Epoch: 6| Step: 6
Training loss: 2.2180519104003906
Validation loss: 2.0653701225916543

Epoch: 6| Step: 7
Training loss: 1.5109491348266602
Validation loss: 2.047272205352783

Epoch: 6| Step: 8
Training loss: 2.4983534812927246
Validation loss: 2.0589696764945984

Epoch: 6| Step: 9
Training loss: 2.424999237060547
Validation loss: 2.06022971868515

Epoch: 6| Step: 10
Training loss: 2.5415453910827637
Validation loss: 2.061209976673126

Epoch: 6| Step: 11
Training loss: 2.3785557746887207
Validation loss: 2.067720611890157

Epoch: 6| Step: 12
Training loss: 1.8638548851013184
Validation loss: 2.0638441840807595

Epoch: 6| Step: 13
Training loss: 2.228902816772461
Validation loss: 2.065844694773356

Epoch: 129| Step: 0
Training loss: 2.6520001888275146
Validation loss: 2.0706188678741455

Epoch: 6| Step: 1
Training loss: 2.0277457237243652
Validation loss: 2.0672807693481445

Epoch: 6| Step: 2
Training loss: 2.006049156188965
Validation loss: 2.07771235704422

Epoch: 6| Step: 3
Training loss: 2.33817982673645
Validation loss: 2.0728666186332703

Epoch: 6| Step: 4
Training loss: 1.6819407939910889
Validation loss: 2.0807188153266907

Epoch: 6| Step: 5
Training loss: 2.1686840057373047
Validation loss: 2.083514670530955

Epoch: 6| Step: 6
Training loss: 1.4194905757904053
Validation loss: 2.057751417160034

Epoch: 6| Step: 7
Training loss: 1.7873916625976562
Validation loss: 2.0643701950709024

Epoch: 6| Step: 8
Training loss: 1.8050538301467896
Validation loss: 2.065150876839956

Epoch: 6| Step: 9
Training loss: 2.288254976272583
Validation loss: 2.0668692191441855

Epoch: 6| Step: 10
Training loss: 2.0036516189575195
Validation loss: 2.0730881492296853

Epoch: 6| Step: 11
Training loss: 2.2274205684661865
Validation loss: 2.07761816183726

Epoch: 6| Step: 12
Training loss: 2.39620304107666
Validation loss: 2.0725579857826233

Epoch: 6| Step: 13
Training loss: 1.8466994762420654
Validation loss: 2.0791616638501487

Epoch: 130| Step: 0
Training loss: 2.9774632453918457
Validation loss: 2.083548128604889

Epoch: 6| Step: 1
Training loss: 1.6991477012634277
Validation loss: 2.0586196978886924

Epoch: 6| Step: 2
Training loss: 1.756717562675476
Validation loss: 2.060567637284597

Epoch: 6| Step: 3
Training loss: 1.9073271751403809
Validation loss: 2.0623231331507363

Epoch: 6| Step: 4
Training loss: 2.0396711826324463
Validation loss: 2.065690199534098

Epoch: 6| Step: 5
Training loss: 1.8818413019180298
Validation loss: 2.0683602690696716

Epoch: 6| Step: 6
Training loss: 1.7574385404586792
Validation loss: 2.057458221912384

Epoch: 6| Step: 7
Training loss: 1.6787545680999756
Validation loss: 2.058773636817932

Epoch: 6| Step: 8
Training loss: 2.624183416366577
Validation loss: 2.0740846395492554

Epoch: 6| Step: 9
Training loss: 2.3182809352874756
Validation loss: 2.0655378897984824

Epoch: 6| Step: 10
Training loss: 2.181750535964966
Validation loss: 2.073177774747213

Epoch: 6| Step: 11
Training loss: 2.43391489982605
Validation loss: 2.0901591777801514

Epoch: 6| Step: 12
Training loss: 1.511474370956421
Validation loss: 2.076272209485372

Epoch: 6| Step: 13
Training loss: 1.9021525382995605
Validation loss: 2.0742405454317727

Epoch: 131| Step: 0
Training loss: 2.0995869636535645
Validation loss: 2.0906704664230347

Epoch: 6| Step: 1
Training loss: 2.025578260421753
Validation loss: 2.083236654599508

Epoch: 6| Step: 2
Training loss: 1.9461491107940674
Validation loss: 2.072326918443044

Epoch: 6| Step: 3
Training loss: 1.8106356859207153
Validation loss: 2.0698341727256775

Epoch: 6| Step: 4
Training loss: 2.514451026916504
Validation loss: 2.0724226037661233

Epoch: 6| Step: 5
Training loss: 1.4449868202209473
Validation loss: 2.0615788300832114

Epoch: 6| Step: 6
Training loss: 1.418436050415039
Validation loss: 2.0610481103261313

Epoch: 6| Step: 7
Training loss: 2.446707248687744
Validation loss: 2.068821986516317

Epoch: 6| Step: 8
Training loss: 2.174051284790039
Validation loss: 2.07982869942983

Epoch: 6| Step: 9
Training loss: 2.539207696914673
Validation loss: 2.0786738793055215

Epoch: 6| Step: 10
Training loss: 2.3160624504089355
Validation loss: 2.071014702320099

Epoch: 6| Step: 11
Training loss: 2.2804293632507324
Validation loss: 2.086838205655416

Epoch: 6| Step: 12
Training loss: 1.449660301208496
Validation loss: 2.0728710691134133

Epoch: 6| Step: 13
Training loss: 2.207993507385254
Validation loss: 2.053372939427694

Epoch: 132| Step: 0
Training loss: 2.066465377807617
Validation loss: 2.066254993279775

Epoch: 6| Step: 1
Training loss: 2.217466354370117
Validation loss: 2.0678195556004844

Epoch: 6| Step: 2
Training loss: 1.7852448225021362
Validation loss: 2.0607385436693826

Epoch: 6| Step: 3
Training loss: 1.8205857276916504
Validation loss: 2.0612366795539856

Epoch: 6| Step: 4
Training loss: 2.2458105087280273
Validation loss: 2.062623222668966

Epoch: 6| Step: 5
Training loss: 1.9950730800628662
Validation loss: 2.062195976575216

Epoch: 6| Step: 6
Training loss: 2.3795530796051025
Validation loss: 2.0641166965166726

Epoch: 6| Step: 7
Training loss: 1.7139333486557007
Validation loss: 2.063033322493235

Epoch: 6| Step: 8
Training loss: 2.3552491664886475
Validation loss: 2.0702436168988547

Epoch: 6| Step: 9
Training loss: 2.2137670516967773
Validation loss: 2.0627113382021585

Epoch: 6| Step: 10
Training loss: 2.0799977779388428
Validation loss: 2.0537668069203696

Epoch: 6| Step: 11
Training loss: 2.069992780685425
Validation loss: 2.0621970097223916

Epoch: 6| Step: 12
Training loss: 1.9311842918395996
Validation loss: 2.0666648745536804

Epoch: 6| Step: 13
Training loss: 1.954332709312439
Validation loss: 2.0690766970316568

Epoch: 133| Step: 0
Training loss: 2.590153455734253
Validation loss: 2.070337951183319

Epoch: 6| Step: 1
Training loss: 1.8607354164123535
Validation loss: 2.065310756365458

Epoch: 6| Step: 2
Training loss: 2.0085387229919434
Validation loss: 2.076332906881968

Epoch: 6| Step: 3
Training loss: 1.9398876428604126
Validation loss: 2.073550800482432

Epoch: 6| Step: 4
Training loss: 2.2433278560638428
Validation loss: 2.0610763231913247

Epoch: 6| Step: 5
Training loss: 2.520145893096924
Validation loss: 2.0465527772903442

Epoch: 6| Step: 6
Training loss: 1.3804795742034912
Validation loss: 2.038536707560221

Epoch: 6| Step: 7
Training loss: 2.48722505569458
Validation loss: 2.0432435274124146

Epoch: 6| Step: 8
Training loss: 1.8712377548217773
Validation loss: 2.0253445704778037

Epoch: 6| Step: 9
Training loss: 1.8448290824890137
Validation loss: 2.039167642593384

Epoch: 6| Step: 10
Training loss: 2.038712501525879
Validation loss: 2.032713294029236

Epoch: 6| Step: 11
Training loss: 1.8879395723342896
Validation loss: 2.040829916795095

Epoch: 6| Step: 12
Training loss: 1.852129578590393
Validation loss: 2.05205370982488

Epoch: 6| Step: 13
Training loss: 2.9053421020507812
Validation loss: 2.042501171429952

Epoch: 134| Step: 0
Training loss: 2.267544746398926
Validation loss: 2.043642977873484

Epoch: 6| Step: 1
Training loss: 1.5578820705413818
Validation loss: 2.041680018107096

Epoch: 6| Step: 2
Training loss: 2.7785277366638184
Validation loss: 2.0409222841262817

Epoch: 6| Step: 3
Training loss: 2.386434555053711
Validation loss: 2.0264223217964172

Epoch: 6| Step: 4
Training loss: 1.395098090171814
Validation loss: 2.030114452044169

Epoch: 6| Step: 5
Training loss: 1.6290419101715088
Validation loss: 2.0409606099128723

Epoch: 6| Step: 6
Training loss: 2.208430290222168
Validation loss: 2.054434279600779

Epoch: 6| Step: 7
Training loss: 2.0724174976348877
Validation loss: 2.0507866938908896

Epoch: 6| Step: 8
Training loss: 2.4152612686157227
Validation loss: 2.07342263062795

Epoch: 6| Step: 9
Training loss: 1.5063014030456543
Validation loss: 2.074874440828959

Epoch: 6| Step: 10
Training loss: 1.9602530002593994
Validation loss: 2.0634964307149253

Epoch: 6| Step: 11
Training loss: 2.0548131465911865
Validation loss: 2.069074273109436

Epoch: 6| Step: 12
Training loss: 3.0077853202819824
Validation loss: 2.0716580947240195

Epoch: 6| Step: 13
Training loss: 1.8787188529968262
Validation loss: 2.0817039807637534

Epoch: 135| Step: 0
Training loss: 2.2392385005950928
Validation loss: 2.0863795280456543

Epoch: 6| Step: 1
Training loss: 1.8408243656158447
Validation loss: 2.0593262116114297

Epoch: 6| Step: 2
Training loss: 2.1780166625976562
Validation loss: 2.0529433488845825

Epoch: 6| Step: 3
Training loss: 2.246797561645508
Validation loss: 2.0519176721572876

Epoch: 6| Step: 4
Training loss: 1.8850739002227783
Validation loss: 2.0499725937843323

Epoch: 6| Step: 5
Training loss: 2.164634943008423
Validation loss: 2.0419365962346396

Epoch: 6| Step: 6
Training loss: 2.020956039428711
Validation loss: 2.0597852071126304

Epoch: 6| Step: 7
Training loss: 1.5022642612457275
Validation loss: 2.050621271133423

Epoch: 6| Step: 8
Training loss: 2.6086673736572266
Validation loss: 2.0511357188224792

Epoch: 6| Step: 9
Training loss: 2.5377731323242188
Validation loss: 2.0339121421178183

Epoch: 6| Step: 10
Training loss: 2.2669005393981934
Validation loss: 2.0552175641059875

Epoch: 6| Step: 11
Training loss: 1.8191176652908325
Validation loss: 2.039007027943929

Epoch: 6| Step: 12
Training loss: 2.000516891479492
Validation loss: 2.024255375067393

Epoch: 6| Step: 13
Training loss: 2.0096421241760254
Validation loss: 2.0393974979718528

Epoch: 136| Step: 0
Training loss: 2.1063075065612793
Validation loss: 2.0320072770118713

Epoch: 6| Step: 1
Training loss: 2.1227359771728516
Validation loss: 2.0383779207865396

Epoch: 6| Step: 2
Training loss: 2.7176547050476074
Validation loss: 2.0282591183980307

Epoch: 6| Step: 3
Training loss: 1.7055846452713013
Validation loss: 2.0439367095629373

Epoch: 6| Step: 4
Training loss: 1.7440675497055054
Validation loss: 2.039174516995748

Epoch: 6| Step: 5
Training loss: 2.331279754638672
Validation loss: 2.0401541590690613

Epoch: 6| Step: 6
Training loss: 2.38492751121521
Validation loss: 2.0515018105506897

Epoch: 6| Step: 7
Training loss: 2.4968714714050293
Validation loss: 2.043533186117808

Epoch: 6| Step: 8
Training loss: 1.891981601715088
Validation loss: 2.059811611970266

Epoch: 6| Step: 9
Training loss: 2.0926907062530518
Validation loss: 2.051499923070272

Epoch: 6| Step: 10
Training loss: 2.003242254257202
Validation loss: 2.045000672340393

Epoch: 6| Step: 11
Training loss: 1.3901052474975586
Validation loss: 2.062359929084778

Epoch: 6| Step: 12
Training loss: 1.7704269886016846
Validation loss: 2.06160976489385

Epoch: 6| Step: 13
Training loss: 2.0943140983581543
Validation loss: 2.0628334283828735

Epoch: 137| Step: 0
Training loss: 2.697253465652466
Validation loss: 2.0573734442392984

Epoch: 6| Step: 1
Training loss: 2.424166202545166
Validation loss: 2.0575250585873923

Epoch: 6| Step: 2
Training loss: 2.1009390354156494
Validation loss: 2.0531734228134155

Epoch: 6| Step: 3
Training loss: 1.4306814670562744
Validation loss: 2.045227567354838

Epoch: 6| Step: 4
Training loss: 2.50862717628479
Validation loss: 2.058761179447174

Epoch: 6| Step: 5
Training loss: 1.8595014810562134
Validation loss: 2.045862158139547

Epoch: 6| Step: 6
Training loss: 1.442195177078247
Validation loss: 2.0586455265680947

Epoch: 6| Step: 7
Training loss: 1.556411623954773
Validation loss: 2.0497615536053977

Epoch: 6| Step: 8
Training loss: 1.8543922901153564
Validation loss: 2.0509817202885947

Epoch: 6| Step: 9
Training loss: 2.042771339416504
Validation loss: 2.0624226331710815

Epoch: 6| Step: 10
Training loss: 2.4032251834869385
Validation loss: 2.0620859066645303

Epoch: 6| Step: 11
Training loss: 1.8769915103912354
Validation loss: 2.0652676622072854

Epoch: 6| Step: 12
Training loss: 2.4653429985046387
Validation loss: 2.0656904379526773

Epoch: 6| Step: 13
Training loss: 2.154390811920166
Validation loss: 2.063730994860331

Epoch: 138| Step: 0
Training loss: 1.5032885074615479
Validation loss: 2.058213770389557

Epoch: 6| Step: 1
Training loss: 2.253500461578369
Validation loss: 2.066734949747721

Epoch: 6| Step: 2
Training loss: 2.141059160232544
Validation loss: 2.0644758542378745

Epoch: 6| Step: 3
Training loss: 1.7291752099990845
Validation loss: 2.0795512000719705

Epoch: 6| Step: 4
Training loss: 1.5002422332763672
Validation loss: 2.0686941544214883

Epoch: 6| Step: 5
Training loss: 2.07853364944458
Validation loss: 2.0855721831321716

Epoch: 6| Step: 6
Training loss: 1.68326997756958
Validation loss: 2.085533161958059

Epoch: 6| Step: 7
Training loss: 2.0194897651672363
Validation loss: 2.0911314686139426

Epoch: 6| Step: 8
Training loss: 2.6159110069274902
Validation loss: 2.0962337851524353

Epoch: 6| Step: 9
Training loss: 2.2005774974823
Validation loss: 2.100515365600586

Epoch: 6| Step: 10
Training loss: 1.6847553253173828
Validation loss: 2.084455450375875

Epoch: 6| Step: 11
Training loss: 2.835481643676758
Validation loss: 2.0908708771069846

Epoch: 6| Step: 12
Training loss: 1.8071794509887695
Validation loss: 2.076838493347168

Epoch: 6| Step: 13
Training loss: 2.7023253440856934
Validation loss: 2.086721440156301

Epoch: 139| Step: 0
Training loss: 1.7119454145431519
Validation loss: 2.0822471380233765

Epoch: 6| Step: 1
Training loss: 2.4508676528930664
Validation loss: 2.0802506804466248

Epoch: 6| Step: 2
Training loss: 2.5761666297912598
Validation loss: 2.084492007891337

Epoch: 6| Step: 3
Training loss: 2.0367531776428223
Validation loss: 2.074383278687795

Epoch: 6| Step: 4
Training loss: 1.779888391494751
Validation loss: 2.076518952846527

Epoch: 6| Step: 5
Training loss: 2.5776145458221436
Validation loss: 2.081584413846334

Epoch: 6| Step: 6
Training loss: 1.4140477180480957
Validation loss: 2.071007331212362

Epoch: 6| Step: 7
Training loss: 1.4875271320343018
Validation loss: 2.0706240137418113

Epoch: 6| Step: 8
Training loss: 2.2985525131225586
Validation loss: 2.0740595857302346

Epoch: 6| Step: 9
Training loss: 1.9976314306259155
Validation loss: 2.0837749441464744

Epoch: 6| Step: 10
Training loss: 2.2838706970214844
Validation loss: 2.080875277519226

Epoch: 6| Step: 11
Training loss: 2.4291162490844727
Validation loss: 2.0791843136151633

Epoch: 6| Step: 12
Training loss: 1.5689514875411987
Validation loss: 2.0646302501360574

Epoch: 6| Step: 13
Training loss: 2.0649919509887695
Validation loss: 2.0851051012674966

Epoch: 140| Step: 0
Training loss: 2.2681593894958496
Validation loss: 2.085151970386505

Epoch: 6| Step: 1
Training loss: 2.3299622535705566
Validation loss: 2.0791651407877603

Epoch: 6| Step: 2
Training loss: 1.8743836879730225
Validation loss: 2.079216023286184

Epoch: 6| Step: 3
Training loss: 1.8937647342681885
Validation loss: 2.0753793517748513

Epoch: 6| Step: 4
Training loss: 1.6091872453689575
Validation loss: 2.0751809080441794

Epoch: 6| Step: 5
Training loss: 2.014894485473633
Validation loss: 2.05616702636083

Epoch: 6| Step: 6
Training loss: 2.317605972290039
Validation loss: 2.0572557846705117

Epoch: 6| Step: 7
Training loss: 2.13443660736084
Validation loss: 2.0644726951917014

Epoch: 6| Step: 8
Training loss: 2.0892562866210938
Validation loss: 2.057789365450541

Epoch: 6| Step: 9
Training loss: 1.8018828630447388
Validation loss: 2.0545024077097573

Epoch: 6| Step: 10
Training loss: 2.2112526893615723
Validation loss: 2.0574719508488974

Epoch: 6| Step: 11
Training loss: 1.832908272743225
Validation loss: 2.0543988148371377

Epoch: 6| Step: 12
Training loss: 1.7020834684371948
Validation loss: 2.065920650959015

Epoch: 6| Step: 13
Training loss: 2.6950249671936035
Validation loss: 2.0703724225362143

Epoch: 141| Step: 0
Training loss: 1.3833380937576294
Validation loss: 2.0598972042401633

Epoch: 6| Step: 1
Training loss: 2.2908639907836914
Validation loss: 2.073414981365204

Epoch: 6| Step: 2
Training loss: 2.151486396789551
Validation loss: 2.072984536488851

Epoch: 6| Step: 3
Training loss: 2.240631103515625
Validation loss: 2.0531603495279946

Epoch: 6| Step: 4
Training loss: 2.5960750579833984
Validation loss: 2.0803974668184915

Epoch: 6| Step: 5
Training loss: 1.845672369003296
Validation loss: 2.0835256576538086

Epoch: 6| Step: 6
Training loss: 2.0795812606811523
Validation loss: 2.075909892717997

Epoch: 6| Step: 7
Training loss: 2.15518856048584
Validation loss: 2.0825184981028237

Epoch: 6| Step: 8
Training loss: 1.936781883239746
Validation loss: 2.079694231351217

Epoch: 6| Step: 9
Training loss: 1.7984514236450195
Validation loss: 2.084082007408142

Epoch: 6| Step: 10
Training loss: 1.9579977989196777
Validation loss: 2.0778224070866904

Epoch: 6| Step: 11
Training loss: 2.427543878555298
Validation loss: 2.066184182961782

Epoch: 6| Step: 12
Training loss: 1.7326029539108276
Validation loss: 2.063791116078695

Epoch: 6| Step: 13
Training loss: 2.4115681648254395
Validation loss: 2.0724972089131675

Epoch: 142| Step: 0
Training loss: 2.5186028480529785
Validation loss: 2.062684873739878

Epoch: 6| Step: 1
Training loss: 2.1561789512634277
Validation loss: 2.0656628807385764

Epoch: 6| Step: 2
Training loss: 2.04573392868042
Validation loss: 2.0754500230153403

Epoch: 6| Step: 3
Training loss: 2.170600652694702
Validation loss: 2.0788406133651733

Epoch: 6| Step: 4
Training loss: 2.239457130432129
Validation loss: 2.0749009052912393

Epoch: 6| Step: 5
Training loss: 1.9244664907455444
Validation loss: 2.079889635245005

Epoch: 6| Step: 6
Training loss: 1.9278340339660645
Validation loss: 2.1047072410583496

Epoch: 6| Step: 7
Training loss: 2.1976613998413086
Validation loss: 2.0875036915143332

Epoch: 6| Step: 8
Training loss: 2.4051079750061035
Validation loss: 2.095765153566996

Epoch: 6| Step: 9
Training loss: 1.7459040880203247
Validation loss: 2.067067722479502

Epoch: 6| Step: 10
Training loss: 1.4860409498214722
Validation loss: 2.0892359614372253

Epoch: 6| Step: 11
Training loss: 1.7271792888641357
Validation loss: 2.081448753674825

Epoch: 6| Step: 12
Training loss: 2.0473976135253906
Validation loss: 2.0625410874684653

Epoch: 6| Step: 13
Training loss: 2.084190845489502
Validation loss: 2.073844333489736

Epoch: 143| Step: 0
Training loss: 1.9857008457183838
Validation loss: 2.067556083202362

Epoch: 6| Step: 1
Training loss: 2.4111175537109375
Validation loss: 2.0704070329666138

Epoch: 6| Step: 2
Training loss: 1.625090479850769
Validation loss: 2.0795347690582275

Epoch: 6| Step: 3
Training loss: 1.924227237701416
Validation loss: 2.066753844420115

Epoch: 6| Step: 4
Training loss: 2.4300389289855957
Validation loss: 2.0710704723993936

Epoch: 6| Step: 5
Training loss: 2.292280673980713
Validation loss: 2.0728352467219033

Epoch: 6| Step: 6
Training loss: 1.2630188465118408
Validation loss: 2.0782676935195923

Epoch: 6| Step: 7
Training loss: 1.9771125316619873
Validation loss: 2.0709212025006614

Epoch: 6| Step: 8
Training loss: 2.6236696243286133
Validation loss: 2.0789251128832498

Epoch: 6| Step: 9
Training loss: 1.7154231071472168
Validation loss: 2.0631638566652932

Epoch: 6| Step: 10
Training loss: 1.853424072265625
Validation loss: 2.0658411979675293

Epoch: 6| Step: 11
Training loss: 2.149667739868164
Validation loss: 2.0665330290794373

Epoch: 6| Step: 12
Training loss: 1.8739476203918457
Validation loss: 2.081294139226278

Epoch: 6| Step: 13
Training loss: 2.1405715942382812
Validation loss: 2.079116404056549

Epoch: 144| Step: 0
Training loss: 2.080881118774414
Validation loss: 2.0908333460489907

Epoch: 6| Step: 1
Training loss: 2.2691280841827393
Validation loss: 2.060295124848684

Epoch: 6| Step: 2
Training loss: 2.1005454063415527
Validation loss: 2.073044180870056

Epoch: 6| Step: 3
Training loss: 2.684246063232422
Validation loss: 2.063700040181478

Epoch: 6| Step: 4
Training loss: 2.3823962211608887
Validation loss: 2.0736607710520425

Epoch: 6| Step: 5
Training loss: 1.6962323188781738
Validation loss: 2.060843308766683

Epoch: 6| Step: 6
Training loss: 2.0958924293518066
Validation loss: 2.073649764060974

Epoch: 6| Step: 7
Training loss: 2.105581760406494
Validation loss: 2.0594375928243003

Epoch: 6| Step: 8
Training loss: 2.3174448013305664
Validation loss: 2.062257091204325

Epoch: 6| Step: 9
Training loss: 1.526576042175293
Validation loss: 2.0562453269958496

Epoch: 6| Step: 10
Training loss: 2.3687853813171387
Validation loss: 2.0728962222735086

Epoch: 6| Step: 11
Training loss: 2.0556159019470215
Validation loss: 2.078076163927714

Epoch: 6| Step: 12
Training loss: 1.2225409746170044
Validation loss: 2.0877065459887185

Epoch: 6| Step: 13
Training loss: 1.5397964715957642
Validation loss: 2.075706442197164

Epoch: 145| Step: 0
Training loss: 2.56919002532959
Validation loss: 2.1000152428944907

Epoch: 6| Step: 1
Training loss: 2.325737714767456
Validation loss: 2.1117815176645913

Epoch: 6| Step: 2
Training loss: 2.630272388458252
Validation loss: 2.092290222644806

Epoch: 6| Step: 3
Training loss: 2.2895381450653076
Validation loss: 2.0838910341262817

Epoch: 6| Step: 4
Training loss: 1.9977469444274902
Validation loss: 2.0978863636652627

Epoch: 6| Step: 5
Training loss: 2.19952392578125
Validation loss: 2.0882204174995422

Epoch: 6| Step: 6
Training loss: 2.202298164367676
Validation loss: 2.083779275417328

Epoch: 6| Step: 7
Training loss: 1.4572036266326904
Validation loss: 2.0752148628234863

Epoch: 6| Step: 8
Training loss: 1.4032669067382812
Validation loss: 2.0687150955200195

Epoch: 6| Step: 9
Training loss: 2.109800338745117
Validation loss: 2.0597458283106485

Epoch: 6| Step: 10
Training loss: 1.7884019613265991
Validation loss: 2.0512985587120056

Epoch: 6| Step: 11
Training loss: 2.0153679847717285
Validation loss: 2.0563976963361106

Epoch: 6| Step: 12
Training loss: 1.570475459098816
Validation loss: 2.050615588823954

Epoch: 6| Step: 13
Training loss: 1.9571311473846436
Validation loss: 2.0559646089871726

Epoch: 146| Step: 0
Training loss: 2.377042293548584
Validation loss: 2.0516557892163596

Epoch: 6| Step: 1
Training loss: 2.0914673805236816
Validation loss: 2.0587809681892395

Epoch: 6| Step: 2
Training loss: 2.409254550933838
Validation loss: 2.0719121297200522

Epoch: 6| Step: 3
Training loss: 1.9072763919830322
Validation loss: 2.065835177898407

Epoch: 6| Step: 4
Training loss: 2.5947704315185547
Validation loss: 2.088836371898651

Epoch: 6| Step: 5
Training loss: 1.4533944129943848
Validation loss: 2.1125996510187783

Epoch: 6| Step: 6
Training loss: 1.6815342903137207
Validation loss: 2.1108190615971885

Epoch: 6| Step: 7
Training loss: 1.9166206121444702
Validation loss: 2.094418545564016

Epoch: 6| Step: 8
Training loss: 2.0411221981048584
Validation loss: 2.098083277543386

Epoch: 6| Step: 9
Training loss: 2.089298725128174
Validation loss: 2.0858840346336365

Epoch: 6| Step: 10
Training loss: 2.0735316276550293
Validation loss: 2.0740291476249695

Epoch: 6| Step: 11
Training loss: 1.7748786211013794
Validation loss: 2.065831959247589

Epoch: 6| Step: 12
Training loss: 2.3418803215026855
Validation loss: 2.0744705398877463

Epoch: 6| Step: 13
Training loss: 2.1703529357910156
Validation loss: 2.0811148484547934

Epoch: 147| Step: 0
Training loss: 1.6657772064208984
Validation loss: 2.077061414718628

Epoch: 6| Step: 1
Training loss: 1.8880300521850586
Validation loss: 2.0629678765932717

Epoch: 6| Step: 2
Training loss: 2.2327356338500977
Validation loss: 2.0635138352711997

Epoch: 6| Step: 3
Training loss: 2.3586926460266113
Validation loss: 2.071433424949646

Epoch: 6| Step: 4
Training loss: 1.804376244544983
Validation loss: 2.0790456334749856

Epoch: 6| Step: 5
Training loss: 2.115182399749756
Validation loss: 2.06621778011322

Epoch: 6| Step: 6
Training loss: 1.7703063488006592
Validation loss: 2.0770650704701743

Epoch: 6| Step: 7
Training loss: 2.0204567909240723
Validation loss: 2.0803967714309692

Epoch: 6| Step: 8
Training loss: 2.313081741333008
Validation loss: 2.0852587819099426

Epoch: 6| Step: 9
Training loss: 1.7192308902740479
Validation loss: 2.085183620452881

Epoch: 6| Step: 10
Training loss: 2.479018449783325
Validation loss: 2.0796060959498086

Epoch: 6| Step: 11
Training loss: 1.9303576946258545
Validation loss: 2.083234945933024

Epoch: 6| Step: 12
Training loss: 2.802164077758789
Validation loss: 2.088778038819631

Epoch: 6| Step: 13
Training loss: 2.1118342876434326
Validation loss: 2.091129461924235

Epoch: 148| Step: 0
Training loss: 1.8845241069793701
Validation loss: 2.0781076550483704

Epoch: 6| Step: 1
Training loss: 2.3413424491882324
Validation loss: 2.072523534297943

Epoch: 6| Step: 2
Training loss: 2.5915780067443848
Validation loss: 2.0624821980794272

Epoch: 6| Step: 3
Training loss: 1.8365817070007324
Validation loss: 2.0600927472114563

Epoch: 6| Step: 4
Training loss: 2.2201738357543945
Validation loss: 2.0558066964149475

Epoch: 6| Step: 5
Training loss: 1.8275043964385986
Validation loss: 2.0676033099492392

Epoch: 6| Step: 6
Training loss: 2.067798137664795
Validation loss: 2.0661851167678833

Epoch: 6| Step: 7
Training loss: 1.998849868774414
Validation loss: 2.0613556504249573

Epoch: 6| Step: 8
Training loss: 1.7947969436645508
Validation loss: 2.0685585339864097

Epoch: 6| Step: 9
Training loss: 2.2372360229492188
Validation loss: 2.0752110878626504

Epoch: 6| Step: 10
Training loss: 1.7365508079528809
Validation loss: 2.073618729909261

Epoch: 6| Step: 11
Training loss: 2.335599899291992
Validation loss: 2.101106882095337

Epoch: 6| Step: 12
Training loss: 2.1764817237854004
Validation loss: 2.1049538056055703

Epoch: 6| Step: 13
Training loss: 1.7898709774017334
Validation loss: 2.1392099857330322

Epoch: 149| Step: 0
Training loss: 2.1549370288848877
Validation loss: 2.124536116917928

Epoch: 6| Step: 1
Training loss: 2.627267599105835
Validation loss: 2.118952989578247

Epoch: 6| Step: 2
Training loss: 2.749019145965576
Validation loss: 2.100892345110575

Epoch: 6| Step: 3
Training loss: 1.6856870651245117
Validation loss: 2.093675434589386

Epoch: 6| Step: 4
Training loss: 2.285015821456909
Validation loss: 2.083011746406555

Epoch: 6| Step: 5
Training loss: 1.9206924438476562
Validation loss: 2.0610639254252114

Epoch: 6| Step: 6
Training loss: 2.695525646209717
Validation loss: 2.067743460337321

Epoch: 6| Step: 7
Training loss: 1.6224215030670166
Validation loss: 2.0551178654034934

Epoch: 6| Step: 8
Training loss: 2.163832187652588
Validation loss: 2.056523005167643

Epoch: 6| Step: 9
Training loss: 2.2061963081359863
Validation loss: 2.053268551826477

Epoch: 6| Step: 10
Training loss: 1.6122710704803467
Validation loss: 2.042220731576284

Epoch: 6| Step: 11
Training loss: 1.8536934852600098
Validation loss: 2.0303356051445007

Epoch: 6| Step: 12
Training loss: 1.6602401733398438
Validation loss: 2.040121555328369

Epoch: 6| Step: 13
Training loss: 2.3616538047790527
Validation loss: 2.035020391146342

Epoch: 150| Step: 0
Training loss: 2.005337715148926
Validation loss: 2.032784124215444

Epoch: 6| Step: 1
Training loss: 2.3856632709503174
Validation loss: 2.0445327957471213

Epoch: 6| Step: 2
Training loss: 2.632002592086792
Validation loss: 2.061251978079478

Epoch: 6| Step: 3
Training loss: 1.4261010885238647
Validation loss: 2.0533227920532227

Epoch: 6| Step: 4
Training loss: 1.9706276655197144
Validation loss: 2.07231475909551

Epoch: 6| Step: 5
Training loss: 2.031096935272217
Validation loss: 2.0774449507395425

Epoch: 6| Step: 6
Training loss: 2.3316869735717773
Validation loss: 2.0827518701553345

Epoch: 6| Step: 7
Training loss: 2.4230239391326904
Validation loss: 2.087948441505432

Epoch: 6| Step: 8
Training loss: 2.5120720863342285
Validation loss: 2.0730306108792624

Epoch: 6| Step: 9
Training loss: 2.0780224800109863
Validation loss: 2.059343417485555

Epoch: 6| Step: 10
Training loss: 1.8942975997924805
Validation loss: 2.0786099632581077

Epoch: 6| Step: 11
Training loss: 1.6145884990692139
Validation loss: 2.076597352822622

Epoch: 6| Step: 12
Training loss: 1.2386783361434937
Validation loss: 2.0675347646077475

Epoch: 6| Step: 13
Training loss: 2.4527087211608887
Validation loss: 2.0594642559687295

Epoch: 151| Step: 0
Training loss: 2.2831010818481445
Validation loss: 2.0712648232777915

Epoch: 6| Step: 1
Training loss: 3.0613255500793457
Validation loss: 2.0668309330940247

Epoch: 6| Step: 2
Training loss: 2.2554492950439453
Validation loss: 2.0691230297088623

Epoch: 6| Step: 3
Training loss: 1.5344970226287842
Validation loss: 2.0721044540405273

Epoch: 6| Step: 4
Training loss: 1.9917798042297363
Validation loss: 2.0666110118230185

Epoch: 6| Step: 5
Training loss: 1.5775256156921387
Validation loss: 2.065526823202769

Epoch: 6| Step: 6
Training loss: 2.025921106338501
Validation loss: 2.0843380888303122

Epoch: 6| Step: 7
Training loss: 2.5632269382476807
Validation loss: 2.0744520823160806

Epoch: 6| Step: 8
Training loss: 2.1656196117401123
Validation loss: 2.0677753488222756

Epoch: 6| Step: 9
Training loss: 2.1843924522399902
Validation loss: 2.0612383683522544

Epoch: 6| Step: 10
Training loss: 2.3638782501220703
Validation loss: 2.0794885555903115

Epoch: 6| Step: 11
Training loss: 1.389014720916748
Validation loss: 2.073298712571462

Epoch: 6| Step: 12
Training loss: 1.3240587711334229
Validation loss: 2.0762424866358438

Epoch: 6| Step: 13
Training loss: 2.0334129333496094
Validation loss: 2.0795573790868125

Epoch: 152| Step: 0
Training loss: 1.9208496809005737
Validation loss: 2.0693058172861734

Epoch: 6| Step: 1
Training loss: 1.9235492944717407
Validation loss: 2.065477808316549

Epoch: 6| Step: 2
Training loss: 1.9701005220413208
Validation loss: 2.082764506340027

Epoch: 6| Step: 3
Training loss: 2.0633182525634766
Validation loss: 2.0732226371765137

Epoch: 6| Step: 4
Training loss: 2.3473000526428223
Validation loss: 2.0909767349561057

Epoch: 6| Step: 5
Training loss: 1.9363148212432861
Validation loss: 2.0927364031473794

Epoch: 6| Step: 6
Training loss: 2.3042044639587402
Validation loss: 2.0782952904701233

Epoch: 6| Step: 7
Training loss: 2.4356656074523926
Validation loss: 2.0746562480926514

Epoch: 6| Step: 8
Training loss: 1.7705211639404297
Validation loss: 2.06682558854421

Epoch: 6| Step: 9
Training loss: 2.1594486236572266
Validation loss: 2.0577459931373596

Epoch: 6| Step: 10
Training loss: 1.6355316638946533
Validation loss: 2.0790106852849326

Epoch: 6| Step: 11
Training loss: 2.2824738025665283
Validation loss: 2.081727385520935

Epoch: 6| Step: 12
Training loss: 2.0375571250915527
Validation loss: 2.0818862517674765

Epoch: 6| Step: 13
Training loss: 1.7201212644577026
Validation loss: 2.0854822794596353

Epoch: 153| Step: 0
Training loss: 2.186462879180908
Validation loss: 2.0819553335507712

Epoch: 6| Step: 1
Training loss: 2.166008710861206
Validation loss: 2.081890126069387

Epoch: 6| Step: 2
Training loss: 2.2184057235717773
Validation loss: 2.0836191177368164

Epoch: 6| Step: 3
Training loss: 1.8263580799102783
Validation loss: 2.085335652033488

Epoch: 6| Step: 4
Training loss: 2.1129133701324463
Validation loss: 2.1006570061047873

Epoch: 6| Step: 5
Training loss: 1.8040671348571777
Validation loss: 2.110914667447408

Epoch: 6| Step: 6
Training loss: 1.5374891757965088
Validation loss: 2.128814140955607

Epoch: 6| Step: 7
Training loss: 1.758833646774292
Validation loss: 2.104424238204956

Epoch: 6| Step: 8
Training loss: 2.093985080718994
Validation loss: 2.0906596183776855

Epoch: 6| Step: 9
Training loss: 2.1178741455078125
Validation loss: 2.0992899735768638

Epoch: 6| Step: 10
Training loss: 1.4073028564453125
Validation loss: 2.102631131807963

Epoch: 6| Step: 11
Training loss: 2.71812105178833
Validation loss: 2.083639701207479

Epoch: 6| Step: 12
Training loss: 2.6102142333984375
Validation loss: 2.0877605279286704

Epoch: 6| Step: 13
Training loss: 1.8660610914230347
Validation loss: 2.0869006315867105

Epoch: 154| Step: 0
Training loss: 1.9932708740234375
Validation loss: 2.0797914465268454

Epoch: 6| Step: 1
Training loss: 1.5452167987823486
Validation loss: 2.0755069057146707

Epoch: 6| Step: 2
Training loss: 2.402458429336548
Validation loss: 2.089349389076233

Epoch: 6| Step: 3
Training loss: 1.8908214569091797
Validation loss: 2.081416885058085

Epoch: 6| Step: 4
Training loss: 1.9471380710601807
Validation loss: 2.083545466264089

Epoch: 6| Step: 5
Training loss: 2.8411788940429688
Validation loss: 2.084885756174723

Epoch: 6| Step: 6
Training loss: 2.7020859718322754
Validation loss: 2.0929059783617654

Epoch: 6| Step: 7
Training loss: 2.010714054107666
Validation loss: 2.085131267706553

Epoch: 6| Step: 8
Training loss: 2.0052928924560547
Validation loss: 2.092916488647461

Epoch: 6| Step: 9
Training loss: 1.413182258605957
Validation loss: 2.077986478805542

Epoch: 6| Step: 10
Training loss: 2.0224695205688477
Validation loss: 2.0899473826090493

Epoch: 6| Step: 11
Training loss: 1.904282569885254
Validation loss: 2.083262304464976

Epoch: 6| Step: 12
Training loss: 2.0906753540039062
Validation loss: 2.079699079195658

Epoch: 6| Step: 13
Training loss: 1.4890973567962646
Validation loss: 2.082475781440735

Epoch: 155| Step: 0
Training loss: 2.4794628620147705
Validation loss: 2.107416033744812

Epoch: 6| Step: 1
Training loss: 1.9507437944412231
Validation loss: 2.1093507210413613

Epoch: 6| Step: 2
Training loss: 2.1917903423309326
Validation loss: 2.10891584555308

Epoch: 6| Step: 3
Training loss: 2.4587693214416504
Validation loss: 2.114541014035543

Epoch: 6| Step: 4
Training loss: 2.2674741744995117
Validation loss: 2.1212766567866006

Epoch: 6| Step: 5
Training loss: 2.026810884475708
Validation loss: 2.1168859799702964

Epoch: 6| Step: 6
Training loss: 2.5209097862243652
Validation loss: 2.1014027992884317

Epoch: 6| Step: 7
Training loss: 1.3900800943374634
Validation loss: 2.0922367771466575

Epoch: 6| Step: 8
Training loss: 2.0246119499206543
Validation loss: 2.1043269435564675

Epoch: 6| Step: 9
Training loss: 1.5215400457382202
Validation loss: 2.0856212178866067

Epoch: 6| Step: 10
Training loss: 1.3512849807739258
Validation loss: 2.067483445008596

Epoch: 6| Step: 11
Training loss: 2.210174798965454
Validation loss: 2.0916032195091248

Epoch: 6| Step: 12
Training loss: 2.1326539516448975
Validation loss: 2.0881981253623962

Epoch: 6| Step: 13
Training loss: 2.163768768310547
Validation loss: 2.091106951236725

Epoch: 156| Step: 0
Training loss: 2.110241413116455
Validation loss: 2.1059324940045676

Epoch: 6| Step: 1
Training loss: 2.002993106842041
Validation loss: 2.127612809340159

Epoch: 6| Step: 2
Training loss: 1.7748934030532837
Validation loss: 2.1254616181055703

Epoch: 6| Step: 3
Training loss: 2.5898027420043945
Validation loss: 2.1187697450319924

Epoch: 6| Step: 4
Training loss: 2.049687147140503
Validation loss: 2.1234776178995767

Epoch: 6| Step: 5
Training loss: 1.8572288751602173
Validation loss: 2.122074325879415

Epoch: 6| Step: 6
Training loss: 2.8413960933685303
Validation loss: 2.11592568953832

Epoch: 6| Step: 7
Training loss: 2.1622164249420166
Validation loss: 2.107535163561503

Epoch: 6| Step: 8
Training loss: 1.8303731679916382
Validation loss: 2.1035395860671997

Epoch: 6| Step: 9
Training loss: 2.023637533187866
Validation loss: 2.085020105044047

Epoch: 6| Step: 10
Training loss: 2.015368700027466
Validation loss: 2.082684417565664

Epoch: 6| Step: 11
Training loss: 1.6248912811279297
Validation loss: 2.0812886555989585

Epoch: 6| Step: 12
Training loss: 1.3990168571472168
Validation loss: 2.0815077821413674

Epoch: 6| Step: 13
Training loss: 2.4185245037078857
Validation loss: 2.0800079703330994

Epoch: 157| Step: 0
Training loss: 2.1246094703674316
Validation loss: 2.0928306380907693

Epoch: 6| Step: 1
Training loss: 1.6892191171646118
Validation loss: 2.0969612995783486

Epoch: 6| Step: 2
Training loss: 1.849407434463501
Validation loss: 2.091805318991343

Epoch: 6| Step: 3
Training loss: 2.257341146469116
Validation loss: 2.105170806248983

Epoch: 6| Step: 4
Training loss: 2.2829248905181885
Validation loss: 2.096284031867981

Epoch: 6| Step: 5
Training loss: 2.4287216663360596
Validation loss: 2.1012317140897117

Epoch: 6| Step: 6
Training loss: 2.2992048263549805
Validation loss: 2.097600837548574

Epoch: 6| Step: 7
Training loss: 2.1645922660827637
Validation loss: 2.1034012834231057

Epoch: 6| Step: 8
Training loss: 2.098402738571167
Validation loss: 2.1009993155797324

Epoch: 6| Step: 9
Training loss: 2.0442123413085938
Validation loss: 2.092686931292216

Epoch: 6| Step: 10
Training loss: 1.7117295265197754
Validation loss: 2.093560973803202

Epoch: 6| Step: 11
Training loss: 1.3908162117004395
Validation loss: 2.1004759867986045

Epoch: 6| Step: 12
Training loss: 1.8498947620391846
Validation loss: 2.0928042928377786

Epoch: 6| Step: 13
Training loss: 2.3907241821289062
Validation loss: 2.1063210368156433

Epoch: 158| Step: 0
Training loss: 1.6284997463226318
Validation loss: 2.10505340496699

Epoch: 6| Step: 1
Training loss: 2.1997017860412598
Validation loss: 2.0927403966585794

Epoch: 6| Step: 2
Training loss: 1.7130420207977295
Validation loss: 2.0888500809669495

Epoch: 6| Step: 3
Training loss: 1.9796416759490967
Validation loss: 2.0867083072662354

Epoch: 6| Step: 4
Training loss: 1.894618272781372
Validation loss: 2.082962393760681

Epoch: 6| Step: 5
Training loss: 1.88386869430542
Validation loss: 2.0762129028638205

Epoch: 6| Step: 6
Training loss: 2.221464157104492
Validation loss: 2.0868690609931946

Epoch: 6| Step: 7
Training loss: 2.1938700675964355
Validation loss: 2.083937962849935

Epoch: 6| Step: 8
Training loss: 1.805299162864685
Validation loss: 2.0883214275042215

Epoch: 6| Step: 9
Training loss: 1.9118603467941284
Validation loss: 2.0804045597712197

Epoch: 6| Step: 10
Training loss: 2.6706619262695312
Validation loss: 2.0918004314104715

Epoch: 6| Step: 11
Training loss: 2.146980047225952
Validation loss: 2.0816442370414734

Epoch: 6| Step: 12
Training loss: 2.0926337242126465
Validation loss: 2.0990976293881736

Epoch: 6| Step: 13
Training loss: 2.7093210220336914
Validation loss: 2.1049608985582986

Epoch: 159| Step: 0
Training loss: 2.1022183895111084
Validation loss: 2.101071834564209

Epoch: 6| Step: 1
Training loss: 2.2180118560791016
Validation loss: 2.118466317653656

Epoch: 6| Step: 2
Training loss: 2.624070882797241
Validation loss: 2.1298842430114746

Epoch: 6| Step: 3
Training loss: 2.056044578552246
Validation loss: 2.142960806687673

Epoch: 6| Step: 4
Training loss: 2.1505813598632812
Validation loss: 2.1195362210273743

Epoch: 6| Step: 5
Training loss: 2.0148062705993652
Validation loss: 2.1180347204208374

Epoch: 6| Step: 6
Training loss: 1.8801803588867188
Validation loss: 2.115424633026123

Epoch: 6| Step: 7
Training loss: 1.9783260822296143
Validation loss: 2.0950985153516135

Epoch: 6| Step: 8
Training loss: 1.7495906352996826
Validation loss: 2.101094047228495

Epoch: 6| Step: 9
Training loss: 1.8386971950531006
Validation loss: 2.099681476751963

Epoch: 6| Step: 10
Training loss: 2.581458568572998
Validation loss: 2.0860026280085244

Epoch: 6| Step: 11
Training loss: 1.8900766372680664
Validation loss: 2.087013006210327

Epoch: 6| Step: 12
Training loss: 2.0243754386901855
Validation loss: 2.075020452340444

Epoch: 6| Step: 13
Training loss: 1.3275442123413086
Validation loss: 2.077996869881948

Epoch: 160| Step: 0
Training loss: 2.3742053508758545
Validation loss: 2.0621901154518127

Epoch: 6| Step: 1
Training loss: 1.9272925853729248
Validation loss: 2.0651942094167075

Epoch: 6| Step: 2
Training loss: 2.0762860774993896
Validation loss: 2.068349321683248

Epoch: 6| Step: 3
Training loss: 1.737665057182312
Validation loss: 2.0641706188519797

Epoch: 6| Step: 4
Training loss: 2.5373878479003906
Validation loss: 2.07081405321757

Epoch: 6| Step: 5
Training loss: 1.5055164098739624
Validation loss: 2.075556774934133

Epoch: 6| Step: 6
Training loss: 1.7101585865020752
Validation loss: 2.0804216663042703

Epoch: 6| Step: 7
Training loss: 2.0078489780426025
Validation loss: 2.084048330783844

Epoch: 6| Step: 8
Training loss: 1.9271471500396729
Validation loss: 2.090347627798716

Epoch: 6| Step: 9
Training loss: 2.012232780456543
Validation loss: 2.0861796935399375

Epoch: 6| Step: 10
Training loss: 2.867748260498047
Validation loss: 2.094892203807831

Epoch: 6| Step: 11
Training loss: 1.970536231994629
Validation loss: 2.093478739261627

Epoch: 6| Step: 12
Training loss: 2.0060272216796875
Validation loss: 2.0815961559613547

Epoch: 6| Step: 13
Training loss: 1.838214635848999
Validation loss: 2.077996015548706

Epoch: 161| Step: 0
Training loss: 2.3533823490142822
Validation loss: 2.083923796812693

Epoch: 6| Step: 1
Training loss: 2.2026290893554688
Validation loss: 2.095066169897715

Epoch: 6| Step: 2
Training loss: 1.5628587007522583
Validation loss: 2.090471029281616

Epoch: 6| Step: 3
Training loss: 1.7309592962265015
Validation loss: 2.0969544450441995

Epoch: 6| Step: 4
Training loss: 2.6117029190063477
Validation loss: 2.103040039539337

Epoch: 6| Step: 5
Training loss: 1.9288790225982666
Validation loss: 2.0819393197695413

Epoch: 6| Step: 6
Training loss: 2.61979341506958
Validation loss: 2.0727315147717795

Epoch: 6| Step: 7
Training loss: 1.2696757316589355
Validation loss: 2.0750064651171365

Epoch: 6| Step: 8
Training loss: 2.0279793739318848
Validation loss: 2.079662640889486

Epoch: 6| Step: 9
Training loss: 1.5760903358459473
Validation loss: 2.075930595397949

Epoch: 6| Step: 10
Training loss: 1.9507958889007568
Validation loss: 2.075331370035807

Epoch: 6| Step: 11
Training loss: 2.203639507293701
Validation loss: 2.082366724809011

Epoch: 6| Step: 12
Training loss: 1.9133241176605225
Validation loss: 2.0725091099739075

Epoch: 6| Step: 13
Training loss: 2.4295501708984375
Validation loss: 2.093268076578776

Epoch: 162| Step: 0
Training loss: 2.254014015197754
Validation loss: 2.0688278476397195

Epoch: 6| Step: 1
Training loss: 1.582645297050476
Validation loss: 2.0939629077911377

Epoch: 6| Step: 2
Training loss: 1.9854592084884644
Validation loss: 2.0746558904647827

Epoch: 6| Step: 3
Training loss: 1.645698070526123
Validation loss: 2.07913339138031

Epoch: 6| Step: 4
Training loss: 2.0999021530151367
Validation loss: 2.0774391094843545

Epoch: 6| Step: 5
Training loss: 2.6626901626586914
Validation loss: 2.0981614192326865

Epoch: 6| Step: 6
Training loss: 1.9705597162246704
Validation loss: 2.106198728084564

Epoch: 6| Step: 7
Training loss: 2.148507595062256
Validation loss: 2.0926197171211243

Epoch: 6| Step: 8
Training loss: 2.240463972091675
Validation loss: 2.0861451228459678

Epoch: 6| Step: 9
Training loss: 2.3051328659057617
Validation loss: 2.098458449045817

Epoch: 6| Step: 10
Training loss: 2.373882293701172
Validation loss: 2.0744109749794006

Epoch: 6| Step: 11
Training loss: 1.4992561340332031
Validation loss: 2.076818585395813

Epoch: 6| Step: 12
Training loss: 1.6938179731369019
Validation loss: 2.0763081908226013

Epoch: 6| Step: 13
Training loss: 1.5755841732025146
Validation loss: 2.080878476301829

Epoch: 163| Step: 0
Training loss: 1.9827044010162354
Validation loss: 2.074168582757314

Epoch: 6| Step: 1
Training loss: 2.7079129219055176
Validation loss: 2.0806696017583213

Epoch: 6| Step: 2
Training loss: 1.7932265996932983
Validation loss: 2.075459440549215

Epoch: 6| Step: 3
Training loss: 1.8185287714004517
Validation loss: 2.0766497055689492

Epoch: 6| Step: 4
Training loss: 1.9395725727081299
Validation loss: 2.101489305496216

Epoch: 6| Step: 5
Training loss: 1.6784747838974
Validation loss: 2.0987775524457297

Epoch: 6| Step: 6
Training loss: 2.1035571098327637
Validation loss: 2.102934181690216

Epoch: 6| Step: 7
Training loss: 1.8143103122711182
Validation loss: 2.102341671784719

Epoch: 6| Step: 8
Training loss: 2.2881882190704346
Validation loss: 2.1158849199612937

Epoch: 6| Step: 9
Training loss: 1.7420411109924316
Validation loss: 2.109666387240092

Epoch: 6| Step: 10
Training loss: 1.6590816974639893
Validation loss: 2.1328349312146506

Epoch: 6| Step: 11
Training loss: 1.9548473358154297
Validation loss: 2.1399388313293457

Epoch: 6| Step: 12
Training loss: 1.8397529125213623
Validation loss: 2.1267664035161338

Epoch: 6| Step: 13
Training loss: 2.5308918952941895
Validation loss: 2.1202506025632224

Epoch: 164| Step: 0
Training loss: 2.4988720417022705
Validation loss: 2.124462068080902

Epoch: 6| Step: 1
Training loss: 1.7905445098876953
Validation loss: 2.1185678839683533

Epoch: 6| Step: 2
Training loss: 1.942948818206787
Validation loss: 2.131612777709961

Epoch: 6| Step: 3
Training loss: 2.259707450866699
Validation loss: 2.112189769744873

Epoch: 6| Step: 4
Training loss: 1.7984330654144287
Validation loss: 2.1158856948216758

Epoch: 6| Step: 5
Training loss: 2.5134172439575195
Validation loss: 2.1129634579022727

Epoch: 6| Step: 6
Training loss: 2.0495152473449707
Validation loss: 2.115692913532257

Epoch: 6| Step: 7
Training loss: 1.6575772762298584
Validation loss: 2.0883752703666687

Epoch: 6| Step: 8
Training loss: 2.1207833290100098
Validation loss: 2.1068379084269204

Epoch: 6| Step: 9
Training loss: 2.3602166175842285
Validation loss: 2.0916683872540793

Epoch: 6| Step: 10
Training loss: 1.9455456733703613
Validation loss: 2.0925569335619607

Epoch: 6| Step: 11
Training loss: 2.004128932952881
Validation loss: 2.094033976395925

Epoch: 6| Step: 12
Training loss: 1.4496996402740479
Validation loss: 2.1259721914927163

Epoch: 6| Step: 13
Training loss: 1.4992079734802246
Validation loss: 2.112144351005554

Epoch: 165| Step: 0
Training loss: 1.8523409366607666
Validation loss: 2.126064936319987

Epoch: 6| Step: 1
Training loss: 1.5337181091308594
Validation loss: 2.1217745542526245

Epoch: 6| Step: 2
Training loss: 2.0446534156799316
Validation loss: 2.1381993889808655

Epoch: 6| Step: 3
Training loss: 2.3871254920959473
Validation loss: 2.129294693470001

Epoch: 6| Step: 4
Training loss: 1.8852219581604004
Validation loss: 2.1319583654403687

Epoch: 6| Step: 5
Training loss: 2.1329824924468994
Validation loss: 2.139534870783488

Epoch: 6| Step: 6
Training loss: 1.82146155834198
Validation loss: 2.13368684053421

Epoch: 6| Step: 7
Training loss: 2.2200491428375244
Validation loss: 2.1484639048576355

Epoch: 6| Step: 8
Training loss: 2.6679434776306152
Validation loss: 2.1221521894137063

Epoch: 6| Step: 9
Training loss: 1.827434778213501
Validation loss: 2.1273687879244485

Epoch: 6| Step: 10
Training loss: 1.476994514465332
Validation loss: 2.1269298791885376

Epoch: 6| Step: 11
Training loss: 2.4375271797180176
Validation loss: 2.1145429015159607

Epoch: 6| Step: 12
Training loss: 2.043300151824951
Validation loss: 2.117093026638031

Epoch: 6| Step: 13
Training loss: 1.540647268295288
Validation loss: 2.122118512789408

Epoch: 166| Step: 0
Training loss: 2.330137252807617
Validation loss: 2.124251107374827

Epoch: 6| Step: 1
Training loss: 1.6846380233764648
Validation loss: 2.1208858489990234

Epoch: 6| Step: 2
Training loss: 2.262930154800415
Validation loss: 2.118937869866689

Epoch: 6| Step: 3
Training loss: 1.8099873065948486
Validation loss: 2.1328166325887046

Epoch: 6| Step: 4
Training loss: 1.5115985870361328
Validation loss: 2.113600770632426

Epoch: 6| Step: 5
Training loss: 2.325194835662842
Validation loss: 2.123187303543091

Epoch: 6| Step: 6
Training loss: 1.897472620010376
Validation loss: 2.1263251304626465

Epoch: 6| Step: 7
Training loss: 2.1385693550109863
Validation loss: 2.094435135523478

Epoch: 6| Step: 8
Training loss: 1.8782086372375488
Validation loss: 2.107595602671305

Epoch: 6| Step: 9
Training loss: 2.0765504837036133
Validation loss: 2.088292439778646

Epoch: 6| Step: 10
Training loss: 2.2320499420166016
Validation loss: 2.0771230856577554

Epoch: 6| Step: 11
Training loss: 2.0154788494110107
Validation loss: 2.093718965848287

Epoch: 6| Step: 12
Training loss: 1.824344277381897
Validation loss: 2.0863260626792908

Epoch: 6| Step: 13
Training loss: 2.1489925384521484
Validation loss: 2.0876909097035727

Epoch: 167| Step: 0
Training loss: 2.3156838417053223
Validation loss: 2.0788047115008035

Epoch: 6| Step: 1
Training loss: 2.402580976486206
Validation loss: 2.086142579714457

Epoch: 6| Step: 2
Training loss: 2.6257879734039307
Validation loss: 2.081477483113607

Epoch: 6| Step: 3
Training loss: 1.9026015996932983
Validation loss: 2.094477355480194

Epoch: 6| Step: 4
Training loss: 1.2076761722564697
Validation loss: 2.0774476329485574

Epoch: 6| Step: 5
Training loss: 2.03558087348938
Validation loss: 2.0709255933761597

Epoch: 6| Step: 6
Training loss: 1.7295198440551758
Validation loss: 2.0970598260561624

Epoch: 6| Step: 7
Training loss: 1.971693754196167
Validation loss: 2.0953375101089478

Epoch: 6| Step: 8
Training loss: 2.0126302242279053
Validation loss: 2.082973837852478

Epoch: 6| Step: 9
Training loss: 2.108170509338379
Validation loss: 2.07876193523407

Epoch: 6| Step: 10
Training loss: 1.9491455554962158
Validation loss: 2.0838290651639304

Epoch: 6| Step: 11
Training loss: 1.609426736831665
Validation loss: 2.0765573382377625

Epoch: 6| Step: 12
Training loss: 2.1262831687927246
Validation loss: 2.0839569568634033

Epoch: 6| Step: 13
Training loss: 2.119382858276367
Validation loss: 2.081676105658213

Epoch: 168| Step: 0
Training loss: 1.9838861227035522
Validation loss: 2.078771193822225

Epoch: 6| Step: 1
Training loss: 1.7252650260925293
Validation loss: 2.0859679182370505

Epoch: 6| Step: 2
Training loss: 1.9181997776031494
Validation loss: 2.078485985596975

Epoch: 6| Step: 3
Training loss: 1.824050784111023
Validation loss: 2.090391377607981

Epoch: 6| Step: 4
Training loss: 1.8583848476409912
Validation loss: 2.1125904520352683

Epoch: 6| Step: 5
Training loss: 2.2600889205932617
Validation loss: 2.112536052862803

Epoch: 6| Step: 6
Training loss: 1.5430325269699097
Validation loss: 2.128063221772512

Epoch: 6| Step: 7
Training loss: 1.987952470779419
Validation loss: 2.114112595717112

Epoch: 6| Step: 8
Training loss: 1.7885735034942627
Validation loss: 2.11690624554952

Epoch: 6| Step: 9
Training loss: 2.5027503967285156
Validation loss: 2.115011731783549

Epoch: 6| Step: 10
Training loss: 2.1195554733276367
Validation loss: 2.1123305559158325

Epoch: 6| Step: 11
Training loss: 2.1586997509002686
Validation loss: 2.0817747712135315

Epoch: 6| Step: 12
Training loss: 2.030384063720703
Validation loss: 2.0920885602633157

Epoch: 6| Step: 13
Training loss: 2.1009912490844727
Validation loss: 2.0974938670794168

Epoch: 169| Step: 0
Training loss: 2.4409828186035156
Validation loss: 2.081939180692037

Epoch: 6| Step: 1
Training loss: 2.1734862327575684
Validation loss: 2.0865877072016397

Epoch: 6| Step: 2
Training loss: 1.7650721073150635
Validation loss: 2.084271411101023

Epoch: 6| Step: 3
Training loss: 2.0092625617980957
Validation loss: 2.097872336705526

Epoch: 6| Step: 4
Training loss: 1.8866020441055298
Validation loss: 2.085466504096985

Epoch: 6| Step: 5
Training loss: 2.2480616569519043
Validation loss: 2.0860141118367515

Epoch: 6| Step: 6
Training loss: 1.6870402097702026
Validation loss: 2.080844283103943

Epoch: 6| Step: 7
Training loss: 2.2419753074645996
Validation loss: 2.083314379056295

Epoch: 6| Step: 8
Training loss: 1.989976406097412
Validation loss: 2.0814895629882812

Epoch: 6| Step: 9
Training loss: 2.148232936859131
Validation loss: 2.1010443766911826

Epoch: 6| Step: 10
Training loss: 1.5912458896636963
Validation loss: 2.089145243167877

Epoch: 6| Step: 11
Training loss: 1.8148102760314941
Validation loss: 2.0967552264531455

Epoch: 6| Step: 12
Training loss: 2.237677812576294
Validation loss: 2.095116217931112

Epoch: 6| Step: 13
Training loss: 2.353250026702881
Validation loss: 2.0892109672228494

Epoch: 170| Step: 0
Training loss: 2.630373001098633
Validation loss: 2.0929656426111856

Epoch: 6| Step: 1
Training loss: 1.4380545616149902
Validation loss: 2.1222461462020874

Epoch: 6| Step: 2
Training loss: 2.066561698913574
Validation loss: 2.1175452868143716

Epoch: 6| Step: 3
Training loss: 2.394439935684204
Validation loss: 2.1066947976748147

Epoch: 6| Step: 4
Training loss: 1.9381723403930664
Validation loss: 2.0955605109532676

Epoch: 6| Step: 5
Training loss: 1.8300193548202515
Validation loss: 2.093714714050293

Epoch: 6| Step: 6
Training loss: 2.0730769634246826
Validation loss: 2.105503479639689

Epoch: 6| Step: 7
Training loss: 2.600733757019043
Validation loss: 2.0979398488998413

Epoch: 6| Step: 8
Training loss: 1.812411904335022
Validation loss: 2.101104120413462

Epoch: 6| Step: 9
Training loss: 2.0291590690612793
Validation loss: 2.0952651500701904

Epoch: 6| Step: 10
Training loss: 1.5512527227401733
Validation loss: 2.0950894554456077

Epoch: 6| Step: 11
Training loss: 1.7258278131484985
Validation loss: 2.0790494879086814

Epoch: 6| Step: 12
Training loss: 1.826424241065979
Validation loss: 2.085656523704529

Epoch: 6| Step: 13
Training loss: 2.013549566268921
Validation loss: 2.064127723375956

Epoch: 171| Step: 0
Training loss: 1.8733913898468018
Validation loss: 2.0649271607398987

Epoch: 6| Step: 1
Training loss: 2.292034387588501
Validation loss: 2.077250341574351

Epoch: 6| Step: 2
Training loss: 2.1545469760894775
Validation loss: 2.0757219791412354

Epoch: 6| Step: 3
Training loss: 1.7233471870422363
Validation loss: 2.064269800980886

Epoch: 6| Step: 4
Training loss: 2.8874263763427734
Validation loss: 2.0865554014841714

Epoch: 6| Step: 5
Training loss: 1.8145042657852173
Validation loss: 2.079072376092275

Epoch: 6| Step: 6
Training loss: 1.7486838102340698
Validation loss: 2.0842705965042114

Epoch: 6| Step: 7
Training loss: 2.205416202545166
Validation loss: 2.070521672566732

Epoch: 6| Step: 8
Training loss: 1.8960856199264526
Validation loss: 2.0793081720670066

Epoch: 6| Step: 9
Training loss: 2.1274075508117676
Validation loss: 2.0737034678459167

Epoch: 6| Step: 10
Training loss: 2.004978656768799
Validation loss: 2.0772939324378967

Epoch: 6| Step: 11
Training loss: 2.0010342597961426
Validation loss: 2.0703707536061606

Epoch: 6| Step: 12
Training loss: 2.606670379638672
Validation loss: 2.073292076587677

Epoch: 6| Step: 13
Training loss: 2.2118165493011475
Validation loss: 2.080451726913452

Epoch: 172| Step: 0
Training loss: 1.5166051387786865
Validation loss: 2.081783910592397

Epoch: 6| Step: 1
Training loss: 1.7947912216186523
Validation loss: 2.0745485424995422

Epoch: 6| Step: 2
Training loss: 2.2578272819519043
Validation loss: 2.0881800055503845

Epoch: 6| Step: 3
Training loss: 2.150106906890869
Validation loss: 2.100532571474711

Epoch: 6| Step: 4
Training loss: 2.078380823135376
Validation loss: 2.1339780688285828

Epoch: 6| Step: 5
Training loss: 2.379147529602051
Validation loss: 2.1338788668314614

Epoch: 6| Step: 6
Training loss: 2.619790554046631
Validation loss: 2.1324098904927573

Epoch: 6| Step: 7
Training loss: 1.888994812965393
Validation loss: 2.1288530429204306

Epoch: 6| Step: 8
Training loss: 1.8473658561706543
Validation loss: 2.1250857512156167

Epoch: 6| Step: 9
Training loss: 1.7483689785003662
Validation loss: 2.121889809767405

Epoch: 6| Step: 10
Training loss: 2.0356714725494385
Validation loss: 2.096150279045105

Epoch: 6| Step: 11
Training loss: 2.10041880607605
Validation loss: 2.0896187822024026

Epoch: 6| Step: 12
Training loss: 2.40480637550354
Validation loss: 2.0776527722676597

Epoch: 6| Step: 13
Training loss: 1.5100562572479248
Validation loss: 2.0701081355412803

Epoch: 173| Step: 0
Training loss: 1.7975521087646484
Validation loss: 2.0661710699399314

Epoch: 6| Step: 1
Training loss: 2.0008487701416016
Validation loss: 2.068178653717041

Epoch: 6| Step: 2
Training loss: 2.167290210723877
Validation loss: 2.069070518016815

Epoch: 6| Step: 3
Training loss: 2.3088841438293457
Validation loss: 2.06073659658432

Epoch: 6| Step: 4
Training loss: 1.847683072090149
Validation loss: 2.063896973927816

Epoch: 6| Step: 5
Training loss: 1.5658472776412964
Validation loss: 2.0581639210383096

Epoch: 6| Step: 6
Training loss: 2.1376044750213623
Validation loss: 2.064996302127838

Epoch: 6| Step: 7
Training loss: 1.8575516939163208
Validation loss: 2.057937264442444

Epoch: 6| Step: 8
Training loss: 2.0485806465148926
Validation loss: 2.052794655164083

Epoch: 6| Step: 9
Training loss: 1.8919135332107544
Validation loss: 2.060212016105652

Epoch: 6| Step: 10
Training loss: 2.7537169456481934
Validation loss: 2.0519273479779563

Epoch: 6| Step: 11
Training loss: 1.8370949029922485
Validation loss: 2.0670911073684692

Epoch: 6| Step: 12
Training loss: 2.5551347732543945
Validation loss: 2.0523988604545593

Epoch: 6| Step: 13
Training loss: 2.0806734561920166
Validation loss: 2.0743781328201294

Epoch: 174| Step: 0
Training loss: 1.6793146133422852
Validation loss: 2.062028090159098

Epoch: 6| Step: 1
Training loss: 2.129652738571167
Validation loss: 2.072329878807068

Epoch: 6| Step: 2
Training loss: 1.7092021703720093
Validation loss: 2.076207419236501

Epoch: 6| Step: 3
Training loss: 2.6488914489746094
Validation loss: 2.0678451657295227

Epoch: 6| Step: 4
Training loss: 2.5191359519958496
Validation loss: 2.0763849218686423

Epoch: 6| Step: 5
Training loss: 1.9776264429092407
Validation loss: 2.08525158961614

Epoch: 6| Step: 6
Training loss: 2.1715750694274902
Validation loss: 2.090273141860962

Epoch: 6| Step: 7
Training loss: 2.0887937545776367
Validation loss: 2.086577534675598

Epoch: 6| Step: 8
Training loss: 2.8632850646972656
Validation loss: 2.090008874734243

Epoch: 6| Step: 9
Training loss: 1.6708626747131348
Validation loss: 2.074784835179647

Epoch: 6| Step: 10
Training loss: 1.7998226881027222
Validation loss: 2.082904895146688

Epoch: 6| Step: 11
Training loss: 1.3386636972427368
Validation loss: 2.0845455328623452

Epoch: 6| Step: 12
Training loss: 2.0032520294189453
Validation loss: 2.0693167050679526

Epoch: 6| Step: 13
Training loss: 1.8014612197875977
Validation loss: 2.0773578683535256

Epoch: 175| Step: 0
Training loss: 1.970961332321167
Validation loss: 2.0778427918752036

Epoch: 6| Step: 1
Training loss: 2.3151516914367676
Validation loss: 2.084538996219635

Epoch: 6| Step: 2
Training loss: 2.4393298625946045
Validation loss: 2.088802178700765

Epoch: 6| Step: 3
Training loss: 1.4388282299041748
Validation loss: 2.0752045710881553

Epoch: 6| Step: 4
Training loss: 1.9595023393630981
Validation loss: 2.0854163567225137

Epoch: 6| Step: 5
Training loss: 1.6901156902313232
Validation loss: 2.083060304323832

Epoch: 6| Step: 6
Training loss: 2.1354289054870605
Validation loss: 2.0851041873296103

Epoch: 6| Step: 7
Training loss: 1.7995314598083496
Validation loss: 2.087219556172689

Epoch: 6| Step: 8
Training loss: 2.157721519470215
Validation loss: 2.0901954968770347

Epoch: 6| Step: 9
Training loss: 2.4416794776916504
Validation loss: 2.077287753423055

Epoch: 6| Step: 10
Training loss: 1.5380765199661255
Validation loss: 2.081909159819285

Epoch: 6| Step: 11
Training loss: 2.2012133598327637
Validation loss: 2.093968987464905

Epoch: 6| Step: 12
Training loss: 1.5974647998809814
Validation loss: 2.111084302266439

Epoch: 6| Step: 13
Training loss: 2.5270512104034424
Validation loss: 2.1002931594848633

Epoch: 176| Step: 0
Training loss: 1.8360373973846436
Validation loss: 2.0806185404459634

Epoch: 6| Step: 1
Training loss: 1.7901809215545654
Validation loss: 2.0839349826176963

Epoch: 6| Step: 2
Training loss: 2.0057740211486816
Validation loss: 2.0823078552881875

Epoch: 6| Step: 3
Training loss: 2.517014980316162
Validation loss: 2.0782893101374307

Epoch: 6| Step: 4
Training loss: 2.3691043853759766
Validation loss: 2.089431365331014

Epoch: 6| Step: 5
Training loss: 1.8447742462158203
Validation loss: 2.0774797399838767

Epoch: 6| Step: 6
Training loss: 2.1251728534698486
Validation loss: 2.0972116192181907

Epoch: 6| Step: 7
Training loss: 2.0726118087768555
Validation loss: 2.0816261172294617

Epoch: 6| Step: 8
Training loss: 1.6017944812774658
Validation loss: 2.081626057624817

Epoch: 6| Step: 9
Training loss: 1.646501064300537
Validation loss: 2.090835392475128

Epoch: 6| Step: 10
Training loss: 2.005716323852539
Validation loss: 2.100066622098287

Epoch: 6| Step: 11
Training loss: 1.9107692241668701
Validation loss: 2.0881627599398294

Epoch: 6| Step: 12
Training loss: 2.1853556632995605
Validation loss: 2.103199084599813

Epoch: 6| Step: 13
Training loss: 1.9166138172149658
Validation loss: 2.0832117001215615

Epoch: 177| Step: 0
Training loss: 2.1718878746032715
Validation loss: 2.088977118333181

Epoch: 6| Step: 1
Training loss: 2.2024528980255127
Validation loss: 2.0933058063189187

Epoch: 6| Step: 2
Training loss: 1.9544719457626343
Validation loss: 2.0889380176862082

Epoch: 6| Step: 3
Training loss: 2.287487030029297
Validation loss: 2.08003838857015

Epoch: 6| Step: 4
Training loss: 1.8491475582122803
Validation loss: 2.0774100621541343

Epoch: 6| Step: 5
Training loss: 1.530538558959961
Validation loss: 2.0870908896128335

Epoch: 6| Step: 6
Training loss: 2.232192039489746
Validation loss: 2.076774299144745

Epoch: 6| Step: 7
Training loss: 2.2901477813720703
Validation loss: 2.0726232727368674

Epoch: 6| Step: 8
Training loss: 1.7503749132156372
Validation loss: 2.078331708908081

Epoch: 6| Step: 9
Training loss: 2.056107521057129
Validation loss: 2.098175128300985

Epoch: 6| Step: 10
Training loss: 1.3969727754592896
Validation loss: 2.0943315426508584

Epoch: 6| Step: 11
Training loss: 1.7823699712753296
Validation loss: 2.086149434248606

Epoch: 6| Step: 12
Training loss: 2.3445401191711426
Validation loss: 2.0769255956014

Epoch: 6| Step: 13
Training loss: 1.8657495975494385
Validation loss: 2.0979806383450827

Epoch: 178| Step: 0
Training loss: 2.2627549171447754
Validation loss: 2.0920204321543374

Epoch: 6| Step: 1
Training loss: 1.9171262979507446
Validation loss: 2.0906074245770774

Epoch: 6| Step: 2
Training loss: 1.953837275505066
Validation loss: 2.0922496914863586

Epoch: 6| Step: 3
Training loss: 1.782503604888916
Validation loss: 2.093080163002014

Epoch: 6| Step: 4
Training loss: 2.7963719367980957
Validation loss: 2.0845317244529724

Epoch: 6| Step: 5
Training loss: 2.000359535217285
Validation loss: 2.102483073870341

Epoch: 6| Step: 6
Training loss: 1.9123274087905884
Validation loss: 2.0963383515675864

Epoch: 6| Step: 7
Training loss: 1.7142720222473145
Validation loss: 2.097551385561625

Epoch: 6| Step: 8
Training loss: 2.852334976196289
Validation loss: 2.1063525875409446

Epoch: 6| Step: 9
Training loss: 1.7661978006362915
Validation loss: 2.112902839978536

Epoch: 6| Step: 10
Training loss: 1.8044376373291016
Validation loss: 2.0893508195877075

Epoch: 6| Step: 11
Training loss: 1.7422611713409424
Validation loss: 2.0922277371088662

Epoch: 6| Step: 12
Training loss: 2.0730512142181396
Validation loss: 2.097303827603658

Epoch: 6| Step: 13
Training loss: 1.6499130725860596
Validation loss: 2.1025763750076294

Epoch: 179| Step: 0
Training loss: 1.9413126707077026
Validation loss: 2.103049616018931

Epoch: 6| Step: 1
Training loss: 2.2129359245300293
Validation loss: 2.1042727629343667

Epoch: 6| Step: 2
Training loss: 1.8375385999679565
Validation loss: 2.1092101335525513

Epoch: 6| Step: 3
Training loss: 1.572391390800476
Validation loss: 2.1045873562494912

Epoch: 6| Step: 4
Training loss: 1.3747944831848145
Validation loss: 2.135501225789388

Epoch: 6| Step: 5
Training loss: 2.3210859298706055
Validation loss: 2.1114636063575745

Epoch: 6| Step: 6
Training loss: 2.2155303955078125
Validation loss: 2.138301690419515

Epoch: 6| Step: 7
Training loss: 1.5143680572509766
Validation loss: 2.132652242978414

Epoch: 6| Step: 8
Training loss: 2.136504888534546
Validation loss: 2.122756838798523

Epoch: 6| Step: 9
Training loss: 2.231261730194092
Validation loss: 2.1303067803382874

Epoch: 6| Step: 10
Training loss: 2.062570095062256
Validation loss: 2.1203380028406777

Epoch: 6| Step: 11
Training loss: 2.005781650543213
Validation loss: 2.126397649447123

Epoch: 6| Step: 12
Training loss: 2.076167583465576
Validation loss: 2.1226815581321716

Epoch: 6| Step: 13
Training loss: 2.235701322555542
Validation loss: 2.121757706006368

Epoch: 180| Step: 0
Training loss: 1.7783339023590088
Validation loss: 2.1316434939702353

Epoch: 6| Step: 1
Training loss: 1.6521873474121094
Validation loss: 2.1346011559168496

Epoch: 6| Step: 2
Training loss: 2.2079052925109863
Validation loss: 2.1172553102175393

Epoch: 6| Step: 3
Training loss: 1.7461235523223877
Validation loss: 2.1250295639038086

Epoch: 6| Step: 4
Training loss: 2.1851718425750732
Validation loss: 2.126596768697103

Epoch: 6| Step: 5
Training loss: 2.1485772132873535
Validation loss: 2.133608400821686

Epoch: 6| Step: 6
Training loss: 2.3243579864501953
Validation loss: 2.1292728583017984

Epoch: 6| Step: 7
Training loss: 1.78737473487854
Validation loss: 2.1474008560180664

Epoch: 6| Step: 8
Training loss: 1.8914097547531128
Validation loss: 2.125653088092804

Epoch: 6| Step: 9
Training loss: 1.8620290756225586
Validation loss: 2.1483420928319297

Epoch: 6| Step: 10
Training loss: 1.837428092956543
Validation loss: 2.136655608812968

Epoch: 6| Step: 11
Training loss: 1.9982445240020752
Validation loss: 2.1336953043937683

Epoch: 6| Step: 12
Training loss: 2.0855774879455566
Validation loss: 2.1255734165509543

Epoch: 6| Step: 13
Training loss: 2.0789740085601807
Validation loss: 2.1216986974080405

Epoch: 181| Step: 0
Training loss: 2.1237857341766357
Validation loss: 2.124186098575592

Epoch: 6| Step: 1
Training loss: 1.408630132675171
Validation loss: 2.1234174172083535

Epoch: 6| Step: 2
Training loss: 1.7904229164123535
Validation loss: 2.1251072883605957

Epoch: 6| Step: 3
Training loss: 2.0994277000427246
Validation loss: 2.1258346239725747

Epoch: 6| Step: 4
Training loss: 2.2053539752960205
Validation loss: 2.1114139954249063

Epoch: 6| Step: 5
Training loss: 2.3044424057006836
Validation loss: 2.106994867324829

Epoch: 6| Step: 6
Training loss: 1.5913128852844238
Validation loss: 2.1010141571362815

Epoch: 6| Step: 7
Training loss: 1.822917103767395
Validation loss: 2.115508198738098

Epoch: 6| Step: 8
Training loss: 2.3659050464630127
Validation loss: 2.122246563434601

Epoch: 6| Step: 9
Training loss: 2.078096389770508
Validation loss: 2.1224618554115295

Epoch: 6| Step: 10
Training loss: 2.2237298488616943
Validation loss: 2.124751349290212

Epoch: 6| Step: 11
Training loss: 1.7850868701934814
Validation loss: 2.1212340195973716

Epoch: 6| Step: 12
Training loss: 1.7112020254135132
Validation loss: 2.1295114358266196

Epoch: 6| Step: 13
Training loss: 2.033020496368408
Validation loss: 2.1049760580062866

Epoch: 182| Step: 0
Training loss: 2.021390199661255
Validation loss: 2.110105494658152

Epoch: 6| Step: 1
Training loss: 2.2416200637817383
Validation loss: 2.0891709327697754

Epoch: 6| Step: 2
Training loss: 2.0783193111419678
Validation loss: 2.0821345448493958

Epoch: 6| Step: 3
Training loss: 1.6920299530029297
Validation loss: 2.0794875423113504

Epoch: 6| Step: 4
Training loss: 1.646701693534851
Validation loss: 2.079166273276011

Epoch: 6| Step: 5
Training loss: 1.936259388923645
Validation loss: 2.0769686698913574

Epoch: 6| Step: 6
Training loss: 1.9630298614501953
Validation loss: 2.0672951539357505

Epoch: 6| Step: 7
Training loss: 2.307342529296875
Validation loss: 2.0889768600463867

Epoch: 6| Step: 8
Training loss: 1.4801599979400635
Validation loss: 2.0983389417330423

Epoch: 6| Step: 9
Training loss: 2.210822582244873
Validation loss: 2.0981194178263345

Epoch: 6| Step: 10
Training loss: 2.2053821086883545
Validation loss: 2.123157024383545

Epoch: 6| Step: 11
Training loss: 1.8186404705047607
Validation loss: 2.12980055809021

Epoch: 6| Step: 12
Training loss: 1.8823301792144775
Validation loss: 2.124200463294983

Epoch: 6| Step: 13
Training loss: 2.332392930984497
Validation loss: 2.117607851823171

Epoch: 183| Step: 0
Training loss: 2.140192747116089
Validation loss: 2.1351171135902405

Epoch: 6| Step: 1
Training loss: 1.67123544216156
Validation loss: 2.129724303881327

Epoch: 6| Step: 2
Training loss: 2.311290740966797
Validation loss: 2.1080098350842795

Epoch: 6| Step: 3
Training loss: 2.028585433959961
Validation loss: 2.1007508834203086

Epoch: 6| Step: 4
Training loss: 1.9248849153518677
Validation loss: 2.0945814847946167

Epoch: 6| Step: 5
Training loss: 1.643296480178833
Validation loss: 2.1060357292493186

Epoch: 6| Step: 6
Training loss: 2.473857879638672
Validation loss: 2.0786674420038858

Epoch: 6| Step: 7
Training loss: 1.924512267112732
Validation loss: 2.0715571641921997

Epoch: 6| Step: 8
Training loss: 1.4034254550933838
Validation loss: 2.0753331184387207

Epoch: 6| Step: 9
Training loss: 1.76580810546875
Validation loss: 2.075149178504944

Epoch: 6| Step: 10
Training loss: 2.47702693939209
Validation loss: 2.059827208518982

Epoch: 6| Step: 11
Training loss: 1.762967586517334
Validation loss: 2.058587690194448

Epoch: 6| Step: 12
Training loss: 1.8844162225723267
Validation loss: 2.0704472263654075

Epoch: 6| Step: 13
Training loss: 2.3886377811431885
Validation loss: 2.068372925122579

Epoch: 184| Step: 0
Training loss: 1.833479642868042
Validation loss: 2.070764501889547

Epoch: 6| Step: 1
Training loss: 1.2616426944732666
Validation loss: 2.0810707807540894

Epoch: 6| Step: 2
Training loss: 1.7154613733291626
Validation loss: 2.087584654490153

Epoch: 6| Step: 3
Training loss: 2.2360682487487793
Validation loss: 2.0961106220881143

Epoch: 6| Step: 4
Training loss: 2.264888048171997
Validation loss: 2.103826324144999

Epoch: 6| Step: 5
Training loss: 2.0936505794525146
Validation loss: 2.101899186770121

Epoch: 6| Step: 6
Training loss: 2.3152894973754883
Validation loss: 2.1238839626312256

Epoch: 6| Step: 7
Training loss: 2.1799063682556152
Validation loss: 2.1207528710365295

Epoch: 6| Step: 8
Training loss: 1.3669072389602661
Validation loss: 2.1211684544881186

Epoch: 6| Step: 9
Training loss: 2.1850686073303223
Validation loss: 2.117126186688741

Epoch: 6| Step: 10
Training loss: 2.6218936443328857
Validation loss: 2.113457520802816

Epoch: 6| Step: 11
Training loss: 1.4231947660446167
Validation loss: 2.105572740236918

Epoch: 6| Step: 12
Training loss: 1.833958625793457
Validation loss: 2.102794071038564

Epoch: 6| Step: 13
Training loss: 2.190068006515503
Validation loss: 2.105365594228109

Epoch: 185| Step: 0
Training loss: 1.5708351135253906
Validation loss: 2.10367484887441

Epoch: 6| Step: 1
Training loss: 2.0734310150146484
Validation loss: 2.107312877972921

Epoch: 6| Step: 2
Training loss: 1.7413828372955322
Validation loss: 2.0959247748057046

Epoch: 6| Step: 3
Training loss: 2.1200361251831055
Validation loss: 2.1024945179621377

Epoch: 6| Step: 4
Training loss: 1.7795143127441406
Validation loss: 2.113257348537445

Epoch: 6| Step: 5
Training loss: 1.997086763381958
Validation loss: 2.117566227912903

Epoch: 6| Step: 6
Training loss: 1.6794155836105347
Validation loss: 2.10091241200765

Epoch: 6| Step: 7
Training loss: 1.9030017852783203
Validation loss: 2.1072174310684204

Epoch: 6| Step: 8
Training loss: 1.9850149154663086
Validation loss: 2.109417140483856

Epoch: 6| Step: 9
Training loss: 2.5367980003356934
Validation loss: 2.1040082772572837

Epoch: 6| Step: 10
Training loss: 2.0331871509552
Validation loss: 2.092447300752004

Epoch: 6| Step: 11
Training loss: 2.050173282623291
Validation loss: 2.1095489064852395

Epoch: 6| Step: 12
Training loss: 1.6854841709136963
Validation loss: 2.1064157287279763

Epoch: 6| Step: 13
Training loss: 2.4098193645477295
Validation loss: 2.114839712778727

Epoch: 186| Step: 0
Training loss: 2.117546796798706
Validation loss: 2.1115041971206665

Epoch: 6| Step: 1
Training loss: 1.6665695905685425
Validation loss: 2.1183162927627563

Epoch: 6| Step: 2
Training loss: 1.8556417226791382
Validation loss: 2.1130761901537576

Epoch: 6| Step: 3
Training loss: 2.0205626487731934
Validation loss: 2.125640074412028

Epoch: 6| Step: 4
Training loss: 1.9610874652862549
Validation loss: 2.108725925286611

Epoch: 6| Step: 5
Training loss: 1.9487743377685547
Validation loss: 2.1145102779070535

Epoch: 6| Step: 6
Training loss: 2.1226959228515625
Validation loss: 2.1119501193364463

Epoch: 6| Step: 7
Training loss: 1.5908377170562744
Validation loss: 2.1250091195106506

Epoch: 6| Step: 8
Training loss: 2.13261342048645
Validation loss: 2.11104679107666

Epoch: 6| Step: 9
Training loss: 2.398895025253296
Validation loss: 2.115735034147898

Epoch: 6| Step: 10
Training loss: 1.5895801782608032
Validation loss: 2.1149019400278726

Epoch: 6| Step: 11
Training loss: 1.9993770122528076
Validation loss: 2.101643443107605

Epoch: 6| Step: 12
Training loss: 1.9926037788391113
Validation loss: 2.1068794329961142

Epoch: 6| Step: 13
Training loss: 1.9485116004943848
Validation loss: 2.103165626525879

Epoch: 187| Step: 0
Training loss: 1.7928178310394287
Validation loss: 2.108417054017385

Epoch: 6| Step: 1
Training loss: 1.7527101039886475
Validation loss: 2.091486692428589

Epoch: 6| Step: 2
Training loss: 1.9148244857788086
Validation loss: 2.0909777084986367

Epoch: 6| Step: 3
Training loss: 1.6944992542266846
Validation loss: 2.100569486618042

Epoch: 6| Step: 4
Training loss: 1.9210000038146973
Validation loss: 2.0890862544377646

Epoch: 6| Step: 5
Training loss: 2.062281608581543
Validation loss: 2.100803256034851

Epoch: 6| Step: 6
Training loss: 2.75885009765625
Validation loss: 2.1202536622683206

Epoch: 6| Step: 7
Training loss: 1.9968702793121338
Validation loss: 2.108682096004486

Epoch: 6| Step: 8
Training loss: 1.8330061435699463
Validation loss: 2.133106311162313

Epoch: 6| Step: 9
Training loss: 2.197920799255371
Validation loss: 2.11181769768397

Epoch: 6| Step: 10
Training loss: 2.7712345123291016
Validation loss: 2.1226048866907754

Epoch: 6| Step: 11
Training loss: 1.176253080368042
Validation loss: 2.105625331401825

Epoch: 6| Step: 12
Training loss: 2.222080945968628
Validation loss: 2.1057894825935364

Epoch: 6| Step: 13
Training loss: 1.6378414630889893
Validation loss: 2.1048977971076965

Epoch: 188| Step: 0
Training loss: 1.6836872100830078
Validation loss: 2.1019347310066223

Epoch: 6| Step: 1
Training loss: 1.602953314781189
Validation loss: 2.09669953584671

Epoch: 6| Step: 2
Training loss: 2.3433728218078613
Validation loss: 2.1017949183781943

Epoch: 6| Step: 3
Training loss: 1.4862663745880127
Validation loss: 2.093086580435435

Epoch: 6| Step: 4
Training loss: 2.0255496501922607
Validation loss: 2.1015080412228904

Epoch: 6| Step: 5
Training loss: 2.2651095390319824
Validation loss: 2.093559761842092

Epoch: 6| Step: 6
Training loss: 1.4592349529266357
Validation loss: 2.104990303516388

Epoch: 6| Step: 7
Training loss: 1.4977977275848389
Validation loss: 2.1160773038864136

Epoch: 6| Step: 8
Training loss: 2.2887465953826904
Validation loss: 2.1175646781921387

Epoch: 6| Step: 9
Training loss: 2.3716623783111572
Validation loss: 2.118740717569987

Epoch: 6| Step: 10
Training loss: 1.9596693515777588
Validation loss: 2.132281482219696

Epoch: 6| Step: 11
Training loss: 1.7539887428283691
Validation loss: 2.130417764186859

Epoch: 6| Step: 12
Training loss: 2.1689863204956055
Validation loss: 2.1266863544782004

Epoch: 6| Step: 13
Training loss: 2.604123592376709
Validation loss: 2.1084693670272827

Epoch: 189| Step: 0
Training loss: 2.1315369606018066
Validation loss: 2.123305598894755

Epoch: 6| Step: 1
Training loss: 1.0938796997070312
Validation loss: 2.1237462759017944

Epoch: 6| Step: 2
Training loss: 2.4113166332244873
Validation loss: 2.1148282090822854

Epoch: 6| Step: 3
Training loss: 1.9362413883209229
Validation loss: 2.1032459338506064

Epoch: 6| Step: 4
Training loss: 1.657670497894287
Validation loss: 2.109375258286794

Epoch: 6| Step: 5
Training loss: 2.1434898376464844
Validation loss: 2.122897446155548

Epoch: 6| Step: 6
Training loss: 1.7234877347946167
Validation loss: 2.094358801841736

Epoch: 6| Step: 7
Training loss: 1.9600578546524048
Validation loss: 2.0915280183156333

Epoch: 6| Step: 8
Training loss: 2.3067498207092285
Validation loss: 2.097991327444712

Epoch: 6| Step: 9
Training loss: 2.0312929153442383
Validation loss: 2.0886698961257935

Epoch: 6| Step: 10
Training loss: 2.2019705772399902
Validation loss: 2.0785741607348123

Epoch: 6| Step: 11
Training loss: 1.793874740600586
Validation loss: 2.0778757532437644

Epoch: 6| Step: 12
Training loss: 2.051833152770996
Validation loss: 2.0956327517827353

Epoch: 6| Step: 13
Training loss: 2.039092540740967
Validation loss: 2.0865635673205056

Epoch: 190| Step: 0
Training loss: 1.6254692077636719
Validation loss: 2.0976056257883706

Epoch: 6| Step: 1
Training loss: 2.385021686553955
Validation loss: 2.098462621370951

Epoch: 6| Step: 2
Training loss: 1.7939565181732178
Validation loss: 2.0785153905550637

Epoch: 6| Step: 3
Training loss: 1.78805673122406
Validation loss: 2.096059799194336

Epoch: 6| Step: 4
Training loss: 1.9376407861709595
Validation loss: 2.0822322765986123

Epoch: 6| Step: 5
Training loss: 2.091209888458252
Validation loss: 2.0933394034703574

Epoch: 6| Step: 6
Training loss: 2.029236078262329
Validation loss: 2.091788669427236

Epoch: 6| Step: 7
Training loss: 1.443390130996704
Validation loss: 2.1064713398615518

Epoch: 6| Step: 8
Training loss: 1.96283757686615
Validation loss: 2.110177000363668

Epoch: 6| Step: 9
Training loss: 1.8188974857330322
Validation loss: 2.1195213993390403

Epoch: 6| Step: 10
Training loss: 2.2254130840301514
Validation loss: 2.118611693382263

Epoch: 6| Step: 11
Training loss: 2.4081695079803467
Validation loss: 2.1228000720342

Epoch: 6| Step: 12
Training loss: 1.4659785032272339
Validation loss: 2.1322598854700723

Epoch: 6| Step: 13
Training loss: 2.830042839050293
Validation loss: 2.1508854627609253

Epoch: 191| Step: 0
Training loss: 1.773561954498291
Validation loss: 2.1469061970710754

Epoch: 6| Step: 1
Training loss: 2.66377592086792
Validation loss: 2.1362732450167337

Epoch: 6| Step: 2
Training loss: 1.6908901929855347
Validation loss: 2.1090320547421775

Epoch: 6| Step: 3
Training loss: 1.9786728620529175
Validation loss: 2.122216363747915

Epoch: 6| Step: 4
Training loss: 1.8866050243377686
Validation loss: 2.12621146440506

Epoch: 6| Step: 5
Training loss: 2.4688661098480225
Validation loss: 2.131692667802175

Epoch: 6| Step: 6
Training loss: 1.8104264736175537
Validation loss: 2.134840448697408

Epoch: 6| Step: 7
Training loss: 1.5896952152252197
Validation loss: 2.1290653944015503

Epoch: 6| Step: 8
Training loss: 2.0298969745635986
Validation loss: 2.1279537081718445

Epoch: 6| Step: 9
Training loss: 2.4601492881774902
Validation loss: 2.1358121037483215

Epoch: 6| Step: 10
Training loss: 1.732266902923584
Validation loss: 2.1199870705604553

Epoch: 6| Step: 11
Training loss: 1.8363170623779297
Validation loss: 2.1298715472221375

Epoch: 6| Step: 12
Training loss: 1.6030651330947876
Validation loss: 2.1342005530993142

Epoch: 6| Step: 13
Training loss: 1.6933764219284058
Validation loss: 2.1153636972109475

Epoch: 192| Step: 0
Training loss: 1.8783471584320068
Validation loss: 2.0924813747406006

Epoch: 6| Step: 1
Training loss: 1.9613884687423706
Validation loss: 2.106164495150248

Epoch: 6| Step: 2
Training loss: 1.7995588779449463
Validation loss: 2.0840954780578613

Epoch: 6| Step: 3
Training loss: 2.368499279022217
Validation loss: 2.093889852364858

Epoch: 6| Step: 4
Training loss: 1.6716268062591553
Validation loss: 2.095392942428589

Epoch: 6| Step: 5
Training loss: 1.8763165473937988
Validation loss: 2.1094394524892173

Epoch: 6| Step: 6
Training loss: 2.148862361907959
Validation loss: 2.103311240673065

Epoch: 6| Step: 7
Training loss: 2.297765016555786
Validation loss: 2.1038549542427063

Epoch: 6| Step: 8
Training loss: 2.0617835521698
Validation loss: 2.095666686693827

Epoch: 6| Step: 9
Training loss: 2.5462188720703125
Validation loss: 2.0970062812169394

Epoch: 6| Step: 10
Training loss: 2.029280424118042
Validation loss: 2.1063170035680137

Epoch: 6| Step: 11
Training loss: 2.121372938156128
Validation loss: 2.1192213694254556

Epoch: 6| Step: 12
Training loss: 1.8097989559173584
Validation loss: 2.0907290975252786

Epoch: 6| Step: 13
Training loss: 1.4915835857391357
Validation loss: 2.0915017127990723

Epoch: 193| Step: 0
Training loss: 2.0345709323883057
Validation loss: 2.1056124766667685

Epoch: 6| Step: 1
Training loss: 1.9945615530014038
Validation loss: 2.1258525053660073

Epoch: 6| Step: 2
Training loss: 2.5399386882781982
Validation loss: 2.0931379199028015

Epoch: 6| Step: 3
Training loss: 2.0660250186920166
Validation loss: 2.119135399659475

Epoch: 6| Step: 4
Training loss: 2.004436492919922
Validation loss: 2.1130523085594177

Epoch: 6| Step: 5
Training loss: 2.066596508026123
Validation loss: 2.1010165015856423

Epoch: 6| Step: 6
Training loss: 1.8453716039657593
Validation loss: 2.123201290766398

Epoch: 6| Step: 7
Training loss: 2.4498488903045654
Validation loss: 2.1206003228823342

Epoch: 6| Step: 8
Training loss: 1.9438496828079224
Validation loss: 2.125370522340139

Epoch: 6| Step: 9
Training loss: 1.8596608638763428
Validation loss: 2.1121869484583535

Epoch: 6| Step: 10
Training loss: 2.3723607063293457
Validation loss: 2.111694077650706

Epoch: 6| Step: 11
Training loss: 1.3823528289794922
Validation loss: 2.1096267104148865

Epoch: 6| Step: 12
Training loss: 1.5317860841751099
Validation loss: 2.1128421227137246

Epoch: 6| Step: 13
Training loss: 1.7892889976501465
Validation loss: 2.139728089173635

Epoch: 194| Step: 0
Training loss: 2.2891101837158203
Validation loss: 2.1523436109224954

Epoch: 6| Step: 1
Training loss: 2.573312282562256
Validation loss: 2.162580966949463

Epoch: 6| Step: 2
Training loss: 1.97881019115448
Validation loss: 2.158555269241333

Epoch: 6| Step: 3
Training loss: 1.4699838161468506
Validation loss: 2.1545693079630532

Epoch: 6| Step: 4
Training loss: 1.8695486783981323
Validation loss: 2.1139572858810425

Epoch: 6| Step: 5
Training loss: 1.6023247241973877
Validation loss: 2.1146758794784546

Epoch: 6| Step: 6
Training loss: 1.98348867893219
Validation loss: 2.116564095020294

Epoch: 6| Step: 7
Training loss: 1.7113151550292969
Validation loss: 2.107709209124247

Epoch: 6| Step: 8
Training loss: 1.9109796285629272
Validation loss: 2.1070608496665955

Epoch: 6| Step: 9
Training loss: 2.4346165657043457
Validation loss: 2.096771478652954

Epoch: 6| Step: 10
Training loss: 2.219269275665283
Validation loss: 2.107506354649862

Epoch: 6| Step: 11
Training loss: 2.6027672290802
Validation loss: 2.094869097073873

Epoch: 6| Step: 12
Training loss: 1.9365122318267822
Validation loss: 2.1152488787968955

Epoch: 6| Step: 13
Training loss: 2.0770103931427
Validation loss: 2.132367114226023

Epoch: 195| Step: 0
Training loss: 1.9276738166809082
Validation loss: 2.138007899125417

Epoch: 6| Step: 1
Training loss: 2.3360581398010254
Validation loss: 2.1232076287269592

Epoch: 6| Step: 2
Training loss: 1.976745367050171
Validation loss: 2.149403472741445

Epoch: 6| Step: 3
Training loss: 2.7051262855529785
Validation loss: 2.146440108617147

Epoch: 6| Step: 4
Training loss: 2.4444751739501953
Validation loss: 2.1427810192108154

Epoch: 6| Step: 5
Training loss: 1.5326004028320312
Validation loss: 2.145340621471405

Epoch: 6| Step: 6
Training loss: 0.944083571434021
Validation loss: 2.1455363035202026

Epoch: 6| Step: 7
Training loss: 2.402365207672119
Validation loss: 2.146016240119934

Epoch: 6| Step: 8
Training loss: 2.0495519638061523
Validation loss: 2.1091098388036094

Epoch: 6| Step: 9
Training loss: 2.080371856689453
Validation loss: 2.116426090399424

Epoch: 6| Step: 10
Training loss: 2.2694506645202637
Validation loss: 2.1139785846074424

Epoch: 6| Step: 11
Training loss: 1.5730369091033936
Validation loss: 2.1147679885228476

Epoch: 6| Step: 12
Training loss: 1.6920086145401
Validation loss: 2.1103562315305076

Epoch: 6| Step: 13
Training loss: 1.6094567775726318
Validation loss: 2.1124664545059204

Epoch: 196| Step: 0
Training loss: 2.744464159011841
Validation loss: 2.1298153400421143

Epoch: 6| Step: 1
Training loss: 1.8679149150848389
Validation loss: 2.128655175367991

Epoch: 6| Step: 2
Training loss: 2.0382606983184814
Validation loss: 2.137481451034546

Epoch: 6| Step: 3
Training loss: 1.9106543064117432
Validation loss: 2.1510022481282554

Epoch: 6| Step: 4
Training loss: 2.293874979019165
Validation loss: 2.1560444434483848

Epoch: 6| Step: 5
Training loss: 1.6170902252197266
Validation loss: 2.2012956937154136

Epoch: 6| Step: 6
Training loss: 2.0810112953186035
Validation loss: 2.199548363685608

Epoch: 6| Step: 7
Training loss: 1.4530779123306274
Validation loss: 2.184656242529551

Epoch: 6| Step: 8
Training loss: 1.273669958114624
Validation loss: 2.197441120942434

Epoch: 6| Step: 9
Training loss: 2.1182518005371094
Validation loss: 2.1394349336624146

Epoch: 6| Step: 10
Training loss: 1.7488584518432617
Validation loss: 2.149135728677114

Epoch: 6| Step: 11
Training loss: 1.5213619470596313
Validation loss: 2.117796699206034

Epoch: 6| Step: 12
Training loss: 2.8479111194610596
Validation loss: 2.110586663087209

Epoch: 6| Step: 13
Training loss: 1.998637080192566
Validation loss: 2.120533009370168

Epoch: 197| Step: 0
Training loss: 1.960526704788208
Validation loss: 2.098411480585734

Epoch: 6| Step: 1
Training loss: 1.8072673082351685
Validation loss: 2.1062488555908203

Epoch: 6| Step: 2
Training loss: 1.9585392475128174
Validation loss: 2.09850537776947

Epoch: 6| Step: 3
Training loss: 2.1216177940368652
Validation loss: 2.107264836629232

Epoch: 6| Step: 4
Training loss: 2.592296838760376
Validation loss: 2.1076352993647256

Epoch: 6| Step: 5
Training loss: 2.025439739227295
Validation loss: 2.103072941303253

Epoch: 6| Step: 6
Training loss: 1.5270369052886963
Validation loss: 2.1016829212506614

Epoch: 6| Step: 7
Training loss: 1.921479344367981
Validation loss: 2.108297824859619

Epoch: 6| Step: 8
Training loss: 2.14573335647583
Validation loss: 2.103051761786143

Epoch: 6| Step: 9
Training loss: 2.6938514709472656
Validation loss: 2.0940610567728677

Epoch: 6| Step: 10
Training loss: 1.8484618663787842
Validation loss: 2.096598188082377

Epoch: 6| Step: 11
Training loss: 1.9450957775115967
Validation loss: 2.098935882250468

Epoch: 6| Step: 12
Training loss: 2.3325588703155518
Validation loss: 2.1053048173586526

Epoch: 6| Step: 13
Training loss: 1.7639023065567017
Validation loss: 2.113351583480835

Epoch: 198| Step: 0
Training loss: 1.7317063808441162
Validation loss: 2.1257765889167786

Epoch: 6| Step: 1
Training loss: 2.2367160320281982
Validation loss: 2.1145931482315063

Epoch: 6| Step: 2
Training loss: 2.08320951461792
Validation loss: 2.121444821357727

Epoch: 6| Step: 3
Training loss: 1.5234044790267944
Validation loss: 2.12907079855601

Epoch: 6| Step: 4
Training loss: 1.3644558191299438
Validation loss: 2.126515527566274

Epoch: 6| Step: 5
Training loss: 1.7389917373657227
Validation loss: 2.1504352490107217

Epoch: 6| Step: 6
Training loss: 2.0843851566314697
Validation loss: 2.1455984910329184

Epoch: 6| Step: 7
Training loss: 2.1528167724609375
Validation loss: 2.1566854317982993

Epoch: 6| Step: 8
Training loss: 2.0625624656677246
Validation loss: 2.1497933665911355

Epoch: 6| Step: 9
Training loss: 1.4965740442276
Validation loss: 2.1425371368726096

Epoch: 6| Step: 10
Training loss: 2.2883195877075195
Validation loss: 2.137237270673116

Epoch: 6| Step: 11
Training loss: 2.093039035797119
Validation loss: 2.152246415615082

Epoch: 6| Step: 12
Training loss: 2.0507442951202393
Validation loss: 2.146559953689575

Epoch: 6| Step: 13
Training loss: 2.151292562484741
Validation loss: 2.1416120330492654

Epoch: 199| Step: 0
Training loss: 2.060229778289795
Validation loss: 2.1462088227272034

Epoch: 6| Step: 1
Training loss: 2.778204917907715
Validation loss: 2.1287394364674888

Epoch: 6| Step: 2
Training loss: 2.0682854652404785
Validation loss: 2.1277838150660195

Epoch: 6| Step: 3
Training loss: 1.5925941467285156
Validation loss: 2.1279077529907227

Epoch: 6| Step: 4
Training loss: 1.6363725662231445
Validation loss: 2.13032458225886

Epoch: 6| Step: 5
Training loss: 1.611061453819275
Validation loss: 2.1384973923365274

Epoch: 6| Step: 6
Training loss: 1.793421745300293
Validation loss: 2.148059884707133

Epoch: 6| Step: 7
Training loss: 1.4445466995239258
Validation loss: 2.1411118706067405

Epoch: 6| Step: 8
Training loss: 1.344433069229126
Validation loss: 2.1611477533976235

Epoch: 6| Step: 9
Training loss: 2.399653911590576
Validation loss: 2.136846125125885

Epoch: 6| Step: 10
Training loss: 1.9106378555297852
Validation loss: 2.14817347129186

Epoch: 6| Step: 11
Training loss: 1.880338430404663
Validation loss: 2.17663836479187

Epoch: 6| Step: 12
Training loss: 2.470137596130371
Validation loss: 2.1511069536209106

Epoch: 6| Step: 13
Training loss: 2.0063419342041016
Validation loss: 2.150213360786438

Epoch: 200| Step: 0
Training loss: 1.824469804763794
Validation loss: 2.1354651848475137

Epoch: 6| Step: 1
Training loss: 2.083345413208008
Validation loss: 2.149874230225881

Epoch: 6| Step: 2
Training loss: 1.5891389846801758
Validation loss: 2.1243416468302407

Epoch: 6| Step: 3
Training loss: 1.4611806869506836
Validation loss: 2.1239206989606223

Epoch: 6| Step: 4
Training loss: 1.7276110649108887
Validation loss: 2.131017804145813

Epoch: 6| Step: 5
Training loss: 1.7866010665893555
Validation loss: 2.1371466716130576

Epoch: 6| Step: 6
Training loss: 2.9228599071502686
Validation loss: 2.1322719852129617

Epoch: 6| Step: 7
Training loss: 2.648977756500244
Validation loss: 2.129240393638611

Epoch: 6| Step: 8
Training loss: 1.6341966390609741
Validation loss: 2.1340664426485696

Epoch: 6| Step: 9
Training loss: 1.4513802528381348
Validation loss: 2.122070610523224

Epoch: 6| Step: 10
Training loss: 2.081406593322754
Validation loss: 2.136270821094513

Epoch: 6| Step: 11
Training loss: 2.034672737121582
Validation loss: 2.136117100715637

Epoch: 6| Step: 12
Training loss: 1.7613166570663452
Validation loss: 2.170436521371206

Epoch: 6| Step: 13
Training loss: 2.199958086013794
Validation loss: 2.1527065436045327

Epoch: 201| Step: 0
Training loss: 1.8264966011047363
Validation loss: 2.1376485427220664

Epoch: 6| Step: 1
Training loss: 1.9621682167053223
Validation loss: 2.1340325276056924

Epoch: 6| Step: 2
Training loss: 1.7483595609664917
Validation loss: 2.1351462801297507

Epoch: 6| Step: 3
Training loss: 1.58115816116333
Validation loss: 2.1301220258076987

Epoch: 6| Step: 4
Training loss: 1.4941684007644653
Validation loss: 2.115320324897766

Epoch: 6| Step: 5
Training loss: 2.5850424766540527
Validation loss: 2.140353878339132

Epoch: 6| Step: 6
Training loss: 1.6537343263626099
Validation loss: 2.116911828517914

Epoch: 6| Step: 7
Training loss: 2.0854554176330566
Validation loss: 2.116790990034739

Epoch: 6| Step: 8
Training loss: 1.6666624546051025
Validation loss: 2.134189486503601

Epoch: 6| Step: 9
Training loss: 2.21561598777771
Validation loss: 2.130266626675924

Epoch: 6| Step: 10
Training loss: 2.0453102588653564
Validation loss: 2.124058485031128

Epoch: 6| Step: 11
Training loss: 1.7556110620498657
Validation loss: 2.1311103900273642

Epoch: 6| Step: 12
Training loss: 2.273959159851074
Validation loss: 2.126676877339681

Epoch: 6| Step: 13
Training loss: 1.8756588697433472
Validation loss: 2.1484641432762146

Epoch: 202| Step: 0
Training loss: 1.1042736768722534
Validation loss: 2.14600932598114

Epoch: 6| Step: 1
Training loss: 2.0233821868896484
Validation loss: 2.137775480747223

Epoch: 6| Step: 2
Training loss: 1.7874090671539307
Validation loss: 2.1344247261683145

Epoch: 6| Step: 3
Training loss: 1.882655382156372
Validation loss: 2.138263980547587

Epoch: 6| Step: 4
Training loss: 1.8451933860778809
Validation loss: 2.161391258239746

Epoch: 6| Step: 5
Training loss: 1.7559795379638672
Validation loss: 2.14548122882843

Epoch: 6| Step: 6
Training loss: 1.9550445079803467
Validation loss: 2.144780774911245

Epoch: 6| Step: 7
Training loss: 2.1016640663146973
Validation loss: 2.1462412675221763

Epoch: 6| Step: 8
Training loss: 2.5723981857299805
Validation loss: 2.164302965005239

Epoch: 6| Step: 9
Training loss: 1.9499900341033936
Validation loss: 2.15008944272995

Epoch: 6| Step: 10
Training loss: 1.5601505041122437
Validation loss: 2.126876433690389

Epoch: 6| Step: 11
Training loss: 2.331930637359619
Validation loss: 2.1432252128918967

Epoch: 6| Step: 12
Training loss: 1.6579352617263794
Validation loss: 2.126287599404653

Epoch: 6| Step: 13
Training loss: 2.1943256855010986
Validation loss: 2.1269622246424356

Epoch: 203| Step: 0
Training loss: 1.831714391708374
Validation loss: 2.116425077120463

Epoch: 6| Step: 1
Training loss: 1.6312501430511475
Validation loss: 2.119268755118052

Epoch: 6| Step: 2
Training loss: 2.6251955032348633
Validation loss: 2.1332195003827414

Epoch: 6| Step: 3
Training loss: 1.0801308155059814
Validation loss: 2.1339513063430786

Epoch: 6| Step: 4
Training loss: 2.1753950119018555
Validation loss: 2.159865697224935

Epoch: 6| Step: 5
Training loss: 1.7267296314239502
Validation loss: 2.152910888195038

Epoch: 6| Step: 6
Training loss: 1.6496795415878296
Validation loss: 2.1737547318140664

Epoch: 6| Step: 7
Training loss: 2.3037779331207275
Validation loss: 2.1521807511647544

Epoch: 6| Step: 8
Training loss: 1.706087350845337
Validation loss: 2.1694284876187644

Epoch: 6| Step: 9
Training loss: 2.356877326965332
Validation loss: 2.1572252909342446

Epoch: 6| Step: 10
Training loss: 1.9251806735992432
Validation loss: 2.157603641351064

Epoch: 6| Step: 11
Training loss: 1.9443777799606323
Validation loss: 2.1571162740389505

Epoch: 6| Step: 12
Training loss: 2.032559394836426
Validation loss: 2.141950090726217

Epoch: 6| Step: 13
Training loss: 2.017665386199951
Validation loss: 2.1480729977289834

Epoch: 204| Step: 0
Training loss: 2.1380624771118164
Validation loss: 2.1411728858947754

Epoch: 6| Step: 1
Training loss: 2.4088845252990723
Validation loss: 2.140979588031769

Epoch: 6| Step: 2
Training loss: 1.575169324874878
Validation loss: 2.162819226582845

Epoch: 6| Step: 3
Training loss: 1.9451749324798584
Validation loss: 2.1701839764912925

Epoch: 6| Step: 4
Training loss: 1.9777719974517822
Validation loss: 2.1601580580075583

Epoch: 6| Step: 5
Training loss: 1.7379226684570312
Validation loss: 2.1868183414141336

Epoch: 6| Step: 6
Training loss: 1.8267951011657715
Validation loss: 2.1778690616289773

Epoch: 6| Step: 7
Training loss: 2.3142971992492676
Validation loss: 2.158474882443746

Epoch: 6| Step: 8
Training loss: 1.5322060585021973
Validation loss: 2.163124442100525

Epoch: 6| Step: 9
Training loss: 1.4897558689117432
Validation loss: 2.143236597379049

Epoch: 6| Step: 10
Training loss: 1.7547662258148193
Validation loss: 2.138676345348358

Epoch: 6| Step: 11
Training loss: 2.3408212661743164
Validation loss: 2.1427019437154136

Epoch: 6| Step: 12
Training loss: 1.5891646146774292
Validation loss: 2.155079265435537

Epoch: 6| Step: 13
Training loss: 2.2730679512023926
Validation loss: 2.1525245110193887

Epoch: 205| Step: 0
Training loss: 1.664731502532959
Validation loss: 2.147179206212362

Epoch: 6| Step: 1
Training loss: 1.2948377132415771
Validation loss: 2.17106294631958

Epoch: 6| Step: 2
Training loss: 1.9878284931182861
Validation loss: 2.148773451646169

Epoch: 6| Step: 3
Training loss: 1.5332047939300537
Validation loss: 2.1545822819073996

Epoch: 6| Step: 4
Training loss: 2.136737823486328
Validation loss: 2.1593210697174072

Epoch: 6| Step: 5
Training loss: 1.9905732870101929
Validation loss: 2.1487640738487244

Epoch: 6| Step: 6
Training loss: 2.11557674407959
Validation loss: 2.151060918966929

Epoch: 6| Step: 7
Training loss: 1.9620252847671509
Validation loss: 2.1497695446014404

Epoch: 6| Step: 8
Training loss: 1.662280559539795
Validation loss: 2.1625080506006875

Epoch: 6| Step: 9
Training loss: 1.4038677215576172
Validation loss: 2.198976159095764

Epoch: 6| Step: 10
Training loss: 2.0663366317749023
Validation loss: 2.1881102323532104

Epoch: 6| Step: 11
Training loss: 2.7699551582336426
Validation loss: 2.2072805960973105

Epoch: 6| Step: 12
Training loss: 2.1397571563720703
Validation loss: 2.176436106363932

Epoch: 6| Step: 13
Training loss: 2.2292251586914062
Validation loss: 2.1621057589848838

Epoch: 206| Step: 0
Training loss: 2.2627763748168945
Validation loss: 2.1612806916236877

Epoch: 6| Step: 1
Training loss: 2.2717268466949463
Validation loss: 2.158454438050588

Epoch: 6| Step: 2
Training loss: 1.6227152347564697
Validation loss: 2.1365750829378762

Epoch: 6| Step: 3
Training loss: 2.108860492706299
Validation loss: 2.1388854384422302

Epoch: 6| Step: 4
Training loss: 2.0366029739379883
Validation loss: 2.140462060769399

Epoch: 6| Step: 5
Training loss: 2.576348304748535
Validation loss: 2.145685315132141

Epoch: 6| Step: 6
Training loss: 1.8146941661834717
Validation loss: 2.142709235350291

Epoch: 6| Step: 7
Training loss: 1.8030589818954468
Validation loss: 2.131617625554403

Epoch: 6| Step: 8
Training loss: 1.3652859926223755
Validation loss: 2.148063282171885

Epoch: 6| Step: 9
Training loss: 1.8114473819732666
Validation loss: 2.1804176966349282

Epoch: 6| Step: 10
Training loss: 1.8552337884902954
Validation loss: 2.1406623919804892

Epoch: 6| Step: 11
Training loss: 2.127114772796631
Validation loss: 2.1675949494043985

Epoch: 6| Step: 12
Training loss: 1.6114392280578613
Validation loss: 2.1548804442087808

Epoch: 6| Step: 13
Training loss: 2.058169364929199
Validation loss: 2.1502155462900796

Epoch: 207| Step: 0
Training loss: 1.7794665098190308
Validation loss: 2.1558019121487937

Epoch: 6| Step: 1
Training loss: 1.4942870140075684
Validation loss: 2.155882259209951

Epoch: 6| Step: 2
Training loss: 2.257689952850342
Validation loss: 2.171459913253784

Epoch: 6| Step: 3
Training loss: 1.2233085632324219
Validation loss: 2.1610052784283957

Epoch: 6| Step: 4
Training loss: 1.3511724472045898
Validation loss: 2.1401139895121255

Epoch: 6| Step: 5
Training loss: 1.9177486896514893
Validation loss: 2.1571272015571594

Epoch: 6| Step: 6
Training loss: 1.9998648166656494
Validation loss: 2.151238958040873

Epoch: 6| Step: 7
Training loss: 1.8697792291641235
Validation loss: 2.15351136525472

Epoch: 6| Step: 8
Training loss: 2.334240436553955
Validation loss: 2.144946336746216

Epoch: 6| Step: 9
Training loss: 1.488405704498291
Validation loss: 2.15117210149765

Epoch: 6| Step: 10
Training loss: 2.711463451385498
Validation loss: 2.1416186094284058

Epoch: 6| Step: 11
Training loss: 2.2512388229370117
Validation loss: 2.135731299718221

Epoch: 6| Step: 12
Training loss: 1.7994823455810547
Validation loss: 2.1235769391059875

Epoch: 6| Step: 13
Training loss: 2.0516676902770996
Validation loss: 2.1381946206092834

Epoch: 208| Step: 0
Training loss: 1.7918857336044312
Validation loss: 2.1400253971417746

Epoch: 6| Step: 1
Training loss: 2.1964030265808105
Validation loss: 2.1244367361068726

Epoch: 6| Step: 2
Training loss: 1.7128331661224365
Validation loss: 2.138674557209015

Epoch: 6| Step: 3
Training loss: 1.8281850814819336
Validation loss: 2.1463033159573874

Epoch: 6| Step: 4
Training loss: 2.327113151550293
Validation loss: 2.136752506097158

Epoch: 6| Step: 5
Training loss: 1.1371817588806152
Validation loss: 2.1522028048833213

Epoch: 6| Step: 6
Training loss: 2.1192502975463867
Validation loss: 2.130029857158661

Epoch: 6| Step: 7
Training loss: 1.7676929235458374
Validation loss: 2.1504706144332886

Epoch: 6| Step: 8
Training loss: 2.414285659790039
Validation loss: 2.150584101676941

Epoch: 6| Step: 9
Training loss: 1.756020426750183
Validation loss: 2.16048397620519

Epoch: 6| Step: 10
Training loss: 1.8869843482971191
Validation loss: 2.1640102863311768

Epoch: 6| Step: 11
Training loss: 1.963881492614746
Validation loss: 2.174225131670634

Epoch: 6| Step: 12
Training loss: 2.027387857437134
Validation loss: 2.1351977984110513

Epoch: 6| Step: 13
Training loss: 1.416243076324463
Validation loss: 2.181615094343821

Epoch: 209| Step: 0
Training loss: 1.7318720817565918
Validation loss: 2.171849807103475

Epoch: 6| Step: 1
Training loss: 1.674834966659546
Validation loss: 2.155173381169637

Epoch: 6| Step: 2
Training loss: 2.3024039268493652
Validation loss: 2.142370561758677

Epoch: 6| Step: 3
Training loss: 1.3546860218048096
Validation loss: 2.1381170550982156

Epoch: 6| Step: 4
Training loss: 1.5477495193481445
Validation loss: 2.1447388927141824

Epoch: 6| Step: 5
Training loss: 2.0472412109375
Validation loss: 2.1574366092681885

Epoch: 6| Step: 6
Training loss: 1.6355030536651611
Validation loss: 2.147686541080475

Epoch: 6| Step: 7
Training loss: 2.570180892944336
Validation loss: 2.159791111946106

Epoch: 6| Step: 8
Training loss: 2.136361598968506
Validation loss: 2.177570164203644

Epoch: 6| Step: 9
Training loss: 1.9844390153884888
Validation loss: 2.183128853638967

Epoch: 6| Step: 10
Training loss: 1.9834942817687988
Validation loss: 2.183736781279246

Epoch: 6| Step: 11
Training loss: 1.889055848121643
Validation loss: 2.2127259373664856

Epoch: 6| Step: 12
Training loss: 1.6566758155822754
Validation loss: 2.1818366249402366

Epoch: 6| Step: 13
Training loss: 2.5885281562805176
Validation loss: 2.19289759794871

Epoch: 210| Step: 0
Training loss: 1.5450481176376343
Validation loss: 2.1688586473464966

Epoch: 6| Step: 1
Training loss: 2.032665252685547
Validation loss: 2.2086828152338662

Epoch: 6| Step: 2
Training loss: 1.949220895767212
Validation loss: 2.175632099310557

Epoch: 6| Step: 3
Training loss: 1.6608525514602661
Validation loss: 2.182418088118235

Epoch: 6| Step: 4
Training loss: 1.8527467250823975
Validation loss: 2.1702161033948264

Epoch: 6| Step: 5
Training loss: 1.9980288743972778
Validation loss: 2.178786357243856

Epoch: 6| Step: 6
Training loss: 2.4939239025115967
Validation loss: 2.1706859270731607

Epoch: 6| Step: 7
Training loss: 1.8203461170196533
Validation loss: 2.148517886797587

Epoch: 6| Step: 8
Training loss: 2.383647918701172
Validation loss: 2.1622493465741477

Epoch: 6| Step: 9
Training loss: 1.8794958591461182
Validation loss: 2.1365261475245156

Epoch: 6| Step: 10
Training loss: 1.3387303352355957
Validation loss: 2.1465808153152466

Epoch: 6| Step: 11
Training loss: 2.0846657752990723
Validation loss: 2.1574947039286294

Epoch: 6| Step: 12
Training loss: 1.6021291017532349
Validation loss: 2.1503769357999167

Epoch: 6| Step: 13
Training loss: 2.2238826751708984
Validation loss: 2.162694573402405

Epoch: 211| Step: 0
Training loss: 1.5948076248168945
Validation loss: 2.150193174680074

Epoch: 6| Step: 1
Training loss: 1.9939810037612915
Validation loss: 2.1573387384414673

Epoch: 6| Step: 2
Training loss: 1.8365346193313599
Validation loss: 2.1556419730186462

Epoch: 6| Step: 3
Training loss: 1.5455068349838257
Validation loss: 2.15466046333313

Epoch: 6| Step: 4
Training loss: 1.6376508474349976
Validation loss: 2.1529742081960044

Epoch: 6| Step: 5
Training loss: 2.2314963340759277
Validation loss: 2.1582886576652527

Epoch: 6| Step: 6
Training loss: 1.684807300567627
Validation loss: 2.138456881046295

Epoch: 6| Step: 7
Training loss: 1.859371542930603
Validation loss: 2.1637871066729226

Epoch: 6| Step: 8
Training loss: 1.506929636001587
Validation loss: 2.1425349911053977

Epoch: 6| Step: 9
Training loss: 1.8903828859329224
Validation loss: 2.1600763400395713

Epoch: 6| Step: 10
Training loss: 2.4960315227508545
Validation loss: 2.1712356408437095

Epoch: 6| Step: 11
Training loss: 2.366943359375
Validation loss: 2.202591856320699

Epoch: 6| Step: 12
Training loss: 2.0075440406799316
Validation loss: 2.1958659887313843

Epoch: 6| Step: 13
Training loss: 2.1141138076782227
Validation loss: 2.1889162063598633

Epoch: 212| Step: 0
Training loss: 2.048112392425537
Validation loss: 2.206265131632487

Epoch: 6| Step: 1
Training loss: 1.669997215270996
Validation loss: 2.19411830107371

Epoch: 6| Step: 2
Training loss: 1.8919086456298828
Validation loss: 2.1802180210749307

Epoch: 6| Step: 3
Training loss: 1.6173036098480225
Validation loss: 2.1956658562024436

Epoch: 6| Step: 4
Training loss: 2.2104439735412598
Validation loss: 2.1778743863105774

Epoch: 6| Step: 5
Training loss: 1.6789307594299316
Validation loss: 2.1761178771654763

Epoch: 6| Step: 6
Training loss: 1.9214181900024414
Validation loss: 2.145501693089803

Epoch: 6| Step: 7
Training loss: 2.238884925842285
Validation loss: 2.186743895212809

Epoch: 6| Step: 8
Training loss: 2.0122790336608887
Validation loss: 2.16291751464208

Epoch: 6| Step: 9
Training loss: 1.9257807731628418
Validation loss: 2.1507043838500977

Epoch: 6| Step: 10
Training loss: 1.8308932781219482
Validation loss: 2.1503966450691223

Epoch: 6| Step: 11
Training loss: 1.8400346040725708
Validation loss: 2.1358360052108765

Epoch: 6| Step: 12
Training loss: 1.7761255502700806
Validation loss: 2.1345628102620444

Epoch: 6| Step: 13
Training loss: 1.719425082206726
Validation loss: 2.1303699016571045

Epoch: 213| Step: 0
Training loss: 2.3074874877929688
Validation loss: 2.1501399079958596

Epoch: 6| Step: 1
Training loss: 2.088549852371216
Validation loss: 2.141060988108317

Epoch: 6| Step: 2
Training loss: 2.5196847915649414
Validation loss: 2.140864849090576

Epoch: 6| Step: 3
Training loss: 2.218029499053955
Validation loss: 2.163005530834198

Epoch: 6| Step: 4
Training loss: 1.6687366962432861
Validation loss: 2.1496528585751853

Epoch: 6| Step: 5
Training loss: 1.6503329277038574
Validation loss: 2.1521047949790955

Epoch: 6| Step: 6
Training loss: 1.5498101711273193
Validation loss: 2.130242884159088

Epoch: 6| Step: 7
Training loss: 1.5610352754592896
Validation loss: 2.112884302934011

Epoch: 6| Step: 8
Training loss: 1.5585951805114746
Validation loss: 2.115760783354441

Epoch: 6| Step: 9
Training loss: 1.8360989093780518
Validation loss: 2.1160375674565635

Epoch: 6| Step: 10
Training loss: 1.912048101425171
Validation loss: 2.1264055569966636

Epoch: 6| Step: 11
Training loss: 1.8764538764953613
Validation loss: 2.124018609523773

Epoch: 6| Step: 12
Training loss: 2.063009262084961
Validation loss: 2.121527850627899

Epoch: 6| Step: 13
Training loss: 2.101470470428467
Validation loss: 2.13676651318868

Epoch: 214| Step: 0
Training loss: 2.3469271659851074
Validation loss: 2.1659944454828897

Epoch: 6| Step: 1
Training loss: 2.382066249847412
Validation loss: 2.1590274572372437

Epoch: 6| Step: 2
Training loss: 1.5589344501495361
Validation loss: 2.1402918100357056

Epoch: 6| Step: 3
Training loss: 1.7741352319717407
Validation loss: 2.168923099835714

Epoch: 6| Step: 4
Training loss: 1.914871335029602
Validation loss: 2.1678887605667114

Epoch: 6| Step: 5
Training loss: 1.6026349067687988
Validation loss: 2.170307437578837

Epoch: 6| Step: 6
Training loss: 2.461029529571533
Validation loss: 2.1875753005345664

Epoch: 6| Step: 7
Training loss: 2.0414719581604004
Validation loss: 2.175331393877665

Epoch: 6| Step: 8
Training loss: 2.159090518951416
Validation loss: 2.1724841594696045

Epoch: 6| Step: 9
Training loss: 1.498896837234497
Validation loss: 2.1636225978533425

Epoch: 6| Step: 10
Training loss: 2.1870017051696777
Validation loss: 2.190603951613108

Epoch: 6| Step: 11
Training loss: 1.8302321434020996
Validation loss: 2.171048363049825

Epoch: 6| Step: 12
Training loss: 1.3014535903930664
Validation loss: 2.189082443714142

Epoch: 6| Step: 13
Training loss: 1.4227406978607178
Validation loss: 2.167751729488373

Epoch: 215| Step: 0
Training loss: 1.4752142429351807
Validation loss: 2.186885337034861

Epoch: 6| Step: 1
Training loss: 1.9063328504562378
Validation loss: 2.1937562425931296

Epoch: 6| Step: 2
Training loss: 2.0346314907073975
Validation loss: 2.1868634025255838

Epoch: 6| Step: 3
Training loss: 1.7270740270614624
Validation loss: 2.194784720738729

Epoch: 6| Step: 4
Training loss: 1.717904806137085
Validation loss: 2.191348691781362

Epoch: 6| Step: 5
Training loss: 1.9856581687927246
Validation loss: 2.184873123963674

Epoch: 6| Step: 6
Training loss: 1.6589621305465698
Validation loss: 2.1858731706937156

Epoch: 6| Step: 7
Training loss: 1.5986096858978271
Validation loss: 2.194180170694987

Epoch: 6| Step: 8
Training loss: 2.931889057159424
Validation loss: 2.1788240472475686

Epoch: 6| Step: 9
Training loss: 2.2643256187438965
Validation loss: 2.1693596839904785

Epoch: 6| Step: 10
Training loss: 1.9643285274505615
Validation loss: 2.16516383488973

Epoch: 6| Step: 11
Training loss: 1.775122046470642
Validation loss: 2.1617716352144876

Epoch: 6| Step: 12
Training loss: 1.4298123121261597
Validation loss: 2.150005062421163

Epoch: 6| Step: 13
Training loss: 1.606812596321106
Validation loss: 2.1506402293841043

Epoch: 216| Step: 0
Training loss: 1.8941062688827515
Validation loss: 2.1316553950309753

Epoch: 6| Step: 1
Training loss: 1.6835136413574219
Validation loss: 2.1247741977373757

Epoch: 6| Step: 2
Training loss: 2.9096179008483887
Validation loss: 2.130409379800161

Epoch: 6| Step: 3
Training loss: 2.2292299270629883
Validation loss: 2.123384257157644

Epoch: 6| Step: 4
Training loss: 1.710161805152893
Validation loss: 2.12382843097051

Epoch: 6| Step: 5
Training loss: 1.597172737121582
Validation loss: 2.131814877192179

Epoch: 6| Step: 6
Training loss: 2.231217861175537
Validation loss: 2.1380567153294883

Epoch: 6| Step: 7
Training loss: 2.4864730834960938
Validation loss: 2.1478805343310037

Epoch: 6| Step: 8
Training loss: 1.8009190559387207
Validation loss: 2.167493780454

Epoch: 6| Step: 9
Training loss: 1.5151195526123047
Validation loss: 2.186405042807261

Epoch: 6| Step: 10
Training loss: 1.7826101779937744
Validation loss: 2.1827036142349243

Epoch: 6| Step: 11
Training loss: 1.716825008392334
Validation loss: 2.2033321062723794

Epoch: 6| Step: 12
Training loss: 2.319688320159912
Validation loss: 2.2003347078959146

Epoch: 6| Step: 13
Training loss: 1.573149561882019
Validation loss: 2.1886234879493713

Epoch: 217| Step: 0
Training loss: 2.4610435962677
Validation loss: 2.1778385837872825

Epoch: 6| Step: 1
Training loss: 1.8206279277801514
Validation loss: 2.189312001069387

Epoch: 6| Step: 2
Training loss: 1.344504475593567
Validation loss: 2.1938621203104653

Epoch: 6| Step: 3
Training loss: 1.7547531127929688
Validation loss: 2.152007440725962

Epoch: 6| Step: 4
Training loss: 1.9853146076202393
Validation loss: 2.133854548136393

Epoch: 6| Step: 5
Training loss: 1.3011162281036377
Validation loss: 2.1553120414415994

Epoch: 6| Step: 6
Training loss: 2.359417200088501
Validation loss: 2.1566657225290933

Epoch: 6| Step: 7
Training loss: 2.2825002670288086
Validation loss: 2.1321219007174173

Epoch: 6| Step: 8
Training loss: 1.4669468402862549
Validation loss: 2.1521723667780557

Epoch: 6| Step: 9
Training loss: 2.256622314453125
Validation loss: 2.1548984050750732

Epoch: 6| Step: 10
Training loss: 2.316591262817383
Validation loss: 2.1656595269838967

Epoch: 6| Step: 11
Training loss: 2.080029010772705
Validation loss: 2.169901132583618

Epoch: 6| Step: 12
Training loss: 1.982241153717041
Validation loss: 2.1882410844167075

Epoch: 6| Step: 13
Training loss: 1.5523717403411865
Validation loss: 2.185515284538269

Epoch: 218| Step: 0
Training loss: 2.0607175827026367
Validation loss: 2.184939960638682

Epoch: 6| Step: 1
Training loss: 1.7268085479736328
Validation loss: 2.1724575757980347

Epoch: 6| Step: 2
Training loss: 1.7178823947906494
Validation loss: 2.144724686940511

Epoch: 6| Step: 3
Training loss: 2.8851370811462402
Validation loss: 2.1471301317214966

Epoch: 6| Step: 4
Training loss: 1.6619482040405273
Validation loss: 2.1178044080734253

Epoch: 6| Step: 5
Training loss: 2.014098644256592
Validation loss: 2.1363713145256042

Epoch: 6| Step: 6
Training loss: 2.2184224128723145
Validation loss: 2.130212048689524

Epoch: 6| Step: 7
Training loss: 1.8823037147521973
Validation loss: 2.1231690049171448

Epoch: 6| Step: 8
Training loss: 1.548565149307251
Validation loss: 2.113025705019633

Epoch: 6| Step: 9
Training loss: 1.9002363681793213
Validation loss: 2.1435158451398215

Epoch: 6| Step: 10
Training loss: 2.59503436088562
Validation loss: 2.129354457060496

Epoch: 6| Step: 11
Training loss: 1.7386846542358398
Validation loss: 2.14542164405187

Epoch: 6| Step: 12
Training loss: 1.4163341522216797
Validation loss: 2.1482967336972556

Epoch: 6| Step: 13
Training loss: 1.425855278968811
Validation loss: 2.137634555498759

Epoch: 219| Step: 0
Training loss: 2.211120367050171
Validation loss: 2.132927636305491

Epoch: 6| Step: 1
Training loss: 1.7475206851959229
Validation loss: 2.14354008436203

Epoch: 6| Step: 2
Training loss: 2.411966323852539
Validation loss: 2.1555864810943604

Epoch: 6| Step: 3
Training loss: 1.5206146240234375
Validation loss: 2.1545894145965576

Epoch: 6| Step: 4
Training loss: 2.751183032989502
Validation loss: 2.146274149417877

Epoch: 6| Step: 5
Training loss: 1.814664363861084
Validation loss: 2.15740430355072

Epoch: 6| Step: 6
Training loss: 2.079824924468994
Validation loss: 2.1564417282740274

Epoch: 6| Step: 7
Training loss: 2.069155216217041
Validation loss: 2.146108249823252

Epoch: 6| Step: 8
Training loss: 1.563915729522705
Validation loss: 2.147323250770569

Epoch: 6| Step: 9
Training loss: 0.9707475304603577
Validation loss: 2.1674108703931174

Epoch: 6| Step: 10
Training loss: 1.6178901195526123
Validation loss: 2.157410124937693

Epoch: 6| Step: 11
Training loss: 1.7897627353668213
Validation loss: 2.1642475525538125

Epoch: 6| Step: 12
Training loss: 1.7366323471069336
Validation loss: 2.184841195742289

Epoch: 6| Step: 13
Training loss: 1.8906266689300537
Validation loss: 2.1846195062001548

Epoch: 220| Step: 0
Training loss: 2.1315135955810547
Validation loss: 2.2071732680002847

Epoch: 6| Step: 1
Training loss: 1.4165037870407104
Validation loss: 2.1856884956359863

Epoch: 6| Step: 2
Training loss: 1.5256478786468506
Validation loss: 2.1965317726135254

Epoch: 6| Step: 3
Training loss: 1.9043537378311157
Validation loss: 2.205227474371592

Epoch: 6| Step: 4
Training loss: 1.763563871383667
Validation loss: 2.1942719221115112

Epoch: 6| Step: 5
Training loss: 1.6165215969085693
Validation loss: 2.196109930674235

Epoch: 6| Step: 6
Training loss: 2.5908546447753906
Validation loss: 2.193657696247101

Epoch: 6| Step: 7
Training loss: 2.1204278469085693
Validation loss: 2.2043502926826477

Epoch: 6| Step: 8
Training loss: 1.1637459993362427
Validation loss: 2.2197994589805603

Epoch: 6| Step: 9
Training loss: 2.12966251373291
Validation loss: 2.2191259066263833

Epoch: 6| Step: 10
Training loss: 2.127030849456787
Validation loss: 2.2130467891693115

Epoch: 6| Step: 11
Training loss: 2.209768295288086
Validation loss: 2.189926107724508

Epoch: 6| Step: 12
Training loss: 1.7106863260269165
Validation loss: 2.192766865094503

Epoch: 6| Step: 13
Training loss: 1.857979416847229
Validation loss: 2.2005719343821206

Epoch: 221| Step: 0
Training loss: 1.7114664316177368
Validation loss: 2.172633687655131

Epoch: 6| Step: 1
Training loss: 2.1625683307647705
Validation loss: 2.1859253446261087

Epoch: 6| Step: 2
Training loss: 2.067296028137207
Validation loss: 2.1746952732404075

Epoch: 6| Step: 3
Training loss: 2.181009292602539
Validation loss: 2.172922670841217

Epoch: 6| Step: 4
Training loss: 1.7641079425811768
Validation loss: 2.1878050168355307

Epoch: 6| Step: 5
Training loss: 1.6490542888641357
Validation loss: 2.1684511303901672

Epoch: 6| Step: 6
Training loss: 2.1195740699768066
Validation loss: 2.169718583424886

Epoch: 6| Step: 7
Training loss: 1.1380517482757568
Validation loss: 2.1598385771115622

Epoch: 6| Step: 8
Training loss: 1.426398515701294
Validation loss: 2.1618674397468567

Epoch: 6| Step: 9
Training loss: 2.041569709777832
Validation loss: 2.1762203772862754

Epoch: 6| Step: 10
Training loss: 2.4455912113189697
Validation loss: 2.1882416009902954

Epoch: 6| Step: 11
Training loss: 1.596483588218689
Validation loss: 2.2084786097208657

Epoch: 6| Step: 12
Training loss: 1.9621798992156982
Validation loss: 2.2138235767682395

Epoch: 6| Step: 13
Training loss: 1.8832578659057617
Validation loss: 2.2130256493886313

Epoch: 222| Step: 0
Training loss: 1.9095165729522705
Validation loss: 2.2357812325159707

Epoch: 6| Step: 1
Training loss: 2.8924121856689453
Validation loss: 2.216141939163208

Epoch: 6| Step: 2
Training loss: 2.197248935699463
Validation loss: 2.2430888017018638

Epoch: 6| Step: 3
Training loss: 1.5121467113494873
Validation loss: 2.234365701675415

Epoch: 6| Step: 4
Training loss: 2.269113540649414
Validation loss: 2.22649351755778

Epoch: 6| Step: 5
Training loss: 1.8185575008392334
Validation loss: 2.2213064233462014

Epoch: 6| Step: 6
Training loss: 2.1709790229797363
Validation loss: 2.1900736490885415

Epoch: 6| Step: 7
Training loss: 2.36479115486145
Validation loss: 2.1914090116818747

Epoch: 6| Step: 8
Training loss: 1.2851145267486572
Validation loss: 2.1717888911565146

Epoch: 6| Step: 9
Training loss: 2.038309097290039
Validation loss: 2.1687812407811484

Epoch: 6| Step: 10
Training loss: 1.7280980348587036
Validation loss: 2.1497191190719604

Epoch: 6| Step: 11
Training loss: 2.010038375854492
Validation loss: 2.133601129055023

Epoch: 6| Step: 12
Training loss: 1.8069870471954346
Validation loss: 2.144234816233317

Epoch: 6| Step: 13
Training loss: 2.1386313438415527
Validation loss: 2.1390806436538696

Epoch: 223| Step: 0
Training loss: 1.889295220375061
Validation loss: 2.152417461077372

Epoch: 6| Step: 1
Training loss: 2.235513687133789
Validation loss: 2.1528847217559814

Epoch: 6| Step: 2
Training loss: 2.4092068672180176
Validation loss: 2.1795547803243003

Epoch: 6| Step: 3
Training loss: 1.443430781364441
Validation loss: 2.1777490178743997

Epoch: 6| Step: 4
Training loss: 2.002561330795288
Validation loss: 2.189834793408712

Epoch: 6| Step: 5
Training loss: 2.4877467155456543
Validation loss: 2.1831366817156472

Epoch: 6| Step: 6
Training loss: 1.5318607091903687
Validation loss: 2.203460454940796

Epoch: 6| Step: 7
Training loss: 2.41782283782959
Validation loss: 2.2057941357294717

Epoch: 6| Step: 8
Training loss: 2.0171959400177
Validation loss: 2.1943811972935996

Epoch: 6| Step: 9
Training loss: 1.3832460641860962
Validation loss: 2.2141266465187073

Epoch: 6| Step: 10
Training loss: 1.5328166484832764
Validation loss: 2.220680852731069

Epoch: 6| Step: 11
Training loss: 1.5526671409606934
Validation loss: 2.1984156370162964

Epoch: 6| Step: 12
Training loss: 2.151505947113037
Validation loss: 2.2220589319864907

Epoch: 6| Step: 13
Training loss: 1.8750253915786743
Validation loss: 2.2108023365338645

Epoch: 224| Step: 0
Training loss: 1.3076320886611938
Validation loss: 2.1912039518356323

Epoch: 6| Step: 1
Training loss: 1.8640185594558716
Validation loss: 2.1804705262184143

Epoch: 6| Step: 2
Training loss: 2.079087972640991
Validation loss: 2.1623844305674234

Epoch: 6| Step: 3
Training loss: 1.9193453788757324
Validation loss: 2.159909466902415

Epoch: 6| Step: 4
Training loss: 2.221806764602661
Validation loss: 2.163186271985372

Epoch: 6| Step: 5
Training loss: 2.3493356704711914
Validation loss: 2.174552341302236

Epoch: 6| Step: 6
Training loss: 1.3668327331542969
Validation loss: 2.162066916624705

Epoch: 6| Step: 7
Training loss: 2.134861707687378
Validation loss: 2.1841337084770203

Epoch: 6| Step: 8
Training loss: 2.32582950592041
Validation loss: 2.188027799129486

Epoch: 6| Step: 9
Training loss: 2.221588373184204
Validation loss: 2.1930809815724692

Epoch: 6| Step: 10
Training loss: 1.8845081329345703
Validation loss: 2.1989601651827493

Epoch: 6| Step: 11
Training loss: 1.6246179342269897
Validation loss: 2.202455163002014

Epoch: 6| Step: 12
Training loss: 1.5908987522125244
Validation loss: 2.1927598317464194

Epoch: 6| Step: 13
Training loss: 1.4134100675582886
Validation loss: 2.2001006603240967

Epoch: 225| Step: 0
Training loss: 2.0258259773254395
Validation loss: 2.1818142334620156

Epoch: 6| Step: 1
Training loss: 2.2351982593536377
Validation loss: 2.188043793042501

Epoch: 6| Step: 2
Training loss: 2.106233596801758
Validation loss: 2.1797695755958557

Epoch: 6| Step: 3
Training loss: 1.6957060098648071
Validation loss: 2.1644276777903237

Epoch: 6| Step: 4
Training loss: 2.250213861465454
Validation loss: 2.164385656515757

Epoch: 6| Step: 5
Training loss: 2.6983540058135986
Validation loss: 2.171721557776133

Epoch: 6| Step: 6
Training loss: 1.2914185523986816
Validation loss: 2.160536507765452

Epoch: 6| Step: 7
Training loss: 1.4694452285766602
Validation loss: 2.1800339420636496

Epoch: 6| Step: 8
Training loss: 1.755387544631958
Validation loss: 2.176931897799174

Epoch: 6| Step: 9
Training loss: 1.4498419761657715
Validation loss: 2.1624345779418945

Epoch: 6| Step: 10
Training loss: 2.0377631187438965
Validation loss: 2.167120178540548

Epoch: 6| Step: 11
Training loss: 1.5872445106506348
Validation loss: 2.166360636552175

Epoch: 6| Step: 12
Training loss: 1.5664374828338623
Validation loss: 2.15250571568807

Epoch: 6| Step: 13
Training loss: 2.0229499340057373
Validation loss: 2.174477199713389

Epoch: 226| Step: 0
Training loss: 1.7808082103729248
Validation loss: 2.169116954008738

Epoch: 6| Step: 1
Training loss: 2.730891704559326
Validation loss: 2.1619659662246704

Epoch: 6| Step: 2
Training loss: 1.911315679550171
Validation loss: 2.1518105268478394

Epoch: 6| Step: 3
Training loss: 1.555647850036621
Validation loss: 2.174645801385244

Epoch: 6| Step: 4
Training loss: 1.9355924129486084
Validation loss: 2.1736502647399902

Epoch: 6| Step: 5
Training loss: 2.635329008102417
Validation loss: 2.1829118132591248

Epoch: 6| Step: 6
Training loss: 1.5561436414718628
Validation loss: 2.1925445397694907

Epoch: 6| Step: 7
Training loss: 2.0787692070007324
Validation loss: 2.1605140368143716

Epoch: 6| Step: 8
Training loss: 1.3495049476623535
Validation loss: 2.143196185429891

Epoch: 6| Step: 9
Training loss: 1.6141490936279297
Validation loss: 2.1321536103884378

Epoch: 6| Step: 10
Training loss: 1.431448221206665
Validation loss: 2.1367810567220054

Epoch: 6| Step: 11
Training loss: 1.3837896585464478
Validation loss: 2.1111383636792502

Epoch: 6| Step: 12
Training loss: 2.0532004833221436
Validation loss: 2.1344241301218667

Epoch: 6| Step: 13
Training loss: 1.904973030090332
Validation loss: 2.14499568939209

Epoch: 227| Step: 0
Training loss: 1.6206681728363037
Validation loss: 2.147019704182943

Epoch: 6| Step: 1
Training loss: 1.8123884201049805
Validation loss: 2.151073972384135

Epoch: 6| Step: 2
Training loss: 2.478083610534668
Validation loss: 2.1786855260531106

Epoch: 6| Step: 3
Training loss: 1.7521584033966064
Validation loss: 2.1806600689888

Epoch: 6| Step: 4
Training loss: 1.8110504150390625
Validation loss: 2.1897255579630532

Epoch: 6| Step: 5
Training loss: 1.7751728296279907
Validation loss: 2.1966726581255593

Epoch: 6| Step: 6
Training loss: 1.9033863544464111
Validation loss: 2.1810817321141562

Epoch: 6| Step: 7
Training loss: 2.2012360095977783
Validation loss: 2.1883403658866882

Epoch: 6| Step: 8
Training loss: 1.9301466941833496
Validation loss: 2.171204626560211

Epoch: 6| Step: 9
Training loss: 1.738257646560669
Validation loss: 2.2032975554466248

Epoch: 6| Step: 10
Training loss: 1.8287510871887207
Validation loss: 2.1923905412356057

Epoch: 6| Step: 11
Training loss: 1.8898764848709106
Validation loss: 2.204887588818868

Epoch: 6| Step: 12
Training loss: 2.0824813842773438
Validation loss: 2.2295397520065308

Epoch: 6| Step: 13
Training loss: 1.5595662593841553
Validation loss: 2.199943204720815

Epoch: 228| Step: 0
Training loss: 1.193008303642273
Validation loss: 2.207642436027527

Epoch: 6| Step: 1
Training loss: 2.192214012145996
Validation loss: 2.2364784479141235

Epoch: 6| Step: 2
Training loss: 2.4777908325195312
Validation loss: 2.2462394634882608

Epoch: 6| Step: 3
Training loss: 2.6259589195251465
Validation loss: 2.2369869152704873

Epoch: 6| Step: 4
Training loss: 2.547104835510254
Validation loss: 2.1968760093053183

Epoch: 6| Step: 5
Training loss: 1.8538843393325806
Validation loss: 2.2061153252919516

Epoch: 6| Step: 6
Training loss: 1.6128880977630615
Validation loss: 2.2024752100308738

Epoch: 6| Step: 7
Training loss: 2.254016399383545
Validation loss: 2.181176702181498

Epoch: 6| Step: 8
Training loss: 1.4725666046142578
Validation loss: 2.159593383471171

Epoch: 6| Step: 9
Training loss: 1.9715312719345093
Validation loss: 2.1456419229507446

Epoch: 6| Step: 10
Training loss: 1.7368099689483643
Validation loss: 2.1561623414357505

Epoch: 6| Step: 11
Training loss: 1.1476842164993286
Validation loss: 2.1382842659950256

Epoch: 6| Step: 12
Training loss: 2.7383780479431152
Validation loss: 2.145568589369456

Epoch: 6| Step: 13
Training loss: 1.2346473932266235
Validation loss: 2.1626296043395996

Epoch: 229| Step: 0
Training loss: 2.4004669189453125
Validation loss: 2.1592647433280945

Epoch: 6| Step: 1
Training loss: 1.3166815042495728
Validation loss: 2.160863002141317

Epoch: 6| Step: 2
Training loss: 1.9232767820358276
Validation loss: 2.1823700269063315

Epoch: 6| Step: 3
Training loss: 1.9695072174072266
Validation loss: 2.1960670153299966

Epoch: 6| Step: 4
Training loss: 1.6334034204483032
Validation loss: 2.1991071502367654

Epoch: 6| Step: 5
Training loss: 1.7692540884017944
Validation loss: 2.1905338764190674

Epoch: 6| Step: 6
Training loss: 1.7294617891311646
Validation loss: 2.20343554019928

Epoch: 6| Step: 7
Training loss: 2.1938672065734863
Validation loss: 2.220825990041097

Epoch: 6| Step: 8
Training loss: 1.8522800207138062
Validation loss: 2.200287679831187

Epoch: 6| Step: 9
Training loss: 2.0023434162139893
Validation loss: 2.198732574780782

Epoch: 6| Step: 10
Training loss: 1.8370473384857178
Validation loss: 2.1791995565096536

Epoch: 6| Step: 11
Training loss: 1.8825230598449707
Validation loss: 2.1658540765444436

Epoch: 6| Step: 12
Training loss: 1.9656018018722534
Validation loss: 2.168965439001719

Epoch: 6| Step: 13
Training loss: 1.8354060649871826
Validation loss: 2.1779446403185525

Epoch: 230| Step: 0
Training loss: 2.190469264984131
Validation loss: 2.1715014775594077

Epoch: 6| Step: 1
Training loss: 1.7693109512329102
Validation loss: 2.173856536547343

Epoch: 6| Step: 2
Training loss: 2.0451412200927734
Validation loss: 2.1920761466026306

Epoch: 6| Step: 3
Training loss: 1.4431216716766357
Validation loss: 2.163285772005717

Epoch: 6| Step: 4
Training loss: 2.1896309852600098
Validation loss: 2.17478475968043

Epoch: 6| Step: 5
Training loss: 2.004723310470581
Validation loss: 2.1733719309171042

Epoch: 6| Step: 6
Training loss: 1.7033003568649292
Validation loss: 2.180328925450643

Epoch: 6| Step: 7
Training loss: 2.5909624099731445
Validation loss: 2.1796040534973145

Epoch: 6| Step: 8
Training loss: 1.6472034454345703
Validation loss: 2.181997776031494

Epoch: 6| Step: 9
Training loss: 1.9504108428955078
Validation loss: 2.1668060223261514

Epoch: 6| Step: 10
Training loss: 1.4293454885482788
Validation loss: 2.1753890117009482

Epoch: 6| Step: 11
Training loss: 1.5599730014801025
Validation loss: 2.186514596144358

Epoch: 6| Step: 12
Training loss: 1.9383490085601807
Validation loss: 2.1666752894719443

Epoch: 6| Step: 13
Training loss: 1.97398841381073
Validation loss: 2.2020219961802163

Epoch: 231| Step: 0
Training loss: 1.979206919670105
Validation loss: 2.1766284108161926

Epoch: 6| Step: 1
Training loss: 2.166590690612793
Validation loss: 2.1803358793258667

Epoch: 6| Step: 2
Training loss: 1.4932714700698853
Validation loss: 2.182673136393229

Epoch: 6| Step: 3
Training loss: 2.5022969245910645
Validation loss: 2.1622389356295266

Epoch: 6| Step: 4
Training loss: 2.11049747467041
Validation loss: 2.1742679874102273

Epoch: 6| Step: 5
Training loss: 1.7075307369232178
Validation loss: 2.1899372140566506

Epoch: 6| Step: 6
Training loss: 2.288172960281372
Validation loss: 2.178835908571879

Epoch: 6| Step: 7
Training loss: 1.8626554012298584
Validation loss: 2.1760182778040567

Epoch: 6| Step: 8
Training loss: 2.4264650344848633
Validation loss: 2.186922033627828

Epoch: 6| Step: 9
Training loss: 1.6007487773895264
Validation loss: 2.176942984263102

Epoch: 6| Step: 10
Training loss: 1.9490629434585571
Validation loss: 2.1926691929499307

Epoch: 6| Step: 11
Training loss: 1.8200936317443848
Validation loss: 2.1726659138997397

Epoch: 6| Step: 12
Training loss: 1.3699638843536377
Validation loss: 2.176048219203949

Epoch: 6| Step: 13
Training loss: 1.3321540355682373
Validation loss: 2.169741908709208

Epoch: 232| Step: 0
Training loss: 1.7787160873413086
Validation loss: 2.181934356689453

Epoch: 6| Step: 1
Training loss: 1.4527758359909058
Validation loss: 2.1901251077651978

Epoch: 6| Step: 2
Training loss: 1.822413444519043
Validation loss: 2.1814210216204324

Epoch: 6| Step: 3
Training loss: 1.918022632598877
Validation loss: 2.1679741541544595

Epoch: 6| Step: 4
Training loss: 1.4900716543197632
Validation loss: 2.185824771722158

Epoch: 6| Step: 5
Training loss: 1.4924705028533936
Validation loss: 2.1876227458318076

Epoch: 6| Step: 6
Training loss: 2.228802442550659
Validation loss: 2.17218679189682

Epoch: 6| Step: 7
Training loss: 2.511040687561035
Validation loss: 2.1906389594078064

Epoch: 6| Step: 8
Training loss: 2.308168649673462
Validation loss: 2.1935280561447144

Epoch: 6| Step: 9
Training loss: 2.199293613433838
Validation loss: 2.153227130572001

Epoch: 6| Step: 10
Training loss: 1.1946951150894165
Validation loss: 2.176820079485575

Epoch: 6| Step: 11
Training loss: 1.7701435089111328
Validation loss: 2.172050635019938

Epoch: 6| Step: 12
Training loss: 1.6942100524902344
Validation loss: 2.187341809272766

Epoch: 6| Step: 13
Training loss: 2.2599782943725586
Validation loss: 2.1727246840794883

Epoch: 233| Step: 0
Training loss: 1.7748889923095703
Validation loss: 2.172198474407196

Epoch: 6| Step: 1
Training loss: 1.565704345703125
Validation loss: 2.175795237223307

Epoch: 6| Step: 2
Training loss: 1.1559062004089355
Validation loss: 2.188518444697062

Epoch: 6| Step: 3
Training loss: 2.379067897796631
Validation loss: 2.22441832224528

Epoch: 6| Step: 4
Training loss: 1.5772250890731812
Validation loss: 2.2084750135739646

Epoch: 6| Step: 5
Training loss: 1.4016039371490479
Validation loss: 2.2214697202046714

Epoch: 6| Step: 6
Training loss: 1.6644303798675537
Validation loss: 2.207814335823059

Epoch: 6| Step: 7
Training loss: 1.675666093826294
Validation loss: 2.208261013031006

Epoch: 6| Step: 8
Training loss: 1.671493411064148
Validation loss: 2.198527733484904

Epoch: 6| Step: 9
Training loss: 2.7224059104919434
Validation loss: 2.1795912981033325

Epoch: 6| Step: 10
Training loss: 2.367568016052246
Validation loss: 2.163149356842041

Epoch: 6| Step: 11
Training loss: 2.3847527503967285
Validation loss: 2.172745664914449

Epoch: 6| Step: 12
Training loss: 1.8711334466934204
Validation loss: 2.148534655570984

Epoch: 6| Step: 13
Training loss: 1.4565180540084839
Validation loss: 2.173023442427317

Epoch: 234| Step: 0
Training loss: 1.8034651279449463
Validation loss: 2.1619107325871787

Epoch: 6| Step: 1
Training loss: 1.8433235883712769
Validation loss: 2.1639849742253623

Epoch: 6| Step: 2
Training loss: 2.81892728805542
Validation loss: 2.1780654986699424

Epoch: 6| Step: 3
Training loss: 1.431898593902588
Validation loss: 2.1671464840571084

Epoch: 6| Step: 4
Training loss: 2.289766788482666
Validation loss: 2.1697992285092673

Epoch: 6| Step: 5
Training loss: 2.296095371246338
Validation loss: 2.1767449378967285

Epoch: 6| Step: 6
Training loss: 1.5108351707458496
Validation loss: 2.1822426319122314

Epoch: 6| Step: 7
Training loss: 1.3758630752563477
Validation loss: 2.186900715033213

Epoch: 6| Step: 8
Training loss: 1.6419389247894287
Validation loss: 2.1668131748835244

Epoch: 6| Step: 9
Training loss: 1.6173012256622314
Validation loss: 2.2027339736620584

Epoch: 6| Step: 10
Training loss: 1.8318896293640137
Validation loss: 2.1762166023254395

Epoch: 6| Step: 11
Training loss: 2.044776678085327
Validation loss: 2.178894559542338

Epoch: 6| Step: 12
Training loss: 1.5330188274383545
Validation loss: 2.202381710211436

Epoch: 6| Step: 13
Training loss: 1.7771530151367188
Validation loss: 2.221358338991801

Epoch: 235| Step: 0
Training loss: 1.5288188457489014
Validation loss: 2.2013297080993652

Epoch: 6| Step: 1
Training loss: 1.5532820224761963
Validation loss: 2.2001167138417563

Epoch: 6| Step: 2
Training loss: 1.6796956062316895
Validation loss: 2.2156331141789756

Epoch: 6| Step: 3
Training loss: 2.079132080078125
Validation loss: 2.2153612772623696

Epoch: 6| Step: 4
Training loss: 2.0784449577331543
Validation loss: 2.1952205101648965

Epoch: 6| Step: 5
Training loss: 2.1570162773132324
Validation loss: 2.161762535572052

Epoch: 6| Step: 6
Training loss: 2.26798152923584
Validation loss: 2.169124722480774

Epoch: 6| Step: 7
Training loss: 1.861687183380127
Validation loss: 2.1415353218714395

Epoch: 6| Step: 8
Training loss: 1.4568579196929932
Validation loss: 2.143080989519755

Epoch: 6| Step: 9
Training loss: 1.3077943325042725
Validation loss: 2.138909180959066

Epoch: 6| Step: 10
Training loss: 2.329817533493042
Validation loss: 2.1449535886446633

Epoch: 6| Step: 11
Training loss: 1.6699392795562744
Validation loss: 2.1349952618281045

Epoch: 6| Step: 12
Training loss: 2.2164556980133057
Validation loss: 2.159667134284973

Epoch: 6| Step: 13
Training loss: 1.9330189228057861
Validation loss: 2.1679399609565735

Epoch: 236| Step: 0
Training loss: 2.0108580589294434
Validation loss: 2.1630708972613015

Epoch: 6| Step: 1
Training loss: 1.9544131755828857
Validation loss: 2.1569695472717285

Epoch: 6| Step: 2
Training loss: 2.339766263961792
Validation loss: 2.156118154525757

Epoch: 6| Step: 3
Training loss: 1.6825077533721924
Validation loss: 2.172642548878988

Epoch: 6| Step: 4
Training loss: 1.5296635627746582
Validation loss: 2.170932730038961

Epoch: 6| Step: 5
Training loss: 2.1948134899139404
Validation loss: 2.1920888821283975

Epoch: 6| Step: 6
Training loss: 1.7324575185775757
Validation loss: 2.198234260082245

Epoch: 6| Step: 7
Training loss: 1.9784746170043945
Validation loss: 2.2239252726236978

Epoch: 6| Step: 8
Training loss: 2.1086623668670654
Validation loss: 2.2361598014831543

Epoch: 6| Step: 9
Training loss: 1.6844868659973145
Validation loss: 2.2077476183573403

Epoch: 6| Step: 10
Training loss: 2.5103349685668945
Validation loss: 2.217197060585022

Epoch: 6| Step: 11
Training loss: 0.9926325082778931
Validation loss: 2.222444713115692

Epoch: 6| Step: 12
Training loss: 1.8484028577804565
Validation loss: 2.2017339865366616

Epoch: 6| Step: 13
Training loss: 1.6145100593566895
Validation loss: 2.1546972592671714

Epoch: 237| Step: 0
Training loss: 1.234779715538025
Validation loss: 2.156248410542806

Epoch: 6| Step: 1
Training loss: 1.601298451423645
Validation loss: 2.1747237046559653

Epoch: 6| Step: 2
Training loss: 1.7071318626403809
Validation loss: 2.165747900803884

Epoch: 6| Step: 3
Training loss: 1.7006309032440186
Validation loss: 2.167482296625773

Epoch: 6| Step: 4
Training loss: 2.513798475265503
Validation loss: 2.164142906665802

Epoch: 6| Step: 5
Training loss: 2.642312526702881
Validation loss: 2.196470061937968

Epoch: 6| Step: 6
Training loss: 2.660198450088501
Validation loss: 2.2048688928286233

Epoch: 6| Step: 7
Training loss: 2.129141330718994
Validation loss: 2.1905040740966797

Epoch: 6| Step: 8
Training loss: 1.1489770412445068
Validation loss: 2.2085444132486978

Epoch: 6| Step: 9
Training loss: 1.5446285009384155
Validation loss: 2.224982758363088

Epoch: 6| Step: 10
Training loss: 1.6250498294830322
Validation loss: 2.2194131215413413

Epoch: 6| Step: 11
Training loss: 1.4506769180297852
Validation loss: 2.2039968371391296

Epoch: 6| Step: 12
Training loss: 1.493112564086914
Validation loss: 2.1934386690457663

Epoch: 6| Step: 13
Training loss: 2.063671112060547
Validation loss: 2.2087919116020203

Epoch: 238| Step: 0
Training loss: 1.3456439971923828
Validation loss: 2.2159685293833413

Epoch: 6| Step: 1
Training loss: 1.2557334899902344
Validation loss: 2.194103797276815

Epoch: 6| Step: 2
Training loss: 1.687528371810913
Validation loss: 2.2066988150278726

Epoch: 6| Step: 3
Training loss: 2.0630722045898438
Validation loss: 2.2052177588144937

Epoch: 6| Step: 4
Training loss: 1.3541679382324219
Validation loss: 2.2099571029345193

Epoch: 6| Step: 5
Training loss: 1.2671749591827393
Validation loss: 2.192698836326599

Epoch: 6| Step: 6
Training loss: 2.9377799034118652
Validation loss: 2.1946100195248923

Epoch: 6| Step: 7
Training loss: 1.8701173067092896
Validation loss: 2.175901214281718

Epoch: 6| Step: 8
Training loss: 2.260941982269287
Validation loss: 2.196715851624807

Epoch: 6| Step: 9
Training loss: 2.222025156021118
Validation loss: 2.175606449445089

Epoch: 6| Step: 10
Training loss: 1.6285698413848877
Validation loss: 2.1555090149243674

Epoch: 6| Step: 11
Training loss: 2.1748781204223633
Validation loss: 2.121478319168091

Epoch: 6| Step: 12
Training loss: 2.594111680984497
Validation loss: 2.1336085398991904

Epoch: 6| Step: 13
Training loss: 1.6386356353759766
Validation loss: 2.1514952977498374

Epoch: 239| Step: 0
Training loss: 1.8953615427017212
Validation loss: 2.146113554636637

Epoch: 6| Step: 1
Training loss: 1.605510950088501
Validation loss: 2.1333260933558145

Epoch: 6| Step: 2
Training loss: 1.8584706783294678
Validation loss: 2.15479044119517

Epoch: 6| Step: 3
Training loss: 2.233936309814453
Validation loss: 2.14740518728892

Epoch: 6| Step: 4
Training loss: 1.250365972518921
Validation loss: 2.175036132335663

Epoch: 6| Step: 5
Training loss: 2.0584394931793213
Validation loss: 2.1430583596229553

Epoch: 6| Step: 6
Training loss: 1.9239543676376343
Validation loss: 2.1706580320994058

Epoch: 6| Step: 7
Training loss: 1.3607804775238037
Validation loss: 2.1855979561805725

Epoch: 6| Step: 8
Training loss: 1.489365816116333
Validation loss: 2.198684573173523

Epoch: 6| Step: 9
Training loss: 1.9052464962005615
Validation loss: 2.2345962127049765

Epoch: 6| Step: 10
Training loss: 1.9097498655319214
Validation loss: 2.214245319366455

Epoch: 6| Step: 11
Training loss: 2.407595634460449
Validation loss: 2.2186135053634644

Epoch: 6| Step: 12
Training loss: 1.6558146476745605
Validation loss: 2.233116785685221

Epoch: 6| Step: 13
Training loss: 2.6326041221618652
Validation loss: 2.243104656537374

Epoch: 240| Step: 0
Training loss: 1.998403787612915
Validation loss: 2.227433721224467

Epoch: 6| Step: 1
Training loss: 1.5499475002288818
Validation loss: 2.2222567399342856

Epoch: 6| Step: 2
Training loss: 1.8613841533660889
Validation loss: 2.1925827264785767

Epoch: 6| Step: 3
Training loss: 2.1099002361297607
Validation loss: 2.2013763387997947

Epoch: 6| Step: 4
Training loss: 1.5640407800674438
Validation loss: 2.182468672593435

Epoch: 6| Step: 5
Training loss: 1.681342363357544
Validation loss: 2.2111037174860635

Epoch: 6| Step: 6
Training loss: 1.8479599952697754
Validation loss: 2.2007360458374023

Epoch: 6| Step: 7
Training loss: 1.5577285289764404
Validation loss: 2.186874588330587

Epoch: 6| Step: 8
Training loss: 1.500820517539978
Validation loss: 2.188346823056539

Epoch: 6| Step: 9
Training loss: 1.6293723583221436
Validation loss: 2.188876668612162

Epoch: 6| Step: 10
Training loss: 3.474257469177246
Validation loss: 2.1962676843007407

Epoch: 6| Step: 11
Training loss: 1.1716617345809937
Validation loss: 2.2008899450302124

Epoch: 6| Step: 12
Training loss: 1.6358180046081543
Validation loss: 2.193996548652649

Epoch: 6| Step: 13
Training loss: 2.352522850036621
Validation loss: 2.2128682931264243

Epoch: 241| Step: 0
Training loss: 1.6269299983978271
Validation loss: 2.1929133335749307

Epoch: 6| Step: 1
Training loss: 1.7403218746185303
Validation loss: 2.2048094272613525

Epoch: 6| Step: 2
Training loss: 2.0810365676879883
Validation loss: 2.2218464811642966

Epoch: 6| Step: 3
Training loss: 1.6130118370056152
Validation loss: 2.2265685002009072

Epoch: 6| Step: 4
Training loss: 1.8215861320495605
Validation loss: 2.22648294766744

Epoch: 6| Step: 5
Training loss: 1.7313995361328125
Validation loss: 2.2173935969670615

Epoch: 6| Step: 6
Training loss: 1.8691253662109375
Validation loss: 2.237625241279602

Epoch: 6| Step: 7
Training loss: 1.7257415056228638
Validation loss: 2.2202204863230386

Epoch: 6| Step: 8
Training loss: 1.6119539737701416
Validation loss: 2.2336509227752686

Epoch: 6| Step: 9
Training loss: 1.920008897781372
Validation loss: 2.2119589845339456

Epoch: 6| Step: 10
Training loss: 2.3760321140289307
Validation loss: 2.2101439038912454

Epoch: 6| Step: 11
Training loss: 1.5128049850463867
Validation loss: 2.21637233098348

Epoch: 6| Step: 12
Training loss: 1.7405855655670166
Validation loss: 2.210459589958191

Epoch: 6| Step: 13
Training loss: 2.2889046669006348
Validation loss: 2.1940104166666665

Epoch: 242| Step: 0
Training loss: 2.1487531661987305
Validation loss: 2.1940470735232034

Epoch: 6| Step: 1
Training loss: 1.4727790355682373
Validation loss: 2.199028432369232

Epoch: 6| Step: 2
Training loss: 1.8973000049591064
Validation loss: 2.1888666351636252

Epoch: 6| Step: 3
Training loss: 1.3233853578567505
Validation loss: 2.177453656991323

Epoch: 6| Step: 4
Training loss: 1.4814438819885254
Validation loss: 2.1899921894073486

Epoch: 6| Step: 5
Training loss: 2.1097640991210938
Validation loss: 2.1922112703323364

Epoch: 6| Step: 6
Training loss: 2.2194876670837402
Validation loss: 2.1892878810564675

Epoch: 6| Step: 7
Training loss: 1.7363855838775635
Validation loss: 2.168335755666097

Epoch: 6| Step: 8
Training loss: 2.0487093925476074
Validation loss: 2.2011738618214927

Epoch: 6| Step: 9
Training loss: 1.9932835102081299
Validation loss: 2.2096482515335083

Epoch: 6| Step: 10
Training loss: 2.5855512619018555
Validation loss: 2.209898312886556

Epoch: 6| Step: 11
Training loss: 1.4185988903045654
Validation loss: 2.216621915499369

Epoch: 6| Step: 12
Training loss: 1.4062747955322266
Validation loss: 2.20682954788208

Epoch: 6| Step: 13
Training loss: 1.6405282020568848
Validation loss: 2.215551793575287

Epoch: 243| Step: 0
Training loss: 1.6078479290008545
Validation loss: 2.2046276728312173

Epoch: 6| Step: 1
Training loss: 1.7844064235687256
Validation loss: 2.2117778857549033

Epoch: 6| Step: 2
Training loss: 1.8138976097106934
Validation loss: 2.183135151863098

Epoch: 6| Step: 3
Training loss: 1.6722209453582764
Validation loss: 2.1884320974349976

Epoch: 6| Step: 4
Training loss: 1.7690750360488892
Validation loss: 2.1942171851793923

Epoch: 6| Step: 5
Training loss: 2.000108242034912
Validation loss: 2.216023782889048

Epoch: 6| Step: 6
Training loss: 1.5551066398620605
Validation loss: 2.2080811063448587

Epoch: 6| Step: 7
Training loss: 1.5850999355316162
Validation loss: 2.21378360191981

Epoch: 6| Step: 8
Training loss: 1.4095253944396973
Validation loss: 2.2229276299476624

Epoch: 6| Step: 9
Training loss: 1.9135938882827759
Validation loss: 2.233124852180481

Epoch: 6| Step: 10
Training loss: 2.7823872566223145
Validation loss: 2.2520725329717

Epoch: 6| Step: 11
Training loss: 1.795344352722168
Validation loss: 2.2459640502929688

Epoch: 6| Step: 12
Training loss: 2.1712193489074707
Validation loss: 2.238823572794596

Epoch: 6| Step: 13
Training loss: 1.9601736068725586
Validation loss: 2.2392672300338745

Epoch: 244| Step: 0
Training loss: 1.5559487342834473
Validation loss: 2.234944919745127

Epoch: 6| Step: 1
Training loss: 1.6533722877502441
Validation loss: 2.2156982421875

Epoch: 6| Step: 2
Training loss: 1.9468355178833008
Validation loss: 2.202905515829722

Epoch: 6| Step: 3
Training loss: 1.6689320802688599
Validation loss: 2.202589273452759

Epoch: 6| Step: 4
Training loss: 1.6757316589355469
Validation loss: 2.20113197962443

Epoch: 6| Step: 5
Training loss: 3.2361879348754883
Validation loss: 2.1830559968948364

Epoch: 6| Step: 6
Training loss: 1.7011888027191162
Validation loss: 2.189929445584615

Epoch: 6| Step: 7
Training loss: 1.9309544563293457
Validation loss: 2.2078189849853516

Epoch: 6| Step: 8
Training loss: 1.5290933847427368
Validation loss: 2.225940465927124

Epoch: 6| Step: 9
Training loss: 1.8186228275299072
Validation loss: 2.247500777244568

Epoch: 6| Step: 10
Training loss: 2.209815502166748
Validation loss: 2.259308377901713

Epoch: 6| Step: 11
Training loss: 1.3212215900421143
Validation loss: 2.2576840917269387

Epoch: 6| Step: 12
Training loss: 2.044584274291992
Validation loss: 2.2637927532196045

Epoch: 6| Step: 13
Training loss: 1.6298742294311523
Validation loss: 2.2528246442476907

Epoch: 245| Step: 0
Training loss: 2.6088805198669434
Validation loss: 2.2676852544148765

Epoch: 6| Step: 1
Training loss: 1.8404297828674316
Validation loss: 2.235094885031382

Epoch: 6| Step: 2
Training loss: 1.5764596462249756
Validation loss: 2.253552039464315

Epoch: 6| Step: 3
Training loss: 1.9113380908966064
Validation loss: 2.254972298940023

Epoch: 6| Step: 4
Training loss: 2.3440966606140137
Validation loss: 2.2452900807062783

Epoch: 6| Step: 5
Training loss: 1.3712341785430908
Validation loss: 2.2453929781913757

Epoch: 6| Step: 6
Training loss: 1.428335428237915
Validation loss: 2.1955811778704324

Epoch: 6| Step: 7
Training loss: 1.651462197303772
Validation loss: 2.1931259632110596

Epoch: 6| Step: 8
Training loss: 2.0966930389404297
Validation loss: 2.167818625768026

Epoch: 6| Step: 9
Training loss: 2.1933698654174805
Validation loss: 2.1521889170010886

Epoch: 6| Step: 10
Training loss: 1.5754964351654053
Validation loss: 2.1561341087023416

Epoch: 6| Step: 11
Training loss: 1.6351099014282227
Validation loss: 2.1412066419919333

Epoch: 6| Step: 12
Training loss: 2.3909664154052734
Validation loss: 2.163940151532491

Epoch: 6| Step: 13
Training loss: 1.4632723331451416
Validation loss: 2.162897209326426

Epoch: 246| Step: 0
Training loss: 1.8679962158203125
Validation loss: 2.1776235500971475

Epoch: 6| Step: 1
Training loss: 1.1058061122894287
Validation loss: 2.1716046929359436

Epoch: 6| Step: 2
Training loss: 1.9079159498214722
Validation loss: 2.1846525271733603

Epoch: 6| Step: 3
Training loss: 2.2486982345581055
Validation loss: 2.185543874899546

Epoch: 6| Step: 4
Training loss: 1.5675342082977295
Validation loss: 2.1816572149594626

Epoch: 6| Step: 5
Training loss: 1.418058156967163
Validation loss: 2.1864938735961914

Epoch: 6| Step: 6
Training loss: 1.8875806331634521
Validation loss: 2.1937933365503945

Epoch: 6| Step: 7
Training loss: 2.5731658935546875
Validation loss: 2.2043115695317588

Epoch: 6| Step: 8
Training loss: 1.812542200088501
Validation loss: 2.216253320376078

Epoch: 6| Step: 9
Training loss: 1.6809089183807373
Validation loss: 2.227194309234619

Epoch: 6| Step: 10
Training loss: 2.1218695640563965
Validation loss: 2.231676240762075

Epoch: 6| Step: 11
Training loss: 1.6973204612731934
Validation loss: 2.197440028190613

Epoch: 6| Step: 12
Training loss: 1.6738245487213135
Validation loss: 2.2188551823298135

Epoch: 6| Step: 13
Training loss: 2.049645185470581
Validation loss: 2.217222889264425

Epoch: 247| Step: 0
Training loss: 1.9143880605697632
Validation loss: 2.216013471285502

Epoch: 6| Step: 1
Training loss: 1.6124533414840698
Validation loss: 2.2101747592290244

Epoch: 6| Step: 2
Training loss: 1.403249979019165
Validation loss: 2.2289409041404724

Epoch: 6| Step: 3
Training loss: 1.9280692338943481
Validation loss: 2.216139554977417

Epoch: 6| Step: 4
Training loss: 2.115610361099243
Validation loss: 2.2239679098129272

Epoch: 6| Step: 5
Training loss: 1.7981693744659424
Validation loss: 2.199074844519297

Epoch: 6| Step: 6
Training loss: 1.9845669269561768
Validation loss: 2.2084553241729736

Epoch: 6| Step: 7
Training loss: 1.667348861694336
Validation loss: 2.193070371945699

Epoch: 6| Step: 8
Training loss: 1.786146640777588
Validation loss: 2.178755044937134

Epoch: 6| Step: 9
Training loss: 1.390882968902588
Validation loss: 2.1967140436172485

Epoch: 6| Step: 10
Training loss: 2.5246949195861816
Validation loss: 2.1843620340029397

Epoch: 6| Step: 11
Training loss: 1.8223685026168823
Validation loss: 2.1889508962631226

Epoch: 6| Step: 12
Training loss: 1.5770683288574219
Validation loss: 2.181062420209249

Epoch: 6| Step: 13
Training loss: 2.0238025188446045
Validation loss: 2.205409606297811

Epoch: 248| Step: 0
Training loss: 1.7447049617767334
Validation loss: 2.186815102895101

Epoch: 6| Step: 1
Training loss: 1.8639168739318848
Validation loss: 2.212915301322937

Epoch: 6| Step: 2
Training loss: 2.340268135070801
Validation loss: 2.2067394852638245

Epoch: 6| Step: 3
Training loss: 1.6383072137832642
Validation loss: 2.2350803216298423

Epoch: 6| Step: 4
Training loss: 2.038543939590454
Validation loss: 2.2528708775838218

Epoch: 6| Step: 5
Training loss: 2.563969135284424
Validation loss: 2.2662669817606607

Epoch: 6| Step: 6
Training loss: 2.007383108139038
Validation loss: 2.2450345357259116

Epoch: 6| Step: 7
Training loss: 2.640756130218506
Validation loss: 2.2262264490127563

Epoch: 6| Step: 8
Training loss: 1.612072467803955
Validation loss: 2.1928266286849976

Epoch: 6| Step: 9
Training loss: 1.174228310585022
Validation loss: 2.1534008185068765

Epoch: 6| Step: 10
Training loss: 1.7620171308517456
Validation loss: 2.146457850933075

Epoch: 6| Step: 11
Training loss: 1.3268563747406006
Validation loss: 2.140697439511617

Epoch: 6| Step: 12
Training loss: 1.799023985862732
Validation loss: 2.1377630829811096

Epoch: 6| Step: 13
Training loss: 2.4053030014038086
Validation loss: 2.118852516015371

Epoch: 249| Step: 0
Training loss: 2.114332675933838
Validation loss: 2.1344739397366843

Epoch: 6| Step: 1
Training loss: 2.4112915992736816
Validation loss: 2.1330222884813943

Epoch: 6| Step: 2
Training loss: 2.0640180110931396
Validation loss: 2.132811943689982

Epoch: 6| Step: 3
Training loss: 2.050732374191284
Validation loss: 2.140730162461599

Epoch: 6| Step: 4
Training loss: 2.113541603088379
Validation loss: 2.151884396870931

Epoch: 6| Step: 5
Training loss: 1.7118982076644897
Validation loss: 2.1246159871419272

Epoch: 6| Step: 6
Training loss: 1.1984556913375854
Validation loss: 2.14118484656016

Epoch: 6| Step: 7
Training loss: 1.4806193113327026
Validation loss: 2.1450230280558267

Epoch: 6| Step: 8
Training loss: 2.2760088443756104
Validation loss: 2.1558579206466675

Epoch: 6| Step: 9
Training loss: 1.6901735067367554
Validation loss: 2.1665010253588357

Epoch: 6| Step: 10
Training loss: 1.9426987171173096
Validation loss: 2.1638695001602173

Epoch: 6| Step: 11
Training loss: 1.7023046016693115
Validation loss: 2.1488287647565207

Epoch: 6| Step: 12
Training loss: 1.713600754737854
Validation loss: 2.145672857761383

Epoch: 6| Step: 13
Training loss: 2.013871192932129
Validation loss: 2.1623481909434

Epoch: 250| Step: 0
Training loss: 2.117969274520874
Validation loss: 2.147982736428579

Epoch: 6| Step: 1
Training loss: 2.2873892784118652
Validation loss: 2.161677141984304

Epoch: 6| Step: 2
Training loss: 1.9368892908096313
Validation loss: 2.1544628143310547

Epoch: 6| Step: 3
Training loss: 1.8224000930786133
Validation loss: 2.164975186189016

Epoch: 6| Step: 4
Training loss: 1.7498517036437988
Validation loss: 2.1693992614746094

Epoch: 6| Step: 5
Training loss: 1.1381616592407227
Validation loss: 2.161040425300598

Epoch: 6| Step: 6
Training loss: 1.7787092924118042
Validation loss: 2.1687801082928977

Epoch: 6| Step: 7
Training loss: 1.8759981393814087
Validation loss: 2.160043954849243

Epoch: 6| Step: 8
Training loss: 1.9872567653656006
Validation loss: 2.1665988167126975

Epoch: 6| Step: 9
Training loss: 1.8507853746414185
Validation loss: 2.1597211758295694

Epoch: 6| Step: 10
Training loss: 1.6311166286468506
Validation loss: 2.1506837209065757

Epoch: 6| Step: 11
Training loss: 2.312910318374634
Validation loss: 2.155302802721659

Epoch: 6| Step: 12
Training loss: 1.7184631824493408
Validation loss: 2.1567440231641135

Epoch: 6| Step: 13
Training loss: 2.2526357173919678
Validation loss: 2.168144961198171

Epoch: 251| Step: 0
Training loss: 1.541823387145996
Validation loss: 2.1565315326054892

Epoch: 6| Step: 1
Training loss: 2.69050669670105
Validation loss: 2.180215080579122

Epoch: 6| Step: 2
Training loss: 1.5170462131500244
Validation loss: 2.181693355242411

Epoch: 6| Step: 3
Training loss: 1.8277901411056519
Validation loss: 2.1863657236099243

Epoch: 6| Step: 4
Training loss: 1.8511290550231934
Validation loss: 2.201743264993032

Epoch: 6| Step: 5
Training loss: 2.0274181365966797
Validation loss: 2.1879387696584067

Epoch: 6| Step: 6
Training loss: 1.9738037586212158
Validation loss: 2.1934718092282615

Epoch: 6| Step: 7
Training loss: 1.6804759502410889
Validation loss: 2.203599969546

Epoch: 6| Step: 8
Training loss: 1.2491059303283691
Validation loss: 2.2014862298965454

Epoch: 6| Step: 9
Training loss: 2.021921157836914
Validation loss: 2.2061678171157837

Epoch: 6| Step: 10
Training loss: 2.7218518257141113
Validation loss: 2.1959455013275146

Epoch: 6| Step: 11
Training loss: 1.5140596628189087
Validation loss: 2.2068832317988076

Epoch: 6| Step: 12
Training loss: 1.5689778327941895
Validation loss: 2.201212724049886

Epoch: 6| Step: 13
Training loss: 1.746411919593811
Validation loss: 2.168642004330953

Epoch: 252| Step: 0
Training loss: 1.741589069366455
Validation loss: 2.171739399433136

Epoch: 6| Step: 1
Training loss: 1.991053581237793
Validation loss: 2.196650822957357

Epoch: 6| Step: 2
Training loss: 1.9011605978012085
Validation loss: 2.1928937832514444

Epoch: 6| Step: 3
Training loss: 1.7433199882507324
Validation loss: 2.211838682492574

Epoch: 6| Step: 4
Training loss: 1.5512537956237793
Validation loss: 2.2144920229911804

Epoch: 6| Step: 5
Training loss: 1.0422093868255615
Validation loss: 2.2064101298650107

Epoch: 6| Step: 6
Training loss: 1.8616832494735718
Validation loss: 2.1951833168665567

Epoch: 6| Step: 7
Training loss: 1.3947930335998535
Validation loss: 2.2025533517201743

Epoch: 6| Step: 8
Training loss: 1.9593768119812012
Validation loss: 2.1976048747698465

Epoch: 6| Step: 9
Training loss: 1.3987951278686523
Validation loss: 2.179036577542623

Epoch: 6| Step: 10
Training loss: 2.459105968475342
Validation loss: 2.1775857408841452

Epoch: 6| Step: 11
Training loss: 1.7548326253890991
Validation loss: 2.175164262453715

Epoch: 6| Step: 12
Training loss: 1.94522225856781
Validation loss: 2.1714239517847695

Epoch: 6| Step: 13
Training loss: 2.846944808959961
Validation loss: 2.177572190761566

Epoch: 253| Step: 0
Training loss: 1.158616065979004
Validation loss: 2.171007672945658

Epoch: 6| Step: 1
Training loss: 1.6113898754119873
Validation loss: 2.178629159927368

Epoch: 6| Step: 2
Training loss: 1.7015999555587769
Validation loss: 2.1726558208465576

Epoch: 6| Step: 3
Training loss: 1.7292399406433105
Validation loss: 2.182341535886129

Epoch: 6| Step: 4
Training loss: 1.7106229066848755
Validation loss: 2.1738658944765725

Epoch: 6| Step: 5
Training loss: 1.5511071681976318
Validation loss: 2.1681936581929526

Epoch: 6| Step: 6
Training loss: 2.2757580280303955
Validation loss: 2.184549927711487

Epoch: 6| Step: 7
Training loss: 1.9206840991973877
Validation loss: 2.176080107688904

Epoch: 6| Step: 8
Training loss: 1.6594971418380737
Validation loss: 2.201850692431132

Epoch: 6| Step: 9
Training loss: 2.2170233726501465
Validation loss: 2.1979138255119324

Epoch: 6| Step: 10
Training loss: 1.581155776977539
Validation loss: 2.2111353476842246

Epoch: 6| Step: 11
Training loss: 1.6034165620803833
Validation loss: 2.2287290493647256

Epoch: 6| Step: 12
Training loss: 2.2198243141174316
Validation loss: 2.212937653064728

Epoch: 6| Step: 13
Training loss: 2.5656070709228516
Validation loss: 2.2245908776919046

Epoch: 254| Step: 0
Training loss: 2.001422882080078
Validation loss: 2.21316397190094

Epoch: 6| Step: 1
Training loss: 1.9156570434570312
Validation loss: 2.2243682146072388

Epoch: 6| Step: 2
Training loss: 1.8547285795211792
Validation loss: 2.181117057800293

Epoch: 6| Step: 3
Training loss: 1.7805818319320679
Validation loss: 2.2227243185043335

Epoch: 6| Step: 4
Training loss: 1.5140936374664307
Validation loss: 2.2054529190063477

Epoch: 6| Step: 5
Training loss: 1.3379238843917847
Validation loss: 2.2064022024472556

Epoch: 6| Step: 6
Training loss: 2.0996837615966797
Validation loss: 2.1730939745903015

Epoch: 6| Step: 7
Training loss: 1.5454576015472412
Validation loss: 2.203582684199015

Epoch: 6| Step: 8
Training loss: 1.408611536026001
Validation loss: 2.2189867893854776

Epoch: 6| Step: 9
Training loss: 1.9592010974884033
Validation loss: 2.199735720952352

Epoch: 6| Step: 10
Training loss: 1.3652493953704834
Validation loss: 2.192232072353363

Epoch: 6| Step: 11
Training loss: 1.511626124382019
Validation loss: 2.225711484750112

Epoch: 6| Step: 12
Training loss: 2.2929701805114746
Validation loss: 2.2064582904179892

Epoch: 6| Step: 13
Training loss: 2.5336532592773438
Validation loss: 2.2292749087015786

Epoch: 255| Step: 0
Training loss: 1.7075755596160889
Validation loss: 2.21526030699412

Epoch: 6| Step: 1
Training loss: 1.4941898584365845
Validation loss: 2.2249489625295005

Epoch: 6| Step: 2
Training loss: 1.031798005104065
Validation loss: 2.223193804423014

Epoch: 6| Step: 3
Training loss: 1.908677339553833
Validation loss: 2.2466994325319924

Epoch: 6| Step: 4
Training loss: 1.6937203407287598
Validation loss: 2.233400742212931

Epoch: 6| Step: 5
Training loss: 2.333772659301758
Validation loss: 2.215794642766317

Epoch: 6| Step: 6
Training loss: 2.196582794189453
Validation loss: 2.219227651755015

Epoch: 6| Step: 7
Training loss: 1.6853656768798828
Validation loss: 2.2060301899909973

Epoch: 6| Step: 8
Training loss: 1.913630723953247
Validation loss: 2.214924693107605

Epoch: 6| Step: 9
Training loss: 2.2069640159606934
Validation loss: 2.195406357447306

Epoch: 6| Step: 10
Training loss: 1.9243450164794922
Validation loss: 2.191826860109965

Epoch: 6| Step: 11
Training loss: 1.6472673416137695
Validation loss: 2.1825263500213623

Epoch: 6| Step: 12
Training loss: 1.7453511953353882
Validation loss: 2.2026272416114807

Epoch: 6| Step: 13
Training loss: 1.7889244556427002
Validation loss: 2.197950065135956

Epoch: 256| Step: 0
Training loss: 2.204928398132324
Validation loss: 2.207125941912333

Epoch: 6| Step: 1
Training loss: 2.111293315887451
Validation loss: 2.2206488450368247

Epoch: 6| Step: 2
Training loss: 1.7697248458862305
Validation loss: 2.218659977118174

Epoch: 6| Step: 3
Training loss: 1.9586290121078491
Validation loss: 2.232043445110321

Epoch: 6| Step: 4
Training loss: 1.4635519981384277
Validation loss: 2.251542011896769

Epoch: 6| Step: 5
Training loss: 1.870607852935791
Validation loss: 2.2404975493748984

Epoch: 6| Step: 6
Training loss: 1.5994906425476074
Validation loss: 2.272915522257487

Epoch: 6| Step: 7
Training loss: 1.62116539478302
Validation loss: 2.251962681611379

Epoch: 6| Step: 8
Training loss: 1.2988271713256836
Validation loss: 2.2290937105814614

Epoch: 6| Step: 9
Training loss: 1.3794386386871338
Validation loss: 2.2480766574541726

Epoch: 6| Step: 10
Training loss: 1.769209623336792
Validation loss: 2.228820562362671

Epoch: 6| Step: 11
Training loss: 2.585360288619995
Validation loss: 2.2589985529581704

Epoch: 6| Step: 12
Training loss: 1.9303467273712158
Validation loss: 2.231931765874227

Epoch: 6| Step: 13
Training loss: 1.6055322885513306
Validation loss: 2.246978481610616

Epoch: 257| Step: 0
Training loss: 1.8819191455841064
Validation loss: 2.243011236190796

Epoch: 6| Step: 1
Training loss: 1.7448365688323975
Validation loss: 2.245717187722524

Epoch: 6| Step: 2
Training loss: 1.5456289052963257
Validation loss: 2.228009740511576

Epoch: 6| Step: 3
Training loss: 1.722343921661377
Validation loss: 2.2022110422452292

Epoch: 6| Step: 4
Training loss: 1.8169409036636353
Validation loss: 2.2152123053868613

Epoch: 6| Step: 5
Training loss: 1.2472970485687256
Validation loss: 2.200378437836965

Epoch: 6| Step: 6
Training loss: 2.0900914669036865
Validation loss: 2.20602293809255

Epoch: 6| Step: 7
Training loss: 1.7388721704483032
Validation loss: 2.1838279167811074

Epoch: 6| Step: 8
Training loss: 1.5969960689544678
Validation loss: 2.150684356689453

Epoch: 6| Step: 9
Training loss: 2.360245704650879
Validation loss: 2.1982098817825317

Epoch: 6| Step: 10
Training loss: 1.5413904190063477
Validation loss: 2.1722834507624307

Epoch: 6| Step: 11
Training loss: 1.6925371885299683
Validation loss: 2.162780523300171

Epoch: 6| Step: 12
Training loss: 1.7006951570510864
Validation loss: 2.165016492207845

Epoch: 6| Step: 13
Training loss: 2.5192437171936035
Validation loss: 2.152636488278707

Epoch: 258| Step: 0
Training loss: 1.132765293121338
Validation loss: 2.1656669775644937

Epoch: 6| Step: 1
Training loss: 1.7587867975234985
Validation loss: 2.197200834751129

Epoch: 6| Step: 2
Training loss: 2.0841660499572754
Validation loss: 2.160122533639272

Epoch: 6| Step: 3
Training loss: 2.147374153137207
Validation loss: 2.1827632784843445

Epoch: 6| Step: 4
Training loss: 1.8205598592758179
Validation loss: 2.174749195575714

Epoch: 6| Step: 5
Training loss: 2.2268760204315186
Validation loss: 2.174386282761892

Epoch: 6| Step: 6
Training loss: 2.557333469390869
Validation loss: 2.193537096182505

Epoch: 6| Step: 7
Training loss: 1.8087995052337646
Validation loss: 2.1873157819112143

Epoch: 6| Step: 8
Training loss: 2.1053314208984375
Validation loss: 2.1843775113423667

Epoch: 6| Step: 9
Training loss: 1.427527666091919
Validation loss: 2.1899112661679587

Epoch: 6| Step: 10
Training loss: 1.3449125289916992
Validation loss: 2.183869262536367

Epoch: 6| Step: 11
Training loss: 1.2953519821166992
Validation loss: 2.178859233856201

Epoch: 6| Step: 12
Training loss: 2.401151657104492
Validation loss: 2.191630025704702

Epoch: 6| Step: 13
Training loss: 1.4627923965454102
Validation loss: 2.219498793284098

Epoch: 259| Step: 0
Training loss: 1.0881528854370117
Validation loss: 2.241572896639506

Epoch: 6| Step: 1
Training loss: 1.4392216205596924
Validation loss: 2.260197877883911

Epoch: 6| Step: 2
Training loss: 2.4106392860412598
Validation loss: 2.2767727375030518

Epoch: 6| Step: 3
Training loss: 1.4859496355056763
Validation loss: 2.258333404858907

Epoch: 6| Step: 4
Training loss: 1.8536310195922852
Validation loss: 2.2430343627929688

Epoch: 6| Step: 5
Training loss: 1.6172823905944824
Validation loss: 2.225372632344564

Epoch: 6| Step: 6
Training loss: 1.4300353527069092
Validation loss: 2.2243446906407676

Epoch: 6| Step: 7
Training loss: 1.807861089706421
Validation loss: 2.2032923698425293

Epoch: 6| Step: 8
Training loss: 1.070631504058838
Validation loss: 2.2161969343821206

Epoch: 6| Step: 9
Training loss: 2.124912977218628
Validation loss: 2.199925740559896

Epoch: 6| Step: 10
Training loss: 2.5432777404785156
Validation loss: 2.210712711016337

Epoch: 6| Step: 11
Training loss: 2.1741676330566406
Validation loss: 2.2155455350875854

Epoch: 6| Step: 12
Training loss: 1.8330837488174438
Validation loss: 2.2259649435679116

Epoch: 6| Step: 13
Training loss: 2.357459306716919
Validation loss: 2.2435573736826577

Epoch: 260| Step: 0
Training loss: 1.266740083694458
Validation loss: 2.2278125286102295

Epoch: 6| Step: 1
Training loss: 1.4976541996002197
Validation loss: 2.246886372566223

Epoch: 6| Step: 2
Training loss: 1.7583364248275757
Validation loss: 2.228454331556956

Epoch: 6| Step: 3
Training loss: 1.86771559715271
Validation loss: 2.246784587701162

Epoch: 6| Step: 4
Training loss: 1.67400062084198
Validation loss: 2.2571975191434226

Epoch: 6| Step: 5
Training loss: 1.9116700887680054
Validation loss: 2.253404458363851

Epoch: 6| Step: 6
Training loss: 1.8967549800872803
Validation loss: 2.274035334587097

Epoch: 6| Step: 7
Training loss: 2.093862295150757
Validation loss: 2.2440982659657798

Epoch: 6| Step: 8
Training loss: 1.7665657997131348
Validation loss: 2.2284411787986755

Epoch: 6| Step: 9
Training loss: 1.65704345703125
Validation loss: 2.247222145398458

Epoch: 6| Step: 10
Training loss: 1.7638897895812988
Validation loss: 2.213801940282186

Epoch: 6| Step: 11
Training loss: 1.8497583866119385
Validation loss: 2.181909759839376

Epoch: 6| Step: 12
Training loss: 2.0294463634490967
Validation loss: 2.18836518128713

Epoch: 6| Step: 13
Training loss: 2.104456663131714
Validation loss: 2.1971827348073325

Epoch: 261| Step: 0
Training loss: 1.7763655185699463
Validation loss: 2.203547398249308

Epoch: 6| Step: 1
Training loss: 1.856342077255249
Validation loss: 2.2134708960851035

Epoch: 6| Step: 2
Training loss: 1.3847174644470215
Validation loss: 2.207981546719869

Epoch: 6| Step: 3
Training loss: 1.672969937324524
Validation loss: 2.2271523078282676

Epoch: 6| Step: 4
Training loss: 2.1012566089630127
Validation loss: 2.241324504216512

Epoch: 6| Step: 5
Training loss: 1.6398617029190063
Validation loss: 2.270189881324768

Epoch: 6| Step: 6
Training loss: 2.1425251960754395
Validation loss: 2.2796608209609985

Epoch: 6| Step: 7
Training loss: 2.434469223022461
Validation loss: 2.2737197478612265

Epoch: 6| Step: 8
Training loss: 2.0827324390411377
Validation loss: 2.2765217224756875

Epoch: 6| Step: 9
Training loss: 2.310206413269043
Validation loss: 2.3015010754267373

Epoch: 6| Step: 10
Training loss: 1.2637627124786377
Validation loss: 2.2474040587743125

Epoch: 6| Step: 11
Training loss: 1.5971945524215698
Validation loss: 2.2385573784510293

Epoch: 6| Step: 12
Training loss: 1.7599081993103027
Validation loss: 2.207938293615977

Epoch: 6| Step: 13
Training loss: 1.5452322959899902
Validation loss: 2.2238530119260154

Epoch: 262| Step: 0
Training loss: 1.476235032081604
Validation loss: 2.2196367184321084

Epoch: 6| Step: 1
Training loss: 2.4946985244750977
Validation loss: 2.1995022098223367

Epoch: 6| Step: 2
Training loss: 1.7456727027893066
Validation loss: 2.2277695536613464

Epoch: 6| Step: 3
Training loss: 1.9782629013061523
Validation loss: 2.212441841761271

Epoch: 6| Step: 4
Training loss: 1.7503278255462646
Validation loss: 2.1950777570406594

Epoch: 6| Step: 5
Training loss: 2.6808266639709473
Validation loss: 2.206859827041626

Epoch: 6| Step: 6
Training loss: 1.9692840576171875
Validation loss: 2.19935534397761

Epoch: 6| Step: 7
Training loss: 1.4968920946121216
Validation loss: 2.1874343355496726

Epoch: 6| Step: 8
Training loss: 2.1815757751464844
Validation loss: 2.1850645542144775

Epoch: 6| Step: 9
Training loss: 1.995341181755066
Validation loss: 2.1953552961349487

Epoch: 6| Step: 10
Training loss: 1.515320062637329
Validation loss: 2.201402187347412

Epoch: 6| Step: 11
Training loss: 2.2367451190948486
Validation loss: 2.222173273563385

Epoch: 6| Step: 12
Training loss: 1.3782893419265747
Validation loss: 2.231997311115265

Epoch: 6| Step: 13
Training loss: 1.6924996376037598
Validation loss: 2.210609217484792

Epoch: 263| Step: 0
Training loss: 1.9798285961151123
Validation loss: 2.1988726456960044

Epoch: 6| Step: 1
Training loss: 1.6267530918121338
Validation loss: 2.193865199883779

Epoch: 6| Step: 2
Training loss: 1.9241939783096313
Validation loss: 2.192592899004618

Epoch: 6| Step: 3
Training loss: 2.010913133621216
Validation loss: 2.1913649638493857

Epoch: 6| Step: 4
Training loss: 1.8575563430786133
Validation loss: 2.2009347875912986

Epoch: 6| Step: 5
Training loss: 1.5014450550079346
Validation loss: 2.1824127435684204

Epoch: 6| Step: 6
Training loss: 1.903564214706421
Validation loss: 2.1685410936673484

Epoch: 6| Step: 7
Training loss: 2.0652639865875244
Validation loss: 2.1813757022221885

Epoch: 6| Step: 8
Training loss: 1.9476516246795654
Validation loss: 2.179322342077891

Epoch: 6| Step: 9
Training loss: 1.4500079154968262
Validation loss: 2.174982031186422

Epoch: 6| Step: 10
Training loss: 2.4348537921905518
Validation loss: 2.193382898966471

Epoch: 6| Step: 11
Training loss: 2.2645955085754395
Validation loss: 2.189130743344625

Epoch: 6| Step: 12
Training loss: 1.5648657083511353
Validation loss: 2.2154688835144043

Epoch: 6| Step: 13
Training loss: 1.2400273084640503
Validation loss: 2.1823235551516214

Epoch: 264| Step: 0
Training loss: 1.9871306419372559
Validation loss: 2.210750162601471

Epoch: 6| Step: 1
Training loss: 1.462904453277588
Validation loss: 2.2019694248835244

Epoch: 6| Step: 2
Training loss: 2.147249460220337
Validation loss: 2.1996294061342874

Epoch: 6| Step: 3
Training loss: 1.5595474243164062
Validation loss: 2.188622832298279

Epoch: 6| Step: 4
Training loss: 1.867325782775879
Validation loss: 2.165188709894816

Epoch: 6| Step: 5
Training loss: 1.3912954330444336
Validation loss: 2.1723201672236123

Epoch: 6| Step: 6
Training loss: 1.698464035987854
Validation loss: 2.1790154178937278

Epoch: 6| Step: 7
Training loss: 2.638418674468994
Validation loss: 2.1952128807703652

Epoch: 6| Step: 8
Training loss: 1.645700454711914
Validation loss: 2.1691590746243796

Epoch: 6| Step: 9
Training loss: 1.7073454856872559
Validation loss: 2.154195547103882

Epoch: 6| Step: 10
Training loss: 1.9066357612609863
Validation loss: 2.179977456728617

Epoch: 6| Step: 11
Training loss: 1.4014427661895752
Validation loss: 2.1657076478004456

Epoch: 6| Step: 12
Training loss: 1.588144302368164
Validation loss: 2.191340744495392

Epoch: 6| Step: 13
Training loss: 2.044304847717285
Validation loss: 2.1735713879267373

Epoch: 265| Step: 0
Training loss: 1.6551108360290527
Validation loss: 2.1931126912434897

Epoch: 6| Step: 1
Training loss: 1.7419570684432983
Validation loss: 2.254272977511088

Epoch: 6| Step: 2
Training loss: 0.9903508424758911
Validation loss: 2.26995579401652

Epoch: 6| Step: 3
Training loss: 2.4389777183532715
Validation loss: 2.2855833967526755

Epoch: 6| Step: 4
Training loss: 1.9314521551132202
Validation loss: 2.286999066670736

Epoch: 6| Step: 5
Training loss: 1.425572156906128
Validation loss: 2.289409101009369

Epoch: 6| Step: 6
Training loss: 2.481149673461914
Validation loss: 2.28340882062912

Epoch: 6| Step: 7
Training loss: 2.1557798385620117
Validation loss: 2.288065175215403

Epoch: 6| Step: 8
Training loss: 1.4099810123443604
Validation loss: 2.2552244861920676

Epoch: 6| Step: 9
Training loss: 2.139500617980957
Validation loss: 2.228941798210144

Epoch: 6| Step: 10
Training loss: 1.413098931312561
Validation loss: 2.1911223332087197

Epoch: 6| Step: 11
Training loss: 1.5176905393600464
Validation loss: 2.1889153718948364

Epoch: 6| Step: 12
Training loss: 1.419513463973999
Validation loss: 2.189245283603668

Epoch: 6| Step: 13
Training loss: 2.7621397972106934
Validation loss: 2.187867025534312

Epoch: 266| Step: 0
Training loss: 2.5084176063537598
Validation loss: 2.1782314777374268

Epoch: 6| Step: 1
Training loss: 1.9065608978271484
Validation loss: 2.191579023996989

Epoch: 6| Step: 2
Training loss: 1.8906010389328003
Validation loss: 2.193359931310018

Epoch: 6| Step: 3
Training loss: 2.087407112121582
Validation loss: 2.170321802298228

Epoch: 6| Step: 4
Training loss: 1.7233798503875732
Validation loss: 2.1863059997558594

Epoch: 6| Step: 5
Training loss: 1.5705530643463135
Validation loss: 2.170641303062439

Epoch: 6| Step: 6
Training loss: 2.088165044784546
Validation loss: 2.179340382417043

Epoch: 6| Step: 7
Training loss: 2.1937167644500732
Validation loss: 2.167521913846334

Epoch: 6| Step: 8
Training loss: 1.7264827489852905
Validation loss: 2.1760777036348977

Epoch: 6| Step: 9
Training loss: 1.6186606884002686
Validation loss: 2.199219842751821

Epoch: 6| Step: 10
Training loss: 1.7487330436706543
Validation loss: 2.217728594938914

Epoch: 6| Step: 11
Training loss: 1.3677902221679688
Validation loss: 2.2365734775861106

Epoch: 6| Step: 12
Training loss: 1.6683017015457153
Validation loss: 2.2816352248191833

Epoch: 6| Step: 13
Training loss: 1.9218082427978516
Validation loss: 2.2730748653411865

Epoch: 267| Step: 0
Training loss: 1.9354281425476074
Validation loss: 2.286569674809774

Epoch: 6| Step: 1
Training loss: 1.6481980085372925
Validation loss: 2.3036059737205505

Epoch: 6| Step: 2
Training loss: 2.094460964202881
Validation loss: 2.2662786642710366

Epoch: 6| Step: 3
Training loss: 1.7917189598083496
Validation loss: 2.2782159447669983

Epoch: 6| Step: 4
Training loss: 1.481762409210205
Validation loss: 2.292274455229441

Epoch: 6| Step: 5
Training loss: 2.289727210998535
Validation loss: 2.3137108087539673

Epoch: 6| Step: 6
Training loss: 1.569271206855774
Validation loss: 2.3002206881841025

Epoch: 6| Step: 7
Training loss: 1.6140027046203613
Validation loss: 2.2509533166885376

Epoch: 6| Step: 8
Training loss: 1.9924886226654053
Validation loss: 2.2610612511634827

Epoch: 6| Step: 9
Training loss: 2.4070241451263428
Validation loss: 2.221888542175293

Epoch: 6| Step: 10
Training loss: 1.9554314613342285
Validation loss: 2.21493669350942

Epoch: 6| Step: 11
Training loss: 1.5510081052780151
Validation loss: 2.2158720890680947

Epoch: 6| Step: 12
Training loss: 1.813124179840088
Validation loss: 2.202647944291433

Epoch: 6| Step: 13
Training loss: 2.382777690887451
Validation loss: 2.1938472986221313

Epoch: 268| Step: 0
Training loss: 2.155087471008301
Validation loss: 2.195853114128113

Epoch: 6| Step: 1
Training loss: 1.639160394668579
Validation loss: 2.177336553732554

Epoch: 6| Step: 2
Training loss: 2.3290116786956787
Validation loss: 2.198371469974518

Epoch: 6| Step: 3
Training loss: 2.6497371196746826
Validation loss: 2.2126431663831077

Epoch: 6| Step: 4
Training loss: 1.5182443857192993
Validation loss: 2.1901575922966003

Epoch: 6| Step: 5
Training loss: 1.8325215578079224
Validation loss: 2.2053635716438293

Epoch: 6| Step: 6
Training loss: 1.9746640920639038
Validation loss: 2.199265480041504

Epoch: 6| Step: 7
Training loss: 1.6171647310256958
Validation loss: 2.207469940185547

Epoch: 6| Step: 8
Training loss: 1.865625262260437
Validation loss: 2.206422766049703

Epoch: 6| Step: 9
Training loss: 1.5782530307769775
Validation loss: 2.197559654712677

Epoch: 6| Step: 10
Training loss: 1.670135498046875
Validation loss: 2.214712301890055

Epoch: 6| Step: 11
Training loss: 1.71712327003479
Validation loss: 2.20786315202713

Epoch: 6| Step: 12
Training loss: 2.06807279586792
Validation loss: 2.225361923376719

Epoch: 6| Step: 13
Training loss: 2.499462127685547
Validation loss: 2.2420228322347007

Epoch: 269| Step: 0
Training loss: 1.4043562412261963
Validation loss: 2.2220835089683533

Epoch: 6| Step: 1
Training loss: 1.9562714099884033
Validation loss: 2.2640179793039956

Epoch: 6| Step: 2
Training loss: 1.852405071258545
Validation loss: 2.2555423974990845

Epoch: 6| Step: 3
Training loss: 1.8255653381347656
Validation loss: 2.264127771059672

Epoch: 6| Step: 4
Training loss: 1.8424913883209229
Validation loss: 2.255862593650818

Epoch: 6| Step: 5
Training loss: 1.9684545993804932
Validation loss: 2.242818037668864

Epoch: 6| Step: 6
Training loss: 2.1563408374786377
Validation loss: 2.243345340092977

Epoch: 6| Step: 7
Training loss: 1.5175862312316895
Validation loss: 2.2178688446680703

Epoch: 6| Step: 8
Training loss: 1.8716223239898682
Validation loss: 2.180859386920929

Epoch: 6| Step: 9
Training loss: 1.3352750539779663
Validation loss: 2.185927907625834

Epoch: 6| Step: 10
Training loss: 2.0596208572387695
Validation loss: 2.1595402558644614

Epoch: 6| Step: 11
Training loss: 1.8409593105316162
Validation loss: 2.1782291531562805

Epoch: 6| Step: 12
Training loss: 2.13019061088562
Validation loss: 2.1823853651682534

Epoch: 6| Step: 13
Training loss: 1.1838397979736328
Validation loss: 2.1616501410802207

Epoch: 270| Step: 0
Training loss: 1.8743315935134888
Validation loss: 2.187148849169413

Epoch: 6| Step: 1
Training loss: 2.4962172508239746
Validation loss: 2.235839049021403

Epoch: 6| Step: 2
Training loss: 2.1919660568237305
Validation loss: 2.2316736380259194

Epoch: 6| Step: 3
Training loss: 2.021857500076294
Validation loss: 2.2471338907877603

Epoch: 6| Step: 4
Training loss: 1.7614156007766724
Validation loss: 2.222791870435079

Epoch: 6| Step: 5
Training loss: 1.7081857919692993
Validation loss: 2.2068636218706765

Epoch: 6| Step: 6
Training loss: 1.5239198207855225
Validation loss: 2.198631485303243

Epoch: 6| Step: 7
Training loss: 1.2504262924194336
Validation loss: 2.1982980569203696

Epoch: 6| Step: 8
Training loss: 2.1664209365844727
Validation loss: 2.196226934591929

Epoch: 6| Step: 9
Training loss: 1.3408575057983398
Validation loss: 2.1851919293403625

Epoch: 6| Step: 10
Training loss: 1.7665280103683472
Validation loss: 2.2143059372901917

Epoch: 6| Step: 11
Training loss: 1.8690729141235352
Validation loss: 2.2282564640045166

Epoch: 6| Step: 12
Training loss: 1.317439079284668
Validation loss: 2.204100807507833

Epoch: 6| Step: 13
Training loss: 2.116621494293213
Validation loss: 2.1882869402567544

Epoch: 271| Step: 0
Training loss: 1.7715675830841064
Validation loss: 2.229255179564158

Epoch: 6| Step: 1
Training loss: 1.8524333238601685
Validation loss: 2.226946850617727

Epoch: 6| Step: 2
Training loss: 2.0432181358337402
Validation loss: 2.2106195290883384

Epoch: 6| Step: 3
Training loss: 2.063246250152588
Validation loss: 2.2362020214398703

Epoch: 6| Step: 4
Training loss: 1.8566447496414185
Validation loss: 2.226533055305481

Epoch: 6| Step: 5
Training loss: 1.7033498287200928
Validation loss: 2.2285720109939575

Epoch: 6| Step: 6
Training loss: 1.7849633693695068
Validation loss: 2.2556776801745095

Epoch: 6| Step: 7
Training loss: 2.0554752349853516
Validation loss: 2.265398899714152

Epoch: 6| Step: 8
Training loss: 1.159470558166504
Validation loss: 2.248019556204478

Epoch: 6| Step: 9
Training loss: 1.4194953441619873
Validation loss: 2.2511502703030906

Epoch: 6| Step: 10
Training loss: 1.6338205337524414
Validation loss: 2.2469617128372192

Epoch: 6| Step: 11
Training loss: 1.8110380172729492
Validation loss: 2.2464789549509683

Epoch: 6| Step: 12
Training loss: 1.798332929611206
Validation loss: 2.2569053173065186

Epoch: 6| Step: 13
Training loss: 1.7892451286315918
Validation loss: 2.2280142505963645

Epoch: 272| Step: 0
Training loss: 1.7775871753692627
Validation loss: 2.2365589141845703

Epoch: 6| Step: 1
Training loss: 1.880929946899414
Validation loss: 2.245576878388723

Epoch: 6| Step: 2
Training loss: 1.9477286338806152
Validation loss: 2.203469753265381

Epoch: 6| Step: 3
Training loss: 2.1853373050689697
Validation loss: 2.2266393105189004

Epoch: 6| Step: 4
Training loss: 1.660875678062439
Validation loss: 2.190042734146118

Epoch: 6| Step: 5
Training loss: 1.581160306930542
Validation loss: 2.2140175104141235

Epoch: 6| Step: 6
Training loss: 0.9899316430091858
Validation loss: 2.2037054697672525

Epoch: 6| Step: 7
Training loss: 1.7506734132766724
Validation loss: 2.204520662625631

Epoch: 6| Step: 8
Training loss: 1.6958701610565186
Validation loss: 2.2460471789042153

Epoch: 6| Step: 9
Training loss: 1.7986587285995483
Validation loss: 2.246386726697286

Epoch: 6| Step: 10
Training loss: 1.5116691589355469
Validation loss: 2.264818847179413

Epoch: 6| Step: 11
Training loss: 2.207444190979004
Validation loss: 2.2538057764371238

Epoch: 6| Step: 12
Training loss: 1.522010326385498
Validation loss: 2.262937545776367

Epoch: 6| Step: 13
Training loss: 2.84226655960083
Validation loss: 2.24536923567454

Epoch: 273| Step: 0
Training loss: 2.148564338684082
Validation loss: 2.267819404602051

Epoch: 6| Step: 1
Training loss: 1.7043604850769043
Validation loss: 2.2585204442342124

Epoch: 6| Step: 2
Training loss: 1.5103918313980103
Validation loss: 2.2355321645736694

Epoch: 6| Step: 3
Training loss: 1.6168962717056274
Validation loss: 2.251705765724182

Epoch: 6| Step: 4
Training loss: 1.5593006610870361
Validation loss: 2.22862046957016

Epoch: 6| Step: 5
Training loss: 1.6813721656799316
Validation loss: 2.204504410425822

Epoch: 6| Step: 6
Training loss: 1.8285362720489502
Validation loss: 2.214499076207479

Epoch: 6| Step: 7
Training loss: 1.3478199243545532
Validation loss: 2.22034752368927

Epoch: 6| Step: 8
Training loss: 1.917649269104004
Validation loss: 2.1957947810490928

Epoch: 6| Step: 9
Training loss: 1.858554482460022
Validation loss: 2.199610491593679

Epoch: 6| Step: 10
Training loss: 1.8982380628585815
Validation loss: 2.212321857611338

Epoch: 6| Step: 11
Training loss: 2.102428436279297
Validation loss: 2.206733822822571

Epoch: 6| Step: 12
Training loss: 1.8324223756790161
Validation loss: 2.1957838336626687

Epoch: 6| Step: 13
Training loss: 2.157609462738037
Validation loss: 2.1934289932250977

Epoch: 274| Step: 0
Training loss: 1.7555497884750366
Validation loss: 2.205894947052002

Epoch: 6| Step: 1
Training loss: 1.955021858215332
Validation loss: 2.2202176054318747

Epoch: 6| Step: 2
Training loss: 1.8512365818023682
Validation loss: 2.23458194732666

Epoch: 6| Step: 3
Training loss: 1.4454761743545532
Validation loss: 2.1997116605440774

Epoch: 6| Step: 4
Training loss: 2.057591199874878
Validation loss: 2.1840116580327353

Epoch: 6| Step: 5
Training loss: 2.293950080871582
Validation loss: 2.184968968232473

Epoch: 6| Step: 6
Training loss: 2.08554744720459
Validation loss: 2.158114790916443

Epoch: 6| Step: 7
Training loss: 1.9417954683303833
Validation loss: 2.1578712860743203

Epoch: 6| Step: 8
Training loss: 1.665825366973877
Validation loss: 2.1733943819999695

Epoch: 6| Step: 9
Training loss: 1.4623347520828247
Validation loss: 2.172384798526764

Epoch: 6| Step: 10
Training loss: 2.0513057708740234
Validation loss: 2.1605913043022156

Epoch: 6| Step: 11
Training loss: 1.269228219985962
Validation loss: 2.1772531867027283

Epoch: 6| Step: 12
Training loss: 1.213533878326416
Validation loss: 2.1655263106028237

Epoch: 6| Step: 13
Training loss: 2.0032777786254883
Validation loss: 2.2075581550598145

Epoch: 275| Step: 0
Training loss: 2.3382859230041504
Validation loss: 2.230561852455139

Epoch: 6| Step: 1
Training loss: 1.5513606071472168
Validation loss: 2.231837193171183

Epoch: 6| Step: 2
Training loss: 2.188237190246582
Validation loss: 2.2321258385976157

Epoch: 6| Step: 3
Training loss: 2.080435276031494
Validation loss: 2.227705160776774

Epoch: 6| Step: 4
Training loss: 1.519430160522461
Validation loss: 2.2207887967427573

Epoch: 6| Step: 5
Training loss: 2.53971004486084
Validation loss: 2.238898495833079

Epoch: 6| Step: 6
Training loss: 1.7022918462753296
Validation loss: 2.2237230936686196

Epoch: 6| Step: 7
Training loss: 1.5900962352752686
Validation loss: 2.24673459927241

Epoch: 6| Step: 8
Training loss: 1.322526454925537
Validation loss: 2.2339198191960654

Epoch: 6| Step: 9
Training loss: 1.8753799200057983
Validation loss: 2.2293741703033447

Epoch: 6| Step: 10
Training loss: 1.1981136798858643
Validation loss: 2.218778888384501

Epoch: 6| Step: 11
Training loss: 1.2251050472259521
Validation loss: 2.197814126809438

Epoch: 6| Step: 12
Training loss: 1.619415044784546
Validation loss: 2.2262195348739624

Epoch: 6| Step: 13
Training loss: 2.0484843254089355
Validation loss: 2.2211360931396484

Epoch: 276| Step: 0
Training loss: 1.6712908744812012
Validation loss: 2.1895553867022195

Epoch: 6| Step: 1
Training loss: 1.5792620182037354
Validation loss: 2.2019992669423423

Epoch: 6| Step: 2
Training loss: 1.0687992572784424
Validation loss: 2.1676130890846252

Epoch: 6| Step: 3
Training loss: 2.107330799102783
Validation loss: 2.1758351723353067

Epoch: 6| Step: 4
Training loss: 2.474581718444824
Validation loss: 2.18379282951355

Epoch: 6| Step: 5
Training loss: 2.0948572158813477
Validation loss: 2.1829280853271484

Epoch: 6| Step: 6
Training loss: 1.8752837181091309
Validation loss: 2.174063205718994

Epoch: 6| Step: 7
Training loss: 2.1202425956726074
Validation loss: 2.1894742449124656

Epoch: 6| Step: 8
Training loss: 1.4701719284057617
Validation loss: 2.1759491562843323

Epoch: 6| Step: 9
Training loss: 1.838719367980957
Validation loss: 2.198228418827057

Epoch: 6| Step: 10
Training loss: 1.5375100374221802
Validation loss: 2.213692585627238

Epoch: 6| Step: 11
Training loss: 2.0714828968048096
Validation loss: 2.2001470923423767

Epoch: 6| Step: 12
Training loss: 1.3496910333633423
Validation loss: 2.2104425628980002

Epoch: 6| Step: 13
Training loss: 1.8946648836135864
Validation loss: 2.1916587352752686

Epoch: 277| Step: 0
Training loss: 1.923835039138794
Validation loss: 2.1989200909932456

Epoch: 6| Step: 1
Training loss: 1.6665120124816895
Validation loss: 2.211972653865814

Epoch: 6| Step: 2
Training loss: 2.0205979347229004
Validation loss: 2.2334494392077127

Epoch: 6| Step: 3
Training loss: 1.9961515665054321
Validation loss: 2.2252813975016275

Epoch: 6| Step: 4
Training loss: 1.3484032154083252
Validation loss: 2.2136013905207315

Epoch: 6| Step: 5
Training loss: 1.5948822498321533
Validation loss: 2.2250123620033264

Epoch: 6| Step: 6
Training loss: 1.6700994968414307
Validation loss: 2.2183928887049356

Epoch: 6| Step: 7
Training loss: 2.049097776412964
Validation loss: 2.229378879070282

Epoch: 6| Step: 8
Training loss: 2.04604434967041
Validation loss: 2.203959107398987

Epoch: 6| Step: 9
Training loss: 1.035582423210144
Validation loss: 2.2303919593493142

Epoch: 6| Step: 10
Training loss: 1.6312062740325928
Validation loss: 2.261124769846598

Epoch: 6| Step: 11
Training loss: 1.3787620067596436
Validation loss: 2.228861073652903

Epoch: 6| Step: 12
Training loss: 2.290510654449463
Validation loss: 2.2557029128074646

Epoch: 6| Step: 13
Training loss: 2.190600872039795
Validation loss: 2.288783093293508

Epoch: 278| Step: 0
Training loss: 1.405515432357788
Validation loss: 2.2849087715148926

Epoch: 6| Step: 1
Training loss: 1.251806378364563
Validation loss: 2.268944561481476

Epoch: 6| Step: 2
Training loss: 1.6519737243652344
Validation loss: 2.299812396367391

Epoch: 6| Step: 3
Training loss: 1.7387712001800537
Validation loss: 2.2601881424585977

Epoch: 6| Step: 4
Training loss: 1.630637764930725
Validation loss: 2.2341418266296387

Epoch: 6| Step: 5
Training loss: 1.9328103065490723
Validation loss: 2.2293758591016135

Epoch: 6| Step: 6
Training loss: 2.7036185264587402
Validation loss: 2.2512585123380027

Epoch: 6| Step: 7
Training loss: 2.2113542556762695
Validation loss: 2.241871456305186

Epoch: 6| Step: 8
Training loss: 1.424399495124817
Validation loss: 2.2503434816996255

Epoch: 6| Step: 9
Training loss: 2.3539485931396484
Validation loss: 2.220174193382263

Epoch: 6| Step: 10
Training loss: 2.1713786125183105
Validation loss: 2.2653398315111795

Epoch: 6| Step: 11
Training loss: 1.1102113723754883
Validation loss: 2.254897932211558

Epoch: 6| Step: 12
Training loss: 2.1179592609405518
Validation loss: 2.2541564305623374

Epoch: 6| Step: 13
Training loss: 1.1018484830856323
Validation loss: 2.320642669995626

Epoch: 279| Step: 0
Training loss: 1.0055197477340698
Validation loss: 2.286367575327555

Epoch: 6| Step: 1
Training loss: 1.4381208419799805
Validation loss: 2.287059267361959

Epoch: 6| Step: 2
Training loss: 2.474581241607666
Validation loss: 2.2662692070007324

Epoch: 6| Step: 3
Training loss: 2.067256450653076
Validation loss: 2.263711988925934

Epoch: 6| Step: 4
Training loss: 2.342529773712158
Validation loss: 2.274418512980143

Epoch: 6| Step: 5
Training loss: 2.26759672164917
Validation loss: 2.239196300506592

Epoch: 6| Step: 6
Training loss: 1.96058988571167
Validation loss: 2.2371994058291116

Epoch: 6| Step: 7
Training loss: 1.456989049911499
Validation loss: 2.2142738898595176

Epoch: 6| Step: 8
Training loss: 1.5925707817077637
Validation loss: 2.2302961945533752

Epoch: 6| Step: 9
Training loss: 1.5018702745437622
Validation loss: 2.224580486615499

Epoch: 6| Step: 10
Training loss: 1.784956455230713
Validation loss: 2.212363123893738

Epoch: 6| Step: 11
Training loss: 1.5939412117004395
Validation loss: 2.2169922987620034

Epoch: 6| Step: 12
Training loss: 1.5731658935546875
Validation loss: 2.2103174924850464

Epoch: 6| Step: 13
Training loss: 1.8010118007659912
Validation loss: 2.2334877252578735

Epoch: 280| Step: 0
Training loss: 1.4639570713043213
Validation loss: 2.2638316551844277

Epoch: 6| Step: 1
Training loss: 1.8932294845581055
Validation loss: 2.213875412940979

Epoch: 6| Step: 2
Training loss: 1.8445128202438354
Validation loss: 2.269683917363485

Epoch: 6| Step: 3
Training loss: 1.3943681716918945
Validation loss: 2.251929541428884

Epoch: 6| Step: 4
Training loss: 1.5703580379486084
Validation loss: 2.2658037741978965

Epoch: 6| Step: 5
Training loss: 1.4035396575927734
Validation loss: 2.2418111761411033

Epoch: 6| Step: 6
Training loss: 1.5539095401763916
Validation loss: 2.21464333931605

Epoch: 6| Step: 7
Training loss: 1.7159992456436157
Validation loss: 2.2281093994776406

Epoch: 6| Step: 8
Training loss: 2.0985944271087646
Validation loss: 2.2032395203908286

Epoch: 6| Step: 9
Training loss: 2.0155763626098633
Validation loss: 2.1836361090342202

Epoch: 6| Step: 10
Training loss: 2.068467140197754
Validation loss: 2.1902774771054587

Epoch: 6| Step: 11
Training loss: 2.4939565658569336
Validation loss: 2.1867624123891196

Epoch: 6| Step: 12
Training loss: 1.3954025506973267
Validation loss: 2.2103020350138345

Epoch: 6| Step: 13
Training loss: 2.3388993740081787
Validation loss: 2.219907601674398

Epoch: 281| Step: 0
Training loss: 1.1895751953125
Validation loss: 2.2379988034566245

Epoch: 6| Step: 1
Training loss: 1.4657161235809326
Validation loss: 2.2610687017440796

Epoch: 6| Step: 2
Training loss: 1.5533502101898193
Validation loss: 2.261509676774343

Epoch: 6| Step: 3
Training loss: 1.3479242324829102
Validation loss: 2.2783698240915933

Epoch: 6| Step: 4
Training loss: 1.198062539100647
Validation loss: 2.284944216410319

Epoch: 6| Step: 5
Training loss: 1.2270283699035645
Validation loss: 2.252935290336609

Epoch: 6| Step: 6
Training loss: 1.746795892715454
Validation loss: 2.243110418319702

Epoch: 6| Step: 7
Training loss: 2.6263387203216553
Validation loss: 2.2240161101023355

Epoch: 6| Step: 8
Training loss: 2.7027926445007324
Validation loss: 2.2197205225626626

Epoch: 6| Step: 9
Training loss: 1.5621957778930664
Validation loss: 2.219668745994568

Epoch: 6| Step: 10
Training loss: 1.5320453643798828
Validation loss: 2.2180540561676025

Epoch: 6| Step: 11
Training loss: 2.131913185119629
Validation loss: 2.220297952493032

Epoch: 6| Step: 12
Training loss: 1.492979645729065
Validation loss: 2.2189729809761047

Epoch: 6| Step: 13
Training loss: 2.5144405364990234
Validation loss: 2.2169315218925476

Epoch: 282| Step: 0
Training loss: 1.344313383102417
Validation loss: 2.237682064374288

Epoch: 6| Step: 1
Training loss: 1.653762698173523
Validation loss: 2.2237141529719033

Epoch: 6| Step: 2
Training loss: 2.1687841415405273
Validation loss: 2.212190349896749

Epoch: 6| Step: 3
Training loss: 1.6479361057281494
Validation loss: 2.201162417729696

Epoch: 6| Step: 4
Training loss: 1.7212820053100586
Validation loss: 2.219623545805613

Epoch: 6| Step: 5
Training loss: 1.5952895879745483
Validation loss: 2.213164746761322

Epoch: 6| Step: 6
Training loss: 1.549422264099121
Validation loss: 2.1900119185447693

Epoch: 6| Step: 7
Training loss: 1.6572269201278687
Validation loss: 2.1966007947921753

Epoch: 6| Step: 8
Training loss: 1.5523267984390259
Validation loss: 2.2170941829681396

Epoch: 6| Step: 9
Training loss: 1.9991075992584229
Validation loss: 2.20731782913208

Epoch: 6| Step: 10
Training loss: 1.7721470594406128
Validation loss: 2.2044546206792197

Epoch: 6| Step: 11
Training loss: 1.997475504875183
Validation loss: 2.222022593021393

Epoch: 6| Step: 12
Training loss: 1.61994469165802
Validation loss: 2.233612855275472

Epoch: 6| Step: 13
Training loss: 2.523937225341797
Validation loss: 2.24179474512736

Epoch: 283| Step: 0
Training loss: 1.390740156173706
Validation loss: 2.259997765223185

Epoch: 6| Step: 1
Training loss: 1.5485997200012207
Validation loss: 2.2772690455118814

Epoch: 6| Step: 2
Training loss: 1.6228139400482178
Validation loss: 2.297425409158071

Epoch: 6| Step: 3
Training loss: 1.6293277740478516
Validation loss: 2.2815332214037576

Epoch: 6| Step: 4
Training loss: 1.8429553508758545
Validation loss: 2.2706343730290732

Epoch: 6| Step: 5
Training loss: 1.7806544303894043
Validation loss: 2.262090504169464

Epoch: 6| Step: 6
Training loss: 1.857871413230896
Validation loss: 2.237419525782267

Epoch: 6| Step: 7
Training loss: 1.8277229070663452
Validation loss: 2.201774517695109

Epoch: 6| Step: 8
Training loss: 2.1229329109191895
Validation loss: 2.225361406803131

Epoch: 6| Step: 9
Training loss: 1.2684087753295898
Validation loss: 2.2143220702807107

Epoch: 6| Step: 10
Training loss: 1.8108961582183838
Validation loss: 2.2596481243769326

Epoch: 6| Step: 11
Training loss: 2.331533432006836
Validation loss: 2.2606781323750815

Epoch: 6| Step: 12
Training loss: 2.2711410522460938
Validation loss: 2.2701753775278726

Epoch: 6| Step: 13
Training loss: 1.827451467514038
Validation loss: 2.2846781611442566

Epoch: 284| Step: 0
Training loss: 1.743042230606079
Validation loss: 2.288448135058085

Epoch: 6| Step: 1
Training loss: 1.479815125465393
Validation loss: 2.293066680431366

Epoch: 6| Step: 2
Training loss: 1.4052844047546387
Validation loss: 2.2972249388694763

Epoch: 6| Step: 3
Training loss: 1.7080025672912598
Validation loss: 2.2749510208765664

Epoch: 6| Step: 4
Training loss: 1.8407819271087646
Validation loss: 2.2572667002677917

Epoch: 6| Step: 5
Training loss: 1.7066895961761475
Validation loss: 2.247126837571462

Epoch: 6| Step: 6
Training loss: 1.5523202419281006
Validation loss: 2.246592899163564

Epoch: 6| Step: 7
Training loss: 1.8814024925231934
Validation loss: 2.219568411509196

Epoch: 6| Step: 8
Training loss: 1.9038755893707275
Validation loss: 2.2314087947209678

Epoch: 6| Step: 9
Training loss: 1.1475961208343506
Validation loss: 2.2287113070487976

Epoch: 6| Step: 10
Training loss: 1.9677010774612427
Validation loss: 2.2540444135665894

Epoch: 6| Step: 11
Training loss: 1.8096532821655273
Validation loss: 2.2345951994260154

Epoch: 6| Step: 12
Training loss: 2.424525022506714
Validation loss: 2.1980141599973044

Epoch: 6| Step: 13
Training loss: 1.9763420820236206
Validation loss: 2.2374610702196756

Epoch: 285| Step: 0
Training loss: 2.357717514038086
Validation loss: 2.2610405484835305

Epoch: 6| Step: 1
Training loss: 1.8464138507843018
Validation loss: 2.2416635354359946

Epoch: 6| Step: 2
Training loss: 1.704662561416626
Validation loss: 2.2662672201792398

Epoch: 6| Step: 3
Training loss: 1.605783462524414
Validation loss: 2.2527517080307007

Epoch: 6| Step: 4
Training loss: 1.7773401737213135
Validation loss: 2.2375561594963074

Epoch: 6| Step: 5
Training loss: 1.966414451599121
Validation loss: 2.2703505953152976

Epoch: 6| Step: 6
Training loss: 1.7126805782318115
Validation loss: 2.234436492125193

Epoch: 6| Step: 7
Training loss: 0.9636454582214355
Validation loss: 2.23045019308726

Epoch: 6| Step: 8
Training loss: 1.7987308502197266
Validation loss: 2.2341837088267007

Epoch: 6| Step: 9
Training loss: 1.5941493511199951
Validation loss: 2.2209166089693704

Epoch: 6| Step: 10
Training loss: 1.4081510305404663
Validation loss: 2.2262802720069885

Epoch: 6| Step: 11
Training loss: 1.7117500305175781
Validation loss: 2.213124175866445

Epoch: 6| Step: 12
Training loss: 2.192080497741699
Validation loss: 2.20464160044988

Epoch: 6| Step: 13
Training loss: 1.9902812242507935
Validation loss: 2.1998005906740823

Epoch: 286| Step: 0
Training loss: 1.1045290231704712
Validation loss: 2.2095806996027627

Epoch: 6| Step: 1
Training loss: 2.213228940963745
Validation loss: 2.210057020187378

Epoch: 6| Step: 2
Training loss: 1.9963843822479248
Validation loss: 2.2015973726908364

Epoch: 6| Step: 3
Training loss: 2.092010498046875
Validation loss: 2.228436827659607

Epoch: 6| Step: 4
Training loss: 1.4217865467071533
Validation loss: 2.219762603441874

Epoch: 6| Step: 5
Training loss: 2.0265321731567383
Validation loss: 2.24799245595932

Epoch: 6| Step: 6
Training loss: 2.179593563079834
Validation loss: 2.216150641441345

Epoch: 6| Step: 7
Training loss: 0.9367942214012146
Validation loss: 2.2160723209381104

Epoch: 6| Step: 8
Training loss: 2.381993293762207
Validation loss: 2.2148998181025186

Epoch: 6| Step: 9
Training loss: 1.8446921110153198
Validation loss: 2.230475982030233

Epoch: 6| Step: 10
Training loss: 1.4210929870605469
Validation loss: 2.2025925517082214

Epoch: 6| Step: 11
Training loss: 1.8028123378753662
Validation loss: 2.204891641934713

Epoch: 6| Step: 12
Training loss: 1.256404995918274
Validation loss: 2.189665754636129

Epoch: 6| Step: 13
Training loss: 1.7551994323730469
Validation loss: 2.2358835140864053

Epoch: 287| Step: 0
Training loss: 1.092602252960205
Validation loss: 2.19090743859609

Epoch: 6| Step: 1
Training loss: 1.385319709777832
Validation loss: 2.2307225664456687

Epoch: 6| Step: 2
Training loss: 1.6976649761199951
Validation loss: 2.229145348072052

Epoch: 6| Step: 3
Training loss: 1.2350010871887207
Validation loss: 2.2596415678660073

Epoch: 6| Step: 4
Training loss: 1.8382072448730469
Validation loss: 2.2655028104782104

Epoch: 6| Step: 5
Training loss: 2.1918623447418213
Validation loss: 2.2560476064682007

Epoch: 6| Step: 6
Training loss: 1.6175389289855957
Validation loss: 2.288201113541921

Epoch: 6| Step: 7
Training loss: 2.649536609649658
Validation loss: 2.290057202180227

Epoch: 6| Step: 8
Training loss: 1.5180656909942627
Validation loss: 2.2805149157842

Epoch: 6| Step: 9
Training loss: 1.0988380908966064
Validation loss: 2.283581097920736

Epoch: 6| Step: 10
Training loss: 1.8410015106201172
Validation loss: 2.2416050831476846

Epoch: 6| Step: 11
Training loss: 2.2411603927612305
Validation loss: 2.2507986227671304

Epoch: 6| Step: 12
Training loss: 1.5586875677108765
Validation loss: 2.2334930698076882

Epoch: 6| Step: 13
Training loss: 2.333430528640747
Validation loss: 2.244654635588328

Epoch: 288| Step: 0
Training loss: 1.7084267139434814
Validation loss: 2.2231659293174744

Epoch: 6| Step: 1
Training loss: 1.5673131942749023
Validation loss: 2.210258642832438

Epoch: 6| Step: 2
Training loss: 1.5956928730010986
Validation loss: 2.2125640710194907

Epoch: 6| Step: 3
Training loss: 2.056403636932373
Validation loss: 2.228951334953308

Epoch: 6| Step: 4
Training loss: 2.2240958213806152
Validation loss: 2.2290513118108115

Epoch: 6| Step: 5
Training loss: 2.21763014793396
Validation loss: 2.2183320124944053

Epoch: 6| Step: 6
Training loss: 1.6558103561401367
Validation loss: 2.214966297149658

Epoch: 6| Step: 7
Training loss: 2.043856620788574
Validation loss: 2.2464862863222756

Epoch: 6| Step: 8
Training loss: 1.0874513387680054
Validation loss: 2.242019852002462

Epoch: 6| Step: 9
Training loss: 1.9397711753845215
Validation loss: 2.296288788318634

Epoch: 6| Step: 10
Training loss: 1.655785322189331
Validation loss: 2.2894644141197205

Epoch: 6| Step: 11
Training loss: 1.6038075685501099
Validation loss: 2.275727649529775

Epoch: 6| Step: 12
Training loss: 2.103294849395752
Validation loss: 2.283056060473124

Epoch: 6| Step: 13
Training loss: 1.436809778213501
Validation loss: 2.2330052852630615

Epoch: 289| Step: 0
Training loss: 2.307246208190918
Validation loss: 2.222838838895162

Epoch: 6| Step: 1
Training loss: 1.7417491674423218
Validation loss: 2.235173980394999

Epoch: 6| Step: 2
Training loss: 1.9042437076568604
Validation loss: 2.2238477071126304

Epoch: 6| Step: 3
Training loss: 2.291881561279297
Validation loss: 2.205674727757772

Epoch: 6| Step: 4
Training loss: 2.32016658782959
Validation loss: 2.178322732448578

Epoch: 6| Step: 5
Training loss: 2.026937961578369
Validation loss: 2.219639559586843

Epoch: 6| Step: 6
Training loss: 1.6157422065734863
Validation loss: 2.210151712099711

Epoch: 6| Step: 7
Training loss: 1.186119556427002
Validation loss: 2.230294326941172

Epoch: 6| Step: 8
Training loss: 1.5190056562423706
Validation loss: 2.252688785394033

Epoch: 6| Step: 9
Training loss: 1.6295582056045532
Validation loss: 2.238603154818217

Epoch: 6| Step: 10
Training loss: 1.3377279043197632
Validation loss: 2.235194762547811

Epoch: 6| Step: 11
Training loss: 1.2279407978057861
Validation loss: 2.237157702445984

Epoch: 6| Step: 12
Training loss: 1.5967121124267578
Validation loss: 2.2255437771479287

Epoch: 6| Step: 13
Training loss: 1.6998181343078613
Validation loss: 2.2039063970247903

Epoch: 290| Step: 0
Training loss: 2.1713361740112305
Validation loss: 2.2142545183499656

Epoch: 6| Step: 1
Training loss: 1.1099960803985596
Validation loss: 2.207572122414907

Epoch: 6| Step: 2
Training loss: 2.168691873550415
Validation loss: 2.2286970615386963

Epoch: 6| Step: 3
Training loss: 1.376176118850708
Validation loss: 2.239669601122538

Epoch: 6| Step: 4
Training loss: 1.9955639839172363
Validation loss: 2.2555208603541055

Epoch: 6| Step: 5
Training loss: 1.1306569576263428
Validation loss: 2.2336822350819907

Epoch: 6| Step: 6
Training loss: 1.624243140220642
Validation loss: 2.254352112611135

Epoch: 6| Step: 7
Training loss: 2.404142141342163
Validation loss: 2.2749298413594565

Epoch: 6| Step: 8
Training loss: 1.32725191116333
Validation loss: 2.2624919017155967

Epoch: 6| Step: 9
Training loss: 1.4767775535583496
Validation loss: 2.2294283310572305

Epoch: 6| Step: 10
Training loss: 2.051170825958252
Validation loss: 2.229663848876953

Epoch: 6| Step: 11
Training loss: 1.7654550075531006
Validation loss: 2.2267417112986245

Epoch: 6| Step: 12
Training loss: 1.3456956148147583
Validation loss: 2.1863582332928977

Epoch: 6| Step: 13
Training loss: 2.1221494674682617
Validation loss: 2.167264441649119

Epoch: 291| Step: 0
Training loss: 1.780897855758667
Validation loss: 2.1738510529200235

Epoch: 6| Step: 1
Training loss: 1.6312044858932495
Validation loss: 2.184519350528717

Epoch: 6| Step: 2
Training loss: 2.1536521911621094
Validation loss: 2.197351098060608

Epoch: 6| Step: 3
Training loss: 1.8052552938461304
Validation loss: 2.177531441052755

Epoch: 6| Step: 4
Training loss: 1.7146551609039307
Validation loss: 2.199159801006317

Epoch: 6| Step: 5
Training loss: 1.8132262229919434
Validation loss: 2.175188660621643

Epoch: 6| Step: 6
Training loss: 1.77576744556427
Validation loss: 2.2024495204289756

Epoch: 6| Step: 7
Training loss: 2.041429042816162
Validation loss: 2.2302565375963845

Epoch: 6| Step: 8
Training loss: 1.5103408098220825
Validation loss: 2.2483511368433633

Epoch: 6| Step: 9
Training loss: 1.9157651662826538
Validation loss: 2.304911414782206

Epoch: 6| Step: 10
Training loss: 1.582084059715271
Validation loss: 2.2871968348821006

Epoch: 6| Step: 11
Training loss: 1.9265727996826172
Validation loss: 2.2919721404711404

Epoch: 6| Step: 12
Training loss: 1.3497328758239746
Validation loss: 2.270996868610382

Epoch: 6| Step: 13
Training loss: 2.1428062915802
Validation loss: 2.257450977961222

Epoch: 292| Step: 0
Training loss: 1.7613075971603394
Validation loss: 2.2298372983932495

Epoch: 6| Step: 1
Training loss: 1.1445825099945068
Validation loss: 2.198163946469625

Epoch: 6| Step: 2
Training loss: 1.6164155006408691
Validation loss: 2.193451722462972

Epoch: 6| Step: 3
Training loss: 1.8807919025421143
Validation loss: 2.19479231039683

Epoch: 6| Step: 4
Training loss: 1.7096631526947021
Validation loss: 2.207701861858368

Epoch: 6| Step: 5
Training loss: 1.7979779243469238
Validation loss: 2.190091331799825

Epoch: 6| Step: 6
Training loss: 2.073307991027832
Validation loss: 2.194327930609385

Epoch: 6| Step: 7
Training loss: 1.484762191772461
Validation loss: 2.2081310749053955

Epoch: 6| Step: 8
Training loss: 2.228513240814209
Validation loss: 2.206736365954081

Epoch: 6| Step: 9
Training loss: 1.9930164813995361
Validation loss: 2.218521157900492

Epoch: 6| Step: 10
Training loss: 1.8298721313476562
Validation loss: 2.212581733862559

Epoch: 6| Step: 11
Training loss: 2.071143865585327
Validation loss: 2.221538265546163

Epoch: 6| Step: 12
Training loss: 1.4986796379089355
Validation loss: 2.268727203210195

Epoch: 6| Step: 13
Training loss: 1.7234821319580078
Validation loss: 2.2770062685012817

Epoch: 293| Step: 0
Training loss: 2.1011250019073486
Validation loss: 2.3116648395856223

Epoch: 6| Step: 1
Training loss: 2.311262607574463
Validation loss: 2.283822556336721

Epoch: 6| Step: 2
Training loss: 1.91526460647583
Validation loss: 2.278173645337423

Epoch: 6| Step: 3
Training loss: 1.3303203582763672
Validation loss: 2.3156283100446067

Epoch: 6| Step: 4
Training loss: 2.059236526489258
Validation loss: 2.2855480909347534

Epoch: 6| Step: 5
Training loss: 1.852084755897522
Validation loss: 2.2635092536608377

Epoch: 6| Step: 6
Training loss: 1.9773585796356201
Validation loss: 2.2535152037938437

Epoch: 6| Step: 7
Training loss: 1.6742963790893555
Validation loss: 2.206562121709188

Epoch: 6| Step: 8
Training loss: 1.4899433851242065
Validation loss: 2.220354596773783

Epoch: 6| Step: 9
Training loss: 1.4815024137496948
Validation loss: 2.2203789154688516

Epoch: 6| Step: 10
Training loss: 2.167194366455078
Validation loss: 2.20851594209671

Epoch: 6| Step: 11
Training loss: 1.8419471979141235
Validation loss: 2.1867300271987915

Epoch: 6| Step: 12
Training loss: 1.172599196434021
Validation loss: 2.212474445501963

Epoch: 6| Step: 13
Training loss: 1.8712657690048218
Validation loss: 2.2414190769195557

Epoch: 294| Step: 0
Training loss: 1.5114154815673828
Validation loss: 2.2251898845036826

Epoch: 6| Step: 1
Training loss: 1.9743374586105347
Validation loss: 2.23921799659729

Epoch: 6| Step: 2
Training loss: 2.036853790283203
Validation loss: 2.268009980519613

Epoch: 6| Step: 3
Training loss: 1.375003695487976
Validation loss: 2.265352189540863

Epoch: 6| Step: 4
Training loss: 1.5336848497390747
Validation loss: 2.282278001308441

Epoch: 6| Step: 5
Training loss: 1.8736110925674438
Validation loss: 2.298016607761383

Epoch: 6| Step: 6
Training loss: 1.7096881866455078
Validation loss: 2.2705215017000833

Epoch: 6| Step: 7
Training loss: 1.455913782119751
Validation loss: 2.2676347692807517

Epoch: 6| Step: 8
Training loss: 1.4456459283828735
Validation loss: 2.2910354137420654

Epoch: 6| Step: 9
Training loss: 1.7793464660644531
Validation loss: 2.286012868086497

Epoch: 6| Step: 10
Training loss: 1.703951358795166
Validation loss: 2.253718376159668

Epoch: 6| Step: 11
Training loss: 1.4095685482025146
Validation loss: 2.2172183990478516

Epoch: 6| Step: 12
Training loss: 1.657653570175171
Validation loss: 2.2175068060557046

Epoch: 6| Step: 13
Training loss: 2.420694589614868
Validation loss: 2.211172938346863

Epoch: 295| Step: 0
Training loss: 2.183781385421753
Validation loss: 2.1920315424601235

Epoch: 6| Step: 1
Training loss: 1.721332311630249
Validation loss: 2.210216840108236

Epoch: 6| Step: 2
Training loss: 1.3068182468414307
Validation loss: 2.191819489002228

Epoch: 6| Step: 3
Training loss: 1.794543981552124
Validation loss: 2.191486438115438

Epoch: 6| Step: 4
Training loss: 2.047006130218506
Validation loss: 2.175169209639231

Epoch: 6| Step: 5
Training loss: 1.7632262706756592
Validation loss: 2.177874426047007

Epoch: 6| Step: 6
Training loss: 1.130159854888916
Validation loss: 2.2169151107470193

Epoch: 6| Step: 7
Training loss: 1.296962022781372
Validation loss: 2.21372397740682

Epoch: 6| Step: 8
Training loss: 2.0417251586914062
Validation loss: 2.23542050520579

Epoch: 6| Step: 9
Training loss: 1.842822790145874
Validation loss: 2.253171761830648

Epoch: 6| Step: 10
Training loss: 2.2379050254821777
Validation loss: 2.2676369547843933

Epoch: 6| Step: 11
Training loss: 1.7537169456481934
Validation loss: 2.262321949005127

Epoch: 6| Step: 12
Training loss: 1.282287359237671
Validation loss: 2.264035979906718

Epoch: 6| Step: 13
Training loss: 1.727461814880371
Validation loss: 2.2491331895192466

Epoch: 296| Step: 0
Training loss: 2.244800329208374
Validation loss: 2.2429442207018533

Epoch: 6| Step: 1
Training loss: 2.278426170349121
Validation loss: 2.2366384466489158

Epoch: 6| Step: 2
Training loss: 1.3502610921859741
Validation loss: 2.2290441592534385

Epoch: 6| Step: 3
Training loss: 1.686389446258545
Validation loss: 2.2314223448435464

Epoch: 6| Step: 4
Training loss: 1.2679128646850586
Validation loss: 2.2285916805267334

Epoch: 6| Step: 5
Training loss: 1.4152286052703857
Validation loss: 2.2217385172843933

Epoch: 6| Step: 6
Training loss: 1.6076689958572388
Validation loss: 2.2282325824101767

Epoch: 6| Step: 7
Training loss: 1.199446678161621
Validation loss: 2.205078601837158

Epoch: 6| Step: 8
Training loss: 2.1254324913024902
Validation loss: 2.221421182155609

Epoch: 6| Step: 9
Training loss: 2.1515703201293945
Validation loss: 2.2329752246538797

Epoch: 6| Step: 10
Training loss: 1.914947748184204
Validation loss: 2.211130758126577

Epoch: 6| Step: 11
Training loss: 1.3764451742172241
Validation loss: 2.215270737806956

Epoch: 6| Step: 12
Training loss: 1.556020736694336
Validation loss: 2.2433012326558432

Epoch: 6| Step: 13
Training loss: 1.8784093856811523
Validation loss: 2.2868009408315024

Epoch: 297| Step: 0
Training loss: 1.4929978847503662
Validation loss: 2.3042293588320413

Epoch: 6| Step: 1
Training loss: 1.4356950521469116
Validation loss: 2.321191430091858

Epoch: 6| Step: 2
Training loss: 1.9473921060562134
Validation loss: 2.3001606464385986

Epoch: 6| Step: 3
Training loss: 2.0224416255950928
Validation loss: 2.341625928878784

Epoch: 6| Step: 4
Training loss: 1.9453108310699463
Validation loss: 2.3337214390436807

Epoch: 6| Step: 5
Training loss: 1.1006749868392944
Validation loss: 2.30600643157959

Epoch: 6| Step: 6
Training loss: 1.8703410625457764
Validation loss: 2.2816962003707886

Epoch: 6| Step: 7
Training loss: 1.9910616874694824
Validation loss: 2.229138731956482

Epoch: 6| Step: 8
Training loss: 1.812158226966858
Validation loss: 2.238987664381663

Epoch: 6| Step: 9
Training loss: 1.6818771362304688
Validation loss: 2.228064715862274

Epoch: 6| Step: 10
Training loss: 1.233135461807251
Validation loss: 2.2011417746543884

Epoch: 6| Step: 11
Training loss: 1.6794636249542236
Validation loss: 2.2156259417533875

Epoch: 6| Step: 12
Training loss: 2.104001045227051
Validation loss: 2.190312306086222

Epoch: 6| Step: 13
Training loss: 2.2932424545288086
Validation loss: 2.1946170330047607

Epoch: 298| Step: 0
Training loss: 1.844565749168396
Validation loss: 2.2067880233128867

Epoch: 6| Step: 1
Training loss: 1.621696949005127
Validation loss: 2.2301032344500222

Epoch: 6| Step: 2
Training loss: 1.5179699659347534
Validation loss: 2.2462125221888223

Epoch: 6| Step: 3
Training loss: 1.9470701217651367
Validation loss: 2.2565344969431558

Epoch: 6| Step: 4
Training loss: 1.4902554750442505
Validation loss: 2.2535062432289124

Epoch: 6| Step: 5
Training loss: 2.034815549850464
Validation loss: 2.2727718551953635

Epoch: 6| Step: 6
Training loss: 1.9101338386535645
Validation loss: 2.2801488439242044

Epoch: 6| Step: 7
Training loss: 1.931460976600647
Validation loss: 2.268013596534729

Epoch: 6| Step: 8
Training loss: 1.5894347429275513
Validation loss: 2.2701140642166138

Epoch: 6| Step: 9
Training loss: 1.6981842517852783
Validation loss: 2.245005706946055

Epoch: 6| Step: 10
Training loss: 1.5751882791519165
Validation loss: 2.2398943305015564

Epoch: 6| Step: 11
Training loss: 1.777186632156372
Validation loss: 2.2268636425336203

Epoch: 6| Step: 12
Training loss: 1.8150742053985596
Validation loss: 2.229939560095469

Epoch: 6| Step: 13
Training loss: 1.6911499500274658
Validation loss: 2.2093181014060974

Epoch: 299| Step: 0
Training loss: 1.6309155225753784
Validation loss: 2.218140502770742

Epoch: 6| Step: 1
Training loss: 2.2158312797546387
Validation loss: 2.219407300154368

Epoch: 6| Step: 2
Training loss: 2.0499205589294434
Validation loss: 2.1948370337486267

Epoch: 6| Step: 3
Training loss: 1.5467298030853271
Validation loss: 2.226267476876577

Epoch: 6| Step: 4
Training loss: 2.5113840103149414
Validation loss: 2.245925943056742

Epoch: 6| Step: 5
Training loss: 1.7979906797409058
Validation loss: 2.2392197052637735

Epoch: 6| Step: 6
Training loss: 1.1866167783737183
Validation loss: 2.31676975886027

Epoch: 6| Step: 7
Training loss: 1.7919098138809204
Validation loss: 2.31383607784907

Epoch: 6| Step: 8
Training loss: 1.4547054767608643
Validation loss: 2.340375542640686

Epoch: 6| Step: 9
Training loss: 1.6143393516540527
Validation loss: 2.3240967790285745

Epoch: 6| Step: 10
Training loss: 1.4699008464813232
Validation loss: 2.315449515978495

Epoch: 6| Step: 11
Training loss: 2.2712607383728027
Validation loss: 2.309139351050059

Epoch: 6| Step: 12
Training loss: 2.1670145988464355
Validation loss: 2.3490896622339883

Epoch: 6| Step: 13
Training loss: 1.4605417251586914
Validation loss: 2.306354502836863

Epoch: 300| Step: 0
Training loss: 1.8164137601852417
Validation loss: 2.3306477665901184

Epoch: 6| Step: 1
Training loss: 2.0708396434783936
Validation loss: 2.283318201700846

Epoch: 6| Step: 2
Training loss: 1.6902052164077759
Validation loss: 2.2586373686790466

Epoch: 6| Step: 3
Training loss: 1.520201563835144
Validation loss: 2.2557814518610635

Epoch: 6| Step: 4
Training loss: 1.5136754512786865
Validation loss: 2.241568148136139

Epoch: 6| Step: 5
Training loss: 1.3073630332946777
Validation loss: 2.2649903496106467

Epoch: 6| Step: 6
Training loss: 2.0671777725219727
Validation loss: 2.2625189224878945

Epoch: 6| Step: 7
Training loss: 1.7676491737365723
Validation loss: 2.243068039417267

Epoch: 6| Step: 8
Training loss: 1.7376903295516968
Validation loss: 2.255080779393514

Epoch: 6| Step: 9
Training loss: 1.3634523153305054
Validation loss: 2.2489312092463174

Epoch: 6| Step: 10
Training loss: 1.2153778076171875
Validation loss: 2.2373661200205484

Epoch: 6| Step: 11
Training loss: 1.4844298362731934
Validation loss: 2.2615675926208496

Epoch: 6| Step: 12
Training loss: 1.7389910221099854
Validation loss: 2.2631056904792786

Epoch: 6| Step: 13
Training loss: 2.2286224365234375
Validation loss: 2.2727877100308738

Epoch: 301| Step: 0
Training loss: 1.6126065254211426
Validation loss: 2.3095662593841553

Epoch: 6| Step: 1
Training loss: 2.3455889225006104
Validation loss: 2.282813767592112

Epoch: 6| Step: 2
Training loss: 1.0776832103729248
Validation loss: 2.3311355908711753

Epoch: 6| Step: 3
Training loss: 1.5611321926116943
Validation loss: 2.2745502392450967

Epoch: 6| Step: 4
Training loss: 1.3645851612091064
Validation loss: 2.2786413033803306

Epoch: 6| Step: 5
Training loss: 2.2367281913757324
Validation loss: 2.297728419303894

Epoch: 6| Step: 6
Training loss: 1.5455188751220703
Validation loss: 2.3016496499379477

Epoch: 6| Step: 7
Training loss: 2.1651577949523926
Validation loss: 2.263730525970459

Epoch: 6| Step: 8
Training loss: 1.1417031288146973
Validation loss: 2.256821930408478

Epoch: 6| Step: 9
Training loss: 2.11993408203125
Validation loss: 2.282197058200836

Epoch: 6| Step: 10
Training loss: 1.660358190536499
Validation loss: 2.276401400566101

Epoch: 6| Step: 11
Training loss: 1.460776925086975
Validation loss: 2.2622634967168174

Epoch: 6| Step: 12
Training loss: 2.1951231956481934
Validation loss: 2.2677361567815146

Epoch: 6| Step: 13
Training loss: 1.4205470085144043
Validation loss: 2.2836408217748008

Epoch: 302| Step: 0
Training loss: 1.7984561920166016
Validation loss: 2.2597891092300415

Epoch: 6| Step: 1
Training loss: 1.448305368423462
Validation loss: 2.2636290391286216

Epoch: 6| Step: 2
Training loss: 2.0383152961730957
Validation loss: 2.276257316271464

Epoch: 6| Step: 3
Training loss: 1.7793166637420654
Validation loss: 2.2502347230911255

Epoch: 6| Step: 4
Training loss: 1.2574800252914429
Validation loss: 2.27682497104009

Epoch: 6| Step: 5
Training loss: 1.8775852918624878
Validation loss: 2.2830361127853394

Epoch: 6| Step: 6
Training loss: 1.0930322408676147
Validation loss: 2.2600605289141336

Epoch: 6| Step: 7
Training loss: 1.6646708250045776
Validation loss: 2.2873595555623374

Epoch: 6| Step: 8
Training loss: 1.5862557888031006
Validation loss: 2.2385013898213706

Epoch: 6| Step: 9
Training loss: 1.6617538928985596
Validation loss: 2.2586422165234885

Epoch: 6| Step: 10
Training loss: 2.099794387817383
Validation loss: 2.218501170476278

Epoch: 6| Step: 11
Training loss: 2.1855251789093018
Validation loss: 2.213581085205078

Epoch: 6| Step: 12
Training loss: 1.398396372795105
Validation loss: 2.2037304043769836

Epoch: 6| Step: 13
Training loss: 1.8211997747421265
Validation loss: 2.2073623140652976

Epoch: 303| Step: 0
Training loss: 1.2348146438598633
Validation loss: 2.2166378100713096

Epoch: 6| Step: 1
Training loss: 1.6362292766571045
Validation loss: 2.2122787833213806

Epoch: 6| Step: 2
Training loss: 1.2976655960083008
Validation loss: 2.2088820338249207

Epoch: 6| Step: 3
Training loss: 2.199117660522461
Validation loss: 2.205900410811106

Epoch: 6| Step: 4
Training loss: 1.5500283241271973
Validation loss: 2.2288665572802224

Epoch: 6| Step: 5
Training loss: 2.1501457691192627
Validation loss: 2.1978399952252707

Epoch: 6| Step: 6
Training loss: 1.4474332332611084
Validation loss: 2.276338299115499

Epoch: 6| Step: 7
Training loss: 2.3713507652282715
Validation loss: 2.2847123940785727

Epoch: 6| Step: 8
Training loss: 1.769439697265625
Validation loss: 2.323870539665222

Epoch: 6| Step: 9
Training loss: 1.609712839126587
Validation loss: 2.329752047856649

Epoch: 6| Step: 10
Training loss: 1.6254913806915283
Validation loss: 2.3201691309611

Epoch: 6| Step: 11
Training loss: 2.0466721057891846
Validation loss: 2.33259117603302

Epoch: 6| Step: 12
Training loss: 2.026416301727295
Validation loss: 2.3536760807037354

Epoch: 6| Step: 13
Training loss: 1.8913416862487793
Validation loss: 2.3287493785222373

Epoch: 304| Step: 0
Training loss: 2.114807367324829
Validation loss: 2.255013187726339

Epoch: 6| Step: 1
Training loss: 1.773832082748413
Validation loss: 2.2590297063191733

Epoch: 6| Step: 2
Training loss: 1.718427300453186
Validation loss: 2.2072052558263144

Epoch: 6| Step: 3
Training loss: 1.5152932405471802
Validation loss: 2.2081488768259683

Epoch: 6| Step: 4
Training loss: 1.274682879447937
Validation loss: 2.1874396006266275

Epoch: 6| Step: 5
Training loss: 1.4798229932785034
Validation loss: 2.2044758200645447

Epoch: 6| Step: 6
Training loss: 1.7753667831420898
Validation loss: 2.2199078798294067

Epoch: 6| Step: 7
Training loss: 1.5704456567764282
Validation loss: 2.213344176610311

Epoch: 6| Step: 8
Training loss: 1.3436044454574585
Validation loss: 2.266557494799296

Epoch: 6| Step: 9
Training loss: 1.9657912254333496
Validation loss: 2.2630976835886636

Epoch: 6| Step: 10
Training loss: 2.434123992919922
Validation loss: 2.2909949024518332

Epoch: 6| Step: 11
Training loss: 1.9798123836517334
Validation loss: 2.28819078207016

Epoch: 6| Step: 12
Training loss: 1.686840295791626
Validation loss: 2.268690208594004

Epoch: 6| Step: 13
Training loss: 1.7872662544250488
Validation loss: 2.2661561965942383

Epoch: 305| Step: 0
Training loss: 1.8796565532684326
Validation loss: 2.2814132372538247

Epoch: 6| Step: 1
Training loss: 1.534057378768921
Validation loss: 2.272845208644867

Epoch: 6| Step: 2
Training loss: 0.9181803464889526
Validation loss: 2.2395654916763306

Epoch: 6| Step: 3
Training loss: 1.9403603076934814
Validation loss: 2.267320692539215

Epoch: 6| Step: 4
Training loss: 1.8734164237976074
Validation loss: 2.261083801587423

Epoch: 6| Step: 5
Training loss: 1.3823812007904053
Validation loss: 2.259394963582357

Epoch: 6| Step: 6
Training loss: 1.374576210975647
Validation loss: 2.21231077114741

Epoch: 6| Step: 7
Training loss: 1.322439193725586
Validation loss: 2.2170485258102417

Epoch: 6| Step: 8
Training loss: 1.5481505393981934
Validation loss: 2.2462138533592224

Epoch: 6| Step: 9
Training loss: 1.3720393180847168
Validation loss: 2.2173747221628823

Epoch: 6| Step: 10
Training loss: 2.2401723861694336
Validation loss: 2.211952884991964

Epoch: 6| Step: 11
Training loss: 2.030862331390381
Validation loss: 2.227044920126597

Epoch: 6| Step: 12
Training loss: 2.091299533843994
Validation loss: 2.2136879364649453

Epoch: 6| Step: 13
Training loss: 2.3274078369140625
Validation loss: 2.2396135330200195

Epoch: 306| Step: 0
Training loss: 1.5834451913833618
Validation loss: 2.2541997035344443

Epoch: 6| Step: 1
Training loss: 1.7983180284500122
Validation loss: 2.231792628765106

Epoch: 6| Step: 2
Training loss: 1.745593547821045
Validation loss: 2.2703118324279785

Epoch: 6| Step: 3
Training loss: 1.5826606750488281
Validation loss: 2.2487035592397056

Epoch: 6| Step: 4
Training loss: 1.651641845703125
Validation loss: 2.2492770155270896

Epoch: 6| Step: 5
Training loss: 1.5052809715270996
Validation loss: 2.2707709471384683

Epoch: 6| Step: 6
Training loss: 1.8972035646438599
Validation loss: 2.296243409315745

Epoch: 6| Step: 7
Training loss: 1.558307409286499
Validation loss: 2.273557106653849

Epoch: 6| Step: 8
Training loss: 1.689603567123413
Validation loss: 2.2622271180152893

Epoch: 6| Step: 9
Training loss: 1.1876134872436523
Validation loss: 2.2640090386072793

Epoch: 6| Step: 10
Training loss: 1.7838307619094849
Validation loss: 2.252675493558248

Epoch: 6| Step: 11
Training loss: 1.517510175704956
Validation loss: 2.25469179948171

Epoch: 6| Step: 12
Training loss: 1.5478602647781372
Validation loss: 2.239429314931234

Epoch: 6| Step: 13
Training loss: 2.4745945930480957
Validation loss: 2.2273691097895303

Epoch: 307| Step: 0
Training loss: 1.6403828859329224
Validation loss: 2.2489149371782937

Epoch: 6| Step: 1
Training loss: 1.5535203218460083
Validation loss: 2.28397798538208

Epoch: 6| Step: 2
Training loss: 2.3703246116638184
Validation loss: 2.2759310007095337

Epoch: 6| Step: 3
Training loss: 1.5985279083251953
Validation loss: 2.3147113919258118

Epoch: 6| Step: 4
Training loss: 1.6197307109832764
Validation loss: 2.283203959465027

Epoch: 6| Step: 5
Training loss: 1.600985050201416
Validation loss: 2.3036722342173257

Epoch: 6| Step: 6
Training loss: 1.6424812078475952
Validation loss: 2.291447321573893

Epoch: 6| Step: 7
Training loss: 1.522719383239746
Validation loss: 2.28082541624705

Epoch: 6| Step: 8
Training loss: 2.090985059738159
Validation loss: 2.2590654293696084

Epoch: 6| Step: 9
Training loss: 1.879660725593567
Validation loss: 2.25150465965271

Epoch: 6| Step: 10
Training loss: 1.1610054969787598
Validation loss: 2.2337104280789695

Epoch: 6| Step: 11
Training loss: 1.9814575910568237
Validation loss: 2.231770714124044

Epoch: 6| Step: 12
Training loss: 1.7113417387008667
Validation loss: 2.2148616711298623

Epoch: 6| Step: 13
Training loss: 1.111620306968689
Validation loss: 2.2325915098190308

Epoch: 308| Step: 0
Training loss: 1.2378870248794556
Validation loss: 2.228575607140859

Epoch: 6| Step: 1
Training loss: 0.7504004836082458
Validation loss: 2.23013569911321

Epoch: 6| Step: 2
Training loss: 1.4128344058990479
Validation loss: 2.2411392529805503

Epoch: 6| Step: 3
Training loss: 1.1710128784179688
Validation loss: 2.2490700085957847

Epoch: 6| Step: 4
Training loss: 1.3381301164627075
Validation loss: 2.2729076147079468

Epoch: 6| Step: 5
Training loss: 2.1804256439208984
Validation loss: 2.286880890528361

Epoch: 6| Step: 6
Training loss: 2.2491979598999023
Validation loss: 2.2836151321729026

Epoch: 6| Step: 7
Training loss: 1.4392757415771484
Validation loss: 2.280252536137899

Epoch: 6| Step: 8
Training loss: 2.3527510166168213
Validation loss: 2.2705007791519165

Epoch: 6| Step: 9
Training loss: 1.5674920082092285
Validation loss: 2.3058549960454306

Epoch: 6| Step: 10
Training loss: 2.404531478881836
Validation loss: 2.2977205514907837

Epoch: 6| Step: 11
Training loss: 1.894624948501587
Validation loss: 2.2881200710932412

Epoch: 6| Step: 12
Training loss: 1.5798664093017578
Validation loss: 2.2787298361460366

Epoch: 6| Step: 13
Training loss: 1.7136647701263428
Validation loss: 2.2759330670038858

Epoch: 309| Step: 0
Training loss: 1.7534198760986328
Validation loss: 2.2600000699361167

Epoch: 6| Step: 1
Training loss: 1.6336913108825684
Validation loss: 2.273282547791799

Epoch: 6| Step: 2
Training loss: 1.9290155172348022
Validation loss: 2.2758320371309915

Epoch: 6| Step: 3
Training loss: 1.1638975143432617
Validation loss: 2.2579010327657065

Epoch: 6| Step: 4
Training loss: 1.1390206813812256
Validation loss: 2.270562767982483

Epoch: 6| Step: 5
Training loss: 1.7742409706115723
Validation loss: 2.232222596804301

Epoch: 6| Step: 6
Training loss: 2.292853593826294
Validation loss: 2.246125022570292

Epoch: 6| Step: 7
Training loss: 2.1670427322387695
Validation loss: 2.2567644317944846

Epoch: 6| Step: 8
Training loss: 1.95121431350708
Validation loss: 2.235639989376068

Epoch: 6| Step: 9
Training loss: 1.6417579650878906
Validation loss: 2.2389868100484214

Epoch: 6| Step: 10
Training loss: 1.7300843000411987
Validation loss: 2.222236712773641

Epoch: 6| Step: 11
Training loss: 1.547266960144043
Validation loss: 2.2226619124412537

Epoch: 6| Step: 12
Training loss: 1.3209141492843628
Validation loss: 2.2169591387112937

Epoch: 6| Step: 13
Training loss: 1.4682048559188843
Validation loss: 2.222336729367574

Epoch: 310| Step: 0
Training loss: 1.7282600402832031
Validation loss: 2.277198632558187

Epoch: 6| Step: 1
Training loss: 1.4766579866409302
Validation loss: 2.2347899675369263

Epoch: 6| Step: 2
Training loss: 2.154362201690674
Validation loss: 2.26685106754303

Epoch: 6| Step: 3
Training loss: 1.464143991470337
Validation loss: 2.2352556387583413

Epoch: 6| Step: 4
Training loss: 1.4640660285949707
Validation loss: 2.259797235329946

Epoch: 6| Step: 5
Training loss: 1.967629313468933
Validation loss: 2.282875974973043

Epoch: 6| Step: 6
Training loss: 1.7358994483947754
Validation loss: 2.2733856042226157

Epoch: 6| Step: 7
Training loss: 2.0774435997009277
Validation loss: 2.28100848197937

Epoch: 6| Step: 8
Training loss: 1.4891643524169922
Validation loss: 2.284493068854014

Epoch: 6| Step: 9
Training loss: 1.4909989833831787
Validation loss: 2.285846749941508

Epoch: 6| Step: 10
Training loss: 1.204908847808838
Validation loss: 2.2819147308667502

Epoch: 6| Step: 11
Training loss: 1.5772426128387451
Validation loss: 2.305372655391693

Epoch: 6| Step: 12
Training loss: 1.9244494438171387
Validation loss: 2.3020722468694053

Epoch: 6| Step: 13
Training loss: 1.8923625946044922
Validation loss: 2.2896628777186074

Epoch: 311| Step: 0
Training loss: 2.577232599258423
Validation loss: 2.244916836420695

Epoch: 6| Step: 1
Training loss: 1.3848401308059692
Validation loss: 2.263412872950236

Epoch: 6| Step: 2
Training loss: 1.296566367149353
Validation loss: 2.2336233059565225

Epoch: 6| Step: 3
Training loss: 1.2127647399902344
Validation loss: 2.229180117448171

Epoch: 6| Step: 4
Training loss: 1.8913263082504272
Validation loss: 2.2290173172950745

Epoch: 6| Step: 5
Training loss: 2.138782024383545
Validation loss: 2.2383978962898254

Epoch: 6| Step: 6
Training loss: 1.6014071702957153
Validation loss: 2.216393073399862

Epoch: 6| Step: 7
Training loss: 1.5508763790130615
Validation loss: 2.222094972928365

Epoch: 6| Step: 8
Training loss: 1.9100456237792969
Validation loss: 2.2143120765686035

Epoch: 6| Step: 9
Training loss: 2.4646191596984863
Validation loss: 2.207852760950724

Epoch: 6| Step: 10
Training loss: 1.242313027381897
Validation loss: 2.2193883458773294

Epoch: 6| Step: 11
Training loss: 1.898202657699585
Validation loss: 2.28974856932958

Epoch: 6| Step: 12
Training loss: 1.5420953035354614
Validation loss: 2.2928167978922525

Epoch: 6| Step: 13
Training loss: 1.3964037895202637
Validation loss: 2.324849843978882

Epoch: 312| Step: 0
Training loss: 1.1084882020950317
Validation loss: 2.3209672967592874

Epoch: 6| Step: 1
Training loss: 1.666834831237793
Validation loss: 2.341761291027069

Epoch: 6| Step: 2
Training loss: 1.9364924430847168
Validation loss: 2.3300288915634155

Epoch: 6| Step: 3
Training loss: 1.9817030429840088
Validation loss: 2.360326826572418

Epoch: 6| Step: 4
Training loss: 2.163060188293457
Validation loss: 2.294268806775411

Epoch: 6| Step: 5
Training loss: 1.2878491878509521
Validation loss: 2.2854913473129272

Epoch: 6| Step: 6
Training loss: 2.240257740020752
Validation loss: 2.2628550926844277

Epoch: 6| Step: 7
Training loss: 1.2144417762756348
Validation loss: 2.281357924143473

Epoch: 6| Step: 8
Training loss: 1.5108261108398438
Validation loss: 2.2264848351478577

Epoch: 6| Step: 9
Training loss: 1.9248099327087402
Validation loss: 2.2516272266705832

Epoch: 6| Step: 10
Training loss: 1.4249910116195679
Validation loss: 2.283318519592285

Epoch: 6| Step: 11
Training loss: 2.0064356327056885
Validation loss: 2.27869975566864

Epoch: 6| Step: 12
Training loss: 1.7634479999542236
Validation loss: 2.2767617106437683

Epoch: 6| Step: 13
Training loss: 1.8573698997497559
Validation loss: 2.2790770729382834

Epoch: 313| Step: 0
Training loss: 1.760977029800415
Validation loss: 2.3159102400143943

Epoch: 6| Step: 1
Training loss: 2.3788704872131348
Validation loss: 2.336815814177195

Epoch: 6| Step: 2
Training loss: 2.0254945755004883
Validation loss: 2.3208211064338684

Epoch: 6| Step: 3
Training loss: 1.4397480487823486
Validation loss: 2.2724005778630576

Epoch: 6| Step: 4
Training loss: 2.2122395038604736
Validation loss: 2.2636257807413735

Epoch: 6| Step: 5
Training loss: 1.406935214996338
Validation loss: 2.2468871672948203

Epoch: 6| Step: 6
Training loss: 1.7850290536880493
Validation loss: 2.252821167310079

Epoch: 6| Step: 7
Training loss: 0.8899656534194946
Validation loss: 2.228972295920054

Epoch: 6| Step: 8
Training loss: 1.4604825973510742
Validation loss: 2.2231303652127585

Epoch: 6| Step: 9
Training loss: 1.544844388961792
Validation loss: 2.261762579282125

Epoch: 6| Step: 10
Training loss: 2.2129433155059814
Validation loss: 2.2351087530454

Epoch: 6| Step: 11
Training loss: 1.5069230794906616
Validation loss: 2.227148175239563

Epoch: 6| Step: 12
Training loss: 1.1735785007476807
Validation loss: 2.2567080656687417

Epoch: 6| Step: 13
Training loss: 1.573995590209961
Validation loss: 2.2703009446461997

Epoch: 314| Step: 0
Training loss: 1.6492245197296143
Validation loss: 2.2994341452916465

Epoch: 6| Step: 1
Training loss: 1.7028706073760986
Validation loss: 2.29795374472936

Epoch: 6| Step: 2
Training loss: 2.054408311843872
Validation loss: 2.292938450972239

Epoch: 6| Step: 3
Training loss: 1.5372833013534546
Validation loss: 2.271632353464762

Epoch: 6| Step: 4
Training loss: 1.0914161205291748
Validation loss: 2.246984302997589

Epoch: 6| Step: 5
Training loss: 0.8727760314941406
Validation loss: 2.242702305316925

Epoch: 6| Step: 6
Training loss: 1.8121494054794312
Validation loss: 2.2371549208958945

Epoch: 6| Step: 7
Training loss: 2.4272892475128174
Validation loss: 2.2195021708806357

Epoch: 6| Step: 8
Training loss: 1.91891348361969
Validation loss: 2.2149060368537903

Epoch: 6| Step: 9
Training loss: 1.596712589263916
Validation loss: 2.231686015923818

Epoch: 6| Step: 10
Training loss: 1.689752459526062
Validation loss: 2.2420740922292075

Epoch: 6| Step: 11
Training loss: 2.213038444519043
Validation loss: 2.239672541618347

Epoch: 6| Step: 12
Training loss: 1.4838374853134155
Validation loss: 2.2437750101089478

Epoch: 6| Step: 13
Training loss: 1.9343898296356201
Validation loss: 2.260755101839701

Epoch: 315| Step: 0
Training loss: 2.5107884407043457
Validation loss: 2.2975687185923257

Epoch: 6| Step: 1
Training loss: 2.0048537254333496
Validation loss: 2.3284634749094644

Epoch: 6| Step: 2
Training loss: 1.8542275428771973
Validation loss: 2.3549965222676597

Epoch: 6| Step: 3
Training loss: 1.4418593645095825
Validation loss: 2.344292720158895

Epoch: 6| Step: 4
Training loss: 1.9267289638519287
Validation loss: 2.332588811715444

Epoch: 6| Step: 5
Training loss: 1.242719054222107
Validation loss: 2.292362372080485

Epoch: 6| Step: 6
Training loss: 1.575313687324524
Validation loss: 2.2645074923833213

Epoch: 6| Step: 7
Training loss: 1.4642143249511719
Validation loss: 2.239481806755066

Epoch: 6| Step: 8
Training loss: 1.375030755996704
Validation loss: 2.2247931162516275

Epoch: 6| Step: 9
Training loss: 2.1026628017425537
Validation loss: 2.2321895758310952

Epoch: 6| Step: 10
Training loss: 1.4339470863342285
Validation loss: 2.2018917004267373

Epoch: 6| Step: 11
Training loss: 1.4467072486877441
Validation loss: 2.212798992792765

Epoch: 6| Step: 12
Training loss: 1.2989909648895264
Validation loss: 2.1852288246154785

Epoch: 6| Step: 13
Training loss: 1.8388662338256836
Validation loss: 2.2145785888036094

Epoch: 316| Step: 0
Training loss: 1.8654630184173584
Validation loss: 2.1995747486750283

Epoch: 6| Step: 1
Training loss: 1.3546422719955444
Validation loss: 2.1955241560935974

Epoch: 6| Step: 2
Training loss: 1.7643728256225586
Validation loss: 2.188406229019165

Epoch: 6| Step: 3
Training loss: 1.0828694105148315
Validation loss: 2.211955487728119

Epoch: 6| Step: 4
Training loss: 1.7958393096923828
Validation loss: 2.2102166215578714

Epoch: 6| Step: 5
Training loss: 1.860684871673584
Validation loss: 2.2367483377456665

Epoch: 6| Step: 6
Training loss: 1.606542944908142
Validation loss: 2.2539868156115213

Epoch: 6| Step: 7
Training loss: 1.4229307174682617
Validation loss: 2.2657624880472818

Epoch: 6| Step: 8
Training loss: 1.8494747877120972
Validation loss: 2.3306963443756104

Epoch: 6| Step: 9
Training loss: 1.8867106437683105
Validation loss: 2.3316750526428223

Epoch: 6| Step: 10
Training loss: 1.9455783367156982
Validation loss: 2.3422704935073853

Epoch: 6| Step: 11
Training loss: 1.5913797616958618
Validation loss: 2.3213449319203696

Epoch: 6| Step: 12
Training loss: 1.9162962436676025
Validation loss: 2.3336286147435508

Epoch: 6| Step: 13
Training loss: 1.5892390012741089
Validation loss: 2.325323144594828

Epoch: 317| Step: 0
Training loss: 1.4254817962646484
Validation loss: 2.279751459757487

Epoch: 6| Step: 1
Training loss: 1.469787359237671
Validation loss: 2.249838133653005

Epoch: 6| Step: 2
Training loss: 0.7631747126579285
Validation loss: 2.2821152408917746

Epoch: 6| Step: 3
Training loss: 1.8660579919815063
Validation loss: 2.2349558075269065

Epoch: 6| Step: 4
Training loss: 2.687415838241577
Validation loss: 2.24836399157842

Epoch: 6| Step: 5
Training loss: 1.7482082843780518
Validation loss: 2.2600291967391968

Epoch: 6| Step: 6
Training loss: 1.2921345233917236
Validation loss: 2.2604877948760986

Epoch: 6| Step: 7
Training loss: 1.5531766414642334
Validation loss: 2.278761148452759

Epoch: 6| Step: 8
Training loss: 1.9963593482971191
Validation loss: 2.28234734137853

Epoch: 6| Step: 9
Training loss: 1.636936068534851
Validation loss: 2.2984018325805664

Epoch: 6| Step: 10
Training loss: 1.9509131908416748
Validation loss: 2.2943592270215354

Epoch: 6| Step: 11
Training loss: 0.9861350059509277
Validation loss: 2.301154454549154

Epoch: 6| Step: 12
Training loss: 2.16552734375
Validation loss: 2.298257827758789

Epoch: 6| Step: 13
Training loss: 2.0595836639404297
Validation loss: 2.3001798590024314

Epoch: 318| Step: 0
Training loss: 1.1568117141723633
Validation loss: 2.2795942227045694

Epoch: 6| Step: 1
Training loss: 1.509315013885498
Validation loss: 2.2593886057535806

Epoch: 6| Step: 2
Training loss: 1.8470698595046997
Validation loss: 2.205298046271006

Epoch: 6| Step: 3
Training loss: 1.3791269063949585
Validation loss: 2.2129813035329184

Epoch: 6| Step: 4
Training loss: 1.1682229042053223
Validation loss: 2.2018544475237527

Epoch: 6| Step: 5
Training loss: 1.8410909175872803
Validation loss: 2.1932471990585327

Epoch: 6| Step: 6
Training loss: 2.1382453441619873
Validation loss: 2.216440200805664

Epoch: 6| Step: 7
Training loss: 1.2315212488174438
Validation loss: 2.213529805342356

Epoch: 6| Step: 8
Training loss: 1.8415884971618652
Validation loss: 2.2053150733311973

Epoch: 6| Step: 9
Training loss: 2.430677890777588
Validation loss: 2.228546619415283

Epoch: 6| Step: 10
Training loss: 2.304299831390381
Validation loss: 2.2333112557729087

Epoch: 6| Step: 11
Training loss: 1.308997392654419
Validation loss: 2.226334512233734

Epoch: 6| Step: 12
Training loss: 1.6973528861999512
Validation loss: 2.2496671279271445

Epoch: 6| Step: 13
Training loss: 1.6547081470489502
Validation loss: 2.2797257900238037

Epoch: 319| Step: 0
Training loss: 1.1736946105957031
Validation loss: 2.263974964618683

Epoch: 6| Step: 1
Training loss: 1.1081640720367432
Validation loss: 2.275810480117798

Epoch: 6| Step: 2
Training loss: 1.5418901443481445
Validation loss: 2.2540986935297647

Epoch: 6| Step: 3
Training loss: 2.5862505435943604
Validation loss: 2.292550961176554

Epoch: 6| Step: 4
Training loss: 1.6342564821243286
Validation loss: 2.2601199746131897

Epoch: 6| Step: 5
Training loss: 1.993314504623413
Validation loss: 2.2631986935933432

Epoch: 6| Step: 6
Training loss: 1.2028040885925293
Validation loss: 2.295961300532023

Epoch: 6| Step: 7
Training loss: 1.8027534484863281
Validation loss: 2.2700780232747397

Epoch: 6| Step: 8
Training loss: 1.3028475046157837
Validation loss: 2.2855873107910156

Epoch: 6| Step: 9
Training loss: 1.2499682903289795
Validation loss: 2.281732678413391

Epoch: 6| Step: 10
Training loss: 1.8987773656845093
Validation loss: 2.259128828843435

Epoch: 6| Step: 11
Training loss: 1.4708819389343262
Validation loss: 2.253668248653412

Epoch: 6| Step: 12
Training loss: 1.9085195064544678
Validation loss: 2.229582667350769

Epoch: 6| Step: 13
Training loss: 2.098991870880127
Validation loss: 2.2617502013842263

Epoch: 320| Step: 0
Training loss: 1.333957552909851
Validation loss: 2.24459707736969

Epoch: 6| Step: 1
Training loss: 1.8784540891647339
Validation loss: 2.281278669834137

Epoch: 6| Step: 2
Training loss: 1.4647772312164307
Validation loss: 2.2662254571914673

Epoch: 6| Step: 3
Training loss: 1.3425121307373047
Validation loss: 2.2881510853767395

Epoch: 6| Step: 4
Training loss: 1.9581247568130493
Validation loss: 2.2770885825157166

Epoch: 6| Step: 5
Training loss: 1.6258232593536377
Validation loss: 2.2642343441645303

Epoch: 6| Step: 6
Training loss: 1.6224348545074463
Validation loss: 2.2396141489346824

Epoch: 6| Step: 7
Training loss: 2.6611454486846924
Validation loss: 2.2575053373972573

Epoch: 6| Step: 8
Training loss: 1.4932866096496582
Validation loss: 2.24475226799647

Epoch: 6| Step: 9
Training loss: 1.3874467611312866
Validation loss: 2.247737546761831

Epoch: 6| Step: 10
Training loss: 1.1411387920379639
Validation loss: 2.2347165942192078

Epoch: 6| Step: 11
Training loss: 2.8488006591796875
Validation loss: 2.213957726955414

Epoch: 6| Step: 12
Training loss: 1.4708601236343384
Validation loss: 2.225245932737986

Epoch: 6| Step: 13
Training loss: 1.3949278593063354
Validation loss: 2.1939056118329368

Epoch: 321| Step: 0
Training loss: 2.14713454246521
Validation loss: 2.2042463620503745

Epoch: 6| Step: 1
Training loss: 1.3394699096679688
Validation loss: 2.2150312860806785

Epoch: 6| Step: 2
Training loss: 1.0600022077560425
Validation loss: 2.2030422687530518

Epoch: 6| Step: 3
Training loss: 2.037614345550537
Validation loss: 2.207089841365814

Epoch: 6| Step: 4
Training loss: 1.606594204902649
Validation loss: 2.210605025291443

Epoch: 6| Step: 5
Training loss: 2.057307243347168
Validation loss: 2.2533602714538574

Epoch: 6| Step: 6
Training loss: 2.304147243499756
Validation loss: 2.231735050678253

Epoch: 6| Step: 7
Training loss: 1.2892658710479736
Validation loss: 2.228805442651113

Epoch: 6| Step: 8
Training loss: 1.6210342645645142
Validation loss: 2.251363972822825

Epoch: 6| Step: 9
Training loss: 1.3971984386444092
Validation loss: 2.2376550436019897

Epoch: 6| Step: 10
Training loss: 1.743403434753418
Validation loss: 2.2183942000071206

Epoch: 6| Step: 11
Training loss: 1.8456065654754639
Validation loss: 2.268406848112742

Epoch: 6| Step: 12
Training loss: 1.5943927764892578
Validation loss: 2.2586336533228555

Epoch: 6| Step: 13
Training loss: 1.5538136959075928
Validation loss: 2.245614926020304

Epoch: 322| Step: 0
Training loss: 1.7400559186935425
Validation loss: 2.272801617781321

Epoch: 6| Step: 1
Training loss: 1.5487408638000488
Validation loss: 2.2960270643234253

Epoch: 6| Step: 2
Training loss: 2.032151699066162
Validation loss: 2.3365866343180337

Epoch: 6| Step: 3
Training loss: 1.4041709899902344
Validation loss: 2.3423553109169006

Epoch: 6| Step: 4
Training loss: 2.2167630195617676
Validation loss: 2.3471486369768777

Epoch: 6| Step: 5
Training loss: 2.115671157836914
Validation loss: 2.3323326110839844

Epoch: 6| Step: 6
Training loss: 1.6881625652313232
Validation loss: 2.3367294867833457

Epoch: 6| Step: 7
Training loss: 1.8744194507598877
Validation loss: 2.350894033908844

Epoch: 6| Step: 8
Training loss: 1.9470397233963013
Validation loss: 2.3495994011561074

Epoch: 6| Step: 9
Training loss: 1.549380898475647
Validation loss: 2.332795282204946

Epoch: 6| Step: 10
Training loss: 1.1341493129730225
Validation loss: 2.3328724304835

Epoch: 6| Step: 11
Training loss: 1.9190515279769897
Validation loss: 2.291574319203695

Epoch: 6| Step: 12
Training loss: 1.5508160591125488
Validation loss: 2.299552400906881

Epoch: 6| Step: 13
Training loss: 1.0998061895370483
Validation loss: 2.3103383580843606

Epoch: 323| Step: 0
Training loss: 2.305440902709961
Validation loss: 2.296805719534556

Epoch: 6| Step: 1
Training loss: 2.5268356800079346
Validation loss: 2.2921146353085837

Epoch: 6| Step: 2
Training loss: 1.1419316530227661
Validation loss: 2.2911521196365356

Epoch: 6| Step: 3
Training loss: 1.5674785375595093
Validation loss: 2.3174702326456704

Epoch: 6| Step: 4
Training loss: 1.3842601776123047
Validation loss: 2.3113246957461038

Epoch: 6| Step: 5
Training loss: 1.9507763385772705
Validation loss: 2.3237187465031943

Epoch: 6| Step: 6
Training loss: 1.6448092460632324
Validation loss: 2.3179614742596946

Epoch: 6| Step: 7
Training loss: 1.5379250049591064
Validation loss: 2.288035790125529

Epoch: 6| Step: 8
Training loss: 1.6016924381256104
Validation loss: 2.323137720425924

Epoch: 6| Step: 9
Training loss: 1.6473784446716309
Validation loss: 2.3343766927719116

Epoch: 6| Step: 10
Training loss: 1.17103111743927
Validation loss: 2.2972867488861084

Epoch: 6| Step: 11
Training loss: 2.129312038421631
Validation loss: 2.289894421895345

Epoch: 6| Step: 12
Training loss: 1.7841854095458984
Validation loss: 2.265462795893351

Epoch: 6| Step: 13
Training loss: 1.2680118083953857
Validation loss: 2.300773322582245

Epoch: 324| Step: 0
Training loss: 2.080676794052124
Validation loss: 2.2786981662114463

Epoch: 6| Step: 1
Training loss: 1.4275258779525757
Validation loss: 2.2619953950246177

Epoch: 6| Step: 2
Training loss: 1.8683137893676758
Validation loss: 2.253857453664144

Epoch: 6| Step: 3
Training loss: 1.7557148933410645
Validation loss: 2.2430634101231894

Epoch: 6| Step: 4
Training loss: 1.8692619800567627
Validation loss: 2.2466257015864053

Epoch: 6| Step: 5
Training loss: 1.8382225036621094
Validation loss: 2.2528286576271057

Epoch: 6| Step: 6
Training loss: 1.0868861675262451
Validation loss: 2.264563004175822

Epoch: 6| Step: 7
Training loss: 1.7043211460113525
Validation loss: 2.237589875857035

Epoch: 6| Step: 8
Training loss: 2.07818603515625
Validation loss: 2.284861167271932

Epoch: 6| Step: 9
Training loss: 2.2899274826049805
Validation loss: 2.3147659301757812

Epoch: 6| Step: 10
Training loss: 1.6794593334197998
Validation loss: 2.3207637071609497

Epoch: 6| Step: 11
Training loss: 1.6789038181304932
Validation loss: 2.3159248431523642

Epoch: 6| Step: 12
Training loss: 1.0794960260391235
Validation loss: 2.3337173461914062

Epoch: 6| Step: 13
Training loss: 1.4964776039123535
Validation loss: 2.3297566175460815

Epoch: 325| Step: 0
Training loss: 0.9578284621238708
Validation loss: 2.2891258001327515

Epoch: 6| Step: 1
Training loss: 1.275020718574524
Validation loss: 2.2743367751439414

Epoch: 6| Step: 2
Training loss: 1.646161437034607
Validation loss: 2.2507628202438354

Epoch: 6| Step: 3
Training loss: 1.2682548761367798
Validation loss: 2.263146162033081

Epoch: 6| Step: 4
Training loss: 1.8935441970825195
Validation loss: 2.276453733444214

Epoch: 6| Step: 5
Training loss: 1.4068193435668945
Validation loss: 2.275230288505554

Epoch: 6| Step: 6
Training loss: 1.7922866344451904
Validation loss: 2.2605092922846475

Epoch: 6| Step: 7
Training loss: 1.902930736541748
Validation loss: 2.249995748202006

Epoch: 6| Step: 8
Training loss: 1.3657889366149902
Validation loss: 2.2393314441045127

Epoch: 6| Step: 9
Training loss: 1.8959811925888062
Validation loss: 2.228135069211324

Epoch: 6| Step: 10
Training loss: 1.93988835811615
Validation loss: 2.2408878803253174

Epoch: 6| Step: 11
Training loss: 1.7174652814865112
Validation loss: 2.268339157104492

Epoch: 6| Step: 12
Training loss: 1.6490943431854248
Validation loss: 2.234975278377533

Epoch: 6| Step: 13
Training loss: 2.071262836456299
Validation loss: 2.23874431848526

Epoch: 326| Step: 0
Training loss: 1.4960294961929321
Validation loss: 2.2651988665262857

Epoch: 6| Step: 1
Training loss: 1.6781010627746582
Validation loss: 2.2712966005007424

Epoch: 6| Step: 2
Training loss: 1.7471891641616821
Validation loss: 2.26375679175059

Epoch: 6| Step: 3
Training loss: 1.3015613555908203
Validation loss: 2.246845861275991

Epoch: 6| Step: 4
Training loss: 1.586108922958374
Validation loss: 2.2751962343851724

Epoch: 6| Step: 5
Training loss: 1.4652353525161743
Validation loss: 2.268913666407267

Epoch: 6| Step: 6
Training loss: 2.128519058227539
Validation loss: 2.2397212982177734

Epoch: 6| Step: 7
Training loss: 2.628113031387329
Validation loss: 2.2771708766619363

Epoch: 6| Step: 8
Training loss: 2.1096577644348145
Validation loss: 2.2810327212015786

Epoch: 6| Step: 9
Training loss: 1.5798479318618774
Validation loss: 2.251310427983602

Epoch: 6| Step: 10
Training loss: 0.8517071604728699
Validation loss: 2.239498237768809

Epoch: 6| Step: 11
Training loss: 1.663908839225769
Validation loss: 2.26149312655131

Epoch: 6| Step: 12
Training loss: 1.1972311735153198
Validation loss: 2.2762993574142456

Epoch: 6| Step: 13
Training loss: 1.5582553148269653
Validation loss: 2.2649048368136087

Epoch: 327| Step: 0
Training loss: 1.695950984954834
Validation loss: 2.278702954451243

Epoch: 6| Step: 1
Training loss: 2.006340980529785
Validation loss: 2.305458684762319

Epoch: 6| Step: 2
Training loss: 1.8054581880569458
Validation loss: 2.269338309764862

Epoch: 6| Step: 3
Training loss: 1.7532312870025635
Validation loss: 2.2571540077527366

Epoch: 6| Step: 4
Training loss: 1.2822805643081665
Validation loss: 2.2744682232538858

Epoch: 6| Step: 5
Training loss: 1.1970534324645996
Validation loss: 2.2627761562665305

Epoch: 6| Step: 6
Training loss: 2.2933573722839355
Validation loss: 2.2285101811091104

Epoch: 6| Step: 7
Training loss: 1.360959768295288
Validation loss: 2.261830449104309

Epoch: 6| Step: 8
Training loss: 1.6341480016708374
Validation loss: 2.2401073575019836

Epoch: 6| Step: 9
Training loss: 1.0674439668655396
Validation loss: 2.237482229868571

Epoch: 6| Step: 10
Training loss: 1.7320071458816528
Validation loss: 2.2447621822357178

Epoch: 6| Step: 11
Training loss: 2.0137665271759033
Validation loss: 2.2798900604248047

Epoch: 6| Step: 12
Training loss: 1.4647353887557983
Validation loss: 2.2818992336591086

Epoch: 6| Step: 13
Training loss: 1.7749111652374268
Validation loss: 2.2654599150021872

Epoch: 328| Step: 0
Training loss: 2.0291171073913574
Validation loss: 2.2916918794314065

Epoch: 6| Step: 1
Training loss: 2.4064531326293945
Validation loss: 2.2904948592185974

Epoch: 6| Step: 2
Training loss: 1.2452954053878784
Validation loss: 2.300442695617676

Epoch: 6| Step: 3
Training loss: 1.5788631439208984
Validation loss: 2.282213012377421

Epoch: 6| Step: 4
Training loss: 1.3296047449111938
Validation loss: 2.3303984800974527

Epoch: 6| Step: 5
Training loss: 2.0311527252197266
Validation loss: 2.325028936068217

Epoch: 6| Step: 6
Training loss: 1.882314920425415
Validation loss: 2.286013662815094

Epoch: 6| Step: 7
Training loss: 1.5711396932601929
Validation loss: 2.2926896015803018

Epoch: 6| Step: 8
Training loss: 1.967687726020813
Validation loss: 2.258263647556305

Epoch: 6| Step: 9
Training loss: 1.4448965787887573
Validation loss: 2.229156970977783

Epoch: 6| Step: 10
Training loss: 1.1787912845611572
Validation loss: 2.228189786275228

Epoch: 6| Step: 11
Training loss: 1.517043113708496
Validation loss: 2.2524953484535217

Epoch: 6| Step: 12
Training loss: 2.0137269496917725
Validation loss: 2.2238319714864097

Epoch: 6| Step: 13
Training loss: 1.194238543510437
Validation loss: 2.253688156604767

Epoch: 329| Step: 0
Training loss: 1.1893442869186401
Validation loss: 2.276274879773458

Epoch: 6| Step: 1
Training loss: 1.8997304439544678
Validation loss: 2.290473977724711

Epoch: 6| Step: 2
Training loss: 1.5458645820617676
Validation loss: 2.328476905822754

Epoch: 6| Step: 3
Training loss: 2.357534646987915
Validation loss: 2.3122607469558716

Epoch: 6| Step: 4
Training loss: 1.2184975147247314
Validation loss: 2.306506633758545

Epoch: 6| Step: 5
Training loss: 1.1700935363769531
Validation loss: 2.3311848441759744

Epoch: 6| Step: 6
Training loss: 1.6614224910736084
Validation loss: 2.3103750944137573

Epoch: 6| Step: 7
Training loss: 1.2233248949050903
Validation loss: 2.282300055027008

Epoch: 6| Step: 8
Training loss: 2.272902488708496
Validation loss: 2.312369187672933

Epoch: 6| Step: 9
Training loss: 1.5020315647125244
Validation loss: 2.2920348842938743

Epoch: 6| Step: 10
Training loss: 1.6269932985305786
Validation loss: 2.2726639906565347

Epoch: 6| Step: 11
Training loss: 1.6477835178375244
Validation loss: 2.3138274550437927

Epoch: 6| Step: 12
Training loss: 2.256995916366577
Validation loss: 2.2483208179473877

Epoch: 6| Step: 13
Training loss: 1.6446220874786377
Validation loss: 2.272828757762909

Epoch: 330| Step: 0
Training loss: 1.8669562339782715
Validation loss: 2.2891691525777182

Epoch: 6| Step: 1
Training loss: 1.468381643295288
Validation loss: 2.29261847337087

Epoch: 6| Step: 2
Training loss: 1.2799997329711914
Validation loss: 2.3049182891845703

Epoch: 6| Step: 3
Training loss: 1.2250795364379883
Validation loss: 2.3093762000401816

Epoch: 6| Step: 4
Training loss: 2.1212668418884277
Validation loss: 2.3310097455978394

Epoch: 6| Step: 5
Training loss: 1.2750513553619385
Validation loss: 2.33964866399765

Epoch: 6| Step: 6
Training loss: 1.9832370281219482
Validation loss: 2.3261566162109375

Epoch: 6| Step: 7
Training loss: 1.4146428108215332
Validation loss: 2.3491648038228354

Epoch: 6| Step: 8
Training loss: 1.8109221458435059
Validation loss: 2.3509933749834695

Epoch: 6| Step: 9
Training loss: 1.9376016855239868
Validation loss: 2.3586840629577637

Epoch: 6| Step: 10
Training loss: 1.6180109977722168
Validation loss: 2.305708964665731

Epoch: 6| Step: 11
Training loss: 1.4260268211364746
Validation loss: 2.284734090169271

Epoch: 6| Step: 12
Training loss: 1.5162582397460938
Validation loss: 2.311232626438141

Epoch: 6| Step: 13
Training loss: 2.338379383087158
Validation loss: 2.3040876189867654

Epoch: 331| Step: 0
Training loss: 1.6160519123077393
Validation loss: 2.291345556577047

Epoch: 6| Step: 1
Training loss: 1.4887592792510986
Validation loss: 2.2534104386965432

Epoch: 6| Step: 2
Training loss: 1.9925724267959595
Validation loss: 2.239833970864614

Epoch: 6| Step: 3
Training loss: 1.976597785949707
Validation loss: 2.2523731787999473

Epoch: 6| Step: 4
Training loss: 1.5902431011199951
Validation loss: 2.253063122431437

Epoch: 6| Step: 5
Training loss: 1.537192940711975
Validation loss: 2.2604602177937827

Epoch: 6| Step: 6
Training loss: 1.19288969039917
Validation loss: 2.2740255991617837

Epoch: 6| Step: 7
Training loss: 2.7216832637786865
Validation loss: 2.2771889368693032

Epoch: 6| Step: 8
Training loss: 1.3195143938064575
Validation loss: 2.2835699717203775

Epoch: 6| Step: 9
Training loss: 1.7490344047546387
Validation loss: 2.3167986075083413

Epoch: 6| Step: 10
Training loss: 1.5955629348754883
Validation loss: 2.325333833694458

Epoch: 6| Step: 11
Training loss: 1.7217841148376465
Validation loss: 2.332762082417806

Epoch: 6| Step: 12
Training loss: 1.6240381002426147
Validation loss: 2.348698616027832

Epoch: 6| Step: 13
Training loss: 1.3700186014175415
Validation loss: 2.321965436140696

Epoch: 332| Step: 0
Training loss: 1.735187292098999
Validation loss: 2.3434605995814004

Epoch: 6| Step: 1
Training loss: 1.6620522737503052
Validation loss: 2.344970226287842

Epoch: 6| Step: 2
Training loss: 1.9307191371917725
Validation loss: 2.3331955671310425

Epoch: 6| Step: 3
Training loss: 1.694307565689087
Validation loss: 2.283690611521403

Epoch: 6| Step: 4
Training loss: 0.8689180612564087
Validation loss: 2.2780989011128745

Epoch: 6| Step: 5
Training loss: 1.2623144388198853
Validation loss: 2.23065976301829

Epoch: 6| Step: 6
Training loss: 2.1085598468780518
Validation loss: 2.2379400928815207

Epoch: 6| Step: 7
Training loss: 1.9303785562515259
Validation loss: 2.2462340593338013

Epoch: 6| Step: 8
Training loss: 1.4567251205444336
Validation loss: 2.240908622741699

Epoch: 6| Step: 9
Training loss: 1.316380500793457
Validation loss: 2.247320612271627

Epoch: 6| Step: 10
Training loss: 1.7190890312194824
Validation loss: 2.2498496969540915

Epoch: 6| Step: 11
Training loss: 1.366140365600586
Validation loss: 2.2752749919891357

Epoch: 6| Step: 12
Training loss: 1.24932861328125
Validation loss: 2.2237019340197244

Epoch: 6| Step: 13
Training loss: 2.659346580505371
Validation loss: 2.27412740389506

Epoch: 333| Step: 0
Training loss: 1.9039254188537598
Validation loss: 2.270181715488434

Epoch: 6| Step: 1
Training loss: 1.0812959671020508
Validation loss: 2.2929739157358804

Epoch: 6| Step: 2
Training loss: 1.5326225757598877
Validation loss: 2.304557998975118

Epoch: 6| Step: 3
Training loss: 1.5545905828475952
Validation loss: 2.2721901734670005

Epoch: 6| Step: 4
Training loss: 1.5033241510391235
Validation loss: 2.2800397872924805

Epoch: 6| Step: 5
Training loss: 1.8761687278747559
Validation loss: 2.2617754340171814

Epoch: 6| Step: 6
Training loss: 1.2695163488388062
Validation loss: 2.2983633081118264

Epoch: 6| Step: 7
Training loss: 1.1234138011932373
Validation loss: 2.2784327268600464

Epoch: 6| Step: 8
Training loss: 1.9641319513320923
Validation loss: 2.2840338548024497

Epoch: 6| Step: 9
Training loss: 1.8593348264694214
Validation loss: 2.2740683952967324

Epoch: 6| Step: 10
Training loss: 1.7355315685272217
Validation loss: 2.2633586128552756

Epoch: 6| Step: 11
Training loss: 2.430332660675049
Validation loss: 2.284890095392863

Epoch: 6| Step: 12
Training loss: 0.9423732757568359
Validation loss: 2.2871132095654807

Epoch: 6| Step: 13
Training loss: 1.7794831991195679
Validation loss: 2.2942955493927

Epoch: 334| Step: 0
Training loss: 1.3428573608398438
Validation loss: 2.304785450299581

Epoch: 6| Step: 1
Training loss: 1.6433889865875244
Validation loss: 2.3357625802357993

Epoch: 6| Step: 2
Training loss: 1.7630119323730469
Validation loss: 2.3454509973526

Epoch: 6| Step: 3
Training loss: 1.9036784172058105
Validation loss: 2.336150328318278

Epoch: 6| Step: 4
Training loss: 1.2010198831558228
Validation loss: 2.3282830715179443

Epoch: 6| Step: 5
Training loss: 1.5718474388122559
Validation loss: 2.3206181128819785

Epoch: 6| Step: 6
Training loss: 1.5673904418945312
Validation loss: 2.2572745084762573

Epoch: 6| Step: 7
Training loss: 2.0023670196533203
Validation loss: 2.297771175702413

Epoch: 6| Step: 8
Training loss: 1.264070749282837
Validation loss: 2.2994381189346313

Epoch: 6| Step: 9
Training loss: 2.0191338062286377
Validation loss: 2.2572412689526877

Epoch: 6| Step: 10
Training loss: 1.2040127515792847
Validation loss: 2.2327563166618347

Epoch: 6| Step: 11
Training loss: 1.7623871564865112
Validation loss: 2.268526554107666

Epoch: 6| Step: 12
Training loss: 1.8741052150726318
Validation loss: 2.2631203730901084

Epoch: 6| Step: 13
Training loss: 1.219712257385254
Validation loss: 2.2893351117769876

Epoch: 335| Step: 0
Training loss: 1.087801456451416
Validation loss: 2.2851624290148416

Epoch: 6| Step: 1
Training loss: 2.1465444564819336
Validation loss: 2.302585860093435

Epoch: 6| Step: 2
Training loss: 1.2927024364471436
Validation loss: 2.3111006021499634

Epoch: 6| Step: 3
Training loss: 1.129065990447998
Validation loss: 2.3192347288131714

Epoch: 6| Step: 4
Training loss: 1.9339401721954346
Validation loss: 2.2963958581288657

Epoch: 6| Step: 5
Training loss: 1.25319504737854
Validation loss: 2.266221026579539

Epoch: 6| Step: 6
Training loss: 2.540099620819092
Validation loss: 2.2788361310958862

Epoch: 6| Step: 7
Training loss: 1.5425950288772583
Validation loss: 2.276859780152639

Epoch: 6| Step: 8
Training loss: 2.0191287994384766
Validation loss: 2.3143304586410522

Epoch: 6| Step: 9
Training loss: 1.124202013015747
Validation loss: 2.305915276209513

Epoch: 6| Step: 10
Training loss: 2.1392738819122314
Validation loss: 2.306544621785482

Epoch: 6| Step: 11
Training loss: 1.425279140472412
Validation loss: 2.302315970261892

Epoch: 6| Step: 12
Training loss: 1.6564640998840332
Validation loss: 2.295471648375193

Epoch: 6| Step: 13
Training loss: 1.1886156797409058
Validation loss: 2.304021338621775

Epoch: 336| Step: 0
Training loss: 1.3953152894973755
Validation loss: 2.3315533995628357

Epoch: 6| Step: 1
Training loss: 1.2938863039016724
Validation loss: 2.3647584517796836

Epoch: 6| Step: 2
Training loss: 1.4294171333312988
Validation loss: 2.3394776582717896

Epoch: 6| Step: 3
Training loss: 1.565700888633728
Validation loss: 2.3122761646906533

Epoch: 6| Step: 4
Training loss: 1.2288613319396973
Validation loss: 2.3169750372568765

Epoch: 6| Step: 5
Training loss: 1.6892927885055542
Validation loss: 2.278019368648529

Epoch: 6| Step: 6
Training loss: 1.848339557647705
Validation loss: 2.2684048811594644

Epoch: 6| Step: 7
Training loss: 1.8249109983444214
Validation loss: 2.3101022243499756

Epoch: 6| Step: 8
Training loss: 2.1048922538757324
Validation loss: 2.3048715790112815

Epoch: 6| Step: 9
Training loss: 1.7023396492004395
Validation loss: 2.3192275762557983

Epoch: 6| Step: 10
Training loss: 1.467238426208496
Validation loss: 2.337070862452189

Epoch: 6| Step: 11
Training loss: 1.9932830333709717
Validation loss: 2.3525043527285256

Epoch: 6| Step: 12
Training loss: 0.8040568828582764
Validation loss: 2.329142411549886

Epoch: 6| Step: 13
Training loss: 2.0702579021453857
Validation loss: 2.346065104007721

Epoch: 337| Step: 0
Training loss: 0.9093694090843201
Validation loss: 2.3769254088401794

Epoch: 6| Step: 1
Training loss: 1.352562665939331
Validation loss: 2.3823867638905845

Epoch: 6| Step: 2
Training loss: 1.2800159454345703
Validation loss: 2.3332382837931314

Epoch: 6| Step: 3
Training loss: 1.8299424648284912
Validation loss: 2.2634340127309165

Epoch: 6| Step: 4
Training loss: 1.747439980506897
Validation loss: 2.2869988878568015

Epoch: 6| Step: 5
Training loss: 1.2099095582962036
Validation loss: 2.219530542691549

Epoch: 6| Step: 6
Training loss: 2.0661511421203613
Validation loss: 2.235815942287445

Epoch: 6| Step: 7
Training loss: 1.9400043487548828
Validation loss: 2.219668706258138

Epoch: 6| Step: 8
Training loss: 1.9660389423370361
Validation loss: 2.2134933272997537

Epoch: 6| Step: 9
Training loss: 2.2905309200286865
Validation loss: 2.2029542922973633

Epoch: 6| Step: 10
Training loss: 1.8567774295806885
Validation loss: 2.1884317795435586

Epoch: 6| Step: 11
Training loss: 1.9118835926055908
Validation loss: 2.20929084221522

Epoch: 6| Step: 12
Training loss: 1.881886601448059
Validation loss: 2.2258946895599365

Epoch: 6| Step: 13
Training loss: 1.5956865549087524
Validation loss: 2.2253605922063193

Epoch: 338| Step: 0
Training loss: 1.2559046745300293
Validation loss: 2.2601895531018577

Epoch: 6| Step: 1
Training loss: 1.112424373626709
Validation loss: 2.2530271410942078

Epoch: 6| Step: 2
Training loss: 1.4859169721603394
Validation loss: 2.2630906899770102

Epoch: 6| Step: 3
Training loss: 2.065049409866333
Validation loss: 2.285517772038778

Epoch: 6| Step: 4
Training loss: 2.4242827892303467
Validation loss: 2.355038285255432

Epoch: 6| Step: 5
Training loss: 1.143934726715088
Validation loss: 2.3506569465001426

Epoch: 6| Step: 6
Training loss: 1.1159186363220215
Validation loss: 2.3591255346934

Epoch: 6| Step: 7
Training loss: 1.3748424053192139
Validation loss: 2.327131470044454

Epoch: 6| Step: 8
Training loss: 1.751497745513916
Validation loss: 2.3538678884506226

Epoch: 6| Step: 9
Training loss: 1.4508051872253418
Validation loss: 2.3371407190958657

Epoch: 6| Step: 10
Training loss: 2.1860454082489014
Validation loss: 2.3820489645004272

Epoch: 6| Step: 11
Training loss: 1.5516834259033203
Validation loss: 2.369228720664978

Epoch: 6| Step: 12
Training loss: 1.8610384464263916
Validation loss: 2.3299859364827475

Epoch: 6| Step: 13
Training loss: 1.9422051906585693
Validation loss: 2.3110541899998984

Epoch: 339| Step: 0
Training loss: 1.8118767738342285
Validation loss: 2.27235875527064

Epoch: 6| Step: 1
Training loss: 2.0000181198120117
Validation loss: 2.278420607248942

Epoch: 6| Step: 2
Training loss: 1.0042376518249512
Validation loss: 2.26245125134786

Epoch: 6| Step: 3
Training loss: 2.023409366607666
Validation loss: 2.2492176294326782

Epoch: 6| Step: 4
Training loss: 1.8731839656829834
Validation loss: 2.246980826059977

Epoch: 6| Step: 5
Training loss: 1.1883209943771362
Validation loss: 2.255274792512258

Epoch: 6| Step: 6
Training loss: 1.2170637845993042
Validation loss: 2.2775431672732034

Epoch: 6| Step: 7
Training loss: 0.8957561254501343
Validation loss: 2.2904001474380493

Epoch: 6| Step: 8
Training loss: 1.6165740489959717
Validation loss: 2.3043216665585837

Epoch: 6| Step: 9
Training loss: 1.9803791046142578
Validation loss: 2.315503557523092

Epoch: 6| Step: 10
Training loss: 1.7995048761367798
Validation loss: 2.3365469376246133

Epoch: 6| Step: 11
Training loss: 1.6341500282287598
Validation loss: 2.297887166341146

Epoch: 6| Step: 12
Training loss: 1.5061471462249756
Validation loss: 2.2970559199651084

Epoch: 6| Step: 13
Training loss: 2.111471176147461
Validation loss: 2.31601889928182

Epoch: 340| Step: 0
Training loss: 1.6746512651443481
Validation loss: 2.271163602670034

Epoch: 6| Step: 1
Training loss: 1.496704339981079
Validation loss: 2.2521815101305642

Epoch: 6| Step: 2
Training loss: 1.8114612102508545
Validation loss: 2.2289268573125205

Epoch: 6| Step: 3
Training loss: 0.9180601239204407
Validation loss: 2.205288827419281

Epoch: 6| Step: 4
Training loss: 1.0100507736206055
Validation loss: 2.264397939046224

Epoch: 6| Step: 5
Training loss: 1.7390066385269165
Validation loss: 2.2514726519584656

Epoch: 6| Step: 6
Training loss: 1.4848355054855347
Validation loss: 2.244571586449941

Epoch: 6| Step: 7
Training loss: 1.7704569101333618
Validation loss: 2.24416975180308

Epoch: 6| Step: 8
Training loss: 1.5976133346557617
Validation loss: 2.220302939414978

Epoch: 6| Step: 9
Training loss: 1.612247347831726
Validation loss: 2.2376792629559836

Epoch: 6| Step: 10
Training loss: 2.49564790725708
Validation loss: 2.2243804335594177

Epoch: 6| Step: 11
Training loss: 1.9811558723449707
Validation loss: 2.2408297459284463

Epoch: 6| Step: 12
Training loss: 1.8201301097869873
Validation loss: 2.2101568579673767

Epoch: 6| Step: 13
Training loss: 1.661646842956543
Validation loss: 2.2647190491358438

Epoch: 341| Step: 0
Training loss: 1.8746196031570435
Validation loss: 2.330474396546682

Epoch: 6| Step: 1
Training loss: 1.8273394107818604
Validation loss: 2.357636511325836

Epoch: 6| Step: 2
Training loss: 1.9437854290008545
Validation loss: 2.34302548567454

Epoch: 6| Step: 3
Training loss: 1.8842684030532837
Validation loss: 2.3536723852157593

Epoch: 6| Step: 4
Training loss: 1.1895992755889893
Validation loss: 2.325428009033203

Epoch: 6| Step: 5
Training loss: 1.0169678926467896
Validation loss: 2.352476159731547

Epoch: 6| Step: 6
Training loss: 1.00737464427948
Validation loss: 2.3421971003214517

Epoch: 6| Step: 7
Training loss: 2.1133599281311035
Validation loss: 2.2939404249191284

Epoch: 6| Step: 8
Training loss: 1.5515835285186768
Validation loss: 2.2706828514734902

Epoch: 6| Step: 9
Training loss: 1.540289044380188
Validation loss: 2.2619005044301352

Epoch: 6| Step: 10
Training loss: 1.7138034105300903
Validation loss: 2.2456396023432412

Epoch: 6| Step: 11
Training loss: 1.7297810316085815
Validation loss: 2.266336421171824

Epoch: 6| Step: 12
Training loss: 2.3286757469177246
Validation loss: 2.2633336981137595

Epoch: 6| Step: 13
Training loss: 1.630654215812683
Validation loss: 2.2628976106643677

Epoch: 342| Step: 0
Training loss: 1.3299789428710938
Validation loss: 2.2745973070462546

Epoch: 6| Step: 1
Training loss: 1.3871819972991943
Validation loss: 2.2732680439949036

Epoch: 6| Step: 2
Training loss: 1.4003950357437134
Validation loss: 2.242023150126139

Epoch: 6| Step: 3
Training loss: 1.5672646760940552
Validation loss: 2.2651174465815225

Epoch: 6| Step: 4
Training loss: 1.3826652765274048
Validation loss: 2.3371225198109946

Epoch: 6| Step: 5
Training loss: 0.8883522748947144
Validation loss: 2.3320613503456116

Epoch: 6| Step: 6
Training loss: 2.1197900772094727
Validation loss: 2.347231368223826

Epoch: 6| Step: 7
Training loss: 1.3633031845092773
Validation loss: 2.400774657726288

Epoch: 6| Step: 8
Training loss: 1.7279002666473389
Validation loss: 2.3793931007385254

Epoch: 6| Step: 9
Training loss: 1.9124068021774292
Validation loss: 2.350020448366801

Epoch: 6| Step: 10
Training loss: 1.861057996749878
Validation loss: 2.3349428176879883

Epoch: 6| Step: 11
Training loss: 2.2358710765838623
Validation loss: 2.363913059234619

Epoch: 6| Step: 12
Training loss: 1.5669702291488647
Validation loss: 2.3251073360443115

Epoch: 6| Step: 13
Training loss: 2.000314712524414
Validation loss: 2.3312506079673767

Epoch: 343| Step: 0
Training loss: 1.4862847328186035
Validation loss: 2.343855142593384

Epoch: 6| Step: 1
Training loss: 1.4756441116333008
Validation loss: 2.319601595401764

Epoch: 6| Step: 2
Training loss: 1.31702721118927
Validation loss: 2.3023066918055215

Epoch: 6| Step: 3
Training loss: 1.5439789295196533
Validation loss: 2.2540531953175864

Epoch: 6| Step: 4
Training loss: 1.413360834121704
Validation loss: 2.2722408970197043

Epoch: 6| Step: 5
Training loss: 1.4566903114318848
Validation loss: 2.22399232784907

Epoch: 6| Step: 6
Training loss: 1.6585452556610107
Validation loss: 2.2095062136650085

Epoch: 6| Step: 7
Training loss: 2.3929874897003174
Validation loss: 2.2472945054372153

Epoch: 6| Step: 8
Training loss: 2.0899205207824707
Validation loss: 2.232488214969635

Epoch: 6| Step: 9
Training loss: 1.5829581022262573
Validation loss: 2.274280846118927

Epoch: 6| Step: 10
Training loss: 1.2099790573120117
Validation loss: 2.2428685625394187

Epoch: 6| Step: 11
Training loss: 1.3621126413345337
Validation loss: 2.227458397547404

Epoch: 6| Step: 12
Training loss: 1.8498517274856567
Validation loss: 2.2372616132100425

Epoch: 6| Step: 13
Training loss: 1.8544423580169678
Validation loss: 2.233349363009135

Epoch: 344| Step: 0
Training loss: 1.5471676588058472
Validation loss: 2.2306190729141235

Epoch: 6| Step: 1
Training loss: 2.2583553791046143
Validation loss: 2.2046979467074075

Epoch: 6| Step: 2
Training loss: 1.1069793701171875
Validation loss: 2.265702764193217

Epoch: 6| Step: 3
Training loss: 1.1321659088134766
Validation loss: 2.210698207219442

Epoch: 6| Step: 4
Training loss: 1.5088802576065063
Validation loss: 2.2337783773740134

Epoch: 6| Step: 5
Training loss: 1.142736554145813
Validation loss: 2.238009055455526

Epoch: 6| Step: 6
Training loss: 2.4098124504089355
Validation loss: 2.2602503498395285

Epoch: 6| Step: 7
Training loss: 1.1227933168411255
Validation loss: 2.240041355292002

Epoch: 6| Step: 8
Training loss: 1.0691834688186646
Validation loss: 2.246462821960449

Epoch: 6| Step: 9
Training loss: 2.0325989723205566
Validation loss: 2.2514308094978333

Epoch: 6| Step: 10
Training loss: 2.3713624477386475
Validation loss: 2.244425654411316

Epoch: 6| Step: 11
Training loss: 1.080119013786316
Validation loss: 2.2628217538197837

Epoch: 6| Step: 12
Training loss: 1.6989684104919434
Validation loss: 2.316712578137716

Epoch: 6| Step: 13
Training loss: 2.2145957946777344
Validation loss: 2.294343868891398

Epoch: 345| Step: 0
Training loss: 1.2919923067092896
Validation loss: 2.3132362167040506

Epoch: 6| Step: 1
Training loss: 1.3720197677612305
Validation loss: 2.316978613535563

Epoch: 6| Step: 2
Training loss: 2.3938064575195312
Validation loss: 2.3168428341547647

Epoch: 6| Step: 3
Training loss: 1.9864826202392578
Validation loss: 2.338684101899465

Epoch: 6| Step: 4
Training loss: 1.3324353694915771
Validation loss: 2.3655224641164145

Epoch: 6| Step: 5
Training loss: 1.7143343687057495
Validation loss: 2.308479428291321

Epoch: 6| Step: 6
Training loss: 2.1529946327209473
Validation loss: 2.3352313240369162

Epoch: 6| Step: 7
Training loss: 1.8638511896133423
Validation loss: 2.29999311765035

Epoch: 6| Step: 8
Training loss: 1.7294965982437134
Validation loss: 2.317169944445292

Epoch: 6| Step: 9
Training loss: 1.150768756866455
Validation loss: 2.3195889393488565

Epoch: 6| Step: 10
Training loss: 1.8901138305664062
Validation loss: 2.3170077006022134

Epoch: 6| Step: 11
Training loss: 0.9809080362319946
Validation loss: 2.3210641344388327

Epoch: 6| Step: 12
Training loss: 1.0643863677978516
Validation loss: 2.265594561894735

Epoch: 6| Step: 13
Training loss: 1.3558759689331055
Validation loss: 2.2930296460787454

Epoch: 346| Step: 0
Training loss: 1.9172147512435913
Validation loss: 2.227968394756317

Epoch: 6| Step: 1
Training loss: 1.558464527130127
Validation loss: 2.2737956444422402

Epoch: 6| Step: 2
Training loss: 1.810533881187439
Validation loss: 2.2375201185544333

Epoch: 6| Step: 3
Training loss: 1.0979357957839966
Validation loss: 2.2360204060872397

Epoch: 6| Step: 4
Training loss: 1.5303797721862793
Validation loss: 2.2678738236427307

Epoch: 6| Step: 5
Training loss: 1.2874314785003662
Validation loss: 2.2655733029047647

Epoch: 6| Step: 6
Training loss: 2.2755227088928223
Validation loss: 2.240586260954539

Epoch: 6| Step: 7
Training loss: 1.3392237424850464
Validation loss: 2.277513881524404

Epoch: 6| Step: 8
Training loss: 1.8943562507629395
Validation loss: 2.2848133643468223

Epoch: 6| Step: 9
Training loss: 1.7009739875793457
Validation loss: 2.2816922863324485

Epoch: 6| Step: 10
Training loss: 1.5647799968719482
Validation loss: 2.3064138293266296

Epoch: 6| Step: 11
Training loss: 1.1703901290893555
Validation loss: 2.2705324490865073

Epoch: 6| Step: 12
Training loss: 1.876810908317566
Validation loss: 2.28029465675354

Epoch: 6| Step: 13
Training loss: 1.4427785873413086
Validation loss: 2.258649011452993

Epoch: 347| Step: 0
Training loss: 1.6633297204971313
Validation loss: 2.239108383655548

Epoch: 6| Step: 1
Training loss: 1.007134199142456
Validation loss: 2.2772266070048013

Epoch: 6| Step: 2
Training loss: 1.3102072477340698
Validation loss: 2.243163824081421

Epoch: 6| Step: 3
Training loss: 1.8190544843673706
Validation loss: 2.2741465965906777

Epoch: 6| Step: 4
Training loss: 0.9655581712722778
Validation loss: 2.299648384253184

Epoch: 6| Step: 5
Training loss: 1.4296982288360596
Validation loss: 2.3253133495648703

Epoch: 6| Step: 6
Training loss: 1.1435387134552002
Validation loss: 2.338154753049215

Epoch: 6| Step: 7
Training loss: 2.152721405029297
Validation loss: 2.3115201791127524

Epoch: 6| Step: 8
Training loss: 1.8437700271606445
Validation loss: 2.350999573866526

Epoch: 6| Step: 9
Training loss: 1.0580135583877563
Validation loss: 2.355693260828654

Epoch: 6| Step: 10
Training loss: 1.841639518737793
Validation loss: 2.3090233206748962

Epoch: 6| Step: 11
Training loss: 2.4160826206207275
Validation loss: 2.320001721382141

Epoch: 6| Step: 12
Training loss: 1.5425753593444824
Validation loss: 2.3160738547643027

Epoch: 6| Step: 13
Training loss: 1.6364085674285889
Validation loss: 2.337259292602539

Epoch: 348| Step: 0
Training loss: 2.008058786392212
Validation loss: 2.3115132649739585

Epoch: 6| Step: 1
Training loss: 1.9366931915283203
Validation loss: 2.2896505196889243

Epoch: 6| Step: 2
Training loss: 1.3460389375686646
Validation loss: 2.3281956116358438

Epoch: 6| Step: 3
Training loss: 1.824966311454773
Validation loss: 2.3177741169929504

Epoch: 6| Step: 4
Training loss: 2.0398807525634766
Validation loss: 2.3055390318234763

Epoch: 6| Step: 5
Training loss: 1.4983855485916138
Validation loss: 2.2515453497568765

Epoch: 6| Step: 6
Training loss: 1.2215861082077026
Validation loss: 2.208171804745992

Epoch: 6| Step: 7
Training loss: 1.1753270626068115
Validation loss: 2.215096930662791

Epoch: 6| Step: 8
Training loss: 1.3261752128601074
Validation loss: 2.203885813554128

Epoch: 6| Step: 9
Training loss: 1.8229715824127197
Validation loss: 2.1996305187543235

Epoch: 6| Step: 10
Training loss: 1.764286994934082
Validation loss: 2.1993340253829956

Epoch: 6| Step: 11
Training loss: 1.4666099548339844
Validation loss: 2.228849411010742

Epoch: 6| Step: 12
Training loss: 1.215184211730957
Validation loss: 2.2790892521540322

Epoch: 6| Step: 13
Training loss: 1.9400544166564941
Validation loss: 2.2831960121790567

Epoch: 349| Step: 0
Training loss: 1.0918562412261963
Validation loss: 2.221500118573507

Epoch: 6| Step: 1
Training loss: 1.1495654582977295
Validation loss: 2.220421473185221

Epoch: 6| Step: 2
Training loss: 1.0014355182647705
Validation loss: 2.2349550326665244

Epoch: 6| Step: 3
Training loss: 1.315276861190796
Validation loss: 2.2415003577868142

Epoch: 6| Step: 4
Training loss: 1.5662802457809448
Validation loss: 2.2577039202054343

Epoch: 6| Step: 5
Training loss: 1.5758564472198486
Validation loss: 2.269229551156362

Epoch: 6| Step: 6
Training loss: 1.2930161952972412
Validation loss: 2.2483083605766296

Epoch: 6| Step: 7
Training loss: 1.956438422203064
Validation loss: 2.2666043837865195

Epoch: 6| Step: 8
Training loss: 1.8365516662597656
Validation loss: 2.2450624108314514

Epoch: 6| Step: 9
Training loss: 2.2660412788391113
Validation loss: 2.2930060625076294

Epoch: 6| Step: 10
Training loss: 1.755610466003418
Validation loss: 2.2712201873461404

Epoch: 6| Step: 11
Training loss: 1.2092372179031372
Validation loss: 2.300709307193756

Epoch: 6| Step: 12
Training loss: 1.908068299293518
Validation loss: 2.3012690941492715

Epoch: 6| Step: 13
Training loss: 2.1063711643218994
Validation loss: 2.3168782393137612

Epoch: 350| Step: 0
Training loss: 1.7630119323730469
Validation loss: 2.3424641291300454

Epoch: 6| Step: 1
Training loss: 1.4668811559677124
Validation loss: 2.329582929611206

Epoch: 6| Step: 2
Training loss: 1.3842270374298096
Validation loss: 2.304873843987783

Epoch: 6| Step: 3
Training loss: 1.877800464630127
Validation loss: 2.3436022202173867

Epoch: 6| Step: 4
Training loss: 1.754387617111206
Validation loss: 2.337019999821981

Epoch: 6| Step: 5
Training loss: 1.8207297325134277
Validation loss: 2.2807050744692483

Epoch: 6| Step: 6
Training loss: 1.0526031255722046
Validation loss: 2.3108665545781455

Epoch: 6| Step: 7
Training loss: 1.5842117071151733
Validation loss: 2.2610086599985757

Epoch: 6| Step: 8
Training loss: 2.0994529724121094
Validation loss: 2.259586811065674

Epoch: 6| Step: 9
Training loss: 0.8560706973075867
Validation loss: 2.276462515195211

Epoch: 6| Step: 10
Training loss: 1.4051918983459473
Validation loss: 2.339983026186625

Epoch: 6| Step: 11
Training loss: 2.213489055633545
Validation loss: 2.346090773741404

Epoch: 6| Step: 12
Training loss: 1.0807758569717407
Validation loss: 2.3490604956944785

Epoch: 6| Step: 13
Training loss: 1.6096373796463013
Validation loss: 2.328663984934489

Epoch: 351| Step: 0
Training loss: 1.2875092029571533
Validation loss: 2.3688175479571023

Epoch: 6| Step: 1
Training loss: 2.5490715503692627
Validation loss: 2.3832144339879355

Epoch: 6| Step: 2
Training loss: 1.1567635536193848
Validation loss: 2.363093137741089

Epoch: 6| Step: 3
Training loss: 1.6755620241165161
Validation loss: 2.3762660026550293

Epoch: 6| Step: 4
Training loss: 1.1405869722366333
Validation loss: 2.339468518892924

Epoch: 6| Step: 5
Training loss: 1.84083890914917
Validation loss: 2.316966633001963

Epoch: 6| Step: 6
Training loss: 1.4931941032409668
Validation loss: 2.327734351158142

Epoch: 6| Step: 7
Training loss: 1.5940260887145996
Validation loss: 2.283751130104065

Epoch: 6| Step: 8
Training loss: 1.9242056608200073
Validation loss: 2.3192813793818154

Epoch: 6| Step: 9
Training loss: 1.4960029125213623
Validation loss: 2.3154359261194863

Epoch: 6| Step: 10
Training loss: 1.563054084777832
Validation loss: 2.313994606335958

Epoch: 6| Step: 11
Training loss: 1.2495908737182617
Validation loss: 2.3013658920923867

Epoch: 6| Step: 12
Training loss: 1.5115694999694824
Validation loss: 2.2945071856180825

Epoch: 6| Step: 13
Training loss: 1.2893257141113281
Validation loss: 2.2678548097610474

Epoch: 352| Step: 0
Training loss: 1.4412646293640137
Validation loss: 2.2464255690574646

Epoch: 6| Step: 1
Training loss: 1.7655614614486694
Validation loss: 2.2407845656077066

Epoch: 6| Step: 2
Training loss: 1.9448509216308594
Validation loss: 2.2207356890042624

Epoch: 6| Step: 3
Training loss: 1.9215283393859863
Validation loss: 2.2404791911443076

Epoch: 6| Step: 4
Training loss: 1.6320363283157349
Validation loss: 2.2045202255249023

Epoch: 6| Step: 5
Training loss: 2.543545722961426
Validation loss: 2.211896300315857

Epoch: 6| Step: 6
Training loss: 1.311376690864563
Validation loss: 2.208870848019918

Epoch: 6| Step: 7
Training loss: 0.8757809400558472
Validation loss: 2.178734223047892

Epoch: 6| Step: 8
Training loss: 1.0895626544952393
Validation loss: 2.1979508996009827

Epoch: 6| Step: 9
Training loss: 1.5556074380874634
Validation loss: 2.189301908016205

Epoch: 6| Step: 10
Training loss: 0.9530667066574097
Validation loss: 2.1846452951431274

Epoch: 6| Step: 11
Training loss: 1.9398396015167236
Validation loss: 2.1944733460744223

Epoch: 6| Step: 12
Training loss: 2.4577438831329346
Validation loss: 2.2050857146581015

Epoch: 6| Step: 13
Training loss: 0.8767256736755371
Validation loss: 2.2005436023076377

Epoch: 353| Step: 0
Training loss: 2.4625930786132812
Validation loss: 2.2590765953063965

Epoch: 6| Step: 1
Training loss: 0.8622369766235352
Validation loss: 2.2632278005282083

Epoch: 6| Step: 2
Training loss: 1.6937686204910278
Validation loss: 2.2575667897860208

Epoch: 6| Step: 3
Training loss: 1.598982572555542
Validation loss: 2.26829993724823

Epoch: 6| Step: 4
Training loss: 1.5548206567764282
Validation loss: 2.2329189777374268

Epoch: 6| Step: 5
Training loss: 1.5567924976348877
Validation loss: 2.2825931310653687

Epoch: 6| Step: 6
Training loss: 0.9651103615760803
Validation loss: 2.279095987478892

Epoch: 6| Step: 7
Training loss: 1.3148107528686523
Validation loss: 2.318049430847168

Epoch: 6| Step: 8
Training loss: 1.945351004600525
Validation loss: 2.331201116243998

Epoch: 6| Step: 9
Training loss: 1.3678677082061768
Validation loss: 2.3267720341682434

Epoch: 6| Step: 10
Training loss: 1.1853084564208984
Validation loss: 2.3009078900019326

Epoch: 6| Step: 11
Training loss: 1.7289392948150635
Validation loss: 2.313438137372335

Epoch: 6| Step: 12
Training loss: 1.6617000102996826
Validation loss: 2.30637264251709

Epoch: 6| Step: 13
Training loss: 1.8029091358184814
Validation loss: 2.3286401828130088

Epoch: 354| Step: 0
Training loss: 1.6117243766784668
Validation loss: 2.311596433321635

Epoch: 6| Step: 1
Training loss: 1.8222596645355225
Validation loss: 2.3386067152023315

Epoch: 6| Step: 2
Training loss: 1.610306739807129
Validation loss: 2.350102881590525

Epoch: 6| Step: 3
Training loss: 1.4295220375061035
Validation loss: 2.3355385462443032

Epoch: 6| Step: 4
Training loss: 1.772742509841919
Validation loss: 2.363492012023926

Epoch: 6| Step: 5
Training loss: 1.6150197982788086
Validation loss: 2.343005100886027

Epoch: 6| Step: 6
Training loss: 2.109665632247925
Validation loss: 2.3370357155799866

Epoch: 6| Step: 7
Training loss: 1.068671464920044
Validation loss: 2.3236273527145386

Epoch: 6| Step: 8
Training loss: 1.0319761037826538
Validation loss: 2.2865917682647705

Epoch: 6| Step: 9
Training loss: 2.0826256275177
Validation loss: 2.2586878736813865

Epoch: 6| Step: 10
Training loss: 1.2991466522216797
Validation loss: 2.2294783194859824

Epoch: 6| Step: 11
Training loss: 1.386605143547058
Validation loss: 2.287359893321991

Epoch: 6| Step: 12
Training loss: 2.0861876010894775
Validation loss: 2.2740451892217

Epoch: 6| Step: 13
Training loss: 1.1895159482955933
Validation loss: 2.284857193628947

Epoch: 355| Step: 0
Training loss: 1.3674150705337524
Validation loss: 2.2735018134117126

Epoch: 6| Step: 1
Training loss: 1.199507713317871
Validation loss: 2.328134059906006

Epoch: 6| Step: 2
Training loss: 1.8356623649597168
Validation loss: 2.2494855523109436

Epoch: 6| Step: 3
Training loss: 1.5532724857330322
Validation loss: 2.254602094491323

Epoch: 6| Step: 4
Training loss: 1.0634430646896362
Validation loss: 2.2539459069569907

Epoch: 6| Step: 5
Training loss: 1.5524874925613403
Validation loss: 2.221014618873596

Epoch: 6| Step: 6
Training loss: 1.0558363199234009
Validation loss: 2.198231339454651

Epoch: 6| Step: 7
Training loss: 1.2412900924682617
Validation loss: 2.206435720125834

Epoch: 6| Step: 8
Training loss: 1.7162151336669922
Validation loss: 2.2467041611671448

Epoch: 6| Step: 9
Training loss: 2.3052566051483154
Validation loss: 2.2590240240097046

Epoch: 6| Step: 10
Training loss: 1.2813550233840942
Validation loss: 2.2869986097017923

Epoch: 6| Step: 11
Training loss: 1.73044753074646
Validation loss: 2.2829838196436563

Epoch: 6| Step: 12
Training loss: 3.1003835201263428
Validation loss: 2.2971343398094177

Epoch: 6| Step: 13
Training loss: 1.2767508029937744
Validation loss: 2.330864667892456

Epoch: 356| Step: 0
Training loss: 1.7904384136199951
Validation loss: 2.3346680800120034

Epoch: 6| Step: 1
Training loss: 0.8689560890197754
Validation loss: 2.34983100493749

Epoch: 6| Step: 2
Training loss: 1.4331392049789429
Validation loss: 2.3738983074824014

Epoch: 6| Step: 3
Training loss: 1.685018539428711
Validation loss: 2.3372565309206643

Epoch: 6| Step: 4
Training loss: 0.8526236414909363
Validation loss: 2.2701706687609353

Epoch: 6| Step: 5
Training loss: 1.8286738395690918
Validation loss: 2.286706348260244

Epoch: 6| Step: 6
Training loss: 1.5647960901260376
Validation loss: 2.2832373778025308

Epoch: 6| Step: 7
Training loss: 1.6953297853469849
Validation loss: 2.2892435789108276

Epoch: 6| Step: 8
Training loss: 1.4348952770233154
Validation loss: 2.2211486101150513

Epoch: 6| Step: 9
Training loss: 1.1715532541275024
Validation loss: 2.2455081741015115

Epoch: 6| Step: 10
Training loss: 2.1715333461761475
Validation loss: 2.254657963911692

Epoch: 6| Step: 11
Training loss: 1.864519715309143
Validation loss: 2.2576376597086587

Epoch: 6| Step: 12
Training loss: 1.2818859815597534
Validation loss: 2.2555869817733765

Epoch: 6| Step: 13
Training loss: 2.5330209732055664
Validation loss: 2.3044137159983316

Epoch: 357| Step: 0
Training loss: 1.0412216186523438
Validation loss: 2.325904885927836

Epoch: 6| Step: 1
Training loss: 0.9866732358932495
Validation loss: 2.3252824743588767

Epoch: 6| Step: 2
Training loss: 1.8830550909042358
Validation loss: 2.3677250941594443

Epoch: 6| Step: 3
Training loss: 1.6859896183013916
Validation loss: 2.344180146853129

Epoch: 6| Step: 4
Training loss: 1.841109275817871
Validation loss: 2.3734887838363647

Epoch: 6| Step: 5
Training loss: 1.4074574708938599
Validation loss: 2.3882243235905967

Epoch: 6| Step: 6
Training loss: 0.850475549697876
Validation loss: 2.376555641492208

Epoch: 6| Step: 7
Training loss: 2.340259075164795
Validation loss: 2.3641360799471536

Epoch: 6| Step: 8
Training loss: 2.176499843597412
Validation loss: 2.325748642285665

Epoch: 6| Step: 9
Training loss: 1.8032071590423584
Validation loss: 2.3391215999921164

Epoch: 6| Step: 10
Training loss: 1.4396262168884277
Validation loss: 2.3025774161020913

Epoch: 6| Step: 11
Training loss: 1.7636278867721558
Validation loss: 2.300888995329539

Epoch: 6| Step: 12
Training loss: 1.2149031162261963
Validation loss: 2.324496785799662

Epoch: 6| Step: 13
Training loss: 1.4342501163482666
Validation loss: 2.3340055545171103

Epoch: 358| Step: 0
Training loss: 1.661170482635498
Validation loss: 2.3583385944366455

Epoch: 6| Step: 1
Training loss: 1.9180622100830078
Validation loss: 2.3421375155448914

Epoch: 6| Step: 2
Training loss: 1.8411098718643188
Validation loss: 2.370760440826416

Epoch: 6| Step: 3
Training loss: 1.528287649154663
Validation loss: 2.3403448263804116

Epoch: 6| Step: 4
Training loss: 1.4301073551177979
Validation loss: 2.3629568815231323

Epoch: 6| Step: 5
Training loss: 1.9288008213043213
Validation loss: 2.403029481569926

Epoch: 6| Step: 6
Training loss: 1.7380956411361694
Validation loss: 2.3619101643562317

Epoch: 6| Step: 7
Training loss: 1.2281438112258911
Validation loss: 2.34219761689504

Epoch: 6| Step: 8
Training loss: 1.4856631755828857
Validation loss: 2.3340057929356894

Epoch: 6| Step: 9
Training loss: 1.3990132808685303
Validation loss: 2.340992013613383

Epoch: 6| Step: 10
Training loss: 1.3149546384811401
Validation loss: 2.3231367270151773

Epoch: 6| Step: 11
Training loss: 1.1715960502624512
Validation loss: 2.285867472489675

Epoch: 6| Step: 12
Training loss: 1.404059648513794
Validation loss: 2.3084800044695535

Epoch: 6| Step: 13
Training loss: 1.355668067932129
Validation loss: 2.284952243169149

Epoch: 359| Step: 0
Training loss: 1.8116340637207031
Validation loss: 2.317800541718801

Epoch: 6| Step: 1
Training loss: 1.2943469285964966
Validation loss: 2.36266436179479

Epoch: 6| Step: 2
Training loss: 1.1035664081573486
Validation loss: 2.334853768348694

Epoch: 6| Step: 3
Training loss: 1.5973243713378906
Validation loss: 2.339811861515045

Epoch: 6| Step: 4
Training loss: 1.9211868047714233
Validation loss: 2.369247317314148

Epoch: 6| Step: 5
Training loss: 1.7774208784103394
Validation loss: 2.308408578236898

Epoch: 6| Step: 6
Training loss: 0.8837074041366577
Validation loss: 2.3424120942751565

Epoch: 6| Step: 7
Training loss: 1.764930248260498
Validation loss: 2.31021785736084

Epoch: 6| Step: 8
Training loss: 1.555060625076294
Validation loss: 2.332835853099823

Epoch: 6| Step: 9
Training loss: 2.006758213043213
Validation loss: 2.294236203034719

Epoch: 6| Step: 10
Training loss: 2.0117907524108887
Validation loss: 2.317511240641276

Epoch: 6| Step: 11
Training loss: 1.1750606298446655
Validation loss: 2.330712934335073

Epoch: 6| Step: 12
Training loss: 1.251413106918335
Validation loss: 2.2803348700205484

Epoch: 6| Step: 13
Training loss: 1.46281099319458
Validation loss: 2.3302940924962363

Epoch: 360| Step: 0
Training loss: 1.29904043674469
Validation loss: 2.2949483394622803

Epoch: 6| Step: 1
Training loss: 1.4760522842407227
Validation loss: 2.2782861789067588

Epoch: 6| Step: 2
Training loss: 1.7594460248947144
Validation loss: 2.296876629193624

Epoch: 6| Step: 3
Training loss: 1.4468951225280762
Validation loss: 2.2894049882888794

Epoch: 6| Step: 4
Training loss: 0.8982324004173279
Validation loss: 2.30339777469635

Epoch: 6| Step: 5
Training loss: 1.5592896938323975
Validation loss: 2.331852674484253

Epoch: 6| Step: 6
Training loss: 2.0748162269592285
Validation loss: 2.3402257164319358

Epoch: 6| Step: 7
Training loss: 1.6462070941925049
Validation loss: 2.3804451624552407

Epoch: 6| Step: 8
Training loss: 2.0177390575408936
Validation loss: 2.362721343835195

Epoch: 6| Step: 9
Training loss: 1.7918143272399902
Validation loss: 2.317998210589091

Epoch: 6| Step: 10
Training loss: 1.5146889686584473
Validation loss: 2.3490901589393616

Epoch: 6| Step: 11
Training loss: 1.92173171043396
Validation loss: 2.3454710642496743

Epoch: 6| Step: 12
Training loss: 1.0729799270629883
Validation loss: 2.3044159611066184

Epoch: 6| Step: 13
Training loss: 1.719090223312378
Validation loss: 2.3200767040252686

Epoch: 361| Step: 0
Training loss: 1.4547879695892334
Validation loss: 2.3067031701405845

Epoch: 6| Step: 1
Training loss: 1.3326836824417114
Validation loss: 2.2633046905199685

Epoch: 6| Step: 2
Training loss: 1.4960682392120361
Validation loss: 2.232430557409922

Epoch: 6| Step: 3
Training loss: 2.544557571411133
Validation loss: 2.269783914089203

Epoch: 6| Step: 4
Training loss: 1.325204610824585
Validation loss: 2.272491216659546

Epoch: 6| Step: 5
Training loss: 1.4951300621032715
Validation loss: 2.2702815731366477

Epoch: 6| Step: 6
Training loss: 1.1362372636795044
Validation loss: 2.266083538532257

Epoch: 6| Step: 7
Training loss: 1.3995044231414795
Validation loss: 2.291145980358124

Epoch: 6| Step: 8
Training loss: 2.0567026138305664
Validation loss: 2.28950305779775

Epoch: 6| Step: 9
Training loss: 1.1064481735229492
Validation loss: 2.3000587622324624

Epoch: 6| Step: 10
Training loss: 1.8497250080108643
Validation loss: 2.3350998957951865

Epoch: 6| Step: 11
Training loss: 1.248243808746338
Validation loss: 2.330981194972992

Epoch: 6| Step: 12
Training loss: 2.061579704284668
Validation loss: 2.3266987999280295

Epoch: 6| Step: 13
Training loss: 1.9273910522460938
Validation loss: 2.306699573993683

Epoch: 362| Step: 0
Training loss: 2.2027933597564697
Validation loss: 2.2994802792867026

Epoch: 6| Step: 1
Training loss: 2.2074685096740723
Validation loss: 2.2741026083628335

Epoch: 6| Step: 2
Training loss: 1.4914497137069702
Validation loss: 2.269757330417633

Epoch: 6| Step: 3
Training loss: 2.483447790145874
Validation loss: 2.2493085066477456

Epoch: 6| Step: 4
Training loss: 1.6992329359054565
Validation loss: 2.201347609361013

Epoch: 6| Step: 5
Training loss: 1.428127408027649
Validation loss: 2.2203644116719565

Epoch: 6| Step: 6
Training loss: 1.7742758989334106
Validation loss: 2.1992080410321555

Epoch: 6| Step: 7
Training loss: 1.3374292850494385
Validation loss: 2.1660879254341125

Epoch: 6| Step: 8
Training loss: 1.2200790643692017
Validation loss: 2.1876467068990073

Epoch: 6| Step: 9
Training loss: 0.952630877494812
Validation loss: 2.184922675291697

Epoch: 6| Step: 10
Training loss: 1.0587024688720703
Validation loss: 2.2086564898490906

Epoch: 6| Step: 11
Training loss: 1.6062570810317993
Validation loss: 2.181042273839315

Epoch: 6| Step: 12
Training loss: 1.0246039628982544
Validation loss: 2.186105410257975

Epoch: 6| Step: 13
Training loss: 2.156895160675049
Validation loss: 2.203180968761444

Epoch: 363| Step: 0
Training loss: 1.6063816547393799
Validation loss: 2.209822158018748

Epoch: 6| Step: 1
Training loss: 1.5953514575958252
Validation loss: 2.215628981590271

Epoch: 6| Step: 2
Training loss: 1.3781707286834717
Validation loss: 2.2036374608675637

Epoch: 6| Step: 3
Training loss: 1.9646326303482056
Validation loss: 2.2164546251296997

Epoch: 6| Step: 4
Training loss: 1.2656214237213135
Validation loss: 2.2210081418355307

Epoch: 6| Step: 5
Training loss: 1.212404489517212
Validation loss: 2.226092219352722

Epoch: 6| Step: 6
Training loss: 1.3280493021011353
Validation loss: 2.216739614804586

Epoch: 6| Step: 7
Training loss: 1.2314801216125488
Validation loss: 2.230623443921407

Epoch: 6| Step: 8
Training loss: 1.926508903503418
Validation loss: 2.2170390685399375

Epoch: 6| Step: 9
Training loss: 1.4619736671447754
Validation loss: 2.239235520362854

Epoch: 6| Step: 10
Training loss: 1.1362545490264893
Validation loss: 2.248949567476908

Epoch: 6| Step: 11
Training loss: 2.7709360122680664
Validation loss: 2.2670336961746216

Epoch: 6| Step: 12
Training loss: 1.560105562210083
Validation loss: 2.260085105895996

Epoch: 6| Step: 13
Training loss: 2.057682514190674
Validation loss: 2.306452771027883

Epoch: 364| Step: 0
Training loss: 1.1169109344482422
Validation loss: 2.265723546346029

Epoch: 6| Step: 1
Training loss: 1.516013503074646
Validation loss: 2.242070515950521

Epoch: 6| Step: 2
Training loss: 1.7703666687011719
Validation loss: 2.2518783807754517

Epoch: 6| Step: 3
Training loss: 1.4423792362213135
Validation loss: 2.2385268608729043

Epoch: 6| Step: 4
Training loss: 1.7105931043624878
Validation loss: 2.2607133984565735

Epoch: 6| Step: 5
Training loss: 1.6060500144958496
Validation loss: 2.252688984076182

Epoch: 6| Step: 6
Training loss: 2.1269006729125977
Validation loss: 2.2711590925852456

Epoch: 6| Step: 7
Training loss: 3.377647876739502
Validation loss: 2.2897328535715737

Epoch: 6| Step: 8
Training loss: 1.4604628086090088
Validation loss: 2.257656236489614

Epoch: 6| Step: 9
Training loss: 1.4816925525665283
Validation loss: 2.278431157271067

Epoch: 6| Step: 10
Training loss: 1.33054780960083
Validation loss: 2.261097550392151

Epoch: 6| Step: 11
Training loss: 1.8775815963745117
Validation loss: 2.271696666876475

Epoch: 6| Step: 12
Training loss: 1.6955583095550537
Validation loss: 2.251299341519674

Epoch: 6| Step: 13
Training loss: 1.7345061302185059
Validation loss: 2.24545681476593

Epoch: 365| Step: 0
Training loss: 1.7862393856048584
Validation loss: 2.205185135205587

Epoch: 6| Step: 1
Training loss: 1.6831543445587158
Validation loss: 2.2511435945828757

Epoch: 6| Step: 2
Training loss: 2.039010763168335
Validation loss: 2.25195848941803

Epoch: 6| Step: 3
Training loss: 1.7968719005584717
Validation loss: 2.334500551223755

Epoch: 6| Step: 4
Training loss: 1.6297619342803955
Validation loss: 2.3350037336349487

Epoch: 6| Step: 5
Training loss: 1.171818733215332
Validation loss: 2.3317995270093284

Epoch: 6| Step: 6
Training loss: 1.6399707794189453
Validation loss: 2.26714551448822

Epoch: 6| Step: 7
Training loss: 1.5593562126159668
Validation loss: 2.2374959190686545

Epoch: 6| Step: 8
Training loss: 1.6226736307144165
Validation loss: 2.2843467593193054

Epoch: 6| Step: 9
Training loss: 1.4762762784957886
Validation loss: 2.236400544643402

Epoch: 6| Step: 10
Training loss: 1.6253703832626343
Validation loss: 2.239341457684835

Epoch: 6| Step: 11
Training loss: 2.054856538772583
Validation loss: 2.254981815814972

Epoch: 6| Step: 12
Training loss: 1.9685146808624268
Validation loss: 2.2429118156433105

Epoch: 6| Step: 13
Training loss: 1.5076626539230347
Validation loss: 2.2448810338974

Epoch: 366| Step: 0
Training loss: 1.9656680822372437
Validation loss: 2.229296008745829

Epoch: 6| Step: 1
Training loss: 1.3290644884109497
Validation loss: 2.254709541797638

Epoch: 6| Step: 2
Training loss: 1.226696491241455
Validation loss: 2.2559879620869956

Epoch: 6| Step: 3
Training loss: 1.3374669551849365
Validation loss: 2.300039271513621

Epoch: 6| Step: 4
Training loss: 1.64544677734375
Validation loss: 2.3599566419919333

Epoch: 6| Step: 5
Training loss: 1.3275001049041748
Validation loss: 2.3643778363863626

Epoch: 6| Step: 6
Training loss: 1.5996677875518799
Validation loss: 2.3256142934163413

Epoch: 6| Step: 7
Training loss: 2.254751205444336
Validation loss: 2.354398171106974

Epoch: 6| Step: 8
Training loss: 2.103447437286377
Validation loss: 2.3040060003598533

Epoch: 6| Step: 9
Training loss: 2.747312068939209
Validation loss: 2.35024493932724

Epoch: 6| Step: 10
Training loss: 1.026572585105896
Validation loss: 2.3037004470825195

Epoch: 6| Step: 11
Training loss: 2.2721610069274902
Validation loss: 2.2948997418085733

Epoch: 6| Step: 12
Training loss: 0.9060279130935669
Validation loss: 2.2816932996114097

Epoch: 6| Step: 13
Training loss: 1.7900536060333252
Validation loss: 2.299450715382894

Epoch: 367| Step: 0
Training loss: 1.4263701438903809
Validation loss: 2.286374310652415

Epoch: 6| Step: 1
Training loss: 1.357682466506958
Validation loss: 2.303849180539449

Epoch: 6| Step: 2
Training loss: 2.3628487586975098
Validation loss: 2.2970458467801413

Epoch: 6| Step: 3
Training loss: 0.8403151035308838
Validation loss: 2.3324148654937744

Epoch: 6| Step: 4
Training loss: 1.7839651107788086
Validation loss: 2.301805237929026

Epoch: 6| Step: 5
Training loss: 1.7835525274276733
Validation loss: 2.324977080027262

Epoch: 6| Step: 6
Training loss: 1.6789400577545166
Validation loss: 2.334439973036448

Epoch: 6| Step: 7
Training loss: 1.348252773284912
Validation loss: 2.317096988360087

Epoch: 6| Step: 8
Training loss: 1.3305050134658813
Validation loss: 2.3430422147115073

Epoch: 6| Step: 9
Training loss: 1.0468147993087769
Validation loss: 2.2885605891545615

Epoch: 6| Step: 10
Training loss: 1.9600114822387695
Validation loss: 2.261937936147054

Epoch: 6| Step: 11
Training loss: 1.8848130702972412
Validation loss: 2.2877370317777

Epoch: 6| Step: 12
Training loss: 1.3089460134506226
Validation loss: 2.2748087644577026

Epoch: 6| Step: 13
Training loss: 1.6592490673065186
Validation loss: 2.246139923731486

Epoch: 368| Step: 0
Training loss: 1.4052369594573975
Validation loss: 2.2416935563087463

Epoch: 6| Step: 1
Training loss: 1.4436767101287842
Validation loss: 2.232508679231008

Epoch: 6| Step: 2
Training loss: 2.983018159866333
Validation loss: 2.2756007512410483

Epoch: 6| Step: 3
Training loss: 1.5726861953735352
Validation loss: 2.2569500207901

Epoch: 6| Step: 4
Training loss: 1.584058165550232
Validation loss: 2.320669094721476

Epoch: 6| Step: 5
Training loss: 1.10616135597229
Validation loss: 2.2791006565093994

Epoch: 6| Step: 6
Training loss: 1.4468083381652832
Validation loss: 2.3296948671340942

Epoch: 6| Step: 7
Training loss: 0.9180675745010376
Validation loss: 2.304715077082316

Epoch: 6| Step: 8
Training loss: 1.746816873550415
Validation loss: 2.2968122561772666

Epoch: 6| Step: 9
Training loss: 1.0806117057800293
Validation loss: 2.2835024197896323

Epoch: 6| Step: 10
Training loss: 1.7903584241867065
Validation loss: 2.2712684869766235

Epoch: 6| Step: 11
Training loss: 2.297755241394043
Validation loss: 2.2584713300069175

Epoch: 6| Step: 12
Training loss: 1.2300138473510742
Validation loss: 2.2299766341845193

Epoch: 6| Step: 13
Training loss: 0.968262255191803
Validation loss: 2.2563130855560303

Epoch: 369| Step: 0
Training loss: 1.7881979942321777
Validation loss: 2.2348565657933555

Epoch: 6| Step: 1
Training loss: 2.1986374855041504
Validation loss: 2.2424269715944924

Epoch: 6| Step: 2
Training loss: 1.6661300659179688
Validation loss: 2.2040139039357505

Epoch: 6| Step: 3
Training loss: 1.2899960279464722
Validation loss: 2.2463310956954956

Epoch: 6| Step: 4
Training loss: 1.1160542964935303
Validation loss: 2.180261472860972

Epoch: 6| Step: 5
Training loss: 1.997156023979187
Validation loss: 2.1932358344395957

Epoch: 6| Step: 6
Training loss: 2.247506618499756
Validation loss: 2.191216071446737

Epoch: 6| Step: 7
Training loss: 1.1513712406158447
Validation loss: 2.180487076441447

Epoch: 6| Step: 8
Training loss: 1.370144009590149
Validation loss: 2.247151573499044

Epoch: 6| Step: 9
Training loss: 1.264491319656372
Validation loss: 2.2556532621383667

Epoch: 6| Step: 10
Training loss: 1.3093832731246948
Validation loss: 2.279806653658549

Epoch: 6| Step: 11
Training loss: 1.8002275228500366
Validation loss: 2.2746156454086304

Epoch: 6| Step: 12
Training loss: 1.822854995727539
Validation loss: 2.301403284072876

Epoch: 6| Step: 13
Training loss: 1.102080225944519
Validation loss: 2.305555582046509

Epoch: 370| Step: 0
Training loss: 1.7395840883255005
Validation loss: 2.335229992866516

Epoch: 6| Step: 1
Training loss: 2.0224809646606445
Validation loss: 2.3306824763615928

Epoch: 6| Step: 2
Training loss: 1.4175875186920166
Validation loss: 2.3093536297480264

Epoch: 6| Step: 3
Training loss: 1.4963104724884033
Validation loss: 2.2873830000559487

Epoch: 6| Step: 4
Training loss: 1.2824372053146362
Validation loss: 2.301497677961985

Epoch: 6| Step: 5
Training loss: 2.08050537109375
Validation loss: 2.333004673322042

Epoch: 6| Step: 6
Training loss: 1.6846095323562622
Validation loss: 2.346161206563314

Epoch: 6| Step: 7
Training loss: 1.152371883392334
Validation loss: 2.33122452100118

Epoch: 6| Step: 8
Training loss: 1.6075494289398193
Validation loss: 2.3675336837768555

Epoch: 6| Step: 9
Training loss: 1.2668380737304688
Validation loss: 2.3431954383850098

Epoch: 6| Step: 10
Training loss: 1.1773923635482788
Validation loss: 2.3322280645370483

Epoch: 6| Step: 11
Training loss: 1.5007165670394897
Validation loss: 2.3082973957061768

Epoch: 6| Step: 12
Training loss: 1.306361436843872
Validation loss: 2.353752533594767

Epoch: 6| Step: 13
Training loss: 1.6391396522521973
Validation loss: 2.3409621318181357

Epoch: 371| Step: 0
Training loss: 1.1194918155670166
Validation loss: 2.316882828871409

Epoch: 6| Step: 1
Training loss: 2.4531002044677734
Validation loss: 2.2856228351593018

Epoch: 6| Step: 2
Training loss: 1.1326642036437988
Validation loss: 2.3400519688924155

Epoch: 6| Step: 3
Training loss: 1.66677725315094
Validation loss: 2.3292625347773233

Epoch: 6| Step: 4
Training loss: 0.8373560905456543
Validation loss: 2.3552567760149636

Epoch: 6| Step: 5
Training loss: 1.2867817878723145
Validation loss: 2.3430453538894653

Epoch: 6| Step: 6
Training loss: 1.6835591793060303
Validation loss: 2.3599166870117188

Epoch: 6| Step: 7
Training loss: 1.6646769046783447
Validation loss: 2.3237126072247825

Epoch: 6| Step: 8
Training loss: 1.2687907218933105
Validation loss: 2.3302802642186484

Epoch: 6| Step: 9
Training loss: 1.0152802467346191
Validation loss: 2.3082074920336404

Epoch: 6| Step: 10
Training loss: 1.6818562746047974
Validation loss: 2.322601795196533

Epoch: 6| Step: 11
Training loss: 1.7289667129516602
Validation loss: 2.294146498044332

Epoch: 6| Step: 12
Training loss: 1.8495068550109863
Validation loss: 2.29718808333079

Epoch: 6| Step: 13
Training loss: 2.1495914459228516
Validation loss: 2.241153061389923

Epoch: 372| Step: 0
Training loss: 1.9149564504623413
Validation loss: 2.272693693637848

Epoch: 6| Step: 1
Training loss: 1.5516085624694824
Validation loss: 2.295235256354014

Epoch: 6| Step: 2
Training loss: 0.9106528759002686
Validation loss: 2.286336580912272

Epoch: 6| Step: 3
Training loss: 2.0560660362243652
Validation loss: 2.3304751912752786

Epoch: 6| Step: 4
Training loss: 1.1450252532958984
Validation loss: 2.2806316216786704

Epoch: 6| Step: 5
Training loss: 1.6042439937591553
Validation loss: 2.317170818646749

Epoch: 6| Step: 6
Training loss: 1.85259211063385
Validation loss: 2.3168043295542398

Epoch: 6| Step: 7
Training loss: 1.4191384315490723
Validation loss: 2.2928909262021384

Epoch: 6| Step: 8
Training loss: 1.1571691036224365
Validation loss: 2.3211090167363486

Epoch: 6| Step: 9
Training loss: 1.9348589181900024
Validation loss: 2.3446773091952005

Epoch: 6| Step: 10
Training loss: 1.6638293266296387
Validation loss: 2.317430774370829

Epoch: 6| Step: 11
Training loss: 1.83515465259552
Validation loss: 2.2963527043660483

Epoch: 6| Step: 12
Training loss: 1.767378568649292
Validation loss: 2.315486192703247

Epoch: 6| Step: 13
Training loss: 1.4652988910675049
Validation loss: 2.30037385225296

Epoch: 373| Step: 0
Training loss: 1.7150274515151978
Validation loss: 2.301948388417562

Epoch: 6| Step: 1
Training loss: 1.9306716918945312
Validation loss: 2.3034117221832275

Epoch: 6| Step: 2
Training loss: 2.123854875564575
Validation loss: 2.313846309979757

Epoch: 6| Step: 3
Training loss: 1.7423691749572754
Validation loss: 2.325725555419922

Epoch: 6| Step: 4
Training loss: 1.687352180480957
Validation loss: 2.314908583958944

Epoch: 6| Step: 5
Training loss: 1.6917505264282227
Validation loss: 2.336696902910868

Epoch: 6| Step: 6
Training loss: 1.39461350440979
Validation loss: 2.3378268480300903

Epoch: 6| Step: 7
Training loss: 1.07064688205719
Validation loss: 2.314303239186605

Epoch: 6| Step: 8
Training loss: 1.144120454788208
Validation loss: 2.3411226669947305

Epoch: 6| Step: 9
Training loss: 1.6819244623184204
Validation loss: 2.3232556780179343

Epoch: 6| Step: 10
Training loss: 1.0580532550811768
Validation loss: 2.351275165875753

Epoch: 6| Step: 11
Training loss: 2.0558557510375977
Validation loss: 2.3631965716679892

Epoch: 6| Step: 12
Training loss: 1.0232845544815063
Validation loss: 2.3537463347117105

Epoch: 6| Step: 13
Training loss: 0.9470642805099487
Validation loss: 2.356568972269694

Epoch: 374| Step: 0
Training loss: 1.1333136558532715
Validation loss: 2.326801617940267

Epoch: 6| Step: 1
Training loss: 1.4709683656692505
Validation loss: 2.332405666510264

Epoch: 6| Step: 2
Training loss: 1.0254772901535034
Validation loss: 2.3246750036875405

Epoch: 6| Step: 3
Training loss: 2.1452410221099854
Validation loss: 2.30457333723704

Epoch: 6| Step: 4
Training loss: 1.3547120094299316
Validation loss: 2.329093058904012

Epoch: 6| Step: 5
Training loss: 1.2519505023956299
Validation loss: 2.298069794972738

Epoch: 6| Step: 6
Training loss: 1.7587882280349731
Validation loss: 2.3074633876482644

Epoch: 6| Step: 7
Training loss: 1.115820050239563
Validation loss: 2.295312225818634

Epoch: 6| Step: 8
Training loss: 2.190385580062866
Validation loss: 2.309301972389221

Epoch: 6| Step: 9
Training loss: 1.4146257638931274
Validation loss: 2.318288425604502

Epoch: 6| Step: 10
Training loss: 1.7013447284698486
Validation loss: 2.2970133622487388

Epoch: 6| Step: 11
Training loss: 1.684226155281067
Validation loss: 2.270331939061483

Epoch: 6| Step: 12
Training loss: 1.864298939704895
Validation loss: 2.268031656742096

Epoch: 6| Step: 13
Training loss: 1.6097218990325928
Validation loss: 2.254060467084249

Epoch: 375| Step: 0
Training loss: 1.3646490573883057
Validation loss: 2.2646581331888833

Epoch: 6| Step: 1
Training loss: 1.4022029638290405
Validation loss: 2.2839754025141397

Epoch: 6| Step: 2
Training loss: 1.5149623155593872
Validation loss: 2.257360100746155

Epoch: 6| Step: 3
Training loss: 1.4318418502807617
Validation loss: 2.295226732889811

Epoch: 6| Step: 4
Training loss: 1.776875615119934
Validation loss: 2.2883962392807007

Epoch: 6| Step: 5
Training loss: 1.4569405317306519
Validation loss: 2.315266966819763

Epoch: 6| Step: 6
Training loss: 1.3019078969955444
Validation loss: 2.3455111384391785

Epoch: 6| Step: 7
Training loss: 1.3175934553146362
Validation loss: 2.3608611424764

Epoch: 6| Step: 8
Training loss: 1.4673157930374146
Validation loss: 2.338395675023397

Epoch: 6| Step: 9
Training loss: 1.6554440259933472
Validation loss: 2.3341649770736694

Epoch: 6| Step: 10
Training loss: 1.565377950668335
Validation loss: 2.3403120040893555

Epoch: 6| Step: 11
Training loss: 1.1685173511505127
Validation loss: 2.354768435160319

Epoch: 6| Step: 12
Training loss: 2.7558441162109375
Validation loss: 2.3109827836354575

Epoch: 6| Step: 13
Training loss: 1.8178091049194336
Validation loss: 2.330041468143463

Epoch: 376| Step: 0
Training loss: 1.9832053184509277
Validation loss: 2.292157212893168

Epoch: 6| Step: 1
Training loss: 1.735486626625061
Validation loss: 2.2382583618164062

Epoch: 6| Step: 2
Training loss: 1.7213488817214966
Validation loss: 2.214341143767039

Epoch: 6| Step: 3
Training loss: 1.1496331691741943
Validation loss: 2.239679217338562

Epoch: 6| Step: 4
Training loss: 1.0519394874572754
Validation loss: 2.1927687525749207

Epoch: 6| Step: 5
Training loss: 2.464162826538086
Validation loss: 2.1967200438181558

Epoch: 6| Step: 6
Training loss: 1.0260846614837646
Validation loss: 2.189943273862203

Epoch: 6| Step: 7
Training loss: 1.0117028951644897
Validation loss: 2.2089941700299582

Epoch: 6| Step: 8
Training loss: 1.398005485534668
Validation loss: 2.2044658263524375

Epoch: 6| Step: 9
Training loss: 1.5512968301773071
Validation loss: 2.276413937409719

Epoch: 6| Step: 10
Training loss: 1.749358892440796
Validation loss: 2.297743797302246

Epoch: 6| Step: 11
Training loss: 1.9267888069152832
Validation loss: 2.274480144182841

Epoch: 6| Step: 12
Training loss: 1.4969305992126465
Validation loss: 2.281102101008097

Epoch: 6| Step: 13
Training loss: 1.3820185661315918
Validation loss: 2.323969562848409

Epoch: 377| Step: 0
Training loss: 1.3256428241729736
Validation loss: 2.300760507583618

Epoch: 6| Step: 1
Training loss: 1.1104639768600464
Validation loss: 2.323935846487681

Epoch: 6| Step: 2
Training loss: 1.871427059173584
Validation loss: 2.2564387718836465

Epoch: 6| Step: 3
Training loss: 1.8892362117767334
Validation loss: 2.3089003960291543

Epoch: 6| Step: 4
Training loss: 1.7769557237625122
Validation loss: 2.27044016122818

Epoch: 6| Step: 5
Training loss: 1.466636061668396
Validation loss: 2.3006542126337686

Epoch: 6| Step: 6
Training loss: 1.9336220026016235
Validation loss: 2.276131272315979

Epoch: 6| Step: 7
Training loss: 1.7894024848937988
Validation loss: 2.281009833017985

Epoch: 6| Step: 8
Training loss: 1.6289869546890259
Validation loss: 2.2949655254681907

Epoch: 6| Step: 9
Training loss: 1.0846198797225952
Validation loss: 2.3245724042256675

Epoch: 6| Step: 10
Training loss: 1.4740078449249268
Validation loss: 2.3292179505030313

Epoch: 6| Step: 11
Training loss: 1.0325891971588135
Validation loss: 2.3647904793421426

Epoch: 6| Step: 12
Training loss: 2.0090203285217285
Validation loss: 2.3494613567988076

Epoch: 6| Step: 13
Training loss: 1.2675418853759766
Validation loss: 2.338032285372416

Epoch: 378| Step: 0
Training loss: 1.6060359477996826
Validation loss: 2.3357948859532676

Epoch: 6| Step: 1
Training loss: 1.0866516828536987
Validation loss: 2.3152065873146057

Epoch: 6| Step: 2
Training loss: 1.043834924697876
Validation loss: 2.3257968028386435

Epoch: 6| Step: 3
Training loss: 1.8272349834442139
Validation loss: 2.2559156020482383

Epoch: 6| Step: 4
Training loss: 0.8618528246879578
Validation loss: 2.27917750676473

Epoch: 6| Step: 5
Training loss: 1.6881681680679321
Validation loss: 2.242220878601074

Epoch: 6| Step: 6
Training loss: 1.520344614982605
Validation loss: 2.238513429959615

Epoch: 6| Step: 7
Training loss: 2.319586753845215
Validation loss: 2.2485410968462625

Epoch: 6| Step: 8
Training loss: 1.9025951623916626
Validation loss: 2.2494185964266458

Epoch: 6| Step: 9
Training loss: 1.3844342231750488
Validation loss: 2.239477793375651

Epoch: 6| Step: 10
Training loss: 1.763214349746704
Validation loss: 2.2398080627123513

Epoch: 6| Step: 11
Training loss: 1.603851079940796
Validation loss: 2.261228700478872

Epoch: 6| Step: 12
Training loss: 2.3268747329711914
Validation loss: 2.2100961407025657

Epoch: 6| Step: 13
Training loss: 1.9990172386169434
Validation loss: 2.2476335763931274

Epoch: 379| Step: 0
Training loss: 2.185770034790039
Validation loss: 2.2894622087478638

Epoch: 6| Step: 1
Training loss: 1.410149335861206
Validation loss: 2.301131308078766

Epoch: 6| Step: 2
Training loss: 1.4237756729125977
Validation loss: 2.3605779012044272

Epoch: 6| Step: 3
Training loss: 1.6320356130599976
Validation loss: 2.3718758821487427

Epoch: 6| Step: 4
Training loss: 1.5206973552703857
Validation loss: 2.3379244804382324

Epoch: 6| Step: 5
Training loss: 1.9833543300628662
Validation loss: 2.3559571305910745

Epoch: 6| Step: 6
Training loss: 1.3683254718780518
Validation loss: 2.353722890218099

Epoch: 6| Step: 7
Training loss: 2.009307861328125
Validation loss: 2.3599989215532937

Epoch: 6| Step: 8
Training loss: 1.1459051370620728
Validation loss: 2.3913182814915976

Epoch: 6| Step: 9
Training loss: 0.9753822684288025
Validation loss: 2.362005273501078

Epoch: 6| Step: 10
Training loss: 1.1417670249938965
Validation loss: 2.3326187332471213

Epoch: 6| Step: 11
Training loss: 1.6819576025009155
Validation loss: 2.274345278739929

Epoch: 6| Step: 12
Training loss: 1.2871167659759521
Validation loss: 2.257783035437266

Epoch: 6| Step: 13
Training loss: 1.9451323747634888
Validation loss: 2.2648157278696694

Epoch: 380| Step: 0
Training loss: 1.8363975286483765
Validation loss: 2.2853892842928567

Epoch: 6| Step: 1
Training loss: 2.4698569774627686
Validation loss: 2.2905076344807944

Epoch: 6| Step: 2
Training loss: 2.002777576446533
Validation loss: 2.267685890197754

Epoch: 6| Step: 3
Training loss: 1.7373082637786865
Validation loss: 2.2688881357510886

Epoch: 6| Step: 4
Training loss: 1.1592415571212769
Validation loss: 2.2325849533081055

Epoch: 6| Step: 5
Training loss: 2.759889841079712
Validation loss: 2.255206306775411

Epoch: 6| Step: 6
Training loss: 1.466793179512024
Validation loss: 2.2438756624857583

Epoch: 6| Step: 7
Training loss: 0.8700538873672485
Validation loss: 2.262469311555227

Epoch: 6| Step: 8
Training loss: 1.0278918743133545
Validation loss: 2.294094701608022

Epoch: 6| Step: 9
Training loss: 1.2528107166290283
Validation loss: 2.2919400533040366

Epoch: 6| Step: 10
Training loss: 2.237809181213379
Validation loss: 2.306965152422587

Epoch: 6| Step: 11
Training loss: 1.3196134567260742
Validation loss: 2.2936435540517173

Epoch: 6| Step: 12
Training loss: 1.4137063026428223
Validation loss: 2.3079638481140137

Epoch: 6| Step: 13
Training loss: 1.8048720359802246
Validation loss: 2.296005606651306

Epoch: 381| Step: 0
Training loss: 0.9161970019340515
Validation loss: 2.3187397718429565

Epoch: 6| Step: 1
Training loss: 1.352079153060913
Validation loss: 2.3076442082722983

Epoch: 6| Step: 2
Training loss: 1.4718657732009888
Validation loss: 2.310345470905304

Epoch: 6| Step: 3
Training loss: 2.0882315635681152
Validation loss: 2.2986032565434775

Epoch: 6| Step: 4
Training loss: 1.6995360851287842
Validation loss: 2.2979705731074014

Epoch: 6| Step: 5
Training loss: 1.4334673881530762
Validation loss: 2.269420325756073

Epoch: 6| Step: 6
Training loss: 1.2027556896209717
Validation loss: 2.283383846282959

Epoch: 6| Step: 7
Training loss: 1.7139101028442383
Validation loss: 2.304133494695028

Epoch: 6| Step: 8
Training loss: 1.2470178604125977
Validation loss: 2.28550124168396

Epoch: 6| Step: 9
Training loss: 1.25273859500885
Validation loss: 2.2755875984827676

Epoch: 6| Step: 10
Training loss: 1.342606544494629
Validation loss: 2.291650712490082

Epoch: 6| Step: 11
Training loss: 1.3696818351745605
Validation loss: 2.2921257813771567

Epoch: 6| Step: 12
Training loss: 1.9954688549041748
Validation loss: 2.27397084236145

Epoch: 6| Step: 13
Training loss: 1.847576379776001
Validation loss: 2.3143575191497803

Epoch: 382| Step: 0
Training loss: 1.734861135482788
Validation loss: 2.2613665064175925

Epoch: 6| Step: 1
Training loss: 0.8128867149353027
Validation loss: 2.294278840223948

Epoch: 6| Step: 2
Training loss: 1.2374036312103271
Validation loss: 2.264773408571879

Epoch: 6| Step: 3
Training loss: 1.705904483795166
Validation loss: 2.271656254927317

Epoch: 6| Step: 4
Training loss: 2.082977294921875
Validation loss: 2.2644031842549643

Epoch: 6| Step: 5
Training loss: 1.4088140726089478
Validation loss: 2.2640366752942405

Epoch: 6| Step: 6
Training loss: 1.479454517364502
Validation loss: 2.2823694944381714

Epoch: 6| Step: 7
Training loss: 1.729237675666809
Validation loss: 2.2771634658177695

Epoch: 6| Step: 8
Training loss: 1.2807782888412476
Validation loss: 2.319755792617798

Epoch: 6| Step: 9
Training loss: 1.816160798072815
Validation loss: 2.309980829556783

Epoch: 6| Step: 10
Training loss: 1.613562822341919
Validation loss: 2.31100332736969

Epoch: 6| Step: 11
Training loss: 0.9818241596221924
Validation loss: 2.3089245160420737

Epoch: 6| Step: 12
Training loss: 1.0832114219665527
Validation loss: 2.3210124373435974

Epoch: 6| Step: 13
Training loss: 2.1685571670532227
Validation loss: 2.300542692343394

Epoch: 383| Step: 0
Training loss: 1.2205766439437866
Validation loss: 2.2885148922602334

Epoch: 6| Step: 1
Training loss: 1.2133845090866089
Validation loss: 2.2705196738243103

Epoch: 6| Step: 2
Training loss: 1.659477949142456
Validation loss: 2.2568616469701133

Epoch: 6| Step: 3
Training loss: 1.3056330680847168
Validation loss: 2.2766239444414773

Epoch: 6| Step: 4
Training loss: 1.0164694786071777
Validation loss: 2.276575028896332

Epoch: 6| Step: 5
Training loss: 1.069969654083252
Validation loss: 2.280533035596212

Epoch: 6| Step: 6
Training loss: 0.8796833753585815
Validation loss: 2.2878655989964805

Epoch: 6| Step: 7
Training loss: 1.2548942565917969
Validation loss: 2.287695070107778

Epoch: 6| Step: 8
Training loss: 2.0694870948791504
Validation loss: 2.3024153113365173

Epoch: 6| Step: 9
Training loss: 1.8138351440429688
Validation loss: 2.313474655151367

Epoch: 6| Step: 10
Training loss: 2.0172300338745117
Validation loss: 2.3454366525014243

Epoch: 6| Step: 11
Training loss: 1.6481337547302246
Validation loss: 2.317155361175537

Epoch: 6| Step: 12
Training loss: 0.9017753005027771
Validation loss: 2.3016844193140664

Epoch: 6| Step: 13
Training loss: 2.5415685176849365
Validation loss: 2.3262619376182556

Epoch: 384| Step: 0
Training loss: 2.1663429737091064
Validation loss: 2.325366954008738

Epoch: 6| Step: 1
Training loss: 0.7100132703781128
Validation loss: 2.2708484133084617

Epoch: 6| Step: 2
Training loss: 0.833976686000824
Validation loss: 2.3141987323760986

Epoch: 6| Step: 3
Training loss: 1.400934100151062
Validation loss: 2.3259843985239663

Epoch: 6| Step: 4
Training loss: 1.49711275100708
Validation loss: 2.310723344484965

Epoch: 6| Step: 5
Training loss: 1.0665717124938965
Validation loss: 2.315903107325236

Epoch: 6| Step: 6
Training loss: 1.4322106838226318
Validation loss: 2.3035354216893515

Epoch: 6| Step: 7
Training loss: 1.8324553966522217
Validation loss: 2.303025503953298

Epoch: 6| Step: 8
Training loss: 2.164902448654175
Validation loss: 2.3017523288726807

Epoch: 6| Step: 9
Training loss: 1.770782232284546
Validation loss: 2.285157243410746

Epoch: 6| Step: 10
Training loss: 1.4422662258148193
Validation loss: 2.2837785879770913

Epoch: 6| Step: 11
Training loss: 2.01216459274292
Validation loss: 2.298886020978292

Epoch: 6| Step: 12
Training loss: 1.0404338836669922
Validation loss: 2.301438013712565

Epoch: 6| Step: 13
Training loss: 1.3278377056121826
Validation loss: 2.293550670146942

Epoch: 385| Step: 0
Training loss: 1.2445271015167236
Validation loss: 2.284843703111013

Epoch: 6| Step: 1
Training loss: 1.465181827545166
Validation loss: 2.3236429492632547

Epoch: 6| Step: 2
Training loss: 1.7720568180084229
Validation loss: 2.3069365421930947

Epoch: 6| Step: 3
Training loss: 1.4544084072113037
Validation loss: 2.299118677775065

Epoch: 6| Step: 4
Training loss: 1.3318792581558228
Validation loss: 2.3024983008702598

Epoch: 6| Step: 5
Training loss: 1.398417353630066
Validation loss: 2.265180985132853

Epoch: 6| Step: 6
Training loss: 0.8816453218460083
Validation loss: 2.3093464374542236

Epoch: 6| Step: 7
Training loss: 1.2049658298492432
Validation loss: 2.313802202542623

Epoch: 6| Step: 8
Training loss: 1.3282816410064697
Validation loss: 2.302411675453186

Epoch: 6| Step: 9
Training loss: 1.6993331909179688
Validation loss: 2.2802344957987466

Epoch: 6| Step: 10
Training loss: 1.8990752696990967
Validation loss: 2.3075292905171714

Epoch: 6| Step: 11
Training loss: 1.7346645593643188
Validation loss: 2.30703874429067

Epoch: 6| Step: 12
Training loss: 1.708181619644165
Validation loss: 2.260562459627787

Epoch: 6| Step: 13
Training loss: 1.3533235788345337
Validation loss: 2.2755613327026367

Epoch: 386| Step: 0
Training loss: 1.773315668106079
Validation loss: 2.251338084538778

Epoch: 6| Step: 1
Training loss: 1.1612145900726318
Validation loss: 2.2728341817855835

Epoch: 6| Step: 2
Training loss: 1.4450929164886475
Validation loss: 2.2727394104003906

Epoch: 6| Step: 3
Training loss: 2.0298616886138916
Validation loss: 2.263979891935984

Epoch: 6| Step: 4
Training loss: 2.2396469116210938
Validation loss: 2.340275764465332

Epoch: 6| Step: 5
Training loss: 1.3055837154388428
Validation loss: 2.3450125058492026

Epoch: 6| Step: 6
Training loss: 0.9694515466690063
Validation loss: 2.327447990576426

Epoch: 6| Step: 7
Training loss: 1.7283596992492676
Validation loss: 2.3603657285372415

Epoch: 6| Step: 8
Training loss: 0.9863094091415405
Validation loss: 2.308912714322408

Epoch: 6| Step: 9
Training loss: 1.275168538093567
Validation loss: 2.3098042011260986

Epoch: 6| Step: 10
Training loss: 1.098071575164795
Validation loss: 2.3227198123931885

Epoch: 6| Step: 11
Training loss: 2.0676369667053223
Validation loss: 2.278253356615702

Epoch: 6| Step: 12
Training loss: 1.5267915725708008
Validation loss: 2.244361718495687

Epoch: 6| Step: 13
Training loss: 1.841650366783142
Validation loss: 2.2361018657684326

Epoch: 387| Step: 0
Training loss: 1.5546550750732422
Validation loss: 2.218629697958628

Epoch: 6| Step: 1
Training loss: 1.808151125907898
Validation loss: 2.220937271912893

Epoch: 6| Step: 2
Training loss: 1.5027031898498535
Validation loss: 2.2370869318644204

Epoch: 6| Step: 3
Training loss: 1.1386594772338867
Validation loss: 2.201771338780721

Epoch: 6| Step: 4
Training loss: 1.3983948230743408
Validation loss: 2.292780041694641

Epoch: 6| Step: 5
Training loss: 1.7767937183380127
Validation loss: 2.3032987117767334

Epoch: 6| Step: 6
Training loss: 1.129767656326294
Validation loss: 2.2889710863431296

Epoch: 6| Step: 7
Training loss: 1.466564416885376
Validation loss: 2.3324842850367227

Epoch: 6| Step: 8
Training loss: 1.4259443283081055
Validation loss: 2.266206383705139

Epoch: 6| Step: 9
Training loss: 0.8955211043357849
Validation loss: 2.31243505080541

Epoch: 6| Step: 10
Training loss: 1.6680089235305786
Validation loss: 2.2632763187090554

Epoch: 6| Step: 11
Training loss: 2.1800827980041504
Validation loss: 2.300771335760752

Epoch: 6| Step: 12
Training loss: 0.9019646644592285
Validation loss: 2.3318042357762656

Epoch: 6| Step: 13
Training loss: 1.7536897659301758
Validation loss: 2.28356138865153

Epoch: 388| Step: 0
Training loss: 2.4190328121185303
Validation loss: 2.3325706322987876

Epoch: 6| Step: 1
Training loss: 1.1923754215240479
Validation loss: 2.330302675565084

Epoch: 6| Step: 2
Training loss: 1.869858741760254
Validation loss: 2.304754992326101

Epoch: 6| Step: 3
Training loss: 0.9564173221588135
Validation loss: 2.302180608113607

Epoch: 6| Step: 4
Training loss: 1.635298252105713
Validation loss: 2.304629683494568

Epoch: 6| Step: 5
Training loss: 1.532015323638916
Validation loss: 2.267679214477539

Epoch: 6| Step: 6
Training loss: 1.0138612985610962
Validation loss: 2.301919480164846

Epoch: 6| Step: 7
Training loss: 1.4190573692321777
Validation loss: 2.362645904223124

Epoch: 6| Step: 8
Training loss: 1.1843717098236084
Validation loss: 2.3236819307009378

Epoch: 6| Step: 9
Training loss: 1.6298222541809082
Validation loss: 2.306695202986399

Epoch: 6| Step: 10
Training loss: 1.115440011024475
Validation loss: 2.3334239721298218

Epoch: 6| Step: 11
Training loss: 1.8154678344726562
Validation loss: 2.320020834604899

Epoch: 6| Step: 12
Training loss: 1.3462066650390625
Validation loss: 2.3582176168759665

Epoch: 6| Step: 13
Training loss: 1.391503095626831
Validation loss: 2.3326237996419272

Epoch: 389| Step: 0
Training loss: 0.794833779335022
Validation loss: 2.3011597792307534

Epoch: 6| Step: 1
Training loss: 0.9675455689430237
Validation loss: 2.304652452468872

Epoch: 6| Step: 2
Training loss: 1.491898775100708
Validation loss: 2.308506429195404

Epoch: 6| Step: 3
Training loss: 1.3806064128875732
Validation loss: 2.2827580769856772

Epoch: 6| Step: 4
Training loss: 1.680549144744873
Validation loss: 2.26513264576594

Epoch: 6| Step: 5
Training loss: 1.5539861917495728
Validation loss: 2.2425551414489746

Epoch: 6| Step: 6
Training loss: 1.1868517398834229
Validation loss: 2.2325082818667092

Epoch: 6| Step: 7
Training loss: 2.1491501331329346
Validation loss: 2.244201382001241

Epoch: 6| Step: 8
Training loss: 1.3977380990982056
Validation loss: 2.2159082293510437

Epoch: 6| Step: 9
Training loss: 1.5579485893249512
Validation loss: 2.239339311917623

Epoch: 6| Step: 10
Training loss: 1.221632719039917
Validation loss: 2.21850194533666

Epoch: 6| Step: 11
Training loss: 2.3617894649505615
Validation loss: 2.2701464891433716

Epoch: 6| Step: 12
Training loss: 1.8759888410568237
Validation loss: 2.326844851175944

Epoch: 6| Step: 13
Training loss: 1.4730573892593384
Validation loss: 2.333643078804016

Epoch: 390| Step: 0
Training loss: 2.034533739089966
Validation loss: 2.36032764116923

Epoch: 6| Step: 1
Training loss: 0.9140106439590454
Validation loss: 2.354951818784078

Epoch: 6| Step: 2
Training loss: 0.9009290337562561
Validation loss: 2.349489430586497

Epoch: 6| Step: 3
Training loss: 1.384734034538269
Validation loss: 2.3778288761774697

Epoch: 6| Step: 4
Training loss: 1.4186911582946777
Validation loss: 2.3617811799049377

Epoch: 6| Step: 5
Training loss: 1.5094506740570068
Validation loss: 2.3563797076543174

Epoch: 6| Step: 6
Training loss: 1.2285513877868652
Validation loss: 2.3120355208714805

Epoch: 6| Step: 7
Training loss: 1.811539649963379
Validation loss: 2.2898346185684204

Epoch: 6| Step: 8
Training loss: 1.5269484519958496
Validation loss: 2.2609347701072693

Epoch: 6| Step: 9
Training loss: 1.9562971591949463
Validation loss: 2.2427948911984763

Epoch: 6| Step: 10
Training loss: 1.2711451053619385
Validation loss: 2.2493196725845337

Epoch: 6| Step: 11
Training loss: 2.2998886108398438
Validation loss: 2.222952047983805

Epoch: 6| Step: 12
Training loss: 1.913730263710022
Validation loss: 2.252362012863159

Epoch: 6| Step: 13
Training loss: 1.3504412174224854
Validation loss: 2.2236341635386148

Epoch: 391| Step: 0
Training loss: 1.5024442672729492
Validation loss: 2.2397448221842446

Epoch: 6| Step: 1
Training loss: 1.7867811918258667
Validation loss: 2.224373936653137

Epoch: 6| Step: 2
Training loss: 1.4803531169891357
Validation loss: 2.2244115273157754

Epoch: 6| Step: 3
Training loss: 1.3700515031814575
Validation loss: 2.2861802180608115

Epoch: 6| Step: 4
Training loss: 1.1589275598526
Validation loss: 2.2862385511398315

Epoch: 6| Step: 5
Training loss: 1.1710619926452637
Validation loss: 2.2993086775143943

Epoch: 6| Step: 6
Training loss: 1.039566159248352
Validation loss: 2.317070484161377

Epoch: 6| Step: 7
Training loss: 1.615417242050171
Validation loss: 2.3255796432495117

Epoch: 6| Step: 8
Training loss: 2.101008892059326
Validation loss: 2.3029245138168335

Epoch: 6| Step: 9
Training loss: 1.3214834928512573
Validation loss: 2.2962820331255593

Epoch: 6| Step: 10
Training loss: 1.3486149311065674
Validation loss: 2.308087627092997

Epoch: 6| Step: 11
Training loss: 2.41908860206604
Validation loss: 2.280073960622152

Epoch: 6| Step: 12
Training loss: 0.9852984547615051
Validation loss: 2.2793301939964294

Epoch: 6| Step: 13
Training loss: 1.508151888847351
Validation loss: 2.2834444642066956

Epoch: 392| Step: 0
Training loss: 1.1161744594573975
Validation loss: 2.242566704750061

Epoch: 6| Step: 1
Training loss: 1.2128549814224243
Validation loss: 2.2613603274027505

Epoch: 6| Step: 2
Training loss: 1.4072335958480835
Validation loss: 2.2388209899266562

Epoch: 6| Step: 3
Training loss: 1.7388378381729126
Validation loss: 2.265323579311371

Epoch: 6| Step: 4
Training loss: 1.5606698989868164
Validation loss: 2.2615201274553933

Epoch: 6| Step: 5
Training loss: 1.4160590171813965
Validation loss: 2.2943252126375833

Epoch: 6| Step: 6
Training loss: 1.6787666082382202
Validation loss: 2.2724954088528952

Epoch: 6| Step: 7
Training loss: 1.5659208297729492
Validation loss: 2.2662259538968406

Epoch: 6| Step: 8
Training loss: 1.5383754968643188
Validation loss: 2.210789124170939

Epoch: 6| Step: 9
Training loss: 1.215583086013794
Validation loss: 2.272808313369751

Epoch: 6| Step: 10
Training loss: 1.7753405570983887
Validation loss: 2.240449070930481

Epoch: 6| Step: 11
Training loss: 1.8574678897857666
Validation loss: 2.2744892239570618

Epoch: 6| Step: 12
Training loss: 1.5830409526824951
Validation loss: 2.307121833165487

Epoch: 6| Step: 13
Training loss: 0.9992504715919495
Validation loss: 2.3577653964360556

Epoch: 393| Step: 0
Training loss: 1.2946867942810059
Validation loss: 2.3660899003346763

Epoch: 6| Step: 1
Training loss: 0.9984421730041504
Validation loss: 2.3673198421796164

Epoch: 6| Step: 2
Training loss: 1.1920806169509888
Validation loss: 2.368135690689087

Epoch: 6| Step: 3
Training loss: 1.475804328918457
Validation loss: 2.290188948313395

Epoch: 6| Step: 4
Training loss: 1.4927895069122314
Validation loss: 2.3173943758010864

Epoch: 6| Step: 5
Training loss: 1.6127064228057861
Validation loss: 2.3127148747444153

Epoch: 6| Step: 6
Training loss: 1.6338305473327637
Validation loss: 2.2938708662986755

Epoch: 6| Step: 7
Training loss: 1.0335190296173096
Validation loss: 2.3007517655690513

Epoch: 6| Step: 8
Training loss: 2.2132785320281982
Validation loss: 2.279300630092621

Epoch: 6| Step: 9
Training loss: 1.431396484375
Validation loss: 2.325670917828878

Epoch: 6| Step: 10
Training loss: 1.4064557552337646
Validation loss: 2.2901042302449546

Epoch: 6| Step: 11
Training loss: 2.396892786026001
Validation loss: 2.3378856579462686

Epoch: 6| Step: 12
Training loss: 1.2501771450042725
Validation loss: 2.2842212120691934

Epoch: 6| Step: 13
Training loss: 1.3915351629257202
Validation loss: 2.264917492866516

Epoch: 394| Step: 0
Training loss: 1.4897572994232178
Validation loss: 2.2611630956331887

Epoch: 6| Step: 1
Training loss: 1.5775386095046997
Validation loss: 2.2496801614761353

Epoch: 6| Step: 2
Training loss: 1.2608436346054077
Validation loss: 2.234424650669098

Epoch: 6| Step: 3
Training loss: 1.9649925231933594
Validation loss: 2.2722780307133994

Epoch: 6| Step: 4
Training loss: 0.9484878778457642
Validation loss: 2.2785175244013467

Epoch: 6| Step: 5
Training loss: 1.0768154859542847
Validation loss: 2.2578904827435813

Epoch: 6| Step: 6
Training loss: 1.3789865970611572
Validation loss: 2.2775365908940635

Epoch: 6| Step: 7
Training loss: 0.7181097269058228
Validation loss: 2.2676531076431274

Epoch: 6| Step: 8
Training loss: 1.8655654191970825
Validation loss: 2.306767165660858

Epoch: 6| Step: 9
Training loss: 1.939899206161499
Validation loss: 2.3272348244984946

Epoch: 6| Step: 10
Training loss: 1.6503620147705078
Validation loss: 2.2877374291419983

Epoch: 6| Step: 11
Training loss: 1.3597056865692139
Validation loss: 2.2907546957333884

Epoch: 6| Step: 12
Training loss: 1.3149728775024414
Validation loss: 2.24643482764562

Epoch: 6| Step: 13
Training loss: 1.9149396419525146
Validation loss: 2.2487770716349282

Epoch: 395| Step: 0
Training loss: 1.956733226776123
Validation loss: 2.237196385860443

Epoch: 6| Step: 1
Training loss: 1.380749225616455
Validation loss: 2.259716014067332

Epoch: 6| Step: 2
Training loss: 1.6563767194747925
Validation loss: 2.3124820590019226

Epoch: 6| Step: 3
Training loss: 1.5617353916168213
Validation loss: 2.3012807766596475

Epoch: 6| Step: 4
Training loss: 1.1627906560897827
Validation loss: 2.290671467781067

Epoch: 6| Step: 5
Training loss: 1.5282175540924072
Validation loss: 2.293411135673523

Epoch: 6| Step: 6
Training loss: 1.2678565979003906
Validation loss: 2.2963346441586814

Epoch: 6| Step: 7
Training loss: 1.0844054222106934
Validation loss: 2.2736398379007974

Epoch: 6| Step: 8
Training loss: 1.8729445934295654
Validation loss: 2.3281351725260415

Epoch: 6| Step: 9
Training loss: 1.8624851703643799
Validation loss: 2.2879544496536255

Epoch: 6| Step: 10
Training loss: 1.6823575496673584
Validation loss: 2.2920533816019693

Epoch: 6| Step: 11
Training loss: 0.9741272926330566
Validation loss: 2.278311808904012

Epoch: 6| Step: 12
Training loss: 0.5217810869216919
Validation loss: 2.27022393544515

Epoch: 6| Step: 13
Training loss: 1.5605103969573975
Validation loss: 2.251217544078827

Epoch: 396| Step: 0
Training loss: 1.4006619453430176
Validation loss: 2.2856914599736533

Epoch: 6| Step: 1
Training loss: 0.9600233435630798
Validation loss: 2.28467857837677

Epoch: 6| Step: 2
Training loss: 1.0732048749923706
Validation loss: 2.2738603750864663

Epoch: 6| Step: 3
Training loss: 1.4584333896636963
Validation loss: 2.3054218689600625

Epoch: 6| Step: 4
Training loss: 1.243744969367981
Validation loss: 2.295412262280782

Epoch: 6| Step: 5
Training loss: 1.524005651473999
Validation loss: 2.318287114302317

Epoch: 6| Step: 6
Training loss: 1.3573386669158936
Validation loss: 2.2964139382044473

Epoch: 6| Step: 7
Training loss: 1.5288524627685547
Validation loss: 2.241156737009684

Epoch: 6| Step: 8
Training loss: 1.2479133605957031
Validation loss: 2.232967734336853

Epoch: 6| Step: 9
Training loss: 1.6620287895202637
Validation loss: 2.2285925348599753

Epoch: 6| Step: 10
Training loss: 1.3089359998703003
Validation loss: 2.2205305894215903

Epoch: 6| Step: 11
Training loss: 1.280154824256897
Validation loss: 2.2533366084098816

Epoch: 6| Step: 12
Training loss: 2.1761016845703125
Validation loss: 2.252740740776062

Epoch: 6| Step: 13
Training loss: 1.4703288078308105
Validation loss: 2.229514956474304

Epoch: 397| Step: 0
Training loss: 1.3976149559020996
Validation loss: 2.2112117608388266

Epoch: 6| Step: 1
Training loss: 1.298766851425171
Validation loss: 2.249597132205963

Epoch: 6| Step: 2
Training loss: 0.9973272681236267
Validation loss: 2.2401445508003235

Epoch: 6| Step: 3
Training loss: 1.197414517402649
Validation loss: 2.2257885138193765

Epoch: 6| Step: 4
Training loss: 1.5419248342514038
Validation loss: 2.2487242023150125

Epoch: 6| Step: 5
Training loss: 1.6381397247314453
Validation loss: 2.266030232111613

Epoch: 6| Step: 6
Training loss: 1.475314974784851
Validation loss: 2.2547406752904258

Epoch: 6| Step: 7
Training loss: 1.8105580806732178
Validation loss: 2.2736856738726297

Epoch: 6| Step: 8
Training loss: 1.8895289897918701
Validation loss: 2.280206640561422

Epoch: 6| Step: 9
Training loss: 1.2957711219787598
Validation loss: 2.3262940645217896

Epoch: 6| Step: 10
Training loss: 1.1264270544052124
Validation loss: 2.33513871828715

Epoch: 6| Step: 11
Training loss: 1.8877230882644653
Validation loss: 2.3139704863230386

Epoch: 6| Step: 12
Training loss: 0.7028414011001587
Validation loss: 2.3291055560112

Epoch: 6| Step: 13
Training loss: 1.622089147567749
Validation loss: 2.3321880300839744

Epoch: 398| Step: 0
Training loss: 1.4808517694473267
Validation loss: 2.36189866065979

Epoch: 6| Step: 1
Training loss: 0.6744928359985352
Validation loss: 2.287506659825643

Epoch: 6| Step: 2
Training loss: 1.2569782733917236
Validation loss: 2.303988496462504

Epoch: 6| Step: 3
Training loss: 0.9589720964431763
Validation loss: 2.2751410802205405

Epoch: 6| Step: 4
Training loss: 1.6569024324417114
Validation loss: 2.222708543141683

Epoch: 6| Step: 5
Training loss: 2.3271193504333496
Validation loss: 2.220807512601217

Epoch: 6| Step: 6
Training loss: 2.3322391510009766
Validation loss: 2.2141099174817405

Epoch: 6| Step: 7
Training loss: 1.123935580253601
Validation loss: 2.257578651110331

Epoch: 6| Step: 8
Training loss: 2.7101616859436035
Validation loss: 2.2104185620943704

Epoch: 6| Step: 9
Training loss: 1.5248093605041504
Validation loss: 2.2509613037109375

Epoch: 6| Step: 10
Training loss: 1.5818912982940674
Validation loss: 2.298716425895691

Epoch: 6| Step: 11
Training loss: 1.2487425804138184
Validation loss: 2.2841073473294577

Epoch: 6| Step: 12
Training loss: 0.8363224267959595
Validation loss: 2.3106513619422913

Epoch: 6| Step: 13
Training loss: 1.190396785736084
Validation loss: 2.339927872021993

Epoch: 399| Step: 0
Training loss: 0.9330217838287354
Validation loss: 2.3458386659622192

Epoch: 6| Step: 1
Training loss: 1.7654461860656738
Validation loss: 2.3354849418004355

Epoch: 6| Step: 2
Training loss: 1.3064954280853271
Validation loss: 2.3330875833829245

Epoch: 6| Step: 3
Training loss: 1.5368345975875854
Validation loss: 2.3142141898473105

Epoch: 6| Step: 4
Training loss: 1.7316173315048218
Validation loss: 2.2955825130144754

Epoch: 6| Step: 5
Training loss: 2.400832176208496
Validation loss: 2.269546091556549

Epoch: 6| Step: 6
Training loss: 1.4961810111999512
Validation loss: 2.2829853892326355

Epoch: 6| Step: 7
Training loss: 1.4309000968933105
Validation loss: 2.26458078622818

Epoch: 6| Step: 8
Training loss: 1.1100761890411377
Validation loss: 2.2615065972010293

Epoch: 6| Step: 9
Training loss: 1.3869876861572266
Validation loss: 2.321469227472941

Epoch: 6| Step: 10
Training loss: 0.8630935549736023
Validation loss: 2.3238031466801963

Epoch: 6| Step: 11
Training loss: 1.3939077854156494
Validation loss: 2.297965486844381

Epoch: 6| Step: 12
Training loss: 1.2859470844268799
Validation loss: 2.2910654743512473

Epoch: 6| Step: 13
Training loss: 1.3818755149841309
Validation loss: 2.288164734840393

Epoch: 400| Step: 0
Training loss: 1.4798080921173096
Validation loss: 2.3011628786722818

Epoch: 6| Step: 1
Training loss: 1.4160633087158203
Validation loss: 2.333507478237152

Epoch: 6| Step: 2
Training loss: 1.0843714475631714
Validation loss: 2.2685488859812417

Epoch: 6| Step: 3
Training loss: 1.207516074180603
Validation loss: 2.2655086517333984

Epoch: 6| Step: 4
Training loss: 1.1396342515945435
Validation loss: 2.2593219677607217

Epoch: 6| Step: 5
Training loss: 2.446653127670288
Validation loss: 2.2840617100397744

Epoch: 6| Step: 6
Training loss: 1.642411470413208
Validation loss: 2.278273602326711

Epoch: 6| Step: 7
Training loss: 1.0808067321777344
Validation loss: 2.2261420687039695

Epoch: 6| Step: 8
Training loss: 1.5275787115097046
Validation loss: 2.2515532970428467

Epoch: 6| Step: 9
Training loss: 1.8942499160766602
Validation loss: 2.193619728088379

Epoch: 6| Step: 10
Training loss: 1.5787893533706665
Validation loss: 2.2183868885040283

Epoch: 6| Step: 11
Training loss: 1.0508345365524292
Validation loss: 2.2517053286234536

Epoch: 6| Step: 12
Training loss: 2.068307399749756
Validation loss: 2.2049991687138877

Epoch: 6| Step: 13
Training loss: 1.1229897737503052
Validation loss: 2.2351855436960855

Testing loss: 2.1097070158814355

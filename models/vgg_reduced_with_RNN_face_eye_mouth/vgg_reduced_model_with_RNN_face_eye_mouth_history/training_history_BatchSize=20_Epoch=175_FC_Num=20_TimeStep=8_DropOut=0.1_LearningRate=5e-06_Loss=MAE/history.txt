Epoch: 1| Step: 0
Training loss: 6.007226467132568
Validation loss: 5.293626268704732

Epoch: 5| Step: 1
Training loss: 5.5807600021362305
Validation loss: 5.2922723690668745

Epoch: 5| Step: 2
Training loss: 5.249378204345703
Validation loss: 5.290953477223714

Epoch: 5| Step: 3
Training loss: 5.228093147277832
Validation loss: 5.289608160654704

Epoch: 5| Step: 4
Training loss: 6.427098274230957
Validation loss: 5.288379192352295

Epoch: 5| Step: 5
Training loss: 5.105618476867676
Validation loss: 5.286951303482056

Epoch: 5| Step: 6
Training loss: 5.966001987457275
Validation loss: 5.285558660825093

Epoch: 5| Step: 7
Training loss: 4.448482513427734
Validation loss: 5.284050802389781

Epoch: 5| Step: 8
Training loss: 5.600643157958984
Validation loss: 5.282510439554851

Epoch: 5| Step: 9
Training loss: 4.363974571228027
Validation loss: 5.280980745951335

Epoch: 5| Step: 10
Training loss: 4.907085418701172
Validation loss: 5.279285629590352

Epoch: 5| Step: 11
Training loss: 5.488231658935547
Validation loss: 5.277544438838959

Epoch: 2| Step: 0
Training loss: 4.704756259918213
Validation loss: 5.2758088906606035

Epoch: 5| Step: 1
Training loss: 6.236283779144287
Validation loss: 5.273970107237498

Epoch: 5| Step: 2
Training loss: 4.755251884460449
Validation loss: 5.2720222274462385

Epoch: 5| Step: 3
Training loss: 4.084314823150635
Validation loss: 5.269962926705678

Epoch: 5| Step: 4
Training loss: 5.774460792541504
Validation loss: 5.26788208882014

Epoch: 5| Step: 5
Training loss: 5.357687473297119
Validation loss: 5.2656053105990095

Epoch: 5| Step: 6
Training loss: 5.547367095947266
Validation loss: 5.2633049090703325

Epoch: 5| Step: 7
Training loss: 5.295215129852295
Validation loss: 5.260808527469635

Epoch: 5| Step: 8
Training loss: 5.527514457702637
Validation loss: 5.258215049902598

Epoch: 5| Step: 9
Training loss: 5.6809163093566895
Validation loss: 5.2554335196812945

Epoch: 5| Step: 10
Training loss: 5.620424270629883
Validation loss: 5.252645472685496

Epoch: 5| Step: 11
Training loss: 5.795177459716797
Validation loss: 5.249558766682942

Epoch: 3| Step: 0
Training loss: 4.089535713195801
Validation loss: 5.246405263741811

Epoch: 5| Step: 1
Training loss: 4.509553909301758
Validation loss: 5.242980182170868

Epoch: 5| Step: 2
Training loss: 5.46799898147583
Validation loss: 5.239521602789561

Epoch: 5| Step: 3
Training loss: 5.7576003074646
Validation loss: 5.235816995302836

Epoch: 5| Step: 4
Training loss: 5.4417619705200195
Validation loss: 5.231857081254323

Epoch: 5| Step: 5
Training loss: 5.106413841247559
Validation loss: 5.227736075719197

Epoch: 5| Step: 6
Training loss: 5.51675271987915
Validation loss: 5.223320543766022

Epoch: 5| Step: 7
Training loss: 5.8954691886901855
Validation loss: 5.218612809975942

Epoch: 5| Step: 8
Training loss: 5.289246559143066
Validation loss: 5.21371332804362

Epoch: 5| Step: 9
Training loss: 5.657445907592773
Validation loss: 5.208747526009877

Epoch: 5| Step: 10
Training loss: 5.3851799964904785
Validation loss: 5.203188459078471

Epoch: 5| Step: 11
Training loss: 5.970908164978027
Validation loss: 5.197408298651378

Epoch: 4| Step: 0
Training loss: 6.067665100097656
Validation loss: 5.191485643386841

Epoch: 5| Step: 1
Training loss: 5.450998306274414
Validation loss: 5.185277223587036

Epoch: 5| Step: 2
Training loss: 4.92879056930542
Validation loss: 5.1785767277081804

Epoch: 5| Step: 3
Training loss: 5.733829975128174
Validation loss: 5.171874980131785

Epoch: 5| Step: 4
Training loss: 5.074431419372559
Validation loss: 5.1645094354947405

Epoch: 5| Step: 5
Training loss: 4.5954813957214355
Validation loss: 5.157310465971629

Epoch: 5| Step: 6
Training loss: 5.5127363204956055
Validation loss: 5.149584750334422

Epoch: 5| Step: 7
Training loss: 5.1178059577941895
Validation loss: 5.141421099503835

Epoch: 5| Step: 8
Training loss: 4.623494625091553
Validation loss: 5.132842123508453

Epoch: 5| Step: 9
Training loss: 5.142984867095947
Validation loss: 5.124205032984416

Epoch: 5| Step: 10
Training loss: 5.193631172180176
Validation loss: 5.115220546722412

Epoch: 5| Step: 11
Training loss: 5.45725154876709
Validation loss: 5.105738838513692

Epoch: 5| Step: 0
Training loss: 4.715278148651123
Validation loss: 5.0962286194165545

Epoch: 5| Step: 1
Training loss: 4.539202690124512
Validation loss: 5.086647550264995

Epoch: 5| Step: 2
Training loss: 5.863640785217285
Validation loss: 5.076505243778229

Epoch: 5| Step: 3
Training loss: 6.0375261306762695
Validation loss: 5.066307306289673

Epoch: 5| Step: 4
Training loss: 5.22961950302124
Validation loss: 5.055699169635773

Epoch: 5| Step: 5
Training loss: 5.2369384765625
Validation loss: 5.044862508773804

Epoch: 5| Step: 6
Training loss: 5.13234281539917
Validation loss: 5.0341130296389265

Epoch: 5| Step: 7
Training loss: 5.551276206970215
Validation loss: 5.023157000541687

Epoch: 5| Step: 8
Training loss: 4.292872428894043
Validation loss: 5.011782308419545

Epoch: 5| Step: 9
Training loss: 5.128116607666016
Validation loss: 5.000945210456848

Epoch: 5| Step: 10
Training loss: 4.615663051605225
Validation loss: 4.990131139755249

Epoch: 5| Step: 11
Training loss: 5.005336761474609
Validation loss: 4.978991846243541

Epoch: 6| Step: 0
Training loss: 3.9418320655822754
Validation loss: 4.968241989612579

Epoch: 5| Step: 1
Training loss: 5.048698425292969
Validation loss: 4.957769354184468

Epoch: 5| Step: 2
Training loss: 4.7962822914123535
Validation loss: 4.947356780370076

Epoch: 5| Step: 3
Training loss: 4.190316677093506
Validation loss: 4.937532444794972

Epoch: 5| Step: 4
Training loss: 5.008063316345215
Validation loss: 4.927425702412923

Epoch: 5| Step: 5
Training loss: 5.396145820617676
Validation loss: 4.917623559633891

Epoch: 5| Step: 6
Training loss: 5.692177772521973
Validation loss: 4.908314605553945

Epoch: 5| Step: 7
Training loss: 6.0168986320495605
Validation loss: 4.899083137512207

Epoch: 5| Step: 8
Training loss: 4.152154445648193
Validation loss: 4.890213330586751

Epoch: 5| Step: 9
Training loss: 6.071338653564453
Validation loss: 4.881256679693858

Epoch: 5| Step: 10
Training loss: 4.727592945098877
Validation loss: 4.872886995474498

Epoch: 5| Step: 11
Training loss: 4.911009788513184
Validation loss: 4.864636977513631

Epoch: 7| Step: 0
Training loss: 4.618393898010254
Validation loss: 4.857133289178212

Epoch: 5| Step: 1
Training loss: 5.044365882873535
Validation loss: 4.849332809448242

Epoch: 5| Step: 2
Training loss: 5.078061103820801
Validation loss: 4.8418603738149

Epoch: 5| Step: 3
Training loss: 4.594058990478516
Validation loss: 4.834243913491567

Epoch: 5| Step: 4
Training loss: 4.320318698883057
Validation loss: 4.8268411954243975

Epoch: 5| Step: 5
Training loss: 4.569339752197266
Validation loss: 4.819539705912272

Epoch: 5| Step: 6
Training loss: 4.784248352050781
Validation loss: 4.812222182750702

Epoch: 5| Step: 7
Training loss: 4.9002203941345215
Validation loss: 4.804744044939677

Epoch: 5| Step: 8
Training loss: 4.594165802001953
Validation loss: 4.795949101448059

Epoch: 5| Step: 9
Training loss: 5.39589262008667
Validation loss: 4.788078506787618

Epoch: 5| Step: 10
Training loss: 6.133223533630371
Validation loss: 4.779481490453084

Epoch: 5| Step: 11
Training loss: 4.573703765869141
Validation loss: 4.771922638018926

Epoch: 8| Step: 0
Training loss: 3.913475751876831
Validation loss: 4.76403949658076

Epoch: 5| Step: 1
Training loss: 4.510653018951416
Validation loss: 4.756642599900563

Epoch: 5| Step: 2
Training loss: 4.529977798461914
Validation loss: 4.749893287817637

Epoch: 5| Step: 3
Training loss: 5.798341751098633
Validation loss: 4.742675552765529

Epoch: 5| Step: 4
Training loss: 4.24646520614624
Validation loss: 4.735186676184337

Epoch: 5| Step: 5
Training loss: 5.433585166931152
Validation loss: 4.727663576602936

Epoch: 5| Step: 6
Training loss: 4.290057182312012
Validation loss: 4.7200836936632795

Epoch: 5| Step: 7
Training loss: 5.818167209625244
Validation loss: 4.712729394435883

Epoch: 5| Step: 8
Training loss: 4.649033546447754
Validation loss: 4.705971280733745

Epoch: 5| Step: 9
Training loss: 4.536155700683594
Validation loss: 4.698743770519893

Epoch: 5| Step: 10
Training loss: 5.124122142791748
Validation loss: 4.692109207312266

Epoch: 5| Step: 11
Training loss: 5.612282752990723
Validation loss: 4.684824059406917

Epoch: 9| Step: 0
Training loss: 4.977649688720703
Validation loss: 4.67793944478035

Epoch: 5| Step: 1
Training loss: 5.0395708084106445
Validation loss: 4.669552803039551

Epoch: 5| Step: 2
Training loss: 4.952854156494141
Validation loss: 4.662938316663106

Epoch: 5| Step: 3
Training loss: 5.595884799957275
Validation loss: 4.65562520424525

Epoch: 5| Step: 4
Training loss: 5.021248817443848
Validation loss: 4.648764669895172

Epoch: 5| Step: 5
Training loss: 4.213133335113525
Validation loss: 4.641422192255656

Epoch: 5| Step: 6
Training loss: 4.419028282165527
Validation loss: 4.634520769119263

Epoch: 5| Step: 7
Training loss: 4.704874515533447
Validation loss: 4.628084222475688

Epoch: 5| Step: 8
Training loss: 4.633932590484619
Validation loss: 4.621141791343689

Epoch: 5| Step: 9
Training loss: 4.1330389976501465
Validation loss: 4.615131656328837

Epoch: 5| Step: 10
Training loss: 4.26669979095459
Validation loss: 4.608790914217631

Epoch: 5| Step: 11
Training loss: 5.532625198364258
Validation loss: 4.601941823959351

Epoch: 10| Step: 0
Training loss: 4.167912483215332
Validation loss: 4.595196108023326

Epoch: 5| Step: 1
Training loss: 4.939907073974609
Validation loss: 4.5888111889362335

Epoch: 5| Step: 2
Training loss: 3.851599931716919
Validation loss: 4.581594367822011

Epoch: 5| Step: 3
Training loss: 4.1172661781311035
Validation loss: 4.575758079687755

Epoch: 5| Step: 4
Training loss: 5.656461715698242
Validation loss: 4.568676034609477

Epoch: 5| Step: 5
Training loss: 4.794987678527832
Validation loss: 4.561137715975444

Epoch: 5| Step: 6
Training loss: 4.98721981048584
Validation loss: 4.552936414877574

Epoch: 5| Step: 7
Training loss: 5.296998977661133
Validation loss: 4.543588737646739

Epoch: 5| Step: 8
Training loss: 4.063172817230225
Validation loss: 4.536622583866119

Epoch: 5| Step: 9
Training loss: 4.914031505584717
Validation loss: 4.530889888604482

Epoch: 5| Step: 10
Training loss: 4.7272748947143555
Validation loss: 4.52407893538475

Epoch: 5| Step: 11
Training loss: 3.3102550506591797
Validation loss: 4.518273035685222

Epoch: 11| Step: 0
Training loss: 4.930827617645264
Validation loss: 4.511183609565099

Epoch: 5| Step: 1
Training loss: 4.517180442810059
Validation loss: 4.504621942838033

Epoch: 5| Step: 2
Training loss: 3.4467380046844482
Validation loss: 4.497751325368881

Epoch: 5| Step: 3
Training loss: 4.591227054595947
Validation loss: 4.490914016962051

Epoch: 5| Step: 4
Training loss: 5.135890007019043
Validation loss: 4.483852436145146

Epoch: 5| Step: 5
Training loss: 5.264123439788818
Validation loss: 4.476182858149211

Epoch: 5| Step: 6
Training loss: 4.591920852661133
Validation loss: 4.46962105234464

Epoch: 5| Step: 7
Training loss: 4.224425315856934
Validation loss: 4.4627650280793505

Epoch: 5| Step: 8
Training loss: 4.698190212249756
Validation loss: 4.455422560373942

Epoch: 5| Step: 9
Training loss: 4.0561747550964355
Validation loss: 4.44824477036794

Epoch: 5| Step: 10
Training loss: 5.009578704833984
Validation loss: 4.441282610098521

Epoch: 5| Step: 11
Training loss: 4.352424621582031
Validation loss: 4.433963934580485

Epoch: 12| Step: 0
Training loss: 5.0271735191345215
Validation loss: 4.427415907382965

Epoch: 5| Step: 1
Training loss: 4.645780563354492
Validation loss: 4.420454720656077

Epoch: 5| Step: 2
Training loss: 4.326849460601807
Validation loss: 4.413436313470204

Epoch: 5| Step: 3
Training loss: 4.69260835647583
Validation loss: 4.406995703776677

Epoch: 5| Step: 4
Training loss: 5.178400993347168
Validation loss: 4.40066984295845

Epoch: 5| Step: 5
Training loss: 4.210381507873535
Validation loss: 4.393693109353383

Epoch: 5| Step: 6
Training loss: 4.410081386566162
Validation loss: 4.386333088080089

Epoch: 5| Step: 7
Training loss: 3.5884194374084473
Validation loss: 4.378527830044429

Epoch: 5| Step: 8
Training loss: 3.8884811401367188
Validation loss: 4.372279057900111

Epoch: 5| Step: 9
Training loss: 4.992915153503418
Validation loss: 4.3659265240033465

Epoch: 5| Step: 10
Training loss: 4.859686851501465
Validation loss: 4.3600560029347735

Epoch: 5| Step: 11
Training loss: 3.3420825004577637
Validation loss: 4.352787246306737

Epoch: 13| Step: 0
Training loss: 4.139833927154541
Validation loss: 4.345889349778493

Epoch: 5| Step: 1
Training loss: 4.856481075286865
Validation loss: 4.339902381102244

Epoch: 5| Step: 2
Training loss: 4.535946369171143
Validation loss: 4.33341109752655

Epoch: 5| Step: 3
Training loss: 4.2086639404296875
Validation loss: 4.327175746361415

Epoch: 5| Step: 4
Training loss: 4.170116424560547
Validation loss: 4.319574346144994

Epoch: 5| Step: 5
Training loss: 4.514772415161133
Validation loss: 4.31342476606369

Epoch: 5| Step: 6
Training loss: 4.96632194519043
Validation loss: 4.307357460260391

Epoch: 5| Step: 7
Training loss: 3.81471586227417
Validation loss: 4.301790038744609

Epoch: 5| Step: 8
Training loss: 4.57769250869751
Validation loss: 4.294959217309952

Epoch: 5| Step: 9
Training loss: 4.017165660858154
Validation loss: 4.289097259442012

Epoch: 5| Step: 10
Training loss: 5.000776767730713
Validation loss: 4.283032397429149

Epoch: 5| Step: 11
Training loss: 4.449710369110107
Validation loss: 4.276374777158101

Epoch: 14| Step: 0
Training loss: 4.144549369812012
Validation loss: 4.272234400113423

Epoch: 5| Step: 1
Training loss: 5.094077110290527
Validation loss: 4.268744945526123

Epoch: 5| Step: 2
Training loss: 3.4796714782714844
Validation loss: 4.263446390628815

Epoch: 5| Step: 3
Training loss: 3.977006435394287
Validation loss: 4.257722020149231

Epoch: 5| Step: 4
Training loss: 3.248694658279419
Validation loss: 4.251673738161723

Epoch: 5| Step: 5
Training loss: 4.843029499053955
Validation loss: 4.243834018707275

Epoch: 5| Step: 6
Training loss: 5.225835800170898
Validation loss: 4.2374125719070435

Epoch: 5| Step: 7
Training loss: 4.2917375564575195
Validation loss: 4.231153647104899

Epoch: 5| Step: 8
Training loss: 5.0279717445373535
Validation loss: 4.225480715433757

Epoch: 5| Step: 9
Training loss: 4.384261131286621
Validation loss: 4.219982494910558

Epoch: 5| Step: 10
Training loss: 4.6099958419799805
Validation loss: 4.213190009196599

Epoch: 5| Step: 11
Training loss: 3.112743854522705
Validation loss: 4.206845074892044

Epoch: 15| Step: 0
Training loss: 4.7896294593811035
Validation loss: 4.201027313868205

Epoch: 5| Step: 1
Training loss: 4.527318000793457
Validation loss: 4.193570444981257

Epoch: 5| Step: 2
Training loss: 4.295344352722168
Validation loss: 4.188145150740941

Epoch: 5| Step: 3
Training loss: 3.7127959728240967
Validation loss: 4.181643982728322

Epoch: 5| Step: 4
Training loss: 4.629153251647949
Validation loss: 4.1752383907636

Epoch: 5| Step: 5
Training loss: 4.109775543212891
Validation loss: 4.168659230073293

Epoch: 5| Step: 6
Training loss: 4.376511573791504
Validation loss: 4.162270784378052

Epoch: 5| Step: 7
Training loss: 3.9131970405578613
Validation loss: 4.156553367773692

Epoch: 5| Step: 8
Training loss: 4.348010063171387
Validation loss: 4.151155531406403

Epoch: 5| Step: 9
Training loss: 4.457197189331055
Validation loss: 4.143935839335124

Epoch: 5| Step: 10
Training loss: 3.917902708053589
Validation loss: 4.137464165687561

Epoch: 5| Step: 11
Training loss: 5.576986312866211
Validation loss: 4.13236532608668

Epoch: 16| Step: 0
Training loss: 4.317837238311768
Validation loss: 4.127502590417862

Epoch: 5| Step: 1
Training loss: 4.927659034729004
Validation loss: 4.124521603186925

Epoch: 5| Step: 2
Training loss: 4.608244895935059
Validation loss: 4.117679138978322

Epoch: 5| Step: 3
Training loss: 4.593293190002441
Validation loss: 4.111366430918376

Epoch: 5| Step: 4
Training loss: 4.601761817932129
Validation loss: 4.103815704584122

Epoch: 5| Step: 5
Training loss: 4.260268211364746
Validation loss: 4.098216066757838

Epoch: 5| Step: 6
Training loss: 3.543674945831299
Validation loss: 4.093383620182673

Epoch: 5| Step: 7
Training loss: 4.13531494140625
Validation loss: 4.087405016024907

Epoch: 5| Step: 8
Training loss: 4.604950904846191
Validation loss: 4.081307053565979

Epoch: 5| Step: 9
Training loss: 3.1856932640075684
Validation loss: 4.076387385527293

Epoch: 5| Step: 10
Training loss: 3.9688727855682373
Validation loss: 4.072186271349589

Epoch: 5| Step: 11
Training loss: 3.656958818435669
Validation loss: 4.065755526224772

Epoch: 17| Step: 0
Training loss: 5.427680015563965
Validation loss: 4.060937662919362

Epoch: 5| Step: 1
Training loss: 3.9941229820251465
Validation loss: 4.055224200089772

Epoch: 5| Step: 2
Training loss: 4.207613945007324
Validation loss: 4.0518491466840105

Epoch: 5| Step: 3
Training loss: 4.251835823059082
Validation loss: 4.0469508071740465

Epoch: 5| Step: 4
Training loss: 4.868816375732422
Validation loss: 4.0419800480206804

Epoch: 5| Step: 5
Training loss: 3.204082489013672
Validation loss: 4.037141809860866

Epoch: 5| Step: 6
Training loss: 4.098366737365723
Validation loss: 4.032001415888469

Epoch: 5| Step: 7
Training loss: 3.5318245887756348
Validation loss: 4.026317963997523

Epoch: 5| Step: 8
Training loss: 4.793582916259766
Validation loss: 4.0197873711586

Epoch: 5| Step: 9
Training loss: 3.173107624053955
Validation loss: 4.0146677394707995

Epoch: 5| Step: 10
Training loss: 4.5485968589782715
Validation loss: 4.009955992301305

Epoch: 5| Step: 11
Training loss: 3.502254009246826
Validation loss: 4.005442539850871

Epoch: 18| Step: 0
Training loss: 3.9080028533935547
Validation loss: 3.9995951453844705

Epoch: 5| Step: 1
Training loss: 3.5915331840515137
Validation loss: 3.994138995806376

Epoch: 5| Step: 2
Training loss: 5.237041473388672
Validation loss: 3.989724099636078

Epoch: 5| Step: 3
Training loss: 4.1235504150390625
Validation loss: 3.983277569214503

Epoch: 5| Step: 4
Training loss: 3.517885684967041
Validation loss: 3.9798645873864493

Epoch: 5| Step: 5
Training loss: 4.3940749168396
Validation loss: 3.973635286092758

Epoch: 5| Step: 6
Training loss: 4.5595526695251465
Validation loss: 3.9672735234101615

Epoch: 5| Step: 7
Training loss: 3.5845274925231934
Validation loss: 3.96243953704834

Epoch: 5| Step: 8
Training loss: 3.796292781829834
Validation loss: 3.957873453696569

Epoch: 5| Step: 9
Training loss: 4.690020561218262
Validation loss: 3.9522833128770194

Epoch: 5| Step: 10
Training loss: 4.198094367980957
Validation loss: 3.9476290941238403

Epoch: 5| Step: 11
Training loss: 2.6260619163513184
Validation loss: 3.9419180750846863

Epoch: 19| Step: 0
Training loss: 3.7270634174346924
Validation loss: 3.937847822904587

Epoch: 5| Step: 1
Training loss: 4.061744213104248
Validation loss: 3.932773301998774

Epoch: 5| Step: 2
Training loss: 4.594459056854248
Validation loss: 3.9271788696448007

Epoch: 5| Step: 3
Training loss: 3.369562864303589
Validation loss: 3.9224804639816284

Epoch: 5| Step: 4
Training loss: 4.149588584899902
Validation loss: 3.9168091118335724

Epoch: 5| Step: 5
Training loss: 3.9594619274139404
Validation loss: 3.912718733151754

Epoch: 5| Step: 6
Training loss: 4.935751914978027
Validation loss: 3.9073531230290732

Epoch: 5| Step: 7
Training loss: 3.849679470062256
Validation loss: 3.9025448858737946

Epoch: 5| Step: 8
Training loss: 3.7663562297821045
Validation loss: 3.8975413143634796

Epoch: 5| Step: 9
Training loss: 4.381678581237793
Validation loss: 3.8928951422373452

Epoch: 5| Step: 10
Training loss: 4.0448689460754395
Validation loss: 3.8875584999720254

Epoch: 5| Step: 11
Training loss: 3.0979671478271484
Validation loss: 3.883582184712092

Epoch: 20| Step: 0
Training loss: 4.221912860870361
Validation loss: 3.8784521420796714

Epoch: 5| Step: 1
Training loss: 3.6635773181915283
Validation loss: 3.873239537080129

Epoch: 5| Step: 2
Training loss: 4.50644063949585
Validation loss: 3.8690469761689505

Epoch: 5| Step: 3
Training loss: 3.5450077056884766
Validation loss: 3.8633284171422324

Epoch: 5| Step: 4
Training loss: 3.9477076530456543
Validation loss: 3.8576569656531015

Epoch: 5| Step: 5
Training loss: 4.36623477935791
Validation loss: 3.8526112834612527

Epoch: 5| Step: 6
Training loss: 4.414393424987793
Validation loss: 3.84787580370903

Epoch: 5| Step: 7
Training loss: 3.9654650688171387
Validation loss: 3.8437293767929077

Epoch: 5| Step: 8
Training loss: 3.1986241340637207
Validation loss: 3.8388741314411163

Epoch: 5| Step: 9
Training loss: 3.2438087463378906
Validation loss: 3.8351951837539673

Epoch: 5| Step: 10
Training loss: 4.850592613220215
Validation loss: 3.8300349613030753

Epoch: 5| Step: 11
Training loss: 4.675455093383789
Validation loss: 3.8249653776486716

Epoch: 21| Step: 0
Training loss: 4.241849422454834
Validation loss: 3.8190640012423196

Epoch: 5| Step: 1
Training loss: 3.8577945232391357
Validation loss: 3.81438210606575

Epoch: 5| Step: 2
Training loss: 3.771254062652588
Validation loss: 3.809259057044983

Epoch: 5| Step: 3
Training loss: 3.605004072189331
Validation loss: 3.804171105225881

Epoch: 5| Step: 4
Training loss: 4.58270788192749
Validation loss: 3.8001706103483834

Epoch: 5| Step: 5
Training loss: 4.072458744049072
Validation loss: 3.795242557922999

Epoch: 5| Step: 6
Training loss: 3.5358052253723145
Validation loss: 3.7908312479654946

Epoch: 5| Step: 7
Training loss: 3.90209698677063
Validation loss: 3.7867571214834848

Epoch: 5| Step: 8
Training loss: 4.101963996887207
Validation loss: 3.78117627898852

Epoch: 5| Step: 9
Training loss: 3.6653199195861816
Validation loss: 3.777528087298075

Epoch: 5| Step: 10
Training loss: 4.212815284729004
Validation loss: 3.7725402613480887

Epoch: 5| Step: 11
Training loss: 3.477005958557129
Validation loss: 3.7672036488850913

Epoch: 22| Step: 0
Training loss: 3.653653621673584
Validation loss: 3.7621330420176187

Epoch: 5| Step: 1
Training loss: 4.685269355773926
Validation loss: 3.7585675517717996

Epoch: 5| Step: 2
Training loss: 3.7718403339385986
Validation loss: 3.7542638580004373

Epoch: 5| Step: 3
Training loss: 4.198973178863525
Validation loss: 3.7488835155963898

Epoch: 5| Step: 4
Training loss: 3.7519078254699707
Validation loss: 3.743826409180959

Epoch: 5| Step: 5
Training loss: 3.1071274280548096
Validation loss: 3.738669921954473

Epoch: 5| Step: 6
Training loss: 3.143967628479004
Validation loss: 3.7353100081284842

Epoch: 5| Step: 7
Training loss: 5.026036739349365
Validation loss: 3.7297251224517822

Epoch: 5| Step: 8
Training loss: 3.0695993900299072
Validation loss: 3.724320391813914

Epoch: 5| Step: 9
Training loss: 4.334078311920166
Validation loss: 3.7217766543229422

Epoch: 5| Step: 10
Training loss: 4.110607147216797
Validation loss: 3.7182682951291404

Epoch: 5| Step: 11
Training loss: 3.9631998538970947
Validation loss: 3.715192586183548

Epoch: 23| Step: 0
Training loss: 3.432401180267334
Validation loss: 3.710405727227529

Epoch: 5| Step: 1
Training loss: 3.891171932220459
Validation loss: 3.705365081628164

Epoch: 5| Step: 2
Training loss: 3.9336605072021484
Validation loss: 3.6995751162370047

Epoch: 5| Step: 3
Training loss: 3.737020492553711
Validation loss: 3.6942864855130515

Epoch: 5| Step: 4
Training loss: 3.803630828857422
Validation loss: 3.6895125210285187

Epoch: 5| Step: 5
Training loss: 4.217902183532715
Validation loss: 3.6849947472413382

Epoch: 5| Step: 6
Training loss: 4.54484224319458
Validation loss: 3.6794975101947784

Epoch: 5| Step: 7
Training loss: 3.877159595489502
Validation loss: 3.6727559169133506

Epoch: 5| Step: 8
Training loss: 3.6048362255096436
Validation loss: 3.6682994663715363

Epoch: 5| Step: 9
Training loss: 3.0845537185668945
Validation loss: 3.6624185542265573

Epoch: 5| Step: 10
Training loss: 4.2918009757995605
Validation loss: 3.658878336350123

Epoch: 5| Step: 11
Training loss: 3.1366357803344727
Validation loss: 3.6527344087759652

Epoch: 24| Step: 0
Training loss: 3.911099672317505
Validation loss: 3.6481232345104218

Epoch: 5| Step: 1
Training loss: 4.2118096351623535
Validation loss: 3.6438054740428925

Epoch: 5| Step: 2
Training loss: 2.9137017726898193
Validation loss: 3.638714094956716

Epoch: 5| Step: 3
Training loss: 3.8828742504119873
Validation loss: 3.6341551542282104

Epoch: 5| Step: 4
Training loss: 3.723017930984497
Validation loss: 3.6281351149082184

Epoch: 5| Step: 5
Training loss: 4.43519401550293
Validation loss: 3.6242845356464386

Epoch: 5| Step: 6
Training loss: 3.561213970184326
Validation loss: 3.62148118019104

Epoch: 5| Step: 7
Training loss: 4.358924865722656
Validation loss: 3.6163128515084586

Epoch: 5| Step: 8
Training loss: 3.414233446121216
Validation loss: 3.6105989714463553

Epoch: 5| Step: 9
Training loss: 3.428013563156128
Validation loss: 3.6052993734677634

Epoch: 5| Step: 10
Training loss: 3.914189100265503
Validation loss: 3.6002953549226127

Epoch: 5| Step: 11
Training loss: 3.284801483154297
Validation loss: 3.596988171339035

Epoch: 25| Step: 0
Training loss: 3.6160500049591064
Validation loss: 3.5912635922431946

Epoch: 5| Step: 1
Training loss: 3.3434205055236816
Validation loss: 3.587106386820475

Epoch: 5| Step: 2
Training loss: 4.6789870262146
Validation loss: 3.581913560628891

Epoch: 5| Step: 3
Training loss: 3.8709139823913574
Validation loss: 3.5765991707642875

Epoch: 5| Step: 4
Training loss: 4.265473365783691
Validation loss: 3.5713470379511514

Epoch: 5| Step: 5
Training loss: 3.975114345550537
Validation loss: 3.566046267747879

Epoch: 5| Step: 6
Training loss: 3.8729236125946045
Validation loss: 3.561805764834086

Epoch: 5| Step: 7
Training loss: 3.893007755279541
Validation loss: 3.5562384029229483

Epoch: 5| Step: 8
Training loss: 2.64322566986084
Validation loss: 3.551523139079412

Epoch: 5| Step: 9
Training loss: 3.357455015182495
Validation loss: 3.546940863132477

Epoch: 5| Step: 10
Training loss: 3.492036819458008
Validation loss: 3.542418986558914

Epoch: 5| Step: 11
Training loss: 3.9820878505706787
Validation loss: 3.537376274665197

Epoch: 26| Step: 0
Training loss: 3.6597628593444824
Validation loss: 3.5328897138436637

Epoch: 5| Step: 1
Training loss: 3.804927110671997
Validation loss: 3.5276754101117453

Epoch: 5| Step: 2
Training loss: 2.855818271636963
Validation loss: 3.5244551400343576

Epoch: 5| Step: 3
Training loss: 3.613044023513794
Validation loss: 3.520284205675125

Epoch: 5| Step: 4
Training loss: 2.8073463439941406
Validation loss: 3.516151616970698

Epoch: 5| Step: 5
Training loss: 5.228288173675537
Validation loss: 3.512099494536718

Epoch: 5| Step: 6
Training loss: 4.641975402832031
Validation loss: 3.507678667704264

Epoch: 5| Step: 7
Training loss: 2.9969208240509033
Validation loss: 3.5027164121468863

Epoch: 5| Step: 8
Training loss: 3.6522719860076904
Validation loss: 3.497647762298584

Epoch: 5| Step: 9
Training loss: 3.639676570892334
Validation loss: 3.4940994481245675

Epoch: 5| Step: 10
Training loss: 3.2534332275390625
Validation loss: 3.489539007345835

Epoch: 5| Step: 11
Training loss: 5.009844779968262
Validation loss: 3.4851341545581818

Epoch: 27| Step: 0
Training loss: 4.0698771476745605
Validation loss: 3.4808230300744376

Epoch: 5| Step: 1
Training loss: 3.5777626037597656
Validation loss: 3.4756146570046744

Epoch: 5| Step: 2
Training loss: 3.6845734119415283
Validation loss: 3.471207708120346

Epoch: 5| Step: 3
Training loss: 3.489187240600586
Validation loss: 3.4658599495887756

Epoch: 5| Step: 4
Training loss: 3.803079605102539
Validation loss: 3.4615234633286796

Epoch: 5| Step: 5
Training loss: 3.8323733806610107
Validation loss: 3.456621080636978

Epoch: 5| Step: 6
Training loss: 3.258577346801758
Validation loss: 3.4509557088216147

Epoch: 5| Step: 7
Training loss: 2.9571175575256348
Validation loss: 3.4467537005742392

Epoch: 5| Step: 8
Training loss: 2.987809658050537
Validation loss: 3.441513111193975

Epoch: 5| Step: 9
Training loss: 4.2589497566223145
Validation loss: 3.436910778284073

Epoch: 5| Step: 10
Training loss: 3.409304141998291
Validation loss: 3.4322011172771454

Epoch: 5| Step: 11
Training loss: 6.155365467071533
Validation loss: 3.4275404115517936

Epoch: 28| Step: 0
Training loss: 3.569836378097534
Validation loss: 3.4238157172997794

Epoch: 5| Step: 1
Training loss: 3.0676283836364746
Validation loss: 3.418534825245539

Epoch: 5| Step: 2
Training loss: 4.4239912033081055
Validation loss: 3.4134316941102347

Epoch: 5| Step: 3
Training loss: 3.241628646850586
Validation loss: 3.4086055159568787

Epoch: 5| Step: 4
Training loss: 3.421952486038208
Validation loss: 3.403881867726644

Epoch: 5| Step: 5
Training loss: 3.768784999847412
Validation loss: 3.3995183308919272

Epoch: 5| Step: 6
Training loss: 3.778104066848755
Validation loss: 3.395909051100413

Epoch: 5| Step: 7
Training loss: 3.785353422164917
Validation loss: 3.3899503449598947

Epoch: 5| Step: 8
Training loss: 2.8625969886779785
Validation loss: 3.3843357861042023

Epoch: 5| Step: 9
Training loss: 3.698693037033081
Validation loss: 3.379567116498947

Epoch: 5| Step: 10
Training loss: 3.9860336780548096
Validation loss: 3.3759699761867523

Epoch: 5| Step: 11
Training loss: 1.7189710140228271
Validation loss: 3.371026406685511

Epoch: 29| Step: 0
Training loss: 3.219613552093506
Validation loss: 3.366727372010549

Epoch: 5| Step: 1
Training loss: 3.478381395339966
Validation loss: 3.366571625073751

Epoch: 5| Step: 2
Training loss: 3.594524383544922
Validation loss: 3.3608350257078805

Epoch: 5| Step: 3
Training loss: 2.9283549785614014
Validation loss: 3.3548315664132438

Epoch: 5| Step: 4
Training loss: 3.460484027862549
Validation loss: 3.349551280339559

Epoch: 5| Step: 5
Training loss: 3.1770858764648438
Validation loss: 3.3436955710252128

Epoch: 5| Step: 6
Training loss: 3.6274428367614746
Validation loss: 3.3399776220321655

Epoch: 5| Step: 7
Training loss: 3.5214009284973145
Validation loss: 3.3353342413902283

Epoch: 5| Step: 8
Training loss: 2.77939510345459
Validation loss: 3.330875982840856

Epoch: 5| Step: 9
Training loss: 3.9007301330566406
Validation loss: 3.3280592461427054

Epoch: 5| Step: 10
Training loss: 4.476457118988037
Validation loss: 3.321795403957367

Epoch: 5| Step: 11
Training loss: 6.074394226074219
Validation loss: 3.318558156490326

Epoch: 30| Step: 0
Training loss: 3.187349796295166
Validation loss: 3.312696466843287

Epoch: 5| Step: 1
Training loss: 2.9709410667419434
Validation loss: 3.3090316553910575

Epoch: 5| Step: 2
Training loss: 3.9866714477539062
Validation loss: 3.3044059773286185

Epoch: 5| Step: 3
Training loss: 3.7798895835876465
Validation loss: 3.2998542189598083

Epoch: 5| Step: 4
Training loss: 3.057621717453003
Validation loss: 3.2952127953370414

Epoch: 5| Step: 5
Training loss: 4.271031379699707
Validation loss: 3.2912526230017343

Epoch: 5| Step: 6
Training loss: 3.0367565155029297
Validation loss: 3.2861479818820953

Epoch: 5| Step: 7
Training loss: 3.711308002471924
Validation loss: 3.2834063371022544

Epoch: 5| Step: 8
Training loss: 3.7545173168182373
Validation loss: 3.2775089343388877

Epoch: 5| Step: 9
Training loss: 3.2394232749938965
Validation loss: 3.273142824570338

Epoch: 5| Step: 10
Training loss: 3.0367350578308105
Validation loss: 3.2689578533172607

Epoch: 5| Step: 11
Training loss: 3.9958744049072266
Validation loss: 3.2656320929527283

Epoch: 31| Step: 0
Training loss: 3.457071304321289
Validation loss: 3.260507822036743

Epoch: 5| Step: 1
Training loss: 3.802609920501709
Validation loss: 3.255938450495402

Epoch: 5| Step: 2
Training loss: 3.4758777618408203
Validation loss: 3.2516792515913644

Epoch: 5| Step: 3
Training loss: 3.1598305702209473
Validation loss: 3.2485403219858804

Epoch: 5| Step: 4
Training loss: 2.7171859741210938
Validation loss: 3.244846055905024

Epoch: 5| Step: 5
Training loss: 3.1755471229553223
Validation loss: 3.2410098810990653

Epoch: 5| Step: 6
Training loss: 3.3420276641845703
Validation loss: 3.2365159392356873

Epoch: 5| Step: 7
Training loss: 3.793022871017456
Validation loss: 3.233295520146688

Epoch: 5| Step: 8
Training loss: 3.807124376296997
Validation loss: 3.2283713718255362

Epoch: 5| Step: 9
Training loss: 3.893571376800537
Validation loss: 3.2240212758382163

Epoch: 5| Step: 10
Training loss: 2.997213125228882
Validation loss: 3.2194075087706246

Epoch: 5| Step: 11
Training loss: 3.221625804901123
Validation loss: 3.2163859407107034

Epoch: 32| Step: 0
Training loss: 3.752883195877075
Validation loss: 3.2103545665740967

Epoch: 5| Step: 1
Training loss: 3.0162134170532227
Validation loss: 3.2063067853450775

Epoch: 5| Step: 2
Training loss: 4.402390480041504
Validation loss: 3.20217897494634

Epoch: 5| Step: 3
Training loss: 3.7005698680877686
Validation loss: 3.197761297225952

Epoch: 5| Step: 4
Training loss: 2.8455138206481934
Validation loss: 3.1925258139769235

Epoch: 5| Step: 5
Training loss: 2.83845853805542
Validation loss: 3.1892750362555184

Epoch: 5| Step: 6
Training loss: 3.4943745136260986
Validation loss: 3.1854145427544913

Epoch: 5| Step: 7
Training loss: 3.187262773513794
Validation loss: 3.1806098024050393

Epoch: 5| Step: 8
Training loss: 3.7195301055908203
Validation loss: 3.1770687103271484

Epoch: 5| Step: 9
Training loss: 3.2200214862823486
Validation loss: 3.1731050113836923

Epoch: 5| Step: 10
Training loss: 2.8000106811523438
Validation loss: 3.1672709385553994

Epoch: 5| Step: 11
Training loss: 3.8359127044677734
Validation loss: 3.1631437639395394

Epoch: 33| Step: 0
Training loss: 3.7963833808898926
Validation loss: 3.1593983471393585

Epoch: 5| Step: 1
Training loss: 2.621311902999878
Validation loss: 3.156530956427256

Epoch: 5| Step: 2
Training loss: 3.8375155925750732
Validation loss: 3.1521416703859964

Epoch: 5| Step: 3
Training loss: 3.4496612548828125
Validation loss: 3.1480777064959207

Epoch: 5| Step: 4
Training loss: 3.7613518238067627
Validation loss: 3.1437325179576874

Epoch: 5| Step: 5
Training loss: 3.129290819168091
Validation loss: 3.1393417020638785

Epoch: 5| Step: 6
Training loss: 3.328737735748291
Validation loss: 3.1356899638970694

Epoch: 5| Step: 7
Training loss: 3.16017484664917
Validation loss: 3.1305118103822074

Epoch: 5| Step: 8
Training loss: 3.651644229888916
Validation loss: 3.1270662943522134

Epoch: 5| Step: 9
Training loss: 3.2196593284606934
Validation loss: 3.1225615243117013

Epoch: 5| Step: 10
Training loss: 3.1612071990966797
Validation loss: 3.117485682169596

Epoch: 5| Step: 11
Training loss: 0.7153735756874084
Validation loss: 3.1147461334864297

Epoch: 34| Step: 0
Training loss: 2.9346072673797607
Validation loss: 3.1114978988965354

Epoch: 5| Step: 1
Training loss: 3.4226620197296143
Validation loss: 3.1089249551296234

Epoch: 5| Step: 2
Training loss: 3.046037435531616
Validation loss: 3.1041915019353232

Epoch: 5| Step: 3
Training loss: 3.6185736656188965
Validation loss: 3.1012785335381827

Epoch: 5| Step: 4
Training loss: 3.6536307334899902
Validation loss: 3.097971628109614

Epoch: 5| Step: 5
Training loss: 3.2907681465148926
Validation loss: 3.0944594939549765

Epoch: 5| Step: 6
Training loss: 2.678406238555908
Validation loss: 3.0909051100413003

Epoch: 5| Step: 7
Training loss: 2.932323455810547
Validation loss: 3.0871422290802

Epoch: 5| Step: 8
Training loss: 3.1990294456481934
Validation loss: 3.084212750196457

Epoch: 5| Step: 9
Training loss: 3.341651439666748
Validation loss: 3.0811107456684113

Epoch: 5| Step: 10
Training loss: 3.896728515625
Validation loss: 3.076906551917394

Epoch: 5| Step: 11
Training loss: 3.698930025100708
Validation loss: 3.0742684801419577

Epoch: 35| Step: 0
Training loss: 3.4288718700408936
Validation loss: 3.070945213238398

Epoch: 5| Step: 1
Training loss: 3.6266417503356934
Validation loss: 3.066082934538523

Epoch: 5| Step: 2
Training loss: 3.4646689891815186
Validation loss: 3.062262107928594

Epoch: 5| Step: 3
Training loss: 3.3651976585388184
Validation loss: 3.0587224761644998

Epoch: 5| Step: 4
Training loss: 2.8860421180725098
Validation loss: 3.055443435907364

Epoch: 5| Step: 5
Training loss: 3.5255656242370605
Validation loss: 3.051102101802826

Epoch: 5| Step: 6
Training loss: 2.93703031539917
Validation loss: 3.0468832751115165

Epoch: 5| Step: 7
Training loss: 2.4103972911834717
Validation loss: 3.043179968992869

Epoch: 5| Step: 8
Training loss: 3.8093395233154297
Validation loss: 3.039270341396332

Epoch: 5| Step: 9
Training loss: 2.568382740020752
Validation loss: 3.036229133605957

Epoch: 5| Step: 10
Training loss: 3.6992805004119873
Validation loss: 3.0318274796009064

Epoch: 5| Step: 11
Training loss: 2.8998489379882812
Validation loss: 3.029584934314092

Epoch: 36| Step: 0
Training loss: 3.656062364578247
Validation loss: 3.0248881181081138

Epoch: 5| Step: 1
Training loss: 2.359703302383423
Validation loss: 3.0207357505957284

Epoch: 5| Step: 2
Training loss: 3.0374741554260254
Validation loss: 3.018120219310125

Epoch: 5| Step: 3
Training loss: 3.269570827484131
Validation loss: 3.0141032338142395

Epoch: 5| Step: 4
Training loss: 2.6128952503204346
Validation loss: 3.0117450952529907

Epoch: 5| Step: 5
Training loss: 3.809999465942383
Validation loss: 3.009746899207433

Epoch: 5| Step: 6
Training loss: 3.3378424644470215
Validation loss: 3.0047727723916373

Epoch: 5| Step: 7
Training loss: 3.8062260150909424
Validation loss: 3.002207030852636

Epoch: 5| Step: 8
Training loss: 3.1811282634735107
Validation loss: 2.997978925704956

Epoch: 5| Step: 9
Training loss: 3.223041534423828
Validation loss: 2.9951774378617606

Epoch: 5| Step: 10
Training loss: 2.8208203315734863
Validation loss: 2.9899742801984153

Epoch: 5| Step: 11
Training loss: 3.544004440307617
Validation loss: 2.985985438028971

Epoch: 37| Step: 0
Training loss: 3.3190879821777344
Validation loss: 2.983162820339203

Epoch: 5| Step: 1
Training loss: 3.272751569747925
Validation loss: 2.9799678226312003

Epoch: 5| Step: 2
Training loss: 2.637059450149536
Validation loss: 2.976486086845398

Epoch: 5| Step: 3
Training loss: 3.6016178131103516
Validation loss: 2.972612420717875

Epoch: 5| Step: 4
Training loss: 3.212858200073242
Validation loss: 2.9699354569117227

Epoch: 5| Step: 5
Training loss: 3.1175332069396973
Validation loss: 2.9670440951983132

Epoch: 5| Step: 6
Training loss: 2.999601364135742
Validation loss: 2.9625452955563865

Epoch: 5| Step: 7
Training loss: 3.817600965499878
Validation loss: 2.958559383948644

Epoch: 5| Step: 8
Training loss: 2.8265037536621094
Validation loss: 2.9555536806583405

Epoch: 5| Step: 9
Training loss: 3.126642942428589
Validation loss: 2.9520509839057922

Epoch: 5| Step: 10
Training loss: 2.8883564472198486
Validation loss: 2.949232260386149

Epoch: 5| Step: 11
Training loss: 2.980205535888672
Validation loss: 2.9464498360951743

Epoch: 38| Step: 0
Training loss: 2.885565996170044
Validation loss: 2.9419005215168

Epoch: 5| Step: 1
Training loss: 4.018294334411621
Validation loss: 2.9402249654134116

Epoch: 5| Step: 2
Training loss: 3.8601272106170654
Validation loss: 2.93615914384524

Epoch: 5| Step: 3
Training loss: 2.8507907390594482
Validation loss: 2.930734554926554

Epoch: 5| Step: 4
Training loss: 3.2845985889434814
Validation loss: 2.927268236875534

Epoch: 5| Step: 5
Training loss: 3.3548080921173096
Validation loss: 2.923824906349182

Epoch: 5| Step: 6
Training loss: 3.0957748889923096
Validation loss: 2.9198267261187234

Epoch: 5| Step: 7
Training loss: 3.281935453414917
Validation loss: 2.9175876478354135

Epoch: 5| Step: 8
Training loss: 2.721575975418091
Validation loss: 2.914414872725805

Epoch: 5| Step: 9
Training loss: 3.4384429454803467
Validation loss: 2.9101919134457908

Epoch: 5| Step: 10
Training loss: 2.0256187915802
Validation loss: 2.9066300094127655

Epoch: 5| Step: 11
Training loss: 0.905299961566925
Validation loss: 2.904192308584849

Epoch: 39| Step: 0
Training loss: 3.5053367614746094
Validation loss: 2.900116632382075

Epoch: 5| Step: 1
Training loss: 3.1490561962127686
Validation loss: 2.897209584712982

Epoch: 5| Step: 2
Training loss: 3.6491379737854004
Validation loss: 2.8941944936911264

Epoch: 5| Step: 3
Training loss: 3.043173313140869
Validation loss: 2.8930711249510446

Epoch: 5| Step: 4
Training loss: 2.492675304412842
Validation loss: 2.889057159423828

Epoch: 5| Step: 5
Training loss: 2.8313660621643066
Validation loss: 2.884168267250061

Epoch: 5| Step: 6
Training loss: 2.801473379135132
Validation loss: 2.879978517691294

Epoch: 5| Step: 7
Training loss: 2.8049046993255615
Validation loss: 2.875336915254593

Epoch: 5| Step: 8
Training loss: 3.157165050506592
Validation loss: 2.872613877058029

Epoch: 5| Step: 9
Training loss: 3.390906810760498
Validation loss: 2.8692688196897507

Epoch: 5| Step: 10
Training loss: 3.508246660232544
Validation loss: 2.866434782743454

Epoch: 5| Step: 11
Training loss: 1.2454118728637695
Validation loss: 2.8632054328918457

Epoch: 40| Step: 0
Training loss: 3.5273995399475098
Validation loss: 2.8596016565958657

Epoch: 5| Step: 1
Training loss: 2.9917190074920654
Validation loss: 2.858120242754618

Epoch: 5| Step: 2
Training loss: 3.513733386993408
Validation loss: 2.854750086863836

Epoch: 5| Step: 3
Training loss: 2.946873188018799
Validation loss: 2.8518994947274527

Epoch: 5| Step: 4
Training loss: 3.2872188091278076
Validation loss: 2.8517221013704934

Epoch: 5| Step: 5
Training loss: 3.253173828125
Validation loss: 2.8494050800800323

Epoch: 5| Step: 6
Training loss: 2.9491817951202393
Validation loss: 2.8468876679738364

Epoch: 5| Step: 7
Training loss: 2.5490243434906006
Validation loss: 2.8407275875409446

Epoch: 5| Step: 8
Training loss: 2.4027791023254395
Validation loss: 2.834336290756861

Epoch: 5| Step: 9
Training loss: 3.0496692657470703
Validation loss: 2.831688791513443

Epoch: 5| Step: 10
Training loss: 3.2375731468200684
Validation loss: 2.82914533217748

Epoch: 5| Step: 11
Training loss: 2.4711198806762695
Validation loss: 2.8245839277903237

Epoch: 41| Step: 0
Training loss: 2.5911169052124023
Validation loss: 2.822277079025904

Epoch: 5| Step: 1
Training loss: 2.7816736698150635
Validation loss: 2.8201627930005393

Epoch: 5| Step: 2
Training loss: 3.4443347454071045
Validation loss: 2.8180278638998666

Epoch: 5| Step: 3
Training loss: 2.952883243560791
Validation loss: 2.814061149954796

Epoch: 5| Step: 4
Training loss: 3.5284645557403564
Validation loss: 2.810838371515274

Epoch: 5| Step: 5
Training loss: 2.804576873779297
Validation loss: 2.807793766260147

Epoch: 5| Step: 6
Training loss: 2.872066020965576
Validation loss: 2.803012182315191

Epoch: 5| Step: 7
Training loss: 3.1281137466430664
Validation loss: 2.8011602660020194

Epoch: 5| Step: 8
Training loss: 2.671724796295166
Validation loss: 2.7986570497353873

Epoch: 5| Step: 9
Training loss: 3.0665440559387207
Validation loss: 2.7942108313242593

Epoch: 5| Step: 10
Training loss: 3.46061635017395
Validation loss: 2.7908340394496918

Epoch: 5| Step: 11
Training loss: 2.315061092376709
Validation loss: 2.7912753323713937

Epoch: 42| Step: 0
Training loss: 3.3513312339782715
Validation loss: 2.795493503411611

Epoch: 5| Step: 1
Training loss: 3.4234671592712402
Validation loss: 2.8018769919872284

Epoch: 5| Step: 2
Training loss: 3.311983108520508
Validation loss: 2.798426240682602

Epoch: 5| Step: 3
Training loss: 2.8287479877471924
Validation loss: 2.784263143936793

Epoch: 5| Step: 4
Training loss: 2.534276008605957
Validation loss: 2.775793731212616

Epoch: 5| Step: 5
Training loss: 3.0258114337921143
Validation loss: 2.7732823193073273

Epoch: 5| Step: 6
Training loss: 2.861072540283203
Validation loss: 2.7711375951766968

Epoch: 5| Step: 7
Training loss: 3.106966018676758
Validation loss: 2.7711931069691977

Epoch: 5| Step: 8
Training loss: 2.889089584350586
Validation loss: 2.7683254877726235

Epoch: 5| Step: 9
Training loss: 2.8825740814208984
Validation loss: 2.7659492592016854

Epoch: 5| Step: 10
Training loss: 2.4240689277648926
Validation loss: 2.7617138028144836

Epoch: 5| Step: 11
Training loss: 3.949483871459961
Validation loss: 2.7567829390366874

Epoch: 43| Step: 0
Training loss: 2.5581135749816895
Validation loss: 2.7513338327407837

Epoch: 5| Step: 1
Training loss: 3.5773086547851562
Validation loss: 2.751636952161789

Epoch: 5| Step: 2
Training loss: 2.8048524856567383
Validation loss: 2.747336824735006

Epoch: 5| Step: 3
Training loss: 3.064781427383423
Validation loss: 2.7440304160118103

Epoch: 5| Step: 4
Training loss: 2.613590717315674
Validation loss: 2.7423742214838662

Epoch: 5| Step: 5
Training loss: 2.929079532623291
Validation loss: 2.742806782325109

Epoch: 5| Step: 6
Training loss: 3.19508695602417
Validation loss: 2.74142062664032

Epoch: 5| Step: 7
Training loss: 2.695558547973633
Validation loss: 2.7323628266652427

Epoch: 5| Step: 8
Training loss: 2.5192110538482666
Validation loss: 2.7301561137040458

Epoch: 5| Step: 9
Training loss: 3.3088607788085938
Validation loss: 2.7267134686311087

Epoch: 5| Step: 10
Training loss: 3.2321457862854004
Validation loss: 2.725260555744171

Epoch: 5| Step: 11
Training loss: 2.3029608726501465
Validation loss: 2.722379277149836

Epoch: 44| Step: 0
Training loss: 2.6761183738708496
Validation loss: 2.71850848197937

Epoch: 5| Step: 1
Training loss: 2.3719799518585205
Validation loss: 2.7165631651878357

Epoch: 5| Step: 2
Training loss: 2.7453548908233643
Validation loss: 2.7137160698572793

Epoch: 5| Step: 3
Training loss: 2.468491315841675
Validation loss: 2.708567281564077

Epoch: 5| Step: 4
Training loss: 2.603827953338623
Validation loss: 2.707668354113897

Epoch: 5| Step: 5
Training loss: 3.0653934478759766
Validation loss: 2.705396741628647

Epoch: 5| Step: 6
Training loss: 2.9952211380004883
Validation loss: 2.703957885503769

Epoch: 5| Step: 7
Training loss: 3.6809189319610596
Validation loss: 2.699125607808431

Epoch: 5| Step: 8
Training loss: 3.1570327281951904
Validation loss: 2.6965104043483734

Epoch: 5| Step: 9
Training loss: 2.8333702087402344
Validation loss: 2.692985932032267

Epoch: 5| Step: 10
Training loss: 3.1418404579162598
Validation loss: 2.6892952223618827

Epoch: 5| Step: 11
Training loss: 3.94953989982605
Validation loss: 2.68504532178243

Epoch: 45| Step: 0
Training loss: 3.125540018081665
Validation loss: 2.6828551590442657

Epoch: 5| Step: 1
Training loss: 2.5673301219940186
Validation loss: 2.6907199124495187

Epoch: 5| Step: 2
Training loss: 2.6472511291503906
Validation loss: 2.707355519135793

Epoch: 5| Step: 3
Training loss: 2.712111234664917
Validation loss: 2.689856161673864

Epoch: 5| Step: 4
Training loss: 3.06589412689209
Validation loss: 2.670318861802419

Epoch: 5| Step: 5
Training loss: 2.9614994525909424
Validation loss: 2.6634138425191245

Epoch: 5| Step: 6
Training loss: 2.7391018867492676
Validation loss: 2.6613833705584207

Epoch: 5| Step: 7
Training loss: 2.9751946926116943
Validation loss: 2.661469509204229

Epoch: 5| Step: 8
Training loss: 2.897725820541382
Validation loss: 2.662415931622187

Epoch: 5| Step: 9
Training loss: 3.375314712524414
Validation loss: 2.6553176840146384

Epoch: 5| Step: 10
Training loss: 2.5088582038879395
Validation loss: 2.649733692407608

Epoch: 5| Step: 11
Training loss: 3.157296895980835
Validation loss: 2.648230870564779

Epoch: 46| Step: 0
Training loss: 3.007838010787964
Validation loss: 2.6407616833845773

Epoch: 5| Step: 1
Training loss: 2.727860689163208
Validation loss: 2.6375453074773154

Epoch: 5| Step: 2
Training loss: 2.44285249710083
Validation loss: 2.6399113833904266

Epoch: 5| Step: 3
Training loss: 3.3570263385772705
Validation loss: 2.6433700124422708

Epoch: 5| Step: 4
Training loss: 3.1066017150878906
Validation loss: 2.6384561558564505

Epoch: 5| Step: 5
Training loss: 2.8784310817718506
Validation loss: 2.63152351975441

Epoch: 5| Step: 6
Training loss: 2.8262245655059814
Validation loss: 2.625741163889567

Epoch: 5| Step: 7
Training loss: 2.830812454223633
Validation loss: 2.6213821470737457

Epoch: 5| Step: 8
Training loss: 2.500795602798462
Validation loss: 2.619852989912033

Epoch: 5| Step: 9
Training loss: 3.102055788040161
Validation loss: 2.6232020606597266

Epoch: 5| Step: 10
Training loss: 2.36672043800354
Validation loss: 2.622324933608373

Epoch: 5| Step: 11
Training loss: 2.9359285831451416
Validation loss: 2.6165800293286643

Epoch: 47| Step: 0
Training loss: 2.6267504692077637
Validation loss: 2.608810007572174

Epoch: 5| Step: 1
Training loss: 2.690180540084839
Validation loss: 2.6018079221248627

Epoch: 5| Step: 2
Training loss: 3.0277857780456543
Validation loss: 2.5977689723173776

Epoch: 5| Step: 3
Training loss: 2.3049073219299316
Validation loss: 2.594316611687342

Epoch: 5| Step: 4
Training loss: 3.536764621734619
Validation loss: 2.592511296272278

Epoch: 5| Step: 5
Training loss: 2.240593194961548
Validation loss: 2.590307801961899

Epoch: 5| Step: 6
Training loss: 2.9441022872924805
Validation loss: 2.585999995470047

Epoch: 5| Step: 7
Training loss: 2.9326889514923096
Validation loss: 2.583487495779991

Epoch: 5| Step: 8
Training loss: 2.6371452808380127
Validation loss: 2.5761919816335044

Epoch: 5| Step: 9
Training loss: 2.1487972736358643
Validation loss: 2.5769963363806405

Epoch: 5| Step: 10
Training loss: 3.5120863914489746
Validation loss: 2.5722125271956124

Epoch: 5| Step: 11
Training loss: 3.0889532566070557
Validation loss: 2.569690396388372

Epoch: 48| Step: 0
Training loss: 2.2979490756988525
Validation loss: 2.564093679189682

Epoch: 5| Step: 1
Training loss: 2.888122081756592
Validation loss: 2.5644855946302414

Epoch: 5| Step: 2
Training loss: 2.3722190856933594
Validation loss: 2.5581763088703156

Epoch: 5| Step: 3
Training loss: 2.405735731124878
Validation loss: 2.5545713206132254

Epoch: 5| Step: 4
Training loss: 3.001525402069092
Validation loss: 2.5535972913106284

Epoch: 5| Step: 5
Training loss: 2.6481659412384033
Validation loss: 2.547916824618975

Epoch: 5| Step: 6
Training loss: 2.85835337638855
Validation loss: 2.547058254480362

Epoch: 5| Step: 7
Training loss: 2.977921962738037
Validation loss: 2.5434426367282867

Epoch: 5| Step: 8
Training loss: 2.9817872047424316
Validation loss: 2.5423931380112967

Epoch: 5| Step: 9
Training loss: 3.058845043182373
Validation loss: 2.540412962436676

Epoch: 5| Step: 10
Training loss: 2.5944344997406006
Validation loss: 2.5353802939256034

Epoch: 5| Step: 11
Training loss: 3.470191240310669
Validation loss: 2.536510080099106

Epoch: 49| Step: 0
Training loss: 2.953289270401001
Validation loss: 2.5312737623850503

Epoch: 5| Step: 1
Training loss: 3.145683765411377
Validation loss: 2.5269981622695923

Epoch: 5| Step: 2
Training loss: 3.025639295578003
Validation loss: 2.5233452916145325

Epoch: 5| Step: 3
Training loss: 2.492035388946533
Validation loss: 2.5201160510381064

Epoch: 5| Step: 4
Training loss: 2.7874159812927246
Validation loss: 2.518475279211998

Epoch: 5| Step: 5
Training loss: 2.343780994415283
Validation loss: 2.5133341948191323

Epoch: 5| Step: 6
Training loss: 3.016486406326294
Validation loss: 2.510902722676595

Epoch: 5| Step: 7
Training loss: 2.9799447059631348
Validation loss: 2.508414328098297

Epoch: 5| Step: 8
Training loss: 2.9773812294006348
Validation loss: 2.5072301030158997

Epoch: 5| Step: 9
Training loss: 2.0840179920196533
Validation loss: 2.5026865204175315

Epoch: 5| Step: 10
Training loss: 2.065281391143799
Validation loss: 2.5001246432463327

Epoch: 5| Step: 11
Training loss: 2.421717882156372
Validation loss: 2.4969505866368613

Epoch: 50| Step: 0
Training loss: 2.676319122314453
Validation loss: 2.4928167859713235

Epoch: 5| Step: 1
Training loss: 2.8209333419799805
Validation loss: 2.4935315251350403

Epoch: 5| Step: 2
Training loss: 2.225520610809326
Validation loss: 2.4900395770867667

Epoch: 5| Step: 3
Training loss: 2.402299404144287
Validation loss: 2.487698972225189

Epoch: 5| Step: 4
Training loss: 2.518660068511963
Validation loss: 2.483658413092295

Epoch: 5| Step: 5
Training loss: 2.1201069355010986
Validation loss: 2.4806548058986664

Epoch: 5| Step: 6
Training loss: 2.5379245281219482
Validation loss: 2.4783174792925515

Epoch: 5| Step: 7
Training loss: 3.2641377449035645
Validation loss: 2.471344441175461

Epoch: 5| Step: 8
Training loss: 2.78041410446167
Validation loss: 2.475757052501043

Epoch: 5| Step: 9
Training loss: 3.0585947036743164
Validation loss: 2.467948247989019

Epoch: 5| Step: 10
Training loss: 2.9595203399658203
Validation loss: 2.4670232137044272

Epoch: 5| Step: 11
Training loss: 2.5701510906219482
Validation loss: 2.4635574916998544

Epoch: 51| Step: 0
Training loss: 2.9744791984558105
Validation loss: 2.4626831064621606

Epoch: 5| Step: 1
Training loss: 3.3021187782287598
Validation loss: 2.46089498202006

Epoch: 5| Step: 2
Training loss: 2.6406795978546143
Validation loss: 2.4601743866999946

Epoch: 5| Step: 3
Training loss: 3.1037468910217285
Validation loss: 2.4556632240613303

Epoch: 5| Step: 4
Training loss: 2.2346041202545166
Validation loss: 2.4504935344060264

Epoch: 5| Step: 5
Training loss: 2.4128711223602295
Validation loss: 2.4476308623949685

Epoch: 5| Step: 6
Training loss: 2.782552480697632
Validation loss: 2.4426233768463135

Epoch: 5| Step: 7
Training loss: 2.6025118827819824
Validation loss: 2.43844743569692

Epoch: 5| Step: 8
Training loss: 2.117664337158203
Validation loss: 2.437339802583059

Epoch: 5| Step: 9
Training loss: 2.641119956970215
Validation loss: 2.4353783229986825

Epoch: 5| Step: 10
Training loss: 2.2135260105133057
Validation loss: 2.4341478745142617

Epoch: 5| Step: 11
Training loss: 2.6745896339416504
Validation loss: 2.42990180850029

Epoch: 52| Step: 0
Training loss: 3.6508960723876953
Validation loss: 2.4227579534053802

Epoch: 5| Step: 1
Training loss: 2.6034748554229736
Validation loss: 2.418317178885142

Epoch: 5| Step: 2
Training loss: 1.9831206798553467
Validation loss: 2.41686350107193

Epoch: 5| Step: 3
Training loss: 2.425217866897583
Validation loss: 2.414259751637777

Epoch: 5| Step: 4
Training loss: 2.909477710723877
Validation loss: 2.411661684513092

Epoch: 5| Step: 5
Training loss: 2.437242031097412
Validation loss: 2.413213938474655

Epoch: 5| Step: 6
Training loss: 2.424978733062744
Validation loss: 2.4083931148052216

Epoch: 5| Step: 7
Training loss: 2.4875128269195557
Validation loss: 2.4016748567422233

Epoch: 5| Step: 8
Training loss: 2.1723852157592773
Validation loss: 2.4010979930559793

Epoch: 5| Step: 9
Training loss: 3.2127017974853516
Validation loss: 2.3991053799788156

Epoch: 5| Step: 10
Training loss: 2.1788113117218018
Validation loss: 2.3965376069148383

Epoch: 5| Step: 11
Training loss: 2.9121341705322266
Validation loss: 2.391174614429474

Epoch: 53| Step: 0
Training loss: 2.4406533241271973
Validation loss: 2.3916458984216056

Epoch: 5| Step: 1
Training loss: 2.635442018508911
Validation loss: 2.385043114423752

Epoch: 5| Step: 2
Training loss: 3.050494432449341
Validation loss: 2.386483758687973

Epoch: 5| Step: 3
Training loss: 2.2930545806884766
Validation loss: 2.3804271519184113

Epoch: 5| Step: 4
Training loss: 1.9932146072387695
Validation loss: 2.378009796142578

Epoch: 5| Step: 5
Training loss: 2.7200822830200195
Validation loss: 2.376278594136238

Epoch: 5| Step: 6
Training loss: 2.621323823928833
Validation loss: 2.3736110627651215

Epoch: 5| Step: 7
Training loss: 2.519562244415283
Validation loss: 2.3726825217405954

Epoch: 5| Step: 8
Training loss: 2.2792439460754395
Validation loss: 2.3707591791947684

Epoch: 5| Step: 9
Training loss: 2.952287197113037
Validation loss: 2.368244936068853

Epoch: 5| Step: 10
Training loss: 2.938178777694702
Validation loss: 2.36203004916509

Epoch: 5| Step: 11
Training loss: 0.7625279426574707
Validation loss: 2.3614223351081214

Epoch: 54| Step: 0
Training loss: 2.623783826828003
Validation loss: 2.36026460925738

Epoch: 5| Step: 1
Training loss: 2.2767560482025146
Validation loss: 2.356781452894211

Epoch: 5| Step: 2
Training loss: 3.0045907497406006
Validation loss: 2.3526664574941

Epoch: 5| Step: 3
Training loss: 2.7455742359161377
Validation loss: 2.3530475397904715

Epoch: 5| Step: 4
Training loss: 2.4762377738952637
Validation loss: 2.3536749184131622

Epoch: 5| Step: 5
Training loss: 2.417900800704956
Validation loss: 2.3470648477474847

Epoch: 5| Step: 6
Training loss: 2.336801052093506
Validation loss: 2.3432797690232596

Epoch: 5| Step: 7
Training loss: 2.142251968383789
Validation loss: 2.3405468414227166

Epoch: 5| Step: 8
Training loss: 2.5536441802978516
Validation loss: 2.338613043228785

Epoch: 5| Step: 9
Training loss: 2.3872485160827637
Validation loss: 2.3352248122294745

Epoch: 5| Step: 10
Training loss: 2.7078044414520264
Validation loss: 2.3329417407512665

Epoch: 5| Step: 11
Training loss: 3.0474510192871094
Validation loss: 2.332881381114324

Epoch: 55| Step: 0
Training loss: 2.0452635288238525
Validation loss: 2.3286198526620865

Epoch: 5| Step: 1
Training loss: 3.217097759246826
Validation loss: 2.3305495778719583

Epoch: 5| Step: 2
Training loss: 2.1531805992126465
Validation loss: 2.326476057370504

Epoch: 5| Step: 3
Training loss: 2.417480945587158
Validation loss: 2.326078643401464

Epoch: 5| Step: 4
Training loss: 2.008848190307617
Validation loss: 2.3171556890010834

Epoch: 5| Step: 5
Training loss: 3.345740556716919
Validation loss: 2.3197283943494162

Epoch: 5| Step: 6
Training loss: 2.6606998443603516
Validation loss: 2.313754697640737

Epoch: 5| Step: 7
Training loss: 2.2219653129577637
Validation loss: 2.3152085542678833

Epoch: 5| Step: 8
Training loss: 2.4586181640625
Validation loss: 2.312450016538302

Epoch: 5| Step: 9
Training loss: 2.992096424102783
Validation loss: 2.3055418779452643

Epoch: 5| Step: 10
Training loss: 1.950316071510315
Validation loss: 2.302995433410009

Epoch: 5| Step: 11
Training loss: 1.8381973505020142
Validation loss: 2.30122734606266

Epoch: 56| Step: 0
Training loss: 2.385251760482788
Validation loss: 2.3021730879942575

Epoch: 5| Step: 1
Training loss: 2.3327953815460205
Validation loss: 2.3061401744683585

Epoch: 5| Step: 2
Training loss: 2.9406421184539795
Validation loss: 2.3053149580955505

Epoch: 5| Step: 3
Training loss: 2.4969639778137207
Validation loss: 2.309727350870768

Epoch: 5| Step: 4
Training loss: 2.701709270477295
Validation loss: 2.3127989172935486

Epoch: 5| Step: 5
Training loss: 2.3000237941741943
Validation loss: 2.3023933271567025

Epoch: 5| Step: 6
Training loss: 2.6373836994171143
Validation loss: 2.30244010190169

Epoch: 5| Step: 7
Training loss: 2.805184841156006
Validation loss: 2.2905276964108148

Epoch: 5| Step: 8
Training loss: 1.9188629388809204
Validation loss: 2.2837255895137787

Epoch: 5| Step: 9
Training loss: 2.194669723510742
Validation loss: 2.282817949851354

Epoch: 5| Step: 10
Training loss: 2.350770950317383
Validation loss: 2.2779263804356256

Epoch: 5| Step: 11
Training loss: 2.679135322570801
Validation loss: 2.27676784992218

Epoch: 57| Step: 0
Training loss: 2.309601306915283
Validation loss: 2.277597874403

Epoch: 5| Step: 1
Training loss: 1.8992202281951904
Validation loss: 2.2745598455270133

Epoch: 5| Step: 2
Training loss: 2.928992986679077
Validation loss: 2.2721117039521537

Epoch: 5| Step: 3
Training loss: 2.3951873779296875
Validation loss: 2.2770062734683356

Epoch: 5| Step: 4
Training loss: 2.3216655254364014
Validation loss: 2.2744900286197662

Epoch: 5| Step: 5
Training loss: 2.8806161880493164
Validation loss: 2.268910984198252

Epoch: 5| Step: 6
Training loss: 2.529329776763916
Validation loss: 2.262847681840261

Epoch: 5| Step: 7
Training loss: 2.183370590209961
Validation loss: 2.2554640571276345

Epoch: 5| Step: 8
Training loss: 2.310945987701416
Validation loss: 2.248972326517105

Epoch: 5| Step: 9
Training loss: 2.9239466190338135
Validation loss: 2.251904477675756

Epoch: 5| Step: 10
Training loss: 2.25864577293396
Validation loss: 2.254340241352717

Epoch: 5| Step: 11
Training loss: 1.3617476224899292
Validation loss: 2.248573655883471

Epoch: 58| Step: 0
Training loss: 2.5642261505126953
Validation loss: 2.2484417408704758

Epoch: 5| Step: 1
Training loss: 2.1813814640045166
Validation loss: 2.248559832572937

Epoch: 5| Step: 2
Training loss: 2.7244980335235596
Validation loss: 2.248173971970876

Epoch: 5| Step: 3
Training loss: 2.2265336513519287
Validation loss: 2.246723473072052

Epoch: 5| Step: 4
Training loss: 2.2368369102478027
Validation loss: 2.2467719862858453

Epoch: 5| Step: 5
Training loss: 1.928910493850708
Validation loss: 2.2434189915657043

Epoch: 5| Step: 6
Training loss: 2.107792377471924
Validation loss: 2.2445140282313027

Epoch: 5| Step: 7
Training loss: 2.747932195663452
Validation loss: 2.237825353940328

Epoch: 5| Step: 8
Training loss: 2.8689026832580566
Validation loss: 2.235654224952062

Epoch: 5| Step: 9
Training loss: 2.6119675636291504
Validation loss: 2.2280535300572715

Epoch: 5| Step: 10
Training loss: 2.1790730953216553
Validation loss: 2.22747611006101

Epoch: 5| Step: 11
Training loss: 2.5953402519226074
Validation loss: 2.222778727610906

Epoch: 59| Step: 0
Training loss: 2.5128753185272217
Validation loss: 2.2215148508548737

Epoch: 5| Step: 1
Training loss: 2.3661048412323
Validation loss: 2.215712239344915

Epoch: 5| Step: 2
Training loss: 1.7988624572753906
Validation loss: 2.2144114077091217

Epoch: 5| Step: 3
Training loss: 2.304762125015259
Validation loss: 2.212064196666082

Epoch: 5| Step: 4
Training loss: 2.2350993156433105
Validation loss: 2.2071735908587775

Epoch: 5| Step: 5
Training loss: 2.316523313522339
Validation loss: 2.2126786609490714

Epoch: 5| Step: 6
Training loss: 2.259746551513672
Validation loss: 2.220407118399938

Epoch: 5| Step: 7
Training loss: 3.1711554527282715
Validation loss: 2.2117478052775064

Epoch: 5| Step: 8
Training loss: 1.771227478981018
Validation loss: 2.201987624168396

Epoch: 5| Step: 9
Training loss: 2.752882242202759
Validation loss: 2.2026874125003815

Epoch: 5| Step: 10
Training loss: 2.5043065547943115
Validation loss: 2.2010001242160797

Epoch: 5| Step: 11
Training loss: 2.4751460552215576
Validation loss: 2.1954577565193176

Epoch: 60| Step: 0
Training loss: 2.5613951683044434
Validation loss: 2.1998028059800467

Epoch: 5| Step: 1
Training loss: 2.325700283050537
Validation loss: 2.1955191691716514

Epoch: 5| Step: 2
Training loss: 2.1538121700286865
Validation loss: 2.196514089902242

Epoch: 5| Step: 3
Training loss: 2.014827251434326
Validation loss: 2.195586989323298

Epoch: 5| Step: 4
Training loss: 2.6214709281921387
Validation loss: 2.1929146299759545

Epoch: 5| Step: 5
Training loss: 2.5362296104431152
Validation loss: 2.1939565390348434

Epoch: 5| Step: 6
Training loss: 2.235715389251709
Validation loss: 2.1865646640459695

Epoch: 5| Step: 7
Training loss: 2.014705181121826
Validation loss: 2.1862247784932456

Epoch: 5| Step: 8
Training loss: 2.342910051345825
Validation loss: 2.1857814391454062

Epoch: 5| Step: 9
Training loss: 2.3122990131378174
Validation loss: 2.1812768429517746

Epoch: 5| Step: 10
Training loss: 2.394373655319214
Validation loss: 2.1800465832153955

Epoch: 5| Step: 11
Training loss: 3.651306629180908
Validation loss: 2.1756987273693085

Epoch: 61| Step: 0
Training loss: 2.5302841663360596
Validation loss: 2.177958980202675

Epoch: 5| Step: 1
Training loss: 2.083742618560791
Validation loss: 2.1782171179850898

Epoch: 5| Step: 2
Training loss: 2.262322425842285
Validation loss: 2.1782409697771072

Epoch: 5| Step: 3
Training loss: 2.6644675731658936
Validation loss: 2.1708867251873016

Epoch: 5| Step: 4
Training loss: 2.1504971981048584
Validation loss: 2.1696184078852334

Epoch: 5| Step: 5
Training loss: 2.5388522148132324
Validation loss: 2.168462043007215

Epoch: 5| Step: 6
Training loss: 1.9532816410064697
Validation loss: 2.1684986551602683

Epoch: 5| Step: 7
Training loss: 2.047872543334961
Validation loss: 2.1635592778523765

Epoch: 5| Step: 8
Training loss: 2.5545058250427246
Validation loss: 2.1628915766874948

Epoch: 5| Step: 9
Training loss: 2.6329798698425293
Validation loss: 2.1617398262023926

Epoch: 5| Step: 10
Training loss: 2.2847952842712402
Validation loss: 2.1561791549126306

Epoch: 5| Step: 11
Training loss: 1.5331114530563354
Validation loss: 2.1549283613761268

Epoch: 62| Step: 0
Training loss: 2.607200860977173
Validation loss: 2.1542459477980933

Epoch: 5| Step: 1
Training loss: 2.2295851707458496
Validation loss: 2.152237961689631

Epoch: 5| Step: 2
Training loss: 2.3615665435791016
Validation loss: 2.157451808452606

Epoch: 5| Step: 3
Training loss: 1.7374712228775024
Validation loss: 2.147854303320249

Epoch: 5| Step: 4
Training loss: 2.0208914279937744
Validation loss: 2.1473235885302224

Epoch: 5| Step: 5
Training loss: 1.9895431995391846
Validation loss: 2.150162344177564

Epoch: 5| Step: 6
Training loss: 2.0710315704345703
Validation loss: 2.1425305704275766

Epoch: 5| Step: 7
Training loss: 3.039198637008667
Validation loss: 2.1485699117183685

Epoch: 5| Step: 8
Training loss: 2.5558371543884277
Validation loss: 2.1461443503697715

Epoch: 5| Step: 9
Training loss: 2.5829291343688965
Validation loss: 2.1489532589912415

Epoch: 5| Step: 10
Training loss: 2.004060983657837
Validation loss: 2.1502005606889725

Epoch: 5| Step: 11
Training loss: 3.3631749153137207
Validation loss: 2.1465648859739304

Epoch: 63| Step: 0
Training loss: 2.1742453575134277
Validation loss: 2.142922878265381

Epoch: 5| Step: 1
Training loss: 3.013796329498291
Validation loss: 2.141690502564112

Epoch: 5| Step: 2
Training loss: 2.3754851818084717
Validation loss: 2.1425673266251883

Epoch: 5| Step: 3
Training loss: 2.184715747833252
Validation loss: 2.133094290892283

Epoch: 5| Step: 4
Training loss: 2.1221094131469727
Validation loss: 2.134888937075933

Epoch: 5| Step: 5
Training loss: 2.4650321006774902
Validation loss: 2.130938077966372

Epoch: 5| Step: 6
Training loss: 2.2590627670288086
Validation loss: 2.132370521624883

Epoch: 5| Step: 7
Training loss: 2.330524206161499
Validation loss: 2.1301484604676566

Epoch: 5| Step: 8
Training loss: 1.8918788433074951
Validation loss: 2.12710894147555

Epoch: 5| Step: 9
Training loss: 2.217106342315674
Validation loss: 2.1242602417866387

Epoch: 5| Step: 10
Training loss: 2.2427186965942383
Validation loss: 2.1265356292327247

Epoch: 5| Step: 11
Training loss: 2.058187484741211
Validation loss: 2.1272088636954627

Epoch: 64| Step: 0
Training loss: 2.428272008895874
Validation loss: 2.1218872666358948

Epoch: 5| Step: 1
Training loss: 2.1391704082489014
Validation loss: 2.1228480488061905

Epoch: 5| Step: 2
Training loss: 2.5631699562072754
Validation loss: 2.1213785310586295

Epoch: 5| Step: 3
Training loss: 2.270253896713257
Validation loss: 2.1258518993854523

Epoch: 5| Step: 4
Training loss: 2.502082347869873
Validation loss: 2.129585420091947

Epoch: 5| Step: 5
Training loss: 1.6245616674423218
Validation loss: 2.1317631850639978

Epoch: 5| Step: 6
Training loss: 2.4923720359802246
Validation loss: 2.1356941858927407

Epoch: 5| Step: 7
Training loss: 2.836387872695923
Validation loss: 2.1361987541119256

Epoch: 5| Step: 8
Training loss: 2.293076276779175
Validation loss: 2.1342016557852426

Epoch: 5| Step: 9
Training loss: 2.5006134510040283
Validation loss: 2.130517840385437

Epoch: 5| Step: 10
Training loss: 2.0619797706604004
Validation loss: 2.1270095258951187

Epoch: 5| Step: 11
Training loss: 1.1189677715301514
Validation loss: 2.1224642395973206

Epoch: 65| Step: 0
Training loss: 2.9582149982452393
Validation loss: 2.119315505027771

Epoch: 5| Step: 1
Training loss: 2.119483232498169
Validation loss: 2.1151676227649054

Epoch: 5| Step: 2
Training loss: 2.369044780731201
Validation loss: 2.1129617939392724

Epoch: 5| Step: 3
Training loss: 2.3928442001342773
Validation loss: 2.106517588098844

Epoch: 5| Step: 4
Training loss: 2.406472682952881
Validation loss: 2.1081807911396027

Epoch: 5| Step: 5
Training loss: 2.7053158283233643
Validation loss: 2.10145732263724

Epoch: 5| Step: 6
Training loss: 2.155261754989624
Validation loss: 2.09985122581323

Epoch: 5| Step: 7
Training loss: 1.9539827108383179
Validation loss: 2.096900999546051

Epoch: 5| Step: 8
Training loss: 1.7444608211517334
Validation loss: 2.1041684051354728

Epoch: 5| Step: 9
Training loss: 1.903036117553711
Validation loss: 2.098547955354055

Epoch: 5| Step: 10
Training loss: 2.1608099937438965
Validation loss: 2.0960231920083365

Epoch: 5| Step: 11
Training loss: 2.726557493209839
Validation loss: 2.0979780703783035

Epoch: 66| Step: 0
Training loss: 2.243821144104004
Validation loss: 2.0896248569091163

Epoch: 5| Step: 1
Training loss: 2.2922823429107666
Validation loss: 2.0892897099256516

Epoch: 5| Step: 2
Training loss: 2.1724114418029785
Validation loss: 2.094718317190806

Epoch: 5| Step: 3
Training loss: 2.357761859893799
Validation loss: 2.0991096844275794

Epoch: 5| Step: 4
Training loss: 2.464475393295288
Validation loss: 2.09822807709376

Epoch: 5| Step: 5
Training loss: 1.650679588317871
Validation loss: 2.0963491201400757

Epoch: 5| Step: 6
Training loss: 2.6903748512268066
Validation loss: 2.1020685335000358

Epoch: 5| Step: 7
Training loss: 2.5904288291931152
Validation loss: 2.094519982735316

Epoch: 5| Step: 8
Training loss: 2.45784330368042
Validation loss: 2.095092718799909

Epoch: 5| Step: 9
Training loss: 2.05232572555542
Validation loss: 2.0903976261615753

Epoch: 5| Step: 10
Training loss: 1.8443610668182373
Validation loss: 2.095079297820727

Epoch: 5| Step: 11
Training loss: 2.962862491607666
Validation loss: 2.0933807094891868

Epoch: 67| Step: 0
Training loss: 2.367880344390869
Validation loss: 2.088144068916639

Epoch: 5| Step: 1
Training loss: 2.3287723064422607
Validation loss: 2.0809843788544335

Epoch: 5| Step: 2
Training loss: 2.413477897644043
Validation loss: 2.0813646713892617

Epoch: 5| Step: 3
Training loss: 2.121824026107788
Validation loss: 2.0885754078626633

Epoch: 5| Step: 4
Training loss: 2.4296727180480957
Validation loss: 2.079783702890078

Epoch: 5| Step: 5
Training loss: 2.4973676204681396
Validation loss: 2.0797503292560577

Epoch: 5| Step: 6
Training loss: 2.408982038497925
Validation loss: 2.077352891365687

Epoch: 5| Step: 7
Training loss: 1.8393363952636719
Validation loss: 2.080772951245308

Epoch: 5| Step: 8
Training loss: 2.263148307800293
Validation loss: 2.0806782941023507

Epoch: 5| Step: 9
Training loss: 1.898949384689331
Validation loss: 2.06913460791111

Epoch: 5| Step: 10
Training loss: 2.324127674102783
Validation loss: 2.0694180925687156

Epoch: 5| Step: 11
Training loss: 2.165894031524658
Validation loss: 2.067269374926885

Epoch: 68| Step: 0
Training loss: 2.1882665157318115
Validation loss: 2.066404422124227

Epoch: 5| Step: 1
Training loss: 2.5650875568389893
Validation loss: 2.071507434050242

Epoch: 5| Step: 2
Training loss: 2.5159096717834473
Validation loss: 2.0749416947364807

Epoch: 5| Step: 3
Training loss: 1.7956550121307373
Validation loss: 2.073773225148519

Epoch: 5| Step: 4
Training loss: 2.320977210998535
Validation loss: 2.080294445157051

Epoch: 5| Step: 5
Training loss: 2.0760326385498047
Validation loss: 2.0804987798134484

Epoch: 5| Step: 6
Training loss: 2.3926565647125244
Validation loss: 2.0775786886612573

Epoch: 5| Step: 7
Training loss: 2.0440070629119873
Validation loss: 2.0764758735895157

Epoch: 5| Step: 8
Training loss: 2.2053115367889404
Validation loss: 2.073106358448664

Epoch: 5| Step: 9
Training loss: 2.3957295417785645
Validation loss: 2.0696576833724976

Epoch: 5| Step: 10
Training loss: 2.3298816680908203
Validation loss: 2.0646767218907676

Epoch: 5| Step: 11
Training loss: 1.8564584255218506
Validation loss: 2.0652100344498954

Epoch: 69| Step: 0
Training loss: 2.241934299468994
Validation loss: 2.062486246228218

Epoch: 5| Step: 1
Training loss: 2.2815239429473877
Validation loss: 2.0562741806109748

Epoch: 5| Step: 2
Training loss: 1.835222601890564
Validation loss: 2.0575498094161353

Epoch: 5| Step: 3
Training loss: 2.5679984092712402
Validation loss: 2.0553388049205146

Epoch: 5| Step: 4
Training loss: 2.1954801082611084
Validation loss: 2.0535956571499505

Epoch: 5| Step: 5
Training loss: 2.080040693283081
Validation loss: 2.0526003738244376

Epoch: 5| Step: 6
Training loss: 2.4668989181518555
Validation loss: 2.053941528002421

Epoch: 5| Step: 7
Training loss: 2.192671537399292
Validation loss: 2.051561156908671

Epoch: 5| Step: 8
Training loss: 1.7862708568572998
Validation loss: 2.05177732805411

Epoch: 5| Step: 9
Training loss: 2.2947003841400146
Validation loss: 2.0520674685637155

Epoch: 5| Step: 10
Training loss: 2.540274143218994
Validation loss: 2.0447473376989365

Epoch: 5| Step: 11
Training loss: 2.8714747428894043
Validation loss: 2.051108807325363

Epoch: 70| Step: 0
Training loss: 2.3040871620178223
Validation loss: 2.0518786311149597

Epoch: 5| Step: 1
Training loss: 2.9880404472351074
Validation loss: 2.0455756882826486

Epoch: 5| Step: 2
Training loss: 1.803733468055725
Validation loss: 2.0473559598128

Epoch: 5| Step: 3
Training loss: 2.1124167442321777
Validation loss: 2.051219125588735

Epoch: 5| Step: 4
Training loss: 1.9231857061386108
Validation loss: 2.054757987459501

Epoch: 5| Step: 5
Training loss: 2.770516872406006
Validation loss: 2.059663991133372

Epoch: 5| Step: 6
Training loss: 2.3098018169403076
Validation loss: 2.0568126291036606

Epoch: 5| Step: 7
Training loss: 2.622209072113037
Validation loss: 2.0545979837576547

Epoch: 5| Step: 8
Training loss: 1.7807588577270508
Validation loss: 2.056200315554937

Epoch: 5| Step: 9
Training loss: 2.250250816345215
Validation loss: 2.056609123945236

Epoch: 5| Step: 10
Training loss: 1.7287582159042358
Validation loss: 2.0530220915873847

Epoch: 5| Step: 11
Training loss: 2.1341147422790527
Validation loss: 2.045902301867803

Epoch: 71| Step: 0
Training loss: 2.4919817447662354
Validation loss: 2.0475890586773553

Epoch: 5| Step: 1
Training loss: 2.274019718170166
Validation loss: 2.049254814783732

Epoch: 5| Step: 2
Training loss: 2.1515724658966064
Validation loss: 2.0530350704987845

Epoch: 5| Step: 3
Training loss: 2.871107816696167
Validation loss: 2.0587486773729324

Epoch: 5| Step: 4
Training loss: 2.3496193885803223
Validation loss: 2.0573346515496573

Epoch: 5| Step: 5
Training loss: 2.031550884246826
Validation loss: 2.0498209943374

Epoch: 5| Step: 6
Training loss: 1.8391087055206299
Validation loss: 2.0527639339367547

Epoch: 5| Step: 7
Training loss: 1.4262888431549072
Validation loss: 2.04743059972922

Epoch: 5| Step: 8
Training loss: 1.9442096948623657
Validation loss: 2.0419020305077233

Epoch: 5| Step: 9
Training loss: 2.5395286083221436
Validation loss: 2.0328355878591537

Epoch: 5| Step: 10
Training loss: 2.5871291160583496
Validation loss: 2.044628153244654

Epoch: 5| Step: 11
Training loss: 2.754042863845825
Validation loss: 2.0442094455162683

Epoch: 72| Step: 0
Training loss: 1.93682062625885
Validation loss: 2.0465963184833527

Epoch: 5| Step: 1
Training loss: 2.1592483520507812
Validation loss: 2.0491771896680198

Epoch: 5| Step: 2
Training loss: 2.36797833442688
Validation loss: 2.0579217076301575

Epoch: 5| Step: 3
Training loss: 1.8493369817733765
Validation loss: 2.054338609178861

Epoch: 5| Step: 4
Training loss: 2.490330219268799
Validation loss: 2.052900865674019

Epoch: 5| Step: 5
Training loss: 2.249044179916382
Validation loss: 2.0577085316181183

Epoch: 5| Step: 6
Training loss: 2.5439352989196777
Validation loss: 2.052848989764849

Epoch: 5| Step: 7
Training loss: 1.8434594869613647
Validation loss: 2.0491361816724143

Epoch: 5| Step: 8
Training loss: 2.2547032833099365
Validation loss: 2.0439789493878684

Epoch: 5| Step: 9
Training loss: 2.3871283531188965
Validation loss: 2.0408248752355576

Epoch: 5| Step: 10
Training loss: 2.382230043411255
Validation loss: 2.0299181391795478

Epoch: 5| Step: 11
Training loss: 2.272590160369873
Validation loss: 2.02999738852183

Epoch: 73| Step: 0
Training loss: 2.8618712425231934
Validation loss: 2.040980170170466

Epoch: 5| Step: 1
Training loss: 1.8006260395050049
Validation loss: 2.0522669007380805

Epoch: 5| Step: 2
Training loss: 1.790218710899353
Validation loss: 2.0610965142647424

Epoch: 5| Step: 3
Training loss: 1.9272006750106812
Validation loss: 2.0852867662906647

Epoch: 5| Step: 4
Training loss: 2.3414530754089355
Validation loss: 2.122960239648819

Epoch: 5| Step: 5
Training loss: 1.9349033832550049
Validation loss: 2.119947150349617

Epoch: 5| Step: 6
Training loss: 2.637763261795044
Validation loss: 2.0876021534204483

Epoch: 5| Step: 7
Training loss: 2.326747417449951
Validation loss: 2.067219004034996

Epoch: 5| Step: 8
Training loss: 2.246819257736206
Validation loss: 2.0470488518476486

Epoch: 5| Step: 9
Training loss: 2.4120662212371826
Validation loss: 2.035784070690473

Epoch: 5| Step: 10
Training loss: 2.158867597579956
Validation loss: 2.0402505894502005

Epoch: 5| Step: 11
Training loss: 2.2439310550689697
Validation loss: 2.059274340669314

Epoch: 74| Step: 0
Training loss: 2.4093785285949707
Validation loss: 2.0901830146710076

Epoch: 5| Step: 1
Training loss: 2.2548186779022217
Validation loss: 2.106577684481939

Epoch: 5| Step: 2
Training loss: 2.1985397338867188
Validation loss: 2.110146919886271

Epoch: 5| Step: 3
Training loss: 2.0379457473754883
Validation loss: 2.143983672062556

Epoch: 5| Step: 4
Training loss: 2.4252724647521973
Validation loss: 2.136445646484693

Epoch: 5| Step: 5
Training loss: 2.099874258041382
Validation loss: 2.1451824406782785

Epoch: 5| Step: 6
Training loss: 2.6034045219421387
Validation loss: 2.1512569884459176

Epoch: 5| Step: 7
Training loss: 1.9202215671539307
Validation loss: 2.148812065521876

Epoch: 5| Step: 8
Training loss: 2.6151509284973145
Validation loss: 2.150127266844114

Epoch: 5| Step: 9
Training loss: 2.401022434234619
Validation loss: 2.1374958654244742

Epoch: 5| Step: 10
Training loss: 2.347242832183838
Validation loss: 2.1098743627468743

Epoch: 5| Step: 11
Training loss: 2.3599720001220703
Validation loss: 2.0957107146581015

Epoch: 75| Step: 0
Training loss: 2.371701717376709
Validation loss: 2.090070287386576

Epoch: 5| Step: 1
Training loss: 1.9630874395370483
Validation loss: 2.0854121247927346

Epoch: 5| Step: 2
Training loss: 2.464942455291748
Validation loss: 2.072868982950846

Epoch: 5| Step: 3
Training loss: 2.317805290222168
Validation loss: 2.068118691444397

Epoch: 5| Step: 4
Training loss: 2.096987247467041
Validation loss: 2.057548845807711

Epoch: 5| Step: 5
Training loss: 2.225947856903076
Validation loss: 2.041608452796936

Epoch: 5| Step: 6
Training loss: 2.6001698970794678
Validation loss: 2.034747451543808

Epoch: 5| Step: 7
Training loss: 2.389763593673706
Validation loss: 2.029136290152868

Epoch: 5| Step: 8
Training loss: 1.6803497076034546
Validation loss: 2.0366680224736533

Epoch: 5| Step: 9
Training loss: 2.7074713706970215
Validation loss: 2.054109742244085

Epoch: 5| Step: 10
Training loss: 1.830458641052246
Validation loss: 2.0617354611555734

Epoch: 5| Step: 11
Training loss: 2.451484441757202
Validation loss: 2.061440626780192

Epoch: 76| Step: 0
Training loss: 2.7542567253112793
Validation loss: 2.047357032696406

Epoch: 5| Step: 1
Training loss: 1.9491336345672607
Validation loss: 2.0338956912358603

Epoch: 5| Step: 2
Training loss: 1.9307587146759033
Validation loss: 2.0430070559183755

Epoch: 5| Step: 3
Training loss: 2.2922799587249756
Validation loss: 2.042960728208224

Epoch: 5| Step: 4
Training loss: 1.8238067626953125
Validation loss: 2.047374432285627

Epoch: 5| Step: 5
Training loss: 2.557410717010498
Validation loss: 2.049621800581614

Epoch: 5| Step: 6
Training loss: 2.5025012493133545
Validation loss: 2.0511171221733093

Epoch: 5| Step: 7
Training loss: 1.8957493305206299
Validation loss: 2.0550120373566947

Epoch: 5| Step: 8
Training loss: 2.0297131538391113
Validation loss: 2.049154207110405

Epoch: 5| Step: 9
Training loss: 2.4931514263153076
Validation loss: 2.0609056850274405

Epoch: 5| Step: 10
Training loss: 2.265498638153076
Validation loss: 2.0589095006386438

Epoch: 5| Step: 11
Training loss: 2.772519826889038
Validation loss: 2.059009090065956

Epoch: 77| Step: 0
Training loss: 2.2908687591552734
Validation loss: 2.0569207121928534

Epoch: 5| Step: 1
Training loss: 2.019927740097046
Validation loss: 2.0614869048198066

Epoch: 5| Step: 2
Training loss: 2.103680372238159
Validation loss: 2.059129476547241

Epoch: 5| Step: 3
Training loss: 2.501788377761841
Validation loss: 2.060542787114779

Epoch: 5| Step: 4
Training loss: 2.0269722938537598
Validation loss: 2.0539558132489524

Epoch: 5| Step: 5
Training loss: 2.184467315673828
Validation loss: 2.053074469168981

Epoch: 5| Step: 6
Training loss: 2.066220998764038
Validation loss: 2.0545389701922736

Epoch: 5| Step: 7
Training loss: 2.1866977214813232
Validation loss: 2.0516868084669113

Epoch: 5| Step: 8
Training loss: 2.211841344833374
Validation loss: 2.0442900309960046

Epoch: 5| Step: 9
Training loss: 2.5417680740356445
Validation loss: 2.0418910533189774

Epoch: 5| Step: 10
Training loss: 2.403642416000366
Validation loss: 2.0395971735318503

Epoch: 5| Step: 11
Training loss: 2.2707886695861816
Validation loss: 2.0377837419509888

Epoch: 78| Step: 0
Training loss: 2.3313393592834473
Validation loss: 2.0295929511388144

Epoch: 5| Step: 1
Training loss: 1.9910014867782593
Validation loss: 2.0221468110879264

Epoch: 5| Step: 2
Training loss: 2.315500497817993
Validation loss: 2.0215009103218713

Epoch: 5| Step: 3
Training loss: 2.0301353931427
Validation loss: 2.0280060867468515

Epoch: 5| Step: 4
Training loss: 2.0473642349243164
Validation loss: 2.019104694326719

Epoch: 5| Step: 5
Training loss: 2.1005775928497314
Validation loss: 2.027517184615135

Epoch: 5| Step: 6
Training loss: 2.4334092140197754
Validation loss: 2.0336768428484597

Epoch: 5| Step: 7
Training loss: 2.2676968574523926
Validation loss: 2.031987557808558

Epoch: 5| Step: 8
Training loss: 2.5763144493103027
Validation loss: 2.0309120814005532

Epoch: 5| Step: 9
Training loss: 1.728065848350525
Validation loss: 2.030769864718119

Epoch: 5| Step: 10
Training loss: 2.4170432090759277
Validation loss: 2.030404438575109

Epoch: 5| Step: 11
Training loss: 2.5282983779907227
Validation loss: 2.03371192018191

Epoch: 79| Step: 0
Training loss: 2.2006566524505615
Validation loss: 2.0234599808851876

Epoch: 5| Step: 1
Training loss: 2.523859739303589
Validation loss: 2.025453348954519

Epoch: 5| Step: 2
Training loss: 1.897038459777832
Validation loss: 2.0289738923311234

Epoch: 5| Step: 3
Training loss: 2.626124858856201
Validation loss: 2.025321905811628

Epoch: 5| Step: 4
Training loss: 2.642857551574707
Validation loss: 2.0239723225434623

Epoch: 5| Step: 5
Training loss: 1.9450126886367798
Validation loss: 2.0168609619140625

Epoch: 5| Step: 6
Training loss: 2.117912769317627
Validation loss: 2.0141136844952903

Epoch: 5| Step: 7
Training loss: 2.244050979614258
Validation loss: 2.0039623330036798

Epoch: 5| Step: 8
Training loss: 1.87367844581604
Validation loss: 2.0127558410167694

Epoch: 5| Step: 9
Training loss: 2.482447862625122
Validation loss: 2.014479786157608

Epoch: 5| Step: 10
Training loss: 1.431551456451416
Validation loss: 2.0230679909388223

Epoch: 5| Step: 11
Training loss: 3.094780445098877
Validation loss: 2.0238533119360604

Epoch: 80| Step: 0
Training loss: 1.7583305835723877
Validation loss: 2.0340664933125177

Epoch: 5| Step: 1
Training loss: 2.2154319286346436
Validation loss: 2.0347240020831427

Epoch: 5| Step: 2
Training loss: 2.0104243755340576
Validation loss: 2.0439967860778174

Epoch: 5| Step: 3
Training loss: 2.536071538925171
Validation loss: 2.0417244831720986

Epoch: 5| Step: 4
Training loss: 2.272385597229004
Validation loss: 2.0475176721811295

Epoch: 5| Step: 5
Training loss: 2.1607093811035156
Validation loss: 2.045918896794319

Epoch: 5| Step: 6
Training loss: 2.5196237564086914
Validation loss: 2.041312590241432

Epoch: 5| Step: 7
Training loss: 2.0681633949279785
Validation loss: 2.040418028831482

Epoch: 5| Step: 8
Training loss: 2.382063388824463
Validation loss: 2.035523151357969

Epoch: 5| Step: 9
Training loss: 2.373901844024658
Validation loss: 2.0329344322284064

Epoch: 5| Step: 10
Training loss: 2.121670722961426
Validation loss: 2.0266550928354263

Epoch: 5| Step: 11
Training loss: 1.4209624528884888
Validation loss: 2.0277457733949027

Epoch: 81| Step: 0
Training loss: 1.9022111892700195
Validation loss: 2.0218749046325684

Epoch: 5| Step: 1
Training loss: 2.312642812728882
Validation loss: 2.0228315790494285

Epoch: 5| Step: 2
Training loss: 1.422147274017334
Validation loss: 2.0156018237272897

Epoch: 5| Step: 3
Training loss: 2.5869128704071045
Validation loss: 2.0033389131228128

Epoch: 5| Step: 4
Training loss: 2.6722562313079834
Validation loss: 2.008650923768679

Epoch: 5| Step: 5
Training loss: 2.2126541137695312
Validation loss: 2.0099147856235504

Epoch: 5| Step: 6
Training loss: 2.749619722366333
Validation loss: 2.0128362576166787

Epoch: 5| Step: 7
Training loss: 2.1032283306121826
Validation loss: 2.01302078862985

Epoch: 5| Step: 8
Training loss: 2.1178195476531982
Validation loss: 2.0222253451744714

Epoch: 5| Step: 9
Training loss: 2.1059157848358154
Validation loss: 2.0198029478391013

Epoch: 5| Step: 10
Training loss: 1.8122360706329346
Validation loss: 2.0315799365441003

Epoch: 5| Step: 11
Training loss: 1.8380916118621826
Validation loss: 2.0282848328351974

Epoch: 82| Step: 0
Training loss: 1.8453172445297241
Validation loss: 2.0348942826191583

Epoch: 5| Step: 1
Training loss: 2.56099009513855
Validation loss: 2.019512956341108

Epoch: 5| Step: 2
Training loss: 2.4491500854492188
Validation loss: 2.022717530528704

Epoch: 5| Step: 3
Training loss: 2.197942018508911
Validation loss: 2.025966023405393

Epoch: 5| Step: 4
Training loss: 1.8992879390716553
Validation loss: 2.019914925098419

Epoch: 5| Step: 5
Training loss: 2.3346853256225586
Validation loss: 2.018042509754499

Epoch: 5| Step: 6
Training loss: 1.4828648567199707
Validation loss: 2.0231536676486335

Epoch: 5| Step: 7
Training loss: 2.4611449241638184
Validation loss: 2.01405302186807

Epoch: 5| Step: 8
Training loss: 1.879522681236267
Validation loss: 2.012660394112269

Epoch: 5| Step: 9
Training loss: 2.453158140182495
Validation loss: 2.012894024451574

Epoch: 5| Step: 10
Training loss: 2.414470911026001
Validation loss: 2.01706326007843

Epoch: 5| Step: 11
Training loss: 2.111727714538574
Validation loss: 2.0142972568670907

Epoch: 83| Step: 0
Training loss: 1.6543235778808594
Validation loss: 2.0238827814658484

Epoch: 5| Step: 1
Training loss: 2.286130428314209
Validation loss: 2.026129881540934

Epoch: 5| Step: 2
Training loss: 2.3944382667541504
Validation loss: 2.026377330223719

Epoch: 5| Step: 3
Training loss: 2.213160276412964
Validation loss: 2.0218240718046823

Epoch: 5| Step: 4
Training loss: 2.1213278770446777
Validation loss: 2.017353509863218

Epoch: 5| Step: 5
Training loss: 2.2053911685943604
Validation loss: 2.017331932981809

Epoch: 5| Step: 6
Training loss: 2.366206407546997
Validation loss: 2.015660579005877

Epoch: 5| Step: 7
Training loss: 2.4415135383605957
Validation loss: 2.014178837339083

Epoch: 5| Step: 8
Training loss: 2.153721570968628
Validation loss: 2.0152613619963327

Epoch: 5| Step: 9
Training loss: 2.293581485748291
Validation loss: 2.0193584312995276

Epoch: 5| Step: 10
Training loss: 2.3437821865081787
Validation loss: 2.0151924391587577

Epoch: 5| Step: 11
Training loss: 0.6397532224655151
Validation loss: 2.021733487645785

Epoch: 84| Step: 0
Training loss: 2.034400224685669
Validation loss: 2.014855374892553

Epoch: 5| Step: 1
Training loss: 2.0205702781677246
Validation loss: 2.0221119125684104

Epoch: 5| Step: 2
Training loss: 1.9287261962890625
Validation loss: 2.0235595603783927

Epoch: 5| Step: 3
Training loss: 1.697368860244751
Validation loss: 2.0241488913695016

Epoch: 5| Step: 4
Training loss: 1.5846214294433594
Validation loss: 2.0235131084918976

Epoch: 5| Step: 5
Training loss: 2.1155638694763184
Validation loss: 2.029716834425926

Epoch: 5| Step: 6
Training loss: 2.61047625541687
Validation loss: 2.0233719795942307

Epoch: 5| Step: 7
Training loss: 2.473567247390747
Validation loss: 2.0235396226247153

Epoch: 5| Step: 8
Training loss: 2.095487117767334
Validation loss: 2.0186712642510733

Epoch: 5| Step: 9
Training loss: 2.5400421619415283
Validation loss: 2.0190779815117517

Epoch: 5| Step: 10
Training loss: 2.643092393875122
Validation loss: 2.017367179195086

Epoch: 5| Step: 11
Training loss: 3.138678550720215
Validation loss: 2.0195797433455787

Epoch: 85| Step: 0
Training loss: 2.317533493041992
Validation loss: 2.014408911267916

Epoch: 5| Step: 1
Training loss: 2.0677297115325928
Validation loss: 2.010615428288778

Epoch: 5| Step: 2
Training loss: 2.5494160652160645
Validation loss: 2.0257893105347953

Epoch: 5| Step: 3
Training loss: 1.9372628927230835
Validation loss: 2.023492326339086

Epoch: 5| Step: 4
Training loss: 1.394491195678711
Validation loss: 2.018492286403974

Epoch: 5| Step: 5
Training loss: 2.438464403152466
Validation loss: 2.028271645307541

Epoch: 5| Step: 6
Training loss: 1.9341901540756226
Validation loss: 2.024630223711332

Epoch: 5| Step: 7
Training loss: 2.4952704906463623
Validation loss: 2.025831937789917

Epoch: 5| Step: 8
Training loss: 2.4503848552703857
Validation loss: 2.022870346903801

Epoch: 5| Step: 9
Training loss: 2.0412111282348633
Validation loss: 2.0142160008351007

Epoch: 5| Step: 10
Training loss: 2.1825594902038574
Validation loss: 2.0101312647263208

Epoch: 5| Step: 11
Training loss: 1.6548460721969604
Validation loss: 2.0066095292568207

Epoch: 86| Step: 0
Training loss: 1.827879548072815
Validation loss: 1.996247912446658

Epoch: 5| Step: 1
Training loss: 2.1358542442321777
Validation loss: 2.002738873163859

Epoch: 5| Step: 2
Training loss: 2.1632962226867676
Validation loss: 2.008581221103668

Epoch: 5| Step: 3
Training loss: 2.425297260284424
Validation loss: 2.006366471449534

Epoch: 5| Step: 4
Training loss: 1.7294740676879883
Validation loss: 2.006211042404175

Epoch: 5| Step: 5
Training loss: 1.8722184896469116
Validation loss: 2.0081617732842765

Epoch: 5| Step: 6
Training loss: 2.4235565662384033
Validation loss: 2.0134614358345666

Epoch: 5| Step: 7
Training loss: 2.290710687637329
Validation loss: 2.0090567767620087

Epoch: 5| Step: 8
Training loss: 2.468627452850342
Validation loss: 2.011912832657496

Epoch: 5| Step: 9
Training loss: 2.369985580444336
Validation loss: 2.014962395032247

Epoch: 5| Step: 10
Training loss: 2.139626979827881
Validation loss: 2.006248523791631

Epoch: 5| Step: 11
Training loss: 1.3918637037277222
Validation loss: 2.010467310746511

Epoch: 87| Step: 0
Training loss: 1.9187452793121338
Validation loss: 2.009045069416364

Epoch: 5| Step: 1
Training loss: 2.6026244163513184
Validation loss: 2.01612921555837

Epoch: 5| Step: 2
Training loss: 1.976125955581665
Validation loss: 2.017932633558909

Epoch: 5| Step: 3
Training loss: 2.7567028999328613
Validation loss: 2.0254749208688736

Epoch: 5| Step: 4
Training loss: 1.8953838348388672
Validation loss: 2.0268751680850983

Epoch: 5| Step: 5
Training loss: 1.6373964548110962
Validation loss: 2.0251396099726358

Epoch: 5| Step: 6
Training loss: 2.299126148223877
Validation loss: 2.032691309849421

Epoch: 5| Step: 7
Training loss: 2.117846965789795
Validation loss: 2.0360448509454727

Epoch: 5| Step: 8
Training loss: 2.144397258758545
Validation loss: 2.0464340647061667

Epoch: 5| Step: 9
Training loss: 1.8889167308807373
Validation loss: 2.0293205132087073

Epoch: 5| Step: 10
Training loss: 2.344052791595459
Validation loss: 2.0385614186525345

Epoch: 5| Step: 11
Training loss: 2.234175682067871
Validation loss: 2.037565380334854

Epoch: 88| Step: 0
Training loss: 2.083195924758911
Validation loss: 2.0386127481857934

Epoch: 5| Step: 1
Training loss: 1.9657100439071655
Validation loss: 2.0306927263736725

Epoch: 5| Step: 2
Training loss: 2.299468755722046
Validation loss: 2.027220075329145

Epoch: 5| Step: 3
Training loss: 1.8429737091064453
Validation loss: 2.018247331182162

Epoch: 5| Step: 4
Training loss: 1.9999481439590454
Validation loss: 2.020482877890269

Epoch: 5| Step: 5
Training loss: 2.437592029571533
Validation loss: 2.017042095462481

Epoch: 5| Step: 6
Training loss: 1.9261529445648193
Validation loss: 2.022237241268158

Epoch: 5| Step: 7
Training loss: 2.445530414581299
Validation loss: 2.024462342262268

Epoch: 5| Step: 8
Training loss: 2.1092827320098877
Validation loss: 2.0212255964676538

Epoch: 5| Step: 9
Training loss: 2.0207371711730957
Validation loss: 2.0340719272693

Epoch: 5| Step: 10
Training loss: 2.243013381958008
Validation loss: 2.0295509149630866

Epoch: 5| Step: 11
Training loss: 3.6981334686279297
Validation loss: 2.0273280094067254

Epoch: 89| Step: 0
Training loss: 2.064565420150757
Validation loss: 2.029241849978765

Epoch: 5| Step: 1
Training loss: 1.559577465057373
Validation loss: 2.0233260740836463

Epoch: 5| Step: 2
Training loss: 2.040639638900757
Validation loss: 2.0265246530373893

Epoch: 5| Step: 3
Training loss: 2.3668532371520996
Validation loss: 2.0287759651740394

Epoch: 5| Step: 4
Training loss: 2.4559969902038574
Validation loss: 2.0182521641254425

Epoch: 5| Step: 5
Training loss: 1.7011131048202515
Validation loss: 2.0108082791169486

Epoch: 5| Step: 6
Training loss: 2.2785427570343018
Validation loss: 2.0223658680915833

Epoch: 5| Step: 7
Training loss: 2.3030457496643066
Validation loss: 2.0167908618847528

Epoch: 5| Step: 8
Training loss: 2.143263339996338
Validation loss: 2.024286756912867

Epoch: 5| Step: 9
Training loss: 2.399014711380005
Validation loss: 2.0183503280083337

Epoch: 5| Step: 10
Training loss: 2.214515209197998
Validation loss: 2.021657799681028

Epoch: 5| Step: 11
Training loss: 3.1344051361083984
Validation loss: 2.032787948846817

Epoch: 90| Step: 0
Training loss: 1.9621998071670532
Validation loss: 2.037167345484098

Epoch: 5| Step: 1
Training loss: 2.316044330596924
Validation loss: 2.0395788848400116

Epoch: 5| Step: 2
Training loss: 2.6391491889953613
Validation loss: 2.048256998260816

Epoch: 5| Step: 3
Training loss: 2.430990695953369
Validation loss: 2.0477267305056253

Epoch: 5| Step: 4
Training loss: 2.239067554473877
Validation loss: 2.0588470846414566

Epoch: 5| Step: 5
Training loss: 2.171005964279175
Validation loss: 2.048363054792086

Epoch: 5| Step: 6
Training loss: 1.5294275283813477
Validation loss: 2.0351507663726807

Epoch: 5| Step: 7
Training loss: 2.333726406097412
Validation loss: 2.0356443425019584

Epoch: 5| Step: 8
Training loss: 2.32959246635437
Validation loss: 2.034920816620191

Epoch: 5| Step: 9
Training loss: 1.631177544593811
Validation loss: 2.027291809519132

Epoch: 5| Step: 10
Training loss: 2.1263740062713623
Validation loss: 2.020361691713333

Epoch: 5| Step: 11
Training loss: 2.0768373012542725
Validation loss: 2.0173297127087912

Epoch: 91| Step: 0
Training loss: 2.2300236225128174
Validation loss: 2.0205131669839225

Epoch: 5| Step: 1
Training loss: 2.11421799659729
Validation loss: 2.0256439050038657

Epoch: 5| Step: 2
Training loss: 2.3421807289123535
Validation loss: 2.022358328104019

Epoch: 5| Step: 3
Training loss: 1.6944738626480103
Validation loss: 2.014071981112162

Epoch: 5| Step: 4
Training loss: 2.2221903800964355
Validation loss: 2.012156476577123

Epoch: 5| Step: 5
Training loss: 2.3394415378570557
Validation loss: 2.015012249350548

Epoch: 5| Step: 6
Training loss: 2.2228121757507324
Validation loss: 2.018608331680298

Epoch: 5| Step: 7
Training loss: 2.299684524536133
Validation loss: 2.012330859899521

Epoch: 5| Step: 8
Training loss: 1.690665602684021
Validation loss: 2.0136010448137918

Epoch: 5| Step: 9
Training loss: 2.378692626953125
Validation loss: 2.019732008377711

Epoch: 5| Step: 10
Training loss: 1.7080425024032593
Validation loss: 2.0260644952456155

Epoch: 5| Step: 11
Training loss: 3.7624754905700684
Validation loss: 2.028508389989535

Epoch: 92| Step: 0
Training loss: 2.193326473236084
Validation loss: 2.0221627900997796

Epoch: 5| Step: 1
Training loss: 2.493349552154541
Validation loss: 2.032426839073499

Epoch: 5| Step: 2
Training loss: 1.916076898574829
Validation loss: 2.0393839528163276

Epoch: 5| Step: 3
Training loss: 2.0587239265441895
Validation loss: 2.041679471731186

Epoch: 5| Step: 4
Training loss: 2.0532474517822266
Validation loss: 2.049145539601644

Epoch: 5| Step: 5
Training loss: 2.5189647674560547
Validation loss: 2.0379094233115516

Epoch: 5| Step: 6
Training loss: 2.055818796157837
Validation loss: 2.0344716062148414

Epoch: 5| Step: 7
Training loss: 1.769152283668518
Validation loss: 2.039930288990339

Epoch: 5| Step: 8
Training loss: 2.4060158729553223
Validation loss: 2.0304937859376273

Epoch: 5| Step: 9
Training loss: 2.0096042156219482
Validation loss: 2.032316654920578

Epoch: 5| Step: 10
Training loss: 2.1762309074401855
Validation loss: 2.0232864369948707

Epoch: 5| Step: 11
Training loss: 2.048535108566284
Validation loss: 2.0226678997278214

Epoch: 93| Step: 0
Training loss: 2.271984338760376
Validation loss: 2.0190200358629227

Epoch: 5| Step: 1
Training loss: 1.7196238040924072
Validation loss: 2.021793097257614

Epoch: 5| Step: 2
Training loss: 2.44865083694458
Validation loss: 2.0200615177551904

Epoch: 5| Step: 3
Training loss: 2.1141653060913086
Validation loss: 2.0215548227230706

Epoch: 5| Step: 4
Training loss: 1.9367485046386719
Validation loss: 2.019737794995308

Epoch: 5| Step: 5
Training loss: 2.3890156745910645
Validation loss: 2.015484149257342

Epoch: 5| Step: 6
Training loss: 2.1897153854370117
Validation loss: 2.0118466168642044

Epoch: 5| Step: 7
Training loss: 2.2841992378234863
Validation loss: 2.01568811138471

Epoch: 5| Step: 8
Training loss: 1.9170093536376953
Validation loss: 2.0127818485101066

Epoch: 5| Step: 9
Training loss: 1.945708990097046
Validation loss: 2.012258688608805

Epoch: 5| Step: 10
Training loss: 2.262944221496582
Validation loss: 2.019900624950727

Epoch: 5| Step: 11
Training loss: 2.1388163566589355
Validation loss: 2.017107903957367

Epoch: 94| Step: 0
Training loss: 2.410386800765991
Validation loss: 2.0281406392653785

Epoch: 5| Step: 1
Training loss: 2.2634825706481934
Validation loss: 2.0236463199059167

Epoch: 5| Step: 2
Training loss: 2.790837049484253
Validation loss: 2.030689721306165

Epoch: 5| Step: 3
Training loss: 1.7823877334594727
Validation loss: 2.0218906750281653

Epoch: 5| Step: 4
Training loss: 2.138282060623169
Validation loss: 2.029719283183416

Epoch: 5| Step: 5
Training loss: 2.1529178619384766
Validation loss: 2.029399032394091

Epoch: 5| Step: 6
Training loss: 2.2063310146331787
Validation loss: 2.033681790033976

Epoch: 5| Step: 7
Training loss: 2.1040267944335938
Validation loss: 2.039508586128553

Epoch: 5| Step: 8
Training loss: 1.8479598760604858
Validation loss: 2.0204149037599564

Epoch: 5| Step: 9
Training loss: 1.3853583335876465
Validation loss: 2.027814413110415

Epoch: 5| Step: 10
Training loss: 2.02311372756958
Validation loss: 2.0412378360827765

Epoch: 5| Step: 11
Training loss: 3.1775617599487305
Validation loss: 2.0301406532526016

Epoch: 95| Step: 0
Training loss: 2.274980068206787
Validation loss: 2.0621178845564523

Epoch: 5| Step: 1
Training loss: 2.500434637069702
Validation loss: 2.069063281019529

Epoch: 5| Step: 2
Training loss: 2.1318066120147705
Validation loss: 2.098259742061297

Epoch: 5| Step: 3
Training loss: 2.291740894317627
Validation loss: 2.0914839009443917

Epoch: 5| Step: 4
Training loss: 2.011634111404419
Validation loss: 2.085103372732798

Epoch: 5| Step: 5
Training loss: 2.179802179336548
Validation loss: 2.0696318397919335

Epoch: 5| Step: 6
Training loss: 2.4456186294555664
Validation loss: 2.044546072681745

Epoch: 5| Step: 7
Training loss: 1.479404091835022
Validation loss: 2.0201486001412072

Epoch: 5| Step: 8
Training loss: 2.134448528289795
Validation loss: 2.0072834342718124

Epoch: 5| Step: 9
Training loss: 2.2934317588806152
Validation loss: 2.0049601246913276

Epoch: 5| Step: 10
Training loss: 2.122885227203369
Validation loss: 2.017242580652237

Epoch: 5| Step: 11
Training loss: 2.431307792663574
Validation loss: 2.019174630443255

Epoch: 96| Step: 0
Training loss: 2.550825834274292
Validation loss: 2.0196069926023483

Epoch: 5| Step: 1
Training loss: 2.1330010890960693
Validation loss: 2.0098821570475898

Epoch: 5| Step: 2
Training loss: 2.5135538578033447
Validation loss: 2.0126097251971564

Epoch: 5| Step: 3
Training loss: 2.586677074432373
Validation loss: 2.010866994659106

Epoch: 5| Step: 4
Training loss: 2.4057915210723877
Validation loss: 2.0142259101072946

Epoch: 5| Step: 5
Training loss: 2.158341884613037
Validation loss: 2.0207031220197678

Epoch: 5| Step: 6
Training loss: 2.108776807785034
Validation loss: 2.018304472168287

Epoch: 5| Step: 7
Training loss: 2.1496737003326416
Validation loss: 2.0149473945299783

Epoch: 5| Step: 8
Training loss: 1.9943050146102905
Validation loss: 2.030006925264994

Epoch: 5| Step: 9
Training loss: 1.9107109308242798
Validation loss: 2.0329961528380713

Epoch: 5| Step: 10
Training loss: 1.3768726587295532
Validation loss: 2.0299895306428275

Epoch: 5| Step: 11
Training loss: 1.2183653116226196
Validation loss: 2.0292347917954126

Epoch: 97| Step: 0
Training loss: 1.9918838739395142
Validation loss: 2.0353924185037613

Epoch: 5| Step: 1
Training loss: 2.679966926574707
Validation loss: 2.035015399257342

Epoch: 5| Step: 2
Training loss: 1.8653137683868408
Validation loss: 2.0230910877386727

Epoch: 5| Step: 3
Training loss: 2.112678050994873
Validation loss: 2.0189982702334723

Epoch: 5| Step: 4
Training loss: 2.2898643016815186
Validation loss: 2.0253426929314933

Epoch: 5| Step: 5
Training loss: 1.8615772724151611
Validation loss: 2.016740689675013

Epoch: 5| Step: 6
Training loss: 2.6274447441101074
Validation loss: 2.016149456302325

Epoch: 5| Step: 7
Training loss: 1.9613525867462158
Validation loss: 2.011533876260122

Epoch: 5| Step: 8
Training loss: 2.0164427757263184
Validation loss: 2.0097007701794305

Epoch: 5| Step: 9
Training loss: 2.520350456237793
Validation loss: 2.0120578010876975

Epoch: 5| Step: 10
Training loss: 1.4854986667633057
Validation loss: 2.025452504555384

Epoch: 5| Step: 11
Training loss: 2.735750198364258
Validation loss: 2.0162273198366165

Epoch: 98| Step: 0
Training loss: 1.6440662145614624
Validation loss: 2.0298306544621787

Epoch: 5| Step: 1
Training loss: 1.6106703281402588
Validation loss: 2.0175090730190277

Epoch: 5| Step: 2
Training loss: 2.208249568939209
Validation loss: 2.018771434823672

Epoch: 5| Step: 3
Training loss: 2.4749763011932373
Validation loss: 2.016882131497065

Epoch: 5| Step: 4
Training loss: 2.3504045009613037
Validation loss: 2.0102164099613824

Epoch: 5| Step: 5
Training loss: 1.6842353343963623
Validation loss: 2.0183247874180474

Epoch: 5| Step: 6
Training loss: 2.0570785999298096
Validation loss: 2.017441208163897

Epoch: 5| Step: 7
Training loss: 2.2545957565307617
Validation loss: 2.021375223994255

Epoch: 5| Step: 8
Training loss: 2.6029183864593506
Validation loss: 2.01411272585392

Epoch: 5| Step: 9
Training loss: 1.9729721546173096
Validation loss: 2.020511503020922

Epoch: 5| Step: 10
Training loss: 2.461270332336426
Validation loss: 2.0174918472766876

Epoch: 5| Step: 11
Training loss: 2.345733165740967
Validation loss: 2.016972243785858

Epoch: 99| Step: 0
Training loss: 2.234405517578125
Validation loss: 2.026883418361346

Epoch: 5| Step: 1
Training loss: 2.414249897003174
Validation loss: 2.028745969136556

Epoch: 5| Step: 2
Training loss: 2.218505859375
Validation loss: 2.0311197886864343

Epoch: 5| Step: 3
Training loss: 2.3074183464050293
Validation loss: 2.0155639350414276

Epoch: 5| Step: 4
Training loss: 1.9705851078033447
Validation loss: 2.015902424852053

Epoch: 5| Step: 5
Training loss: 1.9753614664077759
Validation loss: 2.0202059845129647

Epoch: 5| Step: 6
Training loss: 1.863796591758728
Validation loss: 2.0233191748460135

Epoch: 5| Step: 7
Training loss: 2.099440813064575
Validation loss: 2.042740801970164

Epoch: 5| Step: 8
Training loss: 2.160224437713623
Validation loss: 2.0218409647544227

Epoch: 5| Step: 9
Training loss: 2.2351293563842773
Validation loss: 2.0220640202363334

Epoch: 5| Step: 10
Training loss: 1.7128400802612305
Validation loss: 2.0236861457427344

Epoch: 5| Step: 11
Training loss: 3.4605655670166016
Validation loss: 2.025983691215515

Epoch: 100| Step: 0
Training loss: 2.0125033855438232
Validation loss: 2.026150037844976

Epoch: 5| Step: 1
Training loss: 2.2127373218536377
Validation loss: 2.015302141507467

Epoch: 5| Step: 2
Training loss: 2.421619415283203
Validation loss: 2.0050958494345346

Epoch: 5| Step: 3
Training loss: 2.499415874481201
Validation loss: 2.0187537322441735

Epoch: 5| Step: 4
Training loss: 2.128631591796875
Validation loss: 2.011729364593824

Epoch: 5| Step: 5
Training loss: 2.045567035675049
Validation loss: 2.0126627733310065

Epoch: 5| Step: 6
Training loss: 2.261855125427246
Validation loss: 2.0129312525192895

Epoch: 5| Step: 7
Training loss: 1.8263375759124756
Validation loss: 2.0179666628440223

Epoch: 5| Step: 8
Training loss: 1.8148224353790283
Validation loss: 2.0175120135148368

Epoch: 5| Step: 9
Training loss: 1.9362304210662842
Validation loss: 2.017595355709394

Epoch: 5| Step: 10
Training loss: 2.2769253253936768
Validation loss: 2.024588336547216

Epoch: 5| Step: 11
Training loss: 2.474630832672119
Validation loss: 2.0326606184244156

Epoch: 101| Step: 0
Training loss: 2.4029488563537598
Validation loss: 2.0366899371147156

Epoch: 5| Step: 1
Training loss: 1.8040052652359009
Validation loss: 2.0318649262189865

Epoch: 5| Step: 2
Training loss: 2.268568277359009
Validation loss: 2.038743649919828

Epoch: 5| Step: 3
Training loss: 2.2573182582855225
Validation loss: 2.0369455069303513

Epoch: 5| Step: 4
Training loss: 2.0822994709014893
Validation loss: 2.0384602149327598

Epoch: 5| Step: 5
Training loss: 1.951183557510376
Validation loss: 2.0304687221844993

Epoch: 5| Step: 6
Training loss: 1.7923879623413086
Validation loss: 2.0368760377168655

Epoch: 5| Step: 7
Training loss: 2.3545620441436768
Validation loss: 2.0448140452305474

Epoch: 5| Step: 8
Training loss: 1.964879035949707
Validation loss: 2.02957850197951

Epoch: 5| Step: 9
Training loss: 2.148226261138916
Validation loss: 2.024465332428614

Epoch: 5| Step: 10
Training loss: 2.5113301277160645
Validation loss: 2.01800803343455

Epoch: 5| Step: 11
Training loss: 1.569321870803833
Validation loss: 2.005402018626531

Epoch: 102| Step: 0
Training loss: 2.0095884799957275
Validation loss: 2.023647283514341

Epoch: 5| Step: 1
Training loss: 2.0248756408691406
Validation loss: 2.0225579887628555

Epoch: 5| Step: 2
Training loss: 1.8849983215332031
Validation loss: 2.0308822194735208

Epoch: 5| Step: 3
Training loss: 2.314887523651123
Validation loss: 2.0309795439243317

Epoch: 5| Step: 4
Training loss: 2.2740840911865234
Validation loss: 2.0357021441062293

Epoch: 5| Step: 5
Training loss: 1.6457446813583374
Validation loss: 2.0296984761953354

Epoch: 5| Step: 6
Training loss: 2.0666675567626953
Validation loss: 2.033989354968071

Epoch: 5| Step: 7
Training loss: 2.0815577507019043
Validation loss: 2.02460145453612

Epoch: 5| Step: 8
Training loss: 2.1853222846984863
Validation loss: 2.0262093047300973

Epoch: 5| Step: 9
Training loss: 2.3710508346557617
Validation loss: 2.0286944607893624

Epoch: 5| Step: 10
Training loss: 2.533090591430664
Validation loss: 2.0216143131256104

Epoch: 5| Step: 11
Training loss: 1.505789875984192
Validation loss: 2.026994357506434

Epoch: 103| Step: 0
Training loss: 2.081029176712036
Validation loss: 2.018297885855039

Epoch: 5| Step: 1
Training loss: 1.6855512857437134
Validation loss: 2.024120738108953

Epoch: 5| Step: 2
Training loss: 2.3280081748962402
Validation loss: 2.0271697491407394

Epoch: 5| Step: 3
Training loss: 1.977826714515686
Validation loss: 2.017865464091301

Epoch: 5| Step: 4
Training loss: 1.8790019750595093
Validation loss: 2.024345815181732

Epoch: 5| Step: 5
Training loss: 2.224557399749756
Validation loss: 2.0203599830468497

Epoch: 5| Step: 6
Training loss: 2.0564255714416504
Validation loss: 2.026435762643814

Epoch: 5| Step: 7
Training loss: 1.971544861793518
Validation loss: 2.02441376944383

Epoch: 5| Step: 8
Training loss: 2.6097912788391113
Validation loss: 2.016698027650515

Epoch: 5| Step: 9
Training loss: 2.166027545928955
Validation loss: 2.0149661799271903

Epoch: 5| Step: 10
Training loss: 2.3268849849700928
Validation loss: 2.0209366430838904

Epoch: 5| Step: 11
Training loss: 2.096050262451172
Validation loss: 2.0175984551509223

Epoch: 104| Step: 0
Training loss: 2.0482547283172607
Validation loss: 2.0149723291397095

Epoch: 5| Step: 1
Training loss: 2.1345248222351074
Validation loss: 2.008461723725001

Epoch: 5| Step: 2
Training loss: 1.7180683612823486
Validation loss: 2.0169099122285843

Epoch: 5| Step: 3
Training loss: 2.312964677810669
Validation loss: 2.008270646135012

Epoch: 5| Step: 4
Training loss: 1.5649478435516357
Validation loss: 2.0147292713324227

Epoch: 5| Step: 5
Training loss: 2.457347869873047
Validation loss: 2.0171734541654587

Epoch: 5| Step: 6
Training loss: 2.1100571155548096
Validation loss: 2.0187866936127343

Epoch: 5| Step: 7
Training loss: 2.408581256866455
Validation loss: 2.0176127354303994

Epoch: 5| Step: 8
Training loss: 1.9142471551895142
Validation loss: 2.0203148275613785

Epoch: 5| Step: 9
Training loss: 2.5105679035186768
Validation loss: 2.028125156958898

Epoch: 5| Step: 10
Training loss: 2.2118828296661377
Validation loss: 2.017044792572657

Epoch: 5| Step: 11
Training loss: 1.5445141792297363
Validation loss: 2.0152112593253455

Epoch: 105| Step: 0
Training loss: 2.5238029956817627
Validation loss: 2.0125210732221603

Epoch: 5| Step: 1
Training loss: 1.8736894130706787
Validation loss: 2.0209089666604996

Epoch: 5| Step: 2
Training loss: 2.1313884258270264
Validation loss: 2.013774315516154

Epoch: 5| Step: 3
Training loss: 1.8627307415008545
Validation loss: 2.012016703685125

Epoch: 5| Step: 4
Training loss: 2.387765884399414
Validation loss: 2.0128131608168283

Epoch: 5| Step: 5
Training loss: 2.2428927421569824
Validation loss: 2.0120913485685983

Epoch: 5| Step: 6
Training loss: 1.5984876155853271
Validation loss: 2.021459624171257

Epoch: 5| Step: 7
Training loss: 1.6110389232635498
Validation loss: 2.0208178559939065

Epoch: 5| Step: 8
Training loss: 2.361778736114502
Validation loss: 2.012449080745379

Epoch: 5| Step: 9
Training loss: 2.352905750274658
Validation loss: 2.0227318604787192

Epoch: 5| Step: 10
Training loss: 2.38250994682312
Validation loss: 2.030943349003792

Epoch: 5| Step: 11
Training loss: 1.2515482902526855
Validation loss: 2.0170469731092453

Epoch: 106| Step: 0
Training loss: 1.911625623703003
Validation loss: 2.013502836227417

Epoch: 5| Step: 1
Training loss: 1.7036941051483154
Validation loss: 2.0076108326514563

Epoch: 5| Step: 2
Training loss: 2.0770788192749023
Validation loss: 2.020217627286911

Epoch: 5| Step: 3
Training loss: 1.4776242971420288
Validation loss: 2.0143226434787116

Epoch: 5| Step: 4
Training loss: 2.101968765258789
Validation loss: 2.011449789007505

Epoch: 5| Step: 5
Training loss: 2.196240186691284
Validation loss: 2.003067980209986

Epoch: 5| Step: 6
Training loss: 2.4637222290039062
Validation loss: 2.004536430040995

Epoch: 5| Step: 7
Training loss: 2.260801315307617
Validation loss: 2.0076637317736945

Epoch: 5| Step: 8
Training loss: 2.449730634689331
Validation loss: 2.0052487204472222

Epoch: 5| Step: 9
Training loss: 2.21794056892395
Validation loss: 2.0088615318139396

Epoch: 5| Step: 10
Training loss: 2.1952714920043945
Validation loss: 2.0147809237241745

Epoch: 5| Step: 11
Training loss: 2.777369499206543
Validation loss: 2.0259543557961783

Epoch: 107| Step: 0
Training loss: 1.7044627666473389
Validation loss: 2.0269212226072946

Epoch: 5| Step: 1
Training loss: 2.486219882965088
Validation loss: 2.0270588248968124

Epoch: 5| Step: 2
Training loss: 2.528432846069336
Validation loss: 2.033665264646212

Epoch: 5| Step: 3
Training loss: 2.2457990646362305
Validation loss: 2.043430065115293

Epoch: 5| Step: 4
Training loss: 1.6808269023895264
Validation loss: 2.03048899769783

Epoch: 5| Step: 5
Training loss: 2.131526231765747
Validation loss: 2.0227351586023965

Epoch: 5| Step: 6
Training loss: 1.8157627582550049
Validation loss: 2.023925617337227

Epoch: 5| Step: 7
Training loss: 2.1583595275878906
Validation loss: 2.0226101179917655

Epoch: 5| Step: 8
Training loss: 2.7647883892059326
Validation loss: 2.022619982560476

Epoch: 5| Step: 9
Training loss: 1.7752691507339478
Validation loss: 2.017531697948774

Epoch: 5| Step: 10
Training loss: 1.9999024868011475
Validation loss: 2.019390568137169

Epoch: 5| Step: 11
Training loss: 2.813333511352539
Validation loss: 2.0292892903089523

Epoch: 108| Step: 0
Training loss: 2.2202823162078857
Validation loss: 2.025516072909037

Epoch: 5| Step: 1
Training loss: 1.9263393878936768
Validation loss: 2.021265293161074

Epoch: 5| Step: 2
Training loss: 2.0331673622131348
Validation loss: 2.033828611175219

Epoch: 5| Step: 3
Training loss: 2.4611687660217285
Validation loss: 2.022794192035993

Epoch: 5| Step: 4
Training loss: 2.202267646789551
Validation loss: 2.0425623257954917

Epoch: 5| Step: 5
Training loss: 1.9950698614120483
Validation loss: 2.0369421740372977

Epoch: 5| Step: 6
Training loss: 1.8843495845794678
Validation loss: 2.032047376036644

Epoch: 5| Step: 7
Training loss: 2.2487168312072754
Validation loss: 2.0270005961259208

Epoch: 5| Step: 8
Training loss: 1.8727128505706787
Validation loss: 2.0188229580720267

Epoch: 5| Step: 9
Training loss: 2.1750049591064453
Validation loss: 2.028754472732544

Epoch: 5| Step: 10
Training loss: 2.287391185760498
Validation loss: 2.0222654889027276

Epoch: 5| Step: 11
Training loss: 2.013519287109375
Validation loss: 2.011586825052897

Epoch: 109| Step: 0
Training loss: 1.7119853496551514
Validation loss: 2.0258675118287406

Epoch: 5| Step: 1
Training loss: 2.3528263568878174
Validation loss: 2.0285144795974097

Epoch: 5| Step: 2
Training loss: 1.908628225326538
Validation loss: 2.022615830103556

Epoch: 5| Step: 3
Training loss: 2.642998456954956
Validation loss: 2.024768074353536

Epoch: 5| Step: 4
Training loss: 1.876123070716858
Validation loss: 2.0296384940544763

Epoch: 5| Step: 5
Training loss: 2.227850914001465
Validation loss: 2.022203544775645

Epoch: 5| Step: 6
Training loss: 1.9170589447021484
Validation loss: 2.0216576953728995

Epoch: 5| Step: 7
Training loss: 2.464369297027588
Validation loss: 2.0225213120381036

Epoch: 5| Step: 8
Training loss: 1.9556400775909424
Validation loss: 2.0242121318976083

Epoch: 5| Step: 9
Training loss: 2.455277681350708
Validation loss: 2.0267795473337173

Epoch: 5| Step: 10
Training loss: 1.6997501850128174
Validation loss: 2.022384002804756

Epoch: 5| Step: 11
Training loss: 1.168084740638733
Validation loss: 2.01979923248291

Epoch: 110| Step: 0
Training loss: 2.208228826522827
Validation loss: 2.0310821731885276

Epoch: 5| Step: 1
Training loss: 2.1878185272216797
Validation loss: 2.020605022708575

Epoch: 5| Step: 2
Training loss: 2.5009965896606445
Validation loss: 2.0177537699540458

Epoch: 5| Step: 3
Training loss: 1.9026696681976318
Validation loss: 2.0064283361037574

Epoch: 5| Step: 4
Training loss: 2.2411763668060303
Validation loss: 2.0202584813038507

Epoch: 5| Step: 5
Training loss: 2.2389607429504395
Validation loss: 2.0210821429888406

Epoch: 5| Step: 6
Training loss: 1.861515760421753
Validation loss: 2.0247268825769424

Epoch: 5| Step: 7
Training loss: 1.7990392446517944
Validation loss: 2.024687041838964

Epoch: 5| Step: 8
Training loss: 2.531881093978882
Validation loss: 2.0221487482388816

Epoch: 5| Step: 9
Training loss: 1.9037907123565674
Validation loss: 2.037584642569224

Epoch: 5| Step: 10
Training loss: 1.8962533473968506
Validation loss: 2.04364113509655

Epoch: 5| Step: 11
Training loss: 1.7146201133728027
Validation loss: 2.03743007282416

Epoch: 111| Step: 0
Training loss: 1.6210922002792358
Validation loss: 2.046353022257487

Epoch: 5| Step: 1
Training loss: 2.163684368133545
Validation loss: 2.044892077644666

Epoch: 5| Step: 2
Training loss: 2.5164971351623535
Validation loss: 2.0512996266285577

Epoch: 5| Step: 3
Training loss: 2.2889881134033203
Validation loss: 2.0421292136112847

Epoch: 5| Step: 4
Training loss: 2.016075611114502
Validation loss: 2.0307125796874366

Epoch: 5| Step: 5
Training loss: 2.1756749153137207
Validation loss: 2.038065418601036

Epoch: 5| Step: 6
Training loss: 1.8694489002227783
Validation loss: 2.0262754261493683

Epoch: 5| Step: 7
Training loss: 2.443000555038452
Validation loss: 2.0171743979056678

Epoch: 5| Step: 8
Training loss: 2.179635524749756
Validation loss: 2.0368926326433816

Epoch: 5| Step: 9
Training loss: 2.347108840942383
Validation loss: 2.0284507671991983

Epoch: 5| Step: 10
Training loss: 1.7799293994903564
Validation loss: 2.025255486369133

Epoch: 5| Step: 11
Training loss: 1.2975038290023804
Validation loss: 2.0236222445964813

Epoch: 112| Step: 0
Training loss: 2.3558411598205566
Validation loss: 2.0220563312371573

Epoch: 5| Step: 1
Training loss: 2.1585075855255127
Validation loss: 2.0202055126428604

Epoch: 5| Step: 2
Training loss: 2.2197680473327637
Validation loss: 2.0170918802420297

Epoch: 5| Step: 3
Training loss: 1.6173908710479736
Validation loss: 2.019273226459821

Epoch: 5| Step: 4
Training loss: 2.5480682849884033
Validation loss: 2.0224638134241104

Epoch: 5| Step: 5
Training loss: 1.3921617269515991
Validation loss: 2.018675218025843

Epoch: 5| Step: 6
Training loss: 2.3249642848968506
Validation loss: 2.023897777001063

Epoch: 5| Step: 7
Training loss: 2.4051387310028076
Validation loss: 2.028460477789243

Epoch: 5| Step: 8
Training loss: 2.370936870574951
Validation loss: 2.0324172526597977

Epoch: 5| Step: 9
Training loss: 1.617859125137329
Validation loss: 2.0230690290530524

Epoch: 5| Step: 10
Training loss: 2.039811372756958
Validation loss: 2.0186567852894464

Epoch: 5| Step: 11
Training loss: 2.393974781036377
Validation loss: 2.031687801082929

Epoch: 113| Step: 0
Training loss: 1.378957748413086
Validation loss: 2.030085191130638

Epoch: 5| Step: 1
Training loss: 2.314931869506836
Validation loss: 2.031599218646685

Epoch: 5| Step: 2
Training loss: 2.231640338897705
Validation loss: 2.03404101729393

Epoch: 5| Step: 3
Training loss: 2.6234607696533203
Validation loss: 2.0324947436650596

Epoch: 5| Step: 4
Training loss: 1.7966448068618774
Validation loss: 2.0277638683716455

Epoch: 5| Step: 5
Training loss: 2.14668869972229
Validation loss: 2.0138507882754006

Epoch: 5| Step: 6
Training loss: 2.0835368633270264
Validation loss: 2.01171113550663

Epoch: 5| Step: 7
Training loss: 2.3231072425842285
Validation loss: 2.013746832807859

Epoch: 5| Step: 8
Training loss: 1.6538183689117432
Validation loss: 2.016296570499738

Epoch: 5| Step: 9
Training loss: 2.0536208152770996
Validation loss: 2.0203840285539627

Epoch: 5| Step: 10
Training loss: 2.2558863162994385
Validation loss: 2.021690954764684

Epoch: 5| Step: 11
Training loss: 3.094102621078491
Validation loss: 2.0240728557109833

Epoch: 114| Step: 0
Training loss: 1.9845209121704102
Validation loss: 2.0266150534152985

Epoch: 5| Step: 1
Training loss: 2.186021327972412
Validation loss: 2.020129462083181

Epoch: 5| Step: 2
Training loss: 2.3382620811462402
Validation loss: 2.0279219845930734

Epoch: 5| Step: 3
Training loss: 2.4557366371154785
Validation loss: 2.0198712746302285

Epoch: 5| Step: 4
Training loss: 1.4436699151992798
Validation loss: 2.0218074520428977

Epoch: 5| Step: 5
Training loss: 2.133694648742676
Validation loss: 2.020128314693769

Epoch: 5| Step: 6
Training loss: 2.5694191455841064
Validation loss: 2.0122756163279214

Epoch: 5| Step: 7
Training loss: 2.139186382293701
Validation loss: 2.005742977062861

Epoch: 5| Step: 8
Training loss: 1.801906943321228
Validation loss: 2.0090671380360923

Epoch: 5| Step: 9
Training loss: 2.2530274391174316
Validation loss: 2.0098057836294174

Epoch: 5| Step: 10
Training loss: 1.9460300207138062
Validation loss: 2.0100371142228446

Epoch: 5| Step: 11
Training loss: 1.864674687385559
Validation loss: 2.012623354792595

Epoch: 115| Step: 0
Training loss: 2.6702640056610107
Validation loss: 2.0163666009902954

Epoch: 5| Step: 1
Training loss: 2.3123226165771484
Validation loss: 2.0352432976166406

Epoch: 5| Step: 2
Training loss: 1.938572645187378
Validation loss: 2.030900463461876

Epoch: 5| Step: 3
Training loss: 1.5774593353271484
Validation loss: 2.041561638315519

Epoch: 5| Step: 4
Training loss: 1.8414103984832764
Validation loss: 2.040718267361323

Epoch: 5| Step: 5
Training loss: 1.9865920543670654
Validation loss: 2.0343477924664817

Epoch: 5| Step: 6
Training loss: 2.0411746501922607
Validation loss: 2.0256145050128302

Epoch: 5| Step: 7
Training loss: 1.7333459854125977
Validation loss: 2.0192767083644867

Epoch: 5| Step: 8
Training loss: 2.1939549446105957
Validation loss: 2.016108989715576

Epoch: 5| Step: 9
Training loss: 2.526939868927002
Validation loss: 2.0105788111686707

Epoch: 5| Step: 10
Training loss: 2.4342963695526123
Validation loss: 2.018262987335523

Epoch: 5| Step: 11
Training loss: 1.5306978225708008
Validation loss: 2.00976969798406

Epoch: 116| Step: 0
Training loss: 1.7565561532974243
Validation loss: 2.013204594453176

Epoch: 5| Step: 1
Training loss: 1.9460551738739014
Validation loss: 2.0196050802866616

Epoch: 5| Step: 2
Training loss: 1.5753567218780518
Validation loss: 2.015700106819471

Epoch: 5| Step: 3
Training loss: 1.632709264755249
Validation loss: 2.025080939133962

Epoch: 5| Step: 4
Training loss: 2.5297038555145264
Validation loss: 2.0395931154489517

Epoch: 5| Step: 5
Training loss: 2.7171692848205566
Validation loss: 2.034913475314776

Epoch: 5| Step: 6
Training loss: 2.3597190380096436
Validation loss: 2.045737082759539

Epoch: 5| Step: 7
Training loss: 2.228407382965088
Validation loss: 2.045824403564135

Epoch: 5| Step: 8
Training loss: 1.76986825466156
Validation loss: 2.044464021921158

Epoch: 5| Step: 9
Training loss: 1.892425537109375
Validation loss: 2.0390436748663583

Epoch: 5| Step: 10
Training loss: 2.370016098022461
Validation loss: 2.0391547630230584

Epoch: 5| Step: 11
Training loss: 3.3440918922424316
Validation loss: 2.0599476099014282

Epoch: 117| Step: 0
Training loss: 1.9184367656707764
Validation loss: 2.043475200732549

Epoch: 5| Step: 1
Training loss: 2.353898525238037
Validation loss: 2.0413573682308197

Epoch: 5| Step: 2
Training loss: 1.8743619918823242
Validation loss: 2.035894756515821

Epoch: 5| Step: 3
Training loss: 1.8536237478256226
Validation loss: 2.0220762689908347

Epoch: 5| Step: 4
Training loss: 2.1045289039611816
Validation loss: 2.013526971141497

Epoch: 5| Step: 5
Training loss: 2.2087230682373047
Validation loss: 2.0029870917399726

Epoch: 5| Step: 6
Training loss: 2.4113383293151855
Validation loss: 2.0106026430924735

Epoch: 5| Step: 7
Training loss: 1.8010456562042236
Validation loss: 2.0055809368689856

Epoch: 5| Step: 8
Training loss: 1.9316247701644897
Validation loss: 2.010797436038653

Epoch: 5| Step: 9
Training loss: 2.4041690826416016
Validation loss: 2.013341178496679

Epoch: 5| Step: 10
Training loss: 2.139498233795166
Validation loss: 2.010632957021395

Epoch: 5| Step: 11
Training loss: 2.154583692550659
Validation loss: 2.0034055362145105

Epoch: 118| Step: 0
Training loss: 1.7574230432510376
Validation loss: 2.0019367883602777

Epoch: 5| Step: 1
Training loss: 2.0339226722717285
Validation loss: 2.0199086914459863

Epoch: 5| Step: 2
Training loss: 2.2231545448303223
Validation loss: 2.0300745417674384

Epoch: 5| Step: 3
Training loss: 2.615149736404419
Validation loss: 2.0320277512073517

Epoch: 5| Step: 4
Training loss: 2.235988140106201
Validation loss: 2.0423526763916016

Epoch: 5| Step: 5
Training loss: 2.16313099861145
Validation loss: 2.039530485868454

Epoch: 5| Step: 6
Training loss: 2.2407615184783936
Validation loss: 2.0437822540601096

Epoch: 5| Step: 7
Training loss: 1.8325328826904297
Validation loss: 2.038532684246699

Epoch: 5| Step: 8
Training loss: 2.2738027572631836
Validation loss: 2.033604840437571

Epoch: 5| Step: 9
Training loss: 1.9640758037567139
Validation loss: 2.0292787502209344

Epoch: 5| Step: 10
Training loss: 1.6945743560791016
Validation loss: 2.0325127790371575

Epoch: 5| Step: 11
Training loss: 2.3535375595092773
Validation loss: 2.0278006345033646

Epoch: 119| Step: 0
Training loss: 2.6366257667541504
Validation loss: 2.0359279861052832

Epoch: 5| Step: 1
Training loss: 2.3471503257751465
Validation loss: 2.027595341205597

Epoch: 5| Step: 2
Training loss: 2.5097944736480713
Validation loss: 2.031871403257052

Epoch: 5| Step: 3
Training loss: 1.6521265506744385
Validation loss: 2.028625170389811

Epoch: 5| Step: 4
Training loss: 2.2666308879852295
Validation loss: 2.0285711089769998

Epoch: 5| Step: 5
Training loss: 1.6969935894012451
Validation loss: 2.0334749271472297

Epoch: 5| Step: 6
Training loss: 2.1951541900634766
Validation loss: 2.026136169830958

Epoch: 5| Step: 7
Training loss: 1.742685317993164
Validation loss: 2.0379165410995483

Epoch: 5| Step: 8
Training loss: 1.6153688430786133
Validation loss: 2.030053292711576

Epoch: 5| Step: 9
Training loss: 2.111558198928833
Validation loss: 2.0456039011478424

Epoch: 5| Step: 10
Training loss: 1.8893651962280273
Validation loss: 2.0422596484422684

Epoch: 5| Step: 11
Training loss: 3.2974095344543457
Validation loss: 2.027026613553365

Epoch: 120| Step: 0
Training loss: 1.9117647409439087
Validation loss: 2.04595023393631

Epoch: 5| Step: 1
Training loss: 2.510376453399658
Validation loss: 2.048493206501007

Epoch: 5| Step: 2
Training loss: 1.6301826238632202
Validation loss: 2.0490057319402695

Epoch: 5| Step: 3
Training loss: 2.098145008087158
Validation loss: 2.038447991013527

Epoch: 5| Step: 4
Training loss: 1.6429191827774048
Validation loss: 2.0366883873939514

Epoch: 5| Step: 5
Training loss: 2.155489444732666
Validation loss: 2.039124697446823

Epoch: 5| Step: 6
Training loss: 2.4820199012756348
Validation loss: 2.03372415403525

Epoch: 5| Step: 7
Training loss: 1.6026983261108398
Validation loss: 2.0299848318099976

Epoch: 5| Step: 8
Training loss: 2.1804256439208984
Validation loss: 2.030945365627607

Epoch: 5| Step: 9
Training loss: 2.0724194049835205
Validation loss: 2.026402458548546

Epoch: 5| Step: 10
Training loss: 2.5668773651123047
Validation loss: 2.0287643323342004

Epoch: 5| Step: 11
Training loss: 1.9775065183639526
Validation loss: 2.0302844842274985

Epoch: 121| Step: 0
Training loss: 1.7631343603134155
Validation loss: 2.030153458317121

Epoch: 5| Step: 1
Training loss: 1.9579375982284546
Validation loss: 2.0342500458161035

Epoch: 5| Step: 2
Training loss: 2.1401400566101074
Validation loss: 2.0337393631537757

Epoch: 5| Step: 3
Training loss: 2.497819662094116
Validation loss: 2.038403724630674

Epoch: 5| Step: 4
Training loss: 1.977726697921753
Validation loss: 2.0343664089838662

Epoch: 5| Step: 5
Training loss: 2.105762243270874
Validation loss: 2.0309724559386573

Epoch: 5| Step: 6
Training loss: 2.2690258026123047
Validation loss: 2.0416657527287803

Epoch: 5| Step: 7
Training loss: 1.9314212799072266
Validation loss: 2.0437018275260925

Epoch: 5| Step: 8
Training loss: 1.6808884143829346
Validation loss: 2.044098357359568

Epoch: 5| Step: 9
Training loss: 2.1525604724884033
Validation loss: 2.048060029745102

Epoch: 5| Step: 10
Training loss: 2.3747124671936035
Validation loss: 2.0592463264862695

Epoch: 5| Step: 11
Training loss: 2.1899802684783936
Validation loss: 2.047965258359909

Epoch: 122| Step: 0
Training loss: 1.86639404296875
Validation loss: 2.0470826079448066

Epoch: 5| Step: 1
Training loss: 2.361117124557495
Validation loss: 2.0373565753300986

Epoch: 5| Step: 2
Training loss: 1.7593162059783936
Validation loss: 2.036152794957161

Epoch: 5| Step: 3
Training loss: 1.910591721534729
Validation loss: 2.032164062062899

Epoch: 5| Step: 4
Training loss: 1.8476860523223877
Validation loss: 2.0263209839661918

Epoch: 5| Step: 5
Training loss: 2.217040777206421
Validation loss: 2.0250988701979318

Epoch: 5| Step: 6
Training loss: 2.3887205123901367
Validation loss: 2.020968496799469

Epoch: 5| Step: 7
Training loss: 2.1834869384765625
Validation loss: 2.023192952076594

Epoch: 5| Step: 8
Training loss: 1.9065277576446533
Validation loss: 2.0268484155337014

Epoch: 5| Step: 9
Training loss: 2.2184948921203613
Validation loss: 2.03696608543396

Epoch: 5| Step: 10
Training loss: 2.2458841800689697
Validation loss: 2.0436971485614777

Epoch: 5| Step: 11
Training loss: 1.5154063701629639
Validation loss: 2.030018871029218

Epoch: 123| Step: 0
Training loss: 2.2168498039245605
Validation loss: 2.0464671552181244

Epoch: 5| Step: 1
Training loss: 1.9321445226669312
Validation loss: 2.0523626705010733

Epoch: 5| Step: 2
Training loss: 1.906672716140747
Validation loss: 2.052440067132314

Epoch: 5| Step: 3
Training loss: 2.041264295578003
Validation loss: 2.042358413338661

Epoch: 5| Step: 4
Training loss: 2.264425754547119
Validation loss: 2.044724370042483

Epoch: 5| Step: 5
Training loss: 2.5557563304901123
Validation loss: 2.042678172389666

Epoch: 5| Step: 6
Training loss: 1.871861219406128
Validation loss: 2.0508747696876526

Epoch: 5| Step: 7
Training loss: 1.7613375186920166
Validation loss: 2.0467774172623954

Epoch: 5| Step: 8
Training loss: 2.258787155151367
Validation loss: 2.044452046354612

Epoch: 5| Step: 9
Training loss: 1.8400354385375977
Validation loss: 2.043326328198115

Epoch: 5| Step: 10
Training loss: 2.1068363189697266
Validation loss: 2.042170455058416

Epoch: 5| Step: 11
Training loss: 2.0534610748291016
Validation loss: 2.036119729280472

Epoch: 124| Step: 0
Training loss: 2.137998342514038
Validation loss: 2.0280277878046036

Epoch: 5| Step: 1
Training loss: 2.128852367401123
Validation loss: 2.027841438849767

Epoch: 5| Step: 2
Training loss: 1.5410528182983398
Validation loss: 2.0337503999471664

Epoch: 5| Step: 3
Training loss: 2.320392370223999
Validation loss: 2.0319801568984985

Epoch: 5| Step: 4
Training loss: 1.7275598049163818
Validation loss: 2.036569664875666

Epoch: 5| Step: 5
Training loss: 2.1694750785827637
Validation loss: 2.031288812557856

Epoch: 5| Step: 6
Training loss: 2.0231332778930664
Validation loss: 2.0359573711951575

Epoch: 5| Step: 7
Training loss: 2.0470874309539795
Validation loss: 2.0312082866827645

Epoch: 5| Step: 8
Training loss: 2.392087936401367
Validation loss: 2.0329877585172653

Epoch: 5| Step: 9
Training loss: 2.3905978202819824
Validation loss: 2.046257957816124

Epoch: 5| Step: 10
Training loss: 2.1708014011383057
Validation loss: 2.053764666120211

Epoch: 5| Step: 11
Training loss: 1.6496238708496094
Validation loss: 2.050862029194832

Epoch: 125| Step: 0
Training loss: 1.8246681690216064
Validation loss: 2.0650163094202676

Epoch: 5| Step: 1
Training loss: 2.410818099975586
Validation loss: 2.062720770637194

Epoch: 5| Step: 2
Training loss: 2.612828493118286
Validation loss: 2.0762658615907035

Epoch: 5| Step: 3
Training loss: 2.00932240486145
Validation loss: 2.0705232471227646

Epoch: 5| Step: 4
Training loss: 2.0961754322052
Validation loss: 2.079085037112236

Epoch: 5| Step: 5
Training loss: 2.127631902694702
Validation loss: 2.0752053012450538

Epoch: 5| Step: 6
Training loss: 2.2822747230529785
Validation loss: 2.060418407122294

Epoch: 5| Step: 7
Training loss: 2.4119553565979004
Validation loss: 2.0461333145697913

Epoch: 5| Step: 8
Training loss: 1.8858115673065186
Validation loss: 2.0406443228324256

Epoch: 5| Step: 9
Training loss: 1.493238091468811
Validation loss: 2.0365966459115348

Epoch: 5| Step: 10
Training loss: 1.6674009561538696
Validation loss: 2.0301128129164376

Epoch: 5| Step: 11
Training loss: 2.9394371509552
Validation loss: 2.029118776321411

Epoch: 126| Step: 0
Training loss: 2.5936033725738525
Validation loss: 2.033675322930018

Epoch: 5| Step: 1
Training loss: 2.011387586593628
Validation loss: 2.036809653043747

Epoch: 5| Step: 2
Training loss: 2.068186044692993
Validation loss: 2.0303626656532288

Epoch: 5| Step: 3
Training loss: 2.010251760482788
Validation loss: 2.031478241086006

Epoch: 5| Step: 4
Training loss: 2.221212863922119
Validation loss: 2.0338234504063926

Epoch: 5| Step: 5
Training loss: 1.6019121408462524
Validation loss: 2.024841477473577

Epoch: 5| Step: 6
Training loss: 2.359084129333496
Validation loss: 2.0233100155989328

Epoch: 5| Step: 7
Training loss: 1.701926827430725
Validation loss: 2.037608573834101

Epoch: 5| Step: 8
Training loss: 2.158559560775757
Validation loss: 2.0428307056427

Epoch: 5| Step: 9
Training loss: 2.311077833175659
Validation loss: 2.0209609071413674

Epoch: 5| Step: 10
Training loss: 1.9643325805664062
Validation loss: 2.0401404201984406

Epoch: 5| Step: 11
Training loss: 2.024129867553711
Validation loss: 2.052168423930804

Epoch: 127| Step: 0
Training loss: 1.4672162532806396
Validation loss: 2.0402444303035736

Epoch: 5| Step: 1
Training loss: 1.837365746498108
Validation loss: 2.048622782031695

Epoch: 5| Step: 2
Training loss: 1.5815850496292114
Validation loss: 2.0513918648163476

Epoch: 5| Step: 3
Training loss: 2.013611078262329
Validation loss: 2.0389470557371774

Epoch: 5| Step: 4
Training loss: 2.2277393341064453
Validation loss: 2.0342739870150885

Epoch: 5| Step: 5
Training loss: 2.6056110858917236
Validation loss: 2.0367551892995834

Epoch: 5| Step: 6
Training loss: 2.5847506523132324
Validation loss: 2.0375949243704476

Epoch: 5| Step: 7
Training loss: 2.321864604949951
Validation loss: 2.041684627532959

Epoch: 5| Step: 8
Training loss: 2.17513108253479
Validation loss: 2.033627981940905

Epoch: 5| Step: 9
Training loss: 2.144519090652466
Validation loss: 2.036573509375254

Epoch: 5| Step: 10
Training loss: 2.050717830657959
Validation loss: 2.0317505101362863

Epoch: 5| Step: 11
Training loss: 1.3852840662002563
Validation loss: 2.0324813624223075

Epoch: 128| Step: 0
Training loss: 2.265021324157715
Validation loss: 2.0424844523270926

Epoch: 5| Step: 1
Training loss: 1.8983370065689087
Validation loss: 2.036463220914205

Epoch: 5| Step: 2
Training loss: 2.599095106124878
Validation loss: 2.036534254749616

Epoch: 5| Step: 3
Training loss: 2.177747964859009
Validation loss: 2.03502128024896

Epoch: 5| Step: 4
Training loss: 2.1591038703918457
Validation loss: 2.0396019220352173

Epoch: 5| Step: 5
Training loss: 2.122039318084717
Validation loss: 2.041239246726036

Epoch: 5| Step: 6
Training loss: 1.6834852695465088
Validation loss: 2.04335688551267

Epoch: 5| Step: 7
Training loss: 1.844369649887085
Validation loss: 2.0611567993958793

Epoch: 5| Step: 8
Training loss: 2.4165241718292236
Validation loss: 2.0555590242147446

Epoch: 5| Step: 9
Training loss: 1.9923206567764282
Validation loss: 2.0511138439178467

Epoch: 5| Step: 10
Training loss: 1.7456289529800415
Validation loss: 2.05661474665006

Epoch: 5| Step: 11
Training loss: 0.8618612289428711
Validation loss: 2.0627547105153403

Epoch: 129| Step: 0
Training loss: 1.6847892999649048
Validation loss: 2.048031841715177

Epoch: 5| Step: 1
Training loss: 2.0619757175445557
Validation loss: 2.0505039940277734

Epoch: 5| Step: 2
Training loss: 1.6084001064300537
Validation loss: 2.0458888113498688

Epoch: 5| Step: 3
Training loss: 2.026426315307617
Validation loss: 2.0523106704155603

Epoch: 5| Step: 4
Training loss: 1.6997820138931274
Validation loss: 2.0411122341950736

Epoch: 5| Step: 5
Training loss: 1.872396469116211
Validation loss: 2.0439924051364264

Epoch: 5| Step: 6
Training loss: 2.223024845123291
Validation loss: 2.0393288334210715

Epoch: 5| Step: 7
Training loss: 2.1409268379211426
Validation loss: 2.0377901444832482

Epoch: 5| Step: 8
Training loss: 2.5211257934570312
Validation loss: 2.0390200863281884

Epoch: 5| Step: 9
Training loss: 2.3728444576263428
Validation loss: 2.048353378971418

Epoch: 5| Step: 10
Training loss: 2.106070041656494
Validation loss: 2.0423770944277444

Epoch: 5| Step: 11
Training loss: 3.6690430641174316
Validation loss: 2.0399343570073447

Epoch: 130| Step: 0
Training loss: 2.2078044414520264
Validation loss: 2.043501302599907

Epoch: 5| Step: 1
Training loss: 1.839878797531128
Validation loss: 2.043904349207878

Epoch: 5| Step: 2
Training loss: 2.2959766387939453
Validation loss: 2.0341239223877587

Epoch: 5| Step: 3
Training loss: 1.7539575099945068
Validation loss: 2.0407019605239234

Epoch: 5| Step: 4
Training loss: 2.4507336616516113
Validation loss: 2.0449166148900986

Epoch: 5| Step: 5
Training loss: 2.2628061771392822
Validation loss: 2.0422416031360626

Epoch: 5| Step: 6
Training loss: 2.018974781036377
Validation loss: 2.046849196155866

Epoch: 5| Step: 7
Training loss: 1.9478759765625
Validation loss: 2.0582697490851083

Epoch: 5| Step: 8
Training loss: 1.7987489700317383
Validation loss: 2.0451988726854324

Epoch: 5| Step: 9
Training loss: 2.021249532699585
Validation loss: 2.0556301226218543

Epoch: 5| Step: 10
Training loss: 2.2672476768493652
Validation loss: 2.074686676263809

Epoch: 5| Step: 11
Training loss: 0.6245317459106445
Validation loss: 2.0603850235541663

Epoch: 131| Step: 0
Training loss: 2.3368849754333496
Validation loss: 2.060113767782847

Epoch: 5| Step: 1
Training loss: 1.9545555114746094
Validation loss: 2.060514514644941

Epoch: 5| Step: 2
Training loss: 1.6239449977874756
Validation loss: 2.0505861043930054

Epoch: 5| Step: 3
Training loss: 2.286820888519287
Validation loss: 2.0501412749290466

Epoch: 5| Step: 4
Training loss: 2.458127498626709
Validation loss: 2.0461157808701196

Epoch: 5| Step: 5
Training loss: 2.8148818016052246
Validation loss: 2.0452966640392938

Epoch: 5| Step: 6
Training loss: 2.4950053691864014
Validation loss: 2.042331263422966

Epoch: 5| Step: 7
Training loss: 1.785007119178772
Validation loss: 2.037897984186808

Epoch: 5| Step: 8
Training loss: 1.4189226627349854
Validation loss: 2.0449555863936744

Epoch: 5| Step: 9
Training loss: 1.7577896118164062
Validation loss: 2.0319248040517173

Epoch: 5| Step: 10
Training loss: 1.8940757513046265
Validation loss: 2.040233607093493

Epoch: 5| Step: 11
Training loss: 1.671810269355774
Validation loss: 2.0487722059090934

Epoch: 132| Step: 0
Training loss: 2.018181562423706
Validation loss: 2.0515154898166656

Epoch: 5| Step: 1
Training loss: 1.7897058725357056
Validation loss: 2.0396412909030914

Epoch: 5| Step: 2
Training loss: 2.1416873931884766
Validation loss: 2.0425527493158975

Epoch: 5| Step: 3
Training loss: 1.9773333072662354
Validation loss: 2.0375725080569587

Epoch: 5| Step: 4
Training loss: 2.1832540035247803
Validation loss: 2.0473340352376304

Epoch: 5| Step: 5
Training loss: 2.0360960960388184
Validation loss: 2.047831585009893

Epoch: 5| Step: 6
Training loss: 2.2005207538604736
Validation loss: 2.0368344833453498

Epoch: 5| Step: 7
Training loss: 2.038423538208008
Validation loss: 2.0651978005965552

Epoch: 5| Step: 8
Training loss: 2.2779293060302734
Validation loss: 2.0543387879927955

Epoch: 5| Step: 9
Training loss: 1.906825065612793
Validation loss: 2.0651248395442963

Epoch: 5| Step: 10
Training loss: 2.2028963565826416
Validation loss: 2.0637986908356347

Epoch: 5| Step: 11
Training loss: 1.8862779140472412
Validation loss: 2.0686359455188117

Epoch: 133| Step: 0
Training loss: 1.8755378723144531
Validation loss: 2.0538377910852432

Epoch: 5| Step: 1
Training loss: 1.7588087320327759
Validation loss: 2.043627838293711

Epoch: 5| Step: 2
Training loss: 2.2193753719329834
Validation loss: 2.0400706281264624

Epoch: 5| Step: 3
Training loss: 2.206000566482544
Validation loss: 2.04083118836085

Epoch: 5| Step: 4
Training loss: 2.1059045791625977
Validation loss: 2.029949034253756

Epoch: 5| Step: 5
Training loss: 1.8883857727050781
Validation loss: 2.0318511029084525

Epoch: 5| Step: 6
Training loss: 2.1279826164245605
Validation loss: 2.0512953797976174

Epoch: 5| Step: 7
Training loss: 2.778921127319336
Validation loss: 2.0482514252265296

Epoch: 5| Step: 8
Training loss: 1.9948288202285767
Validation loss: 2.051719297965368

Epoch: 5| Step: 9
Training loss: 1.8791110515594482
Validation loss: 2.0391835272312164

Epoch: 5| Step: 10
Training loss: 1.868861436843872
Validation loss: 2.0408352663119635

Epoch: 5| Step: 11
Training loss: 1.6676828861236572
Validation loss: 2.0450933277606964

Epoch: 134| Step: 0
Training loss: 2.250148296356201
Validation loss: 2.053234269221624

Epoch: 5| Step: 1
Training loss: 1.8676115274429321
Validation loss: 2.062632232904434

Epoch: 5| Step: 2
Training loss: 2.087292194366455
Validation loss: 2.063223217924436

Epoch: 5| Step: 3
Training loss: 2.876373529434204
Validation loss: 2.053282847007116

Epoch: 5| Step: 4
Training loss: 1.7037076950073242
Validation loss: 2.0560386776924133

Epoch: 5| Step: 5
Training loss: 2.2024130821228027
Validation loss: 2.0528197089831033

Epoch: 5| Step: 6
Training loss: 1.8608213663101196
Validation loss: 2.054688890775045

Epoch: 5| Step: 7
Training loss: 1.8853439092636108
Validation loss: 2.0444498509168625

Epoch: 5| Step: 8
Training loss: 1.6811237335205078
Validation loss: 2.0607904444138208

Epoch: 5| Step: 9
Training loss: 2.0893795490264893
Validation loss: 2.0635401209195456

Epoch: 5| Step: 10
Training loss: 2.1418066024780273
Validation loss: 2.0617035826047263

Epoch: 5| Step: 11
Training loss: 1.2888050079345703
Validation loss: 2.0842284162839255

Epoch: 135| Step: 0
Training loss: 2.21915864944458
Validation loss: 2.078398808836937

Epoch: 5| Step: 1
Training loss: 1.6625087261199951
Validation loss: 2.0554089496533074

Epoch: 5| Step: 2
Training loss: 1.9842088222503662
Validation loss: 2.066637560725212

Epoch: 5| Step: 3
Training loss: 2.186044216156006
Validation loss: 2.065199390053749

Epoch: 5| Step: 4
Training loss: 2.5861480236053467
Validation loss: 2.048475687702497

Epoch: 5| Step: 5
Training loss: 1.4767287969589233
Validation loss: 2.043347587188085

Epoch: 5| Step: 6
Training loss: 2.0806210041046143
Validation loss: 2.0442655235528946

Epoch: 5| Step: 7
Training loss: 1.9786920547485352
Validation loss: 2.041838596264521

Epoch: 5| Step: 8
Training loss: 1.9453895092010498
Validation loss: 2.034429977337519

Epoch: 5| Step: 9
Training loss: 2.4532253742218018
Validation loss: 2.040887882312139

Epoch: 5| Step: 10
Training loss: 2.070493221282959
Validation loss: 2.0383940686782203

Epoch: 5| Step: 11
Training loss: 1.4610955715179443
Validation loss: 2.0344080875317254

Epoch: 136| Step: 0
Training loss: 2.411867618560791
Validation loss: 2.0434500773747764

Epoch: 5| Step: 1
Training loss: 2.323617458343506
Validation loss: 2.0530113826195397

Epoch: 5| Step: 2
Training loss: 1.8517262935638428
Validation loss: 2.0433199803034463

Epoch: 5| Step: 3
Training loss: 2.0168545246124268
Validation loss: 2.0578361401955285

Epoch: 5| Step: 4
Training loss: 2.2780344486236572
Validation loss: 2.06687260667483

Epoch: 5| Step: 5
Training loss: 2.2778608798980713
Validation loss: 2.067081242799759

Epoch: 5| Step: 6
Training loss: 2.0765273571014404
Validation loss: 2.067700440684954

Epoch: 5| Step: 7
Training loss: 2.1296184062957764
Validation loss: 2.0512407372395196

Epoch: 5| Step: 8
Training loss: 1.7028201818466187
Validation loss: 2.0592068334420524

Epoch: 5| Step: 9
Training loss: 1.7514560222625732
Validation loss: 2.0608494579792023

Epoch: 5| Step: 10
Training loss: 1.8157482147216797
Validation loss: 2.0577220221360526

Epoch: 5| Step: 11
Training loss: 1.0808534622192383
Validation loss: 2.060366988182068

Epoch: 137| Step: 0
Training loss: 2.225029468536377
Validation loss: 2.054490720232328

Epoch: 5| Step: 1
Training loss: 1.8780781030654907
Validation loss: 2.0488772292931876

Epoch: 5| Step: 2
Training loss: 2.625753402709961
Validation loss: 2.040070131421089

Epoch: 5| Step: 3
Training loss: 1.6796497106552124
Validation loss: 2.0435846795638404

Epoch: 5| Step: 4
Training loss: 1.937199354171753
Validation loss: 2.032478854060173

Epoch: 5| Step: 5
Training loss: 2.1480278968811035
Validation loss: 2.0505848626295724

Epoch: 5| Step: 6
Training loss: 2.1489739418029785
Validation loss: 2.064711714784304

Epoch: 5| Step: 7
Training loss: 2.0932278633117676
Validation loss: 2.055181384086609

Epoch: 5| Step: 8
Training loss: 2.392991304397583
Validation loss: 2.0521366546551385

Epoch: 5| Step: 9
Training loss: 1.7862904071807861
Validation loss: 2.061737676461538

Epoch: 5| Step: 10
Training loss: 1.6523586511611938
Validation loss: 2.068473443388939

Epoch: 5| Step: 11
Training loss: 1.7154430150985718
Validation loss: 2.0640558997790017

Epoch: 138| Step: 0
Training loss: 1.9116299152374268
Validation loss: 2.0763442665338516

Epoch: 5| Step: 1
Training loss: 1.6302244663238525
Validation loss: 2.061485787232717

Epoch: 5| Step: 2
Training loss: 2.566071033477783
Validation loss: 2.053483555714289

Epoch: 5| Step: 3
Training loss: 1.9548590183258057
Validation loss: 2.0630037039518356

Epoch: 5| Step: 4
Training loss: 2.094616651535034
Validation loss: 2.0536198069651923

Epoch: 5| Step: 5
Training loss: 2.2165069580078125
Validation loss: 2.068062424659729

Epoch: 5| Step: 6
Training loss: 2.265258550643921
Validation loss: 2.066026523709297

Epoch: 5| Step: 7
Training loss: 1.775577187538147
Validation loss: 2.0650737285614014

Epoch: 5| Step: 8
Training loss: 2.005284309387207
Validation loss: 2.054551879564921

Epoch: 5| Step: 9
Training loss: 2.1953799724578857
Validation loss: 2.051614115635554

Epoch: 5| Step: 10
Training loss: 1.9863818883895874
Validation loss: 2.0526076356569924

Epoch: 5| Step: 11
Training loss: 1.1732304096221924
Validation loss: 2.0534135500590005

Epoch: 139| Step: 0
Training loss: 1.7749916315078735
Validation loss: 2.0600217630465827

Epoch: 5| Step: 1
Training loss: 2.376533269882202
Validation loss: 2.0620178133249283

Epoch: 5| Step: 2
Training loss: 2.1731982231140137
Validation loss: 2.059817925095558

Epoch: 5| Step: 3
Training loss: 2.0859169960021973
Validation loss: 2.0618994534015656

Epoch: 5| Step: 4
Training loss: 2.6225101947784424
Validation loss: 2.06194398800532

Epoch: 5| Step: 5
Training loss: 2.112410306930542
Validation loss: 2.0681554774443307

Epoch: 5| Step: 6
Training loss: 2.2143945693969727
Validation loss: 2.0717759976784387

Epoch: 5| Step: 7
Training loss: 1.4949049949645996
Validation loss: 2.059518982966741

Epoch: 5| Step: 8
Training loss: 1.9773486852645874
Validation loss: 2.069971506794294

Epoch: 5| Step: 9
Training loss: 1.7352920770645142
Validation loss: 2.050075809160868

Epoch: 5| Step: 10
Training loss: 1.8754847049713135
Validation loss: 2.0609086404244104

Epoch: 5| Step: 11
Training loss: 1.8469018936157227
Validation loss: 2.055370882153511

Epoch: 140| Step: 0
Training loss: 2.1263463497161865
Validation loss: 2.056887979308764

Epoch: 5| Step: 1
Training loss: 2.0440499782562256
Validation loss: 2.0525769094626107

Epoch: 5| Step: 2
Training loss: 2.1928162574768066
Validation loss: 2.0568957726160684

Epoch: 5| Step: 3
Training loss: 1.7954051494598389
Validation loss: 2.051271920402845

Epoch: 5| Step: 4
Training loss: 2.512289524078369
Validation loss: 2.059864565730095

Epoch: 5| Step: 5
Training loss: 2.054361343383789
Validation loss: 2.0476815650860467

Epoch: 5| Step: 6
Training loss: 1.949939489364624
Validation loss: 2.045993814865748

Epoch: 5| Step: 7
Training loss: 2.357614755630493
Validation loss: 2.0449270009994507

Epoch: 5| Step: 8
Training loss: 1.901785135269165
Validation loss: 2.037026902039846

Epoch: 5| Step: 9
Training loss: 1.5970420837402344
Validation loss: 2.043153087298075

Epoch: 5| Step: 10
Training loss: 1.8476753234863281
Validation loss: 2.0412658204634986

Epoch: 5| Step: 11
Training loss: 3.160527229309082
Validation loss: 2.0317028760910034

Epoch: 141| Step: 0
Training loss: 1.794369101524353
Validation loss: 2.041513830423355

Epoch: 5| Step: 1
Training loss: 1.88351571559906
Validation loss: 2.03097727894783

Epoch: 5| Step: 2
Training loss: 1.842984914779663
Validation loss: 2.0552836656570435

Epoch: 5| Step: 3
Training loss: 1.9583911895751953
Validation loss: 2.042542020479838

Epoch: 5| Step: 4
Training loss: 1.8251060247421265
Validation loss: 2.0631729563077292

Epoch: 5| Step: 5
Training loss: 2.1749751567840576
Validation loss: 2.046275794506073

Epoch: 5| Step: 6
Training loss: 2.3271162509918213
Validation loss: 2.053772618373235

Epoch: 5| Step: 7
Training loss: 1.7545182704925537
Validation loss: 2.057731583714485

Epoch: 5| Step: 8
Training loss: 2.6523211002349854
Validation loss: 2.0614286114772162

Epoch: 5| Step: 9
Training loss: 1.8951095342636108
Validation loss: 2.0819429407517114

Epoch: 5| Step: 10
Training loss: 2.3618040084838867
Validation loss: 2.0679773539304733

Epoch: 5| Step: 11
Training loss: 1.6808428764343262
Validation loss: 2.083111196756363

Epoch: 142| Step: 0
Training loss: 2.1382181644439697
Validation loss: 2.087235147754351

Epoch: 5| Step: 1
Training loss: 2.5858891010284424
Validation loss: 2.0790668229262033

Epoch: 5| Step: 2
Training loss: 1.9454008340835571
Validation loss: 2.0828828116257987

Epoch: 5| Step: 3
Training loss: 2.3436572551727295
Validation loss: 2.0637354850769043

Epoch: 5| Step: 4
Training loss: 1.7409616708755493
Validation loss: 2.053860619664192

Epoch: 5| Step: 5
Training loss: 1.9362825155258179
Validation loss: 2.053937425216039

Epoch: 5| Step: 6
Training loss: 2.039585590362549
Validation loss: 2.0535422960917153

Epoch: 5| Step: 7
Training loss: 2.073430061340332
Validation loss: 2.044414311647415

Epoch: 5| Step: 8
Training loss: 1.7021557092666626
Validation loss: 2.0440582086642585

Epoch: 5| Step: 9
Training loss: 2.0576395988464355
Validation loss: 2.0348538955052695

Epoch: 5| Step: 10
Training loss: 2.052706003189087
Validation loss: 2.0454753835995994

Epoch: 5| Step: 11
Training loss: 1.734780192375183
Validation loss: 2.0437938471635184

Epoch: 143| Step: 0
Training loss: 2.0958733558654785
Validation loss: 2.037722279628118

Epoch: 5| Step: 1
Training loss: 1.9964596033096313
Validation loss: 2.0409579426050186

Epoch: 5| Step: 2
Training loss: 2.534444570541382
Validation loss: 2.0574979037046432

Epoch: 5| Step: 3
Training loss: 1.9717594385147095
Validation loss: 2.052803119023641

Epoch: 5| Step: 4
Training loss: 1.8680779933929443
Validation loss: 2.0505683720111847

Epoch: 5| Step: 5
Training loss: 2.042375326156616
Validation loss: 2.0584664940834045

Epoch: 5| Step: 6
Training loss: 1.7097724676132202
Validation loss: 2.054575890302658

Epoch: 5| Step: 7
Training loss: 1.6407413482666016
Validation loss: 2.0592588831981025

Epoch: 5| Step: 8
Training loss: 2.1036124229431152
Validation loss: 2.0545565684636435

Epoch: 5| Step: 9
Training loss: 2.35162353515625
Validation loss: 2.0658652931451797

Epoch: 5| Step: 10
Training loss: 2.033048391342163
Validation loss: 2.073293243845304

Epoch: 5| Step: 11
Training loss: 2.192884922027588
Validation loss: 2.0679185489813485

Epoch: 144| Step: 0
Training loss: 1.8614883422851562
Validation loss: 2.0664551903804145

Epoch: 5| Step: 1
Training loss: 1.9451210498809814
Validation loss: 2.070568710565567

Epoch: 5| Step: 2
Training loss: 1.3549470901489258
Validation loss: 2.07272307574749

Epoch: 5| Step: 3
Training loss: 1.9753742218017578
Validation loss: 2.0730552723010383

Epoch: 5| Step: 4
Training loss: 2.333466053009033
Validation loss: 2.0570188661416373

Epoch: 5| Step: 5
Training loss: 1.8896458148956299
Validation loss: 2.0610173791646957

Epoch: 5| Step: 6
Training loss: 2.4698290824890137
Validation loss: 2.058153197169304

Epoch: 5| Step: 7
Training loss: 1.923399567604065
Validation loss: 2.0621687968571982

Epoch: 5| Step: 8
Training loss: 2.182795286178589
Validation loss: 2.0567032198111215

Epoch: 5| Step: 9
Training loss: 2.096247911453247
Validation loss: 2.069739575187365

Epoch: 5| Step: 10
Training loss: 1.9911296367645264
Validation loss: 2.0694262782732644

Epoch: 5| Step: 11
Training loss: 3.2212085723876953
Validation loss: 2.0765096892913184

Epoch: 145| Step: 0
Training loss: 2.173109531402588
Validation loss: 2.07490407427152

Epoch: 5| Step: 1
Training loss: 2.1241226196289062
Validation loss: 2.0698831925789514

Epoch: 5| Step: 2
Training loss: 1.6403623819351196
Validation loss: 2.0568270782629647

Epoch: 5| Step: 3
Training loss: 2.2534117698669434
Validation loss: 2.048031578461329

Epoch: 5| Step: 4
Training loss: 2.043531894683838
Validation loss: 2.0487136989831924

Epoch: 5| Step: 5
Training loss: 2.8338370323181152
Validation loss: 2.0470582048098245

Epoch: 5| Step: 6
Training loss: 2.4562582969665527
Validation loss: 2.0428633242845535

Epoch: 5| Step: 7
Training loss: 1.7880637645721436
Validation loss: 2.043477992216746

Epoch: 5| Step: 8
Training loss: 1.8473079204559326
Validation loss: 2.052405004700025

Epoch: 5| Step: 9
Training loss: 1.411686897277832
Validation loss: 2.050450642903646

Epoch: 5| Step: 10
Training loss: 1.9147840738296509
Validation loss: 2.0578468640645347

Epoch: 5| Step: 11
Training loss: 2.135312795639038
Validation loss: 2.0504583716392517

Epoch: 146| Step: 0
Training loss: 1.6581720113754272
Validation loss: 2.050610194603602

Epoch: 5| Step: 1
Training loss: 2.1238315105438232
Validation loss: 2.0504203140735626

Epoch: 5| Step: 2
Training loss: 2.2284984588623047
Validation loss: 2.0650868862867355

Epoch: 5| Step: 3
Training loss: 2.132056474685669
Validation loss: 2.0662443240483603

Epoch: 5| Step: 4
Training loss: 1.86812424659729
Validation loss: 2.0636817862590155

Epoch: 5| Step: 5
Training loss: 1.9499822854995728
Validation loss: 2.060258761048317

Epoch: 5| Step: 6
Training loss: 2.5345065593719482
Validation loss: 2.0649170875549316

Epoch: 5| Step: 7
Training loss: 1.991675615310669
Validation loss: 2.07852666079998

Epoch: 5| Step: 8
Training loss: 1.8961241245269775
Validation loss: 2.0764502783616385

Epoch: 5| Step: 9
Training loss: 1.789851427078247
Validation loss: 2.083582649628321

Epoch: 5| Step: 10
Training loss: 2.108156442642212
Validation loss: 2.0780468185742698

Epoch: 5| Step: 11
Training loss: 1.9834538698196411
Validation loss: 2.0742413699626923

Epoch: 147| Step: 0
Training loss: 2.0132415294647217
Validation loss: 2.0695762584606805

Epoch: 5| Step: 1
Training loss: 1.7985941171646118
Validation loss: 2.0694705148537955

Epoch: 5| Step: 2
Training loss: 2.2096288204193115
Validation loss: 2.0669759859641395

Epoch: 5| Step: 3
Training loss: 2.3041439056396484
Validation loss: 2.060051202774048

Epoch: 5| Step: 4
Training loss: 2.4057204723358154
Validation loss: 2.0654004514217377

Epoch: 5| Step: 5
Training loss: 2.0611419677734375
Validation loss: 2.0601147214571633

Epoch: 5| Step: 6
Training loss: 1.7085949182510376
Validation loss: 2.056794434785843

Epoch: 5| Step: 7
Training loss: 1.7024167776107788
Validation loss: 2.0553651551405587

Epoch: 5| Step: 8
Training loss: 1.9411933422088623
Validation loss: 2.0588940431674323

Epoch: 5| Step: 9
Training loss: 2.154719829559326
Validation loss: 2.065726011991501

Epoch: 5| Step: 10
Training loss: 2.176260471343994
Validation loss: 2.0669254660606384

Epoch: 5| Step: 11
Training loss: 0.6385483741760254
Validation loss: 2.0648653159538903

Epoch: 148| Step: 0
Training loss: 2.2411978244781494
Validation loss: 2.064432293176651

Epoch: 5| Step: 1
Training loss: 1.6888316869735718
Validation loss: 2.070685942967733

Epoch: 5| Step: 2
Training loss: 2.2845914363861084
Validation loss: 2.0589738289515176

Epoch: 5| Step: 3
Training loss: 1.5702215433120728
Validation loss: 2.0610442707935968

Epoch: 5| Step: 4
Training loss: 2.122586488723755
Validation loss: 2.065914273262024

Epoch: 5| Step: 5
Training loss: 2.022085189819336
Validation loss: 2.059395968914032

Epoch: 5| Step: 6
Training loss: 2.470975875854492
Validation loss: 2.054574583967527

Epoch: 5| Step: 7
Training loss: 1.9462134838104248
Validation loss: 2.0571988622347512

Epoch: 5| Step: 8
Training loss: 2.1648828983306885
Validation loss: 2.051817297935486

Epoch: 5| Step: 9
Training loss: 2.1406023502349854
Validation loss: 2.048120617866516

Epoch: 5| Step: 10
Training loss: 1.5392084121704102
Validation loss: 2.044492398699125

Epoch: 5| Step: 11
Training loss: 2.3038995265960693
Validation loss: 2.039919594923655

Epoch: 149| Step: 0
Training loss: 2.023616075515747
Validation loss: 2.050239160656929

Epoch: 5| Step: 1
Training loss: 1.9460976123809814
Validation loss: 2.0352852096160254

Epoch: 5| Step: 2
Training loss: 2.2197327613830566
Validation loss: 2.042975281675657

Epoch: 5| Step: 3
Training loss: 2.0784101486206055
Validation loss: 2.040080656607946

Epoch: 5| Step: 4
Training loss: 1.8329203128814697
Validation loss: 2.0449189643065133

Epoch: 5| Step: 5
Training loss: 2.414971351623535
Validation loss: 2.044147416949272

Epoch: 5| Step: 6
Training loss: 2.204418897628784
Validation loss: 2.046765943368276

Epoch: 5| Step: 7
Training loss: 1.9517358541488647
Validation loss: 2.0482141574223838

Epoch: 5| Step: 8
Training loss: 1.8458807468414307
Validation loss: 2.052556206782659

Epoch: 5| Step: 9
Training loss: 2.084573745727539
Validation loss: 2.0523636490106583

Epoch: 5| Step: 10
Training loss: 1.8108479976654053
Validation loss: 2.0610982924699783

Epoch: 5| Step: 11
Training loss: 1.7671053409576416
Validation loss: 2.064204514026642

Epoch: 150| Step: 0
Training loss: 1.6866455078125
Validation loss: 2.066214303175608

Epoch: 5| Step: 1
Training loss: 2.3904662132263184
Validation loss: 2.0779117246468863

Epoch: 5| Step: 2
Training loss: 1.8028265237808228
Validation loss: 2.0753300239642463

Epoch: 5| Step: 3
Training loss: 2.228337526321411
Validation loss: 2.073381185531616

Epoch: 5| Step: 4
Training loss: 2.0684993267059326
Validation loss: 2.0688378860553107

Epoch: 5| Step: 5
Training loss: 2.529667615890503
Validation loss: 2.066356291373571

Epoch: 5| Step: 6
Training loss: 2.3162648677825928
Validation loss: 2.0578139821688333

Epoch: 5| Step: 7
Training loss: 1.9612882137298584
Validation loss: 2.0583618928988776

Epoch: 5| Step: 8
Training loss: 1.6719791889190674
Validation loss: 2.050350144505501

Epoch: 5| Step: 9
Training loss: 1.9070600271224976
Validation loss: 2.036324232816696

Epoch: 5| Step: 10
Training loss: 2.039382219314575
Validation loss: 2.038468380769094

Epoch: 5| Step: 11
Training loss: 1.9159373044967651
Validation loss: 2.0320464372634888

Epoch: 151| Step: 0
Training loss: 2.420320510864258
Validation loss: 2.027095764875412

Epoch: 5| Step: 1
Training loss: 2.057631731033325
Validation loss: 2.0392069717248282

Epoch: 5| Step: 2
Training loss: 2.0275304317474365
Validation loss: 2.0341263661781945

Epoch: 5| Step: 3
Training loss: 2.075434446334839
Validation loss: 2.0388669818639755

Epoch: 5| Step: 4
Training loss: 1.842681884765625
Validation loss: 2.0350000262260437

Epoch: 5| Step: 5
Training loss: 1.9444034099578857
Validation loss: 2.048284391562144

Epoch: 5| Step: 6
Training loss: 1.7780492305755615
Validation loss: 2.0429765035708747

Epoch: 5| Step: 7
Training loss: 2.414508104324341
Validation loss: 2.0474467178185782

Epoch: 5| Step: 8
Training loss: 2.394439697265625
Validation loss: 2.042970985174179

Epoch: 5| Step: 9
Training loss: 1.802804946899414
Validation loss: 2.051413282752037

Epoch: 5| Step: 10
Training loss: 2.046164035797119
Validation loss: 2.0367905100186667

Epoch: 5| Step: 11
Training loss: 3.622915267944336
Validation loss: 2.0305997331937156

Epoch: 152| Step: 0
Training loss: 1.7307517528533936
Validation loss: 2.0413117657105126

Epoch: 5| Step: 1
Training loss: 2.5823287963867188
Validation loss: 2.0340790897607803

Epoch: 5| Step: 2
Training loss: 2.2047476768493652
Validation loss: 2.0327290495236716

Epoch: 5| Step: 3
Training loss: 2.1568939685821533
Validation loss: 2.0422457605600357

Epoch: 5| Step: 4
Training loss: 1.9396278858184814
Validation loss: 2.0591271072626114

Epoch: 5| Step: 5
Training loss: 1.8967626094818115
Validation loss: 2.0594666252533593

Epoch: 5| Step: 6
Training loss: 2.8319191932678223
Validation loss: 2.0616271247466407

Epoch: 5| Step: 7
Training loss: 1.8963031768798828
Validation loss: 2.0741592595974603

Epoch: 5| Step: 8
Training loss: 1.7453296184539795
Validation loss: 2.071049372355143

Epoch: 5| Step: 9
Training loss: 1.828809142112732
Validation loss: 2.0576411187648773

Epoch: 5| Step: 10
Training loss: 1.8236091136932373
Validation loss: 2.065752704938253

Epoch: 5| Step: 11
Training loss: 1.7496886253356934
Validation loss: 2.0587403376897178

Epoch: 153| Step: 0
Training loss: 2.1799862384796143
Validation loss: 2.0508656253417334

Epoch: 5| Step: 1
Training loss: 1.9441295862197876
Validation loss: 2.0619724790255227

Epoch: 5| Step: 2
Training loss: 1.8684848546981812
Validation loss: 2.0410554508368173

Epoch: 5| Step: 3
Training loss: 1.7704622745513916
Validation loss: 2.050431326031685

Epoch: 5| Step: 4
Training loss: 2.266505718231201
Validation loss: 2.0444480826457343

Epoch: 5| Step: 5
Training loss: 1.9482524394989014
Validation loss: 2.028876776496569

Epoch: 5| Step: 6
Training loss: 2.497373104095459
Validation loss: 2.0468912422657013

Epoch: 5| Step: 7
Training loss: 2.0045294761657715
Validation loss: 2.0374711453914642

Epoch: 5| Step: 8
Training loss: 2.228516101837158
Validation loss: 2.0320111960172653

Epoch: 5| Step: 9
Training loss: 2.0278279781341553
Validation loss: 2.0344306031862893

Epoch: 5| Step: 10
Training loss: 1.7074416875839233
Validation loss: 2.0394089172283807

Epoch: 5| Step: 11
Training loss: 1.4306327104568481
Validation loss: 2.034515827894211

Epoch: 154| Step: 0
Training loss: 1.747169852256775
Validation loss: 2.036170542240143

Epoch: 5| Step: 1
Training loss: 2.4532310962677
Validation loss: 2.043521672487259

Epoch: 5| Step: 2
Training loss: 2.0828042030334473
Validation loss: 2.0354550182819366

Epoch: 5| Step: 3
Training loss: 1.9568593502044678
Validation loss: 2.0320274283488593

Epoch: 5| Step: 4
Training loss: 1.7510006427764893
Validation loss: 2.045746540029844

Epoch: 5| Step: 5
Training loss: 2.2315852642059326
Validation loss: 2.051039273540179

Epoch: 5| Step: 6
Training loss: 1.9386059045791626
Validation loss: 2.048468073209127

Epoch: 5| Step: 7
Training loss: 1.7305749654769897
Validation loss: 2.054430757959684

Epoch: 5| Step: 8
Training loss: 2.2841410636901855
Validation loss: 2.063974012931188

Epoch: 5| Step: 9
Training loss: 2.1917026042938232
Validation loss: 2.059563472867012

Epoch: 5| Step: 10
Training loss: 1.8575842380523682
Validation loss: 2.065865029891332

Epoch: 5| Step: 11
Training loss: 2.272026538848877
Validation loss: 2.0615655382474265

Epoch: 155| Step: 0
Training loss: 2.0863969326019287
Validation loss: 2.0512278974056244

Epoch: 5| Step: 1
Training loss: 1.6997642517089844
Validation loss: 2.0604924460252128

Epoch: 5| Step: 2
Training loss: 2.0663037300109863
Validation loss: 2.0633464803298316

Epoch: 5| Step: 3
Training loss: 2.2594292163848877
Validation loss: 2.0461506644884744

Epoch: 5| Step: 4
Training loss: 1.9559478759765625
Validation loss: 2.0454880793889365

Epoch: 5| Step: 5
Training loss: 1.5388233661651611
Validation loss: 2.0551809867223105

Epoch: 5| Step: 6
Training loss: 1.5180975198745728
Validation loss: 2.0429653972387314

Epoch: 5| Step: 7
Training loss: 2.0964584350585938
Validation loss: 2.0456875214974084

Epoch: 5| Step: 8
Training loss: 2.3852121829986572
Validation loss: 2.0590593367815018

Epoch: 5| Step: 9
Training loss: 1.9573049545288086
Validation loss: 2.0623151610294976

Epoch: 5| Step: 10
Training loss: 2.52217173576355
Validation loss: 2.0592034508784614

Epoch: 5| Step: 11
Training loss: 2.2528798580169678
Validation loss: 2.0509641667207084

Epoch: 156| Step: 0
Training loss: 2.033691167831421
Validation loss: 2.061605061093966

Epoch: 5| Step: 1
Training loss: 2.071645498275757
Validation loss: 2.0658007065455117

Epoch: 5| Step: 2
Training loss: 2.2725777626037598
Validation loss: 2.0629798769950867

Epoch: 5| Step: 3
Training loss: 2.569293975830078
Validation loss: 2.0636742363373437

Epoch: 5| Step: 4
Training loss: 1.9184986352920532
Validation loss: 2.0592375149329505

Epoch: 5| Step: 5
Training loss: 1.9599628448486328
Validation loss: 2.0591821124156318

Epoch: 5| Step: 6
Training loss: 1.2633216381072998
Validation loss: 2.0615729689598083

Epoch: 5| Step: 7
Training loss: 1.5656683444976807
Validation loss: 2.0695396214723587

Epoch: 5| Step: 8
Training loss: 2.138622283935547
Validation loss: 2.0707411666711173

Epoch: 5| Step: 9
Training loss: 2.1720314025878906
Validation loss: 2.070697103937467

Epoch: 5| Step: 10
Training loss: 1.827584981918335
Validation loss: 2.078279569745064

Epoch: 5| Step: 11
Training loss: 2.7687809467315674
Validation loss: 2.080805480480194

Epoch: 157| Step: 0
Training loss: 2.114480495452881
Validation loss: 2.080970068772634

Epoch: 5| Step: 1
Training loss: 2.2700562477111816
Validation loss: 2.0850776731967926

Epoch: 5| Step: 2
Training loss: 1.3712689876556396
Validation loss: 2.0876507262388864

Epoch: 5| Step: 3
Training loss: 1.9414074420928955
Validation loss: 2.086440920829773

Epoch: 5| Step: 4
Training loss: 1.7435109615325928
Validation loss: 2.089066689213117

Epoch: 5| Step: 5
Training loss: 1.9928321838378906
Validation loss: 2.0821349124113717

Epoch: 5| Step: 6
Training loss: 1.8103954792022705
Validation loss: 2.0757760107517242

Epoch: 5| Step: 7
Training loss: 2.5071284770965576
Validation loss: 2.0644406179587045

Epoch: 5| Step: 8
Training loss: 2.354454517364502
Validation loss: 2.0644410451253257

Epoch: 5| Step: 9
Training loss: 1.9823068380355835
Validation loss: 2.076761489113172

Epoch: 5| Step: 10
Training loss: 1.9224460124969482
Validation loss: 2.07730495929718

Epoch: 5| Step: 11
Training loss: 2.2604103088378906
Validation loss: 2.074831093351046

Epoch: 158| Step: 0
Training loss: 1.7497632503509521
Validation loss: 2.07910684744517

Epoch: 5| Step: 1
Training loss: 1.9519622325897217
Validation loss: 2.071156750122706

Epoch: 5| Step: 2
Training loss: 2.1545588970184326
Validation loss: 2.061068872610728

Epoch: 5| Step: 3
Training loss: 2.4085631370544434
Validation loss: 2.08357897400856

Epoch: 5| Step: 4
Training loss: 2.048837661743164
Validation loss: 2.0822461048762

Epoch: 5| Step: 5
Training loss: 2.425203800201416
Validation loss: 2.0746106753746667

Epoch: 5| Step: 6
Training loss: 2.107780933380127
Validation loss: 2.0737661818663278

Epoch: 5| Step: 7
Training loss: 1.812363862991333
Validation loss: 2.0797309080759683

Epoch: 5| Step: 8
Training loss: 1.6796913146972656
Validation loss: 2.0834754010041556

Epoch: 5| Step: 9
Training loss: 1.620250940322876
Validation loss: 2.08691214521726

Epoch: 5| Step: 10
Training loss: 1.8792177438735962
Validation loss: 2.0734778394301734

Epoch: 5| Step: 11
Training loss: 2.257181406021118
Validation loss: 2.08375591536363

Epoch: 159| Step: 0
Training loss: 2.1325430870056152
Validation loss: 2.091393123070399

Epoch: 5| Step: 1
Training loss: 1.7032111883163452
Validation loss: 2.084165299932162

Epoch: 5| Step: 2
Training loss: 2.191237449645996
Validation loss: 2.086631158987681

Epoch: 5| Step: 3
Training loss: 1.8224265575408936
Validation loss: 2.0856596579154334

Epoch: 5| Step: 4
Training loss: 1.9400837421417236
Validation loss: 2.0876491169134774

Epoch: 5| Step: 5
Training loss: 2.311844825744629
Validation loss: 2.081415976087252

Epoch: 5| Step: 6
Training loss: 2.163681745529175
Validation loss: 2.086410795648893

Epoch: 5| Step: 7
Training loss: 2.334132432937622
Validation loss: 2.078912913799286

Epoch: 5| Step: 8
Training loss: 1.5702698230743408
Validation loss: 2.0765441407759986

Epoch: 5| Step: 9
Training loss: 1.8195339441299438
Validation loss: 2.062011167407036

Epoch: 5| Step: 10
Training loss: 2.0186848640441895
Validation loss: 2.0550980418920517

Epoch: 5| Step: 11
Training loss: 1.9613780975341797
Validation loss: 2.0479066322247186

Epoch: 160| Step: 0
Training loss: 1.6952741146087646
Validation loss: 2.046204760670662

Epoch: 5| Step: 1
Training loss: 1.9989255666732788
Validation loss: 2.0482667684555054

Epoch: 5| Step: 2
Training loss: 2.037370443344116
Validation loss: 2.050798714160919

Epoch: 5| Step: 3
Training loss: 2.1217310428619385
Validation loss: 2.03854272266229

Epoch: 5| Step: 4
Training loss: 2.0730409622192383
Validation loss: 2.048070246974627

Epoch: 5| Step: 5
Training loss: 1.6621240377426147
Validation loss: 2.0529912412166595

Epoch: 5| Step: 6
Training loss: 2.1722235679626465
Validation loss: 2.060225715239843

Epoch: 5| Step: 7
Training loss: 2.4025321006774902
Validation loss: 2.0709908455610275

Epoch: 5| Step: 8
Training loss: 1.6813433170318604
Validation loss: 2.0731679697831473

Epoch: 5| Step: 9
Training loss: 2.41863751411438
Validation loss: 2.0847638895114264

Epoch: 5| Step: 10
Training loss: 2.356322765350342
Validation loss: 2.0846889863411584

Epoch: 5| Step: 11
Training loss: 1.5269533395767212
Validation loss: 2.0893896172444024

Epoch: 161| Step: 0
Training loss: 2.1089041233062744
Validation loss: 2.0854389468828836

Epoch: 5| Step: 1
Training loss: 2.261366128921509
Validation loss: 2.074519177277883

Epoch: 5| Step: 2
Training loss: 1.3057785034179688
Validation loss: 2.0659113128980002

Epoch: 5| Step: 3
Training loss: 1.7409255504608154
Validation loss: 2.086411734422048

Epoch: 5| Step: 4
Training loss: 1.6808745861053467
Validation loss: 2.0839901715517044

Epoch: 5| Step: 5
Training loss: 2.7830986976623535
Validation loss: 2.060021534562111

Epoch: 5| Step: 6
Training loss: 2.0183825492858887
Validation loss: 2.0653193990389505

Epoch: 5| Step: 7
Training loss: 1.5324004888534546
Validation loss: 2.078247904777527

Epoch: 5| Step: 8
Training loss: 2.2049145698547363
Validation loss: 2.0724832067886987

Epoch: 5| Step: 9
Training loss: 2.5254616737365723
Validation loss: 2.0688754518826804

Epoch: 5| Step: 10
Training loss: 2.023566484451294
Validation loss: 2.0651428351799646

Epoch: 5| Step: 11
Training loss: 1.2609180212020874
Validation loss: 2.0561577727397284

Epoch: 162| Step: 0
Training loss: 2.0206596851348877
Validation loss: 2.0569649090369544

Epoch: 5| Step: 1
Training loss: 2.077636957168579
Validation loss: 2.0534005165100098

Epoch: 5| Step: 2
Training loss: 1.744389295578003
Validation loss: 2.0652750929196677

Epoch: 5| Step: 3
Training loss: 2.4083335399627686
Validation loss: 2.059689477086067

Epoch: 5| Step: 4
Training loss: 1.666590929031372
Validation loss: 2.058234473069509

Epoch: 5| Step: 5
Training loss: 2.049720287322998
Validation loss: 2.0649743427832923

Epoch: 5| Step: 6
Training loss: 1.437511682510376
Validation loss: 2.0701341132322946

Epoch: 5| Step: 7
Training loss: 2.1143786907196045
Validation loss: 2.0768909752368927

Epoch: 5| Step: 8
Training loss: 2.527073621749878
Validation loss: 2.076662873228391

Epoch: 5| Step: 9
Training loss: 2.1048741340637207
Validation loss: 2.0898844649394355

Epoch: 5| Step: 10
Training loss: 2.07663893699646
Validation loss: 2.0999259700377784

Epoch: 5| Step: 11
Training loss: 0.9795149564743042
Validation loss: 2.0905812084674835

Epoch: 163| Step: 0
Training loss: 1.8142821788787842
Validation loss: 2.0846296598513923

Epoch: 5| Step: 1
Training loss: 1.8115301132202148
Validation loss: 2.077406585216522

Epoch: 5| Step: 2
Training loss: 2.596318006515503
Validation loss: 2.0798602203528085

Epoch: 5| Step: 3
Training loss: 1.8057372570037842
Validation loss: 2.0748535891373954

Epoch: 5| Step: 4
Training loss: 1.9353443384170532
Validation loss: 2.080000345905622

Epoch: 5| Step: 5
Training loss: 1.8081245422363281
Validation loss: 2.0706293682257333

Epoch: 5| Step: 6
Training loss: 2.7546870708465576
Validation loss: 2.0754890044530234

Epoch: 5| Step: 7
Training loss: 1.834041953086853
Validation loss: 2.0706601987282434

Epoch: 5| Step: 8
Training loss: 1.579988718032837
Validation loss: 2.0622022847334542

Epoch: 5| Step: 9
Training loss: 1.8482158184051514
Validation loss: 2.0677850047747293

Epoch: 5| Step: 10
Training loss: 2.2275454998016357
Validation loss: 2.0683526694774628

Epoch: 5| Step: 11
Training loss: 1.926601767539978
Validation loss: 2.0810544143120446

Epoch: 164| Step: 0
Training loss: 1.5734968185424805
Validation loss: 2.072698339819908

Epoch: 5| Step: 1
Training loss: 1.9144203662872314
Validation loss: 2.078718274831772

Epoch: 5| Step: 2
Training loss: 2.349729061126709
Validation loss: 2.0755485395590463

Epoch: 5| Step: 3
Training loss: 2.234934091567993
Validation loss: 2.0937346865733466

Epoch: 5| Step: 4
Training loss: 2.124453067779541
Validation loss: 2.0845923175414405

Epoch: 5| Step: 5
Training loss: 1.884312391281128
Validation loss: 2.092837338646253

Epoch: 5| Step: 6
Training loss: 2.0643696784973145
Validation loss: 2.0910278409719467

Epoch: 5| Step: 7
Training loss: 1.5839251279830933
Validation loss: 2.097129538655281

Epoch: 5| Step: 8
Training loss: 2.063668727874756
Validation loss: 2.069629887739817

Epoch: 5| Step: 9
Training loss: 1.8241245746612549
Validation loss: 2.0831952542066574

Epoch: 5| Step: 10
Training loss: 2.382521629333496
Validation loss: 2.083771139383316

Epoch: 5| Step: 11
Training loss: 1.508447527885437
Validation loss: 2.0750918140014014

Epoch: 165| Step: 0
Training loss: 1.6451066732406616
Validation loss: 2.0700490921735764

Epoch: 5| Step: 1
Training loss: 2.1689374446868896
Validation loss: 2.060257002711296

Epoch: 5| Step: 2
Training loss: 2.455153703689575
Validation loss: 2.0672211150328317

Epoch: 5| Step: 3
Training loss: 2.185978651046753
Validation loss: 2.057085474332174

Epoch: 5| Step: 4
Training loss: 1.896376609802246
Validation loss: 2.05576054751873

Epoch: 5| Step: 5
Training loss: 2.1044068336486816
Validation loss: 2.053610255320867

Epoch: 5| Step: 6
Training loss: 2.323122024536133
Validation loss: 2.0551309287548065

Epoch: 5| Step: 7
Training loss: 1.7993084192276
Validation loss: 2.0666220486164093

Epoch: 5| Step: 8
Training loss: 1.6783374547958374
Validation loss: 2.065103049079577

Epoch: 5| Step: 9
Training loss: 1.8701145648956299
Validation loss: 2.082686717311541

Epoch: 5| Step: 10
Training loss: 2.1386990547180176
Validation loss: 2.0813763042291007

Epoch: 5| Step: 11
Training loss: 1.0990324020385742
Validation loss: 2.074399918317795

Epoch: 166| Step: 0
Training loss: 1.7178795337677002
Validation loss: 2.0900699297587075

Epoch: 5| Step: 1
Training loss: 1.8747968673706055
Validation loss: 2.096717362602552

Epoch: 5| Step: 2
Training loss: 2.059224843978882
Validation loss: 2.1038334667682648

Epoch: 5| Step: 3
Training loss: 2.252562999725342
Validation loss: 2.0945626894632974

Epoch: 5| Step: 4
Training loss: 2.1377339363098145
Validation loss: 2.0841134786605835

Epoch: 5| Step: 5
Training loss: 2.1534345149993896
Validation loss: 2.0842803021272025

Epoch: 5| Step: 6
Training loss: 2.024998426437378
Validation loss: 2.0873490472634635

Epoch: 5| Step: 7
Training loss: 1.5879007577896118
Validation loss: 2.069515953461329

Epoch: 5| Step: 8
Training loss: 1.954159140586853
Validation loss: 2.0815509458382926

Epoch: 5| Step: 9
Training loss: 1.8647514581680298
Validation loss: 2.070544128616651

Epoch: 5| Step: 10
Training loss: 1.952414870262146
Validation loss: 2.0915620078643165

Epoch: 5| Step: 11
Training loss: 3.0949318408966064
Validation loss: 2.0997227231661477

Epoch: 167| Step: 0
Training loss: 1.5137519836425781
Validation loss: 2.079446623722712

Epoch: 5| Step: 1
Training loss: 1.3627961874008179
Validation loss: 2.0933707555135093

Epoch: 5| Step: 2
Training loss: 2.029325485229492
Validation loss: 2.101626599828402

Epoch: 5| Step: 3
Training loss: 1.8687684535980225
Validation loss: 2.1157876004775367

Epoch: 5| Step: 4
Training loss: 2.887272357940674
Validation loss: 2.103883594274521

Epoch: 5| Step: 5
Training loss: 1.9036153554916382
Validation loss: 2.1085733671983085

Epoch: 5| Step: 6
Training loss: 2.2328667640686035
Validation loss: 2.102756236990293

Epoch: 5| Step: 7
Training loss: 2.311602830886841
Validation loss: 2.092797815799713

Epoch: 5| Step: 8
Training loss: 1.5955567359924316
Validation loss: 2.090138385693232

Epoch: 5| Step: 9
Training loss: 2.1330342292785645
Validation loss: 2.079027384519577

Epoch: 5| Step: 10
Training loss: 1.9779236316680908
Validation loss: 2.075261985262235

Epoch: 5| Step: 11
Training loss: 2.8225462436676025
Validation loss: 2.0730173786481223

Epoch: 168| Step: 0
Training loss: 1.6954740285873413
Validation loss: 2.0810745606819787

Epoch: 5| Step: 1
Training loss: 1.787940263748169
Validation loss: 2.077168812354406

Epoch: 5| Step: 2
Training loss: 2.387263536453247
Validation loss: 2.0684666534264884

Epoch: 5| Step: 3
Training loss: 2.4634275436401367
Validation loss: 2.0734359274307885

Epoch: 5| Step: 4
Training loss: 2.14461088180542
Validation loss: 2.0585077702999115

Epoch: 5| Step: 5
Training loss: 1.993920087814331
Validation loss: 2.0680909156799316

Epoch: 5| Step: 6
Training loss: 1.4641480445861816
Validation loss: 2.085634013017019

Epoch: 5| Step: 7
Training loss: 2.4601199626922607
Validation loss: 2.08653295536836

Epoch: 5| Step: 8
Training loss: 1.7504535913467407
Validation loss: 2.076673462986946

Epoch: 5| Step: 9
Training loss: 1.954159140586853
Validation loss: 2.112285633881887

Epoch: 5| Step: 10
Training loss: 2.0255062580108643
Validation loss: 2.1063919762770333

Epoch: 5| Step: 11
Training loss: 2.544632911682129
Validation loss: 2.1085920681556067

Epoch: 169| Step: 0
Training loss: 1.8399174213409424
Validation loss: 2.095093766848246

Epoch: 5| Step: 1
Training loss: 1.4866889715194702
Validation loss: 2.0917640974124274

Epoch: 5| Step: 2
Training loss: 2.1708734035491943
Validation loss: 2.086238443851471

Epoch: 5| Step: 3
Training loss: 2.417750120162964
Validation loss: 2.0681475549936295

Epoch: 5| Step: 4
Training loss: 2.374011754989624
Validation loss: 2.0659679720799127

Epoch: 5| Step: 5
Training loss: 1.6444000005722046
Validation loss: 2.066391482949257

Epoch: 5| Step: 6
Training loss: 1.9725414514541626
Validation loss: 2.057630881667137

Epoch: 5| Step: 7
Training loss: 2.160871982574463
Validation loss: 2.0677257676919303

Epoch: 5| Step: 8
Training loss: 2.4768567085266113
Validation loss: 2.0620327989260354

Epoch: 5| Step: 9
Training loss: 1.4272465705871582
Validation loss: 2.0703783382972083

Epoch: 5| Step: 10
Training loss: 2.129345417022705
Validation loss: 2.0653163293997445

Epoch: 5| Step: 11
Training loss: 1.7106657028198242
Validation loss: 2.0740505258242288

Epoch: 170| Step: 0
Training loss: 2.3830127716064453
Validation loss: 2.073385407527288

Epoch: 5| Step: 1
Training loss: 1.7530628442764282
Validation loss: 2.076546003421148

Epoch: 5| Step: 2
Training loss: 2.04742693901062
Validation loss: 2.0638607889413834

Epoch: 5| Step: 3
Training loss: 1.8354326486587524
Validation loss: 2.079570010304451

Epoch: 5| Step: 4
Training loss: 1.7376213073730469
Validation loss: 2.0730621417363486

Epoch: 5| Step: 5
Training loss: 2.351212978363037
Validation loss: 2.086196944117546

Epoch: 5| Step: 6
Training loss: 2.107027769088745
Validation loss: 2.0756371368964515

Epoch: 5| Step: 7
Training loss: 1.643564224243164
Validation loss: 2.086191182335218

Epoch: 5| Step: 8
Training loss: 2.328800678253174
Validation loss: 2.094113846619924

Epoch: 5| Step: 9
Training loss: 2.0324835777282715
Validation loss: 2.0718648384014764

Epoch: 5| Step: 10
Training loss: 1.7302805185317993
Validation loss: 2.0810041228930154

Epoch: 5| Step: 11
Training loss: 1.606741189956665
Validation loss: 2.0945340593655906

Epoch: 171| Step: 0
Training loss: 1.7805343866348267
Validation loss: 2.090295751889547

Epoch: 5| Step: 1
Training loss: 2.6950347423553467
Validation loss: 2.073361242810885

Epoch: 5| Step: 2
Training loss: 1.9215492010116577
Validation loss: 2.077096497019132

Epoch: 5| Step: 3
Training loss: 2.12127947807312
Validation loss: 2.0692755033572516

Epoch: 5| Step: 4
Training loss: 2.0745537281036377
Validation loss: 2.0699357042709985

Epoch: 5| Step: 5
Training loss: 1.5243732929229736
Validation loss: 2.0665225187937417

Epoch: 5| Step: 6
Training loss: 1.875440239906311
Validation loss: 2.0683603286743164

Epoch: 5| Step: 7
Training loss: 2.1745238304138184
Validation loss: 2.0671426902214685

Epoch: 5| Step: 8
Training loss: 1.9579966068267822
Validation loss: 2.0722486774126687

Epoch: 5| Step: 9
Training loss: 2.0820188522338867
Validation loss: 2.0823512077331543

Epoch: 5| Step: 10
Training loss: 1.9785175323486328
Validation loss: 2.0700457046429315

Epoch: 5| Step: 11
Training loss: 1.3653407096862793
Validation loss: 2.071230153242747

Epoch: 172| Step: 0
Training loss: 2.0063729286193848
Validation loss: 2.0833916664123535

Epoch: 5| Step: 1
Training loss: 1.6196467876434326
Validation loss: 2.0960543702046075

Epoch: 5| Step: 2
Training loss: 2.539353847503662
Validation loss: 2.1017217934131622

Epoch: 5| Step: 3
Training loss: 2.2771191596984863
Validation loss: 2.0731337815523148

Epoch: 5| Step: 4
Training loss: 1.9606050252914429
Validation loss: 2.088082184394201

Epoch: 5| Step: 5
Training loss: 2.5482587814331055
Validation loss: 2.087141623099645

Epoch: 5| Step: 6
Training loss: 2.126115560531616
Validation loss: 2.077657992641131

Epoch: 5| Step: 7
Training loss: 1.6204617023468018
Validation loss: 2.0745009581247964

Epoch: 5| Step: 8
Training loss: 2.100449800491333
Validation loss: 2.078640267252922

Epoch: 5| Step: 9
Training loss: 1.474164605140686
Validation loss: 2.0734210362037024

Epoch: 5| Step: 10
Training loss: 1.9308903217315674
Validation loss: 2.071416144569715

Epoch: 5| Step: 11
Training loss: 1.8699356317520142
Validation loss: 2.0773196617762246

Epoch: 173| Step: 0
Training loss: 2.0632925033569336
Validation loss: 2.076511412858963

Epoch: 5| Step: 1
Training loss: 2.2293617725372314
Validation loss: 2.0930395076672235

Epoch: 5| Step: 2
Training loss: 1.54973304271698
Validation loss: 2.088475992282232

Epoch: 5| Step: 3
Training loss: 1.8890399932861328
Validation loss: 2.0979930063088736

Epoch: 5| Step: 4
Training loss: 1.4239195585250854
Validation loss: 2.1145392060279846

Epoch: 5| Step: 5
Training loss: 1.9037694931030273
Validation loss: 2.099961201349894

Epoch: 5| Step: 6
Training loss: 1.9064353704452515
Validation loss: 2.096402352054914

Epoch: 5| Step: 7
Training loss: 2.492832899093628
Validation loss: 2.1090295612812042

Epoch: 5| Step: 8
Training loss: 2.481982469558716
Validation loss: 2.095770632227262

Epoch: 5| Step: 9
Training loss: 1.9635131359100342
Validation loss: 2.094239686926206

Epoch: 5| Step: 10
Training loss: 2.1796584129333496
Validation loss: 2.0774305562178292

Epoch: 5| Step: 11
Training loss: 1.8761873245239258
Validation loss: 2.080868457754453

Epoch: 174| Step: 0
Training loss: 1.4990745782852173
Validation loss: 2.072752704222997

Epoch: 5| Step: 1
Training loss: 1.5797168016433716
Validation loss: 2.068214699625969

Epoch: 5| Step: 2
Training loss: 1.963068962097168
Validation loss: 2.072961380084356

Epoch: 5| Step: 3
Training loss: 2.1155619621276855
Validation loss: 2.0705673744281134

Epoch: 5| Step: 4
Training loss: 2.0997562408447266
Validation loss: 2.0768473148345947

Epoch: 5| Step: 5
Training loss: 1.8771541118621826
Validation loss: 2.0775148272514343

Epoch: 5| Step: 6
Training loss: 2.2312583923339844
Validation loss: 2.0751372426748276

Epoch: 5| Step: 7
Training loss: 2.1699554920196533
Validation loss: 2.0737705131371817

Epoch: 5| Step: 8
Training loss: 2.0188746452331543
Validation loss: 2.079556037982305

Epoch: 5| Step: 9
Training loss: 2.1960482597351074
Validation loss: 2.076778029402097

Epoch: 5| Step: 10
Training loss: 2.2218854427337646
Validation loss: 2.0765971541404724

Epoch: 5| Step: 11
Training loss: 1.9539475440979004
Validation loss: 2.0933089703321457

Epoch: 175| Step: 0
Training loss: 1.7051210403442383
Validation loss: 2.0951762745777764

Epoch: 5| Step: 1
Training loss: 1.7055127620697021
Validation loss: 2.0832176903883615

Epoch: 5| Step: 2
Training loss: 1.9580962657928467
Validation loss: 2.092496986190478

Epoch: 5| Step: 3
Training loss: 1.7739343643188477
Validation loss: 2.096830422679583

Epoch: 5| Step: 4
Training loss: 2.067919969558716
Validation loss: 2.1004838099082312

Epoch: 5| Step: 5
Training loss: 1.847812294960022
Validation loss: 2.0970270981391272

Epoch: 5| Step: 6
Training loss: 1.8658157587051392
Validation loss: 2.091242382923762

Epoch: 5| Step: 7
Training loss: 2.1712865829467773
Validation loss: 2.09902116159598

Epoch: 5| Step: 8
Training loss: 2.0516059398651123
Validation loss: 2.1016777604818344

Epoch: 5| Step: 9
Training loss: 2.1422245502471924
Validation loss: 2.0947323044141135

Epoch: 5| Step: 10
Training loss: 2.115940809249878
Validation loss: 2.101182426015536

Epoch: 5| Step: 11
Training loss: 2.6525516510009766
Validation loss: 2.0785920321941376

Testing loss: 1.7204271803656928

Epoch: 1| Step: 0
Training loss: 4.894736289978027
Validation loss: 5.29997197786967

Epoch: 6| Step: 1
Training loss: 5.280636787414551
Validation loss: 5.298313458760579

Epoch: 6| Step: 2
Training loss: 6.635229110717773
Validation loss: 5.296546300252278

Epoch: 6| Step: 3
Training loss: 5.808187484741211
Validation loss: 5.29478391011556

Epoch: 6| Step: 4
Training loss: 5.587390899658203
Validation loss: 5.293108940124512

Epoch: 6| Step: 5
Training loss: 5.623379707336426
Validation loss: 5.291211525599162

Epoch: 6| Step: 6
Training loss: 3.7599732875823975
Validation loss: 5.289410750071208

Epoch: 6| Step: 7
Training loss: 5.632891654968262
Validation loss: 5.28756841023763

Epoch: 6| Step: 8
Training loss: 5.443301200866699
Validation loss: 5.285677433013916

Epoch: 6| Step: 9
Training loss: 5.470741271972656
Validation loss: 5.283669551213582

Epoch: 6| Step: 10
Training loss: 4.314167499542236
Validation loss: 5.281678120295207

Epoch: 6| Step: 11
Training loss: 5.7038960456848145
Validation loss: 5.279610872268677

Epoch: 6| Step: 12
Training loss: 5.14840030670166
Validation loss: 5.277467330296834

Epoch: 6| Step: 13
Training loss: 5.697373867034912
Validation loss: 5.275222698847453

Epoch: 2| Step: 0
Training loss: 6.289076805114746
Validation loss: 5.273008743921916

Epoch: 6| Step: 1
Training loss: 4.910915851593018
Validation loss: 5.2704916795094805

Epoch: 6| Step: 2
Training loss: 5.700387954711914
Validation loss: 5.2680323123931885

Epoch: 6| Step: 3
Training loss: 4.157953262329102
Validation loss: 5.265355666478475

Epoch: 6| Step: 4
Training loss: 4.1506218910217285
Validation loss: 5.262629350026448

Epoch: 6| Step: 5
Training loss: 4.602920055389404
Validation loss: 5.259684165318807

Epoch: 6| Step: 6
Training loss: 5.505796909332275
Validation loss: 5.2565750281016035

Epoch: 6| Step: 7
Training loss: 6.216817378997803
Validation loss: 5.253457864125569

Epoch: 6| Step: 8
Training loss: 5.755575180053711
Validation loss: 5.250097354253133

Epoch: 6| Step: 9
Training loss: 4.700722694396973
Validation loss: 5.246430238087972

Epoch: 6| Step: 10
Training loss: 5.577944755554199
Validation loss: 5.242760896682739

Epoch: 6| Step: 11
Training loss: 4.537723064422607
Validation loss: 5.238862673441569

Epoch: 6| Step: 12
Training loss: 7.083009719848633
Validation loss: 5.234636863072713

Epoch: 6| Step: 13
Training loss: 5.337065696716309
Validation loss: 5.230204900105794

Epoch: 3| Step: 0
Training loss: 5.389074325561523
Validation loss: 5.225832223892212

Epoch: 6| Step: 1
Training loss: 4.863157272338867
Validation loss: 5.220864613850911

Epoch: 6| Step: 2
Training loss: 5.189795970916748
Validation loss: 5.215886116027832

Epoch: 6| Step: 3
Training loss: 4.718093395233154
Validation loss: 5.210399309794108

Epoch: 6| Step: 4
Training loss: 4.362078666687012
Validation loss: 5.204966068267822

Epoch: 6| Step: 5
Training loss: 4.857590675354004
Validation loss: 5.199063221613566

Epoch: 6| Step: 6
Training loss: 4.859072685241699
Validation loss: 5.192978938420613

Epoch: 6| Step: 7
Training loss: 6.157017707824707
Validation loss: 5.186424970626831

Epoch: 6| Step: 8
Training loss: 5.422389030456543
Validation loss: 5.179603656133016

Epoch: 6| Step: 9
Training loss: 5.729160308837891
Validation loss: 5.172699848810832

Epoch: 6| Step: 10
Training loss: 4.576876640319824
Validation loss: 5.164968093236287

Epoch: 6| Step: 11
Training loss: 5.554836273193359
Validation loss: 5.157295227050781

Epoch: 6| Step: 12
Training loss: 5.236311912536621
Validation loss: 5.1489185492197675

Epoch: 6| Step: 13
Training loss: 6.677022933959961
Validation loss: 5.140252987543742

Epoch: 4| Step: 0
Training loss: 6.120786666870117
Validation loss: 5.1313661734263105

Epoch: 6| Step: 1
Training loss: 5.763652801513672
Validation loss: 5.122045238812764

Epoch: 6| Step: 2
Training loss: 5.490848541259766
Validation loss: 5.1123143037160235

Epoch: 6| Step: 3
Training loss: 5.561680316925049
Validation loss: 5.1024595101674395

Epoch: 6| Step: 4
Training loss: 4.499784469604492
Validation loss: 5.09188175201416

Epoch: 6| Step: 5
Training loss: 5.387284755706787
Validation loss: 5.081628640492757

Epoch: 6| Step: 6
Training loss: 4.996417045593262
Validation loss: 5.070806344350179

Epoch: 6| Step: 7
Training loss: 5.730729103088379
Validation loss: 5.059472401936849

Epoch: 6| Step: 8
Training loss: 5.4455461502075195
Validation loss: 5.048123399416606

Epoch: 6| Step: 9
Training loss: 5.360883712768555
Validation loss: 5.036550561587016

Epoch: 6| Step: 10
Training loss: 3.542491912841797
Validation loss: 5.02513575553894

Epoch: 6| Step: 11
Training loss: 5.035933017730713
Validation loss: 5.012829144795735

Epoch: 6| Step: 12
Training loss: 4.878222942352295
Validation loss: 5.0006210009257

Epoch: 6| Step: 13
Training loss: 4.143209934234619
Validation loss: 4.989120006561279

Epoch: 5| Step: 0
Training loss: 4.9285993576049805
Validation loss: 4.977415482203166

Epoch: 6| Step: 1
Training loss: 4.705573081970215
Validation loss: 4.965209166208903

Epoch: 6| Step: 2
Training loss: 5.963688850402832
Validation loss: 4.95297904809316

Epoch: 6| Step: 3
Training loss: 4.210023880004883
Validation loss: 4.9406905968983965

Epoch: 6| Step: 4
Training loss: 4.431629180908203
Validation loss: 4.928503791491191

Epoch: 6| Step: 5
Training loss: 4.869291305541992
Validation loss: 4.916241566340129

Epoch: 6| Step: 6
Training loss: 5.183746337890625
Validation loss: 4.904298543930054

Epoch: 6| Step: 7
Training loss: 4.58003568649292
Validation loss: 4.891719500223796

Epoch: 6| Step: 8
Training loss: 4.628403186798096
Validation loss: 4.880261421203613

Epoch: 6| Step: 9
Training loss: 5.641685962677002
Validation loss: 4.868154088656108

Epoch: 6| Step: 10
Training loss: 5.831514835357666
Validation loss: 4.856510559717814

Epoch: 6| Step: 11
Training loss: 5.085714340209961
Validation loss: 4.845966895421346

Epoch: 6| Step: 12
Training loss: 4.703152656555176
Validation loss: 4.835475842158

Epoch: 6| Step: 13
Training loss: 5.014444351196289
Validation loss: 4.825139999389648

Epoch: 6| Step: 0
Training loss: 5.562171936035156
Validation loss: 4.815512498219808

Epoch: 6| Step: 1
Training loss: 4.116501808166504
Validation loss: 4.806392232577006

Epoch: 6| Step: 2
Training loss: 5.076845169067383
Validation loss: 4.797096014022827

Epoch: 6| Step: 3
Training loss: 4.302642822265625
Validation loss: 4.788194894790649

Epoch: 6| Step: 4
Training loss: 4.990020751953125
Validation loss: 4.779637416203816

Epoch: 6| Step: 5
Training loss: 5.419108867645264
Validation loss: 4.7706780433654785

Epoch: 6| Step: 6
Training loss: 4.734854698181152
Validation loss: 4.762021064758301

Epoch: 6| Step: 7
Training loss: 4.169895648956299
Validation loss: 4.753881295522054

Epoch: 6| Step: 8
Training loss: 5.578064918518066
Validation loss: 4.745293617248535

Epoch: 6| Step: 9
Training loss: 4.410635948181152
Validation loss: 4.737154920895894

Epoch: 6| Step: 10
Training loss: 3.9939255714416504
Validation loss: 4.729147831598918

Epoch: 6| Step: 11
Training loss: 5.638004302978516
Validation loss: 4.721405744552612

Epoch: 6| Step: 12
Training loss: 4.905611515045166
Validation loss: 4.713035146395366

Epoch: 6| Step: 13
Training loss: 4.977602481842041
Validation loss: 4.705034534136455

Epoch: 7| Step: 0
Training loss: 4.559343338012695
Validation loss: 4.696668783823649

Epoch: 6| Step: 1
Training loss: 4.683080673217773
Validation loss: 4.688762903213501

Epoch: 6| Step: 2
Training loss: 5.274509429931641
Validation loss: 4.680480400721232

Epoch: 6| Step: 3
Training loss: 3.8418521881103516
Validation loss: 4.6722515026728315

Epoch: 6| Step: 4
Training loss: 3.4738354682922363
Validation loss: 4.6642528374989825

Epoch: 6| Step: 5
Training loss: 4.858128547668457
Validation loss: 4.656217575073242

Epoch: 6| Step: 6
Training loss: 5.06992244720459
Validation loss: 4.647635658582051

Epoch: 6| Step: 7
Training loss: 5.64178991317749
Validation loss: 4.639895439147949

Epoch: 6| Step: 8
Training loss: 4.92994499206543
Validation loss: 4.632094144821167

Epoch: 6| Step: 9
Training loss: 4.755579471588135
Validation loss: 4.6252867380778

Epoch: 6| Step: 10
Training loss: 5.302883148193359
Validation loss: 4.617733160654704

Epoch: 6| Step: 11
Training loss: 4.958087921142578
Validation loss: 4.610490719477336

Epoch: 6| Step: 12
Training loss: 4.920027732849121
Validation loss: 4.603161652882894

Epoch: 6| Step: 13
Training loss: 4.115468502044678
Validation loss: 4.596120913823445

Epoch: 8| Step: 0
Training loss: 3.463794231414795
Validation loss: 4.588780919710795

Epoch: 6| Step: 1
Training loss: 4.567692756652832
Validation loss: 4.582070668538411

Epoch: 6| Step: 2
Training loss: 4.58895206451416
Validation loss: 4.57489279905955

Epoch: 6| Step: 3
Training loss: 4.595730781555176
Validation loss: 4.568170706431071

Epoch: 6| Step: 4
Training loss: 5.360090732574463
Validation loss: 4.5616099039713545

Epoch: 6| Step: 5
Training loss: 3.74462628364563
Validation loss: 4.555072784423828

Epoch: 6| Step: 6
Training loss: 4.347701072692871
Validation loss: 4.548295259475708

Epoch: 6| Step: 7
Training loss: 4.228161811828613
Validation loss: 4.5417182842890425

Epoch: 6| Step: 8
Training loss: 4.773342132568359
Validation loss: 4.534994920094808

Epoch: 6| Step: 9
Training loss: 5.511471748352051
Validation loss: 4.5282487869262695

Epoch: 6| Step: 10
Training loss: 5.391196250915527
Validation loss: 4.521904865900676

Epoch: 6| Step: 11
Training loss: 5.163700103759766
Validation loss: 4.514874537785848

Epoch: 6| Step: 12
Training loss: 5.061110496520996
Validation loss: 4.508297483126323

Epoch: 6| Step: 13
Training loss: 4.244050979614258
Validation loss: 4.502138535181682

Epoch: 9| Step: 0
Training loss: 5.21611213684082
Validation loss: 4.496130307515462

Epoch: 6| Step: 1
Training loss: 4.650260925292969
Validation loss: 4.490418116251628

Epoch: 6| Step: 2
Training loss: 3.771395206451416
Validation loss: 4.483686526616414

Epoch: 6| Step: 3
Training loss: 4.488964557647705
Validation loss: 4.477747122446696

Epoch: 6| Step: 4
Training loss: 4.380641937255859
Validation loss: 4.472230076789856

Epoch: 6| Step: 5
Training loss: 4.476061820983887
Validation loss: 4.465466698010762

Epoch: 6| Step: 6
Training loss: 4.588813781738281
Validation loss: 4.459722916285197

Epoch: 6| Step: 7
Training loss: 3.957728385925293
Validation loss: 4.453822493553162

Epoch: 6| Step: 8
Training loss: 4.033590316772461
Validation loss: 4.448760588963826

Epoch: 6| Step: 9
Training loss: 4.081111907958984
Validation loss: 4.443383137385051

Epoch: 6| Step: 10
Training loss: 4.716263771057129
Validation loss: 4.437560240427653

Epoch: 6| Step: 11
Training loss: 5.422449588775635
Validation loss: 4.431731065114339

Epoch: 6| Step: 12
Training loss: 6.205660820007324
Validation loss: 4.426331480344136

Epoch: 6| Step: 13
Training loss: 3.968569278717041
Validation loss: 4.420782168706258

Epoch: 10| Step: 0
Training loss: 5.606086730957031
Validation loss: 4.415826757748921

Epoch: 6| Step: 1
Training loss: 4.547026634216309
Validation loss: 4.410523414611816

Epoch: 6| Step: 2
Training loss: 4.494309425354004
Validation loss: 4.4061226050059

Epoch: 6| Step: 3
Training loss: 4.569540023803711
Validation loss: 4.4001624584198

Epoch: 6| Step: 4
Training loss: 4.5514984130859375
Validation loss: 4.395017822583516

Epoch: 6| Step: 5
Training loss: 4.786136627197266
Validation loss: 4.389261960983276

Epoch: 6| Step: 6
Training loss: 5.4279069900512695
Validation loss: 4.384266058603923

Epoch: 6| Step: 7
Training loss: 5.307436466217041
Validation loss: 4.378320813179016

Epoch: 6| Step: 8
Training loss: 4.63520622253418
Validation loss: 4.373818318049113

Epoch: 6| Step: 9
Training loss: 4.85396671295166
Validation loss: 4.368583917617798

Epoch: 6| Step: 10
Training loss: 4.176664352416992
Validation loss: 4.362624486287435

Epoch: 6| Step: 11
Training loss: 2.986392021179199
Validation loss: 4.357019424438477

Epoch: 6| Step: 12
Training loss: 3.8609330654144287
Validation loss: 4.352052609125773

Epoch: 6| Step: 13
Training loss: 3.1821770668029785
Validation loss: 4.3481413920720415

Epoch: 11| Step: 0
Training loss: 5.235407829284668
Validation loss: 4.341946522394816

Epoch: 6| Step: 1
Training loss: 5.81148624420166
Validation loss: 4.335812171300252

Epoch: 6| Step: 2
Training loss: 4.047950744628906
Validation loss: 4.331521471341451

Epoch: 6| Step: 3
Training loss: 5.183813571929932
Validation loss: 4.326050360997518

Epoch: 6| Step: 4
Training loss: 4.076253890991211
Validation loss: 4.319980422655742

Epoch: 6| Step: 5
Training loss: 5.510239601135254
Validation loss: 4.317151427268982

Epoch: 6| Step: 6
Training loss: 5.181025981903076
Validation loss: 4.311790068944295

Epoch: 6| Step: 7
Training loss: 3.5447378158569336
Validation loss: 4.306291818618774

Epoch: 6| Step: 8
Training loss: 3.9008429050445557
Validation loss: 4.302235205968221

Epoch: 6| Step: 9
Training loss: 4.363455772399902
Validation loss: 4.297124822934468

Epoch: 6| Step: 10
Training loss: 3.631734848022461
Validation loss: 4.291416049003601

Epoch: 6| Step: 11
Training loss: 4.270809173583984
Validation loss: 4.285850604375203

Epoch: 6| Step: 12
Training loss: 3.320390462875366
Validation loss: 4.28008230527242

Epoch: 6| Step: 13
Training loss: 4.001448631286621
Validation loss: 4.275696158409119

Epoch: 12| Step: 0
Training loss: 5.628238677978516
Validation loss: 4.272198597590129

Epoch: 6| Step: 1
Training loss: 4.269688606262207
Validation loss: 4.264429489771525

Epoch: 6| Step: 2
Training loss: 4.9071502685546875
Validation loss: 4.25955855846405

Epoch: 6| Step: 3
Training loss: 4.107321739196777
Validation loss: 4.255467454592387

Epoch: 6| Step: 4
Training loss: 3.960979461669922
Validation loss: 4.249986012776692

Epoch: 6| Step: 5
Training loss: 2.9686498641967773
Validation loss: 4.245114962259929

Epoch: 6| Step: 6
Training loss: 4.4263014793396
Validation loss: 4.239544669787089

Epoch: 6| Step: 7
Training loss: 4.003537654876709
Validation loss: 4.234793186187744

Epoch: 6| Step: 8
Training loss: 5.0544843673706055
Validation loss: 4.230773091316223

Epoch: 6| Step: 9
Training loss: 4.912262916564941
Validation loss: 4.225390791893005

Epoch: 6| Step: 10
Training loss: 4.717563152313232
Validation loss: 4.219191074371338

Epoch: 6| Step: 11
Training loss: 3.2324378490448
Validation loss: 4.214582761128743

Epoch: 6| Step: 12
Training loss: 4.063164710998535
Validation loss: 4.209328730901082

Epoch: 6| Step: 13
Training loss: 4.9031901359558105
Validation loss: 4.204235712687175

Epoch: 13| Step: 0
Training loss: 3.584418296813965
Validation loss: 4.199119170506795

Epoch: 6| Step: 1
Training loss: 4.868779182434082
Validation loss: 4.194870074590047

Epoch: 6| Step: 2
Training loss: 4.437121868133545
Validation loss: 4.189608812332153

Epoch: 6| Step: 3
Training loss: 4.311825275421143
Validation loss: 4.183783253033956

Epoch: 6| Step: 4
Training loss: 3.5298750400543213
Validation loss: 4.178927898406982

Epoch: 6| Step: 5
Training loss: 4.624744892120361
Validation loss: 4.17369004090627

Epoch: 6| Step: 6
Training loss: 4.011021614074707
Validation loss: 4.168934981028239

Epoch: 6| Step: 7
Training loss: 4.0885210037231445
Validation loss: 4.163816809654236

Epoch: 6| Step: 8
Training loss: 5.402174949645996
Validation loss: 4.15889569123586

Epoch: 6| Step: 9
Training loss: 5.068122386932373
Validation loss: 4.153951525688171

Epoch: 6| Step: 10
Training loss: 4.830554008483887
Validation loss: 4.148462136586507

Epoch: 6| Step: 11
Training loss: 3.83217716217041
Validation loss: 4.143815954526265

Epoch: 6| Step: 12
Training loss: 4.033295154571533
Validation loss: 4.138780514399211

Epoch: 6| Step: 13
Training loss: 3.6247334480285645
Validation loss: 4.135854601860046

Epoch: 14| Step: 0
Training loss: 4.268120765686035
Validation loss: 4.130368987719218

Epoch: 6| Step: 1
Training loss: 3.0518345832824707
Validation loss: 4.125527620315552

Epoch: 6| Step: 2
Training loss: 4.133150577545166
Validation loss: 4.121398448944092

Epoch: 6| Step: 3
Training loss: 4.860428333282471
Validation loss: 4.116867303848267

Epoch: 6| Step: 4
Training loss: 4.239256858825684
Validation loss: 4.113049507141113

Epoch: 6| Step: 5
Training loss: 4.931044578552246
Validation loss: 4.107620676358541

Epoch: 6| Step: 6
Training loss: 4.756875514984131
Validation loss: 4.102397243181865

Epoch: 6| Step: 7
Training loss: 4.434879302978516
Validation loss: 4.096693793932597

Epoch: 6| Step: 8
Training loss: 4.043470859527588
Validation loss: 4.091391046841939

Epoch: 6| Step: 9
Training loss: 4.965626239776611
Validation loss: 4.086381038029988

Epoch: 6| Step: 10
Training loss: 4.117156028747559
Validation loss: 4.081983645757039

Epoch: 6| Step: 11
Training loss: 3.384232759475708
Validation loss: 4.076933701833089

Epoch: 6| Step: 12
Training loss: 3.9964232444763184
Validation loss: 4.072344382603963

Epoch: 6| Step: 13
Training loss: 4.177758693695068
Validation loss: 4.067435582478841

Epoch: 15| Step: 0
Training loss: 5.56678581237793
Validation loss: 4.062582015991211

Epoch: 6| Step: 1
Training loss: 3.939659595489502
Validation loss: 4.057653387387593

Epoch: 6| Step: 2
Training loss: 3.4336471557617188
Validation loss: 4.053265412648519

Epoch: 6| Step: 3
Training loss: 3.5863287448883057
Validation loss: 4.048032323519389

Epoch: 6| Step: 4
Training loss: 4.107526779174805
Validation loss: 4.043673634529114

Epoch: 6| Step: 5
Training loss: 4.9580159187316895
Validation loss: 4.039777994155884

Epoch: 6| Step: 6
Training loss: 4.323296546936035
Validation loss: 4.034473896026611

Epoch: 6| Step: 7
Training loss: 4.770740509033203
Validation loss: 4.030438502629598

Epoch: 6| Step: 8
Training loss: 4.062087059020996
Validation loss: 4.025285919507344

Epoch: 6| Step: 9
Training loss: 4.988157272338867
Validation loss: 4.020866870880127

Epoch: 6| Step: 10
Training loss: 3.1829605102539062
Validation loss: 4.016125321388245

Epoch: 6| Step: 11
Training loss: 4.610790252685547
Validation loss: 4.0114850997924805

Epoch: 6| Step: 12
Training loss: 3.485370397567749
Validation loss: 4.007118980089824

Epoch: 6| Step: 13
Training loss: 3.4446983337402344
Validation loss: 4.00270430246989

Epoch: 16| Step: 0
Training loss: 5.505560874938965
Validation loss: 3.9975464741388955

Epoch: 6| Step: 1
Training loss: 4.644046783447266
Validation loss: 3.9924043814341226

Epoch: 6| Step: 2
Training loss: 3.03602933883667
Validation loss: 3.987525780995687

Epoch: 6| Step: 3
Training loss: 4.157494068145752
Validation loss: 3.983458399772644

Epoch: 6| Step: 4
Training loss: 5.32232141494751
Validation loss: 3.978012124697367

Epoch: 6| Step: 5
Training loss: 3.897078514099121
Validation loss: 3.972873568534851

Epoch: 6| Step: 6
Training loss: 3.7593235969543457
Validation loss: 3.9682454665501914

Epoch: 6| Step: 7
Training loss: 3.7340035438537598
Validation loss: 3.9632877508799234

Epoch: 6| Step: 8
Training loss: 3.5043532848358154
Validation loss: 3.958362897237142

Epoch: 6| Step: 9
Training loss: 4.105179786682129
Validation loss: 3.9534608125686646

Epoch: 6| Step: 10
Training loss: 4.723925590515137
Validation loss: 3.9486228624979653

Epoch: 6| Step: 11
Training loss: 3.9892847537994385
Validation loss: 3.9442156553268433

Epoch: 6| Step: 12
Training loss: 3.691256523132324
Validation loss: 3.9399075508117676

Epoch: 6| Step: 13
Training loss: 3.536595582962036
Validation loss: 3.9349529345830283

Epoch: 17| Step: 0
Training loss: 3.669677734375
Validation loss: 3.9311182498931885

Epoch: 6| Step: 1
Training loss: 3.5849556922912598
Validation loss: 3.926973740259806

Epoch: 6| Step: 2
Training loss: 4.513071537017822
Validation loss: 3.9228134950002036

Epoch: 6| Step: 3
Training loss: 4.124995708465576
Validation loss: 3.9185022910435996

Epoch: 6| Step: 4
Training loss: 5.066041469573975
Validation loss: 3.9139463106791177

Epoch: 6| Step: 5
Training loss: 3.6787307262420654
Validation loss: 3.908383011817932

Epoch: 6| Step: 6
Training loss: 4.391095161437988
Validation loss: 3.9041722615559897

Epoch: 6| Step: 7
Training loss: 4.178102493286133
Validation loss: 3.899581472078959

Epoch: 6| Step: 8
Training loss: 3.653374671936035
Validation loss: 3.8947115739186606

Epoch: 6| Step: 9
Training loss: 4.470377445220947
Validation loss: 3.890330990155538

Epoch: 6| Step: 10
Training loss: 3.74527645111084
Validation loss: 3.885852257410685

Epoch: 6| Step: 11
Training loss: 3.7871036529541016
Validation loss: 3.881457487742106

Epoch: 6| Step: 12
Training loss: 3.9756627082824707
Validation loss: 3.878004789352417

Epoch: 6| Step: 13
Training loss: 3.8835670948028564
Validation loss: 3.8728665510813394

Epoch: 18| Step: 0
Training loss: 4.01812219619751
Validation loss: 3.868804136912028

Epoch: 6| Step: 1
Training loss: 4.3848557472229
Validation loss: 3.864383578300476

Epoch: 6| Step: 2
Training loss: 3.248126983642578
Validation loss: 3.8597115675608316

Epoch: 6| Step: 3
Training loss: 4.471529006958008
Validation loss: 3.8551281690597534

Epoch: 6| Step: 4
Training loss: 3.6441197395324707
Validation loss: 3.850433429082235

Epoch: 6| Step: 5
Training loss: 3.6767826080322266
Validation loss: 3.8465404907862344

Epoch: 6| Step: 6
Training loss: 3.8015546798706055
Validation loss: 3.8425451517105103

Epoch: 6| Step: 7
Training loss: 4.509615898132324
Validation loss: 3.837171276410421

Epoch: 6| Step: 8
Training loss: 3.8112595081329346
Validation loss: 3.832898179690043

Epoch: 6| Step: 9
Training loss: 2.7482519149780273
Validation loss: 3.8284409443537393

Epoch: 6| Step: 10
Training loss: 3.772430419921875
Validation loss: 3.8240421215693154

Epoch: 6| Step: 11
Training loss: 4.152248382568359
Validation loss: 3.819425861040751

Epoch: 6| Step: 12
Training loss: 4.282781600952148
Validation loss: 3.815186301867167

Epoch: 6| Step: 13
Training loss: 5.386508941650391
Validation loss: 3.8102500836054483

Epoch: 19| Step: 0
Training loss: 3.965179920196533
Validation loss: 3.8065311113993325

Epoch: 6| Step: 1
Training loss: 4.186221122741699
Validation loss: 3.8019436995188394

Epoch: 6| Step: 2
Training loss: 3.4008469581604004
Validation loss: 3.79741640885671

Epoch: 6| Step: 3
Training loss: 4.033436298370361
Validation loss: 3.793652892112732

Epoch: 6| Step: 4
Training loss: 4.048554420471191
Validation loss: 3.789353688557943

Epoch: 6| Step: 5
Training loss: 4.317837715148926
Validation loss: 3.7835365533828735

Epoch: 6| Step: 6
Training loss: 3.0784854888916016
Validation loss: 3.780280272165934

Epoch: 6| Step: 7
Training loss: 4.647112846374512
Validation loss: 3.775071899096171

Epoch: 6| Step: 8
Training loss: 4.554656028747559
Validation loss: 3.7703585227330527

Epoch: 6| Step: 9
Training loss: 4.435370445251465
Validation loss: 3.7663729588190713

Epoch: 6| Step: 10
Training loss: 3.1119418144226074
Validation loss: 3.760189970334371

Epoch: 6| Step: 11
Training loss: 3.740772247314453
Validation loss: 3.7571836709976196

Epoch: 6| Step: 12
Training loss: 3.230344772338867
Validation loss: 3.7538212140401206

Epoch: 6| Step: 13
Training loss: 4.357071399688721
Validation loss: 3.750191569328308

Epoch: 20| Step: 0
Training loss: 3.3961894512176514
Validation loss: 3.7422680854797363

Epoch: 6| Step: 1
Training loss: 3.3046722412109375
Validation loss: 3.739570458730062

Epoch: 6| Step: 2
Training loss: 3.646585464477539
Validation loss: 3.735226035118103

Epoch: 6| Step: 3
Training loss: 3.7697665691375732
Validation loss: 3.7320114771525064

Epoch: 6| Step: 4
Training loss: 4.633174896240234
Validation loss: 3.7274877230326333

Epoch: 6| Step: 5
Training loss: 3.549975872039795
Validation loss: 3.7232690254847207

Epoch: 6| Step: 6
Training loss: 4.5516133308410645
Validation loss: 3.7178311347961426

Epoch: 6| Step: 7
Training loss: 3.5806121826171875
Validation loss: 3.712856650352478

Epoch: 6| Step: 8
Training loss: 4.83958625793457
Validation loss: 3.7086249192555747

Epoch: 6| Step: 9
Training loss: 3.733222484588623
Validation loss: 3.70379372437795

Epoch: 6| Step: 10
Training loss: 4.135716915130615
Validation loss: 3.6991562048594155

Epoch: 6| Step: 11
Training loss: 3.884554386138916
Validation loss: 3.694240093231201

Epoch: 6| Step: 12
Training loss: 2.77398943901062
Validation loss: 3.689648667971293

Epoch: 6| Step: 13
Training loss: 4.465359210968018
Validation loss: 3.6854604880015054

Epoch: 21| Step: 0
Training loss: 2.342648506164551
Validation loss: 3.680787285168966

Epoch: 6| Step: 1
Training loss: 3.2365713119506836
Validation loss: 3.6756450732549033

Epoch: 6| Step: 2
Training loss: 3.576401710510254
Validation loss: 3.671238660812378

Epoch: 6| Step: 3
Training loss: 3.3256912231445312
Validation loss: 3.667704780896505

Epoch: 6| Step: 4
Training loss: 2.6699986457824707
Validation loss: 3.663376490275065

Epoch: 6| Step: 5
Training loss: 4.1948652267456055
Validation loss: 3.6603410641352334

Epoch: 6| Step: 6
Training loss: 3.9111716747283936
Validation loss: 3.6565170685450235

Epoch: 6| Step: 7
Training loss: 4.208337783813477
Validation loss: 3.6522282361984253

Epoch: 6| Step: 8
Training loss: 4.862403869628906
Validation loss: 3.647316495577494

Epoch: 6| Step: 9
Training loss: 3.5571160316467285
Validation loss: 3.6426614125569663

Epoch: 6| Step: 10
Training loss: 3.713663101196289
Validation loss: 3.637315511703491

Epoch: 6| Step: 11
Training loss: 4.413931846618652
Validation loss: 3.632794181505839

Epoch: 6| Step: 12
Training loss: 4.735848426818848
Validation loss: 3.6329813400904336

Epoch: 6| Step: 13
Training loss: 4.635988235473633
Validation loss: 3.626476287841797

Epoch: 22| Step: 0
Training loss: 3.575305938720703
Validation loss: 3.6249136130015054

Epoch: 6| Step: 1
Training loss: 4.063284397125244
Validation loss: 3.619657834370931

Epoch: 6| Step: 2
Training loss: 4.058971405029297
Validation loss: 3.614533265431722

Epoch: 6| Step: 3
Training loss: 3.46939754486084
Validation loss: 3.610457936922709

Epoch: 6| Step: 4
Training loss: 3.1122183799743652
Validation loss: 3.605763634045919

Epoch: 6| Step: 5
Training loss: 4.146912574768066
Validation loss: 3.6026737689971924

Epoch: 6| Step: 6
Training loss: 3.807620048522949
Validation loss: 3.5988873640696206

Epoch: 6| Step: 7
Training loss: 4.402292251586914
Validation loss: 3.594234744707743

Epoch: 6| Step: 8
Training loss: 3.9759678840637207
Validation loss: 3.587773164113363

Epoch: 6| Step: 9
Training loss: 3.9221627712249756
Validation loss: 3.582669178644816

Epoch: 6| Step: 10
Training loss: 3.6753005981445312
Validation loss: 3.576584736506144

Epoch: 6| Step: 11
Training loss: 3.7083888053894043
Validation loss: 3.5714776515960693

Epoch: 6| Step: 12
Training loss: 4.042296409606934
Validation loss: 3.567082484563192

Epoch: 6| Step: 13
Training loss: 2.66945743560791
Validation loss: 3.561866203943888

Epoch: 23| Step: 0
Training loss: 3.6718153953552246
Validation loss: 3.557230075200399

Epoch: 6| Step: 1
Training loss: 3.743468999862671
Validation loss: 3.552499016125997

Epoch: 6| Step: 2
Training loss: 2.9303574562072754
Validation loss: 3.5478850603103638

Epoch: 6| Step: 3
Training loss: 3.8574178218841553
Validation loss: 3.543137272198995

Epoch: 6| Step: 4
Training loss: 4.260843276977539
Validation loss: 3.539081414540609

Epoch: 6| Step: 5
Training loss: 3.4879488945007324
Validation loss: 3.534704049428304

Epoch: 6| Step: 6
Training loss: 3.0378761291503906
Validation loss: 3.5298712253570557

Epoch: 6| Step: 7
Training loss: 3.745941638946533
Validation loss: 3.5253798961639404

Epoch: 6| Step: 8
Training loss: 3.5780553817749023
Validation loss: 3.5211105346679688

Epoch: 6| Step: 9
Training loss: 3.8073723316192627
Validation loss: 3.5161115725835166

Epoch: 6| Step: 10
Training loss: 4.014975547790527
Validation loss: 3.511945605278015

Epoch: 6| Step: 11
Training loss: 3.8995230197906494
Validation loss: 3.507620414098104

Epoch: 6| Step: 12
Training loss: 4.225140571594238
Validation loss: 3.503041942914327

Epoch: 6| Step: 13
Training loss: 3.4418516159057617
Validation loss: 3.4980507294336953

Epoch: 24| Step: 0
Training loss: 4.14815616607666
Validation loss: 3.4927695592244468

Epoch: 6| Step: 1
Training loss: 3.9089555740356445
Validation loss: 3.488603671391805

Epoch: 6| Step: 2
Training loss: 3.430026054382324
Validation loss: 3.4843794902165732

Epoch: 6| Step: 3
Training loss: 3.275595188140869
Validation loss: 3.4798717896143594

Epoch: 6| Step: 4
Training loss: 2.746016263961792
Validation loss: 3.4743708769480386

Epoch: 6| Step: 5
Training loss: 3.713611602783203
Validation loss: 3.4699306090672812

Epoch: 6| Step: 6
Training loss: 4.115079879760742
Validation loss: 3.465113639831543

Epoch: 6| Step: 7
Training loss: 3.7762842178344727
Validation loss: 3.4604907830556235

Epoch: 6| Step: 8
Training loss: 3.7723207473754883
Validation loss: 3.456092198689779

Epoch: 6| Step: 9
Training loss: 3.88521671295166
Validation loss: 3.451326568921407

Epoch: 6| Step: 10
Training loss: 3.2351622581481934
Validation loss: 3.4467823108037314

Epoch: 6| Step: 11
Training loss: 3.639472484588623
Validation loss: 3.44227925936381

Epoch: 6| Step: 12
Training loss: 3.3855772018432617
Validation loss: 3.4377398093541465

Epoch: 6| Step: 13
Training loss: 3.7753796577453613
Validation loss: 3.43312931060791

Epoch: 25| Step: 0
Training loss: 3.1373374462127686
Validation loss: 3.4283170302708945

Epoch: 6| Step: 1
Training loss: 3.6374778747558594
Validation loss: 3.4235363006591797

Epoch: 6| Step: 2
Training loss: 4.013102054595947
Validation loss: 3.418712615966797

Epoch: 6| Step: 3
Training loss: 2.8955905437469482
Validation loss: 3.414251168568929

Epoch: 6| Step: 4
Training loss: 4.243019104003906
Validation loss: 3.4086950620015464

Epoch: 6| Step: 5
Training loss: 3.7934370040893555
Validation loss: 3.4044934113820395

Epoch: 6| Step: 6
Training loss: 4.348324298858643
Validation loss: 3.398747762044271

Epoch: 6| Step: 7
Training loss: 2.5958352088928223
Validation loss: 3.394087473551432

Epoch: 6| Step: 8
Training loss: 3.0172195434570312
Validation loss: 3.388882557551066

Epoch: 6| Step: 9
Training loss: 3.617194652557373
Validation loss: 3.3844035863876343

Epoch: 6| Step: 10
Training loss: 3.682666540145874
Validation loss: 3.3797772328058877

Epoch: 6| Step: 11
Training loss: 3.929258346557617
Validation loss: 3.375601132710775

Epoch: 6| Step: 12
Training loss: 3.4956729412078857
Validation loss: 3.371019959449768

Epoch: 6| Step: 13
Training loss: 3.542271137237549
Validation loss: 3.366790453592936

Epoch: 26| Step: 0
Training loss: 2.9823195934295654
Validation loss: 3.3622899850209556

Epoch: 6| Step: 1
Training loss: 3.262784242630005
Validation loss: 3.3580747842788696

Epoch: 6| Step: 2
Training loss: 4.0834808349609375
Validation loss: 3.3532236417134604

Epoch: 6| Step: 3
Training loss: 3.0673305988311768
Validation loss: 3.348940054575602

Epoch: 6| Step: 4
Training loss: 3.0174736976623535
Validation loss: 3.344915986061096

Epoch: 6| Step: 5
Training loss: 3.613504648208618
Validation loss: 3.339940905570984

Epoch: 6| Step: 6
Training loss: 3.570375680923462
Validation loss: 3.335551142692566

Epoch: 6| Step: 7
Training loss: 3.899888277053833
Validation loss: 3.3310839335123696

Epoch: 6| Step: 8
Training loss: 3.8462390899658203
Validation loss: 3.3269598484039307

Epoch: 6| Step: 9
Training loss: 3.7220494747161865
Validation loss: 3.3222752809524536

Epoch: 6| Step: 10
Training loss: 3.4985270500183105
Validation loss: 3.3174725770950317

Epoch: 6| Step: 11
Training loss: 3.3411474227905273
Validation loss: 3.3140260378519693

Epoch: 6| Step: 12
Training loss: 3.2143607139587402
Validation loss: 3.3092125256856284

Epoch: 6| Step: 13
Training loss: 3.9451026916503906
Validation loss: 3.3041130701700845

Epoch: 27| Step: 0
Training loss: 3.4277515411376953
Validation loss: 3.2999887069066367

Epoch: 6| Step: 1
Training loss: 3.91359806060791
Validation loss: 3.2963438431421914

Epoch: 6| Step: 2
Training loss: 3.7323036193847656
Validation loss: 3.292612910270691

Epoch: 6| Step: 3
Training loss: 3.080829620361328
Validation loss: 3.288761576016744

Epoch: 6| Step: 4
Training loss: 3.266550302505493
Validation loss: 3.284511923789978

Epoch: 6| Step: 5
Training loss: 2.9158382415771484
Validation loss: 3.2812954584757485

Epoch: 6| Step: 6
Training loss: 3.5067059993743896
Validation loss: 3.275664289792379

Epoch: 6| Step: 7
Training loss: 3.8510189056396484
Validation loss: 3.2709811528523765

Epoch: 6| Step: 8
Training loss: 3.388908863067627
Validation loss: 3.2663296461105347

Epoch: 6| Step: 9
Training loss: 3.261857509613037
Validation loss: 3.2616196473439536

Epoch: 6| Step: 10
Training loss: 3.13107967376709
Validation loss: 3.2565797567367554

Epoch: 6| Step: 11
Training loss: 3.8981144428253174
Validation loss: 3.251969854036967

Epoch: 6| Step: 12
Training loss: 4.012801170349121
Validation loss: 3.2463727394739785

Epoch: 6| Step: 13
Training loss: 2.8859901428222656
Validation loss: 3.241684317588806

Epoch: 28| Step: 0
Training loss: 4.009731292724609
Validation loss: 3.2367199659347534

Epoch: 6| Step: 1
Training loss: 3.3703441619873047
Validation loss: 3.232315023740133

Epoch: 6| Step: 2
Training loss: 2.9183201789855957
Validation loss: 3.2280571460723877

Epoch: 6| Step: 3
Training loss: 3.2982726097106934
Validation loss: 3.224215547243754

Epoch: 6| Step: 4
Training loss: 2.8437376022338867
Validation loss: 3.219966769218445

Epoch: 6| Step: 5
Training loss: 2.72147274017334
Validation loss: 3.2159467935562134

Epoch: 6| Step: 6
Training loss: 3.516601085662842
Validation loss: 3.2124327023824057

Epoch: 6| Step: 7
Training loss: 3.5161209106445312
Validation loss: 3.20767871538798

Epoch: 6| Step: 8
Training loss: 3.72099232673645
Validation loss: 3.2029902935028076

Epoch: 6| Step: 9
Training loss: 3.9744153022766113
Validation loss: 3.1980408827463784

Epoch: 6| Step: 10
Training loss: 2.9217443466186523
Validation loss: 3.193214178085327

Epoch: 6| Step: 11
Training loss: 3.744579315185547
Validation loss: 3.1894485553105674

Epoch: 6| Step: 12
Training loss: 3.58878755569458
Validation loss: 3.1855703592300415

Epoch: 6| Step: 13
Training loss: 3.303969383239746
Validation loss: 3.181224266688029

Epoch: 29| Step: 0
Training loss: 3.3378994464874268
Validation loss: 3.176795999209086

Epoch: 6| Step: 1
Training loss: 3.479184627532959
Validation loss: 3.1731654604276023

Epoch: 6| Step: 2
Training loss: 3.175875186920166
Validation loss: 3.169371008872986

Epoch: 6| Step: 3
Training loss: 2.3891797065734863
Validation loss: 3.1632570028305054

Epoch: 6| Step: 4
Training loss: 3.3679709434509277
Validation loss: 3.158220410346985

Epoch: 6| Step: 5
Training loss: 2.890988349914551
Validation loss: 3.1545724868774414

Epoch: 6| Step: 6
Training loss: 3.9706287384033203
Validation loss: 3.150898814201355

Epoch: 6| Step: 7
Training loss: 3.244415283203125
Validation loss: 3.1465534369150796

Epoch: 6| Step: 8
Training loss: 4.509442329406738
Validation loss: 3.1427818536758423

Epoch: 6| Step: 9
Training loss: 3.5210862159729004
Validation loss: 3.1377320289611816

Epoch: 6| Step: 10
Training loss: 2.8181698322296143
Validation loss: 3.1332881848017373

Epoch: 6| Step: 11
Training loss: 3.3178701400756836
Validation loss: 3.1295069058736167

Epoch: 6| Step: 12
Training loss: 2.9653408527374268
Validation loss: 3.1249937613805137

Epoch: 6| Step: 13
Training loss: 3.6854288578033447
Validation loss: 3.1207117636998496

Epoch: 30| Step: 0
Training loss: 3.228079080581665
Validation loss: 3.11697248617808

Epoch: 6| Step: 1
Training loss: 3.4629969596862793
Validation loss: 3.112792730331421

Epoch: 6| Step: 2
Training loss: 2.93434476852417
Validation loss: 3.1085570653279624

Epoch: 6| Step: 3
Training loss: 3.5172502994537354
Validation loss: 3.104646841684977

Epoch: 6| Step: 4
Training loss: 2.963702440261841
Validation loss: 3.1002996365229287

Epoch: 6| Step: 5
Training loss: 3.1753249168395996
Validation loss: 3.097395102183024

Epoch: 6| Step: 6
Training loss: 4.658635139465332
Validation loss: 3.094650228818258

Epoch: 6| Step: 7
Training loss: 3.961026191711426
Validation loss: 3.0900540749231973

Epoch: 6| Step: 8
Training loss: 2.755049467086792
Validation loss: 3.089269002278646

Epoch: 6| Step: 9
Training loss: 2.9643445014953613
Validation loss: 3.082290530204773

Epoch: 6| Step: 10
Training loss: 3.337538242340088
Validation loss: 3.077774167060852

Epoch: 6| Step: 11
Training loss: 2.993752956390381
Validation loss: 3.0753082036972046

Epoch: 6| Step: 12
Training loss: 3.136058807373047
Validation loss: 3.0721364418665567

Epoch: 6| Step: 13
Training loss: 2.8363382816314697
Validation loss: 3.0688893795013428

Epoch: 31| Step: 0
Training loss: 2.649092197418213
Validation loss: 3.0655449628829956

Epoch: 6| Step: 1
Training loss: 2.978790283203125
Validation loss: 3.0616726875305176

Epoch: 6| Step: 2
Training loss: 3.290480136871338
Validation loss: 3.0574049154917398

Epoch: 6| Step: 3
Training loss: 4.174867630004883
Validation loss: 3.0538238286972046

Epoch: 6| Step: 4
Training loss: 2.6325490474700928
Validation loss: 3.050563414891561

Epoch: 6| Step: 5
Training loss: 3.612846851348877
Validation loss: 3.0464946230252585

Epoch: 6| Step: 6
Training loss: 3.3617522716522217
Validation loss: 3.0423795779546103

Epoch: 6| Step: 7
Training loss: 3.3654024600982666
Validation loss: 3.038352608680725

Epoch: 6| Step: 8
Training loss: 2.038048267364502
Validation loss: 3.0341309706370034

Epoch: 6| Step: 9
Training loss: 2.630645751953125
Validation loss: 3.0312535762786865

Epoch: 6| Step: 10
Training loss: 4.224513053894043
Validation loss: 3.0278778076171875

Epoch: 6| Step: 11
Training loss: 3.018951416015625
Validation loss: 3.0252493222554526

Epoch: 6| Step: 12
Training loss: 4.209116458892822
Validation loss: 3.0238080819447837

Epoch: 6| Step: 13
Training loss: 3.0500428676605225
Validation loss: 3.016806940237681

Epoch: 32| Step: 0
Training loss: 2.889824867248535
Validation loss: 3.0143707195917764

Epoch: 6| Step: 1
Training loss: 2.9332797527313232
Validation loss: 3.0118806759516397

Epoch: 6| Step: 2
Training loss: 2.8279776573181152
Validation loss: 3.008687376976013

Epoch: 6| Step: 3
Training loss: 4.158430099487305
Validation loss: 3.003071109453837

Epoch: 6| Step: 4
Training loss: 2.793368339538574
Validation loss: 2.999753475189209

Epoch: 6| Step: 5
Training loss: 3.6026577949523926
Validation loss: 2.9963857730229697

Epoch: 6| Step: 6
Training loss: 3.355897903442383
Validation loss: 2.9929861227671304

Epoch: 6| Step: 7
Training loss: 2.426882266998291
Validation loss: 2.9894718726476035

Epoch: 6| Step: 8
Training loss: 3.177786350250244
Validation loss: 2.986335833867391

Epoch: 6| Step: 9
Training loss: 2.8915610313415527
Validation loss: 2.982978105545044

Epoch: 6| Step: 10
Training loss: 3.255401611328125
Validation loss: 2.9788211981455484

Epoch: 6| Step: 11
Training loss: 3.773327350616455
Validation loss: 2.9757885535558066

Epoch: 6| Step: 12
Training loss: 4.006501197814941
Validation loss: 2.976178208986918

Epoch: 6| Step: 13
Training loss: 2.5305755138397217
Validation loss: 2.970101078351339

Epoch: 33| Step: 0
Training loss: 2.739480972290039
Validation loss: 2.9639844497044883

Epoch: 6| Step: 1
Training loss: 2.464951753616333
Validation loss: 2.960187832514445

Epoch: 6| Step: 2
Training loss: 2.8134853839874268
Validation loss: 2.957625389099121

Epoch: 6| Step: 3
Training loss: 3.594444751739502
Validation loss: 2.9559404452641806

Epoch: 6| Step: 4
Training loss: 4.01538610458374
Validation loss: 2.95277992884318

Epoch: 6| Step: 5
Training loss: 3.2559263706207275
Validation loss: 2.947676142056783

Epoch: 6| Step: 6
Training loss: 3.3394718170166016
Validation loss: 2.9439677794774375

Epoch: 6| Step: 7
Training loss: 2.6035685539245605
Validation loss: 2.9389143784840903

Epoch: 6| Step: 8
Training loss: 3.563417673110962
Validation loss: 2.935691316922506

Epoch: 6| Step: 9
Training loss: 2.4163732528686523
Validation loss: 2.9314289887746177

Epoch: 6| Step: 10
Training loss: 3.467520236968994
Validation loss: 2.927479346593221

Epoch: 6| Step: 11
Training loss: 3.4138500690460205
Validation loss: 2.9242336750030518

Epoch: 6| Step: 12
Training loss: 2.9614675045013428
Validation loss: 2.9210398197174072

Epoch: 6| Step: 13
Training loss: 3.3489484786987305
Validation loss: 2.9178411960601807

Epoch: 34| Step: 0
Training loss: 2.914414882659912
Validation loss: 2.915143887201945

Epoch: 6| Step: 1
Training loss: 2.8559556007385254
Validation loss: 2.911714514096578

Epoch: 6| Step: 2
Training loss: 3.408362865447998
Validation loss: 2.9083605209986367

Epoch: 6| Step: 3
Training loss: 2.5864880084991455
Validation loss: 2.905383308728536

Epoch: 6| Step: 4
Training loss: 2.614135980606079
Validation loss: 2.9017796913782754

Epoch: 6| Step: 5
Training loss: 2.9862966537475586
Validation loss: 2.899265925089518

Epoch: 6| Step: 6
Training loss: 3.3291497230529785
Validation loss: 2.895127455393473

Epoch: 6| Step: 7
Training loss: 2.712362289428711
Validation loss: 2.892499486605326

Epoch: 6| Step: 8
Training loss: 3.190267562866211
Validation loss: 2.888778487841288

Epoch: 6| Step: 9
Training loss: 3.669149875640869
Validation loss: 2.8856197198232016

Epoch: 6| Step: 10
Training loss: 3.0851712226867676
Validation loss: 2.8812584479649863

Epoch: 6| Step: 11
Training loss: 3.571397542953491
Validation loss: 2.8785698413848877

Epoch: 6| Step: 12
Training loss: 3.4328582286834717
Validation loss: 2.8749777475992837

Epoch: 6| Step: 13
Training loss: 3.0130069255828857
Validation loss: 2.8713608582814536

Epoch: 35| Step: 0
Training loss: 2.765871286392212
Validation loss: 2.868176738421122

Epoch: 6| Step: 1
Training loss: 3.5262951850891113
Validation loss: 2.865013519922892

Epoch: 6| Step: 2
Training loss: 2.3434927463531494
Validation loss: 2.861420472462972

Epoch: 6| Step: 3
Training loss: 2.5388360023498535
Validation loss: 2.8580415646235147

Epoch: 6| Step: 4
Training loss: 3.399484872817993
Validation loss: 2.8549627463022866

Epoch: 6| Step: 5
Training loss: 2.762603998184204
Validation loss: 2.851052761077881

Epoch: 6| Step: 6
Training loss: 3.483682632446289
Validation loss: 2.84856915473938

Epoch: 6| Step: 7
Training loss: 2.5740389823913574
Validation loss: 2.845227281252543

Epoch: 6| Step: 8
Training loss: 2.878429889678955
Validation loss: 2.8420353730519614

Epoch: 6| Step: 9
Training loss: 3.4469056129455566
Validation loss: 2.8386081059773765

Epoch: 6| Step: 10
Training loss: 2.866687297821045
Validation loss: 2.834572712580363

Epoch: 6| Step: 11
Training loss: 3.559938430786133
Validation loss: 2.8317209482192993

Epoch: 6| Step: 12
Training loss: 3.007117509841919
Validation loss: 2.8285479148228965

Epoch: 6| Step: 13
Training loss: 3.64274263381958
Validation loss: 2.825596729914347

Epoch: 36| Step: 0
Training loss: 2.6177797317504883
Validation loss: 2.8220462004343667

Epoch: 6| Step: 1
Training loss: 3.4201807975769043
Validation loss: 2.819648265838623

Epoch: 6| Step: 2
Training loss: 3.218096971511841
Validation loss: 2.8159759640693665

Epoch: 6| Step: 3
Training loss: 3.1986937522888184
Validation loss: 2.8129434982935586

Epoch: 6| Step: 4
Training loss: 3.070242404937744
Validation loss: 2.810983101526896

Epoch: 6| Step: 5
Training loss: 3.1581740379333496
Validation loss: 2.8070388436317444

Epoch: 6| Step: 6
Training loss: 3.1597325801849365
Validation loss: 2.803582469622294

Epoch: 6| Step: 7
Training loss: 2.2775802612304688
Validation loss: 2.79978617032369

Epoch: 6| Step: 8
Training loss: 1.9358001947402954
Validation loss: 2.7972111304601035

Epoch: 6| Step: 9
Training loss: 3.046018600463867
Validation loss: 2.794606645901998

Epoch: 6| Step: 10
Training loss: 4.229775428771973
Validation loss: 2.7917282581329346

Epoch: 6| Step: 11
Training loss: 2.5773348808288574
Validation loss: 2.7893784046173096

Epoch: 6| Step: 12
Training loss: 3.417342185974121
Validation loss: 2.7863720655441284

Epoch: 6| Step: 13
Training loss: 2.8854756355285645
Validation loss: 2.7827788988749185

Epoch: 37| Step: 0
Training loss: 3.2858657836914062
Validation loss: 2.7795321941375732

Epoch: 6| Step: 1
Training loss: 3.45613169670105
Validation loss: 2.779105027516683

Epoch: 6| Step: 2
Training loss: 3.314253330230713
Validation loss: 2.7839306195576987

Epoch: 6| Step: 3
Training loss: 3.350656747817993
Validation loss: 2.7697558403015137

Epoch: 6| Step: 4
Training loss: 2.8997325897216797
Validation loss: 2.7678604125976562

Epoch: 6| Step: 5
Training loss: 3.0031518936157227
Validation loss: 2.7661482095718384

Epoch: 6| Step: 6
Training loss: 2.3848953247070312
Validation loss: 2.7635422547658286

Epoch: 6| Step: 7
Training loss: 2.472520351409912
Validation loss: 2.7601822217305503

Epoch: 6| Step: 8
Training loss: 2.641674041748047
Validation loss: 2.7581849892934165

Epoch: 6| Step: 9
Training loss: 2.763704776763916
Validation loss: 2.7547008196512857

Epoch: 6| Step: 10
Training loss: 2.7536532878875732
Validation loss: 2.7524315118789673

Epoch: 6| Step: 11
Training loss: 2.459690809249878
Validation loss: 2.7498450676600137

Epoch: 6| Step: 12
Training loss: 3.533949613571167
Validation loss: 2.7477115392684937

Epoch: 6| Step: 13
Training loss: 3.3573389053344727
Validation loss: 2.745794177055359

Epoch: 38| Step: 0
Training loss: 2.51444411277771
Validation loss: 2.742401043574015

Epoch: 6| Step: 1
Training loss: 3.9192211627960205
Validation loss: 2.7416104475657144

Epoch: 6| Step: 2
Training loss: 2.8815500736236572
Validation loss: 2.738600770632426

Epoch: 6| Step: 3
Training loss: 2.952975273132324
Validation loss: 2.7347177267074585

Epoch: 6| Step: 4
Training loss: 2.9603049755096436
Validation loss: 2.7311089833577475

Epoch: 6| Step: 5
Training loss: 2.2254514694213867
Validation loss: 2.7269175052642822

Epoch: 6| Step: 6
Training loss: 2.4329042434692383
Validation loss: 2.7247979243596396

Epoch: 6| Step: 7
Training loss: 2.9634203910827637
Validation loss: 2.720829129219055

Epoch: 6| Step: 8
Training loss: 3.067549467086792
Validation loss: 2.7176397244135537

Epoch: 6| Step: 9
Training loss: 2.645223617553711
Validation loss: 2.7142350673675537

Epoch: 6| Step: 10
Training loss: 3.6126956939697266
Validation loss: 2.710536321004232

Epoch: 6| Step: 11
Training loss: 3.240273952484131
Validation loss: 2.7084426482518515

Epoch: 6| Step: 12
Training loss: 3.1036391258239746
Validation loss: 2.703646938006083

Epoch: 6| Step: 13
Training loss: 2.5133352279663086
Validation loss: 2.701306621233622

Epoch: 39| Step: 0
Training loss: 2.960052967071533
Validation loss: 2.699735919634501

Epoch: 6| Step: 1
Training loss: 3.1422133445739746
Validation loss: 2.6961398124694824

Epoch: 6| Step: 2
Training loss: 2.2704806327819824
Validation loss: 2.692087968190511

Epoch: 6| Step: 3
Training loss: 3.0596625804901123
Validation loss: 2.6892894903818765

Epoch: 6| Step: 4
Training loss: 3.2691307067871094
Validation loss: 2.686070124308268

Epoch: 6| Step: 5
Training loss: 3.6593379974365234
Validation loss: 2.683504581451416

Epoch: 6| Step: 6
Training loss: 2.3213441371917725
Validation loss: 2.6812421480814614

Epoch: 6| Step: 7
Training loss: 3.018435478210449
Validation loss: 2.6781508525212607

Epoch: 6| Step: 8
Training loss: 2.8065829277038574
Validation loss: 2.6767400900522866

Epoch: 6| Step: 9
Training loss: 3.1699109077453613
Validation loss: 2.6748943527539573

Epoch: 6| Step: 10
Training loss: 3.032322645187378
Validation loss: 2.672798275947571

Epoch: 6| Step: 11
Training loss: 2.5078048706054688
Validation loss: 2.6707602938016257

Epoch: 6| Step: 12
Training loss: 2.54714035987854
Validation loss: 2.667966604232788

Epoch: 6| Step: 13
Training loss: 2.667651891708374
Validation loss: 2.66603676478068

Epoch: 40| Step: 0
Training loss: 3.245377779006958
Validation loss: 2.6628551483154297

Epoch: 6| Step: 1
Training loss: 2.0855798721313477
Validation loss: 2.659807483355204

Epoch: 6| Step: 2
Training loss: 2.609463691711426
Validation loss: 2.65770161151886

Epoch: 6| Step: 3
Training loss: 2.8165905475616455
Validation loss: 2.654229243596395

Epoch: 6| Step: 4
Training loss: 2.8123133182525635
Validation loss: 2.6515809297561646

Epoch: 6| Step: 5
Training loss: 3.314089775085449
Validation loss: 2.648111661275228

Epoch: 6| Step: 6
Training loss: 3.2481093406677246
Validation loss: 2.643968125184377

Epoch: 6| Step: 7
Training loss: 3.194959878921509
Validation loss: 2.6404032707214355

Epoch: 6| Step: 8
Training loss: 2.717562198638916
Validation loss: 2.6373334725697837

Epoch: 6| Step: 9
Training loss: 2.6399471759796143
Validation loss: 2.6317872206370034

Epoch: 6| Step: 10
Training loss: 2.7533953189849854
Validation loss: 2.6287808815638223

Epoch: 6| Step: 11
Training loss: 3.05075740814209
Validation loss: 2.624751845995585

Epoch: 6| Step: 12
Training loss: 2.3552052974700928
Validation loss: 2.6192044814427695

Epoch: 6| Step: 13
Training loss: 3.004671335220337
Validation loss: 2.61624546845754

Epoch: 41| Step: 0
Training loss: 2.4498047828674316
Validation loss: 2.615110754966736

Epoch: 6| Step: 1
Training loss: 2.632843494415283
Validation loss: 2.6142423152923584

Epoch: 6| Step: 2
Training loss: 3.24881649017334
Validation loss: 2.606051047643026

Epoch: 6| Step: 3
Training loss: 3.1236228942871094
Validation loss: 2.603853086630503

Epoch: 6| Step: 4
Training loss: 1.9236385822296143
Validation loss: 2.6015246907869973

Epoch: 6| Step: 5
Training loss: 2.0921244621276855
Validation loss: 2.598475535710653

Epoch: 6| Step: 6
Training loss: 2.8322339057922363
Validation loss: 2.59589676062266

Epoch: 6| Step: 7
Training loss: 3.086491823196411
Validation loss: 2.590173880259196

Epoch: 6| Step: 8
Training loss: 3.566096782684326
Validation loss: 2.588403304417928

Epoch: 6| Step: 9
Training loss: 2.9164860248565674
Validation loss: 2.585012753804525

Epoch: 6| Step: 10
Training loss: 2.792419910430908
Validation loss: 2.5835246245066323

Epoch: 6| Step: 11
Training loss: 3.114494800567627
Validation loss: 2.581337571144104

Epoch: 6| Step: 12
Training loss: 2.5210137367248535
Validation loss: 2.5818820794423423

Epoch: 6| Step: 13
Training loss: 2.9410324096679688
Validation loss: 2.5766647259394326

Epoch: 42| Step: 0
Training loss: 3.3686296939849854
Validation loss: 2.575633486111959

Epoch: 6| Step: 1
Training loss: 2.4881362915039062
Validation loss: 2.5741711060206094

Epoch: 6| Step: 2
Training loss: 2.7438111305236816
Validation loss: 2.572186529636383

Epoch: 6| Step: 3
Training loss: 2.4909427165985107
Validation loss: 2.5704834858576455

Epoch: 6| Step: 4
Training loss: 3.373131275177002
Validation loss: 2.567493518193563

Epoch: 6| Step: 5
Training loss: 3.0162806510925293
Validation loss: 2.56623383363088

Epoch: 6| Step: 6
Training loss: 2.4760396480560303
Validation loss: 2.5615828037261963

Epoch: 6| Step: 7
Training loss: 2.5223445892333984
Validation loss: 2.558267037073771

Epoch: 6| Step: 8
Training loss: 2.26863694190979
Validation loss: 2.557915290196737

Epoch: 6| Step: 9
Training loss: 2.1961379051208496
Validation loss: 2.5515565872192383

Epoch: 6| Step: 10
Training loss: 2.5430076122283936
Validation loss: 2.547034760316213

Epoch: 6| Step: 11
Training loss: 3.5508151054382324
Validation loss: 2.5438539187113443

Epoch: 6| Step: 12
Training loss: 3.114290714263916
Validation loss: 2.5428173939387

Epoch: 6| Step: 13
Training loss: 2.4933981895446777
Validation loss: 2.536785384019216

Epoch: 43| Step: 0
Training loss: 3.0272412300109863
Validation loss: 2.5379860003789267

Epoch: 6| Step: 1
Training loss: 2.4317898750305176
Validation loss: 2.534743010997772

Epoch: 6| Step: 2
Training loss: 2.429410457611084
Validation loss: 2.530534505844116

Epoch: 6| Step: 3
Training loss: 2.8717479705810547
Validation loss: 2.527697960535685

Epoch: 6| Step: 4
Training loss: 3.2253782749176025
Validation loss: 2.5253443320592246

Epoch: 6| Step: 5
Training loss: 2.4729578495025635
Validation loss: 2.518487056096395

Epoch: 6| Step: 6
Training loss: 3.019230365753174
Validation loss: 2.5179892977078757

Epoch: 6| Step: 7
Training loss: 3.2556419372558594
Validation loss: 2.5181233882904053

Epoch: 6| Step: 8
Training loss: 2.232635021209717
Validation loss: 2.5115248362223306

Epoch: 6| Step: 9
Training loss: 2.5338282585144043
Validation loss: 2.5111995538075766

Epoch: 6| Step: 10
Training loss: 2.971168041229248
Validation loss: 2.504901647567749

Epoch: 6| Step: 11
Training loss: 2.9182796478271484
Validation loss: 2.505238393942515

Epoch: 6| Step: 12
Training loss: 2.3546721935272217
Validation loss: 2.498767137527466

Epoch: 6| Step: 13
Training loss: 2.2992055416107178
Validation loss: 2.5019705295562744

Epoch: 44| Step: 0
Training loss: 3.278769016265869
Validation loss: 2.4975006580352783

Epoch: 6| Step: 1
Training loss: 2.8504486083984375
Validation loss: 2.497932473818461

Epoch: 6| Step: 2
Training loss: 3.000248908996582
Validation loss: 2.49012823899587

Epoch: 6| Step: 3
Training loss: 1.8511574268341064
Validation loss: 2.487077832221985

Epoch: 6| Step: 4
Training loss: 2.5761406421661377
Validation loss: 2.4843897620836892

Epoch: 6| Step: 5
Training loss: 2.7598352432250977
Validation loss: 2.4831198255221048

Epoch: 6| Step: 6
Training loss: 2.5488831996917725
Validation loss: 2.4824212789535522

Epoch: 6| Step: 7
Training loss: 2.522331953048706
Validation loss: 2.481102228164673

Epoch: 6| Step: 8
Training loss: 2.627694606781006
Validation loss: 2.4804240067799888

Epoch: 6| Step: 9
Training loss: 2.309694766998291
Validation loss: 2.4741501808166504

Epoch: 6| Step: 10
Training loss: 2.81923246383667
Validation loss: 2.4701159397761026

Epoch: 6| Step: 11
Training loss: 2.6116881370544434
Validation loss: 2.465100367863973

Epoch: 6| Step: 12
Training loss: 2.7291371822357178
Validation loss: 2.464739203453064

Epoch: 6| Step: 13
Training loss: 3.0588467121124268
Validation loss: 2.4658393462498984

Epoch: 45| Step: 0
Training loss: 2.39076566696167
Validation loss: 2.465140461921692

Epoch: 6| Step: 1
Training loss: 3.5659844875335693
Validation loss: 2.4654000997543335

Epoch: 6| Step: 2
Training loss: 2.518857955932617
Validation loss: 2.4646779696146646

Epoch: 6| Step: 3
Training loss: 2.4313673973083496
Validation loss: 2.4621655543645224

Epoch: 6| Step: 4
Training loss: 1.9830358028411865
Validation loss: 2.459685524304708

Epoch: 6| Step: 5
Training loss: 2.886232376098633
Validation loss: 2.458218236764272

Epoch: 6| Step: 6
Training loss: 2.419055223464966
Validation loss: 2.4553877115249634

Epoch: 6| Step: 7
Training loss: 2.8506689071655273
Validation loss: 2.4522550106048584

Epoch: 6| Step: 8
Training loss: 3.114196300506592
Validation loss: 2.4503378868103027

Epoch: 6| Step: 9
Training loss: 3.5631134510040283
Validation loss: 2.4450670878092446

Epoch: 6| Step: 10
Training loss: 2.463855743408203
Validation loss: 2.442082385222117

Epoch: 6| Step: 11
Training loss: 2.6448464393615723
Validation loss: 2.4375397761662803

Epoch: 6| Step: 12
Training loss: 1.7693742513656616
Validation loss: 2.435374617576599

Epoch: 6| Step: 13
Training loss: 2.441361904144287
Validation loss: 2.4316910902659097

Epoch: 46| Step: 0
Training loss: 2.691789150238037
Validation loss: 2.434848109881083

Epoch: 6| Step: 1
Training loss: 3.04605770111084
Validation loss: 2.427355488141378

Epoch: 6| Step: 2
Training loss: 2.075564384460449
Validation loss: 2.422980626424154

Epoch: 6| Step: 3
Training loss: 3.3574790954589844
Validation loss: 2.420366326967875

Epoch: 6| Step: 4
Training loss: 2.7898221015930176
Validation loss: 2.419664144515991

Epoch: 6| Step: 5
Training loss: 2.5285916328430176
Validation loss: 2.4183990557988486

Epoch: 6| Step: 6
Training loss: 2.1561129093170166
Validation loss: 2.4170674482981362

Epoch: 6| Step: 7
Training loss: 2.641126871109009
Validation loss: 2.414855639139811

Epoch: 6| Step: 8
Training loss: 2.938399314880371
Validation loss: 2.4152004718780518

Epoch: 6| Step: 9
Training loss: 2.9744977951049805
Validation loss: 2.409887433052063

Epoch: 6| Step: 10
Training loss: 2.282294750213623
Validation loss: 2.408491770426432

Epoch: 6| Step: 11
Training loss: 2.23721981048584
Validation loss: 2.4048224687576294

Epoch: 6| Step: 12
Training loss: 2.5762710571289062
Validation loss: 2.398280362288157

Epoch: 6| Step: 13
Training loss: 2.20085072517395
Validation loss: 2.3962626655896506

Epoch: 47| Step: 0
Training loss: 2.8677096366882324
Validation loss: 2.3947258790334067

Epoch: 6| Step: 1
Training loss: 2.76387357711792
Validation loss: 2.3937531312306723

Epoch: 6| Step: 2
Training loss: 2.571133852005005
Validation loss: 2.389646291732788

Epoch: 6| Step: 3
Training loss: 2.736567497253418
Validation loss: 2.3885503013928733

Epoch: 6| Step: 4
Training loss: 2.9343791007995605
Validation loss: 2.3840611775716147

Epoch: 6| Step: 5
Training loss: 3.0949268341064453
Validation loss: 2.3813523054122925

Epoch: 6| Step: 6
Training loss: 2.970900535583496
Validation loss: 2.379448930422465

Epoch: 6| Step: 7
Training loss: 2.410191059112549
Validation loss: 2.3758841355641684

Epoch: 6| Step: 8
Training loss: 1.9518678188323975
Validation loss: 2.372242271900177

Epoch: 6| Step: 9
Training loss: 2.4119389057159424
Validation loss: 2.368778109550476

Epoch: 6| Step: 10
Training loss: 2.7606866359710693
Validation loss: 2.370107094446818

Epoch: 6| Step: 11
Training loss: 1.8271039724349976
Validation loss: 2.3635331789652505

Epoch: 6| Step: 12
Training loss: 1.9511442184448242
Validation loss: 2.3619735638300576

Epoch: 6| Step: 13
Training loss: 2.6114587783813477
Validation loss: 2.359725813070933

Epoch: 48| Step: 0
Training loss: 2.064833641052246
Validation loss: 2.356073717276255

Epoch: 6| Step: 1
Training loss: 2.178455352783203
Validation loss: 2.354203979174296

Epoch: 6| Step: 2
Training loss: 2.6251039505004883
Validation loss: 2.356052875518799

Epoch: 6| Step: 3
Training loss: 2.8491263389587402
Validation loss: 2.3485668897628784

Epoch: 6| Step: 4
Training loss: 2.235414981842041
Validation loss: 2.349490463733673

Epoch: 6| Step: 5
Training loss: 2.864349126815796
Validation loss: 2.3480867544809976

Epoch: 6| Step: 6
Training loss: 2.4235970973968506
Validation loss: 2.346718708674113

Epoch: 6| Step: 7
Training loss: 2.271651268005371
Validation loss: 2.345110595226288

Epoch: 6| Step: 8
Training loss: 3.4110469818115234
Validation loss: 2.3416425387064614

Epoch: 6| Step: 9
Training loss: 2.491959571838379
Validation loss: 2.3390202124913535

Epoch: 6| Step: 10
Training loss: 2.4333112239837646
Validation loss: 2.332449436187744

Epoch: 6| Step: 11
Training loss: 1.9483537673950195
Validation loss: 2.33363538980484

Epoch: 6| Step: 12
Training loss: 3.0876669883728027
Validation loss: 2.33391805489858

Epoch: 6| Step: 13
Training loss: 2.4901576042175293
Validation loss: 2.330634872118632

Epoch: 49| Step: 0
Training loss: 2.6201629638671875
Validation loss: 2.325524012247721

Epoch: 6| Step: 1
Training loss: 2.785659074783325
Validation loss: 2.318357268969218

Epoch: 6| Step: 2
Training loss: 2.6171741485595703
Validation loss: 2.3163175185521445

Epoch: 6| Step: 3
Training loss: 2.0599074363708496
Validation loss: 2.3107428749402366

Epoch: 6| Step: 4
Training loss: 3.56937575340271
Validation loss: 2.3106160958607993

Epoch: 6| Step: 5
Training loss: 2.4534494876861572
Validation loss: 2.31207803885142

Epoch: 6| Step: 6
Training loss: 2.549783229827881
Validation loss: 2.320907453695933

Epoch: 6| Step: 7
Training loss: 2.1600608825683594
Validation loss: 2.309738020102183

Epoch: 6| Step: 8
Training loss: 2.157094717025757
Validation loss: 2.3173587719599404

Epoch: 6| Step: 9
Training loss: 2.353870391845703
Validation loss: 2.3193429112434387

Epoch: 6| Step: 10
Training loss: 2.149528980255127
Validation loss: 2.3077492316563926

Epoch: 6| Step: 11
Training loss: 1.8903194665908813
Validation loss: 2.2982988754908242

Epoch: 6| Step: 12
Training loss: 2.944674253463745
Validation loss: 2.2973198890686035

Epoch: 6| Step: 13
Training loss: 2.563854932785034
Validation loss: 2.2975672086079917

Epoch: 50| Step: 0
Training loss: 2.358107089996338
Validation loss: 2.30138236284256

Epoch: 6| Step: 1
Training loss: 2.1516008377075195
Validation loss: 2.3065555890401206

Epoch: 6| Step: 2
Training loss: 2.625650405883789
Validation loss: 2.3182331323623657

Epoch: 6| Step: 3
Training loss: 2.1834659576416016
Validation loss: 2.3118338584899902

Epoch: 6| Step: 4
Training loss: 2.3783669471740723
Validation loss: 2.310283899307251

Epoch: 6| Step: 5
Training loss: 2.185868740081787
Validation loss: 2.3053431113560996

Epoch: 6| Step: 6
Training loss: 2.4513792991638184
Validation loss: 2.302314579486847

Epoch: 6| Step: 7
Training loss: 2.734581708908081
Validation loss: 2.296820819377899

Epoch: 6| Step: 8
Training loss: 2.442923069000244
Validation loss: 2.297198315461477

Epoch: 6| Step: 9
Training loss: 2.66912841796875
Validation loss: 2.2940314213434854

Epoch: 6| Step: 10
Training loss: 2.517005443572998
Validation loss: 2.2909218072891235

Epoch: 6| Step: 11
Training loss: 3.0589911937713623
Validation loss: 2.289873997370402

Epoch: 6| Step: 12
Training loss: 2.3263168334960938
Validation loss: 2.280526081720988

Epoch: 6| Step: 13
Training loss: 2.549696922302246
Validation loss: 2.2746095259984336

Epoch: 51| Step: 0
Training loss: 3.02321720123291
Validation loss: 2.2735260128974915

Epoch: 6| Step: 1
Training loss: 1.9205526113510132
Validation loss: 2.2689641316731772

Epoch: 6| Step: 2
Training loss: 2.2366433143615723
Validation loss: 2.263852854569753

Epoch: 6| Step: 3
Training loss: 2.4915246963500977
Validation loss: 2.2630138198534646

Epoch: 6| Step: 4
Training loss: 2.686189651489258
Validation loss: 2.260013302167257

Epoch: 6| Step: 5
Training loss: 2.0613903999328613
Validation loss: 2.2522385716438293

Epoch: 6| Step: 6
Training loss: 2.9616875648498535
Validation loss: 2.2519418597221375

Epoch: 6| Step: 7
Training loss: 2.4051637649536133
Validation loss: 2.2481327851613364

Epoch: 6| Step: 8
Training loss: 2.8316287994384766
Validation loss: 2.2497013211250305

Epoch: 6| Step: 9
Training loss: 2.2591538429260254
Validation loss: 2.2466957171758017

Epoch: 6| Step: 10
Training loss: 2.15132999420166
Validation loss: 2.2411638299624124

Epoch: 6| Step: 11
Training loss: 2.055427312850952
Validation loss: 2.239223003387451

Epoch: 6| Step: 12
Training loss: 2.0932912826538086
Validation loss: 2.2372450033823648

Epoch: 6| Step: 13
Training loss: 2.7022013664245605
Validation loss: 2.2342055241266885

Epoch: 52| Step: 0
Training loss: 2.3830976486206055
Validation loss: 2.2323583364486694

Epoch: 6| Step: 1
Training loss: 2.318650245666504
Validation loss: 2.2313857078552246

Epoch: 6| Step: 2
Training loss: 2.3735687732696533
Validation loss: 2.22390878200531

Epoch: 6| Step: 3
Training loss: 2.15073299407959
Validation loss: 2.228015422821045

Epoch: 6| Step: 4
Training loss: 2.459610939025879
Validation loss: 2.225583771864573

Epoch: 6| Step: 5
Training loss: 1.976891040802002
Validation loss: 2.220796783765157

Epoch: 6| Step: 6
Training loss: 1.9899265766143799
Validation loss: 2.2200339833895364

Epoch: 6| Step: 7
Training loss: 3.01985502243042
Validation loss: 2.2170929312705994

Epoch: 6| Step: 8
Training loss: 2.2944533824920654
Validation loss: 2.2155470848083496

Epoch: 6| Step: 9
Training loss: 2.589695453643799
Validation loss: 2.2072900931040444

Epoch: 6| Step: 10
Training loss: 1.5049303770065308
Validation loss: 2.211537758509318

Epoch: 6| Step: 11
Training loss: 2.4778308868408203
Validation loss: 2.205247402191162

Epoch: 6| Step: 12
Training loss: 2.8395233154296875
Validation loss: 2.2093594471613565

Epoch: 6| Step: 13
Training loss: 2.9690775871276855
Validation loss: 2.2040393153826394

Epoch: 53| Step: 0
Training loss: 2.869328498840332
Validation loss: 2.2100179195404053

Epoch: 6| Step: 1
Training loss: 2.40083646774292
Validation loss: 2.2012391487757363

Epoch: 6| Step: 2
Training loss: 3.2068309783935547
Validation loss: 2.205161968866984

Epoch: 6| Step: 3
Training loss: 2.5608999729156494
Validation loss: 2.206957221031189

Epoch: 6| Step: 4
Training loss: 2.496478319168091
Validation loss: 2.200679838657379

Epoch: 6| Step: 5
Training loss: 2.7468066215515137
Validation loss: 2.1984193126360574

Epoch: 6| Step: 6
Training loss: 2.087693691253662
Validation loss: 2.196279684702555

Epoch: 6| Step: 7
Training loss: 1.8368314504623413
Validation loss: 2.192100783189138

Epoch: 6| Step: 8
Training loss: 2.417224407196045
Validation loss: 2.1943904161453247

Epoch: 6| Step: 9
Training loss: 1.8623228073120117
Validation loss: 2.1930512189865112

Epoch: 6| Step: 10
Training loss: 2.386728048324585
Validation loss: 2.1962353587150574

Epoch: 6| Step: 11
Training loss: 2.1685194969177246
Validation loss: 2.189098576704661

Epoch: 6| Step: 12
Training loss: 2.0584983825683594
Validation loss: 2.1854174534479776

Epoch: 6| Step: 13
Training loss: 1.9007737636566162
Validation loss: 2.1840593814849854

Epoch: 54| Step: 0
Training loss: 1.9406564235687256
Validation loss: 2.1797791520754495

Epoch: 6| Step: 1
Training loss: 2.4451537132263184
Validation loss: 2.1807363629341125

Epoch: 6| Step: 2
Training loss: 2.4066176414489746
Validation loss: 2.1671745975812278

Epoch: 6| Step: 3
Training loss: 1.8193495273590088
Validation loss: 2.180326680342356

Epoch: 6| Step: 4
Training loss: 2.437349319458008
Validation loss: 2.168396015961965

Epoch: 6| Step: 5
Training loss: 2.6879868507385254
Validation loss: 2.1857152183850608

Epoch: 6| Step: 6
Training loss: 2.258113384246826
Validation loss: 2.1784552733103433

Epoch: 6| Step: 7
Training loss: 2.879342555999756
Validation loss: 2.165800392627716

Epoch: 6| Step: 8
Training loss: 2.1940245628356934
Validation loss: 2.1720399856567383

Epoch: 6| Step: 9
Training loss: 1.460723876953125
Validation loss: 2.1750700076421103

Epoch: 6| Step: 10
Training loss: 2.4617481231689453
Validation loss: 2.1733902295430503

Epoch: 6| Step: 11
Training loss: 2.7870054244995117
Validation loss: 2.173841575781504

Epoch: 6| Step: 12
Training loss: 2.4329395294189453
Validation loss: 2.177691558996836

Epoch: 6| Step: 13
Training loss: 2.531075954437256
Validation loss: 2.173885941505432

Epoch: 55| Step: 0
Training loss: 2.2446417808532715
Validation loss: 2.1762139797210693

Epoch: 6| Step: 1
Training loss: 2.4247214794158936
Validation loss: 2.1709554195404053

Epoch: 6| Step: 2
Training loss: 2.175806999206543
Validation loss: 2.170619487762451

Epoch: 6| Step: 3
Training loss: 1.8737215995788574
Validation loss: 2.171474734942118

Epoch: 6| Step: 4
Training loss: 2.0709733963012695
Validation loss: 2.168603837490082

Epoch: 6| Step: 5
Training loss: 2.7063000202178955
Validation loss: 2.164822061856588

Epoch: 6| Step: 6
Training loss: 2.486697196960449
Validation loss: 2.156833827495575

Epoch: 6| Step: 7
Training loss: 2.6048245429992676
Validation loss: 2.1577443281809487

Epoch: 6| Step: 8
Training loss: 2.149624824523926
Validation loss: 2.1512582500775657

Epoch: 6| Step: 9
Training loss: 2.0387511253356934
Validation loss: 2.142364184061686

Epoch: 6| Step: 10
Training loss: 2.44602370262146
Validation loss: 2.1395163536071777

Epoch: 6| Step: 11
Training loss: 2.1123621463775635
Validation loss: 2.1311152577400208

Epoch: 6| Step: 12
Training loss: 2.6370630264282227
Validation loss: 2.1428303519884744

Epoch: 6| Step: 13
Training loss: 2.6050524711608887
Validation loss: 2.1587392489115396

Epoch: 56| Step: 0
Training loss: 2.271298885345459
Validation loss: 2.15436319510142

Epoch: 6| Step: 1
Training loss: 2.1213884353637695
Validation loss: 2.1557494004567466

Epoch: 6| Step: 2
Training loss: 2.853275775909424
Validation loss: 2.1608884731928506

Epoch: 6| Step: 3
Training loss: 2.622921943664551
Validation loss: 2.1486401557922363

Epoch: 6| Step: 4
Training loss: 3.011284828186035
Validation loss: 2.1431477665901184

Epoch: 6| Step: 5
Training loss: 2.6977992057800293
Validation loss: 2.1432543198267617

Epoch: 6| Step: 6
Training loss: 2.3696279525756836
Validation loss: 2.1490947206815085

Epoch: 6| Step: 7
Training loss: 2.287696361541748
Validation loss: 2.1513242522875466

Epoch: 6| Step: 8
Training loss: 2.0240976810455322
Validation loss: 2.1621611515680947

Epoch: 6| Step: 9
Training loss: 1.72913658618927
Validation loss: 2.1654796401659646

Epoch: 6| Step: 10
Training loss: 2.2101640701293945
Validation loss: 2.1708068251609802

Epoch: 6| Step: 11
Training loss: 2.388545513153076
Validation loss: 2.173042416572571

Epoch: 6| Step: 12
Training loss: 1.3668549060821533
Validation loss: 2.1735177834828696

Epoch: 6| Step: 13
Training loss: 2.675046920776367
Validation loss: 2.1760340531667075

Epoch: 57| Step: 0
Training loss: 2.6005308628082275
Validation loss: 2.1751298308372498

Epoch: 6| Step: 1
Training loss: 2.141916275024414
Validation loss: 2.168501079082489

Epoch: 6| Step: 2
Training loss: 1.7847599983215332
Validation loss: 2.1708878874778748

Epoch: 6| Step: 3
Training loss: 2.5479166507720947
Validation loss: 2.1657065550486245

Epoch: 6| Step: 4
Training loss: 2.1570887565612793
Validation loss: 2.1594333251317344

Epoch: 6| Step: 5
Training loss: 2.5447325706481934
Validation loss: 2.153510332107544

Epoch: 6| Step: 6
Training loss: 2.77653169631958
Validation loss: 2.154080947240194

Epoch: 6| Step: 7
Training loss: 2.213459014892578
Validation loss: 2.1495463252067566

Epoch: 6| Step: 8
Training loss: 2.42413330078125
Validation loss: 2.1485888560613

Epoch: 6| Step: 9
Training loss: 1.8810797929763794
Validation loss: 2.141076842943827

Epoch: 6| Step: 10
Training loss: 2.463433265686035
Validation loss: 2.1363545258839927

Epoch: 6| Step: 11
Training loss: 2.2834010124206543
Validation loss: 2.1372563242912292

Epoch: 6| Step: 12
Training loss: 2.4094972610473633
Validation loss: 2.1348968744277954

Epoch: 6| Step: 13
Training loss: 2.4267678260803223
Validation loss: 2.1298206448554993

Epoch: 58| Step: 0
Training loss: 2.2759265899658203
Validation loss: 2.1272111336390176

Epoch: 6| Step: 1
Training loss: 2.066877841949463
Validation loss: 2.1280038356781006

Epoch: 6| Step: 2
Training loss: 2.8820619583129883
Validation loss: 2.121243158976237

Epoch: 6| Step: 3
Training loss: 2.423980236053467
Validation loss: 2.1204127271970115

Epoch: 6| Step: 4
Training loss: 2.4332876205444336
Validation loss: 2.114894131819407

Epoch: 6| Step: 5
Training loss: 1.8955342769622803
Validation loss: 2.111159006754557

Epoch: 6| Step: 6
Training loss: 2.401247262954712
Validation loss: 2.108872930208842

Epoch: 6| Step: 7
Training loss: 2.092402935028076
Validation loss: 2.1133321126302085

Epoch: 6| Step: 8
Training loss: 2.032078981399536
Validation loss: 2.1041842301686606

Epoch: 6| Step: 9
Training loss: 2.2637133598327637
Validation loss: 2.1051276524861655

Epoch: 6| Step: 10
Training loss: 1.4776889085769653
Validation loss: 2.099743366241455

Epoch: 6| Step: 11
Training loss: 2.1961512565612793
Validation loss: 2.0989696979522705

Epoch: 6| Step: 12
Training loss: 2.590285062789917
Validation loss: 2.1122048695882163

Epoch: 6| Step: 13
Training loss: 2.9759349822998047
Validation loss: 2.114138662815094

Epoch: 59| Step: 0
Training loss: 1.9206186532974243
Validation loss: 2.1043086647987366

Epoch: 6| Step: 1
Training loss: 2.505892753601074
Validation loss: 2.1108864347139993

Epoch: 6| Step: 2
Training loss: 1.766530156135559
Validation loss: 2.092409869035085

Epoch: 6| Step: 3
Training loss: 2.591975688934326
Validation loss: 2.100963910420736

Epoch: 6| Step: 4
Training loss: 2.231771945953369
Validation loss: 2.0881495475769043

Epoch: 6| Step: 5
Training loss: 2.027459144592285
Validation loss: 2.089773734410604

Epoch: 6| Step: 6
Training loss: 2.4287638664245605
Validation loss: 2.0915383299191794

Epoch: 6| Step: 7
Training loss: 2.267366886138916
Validation loss: 2.090219179789225

Epoch: 6| Step: 8
Training loss: 1.749863624572754
Validation loss: 2.093484580516815

Epoch: 6| Step: 9
Training loss: 2.7414357662200928
Validation loss: 2.0912411212921143

Epoch: 6| Step: 10
Training loss: 2.19718599319458
Validation loss: 2.093849023183187

Epoch: 6| Step: 11
Training loss: 2.2730937004089355
Validation loss: 2.0919803977012634

Epoch: 6| Step: 12
Training loss: 2.195730447769165
Validation loss: 2.087732195854187

Epoch: 6| Step: 13
Training loss: 2.670588970184326
Validation loss: 2.090579847494761

Epoch: 60| Step: 0
Training loss: 2.569662094116211
Validation loss: 2.0904307762781777

Epoch: 6| Step: 1
Training loss: 1.7238459587097168
Validation loss: 2.0863720575968423

Epoch: 6| Step: 2
Training loss: 2.4033398628234863
Validation loss: 2.08829394976298

Epoch: 6| Step: 3
Training loss: 2.7829232215881348
Validation loss: 2.0789650281270347

Epoch: 6| Step: 4
Training loss: 2.0775914192199707
Validation loss: 2.0939804514249167

Epoch: 6| Step: 5
Training loss: 2.4643702507019043
Validation loss: 2.1104617516199746

Epoch: 6| Step: 6
Training loss: 2.342529058456421
Validation loss: 2.107869644959768

Epoch: 6| Step: 7
Training loss: 2.6746718883514404
Validation loss: 2.1018152236938477

Epoch: 6| Step: 8
Training loss: 2.321366548538208
Validation loss: 2.076604644457499

Epoch: 6| Step: 9
Training loss: 1.9086614847183228
Validation loss: 2.0843027234077454

Epoch: 6| Step: 10
Training loss: 1.7951693534851074
Validation loss: 2.076059639453888

Epoch: 6| Step: 11
Training loss: 2.6640982627868652
Validation loss: 2.0870386163393655

Epoch: 6| Step: 12
Training loss: 1.6771864891052246
Validation loss: 2.0906922022501626

Epoch: 6| Step: 13
Training loss: 2.2078399658203125
Validation loss: 2.087404787540436

Epoch: 61| Step: 0
Training loss: 1.916806697845459
Validation loss: 2.0885979930559793

Epoch: 6| Step: 1
Training loss: 2.37554931640625
Validation loss: 2.089688003063202

Epoch: 6| Step: 2
Training loss: 2.0735552310943604
Validation loss: 2.0924638708432517

Epoch: 6| Step: 3
Training loss: 2.8100926876068115
Validation loss: 2.0874600410461426

Epoch: 6| Step: 4
Training loss: 2.092528820037842
Validation loss: 2.0932758450508118

Epoch: 6| Step: 5
Training loss: 2.1504220962524414
Validation loss: 2.0876047611236572

Epoch: 6| Step: 6
Training loss: 2.1396772861480713
Validation loss: 2.0862194101015725

Epoch: 6| Step: 7
Training loss: 2.405945062637329
Validation loss: 2.0855774879455566

Epoch: 6| Step: 8
Training loss: 1.7069954872131348
Validation loss: 2.0789164702097573

Epoch: 6| Step: 9
Training loss: 2.707395553588867
Validation loss: 2.072796861330668

Epoch: 6| Step: 10
Training loss: 2.407146453857422
Validation loss: 2.0746787190437317

Epoch: 6| Step: 11
Training loss: 1.8760517835617065
Validation loss: 2.064306298891703

Epoch: 6| Step: 12
Training loss: 2.2102980613708496
Validation loss: 2.053676664829254

Epoch: 6| Step: 13
Training loss: 2.8801050186157227
Validation loss: 2.05586177110672

Epoch: 62| Step: 0
Training loss: 2.6252222061157227
Validation loss: 2.057176887989044

Epoch: 6| Step: 1
Training loss: 2.1127285957336426
Validation loss: 2.0724668900171914

Epoch: 6| Step: 2
Training loss: 2.9338245391845703
Validation loss: 2.063782811164856

Epoch: 6| Step: 3
Training loss: 1.7988345623016357
Validation loss: 2.067517042160034

Epoch: 6| Step: 4
Training loss: 2.3997962474823
Validation loss: 2.0573561787605286

Epoch: 6| Step: 5
Training loss: 1.816123604774475
Validation loss: 2.054811716079712

Epoch: 6| Step: 6
Training loss: 2.1476850509643555
Validation loss: 2.057193477948507

Epoch: 6| Step: 7
Training loss: 2.4488720893859863
Validation loss: 2.0573626359303794

Epoch: 6| Step: 8
Training loss: 2.296717643737793
Validation loss: 2.0684322317441306

Epoch: 6| Step: 9
Training loss: 2.317451238632202
Validation loss: 2.076483190059662

Epoch: 6| Step: 10
Training loss: 2.366011142730713
Validation loss: 2.0698657433191934

Epoch: 6| Step: 11
Training loss: 1.6291160583496094
Validation loss: 2.072118103504181

Epoch: 6| Step: 12
Training loss: 2.1336517333984375
Validation loss: 2.0824584563573203

Epoch: 6| Step: 13
Training loss: 2.3765125274658203
Validation loss: 2.05789452791214

Epoch: 63| Step: 0
Training loss: 2.1402978897094727
Validation loss: 2.044619003931681

Epoch: 6| Step: 1
Training loss: 2.3646652698516846
Validation loss: 2.0489649176597595

Epoch: 6| Step: 2
Training loss: 2.3250491619110107
Validation loss: 2.049758017063141

Epoch: 6| Step: 3
Training loss: 2.0556039810180664
Validation loss: 2.049466609954834

Epoch: 6| Step: 4
Training loss: 1.9821815490722656
Validation loss: 2.0512732664744058

Epoch: 6| Step: 5
Training loss: 2.1690897941589355
Validation loss: 2.0462098916371665

Epoch: 6| Step: 6
Training loss: 2.5713272094726562
Validation loss: 2.050476094086965

Epoch: 6| Step: 7
Training loss: 2.554952383041382
Validation loss: 2.045211970806122

Epoch: 6| Step: 8
Training loss: 2.5681591033935547
Validation loss: 2.055915196736654

Epoch: 6| Step: 9
Training loss: 1.822428822517395
Validation loss: 2.051079591115316

Epoch: 6| Step: 10
Training loss: 2.275770664215088
Validation loss: 2.0537167390187583

Epoch: 6| Step: 11
Training loss: 2.279816150665283
Validation loss: 2.0460185209910073

Epoch: 6| Step: 12
Training loss: 1.9169002771377563
Validation loss: 2.0498124957084656

Epoch: 6| Step: 13
Training loss: 2.1842293739318848
Validation loss: 2.0438921650250754

Epoch: 64| Step: 0
Training loss: 2.07531476020813
Validation loss: 2.0436132550239563

Epoch: 6| Step: 1
Training loss: 1.8267688751220703
Validation loss: 2.0361393292744956

Epoch: 6| Step: 2
Training loss: 2.5660476684570312
Validation loss: 2.0406043330828347

Epoch: 6| Step: 3
Training loss: 2.2335708141326904
Validation loss: 2.0509138703346252

Epoch: 6| Step: 4
Training loss: 2.1236371994018555
Validation loss: 2.057110865910848

Epoch: 6| Step: 5
Training loss: 2.506246566772461
Validation loss: 2.054086605707804

Epoch: 6| Step: 6
Training loss: 1.753576397895813
Validation loss: 2.0499441425005593

Epoch: 6| Step: 7
Training loss: 1.5090043544769287
Validation loss: 2.0487840374310813

Epoch: 6| Step: 8
Training loss: 2.982006549835205
Validation loss: 2.0382267832756042

Epoch: 6| Step: 9
Training loss: 2.62011456489563
Validation loss: 2.037509560585022

Epoch: 6| Step: 10
Training loss: 2.513529062271118
Validation loss: 2.039024551709493

Epoch: 6| Step: 11
Training loss: 2.2752881050109863
Validation loss: 2.03592586517334

Epoch: 6| Step: 12
Training loss: 1.7872824668884277
Validation loss: 2.0419546167055764

Epoch: 6| Step: 13
Training loss: 2.244291067123413
Validation loss: 2.04516593615214

Epoch: 65| Step: 0
Training loss: 1.636653184890747
Validation loss: 2.0433495243390403

Epoch: 6| Step: 1
Training loss: 2.7291007041931152
Validation loss: 2.041032056013743

Epoch: 6| Step: 2
Training loss: 2.5533063411712646
Validation loss: 2.0455423394838967

Epoch: 6| Step: 3
Training loss: 2.4024581909179688
Validation loss: 2.049974183241526

Epoch: 6| Step: 4
Training loss: 2.343440055847168
Validation loss: 2.0520830551783242

Epoch: 6| Step: 5
Training loss: 2.4968156814575195
Validation loss: 2.058958133061727

Epoch: 6| Step: 6
Training loss: 2.732558250427246
Validation loss: 2.050775090853373

Epoch: 6| Step: 7
Training loss: 1.718268871307373
Validation loss: 2.0388243198394775

Epoch: 6| Step: 8
Training loss: 1.877834439277649
Validation loss: 2.0389922857284546

Epoch: 6| Step: 9
Training loss: 2.0037426948547363
Validation loss: 2.0440553228060403

Epoch: 6| Step: 10
Training loss: 2.1124749183654785
Validation loss: 2.0384615461031594

Epoch: 6| Step: 11
Training loss: 1.890251874923706
Validation loss: 2.046523114045461

Epoch: 6| Step: 12
Training loss: 2.8004865646362305
Validation loss: 2.0477936267852783

Epoch: 6| Step: 13
Training loss: 1.5326248407363892
Validation loss: 2.044565260410309

Epoch: 66| Step: 0
Training loss: 2.4437508583068848
Validation loss: 2.043346643447876

Epoch: 6| Step: 1
Training loss: 2.295825242996216
Validation loss: 2.0401440858840942

Epoch: 6| Step: 2
Training loss: 1.966262936592102
Validation loss: 2.0337045788764954

Epoch: 6| Step: 3
Training loss: 2.103803873062134
Validation loss: 2.03585284948349

Epoch: 6| Step: 4
Training loss: 2.3411474227905273
Validation loss: 2.0358725786209106

Epoch: 6| Step: 5
Training loss: 1.9988481998443604
Validation loss: 2.041912615299225

Epoch: 6| Step: 6
Training loss: 2.4672012329101562
Validation loss: 2.0349735220273337

Epoch: 6| Step: 7
Training loss: 1.3504855632781982
Validation loss: 2.042266587416331

Epoch: 6| Step: 8
Training loss: 2.26832914352417
Validation loss: 2.0315629641215005

Epoch: 6| Step: 9
Training loss: 2.6314327716827393
Validation loss: 2.0501241286595664

Epoch: 6| Step: 10
Training loss: 2.450052261352539
Validation loss: 2.0419243971506753

Epoch: 6| Step: 11
Training loss: 1.7593193054199219
Validation loss: 2.0396269957224527

Epoch: 6| Step: 12
Training loss: 2.6632490158081055
Validation loss: 2.0361491044362388

Epoch: 6| Step: 13
Training loss: 1.9986772537231445
Validation loss: 2.0393122831980386

Epoch: 67| Step: 0
Training loss: 2.2238149642944336
Validation loss: 2.0366694728533425

Epoch: 6| Step: 1
Training loss: 2.0111873149871826
Validation loss: 2.039360225200653

Epoch: 6| Step: 2
Training loss: 3.144050359725952
Validation loss: 2.0448912978172302

Epoch: 6| Step: 3
Training loss: 2.957953929901123
Validation loss: 2.0429461002349854

Epoch: 6| Step: 4
Training loss: 1.9940602779388428
Validation loss: 2.033500095208486

Epoch: 6| Step: 5
Training loss: 1.9705331325531006
Validation loss: 2.0302604039510093

Epoch: 6| Step: 6
Training loss: 1.8316264152526855
Validation loss: 2.0376810232798257

Epoch: 6| Step: 7
Training loss: 1.5761704444885254
Validation loss: 2.0446732441584268

Epoch: 6| Step: 8
Training loss: 2.513284206390381
Validation loss: 2.033400446176529

Epoch: 6| Step: 9
Training loss: 2.2522172927856445
Validation loss: 2.039059360822042

Epoch: 6| Step: 10
Training loss: 1.873586654663086
Validation loss: 2.039952258268992

Epoch: 6| Step: 11
Training loss: 2.182755470275879
Validation loss: 2.0414777199427285

Epoch: 6| Step: 12
Training loss: 2.063978433609009
Validation loss: 2.029683073361715

Epoch: 6| Step: 13
Training loss: 1.9193353652954102
Validation loss: 2.0340707103411355

Epoch: 68| Step: 0
Training loss: 3.0473127365112305
Validation loss: 2.036965032418569

Epoch: 6| Step: 1
Training loss: 1.691274881362915
Validation loss: 2.0314615964889526

Epoch: 6| Step: 2
Training loss: 2.7276339530944824
Validation loss: 2.029743015766144

Epoch: 6| Step: 3
Training loss: 2.410264253616333
Validation loss: 2.031284968058268

Epoch: 6| Step: 4
Training loss: 1.8052436113357544
Validation loss: 2.039730747540792

Epoch: 6| Step: 5
Training loss: 2.430452346801758
Validation loss: 2.036439518133799

Epoch: 6| Step: 6
Training loss: 2.185338020324707
Validation loss: 2.0371349851290383

Epoch: 6| Step: 7
Training loss: 2.084934949874878
Validation loss: 2.0394006967544556

Epoch: 6| Step: 8
Training loss: 1.8415632247924805
Validation loss: 2.0376093983650208

Epoch: 6| Step: 9
Training loss: 1.7300001382827759
Validation loss: 2.046714961528778

Epoch: 6| Step: 10
Training loss: 2.180544853210449
Validation loss: 2.0519940853118896

Epoch: 6| Step: 11
Training loss: 2.3227577209472656
Validation loss: 2.0501357316970825

Epoch: 6| Step: 12
Training loss: 1.6841830015182495
Validation loss: 2.0489618380864463

Epoch: 6| Step: 13
Training loss: 2.2191760540008545
Validation loss: 2.051156500975291

Epoch: 69| Step: 0
Training loss: 1.402914047241211
Validation loss: 2.046369751294454

Epoch: 6| Step: 1
Training loss: 1.4193642139434814
Validation loss: 2.038823366165161

Epoch: 6| Step: 2
Training loss: 2.0851619243621826
Validation loss: 2.0502166946729026

Epoch: 6| Step: 3
Training loss: 2.742252826690674
Validation loss: 2.0448164343833923

Epoch: 6| Step: 4
Training loss: 2.5401110649108887
Validation loss: 2.0331114133199057

Epoch: 6| Step: 5
Training loss: 2.0292797088623047
Validation loss: 2.0310649275779724

Epoch: 6| Step: 6
Training loss: 2.5625109672546387
Validation loss: 2.035879294077555

Epoch: 6| Step: 7
Training loss: 1.9488139152526855
Validation loss: 2.043062369028727

Epoch: 6| Step: 8
Training loss: 2.4353737831115723
Validation loss: 2.0374414920806885

Epoch: 6| Step: 9
Training loss: 2.0543830394744873
Validation loss: 2.048205236593882

Epoch: 6| Step: 10
Training loss: 2.5070641040802
Validation loss: 2.0392050743103027

Epoch: 6| Step: 11
Training loss: 1.9633699655532837
Validation loss: 2.052088419596354

Epoch: 6| Step: 12
Training loss: 2.172029733657837
Validation loss: 2.0500550270080566

Epoch: 6| Step: 13
Training loss: 2.614922523498535
Validation loss: 2.0473536054293313

Epoch: 70| Step: 0
Training loss: 1.8601866960525513
Validation loss: 2.0426668922106423

Epoch: 6| Step: 1
Training loss: 2.598525047302246
Validation loss: 2.041327635447184

Epoch: 6| Step: 2
Training loss: 1.8520495891571045
Validation loss: 2.0401841203371682

Epoch: 6| Step: 3
Training loss: 2.3420281410217285
Validation loss: 2.0252116123835244

Epoch: 6| Step: 4
Training loss: 2.117094039916992
Validation loss: 2.0310660203297934

Epoch: 6| Step: 5
Training loss: 1.5532764196395874
Validation loss: 2.0193480253219604

Epoch: 6| Step: 6
Training loss: 1.9881842136383057
Validation loss: 2.01926984389623

Epoch: 6| Step: 7
Training loss: 2.4740169048309326
Validation loss: 2.0143532355626426

Epoch: 6| Step: 8
Training loss: 2.3228845596313477
Validation loss: 2.0206122597058616

Epoch: 6| Step: 9
Training loss: 2.4108104705810547
Validation loss: 2.026873449484507

Epoch: 6| Step: 10
Training loss: 1.981497883796692
Validation loss: 2.0265270471572876

Epoch: 6| Step: 11
Training loss: 2.7437455654144287
Validation loss: 2.021929840246836

Epoch: 6| Step: 12
Training loss: 2.0232739448547363
Validation loss: 2.02889750401179

Epoch: 6| Step: 13
Training loss: 2.1285512447357178
Validation loss: 2.036699871222178

Epoch: 71| Step: 0
Training loss: 1.7389180660247803
Validation loss: 2.0375770131746926

Epoch: 6| Step: 1
Training loss: 2.1857681274414062
Validation loss: 2.0415530602137246

Epoch: 6| Step: 2
Training loss: 2.100182056427002
Validation loss: 2.045192758242289

Epoch: 6| Step: 3
Training loss: 1.4694749116897583
Validation loss: 2.0451276501019797

Epoch: 6| Step: 4
Training loss: 1.8992122411727905
Validation loss: 2.0395083824793496

Epoch: 6| Step: 5
Training loss: 2.650679588317871
Validation loss: 2.030559539794922

Epoch: 6| Step: 6
Training loss: 1.7829253673553467
Validation loss: 2.037901282310486

Epoch: 6| Step: 7
Training loss: 2.165619373321533
Validation loss: 2.03742516040802

Epoch: 6| Step: 8
Training loss: 2.6470260620117188
Validation loss: 2.0451360940933228

Epoch: 6| Step: 9
Training loss: 2.45212721824646
Validation loss: 2.036608119805654

Epoch: 6| Step: 10
Training loss: 1.8498942852020264
Validation loss: 2.0209381381670632

Epoch: 6| Step: 11
Training loss: 2.5126614570617676
Validation loss: 2.026981214682261

Epoch: 6| Step: 12
Training loss: 2.492614269256592
Validation loss: 2.031820853551229

Epoch: 6| Step: 13
Training loss: 2.3296279907226562
Validation loss: 2.029273589452108

Epoch: 72| Step: 0
Training loss: 2.015639305114746
Validation loss: 2.029223879178365

Epoch: 6| Step: 1
Training loss: 2.6632537841796875
Validation loss: 2.034883280595144

Epoch: 6| Step: 2
Training loss: 1.9993438720703125
Validation loss: 2.0255213379859924

Epoch: 6| Step: 3
Training loss: 2.5894131660461426
Validation loss: 2.0269709825515747

Epoch: 6| Step: 4
Training loss: 2.320718765258789
Validation loss: 2.0299627979596457

Epoch: 6| Step: 5
Training loss: 2.141219139099121
Validation loss: 2.0313893953959146

Epoch: 6| Step: 6
Training loss: 2.446943759918213
Validation loss: 2.0438684225082397

Epoch: 6| Step: 7
Training loss: 1.9325668811798096
Validation loss: 2.043224573135376

Epoch: 6| Step: 8
Training loss: 2.507876396179199
Validation loss: 2.0571605960528054

Epoch: 6| Step: 9
Training loss: 1.7541742324829102
Validation loss: 2.0417849818865457

Epoch: 6| Step: 10
Training loss: 1.7811059951782227
Validation loss: 2.0492295622825623

Epoch: 6| Step: 11
Training loss: 1.868891954421997
Validation loss: 2.062502443790436

Epoch: 6| Step: 12
Training loss: 2.2095985412597656
Validation loss: 2.042911867300669

Epoch: 6| Step: 13
Training loss: 2.151827812194824
Validation loss: 2.0442379315694175

Epoch: 73| Step: 0
Training loss: 2.0217981338500977
Validation loss: 2.026646931966146

Epoch: 6| Step: 1
Training loss: 1.641899824142456
Validation loss: 2.03004656235377

Epoch: 6| Step: 2
Training loss: 2.426395893096924
Validation loss: 2.02254585425059

Epoch: 6| Step: 3
Training loss: 2.352417469024658
Validation loss: 2.026295224825541

Epoch: 6| Step: 4
Training loss: 2.244490146636963
Validation loss: 2.033562183380127

Epoch: 6| Step: 5
Training loss: 1.5586392879486084
Validation loss: 2.0314435561498008

Epoch: 6| Step: 6
Training loss: 2.4788568019866943
Validation loss: 2.0263938903808594

Epoch: 6| Step: 7
Training loss: 1.8416088819503784
Validation loss: 2.024583180745443

Epoch: 6| Step: 8
Training loss: 2.295074939727783
Validation loss: 2.02581520875295

Epoch: 6| Step: 9
Training loss: 2.244744300842285
Validation loss: 2.0339125990867615

Epoch: 6| Step: 10
Training loss: 1.9466798305511475
Validation loss: 2.0356489221254983

Epoch: 6| Step: 11
Training loss: 2.5796825885772705
Validation loss: 2.0325494607289634

Epoch: 6| Step: 12
Training loss: 2.137505531311035
Validation loss: 2.0337331891059875

Epoch: 6| Step: 13
Training loss: 2.60312557220459
Validation loss: 2.0252604285875955

Epoch: 74| Step: 0
Training loss: 2.3433356285095215
Validation loss: 2.0252216259638467

Epoch: 6| Step: 1
Training loss: 2.136094093322754
Validation loss: 2.014128545920054

Epoch: 6| Step: 2
Training loss: 2.1970677375793457
Validation loss: 2.0276188453038535

Epoch: 6| Step: 3
Training loss: 2.4283108711242676
Validation loss: 2.0288017789522805

Epoch: 6| Step: 4
Training loss: 2.214613914489746
Validation loss: 2.0150685707728067

Epoch: 6| Step: 5
Training loss: 2.168154716491699
Validation loss: 2.0201760133107505

Epoch: 6| Step: 6
Training loss: 2.60796856880188
Validation loss: 2.0159253080685935

Epoch: 6| Step: 7
Training loss: 2.5106215476989746
Validation loss: 2.0180964867273965

Epoch: 6| Step: 8
Training loss: 2.289665699005127
Validation loss: 2.0249921679496765

Epoch: 6| Step: 9
Training loss: 2.4556941986083984
Validation loss: 2.030228396256765

Epoch: 6| Step: 10
Training loss: 1.8237698078155518
Validation loss: 2.0283267895380654

Epoch: 6| Step: 11
Training loss: 1.9014179706573486
Validation loss: 2.0246733824412027

Epoch: 6| Step: 12
Training loss: 1.4349842071533203
Validation loss: 2.0337392886479697

Epoch: 6| Step: 13
Training loss: 1.7299437522888184
Validation loss: 2.0427496234575906

Epoch: 75| Step: 0
Training loss: 1.9862444400787354
Validation loss: 2.0373819867769876

Epoch: 6| Step: 1
Training loss: 1.543001413345337
Validation loss: 2.0281872948010764

Epoch: 6| Step: 2
Training loss: 2.19791316986084
Validation loss: 2.039213538169861

Epoch: 6| Step: 3
Training loss: 3.0992887020111084
Validation loss: 2.0337069034576416

Epoch: 6| Step: 4
Training loss: 1.7200175523757935
Validation loss: 2.0339683492978415

Epoch: 6| Step: 5
Training loss: 1.5814268589019775
Validation loss: 2.022817234198252

Epoch: 6| Step: 6
Training loss: 2.6353511810302734
Validation loss: 2.0054061015446982

Epoch: 6| Step: 7
Training loss: 2.7249646186828613
Validation loss: 2.003423810005188

Epoch: 6| Step: 8
Training loss: 1.537704348564148
Validation loss: 2.010336677233378

Epoch: 6| Step: 9
Training loss: 2.2244176864624023
Validation loss: 2.0117690364519754

Epoch: 6| Step: 10
Training loss: 2.6858057975769043
Validation loss: 2.0077579418818154

Epoch: 6| Step: 11
Training loss: 2.292051315307617
Validation loss: 1.9972459475199382

Epoch: 6| Step: 12
Training loss: 2.1788125038146973
Validation loss: 2.002003788948059

Epoch: 6| Step: 13
Training loss: 1.9253127574920654
Validation loss: 2.0101743737856546

Epoch: 76| Step: 0
Training loss: 2.1344947814941406
Validation loss: 2.0070581833521524

Epoch: 6| Step: 1
Training loss: 2.5321900844573975
Validation loss: 2.0098835031191506

Epoch: 6| Step: 2
Training loss: 2.0669093132019043
Validation loss: 2.0101715524991355

Epoch: 6| Step: 3
Training loss: 1.931369423866272
Validation loss: 2.0148160457611084

Epoch: 6| Step: 4
Training loss: 2.092989444732666
Validation loss: 2.0145134329795837

Epoch: 6| Step: 5
Training loss: 2.104646682739258
Validation loss: 2.020153065522512

Epoch: 6| Step: 6
Training loss: 2.4702367782592773
Validation loss: 2.0387812654177346

Epoch: 6| Step: 7
Training loss: 2.0141007900238037
Validation loss: 2.029590348402659

Epoch: 6| Step: 8
Training loss: 1.8079673051834106
Validation loss: 2.03214039405187

Epoch: 6| Step: 9
Training loss: 2.0827794075012207
Validation loss: 2.0337464412053428

Epoch: 6| Step: 10
Training loss: 2.216765880584717
Validation loss: 2.054514785607656

Epoch: 6| Step: 11
Training loss: 2.075327157974243
Validation loss: 2.0579973657925925

Epoch: 6| Step: 12
Training loss: 2.224902629852295
Validation loss: 2.0618825952212014

Epoch: 6| Step: 13
Training loss: 2.56776762008667
Validation loss: 2.0608988602956138

Epoch: 77| Step: 0
Training loss: 2.3151803016662598
Validation loss: 2.0402684410413108

Epoch: 6| Step: 1
Training loss: 1.9852728843688965
Validation loss: 2.0478291312853494

Epoch: 6| Step: 2
Training loss: 1.7366586923599243
Validation loss: 2.016541759173075

Epoch: 6| Step: 3
Training loss: 1.91403067111969
Validation loss: 2.0174809098243713

Epoch: 6| Step: 4
Training loss: 2.122497081756592
Validation loss: 2.019615729649862

Epoch: 6| Step: 5
Training loss: 2.6485390663146973
Validation loss: 2.0167671044667563

Epoch: 6| Step: 6
Training loss: 1.8174355030059814
Validation loss: 2.019312043984731

Epoch: 6| Step: 7
Training loss: 1.9333189725875854
Validation loss: 2.017892758051554

Epoch: 6| Step: 8
Training loss: 2.9181156158447266
Validation loss: 2.0202858249346414

Epoch: 6| Step: 9
Training loss: 2.344156265258789
Validation loss: 2.0269511540730796

Epoch: 6| Step: 10
Training loss: 1.8202955722808838
Validation loss: 2.0197636087735495

Epoch: 6| Step: 11
Training loss: 1.8446173667907715
Validation loss: 2.0156684120496116

Epoch: 6| Step: 12
Training loss: 2.4450063705444336
Validation loss: 2.016705791155497

Epoch: 6| Step: 13
Training loss: 2.1764698028564453
Validation loss: 2.033958653608958

Epoch: 78| Step: 0
Training loss: 2.1695847511291504
Validation loss: 2.0313252210617065

Epoch: 6| Step: 1
Training loss: 2.208474636077881
Validation loss: 2.0261478821436563

Epoch: 6| Step: 2
Training loss: 2.304697036743164
Validation loss: 2.0302064418792725

Epoch: 6| Step: 3
Training loss: 1.7571091651916504
Validation loss: 2.0272879600524902

Epoch: 6| Step: 4
Training loss: 2.5821611881256104
Validation loss: 2.024837374687195

Epoch: 6| Step: 5
Training loss: 3.2417726516723633
Validation loss: 2.0165195067723594

Epoch: 6| Step: 6
Training loss: 1.842153549194336
Validation loss: 2.0278081695238748

Epoch: 6| Step: 7
Training loss: 1.5611408948898315
Validation loss: 2.020838419596354

Epoch: 6| Step: 8
Training loss: 2.760162353515625
Validation loss: 2.036031981309255

Epoch: 6| Step: 9
Training loss: 2.1311678886413574
Validation loss: 2.031029005845388

Epoch: 6| Step: 10
Training loss: 2.5136635303497314
Validation loss: 2.0329877138137817

Epoch: 6| Step: 11
Training loss: 1.9104278087615967
Validation loss: 2.024862806002299

Epoch: 6| Step: 12
Training loss: 1.4745105504989624
Validation loss: 2.0293424924214682

Epoch: 6| Step: 13
Training loss: 1.601572871208191
Validation loss: 2.021717389424642

Epoch: 79| Step: 0
Training loss: 1.682388186454773
Validation loss: 2.0283137957255044

Epoch: 6| Step: 1
Training loss: 2.171095371246338
Validation loss: 2.014540751775106

Epoch: 6| Step: 2
Training loss: 2.0166306495666504
Validation loss: 2.016548275947571

Epoch: 6| Step: 3
Training loss: 1.8448413610458374
Validation loss: 2.019584576288859

Epoch: 6| Step: 4
Training loss: 2.6057941913604736
Validation loss: 2.025598704814911

Epoch: 6| Step: 5
Training loss: 2.2302017211914062
Validation loss: 2.0272290309270224

Epoch: 6| Step: 6
Training loss: 2.192260265350342
Validation loss: 2.02518630027771

Epoch: 6| Step: 7
Training loss: 2.0847158432006836
Validation loss: 2.0258635878562927

Epoch: 6| Step: 8
Training loss: 1.946023941040039
Validation loss: 2.040601154168447

Epoch: 6| Step: 9
Training loss: 2.299440383911133
Validation loss: 2.03659721215566

Epoch: 6| Step: 10
Training loss: 2.0238847732543945
Validation loss: 2.036843220392863

Epoch: 6| Step: 11
Training loss: 2.1759109497070312
Validation loss: 2.020515263080597

Epoch: 6| Step: 12
Training loss: 2.381239414215088
Validation loss: 2.026374042034149

Epoch: 6| Step: 13
Training loss: 2.3181159496307373
Validation loss: 2.012410740057627

Epoch: 80| Step: 0
Training loss: 1.5979995727539062
Validation loss: 2.01971964041392

Epoch: 6| Step: 1
Training loss: 1.7206950187683105
Validation loss: 2.016208211580912

Epoch: 6| Step: 2
Training loss: 2.4442970752716064
Validation loss: 2.022027850151062

Epoch: 6| Step: 3
Training loss: 2.288303852081299
Validation loss: 2.017037649949392

Epoch: 6| Step: 4
Training loss: 2.570559024810791
Validation loss: 2.031861146291097

Epoch: 6| Step: 5
Training loss: 1.7979273796081543
Validation loss: 2.0368383129437766

Epoch: 6| Step: 6
Training loss: 2.960066080093384
Validation loss: 2.034330666065216

Epoch: 6| Step: 7
Training loss: 2.2247166633605957
Validation loss: 2.023128926753998

Epoch: 6| Step: 8
Training loss: 2.0010204315185547
Validation loss: 2.034426689147949

Epoch: 6| Step: 9
Training loss: 1.5855333805084229
Validation loss: 2.0304436087608337

Epoch: 6| Step: 10
Training loss: 2.7317848205566406
Validation loss: 2.032815416653951

Epoch: 6| Step: 11
Training loss: 2.314021348953247
Validation loss: 2.0196924408276877

Epoch: 6| Step: 12
Training loss: 2.1334147453308105
Validation loss: 2.0227925976117453

Epoch: 6| Step: 13
Training loss: 1.7735073566436768
Validation loss: 2.0235177477200827

Epoch: 81| Step: 0
Training loss: 2.73994779586792
Validation loss: 2.0186365246772766

Epoch: 6| Step: 1
Training loss: 1.8967692852020264
Validation loss: 2.0258827408154807

Epoch: 6| Step: 2
Training loss: 1.774235486984253
Validation loss: 2.027957042058309

Epoch: 6| Step: 3
Training loss: 1.8623316287994385
Validation loss: 2.031176745891571

Epoch: 6| Step: 4
Training loss: 2.1724281311035156
Validation loss: 2.0324347217877707

Epoch: 6| Step: 5
Training loss: 2.2562499046325684
Validation loss: 2.0461411078770957

Epoch: 6| Step: 6
Training loss: 2.239086151123047
Validation loss: 2.049144148826599

Epoch: 6| Step: 7
Training loss: 2.0890612602233887
Validation loss: 2.0510077476501465

Epoch: 6| Step: 8
Training loss: 2.7881996631622314
Validation loss: 2.0373487869898477

Epoch: 6| Step: 9
Training loss: 2.3137707710266113
Validation loss: 2.0395995378494263

Epoch: 6| Step: 10
Training loss: 2.0296454429626465
Validation loss: 2.0312084754308066

Epoch: 6| Step: 11
Training loss: 2.0199501514434814
Validation loss: 2.0257053772608438

Epoch: 6| Step: 12
Training loss: 1.8745043277740479
Validation loss: 2.0233121713002524

Epoch: 6| Step: 13
Training loss: 1.9646109342575073
Validation loss: 2.017005463441213

Epoch: 82| Step: 0
Training loss: 1.8715324401855469
Validation loss: 2.035600463549296

Epoch: 6| Step: 1
Training loss: 1.9733467102050781
Validation loss: 2.0269571940104165

Epoch: 6| Step: 2
Training loss: 2.3864190578460693
Validation loss: 2.0212735136349997

Epoch: 6| Step: 3
Training loss: 2.0070509910583496
Validation loss: 2.022300978501638

Epoch: 6| Step: 4
Training loss: 1.829340934753418
Validation loss: 2.0248637994130454

Epoch: 6| Step: 5
Training loss: 2.064838409423828
Validation loss: 2.0378619035085044

Epoch: 6| Step: 6
Training loss: 2.217465877532959
Validation loss: 2.032481034596761

Epoch: 6| Step: 7
Training loss: 2.495903491973877
Validation loss: 2.0363815228144326

Epoch: 6| Step: 8
Training loss: 2.1382975578308105
Validation loss: 2.0300432244936624

Epoch: 6| Step: 9
Training loss: 2.5260417461395264
Validation loss: 2.0252222418785095

Epoch: 6| Step: 10
Training loss: 2.168654441833496
Validation loss: 2.05158931016922

Epoch: 6| Step: 11
Training loss: 2.4720685482025146
Validation loss: 2.037488639354706

Epoch: 6| Step: 12
Training loss: 1.9037883281707764
Validation loss: 2.048745890458425

Epoch: 6| Step: 13
Training loss: 1.9046778678894043
Validation loss: 2.0350728233655295

Epoch: 83| Step: 0
Training loss: 2.1581215858459473
Validation loss: 2.0303550362586975

Epoch: 6| Step: 1
Training loss: 2.6103148460388184
Validation loss: 2.018257121245066

Epoch: 6| Step: 2
Training loss: 2.692744255065918
Validation loss: 2.0200278957684836

Epoch: 6| Step: 3
Training loss: 1.6814281940460205
Validation loss: 2.0162477095921836

Epoch: 6| Step: 4
Training loss: 2.024445056915283
Validation loss: 2.014459470907847

Epoch: 6| Step: 5
Training loss: 2.0153284072875977
Validation loss: 2.008047123750051

Epoch: 6| Step: 6
Training loss: 2.865767240524292
Validation loss: 2.012627979119619

Epoch: 6| Step: 7
Training loss: 2.122117519378662
Validation loss: 2.0084590117136636

Epoch: 6| Step: 8
Training loss: 2.162662982940674
Validation loss: 2.017859081427256

Epoch: 6| Step: 9
Training loss: 2.4137802124023438
Validation loss: 2.0138867100079856

Epoch: 6| Step: 10
Training loss: 1.5116028785705566
Validation loss: 2.0196468234062195

Epoch: 6| Step: 11
Training loss: 1.341809630393982
Validation loss: 2.0242162942886353

Epoch: 6| Step: 12
Training loss: 2.057065486907959
Validation loss: 2.0272515018781028

Epoch: 6| Step: 13
Training loss: 2.2743349075317383
Validation loss: 2.0390541354815164

Epoch: 84| Step: 0
Training loss: 1.9382339715957642
Validation loss: 2.0508755842844644

Epoch: 6| Step: 1
Training loss: 1.893157958984375
Validation loss: 2.0683391094207764

Epoch: 6| Step: 2
Training loss: 2.875436782836914
Validation loss: 2.0733248790105185

Epoch: 6| Step: 3
Training loss: 1.8800954818725586
Validation loss: 2.054844379425049

Epoch: 6| Step: 4
Training loss: 2.806424617767334
Validation loss: 2.061090052127838

Epoch: 6| Step: 5
Training loss: 1.9148733615875244
Validation loss: 2.042434553305308

Epoch: 6| Step: 6
Training loss: 2.5058608055114746
Validation loss: 2.0235321124394736

Epoch: 6| Step: 7
Training loss: 2.284001350402832
Validation loss: 2.0178104043006897

Epoch: 6| Step: 8
Training loss: 1.9870787858963013
Validation loss: 2.0117582082748413

Epoch: 6| Step: 9
Training loss: 2.1761958599090576
Validation loss: 2.0168205897013345

Epoch: 6| Step: 10
Training loss: 2.630702495574951
Validation loss: 2.026572902997335

Epoch: 6| Step: 11
Training loss: 1.5161529779434204
Validation loss: 2.0288694699605307

Epoch: 6| Step: 12
Training loss: 2.188248872756958
Validation loss: 2.031019369761149

Epoch: 6| Step: 13
Training loss: 2.3629000186920166
Validation loss: 2.031136671702067

Epoch: 85| Step: 0
Training loss: 1.9423731565475464
Validation loss: 2.036271611849467

Epoch: 6| Step: 1
Training loss: 2.3584518432617188
Validation loss: 2.04114963610967

Epoch: 6| Step: 2
Training loss: 1.996307134628296
Validation loss: 2.0353160897890725

Epoch: 6| Step: 3
Training loss: 1.8280917406082153
Validation loss: 2.0419548749923706

Epoch: 6| Step: 4
Training loss: 2.0274434089660645
Validation loss: 2.0329580505688987

Epoch: 6| Step: 5
Training loss: 3.0492749214172363
Validation loss: 2.039660652478536

Epoch: 6| Step: 6
Training loss: 2.121574878692627
Validation loss: 2.0296676556269326

Epoch: 6| Step: 7
Training loss: 1.7554774284362793
Validation loss: 2.032395124435425

Epoch: 6| Step: 8
Training loss: 2.0554661750793457
Validation loss: 2.029335935910543

Epoch: 6| Step: 9
Training loss: 2.6895527839660645
Validation loss: 2.0225185553232827

Epoch: 6| Step: 10
Training loss: 1.8514717817306519
Validation loss: 2.0337910254796348

Epoch: 6| Step: 11
Training loss: 2.8657023906707764
Validation loss: 2.0292290846506753

Epoch: 6| Step: 12
Training loss: 1.9327890872955322
Validation loss: 2.0188812812169394

Epoch: 6| Step: 13
Training loss: 2.0930542945861816
Validation loss: 2.0174140334129333

Epoch: 86| Step: 0
Training loss: 2.8456978797912598
Validation loss: 2.0162980953852334

Epoch: 6| Step: 1
Training loss: 2.240370750427246
Validation loss: 2.0248168309529624

Epoch: 6| Step: 2
Training loss: 2.47634220123291
Validation loss: 2.019102394580841

Epoch: 6| Step: 3
Training loss: 1.982473373413086
Validation loss: 2.022736211617788

Epoch: 6| Step: 4
Training loss: 2.093290328979492
Validation loss: 2.02859228849411

Epoch: 6| Step: 5
Training loss: 1.5460736751556396
Validation loss: 2.022204041481018

Epoch: 6| Step: 6
Training loss: 1.7813483476638794
Validation loss: 2.0243966579437256

Epoch: 6| Step: 7
Training loss: 1.9051740169525146
Validation loss: 2.025777498881022

Epoch: 6| Step: 8
Training loss: 2.2100563049316406
Validation loss: 2.042663037776947

Epoch: 6| Step: 9
Training loss: 2.3922040462493896
Validation loss: 2.044974128405253

Epoch: 6| Step: 10
Training loss: 1.570671558380127
Validation loss: 2.0569428404172263

Epoch: 6| Step: 11
Training loss: 1.753676414489746
Validation loss: 2.053269326686859

Epoch: 6| Step: 12
Training loss: 3.0663504600524902
Validation loss: 2.051111360390981

Epoch: 6| Step: 13
Training loss: 2.2048025131225586
Validation loss: 2.049198806285858

Epoch: 87| Step: 0
Training loss: 2.099024534225464
Validation loss: 2.037389953931173

Epoch: 6| Step: 1
Training loss: 2.426492214202881
Validation loss: 2.0398205320040383

Epoch: 6| Step: 2
Training loss: 1.988156795501709
Validation loss: 2.029484232266744

Epoch: 6| Step: 3
Training loss: 1.9034204483032227
Validation loss: 2.027776539325714

Epoch: 6| Step: 4
Training loss: 2.004018783569336
Validation loss: 2.0160863399505615

Epoch: 6| Step: 5
Training loss: 2.0073935985565186
Validation loss: 2.017407397429148

Epoch: 6| Step: 6
Training loss: 2.1930184364318848
Validation loss: 2.0114537278811135

Epoch: 6| Step: 7
Training loss: 2.085280418395996
Validation loss: 2.0161632696787515

Epoch: 6| Step: 8
Training loss: 2.5054118633270264
Validation loss: 2.024074375629425

Epoch: 6| Step: 9
Training loss: 2.5166964530944824
Validation loss: 2.016133646170298

Epoch: 6| Step: 10
Training loss: 1.602711796760559
Validation loss: 2.0141830245653787

Epoch: 6| Step: 11
Training loss: 1.5935263633728027
Validation loss: 2.0103611747423806

Epoch: 6| Step: 12
Training loss: 2.8179850578308105
Validation loss: 2.010999619960785

Epoch: 6| Step: 13
Training loss: 2.1754417419433594
Validation loss: 2.0168089469273887

Epoch: 88| Step: 0
Training loss: 1.953057885169983
Validation loss: 2.0153892040252686

Epoch: 6| Step: 1
Training loss: 3.0896661281585693
Validation loss: 2.0200902819633484

Epoch: 6| Step: 2
Training loss: 2.1342406272888184
Validation loss: 2.027406334877014

Epoch: 6| Step: 3
Training loss: 2.1088411808013916
Validation loss: 2.031190017859141

Epoch: 6| Step: 4
Training loss: 2.09415864944458
Validation loss: 2.0394389430681863

Epoch: 6| Step: 5
Training loss: 1.2614128589630127
Validation loss: 2.0327367385228476

Epoch: 6| Step: 6
Training loss: 2.386822462081909
Validation loss: 2.03296951452891

Epoch: 6| Step: 7
Training loss: 2.357743263244629
Validation loss: 2.039766232172648

Epoch: 6| Step: 8
Training loss: 2.2801384925842285
Validation loss: 2.0363669991493225

Epoch: 6| Step: 9
Training loss: 1.8222216367721558
Validation loss: 2.0206549763679504

Epoch: 6| Step: 10
Training loss: 2.0705580711364746
Validation loss: 2.0126363237698874

Epoch: 6| Step: 11
Training loss: 1.7397809028625488
Validation loss: 2.0061326026916504

Epoch: 6| Step: 12
Training loss: 2.3117942810058594
Validation loss: 2.0088366667429605

Epoch: 6| Step: 13
Training loss: 2.5546927452087402
Validation loss: 2.023975412050883

Epoch: 89| Step: 0
Training loss: 2.178236961364746
Validation loss: 2.0192872285842896

Epoch: 6| Step: 1
Training loss: 2.515247344970703
Validation loss: 2.01605361700058

Epoch: 6| Step: 2
Training loss: 2.203545570373535
Validation loss: 2.018526573975881

Epoch: 6| Step: 3
Training loss: 2.1556882858276367
Validation loss: 2.006776213645935

Epoch: 6| Step: 4
Training loss: 1.9591008424758911
Validation loss: 2.020119547843933

Epoch: 6| Step: 5
Training loss: 2.3415608406066895
Validation loss: 2.0232987801233926

Epoch: 6| Step: 6
Training loss: 2.642547607421875
Validation loss: 2.0261639952659607

Epoch: 6| Step: 7
Training loss: 1.7749838829040527
Validation loss: 2.03224116563797

Epoch: 6| Step: 8
Training loss: 1.822969675064087
Validation loss: 2.0323225061098733

Epoch: 6| Step: 9
Training loss: 2.318110942840576
Validation loss: 2.014706472555796

Epoch: 6| Step: 10
Training loss: 1.7776919603347778
Validation loss: 2.023266692956289

Epoch: 6| Step: 11
Training loss: 1.5179486274719238
Validation loss: 2.0285016894340515

Epoch: 6| Step: 12
Training loss: 2.165496826171875
Validation loss: 2.031607985496521

Epoch: 6| Step: 13
Training loss: 2.3698320388793945
Validation loss: 2.0306366880734763

Epoch: 90| Step: 0
Training loss: 2.324702739715576
Validation loss: 2.0196950435638428

Epoch: 6| Step: 1
Training loss: 2.1876919269561768
Validation loss: 2.021997551123301

Epoch: 6| Step: 2
Training loss: 2.192516803741455
Validation loss: 2.0173492431640625

Epoch: 6| Step: 3
Training loss: 2.036619186401367
Validation loss: 2.011171539624532

Epoch: 6| Step: 4
Training loss: 1.9455618858337402
Validation loss: 1.9971310297648113

Epoch: 6| Step: 5
Training loss: 2.0915658473968506
Validation loss: 2.009065647919973

Epoch: 6| Step: 6
Training loss: 2.6662919521331787
Validation loss: 2.003027617931366

Epoch: 6| Step: 7
Training loss: 2.1030020713806152
Validation loss: 2.0021603107452393

Epoch: 6| Step: 8
Training loss: 2.261953592300415
Validation loss: 1.9995799859364827

Epoch: 6| Step: 9
Training loss: 1.9442787170410156
Validation loss: 2.0026666124661765

Epoch: 6| Step: 10
Training loss: 2.2505722045898438
Validation loss: 2.0110798478126526

Epoch: 6| Step: 11
Training loss: 2.130417823791504
Validation loss: 2.011797547340393

Epoch: 6| Step: 12
Training loss: 1.9110753536224365
Validation loss: 2.021883189678192

Epoch: 6| Step: 13
Training loss: 1.8469293117523193
Validation loss: 2.0281821489334106

Epoch: 91| Step: 0
Training loss: 2.3401570320129395
Validation loss: 2.037913759549459

Epoch: 6| Step: 1
Training loss: 2.0421619415283203
Validation loss: 2.03738264242808

Epoch: 6| Step: 2
Training loss: 2.609860420227051
Validation loss: 2.0346054633458457

Epoch: 6| Step: 3
Training loss: 1.8856236934661865
Validation loss: 2.046212613582611

Epoch: 6| Step: 4
Training loss: 1.8922388553619385
Validation loss: 2.0419554511706033

Epoch: 6| Step: 5
Training loss: 1.966094970703125
Validation loss: 2.0304694970448813

Epoch: 6| Step: 6
Training loss: 2.319249391555786
Validation loss: 2.025717039903005

Epoch: 6| Step: 7
Training loss: 2.5150554180145264
Validation loss: 2.01120134194692

Epoch: 6| Step: 8
Training loss: 2.040092945098877
Validation loss: 2.0184728701909385

Epoch: 6| Step: 9
Training loss: 1.863108515739441
Validation loss: 2.012841800848643

Epoch: 6| Step: 10
Training loss: 2.1998682022094727
Validation loss: 2.024792194366455

Epoch: 6| Step: 11
Training loss: 1.6840261220932007
Validation loss: 2.0260125398635864

Epoch: 6| Step: 12
Training loss: 2.616212844848633
Validation loss: 2.0279159943262735

Epoch: 6| Step: 13
Training loss: 2.253418207168579
Validation loss: 2.0229671001434326

Epoch: 92| Step: 0
Training loss: 2.3875715732574463
Validation loss: 2.0247719089190164

Epoch: 6| Step: 1
Training loss: 2.538020133972168
Validation loss: 2.0206253925959268

Epoch: 6| Step: 2
Training loss: 2.3863399028778076
Validation loss: 2.018569548924764

Epoch: 6| Step: 3
Training loss: 2.5206122398376465
Validation loss: 2.0184932549794516

Epoch: 6| Step: 4
Training loss: 2.1841139793395996
Validation loss: 2.0124035080273948

Epoch: 6| Step: 5
Training loss: 1.6960490942001343
Validation loss: 2.019816040992737

Epoch: 6| Step: 6
Training loss: 1.6773473024368286
Validation loss: 2.022684315840403

Epoch: 6| Step: 7
Training loss: 2.338388681411743
Validation loss: 2.023209889729818

Epoch: 6| Step: 8
Training loss: 1.5910577774047852
Validation loss: 2.0152855714162192

Epoch: 6| Step: 9
Training loss: 2.486953020095825
Validation loss: 2.006914178530375

Epoch: 6| Step: 10
Training loss: 2.079819679260254
Validation loss: 2.0056750178337097

Epoch: 6| Step: 11
Training loss: 2.3361735343933105
Validation loss: 2.0085409283638

Epoch: 6| Step: 12
Training loss: 2.4657692909240723
Validation loss: 2.006150722503662

Epoch: 6| Step: 13
Training loss: 1.732927918434143
Validation loss: 2.002245465914408

Epoch: 93| Step: 0
Training loss: 2.399506092071533
Validation loss: 2.011711835861206

Epoch: 6| Step: 1
Training loss: 1.6398086547851562
Validation loss: 2.0211047728856406

Epoch: 6| Step: 2
Training loss: 1.8964812755584717
Validation loss: 2.0249502857526145

Epoch: 6| Step: 3
Training loss: 1.9071483612060547
Validation loss: 2.0337185064951577

Epoch: 6| Step: 4
Training loss: 2.295125961303711
Validation loss: 2.0352532466252646

Epoch: 6| Step: 5
Training loss: 1.7893085479736328
Validation loss: 2.0388583540916443

Epoch: 6| Step: 6
Training loss: 1.7422208786010742
Validation loss: 2.0422568718592324

Epoch: 6| Step: 7
Training loss: 1.8862347602844238
Validation loss: 2.0482420722643533

Epoch: 6| Step: 8
Training loss: 2.794445037841797
Validation loss: 2.0462531646092734

Epoch: 6| Step: 9
Training loss: 2.14997935295105
Validation loss: 2.043297509352366

Epoch: 6| Step: 10
Training loss: 2.7749366760253906
Validation loss: 2.043319841225942

Epoch: 6| Step: 11
Training loss: 2.3299875259399414
Validation loss: 2.055328369140625

Epoch: 6| Step: 12
Training loss: 2.0166990756988525
Validation loss: 2.0474383433659873

Epoch: 6| Step: 13
Training loss: 2.3065545558929443
Validation loss: 2.0462265412012735

Epoch: 94| Step: 0
Training loss: 2.579676389694214
Validation loss: 2.046847879886627

Epoch: 6| Step: 1
Training loss: 2.3151204586029053
Validation loss: 2.0456899404525757

Epoch: 6| Step: 2
Training loss: 2.1233487129211426
Validation loss: 2.031838814417521

Epoch: 6| Step: 3
Training loss: 1.381753921508789
Validation loss: 2.0221012433369956

Epoch: 6| Step: 4
Training loss: 2.042907953262329
Validation loss: 2.0147489110628762

Epoch: 6| Step: 5
Training loss: 2.6330630779266357
Validation loss: 2.0144506295522056

Epoch: 6| Step: 6
Training loss: 2.007587432861328
Validation loss: 2.018184026082357

Epoch: 6| Step: 7
Training loss: 1.9678535461425781
Validation loss: 2.023188889026642

Epoch: 6| Step: 8
Training loss: 1.697110891342163
Validation loss: 2.018821914990743

Epoch: 6| Step: 9
Training loss: 2.7186341285705566
Validation loss: 2.0248236060142517

Epoch: 6| Step: 10
Training loss: 2.2161474227905273
Validation loss: 2.0179114937782288

Epoch: 6| Step: 11
Training loss: 2.062485933303833
Validation loss: 2.0134738882382712

Epoch: 6| Step: 12
Training loss: 2.2220168113708496
Validation loss: 2.011328419049581

Epoch: 6| Step: 13
Training loss: 1.7364501953125
Validation loss: 2.0192712346712747

Epoch: 95| Step: 0
Training loss: 1.9979721307754517
Validation loss: 2.0299054384231567

Epoch: 6| Step: 1
Training loss: 2.1836256980895996
Validation loss: 2.0394530296325684

Epoch: 6| Step: 2
Training loss: 2.013587474822998
Validation loss: 2.0300148526827493

Epoch: 6| Step: 3
Training loss: 2.067082166671753
Validation loss: 2.0360570152600608

Epoch: 6| Step: 4
Training loss: 2.1226871013641357
Validation loss: 2.036406099796295

Epoch: 6| Step: 5
Training loss: 1.8648457527160645
Validation loss: 2.0465662082036338

Epoch: 6| Step: 6
Training loss: 2.1341841220855713
Validation loss: 2.051272908846537

Epoch: 6| Step: 7
Training loss: 2.784092903137207
Validation loss: 2.049762268861135

Epoch: 6| Step: 8
Training loss: 2.208216905593872
Validation loss: 2.0470018784205117

Epoch: 6| Step: 9
Training loss: 2.0687355995178223
Validation loss: 2.0462335546811423

Epoch: 6| Step: 10
Training loss: 1.5334148406982422
Validation loss: 2.045616547266642

Epoch: 6| Step: 11
Training loss: 2.1937997341156006
Validation loss: 2.0301138162612915

Epoch: 6| Step: 12
Training loss: 2.200615644454956
Validation loss: 2.0348269740740457

Epoch: 6| Step: 13
Training loss: 2.141235828399658
Validation loss: 2.02020392815272

Epoch: 96| Step: 0
Training loss: 1.9503668546676636
Validation loss: 2.021086573600769

Epoch: 6| Step: 1
Training loss: 2.0836033821105957
Validation loss: 2.028159777323405

Epoch: 6| Step: 2
Training loss: 2.3351950645446777
Validation loss: 2.0228299299875894

Epoch: 6| Step: 3
Training loss: 1.4563488960266113
Validation loss: 2.0180678367614746

Epoch: 6| Step: 4
Training loss: 1.738527774810791
Validation loss: 2.0162076354026794

Epoch: 6| Step: 5
Training loss: 2.089261531829834
Validation loss: 2.017991006374359

Epoch: 6| Step: 6
Training loss: 2.5650973320007324
Validation loss: 2.0198030869166055

Epoch: 6| Step: 7
Training loss: 1.9696950912475586
Validation loss: 2.0195157130559287

Epoch: 6| Step: 8
Training loss: 2.2842161655426025
Validation loss: 2.0199842850367227

Epoch: 6| Step: 9
Training loss: 2.0100326538085938
Validation loss: 2.02265328168869

Epoch: 6| Step: 10
Training loss: 2.5778968334198
Validation loss: 2.026036540667216

Epoch: 6| Step: 11
Training loss: 2.0859856605529785
Validation loss: 2.019162654876709

Epoch: 6| Step: 12
Training loss: 2.3812096118927
Validation loss: 2.013487617174784

Epoch: 6| Step: 13
Training loss: 2.0990543365478516
Validation loss: 2.020254373550415

Epoch: 97| Step: 0
Training loss: 2.4036455154418945
Validation loss: 2.0220361153284707

Epoch: 6| Step: 1
Training loss: 1.8232401609420776
Validation loss: 2.0161046981811523

Epoch: 6| Step: 2
Training loss: 1.8961752653121948
Validation loss: 2.0336186289787292

Epoch: 6| Step: 3
Training loss: 2.4925379753112793
Validation loss: 2.028648296991984

Epoch: 6| Step: 4
Training loss: 1.4969803094863892
Validation loss: 2.0266834100087485

Epoch: 6| Step: 5
Training loss: 2.255868434906006
Validation loss: 2.0377601186434426

Epoch: 6| Step: 6
Training loss: 2.5478272438049316
Validation loss: 2.038207987944285

Epoch: 6| Step: 7
Training loss: 2.148515224456787
Validation loss: 2.0363470911979675

Epoch: 6| Step: 8
Training loss: 1.9421794414520264
Validation loss: 2.0446868936220803

Epoch: 6| Step: 9
Training loss: 2.1640989780426025
Validation loss: 2.0321707924207053

Epoch: 6| Step: 10
Training loss: 1.9613277912139893
Validation loss: 2.0385874112447104

Epoch: 6| Step: 11
Training loss: 2.243986129760742
Validation loss: 2.029666006565094

Epoch: 6| Step: 12
Training loss: 2.0273663997650146
Validation loss: 2.0552648305892944

Epoch: 6| Step: 13
Training loss: 2.1329002380371094
Validation loss: 2.034402012825012

Epoch: 98| Step: 0
Training loss: 2.4506349563598633
Validation loss: 2.0262791315714517

Epoch: 6| Step: 1
Training loss: 1.6508766412734985
Validation loss: 2.0249377489089966

Epoch: 6| Step: 2
Training loss: 1.8806753158569336
Validation loss: 2.0180333058039346

Epoch: 6| Step: 3
Training loss: 1.883011817932129
Validation loss: 2.0226173400878906

Epoch: 6| Step: 4
Training loss: 2.1557703018188477
Validation loss: 2.027731418609619

Epoch: 6| Step: 5
Training loss: 1.933618426322937
Validation loss: 2.0184688766797385

Epoch: 6| Step: 6
Training loss: 2.34047794342041
Validation loss: 2.024979829788208

Epoch: 6| Step: 7
Training loss: 2.70817494392395
Validation loss: 2.0134915113449097

Epoch: 6| Step: 8
Training loss: 2.2551662921905518
Validation loss: 2.016179144382477

Epoch: 6| Step: 9
Training loss: 1.6335718631744385
Validation loss: 2.008768916130066

Epoch: 6| Step: 10
Training loss: 1.8161916732788086
Validation loss: 2.007131735483805

Epoch: 6| Step: 11
Training loss: 2.5940282344818115
Validation loss: 2.0043750206629434

Epoch: 6| Step: 12
Training loss: 2.066659927368164
Validation loss: 2.0269762674967446

Epoch: 6| Step: 13
Training loss: 2.155524253845215
Validation loss: 2.034209907054901

Epoch: 99| Step: 0
Training loss: 2.144915819168091
Validation loss: 2.0343080361684165

Epoch: 6| Step: 1
Training loss: 1.8235780000686646
Validation loss: 2.0370004177093506

Epoch: 6| Step: 2
Training loss: 2.0579142570495605
Validation loss: 2.0310908555984497

Epoch: 6| Step: 3
Training loss: 2.0833137035369873
Validation loss: 2.0217849016189575

Epoch: 6| Step: 4
Training loss: 2.0086183547973633
Validation loss: 2.0284682512283325

Epoch: 6| Step: 5
Training loss: 2.91021728515625
Validation loss: 2.021010180314382

Epoch: 6| Step: 6
Training loss: 2.1192731857299805
Validation loss: 2.0190380613009133

Epoch: 6| Step: 7
Training loss: 2.1699376106262207
Validation loss: 2.026579717795054

Epoch: 6| Step: 8
Training loss: 1.9914089441299438
Validation loss: 2.026177704334259

Epoch: 6| Step: 9
Training loss: 2.530282974243164
Validation loss: 2.022876183191935

Epoch: 6| Step: 10
Training loss: 1.6510260105133057
Validation loss: 2.0326390067736306

Epoch: 6| Step: 11
Training loss: 1.7791709899902344
Validation loss: 2.034167389074961

Epoch: 6| Step: 12
Training loss: 1.682973861694336
Validation loss: 2.0427451531092324

Epoch: 6| Step: 13
Training loss: 2.4922451972961426
Validation loss: 2.0329460899035134

Epoch: 100| Step: 0
Training loss: 2.181204319000244
Validation loss: 2.02789314587911

Epoch: 6| Step: 1
Training loss: 1.8371524810791016
Validation loss: 2.0315029422442117

Epoch: 6| Step: 2
Training loss: 1.8691418170928955
Validation loss: 2.0285561084747314

Epoch: 6| Step: 3
Training loss: 2.1600756645202637
Validation loss: 2.0418736735979715

Epoch: 6| Step: 4
Training loss: 1.6048583984375
Validation loss: 2.0350210666656494

Epoch: 6| Step: 5
Training loss: 2.013429641723633
Validation loss: 2.0382646123568215

Epoch: 6| Step: 6
Training loss: 2.169632911682129
Validation loss: 2.0389225284258523

Epoch: 6| Step: 7
Training loss: 2.102968692779541
Validation loss: 2.039879580338796

Epoch: 6| Step: 8
Training loss: 2.611746311187744
Validation loss: 2.0339735945065818

Epoch: 6| Step: 9
Training loss: 2.691929578781128
Validation loss: 2.037605663140615

Epoch: 6| Step: 10
Training loss: 1.7263652086257935
Validation loss: 2.028663376967112

Epoch: 6| Step: 11
Training loss: 2.402053117752075
Validation loss: 2.0278877218564353

Epoch: 6| Step: 12
Training loss: 1.5109457969665527
Validation loss: 2.033860703309377

Epoch: 6| Step: 13
Training loss: 2.4181671142578125
Validation loss: 2.027942140897115

Epoch: 101| Step: 0
Training loss: 1.3428906202316284
Validation loss: 2.0295337041219077

Epoch: 6| Step: 1
Training loss: 1.7170424461364746
Validation loss: 2.033200740814209

Epoch: 6| Step: 2
Training loss: 2.5020341873168945
Validation loss: 2.0201336145401

Epoch: 6| Step: 3
Training loss: 1.7503876686096191
Validation loss: 2.0332119862238565

Epoch: 6| Step: 4
Training loss: 2.246736764907837
Validation loss: 2.024398942788442

Epoch: 6| Step: 5
Training loss: 2.240187644958496
Validation loss: 2.030808945496877

Epoch: 6| Step: 6
Training loss: 2.525324583053589
Validation loss: 2.037580927213033

Epoch: 6| Step: 7
Training loss: 1.931603193283081
Validation loss: 2.043780783812205

Epoch: 6| Step: 8
Training loss: 2.2889578342437744
Validation loss: 2.0379786292711892

Epoch: 6| Step: 9
Training loss: 2.373096227645874
Validation loss: 2.0371368527412415

Epoch: 6| Step: 10
Training loss: 3.0358853340148926
Validation loss: 2.031996170679728

Epoch: 6| Step: 11
Training loss: 2.164092779159546
Validation loss: 2.0377956430117288

Epoch: 6| Step: 12
Training loss: 1.7904118299484253
Validation loss: 2.0486199061075845

Epoch: 6| Step: 13
Training loss: 1.5845940113067627
Validation loss: 2.044059932231903

Epoch: 102| Step: 0
Training loss: 2.056802749633789
Validation loss: 2.054618159929911

Epoch: 6| Step: 1
Training loss: 1.1565717458724976
Validation loss: 2.0358261466026306

Epoch: 6| Step: 2
Training loss: 2.505410671234131
Validation loss: 2.03761488199234

Epoch: 6| Step: 3
Training loss: 2.2668356895446777
Validation loss: 2.0289269288380942

Epoch: 6| Step: 4
Training loss: 2.4588513374328613
Validation loss: 2.0293403466542563

Epoch: 6| Step: 5
Training loss: 2.6913013458251953
Validation loss: 2.016085207462311

Epoch: 6| Step: 6
Training loss: 2.071345090866089
Validation loss: 2.0209930737813315

Epoch: 6| Step: 7
Training loss: 2.1036953926086426
Validation loss: 2.0188111662864685

Epoch: 6| Step: 8
Training loss: 2.7895240783691406
Validation loss: 2.021069963773092

Epoch: 6| Step: 9
Training loss: 1.9777860641479492
Validation loss: 2.0211413900057473

Epoch: 6| Step: 10
Training loss: 1.803191065788269
Validation loss: 2.024234116077423

Epoch: 6| Step: 11
Training loss: 1.6489038467407227
Validation loss: 2.0277462204297385

Epoch: 6| Step: 12
Training loss: 2.186887502670288
Validation loss: 2.0235284169514975

Epoch: 6| Step: 13
Training loss: 1.9897103309631348
Validation loss: 2.0171378453572593

Epoch: 103| Step: 0
Training loss: 2.4342501163482666
Validation loss: 2.0169469912846885

Epoch: 6| Step: 1
Training loss: 1.978509545326233
Validation loss: 2.024817188580831

Epoch: 6| Step: 2
Training loss: 1.8697149753570557
Validation loss: 2.0247071782747903

Epoch: 6| Step: 3
Training loss: 2.0323190689086914
Validation loss: 2.0351374546686807

Epoch: 6| Step: 4
Training loss: 2.3480725288391113
Validation loss: 2.035595198472341

Epoch: 6| Step: 5
Training loss: 2.0066936016082764
Validation loss: 2.0409478147824607

Epoch: 6| Step: 6
Training loss: 2.206763982772827
Validation loss: 2.0557030042012534

Epoch: 6| Step: 7
Training loss: 1.5254559516906738
Validation loss: 2.048399051030477

Epoch: 6| Step: 8
Training loss: 2.2454402446746826
Validation loss: 2.050385296344757

Epoch: 6| Step: 9
Training loss: 2.346935987472534
Validation loss: 2.0435397624969482

Epoch: 6| Step: 10
Training loss: 1.6671583652496338
Validation loss: 2.0424442688624063

Epoch: 6| Step: 11
Training loss: 2.399425983428955
Validation loss: 2.0440492232640586

Epoch: 6| Step: 12
Training loss: 2.3635528087615967
Validation loss: 2.029078563054403

Epoch: 6| Step: 13
Training loss: 2.1060802936553955
Validation loss: 2.016901512940725

Epoch: 104| Step: 0
Training loss: 2.114276885986328
Validation loss: 2.011367440223694

Epoch: 6| Step: 1
Training loss: 2.131772994995117
Validation loss: 2.0109330217043557

Epoch: 6| Step: 2
Training loss: 1.9671231508255005
Validation loss: 2.0062710642814636

Epoch: 6| Step: 3
Training loss: 1.7875723838806152
Validation loss: 2.0180100003878274

Epoch: 6| Step: 4
Training loss: 2.209385633468628
Validation loss: 2.0267290472984314

Epoch: 6| Step: 5
Training loss: 2.4711620807647705
Validation loss: 2.0274850328763327

Epoch: 6| Step: 6
Training loss: 2.3600499629974365
Validation loss: 2.028888483842214

Epoch: 6| Step: 7
Training loss: 2.0596871376037598
Validation loss: 2.0175930857658386

Epoch: 6| Step: 8
Training loss: 1.927230954170227
Validation loss: 2.017283538977305

Epoch: 6| Step: 9
Training loss: 1.8948765993118286
Validation loss: 2.010271747907003

Epoch: 6| Step: 10
Training loss: 2.08308744430542
Validation loss: 2.018489400545756

Epoch: 6| Step: 11
Training loss: 1.9827316999435425
Validation loss: 2.0187943379084268

Epoch: 6| Step: 12
Training loss: 2.334162712097168
Validation loss: 2.028256595134735

Epoch: 6| Step: 13
Training loss: 1.9815057516098022
Validation loss: 2.0247881213823953

Epoch: 105| Step: 0
Training loss: 2.241093635559082
Validation loss: 2.028743306795756

Epoch: 6| Step: 1
Training loss: 1.9287060499191284
Validation loss: 2.0289149085680642

Epoch: 6| Step: 2
Training loss: 2.1970691680908203
Validation loss: 2.0292933781941733

Epoch: 6| Step: 3
Training loss: 1.7294801473617554
Validation loss: 2.03239897886912

Epoch: 6| Step: 4
Training loss: 2.0086019039154053
Validation loss: 2.029778242111206

Epoch: 6| Step: 5
Training loss: 2.5278115272521973
Validation loss: 2.034403642018636

Epoch: 6| Step: 6
Training loss: 1.56182861328125
Validation loss: 2.036415676275889

Epoch: 6| Step: 7
Training loss: 2.027644634246826
Validation loss: 2.040473461151123

Epoch: 6| Step: 8
Training loss: 2.425076723098755
Validation loss: 2.02099339167277

Epoch: 6| Step: 9
Training loss: 2.0956363677978516
Validation loss: 2.0241923133532205

Epoch: 6| Step: 10
Training loss: 1.6719012260437012
Validation loss: 2.025564173857371

Epoch: 6| Step: 11
Training loss: 2.308377742767334
Validation loss: 2.0250989198684692

Epoch: 6| Step: 12
Training loss: 2.175189971923828
Validation loss: 2.014366070429484

Epoch: 6| Step: 13
Training loss: 2.3795008659362793
Validation loss: 2.0121395190556846

Epoch: 106| Step: 0
Training loss: 1.6491843461990356
Validation loss: 2.0137816270192466

Epoch: 6| Step: 1
Training loss: 1.8721563816070557
Validation loss: 2.013464947541555

Epoch: 6| Step: 2
Training loss: 1.7124613523483276
Validation loss: 2.010060667991638

Epoch: 6| Step: 3
Training loss: 2.4068751335144043
Validation loss: 2.0282466610272727

Epoch: 6| Step: 4
Training loss: 2.335360527038574
Validation loss: 2.049782117207845

Epoch: 6| Step: 5
Training loss: 2.1818647384643555
Validation loss: 2.046847681204478

Epoch: 6| Step: 6
Training loss: 2.325014114379883
Validation loss: 2.0454839865366616

Epoch: 6| Step: 7
Training loss: 1.825339674949646
Validation loss: 2.048900028069814

Epoch: 6| Step: 8
Training loss: 2.4890499114990234
Validation loss: 2.0474881728490195

Epoch: 6| Step: 9
Training loss: 1.8318061828613281
Validation loss: 2.033314804236094

Epoch: 6| Step: 10
Training loss: 1.939422369003296
Validation loss: 2.020971715450287

Epoch: 6| Step: 11
Training loss: 2.06223726272583
Validation loss: 2.0175825158754983

Epoch: 6| Step: 12
Training loss: 1.9845976829528809
Validation loss: 2.011348227659861

Epoch: 6| Step: 13
Training loss: 2.7835350036621094
Validation loss: 2.0056451559066772

Epoch: 107| Step: 0
Training loss: 2.1422159671783447
Validation loss: 2.0138434171676636

Epoch: 6| Step: 1
Training loss: 2.351050853729248
Validation loss: 2.014159599939982

Epoch: 6| Step: 2
Training loss: 2.737022876739502
Validation loss: 2.019549608230591

Epoch: 6| Step: 3
Training loss: 2.0731401443481445
Validation loss: 2.035264035065969

Epoch: 6| Step: 4
Training loss: 1.3179517984390259
Validation loss: 2.0499256253242493

Epoch: 6| Step: 5
Training loss: 2.1641643047332764
Validation loss: 2.037538230419159

Epoch: 6| Step: 6
Training loss: 1.7968295812606812
Validation loss: 2.052222967147827

Epoch: 6| Step: 7
Training loss: 1.8192615509033203
Validation loss: 2.061716536680857

Epoch: 6| Step: 8
Training loss: 2.2695415019989014
Validation loss: 2.0525523026784263

Epoch: 6| Step: 9
Training loss: 1.9633276462554932
Validation loss: 2.0468361576398215

Epoch: 6| Step: 10
Training loss: 2.223140239715576
Validation loss: 2.0396780371665955

Epoch: 6| Step: 11
Training loss: 2.5584936141967773
Validation loss: 2.046029806137085

Epoch: 6| Step: 12
Training loss: 1.921342134475708
Validation loss: 2.0335341493288674

Epoch: 6| Step: 13
Training loss: 1.879514217376709
Validation loss: 2.0113112131754556

Epoch: 108| Step: 0
Training loss: 2.4143199920654297
Validation loss: 2.007624328136444

Epoch: 6| Step: 1
Training loss: 2.68682599067688
Validation loss: 2.01137242714564

Epoch: 6| Step: 2
Training loss: 1.529627799987793
Validation loss: 2.0093055168787637

Epoch: 6| Step: 3
Training loss: 1.6975032091140747
Validation loss: 2.014793813228607

Epoch: 6| Step: 4
Training loss: 2.6898393630981445
Validation loss: 2.0136290192604065

Epoch: 6| Step: 5
Training loss: 2.1386959552764893
Validation loss: 2.0340212980906167

Epoch: 6| Step: 6
Training loss: 1.686521053314209
Validation loss: 2.0318183501561484

Epoch: 6| Step: 7
Training loss: 1.9647550582885742
Validation loss: 2.0378923416137695

Epoch: 6| Step: 8
Training loss: 2.17617130279541
Validation loss: 2.041768729686737

Epoch: 6| Step: 9
Training loss: 2.0902085304260254
Validation loss: 2.05258580048879

Epoch: 6| Step: 10
Training loss: 2.026482582092285
Validation loss: 2.041082719961802

Epoch: 6| Step: 11
Training loss: 2.8667802810668945
Validation loss: 2.051105539004008

Epoch: 6| Step: 12
Training loss: 1.3812626600265503
Validation loss: 2.06804625193278

Epoch: 6| Step: 13
Training loss: 1.7725067138671875
Validation loss: 2.0592575073242188

Epoch: 109| Step: 0
Training loss: 2.0657293796539307
Validation loss: 2.066105008125305

Epoch: 6| Step: 1
Training loss: 2.300112724304199
Validation loss: 2.0591169198354087

Epoch: 6| Step: 2
Training loss: 1.9659450054168701
Validation loss: 2.056804041067759

Epoch: 6| Step: 3
Training loss: 1.6682956218719482
Validation loss: 2.050528029600779

Epoch: 6| Step: 4
Training loss: 1.9629372358322144
Validation loss: 2.036518673102061

Epoch: 6| Step: 5
Training loss: 2.0603408813476562
Validation loss: 2.0331213076909385

Epoch: 6| Step: 6
Training loss: 2.5767879486083984
Validation loss: 2.0352137088775635

Epoch: 6| Step: 7
Training loss: 2.033505916595459
Validation loss: 2.0373308261235556

Epoch: 6| Step: 8
Training loss: 1.6873717308044434
Validation loss: 2.029321571191152

Epoch: 6| Step: 9
Training loss: 2.6299166679382324
Validation loss: 2.0322213172912598

Epoch: 6| Step: 10
Training loss: 1.6704121828079224
Validation loss: 2.035333196322123

Epoch: 6| Step: 11
Training loss: 2.0485777854919434
Validation loss: 2.0446988542874656

Epoch: 6| Step: 12
Training loss: 2.231560707092285
Validation loss: 2.041161676247915

Epoch: 6| Step: 13
Training loss: 2.3369877338409424
Validation loss: 2.0482414762179055

Epoch: 110| Step: 0
Training loss: 1.815848469734192
Validation loss: 2.0499693552652993

Epoch: 6| Step: 1
Training loss: 1.7648651599884033
Validation loss: 2.0414549509684243

Epoch: 6| Step: 2
Training loss: 1.8338227272033691
Validation loss: 2.0588715275128684

Epoch: 6| Step: 3
Training loss: 2.3819961547851562
Validation loss: 2.061432937781016

Epoch: 6| Step: 4
Training loss: 2.444331407546997
Validation loss: 2.07432887951533

Epoch: 6| Step: 5
Training loss: 2.1288506984710693
Validation loss: 2.05111430088679

Epoch: 6| Step: 6
Training loss: 1.8626846075057983
Validation loss: 2.052621146043142

Epoch: 6| Step: 7
Training loss: 2.0493767261505127
Validation loss: 2.048470119635264

Epoch: 6| Step: 8
Training loss: 1.96225905418396
Validation loss: 2.039756953716278

Epoch: 6| Step: 9
Training loss: 1.8581340312957764
Validation loss: 2.039585053920746

Epoch: 6| Step: 10
Training loss: 2.141899585723877
Validation loss: 2.0354884465535483

Epoch: 6| Step: 11
Training loss: 2.184636354446411
Validation loss: 2.029006063938141

Epoch: 6| Step: 12
Training loss: 2.8488287925720215
Validation loss: 2.026356836160024

Epoch: 6| Step: 13
Training loss: 1.717552900314331
Validation loss: 2.032953063646952

Epoch: 111| Step: 0
Training loss: 1.5015802383422852
Validation loss: 2.0314519802729287

Epoch: 6| Step: 1
Training loss: 2.191155433654785
Validation loss: 2.0417142510414124

Epoch: 6| Step: 2
Training loss: 1.9947845935821533
Validation loss: 2.0418256521224976

Epoch: 6| Step: 3
Training loss: 2.0536561012268066
Validation loss: 2.0394096771876016

Epoch: 6| Step: 4
Training loss: 1.8494150638580322
Validation loss: 2.047370493412018

Epoch: 6| Step: 5
Training loss: 2.208009958267212
Validation loss: 2.0565288066864014

Epoch: 6| Step: 6
Training loss: 2.762624740600586
Validation loss: 2.0417047341664634

Epoch: 6| Step: 7
Training loss: 2.1396985054016113
Validation loss: 2.0505131085713706

Epoch: 6| Step: 8
Training loss: 1.9396027326583862
Validation loss: 2.0445237159729004

Epoch: 6| Step: 9
Training loss: 2.230067729949951
Validation loss: 2.049313008785248

Epoch: 6| Step: 10
Training loss: 1.718839406967163
Validation loss: 2.0422714153925576

Epoch: 6| Step: 11
Training loss: 2.071113109588623
Validation loss: 2.0327600240707397

Epoch: 6| Step: 12
Training loss: 1.992763876914978
Validation loss: 2.0396050810813904

Epoch: 6| Step: 13
Training loss: 2.3483850955963135
Validation loss: 2.0412946939468384

Epoch: 112| Step: 0
Training loss: 1.8335932493209839
Validation loss: 2.028582592805227

Epoch: 6| Step: 1
Training loss: 2.0172908306121826
Validation loss: 2.0349241495132446

Epoch: 6| Step: 2
Training loss: 2.0092835426330566
Validation loss: 2.0293195843696594

Epoch: 6| Step: 3
Training loss: 1.9795665740966797
Validation loss: 2.0344943602879844

Epoch: 6| Step: 4
Training loss: 1.6968379020690918
Validation loss: 2.0405007799466452

Epoch: 6| Step: 5
Training loss: 2.2578296661376953
Validation loss: 2.034744143486023

Epoch: 6| Step: 6
Training loss: 2.5206961631774902
Validation loss: 2.0394950111707053

Epoch: 6| Step: 7
Training loss: 2.603837490081787
Validation loss: 2.0419382452964783

Epoch: 6| Step: 8
Training loss: 2.0078935623168945
Validation loss: 2.0609852274258933

Epoch: 6| Step: 9
Training loss: 2.0901951789855957
Validation loss: 2.0555561780929565

Epoch: 6| Step: 10
Training loss: 2.189859390258789
Validation loss: 2.0535729130109153

Epoch: 6| Step: 11
Training loss: 2.159437894821167
Validation loss: 2.056830624739329

Epoch: 6| Step: 12
Training loss: 2.2135839462280273
Validation loss: 2.0527995228767395

Epoch: 6| Step: 13
Training loss: 1.983482003211975
Validation loss: 2.069079359372457

Epoch: 113| Step: 0
Training loss: 1.774954080581665
Validation loss: 2.061849852403005

Epoch: 6| Step: 1
Training loss: 2.032464027404785
Validation loss: 2.039453446865082

Epoch: 6| Step: 2
Training loss: 1.504551649093628
Validation loss: 2.035043160120646

Epoch: 6| Step: 3
Training loss: 2.5624213218688965
Validation loss: 2.020823836326599

Epoch: 6| Step: 4
Training loss: 2.3736534118652344
Validation loss: 2.015438656012217

Epoch: 6| Step: 5
Training loss: 1.8259048461914062
Validation loss: 2.0226131478945413

Epoch: 6| Step: 6
Training loss: 2.435393810272217
Validation loss: 2.0206649700800576

Epoch: 6| Step: 7
Training loss: 1.9305338859558105
Validation loss: 2.040679136912028

Epoch: 6| Step: 8
Training loss: 1.6701405048370361
Validation loss: 2.0350920955340066

Epoch: 6| Step: 9
Training loss: 2.4199728965759277
Validation loss: 2.027787705262502

Epoch: 6| Step: 10
Training loss: 1.7967361211776733
Validation loss: 2.0113961497942605

Epoch: 6| Step: 11
Training loss: 2.446694850921631
Validation loss: 2.026021122932434

Epoch: 6| Step: 12
Training loss: 2.4929566383361816
Validation loss: 2.0129535595575967

Epoch: 6| Step: 13
Training loss: 2.389364242553711
Validation loss: 2.0049718817075095

Epoch: 114| Step: 0
Training loss: 1.9596750736236572
Validation loss: 2.0001996755599976

Epoch: 6| Step: 1
Training loss: 1.7976250648498535
Validation loss: 2.0005489587783813

Epoch: 6| Step: 2
Training loss: 2.3797831535339355
Validation loss: 2.0135364731152854

Epoch: 6| Step: 3
Training loss: 2.3769917488098145
Validation loss: 2.0078787406285605

Epoch: 6| Step: 4
Training loss: 2.3190619945526123
Validation loss: 2.0212855537732444

Epoch: 6| Step: 5
Training loss: 2.304368257522583
Validation loss: 2.0237493316332498

Epoch: 6| Step: 6
Training loss: 2.2915351390838623
Validation loss: 2.0210051933924356

Epoch: 6| Step: 7
Training loss: 1.5178207159042358
Validation loss: 2.026918272177378

Epoch: 6| Step: 8
Training loss: 2.0428075790405273
Validation loss: 2.0226956605911255

Epoch: 6| Step: 9
Training loss: 2.5416951179504395
Validation loss: 2.0138381719589233

Epoch: 6| Step: 10
Training loss: 2.0566470623016357
Validation loss: 2.014076312383016

Epoch: 6| Step: 11
Training loss: 1.9982779026031494
Validation loss: 2.029922068119049

Epoch: 6| Step: 12
Training loss: 1.8860256671905518
Validation loss: 2.0283328890800476

Epoch: 6| Step: 13
Training loss: 1.9871742725372314
Validation loss: 2.040048837661743

Epoch: 115| Step: 0
Training loss: 2.174560308456421
Validation loss: 2.0493091344833374

Epoch: 6| Step: 1
Training loss: 2.373688220977783
Validation loss: 2.0529529253641763

Epoch: 6| Step: 2
Training loss: 2.080535411834717
Validation loss: 2.056856095790863

Epoch: 6| Step: 3
Training loss: 1.9581916332244873
Validation loss: 2.053946852684021

Epoch: 6| Step: 4
Training loss: 2.1545639038085938
Validation loss: 2.0464952190717063

Epoch: 6| Step: 5
Training loss: 1.5540049076080322
Validation loss: 2.0435367623964944

Epoch: 6| Step: 6
Training loss: 2.6027140617370605
Validation loss: 2.044369618097941

Epoch: 6| Step: 7
Training loss: 2.1794893741607666
Validation loss: 2.0402855475743613

Epoch: 6| Step: 8
Training loss: 2.3806471824645996
Validation loss: 2.0324519077936807

Epoch: 6| Step: 9
Training loss: 1.1991335153579712
Validation loss: 2.0361705223719277

Epoch: 6| Step: 10
Training loss: 2.344045877456665
Validation loss: 2.033059557278951

Epoch: 6| Step: 11
Training loss: 1.8634757995605469
Validation loss: 2.025573174158732

Epoch: 6| Step: 12
Training loss: 2.4597597122192383
Validation loss: 2.032322804133097

Epoch: 6| Step: 13
Training loss: 1.7679523229599
Validation loss: 2.0358387430508933

Epoch: 116| Step: 0
Training loss: 2.1417617797851562
Validation loss: 2.0416634678840637

Epoch: 6| Step: 1
Training loss: 1.5435497760772705
Validation loss: 2.0448598861694336

Epoch: 6| Step: 2
Training loss: 2.006253719329834
Validation loss: 2.0554616848627725

Epoch: 6| Step: 3
Training loss: 1.954838752746582
Validation loss: 2.059409240881602

Epoch: 6| Step: 4
Training loss: 1.9583598375320435
Validation loss: 2.0536893804868064

Epoch: 6| Step: 5
Training loss: 2.224963665008545
Validation loss: 2.0489346583684287

Epoch: 6| Step: 6
Training loss: 2.169374465942383
Validation loss: 2.0613739689191184

Epoch: 6| Step: 7
Training loss: 2.294389247894287
Validation loss: 2.0684621334075928

Epoch: 6| Step: 8
Training loss: 2.1463732719421387
Validation loss: 2.053084353605906

Epoch: 6| Step: 9
Training loss: 1.814743995666504
Validation loss: 2.0480632384618125

Epoch: 6| Step: 10
Training loss: 2.0564088821411133
Validation loss: 2.042210817337036

Epoch: 6| Step: 11
Training loss: 2.378897190093994
Validation loss: 2.03313018878301

Epoch: 6| Step: 12
Training loss: 1.8654261827468872
Validation loss: 2.039686361948649

Epoch: 6| Step: 13
Training loss: 2.434328079223633
Validation loss: 2.036983013153076

Epoch: 117| Step: 0
Training loss: 1.8528189659118652
Validation loss: 2.0378562609354653

Epoch: 6| Step: 1
Training loss: 2.639803171157837
Validation loss: 2.0273254116376243

Epoch: 6| Step: 2
Training loss: 2.3121349811553955
Validation loss: 2.0426692565282187

Epoch: 6| Step: 3
Training loss: 2.1488966941833496
Validation loss: 2.0425775051116943

Epoch: 6| Step: 4
Training loss: 1.9219391345977783
Validation loss: 2.036343216896057

Epoch: 6| Step: 5
Training loss: 2.1758816242218018
Validation loss: 2.038734575112661

Epoch: 6| Step: 6
Training loss: 2.386890172958374
Validation loss: 2.0500514904658

Epoch: 6| Step: 7
Training loss: 1.780019998550415
Validation loss: 2.061188220977783

Epoch: 6| Step: 8
Training loss: 2.082740306854248
Validation loss: 2.0453611612319946

Epoch: 6| Step: 9
Training loss: 1.9022207260131836
Validation loss: 2.0570573806762695

Epoch: 6| Step: 10
Training loss: 1.371977686882019
Validation loss: 2.0631505846977234

Epoch: 6| Step: 11
Training loss: 2.1127967834472656
Validation loss: 2.063856522242228

Epoch: 6| Step: 12
Training loss: 2.471383810043335
Validation loss: 2.0621349215507507

Epoch: 6| Step: 13
Training loss: 1.9507181644439697
Validation loss: 2.071076532204946

Epoch: 118| Step: 0
Training loss: 2.093770742416382
Validation loss: 2.0462688406308494

Epoch: 6| Step: 1
Training loss: 1.6945834159851074
Validation loss: 2.056173245112101

Epoch: 6| Step: 2
Training loss: 2.360374927520752
Validation loss: 2.0579525232315063

Epoch: 6| Step: 3
Training loss: 1.9428733587265015
Validation loss: 2.046968956788381

Epoch: 6| Step: 4
Training loss: 1.6476218700408936
Validation loss: 2.055032968521118

Epoch: 6| Step: 5
Training loss: 2.0774288177490234
Validation loss: 2.044156273206075

Epoch: 6| Step: 6
Training loss: 1.7776925563812256
Validation loss: 2.0537718137105307

Epoch: 6| Step: 7
Training loss: 1.8867101669311523
Validation loss: 2.061245004336039

Epoch: 6| Step: 8
Training loss: 2.6199164390563965
Validation loss: 2.0618284742037454

Epoch: 6| Step: 9
Training loss: 2.6527299880981445
Validation loss: 2.0565799673398337

Epoch: 6| Step: 10
Training loss: 1.950622797012329
Validation loss: 2.0588753620783486

Epoch: 6| Step: 11
Training loss: 1.7725462913513184
Validation loss: 2.058483064174652

Epoch: 6| Step: 12
Training loss: 2.3908796310424805
Validation loss: 2.058590571085612

Epoch: 6| Step: 13
Training loss: 1.918452262878418
Validation loss: 2.0520922541618347

Epoch: 119| Step: 0
Training loss: 2.311957597732544
Validation loss: 2.049416959285736

Epoch: 6| Step: 1
Training loss: 2.0666751861572266
Validation loss: 2.0617965261141458

Epoch: 6| Step: 2
Training loss: 2.02498722076416
Validation loss: 2.054672141869863

Epoch: 6| Step: 3
Training loss: 2.567321538925171
Validation loss: 2.0512606700261435

Epoch: 6| Step: 4
Training loss: 1.7631523609161377
Validation loss: 2.0613815585772195

Epoch: 6| Step: 5
Training loss: 2.2021710872650146
Validation loss: 2.0541090766588845

Epoch: 6| Step: 6
Training loss: 1.6835660934448242
Validation loss: 2.0601468880971274

Epoch: 6| Step: 7
Training loss: 2.6779022216796875
Validation loss: 2.058776617050171

Epoch: 6| Step: 8
Training loss: 1.7934715747833252
Validation loss: 2.060429811477661

Epoch: 6| Step: 9
Training loss: 2.0400829315185547
Validation loss: 2.063453038533529

Epoch: 6| Step: 10
Training loss: 1.9508748054504395
Validation loss: 2.0882954796155295

Epoch: 6| Step: 11
Training loss: 1.8689998388290405
Validation loss: 2.0916924675305686

Epoch: 6| Step: 12
Training loss: 1.3089895248413086
Validation loss: 2.069751183191935

Epoch: 6| Step: 13
Training loss: 2.4272613525390625
Validation loss: 2.0748863021532693

Epoch: 120| Step: 0
Training loss: 1.799811601638794
Validation loss: 2.0593997041384378

Epoch: 6| Step: 1
Training loss: 2.060405731201172
Validation loss: 2.063524921735128

Epoch: 6| Step: 2
Training loss: 1.989754557609558
Validation loss: 2.060638427734375

Epoch: 6| Step: 3
Training loss: 2.11045241355896
Validation loss: 2.0546817580858865

Epoch: 6| Step: 4
Training loss: 1.9964001178741455
Validation loss: 2.055899520715078

Epoch: 6| Step: 5
Training loss: 2.5381510257720947
Validation loss: 2.0548331340154014

Epoch: 6| Step: 6
Training loss: 2.1318840980529785
Validation loss: 2.062545875708262

Epoch: 6| Step: 7
Training loss: 2.0719871520996094
Validation loss: 2.0694684386253357

Epoch: 6| Step: 8
Training loss: 1.2619965076446533
Validation loss: 2.074473222096761

Epoch: 6| Step: 9
Training loss: 2.679737091064453
Validation loss: 2.072025199731191

Epoch: 6| Step: 10
Training loss: 1.697535753250122
Validation loss: 2.0730011264483132

Epoch: 6| Step: 11
Training loss: 2.5296194553375244
Validation loss: 2.0823237101236978

Epoch: 6| Step: 12
Training loss: 2.373051643371582
Validation loss: 2.076657255490621

Epoch: 6| Step: 13
Training loss: 1.528260350227356
Validation loss: 2.0573089520136514

Epoch: 121| Step: 0
Training loss: 2.055232524871826
Validation loss: 2.0568687915802

Epoch: 6| Step: 1
Training loss: 1.469260811805725
Validation loss: 2.0545600255330405

Epoch: 6| Step: 2
Training loss: 2.7396955490112305
Validation loss: 2.0379384756088257

Epoch: 6| Step: 3
Training loss: 1.677911639213562
Validation loss: 2.036643167336782

Epoch: 6| Step: 4
Training loss: 2.0468544960021973
Validation loss: 2.038979391256968

Epoch: 6| Step: 5
Training loss: 2.196472644805908
Validation loss: 2.0272300442059836

Epoch: 6| Step: 6
Training loss: 1.6445188522338867
Validation loss: 2.030676782131195

Epoch: 6| Step: 7
Training loss: 2.5463671684265137
Validation loss: 2.0338731606801352

Epoch: 6| Step: 8
Training loss: 2.122515916824341
Validation loss: 2.0434563755989075

Epoch: 6| Step: 9
Training loss: 1.9689993858337402
Validation loss: 2.0427695711453757

Epoch: 6| Step: 10
Training loss: 1.6765272617340088
Validation loss: 2.0419398148854575

Epoch: 6| Step: 11
Training loss: 2.6264190673828125
Validation loss: 2.0567902326583862

Epoch: 6| Step: 12
Training loss: 1.762223720550537
Validation loss: 2.0631051858266196

Epoch: 6| Step: 13
Training loss: 2.3434224128723145
Validation loss: 2.058444917201996

Epoch: 122| Step: 0
Training loss: 2.034919261932373
Validation loss: 2.0639296968777976

Epoch: 6| Step: 1
Training loss: 1.8767166137695312
Validation loss: 2.0555742979049683

Epoch: 6| Step: 2
Training loss: 1.9263068437576294
Validation loss: 2.055912971496582

Epoch: 6| Step: 3
Training loss: 2.3652448654174805
Validation loss: 2.0401726563771567

Epoch: 6| Step: 4
Training loss: 2.4437382221221924
Validation loss: 2.0446258982022605

Epoch: 6| Step: 5
Training loss: 2.1324315071105957
Validation loss: 2.0537514289220176

Epoch: 6| Step: 6
Training loss: 2.2651314735412598
Validation loss: 2.034531573454539

Epoch: 6| Step: 7
Training loss: 1.8690099716186523
Validation loss: 2.0328373511632285

Epoch: 6| Step: 8
Training loss: 2.690244674682617
Validation loss: 2.02900493144989

Epoch: 6| Step: 9
Training loss: 1.9636032581329346
Validation loss: 2.036289652188619

Epoch: 6| Step: 10
Training loss: 1.9597090482711792
Validation loss: 2.035378396511078

Epoch: 6| Step: 11
Training loss: 1.6693724393844604
Validation loss: 2.0428402423858643

Epoch: 6| Step: 12
Training loss: 1.6593523025512695
Validation loss: 2.0602702299753823

Epoch: 6| Step: 13
Training loss: 1.9697513580322266
Validation loss: 2.0751248796780906

Epoch: 123| Step: 0
Training loss: 1.7499552965164185
Validation loss: 2.0782742301623025

Epoch: 6| Step: 1
Training loss: 1.8487626314163208
Validation loss: 2.1081844170888266

Epoch: 6| Step: 2
Training loss: 2.2058615684509277
Validation loss: 2.1292137106259665

Epoch: 6| Step: 3
Training loss: 2.2248997688293457
Validation loss: 2.1185375253359475

Epoch: 6| Step: 4
Training loss: 2.375985622406006
Validation loss: 2.129395763079325

Epoch: 6| Step: 5
Training loss: 2.0343430042266846
Validation loss: 2.0852369467417398

Epoch: 6| Step: 6
Training loss: 2.8318142890930176
Validation loss: 2.063627998034159

Epoch: 6| Step: 7
Training loss: 1.6999452114105225
Validation loss: 2.0580482880274453

Epoch: 6| Step: 8
Training loss: 2.0552749633789062
Validation loss: 2.0440362294514975

Epoch: 6| Step: 9
Training loss: 2.87190580368042
Validation loss: 2.0330395499865213

Epoch: 6| Step: 10
Training loss: 1.4712175130844116
Validation loss: 2.031269292036692

Epoch: 6| Step: 11
Training loss: 1.8056631088256836
Validation loss: 2.0430773496627808

Epoch: 6| Step: 12
Training loss: 2.1971986293792725
Validation loss: 2.040142218271891

Epoch: 6| Step: 13
Training loss: 2.4706125259399414
Validation loss: 2.035427689552307

Epoch: 124| Step: 0
Training loss: 2.198549509048462
Validation loss: 2.032109101613363

Epoch: 6| Step: 1
Training loss: 2.1929538249969482
Validation loss: 2.034654438495636

Epoch: 6| Step: 2
Training loss: 2.1842262744903564
Validation loss: 2.0446348786354065

Epoch: 6| Step: 3
Training loss: 2.292562961578369
Validation loss: 2.038997173309326

Epoch: 6| Step: 4
Training loss: 2.2885079383850098
Validation loss: 2.034553309281667

Epoch: 6| Step: 5
Training loss: 2.0325260162353516
Validation loss: 2.039348582426707

Epoch: 6| Step: 6
Training loss: 2.2619528770446777
Validation loss: 2.0307025710741677

Epoch: 6| Step: 7
Training loss: 2.470419406890869
Validation loss: 2.0443732937177024

Epoch: 6| Step: 8
Training loss: 1.6440728902816772
Validation loss: 2.0307838916778564

Epoch: 6| Step: 9
Training loss: 1.6145172119140625
Validation loss: 2.0355140566825867

Epoch: 6| Step: 10
Training loss: 2.5163676738739014
Validation loss: 2.0331879258155823

Epoch: 6| Step: 11
Training loss: 2.222341537475586
Validation loss: 2.0319537123044333

Epoch: 6| Step: 12
Training loss: 2.149716377258301
Validation loss: 2.0384914875030518

Epoch: 6| Step: 13
Training loss: 1.7013556957244873
Validation loss: 2.025351345539093

Epoch: 125| Step: 0
Training loss: 2.4562435150146484
Validation loss: 2.0257927576700845

Epoch: 6| Step: 1
Training loss: 2.244896650314331
Validation loss: 2.032254636287689

Epoch: 6| Step: 2
Training loss: 1.1951627731323242
Validation loss: 2.03366756439209

Epoch: 6| Step: 3
Training loss: 1.8129018545150757
Validation loss: 2.0277228554089866

Epoch: 6| Step: 4
Training loss: 2.187721014022827
Validation loss: 2.027228315671285

Epoch: 6| Step: 5
Training loss: 1.752401351928711
Validation loss: 2.0364008148511252

Epoch: 6| Step: 6
Training loss: 2.257904529571533
Validation loss: 2.047914743423462

Epoch: 6| Step: 7
Training loss: 2.261976480484009
Validation loss: 2.043355882167816

Epoch: 6| Step: 8
Training loss: 2.074000835418701
Validation loss: 2.0540510217348733

Epoch: 6| Step: 9
Training loss: 1.966975450515747
Validation loss: 2.0469890236854553

Epoch: 6| Step: 10
Training loss: 2.2708334922790527
Validation loss: 2.0649261275927224

Epoch: 6| Step: 11
Training loss: 2.672715902328491
Validation loss: 2.0607372919718423

Epoch: 6| Step: 12
Training loss: 1.8655378818511963
Validation loss: 2.051499366760254

Epoch: 6| Step: 13
Training loss: 1.8094745874404907
Validation loss: 2.06217223405838

Epoch: 126| Step: 0
Training loss: 1.7429416179656982
Validation loss: 2.055233657360077

Epoch: 6| Step: 1
Training loss: 2.5135722160339355
Validation loss: 2.061888794104258

Epoch: 6| Step: 2
Training loss: 1.6121668815612793
Validation loss: 2.0594281951586404

Epoch: 6| Step: 3
Training loss: 1.601736068725586
Validation loss: 2.0604375203450522

Epoch: 6| Step: 4
Training loss: 2.384830951690674
Validation loss: 2.0437358021736145

Epoch: 6| Step: 5
Training loss: 2.031050682067871
Validation loss: 2.050893227259318

Epoch: 6| Step: 6
Training loss: 2.222681999206543
Validation loss: 2.057336131731669

Epoch: 6| Step: 7
Training loss: 2.008387565612793
Validation loss: 2.052369554837545

Epoch: 6| Step: 8
Training loss: 2.104585647583008
Validation loss: 2.053015430768331

Epoch: 6| Step: 9
Training loss: 1.716841459274292
Validation loss: 2.0671722094217935

Epoch: 6| Step: 10
Training loss: 2.0266387462615967
Validation loss: 2.067030429840088

Epoch: 6| Step: 11
Training loss: 1.9312832355499268
Validation loss: 2.064765532811483

Epoch: 6| Step: 12
Training loss: 2.299956798553467
Validation loss: 2.072980761528015

Epoch: 6| Step: 13
Training loss: 2.390801191329956
Validation loss: 2.08123513062795

Epoch: 127| Step: 0
Training loss: 2.1118760108947754
Validation loss: 2.0850577354431152

Epoch: 6| Step: 1
Training loss: 1.8319259881973267
Validation loss: 2.0944191217422485

Epoch: 6| Step: 2
Training loss: 2.547866106033325
Validation loss: 2.0870298941930137

Epoch: 6| Step: 3
Training loss: 2.217710018157959
Validation loss: 2.0866162180900574

Epoch: 6| Step: 4
Training loss: 2.0012564659118652
Validation loss: 2.102718790372213

Epoch: 6| Step: 5
Training loss: 2.4259443283081055
Validation loss: 2.081203361352285

Epoch: 6| Step: 6
Training loss: 2.2852277755737305
Validation loss: 2.072021722793579

Epoch: 6| Step: 7
Training loss: 1.7736074924468994
Validation loss: 2.0608057777086892

Epoch: 6| Step: 8
Training loss: 1.9564900398254395
Validation loss: 2.0592896342277527

Epoch: 6| Step: 9
Training loss: 1.5341744422912598
Validation loss: 2.05918288230896

Epoch: 6| Step: 10
Training loss: 1.9360554218292236
Validation loss: 2.06134965022405

Epoch: 6| Step: 11
Training loss: 1.8448657989501953
Validation loss: 2.05180561542511

Epoch: 6| Step: 12
Training loss: 2.0067355632781982
Validation loss: 2.052211105823517

Epoch: 6| Step: 13
Training loss: 2.2779412269592285
Validation loss: 2.059904476006826

Epoch: 128| Step: 0
Training loss: 2.1770386695861816
Validation loss: 2.057476222515106

Epoch: 6| Step: 1
Training loss: 1.8694534301757812
Validation loss: 2.0561429858207703

Epoch: 6| Step: 2
Training loss: 2.0248849391937256
Validation loss: 2.053415576616923

Epoch: 6| Step: 3
Training loss: 2.6760787963867188
Validation loss: 2.0702451268831887

Epoch: 6| Step: 4
Training loss: 2.426724910736084
Validation loss: 2.0700736045837402

Epoch: 6| Step: 5
Training loss: 1.5664955377578735
Validation loss: 2.070428490638733

Epoch: 6| Step: 6
Training loss: 2.147984504699707
Validation loss: 2.0728601018587747

Epoch: 6| Step: 7
Training loss: 1.8269132375717163
Validation loss: 2.0949652194976807

Epoch: 6| Step: 8
Training loss: 2.223613739013672
Validation loss: 2.0937251249949136

Epoch: 6| Step: 9
Training loss: 1.9463305473327637
Validation loss: 2.0856648286183677

Epoch: 6| Step: 10
Training loss: 1.434224009513855
Validation loss: 2.093000133832296

Epoch: 6| Step: 11
Training loss: 1.9468207359313965
Validation loss: 2.0946789383888245

Epoch: 6| Step: 12
Training loss: 1.899275541305542
Validation loss: 2.08407594760259

Epoch: 6| Step: 13
Training loss: 2.1832573413848877
Validation loss: 2.0774537523587546

Epoch: 129| Step: 0
Training loss: 2.504199504852295
Validation loss: 2.062653124332428

Epoch: 6| Step: 1
Training loss: 1.810258150100708
Validation loss: 2.064359943072001

Epoch: 6| Step: 2
Training loss: 1.805384635925293
Validation loss: 2.0645004908243814

Epoch: 6| Step: 3
Training loss: 2.343719482421875
Validation loss: 2.065556446711222

Epoch: 6| Step: 4
Training loss: 1.7187494039535522
Validation loss: 2.052573323249817

Epoch: 6| Step: 5
Training loss: 1.9055628776550293
Validation loss: 2.0638305942217507

Epoch: 6| Step: 6
Training loss: 2.0458736419677734
Validation loss: 2.0584289034207663

Epoch: 6| Step: 7
Training loss: 1.7221078872680664
Validation loss: 2.07021031777064

Epoch: 6| Step: 8
Training loss: 2.4885220527648926
Validation loss: 2.0791154305140176

Epoch: 6| Step: 9
Training loss: 2.002288818359375
Validation loss: 2.0897005995114646

Epoch: 6| Step: 10
Training loss: 1.7141399383544922
Validation loss: 2.093420664469401

Epoch: 6| Step: 11
Training loss: 2.3393075466156006
Validation loss: 2.1145671606063843

Epoch: 6| Step: 12
Training loss: 1.9806686639785767
Validation loss: 2.1245683232943215

Epoch: 6| Step: 13
Training loss: 2.4323911666870117
Validation loss: 2.117685238520304

Epoch: 130| Step: 0
Training loss: 2.456049680709839
Validation loss: 2.1074766715367637

Epoch: 6| Step: 1
Training loss: 2.5720725059509277
Validation loss: 2.11050812403361

Epoch: 6| Step: 2
Training loss: 2.1047286987304688
Validation loss: 2.092714468638102

Epoch: 6| Step: 3
Training loss: 1.6914037466049194
Validation loss: 2.0773783922195435

Epoch: 6| Step: 4
Training loss: 2.2778966426849365
Validation loss: 2.06268717845281

Epoch: 6| Step: 5
Training loss: 1.9086406230926514
Validation loss: 2.0719194610913596

Epoch: 6| Step: 6
Training loss: 1.8913590908050537
Validation loss: 2.0589842796325684

Epoch: 6| Step: 7
Training loss: 1.7606370449066162
Validation loss: 2.06742525100708

Epoch: 6| Step: 8
Training loss: 1.92698073387146
Validation loss: 2.051621894041697

Epoch: 6| Step: 9
Training loss: 1.7119061946868896
Validation loss: 2.0441563526789346

Epoch: 6| Step: 10
Training loss: 1.5843042135238647
Validation loss: 2.0467421809832254

Epoch: 6| Step: 11
Training loss: 2.205683469772339
Validation loss: 2.0667646129926047

Epoch: 6| Step: 12
Training loss: 2.4214367866516113
Validation loss: 2.063319146633148

Epoch: 6| Step: 13
Training loss: 1.9639551639556885
Validation loss: 2.0841508706410727

Epoch: 131| Step: 0
Training loss: 1.8477283716201782
Validation loss: 2.0887669722239175

Epoch: 6| Step: 1
Training loss: 1.5470155477523804
Validation loss: 2.096464514732361

Epoch: 6| Step: 2
Training loss: 2.3991644382476807
Validation loss: 2.079383393128713

Epoch: 6| Step: 3
Training loss: 2.1325621604919434
Validation loss: 2.0824063221613565

Epoch: 6| Step: 4
Training loss: 1.644150733947754
Validation loss: 2.075255274772644

Epoch: 6| Step: 5
Training loss: 2.344210386276245
Validation loss: 2.0835588574409485

Epoch: 6| Step: 6
Training loss: 2.575732946395874
Validation loss: 2.0903891921043396

Epoch: 6| Step: 7
Training loss: 2.177189350128174
Validation loss: 2.0598960717519126

Epoch: 6| Step: 8
Training loss: 1.8396031856536865
Validation loss: 2.0687203407287598

Epoch: 6| Step: 9
Training loss: 1.7288687229156494
Validation loss: 2.0561185677846274

Epoch: 6| Step: 10
Training loss: 1.3074841499328613
Validation loss: 2.0514259139696756

Epoch: 6| Step: 11
Training loss: 2.183271884918213
Validation loss: 2.060751179854075

Epoch: 6| Step: 12
Training loss: 2.0981903076171875
Validation loss: 2.0694591999053955

Epoch: 6| Step: 13
Training loss: 2.356593132019043
Validation loss: 2.056215981642405

Epoch: 132| Step: 0
Training loss: 2.5145654678344727
Validation loss: 2.073680321375529

Epoch: 6| Step: 1
Training loss: 1.5726293325424194
Validation loss: 2.0662490129470825

Epoch: 6| Step: 2
Training loss: 1.7117640972137451
Validation loss: 2.082220137119293

Epoch: 6| Step: 3
Training loss: 1.9685734510421753
Validation loss: 2.070166905721029

Epoch: 6| Step: 4
Training loss: 1.3659896850585938
Validation loss: 2.0845109820365906

Epoch: 6| Step: 5
Training loss: 2.2445008754730225
Validation loss: 2.0891997615496316

Epoch: 6| Step: 6
Training loss: 2.028376579284668
Validation loss: 2.0978932976722717

Epoch: 6| Step: 7
Training loss: 2.7381157875061035
Validation loss: 2.082854986190796

Epoch: 6| Step: 8
Training loss: 2.420523166656494
Validation loss: 2.1093246936798096

Epoch: 6| Step: 9
Training loss: 1.835796594619751
Validation loss: 2.129330317179362

Epoch: 6| Step: 10
Training loss: 1.8033647537231445
Validation loss: 2.124557296435038

Epoch: 6| Step: 11
Training loss: 1.9899308681488037
Validation loss: 2.1198266744613647

Epoch: 6| Step: 12
Training loss: 1.7333765029907227
Validation loss: 2.0975812872250876

Epoch: 6| Step: 13
Training loss: 2.360045909881592
Validation loss: 2.108620047569275

Epoch: 133| Step: 0
Training loss: 2.5396885871887207
Validation loss: 2.0893593629201255

Epoch: 6| Step: 1
Training loss: 2.170361042022705
Validation loss: 2.0744157632191977

Epoch: 6| Step: 2
Training loss: 1.667502760887146
Validation loss: 2.055446982383728

Epoch: 6| Step: 3
Training loss: 2.2149980068206787
Validation loss: 2.037735323111216

Epoch: 6| Step: 4
Training loss: 2.312887191772461
Validation loss: 2.0495222409566245

Epoch: 6| Step: 5
Training loss: 1.9890403747558594
Validation loss: 2.049184481302897

Epoch: 6| Step: 6
Training loss: 1.7131175994873047
Validation loss: 2.048280398050944

Epoch: 6| Step: 7
Training loss: 2.485257625579834
Validation loss: 2.0482242504755654

Epoch: 6| Step: 8
Training loss: 1.5144116878509521
Validation loss: 2.0561867554982505

Epoch: 6| Step: 9
Training loss: 1.8508238792419434
Validation loss: 2.0668578147888184

Epoch: 6| Step: 10
Training loss: 2.3991994857788086
Validation loss: 2.0614155729611716

Epoch: 6| Step: 11
Training loss: 1.7634904384613037
Validation loss: 2.058471937974294

Epoch: 6| Step: 12
Training loss: 1.9008468389511108
Validation loss: 2.0737682779630027

Epoch: 6| Step: 13
Training loss: 2.081242084503174
Validation loss: 2.0679770906766257

Epoch: 134| Step: 0
Training loss: 1.661475658416748
Validation loss: 2.093626379966736

Epoch: 6| Step: 1
Training loss: 2.0582900047302246
Validation loss: 2.0805601874987283

Epoch: 6| Step: 2
Training loss: 1.60210120677948
Validation loss: 2.082367738087972

Epoch: 6| Step: 3
Training loss: 2.1774797439575195
Validation loss: 2.107912302017212

Epoch: 6| Step: 4
Training loss: 2.644038200378418
Validation loss: 2.106147885322571

Epoch: 6| Step: 5
Training loss: 2.293299674987793
Validation loss: 2.117186407248179

Epoch: 6| Step: 6
Training loss: 2.0064728260040283
Validation loss: 2.09444522857666

Epoch: 6| Step: 7
Training loss: 2.012031316757202
Validation loss: 2.1015750765800476

Epoch: 6| Step: 8
Training loss: 2.013462781906128
Validation loss: 2.074292620023092

Epoch: 6| Step: 9
Training loss: 2.1035518646240234
Validation loss: 2.0612130165100098

Epoch: 6| Step: 10
Training loss: 2.2173805236816406
Validation loss: 2.0450220108032227

Epoch: 6| Step: 11
Training loss: 2.084427833557129
Validation loss: 2.039603590965271

Epoch: 6| Step: 12
Training loss: 2.07938814163208
Validation loss: 2.041411022345225

Epoch: 6| Step: 13
Training loss: 1.7830199003219604
Validation loss: 2.0427266160647073

Epoch: 135| Step: 0
Training loss: 2.316044569015503
Validation loss: 2.0452778339385986

Epoch: 6| Step: 1
Training loss: 2.066242218017578
Validation loss: 2.0480276346206665

Epoch: 6| Step: 2
Training loss: 1.7479726076126099
Validation loss: 2.046182155609131

Epoch: 6| Step: 3
Training loss: 2.113084316253662
Validation loss: 2.0613451401392617

Epoch: 6| Step: 4
Training loss: 2.282578468322754
Validation loss: 2.055557608604431

Epoch: 6| Step: 5
Training loss: 2.4473886489868164
Validation loss: 2.0581114093462625

Epoch: 6| Step: 6
Training loss: 1.3247482776641846
Validation loss: 2.0463621815045676

Epoch: 6| Step: 7
Training loss: 2.5223145484924316
Validation loss: 2.054524600505829

Epoch: 6| Step: 8
Training loss: 2.4038171768188477
Validation loss: 2.067801753679911

Epoch: 6| Step: 9
Training loss: 1.574026346206665
Validation loss: 2.0798121889432273

Epoch: 6| Step: 10
Training loss: 1.9931875467300415
Validation loss: 2.0832283099492392

Epoch: 6| Step: 11
Training loss: 1.4887676239013672
Validation loss: 2.075368086496989

Epoch: 6| Step: 12
Training loss: 2.4927127361297607
Validation loss: 2.097172717253367

Epoch: 6| Step: 13
Training loss: 2.032658576965332
Validation loss: 2.1004775961240134

Epoch: 136| Step: 0
Training loss: 2.175809144973755
Validation loss: 2.0953972935676575

Epoch: 6| Step: 1
Training loss: 1.9303927421569824
Validation loss: 2.0750795006752014

Epoch: 6| Step: 2
Training loss: 1.5830650329589844
Validation loss: 2.069669723510742

Epoch: 6| Step: 3
Training loss: 1.8802040815353394
Validation loss: 2.0494914849599204

Epoch: 6| Step: 4
Training loss: 2.344409465789795
Validation loss: 2.0721879998842874

Epoch: 6| Step: 5
Training loss: 1.9614921808242798
Validation loss: 2.057815968990326

Epoch: 6| Step: 6
Training loss: 1.7796392440795898
Validation loss: 2.0478100180625916

Epoch: 6| Step: 7
Training loss: 2.605173349380493
Validation loss: 2.052060902118683

Epoch: 6| Step: 8
Training loss: 1.923789381980896
Validation loss: 2.0559645096460977

Epoch: 6| Step: 9
Training loss: 2.192767858505249
Validation loss: 2.044331351915995

Epoch: 6| Step: 10
Training loss: 2.684633731842041
Validation loss: 2.050539175669352

Epoch: 6| Step: 11
Training loss: 2.227010488510132
Validation loss: 2.0465722481409707

Epoch: 6| Step: 12
Training loss: 1.8146387338638306
Validation loss: 2.037613868713379

Epoch: 6| Step: 13
Training loss: 1.4206639528274536
Validation loss: 2.0432539780934653

Epoch: 137| Step: 0
Training loss: 1.7821123600006104
Validation loss: 2.0461895068486533

Epoch: 6| Step: 1
Training loss: 2.4347586631774902
Validation loss: 2.057838042577108

Epoch: 6| Step: 2
Training loss: 1.6900043487548828
Validation loss: 2.0563035011291504

Epoch: 6| Step: 3
Training loss: 2.6818201541900635
Validation loss: 2.061340868473053

Epoch: 6| Step: 4
Training loss: 1.8359878063201904
Validation loss: 2.0657901763916016

Epoch: 6| Step: 5
Training loss: 2.3056387901306152
Validation loss: 2.0838036934534707

Epoch: 6| Step: 6
Training loss: 1.9499481916427612
Validation loss: 2.1013193329175315

Epoch: 6| Step: 7
Training loss: 2.3095130920410156
Validation loss: 2.0858881076176963

Epoch: 6| Step: 8
Training loss: 1.5203394889831543
Validation loss: 2.099063754081726

Epoch: 6| Step: 9
Training loss: 1.8557546138763428
Validation loss: 2.1114049752553306

Epoch: 6| Step: 10
Training loss: 2.418755054473877
Validation loss: 2.0931719740231833

Epoch: 6| Step: 11
Training loss: 2.251964569091797
Validation loss: 2.079708695411682

Epoch: 6| Step: 12
Training loss: 1.9334385395050049
Validation loss: 2.0794200698534646

Epoch: 6| Step: 13
Training loss: 1.7239813804626465
Validation loss: 2.062045435110728

Epoch: 138| Step: 0
Training loss: 1.4274203777313232
Validation loss: 2.059226135412852

Epoch: 6| Step: 1
Training loss: 2.1651525497436523
Validation loss: 2.057650645573934

Epoch: 6| Step: 2
Training loss: 1.8303570747375488
Validation loss: 2.056987782319387

Epoch: 6| Step: 3
Training loss: 1.9196648597717285
Validation loss: 2.05320409933726

Epoch: 6| Step: 4
Training loss: 2.030322313308716
Validation loss: 2.0543901721636453

Epoch: 6| Step: 5
Training loss: 2.759824514389038
Validation loss: 2.060761113961538

Epoch: 6| Step: 6
Training loss: 2.7785749435424805
Validation loss: 2.058222770690918

Epoch: 6| Step: 7
Training loss: 1.7928608655929565
Validation loss: 2.0507282813390098

Epoch: 6| Step: 8
Training loss: 1.5955262184143066
Validation loss: 2.060261686642965

Epoch: 6| Step: 9
Training loss: 2.423778533935547
Validation loss: 2.049440582593282

Epoch: 6| Step: 10
Training loss: 1.585223913192749
Validation loss: 2.0700045625368753

Epoch: 6| Step: 11
Training loss: 2.2234058380126953
Validation loss: 2.060718595981598

Epoch: 6| Step: 12
Training loss: 2.353252649307251
Validation loss: 2.0733472108840942

Epoch: 6| Step: 13
Training loss: 1.5563510656356812
Validation loss: 2.065904458363851

Epoch: 139| Step: 0
Training loss: 2.4188666343688965
Validation loss: 2.0829210678736367

Epoch: 6| Step: 1
Training loss: 2.4099903106689453
Validation loss: 2.0634591579437256

Epoch: 6| Step: 2
Training loss: 2.0920114517211914
Validation loss: 2.0630749066670737

Epoch: 6| Step: 3
Training loss: 2.564086437225342
Validation loss: 2.0708054304122925

Epoch: 6| Step: 4
Training loss: 2.1653847694396973
Validation loss: 2.0629716316858926

Epoch: 6| Step: 5
Training loss: 1.6659348011016846
Validation loss: 2.066280722618103

Epoch: 6| Step: 6
Training loss: 1.6928471326828003
Validation loss: 2.082627216974894

Epoch: 6| Step: 7
Training loss: 1.928636074066162
Validation loss: 2.079724073410034

Epoch: 6| Step: 8
Training loss: 2.5897536277770996
Validation loss: 2.0780179301897683

Epoch: 6| Step: 9
Training loss: 1.8883185386657715
Validation loss: 2.080769896507263

Epoch: 6| Step: 10
Training loss: 1.4426194429397583
Validation loss: 2.090253790219625

Epoch: 6| Step: 11
Training loss: 1.7523562908172607
Validation loss: 2.0711113015810647

Epoch: 6| Step: 12
Training loss: 1.8167686462402344
Validation loss: 2.0589861075083413

Epoch: 6| Step: 13
Training loss: 1.6945205926895142
Validation loss: 2.0611248215039573

Epoch: 140| Step: 0
Training loss: 2.0322623252868652
Validation loss: 2.0585897962252298

Epoch: 6| Step: 1
Training loss: 1.5704965591430664
Validation loss: 2.0648926496505737

Epoch: 6| Step: 2
Training loss: 1.9969309568405151
Validation loss: 2.074396332105001

Epoch: 6| Step: 3
Training loss: 2.2156155109405518
Validation loss: 2.0872856775919595

Epoch: 6| Step: 4
Training loss: 2.204285144805908
Validation loss: 2.077083110809326

Epoch: 6| Step: 5
Training loss: 2.295253276824951
Validation loss: 2.1042081117630005

Epoch: 6| Step: 6
Training loss: 2.24446439743042
Validation loss: 2.1088069677352905

Epoch: 6| Step: 7
Training loss: 1.9292805194854736
Validation loss: 2.0907869736353555

Epoch: 6| Step: 8
Training loss: 1.508861780166626
Validation loss: 2.077554186185201

Epoch: 6| Step: 9
Training loss: 1.7797964811325073
Validation loss: 2.0789283911387124

Epoch: 6| Step: 10
Training loss: 2.3323097229003906
Validation loss: 2.085418621699015

Epoch: 6| Step: 11
Training loss: 1.9530575275421143
Validation loss: 2.0706282258033752

Epoch: 6| Step: 12
Training loss: 2.0525062084198
Validation loss: 2.0841792821884155

Epoch: 6| Step: 13
Training loss: 1.9473241567611694
Validation loss: 2.088381767272949

Epoch: 141| Step: 0
Training loss: 1.9342105388641357
Validation loss: 2.077716112136841

Epoch: 6| Step: 1
Training loss: 1.7580984830856323
Validation loss: 2.0890049735705056

Epoch: 6| Step: 2
Training loss: 2.3190174102783203
Validation loss: 2.0886959234873452

Epoch: 6| Step: 3
Training loss: 1.5251498222351074
Validation loss: 2.068419575691223

Epoch: 6| Step: 4
Training loss: 1.935280442237854
Validation loss: 2.055372675259908

Epoch: 6| Step: 5
Training loss: 2.074939727783203
Validation loss: 2.055531919002533

Epoch: 6| Step: 6
Training loss: 2.3483657836914062
Validation loss: 2.0572310090065002

Epoch: 6| Step: 7
Training loss: 2.1720805168151855
Validation loss: 2.049542168776194

Epoch: 6| Step: 8
Training loss: 2.343679428100586
Validation loss: 2.057933251063029

Epoch: 6| Step: 9
Training loss: 1.7743290662765503
Validation loss: 2.066091239452362

Epoch: 6| Step: 10
Training loss: 2.023549795150757
Validation loss: 2.065831442674001

Epoch: 6| Step: 11
Training loss: 1.6939213275909424
Validation loss: 2.0658297340075173

Epoch: 6| Step: 12
Training loss: 2.116703987121582
Validation loss: 2.083466370900472

Epoch: 6| Step: 13
Training loss: 2.1729025840759277
Validation loss: 2.082882265249888

Epoch: 142| Step: 0
Training loss: 2.7537894248962402
Validation loss: 2.106912155946096

Epoch: 6| Step: 1
Training loss: 1.6913249492645264
Validation loss: 2.1020832856496177

Epoch: 6| Step: 2
Training loss: 2.2358858585357666
Validation loss: 2.127141276995341

Epoch: 6| Step: 3
Training loss: 2.2800710201263428
Validation loss: 2.1376280188560486

Epoch: 6| Step: 4
Training loss: 1.4341561794281006
Validation loss: 2.1417091290156045

Epoch: 6| Step: 5
Training loss: 1.9741499423980713
Validation loss: 2.1522735158602395

Epoch: 6| Step: 6
Training loss: 1.733824610710144
Validation loss: 2.1289321184158325

Epoch: 6| Step: 7
Training loss: 2.5765933990478516
Validation loss: 2.101981222629547

Epoch: 6| Step: 8
Training loss: 1.736790418624878
Validation loss: 2.079569101333618

Epoch: 6| Step: 9
Training loss: 1.8623507022857666
Validation loss: 2.073707183202108

Epoch: 6| Step: 10
Training loss: 2.0086007118225098
Validation loss: 2.0404629906018577

Epoch: 6| Step: 11
Training loss: 1.8109009265899658
Validation loss: 2.0522256096204123

Epoch: 6| Step: 12
Training loss: 2.2916483879089355
Validation loss: 2.039095918337504

Epoch: 6| Step: 13
Training loss: 2.2571778297424316
Validation loss: 2.050344983736674

Epoch: 143| Step: 0
Training loss: 2.115610122680664
Validation loss: 2.065596123536428

Epoch: 6| Step: 1
Training loss: 1.8172985315322876
Validation loss: 2.0603065888086953

Epoch: 6| Step: 2
Training loss: 1.67733633518219
Validation loss: 2.050553262233734

Epoch: 6| Step: 3
Training loss: 1.8331207036972046
Validation loss: 2.064070443312327

Epoch: 6| Step: 4
Training loss: 2.804180860519409
Validation loss: 2.0612307588259378

Epoch: 6| Step: 5
Training loss: 1.732323169708252
Validation loss: 2.0509469509124756

Epoch: 6| Step: 6
Training loss: 2.7888376712799072
Validation loss: 2.046366552511851

Epoch: 6| Step: 7
Training loss: 2.140150547027588
Validation loss: 2.041602373123169

Epoch: 6| Step: 8
Training loss: 2.1957972049713135
Validation loss: 2.0617622534434

Epoch: 6| Step: 9
Training loss: 2.210648536682129
Validation loss: 2.0456457138061523

Epoch: 6| Step: 10
Training loss: 1.785337209701538
Validation loss: 2.0483381748199463

Epoch: 6| Step: 11
Training loss: 1.9639410972595215
Validation loss: 2.0466777086257935

Epoch: 6| Step: 12
Training loss: 1.8365072011947632
Validation loss: 2.038781305154165

Epoch: 6| Step: 13
Training loss: 1.9787503480911255
Validation loss: 2.064304749170939

Epoch: 144| Step: 0
Training loss: 1.7184878587722778
Validation loss: 2.07336433728536

Epoch: 6| Step: 1
Training loss: 2.0997114181518555
Validation loss: 2.0978004535039267

Epoch: 6| Step: 2
Training loss: 2.083160161972046
Validation loss: 2.119611700375875

Epoch: 6| Step: 3
Training loss: 2.776937246322632
Validation loss: 2.138128479321798

Epoch: 6| Step: 4
Training loss: 2.4698410034179688
Validation loss: 2.13769938548406

Epoch: 6| Step: 5
Training loss: 2.0703091621398926
Validation loss: 2.1263746420542398

Epoch: 6| Step: 6
Training loss: 1.9610419273376465
Validation loss: 2.112122356891632

Epoch: 6| Step: 7
Training loss: 1.6497209072113037
Validation loss: 2.0877705017725625

Epoch: 6| Step: 8
Training loss: 1.470190405845642
Validation loss: 2.0815352400143943

Epoch: 6| Step: 9
Training loss: 1.8819947242736816
Validation loss: 2.066319545110067

Epoch: 6| Step: 10
Training loss: 2.0429646968841553
Validation loss: 2.0267165899276733

Epoch: 6| Step: 11
Training loss: 2.458042860031128
Validation loss: 2.0278417666753135

Epoch: 6| Step: 12
Training loss: 2.0135629177093506
Validation loss: 2.0202568968137107

Epoch: 6| Step: 13
Training loss: 2.064438819885254
Validation loss: 2.0184815724690757

Epoch: 145| Step: 0
Training loss: 2.5830001831054688
Validation loss: 2.046487828095754

Epoch: 6| Step: 1
Training loss: 1.7160613536834717
Validation loss: 2.0368212262789407

Epoch: 6| Step: 2
Training loss: 1.927636981010437
Validation loss: 2.0517055789629617

Epoch: 6| Step: 3
Training loss: 2.302879810333252
Validation loss: 2.0397934516270957

Epoch: 6| Step: 4
Training loss: 1.9597833156585693
Validation loss: 2.049088776111603

Epoch: 6| Step: 5
Training loss: 2.0788047313690186
Validation loss: 2.048646926879883

Epoch: 6| Step: 6
Training loss: 2.0965757369995117
Validation loss: 2.040972630182902

Epoch: 6| Step: 7
Training loss: 2.1016993522644043
Validation loss: 2.039132217566172

Epoch: 6| Step: 8
Training loss: 2.6153621673583984
Validation loss: 2.0470041831334433

Epoch: 6| Step: 9
Training loss: 2.2592835426330566
Validation loss: 2.032032529513041

Epoch: 6| Step: 10
Training loss: 1.4240849018096924
Validation loss: 2.023655732472738

Epoch: 6| Step: 11
Training loss: 2.279470443725586
Validation loss: 2.011951764424642

Epoch: 6| Step: 12
Training loss: 2.1279144287109375
Validation loss: 2.0082273483276367

Epoch: 6| Step: 13
Training loss: 2.2117581367492676
Validation loss: 2.007209300994873

Epoch: 146| Step: 0
Training loss: 1.597620964050293
Validation loss: 2.012688080469767

Epoch: 6| Step: 1
Training loss: 1.9273955821990967
Validation loss: 2.036628246307373

Epoch: 6| Step: 2
Training loss: 2.4713284969329834
Validation loss: 2.0302518208821616

Epoch: 6| Step: 3
Training loss: 1.880974292755127
Validation loss: 2.039312243461609

Epoch: 6| Step: 4
Training loss: 1.8828964233398438
Validation loss: 2.03811913728714

Epoch: 6| Step: 5
Training loss: 2.0062828063964844
Validation loss: 2.052178223927816

Epoch: 6| Step: 6
Training loss: 2.02134370803833
Validation loss: 2.0557976762453714

Epoch: 6| Step: 7
Training loss: 2.3641397953033447
Validation loss: 2.0535734494527182

Epoch: 6| Step: 8
Training loss: 2.366082191467285
Validation loss: 2.0531720916430154

Epoch: 6| Step: 9
Training loss: 1.9481656551361084
Validation loss: 2.047856072584788

Epoch: 6| Step: 10
Training loss: 2.1613354682922363
Validation loss: 2.0480844974517822

Epoch: 6| Step: 11
Training loss: 2.4757375717163086
Validation loss: 2.056410868962606

Epoch: 6| Step: 12
Training loss: 1.5022917985916138
Validation loss: 2.0591974457105002

Epoch: 6| Step: 13
Training loss: 1.7411770820617676
Validation loss: 2.060197035471598

Epoch: 147| Step: 0
Training loss: 1.831704020500183
Validation loss: 2.069734533627828

Epoch: 6| Step: 1
Training loss: 1.8787188529968262
Validation loss: 2.063177704811096

Epoch: 6| Step: 2
Training loss: 2.7161777019500732
Validation loss: 2.069333632787069

Epoch: 6| Step: 3
Training loss: 1.6484432220458984
Validation loss: 2.064000050226847

Epoch: 6| Step: 4
Training loss: 1.835632085800171
Validation loss: 2.05419268210729

Epoch: 6| Step: 5
Training loss: 2.025254487991333
Validation loss: 2.057459990183512

Epoch: 6| Step: 6
Training loss: 2.051708221435547
Validation loss: 2.0647632280985513

Epoch: 6| Step: 7
Training loss: 1.8223375082015991
Validation loss: 2.063353419303894

Epoch: 6| Step: 8
Training loss: 1.8144886493682861
Validation loss: 2.0780142744382224

Epoch: 6| Step: 9
Training loss: 2.6188230514526367
Validation loss: 2.066072940826416

Epoch: 6| Step: 10
Training loss: 1.6658450365066528
Validation loss: 2.07276984055837

Epoch: 6| Step: 11
Training loss: 2.507449150085449
Validation loss: 2.089547256628672

Epoch: 6| Step: 12
Training loss: 1.9119523763656616
Validation loss: 2.0677340229352317

Epoch: 6| Step: 13
Training loss: 1.96143639087677
Validation loss: 2.0709882775942483

Epoch: 148| Step: 0
Training loss: 2.468346118927002
Validation loss: 2.0725831588109336

Epoch: 6| Step: 1
Training loss: 2.136524200439453
Validation loss: 2.090415100256602

Epoch: 6| Step: 2
Training loss: 1.6894524097442627
Validation loss: 2.0816184282302856

Epoch: 6| Step: 3
Training loss: 2.0717084407806396
Validation loss: 2.080447793006897

Epoch: 6| Step: 4
Training loss: 1.5654890537261963
Validation loss: 2.0847858587900796

Epoch: 6| Step: 5
Training loss: 2.240985155105591
Validation loss: 2.078946510950724

Epoch: 6| Step: 6
Training loss: 1.362675428390503
Validation loss: 2.0822208523750305

Epoch: 6| Step: 7
Training loss: 2.2406835556030273
Validation loss: 2.069136997063955

Epoch: 6| Step: 8
Training loss: 1.729156255722046
Validation loss: 2.063246170679728

Epoch: 6| Step: 9
Training loss: 2.2223496437072754
Validation loss: 2.0534821152687073

Epoch: 6| Step: 10
Training loss: 2.0136380195617676
Validation loss: 2.0686665972073874

Epoch: 6| Step: 11
Training loss: 1.9045759439468384
Validation loss: 2.0593385895093284

Epoch: 6| Step: 12
Training loss: 2.442674398422241
Validation loss: 2.0668590863545737

Epoch: 6| Step: 13
Training loss: 2.169332981109619
Validation loss: 2.059803386529287

Epoch: 149| Step: 0
Training loss: 1.7309004068374634
Validation loss: 2.0477002461751304

Epoch: 6| Step: 1
Training loss: 2.5214061737060547
Validation loss: 2.0498103300730386

Epoch: 6| Step: 2
Training loss: 2.1807539463043213
Validation loss: 2.060081640879313

Epoch: 6| Step: 3
Training loss: 2.072826385498047
Validation loss: 2.061990280946096

Epoch: 6| Step: 4
Training loss: 2.3380126953125
Validation loss: 2.0614450375239053

Epoch: 6| Step: 5
Training loss: 1.7647697925567627
Validation loss: 2.072502831617991

Epoch: 6| Step: 6
Training loss: 1.5206403732299805
Validation loss: 2.0524436235427856

Epoch: 6| Step: 7
Training loss: 2.1168997287750244
Validation loss: 2.065314451853434

Epoch: 6| Step: 8
Training loss: 1.3507893085479736
Validation loss: 2.075665295124054

Epoch: 6| Step: 9
Training loss: 1.580005407333374
Validation loss: 2.0801047484079995

Epoch: 6| Step: 10
Training loss: 2.3748891353607178
Validation loss: 2.0862249533335366

Epoch: 6| Step: 11
Training loss: 1.9462753534317017
Validation loss: 2.1025071144104004

Epoch: 6| Step: 12
Training loss: 2.5433549880981445
Validation loss: 2.0877351562182107

Epoch: 6| Step: 13
Training loss: 1.6889017820358276
Validation loss: 2.0970680117607117

Epoch: 150| Step: 0
Training loss: 2.056504011154175
Validation loss: 2.0963593125343323

Epoch: 6| Step: 1
Training loss: 2.337231397628784
Validation loss: 2.0738696455955505

Epoch: 6| Step: 2
Training loss: 2.0384206771850586
Validation loss: 2.090192139148712

Epoch: 6| Step: 3
Training loss: 1.6708942651748657
Validation loss: 2.0759507417678833

Epoch: 6| Step: 4
Training loss: 2.185457229614258
Validation loss: 2.0949841936429343

Epoch: 6| Step: 5
Training loss: 1.9865450859069824
Validation loss: 2.090801477432251

Epoch: 6| Step: 6
Training loss: 1.8278404474258423
Validation loss: 2.0746534864107766

Epoch: 6| Step: 7
Training loss: 2.2264552116394043
Validation loss: 2.0874305168787637

Epoch: 6| Step: 8
Training loss: 2.037933826446533
Validation loss: 2.0801824728647866

Epoch: 6| Step: 9
Training loss: 2.126568078994751
Validation loss: 2.085572441418966

Epoch: 6| Step: 10
Training loss: 2.0256741046905518
Validation loss: 2.0764897068341575

Epoch: 6| Step: 11
Training loss: 2.2113900184631348
Validation loss: 2.073488493760427

Epoch: 6| Step: 12
Training loss: 1.8887460231781006
Validation loss: 2.093614916006724

Epoch: 6| Step: 13
Training loss: 1.5351015329360962
Validation loss: 2.092699646949768

Epoch: 151| Step: 0
Training loss: 1.974868893623352
Validation loss: 2.0880147020022073

Epoch: 6| Step: 1
Training loss: 1.8307360410690308
Validation loss: 2.0864120721817017

Epoch: 6| Step: 2
Training loss: 2.9398956298828125
Validation loss: 2.0907278259595237

Epoch: 6| Step: 3
Training loss: 2.1089067459106445
Validation loss: 2.087925434112549

Epoch: 6| Step: 4
Training loss: 1.916330099105835
Validation loss: 2.1047834555308023

Epoch: 6| Step: 5
Training loss: 1.9082432985305786
Validation loss: 2.093841334184011

Epoch: 6| Step: 6
Training loss: 1.6827818155288696
Validation loss: 2.1076303720474243

Epoch: 6| Step: 7
Training loss: 2.2851572036743164
Validation loss: 2.1098068952560425

Epoch: 6| Step: 8
Training loss: 2.276785373687744
Validation loss: 2.120206594467163

Epoch: 6| Step: 9
Training loss: 1.492469072341919
Validation loss: 2.120199680328369

Epoch: 6| Step: 10
Training loss: 1.9432662725448608
Validation loss: 2.1017093857129416

Epoch: 6| Step: 11
Training loss: 1.4973053932189941
Validation loss: 2.091149071852366

Epoch: 6| Step: 12
Training loss: 2.1455531120300293
Validation loss: 2.0875835021336875

Epoch: 6| Step: 13
Training loss: 1.9295843839645386
Validation loss: 2.089363475640615

Epoch: 152| Step: 0
Training loss: 1.9507272243499756
Validation loss: 2.087695916493734

Epoch: 6| Step: 1
Training loss: 1.898646354675293
Validation loss: 2.0994896491368613

Epoch: 6| Step: 2
Training loss: 1.8594143390655518
Validation loss: 2.0902229150136313

Epoch: 6| Step: 3
Training loss: 1.9307466745376587
Validation loss: 2.0993226369222007

Epoch: 6| Step: 4
Training loss: 1.7891956567764282
Validation loss: 2.0840415557225547

Epoch: 6| Step: 5
Training loss: 1.5226638317108154
Validation loss: 2.094677448272705

Epoch: 6| Step: 6
Training loss: 1.9682114124298096
Validation loss: 2.098675032456716

Epoch: 6| Step: 7
Training loss: 2.06276273727417
Validation loss: 2.084059456984202

Epoch: 6| Step: 8
Training loss: 1.6306486129760742
Validation loss: 2.1004676620165506

Epoch: 6| Step: 9
Training loss: 1.7942414283752441
Validation loss: 2.105302413304647

Epoch: 6| Step: 10
Training loss: 2.1638598442077637
Validation loss: 2.0980120102564492

Epoch: 6| Step: 11
Training loss: 2.138794422149658
Validation loss: 2.101327816645304

Epoch: 6| Step: 12
Training loss: 3.068477153778076
Validation loss: 2.1035122076670327

Epoch: 6| Step: 13
Training loss: 1.8325225114822388
Validation loss: 2.087103486061096

Epoch: 153| Step: 0
Training loss: 2.1757395267486572
Validation loss: 2.094541529814402

Epoch: 6| Step: 1
Training loss: 2.510556221008301
Validation loss: 2.1031216581662497

Epoch: 6| Step: 2
Training loss: 2.141270160675049
Validation loss: 2.0980100433031716

Epoch: 6| Step: 3
Training loss: 1.6224786043167114
Validation loss: 2.0964855353037515

Epoch: 6| Step: 4
Training loss: 1.8767547607421875
Validation loss: 2.097890019416809

Epoch: 6| Step: 5
Training loss: 2.5437662601470947
Validation loss: 2.093741297721863

Epoch: 6| Step: 6
Training loss: 2.0389204025268555
Validation loss: 2.1003536780675254

Epoch: 6| Step: 7
Training loss: 1.9434537887573242
Validation loss: 2.096976121266683

Epoch: 6| Step: 8
Training loss: 1.2289153337478638
Validation loss: 2.0867026845614114

Epoch: 6| Step: 9
Training loss: 1.568007230758667
Validation loss: 2.1037866274515786

Epoch: 6| Step: 10
Training loss: 2.033371686935425
Validation loss: 2.0973058144251504

Epoch: 6| Step: 11
Training loss: 1.5684894323349
Validation loss: 2.0903426011403403

Epoch: 6| Step: 12
Training loss: 1.8123383522033691
Validation loss: 2.098637819290161

Epoch: 6| Step: 13
Training loss: 2.585723400115967
Validation loss: 2.082714875539144

Epoch: 154| Step: 0
Training loss: 2.072719097137451
Validation loss: 2.0808403491973877

Epoch: 6| Step: 1
Training loss: 2.0916309356689453
Validation loss: 2.0896233121554055

Epoch: 6| Step: 2
Training loss: 2.073535919189453
Validation loss: 2.0891441305478415

Epoch: 6| Step: 3
Training loss: 1.5766340494155884
Validation loss: 2.082657794157664

Epoch: 6| Step: 4
Training loss: 1.7570319175720215
Validation loss: 2.0562407970428467

Epoch: 6| Step: 5
Training loss: 2.098191261291504
Validation loss: 2.0472870667775473

Epoch: 6| Step: 6
Training loss: 1.9757893085479736
Validation loss: 2.0685797929763794

Epoch: 6| Step: 7
Training loss: 1.9514364004135132
Validation loss: 2.051129957040151

Epoch: 6| Step: 8
Training loss: 1.8792085647583008
Validation loss: 2.04694139957428

Epoch: 6| Step: 9
Training loss: 2.3712925910949707
Validation loss: 2.0628572503725686

Epoch: 6| Step: 10
Training loss: 2.2017335891723633
Validation loss: 2.0483544071515403

Epoch: 6| Step: 11
Training loss: 2.4042348861694336
Validation loss: 2.053662359714508

Epoch: 6| Step: 12
Training loss: 2.1603589057922363
Validation loss: 2.047903080781301

Epoch: 6| Step: 13
Training loss: 2.1966781616210938
Validation loss: 2.0585023760795593

Epoch: 155| Step: 0
Training loss: 2.241518974304199
Validation loss: 2.0685529311498008

Epoch: 6| Step: 1
Training loss: 2.1754117012023926
Validation loss: 2.0630755623181662

Epoch: 6| Step: 2
Training loss: 1.9553061723709106
Validation loss: 2.082640846570333

Epoch: 6| Step: 3
Training loss: 1.5725297927856445
Validation loss: 2.077908436457316

Epoch: 6| Step: 4
Training loss: 2.227201223373413
Validation loss: 2.0953523317972818

Epoch: 6| Step: 5
Training loss: 1.5588799715042114
Validation loss: 2.1025959650675454

Epoch: 6| Step: 6
Training loss: 2.233977794647217
Validation loss: 2.1036182244618735

Epoch: 6| Step: 7
Training loss: 1.9199788570404053
Validation loss: 2.1014447013537088

Epoch: 6| Step: 8
Training loss: 2.149829626083374
Validation loss: 2.1067094206809998

Epoch: 6| Step: 9
Training loss: 2.5762016773223877
Validation loss: 2.1038469473520913

Epoch: 6| Step: 10
Training loss: 1.7407982349395752
Validation loss: 2.084099034468333

Epoch: 6| Step: 11
Training loss: 2.3440821170806885
Validation loss: 2.1099074681599936

Epoch: 6| Step: 12
Training loss: 1.7586572170257568
Validation loss: 2.097234090169271

Epoch: 6| Step: 13
Training loss: 1.465228796005249
Validation loss: 2.1039582888285318

Epoch: 156| Step: 0
Training loss: 1.668776273727417
Validation loss: 2.08894807100296

Epoch: 6| Step: 1
Training loss: 2.148670196533203
Validation loss: 2.0913572311401367

Epoch: 6| Step: 2
Training loss: 2.717632532119751
Validation loss: 2.0857259035110474

Epoch: 6| Step: 3
Training loss: 2.3214104175567627
Validation loss: 2.074697176615397

Epoch: 6| Step: 4
Training loss: 1.8277584314346313
Validation loss: 2.0644596815109253

Epoch: 6| Step: 5
Training loss: 1.9937973022460938
Validation loss: 2.0638986031214395

Epoch: 6| Step: 6
Training loss: 1.826930284500122
Validation loss: 2.0786598523457847

Epoch: 6| Step: 7
Training loss: 1.8932607173919678
Validation loss: 2.0810694893201194

Epoch: 6| Step: 8
Training loss: 1.703507661819458
Validation loss: 2.0787742932637534

Epoch: 6| Step: 9
Training loss: 2.3734490871429443
Validation loss: 2.0914002656936646

Epoch: 6| Step: 10
Training loss: 2.2327957153320312
Validation loss: 2.114882210890452

Epoch: 6| Step: 11
Training loss: 1.8741188049316406
Validation loss: 2.1061497728029885

Epoch: 6| Step: 12
Training loss: 1.5967473983764648
Validation loss: 2.104921062787374

Epoch: 6| Step: 13
Training loss: 2.043095350265503
Validation loss: 2.1089813907941184

Epoch: 157| Step: 0
Training loss: 2.136061191558838
Validation loss: 2.112481196721395

Epoch: 6| Step: 1
Training loss: 1.492997407913208
Validation loss: 2.10148694117864

Epoch: 6| Step: 2
Training loss: 1.644010305404663
Validation loss: 2.09105650583903

Epoch: 6| Step: 3
Training loss: 1.7936041355133057
Validation loss: 2.100638528664907

Epoch: 6| Step: 4
Training loss: 1.7746262550354004
Validation loss: 2.102636218070984

Epoch: 6| Step: 5
Training loss: 1.5632903575897217
Validation loss: 2.079923232396444

Epoch: 6| Step: 6
Training loss: 2.447633743286133
Validation loss: 2.0720218618710837

Epoch: 6| Step: 7
Training loss: 1.9452192783355713
Validation loss: 2.0749284625053406

Epoch: 6| Step: 8
Training loss: 1.723108172416687
Validation loss: 2.078028420607249

Epoch: 6| Step: 9
Training loss: 2.584437370300293
Validation loss: 2.082633395989736

Epoch: 6| Step: 10
Training loss: 1.781451940536499
Validation loss: 2.074279268582662

Epoch: 6| Step: 11
Training loss: 1.9719264507293701
Validation loss: 2.1035201152165732

Epoch: 6| Step: 12
Training loss: 2.169142246246338
Validation loss: 2.098567624886831

Epoch: 6| Step: 13
Training loss: 2.604302406311035
Validation loss: 2.0832629203796387

Epoch: 158| Step: 0
Training loss: 1.4832806587219238
Validation loss: 2.0884823203086853

Epoch: 6| Step: 1
Training loss: 1.7535028457641602
Validation loss: 2.0820765495300293

Epoch: 6| Step: 2
Training loss: 2.2215356826782227
Validation loss: 2.108861565589905

Epoch: 6| Step: 3
Training loss: 1.6101499795913696
Validation loss: 2.1068451007207236

Epoch: 6| Step: 4
Training loss: 2.083127498626709
Validation loss: 2.1127372781435647

Epoch: 6| Step: 5
Training loss: 1.7705261707305908
Validation loss: 2.0953749815622964

Epoch: 6| Step: 6
Training loss: 2.062401056289673
Validation loss: 2.1185290217399597

Epoch: 6| Step: 7
Training loss: 1.958702564239502
Validation loss: 2.1164312163988748

Epoch: 6| Step: 8
Training loss: 3.2705678939819336
Validation loss: 2.103980859120687

Epoch: 6| Step: 9
Training loss: 2.1893763542175293
Validation loss: 2.096900920073191

Epoch: 6| Step: 10
Training loss: 1.9330543279647827
Validation loss: 2.113415777683258

Epoch: 6| Step: 11
Training loss: 2.0641531944274902
Validation loss: 2.1115604440371194

Epoch: 6| Step: 12
Training loss: 1.7662361860275269
Validation loss: 2.0946032404899597

Epoch: 6| Step: 13
Training loss: 1.4296298027038574
Validation loss: 2.1076345443725586

Epoch: 159| Step: 0
Training loss: 2.4846463203430176
Validation loss: 2.088962276776632

Epoch: 6| Step: 1
Training loss: 1.9996687173843384
Validation loss: 2.0725292364756265

Epoch: 6| Step: 2
Training loss: 1.889407992362976
Validation loss: 2.064324458440145

Epoch: 6| Step: 3
Training loss: 1.520174503326416
Validation loss: 2.053009847799937

Epoch: 6| Step: 4
Training loss: 2.072122097015381
Validation loss: 2.046205222606659

Epoch: 6| Step: 5
Training loss: 2.069838047027588
Validation loss: 2.056911031405131

Epoch: 6| Step: 6
Training loss: 2.1231493949890137
Validation loss: 2.0674957235654197

Epoch: 6| Step: 7
Training loss: 2.4000706672668457
Validation loss: 2.066476345062256

Epoch: 6| Step: 8
Training loss: 2.2577619552612305
Validation loss: 2.0770832697550454

Epoch: 6| Step: 9
Training loss: 1.9732904434204102
Validation loss: 2.0984851916631064

Epoch: 6| Step: 10
Training loss: 1.9385541677474976
Validation loss: 2.1113274693489075

Epoch: 6| Step: 11
Training loss: 1.3010625839233398
Validation loss: 2.115695595741272

Epoch: 6| Step: 12
Training loss: 1.8373427391052246
Validation loss: 2.143500487009684

Epoch: 6| Step: 13
Training loss: 1.9115915298461914
Validation loss: 2.1463115413983664

Epoch: 160| Step: 0
Training loss: 1.937337040901184
Validation loss: 2.1414820551872253

Epoch: 6| Step: 1
Training loss: 2.148435354232788
Validation loss: 2.1331077814102173

Epoch: 6| Step: 2
Training loss: 2.048921585083008
Validation loss: 2.165029764175415

Epoch: 6| Step: 3
Training loss: 2.174875497817993
Validation loss: 2.16478963692983

Epoch: 6| Step: 4
Training loss: 1.3363075256347656
Validation loss: 2.165723701318105

Epoch: 6| Step: 5
Training loss: 2.354717254638672
Validation loss: 2.1272950569788613

Epoch: 6| Step: 6
Training loss: 2.190539836883545
Validation loss: 2.1393843491872153

Epoch: 6| Step: 7
Training loss: 1.9939978122711182
Validation loss: 2.145706593990326

Epoch: 6| Step: 8
Training loss: 1.898021936416626
Validation loss: 2.1330883502960205

Epoch: 6| Step: 9
Training loss: 1.8181700706481934
Validation loss: 2.100983719031016

Epoch: 6| Step: 10
Training loss: 2.318410873413086
Validation loss: 2.1010303695996604

Epoch: 6| Step: 11
Training loss: 1.4284173250198364
Validation loss: 2.07990300655365

Epoch: 6| Step: 12
Training loss: 1.920457363128662
Validation loss: 2.0619519551595054

Epoch: 6| Step: 13
Training loss: 2.100412607192993
Validation loss: 2.0714115301767984

Epoch: 161| Step: 0
Training loss: 2.06419038772583
Validation loss: 2.0696759025255838

Epoch: 6| Step: 1
Training loss: 1.995462417602539
Validation loss: 2.0740438302357993

Epoch: 6| Step: 2
Training loss: 1.730669617652893
Validation loss: 2.071491301059723

Epoch: 6| Step: 3
Training loss: 2.0688819885253906
Validation loss: 2.068792978922526

Epoch: 6| Step: 4
Training loss: 2.530839204788208
Validation loss: 2.069593886534373

Epoch: 6| Step: 5
Training loss: 1.983693242073059
Validation loss: 2.078100621700287

Epoch: 6| Step: 6
Training loss: 1.7428044080734253
Validation loss: 2.0781072775522866

Epoch: 6| Step: 7
Training loss: 1.594494342803955
Validation loss: 2.0657647450764975

Epoch: 6| Step: 8
Training loss: 1.883209228515625
Validation loss: 2.064738313357035

Epoch: 6| Step: 9
Training loss: 2.814239978790283
Validation loss: 2.0726863145828247

Epoch: 6| Step: 10
Training loss: 1.640657901763916
Validation loss: 2.0753443837165833

Epoch: 6| Step: 11
Training loss: 2.29864501953125
Validation loss: 2.08026917775472

Epoch: 6| Step: 12
Training loss: 2.436546564102173
Validation loss: 2.081386705239614

Epoch: 6| Step: 13
Training loss: 2.018551826477051
Validation loss: 2.080134868621826

Epoch: 162| Step: 0
Training loss: 2.01151180267334
Validation loss: 2.0978732307751975

Epoch: 6| Step: 1
Training loss: 2.021216869354248
Validation loss: 2.088092784086863

Epoch: 6| Step: 2
Training loss: 2.21187686920166
Validation loss: 2.094417134920756

Epoch: 6| Step: 3
Training loss: 2.3143575191497803
Validation loss: 2.1061607201894126

Epoch: 6| Step: 4
Training loss: 2.490673542022705
Validation loss: 2.1105364759763083

Epoch: 6| Step: 5
Training loss: 1.6426515579223633
Validation loss: 2.108896235624949

Epoch: 6| Step: 6
Training loss: 2.0794897079467773
Validation loss: 2.1253105799357095

Epoch: 6| Step: 7
Training loss: 1.2456979751586914
Validation loss: 2.1230390866597495

Epoch: 6| Step: 8
Training loss: 2.3546531200408936
Validation loss: 2.136474331219991

Epoch: 6| Step: 9
Training loss: 1.9205539226531982
Validation loss: 2.1131906708081565

Epoch: 6| Step: 10
Training loss: 1.4178067445755005
Validation loss: 2.1003750562667847

Epoch: 6| Step: 11
Training loss: 1.5836901664733887
Validation loss: 2.0941248138745627

Epoch: 6| Step: 12
Training loss: 2.323155164718628
Validation loss: 2.086678624153137

Epoch: 6| Step: 13
Training loss: 2.218838930130005
Validation loss: 2.070654253164927

Epoch: 163| Step: 0
Training loss: 1.7638516426086426
Validation loss: 2.057741403579712

Epoch: 6| Step: 1
Training loss: 1.8720755577087402
Validation loss: 2.0496241052945456

Epoch: 6| Step: 2
Training loss: 1.6190773248672485
Validation loss: 2.0569360057512918

Epoch: 6| Step: 3
Training loss: 2.588489532470703
Validation loss: 2.055553356806437

Epoch: 6| Step: 4
Training loss: 1.8288934230804443
Validation loss: 2.047563672065735

Epoch: 6| Step: 5
Training loss: 2.4482760429382324
Validation loss: 2.0567103028297424

Epoch: 6| Step: 6
Training loss: 1.7860679626464844
Validation loss: 2.0492573777834573

Epoch: 6| Step: 7
Training loss: 1.876719355583191
Validation loss: 2.04618247350057

Epoch: 6| Step: 8
Training loss: 2.2776734828948975
Validation loss: 2.05158523718516

Epoch: 6| Step: 9
Training loss: 2.213512659072876
Validation loss: 2.055873175462087

Epoch: 6| Step: 10
Training loss: 1.848029375076294
Validation loss: 2.0658671061197915

Epoch: 6| Step: 11
Training loss: 2.0253775119781494
Validation loss: 2.066759010155996

Epoch: 6| Step: 12
Training loss: 2.3532962799072266
Validation loss: 2.073519508043925

Epoch: 6| Step: 13
Training loss: 1.465273380279541
Validation loss: 2.0772001147270203

Epoch: 164| Step: 0
Training loss: 1.9272091388702393
Validation loss: 2.0884934862454734

Epoch: 6| Step: 1
Training loss: 2.137674570083618
Validation loss: 2.097464621067047

Epoch: 6| Step: 2
Training loss: 2.198096513748169
Validation loss: 2.0959129532178244

Epoch: 6| Step: 3
Training loss: 1.7535653114318848
Validation loss: 2.094878931840261

Epoch: 6| Step: 4
Training loss: 1.8661131858825684
Validation loss: 2.084039032459259

Epoch: 6| Step: 5
Training loss: 1.6219689846038818
Validation loss: 2.072210113207499

Epoch: 6| Step: 6
Training loss: 1.5215567350387573
Validation loss: 2.0879923701286316

Epoch: 6| Step: 7
Training loss: 2.074906826019287
Validation loss: 2.0547447005907693

Epoch: 6| Step: 8
Training loss: 2.366525888442993
Validation loss: 2.0541410644849143

Epoch: 6| Step: 9
Training loss: 2.1080210208892822
Validation loss: 2.0627569556236267

Epoch: 6| Step: 10
Training loss: 3.077496290206909
Validation loss: 2.067742864290873

Epoch: 6| Step: 11
Training loss: 1.3795604705810547
Validation loss: 2.056572755177816

Epoch: 6| Step: 12
Training loss: 1.930009126663208
Validation loss: 2.0668094555536904

Epoch: 6| Step: 13
Training loss: 2.3908679485321045
Validation loss: 2.068136195341746

Epoch: 165| Step: 0
Training loss: 1.6389522552490234
Validation loss: 2.061439255873362

Epoch: 6| Step: 1
Training loss: 1.929875135421753
Validation loss: 2.0779225826263428

Epoch: 6| Step: 2
Training loss: 1.6928895711898804
Validation loss: 2.1019054651260376

Epoch: 6| Step: 3
Training loss: 1.4881528615951538
Validation loss: 2.1118237574895224

Epoch: 6| Step: 4
Training loss: 2.5049526691436768
Validation loss: 2.1147825519243875

Epoch: 6| Step: 5
Training loss: 2.179683208465576
Validation loss: 2.112128814061483

Epoch: 6| Step: 6
Training loss: 2.3962502479553223
Validation loss: 2.112265149752299

Epoch: 6| Step: 7
Training loss: 2.0065178871154785
Validation loss: 2.1135504643122354

Epoch: 6| Step: 8
Training loss: 1.6396576166152954
Validation loss: 2.09525473912557

Epoch: 6| Step: 9
Training loss: 1.9023557901382446
Validation loss: 2.0910131136576333

Epoch: 6| Step: 10
Training loss: 2.2275357246398926
Validation loss: 2.0801731944084167

Epoch: 6| Step: 11
Training loss: 2.5906224250793457
Validation loss: 2.0841107964515686

Epoch: 6| Step: 12
Training loss: 1.9548766613006592
Validation loss: 2.081166406472524

Epoch: 6| Step: 13
Training loss: 1.708345890045166
Validation loss: 2.075310230255127

Epoch: 166| Step: 0
Training loss: 2.2349534034729004
Validation loss: 2.080614427725474

Epoch: 6| Step: 1
Training loss: 1.5468533039093018
Validation loss: 2.0776864886283875

Epoch: 6| Step: 2
Training loss: 1.5870916843414307
Validation loss: 2.066335062185923

Epoch: 6| Step: 3
Training loss: 1.5349658727645874
Validation loss: 2.0715553959210715

Epoch: 6| Step: 4
Training loss: 2.3516452312469482
Validation loss: 2.066266437371572

Epoch: 6| Step: 5
Training loss: 2.041691541671753
Validation loss: 2.080508748690287

Epoch: 6| Step: 6
Training loss: 2.0188121795654297
Validation loss: 2.0795323848724365

Epoch: 6| Step: 7
Training loss: 2.3185887336730957
Validation loss: 2.0882915258407593

Epoch: 6| Step: 8
Training loss: 1.8355774879455566
Validation loss: 2.0793213844299316

Epoch: 6| Step: 9
Training loss: 1.769360065460205
Validation loss: 2.0912617246309915

Epoch: 6| Step: 10
Training loss: 2.204373836517334
Validation loss: 2.0889864365259805

Epoch: 6| Step: 11
Training loss: 1.8272080421447754
Validation loss: 2.082916518052419

Epoch: 6| Step: 12
Training loss: 1.8222402334213257
Validation loss: 2.106069286664327

Epoch: 6| Step: 13
Training loss: 2.3100881576538086
Validation loss: 2.1028590202331543

Epoch: 167| Step: 0
Training loss: 1.687401294708252
Validation loss: 2.1129578948020935

Epoch: 6| Step: 1
Training loss: 2.4717183113098145
Validation loss: 2.117120067278544

Epoch: 6| Step: 2
Training loss: 2.133854389190674
Validation loss: 2.1224234302838645

Epoch: 6| Step: 3
Training loss: 1.9323744773864746
Validation loss: 2.1259223421414695

Epoch: 6| Step: 4
Training loss: 1.7923966646194458
Validation loss: 2.1032881339391074

Epoch: 6| Step: 5
Training loss: 1.3771638870239258
Validation loss: 2.0947836438814798

Epoch: 6| Step: 6
Training loss: 1.9897220134735107
Validation loss: 2.104728559652964

Epoch: 6| Step: 7
Training loss: 2.9147844314575195
Validation loss: 2.0964198311169944

Epoch: 6| Step: 8
Training loss: 1.4317309856414795
Validation loss: 2.087076743443807

Epoch: 6| Step: 9
Training loss: 1.7062289714813232
Validation loss: 2.090829829374949

Epoch: 6| Step: 10
Training loss: 1.756023645401001
Validation loss: 2.086837967236837

Epoch: 6| Step: 11
Training loss: 2.173501968383789
Validation loss: 2.097116311391195

Epoch: 6| Step: 12
Training loss: 2.5299224853515625
Validation loss: 2.1025235652923584

Epoch: 6| Step: 13
Training loss: 1.5841619968414307
Validation loss: 2.0998010834058127

Epoch: 168| Step: 0
Training loss: 1.4186301231384277
Validation loss: 2.08503125111262

Epoch: 6| Step: 1
Training loss: 1.6105115413665771
Validation loss: 2.0991039872169495

Epoch: 6| Step: 2
Training loss: 2.2717909812927246
Validation loss: 2.1044785380363464

Epoch: 6| Step: 3
Training loss: 1.9434404373168945
Validation loss: 2.105420549710592

Epoch: 6| Step: 4
Training loss: 1.9290705919265747
Validation loss: 2.1078567504882812

Epoch: 6| Step: 5
Training loss: 2.140737533569336
Validation loss: 2.112614373366038

Epoch: 6| Step: 6
Training loss: 2.11885404586792
Validation loss: 2.1012059251467385

Epoch: 6| Step: 7
Training loss: 2.385317325592041
Validation loss: 2.1075179974238076

Epoch: 6| Step: 8
Training loss: 2.2183961868286133
Validation loss: 2.1042224566141763

Epoch: 6| Step: 9
Training loss: 1.7869426012039185
Validation loss: 2.122519095738729

Epoch: 6| Step: 10
Training loss: 2.1701889038085938
Validation loss: 2.1121655702590942

Epoch: 6| Step: 11
Training loss: 1.553934097290039
Validation loss: 2.105177919069926

Epoch: 6| Step: 12
Training loss: 1.7558850049972534
Validation loss: 2.1070050597190857

Epoch: 6| Step: 13
Training loss: 2.0384984016418457
Validation loss: 2.103308300177256

Epoch: 169| Step: 0
Training loss: 2.566070795059204
Validation loss: 2.0992013017336526

Epoch: 6| Step: 1
Training loss: 1.7803640365600586
Validation loss: 2.101988951365153

Epoch: 6| Step: 2
Training loss: 2.1381168365478516
Validation loss: 2.096116820971171

Epoch: 6| Step: 3
Training loss: 1.696700096130371
Validation loss: 2.1001150210698447

Epoch: 6| Step: 4
Training loss: 1.821073055267334
Validation loss: 2.1000990867614746

Epoch: 6| Step: 5
Training loss: 1.5740437507629395
Validation loss: 2.108985404173533

Epoch: 6| Step: 6
Training loss: 2.51165771484375
Validation loss: 2.1097592314084372

Epoch: 6| Step: 7
Training loss: 1.7246983051300049
Validation loss: 2.1178114811579385

Epoch: 6| Step: 8
Training loss: 2.2247376441955566
Validation loss: 2.0996944308280945

Epoch: 6| Step: 9
Training loss: 2.1757185459136963
Validation loss: 2.0881891449292502

Epoch: 6| Step: 10
Training loss: 2.169252872467041
Validation loss: 2.086766302585602

Epoch: 6| Step: 11
Training loss: 2.0101284980773926
Validation loss: 2.095171888669332

Epoch: 6| Step: 12
Training loss: 1.446004033088684
Validation loss: 2.097270429134369

Epoch: 6| Step: 13
Training loss: 1.5842342376708984
Validation loss: 2.0937907894452414

Epoch: 170| Step: 0
Training loss: 1.818697214126587
Validation loss: 2.1138598720232644

Epoch: 6| Step: 1
Training loss: 1.612398386001587
Validation loss: 2.1147984862327576

Epoch: 6| Step: 2
Training loss: 2.6968770027160645
Validation loss: 2.1113765041033425

Epoch: 6| Step: 3
Training loss: 1.9051105976104736
Validation loss: 2.1134989857673645

Epoch: 6| Step: 4
Training loss: 1.7477689981460571
Validation loss: 2.1331000526746116

Epoch: 6| Step: 5
Training loss: 1.709567666053772
Validation loss: 2.1258835792541504

Epoch: 6| Step: 6
Training loss: 2.1263628005981445
Validation loss: 2.1215933362642923

Epoch: 6| Step: 7
Training loss: 1.7846479415893555
Validation loss: 2.1237492163976035

Epoch: 6| Step: 8
Training loss: 2.0831665992736816
Validation loss: 2.126444379488627

Epoch: 6| Step: 9
Training loss: 1.521348476409912
Validation loss: 2.116129299004873

Epoch: 6| Step: 10
Training loss: 1.9177980422973633
Validation loss: 2.1110082864761353

Epoch: 6| Step: 11
Training loss: 2.1216835975646973
Validation loss: 2.1222081979115806

Epoch: 6| Step: 12
Training loss: 2.9114890098571777
Validation loss: 2.1147138675053916

Epoch: 6| Step: 13
Training loss: 1.27655029296875
Validation loss: 2.112074633439382

Epoch: 171| Step: 0
Training loss: 2.1854403018951416
Validation loss: 2.110596497853597

Epoch: 6| Step: 1
Training loss: 2.043989658355713
Validation loss: 2.106150229771932

Epoch: 6| Step: 2
Training loss: 1.9084417819976807
Validation loss: 2.1176992058753967

Epoch: 6| Step: 3
Training loss: 2.3849716186523438
Validation loss: 2.132015903790792

Epoch: 6| Step: 4
Training loss: 2.4886059761047363
Validation loss: 2.1420767108599343

Epoch: 6| Step: 5
Training loss: 1.5190324783325195
Validation loss: 2.120044390360514

Epoch: 6| Step: 6
Training loss: 1.4902222156524658
Validation loss: 2.1197495261828103

Epoch: 6| Step: 7
Training loss: 1.5683355331420898
Validation loss: 2.1358851194381714

Epoch: 6| Step: 8
Training loss: 2.4138758182525635
Validation loss: 2.1272843281428018

Epoch: 6| Step: 9
Training loss: 2.486788272857666
Validation loss: 2.135429859161377

Epoch: 6| Step: 10
Training loss: 1.848306655883789
Validation loss: 2.121516446272532

Epoch: 6| Step: 11
Training loss: 1.38372004032135
Validation loss: 2.1356639663378396

Epoch: 6| Step: 12
Training loss: 2.034968376159668
Validation loss: 2.125171502431234

Epoch: 6| Step: 13
Training loss: 1.79348886013031
Validation loss: 2.104398866494497

Epoch: 172| Step: 0
Training loss: 1.8274059295654297
Validation loss: 2.1049623489379883

Epoch: 6| Step: 1
Training loss: 2.432493209838867
Validation loss: 2.100660800933838

Epoch: 6| Step: 2
Training loss: 1.893986701965332
Validation loss: 2.089084506034851

Epoch: 6| Step: 3
Training loss: 2.123715400695801
Validation loss: 2.0955228010813394

Epoch: 6| Step: 4
Training loss: 1.5186399221420288
Validation loss: 2.090547740459442

Epoch: 6| Step: 5
Training loss: 1.5938544273376465
Validation loss: 2.0963135957717896

Epoch: 6| Step: 6
Training loss: 2.197598934173584
Validation loss: 2.101287841796875

Epoch: 6| Step: 7
Training loss: 1.6934815645217896
Validation loss: 2.087207774321238

Epoch: 6| Step: 8
Training loss: 2.4866795539855957
Validation loss: 2.110842486222585

Epoch: 6| Step: 9
Training loss: 1.3900315761566162
Validation loss: 2.1252746979395547

Epoch: 6| Step: 10
Training loss: 2.510287046432495
Validation loss: 2.1319758693377175

Epoch: 6| Step: 11
Training loss: 1.6602554321289062
Validation loss: 2.1429022550582886

Epoch: 6| Step: 12
Training loss: 1.611952304840088
Validation loss: 2.1553918719291687

Epoch: 6| Step: 13
Training loss: 2.1840312480926514
Validation loss: 2.161913534005483

Epoch: 173| Step: 0
Training loss: 2.323941230773926
Validation loss: 2.171121676762899

Epoch: 6| Step: 1
Training loss: 1.9609336853027344
Validation loss: 2.1507460276285806

Epoch: 6| Step: 2
Training loss: 2.021800994873047
Validation loss: 2.172253410021464

Epoch: 6| Step: 3
Training loss: 1.6383342742919922
Validation loss: 2.1457314491271973

Epoch: 6| Step: 4
Training loss: 2.1258199214935303
Validation loss: 2.152925173441569

Epoch: 6| Step: 5
Training loss: 1.410367488861084
Validation loss: 2.141906976699829

Epoch: 6| Step: 6
Training loss: 2.0173799991607666
Validation loss: 2.134865860144297

Epoch: 6| Step: 7
Training loss: 1.9050925970077515
Validation loss: 2.141832252343496

Epoch: 6| Step: 8
Training loss: 2.5984034538269043
Validation loss: 2.127814849217733

Epoch: 6| Step: 9
Training loss: 1.760088324546814
Validation loss: 2.117315689722697

Epoch: 6| Step: 10
Training loss: 1.8552484512329102
Validation loss: 2.1232460737228394

Epoch: 6| Step: 11
Training loss: 2.207974910736084
Validation loss: 2.1157725056012473

Epoch: 6| Step: 12
Training loss: 1.7683290243148804
Validation loss: 2.1251158515612283

Epoch: 6| Step: 13
Training loss: 1.671991229057312
Validation loss: 2.1211573680241904

Epoch: 174| Step: 0
Training loss: 1.4649949073791504
Validation loss: 2.1191360553105674

Epoch: 6| Step: 1
Training loss: 1.291886806488037
Validation loss: 2.1618890166282654

Epoch: 6| Step: 2
Training loss: 1.5975096225738525
Validation loss: 2.150538186232249

Epoch: 6| Step: 3
Training loss: 1.9234437942504883
Validation loss: 2.156839966773987

Epoch: 6| Step: 4
Training loss: 2.3450069427490234
Validation loss: 2.140989899635315

Epoch: 6| Step: 5
Training loss: 2.181577205657959
Validation loss: 2.1710766553878784

Epoch: 6| Step: 6
Training loss: 1.9763271808624268
Validation loss: 2.124500791231791

Epoch: 6| Step: 7
Training loss: 2.2873215675354004
Validation loss: 2.1169646581014

Epoch: 6| Step: 8
Training loss: 1.8587076663970947
Validation loss: 2.09448250134786

Epoch: 6| Step: 9
Training loss: 2.3773117065429688
Validation loss: 2.0867240826288858

Epoch: 6| Step: 10
Training loss: 1.914756417274475
Validation loss: 2.0780455668767295

Epoch: 6| Step: 11
Training loss: 2.0757994651794434
Validation loss: 2.092656354109446

Epoch: 6| Step: 12
Training loss: 2.4915521144866943
Validation loss: 2.0802932381629944

Epoch: 6| Step: 13
Training loss: 2.0522334575653076
Validation loss: 2.0907381176948547

Epoch: 175| Step: 0
Training loss: 2.089989185333252
Validation loss: 2.0841606060663858

Epoch: 6| Step: 1
Training loss: 1.8503118753433228
Validation loss: 2.098203738530477

Epoch: 6| Step: 2
Training loss: 1.466095209121704
Validation loss: 2.107823630174001

Epoch: 6| Step: 3
Training loss: 1.7694907188415527
Validation loss: 2.129749119281769

Epoch: 6| Step: 4
Training loss: 2.1495919227600098
Validation loss: 2.141960004965464

Epoch: 6| Step: 5
Training loss: 1.60418701171875
Validation loss: 2.156531731287638

Epoch: 6| Step: 6
Training loss: 1.835348129272461
Validation loss: 2.164461672306061

Epoch: 6| Step: 7
Training loss: 1.3702483177185059
Validation loss: 2.1586252450942993

Epoch: 6| Step: 8
Training loss: 1.8339096307754517
Validation loss: 2.160923699537913

Epoch: 6| Step: 9
Training loss: 2.506864070892334
Validation loss: 2.1518330176671348

Epoch: 6| Step: 10
Training loss: 2.5959930419921875
Validation loss: 2.147985517978668

Epoch: 6| Step: 11
Training loss: 2.5457944869995117
Validation loss: 2.147208273410797

Epoch: 6| Step: 12
Training loss: 2.037834882736206
Validation loss: 2.1475534439086914

Epoch: 6| Step: 13
Training loss: 1.7483054399490356
Validation loss: 2.1586294571558633

Testing loss: 1.7347145054837783

Epoch: 1| Step: 0
Training loss: 5.79017972946167
Validation loss: 5.3077365557352705

Epoch: 6| Step: 1
Training loss: 3.819209575653076
Validation loss: 5.305742820103963

Epoch: 6| Step: 2
Training loss: 5.223782539367676
Validation loss: 5.30396028359731

Epoch: 6| Step: 3
Training loss: 5.140588760375977
Validation loss: 5.302044153213501

Epoch: 6| Step: 4
Training loss: 4.444900035858154
Validation loss: 5.300218979517619

Epoch: 6| Step: 5
Training loss: 6.7617926597595215
Validation loss: 5.2984230518341064

Epoch: 6| Step: 6
Training loss: 5.540892601013184
Validation loss: 5.2966703573862715

Epoch: 6| Step: 7
Training loss: 6.150504112243652
Validation loss: 5.294873555501302

Epoch: 6| Step: 8
Training loss: 5.989332675933838
Validation loss: 5.293207327524821

Epoch: 6| Step: 9
Training loss: 4.568456649780273
Validation loss: 5.29124101003011

Epoch: 6| Step: 10
Training loss: 4.884738922119141
Validation loss: 5.289464871088664

Epoch: 6| Step: 11
Training loss: 5.645459175109863
Validation loss: 5.287708520889282

Epoch: 6| Step: 12
Training loss: 6.104035377502441
Validation loss: 5.28569491704305

Epoch: 6| Step: 13
Training loss: 5.040180206298828
Validation loss: 5.283838987350464

Epoch: 2| Step: 0
Training loss: 5.32045841217041
Validation loss: 5.281776189804077

Epoch: 6| Step: 1
Training loss: 4.0323944091796875
Validation loss: 5.2796681722005205

Epoch: 6| Step: 2
Training loss: 6.084328651428223
Validation loss: 5.277449528376262

Epoch: 6| Step: 3
Training loss: 6.151315689086914
Validation loss: 5.275216182072957

Epoch: 6| Step: 4
Training loss: 5.703761100769043
Validation loss: 5.2727750937143965

Epoch: 6| Step: 5
Training loss: 5.220648765563965
Validation loss: 5.270350694656372

Epoch: 6| Step: 6
Training loss: 6.314586639404297
Validation loss: 5.267628351847331

Epoch: 6| Step: 7
Training loss: 5.342504024505615
Validation loss: 5.264788786570231

Epoch: 6| Step: 8
Training loss: 4.61849308013916
Validation loss: 5.262030998865764

Epoch: 6| Step: 9
Training loss: 5.4558939933776855
Validation loss: 5.259002765019734

Epoch: 6| Step: 10
Training loss: 3.714160919189453
Validation loss: 5.255665381749471

Epoch: 6| Step: 11
Training loss: 5.311124801635742
Validation loss: 5.252280791600545

Epoch: 6| Step: 12
Training loss: 5.415919780731201
Validation loss: 5.248722791671753

Epoch: 6| Step: 13
Training loss: 5.992412090301514
Validation loss: 5.24492867787679

Epoch: 3| Step: 0
Training loss: 4.405563831329346
Validation loss: 5.2408303419748945

Epoch: 6| Step: 1
Training loss: 5.49352502822876
Validation loss: 5.236623287200928

Epoch: 6| Step: 2
Training loss: 5.564211845397949
Validation loss: 5.232277552286784

Epoch: 6| Step: 3
Training loss: 4.517884731292725
Validation loss: 5.227728168169658

Epoch: 6| Step: 4
Training loss: 5.947680473327637
Validation loss: 5.222715298334758

Epoch: 6| Step: 5
Training loss: 5.971489906311035
Validation loss: 5.217569033304851

Epoch: 6| Step: 6
Training loss: 4.7657036781311035
Validation loss: 5.212254126866658

Epoch: 6| Step: 7
Training loss: 5.994507312774658
Validation loss: 5.206583499908447

Epoch: 6| Step: 8
Training loss: 5.623662948608398
Validation loss: 5.2006964683532715

Epoch: 6| Step: 9
Training loss: 5.166586875915527
Validation loss: 5.194686730702718

Epoch: 6| Step: 10
Training loss: 5.219300270080566
Validation loss: 5.188400983810425

Epoch: 6| Step: 11
Training loss: 4.800136566162109
Validation loss: 5.18183962504069

Epoch: 6| Step: 12
Training loss: 4.845640182495117
Validation loss: 5.174696604410808

Epoch: 6| Step: 13
Training loss: 5.577081680297852
Validation loss: 5.167943239212036

Epoch: 4| Step: 0
Training loss: 5.794559955596924
Validation loss: 5.160718123118083

Epoch: 6| Step: 1
Training loss: 5.689704895019531
Validation loss: 5.1530836423238116

Epoch: 6| Step: 2
Training loss: 3.880143404006958
Validation loss: 5.1454652945200605

Epoch: 6| Step: 3
Training loss: 5.863153457641602
Validation loss: 5.13764230410258

Epoch: 6| Step: 4
Training loss: 5.12814998626709
Validation loss: 5.129496097564697

Epoch: 6| Step: 5
Training loss: 5.961562633514404
Validation loss: 5.121209780375163

Epoch: 6| Step: 6
Training loss: 4.044774055480957
Validation loss: 5.113011201222737

Epoch: 6| Step: 7
Training loss: 4.986972808837891
Validation loss: 5.104097048441569

Epoch: 6| Step: 8
Training loss: 4.465360641479492
Validation loss: 5.095477819442749

Epoch: 6| Step: 9
Training loss: 4.329179763793945
Validation loss: 5.08669130007426

Epoch: 6| Step: 10
Training loss: 5.613564491271973
Validation loss: 5.077433188756307

Epoch: 6| Step: 11
Training loss: 6.239105224609375
Validation loss: 5.068093935648601

Epoch: 6| Step: 12
Training loss: 5.171345233917236
Validation loss: 5.058937271436055

Epoch: 6| Step: 13
Training loss: 5.380796432495117
Validation loss: 5.049604336420695

Epoch: 5| Step: 0
Training loss: 4.901032447814941
Validation loss: 5.040448109308879

Epoch: 6| Step: 1
Training loss: 4.740442752838135
Validation loss: 5.030687967936198

Epoch: 6| Step: 2
Training loss: 5.283450126647949
Validation loss: 5.0211959679921465

Epoch: 6| Step: 3
Training loss: 5.009963035583496
Validation loss: 5.011415799458821

Epoch: 6| Step: 4
Training loss: 5.181899070739746
Validation loss: 5.001280307769775

Epoch: 6| Step: 5
Training loss: 4.850790977478027
Validation loss: 4.991660674413045

Epoch: 6| Step: 6
Training loss: 5.106592178344727
Validation loss: 4.9823817412058515

Epoch: 6| Step: 7
Training loss: 4.87632942199707
Validation loss: 4.972939809163411

Epoch: 6| Step: 8
Training loss: 4.170306205749512
Validation loss: 4.963459650675456

Epoch: 6| Step: 9
Training loss: 5.608633041381836
Validation loss: 4.953902244567871

Epoch: 6| Step: 10
Training loss: 5.928960800170898
Validation loss: 4.944101572036743

Epoch: 6| Step: 11
Training loss: 4.768970966339111
Validation loss: 4.9344414075215655

Epoch: 6| Step: 12
Training loss: 5.15736198425293
Validation loss: 4.924825350443522

Epoch: 6| Step: 13
Training loss: 5.237696647644043
Validation loss: 4.915161212285359

Epoch: 6| Step: 0
Training loss: 4.415678024291992
Validation loss: 4.9057871500651045

Epoch: 6| Step: 1
Training loss: 5.389800071716309
Validation loss: 4.89645795027415

Epoch: 6| Step: 2
Training loss: 5.093921661376953
Validation loss: 4.886737028757731

Epoch: 6| Step: 3
Training loss: 3.893738269805908
Validation loss: 4.877362171808879

Epoch: 6| Step: 4
Training loss: 5.191228866577148
Validation loss: 4.86829948425293

Epoch: 6| Step: 5
Training loss: 5.595189094543457
Validation loss: 4.858736515045166

Epoch: 6| Step: 6
Training loss: 5.787586212158203
Validation loss: 4.849596659342448

Epoch: 6| Step: 7
Training loss: 5.058383464813232
Validation loss: 4.840688705444336

Epoch: 6| Step: 8
Training loss: 4.698765754699707
Validation loss: 4.831518252690633

Epoch: 6| Step: 9
Training loss: 4.298163414001465
Validation loss: 4.822977105776469

Epoch: 6| Step: 10
Training loss: 5.78513765335083
Validation loss: 4.814703067143758

Epoch: 6| Step: 11
Training loss: 4.363048553466797
Validation loss: 4.806737581888835

Epoch: 6| Step: 12
Training loss: 5.610043525695801
Validation loss: 4.798490603764852

Epoch: 6| Step: 13
Training loss: 3.8994972705841064
Validation loss: 4.790756106376648

Epoch: 7| Step: 0
Training loss: 6.0940985679626465
Validation loss: 4.783198436101277

Epoch: 6| Step: 1
Training loss: 5.046520709991455
Validation loss: 4.775857051213582

Epoch: 6| Step: 2
Training loss: 4.729981422424316
Validation loss: 4.768610159556071

Epoch: 6| Step: 3
Training loss: 5.484193801879883
Validation loss: 4.761038343111674

Epoch: 6| Step: 4
Training loss: 4.006596565246582
Validation loss: 4.754775206247966

Epoch: 6| Step: 5
Training loss: 4.464937686920166
Validation loss: 4.747692108154297

Epoch: 6| Step: 6
Training loss: 5.608822822570801
Validation loss: 4.740596731503804

Epoch: 6| Step: 7
Training loss: 3.662670612335205
Validation loss: 4.7339794635772705

Epoch: 6| Step: 8
Training loss: 3.936309337615967
Validation loss: 4.727142095565796

Epoch: 6| Step: 9
Training loss: 4.6797685623168945
Validation loss: 4.720378081003825

Epoch: 6| Step: 10
Training loss: 5.114045143127441
Validation loss: 4.714454968770345

Epoch: 6| Step: 11
Training loss: 4.447037696838379
Validation loss: 4.707369486490886

Epoch: 6| Step: 12
Training loss: 4.594939231872559
Validation loss: 4.701090256373088

Epoch: 6| Step: 13
Training loss: 5.755936145782471
Validation loss: 4.694849173227946

Epoch: 8| Step: 0
Training loss: 5.145377159118652
Validation loss: 4.687565088272095

Epoch: 6| Step: 1
Training loss: 4.433681488037109
Validation loss: 4.680904308954875

Epoch: 6| Step: 2
Training loss: 3.428730010986328
Validation loss: 4.674890915552775

Epoch: 6| Step: 3
Training loss: 5.614515781402588
Validation loss: 4.668053030967712

Epoch: 6| Step: 4
Training loss: 4.6595306396484375
Validation loss: 4.66122563680013

Epoch: 6| Step: 5
Training loss: 5.719446182250977
Validation loss: 4.65452520052592

Epoch: 6| Step: 6
Training loss: 5.0127272605896
Validation loss: 4.647962649663289

Epoch: 6| Step: 7
Training loss: 4.3457841873168945
Validation loss: 4.640902678171794

Epoch: 6| Step: 8
Training loss: 4.821712970733643
Validation loss: 4.634439706802368

Epoch: 6| Step: 9
Training loss: 5.297639846801758
Validation loss: 4.627713759740193

Epoch: 6| Step: 10
Training loss: 3.797464370727539
Validation loss: 4.620909372965495

Epoch: 6| Step: 11
Training loss: 4.320160865783691
Validation loss: 4.61444886525472

Epoch: 6| Step: 12
Training loss: 5.021343231201172
Validation loss: 4.6075173219045

Epoch: 6| Step: 13
Training loss: 4.758228302001953
Validation loss: 4.601196686426799

Epoch: 9| Step: 0
Training loss: 3.2659506797790527
Validation loss: 4.5940321286519366

Epoch: 6| Step: 1
Training loss: 4.941596984863281
Validation loss: 4.587687253952026

Epoch: 6| Step: 2
Training loss: 6.2125444412231445
Validation loss: 4.581320762634277

Epoch: 6| Step: 3
Training loss: 5.6286845207214355
Validation loss: 4.574533144632976

Epoch: 6| Step: 4
Training loss: 3.8177330493927
Validation loss: 4.5682750542958575

Epoch: 6| Step: 5
Training loss: 4.965995788574219
Validation loss: 4.561841527620952

Epoch: 6| Step: 6
Training loss: 4.713892936706543
Validation loss: 4.555298805236816

Epoch: 6| Step: 7
Training loss: 3.8226261138916016
Validation loss: 4.549361705780029

Epoch: 6| Step: 8
Training loss: 4.903465747833252
Validation loss: 4.543511708577474

Epoch: 6| Step: 9
Training loss: 5.099736213684082
Validation loss: 4.53675111134847

Epoch: 6| Step: 10
Training loss: 4.123641014099121
Validation loss: 4.530482133229573

Epoch: 6| Step: 11
Training loss: 4.465693473815918
Validation loss: 4.524748682975769

Epoch: 6| Step: 12
Training loss: 4.638818740844727
Validation loss: 4.518234332402547

Epoch: 6| Step: 13
Training loss: 4.577846527099609
Validation loss: 4.511830806732178

Epoch: 10| Step: 0
Training loss: 5.546947002410889
Validation loss: 4.505692799886067

Epoch: 6| Step: 1
Training loss: 5.439083099365234
Validation loss: 4.499393661816915

Epoch: 6| Step: 2
Training loss: 3.368375062942505
Validation loss: 4.493168115615845

Epoch: 6| Step: 3
Training loss: 4.420881748199463
Validation loss: 4.487500826517741

Epoch: 6| Step: 4
Training loss: 5.5892510414123535
Validation loss: 4.481097022692363

Epoch: 6| Step: 5
Training loss: 4.885264873504639
Validation loss: 4.475252230962117

Epoch: 6| Step: 6
Training loss: 4.489599704742432
Validation loss: 4.469310283660889

Epoch: 6| Step: 7
Training loss: 4.532746315002441
Validation loss: 4.463202675183614

Epoch: 6| Step: 8
Training loss: 3.938606023788452
Validation loss: 4.45743449529012

Epoch: 6| Step: 9
Training loss: 4.539235591888428
Validation loss: 4.451569557189941

Epoch: 6| Step: 10
Training loss: 5.694077491760254
Validation loss: 4.445876757303874

Epoch: 6| Step: 11
Training loss: 3.966196060180664
Validation loss: 4.440005699793498

Epoch: 6| Step: 12
Training loss: 3.894272565841675
Validation loss: 4.434482296307881

Epoch: 6| Step: 13
Training loss: 3.7792303562164307
Validation loss: 4.428293585777283

Epoch: 11| Step: 0
Training loss: 5.00891637802124
Validation loss: 4.42261282602946

Epoch: 6| Step: 1
Training loss: 5.2122344970703125
Validation loss: 4.416685342788696

Epoch: 6| Step: 2
Training loss: 4.030423641204834
Validation loss: 4.410995562871297

Epoch: 6| Step: 3
Training loss: 4.501951217651367
Validation loss: 4.40541426340739

Epoch: 6| Step: 4
Training loss: 4.739612102508545
Validation loss: 4.399541934331258

Epoch: 6| Step: 5
Training loss: 4.687751293182373
Validation loss: 4.393297036488851

Epoch: 6| Step: 6
Training loss: 3.9394538402557373
Validation loss: 4.387118657430013

Epoch: 6| Step: 7
Training loss: 4.298714637756348
Validation loss: 4.380543192227681

Epoch: 6| Step: 8
Training loss: 5.059993267059326
Validation loss: 4.374731183052063

Epoch: 6| Step: 9
Training loss: 3.256863594055176
Validation loss: 4.368323405583699

Epoch: 6| Step: 10
Training loss: 4.866758346557617
Validation loss: 4.361601193745931

Epoch: 6| Step: 11
Training loss: 3.046583652496338
Validation loss: 4.355175773302714

Epoch: 6| Step: 12
Training loss: 4.917963981628418
Validation loss: 4.347974936167399

Epoch: 6| Step: 13
Training loss: 5.470282554626465
Validation loss: 4.340880632400513

Epoch: 12| Step: 0
Training loss: 5.4470744132995605
Validation loss: 4.333927392959595

Epoch: 6| Step: 1
Training loss: 4.455865859985352
Validation loss: 4.3267199993133545

Epoch: 6| Step: 2
Training loss: 4.388306617736816
Validation loss: 4.31957745552063

Epoch: 6| Step: 3
Training loss: 3.67230486869812
Validation loss: 4.311599373817444

Epoch: 6| Step: 4
Training loss: 4.445694923400879
Validation loss: 4.30397907892863

Epoch: 6| Step: 5
Training loss: 5.161824703216553
Validation loss: 4.296327392260234

Epoch: 6| Step: 6
Training loss: 5.623888969421387
Validation loss: 4.287852962811788

Epoch: 6| Step: 7
Training loss: 3.8800246715545654
Validation loss: 4.280741373697917

Epoch: 6| Step: 8
Training loss: 3.289885997772217
Validation loss: 4.273565212885539

Epoch: 6| Step: 9
Training loss: 4.246923446655273
Validation loss: 4.267242391904195

Epoch: 6| Step: 10
Training loss: 4.733628273010254
Validation loss: 4.259110649426778

Epoch: 6| Step: 11
Training loss: 5.376925468444824
Validation loss: 4.252717177073161

Epoch: 6| Step: 12
Training loss: 3.056434392929077
Validation loss: 4.246009707450867

Epoch: 6| Step: 13
Training loss: 3.9747636318206787
Validation loss: 4.23966658115387

Epoch: 13| Step: 0
Training loss: 3.2746152877807617
Validation loss: 4.233286619186401

Epoch: 6| Step: 1
Training loss: 3.939622402191162
Validation loss: 4.225858171780904

Epoch: 6| Step: 2
Training loss: 4.448967933654785
Validation loss: 4.222598950068156

Epoch: 6| Step: 3
Training loss: 4.206905364990234
Validation loss: 4.216476917266846

Epoch: 6| Step: 4
Training loss: 3.7825708389282227
Validation loss: 4.209072430928548

Epoch: 6| Step: 5
Training loss: 4.254790782928467
Validation loss: 4.204488118489583

Epoch: 6| Step: 6
Training loss: 5.158754348754883
Validation loss: 4.197514891624451

Epoch: 6| Step: 7
Training loss: 4.865542411804199
Validation loss: 4.19144852956136

Epoch: 6| Step: 8
Training loss: 4.889512538909912
Validation loss: 4.1857309738794966

Epoch: 6| Step: 9
Training loss: 5.074624061584473
Validation loss: 4.179805914560954

Epoch: 6| Step: 10
Training loss: 2.939297914505005
Validation loss: 4.17410929997762

Epoch: 6| Step: 11
Training loss: 3.798644542694092
Validation loss: 4.167651971181233

Epoch: 6| Step: 12
Training loss: 5.098587989807129
Validation loss: 4.161957065264384

Epoch: 6| Step: 13
Training loss: 4.842593193054199
Validation loss: 4.155960559844971

Epoch: 14| Step: 0
Training loss: 4.623495578765869
Validation loss: 4.150806744893392

Epoch: 6| Step: 1
Training loss: 4.811418533325195
Validation loss: 4.144530137379964

Epoch: 6| Step: 2
Training loss: 4.268902778625488
Validation loss: 4.138288617134094

Epoch: 6| Step: 3
Training loss: 4.197605133056641
Validation loss: 4.133364677429199

Epoch: 6| Step: 4
Training loss: 3.800107479095459
Validation loss: 4.1271302700042725

Epoch: 6| Step: 5
Training loss: 3.4255781173706055
Validation loss: 4.122931122779846

Epoch: 6| Step: 6
Training loss: 4.28760290145874
Validation loss: 4.117630044619243

Epoch: 6| Step: 7
Training loss: 5.020013809204102
Validation loss: 4.112257917722066

Epoch: 6| Step: 8
Training loss: 4.240743637084961
Validation loss: 4.106836915016174

Epoch: 6| Step: 9
Training loss: 4.414997100830078
Validation loss: 4.1011656522750854

Epoch: 6| Step: 10
Training loss: 4.819995403289795
Validation loss: 4.095438400904338

Epoch: 6| Step: 11
Training loss: 3.896442413330078
Validation loss: 4.089914878209432

Epoch: 6| Step: 12
Training loss: 3.800694704055786
Validation loss: 4.083491802215576

Epoch: 6| Step: 13
Training loss: 3.93505859375
Validation loss: 4.078874429066976

Epoch: 15| Step: 0
Training loss: 4.974618911743164
Validation loss: 4.073282082875569

Epoch: 6| Step: 1
Training loss: 3.391561508178711
Validation loss: 4.068586587905884

Epoch: 6| Step: 2
Training loss: 4.020627975463867
Validation loss: 4.064061880111694

Epoch: 6| Step: 3
Training loss: 4.552756309509277
Validation loss: 4.059364636739095

Epoch: 6| Step: 4
Training loss: 4.496500015258789
Validation loss: 4.053518255551656

Epoch: 6| Step: 5
Training loss: 4.217086315155029
Validation loss: 4.047863205273946

Epoch: 6| Step: 6
Training loss: 2.774174690246582
Validation loss: 4.0439010461171465

Epoch: 6| Step: 7
Training loss: 4.3644609451293945
Validation loss: 4.039530038833618

Epoch: 6| Step: 8
Training loss: 4.799696445465088
Validation loss: 4.034754594167073

Epoch: 6| Step: 9
Training loss: 4.689309120178223
Validation loss: 4.028922160466512

Epoch: 6| Step: 10
Training loss: 3.419739246368408
Validation loss: 4.023320873578389

Epoch: 6| Step: 11
Training loss: 4.23728084564209
Validation loss: 4.018496990203857

Epoch: 6| Step: 12
Training loss: 4.434909343719482
Validation loss: 4.013127406438191

Epoch: 6| Step: 13
Training loss: 4.174415588378906
Validation loss: 4.008565505345662

Epoch: 16| Step: 0
Training loss: 5.1214680671691895
Validation loss: 4.004744172096252

Epoch: 6| Step: 1
Training loss: 3.5828604698181152
Validation loss: 4.000283082326253

Epoch: 6| Step: 2
Training loss: 4.560626983642578
Validation loss: 3.994885881741842

Epoch: 6| Step: 3
Training loss: 3.5991334915161133
Validation loss: 3.9887267351150513

Epoch: 6| Step: 4
Training loss: 4.518369674682617
Validation loss: 3.98408838113149

Epoch: 6| Step: 5
Training loss: 3.8300552368164062
Validation loss: 3.9780433575312295

Epoch: 6| Step: 6
Training loss: 3.6890945434570312
Validation loss: 3.9742021560668945

Epoch: 6| Step: 7
Training loss: 4.373044013977051
Validation loss: 3.9687389930089316

Epoch: 6| Step: 8
Training loss: 4.045405864715576
Validation loss: 3.96431557337443

Epoch: 6| Step: 9
Training loss: 3.267714738845825
Validation loss: 3.959641377131144

Epoch: 6| Step: 10
Training loss: 4.332154273986816
Validation loss: 3.954570213953654

Epoch: 6| Step: 11
Training loss: 5.244024276733398
Validation loss: 3.950211445490519

Epoch: 6| Step: 12
Training loss: 3.074950933456421
Validation loss: 3.9459676345189414

Epoch: 6| Step: 13
Training loss: 4.410240650177002
Validation loss: 3.9406261444091797

Epoch: 17| Step: 0
Training loss: 2.9535911083221436
Validation loss: 3.9345011711120605

Epoch: 6| Step: 1
Training loss: 5.08892822265625
Validation loss: 3.930254062016805

Epoch: 6| Step: 2
Training loss: 4.430866241455078
Validation loss: 3.926271994908651

Epoch: 6| Step: 3
Training loss: 3.612532377243042
Validation loss: 3.9210819800694785

Epoch: 6| Step: 4
Training loss: 5.306731700897217
Validation loss: 3.9168052673339844

Epoch: 6| Step: 5
Training loss: 3.7712230682373047
Validation loss: 3.910426616668701

Epoch: 6| Step: 6
Training loss: 4.854579925537109
Validation loss: 3.9055681626001992

Epoch: 6| Step: 7
Training loss: 4.229325771331787
Validation loss: 3.901151100794474

Epoch: 6| Step: 8
Training loss: 3.514735698699951
Validation loss: 3.8968525727589927

Epoch: 6| Step: 9
Training loss: 3.620051860809326
Validation loss: 3.892353653907776

Epoch: 6| Step: 10
Training loss: 4.237573146820068
Validation loss: 3.8873844146728516

Epoch: 6| Step: 11
Training loss: 3.2309160232543945
Validation loss: 3.882476806640625

Epoch: 6| Step: 12
Training loss: 3.8117923736572266
Validation loss: 3.877575437227885

Epoch: 6| Step: 13
Training loss: 4.08020544052124
Validation loss: 3.872927745183309

Epoch: 18| Step: 0
Training loss: 3.7523488998413086
Validation loss: 3.8677006562550864

Epoch: 6| Step: 1
Training loss: 4.170499324798584
Validation loss: 3.8635348081588745

Epoch: 6| Step: 2
Training loss: 4.245960712432861
Validation loss: 3.857624650001526

Epoch: 6| Step: 3
Training loss: 4.140921592712402
Validation loss: 3.8526453574498496

Epoch: 6| Step: 4
Training loss: 5.157011985778809
Validation loss: 3.847241202990214

Epoch: 6| Step: 5
Training loss: 3.79193115234375
Validation loss: 3.842746138572693

Epoch: 6| Step: 6
Training loss: 3.62949275970459
Validation loss: 3.83868145942688

Epoch: 6| Step: 7
Training loss: 4.301440715789795
Validation loss: 3.8346010049184165

Epoch: 6| Step: 8
Training loss: 2.8105645179748535
Validation loss: 3.828112165133158

Epoch: 6| Step: 9
Training loss: 3.146622896194458
Validation loss: 3.8237053950627646

Epoch: 6| Step: 10
Training loss: 4.396373271942139
Validation loss: 3.8191588719685874

Epoch: 6| Step: 11
Training loss: 3.7699785232543945
Validation loss: 3.8144386609395347

Epoch: 6| Step: 12
Training loss: 4.376440525054932
Validation loss: 3.810417731602987

Epoch: 6| Step: 13
Training loss: 4.164750099182129
Validation loss: 3.8059276739756265

Epoch: 19| Step: 0
Training loss: 4.317924499511719
Validation loss: 3.801407734553019

Epoch: 6| Step: 1
Training loss: 4.057283401489258
Validation loss: 3.7976282437642417

Epoch: 6| Step: 2
Training loss: 3.7323877811431885
Validation loss: 3.79260261853536

Epoch: 6| Step: 3
Training loss: 4.034169673919678
Validation loss: 3.7896233399709067

Epoch: 6| Step: 4
Training loss: 4.532479286193848
Validation loss: 3.783870259920756

Epoch: 6| Step: 5
Training loss: 4.000941276550293
Validation loss: 3.778867721557617

Epoch: 6| Step: 6
Training loss: 3.9688215255737305
Validation loss: 3.7753872871398926

Epoch: 6| Step: 7
Training loss: 3.4671359062194824
Validation loss: 3.771021564801534

Epoch: 6| Step: 8
Training loss: 4.213309288024902
Validation loss: 3.7671482960383096

Epoch: 6| Step: 9
Training loss: 3.5049238204956055
Validation loss: 3.7626460790634155

Epoch: 6| Step: 10
Training loss: 4.406704425811768
Validation loss: 3.7576041221618652

Epoch: 6| Step: 11
Training loss: 3.755685567855835
Validation loss: 3.752907951672872

Epoch: 6| Step: 12
Training loss: 4.2870354652404785
Validation loss: 3.747904141743978

Epoch: 6| Step: 13
Training loss: 2.7533113956451416
Validation loss: 3.7434374491373696

Epoch: 20| Step: 0
Training loss: 3.5280845165252686
Validation loss: 3.740003784497579

Epoch: 6| Step: 1
Training loss: 3.971916437149048
Validation loss: 3.735921065012614

Epoch: 6| Step: 2
Training loss: 3.2308506965637207
Validation loss: 3.7303443352381387

Epoch: 6| Step: 3
Training loss: 4.198993682861328
Validation loss: 3.7254231373469033

Epoch: 6| Step: 4
Training loss: 4.151364326477051
Validation loss: 3.721493045488993

Epoch: 6| Step: 5
Training loss: 4.437939643859863
Validation loss: 3.717424909273783

Epoch: 6| Step: 6
Training loss: 3.376194953918457
Validation loss: 3.7130587895711265

Epoch: 6| Step: 7
Training loss: 4.314363956451416
Validation loss: 3.707568804423014

Epoch: 6| Step: 8
Training loss: 3.695939540863037
Validation loss: 3.7035332520802817

Epoch: 6| Step: 9
Training loss: 4.0353569984436035
Validation loss: 3.700523932774862

Epoch: 6| Step: 10
Training loss: 3.6566996574401855
Validation loss: 3.6951715548833213

Epoch: 6| Step: 11
Training loss: 3.222670555114746
Validation loss: 3.6912808418273926

Epoch: 6| Step: 12
Training loss: 4.8975419998168945
Validation loss: 3.686222791671753

Epoch: 6| Step: 13
Training loss: 3.4978859424591064
Validation loss: 3.6814560890197754

Epoch: 21| Step: 0
Training loss: 3.9636268615722656
Validation loss: 3.6775598526000977

Epoch: 6| Step: 1
Training loss: 2.729604482650757
Validation loss: 3.673220952351888

Epoch: 6| Step: 2
Training loss: 3.4731383323669434
Validation loss: 3.6692734956741333

Epoch: 6| Step: 3
Training loss: 3.870999574661255
Validation loss: 3.664594054222107

Epoch: 6| Step: 4
Training loss: 3.044221878051758
Validation loss: 3.659232219060262

Epoch: 6| Step: 5
Training loss: 3.7257375717163086
Validation loss: 3.655432383219401

Epoch: 6| Step: 6
Training loss: 4.361213684082031
Validation loss: 3.651394764582316

Epoch: 6| Step: 7
Training loss: 3.921780824661255
Validation loss: 3.645236849784851

Epoch: 6| Step: 8
Training loss: 4.080450534820557
Validation loss: 3.6409262816111245

Epoch: 6| Step: 9
Training loss: 2.499523162841797
Validation loss: 3.6361326773961387

Epoch: 6| Step: 10
Training loss: 4.825226783752441
Validation loss: 3.6317235231399536

Epoch: 6| Step: 11
Training loss: 4.299853324890137
Validation loss: 3.6261795361836753

Epoch: 6| Step: 12
Training loss: 4.529654026031494
Validation loss: 3.6219898064931235

Epoch: 6| Step: 13
Training loss: 4.000176429748535
Validation loss: 3.617219646771749

Epoch: 22| Step: 0
Training loss: 3.968356132507324
Validation loss: 3.6115574836730957

Epoch: 6| Step: 1
Training loss: 3.3684804439544678
Validation loss: 3.6074674924214682

Epoch: 6| Step: 2
Training loss: 3.9336814880371094
Validation loss: 3.6031049887339273

Epoch: 6| Step: 3
Training loss: 2.674861431121826
Validation loss: 3.5981681744257608

Epoch: 6| Step: 4
Training loss: 3.960704803466797
Validation loss: 3.5941535234451294

Epoch: 6| Step: 5
Training loss: 4.1683573722839355
Validation loss: 3.588433106740316

Epoch: 6| Step: 6
Training loss: 3.6362133026123047
Validation loss: 3.584792693456014

Epoch: 6| Step: 7
Training loss: 3.5709316730499268
Validation loss: 3.578648487726847

Epoch: 6| Step: 8
Training loss: 4.240813732147217
Validation loss: 3.5755660931269326

Epoch: 6| Step: 9
Training loss: 3.7908778190612793
Validation loss: 3.5701417128245034

Epoch: 6| Step: 10
Training loss: 3.4798953533172607
Validation loss: 3.5682695309321084

Epoch: 6| Step: 11
Training loss: 3.0957164764404297
Validation loss: 3.566651225090027

Epoch: 6| Step: 12
Training loss: 3.646427631378174
Validation loss: 3.557277957598368

Epoch: 6| Step: 13
Training loss: 4.911654949188232
Validation loss: 3.552550514539083

Epoch: 23| Step: 0
Training loss: 3.3203377723693848
Validation loss: 3.548549771308899

Epoch: 6| Step: 1
Training loss: 3.105742931365967
Validation loss: 3.5444029172261557

Epoch: 6| Step: 2
Training loss: 2.9899823665618896
Validation loss: 3.5381231705347695

Epoch: 6| Step: 3
Training loss: 3.5144131183624268
Validation loss: 3.5324425299962363

Epoch: 6| Step: 4
Training loss: 3.3826522827148438
Validation loss: 3.527327577273051

Epoch: 6| Step: 5
Training loss: 3.8524811267852783
Validation loss: 3.524137536684672

Epoch: 6| Step: 6
Training loss: 2.541855573654175
Validation loss: 3.5213454961776733

Epoch: 6| Step: 7
Training loss: 5.521908283233643
Validation loss: 3.5157474279403687

Epoch: 6| Step: 8
Training loss: 4.24739408493042
Validation loss: 3.5109352668126426

Epoch: 6| Step: 9
Training loss: 4.142910957336426
Validation loss: 3.5061747630437217

Epoch: 6| Step: 10
Training loss: 3.38346529006958
Validation loss: 3.50008487701416

Epoch: 6| Step: 11
Training loss: 4.451486110687256
Validation loss: 3.4977503220240274

Epoch: 6| Step: 12
Training loss: 2.8574938774108887
Validation loss: 3.4930429458618164

Epoch: 6| Step: 13
Training loss: 4.25148868560791
Validation loss: 3.4877744913101196

Epoch: 24| Step: 0
Training loss: 3.470832347869873
Validation loss: 3.4830845991770425

Epoch: 6| Step: 1
Training loss: 3.6586427688598633
Validation loss: 3.478995124499003

Epoch: 6| Step: 2
Training loss: 3.741189956665039
Validation loss: 3.474603811899821

Epoch: 6| Step: 3
Training loss: 2.711092472076416
Validation loss: 3.470587193965912

Epoch: 6| Step: 4
Training loss: 3.1256322860717773
Validation loss: 3.4654347896575928

Epoch: 6| Step: 5
Training loss: 3.610659122467041
Validation loss: 3.4607576529184976

Epoch: 6| Step: 6
Training loss: 2.6966304779052734
Validation loss: 3.4558445612589517

Epoch: 6| Step: 7
Training loss: 3.3170619010925293
Validation loss: 3.4514989852905273

Epoch: 6| Step: 8
Training loss: 3.748901844024658
Validation loss: 3.4468578497568765

Epoch: 6| Step: 9
Training loss: 3.8673133850097656
Validation loss: 3.443330446879069

Epoch: 6| Step: 10
Training loss: 3.3145174980163574
Validation loss: 3.43870210647583

Epoch: 6| Step: 11
Training loss: 4.855064392089844
Validation loss: 3.4348151286443076

Epoch: 6| Step: 12
Training loss: 4.339325904846191
Validation loss: 3.429872433344523

Epoch: 6| Step: 13
Training loss: 4.224188327789307
Validation loss: 3.425618052482605

Epoch: 25| Step: 0
Training loss: 3.779024362564087
Validation loss: 3.421620726585388

Epoch: 6| Step: 1
Training loss: 3.218212604522705
Validation loss: 3.4177594979604087

Epoch: 6| Step: 2
Training loss: 2.8136074542999268
Validation loss: 3.4133598804473877

Epoch: 6| Step: 3
Training loss: 3.8390443325042725
Validation loss: 3.4093465407689414

Epoch: 6| Step: 4
Training loss: 3.8356025218963623
Validation loss: 3.4046247005462646

Epoch: 6| Step: 5
Training loss: 3.199765682220459
Validation loss: 3.3996995290120444

Epoch: 6| Step: 6
Training loss: 3.306337356567383
Validation loss: 3.395948648452759

Epoch: 6| Step: 7
Training loss: 4.509510517120361
Validation loss: 3.391650398572286

Epoch: 6| Step: 8
Training loss: 3.6613001823425293
Validation loss: 3.387449343999227

Epoch: 6| Step: 9
Training loss: 3.479213237762451
Validation loss: 3.383819301923116

Epoch: 6| Step: 10
Training loss: 3.9429101943969727
Validation loss: 3.378357410430908

Epoch: 6| Step: 11
Training loss: 2.90610408782959
Validation loss: 3.374585429827372

Epoch: 6| Step: 12
Training loss: 3.4654171466827393
Validation loss: 3.3694765170415244

Epoch: 6| Step: 13
Training loss: 3.9108219146728516
Validation loss: 3.3655458291371665

Epoch: 26| Step: 0
Training loss: 3.090360164642334
Validation loss: 3.3604023456573486

Epoch: 6| Step: 1
Training loss: 4.319553375244141
Validation loss: 3.3569392363230386

Epoch: 6| Step: 2
Training loss: 3.6643621921539307
Validation loss: 3.3525247971216836

Epoch: 6| Step: 3
Training loss: 3.37715220451355
Validation loss: 3.348918914794922

Epoch: 6| Step: 4
Training loss: 3.969757556915283
Validation loss: 3.347211162249247

Epoch: 6| Step: 5
Training loss: 4.2124857902526855
Validation loss: 3.3393920262654624

Epoch: 6| Step: 6
Training loss: 3.619490146636963
Validation loss: 3.335511326789856

Epoch: 6| Step: 7
Training loss: 3.414707899093628
Validation loss: 3.3312953313191733

Epoch: 6| Step: 8
Training loss: 2.4956493377685547
Validation loss: 3.325959404309591

Epoch: 6| Step: 9
Training loss: 3.153506278991699
Validation loss: 3.322926640510559

Epoch: 6| Step: 10
Training loss: 2.9517760276794434
Validation loss: 3.3192025423049927

Epoch: 6| Step: 11
Training loss: 4.073338508605957
Validation loss: 3.3139834801355996

Epoch: 6| Step: 12
Training loss: 3.418612241744995
Validation loss: 3.3100165526072183

Epoch: 6| Step: 13
Training loss: 3.3359856605529785
Validation loss: 3.305858016014099

Epoch: 27| Step: 0
Training loss: 3.0136561393737793
Validation loss: 3.301430344581604

Epoch: 6| Step: 1
Training loss: 3.2847511768341064
Validation loss: 3.297820727030436

Epoch: 6| Step: 2
Training loss: 3.5202341079711914
Validation loss: 3.2928792238235474

Epoch: 6| Step: 3
Training loss: 3.499763011932373
Validation loss: 3.28850523630778

Epoch: 6| Step: 4
Training loss: 3.722442626953125
Validation loss: 3.284389019012451

Epoch: 6| Step: 5
Training loss: 3.0961217880249023
Validation loss: 3.280742883682251

Epoch: 6| Step: 6
Training loss: 3.4031951427459717
Validation loss: 3.276894768079122

Epoch: 6| Step: 7
Training loss: 3.1978204250335693
Validation loss: 3.272880514462789

Epoch: 6| Step: 8
Training loss: 2.9349207878112793
Validation loss: 3.268911123275757

Epoch: 6| Step: 9
Training loss: 4.353625774383545
Validation loss: 3.2659948269526162

Epoch: 6| Step: 10
Training loss: 3.9081170558929443
Validation loss: 3.2616037130355835

Epoch: 6| Step: 11
Training loss: 4.108716011047363
Validation loss: 3.2576512495676675

Epoch: 6| Step: 12
Training loss: 3.231790781021118
Validation loss: 3.2533932526906333

Epoch: 6| Step: 13
Training loss: 3.0424203872680664
Validation loss: 3.249224583307902

Epoch: 28| Step: 0
Training loss: 3.709524631500244
Validation loss: 3.2451302210489907

Epoch: 6| Step: 1
Training loss: 3.8550057411193848
Validation loss: 3.2410926818847656

Epoch: 6| Step: 2
Training loss: 4.1279683113098145
Validation loss: 3.236061414082845

Epoch: 6| Step: 3
Training loss: 3.5134825706481934
Validation loss: 3.2324370543162027

Epoch: 6| Step: 4
Training loss: 1.823441743850708
Validation loss: 3.228300094604492

Epoch: 6| Step: 5
Training loss: 3.3935999870300293
Validation loss: 3.2237569093704224

Epoch: 6| Step: 6
Training loss: 3.3477425575256348
Validation loss: 3.2197705109914145

Epoch: 6| Step: 7
Training loss: 4.395828723907471
Validation loss: 3.2161274353663125

Epoch: 6| Step: 8
Training loss: 2.82285475730896
Validation loss: 3.2111783027648926

Epoch: 6| Step: 9
Training loss: 3.8268799781799316
Validation loss: 3.207055648167928

Epoch: 6| Step: 10
Training loss: 3.217510461807251
Validation loss: 3.203812519709269

Epoch: 6| Step: 11
Training loss: 3.3798508644104004
Validation loss: 3.199477036794027

Epoch: 6| Step: 12
Training loss: 3.205855369567871
Validation loss: 3.194728453954061

Epoch: 6| Step: 13
Training loss: 2.949821710586548
Validation loss: 3.191152334213257

Epoch: 29| Step: 0
Training loss: 3.014940023422241
Validation loss: 3.186578909556071

Epoch: 6| Step: 1
Training loss: 3.477545738220215
Validation loss: 3.182697375615438

Epoch: 6| Step: 2
Training loss: 3.0510330200195312
Validation loss: 3.1779845555623374

Epoch: 6| Step: 3
Training loss: 3.365471124649048
Validation loss: 3.1737778186798096

Epoch: 6| Step: 4
Training loss: 2.24391770362854
Validation loss: 3.1740843852361045

Epoch: 6| Step: 5
Training loss: 3.291046619415283
Validation loss: 3.1669296423594155

Epoch: 6| Step: 6
Training loss: 3.083435535430908
Validation loss: 3.1610685189565024

Epoch: 6| Step: 7
Training loss: 3.585437297821045
Validation loss: 3.158902168273926

Epoch: 6| Step: 8
Training loss: 4.247996807098389
Validation loss: 3.154078722000122

Epoch: 6| Step: 9
Training loss: 3.5570807456970215
Validation loss: 3.1519179741541543

Epoch: 6| Step: 10
Training loss: 4.000608444213867
Validation loss: 3.1474209229151406

Epoch: 6| Step: 11
Training loss: 3.0059616565704346
Validation loss: 3.141884684562683

Epoch: 6| Step: 12
Training loss: 3.556999683380127
Validation loss: 3.137377977371216

Epoch: 6| Step: 13
Training loss: 3.371924877166748
Validation loss: 3.1339069604873657

Epoch: 30| Step: 0
Training loss: 3.4542274475097656
Validation loss: 3.1310383876164756

Epoch: 6| Step: 1
Training loss: 2.9315693378448486
Validation loss: 3.1281115214029946

Epoch: 6| Step: 2
Training loss: 3.621033191680908
Validation loss: 3.1233224868774414

Epoch: 6| Step: 3
Training loss: 2.571502208709717
Validation loss: 3.118627150853475

Epoch: 6| Step: 4
Training loss: 3.0408458709716797
Validation loss: 3.114545464515686

Epoch: 6| Step: 5
Training loss: 3.6644363403320312
Validation loss: 3.1096726258595786

Epoch: 6| Step: 6
Training loss: 3.240826368331909
Validation loss: 3.1059090892473855

Epoch: 6| Step: 7
Training loss: 3.398416519165039
Validation loss: 3.102249562740326

Epoch: 6| Step: 8
Training loss: 2.8576598167419434
Validation loss: 3.0984472831090293

Epoch: 6| Step: 9
Training loss: 3.605356454849243
Validation loss: 3.0948983828226724

Epoch: 6| Step: 10
Training loss: 3.0214695930480957
Validation loss: 3.0909226338068643

Epoch: 6| Step: 11
Training loss: 3.1181082725524902
Validation loss: 3.0879499912261963

Epoch: 6| Step: 12
Training loss: 3.8266220092773438
Validation loss: 3.0836058060328164

Epoch: 6| Step: 13
Training loss: 3.7562575340270996
Validation loss: 3.0796444416046143

Epoch: 31| Step: 0
Training loss: 2.9773895740509033
Validation loss: 3.076529026031494

Epoch: 6| Step: 1
Training loss: 2.1939589977264404
Validation loss: 3.0732436974843345

Epoch: 6| Step: 2
Training loss: 3.4789505004882812
Validation loss: 3.0704814990361533

Epoch: 6| Step: 3
Training loss: 3.1977691650390625
Validation loss: 3.0662436882654824

Epoch: 6| Step: 4
Training loss: 3.7111048698425293
Validation loss: 3.0619316895802817

Epoch: 6| Step: 5
Training loss: 3.865987777709961
Validation loss: 3.059048851331075

Epoch: 6| Step: 6
Training loss: 2.750337839126587
Validation loss: 3.054617404937744

Epoch: 6| Step: 7
Training loss: 2.835362672805786
Validation loss: 3.050946911176046

Epoch: 6| Step: 8
Training loss: 3.0414891242980957
Validation loss: 3.0470879872639975

Epoch: 6| Step: 9
Training loss: 3.0741543769836426
Validation loss: 3.043615380922953

Epoch: 6| Step: 10
Training loss: 3.1686453819274902
Validation loss: 3.04060693581899

Epoch: 6| Step: 11
Training loss: 4.016633033752441
Validation loss: 3.036746541659037

Epoch: 6| Step: 12
Training loss: 3.1764001846313477
Validation loss: 3.033271392186483

Epoch: 6| Step: 13
Training loss: 3.9113147258758545
Validation loss: 3.0300620794296265

Epoch: 32| Step: 0
Training loss: 3.15156626701355
Validation loss: 3.028375188509623

Epoch: 6| Step: 1
Training loss: 3.429298162460327
Validation loss: 3.024584253629049

Epoch: 6| Step: 2
Training loss: 2.3665390014648438
Validation loss: 3.0214741627375283

Epoch: 6| Step: 3
Training loss: 2.83966064453125
Validation loss: 3.016053875287374

Epoch: 6| Step: 4
Training loss: 3.2746682167053223
Validation loss: 3.010165492693583

Epoch: 6| Step: 5
Training loss: 3.3007125854492188
Validation loss: 3.007028659184774

Epoch: 6| Step: 6
Training loss: 3.883331060409546
Validation loss: 3.004616697629293

Epoch: 6| Step: 7
Training loss: 3.298062324523926
Validation loss: 3.001040061314901

Epoch: 6| Step: 8
Training loss: 3.1712536811828613
Validation loss: 2.997875769933065

Epoch: 6| Step: 9
Training loss: 3.271127462387085
Validation loss: 2.994687080383301

Epoch: 6| Step: 10
Training loss: 2.787810802459717
Validation loss: 2.9910939931869507

Epoch: 6| Step: 11
Training loss: 3.932955741882324
Validation loss: 2.987722118695577

Epoch: 6| Step: 12
Training loss: 2.906244993209839
Validation loss: 2.982532540957133

Epoch: 6| Step: 13
Training loss: 3.138187885284424
Validation loss: 2.978147506713867

Epoch: 33| Step: 0
Training loss: 3.248910665512085
Validation loss: 2.9751761754353843

Epoch: 6| Step: 1
Training loss: 3.4339327812194824
Validation loss: 2.9726902643839517

Epoch: 6| Step: 2
Training loss: 2.8602206707000732
Validation loss: 2.9690531889597573

Epoch: 6| Step: 3
Training loss: 2.223220109939575
Validation loss: 2.9655073086420694

Epoch: 6| Step: 4
Training loss: 2.939924955368042
Validation loss: 2.9626256028811135

Epoch: 6| Step: 5
Training loss: 3.3052163124084473
Validation loss: 2.959198276201884

Epoch: 6| Step: 6
Training loss: 3.536007881164551
Validation loss: 2.9547723531723022

Epoch: 6| Step: 7
Training loss: 3.306650161743164
Validation loss: 2.950885772705078

Epoch: 6| Step: 8
Training loss: 2.551488161087036
Validation loss: 2.947082201639811

Epoch: 6| Step: 9
Training loss: 3.4877524375915527
Validation loss: 2.9443320830663047

Epoch: 6| Step: 10
Training loss: 3.301799774169922
Validation loss: 2.940627694129944

Epoch: 6| Step: 11
Training loss: 2.5263259410858154
Validation loss: 2.937129338582357

Epoch: 6| Step: 12
Training loss: 3.4913413524627686
Validation loss: 2.9336161613464355

Epoch: 6| Step: 13
Training loss: 3.888622999191284
Validation loss: 2.9299811919530234

Epoch: 34| Step: 0
Training loss: 2.952810764312744
Validation loss: 2.9272173245747886

Epoch: 6| Step: 1
Training loss: 3.5130627155303955
Validation loss: 2.9249987602233887

Epoch: 6| Step: 2
Training loss: 2.815877676010132
Validation loss: 2.922402024269104

Epoch: 6| Step: 3
Training loss: 3.6458144187927246
Validation loss: 2.921370267868042

Epoch: 6| Step: 4
Training loss: 3.051003932952881
Validation loss: 2.918097734451294

Epoch: 6| Step: 5
Training loss: 3.079664945602417
Validation loss: 2.9143811464309692

Epoch: 6| Step: 6
Training loss: 3.5273239612579346
Validation loss: 2.9069380362828574

Epoch: 6| Step: 7
Training loss: 3.7365448474884033
Validation loss: 2.903040607770284

Epoch: 6| Step: 8
Training loss: 2.534273624420166
Validation loss: 2.900871157646179

Epoch: 6| Step: 9
Training loss: 2.7426838874816895
Validation loss: 2.8979239463806152

Epoch: 6| Step: 10
Training loss: 3.546881675720215
Validation loss: 2.9006897608439126

Epoch: 6| Step: 11
Training loss: 2.3945579528808594
Validation loss: 2.8962318499883017

Epoch: 6| Step: 12
Training loss: 2.9034132957458496
Validation loss: 2.895928462346395

Epoch: 6| Step: 13
Training loss: 3.102938652038574
Validation loss: 2.8906120459238687

Epoch: 35| Step: 0
Training loss: 3.016078472137451
Validation loss: 2.8841488361358643

Epoch: 6| Step: 1
Training loss: 3.028857707977295
Validation loss: 2.87881072362264

Epoch: 6| Step: 2
Training loss: 2.8950939178466797
Validation loss: 2.875066121419271

Epoch: 6| Step: 3
Training loss: 2.837343215942383
Validation loss: 2.871158242225647

Epoch: 6| Step: 4
Training loss: 3.7759170532226562
Validation loss: 2.8696828285853067

Epoch: 6| Step: 5
Training loss: 2.9882378578186035
Validation loss: 2.868948459625244

Epoch: 6| Step: 6
Training loss: 2.1913647651672363
Validation loss: 2.8659366369247437

Epoch: 6| Step: 7
Training loss: 2.952789783477783
Validation loss: 2.8647424777348838

Epoch: 6| Step: 8
Training loss: 3.636026382446289
Validation loss: 2.8687291542689004

Epoch: 6| Step: 9
Training loss: 3.216459035873413
Validation loss: 2.8620009819666543

Epoch: 6| Step: 10
Training loss: 2.813079595565796
Validation loss: 2.8502175410588584

Epoch: 6| Step: 11
Training loss: 2.831134796142578
Validation loss: 2.843786120414734

Epoch: 6| Step: 12
Training loss: 3.3866119384765625
Validation loss: 2.8379552364349365

Epoch: 6| Step: 13
Training loss: 3.3877878189086914
Validation loss: 2.8350049257278442

Epoch: 36| Step: 0
Training loss: 3.0053250789642334
Validation loss: 2.831482768058777

Epoch: 6| Step: 1
Training loss: 2.6799185276031494
Validation loss: 2.8290305932362876

Epoch: 6| Step: 2
Training loss: 2.986649751663208
Validation loss: 2.8270800908406577

Epoch: 6| Step: 3
Training loss: 3.472939968109131
Validation loss: 2.8229660193125405

Epoch: 6| Step: 4
Training loss: 2.6998143196105957
Validation loss: 2.8185068368911743

Epoch: 6| Step: 5
Training loss: 3.1070475578308105
Validation loss: 2.814766526222229

Epoch: 6| Step: 6
Training loss: 2.7410600185394287
Validation loss: 2.809303879737854

Epoch: 6| Step: 7
Training loss: 3.299043655395508
Validation loss: 2.806219975153605

Epoch: 6| Step: 8
Training loss: 3.084817409515381
Validation loss: 2.804990291595459

Epoch: 6| Step: 9
Training loss: 3.638949394226074
Validation loss: 2.8013933897018433

Epoch: 6| Step: 10
Training loss: 2.658346176147461
Validation loss: 2.7978365421295166

Epoch: 6| Step: 11
Training loss: 2.669682025909424
Validation loss: 2.7948495149612427

Epoch: 6| Step: 12
Training loss: 3.1700897216796875
Validation loss: 2.7963971694310508

Epoch: 6| Step: 13
Training loss: 3.110050678253174
Validation loss: 2.792600433031718

Epoch: 37| Step: 0
Training loss: 3.2757158279418945
Validation loss: 2.7859350442886353

Epoch: 6| Step: 1
Training loss: 3.3618404865264893
Validation loss: 2.7800766626993814

Epoch: 6| Step: 2
Training loss: 2.5965263843536377
Validation loss: 2.7767367362976074

Epoch: 6| Step: 3
Training loss: 2.810274362564087
Validation loss: 2.774485985438029

Epoch: 6| Step: 4
Training loss: 2.6655893325805664
Validation loss: 2.772551496823629

Epoch: 6| Step: 5
Training loss: 3.182978630065918
Validation loss: 2.7699395418167114

Epoch: 6| Step: 6
Training loss: 3.25667667388916
Validation loss: 2.769613742828369

Epoch: 6| Step: 7
Training loss: 3.462207078933716
Validation loss: 2.768133282661438

Epoch: 6| Step: 8
Training loss: 2.406460762023926
Validation loss: 2.761947731177012

Epoch: 6| Step: 9
Training loss: 3.3502511978149414
Validation loss: 2.7571980953216553

Epoch: 6| Step: 10
Training loss: 2.6628623008728027
Validation loss: 2.754755735397339

Epoch: 6| Step: 11
Training loss: 3.333702564239502
Validation loss: 2.7508743007977805

Epoch: 6| Step: 12
Training loss: 2.5820999145507812
Validation loss: 2.7477739254633584

Epoch: 6| Step: 13
Training loss: 2.7279701232910156
Validation loss: 2.7437323331832886

Epoch: 38| Step: 0
Training loss: 2.5056099891662598
Validation loss: 2.7417543729146323

Epoch: 6| Step: 1
Training loss: 2.310267686843872
Validation loss: 2.7396398385365806

Epoch: 6| Step: 2
Training loss: 3.2754769325256348
Validation loss: 2.736483097076416

Epoch: 6| Step: 3
Training loss: 2.662740707397461
Validation loss: 2.732686201731364

Epoch: 6| Step: 4
Training loss: 3.435189723968506
Validation loss: 2.7303991317749023

Epoch: 6| Step: 5
Training loss: 2.822754383087158
Validation loss: 2.72728697458903

Epoch: 6| Step: 6
Training loss: 3.4547312259674072
Validation loss: 2.723118225733439

Epoch: 6| Step: 7
Training loss: 3.2583537101745605
Validation loss: 2.721772789955139

Epoch: 6| Step: 8
Training loss: 2.9510130882263184
Validation loss: 2.717359185218811

Epoch: 6| Step: 9
Training loss: 2.8014636039733887
Validation loss: 2.714014172554016

Epoch: 6| Step: 10
Training loss: 3.1665706634521484
Validation loss: 2.7100677490234375

Epoch: 6| Step: 11
Training loss: 3.376192569732666
Validation loss: 2.70754603544871

Epoch: 6| Step: 12
Training loss: 2.568809986114502
Validation loss: 2.7047807772954306

Epoch: 6| Step: 13
Training loss: 2.46122407913208
Validation loss: 2.7005245685577393

Epoch: 39| Step: 0
Training loss: 3.0425755977630615
Validation loss: 2.6977157592773438

Epoch: 6| Step: 1
Training loss: 3.280167579650879
Validation loss: 2.6949449380238852

Epoch: 6| Step: 2
Training loss: 1.9405784606933594
Validation loss: 2.696197271347046

Epoch: 6| Step: 3
Training loss: 3.0602328777313232
Validation loss: 2.693219304084778

Epoch: 6| Step: 4
Training loss: 3.3949265480041504
Validation loss: 2.6981606086095176

Epoch: 6| Step: 5
Training loss: 2.7331061363220215
Validation loss: 2.690244476000468

Epoch: 6| Step: 6
Training loss: 3.048215389251709
Validation loss: 2.68160609404246

Epoch: 6| Step: 7
Training loss: 2.947293758392334
Validation loss: 2.677953561147054

Epoch: 6| Step: 8
Training loss: 2.511369228363037
Validation loss: 2.6760604778925576

Epoch: 6| Step: 9
Training loss: 2.864877700805664
Validation loss: 2.673019806543986

Epoch: 6| Step: 10
Training loss: 2.4941554069519043
Validation loss: 2.6739160219828286

Epoch: 6| Step: 11
Training loss: 3.2516942024230957
Validation loss: 2.674704909324646

Epoch: 6| Step: 12
Training loss: 2.7803401947021484
Validation loss: 2.6759844621022544

Epoch: 6| Step: 13
Training loss: 3.1103291511535645
Validation loss: 2.6743545532226562

Epoch: 40| Step: 0
Training loss: 3.2025580406188965
Validation loss: 2.6728550593058267

Epoch: 6| Step: 1
Training loss: 2.3124356269836426
Validation loss: 2.6706531047821045

Epoch: 6| Step: 2
Training loss: 3.0585317611694336
Validation loss: 2.6626118421554565

Epoch: 6| Step: 3
Training loss: 2.7721452713012695
Validation loss: 2.6564688682556152

Epoch: 6| Step: 4
Training loss: 3.040083408355713
Validation loss: 2.65049155553182

Epoch: 6| Step: 5
Training loss: 2.585174798965454
Validation loss: 2.6453956365585327

Epoch: 6| Step: 6
Training loss: 2.9278323650360107
Validation loss: 2.6406179666519165

Epoch: 6| Step: 7
Training loss: 3.1414995193481445
Validation loss: 2.6368343035380044

Epoch: 6| Step: 8
Training loss: 2.7323338985443115
Validation loss: 2.6330082416534424

Epoch: 6| Step: 9
Training loss: 2.7154529094696045
Validation loss: 2.6323212385177612

Epoch: 6| Step: 10
Training loss: 3.1459217071533203
Validation loss: 2.630945563316345

Epoch: 6| Step: 11
Training loss: 3.441523551940918
Validation loss: 2.629559357961019

Epoch: 6| Step: 12
Training loss: 2.4045891761779785
Validation loss: 2.626453240712484

Epoch: 6| Step: 13
Training loss: 2.304426908493042
Validation loss: 2.6212775309880576

Epoch: 41| Step: 0
Training loss: 2.7050490379333496
Validation loss: 2.6189254919687905

Epoch: 6| Step: 1
Training loss: 2.18788480758667
Validation loss: 2.6172572573026023

Epoch: 6| Step: 2
Training loss: 2.3270883560180664
Validation loss: 2.611744205156962

Epoch: 6| Step: 3
Training loss: 2.8557119369506836
Validation loss: 2.6034286618232727

Epoch: 6| Step: 4
Training loss: 2.9951462745666504
Validation loss: 2.6012767950693765

Epoch: 6| Step: 5
Training loss: 2.7193570137023926
Validation loss: 2.599843660990397

Epoch: 6| Step: 6
Training loss: 3.0984673500061035
Validation loss: 2.596491972605387

Epoch: 6| Step: 7
Training loss: 3.040891647338867
Validation loss: 2.5935236612955728

Epoch: 6| Step: 8
Training loss: 2.536609172821045
Validation loss: 2.5915913184483848

Epoch: 6| Step: 9
Training loss: 2.7547640800476074
Validation loss: 2.591606855392456

Epoch: 6| Step: 10
Training loss: 2.778359889984131
Validation loss: 2.5886161724726358

Epoch: 6| Step: 11
Training loss: 3.3391871452331543
Validation loss: 2.586182435353597

Epoch: 6| Step: 12
Training loss: 2.473757028579712
Validation loss: 2.5803303321202598

Epoch: 6| Step: 13
Training loss: 3.3976078033447266
Validation loss: 2.577641725540161

Epoch: 42| Step: 0
Training loss: 2.8884634971618652
Validation loss: 2.5737876097361245

Epoch: 6| Step: 1
Training loss: 2.099933385848999
Validation loss: 2.571826179822286

Epoch: 6| Step: 2
Training loss: 2.6780967712402344
Validation loss: 2.5753949085871377

Epoch: 6| Step: 3
Training loss: 2.6820712089538574
Validation loss: 2.576200087865194

Epoch: 6| Step: 4
Training loss: 2.6412501335144043
Validation loss: 2.56801438331604

Epoch: 6| Step: 5
Training loss: 3.0453784465789795
Validation loss: 2.5635244846343994

Epoch: 6| Step: 6
Training loss: 3.2683422565460205
Validation loss: 2.5573023557662964

Epoch: 6| Step: 7
Training loss: 3.0502209663391113
Validation loss: 2.552802840868632

Epoch: 6| Step: 8
Training loss: 2.3374977111816406
Validation loss: 2.5534350872039795

Epoch: 6| Step: 9
Training loss: 2.3263492584228516
Validation loss: 2.5495336453119912

Epoch: 6| Step: 10
Training loss: 2.981706142425537
Validation loss: 2.546302318572998

Epoch: 6| Step: 11
Training loss: 3.010955333709717
Validation loss: 2.548486828804016

Epoch: 6| Step: 12
Training loss: 3.0009005069732666
Validation loss: 2.5510942141215005

Epoch: 6| Step: 13
Training loss: 2.5883395671844482
Validation loss: 2.5465753078460693

Epoch: 43| Step: 0
Training loss: 2.3112666606903076
Validation loss: 2.5339729388554892

Epoch: 6| Step: 1
Training loss: 3.048008918762207
Validation loss: 2.5322200457255044

Epoch: 6| Step: 2
Training loss: 3.065380811691284
Validation loss: 2.5302318731943765

Epoch: 6| Step: 3
Training loss: 1.8895014524459839
Validation loss: 2.5256919860839844

Epoch: 6| Step: 4
Training loss: 2.633439064025879
Validation loss: 2.521786113580068

Epoch: 6| Step: 5
Training loss: 2.9789271354675293
Validation loss: 2.5233997901280723

Epoch: 6| Step: 6
Training loss: 2.491220712661743
Validation loss: 2.519141356150309

Epoch: 6| Step: 7
Training loss: 2.0535168647766113
Validation loss: 2.520027836163839

Epoch: 6| Step: 8
Training loss: 2.769272804260254
Validation loss: 2.5125197172164917

Epoch: 6| Step: 9
Training loss: 3.276947021484375
Validation loss: 2.509629209836324

Epoch: 6| Step: 10
Training loss: 2.5173356533050537
Validation loss: 2.5080469449361167

Epoch: 6| Step: 11
Training loss: 2.582932472229004
Validation loss: 2.504993120829264

Epoch: 6| Step: 12
Training loss: 3.1649177074432373
Validation loss: 2.50148606300354

Epoch: 6| Step: 13
Training loss: 3.1723389625549316
Validation loss: 2.499276081720988

Epoch: 44| Step: 0
Training loss: 2.9330883026123047
Validation loss: 2.496856172879537

Epoch: 6| Step: 1
Training loss: 1.9159563779830933
Validation loss: 2.49365888039271

Epoch: 6| Step: 2
Training loss: 3.098215103149414
Validation loss: 2.495652437210083

Epoch: 6| Step: 3
Training loss: 2.547379493713379
Validation loss: 2.490517735481262

Epoch: 6| Step: 4
Training loss: 2.4375407695770264
Validation loss: 2.4853932857513428

Epoch: 6| Step: 5
Training loss: 2.5280566215515137
Validation loss: 2.483216881752014

Epoch: 6| Step: 6
Training loss: 1.8620902299880981
Validation loss: 2.4795889854431152

Epoch: 6| Step: 7
Training loss: 3.540440797805786
Validation loss: 2.480270485083262

Epoch: 6| Step: 8
Training loss: 3.3260605335235596
Validation loss: 2.4778403441111245

Epoch: 6| Step: 9
Training loss: 2.5913803577423096
Validation loss: 2.4756567080815635

Epoch: 6| Step: 10
Training loss: 2.804567337036133
Validation loss: 2.473421573638916

Epoch: 6| Step: 11
Training loss: 2.3766040802001953
Validation loss: 2.4719760020573935

Epoch: 6| Step: 12
Training loss: 2.631193161010742
Validation loss: 2.4628889163335166

Epoch: 6| Step: 13
Training loss: 2.8315157890319824
Validation loss: 2.461199084917704

Epoch: 45| Step: 0
Training loss: 2.338456153869629
Validation loss: 2.4572639067967734

Epoch: 6| Step: 1
Training loss: 2.401232957839966
Validation loss: 2.455118934313456

Epoch: 6| Step: 2
Training loss: 2.3262879848480225
Validation loss: 2.4563777446746826

Epoch: 6| Step: 3
Training loss: 2.668172597885132
Validation loss: 2.4573585589726767

Epoch: 6| Step: 4
Training loss: 2.697404384613037
Validation loss: 2.4718666871388755

Epoch: 6| Step: 5
Training loss: 2.947230577468872
Validation loss: 2.470723827679952

Epoch: 6| Step: 6
Training loss: 2.88081431388855
Validation loss: 2.456479787826538

Epoch: 6| Step: 7
Training loss: 2.7451560497283936
Validation loss: 2.439698815345764

Epoch: 6| Step: 8
Training loss: 2.909367561340332
Validation loss: 2.434332231680552

Epoch: 6| Step: 9
Training loss: 2.6145312786102295
Validation loss: 2.4337976773579917

Epoch: 6| Step: 10
Training loss: 3.0128982067108154
Validation loss: 2.433657785256704

Epoch: 6| Step: 11
Training loss: 2.549020767211914
Validation loss: 2.43353001276652

Epoch: 6| Step: 12
Training loss: 2.114257335662842
Validation loss: 2.4346981843312583

Epoch: 6| Step: 13
Training loss: 2.6850810050964355
Validation loss: 2.4406190315882363

Epoch: 46| Step: 0
Training loss: 2.100773811340332
Validation loss: 2.440210203329722

Epoch: 6| Step: 1
Training loss: 2.9105641841888428
Validation loss: 2.4374044140179953

Epoch: 6| Step: 2
Training loss: 2.7189316749572754
Validation loss: 2.4398893117904663

Epoch: 6| Step: 3
Training loss: 2.2054734230041504
Validation loss: 2.436401923497518

Epoch: 6| Step: 4
Training loss: 2.60969877243042
Validation loss: 2.4236766497294107

Epoch: 6| Step: 5
Training loss: 1.8482463359832764
Validation loss: 2.412861625353495

Epoch: 6| Step: 6
Training loss: 2.7532496452331543
Validation loss: 2.4107919931411743

Epoch: 6| Step: 7
Training loss: 3.3491055965423584
Validation loss: 2.407183289527893

Epoch: 6| Step: 8
Training loss: 3.0870912075042725
Validation loss: 2.405387202898661

Epoch: 6| Step: 9
Training loss: 2.7676706314086914
Validation loss: 2.3993877172470093

Epoch: 6| Step: 10
Training loss: 2.8883399963378906
Validation loss: 2.394753893216451

Epoch: 6| Step: 11
Training loss: 2.714808940887451
Validation loss: 2.3921941121419272

Epoch: 6| Step: 12
Training loss: 2.26196026802063
Validation loss: 2.3882089058558145

Epoch: 6| Step: 13
Training loss: 2.0704104900360107
Validation loss: 2.386495808760325

Epoch: 47| Step: 0
Training loss: 2.056426525115967
Validation loss: 2.3834656278292337

Epoch: 6| Step: 1
Training loss: 2.4095072746276855
Validation loss: 2.3839221000671387

Epoch: 6| Step: 2
Training loss: 2.1606531143188477
Validation loss: 2.3845223983128867

Epoch: 6| Step: 3
Training loss: 3.106873035430908
Validation loss: 2.3736979961395264

Epoch: 6| Step: 4
Training loss: 3.4625892639160156
Validation loss: 2.375746210416158

Epoch: 6| Step: 5
Training loss: 2.253854990005493
Validation loss: 2.372819264729818

Epoch: 6| Step: 6
Training loss: 2.4756484031677246
Validation loss: 2.3717116912206015

Epoch: 6| Step: 7
Training loss: 2.173208236694336
Validation loss: 2.375824252764384

Epoch: 6| Step: 8
Training loss: 2.7417914867401123
Validation loss: 2.3798317511876426

Epoch: 6| Step: 9
Training loss: 2.5635290145874023
Validation loss: 2.3808059295018515

Epoch: 6| Step: 10
Training loss: 2.775275707244873
Validation loss: 2.3809467951456704

Epoch: 6| Step: 11
Training loss: 2.5316073894500732
Validation loss: 2.3773516019185386

Epoch: 6| Step: 12
Training loss: 2.584280490875244
Validation loss: 2.3663021326065063

Epoch: 6| Step: 13
Training loss: 2.6596357822418213
Validation loss: 2.3626327514648438

Epoch: 48| Step: 0
Training loss: 2.601383686065674
Validation loss: 2.3618895610173545

Epoch: 6| Step: 1
Training loss: 1.8701567649841309
Validation loss: 2.3575122356414795

Epoch: 6| Step: 2
Training loss: 1.60434091091156
Validation loss: 2.3501397172609964

Epoch: 6| Step: 3
Training loss: 2.5310378074645996
Validation loss: 2.3462931712468467

Epoch: 6| Step: 4
Training loss: 2.87882137298584
Validation loss: 2.344649155934652

Epoch: 6| Step: 5
Training loss: 2.25111722946167
Validation loss: 2.340260148048401

Epoch: 6| Step: 6
Training loss: 2.3598060607910156
Validation loss: 2.3407726883888245

Epoch: 6| Step: 7
Training loss: 2.396239995956421
Validation loss: 2.3404717445373535

Epoch: 6| Step: 8
Training loss: 3.0225830078125
Validation loss: 2.335217595100403

Epoch: 6| Step: 9
Training loss: 2.133683443069458
Validation loss: 2.3317094246546426

Epoch: 6| Step: 10
Training loss: 2.2499516010284424
Validation loss: 2.3324642976125083

Epoch: 6| Step: 11
Training loss: 2.906717300415039
Validation loss: 2.3376351992289224

Epoch: 6| Step: 12
Training loss: 2.8242616653442383
Validation loss: 2.3314079443613687

Epoch: 6| Step: 13
Training loss: 3.5803380012512207
Validation loss: 2.327463924884796

Epoch: 49| Step: 0
Training loss: 2.1764345169067383
Validation loss: 2.315901060899099

Epoch: 6| Step: 1
Training loss: 2.8854432106018066
Validation loss: 2.3159741957982383

Epoch: 6| Step: 2
Training loss: 2.687535047531128
Validation loss: 2.314291000366211

Epoch: 6| Step: 3
Training loss: 2.691307544708252
Validation loss: 2.3106298049290976

Epoch: 6| Step: 4
Training loss: 2.3820576667785645
Validation loss: 2.3097593784332275

Epoch: 6| Step: 5
Training loss: 2.9626269340515137
Validation loss: 2.310156603654226

Epoch: 6| Step: 6
Training loss: 2.112863063812256
Validation loss: 2.30623064438502

Epoch: 6| Step: 7
Training loss: 2.7902181148529053
Validation loss: 2.3043682177861533

Epoch: 6| Step: 8
Training loss: 2.0482048988342285
Validation loss: 2.3026205698649087

Epoch: 6| Step: 9
Training loss: 2.4930100440979004
Validation loss: 2.30000102519989

Epoch: 6| Step: 10
Training loss: 2.04995059967041
Validation loss: 2.2974254886309304

Epoch: 6| Step: 11
Training loss: 2.5495429039001465
Validation loss: 2.2957085569699607

Epoch: 6| Step: 12
Training loss: 2.61502742767334
Validation loss: 2.29717093706131

Epoch: 6| Step: 13
Training loss: 2.1857917308807373
Validation loss: 2.2881397008895874

Epoch: 50| Step: 0
Training loss: 2.46640944480896
Validation loss: 2.2867011626561484

Epoch: 6| Step: 1
Training loss: 1.875620722770691
Validation loss: 2.2861732840538025

Epoch: 6| Step: 2
Training loss: 3.4111533164978027
Validation loss: 2.286448041598002

Epoch: 6| Step: 3
Training loss: 2.306182384490967
Validation loss: 2.3075520396232605

Epoch: 6| Step: 4
Training loss: 2.9054200649261475
Validation loss: 2.2868218421936035

Epoch: 6| Step: 5
Training loss: 2.4474124908447266
Validation loss: 2.275665561358134

Epoch: 6| Step: 6
Training loss: 2.478419780731201
Validation loss: 2.2680244048436484

Epoch: 6| Step: 7
Training loss: 2.9576289653778076
Validation loss: 2.269337217013041

Epoch: 6| Step: 8
Training loss: 2.556469678878784
Validation loss: 2.276679813861847

Epoch: 6| Step: 9
Training loss: 1.3511085510253906
Validation loss: 2.2737110455830893

Epoch: 6| Step: 10
Training loss: 1.8975145816802979
Validation loss: 2.270612100760142

Epoch: 6| Step: 11
Training loss: 2.837850570678711
Validation loss: 2.2669756015141806

Epoch: 6| Step: 12
Training loss: 2.3486409187316895
Validation loss: 2.269239862759908

Epoch: 6| Step: 13
Training loss: 2.3974032402038574
Validation loss: 2.2625017563501992

Epoch: 51| Step: 0
Training loss: 2.195795774459839
Validation loss: 2.259941339492798

Epoch: 6| Step: 1
Training loss: 2.4853196144104004
Validation loss: 2.2616581519444785

Epoch: 6| Step: 2
Training loss: 2.4611315727233887
Validation loss: 2.2581763664881387

Epoch: 6| Step: 3
Training loss: 2.5436978340148926
Validation loss: 2.2562853495279946

Epoch: 6| Step: 4
Training loss: 2.324842691421509
Validation loss: 2.2496262788772583

Epoch: 6| Step: 5
Training loss: 1.9205691814422607
Validation loss: 2.247603098551432

Epoch: 6| Step: 6
Training loss: 2.640319347381592
Validation loss: 2.2425049344698587

Epoch: 6| Step: 7
Training loss: 2.8195221424102783
Validation loss: 2.234959920247396

Epoch: 6| Step: 8
Training loss: 2.3552112579345703
Validation loss: 2.239264448483785

Epoch: 6| Step: 9
Training loss: 2.5413565635681152
Validation loss: 2.2367822527885437

Epoch: 6| Step: 10
Training loss: 2.5519018173217773
Validation loss: 2.232516825199127

Epoch: 6| Step: 11
Training loss: 2.0440356731414795
Validation loss: 2.230134387811025

Epoch: 6| Step: 12
Training loss: 2.6164932250976562
Validation loss: 2.227253337701162

Epoch: 6| Step: 13
Training loss: 2.1749868392944336
Validation loss: 2.224255363146464

Epoch: 52| Step: 0
Training loss: 2.293935775756836
Validation loss: 2.226071357727051

Epoch: 6| Step: 1
Training loss: 1.9451594352722168
Validation loss: 2.2227049668629966

Epoch: 6| Step: 2
Training loss: 2.395474433898926
Validation loss: 2.220799664656321

Epoch: 6| Step: 3
Training loss: 2.7722561359405518
Validation loss: 2.219376266002655

Epoch: 6| Step: 4
Training loss: 2.0016510486602783
Validation loss: 2.218496322631836

Epoch: 6| Step: 5
Training loss: 3.1256818771362305
Validation loss: 2.220119376977285

Epoch: 6| Step: 6
Training loss: 2.6048359870910645
Validation loss: 2.219434142112732

Epoch: 6| Step: 7
Training loss: 1.6486215591430664
Validation loss: 2.2170560558636985

Epoch: 6| Step: 8
Training loss: 2.2828304767608643
Validation loss: 2.2105014522870383

Epoch: 6| Step: 9
Training loss: 2.316093683242798
Validation loss: 2.212458848953247

Epoch: 6| Step: 10
Training loss: 2.722346305847168
Validation loss: 2.2091662287712097

Epoch: 6| Step: 11
Training loss: 2.253854274749756
Validation loss: 2.2114035288492837

Epoch: 6| Step: 12
Training loss: 2.5398736000061035
Validation loss: 2.207369943459829

Epoch: 6| Step: 13
Training loss: 2.363344669342041
Validation loss: 2.2030993700027466

Epoch: 53| Step: 0
Training loss: 2.421236991882324
Validation loss: 2.200522323449453

Epoch: 6| Step: 1
Training loss: 2.773836612701416
Validation loss: 2.199518899122874

Epoch: 6| Step: 2
Training loss: 2.5820131301879883
Validation loss: 2.194802463054657

Epoch: 6| Step: 3
Training loss: 2.2601935863494873
Validation loss: 2.1969780921936035

Epoch: 6| Step: 4
Training loss: 2.3083863258361816
Validation loss: 2.194241166114807

Epoch: 6| Step: 5
Training loss: 1.8315585851669312
Validation loss: 2.1887405713399253

Epoch: 6| Step: 6
Training loss: 2.6483898162841797
Validation loss: 2.1937700112660727

Epoch: 6| Step: 7
Training loss: 2.5180506706237793
Validation loss: 2.189060608545939

Epoch: 6| Step: 8
Training loss: 1.9030039310455322
Validation loss: 2.1866909861564636

Epoch: 6| Step: 9
Training loss: 2.41487717628479
Validation loss: 2.1833979884783425

Epoch: 6| Step: 10
Training loss: 2.021974802017212
Validation loss: 2.17773046096166

Epoch: 6| Step: 11
Training loss: 2.7589197158813477
Validation loss: 2.1761135260264077

Epoch: 6| Step: 12
Training loss: 2.6016345024108887
Validation loss: 2.1724021434783936

Epoch: 6| Step: 13
Training loss: 1.772006630897522
Validation loss: 2.18183304866155

Epoch: 54| Step: 0
Training loss: 1.944234848022461
Validation loss: 2.180465559164683

Epoch: 6| Step: 1
Training loss: 1.942442536354065
Validation loss: 2.177131255467733

Epoch: 6| Step: 2
Training loss: 1.8577877283096313
Validation loss: 2.1634434858957925

Epoch: 6| Step: 3
Training loss: 2.3180313110351562
Validation loss: 2.171651244163513

Epoch: 6| Step: 4
Training loss: 2.7620718479156494
Validation loss: 2.1682286262512207

Epoch: 6| Step: 5
Training loss: 2.5189661979675293
Validation loss: 2.1686590115229287

Epoch: 6| Step: 6
Training loss: 2.523094654083252
Validation loss: 2.1739320755004883

Epoch: 6| Step: 7
Training loss: 2.544564723968506
Validation loss: 2.178037961324056

Epoch: 6| Step: 8
Training loss: 2.3838751316070557
Validation loss: 2.1751802166303

Epoch: 6| Step: 9
Training loss: 2.1500282287597656
Validation loss: 2.1763885815938315

Epoch: 6| Step: 10
Training loss: 2.4129855632781982
Validation loss: 2.173542300860087

Epoch: 6| Step: 11
Training loss: 2.765141010284424
Validation loss: 2.1706206997235618

Epoch: 6| Step: 12
Training loss: 2.1402907371520996
Validation loss: 2.1767361164093018

Epoch: 6| Step: 13
Training loss: 2.454164505004883
Validation loss: 2.1731792291005454

Epoch: 55| Step: 0
Training loss: 2.3913791179656982
Validation loss: 2.1579269568125405

Epoch: 6| Step: 1
Training loss: 2.4073288440704346
Validation loss: 2.1558948953946433

Epoch: 6| Step: 2
Training loss: 2.121272325515747
Validation loss: 2.147867520650228

Epoch: 6| Step: 3
Training loss: 2.1422181129455566
Validation loss: 2.1468788186709085

Epoch: 6| Step: 4
Training loss: 1.9886397123336792
Validation loss: 2.144955058892568

Epoch: 6| Step: 5
Training loss: 2.207821846008301
Validation loss: 2.14523975054423

Epoch: 6| Step: 6
Training loss: 2.272665500640869
Validation loss: 2.145884156227112

Epoch: 6| Step: 7
Training loss: 2.488168716430664
Validation loss: 2.145474672317505

Epoch: 6| Step: 8
Training loss: 2.4676015377044678
Validation loss: 2.1347288688023887

Epoch: 6| Step: 9
Training loss: 1.9853664636611938
Validation loss: 2.141165554523468

Epoch: 6| Step: 10
Training loss: 2.7999253273010254
Validation loss: 2.1377841234207153

Epoch: 6| Step: 11
Training loss: 1.6223580837249756
Validation loss: 2.136641204357147

Epoch: 6| Step: 12
Training loss: 2.812533378601074
Validation loss: 2.132568081219991

Epoch: 6| Step: 13
Training loss: 2.600893020629883
Validation loss: 2.1351563533147178

Epoch: 56| Step: 0
Training loss: 2.7480309009552
Validation loss: 2.1398275891939798

Epoch: 6| Step: 1
Training loss: 2.034968137741089
Validation loss: 2.134488344192505

Epoch: 6| Step: 2
Training loss: 2.6950907707214355
Validation loss: 2.1315096020698547

Epoch: 6| Step: 3
Training loss: 2.1376519203186035
Validation loss: 2.132339815298716

Epoch: 6| Step: 4
Training loss: 2.0827972888946533
Validation loss: 2.129428486029307

Epoch: 6| Step: 5
Training loss: 2.529757499694824
Validation loss: 2.12131530046463

Epoch: 6| Step: 6
Training loss: 2.5009288787841797
Validation loss: 2.128483772277832

Epoch: 6| Step: 7
Training loss: 2.2072701454162598
Validation loss: 2.125567396481832

Epoch: 6| Step: 8
Training loss: 2.1220688819885254
Validation loss: 2.1243600845336914

Epoch: 6| Step: 9
Training loss: 1.9231432676315308
Validation loss: 2.124588449796041

Epoch: 6| Step: 10
Training loss: 2.400690793991089
Validation loss: 2.116607407728831

Epoch: 6| Step: 11
Training loss: 2.799743175506592
Validation loss: 2.1145475109418235

Epoch: 6| Step: 12
Training loss: 2.1540720462799072
Validation loss: 2.1185211737950644

Epoch: 6| Step: 13
Training loss: 1.7033106088638306
Validation loss: 2.1277589400609336

Epoch: 57| Step: 0
Training loss: 2.9507267475128174
Validation loss: 2.1195302605628967

Epoch: 6| Step: 1
Training loss: 0.9198093414306641
Validation loss: 2.1247145533561707

Epoch: 6| Step: 2
Training loss: 2.3791394233703613
Validation loss: 2.114515940348307

Epoch: 6| Step: 3
Training loss: 2.6955103874206543
Validation loss: 2.1212193767229715

Epoch: 6| Step: 4
Training loss: 2.5020720958709717
Validation loss: 2.127025385697683

Epoch: 6| Step: 5
Training loss: 2.1600284576416016
Validation loss: 2.1289915243784585

Epoch: 6| Step: 6
Training loss: 2.8277862071990967
Validation loss: 2.124365250269572

Epoch: 6| Step: 7
Training loss: 1.7711515426635742
Validation loss: 2.113954941431681

Epoch: 6| Step: 8
Training loss: 2.1971631050109863
Validation loss: 2.114855488141378

Epoch: 6| Step: 9
Training loss: 2.802220344543457
Validation loss: 2.1092190742492676

Epoch: 6| Step: 10
Training loss: 2.139802932739258
Validation loss: 2.108197490374247

Epoch: 6| Step: 11
Training loss: 1.877229928970337
Validation loss: 2.1066872278849282

Epoch: 6| Step: 12
Training loss: 2.709395408630371
Validation loss: 2.1183753410975137

Epoch: 6| Step: 13
Training loss: 2.251263380050659
Validation loss: 2.127719501654307

Epoch: 58| Step: 0
Training loss: 2.385892868041992
Validation loss: 2.1188960472742715

Epoch: 6| Step: 1
Training loss: 2.223254680633545
Validation loss: 2.1228604515393577

Epoch: 6| Step: 2
Training loss: 1.7152057886123657
Validation loss: 2.113091548283895

Epoch: 6| Step: 3
Training loss: 2.2734436988830566
Validation loss: 2.1184706489245095

Epoch: 6| Step: 4
Training loss: 1.7714697122573853
Validation loss: 2.1162992119789124

Epoch: 6| Step: 5
Training loss: 2.3581786155700684
Validation loss: 2.106813073158264

Epoch: 6| Step: 6
Training loss: 2.298276424407959
Validation loss: 2.1053561170895896

Epoch: 6| Step: 7
Training loss: 2.4759483337402344
Validation loss: 2.102331737677256

Epoch: 6| Step: 8
Training loss: 2.3457324504852295
Validation loss: 2.1050162315368652

Epoch: 6| Step: 9
Training loss: 2.0108509063720703
Validation loss: 2.107553720474243

Epoch: 6| Step: 10
Training loss: 2.258878707885742
Validation loss: 2.1117785771687827

Epoch: 6| Step: 11
Training loss: 2.8204033374786377
Validation loss: 2.0977914333343506

Epoch: 6| Step: 12
Training loss: 2.441359519958496
Validation loss: 2.101616462071737

Epoch: 6| Step: 13
Training loss: 2.574338912963867
Validation loss: 2.1011502941449485

Epoch: 59| Step: 0
Training loss: 1.9228509664535522
Validation loss: 2.105781674385071

Epoch: 6| Step: 1
Training loss: 2.0985217094421387
Validation loss: 2.1061166326204934

Epoch: 6| Step: 2
Training loss: 2.5242300033569336
Validation loss: 2.107984483242035

Epoch: 6| Step: 3
Training loss: 2.608534336090088
Validation loss: 2.1007282733917236

Epoch: 6| Step: 4
Training loss: 2.146198272705078
Validation loss: 2.1017056703567505

Epoch: 6| Step: 5
Training loss: 3.1463494300842285
Validation loss: 2.1047178904215493

Epoch: 6| Step: 6
Training loss: 1.4967488050460815
Validation loss: 2.0991267760594687

Epoch: 6| Step: 7
Training loss: 2.1065542697906494
Validation loss: 2.0941394567489624

Epoch: 6| Step: 8
Training loss: 2.3984220027923584
Validation loss: 2.0939757227897644

Epoch: 6| Step: 9
Training loss: 1.9655535221099854
Validation loss: 2.0827526648839316

Epoch: 6| Step: 10
Training loss: 2.098764657974243
Validation loss: 2.0797013640403748

Epoch: 6| Step: 11
Training loss: 2.459825038909912
Validation loss: 2.0825427571932473

Epoch: 6| Step: 12
Training loss: 2.270630359649658
Validation loss: 2.081249475479126

Epoch: 6| Step: 13
Training loss: 2.526698589324951
Validation loss: 2.076746662457784

Epoch: 60| Step: 0
Training loss: 2.221604347229004
Validation loss: 2.07820196946462

Epoch: 6| Step: 1
Training loss: 2.117098331451416
Validation loss: 2.0827998518943787

Epoch: 6| Step: 2
Training loss: 2.290912389755249
Validation loss: 2.080091178417206

Epoch: 6| Step: 3
Training loss: 2.3260490894317627
Validation loss: 2.069994072119395

Epoch: 6| Step: 4
Training loss: 2.143249034881592
Validation loss: 2.073034187157949

Epoch: 6| Step: 5
Training loss: 2.4610698223114014
Validation loss: 2.0793742537498474

Epoch: 6| Step: 6
Training loss: 2.549734115600586
Validation loss: 2.0749302903811135

Epoch: 6| Step: 7
Training loss: 2.0914149284362793
Validation loss: 2.0769052704175315

Epoch: 6| Step: 8
Training loss: 1.8914891481399536
Validation loss: 2.0773407220840454

Epoch: 6| Step: 9
Training loss: 2.5471835136413574
Validation loss: 2.0737088720003762

Epoch: 6| Step: 10
Training loss: 1.8438842296600342
Validation loss: 2.078875263532003

Epoch: 6| Step: 11
Training loss: 2.6763949394226074
Validation loss: 2.079335312048594

Epoch: 6| Step: 12
Training loss: 2.1588072776794434
Validation loss: 2.0811034440994263

Epoch: 6| Step: 13
Training loss: 2.354949951171875
Validation loss: 2.079894264539083

Epoch: 61| Step: 0
Training loss: 2.447061538696289
Validation loss: 2.0731616218884787

Epoch: 6| Step: 1
Training loss: 2.134855270385742
Validation loss: 2.0781173507372537

Epoch: 6| Step: 2
Training loss: 1.645422101020813
Validation loss: 2.0712775190671286

Epoch: 6| Step: 3
Training loss: 2.341265916824341
Validation loss: 2.0735772450764975

Epoch: 6| Step: 4
Training loss: 2.4825053215026855
Validation loss: 2.0719893972078958

Epoch: 6| Step: 5
Training loss: 2.2745540142059326
Validation loss: 2.0560181736946106

Epoch: 6| Step: 6
Training loss: 1.5963424444198608
Validation loss: 2.072006126244863

Epoch: 6| Step: 7
Training loss: 2.227358818054199
Validation loss: 2.0933895111083984

Epoch: 6| Step: 8
Training loss: 2.7069571018218994
Validation loss: 2.0936608711878457

Epoch: 6| Step: 9
Training loss: 1.9516149759292603
Validation loss: 2.0793658097585044

Epoch: 6| Step: 10
Training loss: 2.499307632446289
Validation loss: 2.065967043240865

Epoch: 6| Step: 11
Training loss: 2.360136032104492
Validation loss: 2.057000517845154

Epoch: 6| Step: 12
Training loss: 2.935490131378174
Validation loss: 2.0555737813313804

Epoch: 6| Step: 13
Training loss: 1.8977971076965332
Validation loss: 2.063928564389547

Epoch: 62| Step: 0
Training loss: 2.3542251586914062
Validation loss: 2.068601429462433

Epoch: 6| Step: 1
Training loss: 2.410244941711426
Validation loss: 2.0712655782699585

Epoch: 6| Step: 2
Training loss: 1.9459781646728516
Validation loss: 2.0762105782826743

Epoch: 6| Step: 3
Training loss: 1.6583205461502075
Validation loss: 2.078704833984375

Epoch: 6| Step: 4
Training loss: 2.3131656646728516
Validation loss: 2.0792724092801413

Epoch: 6| Step: 5
Training loss: 2.8504514694213867
Validation loss: 2.081367313861847

Epoch: 6| Step: 6
Training loss: 2.1808724403381348
Validation loss: 2.078400492668152

Epoch: 6| Step: 7
Training loss: 1.770583987236023
Validation loss: 2.0773966113726297

Epoch: 6| Step: 8
Training loss: 2.469261646270752
Validation loss: 2.0622146924336753

Epoch: 6| Step: 9
Training loss: 2.120816707611084
Validation loss: 2.057077169418335

Epoch: 6| Step: 10
Training loss: 2.14560866355896
Validation loss: 2.0502211451530457

Epoch: 6| Step: 11
Training loss: 2.4090723991394043
Validation loss: 2.043098191420237

Epoch: 6| Step: 12
Training loss: 2.0633907318115234
Validation loss: 2.049594521522522

Epoch: 6| Step: 13
Training loss: 2.763474941253662
Validation loss: 2.0503281354904175

Epoch: 63| Step: 0
Training loss: 1.808333158493042
Validation loss: 2.0531426469484964

Epoch: 6| Step: 1
Training loss: 2.6326956748962402
Validation loss: 2.061953127384186

Epoch: 6| Step: 2
Training loss: 2.654329299926758
Validation loss: 2.061480621496836

Epoch: 6| Step: 3
Training loss: 2.8549180030822754
Validation loss: 2.045674979686737

Epoch: 6| Step: 4
Training loss: 2.33872127532959
Validation loss: 2.047032952308655

Epoch: 6| Step: 5
Training loss: 1.8830015659332275
Validation loss: 2.050951878229777

Epoch: 6| Step: 6
Training loss: 2.8055503368377686
Validation loss: 2.0507596929868064

Epoch: 6| Step: 7
Training loss: 2.6044774055480957
Validation loss: 2.057801902294159

Epoch: 6| Step: 8
Training loss: 2.0213241577148438
Validation loss: 2.057773530483246

Epoch: 6| Step: 9
Training loss: 2.307905435562134
Validation loss: 2.0593822797139487

Epoch: 6| Step: 10
Training loss: 1.9014003276824951
Validation loss: 2.053534229596456

Epoch: 6| Step: 11
Training loss: 2.484226703643799
Validation loss: 2.0557573437690735

Epoch: 6| Step: 12
Training loss: 1.5524948835372925
Validation loss: 2.0540619095166526

Epoch: 6| Step: 13
Training loss: 1.5396373271942139
Validation loss: 2.0554878314336142

Epoch: 64| Step: 0
Training loss: 2.729170560836792
Validation loss: 2.044652760028839

Epoch: 6| Step: 1
Training loss: 2.5094738006591797
Validation loss: 2.041435976823171

Epoch: 6| Step: 2
Training loss: 1.8075931072235107
Validation loss: 2.042077342669169

Epoch: 6| Step: 3
Training loss: 2.5142264366149902
Validation loss: 2.0458550254503884

Epoch: 6| Step: 4
Training loss: 2.324082374572754
Validation loss: 2.053568720817566

Epoch: 6| Step: 5
Training loss: 2.2886784076690674
Validation loss: 2.0636255145072937

Epoch: 6| Step: 6
Training loss: 2.3748703002929688
Validation loss: 2.088589072227478

Epoch: 6| Step: 7
Training loss: 2.113706350326538
Validation loss: 2.1042734185854592

Epoch: 6| Step: 8
Training loss: 2.836855173110962
Validation loss: 2.0916927655537925

Epoch: 6| Step: 9
Training loss: 1.5010452270507812
Validation loss: 2.0574235717455545

Epoch: 6| Step: 10
Training loss: 2.3217194080352783
Validation loss: 2.046231667200724

Epoch: 6| Step: 11
Training loss: 1.7221873998641968
Validation loss: 2.0355096062024436

Epoch: 6| Step: 12
Training loss: 2.3867125511169434
Validation loss: 2.037320593992869

Epoch: 6| Step: 13
Training loss: 2.3529906272888184
Validation loss: 2.043215791384379

Epoch: 65| Step: 0
Training loss: 1.867459774017334
Validation loss: 2.050253967444102

Epoch: 6| Step: 1
Training loss: 2.5333240032196045
Validation loss: 2.0591832200686135

Epoch: 6| Step: 2
Training loss: 2.421380043029785
Validation loss: 2.0725260575612388

Epoch: 6| Step: 3
Training loss: 1.7879891395568848
Validation loss: 2.080882469813029

Epoch: 6| Step: 4
Training loss: 2.3880696296691895
Validation loss: 2.085483729839325

Epoch: 6| Step: 5
Training loss: 1.888408899307251
Validation loss: 2.095307469367981

Epoch: 6| Step: 6
Training loss: 2.7521469593048096
Validation loss: 2.0966994365056357

Epoch: 6| Step: 7
Training loss: 1.976365327835083
Validation loss: 2.09368097782135

Epoch: 6| Step: 8
Training loss: 2.1248703002929688
Validation loss: 2.085246443748474

Epoch: 6| Step: 9
Training loss: 2.5082359313964844
Validation loss: 2.0813119808832803

Epoch: 6| Step: 10
Training loss: 3.017299175262451
Validation loss: 2.0702523787816367

Epoch: 6| Step: 11
Training loss: 1.8666284084320068
Validation loss: 2.069372852643331

Epoch: 6| Step: 12
Training loss: 2.3110742568969727
Validation loss: 2.061079521973928

Epoch: 6| Step: 13
Training loss: 2.241921901702881
Validation loss: 2.054814259211222

Epoch: 66| Step: 0
Training loss: 2.3269460201263428
Validation loss: 2.0504725774129233

Epoch: 6| Step: 1
Training loss: 1.97856867313385
Validation loss: 2.0431183576583862

Epoch: 6| Step: 2
Training loss: 2.0921223163604736
Validation loss: 2.037390093008677

Epoch: 6| Step: 3
Training loss: 2.53747820854187
Validation loss: 2.039578398068746

Epoch: 6| Step: 4
Training loss: 2.315232038497925
Validation loss: 2.0333065191904702

Epoch: 6| Step: 5
Training loss: 1.8312005996704102
Validation loss: 2.025235633055369

Epoch: 6| Step: 6
Training loss: 2.4032702445983887
Validation loss: 2.023004730542501

Epoch: 6| Step: 7
Training loss: 2.1133852005004883
Validation loss: 2.024981419245402

Epoch: 6| Step: 8
Training loss: 2.3504562377929688
Validation loss: 2.021468937397003

Epoch: 6| Step: 9
Training loss: 1.6782081127166748
Validation loss: 2.025590101877848

Epoch: 6| Step: 10
Training loss: 3.061448812484741
Validation loss: 2.029501259326935

Epoch: 6| Step: 11
Training loss: 1.8346216678619385
Validation loss: 2.0286741256713867

Epoch: 6| Step: 12
Training loss: 2.050785779953003
Validation loss: 2.023706058661143

Epoch: 6| Step: 13
Training loss: 2.4802448749542236
Validation loss: 2.024610241254171

Epoch: 67| Step: 0
Training loss: 2.4945337772369385
Validation loss: 2.0268738865852356

Epoch: 6| Step: 1
Training loss: 2.0338072776794434
Validation loss: 2.0270400842030845

Epoch: 6| Step: 2
Training loss: 2.040907382965088
Validation loss: 2.025752902030945

Epoch: 6| Step: 3
Training loss: 1.7440510988235474
Validation loss: 2.0349788467089334

Epoch: 6| Step: 4
Training loss: 2.8665659427642822
Validation loss: 2.0361753503481546

Epoch: 6| Step: 5
Training loss: 1.721372127532959
Validation loss: 2.031484325726827

Epoch: 6| Step: 6
Training loss: 2.7078144550323486
Validation loss: 2.0298457940419516

Epoch: 6| Step: 7
Training loss: 1.906233787536621
Validation loss: 2.034345289071401

Epoch: 6| Step: 8
Training loss: 2.488011360168457
Validation loss: 2.031830310821533

Epoch: 6| Step: 9
Training loss: 1.7913107872009277
Validation loss: 2.03144363562266

Epoch: 6| Step: 10
Training loss: 2.0900259017944336
Validation loss: 2.034860293070475

Epoch: 6| Step: 11
Training loss: 2.2200989723205566
Validation loss: 2.032472829023997

Epoch: 6| Step: 12
Training loss: 2.5391693115234375
Validation loss: 2.0396536191304526

Epoch: 6| Step: 13
Training loss: 2.3207972049713135
Validation loss: 2.0466097394625344

Epoch: 68| Step: 0
Training loss: 1.7056907415390015
Validation loss: 2.049272874991099

Epoch: 6| Step: 1
Training loss: 2.0242857933044434
Validation loss: 2.0417067209879556

Epoch: 6| Step: 2
Training loss: 1.7537962198257446
Validation loss: 2.043469409147898

Epoch: 6| Step: 3
Training loss: 2.841116428375244
Validation loss: 2.043488343556722

Epoch: 6| Step: 4
Training loss: 2.0256431102752686
Validation loss: 2.0436890522638955

Epoch: 6| Step: 5
Training loss: 2.5865962505340576
Validation loss: 2.041893462340037

Epoch: 6| Step: 6
Training loss: 2.4155755043029785
Validation loss: 2.0442360838254294

Epoch: 6| Step: 7
Training loss: 1.9830189943313599
Validation loss: 2.0385714968045554

Epoch: 6| Step: 8
Training loss: 2.8967533111572266
Validation loss: 2.038205643494924

Epoch: 6| Step: 9
Training loss: 2.561830759048462
Validation loss: 2.0373077392578125

Epoch: 6| Step: 10
Training loss: 1.960929036140442
Validation loss: 2.0368957916895547

Epoch: 6| Step: 11
Training loss: 2.2166566848754883
Validation loss: 2.030914008617401

Epoch: 6| Step: 12
Training loss: 2.3207225799560547
Validation loss: 2.0211417078971863

Epoch: 6| Step: 13
Training loss: 1.8201606273651123
Validation loss: 2.0277514656384787

Epoch: 69| Step: 0
Training loss: 2.191106081008911
Validation loss: 2.0276846090952554

Epoch: 6| Step: 1
Training loss: 1.637203574180603
Validation loss: 2.021100640296936

Epoch: 6| Step: 2
Training loss: 2.476822853088379
Validation loss: 2.0263918042182922

Epoch: 6| Step: 3
Training loss: 2.1669130325317383
Validation loss: 2.04084450006485

Epoch: 6| Step: 4
Training loss: 2.125546455383301
Validation loss: 2.0402637124061584

Epoch: 6| Step: 5
Training loss: 2.7882518768310547
Validation loss: 2.03981477022171

Epoch: 6| Step: 6
Training loss: 2.5168752670288086
Validation loss: 2.0293351809183755

Epoch: 6| Step: 7
Training loss: 1.843970537185669
Validation loss: 2.021068811416626

Epoch: 6| Step: 8
Training loss: 2.4774668216705322
Validation loss: 2.0193561712900796

Epoch: 6| Step: 9
Training loss: 2.075684070587158
Validation loss: 2.0132639408111572

Epoch: 6| Step: 10
Training loss: 2.2532050609588623
Validation loss: 2.0267077883084617

Epoch: 6| Step: 11
Training loss: 2.211547374725342
Validation loss: 2.0238011876742044

Epoch: 6| Step: 12
Training loss: 2.180830717086792
Validation loss: 2.0229759017626443

Epoch: 6| Step: 13
Training loss: 1.863346815109253
Validation loss: 2.02870637178421

Epoch: 70| Step: 0
Training loss: 1.874967336654663
Validation loss: 2.0287063916524253

Epoch: 6| Step: 1
Training loss: 2.493393898010254
Validation loss: 2.029823442300161

Epoch: 6| Step: 2
Training loss: 1.5842056274414062
Validation loss: 2.027821739514669

Epoch: 6| Step: 3
Training loss: 1.9527654647827148
Validation loss: 2.0364029804865518

Epoch: 6| Step: 4
Training loss: 1.9165451526641846
Validation loss: 2.033714930216471

Epoch: 6| Step: 5
Training loss: 3.2747583389282227
Validation loss: 2.0341764291127524

Epoch: 6| Step: 6
Training loss: 1.7540130615234375
Validation loss: 2.031261444091797

Epoch: 6| Step: 7
Training loss: 2.2569420337677
Validation loss: 2.03734960158666

Epoch: 6| Step: 8
Training loss: 2.350789785385132
Validation loss: 2.032475233078003

Epoch: 6| Step: 9
Training loss: 1.9215564727783203
Validation loss: 2.019802769025167

Epoch: 6| Step: 10
Training loss: 2.3842904567718506
Validation loss: 2.026795963446299

Epoch: 6| Step: 11
Training loss: 2.520155906677246
Validation loss: 2.024245282014211

Epoch: 6| Step: 12
Training loss: 1.625059962272644
Validation loss: 2.024736841519674

Epoch: 6| Step: 13
Training loss: 2.925234794616699
Validation loss: 2.0186642607053122

Epoch: 71| Step: 0
Training loss: 2.029520273208618
Validation loss: 2.0230101148287454

Epoch: 6| Step: 1
Training loss: 1.9393188953399658
Validation loss: 2.0295284390449524

Epoch: 6| Step: 2
Training loss: 1.7367000579833984
Validation loss: 2.0208080808321633

Epoch: 6| Step: 3
Training loss: 1.8172111511230469
Validation loss: 2.0305077036221824

Epoch: 6| Step: 4
Training loss: 1.869788646697998
Validation loss: 2.034922460714976

Epoch: 6| Step: 5
Training loss: 2.33743953704834
Validation loss: 2.0340033968289695

Epoch: 6| Step: 6
Training loss: 2.787964344024658
Validation loss: 2.046237667401632

Epoch: 6| Step: 7
Training loss: 2.272808074951172
Validation loss: 2.0455750226974487

Epoch: 6| Step: 8
Training loss: 2.6131348609924316
Validation loss: 2.0495959520339966

Epoch: 6| Step: 9
Training loss: 2.2278075218200684
Validation loss: 2.0413628617922464

Epoch: 6| Step: 10
Training loss: 2.4962992668151855
Validation loss: 2.049105445543925

Epoch: 6| Step: 11
Training loss: 2.079266309738159
Validation loss: 2.0389777421951294

Epoch: 6| Step: 12
Training loss: 2.638047456741333
Validation loss: 2.0341938138008118

Epoch: 6| Step: 13
Training loss: 1.8652122020721436
Validation loss: 2.0254394014676413

Epoch: 72| Step: 0
Training loss: 2.8853306770324707
Validation loss: 2.019502818584442

Epoch: 6| Step: 1
Training loss: 2.1468794345855713
Validation loss: 2.0191545287768045

Epoch: 6| Step: 2
Training loss: 1.6835945844650269
Validation loss: 2.0249980688095093

Epoch: 6| Step: 3
Training loss: 2.2478818893432617
Validation loss: 2.0228574872016907

Epoch: 6| Step: 4
Training loss: 1.9943363666534424
Validation loss: 2.025413155555725

Epoch: 6| Step: 5
Training loss: 2.712857723236084
Validation loss: 2.0350778698921204

Epoch: 6| Step: 6
Training loss: 2.14916729927063
Validation loss: 2.032274146874746

Epoch: 6| Step: 7
Training loss: 2.0282416343688965
Validation loss: 2.036456306775411

Epoch: 6| Step: 8
Training loss: 1.729711651802063
Validation loss: 2.0310720801353455

Epoch: 6| Step: 9
Training loss: 2.4537248611450195
Validation loss: 2.036158005396525

Epoch: 6| Step: 10
Training loss: 1.8931283950805664
Validation loss: 2.0348084568977356

Epoch: 6| Step: 11
Training loss: 2.665705680847168
Validation loss: 2.0354735453923545

Epoch: 6| Step: 12
Training loss: 2.0219855308532715
Validation loss: 2.037239670753479

Epoch: 6| Step: 13
Training loss: 2.2127771377563477
Validation loss: 2.0331914027531943

Epoch: 73| Step: 0
Training loss: 3.098541736602783
Validation loss: 2.0280043284098306

Epoch: 6| Step: 1
Training loss: 1.5124430656433105
Validation loss: 2.0190977255503335

Epoch: 6| Step: 2
Training loss: 2.315980911254883
Validation loss: 2.0176114241282144

Epoch: 6| Step: 3
Training loss: 1.9575105905532837
Validation loss: 2.027234057585398

Epoch: 6| Step: 4
Training loss: 2.3996987342834473
Validation loss: 2.048701286315918

Epoch: 6| Step: 5
Training loss: 2.197028636932373
Validation loss: 2.056432326634725

Epoch: 6| Step: 6
Training loss: 2.4592368602752686
Validation loss: 2.073658506075541

Epoch: 6| Step: 7
Training loss: 1.683931589126587
Validation loss: 2.0627472003300986

Epoch: 6| Step: 8
Training loss: 2.452824592590332
Validation loss: 2.049865206082662

Epoch: 6| Step: 9
Training loss: 1.8639824390411377
Validation loss: 2.0585959752400718

Epoch: 6| Step: 10
Training loss: 2.2185442447662354
Validation loss: 2.0516369541486106

Epoch: 6| Step: 11
Training loss: 2.4255056381225586
Validation loss: 2.037966310977936

Epoch: 6| Step: 12
Training loss: 2.440511703491211
Validation loss: 2.028840442498525

Epoch: 6| Step: 13
Training loss: 2.0984086990356445
Validation loss: 2.0299757520357766

Epoch: 74| Step: 0
Training loss: 3.1514902114868164
Validation loss: 2.022987723350525

Epoch: 6| Step: 1
Training loss: 2.538397789001465
Validation loss: 2.011995335419973

Epoch: 6| Step: 2
Training loss: 2.2175800800323486
Validation loss: 2.019522547721863

Epoch: 6| Step: 3
Training loss: 1.9150090217590332
Validation loss: 2.024531821409861

Epoch: 6| Step: 4
Training loss: 2.3843703269958496
Validation loss: 2.0302600065867105

Epoch: 6| Step: 5
Training loss: 2.062561511993408
Validation loss: 2.0281891425450644

Epoch: 6| Step: 6
Training loss: 2.0364484786987305
Validation loss: 2.033524493376414

Epoch: 6| Step: 7
Training loss: 2.9412033557891846
Validation loss: 2.036371330420176

Epoch: 6| Step: 8
Training loss: 2.157442092895508
Validation loss: 2.0395771662394204

Epoch: 6| Step: 9
Training loss: 1.9641196727752686
Validation loss: 2.0390674273173013

Epoch: 6| Step: 10
Training loss: 2.1293416023254395
Validation loss: 2.036480208237966

Epoch: 6| Step: 11
Training loss: 1.875673770904541
Validation loss: 2.033248007297516

Epoch: 6| Step: 12
Training loss: 1.7720792293548584
Validation loss: 2.022024234135946

Epoch: 6| Step: 13
Training loss: 1.792701244354248
Validation loss: 2.0254204074541726

Epoch: 75| Step: 0
Training loss: 1.7106624841690063
Validation loss: 2.023149391015371

Epoch: 6| Step: 1
Training loss: 2.4245665073394775
Validation loss: 2.0217143893241882

Epoch: 6| Step: 2
Training loss: 2.8231348991394043
Validation loss: 2.0157325069109597

Epoch: 6| Step: 3
Training loss: 1.5984718799591064
Validation loss: 2.0197736819585166

Epoch: 6| Step: 4
Training loss: 2.0513272285461426
Validation loss: 2.017117142677307

Epoch: 6| Step: 5
Training loss: 2.3173460960388184
Validation loss: 2.0299454728762307

Epoch: 6| Step: 6
Training loss: 2.2575650215148926
Validation loss: 2.0333614548047385

Epoch: 6| Step: 7
Training loss: 2.8067216873168945
Validation loss: 2.047143121560415

Epoch: 6| Step: 8
Training loss: 1.8627746105194092
Validation loss: 2.0591718355814614

Epoch: 6| Step: 9
Training loss: 2.0880937576293945
Validation loss: 2.061878283818563

Epoch: 6| Step: 10
Training loss: 2.2592992782592773
Validation loss: 2.076993981997172

Epoch: 6| Step: 11
Training loss: 2.024035930633545
Validation loss: 2.061085840066274

Epoch: 6| Step: 12
Training loss: 2.172844886779785
Validation loss: 2.0399945775667825

Epoch: 6| Step: 13
Training loss: 2.581380844116211
Validation loss: 2.0215712189674377

Epoch: 76| Step: 0
Training loss: 1.88685142993927
Validation loss: 2.012421806653341

Epoch: 6| Step: 1
Training loss: 2.284383773803711
Validation loss: 2.0229490598042807

Epoch: 6| Step: 2
Training loss: 1.7492222785949707
Validation loss: 2.0202638705571494

Epoch: 6| Step: 3
Training loss: 1.9701273441314697
Validation loss: 2.0330521066983542

Epoch: 6| Step: 4
Training loss: 2.610034942626953
Validation loss: 2.0383246143658957

Epoch: 6| Step: 5
Training loss: 2.293473958969116
Validation loss: 2.043763041496277

Epoch: 6| Step: 6
Training loss: 2.584845542907715
Validation loss: 2.0482288201649985

Epoch: 6| Step: 7
Training loss: 1.5683053731918335
Validation loss: 2.051372706890106

Epoch: 6| Step: 8
Training loss: 2.327667236328125
Validation loss: 2.0524679819742837

Epoch: 6| Step: 9
Training loss: 2.6389548778533936
Validation loss: 2.0493733485539756

Epoch: 6| Step: 10
Training loss: 2.958812713623047
Validation loss: 2.0480782985687256

Epoch: 6| Step: 11
Training loss: 2.5073466300964355
Validation loss: 2.045430580774943

Epoch: 6| Step: 12
Training loss: 1.76003098487854
Validation loss: 2.0434487660725913

Epoch: 6| Step: 13
Training loss: 1.6849124431610107
Validation loss: 2.043739358584086

Epoch: 77| Step: 0
Training loss: 2.8263235092163086
Validation loss: 2.034797211488088

Epoch: 6| Step: 1
Training loss: 2.1444272994995117
Validation loss: 2.035045882066091

Epoch: 6| Step: 2
Training loss: 2.126129627227783
Validation loss: 2.0334893465042114

Epoch: 6| Step: 3
Training loss: 2.3594772815704346
Validation loss: 2.0335631171862283

Epoch: 6| Step: 4
Training loss: 1.9701850414276123
Validation loss: 2.0211948355038962

Epoch: 6| Step: 5
Training loss: 2.1826515197753906
Validation loss: 2.0147517323493958

Epoch: 6| Step: 6
Training loss: 2.4300649166107178
Validation loss: 2.0122801661491394

Epoch: 6| Step: 7
Training loss: 1.7501802444458008
Validation loss: 2.0190224250157676

Epoch: 6| Step: 8
Training loss: 1.8192025423049927
Validation loss: 2.0198641419410706

Epoch: 6| Step: 9
Training loss: 2.072502613067627
Validation loss: 2.0230220754941306

Epoch: 6| Step: 10
Training loss: 2.3244566917419434
Validation loss: 2.0246789852778115

Epoch: 6| Step: 11
Training loss: 2.001525402069092
Validation loss: 2.035309314727783

Epoch: 6| Step: 12
Training loss: 2.018305778503418
Validation loss: 2.050317863623301

Epoch: 6| Step: 13
Training loss: 2.379983901977539
Validation loss: 2.059358457724253

Epoch: 78| Step: 0
Training loss: 2.4158124923706055
Validation loss: 2.0523948868115744

Epoch: 6| Step: 1
Training loss: 1.9824776649475098
Validation loss: 2.033425211906433

Epoch: 6| Step: 2
Training loss: 2.744356870651245
Validation loss: 2.031253377596537

Epoch: 6| Step: 3
Training loss: 1.7750599384307861
Validation loss: 2.0351428190867105

Epoch: 6| Step: 4
Training loss: 2.3031299114227295
Validation loss: 2.0398333072662354

Epoch: 6| Step: 5
Training loss: 2.092339277267456
Validation loss: 2.0269705255826316

Epoch: 6| Step: 6
Training loss: 1.7385952472686768
Validation loss: 2.0446035861968994

Epoch: 6| Step: 7
Training loss: 1.7062450647354126
Validation loss: 2.037046194076538

Epoch: 6| Step: 8
Training loss: 2.246666193008423
Validation loss: 2.0337859392166138

Epoch: 6| Step: 9
Training loss: 3.0866427421569824
Validation loss: 2.0185132026672363

Epoch: 6| Step: 10
Training loss: 2.0046427249908447
Validation loss: 2.0121794939041138

Epoch: 6| Step: 11
Training loss: 2.0506701469421387
Validation loss: 2.0074938535690308

Epoch: 6| Step: 12
Training loss: 2.354814291000366
Validation loss: 2.0312893788019815

Epoch: 6| Step: 13
Training loss: 2.1390864849090576
Validation loss: 2.037804146607717

Epoch: 79| Step: 0
Training loss: 2.1798291206359863
Validation loss: 2.047809362411499

Epoch: 6| Step: 1
Training loss: 2.597561836242676
Validation loss: 2.062388241291046

Epoch: 6| Step: 2
Training loss: 1.8290425539016724
Validation loss: 2.0613343715667725

Epoch: 6| Step: 3
Training loss: 2.8644320964813232
Validation loss: 2.0623045762379966

Epoch: 6| Step: 4
Training loss: 2.4854958057403564
Validation loss: 2.0669249494870505

Epoch: 6| Step: 5
Training loss: 2.432162284851074
Validation loss: 2.067882001399994

Epoch: 6| Step: 6
Training loss: 1.466009497642517
Validation loss: 2.0712778170903525

Epoch: 6| Step: 7
Training loss: 2.3493857383728027
Validation loss: 2.078296979268392

Epoch: 6| Step: 8
Training loss: 2.032499313354492
Validation loss: 2.069537897904714

Epoch: 6| Step: 9
Training loss: 2.415459156036377
Validation loss: 2.07381534576416

Epoch: 6| Step: 10
Training loss: 2.200284957885742
Validation loss: 2.067563851674398

Epoch: 6| Step: 11
Training loss: 2.3697474002838135
Validation loss: 2.066034456094106

Epoch: 6| Step: 12
Training loss: 1.987629771232605
Validation loss: 2.065713942050934

Epoch: 6| Step: 13
Training loss: 2.1803719997406006
Validation loss: 2.054720640182495

Epoch: 80| Step: 0
Training loss: 2.0908946990966797
Validation loss: 2.057478586832682

Epoch: 6| Step: 1
Training loss: 1.778192400932312
Validation loss: 2.054410477479299

Epoch: 6| Step: 2
Training loss: 2.110018253326416
Validation loss: 2.052644590536753

Epoch: 6| Step: 3
Training loss: 2.4192540645599365
Validation loss: 2.0441070596377053

Epoch: 6| Step: 4
Training loss: 2.1575629711151123
Validation loss: 2.0475441416104636

Epoch: 6| Step: 5
Training loss: 2.5613248348236084
Validation loss: 2.0423011581103006

Epoch: 6| Step: 6
Training loss: 1.9986509084701538
Validation loss: 2.0315303007761636

Epoch: 6| Step: 7
Training loss: 2.296046257019043
Validation loss: 2.026430308818817

Epoch: 6| Step: 8
Training loss: 2.2892587184906006
Validation loss: 2.016716162363688

Epoch: 6| Step: 9
Training loss: 2.0022456645965576
Validation loss: 2.0159629782040915

Epoch: 6| Step: 10
Training loss: 2.4687490463256836
Validation loss: 2.0111954609553018

Epoch: 6| Step: 11
Training loss: 2.274718999862671
Validation loss: 2.012815554936727

Epoch: 6| Step: 12
Training loss: 2.4811768531799316
Validation loss: 2.0125967860221863

Epoch: 6| Step: 13
Training loss: 1.840837836265564
Validation loss: 2.013090948263804

Epoch: 81| Step: 0
Training loss: 2.095229387283325
Validation loss: 2.026979148387909

Epoch: 6| Step: 1
Training loss: 1.9887568950653076
Validation loss: 2.0426560044288635

Epoch: 6| Step: 2
Training loss: 2.6521573066711426
Validation loss: 2.052293320496877

Epoch: 6| Step: 3
Training loss: 2.386922597885132
Validation loss: 2.0617533524831138

Epoch: 6| Step: 4
Training loss: 2.3610148429870605
Validation loss: 2.0676445364952087

Epoch: 6| Step: 5
Training loss: 2.0460774898529053
Validation loss: 2.075180252393087

Epoch: 6| Step: 6
Training loss: 2.639070749282837
Validation loss: 2.0535614689191184

Epoch: 6| Step: 7
Training loss: 2.219052791595459
Validation loss: 2.05195152759552

Epoch: 6| Step: 8
Training loss: 1.8851852416992188
Validation loss: 2.0409589211146035

Epoch: 6| Step: 9
Training loss: 2.3033463954925537
Validation loss: 2.0348363320032754

Epoch: 6| Step: 10
Training loss: 2.042034864425659
Validation loss: 2.0390754342079163

Epoch: 6| Step: 11
Training loss: 2.211252212524414
Validation loss: 2.0244024197260537

Epoch: 6| Step: 12
Training loss: 2.0728936195373535
Validation loss: 2.029178778330485

Epoch: 6| Step: 13
Training loss: 1.883475422859192
Validation loss: 2.017320930957794

Epoch: 82| Step: 0
Training loss: 2.4168176651000977
Validation loss: 2.017088154951731

Epoch: 6| Step: 1
Training loss: 2.044039249420166
Validation loss: 2.0151639382044473

Epoch: 6| Step: 2
Training loss: 1.8744713068008423
Validation loss: 2.0179709792137146

Epoch: 6| Step: 3
Training loss: 2.2066164016723633
Validation loss: 2.0110387603441873

Epoch: 6| Step: 4
Training loss: 2.1509246826171875
Validation loss: 2.0141352812449136

Epoch: 6| Step: 5
Training loss: 2.1860995292663574
Validation loss: 2.008494257926941

Epoch: 6| Step: 6
Training loss: 3.2051737308502197
Validation loss: 2.0109259287516275

Epoch: 6| Step: 7
Training loss: 2.3328847885131836
Validation loss: 2.0165974299112954

Epoch: 6| Step: 8
Training loss: 1.8884896039962769
Validation loss: 2.022278348604838

Epoch: 6| Step: 9
Training loss: 1.8365378379821777
Validation loss: 2.0202595591545105

Epoch: 6| Step: 10
Training loss: 1.9195992946624756
Validation loss: 2.0215079188346863

Epoch: 6| Step: 11
Training loss: 2.073354721069336
Validation loss: 2.0267698963483176

Epoch: 6| Step: 12
Training loss: 2.121058225631714
Validation loss: 2.0285156766573587

Epoch: 6| Step: 13
Training loss: 2.167725086212158
Validation loss: 2.0241336623827615

Epoch: 83| Step: 0
Training loss: 1.5981413125991821
Validation loss: 2.0188399155934653

Epoch: 6| Step: 1
Training loss: 2.793700695037842
Validation loss: 2.0150214036305747

Epoch: 6| Step: 2
Training loss: 1.7452034950256348
Validation loss: 2.017500420411428

Epoch: 6| Step: 3
Training loss: 1.8966227769851685
Validation loss: 2.0099941889444985

Epoch: 6| Step: 4
Training loss: 2.1877379417419434
Validation loss: 2.0268510580062866

Epoch: 6| Step: 5
Training loss: 1.8967633247375488
Validation loss: 2.0280704696973166

Epoch: 6| Step: 6
Training loss: 2.419231653213501
Validation loss: 2.0336238145828247

Epoch: 6| Step: 7
Training loss: 2.449936866760254
Validation loss: 2.046119471391042

Epoch: 6| Step: 8
Training loss: 2.348511219024658
Validation loss: 2.0344616969426474

Epoch: 6| Step: 9
Training loss: 2.0489301681518555
Validation loss: 2.0196956197420755

Epoch: 6| Step: 10
Training loss: 2.870929002761841
Validation loss: 2.017002006371816

Epoch: 6| Step: 11
Training loss: 2.557762622833252
Validation loss: 2.0077335834503174

Epoch: 6| Step: 12
Training loss: 1.9893720149993896
Validation loss: 2.0111234982808432

Epoch: 6| Step: 13
Training loss: 1.9934861660003662
Validation loss: 2.016043265660604

Epoch: 84| Step: 0
Training loss: 2.4499495029449463
Validation loss: 2.034219443798065

Epoch: 6| Step: 1
Training loss: 2.6614019870758057
Validation loss: 2.0325751503308616

Epoch: 6| Step: 2
Training loss: 2.0904436111450195
Validation loss: 2.0438785354296365

Epoch: 6| Step: 3
Training loss: 1.962267279624939
Validation loss: 2.03754731019338

Epoch: 6| Step: 4
Training loss: 2.25577974319458
Validation loss: 2.038740853468577

Epoch: 6| Step: 5
Training loss: 1.6098788976669312
Validation loss: 2.0376948515574136

Epoch: 6| Step: 6
Training loss: 2.4636967182159424
Validation loss: 2.0311617453893027

Epoch: 6| Step: 7
Training loss: 2.31660532951355
Validation loss: 2.017788271109263

Epoch: 6| Step: 8
Training loss: 2.4878244400024414
Validation loss: 2.026026467482249

Epoch: 6| Step: 9
Training loss: 2.224824905395508
Validation loss: 2.019155979156494

Epoch: 6| Step: 10
Training loss: 2.1171927452087402
Validation loss: 2.0191824634869895

Epoch: 6| Step: 11
Training loss: 2.0792126655578613
Validation loss: 2.022614379723867

Epoch: 6| Step: 12
Training loss: 1.8082995414733887
Validation loss: 2.020098249117533

Epoch: 6| Step: 13
Training loss: 1.9892395734786987
Validation loss: 2.0125648975372314

Epoch: 85| Step: 0
Training loss: 2.29300594329834
Validation loss: 2.01225874821345

Epoch: 6| Step: 1
Training loss: 1.8738852739334106
Validation loss: 2.0166038274765015

Epoch: 6| Step: 2
Training loss: 1.9819648265838623
Validation loss: 2.0168961882591248

Epoch: 6| Step: 3
Training loss: 2.397228240966797
Validation loss: 2.0280379255612693

Epoch: 6| Step: 4
Training loss: 2.1808269023895264
Validation loss: 2.0142253835995994

Epoch: 6| Step: 5
Training loss: 2.025825023651123
Validation loss: 2.014926850795746

Epoch: 6| Step: 6
Training loss: 2.141317129135132
Validation loss: 2.015792806943258

Epoch: 6| Step: 7
Training loss: 1.9083470106124878
Validation loss: 2.0120001633961997

Epoch: 6| Step: 8
Training loss: 2.3122737407684326
Validation loss: 2.0164296428362527

Epoch: 6| Step: 9
Training loss: 2.4014408588409424
Validation loss: 2.013676961263021

Epoch: 6| Step: 10
Training loss: 2.09407114982605
Validation loss: 2.0101006825764975

Epoch: 6| Step: 11
Training loss: 2.8421342372894287
Validation loss: 2.022236704826355

Epoch: 6| Step: 12
Training loss: 1.7608296871185303
Validation loss: 2.0287252267201743

Epoch: 6| Step: 13
Training loss: 2.037125587463379
Validation loss: 2.0166043043136597

Epoch: 86| Step: 0
Training loss: 1.5475014448165894
Validation loss: 2.016128202279409

Epoch: 6| Step: 1
Training loss: 2.662511110305786
Validation loss: 2.027852733929952

Epoch: 6| Step: 2
Training loss: 1.6713919639587402
Validation loss: 2.022657851378123

Epoch: 6| Step: 3
Training loss: 2.3402962684631348
Validation loss: 2.0240273078282676

Epoch: 6| Step: 4
Training loss: 2.445742130279541
Validation loss: 2.0239232579867044

Epoch: 6| Step: 5
Training loss: 1.9098448753356934
Validation loss: 2.0278329054514566

Epoch: 6| Step: 6
Training loss: 2.5081841945648193
Validation loss: 2.0224053064982095

Epoch: 6| Step: 7
Training loss: 2.397631883621216
Validation loss: 2.042194664478302

Epoch: 6| Step: 8
Training loss: 1.8824105262756348
Validation loss: 2.0430593490600586

Epoch: 6| Step: 9
Training loss: 2.1946041584014893
Validation loss: 2.03748627503713

Epoch: 6| Step: 10
Training loss: 1.6084861755371094
Validation loss: 2.037521799405416

Epoch: 6| Step: 11
Training loss: 3.141667366027832
Validation loss: 2.0468713442484536

Epoch: 6| Step: 12
Training loss: 2.072762966156006
Validation loss: 2.056505560874939

Epoch: 6| Step: 13
Training loss: 1.746557593345642
Validation loss: 2.033920109272003

Epoch: 87| Step: 0
Training loss: 1.8950402736663818
Validation loss: 2.0336660941441855

Epoch: 6| Step: 1
Training loss: 2.428201198577881
Validation loss: 2.0393795371055603

Epoch: 6| Step: 2
Training loss: 2.1460795402526855
Validation loss: 2.0363085865974426

Epoch: 6| Step: 3
Training loss: 1.6910655498504639
Validation loss: 2.0200456182161965

Epoch: 6| Step: 4
Training loss: 1.0547044277191162
Validation loss: 2.019522468249003

Epoch: 6| Step: 5
Training loss: 2.1356358528137207
Validation loss: 2.025739232699076

Epoch: 6| Step: 6
Training loss: 1.9420305490493774
Validation loss: 2.023131310939789

Epoch: 6| Step: 7
Training loss: 2.3801984786987305
Validation loss: 2.023136476675669

Epoch: 6| Step: 8
Training loss: 2.6392149925231934
Validation loss: 2.0177283883094788

Epoch: 6| Step: 9
Training loss: 2.195096969604492
Validation loss: 2.020220637321472

Epoch: 6| Step: 10
Training loss: 1.7404842376708984
Validation loss: 2.02472323179245

Epoch: 6| Step: 11
Training loss: 2.4268455505371094
Validation loss: 2.0096524953842163

Epoch: 6| Step: 12
Training loss: 2.6123626232147217
Validation loss: 2.0197513898213706

Epoch: 6| Step: 13
Training loss: 2.801624298095703
Validation loss: 2.034688174724579

Epoch: 88| Step: 0
Training loss: 2.391627311706543
Validation loss: 2.025688906510671

Epoch: 6| Step: 1
Training loss: 2.327080011367798
Validation loss: 2.0386467973391214

Epoch: 6| Step: 2
Training loss: 2.168541193008423
Validation loss: 2.0459794799486795

Epoch: 6| Step: 3
Training loss: 2.095449447631836
Validation loss: 2.062514861424764

Epoch: 6| Step: 4
Training loss: 2.127868890762329
Validation loss: 2.082030196984609

Epoch: 6| Step: 5
Training loss: 2.065735340118408
Validation loss: 2.099013070265452

Epoch: 6| Step: 6
Training loss: 2.5961153507232666
Validation loss: 2.1001944144566855

Epoch: 6| Step: 7
Training loss: 1.5034259557724
Validation loss: 2.075831393400828

Epoch: 6| Step: 8
Training loss: 2.113086223602295
Validation loss: 2.0477188428243003

Epoch: 6| Step: 9
Training loss: 1.9249355792999268
Validation loss: 2.03045121828715

Epoch: 6| Step: 10
Training loss: 2.089526653289795
Validation loss: 2.01256795724233

Epoch: 6| Step: 11
Training loss: 2.8704214096069336
Validation loss: 1.999443272749583

Epoch: 6| Step: 12
Training loss: 1.960150957107544
Validation loss: 2.0042113661766052

Epoch: 6| Step: 13
Training loss: 2.3982272148132324
Validation loss: 1.9977372288703918

Epoch: 89| Step: 0
Training loss: 2.0219178199768066
Validation loss: 2.008041520913442

Epoch: 6| Step: 1
Training loss: 2.2918505668640137
Validation loss: 2.017701824506124

Epoch: 6| Step: 2
Training loss: 1.9953978061676025
Validation loss: 2.023127496242523

Epoch: 6| Step: 3
Training loss: 1.9390931129455566
Validation loss: 2.025586406389872

Epoch: 6| Step: 4
Training loss: 2.723891258239746
Validation loss: 2.0332210063934326

Epoch: 6| Step: 5
Training loss: 2.181527614593506
Validation loss: 2.03074719508489

Epoch: 6| Step: 6
Training loss: 2.588589668273926
Validation loss: 2.0366319020589194

Epoch: 6| Step: 7
Training loss: 2.4795987606048584
Validation loss: 2.0334416230519614

Epoch: 6| Step: 8
Training loss: 1.9825644493103027
Validation loss: 2.0351699590682983

Epoch: 6| Step: 9
Training loss: 1.816233515739441
Validation loss: 2.025281916062037

Epoch: 6| Step: 10
Training loss: 2.789760112762451
Validation loss: 2.022389511267344

Epoch: 6| Step: 11
Training loss: 2.4982402324676514
Validation loss: 2.010961135228475

Epoch: 6| Step: 12
Training loss: 1.4025661945343018
Validation loss: 2.006111760934194

Epoch: 6| Step: 13
Training loss: 1.8312362432479858
Validation loss: 2.0107276240984597

Epoch: 90| Step: 0
Training loss: 2.6124773025512695
Validation loss: 2.0036935806274414

Epoch: 6| Step: 1
Training loss: 2.3193202018737793
Validation loss: 2.0044649839401245

Epoch: 6| Step: 2
Training loss: 1.99998140335083
Validation loss: 2.006731847922007

Epoch: 6| Step: 3
Training loss: 2.8738560676574707
Validation loss: 2.0026809573173523

Epoch: 6| Step: 4
Training loss: 2.1961541175842285
Validation loss: 2.010846217473348

Epoch: 6| Step: 5
Training loss: 2.1067283153533936
Validation loss: 2.0158033768335977

Epoch: 6| Step: 6
Training loss: 1.8100581169128418
Validation loss: 2.025273402531942

Epoch: 6| Step: 7
Training loss: 2.1535825729370117
Validation loss: 2.0285987854003906

Epoch: 6| Step: 8
Training loss: 2.4454755783081055
Validation loss: 2.0352799892425537

Epoch: 6| Step: 9
Training loss: 2.009758949279785
Validation loss: 2.0446629722913108

Epoch: 6| Step: 10
Training loss: 1.7652071714401245
Validation loss: 2.0402235786120095

Epoch: 6| Step: 11
Training loss: 2.188852071762085
Validation loss: 2.046961029370626

Epoch: 6| Step: 12
Training loss: 2.060303211212158
Validation loss: 2.0374222000439963

Epoch: 6| Step: 13
Training loss: 1.7704330682754517
Validation loss: 2.0110818346341452

Epoch: 91| Step: 0
Training loss: 2.3357973098754883
Validation loss: 2.0090288122495017

Epoch: 6| Step: 1
Training loss: 2.206522226333618
Validation loss: 2.0039886236190796

Epoch: 6| Step: 2
Training loss: 1.8111834526062012
Validation loss: 1.9992349942525227

Epoch: 6| Step: 3
Training loss: 2.6523399353027344
Validation loss: 2.0058770974477134

Epoch: 6| Step: 4
Training loss: 1.8269140720367432
Validation loss: 2.006451447804769

Epoch: 6| Step: 5
Training loss: 2.7938551902770996
Validation loss: 2.010246495405833

Epoch: 6| Step: 6
Training loss: 1.6626983880996704
Validation loss: 2.0183604756991067

Epoch: 6| Step: 7
Training loss: 1.990360975265503
Validation loss: 2.0194585720698037

Epoch: 6| Step: 8
Training loss: 1.8162437677383423
Validation loss: 2.026262044906616

Epoch: 6| Step: 9
Training loss: 2.8920035362243652
Validation loss: 2.014468173185984

Epoch: 6| Step: 10
Training loss: 1.5401819944381714
Validation loss: 2.011149227619171

Epoch: 6| Step: 11
Training loss: 2.008678913116455
Validation loss: 2.0107245246569314

Epoch: 6| Step: 12
Training loss: 2.636993408203125
Validation loss: 1.9997678796450298

Epoch: 6| Step: 13
Training loss: 2.0595133304595947
Validation loss: 2.008428156375885

Epoch: 92| Step: 0
Training loss: 1.7921297550201416
Validation loss: 2.0110663771629333

Epoch: 6| Step: 1
Training loss: 2.304671287536621
Validation loss: 2.007385512193044

Epoch: 6| Step: 2
Training loss: 2.6913604736328125
Validation loss: 2.0036992828051248

Epoch: 6| Step: 3
Training loss: 2.3049376010894775
Validation loss: 2.0023783644040427

Epoch: 6| Step: 4
Training loss: 2.075913906097412
Validation loss: 2.007376710573832

Epoch: 6| Step: 5
Training loss: 1.7834687232971191
Validation loss: 2.013735353946686

Epoch: 6| Step: 6
Training loss: 1.7660090923309326
Validation loss: 2.007566591103872

Epoch: 6| Step: 7
Training loss: 2.052598237991333
Validation loss: 2.014792780081431

Epoch: 6| Step: 8
Training loss: 1.8571114540100098
Validation loss: 2.0195528666178384

Epoch: 6| Step: 9
Training loss: 2.2245049476623535
Validation loss: 2.0311898390452066

Epoch: 6| Step: 10
Training loss: 2.3129191398620605
Validation loss: 2.0415799617767334

Epoch: 6| Step: 11
Training loss: 2.753443717956543
Validation loss: 2.027748723824819

Epoch: 6| Step: 12
Training loss: 1.1994051933288574
Validation loss: 2.041630188624064

Epoch: 6| Step: 13
Training loss: 2.6630775928497314
Validation loss: 2.045170327027639

Epoch: 93| Step: 0
Training loss: 2.098473072052002
Validation loss: 2.0349847873051963

Epoch: 6| Step: 1
Training loss: 2.379483699798584
Validation loss: 2.0393506288528442

Epoch: 6| Step: 2
Training loss: 2.0333402156829834
Validation loss: 2.054498851299286

Epoch: 6| Step: 3
Training loss: 2.7686192989349365
Validation loss: 2.0441505908966064

Epoch: 6| Step: 4
Training loss: 1.3566879034042358
Validation loss: 2.0411720275878906

Epoch: 6| Step: 5
Training loss: 2.0456881523132324
Validation loss: 2.0427563389142356

Epoch: 6| Step: 6
Training loss: 2.067006826400757
Validation loss: 2.0417200326919556

Epoch: 6| Step: 7
Training loss: 1.8531253337860107
Validation loss: 2.0362435579299927

Epoch: 6| Step: 8
Training loss: 2.0306570529937744
Validation loss: 2.0370552937189736

Epoch: 6| Step: 9
Training loss: 2.660177707672119
Validation loss: 2.033757666746775

Epoch: 6| Step: 10
Training loss: 2.1702303886413574
Validation loss: 2.033096730709076

Epoch: 6| Step: 11
Training loss: 2.226621627807617
Validation loss: 2.0153621832529702

Epoch: 6| Step: 12
Training loss: 2.3803892135620117
Validation loss: 2.0071764985720315

Epoch: 6| Step: 13
Training loss: 2.033735752105713
Validation loss: 2.00955859820048

Epoch: 94| Step: 0
Training loss: 2.3584797382354736
Validation loss: 2.0075292587280273

Epoch: 6| Step: 1
Training loss: 1.9741299152374268
Validation loss: 2.0010977586110434

Epoch: 6| Step: 2
Training loss: 2.1961584091186523
Validation loss: 2.0109591086705527

Epoch: 6| Step: 3
Training loss: 1.824122667312622
Validation loss: 2.005805015563965

Epoch: 6| Step: 4
Training loss: 2.5806198120117188
Validation loss: 2.012913624445597

Epoch: 6| Step: 5
Training loss: 2.2615723609924316
Validation loss: 2.0085273385047913

Epoch: 6| Step: 6
Training loss: 2.1833763122558594
Validation loss: 2.012418528397878

Epoch: 6| Step: 7
Training loss: 2.1723861694335938
Validation loss: 2.0113369822502136

Epoch: 6| Step: 8
Training loss: 1.8398330211639404
Validation loss: 2.007019062836965

Epoch: 6| Step: 9
Training loss: 1.956693410873413
Validation loss: 2.0074946880340576

Epoch: 6| Step: 10
Training loss: 2.4717493057250977
Validation loss: 2.0156681140263877

Epoch: 6| Step: 11
Training loss: 2.478116989135742
Validation loss: 2.0176437298456826

Epoch: 6| Step: 12
Training loss: 1.926767349243164
Validation loss: 2.028864641984304

Epoch: 6| Step: 13
Training loss: 1.7708094120025635
Validation loss: 2.025373081366221

Epoch: 95| Step: 0
Training loss: 2.2019338607788086
Validation loss: 2.029378970464071

Epoch: 6| Step: 1
Training loss: 2.1295888423919678
Validation loss: 2.0417107144991555

Epoch: 6| Step: 2
Training loss: 1.9737043380737305
Validation loss: 2.052464485168457

Epoch: 6| Step: 3
Training loss: 1.807720422744751
Validation loss: 2.042373538017273

Epoch: 6| Step: 4
Training loss: 2.092799186706543
Validation loss: 2.0448684295018515

Epoch: 6| Step: 5
Training loss: 2.1034088134765625
Validation loss: 2.0394906997680664

Epoch: 6| Step: 6
Training loss: 2.2976198196411133
Validation loss: 2.0359895626703897

Epoch: 6| Step: 7
Training loss: 2.1429059505462646
Validation loss: 2.039728065331777

Epoch: 6| Step: 8
Training loss: 2.3780951499938965
Validation loss: 2.0304089784622192

Epoch: 6| Step: 9
Training loss: 2.085141658782959
Validation loss: 2.03872819741567

Epoch: 6| Step: 10
Training loss: 2.1197705268859863
Validation loss: 2.024922470251719

Epoch: 6| Step: 11
Training loss: 2.07832670211792
Validation loss: 2.035119414329529

Epoch: 6| Step: 12
Training loss: 2.304208755493164
Validation loss: 2.0197837154070535

Epoch: 6| Step: 13
Training loss: 2.09169864654541
Validation loss: 2.019160568714142

Epoch: 96| Step: 0
Training loss: 2.098787307739258
Validation loss: 2.0093849500020347

Epoch: 6| Step: 1
Training loss: 1.8460822105407715
Validation loss: 2.005580643812815

Epoch: 6| Step: 2
Training loss: 2.1738951206207275
Validation loss: 2.019867797692617

Epoch: 6| Step: 3
Training loss: 2.2055578231811523
Validation loss: 2.015053610006968

Epoch: 6| Step: 4
Training loss: 1.6556520462036133
Validation loss: 2.020986497402191

Epoch: 6| Step: 5
Training loss: 2.5162546634674072
Validation loss: 2.0141170422236123

Epoch: 6| Step: 6
Training loss: 2.4313580989837646
Validation loss: 2.0172428687413535

Epoch: 6| Step: 7
Training loss: 2.284107208251953
Validation loss: 2.03414249420166

Epoch: 6| Step: 8
Training loss: 2.021310567855835
Validation loss: 2.0245245496431985

Epoch: 6| Step: 9
Training loss: 2.3268799781799316
Validation loss: 2.030856132507324

Epoch: 6| Step: 10
Training loss: 2.3746540546417236
Validation loss: 2.0349557995796204

Epoch: 6| Step: 11
Training loss: 2.193354368209839
Validation loss: 2.0296877225240073

Epoch: 6| Step: 12
Training loss: 1.0896093845367432
Validation loss: 2.0426790912946067

Epoch: 6| Step: 13
Training loss: 2.8423354625701904
Validation loss: 2.0475374460220337

Epoch: 97| Step: 0
Training loss: 2.245365619659424
Validation loss: 2.0424758990605674

Epoch: 6| Step: 1
Training loss: 2.1554856300354004
Validation loss: 2.0398409167925515

Epoch: 6| Step: 2
Training loss: 2.1892294883728027
Validation loss: 2.0346821745236716

Epoch: 6| Step: 3
Training loss: 2.370709180831909
Validation loss: 2.028693656126658

Epoch: 6| Step: 4
Training loss: 1.8737622499465942
Validation loss: 2.0240806341171265

Epoch: 6| Step: 5
Training loss: 2.5839898586273193
Validation loss: 2.022708316644033

Epoch: 6| Step: 6
Training loss: 2.1194567680358887
Validation loss: 2.01489911476771

Epoch: 6| Step: 7
Training loss: 2.155449867248535
Validation loss: 2.0094106594721475

Epoch: 6| Step: 8
Training loss: 2.0687379837036133
Validation loss: 2.0162349343299866

Epoch: 6| Step: 9
Training loss: 1.8897128105163574
Validation loss: 2.0196531812349954

Epoch: 6| Step: 10
Training loss: 2.091944694519043
Validation loss: 2.0149106979370117

Epoch: 6| Step: 11
Training loss: 2.347592353820801
Validation loss: 2.017541507879893

Epoch: 6| Step: 12
Training loss: 1.8289551734924316
Validation loss: 2.0243581334749856

Epoch: 6| Step: 13
Training loss: 1.837815761566162
Validation loss: 2.0259611209233603

Epoch: 98| Step: 0
Training loss: 2.0516369342803955
Validation loss: 2.0294089317321777

Epoch: 6| Step: 1
Training loss: 1.9672644138336182
Validation loss: 2.029666006565094

Epoch: 6| Step: 2
Training loss: 3.1232409477233887
Validation loss: 2.03266171614329

Epoch: 6| Step: 3
Training loss: 1.878473162651062
Validation loss: 2.0411803921063743

Epoch: 6| Step: 4
Training loss: 1.389139175415039
Validation loss: 2.0336063504219055

Epoch: 6| Step: 5
Training loss: 2.126807689666748
Validation loss: 2.0326064626375833

Epoch: 6| Step: 6
Training loss: 2.266937255859375
Validation loss: 2.040450135866801

Epoch: 6| Step: 7
Training loss: 1.8739228248596191
Validation loss: 2.0418031414349875

Epoch: 6| Step: 8
Training loss: 1.8347898721694946
Validation loss: 2.0406409899393716

Epoch: 6| Step: 9
Training loss: 2.1093335151672363
Validation loss: 2.042281925678253

Epoch: 6| Step: 10
Training loss: 2.189474105834961
Validation loss: 2.037044962247213

Epoch: 6| Step: 11
Training loss: 2.5299127101898193
Validation loss: 2.0419251521428428

Epoch: 6| Step: 12
Training loss: 2.094008684158325
Validation loss: 2.0310874581336975

Epoch: 6| Step: 13
Training loss: 2.216902256011963
Validation loss: 2.0205224553743997

Epoch: 99| Step: 0
Training loss: 2.513568878173828
Validation loss: 2.0333111683527627

Epoch: 6| Step: 1
Training loss: 2.180154800415039
Validation loss: 2.0257962346076965

Epoch: 6| Step: 2
Training loss: 2.284512519836426
Validation loss: 2.025572101275126

Epoch: 6| Step: 3
Training loss: 2.3446784019470215
Validation loss: 2.0224465131759644

Epoch: 6| Step: 4
Training loss: 2.183387279510498
Validation loss: 2.022324721018473

Epoch: 6| Step: 5
Training loss: 1.750421166419983
Validation loss: 2.009606579939524

Epoch: 6| Step: 6
Training loss: 2.01804256439209
Validation loss: 2.013499935468038

Epoch: 6| Step: 7
Training loss: 1.653559923171997
Validation loss: 2.0188855727513633

Epoch: 6| Step: 8
Training loss: 1.5949963331222534
Validation loss: 2.016135811805725

Epoch: 6| Step: 9
Training loss: 2.0597290992736816
Validation loss: 2.0198974212010703

Epoch: 6| Step: 10
Training loss: 2.1416115760803223
Validation loss: 2.0232481161753335

Epoch: 6| Step: 11
Training loss: 2.717947483062744
Validation loss: 2.019333521525065

Epoch: 6| Step: 12
Training loss: 1.918487548828125
Validation loss: 2.0266185800234475

Epoch: 6| Step: 13
Training loss: 2.2791411876678467
Validation loss: 2.0220369895299277

Epoch: 100| Step: 0
Training loss: 1.3922399282455444
Validation loss: 2.0275455315907798

Epoch: 6| Step: 1
Training loss: 2.0975489616394043
Validation loss: 2.023062825202942

Epoch: 6| Step: 2
Training loss: 2.249666213989258
Validation loss: 2.030782183011373

Epoch: 6| Step: 3
Training loss: 3.0146613121032715
Validation loss: 2.0234833359718323

Epoch: 6| Step: 4
Training loss: 2.1700589656829834
Validation loss: 2.0214534600575766

Epoch: 6| Step: 5
Training loss: 2.194768190383911
Validation loss: 2.0147318840026855

Epoch: 6| Step: 6
Training loss: 1.816411018371582
Validation loss: 2.0138869682947793

Epoch: 6| Step: 7
Training loss: 2.2102620601654053
Validation loss: 2.0121899247169495

Epoch: 6| Step: 8
Training loss: 2.8003287315368652
Validation loss: 2.0143407781918845

Epoch: 6| Step: 9
Training loss: 2.030463218688965
Validation loss: 2.0122660398483276

Epoch: 6| Step: 10
Training loss: 2.054798126220703
Validation loss: 2.009836951891581

Epoch: 6| Step: 11
Training loss: 1.9618098735809326
Validation loss: 2.0138433376948037

Epoch: 6| Step: 12
Training loss: 1.8053395748138428
Validation loss: 2.0287938515345254

Epoch: 6| Step: 13
Training loss: 1.9438790082931519
Validation loss: 2.027381718158722

Epoch: 101| Step: 0
Training loss: 1.6951520442962646
Validation loss: 2.0388633608818054

Epoch: 6| Step: 1
Training loss: 2.83447265625
Validation loss: 2.046786308288574

Epoch: 6| Step: 2
Training loss: 2.498636484146118
Validation loss: 2.0402708848317466

Epoch: 6| Step: 3
Training loss: 2.047736406326294
Validation loss: 2.0467734138170877

Epoch: 6| Step: 4
Training loss: 1.666967749595642
Validation loss: 2.042056361834208

Epoch: 6| Step: 5
Training loss: 2.0726919174194336
Validation loss: 2.039745291074117

Epoch: 6| Step: 6
Training loss: 2.378568410873413
Validation loss: 2.034656544526418

Epoch: 6| Step: 7
Training loss: 2.2591640949249268
Validation loss: 2.037634293238322

Epoch: 6| Step: 8
Training loss: 2.097046375274658
Validation loss: 2.0296808083852134

Epoch: 6| Step: 9
Training loss: 2.099660873413086
Validation loss: 2.023626764615377

Epoch: 6| Step: 10
Training loss: 2.1329903602600098
Validation loss: 2.002112030982971

Epoch: 6| Step: 11
Training loss: 2.217043876647949
Validation loss: 2.0140766302744546

Epoch: 6| Step: 12
Training loss: 2.123605489730835
Validation loss: 2.0199885964393616

Epoch: 6| Step: 13
Training loss: 1.5483843088150024
Validation loss: 2.0053164958953857

Epoch: 102| Step: 0
Training loss: 1.7315452098846436
Validation loss: 2.0059205492337546

Epoch: 6| Step: 1
Training loss: 2.356611967086792
Validation loss: 2.0038771629333496

Epoch: 6| Step: 2
Training loss: 2.4120266437530518
Validation loss: 2.00696208079656

Epoch: 6| Step: 3
Training loss: 2.420356035232544
Validation loss: 2.010506888230642

Epoch: 6| Step: 4
Training loss: 2.063293933868408
Validation loss: 2.009877880414327

Epoch: 6| Step: 5
Training loss: 1.8225154876708984
Validation loss: 2.0189077059427896

Epoch: 6| Step: 6
Training loss: 2.4244487285614014
Validation loss: 2.0185601909955344

Epoch: 6| Step: 7
Training loss: 1.9366052150726318
Validation loss: 2.0223833123842874

Epoch: 6| Step: 8
Training loss: 1.7939629554748535
Validation loss: 2.0304700136184692

Epoch: 6| Step: 9
Training loss: 2.3606178760528564
Validation loss: 2.0319080551465354

Epoch: 6| Step: 10
Training loss: 1.971132755279541
Validation loss: 2.020215113957723

Epoch: 6| Step: 11
Training loss: 1.777585506439209
Validation loss: 2.0208838184674582

Epoch: 6| Step: 12
Training loss: 2.578561782836914
Validation loss: 2.024296820163727

Epoch: 6| Step: 13
Training loss: 2.1459505558013916
Validation loss: 2.0185499588648477

Epoch: 103| Step: 0
Training loss: 2.7272868156433105
Validation loss: 2.0131784280141196

Epoch: 6| Step: 1
Training loss: 1.5490442514419556
Validation loss: 2.026273171106974

Epoch: 6| Step: 2
Training loss: 2.300524950027466
Validation loss: 2.0152955253918967

Epoch: 6| Step: 3
Training loss: 2.217102289199829
Validation loss: 2.022313197453817

Epoch: 6| Step: 4
Training loss: 2.700953960418701
Validation loss: 2.015843093395233

Epoch: 6| Step: 5
Training loss: 2.267936944961548
Validation loss: 2.0141027371088662

Epoch: 6| Step: 6
Training loss: 1.9213783740997314
Validation loss: 2.018350382645925

Epoch: 6| Step: 7
Training loss: 1.476440668106079
Validation loss: 2.0058801571528115

Epoch: 6| Step: 8
Training loss: 2.5239245891571045
Validation loss: 2.008732815583547

Epoch: 6| Step: 9
Training loss: 1.9458805322647095
Validation loss: 2.0137735406557717

Epoch: 6| Step: 10
Training loss: 2.0917129516601562
Validation loss: 2.012481609980265

Epoch: 6| Step: 11
Training loss: 2.158634901046753
Validation loss: 2.0093705455462136

Epoch: 6| Step: 12
Training loss: 1.5708768367767334
Validation loss: 2.0239623387654624

Epoch: 6| Step: 13
Training loss: 2.0644843578338623
Validation loss: 2.022857666015625

Epoch: 104| Step: 0
Training loss: 2.390608072280884
Validation loss: 2.019708275794983

Epoch: 6| Step: 1
Training loss: 1.8070154190063477
Validation loss: 2.033832828203837

Epoch: 6| Step: 2
Training loss: 2.6177756786346436
Validation loss: 2.028812368710836

Epoch: 6| Step: 3
Training loss: 1.9314513206481934
Validation loss: 2.0143487254778543

Epoch: 6| Step: 4
Training loss: 1.8599302768707275
Validation loss: 2.0246982177098594

Epoch: 6| Step: 5
Training loss: 1.606576681137085
Validation loss: 2.022841513156891

Epoch: 6| Step: 6
Training loss: 1.6137361526489258
Validation loss: 2.0294683376948037

Epoch: 6| Step: 7
Training loss: 2.217636823654175
Validation loss: 2.018126686414083

Epoch: 6| Step: 8
Training loss: 2.4647629261016846
Validation loss: 2.0207294623057046

Epoch: 6| Step: 9
Training loss: 1.6779687404632568
Validation loss: 2.021511455376943

Epoch: 6| Step: 10
Training loss: 1.7122955322265625
Validation loss: 2.015963315963745

Epoch: 6| Step: 11
Training loss: 2.1464426517486572
Validation loss: 2.025817354520162

Epoch: 6| Step: 12
Training loss: 3.3205339908599854
Validation loss: 2.024145563443502

Epoch: 6| Step: 13
Training loss: 2.2128541469573975
Validation loss: 2.024014820655187

Epoch: 105| Step: 0
Training loss: 2.564840078353882
Validation loss: 2.032911022504171

Epoch: 6| Step: 1
Training loss: 2.219308853149414
Validation loss: 2.041638274987539

Epoch: 6| Step: 2
Training loss: 1.7812772989273071
Validation loss: 2.029971122741699

Epoch: 6| Step: 3
Training loss: 2.691138505935669
Validation loss: 2.0375877221425376

Epoch: 6| Step: 4
Training loss: 2.4029202461242676
Validation loss: 2.0461763540903726

Epoch: 6| Step: 5
Training loss: 1.7759836912155151
Validation loss: 2.0388526121775308

Epoch: 6| Step: 6
Training loss: 1.4022877216339111
Validation loss: 2.0373135606447854

Epoch: 6| Step: 7
Training loss: 2.203946352005005
Validation loss: 2.0515375336011252

Epoch: 6| Step: 8
Training loss: 2.0777859687805176
Validation loss: 2.049107253551483

Epoch: 6| Step: 9
Training loss: 2.447801113128662
Validation loss: 2.037944217522939

Epoch: 6| Step: 10
Training loss: 2.221524715423584
Validation loss: 2.045741101106008

Epoch: 6| Step: 11
Training loss: 1.2706025838851929
Validation loss: 2.0445592602094016

Epoch: 6| Step: 12
Training loss: 1.7660483121871948
Validation loss: 2.0384558836619058

Epoch: 6| Step: 13
Training loss: 2.492724895477295
Validation loss: 2.033085346221924

Epoch: 106| Step: 0
Training loss: 2.0471456050872803
Validation loss: 2.0285639564196267

Epoch: 6| Step: 1
Training loss: 2.025559663772583
Validation loss: 2.0219821532567344

Epoch: 6| Step: 2
Training loss: 1.79790461063385
Validation loss: 2.030636409918467

Epoch: 6| Step: 3
Training loss: 1.9531707763671875
Validation loss: 2.03146622578303

Epoch: 6| Step: 4
Training loss: 1.7015876770019531
Validation loss: 2.0193880200386047

Epoch: 6| Step: 5
Training loss: 1.8845168352127075
Validation loss: 2.022046605745951

Epoch: 6| Step: 6
Training loss: 2.125275135040283
Validation loss: 2.0263187686602273

Epoch: 6| Step: 7
Training loss: 2.1972169876098633
Validation loss: 2.021543780962626

Epoch: 6| Step: 8
Training loss: 2.161064863204956
Validation loss: 2.0115631222724915

Epoch: 6| Step: 9
Training loss: 2.6253488063812256
Validation loss: 2.0244639118512473

Epoch: 6| Step: 10
Training loss: 2.0724825859069824
Validation loss: 2.0190330743789673

Epoch: 6| Step: 11
Training loss: 1.8781211376190186
Validation loss: 2.0338669617970786

Epoch: 6| Step: 12
Training loss: 2.2546000480651855
Validation loss: 2.0355018774668374

Epoch: 6| Step: 13
Training loss: 2.671891212463379
Validation loss: 2.0364184379577637

Epoch: 107| Step: 0
Training loss: 2.3577523231506348
Validation loss: 2.0449480613072715

Epoch: 6| Step: 1
Training loss: 2.02700138092041
Validation loss: 2.0282315810521445

Epoch: 6| Step: 2
Training loss: 2.2172584533691406
Validation loss: 2.0281439622243247

Epoch: 6| Step: 3
Training loss: 2.355861186981201
Validation loss: 2.032858113447825

Epoch: 6| Step: 4
Training loss: 1.879960060119629
Validation loss: 2.0288652181625366

Epoch: 6| Step: 5
Training loss: 2.999134063720703
Validation loss: 2.031253933906555

Epoch: 6| Step: 6
Training loss: 2.1904027462005615
Validation loss: 2.024406651655833

Epoch: 6| Step: 7
Training loss: 1.7263778448104858
Validation loss: 2.020160893599192

Epoch: 6| Step: 8
Training loss: 1.8398809432983398
Validation loss: 2.0258345206578574

Epoch: 6| Step: 9
Training loss: 2.3928418159484863
Validation loss: 2.025823394457499

Epoch: 6| Step: 10
Training loss: 1.6755361557006836
Validation loss: 2.023899793624878

Epoch: 6| Step: 11
Training loss: 2.176684856414795
Validation loss: 2.0385409196217856

Epoch: 6| Step: 12
Training loss: 1.6482058763504028
Validation loss: 2.0225799878438315

Epoch: 6| Step: 13
Training loss: 1.9075536727905273
Validation loss: 2.043593923250834

Epoch: 108| Step: 0
Training loss: 2.2425479888916016
Validation loss: 2.0214755535125732

Epoch: 6| Step: 1
Training loss: 2.032477378845215
Validation loss: 2.028114080429077

Epoch: 6| Step: 2
Training loss: 1.904143214225769
Validation loss: 2.02007387081782

Epoch: 6| Step: 3
Training loss: 2.074613571166992
Validation loss: 2.022768477598826

Epoch: 6| Step: 4
Training loss: 2.189840316772461
Validation loss: 2.027610262235006

Epoch: 6| Step: 5
Training loss: 2.392798900604248
Validation loss: 2.037516256173452

Epoch: 6| Step: 6
Training loss: 2.273613691329956
Validation loss: 2.035144786039988

Epoch: 6| Step: 7
Training loss: 2.0789036750793457
Validation loss: 2.0335031747817993

Epoch: 6| Step: 8
Training loss: 1.2897307872772217
Validation loss: 2.02548356850942

Epoch: 6| Step: 9
Training loss: 2.3711915016174316
Validation loss: 2.0323305328687034

Epoch: 6| Step: 10
Training loss: 2.237578868865967
Validation loss: 2.04490065574646

Epoch: 6| Step: 11
Training loss: 1.5303385257720947
Validation loss: 2.0303371946016946

Epoch: 6| Step: 12
Training loss: 2.0015082359313965
Validation loss: 2.024515231450399

Epoch: 6| Step: 13
Training loss: 2.470289707183838
Validation loss: 2.0238014856974282

Epoch: 109| Step: 0
Training loss: 2.7049829959869385
Validation loss: 2.022301276524862

Epoch: 6| Step: 1
Training loss: 1.7576968669891357
Validation loss: 2.0149068236351013

Epoch: 6| Step: 2
Training loss: 1.827391505241394
Validation loss: 2.012029747168223

Epoch: 6| Step: 3
Training loss: 1.7986996173858643
Validation loss: 2.0220346252123513

Epoch: 6| Step: 4
Training loss: 2.8797755241394043
Validation loss: 2.0260584950447083

Epoch: 6| Step: 5
Training loss: 1.9678595066070557
Validation loss: 2.0188708901405334

Epoch: 6| Step: 6
Training loss: 1.98410165309906
Validation loss: 2.017634093761444

Epoch: 6| Step: 7
Training loss: 2.3108954429626465
Validation loss: 2.029740869998932

Epoch: 6| Step: 8
Training loss: 2.5622191429138184
Validation loss: 2.03304115931193

Epoch: 6| Step: 9
Training loss: 2.123493194580078
Validation loss: 2.0204989512761435

Epoch: 6| Step: 10
Training loss: 1.466105580329895
Validation loss: 2.028354366620382

Epoch: 6| Step: 11
Training loss: 2.268789291381836
Validation loss: 2.0358802874883017

Epoch: 6| Step: 12
Training loss: 1.9731286764144897
Validation loss: 2.0400843421618142

Epoch: 6| Step: 13
Training loss: 2.015014171600342
Validation loss: 2.0416609048843384

Epoch: 110| Step: 0
Training loss: 2.243821620941162
Validation loss: 2.0418527126312256

Epoch: 6| Step: 1
Training loss: 2.309521198272705
Validation loss: 2.048012673854828

Epoch: 6| Step: 2
Training loss: 2.34295916557312
Validation loss: 2.0517280101776123

Epoch: 6| Step: 3
Training loss: 2.0644192695617676
Validation loss: 2.039430538813273

Epoch: 6| Step: 4
Training loss: 2.1645631790161133
Validation loss: 2.0339060624440513

Epoch: 6| Step: 5
Training loss: 1.62324059009552
Validation loss: 2.0412891705830893

Epoch: 6| Step: 6
Training loss: 2.2083747386932373
Validation loss: 2.026998003323873

Epoch: 6| Step: 7
Training loss: 2.257277727127075
Validation loss: 2.030702809492747

Epoch: 6| Step: 8
Training loss: 1.9829521179199219
Validation loss: 2.0291606386502585

Epoch: 6| Step: 9
Training loss: 2.026829719543457
Validation loss: 2.0307622154553733

Epoch: 6| Step: 10
Training loss: 2.817227602005005
Validation loss: 2.027352213859558

Epoch: 6| Step: 11
Training loss: 1.3800649642944336
Validation loss: 2.0301843682924905

Epoch: 6| Step: 12
Training loss: 2.3490405082702637
Validation loss: 2.0362828771273294

Epoch: 6| Step: 13
Training loss: 1.5607373714447021
Validation loss: 2.0417794783910117

Epoch: 111| Step: 0
Training loss: 2.471639633178711
Validation loss: 2.038795590400696

Epoch: 6| Step: 1
Training loss: 1.564220905303955
Validation loss: 2.03135613600413

Epoch: 6| Step: 2
Training loss: 1.7864832878112793
Validation loss: 2.0224563082059226

Epoch: 6| Step: 3
Training loss: 2.2999963760375977
Validation loss: 2.0183683236440024

Epoch: 6| Step: 4
Training loss: 2.202375650405884
Validation loss: 2.0265784859657288

Epoch: 6| Step: 5
Training loss: 1.8660252094268799
Validation loss: 2.0201757152875266

Epoch: 6| Step: 6
Training loss: 1.8632094860076904
Validation loss: 2.01964940627416

Epoch: 6| Step: 7
Training loss: 2.140399932861328
Validation loss: 2.0144699017206826

Epoch: 6| Step: 8
Training loss: 1.9526201486587524
Validation loss: 2.023190160592397

Epoch: 6| Step: 9
Training loss: 2.044788360595703
Validation loss: 2.012473007043203

Epoch: 6| Step: 10
Training loss: 2.0856828689575195
Validation loss: 2.0204447706540427

Epoch: 6| Step: 11
Training loss: 2.3443665504455566
Validation loss: 2.030695597330729

Epoch: 6| Step: 12
Training loss: 2.0567848682403564
Validation loss: 2.028421143690745

Epoch: 6| Step: 13
Training loss: 2.626784563064575
Validation loss: 2.0385536352793374

Epoch: 112| Step: 0
Training loss: 2.2936205863952637
Validation loss: 2.038000762462616

Epoch: 6| Step: 1
Training loss: 1.731611967086792
Validation loss: 2.034165859222412

Epoch: 6| Step: 2
Training loss: 2.3467228412628174
Validation loss: 2.043359716733297

Epoch: 6| Step: 3
Training loss: 2.2332217693328857
Validation loss: 2.0241509477297464

Epoch: 6| Step: 4
Training loss: 1.6135305166244507
Validation loss: 2.0510269006093345

Epoch: 6| Step: 5
Training loss: 2.0229036808013916
Validation loss: 2.031653046607971

Epoch: 6| Step: 6
Training loss: 1.7051091194152832
Validation loss: 2.025601347287496

Epoch: 6| Step: 7
Training loss: 2.5434415340423584
Validation loss: 2.037584046522776

Epoch: 6| Step: 8
Training loss: 2.540602207183838
Validation loss: 2.0316428939501443

Epoch: 6| Step: 9
Training loss: 1.9345173835754395
Validation loss: 2.0340182185173035

Epoch: 6| Step: 10
Training loss: 2.674940586090088
Validation loss: 2.0300296942392984

Epoch: 6| Step: 11
Training loss: 2.2776825428009033
Validation loss: 2.0386591951052346

Epoch: 6| Step: 12
Training loss: 1.75849187374115
Validation loss: 2.038905620574951

Epoch: 6| Step: 13
Training loss: 1.7226388454437256
Validation loss: 2.025339444478353

Epoch: 113| Step: 0
Training loss: 1.789820909500122
Validation loss: 2.0364091197649636

Epoch: 6| Step: 1
Training loss: 2.9383225440979004
Validation loss: 2.025241275628408

Epoch: 6| Step: 2
Training loss: 2.060662269592285
Validation loss: 2.037514547506968

Epoch: 6| Step: 3
Training loss: 1.5471690893173218
Validation loss: 2.0267179012298584

Epoch: 6| Step: 4
Training loss: 1.9907721281051636
Validation loss: 2.0214413603146872

Epoch: 6| Step: 5
Training loss: 1.5028324127197266
Validation loss: 2.03944855928421

Epoch: 6| Step: 6
Training loss: 2.304263114929199
Validation loss: 2.035814940929413

Epoch: 6| Step: 7
Training loss: 2.1237921714782715
Validation loss: 2.0288297732671103

Epoch: 6| Step: 8
Training loss: 1.563319444656372
Validation loss: 2.036177138487498

Epoch: 6| Step: 9
Training loss: 1.7883018255233765
Validation loss: 2.035707334677378

Epoch: 6| Step: 10
Training loss: 2.779608726501465
Validation loss: 2.032033999760946

Epoch: 6| Step: 11
Training loss: 2.0050270557403564
Validation loss: 2.03029203414917

Epoch: 6| Step: 12
Training loss: 2.5327095985412598
Validation loss: 2.040265142917633

Epoch: 6| Step: 13
Training loss: 2.147759437561035
Validation loss: 2.0282464623451233

Epoch: 114| Step: 0
Training loss: 1.9343444108963013
Validation loss: 2.0262810389200845

Epoch: 6| Step: 1
Training loss: 2.1740102767944336
Validation loss: 2.027894059816996

Epoch: 6| Step: 2
Training loss: 1.9336669445037842
Validation loss: 2.0361624161402383

Epoch: 6| Step: 3
Training loss: 2.3350987434387207
Validation loss: 2.043979028860728

Epoch: 6| Step: 4
Training loss: 1.9530826807022095
Validation loss: 2.0546920696894326

Epoch: 6| Step: 5
Training loss: 2.021874189376831
Validation loss: 2.0515034993489585

Epoch: 6| Step: 6
Training loss: 2.2318122386932373
Validation loss: 2.0601436297098794

Epoch: 6| Step: 7
Training loss: 2.4763312339782715
Validation loss: 2.058724125226339

Epoch: 6| Step: 8
Training loss: 2.5252766609191895
Validation loss: 2.0531436800956726

Epoch: 6| Step: 9
Training loss: 1.6565572023391724
Validation loss: 2.054966628551483

Epoch: 6| Step: 10
Training loss: 2.5385990142822266
Validation loss: 2.0460125207901

Epoch: 6| Step: 11
Training loss: 2.2077629566192627
Validation loss: 2.0507067839304605

Epoch: 6| Step: 12
Training loss: 2.37652587890625
Validation loss: 2.0471696257591248

Epoch: 6| Step: 13
Training loss: 1.8138551712036133
Validation loss: 2.0337210297584534

Epoch: 115| Step: 0
Training loss: 2.0849497318267822
Validation loss: 2.03079346815745

Epoch: 6| Step: 1
Training loss: 2.147334098815918
Validation loss: 2.025666077931722

Epoch: 6| Step: 2
Training loss: 2.625110149383545
Validation loss: 2.01769483089447

Epoch: 6| Step: 3
Training loss: 2.082974433898926
Validation loss: 2.0282987554868064

Epoch: 6| Step: 4
Training loss: 2.530137062072754
Validation loss: 2.026414136091868

Epoch: 6| Step: 5
Training loss: 1.9790979623794556
Validation loss: 2.03976317246755

Epoch: 6| Step: 6
Training loss: 2.052767515182495
Validation loss: 2.0494779149691262

Epoch: 6| Step: 7
Training loss: 2.57344388961792
Validation loss: 2.0575443307558694

Epoch: 6| Step: 8
Training loss: 1.4472241401672363
Validation loss: 2.0566766460736594

Epoch: 6| Step: 9
Training loss: 1.62730073928833
Validation loss: 2.053978602091471

Epoch: 6| Step: 10
Training loss: 1.91741144657135
Validation loss: 2.060853282610575

Epoch: 6| Step: 11
Training loss: 1.9819605350494385
Validation loss: 2.05537348985672

Epoch: 6| Step: 12
Training loss: 1.9603347778320312
Validation loss: 2.049221475919088

Epoch: 6| Step: 13
Training loss: 2.2778539657592773
Validation loss: 2.043825169404348

Epoch: 116| Step: 0
Training loss: 2.5979156494140625
Validation loss: 2.046759088834127

Epoch: 6| Step: 1
Training loss: 2.008981704711914
Validation loss: 2.0325681567192078

Epoch: 6| Step: 2
Training loss: 1.8161381483078003
Validation loss: 2.0301470955212912

Epoch: 6| Step: 3
Training loss: 2.0023090839385986
Validation loss: 2.026598791281382

Epoch: 6| Step: 4
Training loss: 2.2098095417022705
Validation loss: 2.010587771733602

Epoch: 6| Step: 5
Training loss: 1.867828369140625
Validation loss: 2.012842814127604

Epoch: 6| Step: 6
Training loss: 1.9066362380981445
Validation loss: 2.0133097171783447

Epoch: 6| Step: 7
Training loss: 1.949143648147583
Validation loss: 2.0142327149709067

Epoch: 6| Step: 8
Training loss: 2.3976168632507324
Validation loss: 2.0106879274050393

Epoch: 6| Step: 9
Training loss: 2.5317862033843994
Validation loss: 2.027766764163971

Epoch: 6| Step: 10
Training loss: 1.5568428039550781
Validation loss: 2.0146721800168357

Epoch: 6| Step: 11
Training loss: 1.8609328269958496
Validation loss: 2.0171119372049966

Epoch: 6| Step: 12
Training loss: 2.6833553314208984
Validation loss: 2.008548061052958

Epoch: 6| Step: 13
Training loss: 2.149555206298828
Validation loss: 2.016794125239054

Epoch: 117| Step: 0
Training loss: 2.5486841201782227
Validation loss: 2.014568289120992

Epoch: 6| Step: 1
Training loss: 2.2633371353149414
Validation loss: 2.01101944843928

Epoch: 6| Step: 2
Training loss: 1.8046154975891113
Validation loss: 2.0219844977060952

Epoch: 6| Step: 3
Training loss: 1.4439640045166016
Validation loss: 2.0091689825057983

Epoch: 6| Step: 4
Training loss: 1.576549768447876
Validation loss: 2.0148388147354126

Epoch: 6| Step: 5
Training loss: 2.4705936908721924
Validation loss: 2.0167336066563926

Epoch: 6| Step: 6
Training loss: 2.166778564453125
Validation loss: 2.0236757596333823

Epoch: 6| Step: 7
Training loss: 2.2976293563842773
Validation loss: 2.0264432032903037

Epoch: 6| Step: 8
Training loss: 1.4402360916137695
Validation loss: 2.0264066060384116

Epoch: 6| Step: 9
Training loss: 2.331218719482422
Validation loss: 2.011550704638163

Epoch: 6| Step: 10
Training loss: 2.3741581439971924
Validation loss: 2.0093579093615213

Epoch: 6| Step: 11
Training loss: 2.1603989601135254
Validation loss: 2.0130221843719482

Epoch: 6| Step: 12
Training loss: 2.955784320831299
Validation loss: 2.001891791820526

Epoch: 6| Step: 13
Training loss: 1.6164965629577637
Validation loss: 2.010709524154663

Epoch: 118| Step: 0
Training loss: 1.7679190635681152
Validation loss: 2.0115946729977927

Epoch: 6| Step: 1
Training loss: 2.618060827255249
Validation loss: 2.015140414237976

Epoch: 6| Step: 2
Training loss: 1.8827660083770752
Validation loss: 2.0150814851125083

Epoch: 6| Step: 3
Training loss: 2.211984395980835
Validation loss: 2.016663074493408

Epoch: 6| Step: 4
Training loss: 1.9658219814300537
Validation loss: 2.024569590886434

Epoch: 6| Step: 5
Training loss: 2.269954204559326
Validation loss: 2.016752322514852

Epoch: 6| Step: 6
Training loss: 1.8477520942687988
Validation loss: 2.0167460640271506

Epoch: 6| Step: 7
Training loss: 2.0275368690490723
Validation loss: 2.0150186419487

Epoch: 6| Step: 8
Training loss: 2.7272653579711914
Validation loss: 2.02837598323822

Epoch: 6| Step: 9
Training loss: 1.4201297760009766
Validation loss: 2.039164900779724

Epoch: 6| Step: 10
Training loss: 2.705493927001953
Validation loss: 2.0467597246170044

Epoch: 6| Step: 11
Training loss: 1.862780213356018
Validation loss: 2.0564767916997275

Epoch: 6| Step: 12
Training loss: 2.0069611072540283
Validation loss: 2.045519391695658

Epoch: 6| Step: 13
Training loss: 2.185757637023926
Validation loss: 2.055376629034678

Epoch: 119| Step: 0
Training loss: 2.2885241508483887
Validation loss: 2.048386295636495

Epoch: 6| Step: 1
Training loss: 2.594115734100342
Validation loss: 2.05432520310084

Epoch: 6| Step: 2
Training loss: 1.771541953086853
Validation loss: 2.0473897059758506

Epoch: 6| Step: 3
Training loss: 1.9262053966522217
Validation loss: 2.047710736592611

Epoch: 6| Step: 4
Training loss: 1.9857697486877441
Validation loss: 2.049599289894104

Epoch: 6| Step: 5
Training loss: 2.0842857360839844
Validation loss: 2.0502015153566995

Epoch: 6| Step: 6
Training loss: 1.9847745895385742
Validation loss: 2.049768388271332

Epoch: 6| Step: 7
Training loss: 2.3384854793548584
Validation loss: 2.0404345790545144

Epoch: 6| Step: 8
Training loss: 1.8225563764572144
Validation loss: 2.028013904889425

Epoch: 6| Step: 9
Training loss: 2.533008575439453
Validation loss: 2.024109164873759

Epoch: 6| Step: 10
Training loss: 1.7485666275024414
Validation loss: 2.034307857354482

Epoch: 6| Step: 11
Training loss: 1.7289800643920898
Validation loss: 2.022928456465403

Epoch: 6| Step: 12
Training loss: 1.6519838571548462
Validation loss: 2.002490997314453

Epoch: 6| Step: 13
Training loss: 2.6690633296966553
Validation loss: 2.0005564093589783

Epoch: 120| Step: 0
Training loss: 1.9892563819885254
Validation loss: 2.020976702372233

Epoch: 6| Step: 1
Training loss: 2.1812682151794434
Validation loss: 2.0100879271825156

Epoch: 6| Step: 2
Training loss: 2.7110486030578613
Validation loss: 2.00800887743632

Epoch: 6| Step: 3
Training loss: 1.92194664478302
Validation loss: 2.002188781897227

Epoch: 6| Step: 4
Training loss: 2.236281394958496
Validation loss: 1.9992527763048809

Epoch: 6| Step: 5
Training loss: 1.7800068855285645
Validation loss: 2.0093165238698325

Epoch: 6| Step: 6
Training loss: 1.4514477252960205
Validation loss: 2.001901129881541

Epoch: 6| Step: 7
Training loss: 2.1113719940185547
Validation loss: 2.007234811782837

Epoch: 6| Step: 8
Training loss: 1.4272829294204712
Validation loss: 2.0101587573687234

Epoch: 6| Step: 9
Training loss: 2.5934696197509766
Validation loss: 2.0010547041893005

Epoch: 6| Step: 10
Training loss: 2.2494349479675293
Validation loss: 2.0163191755612693

Epoch: 6| Step: 11
Training loss: 2.011321544647217
Validation loss: 2.0146220922470093

Epoch: 6| Step: 12
Training loss: 2.2726974487304688
Validation loss: 2.019818981488546

Epoch: 6| Step: 13
Training loss: 2.5193023681640625
Validation loss: 2.0212549368540444

Epoch: 121| Step: 0
Training loss: 2.468461036682129
Validation loss: 2.0132211248079934

Epoch: 6| Step: 1
Training loss: 2.332850933074951
Validation loss: 2.0170113841692605

Epoch: 6| Step: 2
Training loss: 2.2633557319641113
Validation loss: 2.0169673959414163

Epoch: 6| Step: 3
Training loss: 1.9035035371780396
Validation loss: 2.0249720414479575

Epoch: 6| Step: 4
Training loss: 2.1166210174560547
Validation loss: 2.0157169898351035

Epoch: 6| Step: 5
Training loss: 2.637815237045288
Validation loss: 2.0274507204691568

Epoch: 6| Step: 6
Training loss: 2.2623376846313477
Validation loss: 2.0248468120892844

Epoch: 6| Step: 7
Training loss: 2.0072295665740967
Validation loss: 2.0231083234151206

Epoch: 6| Step: 8
Training loss: 1.5875526666641235
Validation loss: 2.0285109480222068

Epoch: 6| Step: 9
Training loss: 2.092078447341919
Validation loss: 2.035674532254537

Epoch: 6| Step: 10
Training loss: 2.332608461380005
Validation loss: 2.038650910059611

Epoch: 6| Step: 11
Training loss: 1.7621283531188965
Validation loss: 2.0528517365455627

Epoch: 6| Step: 12
Training loss: 1.7778949737548828
Validation loss: 2.0663119753201804

Epoch: 6| Step: 13
Training loss: 1.6009726524353027
Validation loss: 2.052125891049703

Epoch: 122| Step: 0
Training loss: 2.392101526260376
Validation loss: 2.0620356798171997

Epoch: 6| Step: 1
Training loss: 2.079258680343628
Validation loss: 2.0543020963668823

Epoch: 6| Step: 2
Training loss: 2.2472951412200928
Validation loss: 2.053758958975474

Epoch: 6| Step: 3
Training loss: 2.21036434173584
Validation loss: 2.060081203778585

Epoch: 6| Step: 4
Training loss: 2.011744260787964
Validation loss: 2.0475444197654724

Epoch: 6| Step: 5
Training loss: 1.7531874179840088
Validation loss: 2.0449498693148294

Epoch: 6| Step: 6
Training loss: 2.307279586791992
Validation loss: 2.0464747150739035

Epoch: 6| Step: 7
Training loss: 1.91204833984375
Validation loss: 2.038997491200765

Epoch: 6| Step: 8
Training loss: 1.410266399383545
Validation loss: 2.03705487648646

Epoch: 6| Step: 9
Training loss: 1.835761547088623
Validation loss: 2.042685031890869

Epoch: 6| Step: 10
Training loss: 2.1054434776306152
Validation loss: 2.0419828494389853

Epoch: 6| Step: 11
Training loss: 2.2256202697753906
Validation loss: 2.0489418307940164

Epoch: 6| Step: 12
Training loss: 2.3147454261779785
Validation loss: 2.048522671063741

Epoch: 6| Step: 13
Training loss: 2.2993581295013428
Validation loss: 2.0359403689702353

Epoch: 123| Step: 0
Training loss: 2.2193844318389893
Validation loss: 2.0480935176213584

Epoch: 6| Step: 1
Training loss: 2.4592385292053223
Validation loss: 2.034276862939199

Epoch: 6| Step: 2
Training loss: 2.2140140533447266
Validation loss: 2.0404755075772605

Epoch: 6| Step: 3
Training loss: 2.208698272705078
Validation loss: 2.0210934480031333

Epoch: 6| Step: 4
Training loss: 1.577925443649292
Validation loss: 2.027825196584066

Epoch: 6| Step: 5
Training loss: 1.6469521522521973
Validation loss: 2.0280437668164573

Epoch: 6| Step: 6
Training loss: 2.0836775302886963
Validation loss: 2.035074512163798

Epoch: 6| Step: 7
Training loss: 2.562656879425049
Validation loss: 2.0309508244196572

Epoch: 6| Step: 8
Training loss: 1.7108354568481445
Validation loss: 2.031984329223633

Epoch: 6| Step: 9
Training loss: 1.7942665815353394
Validation loss: 2.0220099091529846

Epoch: 6| Step: 10
Training loss: 2.214012622833252
Validation loss: 2.0344954133033752

Epoch: 6| Step: 11
Training loss: 1.7957745790481567
Validation loss: 2.0283969044685364

Epoch: 6| Step: 12
Training loss: 2.459954023361206
Validation loss: 2.044173260529836

Epoch: 6| Step: 13
Training loss: 2.0792646408081055
Validation loss: 2.050622363885244

Epoch: 124| Step: 0
Training loss: 1.9714596271514893
Validation loss: 2.0444921056429544

Epoch: 6| Step: 1
Training loss: 2.551360845565796
Validation loss: 2.0439762075742087

Epoch: 6| Step: 2
Training loss: 2.6186630725860596
Validation loss: 2.041010002295176

Epoch: 6| Step: 3
Training loss: 1.8608049154281616
Validation loss: 2.043713927268982

Epoch: 6| Step: 4
Training loss: 2.188896417617798
Validation loss: 2.0415125091870627

Epoch: 6| Step: 5
Training loss: 2.115830898284912
Validation loss: 2.0452168385187783

Epoch: 6| Step: 6
Training loss: 1.6583311557769775
Validation loss: 2.0367801984151206

Epoch: 6| Step: 7
Training loss: 1.8503549098968506
Validation loss: 2.029710034529368

Epoch: 6| Step: 8
Training loss: 1.9278485774993896
Validation loss: 2.0374231735865274

Epoch: 6| Step: 9
Training loss: 2.5195183753967285
Validation loss: 2.025485078493754

Epoch: 6| Step: 10
Training loss: 1.5975112915039062
Validation loss: 2.0245564778645835

Epoch: 6| Step: 11
Training loss: 1.295602798461914
Validation loss: 2.0360347429911294

Epoch: 6| Step: 12
Training loss: 2.985898971557617
Validation loss: 2.021185835202535

Epoch: 6| Step: 13
Training loss: 1.8728145360946655
Validation loss: 2.0197450518608093

Epoch: 125| Step: 0
Training loss: 2.3235254287719727
Validation loss: 2.041806956132253

Epoch: 6| Step: 1
Training loss: 1.9454412460327148
Validation loss: 2.0450764695803323

Epoch: 6| Step: 2
Training loss: 1.970397710800171
Validation loss: 2.0381524562835693

Epoch: 6| Step: 3
Training loss: 2.343153715133667
Validation loss: 2.0411297082901

Epoch: 6| Step: 4
Training loss: 1.9236221313476562
Validation loss: 2.037940045197805

Epoch: 6| Step: 5
Training loss: 2.2053585052490234
Validation loss: 2.0479522744814553

Epoch: 6| Step: 6
Training loss: 1.732289433479309
Validation loss: 2.03586079676946

Epoch: 6| Step: 7
Training loss: 2.390774965286255
Validation loss: 2.0508985916773477

Epoch: 6| Step: 8
Training loss: 2.171048164367676
Validation loss: 2.0656062364578247

Epoch: 6| Step: 9
Training loss: 2.458543300628662
Validation loss: 2.0581258138020835

Epoch: 6| Step: 10
Training loss: 1.4511420726776123
Validation loss: 2.0472567876180015

Epoch: 6| Step: 11
Training loss: 1.5745776891708374
Validation loss: 2.0626258850097656

Epoch: 6| Step: 12
Training loss: 2.3138375282287598
Validation loss: 2.057605187098185

Epoch: 6| Step: 13
Training loss: 1.9692065715789795
Validation loss: 2.066377659638723

Epoch: 126| Step: 0
Training loss: 1.7977344989776611
Validation loss: 2.0666980147361755

Epoch: 6| Step: 1
Training loss: 2.055500030517578
Validation loss: 2.0664609471956887

Epoch: 6| Step: 2
Training loss: 2.1023945808410645
Validation loss: 2.0761255820592246

Epoch: 6| Step: 3
Training loss: 1.701689600944519
Validation loss: 2.077401419480642

Epoch: 6| Step: 4
Training loss: 2.569790840148926
Validation loss: 2.0758765935897827

Epoch: 6| Step: 5
Training loss: 1.7775065898895264
Validation loss: 2.0629342794418335

Epoch: 6| Step: 6
Training loss: 1.7039555311203003
Validation loss: 2.077779233455658

Epoch: 6| Step: 7
Training loss: 2.1721601486206055
Validation loss: 2.0521631638209024

Epoch: 6| Step: 8
Training loss: 1.9705102443695068
Validation loss: 2.0624708930651345

Epoch: 6| Step: 9
Training loss: 2.0341298580169678
Validation loss: 2.0636494557062783

Epoch: 6| Step: 10
Training loss: 2.7317943572998047
Validation loss: 2.0703733364741006

Epoch: 6| Step: 11
Training loss: 2.6213126182556152
Validation loss: 2.0763611793518066

Epoch: 6| Step: 12
Training loss: 2.2787342071533203
Validation loss: 2.074877361456553

Epoch: 6| Step: 13
Training loss: 1.2926642894744873
Validation loss: 2.060695985953013

Epoch: 127| Step: 0
Training loss: 1.875688076019287
Validation loss: 2.0583911339441934

Epoch: 6| Step: 1
Training loss: 2.4788684844970703
Validation loss: 2.0530047019322715

Epoch: 6| Step: 2
Training loss: 2.3430869579315186
Validation loss: 2.0588611364364624

Epoch: 6| Step: 3
Training loss: 1.8352258205413818
Validation loss: 2.041090269883474

Epoch: 6| Step: 4
Training loss: 2.5507946014404297
Validation loss: 2.0499130884806314

Epoch: 6| Step: 5
Training loss: 1.3864575624465942
Validation loss: 2.0538095831871033

Epoch: 6| Step: 6
Training loss: 1.567110538482666
Validation loss: 2.058800995349884

Epoch: 6| Step: 7
Training loss: 1.41531503200531
Validation loss: 2.0643038551012673

Epoch: 6| Step: 8
Training loss: 2.658456802368164
Validation loss: 2.077854812145233

Epoch: 6| Step: 9
Training loss: 1.6683166027069092
Validation loss: 2.0865930716196694

Epoch: 6| Step: 10
Training loss: 2.038547992706299
Validation loss: 2.104638695716858

Epoch: 6| Step: 11
Training loss: 2.4256155490875244
Validation loss: 2.0966352820396423

Epoch: 6| Step: 12
Training loss: 2.8329787254333496
Validation loss: 2.0830487608909607

Epoch: 6| Step: 13
Training loss: 1.9659452438354492
Validation loss: 2.066190262635549

Epoch: 128| Step: 0
Training loss: 1.918845772743225
Validation loss: 2.0679694612820945

Epoch: 6| Step: 1
Training loss: 2.2404513359069824
Validation loss: 2.070082445939382

Epoch: 6| Step: 2
Training loss: 2.015200138092041
Validation loss: 2.0620638529459634

Epoch: 6| Step: 3
Training loss: 1.7177038192749023
Validation loss: 2.0458487470944724

Epoch: 6| Step: 4
Training loss: 2.2939651012420654
Validation loss: 2.0612085858980813

Epoch: 6| Step: 5
Training loss: 1.929090142250061
Validation loss: 2.0639532605806985

Epoch: 6| Step: 6
Training loss: 2.1193315982818604
Validation loss: 2.069210628668467

Epoch: 6| Step: 7
Training loss: 1.5548088550567627
Validation loss: 2.0747116605440774

Epoch: 6| Step: 8
Training loss: 2.423428535461426
Validation loss: 2.0723478396733603

Epoch: 6| Step: 9
Training loss: 2.696458339691162
Validation loss: 2.0818985303243003

Epoch: 6| Step: 10
Training loss: 2.862159013748169
Validation loss: 2.0672029654184976

Epoch: 6| Step: 11
Training loss: 1.4144766330718994
Validation loss: 2.047205070654551

Epoch: 6| Step: 12
Training loss: 1.5178066492080688
Validation loss: 2.0451483925183616

Epoch: 6| Step: 13
Training loss: 2.246415615081787
Validation loss: 2.0425893863042197

Epoch: 129| Step: 0
Training loss: 2.0691189765930176
Validation loss: 2.0437453587849936

Epoch: 6| Step: 1
Training loss: 2.2109451293945312
Validation loss: 2.034380038579305

Epoch: 6| Step: 2
Training loss: 2.412208080291748
Validation loss: 2.0328880548477173

Epoch: 6| Step: 3
Training loss: 2.264615535736084
Validation loss: 2.0366605520248413

Epoch: 6| Step: 4
Training loss: 1.4902808666229248
Validation loss: 2.0413092970848083

Epoch: 6| Step: 5
Training loss: 2.084139823913574
Validation loss: 2.029479463895162

Epoch: 6| Step: 6
Training loss: 1.907659888267517
Validation loss: 2.0384357372919717

Epoch: 6| Step: 7
Training loss: 2.2849836349487305
Validation loss: 2.0418537259101868

Epoch: 6| Step: 8
Training loss: 2.3499643802642822
Validation loss: 2.0345850785573325

Epoch: 6| Step: 9
Training loss: 1.7723215818405151
Validation loss: 2.0316060384114585

Epoch: 6| Step: 10
Training loss: 2.5714263916015625
Validation loss: 2.0280495087305703

Epoch: 6| Step: 11
Training loss: 2.612590789794922
Validation loss: 2.036932587623596

Epoch: 6| Step: 12
Training loss: 1.6618233919143677
Validation loss: 2.02411154905955

Epoch: 6| Step: 13
Training loss: 1.9246755838394165
Validation loss: 2.0321017702420554

Epoch: 130| Step: 0
Training loss: 1.671045184135437
Validation loss: 2.0313618183135986

Epoch: 6| Step: 1
Training loss: 2.1721043586730957
Validation loss: 2.0298941135406494

Epoch: 6| Step: 2
Training loss: 2.585171699523926
Validation loss: 2.049198031425476

Epoch: 6| Step: 3
Training loss: 1.8990644216537476
Validation loss: 2.0372620820999146

Epoch: 6| Step: 4
Training loss: 2.0736541748046875
Validation loss: 2.0596087177594504

Epoch: 6| Step: 5
Training loss: 1.9480589628219604
Validation loss: 2.052577714125315

Epoch: 6| Step: 6
Training loss: 2.14005184173584
Validation loss: 2.0474432508150735

Epoch: 6| Step: 7
Training loss: 2.252575397491455
Validation loss: 2.044872840245565

Epoch: 6| Step: 8
Training loss: 1.4364807605743408
Validation loss: 2.042967140674591

Epoch: 6| Step: 9
Training loss: 2.2381539344787598
Validation loss: 2.0540371537208557

Epoch: 6| Step: 10
Training loss: 1.8884631395339966
Validation loss: 2.039001206556956

Epoch: 6| Step: 11
Training loss: 2.3520989418029785
Validation loss: 2.0597365498542786

Epoch: 6| Step: 12
Training loss: 1.9510694742202759
Validation loss: 2.0595279137293496

Epoch: 6| Step: 13
Training loss: 2.3108270168304443
Validation loss: 2.0614793499310813

Epoch: 131| Step: 0
Training loss: 2.209167718887329
Validation loss: 2.0599875847498574

Epoch: 6| Step: 1
Training loss: 2.291375160217285
Validation loss: 2.0526798963546753

Epoch: 6| Step: 2
Training loss: 2.4066414833068848
Validation loss: 2.045848230520884

Epoch: 6| Step: 3
Training loss: 2.613060235977173
Validation loss: 2.0525707801183066

Epoch: 6| Step: 4
Training loss: 1.8130762577056885
Validation loss: 2.0490104953447976

Epoch: 6| Step: 5
Training loss: 2.128659725189209
Validation loss: 2.038561205069224

Epoch: 6| Step: 6
Training loss: 1.8866772651672363
Validation loss: 2.0348256627718606

Epoch: 6| Step: 7
Training loss: 1.737724781036377
Validation loss: 2.030258814493815

Epoch: 6| Step: 8
Training loss: 1.5334961414337158
Validation loss: 2.0441673398017883

Epoch: 6| Step: 9
Training loss: 1.9770668745040894
Validation loss: 2.0563350319862366

Epoch: 6| Step: 10
Training loss: 1.6906386613845825
Validation loss: 2.0595543384552

Epoch: 6| Step: 11
Training loss: 2.987720489501953
Validation loss: 2.0497081875801086

Epoch: 6| Step: 12
Training loss: 1.5631226301193237
Validation loss: 2.064900795618693

Epoch: 6| Step: 13
Training loss: 1.9279922246932983
Validation loss: 2.0524600545565286

Epoch: 132| Step: 0
Training loss: 1.3360681533813477
Validation loss: 2.062336524327596

Epoch: 6| Step: 1
Training loss: 1.966721534729004
Validation loss: 2.0606038173039756

Epoch: 6| Step: 2
Training loss: 2.34985613822937
Validation loss: 2.0796772241592407

Epoch: 6| Step: 3
Training loss: 1.7218124866485596
Validation loss: 2.072148601214091

Epoch: 6| Step: 4
Training loss: 1.84330415725708
Validation loss: 2.075060804684957

Epoch: 6| Step: 5
Training loss: 2.742121458053589
Validation loss: 2.069989581902822

Epoch: 6| Step: 6
Training loss: 2.103888511657715
Validation loss: 2.08996852238973

Epoch: 6| Step: 7
Training loss: 1.9679503440856934
Validation loss: 2.0765148798624673

Epoch: 6| Step: 8
Training loss: 1.7164719104766846
Validation loss: 2.0542252461115518

Epoch: 6| Step: 9
Training loss: 2.2679619789123535
Validation loss: 2.052218576272329

Epoch: 6| Step: 10
Training loss: 2.3304290771484375
Validation loss: 2.0535555680592856

Epoch: 6| Step: 11
Training loss: 1.8292219638824463
Validation loss: 2.0657395323117576

Epoch: 6| Step: 12
Training loss: 2.406034469604492
Validation loss: 2.066064695517222

Epoch: 6| Step: 13
Training loss: 1.9989914894104004
Validation loss: 2.0609583457310996

Epoch: 133| Step: 0
Training loss: 2.4959874153137207
Validation loss: 2.071248690287272

Epoch: 6| Step: 1
Training loss: 2.24957275390625
Validation loss: 2.068555474281311

Epoch: 6| Step: 2
Training loss: 2.026912212371826
Validation loss: 2.0697835882504783

Epoch: 6| Step: 3
Training loss: 1.1953411102294922
Validation loss: 2.068080266316732

Epoch: 6| Step: 4
Training loss: 2.2934653759002686
Validation loss: 2.0547459721565247

Epoch: 6| Step: 5
Training loss: 2.0066466331481934
Validation loss: 2.057659943898519

Epoch: 6| Step: 6
Training loss: 1.9009617567062378
Validation loss: 2.0631468296051025

Epoch: 6| Step: 7
Training loss: 1.6454449892044067
Validation loss: 2.0626335740089417

Epoch: 6| Step: 8
Training loss: 2.2426297664642334
Validation loss: 2.0619710286458335

Epoch: 6| Step: 9
Training loss: 1.7508361339569092
Validation loss: 2.050909777482351

Epoch: 6| Step: 10
Training loss: 1.85299551486969
Validation loss: 2.049889942010244

Epoch: 6| Step: 11
Training loss: 2.660881280899048
Validation loss: 2.0591891209284463

Epoch: 6| Step: 12
Training loss: 2.1520137786865234
Validation loss: 2.0586458245913186

Epoch: 6| Step: 13
Training loss: 1.9604270458221436
Validation loss: 2.052057365576426

Epoch: 134| Step: 0
Training loss: 2.023850440979004
Validation loss: 2.0519600907961526

Epoch: 6| Step: 1
Training loss: 2.274113178253174
Validation loss: 2.0501338243484497

Epoch: 6| Step: 2
Training loss: 2.4090099334716797
Validation loss: 2.0437729756037393

Epoch: 6| Step: 3
Training loss: 2.191713809967041
Validation loss: 2.043541411558787

Epoch: 6| Step: 4
Training loss: 1.7276560068130493
Validation loss: 2.0521228909492493

Epoch: 6| Step: 5
Training loss: 1.4869943857192993
Validation loss: 2.047429382801056

Epoch: 6| Step: 6
Training loss: 2.2891883850097656
Validation loss: 2.0505375464757285

Epoch: 6| Step: 7
Training loss: 1.7887336015701294
Validation loss: 2.056434392929077

Epoch: 6| Step: 8
Training loss: 1.811400055885315
Validation loss: 2.05748442808787

Epoch: 6| Step: 9
Training loss: 1.9231804609298706
Validation loss: 2.051909406979879

Epoch: 6| Step: 10
Training loss: 2.0565710067749023
Validation loss: 2.0684441328048706

Epoch: 6| Step: 11
Training loss: 2.587764024734497
Validation loss: 2.057510236899058

Epoch: 6| Step: 12
Training loss: 1.7153297662734985
Validation loss: 2.072137435277303

Epoch: 6| Step: 13
Training loss: 2.3103137016296387
Validation loss: 2.059212783972422

Epoch: 135| Step: 0
Training loss: 2.139495372772217
Validation loss: 2.0534369548161826

Epoch: 6| Step: 1
Training loss: 1.7500592470169067
Validation loss: 2.054064909617106

Epoch: 6| Step: 2
Training loss: 1.9586362838745117
Validation loss: 2.056647022565206

Epoch: 6| Step: 3
Training loss: 1.7277952432632446
Validation loss: 2.051202098528544

Epoch: 6| Step: 4
Training loss: 2.2127678394317627
Validation loss: 2.0442484418551126

Epoch: 6| Step: 5
Training loss: 1.9581971168518066
Validation loss: 2.0652644634246826

Epoch: 6| Step: 6
Training loss: 1.4475319385528564
Validation loss: 2.07329531510671

Epoch: 6| Step: 7
Training loss: 2.150714874267578
Validation loss: 2.0771997968355813

Epoch: 6| Step: 8
Training loss: 1.844103455543518
Validation loss: 2.0621002316474915

Epoch: 6| Step: 9
Training loss: 2.356797695159912
Validation loss: 2.0731016596158347

Epoch: 6| Step: 10
Training loss: 1.995739459991455
Validation loss: 2.083742062250773

Epoch: 6| Step: 11
Training loss: 1.873831868171692
Validation loss: 2.0728702346483865

Epoch: 6| Step: 12
Training loss: 2.5480973720550537
Validation loss: 2.072541832923889

Epoch: 6| Step: 13
Training loss: 2.58901309967041
Validation loss: 2.080643892288208

Epoch: 136| Step: 0
Training loss: 1.5855756998062134
Validation loss: 2.0565433502197266

Epoch: 6| Step: 1
Training loss: 2.0434255599975586
Validation loss: 2.0572849909464517

Epoch: 6| Step: 2
Training loss: 2.3464717864990234
Validation loss: 2.0564582149187722

Epoch: 6| Step: 3
Training loss: 1.9838486909866333
Validation loss: 2.045815964539846

Epoch: 6| Step: 4
Training loss: 2.2725963592529297
Validation loss: 2.046818733215332

Epoch: 6| Step: 5
Training loss: 1.4069194793701172
Validation loss: 2.046729882558187

Epoch: 6| Step: 6
Training loss: 2.533937692642212
Validation loss: 2.058143377304077

Epoch: 6| Step: 7
Training loss: 1.940245270729065
Validation loss: 2.032963593800863

Epoch: 6| Step: 8
Training loss: 1.4842652082443237
Validation loss: 2.0453752279281616

Epoch: 6| Step: 9
Training loss: 2.6500906944274902
Validation loss: 2.0404641230901084

Epoch: 6| Step: 10
Training loss: 2.251364231109619
Validation loss: 2.051434278488159

Epoch: 6| Step: 11
Training loss: 2.174304962158203
Validation loss: 2.0498746832211814

Epoch: 6| Step: 12
Training loss: 2.349473476409912
Validation loss: 2.052122473716736

Epoch: 6| Step: 13
Training loss: 1.5921976566314697
Validation loss: 2.0548465251922607

Epoch: 137| Step: 0
Training loss: 1.726889729499817
Validation loss: 2.0616076389948526

Epoch: 6| Step: 1
Training loss: 2.3673393726348877
Validation loss: 2.046192208925883

Epoch: 6| Step: 2
Training loss: 1.4893126487731934
Validation loss: 2.0557831128438315

Epoch: 6| Step: 3
Training loss: 1.860161542892456
Validation loss: 2.069543242454529

Epoch: 6| Step: 4
Training loss: 2.348238706588745
Validation loss: 2.0735342502593994

Epoch: 6| Step: 5
Training loss: 1.7722561359405518
Validation loss: 2.0837746063868203

Epoch: 6| Step: 6
Training loss: 1.4766875505447388
Validation loss: 2.071150620778402

Epoch: 6| Step: 7
Training loss: 2.193279266357422
Validation loss: 2.095130443572998

Epoch: 6| Step: 8
Training loss: 3.319310426712036
Validation loss: 2.1055694818496704

Epoch: 6| Step: 9
Training loss: 1.6574227809906006
Validation loss: 2.086533228556315

Epoch: 6| Step: 10
Training loss: 2.1604020595550537
Validation loss: 2.0867844422658286

Epoch: 6| Step: 11
Training loss: 2.2242984771728516
Validation loss: 2.0834315021832785

Epoch: 6| Step: 12
Training loss: 2.755227565765381
Validation loss: 2.07631649573644

Epoch: 6| Step: 13
Training loss: 1.3591797351837158
Validation loss: 2.063215176264445

Epoch: 138| Step: 0
Training loss: 1.8338696956634521
Validation loss: 2.0772719780604043

Epoch: 6| Step: 1
Training loss: 2.1188251972198486
Validation loss: 2.067140976587931

Epoch: 6| Step: 2
Training loss: 1.188101053237915
Validation loss: 2.0747201641400657

Epoch: 6| Step: 3
Training loss: 2.121220111846924
Validation loss: 2.064571479956309

Epoch: 6| Step: 4
Training loss: 1.4575672149658203
Validation loss: 2.078206996122996

Epoch: 6| Step: 5
Training loss: 2.331829071044922
Validation loss: 2.0800461769104004

Epoch: 6| Step: 6
Training loss: 2.186617374420166
Validation loss: 2.07389364639918

Epoch: 6| Step: 7
Training loss: 2.1368649005889893
Validation loss: 2.0772634545962014

Epoch: 6| Step: 8
Training loss: 2.4795312881469727
Validation loss: 2.0623794198036194

Epoch: 6| Step: 9
Training loss: 1.8739526271820068
Validation loss: 2.0619781215985618

Epoch: 6| Step: 10
Training loss: 2.6788861751556396
Validation loss: 2.057202180226644

Epoch: 6| Step: 11
Training loss: 2.327690601348877
Validation loss: 2.072209656238556

Epoch: 6| Step: 12
Training loss: 1.9684677124023438
Validation loss: 2.0603545705477395

Epoch: 6| Step: 13
Training loss: 2.2262356281280518
Validation loss: 2.0631781021753945

Epoch: 139| Step: 0
Training loss: 2.305530071258545
Validation loss: 2.0747044881184897

Epoch: 6| Step: 1
Training loss: 1.9281721115112305
Validation loss: 2.0576878786087036

Epoch: 6| Step: 2
Training loss: 2.207723617553711
Validation loss: 2.0694658954938254

Epoch: 6| Step: 3
Training loss: 2.3005776405334473
Validation loss: 2.063452959060669

Epoch: 6| Step: 4
Training loss: 2.3474700450897217
Validation loss: 2.0725231170654297

Epoch: 6| Step: 5
Training loss: 1.9998775720596313
Validation loss: 2.0957653721173606

Epoch: 6| Step: 6
Training loss: 1.7859840393066406
Validation loss: 2.0825221935908

Epoch: 6| Step: 7
Training loss: 1.8821239471435547
Validation loss: 2.093724568684896

Epoch: 6| Step: 8
Training loss: 1.5987259149551392
Validation loss: 2.088148295879364

Epoch: 6| Step: 9
Training loss: 1.4664064645767212
Validation loss: 2.0889969070752463

Epoch: 6| Step: 10
Training loss: 2.385148048400879
Validation loss: 2.088689128557841

Epoch: 6| Step: 11
Training loss: 2.116034507751465
Validation loss: 2.0850814382235208

Epoch: 6| Step: 12
Training loss: 2.0478110313415527
Validation loss: 2.0720370411872864

Epoch: 6| Step: 13
Training loss: 2.029982328414917
Validation loss: 2.0822519858678183

Epoch: 140| Step: 0
Training loss: 1.1222110986709595
Validation loss: 2.0664199590682983

Epoch: 6| Step: 1
Training loss: 2.175353765487671
Validation loss: 2.0721569061279297

Epoch: 6| Step: 2
Training loss: 2.219709873199463
Validation loss: 2.0782106518745422

Epoch: 6| Step: 3
Training loss: 2.896827459335327
Validation loss: 2.07564514875412

Epoch: 6| Step: 4
Training loss: 2.016064167022705
Validation loss: 2.0761305888493857

Epoch: 6| Step: 5
Training loss: 1.989207148551941
Validation loss: 2.0777557690938315

Epoch: 6| Step: 6
Training loss: 2.0222396850585938
Validation loss: 2.06457511583964

Epoch: 6| Step: 7
Training loss: 1.5543524026870728
Validation loss: 2.068955898284912

Epoch: 6| Step: 8
Training loss: 1.4829397201538086
Validation loss: 2.0696755250295005

Epoch: 6| Step: 9
Training loss: 1.937533974647522
Validation loss: 2.0694793661435447

Epoch: 6| Step: 10
Training loss: 1.986940622329712
Validation loss: 2.081032931804657

Epoch: 6| Step: 11
Training loss: 2.513442039489746
Validation loss: 2.0857441425323486

Epoch: 6| Step: 12
Training loss: 1.8465808629989624
Validation loss: 2.0770874420801797

Epoch: 6| Step: 13
Training loss: 2.6169238090515137
Validation loss: 2.0754376451174417

Epoch: 141| Step: 0
Training loss: 1.970785140991211
Validation loss: 2.0733758211135864

Epoch: 6| Step: 1
Training loss: 1.6372957229614258
Validation loss: 2.065953493118286

Epoch: 6| Step: 2
Training loss: 0.8559831380844116
Validation loss: 2.0612991054852805

Epoch: 6| Step: 3
Training loss: 2.7342593669891357
Validation loss: 2.0655994613965354

Epoch: 6| Step: 4
Training loss: 1.7070927619934082
Validation loss: 2.0545602639516196

Epoch: 6| Step: 5
Training loss: 1.9511088132858276
Validation loss: 2.0596479376157126

Epoch: 6| Step: 6
Training loss: 1.5197982788085938
Validation loss: 2.0460562705993652

Epoch: 6| Step: 7
Training loss: 2.5782456398010254
Validation loss: 2.060347239176432

Epoch: 6| Step: 8
Training loss: 2.0382070541381836
Validation loss: 2.064944803714752

Epoch: 6| Step: 9
Training loss: 1.8741040229797363
Validation loss: 2.0621337493260703

Epoch: 6| Step: 10
Training loss: 2.658935546875
Validation loss: 2.0481642285982766

Epoch: 6| Step: 11
Training loss: 2.2686734199523926
Validation loss: 2.0559447209040322

Epoch: 6| Step: 12
Training loss: 1.8865611553192139
Validation loss: 2.0743807752927146

Epoch: 6| Step: 13
Training loss: 2.646310329437256
Validation loss: 2.0686175425847373

Epoch: 142| Step: 0
Training loss: 2.124258518218994
Validation loss: 2.05797415971756

Epoch: 6| Step: 1
Training loss: 1.7963742017745972
Validation loss: 2.0665125846862793

Epoch: 6| Step: 2
Training loss: 1.5285561084747314
Validation loss: 2.060987333456675

Epoch: 6| Step: 3
Training loss: 2.315244436264038
Validation loss: 2.0705882708231607

Epoch: 6| Step: 4
Training loss: 1.7297855615615845
Validation loss: 2.072532673676809

Epoch: 6| Step: 5
Training loss: 2.4338278770446777
Validation loss: 2.053725838661194

Epoch: 6| Step: 6
Training loss: 1.649343490600586
Validation loss: 2.0617716709772744

Epoch: 6| Step: 7
Training loss: 1.9814612865447998
Validation loss: 2.0620075464248657

Epoch: 6| Step: 8
Training loss: 1.892685055732727
Validation loss: 2.0654356877009072

Epoch: 6| Step: 9
Training loss: 2.159000873565674
Validation loss: 2.0582677721977234

Epoch: 6| Step: 10
Training loss: 2.1453170776367188
Validation loss: 2.0628541509310403

Epoch: 6| Step: 11
Training loss: 2.3013250827789307
Validation loss: 2.0575814843177795

Epoch: 6| Step: 12
Training loss: 1.9751890897750854
Validation loss: 2.0726837317148843

Epoch: 6| Step: 13
Training loss: 2.323164701461792
Validation loss: 2.0498945116996765

Epoch: 143| Step: 0
Training loss: 2.146754503250122
Validation loss: 2.0598219434420266

Epoch: 6| Step: 1
Training loss: 2.1313233375549316
Validation loss: 2.064290006955465

Epoch: 6| Step: 2
Training loss: 2.23732328414917
Validation loss: 2.0822667876879373

Epoch: 6| Step: 3
Training loss: 2.073888063430786
Validation loss: 2.076799472173055

Epoch: 6| Step: 4
Training loss: 1.9567209482192993
Validation loss: 2.0726691683133445

Epoch: 6| Step: 5
Training loss: 1.942275047302246
Validation loss: 2.0664135615030923

Epoch: 6| Step: 6
Training loss: 2.0859127044677734
Validation loss: 2.0712706049283347

Epoch: 6| Step: 7
Training loss: 2.395047187805176
Validation loss: 2.0686680674552917

Epoch: 6| Step: 8
Training loss: 1.9429374933242798
Validation loss: 2.0754770239194236

Epoch: 6| Step: 9
Training loss: 2.0081844329833984
Validation loss: 2.065079172452291

Epoch: 6| Step: 10
Training loss: 1.78444242477417
Validation loss: 2.058710793654124

Epoch: 6| Step: 11
Training loss: 1.9124727249145508
Validation loss: 2.0709123611450195

Epoch: 6| Step: 12
Training loss: 1.698380470275879
Validation loss: 2.0645051995913186

Epoch: 6| Step: 13
Training loss: 1.937190294265747
Validation loss: 2.06271630525589

Epoch: 144| Step: 0
Training loss: 1.815368890762329
Validation loss: 2.054666300614675

Epoch: 6| Step: 1
Training loss: 1.7454445362091064
Validation loss: 2.0616235534350076

Epoch: 6| Step: 2
Training loss: 1.9244163036346436
Validation loss: 2.068241993586222

Epoch: 6| Step: 3
Training loss: 1.6267600059509277
Validation loss: 2.069685141245524

Epoch: 6| Step: 4
Training loss: 1.7608084678649902
Validation loss: 2.0866681337356567

Epoch: 6| Step: 5
Training loss: 1.4736772775650024
Validation loss: 2.100296119848887

Epoch: 6| Step: 6
Training loss: 2.8987483978271484
Validation loss: 2.106614271799723

Epoch: 6| Step: 7
Training loss: 1.5686285495758057
Validation loss: 2.1230973998705545

Epoch: 6| Step: 8
Training loss: 2.4016528129577637
Validation loss: 2.107330540815989

Epoch: 6| Step: 9
Training loss: 1.9692058563232422
Validation loss: 2.1045213540395102

Epoch: 6| Step: 10
Training loss: 2.1341238021850586
Validation loss: 2.1081329584121704

Epoch: 6| Step: 11
Training loss: 2.5907979011535645
Validation loss: 2.096527099609375

Epoch: 6| Step: 12
Training loss: 1.9952242374420166
Validation loss: 2.095768451690674

Epoch: 6| Step: 13
Training loss: 2.7591552734375
Validation loss: 2.0825648506482444

Epoch: 145| Step: 0
Training loss: 1.8794386386871338
Validation loss: 2.078700840473175

Epoch: 6| Step: 1
Training loss: 1.980440378189087
Validation loss: 2.0749250451723733

Epoch: 6| Step: 2
Training loss: 1.6614679098129272
Validation loss: 2.083082298437754

Epoch: 6| Step: 3
Training loss: 1.7814971208572388
Validation loss: 2.076439360777537

Epoch: 6| Step: 4
Training loss: 1.573058843612671
Validation loss: 2.064115126927694

Epoch: 6| Step: 5
Training loss: 2.7165095806121826
Validation loss: 2.066970646381378

Epoch: 6| Step: 6
Training loss: 2.0886473655700684
Validation loss: 2.0721261699994407

Epoch: 6| Step: 7
Training loss: 2.330369710922241
Validation loss: 2.079792300860087

Epoch: 6| Step: 8
Training loss: 1.6667160987854004
Validation loss: 2.0794853369394937

Epoch: 6| Step: 9
Training loss: 2.12667179107666
Validation loss: 2.0900814731915793

Epoch: 6| Step: 10
Training loss: 1.907360553741455
Validation loss: 2.0891002813975015

Epoch: 6| Step: 11
Training loss: 2.3076171875
Validation loss: 2.0874367554982505

Epoch: 6| Step: 12
Training loss: 2.067925214767456
Validation loss: 2.0828421910603843

Epoch: 6| Step: 13
Training loss: 2.1559205055236816
Validation loss: 2.070583462715149

Epoch: 146| Step: 0
Training loss: 1.9471263885498047
Validation loss: 2.0888838171958923

Epoch: 6| Step: 1
Training loss: 2.471804618835449
Validation loss: 2.07394806543986

Epoch: 6| Step: 2
Training loss: 2.3064498901367188
Validation loss: 2.0765305757522583

Epoch: 6| Step: 3
Training loss: 1.736879825592041
Validation loss: 2.0617469350496926

Epoch: 6| Step: 4
Training loss: 1.8642570972442627
Validation loss: 2.0684218207995095

Epoch: 6| Step: 5
Training loss: 2.248638391494751
Validation loss: 2.0694408218065896

Epoch: 6| Step: 6
Training loss: 2.2779810428619385
Validation loss: 2.080406685670217

Epoch: 6| Step: 7
Training loss: 2.0012552738189697
Validation loss: 2.0629891554514566

Epoch: 6| Step: 8
Training loss: 1.713857650756836
Validation loss: 2.0631097555160522

Epoch: 6| Step: 9
Training loss: 1.602343201637268
Validation loss: 2.0787131985028586

Epoch: 6| Step: 10
Training loss: 2.1608195304870605
Validation loss: 2.0773247281710305

Epoch: 6| Step: 11
Training loss: 1.7188174724578857
Validation loss: 2.076279123624166

Epoch: 6| Step: 12
Training loss: 1.661290168762207
Validation loss: 2.0700976649920144

Epoch: 6| Step: 13
Training loss: 2.252932071685791
Validation loss: 2.068859040737152

Epoch: 147| Step: 0
Training loss: 1.8354713916778564
Validation loss: 2.075894216696421

Epoch: 6| Step: 1
Training loss: 2.0658626556396484
Validation loss: 2.0899894634882608

Epoch: 6| Step: 2
Training loss: 2.14133358001709
Validation loss: 2.076420843601227

Epoch: 6| Step: 3
Training loss: 2.177701711654663
Validation loss: 2.0882253448168435

Epoch: 6| Step: 4
Training loss: 2.716466188430786
Validation loss: 2.077656328678131

Epoch: 6| Step: 5
Training loss: 1.7472254037857056
Validation loss: 2.0864274899164834

Epoch: 6| Step: 6
Training loss: 2.1284565925598145
Validation loss: 2.0722921888033548

Epoch: 6| Step: 7
Training loss: 1.7597987651824951
Validation loss: 2.0562130411465964

Epoch: 6| Step: 8
Training loss: 2.3139686584472656
Validation loss: 2.05335396528244

Epoch: 6| Step: 9
Training loss: 2.3997812271118164
Validation loss: 2.0463576714197793

Epoch: 6| Step: 10
Training loss: 1.707278847694397
Validation loss: 2.055139640967051

Epoch: 6| Step: 11
Training loss: 2.4211764335632324
Validation loss: 2.0565709670384726

Epoch: 6| Step: 12
Training loss: 2.039457321166992
Validation loss: 2.0478492379188538

Epoch: 6| Step: 13
Training loss: 1.6379669904708862
Validation loss: 2.0546685059865317

Epoch: 148| Step: 0
Training loss: 2.1534969806671143
Validation loss: 2.062719742457072

Epoch: 6| Step: 1
Training loss: 1.7404897212982178
Validation loss: 2.0626004536946616

Epoch: 6| Step: 2
Training loss: 2.1030924320220947
Validation loss: 2.0551472306251526

Epoch: 6| Step: 3
Training loss: 2.2011170387268066
Validation loss: 2.0752546985944114

Epoch: 6| Step: 4
Training loss: 2.274799108505249
Validation loss: 2.0650731722513833

Epoch: 6| Step: 5
Training loss: 2.2717673778533936
Validation loss: 2.0734864473342896

Epoch: 6| Step: 6
Training loss: 2.2702083587646484
Validation loss: 2.089496692021688

Epoch: 6| Step: 7
Training loss: 2.4664015769958496
Validation loss: 2.0841126640637717

Epoch: 6| Step: 8
Training loss: 1.391879677772522
Validation loss: 2.0893551309903464

Epoch: 6| Step: 9
Training loss: 1.4938099384307861
Validation loss: 2.0990309913953147

Epoch: 6| Step: 10
Training loss: 2.1214585304260254
Validation loss: 2.1047780513763428

Epoch: 6| Step: 11
Training loss: 1.7882895469665527
Validation loss: 2.097177803516388

Epoch: 6| Step: 12
Training loss: 1.9270025491714478
Validation loss: 2.092180152734121

Epoch: 6| Step: 13
Training loss: 1.9359898567199707
Validation loss: 2.091304620107015

Epoch: 149| Step: 0
Training loss: 1.9293924570083618
Validation loss: 2.0910590092341104

Epoch: 6| Step: 1
Training loss: 2.0679781436920166
Validation loss: 2.091159721215566

Epoch: 6| Step: 2
Training loss: 2.048985481262207
Validation loss: 2.0832082827885947

Epoch: 6| Step: 3
Training loss: 1.9724920988082886
Validation loss: 2.0773309071858725

Epoch: 6| Step: 4
Training loss: 1.8349134922027588
Validation loss: 2.0921175877253213

Epoch: 6| Step: 5
Training loss: 1.5584744215011597
Validation loss: 2.0731411576271057

Epoch: 6| Step: 6
Training loss: 1.5686132907867432
Validation loss: 2.0843069553375244

Epoch: 6| Step: 7
Training loss: 1.8259395360946655
Validation loss: 2.075032949447632

Epoch: 6| Step: 8
Training loss: 2.824981689453125
Validation loss: 2.0841378768285117

Epoch: 6| Step: 9
Training loss: 2.1718974113464355
Validation loss: 2.079943597316742

Epoch: 6| Step: 10
Training loss: 2.3234877586364746
Validation loss: 2.095455447832743

Epoch: 6| Step: 11
Training loss: 1.2066582441329956
Validation loss: 2.092079222202301

Epoch: 6| Step: 12
Training loss: 2.1206631660461426
Validation loss: 2.0799267490704856

Epoch: 6| Step: 13
Training loss: 2.5447998046875
Validation loss: 2.0733105341593423

Epoch: 150| Step: 0
Training loss: 2.1154990196228027
Validation loss: 2.063801924387614

Epoch: 6| Step: 1
Training loss: 1.7097175121307373
Validation loss: 2.078235407670339

Epoch: 6| Step: 2
Training loss: 1.9448275566101074
Validation loss: 2.0739940404891968

Epoch: 6| Step: 3
Training loss: 1.7950327396392822
Validation loss: 2.0814483761787415

Epoch: 6| Step: 4
Training loss: 2.2249691486358643
Validation loss: 2.071249087651571

Epoch: 6| Step: 5
Training loss: 2.1469919681549072
Validation loss: 2.073316514492035

Epoch: 6| Step: 6
Training loss: 2.164503574371338
Validation loss: 2.078317085901896

Epoch: 6| Step: 7
Training loss: 2.5707879066467285
Validation loss: 2.0817942023277283

Epoch: 6| Step: 8
Training loss: 1.1192419528961182
Validation loss: 2.0809330344200134

Epoch: 6| Step: 9
Training loss: 2.049222707748413
Validation loss: 2.08579550186793

Epoch: 6| Step: 10
Training loss: 2.251833200454712
Validation loss: 2.096317410469055

Epoch: 6| Step: 11
Training loss: 2.256373167037964
Validation loss: 2.082283536593119

Epoch: 6| Step: 12
Training loss: 2.4903042316436768
Validation loss: 2.080394983291626

Epoch: 6| Step: 13
Training loss: 1.2225099802017212
Validation loss: 2.082434276739756

Epoch: 151| Step: 0
Training loss: 2.411468029022217
Validation loss: 2.0912668903668723

Epoch: 6| Step: 1
Training loss: 1.2944819927215576
Validation loss: 2.083721657594045

Epoch: 6| Step: 2
Training loss: 1.8208638429641724
Validation loss: 2.0689539511998496

Epoch: 6| Step: 3
Training loss: 2.3742785453796387
Validation loss: 2.079590400060018

Epoch: 6| Step: 4
Training loss: 2.1699371337890625
Validation loss: 2.0773747762044272

Epoch: 6| Step: 5
Training loss: 1.2924554347991943
Validation loss: 2.071246107419332

Epoch: 6| Step: 6
Training loss: 1.850728988647461
Validation loss: 2.085162858168284

Epoch: 6| Step: 7
Training loss: 2.1005046367645264
Validation loss: 2.0609717766443887

Epoch: 6| Step: 8
Training loss: 2.405200958251953
Validation loss: 2.080333630243937

Epoch: 6| Step: 9
Training loss: 1.7259712219238281
Validation loss: 2.0843106309572854

Epoch: 6| Step: 10
Training loss: 1.8993251323699951
Validation loss: 2.091742237408956

Epoch: 6| Step: 11
Training loss: 2.1393635272979736
Validation loss: 2.104999542236328

Epoch: 6| Step: 12
Training loss: 1.7151795625686646
Validation loss: 2.0978907148043313

Epoch: 6| Step: 13
Training loss: 2.6097607612609863
Validation loss: 2.09508208433787

Epoch: 152| Step: 0
Training loss: 2.243039131164551
Validation loss: 2.0993106762568154

Epoch: 6| Step: 1
Training loss: 1.7473969459533691
Validation loss: 2.085373024145762

Epoch: 6| Step: 2
Training loss: 2.371598243713379
Validation loss: 2.0778345266977944

Epoch: 6| Step: 3
Training loss: 2.1904687881469727
Validation loss: 2.0793546040852866

Epoch: 6| Step: 4
Training loss: 1.8811746835708618
Validation loss: 2.0740113854408264

Epoch: 6| Step: 5
Training loss: 1.9483476877212524
Validation loss: 2.0635056694348655

Epoch: 6| Step: 6
Training loss: 2.108711004257202
Validation loss: 2.063812712828318

Epoch: 6| Step: 7
Training loss: 1.9229580163955688
Validation loss: 2.0660711526870728

Epoch: 6| Step: 8
Training loss: 1.9404489994049072
Validation loss: 2.0676339864730835

Epoch: 6| Step: 9
Training loss: 1.996969223022461
Validation loss: 2.0629387299219766

Epoch: 6| Step: 10
Training loss: 1.959549903869629
Validation loss: 2.069766382376353

Epoch: 6| Step: 11
Training loss: 1.8122352361679077
Validation loss: 2.056494494279226

Epoch: 6| Step: 12
Training loss: 1.9057203531265259
Validation loss: 2.053303360939026

Epoch: 6| Step: 13
Training loss: 2.0721631050109863
Validation loss: 2.058036466439565

Epoch: 153| Step: 0
Training loss: 2.5418081283569336
Validation loss: 2.0590308904647827

Epoch: 6| Step: 1
Training loss: 1.966321587562561
Validation loss: 2.0519147912661233

Epoch: 6| Step: 2
Training loss: 2.175804615020752
Validation loss: 2.062861363093058

Epoch: 6| Step: 3
Training loss: 2.7533516883850098
Validation loss: 2.0554911295572915

Epoch: 6| Step: 4
Training loss: 2.176091194152832
Validation loss: 2.0707512299219766

Epoch: 6| Step: 5
Training loss: 1.2848628759384155
Validation loss: 2.0749518672625222

Epoch: 6| Step: 6
Training loss: 2.0657811164855957
Validation loss: 2.0693050622940063

Epoch: 6| Step: 7
Training loss: 2.1126708984375
Validation loss: 2.0679001609484353

Epoch: 6| Step: 8
Training loss: 2.1226842403411865
Validation loss: 2.0860569874445596

Epoch: 6| Step: 9
Training loss: 1.3885828256607056
Validation loss: 2.0799689292907715

Epoch: 6| Step: 10
Training loss: 1.4661555290222168
Validation loss: 2.0799490014712014

Epoch: 6| Step: 11
Training loss: 2.1531786918640137
Validation loss: 2.0750224590301514

Epoch: 6| Step: 12
Training loss: 2.0774521827697754
Validation loss: 2.0742197831471763

Epoch: 6| Step: 13
Training loss: 2.0123395919799805
Validation loss: 2.105635126431783

Epoch: 154| Step: 0
Training loss: 1.650336503982544
Validation loss: 2.100613534450531

Epoch: 6| Step: 1
Training loss: 1.5816117525100708
Validation loss: 2.0879605213801065

Epoch: 6| Step: 2
Training loss: 2.337777853012085
Validation loss: 2.1003921826680503

Epoch: 6| Step: 3
Training loss: 1.5046279430389404
Validation loss: 2.0914724469184875

Epoch: 6| Step: 4
Training loss: 1.9381449222564697
Validation loss: 2.0967167615890503

Epoch: 6| Step: 5
Training loss: 2.3926305770874023
Validation loss: 2.0907464226086936

Epoch: 6| Step: 6
Training loss: 1.1941306591033936
Validation loss: 2.0936731696128845

Epoch: 6| Step: 7
Training loss: 2.7126457691192627
Validation loss: 2.085518022378286

Epoch: 6| Step: 8
Training loss: 2.6140708923339844
Validation loss: 2.077787697315216

Epoch: 6| Step: 9
Training loss: 1.8782137632369995
Validation loss: 2.0768580039342246

Epoch: 6| Step: 10
Training loss: 1.6340842247009277
Validation loss: 2.0832122365633645

Epoch: 6| Step: 11
Training loss: 2.2771031856536865
Validation loss: 2.0865301489830017

Epoch: 6| Step: 12
Training loss: 1.8650171756744385
Validation loss: 2.097002545992533

Epoch: 6| Step: 13
Training loss: 2.573435068130493
Validation loss: 2.0942591627438865

Epoch: 155| Step: 0
Training loss: 1.9420311450958252
Validation loss: 2.082937717437744

Epoch: 6| Step: 1
Training loss: 1.4865522384643555
Validation loss: 2.0997078816095986

Epoch: 6| Step: 2
Training loss: 2.0423543453216553
Validation loss: 2.0849977930386863

Epoch: 6| Step: 3
Training loss: 2.877657413482666
Validation loss: 2.0800341367721558

Epoch: 6| Step: 4
Training loss: 2.364565849304199
Validation loss: 2.073408842086792

Epoch: 6| Step: 5
Training loss: 1.4375948905944824
Validation loss: 2.0678794384002686

Epoch: 6| Step: 6
Training loss: 1.7650644779205322
Validation loss: 2.0621222257614136

Epoch: 6| Step: 7
Training loss: 1.7654201984405518
Validation loss: 2.0651440620422363

Epoch: 6| Step: 8
Training loss: 2.0655553340911865
Validation loss: 2.0617233316103616

Epoch: 6| Step: 9
Training loss: 2.0377390384674072
Validation loss: 2.0739255944887796

Epoch: 6| Step: 10
Training loss: 1.9385192394256592
Validation loss: 2.066222290198008

Epoch: 6| Step: 11
Training loss: 2.6683454513549805
Validation loss: 2.07255091269811

Epoch: 6| Step: 12
Training loss: 2.0147316455841064
Validation loss: 2.070301353931427

Epoch: 6| Step: 13
Training loss: 1.7678782939910889
Validation loss: 2.086296876271566

Epoch: 156| Step: 0
Training loss: 2.3372864723205566
Validation loss: 2.0885053674379983

Epoch: 6| Step: 1
Training loss: 1.82222580909729
Validation loss: 2.0970576206843057

Epoch: 6| Step: 2
Training loss: 1.8553478717803955
Validation loss: 2.090952972571055

Epoch: 6| Step: 3
Training loss: 2.1105260848999023
Validation loss: 2.0897849798202515

Epoch: 6| Step: 4
Training loss: 1.5208301544189453
Validation loss: 2.0897435545921326

Epoch: 6| Step: 5
Training loss: 2.2234761714935303
Validation loss: 2.0881758530934653

Epoch: 6| Step: 6
Training loss: 1.8553142547607422
Validation loss: 2.0964057048161826

Epoch: 6| Step: 7
Training loss: 2.3176636695861816
Validation loss: 2.1143826444943747

Epoch: 6| Step: 8
Training loss: 1.3851345777511597
Validation loss: 2.1147472063700357

Epoch: 6| Step: 9
Training loss: 2.044348955154419
Validation loss: 2.099024514357249

Epoch: 6| Step: 10
Training loss: 1.5397132635116577
Validation loss: 2.1022342443466187

Epoch: 6| Step: 11
Training loss: 2.3643436431884766
Validation loss: 2.1198484301567078

Epoch: 6| Step: 12
Training loss: 2.1129984855651855
Validation loss: 2.095697561899821

Epoch: 6| Step: 13
Training loss: 2.566826343536377
Validation loss: 2.093593200047811

Epoch: 157| Step: 0
Training loss: 2.1219229698181152
Validation loss: 2.0954416394233704

Epoch: 6| Step: 1
Training loss: 2.0730717182159424
Validation loss: 2.093007485071818

Epoch: 6| Step: 2
Training loss: 2.0218935012817383
Validation loss: 2.0833993156751

Epoch: 6| Step: 3
Training loss: 2.239638566970825
Validation loss: 2.0749451915423074

Epoch: 6| Step: 4
Training loss: 2.460972785949707
Validation loss: 2.076652944087982

Epoch: 6| Step: 5
Training loss: 1.4168075323104858
Validation loss: 2.076370040575663

Epoch: 6| Step: 6
Training loss: 1.8345063924789429
Validation loss: 2.082921882470449

Epoch: 6| Step: 7
Training loss: 1.482464075088501
Validation loss: 2.0758315324783325

Epoch: 6| Step: 8
Training loss: 1.9809396266937256
Validation loss: 2.0748271544774375

Epoch: 6| Step: 9
Training loss: 1.8768243789672852
Validation loss: 2.0805001854896545

Epoch: 6| Step: 10
Training loss: 2.3911948204040527
Validation loss: 2.084876775741577

Epoch: 6| Step: 11
Training loss: 2.184798240661621
Validation loss: 2.0857190688451133

Epoch: 6| Step: 12
Training loss: 1.8536490201950073
Validation loss: 2.0726004242897034

Epoch: 6| Step: 13
Training loss: 2.1302108764648438
Validation loss: 2.0808130900065103

Epoch: 158| Step: 0
Training loss: 1.5145901441574097
Validation loss: 2.0940062006314597

Epoch: 6| Step: 1
Training loss: 2.092377185821533
Validation loss: 2.0843212008476257

Epoch: 6| Step: 2
Training loss: 1.7697932720184326
Validation loss: 2.0983049670855203

Epoch: 6| Step: 3
Training loss: 1.9485129117965698
Validation loss: 2.08199151357015

Epoch: 6| Step: 4
Training loss: 1.9204871654510498
Validation loss: 2.087774415810903

Epoch: 6| Step: 5
Training loss: 1.9912400245666504
Validation loss: 2.0792171160380044

Epoch: 6| Step: 6
Training loss: 2.5554075241088867
Validation loss: 2.075620452562968

Epoch: 6| Step: 7
Training loss: 1.8433442115783691
Validation loss: 2.078794280687968

Epoch: 6| Step: 8
Training loss: 2.2364141941070557
Validation loss: 2.0675066709518433

Epoch: 6| Step: 9
Training loss: 2.1960740089416504
Validation loss: 2.0659150878588357

Epoch: 6| Step: 10
Training loss: 1.8405988216400146
Validation loss: 2.0695056915283203

Epoch: 6| Step: 11
Training loss: 2.5985872745513916
Validation loss: 2.0761616031328836

Epoch: 6| Step: 12
Training loss: 1.5503350496292114
Validation loss: 2.061999758084615

Epoch: 6| Step: 13
Training loss: 1.9278829097747803
Validation loss: 2.0669058163960776

Epoch: 159| Step: 0
Training loss: 2.030766248703003
Validation loss: 2.059667448202769

Epoch: 6| Step: 1
Training loss: 2.107506275177002
Validation loss: 2.068035284678141

Epoch: 6| Step: 2
Training loss: 1.583434820175171
Validation loss: 2.075447122255961

Epoch: 6| Step: 3
Training loss: 2.108274459838867
Validation loss: 2.077050507068634

Epoch: 6| Step: 4
Training loss: 1.7530460357666016
Validation loss: 2.0765514771143594

Epoch: 6| Step: 5
Training loss: 2.2882399559020996
Validation loss: 2.083968828121821

Epoch: 6| Step: 6
Training loss: 2.465975522994995
Validation loss: 2.0996113419532776

Epoch: 6| Step: 7
Training loss: 1.756697177886963
Validation loss: 2.1078174312909446

Epoch: 6| Step: 8
Training loss: 1.4593455791473389
Validation loss: 2.109973112742106

Epoch: 6| Step: 9
Training loss: 2.3305368423461914
Validation loss: 2.1159090995788574

Epoch: 6| Step: 10
Training loss: 2.1928062438964844
Validation loss: 2.1181525389353433

Epoch: 6| Step: 11
Training loss: 2.330566167831421
Validation loss: 2.117059826850891

Epoch: 6| Step: 12
Training loss: 2.2405340671539307
Validation loss: 2.1086103916168213

Epoch: 6| Step: 13
Training loss: 1.3174641132354736
Validation loss: 2.103118042151133

Epoch: 160| Step: 0
Training loss: 2.2256743907928467
Validation loss: 2.1103960275650024

Epoch: 6| Step: 1
Training loss: 1.6337237358093262
Validation loss: 2.0982435941696167

Epoch: 6| Step: 2
Training loss: 1.7417171001434326
Validation loss: 2.079435626665751

Epoch: 6| Step: 3
Training loss: 2.068162679672241
Validation loss: 2.084040423234304

Epoch: 6| Step: 4
Training loss: 2.006734609603882
Validation loss: 2.0821345249811807

Epoch: 6| Step: 5
Training loss: 1.974217176437378
Validation loss: 2.070007562637329

Epoch: 6| Step: 6
Training loss: 2.429251194000244
Validation loss: 2.070246378580729

Epoch: 6| Step: 7
Training loss: 1.9503659009933472
Validation loss: 2.0656474828720093

Epoch: 6| Step: 8
Training loss: 2.250028133392334
Validation loss: 2.0646994511286416

Epoch: 6| Step: 9
Training loss: 1.7444146871566772
Validation loss: 2.055606206258138

Epoch: 6| Step: 10
Training loss: 2.078449249267578
Validation loss: 2.0748544732729592

Epoch: 6| Step: 11
Training loss: 2.0408284664154053
Validation loss: 2.050428807735443

Epoch: 6| Step: 12
Training loss: 1.9293746948242188
Validation loss: 2.0676721731821694

Epoch: 6| Step: 13
Training loss: 1.747702956199646
Validation loss: 2.073628862698873

Epoch: 161| Step: 0
Training loss: 1.615251064300537
Validation loss: 2.0838892658551535

Epoch: 6| Step: 1
Training loss: 2.3096089363098145
Validation loss: 2.081080754597982

Epoch: 6| Step: 2
Training loss: 2.270447254180908
Validation loss: 2.0790722767512

Epoch: 6| Step: 3
Training loss: 2.325960159301758
Validation loss: 2.081774036089579

Epoch: 6| Step: 4
Training loss: 1.932918667793274
Validation loss: 2.0899487733840942

Epoch: 6| Step: 5
Training loss: 1.9414573907852173
Validation loss: 2.0964906215667725

Epoch: 6| Step: 6
Training loss: 1.3871166706085205
Validation loss: 2.1097251375516257

Epoch: 6| Step: 7
Training loss: 1.6935495138168335
Validation loss: 2.101589322090149

Epoch: 6| Step: 8
Training loss: 2.0817065238952637
Validation loss: 2.104472060998281

Epoch: 6| Step: 9
Training loss: 2.5034098625183105
Validation loss: 2.0929572582244873

Epoch: 6| Step: 10
Training loss: 1.4095239639282227
Validation loss: 2.09218962987264

Epoch: 6| Step: 11
Training loss: 1.9387965202331543
Validation loss: 2.0725892583529153

Epoch: 6| Step: 12
Training loss: 2.840770721435547
Validation loss: 2.0839967330296836

Epoch: 6| Step: 13
Training loss: 1.7223926782608032
Validation loss: 2.0803060134251914

Epoch: 162| Step: 0
Training loss: 2.1151809692382812
Validation loss: 2.0742623011271157

Epoch: 6| Step: 1
Training loss: 2.090880870819092
Validation loss: 2.083000739415487

Epoch: 6| Step: 2
Training loss: 2.683753490447998
Validation loss: 2.0750847260157266

Epoch: 6| Step: 3
Training loss: 2.307497024536133
Validation loss: 2.0672783851623535

Epoch: 6| Step: 4
Training loss: 1.73390531539917
Validation loss: 2.0814767281214395

Epoch: 6| Step: 5
Training loss: 2.008166790008545
Validation loss: 2.0782059033711753

Epoch: 6| Step: 6
Training loss: 1.7824379205703735
Validation loss: 2.0891747077306113

Epoch: 6| Step: 7
Training loss: 1.7848443984985352
Validation loss: 2.082461496194204

Epoch: 6| Step: 8
Training loss: 1.4404546022415161
Validation loss: 2.076499899228414

Epoch: 6| Step: 9
Training loss: 2.3133928775787354
Validation loss: 2.078610142072042

Epoch: 6| Step: 10
Training loss: 1.7873789072036743
Validation loss: 2.0871461033821106

Epoch: 6| Step: 11
Training loss: 1.5088303089141846
Validation loss: 2.0766380627950034

Epoch: 6| Step: 12
Training loss: 1.9895888566970825
Validation loss: 2.091731866200765

Epoch: 6| Step: 13
Training loss: 2.373425245285034
Validation loss: 2.089249074459076

Epoch: 163| Step: 0
Training loss: 2.1553330421447754
Validation loss: 2.0924124320348105

Epoch: 6| Step: 1
Training loss: 2.1766469478607178
Validation loss: 2.099029024442037

Epoch: 6| Step: 2
Training loss: 1.939202070236206
Validation loss: 2.0879059433937073

Epoch: 6| Step: 3
Training loss: 1.6866941452026367
Validation loss: 2.0963883996009827

Epoch: 6| Step: 4
Training loss: 1.4273667335510254
Validation loss: 2.0919368267059326

Epoch: 6| Step: 5
Training loss: 2.3465051651000977
Validation loss: 2.088376800219218

Epoch: 6| Step: 6
Training loss: 1.668065071105957
Validation loss: 2.099750280380249

Epoch: 6| Step: 7
Training loss: 1.732771873474121
Validation loss: 2.1019248366355896

Epoch: 6| Step: 8
Training loss: 2.174898624420166
Validation loss: 2.105357050895691

Epoch: 6| Step: 9
Training loss: 1.675377607345581
Validation loss: 2.104104439417521

Epoch: 6| Step: 10
Training loss: 2.1913323402404785
Validation loss: 2.107083797454834

Epoch: 6| Step: 11
Training loss: 1.9430968761444092
Validation loss: 2.1085447867711387

Epoch: 6| Step: 12
Training loss: 2.1888532638549805
Validation loss: 2.103343407313029

Epoch: 6| Step: 13
Training loss: 2.259286403656006
Validation loss: 2.0920870701471963

Epoch: 164| Step: 0
Training loss: 1.5636019706726074
Validation loss: 2.1036102771759033

Epoch: 6| Step: 1
Training loss: 1.6172010898590088
Validation loss: 2.0920785665512085

Epoch: 6| Step: 2
Training loss: 2.110121965408325
Validation loss: 2.089590847492218

Epoch: 6| Step: 3
Training loss: 1.3099397420883179
Validation loss: 2.093532303969065

Epoch: 6| Step: 4
Training loss: 1.9455143213272095
Validation loss: 2.0977473855018616

Epoch: 6| Step: 5
Training loss: 2.0643064975738525
Validation loss: 2.0938977201779685

Epoch: 6| Step: 6
Training loss: 2.2823076248168945
Validation loss: 2.1015384594599404

Epoch: 6| Step: 7
Training loss: 1.7090275287628174
Validation loss: 2.101149300734202

Epoch: 6| Step: 8
Training loss: 2.140496015548706
Validation loss: 2.091049532095591

Epoch: 6| Step: 9
Training loss: 2.767294406890869
Validation loss: 2.0986997286478677

Epoch: 6| Step: 10
Training loss: 2.3448972702026367
Validation loss: 2.0843451023101807

Epoch: 6| Step: 11
Training loss: 2.739316463470459
Validation loss: 2.0884949962298074

Epoch: 6| Step: 12
Training loss: 1.526705026626587
Validation loss: 2.086616317431132

Epoch: 6| Step: 13
Training loss: 1.492751121520996
Validation loss: 2.09162966410319

Epoch: 165| Step: 0
Training loss: 2.2101876735687256
Validation loss: 2.098550776640574

Epoch: 6| Step: 1
Training loss: 1.8483941555023193
Validation loss: 2.0883384545644126

Epoch: 6| Step: 2
Training loss: 1.7173447608947754
Validation loss: 2.092310647169749

Epoch: 6| Step: 3
Training loss: 1.8962485790252686
Validation loss: 2.097026288509369

Epoch: 6| Step: 4
Training loss: 2.6715784072875977
Validation loss: 2.108149766921997

Epoch: 6| Step: 5
Training loss: 1.91184401512146
Validation loss: 2.108931243419647

Epoch: 6| Step: 6
Training loss: 2.132950782775879
Validation loss: 2.110428194204966

Epoch: 6| Step: 7
Training loss: 2.1371214389801025
Validation loss: 2.1126205722490945

Epoch: 6| Step: 8
Training loss: 1.6242412328720093
Validation loss: 2.107405662536621

Epoch: 6| Step: 9
Training loss: 1.9247705936431885
Validation loss: 2.0971782008806863

Epoch: 6| Step: 10
Training loss: 1.9113187789916992
Validation loss: 2.0891913970311484

Epoch: 6| Step: 11
Training loss: 1.4509377479553223
Validation loss: 2.089203874270121

Epoch: 6| Step: 12
Training loss: 2.3230533599853516
Validation loss: 2.0795406301816306

Epoch: 6| Step: 13
Training loss: 2.261847496032715
Validation loss: 2.083304842313131

Epoch: 166| Step: 0
Training loss: 2.005986213684082
Validation loss: 2.085785150527954

Epoch: 6| Step: 1
Training loss: 1.9657098054885864
Validation loss: 2.092232803503672

Epoch: 6| Step: 2
Training loss: 1.7991575002670288
Validation loss: 2.081992427508036

Epoch: 6| Step: 3
Training loss: 2.581737756729126
Validation loss: 2.0978048841158548

Epoch: 6| Step: 4
Training loss: 1.5782138109207153
Validation loss: 2.106821576754252

Epoch: 6| Step: 5
Training loss: 1.8286842107772827
Validation loss: 2.101312518119812

Epoch: 6| Step: 6
Training loss: 1.8316986560821533
Validation loss: 2.10564653078715

Epoch: 6| Step: 7
Training loss: 1.8326663970947266
Validation loss: 2.0853142539660134

Epoch: 6| Step: 8
Training loss: 1.5508043766021729
Validation loss: 2.091807246208191

Epoch: 6| Step: 9
Training loss: 1.7538012266159058
Validation loss: 2.0983964602152505

Epoch: 6| Step: 10
Training loss: 2.0736114978790283
Validation loss: 2.09309591849645

Epoch: 6| Step: 11
Training loss: 2.1672370433807373
Validation loss: 2.086811145146688

Epoch: 6| Step: 12
Training loss: 1.6007858514785767
Validation loss: 2.0902296702067056

Epoch: 6| Step: 13
Training loss: 3.172938823699951
Validation loss: 2.0965663393338523

Epoch: 167| Step: 0
Training loss: 2.343423843383789
Validation loss: 2.0931540528933206

Epoch: 6| Step: 1
Training loss: 2.2470786571502686
Validation loss: 2.091885288556417

Epoch: 6| Step: 2
Training loss: 2.271010637283325
Validation loss: 2.0946412682533264

Epoch: 6| Step: 3
Training loss: 1.59330415725708
Validation loss: 2.1103113492329917

Epoch: 6| Step: 4
Training loss: 1.3284499645233154
Validation loss: 2.108288129170736

Epoch: 6| Step: 5
Training loss: 1.8980871438980103
Validation loss: 2.101016342639923

Epoch: 6| Step: 6
Training loss: 1.8751063346862793
Validation loss: 2.104486624399821

Epoch: 6| Step: 7
Training loss: 1.9843902587890625
Validation loss: 2.1162112951278687

Epoch: 6| Step: 8
Training loss: 1.5921893119812012
Validation loss: 2.1017874280611673

Epoch: 6| Step: 9
Training loss: 2.334169864654541
Validation loss: 2.1059235334396362

Epoch: 6| Step: 10
Training loss: 2.0899953842163086
Validation loss: 2.109020173549652

Epoch: 6| Step: 11
Training loss: 1.7446463108062744
Validation loss: 2.112716555595398

Epoch: 6| Step: 12
Training loss: 2.2924091815948486
Validation loss: 2.098422646522522

Epoch: 6| Step: 13
Training loss: 2.0016393661499023
Validation loss: 2.1185207764307656

Epoch: 168| Step: 0
Training loss: 1.3994765281677246
Validation loss: 2.113248268763224

Epoch: 6| Step: 1
Training loss: 1.9612820148468018
Validation loss: 2.1003243923187256

Epoch: 6| Step: 2
Training loss: 2.107609272003174
Validation loss: 2.10310967763265

Epoch: 6| Step: 3
Training loss: 2.2085230350494385
Validation loss: 2.1033546328544617

Epoch: 6| Step: 4
Training loss: 1.9022125005722046
Validation loss: 2.112242102622986

Epoch: 6| Step: 5
Training loss: 2.363985061645508
Validation loss: 2.1066153049468994

Epoch: 6| Step: 6
Training loss: 2.0345828533172607
Validation loss: 2.0982473889986673

Epoch: 6| Step: 7
Training loss: 2.4263863563537598
Validation loss: 2.1106091737747192

Epoch: 6| Step: 8
Training loss: 1.7838497161865234
Validation loss: 2.101294994354248

Epoch: 6| Step: 9
Training loss: 1.922135829925537
Validation loss: 2.09480086962382

Epoch: 6| Step: 10
Training loss: 1.4970799684524536
Validation loss: 2.1019460558891296

Epoch: 6| Step: 11
Training loss: 2.2530484199523926
Validation loss: 2.103205700715383

Epoch: 6| Step: 12
Training loss: 2.131364345550537
Validation loss: 2.091139336427053

Epoch: 6| Step: 13
Training loss: 1.667971134185791
Validation loss: 2.0847190221150718

Epoch: 169| Step: 0
Training loss: 1.738007664680481
Validation loss: 2.084663192431132

Epoch: 6| Step: 1
Training loss: 1.57668137550354
Validation loss: 2.0962650775909424

Epoch: 6| Step: 2
Training loss: 1.9289830923080444
Validation loss: 2.091804087162018

Epoch: 6| Step: 3
Training loss: 2.3025760650634766
Validation loss: 2.082071602344513

Epoch: 6| Step: 4
Training loss: 1.9843647480010986
Validation loss: 2.0771307945251465

Epoch: 6| Step: 5
Training loss: 1.697776198387146
Validation loss: 2.0892406702041626

Epoch: 6| Step: 6
Training loss: 2.2465968132019043
Validation loss: 2.097767690817515

Epoch: 6| Step: 7
Training loss: 2.3712291717529297
Validation loss: 2.090181310971578

Epoch: 6| Step: 8
Training loss: 2.804320812225342
Validation loss: 2.089414099852244

Epoch: 6| Step: 9
Training loss: 1.6759356260299683
Validation loss: 2.0939857363700867

Epoch: 6| Step: 10
Training loss: 1.6636806726455688
Validation loss: 2.1041680177052817

Epoch: 6| Step: 11
Training loss: 1.650235652923584
Validation loss: 2.1105440855026245

Epoch: 6| Step: 12
Training loss: 2.5141079425811768
Validation loss: 2.0865874886512756

Epoch: 6| Step: 13
Training loss: 1.7774205207824707
Validation loss: 2.0934299429257712

Epoch: 170| Step: 0
Training loss: 1.9412479400634766
Validation loss: 2.0770594080289206

Epoch: 6| Step: 1
Training loss: 1.870883584022522
Validation loss: 2.087819437185923

Epoch: 6| Step: 2
Training loss: 1.6431151628494263
Validation loss: 2.0898322661717734

Epoch: 6| Step: 3
Training loss: 1.7649749517440796
Validation loss: 2.083988587061564

Epoch: 6| Step: 4
Training loss: 2.023615837097168
Validation loss: 2.0717047850290933

Epoch: 6| Step: 5
Training loss: 1.8160898685455322
Validation loss: 2.0720139741897583

Epoch: 6| Step: 6
Training loss: 2.1442439556121826
Validation loss: 2.08174721399943

Epoch: 6| Step: 7
Training loss: 2.333228588104248
Validation loss: 2.0992315212885537

Epoch: 6| Step: 8
Training loss: 2.3448333740234375
Validation loss: 2.081072191397349

Epoch: 6| Step: 9
Training loss: 1.212376356124878
Validation loss: 2.103065272172292

Epoch: 6| Step: 10
Training loss: 1.9535753726959229
Validation loss: 2.0838237007459006

Epoch: 6| Step: 11
Training loss: 2.342989444732666
Validation loss: 2.1146464149157205

Epoch: 6| Step: 12
Training loss: 2.6082606315612793
Validation loss: 2.1147215366363525

Epoch: 6| Step: 13
Training loss: 1.887024164199829
Validation loss: 2.100336273511251

Epoch: 171| Step: 0
Training loss: 2.3031020164489746
Validation loss: 2.0998194217681885

Epoch: 6| Step: 1
Training loss: 1.8962708711624146
Validation loss: 2.0968939065933228

Epoch: 6| Step: 2
Training loss: 1.941840648651123
Validation loss: 2.0922610561052957

Epoch: 6| Step: 3
Training loss: 2.371720790863037
Validation loss: 2.0965359608332315

Epoch: 6| Step: 4
Training loss: 2.083239793777466
Validation loss: 2.0868569016456604

Epoch: 6| Step: 5
Training loss: 1.710064172744751
Validation loss: 2.0784005920092263

Epoch: 6| Step: 6
Training loss: 2.019857883453369
Validation loss: 2.0784982442855835

Epoch: 6| Step: 7
Training loss: 1.814051866531372
Validation loss: 2.0834539930025735

Epoch: 6| Step: 8
Training loss: 2.0850932598114014
Validation loss: 2.0761564572652182

Epoch: 6| Step: 9
Training loss: 1.6297894716262817
Validation loss: 2.077573219935099

Epoch: 6| Step: 10
Training loss: 1.8222565650939941
Validation loss: 2.090267539024353

Epoch: 6| Step: 11
Training loss: 1.692925214767456
Validation loss: 2.0968174934387207

Epoch: 6| Step: 12
Training loss: 2.299947738647461
Validation loss: 2.1026079058647156

Epoch: 6| Step: 13
Training loss: 1.9712116718292236
Validation loss: 2.0953214963277182

Epoch: 172| Step: 0
Training loss: 1.9474544525146484
Validation loss: 2.0854883790016174

Epoch: 6| Step: 1
Training loss: 2.3599600791931152
Validation loss: 2.0932116707166037

Epoch: 6| Step: 2
Training loss: 2.5522336959838867
Validation loss: 2.0884600083033242

Epoch: 6| Step: 3
Training loss: 1.981339931488037
Validation loss: 2.0823257764180503

Epoch: 6| Step: 4
Training loss: 1.650843858718872
Validation loss: 2.099087874094645

Epoch: 6| Step: 5
Training loss: 1.8124022483825684
Validation loss: 2.095427632331848

Epoch: 6| Step: 6
Training loss: 1.2962123155593872
Validation loss: 2.0833201011021933

Epoch: 6| Step: 7
Training loss: 1.6358864307403564
Validation loss: 2.078853130340576

Epoch: 6| Step: 8
Training loss: 1.869153380393982
Validation loss: 2.0914305647214255

Epoch: 6| Step: 9
Training loss: 1.4403812885284424
Validation loss: 2.085307240486145

Epoch: 6| Step: 10
Training loss: 2.2124123573303223
Validation loss: 2.097464680671692

Epoch: 6| Step: 11
Training loss: 2.3129546642303467
Validation loss: 2.0966769456863403

Epoch: 6| Step: 12
Training loss: 2.0200934410095215
Validation loss: 2.1053329507509866

Epoch: 6| Step: 13
Training loss: 2.3912720680236816
Validation loss: 2.1080561876296997

Epoch: 173| Step: 0
Training loss: 2.4861321449279785
Validation loss: 2.1093979279200235

Epoch: 6| Step: 1
Training loss: 2.386427879333496
Validation loss: 2.1154127717018127

Epoch: 6| Step: 2
Training loss: 1.8621459007263184
Validation loss: 2.1103224754333496

Epoch: 6| Step: 3
Training loss: 1.7017098665237427
Validation loss: 2.1103299458821616

Epoch: 6| Step: 4
Training loss: 2.0017476081848145
Validation loss: 2.1059298515319824

Epoch: 6| Step: 5
Training loss: 2.4311954975128174
Validation loss: 2.102422833442688

Epoch: 6| Step: 6
Training loss: 1.844416856765747
Validation loss: 2.1030235290527344

Epoch: 6| Step: 7
Training loss: 2.013711452484131
Validation loss: 2.0981145103772483

Epoch: 6| Step: 8
Training loss: 1.9107282161712646
Validation loss: 2.086595276991526

Epoch: 6| Step: 9
Training loss: 1.6601958274841309
Validation loss: 2.0851465861002603

Epoch: 6| Step: 10
Training loss: 1.6743080615997314
Validation loss: 2.0915309389432273

Epoch: 6| Step: 11
Training loss: 1.831830620765686
Validation loss: 2.075437903404236

Epoch: 6| Step: 12
Training loss: 2.2392544746398926
Validation loss: 2.0842238465944924

Epoch: 6| Step: 13
Training loss: 1.6093709468841553
Validation loss: 2.0988871256510415

Epoch: 174| Step: 0
Training loss: 1.7582416534423828
Validation loss: 2.0716243783632913

Epoch: 6| Step: 1
Training loss: 2.5054407119750977
Validation loss: 2.084074020385742

Epoch: 6| Step: 2
Training loss: 1.5727369785308838
Validation loss: 2.0663397908210754

Epoch: 6| Step: 3
Training loss: 1.785994291305542
Validation loss: 2.0872422655423484

Epoch: 6| Step: 4
Training loss: 2.3718795776367188
Validation loss: 2.087408701578776

Epoch: 6| Step: 5
Training loss: 2.1153650283813477
Validation loss: 2.099547346433004

Epoch: 6| Step: 6
Training loss: 2.1274571418762207
Validation loss: 2.098582446575165

Epoch: 6| Step: 7
Training loss: 1.6130791902542114
Validation loss: 2.0908217430114746

Epoch: 6| Step: 8
Training loss: 2.214885711669922
Validation loss: 2.090414504210154

Epoch: 6| Step: 9
Training loss: 1.6405422687530518
Validation loss: 2.0875514348347983

Epoch: 6| Step: 10
Training loss: 2.254636764526367
Validation loss: 2.0944369037946067

Epoch: 6| Step: 11
Training loss: 1.907905101776123
Validation loss: 2.0917008916536965

Epoch: 6| Step: 12
Training loss: 2.135165214538574
Validation loss: 2.0918206175168357

Epoch: 6| Step: 13
Training loss: 1.5075786113739014
Validation loss: 2.111407200495402

Epoch: 175| Step: 0
Training loss: 2.4347383975982666
Validation loss: 2.0893378059069314

Epoch: 6| Step: 1
Training loss: 1.3523128032684326
Validation loss: 2.085587461789449

Epoch: 6| Step: 2
Training loss: 1.8578400611877441
Validation loss: 2.086574832598368

Epoch: 6| Step: 3
Training loss: 2.0428216457366943
Validation loss: 2.091210583845774

Epoch: 6| Step: 4
Training loss: 2.6210408210754395
Validation loss: 2.0850680470466614

Epoch: 6| Step: 5
Training loss: 2.5738627910614014
Validation loss: 2.095236579577128

Epoch: 6| Step: 6
Training loss: 1.549286127090454
Validation loss: 2.1030670404434204

Epoch: 6| Step: 7
Training loss: 1.8765150308609009
Validation loss: 2.0946382681528726

Epoch: 6| Step: 8
Training loss: 1.9944565296173096
Validation loss: 2.1035130818684897

Epoch: 6| Step: 9
Training loss: 1.3427040576934814
Validation loss: 2.1010719339052835

Epoch: 6| Step: 10
Training loss: 1.7647120952606201
Validation loss: 2.100988248984019

Epoch: 6| Step: 11
Training loss: 2.0744242668151855
Validation loss: 2.1235183278719583

Epoch: 6| Step: 12
Training loss: 1.7872856855392456
Validation loss: 2.110740145047506

Epoch: 6| Step: 13
Training loss: 2.2914395332336426
Validation loss: 2.1030917962392173

Testing loss: 1.6812816729648508

Epoch: 1| Step: 0
Training loss: 5.9679131507873535
Validation loss: 5.360749165217082

Epoch: 6| Step: 1
Training loss: 4.884416103363037
Validation loss: 5.358744223912557

Epoch: 6| Step: 2
Training loss: 6.189194679260254
Validation loss: 5.356849988301595

Epoch: 6| Step: 3
Training loss: 5.087024211883545
Validation loss: 5.3549708525339765

Epoch: 6| Step: 4
Training loss: 5.14207649230957
Validation loss: 5.3531850179036455

Epoch: 6| Step: 5
Training loss: 6.5840935707092285
Validation loss: 5.351447105407715

Epoch: 6| Step: 6
Training loss: 5.544624328613281
Validation loss: 5.349698543548584

Epoch: 6| Step: 7
Training loss: 4.31220006942749
Validation loss: 5.347995758056641

Epoch: 6| Step: 8
Training loss: 4.953369140625
Validation loss: 5.346358855565389

Epoch: 6| Step: 9
Training loss: 4.863511085510254
Validation loss: 5.344762643178304

Epoch: 6| Step: 10
Training loss: 5.81481409072876
Validation loss: 5.343138774236043

Epoch: 6| Step: 11
Training loss: 5.166897296905518
Validation loss: 5.341548681259155

Epoch: 6| Step: 12
Training loss: 5.653421401977539
Validation loss: 5.339882771174113

Epoch: 6| Step: 13
Training loss: 5.687805652618408
Validation loss: 5.338109970092773

Epoch: 2| Step: 0
Training loss: 5.180776119232178
Validation loss: 5.336273113886516

Epoch: 6| Step: 1
Training loss: 4.822916030883789
Validation loss: 5.334385236104329

Epoch: 6| Step: 2
Training loss: 4.84837007522583
Validation loss: 5.332441091537476

Epoch: 6| Step: 3
Training loss: 5.293542861938477
Validation loss: 5.330422242482503

Epoch: 6| Step: 4
Training loss: 5.488941192626953
Validation loss: 5.328330437342326

Epoch: 6| Step: 5
Training loss: 4.165456771850586
Validation loss: 5.325983961423238

Epoch: 6| Step: 6
Training loss: 5.807528972625732
Validation loss: 5.323633273442586

Epoch: 6| Step: 7
Training loss: 6.414837837219238
Validation loss: 5.321122328440349

Epoch: 6| Step: 8
Training loss: 6.233160972595215
Validation loss: 5.318576494852702

Epoch: 6| Step: 9
Training loss: 4.732224941253662
Validation loss: 5.315863450368245

Epoch: 6| Step: 10
Training loss: 6.105877876281738
Validation loss: 5.312996784845988

Epoch: 6| Step: 11
Training loss: 5.618487358093262
Validation loss: 5.309998512268066

Epoch: 6| Step: 12
Training loss: 4.945590019226074
Validation loss: 5.3069140911102295

Epoch: 6| Step: 13
Training loss: 5.816431045532227
Validation loss: 5.303597450256348

Epoch: 3| Step: 0
Training loss: 5.1195526123046875
Validation loss: 5.300296386082967

Epoch: 6| Step: 1
Training loss: 5.7852888107299805
Validation loss: 5.296620607376099

Epoch: 6| Step: 2
Training loss: 5.032393455505371
Validation loss: 5.292877117792766

Epoch: 6| Step: 3
Training loss: 4.910608768463135
Validation loss: 5.288782278696696

Epoch: 6| Step: 4
Training loss: 5.681804656982422
Validation loss: 5.284761905670166

Epoch: 6| Step: 5
Training loss: 6.199433326721191
Validation loss: 5.280330379803975

Epoch: 6| Step: 6
Training loss: 5.709594249725342
Validation loss: 5.275690237681071

Epoch: 6| Step: 7
Training loss: 4.955965995788574
Validation loss: 5.270894606908162

Epoch: 6| Step: 8
Training loss: 6.120751857757568
Validation loss: 5.26576582590739

Epoch: 6| Step: 9
Training loss: 5.2755126953125
Validation loss: 5.260707855224609

Epoch: 6| Step: 10
Training loss: 5.33736515045166
Validation loss: 5.255165974299113

Epoch: 6| Step: 11
Training loss: 5.376009941101074
Validation loss: 5.249334255854289

Epoch: 6| Step: 12
Training loss: 4.901360511779785
Validation loss: 5.243274529774983

Epoch: 6| Step: 13
Training loss: 4.380442142486572
Validation loss: 5.2371541659037275

Epoch: 4| Step: 0
Training loss: 4.7526960372924805
Validation loss: 5.230498870213826

Epoch: 6| Step: 1
Training loss: 4.964456558227539
Validation loss: 5.223939657211304

Epoch: 6| Step: 2
Training loss: 5.766134262084961
Validation loss: 5.216909090677897

Epoch: 6| Step: 3
Training loss: 4.306982517242432
Validation loss: 5.209928830464681

Epoch: 6| Step: 4
Training loss: 5.612752914428711
Validation loss: 5.202452818552653

Epoch: 6| Step: 5
Training loss: 5.4925923347473145
Validation loss: 5.194783687591553

Epoch: 6| Step: 6
Training loss: 5.090944290161133
Validation loss: 5.1873931884765625

Epoch: 6| Step: 7
Training loss: 5.709271430969238
Validation loss: 5.179585695266724

Epoch: 6| Step: 8
Training loss: 5.334991455078125
Validation loss: 5.171384731928508

Epoch: 6| Step: 9
Training loss: 5.185734748840332
Validation loss: 5.163344144821167

Epoch: 6| Step: 10
Training loss: 5.2674241065979
Validation loss: 5.155084530512492

Epoch: 6| Step: 11
Training loss: 4.468794822692871
Validation loss: 5.146482467651367

Epoch: 6| Step: 12
Training loss: 6.224631309509277
Validation loss: 5.137835184733073

Epoch: 6| Step: 13
Training loss: 5.3827924728393555
Validation loss: 5.129078348477681

Epoch: 5| Step: 0
Training loss: 5.336791038513184
Validation loss: 5.120255629221599

Epoch: 6| Step: 1
Training loss: 4.991530418395996
Validation loss: 5.11102310816447

Epoch: 6| Step: 2
Training loss: 5.2586774826049805
Validation loss: 5.101812203725179

Epoch: 6| Step: 3
Training loss: 5.4965996742248535
Validation loss: 5.092479149500529

Epoch: 6| Step: 4
Training loss: 6.219379425048828
Validation loss: 5.083403825759888

Epoch: 6| Step: 5
Training loss: 4.907533168792725
Validation loss: 5.073747555414836

Epoch: 6| Step: 6
Training loss: 4.403453350067139
Validation loss: 5.0641021728515625

Epoch: 6| Step: 7
Training loss: 6.373091697692871
Validation loss: 5.054982821146647

Epoch: 6| Step: 8
Training loss: 3.825866222381592
Validation loss: 5.045475880304973

Epoch: 6| Step: 9
Training loss: 5.237424373626709
Validation loss: 5.0359437465667725

Epoch: 6| Step: 10
Training loss: 5.8279523849487305
Validation loss: 5.0264198780059814

Epoch: 6| Step: 11
Training loss: 5.308549404144287
Validation loss: 5.017088413238525

Epoch: 6| Step: 12
Training loss: 3.559953212738037
Validation loss: 5.00820255279541

Epoch: 6| Step: 13
Training loss: 5.176644802093506
Validation loss: 4.998996337254842

Epoch: 6| Step: 0
Training loss: 6.12502908706665
Validation loss: 4.990564584732056

Epoch: 6| Step: 1
Training loss: 5.255860328674316
Validation loss: 4.981738011042277

Epoch: 6| Step: 2
Training loss: 5.360743522644043
Validation loss: 4.973133722941081

Epoch: 6| Step: 3
Training loss: 5.154862403869629
Validation loss: 4.9646561940511065

Epoch: 6| Step: 4
Training loss: 6.128345012664795
Validation loss: 4.956142505009969

Epoch: 6| Step: 5
Training loss: 4.740270614624023
Validation loss: 4.947522242863973

Epoch: 6| Step: 6
Training loss: 4.967403888702393
Validation loss: 4.938920100529988

Epoch: 6| Step: 7
Training loss: 4.563440799713135
Validation loss: 4.930639982223511

Epoch: 6| Step: 8
Training loss: 5.641823768615723
Validation loss: 4.921931664148967

Epoch: 6| Step: 9
Training loss: 4.314582347869873
Validation loss: 4.913545807202657

Epoch: 6| Step: 10
Training loss: 5.125064849853516
Validation loss: 4.905652602513631

Epoch: 6| Step: 11
Training loss: 4.875217914581299
Validation loss: 4.8977062702178955

Epoch: 6| Step: 12
Training loss: 3.82149338722229
Validation loss: 4.890373865763347

Epoch: 6| Step: 13
Training loss: 4.179892539978027
Validation loss: 4.882905880610148

Epoch: 7| Step: 0
Training loss: 4.32863712310791
Validation loss: 4.87604284286499

Epoch: 6| Step: 1
Training loss: 4.522189140319824
Validation loss: 4.8692779541015625

Epoch: 6| Step: 2
Training loss: 5.272300720214844
Validation loss: 4.862807234128316

Epoch: 6| Step: 3
Training loss: 5.265366554260254
Validation loss: 4.856226126352946

Epoch: 6| Step: 4
Training loss: 5.432668685913086
Validation loss: 4.85004719098409

Epoch: 6| Step: 5
Training loss: 4.969747066497803
Validation loss: 4.844008048375447

Epoch: 6| Step: 6
Training loss: 4.533885955810547
Validation loss: 4.837931474049886

Epoch: 6| Step: 7
Training loss: 5.408281326293945
Validation loss: 4.8319514989852905

Epoch: 6| Step: 8
Training loss: 3.8280344009399414
Validation loss: 4.8261222044626875

Epoch: 6| Step: 9
Training loss: 5.498012542724609
Validation loss: 4.820251067479451

Epoch: 6| Step: 10
Training loss: 4.421564102172852
Validation loss: 4.814697186152141

Epoch: 6| Step: 11
Training loss: 5.411587238311768
Validation loss: 4.808939258257548

Epoch: 6| Step: 12
Training loss: 4.687633037567139
Validation loss: 4.803277174631755

Epoch: 6| Step: 13
Training loss: 5.353794097900391
Validation loss: 4.797595818837483

Epoch: 8| Step: 0
Training loss: 4.874852180480957
Validation loss: 4.791958173116048

Epoch: 6| Step: 1
Training loss: 5.322438716888428
Validation loss: 4.7863063017527265

Epoch: 6| Step: 2
Training loss: 4.542455196380615
Validation loss: 4.7809897263844805

Epoch: 6| Step: 3
Training loss: 5.924332618713379
Validation loss: 4.775059620539348

Epoch: 6| Step: 4
Training loss: 5.664280891418457
Validation loss: 4.769705573717753

Epoch: 6| Step: 5
Training loss: 4.280428886413574
Validation loss: 4.764029423395793

Epoch: 6| Step: 6
Training loss: 3.9043874740600586
Validation loss: 4.7582065264383955

Epoch: 6| Step: 7
Training loss: 5.869519233703613
Validation loss: 4.75306499004364

Epoch: 6| Step: 8
Training loss: 3.6752419471740723
Validation loss: 4.747577548027039

Epoch: 6| Step: 9
Training loss: 4.794098377227783
Validation loss: 4.742039442062378

Epoch: 6| Step: 10
Training loss: 5.0807414054870605
Validation loss: 4.737118403116862

Epoch: 6| Step: 11
Training loss: 5.14090633392334
Validation loss: 4.731968482335408

Epoch: 6| Step: 12
Training loss: 4.8795084953308105
Validation loss: 4.727112015088399

Epoch: 6| Step: 13
Training loss: 3.9150686264038086
Validation loss: 4.722009658813477

Epoch: 9| Step: 0
Training loss: 4.681755065917969
Validation loss: 4.716547966003418

Epoch: 6| Step: 1
Training loss: 4.457212448120117
Validation loss: 4.711329420407613

Epoch: 6| Step: 2
Training loss: 4.205179214477539
Validation loss: 4.706615646680196

Epoch: 6| Step: 3
Training loss: 5.145011901855469
Validation loss: 4.701386173566182

Epoch: 6| Step: 4
Training loss: 4.676813125610352
Validation loss: 4.696339925130208

Epoch: 6| Step: 5
Training loss: 4.418871879577637
Validation loss: 4.690844297409058

Epoch: 6| Step: 6
Training loss: 3.9981019496917725
Validation loss: 4.685360034306844

Epoch: 6| Step: 7
Training loss: 4.572622299194336
Validation loss: 4.680647174517314

Epoch: 6| Step: 8
Training loss: 4.833209037780762
Validation loss: 4.675014138221741

Epoch: 6| Step: 9
Training loss: 4.780170440673828
Validation loss: 4.669481039047241

Epoch: 6| Step: 10
Training loss: 4.786506175994873
Validation loss: 4.663974603017171

Epoch: 6| Step: 11
Training loss: 5.6096391677856445
Validation loss: 4.6580774784088135

Epoch: 6| Step: 12
Training loss: 5.249794006347656
Validation loss: 4.652864535649617

Epoch: 6| Step: 13
Training loss: 5.470185279846191
Validation loss: 4.647984981536865

Epoch: 10| Step: 0
Training loss: 4.669503688812256
Validation loss: 4.642437100410461

Epoch: 6| Step: 1
Training loss: 5.093729496002197
Validation loss: 4.6371099551518755

Epoch: 6| Step: 2
Training loss: 5.242532253265381
Validation loss: 4.6315492788950605

Epoch: 6| Step: 3
Training loss: 4.310943603515625
Validation loss: 4.625939846038818

Epoch: 6| Step: 4
Training loss: 4.100385665893555
Validation loss: 4.620814402898152

Epoch: 6| Step: 5
Training loss: 5.2650065422058105
Validation loss: 4.615084012349446

Epoch: 6| Step: 6
Training loss: 5.462841033935547
Validation loss: 4.609894752502441

Epoch: 6| Step: 7
Training loss: 4.316051483154297
Validation loss: 4.6041741371154785

Epoch: 6| Step: 8
Training loss: 5.328005790710449
Validation loss: 4.598684946695964

Epoch: 6| Step: 9
Training loss: 4.918640613555908
Validation loss: 4.593319932619731

Epoch: 6| Step: 10
Training loss: 3.739604949951172
Validation loss: 4.587937355041504

Epoch: 6| Step: 11
Training loss: 4.921640872955322
Validation loss: 4.582521080970764

Epoch: 6| Step: 12
Training loss: 4.4723334312438965
Validation loss: 4.57682478427887

Epoch: 6| Step: 13
Training loss: 4.065884590148926
Validation loss: 4.571418682734172

Epoch: 11| Step: 0
Training loss: 4.559053421020508
Validation loss: 4.5660436153411865

Epoch: 6| Step: 1
Training loss: 5.198221206665039
Validation loss: 4.56021253267924

Epoch: 6| Step: 2
Training loss: 4.282285690307617
Validation loss: 4.5546168486277265

Epoch: 6| Step: 3
Training loss: 4.917459487915039
Validation loss: 4.548590024312337

Epoch: 6| Step: 4
Training loss: 5.514136791229248
Validation loss: 4.543232202529907

Epoch: 6| Step: 5
Training loss: 3.9233572483062744
Validation loss: 4.537553946177165

Epoch: 6| Step: 6
Training loss: 4.703673839569092
Validation loss: 4.531518459320068

Epoch: 6| Step: 7
Training loss: 5.013021469116211
Validation loss: 4.5254965623219805

Epoch: 6| Step: 8
Training loss: 4.994274139404297
Validation loss: 4.519762754440308

Epoch: 6| Step: 9
Training loss: 4.747413158416748
Validation loss: 4.513860861460368

Epoch: 6| Step: 10
Training loss: 5.015443801879883
Validation loss: 4.507668773333232

Epoch: 6| Step: 11
Training loss: 3.7113375663757324
Validation loss: 4.5011599858601885

Epoch: 6| Step: 12
Training loss: 3.930887222290039
Validation loss: 4.494678934415181

Epoch: 6| Step: 13
Training loss: 4.361568450927734
Validation loss: 4.488243103027344

Epoch: 12| Step: 0
Training loss: 3.9337332248687744
Validation loss: 4.4817798137664795

Epoch: 6| Step: 1
Training loss: 3.954691171646118
Validation loss: 4.476067066192627

Epoch: 6| Step: 2
Training loss: 4.5180888175964355
Validation loss: 4.469974040985107

Epoch: 6| Step: 3
Training loss: 4.535224914550781
Validation loss: 4.463177919387817

Epoch: 6| Step: 4
Training loss: 4.973550319671631
Validation loss: 4.457504232724507

Epoch: 6| Step: 5
Training loss: 5.252520561218262
Validation loss: 4.451868454615275

Epoch: 6| Step: 6
Training loss: 4.795830726623535
Validation loss: 4.445480267206828

Epoch: 6| Step: 7
Training loss: 4.378880023956299
Validation loss: 4.439838886260986

Epoch: 6| Step: 8
Training loss: 5.655030250549316
Validation loss: 4.433284838994344

Epoch: 6| Step: 9
Training loss: 4.287700653076172
Validation loss: 4.42759354909261

Epoch: 6| Step: 10
Training loss: 5.070756912231445
Validation loss: 4.42118223508199

Epoch: 6| Step: 11
Training loss: 4.718483924865723
Validation loss: 4.415156284968059

Epoch: 6| Step: 12
Training loss: 4.96484375
Validation loss: 4.409014066060384

Epoch: 6| Step: 13
Training loss: 2.719292163848877
Validation loss: 4.403210282325745

Epoch: 13| Step: 0
Training loss: 4.332906723022461
Validation loss: 4.396348436673482

Epoch: 6| Step: 1
Training loss: 4.4513092041015625
Validation loss: 4.390583594640096

Epoch: 6| Step: 2
Training loss: 3.283226728439331
Validation loss: 4.384928425153096

Epoch: 6| Step: 3
Training loss: 4.669820785522461
Validation loss: 4.3783308664957685

Epoch: 6| Step: 4
Training loss: 5.137782096862793
Validation loss: 4.373246192932129

Epoch: 6| Step: 5
Training loss: 4.668879508972168
Validation loss: 4.3674518664677935

Epoch: 6| Step: 6
Training loss: 5.093845844268799
Validation loss: 4.360609571139018

Epoch: 6| Step: 7
Training loss: 3.6276583671569824
Validation loss: 4.355197787284851

Epoch: 6| Step: 8
Training loss: 4.553833961486816
Validation loss: 4.350687503814697

Epoch: 6| Step: 9
Training loss: 5.294307708740234
Validation loss: 4.346017678578694

Epoch: 6| Step: 10
Training loss: 4.062094688415527
Validation loss: 4.342069904009501

Epoch: 6| Step: 11
Training loss: 4.023507118225098
Validation loss: 4.333288590113322

Epoch: 6| Step: 12
Training loss: 4.418583869934082
Validation loss: 4.329695661862691

Epoch: 6| Step: 13
Training loss: 5.104040145874023
Validation loss: 4.32569694519043

Epoch: 14| Step: 0
Training loss: 3.895498752593994
Validation loss: 4.319849848747253

Epoch: 6| Step: 1
Training loss: 5.400920867919922
Validation loss: 4.314798672993978

Epoch: 6| Step: 2
Training loss: 4.695279121398926
Validation loss: 4.30974268913269

Epoch: 6| Step: 3
Training loss: 3.4474234580993652
Validation loss: 4.302178064982097

Epoch: 6| Step: 4
Training loss: 4.295140743255615
Validation loss: 4.296720822652181

Epoch: 6| Step: 5
Training loss: 5.225342750549316
Validation loss: 4.293283303578694

Epoch: 6| Step: 6
Training loss: 3.4836723804473877
Validation loss: 4.28781521320343

Epoch: 6| Step: 7
Training loss: 3.874016523361206
Validation loss: 4.281002322832744

Epoch: 6| Step: 8
Training loss: 4.338227272033691
Validation loss: 4.276895046234131

Epoch: 6| Step: 9
Training loss: 3.6852118968963623
Validation loss: 4.271104733149211

Epoch: 6| Step: 10
Training loss: 5.561707496643066
Validation loss: 4.265464107195537

Epoch: 6| Step: 11
Training loss: 5.04911994934082
Validation loss: 4.260918776194255

Epoch: 6| Step: 12
Training loss: 4.238831520080566
Validation loss: 4.255820592244466

Epoch: 6| Step: 13
Training loss: 4.5724406242370605
Validation loss: 4.250614245732625

Epoch: 15| Step: 0
Training loss: 4.682166576385498
Validation loss: 4.24476687113444

Epoch: 6| Step: 1
Training loss: 4.520854473114014
Validation loss: 4.239632606506348

Epoch: 6| Step: 2
Training loss: 4.638327598571777
Validation loss: 4.2364537715911865

Epoch: 6| Step: 3
Training loss: 4.126175880432129
Validation loss: 4.231249769528707

Epoch: 6| Step: 4
Training loss: 5.004700183868408
Validation loss: 4.226287444432576

Epoch: 6| Step: 5
Training loss: 4.396399021148682
Validation loss: 4.22053857644399

Epoch: 6| Step: 6
Training loss: 4.596096038818359
Validation loss: 4.214264790217082

Epoch: 6| Step: 7
Training loss: 4.907238006591797
Validation loss: 4.209162871042888

Epoch: 6| Step: 8
Training loss: 3.6230738162994385
Validation loss: 4.205256422360738

Epoch: 6| Step: 9
Training loss: 4.196014881134033
Validation loss: 4.201820532480876

Epoch: 6| Step: 10
Training loss: 4.115673065185547
Validation loss: 4.197207450866699

Epoch: 6| Step: 11
Training loss: 3.9780285358428955
Validation loss: 4.191815574963887

Epoch: 6| Step: 12
Training loss: 3.850536823272705
Validation loss: 4.185075203577678

Epoch: 6| Step: 13
Training loss: 4.205595016479492
Validation loss: 4.180413047472636

Epoch: 16| Step: 0
Training loss: 3.6205456256866455
Validation loss: 4.175930341084798

Epoch: 6| Step: 1
Training loss: 4.510470390319824
Validation loss: 4.171550313631694

Epoch: 6| Step: 2
Training loss: 4.668327331542969
Validation loss: 4.167130947113037

Epoch: 6| Step: 3
Training loss: 3.746499538421631
Validation loss: 4.161962707837422

Epoch: 6| Step: 4
Training loss: 4.956197261810303
Validation loss: 4.158931136131287

Epoch: 6| Step: 5
Training loss: 3.743332624435425
Validation loss: 4.152835408846538

Epoch: 6| Step: 6
Training loss: 5.236680030822754
Validation loss: 4.146995663642883

Epoch: 6| Step: 7
Training loss: 3.8880457878112793
Validation loss: 4.141721884409587

Epoch: 6| Step: 8
Training loss: 4.380135536193848
Validation loss: 4.13738230864207

Epoch: 6| Step: 9
Training loss: 5.356740951538086
Validation loss: 4.131855368614197

Epoch: 6| Step: 10
Training loss: 4.782711029052734
Validation loss: 4.128775238990784

Epoch: 6| Step: 11
Training loss: 4.5092363357543945
Validation loss: 4.122656226158142

Epoch: 6| Step: 12
Training loss: 3.3770201206207275
Validation loss: 4.117965976397197

Epoch: 6| Step: 13
Training loss: 3.21930193901062
Validation loss: 4.112660090128581

Epoch: 17| Step: 0
Training loss: 4.235410690307617
Validation loss: 4.108614961306254

Epoch: 6| Step: 1
Training loss: 4.770905494689941
Validation loss: 4.105715990066528

Epoch: 6| Step: 2
Training loss: 3.8127527236938477
Validation loss: 4.099359234174092

Epoch: 6| Step: 3
Training loss: 4.053402423858643
Validation loss: 4.0945537487665815

Epoch: 6| Step: 4
Training loss: 4.146393775939941
Validation loss: 4.0912372668584185

Epoch: 6| Step: 5
Training loss: 4.518497467041016
Validation loss: 4.087740023930867

Epoch: 6| Step: 6
Training loss: 3.818251132965088
Validation loss: 4.083144505818685

Epoch: 6| Step: 7
Training loss: 4.6386213302612305
Validation loss: 4.077580849329631

Epoch: 6| Step: 8
Training loss: 4.2679924964904785
Validation loss: 4.073455333709717

Epoch: 6| Step: 9
Training loss: 4.210013389587402
Validation loss: 4.0689525206883745

Epoch: 6| Step: 10
Training loss: 4.315210819244385
Validation loss: 4.063654541969299

Epoch: 6| Step: 11
Training loss: 2.699489116668701
Validation loss: 4.059101263682048

Epoch: 6| Step: 12
Training loss: 5.583885192871094
Validation loss: 4.05428159236908

Epoch: 6| Step: 13
Training loss: 4.042311191558838
Validation loss: 4.050638993581136

Epoch: 18| Step: 0
Training loss: 4.843131065368652
Validation loss: 4.046062231063843

Epoch: 6| Step: 1
Training loss: 4.0262131690979
Validation loss: 4.04173203309377

Epoch: 6| Step: 2
Training loss: 5.032391548156738
Validation loss: 4.038413921991984

Epoch: 6| Step: 3
Training loss: 4.125279426574707
Validation loss: 4.032458861668904

Epoch: 6| Step: 4
Training loss: 3.0304439067840576
Validation loss: 4.02894401550293

Epoch: 6| Step: 5
Training loss: 2.780454158782959
Validation loss: 4.02495014667511

Epoch: 6| Step: 6
Training loss: 4.059683799743652
Validation loss: 4.020108064015706

Epoch: 6| Step: 7
Training loss: 5.026970863342285
Validation loss: 4.016093413035075

Epoch: 6| Step: 8
Training loss: 4.99672794342041
Validation loss: 4.0102115869522095

Epoch: 6| Step: 9
Training loss: 4.351981163024902
Validation loss: 4.006097316741943

Epoch: 6| Step: 10
Training loss: 3.846785068511963
Validation loss: 4.0027337074279785

Epoch: 6| Step: 11
Training loss: 4.204043388366699
Validation loss: 3.997827967007955

Epoch: 6| Step: 12
Training loss: 4.1170806884765625
Validation loss: 3.9939964612325034

Epoch: 6| Step: 13
Training loss: 3.831299304962158
Validation loss: 3.9887189467748008

Epoch: 19| Step: 0
Training loss: 2.477321147918701
Validation loss: 3.9835238456726074

Epoch: 6| Step: 1
Training loss: 4.556867599487305
Validation loss: 3.979516386985779

Epoch: 6| Step: 2
Training loss: 3.376155376434326
Validation loss: 3.975555976231893

Epoch: 6| Step: 3
Training loss: 4.75529670715332
Validation loss: 3.9725266297658286

Epoch: 6| Step: 4
Training loss: 4.96218729019165
Validation loss: 3.9665945370992026

Epoch: 6| Step: 5
Training loss: 4.324963569641113
Validation loss: 3.9612181981404624

Epoch: 6| Step: 6
Training loss: 4.131404399871826
Validation loss: 3.956738233566284

Epoch: 6| Step: 7
Training loss: 4.425290584564209
Validation loss: 3.9534069697062173

Epoch: 6| Step: 8
Training loss: 3.881617784500122
Validation loss: 3.948527733484904

Epoch: 6| Step: 9
Training loss: 4.188440799713135
Validation loss: 3.9440036614735923

Epoch: 6| Step: 10
Training loss: 3.608661413192749
Validation loss: 3.939588268597921

Epoch: 6| Step: 11
Training loss: 4.302738189697266
Validation loss: 3.9346431493759155

Epoch: 6| Step: 12
Training loss: 5.01839017868042
Validation loss: 3.931140343348185

Epoch: 6| Step: 13
Training loss: 3.424954414367676
Validation loss: 3.925670544306437

Epoch: 20| Step: 0
Training loss: 5.079435348510742
Validation loss: 3.9221085707346597

Epoch: 6| Step: 1
Training loss: 3.9063971042633057
Validation loss: 3.917020042737325

Epoch: 6| Step: 2
Training loss: 3.5430028438568115
Validation loss: 3.9130574464797974

Epoch: 6| Step: 3
Training loss: 3.8230674266815186
Validation loss: 3.9082732995351157

Epoch: 6| Step: 4
Training loss: 3.714744806289673
Validation loss: 3.9051194190979004

Epoch: 6| Step: 5
Training loss: 4.745253562927246
Validation loss: 3.899723529815674

Epoch: 6| Step: 6
Training loss: 3.665505886077881
Validation loss: 3.896600365638733

Epoch: 6| Step: 7
Training loss: 4.186602592468262
Validation loss: 3.891599098841349

Epoch: 6| Step: 8
Training loss: 4.169017791748047
Validation loss: 3.8870994647343955

Epoch: 6| Step: 9
Training loss: 4.4143805503845215
Validation loss: 3.8820932308832803

Epoch: 6| Step: 10
Training loss: 3.6794400215148926
Validation loss: 3.878039757410685

Epoch: 6| Step: 11
Training loss: 3.7474722862243652
Validation loss: 3.8739901383717856

Epoch: 6| Step: 12
Training loss: 3.745666027069092
Validation loss: 3.870245377222697

Epoch: 6| Step: 13
Training loss: 4.186753749847412
Validation loss: 3.8645870288213096

Epoch: 21| Step: 0
Training loss: 3.009066581726074
Validation loss: 3.8609917561213174

Epoch: 6| Step: 1
Training loss: 4.177989959716797
Validation loss: 3.856878161430359

Epoch: 6| Step: 2
Training loss: 3.4481353759765625
Validation loss: 3.8543903827667236

Epoch: 6| Step: 3
Training loss: 3.4399094581604004
Validation loss: 3.8489814599355063

Epoch: 6| Step: 4
Training loss: 4.919287204742432
Validation loss: 3.8457254568735757

Epoch: 6| Step: 5
Training loss: 3.5594489574432373
Validation loss: 3.8400644063949585

Epoch: 6| Step: 6
Training loss: 3.1696062088012695
Validation loss: 3.8357808192571006

Epoch: 6| Step: 7
Training loss: 3.94988751411438
Validation loss: 3.832029422124227

Epoch: 6| Step: 8
Training loss: 4.639237880706787
Validation loss: 3.827846964200338

Epoch: 6| Step: 9
Training loss: 4.314756393432617
Validation loss: 3.823191205660502

Epoch: 6| Step: 10
Training loss: 4.180546760559082
Validation loss: 3.819053292274475

Epoch: 6| Step: 11
Training loss: 4.063092231750488
Validation loss: 3.8136607011159263

Epoch: 6| Step: 12
Training loss: 4.169548988342285
Validation loss: 3.810032765070597

Epoch: 6| Step: 13
Training loss: 4.758248805999756
Validation loss: 3.8074408769607544

Epoch: 22| Step: 0
Training loss: 3.2215170860290527
Validation loss: 3.804585854212443

Epoch: 6| Step: 1
Training loss: 4.569822311401367
Validation loss: 3.798602898915609

Epoch: 6| Step: 2
Training loss: 3.166403293609619
Validation loss: 3.7927725315093994

Epoch: 6| Step: 3
Training loss: 4.131664752960205
Validation loss: 3.791190226872762

Epoch: 6| Step: 4
Training loss: 4.544904708862305
Validation loss: 3.785079836845398

Epoch: 6| Step: 5
Training loss: 3.4145121574401855
Validation loss: 3.7800420125325522

Epoch: 6| Step: 6
Training loss: 3.357832670211792
Validation loss: 3.775257150332133

Epoch: 6| Step: 7
Training loss: 3.304035186767578
Validation loss: 3.771846612294515

Epoch: 6| Step: 8
Training loss: 4.429450035095215
Validation loss: 3.7679818868637085

Epoch: 6| Step: 9
Training loss: 3.9388694763183594
Validation loss: 3.7647939125696817

Epoch: 6| Step: 10
Training loss: 4.520285606384277
Validation loss: 3.7601850827534995

Epoch: 6| Step: 11
Training loss: 4.146301746368408
Validation loss: 3.755017360051473

Epoch: 6| Step: 12
Training loss: 4.061240196228027
Validation loss: 3.7504839102427163

Epoch: 6| Step: 13
Training loss: 4.249434947967529
Validation loss: 3.745977997779846

Epoch: 23| Step: 0
Training loss: 4.447444915771484
Validation loss: 3.741394797960917

Epoch: 6| Step: 1
Training loss: 4.3453369140625
Validation loss: 3.736631711324056

Epoch: 6| Step: 2
Training loss: 3.6417076587677
Validation loss: 3.730971574783325

Epoch: 6| Step: 3
Training loss: 3.815802574157715
Validation loss: 3.7266040245691934

Epoch: 6| Step: 4
Training loss: 3.651961326599121
Validation loss: 3.721858541170756

Epoch: 6| Step: 5
Training loss: 3.847761869430542
Validation loss: 3.7174085776011148

Epoch: 6| Step: 6
Training loss: 3.9423484802246094
Validation loss: 3.713539163271586

Epoch: 6| Step: 7
Training loss: 3.693878173828125
Validation loss: 3.7097602685292563

Epoch: 6| Step: 8
Training loss: 4.094657897949219
Validation loss: 3.704456369082133

Epoch: 6| Step: 9
Training loss: 5.05775785446167
Validation loss: 3.6995238860448203

Epoch: 6| Step: 10
Training loss: 3.258652925491333
Validation loss: 3.6955206791559854

Epoch: 6| Step: 11
Training loss: 3.556338310241699
Validation loss: 3.6927963495254517

Epoch: 6| Step: 12
Training loss: 3.3980391025543213
Validation loss: 3.688132087389628

Epoch: 6| Step: 13
Training loss: 3.4669716358184814
Validation loss: 3.6828039487202964

Epoch: 24| Step: 0
Training loss: 3.5982465744018555
Validation loss: 3.6771533091863

Epoch: 6| Step: 1
Training loss: 3.9189493656158447
Validation loss: 3.6728058656056723

Epoch: 6| Step: 2
Training loss: 3.738938331604004
Validation loss: 3.6687432527542114

Epoch: 6| Step: 3
Training loss: 3.0696163177490234
Validation loss: 3.66516649723053

Epoch: 6| Step: 4
Training loss: 3.057131290435791
Validation loss: 3.6604373455047607

Epoch: 6| Step: 5
Training loss: 3.762031078338623
Validation loss: 3.6564173301060996

Epoch: 6| Step: 6
Training loss: 4.386972904205322
Validation loss: 3.6526052951812744

Epoch: 6| Step: 7
Training loss: 4.401252746582031
Validation loss: 3.6492476065953574

Epoch: 6| Step: 8
Training loss: 3.9699466228485107
Validation loss: 3.6443915367126465

Epoch: 6| Step: 9
Training loss: 4.327286243438721
Validation loss: 3.639781912167867

Epoch: 6| Step: 10
Training loss: 4.475397109985352
Validation loss: 3.6345860958099365

Epoch: 6| Step: 11
Training loss: 4.306489944458008
Validation loss: 3.6295323769251504

Epoch: 6| Step: 12
Training loss: 3.375406265258789
Validation loss: 3.624821742375692

Epoch: 6| Step: 13
Training loss: 2.9866714477539062
Validation loss: 3.619818329811096

Epoch: 25| Step: 0
Training loss: 2.8737850189208984
Validation loss: 3.6153361399968467

Epoch: 6| Step: 1
Training loss: 3.197321891784668
Validation loss: 3.610467235247294

Epoch: 6| Step: 2
Training loss: 3.900364875793457
Validation loss: 3.6059155464172363

Epoch: 6| Step: 3
Training loss: 4.461383819580078
Validation loss: 3.601479252179464

Epoch: 6| Step: 4
Training loss: 3.2402002811431885
Validation loss: 3.596664388974508

Epoch: 6| Step: 5
Training loss: 3.981365442276001
Validation loss: 3.5922865072886148

Epoch: 6| Step: 6
Training loss: 3.825990676879883
Validation loss: 3.587840477625529

Epoch: 6| Step: 7
Training loss: 3.3193817138671875
Validation loss: 3.582547942797343

Epoch: 6| Step: 8
Training loss: 4.047352313995361
Validation loss: 3.577909549077352

Epoch: 6| Step: 9
Training loss: 4.533566951751709
Validation loss: 3.5737000703811646

Epoch: 6| Step: 10
Training loss: 2.9556937217712402
Validation loss: 3.5685230493545532

Epoch: 6| Step: 11
Training loss: 3.96335506439209
Validation loss: 3.5654441912968955

Epoch: 6| Step: 12
Training loss: 3.9229419231414795
Validation loss: 3.55984890460968

Epoch: 6| Step: 13
Training loss: 4.295258522033691
Validation loss: 3.555444320042928

Epoch: 26| Step: 0
Training loss: 3.471022605895996
Validation loss: 3.550460418065389

Epoch: 6| Step: 1
Training loss: 3.268566131591797
Validation loss: 3.5459193785985312

Epoch: 6| Step: 2
Training loss: 3.719593048095703
Validation loss: 3.542628288269043

Epoch: 6| Step: 3
Training loss: 3.6989810466766357
Validation loss: 3.5383505821228027

Epoch: 6| Step: 4
Training loss: 3.3919150829315186
Validation loss: 3.535022258758545

Epoch: 6| Step: 5
Training loss: 3.5939972400665283
Validation loss: 3.528990109761556

Epoch: 6| Step: 6
Training loss: 4.299561500549316
Validation loss: 3.5244799057642617

Epoch: 6| Step: 7
Training loss: 4.965051651000977
Validation loss: 3.519091248512268

Epoch: 6| Step: 8
Training loss: 3.0799598693847656
Validation loss: 3.515314221382141

Epoch: 6| Step: 9
Training loss: 3.8350679874420166
Validation loss: 3.510060946146647

Epoch: 6| Step: 10
Training loss: 4.42117977142334
Validation loss: 3.5057911475499473

Epoch: 6| Step: 11
Training loss: 3.0595874786376953
Validation loss: 3.501136064529419

Epoch: 6| Step: 12
Training loss: 4.4132914543151855
Validation loss: 3.4971105257670083

Epoch: 6| Step: 13
Training loss: 2.4368538856506348
Validation loss: 3.494650642077128

Epoch: 27| Step: 0
Training loss: 3.0268139839172363
Validation loss: 3.488931735356649

Epoch: 6| Step: 1
Training loss: 3.6219098567962646
Validation loss: 3.4951802492141724

Epoch: 6| Step: 2
Training loss: 3.8201255798339844
Validation loss: 3.4803497791290283

Epoch: 6| Step: 3
Training loss: 3.2516191005706787
Validation loss: 3.4774936040242515

Epoch: 6| Step: 4
Training loss: 3.436948537826538
Validation loss: 3.477384606997172

Epoch: 6| Step: 5
Training loss: 3.5613293647766113
Validation loss: 3.476444443066915

Epoch: 6| Step: 6
Training loss: 3.0471701622009277
Validation loss: 3.47194234530131

Epoch: 6| Step: 7
Training loss: 3.625013828277588
Validation loss: 3.466217358907064

Epoch: 6| Step: 8
Training loss: 4.263214111328125
Validation loss: 3.4625861247380576

Epoch: 6| Step: 9
Training loss: 3.9152920246124268
Validation loss: 3.4546507596969604

Epoch: 6| Step: 10
Training loss: 3.2352800369262695
Validation loss: 3.449970841407776

Epoch: 6| Step: 11
Training loss: 3.0878350734710693
Validation loss: 3.4452020724614463

Epoch: 6| Step: 12
Training loss: 4.66880989074707
Validation loss: 3.441628416379293

Epoch: 6| Step: 13
Training loss: 4.291332244873047
Validation loss: 3.437517245610555

Epoch: 28| Step: 0
Training loss: 3.378079414367676
Validation loss: 3.4332926273345947

Epoch: 6| Step: 1
Training loss: 3.44618558883667
Validation loss: 3.4291752576828003

Epoch: 6| Step: 2
Training loss: 3.9305050373077393
Validation loss: 3.423375209172567

Epoch: 6| Step: 3
Training loss: 3.6924118995666504
Validation loss: 3.419568498929342

Epoch: 6| Step: 4
Training loss: 3.9442648887634277
Validation loss: 3.4142072200775146

Epoch: 6| Step: 5
Training loss: 3.4306533336639404
Validation loss: 3.4104199409484863

Epoch: 6| Step: 6
Training loss: 3.159066677093506
Validation loss: 3.4050297339757285

Epoch: 6| Step: 7
Training loss: 3.6823034286499023
Validation loss: 3.399707555770874

Epoch: 6| Step: 8
Training loss: 4.425263404846191
Validation loss: 3.394367059071859

Epoch: 6| Step: 9
Training loss: 3.5689404010772705
Validation loss: 3.389960686365763

Epoch: 6| Step: 10
Training loss: 3.1659750938415527
Validation loss: 3.3857186237970986

Epoch: 6| Step: 11
Training loss: 3.768167495727539
Validation loss: 3.382936954498291

Epoch: 6| Step: 12
Training loss: 2.8908376693725586
Validation loss: 3.3800989786783853

Epoch: 6| Step: 13
Training loss: 3.54679536819458
Validation loss: 3.3749308983484902

Epoch: 29| Step: 0
Training loss: 4.817174434661865
Validation loss: 3.3689759969711304

Epoch: 6| Step: 1
Training loss: 3.0357537269592285
Validation loss: 3.365172108014425

Epoch: 6| Step: 2
Training loss: 2.8645224571228027
Validation loss: 3.3600266377131143

Epoch: 6| Step: 3
Training loss: 3.002321720123291
Validation loss: 3.3560383717219033

Epoch: 6| Step: 4
Training loss: 3.002683639526367
Validation loss: 3.3531461159388223

Epoch: 6| Step: 5
Training loss: 3.5680112838745117
Validation loss: 3.349112947781881

Epoch: 6| Step: 6
Training loss: 3.160639762878418
Validation loss: 3.3440405130386353

Epoch: 6| Step: 7
Training loss: 3.7460432052612305
Validation loss: 3.339435895284017

Epoch: 6| Step: 8
Training loss: 3.3362560272216797
Validation loss: 3.3352158864339194

Epoch: 6| Step: 9
Training loss: 4.196461200714111
Validation loss: 3.3309485912323

Epoch: 6| Step: 10
Training loss: 4.078852653503418
Validation loss: 3.3271602789560952

Epoch: 6| Step: 11
Training loss: 3.6750922203063965
Validation loss: 3.3226404984792075

Epoch: 6| Step: 12
Training loss: 2.95595121383667
Validation loss: 3.318567673365275

Epoch: 6| Step: 13
Training loss: 3.744647741317749
Validation loss: 3.314316749572754

Epoch: 30| Step: 0
Training loss: 4.3824567794799805
Validation loss: 3.310327967007955

Epoch: 6| Step: 1
Training loss: 3.5141818523406982
Validation loss: 3.3059309323628745

Epoch: 6| Step: 2
Training loss: 3.1056692600250244
Validation loss: 3.301305055618286

Epoch: 6| Step: 3
Training loss: 2.8369224071502686
Validation loss: 3.2971333662668862

Epoch: 6| Step: 4
Training loss: 3.726797580718994
Validation loss: 3.2928003072738647

Epoch: 6| Step: 5
Training loss: 3.702066421508789
Validation loss: 3.2887707948684692

Epoch: 6| Step: 6
Training loss: 3.604797840118408
Validation loss: 3.284786264101664

Epoch: 6| Step: 7
Training loss: 3.761573314666748
Validation loss: 3.2806294361750283

Epoch: 6| Step: 8
Training loss: 3.2780349254608154
Validation loss: 3.2757116158803306

Epoch: 6| Step: 9
Training loss: 3.2609243392944336
Validation loss: 3.2727712790171304

Epoch: 6| Step: 10
Training loss: 3.7204742431640625
Validation loss: 3.2690243323644004

Epoch: 6| Step: 11
Training loss: 2.9157471656799316
Validation loss: 3.2646471659342446

Epoch: 6| Step: 12
Training loss: 3.412407875061035
Validation loss: 3.2610205809275308

Epoch: 6| Step: 13
Training loss: 3.2438459396362305
Validation loss: 3.2576040824254355

Epoch: 31| Step: 0
Training loss: 3.59735369682312
Validation loss: 3.254084825515747

Epoch: 6| Step: 1
Training loss: 4.435299396514893
Validation loss: 3.2502570152282715

Epoch: 6| Step: 2
Training loss: 3.360521078109741
Validation loss: 3.245893200238546

Epoch: 6| Step: 3
Training loss: 3.0871005058288574
Validation loss: 3.2428236405054727

Epoch: 6| Step: 4
Training loss: 3.3265762329101562
Validation loss: 3.238818963368734

Epoch: 6| Step: 5
Training loss: 3.631136655807495
Validation loss: 3.233056346575419

Epoch: 6| Step: 6
Training loss: 3.8212060928344727
Validation loss: 3.2298908631006875

Epoch: 6| Step: 7
Training loss: 3.2142839431762695
Validation loss: 3.2260371446609497

Epoch: 6| Step: 8
Training loss: 2.3456311225891113
Validation loss: 3.221617897351583

Epoch: 6| Step: 9
Training loss: 3.7889976501464844
Validation loss: 3.2179054816563926

Epoch: 6| Step: 10
Training loss: 3.6476128101348877
Validation loss: 3.2144882678985596

Epoch: 6| Step: 11
Training loss: 3.0816755294799805
Validation loss: 3.210683822631836

Epoch: 6| Step: 12
Training loss: 3.7224080562591553
Validation loss: 3.2065453131993613

Epoch: 6| Step: 13
Training loss: 2.6594104766845703
Validation loss: 3.203174074490865

Epoch: 32| Step: 0
Training loss: 3.8249170780181885
Validation loss: 3.199251651763916

Epoch: 6| Step: 1
Training loss: 3.5044679641723633
Validation loss: 3.1955485343933105

Epoch: 6| Step: 2
Training loss: 3.0702147483825684
Validation loss: 3.1917113065719604

Epoch: 6| Step: 3
Training loss: 3.1497883796691895
Validation loss: 3.1881311337153115

Epoch: 6| Step: 4
Training loss: 3.3789260387420654
Validation loss: 3.1842856804529824

Epoch: 6| Step: 5
Training loss: 3.2569761276245117
Validation loss: 3.180641690889994

Epoch: 6| Step: 6
Training loss: 3.734079360961914
Validation loss: 3.1767799059549966

Epoch: 6| Step: 7
Training loss: 3.2930383682250977
Validation loss: 3.173001766204834

Epoch: 6| Step: 8
Training loss: 4.042720794677734
Validation loss: 3.168688098589579

Epoch: 6| Step: 9
Training loss: 2.221344470977783
Validation loss: 3.164900024731954

Epoch: 6| Step: 10
Training loss: 3.0217928886413574
Validation loss: 3.1608376502990723

Epoch: 6| Step: 11
Training loss: 3.835827589035034
Validation loss: 3.157147447268168

Epoch: 6| Step: 12
Training loss: 3.0007736682891846
Validation loss: 3.153770844141642

Epoch: 6| Step: 13
Training loss: 3.679298162460327
Validation loss: 3.1497918367385864

Epoch: 33| Step: 0
Training loss: 3.132692337036133
Validation loss: 3.1462721824645996

Epoch: 6| Step: 1
Training loss: 3.64646577835083
Validation loss: 3.1422571738560996

Epoch: 6| Step: 2
Training loss: 3.5240187644958496
Validation loss: 3.138746658960978

Epoch: 6| Step: 3
Training loss: 3.6552915573120117
Validation loss: 3.1351542870203652

Epoch: 6| Step: 4
Training loss: 3.4623193740844727
Validation loss: 3.131026347478231

Epoch: 6| Step: 5
Training loss: 3.7589380741119385
Validation loss: 3.1272284189860025

Epoch: 6| Step: 6
Training loss: 3.671098232269287
Validation loss: 3.123230814933777

Epoch: 6| Step: 7
Training loss: 2.467137575149536
Validation loss: 3.1198015610376992

Epoch: 6| Step: 8
Training loss: 3.556349277496338
Validation loss: 3.115671674410502

Epoch: 6| Step: 9
Training loss: 3.1351566314697266
Validation loss: 3.1120315392812095

Epoch: 6| Step: 10
Training loss: 2.6627907752990723
Validation loss: 3.1078197161356607

Epoch: 6| Step: 11
Training loss: 3.5491862297058105
Validation loss: 3.1031848986943564

Epoch: 6| Step: 12
Training loss: 2.29313325881958
Validation loss: 3.0999319156010947

Epoch: 6| Step: 13
Training loss: 3.8356070518493652
Validation loss: 3.0968295335769653

Epoch: 34| Step: 0
Training loss: 3.2136757373809814
Validation loss: 3.0937273502349854

Epoch: 6| Step: 1
Training loss: 3.341914176940918
Validation loss: 3.089805801709493

Epoch: 6| Step: 2
Training loss: 3.4088850021362305
Validation loss: 3.0864197413126626

Epoch: 6| Step: 3
Training loss: 2.6507225036621094
Validation loss: 3.082250634829203

Epoch: 6| Step: 4
Training loss: 2.93392276763916
Validation loss: 3.078216870625814

Epoch: 6| Step: 5
Training loss: 3.0055441856384277
Validation loss: 3.075568993886312

Epoch: 6| Step: 6
Training loss: 3.567244052886963
Validation loss: 3.070886254310608

Epoch: 6| Step: 7
Training loss: 3.984544277191162
Validation loss: 3.067633589108785

Epoch: 6| Step: 8
Training loss: 2.67283296585083
Validation loss: 3.0634700854619346

Epoch: 6| Step: 9
Training loss: 3.063941717147827
Validation loss: 3.060631354649862

Epoch: 6| Step: 10
Training loss: 3.2061092853546143
Validation loss: 3.0579455693562827

Epoch: 6| Step: 11
Training loss: 3.877767324447632
Validation loss: 3.0541884501775107

Epoch: 6| Step: 12
Training loss: 3.7251675128936768
Validation loss: 3.0511950651804605

Epoch: 6| Step: 13
Training loss: 3.0112833976745605
Validation loss: 3.048206845919291

Epoch: 35| Step: 0
Training loss: 3.0766618251800537
Validation loss: 3.044953227043152

Epoch: 6| Step: 1
Training loss: 3.4658889770507812
Validation loss: 3.0412506262461343

Epoch: 6| Step: 2
Training loss: 2.1766018867492676
Validation loss: 3.0380987326304116

Epoch: 6| Step: 3
Training loss: 3.738067150115967
Validation loss: 3.035045544306437

Epoch: 6| Step: 4
Training loss: 2.9815878868103027
Validation loss: 3.0316467682520547

Epoch: 6| Step: 5
Training loss: 4.21391487121582
Validation loss: 3.0285367568333945

Epoch: 6| Step: 6
Training loss: 3.238745927810669
Validation loss: 3.0252583821614585

Epoch: 6| Step: 7
Training loss: 3.179044723510742
Validation loss: 3.021601398785909

Epoch: 6| Step: 8
Training loss: 3.739863872528076
Validation loss: 3.0175413290659585

Epoch: 6| Step: 9
Training loss: 1.9345086812973022
Validation loss: 3.0152185360590615

Epoch: 6| Step: 10
Training loss: 3.5019545555114746
Validation loss: 3.0112549463907876

Epoch: 6| Step: 11
Training loss: 3.5435891151428223
Validation loss: 3.008095860481262

Epoch: 6| Step: 12
Training loss: 2.9967384338378906
Validation loss: 3.003716985384623

Epoch: 6| Step: 13
Training loss: 3.2459375858306885
Validation loss: 3.000701626141866

Epoch: 36| Step: 0
Training loss: 4.122072696685791
Validation loss: 2.997639616330465

Epoch: 6| Step: 1
Training loss: 3.8233108520507812
Validation loss: 2.995026151339213

Epoch: 6| Step: 2
Training loss: 3.147439479827881
Validation loss: 2.991945743560791

Epoch: 6| Step: 3
Training loss: 3.7052807807922363
Validation loss: 2.987665375073751

Epoch: 6| Step: 4
Training loss: 2.842883586883545
Validation loss: 2.9839144945144653

Epoch: 6| Step: 5
Training loss: 3.0755984783172607
Validation loss: 2.9805895487467446

Epoch: 6| Step: 6
Training loss: 2.289602756500244
Validation loss: 2.9774832328160605

Epoch: 6| Step: 7
Training loss: 2.9848029613494873
Validation loss: 2.9751694599787393

Epoch: 6| Step: 8
Training loss: 3.6011500358581543
Validation loss: 2.972705364227295

Epoch: 6| Step: 9
Training loss: 2.873100519180298
Validation loss: 2.968776822090149

Epoch: 6| Step: 10
Training loss: 3.033095598220825
Validation loss: 2.965365211168925

Epoch: 6| Step: 11
Training loss: 2.7112481594085693
Validation loss: 2.961982806523641

Epoch: 6| Step: 12
Training loss: 3.507183074951172
Validation loss: 2.9582706292470298

Epoch: 6| Step: 13
Training loss: 2.7200722694396973
Validation loss: 2.954427202542623

Epoch: 37| Step: 0
Training loss: 2.858668804168701
Validation loss: 2.9514627854029336

Epoch: 6| Step: 1
Training loss: 3.859834909439087
Validation loss: 2.94836962223053

Epoch: 6| Step: 2
Training loss: 2.007859230041504
Validation loss: 2.945987343788147

Epoch: 6| Step: 3
Training loss: 3.4255776405334473
Validation loss: 2.942374507586161

Epoch: 6| Step: 4
Training loss: 4.241307258605957
Validation loss: 2.938966075579325

Epoch: 6| Step: 5
Training loss: 2.853020668029785
Validation loss: 2.935258070627848

Epoch: 6| Step: 6
Training loss: 2.738633632659912
Validation loss: 2.9314905802408853

Epoch: 6| Step: 7
Training loss: 2.916297435760498
Validation loss: 2.9278072516123452

Epoch: 6| Step: 8
Training loss: 2.8337583541870117
Validation loss: 2.924945513407389

Epoch: 6| Step: 9
Training loss: 3.4586470127105713
Validation loss: 2.922288417816162

Epoch: 6| Step: 10
Training loss: 4.231540679931641
Validation loss: 2.9194552501042685

Epoch: 6| Step: 11
Training loss: 3.010802984237671
Validation loss: 2.9141228596369424

Epoch: 6| Step: 12
Training loss: 2.643859386444092
Validation loss: 2.9123311837514243

Epoch: 6| Step: 13
Training loss: 2.7976393699645996
Validation loss: 2.909805258115133

Epoch: 38| Step: 0
Training loss: 3.323892593383789
Validation loss: 2.907324035962423

Epoch: 6| Step: 1
Training loss: 3.213613986968994
Validation loss: 2.903105060259501

Epoch: 6| Step: 2
Training loss: 2.3303110599517822
Validation loss: 2.898831764856974

Epoch: 6| Step: 3
Training loss: 4.032211780548096
Validation loss: 2.896582086881002

Epoch: 6| Step: 4
Training loss: 3.28843355178833
Validation loss: 2.893080155054728

Epoch: 6| Step: 5
Training loss: 3.302311897277832
Validation loss: 2.889235178629557

Epoch: 6| Step: 6
Training loss: 3.3262319564819336
Validation loss: 2.8864568074544272

Epoch: 6| Step: 7
Training loss: 3.61957049369812
Validation loss: 2.8827131191889444

Epoch: 6| Step: 8
Training loss: 2.363055467605591
Validation loss: 2.8795698086420694

Epoch: 6| Step: 9
Training loss: 3.171816825866699
Validation loss: 2.8769368728001914

Epoch: 6| Step: 10
Training loss: 2.8108437061309814
Validation loss: 2.875216086705526

Epoch: 6| Step: 11
Training loss: 2.708996295928955
Validation loss: 2.870915095011393

Epoch: 6| Step: 12
Training loss: 2.73563814163208
Validation loss: 2.8682839473088584

Epoch: 6| Step: 13
Training loss: 3.093147039413452
Validation loss: 2.8656421105066934

Epoch: 39| Step: 0
Training loss: 3.1320385932922363
Validation loss: 2.861912806828817

Epoch: 6| Step: 1
Training loss: 2.6923017501831055
Validation loss: 2.859816869099935

Epoch: 6| Step: 2
Training loss: 2.793886423110962
Validation loss: 2.8580826123555503

Epoch: 6| Step: 3
Training loss: 2.6819889545440674
Validation loss: 2.854020595550537

Epoch: 6| Step: 4
Training loss: 3.023219585418701
Validation loss: 2.8517202138900757

Epoch: 6| Step: 5
Training loss: 3.450568199157715
Validation loss: 2.8472983837127686

Epoch: 6| Step: 6
Training loss: 4.053199768066406
Validation loss: 2.8415772318840027

Epoch: 6| Step: 7
Training loss: 2.7787108421325684
Validation loss: 2.8403743902842202

Epoch: 6| Step: 8
Training loss: 2.3938097953796387
Validation loss: 2.8364993731180825

Epoch: 6| Step: 9
Training loss: 3.287597417831421
Validation loss: 2.834414521853129

Epoch: 6| Step: 10
Training loss: 2.9207043647766113
Validation loss: 2.8317168951034546

Epoch: 6| Step: 11
Training loss: 2.9819095134735107
Validation loss: 2.8288281758626304

Epoch: 6| Step: 12
Training loss: 3.187386989593506
Validation loss: 2.8253058592478433

Epoch: 6| Step: 13
Training loss: 3.374640941619873
Validation loss: 2.8231958548227944

Epoch: 40| Step: 0
Training loss: 2.941284418106079
Validation loss: 2.8201448122660318

Epoch: 6| Step: 1
Training loss: 3.02001953125
Validation loss: 2.8175572951634726

Epoch: 6| Step: 2
Training loss: 3.3657236099243164
Validation loss: 2.8174948692321777

Epoch: 6| Step: 3
Training loss: 3.5049631595611572
Validation loss: 2.8125402132670083

Epoch: 6| Step: 4
Training loss: 2.989240884780884
Validation loss: 2.808476368586222

Epoch: 6| Step: 5
Training loss: 3.523122787475586
Validation loss: 2.8055651585261026

Epoch: 6| Step: 6
Training loss: 2.862947940826416
Validation loss: 2.803829312324524

Epoch: 6| Step: 7
Training loss: 2.2674291133880615
Validation loss: 2.8014522393544516

Epoch: 6| Step: 8
Training loss: 2.76993989944458
Validation loss: 2.798415501912435

Epoch: 6| Step: 9
Training loss: 2.760607957839966
Validation loss: 2.795112053553263

Epoch: 6| Step: 10
Training loss: 2.520801305770874
Validation loss: 2.7928239504496255

Epoch: 6| Step: 11
Training loss: 3.716184139251709
Validation loss: 2.7902818520863852

Epoch: 6| Step: 12
Training loss: 2.8677093982696533
Validation loss: 2.7875601053237915

Epoch: 6| Step: 13
Training loss: 3.1016693115234375
Validation loss: 2.784603993097941

Epoch: 41| Step: 0
Training loss: 2.7001113891601562
Validation loss: 2.7823490500450134

Epoch: 6| Step: 1
Training loss: 2.9001145362854004
Validation loss: 2.7793354988098145

Epoch: 6| Step: 2
Training loss: 3.241567611694336
Validation loss: 2.7757683595021567

Epoch: 6| Step: 3
Training loss: 3.1562037467956543
Validation loss: 2.7729146083196006

Epoch: 6| Step: 4
Training loss: 3.493666648864746
Validation loss: 2.7701205809911094

Epoch: 6| Step: 5
Training loss: 2.7712533473968506
Validation loss: 2.768282413482666

Epoch: 6| Step: 6
Training loss: 3.6221203804016113
Validation loss: 2.7654457489649453

Epoch: 6| Step: 7
Training loss: 2.7439141273498535
Validation loss: 2.7633610169092813

Epoch: 6| Step: 8
Training loss: 2.630429983139038
Validation loss: 2.7610836823781333

Epoch: 6| Step: 9
Training loss: 3.009934902191162
Validation loss: 2.757727026939392

Epoch: 6| Step: 10
Training loss: 3.502563953399658
Validation loss: 2.756049950917562

Epoch: 6| Step: 11
Training loss: 2.9137892723083496
Validation loss: 2.752107302347819

Epoch: 6| Step: 12
Training loss: 2.684317111968994
Validation loss: 2.748356580734253

Epoch: 6| Step: 13
Training loss: 2.2874107360839844
Validation loss: 2.7468359073003135

Epoch: 42| Step: 0
Training loss: 2.750086784362793
Validation loss: 2.74505885442098

Epoch: 6| Step: 1
Training loss: 3.119906425476074
Validation loss: 2.742515762646993

Epoch: 6| Step: 2
Training loss: 2.5776784420013428
Validation loss: 2.7412184476852417

Epoch: 6| Step: 3
Training loss: 3.794994354248047
Validation loss: 2.7380934953689575

Epoch: 6| Step: 4
Training loss: 3.8747828006744385
Validation loss: 2.740197777748108

Epoch: 6| Step: 5
Training loss: 2.7517199516296387
Validation loss: 2.732440710067749

Epoch: 6| Step: 6
Training loss: 2.8498423099517822
Validation loss: 2.7264342308044434

Epoch: 6| Step: 7
Training loss: 3.4451026916503906
Validation loss: 2.7241602341334024

Epoch: 6| Step: 8
Training loss: 2.933500289916992
Validation loss: 2.722169836362203

Epoch: 6| Step: 9
Training loss: 3.131901502609253
Validation loss: 2.7174129486083984

Epoch: 6| Step: 10
Training loss: 1.9569239616394043
Validation loss: 2.714071790377299

Epoch: 6| Step: 11
Training loss: 2.941657543182373
Validation loss: 2.7110035022099814

Epoch: 6| Step: 12
Training loss: 2.384032964706421
Validation loss: 2.7142262856165567

Epoch: 6| Step: 13
Training loss: 2.570856809616089
Validation loss: 2.7100948492685952

Epoch: 43| Step: 0
Training loss: 3.132058620452881
Validation loss: 2.705787022908529

Epoch: 6| Step: 1
Training loss: 3.344167709350586
Validation loss: 2.7011988957722983

Epoch: 6| Step: 2
Training loss: 2.740847587585449
Validation loss: 2.6990347703297934

Epoch: 6| Step: 3
Training loss: 2.295527696609497
Validation loss: 2.696963310241699

Epoch: 6| Step: 4
Training loss: 3.077730178833008
Validation loss: 2.695444901784261

Epoch: 6| Step: 5
Training loss: 3.1662750244140625
Validation loss: 2.692670543988546

Epoch: 6| Step: 6
Training loss: 2.8233141899108887
Validation loss: 2.687457243601481

Epoch: 6| Step: 7
Training loss: 2.21077299118042
Validation loss: 2.686172664165497

Epoch: 6| Step: 8
Training loss: 3.410536289215088
Validation loss: 2.6818241477012634

Epoch: 6| Step: 9
Training loss: 2.3847708702087402
Validation loss: 2.6787960131963096

Epoch: 6| Step: 10
Training loss: 3.3385257720947266
Validation loss: 2.675111174583435

Epoch: 6| Step: 11
Training loss: 3.0977535247802734
Validation loss: 2.6742254892985025

Epoch: 6| Step: 12
Training loss: 2.7202281951904297
Validation loss: 2.6715941429138184

Epoch: 6| Step: 13
Training loss: 2.7910637855529785
Validation loss: 2.6685731410980225

Epoch: 44| Step: 0
Training loss: 2.881892442703247
Validation loss: 2.6655708948771157

Epoch: 6| Step: 1
Training loss: 2.2792162895202637
Validation loss: 2.663761536280314

Epoch: 6| Step: 2
Training loss: 3.1529293060302734
Validation loss: 2.6637046337127686

Epoch: 6| Step: 3
Training loss: 2.497378349304199
Validation loss: 2.6600992679595947

Epoch: 6| Step: 4
Training loss: 2.6227493286132812
Validation loss: 2.6609328190485635

Epoch: 6| Step: 5
Training loss: 2.8458030223846436
Validation loss: 2.654046972592672

Epoch: 6| Step: 6
Training loss: 3.3690083026885986
Validation loss: 2.6505763133366904

Epoch: 6| Step: 7
Training loss: 3.0448265075683594
Validation loss: 2.649114469687144

Epoch: 6| Step: 8
Training loss: 2.4627790451049805
Validation loss: 2.649032990137736

Epoch: 6| Step: 9
Training loss: 2.746817111968994
Validation loss: 2.647921085357666

Epoch: 6| Step: 10
Training loss: 3.0422048568725586
Validation loss: 2.654634197552999

Epoch: 6| Step: 11
Training loss: 3.1789095401763916
Validation loss: 2.644178628921509

Epoch: 6| Step: 12
Training loss: 2.6462502479553223
Validation loss: 2.637403587500254

Epoch: 6| Step: 13
Training loss: 3.20349383354187
Validation loss: 2.6319588820139566

Epoch: 45| Step: 0
Training loss: 2.9096765518188477
Validation loss: 2.6297492186228433

Epoch: 6| Step: 1
Training loss: 3.308471202850342
Validation loss: 2.625793178876241

Epoch: 6| Step: 2
Training loss: 2.6121902465820312
Validation loss: 2.6236495971679688

Epoch: 6| Step: 3
Training loss: 2.1164350509643555
Validation loss: 2.6216849088668823

Epoch: 6| Step: 4
Training loss: 2.317674160003662
Validation loss: 2.621765414873759

Epoch: 6| Step: 5
Training loss: 3.111358880996704
Validation loss: 2.624165415763855

Epoch: 6| Step: 6
Training loss: 3.0717387199401855
Validation loss: 2.6205590764681497

Epoch: 6| Step: 7
Training loss: 2.7893314361572266
Validation loss: 2.6161324183146157

Epoch: 6| Step: 8
Training loss: 3.386291027069092
Validation loss: 2.612300078074137

Epoch: 6| Step: 9
Training loss: 2.4966282844543457
Validation loss: 2.606162369251251

Epoch: 6| Step: 10
Training loss: 3.1791727542877197
Validation loss: 2.604569355646769

Epoch: 6| Step: 11
Training loss: 2.756362199783325
Validation loss: 2.6072747707366943

Epoch: 6| Step: 12
Training loss: 2.923431873321533
Validation loss: 2.597352067629496

Epoch: 6| Step: 13
Training loss: 2.4107046127319336
Validation loss: 2.588281830151876

Epoch: 46| Step: 0
Training loss: 2.678279399871826
Validation loss: 2.5891167322794595

Epoch: 6| Step: 1
Training loss: 2.7395272254943848
Validation loss: 2.5860735177993774

Epoch: 6| Step: 2
Training loss: 3.087867259979248
Validation loss: 2.588641365369161

Epoch: 6| Step: 3
Training loss: 3.354987382888794
Validation loss: 2.5887884298960366

Epoch: 6| Step: 4
Training loss: 2.1931400299072266
Validation loss: 2.582851012547811

Epoch: 6| Step: 5
Training loss: 2.9725570678710938
Validation loss: 2.580005685488383

Epoch: 6| Step: 6
Training loss: 3.0978312492370605
Validation loss: 2.5763309796651206

Epoch: 6| Step: 7
Training loss: 2.8405356407165527
Validation loss: 2.573218504587809

Epoch: 6| Step: 8
Training loss: 2.9954299926757812
Validation loss: 2.570352872212728

Epoch: 6| Step: 9
Training loss: 2.8462371826171875
Validation loss: 2.5673341751098633

Epoch: 6| Step: 10
Training loss: 1.8336139917373657
Validation loss: 2.564172347386678

Epoch: 6| Step: 11
Training loss: 2.3712518215179443
Validation loss: 2.5621629556020102

Epoch: 6| Step: 12
Training loss: 2.514010429382324
Validation loss: 2.557604710261027

Epoch: 6| Step: 13
Training loss: 3.2758688926696777
Validation loss: 2.556208292643229

Epoch: 47| Step: 0
Training loss: 2.5538406372070312
Validation loss: 2.552996496359507

Epoch: 6| Step: 1
Training loss: 2.7829465866088867
Validation loss: 2.5477312803268433

Epoch: 6| Step: 2
Training loss: 2.9902210235595703
Validation loss: 2.5458876689275107

Epoch: 6| Step: 3
Training loss: 2.6707253456115723
Validation loss: 2.5440721114476523

Epoch: 6| Step: 4
Training loss: 3.1936118602752686
Validation loss: 2.54353133837382

Epoch: 6| Step: 5
Training loss: 2.700387477874756
Validation loss: 2.5385071833928428

Epoch: 6| Step: 6
Training loss: 2.6418490409851074
Validation loss: 2.5379070043563843

Epoch: 6| Step: 7
Training loss: 2.710566282272339
Validation loss: 2.531889100869497

Epoch: 6| Step: 8
Training loss: 2.7638866901397705
Validation loss: 2.528765241305033

Epoch: 6| Step: 9
Training loss: 2.5880913734436035
Validation loss: 2.5264668464660645

Epoch: 6| Step: 10
Training loss: 2.4274020195007324
Validation loss: 2.5231901009877524

Epoch: 6| Step: 11
Training loss: 2.6117653846740723
Validation loss: 2.5247421860694885

Epoch: 6| Step: 12
Training loss: 2.9117393493652344
Validation loss: 2.5180503527323403

Epoch: 6| Step: 13
Training loss: 2.6617727279663086
Validation loss: 2.518941501776377

Epoch: 48| Step: 0
Training loss: 2.1978297233581543
Validation loss: 2.516695022583008

Epoch: 6| Step: 1
Training loss: 2.207598924636841
Validation loss: 2.517439047495524

Epoch: 6| Step: 2
Training loss: 2.357700824737549
Validation loss: 2.5231398344039917

Epoch: 6| Step: 3
Training loss: 2.9418740272521973
Validation loss: 2.516764243443807

Epoch: 6| Step: 4
Training loss: 2.4300498962402344
Validation loss: 2.5103946129480996

Epoch: 6| Step: 5
Training loss: 3.035825729370117
Validation loss: 2.507571220397949

Epoch: 6| Step: 6
Training loss: 2.660629987716675
Validation loss: 2.506118734677633

Epoch: 6| Step: 7
Training loss: 2.9464776515960693
Validation loss: 2.4972514112790427

Epoch: 6| Step: 8
Training loss: 3.0445756912231445
Validation loss: 2.495043377081553

Epoch: 6| Step: 9
Training loss: 2.3395626544952393
Validation loss: 2.495693564414978

Epoch: 6| Step: 10
Training loss: 3.4681148529052734
Validation loss: 2.4887486696243286

Epoch: 6| Step: 11
Training loss: 2.3800809383392334
Validation loss: 2.494719982147217

Epoch: 6| Step: 12
Training loss: 2.5406885147094727
Validation loss: 2.4931857188542685

Epoch: 6| Step: 13
Training loss: 3.10068678855896
Validation loss: 2.4882630904515586

Epoch: 49| Step: 0
Training loss: 2.615797758102417
Validation loss: 2.484210968017578

Epoch: 6| Step: 1
Training loss: 2.902675151824951
Validation loss: 2.4794321854909263

Epoch: 6| Step: 2
Training loss: 2.2910714149475098
Validation loss: 2.4740280707677207

Epoch: 6| Step: 3
Training loss: 2.034109592437744
Validation loss: 2.473982254664103

Epoch: 6| Step: 4
Training loss: 2.6907505989074707
Validation loss: 2.466533660888672

Epoch: 6| Step: 5
Training loss: 2.947354793548584
Validation loss: 2.4622644583384194

Epoch: 6| Step: 6
Training loss: 2.9335479736328125
Validation loss: 2.4600849548975625

Epoch: 6| Step: 7
Training loss: 2.777949810028076
Validation loss: 2.461578289667765

Epoch: 6| Step: 8
Training loss: 2.8446357250213623
Validation loss: 2.46247406800588

Epoch: 6| Step: 9
Training loss: 2.711848258972168
Validation loss: 2.455521742502848

Epoch: 6| Step: 10
Training loss: 3.0084288120269775
Validation loss: 2.4510603745778403

Epoch: 6| Step: 11
Training loss: 2.1737043857574463
Validation loss: 2.4489439328511557

Epoch: 6| Step: 12
Training loss: 2.5741467475891113
Validation loss: 2.4466758171717324

Epoch: 6| Step: 13
Training loss: 2.6499671936035156
Validation loss: 2.4432862997055054

Epoch: 50| Step: 0
Training loss: 2.2118148803710938
Validation loss: 2.444622198740641

Epoch: 6| Step: 1
Training loss: 2.68318510055542
Validation loss: 2.4438244104385376

Epoch: 6| Step: 2
Training loss: 2.8958499431610107
Validation loss: 2.4403894344965615

Epoch: 6| Step: 3
Training loss: 2.8693747520446777
Validation loss: 2.4399744272232056

Epoch: 6| Step: 4
Training loss: 2.981929302215576
Validation loss: 2.435296098391215

Epoch: 6| Step: 5
Training loss: 2.7423319816589355
Validation loss: 2.4338951110839844

Epoch: 6| Step: 6
Training loss: 2.43184757232666
Validation loss: 2.4317135016123452

Epoch: 6| Step: 7
Training loss: 2.415341377258301
Validation loss: 2.4311478535334268

Epoch: 6| Step: 8
Training loss: 1.6204532384872437
Validation loss: 2.4267738660176597

Epoch: 6| Step: 9
Training loss: 2.7635655403137207
Validation loss: 2.423229177792867

Epoch: 6| Step: 10
Training loss: 2.8429319858551025
Validation loss: 2.420951227347056

Epoch: 6| Step: 11
Training loss: 2.613454580307007
Validation loss: 2.418556491533915

Epoch: 6| Step: 12
Training loss: 2.6711320877075195
Validation loss: 2.4195854663848877

Epoch: 6| Step: 13
Training loss: 2.8483974933624268
Validation loss: 2.418371319770813

Epoch: 51| Step: 0
Training loss: 2.2671267986297607
Validation loss: 2.4159470001856485

Epoch: 6| Step: 1
Training loss: 2.295191764831543
Validation loss: 2.414239287376404

Epoch: 6| Step: 2
Training loss: 2.3443539142608643
Validation loss: 2.408006966114044

Epoch: 6| Step: 3
Training loss: 3.057600736618042
Validation loss: 2.4046332836151123

Epoch: 6| Step: 4
Training loss: 2.285470485687256
Validation loss: 2.406199892361959

Epoch: 6| Step: 5
Training loss: 2.455146312713623
Validation loss: 2.4042183558146157

Epoch: 6| Step: 6
Training loss: 2.6242592334747314
Validation loss: 2.3992050886154175

Epoch: 6| Step: 7
Training loss: 2.363616943359375
Validation loss: 2.3966315189997354

Epoch: 6| Step: 8
Training loss: 2.871428966522217
Validation loss: 2.391926368077596

Epoch: 6| Step: 9
Training loss: 3.027092456817627
Validation loss: 2.391295591990153

Epoch: 6| Step: 10
Training loss: 2.434791088104248
Validation loss: 2.3852819403012595

Epoch: 6| Step: 11
Training loss: 2.097607374191284
Validation loss: 2.387560327847799

Epoch: 6| Step: 12
Training loss: 3.0299172401428223
Validation loss: 2.3860788544019065

Epoch: 6| Step: 13
Training loss: 2.9759674072265625
Validation loss: 2.3798864285151162

Epoch: 52| Step: 0
Training loss: 2.609043598175049
Validation loss: 2.3781479597091675

Epoch: 6| Step: 1
Training loss: 2.2970986366271973
Validation loss: 2.37627249956131

Epoch: 6| Step: 2
Training loss: 2.905564546585083
Validation loss: 2.3782243529955545

Epoch: 6| Step: 3
Training loss: 2.491598129272461
Validation loss: 2.3723371227582297

Epoch: 6| Step: 4
Training loss: 2.2858824729919434
Validation loss: 2.3715951442718506

Epoch: 6| Step: 5
Training loss: 2.391770839691162
Validation loss: 2.368353803952535

Epoch: 6| Step: 6
Training loss: 2.065091609954834
Validation loss: 2.37151437997818

Epoch: 6| Step: 7
Training loss: 3.114354372024536
Validation loss: 2.3699716528256736

Epoch: 6| Step: 8
Training loss: 2.600144863128662
Validation loss: 2.3675785859425864

Epoch: 6| Step: 9
Training loss: 2.315424919128418
Validation loss: 2.363141576449076

Epoch: 6| Step: 10
Training loss: 3.291184663772583
Validation loss: 2.358157833417257

Epoch: 6| Step: 11
Training loss: 2.6422786712646484
Validation loss: 2.3541325330734253

Epoch: 6| Step: 12
Training loss: 2.708881139755249
Validation loss: 2.3492882450421653

Epoch: 6| Step: 13
Training loss: 1.8501337766647339
Validation loss: 2.3463743527730307

Epoch: 53| Step: 0
Training loss: 3.2075953483581543
Validation loss: 2.3461955388387046

Epoch: 6| Step: 1
Training loss: 2.1861801147460938
Validation loss: 2.3494020303090415

Epoch: 6| Step: 2
Training loss: 2.4727046489715576
Validation loss: 2.3499874075253806

Epoch: 6| Step: 3
Training loss: 2.327467918395996
Validation loss: 2.3477917512257895

Epoch: 6| Step: 4
Training loss: 2.291165351867676
Validation loss: 2.3353330294291177

Epoch: 6| Step: 5
Training loss: 2.778087854385376
Validation loss: 2.3343369364738464

Epoch: 6| Step: 6
Training loss: 2.9157958030700684
Validation loss: 2.3318528731664023

Epoch: 6| Step: 7
Training loss: 2.6890406608581543
Validation loss: 2.3307814995447793

Epoch: 6| Step: 8
Training loss: 2.1282780170440674
Validation loss: 2.3281362454096475

Epoch: 6| Step: 9
Training loss: 2.712686538696289
Validation loss: 2.324909428755442

Epoch: 6| Step: 10
Training loss: 2.933314085006714
Validation loss: 2.3231462041536965

Epoch: 6| Step: 11
Training loss: 2.5511903762817383
Validation loss: 2.3249815702438354

Epoch: 6| Step: 12
Training loss: 1.4992924928665161
Validation loss: 2.3219086130460105

Epoch: 6| Step: 13
Training loss: 2.4071924686431885
Validation loss: 2.3183510303497314

Epoch: 54| Step: 0
Training loss: 2.936121940612793
Validation loss: 2.319711526234945

Epoch: 6| Step: 1
Training loss: 2.495154857635498
Validation loss: 2.3199324210484824

Epoch: 6| Step: 2
Training loss: 1.8346607685089111
Validation loss: 2.3151325384775796

Epoch: 6| Step: 3
Training loss: 2.5447278022766113
Validation loss: 2.3146762450536094

Epoch: 6| Step: 4
Training loss: 2.848590850830078
Validation loss: 2.3145604729652405

Epoch: 6| Step: 5
Training loss: 2.6072731018066406
Validation loss: 2.3075289527575173

Epoch: 6| Step: 6
Training loss: 2.272754192352295
Validation loss: 2.305496335029602

Epoch: 6| Step: 7
Training loss: 2.5556905269622803
Validation loss: 2.3030871947606406

Epoch: 6| Step: 8
Training loss: 1.9860695600509644
Validation loss: 2.3043705821037292

Epoch: 6| Step: 9
Training loss: 2.555544853210449
Validation loss: 2.297231674194336

Epoch: 6| Step: 10
Training loss: 2.348994493484497
Validation loss: 2.292433818181356

Epoch: 6| Step: 11
Training loss: 2.646423816680908
Validation loss: 2.2925928831100464

Epoch: 6| Step: 12
Training loss: 2.547250747680664
Validation loss: 2.2907535632451377

Epoch: 6| Step: 13
Training loss: 2.4593849182128906
Validation loss: 2.2860880692799888

Epoch: 55| Step: 0
Training loss: 2.687382221221924
Validation loss: 2.283804714679718

Epoch: 6| Step: 1
Training loss: 2.6958603858947754
Validation loss: 2.2819788455963135

Epoch: 6| Step: 2
Training loss: 2.6865861415863037
Validation loss: 2.281555394331614

Epoch: 6| Step: 3
Training loss: 2.713104486465454
Validation loss: 2.281736115614573

Epoch: 6| Step: 4
Training loss: 2.5891802310943604
Validation loss: 2.274594267209371

Epoch: 6| Step: 5
Training loss: 2.156064033508301
Validation loss: 2.2754719654719033

Epoch: 6| Step: 6
Training loss: 2.364086866378784
Validation loss: 2.2750249107678733

Epoch: 6| Step: 7
Training loss: 2.6357476711273193
Validation loss: 2.2731264432271323

Epoch: 6| Step: 8
Training loss: 2.0750107765197754
Validation loss: 2.2685711781183877

Epoch: 6| Step: 9
Training loss: 2.48044490814209
Validation loss: 2.2644352515538535

Epoch: 6| Step: 10
Training loss: 2.612177610397339
Validation loss: 2.2675243417421975

Epoch: 6| Step: 11
Training loss: 2.3637051582336426
Validation loss: 2.2626311779022217

Epoch: 6| Step: 12
Training loss: 2.4533965587615967
Validation loss: 2.2573177218437195

Epoch: 6| Step: 13
Training loss: 1.6084446907043457
Validation loss: 2.254193683465322

Epoch: 56| Step: 0
Training loss: 2.710172176361084
Validation loss: 2.248037815093994

Epoch: 6| Step: 1
Training loss: 2.3172733783721924
Validation loss: 2.2464691003163657

Epoch: 6| Step: 2
Training loss: 1.7288639545440674
Validation loss: 2.244544247786204

Epoch: 6| Step: 3
Training loss: 2.284170150756836
Validation loss: 2.2457266648610434

Epoch: 6| Step: 4
Training loss: 2.54999041557312
Validation loss: 2.2434414426485696

Epoch: 6| Step: 5
Training loss: 2.7238008975982666
Validation loss: 2.239338437716166

Epoch: 6| Step: 6
Training loss: 2.6590709686279297
Validation loss: 2.2328087091445923

Epoch: 6| Step: 7
Training loss: 2.6226210594177246
Validation loss: 2.235032637914022

Epoch: 6| Step: 8
Training loss: 2.3622565269470215
Validation loss: 2.230314334233602

Epoch: 6| Step: 9
Training loss: 2.630969524383545
Validation loss: 2.2316503127415976

Epoch: 6| Step: 10
Training loss: 1.8631212711334229
Validation loss: 2.2316131393114724

Epoch: 6| Step: 11
Training loss: 2.3221726417541504
Validation loss: 2.2289243936538696

Epoch: 6| Step: 12
Training loss: 2.0699822902679443
Validation loss: 2.227562189102173

Epoch: 6| Step: 13
Training loss: 2.7320268154144287
Validation loss: 2.229931632677714

Epoch: 57| Step: 0
Training loss: 3.6828742027282715
Validation loss: 2.2257683078447976

Epoch: 6| Step: 1
Training loss: 2.1253786087036133
Validation loss: 2.220519542694092

Epoch: 6| Step: 2
Training loss: 2.6217308044433594
Validation loss: 2.2190928061803183

Epoch: 6| Step: 3
Training loss: 2.549898862838745
Validation loss: 2.218872547149658

Epoch: 6| Step: 4
Training loss: 2.55859112739563
Validation loss: 2.2146348357200623

Epoch: 6| Step: 5
Training loss: 2.1591835021972656
Validation loss: 2.2087133526802063

Epoch: 6| Step: 6
Training loss: 2.4632298946380615
Validation loss: 2.210904280344645

Epoch: 6| Step: 7
Training loss: 1.9360734224319458
Validation loss: 2.210024118423462

Epoch: 6| Step: 8
Training loss: 2.3047373294830322
Validation loss: 2.2082719206809998

Epoch: 6| Step: 9
Training loss: 2.41113543510437
Validation loss: 2.2091814279556274

Epoch: 6| Step: 10
Training loss: 1.5510233640670776
Validation loss: 2.2000885009765625

Epoch: 6| Step: 11
Training loss: 2.645150661468506
Validation loss: 2.203313191731771

Epoch: 6| Step: 12
Training loss: 2.1240572929382324
Validation loss: 2.1996911764144897

Epoch: 6| Step: 13
Training loss: 2.067408323287964
Validation loss: 2.201240380605062

Epoch: 58| Step: 0
Training loss: 2.2756154537200928
Validation loss: 2.1923197905222573

Epoch: 6| Step: 1
Training loss: 1.880751609802246
Validation loss: 2.1971464355786643

Epoch: 6| Step: 2
Training loss: 2.1531920433044434
Validation loss: 2.1917516589164734

Epoch: 6| Step: 3
Training loss: 2.3132739067077637
Validation loss: 2.194659690062205

Epoch: 6| Step: 4
Training loss: 2.468923568725586
Validation loss: 2.192112386226654

Epoch: 6| Step: 5
Training loss: 2.001072883605957
Validation loss: 2.1875741680463157

Epoch: 6| Step: 6
Training loss: 2.4126808643341064
Validation loss: 2.1900498469670615

Epoch: 6| Step: 7
Training loss: 2.5104377269744873
Validation loss: 2.1861400604248047

Epoch: 6| Step: 8
Training loss: 1.922999620437622
Validation loss: 2.1862359642982483

Epoch: 6| Step: 9
Training loss: 3.1256039142608643
Validation loss: 2.1857328613599143

Epoch: 6| Step: 10
Training loss: 2.2264885902404785
Validation loss: 2.1784118016560874

Epoch: 6| Step: 11
Training loss: 2.138991355895996
Validation loss: 2.181163469950358

Epoch: 6| Step: 12
Training loss: 1.9597222805023193
Validation loss: 2.172465225060781

Epoch: 6| Step: 13
Training loss: 3.4289968013763428
Validation loss: 2.166196584701538

Epoch: 59| Step: 0
Training loss: 1.7807594537734985
Validation loss: 2.1707500219345093

Epoch: 6| Step: 1
Training loss: 1.802569031715393
Validation loss: 2.1700056393941245

Epoch: 6| Step: 2
Training loss: 2.194120407104492
Validation loss: 2.1693098147710166

Epoch: 6| Step: 3
Training loss: 2.3110857009887695
Validation loss: 2.1692047715187073

Epoch: 6| Step: 4
Training loss: 2.187404155731201
Validation loss: 2.1667685508728027

Epoch: 6| Step: 5
Training loss: 2.5835742950439453
Validation loss: 2.1651891668637595

Epoch: 6| Step: 6
Training loss: 2.8115367889404297
Validation loss: 2.164586385091146

Epoch: 6| Step: 7
Training loss: 2.8500561714172363
Validation loss: 2.16070826848348

Epoch: 6| Step: 8
Training loss: 2.4451613426208496
Validation loss: 2.1609877546628318

Epoch: 6| Step: 9
Training loss: 2.104158401489258
Validation loss: 2.159940719604492

Epoch: 6| Step: 10
Training loss: 2.7333755493164062
Validation loss: 2.159742554028829

Epoch: 6| Step: 11
Training loss: 1.8465708494186401
Validation loss: 2.1605368852615356

Epoch: 6| Step: 12
Training loss: 2.422952651977539
Validation loss: 2.1793905099232993

Epoch: 6| Step: 13
Training loss: 2.469615936279297
Validation loss: 2.1784687836964927

Epoch: 60| Step: 0
Training loss: 2.001107692718506
Validation loss: 2.180589437484741

Epoch: 6| Step: 1
Training loss: 2.9767913818359375
Validation loss: 2.1799029111862183

Epoch: 6| Step: 2
Training loss: 2.167768955230713
Validation loss: 2.182730515797933

Epoch: 6| Step: 3
Training loss: 2.0131661891937256
Validation loss: 2.173042416572571

Epoch: 6| Step: 4
Training loss: 2.3204686641693115
Validation loss: 2.1630231738090515

Epoch: 6| Step: 5
Training loss: 2.0998404026031494
Validation loss: 2.161566456158956

Epoch: 6| Step: 6
Training loss: 2.391087293624878
Validation loss: 2.153674324353536

Epoch: 6| Step: 7
Training loss: 2.381190776824951
Validation loss: 2.1616970896720886

Epoch: 6| Step: 8
Training loss: 2.456787109375
Validation loss: 2.1565902630488076

Epoch: 6| Step: 9
Training loss: 1.8193914890289307
Validation loss: 2.1612898310025535

Epoch: 6| Step: 10
Training loss: 2.055898427963257
Validation loss: 2.1622817516326904

Epoch: 6| Step: 11
Training loss: 2.580643892288208
Validation loss: 2.1626060605049133

Epoch: 6| Step: 12
Training loss: 2.6584842205047607
Validation loss: 2.1577877203623452

Epoch: 6| Step: 13
Training loss: 2.608011245727539
Validation loss: 2.1595892707506814

Epoch: 61| Step: 0
Training loss: 1.8862173557281494
Validation loss: 2.1527300278345742

Epoch: 6| Step: 1
Training loss: 2.456702470779419
Validation loss: 2.149182399113973

Epoch: 6| Step: 2
Training loss: 1.716895580291748
Validation loss: 2.140829602877299

Epoch: 6| Step: 3
Training loss: 2.048494815826416
Validation loss: 2.1331894397735596

Epoch: 6| Step: 4
Training loss: 1.9731519222259521
Validation loss: 2.1350505550702414

Epoch: 6| Step: 5
Training loss: 2.5404906272888184
Validation loss: 2.133189618587494

Epoch: 6| Step: 6
Training loss: 2.541694164276123
Validation loss: 2.1372339328130088

Epoch: 6| Step: 7
Training loss: 2.3389110565185547
Validation loss: 2.1392772595087686

Epoch: 6| Step: 8
Training loss: 2.767815589904785
Validation loss: 2.135028819243113

Epoch: 6| Step: 9
Training loss: 1.9584729671478271
Validation loss: 2.131730238596598

Epoch: 6| Step: 10
Training loss: 2.877751350402832
Validation loss: 2.135510285695394

Epoch: 6| Step: 11
Training loss: 2.60734224319458
Validation loss: 2.1326545079549155

Epoch: 6| Step: 12
Training loss: 1.7451503276824951
Validation loss: 2.1328027645746865

Epoch: 6| Step: 13
Training loss: 2.691411018371582
Validation loss: 2.124878148237864

Epoch: 62| Step: 0
Training loss: 2.001908302307129
Validation loss: 2.127496898174286

Epoch: 6| Step: 1
Training loss: 3.0285086631774902
Validation loss: 2.118505080540975

Epoch: 6| Step: 2
Training loss: 2.028384208679199
Validation loss: 2.1159916122754416

Epoch: 6| Step: 3
Training loss: 3.2213621139526367
Validation loss: 2.1176536083221436

Epoch: 6| Step: 4
Training loss: 2.2924251556396484
Validation loss: 2.1224191387494407

Epoch: 6| Step: 5
Training loss: 1.7226872444152832
Validation loss: 2.117176870505015

Epoch: 6| Step: 6
Training loss: 2.1155786514282227
Validation loss: 2.122368633747101

Epoch: 6| Step: 7
Training loss: 2.9816818237304688
Validation loss: 2.115624407927195

Epoch: 6| Step: 8
Training loss: 1.9997825622558594
Validation loss: 2.116132835547129

Epoch: 6| Step: 9
Training loss: 2.4224586486816406
Validation loss: 2.1144705613454184

Epoch: 6| Step: 10
Training loss: 2.1586380004882812
Validation loss: 2.110313375790914

Epoch: 6| Step: 11
Training loss: 1.6390548944473267
Validation loss: 2.10916797320048

Epoch: 6| Step: 12
Training loss: 2.5644826889038086
Validation loss: 2.109296500682831

Epoch: 6| Step: 13
Training loss: 1.976670503616333
Validation loss: 2.105527321497599

Epoch: 63| Step: 0
Training loss: 2.278280735015869
Validation loss: 2.1044819355010986

Epoch: 6| Step: 1
Training loss: 3.0329151153564453
Validation loss: 2.1070605715115867

Epoch: 6| Step: 2
Training loss: 2.702432155609131
Validation loss: 2.103878676891327

Epoch: 6| Step: 3
Training loss: 1.7660973072052002
Validation loss: 2.10042397181193

Epoch: 6| Step: 4
Training loss: 2.1992318630218506
Validation loss: 2.0974287390708923

Epoch: 6| Step: 5
Training loss: 2.314596176147461
Validation loss: 2.106751481691996

Epoch: 6| Step: 6
Training loss: 1.6383154392242432
Validation loss: 2.122254232565562

Epoch: 6| Step: 7
Training loss: 1.4721906185150146
Validation loss: 2.1207806865374246

Epoch: 6| Step: 8
Training loss: 2.1910147666931152
Validation loss: 2.123825192451477

Epoch: 6| Step: 9
Training loss: 2.5181312561035156
Validation loss: 2.1250857512156167

Epoch: 6| Step: 10
Training loss: 2.76088285446167
Validation loss: 2.1296786268552146

Epoch: 6| Step: 11
Training loss: 2.3389852046966553
Validation loss: 2.119314750035604

Epoch: 6| Step: 12
Training loss: 2.5265040397644043
Validation loss: 2.118415574232737

Epoch: 6| Step: 13
Training loss: 2.395017147064209
Validation loss: 2.111775894959768

Epoch: 64| Step: 0
Training loss: 2.0308475494384766
Validation loss: 2.1073936025301614

Epoch: 6| Step: 1
Training loss: 2.313754081726074
Validation loss: 2.1023303270339966

Epoch: 6| Step: 2
Training loss: 1.6533777713775635
Validation loss: 2.10076767206192

Epoch: 6| Step: 3
Training loss: 2.293142080307007
Validation loss: 2.0957159797350564

Epoch: 6| Step: 4
Training loss: 2.240910768508911
Validation loss: 2.091818908850352

Epoch: 6| Step: 5
Training loss: 2.1245436668395996
Validation loss: 2.097390115261078

Epoch: 6| Step: 6
Training loss: 2.5554046630859375
Validation loss: 2.098810354868571

Epoch: 6| Step: 7
Training loss: 2.3071937561035156
Validation loss: 2.0918791691462197

Epoch: 6| Step: 8
Training loss: 2.4116923809051514
Validation loss: 2.101771374543508

Epoch: 6| Step: 9
Training loss: 2.683541774749756
Validation loss: 2.1112634340922036

Epoch: 6| Step: 10
Training loss: 2.682040214538574
Validation loss: 2.1034761468569436

Epoch: 6| Step: 11
Training loss: 1.9337619543075562
Validation loss: 2.1011482874552407

Epoch: 6| Step: 12
Training loss: 2.2866733074188232
Validation loss: 2.094135363896688

Epoch: 6| Step: 13
Training loss: 2.4784440994262695
Validation loss: 2.0891456405321756

Epoch: 65| Step: 0
Training loss: 2.6837360858917236
Validation loss: 2.090572694937388

Epoch: 6| Step: 1
Training loss: 2.3710572719573975
Validation loss: 2.0788633028666177

Epoch: 6| Step: 2
Training loss: 1.6398954391479492
Validation loss: 2.0761987765630088

Epoch: 6| Step: 3
Training loss: 1.5036542415618896
Validation loss: 2.0696192383766174

Epoch: 6| Step: 4
Training loss: 2.8978018760681152
Validation loss: 2.070677638053894

Epoch: 6| Step: 5
Training loss: 2.0844554901123047
Validation loss: 2.075164020061493

Epoch: 6| Step: 6
Training loss: 2.4100632667541504
Validation loss: 2.079951206843058

Epoch: 6| Step: 7
Training loss: 2.6242387294769287
Validation loss: 2.0797810753186545

Epoch: 6| Step: 8
Training loss: 2.4605023860931396
Validation loss: 2.080079217751821

Epoch: 6| Step: 9
Training loss: 2.070870876312256
Validation loss: 2.0763150453567505

Epoch: 6| Step: 10
Training loss: 2.1293745040893555
Validation loss: 2.080139180024465

Epoch: 6| Step: 11
Training loss: 2.657428741455078
Validation loss: 2.07659383614858

Epoch: 6| Step: 12
Training loss: 2.31858229637146
Validation loss: 2.074056386947632

Epoch: 6| Step: 13
Training loss: 1.7763152122497559
Validation loss: 2.06906392176946

Epoch: 66| Step: 0
Training loss: 2.265346050262451
Validation loss: 2.069150388240814

Epoch: 6| Step: 1
Training loss: 2.661825656890869
Validation loss: 2.069652020931244

Epoch: 6| Step: 2
Training loss: 1.9876264333724976
Validation loss: 2.0692131519317627

Epoch: 6| Step: 3
Training loss: 1.9107651710510254
Validation loss: 2.0690852204958596

Epoch: 6| Step: 4
Training loss: 2.136726140975952
Validation loss: 2.0635133385658264

Epoch: 6| Step: 5
Training loss: 2.108490467071533
Validation loss: 2.065259555975596

Epoch: 6| Step: 6
Training loss: 1.941542387008667
Validation loss: 2.0626765489578247

Epoch: 6| Step: 7
Training loss: 2.726632833480835
Validation loss: 2.069542646408081

Epoch: 6| Step: 8
Training loss: 1.8019500970840454
Validation loss: 2.0591400067011514

Epoch: 6| Step: 9
Training loss: 2.3846821784973145
Validation loss: 2.0616148114204407

Epoch: 6| Step: 10
Training loss: 2.3351550102233887
Validation loss: 2.0639195442199707

Epoch: 6| Step: 11
Training loss: 2.6469619274139404
Validation loss: 2.065489927927653

Epoch: 6| Step: 12
Training loss: 2.6463463306427
Validation loss: 2.0578855077425637

Epoch: 6| Step: 13
Training loss: 1.9255869388580322
Validation loss: 2.060205817222595

Epoch: 67| Step: 0
Training loss: 2.267457962036133
Validation loss: 2.066097299257914

Epoch: 6| Step: 1
Training loss: 2.5052061080932617
Validation loss: 2.057786703109741

Epoch: 6| Step: 2
Training loss: 2.36594820022583
Validation loss: 2.0633041063944497

Epoch: 6| Step: 3
Training loss: 2.04293155670166
Validation loss: 2.0590842763582864

Epoch: 6| Step: 4
Training loss: 1.9337711334228516
Validation loss: 2.0608040491739907

Epoch: 6| Step: 5
Training loss: 2.285701036453247
Validation loss: 2.054435054461161

Epoch: 6| Step: 6
Training loss: 2.5144968032836914
Validation loss: 2.0631028016408286

Epoch: 6| Step: 7
Training loss: 2.505605697631836
Validation loss: 2.0596124132474265

Epoch: 6| Step: 8
Training loss: 1.9205608367919922
Validation loss: 2.057992140452067

Epoch: 6| Step: 9
Training loss: 1.9654656648635864
Validation loss: 2.0528095165888467

Epoch: 6| Step: 10
Training loss: 1.9200208187103271
Validation loss: 2.0533700585365295

Epoch: 6| Step: 11
Training loss: 2.446010112762451
Validation loss: 2.0516044894854226

Epoch: 6| Step: 12
Training loss: 2.5512213706970215
Validation loss: 2.0563995838165283

Epoch: 6| Step: 13
Training loss: 2.1073813438415527
Validation loss: 2.055901308854421

Epoch: 68| Step: 0
Training loss: 1.2635382413864136
Validation loss: 2.0595861077308655

Epoch: 6| Step: 1
Training loss: 2.4095425605773926
Validation loss: 2.0507316986719766

Epoch: 6| Step: 2
Training loss: 1.928298830986023
Validation loss: 2.0468032360076904

Epoch: 6| Step: 3
Training loss: 1.8960072994232178
Validation loss: 2.0510268211364746

Epoch: 6| Step: 4
Training loss: 2.5244011878967285
Validation loss: 2.0410109758377075

Epoch: 6| Step: 5
Training loss: 1.856286883354187
Validation loss: 2.0531144936879477

Epoch: 6| Step: 6
Training loss: 2.4340662956237793
Validation loss: 2.059324026107788

Epoch: 6| Step: 7
Training loss: 2.5093154907226562
Validation loss: 2.062521000703176

Epoch: 6| Step: 8
Training loss: 2.4142017364501953
Validation loss: 2.0576518972714744

Epoch: 6| Step: 9
Training loss: 2.522639751434326
Validation loss: 2.0565261244773865

Epoch: 6| Step: 10
Training loss: 2.1659624576568604
Validation loss: 2.0549636681874595

Epoch: 6| Step: 11
Training loss: 2.1022865772247314
Validation loss: 2.0540804068247476

Epoch: 6| Step: 12
Training loss: 2.498412609100342
Validation loss: 2.051500082015991

Epoch: 6| Step: 13
Training loss: 2.757537364959717
Validation loss: 2.0476601123809814

Epoch: 69| Step: 0
Training loss: 2.6943752765655518
Validation loss: 2.0411925315856934

Epoch: 6| Step: 1
Training loss: 2.3387985229492188
Validation loss: 2.0414170225461326

Epoch: 6| Step: 2
Training loss: 1.569838523864746
Validation loss: 2.041698932647705

Epoch: 6| Step: 3
Training loss: 2.623554229736328
Validation loss: 2.0355657736460366

Epoch: 6| Step: 4
Training loss: 1.8136316537857056
Validation loss: 2.044454793135325

Epoch: 6| Step: 5
Training loss: 2.1828360557556152
Validation loss: 2.0538793802261353

Epoch: 6| Step: 6
Training loss: 1.8569204807281494
Validation loss: 2.0746766130129495

Epoch: 6| Step: 7
Training loss: 2.672816514968872
Validation loss: 2.1002298990885415

Epoch: 6| Step: 8
Training loss: 2.365849494934082
Validation loss: 2.1118380228678384

Epoch: 6| Step: 9
Training loss: 1.817358374595642
Validation loss: 2.103217919667562

Epoch: 6| Step: 10
Training loss: 2.200894832611084
Validation loss: 2.078979730606079

Epoch: 6| Step: 11
Training loss: 2.5596981048583984
Validation loss: 2.050313413143158

Epoch: 6| Step: 12
Training loss: 2.028710126876831
Validation loss: 2.044241786003113

Epoch: 6| Step: 13
Training loss: 2.757526397705078
Validation loss: 2.054185688495636

Epoch: 70| Step: 0
Training loss: 2.411172866821289
Validation loss: 2.0578957200050354

Epoch: 6| Step: 1
Training loss: 2.031939744949341
Validation loss: 2.0693543354670205

Epoch: 6| Step: 2
Training loss: 2.11134672164917
Validation loss: 2.074696401755015

Epoch: 6| Step: 3
Training loss: 2.8249197006225586
Validation loss: 2.083892544110616

Epoch: 6| Step: 4
Training loss: 2.6550498008728027
Validation loss: 2.0899768471717834

Epoch: 6| Step: 5
Training loss: 2.993809223175049
Validation loss: 2.090958575407664

Epoch: 6| Step: 6
Training loss: 2.3700008392333984
Validation loss: 2.086905380090078

Epoch: 6| Step: 7
Training loss: 2.6504831314086914
Validation loss: 2.081371784210205

Epoch: 6| Step: 8
Training loss: 1.3191075325012207
Validation loss: 2.0732412536938987

Epoch: 6| Step: 9
Training loss: 2.1878857612609863
Validation loss: 2.0721022288004556

Epoch: 6| Step: 10
Training loss: 2.0873794555664062
Validation loss: 2.0698086420694985

Epoch: 6| Step: 11
Training loss: 2.3414628505706787
Validation loss: 2.071490486462911

Epoch: 6| Step: 12
Training loss: 2.0704550743103027
Validation loss: 2.0621784130732217

Epoch: 6| Step: 13
Training loss: 1.6090987920761108
Validation loss: 2.0684622327486673

Epoch: 71| Step: 0
Training loss: 2.0268115997314453
Validation loss: 2.0640528400739035

Epoch: 6| Step: 1
Training loss: 1.9667813777923584
Validation loss: 2.067127247651418

Epoch: 6| Step: 2
Training loss: 1.7380125522613525
Validation loss: 2.058609426021576

Epoch: 6| Step: 3
Training loss: 2.247459650039673
Validation loss: 2.0569051106770835

Epoch: 6| Step: 4
Training loss: 2.4927515983581543
Validation loss: 2.0495027701059976

Epoch: 6| Step: 5
Training loss: 2.730013132095337
Validation loss: 2.0497503081957498

Epoch: 6| Step: 6
Training loss: 2.4027504920959473
Validation loss: 2.048472245534261

Epoch: 6| Step: 7
Training loss: 2.510201930999756
Validation loss: 2.0394147634506226

Epoch: 6| Step: 8
Training loss: 2.709597110748291
Validation loss: 2.035528838634491

Epoch: 6| Step: 9
Training loss: 1.8144011497497559
Validation loss: 2.041650434335073

Epoch: 6| Step: 10
Training loss: 2.0673112869262695
Validation loss: 2.057903985182444

Epoch: 6| Step: 11
Training loss: 2.379732608795166
Validation loss: 2.0642104744911194

Epoch: 6| Step: 12
Training loss: 2.2076339721679688
Validation loss: 2.0550673405329385

Epoch: 6| Step: 13
Training loss: 1.8108850717544556
Validation loss: 2.0474295020103455

Epoch: 72| Step: 0
Training loss: 2.0264668464660645
Validation loss: 2.0464097460110984

Epoch: 6| Step: 1
Training loss: 2.085151433944702
Validation loss: 2.042719304561615

Epoch: 6| Step: 2
Training loss: 2.213420867919922
Validation loss: 2.044400691986084

Epoch: 6| Step: 3
Training loss: 1.7703676223754883
Validation loss: 2.0592743953069053

Epoch: 6| Step: 4
Training loss: 2.5849146842956543
Validation loss: 2.0472297271092734

Epoch: 6| Step: 5
Training loss: 2.401700973510742
Validation loss: 2.053447147210439

Epoch: 6| Step: 6
Training loss: 1.7874908447265625
Validation loss: 2.0417364835739136

Epoch: 6| Step: 7
Training loss: 1.8918848037719727
Validation loss: 2.029046336809794

Epoch: 6| Step: 8
Training loss: 2.663325786590576
Validation loss: 2.0286160906155906

Epoch: 6| Step: 9
Training loss: 1.6646933555603027
Validation loss: 2.0181405345598855

Epoch: 6| Step: 10
Training loss: 2.6395654678344727
Validation loss: 2.033586005369822

Epoch: 6| Step: 11
Training loss: 2.5716733932495117
Validation loss: 2.0437729756037393

Epoch: 6| Step: 12
Training loss: 2.1607422828674316
Validation loss: 2.0517945090929666

Epoch: 6| Step: 13
Training loss: 2.5948009490966797
Validation loss: 2.0627732078234353

Epoch: 73| Step: 0
Training loss: 2.9912023544311523
Validation loss: 2.0685444275538125

Epoch: 6| Step: 1
Training loss: 2.4063076972961426
Validation loss: 2.071895162264506

Epoch: 6| Step: 2
Training loss: 1.747819423675537
Validation loss: 2.0743568936983743

Epoch: 6| Step: 3
Training loss: 2.1529297828674316
Validation loss: 2.054645220438639

Epoch: 6| Step: 4
Training loss: 2.4369397163391113
Validation loss: 2.051272471745809

Epoch: 6| Step: 5
Training loss: 2.277621269226074
Validation loss: 2.0399468541145325

Epoch: 6| Step: 6
Training loss: 1.5100898742675781
Validation loss: 2.034978965918223

Epoch: 6| Step: 7
Training loss: 2.1444478034973145
Validation loss: 2.023858447869619

Epoch: 6| Step: 8
Training loss: 1.9945181608200073
Validation loss: 2.025711238384247

Epoch: 6| Step: 9
Training loss: 2.666746139526367
Validation loss: 2.0164455572764077

Epoch: 6| Step: 10
Training loss: 1.6678820848464966
Validation loss: 2.014266033967336

Epoch: 6| Step: 11
Training loss: 2.231001377105713
Validation loss: 2.02012566725413

Epoch: 6| Step: 12
Training loss: 2.9576268196105957
Validation loss: 2.0214904149373374

Epoch: 6| Step: 13
Training loss: 2.0769572257995605
Validation loss: 2.0157523353894553

Epoch: 74| Step: 0
Training loss: 2.448086738586426
Validation loss: 2.025644302368164

Epoch: 6| Step: 1
Training loss: 2.0402464866638184
Validation loss: 2.0235066016515098

Epoch: 6| Step: 2
Training loss: 2.5736148357391357
Validation loss: 2.0205559134483337

Epoch: 6| Step: 3
Training loss: 2.4851908683776855
Validation loss: 2.0181307593981423

Epoch: 6| Step: 4
Training loss: 1.987694263458252
Validation loss: 2.024234652519226

Epoch: 6| Step: 5
Training loss: 2.4316821098327637
Validation loss: 2.0195077459017434

Epoch: 6| Step: 6
Training loss: 1.803483009338379
Validation loss: 2.025679330031077

Epoch: 6| Step: 7
Training loss: 2.6630020141601562
Validation loss: 2.0201446413993835

Epoch: 6| Step: 8
Training loss: 1.892138957977295
Validation loss: 2.019067128499349

Epoch: 6| Step: 9
Training loss: 2.289705276489258
Validation loss: 2.028092344601949

Epoch: 6| Step: 10
Training loss: 1.9197008609771729
Validation loss: 2.0220221082369485

Epoch: 6| Step: 11
Training loss: 1.9871532917022705
Validation loss: 2.039819320042928

Epoch: 6| Step: 12
Training loss: 2.4874463081359863
Validation loss: 2.029189964135488

Epoch: 6| Step: 13
Training loss: 1.9453527927398682
Validation loss: 2.0222293933232627

Epoch: 75| Step: 0
Training loss: 1.8284013271331787
Validation loss: 2.020880659421285

Epoch: 6| Step: 1
Training loss: 2.2707667350769043
Validation loss: 2.018909136454264

Epoch: 6| Step: 2
Training loss: 2.2767446041107178
Validation loss: 2.0175225138664246

Epoch: 6| Step: 3
Training loss: 2.081510543823242
Validation loss: 2.0198477705319724

Epoch: 6| Step: 4
Training loss: 2.1220641136169434
Validation loss: 2.0164949099222818

Epoch: 6| Step: 5
Training loss: 2.1565818786621094
Validation loss: 2.014142076174418

Epoch: 6| Step: 6
Training loss: 1.8002015352249146
Validation loss: 2.010854740937551

Epoch: 6| Step: 7
Training loss: 2.6606316566467285
Validation loss: 2.012964447339376

Epoch: 6| Step: 8
Training loss: 2.646758794784546
Validation loss: 2.016894360383352

Epoch: 6| Step: 9
Training loss: 2.063121795654297
Validation loss: 2.0083958307902017

Epoch: 6| Step: 10
Training loss: 1.8850133419036865
Validation loss: 2.0075356364250183

Epoch: 6| Step: 11
Training loss: 2.4111342430114746
Validation loss: 2.014970064163208

Epoch: 6| Step: 12
Training loss: 2.571014165878296
Validation loss: 2.0222176909446716

Epoch: 6| Step: 13
Training loss: 1.9712677001953125
Validation loss: 2.0257916847864785

Epoch: 76| Step: 0
Training loss: 2.6106719970703125
Validation loss: 2.022477666536967

Epoch: 6| Step: 1
Training loss: 2.5113868713378906
Validation loss: 2.0143599112828574

Epoch: 6| Step: 2
Training loss: 1.9266655445098877
Validation loss: 2.011841615041097

Epoch: 6| Step: 3
Training loss: 1.5888514518737793
Validation loss: 2.012754956881205

Epoch: 6| Step: 4
Training loss: 2.1344051361083984
Validation loss: 2.0124858220418296

Epoch: 6| Step: 5
Training loss: 2.469538450241089
Validation loss: 2.0185507337252298

Epoch: 6| Step: 6
Training loss: 2.1047377586364746
Validation loss: 2.024819016456604

Epoch: 6| Step: 7
Training loss: 2.1421892642974854
Validation loss: 2.024858017762502

Epoch: 6| Step: 8
Training loss: 1.2772202491760254
Validation loss: 2.0317800839742026

Epoch: 6| Step: 9
Training loss: 2.877249002456665
Validation loss: 2.019469161828359

Epoch: 6| Step: 10
Training loss: 2.4774510860443115
Validation loss: 2.0115370750427246

Epoch: 6| Step: 11
Training loss: 2.3918044567108154
Validation loss: 2.019704739252726

Epoch: 6| Step: 12
Training loss: 1.7588187456130981
Validation loss: 2.0102760593096414

Epoch: 6| Step: 13
Training loss: 2.3770411014556885
Validation loss: 2.0184765259424844

Epoch: 77| Step: 0
Training loss: 1.7435463666915894
Validation loss: 2.0148865779240928

Epoch: 6| Step: 1
Training loss: 2.167405366897583
Validation loss: 2.0175071159998574

Epoch: 6| Step: 2
Training loss: 1.9129745960235596
Validation loss: 2.019212524096171

Epoch: 6| Step: 3
Training loss: 2.1621580123901367
Validation loss: 2.021694997946421

Epoch: 6| Step: 4
Training loss: 3.0856990814208984
Validation loss: 2.031011164188385

Epoch: 6| Step: 5
Training loss: 1.922163724899292
Validation loss: 2.036146819591522

Epoch: 6| Step: 6
Training loss: 2.5692715644836426
Validation loss: 2.0314550201098123

Epoch: 6| Step: 7
Training loss: 2.3415825366973877
Validation loss: 2.0386582612991333

Epoch: 6| Step: 8
Training loss: 2.0445961952209473
Validation loss: 2.030088245868683

Epoch: 6| Step: 9
Training loss: 1.8023895025253296
Validation loss: 2.0367099046707153

Epoch: 6| Step: 10
Training loss: 2.351081371307373
Validation loss: 2.025598963101705

Epoch: 6| Step: 11
Training loss: 2.017307758331299
Validation loss: 2.012625435988108

Epoch: 6| Step: 12
Training loss: 2.119205951690674
Validation loss: 2.013198892275492

Epoch: 6| Step: 13
Training loss: 2.1887197494506836
Validation loss: 2.0112485885620117

Epoch: 78| Step: 0
Training loss: 1.8403937816619873
Validation loss: 2.0192068020502725

Epoch: 6| Step: 1
Training loss: 2.0810348987579346
Validation loss: 2.008307933807373

Epoch: 6| Step: 2
Training loss: 2.2282371520996094
Validation loss: 2.008908192316691

Epoch: 6| Step: 3
Training loss: 2.1516833305358887
Validation loss: 2.014631430308024

Epoch: 6| Step: 4
Training loss: 1.9384340047836304
Validation loss: 2.0174370805422464

Epoch: 6| Step: 5
Training loss: 2.5668528079986572
Validation loss: 2.0120604634284973

Epoch: 6| Step: 6
Training loss: 2.045055866241455
Validation loss: 2.0094181497891745

Epoch: 6| Step: 7
Training loss: 2.259854555130005
Validation loss: 2.013608396053314

Epoch: 6| Step: 8
Training loss: 2.11379337310791
Validation loss: 2.01460854212443

Epoch: 6| Step: 9
Training loss: 2.464132308959961
Validation loss: 2.0162660678227744

Epoch: 6| Step: 10
Training loss: 1.6595871448516846
Validation loss: 2.015418271223704

Epoch: 6| Step: 11
Training loss: 2.120861768722534
Validation loss: 2.021796941757202

Epoch: 6| Step: 12
Training loss: 2.153017044067383
Validation loss: 2.0357741316159568

Epoch: 6| Step: 13
Training loss: 2.8241450786590576
Validation loss: 2.0400370558102927

Epoch: 79| Step: 0
Training loss: 2.135763168334961
Validation loss: 2.0367493629455566

Epoch: 6| Step: 1
Training loss: 1.5786375999450684
Validation loss: 2.0482050577799478

Epoch: 6| Step: 2
Training loss: 2.151649236679077
Validation loss: 2.037001649538676

Epoch: 6| Step: 3
Training loss: 2.243638515472412
Validation loss: 2.0512668093045554

Epoch: 6| Step: 4
Training loss: 2.5469603538513184
Validation loss: 2.0551964044570923

Epoch: 6| Step: 5
Training loss: 2.732883930206299
Validation loss: 2.057256042957306

Epoch: 6| Step: 6
Training loss: 2.1982860565185547
Validation loss: 2.049850583076477

Epoch: 6| Step: 7
Training loss: 1.844590663909912
Validation loss: 2.0356990893681846

Epoch: 6| Step: 8
Training loss: 2.8189446926116943
Validation loss: 2.0345301032066345

Epoch: 6| Step: 9
Training loss: 1.9862396717071533
Validation loss: 2.0275574127833047

Epoch: 6| Step: 10
Training loss: 2.170881748199463
Validation loss: 2.016918957233429

Epoch: 6| Step: 11
Training loss: 1.7923953533172607
Validation loss: 2.010522782802582

Epoch: 6| Step: 12
Training loss: 2.1027870178222656
Validation loss: 2.0197102626164756

Epoch: 6| Step: 13
Training loss: 2.3370914459228516
Validation loss: 2.01612381140391

Epoch: 80| Step: 0
Training loss: 2.0640647411346436
Validation loss: 2.029464900493622

Epoch: 6| Step: 1
Training loss: 1.5832310914993286
Validation loss: 2.0314465165138245

Epoch: 6| Step: 2
Training loss: 1.649519920349121
Validation loss: 2.029072324434916

Epoch: 6| Step: 3
Training loss: 2.2532827854156494
Validation loss: 2.03847728172938

Epoch: 6| Step: 4
Training loss: 2.2599048614501953
Validation loss: 2.038848042488098

Epoch: 6| Step: 5
Training loss: 2.6562161445617676
Validation loss: 2.036136746406555

Epoch: 6| Step: 6
Training loss: 2.1090691089630127
Validation loss: 2.0350863536198935

Epoch: 6| Step: 7
Training loss: 2.57890248298645
Validation loss: 2.0329473416010537

Epoch: 6| Step: 8
Training loss: 1.8981120586395264
Validation loss: 2.033696432908376

Epoch: 6| Step: 9
Training loss: 2.583674430847168
Validation loss: 2.0248010953267417

Epoch: 6| Step: 10
Training loss: 2.4068799018859863
Validation loss: 2.0118072231610618

Epoch: 6| Step: 11
Training loss: 1.8771388530731201
Validation loss: 2.0136361519495645

Epoch: 6| Step: 12
Training loss: 2.2105774879455566
Validation loss: 2.0185208717981973

Epoch: 6| Step: 13
Training loss: 2.612718105316162
Validation loss: 2.022957980632782

Epoch: 81| Step: 0
Training loss: 2.499898910522461
Validation loss: 2.0285117824872336

Epoch: 6| Step: 1
Training loss: 2.1953840255737305
Validation loss: 2.0387953718503318

Epoch: 6| Step: 2
Training loss: 2.4869093894958496
Validation loss: 2.0497272411982217

Epoch: 6| Step: 3
Training loss: 2.011988639831543
Validation loss: 2.0327597657839456

Epoch: 6| Step: 4
Training loss: 1.5340114831924438
Validation loss: 2.0286983251571655

Epoch: 6| Step: 5
Training loss: 2.0727860927581787
Validation loss: 2.0402989387512207

Epoch: 6| Step: 6
Training loss: 2.1437606811523438
Validation loss: 2.0380612214406333

Epoch: 6| Step: 7
Training loss: 1.8057327270507812
Validation loss: 2.0276702642440796

Epoch: 6| Step: 8
Training loss: 2.5304598808288574
Validation loss: 2.015814224878947

Epoch: 6| Step: 9
Training loss: 2.2819266319274902
Validation loss: 2.0117522279421487

Epoch: 6| Step: 10
Training loss: 1.9464259147644043
Validation loss: 2.016597867012024

Epoch: 6| Step: 11
Training loss: 2.5215237140655518
Validation loss: 2.0140836238861084

Epoch: 6| Step: 12
Training loss: 2.2474470138549805
Validation loss: 2.018695871035258

Epoch: 6| Step: 13
Training loss: 1.9568122625350952
Validation loss: 2.018689672152201

Epoch: 82| Step: 0
Training loss: 2.283094882965088
Validation loss: 2.017202138900757

Epoch: 6| Step: 1
Training loss: 2.2178778648376465
Validation loss: 2.0192659298578897

Epoch: 6| Step: 2
Training loss: 2.6362199783325195
Validation loss: 2.0137643416722617

Epoch: 6| Step: 3
Training loss: 1.6832809448242188
Validation loss: 2.0205169916152954

Epoch: 6| Step: 4
Training loss: 2.5591139793395996
Validation loss: 2.0187769730885825

Epoch: 6| Step: 5
Training loss: 2.043083667755127
Validation loss: 2.012726664543152

Epoch: 6| Step: 6
Training loss: 2.2424662113189697
Validation loss: 2.0094621976216636

Epoch: 6| Step: 7
Training loss: 1.9913437366485596
Validation loss: 2.0092461506525674

Epoch: 6| Step: 8
Training loss: 2.2256152629852295
Validation loss: 2.004051705201467

Epoch: 6| Step: 9
Training loss: 1.6787625551223755
Validation loss: 2.0029825369517007

Epoch: 6| Step: 10
Training loss: 2.5960826873779297
Validation loss: 2.0134947697321572

Epoch: 6| Step: 11
Training loss: 2.5242669582366943
Validation loss: 2.023406684398651

Epoch: 6| Step: 12
Training loss: 1.901907205581665
Validation loss: 2.0164587100346885

Epoch: 6| Step: 13
Training loss: 1.889136791229248
Validation loss: 2.0255719224611917

Epoch: 83| Step: 0
Training loss: 1.918291687965393
Validation loss: 2.0247132778167725

Epoch: 6| Step: 1
Training loss: 1.8543468713760376
Validation loss: 2.0275606910387673

Epoch: 6| Step: 2
Training loss: 1.8016865253448486
Validation loss: 2.0290212829907737

Epoch: 6| Step: 3
Training loss: 2.7467880249023438
Validation loss: 2.0202096700668335

Epoch: 6| Step: 4
Training loss: 2.011050224304199
Validation loss: 2.016956408818563

Epoch: 6| Step: 5
Training loss: 1.8562355041503906
Validation loss: 2.017932097117106

Epoch: 6| Step: 6
Training loss: 2.742321014404297
Validation loss: 2.0022154450416565

Epoch: 6| Step: 7
Training loss: 2.142749071121216
Validation loss: 2.0151127179463706

Epoch: 6| Step: 8
Training loss: 2.4867992401123047
Validation loss: 2.0054203867912292

Epoch: 6| Step: 9
Training loss: 2.299236297607422
Validation loss: 2.0209996501604715

Epoch: 6| Step: 10
Training loss: 2.0328104496002197
Validation loss: 2.0077842076619468

Epoch: 6| Step: 11
Training loss: 1.6749610900878906
Validation loss: 2.022589087486267

Epoch: 6| Step: 12
Training loss: 2.441040277481079
Validation loss: 2.035416225592295

Epoch: 6| Step: 13
Training loss: 2.2428252696990967
Validation loss: 2.0270451505978904

Epoch: 84| Step: 0
Training loss: 2.3691322803497314
Validation loss: 2.02963395913442

Epoch: 6| Step: 1
Training loss: 1.8939846754074097
Validation loss: 2.040556013584137

Epoch: 6| Step: 2
Training loss: 2.0513343811035156
Validation loss: 2.0362589955329895

Epoch: 6| Step: 3
Training loss: 2.4389185905456543
Validation loss: 2.0366037885348

Epoch: 6| Step: 4
Training loss: 2.296889066696167
Validation loss: 2.0215160250663757

Epoch: 6| Step: 5
Training loss: 1.7923486232757568
Validation loss: 2.0117786725362143

Epoch: 6| Step: 6
Training loss: 1.736640214920044
Validation loss: 2.0117023587226868

Epoch: 6| Step: 7
Training loss: 1.7547156810760498
Validation loss: 2.008463521798452

Epoch: 6| Step: 8
Training loss: 2.245513439178467
Validation loss: 2.012178897857666

Epoch: 6| Step: 9
Training loss: 2.168334722518921
Validation loss: 2.016929805278778

Epoch: 6| Step: 10
Training loss: 1.4981417655944824
Validation loss: 2.001549184322357

Epoch: 6| Step: 11
Training loss: 2.7583365440368652
Validation loss: 2.006578723589579

Epoch: 6| Step: 12
Training loss: 2.2725119590759277
Validation loss: 2.0138526558876038

Epoch: 6| Step: 13
Training loss: 3.076533317565918
Validation loss: 2.011059125264486

Epoch: 85| Step: 0
Training loss: 2.02657151222229
Validation loss: 2.025536040465037

Epoch: 6| Step: 1
Training loss: 2.2830801010131836
Validation loss: 2.0468828876813254

Epoch: 6| Step: 2
Training loss: 2.3845596313476562
Validation loss: 2.0559412638346353

Epoch: 6| Step: 3
Training loss: 2.2237045764923096
Validation loss: 2.065952777862549

Epoch: 6| Step: 4
Training loss: 3.060061454772949
Validation loss: 2.059392511844635

Epoch: 6| Step: 5
Training loss: 2.1421103477478027
Validation loss: 2.042391777038574

Epoch: 6| Step: 6
Training loss: 2.1484627723693848
Validation loss: 2.041467229525248

Epoch: 6| Step: 7
Training loss: 1.8711915016174316
Validation loss: 2.0258238911628723

Epoch: 6| Step: 8
Training loss: 1.846792459487915
Validation loss: 2.024204949537913

Epoch: 6| Step: 9
Training loss: 1.979539394378662
Validation loss: 2.0193108717600503

Epoch: 6| Step: 10
Training loss: 2.143765449523926
Validation loss: 2.0130894780158997

Epoch: 6| Step: 11
Training loss: 1.463883638381958
Validation loss: 2.0165037314097085

Epoch: 6| Step: 12
Training loss: 2.463543176651001
Validation loss: 2.015281101067861

Epoch: 6| Step: 13
Training loss: 2.2059359550476074
Validation loss: 2.0143055518468223

Epoch: 86| Step: 0
Training loss: 1.898620367050171
Validation loss: 2.016414523124695

Epoch: 6| Step: 1
Training loss: 1.7536706924438477
Validation loss: 2.023731529712677

Epoch: 6| Step: 2
Training loss: 1.5286625623703003
Validation loss: 2.0185603300730386

Epoch: 6| Step: 3
Training loss: 2.1737618446350098
Validation loss: 2.0225780407587686

Epoch: 6| Step: 4
Training loss: 2.287322521209717
Validation loss: 2.030903915564219

Epoch: 6| Step: 5
Training loss: 2.3872594833374023
Validation loss: 2.0222710172335305

Epoch: 6| Step: 6
Training loss: 1.9697110652923584
Validation loss: 2.0227832794189453

Epoch: 6| Step: 7
Training loss: 2.574552059173584
Validation loss: 2.024088124434153

Epoch: 6| Step: 8
Training loss: 2.5559403896331787
Validation loss: 2.021180729071299

Epoch: 6| Step: 9
Training loss: 2.077238082885742
Validation loss: 2.0176676710446677

Epoch: 6| Step: 10
Training loss: 2.22087025642395
Validation loss: 2.0173781911532083

Epoch: 6| Step: 11
Training loss: 1.986893892288208
Validation loss: 2.0144832730293274

Epoch: 6| Step: 12
Training loss: 2.9186713695526123
Validation loss: 2.0282583832740784

Epoch: 6| Step: 13
Training loss: 1.9675641059875488
Validation loss: 2.037865161895752

Epoch: 87| Step: 0
Training loss: 1.3749289512634277
Validation loss: 2.0459607442220054

Epoch: 6| Step: 1
Training loss: 2.708993911743164
Validation loss: 2.0301159222920737

Epoch: 6| Step: 2
Training loss: 2.3178889751434326
Validation loss: 2.0440666675567627

Epoch: 6| Step: 3
Training loss: 2.65427303314209
Validation loss: 2.046245038509369

Epoch: 6| Step: 4
Training loss: 2.2343521118164062
Validation loss: 2.050231138865153

Epoch: 6| Step: 5
Training loss: 1.864903211593628
Validation loss: 2.045216461022695

Epoch: 6| Step: 6
Training loss: 1.964707851409912
Validation loss: 2.0250123341878257

Epoch: 6| Step: 7
Training loss: 2.133458375930786
Validation loss: 2.018553892771403

Epoch: 6| Step: 8
Training loss: 2.155660390853882
Validation loss: 2.0117363135019937

Epoch: 6| Step: 9
Training loss: 2.0549380779266357
Validation loss: 2.016306757926941

Epoch: 6| Step: 10
Training loss: 2.1430516242980957
Validation loss: 2.0163023869196572

Epoch: 6| Step: 11
Training loss: 2.3058934211730957
Validation loss: 2.0146148602167764

Epoch: 6| Step: 12
Training loss: 2.2252068519592285
Validation loss: 2.019165555636088

Epoch: 6| Step: 13
Training loss: 2.07472825050354
Validation loss: 2.01560777425766

Epoch: 88| Step: 0
Training loss: 2.425840377807617
Validation loss: 2.0206916133562722

Epoch: 6| Step: 1
Training loss: 2.472259998321533
Validation loss: 2.0229173501332602

Epoch: 6| Step: 2
Training loss: 2.5913825035095215
Validation loss: 2.0257322192192078

Epoch: 6| Step: 3
Training loss: 1.2445791959762573
Validation loss: 2.0192984342575073

Epoch: 6| Step: 4
Training loss: 1.7072625160217285
Validation loss: 2.022363841533661

Epoch: 6| Step: 5
Training loss: 2.385099411010742
Validation loss: 2.0259525775909424

Epoch: 6| Step: 6
Training loss: 1.9242724180221558
Validation loss: 2.0174249609311423

Epoch: 6| Step: 7
Training loss: 2.140565872192383
Validation loss: 2.0206778049468994

Epoch: 6| Step: 8
Training loss: 1.591231346130371
Validation loss: 2.013513962427775

Epoch: 6| Step: 9
Training loss: 2.2037758827209473
Validation loss: 2.0142439802487693

Epoch: 6| Step: 10
Training loss: 2.291403293609619
Validation loss: 2.014980673789978

Epoch: 6| Step: 11
Training loss: 2.858957290649414
Validation loss: 2.012536327044169

Epoch: 6| Step: 12
Training loss: 1.9387351274490356
Validation loss: 2.0239855448404946

Epoch: 6| Step: 13
Training loss: 2.389603614807129
Validation loss: 2.0122179786364236

Epoch: 89| Step: 0
Training loss: 2.249314546585083
Validation loss: 2.0351306001345315

Epoch: 6| Step: 1
Training loss: 2.4849820137023926
Validation loss: 2.0350950757662454

Epoch: 6| Step: 2
Training loss: 2.0227036476135254
Validation loss: 2.0265515645345054

Epoch: 6| Step: 3
Training loss: 1.5658760070800781
Validation loss: 2.021549383799235

Epoch: 6| Step: 4
Training loss: 2.1597678661346436
Validation loss: 2.021818776925405

Epoch: 6| Step: 5
Training loss: 2.3562521934509277
Validation loss: 2.014761745929718

Epoch: 6| Step: 6
Training loss: 2.249596118927002
Validation loss: 2.0130560199419656

Epoch: 6| Step: 7
Training loss: 2.078298807144165
Validation loss: 2.015471398830414

Epoch: 6| Step: 8
Training loss: 2.46730899810791
Validation loss: 2.0080952843030295

Epoch: 6| Step: 9
Training loss: 2.0517077445983887
Validation loss: 2.0075987378756204

Epoch: 6| Step: 10
Training loss: 1.89902925491333
Validation loss: 2.0059770941734314

Epoch: 6| Step: 11
Training loss: 1.9621952772140503
Validation loss: 2.0089121063550315

Epoch: 6| Step: 12
Training loss: 2.069033622741699
Validation loss: 2.0049686829249063

Epoch: 6| Step: 13
Training loss: 2.397972583770752
Validation loss: 2.0164939562479653

Epoch: 90| Step: 0
Training loss: 1.3871791362762451
Validation loss: 2.003653625647227

Epoch: 6| Step: 1
Training loss: 1.6400809288024902
Validation loss: 2.0067705114682517

Epoch: 6| Step: 2
Training loss: 2.4579787254333496
Validation loss: 2.0068814953168235

Epoch: 6| Step: 3
Training loss: 3.004808187484741
Validation loss: 2.0134446819623313

Epoch: 6| Step: 4
Training loss: 2.2562971115112305
Validation loss: 2.0198631087938943

Epoch: 6| Step: 5
Training loss: 1.9066113233566284
Validation loss: 2.0147124330202737

Epoch: 6| Step: 6
Training loss: 2.1769628524780273
Validation loss: 2.0165116588274636

Epoch: 6| Step: 7
Training loss: 2.5283384323120117
Validation loss: 2.028321862220764

Epoch: 6| Step: 8
Training loss: 2.0063185691833496
Validation loss: 2.0210185647010803

Epoch: 6| Step: 9
Training loss: 2.0085325241088867
Validation loss: 2.0270484487215676

Epoch: 6| Step: 10
Training loss: 1.9419153928756714
Validation loss: 2.034796337286631

Epoch: 6| Step: 11
Training loss: 2.1943106651306152
Validation loss: 2.0273571610450745

Epoch: 6| Step: 12
Training loss: 2.4381484985351562
Validation loss: 2.0273436506589255

Epoch: 6| Step: 13
Training loss: 2.0230703353881836
Validation loss: 2.0248491168022156

Epoch: 91| Step: 0
Training loss: 2.3469958305358887
Validation loss: 2.019281506538391

Epoch: 6| Step: 1
Training loss: 1.9704866409301758
Validation loss: 2.0222529570261636

Epoch: 6| Step: 2
Training loss: 1.8531090021133423
Validation loss: 2.0254178444544473

Epoch: 6| Step: 3
Training loss: 1.6543059349060059
Validation loss: 2.0256187121073403

Epoch: 6| Step: 4
Training loss: 1.5470629930496216
Validation loss: 2.0296330054601035

Epoch: 6| Step: 5
Training loss: 2.308015823364258
Validation loss: 2.024265706539154

Epoch: 6| Step: 6
Training loss: 2.1112101078033447
Validation loss: 2.027828017870585

Epoch: 6| Step: 7
Training loss: 2.5840225219726562
Validation loss: 2.024364630381266

Epoch: 6| Step: 8
Training loss: 2.8241665363311768
Validation loss: 2.019581437110901

Epoch: 6| Step: 9
Training loss: 2.0225045680999756
Validation loss: 2.0057495633761087

Epoch: 6| Step: 10
Training loss: 2.3551454544067383
Validation loss: 2.0216287970542908

Epoch: 6| Step: 11
Training loss: 2.0591468811035156
Validation loss: 2.0135367711385093

Epoch: 6| Step: 12
Training loss: 2.066408634185791
Validation loss: 2.0137625336647034

Epoch: 6| Step: 13
Training loss: 2.281282663345337
Validation loss: 2.0210513273874917

Epoch: 92| Step: 0
Training loss: 2.595475673675537
Validation loss: 2.0224518378575644

Epoch: 6| Step: 1
Training loss: 1.6797269582748413
Validation loss: 2.0299641291300454

Epoch: 6| Step: 2
Training loss: 1.8629992008209229
Validation loss: 2.033942679564158

Epoch: 6| Step: 3
Training loss: 2.3183634281158447
Validation loss: 2.060714860757192

Epoch: 6| Step: 4
Training loss: 2.2929799556732178
Validation loss: 2.07001801331838

Epoch: 6| Step: 5
Training loss: 2.3089401721954346
Validation loss: 2.0733471512794495

Epoch: 6| Step: 6
Training loss: 2.3069875240325928
Validation loss: 2.076922277609507

Epoch: 6| Step: 7
Training loss: 2.172062873840332
Validation loss: 2.070177952448527

Epoch: 6| Step: 8
Training loss: 2.1586461067199707
Validation loss: 2.0720148285230002

Epoch: 6| Step: 9
Training loss: 2.574972629547119
Validation loss: 2.0612550377845764

Epoch: 6| Step: 10
Training loss: 1.7712914943695068
Validation loss: 2.0424991647402444

Epoch: 6| Step: 11
Training loss: 2.4166367053985596
Validation loss: 2.0296345353126526

Epoch: 6| Step: 12
Training loss: 2.0966482162475586
Validation loss: 2.0197309851646423

Epoch: 6| Step: 13
Training loss: 1.7770813703536987
Validation loss: 2.0067797700564065

Epoch: 93| Step: 0
Training loss: 1.7118221521377563
Validation loss: 2.0063884258270264

Epoch: 6| Step: 1
Training loss: 2.228473424911499
Validation loss: 2.001823981602987

Epoch: 6| Step: 2
Training loss: 2.3746323585510254
Validation loss: 1.9921688437461853

Epoch: 6| Step: 3
Training loss: 2.6031203269958496
Validation loss: 1.996695101261139

Epoch: 6| Step: 4
Training loss: 2.7987890243530273
Validation loss: 2.0015149315198264

Epoch: 6| Step: 5
Training loss: 1.9641133546829224
Validation loss: 2.0070047974586487

Epoch: 6| Step: 6
Training loss: 1.9722121953964233
Validation loss: 1.99886155128479

Epoch: 6| Step: 7
Training loss: 2.0671207904815674
Validation loss: 2.005437513192495

Epoch: 6| Step: 8
Training loss: 1.3895330429077148
Validation loss: 2.0056504607200623

Epoch: 6| Step: 9
Training loss: 2.009761095046997
Validation loss: 2.0179750124613443

Epoch: 6| Step: 10
Training loss: 1.9017179012298584
Validation loss: 2.0172142386436462

Epoch: 6| Step: 11
Training loss: 2.317356824874878
Validation loss: 2.030076583226522

Epoch: 6| Step: 12
Training loss: 2.4091711044311523
Validation loss: 2.026519497235616

Epoch: 6| Step: 13
Training loss: 2.4933290481567383
Validation loss: 2.023602863152822

Epoch: 94| Step: 0
Training loss: 2.3449525833129883
Validation loss: 2.0289730032285056

Epoch: 6| Step: 1
Training loss: 1.7084065675735474
Validation loss: 2.0190255641937256

Epoch: 6| Step: 2
Training loss: 2.2972524166107178
Validation loss: 2.0172142585118613

Epoch: 6| Step: 3
Training loss: 1.9749822616577148
Validation loss: 2.009707828362783

Epoch: 6| Step: 4
Training loss: 1.6369080543518066
Validation loss: 2.012692868709564

Epoch: 6| Step: 5
Training loss: 2.344041109085083
Validation loss: 2.0220309098561606

Epoch: 6| Step: 6
Training loss: 2.1058387756347656
Validation loss: 2.0209163626035056

Epoch: 6| Step: 7
Training loss: 2.431041717529297
Validation loss: 2.0183799068133035

Epoch: 6| Step: 8
Training loss: 2.3547589778900146
Validation loss: 2.021822909514109

Epoch: 6| Step: 9
Training loss: 2.259594202041626
Validation loss: 2.016180237134298

Epoch: 6| Step: 10
Training loss: 1.8122098445892334
Validation loss: 2.014029622077942

Epoch: 6| Step: 11
Training loss: 2.827850818634033
Validation loss: 2.011337697505951

Epoch: 6| Step: 12
Training loss: 1.8637326955795288
Validation loss: 2.010325074195862

Epoch: 6| Step: 13
Training loss: 2.215190887451172
Validation loss: 2.003861963748932

Epoch: 95| Step: 0
Training loss: 1.8033679723739624
Validation loss: 2.019519031047821

Epoch: 6| Step: 1
Training loss: 2.3189306259155273
Validation loss: 2.016934553782145

Epoch: 6| Step: 2
Training loss: 2.110337972640991
Validation loss: 2.0255038936932883

Epoch: 6| Step: 3
Training loss: 2.3330318927764893
Validation loss: 2.0218164920806885

Epoch: 6| Step: 4
Training loss: 1.783974528312683
Validation loss: 2.0276559789975486

Epoch: 6| Step: 5
Training loss: 2.1060380935668945
Validation loss: 2.0361088514328003

Epoch: 6| Step: 6
Training loss: 2.5317811965942383
Validation loss: 2.048540790875753

Epoch: 6| Step: 7
Training loss: 2.7253899574279785
Validation loss: 2.0399575233459473

Epoch: 6| Step: 8
Training loss: 2.282001495361328
Validation loss: 2.033417900403341

Epoch: 6| Step: 9
Training loss: 2.4190454483032227
Validation loss: 2.027655084927877

Epoch: 6| Step: 10
Training loss: 2.3389244079589844
Validation loss: 2.007542908191681

Epoch: 6| Step: 11
Training loss: 1.2522034645080566
Validation loss: 2.004715859889984

Epoch: 6| Step: 12
Training loss: 2.136481761932373
Validation loss: 2.0077291131019592

Epoch: 6| Step: 13
Training loss: 1.973421335220337
Validation loss: 1.9948613047599792

Epoch: 96| Step: 0
Training loss: 1.98988676071167
Validation loss: 2.0009524822235107

Epoch: 6| Step: 1
Training loss: 1.871023416519165
Validation loss: 2.008098284403483

Epoch: 6| Step: 2
Training loss: 1.9921011924743652
Validation loss: 1.9929916461308796

Epoch: 6| Step: 3
Training loss: 2.684809684753418
Validation loss: 2.000296692053477

Epoch: 6| Step: 4
Training loss: 2.4248759746551514
Validation loss: 1.9987892707188923

Epoch: 6| Step: 5
Training loss: 2.210775375366211
Validation loss: 2.0004295110702515

Epoch: 6| Step: 6
Training loss: 1.7410552501678467
Validation loss: 2.0119428435961404

Epoch: 6| Step: 7
Training loss: 2.2084248065948486
Validation loss: 2.010163684686025

Epoch: 6| Step: 8
Training loss: 2.249297618865967
Validation loss: 2.0213459928830466

Epoch: 6| Step: 9
Training loss: 1.8536908626556396
Validation loss: 2.0179080168406167

Epoch: 6| Step: 10
Training loss: 2.1543102264404297
Validation loss: 2.0171852310498557

Epoch: 6| Step: 11
Training loss: 2.073026418685913
Validation loss: 2.025483727455139

Epoch: 6| Step: 12
Training loss: 2.385986089706421
Validation loss: 2.031150460243225

Epoch: 6| Step: 13
Training loss: 2.1614596843719482
Validation loss: 2.0218507250150046

Epoch: 97| Step: 0
Training loss: 1.8745852708816528
Validation loss: 2.047670324643453

Epoch: 6| Step: 1
Training loss: 1.7905378341674805
Validation loss: 2.0391127665837607

Epoch: 6| Step: 2
Training loss: 3.3329460620880127
Validation loss: 2.0361719528834024

Epoch: 6| Step: 3
Training loss: 1.9704875946044922
Validation loss: 2.037801464398702

Epoch: 6| Step: 4
Training loss: 2.347473621368408
Validation loss: 2.0346128741900125

Epoch: 6| Step: 5
Training loss: 2.287191152572632
Validation loss: 2.021054526170095

Epoch: 6| Step: 6
Training loss: 1.7660293579101562
Validation loss: 2.013060212135315

Epoch: 6| Step: 7
Training loss: 1.9472368955612183
Validation loss: 2.0111271937688193

Epoch: 6| Step: 8
Training loss: 1.8715951442718506
Validation loss: 2.011034448941549

Epoch: 6| Step: 9
Training loss: 3.005093812942505
Validation loss: 2.009652574857076

Epoch: 6| Step: 10
Training loss: 1.7987253665924072
Validation loss: 2.013318101565043

Epoch: 6| Step: 11
Training loss: 2.2941482067108154
Validation loss: 2.014902969201406

Epoch: 6| Step: 12
Training loss: 1.7197378873825073
Validation loss: 2.006255845228831

Epoch: 6| Step: 13
Training loss: 2.0076446533203125
Validation loss: 2.0129806995391846

Epoch: 98| Step: 0
Training loss: 2.313864231109619
Validation loss: 2.0076811710993447

Epoch: 6| Step: 1
Training loss: 2.0000500679016113
Validation loss: 2.0143543680508933

Epoch: 6| Step: 2
Training loss: 2.0324349403381348
Validation loss: 2.014318108558655

Epoch: 6| Step: 3
Training loss: 2.1019644737243652
Validation loss: 2.0101467768351235

Epoch: 6| Step: 4
Training loss: 2.199211359024048
Validation loss: 2.0099120338757834

Epoch: 6| Step: 5
Training loss: 2.333970785140991
Validation loss: 2.0153397719065347

Epoch: 6| Step: 6
Training loss: 2.157557249069214
Validation loss: 2.0181058843930564

Epoch: 6| Step: 7
Training loss: 2.281625270843506
Validation loss: 2.015527685483297

Epoch: 6| Step: 8
Training loss: 1.4044424295425415
Validation loss: 2.026446004708608

Epoch: 6| Step: 9
Training loss: 2.1176340579986572
Validation loss: 2.035642981529236

Epoch: 6| Step: 10
Training loss: 1.677607536315918
Validation loss: 2.0192176898320517

Epoch: 6| Step: 11
Training loss: 2.223534107208252
Validation loss: 2.031948526700338

Epoch: 6| Step: 12
Training loss: 2.4106507301330566
Validation loss: 2.0303816397984824

Epoch: 6| Step: 13
Training loss: 2.574737071990967
Validation loss: 2.026538610458374

Epoch: 99| Step: 0
Training loss: 1.9062234163284302
Validation loss: 2.035573720932007

Epoch: 6| Step: 1
Training loss: 2.6418662071228027
Validation loss: 2.0368672609329224

Epoch: 6| Step: 2
Training loss: 1.9759951829910278
Validation loss: 2.0358668764432273

Epoch: 6| Step: 3
Training loss: 2.057673454284668
Validation loss: 2.0349183479944863

Epoch: 6| Step: 4
Training loss: 2.2327864170074463
Validation loss: 2.0198744932810464

Epoch: 6| Step: 5
Training loss: 1.826640009880066
Validation loss: 2.0206856727600098

Epoch: 6| Step: 6
Training loss: 2.5577878952026367
Validation loss: 2.019761085510254

Epoch: 6| Step: 7
Training loss: 2.034243583679199
Validation loss: 2.0081472992897034

Epoch: 6| Step: 8
Training loss: 2.4843850135803223
Validation loss: 2.01288249095281

Epoch: 6| Step: 9
Training loss: 1.4894800186157227
Validation loss: 2.0124701062838235

Epoch: 6| Step: 10
Training loss: 2.228382110595703
Validation loss: 2.002299646536509

Epoch: 6| Step: 11
Training loss: 1.9776225090026855
Validation loss: 2.013148307800293

Epoch: 6| Step: 12
Training loss: 2.350184917449951
Validation loss: 2.0125266710917153

Epoch: 6| Step: 13
Training loss: 2.040036201477051
Validation loss: 2.0091908971468606

Epoch: 100| Step: 0
Training loss: 2.3341758251190186
Validation loss: 2.008646329243978

Epoch: 6| Step: 1
Training loss: 2.137928009033203
Validation loss: 2.014204661051432

Epoch: 6| Step: 2
Training loss: 1.6030869483947754
Validation loss: 2.019606649875641

Epoch: 6| Step: 3
Training loss: 1.5804095268249512
Validation loss: 2.012965202331543

Epoch: 6| Step: 4
Training loss: 2.340850830078125
Validation loss: 2.0047117471694946

Epoch: 6| Step: 5
Training loss: 1.8435159921646118
Validation loss: 2.0065163175264993

Epoch: 6| Step: 6
Training loss: 2.352743148803711
Validation loss: 2.007481018702189

Epoch: 6| Step: 7
Training loss: 2.100517749786377
Validation loss: 2.0127607583999634

Epoch: 6| Step: 8
Training loss: 2.564237594604492
Validation loss: 2.0156105160713196

Epoch: 6| Step: 9
Training loss: 2.7168729305267334
Validation loss: 2.015259842077891

Epoch: 6| Step: 10
Training loss: 2.3963327407836914
Validation loss: 2.018036643664042

Epoch: 6| Step: 11
Training loss: 1.7139818668365479
Validation loss: 2.02052640914917

Epoch: 6| Step: 12
Training loss: 1.9550755023956299
Validation loss: 2.0132597287495932

Epoch: 6| Step: 13
Training loss: 2.1985058784484863
Validation loss: 2.009402314821879

Epoch: 101| Step: 0
Training loss: 1.5876243114471436
Validation loss: 2.0188111464182534

Epoch: 6| Step: 1
Training loss: 1.6086658239364624
Validation loss: 2.0213253696759543

Epoch: 6| Step: 2
Training loss: 2.167888641357422
Validation loss: 2.0232490499814353

Epoch: 6| Step: 3
Training loss: 2.4696688652038574
Validation loss: 2.0238972306251526

Epoch: 6| Step: 4
Training loss: 2.0240631103515625
Validation loss: 2.025011658668518

Epoch: 6| Step: 5
Training loss: 2.4180517196655273
Validation loss: 2.0422635475794473

Epoch: 6| Step: 6
Training loss: 2.194977283477783
Validation loss: 2.0393678744633994

Epoch: 6| Step: 7
Training loss: 2.7973411083221436
Validation loss: 2.0398420691490173

Epoch: 6| Step: 8
Training loss: 1.7553616762161255
Validation loss: 2.0480666557947793

Epoch: 6| Step: 9
Training loss: 2.735597610473633
Validation loss: 2.035019556681315

Epoch: 6| Step: 10
Training loss: 1.9428160190582275
Validation loss: 2.026999572912852

Epoch: 6| Step: 11
Training loss: 1.7385284900665283
Validation loss: 2.019357661406199

Epoch: 6| Step: 12
Training loss: 2.2291245460510254
Validation loss: 2.02423365910848

Epoch: 6| Step: 13
Training loss: 2.204042434692383
Validation loss: 2.0091787576675415

Epoch: 102| Step: 0
Training loss: 2.213325023651123
Validation loss: 2.0148517886797586

Epoch: 6| Step: 1
Training loss: 1.5639755725860596
Validation loss: 2.0108107924461365

Epoch: 6| Step: 2
Training loss: 1.6208454370498657
Validation loss: 2.015385170777639

Epoch: 6| Step: 3
Training loss: 2.931654930114746
Validation loss: 2.02019468943278

Epoch: 6| Step: 4
Training loss: 2.1270575523376465
Validation loss: 2.0242255131403604

Epoch: 6| Step: 5
Training loss: 2.181790351867676
Validation loss: 2.0298609932263694

Epoch: 6| Step: 6
Training loss: 1.8506892919540405
Validation loss: 2.0286989212036133

Epoch: 6| Step: 7
Training loss: 2.1698973178863525
Validation loss: 2.0239591201146445

Epoch: 6| Step: 8
Training loss: 2.259037733078003
Validation loss: 2.02279402812322

Epoch: 6| Step: 9
Training loss: 2.380786180496216
Validation loss: 2.0148932337760925

Epoch: 6| Step: 10
Training loss: 2.428424835205078
Validation loss: 2.0093308488527932

Epoch: 6| Step: 11
Training loss: 1.5223441123962402
Validation loss: 2.0130667686462402

Epoch: 6| Step: 12
Training loss: 1.79899001121521
Validation loss: 2.0154001116752625

Epoch: 6| Step: 13
Training loss: 2.7256927490234375
Validation loss: 2.0107054313023887

Epoch: 103| Step: 0
Training loss: 2.4916772842407227
Validation loss: 2.0106505155563354

Epoch: 6| Step: 1
Training loss: 2.5183656215667725
Validation loss: 2.008780618508657

Epoch: 6| Step: 2
Training loss: 2.208120822906494
Validation loss: 2.0009859800338745

Epoch: 6| Step: 3
Training loss: 2.482926845550537
Validation loss: 2.003250022729238

Epoch: 6| Step: 4
Training loss: 1.7547290325164795
Validation loss: 2.002149820327759

Epoch: 6| Step: 5
Training loss: 2.1705145835876465
Validation loss: 2.0025116999944053

Epoch: 6| Step: 6
Training loss: 2.0691676139831543
Validation loss: 2.008146266142527

Epoch: 6| Step: 7
Training loss: 1.951298713684082
Validation loss: 2.0021360516548157

Epoch: 6| Step: 8
Training loss: 2.072300672531128
Validation loss: 2.012564162413279

Epoch: 6| Step: 9
Training loss: 1.7739348411560059
Validation loss: 2.007665733496348

Epoch: 6| Step: 10
Training loss: 1.9746824502944946
Validation loss: 2.015745460987091

Epoch: 6| Step: 11
Training loss: 2.5940980911254883
Validation loss: 2.0175660649935403

Epoch: 6| Step: 12
Training loss: 1.5521352291107178
Validation loss: 2.02145516872406

Epoch: 6| Step: 13
Training loss: 1.9989686012268066
Validation loss: 2.0293923219045005

Epoch: 104| Step: 0
Training loss: 1.8439760208129883
Validation loss: 2.02773783604304

Epoch: 6| Step: 1
Training loss: 2.336003065109253
Validation loss: 2.021919310092926

Epoch: 6| Step: 2
Training loss: 2.4764161109924316
Validation loss: 2.026247521241506

Epoch: 6| Step: 3
Training loss: 1.8190207481384277
Validation loss: 2.0238035519917807

Epoch: 6| Step: 4
Training loss: 1.4432671070098877
Validation loss: 2.0208462675412497

Epoch: 6| Step: 5
Training loss: 2.5030200481414795
Validation loss: 2.0178459882736206

Epoch: 6| Step: 6
Training loss: 1.856276035308838
Validation loss: 2.0179481307665506

Epoch: 6| Step: 7
Training loss: 2.4119374752044678
Validation loss: 2.0140414039293923

Epoch: 6| Step: 8
Training loss: 2.2640857696533203
Validation loss: 2.004422644774119

Epoch: 6| Step: 9
Training loss: 1.9888795614242554
Validation loss: 2.005097726980845

Epoch: 6| Step: 10
Training loss: 2.0980265140533447
Validation loss: 2.0095086892445884

Epoch: 6| Step: 11
Training loss: 2.392637014389038
Validation loss: 2.0094223618507385

Epoch: 6| Step: 12
Training loss: 2.001880168914795
Validation loss: 2.014234979947408

Epoch: 6| Step: 13
Training loss: 2.3146448135375977
Validation loss: 2.012573500474294

Epoch: 105| Step: 0
Training loss: 2.3555264472961426
Validation loss: 2.0186142126719155

Epoch: 6| Step: 1
Training loss: 2.4712841510772705
Validation loss: 2.027855714162191

Epoch: 6| Step: 2
Training loss: 1.777505874633789
Validation loss: 2.0165981451670327

Epoch: 6| Step: 3
Training loss: 2.0454015731811523
Validation loss: 2.0238271752993264

Epoch: 6| Step: 4
Training loss: 2.607064723968506
Validation loss: 2.0110687017440796

Epoch: 6| Step: 5
Training loss: 2.5469417572021484
Validation loss: 2.0094895164171853

Epoch: 6| Step: 6
Training loss: 1.8461930751800537
Validation loss: 2.011347313721975

Epoch: 6| Step: 7
Training loss: 2.231546640396118
Validation loss: 2.009381910165151

Epoch: 6| Step: 8
Training loss: 1.7759424448013306
Validation loss: 2.0113240281740823

Epoch: 6| Step: 9
Training loss: 2.4666507244110107
Validation loss: 2.020547946294149

Epoch: 6| Step: 10
Training loss: 1.4740952253341675
Validation loss: 2.0197896560033164

Epoch: 6| Step: 11
Training loss: 1.9705533981323242
Validation loss: 2.021632750829061

Epoch: 6| Step: 12
Training loss: 2.0507631301879883
Validation loss: 2.012194275856018

Epoch: 6| Step: 13
Training loss: 1.9572795629501343
Validation loss: 2.0199636022249856

Epoch: 106| Step: 0
Training loss: 1.606610655784607
Validation loss: 2.021305739879608

Epoch: 6| Step: 1
Training loss: 1.6040807962417603
Validation loss: 2.0273406306902566

Epoch: 6| Step: 2
Training loss: 1.9859161376953125
Validation loss: 2.025129775206248

Epoch: 6| Step: 3
Training loss: 2.3275303840637207
Validation loss: 2.022331396738688

Epoch: 6| Step: 4
Training loss: 2.4610817432403564
Validation loss: 2.014462391535441

Epoch: 6| Step: 5
Training loss: 2.8878426551818848
Validation loss: 2.0278550386428833

Epoch: 6| Step: 6
Training loss: 1.9440561532974243
Validation loss: 2.0065024892489114

Epoch: 6| Step: 7
Training loss: 2.9258124828338623
Validation loss: 2.0162788033485413

Epoch: 6| Step: 8
Training loss: 2.2384369373321533
Validation loss: 2.0155282219251

Epoch: 6| Step: 9
Training loss: 1.9029526710510254
Validation loss: 2.018260141213735

Epoch: 6| Step: 10
Training loss: 1.8686935901641846
Validation loss: 2.0140000581741333

Epoch: 6| Step: 11
Training loss: 2.20845365524292
Validation loss: 2.0120690862337747

Epoch: 6| Step: 12
Training loss: 1.3861417770385742
Validation loss: 2.005419135093689

Epoch: 6| Step: 13
Training loss: 2.2555837631225586
Validation loss: 2.0106921593348184

Epoch: 107| Step: 0
Training loss: 2.5069098472595215
Validation loss: 2.007972458998362

Epoch: 6| Step: 1
Training loss: 1.8298890590667725
Validation loss: 2.010410030682882

Epoch: 6| Step: 2
Training loss: 2.2816643714904785
Validation loss: 1.9970727960268657

Epoch: 6| Step: 3
Training loss: 2.0033698081970215
Validation loss: 2.0120689471562705

Epoch: 6| Step: 4
Training loss: 2.0520589351654053
Validation loss: 2.0099828044573465

Epoch: 6| Step: 5
Training loss: 2.4630589485168457
Validation loss: 2.014090438683828

Epoch: 6| Step: 6
Training loss: 2.5346031188964844
Validation loss: 2.016168455282847

Epoch: 6| Step: 7
Training loss: 1.8890306949615479
Validation loss: 2.028053641319275

Epoch: 6| Step: 8
Training loss: 1.6309139728546143
Validation loss: 2.024277448654175

Epoch: 6| Step: 9
Training loss: 2.2401580810546875
Validation loss: 2.040192484855652

Epoch: 6| Step: 10
Training loss: 1.9102072715759277
Validation loss: 2.0455603202184043

Epoch: 6| Step: 11
Training loss: 2.4901671409606934
Validation loss: 2.0506888230641684

Epoch: 6| Step: 12
Training loss: 2.4380042552948
Validation loss: 2.0480588475863137

Epoch: 6| Step: 13
Training loss: 1.5889668464660645
Validation loss: 2.0434875090916953

Epoch: 108| Step: 0
Training loss: 1.799996256828308
Validation loss: 2.031556804974874

Epoch: 6| Step: 1
Training loss: 1.77979576587677
Validation loss: 2.0150078932444253

Epoch: 6| Step: 2
Training loss: 1.6297893524169922
Validation loss: 2.0110049645105996

Epoch: 6| Step: 3
Training loss: 2.4332170486450195
Validation loss: 2.0072253346443176

Epoch: 6| Step: 4
Training loss: 2.403841495513916
Validation loss: 1.9976653258005779

Epoch: 6| Step: 5
Training loss: 1.7591419219970703
Validation loss: 2.005061070124308

Epoch: 6| Step: 6
Training loss: 2.523979663848877
Validation loss: 2.0079360802968345

Epoch: 6| Step: 7
Training loss: 1.6702008247375488
Validation loss: 2.014019727706909

Epoch: 6| Step: 8
Training loss: 2.414639949798584
Validation loss: 2.023156762123108

Epoch: 6| Step: 9
Training loss: 2.6952414512634277
Validation loss: 2.0229854186375937

Epoch: 6| Step: 10
Training loss: 2.2675857543945312
Validation loss: 2.0174351135889688

Epoch: 6| Step: 11
Training loss: 2.6322896480560303
Validation loss: 2.0204649368921914

Epoch: 6| Step: 12
Training loss: 2.27937388420105
Validation loss: 2.019718329111735

Epoch: 6| Step: 13
Training loss: 1.9447331428527832
Validation loss: 2.0170825322469077

Epoch: 109| Step: 0
Training loss: 1.4467941522598267
Validation loss: 2.0219630797704062

Epoch: 6| Step: 1
Training loss: 2.275106906890869
Validation loss: 2.014435430367788

Epoch: 6| Step: 2
Training loss: 2.5808205604553223
Validation loss: 2.0140734910964966

Epoch: 6| Step: 3
Training loss: 1.24253511428833
Validation loss: 2.0011571049690247

Epoch: 6| Step: 4
Training loss: 2.0815343856811523
Validation loss: 2.002940575281779

Epoch: 6| Step: 5
Training loss: 2.7227187156677246
Validation loss: 1.9941125710805256

Epoch: 6| Step: 6
Training loss: 1.9687409400939941
Validation loss: 1.9892073273658752

Epoch: 6| Step: 7
Training loss: 1.9002755880355835
Validation loss: 2.006998082002004

Epoch: 6| Step: 8
Training loss: 2.599219799041748
Validation loss: 2.014254848162333

Epoch: 6| Step: 9
Training loss: 2.2509021759033203
Validation loss: 2.018908937772115

Epoch: 6| Step: 10
Training loss: 2.2505733966827393
Validation loss: 2.0241830547650657

Epoch: 6| Step: 11
Training loss: 2.473512649536133
Validation loss: 2.0413149992624917

Epoch: 6| Step: 12
Training loss: 2.231494426727295
Validation loss: 2.0445187290509543

Epoch: 6| Step: 13
Training loss: 2.0836219787597656
Validation loss: 2.0450334946314492

Epoch: 110| Step: 0
Training loss: 1.4772652387619019
Validation loss: 2.032046655813853

Epoch: 6| Step: 1
Training loss: 2.0607903003692627
Validation loss: 2.0273044109344482

Epoch: 6| Step: 2
Training loss: 1.9660348892211914
Validation loss: 2.0357369979222617

Epoch: 6| Step: 3
Training loss: 1.3527171611785889
Validation loss: 2.037995715936025

Epoch: 6| Step: 4
Training loss: 2.1925899982452393
Validation loss: 2.040896753470103

Epoch: 6| Step: 5
Training loss: 2.664407730102539
Validation loss: 2.026611010233561

Epoch: 6| Step: 6
Training loss: 2.0456955432891846
Validation loss: 2.0306748350461326

Epoch: 6| Step: 7
Training loss: 2.894535541534424
Validation loss: 2.018120070298513

Epoch: 6| Step: 8
Training loss: 2.244962453842163
Validation loss: 2.021226167678833

Epoch: 6| Step: 9
Training loss: 2.135281562805176
Validation loss: 2.0145235459009805

Epoch: 6| Step: 10
Training loss: 2.324151039123535
Validation loss: 2.0156834920247397

Epoch: 6| Step: 11
Training loss: 2.013235569000244
Validation loss: 2.014197905858358

Epoch: 6| Step: 12
Training loss: 2.5568575859069824
Validation loss: 2.0271319150924683

Epoch: 6| Step: 13
Training loss: 1.674961805343628
Validation loss: 2.0186439752578735

Epoch: 111| Step: 0
Training loss: 1.9917857646942139
Validation loss: 2.0368462204933167

Epoch: 6| Step: 1
Training loss: 2.563795328140259
Validation loss: 2.0410744349161782

Epoch: 6| Step: 2
Training loss: 2.0992493629455566
Validation loss: 2.051598906517029

Epoch: 6| Step: 3
Training loss: 2.628096580505371
Validation loss: 2.049520254135132

Epoch: 6| Step: 4
Training loss: 2.4143226146698
Validation loss: 2.0475552479426065

Epoch: 6| Step: 5
Training loss: 1.7686020135879517
Validation loss: 2.0518574913342795

Epoch: 6| Step: 6
Training loss: 1.7189640998840332
Validation loss: 2.05138365427653

Epoch: 6| Step: 7
Training loss: 2.538147449493408
Validation loss: 2.0402756730715432

Epoch: 6| Step: 8
Training loss: 2.145606517791748
Validation loss: 2.0452240904172263

Epoch: 6| Step: 9
Training loss: 2.0224952697753906
Validation loss: 2.0404003063837686

Epoch: 6| Step: 10
Training loss: 2.1161646842956543
Validation loss: 2.0279592672983804

Epoch: 6| Step: 11
Training loss: 2.1208014488220215
Validation loss: 2.0222439567248025

Epoch: 6| Step: 12
Training loss: 1.721394658088684
Validation loss: 2.0242169300715127

Epoch: 6| Step: 13
Training loss: 1.7203755378723145
Validation loss: 2.017281969388326

Epoch: 112| Step: 0
Training loss: 2.1047863960266113
Validation loss: 2.0194881359736123

Epoch: 6| Step: 1
Training loss: 2.2956433296203613
Validation loss: 2.015716095765432

Epoch: 6| Step: 2
Training loss: 2.2430996894836426
Validation loss: 2.0163363019625344

Epoch: 6| Step: 3
Training loss: 1.781103491783142
Validation loss: 2.017701248327891

Epoch: 6| Step: 4
Training loss: 1.7945739030838013
Validation loss: 2.009941736857096

Epoch: 6| Step: 5
Training loss: 1.864403486251831
Validation loss: 2.0182653069496155

Epoch: 6| Step: 6
Training loss: 2.3339290618896484
Validation loss: 2.0134553710619607

Epoch: 6| Step: 7
Training loss: 1.9428889751434326
Validation loss: 2.025038778781891

Epoch: 6| Step: 8
Training loss: 1.6698074340820312
Validation loss: 2.020615736643473

Epoch: 6| Step: 9
Training loss: 2.6259570121765137
Validation loss: 2.0190011660257974

Epoch: 6| Step: 10
Training loss: 1.739173412322998
Validation loss: 2.021212418874105

Epoch: 6| Step: 11
Training loss: 2.4732675552368164
Validation loss: 2.0125602086385093

Epoch: 6| Step: 12
Training loss: 2.424173355102539
Validation loss: 2.018727441628774

Epoch: 6| Step: 13
Training loss: 2.0692849159240723
Validation loss: 2.0193912982940674

Epoch: 113| Step: 0
Training loss: 2.887917995452881
Validation loss: 2.012783090273539

Epoch: 6| Step: 1
Training loss: 2.0643954277038574
Validation loss: 2.0090023080507913

Epoch: 6| Step: 2
Training loss: 2.129868984222412
Validation loss: 2.0185497999191284

Epoch: 6| Step: 3
Training loss: 2.0556681156158447
Validation loss: 2.017104665438334

Epoch: 6| Step: 4
Training loss: 2.033641815185547
Validation loss: 2.010797838370005

Epoch: 6| Step: 5
Training loss: 1.7439671754837036
Validation loss: 2.007893661657969

Epoch: 6| Step: 6
Training loss: 2.1789729595184326
Validation loss: 2.0138686498006186

Epoch: 6| Step: 7
Training loss: 1.45792555809021
Validation loss: 2.028573532899221

Epoch: 6| Step: 8
Training loss: 2.4740593433380127
Validation loss: 2.0260687271753945

Epoch: 6| Step: 9
Training loss: 2.3511486053466797
Validation loss: 2.01911993821462

Epoch: 6| Step: 10
Training loss: 2.24173903465271
Validation loss: 2.0222952564557395

Epoch: 6| Step: 11
Training loss: 1.921458125114441
Validation loss: 2.026908298333486

Epoch: 6| Step: 12
Training loss: 1.7417800426483154
Validation loss: 2.036711494127909

Epoch: 6| Step: 13
Training loss: 2.155801296234131
Validation loss: 2.022119422753652

Epoch: 114| Step: 0
Training loss: 1.801361322402954
Validation loss: 2.0265405774116516

Epoch: 6| Step: 1
Training loss: 2.0926761627197266
Validation loss: 2.023564338684082

Epoch: 6| Step: 2
Training loss: 2.3719544410705566
Validation loss: 2.0156942009925842

Epoch: 6| Step: 3
Training loss: 1.5318197011947632
Validation loss: 2.025026321411133

Epoch: 6| Step: 4
Training loss: 2.379978656768799
Validation loss: 2.016958932081858

Epoch: 6| Step: 5
Training loss: 1.9680664539337158
Validation loss: 2.0202791690826416

Epoch: 6| Step: 6
Training loss: 2.553394079208374
Validation loss: 2.0271713534990945

Epoch: 6| Step: 7
Training loss: 2.264039993286133
Validation loss: 2.02455206712087

Epoch: 6| Step: 8
Training loss: 2.1299705505371094
Validation loss: 2.0182281335194907

Epoch: 6| Step: 9
Training loss: 1.759255290031433
Validation loss: 2.0279073119163513

Epoch: 6| Step: 10
Training loss: 2.2757680416107178
Validation loss: 2.0360330740610757

Epoch: 6| Step: 11
Training loss: 2.304945230484009
Validation loss: 2.035625716050466

Epoch: 6| Step: 12
Training loss: 2.3574767112731934
Validation loss: 2.033242424329122

Epoch: 6| Step: 13
Training loss: 1.5248148441314697
Validation loss: 2.0372031529744468

Epoch: 115| Step: 0
Training loss: 1.821349859237671
Validation loss: 2.031746983528137

Epoch: 6| Step: 1
Training loss: 2.4192514419555664
Validation loss: 2.043014645576477

Epoch: 6| Step: 2
Training loss: 1.8922626972198486
Validation loss: 2.0417350133260093

Epoch: 6| Step: 3
Training loss: 2.297232151031494
Validation loss: 2.033310910065969

Epoch: 6| Step: 4
Training loss: 2.477956771850586
Validation loss: 2.0290892720222473

Epoch: 6| Step: 5
Training loss: 1.4085760116577148
Validation loss: 2.034596880276998

Epoch: 6| Step: 6
Training loss: 2.1330275535583496
Validation loss: 2.023068606853485

Epoch: 6| Step: 7
Training loss: 2.2526302337646484
Validation loss: 2.0224589904149375

Epoch: 6| Step: 8
Training loss: 2.324429988861084
Validation loss: 2.018750846385956

Epoch: 6| Step: 9
Training loss: 2.4031100273132324
Validation loss: 2.0201159715652466

Epoch: 6| Step: 10
Training loss: 2.113919973373413
Validation loss: 2.015712022781372

Epoch: 6| Step: 11
Training loss: 2.6740784645080566
Validation loss: 2.023481329282125

Epoch: 6| Step: 12
Training loss: 1.5102653503417969
Validation loss: 2.0193378726641336

Epoch: 6| Step: 13
Training loss: 1.8134169578552246
Validation loss: 2.0219798485438027

Epoch: 116| Step: 0
Training loss: 2.0385427474975586
Validation loss: 2.02051317691803

Epoch: 6| Step: 1
Training loss: 2.127129554748535
Validation loss: 2.0168399810791016

Epoch: 6| Step: 2
Training loss: 2.70247483253479
Validation loss: 2.0150511860847473

Epoch: 6| Step: 3
Training loss: 1.7658897638320923
Validation loss: 2.018043319384257

Epoch: 6| Step: 4
Training loss: 1.8696050643920898
Validation loss: 2.0175440311431885

Epoch: 6| Step: 5
Training loss: 1.7243413925170898
Validation loss: 2.016426126162211

Epoch: 6| Step: 6
Training loss: 1.9636489152908325
Validation loss: 2.027346154054006

Epoch: 6| Step: 7
Training loss: 1.7433841228485107
Validation loss: 2.022650957107544

Epoch: 6| Step: 8
Training loss: 2.4436988830566406
Validation loss: 2.044655680656433

Epoch: 6| Step: 9
Training loss: 1.4752788543701172
Validation loss: 2.023858984311422

Epoch: 6| Step: 10
Training loss: 2.6244680881500244
Validation loss: 2.029066483179728

Epoch: 6| Step: 11
Training loss: 2.056154251098633
Validation loss: 2.02421901623408

Epoch: 6| Step: 12
Training loss: 2.4930994510650635
Validation loss: 2.026675740877787

Epoch: 6| Step: 13
Training loss: 2.3474502563476562
Validation loss: 2.0172473589579263

Epoch: 117| Step: 0
Training loss: 1.791475772857666
Validation loss: 2.027665118376414

Epoch: 6| Step: 1
Training loss: 2.2165687084198
Validation loss: 2.0207838813463845

Epoch: 6| Step: 2
Training loss: 1.6667956113815308
Validation loss: 2.0265328884124756

Epoch: 6| Step: 3
Training loss: 1.7430768013000488
Validation loss: 2.0203818480173745

Epoch: 6| Step: 4
Training loss: 2.191880702972412
Validation loss: 2.0243789156277976

Epoch: 6| Step: 5
Training loss: 1.7492175102233887
Validation loss: 2.014284392197927

Epoch: 6| Step: 6
Training loss: 2.2624876499176025
Validation loss: 2.0175496538480124

Epoch: 6| Step: 7
Training loss: 1.696939468383789
Validation loss: 2.010386824607849

Epoch: 6| Step: 8
Training loss: 2.294802665710449
Validation loss: 2.0160221060117087

Epoch: 6| Step: 9
Training loss: 2.961477756500244
Validation loss: 2.0166094104448953

Epoch: 6| Step: 10
Training loss: 2.160430908203125
Validation loss: 2.0279836455980935

Epoch: 6| Step: 11
Training loss: 2.0730233192443848
Validation loss: 2.0160034696261087

Epoch: 6| Step: 12
Training loss: 2.1428239345550537
Validation loss: 2.0206143458684287

Epoch: 6| Step: 13
Training loss: 2.398005962371826
Validation loss: 2.0184183518091836

Epoch: 118| Step: 0
Training loss: 1.8917399644851685
Validation loss: 2.020903706550598

Epoch: 6| Step: 1
Training loss: 1.711817741394043
Validation loss: 2.0168779293696084

Epoch: 6| Step: 2
Training loss: 2.121467113494873
Validation loss: 2.0217943588892617

Epoch: 6| Step: 3
Training loss: 1.8434211015701294
Validation loss: 2.0201030174891152

Epoch: 6| Step: 4
Training loss: 1.7985844612121582
Validation loss: 2.022789776325226

Epoch: 6| Step: 5
Training loss: 2.0877950191497803
Validation loss: 2.0229289134343467

Epoch: 6| Step: 6
Training loss: 2.30134916305542
Validation loss: 2.0297069350878396

Epoch: 6| Step: 7
Training loss: 2.6375904083251953
Validation loss: 2.0340190529823303

Epoch: 6| Step: 8
Training loss: 2.589632272720337
Validation loss: 2.020651559034983

Epoch: 6| Step: 9
Training loss: 1.550790786743164
Validation loss: 2.0203017393747964

Epoch: 6| Step: 10
Training loss: 2.993487596511841
Validation loss: 2.0206925868988037

Epoch: 6| Step: 11
Training loss: 1.9132449626922607
Validation loss: 2.0235915978749595

Epoch: 6| Step: 12
Training loss: 2.0992870330810547
Validation loss: 2.0180704593658447

Epoch: 6| Step: 13
Training loss: 1.6814870834350586
Validation loss: 2.030381719271342

Epoch: 119| Step: 0
Training loss: 2.3381521701812744
Validation loss: 2.0090453227361045

Epoch: 6| Step: 1
Training loss: 2.299686908721924
Validation loss: 2.0214586655298867

Epoch: 6| Step: 2
Training loss: 2.060518980026245
Validation loss: 2.029455761114756

Epoch: 6| Step: 3
Training loss: 1.4145894050598145
Validation loss: 2.0191157261530557

Epoch: 6| Step: 4
Training loss: 2.3867483139038086
Validation loss: 2.0196131269137063

Epoch: 6| Step: 5
Training loss: 2.149691581726074
Validation loss: 2.0228408376375833

Epoch: 6| Step: 6
Training loss: 1.9176664352416992
Validation loss: 2.0225499669710794

Epoch: 6| Step: 7
Training loss: 1.9969581365585327
Validation loss: 2.01384574174881

Epoch: 6| Step: 8
Training loss: 1.7456724643707275
Validation loss: 2.0218809247016907

Epoch: 6| Step: 9
Training loss: 2.23732852935791
Validation loss: 2.0372811754544577

Epoch: 6| Step: 10
Training loss: 2.009326934814453
Validation loss: 2.0290247797966003

Epoch: 6| Step: 11
Training loss: 2.005370616912842
Validation loss: 2.0336939493815103

Epoch: 6| Step: 12
Training loss: 2.2281391620635986
Validation loss: 2.0435630281766257

Epoch: 6| Step: 13
Training loss: 2.259145498275757
Validation loss: 2.0461861888567605

Epoch: 120| Step: 0
Training loss: 2.089414596557617
Validation loss: 2.037796934445699

Epoch: 6| Step: 1
Training loss: 1.9806896448135376
Validation loss: 2.036298235257467

Epoch: 6| Step: 2
Training loss: 1.7516506910324097
Validation loss: 2.036110758781433

Epoch: 6| Step: 3
Training loss: 2.463095188140869
Validation loss: 2.038125236829122

Epoch: 6| Step: 4
Training loss: 1.8245280981063843
Validation loss: 2.044967850049337

Epoch: 6| Step: 5
Training loss: 1.6845815181732178
Validation loss: 2.031436562538147

Epoch: 6| Step: 6
Training loss: 2.255380630493164
Validation loss: 2.039979100227356

Epoch: 6| Step: 7
Training loss: 2.2661972045898438
Validation loss: 2.032772640387217

Epoch: 6| Step: 8
Training loss: 2.095458984375
Validation loss: 2.0323845942815146

Epoch: 6| Step: 9
Training loss: 2.578464984893799
Validation loss: 2.0193408131599426

Epoch: 6| Step: 10
Training loss: 2.0244946479797363
Validation loss: 2.006812334060669

Epoch: 6| Step: 11
Training loss: 1.9116657972335815
Validation loss: 2.0273133516311646

Epoch: 6| Step: 12
Training loss: 2.1891427040100098
Validation loss: 2.02588560183843

Epoch: 6| Step: 13
Training loss: 2.1911237239837646
Validation loss: 2.015403906504313

Epoch: 121| Step: 0
Training loss: 2.502624750137329
Validation loss: 2.018676499525706

Epoch: 6| Step: 1
Training loss: 2.532015800476074
Validation loss: 2.0141825675964355

Epoch: 6| Step: 2
Training loss: 2.496647834777832
Validation loss: 2.0172570943832397

Epoch: 6| Step: 3
Training loss: 2.305415391921997
Validation loss: 2.012246032555898

Epoch: 6| Step: 4
Training loss: 1.7746676206588745
Validation loss: 2.0162448287010193

Epoch: 6| Step: 5
Training loss: 2.190316677093506
Validation loss: 2.0299678842226663

Epoch: 6| Step: 6
Training loss: 2.174407720565796
Validation loss: 2.0108771125475564

Epoch: 6| Step: 7
Training loss: 1.7935190200805664
Validation loss: 2.0244157115618386

Epoch: 6| Step: 8
Training loss: 2.0252201557159424
Validation loss: 2.0050341884295144

Epoch: 6| Step: 9
Training loss: 1.770727515220642
Validation loss: 2.012427548567454

Epoch: 6| Step: 10
Training loss: 1.8736987113952637
Validation loss: 2.012554168701172

Epoch: 6| Step: 11
Training loss: 1.9627456665039062
Validation loss: 2.0178887844085693

Epoch: 6| Step: 12
Training loss: 2.024390697479248
Validation loss: 2.0172623793284097

Epoch: 6| Step: 13
Training loss: 1.7954200506210327
Validation loss: 2.0251787304878235

Epoch: 122| Step: 0
Training loss: 2.4262232780456543
Validation loss: 2.0210925141970315

Epoch: 6| Step: 1
Training loss: 2.1754913330078125
Validation loss: 2.023600618044535

Epoch: 6| Step: 2
Training loss: 2.0051944255828857
Validation loss: 2.0375225146611533

Epoch: 6| Step: 3
Training loss: 2.6834053993225098
Validation loss: 2.0312671661376953

Epoch: 6| Step: 4
Training loss: 1.9726498126983643
Validation loss: 2.0283570289611816

Epoch: 6| Step: 5
Training loss: 2.114716053009033
Validation loss: 2.02941902478536

Epoch: 6| Step: 6
Training loss: 1.698784589767456
Validation loss: 2.0297415455182395

Epoch: 6| Step: 7
Training loss: 2.1814827919006348
Validation loss: 2.031106491883596

Epoch: 6| Step: 8
Training loss: 2.470714569091797
Validation loss: 2.0193214217821756

Epoch: 6| Step: 9
Training loss: 1.8118298053741455
Validation loss: 2.023335079352061

Epoch: 6| Step: 10
Training loss: 2.1463968753814697
Validation loss: 2.0235562523206077

Epoch: 6| Step: 11
Training loss: 1.6159030199050903
Validation loss: 2.0301037430763245

Epoch: 6| Step: 12
Training loss: 2.0747432708740234
Validation loss: 2.0294012228647866

Epoch: 6| Step: 13
Training loss: 1.7692190408706665
Validation loss: 2.0198656916618347

Epoch: 123| Step: 0
Training loss: 1.8910894393920898
Validation loss: 2.0128406286239624

Epoch: 6| Step: 1
Training loss: 2.6794967651367188
Validation loss: 2.0282854636510215

Epoch: 6| Step: 2
Training loss: 1.7841686010360718
Validation loss: 2.0223790407180786

Epoch: 6| Step: 3
Training loss: 2.1037936210632324
Validation loss: 2.0378708442052207

Epoch: 6| Step: 4
Training loss: 1.8635001182556152
Validation loss: 2.03933447599411

Epoch: 6| Step: 5
Training loss: 2.371713638305664
Validation loss: 2.0370999773343406

Epoch: 6| Step: 6
Training loss: 1.6104085445404053
Validation loss: 2.040955205758413

Epoch: 6| Step: 7
Training loss: 2.41654109954834
Validation loss: 2.044014116128286

Epoch: 6| Step: 8
Training loss: 1.7137928009033203
Validation loss: 2.0165778199831643

Epoch: 6| Step: 9
Training loss: 2.2482008934020996
Validation loss: 2.021662930647532

Epoch: 6| Step: 10
Training loss: 1.4112838506698608
Validation loss: 2.0175304810206094

Epoch: 6| Step: 11
Training loss: 2.2259931564331055
Validation loss: 2.0187599857648215

Epoch: 6| Step: 12
Training loss: 2.694855213165283
Validation loss: 2.0213469664255777

Epoch: 6| Step: 13
Training loss: 2.257147789001465
Validation loss: 2.0243203242619834

Epoch: 124| Step: 0
Training loss: 2.2468819618225098
Validation loss: 2.02835746606191

Epoch: 6| Step: 1
Training loss: 1.6120400428771973
Validation loss: 2.0188215176264444

Epoch: 6| Step: 2
Training loss: 2.2021327018737793
Validation loss: 2.0180433988571167

Epoch: 6| Step: 3
Training loss: 1.9351167678833008
Validation loss: 2.0122275352478027

Epoch: 6| Step: 4
Training loss: 2.2440803050994873
Validation loss: 2.0219810605049133

Epoch: 6| Step: 5
Training loss: 2.04058575630188
Validation loss: 2.0251760880152383

Epoch: 6| Step: 6
Training loss: 1.4397414922714233
Validation loss: 2.0248660246531167

Epoch: 6| Step: 7
Training loss: 2.53468656539917
Validation loss: 2.028252124786377

Epoch: 6| Step: 8
Training loss: 2.4334781169891357
Validation loss: 2.018132289250692

Epoch: 6| Step: 9
Training loss: 2.0212810039520264
Validation loss: 2.0216055512428284

Epoch: 6| Step: 10
Training loss: 1.8684524297714233
Validation loss: 2.025888681411743

Epoch: 6| Step: 11
Training loss: 2.5411181449890137
Validation loss: 2.0273785988489785

Epoch: 6| Step: 12
Training loss: 2.102201461791992
Validation loss: 2.0441545248031616

Epoch: 6| Step: 13
Training loss: 1.909325122833252
Validation loss: 2.059451401233673

Epoch: 125| Step: 0
Training loss: 2.2930030822753906
Validation loss: 2.058749516805013

Epoch: 6| Step: 1
Training loss: 1.7497384548187256
Validation loss: 2.046144704023997

Epoch: 6| Step: 2
Training loss: 1.9574737548828125
Validation loss: 2.043315311272939

Epoch: 6| Step: 3
Training loss: 2.1877379417419434
Validation loss: 2.040359616279602

Epoch: 6| Step: 4
Training loss: 1.9955214262008667
Validation loss: 2.034382939338684

Epoch: 6| Step: 5
Training loss: 2.128960371017456
Validation loss: 2.039018174012502

Epoch: 6| Step: 6
Training loss: 1.778065800666809
Validation loss: 2.031095266342163

Epoch: 6| Step: 7
Training loss: 1.4514176845550537
Validation loss: 2.022580862045288

Epoch: 6| Step: 8
Training loss: 2.3553757667541504
Validation loss: 2.029550770918528

Epoch: 6| Step: 9
Training loss: 2.3960797786712646
Validation loss: 2.014731705188751

Epoch: 6| Step: 10
Training loss: 1.9630584716796875
Validation loss: 2.02018533150355

Epoch: 6| Step: 11
Training loss: 2.0974206924438477
Validation loss: 2.0234711368878684

Epoch: 6| Step: 12
Training loss: 2.919161081314087
Validation loss: 2.0243850549062095

Epoch: 6| Step: 13
Training loss: 2.0113182067871094
Validation loss: 2.025804956754049

Epoch: 126| Step: 0
Training loss: 2.33099627494812
Validation loss: 2.0369149446487427

Epoch: 6| Step: 1
Training loss: 2.304719924926758
Validation loss: 2.026510496934255

Epoch: 6| Step: 2
Training loss: 2.6590051651000977
Validation loss: 2.0327754418055215

Epoch: 6| Step: 3
Training loss: 1.5289502143859863
Validation loss: 2.042918562889099

Epoch: 6| Step: 4
Training loss: 1.9035906791687012
Validation loss: 2.0269657373428345

Epoch: 6| Step: 5
Training loss: 1.8395140171051025
Validation loss: 2.0348086754480996

Epoch: 6| Step: 6
Training loss: 1.9424265623092651
Validation loss: 2.0383484164873757

Epoch: 6| Step: 7
Training loss: 2.419910192489624
Validation loss: 2.0396317839622498

Epoch: 6| Step: 8
Training loss: 1.8806668519973755
Validation loss: 2.027103920777639

Epoch: 6| Step: 9
Training loss: 1.869307518005371
Validation loss: 2.031961957613627

Epoch: 6| Step: 10
Training loss: 2.386380910873413
Validation loss: 2.023551642894745

Epoch: 6| Step: 11
Training loss: 2.0634841918945312
Validation loss: 2.0217812856038413

Epoch: 6| Step: 12
Training loss: 2.0285210609436035
Validation loss: 2.026681343714396

Epoch: 6| Step: 13
Training loss: 2.027451753616333
Validation loss: 2.020327011744181

Epoch: 127| Step: 0
Training loss: 1.4551243782043457
Validation loss: 2.018036166826884

Epoch: 6| Step: 1
Training loss: 1.8384462594985962
Validation loss: 2.0179696679115295

Epoch: 6| Step: 2
Training loss: 1.833125114440918
Validation loss: 2.0278863509496055

Epoch: 6| Step: 3
Training loss: 2.4737489223480225
Validation loss: 2.0318801005681357

Epoch: 6| Step: 4
Training loss: 2.226865291595459
Validation loss: 2.03560076157252

Epoch: 6| Step: 5
Training loss: 2.171013355255127
Validation loss: 2.0460368593533835

Epoch: 6| Step: 6
Training loss: 1.9815069437026978
Validation loss: 2.034936547279358

Epoch: 6| Step: 7
Training loss: 2.182188034057617
Validation loss: 2.0356266697247825

Epoch: 6| Step: 8
Training loss: 1.8266856670379639
Validation loss: 2.029243012269338

Epoch: 6| Step: 9
Training loss: 2.6488585472106934
Validation loss: 2.021980047225952

Epoch: 6| Step: 10
Training loss: 2.3054347038269043
Validation loss: 2.0276532570521035

Epoch: 6| Step: 11
Training loss: 2.418612003326416
Validation loss: 2.022661109765371

Epoch: 6| Step: 12
Training loss: 1.9322481155395508
Validation loss: 2.0258765618006387

Epoch: 6| Step: 13
Training loss: 1.9805324077606201
Validation loss: 2.029337704181671

Epoch: 128| Step: 0
Training loss: 2.1358776092529297
Validation loss: 2.0270687341690063

Epoch: 6| Step: 1
Training loss: 2.1646342277526855
Validation loss: 2.02804829676946

Epoch: 6| Step: 2
Training loss: 2.1365413665771484
Validation loss: 2.0418380300203958

Epoch: 6| Step: 3
Training loss: 1.359093427658081
Validation loss: 2.036364754041036

Epoch: 6| Step: 4
Training loss: 2.2016139030456543
Validation loss: 2.0557623306910195

Epoch: 6| Step: 5
Training loss: 2.474757194519043
Validation loss: 2.0574769576390586

Epoch: 6| Step: 6
Training loss: 1.842912197113037
Validation loss: 2.0560354193051658

Epoch: 6| Step: 7
Training loss: 1.6856558322906494
Validation loss: 2.068482200304667

Epoch: 6| Step: 8
Training loss: 2.8421480655670166
Validation loss: 2.0653730630874634

Epoch: 6| Step: 9
Training loss: 2.175046443939209
Validation loss: 2.0601102113723755

Epoch: 6| Step: 10
Training loss: 1.8364161252975464
Validation loss: 2.0643086433410645

Epoch: 6| Step: 11
Training loss: 2.3495655059814453
Validation loss: 2.052435119946798

Epoch: 6| Step: 12
Training loss: 2.0577855110168457
Validation loss: 2.0282339056332908

Epoch: 6| Step: 13
Training loss: 1.7775189876556396
Validation loss: 2.027875026067098

Epoch: 129| Step: 0
Training loss: 1.6338226795196533
Validation loss: 2.0141688783963523

Epoch: 6| Step: 1
Training loss: 1.6859772205352783
Validation loss: 2.030061443646749

Epoch: 6| Step: 2
Training loss: 2.424609661102295
Validation loss: 2.0242521365483603

Epoch: 6| Step: 3
Training loss: 1.8910326957702637
Validation loss: 2.024179458618164

Epoch: 6| Step: 4
Training loss: 1.5971192121505737
Validation loss: 2.0276441176732383

Epoch: 6| Step: 5
Training loss: 1.8891974687576294
Validation loss: 2.0252752900123596

Epoch: 6| Step: 6
Training loss: 2.525014877319336
Validation loss: 2.0283456246058145

Epoch: 6| Step: 7
Training loss: 2.1322548389434814
Validation loss: 2.0320547819137573

Epoch: 6| Step: 8
Training loss: 2.0584144592285156
Validation loss: 2.0356560945510864

Epoch: 6| Step: 9
Training loss: 2.5228500366210938
Validation loss: 2.040845592816671

Epoch: 6| Step: 10
Training loss: 2.0173943042755127
Validation loss: 2.0275592605272927

Epoch: 6| Step: 11
Training loss: 2.329549789428711
Validation loss: 2.0385395685831704

Epoch: 6| Step: 12
Training loss: 2.3195247650146484
Validation loss: 2.0493341088294983

Epoch: 6| Step: 13
Training loss: 1.9414736032485962
Validation loss: 2.0373706022898355

Epoch: 130| Step: 0
Training loss: 2.1208200454711914
Validation loss: 2.0442859331766763

Epoch: 6| Step: 1
Training loss: 1.8730792999267578
Validation loss: 2.032007614771525

Epoch: 6| Step: 2
Training loss: 2.5558221340179443
Validation loss: 2.0367037653923035

Epoch: 6| Step: 3
Training loss: 2.340756416320801
Validation loss: 2.037749409675598

Epoch: 6| Step: 4
Training loss: 1.6545941829681396
Validation loss: 2.037038822968801

Epoch: 6| Step: 5
Training loss: 2.4221668243408203
Validation loss: 2.0443524718284607

Epoch: 6| Step: 6
Training loss: 1.667564868927002
Validation loss: 2.046196480592092

Epoch: 6| Step: 7
Training loss: 2.0965287685394287
Validation loss: 2.0525283018747964

Epoch: 6| Step: 8
Training loss: 1.923034429550171
Validation loss: 2.044941544532776

Epoch: 6| Step: 9
Training loss: 1.9708784818649292
Validation loss: 2.04603519042333

Epoch: 6| Step: 10
Training loss: 2.089237689971924
Validation loss: 2.0486331582069397

Epoch: 6| Step: 11
Training loss: 1.9108641147613525
Validation loss: 2.052127559979757

Epoch: 6| Step: 12
Training loss: 1.800270438194275
Validation loss: 2.0450313687324524

Epoch: 6| Step: 13
Training loss: 2.32285737991333
Validation loss: 2.0531500577926636

Epoch: 131| Step: 0
Training loss: 2.066885232925415
Validation loss: 2.0308945178985596

Epoch: 6| Step: 1
Training loss: 1.6328957080841064
Validation loss: 2.049152592817942

Epoch: 6| Step: 2
Training loss: 2.2775707244873047
Validation loss: 2.0356876651446023

Epoch: 6| Step: 3
Training loss: 1.8463964462280273
Validation loss: 2.0307600696881614

Epoch: 6| Step: 4
Training loss: 1.557929277420044
Validation loss: 2.0323866407076516

Epoch: 6| Step: 5
Training loss: 1.9983395338058472
Validation loss: 2.033528665701548

Epoch: 6| Step: 6
Training loss: 2.877070426940918
Validation loss: 2.025534212589264

Epoch: 6| Step: 7
Training loss: 2.2635657787323
Validation loss: 2.0293134252230325

Epoch: 6| Step: 8
Training loss: 1.7560182809829712
Validation loss: 2.0463566978772483

Epoch: 6| Step: 9
Training loss: 2.3247735500335693
Validation loss: 2.051801005999247

Epoch: 6| Step: 10
Training loss: 2.168851852416992
Validation loss: 2.0512532393137612

Epoch: 6| Step: 11
Training loss: 1.8927388191223145
Validation loss: 2.0223851998647056

Epoch: 6| Step: 12
Training loss: 2.248974084854126
Validation loss: 2.0281702478726706

Epoch: 6| Step: 13
Training loss: 2.0595149993896484
Validation loss: 2.0222649574279785

Epoch: 132| Step: 0
Training loss: 2.3464126586914062
Validation loss: 2.0258791049321494

Epoch: 6| Step: 1
Training loss: 3.033581495285034
Validation loss: 2.0283451477686563

Epoch: 6| Step: 2
Training loss: 1.85368013381958
Validation loss: 2.0237519343694053

Epoch: 6| Step: 3
Training loss: 1.6772605180740356
Validation loss: 2.0253352920214334

Epoch: 6| Step: 4
Training loss: 1.58866548538208
Validation loss: 2.0276393493016562

Epoch: 6| Step: 5
Training loss: 1.7538516521453857
Validation loss: 2.0351131359736123

Epoch: 6| Step: 6
Training loss: 2.474452257156372
Validation loss: 2.032223105430603

Epoch: 6| Step: 7
Training loss: 2.1312763690948486
Validation loss: 2.0349114735921225

Epoch: 6| Step: 8
Training loss: 1.9027506113052368
Validation loss: 2.0244466066360474

Epoch: 6| Step: 9
Training loss: 2.061126232147217
Validation loss: 2.02311380704244

Epoch: 6| Step: 10
Training loss: 2.0828096866607666
Validation loss: 2.0281655192375183

Epoch: 6| Step: 11
Training loss: 2.1368765830993652
Validation loss: 2.031891147295634

Epoch: 6| Step: 12
Training loss: 2.017897367477417
Validation loss: 2.0345759789148965

Epoch: 6| Step: 13
Training loss: 1.7522130012512207
Validation loss: 2.0448640982309976

Epoch: 133| Step: 0
Training loss: 2.033629894256592
Validation loss: 2.044256250063578

Epoch: 6| Step: 1
Training loss: 1.7004011869430542
Validation loss: 2.0401574969291687

Epoch: 6| Step: 2
Training loss: 1.400191068649292
Validation loss: 2.0420626401901245

Epoch: 6| Step: 3
Training loss: 2.4029436111450195
Validation loss: 2.0408055186271667

Epoch: 6| Step: 4
Training loss: 1.870497226715088
Validation loss: 2.043221414089203

Epoch: 6| Step: 5
Training loss: 2.3895211219787598
Validation loss: 2.034431219100952

Epoch: 6| Step: 6
Training loss: 1.8405876159667969
Validation loss: 2.0410364468892417

Epoch: 6| Step: 7
Training loss: 2.5637903213500977
Validation loss: 2.0439491669336953

Epoch: 6| Step: 8
Training loss: 1.854535698890686
Validation loss: 2.042003353436788

Epoch: 6| Step: 9
Training loss: 2.2303760051727295
Validation loss: 2.0286324620246887

Epoch: 6| Step: 10
Training loss: 1.5377111434936523
Validation loss: 2.0455344716707864

Epoch: 6| Step: 11
Training loss: 1.9592602252960205
Validation loss: 2.0347342689832053

Epoch: 6| Step: 12
Training loss: 2.3458337783813477
Validation loss: 2.053813894589742

Epoch: 6| Step: 13
Training loss: 2.5289244651794434
Validation loss: 2.0532687107721963

Epoch: 134| Step: 0
Training loss: 2.3141260147094727
Validation loss: 2.0718111594518027

Epoch: 6| Step: 1
Training loss: 2.4240293502807617
Validation loss: 2.0554152528444924

Epoch: 6| Step: 2
Training loss: 1.8514158725738525
Validation loss: 2.0570273796717324

Epoch: 6| Step: 3
Training loss: 2.776618480682373
Validation loss: 2.0568811496098838

Epoch: 6| Step: 4
Training loss: 1.9976245164871216
Validation loss: 2.048809230327606

Epoch: 6| Step: 5
Training loss: 2.0058822631835938
Validation loss: 2.0484962264696756

Epoch: 6| Step: 6
Training loss: 1.8622856140136719
Validation loss: 2.037534157435099

Epoch: 6| Step: 7
Training loss: 2.1693668365478516
Validation loss: 2.0446064273516336

Epoch: 6| Step: 8
Training loss: 2.0005393028259277
Validation loss: 2.0586083134015403

Epoch: 6| Step: 9
Training loss: 1.9799541234970093
Validation loss: 2.0619762738545737

Epoch: 6| Step: 10
Training loss: 2.295412540435791
Validation loss: 2.0579346815745034

Epoch: 6| Step: 11
Training loss: 2.018861770629883
Validation loss: 2.0597466826438904

Epoch: 6| Step: 12
Training loss: 1.7663710117340088
Validation loss: 2.049985190232595

Epoch: 6| Step: 13
Training loss: 1.3556281328201294
Validation loss: 2.0379369854927063

Epoch: 135| Step: 0
Training loss: 1.8726441860198975
Validation loss: 2.040049950281779

Epoch: 6| Step: 1
Training loss: 2.6684155464172363
Validation loss: 2.0419212579727173

Epoch: 6| Step: 2
Training loss: 2.3560903072357178
Validation loss: 2.0463637113571167

Epoch: 6| Step: 3
Training loss: 2.3520679473876953
Validation loss: 2.0398791233698526

Epoch: 6| Step: 4
Training loss: 2.020707368850708
Validation loss: 2.0376086036364236

Epoch: 6| Step: 5
Training loss: 1.733323335647583
Validation loss: 2.0351390838623047

Epoch: 6| Step: 6
Training loss: 1.9588074684143066
Validation loss: 2.036943276723226

Epoch: 6| Step: 7
Training loss: 1.6222563982009888
Validation loss: 2.0341779788335166

Epoch: 6| Step: 8
Training loss: 2.01654314994812
Validation loss: 2.0314496755599976

Epoch: 6| Step: 9
Training loss: 2.1687567234039307
Validation loss: 2.0257478753725686

Epoch: 6| Step: 10
Training loss: 2.6492018699645996
Validation loss: 2.0341341296831765

Epoch: 6| Step: 11
Training loss: 1.2168402671813965
Validation loss: 2.0382169087727866

Epoch: 6| Step: 12
Training loss: 1.965604305267334
Validation loss: 2.03397536277771

Epoch: 6| Step: 13
Training loss: 2.1558995246887207
Validation loss: 2.0560627977053323

Epoch: 136| Step: 0
Training loss: 1.9322490692138672
Validation loss: 2.06157918771108

Epoch: 6| Step: 1
Training loss: 2.6983141899108887
Validation loss: 2.065769096215566

Epoch: 6| Step: 2
Training loss: 2.9257500171661377
Validation loss: 2.067646026611328

Epoch: 6| Step: 3
Training loss: 1.930940866470337
Validation loss: 2.0638511180877686

Epoch: 6| Step: 4
Training loss: 2.1470208168029785
Validation loss: 2.046743909517924

Epoch: 6| Step: 5
Training loss: 1.3966917991638184
Validation loss: 2.030574162801107

Epoch: 6| Step: 6
Training loss: 1.829080581665039
Validation loss: 2.038845479488373

Epoch: 6| Step: 7
Training loss: 2.1889841556549072
Validation loss: 2.0352999766667685

Epoch: 6| Step: 8
Training loss: 1.551137924194336
Validation loss: 2.035323123137156

Epoch: 6| Step: 9
Training loss: 2.320024013519287
Validation loss: 2.0308001240094504

Epoch: 6| Step: 10
Training loss: 1.7967877388000488
Validation loss: 2.038407802581787

Epoch: 6| Step: 11
Training loss: 2.647010326385498
Validation loss: 2.036168416341146

Epoch: 6| Step: 12
Training loss: 1.4630475044250488
Validation loss: 2.0434022744496665

Epoch: 6| Step: 13
Training loss: 2.1730988025665283
Validation loss: 2.0457546710968018

Epoch: 137| Step: 0
Training loss: 2.0906543731689453
Validation loss: 2.0529049237569175

Epoch: 6| Step: 1
Training loss: 1.8011196851730347
Validation loss: 2.047690828641256

Epoch: 6| Step: 2
Training loss: 1.89835786819458
Validation loss: 2.059316555658976

Epoch: 6| Step: 3
Training loss: 2.1356823444366455
Validation loss: 2.0733635425567627

Epoch: 6| Step: 4
Training loss: 2.284834384918213
Validation loss: 2.0832329193751016

Epoch: 6| Step: 5
Training loss: 1.7156546115875244
Validation loss: 2.070964972178141

Epoch: 6| Step: 6
Training loss: 2.257296562194824
Validation loss: 2.0732295513153076

Epoch: 6| Step: 7
Training loss: 2.370450735092163
Validation loss: 2.0667830308278403

Epoch: 6| Step: 8
Training loss: 1.3654558658599854
Validation loss: 2.063325603802999

Epoch: 6| Step: 9
Training loss: 1.9879072904586792
Validation loss: 2.0585103233655295

Epoch: 6| Step: 10
Training loss: 1.866723895072937
Validation loss: 2.0525919795036316

Epoch: 6| Step: 11
Training loss: 2.6307172775268555
Validation loss: 2.0554256439208984

Epoch: 6| Step: 12
Training loss: 2.012950897216797
Validation loss: 2.0435511668523154

Epoch: 6| Step: 13
Training loss: 2.229464530944824
Validation loss: 2.047818044821421

Epoch: 138| Step: 0
Training loss: 2.439332962036133
Validation loss: 2.037361443042755

Epoch: 6| Step: 1
Training loss: 1.4409046173095703
Validation loss: 2.0509501298268638

Epoch: 6| Step: 2
Training loss: 2.2025146484375
Validation loss: 2.0400282740592957

Epoch: 6| Step: 3
Training loss: 1.971681833267212
Validation loss: 2.0401637951533

Epoch: 6| Step: 4
Training loss: 2.045388698577881
Validation loss: 2.0379430452982583

Epoch: 6| Step: 5
Training loss: 1.8846509456634521
Validation loss: 2.0399174888928733

Epoch: 6| Step: 6
Training loss: 2.4396700859069824
Validation loss: 2.0465358893076577

Epoch: 6| Step: 7
Training loss: 1.6385891437530518
Validation loss: 2.0474327206611633

Epoch: 6| Step: 8
Training loss: 2.3090226650238037
Validation loss: 2.0528777639071145

Epoch: 6| Step: 9
Training loss: 1.7321662902832031
Validation loss: 2.0543206532796225

Epoch: 6| Step: 10
Training loss: 1.7821364402770996
Validation loss: 2.0405651132265725

Epoch: 6| Step: 11
Training loss: 2.0880610942840576
Validation loss: 2.0323981444040933

Epoch: 6| Step: 12
Training loss: 2.2638092041015625
Validation loss: 2.042637050151825

Epoch: 6| Step: 13
Training loss: 2.363142728805542
Validation loss: 2.0411879420280457

Epoch: 139| Step: 0
Training loss: 2.0441691875457764
Validation loss: 2.0470171570777893

Epoch: 6| Step: 1
Training loss: 2.125782012939453
Validation loss: 2.0362449089686074

Epoch: 6| Step: 2
Training loss: 1.6689586639404297
Validation loss: 2.0319649974505105

Epoch: 6| Step: 3
Training loss: 2.2821221351623535
Validation loss: 2.0386223594347634

Epoch: 6| Step: 4
Training loss: 1.883589506149292
Validation loss: 2.0390038092931113

Epoch: 6| Step: 5
Training loss: 2.1140036582946777
Validation loss: 2.0458052158355713

Epoch: 6| Step: 6
Training loss: 2.0303573608398438
Validation loss: 2.045973797639211

Epoch: 6| Step: 7
Training loss: 1.8776726722717285
Validation loss: 2.0576156973838806

Epoch: 6| Step: 8
Training loss: 1.7666454315185547
Validation loss: 2.0473790764808655

Epoch: 6| Step: 9
Training loss: 2.1387877464294434
Validation loss: 2.0554595589637756

Epoch: 6| Step: 10
Training loss: 2.3409645557403564
Validation loss: 2.0530559023221335

Epoch: 6| Step: 11
Training loss: 2.207533359527588
Validation loss: 2.0524838169415793

Epoch: 6| Step: 12
Training loss: 2.1400251388549805
Validation loss: 2.0549726287523904

Epoch: 6| Step: 13
Training loss: 1.8097388744354248
Validation loss: 2.056155522664388

Epoch: 140| Step: 0
Training loss: 2.277665138244629
Validation loss: 2.0421460270881653

Epoch: 6| Step: 1
Training loss: 1.980366587638855
Validation loss: 2.05033411582311

Epoch: 6| Step: 2
Training loss: 2.384666919708252
Validation loss: 2.058025379975637

Epoch: 6| Step: 3
Training loss: 1.6473727226257324
Validation loss: 2.0592936078707376

Epoch: 6| Step: 4
Training loss: 2.054290771484375
Validation loss: 2.070443789164225

Epoch: 6| Step: 5
Training loss: 2.053030014038086
Validation loss: 2.065104285875956

Epoch: 6| Step: 6
Training loss: 1.950685977935791
Validation loss: 2.072731296221415

Epoch: 6| Step: 7
Training loss: 2.1280946731567383
Validation loss: 2.07145102818807

Epoch: 6| Step: 8
Training loss: 2.6209263801574707
Validation loss: 2.0673885146776834

Epoch: 6| Step: 9
Training loss: 1.9449976682662964
Validation loss: 2.0484715501467385

Epoch: 6| Step: 10
Training loss: 1.6334376335144043
Validation loss: 2.061773955821991

Epoch: 6| Step: 11
Training loss: 1.6371958255767822
Validation loss: 2.0658156871795654

Epoch: 6| Step: 12
Training loss: 2.8079962730407715
Validation loss: 2.061833838621775

Epoch: 6| Step: 13
Training loss: 1.3997275829315186
Validation loss: 2.050592601299286

Epoch: 141| Step: 0
Training loss: 2.1396484375
Validation loss: 2.0557382305463157

Epoch: 6| Step: 1
Training loss: 1.7805249691009521
Validation loss: 2.0498910347620645

Epoch: 6| Step: 2
Training loss: 1.6395559310913086
Validation loss: 2.0489815870920816

Epoch: 6| Step: 3
Training loss: 2.156031608581543
Validation loss: 2.042564411958059

Epoch: 6| Step: 4
Training loss: 1.6781795024871826
Validation loss: 2.0464775562286377

Epoch: 6| Step: 5
Training loss: 2.237962007522583
Validation loss: 2.0415385166803994

Epoch: 6| Step: 6
Training loss: 2.4844632148742676
Validation loss: 2.033185879389445

Epoch: 6| Step: 7
Training loss: 2.5992660522460938
Validation loss: 2.0370417833328247

Epoch: 6| Step: 8
Training loss: 2.229024648666382
Validation loss: 2.0415715177853904

Epoch: 6| Step: 9
Training loss: 2.0536417961120605
Validation loss: 2.0427521467208862

Epoch: 6| Step: 10
Training loss: 2.0821895599365234
Validation loss: 2.0517563422520957

Epoch: 6| Step: 11
Training loss: 1.395305871963501
Validation loss: 2.0482755303382874

Epoch: 6| Step: 12
Training loss: 1.7995089292526245
Validation loss: 2.0625307162602744

Epoch: 6| Step: 13
Training loss: 2.159313201904297
Validation loss: 2.077295939127604

Epoch: 142| Step: 0
Training loss: 2.197894811630249
Validation loss: 2.0998453895250955

Epoch: 6| Step: 1
Training loss: 2.2102785110473633
Validation loss: 2.0774202148119607

Epoch: 6| Step: 2
Training loss: 2.7578439712524414
Validation loss: 2.0902028679847717

Epoch: 6| Step: 3
Training loss: 1.4614665508270264
Validation loss: 2.0914265314737954

Epoch: 6| Step: 4
Training loss: 2.2057607173919678
Validation loss: 2.089606602986654

Epoch: 6| Step: 5
Training loss: 1.6015632152557373
Validation loss: 2.069939653078715

Epoch: 6| Step: 6
Training loss: 1.9427003860473633
Validation loss: 2.0785083572069802

Epoch: 6| Step: 7
Training loss: 1.4956984519958496
Validation loss: 2.059830923875173

Epoch: 6| Step: 8
Training loss: 2.1532206535339355
Validation loss: 2.078446388244629

Epoch: 6| Step: 9
Training loss: 1.5259723663330078
Validation loss: 2.0627166430155435

Epoch: 6| Step: 10
Training loss: 2.3319525718688965
Validation loss: 2.0638567407925925

Epoch: 6| Step: 11
Training loss: 1.79239022731781
Validation loss: 2.066091159979502

Epoch: 6| Step: 12
Training loss: 2.1458420753479004
Validation loss: 2.059453626473745

Epoch: 6| Step: 13
Training loss: 2.6037392616271973
Validation loss: 2.070399761199951

Epoch: 143| Step: 0
Training loss: 2.641845226287842
Validation loss: 2.0698587695757547

Epoch: 6| Step: 1
Training loss: 1.4415388107299805
Validation loss: 2.0704265038172402

Epoch: 6| Step: 2
Training loss: 1.8713358640670776
Validation loss: 2.0636526346206665

Epoch: 6| Step: 3
Training loss: 1.6915264129638672
Validation loss: 2.0568721493085227

Epoch: 6| Step: 4
Training loss: 1.6786744594573975
Validation loss: 2.0591218868891397

Epoch: 6| Step: 5
Training loss: 2.8279638290405273
Validation loss: 2.0642990867296853

Epoch: 6| Step: 6
Training loss: 1.8922624588012695
Validation loss: 2.0761996308962503

Epoch: 6| Step: 7
Training loss: 1.5578069686889648
Validation loss: 2.069616138935089

Epoch: 6| Step: 8
Training loss: 1.6760824918746948
Validation loss: 2.080860515435537

Epoch: 6| Step: 9
Training loss: 2.4115045070648193
Validation loss: 2.0798048973083496

Epoch: 6| Step: 10
Training loss: 1.8215172290802002
Validation loss: 2.07609615723292

Epoch: 6| Step: 11
Training loss: 2.3061141967773438
Validation loss: 2.0830979545911155

Epoch: 6| Step: 12
Training loss: 2.365603446960449
Validation loss: 2.0685975154240928

Epoch: 6| Step: 13
Training loss: 1.9573960304260254
Validation loss: 2.073396404584249

Epoch: 144| Step: 0
Training loss: 1.5395727157592773
Validation loss: 2.087765951951345

Epoch: 6| Step: 1
Training loss: 2.627423048019409
Validation loss: 2.066242774327596

Epoch: 6| Step: 2
Training loss: 1.6135088205337524
Validation loss: 2.062743882338206

Epoch: 6| Step: 3
Training loss: 2.0053179264068604
Validation loss: 2.067115902900696

Epoch: 6| Step: 4
Training loss: 1.7799315452575684
Validation loss: 2.0557558139165244

Epoch: 6| Step: 5
Training loss: 2.2672228813171387
Validation loss: 2.0558831294377646

Epoch: 6| Step: 6
Training loss: 2.151641368865967
Validation loss: 2.059484124183655

Epoch: 6| Step: 7
Training loss: 1.9452300071716309
Validation loss: 2.055827498435974

Epoch: 6| Step: 8
Training loss: 1.689173698425293
Validation loss: 2.060897946357727

Epoch: 6| Step: 9
Training loss: 2.5163254737854004
Validation loss: 2.0637176434199014

Epoch: 6| Step: 10
Training loss: 2.426760196685791
Validation loss: 2.062185267607371

Epoch: 6| Step: 11
Training loss: 1.924339771270752
Validation loss: 2.0525191823641458

Epoch: 6| Step: 12
Training loss: 1.3860244750976562
Validation loss: 2.06710954507192

Epoch: 6| Step: 13
Training loss: 2.237579584121704
Validation loss: 2.076153596242269

Epoch: 145| Step: 0
Training loss: 1.8023545742034912
Validation loss: 2.0723318258921304

Epoch: 6| Step: 1
Training loss: 1.9339580535888672
Validation loss: 2.070814828077952

Epoch: 6| Step: 2
Training loss: 1.9971354007720947
Validation loss: 2.072707712650299

Epoch: 6| Step: 3
Training loss: 3.078481435775757
Validation loss: 2.0608975489934287

Epoch: 6| Step: 4
Training loss: 2.209557294845581
Validation loss: 2.059564391771952

Epoch: 6| Step: 5
Training loss: 2.009871482849121
Validation loss: 2.056816577911377

Epoch: 6| Step: 6
Training loss: 2.0319108963012695
Validation loss: 2.0585328340530396

Epoch: 6| Step: 7
Training loss: 1.8218175172805786
Validation loss: 2.054974675178528

Epoch: 6| Step: 8
Training loss: 2.2249295711517334
Validation loss: 2.049607217311859

Epoch: 6| Step: 9
Training loss: 1.8676953315734863
Validation loss: 2.068306306997935

Epoch: 6| Step: 10
Training loss: 1.8997719287872314
Validation loss: 2.0613346497217813

Epoch: 6| Step: 11
Training loss: 2.0596694946289062
Validation loss: 2.0568770368893943

Epoch: 6| Step: 12
Training loss: 1.5299038887023926
Validation loss: 2.0646289189656577

Epoch: 6| Step: 13
Training loss: 1.848074197769165
Validation loss: 2.0752709110577903

Epoch: 146| Step: 0
Training loss: 2.0717530250549316
Validation loss: 2.0920769572257996

Epoch: 6| Step: 1
Training loss: 2.1669535636901855
Validation loss: 2.0773279070854187

Epoch: 6| Step: 2
Training loss: 2.529635190963745
Validation loss: 2.071630517641703

Epoch: 6| Step: 3
Training loss: 2.0786843299865723
Validation loss: 2.085267802079519

Epoch: 6| Step: 4
Training loss: 1.8433018922805786
Validation loss: 2.0736566384633384

Epoch: 6| Step: 5
Training loss: 2.420921802520752
Validation loss: 2.0558040539423623

Epoch: 6| Step: 6
Training loss: 1.8488153219223022
Validation loss: 2.0660247206687927

Epoch: 6| Step: 7
Training loss: 1.8732928037643433
Validation loss: 2.0556746125221252

Epoch: 6| Step: 8
Training loss: 1.2216553688049316
Validation loss: 2.0528688033421836

Epoch: 6| Step: 9
Training loss: 2.1267752647399902
Validation loss: 2.06613035996755

Epoch: 6| Step: 10
Training loss: 1.6935853958129883
Validation loss: 2.0693424940109253

Epoch: 6| Step: 11
Training loss: 2.1565310955047607
Validation loss: 2.0744513273239136

Epoch: 6| Step: 12
Training loss: 2.0318057537078857
Validation loss: 2.068492829799652

Epoch: 6| Step: 13
Training loss: 2.2646429538726807
Validation loss: 2.084717114766439

Epoch: 147| Step: 0
Training loss: 1.6996543407440186
Validation loss: 2.089918792247772

Epoch: 6| Step: 1
Training loss: 2.2237977981567383
Validation loss: 2.0798717737197876

Epoch: 6| Step: 2
Training loss: 1.6867941617965698
Validation loss: 2.0781168937683105

Epoch: 6| Step: 3
Training loss: 2.536855697631836
Validation loss: 2.0641918182373047

Epoch: 6| Step: 4
Training loss: 1.6874639987945557
Validation loss: 2.07076766093572

Epoch: 6| Step: 5
Training loss: 2.3496570587158203
Validation loss: 2.064314583937327

Epoch: 6| Step: 6
Training loss: 2.5410232543945312
Validation loss: 2.073089520136515

Epoch: 6| Step: 7
Training loss: 2.443812131881714
Validation loss: 2.0580442945162454

Epoch: 6| Step: 8
Training loss: 1.1412063837051392
Validation loss: 2.061457892258962

Epoch: 6| Step: 9
Training loss: 1.7255315780639648
Validation loss: 2.055883288383484

Epoch: 6| Step: 10
Training loss: 1.7492345571517944
Validation loss: 2.0632632970809937

Epoch: 6| Step: 11
Training loss: 2.2597310543060303
Validation loss: 2.0671092867851257

Epoch: 6| Step: 12
Training loss: 2.2408294677734375
Validation loss: 2.076302985350291

Epoch: 6| Step: 13
Training loss: 2.028036594390869
Validation loss: 2.0668299992879233

Epoch: 148| Step: 0
Training loss: 2.701141834259033
Validation loss: 2.069951375325521

Epoch: 6| Step: 1
Training loss: 1.3846534490585327
Validation loss: 2.075605591138204

Epoch: 6| Step: 2
Training loss: 2.312129020690918
Validation loss: 2.0864555835723877

Epoch: 6| Step: 3
Training loss: 1.4835102558135986
Validation loss: 2.0720947782198587

Epoch: 6| Step: 4
Training loss: 2.0448293685913086
Validation loss: 2.0784353216489158

Epoch: 6| Step: 5
Training loss: 2.5771291255950928
Validation loss: 2.07100244363149

Epoch: 6| Step: 6
Training loss: 2.58255672454834
Validation loss: 2.0669078826904297

Epoch: 6| Step: 7
Training loss: 1.8465620279312134
Validation loss: 2.066479663054148

Epoch: 6| Step: 8
Training loss: 2.0482125282287598
Validation loss: 2.069553474585215

Epoch: 6| Step: 9
Training loss: 1.5991401672363281
Validation loss: 2.0666019717852273

Epoch: 6| Step: 10
Training loss: 1.3441581726074219
Validation loss: 2.0702338417371116

Epoch: 6| Step: 11
Training loss: 1.8299157619476318
Validation loss: 2.0623364051183066

Epoch: 6| Step: 12
Training loss: 2.228137969970703
Validation loss: 2.074736018975576

Epoch: 6| Step: 13
Training loss: 2.1213645935058594
Validation loss: 2.083942453066508

Epoch: 149| Step: 0
Training loss: 1.9356799125671387
Validation loss: 2.103054185708364

Epoch: 6| Step: 1
Training loss: 2.4163169860839844
Validation loss: 2.10197780529658

Epoch: 6| Step: 2
Training loss: 1.873823881149292
Validation loss: 2.0935807625452676

Epoch: 6| Step: 3
Training loss: 1.8822736740112305
Validation loss: 2.100798765818278

Epoch: 6| Step: 4
Training loss: 1.5154786109924316
Validation loss: 2.099565029144287

Epoch: 6| Step: 5
Training loss: 2.181913375854492
Validation loss: 2.0905744433403015

Epoch: 6| Step: 6
Training loss: 1.9944101572036743
Validation loss: 2.0924927592277527

Epoch: 6| Step: 7
Training loss: 1.8260817527770996
Validation loss: 2.0649398962656655

Epoch: 6| Step: 8
Training loss: 2.022859573364258
Validation loss: 2.057806074619293

Epoch: 6| Step: 9
Training loss: 1.8215141296386719
Validation loss: 2.0583224097887673

Epoch: 6| Step: 10
Training loss: 2.866629123687744
Validation loss: 2.047049880027771

Epoch: 6| Step: 11
Training loss: 2.0123684406280518
Validation loss: 2.0506065686543784

Epoch: 6| Step: 12
Training loss: 1.80721914768219
Validation loss: 2.065264026323954

Epoch: 6| Step: 13
Training loss: 2.5307915210723877
Validation loss: 2.061992029349009

Epoch: 150| Step: 0
Training loss: 2.287351608276367
Validation loss: 2.068023602167765

Epoch: 6| Step: 1
Training loss: 1.8115142583847046
Validation loss: 2.078252633412679

Epoch: 6| Step: 2
Training loss: 1.9940617084503174
Validation loss: 2.0947448015213013

Epoch: 6| Step: 3
Training loss: 2.531376361846924
Validation loss: 2.098744034767151

Epoch: 6| Step: 4
Training loss: 1.4822211265563965
Validation loss: 2.090237339337667

Epoch: 6| Step: 5
Training loss: 2.070115566253662
Validation loss: 2.088618894418081

Epoch: 6| Step: 6
Training loss: 2.6359336376190186
Validation loss: 2.1270917455355325

Epoch: 6| Step: 7
Training loss: 2.435120105743408
Validation loss: 2.0926425655682883

Epoch: 6| Step: 8
Training loss: 2.0706472396850586
Validation loss: 2.1030364632606506

Epoch: 6| Step: 9
Training loss: 1.7190972566604614
Validation loss: 2.0795607368151345

Epoch: 6| Step: 10
Training loss: 1.3879754543304443
Validation loss: 2.08644046386083

Epoch: 6| Step: 11
Training loss: 1.8464784622192383
Validation loss: 2.0683135191599527

Epoch: 6| Step: 12
Training loss: 2.181145191192627
Validation loss: 2.067016323407491

Epoch: 6| Step: 13
Training loss: 2.1409912109375
Validation loss: 2.0390790502230325

Testing loss: 1.6860523515468022

Epoch: 1| Step: 0
Training loss: 5.116371154785156
Validation loss: 5.371665875116984

Epoch: 6| Step: 1
Training loss: 6.139603137969971
Validation loss: 5.369793891906738

Epoch: 6| Step: 2
Training loss: 6.644212245941162
Validation loss: 5.368045171101888

Epoch: 6| Step: 3
Training loss: 4.492387771606445
Validation loss: 5.366428295771281

Epoch: 6| Step: 4
Training loss: 5.441269874572754
Validation loss: 5.364961385726929

Epoch: 6| Step: 5
Training loss: 6.132852077484131
Validation loss: 5.363428433736165

Epoch: 6| Step: 6
Training loss: 4.326107978820801
Validation loss: 5.361971298853557

Epoch: 6| Step: 7
Training loss: 5.287042617797852
Validation loss: 5.360553979873657

Epoch: 6| Step: 8
Training loss: 5.997555255889893
Validation loss: 5.359137852986653

Epoch: 6| Step: 9
Training loss: 5.627097129821777
Validation loss: 5.357682228088379

Epoch: 6| Step: 10
Training loss: 5.4009599685668945
Validation loss: 5.356300592422485

Epoch: 6| Step: 11
Training loss: 5.111480236053467
Validation loss: 5.354804277420044

Epoch: 6| Step: 12
Training loss: 4.874609470367432
Validation loss: 5.353258689244588

Epoch: 6| Step: 13
Training loss: 5.434179306030273
Validation loss: 5.351762533187866

Epoch: 2| Step: 0
Training loss: 4.950636863708496
Validation loss: 5.350128014882405

Epoch: 6| Step: 1
Training loss: 6.299295902252197
Validation loss: 5.348414341608684

Epoch: 6| Step: 2
Training loss: 6.05855655670166
Validation loss: 5.3466871579488116

Epoch: 6| Step: 3
Training loss: 4.922616004943848
Validation loss: 5.3448366324106855

Epoch: 6| Step: 4
Training loss: 6.3604936599731445
Validation loss: 5.342915614446004

Epoch: 6| Step: 5
Training loss: 4.931621551513672
Validation loss: 5.340916554133098

Epoch: 6| Step: 6
Training loss: 4.6609907150268555
Validation loss: 5.338858604431152

Epoch: 6| Step: 7
Training loss: 5.874225616455078
Validation loss: 5.336719433466594

Epoch: 6| Step: 8
Training loss: 5.430458068847656
Validation loss: 5.334442218144734

Epoch: 6| Step: 9
Training loss: 4.939189910888672
Validation loss: 5.332087874412537

Epoch: 6| Step: 10
Training loss: 4.347124099731445
Validation loss: 5.329626401265462

Epoch: 6| Step: 11
Training loss: 5.809725761413574
Validation loss: 5.327074766159058

Epoch: 6| Step: 12
Training loss: 5.490095615386963
Validation loss: 5.32438063621521

Epoch: 6| Step: 13
Training loss: 5.613203048706055
Validation loss: 5.321597973505656

Epoch: 3| Step: 0
Training loss: 5.938409328460693
Validation loss: 5.318798144658406

Epoch: 6| Step: 1
Training loss: 5.408402442932129
Validation loss: 5.315749724706014

Epoch: 6| Step: 2
Training loss: 5.358908653259277
Validation loss: 5.312533458073934

Epoch: 6| Step: 3
Training loss: 4.515137195587158
Validation loss: 5.30929176012675

Epoch: 6| Step: 4
Training loss: 4.748878479003906
Validation loss: 5.305786530176799

Epoch: 6| Step: 5
Training loss: 5.280880451202393
Validation loss: 5.302173296610515

Epoch: 6| Step: 6
Training loss: 5.055826663970947
Validation loss: 5.298381010691325

Epoch: 6| Step: 7
Training loss: 4.115854740142822
Validation loss: 5.294414122899373

Epoch: 6| Step: 8
Training loss: 7.099278926849365
Validation loss: 5.290209611256917

Epoch: 6| Step: 9
Training loss: 4.779390335083008
Validation loss: 5.28587794303894

Epoch: 6| Step: 10
Training loss: 5.52720832824707
Validation loss: 5.281349182128906

Epoch: 6| Step: 11
Training loss: 5.109206199645996
Validation loss: 5.276553710301717

Epoch: 6| Step: 12
Training loss: 5.439964294433594
Validation loss: 5.271540721257527

Epoch: 6| Step: 13
Training loss: 6.719585418701172
Validation loss: 5.266281843185425

Epoch: 4| Step: 0
Training loss: 3.5783462524414062
Validation loss: 5.260883331298828

Epoch: 6| Step: 1
Training loss: 6.120702743530273
Validation loss: 5.255293607711792

Epoch: 6| Step: 2
Training loss: 4.740422248840332
Validation loss: 5.2493964831034345

Epoch: 6| Step: 3
Training loss: 5.108432769775391
Validation loss: 5.243448853492737

Epoch: 6| Step: 4
Training loss: 4.392782211303711
Validation loss: 5.237216234207153

Epoch: 6| Step: 5
Training loss: 6.139655113220215
Validation loss: 5.230713526407878

Epoch: 6| Step: 6
Training loss: 5.1825714111328125
Validation loss: 5.224176565806071

Epoch: 6| Step: 7
Training loss: 5.982172012329102
Validation loss: 5.217216809590657

Epoch: 6| Step: 8
Training loss: 5.065430641174316
Validation loss: 5.210100173950195

Epoch: 6| Step: 9
Training loss: 5.40631103515625
Validation loss: 5.20296589533488

Epoch: 6| Step: 10
Training loss: 5.265208721160889
Validation loss: 5.195396741231282

Epoch: 6| Step: 11
Training loss: 6.094232559204102
Validation loss: 5.187986612319946

Epoch: 6| Step: 12
Training loss: 5.576373100280762
Validation loss: 5.179992755254109

Epoch: 6| Step: 13
Training loss: 5.406624794006348
Validation loss: 5.171818971633911

Epoch: 5| Step: 0
Training loss: 4.394077301025391
Validation loss: 5.163371046384175

Epoch: 6| Step: 1
Training loss: 4.889791011810303
Validation loss: 5.154498020807902

Epoch: 6| Step: 2
Training loss: 5.543583869934082
Validation loss: 5.145573973655701

Epoch: 6| Step: 3
Training loss: 5.680063724517822
Validation loss: 5.135836203893025

Epoch: 6| Step: 4
Training loss: 4.356583595275879
Validation loss: 5.1264848709106445

Epoch: 6| Step: 5
Training loss: 5.678901672363281
Validation loss: 5.1161971886952715

Epoch: 6| Step: 6
Training loss: 4.449779033660889
Validation loss: 5.1060755252838135

Epoch: 6| Step: 7
Training loss: 4.901126861572266
Validation loss: 5.09533413251241

Epoch: 6| Step: 8
Training loss: 6.008085250854492
Validation loss: 5.084169308344523

Epoch: 6| Step: 9
Training loss: 5.8018975257873535
Validation loss: 5.072888374328613

Epoch: 6| Step: 10
Training loss: 6.018512725830078
Validation loss: 5.0611410935719805

Epoch: 6| Step: 11
Training loss: 5.941859245300293
Validation loss: 5.050299962361653

Epoch: 6| Step: 12
Training loss: 3.9225168228149414
Validation loss: 5.037809252738953

Epoch: 6| Step: 13
Training loss: 4.845965385437012
Validation loss: 5.026309251785278

Epoch: 6| Step: 0
Training loss: 5.423151016235352
Validation loss: 5.014410416285197

Epoch: 6| Step: 1
Training loss: 4.815889358520508
Validation loss: 5.002546866734822

Epoch: 6| Step: 2
Training loss: 5.7196502685546875
Validation loss: 4.990471363067627

Epoch: 6| Step: 3
Training loss: 3.9564318656921387
Validation loss: 4.978208700815837

Epoch: 6| Step: 4
Training loss: 5.517643928527832
Validation loss: 4.965960423151652

Epoch: 6| Step: 5
Training loss: 3.580368995666504
Validation loss: 4.953777154286702

Epoch: 6| Step: 6
Training loss: 5.460775375366211
Validation loss: 4.941742976506551

Epoch: 6| Step: 7
Training loss: 4.786924362182617
Validation loss: 4.929632107416789

Epoch: 6| Step: 8
Training loss: 5.425704002380371
Validation loss: 4.917843500773112

Epoch: 6| Step: 9
Training loss: 4.753213405609131
Validation loss: 4.905889987945557

Epoch: 6| Step: 10
Training loss: 5.088841915130615
Validation loss: 4.894414226214091

Epoch: 6| Step: 11
Training loss: 4.654388427734375
Validation loss: 4.882747411727905

Epoch: 6| Step: 12
Training loss: 6.549344062805176
Validation loss: 4.871028502782186

Epoch: 6| Step: 13
Training loss: 4.5432610511779785
Validation loss: 4.859304507573445

Epoch: 7| Step: 0
Training loss: 5.602863311767578
Validation loss: 4.847881714502971

Epoch: 6| Step: 1
Training loss: 4.704710960388184
Validation loss: 4.83616026242574

Epoch: 6| Step: 2
Training loss: 5.394223213195801
Validation loss: 4.82593035697937

Epoch: 6| Step: 3
Training loss: 4.415883541107178
Validation loss: 4.816479404767354

Epoch: 6| Step: 4
Training loss: 4.043616771697998
Validation loss: 4.80712087949117

Epoch: 6| Step: 5
Training loss: 5.91697359085083
Validation loss: 4.798175096511841

Epoch: 6| Step: 6
Training loss: 5.447717666625977
Validation loss: 4.78896427154541

Epoch: 6| Step: 7
Training loss: 5.492879867553711
Validation loss: 4.7792768478393555

Epoch: 6| Step: 8
Training loss: 4.536017894744873
Validation loss: 4.769941012064616

Epoch: 6| Step: 9
Training loss: 4.25248384475708
Validation loss: 4.7611769040425616

Epoch: 6| Step: 10
Training loss: 5.027102470397949
Validation loss: 4.752095937728882

Epoch: 6| Step: 11
Training loss: 5.39101505279541
Validation loss: 4.744096557299296

Epoch: 6| Step: 12
Training loss: 4.058847904205322
Validation loss: 4.736345370610555

Epoch: 6| Step: 13
Training loss: 3.9809982776641846
Validation loss: 4.728575348854065

Epoch: 8| Step: 0
Training loss: 3.5573229789733887
Validation loss: 4.721201618512471

Epoch: 6| Step: 1
Training loss: 3.7955219745635986
Validation loss: 4.71324626604716

Epoch: 6| Step: 2
Training loss: 4.550533294677734
Validation loss: 4.7055076360702515

Epoch: 6| Step: 3
Training loss: 4.867943286895752
Validation loss: 4.69801942507426

Epoch: 6| Step: 4
Training loss: 4.7848405838012695
Validation loss: 4.69098965326945

Epoch: 6| Step: 5
Training loss: 5.226603031158447
Validation loss: 4.684484640757243

Epoch: 6| Step: 6
Training loss: 4.852999210357666
Validation loss: 4.677478551864624

Epoch: 6| Step: 7
Training loss: 4.105883598327637
Validation loss: 4.671934684117635

Epoch: 6| Step: 8
Training loss: 4.358022689819336
Validation loss: 4.666106065114339

Epoch: 6| Step: 9
Training loss: 4.401272773742676
Validation loss: 4.6601951122283936

Epoch: 6| Step: 10
Training loss: 5.485039234161377
Validation loss: 4.654388626416524

Epoch: 6| Step: 11
Training loss: 5.060667514801025
Validation loss: 4.649080197016398

Epoch: 6| Step: 12
Training loss: 5.974706649780273
Validation loss: 4.642424662907918

Epoch: 6| Step: 13
Training loss: 5.78125
Validation loss: 4.6373997529347735

Epoch: 9| Step: 0
Training loss: 4.1018757820129395
Validation loss: 4.631588697433472

Epoch: 6| Step: 1
Training loss: 5.5487494468688965
Validation loss: 4.626807649930318

Epoch: 6| Step: 2
Training loss: 4.318783760070801
Validation loss: 4.6214213371276855

Epoch: 6| Step: 3
Training loss: 4.727258205413818
Validation loss: 4.616625547409058

Epoch: 6| Step: 4
Training loss: 4.14632511138916
Validation loss: 4.611329595247905

Epoch: 6| Step: 5
Training loss: 5.134531497955322
Validation loss: 4.606275240580241

Epoch: 6| Step: 6
Training loss: 4.531829833984375
Validation loss: 4.6017820835113525

Epoch: 6| Step: 7
Training loss: 4.3954949378967285
Validation loss: 4.596731901168823

Epoch: 6| Step: 8
Training loss: 4.5933732986450195
Validation loss: 4.5916885534922285

Epoch: 6| Step: 9
Training loss: 5.11775016784668
Validation loss: 4.587088425954183

Epoch: 6| Step: 10
Training loss: 4.16342306137085
Validation loss: 4.58238164583842

Epoch: 6| Step: 11
Training loss: 4.8694000244140625
Validation loss: 4.577146053314209

Epoch: 6| Step: 12
Training loss: 4.661660671234131
Validation loss: 4.572088877360026

Epoch: 6| Step: 13
Training loss: 5.480195045471191
Validation loss: 4.567322015762329

Epoch: 10| Step: 0
Training loss: 4.066095352172852
Validation loss: 4.5625996589660645

Epoch: 6| Step: 1
Training loss: 4.53524923324585
Validation loss: 4.55813725789388

Epoch: 6| Step: 2
Training loss: 5.028109550476074
Validation loss: 4.554032643636067

Epoch: 6| Step: 3
Training loss: 4.075103282928467
Validation loss: 4.549534400304158

Epoch: 6| Step: 4
Training loss: 5.121493339538574
Validation loss: 4.5445544719696045

Epoch: 6| Step: 5
Training loss: 5.086585998535156
Validation loss: 4.539688587188721

Epoch: 6| Step: 6
Training loss: 4.570678234100342
Validation loss: 4.5347535610198975

Epoch: 6| Step: 7
Training loss: 5.27767276763916
Validation loss: 4.530166268348694

Epoch: 6| Step: 8
Training loss: 4.906391143798828
Validation loss: 4.526348193486531

Epoch: 6| Step: 9
Training loss: 4.494105339050293
Validation loss: 4.521578431129456

Epoch: 6| Step: 10
Training loss: 4.478368759155273
Validation loss: 4.517332355181376

Epoch: 6| Step: 11
Training loss: 4.4698333740234375
Validation loss: 4.512767632802327

Epoch: 6| Step: 12
Training loss: 4.788780212402344
Validation loss: 4.508081356684367

Epoch: 6| Step: 13
Training loss: 4.046799182891846
Validation loss: 4.503769834836324

Epoch: 11| Step: 0
Training loss: 4.030590534210205
Validation loss: 4.499638398488362

Epoch: 6| Step: 1
Training loss: 4.959670066833496
Validation loss: 4.495526870091756

Epoch: 6| Step: 2
Training loss: 4.451015472412109
Validation loss: 4.4908367395401

Epoch: 6| Step: 3
Training loss: 4.7647705078125
Validation loss: 4.486621379852295

Epoch: 6| Step: 4
Training loss: 4.934924125671387
Validation loss: 4.482593059539795

Epoch: 6| Step: 5
Training loss: 4.9820404052734375
Validation loss: 4.4781138102213545

Epoch: 6| Step: 6
Training loss: 3.886307954788208
Validation loss: 4.473892649014791

Epoch: 6| Step: 7
Training loss: 4.52850341796875
Validation loss: 4.469716707865397

Epoch: 6| Step: 8
Training loss: 5.197234153747559
Validation loss: 4.465659300486247

Epoch: 6| Step: 9
Training loss: 3.892772912979126
Validation loss: 4.461511691411336

Epoch: 6| Step: 10
Training loss: 4.469760894775391
Validation loss: 4.457212368647258

Epoch: 6| Step: 11
Training loss: 4.942168712615967
Validation loss: 4.452654600143433

Epoch: 6| Step: 12
Training loss: 4.247066497802734
Validation loss: 4.447951912879944

Epoch: 6| Step: 13
Training loss: 4.860042572021484
Validation loss: 4.443944215774536

Epoch: 12| Step: 0
Training loss: 4.936286926269531
Validation loss: 4.439279317855835

Epoch: 6| Step: 1
Training loss: 4.795296669006348
Validation loss: 4.4348963896433515

Epoch: 6| Step: 2
Training loss: 5.4978790283203125
Validation loss: 4.430419723192851

Epoch: 6| Step: 3
Training loss: 4.1187744140625
Validation loss: 4.425904353459676

Epoch: 6| Step: 4
Training loss: 5.671422958374023
Validation loss: 4.4211851755778

Epoch: 6| Step: 5
Training loss: 4.209820747375488
Validation loss: 4.416394591331482

Epoch: 6| Step: 6
Training loss: 5.419114589691162
Validation loss: 4.41256324450175

Epoch: 6| Step: 7
Training loss: 5.098327159881592
Validation loss: 4.4079538981119795

Epoch: 6| Step: 8
Training loss: 3.8634371757507324
Validation loss: 4.403181076049805

Epoch: 6| Step: 9
Training loss: 4.8264923095703125
Validation loss: 4.399314125378926

Epoch: 6| Step: 10
Training loss: 4.3432416915893555
Validation loss: 4.394310514132182

Epoch: 6| Step: 11
Training loss: 3.447779893875122
Validation loss: 4.390404383341472

Epoch: 6| Step: 12
Training loss: 3.3541197776794434
Validation loss: 4.3857548634211225

Epoch: 6| Step: 13
Training loss: 3.821627378463745
Validation loss: 4.381166974703471

Epoch: 13| Step: 0
Training loss: 3.945983409881592
Validation loss: 4.376648386319478

Epoch: 6| Step: 1
Training loss: 4.905653953552246
Validation loss: 4.372316638628642

Epoch: 6| Step: 2
Training loss: 4.114865303039551
Validation loss: 4.368130485216777

Epoch: 6| Step: 3
Training loss: 3.7406764030456543
Validation loss: 4.364026467005412

Epoch: 6| Step: 4
Training loss: 4.344456195831299
Validation loss: 4.35947569211324

Epoch: 6| Step: 5
Training loss: 4.232278823852539
Validation loss: 4.354489008585612

Epoch: 6| Step: 6
Training loss: 4.608968734741211
Validation loss: 4.349890788396199

Epoch: 6| Step: 7
Training loss: 4.530365467071533
Validation loss: 4.344932516415914

Epoch: 6| Step: 8
Training loss: 4.276464462280273
Validation loss: 4.341269493103027

Epoch: 6| Step: 9
Training loss: 5.178915977478027
Validation loss: 4.336999416351318

Epoch: 6| Step: 10
Training loss: 4.248824119567871
Validation loss: 4.331619739532471

Epoch: 6| Step: 11
Training loss: 4.7487640380859375
Validation loss: 4.326990087827046

Epoch: 6| Step: 12
Training loss: 4.821723461151123
Validation loss: 4.3223923444747925

Epoch: 6| Step: 13
Training loss: 4.887563705444336
Validation loss: 4.317886432011922

Epoch: 14| Step: 0
Training loss: 4.359482288360596
Validation loss: 4.31310232480367

Epoch: 6| Step: 1
Training loss: 4.437085151672363
Validation loss: 4.3076536655426025

Epoch: 6| Step: 2
Training loss: 4.314254283905029
Validation loss: 4.303295453389485

Epoch: 6| Step: 3
Training loss: 4.313907146453857
Validation loss: 4.298675537109375

Epoch: 6| Step: 4
Training loss: 4.448936462402344
Validation loss: 4.293557167053223

Epoch: 6| Step: 5
Training loss: 5.025983810424805
Validation loss: 4.2887576421101885

Epoch: 6| Step: 6
Training loss: 4.812171459197998
Validation loss: 4.2835861047108965

Epoch: 6| Step: 7
Training loss: 4.259400844573975
Validation loss: 4.278492569923401

Epoch: 6| Step: 8
Training loss: 4.622346878051758
Validation loss: 4.274036407470703

Epoch: 6| Step: 9
Training loss: 3.315065383911133
Validation loss: 4.269055525461833

Epoch: 6| Step: 10
Training loss: 3.6764931678771973
Validation loss: 4.264039595921834

Epoch: 6| Step: 11
Training loss: 3.6682276725769043
Validation loss: 4.258939305941264

Epoch: 6| Step: 12
Training loss: 5.687972068786621
Validation loss: 4.254254500071208

Epoch: 6| Step: 13
Training loss: 4.764541149139404
Validation loss: 4.248270153999329

Epoch: 15| Step: 0
Training loss: 3.3148233890533447
Validation loss: 4.243484457333882

Epoch: 6| Step: 1
Training loss: 4.641874313354492
Validation loss: 4.2382858991622925

Epoch: 6| Step: 2
Training loss: 3.3552441596984863
Validation loss: 4.233960866928101

Epoch: 6| Step: 3
Training loss: 4.652675628662109
Validation loss: 4.227793733278911

Epoch: 6| Step: 4
Training loss: 4.57939338684082
Validation loss: 4.22253680229187

Epoch: 6| Step: 5
Training loss: 3.726994276046753
Validation loss: 4.21697994073232

Epoch: 6| Step: 6
Training loss: 6.353386402130127
Validation loss: 4.211851080258687

Epoch: 6| Step: 7
Training loss: 4.687986373901367
Validation loss: 4.206209659576416

Epoch: 6| Step: 8
Training loss: 3.5977702140808105
Validation loss: 4.200826207796733

Epoch: 6| Step: 9
Training loss: 4.2635297775268555
Validation loss: 4.195606390635173

Epoch: 6| Step: 10
Training loss: 5.053103446960449
Validation loss: 4.190194567044576

Epoch: 6| Step: 11
Training loss: 4.398690223693848
Validation loss: 4.184723138809204

Epoch: 6| Step: 12
Training loss: 4.388278961181641
Validation loss: 4.1796804666519165

Epoch: 6| Step: 13
Training loss: 3.786379814147949
Validation loss: 4.174930890401204

Epoch: 16| Step: 0
Training loss: 3.7814154624938965
Validation loss: 4.169666687647502

Epoch: 6| Step: 1
Training loss: 4.618340969085693
Validation loss: 4.164374828338623

Epoch: 6| Step: 2
Training loss: 3.418170690536499
Validation loss: 4.159208655357361

Epoch: 6| Step: 3
Training loss: 3.34283447265625
Validation loss: 4.153582493464152

Epoch: 6| Step: 4
Training loss: 3.7819619178771973
Validation loss: 4.14897620677948

Epoch: 6| Step: 5
Training loss: 4.66449499130249
Validation loss: 4.144269108772278

Epoch: 6| Step: 6
Training loss: 5.715407371520996
Validation loss: 4.13893445332845

Epoch: 6| Step: 7
Training loss: 3.984288215637207
Validation loss: 4.133435487747192

Epoch: 6| Step: 8
Training loss: 5.170624732971191
Validation loss: 4.128363609313965

Epoch: 6| Step: 9
Training loss: 3.534698009490967
Validation loss: 4.124182979265849

Epoch: 6| Step: 10
Training loss: 4.67460823059082
Validation loss: 4.118437687555949

Epoch: 6| Step: 11
Training loss: 4.252685070037842
Validation loss: 4.113115827242534

Epoch: 6| Step: 12
Training loss: 4.727961540222168
Validation loss: 4.108229478200276

Epoch: 6| Step: 13
Training loss: 4.19291877746582
Validation loss: 4.103851119677226

Epoch: 17| Step: 0
Training loss: 3.9550304412841797
Validation loss: 4.0983221133550005

Epoch: 6| Step: 1
Training loss: 4.082202911376953
Validation loss: 4.093110799789429

Epoch: 6| Step: 2
Training loss: 4.842711448669434
Validation loss: 4.0888698895772295

Epoch: 6| Step: 3
Training loss: 5.293337345123291
Validation loss: 4.084152102470398

Epoch: 6| Step: 4
Training loss: 3.7834887504577637
Validation loss: 4.079287091890971

Epoch: 6| Step: 5
Training loss: 4.267057418823242
Validation loss: 4.073309580485026

Epoch: 6| Step: 6
Training loss: 4.142563819885254
Validation loss: 4.068619887034099

Epoch: 6| Step: 7
Training loss: 4.324326515197754
Validation loss: 4.063639958699544

Epoch: 6| Step: 8
Training loss: 3.887829065322876
Validation loss: 4.0581973393758135

Epoch: 6| Step: 9
Training loss: 3.5672075748443604
Validation loss: 4.054121216138204

Epoch: 6| Step: 10
Training loss: 3.9080169200897217
Validation loss: 4.050315062204997

Epoch: 6| Step: 11
Training loss: 4.325069427490234
Validation loss: 4.044324437777202

Epoch: 6| Step: 12
Training loss: 4.586670875549316
Validation loss: 4.039491454760234

Epoch: 6| Step: 13
Training loss: 3.963764190673828
Validation loss: 4.035072326660156

Epoch: 18| Step: 0
Training loss: 4.04307746887207
Validation loss: 4.029866814613342

Epoch: 6| Step: 1
Training loss: 3.671238660812378
Validation loss: 4.0249704122543335

Epoch: 6| Step: 2
Training loss: 4.516378402709961
Validation loss: 4.01991347471873

Epoch: 6| Step: 3
Training loss: 4.765303611755371
Validation loss: 4.0155811707178755

Epoch: 6| Step: 4
Training loss: 4.2625274658203125
Validation loss: 4.010816733042399

Epoch: 6| Step: 5
Training loss: 3.799774646759033
Validation loss: 4.004914363225301

Epoch: 6| Step: 6
Training loss: 4.482151031494141
Validation loss: 4.000831564267476

Epoch: 6| Step: 7
Training loss: 3.475635051727295
Validation loss: 3.9952561060587564

Epoch: 6| Step: 8
Training loss: 4.527297496795654
Validation loss: 3.9912322362264

Epoch: 6| Step: 9
Training loss: 4.508209705352783
Validation loss: 3.9852030674616494

Epoch: 6| Step: 10
Training loss: 3.352398157119751
Validation loss: 3.982175668080648

Epoch: 6| Step: 11
Training loss: 4.586066722869873
Validation loss: 3.9793734153111777

Epoch: 6| Step: 12
Training loss: 4.710092544555664
Validation loss: 3.9726908604303994

Epoch: 6| Step: 13
Training loss: 3.3060171604156494
Validation loss: 3.9658756256103516

Epoch: 19| Step: 0
Training loss: 3.673065185546875
Validation loss: 3.9612577756245932

Epoch: 6| Step: 1
Training loss: 4.166600227355957
Validation loss: 3.958203395207723

Epoch: 6| Step: 2
Training loss: 3.6882948875427246
Validation loss: 3.951366106669108

Epoch: 6| Step: 3
Training loss: 3.308393955230713
Validation loss: 3.945002317428589

Epoch: 6| Step: 4
Training loss: 4.518774032592773
Validation loss: 3.942854960759481

Epoch: 6| Step: 5
Training loss: 4.06597900390625
Validation loss: 3.9384659131368003

Epoch: 6| Step: 6
Training loss: 3.974364995956421
Validation loss: 3.932727495829264

Epoch: 6| Step: 7
Training loss: 4.4306817054748535
Validation loss: 3.9254890282948813

Epoch: 6| Step: 8
Training loss: 3.6158218383789062
Validation loss: 3.920508623123169

Epoch: 6| Step: 9
Training loss: 4.41701078414917
Validation loss: 3.91594660282135

Epoch: 6| Step: 10
Training loss: 4.302834510803223
Validation loss: 3.909395376841227

Epoch: 6| Step: 11
Training loss: 4.702282428741455
Validation loss: 3.9057833751042685

Epoch: 6| Step: 12
Training loss: 4.999725341796875
Validation loss: 3.900930166244507

Epoch: 6| Step: 13
Training loss: 3.184394121170044
Validation loss: 3.8947864373524985

Epoch: 20| Step: 0
Training loss: 4.367720603942871
Validation loss: 3.889901955922445

Epoch: 6| Step: 1
Training loss: 4.398611545562744
Validation loss: 3.8839391072591147

Epoch: 6| Step: 2
Training loss: 3.9747862815856934
Validation loss: 3.8799568812052407

Epoch: 6| Step: 3
Training loss: 4.011429786682129
Validation loss: 3.87471870581309

Epoch: 6| Step: 4
Training loss: 2.324728012084961
Validation loss: 3.869502584139506

Epoch: 6| Step: 5
Training loss: 3.294631004333496
Validation loss: 3.864702343940735

Epoch: 6| Step: 6
Training loss: 3.8606419563293457
Validation loss: 3.8597259521484375

Epoch: 6| Step: 7
Training loss: 5.07448673248291
Validation loss: 3.8549695014953613

Epoch: 6| Step: 8
Training loss: 4.341621398925781
Validation loss: 3.850631753603617

Epoch: 6| Step: 9
Training loss: 3.9264206886291504
Validation loss: 3.8456794023513794

Epoch: 6| Step: 10
Training loss: 3.6456453800201416
Validation loss: 3.8408069610595703

Epoch: 6| Step: 11
Training loss: 2.7696259021759033
Validation loss: 3.8372241258621216

Epoch: 6| Step: 12
Training loss: 5.134809970855713
Validation loss: 3.831553101539612

Epoch: 6| Step: 13
Training loss: 4.992036819458008
Validation loss: 3.826972762743632

Epoch: 21| Step: 0
Training loss: 4.432307720184326
Validation loss: 3.821316202481588

Epoch: 6| Step: 1
Training loss: 4.196311950683594
Validation loss: 3.817330757776896

Epoch: 6| Step: 2
Training loss: 3.4933111667633057
Validation loss: 3.8125298420588174

Epoch: 6| Step: 3
Training loss: 4.022670269012451
Validation loss: 3.8072375059127808

Epoch: 6| Step: 4
Training loss: 3.335132360458374
Validation loss: 3.8028595447540283

Epoch: 6| Step: 5
Training loss: 3.472069501876831
Validation loss: 3.797063112258911

Epoch: 6| Step: 6
Training loss: 4.612857818603516
Validation loss: 3.7912777264912925

Epoch: 6| Step: 7
Training loss: 3.4683477878570557
Validation loss: 3.7852754990259805

Epoch: 6| Step: 8
Training loss: 4.452670097351074
Validation loss: 3.780748883883158

Epoch: 6| Step: 9
Training loss: 3.919642925262451
Validation loss: 3.774364431699117

Epoch: 6| Step: 10
Training loss: 4.366109848022461
Validation loss: 3.7694841225941977

Epoch: 6| Step: 11
Training loss: 2.795217514038086
Validation loss: 3.7642752726872764

Epoch: 6| Step: 12
Training loss: 4.984066009521484
Validation loss: 3.7589652140935264

Epoch: 6| Step: 13
Training loss: 3.6797890663146973
Validation loss: 3.7548289696375527

Epoch: 22| Step: 0
Training loss: 4.4110307693481445
Validation loss: 3.7500096956888833

Epoch: 6| Step: 1
Training loss: 4.908542633056641
Validation loss: 3.745510697364807

Epoch: 6| Step: 2
Training loss: 4.071342468261719
Validation loss: 3.74046258131663

Epoch: 6| Step: 3
Training loss: 3.877148151397705
Validation loss: 3.7356715202331543

Epoch: 6| Step: 4
Training loss: 4.326174736022949
Validation loss: 3.73087743918101

Epoch: 6| Step: 5
Training loss: 3.866403579711914
Validation loss: 3.7263676722844443

Epoch: 6| Step: 6
Training loss: 3.213069438934326
Validation loss: 3.7222599188486734

Epoch: 6| Step: 7
Training loss: 4.114131927490234
Validation loss: 3.717237671216329

Epoch: 6| Step: 8
Training loss: 4.2198052406311035
Validation loss: 3.713328798611959

Epoch: 6| Step: 9
Training loss: 3.8793516159057617
Validation loss: 3.7079636255900064

Epoch: 6| Step: 10
Training loss: 3.2922396659851074
Validation loss: 3.704164465268453

Epoch: 6| Step: 11
Training loss: 3.6839208602905273
Validation loss: 3.698965589205424

Epoch: 6| Step: 12
Training loss: 2.893381118774414
Validation loss: 3.695094188054403

Epoch: 6| Step: 13
Training loss: 3.572760581970215
Validation loss: 3.6904039780298867

Epoch: 23| Step: 0
Training loss: 4.1628737449646
Validation loss: 3.6869313716888428

Epoch: 6| Step: 1
Training loss: 3.9375598430633545
Validation loss: 3.6822009086608887

Epoch: 6| Step: 2
Training loss: 3.6870152950286865
Validation loss: 3.6772059996922812

Epoch: 6| Step: 3
Training loss: 3.775438070297241
Validation loss: 3.673003156979879

Epoch: 6| Step: 4
Training loss: 3.0297083854675293
Validation loss: 3.668689767519633

Epoch: 6| Step: 5
Training loss: 4.534628868103027
Validation loss: 3.664202928543091

Epoch: 6| Step: 6
Training loss: 3.7375118732452393
Validation loss: 3.6595898071924844

Epoch: 6| Step: 7
Training loss: 3.838130235671997
Validation loss: 3.6541141271591187

Epoch: 6| Step: 8
Training loss: 3.804192066192627
Validation loss: 3.6496536334355674

Epoch: 6| Step: 9
Training loss: 3.2760143280029297
Validation loss: 3.6449204285939536

Epoch: 6| Step: 10
Training loss: 3.1829605102539062
Validation loss: 3.6417685747146606

Epoch: 6| Step: 11
Training loss: 4.560457229614258
Validation loss: 3.63654100894928

Epoch: 6| Step: 12
Training loss: 3.400573253631592
Validation loss: 3.6322181224823

Epoch: 6| Step: 13
Training loss: 4.564908504486084
Validation loss: 3.628140449523926

Epoch: 24| Step: 0
Training loss: 4.152087211608887
Validation loss: 3.6237537463506064

Epoch: 6| Step: 1
Training loss: 4.394543170928955
Validation loss: 3.619258681933085

Epoch: 6| Step: 2
Training loss: 3.5981643199920654
Validation loss: 3.6146036783854165

Epoch: 6| Step: 3
Training loss: 2.8147530555725098
Validation loss: 3.609983205795288

Epoch: 6| Step: 4
Training loss: 2.967273473739624
Validation loss: 3.606397350629171

Epoch: 6| Step: 5
Training loss: 3.486435890197754
Validation loss: 3.602519154548645

Epoch: 6| Step: 6
Training loss: 4.68644380569458
Validation loss: 3.5980422099431357

Epoch: 6| Step: 7
Training loss: 3.0009541511535645
Validation loss: 3.593851884206136

Epoch: 6| Step: 8
Training loss: 4.189659118652344
Validation loss: 3.588908871014913

Epoch: 6| Step: 9
Training loss: 4.1963934898376465
Validation loss: 3.584581653277079

Epoch: 6| Step: 10
Training loss: 3.6980128288269043
Validation loss: 3.580094337463379

Epoch: 6| Step: 11
Training loss: 4.956484794616699
Validation loss: 3.576128919919332

Epoch: 6| Step: 12
Training loss: 3.3806095123291016
Validation loss: 3.5707918405532837

Epoch: 6| Step: 13
Training loss: 3.0979175567626953
Validation loss: 3.5672756830851235

Epoch: 25| Step: 0
Training loss: 3.7670650482177734
Validation loss: 3.562235474586487

Epoch: 6| Step: 1
Training loss: 3.998271942138672
Validation loss: 3.557054956754049

Epoch: 6| Step: 2
Training loss: 3.0117745399475098
Validation loss: 3.5526645183563232

Epoch: 6| Step: 3
Training loss: 3.904461145401001
Validation loss: 3.5477234919865928

Epoch: 6| Step: 4
Training loss: 3.425490379333496
Validation loss: 3.543119033177694

Epoch: 6| Step: 5
Training loss: 4.075331687927246
Validation loss: 3.539582928021749

Epoch: 6| Step: 6
Training loss: 3.9596621990203857
Validation loss: 3.535274863243103

Epoch: 6| Step: 7
Training loss: 5.079692363739014
Validation loss: 3.530784805615743

Epoch: 6| Step: 8
Training loss: 3.6715869903564453
Validation loss: 3.5261542797088623

Epoch: 6| Step: 9
Training loss: 3.536963939666748
Validation loss: 3.5199926694234214

Epoch: 6| Step: 10
Training loss: 3.7260565757751465
Validation loss: 3.5160805384318032

Epoch: 6| Step: 11
Training loss: 3.4545323848724365
Validation loss: 3.5115906794865928

Epoch: 6| Step: 12
Training loss: 2.7364978790283203
Validation loss: 3.5073010126749673

Epoch: 6| Step: 13
Training loss: 3.41511869430542
Validation loss: 3.5028297901153564

Epoch: 26| Step: 0
Training loss: 3.8897581100463867
Validation loss: 3.498890439669291

Epoch: 6| Step: 1
Training loss: 4.326623916625977
Validation loss: 3.4928536812464395

Epoch: 6| Step: 2
Training loss: 3.628934383392334
Validation loss: 3.4879525105158486

Epoch: 6| Step: 3
Training loss: 3.075617790222168
Validation loss: 3.4836206833521524

Epoch: 6| Step: 4
Training loss: 3.5677201747894287
Validation loss: 3.479646325111389

Epoch: 6| Step: 5
Training loss: 3.1789393424987793
Validation loss: 3.474829912185669

Epoch: 6| Step: 6
Training loss: 3.801114797592163
Validation loss: 3.4706308841705322

Epoch: 6| Step: 7
Training loss: 4.306310176849365
Validation loss: 3.46501632531484

Epoch: 6| Step: 8
Training loss: 3.347747802734375
Validation loss: 3.4608538150787354

Epoch: 6| Step: 9
Training loss: 3.2859959602355957
Validation loss: 3.4742868741353354

Epoch: 6| Step: 10
Training loss: 4.488219261169434
Validation loss: 3.4518667459487915

Epoch: 6| Step: 11
Training loss: 4.055534839630127
Validation loss: 3.4471896092096963

Epoch: 6| Step: 12
Training loss: 2.7370338439941406
Validation loss: 3.4453402757644653

Epoch: 6| Step: 13
Training loss: 3.2380313873291016
Validation loss: 3.4440507888793945

Epoch: 27| Step: 0
Training loss: 2.713986396789551
Validation loss: 3.441367785135905

Epoch: 6| Step: 1
Training loss: 3.802495002746582
Validation loss: 3.434733033180237

Epoch: 6| Step: 2
Training loss: 3.4768288135528564
Validation loss: 3.427763740221659

Epoch: 6| Step: 3
Training loss: 4.027953147888184
Validation loss: 3.4225031534830728

Epoch: 6| Step: 4
Training loss: 4.41636323928833
Validation loss: 3.4192803303400674

Epoch: 6| Step: 5
Training loss: 3.0008769035339355
Validation loss: 3.415841817855835

Epoch: 6| Step: 6
Training loss: 4.003679275512695
Validation loss: 3.4123318195343018

Epoch: 6| Step: 7
Training loss: 3.5729775428771973
Validation loss: 3.407461961110433

Epoch: 6| Step: 8
Training loss: 3.5885796546936035
Validation loss: 3.401535987854004

Epoch: 6| Step: 9
Training loss: 3.7016191482543945
Validation loss: 3.3958565394083657

Epoch: 6| Step: 10
Training loss: 3.953611373901367
Validation loss: 3.391166567802429

Epoch: 6| Step: 11
Training loss: 3.0623981952667236
Validation loss: 3.386279503504435

Epoch: 6| Step: 12
Training loss: 3.4314699172973633
Validation loss: 3.3817894061406455

Epoch: 6| Step: 13
Training loss: 3.3090462684631348
Validation loss: 3.377653161684672

Epoch: 28| Step: 0
Training loss: 3.392219305038452
Validation loss: 3.373017986615499

Epoch: 6| Step: 1
Training loss: 3.4123125076293945
Validation loss: 3.368849515914917

Epoch: 6| Step: 2
Training loss: 2.731630802154541
Validation loss: 3.363693634668986

Epoch: 6| Step: 3
Training loss: 4.65781307220459
Validation loss: 3.359293222427368

Epoch: 6| Step: 4
Training loss: 3.545650005340576
Validation loss: 3.3548384507497153

Epoch: 6| Step: 5
Training loss: 3.872563123703003
Validation loss: 3.3500102758407593

Epoch: 6| Step: 6
Training loss: 3.49137020111084
Validation loss: 3.346219619115194

Epoch: 6| Step: 7
Training loss: 3.020125150680542
Validation loss: 3.3418947060902915

Epoch: 6| Step: 8
Training loss: 2.5998005867004395
Validation loss: 3.338083783785502

Epoch: 6| Step: 9
Training loss: 4.884060382843018
Validation loss: 3.334246595700582

Epoch: 6| Step: 10
Training loss: 3.6076500415802
Validation loss: 3.3295110066731772

Epoch: 6| Step: 11
Training loss: 3.2976629734039307
Validation loss: 3.324841578801473

Epoch: 6| Step: 12
Training loss: 2.9525365829467773
Validation loss: 3.3197638193766275

Epoch: 6| Step: 13
Training loss: 3.724365472793579
Validation loss: 3.315733313560486

Epoch: 29| Step: 0
Training loss: 3.4185426235198975
Validation loss: 3.3114583492279053

Epoch: 6| Step: 1
Training loss: 3.9046902656555176
Validation loss: 3.3070626258850098

Epoch: 6| Step: 2
Training loss: 3.135427474975586
Validation loss: 3.3027597665786743

Epoch: 6| Step: 3
Training loss: 3.148369550704956
Validation loss: 3.2987337907155356

Epoch: 6| Step: 4
Training loss: 4.150637626647949
Validation loss: 3.293902595837911

Epoch: 6| Step: 5
Training loss: 4.465554237365723
Validation loss: 3.2893275817235312

Epoch: 6| Step: 6
Training loss: 3.744022846221924
Validation loss: 3.2849775155385337

Epoch: 6| Step: 7
Training loss: 3.4765310287475586
Validation loss: 3.280625025431315

Epoch: 6| Step: 8
Training loss: 2.6894655227661133
Validation loss: 3.275991956392924

Epoch: 6| Step: 9
Training loss: 2.960799217224121
Validation loss: 3.2718271811803183

Epoch: 6| Step: 10
Training loss: 3.7930827140808105
Validation loss: 3.267891804377238

Epoch: 6| Step: 11
Training loss: 2.707780361175537
Validation loss: 3.264053146044413

Epoch: 6| Step: 12
Training loss: 3.632591724395752
Validation loss: 3.260258913040161

Epoch: 6| Step: 13
Training loss: 3.2205123901367188
Validation loss: 3.2562286059061685

Epoch: 30| Step: 0
Training loss: 3.905808925628662
Validation loss: 3.2518552939097085

Epoch: 6| Step: 1
Training loss: 3.576864004135132
Validation loss: 3.2478466431299844

Epoch: 6| Step: 2
Training loss: 3.7644400596618652
Validation loss: 3.244142254193624

Epoch: 6| Step: 3
Training loss: 2.699406862258911
Validation loss: 3.2393506368001304

Epoch: 6| Step: 4
Training loss: 3.183436393737793
Validation loss: 3.2351012229919434

Epoch: 6| Step: 5
Training loss: 3.520864486694336
Validation loss: 3.230892539024353

Epoch: 6| Step: 6
Training loss: 3.7647745609283447
Validation loss: 3.2261244853337607

Epoch: 6| Step: 7
Training loss: 3.050398349761963
Validation loss: 3.223602294921875

Epoch: 6| Step: 8
Training loss: 3.5058562755584717
Validation loss: 3.2190611362457275

Epoch: 6| Step: 9
Training loss: 3.488834857940674
Validation loss: 3.2152748107910156

Epoch: 6| Step: 10
Training loss: 3.503091812133789
Validation loss: 3.211850126584371

Epoch: 6| Step: 11
Training loss: 3.1482479572296143
Validation loss: 3.207607309023539

Epoch: 6| Step: 12
Training loss: 3.501164436340332
Validation loss: 3.2045045296351113

Epoch: 6| Step: 13
Training loss: 3.057267665863037
Validation loss: 3.19959549109141

Epoch: 31| Step: 0
Training loss: 4.230136394500732
Validation loss: 3.195264379183451

Epoch: 6| Step: 1
Training loss: 2.888988494873047
Validation loss: 3.1904048522313437

Epoch: 6| Step: 2
Training loss: 3.1716136932373047
Validation loss: 3.1859025955200195

Epoch: 6| Step: 3
Training loss: 4.113354682922363
Validation loss: 3.1815158923467

Epoch: 6| Step: 4
Training loss: 3.617154359817505
Validation loss: 3.177904566129049

Epoch: 6| Step: 5
Training loss: 2.186990737915039
Validation loss: 3.174200495084127

Epoch: 6| Step: 6
Training loss: 2.315674304962158
Validation loss: 3.1704653104146323

Epoch: 6| Step: 7
Training loss: 3.508629322052002
Validation loss: 3.1653252045313516

Epoch: 6| Step: 8
Training loss: 3.192369222640991
Validation loss: 3.1639966567357383

Epoch: 6| Step: 9
Training loss: 2.289069652557373
Validation loss: 3.158056378364563

Epoch: 6| Step: 10
Training loss: 3.59254789352417
Validation loss: 3.155015230178833

Epoch: 6| Step: 11
Training loss: 3.28711199760437
Validation loss: 3.1508153676986694

Epoch: 6| Step: 12
Training loss: 4.759652137756348
Validation loss: 3.1457447608311973

Epoch: 6| Step: 13
Training loss: 3.7686233520507812
Validation loss: 3.143470525741577

Epoch: 32| Step: 0
Training loss: 3.229297161102295
Validation loss: 3.1378644704818726

Epoch: 6| Step: 1
Training loss: 2.6735193729400635
Validation loss: 3.1337063709894815

Epoch: 6| Step: 2
Training loss: 2.963531255722046
Validation loss: 3.1301056941350303

Epoch: 6| Step: 3
Training loss: 3.2683019638061523
Validation loss: 3.1260600884755454

Epoch: 6| Step: 4
Training loss: 3.5706586837768555
Validation loss: 3.1223655939102173

Epoch: 6| Step: 5
Training loss: 2.513166904449463
Validation loss: 3.1187573273976645

Epoch: 6| Step: 6
Training loss: 3.2639760971069336
Validation loss: 3.1143712202707925

Epoch: 6| Step: 7
Training loss: 3.4728450775146484
Validation loss: 3.109756509462992

Epoch: 6| Step: 8
Training loss: 3.8730812072753906
Validation loss: 3.1052608489990234

Epoch: 6| Step: 9
Training loss: 3.605546474456787
Validation loss: 3.100779970486959

Epoch: 6| Step: 10
Training loss: 3.4047069549560547
Validation loss: 3.0973995526631675

Epoch: 6| Step: 11
Training loss: 3.1140084266662598
Validation loss: 3.0934520165125527

Epoch: 6| Step: 12
Training loss: 3.7330827713012695
Validation loss: 3.08936341603597

Epoch: 6| Step: 13
Training loss: 3.514763355255127
Validation loss: 3.086183786392212

Epoch: 33| Step: 0
Training loss: 3.176795244216919
Validation loss: 3.083102504412333

Epoch: 6| Step: 1
Training loss: 2.4267420768737793
Validation loss: 3.0831884940465293

Epoch: 6| Step: 2
Training loss: 3.983846664428711
Validation loss: 3.0803028345108032

Epoch: 6| Step: 3
Training loss: 3.240058183670044
Validation loss: 3.075144370396932

Epoch: 6| Step: 4
Training loss: 3.2098548412323
Validation loss: 3.06904931863149

Epoch: 6| Step: 5
Training loss: 2.928464889526367
Validation loss: 3.0642645756403604

Epoch: 6| Step: 6
Training loss: 2.5787696838378906
Validation loss: 3.060936768849691

Epoch: 6| Step: 7
Training loss: 2.828101396560669
Validation loss: 3.057940721511841

Epoch: 6| Step: 8
Training loss: 3.5386719703674316
Validation loss: 3.0545997619628906

Epoch: 6| Step: 9
Training loss: 3.971726655960083
Validation loss: 3.051284829775492

Epoch: 6| Step: 10
Training loss: 3.3428640365600586
Validation loss: 3.0475274324417114

Epoch: 6| Step: 11
Training loss: 3.6545474529266357
Validation loss: 3.0438780387242637

Epoch: 6| Step: 12
Training loss: 2.713784694671631
Validation loss: 3.040309190750122

Epoch: 6| Step: 13
Training loss: 3.86081862449646
Validation loss: 3.036096374193827

Epoch: 34| Step: 0
Training loss: 2.9252750873565674
Validation loss: 3.0323534409205117

Epoch: 6| Step: 1
Training loss: 4.279657363891602
Validation loss: 3.028160572052002

Epoch: 6| Step: 2
Training loss: 3.3002655506134033
Validation loss: 3.024734675884247

Epoch: 6| Step: 3
Training loss: 3.088174819946289
Validation loss: 3.020748813947042

Epoch: 6| Step: 4
Training loss: 2.7817564010620117
Validation loss: 3.017046054204305

Epoch: 6| Step: 5
Training loss: 3.2807376384735107
Validation loss: 3.013226588567098

Epoch: 6| Step: 6
Training loss: 3.510971784591675
Validation loss: 3.0092999935150146

Epoch: 6| Step: 7
Training loss: 3.4408488273620605
Validation loss: 3.005116581916809

Epoch: 6| Step: 8
Training loss: 3.1495141983032227
Validation loss: 3.0014594395955405

Epoch: 6| Step: 9
Training loss: 2.4823217391967773
Validation loss: 2.996869921684265

Epoch: 6| Step: 10
Training loss: 2.7455239295959473
Validation loss: 2.993685801823934

Epoch: 6| Step: 11
Training loss: 3.6544032096862793
Validation loss: 2.989943504333496

Epoch: 6| Step: 12
Training loss: 2.836642265319824
Validation loss: 2.986340562502543

Epoch: 6| Step: 13
Training loss: 3.371281623840332
Validation loss: 2.982434789339701

Epoch: 35| Step: 0
Training loss: 2.531996726989746
Validation loss: 2.979055921236674

Epoch: 6| Step: 1
Training loss: 2.3763339519500732
Validation loss: 2.976168950398763

Epoch: 6| Step: 2
Training loss: 2.4919443130493164
Validation loss: 2.9727349281311035

Epoch: 6| Step: 3
Training loss: 4.19036865234375
Validation loss: 2.9689036210378013

Epoch: 6| Step: 4
Training loss: 3.711860418319702
Validation loss: 2.9662869771321616

Epoch: 6| Step: 5
Training loss: 2.6514735221862793
Validation loss: 2.9630885124206543

Epoch: 6| Step: 6
Training loss: 3.4811668395996094
Validation loss: 2.95981494585673

Epoch: 6| Step: 7
Training loss: 3.337109088897705
Validation loss: 2.9568232695261636

Epoch: 6| Step: 8
Training loss: 3.0633723735809326
Validation loss: 2.9531508684158325

Epoch: 6| Step: 9
Training loss: 3.370573043823242
Validation loss: 2.9486207564671836

Epoch: 6| Step: 10
Training loss: 3.7588582038879395
Validation loss: 2.945398290952047

Epoch: 6| Step: 11
Training loss: 4.045660972595215
Validation loss: 2.9425769249598184

Epoch: 6| Step: 12
Training loss: 2.7100989818573
Validation loss: 2.93913205464681

Epoch: 6| Step: 13
Training loss: 2.4178555011749268
Validation loss: 2.9344677130381265

Epoch: 36| Step: 0
Training loss: 2.9000377655029297
Validation loss: 2.93402898311615

Epoch: 6| Step: 1
Training loss: 2.386514663696289
Validation loss: 2.9312768379847207

Epoch: 6| Step: 2
Training loss: 3.620920181274414
Validation loss: 2.936575412750244

Epoch: 6| Step: 3
Training loss: 3.7883615493774414
Validation loss: 2.945945143699646

Epoch: 6| Step: 4
Training loss: 2.880493640899658
Validation loss: 2.917038003603617

Epoch: 6| Step: 5
Training loss: 3.3313093185424805
Validation loss: 2.914721369743347

Epoch: 6| Step: 6
Training loss: 2.793264627456665
Validation loss: 2.912679394086202

Epoch: 6| Step: 7
Training loss: 2.34322452545166
Validation loss: 2.913197378317515

Epoch: 6| Step: 8
Training loss: 3.2376837730407715
Validation loss: 2.9207918445269265

Epoch: 6| Step: 9
Training loss: 3.883772373199463
Validation loss: 2.941167672475179

Epoch: 6| Step: 10
Training loss: 3.5586836338043213
Validation loss: 2.9157071510950723

Epoch: 6| Step: 11
Training loss: 2.8183953762054443
Validation loss: 2.8996606270472207

Epoch: 6| Step: 12
Training loss: 2.5175857543945312
Validation loss: 2.8955679734547934

Epoch: 6| Step: 13
Training loss: 3.625331401824951
Validation loss: 2.901759465535482

Epoch: 37| Step: 0
Training loss: 2.823930501937866
Validation loss: 2.9039125045140586

Epoch: 6| Step: 1
Training loss: 3.61602783203125
Validation loss: 2.918059547742208

Epoch: 6| Step: 2
Training loss: 2.961683988571167
Validation loss: 2.900524695714315

Epoch: 6| Step: 3
Training loss: 3.2791454792022705
Validation loss: 2.8936145305633545

Epoch: 6| Step: 4
Training loss: 3.170720100402832
Validation loss: 2.8818883498509726

Epoch: 6| Step: 5
Training loss: 2.9168448448181152
Validation loss: 2.8805354038874307

Epoch: 6| Step: 6
Training loss: 3.5272600650787354
Validation loss: 2.870585560798645

Epoch: 6| Step: 7
Training loss: 2.7800509929656982
Validation loss: 2.868846853574117

Epoch: 6| Step: 8
Training loss: 3.469332695007324
Validation loss: 2.864865859349569

Epoch: 6| Step: 9
Training loss: 3.5821056365966797
Validation loss: 2.8611297607421875

Epoch: 6| Step: 10
Training loss: 2.6537864208221436
Validation loss: 2.858984589576721

Epoch: 6| Step: 11
Training loss: 2.2803335189819336
Validation loss: 2.8549722830454507

Epoch: 6| Step: 12
Training loss: 2.767733335494995
Validation loss: 2.853752613067627

Epoch: 6| Step: 13
Training loss: 3.2562406063079834
Validation loss: 2.850022037823995

Epoch: 38| Step: 0
Training loss: 3.5279927253723145
Validation loss: 2.8448841174443564

Epoch: 6| Step: 1
Training loss: 3.3328051567077637
Validation loss: 2.8406885862350464

Epoch: 6| Step: 2
Training loss: 3.7870843410491943
Validation loss: 2.837437311808268

Epoch: 6| Step: 3
Training loss: 2.9072704315185547
Validation loss: 2.831906040509542

Epoch: 6| Step: 4
Training loss: 2.3908536434173584
Validation loss: 2.828722357749939

Epoch: 6| Step: 5
Training loss: 3.7070603370666504
Validation loss: 2.8234012524286904

Epoch: 6| Step: 6
Training loss: 2.9418203830718994
Validation loss: 2.8200852076212564

Epoch: 6| Step: 7
Training loss: 2.749833106994629
Validation loss: 2.8170817693074546

Epoch: 6| Step: 8
Training loss: 2.0024526119232178
Validation loss: 2.816202441851298

Epoch: 6| Step: 9
Training loss: 2.797966718673706
Validation loss: 2.81572687625885

Epoch: 6| Step: 10
Training loss: 2.6166892051696777
Validation loss: 2.8269510666529336

Epoch: 6| Step: 11
Training loss: 3.182410478591919
Validation loss: 2.818785548210144

Epoch: 6| Step: 12
Training loss: 3.215837001800537
Validation loss: 2.8051883776982627

Epoch: 6| Step: 13
Training loss: 3.3086729049682617
Validation loss: 2.798830509185791

Epoch: 39| Step: 0
Training loss: 2.4651827812194824
Validation loss: 2.7965627113978067

Epoch: 6| Step: 1
Training loss: 2.7302188873291016
Validation loss: 2.795355955759684

Epoch: 6| Step: 2
Training loss: 3.264592170715332
Validation loss: 2.7932265202204385

Epoch: 6| Step: 3
Training loss: 2.822641134262085
Validation loss: 2.7922863960266113

Epoch: 6| Step: 4
Training loss: 3.2715892791748047
Validation loss: 2.7909333308537803

Epoch: 6| Step: 5
Training loss: 3.2827978134155273
Validation loss: 2.78683332602183

Epoch: 6| Step: 6
Training loss: 2.7283926010131836
Validation loss: 2.7819739977518716

Epoch: 6| Step: 7
Training loss: 2.3884894847869873
Validation loss: 2.778300642967224

Epoch: 6| Step: 8
Training loss: 3.0993499755859375
Validation loss: 2.7744125922520957

Epoch: 6| Step: 9
Training loss: 3.78169322013855
Validation loss: 2.7697787086168923

Epoch: 6| Step: 10
Training loss: 3.2307329177856445
Validation loss: 2.766639292240143

Epoch: 6| Step: 11
Training loss: 3.172712802886963
Validation loss: 2.7635151147842407

Epoch: 6| Step: 12
Training loss: 2.6327130794525146
Validation loss: 2.7615698178609214

Epoch: 6| Step: 13
Training loss: 2.9923391342163086
Validation loss: 2.758250037829081

Epoch: 40| Step: 0
Training loss: 3.0681941509246826
Validation loss: 2.7554247776667276

Epoch: 6| Step: 1
Training loss: 2.3019134998321533
Validation loss: 2.7539114554723105

Epoch: 6| Step: 2
Training loss: 3.184522867202759
Validation loss: 2.750469366709391

Epoch: 6| Step: 3
Training loss: 3.8942368030548096
Validation loss: 2.7508127689361572

Epoch: 6| Step: 4
Training loss: 3.2248237133026123
Validation loss: 2.7506800095240274

Epoch: 6| Step: 5
Training loss: 2.5570693016052246
Validation loss: 2.7484991550445557

Epoch: 6| Step: 6
Training loss: 2.6812493801116943
Validation loss: 2.7456807295481362

Epoch: 6| Step: 7
Training loss: 3.4752728939056396
Validation loss: 2.7386442025502524

Epoch: 6| Step: 8
Training loss: 3.1395416259765625
Validation loss: 2.7321908473968506

Epoch: 6| Step: 9
Training loss: 3.3388824462890625
Validation loss: 2.7263116439183555

Epoch: 6| Step: 10
Training loss: 2.7446370124816895
Validation loss: 2.7245622078577676

Epoch: 6| Step: 11
Training loss: 2.1038646697998047
Validation loss: 2.724334716796875

Epoch: 6| Step: 12
Training loss: 2.600069284439087
Validation loss: 2.7240912119547525

Epoch: 6| Step: 13
Training loss: 2.949939727783203
Validation loss: 2.722396731376648

Epoch: 41| Step: 0
Training loss: 2.863997459411621
Validation loss: 2.721895615259806

Epoch: 6| Step: 1
Training loss: 2.7089791297912598
Validation loss: 2.7192573149998984

Epoch: 6| Step: 2
Training loss: 3.1849730014801025
Validation loss: 2.713839371999105

Epoch: 6| Step: 3
Training loss: 2.4581923484802246
Validation loss: 2.7090327541033425

Epoch: 6| Step: 4
Training loss: 2.9865856170654297
Validation loss: 2.7053386767705283

Epoch: 6| Step: 5
Training loss: 3.490227699279785
Validation loss: 2.7017576694488525

Epoch: 6| Step: 6
Training loss: 2.391491413116455
Validation loss: 2.6983280976613364

Epoch: 6| Step: 7
Training loss: 3.5768260955810547
Validation loss: 2.6942697763442993

Epoch: 6| Step: 8
Training loss: 2.6943914890289307
Validation loss: 2.6906898617744446

Epoch: 6| Step: 9
Training loss: 2.98024320602417
Validation loss: 2.686097582181295

Epoch: 6| Step: 10
Training loss: 2.90179443359375
Validation loss: 2.6839974323908486

Epoch: 6| Step: 11
Training loss: 2.4140868186950684
Validation loss: 2.6823920408884683

Epoch: 6| Step: 12
Training loss: 3.1330819129943848
Validation loss: 2.680128196875254

Epoch: 6| Step: 13
Training loss: 2.863276481628418
Validation loss: 2.676908493041992

Epoch: 42| Step: 0
Training loss: 2.3286266326904297
Validation loss: 2.6727983951568604

Epoch: 6| Step: 1
Training loss: 2.395186424255371
Validation loss: 2.6685919562975564

Epoch: 6| Step: 2
Training loss: 2.711479663848877
Validation loss: 2.666434168815613

Epoch: 6| Step: 3
Training loss: 2.6567039489746094
Validation loss: 2.6608417431513467

Epoch: 6| Step: 4
Training loss: 2.6848692893981934
Validation loss: 2.6589578787485757

Epoch: 6| Step: 5
Training loss: 2.7675209045410156
Validation loss: 2.6566853125890098

Epoch: 6| Step: 6
Training loss: 3.0378429889678955
Validation loss: 2.6522090633710227

Epoch: 6| Step: 7
Training loss: 3.3155510425567627
Validation loss: 2.648210565249125

Epoch: 6| Step: 8
Training loss: 3.1732425689697266
Validation loss: 2.6455397407213845

Epoch: 6| Step: 9
Training loss: 3.8019423484802246
Validation loss: 2.642441431681315

Epoch: 6| Step: 10
Training loss: 2.4425253868103027
Validation loss: 2.6382851600646973

Epoch: 6| Step: 11
Training loss: 2.5054259300231934
Validation loss: 2.635223944981893

Epoch: 6| Step: 12
Training loss: 2.744413137435913
Validation loss: 2.631640354792277

Epoch: 6| Step: 13
Training loss: 3.367863178253174
Validation loss: 2.629196365674337

Epoch: 43| Step: 0
Training loss: 2.629032611846924
Validation loss: 2.6253095666567483

Epoch: 6| Step: 1
Training loss: 2.767836570739746
Validation loss: 2.624136964480082

Epoch: 6| Step: 2
Training loss: 3.1600325107574463
Validation loss: 2.621514836947123

Epoch: 6| Step: 3
Training loss: 2.5834107398986816
Validation loss: 2.6177423000335693

Epoch: 6| Step: 4
Training loss: 2.584106683731079
Validation loss: 2.6198740800221763

Epoch: 6| Step: 5
Training loss: 2.350505828857422
Validation loss: 2.618878185749054

Epoch: 6| Step: 6
Training loss: 2.6949195861816406
Validation loss: 2.622126499811808

Epoch: 6| Step: 7
Training loss: 3.378474235534668
Validation loss: 2.614152987798055

Epoch: 6| Step: 8
Training loss: 3.091026544570923
Validation loss: 2.6036245425542197

Epoch: 6| Step: 9
Training loss: 3.057758331298828
Validation loss: 2.599449038505554

Epoch: 6| Step: 10
Training loss: 2.618312358856201
Validation loss: 2.596373756726583

Epoch: 6| Step: 11
Training loss: 3.09171724319458
Validation loss: 2.59390127658844

Epoch: 6| Step: 12
Training loss: 2.999925136566162
Validation loss: 2.589599291483561

Epoch: 6| Step: 13
Training loss: 2.3250982761383057
Validation loss: 2.587331851323446

Epoch: 44| Step: 0
Training loss: 2.6291067600250244
Validation loss: 2.5864606300989785

Epoch: 6| Step: 1
Training loss: 2.8113415241241455
Validation loss: 2.582958002885183

Epoch: 6| Step: 2
Training loss: 3.380448818206787
Validation loss: 2.5820106665293374

Epoch: 6| Step: 3
Training loss: 2.4495601654052734
Validation loss: 2.5796163082122803

Epoch: 6| Step: 4
Training loss: 2.4293642044067383
Validation loss: 2.574317971865336

Epoch: 6| Step: 5
Training loss: 3.1973683834075928
Validation loss: 2.5708322326342263

Epoch: 6| Step: 6
Training loss: 2.831160068511963
Validation loss: 2.5686947107315063

Epoch: 6| Step: 7
Training loss: 2.840114116668701
Validation loss: 2.567044218381246

Epoch: 6| Step: 8
Training loss: 2.42525053024292
Validation loss: 2.56329615910848

Epoch: 6| Step: 9
Training loss: 2.9293746948242188
Validation loss: 2.5636988480885825

Epoch: 6| Step: 10
Training loss: 2.8300395011901855
Validation loss: 2.5626330176989236

Epoch: 6| Step: 11
Training loss: 3.1031336784362793
Validation loss: 2.5584455331166587

Epoch: 6| Step: 12
Training loss: 2.165034055709839
Validation loss: 2.5519055128097534

Epoch: 6| Step: 13
Training loss: 2.6771302223205566
Validation loss: 2.547147552172343

Epoch: 45| Step: 0
Training loss: 3.057192325592041
Validation loss: 2.5539183219273887

Epoch: 6| Step: 1
Training loss: 1.9776617288589478
Validation loss: 2.549833277861277

Epoch: 6| Step: 2
Training loss: 2.433582305908203
Validation loss: 2.5457941691080728

Epoch: 6| Step: 3
Training loss: 3.066960573196411
Validation loss: 2.5431896845499673

Epoch: 6| Step: 4
Training loss: 2.586928367614746
Validation loss: 2.538992444674174

Epoch: 6| Step: 5
Training loss: 2.9331302642822266
Validation loss: 2.5382452408472695

Epoch: 6| Step: 6
Training loss: 2.709265947341919
Validation loss: 2.533790667851766

Epoch: 6| Step: 7
Training loss: 2.796118974685669
Validation loss: 2.544126550356547

Epoch: 6| Step: 8
Training loss: 2.6914591789245605
Validation loss: 2.535166025161743

Epoch: 6| Step: 9
Training loss: 2.9656691551208496
Validation loss: 2.5269377628962197

Epoch: 6| Step: 10
Training loss: 2.8200020790100098
Validation loss: 2.5189460118611655

Epoch: 6| Step: 11
Training loss: 2.1377744674682617
Validation loss: 2.516436457633972

Epoch: 6| Step: 12
Training loss: 2.541184663772583
Validation loss: 2.514732559521993

Epoch: 6| Step: 13
Training loss: 3.430973529815674
Validation loss: 2.513364295164744

Epoch: 46| Step: 0
Training loss: 3.029975175857544
Validation loss: 2.513400594393412

Epoch: 6| Step: 1
Training loss: 2.522590160369873
Validation loss: 2.51119601726532

Epoch: 6| Step: 2
Training loss: 2.9019479751586914
Validation loss: 2.507951637109121

Epoch: 6| Step: 3
Training loss: 2.769709825515747
Validation loss: 2.5061252117156982

Epoch: 6| Step: 4
Training loss: 3.164489984512329
Validation loss: 2.502705772717794

Epoch: 6| Step: 5
Training loss: 2.906308650970459
Validation loss: 2.4983906149864197

Epoch: 6| Step: 6
Training loss: 2.438316822052002
Validation loss: 2.4939906199773154

Epoch: 6| Step: 7
Training loss: 2.005976438522339
Validation loss: 2.491263508796692

Epoch: 6| Step: 8
Training loss: 3.5167837142944336
Validation loss: 2.4883135159810386

Epoch: 6| Step: 9
Training loss: 2.013287305831909
Validation loss: 2.486116409301758

Epoch: 6| Step: 10
Training loss: 2.4011807441711426
Validation loss: 2.484183192253113

Epoch: 6| Step: 11
Training loss: 2.468438148498535
Validation loss: 2.4816769560178122

Epoch: 6| Step: 12
Training loss: 2.5396647453308105
Validation loss: 2.481104850769043

Epoch: 6| Step: 13
Training loss: 2.945744037628174
Validation loss: 2.477171262105306

Epoch: 47| Step: 0
Training loss: 2.1910462379455566
Validation loss: 2.4725948174794516

Epoch: 6| Step: 1
Training loss: 2.8760833740234375
Validation loss: 2.4761107762654624

Epoch: 6| Step: 2
Training loss: 2.202726125717163
Validation loss: 2.476553718249003

Epoch: 6| Step: 3
Training loss: 2.7222328186035156
Validation loss: 2.4739925861358643

Epoch: 6| Step: 4
Training loss: 2.9819247722625732
Validation loss: 2.470946749051412

Epoch: 6| Step: 5
Training loss: 2.9636855125427246
Validation loss: 2.466264843940735

Epoch: 6| Step: 6
Training loss: 2.5633745193481445
Validation loss: 2.463908632596334

Epoch: 6| Step: 7
Training loss: 2.5707473754882812
Validation loss: 2.4559391339619956

Epoch: 6| Step: 8
Training loss: 2.7131032943725586
Validation loss: 2.4537522991498313

Epoch: 6| Step: 9
Training loss: 2.598773956298828
Validation loss: 2.4531537890434265

Epoch: 6| Step: 10
Training loss: 2.5642995834350586
Validation loss: 2.4488051335016885

Epoch: 6| Step: 11
Training loss: 2.779663324356079
Validation loss: 2.4483320315678916

Epoch: 6| Step: 12
Training loss: 2.746533155441284
Validation loss: 2.4461715817451477

Epoch: 6| Step: 13
Training loss: 2.5177762508392334
Validation loss: 2.443660100301107

Epoch: 48| Step: 0
Training loss: 3.271772861480713
Validation loss: 2.4452385902404785

Epoch: 6| Step: 1
Training loss: 2.6544137001037598
Validation loss: 2.443948249022166

Epoch: 6| Step: 2
Training loss: 2.675535202026367
Validation loss: 2.4483806689580283

Epoch: 6| Step: 3
Training loss: 2.660881519317627
Validation loss: 2.4475184082984924

Epoch: 6| Step: 4
Training loss: 2.3242430686950684
Validation loss: 2.444776097933451

Epoch: 6| Step: 5
Training loss: 2.315741539001465
Validation loss: 2.4395203590393066

Epoch: 6| Step: 6
Training loss: 3.0273518562316895
Validation loss: 2.431971867879232

Epoch: 6| Step: 7
Training loss: 2.512779474258423
Validation loss: 2.4281115929285684

Epoch: 6| Step: 8
Training loss: 2.5328948497772217
Validation loss: 2.423559387524923

Epoch: 6| Step: 9
Training loss: 2.469373941421509
Validation loss: 2.417254467805227

Epoch: 6| Step: 10
Training loss: 2.3172645568847656
Validation loss: 2.413205941518148

Epoch: 6| Step: 11
Training loss: 2.5541577339172363
Validation loss: 2.408985137939453

Epoch: 6| Step: 12
Training loss: 2.7465131282806396
Validation loss: 2.4051912228266397

Epoch: 6| Step: 13
Training loss: 2.492495059967041
Validation loss: 2.4059178829193115

Epoch: 49| Step: 0
Training loss: 1.8259978294372559
Validation loss: 2.4007235169410706

Epoch: 6| Step: 1
Training loss: 2.045736789703369
Validation loss: 2.4057528972625732

Epoch: 6| Step: 2
Training loss: 2.8073699474334717
Validation loss: 2.431579569975535

Epoch: 6| Step: 3
Training loss: 2.613339900970459
Validation loss: 2.404991328716278

Epoch: 6| Step: 4
Training loss: 2.4533233642578125
Validation loss: 2.4031302531560264

Epoch: 6| Step: 5
Training loss: 2.160430908203125
Validation loss: 2.39256219069163

Epoch: 6| Step: 6
Training loss: 2.787614583969116
Validation loss: 2.398594637711843

Epoch: 6| Step: 7
Training loss: 3.430269718170166
Validation loss: 2.3919105529785156

Epoch: 6| Step: 8
Training loss: 2.6266379356384277
Validation loss: 2.3896034161249795

Epoch: 6| Step: 9
Training loss: 3.018476963043213
Validation loss: 2.384337902069092

Epoch: 6| Step: 10
Training loss: 2.755019187927246
Validation loss: 2.3852502504984536

Epoch: 6| Step: 11
Training loss: 3.057723045349121
Validation loss: 2.38138880332311

Epoch: 6| Step: 12
Training loss: 2.8666858673095703
Validation loss: 2.37470414241155

Epoch: 6| Step: 13
Training loss: 1.47798752784729
Validation loss: 2.3725942770640054

Epoch: 50| Step: 0
Training loss: 1.9249498844146729
Validation loss: 2.367415408293406

Epoch: 6| Step: 1
Training loss: 2.944794178009033
Validation loss: 2.3698768615722656

Epoch: 6| Step: 2
Training loss: 2.558971881866455
Validation loss: 2.3691720366477966

Epoch: 6| Step: 3
Training loss: 2.662964344024658
Validation loss: 2.364763160546621

Epoch: 6| Step: 4
Training loss: 3.3058369159698486
Validation loss: 2.3625513116518655

Epoch: 6| Step: 5
Training loss: 2.7462446689605713
Validation loss: 2.3603522380193076

Epoch: 6| Step: 6
Training loss: 2.9536070823669434
Validation loss: 2.3607835173606873

Epoch: 6| Step: 7
Training loss: 3.1715011596679688
Validation loss: 2.358137766520182

Epoch: 6| Step: 8
Training loss: 2.1411361694335938
Validation loss: 2.354089935620626

Epoch: 6| Step: 9
Training loss: 1.8595952987670898
Validation loss: 2.351948539415995

Epoch: 6| Step: 10
Training loss: 2.4703168869018555
Validation loss: 2.350910802682241

Epoch: 6| Step: 11
Training loss: 2.8576016426086426
Validation loss: 2.3462777535120645

Epoch: 6| Step: 12
Training loss: 2.258049488067627
Validation loss: 2.3450668255488076

Epoch: 6| Step: 13
Training loss: 1.7174415588378906
Validation loss: 2.3420449693997702

Epoch: 51| Step: 0
Training loss: 2.35665225982666
Validation loss: 2.340953509012858

Epoch: 6| Step: 1
Training loss: 2.4061570167541504
Validation loss: 2.3380237817764282

Epoch: 6| Step: 2
Training loss: 3.6525063514709473
Validation loss: 2.33467698097229

Epoch: 6| Step: 3
Training loss: 2.0795609951019287
Validation loss: 2.3313538432121277

Epoch: 6| Step: 4
Training loss: 2.1803770065307617
Validation loss: 2.328472097714742

Epoch: 6| Step: 5
Training loss: 2.3108596801757812
Validation loss: 2.3276383678118386

Epoch: 6| Step: 6
Training loss: 2.7115042209625244
Validation loss: 2.3243440786997476

Epoch: 6| Step: 7
Training loss: 2.084624767303467
Validation loss: 2.32173482577006

Epoch: 6| Step: 8
Training loss: 2.4725446701049805
Validation loss: 2.320810238520304

Epoch: 6| Step: 9
Training loss: 1.991990327835083
Validation loss: 2.319418648878733

Epoch: 6| Step: 10
Training loss: 2.734649181365967
Validation loss: 2.310473541418711

Epoch: 6| Step: 11
Training loss: 3.0139966011047363
Validation loss: 2.3099186023076377

Epoch: 6| Step: 12
Training loss: 2.470219135284424
Validation loss: 2.3078171809514365

Epoch: 6| Step: 13
Training loss: 2.4983444213867188
Validation loss: 2.3021786212921143

Epoch: 52| Step: 0
Training loss: 2.3829784393310547
Validation loss: 2.3030335704485574

Epoch: 6| Step: 1
Training loss: 2.8035991191864014
Validation loss: 2.30117134253184

Epoch: 6| Step: 2
Training loss: 2.241763114929199
Validation loss: 2.2992104291915894

Epoch: 6| Step: 3
Training loss: 3.1315016746520996
Validation loss: 2.2972120443979898

Epoch: 6| Step: 4
Training loss: 2.6681408882141113
Validation loss: 2.2952997287114463

Epoch: 6| Step: 5
Training loss: 2.4530749320983887
Validation loss: 2.29543670018514

Epoch: 6| Step: 6
Training loss: 2.1972599029541016
Validation loss: 2.288537919521332

Epoch: 6| Step: 7
Training loss: 2.538788080215454
Validation loss: 2.2860100269317627

Epoch: 6| Step: 8
Training loss: 2.0078787803649902
Validation loss: 2.28447691599528

Epoch: 6| Step: 9
Training loss: 2.0705325603485107
Validation loss: 2.2863654692967734

Epoch: 6| Step: 10
Training loss: 2.5512945652008057
Validation loss: 2.2870516777038574

Epoch: 6| Step: 11
Training loss: 2.281672954559326
Validation loss: 2.2959996859232583

Epoch: 6| Step: 12
Training loss: 2.577361822128296
Validation loss: 2.2794028321901956

Epoch: 6| Step: 13
Training loss: 2.465001106262207
Validation loss: 2.2718097964922586

Epoch: 53| Step: 0
Training loss: 2.762218475341797
Validation loss: 2.2674506505330405

Epoch: 6| Step: 1
Training loss: 2.6356678009033203
Validation loss: 2.2684601545333862

Epoch: 6| Step: 2
Training loss: 2.40944242477417
Validation loss: 2.2688918511072793

Epoch: 6| Step: 3
Training loss: 2.337960720062256
Validation loss: 2.267560680707296

Epoch: 6| Step: 4
Training loss: 1.9605880975723267
Validation loss: 2.2651063601175943

Epoch: 6| Step: 5
Training loss: 1.9584312438964844
Validation loss: 2.2622498869895935

Epoch: 6| Step: 6
Training loss: 2.3938560485839844
Validation loss: 2.2604258259137473

Epoch: 6| Step: 7
Training loss: 2.5332484245300293
Validation loss: 2.256797750790914

Epoch: 6| Step: 8
Training loss: 2.6809749603271484
Validation loss: 2.257838865121206

Epoch: 6| Step: 9
Training loss: 2.6116907596588135
Validation loss: 2.2469027042388916

Epoch: 6| Step: 10
Training loss: 2.611786365509033
Validation loss: 2.245593309402466

Epoch: 6| Step: 11
Training loss: 2.610261917114258
Validation loss: 2.2485544085502625

Epoch: 6| Step: 12
Training loss: 2.3396897315979004
Validation loss: 2.242304484049479

Epoch: 6| Step: 13
Training loss: 1.97908616065979
Validation loss: 2.2449992100397744

Epoch: 54| Step: 0
Training loss: 3.0154640674591064
Validation loss: 2.2390709718068442

Epoch: 6| Step: 1
Training loss: 2.25370454788208
Validation loss: 2.239338537057241

Epoch: 6| Step: 2
Training loss: 1.9809165000915527
Validation loss: 2.237487276395162

Epoch: 6| Step: 3
Training loss: 3.008211135864258
Validation loss: 2.2383976380030313

Epoch: 6| Step: 4
Training loss: 2.478203773498535
Validation loss: 2.234864870707194

Epoch: 6| Step: 5
Training loss: 2.6013567447662354
Validation loss: 2.2334201335906982

Epoch: 6| Step: 6
Training loss: 1.8910901546478271
Validation loss: 2.2378072341283164

Epoch: 6| Step: 7
Training loss: 3.0268001556396484
Validation loss: 2.231167415777842

Epoch: 6| Step: 8
Training loss: 2.6515326499938965
Validation loss: 2.225318173567454

Epoch: 6| Step: 9
Training loss: 1.5352822542190552
Validation loss: 2.223448852698008

Epoch: 6| Step: 10
Training loss: 2.291961908340454
Validation loss: 2.22425768772761

Epoch: 6| Step: 11
Training loss: 2.317685127258301
Validation loss: 2.2182432214419046

Epoch: 6| Step: 12
Training loss: 2.1249773502349854
Validation loss: 2.2240719000498452

Epoch: 6| Step: 13
Training loss: 2.1801977157592773
Validation loss: 2.2154550751050315

Epoch: 55| Step: 0
Training loss: 2.5463931560516357
Validation loss: 2.2174248099327087

Epoch: 6| Step: 1
Training loss: 2.434809684753418
Validation loss: 2.2129971385002136

Epoch: 6| Step: 2
Training loss: 2.6892857551574707
Validation loss: 2.208820621172587

Epoch: 6| Step: 3
Training loss: 2.798290252685547
Validation loss: 2.2055745124816895

Epoch: 6| Step: 4
Training loss: 2.3641083240509033
Validation loss: 2.207483092943827

Epoch: 6| Step: 5
Training loss: 2.4574766159057617
Validation loss: 2.204804619153341

Epoch: 6| Step: 6
Training loss: 2.402040958404541
Validation loss: 2.2004337310791016

Epoch: 6| Step: 7
Training loss: 2.19681453704834
Validation loss: 2.20495198170344

Epoch: 6| Step: 8
Training loss: 1.8778973817825317
Validation loss: 2.2073835333188376

Epoch: 6| Step: 9
Training loss: 2.3876090049743652
Validation loss: 2.201980690161387

Epoch: 6| Step: 10
Training loss: 2.0144968032836914
Validation loss: 2.2033129930496216

Epoch: 6| Step: 11
Training loss: 2.517024517059326
Validation loss: 2.201066513856252

Epoch: 6| Step: 12
Training loss: 1.8552337884902954
Validation loss: 2.193423271179199

Epoch: 6| Step: 13
Training loss: 2.561596393585205
Validation loss: 2.1988516052563987

Epoch: 56| Step: 0
Training loss: 2.1494572162628174
Validation loss: 2.1897271076838174

Epoch: 6| Step: 1
Training loss: 2.1626453399658203
Validation loss: 2.1869494716326394

Epoch: 6| Step: 2
Training loss: 2.0128426551818848
Validation loss: 2.1872073809305825

Epoch: 6| Step: 3
Training loss: 2.7663097381591797
Validation loss: 2.1799588203430176

Epoch: 6| Step: 4
Training loss: 2.1061882972717285
Validation loss: 2.1897539099057517

Epoch: 6| Step: 5
Training loss: 2.5977158546447754
Validation loss: 2.191137909889221

Epoch: 6| Step: 6
Training loss: 2.627680778503418
Validation loss: 2.203904092311859

Epoch: 6| Step: 7
Training loss: 2.1643576622009277
Validation loss: 2.190695106983185

Epoch: 6| Step: 8
Training loss: 2.8608384132385254
Validation loss: 2.1795220375061035

Epoch: 6| Step: 9
Training loss: 2.3709945678710938
Validation loss: 2.172767082850138

Epoch: 6| Step: 10
Training loss: 2.3597471714019775
Validation loss: 2.1773299972216287

Epoch: 6| Step: 11
Training loss: 2.7566990852355957
Validation loss: 2.174752871195475

Epoch: 6| Step: 12
Training loss: 2.3113303184509277
Validation loss: 2.179879903793335

Epoch: 6| Step: 13
Training loss: 1.656517744064331
Validation loss: 2.1797343095143638

Epoch: 57| Step: 0
Training loss: 2.2207345962524414
Validation loss: 2.1843904654184976

Epoch: 6| Step: 1
Training loss: 2.4642999172210693
Validation loss: 2.1852492888768515

Epoch: 6| Step: 2
Training loss: 1.8111212253570557
Validation loss: 2.1907239158948264

Epoch: 6| Step: 3
Training loss: 2.243028163909912
Validation loss: 2.1867071986198425

Epoch: 6| Step: 4
Training loss: 2.941025972366333
Validation loss: 2.1848930517832437

Epoch: 6| Step: 5
Training loss: 1.6996139287948608
Validation loss: 2.180343508720398

Epoch: 6| Step: 6
Training loss: 2.3362040519714355
Validation loss: 2.180647532145182

Epoch: 6| Step: 7
Training loss: 1.611404538154602
Validation loss: 2.176528215408325

Epoch: 6| Step: 8
Training loss: 2.4069197177886963
Validation loss: 2.175940155982971

Epoch: 6| Step: 9
Training loss: 2.5665230751037598
Validation loss: 2.1771410703659058

Epoch: 6| Step: 10
Training loss: 2.4940192699432373
Validation loss: 2.178418238957723

Epoch: 6| Step: 11
Training loss: 2.270242929458618
Validation loss: 2.16971089442571

Epoch: 6| Step: 12
Training loss: 2.919455051422119
Validation loss: 2.170630951722463

Epoch: 6| Step: 13
Training loss: 2.6703808307647705
Validation loss: 2.1549530824025473

Epoch: 58| Step: 0
Training loss: 2.8149774074554443
Validation loss: 2.1446203192075095

Epoch: 6| Step: 1
Training loss: 2.2300913333892822
Validation loss: 2.1441648403803506

Epoch: 6| Step: 2
Training loss: 2.041134834289551
Validation loss: 2.1529637773831687

Epoch: 6| Step: 3
Training loss: 2.38552188873291
Validation loss: 2.1487490932146707

Epoch: 6| Step: 4
Training loss: 2.130375862121582
Validation loss: 2.142543911933899

Epoch: 6| Step: 5
Training loss: 2.155874490737915
Validation loss: 2.1513257026672363

Epoch: 6| Step: 6
Training loss: 2.5875391960144043
Validation loss: 2.1533512274424234

Epoch: 6| Step: 7
Training loss: 2.3120288848876953
Validation loss: 2.159412066141764

Epoch: 6| Step: 8
Training loss: 1.7704681158065796
Validation loss: 2.1444422403971353

Epoch: 6| Step: 9
Training loss: 2.27744197845459
Validation loss: 2.144265572230021

Epoch: 6| Step: 10
Training loss: 2.0401763916015625
Validation loss: 2.1470974485079446

Epoch: 6| Step: 11
Training loss: 2.914411783218384
Validation loss: 2.1463217536608377

Epoch: 6| Step: 12
Training loss: 3.0930707454681396
Validation loss: 2.1395486195882163

Epoch: 6| Step: 13
Training loss: 1.6291399002075195
Validation loss: 2.1419072349866233

Epoch: 59| Step: 0
Training loss: 3.098980665206909
Validation loss: 2.1450898249944053

Epoch: 6| Step: 1
Training loss: 2.5390100479125977
Validation loss: 2.1339046955108643

Epoch: 6| Step: 2
Training loss: 2.17754864692688
Validation loss: 2.135823925336202

Epoch: 6| Step: 3
Training loss: 2.8158767223358154
Validation loss: 2.130926171938578

Epoch: 6| Step: 4
Training loss: 2.2444443702697754
Validation loss: 2.138321359952291

Epoch: 6| Step: 5
Training loss: 2.102083206176758
Validation loss: 2.1328894893328347

Epoch: 6| Step: 6
Training loss: 2.2351202964782715
Validation loss: 2.133737881978353

Epoch: 6| Step: 7
Training loss: 2.9259862899780273
Validation loss: 2.138032019138336

Epoch: 6| Step: 8
Training loss: 2.533501386642456
Validation loss: 2.1368900338808694

Epoch: 6| Step: 9
Training loss: 1.625525712966919
Validation loss: 2.138619919617971

Epoch: 6| Step: 10
Training loss: 1.8980001211166382
Validation loss: 2.1318711241086326

Epoch: 6| Step: 11
Training loss: 1.9889317750930786
Validation loss: 2.1264657378196716

Epoch: 6| Step: 12
Training loss: 1.9221779108047485
Validation loss: 2.1269818941752114

Epoch: 6| Step: 13
Training loss: 2.1323161125183105
Validation loss: 2.1294318238894143

Epoch: 60| Step: 0
Training loss: 1.995786428451538
Validation loss: 2.126322408517202

Epoch: 6| Step: 1
Training loss: 1.8263301849365234
Validation loss: 2.1294501622517905

Epoch: 6| Step: 2
Training loss: 2.0832412242889404
Validation loss: 2.125331381956736

Epoch: 6| Step: 3
Training loss: 2.336057662963867
Validation loss: 2.1183930039405823

Epoch: 6| Step: 4
Training loss: 2.7838830947875977
Validation loss: 2.1228997906049094

Epoch: 6| Step: 5
Training loss: 2.348389148712158
Validation loss: 2.1146217385927835

Epoch: 6| Step: 6
Training loss: 2.7689549922943115
Validation loss: 2.1139315565427146

Epoch: 6| Step: 7
Training loss: 1.7728495597839355
Validation loss: 2.1075597206751504

Epoch: 6| Step: 8
Training loss: 2.569375991821289
Validation loss: 2.1029807925224304

Epoch: 6| Step: 9
Training loss: 2.583484649658203
Validation loss: 2.105354289213816

Epoch: 6| Step: 10
Training loss: 2.3246777057647705
Validation loss: 2.1109538475672402

Epoch: 6| Step: 11
Training loss: 2.3414032459259033
Validation loss: 2.1471721529960632

Epoch: 6| Step: 12
Training loss: 2.475958824157715
Validation loss: 2.1569217244784036

Epoch: 6| Step: 13
Training loss: 1.7299963235855103
Validation loss: 2.1844639778137207

Epoch: 61| Step: 0
Training loss: 1.6635586023330688
Validation loss: 2.1879454851150513

Epoch: 6| Step: 1
Training loss: 2.9348599910736084
Validation loss: 2.16713680823644

Epoch: 6| Step: 2
Training loss: 2.6496546268463135
Validation loss: 2.123575131098429

Epoch: 6| Step: 3
Training loss: 2.1082887649536133
Validation loss: 2.1020588477452598

Epoch: 6| Step: 4
Training loss: 2.131700038909912
Validation loss: 2.106816530227661

Epoch: 6| Step: 5
Training loss: 2.2662620544433594
Validation loss: 2.116746723651886

Epoch: 6| Step: 6
Training loss: 2.1661128997802734
Validation loss: 2.1327618956565857

Epoch: 6| Step: 7
Training loss: 2.684162139892578
Validation loss: 2.1487860878308616

Epoch: 6| Step: 8
Training loss: 2.245710849761963
Validation loss: 2.1760913928349814

Epoch: 6| Step: 9
Training loss: 2.302783966064453
Validation loss: 2.191194732983907

Epoch: 6| Step: 10
Training loss: 1.7629330158233643
Validation loss: 2.2181900342305503

Epoch: 6| Step: 11
Training loss: 2.3580102920532227
Validation loss: 2.2152721881866455

Epoch: 6| Step: 12
Training loss: 2.4792208671569824
Validation loss: 2.2151063680648804

Epoch: 6| Step: 13
Training loss: 2.821627140045166
Validation loss: 2.2071595986684165

Epoch: 62| Step: 0
Training loss: 2.445370674133301
Validation loss: 2.1999385356903076

Epoch: 6| Step: 1
Training loss: 2.074154853820801
Validation loss: 2.1696726083755493

Epoch: 6| Step: 2
Training loss: 3.0222830772399902
Validation loss: 2.1436654726664224

Epoch: 6| Step: 3
Training loss: 2.1252145767211914
Validation loss: 2.132134755452474

Epoch: 6| Step: 4
Training loss: 2.396547317504883
Validation loss: 2.1230894327163696

Epoch: 6| Step: 5
Training loss: 2.0658700466156006
Validation loss: 2.1162039240201316

Epoch: 6| Step: 6
Training loss: 2.236112594604492
Validation loss: 2.1138586600621543

Epoch: 6| Step: 7
Training loss: 1.7240092754364014
Validation loss: 2.1091063618659973

Epoch: 6| Step: 8
Training loss: 2.2341012954711914
Validation loss: 2.1061237851778665

Epoch: 6| Step: 9
Training loss: 2.3443918228149414
Validation loss: 2.1003396113713584

Epoch: 6| Step: 10
Training loss: 2.3445723056793213
Validation loss: 2.097794016202291

Epoch: 6| Step: 11
Training loss: 2.2837953567504883
Validation loss: 2.098202387491862

Epoch: 6| Step: 12
Training loss: 2.2474112510681152
Validation loss: 2.0888006488482156

Epoch: 6| Step: 13
Training loss: 2.7262725830078125
Validation loss: 2.0930862029393515

Epoch: 63| Step: 0
Training loss: 2.532665252685547
Validation loss: 2.087161898612976

Epoch: 6| Step: 1
Training loss: 2.099794626235962
Validation loss: 2.093587358792623

Epoch: 6| Step: 2
Training loss: 1.984729528427124
Validation loss: 2.079436937967936

Epoch: 6| Step: 3
Training loss: 2.6490941047668457
Validation loss: 2.077361245950063

Epoch: 6| Step: 4
Training loss: 2.5657639503479004
Validation loss: 2.0783565839131675

Epoch: 6| Step: 5
Training loss: 1.9679248332977295
Validation loss: 2.0765979488690696

Epoch: 6| Step: 6
Training loss: 2.4507527351379395
Validation loss: 2.079099178314209

Epoch: 6| Step: 7
Training loss: 1.9297794103622437
Validation loss: 2.0726446410020194

Epoch: 6| Step: 8
Training loss: 2.078756332397461
Validation loss: 2.0775400400161743

Epoch: 6| Step: 9
Training loss: 2.3565049171447754
Validation loss: 2.0816497206687927

Epoch: 6| Step: 10
Training loss: 2.0174081325531006
Validation loss: 2.078017473220825

Epoch: 6| Step: 11
Training loss: 2.0166471004486084
Validation loss: 2.073242644468943

Epoch: 6| Step: 12
Training loss: 2.8457579612731934
Validation loss: 2.074224591255188

Epoch: 6| Step: 13
Training loss: 2.1254847049713135
Validation loss: 2.0727570255597434

Epoch: 64| Step: 0
Training loss: 1.7953882217407227
Validation loss: 2.066120902697245

Epoch: 6| Step: 1
Training loss: 2.5557827949523926
Validation loss: 2.0711198250452676

Epoch: 6| Step: 2
Training loss: 1.778359293937683
Validation loss: 2.075931708017985

Epoch: 6| Step: 3
Training loss: 2.6267595291137695
Validation loss: 2.0688253243764243

Epoch: 6| Step: 4
Training loss: 2.442629098892212
Validation loss: 2.074488043785095

Epoch: 6| Step: 5
Training loss: 2.373997211456299
Validation loss: 2.075022498766581

Epoch: 6| Step: 6
Training loss: 2.7780566215515137
Validation loss: 2.0740355054537454

Epoch: 6| Step: 7
Training loss: 2.3709890842437744
Validation loss: 2.075798193613688

Epoch: 6| Step: 8
Training loss: 2.8430895805358887
Validation loss: 2.0629507501920066

Epoch: 6| Step: 9
Training loss: 1.6900451183319092
Validation loss: 2.070449729760488

Epoch: 6| Step: 10
Training loss: 2.281066656112671
Validation loss: 2.076169788837433

Epoch: 6| Step: 11
Training loss: 1.8610543012619019
Validation loss: 2.065962036450704

Epoch: 6| Step: 12
Training loss: 1.819101095199585
Validation loss: 2.0686352054278054

Epoch: 6| Step: 13
Training loss: 2.2402729988098145
Validation loss: 2.0631383260091147

Epoch: 65| Step: 0
Training loss: 1.8068203926086426
Validation loss: 2.06066224972407

Epoch: 6| Step: 1
Training loss: 2.0272278785705566
Validation loss: 2.0841326117515564

Epoch: 6| Step: 2
Training loss: 2.219674825668335
Validation loss: 2.110775371392568

Epoch: 6| Step: 3
Training loss: 1.9997057914733887
Validation loss: 2.1006649335225425

Epoch: 6| Step: 4
Training loss: 2.563474655151367
Validation loss: 2.098840574423472

Epoch: 6| Step: 5
Training loss: 1.9196220636367798
Validation loss: 2.1069541374842324

Epoch: 6| Step: 6
Training loss: 2.0037999153137207
Validation loss: 2.0953198273976645

Epoch: 6| Step: 7
Training loss: 1.7403223514556885
Validation loss: 2.0729525287946067

Epoch: 6| Step: 8
Training loss: 2.555000066757202
Validation loss: 2.0581684509913125

Epoch: 6| Step: 9
Training loss: 2.340013027191162
Validation loss: 2.0503310561180115

Epoch: 6| Step: 10
Training loss: 2.6610305309295654
Validation loss: 2.0643403728803

Epoch: 6| Step: 11
Training loss: 2.308767318725586
Validation loss: 2.067824125289917

Epoch: 6| Step: 12
Training loss: 2.4238100051879883
Validation loss: 2.074845810731252

Epoch: 6| Step: 13
Training loss: 2.9495787620544434
Validation loss: 2.0797484517097473

Epoch: 66| Step: 0
Training loss: 2.3948230743408203
Validation loss: 2.094223956267039

Epoch: 6| Step: 1
Training loss: 1.9145803451538086
Validation loss: 2.0887301166852317

Epoch: 6| Step: 2
Training loss: 1.7330750226974487
Validation loss: 2.0973729689915976

Epoch: 6| Step: 3
Training loss: 2.250727653503418
Validation loss: 2.0996581514676413

Epoch: 6| Step: 4
Training loss: 2.110841989517212
Validation loss: 2.0788254539171853

Epoch: 6| Step: 5
Training loss: 2.512333869934082
Validation loss: 2.0749382972717285

Epoch: 6| Step: 6
Training loss: 2.4210028648376465
Validation loss: 2.075309713681539

Epoch: 6| Step: 7
Training loss: 2.6279702186584473
Validation loss: 2.071332633495331

Epoch: 6| Step: 8
Training loss: 2.276205539703369
Validation loss: 2.068648099899292

Epoch: 6| Step: 9
Training loss: 1.8648548126220703
Validation loss: 2.064438303311666

Epoch: 6| Step: 10
Training loss: 2.313000440597534
Validation loss: 2.06641415754954

Epoch: 6| Step: 11
Training loss: 2.3017077445983887
Validation loss: 2.060493270556132

Epoch: 6| Step: 12
Training loss: 2.677882671356201
Validation loss: 2.063059071699778

Epoch: 6| Step: 13
Training loss: 2.309368371963501
Validation loss: 2.0615832805633545

Epoch: 67| Step: 0
Training loss: 2.789486885070801
Validation loss: 2.0617373983065286

Epoch: 6| Step: 1
Training loss: 2.024707078933716
Validation loss: 2.064787189165751

Epoch: 6| Step: 2
Training loss: 2.122852325439453
Validation loss: 2.0614174604415894

Epoch: 6| Step: 3
Training loss: 2.5402889251708984
Validation loss: 2.066819429397583

Epoch: 6| Step: 4
Training loss: 2.9009995460510254
Validation loss: 2.065224568049113

Epoch: 6| Step: 5
Training loss: 2.0195658206939697
Validation loss: 2.0578971107800803

Epoch: 6| Step: 6
Training loss: 1.8319716453552246
Validation loss: 2.0540961821873984

Epoch: 6| Step: 7
Training loss: 1.9797937870025635
Validation loss: 2.055200775464376

Epoch: 6| Step: 8
Training loss: 1.9375158548355103
Validation loss: 2.05450048049291

Epoch: 6| Step: 9
Training loss: 2.4122214317321777
Validation loss: 2.0510418812433877

Epoch: 6| Step: 10
Training loss: 1.764268159866333
Validation loss: 2.0483310421307883

Epoch: 6| Step: 11
Training loss: 2.065397262573242
Validation loss: 2.051613171895345

Epoch: 6| Step: 12
Training loss: 2.217353343963623
Validation loss: 2.065669218699137

Epoch: 6| Step: 13
Training loss: 2.6111249923706055
Validation loss: 2.07340278228124

Epoch: 68| Step: 0
Training loss: 1.1217581033706665
Validation loss: 2.077972869078318

Epoch: 6| Step: 1
Training loss: 2.2261252403259277
Validation loss: 2.0785325368245444

Epoch: 6| Step: 2
Training loss: 2.126025915145874
Validation loss: 2.0831554532051086

Epoch: 6| Step: 3
Training loss: 2.14890193939209
Validation loss: 2.0742037892341614

Epoch: 6| Step: 4
Training loss: 2.146303176879883
Validation loss: 2.054288466771444

Epoch: 6| Step: 5
Training loss: 2.0975501537323
Validation loss: 2.034057319164276

Epoch: 6| Step: 6
Training loss: 2.4017231464385986
Validation loss: 2.0318243702252707

Epoch: 6| Step: 7
Training loss: 2.546058177947998
Validation loss: 2.0397988160451255

Epoch: 6| Step: 8
Training loss: 2.4732260704040527
Validation loss: 2.0387477477391562

Epoch: 6| Step: 9
Training loss: 2.354231834411621
Validation loss: 2.0404757062594094

Epoch: 6| Step: 10
Training loss: 2.7773218154907227
Validation loss: 2.0451067090034485

Epoch: 6| Step: 11
Training loss: 2.433265209197998
Validation loss: 2.0445706248283386

Epoch: 6| Step: 12
Training loss: 2.12458872795105
Validation loss: 2.043792466322581

Epoch: 6| Step: 13
Training loss: 2.329043388366699
Validation loss: 2.0409618814786277

Epoch: 69| Step: 0
Training loss: 2.2567028999328613
Validation loss: 2.042202134927114

Epoch: 6| Step: 1
Training loss: 2.886479139328003
Validation loss: 2.0410961310068765

Epoch: 6| Step: 2
Training loss: 1.7204535007476807
Validation loss: 2.0400463938713074

Epoch: 6| Step: 3
Training loss: 2.5550670623779297
Validation loss: 2.040626307328542

Epoch: 6| Step: 4
Training loss: 1.9651083946228027
Validation loss: 2.0287720958391824

Epoch: 6| Step: 5
Training loss: 2.4908971786499023
Validation loss: 2.0312981406847634

Epoch: 6| Step: 6
Training loss: 2.6985104084014893
Validation loss: 2.0345272421836853

Epoch: 6| Step: 7
Training loss: 1.580761194229126
Validation loss: 2.0368118484814963

Epoch: 6| Step: 8
Training loss: 2.054011821746826
Validation loss: 2.030378739039103

Epoch: 6| Step: 9
Training loss: 1.6219902038574219
Validation loss: 2.0295645197232566

Epoch: 6| Step: 10
Training loss: 2.065746784210205
Validation loss: 2.0385977824529014

Epoch: 6| Step: 11
Training loss: 2.0705411434173584
Validation loss: 2.0251804987589517

Epoch: 6| Step: 12
Training loss: 2.7854113578796387
Validation loss: 2.0394107500712075

Epoch: 6| Step: 13
Training loss: 2.491046190261841
Validation loss: 2.033429662386576

Epoch: 70| Step: 0
Training loss: 2.312953472137451
Validation loss: 2.0433988769849143

Epoch: 6| Step: 1
Training loss: 2.4222800731658936
Validation loss: 2.0410247643788657

Epoch: 6| Step: 2
Training loss: 1.988581657409668
Validation loss: 2.035476565361023

Epoch: 6| Step: 3
Training loss: 2.1092967987060547
Validation loss: 2.0407509803771973

Epoch: 6| Step: 4
Training loss: 2.5021653175354004
Validation loss: 2.040589769681295

Epoch: 6| Step: 5
Training loss: 1.8376165628433228
Validation loss: 2.0298624436060586

Epoch: 6| Step: 6
Training loss: 2.3465380668640137
Validation loss: 2.035198529561361

Epoch: 6| Step: 7
Training loss: 2.1232032775878906
Validation loss: 2.038818359375

Epoch: 6| Step: 8
Training loss: 2.871481418609619
Validation loss: 2.0265901684761047

Epoch: 6| Step: 9
Training loss: 2.355241298675537
Validation loss: 2.0323001543680825

Epoch: 6| Step: 10
Training loss: 1.587594747543335
Validation loss: 2.0321088631947837

Epoch: 6| Step: 11
Training loss: 2.2401604652404785
Validation loss: 2.032829781373342

Epoch: 6| Step: 12
Training loss: 2.20609712600708
Validation loss: 2.02930357058843

Epoch: 6| Step: 13
Training loss: 2.1035265922546387
Validation loss: 2.0313909451166787

Epoch: 71| Step: 0
Training loss: 2.2974305152893066
Validation loss: 2.0313368837038674

Epoch: 6| Step: 1
Training loss: 2.34087872505188
Validation loss: 2.032711406548818

Epoch: 6| Step: 2
Training loss: 2.440765380859375
Validation loss: 2.034009555975596

Epoch: 6| Step: 3
Training loss: 2.411816358566284
Validation loss: 2.028864542643229

Epoch: 6| Step: 4
Training loss: 1.4643110036849976
Validation loss: 2.0374568502108255

Epoch: 6| Step: 5
Training loss: 2.31459379196167
Validation loss: 2.0393377343813577

Epoch: 6| Step: 6
Training loss: 2.5687310695648193
Validation loss: 2.0435047348340354

Epoch: 6| Step: 7
Training loss: 2.06986927986145
Validation loss: 2.0404956936836243

Epoch: 6| Step: 8
Training loss: 2.3916738033294678
Validation loss: 2.045721689860026

Epoch: 6| Step: 9
Training loss: 1.7309743165969849
Validation loss: 2.0309626261393228

Epoch: 6| Step: 10
Training loss: 2.0209927558898926
Validation loss: 2.0436835289001465

Epoch: 6| Step: 11
Training loss: 2.9459619522094727
Validation loss: 2.0348766247431436

Epoch: 6| Step: 12
Training loss: 2.0613818168640137
Validation loss: 2.0277693470319114

Epoch: 6| Step: 13
Training loss: 2.0276315212249756
Validation loss: 2.0278456608454385

Epoch: 72| Step: 0
Training loss: 2.433048725128174
Validation loss: 2.0232661167780557

Epoch: 6| Step: 1
Training loss: 1.559835433959961
Validation loss: 2.02812651793162

Epoch: 6| Step: 2
Training loss: 2.1404080390930176
Validation loss: 2.0325917998949685

Epoch: 6| Step: 3
Training loss: 2.7875962257385254
Validation loss: 2.0375945568084717

Epoch: 6| Step: 4
Training loss: 2.1562089920043945
Validation loss: 2.038613776365916

Epoch: 6| Step: 5
Training loss: 2.361083984375
Validation loss: 2.03629606962204

Epoch: 6| Step: 6
Training loss: 1.7056666612625122
Validation loss: 2.040977120399475

Epoch: 6| Step: 7
Training loss: 1.7535401582717896
Validation loss: 2.045345107714335

Epoch: 6| Step: 8
Training loss: 2.4925670623779297
Validation loss: 2.035120685895284

Epoch: 6| Step: 9
Training loss: 2.803853750228882
Validation loss: 2.0392086704572043

Epoch: 6| Step: 10
Training loss: 1.7884392738342285
Validation loss: 2.035758833090464

Epoch: 6| Step: 11
Training loss: 1.9098538160324097
Validation loss: 2.0346325238545737

Epoch: 6| Step: 12
Training loss: 2.207613468170166
Validation loss: 2.029226839542389

Epoch: 6| Step: 13
Training loss: 2.724933385848999
Validation loss: 2.024937411149343

Epoch: 73| Step: 0
Training loss: 2.088418483734131
Validation loss: 2.0303010741869607

Epoch: 6| Step: 1
Training loss: 1.8278121948242188
Validation loss: 2.0303883155186973

Epoch: 6| Step: 2
Training loss: 2.854811191558838
Validation loss: 2.0379821062088013

Epoch: 6| Step: 3
Training loss: 2.247612953186035
Validation loss: 2.039977510770162

Epoch: 6| Step: 4
Training loss: 2.3560214042663574
Validation loss: 2.036537249883016

Epoch: 6| Step: 5
Training loss: 2.213411331176758
Validation loss: 2.0252575476964316

Epoch: 6| Step: 6
Training loss: 1.8538450002670288
Validation loss: 2.0292730728785195

Epoch: 6| Step: 7
Training loss: 2.000734806060791
Validation loss: 2.0312631328900657

Epoch: 6| Step: 8
Training loss: 1.668339729309082
Validation loss: 2.0263437231381736

Epoch: 6| Step: 9
Training loss: 2.657667398452759
Validation loss: 2.0317689776420593

Epoch: 6| Step: 10
Training loss: 2.5584850311279297
Validation loss: 2.0468010107676187

Epoch: 6| Step: 11
Training loss: 2.292025566101074
Validation loss: 2.042414347330729

Epoch: 6| Step: 12
Training loss: 2.131103754043579
Validation loss: 2.0384147564570108

Epoch: 6| Step: 13
Training loss: 1.8718397617340088
Validation loss: 2.0368153850237527

Epoch: 74| Step: 0
Training loss: 1.9550459384918213
Validation loss: 2.030653258164724

Epoch: 6| Step: 1
Training loss: 2.483358383178711
Validation loss: 2.031276226043701

Epoch: 6| Step: 2
Training loss: 2.602992534637451
Validation loss: 2.0381861130396524

Epoch: 6| Step: 3
Training loss: 2.440739393234253
Validation loss: 2.0360156893730164

Epoch: 6| Step: 4
Training loss: 2.1897177696228027
Validation loss: 2.0388502279917398

Epoch: 6| Step: 5
Training loss: 2.3932623863220215
Validation loss: 2.0402058362960815

Epoch: 6| Step: 6
Training loss: 2.0737557411193848
Validation loss: 2.032389760017395

Epoch: 6| Step: 7
Training loss: 1.7128803730010986
Validation loss: 2.0295365850130715

Epoch: 6| Step: 8
Training loss: 2.480642318725586
Validation loss: 2.0339884956677756

Epoch: 6| Step: 9
Training loss: 1.2922273874282837
Validation loss: 2.0348230997721353

Epoch: 6| Step: 10
Training loss: 2.2430648803710938
Validation loss: 2.0416921377182007

Epoch: 6| Step: 11
Training loss: 2.1136040687561035
Validation loss: 2.032806177934011

Epoch: 6| Step: 12
Training loss: 2.0019447803497314
Validation loss: 2.039752463499705

Epoch: 6| Step: 13
Training loss: 2.3981833457946777
Validation loss: 2.0516207814216614

Epoch: 75| Step: 0
Training loss: 2.47036075592041
Validation loss: 2.041692614555359

Epoch: 6| Step: 1
Training loss: 1.8831121921539307
Validation loss: 2.041766186555227

Epoch: 6| Step: 2
Training loss: 2.8265085220336914
Validation loss: 2.060708224773407

Epoch: 6| Step: 3
Training loss: 2.1565918922424316
Validation loss: 2.057510554790497

Epoch: 6| Step: 4
Training loss: 2.0069427490234375
Validation loss: 2.0603341658910117

Epoch: 6| Step: 5
Training loss: 2.1278488636016846
Validation loss: 2.0491363406181335

Epoch: 6| Step: 6
Training loss: 2.064772605895996
Validation loss: 2.041283746560415

Epoch: 6| Step: 7
Training loss: 1.8567029237747192
Validation loss: 2.027273197968801

Epoch: 6| Step: 8
Training loss: 1.9426178932189941
Validation loss: 2.0292526483535767

Epoch: 6| Step: 9
Training loss: 1.9880211353302002
Validation loss: 2.018878002961477

Epoch: 6| Step: 10
Training loss: 2.5559844970703125
Validation loss: 2.0205326676368713

Epoch: 6| Step: 11
Training loss: 2.3292222023010254
Validation loss: 2.024404982725779

Epoch: 6| Step: 12
Training loss: 1.768010139465332
Validation loss: 2.0384194453557334

Epoch: 6| Step: 13
Training loss: 2.976339101791382
Validation loss: 2.032668709754944

Testing loss: 1.6570441619955378

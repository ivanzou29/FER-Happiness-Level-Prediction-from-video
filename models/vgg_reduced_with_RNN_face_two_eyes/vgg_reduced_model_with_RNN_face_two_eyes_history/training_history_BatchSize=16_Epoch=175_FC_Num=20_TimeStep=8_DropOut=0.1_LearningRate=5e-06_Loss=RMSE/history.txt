Epoch: 1| Step: 0
Training loss: 6.745239274381031
Validation loss: 5.901714918400247

Epoch: 6| Step: 1
Training loss: 6.077527798926839
Validation loss: 5.900088936733475

Epoch: 6| Step: 2
Training loss: 6.192283553774937
Validation loss: 5.89838484515336

Epoch: 6| Step: 3
Training loss: 6.389454331001139
Validation loss: 5.896797191252761

Epoch: 6| Step: 4
Training loss: 6.295576013658321
Validation loss: 5.895039490364466

Epoch: 6| Step: 5
Training loss: 5.584811949539461
Validation loss: 5.893388877207024

Epoch: 6| Step: 6
Training loss: 5.964911539807978
Validation loss: 5.891661184363217

Epoch: 6| Step: 7
Training loss: 6.899247015073456
Validation loss: 5.88994072971196

Epoch: 6| Step: 8
Training loss: 5.260852858585062
Validation loss: 5.8882224987463605

Epoch: 6| Step: 9
Training loss: 5.745926118709666
Validation loss: 5.886436477572856

Epoch: 6| Step: 10
Training loss: 5.919254578751146
Validation loss: 5.884611667729695

Epoch: 6| Step: 11
Training loss: 5.468886542659255
Validation loss: 5.8827069641616765

Epoch: 6| Step: 12
Training loss: 4.494118131326803
Validation loss: 5.880711504951055

Epoch: 6| Step: 13
Training loss: 6.626300324279668
Validation loss: 5.878702958183949

Epoch: 2| Step: 0
Training loss: 6.809446998135029
Validation loss: 5.876531252600026

Epoch: 6| Step: 1
Training loss: 6.390008351017565
Validation loss: 5.874162086632589

Epoch: 6| Step: 2
Training loss: 6.197425972456793
Validation loss: 5.871769551265387

Epoch: 6| Step: 3
Training loss: 5.116224638062635
Validation loss: 5.869149724410583

Epoch: 6| Step: 4
Training loss: 6.019962164167463
Validation loss: 5.866378491360933

Epoch: 6| Step: 5
Training loss: 5.125018608245562
Validation loss: 5.863489982994798

Epoch: 6| Step: 6
Training loss: 6.039459176008385
Validation loss: 5.860508733632606

Epoch: 6| Step: 7
Training loss: 4.737535083006745
Validation loss: 5.857282938151601

Epoch: 6| Step: 8
Training loss: 6.950914764579776
Validation loss: 5.854089657241923

Epoch: 6| Step: 9
Training loss: 5.757550796503354
Validation loss: 5.850567327741573

Epoch: 6| Step: 10
Training loss: 5.336153754883001
Validation loss: 5.846942539528405

Epoch: 6| Step: 11
Training loss: 6.130941195396205
Validation loss: 5.843138093241739

Epoch: 6| Step: 12
Training loss: 5.9319309820480255
Validation loss: 5.839107961242057

Epoch: 6| Step: 13
Training loss: 6.6211577015716845
Validation loss: 5.835037890662364

Epoch: 3| Step: 0
Training loss: 5.8794827504917535
Validation loss: 5.830513608718098

Epoch: 6| Step: 1
Training loss: 5.918113137677546
Validation loss: 5.8260886936300675

Epoch: 6| Step: 2
Training loss: 5.589402880500463
Validation loss: 5.821195299075722

Epoch: 6| Step: 3
Training loss: 5.398713785214768
Validation loss: 5.8162080702399965

Epoch: 6| Step: 4
Training loss: 6.01765387864116
Validation loss: 5.810899438927419

Epoch: 6| Step: 5
Training loss: 5.553464258550341
Validation loss: 5.80545040301057

Epoch: 6| Step: 6
Training loss: 6.7033674289639915
Validation loss: 5.799734008653009

Epoch: 6| Step: 7
Training loss: 4.671293598856611
Validation loss: 5.793860653421048

Epoch: 6| Step: 8
Training loss: 6.073917923619901
Validation loss: 5.787801710796048

Epoch: 6| Step: 9
Training loss: 7.0118933865786985
Validation loss: 5.781871425462753

Epoch: 6| Step: 10
Training loss: 5.90514997299927
Validation loss: 5.774932956719242

Epoch: 6| Step: 11
Training loss: 6.8855548638937885
Validation loss: 5.768207443804916

Epoch: 6| Step: 12
Training loss: 6.1042258961725775
Validation loss: 5.761138217796449

Epoch: 6| Step: 13
Training loss: 4.445182275456895
Validation loss: 5.753906056632224

Epoch: 4| Step: 0
Training loss: 5.411697025070981
Validation loss: 5.746562372457577

Epoch: 6| Step: 1
Training loss: 5.762878921859939
Validation loss: 5.7389364368397064

Epoch: 6| Step: 2
Training loss: 4.805055086294455
Validation loss: 5.731156113582529

Epoch: 6| Step: 3
Training loss: 6.403790853274369
Validation loss: 5.723833503322305

Epoch: 6| Step: 4
Training loss: 6.445892196942143
Validation loss: 5.716015521557651

Epoch: 6| Step: 5
Training loss: 6.255205351876198
Validation loss: 5.707713750177674

Epoch: 6| Step: 6
Training loss: 5.757043256577587
Validation loss: 5.699467486888072

Epoch: 6| Step: 7
Training loss: 5.787772793036074
Validation loss: 5.690995647740894

Epoch: 6| Step: 8
Training loss: 6.253794624433221
Validation loss: 5.682415642392601

Epoch: 6| Step: 9
Training loss: 5.6712467907095006
Validation loss: 5.673565741297548

Epoch: 6| Step: 10
Training loss: 6.382207050346776
Validation loss: 5.664633199068463

Epoch: 6| Step: 11
Training loss: 5.876437072307923
Validation loss: 5.65539712956705

Epoch: 6| Step: 12
Training loss: 5.235993547859617
Validation loss: 5.646173624103867

Epoch: 6| Step: 13
Training loss: 4.990834036255003
Validation loss: 5.63682485909273

Epoch: 5| Step: 0
Training loss: 6.360495128911908
Validation loss: 5.627255503244867

Epoch: 6| Step: 1
Training loss: 6.3680088379381665
Validation loss: 5.61769822530782

Epoch: 6| Step: 2
Training loss: 5.308275046195746
Validation loss: 5.6080850660554

Epoch: 6| Step: 3
Training loss: 5.243690514028186
Validation loss: 5.598465620732037

Epoch: 6| Step: 4
Training loss: 6.117062778102739
Validation loss: 5.589308269878302

Epoch: 6| Step: 5
Training loss: 4.378200559215938
Validation loss: 5.579747638077419

Epoch: 6| Step: 6
Training loss: 4.875354851743606
Validation loss: 5.570843272270442

Epoch: 6| Step: 7
Training loss: 5.518189436544633
Validation loss: 5.561897388062687

Epoch: 6| Step: 8
Training loss: 6.046120451145977
Validation loss: 5.5529402404973265

Epoch: 6| Step: 9
Training loss: 5.390155274201744
Validation loss: 5.543955232361508

Epoch: 6| Step: 10
Training loss: 6.052269867622982
Validation loss: 5.535143069491849

Epoch: 6| Step: 11
Training loss: 5.992692948450977
Validation loss: 5.52647636446392

Epoch: 6| Step: 12
Training loss: 6.561333252866812
Validation loss: 5.517896204419377

Epoch: 6| Step: 13
Training loss: 4.893023113184448
Validation loss: 5.508589106790298

Epoch: 6| Step: 0
Training loss: 6.078081067080943
Validation loss: 5.500623927706308

Epoch: 6| Step: 1
Training loss: 5.839179061775595
Validation loss: 5.491660876505013

Epoch: 6| Step: 2
Training loss: 5.754167456308026
Validation loss: 5.483226978387537

Epoch: 6| Step: 3
Training loss: 6.065802549645765
Validation loss: 5.474032482899634

Epoch: 6| Step: 4
Training loss: 4.8684883644801005
Validation loss: 5.464643815123154

Epoch: 6| Step: 5
Training loss: 5.045102779676061
Validation loss: 5.455147791607821

Epoch: 6| Step: 6
Training loss: 4.622433749477708
Validation loss: 5.44554393829253

Epoch: 6| Step: 7
Training loss: 5.1115281354160995
Validation loss: 5.436982462723919

Epoch: 6| Step: 8
Training loss: 5.674540080116591
Validation loss: 5.429099657177524

Epoch: 6| Step: 9
Training loss: 5.542912833084325
Validation loss: 5.4212218375805215

Epoch: 6| Step: 10
Training loss: 6.103992481372047
Validation loss: 5.413838670442335

Epoch: 6| Step: 11
Training loss: 5.522026173066393
Validation loss: 5.405773634310683

Epoch: 6| Step: 12
Training loss: 5.713153706549317
Validation loss: 5.3979912153381955

Epoch: 6| Step: 13
Training loss: 5.718568892816171
Validation loss: 5.389733577128837

Epoch: 7| Step: 0
Training loss: 5.44152192200277
Validation loss: 5.381619214584823

Epoch: 6| Step: 1
Training loss: 5.395716289100172
Validation loss: 5.373751502713069

Epoch: 6| Step: 2
Training loss: 5.634477218572396
Validation loss: 5.365509763060426

Epoch: 6| Step: 3
Training loss: 5.81687647633315
Validation loss: 5.357998002843044

Epoch: 6| Step: 4
Training loss: 4.372315373304265
Validation loss: 5.350154853077086

Epoch: 6| Step: 5
Training loss: 5.747298559746421
Validation loss: 5.3422274139987

Epoch: 6| Step: 6
Training loss: 6.434138040500744
Validation loss: 5.33526303230125

Epoch: 6| Step: 7
Training loss: 4.837693692058098
Validation loss: 5.327829362082317

Epoch: 6| Step: 8
Training loss: 5.461857926891709
Validation loss: 5.3206343663155335

Epoch: 6| Step: 9
Training loss: 4.573579895075697
Validation loss: 5.3130596988022925

Epoch: 6| Step: 10
Training loss: 6.947594798657168
Validation loss: 5.305521072266758

Epoch: 6| Step: 11
Training loss: 4.0968914063661295
Validation loss: 5.298483785241303

Epoch: 6| Step: 12
Training loss: 5.619880550649137
Validation loss: 5.2904432289185985

Epoch: 6| Step: 13
Training loss: 5.284985547294091
Validation loss: 5.283737259034148

Epoch: 8| Step: 0
Training loss: 5.1411946917303535
Validation loss: 5.27616541253869

Epoch: 6| Step: 1
Training loss: 5.190823880404058
Validation loss: 5.26881217150587

Epoch: 6| Step: 2
Training loss: 5.446632170612692
Validation loss: 5.261496475233529

Epoch: 6| Step: 3
Training loss: 4.542254325341009
Validation loss: 5.253244789961965

Epoch: 6| Step: 4
Training loss: 6.103383436068537
Validation loss: 5.246146392368781

Epoch: 6| Step: 5
Training loss: 5.841693046970997
Validation loss: 5.238941079953199

Epoch: 6| Step: 6
Training loss: 5.279301571746894
Validation loss: 5.232103422336523

Epoch: 6| Step: 7
Training loss: 4.7782930692624666
Validation loss: 5.224419170019046

Epoch: 6| Step: 8
Training loss: 5.677709515769452
Validation loss: 5.218044728270786

Epoch: 6| Step: 9
Training loss: 5.836503212101665
Validation loss: 5.211065547195347

Epoch: 6| Step: 10
Training loss: 4.441257873579713
Validation loss: 5.2043641899176505

Epoch: 6| Step: 11
Training loss: 5.24744607476982
Validation loss: 5.197389727571457

Epoch: 6| Step: 12
Training loss: 5.524405862614341
Validation loss: 5.191492130209282

Epoch: 6| Step: 13
Training loss: 5.588211642354038
Validation loss: 5.185398028309441

Epoch: 9| Step: 0
Training loss: 4.897795177247269
Validation loss: 5.179078217311481

Epoch: 6| Step: 1
Training loss: 4.856825377402613
Validation loss: 5.172712152652692

Epoch: 6| Step: 2
Training loss: 3.9481512451265757
Validation loss: 5.166587100903254

Epoch: 6| Step: 3
Training loss: 5.67055325831361
Validation loss: 5.160136197532717

Epoch: 6| Step: 4
Training loss: 4.6943029248176105
Validation loss: 5.153852456377674

Epoch: 6| Step: 5
Training loss: 6.273301290551192
Validation loss: 5.147270197655345

Epoch: 6| Step: 6
Training loss: 5.550478514241862
Validation loss: 5.141421641854382

Epoch: 6| Step: 7
Training loss: 5.429300198360825
Validation loss: 5.134375610475355

Epoch: 6| Step: 8
Training loss: 5.930193772647935
Validation loss: 5.1273893043625245

Epoch: 6| Step: 9
Training loss: 5.513371080010265
Validation loss: 5.1202974532069945

Epoch: 6| Step: 10
Training loss: 4.664233027268901
Validation loss: 5.113216966070002

Epoch: 6| Step: 11
Training loss: 4.9815225600815625
Validation loss: 5.106465015530214

Epoch: 6| Step: 12
Training loss: 4.295853926975877
Validation loss: 5.0999268133858635

Epoch: 6| Step: 13
Training loss: 6.309379788491868
Validation loss: 5.0926362362670865

Epoch: 10| Step: 0
Training loss: 5.290111803917685
Validation loss: 5.08612133656827

Epoch: 6| Step: 1
Training loss: 5.252888202946665
Validation loss: 5.0795177877646465

Epoch: 6| Step: 2
Training loss: 4.83115526429695
Validation loss: 5.07386013816504

Epoch: 6| Step: 3
Training loss: 4.620758173439951
Validation loss: 5.067033692083223

Epoch: 6| Step: 4
Training loss: 5.903617474039553
Validation loss: 5.05990424149764

Epoch: 6| Step: 5
Training loss: 5.297076826273372
Validation loss: 5.05299659680674

Epoch: 6| Step: 6
Training loss: 4.594335817105863
Validation loss: 5.046154588867179

Epoch: 6| Step: 7
Training loss: 5.574400783205032
Validation loss: 5.04013171619285

Epoch: 6| Step: 8
Training loss: 5.851340520134541
Validation loss: 5.0332769887604165

Epoch: 6| Step: 9
Training loss: 5.285013697379993
Validation loss: 5.02735117981151

Epoch: 6| Step: 10
Training loss: 4.63482900030417
Validation loss: 5.02027460474378

Epoch: 6| Step: 11
Training loss: 5.04753613939427
Validation loss: 5.013214102318108

Epoch: 6| Step: 12
Training loss: 4.6853197240687905
Validation loss: 5.007491285443797

Epoch: 6| Step: 13
Training loss: 5.24455360193034
Validation loss: 5.001739644364887

Epoch: 11| Step: 0
Training loss: 5.143418130099395
Validation loss: 4.994761233855275

Epoch: 6| Step: 1
Training loss: 4.0423063316027426
Validation loss: 4.9882946646521695

Epoch: 6| Step: 2
Training loss: 5.7347701349728535
Validation loss: 4.982308496500895

Epoch: 6| Step: 3
Training loss: 5.521499228955082
Validation loss: 4.976063366446622

Epoch: 6| Step: 4
Training loss: 5.017610816359053
Validation loss: 4.970141234050957

Epoch: 6| Step: 5
Training loss: 5.409010518629947
Validation loss: 4.9636762453336525

Epoch: 6| Step: 6
Training loss: 5.5970721901768234
Validation loss: 4.956721206720707

Epoch: 6| Step: 7
Training loss: 4.771122597444311
Validation loss: 4.951568230508988

Epoch: 6| Step: 8
Training loss: 5.203560659137664
Validation loss: 4.94724114340814

Epoch: 6| Step: 9
Training loss: 5.500386311228867
Validation loss: 4.940851385956793

Epoch: 6| Step: 10
Training loss: 4.878036726959968
Validation loss: 4.932777657444702

Epoch: 6| Step: 11
Training loss: 4.474400928825556
Validation loss: 4.927411488525617

Epoch: 6| Step: 12
Training loss: 5.143987697889173
Validation loss: 4.921683365254935

Epoch: 6| Step: 13
Training loss: 4.374832150100474
Validation loss: 4.916464149485344

Epoch: 12| Step: 0
Training loss: 4.666087977541803
Validation loss: 4.91026002970992

Epoch: 6| Step: 1
Training loss: 5.008218115032944
Validation loss: 4.903900653081729

Epoch: 6| Step: 2
Training loss: 5.190722096802185
Validation loss: 4.898241086811385

Epoch: 6| Step: 3
Training loss: 4.758075475080497
Validation loss: 4.891115499158739

Epoch: 6| Step: 4
Training loss: 5.099674827830631
Validation loss: 4.885646117894592

Epoch: 6| Step: 5
Training loss: 4.6890736290895925
Validation loss: 4.879607053320176

Epoch: 6| Step: 6
Training loss: 4.986430064383467
Validation loss: 4.873463184663668

Epoch: 6| Step: 7
Training loss: 4.132171869652999
Validation loss: 4.867584163367222

Epoch: 6| Step: 8
Training loss: 4.531825377666499
Validation loss: 4.862286217965942

Epoch: 6| Step: 9
Training loss: 5.376761746560141
Validation loss: 4.856301008993321

Epoch: 6| Step: 10
Training loss: 4.9234378331655515
Validation loss: 4.849921644698685

Epoch: 6| Step: 11
Training loss: 5.295545743171102
Validation loss: 4.843326574707216

Epoch: 6| Step: 12
Training loss: 5.5145226260385245
Validation loss: 4.836665561412121

Epoch: 6| Step: 13
Training loss: 5.551240648323193
Validation loss: 4.829933856434869

Epoch: 13| Step: 0
Training loss: 5.479173764223015
Validation loss: 4.823123419376115

Epoch: 6| Step: 1
Training loss: 5.015384656429966
Validation loss: 4.817062560094627

Epoch: 6| Step: 2
Training loss: 5.306408789677258
Validation loss: 4.810386556952695

Epoch: 6| Step: 3
Training loss: 4.695444927157046
Validation loss: 4.805553360327854

Epoch: 6| Step: 4
Training loss: 4.711747790945882
Validation loss: 4.796332246805767

Epoch: 6| Step: 5
Training loss: 4.112625519016771
Validation loss: 4.792151479105797

Epoch: 6| Step: 6
Training loss: 4.215929232761657
Validation loss: 4.787395243083756

Epoch: 6| Step: 7
Training loss: 5.504036549202861
Validation loss: 4.778455062786369

Epoch: 6| Step: 8
Training loss: 5.208917000173574
Validation loss: 4.773071125161538

Epoch: 6| Step: 9
Training loss: 5.604915110398675
Validation loss: 4.766868053974889

Epoch: 6| Step: 10
Training loss: 4.552916612745855
Validation loss: 4.759878694393737

Epoch: 6| Step: 11
Training loss: 5.096914420101142
Validation loss: 4.75312983144581

Epoch: 6| Step: 12
Training loss: 4.647961093628136
Validation loss: 4.7457075299386915

Epoch: 6| Step: 13
Training loss: 4.25212492834465
Validation loss: 4.739523527445762

Epoch: 14| Step: 0
Training loss: 5.0116441562063745
Validation loss: 4.733849860381157

Epoch: 6| Step: 1
Training loss: 4.473808999579853
Validation loss: 4.726787753031552

Epoch: 6| Step: 2
Training loss: 5.119324681925792
Validation loss: 4.72009554577691

Epoch: 6| Step: 3
Training loss: 5.031042059161283
Validation loss: 4.713833502346238

Epoch: 6| Step: 4
Training loss: 4.939507378174274
Validation loss: 4.7084487068196665

Epoch: 6| Step: 5
Training loss: 4.956740735305914
Validation loss: 4.702505854579831

Epoch: 6| Step: 6
Training loss: 4.330262685652894
Validation loss: 4.69553754270386

Epoch: 6| Step: 7
Training loss: 3.933175997631523
Validation loss: 4.688996919459154

Epoch: 6| Step: 8
Training loss: 4.27850209058733
Validation loss: 4.685835648808096

Epoch: 6| Step: 9
Training loss: 4.482229962531119
Validation loss: 4.677378021449402

Epoch: 6| Step: 10
Training loss: 4.412439206490732
Validation loss: 4.67219921675401

Epoch: 6| Step: 11
Training loss: 5.829667528987837
Validation loss: 4.665974764394178

Epoch: 6| Step: 12
Training loss: 5.342606271517764
Validation loss: 4.659919931026747

Epoch: 6| Step: 13
Training loss: 4.9901145487692835
Validation loss: 4.654837955340936

Epoch: 15| Step: 0
Training loss: 4.271337240621503
Validation loss: 4.648260819526155

Epoch: 6| Step: 1
Training loss: 5.197055151518483
Validation loss: 4.642477982098571

Epoch: 6| Step: 2
Training loss: 4.525026032599495
Validation loss: 4.636585776809316

Epoch: 6| Step: 3
Training loss: 4.904418992406356
Validation loss: 4.630205558487097

Epoch: 6| Step: 4
Training loss: 4.839902075415305
Validation loss: 4.6238255599105225

Epoch: 6| Step: 5
Training loss: 4.465430674185529
Validation loss: 4.618128853250481

Epoch: 6| Step: 6
Training loss: 4.3232363417977755
Validation loss: 4.611604037358761

Epoch: 6| Step: 7
Training loss: 4.09572524545469
Validation loss: 4.605295011367201

Epoch: 6| Step: 8
Training loss: 5.324251170629923
Validation loss: 4.5986778833310185

Epoch: 6| Step: 9
Training loss: 4.025786016258944
Validation loss: 4.592306139403041

Epoch: 6| Step: 10
Training loss: 4.3878319161005015
Validation loss: 4.5854823651850785

Epoch: 6| Step: 11
Training loss: 5.5799930126734365
Validation loss: 4.580271090934354

Epoch: 6| Step: 12
Training loss: 3.8919842042514223
Validation loss: 4.574637394138684

Epoch: 6| Step: 13
Training loss: 5.93385668957402
Validation loss: 4.568746653919586

Epoch: 16| Step: 0
Training loss: 3.790708581517154
Validation loss: 4.561982704692848

Epoch: 6| Step: 1
Training loss: 5.294229842848368
Validation loss: 4.556415977995699

Epoch: 6| Step: 2
Training loss: 4.650753591856697
Validation loss: 4.550698611664736

Epoch: 6| Step: 3
Training loss: 5.038459304074359
Validation loss: 4.546366170441316

Epoch: 6| Step: 4
Training loss: 5.02641509518528
Validation loss: 4.5388985985618735

Epoch: 6| Step: 5
Training loss: 3.8949198476357427
Validation loss: 4.531578209927847

Epoch: 6| Step: 6
Training loss: 5.200480204197305
Validation loss: 4.526251655977935

Epoch: 6| Step: 7
Training loss: 4.248204469059546
Validation loss: 4.520752966147407

Epoch: 6| Step: 8
Training loss: 4.833899936131208
Validation loss: 4.514508136756159

Epoch: 6| Step: 9
Training loss: 4.744036293688631
Validation loss: 4.508345336098949

Epoch: 6| Step: 10
Training loss: 4.0286172949239
Validation loss: 4.502917050694334

Epoch: 6| Step: 11
Training loss: 5.420610527759059
Validation loss: 4.496501569372883

Epoch: 6| Step: 12
Training loss: 4.11467861918281
Validation loss: 4.490094125578116

Epoch: 6| Step: 13
Training loss: 4.427243266489185
Validation loss: 4.4840668717524155

Epoch: 17| Step: 0
Training loss: 5.861391905476043
Validation loss: 4.477552338902528

Epoch: 6| Step: 1
Training loss: 4.1917748338053915
Validation loss: 4.4712501059057495

Epoch: 6| Step: 2
Training loss: 4.666970424757012
Validation loss: 4.464760873848064

Epoch: 6| Step: 3
Training loss: 4.401169682132564
Validation loss: 4.459298157202534

Epoch: 6| Step: 4
Training loss: 4.4535693565990515
Validation loss: 4.453399471279147

Epoch: 6| Step: 5
Training loss: 4.725999127065455
Validation loss: 4.447388890897565

Epoch: 6| Step: 6
Training loss: 5.330124644813482
Validation loss: 4.441672857016615

Epoch: 6| Step: 7
Training loss: 4.235588997660714
Validation loss: 4.435753151651896

Epoch: 6| Step: 8
Training loss: 4.458785464929393
Validation loss: 4.430133758976849

Epoch: 6| Step: 9
Training loss: 4.612635400123519
Validation loss: 4.423166470593992

Epoch: 6| Step: 10
Training loss: 4.581299394078452
Validation loss: 4.4176394572874464

Epoch: 6| Step: 11
Training loss: 3.7988767017700726
Validation loss: 4.411889059333756

Epoch: 6| Step: 12
Training loss: 3.687036388154541
Validation loss: 4.406020498949856

Epoch: 6| Step: 13
Training loss: 4.534778142210738
Validation loss: 4.400530049157904

Epoch: 18| Step: 0
Training loss: 4.530180495752535
Validation loss: 4.39518792113385

Epoch: 6| Step: 1
Training loss: 5.055100107229946
Validation loss: 4.389537074649917

Epoch: 6| Step: 2
Training loss: 4.348268861615564
Validation loss: 4.38313916916141

Epoch: 6| Step: 3
Training loss: 3.3785959090230495
Validation loss: 4.37751719403335

Epoch: 6| Step: 4
Training loss: 4.620737534479623
Validation loss: 4.371794043685085

Epoch: 6| Step: 5
Training loss: 4.971343125547041
Validation loss: 4.366358460525244

Epoch: 6| Step: 6
Training loss: 4.344381382563917
Validation loss: 4.36046882861864

Epoch: 6| Step: 7
Training loss: 4.67844013791172
Validation loss: 4.354134749640827

Epoch: 6| Step: 8
Training loss: 3.6903954469253697
Validation loss: 4.349365008255415

Epoch: 6| Step: 9
Training loss: 3.0917276265045888
Validation loss: 4.343345678160175

Epoch: 6| Step: 10
Training loss: 4.872594973723355
Validation loss: 4.338442498669291

Epoch: 6| Step: 11
Training loss: 5.358664952166731
Validation loss: 4.3339488741598435

Epoch: 6| Step: 12
Training loss: 4.385815140961095
Validation loss: 4.326718208484052

Epoch: 6| Step: 13
Training loss: 4.87284764434915
Validation loss: 4.321977272763589

Epoch: 19| Step: 0
Training loss: 4.203446953791015
Validation loss: 4.316553394692172

Epoch: 6| Step: 1
Training loss: 5.488390199930608
Validation loss: 4.3105879152087425

Epoch: 6| Step: 2
Training loss: 4.979176073052155
Validation loss: 4.304884890480591

Epoch: 6| Step: 3
Training loss: 4.041485707010567
Validation loss: 4.299321166134622

Epoch: 6| Step: 4
Training loss: 5.2033258806004605
Validation loss: 4.293685913348031

Epoch: 6| Step: 5
Training loss: 4.157288844560765
Validation loss: 4.2881103644663146

Epoch: 6| Step: 6
Training loss: 4.62476781313824
Validation loss: 4.281571480591479

Epoch: 6| Step: 7
Training loss: 3.78516368255043
Validation loss: 4.276924714301169

Epoch: 6| Step: 8
Training loss: 3.521919549425011
Validation loss: 4.270989515968864

Epoch: 6| Step: 9
Training loss: 3.520309650066118
Validation loss: 4.26590505199999

Epoch: 6| Step: 10
Training loss: 4.543791574238104
Validation loss: 4.261405972973682

Epoch: 6| Step: 11
Training loss: 4.813757001524543
Validation loss: 4.256352688245682

Epoch: 6| Step: 12
Training loss: 4.109804312323849
Validation loss: 4.2502864853285995

Epoch: 6| Step: 13
Training loss: 4.246040968580067
Validation loss: 4.244778922906324

Epoch: 20| Step: 0
Training loss: 4.727946433498094
Validation loss: 4.239781005650029

Epoch: 6| Step: 1
Training loss: 3.981344828531781
Validation loss: 4.233541765613642

Epoch: 6| Step: 2
Training loss: 5.23517768740481
Validation loss: 4.229071041335239

Epoch: 6| Step: 3
Training loss: 3.8394181567695562
Validation loss: 4.224015348012813

Epoch: 6| Step: 4
Training loss: 4.166841477859618
Validation loss: 4.218759324805233

Epoch: 6| Step: 5
Training loss: 3.9507653955522795
Validation loss: 4.212659739588891

Epoch: 6| Step: 6
Training loss: 4.112176556607808
Validation loss: 4.207974308423807

Epoch: 6| Step: 7
Training loss: 4.149705825858275
Validation loss: 4.203089339283452

Epoch: 6| Step: 8
Training loss: 4.7586246292197085
Validation loss: 4.198088094210132

Epoch: 6| Step: 9
Training loss: 4.280480531473337
Validation loss: 4.19259921419614

Epoch: 6| Step: 10
Training loss: 3.994786679871127
Validation loss: 4.187946011148826

Epoch: 6| Step: 11
Training loss: 4.4438972030273405
Validation loss: 4.183043846560215

Epoch: 6| Step: 12
Training loss: 4.830772883653545
Validation loss: 4.1781825127990775

Epoch: 6| Step: 13
Training loss: 3.9860233263250997
Validation loss: 4.172914144610771

Epoch: 21| Step: 0
Training loss: 4.531046895704053
Validation loss: 4.1676436931793885

Epoch: 6| Step: 1
Training loss: 4.328205644978599
Validation loss: 4.162349517973689

Epoch: 6| Step: 2
Training loss: 3.825477400763204
Validation loss: 4.157283587516798

Epoch: 6| Step: 3
Training loss: 3.906911076873603
Validation loss: 4.1531571470479856

Epoch: 6| Step: 4
Training loss: 5.015891950401789
Validation loss: 4.147553315558658

Epoch: 6| Step: 5
Training loss: 3.9308892113312295
Validation loss: 4.1422697435385425

Epoch: 6| Step: 6
Training loss: 4.02386602693658
Validation loss: 4.137127206225048

Epoch: 6| Step: 7
Training loss: 4.80184629217978
Validation loss: 4.131845650796361

Epoch: 6| Step: 8
Training loss: 4.765320815087178
Validation loss: 4.127143899985724

Epoch: 6| Step: 9
Training loss: 3.188254771909012
Validation loss: 4.121499377592658

Epoch: 6| Step: 10
Training loss: 4.345498590692325
Validation loss: 4.117357649837183

Epoch: 6| Step: 11
Training loss: 4.569930831753019
Validation loss: 4.113117983996159

Epoch: 6| Step: 12
Training loss: 4.388294619688775
Validation loss: 4.107714686263632

Epoch: 6| Step: 13
Training loss: 3.744401440120355
Validation loss: 4.103112190189785

Epoch: 22| Step: 0
Training loss: 4.15107462194644
Validation loss: 4.0980047581404415

Epoch: 6| Step: 1
Training loss: 4.035263074880756
Validation loss: 4.093073568199129

Epoch: 6| Step: 2
Training loss: 4.7120175871075585
Validation loss: 4.087733149514012

Epoch: 6| Step: 3
Training loss: 3.8570157766852757
Validation loss: 4.083309420042017

Epoch: 6| Step: 4
Training loss: 4.1785245899747245
Validation loss: 4.077931210723309

Epoch: 6| Step: 5
Training loss: 4.053079333963542
Validation loss: 4.073652735911978

Epoch: 6| Step: 6
Training loss: 4.051780290557376
Validation loss: 4.06877733056362

Epoch: 6| Step: 7
Training loss: 4.314793294859925
Validation loss: 4.063882328485035

Epoch: 6| Step: 8
Training loss: 4.95132558008459
Validation loss: 4.0590358342827795

Epoch: 6| Step: 9
Training loss: 3.4667102211881553
Validation loss: 4.053939211281274

Epoch: 6| Step: 10
Training loss: 4.682338874191366
Validation loss: 4.049646457107268

Epoch: 6| Step: 11
Training loss: 3.9109585879188256
Validation loss: 4.044606322820545

Epoch: 6| Step: 12
Training loss: 3.8900025877171713
Validation loss: 4.040113498050283

Epoch: 6| Step: 13
Training loss: 4.288265967276693
Validation loss: 4.034714661396155

Epoch: 23| Step: 0
Training loss: 4.460558659756523
Validation loss: 4.030855637612791

Epoch: 6| Step: 1
Training loss: 4.094940245475344
Validation loss: 4.024971323325791

Epoch: 6| Step: 2
Training loss: 4.223097983659608
Validation loss: 4.020369857139743

Epoch: 6| Step: 3
Training loss: 3.3481339566721773
Validation loss: 4.015407630588558

Epoch: 6| Step: 4
Training loss: 3.7788085622032077
Validation loss: 4.0108178206449665

Epoch: 6| Step: 5
Training loss: 4.405790913791734
Validation loss: 4.00601620043994

Epoch: 6| Step: 6
Training loss: 4.700774051964428
Validation loss: 4.001495856490177

Epoch: 6| Step: 7
Training loss: 4.286655549726611
Validation loss: 3.9968181312776396

Epoch: 6| Step: 8
Training loss: 4.57869115459964
Validation loss: 3.9919918084503303

Epoch: 6| Step: 9
Training loss: 4.252051587557125
Validation loss: 3.9869806084940684

Epoch: 6| Step: 10
Training loss: 4.859207788270619
Validation loss: 3.982181438916511

Epoch: 6| Step: 11
Training loss: 3.2315723605181192
Validation loss: 3.9770569459583642

Epoch: 6| Step: 12
Training loss: 3.410428625111867
Validation loss: 3.972395216606389

Epoch: 6| Step: 13
Training loss: 3.8152515925369155
Validation loss: 3.967580007072108

Epoch: 24| Step: 0
Training loss: 3.9432434095210196
Validation loss: 3.962863110452577

Epoch: 6| Step: 1
Training loss: 3.9725638259744085
Validation loss: 3.958284250590873

Epoch: 6| Step: 2
Training loss: 4.305716666758775
Validation loss: 3.953704831668065

Epoch: 6| Step: 3
Training loss: 3.999622088700005
Validation loss: 3.9488203613092225

Epoch: 6| Step: 4
Training loss: 3.6702269406534573
Validation loss: 3.9444850343457474

Epoch: 6| Step: 5
Training loss: 4.128141651798598
Validation loss: 3.9392641545748917

Epoch: 6| Step: 6
Training loss: 4.3525126599561
Validation loss: 3.93487134109977

Epoch: 6| Step: 7
Training loss: 3.989648537895615
Validation loss: 3.9299071637966696

Epoch: 6| Step: 8
Training loss: 4.139948721305502
Validation loss: 3.925358455026444

Epoch: 6| Step: 9
Training loss: 4.71976202413354
Validation loss: 3.9208664325502096

Epoch: 6| Step: 10
Training loss: 4.078849406596111
Validation loss: 3.915616050115304

Epoch: 6| Step: 11
Training loss: 4.2891658916414634
Validation loss: 3.9114965580047336

Epoch: 6| Step: 12
Training loss: 2.9000683743539106
Validation loss: 3.9061048557018787

Epoch: 6| Step: 13
Training loss: 4.184686883039403
Validation loss: 3.901635030210548

Epoch: 25| Step: 0
Training loss: 4.612471028885663
Validation loss: 3.897009394942958

Epoch: 6| Step: 1
Training loss: 4.3877571487059885
Validation loss: 3.892436023497558

Epoch: 6| Step: 2
Training loss: 3.914598744219779
Validation loss: 3.8879305332307084

Epoch: 6| Step: 3
Training loss: 3.56128926366315
Validation loss: 3.882823532160457

Epoch: 6| Step: 4
Training loss: 4.63453228118351
Validation loss: 3.87800135306508

Epoch: 6| Step: 5
Training loss: 3.205782648420821
Validation loss: 3.873031116016855

Epoch: 6| Step: 6
Training loss: 3.519639092930186
Validation loss: 3.8683606664165007

Epoch: 6| Step: 7
Training loss: 3.888902029893259
Validation loss: 3.8640677767114826

Epoch: 6| Step: 8
Training loss: 3.317797617207469
Validation loss: 3.8598185090072796

Epoch: 6| Step: 9
Training loss: 4.1418814426255155
Validation loss: 3.855269478772802

Epoch: 6| Step: 10
Training loss: 4.377265779426165
Validation loss: 3.851026640905623

Epoch: 6| Step: 11
Training loss: 4.642156996702207
Validation loss: 3.8463350406172068

Epoch: 6| Step: 12
Training loss: 3.6064254797383186
Validation loss: 3.8416305529261634

Epoch: 6| Step: 13
Training loss: 3.827667333531811
Validation loss: 3.837003229648403

Epoch: 26| Step: 0
Training loss: 3.6098047306158083
Validation loss: 3.833553404641416

Epoch: 6| Step: 1
Training loss: 3.477587465218822
Validation loss: 3.828764549341475

Epoch: 6| Step: 2
Training loss: 4.377612832367364
Validation loss: 3.824799173771856

Epoch: 6| Step: 3
Training loss: 4.25054883218107
Validation loss: 3.8201073589517396

Epoch: 6| Step: 4
Training loss: 3.923046408256967
Validation loss: 3.8153167908388936

Epoch: 6| Step: 5
Training loss: 3.7935566785345904
Validation loss: 3.8105324530316365

Epoch: 6| Step: 6
Training loss: 4.16698543600783
Validation loss: 3.805861545862169

Epoch: 6| Step: 7
Training loss: 3.7363577331550335
Validation loss: 3.8012960097791932

Epoch: 6| Step: 8
Training loss: 3.642608626087112
Validation loss: 3.796867590375036

Epoch: 6| Step: 9
Training loss: 3.971991109163873
Validation loss: 3.7925406625940346

Epoch: 6| Step: 10
Training loss: 3.751385496413318
Validation loss: 3.787926194790196

Epoch: 6| Step: 11
Training loss: 3.9285166154480704
Validation loss: 3.7834637889692844

Epoch: 6| Step: 12
Training loss: 4.782481913714351
Validation loss: 3.778888816391071

Epoch: 6| Step: 13
Training loss: 3.517040730569989
Validation loss: 3.7745760614356185

Epoch: 27| Step: 0
Training loss: 3.9060488229445576
Validation loss: 3.7700799783103873

Epoch: 6| Step: 1
Training loss: 4.148545545762234
Validation loss: 3.765497398061178

Epoch: 6| Step: 2
Training loss: 4.506838370970361
Validation loss: 3.7610888245856864

Epoch: 6| Step: 3
Training loss: 3.574229022568913
Validation loss: 3.7562186177953274

Epoch: 6| Step: 4
Training loss: 3.5889623180419026
Validation loss: 3.751991103650454

Epoch: 6| Step: 5
Training loss: 3.721298498543054
Validation loss: 3.7475482131201234

Epoch: 6| Step: 6
Training loss: 4.033837722472265
Validation loss: 3.742788267923237

Epoch: 6| Step: 7
Training loss: 3.57074719877453
Validation loss: 3.738165917074015

Epoch: 6| Step: 8
Training loss: 3.726887238946369
Validation loss: 3.7333586518814235

Epoch: 6| Step: 9
Training loss: 4.303870814231731
Validation loss: 3.729043962104412

Epoch: 6| Step: 10
Training loss: 3.5995444645361516
Validation loss: 3.724344997294761

Epoch: 6| Step: 11
Training loss: 3.6734968783416093
Validation loss: 3.7196731596932575

Epoch: 6| Step: 12
Training loss: 4.046712864754013
Validation loss: 3.715277752425688

Epoch: 6| Step: 13
Training loss: 3.7432069922776288
Validation loss: 3.710722007517895

Epoch: 28| Step: 0
Training loss: 3.6296822136106845
Validation loss: 3.7067078160213383

Epoch: 6| Step: 1
Training loss: 4.234436485153811
Validation loss: 3.702161725355066

Epoch: 6| Step: 2
Training loss: 4.421189110066732
Validation loss: 3.6980082469220354

Epoch: 6| Step: 3
Training loss: 4.35727786138641
Validation loss: 3.69325666925299

Epoch: 6| Step: 4
Training loss: 3.5550083560550307
Validation loss: 3.6889576805612396

Epoch: 6| Step: 5
Training loss: 3.6077726696487207
Validation loss: 3.685046117658781

Epoch: 6| Step: 6
Training loss: 3.9254596028059847
Validation loss: 3.680104809077078

Epoch: 6| Step: 7
Training loss: 3.495953673215095
Validation loss: 3.675681447624482

Epoch: 6| Step: 8
Training loss: 3.496463078431959
Validation loss: 3.67133884709045

Epoch: 6| Step: 9
Training loss: 3.6057972536220775
Validation loss: 3.6670287704844076

Epoch: 6| Step: 10
Training loss: 4.010094779171617
Validation loss: 3.662854980520964

Epoch: 6| Step: 11
Training loss: 3.3594297892516125
Validation loss: 3.6583391834000816

Epoch: 6| Step: 12
Training loss: 3.743896094323117
Validation loss: 3.654033521592069

Epoch: 6| Step: 13
Training loss: 3.780783127895683
Validation loss: 3.6496755934826264

Epoch: 29| Step: 0
Training loss: 3.883577513214776
Validation loss: 3.6454229214683975

Epoch: 6| Step: 1
Training loss: 3.9167948492190834
Validation loss: 3.6409415569942554

Epoch: 6| Step: 2
Training loss: 4.080775543254472
Validation loss: 3.636947367878443

Epoch: 6| Step: 3
Training loss: 3.760122243639078
Validation loss: 3.632809196662085

Epoch: 6| Step: 4
Training loss: 3.5775361888449315
Validation loss: 3.6283899986446055

Epoch: 6| Step: 5
Training loss: 3.09919167716662
Validation loss: 3.6242531851198643

Epoch: 6| Step: 6
Training loss: 3.9103594106968216
Validation loss: 3.6201475265819103

Epoch: 6| Step: 7
Training loss: 3.9125921415258933
Validation loss: 3.6160513382252106

Epoch: 6| Step: 8
Training loss: 3.8743916926214372
Validation loss: 3.6115836119655684

Epoch: 6| Step: 9
Training loss: 4.000457975872717
Validation loss: 3.6073185730886377

Epoch: 6| Step: 10
Training loss: 3.8604287627858054
Validation loss: 3.6027765331071717

Epoch: 6| Step: 11
Training loss: 3.51421561918899
Validation loss: 3.5982459430999034

Epoch: 6| Step: 12
Training loss: 3.8532624429783877
Validation loss: 3.5944088097020677

Epoch: 6| Step: 13
Training loss: 3.1810114023483345
Validation loss: 3.590173417687143

Epoch: 30| Step: 0
Training loss: 3.6886576758800893
Validation loss: 3.5855823526588493

Epoch: 6| Step: 1
Training loss: 3.315698051546727
Validation loss: 3.5811710413316677

Epoch: 6| Step: 2
Training loss: 3.546207583647404
Validation loss: 3.5773502716073877

Epoch: 6| Step: 3
Training loss: 3.538533219596985
Validation loss: 3.572804856103244

Epoch: 6| Step: 4
Training loss: 2.748639723997386
Validation loss: 3.568963751150916

Epoch: 6| Step: 5
Training loss: 3.369416881789936
Validation loss: 3.565154508797668

Epoch: 6| Step: 6
Training loss: 4.109915926182443
Validation loss: 3.5612179641112864

Epoch: 6| Step: 7
Training loss: 3.7821127167461377
Validation loss: 3.5570861325106433

Epoch: 6| Step: 8
Training loss: 3.816010656713804
Validation loss: 3.552968352171403

Epoch: 6| Step: 9
Training loss: 4.734890245658049
Validation loss: 3.5491674243872153

Epoch: 6| Step: 10
Training loss: 3.7248455706849994
Validation loss: 3.544828521928519

Epoch: 6| Step: 11
Training loss: 3.7367163305416327
Validation loss: 3.5402695686448356

Epoch: 6| Step: 12
Training loss: 3.924294018275149
Validation loss: 3.536124373310634

Epoch: 6| Step: 13
Training loss: 3.3527637809735866
Validation loss: 3.5319522845745053

Epoch: 31| Step: 0
Training loss: 3.636800959689429
Validation loss: 3.5277094463095144

Epoch: 6| Step: 1
Training loss: 3.717506008284433
Validation loss: 3.523503722296659

Epoch: 6| Step: 2
Training loss: 3.4053116652123703
Validation loss: 3.5193144928758544

Epoch: 6| Step: 3
Training loss: 3.4827992250335837
Validation loss: 3.5147237343021422

Epoch: 6| Step: 4
Training loss: 4.47576758764988
Validation loss: 3.511042409907448

Epoch: 6| Step: 5
Training loss: 2.9388658310727416
Validation loss: 3.506896037322657

Epoch: 6| Step: 6
Training loss: 3.9610717269154625
Validation loss: 3.5029833658099134

Epoch: 6| Step: 7
Training loss: 3.293271956959388
Validation loss: 3.4986037466562205

Epoch: 6| Step: 8
Training loss: 3.9450550561889886
Validation loss: 3.494811867638015

Epoch: 6| Step: 9
Training loss: 3.6603399371096934
Validation loss: 3.4904793950570907

Epoch: 6| Step: 10
Training loss: 3.668023479731798
Validation loss: 3.4868421957375224

Epoch: 6| Step: 11
Training loss: 3.167567442888789
Validation loss: 3.4824401432798378

Epoch: 6| Step: 12
Training loss: 3.6021853754443205
Validation loss: 3.478743971009444

Epoch: 6| Step: 13
Training loss: 3.736106822134689
Validation loss: 3.4746012781401583

Epoch: 32| Step: 0
Training loss: 3.9559258350791264
Validation loss: 3.4698980682528617

Epoch: 6| Step: 1
Training loss: 4.067530876844334
Validation loss: 3.4658974038546244

Epoch: 6| Step: 2
Training loss: 3.410525517172345
Validation loss: 3.461435206142098

Epoch: 6| Step: 3
Training loss: 3.535286276096333
Validation loss: 3.4572323957931035

Epoch: 6| Step: 4
Training loss: 3.643390309294602
Validation loss: 3.452906352812177

Epoch: 6| Step: 5
Training loss: 3.677781569167453
Validation loss: 3.4484580192191663

Epoch: 6| Step: 6
Training loss: 3.1200186127327556
Validation loss: 3.4443353606626372

Epoch: 6| Step: 7
Training loss: 3.799400211984149
Validation loss: 3.440428243769064

Epoch: 6| Step: 8
Training loss: 3.363184272883236
Validation loss: 3.43619077777398

Epoch: 6| Step: 9
Training loss: 3.5157839590799416
Validation loss: 3.432391746501352

Epoch: 6| Step: 10
Training loss: 3.598959290227841
Validation loss: 3.4282742704559173

Epoch: 6| Step: 11
Training loss: 3.393555109037744
Validation loss: 3.4246199004792315

Epoch: 6| Step: 12
Training loss: 3.2406806845373244
Validation loss: 3.4205311614257794

Epoch: 6| Step: 13
Training loss: 3.7210326041372235
Validation loss: 3.416842611171104

Epoch: 33| Step: 0
Training loss: 3.6207057905542297
Validation loss: 3.41326768179307

Epoch: 6| Step: 1
Training loss: 3.6064502045644455
Validation loss: 3.4092685495198167

Epoch: 6| Step: 2
Training loss: 3.517477945937784
Validation loss: 3.4055436128078838

Epoch: 6| Step: 3
Training loss: 3.7421362759901
Validation loss: 3.4017163357682234

Epoch: 6| Step: 4
Training loss: 3.405229608129536
Validation loss: 3.3981220201239872

Epoch: 6| Step: 5
Training loss: 4.265243569112721
Validation loss: 3.3937304407800757

Epoch: 6| Step: 6
Training loss: 3.416813979035731
Validation loss: 3.3896331742804944

Epoch: 6| Step: 7
Training loss: 4.0128950643500945
Validation loss: 3.38560408244632

Epoch: 6| Step: 8
Training loss: 3.472854797385803
Validation loss: 3.3816833843559353

Epoch: 6| Step: 9
Training loss: 2.4177466862134853
Validation loss: 3.3775186677384377

Epoch: 6| Step: 10
Training loss: 2.9593791635357127
Validation loss: 3.3736958986504764

Epoch: 6| Step: 11
Training loss: 3.6049486903164727
Validation loss: 3.3701096885330215

Epoch: 6| Step: 12
Training loss: 3.706334007301776
Validation loss: 3.3665112423350485

Epoch: 6| Step: 13
Training loss: 3.282154794105278
Validation loss: 3.3625916214458753

Epoch: 34| Step: 0
Training loss: 3.9732973974851546
Validation loss: 3.3586939039941917

Epoch: 6| Step: 1
Training loss: 2.410381386997944
Validation loss: 3.3544946632579715

Epoch: 6| Step: 2
Training loss: 3.0540133851962743
Validation loss: 3.351110444189703

Epoch: 6| Step: 3
Training loss: 3.784020914007866
Validation loss: 3.3484898541597596

Epoch: 6| Step: 4
Training loss: 4.014822200577019
Validation loss: 3.345434067792096

Epoch: 6| Step: 5
Training loss: 3.8205384107699984
Validation loss: 3.340695040295392

Epoch: 6| Step: 6
Training loss: 3.7312465955249072
Validation loss: 3.3362707824745774

Epoch: 6| Step: 7
Training loss: 2.879239646623391
Validation loss: 3.3327893568267624

Epoch: 6| Step: 8
Training loss: 3.394607385060319
Validation loss: 3.328274014618827

Epoch: 6| Step: 9
Training loss: 3.7774082560983193
Validation loss: 3.3242966866537063

Epoch: 6| Step: 10
Training loss: 3.841059045188325
Validation loss: 3.3208329095844173

Epoch: 6| Step: 11
Training loss: 3.3058266265880603
Validation loss: 3.316791537504477

Epoch: 6| Step: 12
Training loss: 3.0929394825868055
Validation loss: 3.3126444875046723

Epoch: 6| Step: 13
Training loss: 3.151532263698225
Validation loss: 3.309205366139069

Epoch: 35| Step: 0
Training loss: 2.9389440355453154
Validation loss: 3.3053177517391203

Epoch: 6| Step: 1
Training loss: 3.60850573884778
Validation loss: 3.3020983955368695

Epoch: 6| Step: 2
Training loss: 3.788111994182425
Validation loss: 3.2985202727720973

Epoch: 6| Step: 3
Training loss: 3.3675546822076536
Validation loss: 3.2945394816064737

Epoch: 6| Step: 4
Training loss: 3.099450302609275
Validation loss: 3.2913752479158584

Epoch: 6| Step: 5
Training loss: 3.467489339953747
Validation loss: 3.287872667064544

Epoch: 6| Step: 6
Training loss: 3.221918564954238
Validation loss: 3.2840678029318986

Epoch: 6| Step: 7
Training loss: 3.6789461915834547
Validation loss: 3.2808151562488934

Epoch: 6| Step: 8
Training loss: 2.9253825793344763
Validation loss: 3.2773539225102546

Epoch: 6| Step: 9
Training loss: 3.368988334420429
Validation loss: 3.2736952990467163

Epoch: 6| Step: 10
Training loss: 3.616227815667471
Validation loss: 3.2697840827950353

Epoch: 6| Step: 11
Training loss: 4.293034949586264
Validation loss: 3.2666303678039874

Epoch: 6| Step: 12
Training loss: 3.0970558460140944
Validation loss: 3.2627464639809536

Epoch: 6| Step: 13
Training loss: 3.174702403621873
Validation loss: 3.2583596725244828

Epoch: 36| Step: 0
Training loss: 3.0815636651207425
Validation loss: 3.2553252216969533

Epoch: 6| Step: 1
Training loss: 2.8631929849756665
Validation loss: 3.251611444586696

Epoch: 6| Step: 2
Training loss: 2.3408172569207215
Validation loss: 3.2485117316805066

Epoch: 6| Step: 3
Training loss: 3.1488085355387114
Validation loss: 3.2450747215943223

Epoch: 6| Step: 4
Training loss: 3.380999001936947
Validation loss: 3.2412982379909994

Epoch: 6| Step: 5
Training loss: 3.723634221126471
Validation loss: 3.2379822694616194

Epoch: 6| Step: 6
Training loss: 3.14099513667379
Validation loss: 3.2341974427756583

Epoch: 6| Step: 7
Training loss: 3.0334549582554624
Validation loss: 3.2310392713176097

Epoch: 6| Step: 8
Training loss: 3.5999404001600626
Validation loss: 3.227639856654665

Epoch: 6| Step: 9
Training loss: 3.7992861277971133
Validation loss: 3.224140628391746

Epoch: 6| Step: 10
Training loss: 3.2531283068021737
Validation loss: 3.22141180498289

Epoch: 6| Step: 11
Training loss: 4.554169038083831
Validation loss: 3.216870163293254

Epoch: 6| Step: 12
Training loss: 3.334086205972698
Validation loss: 3.2138293409055834

Epoch: 6| Step: 13
Training loss: 3.433627079616393
Validation loss: 3.209729712328188

Epoch: 37| Step: 0
Training loss: 3.636441557222758
Validation loss: 3.2061347529251463

Epoch: 6| Step: 1
Training loss: 3.1231772638314137
Validation loss: 3.202156950270465

Epoch: 6| Step: 2
Training loss: 2.8398356903255646
Validation loss: 3.198813661570647

Epoch: 6| Step: 3
Training loss: 3.1197022354647674
Validation loss: 3.197128414129709

Epoch: 6| Step: 4
Training loss: 3.8639522698734083
Validation loss: 3.195527458888287

Epoch: 6| Step: 5
Training loss: 2.804196200467969
Validation loss: 3.1887504393805193

Epoch: 6| Step: 6
Training loss: 3.341555864838916
Validation loss: 3.1855504550286122

Epoch: 6| Step: 7
Training loss: 3.0077563788392574
Validation loss: 3.1836891456462504

Epoch: 6| Step: 8
Training loss: 3.5265435106326497
Validation loss: 3.182118479276655

Epoch: 6| Step: 9
Training loss: 3.3167823605191193
Validation loss: 3.1795522300196377

Epoch: 6| Step: 10
Training loss: 3.294866591952495
Validation loss: 3.1755508145308142

Epoch: 6| Step: 11
Training loss: 3.5986807790222515
Validation loss: 3.171468148031616

Epoch: 6| Step: 12
Training loss: 2.6104039887295225
Validation loss: 3.167196334076279

Epoch: 6| Step: 13
Training loss: 4.117016088402858
Validation loss: 3.163193144497335

Epoch: 38| Step: 0
Training loss: 3.8381711590377696
Validation loss: 3.1588309785893864

Epoch: 6| Step: 1
Training loss: 3.4173629485459585
Validation loss: 3.1551057736963233

Epoch: 6| Step: 2
Training loss: 3.597449745108207
Validation loss: 3.1512501965504676

Epoch: 6| Step: 3
Training loss: 3.9186134910839168
Validation loss: 3.14741773474105

Epoch: 6| Step: 4
Training loss: 3.047884886222425
Validation loss: 3.1444743591596023

Epoch: 6| Step: 5
Training loss: 2.6048826733780985
Validation loss: 3.140741450489945

Epoch: 6| Step: 6
Training loss: 2.5449686229098107
Validation loss: 3.1365282658830136

Epoch: 6| Step: 7
Training loss: 3.4587638004911367
Validation loss: 3.1331911817392073

Epoch: 6| Step: 8
Training loss: 3.506697649607351
Validation loss: 3.1305555932775446

Epoch: 6| Step: 9
Training loss: 3.0461512561896753
Validation loss: 3.12693370755084

Epoch: 6| Step: 10
Training loss: 2.72007797956884
Validation loss: 3.122573164527348

Epoch: 6| Step: 11
Training loss: 3.300253055002113
Validation loss: 3.1210135655294207

Epoch: 6| Step: 12
Training loss: 3.2248755379250564
Validation loss: 3.116361975608383

Epoch: 6| Step: 13
Training loss: 3.303567610926245
Validation loss: 3.113893187916269

Epoch: 39| Step: 0
Training loss: 3.0805812272413524
Validation loss: 3.1121762946139997

Epoch: 6| Step: 1
Training loss: 3.518257206885777
Validation loss: 3.110723576837635

Epoch: 6| Step: 2
Training loss: 2.4219862389398026
Validation loss: 3.1083023930527593

Epoch: 6| Step: 3
Training loss: 3.0984792301601582
Validation loss: 3.1051900230737193

Epoch: 6| Step: 4
Training loss: 3.3079202049714707
Validation loss: 3.10257794647297

Epoch: 6| Step: 5
Training loss: 3.1553672840326454
Validation loss: 3.0981741845425526

Epoch: 6| Step: 6
Training loss: 3.7798068232347113
Validation loss: 3.0953642875322624

Epoch: 6| Step: 7
Training loss: 3.2701539388930327
Validation loss: 3.0918232219229727

Epoch: 6| Step: 8
Training loss: 3.1130170339087067
Validation loss: 3.0881422190883234

Epoch: 6| Step: 9
Training loss: 3.2722229809878005
Validation loss: 3.0841334224195225

Epoch: 6| Step: 10
Training loss: 3.1497683303654482
Validation loss: 3.080978220108113

Epoch: 6| Step: 11
Training loss: 2.970510663401058
Validation loss: 3.077621870667287

Epoch: 6| Step: 12
Training loss: 3.7954496997268636
Validation loss: 3.074083608834248

Epoch: 6| Step: 13
Training loss: 3.1195107122825223
Validation loss: 3.071512080163066

Epoch: 40| Step: 0
Training loss: 3.1195059737307127
Validation loss: 3.0678214854867565

Epoch: 6| Step: 1
Training loss: 2.7568905075353154
Validation loss: 3.065146425140083

Epoch: 6| Step: 2
Training loss: 3.2260806853107016
Validation loss: 3.0661324996403447

Epoch: 6| Step: 3
Training loss: 3.9070743759023845
Validation loss: 3.0644870721327835

Epoch: 6| Step: 4
Training loss: 2.7315475231982473
Validation loss: 3.0592053136113706

Epoch: 6| Step: 5
Training loss: 3.6323001623350617
Validation loss: 3.0536859794127853

Epoch: 6| Step: 6
Training loss: 3.5066310192757206
Validation loss: 3.050571357908277

Epoch: 6| Step: 7
Training loss: 3.520682397500147
Validation loss: 3.047702618216625

Epoch: 6| Step: 8
Training loss: 2.5189851864041857
Validation loss: 3.0450385925486523

Epoch: 6| Step: 9
Training loss: 2.7357502122147066
Validation loss: 3.0423991135750326

Epoch: 6| Step: 10
Training loss: 3.2728626411069732
Validation loss: 3.0397442704270228

Epoch: 6| Step: 11
Training loss: 3.0153675661248815
Validation loss: 3.0381186751348417

Epoch: 6| Step: 12
Training loss: 3.1303730359069197
Validation loss: 3.03545042729154

Epoch: 6| Step: 13
Training loss: 3.3026334542996856
Validation loss: 3.032506335011548

Epoch: 41| Step: 0
Training loss: 3.34617085035137
Validation loss: 3.0292460185015853

Epoch: 6| Step: 1
Training loss: 3.47683365010893
Validation loss: 3.0268478282691116

Epoch: 6| Step: 2
Training loss: 3.3187318158414842
Validation loss: 3.02318320244023

Epoch: 6| Step: 3
Training loss: 3.3044172316252105
Validation loss: 3.0203180904109743

Epoch: 6| Step: 4
Training loss: 3.4299442829466855
Validation loss: 3.016838975765973

Epoch: 6| Step: 5
Training loss: 3.3516058930119983
Validation loss: 3.013275074212759

Epoch: 6| Step: 6
Training loss: 3.033114145205663
Validation loss: 3.0096892871784706

Epoch: 6| Step: 7
Training loss: 3.071428479546327
Validation loss: 3.006226819346219

Epoch: 6| Step: 8
Training loss: 3.1676788385572117
Validation loss: 3.002845289083285

Epoch: 6| Step: 9
Training loss: 2.518294724429966
Validation loss: 2.9988492573314054

Epoch: 6| Step: 10
Training loss: 2.357528974854615
Validation loss: 2.9964626305665205

Epoch: 6| Step: 11
Training loss: 3.5052317618068414
Validation loss: 2.9949442595154547

Epoch: 6| Step: 12
Training loss: 2.6223664469936527
Validation loss: 2.9926900416734803

Epoch: 6| Step: 13
Training loss: 3.3413950390345595
Validation loss: 2.9902820280211913

Epoch: 42| Step: 0
Training loss: 2.928024104608008
Validation loss: 2.9889857440351992

Epoch: 6| Step: 1
Training loss: 4.172120504977851
Validation loss: 2.986717436591693

Epoch: 6| Step: 2
Training loss: 2.5956000658505274
Validation loss: 2.9840112168183923

Epoch: 6| Step: 3
Training loss: 2.8378783591334202
Validation loss: 2.9837321043336846

Epoch: 6| Step: 4
Training loss: 3.0476882240556304
Validation loss: 2.982324345061333

Epoch: 6| Step: 5
Training loss: 2.8740357150179485
Validation loss: 2.979610641227037

Epoch: 6| Step: 6
Training loss: 3.5623479442604373
Validation loss: 2.97813474127186

Epoch: 6| Step: 7
Training loss: 2.673841500092095
Validation loss: 2.972606309294963

Epoch: 6| Step: 8
Training loss: 3.2068056870566033
Validation loss: 2.969717410223072

Epoch: 6| Step: 9
Training loss: 3.010014349817688
Validation loss: 2.966502506736195

Epoch: 6| Step: 10
Training loss: 2.7052371202145875
Validation loss: 2.964017006419888

Epoch: 6| Step: 11
Training loss: 3.1423489977261823
Validation loss: 2.9614259020120546

Epoch: 6| Step: 12
Training loss: 2.986142898114386
Validation loss: 2.960475378955387

Epoch: 6| Step: 13
Training loss: 3.5188732823513402
Validation loss: 2.962981686819927

Epoch: 43| Step: 0
Training loss: 3.1638474379553667
Validation loss: 2.9538684427215665

Epoch: 6| Step: 1
Training loss: 3.3337256359673275
Validation loss: 2.9518459853706935

Epoch: 6| Step: 2
Training loss: 3.5808273838329066
Validation loss: 2.9489328410022457

Epoch: 6| Step: 3
Training loss: 3.1542228005040682
Validation loss: 2.9477575054718583

Epoch: 6| Step: 4
Training loss: 3.1432702610434187
Validation loss: 2.945282130670857

Epoch: 6| Step: 5
Training loss: 2.337001392408159
Validation loss: 2.9431404044131004

Epoch: 6| Step: 6
Training loss: 3.5612979667970763
Validation loss: 2.9398718788057634

Epoch: 6| Step: 7
Training loss: 2.996598540632479
Validation loss: 2.938765192483422

Epoch: 6| Step: 8
Training loss: 3.3786564552716842
Validation loss: 2.9365321654940755

Epoch: 6| Step: 9
Training loss: 2.058874122014037
Validation loss: 2.9322893042185036

Epoch: 6| Step: 10
Training loss: 2.873598669447304
Validation loss: 2.930352721243199

Epoch: 6| Step: 11
Training loss: 3.0565451512645367
Validation loss: 2.92657066190647

Epoch: 6| Step: 12
Training loss: 2.9255167252018976
Validation loss: 2.9244138461818

Epoch: 6| Step: 13
Training loss: 3.2088070969652676
Validation loss: 2.9216978049989977

Epoch: 44| Step: 0
Training loss: 2.415360283997746
Validation loss: 2.9205322716319984

Epoch: 6| Step: 1
Training loss: 3.399503133034837
Validation loss: 2.9171371943272653

Epoch: 6| Step: 2
Training loss: 2.971648397541068
Validation loss: 2.9155514537806977

Epoch: 6| Step: 3
Training loss: 3.323067704518076
Validation loss: 2.913231016001997

Epoch: 6| Step: 4
Training loss: 3.1311327457364913
Validation loss: 2.909578106350287

Epoch: 6| Step: 5
Training loss: 3.009868602736223
Validation loss: 2.907947632391114

Epoch: 6| Step: 6
Training loss: 2.8928928036653097
Validation loss: 2.906299166793309

Epoch: 6| Step: 7
Training loss: 3.3817382234385374
Validation loss: 2.9027359886953796

Epoch: 6| Step: 8
Training loss: 2.778434614300798
Validation loss: 2.899812973502908

Epoch: 6| Step: 9
Training loss: 3.381128469035315
Validation loss: 2.8977832449926133

Epoch: 6| Step: 10
Training loss: 2.980203959250593
Validation loss: 2.895312891419891

Epoch: 6| Step: 11
Training loss: 2.595223249707106
Validation loss: 2.893584940564756

Epoch: 6| Step: 12
Training loss: 3.095733623173902
Validation loss: 2.8900498376440793

Epoch: 6| Step: 13
Training loss: 3.1203074509183923
Validation loss: 2.8903150650064178

Epoch: 45| Step: 0
Training loss: 2.644362113198148
Validation loss: 2.885650740033522

Epoch: 6| Step: 1
Training loss: 3.3091279626557175
Validation loss: 2.8857675515752916

Epoch: 6| Step: 2
Training loss: 3.1080320899219918
Validation loss: 2.88614973448911

Epoch: 6| Step: 3
Training loss: 2.756907457754311
Validation loss: 2.8859269325601242

Epoch: 6| Step: 4
Training loss: 3.196032305462289
Validation loss: 2.887795763157479

Epoch: 6| Step: 5
Training loss: 2.5757422331150757
Validation loss: 2.884337355346356

Epoch: 6| Step: 6
Training loss: 3.144121946624619
Validation loss: 2.87890514579072

Epoch: 6| Step: 7
Training loss: 3.345786641999782
Validation loss: 2.875737551746024

Epoch: 6| Step: 8
Training loss: 2.6045172696931544
Validation loss: 2.8732342204047288

Epoch: 6| Step: 9
Training loss: 3.437374598209798
Validation loss: 2.870676064762773

Epoch: 6| Step: 10
Training loss: 2.7536598474097733
Validation loss: 2.866987424101057

Epoch: 6| Step: 11
Training loss: 3.038541381403496
Validation loss: 2.86555564793677

Epoch: 6| Step: 12
Training loss: 2.754992201944659
Validation loss: 2.8614005330268766

Epoch: 6| Step: 13
Training loss: 3.363599666892035
Validation loss: 2.859307682655473

Epoch: 46| Step: 0
Training loss: 3.181423154064433
Validation loss: 2.857863035149756

Epoch: 6| Step: 1
Training loss: 3.0805222524301437
Validation loss: 2.8554825127961836

Epoch: 6| Step: 2
Training loss: 2.643929575058661
Validation loss: 2.853452597811314

Epoch: 6| Step: 3
Training loss: 3.6188312527984046
Validation loss: 2.8522701273778157

Epoch: 6| Step: 4
Training loss: 2.4933619108789293
Validation loss: 2.8505586433563264

Epoch: 6| Step: 5
Training loss: 3.0829358918697785
Validation loss: 2.8463712914531363

Epoch: 6| Step: 6
Training loss: 2.6664895555172565
Validation loss: 2.8444619178442236

Epoch: 6| Step: 7
Training loss: 2.7891638814841766
Validation loss: 2.8419718370619367

Epoch: 6| Step: 8
Training loss: 3.0174420668659225
Validation loss: 2.838929642693553

Epoch: 6| Step: 9
Training loss: 2.654603694785191
Validation loss: 2.8369806646253566

Epoch: 6| Step: 10
Training loss: 3.185416344658632
Validation loss: 2.8336263579890466

Epoch: 6| Step: 11
Training loss: 2.8315008258660077
Validation loss: 2.8327964362846085

Epoch: 6| Step: 12
Training loss: 3.22815109198071
Validation loss: 2.8322845696466405

Epoch: 6| Step: 13
Training loss: 3.1230296217374423
Validation loss: 2.8287780176411155

Epoch: 47| Step: 0
Training loss: 3.051288401398383
Validation loss: 2.8271882277847196

Epoch: 6| Step: 1
Training loss: 3.0032885329291834
Validation loss: 2.825723404661845

Epoch: 6| Step: 2
Training loss: 2.3730159302251934
Validation loss: 2.825659244270502

Epoch: 6| Step: 3
Training loss: 3.4172839715820573
Validation loss: 2.823690925262773

Epoch: 6| Step: 4
Training loss: 3.130811247095776
Validation loss: 2.819512709108047

Epoch: 6| Step: 5
Training loss: 2.2800703787398398
Validation loss: 2.817374843716164

Epoch: 6| Step: 6
Training loss: 2.796005369560532
Validation loss: 2.8160383784034178

Epoch: 6| Step: 7
Training loss: 3.064120117143771
Validation loss: 2.8141068848493815

Epoch: 6| Step: 8
Training loss: 3.04692163676247
Validation loss: 2.8135025921824286

Epoch: 6| Step: 9
Training loss: 3.188845630917983
Validation loss: 2.8120530515091553

Epoch: 6| Step: 10
Training loss: 2.899530004365504
Validation loss: 2.8118730623354864

Epoch: 6| Step: 11
Training loss: 2.825744723167511
Validation loss: 2.809754847447499

Epoch: 6| Step: 12
Training loss: 2.813694678990958
Validation loss: 2.808506114938345

Epoch: 6| Step: 13
Training loss: 3.3517761873713305
Validation loss: 2.805790803717459

Epoch: 48| Step: 0
Training loss: 2.9965142025915728
Validation loss: 2.8037835012887133

Epoch: 6| Step: 1
Training loss: 2.8353277554517327
Validation loss: 2.802021370997991

Epoch: 6| Step: 2
Training loss: 2.4271242413873013
Validation loss: 2.7984637786182334

Epoch: 6| Step: 3
Training loss: 3.040362317930544
Validation loss: 2.7962608160299434

Epoch: 6| Step: 4
Training loss: 2.857902272661794
Validation loss: 2.7957346632061872

Epoch: 6| Step: 5
Training loss: 3.5600687397524635
Validation loss: 2.793996028890953

Epoch: 6| Step: 6
Training loss: 2.582012413060071
Validation loss: 2.7940310576498355

Epoch: 6| Step: 7
Training loss: 2.9629704484138597
Validation loss: 2.7898094964931253

Epoch: 6| Step: 8
Training loss: 3.1444881839054037
Validation loss: 2.7859526748800207

Epoch: 6| Step: 9
Training loss: 2.826832175477602
Validation loss: 2.7838589579318254

Epoch: 6| Step: 10
Training loss: 2.521011745560191
Validation loss: 2.7832892153626005

Epoch: 6| Step: 11
Training loss: 3.3133935263044516
Validation loss: 2.7802037474984034

Epoch: 6| Step: 12
Training loss: 3.0461003810500245
Validation loss: 2.7800043152471483

Epoch: 6| Step: 13
Training loss: 2.7041135534900262
Validation loss: 2.777893097920898

Epoch: 49| Step: 0
Training loss: 3.170935129811574
Validation loss: 2.775532177709664

Epoch: 6| Step: 1
Training loss: 2.585317024403139
Validation loss: 2.7714182205315896

Epoch: 6| Step: 2
Training loss: 3.287800671422293
Validation loss: 2.771203773995121

Epoch: 6| Step: 3
Training loss: 2.541551795823424
Validation loss: 2.770832711891353

Epoch: 6| Step: 4
Training loss: 3.0371696637906798
Validation loss: 2.7687114368448444

Epoch: 6| Step: 5
Training loss: 2.9051441016379442
Validation loss: 2.76793063047157

Epoch: 6| Step: 6
Training loss: 2.4773651651439086
Validation loss: 2.768283062347716

Epoch: 6| Step: 7
Training loss: 3.1103302528214285
Validation loss: 2.7686656896012467

Epoch: 6| Step: 8
Training loss: 2.7438599792544798
Validation loss: 2.764771640538246

Epoch: 6| Step: 9
Training loss: 2.924990049165933
Validation loss: 2.7616539011836534

Epoch: 6| Step: 10
Training loss: 2.846541323824361
Validation loss: 2.761936019127869

Epoch: 6| Step: 11
Training loss: 3.150630669811
Validation loss: 2.7620826276439807

Epoch: 6| Step: 12
Training loss: 2.6503358412206075
Validation loss: 2.7594165261737964

Epoch: 6| Step: 13
Training loss: 3.1572810839999326
Validation loss: 2.755611618515936

Epoch: 50| Step: 0
Training loss: 2.4278594864810836
Validation loss: 2.7527399573971647

Epoch: 6| Step: 1
Training loss: 3.1556152611965826
Validation loss: 2.753043787751074

Epoch: 6| Step: 2
Training loss: 2.946782319602779
Validation loss: 2.752492959558928

Epoch: 6| Step: 3
Training loss: 2.919160448903148
Validation loss: 2.750057046471963

Epoch: 6| Step: 4
Training loss: 2.623643433767956
Validation loss: 2.749638938320361

Epoch: 6| Step: 5
Training loss: 2.125771999374052
Validation loss: 2.74902664645128

Epoch: 6| Step: 6
Training loss: 2.8527709072698486
Validation loss: 2.7473583682912177

Epoch: 6| Step: 7
Training loss: 2.505413869144314
Validation loss: 2.74297452611124

Epoch: 6| Step: 8
Training loss: 2.9959097635408343
Validation loss: 2.7421832686896073

Epoch: 6| Step: 9
Training loss: 3.084699706017776
Validation loss: 2.7410210920930527

Epoch: 6| Step: 10
Training loss: 3.3550102799964465
Validation loss: 2.7394073536173726

Epoch: 6| Step: 11
Training loss: 2.9512006353041023
Validation loss: 2.73464958628741

Epoch: 6| Step: 12
Training loss: 3.137077491549783
Validation loss: 2.7346160201161687

Epoch: 6| Step: 13
Training loss: 3.0913381712201553
Validation loss: 2.7335599865408646

Epoch: 51| Step: 0
Training loss: 3.057443921502058
Validation loss: 2.7341950566256585

Epoch: 6| Step: 1
Training loss: 2.43965366782273
Validation loss: 2.73099759737475

Epoch: 6| Step: 2
Training loss: 2.7161610872029924
Validation loss: 2.7319517533304145

Epoch: 6| Step: 3
Training loss: 2.7744492955238194
Validation loss: 2.7319664074492707

Epoch: 6| Step: 4
Training loss: 3.497956769454723
Validation loss: 2.7293821836993875

Epoch: 6| Step: 5
Training loss: 3.168076218218002
Validation loss: 2.7291947331817483

Epoch: 6| Step: 6
Training loss: 3.0153283482246875
Validation loss: 2.729547930189783

Epoch: 6| Step: 7
Training loss: 2.8701499166111937
Validation loss: 2.7285472710571224

Epoch: 6| Step: 8
Training loss: 2.6256217220279576
Validation loss: 2.725642911797777

Epoch: 6| Step: 9
Training loss: 2.8515145075691692
Validation loss: 2.723756499438083

Epoch: 6| Step: 10
Training loss: 2.602174245947879
Validation loss: 2.7230218817870337

Epoch: 6| Step: 11
Training loss: 2.8156014190600884
Validation loss: 2.721765387948884

Epoch: 6| Step: 12
Training loss: 2.876903567175421
Validation loss: 2.7174105359101812

Epoch: 6| Step: 13
Training loss: 2.7015781276376636
Validation loss: 2.7151629875441823

Epoch: 52| Step: 0
Training loss: 2.820435074866224
Validation loss: 2.7128251509321477

Epoch: 6| Step: 1
Training loss: 2.947598892895442
Validation loss: 2.7149015498240483

Epoch: 6| Step: 2
Training loss: 2.7207509712344393
Validation loss: 2.713471180493985

Epoch: 6| Step: 3
Training loss: 2.8171502557832566
Validation loss: 2.7122879402488027

Epoch: 6| Step: 4
Training loss: 3.3565218852645318
Validation loss: 2.7117767207628436

Epoch: 6| Step: 5
Training loss: 2.6663928089982245
Validation loss: 2.709632537463869

Epoch: 6| Step: 6
Training loss: 2.8614012829278104
Validation loss: 2.707335263750443

Epoch: 6| Step: 7
Training loss: 2.8323785818797793
Validation loss: 2.7064098472461926

Epoch: 6| Step: 8
Training loss: 2.8953177635774976
Validation loss: 2.7031611181062227

Epoch: 6| Step: 9
Training loss: 3.164972320483816
Validation loss: 2.7024804083590115

Epoch: 6| Step: 10
Training loss: 2.677281314243183
Validation loss: 2.7017449915422307

Epoch: 6| Step: 11
Training loss: 2.8940007473958493
Validation loss: 2.700719450240899

Epoch: 6| Step: 12
Training loss: 3.0053288497868795
Validation loss: 2.7004210043799097

Epoch: 6| Step: 13
Training loss: 2.0471331018689476
Validation loss: 2.6978961889798208

Epoch: 53| Step: 0
Training loss: 2.3446889395279134
Validation loss: 2.6981747968948926

Epoch: 6| Step: 1
Training loss: 2.7227595787198586
Validation loss: 2.695759921324787

Epoch: 6| Step: 2
Training loss: 2.4556484971023034
Validation loss: 2.6939166891474238

Epoch: 6| Step: 3
Training loss: 3.6775616703010146
Validation loss: 2.695272074962925

Epoch: 6| Step: 4
Training loss: 3.2199482585469794
Validation loss: 2.6934658635300557

Epoch: 6| Step: 5
Training loss: 1.9656912035929475
Validation loss: 2.6927612171599975

Epoch: 6| Step: 6
Training loss: 3.221966071927565
Validation loss: 2.6902668187667613

Epoch: 6| Step: 7
Training loss: 2.340153490558899
Validation loss: 2.689406834806297

Epoch: 6| Step: 8
Training loss: 2.562769991909751
Validation loss: 2.686453904001214

Epoch: 6| Step: 9
Training loss: 2.943082010405198
Validation loss: 2.6862192798725917

Epoch: 6| Step: 10
Training loss: 2.961266974887205
Validation loss: 2.6832242264294828

Epoch: 6| Step: 11
Training loss: 2.7953116880173674
Validation loss: 2.6861123783152854

Epoch: 6| Step: 12
Training loss: 3.1022564090143336
Validation loss: 2.690777975201905

Epoch: 6| Step: 13
Training loss: 2.886187307041191
Validation loss: 2.7036501423970374

Epoch: 54| Step: 0
Training loss: 3.240549284915869
Validation loss: 2.7050717350302462

Epoch: 6| Step: 1
Training loss: 3.419287413038551
Validation loss: 2.704420598768756

Epoch: 6| Step: 2
Training loss: 2.4789994341013677
Validation loss: 2.699472447648918

Epoch: 6| Step: 3
Training loss: 2.832548107469303
Validation loss: 2.6921401220615087

Epoch: 6| Step: 4
Training loss: 2.718763329484206
Validation loss: 2.685556285494876

Epoch: 6| Step: 5
Training loss: 2.5068547686890095
Validation loss: 2.6717713542265824

Epoch: 6| Step: 6
Training loss: 2.521121636500364
Validation loss: 2.6705206399850994

Epoch: 6| Step: 7
Training loss: 2.6401591595443312
Validation loss: 2.6711096903810763

Epoch: 6| Step: 8
Training loss: 2.761229475781762
Validation loss: 2.7243610227873454

Epoch: 6| Step: 9
Training loss: 3.612655667973603
Validation loss: 2.7437323179377637

Epoch: 6| Step: 10
Training loss: 2.6051180716497915
Validation loss: 2.7144005722468028

Epoch: 6| Step: 11
Training loss: 2.7646656851060367
Validation loss: 2.683440772762741

Epoch: 6| Step: 12
Training loss: 2.5337043448702445
Validation loss: 2.6714653905803925

Epoch: 6| Step: 13
Training loss: 2.8218268135831677
Validation loss: 2.683835421752812

Epoch: 55| Step: 0
Training loss: 3.1087513974029015
Validation loss: 2.680695072775249

Epoch: 6| Step: 1
Training loss: 2.852945405252537
Validation loss: 2.6749129397048814

Epoch: 6| Step: 2
Training loss: 2.9658960377330534
Validation loss: 2.6767577679650443

Epoch: 6| Step: 3
Training loss: 3.162898699691357
Validation loss: 2.6694062634358655

Epoch: 6| Step: 4
Training loss: 3.107062778787729
Validation loss: 2.663589390782293

Epoch: 6| Step: 5
Training loss: 3.0155864179445757
Validation loss: 2.66264694954543

Epoch: 6| Step: 6
Training loss: 2.678425928432885
Validation loss: 2.667453758985388

Epoch: 6| Step: 7
Training loss: 2.8882870678975925
Validation loss: 2.662030023636233

Epoch: 6| Step: 8
Training loss: 2.194752251757414
Validation loss: 2.6602417665572893

Epoch: 6| Step: 9
Training loss: 2.270007925481867
Validation loss: 2.6576827280520994

Epoch: 6| Step: 10
Training loss: 2.266602193626043
Validation loss: 2.652616746174687

Epoch: 6| Step: 11
Training loss: 2.5094515474076444
Validation loss: 2.654443822390217

Epoch: 6| Step: 12
Training loss: 3.0113916281287407
Validation loss: 2.6519880922471146

Epoch: 6| Step: 13
Training loss: 2.9927088828550517
Validation loss: 2.6541291090994354

Epoch: 56| Step: 0
Training loss: 2.6603803946328295
Validation loss: 2.652748387913455

Epoch: 6| Step: 1
Training loss: 3.239153000358119
Validation loss: 2.651055676336437

Epoch: 6| Step: 2
Training loss: 2.4873505053813973
Validation loss: 2.651326332222444

Epoch: 6| Step: 3
Training loss: 2.8731267050538265
Validation loss: 2.651027332159733

Epoch: 6| Step: 4
Training loss: 3.030340313826574
Validation loss: 2.6497033019085245

Epoch: 6| Step: 5
Training loss: 2.3947359529651338
Validation loss: 2.649486817747007

Epoch: 6| Step: 6
Training loss: 2.6331285236080633
Validation loss: 2.647877741861958

Epoch: 6| Step: 7
Training loss: 3.0287978488670646
Validation loss: 2.6468809762021768

Epoch: 6| Step: 8
Training loss: 2.1700721777940792
Validation loss: 2.646026068648648

Epoch: 6| Step: 9
Training loss: 2.8151115160954068
Validation loss: 2.6434342829351785

Epoch: 6| Step: 10
Training loss: 3.1764924213805985
Validation loss: 2.643915537648454

Epoch: 6| Step: 11
Training loss: 3.0636499542498803
Validation loss: 2.6424012990784154

Epoch: 6| Step: 12
Training loss: 2.4619464077729423
Validation loss: 2.6408414432220146

Epoch: 6| Step: 13
Training loss: 2.757429317837738
Validation loss: 2.641765175275313

Epoch: 57| Step: 0
Training loss: 2.923432776377464
Validation loss: 2.638497949944122

Epoch: 6| Step: 1
Training loss: 2.769590046289453
Validation loss: 2.637332149787241

Epoch: 6| Step: 2
Training loss: 2.880221724982053
Validation loss: 2.635374232215663

Epoch: 6| Step: 3
Training loss: 2.8501642012559723
Validation loss: 2.638101022348232

Epoch: 6| Step: 4
Training loss: 2.8675099537152025
Validation loss: 2.637683200349366

Epoch: 6| Step: 5
Training loss: 2.6924362979816188
Validation loss: 2.6347605877473006

Epoch: 6| Step: 6
Training loss: 2.712579675022868
Validation loss: 2.6336576046855558

Epoch: 6| Step: 7
Training loss: 2.3779689903066155
Validation loss: 2.630393814244658

Epoch: 6| Step: 8
Training loss: 2.5119438960376526
Validation loss: 2.629182004707523

Epoch: 6| Step: 9
Training loss: 2.5615228790387308
Validation loss: 2.6286947648041967

Epoch: 6| Step: 10
Training loss: 2.724091205067145
Validation loss: 2.6278590496789884

Epoch: 6| Step: 11
Training loss: 3.2694349924226564
Validation loss: 2.627562316823387

Epoch: 6| Step: 12
Training loss: 2.917982803576432
Validation loss: 2.6279047380423326

Epoch: 6| Step: 13
Training loss: 2.5767260830910015
Validation loss: 2.628539348390739

Epoch: 58| Step: 0
Training loss: 2.95525077723906
Validation loss: 2.626762177099264

Epoch: 6| Step: 1
Training loss: 2.5387468816755896
Validation loss: 2.6274174200765077

Epoch: 6| Step: 2
Training loss: 2.835443795266145
Validation loss: 2.629044867796169

Epoch: 6| Step: 3
Training loss: 2.993658993797152
Validation loss: 2.6287869808505966

Epoch: 6| Step: 4
Training loss: 3.099833047124304
Validation loss: 2.631198301434145

Epoch: 6| Step: 5
Training loss: 2.260445402698128
Validation loss: 2.623740847909203

Epoch: 6| Step: 6
Training loss: 2.5094836599878656
Validation loss: 2.621143596419654

Epoch: 6| Step: 7
Training loss: 2.7464856319888495
Validation loss: 2.617361672142911

Epoch: 6| Step: 8
Training loss: 3.115474679701056
Validation loss: 2.6167363019857866

Epoch: 6| Step: 9
Training loss: 2.8282804499102854
Validation loss: 2.6133191115939547

Epoch: 6| Step: 10
Training loss: 2.3730548120687738
Validation loss: 2.6101437804810943

Epoch: 6| Step: 11
Training loss: 2.6657676274019915
Validation loss: 2.6082835094174217

Epoch: 6| Step: 12
Training loss: 2.6153343814021546
Validation loss: 2.608407212288685

Epoch: 6| Step: 13
Training loss: 2.90272488665015
Validation loss: 2.624682906844313

Epoch: 59| Step: 0
Training loss: 2.938257647264614
Validation loss: 2.6340303255120205

Epoch: 6| Step: 1
Training loss: 2.7145721958387656
Validation loss: 2.6025889436618725

Epoch: 6| Step: 2
Training loss: 3.010218700688799
Validation loss: 2.604437994491495

Epoch: 6| Step: 3
Training loss: 2.906933447759365
Validation loss: 2.6050998287390303

Epoch: 6| Step: 4
Training loss: 2.5053786591954816
Validation loss: 2.61179410228761

Epoch: 6| Step: 5
Training loss: 2.9281643175805474
Validation loss: 2.6131944247553935

Epoch: 6| Step: 6
Training loss: 2.9295514291317253
Validation loss: 2.6152863538883606

Epoch: 6| Step: 7
Training loss: 2.4869245007217327
Validation loss: 2.6185088660416826

Epoch: 6| Step: 8
Training loss: 2.706336258357106
Validation loss: 2.6191606286157607

Epoch: 6| Step: 9
Training loss: 2.6289064313823305
Validation loss: 2.622111062905914

Epoch: 6| Step: 10
Training loss: 3.0919720713944367
Validation loss: 2.6251491625352434

Epoch: 6| Step: 11
Training loss: 2.4851481835990787
Validation loss: 2.6257065624392495

Epoch: 6| Step: 12
Training loss: 2.505573354499491
Validation loss: 2.6251382488599804

Epoch: 6| Step: 13
Training loss: 2.7271908798362934
Validation loss: 2.6231611865546056

Epoch: 60| Step: 0
Training loss: 2.604876632551528
Validation loss: 2.6208811947122026

Epoch: 6| Step: 1
Training loss: 3.007194158234926
Validation loss: 2.6194776545289513

Epoch: 6| Step: 2
Training loss: 2.769591423639983
Validation loss: 2.6151591169769275

Epoch: 6| Step: 3
Training loss: 2.6665802186622916
Validation loss: 2.613281037122017

Epoch: 6| Step: 4
Training loss: 2.9895128691517985
Validation loss: 2.614460327983566

Epoch: 6| Step: 5
Training loss: 2.3138747381861693
Validation loss: 2.611154250426634

Epoch: 6| Step: 6
Training loss: 2.638684524742995
Validation loss: 2.610988506410183

Epoch: 6| Step: 7
Training loss: 2.33778782668732
Validation loss: 2.610456139909716

Epoch: 6| Step: 8
Training loss: 2.787278745620203
Validation loss: 2.605787098588987

Epoch: 6| Step: 9
Training loss: 2.798455415175285
Validation loss: 2.6035192880225546

Epoch: 6| Step: 10
Training loss: 2.731231713833907
Validation loss: 2.6012456508137562

Epoch: 6| Step: 11
Training loss: 2.5991579232696145
Validation loss: 2.600477823036363

Epoch: 6| Step: 12
Training loss: 2.909712502871243
Validation loss: 2.5985640925653857

Epoch: 6| Step: 13
Training loss: 3.148393578613464
Validation loss: 2.597571074731402

Epoch: 61| Step: 0
Training loss: 2.940755136267618
Validation loss: 2.5956574745268566

Epoch: 6| Step: 1
Training loss: 2.5084507210300995
Validation loss: 2.595632727715813

Epoch: 6| Step: 2
Training loss: 2.621821704636553
Validation loss: 2.5921797384552496

Epoch: 6| Step: 3
Training loss: 2.675556880704876
Validation loss: 2.589488845680865

Epoch: 6| Step: 4
Training loss: 2.851900096400832
Validation loss: 2.5895594253316165

Epoch: 6| Step: 5
Training loss: 2.4630520893989987
Validation loss: 2.588968527052875

Epoch: 6| Step: 6
Training loss: 2.86505529966128
Validation loss: 2.5886428373178054

Epoch: 6| Step: 7
Training loss: 2.6927798548828297
Validation loss: 2.5896848818774694

Epoch: 6| Step: 8
Training loss: 2.509667681849837
Validation loss: 2.584335414793955

Epoch: 6| Step: 9
Training loss: 2.8153425475100815
Validation loss: 2.581990251778329

Epoch: 6| Step: 10
Training loss: 2.7306397129395017
Validation loss: 2.584377359852757

Epoch: 6| Step: 11
Training loss: 2.65568940473976
Validation loss: 2.5808813600030542

Epoch: 6| Step: 12
Training loss: 2.9927624182500425
Validation loss: 2.582186880341296

Epoch: 6| Step: 13
Training loss: 2.7724574976863128
Validation loss: 2.5798330794657396

Epoch: 62| Step: 0
Training loss: 2.787519353961898
Validation loss: 2.576652537928539

Epoch: 6| Step: 1
Training loss: 2.843280124621109
Validation loss: 2.5807300083891684

Epoch: 6| Step: 2
Training loss: 2.694313018800902
Validation loss: 2.574549667707089

Epoch: 6| Step: 3
Training loss: 3.3321116751907316
Validation loss: 2.577120714422774

Epoch: 6| Step: 4
Training loss: 2.678914573124738
Validation loss: 2.5764964190650126

Epoch: 6| Step: 5
Training loss: 2.614679848965347
Validation loss: 2.580047476329207

Epoch: 6| Step: 6
Training loss: 2.4378261836978057
Validation loss: 2.5747083430868947

Epoch: 6| Step: 7
Training loss: 2.5395210673642605
Validation loss: 2.5743260455310124

Epoch: 6| Step: 8
Training loss: 2.8391397855124514
Validation loss: 2.576081593213458

Epoch: 6| Step: 9
Training loss: 2.776324335895343
Validation loss: 2.5742745053355907

Epoch: 6| Step: 10
Training loss: 2.360259237760461
Validation loss: 2.5713914734192778

Epoch: 6| Step: 11
Training loss: 2.399369148948791
Validation loss: 2.5699951996368817

Epoch: 6| Step: 12
Training loss: 2.9183481728092793
Validation loss: 2.571232516078327

Epoch: 6| Step: 13
Training loss: 2.6002891563135146
Validation loss: 2.569866354271508

Epoch: 63| Step: 0
Training loss: 2.619707312214928
Validation loss: 2.5693716628847048

Epoch: 6| Step: 1
Training loss: 2.8135014764277853
Validation loss: 2.568755399972108

Epoch: 6| Step: 2
Training loss: 2.7665498555660477
Validation loss: 2.5714672242771472

Epoch: 6| Step: 3
Training loss: 2.377701076664841
Validation loss: 2.5704376746774558

Epoch: 6| Step: 4
Training loss: 2.7321068648274727
Validation loss: 2.569542766671658

Epoch: 6| Step: 5
Training loss: 2.6440813366474054
Validation loss: 2.5659168847127427

Epoch: 6| Step: 6
Training loss: 2.7644374905723996
Validation loss: 2.567103120512618

Epoch: 6| Step: 7
Training loss: 2.1276632337335815
Validation loss: 2.5683099584817457

Epoch: 6| Step: 8
Training loss: 3.253149120816706
Validation loss: 2.568977465307885

Epoch: 6| Step: 9
Training loss: 2.520606847840054
Validation loss: 2.564506179636516

Epoch: 6| Step: 10
Training loss: 2.976661179876735
Validation loss: 2.5636547402115353

Epoch: 6| Step: 11
Training loss: 2.766621727942304
Validation loss: 2.562886294269357

Epoch: 6| Step: 12
Training loss: 2.7683385263387015
Validation loss: 2.5671633644385734

Epoch: 6| Step: 13
Training loss: 2.5768917949017256
Validation loss: 2.562287065916591

Epoch: 64| Step: 0
Training loss: 2.6798885703763484
Validation loss: 2.56158297497804

Epoch: 6| Step: 1
Training loss: 2.7876328509366317
Validation loss: 2.559581552320959

Epoch: 6| Step: 2
Training loss: 3.142250361636558
Validation loss: 2.5622035994886403

Epoch: 6| Step: 3
Training loss: 1.956036404292362
Validation loss: 2.558375380809406

Epoch: 6| Step: 4
Training loss: 2.537995005465163
Validation loss: 2.5600165917186706

Epoch: 6| Step: 5
Training loss: 2.919661882237701
Validation loss: 2.5573999468739883

Epoch: 6| Step: 6
Training loss: 2.715358240789597
Validation loss: 2.5581443955013694

Epoch: 6| Step: 7
Training loss: 2.380104602470482
Validation loss: 2.5614657447048925

Epoch: 6| Step: 8
Training loss: 2.9932269887859295
Validation loss: 2.562697565789893

Epoch: 6| Step: 9
Training loss: 2.628686133319477
Validation loss: 2.5595534216265134

Epoch: 6| Step: 10
Training loss: 2.869295182294713
Validation loss: 2.5596262471979867

Epoch: 6| Step: 11
Training loss: 2.8094644059158163
Validation loss: 2.557495129811512

Epoch: 6| Step: 12
Training loss: 2.3821923621502243
Validation loss: 2.5567056226518643

Epoch: 6| Step: 13
Training loss: 2.7906074412307165
Validation loss: 2.556937654831456

Epoch: 65| Step: 0
Training loss: 2.7283583358833887
Validation loss: 2.5606715339055572

Epoch: 6| Step: 1
Training loss: 2.050913081700189
Validation loss: 2.557791299174754

Epoch: 6| Step: 2
Training loss: 2.891501463935715
Validation loss: 2.5559820070444315

Epoch: 6| Step: 3
Training loss: 2.7817046404654664
Validation loss: 2.5554742564063604

Epoch: 6| Step: 4
Training loss: 2.4787872622522302
Validation loss: 2.555186340594025

Epoch: 6| Step: 5
Training loss: 3.1799550236814227
Validation loss: 2.55622186186708

Epoch: 6| Step: 6
Training loss: 2.6825814593657378
Validation loss: 2.5547762825224876

Epoch: 6| Step: 7
Training loss: 2.1723838594250395
Validation loss: 2.5516292609229025

Epoch: 6| Step: 8
Training loss: 3.236829627898117
Validation loss: 2.551876703318285

Epoch: 6| Step: 9
Training loss: 2.974904637451209
Validation loss: 2.549648888419452

Epoch: 6| Step: 10
Training loss: 2.330640828822329
Validation loss: 2.5449331795164953

Epoch: 6| Step: 11
Training loss: 2.913723923391783
Validation loss: 2.5460003089984506

Epoch: 6| Step: 12
Training loss: 2.7929850544486947
Validation loss: 2.54327673543681

Epoch: 6| Step: 13
Training loss: 2.147868799020931
Validation loss: 2.54796500566926

Epoch: 66| Step: 0
Training loss: 2.51796504584693
Validation loss: 2.5483319848329193

Epoch: 6| Step: 1
Training loss: 2.2046388876223912
Validation loss: 2.5494124521028154

Epoch: 6| Step: 2
Training loss: 2.6144363747035513
Validation loss: 2.5522299640078447

Epoch: 6| Step: 3
Training loss: 2.821154270669152
Validation loss: 2.5536287779565594

Epoch: 6| Step: 4
Training loss: 2.531173893290655
Validation loss: 2.5514500409626923

Epoch: 6| Step: 5
Training loss: 3.2563186257538805
Validation loss: 2.55238493609897

Epoch: 6| Step: 6
Training loss: 2.695212718941085
Validation loss: 2.5521927064048713

Epoch: 6| Step: 7
Training loss: 2.9149265184513986
Validation loss: 2.546582281223324

Epoch: 6| Step: 8
Training loss: 2.967318059008483
Validation loss: 2.546495124555068

Epoch: 6| Step: 9
Training loss: 3.047421763341353
Validation loss: 2.5436617468068703

Epoch: 6| Step: 10
Training loss: 2.449396008369543
Validation loss: 2.541295373614445

Epoch: 6| Step: 11
Training loss: 2.7322559101781834
Validation loss: 2.5411152215390733

Epoch: 6| Step: 12
Training loss: 2.3316653170614674
Validation loss: 2.5404184330183357

Epoch: 6| Step: 13
Training loss: 2.1847881401234583
Validation loss: 2.5439011856127722

Epoch: 67| Step: 0
Training loss: 2.2775763626780092
Validation loss: 2.544413385266452

Epoch: 6| Step: 1
Training loss: 2.692324189774827
Validation loss: 2.5475399654848587

Epoch: 6| Step: 2
Training loss: 2.8894040390162803
Validation loss: 2.5505817796839914

Epoch: 6| Step: 3
Training loss: 2.6484832309611885
Validation loss: 2.5529963255090165

Epoch: 6| Step: 4
Training loss: 2.852344478877697
Validation loss: 2.5546695191414557

Epoch: 6| Step: 5
Training loss: 2.3463972273307205
Validation loss: 2.5538135554141093

Epoch: 6| Step: 6
Training loss: 2.671063751739545
Validation loss: 2.5526480744067443

Epoch: 6| Step: 7
Training loss: 2.605517248248495
Validation loss: 2.5540223884178763

Epoch: 6| Step: 8
Training loss: 3.272380649147504
Validation loss: 2.55323440647297

Epoch: 6| Step: 9
Training loss: 2.8002190231763637
Validation loss: 2.550589382399976

Epoch: 6| Step: 10
Training loss: 2.156261720487281
Validation loss: 2.547251870447205

Epoch: 6| Step: 11
Training loss: 2.8338492615642186
Validation loss: 2.5464333227323834

Epoch: 6| Step: 12
Training loss: 2.831687916957907
Validation loss: 2.5455091281453206

Epoch: 6| Step: 13
Training loss: 2.4987897804685124
Validation loss: 2.5446878727219264

Epoch: 68| Step: 0
Training loss: 2.3471140814654863
Validation loss: 2.5412327182305368

Epoch: 6| Step: 1
Training loss: 2.457795186597177
Validation loss: 2.538067275405932

Epoch: 6| Step: 2
Training loss: 2.629647999113791
Validation loss: 2.5375219434970098

Epoch: 6| Step: 3
Training loss: 2.087166070783839
Validation loss: 2.535978006762937

Epoch: 6| Step: 4
Training loss: 3.0948883428445835
Validation loss: 2.531911347573335

Epoch: 6| Step: 5
Training loss: 2.538152255710627
Validation loss: 2.532868656616069

Epoch: 6| Step: 6
Training loss: 3.098553560038734
Validation loss: 2.531557935657161

Epoch: 6| Step: 7
Training loss: 1.9485474311511954
Validation loss: 2.5307557522218076

Epoch: 6| Step: 8
Training loss: 2.5161940600359127
Validation loss: 2.5310346880730825

Epoch: 6| Step: 9
Training loss: 3.0826576282947302
Validation loss: 2.533101382010153

Epoch: 6| Step: 10
Training loss: 2.775901881569937
Validation loss: 2.5325424599485116

Epoch: 6| Step: 11
Training loss: 2.952757635932075
Validation loss: 2.5304751681993993

Epoch: 6| Step: 12
Training loss: 2.7435746990676257
Validation loss: 2.5310206682217147

Epoch: 6| Step: 13
Training loss: 2.695251729528907
Validation loss: 2.52997073275961

Epoch: 69| Step: 0
Training loss: 2.8666276515109574
Validation loss: 2.5273108895399647

Epoch: 6| Step: 1
Training loss: 2.147068433640188
Validation loss: 2.5300538490304376

Epoch: 6| Step: 2
Training loss: 2.5415811576425784
Validation loss: 2.531148272695446

Epoch: 6| Step: 3
Training loss: 2.301393799772403
Validation loss: 2.5318143239092303

Epoch: 6| Step: 4
Training loss: 2.7712594089511793
Validation loss: 2.529497568227224

Epoch: 6| Step: 5
Training loss: 2.592561012661055
Validation loss: 2.530659029697208

Epoch: 6| Step: 6
Training loss: 2.4531432716029333
Validation loss: 2.5288161833583493

Epoch: 6| Step: 7
Training loss: 2.6471773373495773
Validation loss: 2.527759940278925

Epoch: 6| Step: 8
Training loss: 2.4522040515397947
Validation loss: 2.5308299873923636

Epoch: 6| Step: 9
Training loss: 3.035378702031444
Validation loss: 2.5301259060517207

Epoch: 6| Step: 10
Training loss: 2.9964039230437103
Validation loss: 2.529412870557981

Epoch: 6| Step: 11
Training loss: 2.442974398722033
Validation loss: 2.5297444265493767

Epoch: 6| Step: 12
Training loss: 2.683765744579311
Validation loss: 2.5304127002996744

Epoch: 6| Step: 13
Training loss: 3.1413415499430237
Validation loss: 2.530842037841211

Epoch: 70| Step: 0
Training loss: 2.8863731662644803
Validation loss: 2.529096087288459

Epoch: 6| Step: 1
Training loss: 2.7636275324954154
Validation loss: 2.5280917916397896

Epoch: 6| Step: 2
Training loss: 2.789167813575432
Validation loss: 2.5271182148772775

Epoch: 6| Step: 3
Training loss: 2.821951857287979
Validation loss: 2.5230265342757505

Epoch: 6| Step: 4
Training loss: 2.623355532030785
Validation loss: 2.5293508241612073

Epoch: 6| Step: 5
Training loss: 2.555206432774663
Validation loss: 2.5206764319485537

Epoch: 6| Step: 6
Training loss: 2.638718317330467
Validation loss: 2.521396752649056

Epoch: 6| Step: 7
Training loss: 2.5048521638597694
Validation loss: 2.5224487918763683

Epoch: 6| Step: 8
Training loss: 2.6213446097702198
Validation loss: 2.523445090015756

Epoch: 6| Step: 9
Training loss: 2.1966908970578443
Validation loss: 2.521946004681801

Epoch: 6| Step: 10
Training loss: 3.101424745051659
Validation loss: 2.52652402906317

Epoch: 6| Step: 11
Training loss: 2.638146676428841
Validation loss: 2.525323609743596

Epoch: 6| Step: 12
Training loss: 2.6943543431094787
Validation loss: 2.5250293761455027

Epoch: 6| Step: 13
Training loss: 2.2903147322083925
Validation loss: 2.5280594674612598

Epoch: 71| Step: 0
Training loss: 2.1611661292195787
Validation loss: 2.526455754164247

Epoch: 6| Step: 1
Training loss: 2.52186464593551
Validation loss: 2.52744127544297

Epoch: 6| Step: 2
Training loss: 2.4562466919854873
Validation loss: 2.5256651502711014

Epoch: 6| Step: 3
Training loss: 2.207389437886565
Validation loss: 2.526500924955836

Epoch: 6| Step: 4
Training loss: 2.264102819069263
Validation loss: 2.527803924504501

Epoch: 6| Step: 5
Training loss: 2.6314388506389617
Validation loss: 2.526891857628484

Epoch: 6| Step: 6
Training loss: 2.4298166320437526
Validation loss: 2.5254213390759226

Epoch: 6| Step: 7
Training loss: 2.533664164357187
Validation loss: 2.5229189628340722

Epoch: 6| Step: 8
Training loss: 2.807199867082855
Validation loss: 2.5219214957046607

Epoch: 6| Step: 9
Training loss: 2.4860449402691502
Validation loss: 2.5202843775070027

Epoch: 6| Step: 10
Training loss: 3.2859150013717997
Validation loss: 2.519228547909695

Epoch: 6| Step: 11
Training loss: 2.9264515005916194
Validation loss: 2.5164377546307217

Epoch: 6| Step: 12
Training loss: 3.193631427153873
Validation loss: 2.5136581535132807

Epoch: 6| Step: 13
Training loss: 2.84395774669351
Validation loss: 2.5153978138570423

Epoch: 72| Step: 0
Training loss: 2.458720731653147
Validation loss: 2.512955680885447

Epoch: 6| Step: 1
Training loss: 2.8559124102264155
Validation loss: 2.5242101630007063

Epoch: 6| Step: 2
Training loss: 2.750406928732657
Validation loss: 2.528474769869226

Epoch: 6| Step: 3
Training loss: 2.6297317727816547
Validation loss: 2.5258931119568393

Epoch: 6| Step: 4
Training loss: 3.058605286636129
Validation loss: 2.5360035942334958

Epoch: 6| Step: 5
Training loss: 2.0875187855863193
Validation loss: 2.52090411966945

Epoch: 6| Step: 6
Training loss: 3.1151471260313355
Validation loss: 2.5214589711354227

Epoch: 6| Step: 7
Training loss: 2.423498384853334
Validation loss: 2.5278375016540675

Epoch: 6| Step: 8
Training loss: 2.893721947420722
Validation loss: 2.5169538064647434

Epoch: 6| Step: 9
Training loss: 2.9033295098450016
Validation loss: 2.5081170868624003

Epoch: 6| Step: 10
Training loss: 2.0572623592816237
Validation loss: 2.509810162061262

Epoch: 6| Step: 11
Training loss: 2.692176564659895
Validation loss: 2.5142304087208065

Epoch: 6| Step: 12
Training loss: 2.596273642273003
Validation loss: 2.515091280661602

Epoch: 6| Step: 13
Training loss: 2.327773202007999
Validation loss: 2.5136227744957647

Epoch: 73| Step: 0
Training loss: 2.634257658565814
Validation loss: 2.511729381295541

Epoch: 6| Step: 1
Training loss: 3.0864931016848307
Validation loss: 2.509861031189615

Epoch: 6| Step: 2
Training loss: 2.654733741182393
Validation loss: 2.513040545819437

Epoch: 6| Step: 3
Training loss: 2.4033637633846423
Validation loss: 2.5129867366921963

Epoch: 6| Step: 4
Training loss: 2.8762441513884034
Validation loss: 2.5133034871637117

Epoch: 6| Step: 5
Training loss: 2.4661618923374697
Validation loss: 2.5156245261245185

Epoch: 6| Step: 6
Training loss: 2.878219294789562
Validation loss: 2.5159464407639454

Epoch: 6| Step: 7
Training loss: 2.758388988551888
Validation loss: 2.5148148110668065

Epoch: 6| Step: 8
Training loss: 2.216967551765211
Validation loss: 2.5128940425110105

Epoch: 6| Step: 9
Training loss: 2.1508252001843493
Validation loss: 2.5119671182160923

Epoch: 6| Step: 10
Training loss: 2.724272107541788
Validation loss: 2.511931928938851

Epoch: 6| Step: 11
Training loss: 2.5518929131500694
Validation loss: 2.510206598908179

Epoch: 6| Step: 12
Training loss: 2.4360159244110813
Validation loss: 2.5075982023046794

Epoch: 6| Step: 13
Training loss: 2.9879822026632987
Validation loss: 2.5040304120444223

Epoch: 74| Step: 0
Training loss: 2.1709877232609034
Validation loss: 2.511826279050553

Epoch: 6| Step: 1
Training loss: 2.516995737738291
Validation loss: 2.5083346511566993

Epoch: 6| Step: 2
Training loss: 2.0390353730834456
Validation loss: 2.5060590117910273

Epoch: 6| Step: 3
Training loss: 2.7813004907032575
Validation loss: 2.5138352782385605

Epoch: 6| Step: 4
Training loss: 2.341101510572832
Validation loss: 2.5036931892002228

Epoch: 6| Step: 5
Training loss: 2.75250130385698
Validation loss: 2.503454507831809

Epoch: 6| Step: 6
Training loss: 2.941728024657808
Validation loss: 2.504953658283788

Epoch: 6| Step: 7
Training loss: 2.924648987664982
Validation loss: 2.503931276510761

Epoch: 6| Step: 8
Training loss: 2.927365942148834
Validation loss: 2.5065266609491204

Epoch: 6| Step: 9
Training loss: 2.2313703061108354
Validation loss: 2.504773621041349

Epoch: 6| Step: 10
Training loss: 2.5268355128288076
Validation loss: 2.5042468477594704

Epoch: 6| Step: 11
Training loss: 3.1474444240665935
Validation loss: 2.507340209474122

Epoch: 6| Step: 12
Training loss: 2.7511367182480546
Validation loss: 2.503579866796608

Epoch: 6| Step: 13
Training loss: 2.5747148559613464
Validation loss: 2.505970802515565

Epoch: 75| Step: 0
Training loss: 2.6814275936186185
Validation loss: 2.504339076584929

Epoch: 6| Step: 1
Training loss: 2.6168235411867626
Validation loss: 2.5048413764644604

Epoch: 6| Step: 2
Training loss: 2.812159708417531
Validation loss: 2.505798957554243

Epoch: 6| Step: 3
Training loss: 3.035698212653172
Validation loss: 2.5046930767972135

Epoch: 6| Step: 4
Training loss: 2.479340448352725
Validation loss: 2.503461840980323

Epoch: 6| Step: 5
Training loss: 2.5166927938389283
Validation loss: 2.5024655105546416

Epoch: 6| Step: 6
Training loss: 2.489344107866935
Validation loss: 2.5047551549514577

Epoch: 6| Step: 7
Training loss: 2.2498016269973324
Validation loss: 2.5007353654803346

Epoch: 6| Step: 8
Training loss: 2.8802848010176256
Validation loss: 2.501554339727355

Epoch: 6| Step: 9
Training loss: 2.676055582281816
Validation loss: 2.5053499912703296

Epoch: 6| Step: 10
Training loss: 2.4339246693271903
Validation loss: 2.4979075257996017

Epoch: 6| Step: 11
Training loss: 2.859436034504847
Validation loss: 2.496867705116867

Epoch: 6| Step: 12
Training loss: 2.479489495124572
Validation loss: 2.4982284308921567

Epoch: 6| Step: 13
Training loss: 2.458012274342891
Validation loss: 2.504171292961362

Epoch: 76| Step: 0
Training loss: 2.435259057898594
Validation loss: 2.4964378887813186

Epoch: 6| Step: 1
Training loss: 2.972000912025692
Validation loss: 2.496515579677228

Epoch: 6| Step: 2
Training loss: 2.258505744499449
Validation loss: 2.5022374075259792

Epoch: 6| Step: 3
Training loss: 2.656528054437994
Validation loss: 2.498996151607095

Epoch: 6| Step: 4
Training loss: 2.2970819704395526
Validation loss: 2.4946467782928017

Epoch: 6| Step: 5
Training loss: 2.4704894685904866
Validation loss: 2.4966715112680746

Epoch: 6| Step: 6
Training loss: 2.6254737063275244
Validation loss: 2.4930018865203554

Epoch: 6| Step: 7
Training loss: 2.851270519619246
Validation loss: 2.491581587045909

Epoch: 6| Step: 8
Training loss: 2.9087871018991414
Validation loss: 2.500649781025267

Epoch: 6| Step: 9
Training loss: 2.640408478838005
Validation loss: 2.5008324349831943

Epoch: 6| Step: 10
Training loss: 2.196715642974499
Validation loss: 2.501515755503561

Epoch: 6| Step: 11
Training loss: 2.647757743990529
Validation loss: 2.4987389006375325

Epoch: 6| Step: 12
Training loss: 2.9706841843783107
Validation loss: 2.497206255440121

Epoch: 6| Step: 13
Training loss: 2.5886617181111573
Validation loss: 2.4965777656692323

Epoch: 77| Step: 0
Training loss: 3.4150826263485334
Validation loss: 2.4952944658360656

Epoch: 6| Step: 1
Training loss: 2.3548923375491615
Validation loss: 2.4939564293596614

Epoch: 6| Step: 2
Training loss: 2.230249611266023
Validation loss: 2.497858680155981

Epoch: 6| Step: 3
Training loss: 2.5183351501653526
Validation loss: 2.4956114635869473

Epoch: 6| Step: 4
Training loss: 2.3031428963989424
Validation loss: 2.495245736251525

Epoch: 6| Step: 5
Training loss: 2.911888804995598
Validation loss: 2.500449195878642

Epoch: 6| Step: 6
Training loss: 3.195451579529745
Validation loss: 2.5031880872811363

Epoch: 6| Step: 7
Training loss: 2.2506694857314535
Validation loss: 2.497472526531559

Epoch: 6| Step: 8
Training loss: 2.6720098271845942
Validation loss: 2.498927220008672

Epoch: 6| Step: 9
Training loss: 2.3553532561017367
Validation loss: 2.495610739112065

Epoch: 6| Step: 10
Training loss: 2.714210579126199
Validation loss: 2.4987811137163134

Epoch: 6| Step: 11
Training loss: 2.2795028266596353
Validation loss: 2.5011160584592007

Epoch: 6| Step: 12
Training loss: 2.4259716211091593
Validation loss: 2.4985969739081306

Epoch: 6| Step: 13
Training loss: 2.846936798262438
Validation loss: 2.497881977602947

Epoch: 78| Step: 0
Training loss: 2.3176433997041346
Validation loss: 2.4975004734449984

Epoch: 6| Step: 1
Training loss: 2.5933426686521783
Validation loss: 2.4932230489058247

Epoch: 6| Step: 2
Training loss: 2.802152545362527
Validation loss: 2.4934608451176543

Epoch: 6| Step: 3
Training loss: 2.88539780887175
Validation loss: 2.4903835994913632

Epoch: 6| Step: 4
Training loss: 2.4321563051338906
Validation loss: 2.4962482434430386

Epoch: 6| Step: 5
Training loss: 2.1717379959036407
Validation loss: 2.4997222746127843

Epoch: 6| Step: 6
Training loss: 3.091170343282059
Validation loss: 2.4991789741208827

Epoch: 6| Step: 7
Training loss: 2.1870446957458793
Validation loss: 2.496962179827156

Epoch: 6| Step: 8
Training loss: 2.7015573884332067
Validation loss: 2.498266127456276

Epoch: 6| Step: 9
Training loss: 2.7162675595582706
Validation loss: 2.499367570356484

Epoch: 6| Step: 10
Training loss: 2.770261681655918
Validation loss: 2.5104668217841906

Epoch: 6| Step: 11
Training loss: 2.7142581006129767
Validation loss: 2.501182641999079

Epoch: 6| Step: 12
Training loss: 2.397207932805978
Validation loss: 2.505903394772815

Epoch: 6| Step: 13
Training loss: 2.748402998785238
Validation loss: 2.4954469227757317

Epoch: 79| Step: 0
Training loss: 2.8651068931464447
Validation loss: 2.489332295507301

Epoch: 6| Step: 1
Training loss: 2.7744587482101837
Validation loss: 2.490682867216331

Epoch: 6| Step: 2
Training loss: 2.7331289013342555
Validation loss: 2.4910822283928535

Epoch: 6| Step: 3
Training loss: 2.410657833588855
Validation loss: 2.492985198132942

Epoch: 6| Step: 4
Training loss: 2.9301207361961255
Validation loss: 2.4918715736142993

Epoch: 6| Step: 5
Training loss: 2.5255985041689137
Validation loss: 2.488981618483071

Epoch: 6| Step: 6
Training loss: 2.2622624140809955
Validation loss: 2.4859032638753034

Epoch: 6| Step: 7
Training loss: 2.587757311572121
Validation loss: 2.489016621357017

Epoch: 6| Step: 8
Training loss: 2.026810592489078
Validation loss: 2.483300974547589

Epoch: 6| Step: 9
Training loss: 1.855722895834798
Validation loss: 2.4890696954712968

Epoch: 6| Step: 10
Training loss: 2.557841183006985
Validation loss: 2.489730085785426

Epoch: 6| Step: 11
Training loss: 2.2270520123907196
Validation loss: 2.482584710111671

Epoch: 6| Step: 12
Training loss: 3.223685915052248
Validation loss: 2.495761234301342

Epoch: 6| Step: 13
Training loss: 2.9838551972362417
Validation loss: 2.4949820703997476

Epoch: 80| Step: 0
Training loss: 2.7835358216721815
Validation loss: 2.499331559782026

Epoch: 6| Step: 1
Training loss: 2.674207238348908
Validation loss: 2.485422933037131

Epoch: 6| Step: 2
Training loss: 2.5481081358571664
Validation loss: 2.4908376165025405

Epoch: 6| Step: 3
Training loss: 2.0439247826548996
Validation loss: 2.485155554776023

Epoch: 6| Step: 4
Training loss: 2.632276531418005
Validation loss: 2.4857168832226666

Epoch: 6| Step: 5
Training loss: 3.2353596685776056
Validation loss: 2.481991342126263

Epoch: 6| Step: 6
Training loss: 2.4908195735499863
Validation loss: 2.4819829369152373

Epoch: 6| Step: 7
Training loss: 2.175973880260442
Validation loss: 2.4809341915115124

Epoch: 6| Step: 8
Training loss: 2.787073959842005
Validation loss: 2.4837428474291285

Epoch: 6| Step: 9
Training loss: 2.6114917324401996
Validation loss: 2.4823133763559078

Epoch: 6| Step: 10
Training loss: 2.4174831808741857
Validation loss: 2.486431901778225

Epoch: 6| Step: 11
Training loss: 2.9206501234407876
Validation loss: 2.4903733238400045

Epoch: 6| Step: 12
Training loss: 2.8595004028170616
Validation loss: 2.4931675927983514

Epoch: 6| Step: 13
Training loss: 2.234851279548422
Validation loss: 2.482413231139062

Epoch: 81| Step: 0
Training loss: 2.3689605820590103
Validation loss: 2.48142076380333

Epoch: 6| Step: 1
Training loss: 3.1661523769432574
Validation loss: 2.4837363679841657

Epoch: 6| Step: 2
Training loss: 2.9788143420904154
Validation loss: 2.492775603514633

Epoch: 6| Step: 3
Training loss: 2.471237474640366
Validation loss: 2.482490176450021

Epoch: 6| Step: 4
Training loss: 1.9601178526966079
Validation loss: 2.4806917345638886

Epoch: 6| Step: 5
Training loss: 2.974091553028571
Validation loss: 2.4871550226533525

Epoch: 6| Step: 6
Training loss: 2.903039943734325
Validation loss: 2.483376468266757

Epoch: 6| Step: 7
Training loss: 2.0604405380218758
Validation loss: 2.479001197315054

Epoch: 6| Step: 8
Training loss: 2.675147656795179
Validation loss: 2.4767154522023267

Epoch: 6| Step: 9
Training loss: 2.46337866428703
Validation loss: 2.481143473172921

Epoch: 6| Step: 10
Training loss: 2.491891873323841
Validation loss: 2.485360499917419

Epoch: 6| Step: 11
Training loss: 2.054754451922792
Validation loss: 2.485573869217193

Epoch: 6| Step: 12
Training loss: 2.6736433635489267
Validation loss: 2.488535598059805

Epoch: 6| Step: 13
Training loss: 2.9033408422478835
Validation loss: 2.485794301746976

Epoch: 82| Step: 0
Training loss: 2.70605318970434
Validation loss: 2.4824587551351716

Epoch: 6| Step: 1
Training loss: 2.544443666697702
Validation loss: 2.4826605537257684

Epoch: 6| Step: 2
Training loss: 2.654754127712617
Validation loss: 2.488459414536882

Epoch: 6| Step: 3
Training loss: 2.7823233087174066
Validation loss: 2.4876337970680136

Epoch: 6| Step: 4
Training loss: 2.421152314680716
Validation loss: 2.491246315787029

Epoch: 6| Step: 5
Training loss: 2.7570234255897224
Validation loss: 2.4917197909693565

Epoch: 6| Step: 6
Training loss: 2.5399549588392563
Validation loss: 2.49195090578928

Epoch: 6| Step: 7
Training loss: 2.812262122373034
Validation loss: 2.488388330699669

Epoch: 6| Step: 8
Training loss: 2.5711264205706885
Validation loss: 2.488196458873974

Epoch: 6| Step: 9
Training loss: 2.5683885231829833
Validation loss: 2.4864876199783956

Epoch: 6| Step: 10
Training loss: 2.5430857547967194
Validation loss: 2.4825661269924186

Epoch: 6| Step: 11
Training loss: 2.3725120161453663
Validation loss: 2.4828818610625536

Epoch: 6| Step: 12
Training loss: 2.7050340559361565
Validation loss: 2.4885521885449986

Epoch: 6| Step: 13
Training loss: 2.467767061044888
Validation loss: 2.480714256171866

Epoch: 83| Step: 0
Training loss: 2.5423982745050107
Validation loss: 2.4941525898374834

Epoch: 6| Step: 1
Training loss: 2.37766297272196
Validation loss: 2.5300511319317254

Epoch: 6| Step: 2
Training loss: 2.8390499301562855
Validation loss: 2.566561078298365

Epoch: 6| Step: 3
Training loss: 2.009539028847032
Validation loss: 2.58840985541415

Epoch: 6| Step: 4
Training loss: 3.1093832811408255
Validation loss: 2.5933864599178094

Epoch: 6| Step: 5
Training loss: 2.745145153661797
Validation loss: 2.5523227086332008

Epoch: 6| Step: 6
Training loss: 3.279159170398279
Validation loss: 2.5431196614307296

Epoch: 6| Step: 7
Training loss: 2.306457977428127
Validation loss: 2.525428356686337

Epoch: 6| Step: 8
Training loss: 2.417889669231349
Validation loss: 2.515115595512679

Epoch: 6| Step: 9
Training loss: 2.2588336047948268
Validation loss: 2.5098132177211716

Epoch: 6| Step: 10
Training loss: 2.8775507350926457
Validation loss: 2.5001319055727147

Epoch: 6| Step: 11
Training loss: 1.7298091552737322
Validation loss: 2.491681197984184

Epoch: 6| Step: 12
Training loss: 3.2792880050784596
Validation loss: 2.4859517530044277

Epoch: 6| Step: 13
Training loss: 2.945974098594114
Validation loss: 2.484971812476613

Epoch: 84| Step: 0
Training loss: 3.0771578900898136
Validation loss: 2.488941841529581

Epoch: 6| Step: 1
Training loss: 2.729821124242636
Validation loss: 2.48059554289108

Epoch: 6| Step: 2
Training loss: 1.9902389271752405
Validation loss: 2.489431294117496

Epoch: 6| Step: 3
Training loss: 2.559415868635468
Validation loss: 2.4980848765322388

Epoch: 6| Step: 4
Training loss: 2.662865960128776
Validation loss: 2.50245190228998

Epoch: 6| Step: 5
Training loss: 2.435218819532472
Validation loss: 2.506265085607775

Epoch: 6| Step: 6
Training loss: 2.9918074005376116
Validation loss: 2.5055129618758896

Epoch: 6| Step: 7
Training loss: 2.181482157335007
Validation loss: 2.5102717145381983

Epoch: 6| Step: 8
Training loss: 2.635055175435211
Validation loss: 2.503679349540177

Epoch: 6| Step: 9
Training loss: 2.580216824935063
Validation loss: 2.507191201767195

Epoch: 6| Step: 10
Training loss: 2.9836429676792
Validation loss: 2.496502464235662

Epoch: 6| Step: 11
Training loss: 2.839276662505357
Validation loss: 2.4925112459215364

Epoch: 6| Step: 12
Training loss: 2.6089005638581955
Validation loss: 2.495253061674682

Epoch: 6| Step: 13
Training loss: 2.2553708294713255
Validation loss: 2.488840676046012

Epoch: 85| Step: 0
Training loss: 2.5059775416044494
Validation loss: 2.4894362024434105

Epoch: 6| Step: 1
Training loss: 2.708940369483064
Validation loss: 2.489258897994892

Epoch: 6| Step: 2
Training loss: 2.469930439019729
Validation loss: 2.4844640240023788

Epoch: 6| Step: 3
Training loss: 2.4309284929235413
Validation loss: 2.4799642855882365

Epoch: 6| Step: 4
Training loss: 2.6652998302193023
Validation loss: 2.4690220839824897

Epoch: 6| Step: 5
Training loss: 2.725357707632523
Validation loss: 2.476935919077395

Epoch: 6| Step: 6
Training loss: 2.9967349086313697
Validation loss: 2.472017307099513

Epoch: 6| Step: 7
Training loss: 2.716149939402582
Validation loss: 2.471440342217011

Epoch: 6| Step: 8
Training loss: 2.11644334841545
Validation loss: 2.4727507016270267

Epoch: 6| Step: 9
Training loss: 2.6955638823555756
Validation loss: 2.47029363281972

Epoch: 6| Step: 10
Training loss: 2.5266875604689947
Validation loss: 2.4702851717211614

Epoch: 6| Step: 11
Training loss: 2.979762802412732
Validation loss: 2.470198628782766

Epoch: 6| Step: 12
Training loss: 2.6017898597688
Validation loss: 2.476416999722479

Epoch: 6| Step: 13
Training loss: 2.1135763642715686
Validation loss: 2.4758202115274734

Epoch: 86| Step: 0
Training loss: 2.9418536450089325
Validation loss: 2.4736088930929574

Epoch: 6| Step: 1
Training loss: 2.273849816965119
Validation loss: 2.4739252884648897

Epoch: 6| Step: 2
Training loss: 2.5788696138541027
Validation loss: 2.4699689857502327

Epoch: 6| Step: 3
Training loss: 2.8413474864825017
Validation loss: 2.474917165095731

Epoch: 6| Step: 4
Training loss: 2.830606120411516
Validation loss: 2.4666450234271244

Epoch: 6| Step: 5
Training loss: 3.1167528810323315
Validation loss: 2.468338815432721

Epoch: 6| Step: 6
Training loss: 2.4208860285457487
Validation loss: 2.4718091979272683

Epoch: 6| Step: 7
Training loss: 1.9863094002620376
Validation loss: 2.47369093135805

Epoch: 6| Step: 8
Training loss: 2.331251259974735
Validation loss: 2.4662582159703055

Epoch: 6| Step: 9
Training loss: 2.6036962058763917
Validation loss: 2.4653665075915487

Epoch: 6| Step: 10
Training loss: 2.6212574755589784
Validation loss: 2.462387561974355

Epoch: 6| Step: 11
Training loss: 2.4194072333169174
Validation loss: 2.4694263481427776

Epoch: 6| Step: 12
Training loss: 2.6769865342586336
Validation loss: 2.473179959102738

Epoch: 6| Step: 13
Training loss: 2.4055320362216794
Validation loss: 2.485708362721578

Epoch: 87| Step: 0
Training loss: 2.358090689030613
Validation loss: 2.4864024160932465

Epoch: 6| Step: 1
Training loss: 2.4064934347298155
Validation loss: 2.4832971982012375

Epoch: 6| Step: 2
Training loss: 2.332066112366352
Validation loss: 2.4814130772730376

Epoch: 6| Step: 3
Training loss: 2.5459295284829597
Validation loss: 2.4859088185540177

Epoch: 6| Step: 4
Training loss: 1.8186487281106665
Validation loss: 2.4924106159208397

Epoch: 6| Step: 5
Training loss: 2.4453998586632
Validation loss: 2.4818590408025987

Epoch: 6| Step: 6
Training loss: 2.3363418484303726
Validation loss: 2.487251152358084

Epoch: 6| Step: 7
Training loss: 2.8904102142542047
Validation loss: 2.483295918082527

Epoch: 6| Step: 8
Training loss: 3.460664002093844
Validation loss: 2.4781136613917547

Epoch: 6| Step: 9
Training loss: 2.804196455534182
Validation loss: 2.4699883875914947

Epoch: 6| Step: 10
Training loss: 2.909230662370208
Validation loss: 2.4723499303209833

Epoch: 6| Step: 11
Training loss: 2.1964998665646123
Validation loss: 2.4756566987467674

Epoch: 6| Step: 12
Training loss: 2.8161031742485387
Validation loss: 2.4848261729411942

Epoch: 6| Step: 13
Training loss: 2.669533936474358
Validation loss: 2.4860184709656075

Epoch: 88| Step: 0
Training loss: 2.277401329608173
Validation loss: 2.4796197354360388

Epoch: 6| Step: 1
Training loss: 2.6878980297490562
Validation loss: 2.474039182178452

Epoch: 6| Step: 2
Training loss: 3.1342407928061364
Validation loss: 2.4795677651316335

Epoch: 6| Step: 3
Training loss: 2.5156045284978377
Validation loss: 2.47559980580435

Epoch: 6| Step: 4
Training loss: 2.442040347341631
Validation loss: 2.476335212233714

Epoch: 6| Step: 5
Training loss: 2.8691253350086123
Validation loss: 2.4762558932798306

Epoch: 6| Step: 6
Training loss: 2.1596531204448026
Validation loss: 2.4760422024681485

Epoch: 6| Step: 7
Training loss: 2.743884482596344
Validation loss: 2.4794395574408097

Epoch: 6| Step: 8
Training loss: 2.6185892934355537
Validation loss: 2.4798233914813017

Epoch: 6| Step: 9
Training loss: 2.0529132333175393
Validation loss: 2.4809059058216314

Epoch: 6| Step: 10
Training loss: 2.303806356319253
Validation loss: 2.486079161336272

Epoch: 6| Step: 11
Training loss: 2.6159707956287974
Validation loss: 2.4830009931454033

Epoch: 6| Step: 12
Training loss: 2.66972505503972
Validation loss: 2.4840702133836388

Epoch: 6| Step: 13
Training loss: 3.0916065535823476
Validation loss: 2.4829161737369723

Epoch: 89| Step: 0
Training loss: 2.56669171515128
Validation loss: 2.4797751831243526

Epoch: 6| Step: 1
Training loss: 2.389672981139283
Validation loss: 2.47948392606493

Epoch: 6| Step: 2
Training loss: 2.7326831924442567
Validation loss: 2.4746336058865723

Epoch: 6| Step: 3
Training loss: 2.5125878048257793
Validation loss: 2.4694767618609443

Epoch: 6| Step: 4
Training loss: 2.2906049407455384
Validation loss: 2.474416884372692

Epoch: 6| Step: 5
Training loss: 2.6468791146454698
Validation loss: 2.46840927734416

Epoch: 6| Step: 6
Training loss: 2.9415904034318747
Validation loss: 2.471072750060613

Epoch: 6| Step: 7
Training loss: 2.092321107571087
Validation loss: 2.476629647388583

Epoch: 6| Step: 8
Training loss: 2.8016693111893582
Validation loss: 2.465543451157244

Epoch: 6| Step: 9
Training loss: 1.8390179054214157
Validation loss: 2.474380687332266

Epoch: 6| Step: 10
Training loss: 2.668495048119516
Validation loss: 2.4783508310125724

Epoch: 6| Step: 11
Training loss: 2.8623305091487103
Validation loss: 2.483573552381816

Epoch: 6| Step: 12
Training loss: 2.5445850585645147
Validation loss: 2.486015929514026

Epoch: 6| Step: 13
Training loss: 3.024308899679183
Validation loss: 2.4923782514798405

Epoch: 90| Step: 0
Training loss: 3.129907645426057
Validation loss: 2.4972084195193216

Epoch: 6| Step: 1
Training loss: 3.166049813539573
Validation loss: 2.481721640730095

Epoch: 6| Step: 2
Training loss: 2.6432717506223113
Validation loss: 2.4751756477322306

Epoch: 6| Step: 3
Training loss: 1.9841479073889619
Validation loss: 2.4866853923280043

Epoch: 6| Step: 4
Training loss: 2.5647641855448646
Validation loss: 2.5215155936666753

Epoch: 6| Step: 5
Training loss: 1.9515720144243027
Validation loss: 2.5500423028965002

Epoch: 6| Step: 6
Training loss: 2.5730700324883977
Validation loss: 2.573553050384094

Epoch: 6| Step: 7
Training loss: 2.409673954763968
Validation loss: 2.5947500851634184

Epoch: 6| Step: 8
Training loss: 2.477335812144824
Validation loss: 2.6188144314774746

Epoch: 6| Step: 9
Training loss: 2.7056042541709586
Validation loss: 2.630276704913033

Epoch: 6| Step: 10
Training loss: 2.877294868428972
Validation loss: 2.623048647427617

Epoch: 6| Step: 11
Training loss: 3.0697539697530245
Validation loss: 2.594280387852144

Epoch: 6| Step: 12
Training loss: 2.7837482333509564
Validation loss: 2.579083921330515

Epoch: 6| Step: 13
Training loss: 2.9624314376687577
Validation loss: 2.5694046504287047

Epoch: 91| Step: 0
Training loss: 2.3751644278376047
Validation loss: 2.561069895821145

Epoch: 6| Step: 1
Training loss: 3.2433976821814916
Validation loss: 2.552911372373568

Epoch: 6| Step: 2
Training loss: 2.7275969283715855
Validation loss: 2.549859153373305

Epoch: 6| Step: 3
Training loss: 2.945383086231433
Validation loss: 2.5323681500610538

Epoch: 6| Step: 4
Training loss: 2.082100872428795
Validation loss: 2.5126325448977456

Epoch: 6| Step: 5
Training loss: 2.5323515014190865
Validation loss: 2.4946110022213417

Epoch: 6| Step: 6
Training loss: 2.753021054681702
Validation loss: 2.4899240896355903

Epoch: 6| Step: 7
Training loss: 2.0561232971299384
Validation loss: 2.4834049498282273

Epoch: 6| Step: 8
Training loss: 2.9014983351375805
Validation loss: 2.477116904794548

Epoch: 6| Step: 9
Training loss: 2.7543595177734903
Validation loss: 2.4660801514549906

Epoch: 6| Step: 10
Training loss: 2.804753294786673
Validation loss: 2.475709032036842

Epoch: 6| Step: 11
Training loss: 2.629941285932055
Validation loss: 2.4803740716121294

Epoch: 6| Step: 12
Training loss: 2.7687219209302065
Validation loss: 2.4848978224227793

Epoch: 6| Step: 13
Training loss: 2.1827243445800955
Validation loss: 2.483890974129767

Epoch: 92| Step: 0
Training loss: 2.657766201147527
Validation loss: 2.4761180619290197

Epoch: 6| Step: 1
Training loss: 2.1228813380582707
Validation loss: 2.477422795457693

Epoch: 6| Step: 2
Training loss: 2.4432854096143726
Validation loss: 2.4717910964557306

Epoch: 6| Step: 3
Training loss: 2.6217662329499967
Validation loss: 2.4744276839638717

Epoch: 6| Step: 4
Training loss: 2.23136250615641
Validation loss: 2.47005550456007

Epoch: 6| Step: 5
Training loss: 2.7512330845300252
Validation loss: 2.4764680573659845

Epoch: 6| Step: 6
Training loss: 2.5061815133571366
Validation loss: 2.4739495099897484

Epoch: 6| Step: 7
Training loss: 2.7346770201508286
Validation loss: 2.470464111346932

Epoch: 6| Step: 8
Training loss: 2.2320298912890726
Validation loss: 2.470071607856757

Epoch: 6| Step: 9
Training loss: 2.7670847170936934
Validation loss: 2.4725835949195645

Epoch: 6| Step: 10
Training loss: 2.7940343997942985
Validation loss: 2.4724566643764336

Epoch: 6| Step: 11
Training loss: 2.731774188616316
Validation loss: 2.4798821183614006

Epoch: 6| Step: 12
Training loss: 3.0620119912718144
Validation loss: 2.4755797256301504

Epoch: 6| Step: 13
Training loss: 2.560936799901712
Validation loss: 2.472788874969954

Epoch: 93| Step: 0
Training loss: 2.5925609206984817
Validation loss: 2.4750003480750458

Epoch: 6| Step: 1
Training loss: 2.3237628393449508
Validation loss: 2.4705167958961813

Epoch: 6| Step: 2
Training loss: 2.135518463933816
Validation loss: 2.4704077904740633

Epoch: 6| Step: 3
Training loss: 2.250352619938151
Validation loss: 2.4678186679927405

Epoch: 6| Step: 4
Training loss: 2.029372301067582
Validation loss: 2.468146866145854

Epoch: 6| Step: 5
Training loss: 3.369719153227886
Validation loss: 2.4671108743704178

Epoch: 6| Step: 6
Training loss: 2.358893945422717
Validation loss: 2.466048392143552

Epoch: 6| Step: 7
Training loss: 2.338858216971315
Validation loss: 2.4689946596621684

Epoch: 6| Step: 8
Training loss: 2.6439707850978937
Validation loss: 2.4651505832962015

Epoch: 6| Step: 9
Training loss: 2.833531896327854
Validation loss: 2.4685097911522833

Epoch: 6| Step: 10
Training loss: 2.847988951704476
Validation loss: 2.463521579609462

Epoch: 6| Step: 11
Training loss: 2.494813020385165
Validation loss: 2.4596604358914522

Epoch: 6| Step: 12
Training loss: 2.968182800719789
Validation loss: 2.468576384930244

Epoch: 6| Step: 13
Training loss: 2.6845918934143027
Validation loss: 2.4721104570430286

Epoch: 94| Step: 0
Training loss: 2.3449903130890424
Validation loss: 2.474929897201784

Epoch: 6| Step: 1
Training loss: 2.5412509348914982
Validation loss: 2.467887896897618

Epoch: 6| Step: 2
Training loss: 3.0193224757021686
Validation loss: 2.4669009821235246

Epoch: 6| Step: 3
Training loss: 2.294577974565228
Validation loss: 2.4638720419230236

Epoch: 6| Step: 4
Training loss: 2.230210270911249
Validation loss: 2.4659012562776867

Epoch: 6| Step: 5
Training loss: 2.515298762995455
Validation loss: 2.4673151763935772

Epoch: 6| Step: 6
Training loss: 2.5694600926744666
Validation loss: 2.4586530951606695

Epoch: 6| Step: 7
Training loss: 2.443260135968026
Validation loss: 2.455465168413768

Epoch: 6| Step: 8
Training loss: 2.6832369327011953
Validation loss: 2.455733255091015

Epoch: 6| Step: 9
Training loss: 3.2355963570894843
Validation loss: 2.4610981560799785

Epoch: 6| Step: 10
Training loss: 2.5736755816838803
Validation loss: 2.463746985064784

Epoch: 6| Step: 11
Training loss: 2.9390134767766556
Validation loss: 2.4636719543544117

Epoch: 6| Step: 12
Training loss: 2.040323035899049
Validation loss: 2.469553072401633

Epoch: 6| Step: 13
Training loss: 2.4976452227919697
Validation loss: 2.469888014397649

Epoch: 95| Step: 0
Training loss: 2.3400827836611264
Validation loss: 2.469191918073939

Epoch: 6| Step: 1
Training loss: 3.0623066120979825
Validation loss: 2.4683547367487626

Epoch: 6| Step: 2
Training loss: 2.8770454428182926
Validation loss: 2.4648563245642996

Epoch: 6| Step: 3
Training loss: 2.5820013324429762
Validation loss: 2.467998164815648

Epoch: 6| Step: 4
Training loss: 2.527354975915708
Validation loss: 2.4691204726624854

Epoch: 6| Step: 5
Training loss: 2.617778290070486
Validation loss: 2.469195796456985

Epoch: 6| Step: 6
Training loss: 2.7126854969357663
Validation loss: 2.4606990315181156

Epoch: 6| Step: 7
Training loss: 2.5291709367777826
Validation loss: 2.462423386648718

Epoch: 6| Step: 8
Training loss: 2.3939988009482485
Validation loss: 2.4673948632863505

Epoch: 6| Step: 9
Training loss: 2.5903168903451226
Validation loss: 2.459320861401485

Epoch: 6| Step: 10
Training loss: 2.97040038161929
Validation loss: 2.460656447735677

Epoch: 6| Step: 11
Training loss: 1.8141265016711354
Validation loss: 2.4695032797394707

Epoch: 6| Step: 12
Training loss: 2.371307514239223
Validation loss: 2.4620480408277228

Epoch: 6| Step: 13
Training loss: 2.3959086751839638
Validation loss: 2.459396582270035

Epoch: 96| Step: 0
Training loss: 2.590682548371523
Validation loss: 2.459003831192058

Epoch: 6| Step: 1
Training loss: 2.048075433153993
Validation loss: 2.4580640052633607

Epoch: 6| Step: 2
Training loss: 2.9665781207925632
Validation loss: 2.4579054303798085

Epoch: 6| Step: 3
Training loss: 3.0355194542176127
Validation loss: 2.460221863694712

Epoch: 6| Step: 4
Training loss: 2.91913267971955
Validation loss: 2.4758315908125637

Epoch: 6| Step: 5
Training loss: 2.9308204189692466
Validation loss: 2.476237278712571

Epoch: 6| Step: 6
Training loss: 2.6617254034865567
Validation loss: 2.481901236781056

Epoch: 6| Step: 7
Training loss: 2.592392347817354
Validation loss: 2.4783741034066193

Epoch: 6| Step: 8
Training loss: 1.992231959426543
Validation loss: 2.457630126938464

Epoch: 6| Step: 9
Training loss: 2.1906056837245176
Validation loss: 2.464658074254701

Epoch: 6| Step: 10
Training loss: 2.459472995536134
Validation loss: 2.4689445418954232

Epoch: 6| Step: 11
Training loss: 1.7540677342488824
Validation loss: 2.4697598190753425

Epoch: 6| Step: 12
Training loss: 2.726425298610807
Validation loss: 2.4705560331322935

Epoch: 6| Step: 13
Training loss: 2.8064680053039837
Validation loss: 2.4711783172033654

Epoch: 97| Step: 0
Training loss: 2.7813305789547402
Validation loss: 2.4773084638190426

Epoch: 6| Step: 1
Training loss: 2.485715492448686
Validation loss: 2.481166174855528

Epoch: 6| Step: 2
Training loss: 2.886318483593401
Validation loss: 2.47922474662216

Epoch: 6| Step: 3
Training loss: 2.391148927410043
Validation loss: 2.487910028909945

Epoch: 6| Step: 4
Training loss: 2.678822013396106
Validation loss: 2.485432877425103

Epoch: 6| Step: 5
Training loss: 2.5590025131195357
Validation loss: 2.4902038652134273

Epoch: 6| Step: 6
Training loss: 2.314488458162522
Validation loss: 2.4918509229152717

Epoch: 6| Step: 7
Training loss: 2.5313127297999674
Validation loss: 2.484050961520956

Epoch: 6| Step: 8
Training loss: 2.6259847337986475
Validation loss: 2.4867878638915957

Epoch: 6| Step: 9
Training loss: 2.946896074205531
Validation loss: 2.484836271637325

Epoch: 6| Step: 10
Training loss: 2.9512190546774746
Validation loss: 2.484725255687727

Epoch: 6| Step: 11
Training loss: 2.5568433525310703
Validation loss: 2.4807101955698894

Epoch: 6| Step: 12
Training loss: 2.1435355565774996
Validation loss: 2.4756679102501296

Epoch: 6| Step: 13
Training loss: 2.368212789447155
Validation loss: 2.480914715102775

Epoch: 98| Step: 0
Training loss: 2.2885982316020734
Validation loss: 2.465428560567918

Epoch: 6| Step: 1
Training loss: 2.966487946288276
Validation loss: 2.46093326951728

Epoch: 6| Step: 2
Training loss: 3.0928631677068603
Validation loss: 2.466609364780498

Epoch: 6| Step: 3
Training loss: 2.8456133093238742
Validation loss: 2.45659502176846

Epoch: 6| Step: 4
Training loss: 2.7100867439829504
Validation loss: 2.4548924066568194

Epoch: 6| Step: 5
Training loss: 2.338040735051416
Validation loss: 2.460337764584914

Epoch: 6| Step: 6
Training loss: 2.6194528369447747
Validation loss: 2.452424291491428

Epoch: 6| Step: 7
Training loss: 2.058463336787645
Validation loss: 2.4564425799818244

Epoch: 6| Step: 8
Training loss: 2.3497832908540843
Validation loss: 2.4626669806136157

Epoch: 6| Step: 9
Training loss: 2.3452705028822267
Validation loss: 2.4651326747212763

Epoch: 6| Step: 10
Training loss: 2.411442492263428
Validation loss: 2.45979639471989

Epoch: 6| Step: 11
Training loss: 2.5973258130997987
Validation loss: 2.456072904950247

Epoch: 6| Step: 12
Training loss: 3.1471738338164275
Validation loss: 2.4584937178023525

Epoch: 6| Step: 13
Training loss: 1.9839745064835659
Validation loss: 2.4630316729974915

Epoch: 99| Step: 0
Training loss: 3.0092074243586038
Validation loss: 2.463248025008904

Epoch: 6| Step: 1
Training loss: 2.479630456279127
Validation loss: 2.464803623813927

Epoch: 6| Step: 2
Training loss: 2.654016610479515
Validation loss: 2.4645061795778993

Epoch: 6| Step: 3
Training loss: 2.3415963005091434
Validation loss: 2.463016693333318

Epoch: 6| Step: 4
Training loss: 3.3999823906386384
Validation loss: 2.4603094197628717

Epoch: 6| Step: 5
Training loss: 2.5482567157625056
Validation loss: 2.4625184808802465

Epoch: 6| Step: 6
Training loss: 2.7885172334972634
Validation loss: 2.4639021117156505

Epoch: 6| Step: 7
Training loss: 2.6436362167655063
Validation loss: 2.465664025317363

Epoch: 6| Step: 8
Training loss: 1.9743856874993597
Validation loss: 2.4619755246112813

Epoch: 6| Step: 9
Training loss: 2.6865233487538607
Validation loss: 2.465632728013376

Epoch: 6| Step: 10
Training loss: 2.0924176204239195
Validation loss: 2.4632391767440533

Epoch: 6| Step: 11
Training loss: 1.8692845017117763
Validation loss: 2.4687075711881774

Epoch: 6| Step: 12
Training loss: 1.9541428012109079
Validation loss: 2.465134568748629

Epoch: 6| Step: 13
Training loss: 3.1923763897056068
Validation loss: 2.4645566778173786

Epoch: 100| Step: 0
Training loss: 2.12770088439771
Validation loss: 2.459419226178083

Epoch: 6| Step: 1
Training loss: 2.4105319282143762
Validation loss: 2.4624984876753913

Epoch: 6| Step: 2
Training loss: 2.523931401815298
Validation loss: 2.461074098701483

Epoch: 6| Step: 3
Training loss: 2.0394826872647958
Validation loss: 2.4596943293729367

Epoch: 6| Step: 4
Training loss: 2.856978616081006
Validation loss: 2.460285483881056

Epoch: 6| Step: 5
Training loss: 3.276232707281078
Validation loss: 2.458634137214952

Epoch: 6| Step: 6
Training loss: 2.6186961821713957
Validation loss: 2.4505592259403794

Epoch: 6| Step: 7
Training loss: 2.4960096939094294
Validation loss: 2.458553406730166

Epoch: 6| Step: 8
Training loss: 2.5736740994852725
Validation loss: 2.4572988019782773

Epoch: 6| Step: 9
Training loss: 2.7215609871911255
Validation loss: 2.456141162560862

Epoch: 6| Step: 10
Training loss: 2.4051505214989963
Validation loss: 2.462146667846832

Epoch: 6| Step: 11
Training loss: 2.5434335497722236
Validation loss: 2.462596427010634

Epoch: 6| Step: 12
Training loss: 2.289576137578855
Validation loss: 2.4692169745739663

Epoch: 6| Step: 13
Training loss: 2.9047815037824782
Validation loss: 2.459389141967907

Epoch: 101| Step: 0
Training loss: 2.8411956046150855
Validation loss: 2.4647065218667095

Epoch: 6| Step: 1
Training loss: 3.157303738085908
Validation loss: 2.461673977564874

Epoch: 6| Step: 2
Training loss: 2.30781978536294
Validation loss: 2.4581067794459464

Epoch: 6| Step: 3
Training loss: 3.119419149958832
Validation loss: 2.455468655815936

Epoch: 6| Step: 4
Training loss: 2.6134494790068716
Validation loss: 2.454399134361275

Epoch: 6| Step: 5
Training loss: 2.7165537765856107
Validation loss: 2.462801224540318

Epoch: 6| Step: 6
Training loss: 2.4227033613607007
Validation loss: 2.458312993585097

Epoch: 6| Step: 7
Training loss: 1.9816885959105959
Validation loss: 2.4604238064818724

Epoch: 6| Step: 8
Training loss: 1.9902543805345738
Validation loss: 2.461065056941381

Epoch: 6| Step: 9
Training loss: 2.6807008834561343
Validation loss: 2.4604554203694913

Epoch: 6| Step: 10
Training loss: 2.569100787117959
Validation loss: 2.464297307094477

Epoch: 6| Step: 11
Training loss: 2.326939735546976
Validation loss: 2.464224349120708

Epoch: 6| Step: 12
Training loss: 2.6454295854067285
Validation loss: 2.4588157913153528

Epoch: 6| Step: 13
Training loss: 2.331134157140607
Validation loss: 2.462413922207704

Epoch: 102| Step: 0
Training loss: 2.507966889474654
Validation loss: 2.4663975287432174

Epoch: 6| Step: 1
Training loss: 1.8805473282692127
Validation loss: 2.4651411857434042

Epoch: 6| Step: 2
Training loss: 2.6785525039730933
Validation loss: 2.4663141200576986

Epoch: 6| Step: 3
Training loss: 2.5794550383720414
Validation loss: 2.4620482183631345

Epoch: 6| Step: 4
Training loss: 2.436794276655443
Validation loss: 2.4661723655446988

Epoch: 6| Step: 5
Training loss: 2.6259447849857667
Validation loss: 2.4600830532413362

Epoch: 6| Step: 6
Training loss: 2.6579623817554583
Validation loss: 2.467623312695756

Epoch: 6| Step: 7
Training loss: 2.696825923893075
Validation loss: 2.466008027740431

Epoch: 6| Step: 8
Training loss: 2.116498321186484
Validation loss: 2.4664729356938984

Epoch: 6| Step: 9
Training loss: 2.669915802731756
Validation loss: 2.464498633775096

Epoch: 6| Step: 10
Training loss: 3.0974829145426757
Validation loss: 2.4637776611932667

Epoch: 6| Step: 11
Training loss: 2.648426427227268
Validation loss: 2.457651113687104

Epoch: 6| Step: 12
Training loss: 2.463587324325573
Validation loss: 2.4536372498311048

Epoch: 6| Step: 13
Training loss: 2.7514076964867233
Validation loss: 2.457532531352214

Epoch: 103| Step: 0
Training loss: 2.1090726282484455
Validation loss: 2.459364308483857

Epoch: 6| Step: 1
Training loss: 2.603684851261285
Validation loss: 2.4506452625451693

Epoch: 6| Step: 2
Training loss: 2.7296101935302532
Validation loss: 2.4574900867343707

Epoch: 6| Step: 3
Training loss: 2.6535825912132247
Validation loss: 2.451871442323039

Epoch: 6| Step: 4
Training loss: 2.757894714686132
Validation loss: 2.455699444480038

Epoch: 6| Step: 5
Training loss: 2.7822306757823227
Validation loss: 2.4687355057174187

Epoch: 6| Step: 6
Training loss: 2.572748114332658
Validation loss: 2.4594071004437628

Epoch: 6| Step: 7
Training loss: 2.2628003632349456
Validation loss: 2.45687359504721

Epoch: 6| Step: 8
Training loss: 2.537862828815719
Validation loss: 2.452431939256829

Epoch: 6| Step: 9
Training loss: 2.1431110731169087
Validation loss: 2.452490560530949

Epoch: 6| Step: 10
Training loss: 2.4214829650569576
Validation loss: 2.456015469368792

Epoch: 6| Step: 11
Training loss: 3.22482526438873
Validation loss: 2.458236164392503

Epoch: 6| Step: 12
Training loss: 2.167819621137801
Validation loss: 2.4570303039063783

Epoch: 6| Step: 13
Training loss: 2.824548641370125
Validation loss: 2.4635073852304483

Epoch: 104| Step: 0
Training loss: 2.866934700197288
Validation loss: 2.4569031116446483

Epoch: 6| Step: 1
Training loss: 2.8860078795835027
Validation loss: 2.4615356364295105

Epoch: 6| Step: 2
Training loss: 2.7393016791273683
Validation loss: 2.4626936686308603

Epoch: 6| Step: 3
Training loss: 1.7517103284239182
Validation loss: 2.4656617287985214

Epoch: 6| Step: 4
Training loss: 2.796960072848536
Validation loss: 2.462582703313407

Epoch: 6| Step: 5
Training loss: 3.1504719032544632
Validation loss: 2.4671647176884535

Epoch: 6| Step: 6
Training loss: 2.4750823883831963
Validation loss: 2.4587698618932343

Epoch: 6| Step: 7
Training loss: 2.7505768257677667
Validation loss: 2.4600967181971773

Epoch: 6| Step: 8
Training loss: 2.194496736292434
Validation loss: 2.4710821572167623

Epoch: 6| Step: 9
Training loss: 2.311307135495208
Validation loss: 2.469284892985938

Epoch: 6| Step: 10
Training loss: 2.001264887414249
Validation loss: 2.4699606844183633

Epoch: 6| Step: 11
Training loss: 2.646087774422597
Validation loss: 2.470628659378894

Epoch: 6| Step: 12
Training loss: 2.676692701603443
Validation loss: 2.474809108141415

Epoch: 6| Step: 13
Training loss: 2.336313478965755
Validation loss: 2.4693285429161738

Epoch: 105| Step: 0
Training loss: 2.760879928774451
Validation loss: 2.472182989404484

Epoch: 6| Step: 1
Training loss: 2.258915087478173
Validation loss: 2.4674339005500796

Epoch: 6| Step: 2
Training loss: 1.8974752617028803
Validation loss: 2.4713951619281724

Epoch: 6| Step: 3
Training loss: 2.472293580081472
Validation loss: 2.475816889215756

Epoch: 6| Step: 4
Training loss: 2.8564137618449856
Validation loss: 2.4655579480883576

Epoch: 6| Step: 5
Training loss: 2.6497223474690244
Validation loss: 2.459623359312705

Epoch: 6| Step: 6
Training loss: 2.6729963303129423
Validation loss: 2.445972113974628

Epoch: 6| Step: 7
Training loss: 3.202148139676717
Validation loss: 2.4543213974167126

Epoch: 6| Step: 8
Training loss: 2.3431402812061615
Validation loss: 2.458448590523981

Epoch: 6| Step: 9
Training loss: 2.7639759556929366
Validation loss: 2.4632388863720154

Epoch: 6| Step: 10
Training loss: 2.1168194552968473
Validation loss: 2.4667239506831526

Epoch: 6| Step: 11
Training loss: 2.3031864773996293
Validation loss: 2.4754251702528647

Epoch: 6| Step: 12
Training loss: 2.0489731147309223
Validation loss: 2.481755649217319

Epoch: 6| Step: 13
Training loss: 3.0766015324942706
Validation loss: 2.468911097296284

Epoch: 106| Step: 0
Training loss: 2.485530464613387
Validation loss: 2.466568204057272

Epoch: 6| Step: 1
Training loss: 2.7500222812096737
Validation loss: 2.483843636618752

Epoch: 6| Step: 2
Training loss: 3.1723668511047523
Validation loss: 2.4721882212964132

Epoch: 6| Step: 3
Training loss: 2.5894476816416803
Validation loss: 2.470646045613513

Epoch: 6| Step: 4
Training loss: 2.3484866281168917
Validation loss: 2.4759216442127667

Epoch: 6| Step: 5
Training loss: 2.299224897059507
Validation loss: 2.4800049996582003

Epoch: 6| Step: 6
Training loss: 2.9812986485892483
Validation loss: 2.4822810243374964

Epoch: 6| Step: 7
Training loss: 2.2134622237965385
Validation loss: 2.480376418590468

Epoch: 6| Step: 8
Training loss: 2.5765361166876164
Validation loss: 2.469483310902326

Epoch: 6| Step: 9
Training loss: 2.332695078976662
Validation loss: 2.4681948910906706

Epoch: 6| Step: 10
Training loss: 3.2735987512024916
Validation loss: 2.467562168398457

Epoch: 6| Step: 11
Training loss: 2.3439387944794174
Validation loss: 2.447165813678767

Epoch: 6| Step: 12
Training loss: 2.1043896572689103
Validation loss: 2.4587615873952298

Epoch: 6| Step: 13
Training loss: 2.2769134271956517
Validation loss: 2.4533961767806898

Epoch: 107| Step: 0
Training loss: 2.851785227817078
Validation loss: 2.4570356489262424

Epoch: 6| Step: 1
Training loss: 2.566830674005376
Validation loss: 2.453420374231554

Epoch: 6| Step: 2
Training loss: 2.599292761640707
Validation loss: 2.45356757024947

Epoch: 6| Step: 3
Training loss: 2.8291283613785096
Validation loss: 2.451525918182334

Epoch: 6| Step: 4
Training loss: 2.487353285100185
Validation loss: 2.466109718990179

Epoch: 6| Step: 5
Training loss: 2.2121830858751563
Validation loss: 2.476282723685749

Epoch: 6| Step: 6
Training loss: 2.333144237031272
Validation loss: 2.47287781793877

Epoch: 6| Step: 7
Training loss: 2.623916402322481
Validation loss: 2.4641731829186737

Epoch: 6| Step: 8
Training loss: 2.524939406017269
Validation loss: 2.4681667813972883

Epoch: 6| Step: 9
Training loss: 2.4494642411483682
Validation loss: 2.4675080197277075

Epoch: 6| Step: 10
Training loss: 2.641127137965707
Validation loss: 2.457530219150007

Epoch: 6| Step: 11
Training loss: 2.020018173929173
Validation loss: 2.463405054241592

Epoch: 6| Step: 12
Training loss: 2.8738180301639846
Validation loss: 2.456722343202974

Epoch: 6| Step: 13
Training loss: 2.597250357445444
Validation loss: 2.4559917990467857

Epoch: 108| Step: 0
Training loss: 2.2270663578063306
Validation loss: 2.454781023943625

Epoch: 6| Step: 1
Training loss: 2.5024371665360428
Validation loss: 2.452927842691176

Epoch: 6| Step: 2
Training loss: 3.0202212711139658
Validation loss: 2.468560529431771

Epoch: 6| Step: 3
Training loss: 2.405301786589432
Validation loss: 2.4631216377984546

Epoch: 6| Step: 4
Training loss: 2.545210873874024
Validation loss: 2.4603676756897244

Epoch: 6| Step: 5
Training loss: 2.5644538687641725
Validation loss: 2.459998138215102

Epoch: 6| Step: 6
Training loss: 3.4148294735245495
Validation loss: 2.4587649408328356

Epoch: 6| Step: 7
Training loss: 2.1543899474525996
Validation loss: 2.459787429035248

Epoch: 6| Step: 8
Training loss: 2.2867498074459847
Validation loss: 2.4650498519148316

Epoch: 6| Step: 9
Training loss: 2.5361932593814465
Validation loss: 2.4666448059487998

Epoch: 6| Step: 10
Training loss: 2.3113551012057036
Validation loss: 2.4618967760047363

Epoch: 6| Step: 11
Training loss: 2.1397161851723183
Validation loss: 2.4696969578181487

Epoch: 6| Step: 12
Training loss: 2.8653683413317648
Validation loss: 2.4660446860582366

Epoch: 6| Step: 13
Training loss: 2.5084926360312867
Validation loss: 2.472059759476295

Epoch: 109| Step: 0
Training loss: 2.3552653919199282
Validation loss: 2.4609954988618514

Epoch: 6| Step: 1
Training loss: 2.6726537093948024
Validation loss: 2.465525440676623

Epoch: 6| Step: 2
Training loss: 2.6783636176013927
Validation loss: 2.4636985427770384

Epoch: 6| Step: 3
Training loss: 2.4478590505326108
Validation loss: 2.459615475419112

Epoch: 6| Step: 4
Training loss: 2.0621057769402924
Validation loss: 2.4589709462593756

Epoch: 6| Step: 5
Training loss: 2.2383168269287532
Validation loss: 2.455511062553652

Epoch: 6| Step: 6
Training loss: 2.7132746491268147
Validation loss: 2.4567301635955827

Epoch: 6| Step: 7
Training loss: 2.5917802248134643
Validation loss: 2.4580361515311546

Epoch: 6| Step: 8
Training loss: 3.390515584982148
Validation loss: 2.4534280431864244

Epoch: 6| Step: 9
Training loss: 2.514286514458591
Validation loss: 2.452164739415064

Epoch: 6| Step: 10
Training loss: 2.6420201334853957
Validation loss: 2.466451693738388

Epoch: 6| Step: 11
Training loss: 2.093434210349182
Validation loss: 2.4523636999308205

Epoch: 6| Step: 12
Training loss: 2.588865806092851
Validation loss: 2.4608925144181173

Epoch: 6| Step: 13
Training loss: 2.456015469368792
Validation loss: 2.4448898161365675

Epoch: 110| Step: 0
Training loss: 3.1152215174107702
Validation loss: 2.450745515863818

Epoch: 6| Step: 1
Training loss: 2.2239187506578535
Validation loss: 2.448201772982393

Epoch: 6| Step: 2
Training loss: 2.5506478664697734
Validation loss: 2.4674456728050207

Epoch: 6| Step: 3
Training loss: 2.7106184977848593
Validation loss: 2.4664564302994925

Epoch: 6| Step: 4
Training loss: 2.58626579667029
Validation loss: 2.4640788862916807

Epoch: 6| Step: 5
Training loss: 2.4274825612314928
Validation loss: 2.4709829541386066

Epoch: 6| Step: 6
Training loss: 2.2599243002847635
Validation loss: 2.4601136539147777

Epoch: 6| Step: 7
Training loss: 2.465492900602215
Validation loss: 2.467654407621063

Epoch: 6| Step: 8
Training loss: 2.396163011065429
Validation loss: 2.472389837600213

Epoch: 6| Step: 9
Training loss: 2.8275734379386215
Validation loss: 2.4679820318892087

Epoch: 6| Step: 10
Training loss: 2.540877787834106
Validation loss: 2.4745869343218594

Epoch: 6| Step: 11
Training loss: 2.836909356170795
Validation loss: 2.4684298665885316

Epoch: 6| Step: 12
Training loss: 1.8966021673077296
Validation loss: 2.464591842168038

Epoch: 6| Step: 13
Training loss: 2.745078538142854
Validation loss: 2.45950592222494

Epoch: 111| Step: 0
Training loss: 2.482763283435601
Validation loss: 2.456374524869325

Epoch: 6| Step: 1
Training loss: 2.2944794703230373
Validation loss: 2.456917926416884

Epoch: 6| Step: 2
Training loss: 2.398371096011743
Validation loss: 2.4560164563013926

Epoch: 6| Step: 3
Training loss: 2.2045074886853246
Validation loss: 2.452866801869648

Epoch: 6| Step: 4
Training loss: 2.6054912606200546
Validation loss: 2.4573233086679402

Epoch: 6| Step: 5
Training loss: 2.561652252646477
Validation loss: 2.4550238871140664

Epoch: 6| Step: 6
Training loss: 3.130306773401845
Validation loss: 2.4600988987682144

Epoch: 6| Step: 7
Training loss: 1.712624332341815
Validation loss: 2.4650169670392827

Epoch: 6| Step: 8
Training loss: 3.2641406906791506
Validation loss: 2.4628721514298166

Epoch: 6| Step: 9
Training loss: 2.635861134398078
Validation loss: 2.477243532391072

Epoch: 6| Step: 10
Training loss: 2.254453701658103
Validation loss: 2.4627345391389928

Epoch: 6| Step: 11
Training loss: 2.772841182591775
Validation loss: 2.448852738851897

Epoch: 6| Step: 12
Training loss: 2.4667407200840645
Validation loss: 2.4484685289290873

Epoch: 6| Step: 13
Training loss: 2.501326685792241
Validation loss: 2.45712590603827

Epoch: 112| Step: 0
Training loss: 1.853237440741074
Validation loss: 2.4496589196159158

Epoch: 6| Step: 1
Training loss: 3.1057263921230236
Validation loss: 2.4593996924873807

Epoch: 6| Step: 2
Training loss: 2.2450063179453963
Validation loss: 2.467286654044791

Epoch: 6| Step: 3
Training loss: 2.4722012889228218
Validation loss: 2.45998679876016

Epoch: 6| Step: 4
Training loss: 2.5496317915517634
Validation loss: 2.4583726869008684

Epoch: 6| Step: 5
Training loss: 1.9650367353854188
Validation loss: 2.453185273096173

Epoch: 6| Step: 6
Training loss: 2.5314374312719687
Validation loss: 2.453993269057449

Epoch: 6| Step: 7
Training loss: 3.0415715938672423
Validation loss: 2.4451602973736293

Epoch: 6| Step: 8
Training loss: 2.5131109718931155
Validation loss: 2.4435170562629693

Epoch: 6| Step: 9
Training loss: 2.856688691554707
Validation loss: 2.454233125802526

Epoch: 6| Step: 10
Training loss: 2.489927026070003
Validation loss: 2.456665424228425

Epoch: 6| Step: 11
Training loss: 2.7237371545762703
Validation loss: 2.4652377064154045

Epoch: 6| Step: 12
Training loss: 2.010871664420443
Validation loss: 2.468336988257931

Epoch: 6| Step: 13
Training loss: 2.894014752596232
Validation loss: 2.466592417278227

Epoch: 113| Step: 0
Training loss: 2.588269245593106
Validation loss: 2.470464730603998

Epoch: 6| Step: 1
Training loss: 2.2925891984498925
Validation loss: 2.4707877525341

Epoch: 6| Step: 2
Training loss: 2.3398268353546596
Validation loss: 2.467759766746488

Epoch: 6| Step: 3
Training loss: 2.6749752863517773
Validation loss: 2.4621261713115836

Epoch: 6| Step: 4
Training loss: 2.8293621244784317
Validation loss: 2.461327641962373

Epoch: 6| Step: 5
Training loss: 2.557917707979179
Validation loss: 2.4621602406554017

Epoch: 6| Step: 6
Training loss: 2.1684895207857964
Validation loss: 2.4674680415450867

Epoch: 6| Step: 7
Training loss: 2.5739035521950306
Validation loss: 2.4574670127240594

Epoch: 6| Step: 8
Training loss: 2.887152653399325
Validation loss: 2.454225216497963

Epoch: 6| Step: 9
Training loss: 2.743157109394
Validation loss: 2.4649465693627586

Epoch: 6| Step: 10
Training loss: 2.518577785615291
Validation loss: 2.463566210720607

Epoch: 6| Step: 11
Training loss: 2.089833824633265
Validation loss: 2.453714061078085

Epoch: 6| Step: 12
Training loss: 2.627982125951638
Validation loss: 2.457743158997241

Epoch: 6| Step: 13
Training loss: 2.6315313555780833
Validation loss: 2.4646183481144885

Epoch: 114| Step: 0
Training loss: 2.755830911966542
Validation loss: 2.4616059863704853

Epoch: 6| Step: 1
Training loss: 2.393984360340249
Validation loss: 2.4646098836552706

Epoch: 6| Step: 2
Training loss: 2.3998542105581318
Validation loss: 2.470398670280466

Epoch: 6| Step: 3
Training loss: 2.98979629307491
Validation loss: 2.470018712718057

Epoch: 6| Step: 4
Training loss: 2.8834212986164443
Validation loss: 2.472142604612625

Epoch: 6| Step: 5
Training loss: 2.7122518997291216
Validation loss: 2.4654455160705546

Epoch: 6| Step: 6
Training loss: 2.512630030365519
Validation loss: 2.471267302058124

Epoch: 6| Step: 7
Training loss: 2.454534040931855
Validation loss: 2.4733967889972566

Epoch: 6| Step: 8
Training loss: 2.5837241666745325
Validation loss: 2.4734563110690133

Epoch: 6| Step: 9
Training loss: 2.443067208438755
Validation loss: 2.4695471349871245

Epoch: 6| Step: 10
Training loss: 2.012873701383106
Validation loss: 2.473552089598411

Epoch: 6| Step: 11
Training loss: 2.7396169933576826
Validation loss: 2.468378723168511

Epoch: 6| Step: 12
Training loss: 2.9045602131369703
Validation loss: 2.469966829980685

Epoch: 6| Step: 13
Training loss: 1.846365221543748
Validation loss: 2.478107680348007

Epoch: 115| Step: 0
Training loss: 2.4392354606968447
Validation loss: 2.4625088635073733

Epoch: 6| Step: 1
Training loss: 1.7632630340081452
Validation loss: 2.467689141364859

Epoch: 6| Step: 2
Training loss: 2.6405258611544205
Validation loss: 2.478085680245191

Epoch: 6| Step: 3
Training loss: 2.411029774846282
Validation loss: 2.471571480964668

Epoch: 6| Step: 4
Training loss: 2.565147613828725
Validation loss: 2.470187432671842

Epoch: 6| Step: 5
Training loss: 2.8959591698692515
Validation loss: 2.4633196568743907

Epoch: 6| Step: 6
Training loss: 2.4381443052449217
Validation loss: 2.468570380778843

Epoch: 6| Step: 7
Training loss: 2.572756732700929
Validation loss: 2.4604880916640877

Epoch: 6| Step: 8
Training loss: 2.5361408972363435
Validation loss: 2.473669968245794

Epoch: 6| Step: 9
Training loss: 2.514622458907071
Validation loss: 2.4750345613012397

Epoch: 6| Step: 10
Training loss: 2.320641491104737
Validation loss: 2.4693100209570686

Epoch: 6| Step: 11
Training loss: 2.7231569199248633
Validation loss: 2.4601300968747757

Epoch: 6| Step: 12
Training loss: 2.451116432079643
Validation loss: 2.461625793115977

Epoch: 6| Step: 13
Training loss: 3.093143596365881
Validation loss: 2.4600705834990846

Epoch: 116| Step: 0
Training loss: 3.0585451085691613
Validation loss: 2.456963065636125

Epoch: 6| Step: 1
Training loss: 2.616794021406723
Validation loss: 2.4658960352182997

Epoch: 6| Step: 2
Training loss: 2.4555641245608557
Validation loss: 2.4650368995174423

Epoch: 6| Step: 3
Training loss: 2.0093888206659227
Validation loss: 2.4670036029951725

Epoch: 6| Step: 4
Training loss: 2.6366785117010685
Validation loss: 2.463496255489699

Epoch: 6| Step: 5
Training loss: 2.5784687159905664
Validation loss: 2.45897442060965

Epoch: 6| Step: 6
Training loss: 2.300920725093871
Validation loss: 2.4641367305846065

Epoch: 6| Step: 7
Training loss: 2.479739969680334
Validation loss: 2.4546903244769616

Epoch: 6| Step: 8
Training loss: 2.58679738111008
Validation loss: 2.4616490137796943

Epoch: 6| Step: 9
Training loss: 3.0715332710442573
Validation loss: 2.463802522770076

Epoch: 6| Step: 10
Training loss: 1.9939215798525183
Validation loss: 2.45996529888086

Epoch: 6| Step: 11
Training loss: 2.013406878553353
Validation loss: 2.44989849516089

Epoch: 6| Step: 12
Training loss: 2.5794465348140783
Validation loss: 2.4570250316514906

Epoch: 6| Step: 13
Training loss: 2.9058586595454656
Validation loss: 2.4530159992857925

Epoch: 117| Step: 0
Training loss: 2.392979976616509
Validation loss: 2.4592828102395927

Epoch: 6| Step: 1
Training loss: 2.4896234697966486
Validation loss: 2.4586803277595353

Epoch: 6| Step: 2
Training loss: 2.8098922507917576
Validation loss: 2.462836139670214

Epoch: 6| Step: 3
Training loss: 2.309581667849992
Validation loss: 2.4690379365169832

Epoch: 6| Step: 4
Training loss: 2.0005025232323463
Validation loss: 2.4665229506338306

Epoch: 6| Step: 5
Training loss: 2.8701352965467324
Validation loss: 2.4786983870511006

Epoch: 6| Step: 6
Training loss: 2.7131589203467024
Validation loss: 2.477810710589287

Epoch: 6| Step: 7
Training loss: 2.3825655449640424
Validation loss: 2.482012507122044

Epoch: 6| Step: 8
Training loss: 2.5042535835538278
Validation loss: 2.4773496546134086

Epoch: 6| Step: 9
Training loss: 2.255371675162974
Validation loss: 2.4757359165203376

Epoch: 6| Step: 10
Training loss: 2.6947613581095906
Validation loss: 2.4777123141681106

Epoch: 6| Step: 11
Training loss: 2.7954402205670426
Validation loss: 2.47764589377333

Epoch: 6| Step: 12
Training loss: 3.3598592719625295
Validation loss: 2.478405047415734

Epoch: 6| Step: 13
Training loss: 2.0694859420177236
Validation loss: 2.4705811562167366

Epoch: 118| Step: 0
Training loss: 2.571000862124405
Validation loss: 2.472693203473683

Epoch: 6| Step: 1
Training loss: 2.321330145960589
Validation loss: 2.471039310144696

Epoch: 6| Step: 2
Training loss: 2.2890693742564228
Validation loss: 2.4606571421301524

Epoch: 6| Step: 3
Training loss: 3.1029544656690704
Validation loss: 2.4765678991345137

Epoch: 6| Step: 4
Training loss: 2.507482869478625
Validation loss: 2.4809408864917026

Epoch: 6| Step: 5
Training loss: 2.4102463666739933
Validation loss: 2.470277997458995

Epoch: 6| Step: 6
Training loss: 2.640161417158956
Validation loss: 2.477603585226651

Epoch: 6| Step: 7
Training loss: 2.627219578726481
Validation loss: 2.4672231016515633

Epoch: 6| Step: 8
Training loss: 2.7630244382977973
Validation loss: 2.474180397641437

Epoch: 6| Step: 9
Training loss: 2.8743595570951723
Validation loss: 2.46312641302825

Epoch: 6| Step: 10
Training loss: 1.8746639586361733
Validation loss: 2.465058242323862

Epoch: 6| Step: 11
Training loss: 2.1697201364648517
Validation loss: 2.4635281928717547

Epoch: 6| Step: 12
Training loss: 2.610426456791288
Validation loss: 2.4672622865821974

Epoch: 6| Step: 13
Training loss: 2.724602199531322
Validation loss: 2.47058385025832

Epoch: 119| Step: 0
Training loss: 2.437897674203176
Validation loss: 2.4685222826528865

Epoch: 6| Step: 1
Training loss: 2.9406388739523104
Validation loss: 2.468505605841552

Epoch: 6| Step: 2
Training loss: 2.440391097540281
Validation loss: 2.467362009664994

Epoch: 6| Step: 3
Training loss: 2.329852800753977
Validation loss: 2.4589408000047417

Epoch: 6| Step: 4
Training loss: 2.918719704448299
Validation loss: 2.4613925735099165

Epoch: 6| Step: 5
Training loss: 2.475009274706176
Validation loss: 2.4611368653614942

Epoch: 6| Step: 6
Training loss: 2.448907324355593
Validation loss: 2.4569157268493607

Epoch: 6| Step: 7
Training loss: 2.8435554857067533
Validation loss: 2.4568111319057553

Epoch: 6| Step: 8
Training loss: 2.5418937039724776
Validation loss: 2.459949210193116

Epoch: 6| Step: 9
Training loss: 2.365741955626258
Validation loss: 2.4558180101544624

Epoch: 6| Step: 10
Training loss: 1.9102860545108404
Validation loss: 2.460904043464708

Epoch: 6| Step: 11
Training loss: 2.6298913342904076
Validation loss: 2.4589522816424836

Epoch: 6| Step: 12
Training loss: 2.1938367342852154
Validation loss: 2.461097155039388

Epoch: 6| Step: 13
Training loss: 2.6506502253459368
Validation loss: 2.474109240831584

Epoch: 120| Step: 0
Training loss: 2.799863273824602
Validation loss: 2.4700140795132675

Epoch: 6| Step: 1
Training loss: 2.331513569431793
Validation loss: 2.46231539465143

Epoch: 6| Step: 2
Training loss: 2.8991780530313607
Validation loss: 2.4614750673616093

Epoch: 6| Step: 3
Training loss: 2.117129617160209
Validation loss: 2.4685877654130666

Epoch: 6| Step: 4
Training loss: 2.594825797288505
Validation loss: 2.465033474012042

Epoch: 6| Step: 5
Training loss: 1.8856354124637091
Validation loss: 2.4524055932527022

Epoch: 6| Step: 6
Training loss: 2.639454057192839
Validation loss: 2.4595343166970607

Epoch: 6| Step: 7
Training loss: 2.4480914330829
Validation loss: 2.4545179004546167

Epoch: 6| Step: 8
Training loss: 2.181552103102799
Validation loss: 2.4526347745674655

Epoch: 6| Step: 9
Training loss: 2.7745898794278476
Validation loss: 2.45415929384101

Epoch: 6| Step: 10
Training loss: 2.6741982336926178
Validation loss: 2.4550354275345248

Epoch: 6| Step: 11
Training loss: 2.857236635848929
Validation loss: 2.454199407863206

Epoch: 6| Step: 12
Training loss: 2.1863612889645134
Validation loss: 2.457761412435377

Epoch: 6| Step: 13
Training loss: 2.7702679642885855
Validation loss: 2.465282967312743

Epoch: 121| Step: 0
Training loss: 3.5211501721960468
Validation loss: 2.4498829567128304

Epoch: 6| Step: 1
Training loss: 1.8749065375875995
Validation loss: 2.463284458231306

Epoch: 6| Step: 2
Training loss: 2.809702096128349
Validation loss: 2.470799058501698

Epoch: 6| Step: 3
Training loss: 2.3293091495541716
Validation loss: 2.460198233843716

Epoch: 6| Step: 4
Training loss: 2.184408564712172
Validation loss: 2.454237602603331

Epoch: 6| Step: 5
Training loss: 2.140740941381439
Validation loss: 2.4589380608869025

Epoch: 6| Step: 6
Training loss: 2.216229793931965
Validation loss: 2.468322362761638

Epoch: 6| Step: 7
Training loss: 2.371720107176365
Validation loss: 2.4615889882784545

Epoch: 6| Step: 8
Training loss: 3.130984260414741
Validation loss: 2.459591468161261

Epoch: 6| Step: 9
Training loss: 2.034365568124497
Validation loss: 2.462110903702788

Epoch: 6| Step: 10
Training loss: 2.7861114190928564
Validation loss: 2.4631175078625174

Epoch: 6| Step: 11
Training loss: 2.8152272882394844
Validation loss: 2.4736772049571147

Epoch: 6| Step: 12
Training loss: 2.1785076051605117
Validation loss: 2.464198024379517

Epoch: 6| Step: 13
Training loss: 2.218376987963019
Validation loss: 2.468912256115766

Epoch: 122| Step: 0
Training loss: 2.4603781412603922
Validation loss: 2.4606501497303266

Epoch: 6| Step: 1
Training loss: 2.2662420024614987
Validation loss: 2.462108466686703

Epoch: 6| Step: 2
Training loss: 2.264057748603969
Validation loss: 2.4691009915806093

Epoch: 6| Step: 3
Training loss: 2.3948191832136483
Validation loss: 2.469631263118051

Epoch: 6| Step: 4
Training loss: 2.605371933815436
Validation loss: 2.463561533121301

Epoch: 6| Step: 5
Training loss: 2.547604509031187
Validation loss: 2.4690224541443544

Epoch: 6| Step: 6
Training loss: 2.9890273016827864
Validation loss: 2.473800836201073

Epoch: 6| Step: 7
Training loss: 2.61859093230771
Validation loss: 2.4590949371263022

Epoch: 6| Step: 8
Training loss: 1.8654656551433504
Validation loss: 2.463048927328134

Epoch: 6| Step: 9
Training loss: 2.348499419613678
Validation loss: 2.4614038096257427

Epoch: 6| Step: 10
Training loss: 2.3543567974540345
Validation loss: 2.4584258487771704

Epoch: 6| Step: 11
Training loss: 2.9933507665057117
Validation loss: 2.4668848903455487

Epoch: 6| Step: 12
Training loss: 2.1978373822078763
Validation loss: 2.4694068131537374

Epoch: 6| Step: 13
Training loss: 2.9273209843031567
Validation loss: 2.4682933529902833

Epoch: 123| Step: 0
Training loss: 2.702078732555495
Validation loss: 2.466717813150575

Epoch: 6| Step: 1
Training loss: 2.660872711629381
Validation loss: 2.474447315813734

Epoch: 6| Step: 2
Training loss: 2.6346630984210098
Validation loss: 2.468467937725639

Epoch: 6| Step: 3
Training loss: 2.7531862007465904
Validation loss: 2.4569159532755203

Epoch: 6| Step: 4
Training loss: 2.649914175119271
Validation loss: 2.463070190563265

Epoch: 6| Step: 5
Training loss: 2.728843019431665
Validation loss: 2.459027238209145

Epoch: 6| Step: 6
Training loss: 2.264873090476991
Validation loss: 2.458439797720024

Epoch: 6| Step: 7
Training loss: 2.2479654226121375
Validation loss: 2.4617524346803523

Epoch: 6| Step: 8
Training loss: 2.650102030391354
Validation loss: 2.45779293931217

Epoch: 6| Step: 9
Training loss: 1.8445217650968213
Validation loss: 2.4483667054659946

Epoch: 6| Step: 10
Training loss: 2.469382096504629
Validation loss: 2.4622223421592926

Epoch: 6| Step: 11
Training loss: 2.463315753108342
Validation loss: 2.460181694428159

Epoch: 6| Step: 12
Training loss: 2.3354182351952444
Validation loss: 2.462602631290744

Epoch: 6| Step: 13
Training loss: 2.597697000972651
Validation loss: 2.4525314552501243

Epoch: 124| Step: 0
Training loss: 2.6018300878243767
Validation loss: 2.464697960976921

Epoch: 6| Step: 1
Training loss: 2.726827876339355
Validation loss: 2.461383855798508

Epoch: 6| Step: 2
Training loss: 2.6157506841910036
Validation loss: 2.4664495993377202

Epoch: 6| Step: 3
Training loss: 2.3792263373147926
Validation loss: 2.463267536248159

Epoch: 6| Step: 4
Training loss: 2.735674478468214
Validation loss: 2.4675609203755573

Epoch: 6| Step: 5
Training loss: 2.322406478986333
Validation loss: 2.4642494964439132

Epoch: 6| Step: 6
Training loss: 2.6712033503970383
Validation loss: 2.4595283712529334

Epoch: 6| Step: 7
Training loss: 2.5761685432834787
Validation loss: 2.465233580024457

Epoch: 6| Step: 8
Training loss: 2.39708022690038
Validation loss: 2.461656172834829

Epoch: 6| Step: 9
Training loss: 2.320990569326035
Validation loss: 2.462226941610665

Epoch: 6| Step: 10
Training loss: 2.176903591006582
Validation loss: 2.4694284480644435

Epoch: 6| Step: 11
Training loss: 3.229625661079867
Validation loss: 2.4616819517177193

Epoch: 6| Step: 12
Training loss: 2.007672612998416
Validation loss: 2.477705891121863

Epoch: 6| Step: 13
Training loss: 2.1110722248221463
Validation loss: 2.475558601945768

Epoch: 125| Step: 0
Training loss: 2.4357557658389872
Validation loss: 2.495403116643448

Epoch: 6| Step: 1
Training loss: 2.8070106342088414
Validation loss: 2.508016671082222

Epoch: 6| Step: 2
Training loss: 2.9849292180600555
Validation loss: 2.492243512041385

Epoch: 6| Step: 3
Training loss: 2.8563456513062855
Validation loss: 2.4912185459476865

Epoch: 6| Step: 4
Training loss: 2.996464235160776
Validation loss: 2.4861657428481028

Epoch: 6| Step: 5
Training loss: 1.7592644692319648
Validation loss: 2.4715363757243645

Epoch: 6| Step: 6
Training loss: 2.4133360921025533
Validation loss: 2.4679797938801054

Epoch: 6| Step: 7
Training loss: 2.1581989271028177
Validation loss: 2.4724582554681387

Epoch: 6| Step: 8
Training loss: 2.2536331090589217
Validation loss: 2.4700856036437284

Epoch: 6| Step: 9
Training loss: 2.9281356567058614
Validation loss: 2.474320930544773

Epoch: 6| Step: 10
Training loss: 2.205989301370251
Validation loss: 2.468789176791026

Epoch: 6| Step: 11
Training loss: 2.4192758703162265
Validation loss: 2.4739802523528724

Epoch: 6| Step: 12
Training loss: 2.3178668250138434
Validation loss: 2.4711157332359206

Epoch: 6| Step: 13
Training loss: 2.2704040634898517
Validation loss: 2.4639703217727575

Epoch: 126| Step: 0
Training loss: 3.003926886002753
Validation loss: 2.469828712017152

Epoch: 6| Step: 1
Training loss: 2.3955995749724948
Validation loss: 2.463834907762789

Epoch: 6| Step: 2
Training loss: 2.698419521605434
Validation loss: 2.4606206054105653

Epoch: 6| Step: 3
Training loss: 2.592562116211686
Validation loss: 2.4740013252672095

Epoch: 6| Step: 4
Training loss: 3.2161264931204085
Validation loss: 2.4698099846538017

Epoch: 6| Step: 5
Training loss: 2.116868899539056
Validation loss: 2.4703288441904925

Epoch: 6| Step: 6
Training loss: 2.4961865904829383
Validation loss: 2.468146383154737

Epoch: 6| Step: 7
Training loss: 2.2758784348348846
Validation loss: 2.4653817389212906

Epoch: 6| Step: 8
Training loss: 1.7717542684553964
Validation loss: 2.4642671856864813

Epoch: 6| Step: 9
Training loss: 2.9061469603289676
Validation loss: 2.464546326729994

Epoch: 6| Step: 10
Training loss: 2.0266419006181295
Validation loss: 2.4618358284386646

Epoch: 6| Step: 11
Training loss: 2.271609485752716
Validation loss: 2.4572489227117655

Epoch: 6| Step: 12
Training loss: 2.2697830454461445
Validation loss: 2.4640809988336563

Epoch: 6| Step: 13
Training loss: 2.62426038949223
Validation loss: 2.479278487133361

Epoch: 127| Step: 0
Training loss: 2.4402162138713597
Validation loss: 2.4725192385093155

Epoch: 6| Step: 1
Training loss: 2.569282858868309
Validation loss: 2.499768151022892

Epoch: 6| Step: 2
Training loss: 1.875160973314634
Validation loss: 2.4865915419128366

Epoch: 6| Step: 3
Training loss: 2.8509756660185617
Validation loss: 2.4867139398574447

Epoch: 6| Step: 4
Training loss: 2.6754755221384046
Validation loss: 2.4771316788823934

Epoch: 6| Step: 5
Training loss: 2.5350170596901163
Validation loss: 2.4699347184443705

Epoch: 6| Step: 6
Training loss: 2.7202579218929026
Validation loss: 2.4705075956834026

Epoch: 6| Step: 7
Training loss: 2.3338434025156487
Validation loss: 2.4650946084928043

Epoch: 6| Step: 8
Training loss: 2.7605122003878177
Validation loss: 2.462165033888589

Epoch: 6| Step: 9
Training loss: 2.024366246646472
Validation loss: 2.4565757729560342

Epoch: 6| Step: 10
Training loss: 2.641215241506328
Validation loss: 2.463946018275842

Epoch: 6| Step: 11
Training loss: 2.953077850772751
Validation loss: 2.4666720469734007

Epoch: 6| Step: 12
Training loss: 2.255749456196512
Validation loss: 2.463369872949658

Epoch: 6| Step: 13
Training loss: 2.366671968731739
Validation loss: 2.4672337636219805

Epoch: 128| Step: 0
Training loss: 2.7711596954044495
Validation loss: 2.4618764387960845

Epoch: 6| Step: 1
Training loss: 2.861107306699065
Validation loss: 2.462746124102404

Epoch: 6| Step: 2
Training loss: 2.0863741871217023
Validation loss: 2.472001570118814

Epoch: 6| Step: 3
Training loss: 2.6200453001576003
Validation loss: 2.4736064834689473

Epoch: 6| Step: 4
Training loss: 2.79172309894365
Validation loss: 2.463385052101476

Epoch: 6| Step: 5
Training loss: 2.286535437000044
Validation loss: 2.466724965549273

Epoch: 6| Step: 6
Training loss: 2.7055800209880103
Validation loss: 2.47634419823861

Epoch: 6| Step: 7
Training loss: 2.6915568405121983
Validation loss: 2.492010128202721

Epoch: 6| Step: 8
Training loss: 2.3138682467407574
Validation loss: 2.488584954029138

Epoch: 6| Step: 9
Training loss: 2.2656106882629867
Validation loss: 2.47942795431766

Epoch: 6| Step: 10
Training loss: 2.1780324689128094
Validation loss: 2.473564587764288

Epoch: 6| Step: 11
Training loss: 2.5978998285118564
Validation loss: 2.4675212490219405

Epoch: 6| Step: 12
Training loss: 2.307629379001333
Validation loss: 2.466076018417063

Epoch: 6| Step: 13
Training loss: 2.2937423207001157
Validation loss: 2.459220779928382

Epoch: 129| Step: 0
Training loss: 2.3449302244887686
Validation loss: 2.4596125674191938

Epoch: 6| Step: 1
Training loss: 2.4343735622651503
Validation loss: 2.4611461248220188

Epoch: 6| Step: 2
Training loss: 2.2630736619572462
Validation loss: 2.4682214549396884

Epoch: 6| Step: 3
Training loss: 2.205626129902447
Validation loss: 2.454070101545297

Epoch: 6| Step: 4
Training loss: 2.7690248443381673
Validation loss: 2.4667148651964275

Epoch: 6| Step: 5
Training loss: 2.5150344818966985
Validation loss: 2.4632899913235504

Epoch: 6| Step: 6
Training loss: 2.6030442425678957
Validation loss: 2.467957639090578

Epoch: 6| Step: 7
Training loss: 2.6738814466355123
Validation loss: 2.465866110599857

Epoch: 6| Step: 8
Training loss: 2.5404702377573063
Validation loss: 2.470060652478519

Epoch: 6| Step: 9
Training loss: 2.946490065027898
Validation loss: 2.4728627131253154

Epoch: 6| Step: 10
Training loss: 1.8783922662208605
Validation loss: 2.464762505483561

Epoch: 6| Step: 11
Training loss: 2.357247612429575
Validation loss: 2.4693627864503744

Epoch: 6| Step: 12
Training loss: 2.3186959437729624
Validation loss: 2.4731143729908904

Epoch: 6| Step: 13
Training loss: 2.9306048368515727
Validation loss: 2.470653379640639

Epoch: 130| Step: 0
Training loss: 2.8239560249081843
Validation loss: 2.480882296793915

Epoch: 6| Step: 1
Training loss: 2.890860883006559
Validation loss: 2.486235220053118

Epoch: 6| Step: 2
Training loss: 2.53102525254896
Validation loss: 2.489994036477134

Epoch: 6| Step: 3
Training loss: 2.2348530931430166
Validation loss: 2.4934832673287475

Epoch: 6| Step: 4
Training loss: 2.585790900831834
Validation loss: 2.484619112637309

Epoch: 6| Step: 5
Training loss: 2.1977783689936317
Validation loss: 2.4751343565413815

Epoch: 6| Step: 6
Training loss: 2.4287827944737943
Validation loss: 2.47813407379965

Epoch: 6| Step: 7
Training loss: 2.972127980276108
Validation loss: 2.4779490731942984

Epoch: 6| Step: 8
Training loss: 2.871096075147868
Validation loss: 2.472719052096249

Epoch: 6| Step: 9
Training loss: 2.1938505361620035
Validation loss: 2.4704730141576188

Epoch: 6| Step: 10
Training loss: 1.939661297185589
Validation loss: 2.46382960974903

Epoch: 6| Step: 11
Training loss: 2.2252813568528773
Validation loss: 2.4668994035553538

Epoch: 6| Step: 12
Training loss: 2.343224937116903
Validation loss: 2.462298022215012

Epoch: 6| Step: 13
Training loss: 2.68850609333241
Validation loss: 2.4684568142532726

Epoch: 131| Step: 0
Training loss: 2.3519702443816213
Validation loss: 2.474063322282264

Epoch: 6| Step: 1
Training loss: 2.2794123524732033
Validation loss: 2.464217778040133

Epoch: 6| Step: 2
Training loss: 2.7901809971390126
Validation loss: 2.4659179668162894

Epoch: 6| Step: 3
Training loss: 1.8539880245031846
Validation loss: 2.4670935115422785

Epoch: 6| Step: 4
Training loss: 2.751547897967315
Validation loss: 2.476857966979636

Epoch: 6| Step: 5
Training loss: 2.807912518434583
Validation loss: 2.468108113858123

Epoch: 6| Step: 6
Training loss: 2.4100712741110306
Validation loss: 2.465845822243253

Epoch: 6| Step: 7
Training loss: 2.7649380105435344
Validation loss: 2.465309087050033

Epoch: 6| Step: 8
Training loss: 3.0306971724261973
Validation loss: 2.476747820766624

Epoch: 6| Step: 9
Training loss: 2.078926104107092
Validation loss: 2.488725543717079

Epoch: 6| Step: 10
Training loss: 2.0426465103102798
Validation loss: 2.499389494421228

Epoch: 6| Step: 11
Training loss: 2.7177048351362316
Validation loss: 2.5278906960005023

Epoch: 6| Step: 12
Training loss: 3.1581333697653724
Validation loss: 2.533551994306654

Epoch: 6| Step: 13
Training loss: 2.2704641292840817
Validation loss: 2.485847228861059

Epoch: 132| Step: 0
Training loss: 2.5193942247138468
Validation loss: 2.486147458209179

Epoch: 6| Step: 1
Training loss: 2.0880223973998597
Validation loss: 2.4722680082855657

Epoch: 6| Step: 2
Training loss: 2.4030483794182054
Validation loss: 2.4697858351551867

Epoch: 6| Step: 3
Training loss: 2.854879211436037
Validation loss: 2.4657071833578494

Epoch: 6| Step: 4
Training loss: 2.6218513496796296
Validation loss: 2.4637258566994835

Epoch: 6| Step: 5
Training loss: 2.5627380818741194
Validation loss: 2.462423257551728

Epoch: 6| Step: 6
Training loss: 2.5345430029183915
Validation loss: 2.464958224532577

Epoch: 6| Step: 7
Training loss: 2.362484329413864
Validation loss: 2.4806809702522963

Epoch: 6| Step: 8
Training loss: 2.792902762960657
Validation loss: 2.4776022700896645

Epoch: 6| Step: 9
Training loss: 2.6564575114256517
Validation loss: 2.489724739134249

Epoch: 6| Step: 10
Training loss: 2.4756215711809206
Validation loss: 2.4747520271177614

Epoch: 6| Step: 11
Training loss: 2.563048234304866
Validation loss: 2.482415600200239

Epoch: 6| Step: 12
Training loss: 2.3175143957256776
Validation loss: 2.4645703663381413

Epoch: 6| Step: 13
Training loss: 2.220418383377604
Validation loss: 2.4796234532822403

Epoch: 133| Step: 0
Training loss: 2.4633074293551167
Validation loss: 2.489056005986282

Epoch: 6| Step: 1
Training loss: 2.4622349785261233
Validation loss: 2.480150689986987

Epoch: 6| Step: 2
Training loss: 2.8129197549179077
Validation loss: 2.487318394611574

Epoch: 6| Step: 3
Training loss: 2.5841474993389686
Validation loss: 2.4743360746346985

Epoch: 6| Step: 4
Training loss: 2.6498476848374013
Validation loss: 2.4730919428484626

Epoch: 6| Step: 5
Training loss: 2.2508172564289572
Validation loss: 2.477772366038382

Epoch: 6| Step: 6
Training loss: 2.122598412953934
Validation loss: 2.4730021881081496

Epoch: 6| Step: 7
Training loss: 2.984803174045647
Validation loss: 2.4760447381085733

Epoch: 6| Step: 8
Training loss: 2.4391515956248413
Validation loss: 2.493842266163728

Epoch: 6| Step: 9
Training loss: 2.5652878645491093
Validation loss: 2.4953236235082232

Epoch: 6| Step: 10
Training loss: 1.697604971829683
Validation loss: 2.4757809052447546

Epoch: 6| Step: 11
Training loss: 2.599952268162324
Validation loss: 2.4775976510664246

Epoch: 6| Step: 12
Training loss: 2.7399733426544555
Validation loss: 2.4878175504029665

Epoch: 6| Step: 13
Training loss: 2.3415341902034235
Validation loss: 2.485497850710688

Epoch: 134| Step: 0
Training loss: 2.512943331193725
Validation loss: 2.4731847631126294

Epoch: 6| Step: 1
Training loss: 2.587508447605662
Validation loss: 2.4776271212491934

Epoch: 6| Step: 2
Training loss: 2.808488910187947
Validation loss: 2.473347692250868

Epoch: 6| Step: 3
Training loss: 2.2830822001365974
Validation loss: 2.4729373364333243

Epoch: 6| Step: 4
Training loss: 2.7080139754370007
Validation loss: 2.4774281606385644

Epoch: 6| Step: 5
Training loss: 2.5809833749134437
Validation loss: 2.472384662387304

Epoch: 6| Step: 6
Training loss: 2.6328998856800303
Validation loss: 2.471441757102398

Epoch: 6| Step: 7
Training loss: 2.436338490287795
Validation loss: 2.469719338346781

Epoch: 6| Step: 8
Training loss: 2.280279083571704
Validation loss: 2.4712891538300354

Epoch: 6| Step: 9
Training loss: 1.2755571756083102
Validation loss: 2.4610153347402037

Epoch: 6| Step: 10
Training loss: 2.266832858425678
Validation loss: 2.4647446827594948

Epoch: 6| Step: 11
Training loss: 3.2324891420493276
Validation loss: 2.4648452492783424

Epoch: 6| Step: 12
Training loss: 2.6794164759649375
Validation loss: 2.4639906255721242

Epoch: 6| Step: 13
Training loss: 1.9493061420672482
Validation loss: 2.4664556086517897

Epoch: 135| Step: 0
Training loss: 2.9184429391103013
Validation loss: 2.466689477205054

Epoch: 6| Step: 1
Training loss: 2.3402455897119037
Validation loss: 2.470371229071624

Epoch: 6| Step: 2
Training loss: 1.8369885621688182
Validation loss: 2.4836287267317374

Epoch: 6| Step: 3
Training loss: 2.691418651889968
Validation loss: 2.480817611071308

Epoch: 6| Step: 4
Training loss: 2.684273135436295
Validation loss: 2.492080462956419

Epoch: 6| Step: 5
Training loss: 2.413395761802937
Validation loss: 2.5087539315996725

Epoch: 6| Step: 6
Training loss: 2.1674180439697
Validation loss: 2.511104637206105

Epoch: 6| Step: 7
Training loss: 2.97819622390935
Validation loss: 2.5218441305433816

Epoch: 6| Step: 8
Training loss: 2.6665779834147134
Validation loss: 2.4929216712579656

Epoch: 6| Step: 9
Training loss: 2.663906417256872
Validation loss: 2.4990539350160534

Epoch: 6| Step: 10
Training loss: 2.684881309571128
Validation loss: 2.4845827283724704

Epoch: 6| Step: 11
Training loss: 2.0003020535306377
Validation loss: 2.477466879417018

Epoch: 6| Step: 12
Training loss: 1.8682705599096252
Validation loss: 2.4722066252522423

Epoch: 6| Step: 13
Training loss: 2.6051204511504724
Validation loss: 2.47134906431957

Epoch: 136| Step: 0
Training loss: 1.6647601030979506
Validation loss: 2.4726870084419357

Epoch: 6| Step: 1
Training loss: 2.086870192127796
Validation loss: 2.4735036867226188

Epoch: 6| Step: 2
Training loss: 2.2116394007175204
Validation loss: 2.4627321995506315

Epoch: 6| Step: 3
Training loss: 2.989149817603674
Validation loss: 2.474780784557532

Epoch: 6| Step: 4
Training loss: 2.561740855836784
Validation loss: 2.4650026361891118

Epoch: 6| Step: 5
Training loss: 2.991940958807304
Validation loss: 2.4759653777514607

Epoch: 6| Step: 6
Training loss: 2.808198988631934
Validation loss: 2.4668203288836197

Epoch: 6| Step: 7
Training loss: 2.0096780741225415
Validation loss: 2.4743761185022604

Epoch: 6| Step: 8
Training loss: 2.3383109496448506
Validation loss: 2.4606857897686245

Epoch: 6| Step: 9
Training loss: 3.3419200132101943
Validation loss: 2.4607597973157085

Epoch: 6| Step: 10
Training loss: 2.416035854837749
Validation loss: 2.473878981003632

Epoch: 6| Step: 11
Training loss: 2.681636890872764
Validation loss: 2.476761401800334

Epoch: 6| Step: 12
Training loss: 2.010743847513278
Validation loss: 2.4686706787758053

Epoch: 6| Step: 13
Training loss: 2.1141788741037364
Validation loss: 2.4875081456752293

Epoch: 137| Step: 0
Training loss: 2.25378005072796
Validation loss: 2.4903719516224836

Epoch: 6| Step: 1
Training loss: 2.7089619322756824
Validation loss: 2.514585623836238

Epoch: 6| Step: 2
Training loss: 2.9885387833267685
Validation loss: 2.5382723943450762

Epoch: 6| Step: 3
Training loss: 2.6233557137968995
Validation loss: 2.555163386794936

Epoch: 6| Step: 4
Training loss: 2.3097058504428802
Validation loss: 2.5058253289177226

Epoch: 6| Step: 5
Training loss: 2.5507201206424233
Validation loss: 2.485037261535138

Epoch: 6| Step: 6
Training loss: 2.986670445144625
Validation loss: 2.4657116393261007

Epoch: 6| Step: 7
Training loss: 1.8757472774379595
Validation loss: 2.4669235329871073

Epoch: 6| Step: 8
Training loss: 2.6469715303475976
Validation loss: 2.466418553666948

Epoch: 6| Step: 9
Training loss: 1.8552119990945706
Validation loss: 2.468125968643762

Epoch: 6| Step: 10
Training loss: 3.2018778536275834
Validation loss: 2.4725958408491366

Epoch: 6| Step: 11
Training loss: 2.603717083587844
Validation loss: 2.4671498033841934

Epoch: 6| Step: 12
Training loss: 2.491107383738094
Validation loss: 2.4669387063807298

Epoch: 6| Step: 13
Training loss: 2.1172323186993944
Validation loss: 2.465555812636932

Epoch: 138| Step: 0
Training loss: 2.2059591474442293
Validation loss: 2.4767005633229005

Epoch: 6| Step: 1
Training loss: 2.2950141271351687
Validation loss: 2.4670161021222157

Epoch: 6| Step: 2
Training loss: 2.6015846778451275
Validation loss: 2.469256272695809

Epoch: 6| Step: 3
Training loss: 2.2900327117116097
Validation loss: 2.472101302946118

Epoch: 6| Step: 4
Training loss: 3.30843406030655
Validation loss: 2.46069225724176

Epoch: 6| Step: 5
Training loss: 2.7802760101081354
Validation loss: 2.4755114903438566

Epoch: 6| Step: 6
Training loss: 2.7914350233241896
Validation loss: 2.4658348078034136

Epoch: 6| Step: 7
Training loss: 2.1876336193147052
Validation loss: 2.4853756406925496

Epoch: 6| Step: 8
Training loss: 2.3699036177017927
Validation loss: 2.4959001620287427

Epoch: 6| Step: 9
Training loss: 1.8311738893491403
Validation loss: 2.491808919343785

Epoch: 6| Step: 10
Training loss: 2.646846957547963
Validation loss: 2.4994382068107472

Epoch: 6| Step: 11
Training loss: 1.7791213316431005
Validation loss: 2.4979849562379037

Epoch: 6| Step: 12
Training loss: 2.711159468916123
Validation loss: 2.498897913565066

Epoch: 6| Step: 13
Training loss: 2.6345097988118016
Validation loss: 2.5105892190685597

Epoch: 139| Step: 0
Training loss: 2.8869785713269973
Validation loss: 2.5022940600675447

Epoch: 6| Step: 1
Training loss: 2.366493249190015
Validation loss: 2.482020183791115

Epoch: 6| Step: 2
Training loss: 2.635895234533984
Validation loss: 2.4878295456503148

Epoch: 6| Step: 3
Training loss: 2.551723989502688
Validation loss: 2.4855035262000023

Epoch: 6| Step: 4
Training loss: 1.8338118564441679
Validation loss: 2.4967586007994234

Epoch: 6| Step: 5
Training loss: 2.678670884965329
Validation loss: 2.4886078033877572

Epoch: 6| Step: 6
Training loss: 2.402540741574031
Validation loss: 2.4803940488925904

Epoch: 6| Step: 7
Training loss: 2.801124541074637
Validation loss: 2.4888893234824

Epoch: 6| Step: 8
Training loss: 2.610926366813113
Validation loss: 2.49553588618918

Epoch: 6| Step: 9
Training loss: 1.844034463953439
Validation loss: 2.4879720786235966

Epoch: 6| Step: 10
Training loss: 2.9520607297072807
Validation loss: 2.4749970246627853

Epoch: 6| Step: 11
Training loss: 2.3011959200943086
Validation loss: 2.480643118589291

Epoch: 6| Step: 12
Training loss: 2.284283228334117
Validation loss: 2.473332044064716

Epoch: 6| Step: 13
Training loss: 2.218997887746623
Validation loss: 2.480057922143203

Epoch: 140| Step: 0
Training loss: 2.531615996097508
Validation loss: 2.476900111791537

Epoch: 6| Step: 1
Training loss: 2.6293032659561497
Validation loss: 2.471622212662499

Epoch: 6| Step: 2
Training loss: 2.826616844047056
Validation loss: 2.472489402087429

Epoch: 6| Step: 3
Training loss: 2.0437291555479096
Validation loss: 2.4706546180597377

Epoch: 6| Step: 4
Training loss: 2.7679807468362125
Validation loss: 2.467307743865227

Epoch: 6| Step: 5
Training loss: 3.180766304581774
Validation loss: 2.4746758849236743

Epoch: 6| Step: 6
Training loss: 2.254044817759023
Validation loss: 2.4726467925860596

Epoch: 6| Step: 7
Training loss: 2.414976274204006
Validation loss: 2.471392275828275

Epoch: 6| Step: 8
Training loss: 2.4575350052444733
Validation loss: 2.475885019788967

Epoch: 6| Step: 9
Training loss: 2.3652670343791837
Validation loss: 2.468535723823268

Epoch: 6| Step: 10
Training loss: 2.5770949646901355
Validation loss: 2.4741854807659904

Epoch: 6| Step: 11
Training loss: 2.336764900366522
Validation loss: 2.4630724169004847

Epoch: 6| Step: 12
Training loss: 1.9385688663674159
Validation loss: 2.4785764269413915

Epoch: 6| Step: 13
Training loss: 2.0857374688295254
Validation loss: 2.4835992397660864

Epoch: 141| Step: 0
Training loss: 2.313205920548228
Validation loss: 2.4925041356334603

Epoch: 6| Step: 1
Training loss: 2.731932939217415
Validation loss: 2.486272411216659

Epoch: 6| Step: 2
Training loss: 1.9067757303660768
Validation loss: 2.503955699731904

Epoch: 6| Step: 3
Training loss: 2.136508966221331
Validation loss: 2.501265189782262

Epoch: 6| Step: 4
Training loss: 2.629860691979008
Validation loss: 2.527548717302196

Epoch: 6| Step: 5
Training loss: 2.7334010542257037
Validation loss: 2.528989842458941

Epoch: 6| Step: 6
Training loss: 2.8613691203334555
Validation loss: 2.5062106553538546

Epoch: 6| Step: 7
Training loss: 2.6119497239851728
Validation loss: 2.492638111732793

Epoch: 6| Step: 8
Training loss: 2.552801620193106
Validation loss: 2.49716733351901

Epoch: 6| Step: 9
Training loss: 2.7873934498766557
Validation loss: 2.482056293435251

Epoch: 6| Step: 10
Training loss: 2.46845803767671
Validation loss: 2.484168885835844

Epoch: 6| Step: 11
Training loss: 2.159583127802371
Validation loss: 2.474334412483015

Epoch: 6| Step: 12
Training loss: 2.3461027416421514
Validation loss: 2.4781937626923125

Epoch: 6| Step: 13
Training loss: 2.681753357631345
Validation loss: 2.4744792884971694

Epoch: 142| Step: 0
Training loss: 2.249433234145421
Validation loss: 2.472622373488482

Epoch: 6| Step: 1
Training loss: 2.611700974340309
Validation loss: 2.4785186871769485

Epoch: 6| Step: 2
Training loss: 2.3542587689862975
Validation loss: 2.4825536101187926

Epoch: 6| Step: 3
Training loss: 2.6124450531798344
Validation loss: 2.487535525632176

Epoch: 6| Step: 4
Training loss: 2.3335364003600674
Validation loss: 2.4918675551133047

Epoch: 6| Step: 5
Training loss: 2.395891360257441
Validation loss: 2.4869223916062517

Epoch: 6| Step: 6
Training loss: 2.6088316574440604
Validation loss: 2.4852333825173676

Epoch: 6| Step: 7
Training loss: 2.3561692848850324
Validation loss: 2.4888189943420347

Epoch: 6| Step: 8
Training loss: 2.548330066879238
Validation loss: 2.490656415293683

Epoch: 6| Step: 9
Training loss: 3.014263418886123
Validation loss: 2.482867777364899

Epoch: 6| Step: 10
Training loss: 3.0333548248144493
Validation loss: 2.4808416291562567

Epoch: 6| Step: 11
Training loss: 2.7408114292672527
Validation loss: 2.4805470531849982

Epoch: 6| Step: 12
Training loss: 2.0042217519358023
Validation loss: 2.476542475766839

Epoch: 6| Step: 13
Training loss: 2.942953363980387
Validation loss: 2.4732533517879913

Epoch: 143| Step: 0
Training loss: 2.4167877912312115
Validation loss: 2.4721651558832094

Epoch: 6| Step: 1
Training loss: 1.9184845582342083
Validation loss: 2.4653618817616647

Epoch: 6| Step: 2
Training loss: 2.631455431097409
Validation loss: 2.466278255227963

Epoch: 6| Step: 3
Training loss: 2.200696973503952
Validation loss: 2.461306775356719

Epoch: 6| Step: 4
Training loss: 2.6102204009790078
Validation loss: 2.465179541396134

Epoch: 6| Step: 5
Training loss: 2.549524345166502
Validation loss: 2.458490363994792

Epoch: 6| Step: 6
Training loss: 2.725314928807612
Validation loss: 2.469925226477443

Epoch: 6| Step: 7
Training loss: 2.8080437006198933
Validation loss: 2.452783581083735

Epoch: 6| Step: 8
Training loss: 2.255142586845811
Validation loss: 2.46517450417803

Epoch: 6| Step: 9
Training loss: 2.3193984366579175
Validation loss: 2.4724578858206625

Epoch: 6| Step: 10
Training loss: 3.111746185817727
Validation loss: 2.4708559494098536

Epoch: 6| Step: 11
Training loss: 3.1977802624433966
Validation loss: 2.484786009628685

Epoch: 6| Step: 12
Training loss: 1.6407550578564711
Validation loss: 2.48875583213073

Epoch: 6| Step: 13
Training loss: 2.4469337816838856
Validation loss: 2.473373517967851

Epoch: 144| Step: 0
Training loss: 2.2906290884358764
Validation loss: 2.474838298434595

Epoch: 6| Step: 1
Training loss: 2.652464199561618
Validation loss: 2.474966905032235

Epoch: 6| Step: 2
Training loss: 2.590447863147095
Validation loss: 2.4791294437039655

Epoch: 6| Step: 3
Training loss: 3.134034182578161
Validation loss: 2.468691941878342

Epoch: 6| Step: 4
Training loss: 2.37938786784992
Validation loss: 2.464647723593164

Epoch: 6| Step: 5
Training loss: 2.4826216358453777
Validation loss: 2.4731451337687447

Epoch: 6| Step: 6
Training loss: 2.490712844658802
Validation loss: 2.4690980786588335

Epoch: 6| Step: 7
Training loss: 2.726256782421856
Validation loss: 2.476462296986857

Epoch: 6| Step: 8
Training loss: 2.4055227196174944
Validation loss: 2.465507180251888

Epoch: 6| Step: 9
Training loss: 2.2848624276915586
Validation loss: 2.4595514501587865

Epoch: 6| Step: 10
Training loss: 2.8460447673158478
Validation loss: 2.462311408603932

Epoch: 6| Step: 11
Training loss: 2.7617147788271486
Validation loss: 2.463132172330248

Epoch: 6| Step: 12
Training loss: 2.3939050848301777
Validation loss: 2.4618729604709557

Epoch: 6| Step: 13
Training loss: 1.4509219625718637
Validation loss: 2.4592990325479107

Epoch: 145| Step: 0
Training loss: 2.1577260590516687
Validation loss: 2.459788333682388

Epoch: 6| Step: 1
Training loss: 1.7162503185119111
Validation loss: 2.4563245216822773

Epoch: 6| Step: 2
Training loss: 2.9044595761315604
Validation loss: 2.452445614438901

Epoch: 6| Step: 3
Training loss: 2.5934967468900267
Validation loss: 2.4548146448649257

Epoch: 6| Step: 4
Training loss: 3.135413253292283
Validation loss: 2.4550377582720437

Epoch: 6| Step: 5
Training loss: 2.573238666684024
Validation loss: 2.461692912100227

Epoch: 6| Step: 6
Training loss: 2.736152637852902
Validation loss: 2.462239949135406

Epoch: 6| Step: 7
Training loss: 2.280029702076772
Validation loss: 2.46192206815603

Epoch: 6| Step: 8
Training loss: 2.674983397102526
Validation loss: 2.4561678082382215

Epoch: 6| Step: 9
Training loss: 2.775017258873959
Validation loss: 2.4531046062682766

Epoch: 6| Step: 10
Training loss: 2.373866362810655
Validation loss: 2.4688725581913276

Epoch: 6| Step: 11
Training loss: 2.9554443939197634
Validation loss: 2.4614524342943414

Epoch: 6| Step: 12
Training loss: 2.195340193712945
Validation loss: 2.4771894589964947

Epoch: 6| Step: 13
Training loss: 1.5170635200232718
Validation loss: 2.4737587189034813

Epoch: 146| Step: 0
Training loss: 2.951719565223572
Validation loss: 2.454378184565884

Epoch: 6| Step: 1
Training loss: 2.469565727574891
Validation loss: 2.4595684865660368

Epoch: 6| Step: 2
Training loss: 2.3016643804621224
Validation loss: 2.451477874663278

Epoch: 6| Step: 3
Training loss: 2.94932044655995
Validation loss: 2.4510467618950638

Epoch: 6| Step: 4
Training loss: 2.4026033586861404
Validation loss: 2.436932130835127

Epoch: 6| Step: 5
Training loss: 2.448065916881822
Validation loss: 2.4597458795259084

Epoch: 6| Step: 6
Training loss: 2.0739729856021087
Validation loss: 2.450208140633974

Epoch: 6| Step: 7
Training loss: 2.5864817804284415
Validation loss: 2.455532617615267

Epoch: 6| Step: 8
Training loss: 2.8385303928722987
Validation loss: 2.4481386178762503

Epoch: 6| Step: 9
Training loss: 2.625806321328305
Validation loss: 2.4562094020966616

Epoch: 6| Step: 10
Training loss: 2.824288563900898
Validation loss: 2.4677928243986864

Epoch: 6| Step: 11
Training loss: 2.1959659784874437
Validation loss: 2.465751839370039

Epoch: 6| Step: 12
Training loss: 2.141814632227365
Validation loss: 2.4576291406538804

Epoch: 6| Step: 13
Training loss: 2.6355939260183825
Validation loss: 2.465345844409667

Epoch: 147| Step: 0
Training loss: 2.607453878256313
Validation loss: 2.465784859481106

Epoch: 6| Step: 1
Training loss: 1.989421225396345
Validation loss: 2.459906338818633

Epoch: 6| Step: 2
Training loss: 2.3006861202346425
Validation loss: 2.4666784101498775

Epoch: 6| Step: 3
Training loss: 2.526098967751075
Validation loss: 2.4680733538102566

Epoch: 6| Step: 4
Training loss: 3.12221418185875
Validation loss: 2.465479797419761

Epoch: 6| Step: 5
Training loss: 2.178280174386996
Validation loss: 2.46107509167847

Epoch: 6| Step: 6
Training loss: 2.8341051808116298
Validation loss: 2.4708028057028733

Epoch: 6| Step: 7
Training loss: 2.519987789457499
Validation loss: 2.471668674942336

Epoch: 6| Step: 8
Training loss: 2.6060565245464
Validation loss: 2.4708974327105664

Epoch: 6| Step: 9
Training loss: 2.188661757280179
Validation loss: 2.4710584543308256

Epoch: 6| Step: 10
Training loss: 2.1477533534284343
Validation loss: 2.466310833275603

Epoch: 6| Step: 11
Training loss: 2.721281079249307
Validation loss: 2.4622966908341963

Epoch: 6| Step: 12
Training loss: 2.7827504429010195
Validation loss: 2.4812958066925552

Epoch: 6| Step: 13
Training loss: 2.293038934156668
Validation loss: 2.4732807369200716

Epoch: 148| Step: 0
Training loss: 2.21750122173495
Validation loss: 2.4745786565104946

Epoch: 6| Step: 1
Training loss: 2.9320371437165984
Validation loss: 2.4763386301287045

Epoch: 6| Step: 2
Training loss: 2.6321308827076995
Validation loss: 2.463761387732084

Epoch: 6| Step: 3
Training loss: 2.0420966830852127
Validation loss: 2.4724365586748545

Epoch: 6| Step: 4
Training loss: 2.4556873327408932
Validation loss: 2.459352449073792

Epoch: 6| Step: 5
Training loss: 1.8371893328186952
Validation loss: 2.466183225407993

Epoch: 6| Step: 6
Training loss: 2.0934650739695124
Validation loss: 2.4739577658133944

Epoch: 6| Step: 7
Training loss: 2.2750559286432037
Validation loss: 2.472939714572351

Epoch: 6| Step: 8
Training loss: 2.7863522993691863
Validation loss: 2.4670429202674384

Epoch: 6| Step: 9
Training loss: 2.016885880570073
Validation loss: 2.4608940645451716

Epoch: 6| Step: 10
Training loss: 2.3519280741975783
Validation loss: 2.4685635073757997

Epoch: 6| Step: 11
Training loss: 2.921631665220522
Validation loss: 2.4674251719745874

Epoch: 6| Step: 12
Training loss: 2.7721484130057807
Validation loss: 2.472953027288113

Epoch: 6| Step: 13
Training loss: 3.101540361224514
Validation loss: 2.466695848391154

Epoch: 149| Step: 0
Training loss: 2.023388482755706
Validation loss: 2.4659219148054974

Epoch: 6| Step: 1
Training loss: 2.7037600614701778
Validation loss: 2.4646989927994882

Epoch: 6| Step: 2
Training loss: 2.1115982207556527
Validation loss: 2.4650498357949022

Epoch: 6| Step: 3
Training loss: 3.2028617936828923
Validation loss: 2.4713123158541435

Epoch: 6| Step: 4
Training loss: 2.345235735140787
Validation loss: 2.463737662800751

Epoch: 6| Step: 5
Training loss: 2.236968875694066
Validation loss: 2.4607919478389286

Epoch: 6| Step: 6
Training loss: 3.0173540445724716
Validation loss: 2.476647938118003

Epoch: 6| Step: 7
Training loss: 2.0459091586831257
Validation loss: 2.467669930782561

Epoch: 6| Step: 8
Training loss: 2.3849920591785985
Validation loss: 2.4673732830157395

Epoch: 6| Step: 9
Training loss: 2.2076217536571927
Validation loss: 2.4619188400720535

Epoch: 6| Step: 10
Training loss: 2.1060258061203623
Validation loss: 2.4689659957717485

Epoch: 6| Step: 11
Training loss: 2.5816285343034227
Validation loss: 2.4797138256782016

Epoch: 6| Step: 12
Training loss: 2.76312228827612
Validation loss: 2.4753831528293064

Epoch: 6| Step: 13
Training loss: 2.468528641059973
Validation loss: 2.4712272801735264

Epoch: 150| Step: 0
Training loss: 2.032621419547939
Validation loss: 2.490607435418978

Epoch: 6| Step: 1
Training loss: 2.132614182415
Validation loss: 2.4774908979077397

Epoch: 6| Step: 2
Training loss: 2.491398701457481
Validation loss: 2.476438308634432

Epoch: 6| Step: 3
Training loss: 2.1581506506983286
Validation loss: 2.4707680835653303

Epoch: 6| Step: 4
Training loss: 2.4673573875763
Validation loss: 2.478301623970393

Epoch: 6| Step: 5
Training loss: 2.9874729235072515
Validation loss: 2.4735236873619617

Epoch: 6| Step: 6
Training loss: 1.832725084436474
Validation loss: 2.4642030555309646

Epoch: 6| Step: 7
Training loss: 2.3139439791374437
Validation loss: 2.4641878652923976

Epoch: 6| Step: 8
Training loss: 2.948061361008119
Validation loss: 2.465495543794171

Epoch: 6| Step: 9
Training loss: 2.413549276072943
Validation loss: 2.4712751005419435

Epoch: 6| Step: 10
Training loss: 2.3933727953251127
Validation loss: 2.4633213264583946

Epoch: 6| Step: 11
Training loss: 2.4536849433720227
Validation loss: 2.4681629496960773

Epoch: 6| Step: 12
Training loss: 2.8059679259182095
Validation loss: 2.468109417954143

Epoch: 6| Step: 13
Training loss: 3.094722248241707
Validation loss: 2.4785448357694713

Epoch: 151| Step: 0
Training loss: 1.8616856554862153
Validation loss: 2.4883709167261983

Epoch: 6| Step: 1
Training loss: 2.387965898842193
Validation loss: 2.468562364489597

Epoch: 6| Step: 2
Training loss: 2.4588682163911537
Validation loss: 2.478541709496105

Epoch: 6| Step: 3
Training loss: 2.999558575261257
Validation loss: 2.4835184808064836

Epoch: 6| Step: 4
Training loss: 2.5969446569986183
Validation loss: 2.486261191594638

Epoch: 6| Step: 5
Training loss: 2.2812435202310586
Validation loss: 2.485888469986985

Epoch: 6| Step: 6
Training loss: 2.7802075779212183
Validation loss: 2.4754289586082474

Epoch: 6| Step: 7
Training loss: 2.575935219862388
Validation loss: 2.475978489638048

Epoch: 6| Step: 8
Training loss: 1.9271218235880208
Validation loss: 2.4736763134211253

Epoch: 6| Step: 9
Training loss: 2.9657069622005023
Validation loss: 2.4665082579945623

Epoch: 6| Step: 10
Training loss: 2.4634204751355164
Validation loss: 2.4714223103965014

Epoch: 6| Step: 11
Training loss: 2.0918803051464416
Validation loss: 2.4746589444923086

Epoch: 6| Step: 12
Training loss: 2.531429331513961
Validation loss: 2.471431965435794

Epoch: 6| Step: 13
Training loss: 2.635340984800763
Validation loss: 2.475760970991099

Epoch: 152| Step: 0
Training loss: 2.4796051684763087
Validation loss: 2.477616030878692

Epoch: 6| Step: 1
Training loss: 2.7958946854236086
Validation loss: 2.490902895483072

Epoch: 6| Step: 2
Training loss: 2.583092175784409
Validation loss: 2.4903809428246957

Epoch: 6| Step: 3
Training loss: 2.431584638694088
Validation loss: 2.500831195618952

Epoch: 6| Step: 4
Training loss: 2.473783552454032
Validation loss: 2.5090893340147717

Epoch: 6| Step: 5
Training loss: 2.7721090224433835
Validation loss: 2.4993442788079863

Epoch: 6| Step: 6
Training loss: 2.8460476155590397
Validation loss: 2.5083877996908086

Epoch: 6| Step: 7
Training loss: 2.5288913239659343
Validation loss: 2.4872479890996786

Epoch: 6| Step: 8
Training loss: 2.328350491452557
Validation loss: 2.4758169855147094

Epoch: 6| Step: 9
Training loss: 2.241107215226523
Validation loss: 2.4717769334890285

Epoch: 6| Step: 10
Training loss: 2.2820418564270164
Validation loss: 2.4717082716907846

Epoch: 6| Step: 11
Training loss: 2.27514070767405
Validation loss: 2.4782185117148225

Epoch: 6| Step: 12
Training loss: 1.9484903507360822
Validation loss: 2.474588845199696

Epoch: 6| Step: 13
Training loss: 2.610569754635401
Validation loss: 2.468996301267379

Epoch: 153| Step: 0
Training loss: 3.498562381316348
Validation loss: 2.4652295100422448

Epoch: 6| Step: 1
Training loss: 2.3134075136777827
Validation loss: 2.468168987037725

Epoch: 6| Step: 2
Training loss: 1.8408219962276782
Validation loss: 2.474640558774431

Epoch: 6| Step: 3
Training loss: 1.8714069271603988
Validation loss: 2.461166903990234

Epoch: 6| Step: 4
Training loss: 2.608295514337343
Validation loss: 2.478474726137841

Epoch: 6| Step: 5
Training loss: 2.535240324768738
Validation loss: 2.4766369476520493

Epoch: 6| Step: 6
Training loss: 2.1837514093631416
Validation loss: 2.470082096660876

Epoch: 6| Step: 7
Training loss: 2.7872461554065975
Validation loss: 2.4771435173387037

Epoch: 6| Step: 8
Training loss: 1.9268370049931662
Validation loss: 2.477689332255427

Epoch: 6| Step: 9
Training loss: 2.769047230767858
Validation loss: 2.4690167246762336

Epoch: 6| Step: 10
Training loss: 2.944740786324837
Validation loss: 2.4693371521200693

Epoch: 6| Step: 11
Training loss: 2.892551419420104
Validation loss: 2.4787213355616213

Epoch: 6| Step: 12
Training loss: 2.3332205699739177
Validation loss: 2.4773480345834225

Epoch: 6| Step: 13
Training loss: 1.949490759452685
Validation loss: 2.4700313896367416

Epoch: 154| Step: 0
Training loss: 2.2747262140211255
Validation loss: 2.4607543150524753

Epoch: 6| Step: 1
Training loss: 1.7748367906137286
Validation loss: 2.470588096383569

Epoch: 6| Step: 2
Training loss: 1.9862368402351223
Validation loss: 2.480305567702829

Epoch: 6| Step: 3
Training loss: 2.6615262753846687
Validation loss: 2.481884713910806

Epoch: 6| Step: 4
Training loss: 2.297642170987127
Validation loss: 2.4874549024573764

Epoch: 6| Step: 5
Training loss: 2.8325513059678094
Validation loss: 2.4938131150725424

Epoch: 6| Step: 6
Training loss: 2.0906090021590473
Validation loss: 2.493053481143335

Epoch: 6| Step: 7
Training loss: 2.8488962746694875
Validation loss: 2.511787994996487

Epoch: 6| Step: 8
Training loss: 2.8610078079026424
Validation loss: 2.5191219184611957

Epoch: 6| Step: 9
Training loss: 2.2445440900775924
Validation loss: 2.5088148797551684

Epoch: 6| Step: 10
Training loss: 3.2228268479939906
Validation loss: 2.4978512907871933

Epoch: 6| Step: 11
Training loss: 2.703380065526585
Validation loss: 2.493860255379848

Epoch: 6| Step: 12
Training loss: 2.1385835812253675
Validation loss: 2.4965086320034757

Epoch: 6| Step: 13
Training loss: 2.271697437006146
Validation loss: 2.480151042466132

Epoch: 155| Step: 0
Training loss: 2.4971163331088726
Validation loss: 2.492327886471985

Epoch: 6| Step: 1
Training loss: 2.6162049228304323
Validation loss: 2.4814306601760197

Epoch: 6| Step: 2
Training loss: 2.3753242271363706
Validation loss: 2.4783417480657683

Epoch: 6| Step: 3
Training loss: 2.716454512458402
Validation loss: 2.483379532450428

Epoch: 6| Step: 4
Training loss: 2.033072138514532
Validation loss: 2.47185584959898

Epoch: 6| Step: 5
Training loss: 2.9667788737933147
Validation loss: 2.481870544501322

Epoch: 6| Step: 6
Training loss: 2.734715206235616
Validation loss: 2.4787707987784984

Epoch: 6| Step: 7
Training loss: 2.3614211146974022
Validation loss: 2.4751601716434264

Epoch: 6| Step: 8
Training loss: 1.7996628736986036
Validation loss: 2.47842698055195

Epoch: 6| Step: 9
Training loss: 2.452041872808925
Validation loss: 2.476842645791105

Epoch: 6| Step: 10
Training loss: 2.9906913662266223
Validation loss: 2.4783521377356315

Epoch: 6| Step: 11
Training loss: 2.331668486886431
Validation loss: 2.483504648721265

Epoch: 6| Step: 12
Training loss: 1.6862819655593206
Validation loss: 2.481477050898547

Epoch: 6| Step: 13
Training loss: 2.6309441975468557
Validation loss: 2.498102755663464

Epoch: 156| Step: 0
Training loss: 2.7383967006577663
Validation loss: 2.497801115679099

Epoch: 6| Step: 1
Training loss: 2.7165802815126447
Validation loss: 2.5011325337210732

Epoch: 6| Step: 2
Training loss: 2.3711852506927897
Validation loss: 2.49900005528894

Epoch: 6| Step: 3
Training loss: 2.630792675008229
Validation loss: 2.4940243830586373

Epoch: 6| Step: 4
Training loss: 2.1932006669818005
Validation loss: 2.4958628197925212

Epoch: 6| Step: 5
Training loss: 2.7180731796900073
Validation loss: 2.5020564442845905

Epoch: 6| Step: 6
Training loss: 2.472191452044425
Validation loss: 2.4965581725535024

Epoch: 6| Step: 7
Training loss: 2.8731945837497443
Validation loss: 2.503954176263088

Epoch: 6| Step: 8
Training loss: 2.153026659398704
Validation loss: 2.5048965899886086

Epoch: 6| Step: 9
Training loss: 2.296374311825271
Validation loss: 2.494846834489944

Epoch: 6| Step: 10
Training loss: 2.4239990764276236
Validation loss: 2.494181083763286

Epoch: 6| Step: 11
Training loss: 1.9473262918983378
Validation loss: 2.4880947999351952

Epoch: 6| Step: 12
Training loss: 2.4318861259102023
Validation loss: 2.491502721574871

Epoch: 6| Step: 13
Training loss: 2.066854561703808
Validation loss: 2.485117803246432

Epoch: 157| Step: 0
Training loss: 2.1806210942013355
Validation loss: 2.5083051219464743

Epoch: 6| Step: 1
Training loss: 2.2713970452903025
Validation loss: 2.4884865285375795

Epoch: 6| Step: 2
Training loss: 1.7449080774749368
Validation loss: 2.5010234724593414

Epoch: 6| Step: 3
Training loss: 2.3329522184931206
Validation loss: 2.505846657328

Epoch: 6| Step: 4
Training loss: 1.9414701086705735
Validation loss: 2.5118371946450155

Epoch: 6| Step: 5
Training loss: 2.7563566087791944
Validation loss: 2.51589610532488

Epoch: 6| Step: 6
Training loss: 3.197225655484743
Validation loss: 2.499538506192128

Epoch: 6| Step: 7
Training loss: 2.536593037395972
Validation loss: 2.5051719573004427

Epoch: 6| Step: 8
Training loss: 2.4630411511978227
Validation loss: 2.5106265321966994

Epoch: 6| Step: 9
Training loss: 2.0932292005835236
Validation loss: 2.504549639123038

Epoch: 6| Step: 10
Training loss: 2.6979853107060996
Validation loss: 2.4931272450951227

Epoch: 6| Step: 11
Training loss: 2.5028818686297263
Validation loss: 2.4983237209818014

Epoch: 6| Step: 12
Training loss: 2.2996701211294686
Validation loss: 2.4952361096573052

Epoch: 6| Step: 13
Training loss: 2.6100588148074655
Validation loss: 2.4979051077996566

Epoch: 158| Step: 0
Training loss: 2.1546463462434784
Validation loss: 2.4856275443963387

Epoch: 6| Step: 1
Training loss: 2.401289744328714
Validation loss: 2.491388031255044

Epoch: 6| Step: 2
Training loss: 1.7137324527479312
Validation loss: 2.508387514545195

Epoch: 6| Step: 3
Training loss: 1.7720315880855435
Validation loss: 2.5072134219424025

Epoch: 6| Step: 4
Training loss: 1.861523132676406
Validation loss: 2.5133518350191717

Epoch: 6| Step: 5
Training loss: 2.0999777111505495
Validation loss: 2.5141166603730083

Epoch: 6| Step: 6
Training loss: 2.934527414931886
Validation loss: 2.5336425604026243

Epoch: 6| Step: 7
Training loss: 2.7616394117902456
Validation loss: 2.515237182040563

Epoch: 6| Step: 8
Training loss: 2.899120651308304
Validation loss: 2.513266506276331

Epoch: 6| Step: 9
Training loss: 2.44986114108447
Validation loss: 2.4936051039002316

Epoch: 6| Step: 10
Training loss: 2.068520745888683
Validation loss: 2.498566311777489

Epoch: 6| Step: 11
Training loss: 2.8490524322944446
Validation loss: 2.4932665905289704

Epoch: 6| Step: 12
Training loss: 2.7092702174356815
Validation loss: 2.4927249755388847

Epoch: 6| Step: 13
Training loss: 3.118436099597904
Validation loss: 2.502309701818726

Epoch: 159| Step: 0
Training loss: 2.91747062138767
Validation loss: 2.4901032616499887

Epoch: 6| Step: 1
Training loss: 2.6615742896413366
Validation loss: 2.4940463700255058

Epoch: 6| Step: 2
Training loss: 2.9944479747870147
Validation loss: 2.4997837449955096

Epoch: 6| Step: 3
Training loss: 1.8874190711046348
Validation loss: 2.49201285488738

Epoch: 6| Step: 4
Training loss: 2.5580387823999
Validation loss: 2.493236691604541

Epoch: 6| Step: 5
Training loss: 1.8917321360457828
Validation loss: 2.500862044483467

Epoch: 6| Step: 6
Training loss: 3.123955208646546
Validation loss: 2.516364421336953

Epoch: 6| Step: 7
Training loss: 1.9787388456298907
Validation loss: 2.5049248030664586

Epoch: 6| Step: 8
Training loss: 1.807058716930957
Validation loss: 2.52180612462943

Epoch: 6| Step: 9
Training loss: 2.2948252556123716
Validation loss: 2.5085402529039578

Epoch: 6| Step: 10
Training loss: 2.2441584733298066
Validation loss: 2.527372930988157

Epoch: 6| Step: 11
Training loss: 2.345727925165527
Validation loss: 2.525412567035482

Epoch: 6| Step: 12
Training loss: 2.5503232115315044
Validation loss: 2.529044945148162

Epoch: 6| Step: 13
Training loss: 2.3982335106401997
Validation loss: 2.5346508648958572

Epoch: 160| Step: 0
Training loss: 2.3551170883713968
Validation loss: 2.5198630890666225

Epoch: 6| Step: 1
Training loss: 2.465409348488651
Validation loss: 2.535284210565091

Epoch: 6| Step: 2
Training loss: 2.6827126379428567
Validation loss: 2.518185862050838

Epoch: 6| Step: 3
Training loss: 2.542621641596896
Validation loss: 2.5151537499221144

Epoch: 6| Step: 4
Training loss: 2.524001964822237
Validation loss: 2.4959465703818924

Epoch: 6| Step: 5
Training loss: 2.7861353796978183
Validation loss: 2.496447423196789

Epoch: 6| Step: 6
Training loss: 2.1709743251398113
Validation loss: 2.4907375091706143

Epoch: 6| Step: 7
Training loss: 2.4322502138143802
Validation loss: 2.4920586739052313

Epoch: 6| Step: 8
Training loss: 2.616169381318895
Validation loss: 2.4857585900946146

Epoch: 6| Step: 9
Training loss: 2.5634906063603404
Validation loss: 2.491506318022263

Epoch: 6| Step: 10
Training loss: 2.067675137833904
Validation loss: 2.4899698752760253

Epoch: 6| Step: 11
Training loss: 2.1211270212054463
Validation loss: 2.4953116005880176

Epoch: 6| Step: 12
Training loss: 2.4308259020423573
Validation loss: 2.4999142473136784

Epoch: 6| Step: 13
Training loss: 2.3449549311554776
Validation loss: 2.5040695049924695

Epoch: 161| Step: 0
Training loss: 2.3846589485571807
Validation loss: 2.510170530121677

Epoch: 6| Step: 1
Training loss: 2.451281395399384
Validation loss: 2.5075067193095633

Epoch: 6| Step: 2
Training loss: 2.084375947097037
Validation loss: 2.4967014963838365

Epoch: 6| Step: 3
Training loss: 2.239045922909457
Validation loss: 2.514584138412368

Epoch: 6| Step: 4
Training loss: 2.5336422388904754
Validation loss: 2.52058767797924

Epoch: 6| Step: 5
Training loss: 2.3435908962923904
Validation loss: 2.5071457385482763

Epoch: 6| Step: 6
Training loss: 2.795862110366952
Validation loss: 2.517879005448007

Epoch: 6| Step: 7
Training loss: 1.647921947112306
Validation loss: 2.514438365074945

Epoch: 6| Step: 8
Training loss: 2.147315935944165
Validation loss: 2.5218509532691167

Epoch: 6| Step: 9
Training loss: 2.545453944763509
Validation loss: 2.5301909095244848

Epoch: 6| Step: 10
Training loss: 2.779157215504587
Validation loss: 2.52797532677883

Epoch: 6| Step: 11
Training loss: 2.71752410962202
Validation loss: 2.5305928605679013

Epoch: 6| Step: 12
Training loss: 2.396145100971489
Validation loss: 2.539849011797008

Epoch: 6| Step: 13
Training loss: 2.699357687525204
Validation loss: 2.5484188525875955

Epoch: 162| Step: 0
Training loss: 2.185402654912618
Validation loss: 2.5501017032861384

Epoch: 6| Step: 1
Training loss: 2.7006111583606605
Validation loss: 2.5297069163183052

Epoch: 6| Step: 2
Training loss: 1.917664233737894
Validation loss: 2.552483575278009

Epoch: 6| Step: 3
Training loss: 3.2191404827994146
Validation loss: 2.5365778420328073

Epoch: 6| Step: 4
Training loss: 1.826660939379343
Validation loss: 2.5288324624414598

Epoch: 6| Step: 5
Training loss: 2.0494080223780475
Validation loss: 2.5087733027565937

Epoch: 6| Step: 6
Training loss: 2.494908010893108
Validation loss: 2.5014507217102917

Epoch: 6| Step: 7
Training loss: 2.658082038537598
Validation loss: 2.509367827915506

Epoch: 6| Step: 8
Training loss: 2.840663366614699
Validation loss: 2.510110311427177

Epoch: 6| Step: 9
Training loss: 2.288738449125867
Validation loss: 2.505775329338003

Epoch: 6| Step: 10
Training loss: 2.1568991748092836
Validation loss: 2.5210265461251495

Epoch: 6| Step: 11
Training loss: 1.5578307002466936
Validation loss: 2.5111751966916627

Epoch: 6| Step: 12
Training loss: 2.9007182645855987
Validation loss: 2.517193970374946

Epoch: 6| Step: 13
Training loss: 2.6106255555557563
Validation loss: 2.5129768855282575

Epoch: 163| Step: 0
Training loss: 2.5401954353418255
Validation loss: 2.510530751486535

Epoch: 6| Step: 1
Training loss: 2.228367552322145
Validation loss: 2.5154283813975518

Epoch: 6| Step: 2
Training loss: 2.378034560379922
Validation loss: 2.505773458100619

Epoch: 6| Step: 3
Training loss: 2.133732970873162
Validation loss: 2.509740561670615

Epoch: 6| Step: 4
Training loss: 1.6791816127039365
Validation loss: 2.4950441512773525

Epoch: 6| Step: 5
Training loss: 2.0642473159741175
Validation loss: 2.4939082870664686

Epoch: 6| Step: 6
Training loss: 1.7427671007218373
Validation loss: 2.502566077623609

Epoch: 6| Step: 7
Training loss: 2.462107449917689
Validation loss: 2.503442976332961

Epoch: 6| Step: 8
Training loss: 3.376820250015616
Validation loss: 2.5091523327814476

Epoch: 6| Step: 9
Training loss: 1.7627009933253335
Validation loss: 2.5175721745782003

Epoch: 6| Step: 10
Training loss: 2.570479831060662
Validation loss: 2.521068078662893

Epoch: 6| Step: 11
Training loss: 2.5487757417988783
Validation loss: 2.5213632236867802

Epoch: 6| Step: 12
Training loss: 2.558030114444756
Validation loss: 2.540126683162291

Epoch: 6| Step: 13
Training loss: 3.331117672465946
Validation loss: 2.5327009273352887

Epoch: 164| Step: 0
Training loss: 2.7350960788901912
Validation loss: 2.5396516556555495

Epoch: 6| Step: 1
Training loss: 2.511158265127774
Validation loss: 2.5487449505755797

Epoch: 6| Step: 2
Training loss: 2.3717118640631063
Validation loss: 2.5463101905546424

Epoch: 6| Step: 3
Training loss: 2.365579997230514
Validation loss: 2.536093626232488

Epoch: 6| Step: 4
Training loss: 2.4764672871767934
Validation loss: 2.5219169893672864

Epoch: 6| Step: 5
Training loss: 1.9632196406857951
Validation loss: 2.514289311810496

Epoch: 6| Step: 6
Training loss: 2.7226805063313773
Validation loss: 2.5108005868536396

Epoch: 6| Step: 7
Training loss: 2.4517312919904173
Validation loss: 2.4913305485411854

Epoch: 6| Step: 8
Training loss: 2.080413934527706
Validation loss: 2.4975042601362265

Epoch: 6| Step: 9
Training loss: 3.0089705182523665
Validation loss: 2.4932500154765607

Epoch: 6| Step: 10
Training loss: 2.323088247579605
Validation loss: 2.4871296675648926

Epoch: 6| Step: 11
Training loss: 2.0863587600581512
Validation loss: 2.49388194105601

Epoch: 6| Step: 12
Training loss: 2.6819508950263278
Validation loss: 2.4971328665852623

Epoch: 6| Step: 13
Training loss: 2.704977029501925
Validation loss: 2.4951975949464478

Epoch: 165| Step: 0
Training loss: 2.402682644890774
Validation loss: 2.493709884486615

Epoch: 6| Step: 1
Training loss: 2.471672356518168
Validation loss: 2.502826380448867

Epoch: 6| Step: 2
Training loss: 3.315231204938761
Validation loss: 2.497119650951942

Epoch: 6| Step: 3
Training loss: 2.1972569444272976
Validation loss: 2.493928139999626

Epoch: 6| Step: 4
Training loss: 2.4331700919103443
Validation loss: 2.5002550789720104

Epoch: 6| Step: 5
Training loss: 2.08242284588352
Validation loss: 2.49258728966231

Epoch: 6| Step: 6
Training loss: 2.970557375410458
Validation loss: 2.4791749879286

Epoch: 6| Step: 7
Training loss: 2.3598163520350544
Validation loss: 2.527747647195548

Epoch: 6| Step: 8
Training loss: 2.3411050749820976
Validation loss: 2.5279955251992132

Epoch: 6| Step: 9
Training loss: 2.967926393002324
Validation loss: 2.5016728367518666

Epoch: 6| Step: 10
Training loss: 2.0276384378341517
Validation loss: 2.5025662602237535

Epoch: 6| Step: 11
Training loss: 2.7245253683285857
Validation loss: 2.4969289911485824

Epoch: 6| Step: 12
Training loss: 2.072304512255888
Validation loss: 2.497618001391045

Epoch: 6| Step: 13
Training loss: 1.9338794843922407
Validation loss: 2.4798567049354165

Epoch: 166| Step: 0
Training loss: 2.388605399154212
Validation loss: 2.4956624867889277

Epoch: 6| Step: 1
Training loss: 2.821071364156284
Validation loss: 2.4891628617916477

Epoch: 6| Step: 2
Training loss: 2.516852417042438
Validation loss: 2.4854300795707736

Epoch: 6| Step: 3
Training loss: 2.344055054203602
Validation loss: 2.4977488077717216

Epoch: 6| Step: 4
Training loss: 2.323317614337537
Validation loss: 2.4964220112724083

Epoch: 6| Step: 5
Training loss: 1.983339535145186
Validation loss: 2.4879973452708235

Epoch: 6| Step: 6
Training loss: 2.585792283881689
Validation loss: 2.494462205851455

Epoch: 6| Step: 7
Training loss: 2.9249385338218206
Validation loss: 2.4953433059386434

Epoch: 6| Step: 8
Training loss: 2.1176482540326704
Validation loss: 2.4994744304705643

Epoch: 6| Step: 9
Training loss: 2.501094578494818
Validation loss: 2.4975452769531343

Epoch: 6| Step: 10
Training loss: 2.527894044188763
Validation loss: 2.5059019042023785

Epoch: 6| Step: 11
Training loss: 2.308781288649568
Validation loss: 2.498752409854482

Epoch: 6| Step: 12
Training loss: 2.2721621157472054
Validation loss: 2.502179086225661

Epoch: 6| Step: 13
Training loss: 2.6136113116605424
Validation loss: 2.5101057680548657

Epoch: 167| Step: 0
Training loss: 2.0831588926399407
Validation loss: 2.5205915403395696

Epoch: 6| Step: 1
Training loss: 2.854243275154921
Validation loss: 2.5103798279229186

Epoch: 6| Step: 2
Training loss: 2.5623274954440722
Validation loss: 2.5197909908811544

Epoch: 6| Step: 3
Training loss: 2.5314883838065327
Validation loss: 2.5294217544028763

Epoch: 6| Step: 4
Training loss: 2.776462678075573
Validation loss: 2.517688173700782

Epoch: 6| Step: 5
Training loss: 2.6204051720752313
Validation loss: 2.536649259448217

Epoch: 6| Step: 6
Training loss: 1.8997314815509716
Validation loss: 2.5402351449394986

Epoch: 6| Step: 7
Training loss: 2.4300442645275986
Validation loss: 2.54969676527299

Epoch: 6| Step: 8
Training loss: 2.033969291395183
Validation loss: 2.555726364089635

Epoch: 6| Step: 9
Training loss: 2.3694009535647744
Validation loss: 2.5576218950564016

Epoch: 6| Step: 10
Training loss: 2.515867422992651
Validation loss: 2.5150805924801753

Epoch: 6| Step: 11
Training loss: 2.7983031649058625
Validation loss: 2.5333530182659234

Epoch: 6| Step: 12
Training loss: 1.8745429435607546
Validation loss: 2.533952306641842

Epoch: 6| Step: 13
Training loss: 2.520219859568598
Validation loss: 2.520122543463689

Epoch: 168| Step: 0
Training loss: 1.8241856181956808
Validation loss: 2.5166969148026492

Epoch: 6| Step: 1
Training loss: 2.5545712132483467
Validation loss: 2.5084425549685707

Epoch: 6| Step: 2
Training loss: 2.1936064366155077
Validation loss: 2.516567724906108

Epoch: 6| Step: 3
Training loss: 2.052933673275909
Validation loss: 2.5055532370028186

Epoch: 6| Step: 4
Training loss: 1.7194652802960089
Validation loss: 2.5162490481384165

Epoch: 6| Step: 5
Training loss: 2.846737644135577
Validation loss: 2.514168445985658

Epoch: 6| Step: 6
Training loss: 2.9216439058846593
Validation loss: 2.5166597943075293

Epoch: 6| Step: 7
Training loss: 2.777923909688017
Validation loss: 2.500560220891399

Epoch: 6| Step: 8
Training loss: 2.26095657815114
Validation loss: 2.502746726165806

Epoch: 6| Step: 9
Training loss: 2.717636318770835
Validation loss: 2.5063011233981136

Epoch: 6| Step: 10
Training loss: 2.419517895936065
Validation loss: 2.5143406908673662

Epoch: 6| Step: 11
Training loss: 2.5195118511547294
Validation loss: 2.5224068567314886

Epoch: 6| Step: 12
Training loss: 2.226540869892195
Validation loss: 2.53409719265515

Epoch: 6| Step: 13
Training loss: 2.6335630167817508
Validation loss: 2.519707165473816

Epoch: 169| Step: 0
Training loss: 3.1476180380336003
Validation loss: 2.523891333366276

Epoch: 6| Step: 1
Training loss: 2.594300177259564
Validation loss: 2.526272742854696

Epoch: 6| Step: 2
Training loss: 2.2752722189551564
Validation loss: 2.533338681851145

Epoch: 6| Step: 3
Training loss: 2.3277662372024848
Validation loss: 2.5441440536372055

Epoch: 6| Step: 4
Training loss: 1.5848922667607912
Validation loss: 2.565755853634321

Epoch: 6| Step: 5
Training loss: 2.7255673054215808
Validation loss: 2.550791515974229

Epoch: 6| Step: 6
Training loss: 2.2436342257327495
Validation loss: 2.5761226700447653

Epoch: 6| Step: 7
Training loss: 2.5981684359071093
Validation loss: 2.5532650346145562

Epoch: 6| Step: 8
Training loss: 2.5461557695034456
Validation loss: 2.5383888485607558

Epoch: 6| Step: 9
Training loss: 2.152664299165084
Validation loss: 2.53358195072061

Epoch: 6| Step: 10
Training loss: 1.881229352362488
Validation loss: 2.5077522642956778

Epoch: 6| Step: 11
Training loss: 2.4525961336198865
Validation loss: 2.504080700364058

Epoch: 6| Step: 12
Training loss: 2.776650901679461
Validation loss: 2.496006430310949

Epoch: 6| Step: 13
Training loss: 2.2331714723301257
Validation loss: 2.489432060295837

Epoch: 170| Step: 0
Training loss: 2.1638715639129518
Validation loss: 2.5034127941887876

Epoch: 6| Step: 1
Training loss: 1.9030922547077398
Validation loss: 2.4843488647877634

Epoch: 6| Step: 2
Training loss: 1.8561158668015398
Validation loss: 2.490741497587488

Epoch: 6| Step: 3
Training loss: 2.1057085739312638
Validation loss: 2.4888097021253657

Epoch: 6| Step: 4
Training loss: 1.8069081701178529
Validation loss: 2.4920217923328463

Epoch: 6| Step: 5
Training loss: 3.2769861037536208
Validation loss: 2.4911385364546565

Epoch: 6| Step: 6
Training loss: 2.0373375857160703
Validation loss: 2.502900372354723

Epoch: 6| Step: 7
Training loss: 3.1454656693466827
Validation loss: 2.510101018876411

Epoch: 6| Step: 8
Training loss: 1.7127901263098955
Validation loss: 2.5153534231238814

Epoch: 6| Step: 9
Training loss: 2.8078664121721917
Validation loss: 2.5250707169503546

Epoch: 6| Step: 10
Training loss: 3.1965997989517243
Validation loss: 2.547376400011544

Epoch: 6| Step: 11
Training loss: 2.290150458711208
Validation loss: 2.528435983537549

Epoch: 6| Step: 12
Training loss: 2.6752904288664023
Validation loss: 2.5273864994039967

Epoch: 6| Step: 13
Training loss: 2.34900006061942
Validation loss: 2.53771290441864

Epoch: 171| Step: 0
Training loss: 2.242795748733783
Validation loss: 2.5409564813547583

Epoch: 6| Step: 1
Training loss: 2.00044758080012
Validation loss: 2.5340547916748135

Epoch: 6| Step: 2
Training loss: 3.0277532485139402
Validation loss: 2.5365399158890596

Epoch: 6| Step: 3
Training loss: 2.592947410594532
Validation loss: 2.522757030028292

Epoch: 6| Step: 4
Training loss: 1.9393346160608174
Validation loss: 2.5229640866985785

Epoch: 6| Step: 5
Training loss: 2.4603904479393175
Validation loss: 2.5296585825916056

Epoch: 6| Step: 6
Training loss: 2.1549682124508434
Validation loss: 2.5076843419112764

Epoch: 6| Step: 7
Training loss: 2.2981309635768654
Validation loss: 2.4917027272025236

Epoch: 6| Step: 8
Training loss: 2.8906282991957526
Validation loss: 2.4958173253517737

Epoch: 6| Step: 9
Training loss: 2.195680200592501
Validation loss: 2.513704186592218

Epoch: 6| Step: 10
Training loss: 1.3223713729697595
Validation loss: 2.4951453041239327

Epoch: 6| Step: 11
Training loss: 3.1406201414763415
Validation loss: 2.4919448303894023

Epoch: 6| Step: 12
Training loss: 2.387181613324105
Validation loss: 2.490473780873548

Epoch: 6| Step: 13
Training loss: 2.783873617153656
Validation loss: 2.484439241230501

Epoch: 172| Step: 0
Training loss: 2.970251406423099
Validation loss: 2.482600331988884

Epoch: 6| Step: 1
Training loss: 3.167744469846821
Validation loss: 2.488700076895295

Epoch: 6| Step: 2
Training loss: 2.313936972706834
Validation loss: 2.487512091346887

Epoch: 6| Step: 3
Training loss: 2.449956220118896
Validation loss: 2.502217715802585

Epoch: 6| Step: 4
Training loss: 2.0785159553537604
Validation loss: 2.502854799441957

Epoch: 6| Step: 5
Training loss: 1.899551567062491
Validation loss: 2.5468536563283193

Epoch: 6| Step: 6
Training loss: 2.932282054246197
Validation loss: 2.5535910894937186

Epoch: 6| Step: 7
Training loss: 2.234304360293438
Validation loss: 2.597810791313643

Epoch: 6| Step: 8
Training loss: 2.268718738182893
Validation loss: 2.594482670755836

Epoch: 6| Step: 9
Training loss: 2.0013694842837166
Validation loss: 2.5794022141858757

Epoch: 6| Step: 10
Training loss: 2.640524416480704
Validation loss: 2.539692070009784

Epoch: 6| Step: 11
Training loss: 2.8087759157826326
Validation loss: 2.5340496169503828

Epoch: 6| Step: 12
Training loss: 2.500459438068977
Validation loss: 2.514206812328067

Epoch: 6| Step: 13
Training loss: 2.1739232127324963
Validation loss: 2.5092108166460334

Epoch: 173| Step: 0
Training loss: 2.417172795493261
Validation loss: 2.5001194130670896

Epoch: 6| Step: 1
Training loss: 2.4631733743388837
Validation loss: 2.4854262904716027

Epoch: 6| Step: 2
Training loss: 2.06582362248748
Validation loss: 2.486417127016641

Epoch: 6| Step: 3
Training loss: 2.269700797440029
Validation loss: 2.4870540241472847

Epoch: 6| Step: 4
Training loss: 2.549766911344066
Validation loss: 2.4856764865166476

Epoch: 6| Step: 5
Training loss: 2.3662454978065766
Validation loss: 2.4890093174912358

Epoch: 6| Step: 6
Training loss: 3.4035070732024146
Validation loss: 2.4938446084339367

Epoch: 6| Step: 7
Training loss: 2.244424694066706
Validation loss: 2.4933556317370984

Epoch: 6| Step: 8
Training loss: 1.8464600640736843
Validation loss: 2.49315666718612

Epoch: 6| Step: 9
Training loss: 2.0025815991410343
Validation loss: 2.4985097257994715

Epoch: 6| Step: 10
Training loss: 2.486349029333272
Validation loss: 2.4987306392263755

Epoch: 6| Step: 11
Training loss: 2.5078374083045536
Validation loss: 2.4928564132617015

Epoch: 6| Step: 12
Training loss: 2.296456642716653
Validation loss: 2.508519485975819

Epoch: 6| Step: 13
Training loss: 2.9773697847195497
Validation loss: 2.4930298118242082

Epoch: 174| Step: 0
Training loss: 2.2908410983915566
Validation loss: 2.5038859524749735

Epoch: 6| Step: 1
Training loss: 2.2696474344319504
Validation loss: 2.503401302180234

Epoch: 6| Step: 2
Training loss: 2.0726745949748246
Validation loss: 2.514980020235152

Epoch: 6| Step: 3
Training loss: 2.381653150338148
Validation loss: 2.5071922161018794

Epoch: 6| Step: 4
Training loss: 2.216012797563142
Validation loss: 2.523622677270976

Epoch: 6| Step: 5
Training loss: 2.29353515284036
Validation loss: 2.5257148505042566

Epoch: 6| Step: 6
Training loss: 2.8868647681693136
Validation loss: 2.5222456157493847

Epoch: 6| Step: 7
Training loss: 2.395045065087513
Validation loss: 2.562266083252143

Epoch: 6| Step: 8
Training loss: 2.6678368464602658
Validation loss: 2.538847302829646

Epoch: 6| Step: 9
Training loss: 2.132116294945498
Validation loss: 2.5323744580004837

Epoch: 6| Step: 10
Training loss: 2.9413984686152053
Validation loss: 2.53812249414063

Epoch: 6| Step: 11
Training loss: 1.8138855209565974
Validation loss: 2.536159800690799

Epoch: 6| Step: 12
Training loss: 2.1906789296867837
Validation loss: 2.5256328894348696

Epoch: 6| Step: 13
Training loss: 2.881329410807446
Validation loss: 2.5176277244541385

Epoch: 175| Step: 0
Training loss: 2.0905815177010725
Validation loss: 2.5083424928145184

Epoch: 6| Step: 1
Training loss: 2.587066956923557
Validation loss: 2.51869753269422

Epoch: 6| Step: 2
Training loss: 1.9307227195739813
Validation loss: 2.509826239857055

Epoch: 6| Step: 3
Training loss: 2.516637752274227
Validation loss: 2.5153179257710705

Epoch: 6| Step: 4
Training loss: 2.8226303762458453
Validation loss: 2.4958485068452787

Epoch: 6| Step: 5
Training loss: 1.9793612602005144
Validation loss: 2.505434328752142

Epoch: 6| Step: 6
Training loss: 2.559898637761662
Validation loss: 2.5052323898672535

Epoch: 6| Step: 7
Training loss: 2.503104952042551
Validation loss: 2.504957037130268

Epoch: 6| Step: 8
Training loss: 2.221371244042162
Validation loss: 2.487585556373635

Epoch: 6| Step: 9
Training loss: 2.7707656240197425
Validation loss: 2.4977445760013928

Epoch: 6| Step: 10
Training loss: 1.946311479698735
Validation loss: 2.5121184209550904

Epoch: 6| Step: 11
Training loss: 3.048123460917488
Validation loss: 2.494629463757726

Epoch: 6| Step: 12
Training loss: 1.8644483577811986
Validation loss: 2.499815202876629

Epoch: 6| Step: 13
Training loss: 2.524227715408144
Validation loss: 2.504830382791737

Testing loss: 1.9868432278642394

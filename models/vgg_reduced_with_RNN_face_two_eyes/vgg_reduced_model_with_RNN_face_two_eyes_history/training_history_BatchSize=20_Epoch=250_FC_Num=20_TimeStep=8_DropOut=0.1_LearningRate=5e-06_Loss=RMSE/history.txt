Epoch: 1| Step: 0
Training loss: 6.596810304151912
Validation loss: 5.855297897930621

Epoch: 5| Step: 1
Training loss: 5.266653079405693
Validation loss: 5.853377572995294

Epoch: 5| Step: 2
Training loss: 5.209385859633159
Validation loss: 5.851532899096681

Epoch: 5| Step: 3
Training loss: 6.05744235462779
Validation loss: 5.849518877877989

Epoch: 5| Step: 4
Training loss: 6.68195947746588
Validation loss: 5.847603294185441

Epoch: 5| Step: 5
Training loss: 6.179774538143374
Validation loss: 5.845610215417068

Epoch: 5| Step: 6
Training loss: 6.4089701018270695
Validation loss: 5.84346169425234

Epoch: 5| Step: 7
Training loss: 6.316382998469109
Validation loss: 5.8413789940426435

Epoch: 5| Step: 8
Training loss: 6.153900472694699
Validation loss: 5.839086851403186

Epoch: 5| Step: 9
Training loss: 4.30794118329969
Validation loss: 5.836748304306282

Epoch: 5| Step: 10
Training loss: 5.653669074063624
Validation loss: 5.834358420448059

Epoch: 5| Step: 11
Training loss: 7.373135347151313
Validation loss: 5.831955272386615

Epoch: 2| Step: 0
Training loss: 6.1037678072902235
Validation loss: 5.829442936274519

Epoch: 5| Step: 1
Training loss: 5.793210086952851
Validation loss: 5.826643891097249

Epoch: 5| Step: 2
Training loss: 6.057018830399688
Validation loss: 5.823838560751017

Epoch: 5| Step: 3
Training loss: 6.254209397907228
Validation loss: 5.820927263602006

Epoch: 5| Step: 4
Training loss: 4.685446734548897
Validation loss: 5.817931102379706

Epoch: 5| Step: 5
Training loss: 6.169886324189285
Validation loss: 5.814753871354485

Epoch: 5| Step: 6
Training loss: 6.226784887614241
Validation loss: 5.811349515641997

Epoch: 5| Step: 7
Training loss: 6.229661120264877
Validation loss: 5.807802578699784

Epoch: 5| Step: 8
Training loss: 5.4361668848914695
Validation loss: 5.804197500365604

Epoch: 5| Step: 9
Training loss: 6.252544037893758
Validation loss: 5.80034326683056

Epoch: 5| Step: 10
Training loss: 5.759027774670289
Validation loss: 5.7962631903531125

Epoch: 5| Step: 11
Training loss: 6.248609464453496
Validation loss: 5.791780370058126

Epoch: 3| Step: 0
Training loss: 5.702975838160682
Validation loss: 5.787367215154086

Epoch: 5| Step: 1
Training loss: 6.059159135047381
Validation loss: 5.782379665494288

Epoch: 5| Step: 2
Training loss: 6.24729036722106
Validation loss: 5.777371915241785

Epoch: 5| Step: 3
Training loss: 6.130164635393673
Validation loss: 5.772068492852243

Epoch: 5| Step: 4
Training loss: 5.83736981199765
Validation loss: 5.766602279141925

Epoch: 5| Step: 5
Training loss: 6.498617758868336
Validation loss: 5.760888867284615

Epoch: 5| Step: 6
Training loss: 4.743679459239601
Validation loss: 5.754333970521726

Epoch: 5| Step: 7
Training loss: 5.505414724917185
Validation loss: 5.74817867043705

Epoch: 5| Step: 8
Training loss: 5.1575042124003865
Validation loss: 5.741811720128208

Epoch: 5| Step: 9
Training loss: 6.59568085806502
Validation loss: 5.734975854367747

Epoch: 5| Step: 10
Training loss: 5.868447728441915
Validation loss: 5.727779604301733

Epoch: 5| Step: 11
Training loss: 5.953375222862192
Validation loss: 5.720472323128178

Epoch: 4| Step: 0
Training loss: 7.067174664118207
Validation loss: 5.712852299662223

Epoch: 5| Step: 1
Training loss: 5.45061900971911
Validation loss: 5.704764519795568

Epoch: 5| Step: 2
Training loss: 5.794972891690667
Validation loss: 5.6964031861817706

Epoch: 5| Step: 3
Training loss: 5.6043474624380964
Validation loss: 5.6879155283175935

Epoch: 5| Step: 4
Training loss: 6.618541232266585
Validation loss: 5.679380196046011

Epoch: 5| Step: 5
Training loss: 5.837471756468579
Validation loss: 5.670112644323517

Epoch: 5| Step: 6
Training loss: 5.347019556121421
Validation loss: 5.66116337386542

Epoch: 5| Step: 7
Training loss: 5.654508037939556
Validation loss: 5.65151004051503

Epoch: 5| Step: 8
Training loss: 5.728540596710829
Validation loss: 5.64231667551655

Epoch: 5| Step: 9
Training loss: 5.847460542817257
Validation loss: 5.632850805567677

Epoch: 5| Step: 10
Training loss: 4.51246949922604
Validation loss: 5.62301807102477

Epoch: 5| Step: 11
Training loss: 4.789413364619644
Validation loss: 5.613548079563861

Epoch: 5| Step: 0
Training loss: 5.7778026303139915
Validation loss: 5.6036498624426265

Epoch: 5| Step: 1
Training loss: 5.858106959144344
Validation loss: 5.5942856587215095

Epoch: 5| Step: 2
Training loss: 5.784113654153085
Validation loss: 5.5845472050610345

Epoch: 5| Step: 3
Training loss: 5.968725074596189
Validation loss: 5.574885947938266

Epoch: 5| Step: 4
Training loss: 5.694845633085059
Validation loss: 5.565010939944637

Epoch: 5| Step: 5
Training loss: 5.834978834308328
Validation loss: 5.555459456672275

Epoch: 5| Step: 6
Training loss: 5.496176691249613
Validation loss: 5.545980390303792

Epoch: 5| Step: 7
Training loss: 5.733959214107035
Validation loss: 5.536324244323144

Epoch: 5| Step: 8
Training loss: 5.651039379019794
Validation loss: 5.526616096901772

Epoch: 5| Step: 9
Training loss: 4.834972081858243
Validation loss: 5.517137689256667

Epoch: 5| Step: 10
Training loss: 5.700809207161303
Validation loss: 5.507950606542282

Epoch: 5| Step: 11
Training loss: 5.66994643197834
Validation loss: 5.498685412370497

Epoch: 6| Step: 0
Training loss: 4.904115443365944
Validation loss: 5.489760686143663

Epoch: 5| Step: 1
Training loss: 5.899227104640819
Validation loss: 5.48077126372965

Epoch: 5| Step: 2
Training loss: 4.868238601955669
Validation loss: 5.471816868730151

Epoch: 5| Step: 3
Training loss: 6.0650918670263145
Validation loss: 5.462905543714639

Epoch: 5| Step: 4
Training loss: 5.570195222237976
Validation loss: 5.45482375857054

Epoch: 5| Step: 5
Training loss: 5.584483222787995
Validation loss: 5.446371200878769

Epoch: 5| Step: 6
Training loss: 4.367738419451975
Validation loss: 5.438029281755304

Epoch: 5| Step: 7
Training loss: 5.533453723256559
Validation loss: 5.430223324177701

Epoch: 5| Step: 8
Training loss: 6.144766539320587
Validation loss: 5.422107394731877

Epoch: 5| Step: 9
Training loss: 6.101816638533413
Validation loss: 5.414415869985894

Epoch: 5| Step: 10
Training loss: 5.763349544104479
Validation loss: 5.406398403651937

Epoch: 5| Step: 11
Training loss: 6.135919296668949
Validation loss: 5.398412237922532

Epoch: 7| Step: 0
Training loss: 5.645486781145958
Validation loss: 5.3908309823455305

Epoch: 5| Step: 1
Training loss: 5.71452000001456
Validation loss: 5.383074469390267

Epoch: 5| Step: 2
Training loss: 5.501226635176663
Validation loss: 5.37550713859829

Epoch: 5| Step: 3
Training loss: 5.450380700489961
Validation loss: 5.368309329461942

Epoch: 5| Step: 4
Training loss: 5.042935277471698
Validation loss: 5.360980646932089

Epoch: 5| Step: 5
Training loss: 5.857460299140393
Validation loss: 5.353851934996431

Epoch: 5| Step: 6
Training loss: 6.215722041765404
Validation loss: 5.346839732847689

Epoch: 5| Step: 7
Training loss: 5.098213907775788
Validation loss: 5.339890154972025

Epoch: 5| Step: 8
Training loss: 5.1400776951480225
Validation loss: 5.333816978261128

Epoch: 5| Step: 9
Training loss: 5.2327772748142545
Validation loss: 5.327464207219158

Epoch: 5| Step: 10
Training loss: 5.340429512495197
Validation loss: 5.321269833796102

Epoch: 5| Step: 11
Training loss: 4.932804820816303
Validation loss: 5.315703282722666

Epoch: 8| Step: 0
Training loss: 5.277763886461325
Validation loss: 5.309775278301599

Epoch: 5| Step: 1
Training loss: 5.58901197214938
Validation loss: 5.304407515929756

Epoch: 5| Step: 2
Training loss: 4.2692710189878245
Validation loss: 5.299366150446831

Epoch: 5| Step: 3
Training loss: 5.656952029335853
Validation loss: 5.294257636053021

Epoch: 5| Step: 4
Training loss: 5.374948900556521
Validation loss: 5.289093348057698

Epoch: 5| Step: 5
Training loss: 5.556969797477589
Validation loss: 5.283854840991292

Epoch: 5| Step: 6
Training loss: 5.462757598599327
Validation loss: 5.278655662082078

Epoch: 5| Step: 7
Training loss: 5.252075330079258
Validation loss: 5.273780174895395

Epoch: 5| Step: 8
Training loss: 4.787367553472474
Validation loss: 5.268582548268743

Epoch: 5| Step: 9
Training loss: 5.5628007100292525
Validation loss: 5.2634658763730355

Epoch: 5| Step: 10
Training loss: 6.218131029772368
Validation loss: 5.258707757225082

Epoch: 5| Step: 11
Training loss: 6.444598682063265
Validation loss: 5.253631811525702

Epoch: 9| Step: 0
Training loss: 5.9349099382671175
Validation loss: 5.248652186786382

Epoch: 5| Step: 1
Training loss: 4.491482833657939
Validation loss: 5.243330344380964

Epoch: 5| Step: 2
Training loss: 6.156030331482998
Validation loss: 5.238506140682962

Epoch: 5| Step: 3
Training loss: 5.650317373898678
Validation loss: 5.233882193829286

Epoch: 5| Step: 4
Training loss: 5.3127423399459435
Validation loss: 5.2292295834630504

Epoch: 5| Step: 5
Training loss: 5.857570034492492
Validation loss: 5.224579963957813

Epoch: 5| Step: 6
Training loss: 4.579514224348022
Validation loss: 5.2198635596010785

Epoch: 5| Step: 7
Training loss: 5.788983755052329
Validation loss: 5.215274452968079

Epoch: 5| Step: 8
Training loss: 5.3054200510737095
Validation loss: 5.210743417704811

Epoch: 5| Step: 9
Training loss: 5.151283045077027
Validation loss: 5.205887405817333

Epoch: 5| Step: 10
Training loss: 4.35040169099899
Validation loss: 5.201853874665062

Epoch: 5| Step: 11
Training loss: 4.845463012995015
Validation loss: 5.197306460210934

Epoch: 10| Step: 0
Training loss: 5.190309246456567
Validation loss: 5.193002823182755

Epoch: 5| Step: 1
Training loss: 5.789132348178487
Validation loss: 5.188664929832048

Epoch: 5| Step: 2
Training loss: 5.630656640633658
Validation loss: 5.184437541036419

Epoch: 5| Step: 3
Training loss: 4.917046785477658
Validation loss: 5.179847281375976

Epoch: 5| Step: 4
Training loss: 5.7971987569630885
Validation loss: 5.17548056442822

Epoch: 5| Step: 5
Training loss: 5.8841747312657535
Validation loss: 5.1710850937401975

Epoch: 5| Step: 6
Training loss: 5.586141003723103
Validation loss: 5.166239392650959

Epoch: 5| Step: 7
Training loss: 5.103147111647403
Validation loss: 5.1617965228175935

Epoch: 5| Step: 8
Training loss: 5.082608259842821
Validation loss: 5.1571365904320245

Epoch: 5| Step: 9
Training loss: 4.9337054773819675
Validation loss: 5.1526116298383835

Epoch: 5| Step: 10
Training loss: 4.320036122206384
Validation loss: 5.14808123315359

Epoch: 5| Step: 11
Training loss: 4.229655771520496
Validation loss: 5.143755005300627

Epoch: 11| Step: 0
Training loss: 4.623510584845008
Validation loss: 5.13967476268523

Epoch: 5| Step: 1
Training loss: 5.91666974707868
Validation loss: 5.135669487951648

Epoch: 5| Step: 2
Training loss: 5.3560974954017615
Validation loss: 5.131293735480388

Epoch: 5| Step: 3
Training loss: 5.063593051788049
Validation loss: 5.126648885752314

Epoch: 5| Step: 4
Training loss: 5.069911100371862
Validation loss: 5.121878425390632

Epoch: 5| Step: 5
Training loss: 4.590486989501359
Validation loss: 5.1175575413751115

Epoch: 5| Step: 6
Training loss: 5.867595145417528
Validation loss: 5.112761865265672

Epoch: 5| Step: 7
Training loss: 4.409505304456419
Validation loss: 5.107797237142774

Epoch: 5| Step: 8
Training loss: 5.1101096088874405
Validation loss: 5.1035262062847755

Epoch: 5| Step: 9
Training loss: 5.8268360829692964
Validation loss: 5.098415196461848

Epoch: 5| Step: 10
Training loss: 5.582579813351218
Validation loss: 5.0936539763778566

Epoch: 5| Step: 11
Training loss: 5.106842439944755
Validation loss: 5.088579209604707

Epoch: 12| Step: 0
Training loss: 5.42684330717588
Validation loss: 5.083694994938175

Epoch: 5| Step: 1
Training loss: 4.508316408549827
Validation loss: 5.078320833000286

Epoch: 5| Step: 2
Training loss: 5.854528268889023
Validation loss: 5.073450623074215

Epoch: 5| Step: 3
Training loss: 5.17748737357611
Validation loss: 5.068181072374494

Epoch: 5| Step: 4
Training loss: 4.669369232905353
Validation loss: 5.063272723340997

Epoch: 5| Step: 5
Training loss: 5.30364287797544
Validation loss: 5.058238468925065

Epoch: 5| Step: 6
Training loss: 5.106943094193824
Validation loss: 5.05317462526393

Epoch: 5| Step: 7
Training loss: 5.331302892102807
Validation loss: 5.047957210821396

Epoch: 5| Step: 8
Training loss: 5.729033349537877
Validation loss: 5.042768967401909

Epoch: 5| Step: 9
Training loss: 4.948334793359566
Validation loss: 5.03722177246723

Epoch: 5| Step: 10
Training loss: 4.883671994666437
Validation loss: 5.032011191716002

Epoch: 5| Step: 11
Training loss: 4.775646641271227
Validation loss: 5.0267414324020825

Epoch: 13| Step: 0
Training loss: 5.343597588401243
Validation loss: 5.02169861466941

Epoch: 5| Step: 1
Training loss: 5.354139078071654
Validation loss: 5.016055856105739

Epoch: 5| Step: 2
Training loss: 4.708055921275928
Validation loss: 5.01100318896562

Epoch: 5| Step: 3
Training loss: 5.301578200384761
Validation loss: 5.00584814274054

Epoch: 5| Step: 4
Training loss: 4.908859433711807
Validation loss: 5.000977365019077

Epoch: 5| Step: 5
Training loss: 5.519679667535259
Validation loss: 4.996229243982598

Epoch: 5| Step: 6
Training loss: 4.797692046026492
Validation loss: 4.9907543132974626

Epoch: 5| Step: 7
Training loss: 4.677159207527949
Validation loss: 4.98618282271269

Epoch: 5| Step: 8
Training loss: 5.554693852459887
Validation loss: 4.9807272052398694

Epoch: 5| Step: 9
Training loss: 5.21692721576456
Validation loss: 4.976022608209912

Epoch: 5| Step: 10
Training loss: 4.819159124004476
Validation loss: 4.9714497523229815

Epoch: 5| Step: 11
Training loss: 5.327184868258795
Validation loss: 4.9667365915168

Epoch: 14| Step: 0
Training loss: 4.506741665962848
Validation loss: 4.962030805928554

Epoch: 5| Step: 1
Training loss: 3.958368374853462
Validation loss: 4.957139579404897

Epoch: 5| Step: 2
Training loss: 5.5430416992204625
Validation loss: 4.952048063279646

Epoch: 5| Step: 3
Training loss: 4.827213534642238
Validation loss: 4.9475843565593705

Epoch: 5| Step: 4
Training loss: 4.5518132296226925
Validation loss: 4.94271165092633

Epoch: 5| Step: 5
Training loss: 4.392073314353624
Validation loss: 4.938287157174775

Epoch: 5| Step: 6
Training loss: 5.255381414652657
Validation loss: 4.933367855148844

Epoch: 5| Step: 7
Training loss: 5.214857282871018
Validation loss: 4.928928790421602

Epoch: 5| Step: 8
Training loss: 5.266497531219197
Validation loss: 4.923983708523331

Epoch: 5| Step: 9
Training loss: 5.833934680278805
Validation loss: 4.919162483159701

Epoch: 5| Step: 10
Training loss: 5.843348525020844
Validation loss: 4.914195800331771

Epoch: 5| Step: 11
Training loss: 5.661148226567121
Validation loss: 4.909440811140246

Epoch: 15| Step: 0
Training loss: 4.620726802183821
Validation loss: 4.904370006450005

Epoch: 5| Step: 1
Training loss: 5.375286715645094
Validation loss: 4.898990435626568

Epoch: 5| Step: 2
Training loss: 4.898701102481665
Validation loss: 4.894153049609619

Epoch: 5| Step: 3
Training loss: 5.064287035094686
Validation loss: 4.889151963768353

Epoch: 5| Step: 4
Training loss: 4.625449906194726
Validation loss: 4.884392184249448

Epoch: 5| Step: 5
Training loss: 5.349340893796812
Validation loss: 4.879163920040427

Epoch: 5| Step: 6
Training loss: 5.425435680702468
Validation loss: 4.87387250430781

Epoch: 5| Step: 7
Training loss: 4.784635255438798
Validation loss: 4.868376838117935

Epoch: 5| Step: 8
Training loss: 4.973575575982511
Validation loss: 4.863590158152835

Epoch: 5| Step: 9
Training loss: 4.796179506660215
Validation loss: 4.8587070876950635

Epoch: 5| Step: 10
Training loss: 5.222084487739999
Validation loss: 4.853338157228721

Epoch: 5| Step: 11
Training loss: 4.125276267308192
Validation loss: 4.848226320579128

Epoch: 16| Step: 0
Training loss: 4.855045659236532
Validation loss: 4.843275781203331

Epoch: 5| Step: 1
Training loss: 5.781876153802514
Validation loss: 4.838103409688466

Epoch: 5| Step: 2
Training loss: 5.256421657666789
Validation loss: 4.832375911179219

Epoch: 5| Step: 3
Training loss: 5.016309079493195
Validation loss: 4.827488047570015

Epoch: 5| Step: 4
Training loss: 4.4983008673675196
Validation loss: 4.822245873201815

Epoch: 5| Step: 5
Training loss: 4.218182787028114
Validation loss: 4.816770693976698

Epoch: 5| Step: 6
Training loss: 4.6299356466809805
Validation loss: 4.812296289203141

Epoch: 5| Step: 7
Training loss: 4.859852463794414
Validation loss: 4.806787981763347

Epoch: 5| Step: 8
Training loss: 4.823398288657634
Validation loss: 4.8014952353969225

Epoch: 5| Step: 9
Training loss: 4.5675986032851466
Validation loss: 4.796667361658155

Epoch: 5| Step: 10
Training loss: 5.805255928805015
Validation loss: 4.791216541654153

Epoch: 5| Step: 11
Training loss: 3.9545054079935946
Validation loss: 4.786057293196282

Epoch: 17| Step: 0
Training loss: 4.3519236553452
Validation loss: 4.780972919455503

Epoch: 5| Step: 1
Training loss: 4.869017426928697
Validation loss: 4.775453090974265

Epoch: 5| Step: 2
Training loss: 4.142578585425481
Validation loss: 4.77063731105953

Epoch: 5| Step: 3
Training loss: 4.4638211471797105
Validation loss: 4.765113211077783

Epoch: 5| Step: 4
Training loss: 5.219071064284127
Validation loss: 4.760804077789483

Epoch: 5| Step: 5
Training loss: 4.77488514227811
Validation loss: 4.755793652090152

Epoch: 5| Step: 6
Training loss: 5.2082637120997815
Validation loss: 4.750390839810018

Epoch: 5| Step: 7
Training loss: 5.447797823813802
Validation loss: 4.744866441401987

Epoch: 5| Step: 8
Training loss: 5.070579392472443
Validation loss: 4.739179014070562

Epoch: 5| Step: 9
Training loss: 5.412979083629868
Validation loss: 4.733779710331078

Epoch: 5| Step: 10
Training loss: 4.766135266611584
Validation loss: 4.727729783534889

Epoch: 5| Step: 11
Training loss: 3.564061675851368
Validation loss: 4.722617141565911

Epoch: 18| Step: 0
Training loss: 5.509993750262307
Validation loss: 4.717721654404712

Epoch: 5| Step: 1
Training loss: 4.426286740908698
Validation loss: 4.711926956817957

Epoch: 5| Step: 2
Training loss: 4.282018884822961
Validation loss: 4.706245016911854

Epoch: 5| Step: 3
Training loss: 4.184002910591119
Validation loss: 4.700741054909771

Epoch: 5| Step: 4
Training loss: 4.143408649360536
Validation loss: 4.695855911638374

Epoch: 5| Step: 5
Training loss: 4.879220944739835
Validation loss: 4.690529586021491

Epoch: 5| Step: 6
Training loss: 4.942599017666564
Validation loss: 4.685551310371906

Epoch: 5| Step: 7
Training loss: 5.242027405342267
Validation loss: 4.679735848968336

Epoch: 5| Step: 8
Training loss: 5.446395087753394
Validation loss: 4.674479138330939

Epoch: 5| Step: 9
Training loss: 4.700219762006584
Validation loss: 4.669021857950107

Epoch: 5| Step: 10
Training loss: 5.015966100769847
Validation loss: 4.663261101912937

Epoch: 5| Step: 11
Training loss: 4.696603302448101
Validation loss: 4.657802954332018

Epoch: 19| Step: 0
Training loss: 5.038945916771526
Validation loss: 4.651830500240779

Epoch: 5| Step: 1
Training loss: 4.382584237803129
Validation loss: 4.6461054361599565

Epoch: 5| Step: 2
Training loss: 4.014683475772969
Validation loss: 4.640358699945638

Epoch: 5| Step: 3
Training loss: 4.890995876886895
Validation loss: 4.635567830510578

Epoch: 5| Step: 4
Training loss: 5.317973469978257
Validation loss: 4.629724812730092

Epoch: 5| Step: 5
Training loss: 4.665428587758117
Validation loss: 4.624391189232703

Epoch: 5| Step: 6
Training loss: 5.151877844209359
Validation loss: 4.618801938437265

Epoch: 5| Step: 7
Training loss: 4.23296352383195
Validation loss: 4.613299545006325

Epoch: 5| Step: 8
Training loss: 5.463413097613149
Validation loss: 4.608314288665067

Epoch: 5| Step: 9
Training loss: 4.60007420355614
Validation loss: 4.602293473513417

Epoch: 5| Step: 10
Training loss: 4.404930837133727
Validation loss: 4.596742688869652

Epoch: 5| Step: 11
Training loss: 4.164211134557053
Validation loss: 4.590858137435689

Epoch: 20| Step: 0
Training loss: 4.6746113906337055
Validation loss: 4.58533667073309

Epoch: 5| Step: 1
Training loss: 4.177903295439583
Validation loss: 4.579926692842398

Epoch: 5| Step: 2
Training loss: 4.554362525928093
Validation loss: 4.574889965853424

Epoch: 5| Step: 3
Training loss: 4.446198011538904
Validation loss: 4.569526800452506

Epoch: 5| Step: 4
Training loss: 5.291461710327341
Validation loss: 4.564480373533566

Epoch: 5| Step: 5
Training loss: 4.366237912471191
Validation loss: 4.558671668965855

Epoch: 5| Step: 6
Training loss: 4.628471927145188
Validation loss: 4.5541913049606695

Epoch: 5| Step: 7
Training loss: 4.840677184974315
Validation loss: 4.5481408706855735

Epoch: 5| Step: 8
Training loss: 4.6402733666363085
Validation loss: 4.542324603250797

Epoch: 5| Step: 9
Training loss: 5.032832495085048
Validation loss: 4.537567470248581

Epoch: 5| Step: 10
Training loss: 4.820231778598832
Validation loss: 4.531282946587411

Epoch: 5| Step: 11
Training loss: 4.606848486602921
Validation loss: 4.525638394082422

Epoch: 21| Step: 0
Training loss: 3.971415188013091
Validation loss: 4.519826463417549

Epoch: 5| Step: 1
Training loss: 5.225760131738518
Validation loss: 4.515492640824724

Epoch: 5| Step: 2
Training loss: 4.454512530543115
Validation loss: 4.509913179283679

Epoch: 5| Step: 3
Training loss: 4.997758935323947
Validation loss: 4.504485415926216

Epoch: 5| Step: 4
Training loss: 4.763385784446786
Validation loss: 4.499357628007861

Epoch: 5| Step: 5
Training loss: 5.336435409259375
Validation loss: 4.492761751042067

Epoch: 5| Step: 6
Training loss: 4.044128664379106
Validation loss: 4.4874214563960795

Epoch: 5| Step: 7
Training loss: 4.20999299109956
Validation loss: 4.482528926814891

Epoch: 5| Step: 8
Training loss: 4.62887349177491
Validation loss: 4.476444316056238

Epoch: 5| Step: 9
Training loss: 5.028761348915427
Validation loss: 4.470708075715481

Epoch: 5| Step: 10
Training loss: 4.258517985561363
Validation loss: 4.465170086593166

Epoch: 5| Step: 11
Training loss: 2.632073997813941
Validation loss: 4.459025207501892

Epoch: 22| Step: 0
Training loss: 4.088568983371457
Validation loss: 4.45461711312799

Epoch: 5| Step: 1
Training loss: 4.8289466331631274
Validation loss: 4.4495152795234

Epoch: 5| Step: 2
Training loss: 5.023193353442056
Validation loss: 4.444198960571342

Epoch: 5| Step: 3
Training loss: 4.434711708581791
Validation loss: 4.43730065520359

Epoch: 5| Step: 4
Training loss: 3.87951532443511
Validation loss: 4.43267599457484

Epoch: 5| Step: 5
Training loss: 5.4869476109849415
Validation loss: 4.4281242882102765

Epoch: 5| Step: 6
Training loss: 2.9785177061980432
Validation loss: 4.421402882420586

Epoch: 5| Step: 7
Training loss: 4.955246335709114
Validation loss: 4.415680238342699

Epoch: 5| Step: 8
Training loss: 5.139561134334764
Validation loss: 4.410544365698114

Epoch: 5| Step: 9
Training loss: 4.760490077459241
Validation loss: 4.40537028627988

Epoch: 5| Step: 10
Training loss: 3.857959564977954
Validation loss: 4.399194768534182

Epoch: 5| Step: 11
Training loss: 5.03123521506465
Validation loss: 4.393856696456273

Epoch: 23| Step: 0
Training loss: 4.832918719343242
Validation loss: 4.387544884230228

Epoch: 5| Step: 1
Training loss: 4.471687685111051
Validation loss: 4.382663635880904

Epoch: 5| Step: 2
Training loss: 3.9131065312893956
Validation loss: 4.376920187861812

Epoch: 5| Step: 3
Training loss: 4.578892562108977
Validation loss: 4.371082822929772

Epoch: 5| Step: 4
Training loss: 4.151882774050589
Validation loss: 4.365238926463418

Epoch: 5| Step: 5
Training loss: 4.914638273291311
Validation loss: 4.359188914457671

Epoch: 5| Step: 6
Training loss: 4.566328558738153
Validation loss: 4.353630045018565

Epoch: 5| Step: 7
Training loss: 4.645565601155436
Validation loss: 4.3478061893515765

Epoch: 5| Step: 8
Training loss: 4.416141742725849
Validation loss: 4.342291754145607

Epoch: 5| Step: 9
Training loss: 5.240376689853746
Validation loss: 4.336706227384289

Epoch: 5| Step: 10
Training loss: 3.607424122097635
Validation loss: 4.3305820234407575

Epoch: 5| Step: 11
Training loss: 3.409780090490137
Validation loss: 4.324711726436167

Epoch: 24| Step: 0
Training loss: 3.8723049020598315
Validation loss: 4.3203337785542875

Epoch: 5| Step: 1
Training loss: 4.469026356934974
Validation loss: 4.316374323800199

Epoch: 5| Step: 2
Training loss: 4.461122632569325
Validation loss: 4.309412039564631

Epoch: 5| Step: 3
Training loss: 4.418881107095716
Validation loss: 4.30245891837368

Epoch: 5| Step: 4
Training loss: 4.36392685255641
Validation loss: 4.298193443781068

Epoch: 5| Step: 5
Training loss: 4.642813940161611
Validation loss: 4.292393301895025

Epoch: 5| Step: 6
Training loss: 4.628608533081476
Validation loss: 4.2865193603299705

Epoch: 5| Step: 7
Training loss: 4.183484785725003
Validation loss: 4.280889519135925

Epoch: 5| Step: 8
Training loss: 4.620974876957952
Validation loss: 4.2759350923305

Epoch: 5| Step: 9
Training loss: 4.801285198135288
Validation loss: 4.269880210155069

Epoch: 5| Step: 10
Training loss: 3.840703982355074
Validation loss: 4.262544560688935

Epoch: 5| Step: 11
Training loss: 5.452643692942672
Validation loss: 4.257009205300508

Epoch: 25| Step: 0
Training loss: 4.673688759740512
Validation loss: 4.252413620644299

Epoch: 5| Step: 1
Training loss: 4.340012074115324
Validation loss: 4.246157947793112

Epoch: 5| Step: 2
Training loss: 4.415314353387921
Validation loss: 4.239846189381165

Epoch: 5| Step: 3
Training loss: 4.256276600848409
Validation loss: 4.2327037154711356

Epoch: 5| Step: 4
Training loss: 4.456034462786943
Validation loss: 4.227388518653379

Epoch: 5| Step: 5
Training loss: 4.533235101944521
Validation loss: 4.222164063025231

Epoch: 5| Step: 6
Training loss: 4.718202066503346
Validation loss: 4.216269378391386

Epoch: 5| Step: 7
Training loss: 4.027133229448648
Validation loss: 4.209192639926992

Epoch: 5| Step: 8
Training loss: 4.7281332133159415
Validation loss: 4.202598056591

Epoch: 5| Step: 9
Training loss: 3.585666887298981
Validation loss: 4.197410655544775

Epoch: 5| Step: 10
Training loss: 4.246440968007843
Validation loss: 4.191139722291626

Epoch: 5| Step: 11
Training loss: 3.054621937235991
Validation loss: 4.18506186946589

Epoch: 26| Step: 0
Training loss: 4.287627211037601
Validation loss: 4.179907529749622

Epoch: 5| Step: 1
Training loss: 3.825670101334296
Validation loss: 4.173432424968993

Epoch: 5| Step: 2
Training loss: 4.282349382681376
Validation loss: 4.168045195784479

Epoch: 5| Step: 3
Training loss: 4.855684406036433
Validation loss: 4.16174654704591

Epoch: 5| Step: 4
Training loss: 3.747543929378882
Validation loss: 4.1558024277783145

Epoch: 5| Step: 5
Training loss: 4.317217471846739
Validation loss: 4.1512189976408385

Epoch: 5| Step: 6
Training loss: 4.177160679330205
Validation loss: 4.144778772477716

Epoch: 5| Step: 7
Training loss: 4.507864543541645
Validation loss: 4.138275151473003

Epoch: 5| Step: 8
Training loss: 4.626810698855313
Validation loss: 4.133148040322699

Epoch: 5| Step: 9
Training loss: 4.072263048935037
Validation loss: 4.127076753280198

Epoch: 5| Step: 10
Training loss: 4.308283195474414
Validation loss: 4.1208490042907595

Epoch: 5| Step: 11
Training loss: 4.184880362456171
Validation loss: 4.112933408321614

Epoch: 27| Step: 0
Training loss: 3.6153591470398845
Validation loss: 4.109543660308323

Epoch: 5| Step: 1
Training loss: 4.373755795806682
Validation loss: 4.103939169502309

Epoch: 5| Step: 2
Training loss: 4.010650997829605
Validation loss: 4.096559594159365

Epoch: 5| Step: 3
Training loss: 4.894169019978587
Validation loss: 4.090149516735856

Epoch: 5| Step: 4
Training loss: 3.9750995455579132
Validation loss: 4.083738008545176

Epoch: 5| Step: 5
Training loss: 4.530528043913805
Validation loss: 4.078552423442755

Epoch: 5| Step: 6
Training loss: 4.648859700495658
Validation loss: 4.071251475952993

Epoch: 5| Step: 7
Training loss: 3.6258614930485775
Validation loss: 4.066226795254234

Epoch: 5| Step: 8
Training loss: 4.273131755373253
Validation loss: 4.0617831747046775

Epoch: 5| Step: 9
Training loss: 3.9538065809023855
Validation loss: 4.05509327822361

Epoch: 5| Step: 10
Training loss: 4.39254165030432
Validation loss: 4.046879413656334

Epoch: 5| Step: 11
Training loss: 3.307390843303623
Validation loss: 4.041110126612772

Epoch: 28| Step: 0
Training loss: 3.5003637397311733
Validation loss: 4.037354320224549

Epoch: 5| Step: 1
Training loss: 4.3547264116545055
Validation loss: 4.031046652778561

Epoch: 5| Step: 2
Training loss: 4.306862062050885
Validation loss: 4.024741929826039

Epoch: 5| Step: 3
Training loss: 3.5294187405461206
Validation loss: 4.0188388308392025

Epoch: 5| Step: 4
Training loss: 3.567297131678772
Validation loss: 4.014300621516053

Epoch: 5| Step: 5
Training loss: 5.0920637651213205
Validation loss: 4.0103827173686915

Epoch: 5| Step: 6
Training loss: 4.071135278525474
Validation loss: 4.00327113627054

Epoch: 5| Step: 7
Training loss: 4.580251362661881
Validation loss: 3.996450965703671

Epoch: 5| Step: 8
Training loss: 3.9627303480710667
Validation loss: 3.9911320455078108

Epoch: 5| Step: 9
Training loss: 3.7040934523622657
Validation loss: 3.9857009277162225

Epoch: 5| Step: 10
Training loss: 4.392780684316363
Validation loss: 3.979077661061427

Epoch: 5| Step: 11
Training loss: 4.898948338594596
Validation loss: 3.972083926664838

Epoch: 29| Step: 0
Training loss: 3.798353621554344
Validation loss: 3.9662546137492276

Epoch: 5| Step: 1
Training loss: 4.078432268866129
Validation loss: 3.9607111799100254

Epoch: 5| Step: 2
Training loss: 3.42754949087393
Validation loss: 3.955165828784208

Epoch: 5| Step: 3
Training loss: 4.781747231153232
Validation loss: 3.948647346752235

Epoch: 5| Step: 4
Training loss: 4.2365361290567005
Validation loss: 3.941835892306246

Epoch: 5| Step: 5
Training loss: 4.544241125962472
Validation loss: 3.936222373725741

Epoch: 5| Step: 6
Training loss: 4.071695572361025
Validation loss: 3.929671159208487

Epoch: 5| Step: 7
Training loss: 3.5955606957634503
Validation loss: 3.9229275480265673

Epoch: 5| Step: 8
Training loss: 4.454869407169701
Validation loss: 3.917080485349627

Epoch: 5| Step: 9
Training loss: 3.7639554069869283
Validation loss: 3.91162284102159

Epoch: 5| Step: 10
Training loss: 3.79991436410079
Validation loss: 3.9054586142927166

Epoch: 5| Step: 11
Training loss: 4.071610081004
Validation loss: 3.8995167392071926

Epoch: 30| Step: 0
Training loss: 4.225462370181669
Validation loss: 3.8917548851017973

Epoch: 5| Step: 1
Training loss: 3.8400079142965775
Validation loss: 3.886077792226531

Epoch: 5| Step: 2
Training loss: 3.7722430184761286
Validation loss: 3.880242610839732

Epoch: 5| Step: 3
Training loss: 4.117643559077723
Validation loss: 3.873966130393007

Epoch: 5| Step: 4
Training loss: 4.475605647696889
Validation loss: 3.8664911895566725

Epoch: 5| Step: 5
Training loss: 3.6451309544779003
Validation loss: 3.8592056000426793

Epoch: 5| Step: 6
Training loss: 4.679362239583326
Validation loss: 3.853786948116935

Epoch: 5| Step: 7
Training loss: 4.339114782878995
Validation loss: 3.8474157835923095

Epoch: 5| Step: 8
Training loss: 3.269176833109169
Validation loss: 3.8410571882288775

Epoch: 5| Step: 9
Training loss: 3.424695529073776
Validation loss: 3.8338136147502193

Epoch: 5| Step: 10
Training loss: 3.66597478292047
Validation loss: 3.8280726993647076

Epoch: 5| Step: 11
Training loss: 5.064786606536447
Validation loss: 3.822213602857211

Epoch: 31| Step: 0
Training loss: 4.329587833795138
Validation loss: 3.815345051769498

Epoch: 5| Step: 1
Training loss: 3.9676634735485132
Validation loss: 3.8088969783219166

Epoch: 5| Step: 2
Training loss: 4.024226257637143
Validation loss: 3.8017605927629723

Epoch: 5| Step: 3
Training loss: 3.4281457029289544
Validation loss: 3.795554094829534

Epoch: 5| Step: 4
Training loss: 3.7071012243132353
Validation loss: 3.7886668624658606

Epoch: 5| Step: 5
Training loss: 4.130355825913456
Validation loss: 3.782679416353869

Epoch: 5| Step: 6
Training loss: 3.5417373350984227
Validation loss: 3.7757976619764477

Epoch: 5| Step: 7
Training loss: 4.045428043157877
Validation loss: 3.770068215715125

Epoch: 5| Step: 8
Training loss: 4.244701223833576
Validation loss: 3.7629520379172017

Epoch: 5| Step: 9
Training loss: 3.7409171415002382
Validation loss: 3.757768135046836

Epoch: 5| Step: 10
Training loss: 3.964324887154569
Validation loss: 3.751930211593135

Epoch: 5| Step: 11
Training loss: 3.264543125965571
Validation loss: 3.745854576624858

Epoch: 32| Step: 0
Training loss: 3.702042948699887
Validation loss: 3.7408642377119987

Epoch: 5| Step: 1
Training loss: 3.9471131700673765
Validation loss: 3.7346886273022952

Epoch: 5| Step: 2
Training loss: 4.39975362434717
Validation loss: 3.7293956231668517

Epoch: 5| Step: 3
Training loss: 4.022495196465848
Validation loss: 3.72332629017575

Epoch: 5| Step: 4
Training loss: 4.102425039514309
Validation loss: 3.717153719491587

Epoch: 5| Step: 5
Training loss: 3.7317357652839536
Validation loss: 3.711474345677035

Epoch: 5| Step: 6
Training loss: 3.7504306545928388
Validation loss: 3.7048312144973514

Epoch: 5| Step: 7
Training loss: 3.7442529509265876
Validation loss: 3.699964319950623

Epoch: 5| Step: 8
Training loss: 3.929226615073574
Validation loss: 3.695138374726385

Epoch: 5| Step: 9
Training loss: 3.3883432894096206
Validation loss: 3.6895706537690462

Epoch: 5| Step: 10
Training loss: 3.5072364928926074
Validation loss: 3.683126479952925

Epoch: 5| Step: 11
Training loss: 3.8730138795556193
Validation loss: 3.677163556370214

Epoch: 33| Step: 0
Training loss: 3.7753207796743466
Validation loss: 3.672121172289103

Epoch: 5| Step: 1
Training loss: 3.7187653869823474
Validation loss: 3.6669405632601166

Epoch: 5| Step: 2
Training loss: 3.6913731810688097
Validation loss: 3.6615043391253628

Epoch: 5| Step: 3
Training loss: 4.355791485380278
Validation loss: 3.6560255237150283

Epoch: 5| Step: 4
Training loss: 3.373269096436372
Validation loss: 3.650289937918917

Epoch: 5| Step: 5
Training loss: 4.099979698316972
Validation loss: 3.64453402481982

Epoch: 5| Step: 6
Training loss: 3.87102994279212
Validation loss: 3.6383680679898833

Epoch: 5| Step: 7
Training loss: 3.8320647227529543
Validation loss: 3.633421269207189

Epoch: 5| Step: 8
Training loss: 3.443229478105054
Validation loss: 3.6273807348200338

Epoch: 5| Step: 9
Training loss: 4.050781720859423
Validation loss: 3.622263774571734

Epoch: 5| Step: 10
Training loss: 3.4094073859616287
Validation loss: 3.6171292601717386

Epoch: 5| Step: 11
Training loss: 2.891096746767552
Validation loss: 3.611281649625913

Epoch: 34| Step: 0
Training loss: 4.094253392609636
Validation loss: 3.607126804601156

Epoch: 5| Step: 1
Training loss: 3.5702805799397095
Validation loss: 3.601004821453488

Epoch: 5| Step: 2
Training loss: 4.127476266830853
Validation loss: 3.5959625924100376

Epoch: 5| Step: 3
Training loss: 3.6895667281262696
Validation loss: 3.5897169453969155

Epoch: 5| Step: 4
Training loss: 3.4668611074595312
Validation loss: 3.584070710156627

Epoch: 5| Step: 5
Training loss: 3.3785196954193455
Validation loss: 3.578747990774536

Epoch: 5| Step: 6
Training loss: 3.5039661279025345
Validation loss: 3.573567332122203

Epoch: 5| Step: 7
Training loss: 3.4546379813894386
Validation loss: 3.5684515891211936

Epoch: 5| Step: 8
Training loss: 4.3080664802316715
Validation loss: 3.563018922768564

Epoch: 5| Step: 9
Training loss: 3.4606842568360228
Validation loss: 3.5574702008166112

Epoch: 5| Step: 10
Training loss: 3.69772176452796
Validation loss: 3.5519265320177906

Epoch: 5| Step: 11
Training loss: 3.6635122313016075
Validation loss: 3.546836175019888

Epoch: 35| Step: 0
Training loss: 3.5773961020256864
Validation loss: 3.5426447116276543

Epoch: 5| Step: 1
Training loss: 3.4590612970519468
Validation loss: 3.537010103009058

Epoch: 5| Step: 2
Training loss: 3.4969143208192675
Validation loss: 3.531255255051025

Epoch: 5| Step: 3
Training loss: 3.6493002547396896
Validation loss: 3.5270766068717565

Epoch: 5| Step: 4
Training loss: 3.673526084282884
Validation loss: 3.5218443895446567

Epoch: 5| Step: 5
Training loss: 3.9276577692134773
Validation loss: 3.5171555670033268

Epoch: 5| Step: 6
Training loss: 3.997368185173454
Validation loss: 3.5116951120929687

Epoch: 5| Step: 7
Training loss: 3.6080063375852967
Validation loss: 3.5065289183009423

Epoch: 5| Step: 8
Training loss: 3.294831279782236
Validation loss: 3.501353592480138

Epoch: 5| Step: 9
Training loss: 3.7063557498763005
Validation loss: 3.4957127596158686

Epoch: 5| Step: 10
Training loss: 3.8892799105169935
Validation loss: 3.4903332412171304

Epoch: 5| Step: 11
Training loss: 2.8985075826474973
Validation loss: 3.4853570178655384

Epoch: 36| Step: 0
Training loss: 2.8104256503727676
Validation loss: 3.48158443791782

Epoch: 5| Step: 1
Training loss: 3.5115029817177703
Validation loss: 3.476373994552955

Epoch: 5| Step: 2
Training loss: 3.7757056075762856
Validation loss: 3.471346356019917

Epoch: 5| Step: 3
Training loss: 3.6050881032984403
Validation loss: 3.466535972842874

Epoch: 5| Step: 4
Training loss: 3.8937080148359047
Validation loss: 3.4617177056572346

Epoch: 5| Step: 5
Training loss: 4.089532674565382
Validation loss: 3.457154835361596

Epoch: 5| Step: 6
Training loss: 3.2075853507979812
Validation loss: 3.4524995919295725

Epoch: 5| Step: 7
Training loss: 3.618596439035256
Validation loss: 3.447302124867631

Epoch: 5| Step: 8
Training loss: 3.43445577205404
Validation loss: 3.442877949486154

Epoch: 5| Step: 9
Training loss: 3.9876912994895197
Validation loss: 3.438106679843879

Epoch: 5| Step: 10
Training loss: 3.7075817475209876
Validation loss: 3.432626274751633

Epoch: 5| Step: 11
Training loss: 1.6860709320523817
Validation loss: 3.4275030480094655

Epoch: 37| Step: 0
Training loss: 3.53151249542899
Validation loss: 3.4234961410228832

Epoch: 5| Step: 1
Training loss: 3.737615862935514
Validation loss: 3.419046211079519

Epoch: 5| Step: 2
Training loss: 3.663120954036512
Validation loss: 3.413862459678627

Epoch: 5| Step: 3
Training loss: 3.579427290558303
Validation loss: 3.4092035350113

Epoch: 5| Step: 4
Training loss: 2.8654537103974764
Validation loss: 3.404160832984125

Epoch: 5| Step: 5
Training loss: 3.5566929117346904
Validation loss: 3.4007109159415574

Epoch: 5| Step: 6
Training loss: 3.6251321965809726
Validation loss: 3.3958999338620446

Epoch: 5| Step: 7
Training loss: 3.5661566188391576
Validation loss: 3.391706584223759

Epoch: 5| Step: 8
Training loss: 3.351135938126901
Validation loss: 3.3867424564785833

Epoch: 5| Step: 9
Training loss: 3.0712924781380053
Validation loss: 3.3823071308602892

Epoch: 5| Step: 10
Training loss: 4.093231867258514
Validation loss: 3.3772831658635667

Epoch: 5| Step: 11
Training loss: 4.197286982990098
Validation loss: 3.3727817604240267

Epoch: 38| Step: 0
Training loss: 2.395897131913519
Validation loss: 3.367772971202654

Epoch: 5| Step: 1
Training loss: 3.9673948602620697
Validation loss: 3.3633936775724713

Epoch: 5| Step: 2
Training loss: 3.163580662337646
Validation loss: 3.3584150613600934

Epoch: 5| Step: 3
Training loss: 3.8785450624103186
Validation loss: 3.3544425742280777

Epoch: 5| Step: 4
Training loss: 3.5455733948463384
Validation loss: 3.3499522635157004

Epoch: 5| Step: 5
Training loss: 2.897889941540439
Validation loss: 3.344915044444125

Epoch: 5| Step: 6
Training loss: 3.1983025698021335
Validation loss: 3.3409786504641663

Epoch: 5| Step: 7
Training loss: 3.688558135722612
Validation loss: 3.336483537711941

Epoch: 5| Step: 8
Training loss: 4.176458576820377
Validation loss: 3.331237191514995

Epoch: 5| Step: 9
Training loss: 3.359272374204604
Validation loss: 3.3259504404688114

Epoch: 5| Step: 10
Training loss: 3.645321677499297
Validation loss: 3.321901116398579

Epoch: 5| Step: 11
Training loss: 3.5996873984778532
Validation loss: 3.316348605184781

Epoch: 39| Step: 0
Training loss: 3.4780438985220394
Validation loss: 3.312306692372973

Epoch: 5| Step: 1
Training loss: 3.3654681637632096
Validation loss: 3.3107374588413108

Epoch: 5| Step: 2
Training loss: 3.5476835564404814
Validation loss: 3.3063840426679647

Epoch: 5| Step: 3
Training loss: 3.4480387655499687
Validation loss: 3.304327726507598

Epoch: 5| Step: 4
Training loss: 3.1205929006968987
Validation loss: 3.2967700911428697

Epoch: 5| Step: 5
Training loss: 3.2592790627921495
Validation loss: 3.290725945798813

Epoch: 5| Step: 6
Training loss: 3.538544808559391
Validation loss: 3.2855082084095764

Epoch: 5| Step: 7
Training loss: 3.6605460204686335
Validation loss: 3.2811936994294095

Epoch: 5| Step: 8
Training loss: 3.5396555557352003
Validation loss: 3.277296090944606

Epoch: 5| Step: 9
Training loss: 3.456124776327304
Validation loss: 3.2734732675229465

Epoch: 5| Step: 10
Training loss: 3.3495659561234774
Validation loss: 3.269774673618681

Epoch: 5| Step: 11
Training loss: 3.09964229611928
Validation loss: 3.2637989164806953

Epoch: 40| Step: 0
Training loss: 3.409828615999743
Validation loss: 3.258864729119713

Epoch: 5| Step: 1
Training loss: 3.15421267181535
Validation loss: 3.253984374794845

Epoch: 5| Step: 2
Training loss: 3.4867873800277107
Validation loss: 3.249524962113791

Epoch: 5| Step: 3
Training loss: 2.663317802807243
Validation loss: 3.2453826602850824

Epoch: 5| Step: 4
Training loss: 3.4058280473491833
Validation loss: 3.2412468614279852

Epoch: 5| Step: 5
Training loss: 3.62819425662998
Validation loss: 3.237155290611505

Epoch: 5| Step: 6
Training loss: 3.8212986708846106
Validation loss: 3.2324461660404005

Epoch: 5| Step: 7
Training loss: 3.6588561070483436
Validation loss: 3.2278588487154343

Epoch: 5| Step: 8
Training loss: 3.1599576720287996
Validation loss: 3.2230977258865314

Epoch: 5| Step: 9
Training loss: 3.299568702950172
Validation loss: 3.21904162595036

Epoch: 5| Step: 10
Training loss: 3.37086544390938
Validation loss: 3.2143964707376167

Epoch: 5| Step: 11
Training loss: 3.078878707394728
Validation loss: 3.210717298099227

Epoch: 41| Step: 0
Training loss: 3.259659716861035
Validation loss: 3.2065685711314837

Epoch: 5| Step: 1
Training loss: 2.721966999658336
Validation loss: 3.2023043752078157

Epoch: 5| Step: 2
Training loss: 3.416927715346325
Validation loss: 3.198672389636407

Epoch: 5| Step: 3
Training loss: 3.1384561366039065
Validation loss: 3.1944935191342445

Epoch: 5| Step: 4
Training loss: 3.8099979883223702
Validation loss: 3.190464716667347

Epoch: 5| Step: 5
Training loss: 3.298507069514067
Validation loss: 3.186285420182623

Epoch: 5| Step: 6
Training loss: 3.457057457086399
Validation loss: 3.182417385825006

Epoch: 5| Step: 7
Training loss: 3.2082969035933138
Validation loss: 3.178175145556636

Epoch: 5| Step: 8
Training loss: 3.064585384434771
Validation loss: 3.174484745278384

Epoch: 5| Step: 9
Training loss: 3.612014531973399
Validation loss: 3.1706045390699034

Epoch: 5| Step: 10
Training loss: 3.2772359364250243
Validation loss: 3.165856070462366

Epoch: 5| Step: 11
Training loss: 4.207632377840391
Validation loss: 3.162481168708473

Epoch: 42| Step: 0
Training loss: 2.7168729594565013
Validation loss: 3.157728333683043

Epoch: 5| Step: 1
Training loss: 3.4852664645327174
Validation loss: 3.1540142109828335

Epoch: 5| Step: 2
Training loss: 3.2323556391910913
Validation loss: 3.150026430039308

Epoch: 5| Step: 3
Training loss: 3.5977385570542157
Validation loss: 3.1464530079764197

Epoch: 5| Step: 4
Training loss: 3.0802586320561534
Validation loss: 3.142867358521893

Epoch: 5| Step: 5
Training loss: 3.2490833163440747
Validation loss: 3.1389970751397653

Epoch: 5| Step: 6
Training loss: 3.140367487888062
Validation loss: 3.1349079800900865

Epoch: 5| Step: 7
Training loss: 3.5929098722905413
Validation loss: 3.1313752058609614

Epoch: 5| Step: 8
Training loss: 3.0212829956727667
Validation loss: 3.1273975838760686

Epoch: 5| Step: 9
Training loss: 3.179414404686195
Validation loss: 3.123749962972453

Epoch: 5| Step: 10
Training loss: 3.4296950077330983
Validation loss: 3.119754725233811

Epoch: 5| Step: 11
Training loss: 4.27470911832193
Validation loss: 3.1161566410496797

Epoch: 43| Step: 0
Training loss: 3.337998336563016
Validation loss: 3.111953653313452

Epoch: 5| Step: 1
Training loss: 3.239709702426076
Validation loss: 3.1082239017109377

Epoch: 5| Step: 2
Training loss: 3.0882459287700748
Validation loss: 3.1043978590072783

Epoch: 5| Step: 3
Training loss: 3.2715352433078824
Validation loss: 3.1004109202572945

Epoch: 5| Step: 4
Training loss: 3.7116426139318275
Validation loss: 3.0964553031520716

Epoch: 5| Step: 5
Training loss: 2.70237086366019
Validation loss: 3.0921893502272124

Epoch: 5| Step: 6
Training loss: 3.031550441679032
Validation loss: 3.08829329179751

Epoch: 5| Step: 7
Training loss: 3.3442921421806107
Validation loss: 3.0848128639582324

Epoch: 5| Step: 8
Training loss: 3.274901724935177
Validation loss: 3.0805351581037694

Epoch: 5| Step: 9
Training loss: 3.0741294062620903
Validation loss: 3.0770359339557576

Epoch: 5| Step: 10
Training loss: 3.3676246306218567
Validation loss: 3.073221085981735

Epoch: 5| Step: 11
Training loss: 3.382290066381009
Validation loss: 3.0696474476154387

Epoch: 44| Step: 0
Training loss: 3.474788598964708
Validation loss: 3.0663880527871594

Epoch: 5| Step: 1
Training loss: 2.5353277821399396
Validation loss: 3.062388521877308

Epoch: 5| Step: 2
Training loss: 3.0267238843721778
Validation loss: 3.058873663541958

Epoch: 5| Step: 3
Training loss: 3.1438572889945964
Validation loss: 3.0553533278841147

Epoch: 5| Step: 4
Training loss: 3.0759703555024207
Validation loss: 3.051908902509829

Epoch: 5| Step: 5
Training loss: 3.3134243233337863
Validation loss: 3.0480887774886694

Epoch: 5| Step: 6
Training loss: 3.2176753351889165
Validation loss: 3.044761514835231

Epoch: 5| Step: 7
Training loss: 2.810420899686083
Validation loss: 3.0420356726969815

Epoch: 5| Step: 8
Training loss: 3.1411730539572673
Validation loss: 3.038362594068802

Epoch: 5| Step: 9
Training loss: 3.683498474377611
Validation loss: 3.035340672163285

Epoch: 5| Step: 10
Training loss: 3.133195125967691
Validation loss: 3.0320910633913507

Epoch: 5| Step: 11
Training loss: 4.693217948087639
Validation loss: 3.028219221950248

Epoch: 45| Step: 0
Training loss: 3.4689336504734016
Validation loss: 3.024717817202203

Epoch: 5| Step: 1
Training loss: 2.928949776908832
Validation loss: 3.020399527258826

Epoch: 5| Step: 2
Training loss: 3.3249767618156025
Validation loss: 3.016604302395702

Epoch: 5| Step: 3
Training loss: 2.6973073460296124
Validation loss: 3.013089558645423

Epoch: 5| Step: 4
Training loss: 3.1125666660997844
Validation loss: 3.009622341461091

Epoch: 5| Step: 5
Training loss: 3.2070154893606335
Validation loss: 3.0059261996232003

Epoch: 5| Step: 6
Training loss: 3.028750145849586
Validation loss: 3.002107797020467

Epoch: 5| Step: 7
Training loss: 2.9311121210722026
Validation loss: 2.998506087962714

Epoch: 5| Step: 8
Training loss: 3.448509480356462
Validation loss: 2.995099800954097

Epoch: 5| Step: 9
Training loss: 3.253443434316609
Validation loss: 2.9917938099588235

Epoch: 5| Step: 10
Training loss: 3.2209623566742893
Validation loss: 2.9885424132073046

Epoch: 5| Step: 11
Training loss: 2.762708775152776
Validation loss: 2.9850473761619454

Epoch: 46| Step: 0
Training loss: 2.9630204978849255
Validation loss: 2.9823044789410242

Epoch: 5| Step: 1
Training loss: 3.0461691014219823
Validation loss: 2.9791967761690086

Epoch: 5| Step: 2
Training loss: 3.1647479786503667
Validation loss: 2.976298292954616

Epoch: 5| Step: 3
Training loss: 3.106101608569621
Validation loss: 2.9735983727041884

Epoch: 5| Step: 4
Training loss: 3.3169584679474626
Validation loss: 2.9705393702383938

Epoch: 5| Step: 5
Training loss: 3.4138964300552592
Validation loss: 2.967358021910978

Epoch: 5| Step: 6
Training loss: 2.7795355152181247
Validation loss: 2.9640675141341224

Epoch: 5| Step: 7
Training loss: 2.2158539905233186
Validation loss: 2.9616031485640604

Epoch: 5| Step: 8
Training loss: 3.6522284668052567
Validation loss: 2.9585579710728416

Epoch: 5| Step: 9
Training loss: 3.0751912809595674
Validation loss: 2.9553226757193714

Epoch: 5| Step: 10
Training loss: 3.2253984840891445
Validation loss: 2.9526661511344168

Epoch: 5| Step: 11
Training loss: 3.227691379017454
Validation loss: 2.949434406404109

Epoch: 47| Step: 0
Training loss: 2.893909464782277
Validation loss: 2.9465754909959254

Epoch: 5| Step: 1
Training loss: 2.9516489689743532
Validation loss: 2.943507642414109

Epoch: 5| Step: 2
Training loss: 3.277087814285956
Validation loss: 2.941054439233662

Epoch: 5| Step: 3
Training loss: 3.2831673969092603
Validation loss: 2.9379539713864617

Epoch: 5| Step: 4
Training loss: 3.092861163452314
Validation loss: 2.93482591105144

Epoch: 5| Step: 5
Training loss: 2.8888849743384553
Validation loss: 2.932020504588993

Epoch: 5| Step: 6
Training loss: 3.0403378514728643
Validation loss: 2.928593043724954

Epoch: 5| Step: 7
Training loss: 2.5879802645147474
Validation loss: 2.9258481821469107

Epoch: 5| Step: 8
Training loss: 3.298596407316575
Validation loss: 2.9238435502326

Epoch: 5| Step: 9
Training loss: 3.1853006292298027
Validation loss: 2.9206076438677084

Epoch: 5| Step: 10
Training loss: 3.2021867075614137
Validation loss: 2.9182275864116134

Epoch: 5| Step: 11
Training loss: 3.2498001624070754
Validation loss: 2.915421360832238

Epoch: 48| Step: 0
Training loss: 2.8370976032494495
Validation loss: 2.912622053156632

Epoch: 5| Step: 1
Training loss: 3.0607596921660605
Validation loss: 2.909561977258023

Epoch: 5| Step: 2
Training loss: 3.35271841181173
Validation loss: 2.906978358671198

Epoch: 5| Step: 3
Training loss: 2.929506342055277
Validation loss: 2.903719730320692

Epoch: 5| Step: 4
Training loss: 3.001935652295442
Validation loss: 2.9019530021738316

Epoch: 5| Step: 5
Training loss: 3.206656096025308
Validation loss: 2.899063454048351

Epoch: 5| Step: 6
Training loss: 3.2255375968505295
Validation loss: 2.8970958624150565

Epoch: 5| Step: 7
Training loss: 2.748027440851679
Validation loss: 2.8940530743225934

Epoch: 5| Step: 8
Training loss: 2.8378546673780143
Validation loss: 2.892013683622638

Epoch: 5| Step: 9
Training loss: 3.3847167401710068
Validation loss: 2.888580042247898

Epoch: 5| Step: 10
Training loss: 2.8198474278671486
Validation loss: 2.8864216391356696

Epoch: 5| Step: 11
Training loss: 2.9630618564561804
Validation loss: 2.8838472240975985

Epoch: 49| Step: 0
Training loss: 2.5084157438036874
Validation loss: 2.880805042853652

Epoch: 5| Step: 1
Training loss: 3.1900286648697107
Validation loss: 2.877885072325846

Epoch: 5| Step: 2
Training loss: 3.30104993821361
Validation loss: 2.875713802266249

Epoch: 5| Step: 3
Training loss: 3.219769955112131
Validation loss: 2.873050948156758

Epoch: 5| Step: 4
Training loss: 2.9999316525620956
Validation loss: 2.86993578614141

Epoch: 5| Step: 5
Training loss: 3.033185203484564
Validation loss: 2.8675484668268614

Epoch: 5| Step: 6
Training loss: 2.8892756431440443
Validation loss: 2.864778045047854

Epoch: 5| Step: 7
Training loss: 3.1548263813980535
Validation loss: 2.8621792513623014

Epoch: 5| Step: 8
Training loss: 2.8058131942124462
Validation loss: 2.8598161506470854

Epoch: 5| Step: 9
Training loss: 2.972662666303851
Validation loss: 2.856463334333729

Epoch: 5| Step: 10
Training loss: 2.8947496833698967
Validation loss: 2.853524972429051

Epoch: 5| Step: 11
Training loss: 3.2495645084578495
Validation loss: 2.8520105018725546

Epoch: 50| Step: 0
Training loss: 2.9945473391615454
Validation loss: 2.880524548399764

Epoch: 5| Step: 1
Training loss: 3.3826200857788264
Validation loss: 2.847236295344817

Epoch: 5| Step: 2
Training loss: 3.0964353062570127
Validation loss: 2.8447252459729815

Epoch: 5| Step: 3
Training loss: 2.9928416042659354
Validation loss: 2.8434022544104365

Epoch: 5| Step: 4
Training loss: 2.302590868023534
Validation loss: 2.8431234822381364

Epoch: 5| Step: 5
Training loss: 2.729550972784902
Validation loss: 2.8454045069435585

Epoch: 5| Step: 6
Training loss: 3.282698538616449
Validation loss: 2.847469025410755

Epoch: 5| Step: 7
Training loss: 3.215956132779399
Validation loss: 2.843830142446418

Epoch: 5| Step: 8
Training loss: 2.927407478675221
Validation loss: 2.839942461665559

Epoch: 5| Step: 9
Training loss: 3.126379547790585
Validation loss: 2.832529684468448

Epoch: 5| Step: 10
Training loss: 2.7150252686338057
Validation loss: 2.827893558537245

Epoch: 5| Step: 11
Training loss: 2.9318704432425853
Validation loss: 2.8249024108471046

Epoch: 51| Step: 0
Training loss: 2.726211568989168
Validation loss: 2.8228920282130905

Epoch: 5| Step: 1
Training loss: 2.666582722137355
Validation loss: 2.8219936675406876

Epoch: 5| Step: 2
Training loss: 3.604996572802392
Validation loss: 2.8224627785640046

Epoch: 5| Step: 3
Training loss: 2.9839432488943336
Validation loss: 2.818432224290033

Epoch: 5| Step: 4
Training loss: 2.85925092584292
Validation loss: 2.8153263654835667

Epoch: 5| Step: 5
Training loss: 2.7505966752979094
Validation loss: 2.812165487670772

Epoch: 5| Step: 6
Training loss: 3.418636514010988
Validation loss: 2.810084944976269

Epoch: 5| Step: 7
Training loss: 3.063412588738705
Validation loss: 2.8065329796295084

Epoch: 5| Step: 8
Training loss: 2.8318386249146674
Validation loss: 2.8042733144295124

Epoch: 5| Step: 9
Training loss: 2.195502113488956
Validation loss: 2.8017369177912177

Epoch: 5| Step: 10
Training loss: 2.9963929426068114
Validation loss: 2.800246854101416

Epoch: 5| Step: 11
Training loss: 3.686149737888671
Validation loss: 2.7982274839827435

Epoch: 52| Step: 0
Training loss: 2.9653466241742237
Validation loss: 2.7961460172236303

Epoch: 5| Step: 1
Training loss: 3.426344926278294
Validation loss: 2.7942208602337852

Epoch: 5| Step: 2
Training loss: 2.8075094238829634
Validation loss: 2.7922727557947487

Epoch: 5| Step: 3
Training loss: 2.5200944138052024
Validation loss: 2.7904947131572255

Epoch: 5| Step: 4
Training loss: 2.911866206679666
Validation loss: 2.788072422794294

Epoch: 5| Step: 5
Training loss: 2.7403538325095638
Validation loss: 2.7858307152417345

Epoch: 5| Step: 6
Training loss: 3.2302051146014645
Validation loss: 2.7836781521361034

Epoch: 5| Step: 7
Training loss: 2.4571548697662737
Validation loss: 2.7811238013793056

Epoch: 5| Step: 8
Training loss: 3.0092985371256993
Validation loss: 2.7790849595495843

Epoch: 5| Step: 9
Training loss: 3.084643747048209
Validation loss: 2.777052776905937

Epoch: 5| Step: 10
Training loss: 2.790559938362243
Validation loss: 2.774857436060675

Epoch: 5| Step: 11
Training loss: 3.4815465283761537
Validation loss: 2.773181498716227

Epoch: 53| Step: 0
Training loss: 3.1931703284065227
Validation loss: 2.7687268436287336

Epoch: 5| Step: 1
Training loss: 2.656286172059546
Validation loss: 2.767396815500561

Epoch: 5| Step: 2
Training loss: 3.4398548729911864
Validation loss: 2.764423127169061

Epoch: 5| Step: 3
Training loss: 2.938373841518388
Validation loss: 2.7639323980068693

Epoch: 5| Step: 4
Training loss: 2.945912105329047
Validation loss: 2.761556096302824

Epoch: 5| Step: 5
Training loss: 2.812357835355539
Validation loss: 2.759263000288989

Epoch: 5| Step: 6
Training loss: 2.487018066954168
Validation loss: 2.757225899992274

Epoch: 5| Step: 7
Training loss: 3.0113564754551434
Validation loss: 2.755415558308355

Epoch: 5| Step: 8
Training loss: 2.9675508988730663
Validation loss: 2.7538274803349534

Epoch: 5| Step: 9
Training loss: 2.7902603782650153
Validation loss: 2.7505193602814377

Epoch: 5| Step: 10
Training loss: 2.513736560999961
Validation loss: 2.749648443783494

Epoch: 5| Step: 11
Training loss: 3.0025600001099484
Validation loss: 2.748203783722818

Epoch: 54| Step: 0
Training loss: 2.5047828698994645
Validation loss: 2.7464799279343466

Epoch: 5| Step: 1
Training loss: 2.528135007784242
Validation loss: 2.7425406682312223

Epoch: 5| Step: 2
Training loss: 3.087816192868737
Validation loss: 2.7409538109492324

Epoch: 5| Step: 3
Training loss: 2.895646306500266
Validation loss: 2.738241920061768

Epoch: 5| Step: 4
Training loss: 2.9721623134418227
Validation loss: 2.7378410968753655

Epoch: 5| Step: 5
Training loss: 3.100165756470181
Validation loss: 2.734206897472945

Epoch: 5| Step: 6
Training loss: 3.2431143665268047
Validation loss: 2.7316813943705234

Epoch: 5| Step: 7
Training loss: 2.969445879842397
Validation loss: 2.732505398960486

Epoch: 5| Step: 8
Training loss: 2.8442489689311
Validation loss: 2.7284454212882756

Epoch: 5| Step: 9
Training loss: 2.4794003566159803
Validation loss: 2.728330911306317

Epoch: 5| Step: 10
Training loss: 2.569450999300274
Validation loss: 2.7275478947003977

Epoch: 5| Step: 11
Training loss: 4.247121172486627
Validation loss: 2.7293341939531426

Epoch: 55| Step: 0
Training loss: 2.9071076470998065
Validation loss: 2.7237951267478753

Epoch: 5| Step: 1
Training loss: 2.623536201333428
Validation loss: 2.7260234959460417

Epoch: 5| Step: 2
Training loss: 2.9683411969254387
Validation loss: 2.73014363548549

Epoch: 5| Step: 3
Training loss: 2.969610631574722
Validation loss: 2.7314985022550355

Epoch: 5| Step: 4
Training loss: 2.835511062549368
Validation loss: 2.7385435681622448

Epoch: 5| Step: 5
Training loss: 2.723471827244175
Validation loss: 2.7377564693308654

Epoch: 5| Step: 6
Training loss: 2.7838070719452266
Validation loss: 2.7404831084373633

Epoch: 5| Step: 7
Training loss: 3.286751630766863
Validation loss: 2.735496741403842

Epoch: 5| Step: 8
Training loss: 2.518174958196247
Validation loss: 2.7266684496095728

Epoch: 5| Step: 9
Training loss: 3.23676760722049
Validation loss: 2.7219446603275106

Epoch: 5| Step: 10
Training loss: 2.683620758142841
Validation loss: 2.7148945719035127

Epoch: 5| Step: 11
Training loss: 2.50851953349759
Validation loss: 2.711948601980462

Epoch: 56| Step: 0
Training loss: 3.0601303898076813
Validation loss: 2.706482028338201

Epoch: 5| Step: 1
Training loss: 3.390382397669797
Validation loss: 2.7037527020729404

Epoch: 5| Step: 2
Training loss: 3.1859346922019722
Validation loss: 2.6992804101138543

Epoch: 5| Step: 3
Training loss: 2.61608116337803
Validation loss: 2.69661445685214

Epoch: 5| Step: 4
Training loss: 2.983405789679975
Validation loss: 2.696134480622201

Epoch: 5| Step: 5
Training loss: 2.6042708312500835
Validation loss: 2.698324759314402

Epoch: 5| Step: 6
Training loss: 3.0934019567287976
Validation loss: 2.70696072030126

Epoch: 5| Step: 7
Training loss: 2.2683991384198863
Validation loss: 2.6966677442044755

Epoch: 5| Step: 8
Training loss: 2.715647099693637
Validation loss: 2.6876180977711206

Epoch: 5| Step: 9
Training loss: 2.4933520618626104
Validation loss: 2.68770168715715

Epoch: 5| Step: 10
Training loss: 2.6902848829566066
Validation loss: 2.6863928628738565

Epoch: 5| Step: 11
Training loss: 2.660552724905476
Validation loss: 2.6880992842231146

Epoch: 57| Step: 0
Training loss: 2.8006268276488715
Validation loss: 2.6871386699112016

Epoch: 5| Step: 1
Training loss: 3.325132071948186
Validation loss: 2.6859530868802928

Epoch: 5| Step: 2
Training loss: 3.0921673049768463
Validation loss: 2.6833757314728546

Epoch: 5| Step: 3
Training loss: 2.6014176062022667
Validation loss: 2.683828074346132

Epoch: 5| Step: 4
Training loss: 2.3095416141848077
Validation loss: 2.6838149766647392

Epoch: 5| Step: 5
Training loss: 3.087921509175091
Validation loss: 2.6798057048146893

Epoch: 5| Step: 6
Training loss: 2.218493164670988
Validation loss: 2.6767328208509116

Epoch: 5| Step: 7
Training loss: 2.7418471774551993
Validation loss: 2.677570741468683

Epoch: 5| Step: 8
Training loss: 3.27096223678412
Validation loss: 2.673708727046793

Epoch: 5| Step: 9
Training loss: 2.4324879100848045
Validation loss: 2.671343856576436

Epoch: 5| Step: 10
Training loss: 2.92311844912079
Validation loss: 2.668664140682548

Epoch: 5| Step: 11
Training loss: 2.7511694762297862
Validation loss: 2.667190702675176

Epoch: 58| Step: 0
Training loss: 2.906222763754265
Validation loss: 2.6645572534815414

Epoch: 5| Step: 1
Training loss: 2.8382868007833486
Validation loss: 2.665612676814858

Epoch: 5| Step: 2
Training loss: 2.545969140841928
Validation loss: 2.6590161263322494

Epoch: 5| Step: 3
Training loss: 2.891011969980631
Validation loss: 2.6591914028769352

Epoch: 5| Step: 4
Training loss: 3.042776314719177
Validation loss: 2.657103083633192

Epoch: 5| Step: 5
Training loss: 3.0865675657222464
Validation loss: 2.6560029831265353

Epoch: 5| Step: 6
Training loss: 2.7499160753795486
Validation loss: 2.65525922435919

Epoch: 5| Step: 7
Training loss: 2.924475833192297
Validation loss: 2.6538558305351656

Epoch: 5| Step: 8
Training loss: 2.8694818032873064
Validation loss: 2.650380186333524

Epoch: 5| Step: 9
Training loss: 2.0526987176151965
Validation loss: 2.6505187904818284

Epoch: 5| Step: 10
Training loss: 2.8713483425067245
Validation loss: 2.6448593462026726

Epoch: 5| Step: 11
Training loss: 1.854025703243775
Validation loss: 2.647509300646657

Epoch: 59| Step: 0
Training loss: 3.179364012213097
Validation loss: 2.649828231540846

Epoch: 5| Step: 1
Training loss: 2.617590665367383
Validation loss: 2.643012368968657

Epoch: 5| Step: 2
Training loss: 2.944093649338733
Validation loss: 2.638113622109921

Epoch: 5| Step: 3
Training loss: 2.2809698899626043
Validation loss: 2.6386505133618074

Epoch: 5| Step: 4
Training loss: 2.7260921039708608
Validation loss: 2.6382340322781426

Epoch: 5| Step: 5
Training loss: 2.9251680635603217
Validation loss: 2.6391382226494233

Epoch: 5| Step: 6
Training loss: 2.562785807232044
Validation loss: 2.632043803634138

Epoch: 5| Step: 7
Training loss: 2.960451272381462
Validation loss: 2.636178296119253

Epoch: 5| Step: 8
Training loss: 2.434815028408667
Validation loss: 2.6537363313336986

Epoch: 5| Step: 9
Training loss: 3.005594917348301
Validation loss: 2.65119991442229

Epoch: 5| Step: 10
Training loss: 2.816691348477481
Validation loss: 2.6260440014338653

Epoch: 5| Step: 11
Training loss: 3.2843682867066857
Validation loss: 2.6354683555590217

Epoch: 60| Step: 0
Training loss: 2.7044655887885063
Validation loss: 2.6673042285228794

Epoch: 5| Step: 1
Training loss: 2.8504178192652327
Validation loss: 2.6965663112935134

Epoch: 5| Step: 2
Training loss: 2.9139391180104774
Validation loss: 2.71283354034309

Epoch: 5| Step: 3
Training loss: 2.529961717338571
Validation loss: 2.708081876867901

Epoch: 5| Step: 4
Training loss: 2.6691825641210665
Validation loss: 2.687269363561158

Epoch: 5| Step: 5
Training loss: 2.6135420733244117
Validation loss: 2.6576968160963172

Epoch: 5| Step: 6
Training loss: 2.714038317742279
Validation loss: 2.649756426683484

Epoch: 5| Step: 7
Training loss: 3.030161554039504
Validation loss: 2.646001830455275

Epoch: 5| Step: 8
Training loss: 2.7951761552578613
Validation loss: 2.6396414676357374

Epoch: 5| Step: 9
Training loss: 2.972388036109524
Validation loss: 2.633309805036612

Epoch: 5| Step: 10
Training loss: 2.946537643329392
Validation loss: 2.6296115777397753

Epoch: 5| Step: 11
Training loss: 2.771059375998215
Validation loss: 2.6227535974797087

Epoch: 61| Step: 0
Training loss: 2.7262416530588043
Validation loss: 2.6174610507402654

Epoch: 5| Step: 1
Training loss: 2.594463663849129
Validation loss: 2.615370906639995

Epoch: 5| Step: 2
Training loss: 2.8514216976153905
Validation loss: 2.615791897631398

Epoch: 5| Step: 3
Training loss: 2.3327155203568166
Validation loss: 2.611545706799919

Epoch: 5| Step: 4
Training loss: 2.561770451597818
Validation loss: 2.6142221115157875

Epoch: 5| Step: 5
Training loss: 3.1018498285222664
Validation loss: 2.610066286142949

Epoch: 5| Step: 6
Training loss: 3.064437875898165
Validation loss: 2.6101466349508664

Epoch: 5| Step: 7
Training loss: 2.243858751302024
Validation loss: 2.6109627863056923

Epoch: 5| Step: 8
Training loss: 2.770342924464121
Validation loss: 2.6132832343260755

Epoch: 5| Step: 9
Training loss: 3.1219134537444977
Validation loss: 2.615090048779157

Epoch: 5| Step: 10
Training loss: 2.693383275535271
Validation loss: 2.6040698542083445

Epoch: 5| Step: 11
Training loss: 3.2478389891380446
Validation loss: 2.601932064074817

Epoch: 62| Step: 0
Training loss: 2.5933034121724843
Validation loss: 2.6045370347183825

Epoch: 5| Step: 1
Training loss: 2.732279034167363
Validation loss: 2.605877799848318

Epoch: 5| Step: 2
Training loss: 2.736102708221631
Validation loss: 2.607125292804077

Epoch: 5| Step: 3
Training loss: 2.7261401179932276
Validation loss: 2.607605625492967

Epoch: 5| Step: 4
Training loss: 2.9796418209005213
Validation loss: 2.6074814617001487

Epoch: 5| Step: 5
Training loss: 2.884059729244626
Validation loss: 2.6082438741268508

Epoch: 5| Step: 6
Training loss: 2.7913035730737787
Validation loss: 2.608662704130706

Epoch: 5| Step: 7
Training loss: 2.3894796183216362
Validation loss: 2.6081827108526863

Epoch: 5| Step: 8
Training loss: 2.786738263386177
Validation loss: 2.606890494959749

Epoch: 5| Step: 9
Training loss: 2.8702628873255494
Validation loss: 2.6075268670706775

Epoch: 5| Step: 10
Training loss: 2.7912949461677217
Validation loss: 2.607227694439988

Epoch: 5| Step: 11
Training loss: 1.9883575125047759
Validation loss: 2.6066184836171895

Epoch: 63| Step: 0
Training loss: 3.124464523214135
Validation loss: 2.603236911103712

Epoch: 5| Step: 1
Training loss: 2.6770449584902067
Validation loss: 2.5989750241729177

Epoch: 5| Step: 2
Training loss: 2.3419514176357112
Validation loss: 2.594916789803917

Epoch: 5| Step: 3
Training loss: 3.04079954424591
Validation loss: 2.5933519195803423

Epoch: 5| Step: 4
Training loss: 2.5525313211251115
Validation loss: 2.5934346286898595

Epoch: 5| Step: 5
Training loss: 2.9064196465313064
Validation loss: 2.5857129225160267

Epoch: 5| Step: 6
Training loss: 2.942429646936839
Validation loss: 2.584014150419602

Epoch: 5| Step: 7
Training loss: 2.168604632076223
Validation loss: 2.5812853237318274

Epoch: 5| Step: 8
Training loss: 2.8672430464433827
Validation loss: 2.579419641357244

Epoch: 5| Step: 9
Training loss: 2.539516936496379
Validation loss: 2.583803121986194

Epoch: 5| Step: 10
Training loss: 2.79217261858509
Validation loss: 2.5784619197907332

Epoch: 5| Step: 11
Training loss: 2.3489851403803694
Validation loss: 2.582532394491583

Epoch: 64| Step: 0
Training loss: 2.5916477554301856
Validation loss: 2.5854224837916795

Epoch: 5| Step: 1
Training loss: 2.6919385939987306
Validation loss: 2.595404499405333

Epoch: 5| Step: 2
Training loss: 2.7409442572118423
Validation loss: 2.5841443624295577

Epoch: 5| Step: 3
Training loss: 2.641625750813472
Validation loss: 2.595441396973803

Epoch: 5| Step: 4
Training loss: 2.315157008274261
Validation loss: 2.59104693174511

Epoch: 5| Step: 5
Training loss: 2.950685492576027
Validation loss: 2.5853037830785124

Epoch: 5| Step: 6
Training loss: 3.0691414266835055
Validation loss: 2.5758022788426147

Epoch: 5| Step: 7
Training loss: 2.431468347781527
Validation loss: 2.5737092397141534

Epoch: 5| Step: 8
Training loss: 3.076363771296354
Validation loss: 2.5681797443421415

Epoch: 5| Step: 9
Training loss: 2.705295286860813
Validation loss: 2.5705737413099685

Epoch: 5| Step: 10
Training loss: 2.7214043474209904
Validation loss: 2.5721029482654423

Epoch: 5| Step: 11
Training loss: 1.6119326665413207
Validation loss: 2.5765166573719207

Epoch: 65| Step: 0
Training loss: 2.3644785472713834
Validation loss: 2.5782093766401966

Epoch: 5| Step: 1
Training loss: 2.9181203943192933
Validation loss: 2.5819206695038868

Epoch: 5| Step: 2
Training loss: 2.744739268971202
Validation loss: 2.586874597098875

Epoch: 5| Step: 3
Training loss: 2.628856913541511
Validation loss: 2.585860382804888

Epoch: 5| Step: 4
Training loss: 2.762145272519214
Validation loss: 2.585630588815913

Epoch: 5| Step: 5
Training loss: 2.864302761324772
Validation loss: 2.5863770018280636

Epoch: 5| Step: 6
Training loss: 2.8017022442151984
Validation loss: 2.583275926372553

Epoch: 5| Step: 7
Training loss: 3.0704322383915588
Validation loss: 2.5822958029195435

Epoch: 5| Step: 8
Training loss: 3.1112200759546003
Validation loss: 2.578327517791614

Epoch: 5| Step: 9
Training loss: 2.368678765748196
Validation loss: 2.5761280494723846

Epoch: 5| Step: 10
Training loss: 2.2919243638771953
Validation loss: 2.5714410238809666

Epoch: 5| Step: 11
Training loss: 1.5781741748362235
Validation loss: 2.569933379252334

Epoch: 66| Step: 0
Training loss: 3.053333811808497
Validation loss: 2.5572120760298027

Epoch: 5| Step: 1
Training loss: 1.9376052089551843
Validation loss: 2.561020598538373

Epoch: 5| Step: 2
Training loss: 2.9169328477287886
Validation loss: 2.6006342021165403

Epoch: 5| Step: 3
Training loss: 2.8870629710619324
Validation loss: 2.631578033399005

Epoch: 5| Step: 4
Training loss: 2.223414715712495
Validation loss: 2.597206688702439

Epoch: 5| Step: 5
Training loss: 2.8239882758980546
Validation loss: 2.5605322213993897

Epoch: 5| Step: 6
Training loss: 2.128004978733405
Validation loss: 2.549320131562312

Epoch: 5| Step: 7
Training loss: 2.8470176954131503
Validation loss: 2.5568279783465813

Epoch: 5| Step: 8
Training loss: 2.7706992802901826
Validation loss: 2.5584903259340463

Epoch: 5| Step: 9
Training loss: 2.5148638408956914
Validation loss: 2.5615741522564583

Epoch: 5| Step: 10
Training loss: 3.1267179725954444
Validation loss: 2.565614583051987

Epoch: 5| Step: 11
Training loss: 3.7716412741266634
Validation loss: 2.570027394507445

Epoch: 67| Step: 0
Training loss: 3.418833874888763
Validation loss: 2.5710246752711297

Epoch: 5| Step: 1
Training loss: 2.423220254690126
Validation loss: 2.572416153898508

Epoch: 5| Step: 2
Training loss: 2.9474562070794823
Validation loss: 2.5698727750458237

Epoch: 5| Step: 3
Training loss: 2.967596211340008
Validation loss: 2.562905694279331

Epoch: 5| Step: 4
Training loss: 2.7270525735082516
Validation loss: 2.561735023508117

Epoch: 5| Step: 5
Training loss: 2.2694080207710536
Validation loss: 2.558895533966443

Epoch: 5| Step: 6
Training loss: 2.640620937005704
Validation loss: 2.5590651798548696

Epoch: 5| Step: 7
Training loss: 2.401571307789741
Validation loss: 2.5560401850604957

Epoch: 5| Step: 8
Training loss: 2.772928712409319
Validation loss: 2.5499632080079238

Epoch: 5| Step: 9
Training loss: 2.6866475017359432
Validation loss: 2.54633931804366

Epoch: 5| Step: 10
Training loss: 2.224251173216545
Validation loss: 2.542786888108276

Epoch: 5| Step: 11
Training loss: 2.2929103135521136
Validation loss: 2.5426678652878896

Epoch: 68| Step: 0
Training loss: 2.756652242153058
Validation loss: 2.5428387383559845

Epoch: 5| Step: 1
Training loss: 2.3016735995347313
Validation loss: 2.5399295069988663

Epoch: 5| Step: 2
Training loss: 2.5644745081339084
Validation loss: 2.5480363417431153

Epoch: 5| Step: 3
Training loss: 2.924519856487764
Validation loss: 2.5556969004625123

Epoch: 5| Step: 4
Training loss: 3.029280347059602
Validation loss: 2.539185435913578

Epoch: 5| Step: 5
Training loss: 2.195402530439555
Validation loss: 2.5347354228112895

Epoch: 5| Step: 6
Training loss: 2.93411579388806
Validation loss: 2.536233724927629

Epoch: 5| Step: 7
Training loss: 2.2750046824312755
Validation loss: 2.538022110848223

Epoch: 5| Step: 8
Training loss: 2.7855970721335144
Validation loss: 2.5381916880791953

Epoch: 5| Step: 9
Training loss: 2.6898259588047084
Validation loss: 2.5371103773734225

Epoch: 5| Step: 10
Training loss: 2.6734848080701585
Validation loss: 2.5344394087691478

Epoch: 5| Step: 11
Training loss: 3.1587676025673064
Validation loss: 2.5351023185804644

Epoch: 69| Step: 0
Training loss: 2.7365516936125953
Validation loss: 2.5347712713805652

Epoch: 5| Step: 1
Training loss: 2.6988798113616577
Validation loss: 2.5363349936783126

Epoch: 5| Step: 2
Training loss: 2.700791956290249
Validation loss: 2.5289522503289112

Epoch: 5| Step: 3
Training loss: 2.8405040618700914
Validation loss: 2.5322402069081194

Epoch: 5| Step: 4
Training loss: 2.4833442901096525
Validation loss: 2.5338918690295813

Epoch: 5| Step: 5
Training loss: 2.6352769314557145
Validation loss: 2.5320955520401194

Epoch: 5| Step: 6
Training loss: 3.1798749488858453
Validation loss: 2.5308038608801535

Epoch: 5| Step: 7
Training loss: 2.5605163339616435
Validation loss: 2.529396921189625

Epoch: 5| Step: 8
Training loss: 2.1555167900392505
Validation loss: 2.5272751002394642

Epoch: 5| Step: 9
Training loss: 2.357180958394513
Validation loss: 2.5288202256404557

Epoch: 5| Step: 10
Training loss: 2.673213155174508
Validation loss: 2.5275756281471717

Epoch: 5| Step: 11
Training loss: 3.4226636282661285
Validation loss: 2.5261314821825978

Epoch: 70| Step: 0
Training loss: 2.5978572452197075
Validation loss: 2.527680124561255

Epoch: 5| Step: 1
Training loss: 2.670973240782939
Validation loss: 2.524608541114005

Epoch: 5| Step: 2
Training loss: 2.897231681453112
Validation loss: 2.5252308727381307

Epoch: 5| Step: 3
Training loss: 2.8178172393078698
Validation loss: 2.5253127760466665

Epoch: 5| Step: 4
Training loss: 2.8139134564027573
Validation loss: 2.5211125657930302

Epoch: 5| Step: 5
Training loss: 2.7320507525565256
Validation loss: 2.52323348221531

Epoch: 5| Step: 6
Training loss: 2.3762453729141995
Validation loss: 2.523591953113426

Epoch: 5| Step: 7
Training loss: 2.5331442068008547
Validation loss: 2.5218639723336946

Epoch: 5| Step: 8
Training loss: 2.7381447228803486
Validation loss: 2.5194553728531375

Epoch: 5| Step: 9
Training loss: 2.520291756286204
Validation loss: 2.521784411209221

Epoch: 5| Step: 10
Training loss: 2.445680438260753
Validation loss: 2.5148605899142296

Epoch: 5| Step: 11
Training loss: 2.588209922814072
Validation loss: 2.5208569412597224

Epoch: 71| Step: 0
Training loss: 2.9928737879230742
Validation loss: 2.5260941581909004

Epoch: 5| Step: 1
Training loss: 2.263258125205473
Validation loss: 2.5236896904586374

Epoch: 5| Step: 2
Training loss: 2.4409355014904683
Validation loss: 2.5236339905949166

Epoch: 5| Step: 3
Training loss: 2.751883988496793
Validation loss: 2.5325980737166462

Epoch: 5| Step: 4
Training loss: 3.3364936311450357
Validation loss: 2.526447131201107

Epoch: 5| Step: 5
Training loss: 2.8058558503725863
Validation loss: 2.5199934404876907

Epoch: 5| Step: 6
Training loss: 2.4566083340942066
Validation loss: 2.515147533083692

Epoch: 5| Step: 7
Training loss: 2.6171632680198167
Validation loss: 2.516909778665074

Epoch: 5| Step: 8
Training loss: 2.6088422585493123
Validation loss: 2.519805569956663

Epoch: 5| Step: 9
Training loss: 2.336983232953013
Validation loss: 2.522607850017747

Epoch: 5| Step: 10
Training loss: 2.583536468230751
Validation loss: 2.5252894484296284

Epoch: 5| Step: 11
Training loss: 2.182018715139211
Validation loss: 2.5287699421768686

Epoch: 72| Step: 0
Training loss: 2.8114361764365245
Validation loss: 2.5293530000132534

Epoch: 5| Step: 1
Training loss: 2.343772481174733
Validation loss: 2.5318326632675614

Epoch: 5| Step: 2
Training loss: 2.4265931470793856
Validation loss: 2.5216672019953728

Epoch: 5| Step: 3
Training loss: 2.5566702798008
Validation loss: 2.5209935875598863

Epoch: 5| Step: 4
Training loss: 2.578389287175842
Validation loss: 2.516788253204973

Epoch: 5| Step: 5
Training loss: 2.8244205049283417
Validation loss: 2.514710404209699

Epoch: 5| Step: 6
Training loss: 2.8473323852399344
Validation loss: 2.515432125303336

Epoch: 5| Step: 7
Training loss: 2.9064573808738254
Validation loss: 2.5174175810052426

Epoch: 5| Step: 8
Training loss: 2.580225787965527
Validation loss: 2.5126717847169084

Epoch: 5| Step: 9
Training loss: 2.6158387309886915
Validation loss: 2.5146070162130107

Epoch: 5| Step: 10
Training loss: 2.6964544116381735
Validation loss: 2.509960906153643

Epoch: 5| Step: 11
Training loss: 2.344662806459571
Validation loss: 2.511425410534438

Epoch: 73| Step: 0
Training loss: 2.7960619890274923
Validation loss: 2.511266019924189

Epoch: 5| Step: 1
Training loss: 2.4915257834046423
Validation loss: 2.506835117198779

Epoch: 5| Step: 2
Training loss: 2.871272780876479
Validation loss: 2.507003515051878

Epoch: 5| Step: 3
Training loss: 2.829703717577595
Validation loss: 2.5097436609556403

Epoch: 5| Step: 4
Training loss: 2.7248170178774034
Validation loss: 2.5042399413696486

Epoch: 5| Step: 5
Training loss: 2.723815846165901
Validation loss: 2.5051931723111456

Epoch: 5| Step: 6
Training loss: 2.0651343889819045
Validation loss: 2.5190283734299967

Epoch: 5| Step: 7
Training loss: 2.541102883602171
Validation loss: 2.521417634151135

Epoch: 5| Step: 8
Training loss: 2.566365652474512
Validation loss: 2.528125771662685

Epoch: 5| Step: 9
Training loss: 2.831607423835827
Validation loss: 2.514696913563648

Epoch: 5| Step: 10
Training loss: 2.4849266543396227
Validation loss: 2.5028214626601883

Epoch: 5| Step: 11
Training loss: 2.951599372887897
Validation loss: 2.504973499027206

Epoch: 74| Step: 0
Training loss: 2.656274413950304
Validation loss: 2.50540100646812

Epoch: 5| Step: 1
Training loss: 2.645952257101929
Validation loss: 2.5082816005094957

Epoch: 5| Step: 2
Training loss: 2.7694461093830838
Validation loss: 2.510676478689961

Epoch: 5| Step: 3
Training loss: 2.5821110284573843
Validation loss: 2.5176519595679214

Epoch: 5| Step: 4
Training loss: 2.878381813346402
Validation loss: 2.5267996106446837

Epoch: 5| Step: 5
Training loss: 1.7979839054528763
Validation loss: 2.5240500408414697

Epoch: 5| Step: 6
Training loss: 2.8314482832100545
Validation loss: 2.525837417420102

Epoch: 5| Step: 7
Training loss: 3.0384401601096873
Validation loss: 2.5207337617256225

Epoch: 5| Step: 8
Training loss: 2.574091860432495
Validation loss: 2.5179013877551117

Epoch: 5| Step: 9
Training loss: 2.512428195363826
Validation loss: 2.516737701716648

Epoch: 5| Step: 10
Training loss: 2.732848171657821
Validation loss: 2.5082601819086

Epoch: 5| Step: 11
Training loss: 2.414549448227008
Validation loss: 2.5057663180485235

Epoch: 75| Step: 0
Training loss: 3.0139924210805598
Validation loss: 2.50002477951485

Epoch: 5| Step: 1
Training loss: 2.806305315113978
Validation loss: 2.500074476881107

Epoch: 5| Step: 2
Training loss: 2.901737113797163
Validation loss: 2.4982794284815903

Epoch: 5| Step: 3
Training loss: 2.387279787942584
Validation loss: 2.499145405456206

Epoch: 5| Step: 4
Training loss: 2.392692121008781
Validation loss: 2.5102120008707516

Epoch: 5| Step: 5
Training loss: 2.0094456543076666
Validation loss: 2.509585414408273

Epoch: 5| Step: 6
Training loss: 2.5802390938502398
Validation loss: 2.5140012195832546

Epoch: 5| Step: 7
Training loss: 3.0898161258740666
Validation loss: 2.519860412229703

Epoch: 5| Step: 8
Training loss: 2.4383010526107807
Validation loss: 2.503626874790159

Epoch: 5| Step: 9
Training loss: 2.5018508735387313
Validation loss: 2.4984644664321443

Epoch: 5| Step: 10
Training loss: 2.7405376633546084
Validation loss: 2.498936375220314

Epoch: 5| Step: 11
Training loss: 2.4470348203650185
Validation loss: 2.4986236121238563

Epoch: 76| Step: 0
Training loss: 2.531350687167252
Validation loss: 2.4967918036791623

Epoch: 5| Step: 1
Training loss: 3.1918226364976623
Validation loss: 2.494621344043606

Epoch: 5| Step: 2
Training loss: 2.9361629283819375
Validation loss: 2.4951880557552326

Epoch: 5| Step: 3
Training loss: 2.6378977760366014
Validation loss: 2.4950210104843165

Epoch: 5| Step: 4
Training loss: 2.372716810089769
Validation loss: 2.4986288681680993

Epoch: 5| Step: 5
Training loss: 2.4432626731026303
Validation loss: 2.4958112434424358

Epoch: 5| Step: 6
Training loss: 2.6698325755457613
Validation loss: 2.4963848797915578

Epoch: 5| Step: 7
Training loss: 2.436266562580239
Validation loss: 2.4940782846393352

Epoch: 5| Step: 8
Training loss: 2.367265857369073
Validation loss: 2.495115180863919

Epoch: 5| Step: 9
Training loss: 2.2644934623827724
Validation loss: 2.4934974743325626

Epoch: 5| Step: 10
Training loss: 2.734679984386545
Validation loss: 2.49440238055064

Epoch: 5| Step: 11
Training loss: 3.0378459466545484
Validation loss: 2.498693184237681

Epoch: 77| Step: 0
Training loss: 2.8398301492861213
Validation loss: 2.492645408950971

Epoch: 5| Step: 1
Training loss: 2.7099173877412883
Validation loss: 2.4929398026036482

Epoch: 5| Step: 2
Training loss: 2.6386465753320807
Validation loss: 2.496166898782889

Epoch: 5| Step: 3
Training loss: 2.097950152900487
Validation loss: 2.4978904883948676

Epoch: 5| Step: 4
Training loss: 3.2120188727578545
Validation loss: 2.497688237887179

Epoch: 5| Step: 5
Training loss: 2.5120659524435798
Validation loss: 2.4993415164000377

Epoch: 5| Step: 6
Training loss: 3.126651327614753
Validation loss: 2.4925805382835713

Epoch: 5| Step: 7
Training loss: 2.4241593939115975
Validation loss: 2.4892134105642745

Epoch: 5| Step: 8
Training loss: 2.407626959442883
Validation loss: 2.4975168651626154

Epoch: 5| Step: 9
Training loss: 2.5323855831588467
Validation loss: 2.4940452029685485

Epoch: 5| Step: 10
Training loss: 1.9210782764458552
Validation loss: 2.491837168976376

Epoch: 5| Step: 11
Training loss: 2.844744162696584
Validation loss: 2.4876485446366297

Epoch: 78| Step: 0
Training loss: 2.7921937093895175
Validation loss: 2.494156043060531

Epoch: 5| Step: 1
Training loss: 2.960397958036434
Validation loss: 2.490060566340462

Epoch: 5| Step: 2
Training loss: 2.507467180761825
Validation loss: 2.4933827642098483

Epoch: 5| Step: 3
Training loss: 2.481276973702664
Validation loss: 2.4906774827252955

Epoch: 5| Step: 4
Training loss: 2.721372808121833
Validation loss: 2.4918725862117417

Epoch: 5| Step: 5
Training loss: 2.6363671640593744
Validation loss: 2.4909300227673743

Epoch: 5| Step: 6
Training loss: 2.6205528371979807
Validation loss: 2.4885503921792274

Epoch: 5| Step: 7
Training loss: 2.4200944815863465
Validation loss: 2.491792151205486

Epoch: 5| Step: 8
Training loss: 2.4214737098238674
Validation loss: 2.492936642578604

Epoch: 5| Step: 9
Training loss: 2.573958665967915
Validation loss: 2.490777943445049

Epoch: 5| Step: 10
Training loss: 2.4506205026030288
Validation loss: 2.4910351352128

Epoch: 5| Step: 11
Training loss: 3.0946388894274937
Validation loss: 2.491659800148968

Epoch: 79| Step: 0
Training loss: 2.6462475209681773
Validation loss: 2.486188194890146

Epoch: 5| Step: 1
Training loss: 2.7045229786287917
Validation loss: 2.4928466220238907

Epoch: 5| Step: 2
Training loss: 2.998355096635897
Validation loss: 2.4977498259409563

Epoch: 5| Step: 3
Training loss: 2.524959707410764
Validation loss: 2.4892010508336617

Epoch: 5| Step: 4
Training loss: 2.6612260770640757
Validation loss: 2.491864569139619

Epoch: 5| Step: 5
Training loss: 2.238067670022371
Validation loss: 2.500440077989651

Epoch: 5| Step: 6
Training loss: 2.3322083167964585
Validation loss: 2.4960067845309903

Epoch: 5| Step: 7
Training loss: 2.451871612492034
Validation loss: 2.4954302746252104

Epoch: 5| Step: 8
Training loss: 2.4845878461913746
Validation loss: 2.491062101541023

Epoch: 5| Step: 9
Training loss: 3.003317746775746
Validation loss: 2.4904884478039375

Epoch: 5| Step: 10
Training loss: 2.325952117558276
Validation loss: 2.4837902788512864

Epoch: 5| Step: 11
Training loss: 2.9773614567090245
Validation loss: 2.4940370295714143

Epoch: 80| Step: 0
Training loss: 2.879731969546934
Validation loss: 2.4870465827046337

Epoch: 5| Step: 1
Training loss: 2.6189962485258507
Validation loss: 2.4952740743724426

Epoch: 5| Step: 2
Training loss: 2.3262565334746923
Validation loss: 2.488220190140434

Epoch: 5| Step: 3
Training loss: 2.8077431187790354
Validation loss: 2.489918472099239

Epoch: 5| Step: 4
Training loss: 2.4646952685628745
Validation loss: 2.485995046355943

Epoch: 5| Step: 5
Training loss: 2.523429658009535
Validation loss: 2.486157299804926

Epoch: 5| Step: 6
Training loss: 2.5640827036357434
Validation loss: 2.4881777060627077

Epoch: 5| Step: 7
Training loss: 2.7660161835479067
Validation loss: 2.488914209623292

Epoch: 5| Step: 8
Training loss: 2.93507853596255
Validation loss: 2.4893849678232662

Epoch: 5| Step: 9
Training loss: 2.3273004697506217
Validation loss: 2.4930335734279905

Epoch: 5| Step: 10
Training loss: 2.3681585252834747
Validation loss: 2.4877667017315273

Epoch: 5| Step: 11
Training loss: 2.7389042427229295
Validation loss: 2.4889140539609547

Epoch: 81| Step: 0
Training loss: 2.396247485203784
Validation loss: 2.4882869311177522

Epoch: 5| Step: 1
Training loss: 2.8188849981422557
Validation loss: 2.481872930091181

Epoch: 5| Step: 2
Training loss: 2.255525902875261
Validation loss: 2.481341987749617

Epoch: 5| Step: 3
Training loss: 2.5002919026668184
Validation loss: 2.4805445021228447

Epoch: 5| Step: 4
Training loss: 2.772290403597477
Validation loss: 2.48385023976775

Epoch: 5| Step: 5
Training loss: 2.405396347219983
Validation loss: 2.4828555100790606

Epoch: 5| Step: 6
Training loss: 2.217627012793426
Validation loss: 2.4849451118678294

Epoch: 5| Step: 7
Training loss: 2.708927079676408
Validation loss: 2.4795603172474623

Epoch: 5| Step: 8
Training loss: 2.6307696558685576
Validation loss: 2.4843529874198307

Epoch: 5| Step: 9
Training loss: 3.017728240097027
Validation loss: 2.4827776298050823

Epoch: 5| Step: 10
Training loss: 2.6433312808201612
Validation loss: 2.4809843793979742

Epoch: 5| Step: 11
Training loss: 3.3398306149230015
Validation loss: 2.481408221135116

Epoch: 82| Step: 0
Training loss: 2.393160802773121
Validation loss: 2.4907780870258365

Epoch: 5| Step: 1
Training loss: 2.1181523563242686
Validation loss: 2.5172635541113975

Epoch: 5| Step: 2
Training loss: 3.0988365512715546
Validation loss: 2.5665496368067284

Epoch: 5| Step: 3
Training loss: 2.644877694228135
Validation loss: 2.566561368592652

Epoch: 5| Step: 4
Training loss: 2.7923811287710443
Validation loss: 2.613463189710642

Epoch: 5| Step: 5
Training loss: 2.9312375456783326
Validation loss: 2.579613991592509

Epoch: 5| Step: 6
Training loss: 2.937175651661953
Validation loss: 2.519751183819492

Epoch: 5| Step: 7
Training loss: 2.2388615946003925
Validation loss: 2.4920152586726974

Epoch: 5| Step: 8
Training loss: 2.1934070938639825
Validation loss: 2.4896002028038713

Epoch: 5| Step: 9
Training loss: 2.8453483700782463
Validation loss: 2.480499876132422

Epoch: 5| Step: 10
Training loss: 2.790889024365835
Validation loss: 2.482643647771037

Epoch: 5| Step: 11
Training loss: 2.8728641371503985
Validation loss: 2.4902615713990555

Epoch: 83| Step: 0
Training loss: 2.4566224065836924
Validation loss: 2.488495302999268

Epoch: 5| Step: 1
Training loss: 2.224491266961853
Validation loss: 2.498901137608718

Epoch: 5| Step: 2
Training loss: 2.953243515623668
Validation loss: 2.50418028222147

Epoch: 5| Step: 3
Training loss: 2.813692221673231
Validation loss: 2.518390375531026

Epoch: 5| Step: 4
Training loss: 3.0357195749958024
Validation loss: 2.5216808286883965

Epoch: 5| Step: 5
Training loss: 2.9476566447144745
Validation loss: 2.519117797520993

Epoch: 5| Step: 6
Training loss: 2.0940413770477035
Validation loss: 2.518017876698455

Epoch: 5| Step: 7
Training loss: 2.3096113979564423
Validation loss: 2.5100433908374713

Epoch: 5| Step: 8
Training loss: 2.5560538927882828
Validation loss: 2.505347628035308

Epoch: 5| Step: 9
Training loss: 2.6314923969893202
Validation loss: 2.494938869288038

Epoch: 5| Step: 10
Training loss: 2.7232838677622393
Validation loss: 2.4899516903864773

Epoch: 5| Step: 11
Training loss: 2.663228819049303
Validation loss: 2.4849254270310666

Epoch: 84| Step: 0
Training loss: 2.4543011998173836
Validation loss: 2.4861172419092985

Epoch: 5| Step: 1
Training loss: 2.654325438760491
Validation loss: 2.4829523303645855

Epoch: 5| Step: 2
Training loss: 2.7484556543424015
Validation loss: 2.4779017664968817

Epoch: 5| Step: 3
Training loss: 2.585267962765279
Validation loss: 2.479861860549041

Epoch: 5| Step: 4
Training loss: 2.5031437181399174
Validation loss: 2.4791193177539514

Epoch: 5| Step: 5
Training loss: 2.5332492421231048
Validation loss: 2.4717175920268595

Epoch: 5| Step: 6
Training loss: 2.382020111908713
Validation loss: 2.4698842859190644

Epoch: 5| Step: 7
Training loss: 2.683580068139704
Validation loss: 2.4673224438224546

Epoch: 5| Step: 8
Training loss: 3.160075371724616
Validation loss: 2.4678586485461644

Epoch: 5| Step: 9
Training loss: 2.0763907402424406
Validation loss: 2.4694219049118336

Epoch: 5| Step: 10
Training loss: 2.6792712541715846
Validation loss: 2.469425487254835

Epoch: 5| Step: 11
Training loss: 1.7481437784113094
Validation loss: 2.4799178465379748

Epoch: 85| Step: 0
Training loss: 2.1920209760219147
Validation loss: 2.4826433156529037

Epoch: 5| Step: 1
Training loss: 2.4422126597879763
Validation loss: 2.4800173371231984

Epoch: 5| Step: 2
Training loss: 2.791532902335958
Validation loss: 2.4774221859595795

Epoch: 5| Step: 3
Training loss: 2.793490189113974
Validation loss: 2.465168729498533

Epoch: 5| Step: 4
Training loss: 3.1752729583912194
Validation loss: 2.4769765104697776

Epoch: 5| Step: 5
Training loss: 2.8292116216834247
Validation loss: 2.476477419959182

Epoch: 5| Step: 6
Training loss: 2.5771776713006584
Validation loss: 2.477655304038936

Epoch: 5| Step: 7
Training loss: 2.53172015309626
Validation loss: 2.480104498763794

Epoch: 5| Step: 8
Training loss: 2.612468598829583
Validation loss: 2.481999106914965

Epoch: 5| Step: 9
Training loss: 1.9328674185250847
Validation loss: 2.4827588620722785

Epoch: 5| Step: 10
Training loss: 2.499160148694361
Validation loss: 2.479123384968761

Epoch: 5| Step: 11
Training loss: 2.18823044706709
Validation loss: 2.4804382963138334

Epoch: 86| Step: 0
Training loss: 2.8341383257956294
Validation loss: 2.478196973585279

Epoch: 5| Step: 1
Training loss: 2.6969249380357114
Validation loss: 2.4760647463067245

Epoch: 5| Step: 2
Training loss: 2.5425751318469283
Validation loss: 2.4784045704319966

Epoch: 5| Step: 3
Training loss: 2.921440214559609
Validation loss: 2.4770582567319357

Epoch: 5| Step: 4
Training loss: 2.6122196247833043
Validation loss: 2.4814372857594647

Epoch: 5| Step: 5
Training loss: 2.2983785884939327
Validation loss: 2.471614809163187

Epoch: 5| Step: 6
Training loss: 2.383158074006146
Validation loss: 2.475483148711894

Epoch: 5| Step: 7
Training loss: 2.5675570202878064
Validation loss: 2.4787162176533903

Epoch: 5| Step: 8
Training loss: 2.479299771451603
Validation loss: 2.4791132509797573

Epoch: 5| Step: 9
Training loss: 2.413294203881853
Validation loss: 2.4696537427925334

Epoch: 5| Step: 10
Training loss: 2.426082967032918
Validation loss: 2.4781830636943365

Epoch: 5| Step: 11
Training loss: 3.186796522115
Validation loss: 2.4727578606802463

Epoch: 87| Step: 0
Training loss: 2.475110997494992
Validation loss: 2.4744848126155863

Epoch: 5| Step: 1
Training loss: 2.491546644129025
Validation loss: 2.4812958987752753

Epoch: 5| Step: 2
Training loss: 2.334091630969156
Validation loss: 2.4820810079464968

Epoch: 5| Step: 3
Training loss: 2.834876836829543
Validation loss: 2.4758285333370114

Epoch: 5| Step: 4
Training loss: 2.8349335209555275
Validation loss: 2.4802773189016154

Epoch: 5| Step: 5
Training loss: 2.64130677219511
Validation loss: 2.482933661975705

Epoch: 5| Step: 6
Training loss: 2.394083749796067
Validation loss: 2.4774857814674647

Epoch: 5| Step: 7
Training loss: 2.5337045330678696
Validation loss: 2.4765956848360062

Epoch: 5| Step: 8
Training loss: 2.581113066285494
Validation loss: 2.4809322975335744

Epoch: 5| Step: 9
Training loss: 2.4188530557447354
Validation loss: 2.475685068486956

Epoch: 5| Step: 10
Training loss: 2.828074165013818
Validation loss: 2.4760969147531875

Epoch: 5| Step: 11
Training loss: 2.4302615747834424
Validation loss: 2.477280517688631

Epoch: 88| Step: 0
Training loss: 2.297428090461389
Validation loss: 2.4773264006656097

Epoch: 5| Step: 1
Training loss: 2.7085412948189926
Validation loss: 2.4760377650911574

Epoch: 5| Step: 2
Training loss: 2.3813036534630223
Validation loss: 2.4715646761980588

Epoch: 5| Step: 3
Training loss: 2.4874006356947875
Validation loss: 2.4733150542638547

Epoch: 5| Step: 4
Training loss: 2.273865020510286
Validation loss: 2.471335843456799

Epoch: 5| Step: 5
Training loss: 2.790245510497257
Validation loss: 2.465010607634051

Epoch: 5| Step: 6
Training loss: 3.0092826916252373
Validation loss: 2.4748836045237566

Epoch: 5| Step: 7
Training loss: 2.5311803925915846
Validation loss: 2.4757025074898094

Epoch: 5| Step: 8
Training loss: 2.7938866874146915
Validation loss: 2.472625800533341

Epoch: 5| Step: 9
Training loss: 2.575929759045296
Validation loss: 2.469695690762025

Epoch: 5| Step: 10
Training loss: 2.191695848664727
Validation loss: 2.4732357669833243

Epoch: 5| Step: 11
Training loss: 3.443455617406778
Validation loss: 2.484056052441407

Epoch: 89| Step: 0
Training loss: 2.804631224681334
Validation loss: 2.4667144705247033

Epoch: 5| Step: 1
Training loss: 3.234461981299468
Validation loss: 2.476882139772066

Epoch: 5| Step: 2
Training loss: 2.2376694300771955
Validation loss: 2.4697144350815274

Epoch: 5| Step: 3
Training loss: 2.04740491224741
Validation loss: 2.47659224723977

Epoch: 5| Step: 4
Training loss: 2.1310217189747136
Validation loss: 2.4781600941646054

Epoch: 5| Step: 5
Training loss: 2.549268569015237
Validation loss: 2.47477686675737

Epoch: 5| Step: 6
Training loss: 2.214167738993075
Validation loss: 2.4736542759147504

Epoch: 5| Step: 7
Training loss: 2.285166474669967
Validation loss: 2.4748562130438714

Epoch: 5| Step: 8
Training loss: 3.295200302218104
Validation loss: 2.4777508881798607

Epoch: 5| Step: 9
Training loss: 2.7044446072525705
Validation loss: 2.4769729530845503

Epoch: 5| Step: 10
Training loss: 2.5997258481960763
Validation loss: 2.4762683617522314

Epoch: 5| Step: 11
Training loss: 2.8642409981287065
Validation loss: 2.4748754441101797

Epoch: 90| Step: 0
Training loss: 2.6494244112346266
Validation loss: 2.4788717420380038

Epoch: 5| Step: 1
Training loss: 2.84487842346975
Validation loss: 2.4741859826537094

Epoch: 5| Step: 2
Training loss: 2.2804756417975685
Validation loss: 2.479290659926591

Epoch: 5| Step: 3
Training loss: 2.606853706091809
Validation loss: 2.478257803387034

Epoch: 5| Step: 4
Training loss: 2.5235571109544033
Validation loss: 2.477547358487837

Epoch: 5| Step: 5
Training loss: 1.9574077190800054
Validation loss: 2.4746608392572162

Epoch: 5| Step: 6
Training loss: 3.095773516796305
Validation loss: 2.480309909330411

Epoch: 5| Step: 7
Training loss: 2.894906991311169
Validation loss: 2.4784542604209254

Epoch: 5| Step: 8
Training loss: 2.1679158522192385
Validation loss: 2.477098240552171

Epoch: 5| Step: 9
Training loss: 2.590975184867631
Validation loss: 2.473678128620188

Epoch: 5| Step: 10
Training loss: 2.460278232005851
Validation loss: 2.479678487170417

Epoch: 5| Step: 11
Training loss: 2.84330662215798
Validation loss: 2.4722571771599626

Epoch: 91| Step: 0
Training loss: 2.475282645221869
Validation loss: 2.4751390202970946

Epoch: 5| Step: 1
Training loss: 2.697832869672988
Validation loss: 2.4851551990093084

Epoch: 5| Step: 2
Training loss: 2.7585012640259587
Validation loss: 2.4971861660124377

Epoch: 5| Step: 3
Training loss: 2.600573744191984
Validation loss: 2.4935697272081145

Epoch: 5| Step: 4
Training loss: 2.8634439502823676
Validation loss: 2.4943907534392493

Epoch: 5| Step: 5
Training loss: 2.82628997787957
Validation loss: 2.4743968427759833

Epoch: 5| Step: 6
Training loss: 2.529468427459156
Validation loss: 2.4688456633499047

Epoch: 5| Step: 7
Training loss: 2.335136250639674
Validation loss: 2.4746587397612676

Epoch: 5| Step: 8
Training loss: 2.5804322062580427
Validation loss: 2.4750023148265763

Epoch: 5| Step: 9
Training loss: 2.5392024075035793
Validation loss: 2.475092470644392

Epoch: 5| Step: 10
Training loss: 2.3477690760053096
Validation loss: 2.480727142732542

Epoch: 5| Step: 11
Training loss: 2.75893796265759
Validation loss: 2.4766708494726966

Epoch: 92| Step: 0
Training loss: 2.8095545285987815
Validation loss: 2.478062998447825

Epoch: 5| Step: 1
Training loss: 2.583604480500802
Validation loss: 2.4807400011948224

Epoch: 5| Step: 2
Training loss: 3.1346635558246647
Validation loss: 2.4789937918091463

Epoch: 5| Step: 3
Training loss: 3.2097444940052275
Validation loss: 2.4779415081924188

Epoch: 5| Step: 4
Training loss: 2.74587868598527
Validation loss: 2.4773107294909327

Epoch: 5| Step: 5
Training loss: 2.483806137118662
Validation loss: 2.478265900552819

Epoch: 5| Step: 6
Training loss: 1.857166998832066
Validation loss: 2.4767358580844276

Epoch: 5| Step: 7
Training loss: 2.4851034763828403
Validation loss: 2.479303605975205

Epoch: 5| Step: 8
Training loss: 2.3213609580788748
Validation loss: 2.4782757053033024

Epoch: 5| Step: 9
Training loss: 2.1653815150483045
Validation loss: 2.4780601802460596

Epoch: 5| Step: 10
Training loss: 2.4540514320387232
Validation loss: 2.4763128995216213

Epoch: 5| Step: 11
Training loss: 2.4540294753514176
Validation loss: 2.475014500623668

Epoch: 93| Step: 0
Training loss: 2.518898013477526
Validation loss: 2.472067266125151

Epoch: 5| Step: 1
Training loss: 2.2242398109952997
Validation loss: 2.4727032994941087

Epoch: 5| Step: 2
Training loss: 2.4491770860939908
Validation loss: 2.469701764580706

Epoch: 5| Step: 3
Training loss: 2.899459288601888
Validation loss: 2.47009640608485

Epoch: 5| Step: 4
Training loss: 3.0953244913360924
Validation loss: 2.4661569054639347

Epoch: 5| Step: 5
Training loss: 2.6284261778126266
Validation loss: 2.4630290836274833

Epoch: 5| Step: 6
Training loss: 2.4304811218005695
Validation loss: 2.471369452206472

Epoch: 5| Step: 7
Training loss: 2.7794700670464465
Validation loss: 2.4694027057980357

Epoch: 5| Step: 8
Training loss: 1.8943295715583763
Validation loss: 2.4696825696537257

Epoch: 5| Step: 9
Training loss: 2.939364227268692
Validation loss: 2.467301740652907

Epoch: 5| Step: 10
Training loss: 2.4962439932103826
Validation loss: 2.463565448595119

Epoch: 5| Step: 11
Training loss: 1.6914111130712783
Validation loss: 2.4714211406951745

Epoch: 94| Step: 0
Training loss: 2.0042748065834624
Validation loss: 2.4656458706731934

Epoch: 5| Step: 1
Training loss: 2.321025083919936
Validation loss: 2.468862019987205

Epoch: 5| Step: 2
Training loss: 2.669904729736464
Validation loss: 2.4651918241056103

Epoch: 5| Step: 3
Training loss: 3.1117305555222643
Validation loss: 2.4690695346599614

Epoch: 5| Step: 4
Training loss: 2.935733020233425
Validation loss: 2.4695807962376146

Epoch: 5| Step: 5
Training loss: 2.7363223743582505
Validation loss: 2.461480882993111

Epoch: 5| Step: 6
Training loss: 2.9815216004713347
Validation loss: 2.4695586356978017

Epoch: 5| Step: 7
Training loss: 2.18659616598981
Validation loss: 2.468534717750175

Epoch: 5| Step: 8
Training loss: 2.4525890372287544
Validation loss: 2.4720742543607446

Epoch: 5| Step: 9
Training loss: 2.2432080647757577
Validation loss: 2.4717258874581263

Epoch: 5| Step: 10
Training loss: 2.299672194630963
Validation loss: 2.4716057858672102

Epoch: 5| Step: 11
Training loss: 2.839993521454635
Validation loss: 2.470888619879203

Epoch: 95| Step: 0
Training loss: 1.9753855341038007
Validation loss: 2.4680286874765422

Epoch: 5| Step: 1
Training loss: 2.207791087916726
Validation loss: 2.4677393530627976

Epoch: 5| Step: 2
Training loss: 2.7558452732906837
Validation loss: 2.4760227718335788

Epoch: 5| Step: 3
Training loss: 2.2764864791713246
Validation loss: 2.4650910138111692

Epoch: 5| Step: 4
Training loss: 3.0178281020043323
Validation loss: 2.4745697805141975

Epoch: 5| Step: 5
Training loss: 2.899130684356189
Validation loss: 2.4725775683571523

Epoch: 5| Step: 6
Training loss: 2.376523984538021
Validation loss: 2.4733359882523853

Epoch: 5| Step: 7
Training loss: 2.3833953644831865
Validation loss: 2.46962989546551

Epoch: 5| Step: 8
Training loss: 2.900573930181867
Validation loss: 2.470947637508643

Epoch: 5| Step: 9
Training loss: 2.389896655861044
Validation loss: 2.476574140615613

Epoch: 5| Step: 10
Training loss: 2.7847923265756354
Validation loss: 2.4745849230811987

Epoch: 5| Step: 11
Training loss: 2.7640203788657485
Validation loss: 2.4711480666076535

Epoch: 96| Step: 0
Training loss: 2.640761693542702
Validation loss: 2.4740454179929685

Epoch: 5| Step: 1
Training loss: 2.529411684471995
Validation loss: 2.4652619971783634

Epoch: 5| Step: 2
Training loss: 2.4161936417591905
Validation loss: 2.46704809863247

Epoch: 5| Step: 3
Training loss: 2.3414618958055406
Validation loss: 2.4699772227031165

Epoch: 5| Step: 4
Training loss: 2.4165151044301307
Validation loss: 2.4621542531355174

Epoch: 5| Step: 5
Training loss: 2.2830898233957235
Validation loss: 2.472419807816612

Epoch: 5| Step: 6
Training loss: 2.313799312649286
Validation loss: 2.4604701410781695

Epoch: 5| Step: 7
Training loss: 2.984471344516109
Validation loss: 2.466489044256242

Epoch: 5| Step: 8
Training loss: 2.9231068671387757
Validation loss: 2.4686408199842784

Epoch: 5| Step: 9
Training loss: 2.222524736264617
Validation loss: 2.468129192634345

Epoch: 5| Step: 10
Training loss: 3.011416646448289
Validation loss: 2.4686674917090747

Epoch: 5| Step: 11
Training loss: 2.2843816504009835
Validation loss: 2.4675689842027038

Epoch: 97| Step: 0
Training loss: 2.554480121373177
Validation loss: 2.4650560298700745

Epoch: 5| Step: 1
Training loss: 2.5107537725204327
Validation loss: 2.4667799167045112

Epoch: 5| Step: 2
Training loss: 2.2444737704200324
Validation loss: 2.4634450217411272

Epoch: 5| Step: 3
Training loss: 2.3452239424564545
Validation loss: 2.4761623576761105

Epoch: 5| Step: 4
Training loss: 2.2930800038890573
Validation loss: 2.4883631877892514

Epoch: 5| Step: 5
Training loss: 2.7291357800626312
Validation loss: 2.494937264660909

Epoch: 5| Step: 6
Training loss: 2.4787358996999713
Validation loss: 2.463673575312867

Epoch: 5| Step: 7
Training loss: 2.8326597160971714
Validation loss: 2.467043081336499

Epoch: 5| Step: 8
Training loss: 2.8197254193324723
Validation loss: 2.4756362879822484

Epoch: 5| Step: 9
Training loss: 2.643828125288574
Validation loss: 2.468857601895118

Epoch: 5| Step: 10
Training loss: 2.6374087638803676
Validation loss: 2.4734916139692738

Epoch: 5| Step: 11
Training loss: 3.061044152326169
Validation loss: 2.4712206111363844

Epoch: 98| Step: 0
Training loss: 2.5191514786155067
Validation loss: 2.4765956808248113

Epoch: 5| Step: 1
Training loss: 2.6345824679203793
Validation loss: 2.469129802766843

Epoch: 5| Step: 2
Training loss: 2.5184780078114795
Validation loss: 2.474531538295422

Epoch: 5| Step: 3
Training loss: 3.0754536301874404
Validation loss: 2.4710618393226857

Epoch: 5| Step: 4
Training loss: 2.217250048324373
Validation loss: 2.4741967551471618

Epoch: 5| Step: 5
Training loss: 2.7525678263526725
Validation loss: 2.473909664009948

Epoch: 5| Step: 6
Training loss: 2.8480236093849816
Validation loss: 2.474072750189654

Epoch: 5| Step: 7
Training loss: 2.5568034424019794
Validation loss: 2.4798906068042705

Epoch: 5| Step: 8
Training loss: 2.5600994309844882
Validation loss: 2.475624440311971

Epoch: 5| Step: 9
Training loss: 2.4224409857176434
Validation loss: 2.4724348872102877

Epoch: 5| Step: 10
Training loss: 2.2959835891801728
Validation loss: 2.4715740493271126

Epoch: 5| Step: 11
Training loss: 2.2781202193263668
Validation loss: 2.4722535788289357

Epoch: 99| Step: 0
Training loss: 2.354153343320521
Validation loss: 2.478015373213595

Epoch: 5| Step: 1
Training loss: 2.547828168312012
Validation loss: 2.48273367815674

Epoch: 5| Step: 2
Training loss: 2.404153777759542
Validation loss: 2.478128164962024

Epoch: 5| Step: 3
Training loss: 2.408213322255124
Validation loss: 2.472708108447561

Epoch: 5| Step: 4
Training loss: 2.318383027930626
Validation loss: 2.4753365234896734

Epoch: 5| Step: 5
Training loss: 2.8886411715533593
Validation loss: 2.4750514630384295

Epoch: 5| Step: 6
Training loss: 2.720541878746525
Validation loss: 2.472026140018736

Epoch: 5| Step: 7
Training loss: 2.7532310577990717
Validation loss: 2.466328306338989

Epoch: 5| Step: 8
Training loss: 2.6351934245450797
Validation loss: 2.4737054207003926

Epoch: 5| Step: 9
Training loss: 2.659786697239234
Validation loss: 2.475550371525264

Epoch: 5| Step: 10
Training loss: 2.431767888549756
Validation loss: 2.4697767488653075

Epoch: 5| Step: 11
Training loss: 2.8336883958229393
Validation loss: 2.4800040783515334

Epoch: 100| Step: 0
Training loss: 2.429870991049132
Validation loss: 2.4730476282584872

Epoch: 5| Step: 1
Training loss: 2.2569346057644792
Validation loss: 2.476919671906442

Epoch: 5| Step: 2
Training loss: 2.6015131032108947
Validation loss: 2.4726307703425934

Epoch: 5| Step: 3
Training loss: 2.5735822015044305
Validation loss: 2.474643127965926

Epoch: 5| Step: 4
Training loss: 3.1314318271009447
Validation loss: 2.4799101853931123

Epoch: 5| Step: 5
Training loss: 2.999479884519434
Validation loss: 2.4847691821032036

Epoch: 5| Step: 6
Training loss: 2.9505946707626265
Validation loss: 2.4915100081632757

Epoch: 5| Step: 7
Training loss: 2.39966097265028
Validation loss: 2.498257550344997

Epoch: 5| Step: 8
Training loss: 2.812237960476317
Validation loss: 2.4956416604683067

Epoch: 5| Step: 9
Training loss: 2.222865947894373
Validation loss: 2.4999477738647338

Epoch: 5| Step: 10
Training loss: 1.8667955186627017
Validation loss: 2.4990648109953564

Epoch: 5| Step: 11
Training loss: 2.924440614078774
Validation loss: 2.49505825383709

Epoch: 101| Step: 0
Training loss: 2.4731288657111206
Validation loss: 2.500998990813449

Epoch: 5| Step: 1
Training loss: 2.6181631518671487
Validation loss: 2.499322052267845

Epoch: 5| Step: 2
Training loss: 2.722436969962743
Validation loss: 2.4999790468927334

Epoch: 5| Step: 3
Training loss: 2.8771182010940053
Validation loss: 2.4925390770838782

Epoch: 5| Step: 4
Training loss: 2.60614434995507
Validation loss: 2.4909977359975835

Epoch: 5| Step: 5
Training loss: 2.2236533987024676
Validation loss: 2.4891732541859057

Epoch: 5| Step: 6
Training loss: 2.318532035844882
Validation loss: 2.4813929800865324

Epoch: 5| Step: 7
Training loss: 3.00971904622053
Validation loss: 2.4829944877594055

Epoch: 5| Step: 8
Training loss: 2.528544451759776
Validation loss: 2.4781400467623063

Epoch: 5| Step: 9
Training loss: 2.7263761527753503
Validation loss: 2.474546462305303

Epoch: 5| Step: 10
Training loss: 2.6605837306235434
Validation loss: 2.4724645916994423

Epoch: 5| Step: 11
Training loss: 1.495880589849994
Validation loss: 2.475566768133119

Epoch: 102| Step: 0
Training loss: 1.9511132217160227
Validation loss: 2.473181778682833

Epoch: 5| Step: 1
Training loss: 2.242132950494705
Validation loss: 2.4715305877733407

Epoch: 5| Step: 2
Training loss: 2.5052405743232717
Validation loss: 2.471506314406116

Epoch: 5| Step: 3
Training loss: 2.757452317186133
Validation loss: 2.4660081485929233

Epoch: 5| Step: 4
Training loss: 2.7656168964504335
Validation loss: 2.473401030293491

Epoch: 5| Step: 5
Training loss: 2.5438578702282326
Validation loss: 2.469754682596757

Epoch: 5| Step: 6
Training loss: 2.199494095635039
Validation loss: 2.4724282053586766

Epoch: 5| Step: 7
Training loss: 2.7673810136229067
Validation loss: 2.4644244402122375

Epoch: 5| Step: 8
Training loss: 2.754100343614545
Validation loss: 2.4654826380602457

Epoch: 5| Step: 9
Training loss: 2.3264456202736774
Validation loss: 2.469393863494897

Epoch: 5| Step: 10
Training loss: 2.9939856641549785
Validation loss: 2.466006605708991

Epoch: 5| Step: 11
Training loss: 3.4537907989886576
Validation loss: 2.463012534988201

Epoch: 103| Step: 0
Training loss: 2.840108866955573
Validation loss: 2.4672067905192905

Epoch: 5| Step: 1
Training loss: 2.965460792232147
Validation loss: 2.4947737146951474

Epoch: 5| Step: 2
Training loss: 2.3565960617316124
Validation loss: 2.5127658352256597

Epoch: 5| Step: 3
Training loss: 2.91492471901998
Validation loss: 2.5289383328500095

Epoch: 5| Step: 4
Training loss: 2.8524710266086144
Validation loss: 2.500018286638137

Epoch: 5| Step: 5
Training loss: 2.052085243908401
Validation loss: 2.4986183719719173

Epoch: 5| Step: 6
Training loss: 2.7872303306247566
Validation loss: 2.491235664870022

Epoch: 5| Step: 7
Training loss: 2.5247593302329125
Validation loss: 2.4749781798675654

Epoch: 5| Step: 8
Training loss: 2.1999431386015034
Validation loss: 2.4757518384170933

Epoch: 5| Step: 9
Training loss: 2.2993053631244953
Validation loss: 2.4642342057087165

Epoch: 5| Step: 10
Training loss: 2.8038352585311466
Validation loss: 2.4646752728629338

Epoch: 5| Step: 11
Training loss: 2.088806122916754
Validation loss: 2.4643836581963052

Epoch: 104| Step: 0
Training loss: 3.048949645490208
Validation loss: 2.4678033430156554

Epoch: 5| Step: 1
Training loss: 2.223483556832036
Validation loss: 2.467811917285642

Epoch: 5| Step: 2
Training loss: 2.6766481651943046
Validation loss: 2.4710086558208513

Epoch: 5| Step: 3
Training loss: 2.6892542769002743
Validation loss: 2.4735671500411374

Epoch: 5| Step: 4
Training loss: 2.719654678134402
Validation loss: 2.474394353624034

Epoch: 5| Step: 5
Training loss: 2.157395873844653
Validation loss: 2.4765383682118194

Epoch: 5| Step: 6
Training loss: 2.9567106569523416
Validation loss: 2.479954515554059

Epoch: 5| Step: 7
Training loss: 2.318892330109649
Validation loss: 2.481108685544269

Epoch: 5| Step: 8
Training loss: 2.826620133604745
Validation loss: 2.483241230394906

Epoch: 5| Step: 9
Training loss: 2.3937176415868358
Validation loss: 2.4814341711369425

Epoch: 5| Step: 10
Training loss: 2.113442913648076
Validation loss: 2.482571855201739

Epoch: 5| Step: 11
Training loss: 3.1759515132517953
Validation loss: 2.4772174783375456

Epoch: 105| Step: 0
Training loss: 2.897784629974897
Validation loss: 2.4735446115525135

Epoch: 5| Step: 1
Training loss: 2.1641080004594473
Validation loss: 2.472987210591342

Epoch: 5| Step: 2
Training loss: 3.06701918386474
Validation loss: 2.4729704132495476

Epoch: 5| Step: 3
Training loss: 2.3098232138886616
Validation loss: 2.4689963093144605

Epoch: 5| Step: 4
Training loss: 2.326224658793189
Validation loss: 2.4708629330303604

Epoch: 5| Step: 5
Training loss: 2.3333184605079036
Validation loss: 2.468409436311814

Epoch: 5| Step: 6
Training loss: 2.3239720317779393
Validation loss: 2.4661408370281928

Epoch: 5| Step: 7
Training loss: 2.7897563680286748
Validation loss: 2.463983957106828

Epoch: 5| Step: 8
Training loss: 2.2800673463144125
Validation loss: 2.4677274976710537

Epoch: 5| Step: 9
Training loss: 2.5490104288551576
Validation loss: 2.466910612575576

Epoch: 5| Step: 10
Training loss: 2.8979643155277577
Validation loss: 2.471319471034641

Epoch: 5| Step: 11
Training loss: 2.6763573242219554
Validation loss: 2.475127188307636

Epoch: 106| Step: 0
Training loss: 2.4290307436081484
Validation loss: 2.4745605993845756

Epoch: 5| Step: 1
Training loss: 2.4206044463672094
Validation loss: 2.467437270383925

Epoch: 5| Step: 2
Training loss: 2.1641028224892995
Validation loss: 2.473618832767188

Epoch: 5| Step: 3
Training loss: 2.69738557122083
Validation loss: 2.4651730453977456

Epoch: 5| Step: 4
Training loss: 2.6197905846979674
Validation loss: 2.473462881703857

Epoch: 5| Step: 5
Training loss: 2.6949943022941985
Validation loss: 2.4831523765861836

Epoch: 5| Step: 6
Training loss: 2.5120169787349442
Validation loss: 2.473813413424358

Epoch: 5| Step: 7
Training loss: 2.6279626204214313
Validation loss: 2.469280226219629

Epoch: 5| Step: 8
Training loss: 2.472889869585459
Validation loss: 2.476609744033492

Epoch: 5| Step: 9
Training loss: 3.056477756243952
Validation loss: 2.4719658360306616

Epoch: 5| Step: 10
Training loss: 2.560316690883608
Validation loss: 2.4759892222352735

Epoch: 5| Step: 11
Training loss: 2.046968647219638
Validation loss: 2.471881787354691

Epoch: 107| Step: 0
Training loss: 2.9587199177923074
Validation loss: 2.467290183119513

Epoch: 5| Step: 1
Training loss: 2.7503680069649903
Validation loss: 2.47327090031835

Epoch: 5| Step: 2
Training loss: 3.0452612099945005
Validation loss: 2.4690035134538086

Epoch: 5| Step: 3
Training loss: 2.6829019290546743
Validation loss: 2.4692368530325632

Epoch: 5| Step: 4
Training loss: 2.3401420797920114
Validation loss: 2.4725344860434864

Epoch: 5| Step: 5
Training loss: 2.3501241854056594
Validation loss: 2.471299307843725

Epoch: 5| Step: 6
Training loss: 2.496240363792498
Validation loss: 2.469756641458684

Epoch: 5| Step: 7
Training loss: 2.6013222889935417
Validation loss: 2.4725427265030504

Epoch: 5| Step: 8
Training loss: 2.690146097236189
Validation loss: 2.4719779282917305

Epoch: 5| Step: 9
Training loss: 2.266477016855042
Validation loss: 2.4725886090082967

Epoch: 5| Step: 10
Training loss: 2.0487216452684
Validation loss: 2.466525822293507

Epoch: 5| Step: 11
Training loss: 1.680219131438182
Validation loss: 2.4690568387207614

Epoch: 108| Step: 0
Training loss: 2.7424224144281117
Validation loss: 2.4754823962747654

Epoch: 5| Step: 1
Training loss: 3.019185390626689
Validation loss: 2.4667061743493535

Epoch: 5| Step: 2
Training loss: 2.0732406925262508
Validation loss: 2.4679626464768276

Epoch: 5| Step: 3
Training loss: 2.724134878317539
Validation loss: 2.4742801891905852

Epoch: 5| Step: 4
Training loss: 2.5825876164598744
Validation loss: 2.473401130702878

Epoch: 5| Step: 5
Training loss: 3.0239263725803758
Validation loss: 2.476303062929532

Epoch: 5| Step: 6
Training loss: 2.4048650149335344
Validation loss: 2.4713364423967756

Epoch: 5| Step: 7
Training loss: 2.371591229859786
Validation loss: 2.4663005580253854

Epoch: 5| Step: 8
Training loss: 1.9517501875187229
Validation loss: 2.471715502088235

Epoch: 5| Step: 9
Training loss: 2.0310046708068783
Validation loss: 2.473194515706015

Epoch: 5| Step: 10
Training loss: 2.987018472977721
Validation loss: 2.4749298490350498

Epoch: 5| Step: 11
Training loss: 1.7403664415726214
Validation loss: 2.4721289700757225

Epoch: 109| Step: 0
Training loss: 2.819475974505367
Validation loss: 2.4664998805733456

Epoch: 5| Step: 1
Training loss: 2.9064477012413743
Validation loss: 2.4625956363470265

Epoch: 5| Step: 2
Training loss: 2.686557515800763
Validation loss: 2.4800799528175195

Epoch: 5| Step: 3
Training loss: 2.6138913478787953
Validation loss: 2.467680486154548

Epoch: 5| Step: 4
Training loss: 2.549794027948324
Validation loss: 2.4653534883565333

Epoch: 5| Step: 5
Training loss: 2.8404924787741748
Validation loss: 2.4680450253776907

Epoch: 5| Step: 6
Training loss: 2.3264233815939965
Validation loss: 2.467172883473433

Epoch: 5| Step: 7
Training loss: 2.2199918468428725
Validation loss: 2.4653407591772742

Epoch: 5| Step: 8
Training loss: 2.162542146481506
Validation loss: 2.4679341597878746

Epoch: 5| Step: 9
Training loss: 2.5256225762642357
Validation loss: 2.462031800318297

Epoch: 5| Step: 10
Training loss: 2.1525577501088584
Validation loss: 2.4696954534403295

Epoch: 5| Step: 11
Training loss: 3.4252884792045597
Validation loss: 2.462268348389993

Epoch: 110| Step: 0
Training loss: 2.1713576764171183
Validation loss: 2.4702778225257727

Epoch: 5| Step: 1
Training loss: 2.5215521699166907
Validation loss: 2.467392165762846

Epoch: 5| Step: 2
Training loss: 3.023368578379087
Validation loss: 2.472714773472848

Epoch: 5| Step: 3
Training loss: 2.7047746508753407
Validation loss: 2.4664019270754203

Epoch: 5| Step: 4
Training loss: 2.2105362059555995
Validation loss: 2.469692231494378

Epoch: 5| Step: 5
Training loss: 2.252820049076923
Validation loss: 2.475413423906467

Epoch: 5| Step: 6
Training loss: 2.187181177066101
Validation loss: 2.4764377971745555

Epoch: 5| Step: 7
Training loss: 2.690846156513765
Validation loss: 2.4627072908643304

Epoch: 5| Step: 8
Training loss: 2.6217622316682068
Validation loss: 2.467546048725076

Epoch: 5| Step: 9
Training loss: 2.918897284440941
Validation loss: 2.472112699349571

Epoch: 5| Step: 10
Training loss: 2.6934594902701465
Validation loss: 2.4744307070380143

Epoch: 5| Step: 11
Training loss: 1.531824899303195
Validation loss: 2.471865736039361

Epoch: 111| Step: 0
Training loss: 2.9737787320519646
Validation loss: 2.4755684013673567

Epoch: 5| Step: 1
Training loss: 2.5511030429379478
Validation loss: 2.464005514649881

Epoch: 5| Step: 2
Training loss: 2.5741779978280355
Validation loss: 2.467955453387446

Epoch: 5| Step: 3
Training loss: 2.232436185909401
Validation loss: 2.4780344314348555

Epoch: 5| Step: 4
Training loss: 2.030619596812867
Validation loss: 2.467262045000068

Epoch: 5| Step: 5
Training loss: 2.3884975965526514
Validation loss: 2.467901301269856

Epoch: 5| Step: 6
Training loss: 2.2148130707407283
Validation loss: 2.4704485856369

Epoch: 5| Step: 7
Training loss: 2.7253468598958897
Validation loss: 2.4715095339871658

Epoch: 5| Step: 8
Training loss: 2.4951169963918716
Validation loss: 2.465434922924425

Epoch: 5| Step: 9
Training loss: 3.1956730198511685
Validation loss: 2.471452501361878

Epoch: 5| Step: 10
Training loss: 2.487409166374104
Validation loss: 2.4644354125836667

Epoch: 5| Step: 11
Training loss: 2.2572454776682704
Validation loss: 2.4674466269832784

Epoch: 112| Step: 0
Training loss: 2.342121219524259
Validation loss: 2.45845714488993

Epoch: 5| Step: 1
Training loss: 2.577818973758341
Validation loss: 2.466564490697974

Epoch: 5| Step: 2
Training loss: 2.7288205652902993
Validation loss: 2.4567470456922686

Epoch: 5| Step: 3
Training loss: 2.7809630685306654
Validation loss: 2.464049088774803

Epoch: 5| Step: 4
Training loss: 2.3500964895689758
Validation loss: 2.4603914109101206

Epoch: 5| Step: 5
Training loss: 2.848278087647903
Validation loss: 2.4597822717324753

Epoch: 5| Step: 6
Training loss: 2.4654209531181928
Validation loss: 2.46851919197821

Epoch: 5| Step: 7
Training loss: 2.4032496783352446
Validation loss: 2.454262455580451

Epoch: 5| Step: 8
Training loss: 2.317141231350163
Validation loss: 2.469315999171912

Epoch: 5| Step: 9
Training loss: 2.177045198194771
Validation loss: 2.468836038434211

Epoch: 5| Step: 10
Training loss: 2.8354508584058675
Validation loss: 2.47176738831104

Epoch: 5| Step: 11
Training loss: 2.62416372147806
Validation loss: 2.469131452328844

Epoch: 113| Step: 0
Training loss: 2.874684026189182
Validation loss: 2.4650371050476148

Epoch: 5| Step: 1
Training loss: 2.5134152012383306
Validation loss: 2.466825886261551

Epoch: 5| Step: 2
Training loss: 2.6464783390440854
Validation loss: 2.4626110623095334

Epoch: 5| Step: 3
Training loss: 2.5393714129990395
Validation loss: 2.4651125979418707

Epoch: 5| Step: 4
Training loss: 2.919453479362886
Validation loss: 2.464367582229543

Epoch: 5| Step: 5
Training loss: 2.6038227515420003
Validation loss: 2.4622620585421275

Epoch: 5| Step: 6
Training loss: 1.9602008667022965
Validation loss: 2.46352690651298

Epoch: 5| Step: 7
Training loss: 2.7313941620724598
Validation loss: 2.47198846124993

Epoch: 5| Step: 8
Training loss: 2.483684515816952
Validation loss: 2.454331330203188

Epoch: 5| Step: 9
Training loss: 2.441285446229991
Validation loss: 2.465578690075031

Epoch: 5| Step: 10
Training loss: 2.2570286218318247
Validation loss: 2.46943638913138

Epoch: 5| Step: 11
Training loss: 1.7024752576153224
Validation loss: 2.468909194095053

Epoch: 114| Step: 0
Training loss: 3.0143803216838587
Validation loss: 2.464628360323005

Epoch: 5| Step: 1
Training loss: 2.536481090863936
Validation loss: 2.464982884788031

Epoch: 5| Step: 2
Training loss: 2.3614747259858038
Validation loss: 2.4720362548378816

Epoch: 5| Step: 3
Training loss: 2.5267714452364705
Validation loss: 2.470675518298809

Epoch: 5| Step: 4
Training loss: 2.173924528796498
Validation loss: 2.472834544008644

Epoch: 5| Step: 5
Training loss: 2.3017341958907234
Validation loss: 2.4685270192652915

Epoch: 5| Step: 6
Training loss: 2.7315747554631993
Validation loss: 2.472125253013861

Epoch: 5| Step: 7
Training loss: 2.5726041925829857
Validation loss: 2.4676989036005237

Epoch: 5| Step: 8
Training loss: 2.459898811213069
Validation loss: 2.467203690143764

Epoch: 5| Step: 9
Training loss: 3.0443547707443375
Validation loss: 2.465837672207096

Epoch: 5| Step: 10
Training loss: 2.211567495787011
Validation loss: 2.467467836217401

Epoch: 5| Step: 11
Training loss: 2.194913345920484
Validation loss: 2.464499109419331

Epoch: 115| Step: 0
Training loss: 2.4421310447566906
Validation loss: 2.4618131744764256

Epoch: 5| Step: 1
Training loss: 2.62479508826329
Validation loss: 2.458420947232399

Epoch: 5| Step: 2
Training loss: 2.2507249935684244
Validation loss: 2.4610834310547505

Epoch: 5| Step: 3
Training loss: 2.991368911362687
Validation loss: 2.4566867144084545

Epoch: 5| Step: 4
Training loss: 2.501256055009596
Validation loss: 2.469086191568805

Epoch: 5| Step: 5
Training loss: 2.8212184137917915
Validation loss: 2.477111972058301

Epoch: 5| Step: 6
Training loss: 2.216414928721828
Validation loss: 2.4742192756500505

Epoch: 5| Step: 7
Training loss: 2.3007393850976796
Validation loss: 2.471682348190879

Epoch: 5| Step: 8
Training loss: 2.736121094282584
Validation loss: 2.4572059961391224

Epoch: 5| Step: 9
Training loss: 2.648328570711124
Validation loss: 2.465602203895513

Epoch: 5| Step: 10
Training loss: 2.5032662988755185
Validation loss: 2.4633323259237585

Epoch: 5| Step: 11
Training loss: 1.4389251610313416
Validation loss: 2.467201004490646

Epoch: 116| Step: 0
Training loss: 2.809115259301238
Validation loss: 2.4671106569331585

Epoch: 5| Step: 1
Training loss: 2.0626935001116458
Validation loss: 2.469249039120429

Epoch: 5| Step: 2
Training loss: 2.693780967021059
Validation loss: 2.462491431923304

Epoch: 5| Step: 3
Training loss: 3.1321392144435065
Validation loss: 2.4639843199621145

Epoch: 5| Step: 4
Training loss: 1.955771034764444
Validation loss: 2.457927732359214

Epoch: 5| Step: 5
Training loss: 2.902610715188694
Validation loss: 2.4683840838599833

Epoch: 5| Step: 6
Training loss: 1.7956294511208102
Validation loss: 2.467622032497528

Epoch: 5| Step: 7
Training loss: 2.4980226326154615
Validation loss: 2.4635103733153265

Epoch: 5| Step: 8
Training loss: 2.3472915340726117
Validation loss: 2.467547924796374

Epoch: 5| Step: 9
Training loss: 2.573778036593838
Validation loss: 2.480222851036378

Epoch: 5| Step: 10
Training loss: 2.690770044969656
Validation loss: 2.4657890131509315

Epoch: 5| Step: 11
Training loss: 3.5509469609468978
Validation loss: 2.471939750463123

Epoch: 117| Step: 0
Training loss: 2.6881493072368667
Validation loss: 2.4691952774620964

Epoch: 5| Step: 1
Training loss: 2.418479064434386
Validation loss: 2.4750879593192208

Epoch: 5| Step: 2
Training loss: 2.7256940535256873
Validation loss: 2.473613965343828

Epoch: 5| Step: 3
Training loss: 2.779576687590185
Validation loss: 2.475630006015451

Epoch: 5| Step: 4
Training loss: 1.9477590473343407
Validation loss: 2.4662846355323076

Epoch: 5| Step: 5
Training loss: 2.477401543132301
Validation loss: 2.4687349986976646

Epoch: 5| Step: 6
Training loss: 2.476598697241387
Validation loss: 2.470387897309997

Epoch: 5| Step: 7
Training loss: 2.7686418361504117
Validation loss: 2.4716018429377793

Epoch: 5| Step: 8
Training loss: 2.9149077061589455
Validation loss: 2.4733007273127092

Epoch: 5| Step: 9
Training loss: 2.514911621543064
Validation loss: 2.473440424599508

Epoch: 5| Step: 10
Training loss: 2.327053770892394
Validation loss: 2.468300335811854

Epoch: 5| Step: 11
Training loss: 1.8547599643347579
Validation loss: 2.471298278776937

Epoch: 118| Step: 0
Training loss: 2.6303803844015197
Validation loss: 2.4644697160455107

Epoch: 5| Step: 1
Training loss: 2.2377967509821026
Validation loss: 2.4722530685124418

Epoch: 5| Step: 2
Training loss: 2.726352104223446
Validation loss: 2.4765993831547495

Epoch: 5| Step: 3
Training loss: 2.6988043680938474
Validation loss: 2.4753078407321123

Epoch: 5| Step: 4
Training loss: 2.369639923491414
Validation loss: 2.4768277937736434

Epoch: 5| Step: 5
Training loss: 2.231033066747751
Validation loss: 2.470575273545575

Epoch: 5| Step: 6
Training loss: 2.4747979168320984
Validation loss: 2.4762213840079617

Epoch: 5| Step: 7
Training loss: 2.065478976768561
Validation loss: 2.4753155542473686

Epoch: 5| Step: 8
Training loss: 2.911993115238812
Validation loss: 2.4746532240597907

Epoch: 5| Step: 9
Training loss: 2.755124432753195
Validation loss: 2.470137801599238

Epoch: 5| Step: 10
Training loss: 2.6781672399833107
Validation loss: 2.4638322345654897

Epoch: 5| Step: 11
Training loss: 3.885468401662454
Validation loss: 2.4800151660542626

Epoch: 119| Step: 0
Training loss: 3.0530931453546297
Validation loss: 2.4700167259108174

Epoch: 5| Step: 1
Training loss: 2.6009756348497914
Validation loss: 2.4636216678224265

Epoch: 5| Step: 2
Training loss: 1.9006833579259217
Validation loss: 2.466942966833814

Epoch: 5| Step: 3
Training loss: 2.3563064932691096
Validation loss: 2.470491133329056

Epoch: 5| Step: 4
Training loss: 2.729652293632748
Validation loss: 2.4781780088126086

Epoch: 5| Step: 5
Training loss: 2.537423411749723
Validation loss: 2.475438088360375

Epoch: 5| Step: 6
Training loss: 2.365928390580118
Validation loss: 2.4780865661860503

Epoch: 5| Step: 7
Training loss: 2.863002289926011
Validation loss: 2.473924804594147

Epoch: 5| Step: 8
Training loss: 2.6765405621717986
Validation loss: 2.4756536932205937

Epoch: 5| Step: 9
Training loss: 2.689194433421397
Validation loss: 2.4703514924451526

Epoch: 5| Step: 10
Training loss: 2.3014325449006816
Validation loss: 2.4789152872774625

Epoch: 5| Step: 11
Training loss: 2.3060123062665627
Validation loss: 2.476392296880076

Epoch: 120| Step: 0
Training loss: 2.99948735625433
Validation loss: 2.475803587886872

Epoch: 5| Step: 1
Training loss: 2.601751188933137
Validation loss: 2.4784007946415265

Epoch: 5| Step: 2
Training loss: 2.155859428083665
Validation loss: 2.4695811984964378

Epoch: 5| Step: 3
Training loss: 2.0075354477732548
Validation loss: 2.470263005435814

Epoch: 5| Step: 4
Training loss: 2.576365662430373
Validation loss: 2.4698919761513585

Epoch: 5| Step: 5
Training loss: 2.7127783074414147
Validation loss: 2.4670169920383174

Epoch: 5| Step: 6
Training loss: 2.70532049201912
Validation loss: 2.466647758014049

Epoch: 5| Step: 7
Training loss: 2.605633640361907
Validation loss: 2.463529938931337

Epoch: 5| Step: 8
Training loss: 2.8861878026817025
Validation loss: 2.4632733557135333

Epoch: 5| Step: 9
Training loss: 2.1524492019879755
Validation loss: 2.4656874617741886

Epoch: 5| Step: 10
Training loss: 2.673995041356878
Validation loss: 2.469992264723924

Epoch: 5| Step: 11
Training loss: 2.5093670836593396
Validation loss: 2.4650633321690254

Epoch: 121| Step: 0
Training loss: 2.814727070994965
Validation loss: 2.46270660108174

Epoch: 5| Step: 1
Training loss: 2.49693157719189
Validation loss: 2.4623981843674074

Epoch: 5| Step: 2
Training loss: 2.708591996581041
Validation loss: 2.4743906279209535

Epoch: 5| Step: 3
Training loss: 1.8584020938326158
Validation loss: 2.480546656709263

Epoch: 5| Step: 4
Training loss: 3.022821565723105
Validation loss: 2.463519849673896

Epoch: 5| Step: 5
Training loss: 2.835129804112071
Validation loss: 2.4609032764779917

Epoch: 5| Step: 6
Training loss: 2.4132610089264257
Validation loss: 2.4620767691186836

Epoch: 5| Step: 7
Training loss: 2.6734564490123316
Validation loss: 2.4533703675230476

Epoch: 5| Step: 8
Training loss: 2.158976945196529
Validation loss: 2.465753160827585

Epoch: 5| Step: 9
Training loss: 2.572492979019712
Validation loss: 2.4720029284219156

Epoch: 5| Step: 10
Training loss: 2.3092646261431167
Validation loss: 2.4700891991003373

Epoch: 5| Step: 11
Training loss: 2.3498265141508736
Validation loss: 2.4786985633939405

Epoch: 122| Step: 0
Training loss: 2.35111199306038
Validation loss: 2.474326270332553

Epoch: 5| Step: 1
Training loss: 2.1839446466578605
Validation loss: 2.4794478470697783

Epoch: 5| Step: 2
Training loss: 2.486732084506396
Validation loss: 2.4757696260400284

Epoch: 5| Step: 3
Training loss: 2.471217407258614
Validation loss: 2.4732734789657864

Epoch: 5| Step: 4
Training loss: 2.806791658525384
Validation loss: 2.4717211328611497

Epoch: 5| Step: 5
Training loss: 2.8015949338874786
Validation loss: 2.4690778570933176

Epoch: 5| Step: 6
Training loss: 2.1636466718184812
Validation loss: 2.4730524887626197

Epoch: 5| Step: 7
Training loss: 2.6214665746105816
Validation loss: 2.465984582244866

Epoch: 5| Step: 8
Training loss: 2.396302207732253
Validation loss: 2.4672462052134967

Epoch: 5| Step: 9
Training loss: 2.524184361527304
Validation loss: 2.4528355397301356

Epoch: 5| Step: 10
Training loss: 2.9598257750709607
Validation loss: 2.4637916403170355

Epoch: 5| Step: 11
Training loss: 2.95939946552222
Validation loss: 2.477549104691501

Epoch: 123| Step: 0
Training loss: 2.6648391978578094
Validation loss: 2.493957707989729

Epoch: 5| Step: 1
Training loss: 2.5062525761084506
Validation loss: 2.519799873167323

Epoch: 5| Step: 2
Training loss: 2.9085464428608185
Validation loss: 2.5098697665485625

Epoch: 5| Step: 3
Training loss: 2.6314438338478365
Validation loss: 2.502985752850559

Epoch: 5| Step: 4
Training loss: 2.6140953814347565
Validation loss: 2.4903026157578236

Epoch: 5| Step: 5
Training loss: 2.25644861757797
Validation loss: 2.46977063098596

Epoch: 5| Step: 6
Training loss: 1.9708176306434355
Validation loss: 2.462238920316938

Epoch: 5| Step: 7
Training loss: 3.05259738451227
Validation loss: 2.4587811948396996

Epoch: 5| Step: 8
Training loss: 2.884682315147748
Validation loss: 2.466913634794154

Epoch: 5| Step: 9
Training loss: 2.045005818477645
Validation loss: 2.472397897731663

Epoch: 5| Step: 10
Training loss: 2.7422294939526237
Validation loss: 2.4753116573599914

Epoch: 5| Step: 11
Training loss: 1.755857134553458
Validation loss: 2.475472139029019

Epoch: 124| Step: 0
Training loss: 2.5650898941081883
Validation loss: 2.476745626776932

Epoch: 5| Step: 1
Training loss: 2.310201585239028
Validation loss: 2.4804941211113793

Epoch: 5| Step: 2
Training loss: 2.1446027344287537
Validation loss: 2.490213858326686

Epoch: 5| Step: 3
Training loss: 2.5435846523373185
Validation loss: 2.491995163275024

Epoch: 5| Step: 4
Training loss: 2.6729981142173704
Validation loss: 2.5111473465821033

Epoch: 5| Step: 5
Training loss: 2.7482854526745077
Validation loss: 2.511583236590378

Epoch: 5| Step: 6
Training loss: 2.7162239353633963
Validation loss: 2.5171056145692896

Epoch: 5| Step: 7
Training loss: 2.642266119041655
Validation loss: 2.4919024137990764

Epoch: 5| Step: 8
Training loss: 2.287907123294514
Validation loss: 2.4917276370733914

Epoch: 5| Step: 9
Training loss: 3.025526956105594
Validation loss: 2.483686575685341

Epoch: 5| Step: 10
Training loss: 2.966937987818345
Validation loss: 2.4876549739581595

Epoch: 5| Step: 11
Training loss: 2.466188574759277
Validation loss: 2.4824047353149306

Epoch: 125| Step: 0
Training loss: 2.5976063200088655
Validation loss: 2.4820287129508567

Epoch: 5| Step: 1
Training loss: 2.846957734598519
Validation loss: 2.482245078045186

Epoch: 5| Step: 2
Training loss: 2.8991810135478615
Validation loss: 2.4842119183386324

Epoch: 5| Step: 3
Training loss: 2.8625553975304414
Validation loss: 2.4715675821934155

Epoch: 5| Step: 4
Training loss: 2.759965353485693
Validation loss: 2.481894512370391

Epoch: 5| Step: 5
Training loss: 2.1755910136201893
Validation loss: 2.4703487498941143

Epoch: 5| Step: 6
Training loss: 2.7216000581226645
Validation loss: 2.469355947436615

Epoch: 5| Step: 7
Training loss: 2.3111768494124925
Validation loss: 2.469298088622257

Epoch: 5| Step: 8
Training loss: 1.975715425540456
Validation loss: 2.468773918196949

Epoch: 5| Step: 9
Training loss: 2.25316259852769
Validation loss: 2.4608959537611987

Epoch: 5| Step: 10
Training loss: 2.5665028639539043
Validation loss: 2.4642074416546147

Epoch: 5| Step: 11
Training loss: 3.1717763800863135
Validation loss: 2.4585636315260126

Epoch: 126| Step: 0
Training loss: 2.187448337489828
Validation loss: 2.4576264526221645

Epoch: 5| Step: 1
Training loss: 3.0233009169528993
Validation loss: 2.4658830087075034

Epoch: 5| Step: 2
Training loss: 2.339474656889245
Validation loss: 2.472208959892742

Epoch: 5| Step: 3
Training loss: 2.2089954918970376
Validation loss: 2.4751818044273763

Epoch: 5| Step: 4
Training loss: 2.932406615764714
Validation loss: 2.467499846999844

Epoch: 5| Step: 5
Training loss: 2.4312383970165166
Validation loss: 2.4710590935398575

Epoch: 5| Step: 6
Training loss: 2.276034625033056
Validation loss: 2.4599688041332053

Epoch: 5| Step: 7
Training loss: 2.925038955298655
Validation loss: 2.467248657286754

Epoch: 5| Step: 8
Training loss: 2.8124560882531218
Validation loss: 2.4600390212006134

Epoch: 5| Step: 9
Training loss: 2.069982768738019
Validation loss: 2.46905569203947

Epoch: 5| Step: 10
Training loss: 2.5879858841547945
Validation loss: 2.463829375894279

Epoch: 5| Step: 11
Training loss: 2.1798673165916167
Validation loss: 2.453515153380852

Epoch: 127| Step: 0
Training loss: 2.468665785499942
Validation loss: 2.4536502786001058

Epoch: 5| Step: 1
Training loss: 2.4580476778091707
Validation loss: 2.458467966100009

Epoch: 5| Step: 2
Training loss: 2.773516844568151
Validation loss: 2.4538457949467314

Epoch: 5| Step: 3
Training loss: 3.132926349045716
Validation loss: 2.456272810748357

Epoch: 5| Step: 4
Training loss: 2.6599420357605457
Validation loss: 2.453336001998117

Epoch: 5| Step: 5
Training loss: 2.322818109676711
Validation loss: 2.456907551233693

Epoch: 5| Step: 6
Training loss: 2.647328732636001
Validation loss: 2.4585661003401342

Epoch: 5| Step: 7
Training loss: 2.4637632586210976
Validation loss: 2.4627776839769737

Epoch: 5| Step: 8
Training loss: 2.643572364412639
Validation loss: 2.460192653418572

Epoch: 5| Step: 9
Training loss: 2.104714904470808
Validation loss: 2.4612937104305894

Epoch: 5| Step: 10
Training loss: 2.0938889186948306
Validation loss: 2.4564029919359442

Epoch: 5| Step: 11
Training loss: 2.3144790841107494
Validation loss: 2.462833005557357

Epoch: 128| Step: 0
Training loss: 2.169748816125808
Validation loss: 2.4565886041664555

Epoch: 5| Step: 1
Training loss: 2.3102206776226346
Validation loss: 2.4515702490018545

Epoch: 5| Step: 2
Training loss: 2.308615437517103
Validation loss: 2.4624097749475786

Epoch: 5| Step: 3
Training loss: 2.6237502074729977
Validation loss: 2.4815530764837153

Epoch: 5| Step: 4
Training loss: 2.7993698500943363
Validation loss: 2.465827874401823

Epoch: 5| Step: 5
Training loss: 2.9384681242608983
Validation loss: 2.479858675847252

Epoch: 5| Step: 6
Training loss: 2.413557079948126
Validation loss: 2.4684501900487374

Epoch: 5| Step: 7
Training loss: 2.4050260130809793
Validation loss: 2.4643654074511616

Epoch: 5| Step: 8
Training loss: 2.7498411652904924
Validation loss: 2.4472190282170323

Epoch: 5| Step: 9
Training loss: 2.9747493157434484
Validation loss: 2.453866506386465

Epoch: 5| Step: 10
Training loss: 2.383188486921833
Validation loss: 2.4607127293870437

Epoch: 5| Step: 11
Training loss: 2.7734711121147644
Validation loss: 2.464318781272828

Epoch: 129| Step: 0
Training loss: 2.1060024851814236
Validation loss: 2.470249107169963

Epoch: 5| Step: 1
Training loss: 2.5831326027275345
Validation loss: 2.4679967640567546

Epoch: 5| Step: 2
Training loss: 2.8057432604989803
Validation loss: 2.475838472124962

Epoch: 5| Step: 3
Training loss: 2.867122140082264
Validation loss: 2.4802318710199316

Epoch: 5| Step: 4
Training loss: 2.319398539451198
Validation loss: 2.4750194074370753

Epoch: 5| Step: 5
Training loss: 2.7114745385917596
Validation loss: 2.4757945076324757

Epoch: 5| Step: 6
Training loss: 2.462318735180193
Validation loss: 2.47533175174852

Epoch: 5| Step: 7
Training loss: 2.545682944076877
Validation loss: 2.4742805726181345

Epoch: 5| Step: 8
Training loss: 2.6084847216400915
Validation loss: 2.473848020536932

Epoch: 5| Step: 9
Training loss: 2.41136339521174
Validation loss: 2.465177691730842

Epoch: 5| Step: 10
Training loss: 2.7444599006601984
Validation loss: 2.4690061186875205

Epoch: 5| Step: 11
Training loss: 2.398160141803475
Validation loss: 2.4608754589192574

Epoch: 130| Step: 0
Training loss: 2.054833468629823
Validation loss: 2.460784649012653

Epoch: 5| Step: 1
Training loss: 2.619192875306093
Validation loss: 2.4607397333031926

Epoch: 5| Step: 2
Training loss: 2.4621895647675753
Validation loss: 2.456727575671105

Epoch: 5| Step: 3
Training loss: 2.5779028594520383
Validation loss: 2.4519985390659316

Epoch: 5| Step: 4
Training loss: 2.426798092912686
Validation loss: 2.460120801285647

Epoch: 5| Step: 5
Training loss: 2.8280510655639257
Validation loss: 2.453012832377361

Epoch: 5| Step: 6
Training loss: 2.504829605459516
Validation loss: 2.455123463942649

Epoch: 5| Step: 7
Training loss: 2.118454219911384
Validation loss: 2.46053628628036

Epoch: 5| Step: 8
Training loss: 3.1741267888321714
Validation loss: 2.455071246193915

Epoch: 5| Step: 9
Training loss: 2.677486038100369
Validation loss: 2.4597185254465597

Epoch: 5| Step: 10
Training loss: 2.34209566858809
Validation loss: 2.453270051397444

Epoch: 5| Step: 11
Training loss: 1.9020797264284974
Validation loss: 2.4491234679196467

Epoch: 131| Step: 0
Training loss: 2.779751320289088
Validation loss: 2.4529640849602647

Epoch: 5| Step: 1
Training loss: 2.95185655267053
Validation loss: 2.453554237376492

Epoch: 5| Step: 2
Training loss: 2.6538523006261707
Validation loss: 2.454023362743113

Epoch: 5| Step: 3
Training loss: 2.419837439097704
Validation loss: 2.4609762804316166

Epoch: 5| Step: 4
Training loss: 1.9259080206753287
Validation loss: 2.456030657570433

Epoch: 5| Step: 5
Training loss: 3.0827320302987546
Validation loss: 2.455343239732493

Epoch: 5| Step: 6
Training loss: 1.8033192389472734
Validation loss: 2.4574531674053923

Epoch: 5| Step: 7
Training loss: 2.162687780793803
Validation loss: 2.4474601576955095

Epoch: 5| Step: 8
Training loss: 2.6896854652103683
Validation loss: 2.4559818446617507

Epoch: 5| Step: 9
Training loss: 2.3542169436147
Validation loss: 2.463765331112872

Epoch: 5| Step: 10
Training loss: 2.7452622263022723
Validation loss: 2.4665031308550516

Epoch: 5| Step: 11
Training loss: 2.0154223672900424
Validation loss: 2.4705176463511833

Epoch: 132| Step: 0
Training loss: 2.298514682658983
Validation loss: 2.47701581382221

Epoch: 5| Step: 1
Training loss: 2.3653811370652265
Validation loss: 2.4757495432306316

Epoch: 5| Step: 2
Training loss: 2.093446624184755
Validation loss: 2.4771169208359503

Epoch: 5| Step: 3
Training loss: 2.547910047030226
Validation loss: 2.4759890436933825

Epoch: 5| Step: 4
Training loss: 3.0514315450591596
Validation loss: 2.477420782509406

Epoch: 5| Step: 5
Training loss: 2.5209156579901095
Validation loss: 2.473428325431229

Epoch: 5| Step: 6
Training loss: 2.0042511343917693
Validation loss: 2.466376858087923

Epoch: 5| Step: 7
Training loss: 2.857658496375867
Validation loss: 2.4710033611273206

Epoch: 5| Step: 8
Training loss: 3.044739899449188
Validation loss: 2.4726467122340448

Epoch: 5| Step: 9
Training loss: 2.467135807049053
Validation loss: 2.46455374541678

Epoch: 5| Step: 10
Training loss: 2.72769998252587
Validation loss: 2.468147977025065

Epoch: 5| Step: 11
Training loss: 1.147577132165461
Validation loss: 2.454301090531324

Epoch: 133| Step: 0
Training loss: 2.8010879412529284
Validation loss: 2.4611634852075213

Epoch: 5| Step: 1
Training loss: 2.2129386768696166
Validation loss: 2.4638263962602487

Epoch: 5| Step: 2
Training loss: 2.474946466502749
Validation loss: 2.456039960550353

Epoch: 5| Step: 3
Training loss: 2.4353357145519015
Validation loss: 2.4539955926885098

Epoch: 5| Step: 4
Training loss: 2.5180695781351217
Validation loss: 2.4643545093437513

Epoch: 5| Step: 5
Training loss: 2.835048904131701
Validation loss: 2.4556807671338836

Epoch: 5| Step: 6
Training loss: 3.1327974314339135
Validation loss: 2.4802608653185803

Epoch: 5| Step: 7
Training loss: 2.1723508244999956
Validation loss: 2.4669532595426626

Epoch: 5| Step: 8
Training loss: 1.9675775700247071
Validation loss: 2.4718729659762175

Epoch: 5| Step: 9
Training loss: 2.512148712044775
Validation loss: 2.4571378044954897

Epoch: 5| Step: 10
Training loss: 2.750355264083954
Validation loss: 2.445899490737597

Epoch: 5| Step: 11
Training loss: 2.2593541145678446
Validation loss: 2.4508147041519006

Epoch: 134| Step: 0
Training loss: 2.679071294373424
Validation loss: 2.4602166305825466

Epoch: 5| Step: 1
Training loss: 2.2017066664699505
Validation loss: 2.4667923082323004

Epoch: 5| Step: 2
Training loss: 2.593704590916986
Validation loss: 2.4648617614313713

Epoch: 5| Step: 3
Training loss: 1.9166254439965762
Validation loss: 2.4709288100750797

Epoch: 5| Step: 4
Training loss: 3.0666946955795904
Validation loss: 2.46891141516696

Epoch: 5| Step: 5
Training loss: 2.1250670927339224
Validation loss: 2.4684812785824293

Epoch: 5| Step: 6
Training loss: 2.2320722972071754
Validation loss: 2.4700673729090363

Epoch: 5| Step: 7
Training loss: 2.9451756938863403
Validation loss: 2.473781416071709

Epoch: 5| Step: 8
Training loss: 2.432834071931646
Validation loss: 2.469363249088856

Epoch: 5| Step: 9
Training loss: 2.5777094650625765
Validation loss: 2.4695633019379963

Epoch: 5| Step: 10
Training loss: 2.8197999948228993
Validation loss: 2.471522030423694

Epoch: 5| Step: 11
Training loss: 2.907692335988139
Validation loss: 2.463361505019116

Epoch: 135| Step: 0
Training loss: 2.988463791401785
Validation loss: 2.460661512369953

Epoch: 5| Step: 1
Training loss: 2.802952306694846
Validation loss: 2.4561330794153426

Epoch: 5| Step: 2
Training loss: 2.686075409638048
Validation loss: 2.4588228798359655

Epoch: 5| Step: 3
Training loss: 2.6900896414813618
Validation loss: 2.459409592646537

Epoch: 5| Step: 4
Training loss: 2.2994238795326303
Validation loss: 2.457512675447007

Epoch: 5| Step: 5
Training loss: 1.9773144403679568
Validation loss: 2.4488575540756865

Epoch: 5| Step: 6
Training loss: 2.988633557663466
Validation loss: 2.4556898084920284

Epoch: 5| Step: 7
Training loss: 2.3834668871222138
Validation loss: 2.4496301835752385

Epoch: 5| Step: 8
Training loss: 2.580474522740383
Validation loss: 2.4512458942073536

Epoch: 5| Step: 9
Training loss: 2.389574006873646
Validation loss: 2.455905872889963

Epoch: 5| Step: 10
Training loss: 1.917032545522864
Validation loss: 2.4562270400419677

Epoch: 5| Step: 11
Training loss: 1.8721976000634846
Validation loss: 2.456831749608414

Epoch: 136| Step: 0
Training loss: 2.6700367097081203
Validation loss: 2.4592539200470545

Epoch: 5| Step: 1
Training loss: 2.67873612578729
Validation loss: 2.463227416649141

Epoch: 5| Step: 2
Training loss: 2.455821019730658
Validation loss: 2.4556835705652715

Epoch: 5| Step: 3
Training loss: 2.6761524249288637
Validation loss: 2.462768934865148

Epoch: 5| Step: 4
Training loss: 2.264689074600365
Validation loss: 2.4631042750816885

Epoch: 5| Step: 5
Training loss: 2.3549372894295924
Validation loss: 2.4600448038833447

Epoch: 5| Step: 6
Training loss: 2.3849248809500603
Validation loss: 2.4598909564713316

Epoch: 5| Step: 7
Training loss: 2.392494318525849
Validation loss: 2.4589923013160946

Epoch: 5| Step: 8
Training loss: 2.579494228320348
Validation loss: 2.4626803528954095

Epoch: 5| Step: 9
Training loss: 2.816018849049687
Validation loss: 2.464911615613263

Epoch: 5| Step: 10
Training loss: 2.689779601161762
Validation loss: 2.4659405427835894

Epoch: 5| Step: 11
Training loss: 1.6175127669807818
Validation loss: 2.451896315232495

Epoch: 137| Step: 0
Training loss: 3.0255256952680223
Validation loss: 2.4535268345118064

Epoch: 5| Step: 1
Training loss: 2.4558361646387583
Validation loss: 2.4549362679798605

Epoch: 5| Step: 2
Training loss: 2.414058055688415
Validation loss: 2.454678721728899

Epoch: 5| Step: 3
Training loss: 2.3243871804123626
Validation loss: 2.455954335401705

Epoch: 5| Step: 4
Training loss: 2.6587704537290464
Validation loss: 2.449154404277115

Epoch: 5| Step: 5
Training loss: 2.675431945722299
Validation loss: 2.4557568065480764

Epoch: 5| Step: 6
Training loss: 2.0824244487562646
Validation loss: 2.452534152916618

Epoch: 5| Step: 7
Training loss: 2.728939037197019
Validation loss: 2.4493568296310917

Epoch: 5| Step: 8
Training loss: 2.50912840357196
Validation loss: 2.4541637100586193

Epoch: 5| Step: 9
Training loss: 2.5671419108284668
Validation loss: 2.454369235516475

Epoch: 5| Step: 10
Training loss: 2.351898473520601
Validation loss: 2.452578283240729

Epoch: 5| Step: 11
Training loss: 2.2763751474549805
Validation loss: 2.4416676780603113

Epoch: 138| Step: 0
Training loss: 2.296640604735921
Validation loss: 2.45015393685043

Epoch: 5| Step: 1
Training loss: 2.7178180017905786
Validation loss: 2.4555099297749106

Epoch: 5| Step: 2
Training loss: 2.310110146014417
Validation loss: 2.4555781099874054

Epoch: 5| Step: 3
Training loss: 2.7623957515050863
Validation loss: 2.46383396428172

Epoch: 5| Step: 4
Training loss: 2.293043820973508
Validation loss: 2.4675351546004665

Epoch: 5| Step: 5
Training loss: 2.6666524608551567
Validation loss: 2.4652417804137534

Epoch: 5| Step: 6
Training loss: 2.502177434629222
Validation loss: 2.4664141674476543

Epoch: 5| Step: 7
Training loss: 2.660600935952828
Validation loss: 2.4627525337262224

Epoch: 5| Step: 8
Training loss: 2.3432304315054155
Validation loss: 2.465833143949958

Epoch: 5| Step: 9
Training loss: 2.3816517488489812
Validation loss: 2.455504091910467

Epoch: 5| Step: 10
Training loss: 2.80371731525202
Validation loss: 2.4561772360794203

Epoch: 5| Step: 11
Training loss: 2.664751606972322
Validation loss: 2.4475692881309037

Epoch: 139| Step: 0
Training loss: 2.7475056172835015
Validation loss: 2.453222693990513

Epoch: 5| Step: 1
Training loss: 2.2434306137886852
Validation loss: 2.4433820923161576

Epoch: 5| Step: 2
Training loss: 2.0282843434611646
Validation loss: 2.4460674944426675

Epoch: 5| Step: 3
Training loss: 2.020005898986361
Validation loss: 2.451552334422586

Epoch: 5| Step: 4
Training loss: 2.166003957292177
Validation loss: 2.448249645447502

Epoch: 5| Step: 5
Training loss: 1.9437836303745426
Validation loss: 2.458009283615761

Epoch: 5| Step: 6
Training loss: 2.87247206632015
Validation loss: 2.4469707541732335

Epoch: 5| Step: 7
Training loss: 2.7799557878560015
Validation loss: 2.4501675233852636

Epoch: 5| Step: 8
Training loss: 2.9992775047054137
Validation loss: 2.4530292500398994

Epoch: 5| Step: 9
Training loss: 2.809445226892697
Validation loss: 2.4589678274198317

Epoch: 5| Step: 10
Training loss: 2.7093399133577316
Validation loss: 2.4561102252239944

Epoch: 5| Step: 11
Training loss: 2.6833919799113923
Validation loss: 2.4559736093090554

Epoch: 140| Step: 0
Training loss: 3.0734890334711116
Validation loss: 2.4527887409517692

Epoch: 5| Step: 1
Training loss: 2.81651782107154
Validation loss: 2.4508540136114823

Epoch: 5| Step: 2
Training loss: 2.2727182830285964
Validation loss: 2.4464122992442783

Epoch: 5| Step: 3
Training loss: 2.6089417788938793
Validation loss: 2.4442715810995033

Epoch: 5| Step: 4
Training loss: 1.7731256420845642
Validation loss: 2.4516724215992545

Epoch: 5| Step: 5
Training loss: 2.3821968659115864
Validation loss: 2.459124855268627

Epoch: 5| Step: 6
Training loss: 2.5555987584797544
Validation loss: 2.4575924416932566

Epoch: 5| Step: 7
Training loss: 2.678413377367701
Validation loss: 2.45151692427703

Epoch: 5| Step: 8
Training loss: 1.986888225135815
Validation loss: 2.4549458259942396

Epoch: 5| Step: 9
Training loss: 2.525693847201208
Validation loss: 2.4573206971121846

Epoch: 5| Step: 10
Training loss: 2.8451521215011373
Validation loss: 2.467087580288431

Epoch: 5| Step: 11
Training loss: 2.4264445849162586
Validation loss: 2.4599443742780776

Epoch: 141| Step: 0
Training loss: 2.4127932671693313
Validation loss: 2.4591105345181288

Epoch: 5| Step: 1
Training loss: 2.9421437669969004
Validation loss: 2.4580446022543

Epoch: 5| Step: 2
Training loss: 2.242923416428954
Validation loss: 2.4611216844530603

Epoch: 5| Step: 3
Training loss: 2.6730551985304887
Validation loss: 2.462556014093572

Epoch: 5| Step: 4
Training loss: 2.2007152825091185
Validation loss: 2.4596765708947093

Epoch: 5| Step: 5
Training loss: 2.7043685257800907
Validation loss: 2.455676679296376

Epoch: 5| Step: 6
Training loss: 2.73915040592682
Validation loss: 2.45800110759586

Epoch: 5| Step: 7
Training loss: 2.23229948093781
Validation loss: 2.4521458649960475

Epoch: 5| Step: 8
Training loss: 2.6664800777324116
Validation loss: 2.4517159393899304

Epoch: 5| Step: 9
Training loss: 2.6604761947946174
Validation loss: 2.451893787031684

Epoch: 5| Step: 10
Training loss: 2.1278404997945493
Validation loss: 2.447860320775204

Epoch: 5| Step: 11
Training loss: 2.0977549041337937
Validation loss: 2.44444974684381

Epoch: 142| Step: 0
Training loss: 2.57317871942009
Validation loss: 2.453612752885904

Epoch: 5| Step: 1
Training loss: 2.579004033825468
Validation loss: 2.447786605031122

Epoch: 5| Step: 2
Training loss: 2.4317332790141464
Validation loss: 2.440881700240575

Epoch: 5| Step: 3
Training loss: 2.4720382038546935
Validation loss: 2.4448380665113616

Epoch: 5| Step: 4
Training loss: 2.0432384305678006
Validation loss: 2.4539879902776627

Epoch: 5| Step: 5
Training loss: 2.872851522664695
Validation loss: 2.4503391386339652

Epoch: 5| Step: 6
Training loss: 2.9443963444777648
Validation loss: 2.458658561906865

Epoch: 5| Step: 7
Training loss: 2.7728860656135037
Validation loss: 2.4479507741513413

Epoch: 5| Step: 8
Training loss: 2.3906455444251016
Validation loss: 2.455739941915508

Epoch: 5| Step: 9
Training loss: 2.2566153444495534
Validation loss: 2.4552764165084042

Epoch: 5| Step: 10
Training loss: 2.3655390775488976
Validation loss: 2.4489966194949346

Epoch: 5| Step: 11
Training loss: 2.485990850522305
Validation loss: 2.4520645076598386

Epoch: 143| Step: 0
Training loss: 2.460840229352449
Validation loss: 2.4462244163595734

Epoch: 5| Step: 1
Training loss: 3.104019537048205
Validation loss: 2.456461922851594

Epoch: 5| Step: 2
Training loss: 2.977522087111335
Validation loss: 2.4493905128389444

Epoch: 5| Step: 3
Training loss: 2.5108390442227977
Validation loss: 2.4547696401546784

Epoch: 5| Step: 4
Training loss: 2.9399147659714147
Validation loss: 2.460735203738899

Epoch: 5| Step: 5
Training loss: 2.3467587296171244
Validation loss: 2.4505471536572125

Epoch: 5| Step: 6
Training loss: 2.097725240187354
Validation loss: 2.4515678460812715

Epoch: 5| Step: 7
Training loss: 2.0423272549851257
Validation loss: 2.4563795396911914

Epoch: 5| Step: 8
Training loss: 2.3619343600216807
Validation loss: 2.451299324116494

Epoch: 5| Step: 9
Training loss: 1.8864908969314753
Validation loss: 2.453412557460646

Epoch: 5| Step: 10
Training loss: 2.7440347128415743
Validation loss: 2.450200133194127

Epoch: 5| Step: 11
Training loss: 1.2638292170447545
Validation loss: 2.4490640156371164

Epoch: 144| Step: 0
Training loss: 2.583084237999804
Validation loss: 2.460103922150434

Epoch: 5| Step: 1
Training loss: 2.312364522728655
Validation loss: 2.457893640723214

Epoch: 5| Step: 2
Training loss: 2.645868636569091
Validation loss: 2.4632029727331317

Epoch: 5| Step: 3
Training loss: 2.618236912101933
Validation loss: 2.4606088166515776

Epoch: 5| Step: 4
Training loss: 2.8670110414343615
Validation loss: 2.4571307293070093

Epoch: 5| Step: 5
Training loss: 2.5224533760291235
Validation loss: 2.460373776573716

Epoch: 5| Step: 6
Training loss: 2.444087004778322
Validation loss: 2.456900422818603

Epoch: 5| Step: 7
Training loss: 2.3129254542574227
Validation loss: 2.4600194702211424

Epoch: 5| Step: 8
Training loss: 2.9037116666210325
Validation loss: 2.462071039626741

Epoch: 5| Step: 9
Training loss: 1.6072383882498011
Validation loss: 2.45989259607502

Epoch: 5| Step: 10
Training loss: 2.6172519049619773
Validation loss: 2.4536853522854707

Epoch: 5| Step: 11
Training loss: 2.115290168174322
Validation loss: 2.4634680155631505

Epoch: 145| Step: 0
Training loss: 1.7842725574061125
Validation loss: 2.456419995583311

Epoch: 5| Step: 1
Training loss: 3.1410132021136192
Validation loss: 2.45187796140776

Epoch: 5| Step: 2
Training loss: 2.0945375014841083
Validation loss: 2.4632360229792534

Epoch: 5| Step: 3
Training loss: 2.3675307223331665
Validation loss: 2.462293382551477

Epoch: 5| Step: 4
Training loss: 2.3350022909410497
Validation loss: 2.451532665109234

Epoch: 5| Step: 5
Training loss: 2.5858794145312696
Validation loss: 2.4587884410296077

Epoch: 5| Step: 6
Training loss: 2.360896245367505
Validation loss: 2.448635334006393

Epoch: 5| Step: 7
Training loss: 2.9930703875023483
Validation loss: 2.46141053956639

Epoch: 5| Step: 8
Training loss: 2.563121720232629
Validation loss: 2.4604883944731184

Epoch: 5| Step: 9
Training loss: 2.7981052356914065
Validation loss: 2.449562608348184

Epoch: 5| Step: 10
Training loss: 2.1220939062626494
Validation loss: 2.4605071322234306

Epoch: 5| Step: 11
Training loss: 3.190952899160858
Validation loss: 2.458986637355823

Epoch: 146| Step: 0
Training loss: 2.6574560681046546
Validation loss: 2.452662292707621

Epoch: 5| Step: 1
Training loss: 2.7019582005224665
Validation loss: 2.45195492500327

Epoch: 5| Step: 2
Training loss: 2.3096508310488724
Validation loss: 2.460740867711483

Epoch: 5| Step: 3
Training loss: 1.9078360666230063
Validation loss: 2.4491698175517977

Epoch: 5| Step: 4
Training loss: 2.354317505541815
Validation loss: 2.458832471224313

Epoch: 5| Step: 5
Training loss: 1.9725652390438029
Validation loss: 2.4844567867228196

Epoch: 5| Step: 6
Training loss: 2.9352483032835077
Validation loss: 2.494467866918478

Epoch: 5| Step: 7
Training loss: 3.1565102951435415
Validation loss: 2.4741011340772436

Epoch: 5| Step: 8
Training loss: 2.6284075826616506
Validation loss: 2.453948945576099

Epoch: 5| Step: 9
Training loss: 2.2140584424721332
Validation loss: 2.454221233501778

Epoch: 5| Step: 10
Training loss: 2.737110273933508
Validation loss: 2.459552407399592

Epoch: 5| Step: 11
Training loss: 3.1989734075835576
Validation loss: 2.4679743840079924

Epoch: 147| Step: 0
Training loss: 2.561940387934615
Validation loss: 2.474797318729853

Epoch: 5| Step: 1
Training loss: 2.505676881324876
Validation loss: 2.470437527379374

Epoch: 5| Step: 2
Training loss: 2.3086484847704534
Validation loss: 2.469889912822554

Epoch: 5| Step: 3
Training loss: 2.686464154425876
Validation loss: 2.477507269643671

Epoch: 5| Step: 4
Training loss: 2.1800886762589586
Validation loss: 2.487157744675289

Epoch: 5| Step: 5
Training loss: 2.6973806214454537
Validation loss: 2.491987314026884

Epoch: 5| Step: 6
Training loss: 2.3786630991679467
Validation loss: 2.49021019020055

Epoch: 5| Step: 7
Training loss: 3.2487539690440386
Validation loss: 2.4923936047128668

Epoch: 5| Step: 8
Training loss: 2.692049655496022
Validation loss: 2.48997749948485

Epoch: 5| Step: 9
Training loss: 1.920221066761899
Validation loss: 2.4905077217024214

Epoch: 5| Step: 10
Training loss: 2.6856877626396436
Validation loss: 2.491236625886866

Epoch: 5| Step: 11
Training loss: 3.4357678297338405
Validation loss: 2.480790505354034

Epoch: 148| Step: 0
Training loss: 2.720933760639012
Validation loss: 2.48237597815511

Epoch: 5| Step: 1
Training loss: 2.4922904349089166
Validation loss: 2.479393084522372

Epoch: 5| Step: 2
Training loss: 2.543466733114655
Validation loss: 2.4819948382854196

Epoch: 5| Step: 3
Training loss: 2.6878386882902814
Validation loss: 2.4749012137455866

Epoch: 5| Step: 4
Training loss: 2.204222061047657
Validation loss: 2.4721349454921038

Epoch: 5| Step: 5
Training loss: 2.580829842942509
Validation loss: 2.4675180685224993

Epoch: 5| Step: 6
Training loss: 2.2693554913589207
Validation loss: 2.4612860054416443

Epoch: 5| Step: 7
Training loss: 2.366125088723299
Validation loss: 2.4593697191187363

Epoch: 5| Step: 8
Training loss: 2.5865287911482766
Validation loss: 2.456349353544261

Epoch: 5| Step: 9
Training loss: 2.584391797528921
Validation loss: 2.4608428210256332

Epoch: 5| Step: 10
Training loss: 2.8864567577110423
Validation loss: 2.469484944137213

Epoch: 5| Step: 11
Training loss: 2.9471005949074605
Validation loss: 2.4633560083822843

Epoch: 149| Step: 0
Training loss: 2.5328143901688467
Validation loss: 2.4628406895721286

Epoch: 5| Step: 1
Training loss: 2.3385083395120243
Validation loss: 2.4634525465756782

Epoch: 5| Step: 2
Training loss: 3.1152787638882975
Validation loss: 2.4586758024821043

Epoch: 5| Step: 3
Training loss: 2.272959550778746
Validation loss: 2.463295086838833

Epoch: 5| Step: 4
Training loss: 2.669182385475671
Validation loss: 2.453932056374296

Epoch: 5| Step: 5
Training loss: 2.532587616318984
Validation loss: 2.4629109659345705

Epoch: 5| Step: 6
Training loss: 1.9010301633164555
Validation loss: 2.45902652315514

Epoch: 5| Step: 7
Training loss: 2.5624029559856494
Validation loss: 2.4552282440733393

Epoch: 5| Step: 8
Training loss: 2.4773663200088927
Validation loss: 2.459872096911899

Epoch: 5| Step: 9
Training loss: 1.8695108172666655
Validation loss: 2.458490212467234

Epoch: 5| Step: 10
Training loss: 3.048933536874865
Validation loss: 2.457154819229636

Epoch: 5| Step: 11
Training loss: 2.557161772019437
Validation loss: 2.4525155588052483

Epoch: 150| Step: 0
Training loss: 2.4363516034096993
Validation loss: 2.46705796809902

Epoch: 5| Step: 1
Training loss: 2.464051423081178
Validation loss: 2.4603759205604607

Epoch: 5| Step: 2
Training loss: 3.0953468286188737
Validation loss: 2.4561206199537957

Epoch: 5| Step: 3
Training loss: 2.247700151605436
Validation loss: 2.454997271509072

Epoch: 5| Step: 4
Training loss: 2.2460935377037945
Validation loss: 2.4505183185776644

Epoch: 5| Step: 5
Training loss: 2.404055697184659
Validation loss: 2.457704270961407

Epoch: 5| Step: 6
Training loss: 2.2272455606590813
Validation loss: 2.448279947690394

Epoch: 5| Step: 7
Training loss: 2.5295461877910457
Validation loss: 2.4550256594542543

Epoch: 5| Step: 8
Training loss: 2.6981665496772735
Validation loss: 2.4519594829420783

Epoch: 5| Step: 9
Training loss: 2.7964680125171713
Validation loss: 2.4460478703726367

Epoch: 5| Step: 10
Training loss: 2.408078279376165
Validation loss: 2.454966940900023

Epoch: 5| Step: 11
Training loss: 2.082130758974335
Validation loss: 2.4487803328898954

Epoch: 151| Step: 0
Training loss: 2.248201074954216
Validation loss: 2.454672833323395

Epoch: 5| Step: 1
Training loss: 2.5547481611323417
Validation loss: 2.458921955377893

Epoch: 5| Step: 2
Training loss: 2.5726041925829857
Validation loss: 2.4588802963018024

Epoch: 5| Step: 3
Training loss: 2.341312310590767
Validation loss: 2.464034429764014

Epoch: 5| Step: 4
Training loss: 2.8973085410293096
Validation loss: 2.459848968513893

Epoch: 5| Step: 5
Training loss: 2.293412069846834
Validation loss: 2.461867153836037

Epoch: 5| Step: 6
Training loss: 2.647175716175326
Validation loss: 2.457561975359306

Epoch: 5| Step: 7
Training loss: 2.5834636552814905
Validation loss: 2.4567552380164224

Epoch: 5| Step: 8
Training loss: 2.7654718906240996
Validation loss: 2.44900016072871

Epoch: 5| Step: 9
Training loss: 2.473510883758455
Validation loss: 2.457309713185532

Epoch: 5| Step: 10
Training loss: 2.3703262117475847
Validation loss: 2.455509731538577

Epoch: 5| Step: 11
Training loss: 0.8460543213847954
Validation loss: 2.4565912710925755

Epoch: 152| Step: 0
Training loss: 2.395419828694061
Validation loss: 2.4477126881992

Epoch: 5| Step: 1
Training loss: 2.7229812844004844
Validation loss: 2.452120054732216

Epoch: 5| Step: 2
Training loss: 2.1441004030218656
Validation loss: 2.452611156676976

Epoch: 5| Step: 3
Training loss: 2.355891706914962
Validation loss: 2.4537481298130217

Epoch: 5| Step: 4
Training loss: 2.6327854234338863
Validation loss: 2.4629068558128115

Epoch: 5| Step: 5
Training loss: 2.6687933567643674
Validation loss: 2.4561719741270402

Epoch: 5| Step: 6
Training loss: 2.227393493846164
Validation loss: 2.4523294154948605

Epoch: 5| Step: 7
Training loss: 2.7481836042300003
Validation loss: 2.4600131847188504

Epoch: 5| Step: 8
Training loss: 2.69382486615507
Validation loss: 2.457112248818595

Epoch: 5| Step: 9
Training loss: 2.585283179350309
Validation loss: 2.464459421026145

Epoch: 5| Step: 10
Training loss: 2.169081833487665
Validation loss: 2.4573606059090416

Epoch: 5| Step: 11
Training loss: 2.787803814672105
Validation loss: 2.450159280653185

Epoch: 153| Step: 0
Training loss: 2.114290627451752
Validation loss: 2.4417216104915562

Epoch: 5| Step: 1
Training loss: 2.511554338879323
Validation loss: 2.4535685622155627

Epoch: 5| Step: 2
Training loss: 2.565200499195768
Validation loss: 2.4513528095960733

Epoch: 5| Step: 3
Training loss: 2.7878706920798995
Validation loss: 2.458109943832946

Epoch: 5| Step: 4
Training loss: 2.8431498135204873
Validation loss: 2.451097253724914

Epoch: 5| Step: 5
Training loss: 2.9785963103825615
Validation loss: 2.4578310438532918

Epoch: 5| Step: 6
Training loss: 2.553931541469341
Validation loss: 2.4611005335497493

Epoch: 5| Step: 7
Training loss: 2.1238321573353525
Validation loss: 2.4545417266433676

Epoch: 5| Step: 8
Training loss: 1.8972525960794147
Validation loss: 2.4552876280154856

Epoch: 5| Step: 9
Training loss: 2.5076914725408024
Validation loss: 2.4552230569632405

Epoch: 5| Step: 10
Training loss: 2.525253634414044
Validation loss: 2.457780563040292

Epoch: 5| Step: 11
Training loss: 1.5887341575522989
Validation loss: 2.4518836701507483

Epoch: 154| Step: 0
Training loss: 2.3239213512788965
Validation loss: 2.464234104925808

Epoch: 5| Step: 1
Training loss: 2.8207951605583825
Validation loss: 2.4575661995186477

Epoch: 5| Step: 2
Training loss: 2.343974903760018
Validation loss: 2.450232264155339

Epoch: 5| Step: 3
Training loss: 2.723219869281843
Validation loss: 2.4498442764163255

Epoch: 5| Step: 4
Training loss: 2.7849067051874288
Validation loss: 2.4538246096673357

Epoch: 5| Step: 5
Training loss: 1.9735781488865785
Validation loss: 2.4658575013632826

Epoch: 5| Step: 6
Training loss: 3.0476667891464144
Validation loss: 2.4489113565599117

Epoch: 5| Step: 7
Training loss: 2.335152586662556
Validation loss: 2.4550894992658963

Epoch: 5| Step: 8
Training loss: 2.629393760518109
Validation loss: 2.450875451552822

Epoch: 5| Step: 9
Training loss: 1.62477711836322
Validation loss: 2.4547080117835076

Epoch: 5| Step: 10
Training loss: 2.4578671633006617
Validation loss: 2.4579872814518824

Epoch: 5| Step: 11
Training loss: 2.3749722930898236
Validation loss: 2.4600711811436136

Epoch: 155| Step: 0
Training loss: 2.6134619771506076
Validation loss: 2.454522518390363

Epoch: 5| Step: 1
Training loss: 2.858977743600555
Validation loss: 2.46212708720371

Epoch: 5| Step: 2
Training loss: 2.508200261925782
Validation loss: 2.4592391516631804

Epoch: 5| Step: 3
Training loss: 2.8617292207676823
Validation loss: 2.45127861530176

Epoch: 5| Step: 4
Training loss: 1.8513741055345347
Validation loss: 2.457331458644528

Epoch: 5| Step: 5
Training loss: 2.231720100695677
Validation loss: 2.45648601326693

Epoch: 5| Step: 6
Training loss: 2.5523232068325967
Validation loss: 2.461959065731255

Epoch: 5| Step: 7
Training loss: 2.5474551423590452
Validation loss: 2.462711885376186

Epoch: 5| Step: 8
Training loss: 2.1692083440590264
Validation loss: 2.468678600147957

Epoch: 5| Step: 9
Training loss: 2.512079334628104
Validation loss: 2.4623298137612615

Epoch: 5| Step: 10
Training loss: 2.208435368130007
Validation loss: 2.4552200345157194

Epoch: 5| Step: 11
Training loss: 3.553687590092308
Validation loss: 2.461257320391229

Epoch: 156| Step: 0
Training loss: 2.681242555550153
Validation loss: 2.4491915460482176

Epoch: 5| Step: 1
Training loss: 2.8127197179801016
Validation loss: 2.457524697343629

Epoch: 5| Step: 2
Training loss: 2.2502872495607953
Validation loss: 2.4544564296812967

Epoch: 5| Step: 3
Training loss: 2.1366369590935657
Validation loss: 2.4561321734218926

Epoch: 5| Step: 4
Training loss: 2.257402323387169
Validation loss: 2.4529666809051274

Epoch: 5| Step: 5
Training loss: 3.2083498141042983
Validation loss: 2.4624822017265293

Epoch: 5| Step: 6
Training loss: 2.8194747060854466
Validation loss: 2.4653943308887136

Epoch: 5| Step: 7
Training loss: 2.1593936726139575
Validation loss: 2.471654414813011

Epoch: 5| Step: 8
Training loss: 2.3632736836462036
Validation loss: 2.469594969787738

Epoch: 5| Step: 9
Training loss: 2.5191246000263194
Validation loss: 2.467086231359055

Epoch: 5| Step: 10
Training loss: 2.471373504037777
Validation loss: 2.4658898875678443

Epoch: 5| Step: 11
Training loss: 2.341997941320021
Validation loss: 2.4643716375108884

Epoch: 157| Step: 0
Training loss: 2.612054785067747
Validation loss: 2.462606996061013

Epoch: 5| Step: 1
Training loss: 2.6205037073969115
Validation loss: 2.4592047469632377

Epoch: 5| Step: 2
Training loss: 2.750716549546628
Validation loss: 2.4644945605637134

Epoch: 5| Step: 3
Training loss: 2.772618476393246
Validation loss: 2.456301998885469

Epoch: 5| Step: 4
Training loss: 2.5192142257550434
Validation loss: 2.4650409013077215

Epoch: 5| Step: 5
Training loss: 2.460674937940304
Validation loss: 2.4611562439920913

Epoch: 5| Step: 6
Training loss: 2.1845788252097216
Validation loss: 2.470851301692924

Epoch: 5| Step: 7
Training loss: 2.398424974837511
Validation loss: 2.460045340961428

Epoch: 5| Step: 8
Training loss: 2.1400503236433086
Validation loss: 2.462045161927839

Epoch: 5| Step: 9
Training loss: 2.6903322067185913
Validation loss: 2.4609181680878733

Epoch: 5| Step: 10
Training loss: 2.3948092275946045
Validation loss: 2.465250319266788

Epoch: 5| Step: 11
Training loss: 2.002318707092592
Validation loss: 2.4620708318315363

Epoch: 158| Step: 0
Training loss: 2.789562533137004
Validation loss: 2.4611306210587265

Epoch: 5| Step: 1
Training loss: 2.6005532996449876
Validation loss: 2.46268543958746

Epoch: 5| Step: 2
Training loss: 2.5382508844054517
Validation loss: 2.465490938352695

Epoch: 5| Step: 3
Training loss: 2.0336317916870192
Validation loss: 2.4509296191136354

Epoch: 5| Step: 4
Training loss: 2.6077823004051073
Validation loss: 2.456745727477637

Epoch: 5| Step: 5
Training loss: 2.497247515833077
Validation loss: 2.460584754488762

Epoch: 5| Step: 6
Training loss: 2.5577142266635455
Validation loss: 2.4529948797360426

Epoch: 5| Step: 7
Training loss: 2.886103873008714
Validation loss: 2.461020380471298

Epoch: 5| Step: 8
Training loss: 2.3997554574994386
Validation loss: 2.4560716065973636

Epoch: 5| Step: 9
Training loss: 2.223934724371235
Validation loss: 2.467275070269466

Epoch: 5| Step: 10
Training loss: 2.085260974459553
Validation loss: 2.462004800486462

Epoch: 5| Step: 11
Training loss: 2.637609080007033
Validation loss: 2.460502218675043

Epoch: 159| Step: 0
Training loss: 2.0951487864827167
Validation loss: 2.4586120234673925

Epoch: 5| Step: 1
Training loss: 3.182285306750413
Validation loss: 2.452004012545621

Epoch: 5| Step: 2
Training loss: 2.316042377782586
Validation loss: 2.4622235081613746

Epoch: 5| Step: 3
Training loss: 2.542665900085099
Validation loss: 2.458281404679548

Epoch: 5| Step: 4
Training loss: 2.2801026895051724
Validation loss: 2.4479370779174

Epoch: 5| Step: 5
Training loss: 2.6954875502202444
Validation loss: 2.4563342845973746

Epoch: 5| Step: 6
Training loss: 3.0406176356258774
Validation loss: 2.4580173666536855

Epoch: 5| Step: 7
Training loss: 2.0612706074977116
Validation loss: 2.4595793069255825

Epoch: 5| Step: 8
Training loss: 2.3297779948352386
Validation loss: 2.4635231603420635

Epoch: 5| Step: 9
Training loss: 2.105579267159745
Validation loss: 2.4624061259176937

Epoch: 5| Step: 10
Training loss: 2.2861480386480353
Validation loss: 2.4632901002108096

Epoch: 5| Step: 11
Training loss: 2.7650583839805787
Validation loss: 2.476076878730731

Epoch: 160| Step: 0
Training loss: 2.0158361280005854
Validation loss: 2.46396006498445

Epoch: 5| Step: 1
Training loss: 2.5048267499513273
Validation loss: 2.4612073154987786

Epoch: 5| Step: 2
Training loss: 3.3224279508679335
Validation loss: 2.4607905611443415

Epoch: 5| Step: 3
Training loss: 2.1848683467283334
Validation loss: 2.4568529150225835

Epoch: 5| Step: 4
Training loss: 2.287900662379809
Validation loss: 2.4618178796093546

Epoch: 5| Step: 5
Training loss: 2.5400326797305426
Validation loss: 2.4639395331366534

Epoch: 5| Step: 6
Training loss: 2.684116806740452
Validation loss: 2.461403167909302

Epoch: 5| Step: 7
Training loss: 2.374361655129257
Validation loss: 2.4652273058015486

Epoch: 5| Step: 8
Training loss: 2.1715835265929724
Validation loss: 2.453143160240517

Epoch: 5| Step: 9
Training loss: 2.5145506845381114
Validation loss: 2.462497257256957

Epoch: 5| Step: 10
Training loss: 2.473576330842037
Validation loss: 2.465216135469219

Epoch: 5| Step: 11
Training loss: 1.381757473824119
Validation loss: 2.470181625473632

Epoch: 161| Step: 0
Training loss: 2.648211084100618
Validation loss: 2.467110455602346

Epoch: 5| Step: 1
Training loss: 2.4768022006066968
Validation loss: 2.458990616673069

Epoch: 5| Step: 2
Training loss: 3.07735204915857
Validation loss: 2.4686754030312934

Epoch: 5| Step: 3
Training loss: 2.6428223445619214
Validation loss: 2.46625958146459

Epoch: 5| Step: 4
Training loss: 2.119296891739837
Validation loss: 2.4591513272290406

Epoch: 5| Step: 5
Training loss: 2.8042164356487618
Validation loss: 2.465120947838674

Epoch: 5| Step: 6
Training loss: 2.18474536206353
Validation loss: 2.4531324835985076

Epoch: 5| Step: 7
Training loss: 2.226399947808142
Validation loss: 2.463423398803526

Epoch: 5| Step: 8
Training loss: 2.319249895607972
Validation loss: 2.461082268549007

Epoch: 5| Step: 9
Training loss: 2.1652808770573606
Validation loss: 2.46480078642128

Epoch: 5| Step: 10
Training loss: 2.351044658027428
Validation loss: 2.4648669040759192

Epoch: 5| Step: 11
Training loss: 2.5863020256787075
Validation loss: 2.456255836415172

Epoch: 162| Step: 0
Training loss: 2.409144654500554
Validation loss: 2.464065243419732

Epoch: 5| Step: 1
Training loss: 2.1518513138680957
Validation loss: 2.4575924821153676

Epoch: 5| Step: 2
Training loss: 2.7120794260271643
Validation loss: 2.463910247994955

Epoch: 5| Step: 3
Training loss: 2.139265979482037
Validation loss: 2.466826647379728

Epoch: 5| Step: 4
Training loss: 2.346773460822778
Validation loss: 2.4569787695555765

Epoch: 5| Step: 5
Training loss: 2.904266993478592
Validation loss: 2.475578385340942

Epoch: 5| Step: 6
Training loss: 2.5001208276160263
Validation loss: 2.472698815950134

Epoch: 5| Step: 7
Training loss: 2.46660607839182
Validation loss: 2.4551066192373434

Epoch: 5| Step: 8
Training loss: 2.560114983420182
Validation loss: 2.4663873263545293

Epoch: 5| Step: 9
Training loss: 2.2561823361889126
Validation loss: 2.4549173015642567

Epoch: 5| Step: 10
Training loss: 2.315916064215562
Validation loss: 2.461925661412029

Epoch: 5| Step: 11
Training loss: 3.5406393618178766
Validation loss: 2.461693888685697

Epoch: 163| Step: 0
Training loss: 2.4898986829497662
Validation loss: 2.4622515969443124

Epoch: 5| Step: 1
Training loss: 2.3215207637044806
Validation loss: 2.4637764556060477

Epoch: 5| Step: 2
Training loss: 2.4598139059808743
Validation loss: 2.4648483163475947

Epoch: 5| Step: 3
Training loss: 2.803371024844159
Validation loss: 2.468055204813521

Epoch: 5| Step: 4
Training loss: 2.5326990759913794
Validation loss: 2.4703559360147853

Epoch: 5| Step: 5
Training loss: 2.463345757101662
Validation loss: 2.467756336969589

Epoch: 5| Step: 6
Training loss: 2.5001460032744505
Validation loss: 2.466762072311876

Epoch: 5| Step: 7
Training loss: 2.4498609464460195
Validation loss: 2.464489386910456

Epoch: 5| Step: 8
Training loss: 2.0687134523419117
Validation loss: 2.467246571615403

Epoch: 5| Step: 9
Training loss: 2.8324634301164027
Validation loss: 2.4667531238903537

Epoch: 5| Step: 10
Training loss: 2.2817541114132283
Validation loss: 2.4614399714894226

Epoch: 5| Step: 11
Training loss: 3.5908837871735475
Validation loss: 2.471096629694755

Epoch: 164| Step: 0
Training loss: 2.486710704007856
Validation loss: 2.4599720993889203

Epoch: 5| Step: 1
Training loss: 2.630761499432157
Validation loss: 2.467562599167505

Epoch: 5| Step: 2
Training loss: 2.627714932957984
Validation loss: 2.460249959179134

Epoch: 5| Step: 3
Training loss: 2.675129653783304
Validation loss: 2.463923170028521

Epoch: 5| Step: 4
Training loss: 1.826708840152498
Validation loss: 2.462857303903211

Epoch: 5| Step: 5
Training loss: 1.9042233809936533
Validation loss: 2.456117823088216

Epoch: 5| Step: 6
Training loss: 2.7909672748283163
Validation loss: 2.4604306501356716

Epoch: 5| Step: 7
Training loss: 2.9500915448483696
Validation loss: 2.4659701079642384

Epoch: 5| Step: 8
Training loss: 2.5306513514033533
Validation loss: 2.4670887600976985

Epoch: 5| Step: 9
Training loss: 1.9062371800726383
Validation loss: 2.4723643270707996

Epoch: 5| Step: 10
Training loss: 2.6149001417185453
Validation loss: 2.4670937007945426

Epoch: 5| Step: 11
Training loss: 2.5675426272351696
Validation loss: 2.4708837752208352

Epoch: 165| Step: 0
Training loss: 2.856712560948999
Validation loss: 2.4697005457957713

Epoch: 5| Step: 1
Training loss: 2.543365681927079
Validation loss: 2.474132348348635

Epoch: 5| Step: 2
Training loss: 2.2610044520864476
Validation loss: 2.4650383906183055

Epoch: 5| Step: 3
Training loss: 2.7163173271503727
Validation loss: 2.4618667906687004

Epoch: 5| Step: 4
Training loss: 2.4260184990538436
Validation loss: 2.4718551362465697

Epoch: 5| Step: 5
Training loss: 2.573500861642655
Validation loss: 2.461220405101036

Epoch: 5| Step: 6
Training loss: 2.458282103785982
Validation loss: 2.469896271730941

Epoch: 5| Step: 7
Training loss: 1.973953696642205
Validation loss: 2.46510024632934

Epoch: 5| Step: 8
Training loss: 1.9756006607057648
Validation loss: 2.4704344431219925

Epoch: 5| Step: 9
Training loss: 2.46047504257819
Validation loss: 2.464902398509453

Epoch: 5| Step: 10
Training loss: 2.7657488779029338
Validation loss: 2.4591850499851664

Epoch: 5| Step: 11
Training loss: 2.4661614089576007
Validation loss: 2.4640667754244836

Epoch: 166| Step: 0
Training loss: 2.884704465224223
Validation loss: 2.454013570393838

Epoch: 5| Step: 1
Training loss: 2.658873037083859
Validation loss: 2.46360592963268

Epoch: 5| Step: 2
Training loss: 2.501603279996551
Validation loss: 2.4537964001292885

Epoch: 5| Step: 3
Training loss: 2.6136997041829884
Validation loss: 2.462443779853865

Epoch: 5| Step: 4
Training loss: 2.079083329278023
Validation loss: 2.4602957721394945

Epoch: 5| Step: 5
Training loss: 2.6726742268537302
Validation loss: 2.4633866006600593

Epoch: 5| Step: 6
Training loss: 2.6777908241337562
Validation loss: 2.4849837734887448

Epoch: 5| Step: 7
Training loss: 2.249839988952795
Validation loss: 2.486605848217627

Epoch: 5| Step: 8
Training loss: 2.5499020239763452
Validation loss: 2.4731235956408635

Epoch: 5| Step: 9
Training loss: 2.5962950388234742
Validation loss: 2.4583635463043367

Epoch: 5| Step: 10
Training loss: 1.9702920475502534
Validation loss: 2.4553269305992007

Epoch: 5| Step: 11
Training loss: 1.1450749170373316
Validation loss: 2.456561829604432

Epoch: 167| Step: 0
Training loss: 2.4101540737049536
Validation loss: 2.467266884690883

Epoch: 5| Step: 1
Training loss: 2.23734859188698
Validation loss: 2.45751127679617

Epoch: 5| Step: 2
Training loss: 3.056690544720162
Validation loss: 2.46032945094517

Epoch: 5| Step: 3
Training loss: 1.9376443993687673
Validation loss: 2.4592451301224787

Epoch: 5| Step: 4
Training loss: 2.5442714372293826
Validation loss: 2.4661631007867277

Epoch: 5| Step: 5
Training loss: 2.141343713102603
Validation loss: 2.4640722261284047

Epoch: 5| Step: 6
Training loss: 2.490240023737466
Validation loss: 2.464011892775893

Epoch: 5| Step: 7
Training loss: 2.2232969731565704
Validation loss: 2.4662755846790096

Epoch: 5| Step: 8
Training loss: 2.4115291994254178
Validation loss: 2.4641909613995763

Epoch: 5| Step: 9
Training loss: 2.728356675561018
Validation loss: 2.4599677299435014

Epoch: 5| Step: 10
Training loss: 2.7699280792153935
Validation loss: 2.4650379594081486

Epoch: 5| Step: 11
Training loss: 2.3147472082084763
Validation loss: 2.4650847795246493

Epoch: 168| Step: 0
Training loss: 1.6867484785802167
Validation loss: 2.458647212233225

Epoch: 5| Step: 1
Training loss: 2.454261759376701
Validation loss: 2.4617963432626273

Epoch: 5| Step: 2
Training loss: 2.335816447205692
Validation loss: 2.473371963612729

Epoch: 5| Step: 3
Training loss: 2.2438513135218217
Validation loss: 2.478133344215305

Epoch: 5| Step: 4
Training loss: 2.655113605432254
Validation loss: 2.4571006453781123

Epoch: 5| Step: 5
Training loss: 2.502345034348572
Validation loss: 2.4586237935171176

Epoch: 5| Step: 6
Training loss: 2.380150280222934
Validation loss: 2.4562552418867223

Epoch: 5| Step: 7
Training loss: 2.5729608776500044
Validation loss: 2.4624389508549696

Epoch: 5| Step: 8
Training loss: 2.7640272794887197
Validation loss: 2.4600447271578947

Epoch: 5| Step: 9
Training loss: 2.7896323596918626
Validation loss: 2.4535271746201586

Epoch: 5| Step: 10
Training loss: 2.745320413368557
Validation loss: 2.462910719892146

Epoch: 5| Step: 11
Training loss: 2.350207270957432
Validation loss: 2.457087040577585

Epoch: 169| Step: 0
Training loss: 2.2776475445286017
Validation loss: 2.464897808080662

Epoch: 5| Step: 1
Training loss: 2.481166951593727
Validation loss: 2.458112966768446

Epoch: 5| Step: 2
Training loss: 2.7688442825556767
Validation loss: 2.4668689333674636

Epoch: 5| Step: 3
Training loss: 2.429768551765564
Validation loss: 2.473138654676148

Epoch: 5| Step: 4
Training loss: 2.360953907938
Validation loss: 2.461796331156702

Epoch: 5| Step: 5
Training loss: 2.095911760423828
Validation loss: 2.4657829377517677

Epoch: 5| Step: 6
Training loss: 2.65315577980043
Validation loss: 2.4744529443849035

Epoch: 5| Step: 7
Training loss: 2.7150477490520437
Validation loss: 2.4642957167804163

Epoch: 5| Step: 8
Training loss: 1.9663147169595185
Validation loss: 2.4702383134396446

Epoch: 5| Step: 9
Training loss: 2.7160232725986284
Validation loss: 2.459325585440427

Epoch: 5| Step: 10
Training loss: 2.740262390994078
Validation loss: 2.4668267480567176

Epoch: 5| Step: 11
Training loss: 1.552112229182787
Validation loss: 2.4651375024732305

Epoch: 170| Step: 0
Training loss: 2.9368014215156775
Validation loss: 2.460993250462622

Epoch: 5| Step: 1
Training loss: 2.0292866534947698
Validation loss: 2.457761118384719

Epoch: 5| Step: 2
Training loss: 2.5082553935905967
Validation loss: 2.4670353419562523

Epoch: 5| Step: 3
Training loss: 2.361546003893089
Validation loss: 2.4611032642061685

Epoch: 5| Step: 4
Training loss: 2.6291764051092787
Validation loss: 2.4628854943629648

Epoch: 5| Step: 5
Training loss: 2.2649318292392544
Validation loss: 2.4615091699319254

Epoch: 5| Step: 6
Training loss: 2.513553785560208
Validation loss: 2.4584181489429766

Epoch: 5| Step: 7
Training loss: 1.8043400814006085
Validation loss: 2.4607290108986457

Epoch: 5| Step: 8
Training loss: 2.889907335425936
Validation loss: 2.463158004327965

Epoch: 5| Step: 9
Training loss: 2.6655488552295337
Validation loss: 2.4618227138617796

Epoch: 5| Step: 10
Training loss: 2.3178578760715665
Validation loss: 2.468999880204366

Epoch: 5| Step: 11
Training loss: 2.555603796277859
Validation loss: 2.4623008181123813

Epoch: 171| Step: 0
Training loss: 1.8951751558585994
Validation loss: 2.470446229226901

Epoch: 5| Step: 1
Training loss: 2.0888861342788463
Validation loss: 2.470951615655032

Epoch: 5| Step: 2
Training loss: 2.3765769541725352
Validation loss: 2.4782945009736768

Epoch: 5| Step: 3
Training loss: 2.9871635790267277
Validation loss: 2.481217467081154

Epoch: 5| Step: 4
Training loss: 2.46036554380876
Validation loss: 2.4919829887576945

Epoch: 5| Step: 5
Training loss: 2.295930006296149
Validation loss: 2.5175022363384456

Epoch: 5| Step: 6
Training loss: 3.140552121355913
Validation loss: 2.4939314780174096

Epoch: 5| Step: 7
Training loss: 2.450244938031228
Validation loss: 2.462586640513615

Epoch: 5| Step: 8
Training loss: 3.0386267499200343
Validation loss: 2.4670905277972173

Epoch: 5| Step: 9
Training loss: 1.8637610883274185
Validation loss: 2.4628944326356703

Epoch: 5| Step: 10
Training loss: 2.269770755703594
Validation loss: 2.4704184588059452

Epoch: 5| Step: 11
Training loss: 1.3060213682871378
Validation loss: 2.4772083872249815

Epoch: 172| Step: 0
Training loss: 2.2768998146851525
Validation loss: 2.47721135878331

Epoch: 5| Step: 1
Training loss: 2.221021537130155
Validation loss: 2.4752688473807565

Epoch: 5| Step: 2
Training loss: 2.640575611623243
Validation loss: 2.4834913845671864

Epoch: 5| Step: 3
Training loss: 2.9474039519290818
Validation loss: 2.4974230995851037

Epoch: 5| Step: 4
Training loss: 2.547753866946438
Validation loss: 2.5008734011869724

Epoch: 5| Step: 5
Training loss: 2.3665047343916195
Validation loss: 2.510283970524484

Epoch: 5| Step: 6
Training loss: 2.929892408198739
Validation loss: 2.5049237005653064

Epoch: 5| Step: 7
Training loss: 2.8014018330243204
Validation loss: 2.500999024575923

Epoch: 5| Step: 8
Training loss: 2.606699045407827
Validation loss: 2.5050147663183773

Epoch: 5| Step: 9
Training loss: 2.690280185983508
Validation loss: 2.504607978600756

Epoch: 5| Step: 10
Training loss: 2.2702307878716197
Validation loss: 2.49894153122357

Epoch: 5| Step: 11
Training loss: 2.5557982103811523
Validation loss: 2.4907291414512587

Epoch: 173| Step: 0
Training loss: 2.8007243003003213
Validation loss: 2.4799988549368277

Epoch: 5| Step: 1
Training loss: 2.3684429201153923
Validation loss: 2.4694883795587943

Epoch: 5| Step: 2
Training loss: 2.4093922496925275
Validation loss: 2.4686924609790473

Epoch: 5| Step: 3
Training loss: 2.298742145236578
Validation loss: 2.4580956414179225

Epoch: 5| Step: 4
Training loss: 2.21155272641892
Validation loss: 2.462800865544383

Epoch: 5| Step: 5
Training loss: 2.385052138104543
Validation loss: 2.4706316629762726

Epoch: 5| Step: 6
Training loss: 2.7851384444557072
Validation loss: 2.4655677953159088

Epoch: 5| Step: 7
Training loss: 2.5771539882471486
Validation loss: 2.4791089152707415

Epoch: 5| Step: 8
Training loss: 2.558477733981107
Validation loss: 2.4792239131782767

Epoch: 5| Step: 9
Training loss: 2.7825712001907266
Validation loss: 2.477010399621922

Epoch: 5| Step: 10
Training loss: 2.6295830817522936
Validation loss: 2.4826460206137586

Epoch: 5| Step: 11
Training loss: 0.6793834839460458
Validation loss: 2.480715781899293

Epoch: 174| Step: 0
Training loss: 2.6157263477520853
Validation loss: 2.4932070991257245

Epoch: 5| Step: 1
Training loss: 2.174513716432734
Validation loss: 2.4767061386411426

Epoch: 5| Step: 2
Training loss: 2.4205108738595684
Validation loss: 2.4693986587787813

Epoch: 5| Step: 3
Training loss: 2.6122748428493776
Validation loss: 2.477650368365097

Epoch: 5| Step: 4
Training loss: 2.7508326917029504
Validation loss: 2.4701671918726853

Epoch: 5| Step: 5
Training loss: 2.2817179578761335
Validation loss: 2.4651982434894606

Epoch: 5| Step: 6
Training loss: 2.588453100724616
Validation loss: 2.4592237106089976

Epoch: 5| Step: 7
Training loss: 2.4402641860022762
Validation loss: 2.4578793370495213

Epoch: 5| Step: 8
Training loss: 2.7805712439566275
Validation loss: 2.461155045193501

Epoch: 5| Step: 9
Training loss: 2.2036990236303295
Validation loss: 2.4604714976722333

Epoch: 5| Step: 10
Training loss: 2.313042448189498
Validation loss: 2.4592272472147942

Epoch: 5| Step: 11
Training loss: 2.6122812316411905
Validation loss: 2.465457716868654

Epoch: 175| Step: 0
Training loss: 2.2314612323351697
Validation loss: 2.459101356266904

Epoch: 5| Step: 1
Training loss: 2.015243967399091
Validation loss: 2.4614562764416816

Epoch: 5| Step: 2
Training loss: 2.648822947672735
Validation loss: 2.464322909195563

Epoch: 5| Step: 3
Training loss: 2.0731186760342193
Validation loss: 2.464641967832661

Epoch: 5| Step: 4
Training loss: 2.215191849092013
Validation loss: 2.4602120717898055

Epoch: 5| Step: 5
Training loss: 2.6572194742474826
Validation loss: 2.481672470502512

Epoch: 5| Step: 6
Training loss: 2.493675051067188
Validation loss: 2.4726920785663475

Epoch: 5| Step: 7
Training loss: 2.52597135990002
Validation loss: 2.4628470182781013

Epoch: 5| Step: 8
Training loss: 2.613646614295851
Validation loss: 2.4696752407885203

Epoch: 5| Step: 9
Training loss: 2.69293815965047
Validation loss: 2.460589437746947

Epoch: 5| Step: 10
Training loss: 2.719968851416012
Validation loss: 2.469692756418443

Epoch: 5| Step: 11
Training loss: 3.1275936807313203
Validation loss: 2.463870989594592

Epoch: 176| Step: 0
Training loss: 2.273687709397604
Validation loss: 2.465806554371942

Epoch: 5| Step: 1
Training loss: 2.2947284243598287
Validation loss: 2.4686542826417965

Epoch: 5| Step: 2
Training loss: 2.5342836450558197
Validation loss: 2.4622667547482116

Epoch: 5| Step: 3
Training loss: 3.068577710693497
Validation loss: 2.4689235304352506

Epoch: 5| Step: 4
Training loss: 2.572172008250392
Validation loss: 2.4665098851442298

Epoch: 5| Step: 5
Training loss: 1.8465011244067258
Validation loss: 2.468081077860419

Epoch: 5| Step: 6
Training loss: 2.321440964969627
Validation loss: 2.4733964998177034

Epoch: 5| Step: 7
Training loss: 2.225787004307398
Validation loss: 2.476152368033306

Epoch: 5| Step: 8
Training loss: 2.021601370406658
Validation loss: 2.473228215681184

Epoch: 5| Step: 9
Training loss: 2.401503401992742
Validation loss: 2.476555733014638

Epoch: 5| Step: 10
Training loss: 3.0591814394886607
Validation loss: 2.468923120021954

Epoch: 5| Step: 11
Training loss: 3.156003130332255
Validation loss: 2.4687562511863916

Epoch: 177| Step: 0
Training loss: 2.3905735384320157
Validation loss: 2.4731328062060105

Epoch: 5| Step: 1
Training loss: 2.7254220058301577
Validation loss: 2.47520749060435

Epoch: 5| Step: 2
Training loss: 2.3890711902088615
Validation loss: 2.4816345679122507

Epoch: 5| Step: 3
Training loss: 2.449905226295166
Validation loss: 2.4823840058622437

Epoch: 5| Step: 4
Training loss: 2.8124640568449797
Validation loss: 2.484052469202334

Epoch: 5| Step: 5
Training loss: 2.2758482640643956
Validation loss: 2.472875013913885

Epoch: 5| Step: 6
Training loss: 2.495597969212587
Validation loss: 2.469270860475804

Epoch: 5| Step: 7
Training loss: 2.30040547487088
Validation loss: 2.469899388836916

Epoch: 5| Step: 8
Training loss: 2.6930775097253834
Validation loss: 2.4833540348069127

Epoch: 5| Step: 9
Training loss: 2.0930421686057277
Validation loss: 2.468356362681224

Epoch: 5| Step: 10
Training loss: 2.3907077750043624
Validation loss: 2.4755134406375006

Epoch: 5| Step: 11
Training loss: 1.2880657619605491
Validation loss: 2.472991567075681

Epoch: 178| Step: 0
Training loss: 2.7803651923852275
Validation loss: 2.487531631913973

Epoch: 5| Step: 1
Training loss: 2.475349008632636
Validation loss: 2.4683740104188687

Epoch: 5| Step: 2
Training loss: 2.3712853946567876
Validation loss: 2.469893098311914

Epoch: 5| Step: 3
Training loss: 2.390184947882278
Validation loss: 2.4759732256331706

Epoch: 5| Step: 4
Training loss: 2.736209188813068
Validation loss: 2.4731749060454753

Epoch: 5| Step: 5
Training loss: 1.991865423758223
Validation loss: 2.4729652312174517

Epoch: 5| Step: 6
Training loss: 2.3634695939605246
Validation loss: 2.4782026657947953

Epoch: 5| Step: 7
Training loss: 2.4702921046766875
Validation loss: 2.4857683732840643

Epoch: 5| Step: 8
Training loss: 2.1261097871096077
Validation loss: 2.4818776732454144

Epoch: 5| Step: 9
Training loss: 2.7717818355278627
Validation loss: 2.473318528547046

Epoch: 5| Step: 10
Training loss: 2.492395661369298
Validation loss: 2.473696003440703

Epoch: 5| Step: 11
Training loss: 1.7483070904435287
Validation loss: 2.4778658368576387

Epoch: 179| Step: 0
Training loss: 1.9952716366161496
Validation loss: 2.4688951312836114

Epoch: 5| Step: 1
Training loss: 2.5281768793505077
Validation loss: 2.48482496157465

Epoch: 5| Step: 2
Training loss: 2.828084365805721
Validation loss: 2.4800178338250496

Epoch: 5| Step: 3
Training loss: 2.1445047901515424
Validation loss: 2.4771643447836436

Epoch: 5| Step: 4
Training loss: 2.2895130326016133
Validation loss: 2.476405988174095

Epoch: 5| Step: 5
Training loss: 2.7141202772246373
Validation loss: 2.4728165143200282

Epoch: 5| Step: 6
Training loss: 2.2058521463158516
Validation loss: 2.4719185956734746

Epoch: 5| Step: 7
Training loss: 2.835107771290027
Validation loss: 2.4736474347099673

Epoch: 5| Step: 8
Training loss: 2.3519463210046285
Validation loss: 2.4754479354124257

Epoch: 5| Step: 9
Training loss: 2.441518161497547
Validation loss: 2.48324840921018

Epoch: 5| Step: 10
Training loss: 2.4843268239850373
Validation loss: 2.470514160086975

Epoch: 5| Step: 11
Training loss: 1.7845360411010267
Validation loss: 2.461192760651161

Epoch: 180| Step: 0
Training loss: 2.448832066140751
Validation loss: 2.479889525220433

Epoch: 5| Step: 1
Training loss: 2.6399598497169756
Validation loss: 2.47612326143463

Epoch: 5| Step: 2
Training loss: 2.9588230605265022
Validation loss: 2.47845667535225

Epoch: 5| Step: 3
Training loss: 2.3404458724822166
Validation loss: 2.480323302642892

Epoch: 5| Step: 4
Training loss: 2.0722902460100747
Validation loss: 2.473012813112009

Epoch: 5| Step: 5
Training loss: 2.0672513391119276
Validation loss: 2.4688679389306056

Epoch: 5| Step: 6
Training loss: 2.810383572582704
Validation loss: 2.4662357959766865

Epoch: 5| Step: 7
Training loss: 2.2021556913336737
Validation loss: 2.465622455980788

Epoch: 5| Step: 8
Training loss: 1.6842220399723395
Validation loss: 2.4681168641682163

Epoch: 5| Step: 9
Training loss: 2.691948336418674
Validation loss: 2.4693889354459677

Epoch: 5| Step: 10
Training loss: 2.7161925992417286
Validation loss: 2.4816645465732465

Epoch: 5| Step: 11
Training loss: 2.551935422594492
Validation loss: 2.4745020954790977

Epoch: 181| Step: 0
Training loss: 2.593269303601104
Validation loss: 2.471580737507245

Epoch: 5| Step: 1
Training loss: 2.3859824185742875
Validation loss: 2.4641568697393947

Epoch: 5| Step: 2
Training loss: 2.4581094629110463
Validation loss: 2.477367763589366

Epoch: 5| Step: 3
Training loss: 2.4639557268017045
Validation loss: 2.4754985746258726

Epoch: 5| Step: 4
Training loss: 2.7301304598087768
Validation loss: 2.4863758766800914

Epoch: 5| Step: 5
Training loss: 2.216584989529638
Validation loss: 2.4820547805389577

Epoch: 5| Step: 6
Training loss: 2.633951998922796
Validation loss: 2.492704531173474

Epoch: 5| Step: 7
Training loss: 2.1191037223169373
Validation loss: 2.4890771009324655

Epoch: 5| Step: 8
Training loss: 2.4942172881840645
Validation loss: 2.5023202142909775

Epoch: 5| Step: 9
Training loss: 2.199238914275343
Validation loss: 2.502628315714147

Epoch: 5| Step: 10
Training loss: 2.6298287800799165
Validation loss: 2.4861127485918684

Epoch: 5| Step: 11
Training loss: 1.9370091339471032
Validation loss: 2.4812230742888923

Epoch: 182| Step: 0
Training loss: 2.180737970366442
Validation loss: 2.4800411266267504

Epoch: 5| Step: 1
Training loss: 2.476076096383754
Validation loss: 2.4782557710744384

Epoch: 5| Step: 2
Training loss: 2.7889086256135185
Validation loss: 2.472005339609015

Epoch: 5| Step: 3
Training loss: 2.111346305606564
Validation loss: 2.4702230597753574

Epoch: 5| Step: 4
Training loss: 2.680404700567749
Validation loss: 2.4784219983160063

Epoch: 5| Step: 5
Training loss: 2.387730460503061
Validation loss: 2.469962647138882

Epoch: 5| Step: 6
Training loss: 2.256760718537715
Validation loss: 2.470675703255888

Epoch: 5| Step: 7
Training loss: 2.0930835175525595
Validation loss: 2.477421085253725

Epoch: 5| Step: 8
Training loss: 2.1625039999158613
Validation loss: 2.486613055272137

Epoch: 5| Step: 9
Training loss: 2.496429946060161
Validation loss: 2.491903290841149

Epoch: 5| Step: 10
Training loss: 2.558585083924971
Validation loss: 2.4859307374348742

Epoch: 5| Step: 11
Training loss: 4.361150127217464
Validation loss: 2.4960033776485075

Epoch: 183| Step: 0
Training loss: 2.050166039604478
Validation loss: 2.492512533264126

Epoch: 5| Step: 1
Training loss: 2.4058367882194944
Validation loss: 2.4875722261227526

Epoch: 5| Step: 2
Training loss: 2.456615127719885
Validation loss: 2.47923173071061

Epoch: 5| Step: 3
Training loss: 2.747028305757747
Validation loss: 2.4755340510541908

Epoch: 5| Step: 4
Training loss: 2.06577780384907
Validation loss: 2.47207009115894

Epoch: 5| Step: 5
Training loss: 2.231325322437219
Validation loss: 2.4823189070521323

Epoch: 5| Step: 6
Training loss: 2.6110965737373277
Validation loss: 2.4796489091613303

Epoch: 5| Step: 7
Training loss: 2.8889445601095773
Validation loss: 2.47819080834642

Epoch: 5| Step: 8
Training loss: 2.6348860637490965
Validation loss: 2.4760916189039843

Epoch: 5| Step: 9
Training loss: 2.573172604164745
Validation loss: 2.4769986648034923

Epoch: 5| Step: 10
Training loss: 2.2222330397766457
Validation loss: 2.479064588021331

Epoch: 5| Step: 11
Training loss: 1.4024509399591523
Validation loss: 2.486610450507618

Epoch: 184| Step: 0
Training loss: 2.637374502513818
Validation loss: 2.481222115399213

Epoch: 5| Step: 1
Training loss: 2.4955748494777117
Validation loss: 2.471335939930362

Epoch: 5| Step: 2
Training loss: 2.801102411007042
Validation loss: 2.4798842595027506

Epoch: 5| Step: 3
Training loss: 1.9628577689841913
Validation loss: 2.4740982270424707

Epoch: 5| Step: 4
Training loss: 2.018801293444838
Validation loss: 2.4748497404378482

Epoch: 5| Step: 5
Training loss: 2.580179863655768
Validation loss: 2.4720043168641075

Epoch: 5| Step: 6
Training loss: 2.162952124458652
Validation loss: 2.473769364752656

Epoch: 5| Step: 7
Training loss: 2.6057905601778444
Validation loss: 2.4719658199558316

Epoch: 5| Step: 8
Training loss: 2.4298707948094225
Validation loss: 2.473239630991779

Epoch: 5| Step: 9
Training loss: 2.6985499302823373
Validation loss: 2.4783453074946076

Epoch: 5| Step: 10
Training loss: 2.3422759443383843
Validation loss: 2.4757319841812784

Epoch: 5| Step: 11
Training loss: 1.7598133144204042
Validation loss: 2.4713235551059074

Epoch: 185| Step: 0
Training loss: 2.2817812784340887
Validation loss: 2.476138314262769

Epoch: 5| Step: 1
Training loss: 1.9879230408921795
Validation loss: 2.4790538046228456

Epoch: 5| Step: 2
Training loss: 2.4597579793582542
Validation loss: 2.474061451151567

Epoch: 5| Step: 3
Training loss: 2.1749808518893228
Validation loss: 2.4767071654609754

Epoch: 5| Step: 4
Training loss: 2.816607802714596
Validation loss: 2.4753522352552926

Epoch: 5| Step: 5
Training loss: 2.07301286892685
Validation loss: 2.4738055908166263

Epoch: 5| Step: 6
Training loss: 3.000561343763121
Validation loss: 2.477583733801112

Epoch: 5| Step: 7
Training loss: 2.5957871675340645
Validation loss: 2.4737363909834875

Epoch: 5| Step: 8
Training loss: 2.447311802799069
Validation loss: 2.471675145830704

Epoch: 5| Step: 9
Training loss: 2.6993887774607614
Validation loss: 2.4709688227227984

Epoch: 5| Step: 10
Training loss: 1.9589506623703719
Validation loss: 2.482046287490268

Epoch: 5| Step: 11
Training loss: 3.313921065776944
Validation loss: 2.4950410655852453

Epoch: 186| Step: 0
Training loss: 2.052286231632979
Validation loss: 2.4908468293655743

Epoch: 5| Step: 1
Training loss: 2.99904045018172
Validation loss: 2.506338643054671

Epoch: 5| Step: 2
Training loss: 2.2800824038354466
Validation loss: 2.489200136921535

Epoch: 5| Step: 3
Training loss: 2.1840583975673304
Validation loss: 2.507900101824098

Epoch: 5| Step: 4
Training loss: 2.7380059249401425
Validation loss: 2.5036390125257664

Epoch: 5| Step: 5
Training loss: 2.64850708636574
Validation loss: 2.5009824729649996

Epoch: 5| Step: 6
Training loss: 2.193238279646366
Validation loss: 2.495510938696739

Epoch: 5| Step: 7
Training loss: 2.187911512632395
Validation loss: 2.501787399056288

Epoch: 5| Step: 8
Training loss: 2.1499559619740434
Validation loss: 2.4929408904801957

Epoch: 5| Step: 9
Training loss: 2.883462475951391
Validation loss: 2.482591424649834

Epoch: 5| Step: 10
Training loss: 2.4705615579810356
Validation loss: 2.4884132657644926

Epoch: 5| Step: 11
Training loss: 1.7576385411969249
Validation loss: 2.476242554191424

Epoch: 187| Step: 0
Training loss: 2.435690477140589
Validation loss: 2.479182821643243

Epoch: 5| Step: 1
Training loss: 2.4046723779868393
Validation loss: 2.4666718093607964

Epoch: 5| Step: 2
Training loss: 2.180717525689507
Validation loss: 2.48069324829146

Epoch: 5| Step: 3
Training loss: 2.685053221674136
Validation loss: 2.4845571751048925

Epoch: 5| Step: 4
Training loss: 2.1880577466272877
Validation loss: 2.4772300101429154

Epoch: 5| Step: 5
Training loss: 2.41739412298775
Validation loss: 2.475479743681251

Epoch: 5| Step: 6
Training loss: 2.3501028809448727
Validation loss: 2.4765917297948827

Epoch: 5| Step: 7
Training loss: 2.459891154354593
Validation loss: 2.4781427967255083

Epoch: 5| Step: 8
Training loss: 2.156670570822026
Validation loss: 2.4869637547093664

Epoch: 5| Step: 9
Training loss: 2.267612296018641
Validation loss: 2.4888517563137027

Epoch: 5| Step: 10
Training loss: 3.0691869482909593
Validation loss: 2.4964179324474256

Epoch: 5| Step: 11
Training loss: 2.449467161196348
Validation loss: 2.4971492527595553

Epoch: 188| Step: 0
Training loss: 2.804722692752306
Validation loss: 2.4992136990288554

Epoch: 5| Step: 1
Training loss: 3.017664086558781
Validation loss: 2.501401691085115

Epoch: 5| Step: 2
Training loss: 2.919305661096096
Validation loss: 2.492035405730782

Epoch: 5| Step: 3
Training loss: 2.1495184381248333
Validation loss: 2.496080624382201

Epoch: 5| Step: 4
Training loss: 1.94905765456392
Validation loss: 2.4932171678596364

Epoch: 5| Step: 5
Training loss: 2.5553011375977968
Validation loss: 2.4923553370420723

Epoch: 5| Step: 6
Training loss: 2.42403841912319
Validation loss: 2.488747254176456

Epoch: 5| Step: 7
Training loss: 1.8855239528529535
Validation loss: 2.4888507604487216

Epoch: 5| Step: 8
Training loss: 2.1926933765278807
Validation loss: 2.484211646413625

Epoch: 5| Step: 9
Training loss: 2.1789618484007214
Validation loss: 2.479682369182801

Epoch: 5| Step: 10
Training loss: 2.427170213009364
Validation loss: 2.478434612211723

Epoch: 5| Step: 11
Training loss: 1.5566681173809298
Validation loss: 2.4839447456642407

Epoch: 189| Step: 0
Training loss: 2.063542189414279
Validation loss: 2.47927058961112

Epoch: 5| Step: 1
Training loss: 2.2057040654855564
Validation loss: 2.4833314731343448

Epoch: 5| Step: 2
Training loss: 2.16882977957295
Validation loss: 2.4741079720198798

Epoch: 5| Step: 3
Training loss: 2.9240083301284634
Validation loss: 2.467667885724339

Epoch: 5| Step: 4
Training loss: 2.2891606124318313
Validation loss: 2.472077035182755

Epoch: 5| Step: 5
Training loss: 2.755522471489091
Validation loss: 2.478302072914914

Epoch: 5| Step: 6
Training loss: 2.3146198969121037
Validation loss: 2.481272801919833

Epoch: 5| Step: 7
Training loss: 2.295596227385071
Validation loss: 2.483925568816913

Epoch: 5| Step: 8
Training loss: 2.176269586026895
Validation loss: 2.4872718492348915

Epoch: 5| Step: 9
Training loss: 2.7275918586010874
Validation loss: 2.484069319579025

Epoch: 5| Step: 10
Training loss: 2.4663760203024494
Validation loss: 2.494300377135047

Epoch: 5| Step: 11
Training loss: 2.681935071234641
Validation loss: 2.497060370695861

Epoch: 190| Step: 0
Training loss: 2.133357387148832
Validation loss: 2.515563450725737

Epoch: 5| Step: 1
Training loss: 2.4224781884942677
Validation loss: 2.5253139483225784

Epoch: 5| Step: 2
Training loss: 3.194750001824511
Validation loss: 2.509865685829612

Epoch: 5| Step: 3
Training loss: 2.641586670331174
Validation loss: 2.4897271092183515

Epoch: 5| Step: 4
Training loss: 1.7753306631573604
Validation loss: 2.4730745055216174

Epoch: 5| Step: 5
Training loss: 2.1276008173255905
Validation loss: 2.4758094862225257

Epoch: 5| Step: 6
Training loss: 2.406389802116487
Validation loss: 2.4681012753927143

Epoch: 5| Step: 7
Training loss: 2.6794104252140256
Validation loss: 2.4688137263511334

Epoch: 5| Step: 8
Training loss: 2.946997365504428
Validation loss: 2.4607656307856325

Epoch: 5| Step: 9
Training loss: 2.3464982259870872
Validation loss: 2.464459626604478

Epoch: 5| Step: 10
Training loss: 2.13430443307777
Validation loss: 2.4715572443907945

Epoch: 5| Step: 11
Training loss: 2.5194316992203087
Validation loss: 2.464134233084117

Epoch: 191| Step: 0
Training loss: 2.452258108733284
Validation loss: 2.468086914151179

Epoch: 5| Step: 1
Training loss: 2.510406768483847
Validation loss: 2.4772064502963786

Epoch: 5| Step: 2
Training loss: 2.4606789104901527
Validation loss: 2.4734119889499455

Epoch: 5| Step: 3
Training loss: 1.6997780767209931
Validation loss: 2.4729466039317907

Epoch: 5| Step: 4
Training loss: 2.3234511189680505
Validation loss: 2.482727412147601

Epoch: 5| Step: 5
Training loss: 2.5918333946478067
Validation loss: 2.500235820615246

Epoch: 5| Step: 6
Training loss: 2.434669514030109
Validation loss: 2.4938557780051487

Epoch: 5| Step: 7
Training loss: 2.579352069491226
Validation loss: 2.4985558968252777

Epoch: 5| Step: 8
Training loss: 2.3169600289576326
Validation loss: 2.5144822070044137

Epoch: 5| Step: 9
Training loss: 2.520029228737275
Validation loss: 2.509357661673891

Epoch: 5| Step: 10
Training loss: 2.977733952245641
Validation loss: 2.500259378011386

Epoch: 5| Step: 11
Training loss: 2.114248565583128
Validation loss: 2.4944397327024124

Epoch: 192| Step: 0
Training loss: 2.2691177280326102
Validation loss: 2.485685878369755

Epoch: 5| Step: 1
Training loss: 2.3240264043637566
Validation loss: 2.5021105877304826

Epoch: 5| Step: 2
Training loss: 2.5889109316784746
Validation loss: 2.4966764610651855

Epoch: 5| Step: 3
Training loss: 2.319630326711586
Validation loss: 2.5117469695045025

Epoch: 5| Step: 4
Training loss: 2.432608856713706
Validation loss: 2.496312819823679

Epoch: 5| Step: 5
Training loss: 2.593191523347999
Validation loss: 2.4963358133396003

Epoch: 5| Step: 6
Training loss: 2.480897400831431
Validation loss: 2.4797756358081475

Epoch: 5| Step: 7
Training loss: 2.2866975721415344
Validation loss: 2.4640311883208112

Epoch: 5| Step: 8
Training loss: 2.4285696714859704
Validation loss: 2.4727399348861887

Epoch: 5| Step: 9
Training loss: 2.930305761586029
Validation loss: 2.4625365133531467

Epoch: 5| Step: 10
Training loss: 2.104274658856297
Validation loss: 2.4707821799435545

Epoch: 5| Step: 11
Training loss: 2.4989539819138615
Validation loss: 2.478208107454697

Epoch: 193| Step: 0
Training loss: 2.4599221693302815
Validation loss: 2.4716411915643306

Epoch: 5| Step: 1
Training loss: 2.5231490309853393
Validation loss: 2.4621925383131074

Epoch: 5| Step: 2
Training loss: 2.454917981395639
Validation loss: 2.468145763315998

Epoch: 5| Step: 3
Training loss: 2.2044450849656916
Validation loss: 2.4742603412132107

Epoch: 5| Step: 4
Training loss: 2.8908250894114644
Validation loss: 2.478417300655564

Epoch: 5| Step: 5
Training loss: 2.56203605359667
Validation loss: 2.499662583151401

Epoch: 5| Step: 6
Training loss: 2.242992190268769
Validation loss: 2.4963706275391395

Epoch: 5| Step: 7
Training loss: 2.29459407981442
Validation loss: 2.513096709744794

Epoch: 5| Step: 8
Training loss: 1.7376637251303577
Validation loss: 2.5277372994226934

Epoch: 5| Step: 9
Training loss: 2.443238570217517
Validation loss: 2.5146873653818407

Epoch: 5| Step: 10
Training loss: 2.5122839497912377
Validation loss: 2.5165628339703363

Epoch: 5| Step: 11
Training loss: 3.6782835531618465
Validation loss: 2.4929841062909035

Epoch: 194| Step: 0
Training loss: 2.3827122401422565
Validation loss: 2.488828142830739

Epoch: 5| Step: 1
Training loss: 2.0439720243193036
Validation loss: 2.4829518262479127

Epoch: 5| Step: 2
Training loss: 2.5951572874761797
Validation loss: 2.4905544060403235

Epoch: 5| Step: 3
Training loss: 2.366704306056916
Validation loss: 2.486073375275808

Epoch: 5| Step: 4
Training loss: 2.6308235783396614
Validation loss: 2.4758579322926537

Epoch: 5| Step: 5
Training loss: 2.3821790509835368
Validation loss: 2.4874586964537553

Epoch: 5| Step: 6
Training loss: 2.463053057379064
Validation loss: 2.493200819593999

Epoch: 5| Step: 7
Training loss: 2.555901195353638
Validation loss: 2.4959292409828744

Epoch: 5| Step: 8
Training loss: 2.5450549965482714
Validation loss: 2.4940331659191957

Epoch: 5| Step: 9
Training loss: 2.116401779941754
Validation loss: 2.4951145000405965

Epoch: 5| Step: 10
Training loss: 2.511907258976285
Validation loss: 2.5010336249138936

Epoch: 5| Step: 11
Training loss: 2.2056651520850936
Validation loss: 2.4989848440136075

Epoch: 195| Step: 0
Training loss: 1.7888434663314572
Validation loss: 2.5074909673536347

Epoch: 5| Step: 1
Training loss: 2.541270261609635
Validation loss: 2.495825630238724

Epoch: 5| Step: 2
Training loss: 2.7982638017090307
Validation loss: 2.487067560885225

Epoch: 5| Step: 3
Training loss: 2.3792760401859283
Validation loss: 2.4941834535956815

Epoch: 5| Step: 4
Training loss: 2.107745636260527
Validation loss: 2.4966309217658025

Epoch: 5| Step: 5
Training loss: 2.542827549543285
Validation loss: 2.5009568211278936

Epoch: 5| Step: 6
Training loss: 2.4092391632618537
Validation loss: 2.4977724641903296

Epoch: 5| Step: 7
Training loss: 2.1209394142386175
Validation loss: 2.5001653100036267

Epoch: 5| Step: 8
Training loss: 2.57989552131068
Validation loss: 2.4828293848793956

Epoch: 5| Step: 9
Training loss: 2.686897210231007
Validation loss: 2.4905813257208442

Epoch: 5| Step: 10
Training loss: 2.4883803223629513
Validation loss: 2.4953897604768596

Epoch: 5| Step: 11
Training loss: 2.1373736338600735
Validation loss: 2.491209749176408

Epoch: 196| Step: 0
Training loss: 2.3075063245362206
Validation loss: 2.4927824619640657

Epoch: 5| Step: 1
Training loss: 2.7036165000065497
Validation loss: 2.5045038225326666

Epoch: 5| Step: 2
Training loss: 2.070556626231613
Validation loss: 2.5234402320933635

Epoch: 5| Step: 3
Training loss: 2.3888998351721136
Validation loss: 2.5434310676439313

Epoch: 5| Step: 4
Training loss: 2.080988925447926
Validation loss: 2.533101425149043

Epoch: 5| Step: 5
Training loss: 2.2333182657027706
Validation loss: 2.5229044883763803

Epoch: 5| Step: 6
Training loss: 2.5611756787145175
Validation loss: 2.517477141516137

Epoch: 5| Step: 7
Training loss: 2.066415134115177
Validation loss: 2.4994855192410683

Epoch: 5| Step: 8
Training loss: 2.779558588976754
Validation loss: 2.4863822154004547

Epoch: 5| Step: 9
Training loss: 2.752293150818643
Validation loss: 2.484139929152669

Epoch: 5| Step: 10
Training loss: 2.7914648315193102
Validation loss: 2.47272149472488

Epoch: 5| Step: 11
Training loss: 2.123509501400703
Validation loss: 2.478424311068772

Epoch: 197| Step: 0
Training loss: 2.680112932871818
Validation loss: 2.4777247111616014

Epoch: 5| Step: 1
Training loss: 2.4010840113637144
Validation loss: 2.4745729238489558

Epoch: 5| Step: 2
Training loss: 2.486224080176476
Validation loss: 2.475964129952026

Epoch: 5| Step: 3
Training loss: 2.158207543830318
Validation loss: 2.472070143399933

Epoch: 5| Step: 4
Training loss: 1.9762588086737207
Validation loss: 2.4790611538486056

Epoch: 5| Step: 5
Training loss: 2.50479828987514
Validation loss: 2.4768113493620016

Epoch: 5| Step: 6
Training loss: 2.1802707561724866
Validation loss: 2.4808619830799006

Epoch: 5| Step: 7
Training loss: 2.806581925300609
Validation loss: 2.4752634293592894

Epoch: 5| Step: 8
Training loss: 2.187918159844199
Validation loss: 2.4928440277578554

Epoch: 5| Step: 9
Training loss: 3.038577631930531
Validation loss: 2.499053710420222

Epoch: 5| Step: 10
Training loss: 2.239736779449103
Validation loss: 2.499201754446336

Epoch: 5| Step: 11
Training loss: 2.152265543823617
Validation loss: 2.5047794630556153

Epoch: 198| Step: 0
Training loss: 2.8038137450945984
Validation loss: 2.5360517051868086

Epoch: 5| Step: 1
Training loss: 2.2093155913614213
Validation loss: 2.527842300037544

Epoch: 5| Step: 2
Training loss: 2.3849884603928957
Validation loss: 2.534694729544124

Epoch: 5| Step: 3
Training loss: 2.547823395871282
Validation loss: 2.514049071945262

Epoch: 5| Step: 4
Training loss: 2.3170893724876507
Validation loss: 2.5099493451788666

Epoch: 5| Step: 5
Training loss: 2.3338452413419937
Validation loss: 2.498173751969338

Epoch: 5| Step: 6
Training loss: 2.6488347388588442
Validation loss: 2.48705946441494

Epoch: 5| Step: 7
Training loss: 1.8767642939409297
Validation loss: 2.492331437884097

Epoch: 5| Step: 8
Training loss: 2.5011512013167345
Validation loss: 2.4790898973669426

Epoch: 5| Step: 9
Training loss: 2.183658168982133
Validation loss: 2.4878880156634464

Epoch: 5| Step: 10
Training loss: 2.6455720687390722
Validation loss: 2.4873468949374153

Epoch: 5| Step: 11
Training loss: 3.1317859975165483
Validation loss: 2.4823643667471407

Epoch: 199| Step: 0
Training loss: 2.757592038428243
Validation loss: 2.4897450882677172

Epoch: 5| Step: 1
Training loss: 2.271950881299451
Validation loss: 2.4821633024932077

Epoch: 5| Step: 2
Training loss: 2.2030211079227375
Validation loss: 2.496337830932915

Epoch: 5| Step: 3
Training loss: 2.1952375623191878
Validation loss: 2.49740119405787

Epoch: 5| Step: 4
Training loss: 2.153152230897515
Validation loss: 2.5057718643761513

Epoch: 5| Step: 5
Training loss: 2.638249971330307
Validation loss: 2.4973830115685605

Epoch: 5| Step: 6
Training loss: 3.1331410984833368
Validation loss: 2.513135250563652

Epoch: 5| Step: 7
Training loss: 2.161415105793425
Validation loss: 2.5011610394807677

Epoch: 5| Step: 8
Training loss: 2.1152941130905387
Validation loss: 2.504573267003313

Epoch: 5| Step: 9
Training loss: 1.9559442543245937
Validation loss: 2.5061359089403656

Epoch: 5| Step: 10
Training loss: 2.6008981767234935
Validation loss: 2.5133815104659094

Epoch: 5| Step: 11
Training loss: 2.7463895331848067
Validation loss: 2.501580902768967

Epoch: 200| Step: 0
Training loss: 2.1256278737291865
Validation loss: 2.5156704639883647

Epoch: 5| Step: 1
Training loss: 2.1354346173780256
Validation loss: 2.507877800587439

Epoch: 5| Step: 2
Training loss: 2.64822143753625
Validation loss: 2.5145981748427917

Epoch: 5| Step: 3
Training loss: 2.9030534125659724
Validation loss: 2.5173605565183768

Epoch: 5| Step: 4
Training loss: 1.6083093466806568
Validation loss: 2.5169686782539027

Epoch: 5| Step: 5
Training loss: 1.7626205808907274
Validation loss: 2.523630038416534

Epoch: 5| Step: 6
Training loss: 2.984309809906456
Validation loss: 2.5378739651440503

Epoch: 5| Step: 7
Training loss: 2.4429286269115766
Validation loss: 2.539878862651238

Epoch: 5| Step: 8
Training loss: 2.454952458311729
Validation loss: 2.527716737372335

Epoch: 5| Step: 9
Training loss: 1.9759643011343346
Validation loss: 2.5151989064581977

Epoch: 5| Step: 10
Training loss: 2.972456375199781
Validation loss: 2.515205277204862

Epoch: 5| Step: 11
Training loss: 3.271080752853254
Validation loss: 2.4858831350542827

Epoch: 201| Step: 0
Training loss: 2.5248514943783427
Validation loss: 2.500501538672149

Epoch: 5| Step: 1
Training loss: 2.0916755990170746
Validation loss: 2.4840083781422044

Epoch: 5| Step: 2
Training loss: 1.4210231031543739
Validation loss: 2.4841824822837886

Epoch: 5| Step: 3
Training loss: 2.224244098632777
Validation loss: 2.4859368754861055

Epoch: 5| Step: 4
Training loss: 2.296766421288312
Validation loss: 2.4886331394246457

Epoch: 5| Step: 5
Training loss: 2.896863319677942
Validation loss: 2.4948087915988872

Epoch: 5| Step: 6
Training loss: 2.6203734179827136
Validation loss: 2.486058550432366

Epoch: 5| Step: 7
Training loss: 2.230906321666655
Validation loss: 2.4969722294407046

Epoch: 5| Step: 8
Training loss: 2.4410515367317562
Validation loss: 2.4891231635636126

Epoch: 5| Step: 9
Training loss: 2.979466100908007
Validation loss: 2.499538001446243

Epoch: 5| Step: 10
Training loss: 2.6716196930916984
Validation loss: 2.505517017964947

Epoch: 5| Step: 11
Training loss: 1.6323107811012103
Validation loss: 2.4970854618397187

Epoch: 202| Step: 0
Training loss: 3.0107425042572005
Validation loss: 2.494718691263604

Epoch: 5| Step: 1
Training loss: 2.3742053057587125
Validation loss: 2.504084615955276

Epoch: 5| Step: 2
Training loss: 2.822613229431664
Validation loss: 2.496245206994791

Epoch: 5| Step: 3
Training loss: 2.554651102629193
Validation loss: 2.494336034199134

Epoch: 5| Step: 4
Training loss: 1.9253279518085693
Validation loss: 2.496726018179382

Epoch: 5| Step: 5
Training loss: 2.5866264969698776
Validation loss: 2.4952791244800414

Epoch: 5| Step: 6
Training loss: 1.9319826137716072
Validation loss: 2.4862709887887493

Epoch: 5| Step: 7
Training loss: 2.607829292842127
Validation loss: 2.4907091593194397

Epoch: 5| Step: 8
Training loss: 2.4224386236172983
Validation loss: 2.4911712719092134

Epoch: 5| Step: 9
Training loss: 1.8857152572439424
Validation loss: 2.5007531819649857

Epoch: 5| Step: 10
Training loss: 2.2027999962862985
Validation loss: 2.4976073895781097

Epoch: 5| Step: 11
Training loss: 1.5877753071594831
Validation loss: 2.499324373503741

Epoch: 203| Step: 0
Training loss: 2.7918821982313777
Validation loss: 2.504947685804567

Epoch: 5| Step: 1
Training loss: 2.4344977449511265
Validation loss: 2.508138863152675

Epoch: 5| Step: 2
Training loss: 1.752167177254248
Validation loss: 2.506000565366689

Epoch: 5| Step: 3
Training loss: 2.4615840002120786
Validation loss: 2.510096392376426

Epoch: 5| Step: 4
Training loss: 2.4288633856268693
Validation loss: 2.50350713698163

Epoch: 5| Step: 5
Training loss: 2.210844543443948
Validation loss: 2.503356942770477

Epoch: 5| Step: 6
Training loss: 2.6285672245525125
Validation loss: 2.4966385813390954

Epoch: 5| Step: 7
Training loss: 2.631409313607164
Validation loss: 2.5013870365160202

Epoch: 5| Step: 8
Training loss: 2.038767358459259
Validation loss: 2.5056690194308904

Epoch: 5| Step: 9
Training loss: 2.3349686977077235
Validation loss: 2.500199151054181

Epoch: 5| Step: 10
Training loss: 2.44424727275642
Validation loss: 2.5113862382647687

Epoch: 5| Step: 11
Training loss: 2.7995456326896546
Validation loss: 2.511905131289421

Epoch: 204| Step: 0
Training loss: 2.7132560203778593
Validation loss: 2.5081821894072722

Epoch: 5| Step: 1
Training loss: 2.3748403796715385
Validation loss: 2.505971975911168

Epoch: 5| Step: 2
Training loss: 2.426869220594136
Validation loss: 2.5039250992548023

Epoch: 5| Step: 3
Training loss: 2.755501792196938
Validation loss: 2.4933123147897933

Epoch: 5| Step: 4
Training loss: 2.9557386669840895
Validation loss: 2.500638590156577

Epoch: 5| Step: 5
Training loss: 2.5840508069691754
Validation loss: 2.491105944131816

Epoch: 5| Step: 6
Training loss: 1.933928243045555
Validation loss: 2.5006823125206723

Epoch: 5| Step: 7
Training loss: 1.570985412656198
Validation loss: 2.5013779478600475

Epoch: 5| Step: 8
Training loss: 1.9558950083819624
Validation loss: 2.511160982888828

Epoch: 5| Step: 9
Training loss: 2.3877508301316603
Validation loss: 2.5039841297116965

Epoch: 5| Step: 10
Training loss: 2.2608287687591844
Validation loss: 2.4971779830051757

Epoch: 5| Step: 11
Training loss: 2.7784172805464027
Validation loss: 2.5126060196189512

Epoch: 205| Step: 0
Training loss: 2.08662751747107
Validation loss: 2.5243353845570433

Epoch: 5| Step: 1
Training loss: 1.994407284773289
Validation loss: 2.517222274517971

Epoch: 5| Step: 2
Training loss: 2.137727656148077
Validation loss: 2.519528107554918

Epoch: 5| Step: 3
Training loss: 2.461035444944504
Validation loss: 2.5352895963187465

Epoch: 5| Step: 4
Training loss: 1.9295257006315265
Validation loss: 2.5521590059440653

Epoch: 5| Step: 5
Training loss: 2.1545774081779494
Validation loss: 2.5501018474223374

Epoch: 5| Step: 6
Training loss: 2.934714112273352
Validation loss: 2.529699659249589

Epoch: 5| Step: 7
Training loss: 2.738120777690238
Validation loss: 2.519192667446376

Epoch: 5| Step: 8
Training loss: 2.1772533757972807
Validation loss: 2.5217449901588154

Epoch: 5| Step: 9
Training loss: 2.6707837291124212
Validation loss: 2.489816380743268

Epoch: 5| Step: 10
Training loss: 2.927939908458496
Validation loss: 2.4881177136843857

Epoch: 5| Step: 11
Training loss: 2.465853380038177
Validation loss: 2.4866118967115685

Epoch: 206| Step: 0
Training loss: 2.116615133584342
Validation loss: 2.4811102050233025

Epoch: 5| Step: 1
Training loss: 1.8903757987237328
Validation loss: 2.4874241189549067

Epoch: 5| Step: 2
Training loss: 2.24607761543118
Validation loss: 2.4914817847253907

Epoch: 5| Step: 3
Training loss: 3.0583534974586377
Validation loss: 2.4879148963147184

Epoch: 5| Step: 4
Training loss: 2.7497019172721426
Validation loss: 2.4775009904160816

Epoch: 5| Step: 5
Training loss: 2.5729603216712325
Validation loss: 2.478099843234651

Epoch: 5| Step: 6
Training loss: 2.5854515702450356
Validation loss: 2.4871230251919654

Epoch: 5| Step: 7
Training loss: 2.4717463405609434
Validation loss: 2.4867201838367867

Epoch: 5| Step: 8
Training loss: 2.0615333835175753
Validation loss: 2.4909726074613596

Epoch: 5| Step: 9
Training loss: 2.762310736201219
Validation loss: 2.5013329608102914

Epoch: 5| Step: 10
Training loss: 2.460072004923673
Validation loss: 2.4928906882686546

Epoch: 5| Step: 11
Training loss: 2.0822197290245246
Validation loss: 2.496725711807631

Epoch: 207| Step: 0
Training loss: 2.2959652091457454
Validation loss: 2.493450327166644

Epoch: 5| Step: 1
Training loss: 2.156712247532908
Validation loss: 2.508281794575125

Epoch: 5| Step: 2
Training loss: 2.3083952481152346
Validation loss: 2.512633256557353

Epoch: 5| Step: 3
Training loss: 2.217700656383648
Validation loss: 2.5218020829220507

Epoch: 5| Step: 4
Training loss: 2.7386236697737014
Validation loss: 2.5424024983755795

Epoch: 5| Step: 5
Training loss: 2.677619158090621
Validation loss: 2.5372269045003244

Epoch: 5| Step: 6
Training loss: 2.464259639151855
Validation loss: 2.53321122300798

Epoch: 5| Step: 7
Training loss: 2.6716370058307084
Validation loss: 2.531146883335903

Epoch: 5| Step: 8
Training loss: 2.2831590581317376
Validation loss: 2.505601476697022

Epoch: 5| Step: 9
Training loss: 2.287119590938435
Validation loss: 2.5118990547165487

Epoch: 5| Step: 10
Training loss: 2.395673917912364
Validation loss: 2.507286000664689

Epoch: 5| Step: 11
Training loss: 3.565469140299729
Validation loss: 2.495013775956289

Epoch: 208| Step: 0
Training loss: 2.7088353694319083
Validation loss: 2.4905375417034303

Epoch: 5| Step: 1
Training loss: 2.533695499566095
Validation loss: 2.4896768980653197

Epoch: 5| Step: 2
Training loss: 2.20114782907507
Validation loss: 2.4899492467070155

Epoch: 5| Step: 3
Training loss: 2.0683911891143936
Validation loss: 2.483854889158012

Epoch: 5| Step: 4
Training loss: 2.048064606899207
Validation loss: 2.4828159810788635

Epoch: 5| Step: 5
Training loss: 2.6828823784785065
Validation loss: 2.482281720686717

Epoch: 5| Step: 6
Training loss: 2.4038841209347686
Validation loss: 2.488584363232317

Epoch: 5| Step: 7
Training loss: 2.101303141306032
Validation loss: 2.4841406409768516

Epoch: 5| Step: 8
Training loss: 2.503129240453893
Validation loss: 2.4791329779593303

Epoch: 5| Step: 9
Training loss: 2.5138873620728224
Validation loss: 2.4953109675916068

Epoch: 5| Step: 10
Training loss: 2.6884200716856066
Validation loss: 2.485503394304975

Epoch: 5| Step: 11
Training loss: 2.1147712912123007
Validation loss: 2.4948052397314275

Epoch: 209| Step: 0
Training loss: 2.199016229712741
Validation loss: 2.499436422243387

Epoch: 5| Step: 1
Training loss: 1.701425947274473
Validation loss: 2.4842600445972502

Epoch: 5| Step: 2
Training loss: 2.296313366376445
Validation loss: 2.4945920109081547

Epoch: 5| Step: 3
Training loss: 2.568292351490026
Validation loss: 2.4962072689886123

Epoch: 5| Step: 4
Training loss: 2.0166133852559
Validation loss: 2.500469223334209

Epoch: 5| Step: 5
Training loss: 2.4230532825513778
Validation loss: 2.5022613867540864

Epoch: 5| Step: 6
Training loss: 2.695498606573023
Validation loss: 2.506536355152754

Epoch: 5| Step: 7
Training loss: 2.7893456066924047
Validation loss: 2.4888460764852383

Epoch: 5| Step: 8
Training loss: 2.6569108028866624
Validation loss: 2.4970732246176066

Epoch: 5| Step: 9
Training loss: 2.6212676625891427
Validation loss: 2.4926258885455814

Epoch: 5| Step: 10
Training loss: 2.0850664813376816
Validation loss: 2.505932252596719

Epoch: 5| Step: 11
Training loss: 2.7824657226260867
Validation loss: 2.499871731487524

Epoch: 210| Step: 0
Training loss: 2.594152028742722
Validation loss: 2.5035217831085967

Epoch: 5| Step: 1
Training loss: 2.8078850925170604
Validation loss: 2.5043808461999975

Epoch: 5| Step: 2
Training loss: 2.7536167290187703
Validation loss: 2.5157694212406385

Epoch: 5| Step: 3
Training loss: 2.9993899042928103
Validation loss: 2.519622020417064

Epoch: 5| Step: 4
Training loss: 2.3686381009125226
Validation loss: 2.5333267373911

Epoch: 5| Step: 5
Training loss: 1.8664477810364632
Validation loss: 2.5253510124166096

Epoch: 5| Step: 6
Training loss: 2.094600675485925
Validation loss: 2.524243402277069

Epoch: 5| Step: 7
Training loss: 2.3746286403039543
Validation loss: 2.510054634761059

Epoch: 5| Step: 8
Training loss: 1.9625436960203109
Validation loss: 2.518071866308475

Epoch: 5| Step: 9
Training loss: 2.631821805388774
Validation loss: 2.5087063385820247

Epoch: 5| Step: 10
Training loss: 1.7244283337034403
Validation loss: 2.510723049150721

Epoch: 5| Step: 11
Training loss: 1.75823847377714
Validation loss: 2.501451095016079

Epoch: 211| Step: 0
Training loss: 2.6889159442479253
Validation loss: 2.496653454770662

Epoch: 5| Step: 1
Training loss: 2.148580650415602
Validation loss: 2.494235537561971

Epoch: 5| Step: 2
Training loss: 2.4939528762782746
Validation loss: 2.486160516395982

Epoch: 5| Step: 3
Training loss: 2.297352228736033
Validation loss: 2.4971213914254107

Epoch: 5| Step: 4
Training loss: 2.7329197962326632
Validation loss: 2.495727018619196

Epoch: 5| Step: 5
Training loss: 2.2734658151023437
Validation loss: 2.481376421804195

Epoch: 5| Step: 6
Training loss: 2.1742927760723307
Validation loss: 2.502633073119153

Epoch: 5| Step: 7
Training loss: 2.531010086557945
Validation loss: 2.496668157018956

Epoch: 5| Step: 8
Training loss: 2.385380796026752
Validation loss: 2.4996282897224034

Epoch: 5| Step: 9
Training loss: 2.2788672348726298
Validation loss: 2.4951682924734135

Epoch: 5| Step: 10
Training loss: 2.3159162701112526
Validation loss: 2.491087584094014

Epoch: 5| Step: 11
Training loss: 2.319874320736723
Validation loss: 2.5044408853304008

Epoch: 212| Step: 0
Training loss: 2.2362829212134447
Validation loss: 2.5285525646877436

Epoch: 5| Step: 1
Training loss: 2.6774316306338712
Validation loss: 2.545056518833942

Epoch: 5| Step: 2
Training loss: 2.3473013865145864
Validation loss: 2.589451011618105

Epoch: 5| Step: 3
Training loss: 2.730722483795873
Validation loss: 2.6034599655012998

Epoch: 5| Step: 4
Training loss: 2.621228096669904
Validation loss: 2.5929686813454094

Epoch: 5| Step: 5
Training loss: 2.575976592117484
Validation loss: 2.5492089270869043

Epoch: 5| Step: 6
Training loss: 2.944091381841982
Validation loss: 2.5204437279243033

Epoch: 5| Step: 7
Training loss: 2.269394993590224
Validation loss: 2.4994339262316676

Epoch: 5| Step: 8
Training loss: 1.8416127247340093
Validation loss: 2.4825628217115896

Epoch: 5| Step: 9
Training loss: 2.1653408860503958
Validation loss: 2.4705659046692148

Epoch: 5| Step: 10
Training loss: 2.269099865884043
Validation loss: 2.4728536019857796

Epoch: 5| Step: 11
Training loss: 2.6600904638639533
Validation loss: 2.464811765178107

Epoch: 213| Step: 0
Training loss: 2.1448467418632835
Validation loss: 2.469505114096751

Epoch: 5| Step: 1
Training loss: 2.960690450301227
Validation loss: 2.472434067549366

Epoch: 5| Step: 2
Training loss: 2.4742249248151755
Validation loss: 2.467941924526279

Epoch: 5| Step: 3
Training loss: 2.891327973537683
Validation loss: 2.469731564288582

Epoch: 5| Step: 4
Training loss: 2.7392907995729923
Validation loss: 2.469058263018887

Epoch: 5| Step: 5
Training loss: 2.1951531729118963
Validation loss: 2.4672837953496436

Epoch: 5| Step: 6
Training loss: 2.491778109895883
Validation loss: 2.4791246111425567

Epoch: 5| Step: 7
Training loss: 1.9462182567558264
Validation loss: 2.489592517585786

Epoch: 5| Step: 8
Training loss: 2.0524565329926525
Validation loss: 2.489566572816579

Epoch: 5| Step: 9
Training loss: 2.8236117099707747
Validation loss: 2.504547493286375

Epoch: 5| Step: 10
Training loss: 2.0583788996709513
Validation loss: 2.5053887583158754

Epoch: 5| Step: 11
Training loss: 2.5416201811328145
Validation loss: 2.50599338632027

Epoch: 214| Step: 0
Training loss: 2.688632460347814
Validation loss: 2.5207891276453274

Epoch: 5| Step: 1
Training loss: 2.50007362257317
Validation loss: 2.5300895401284524

Epoch: 5| Step: 2
Training loss: 2.7864507847383635
Validation loss: 2.530450942049554

Epoch: 5| Step: 3
Training loss: 2.363846136001136
Validation loss: 2.539222884370116

Epoch: 5| Step: 4
Training loss: 2.737485413534243
Validation loss: 2.540444116538477

Epoch: 5| Step: 5
Training loss: 2.172054036397352
Validation loss: 2.5090206401179205

Epoch: 5| Step: 6
Training loss: 2.157646721970734
Validation loss: 2.5050949671123766

Epoch: 5| Step: 7
Training loss: 2.34864428352838
Validation loss: 2.495546968575901

Epoch: 5| Step: 8
Training loss: 2.1448061685193838
Validation loss: 2.4719358402164415

Epoch: 5| Step: 9
Training loss: 2.8565596598373295
Validation loss: 2.4810793387963064

Epoch: 5| Step: 10
Training loss: 2.2022985978114136
Validation loss: 2.4789984723479197

Epoch: 5| Step: 11
Training loss: 1.4849360158731362
Validation loss: 2.4832951560115437

Epoch: 215| Step: 0
Training loss: 2.3161930803118342
Validation loss: 2.4759994933875578

Epoch: 5| Step: 1
Training loss: 3.3489623042548304
Validation loss: 2.47120419775132

Epoch: 5| Step: 2
Training loss: 1.7863749344676052
Validation loss: 2.471767319987528

Epoch: 5| Step: 3
Training loss: 2.35372663003155
Validation loss: 2.4768474988479894

Epoch: 5| Step: 4
Training loss: 3.156394275351571
Validation loss: 2.490397923901993

Epoch: 5| Step: 5
Training loss: 2.3260856759782014
Validation loss: 2.484249951556377

Epoch: 5| Step: 6
Training loss: 2.3626002820242045
Validation loss: 2.482635312792878

Epoch: 5| Step: 7
Training loss: 2.2422979772910696
Validation loss: 2.487213772132008

Epoch: 5| Step: 8
Training loss: 2.0226942192159107
Validation loss: 2.4898420456145427

Epoch: 5| Step: 9
Training loss: 2.2192762583556105
Validation loss: 2.4981788240509717

Epoch: 5| Step: 10
Training loss: 2.019492291013847
Validation loss: 2.4973438696415817

Epoch: 5| Step: 11
Training loss: 2.337285600101201
Validation loss: 2.5052600400562537

Epoch: 216| Step: 0
Training loss: 2.1540983217515746
Validation loss: 2.512131834495738

Epoch: 5| Step: 1
Training loss: 2.631789554902045
Validation loss: 2.539749015528418

Epoch: 5| Step: 2
Training loss: 2.2394298648740802
Validation loss: 2.5745583610745566

Epoch: 5| Step: 3
Training loss: 2.4141602727849016
Validation loss: 2.587514363892472

Epoch: 5| Step: 4
Training loss: 2.458451532227108
Validation loss: 2.545665797159465

Epoch: 5| Step: 5
Training loss: 2.298923539067048
Validation loss: 2.5307167849512924

Epoch: 5| Step: 6
Training loss: 2.4228334560740206
Validation loss: 2.5193946781650154

Epoch: 5| Step: 7
Training loss: 2.0273330482391554
Validation loss: 2.513270281071196

Epoch: 5| Step: 8
Training loss: 2.4891670123690592
Validation loss: 2.512519012841046

Epoch: 5| Step: 9
Training loss: 2.3783351168751685
Validation loss: 2.487946332572466

Epoch: 5| Step: 10
Training loss: 2.9368775702556786
Validation loss: 2.4986481309588813

Epoch: 5| Step: 11
Training loss: 1.5211519752334386
Validation loss: 2.4820411083973024

Epoch: 217| Step: 0
Training loss: 2.3754391515185436
Validation loss: 2.486090429730292

Epoch: 5| Step: 1
Training loss: 2.4378749852411525
Validation loss: 2.4750658801972687

Epoch: 5| Step: 2
Training loss: 2.1813214921575494
Validation loss: 2.481456261652831

Epoch: 5| Step: 3
Training loss: 2.6709966275369883
Validation loss: 2.4918883930436717

Epoch: 5| Step: 4
Training loss: 2.3520664422842477
Validation loss: 2.4817129423922184

Epoch: 5| Step: 5
Training loss: 2.2415504746890296
Validation loss: 2.486525216872409

Epoch: 5| Step: 6
Training loss: 2.5085607820552456
Validation loss: 2.4775627675399097

Epoch: 5| Step: 7
Training loss: 2.7112252470217024
Validation loss: 2.4898864463143306

Epoch: 5| Step: 8
Training loss: 2.438163275830176
Validation loss: 2.4901870304169744

Epoch: 5| Step: 9
Training loss: 2.647422303382577
Validation loss: 2.492793895327941

Epoch: 5| Step: 10
Training loss: 2.230600650448167
Validation loss: 2.5037834942780655

Epoch: 5| Step: 11
Training loss: 1.3232874563433992
Validation loss: 2.5034533570652115

Epoch: 218| Step: 0
Training loss: 2.26681729218279
Validation loss: 2.516361065703365

Epoch: 5| Step: 1
Training loss: 2.112168447445544
Validation loss: 2.532034913173883

Epoch: 5| Step: 2
Training loss: 2.65501051762396
Validation loss: 2.5383181219298483

Epoch: 5| Step: 3
Training loss: 2.7858930257947585
Validation loss: 2.537678898140944

Epoch: 5| Step: 4
Training loss: 2.3563605244666292
Validation loss: 2.527128321463928

Epoch: 5| Step: 5
Training loss: 1.5653654907542087
Validation loss: 2.531669448510308

Epoch: 5| Step: 6
Training loss: 2.1590282951292465
Validation loss: 2.5228404312102826

Epoch: 5| Step: 7
Training loss: 2.629328202143312
Validation loss: 2.5206532939480324

Epoch: 5| Step: 8
Training loss: 2.321312480161643
Validation loss: 2.5041582295174627

Epoch: 5| Step: 9
Training loss: 2.7009374333365868
Validation loss: 2.5035047085177258

Epoch: 5| Step: 10
Training loss: 2.552756510071639
Validation loss: 2.509346505680672

Epoch: 5| Step: 11
Training loss: 1.5913916698119372
Validation loss: 2.5024526883002793

Epoch: 219| Step: 0
Training loss: 1.8204336494438866
Validation loss: 2.492916443030056

Epoch: 5| Step: 1
Training loss: 2.360619425222911
Validation loss: 2.4932527846336083

Epoch: 5| Step: 2
Training loss: 2.764700697415375
Validation loss: 2.4933419458623876

Epoch: 5| Step: 3
Training loss: 2.1155541232025863
Validation loss: 2.496473897323941

Epoch: 5| Step: 4
Training loss: 2.6904302192036678
Validation loss: 2.4897408927688094

Epoch: 5| Step: 5
Training loss: 1.9479881790280285
Validation loss: 2.5040710601285823

Epoch: 5| Step: 6
Training loss: 1.917139996948204
Validation loss: 2.5033568574517155

Epoch: 5| Step: 7
Training loss: 2.034829140878506
Validation loss: 2.4989795390272604

Epoch: 5| Step: 8
Training loss: 2.2929207116055
Validation loss: 2.4961520224521574

Epoch: 5| Step: 9
Training loss: 2.7610665378089854
Validation loss: 2.501973779669025

Epoch: 5| Step: 10
Training loss: 3.1545215066106214
Validation loss: 2.510076117203637

Epoch: 5| Step: 11
Training loss: 2.572848567499016
Validation loss: 2.5152024058287905

Epoch: 220| Step: 0
Training loss: 2.4107785897534173
Validation loss: 2.516885982414

Epoch: 5| Step: 1
Training loss: 2.360065081394055
Validation loss: 2.4915924537785044

Epoch: 5| Step: 2
Training loss: 1.929618988674196
Validation loss: 2.5087165787400982

Epoch: 5| Step: 3
Training loss: 2.2400879284086046
Validation loss: 2.4969649408889065

Epoch: 5| Step: 4
Training loss: 2.005202441186216
Validation loss: 2.486831566123811

Epoch: 5| Step: 5
Training loss: 2.9327670988573464
Validation loss: 2.490264726840281

Epoch: 5| Step: 6
Training loss: 2.4981164512459375
Validation loss: 2.4894149289706813

Epoch: 5| Step: 7
Training loss: 3.0801165184671153
Validation loss: 2.4926807868052907

Epoch: 5| Step: 8
Training loss: 2.263257598490018
Validation loss: 2.4901947377360156

Epoch: 5| Step: 9
Training loss: 1.7891236007451488
Validation loss: 2.488972837753311

Epoch: 5| Step: 10
Training loss: 2.7392068079587526
Validation loss: 2.484741915592852

Epoch: 5| Step: 11
Training loss: 1.9487214151553032
Validation loss: 2.4931594444159413

Epoch: 221| Step: 0
Training loss: 2.7461995827479315
Validation loss: 2.4909049573584836

Epoch: 5| Step: 1
Training loss: 2.0658415111006985
Validation loss: 2.4956196178954952

Epoch: 5| Step: 2
Training loss: 2.1655041681574128
Validation loss: 2.496659037258356

Epoch: 5| Step: 3
Training loss: 1.9951719182256868
Validation loss: 2.4949771490942685

Epoch: 5| Step: 4
Training loss: 2.7905920626927485
Validation loss: 2.4904895606841704

Epoch: 5| Step: 5
Training loss: 1.9217069598911833
Validation loss: 2.4943990790061092

Epoch: 5| Step: 6
Training loss: 2.677043533524119
Validation loss: 2.49840846065705

Epoch: 5| Step: 7
Training loss: 2.081248295413857
Validation loss: 2.494078023747736

Epoch: 5| Step: 8
Training loss: 2.693553848382646
Validation loss: 2.5092645483859246

Epoch: 5| Step: 9
Training loss: 2.9686655132920534
Validation loss: 2.524181665659542

Epoch: 5| Step: 10
Training loss: 1.8748307469588905
Validation loss: 2.5275117483221305

Epoch: 5| Step: 11
Training loss: 1.9479747158383582
Validation loss: 2.531885942391359

Epoch: 222| Step: 0
Training loss: 2.580021755806918
Validation loss: 2.554805017970592

Epoch: 5| Step: 1
Training loss: 1.9286271243656372
Validation loss: 2.588121392859302

Epoch: 5| Step: 2
Training loss: 2.2731797114771606
Validation loss: 2.601390943703488

Epoch: 5| Step: 3
Training loss: 2.914908196915943
Validation loss: 2.5949709136768524

Epoch: 5| Step: 4
Training loss: 2.7216768843051584
Validation loss: 2.555792150711875

Epoch: 5| Step: 5
Training loss: 2.832405181402071
Validation loss: 2.5457994807396833

Epoch: 5| Step: 6
Training loss: 1.827636262168491
Validation loss: 2.5172786924236252

Epoch: 5| Step: 7
Training loss: 2.014812335227602
Validation loss: 2.5100278249847294

Epoch: 5| Step: 8
Training loss: 2.3604357021425035
Validation loss: 2.502365755255765

Epoch: 5| Step: 9
Training loss: 2.8996694573754667
Validation loss: 2.506364929468674

Epoch: 5| Step: 10
Training loss: 2.003158816618989
Validation loss: 2.496316254140394

Epoch: 5| Step: 11
Training loss: 1.3462736694748723
Validation loss: 2.4873861143025957

Epoch: 223| Step: 0
Training loss: 2.1222836579866686
Validation loss: 2.480651700539394

Epoch: 5| Step: 1
Training loss: 3.2490488274387404
Validation loss: 2.490730369888463

Epoch: 5| Step: 2
Training loss: 2.559841917342684
Validation loss: 2.487393067488038

Epoch: 5| Step: 3
Training loss: 2.1275046678216203
Validation loss: 2.4885532863234423

Epoch: 5| Step: 4
Training loss: 2.0742330029818685
Validation loss: 2.4917514144451007

Epoch: 5| Step: 5
Training loss: 2.339709142720111
Validation loss: 2.494822334042659

Epoch: 5| Step: 6
Training loss: 2.9583819389827632
Validation loss: 2.4917872315733636

Epoch: 5| Step: 7
Training loss: 2.489243828729357
Validation loss: 2.487843629210786

Epoch: 5| Step: 8
Training loss: 1.77825408091587
Validation loss: 2.506311258437828

Epoch: 5| Step: 9
Training loss: 2.458151654356714
Validation loss: 2.5210838285364625

Epoch: 5| Step: 10
Training loss: 2.0073229716192005
Validation loss: 2.5204189698224595

Epoch: 5| Step: 11
Training loss: 1.544824647797881
Validation loss: 2.51439623509321

Epoch: 224| Step: 0
Training loss: 2.8509982452226614
Validation loss: 2.527352220539754

Epoch: 5| Step: 1
Training loss: 2.433146378986416
Validation loss: 2.497100525555008

Epoch: 5| Step: 2
Training loss: 2.1218080670189283
Validation loss: 2.515120769695294

Epoch: 5| Step: 3
Training loss: 2.473890337093255
Validation loss: 2.5185448186709882

Epoch: 5| Step: 4
Training loss: 2.3005265255190386
Validation loss: 2.5165778107040992

Epoch: 5| Step: 5
Training loss: 2.488875529228791
Validation loss: 2.525418719267933

Epoch: 5| Step: 6
Training loss: 2.0587261237014722
Validation loss: 2.5274743071475556

Epoch: 5| Step: 7
Training loss: 2.060853705680202
Validation loss: 2.520998934883892

Epoch: 5| Step: 8
Training loss: 2.5164621038435224
Validation loss: 2.5222836150410584

Epoch: 5| Step: 9
Training loss: 2.133954982286309
Validation loss: 2.5108534893231713

Epoch: 5| Step: 10
Training loss: 2.245352501668833
Validation loss: 2.5193245046399415

Epoch: 5| Step: 11
Training loss: 3.191257281894646
Validation loss: 2.5187600032270145

Epoch: 225| Step: 0
Training loss: 1.962463696866871
Validation loss: 2.5272517357043958

Epoch: 5| Step: 1
Training loss: 2.559878427181784
Validation loss: 2.5418835974708056

Epoch: 5| Step: 2
Training loss: 2.1990793295754734
Validation loss: 2.536962566087782

Epoch: 5| Step: 3
Training loss: 1.902467382905849
Validation loss: 2.5184227685891556

Epoch: 5| Step: 4
Training loss: 2.4145592237118585
Validation loss: 2.519580665123508

Epoch: 5| Step: 5
Training loss: 2.1996684995064477
Validation loss: 2.5324420241472683

Epoch: 5| Step: 6
Training loss: 2.077519034471773
Validation loss: 2.52575000114709

Epoch: 5| Step: 7
Training loss: 3.330784124895775
Validation loss: 2.5259792451257552

Epoch: 5| Step: 8
Training loss: 2.3818555567560162
Validation loss: 2.521730623205272

Epoch: 5| Step: 9
Training loss: 2.198192621296752
Validation loss: 2.5198916569288916

Epoch: 5| Step: 10
Training loss: 2.639287666395296
Validation loss: 2.5174279830352306

Epoch: 5| Step: 11
Training loss: 1.6039871842022182
Validation loss: 2.5096802772427855

Epoch: 226| Step: 0
Training loss: 1.884494394344131
Validation loss: 2.5193876122036194

Epoch: 5| Step: 1
Training loss: 2.9299448129189205
Validation loss: 2.5262274893352794

Epoch: 5| Step: 2
Training loss: 2.5056990514951747
Validation loss: 2.515782017674637

Epoch: 5| Step: 3
Training loss: 2.016689876811658
Validation loss: 2.5278260302974394

Epoch: 5| Step: 4
Training loss: 2.531624001082156
Validation loss: 2.5398642775489186

Epoch: 5| Step: 5
Training loss: 2.2454942516864054
Validation loss: 2.5409195706125067

Epoch: 5| Step: 6
Training loss: 2.8632199643850496
Validation loss: 2.5445837858541247

Epoch: 5| Step: 7
Training loss: 2.3452664365063356
Validation loss: 2.5306851263009458

Epoch: 5| Step: 8
Training loss: 2.14620389485591
Validation loss: 2.5223345870724905

Epoch: 5| Step: 9
Training loss: 2.346591498550989
Validation loss: 2.5206987972982002

Epoch: 5| Step: 10
Training loss: 2.085999334326077
Validation loss: 2.5012908185989504

Epoch: 5| Step: 11
Training loss: 2.0293363506747526
Validation loss: 2.507108841176106

Epoch: 227| Step: 0
Training loss: 2.1353469713204096
Validation loss: 2.5017058751360657

Epoch: 5| Step: 1
Training loss: 2.9864044479087655
Validation loss: 2.497590437630471

Epoch: 5| Step: 2
Training loss: 2.3046641203130873
Validation loss: 2.4860767757867315

Epoch: 5| Step: 3
Training loss: 2.3593619643097563
Validation loss: 2.4906646835439568

Epoch: 5| Step: 4
Training loss: 1.7336802422778543
Validation loss: 2.4912324189431687

Epoch: 5| Step: 5
Training loss: 2.3807427093737124
Validation loss: 2.4971891416447938

Epoch: 5| Step: 6
Training loss: 2.7925584611775345
Validation loss: 2.504212383058189

Epoch: 5| Step: 7
Training loss: 2.275718148034689
Validation loss: 2.5021261075503616

Epoch: 5| Step: 8
Training loss: 2.181542704270682
Validation loss: 2.513063334894586

Epoch: 5| Step: 9
Training loss: 2.1388549168959043
Validation loss: 2.5100232854208846

Epoch: 5| Step: 10
Training loss: 2.783109493353765
Validation loss: 2.52945473668051

Epoch: 5| Step: 11
Training loss: 2.069584556575644
Validation loss: 2.531830599409736

Epoch: 228| Step: 0
Training loss: 2.156676761578022
Validation loss: 2.5568103427959454

Epoch: 5| Step: 1
Training loss: 2.8941427734630643
Validation loss: 2.5760221902985387

Epoch: 5| Step: 2
Training loss: 1.6726710483229592
Validation loss: 2.5499833725094194

Epoch: 5| Step: 3
Training loss: 2.6102875353106776
Validation loss: 2.587591658303325

Epoch: 5| Step: 4
Training loss: 2.2389869309828088
Validation loss: 2.57028201702559

Epoch: 5| Step: 5
Training loss: 2.0849539620419026
Validation loss: 2.5790748657400964

Epoch: 5| Step: 6
Training loss: 2.1577660579779994
Validation loss: 2.5340874000361215

Epoch: 5| Step: 7
Training loss: 2.236610841163382
Validation loss: 2.53938204197334

Epoch: 5| Step: 8
Training loss: 2.481558300630124
Validation loss: 2.527978788815262

Epoch: 5| Step: 9
Training loss: 2.8679095198485753
Validation loss: 2.5115569929260553

Epoch: 5| Step: 10
Training loss: 2.2670209070225176
Validation loss: 2.4992703366398685

Epoch: 5| Step: 11
Training loss: 3.400934954727035
Validation loss: 2.4907331159220365

Epoch: 229| Step: 0
Training loss: 3.0128774825899667
Validation loss: 2.4857976187164

Epoch: 5| Step: 1
Training loss: 2.0681229439128894
Validation loss: 2.4882337365038154

Epoch: 5| Step: 2
Training loss: 2.6295453636665176
Validation loss: 2.4845044364239603

Epoch: 5| Step: 3
Training loss: 2.8998766511137686
Validation loss: 2.4829994288136725

Epoch: 5| Step: 4
Training loss: 1.948164599410554
Validation loss: 2.4736447399889476

Epoch: 5| Step: 5
Training loss: 2.348256369259763
Validation loss: 2.4907265888525325

Epoch: 5| Step: 6
Training loss: 2.1691990016540337
Validation loss: 2.4842857168856707

Epoch: 5| Step: 7
Training loss: 2.458287534987576
Validation loss: 2.4875377220859094

Epoch: 5| Step: 8
Training loss: 2.7150294837264037
Validation loss: 2.4973434877661016

Epoch: 5| Step: 9
Training loss: 2.083247017661775
Validation loss: 2.511529625653797

Epoch: 5| Step: 10
Training loss: 1.8254634033689048
Validation loss: 2.5218503348126373

Epoch: 5| Step: 11
Training loss: 1.3061518874123035
Validation loss: 2.5362244301690375

Epoch: 230| Step: 0
Training loss: 2.3570729323745403
Validation loss: 2.5356852817523663

Epoch: 5| Step: 1
Training loss: 1.9483617454308504
Validation loss: 2.5616750086952775

Epoch: 5| Step: 2
Training loss: 2.0285016760740437
Validation loss: 2.5794922160758094

Epoch: 5| Step: 3
Training loss: 2.005447241352387
Validation loss: 2.578382275007331

Epoch: 5| Step: 4
Training loss: 2.983810131680048
Validation loss: 2.586710469562838

Epoch: 5| Step: 5
Training loss: 2.550188401686087
Validation loss: 2.588589509845502

Epoch: 5| Step: 6
Training loss: 2.535175529136295
Validation loss: 2.5807355129396155

Epoch: 5| Step: 7
Training loss: 1.9780716393190416
Validation loss: 2.5627102610535832

Epoch: 5| Step: 8
Training loss: 2.6015245589505307
Validation loss: 2.537245584521472

Epoch: 5| Step: 9
Training loss: 2.8703446221877384
Validation loss: 2.5302522246908166

Epoch: 5| Step: 10
Training loss: 2.1640399146708957
Validation loss: 2.5426700688136656

Epoch: 5| Step: 11
Training loss: 2.0878780639384784
Validation loss: 2.513652395367078

Epoch: 231| Step: 0
Training loss: 2.388406359808815
Validation loss: 2.5052032523523358

Epoch: 5| Step: 1
Training loss: 1.9980795103448796
Validation loss: 2.5044416072509335

Epoch: 5| Step: 2
Training loss: 2.618154682966461
Validation loss: 2.5004857783425156

Epoch: 5| Step: 3
Training loss: 2.0510147978994184
Validation loss: 2.497448278571899

Epoch: 5| Step: 4
Training loss: 2.3893742499523696
Validation loss: 2.5009797401710907

Epoch: 5| Step: 5
Training loss: 2.049162074711285
Validation loss: 2.502420616497737

Epoch: 5| Step: 6
Training loss: 2.2160761665723054
Validation loss: 2.5053463175491553

Epoch: 5| Step: 7
Training loss: 2.3256788267751114
Validation loss: 2.5033974886947434

Epoch: 5| Step: 8
Training loss: 2.7990795802517
Validation loss: 2.4962286119240424

Epoch: 5| Step: 9
Training loss: 2.379642767182052
Validation loss: 2.535488751456885

Epoch: 5| Step: 10
Training loss: 2.7211113683024504
Validation loss: 2.5394582860534607

Epoch: 5| Step: 11
Training loss: 3.024974816587935
Validation loss: 2.541864154261961

Epoch: 232| Step: 0
Training loss: 2.5451289083213733
Validation loss: 2.545779610882301

Epoch: 5| Step: 1
Training loss: 2.490215226642067
Validation loss: 2.538656345063035

Epoch: 5| Step: 2
Training loss: 1.9297371058709702
Validation loss: 2.522772454335712

Epoch: 5| Step: 3
Training loss: 2.174336746638101
Validation loss: 2.514271152674011

Epoch: 5| Step: 4
Training loss: 2.6134350650150644
Validation loss: 2.499668234428073

Epoch: 5| Step: 5
Training loss: 1.9484056141316073
Validation loss: 2.4973885446918773

Epoch: 5| Step: 6
Training loss: 2.5877021231465585
Validation loss: 2.4947815571675047

Epoch: 5| Step: 7
Training loss: 2.84528720096609
Validation loss: 2.500390749435052

Epoch: 5| Step: 8
Training loss: 2.2944753139329173
Validation loss: 2.4956739547249946

Epoch: 5| Step: 9
Training loss: 1.7840536706745713
Validation loss: 2.5023792984861375

Epoch: 5| Step: 10
Training loss: 2.787389002073994
Validation loss: 2.5012590814031923

Epoch: 5| Step: 11
Training loss: 3.629661194093897
Validation loss: 2.501186216584228

Epoch: 233| Step: 0
Training loss: 2.77044593760766
Validation loss: 2.5132221253309974

Epoch: 5| Step: 1
Training loss: 2.6946452766197413
Validation loss: 2.5303822156245483

Epoch: 5| Step: 2
Training loss: 2.359790992695184
Validation loss: 2.5799287323941016

Epoch: 5| Step: 3
Training loss: 2.2868068374438155
Validation loss: 2.592981033012596

Epoch: 5| Step: 4
Training loss: 2.545720968144417
Validation loss: 2.5891909901043575

Epoch: 5| Step: 5
Training loss: 1.6696861413018083
Validation loss: 2.6004998191281388

Epoch: 5| Step: 6
Training loss: 2.34361510842293
Validation loss: 2.5933928990714685

Epoch: 5| Step: 7
Training loss: 2.307165436632091
Validation loss: 2.5616615133124068

Epoch: 5| Step: 8
Training loss: 2.622553093648231
Validation loss: 2.5529600402196593

Epoch: 5| Step: 9
Training loss: 1.8023123035242294
Validation loss: 2.5330690746971736

Epoch: 5| Step: 10
Training loss: 2.7352695418743145
Validation loss: 2.5216865724415394

Epoch: 5| Step: 11
Training loss: 2.246029529482716
Validation loss: 2.506496539717403

Epoch: 234| Step: 0
Training loss: 2.135054866255522
Validation loss: 2.4964877470842577

Epoch: 5| Step: 1
Training loss: 2.2115859304073364
Validation loss: 2.500106129777781

Epoch: 5| Step: 2
Training loss: 2.5734233177178742
Validation loss: 2.5000001827875704

Epoch: 5| Step: 3
Training loss: 1.9279015805778394
Validation loss: 2.4869368517939066

Epoch: 5| Step: 4
Training loss: 2.468115906224097
Validation loss: 2.484294450201817

Epoch: 5| Step: 5
Training loss: 2.6109586923721957
Validation loss: 2.4908977627219757

Epoch: 5| Step: 6
Training loss: 2.3671194135990064
Validation loss: 2.493202369556028

Epoch: 5| Step: 7
Training loss: 2.8129267898563506
Validation loss: 2.499898868740525

Epoch: 5| Step: 8
Training loss: 1.9607429559871241
Validation loss: 2.5052203470895056

Epoch: 5| Step: 9
Training loss: 2.349708206241784
Validation loss: 2.513043933557996

Epoch: 5| Step: 10
Training loss: 2.4674980755675753
Validation loss: 2.510110723021475

Epoch: 5| Step: 11
Training loss: 3.154344946905361
Validation loss: 2.5295607106426616

Epoch: 235| Step: 0
Training loss: 2.3934893433807973
Validation loss: 2.5407265477455474

Epoch: 5| Step: 1
Training loss: 2.868780125146071
Validation loss: 2.553484210421728

Epoch: 5| Step: 2
Training loss: 2.3658360819203246
Validation loss: 2.550922352898012

Epoch: 5| Step: 3
Training loss: 1.9860845695921465
Validation loss: 2.550380965522418

Epoch: 5| Step: 4
Training loss: 2.6080121347343987
Validation loss: 2.549036505160022

Epoch: 5| Step: 5
Training loss: 2.123667242942344
Validation loss: 2.524116617629918

Epoch: 5| Step: 6
Training loss: 1.5799119859638808
Validation loss: 2.528990563263916

Epoch: 5| Step: 7
Training loss: 3.058883399938439
Validation loss: 2.5176371983516925

Epoch: 5| Step: 8
Training loss: 2.210871719082712
Validation loss: 2.5285189145284503

Epoch: 5| Step: 9
Training loss: 2.478178826573714
Validation loss: 2.5292715770973335

Epoch: 5| Step: 10
Training loss: 2.178073189555686
Validation loss: 2.524710252554416

Epoch: 5| Step: 11
Training loss: 1.828396051641527
Validation loss: 2.5142947563650258

Epoch: 236| Step: 0
Training loss: 2.6063611562612876
Validation loss: 2.513343337057663

Epoch: 5| Step: 1
Training loss: 2.8668926201714044
Validation loss: 2.522271110154934

Epoch: 5| Step: 2
Training loss: 2.1512053814549374
Validation loss: 2.521962265086793

Epoch: 5| Step: 3
Training loss: 2.4520244680991627
Validation loss: 2.524621494804581

Epoch: 5| Step: 4
Training loss: 1.8437911853392963
Validation loss: 2.5292235060646506

Epoch: 5| Step: 5
Training loss: 2.049865169278028
Validation loss: 2.528165036243221

Epoch: 5| Step: 6
Training loss: 2.842462940091902
Validation loss: 2.516960898992986

Epoch: 5| Step: 7
Training loss: 2.0299727899390834
Validation loss: 2.532099612626605

Epoch: 5| Step: 8
Training loss: 2.2423782533324004
Validation loss: 2.530183239625637

Epoch: 5| Step: 9
Training loss: 2.600442547915128
Validation loss: 2.534260921364549

Epoch: 5| Step: 10
Training loss: 1.7146791974411217
Validation loss: 2.5370052238937344

Epoch: 5| Step: 11
Training loss: 2.462808339909628
Validation loss: 2.516891506211357

Epoch: 237| Step: 0
Training loss: 2.969019827375958
Validation loss: 2.5239391950153807

Epoch: 5| Step: 1
Training loss: 1.9153329175401381
Validation loss: 2.5329738112172726

Epoch: 5| Step: 2
Training loss: 2.456249215705882
Validation loss: 2.539128589259072

Epoch: 5| Step: 3
Training loss: 2.286436481994164
Validation loss: 2.5305126788197265

Epoch: 5| Step: 4
Training loss: 1.8275821890659631
Validation loss: 2.534998749247361

Epoch: 5| Step: 5
Training loss: 1.7287534043726083
Validation loss: 2.527873695624079

Epoch: 5| Step: 6
Training loss: 2.180224280763039
Validation loss: 2.5250873624525405

Epoch: 5| Step: 7
Training loss: 2.066589693461333
Validation loss: 2.517745180903345

Epoch: 5| Step: 8
Training loss: 3.112674974785711
Validation loss: 2.5190736931799544

Epoch: 5| Step: 9
Training loss: 2.3971252827980107
Validation loss: 2.5140249838676327

Epoch: 5| Step: 10
Training loss: 2.2758849298789894
Validation loss: 2.5147615080167913

Epoch: 5| Step: 11
Training loss: 3.227027545509951
Validation loss: 2.5193772892400177

Epoch: 238| Step: 0
Training loss: 2.6990156745978435
Validation loss: 2.504560471405372

Epoch: 5| Step: 1
Training loss: 2.0991329628466198
Validation loss: 2.5185551805459987

Epoch: 5| Step: 2
Training loss: 2.525510237856369
Validation loss: 2.5217021845110312

Epoch: 5| Step: 3
Training loss: 1.917350964778606
Validation loss: 2.5111917009006057

Epoch: 5| Step: 4
Training loss: 2.2695356621756813
Validation loss: 2.5276303922271426

Epoch: 5| Step: 5
Training loss: 2.0773218653725127
Validation loss: 2.5128018509455288

Epoch: 5| Step: 6
Training loss: 2.111426817839606
Validation loss: 2.516888204564734

Epoch: 5| Step: 7
Training loss: 2.462093505614562
Validation loss: 2.5048424234783937

Epoch: 5| Step: 8
Training loss: 2.947718277760605
Validation loss: 2.5164408969872487

Epoch: 5| Step: 9
Training loss: 2.252848729141134
Validation loss: 2.5163798374549384

Epoch: 5| Step: 10
Training loss: 2.372245898291159
Validation loss: 2.5163065537958778

Epoch: 5| Step: 11
Training loss: 1.939862318560759
Validation loss: 2.5123608738857195

Epoch: 239| Step: 0
Training loss: 2.0112761668037615
Validation loss: 2.5143028757622528

Epoch: 5| Step: 1
Training loss: 2.4663208225017312
Validation loss: 2.5230647698572155

Epoch: 5| Step: 2
Training loss: 2.316819564201409
Validation loss: 2.5173845199431977

Epoch: 5| Step: 3
Training loss: 2.1735970236335627
Validation loss: 2.5231317191183718

Epoch: 5| Step: 4
Training loss: 2.0326215368440423
Validation loss: 2.517810898568399

Epoch: 5| Step: 5
Training loss: 2.709431870511529
Validation loss: 2.515085334230719

Epoch: 5| Step: 6
Training loss: 2.399138307853205
Validation loss: 2.5234486606221185

Epoch: 5| Step: 7
Training loss: 3.0698628569547295
Validation loss: 2.5347763682087927

Epoch: 5| Step: 8
Training loss: 2.3041743337585294
Validation loss: 2.5171419214139363

Epoch: 5| Step: 9
Training loss: 1.825132545465522
Validation loss: 2.5481547746313673

Epoch: 5| Step: 10
Training loss: 2.0937055896205417
Validation loss: 2.536775276753395

Epoch: 5| Step: 11
Training loss: 2.607131816146375
Validation loss: 2.5414230563458924

Epoch: 240| Step: 0
Training loss: 2.5641722456315867
Validation loss: 2.539702181320905

Epoch: 5| Step: 1
Training loss: 1.7184399411825297
Validation loss: 2.5334995560060642

Epoch: 5| Step: 2
Training loss: 1.5551433565840764
Validation loss: 2.536167638571642

Epoch: 5| Step: 3
Training loss: 2.5415687750637215
Validation loss: 2.539659834802957

Epoch: 5| Step: 4
Training loss: 2.2719226522577296
Validation loss: 2.53499432297923

Epoch: 5| Step: 5
Training loss: 2.2767201219187667
Validation loss: 2.5241801622692623

Epoch: 5| Step: 6
Training loss: 2.5536037561159546
Validation loss: 2.5216268178594494

Epoch: 5| Step: 7
Training loss: 2.174673130422623
Validation loss: 2.533040233856272

Epoch: 5| Step: 8
Training loss: 2.24597942558913
Validation loss: 2.5281622935382666

Epoch: 5| Step: 9
Training loss: 3.1623762747343855
Validation loss: 2.5194321999806677

Epoch: 5| Step: 10
Training loss: 2.2108259948002846
Validation loss: 2.5220867257972723

Epoch: 5| Step: 11
Training loss: 1.8284702464085498
Validation loss: 2.5218412943009234

Epoch: 241| Step: 0
Training loss: 2.849317529177834
Validation loss: 2.5193506220454593

Epoch: 5| Step: 1
Training loss: 2.3980042504280097
Validation loss: 2.5084260049495337

Epoch: 5| Step: 2
Training loss: 2.3183961911961957
Validation loss: 2.5157734331524813

Epoch: 5| Step: 3
Training loss: 2.180743874141228
Validation loss: 2.5196774107655235

Epoch: 5| Step: 4
Training loss: 2.5168243771414676
Validation loss: 2.515703306697148

Epoch: 5| Step: 5
Training loss: 2.3464227314258617
Validation loss: 2.5195762571114875

Epoch: 5| Step: 6
Training loss: 2.1831818603309987
Validation loss: 2.5231085957594006

Epoch: 5| Step: 7
Training loss: 2.041878929262844
Validation loss: 2.5364923350870496

Epoch: 5| Step: 8
Training loss: 2.2579461820421085
Validation loss: 2.539998035718003

Epoch: 5| Step: 9
Training loss: 2.0885768009062273
Validation loss: 2.5575487715892575

Epoch: 5| Step: 10
Training loss: 2.430367524881732
Validation loss: 2.5385060135804767

Epoch: 5| Step: 11
Training loss: 2.2762223324044744
Validation loss: 2.5530224311424434

Epoch: 242| Step: 0
Training loss: 2.326243004724315
Validation loss: 2.5356982513345483

Epoch: 5| Step: 1
Training loss: 2.2254996992328264
Validation loss: 2.5465390972654176

Epoch: 5| Step: 2
Training loss: 2.4157969345749164
Validation loss: 2.5526038994847684

Epoch: 5| Step: 3
Training loss: 2.0972219120499784
Validation loss: 2.5130306296412934

Epoch: 5| Step: 4
Training loss: 2.1733797198690987
Validation loss: 2.5081983489303137

Epoch: 5| Step: 5
Training loss: 1.9694031736924582
Validation loss: 2.5148411589908672

Epoch: 5| Step: 6
Training loss: 3.0138797750105715
Validation loss: 2.5362913532373343

Epoch: 5| Step: 7
Training loss: 2.094949634829712
Validation loss: 2.527286656642312

Epoch: 5| Step: 8
Training loss: 2.0763092139135315
Validation loss: 2.5255429093386046

Epoch: 5| Step: 9
Training loss: 2.0207416736488346
Validation loss: 2.525496364345689

Epoch: 5| Step: 10
Training loss: 2.885539927873812
Validation loss: 2.5319464278851296

Epoch: 5| Step: 11
Training loss: 3.479307038602956
Validation loss: 2.53622276549102

Epoch: 243| Step: 0
Training loss: 2.250070146950697
Validation loss: 2.5381523339888887

Epoch: 5| Step: 1
Training loss: 2.428427808185343
Validation loss: 2.5324127564272687

Epoch: 5| Step: 2
Training loss: 2.4343638663471934
Validation loss: 2.5438591003453435

Epoch: 5| Step: 3
Training loss: 2.5820396527082923
Validation loss: 2.5472123794360875

Epoch: 5| Step: 4
Training loss: 2.162151939297041
Validation loss: 2.5624881391328307

Epoch: 5| Step: 5
Training loss: 3.1173572924269464
Validation loss: 2.5628234321892274

Epoch: 5| Step: 6
Training loss: 1.825765015673171
Validation loss: 2.541046287044728

Epoch: 5| Step: 7
Training loss: 2.057825860955522
Validation loss: 2.542659668467796

Epoch: 5| Step: 8
Training loss: 2.6119931728688583
Validation loss: 2.5266404350054774

Epoch: 5| Step: 9
Training loss: 2.285225735121462
Validation loss: 2.5108303715835794

Epoch: 5| Step: 10
Training loss: 1.9819044819169527
Validation loss: 2.492545270601147

Epoch: 5| Step: 11
Training loss: 1.8563042937793135
Validation loss: 2.5049857135182574

Epoch: 244| Step: 0
Training loss: 2.045977097048432
Validation loss: 2.475954275925393

Epoch: 5| Step: 1
Training loss: 2.5010871430795514
Validation loss: 2.48136064609417

Epoch: 5| Step: 2
Training loss: 2.2729644807657583
Validation loss: 2.4773568765658807

Epoch: 5| Step: 3
Training loss: 2.053610864106625
Validation loss: 2.483664057031374

Epoch: 5| Step: 4
Training loss: 2.338829470272603
Validation loss: 2.4864431725689577

Epoch: 5| Step: 5
Training loss: 2.487680695071436
Validation loss: 2.476912661245331

Epoch: 5| Step: 6
Training loss: 2.4969256571380276
Validation loss: 2.487544491144762

Epoch: 5| Step: 7
Training loss: 3.0065936108303752
Validation loss: 2.4722341666367647

Epoch: 5| Step: 8
Training loss: 2.2848835056918153
Validation loss: 2.478411394497314

Epoch: 5| Step: 9
Training loss: 2.0956398704631494
Validation loss: 2.481829985120144

Epoch: 5| Step: 10
Training loss: 2.3740805803810314
Validation loss: 2.494616394150968

Epoch: 5| Step: 11
Training loss: 1.9073548788960228
Validation loss: 2.478117790388064

Epoch: 245| Step: 0
Training loss: 2.139043516154233
Validation loss: 2.4925208949964017

Epoch: 5| Step: 1
Training loss: 2.063353737507858
Validation loss: 2.4849466909655655

Epoch: 5| Step: 2
Training loss: 2.24423985141543
Validation loss: 2.5070600797050178

Epoch: 5| Step: 3
Training loss: 2.081390568565538
Validation loss: 2.5183404400188953

Epoch: 5| Step: 4
Training loss: 2.9651895153083108
Validation loss: 2.532314500558003

Epoch: 5| Step: 5
Training loss: 2.2966721633306464
Validation loss: 2.526694279688001

Epoch: 5| Step: 6
Training loss: 2.7614996363197375
Validation loss: 2.5149229503531054

Epoch: 5| Step: 7
Training loss: 1.9756978069769193
Validation loss: 2.5192289895607467

Epoch: 5| Step: 8
Training loss: 2.3127686628009894
Validation loss: 2.500291648383345

Epoch: 5| Step: 9
Training loss: 2.4925259447038806
Validation loss: 2.5155919311750896

Epoch: 5| Step: 10
Training loss: 1.991252066209467
Validation loss: 2.520869470894254

Epoch: 5| Step: 11
Training loss: 2.901295037337078
Validation loss: 2.5224193688213

Epoch: 246| Step: 0
Training loss: 1.9664408144605123
Validation loss: 2.5092197799340026

Epoch: 5| Step: 1
Training loss: 1.956328244823018
Validation loss: 2.5040672873349648

Epoch: 5| Step: 2
Training loss: 2.5016257721847284
Validation loss: 2.5124734957801103

Epoch: 5| Step: 3
Training loss: 2.0817813687554616
Validation loss: 2.5162798343184076

Epoch: 5| Step: 4
Training loss: 2.2241605954059374
Validation loss: 2.498271290807397

Epoch: 5| Step: 5
Training loss: 2.283529527500424
Validation loss: 2.50921888123122

Epoch: 5| Step: 6
Training loss: 2.7404313509478415
Validation loss: 2.5049616294988097

Epoch: 5| Step: 7
Training loss: 2.3184879206280424
Validation loss: 2.518324148338915

Epoch: 5| Step: 8
Training loss: 2.1120512762275223
Validation loss: 2.510117902165009

Epoch: 5| Step: 9
Training loss: 2.4021066440091947
Validation loss: 2.5301500607142375

Epoch: 5| Step: 10
Training loss: 2.548244178498111
Validation loss: 2.5188458005609697

Epoch: 5| Step: 11
Training loss: 3.3611568959238
Validation loss: 2.532948399042892

Epoch: 247| Step: 0
Training loss: 2.4438775578059566
Validation loss: 2.543382379553495

Epoch: 5| Step: 1
Training loss: 2.5765059502516445
Validation loss: 2.5329559213397643

Epoch: 5| Step: 2
Training loss: 2.7642515403575594
Validation loss: 2.550348795304673

Epoch: 5| Step: 3
Training loss: 2.50508097260773
Validation loss: 2.535248651373315

Epoch: 5| Step: 4
Training loss: 1.9153825839865362
Validation loss: 2.513638215332409

Epoch: 5| Step: 5
Training loss: 2.3544586698218275
Validation loss: 2.524236355791685

Epoch: 5| Step: 6
Training loss: 2.10944462414031
Validation loss: 2.5125650707441207

Epoch: 5| Step: 7
Training loss: 2.72104083482635
Validation loss: 2.512560385520858

Epoch: 5| Step: 8
Training loss: 2.52696468098832
Validation loss: 2.51896754619792

Epoch: 5| Step: 9
Training loss: 1.5590286604159698
Validation loss: 2.5045581748543713

Epoch: 5| Step: 10
Training loss: 1.9688385232066765
Validation loss: 2.523670012604818

Epoch: 5| Step: 11
Training loss: 1.7828384143440388
Validation loss: 2.5183928054193006

Epoch: 248| Step: 0
Training loss: 1.7971999164882686
Validation loss: 2.530622910894171

Epoch: 5| Step: 1
Training loss: 2.9441657224326656
Validation loss: 2.526746125980662

Epoch: 5| Step: 2
Training loss: 1.6799927190214146
Validation loss: 2.5486250444482357

Epoch: 5| Step: 3
Training loss: 2.3069074897886286
Validation loss: 2.5318172509958705

Epoch: 5| Step: 4
Training loss: 2.9000335165257414
Validation loss: 2.551128163292024

Epoch: 5| Step: 5
Training loss: 2.2415547292147346
Validation loss: 2.535043189805066

Epoch: 5| Step: 6
Training loss: 2.9467548107180117
Validation loss: 2.518534369976543

Epoch: 5| Step: 7
Training loss: 2.483857682787459
Validation loss: 2.504038283050355

Epoch: 5| Step: 8
Training loss: 1.7777323203765456
Validation loss: 2.4978764058024368

Epoch: 5| Step: 9
Training loss: 2.383584019563882
Validation loss: 2.4929800098865837

Epoch: 5| Step: 10
Training loss: 1.788550224424435
Validation loss: 2.5044835576163575

Epoch: 5| Step: 11
Training loss: 2.792500063111553
Validation loss: 2.472931620052626

Epoch: 249| Step: 0
Training loss: 2.865291457072416
Validation loss: 2.5086127338386106

Epoch: 5| Step: 1
Training loss: 2.7989403115872924
Validation loss: 2.5050167610587404

Epoch: 5| Step: 2
Training loss: 2.0762737317368205
Validation loss: 2.5382382781497848

Epoch: 5| Step: 3
Training loss: 1.462140898348143
Validation loss: 2.5317738191175927

Epoch: 5| Step: 4
Training loss: 2.566081265988992
Validation loss: 2.5373273466594704

Epoch: 5| Step: 5
Training loss: 2.518940890440602
Validation loss: 2.537691377992804

Epoch: 5| Step: 6
Training loss: 2.6592885309275216
Validation loss: 2.5308650669921575

Epoch: 5| Step: 7
Training loss: 2.0583933781605914
Validation loss: 2.5361765026725633

Epoch: 5| Step: 8
Training loss: 2.4336365624055696
Validation loss: 2.5303637695677614

Epoch: 5| Step: 9
Training loss: 1.706425679423441
Validation loss: 2.5260708456549903

Epoch: 5| Step: 10
Training loss: 2.1604685773164527
Validation loss: 2.531011946987199

Epoch: 5| Step: 11
Training loss: 0.6190273530275008
Validation loss: 2.5248285992363724

Epoch: 250| Step: 0
Training loss: 2.250519904363279
Validation loss: 2.5318247256454622

Epoch: 5| Step: 1
Training loss: 2.0623101811769806
Validation loss: 2.5340895835817534

Epoch: 5| Step: 2
Training loss: 2.068113836567856
Validation loss: 2.5327116039150726

Epoch: 5| Step: 3
Training loss: 1.8408454387376711
Validation loss: 2.5459986116937916

Epoch: 5| Step: 4
Training loss: 2.230306482508718
Validation loss: 2.556141306486836

Epoch: 5| Step: 5
Training loss: 2.203843236412507
Validation loss: 2.547860105224106

Epoch: 5| Step: 6
Training loss: 2.452419884293755
Validation loss: 2.5314456958239213

Epoch: 5| Step: 7
Training loss: 2.7614869448056036
Validation loss: 2.511346733106205

Epoch: 5| Step: 8
Training loss: 2.2941377899240742
Validation loss: 2.5201943325870944

Epoch: 5| Step: 9
Training loss: 2.820372942754976
Validation loss: 2.5187283638844464

Epoch: 5| Step: 10
Training loss: 2.7098415479050058
Validation loss: 2.514479125412288

Epoch: 5| Step: 11
Training loss: 1.577614578946515
Validation loss: 2.5184142916832393

Testing loss: 2.0443037174152314

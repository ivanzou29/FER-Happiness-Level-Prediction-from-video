Epoch: 1| Step: 0
Training loss: 5.402015686035156
Validation loss: 5.276321729024251

Epoch: 6| Step: 1
Training loss: 5.263906478881836
Validation loss: 5.2744128704071045

Epoch: 6| Step: 2
Training loss: 5.517148017883301
Validation loss: 5.272563775380452

Epoch: 6| Step: 3
Training loss: 4.67173957824707
Validation loss: 5.270734786987305

Epoch: 6| Step: 4
Training loss: 6.163184642791748
Validation loss: 5.268906990687053

Epoch: 6| Step: 5
Training loss: 5.366791725158691
Validation loss: 5.267130374908447

Epoch: 6| Step: 6
Training loss: 4.047744274139404
Validation loss: 5.265341679255168

Epoch: 6| Step: 7
Training loss: 5.294578552246094
Validation loss: 5.2635276317596436

Epoch: 6| Step: 8
Training loss: 7.004323959350586
Validation loss: 5.2616612911224365

Epoch: 6| Step: 9
Training loss: 4.873945713043213
Validation loss: 5.259719530741374

Epoch: 6| Step: 10
Training loss: 4.986907005310059
Validation loss: 5.25758695602417

Epoch: 6| Step: 11
Training loss: 4.390316963195801
Validation loss: 5.255561590194702

Epoch: 6| Step: 12
Training loss: 5.424429416656494
Validation loss: 5.253486076990764

Epoch: 6| Step: 13
Training loss: 6.26448917388916
Validation loss: 5.251156727472941

Epoch: 2| Step: 0
Training loss: 5.5647807121276855
Validation loss: 5.24873161315918

Epoch: 6| Step: 1
Training loss: 4.755863666534424
Validation loss: 5.246233860651652

Epoch: 6| Step: 2
Training loss: 4.470267295837402
Validation loss: 5.243716398874919

Epoch: 6| Step: 3
Training loss: 6.966860294342041
Validation loss: 5.240847746531169

Epoch: 6| Step: 4
Training loss: 6.084103584289551
Validation loss: 5.237974087397258

Epoch: 6| Step: 5
Training loss: 4.8434624671936035
Validation loss: 5.2348737716674805

Epoch: 6| Step: 6
Training loss: 4.912618160247803
Validation loss: 5.231761693954468

Epoch: 6| Step: 7
Training loss: 5.196561813354492
Validation loss: 5.22830867767334

Epoch: 6| Step: 8
Training loss: 5.407174110412598
Validation loss: 5.224742650985718

Epoch: 6| Step: 9
Training loss: 4.940847396850586
Validation loss: 5.220906575520833

Epoch: 6| Step: 10
Training loss: 5.835975646972656
Validation loss: 5.216975371042888

Epoch: 6| Step: 11
Training loss: 4.630047798156738
Validation loss: 5.212820370992024

Epoch: 6| Step: 12
Training loss: 5.632308006286621
Validation loss: 5.208509604136149

Epoch: 6| Step: 13
Training loss: 4.939675331115723
Validation loss: 5.2040064334869385

Epoch: 3| Step: 0
Training loss: 5.110081672668457
Validation loss: 5.199095646540324

Epoch: 6| Step: 1
Training loss: 5.60091495513916
Validation loss: 5.194039185841878

Epoch: 6| Step: 2
Training loss: 5.924792289733887
Validation loss: 5.188813924789429

Epoch: 6| Step: 3
Training loss: 5.6982316970825195
Validation loss: 5.183435916900635

Epoch: 6| Step: 4
Training loss: 4.584072589874268
Validation loss: 5.1775886217753095

Epoch: 6| Step: 5
Training loss: 5.284593105316162
Validation loss: 5.171277125676473

Epoch: 6| Step: 6
Training loss: 5.242082118988037
Validation loss: 5.165041367212932

Epoch: 6| Step: 7
Training loss: 4.592045783996582
Validation loss: 5.15827218691508

Epoch: 6| Step: 8
Training loss: 6.0034637451171875
Validation loss: 5.151502927144368

Epoch: 6| Step: 9
Training loss: 4.884034156799316
Validation loss: 5.144298235575358

Epoch: 6| Step: 10
Training loss: 4.7719902992248535
Validation loss: 5.1368420124053955

Epoch: 6| Step: 11
Training loss: 4.403041839599609
Validation loss: 5.129147847493489

Epoch: 6| Step: 12
Training loss: 5.619667053222656
Validation loss: 5.121231158574422

Epoch: 6| Step: 13
Training loss: 5.509467124938965
Validation loss: 5.113176981608073

Epoch: 4| Step: 0
Training loss: 4.444548606872559
Validation loss: 5.104603211085002

Epoch: 6| Step: 1
Training loss: 5.6737470626831055
Validation loss: 5.09588638941447

Epoch: 6| Step: 2
Training loss: 5.620027542114258
Validation loss: 5.087172031402588

Epoch: 6| Step: 3
Training loss: 5.290668964385986
Validation loss: 5.0781168937683105

Epoch: 6| Step: 4
Training loss: 4.370641231536865
Validation loss: 5.069315433502197

Epoch: 6| Step: 5
Training loss: 3.583066463470459
Validation loss: 5.059928258260091

Epoch: 6| Step: 6
Training loss: 5.016732215881348
Validation loss: 5.050862550735474

Epoch: 6| Step: 7
Training loss: 4.754560470581055
Validation loss: 5.041268984476726

Epoch: 6| Step: 8
Training loss: 5.714068412780762
Validation loss: 5.0322418212890625

Epoch: 6| Step: 9
Training loss: 5.025683879852295
Validation loss: 5.022893746693929

Epoch: 6| Step: 10
Training loss: 5.758476257324219
Validation loss: 5.013317823410034

Epoch: 6| Step: 11
Training loss: 5.255921363830566
Validation loss: 5.004165410995483

Epoch: 6| Step: 12
Training loss: 5.172717094421387
Validation loss: 4.994682312011719

Epoch: 6| Step: 13
Training loss: 6.059255599975586
Validation loss: 4.9857674439748125

Epoch: 5| Step: 0
Training loss: 4.750062942504883
Validation loss: 4.976666609446208

Epoch: 6| Step: 1
Training loss: 4.949369430541992
Validation loss: 4.967633008956909

Epoch: 6| Step: 2
Training loss: 4.372321128845215
Validation loss: 4.9591898918151855

Epoch: 6| Step: 3
Training loss: 4.818332672119141
Validation loss: 4.950361967086792

Epoch: 6| Step: 4
Training loss: 6.0354156494140625
Validation loss: 4.9421219031016035

Epoch: 6| Step: 5
Training loss: 4.652278900146484
Validation loss: 4.934027512868245

Epoch: 6| Step: 6
Training loss: 5.734289646148682
Validation loss: 4.925921122233073

Epoch: 6| Step: 7
Training loss: 4.309590816497803
Validation loss: 4.918420871098836

Epoch: 6| Step: 8
Training loss: 4.880939483642578
Validation loss: 4.9105409781138105

Epoch: 6| Step: 9
Training loss: 4.052037715911865
Validation loss: 4.9034271240234375

Epoch: 6| Step: 10
Training loss: 5.381874084472656
Validation loss: 4.895841121673584

Epoch: 6| Step: 11
Training loss: 6.14119815826416
Validation loss: 4.888654788335164

Epoch: 6| Step: 12
Training loss: 4.87282657623291
Validation loss: 4.881712754567464

Epoch: 6| Step: 13
Training loss: 5.163167953491211
Validation loss: 4.874729911486308

Epoch: 6| Step: 0
Training loss: 4.674973964691162
Validation loss: 4.867969274520874

Epoch: 6| Step: 1
Training loss: 5.508594512939453
Validation loss: 4.861331741015117

Epoch: 6| Step: 2
Training loss: 4.784557342529297
Validation loss: 4.854427973429362

Epoch: 6| Step: 3
Training loss: 4.516427040100098
Validation loss: 4.848217805226644

Epoch: 6| Step: 4
Training loss: 4.685847282409668
Validation loss: 4.841739972432454

Epoch: 6| Step: 5
Training loss: 4.892261028289795
Validation loss: 4.835751056671143

Epoch: 6| Step: 6
Training loss: 5.35041618347168
Validation loss: 4.829485058784485

Epoch: 6| Step: 7
Training loss: 4.306785583496094
Validation loss: 4.8237245082855225

Epoch: 6| Step: 8
Training loss: 3.774822473526001
Validation loss: 4.817788124084473

Epoch: 6| Step: 9
Training loss: 4.936846733093262
Validation loss: 4.812251567840576

Epoch: 6| Step: 10
Training loss: 6.295872688293457
Validation loss: 4.806693236033122

Epoch: 6| Step: 11
Training loss: 5.306051254272461
Validation loss: 4.8010783195495605

Epoch: 6| Step: 12
Training loss: 5.619390487670898
Validation loss: 4.795606692632039

Epoch: 6| Step: 13
Training loss: 4.16922664642334
Validation loss: 4.790144681930542

Epoch: 7| Step: 0
Training loss: 4.816093444824219
Validation loss: 4.784643014272054

Epoch: 6| Step: 1
Training loss: 5.005359649658203
Validation loss: 4.779234488805135

Epoch: 6| Step: 2
Training loss: 3.958599090576172
Validation loss: 4.773815393447876

Epoch: 6| Step: 3
Training loss: 5.496140480041504
Validation loss: 4.7684242725372314

Epoch: 6| Step: 4
Training loss: 5.966753005981445
Validation loss: 4.762973070144653

Epoch: 6| Step: 5
Training loss: 3.856569528579712
Validation loss: 4.757874011993408

Epoch: 6| Step: 6
Training loss: 5.0026960372924805
Validation loss: 4.752311070760091

Epoch: 6| Step: 7
Training loss: 4.726491928100586
Validation loss: 4.746816118558248

Epoch: 6| Step: 8
Training loss: 4.797811508178711
Validation loss: 4.741408507029216

Epoch: 6| Step: 9
Training loss: 4.333793640136719
Validation loss: 4.735745708147685

Epoch: 6| Step: 10
Training loss: 4.346806526184082
Validation loss: 4.729647119839986

Epoch: 6| Step: 11
Training loss: 4.652928352355957
Validation loss: 4.724279403686523

Epoch: 6| Step: 12
Training loss: 5.71733283996582
Validation loss: 4.7187559604644775

Epoch: 6| Step: 13
Training loss: 5.10224723815918
Validation loss: 4.713568369547526

Epoch: 8| Step: 0
Training loss: 4.533071041107178
Validation loss: 4.708116928736369

Epoch: 6| Step: 1
Training loss: 4.359930038452148
Validation loss: 4.702895720799764

Epoch: 6| Step: 2
Training loss: 3.624617338180542
Validation loss: 4.6971564292907715

Epoch: 6| Step: 3
Training loss: 3.630207061767578
Validation loss: 4.691757996877034

Epoch: 6| Step: 4
Training loss: 5.364214897155762
Validation loss: 4.685623566309611

Epoch: 6| Step: 5
Training loss: 5.713122367858887
Validation loss: 4.680080771446228

Epoch: 6| Step: 6
Training loss: 5.026479244232178
Validation loss: 4.67430837949117

Epoch: 6| Step: 7
Training loss: 4.151894569396973
Validation loss: 4.668416738510132

Epoch: 6| Step: 8
Training loss: 4.973897933959961
Validation loss: 4.662503918011983

Epoch: 6| Step: 9
Training loss: 5.399753570556641
Validation loss: 4.656951944033305

Epoch: 6| Step: 10
Training loss: 4.670129776000977
Validation loss: 4.650573809941609

Epoch: 6| Step: 11
Training loss: 5.155334949493408
Validation loss: 4.645235657691956

Epoch: 6| Step: 12
Training loss: 4.029109477996826
Validation loss: 4.639439026514689

Epoch: 6| Step: 13
Training loss: 6.110365390777588
Validation loss: 4.634175697962443

Epoch: 9| Step: 0
Training loss: 5.247859954833984
Validation loss: 4.6288619836171465

Epoch: 6| Step: 1
Training loss: 4.815473556518555
Validation loss: 4.623177766799927

Epoch: 6| Step: 2
Training loss: 4.535430431365967
Validation loss: 4.617800911267598

Epoch: 6| Step: 3
Training loss: 4.862062454223633
Validation loss: 4.612323363622029

Epoch: 6| Step: 4
Training loss: 4.961693286895752
Validation loss: 4.606779019037883

Epoch: 6| Step: 5
Training loss: 5.337815284729004
Validation loss: 4.601370493570964

Epoch: 6| Step: 6
Training loss: 4.139627456665039
Validation loss: 4.595838705698649

Epoch: 6| Step: 7
Training loss: 4.424664497375488
Validation loss: 4.590569257736206

Epoch: 6| Step: 8
Training loss: 5.176538467407227
Validation loss: 4.584815343221028

Epoch: 6| Step: 9
Training loss: 4.413022041320801
Validation loss: 4.579733928044637

Epoch: 6| Step: 10
Training loss: 4.907818794250488
Validation loss: 4.574035724004109

Epoch: 6| Step: 11
Training loss: 4.031866073608398
Validation loss: 4.568713982899983

Epoch: 6| Step: 12
Training loss: 4.838142395019531
Validation loss: 4.563420534133911

Epoch: 6| Step: 13
Training loss: 3.994332790374756
Validation loss: 4.557949264844258

Epoch: 10| Step: 0
Training loss: 3.5684969425201416
Validation loss: 4.553152163823445

Epoch: 6| Step: 1
Training loss: 4.612074851989746
Validation loss: 4.547507921854655

Epoch: 6| Step: 2
Training loss: 5.048768043518066
Validation loss: 4.542408148447673

Epoch: 6| Step: 3
Training loss: 4.534323692321777
Validation loss: 4.536621173222859

Epoch: 6| Step: 4
Training loss: 4.542469024658203
Validation loss: 4.531356334686279

Epoch: 6| Step: 5
Training loss: 5.092000484466553
Validation loss: 4.525906960169475

Epoch: 6| Step: 6
Training loss: 3.721794605255127
Validation loss: 4.5201670328776045

Epoch: 6| Step: 7
Training loss: 5.728595733642578
Validation loss: 4.514954884847005

Epoch: 6| Step: 8
Training loss: 4.992343425750732
Validation loss: 4.5087363719940186

Epoch: 6| Step: 9
Training loss: 4.2589874267578125
Validation loss: 4.503265539805095

Epoch: 6| Step: 10
Training loss: 3.777792453765869
Validation loss: 4.498252709706624

Epoch: 6| Step: 11
Training loss: 4.46335506439209
Validation loss: 4.4924590190251665

Epoch: 6| Step: 12
Training loss: 5.679144859313965
Validation loss: 4.487194776535034

Epoch: 6| Step: 13
Training loss: 4.685354232788086
Validation loss: 4.481388012568156

Epoch: 11| Step: 0
Training loss: 4.459044456481934
Validation loss: 4.476092259089152

Epoch: 6| Step: 1
Training loss: 3.749035358428955
Validation loss: 4.470129172007243

Epoch: 6| Step: 2
Training loss: 5.319340705871582
Validation loss: 4.464482982953389

Epoch: 6| Step: 3
Training loss: 5.010724067687988
Validation loss: 4.457977533340454

Epoch: 6| Step: 4
Training loss: 4.694774627685547
Validation loss: 4.451558033625285

Epoch: 6| Step: 5
Training loss: 3.6461329460144043
Validation loss: 4.4450318813323975

Epoch: 6| Step: 6
Training loss: 4.1715545654296875
Validation loss: 4.439143061637878

Epoch: 6| Step: 7
Training loss: 3.218660593032837
Validation loss: 4.432524522145589

Epoch: 6| Step: 8
Training loss: 4.770454406738281
Validation loss: 4.426335255304973

Epoch: 6| Step: 9
Training loss: 5.01552677154541
Validation loss: 4.420437574386597

Epoch: 6| Step: 10
Training loss: 5.239693641662598
Validation loss: 4.4144675731658936

Epoch: 6| Step: 11
Training loss: 5.23512077331543
Validation loss: 4.408148606618245

Epoch: 6| Step: 12
Training loss: 4.273739814758301
Validation loss: 4.402024666468303

Epoch: 6| Step: 13
Training loss: 4.8515729904174805
Validation loss: 4.395576596260071

Epoch: 12| Step: 0
Training loss: 4.150464057922363
Validation loss: 4.388654390970866

Epoch: 6| Step: 1
Training loss: 4.756404876708984
Validation loss: 4.382114013036092

Epoch: 6| Step: 2
Training loss: 3.480217695236206
Validation loss: 4.37546984354655

Epoch: 6| Step: 3
Training loss: 4.828381538391113
Validation loss: 4.368496894836426

Epoch: 6| Step: 4
Training loss: 4.952675819396973
Validation loss: 4.361961046854655

Epoch: 6| Step: 5
Training loss: 3.9133241176605225
Validation loss: 4.355129957199097

Epoch: 6| Step: 6
Training loss: 4.891560077667236
Validation loss: 4.348665475845337

Epoch: 6| Step: 7
Training loss: 4.767993450164795
Validation loss: 4.342495083808899

Epoch: 6| Step: 8
Training loss: 3.7592270374298096
Validation loss: 4.335845470428467

Epoch: 6| Step: 9
Training loss: 4.691553592681885
Validation loss: 4.329725027084351

Epoch: 6| Step: 10
Training loss: 3.6849913597106934
Validation loss: 4.3235461711883545

Epoch: 6| Step: 11
Training loss: 5.440115928649902
Validation loss: 4.317675630251567

Epoch: 6| Step: 12
Training loss: 4.758241653442383
Validation loss: 4.311252236366272

Epoch: 6| Step: 13
Training loss: 4.471602916717529
Validation loss: 4.305453022321065

Epoch: 13| Step: 0
Training loss: 4.652225017547607
Validation loss: 4.2995478709538775

Epoch: 6| Step: 1
Training loss: 3.5650322437286377
Validation loss: 4.293505350748698

Epoch: 6| Step: 2
Training loss: 4.572081565856934
Validation loss: 4.28764283657074

Epoch: 6| Step: 3
Training loss: 4.391655445098877
Validation loss: 4.281866153081258

Epoch: 6| Step: 4
Training loss: 3.6000473499298096
Validation loss: 4.2759954531987505

Epoch: 6| Step: 5
Training loss: 4.746122360229492
Validation loss: 4.269784013430278

Epoch: 6| Step: 6
Training loss: 5.127260684967041
Validation loss: 4.263902346293132

Epoch: 6| Step: 7
Training loss: 4.450858116149902
Validation loss: 4.256892442703247

Epoch: 6| Step: 8
Training loss: 3.952086925506592
Validation loss: 4.250872572263082

Epoch: 6| Step: 9
Training loss: 3.4067091941833496
Validation loss: 4.244712829589844

Epoch: 6| Step: 10
Training loss: 5.098238945007324
Validation loss: 4.238943099975586

Epoch: 6| Step: 11
Training loss: 4.533607482910156
Validation loss: 4.233568628629048

Epoch: 6| Step: 12
Training loss: 4.695720672607422
Validation loss: 4.226839145024617

Epoch: 6| Step: 13
Training loss: 4.633462905883789
Validation loss: 4.22109580039978

Epoch: 14| Step: 0
Training loss: 5.023442268371582
Validation loss: 4.215455373128255

Epoch: 6| Step: 1
Training loss: 4.02534818649292
Validation loss: 4.209466775258382

Epoch: 6| Step: 2
Training loss: 4.350753307342529
Validation loss: 4.203734040260315

Epoch: 6| Step: 3
Training loss: 4.099125862121582
Validation loss: 4.197234670321147

Epoch: 6| Step: 4
Training loss: 3.3297252655029297
Validation loss: 4.191166241963704

Epoch: 6| Step: 5
Training loss: 4.644722938537598
Validation loss: 4.185320496559143

Epoch: 6| Step: 6
Training loss: 4.073455333709717
Validation loss: 4.179794867833455

Epoch: 6| Step: 7
Training loss: 4.447071075439453
Validation loss: 4.174215277036031

Epoch: 6| Step: 8
Training loss: 4.2847771644592285
Validation loss: 4.1686631838480634

Epoch: 6| Step: 9
Training loss: 5.046290397644043
Validation loss: 4.163432916005452

Epoch: 6| Step: 10
Training loss: 3.594512462615967
Validation loss: 4.157567660013835

Epoch: 6| Step: 11
Training loss: 4.73960018157959
Validation loss: 4.152554591496785

Epoch: 6| Step: 12
Training loss: 3.958254098892212
Validation loss: 4.146996299425761

Epoch: 6| Step: 13
Training loss: 4.776424884796143
Validation loss: 4.142233530680339

Epoch: 15| Step: 0
Training loss: 3.1638145446777344
Validation loss: 4.137053370475769

Epoch: 6| Step: 1
Training loss: 4.1230363845825195
Validation loss: 4.132323622703552

Epoch: 6| Step: 2
Training loss: 4.319918632507324
Validation loss: 4.1276140213012695

Epoch: 6| Step: 3
Training loss: 4.912900447845459
Validation loss: 4.122346798578898

Epoch: 6| Step: 4
Training loss: 4.082981586456299
Validation loss: 4.11739981174469

Epoch: 6| Step: 5
Training loss: 4.288687705993652
Validation loss: 4.112283706665039

Epoch: 6| Step: 6
Training loss: 4.123885154724121
Validation loss: 4.107339859008789

Epoch: 6| Step: 7
Training loss: 4.256239891052246
Validation loss: 4.101855357487996

Epoch: 6| Step: 8
Training loss: 3.673802375793457
Validation loss: 4.097036083539327

Epoch: 6| Step: 9
Training loss: 4.926041603088379
Validation loss: 4.092314720153809

Epoch: 6| Step: 10
Training loss: 4.768460273742676
Validation loss: 4.087701161702474

Epoch: 6| Step: 11
Training loss: 4.914515495300293
Validation loss: 4.082317193349202

Epoch: 6| Step: 12
Training loss: 3.6631264686584473
Validation loss: 4.076706290245056

Epoch: 6| Step: 13
Training loss: 4.206750869750977
Validation loss: 4.07174567381541

Epoch: 16| Step: 0
Training loss: 4.105325222015381
Validation loss: 4.066781163215637

Epoch: 6| Step: 1
Training loss: 4.818881034851074
Validation loss: 4.0621194044748945

Epoch: 6| Step: 2
Training loss: 4.086568355560303
Validation loss: 4.0563600063323975

Epoch: 6| Step: 3
Training loss: 3.605649948120117
Validation loss: 4.051495989163716

Epoch: 6| Step: 4
Training loss: 4.650309085845947
Validation loss: 4.0466132164001465

Epoch: 6| Step: 5
Training loss: 4.2375898361206055
Validation loss: 4.041115363438924

Epoch: 6| Step: 6
Training loss: 4.054536819458008
Validation loss: 4.036373217900594

Epoch: 6| Step: 7
Training loss: 4.248047828674316
Validation loss: 4.03092881043752

Epoch: 6| Step: 8
Training loss: 3.9785122871398926
Validation loss: 4.0262443621953325

Epoch: 6| Step: 9
Training loss: 4.4173126220703125
Validation loss: 4.021451195081075

Epoch: 6| Step: 10
Training loss: 4.202624320983887
Validation loss: 4.0169399579366045

Epoch: 6| Step: 11
Training loss: 3.698185443878174
Validation loss: 4.01192847887675

Epoch: 6| Step: 12
Training loss: 4.209224224090576
Validation loss: 4.006536682446797

Epoch: 6| Step: 13
Training loss: 4.191585063934326
Validation loss: 4.001497030258179

Epoch: 17| Step: 0
Training loss: 4.817111015319824
Validation loss: 3.996048013369242

Epoch: 6| Step: 1
Training loss: 4.509006500244141
Validation loss: 3.9919034242630005

Epoch: 6| Step: 2
Training loss: 3.2295827865600586
Validation loss: 3.9865302244822183

Epoch: 6| Step: 3
Training loss: 4.378749847412109
Validation loss: 3.98162841796875

Epoch: 6| Step: 4
Training loss: 3.807096481323242
Validation loss: 3.9769594271977744

Epoch: 6| Step: 5
Training loss: 4.179354667663574
Validation loss: 3.97249436378479

Epoch: 6| Step: 6
Training loss: 4.390334129333496
Validation loss: 3.9670933882395425

Epoch: 6| Step: 7
Training loss: 3.3976693153381348
Validation loss: 3.9621614615122476

Epoch: 6| Step: 8
Training loss: 4.562229633331299
Validation loss: 3.9579502741495767

Epoch: 6| Step: 9
Training loss: 4.781851768493652
Validation loss: 3.953062574068705

Epoch: 6| Step: 10
Training loss: 4.1762495040893555
Validation loss: 3.947740316390991

Epoch: 6| Step: 11
Training loss: 3.929224729537964
Validation loss: 3.943059762318929

Epoch: 6| Step: 12
Training loss: 3.7154293060302734
Validation loss: 3.937879045804342

Epoch: 6| Step: 13
Training loss: 3.6814754009246826
Validation loss: 3.932571291923523

Epoch: 18| Step: 0
Training loss: 3.3047800064086914
Validation loss: 3.927986184755961

Epoch: 6| Step: 1
Training loss: 4.240908622741699
Validation loss: 3.923907518386841

Epoch: 6| Step: 2
Training loss: 5.068397521972656
Validation loss: 3.918057918548584

Epoch: 6| Step: 3
Training loss: 3.3848752975463867
Validation loss: 3.913757642110189

Epoch: 6| Step: 4
Training loss: 4.243484020233154
Validation loss: 3.90849240620931

Epoch: 6| Step: 5
Training loss: 4.123074531555176
Validation loss: 3.903327703475952

Epoch: 6| Step: 6
Training loss: 3.9180448055267334
Validation loss: 3.898585557937622

Epoch: 6| Step: 7
Training loss: 3.435919761657715
Validation loss: 3.893738349278768

Epoch: 6| Step: 8
Training loss: 3.507648229598999
Validation loss: 3.889735142389933

Epoch: 6| Step: 9
Training loss: 4.385846138000488
Validation loss: 3.8847097158432007

Epoch: 6| Step: 10
Training loss: 4.019594669342041
Validation loss: 3.880175789197286

Epoch: 6| Step: 11
Training loss: 4.204988956451416
Validation loss: 3.875396450360616

Epoch: 6| Step: 12
Training loss: 4.538597106933594
Validation loss: 3.871135632197062

Epoch: 6| Step: 13
Training loss: 4.280032157897949
Validation loss: 3.8662490447362265

Epoch: 19| Step: 0
Training loss: 4.137573719024658
Validation loss: 3.8607599337895713

Epoch: 6| Step: 1
Training loss: 3.87508487701416
Validation loss: 3.856213132540385

Epoch: 6| Step: 2
Training loss: 3.0473709106445312
Validation loss: 3.8520580927530923

Epoch: 6| Step: 3
Training loss: 3.8952763080596924
Validation loss: 3.8471115032831826

Epoch: 6| Step: 4
Training loss: 3.6090240478515625
Validation loss: 3.842840592066447

Epoch: 6| Step: 5
Training loss: 2.4987950325012207
Validation loss: 3.8383628924687705

Epoch: 6| Step: 6
Training loss: 3.6570918560028076
Validation loss: 3.8334972858428955

Epoch: 6| Step: 7
Training loss: 4.041471481323242
Validation loss: 3.8293615182240806

Epoch: 6| Step: 8
Training loss: 5.298762321472168
Validation loss: 3.8235425551732383

Epoch: 6| Step: 9
Training loss: 3.8619003295898438
Validation loss: 3.819243629773458

Epoch: 6| Step: 10
Training loss: 4.30732536315918
Validation loss: 3.8145312070846558

Epoch: 6| Step: 11
Training loss: 5.0027995109558105
Validation loss: 3.809410333633423

Epoch: 6| Step: 12
Training loss: 3.9987738132476807
Validation loss: 3.806013743082682

Epoch: 6| Step: 13
Training loss: 4.540757179260254
Validation loss: 3.801411787668864

Epoch: 20| Step: 0
Training loss: 3.5443215370178223
Validation loss: 3.7959226767222085

Epoch: 6| Step: 1
Training loss: 3.528905153274536
Validation loss: 3.791167894999186

Epoch: 6| Step: 2
Training loss: 4.662153720855713
Validation loss: 3.7856091260910034

Epoch: 6| Step: 3
Training loss: 2.8924179077148438
Validation loss: 3.7813737789789834

Epoch: 6| Step: 4
Training loss: 3.444481372833252
Validation loss: 3.7767444451649985

Epoch: 6| Step: 5
Training loss: 3.769388437271118
Validation loss: 3.7720258235931396

Epoch: 6| Step: 6
Training loss: 4.14487361907959
Validation loss: 3.767949342727661

Epoch: 6| Step: 7
Training loss: 4.162939071655273
Validation loss: 3.7632963260014853

Epoch: 6| Step: 8
Training loss: 4.405718803405762
Validation loss: 3.7588295936584473

Epoch: 6| Step: 9
Training loss: 3.9742610454559326
Validation loss: 3.7545082171758017

Epoch: 6| Step: 10
Training loss: 4.202627182006836
Validation loss: 3.750127832094828

Epoch: 6| Step: 11
Training loss: 5.152360916137695
Validation loss: 3.7450440724690757

Epoch: 6| Step: 12
Training loss: 4.719958782196045
Validation loss: 3.7406179110209146

Epoch: 6| Step: 13
Training loss: 2.327146530151367
Validation loss: 3.735898494720459

Epoch: 21| Step: 0
Training loss: 3.56630802154541
Validation loss: 3.7309065659840903

Epoch: 6| Step: 1
Training loss: 3.3756964206695557
Validation loss: 3.727068066596985

Epoch: 6| Step: 2
Training loss: 3.5640697479248047
Validation loss: 3.7222923835118613

Epoch: 6| Step: 3
Training loss: 3.2807445526123047
Validation loss: 3.7182099421819053

Epoch: 6| Step: 4
Training loss: 3.564706802368164
Validation loss: 3.714361588160197

Epoch: 6| Step: 5
Training loss: 4.62974739074707
Validation loss: 3.7101869583129883

Epoch: 6| Step: 6
Training loss: 4.282430648803711
Validation loss: 3.705385208129883

Epoch: 6| Step: 7
Training loss: 4.403585910797119
Validation loss: 3.701367894808451

Epoch: 6| Step: 8
Training loss: 4.23952579498291
Validation loss: 3.696978290875753

Epoch: 6| Step: 9
Training loss: 3.5540664196014404
Validation loss: 3.692516287167867

Epoch: 6| Step: 10
Training loss: 3.790623664855957
Validation loss: 3.6883533000946045

Epoch: 6| Step: 11
Training loss: 4.072854995727539
Validation loss: 3.6840508778889975

Epoch: 6| Step: 12
Training loss: 4.117458820343018
Validation loss: 3.679837425549825

Epoch: 6| Step: 13
Training loss: 3.6628260612487793
Validation loss: 3.675243099530538

Epoch: 22| Step: 0
Training loss: 2.648529529571533
Validation loss: 3.671703656514486

Epoch: 6| Step: 1
Training loss: 4.035335540771484
Validation loss: 3.667269706726074

Epoch: 6| Step: 2
Training loss: 4.29637336730957
Validation loss: 3.662769873936971

Epoch: 6| Step: 3
Training loss: 5.0052170753479
Validation loss: 3.6588265895843506

Epoch: 6| Step: 4
Training loss: 3.722597122192383
Validation loss: 3.6546396811803183

Epoch: 6| Step: 5
Training loss: 3.440629005432129
Validation loss: 3.6509631077448526

Epoch: 6| Step: 6
Training loss: 3.344630241394043
Validation loss: 3.6460936069488525

Epoch: 6| Step: 7
Training loss: 3.7015109062194824
Validation loss: 3.6418820222218833

Epoch: 6| Step: 8
Training loss: 4.028460502624512
Validation loss: 3.637710173924764

Epoch: 6| Step: 9
Training loss: 3.798103094100952
Validation loss: 3.633039395014445

Epoch: 6| Step: 10
Training loss: 3.928859233856201
Validation loss: 3.629311482111613

Epoch: 6| Step: 11
Training loss: 3.9300198554992676
Validation loss: 3.6251127322514853

Epoch: 6| Step: 12
Training loss: 3.57412052154541
Validation loss: 3.6205130020777383

Epoch: 6| Step: 13
Training loss: 3.8165817260742188
Validation loss: 3.6160444418589273

Epoch: 23| Step: 0
Training loss: 3.7364766597747803
Validation loss: 3.6114911238352456

Epoch: 6| Step: 1
Training loss: 3.6870455741882324
Validation loss: 3.607203960418701

Epoch: 6| Step: 2
Training loss: 3.7319111824035645
Validation loss: 3.603018283843994

Epoch: 6| Step: 3
Training loss: 3.6328954696655273
Validation loss: 3.598429560661316

Epoch: 6| Step: 4
Training loss: 3.8422329425811768
Validation loss: 3.5945369402567544

Epoch: 6| Step: 5
Training loss: 3.0791573524475098
Validation loss: 3.5906301736831665

Epoch: 6| Step: 6
Training loss: 4.139512062072754
Validation loss: 3.5862111250559487

Epoch: 6| Step: 7
Training loss: 4.271695137023926
Validation loss: 3.5819352865219116

Epoch: 6| Step: 8
Training loss: 4.486379146575928
Validation loss: 3.577595909436544

Epoch: 6| Step: 9
Training loss: 2.872875928878784
Validation loss: 3.5728585720062256

Epoch: 6| Step: 10
Training loss: 3.3878579139709473
Validation loss: 3.568134387334188

Epoch: 6| Step: 11
Training loss: 3.263672351837158
Validation loss: 3.5632928212483725

Epoch: 6| Step: 12
Training loss: 4.152613639831543
Validation loss: 3.558395584424337

Epoch: 6| Step: 13
Training loss: 4.154580116271973
Validation loss: 3.554190913836161

Epoch: 24| Step: 0
Training loss: 3.685537338256836
Validation loss: 3.549801150957743

Epoch: 6| Step: 1
Training loss: 4.015005111694336
Validation loss: 3.545029123624166

Epoch: 6| Step: 2
Training loss: 4.114103317260742
Validation loss: 3.540818134943644

Epoch: 6| Step: 3
Training loss: 3.805497884750366
Validation loss: 3.536413550376892

Epoch: 6| Step: 4
Training loss: 3.558436870574951
Validation loss: 3.5317142407099404

Epoch: 6| Step: 5
Training loss: 2.5715839862823486
Validation loss: 3.527262886365255

Epoch: 6| Step: 6
Training loss: 3.4911999702453613
Validation loss: 3.5230215390523276

Epoch: 6| Step: 7
Training loss: 3.869101047515869
Validation loss: 3.518574277559916

Epoch: 6| Step: 8
Training loss: 3.4703779220581055
Validation loss: 3.513471325238546

Epoch: 6| Step: 9
Training loss: 3.057072639465332
Validation loss: 3.510193705558777

Epoch: 6| Step: 10
Training loss: 3.174088954925537
Validation loss: 3.505662202835083

Epoch: 6| Step: 11
Training loss: 3.8436925411224365
Validation loss: 3.5013389190038047

Epoch: 6| Step: 12
Training loss: 3.884486675262451
Validation loss: 3.4969065189361572

Epoch: 6| Step: 13
Training loss: 5.055157661437988
Validation loss: 3.4917782147725425

Epoch: 25| Step: 0
Training loss: 4.863824367523193
Validation loss: 3.4877914985020957

Epoch: 6| Step: 1
Training loss: 3.5683040618896484
Validation loss: 3.4826321999231973

Epoch: 6| Step: 2
Training loss: 3.3709716796875
Validation loss: 3.478019952774048

Epoch: 6| Step: 3
Training loss: 2.7960963249206543
Validation loss: 3.4737380743026733

Epoch: 6| Step: 4
Training loss: 3.257246494293213
Validation loss: 3.468424359957377

Epoch: 6| Step: 5
Training loss: 3.773470163345337
Validation loss: 3.463862657546997

Epoch: 6| Step: 6
Training loss: 3.8160033226013184
Validation loss: 3.4592620531717935

Epoch: 6| Step: 7
Training loss: 3.6827661991119385
Validation loss: 3.4544498523076377

Epoch: 6| Step: 8
Training loss: 3.9245681762695312
Validation loss: 3.4490760962168374

Epoch: 6| Step: 9
Training loss: 3.662954807281494
Validation loss: 3.444439252217611

Epoch: 6| Step: 10
Training loss: 3.391298770904541
Validation loss: 3.4388421376546225

Epoch: 6| Step: 11
Training loss: 3.7441234588623047
Validation loss: 3.4338390827178955

Epoch: 6| Step: 12
Training loss: 4.146539688110352
Validation loss: 3.4287995100021362

Epoch: 6| Step: 13
Training loss: 2.752708911895752
Validation loss: 3.42475692431132

Epoch: 26| Step: 0
Training loss: 4.267486572265625
Validation loss: 3.4200230042139688

Epoch: 6| Step: 1
Training loss: 3.048222064971924
Validation loss: 3.4163220723470054

Epoch: 6| Step: 2
Training loss: 3.7708489894866943
Validation loss: 3.4123455286026

Epoch: 6| Step: 3
Training loss: 2.8710808753967285
Validation loss: 3.4084680477778115

Epoch: 6| Step: 4
Training loss: 3.638288974761963
Validation loss: 3.4039560556411743

Epoch: 6| Step: 5
Training loss: 3.0823018550872803
Validation loss: 3.3995078802108765

Epoch: 6| Step: 6
Training loss: 3.752499580383301
Validation loss: 3.395068605740865

Epoch: 6| Step: 7
Training loss: 3.4597439765930176
Validation loss: 3.3907928466796875

Epoch: 6| Step: 8
Training loss: 4.099235534667969
Validation loss: 3.3869169553120932

Epoch: 6| Step: 9
Training loss: 3.2828025817871094
Validation loss: 3.382050037384033

Epoch: 6| Step: 10
Training loss: 3.636589765548706
Validation loss: 3.377574324607849

Epoch: 6| Step: 11
Training loss: 3.7940561771392822
Validation loss: 3.3732340335845947

Epoch: 6| Step: 12
Training loss: 3.893430233001709
Validation loss: 3.3688714106877646

Epoch: 6| Step: 13
Training loss: 3.243488311767578
Validation loss: 3.364550789197286

Epoch: 27| Step: 0
Training loss: 3.770965337753296
Validation loss: 3.359976887702942

Epoch: 6| Step: 1
Training loss: 4.936855316162109
Validation loss: 3.355452378590902

Epoch: 6| Step: 2
Training loss: 2.790924310684204
Validation loss: 3.3511348168055215

Epoch: 6| Step: 3
Training loss: 4.308162689208984
Validation loss: 3.3470795154571533

Epoch: 6| Step: 4
Training loss: 3.5805320739746094
Validation loss: 3.3429588079452515

Epoch: 6| Step: 5
Training loss: 2.810494899749756
Validation loss: 3.3381473620732627

Epoch: 6| Step: 6
Training loss: 3.563351631164551
Validation loss: 3.3338825702667236

Epoch: 6| Step: 7
Training loss: 3.2177047729492188
Validation loss: 3.3292003870010376

Epoch: 6| Step: 8
Training loss: 3.6621952056884766
Validation loss: 3.3250235319137573

Epoch: 6| Step: 9
Training loss: 3.545449733734131
Validation loss: 3.3209062417348227

Epoch: 6| Step: 10
Training loss: 3.0153684616088867
Validation loss: 3.316701571146647

Epoch: 6| Step: 11
Training loss: 3.0823237895965576
Validation loss: 3.3126022815704346

Epoch: 6| Step: 12
Training loss: 2.955939292907715
Validation loss: 3.30841326713562

Epoch: 6| Step: 13
Training loss: 3.837864398956299
Validation loss: 3.3040448427200317

Epoch: 28| Step: 0
Training loss: 2.970874786376953
Validation loss: 3.2999586264292398

Epoch: 6| Step: 1
Training loss: 2.935295581817627
Validation loss: 3.2959356705347695

Epoch: 6| Step: 2
Training loss: 4.117490768432617
Validation loss: 3.291981339454651

Epoch: 6| Step: 3
Training loss: 3.7734487056732178
Validation loss: 3.2881571451822915

Epoch: 6| Step: 4
Training loss: 3.8019320964813232
Validation loss: 3.284383257230123

Epoch: 6| Step: 5
Training loss: 2.6145925521850586
Validation loss: 3.280713756879171

Epoch: 6| Step: 6
Training loss: 3.677217960357666
Validation loss: 3.2767892281214395

Epoch: 6| Step: 7
Training loss: 3.665841579437256
Validation loss: 3.272991975148519

Epoch: 6| Step: 8
Training loss: 3.999464750289917
Validation loss: 3.2691092093785605

Epoch: 6| Step: 9
Training loss: 3.7051587104797363
Validation loss: 3.265174627304077

Epoch: 6| Step: 10
Training loss: 3.5742263793945312
Validation loss: 3.261243462562561

Epoch: 6| Step: 11
Training loss: 2.499293327331543
Validation loss: 3.256892999013265

Epoch: 6| Step: 12
Training loss: 3.7572150230407715
Validation loss: 3.2528987328211465

Epoch: 6| Step: 13
Training loss: 3.2238612174987793
Validation loss: 3.248676339785258

Epoch: 29| Step: 0
Training loss: 4.007608413696289
Validation loss: 3.244806090990702

Epoch: 6| Step: 1
Training loss: 3.795201301574707
Validation loss: 3.2404708862304688

Epoch: 6| Step: 2
Training loss: 3.1336069107055664
Validation loss: 3.2365490992863974

Epoch: 6| Step: 3
Training loss: 3.2112507820129395
Validation loss: 3.2322870095570884

Epoch: 6| Step: 4
Training loss: 3.8591017723083496
Validation loss: 3.228408416112264

Epoch: 6| Step: 5
Training loss: 3.746211528778076
Validation loss: 3.224183758099874

Epoch: 6| Step: 6
Training loss: 2.86861515045166
Validation loss: 3.220309535662333

Epoch: 6| Step: 7
Training loss: 3.630011558532715
Validation loss: 3.2161779006322226

Epoch: 6| Step: 8
Training loss: 3.3734593391418457
Validation loss: 3.21195121606191

Epoch: 6| Step: 9
Training loss: 3.351555824279785
Validation loss: 3.207791248957316

Epoch: 6| Step: 10
Training loss: 3.434762716293335
Validation loss: 3.203662912050883

Epoch: 6| Step: 11
Training loss: 2.876265287399292
Validation loss: 3.1996689240137735

Epoch: 6| Step: 12
Training loss: 3.10443115234375
Validation loss: 3.19540003935496

Epoch: 6| Step: 13
Training loss: 3.2050483226776123
Validation loss: 3.191618243853251

Epoch: 30| Step: 0
Training loss: 2.8764803409576416
Validation loss: 3.1875303983688354

Epoch: 6| Step: 1
Training loss: 2.9710378646850586
Validation loss: 3.1838133732477822

Epoch: 6| Step: 2
Training loss: 3.6850383281707764
Validation loss: 3.1798285245895386

Epoch: 6| Step: 3
Training loss: 2.8492040634155273
Validation loss: 3.1761837005615234

Epoch: 6| Step: 4
Training loss: 2.823741912841797
Validation loss: 3.17258628209432

Epoch: 6| Step: 5
Training loss: 3.3662309646606445
Validation loss: 3.168658137321472

Epoch: 6| Step: 6
Training loss: 3.244385004043579
Validation loss: 3.16496209303538

Epoch: 6| Step: 7
Training loss: 3.221461772918701
Validation loss: 3.1609557469685874

Epoch: 6| Step: 8
Training loss: 3.652238368988037
Validation loss: 3.1570988098780313

Epoch: 6| Step: 9
Training loss: 3.4549918174743652
Validation loss: 3.153391202290853

Epoch: 6| Step: 10
Training loss: 3.3036410808563232
Validation loss: 3.149322787920634

Epoch: 6| Step: 11
Training loss: 3.284119129180908
Validation loss: 3.1456531286239624

Epoch: 6| Step: 12
Training loss: 4.355106830596924
Validation loss: 3.1420545975367227

Epoch: 6| Step: 13
Training loss: 3.764315605163574
Validation loss: 3.137847661972046

Epoch: 31| Step: 0
Training loss: 2.5558347702026367
Validation loss: 3.1337597370147705

Epoch: 6| Step: 1
Training loss: 3.4085307121276855
Validation loss: 3.13008181254069

Epoch: 6| Step: 2
Training loss: 3.1430881023406982
Validation loss: 3.125797708829244

Epoch: 6| Step: 3
Training loss: 3.622345447540283
Validation loss: 3.1221304337183633

Epoch: 6| Step: 4
Training loss: 3.4686694145202637
Validation loss: 3.1177199681599936

Epoch: 6| Step: 5
Training loss: 3.8931591510772705
Validation loss: 3.1138912439346313

Epoch: 6| Step: 6
Training loss: 3.168788194656372
Validation loss: 3.1097065607706704

Epoch: 6| Step: 7
Training loss: 3.4243264198303223
Validation loss: 3.1056153376897178

Epoch: 6| Step: 8
Training loss: 2.8612494468688965
Validation loss: 3.101536830266317

Epoch: 6| Step: 9
Training loss: 2.989250421524048
Validation loss: 3.097780466079712

Epoch: 6| Step: 10
Training loss: 3.659024238586426
Validation loss: 3.0937960942586265

Epoch: 6| Step: 11
Training loss: 2.9249472618103027
Validation loss: 3.090253154436747

Epoch: 6| Step: 12
Training loss: 3.823915958404541
Validation loss: 3.0869351625442505

Epoch: 6| Step: 13
Training loss: 3.22882342338562
Validation loss: 3.0834683179855347

Epoch: 32| Step: 0
Training loss: 3.4303698539733887
Validation loss: 3.0801777442296348

Epoch: 6| Step: 1
Training loss: 3.531670093536377
Validation loss: 3.076508641242981

Epoch: 6| Step: 2
Training loss: 3.34584903717041
Validation loss: 3.0728166500727334

Epoch: 6| Step: 3
Training loss: 3.530449867248535
Validation loss: 3.069322625796

Epoch: 6| Step: 4
Training loss: 3.5463709831237793
Validation loss: 3.0660831928253174

Epoch: 6| Step: 5
Training loss: 3.0875704288482666
Validation loss: 3.0621191263198853

Epoch: 6| Step: 6
Training loss: 3.0003621578216553
Validation loss: 3.0588636000951133

Epoch: 6| Step: 7
Training loss: 2.92924427986145
Validation loss: 3.0553393761316934

Epoch: 6| Step: 8
Training loss: 2.7300093173980713
Validation loss: 3.0517657995224

Epoch: 6| Step: 9
Training loss: 3.5591049194335938
Validation loss: 3.048292795817057

Epoch: 6| Step: 10
Training loss: 2.931823968887329
Validation loss: 3.04442822933197

Epoch: 6| Step: 11
Training loss: 3.460315704345703
Validation loss: 3.0412843227386475

Epoch: 6| Step: 12
Training loss: 3.4557459354400635
Validation loss: 3.0374625523885093

Epoch: 6| Step: 13
Training loss: 2.9516963958740234
Validation loss: 3.033894419670105

Epoch: 33| Step: 0
Training loss: 3.7254905700683594
Validation loss: 3.0303139289220176

Epoch: 6| Step: 1
Training loss: 3.2008397579193115
Validation loss: 3.0272534688313804

Epoch: 6| Step: 2
Training loss: 3.066959857940674
Validation loss: 3.023714820543925

Epoch: 6| Step: 3
Training loss: 3.069382905960083
Validation loss: 3.020460923512777

Epoch: 6| Step: 4
Training loss: 3.1097373962402344
Validation loss: 3.0167012214660645

Epoch: 6| Step: 5
Training loss: 2.848202705383301
Validation loss: 3.0131240288416543

Epoch: 6| Step: 6
Training loss: 2.748884677886963
Validation loss: 3.009661237398783

Epoch: 6| Step: 7
Training loss: 3.477102279663086
Validation loss: 3.0067745049794516

Epoch: 6| Step: 8
Training loss: 3.434905529022217
Validation loss: 3.003413359324137

Epoch: 6| Step: 9
Training loss: 3.201751947402954
Validation loss: 3.0002280871073403

Epoch: 6| Step: 10
Training loss: 2.8267745971679688
Validation loss: 2.9970054626464844

Epoch: 6| Step: 11
Training loss: 2.57785701751709
Validation loss: 2.99357279141744

Epoch: 6| Step: 12
Training loss: 3.2303121089935303
Validation loss: 2.990252653757731

Epoch: 6| Step: 13
Training loss: 4.323200225830078
Validation loss: 2.987132986386617

Epoch: 34| Step: 0
Training loss: 3.0604772567749023
Validation loss: 2.983554720878601

Epoch: 6| Step: 1
Training loss: 2.568363904953003
Validation loss: 2.9804155429204306

Epoch: 6| Step: 2
Training loss: 2.665743350982666
Validation loss: 2.9770397742589316

Epoch: 6| Step: 3
Training loss: 3.7550411224365234
Validation loss: 2.9739949703216553

Epoch: 6| Step: 4
Training loss: 2.8593668937683105
Validation loss: 2.97098841269811

Epoch: 6| Step: 5
Training loss: 3.605083465576172
Validation loss: 2.9676764806111655

Epoch: 6| Step: 6
Training loss: 3.482816219329834
Validation loss: 2.964276154836019

Epoch: 6| Step: 7
Training loss: 3.189910411834717
Validation loss: 2.9609127044677734

Epoch: 6| Step: 8
Training loss: 2.878471851348877
Validation loss: 2.9576261043548584

Epoch: 6| Step: 9
Training loss: 3.1565051078796387
Validation loss: 2.9544222354888916

Epoch: 6| Step: 10
Training loss: 3.7510647773742676
Validation loss: 2.9512507120768228

Epoch: 6| Step: 11
Training loss: 3.3590543270111084
Validation loss: 2.948040803273519

Epoch: 6| Step: 12
Training loss: 3.235032081604004
Validation loss: 2.944950302441915

Epoch: 6| Step: 13
Training loss: 2.6804676055908203
Validation loss: 2.941211700439453

Epoch: 35| Step: 0
Training loss: 2.6014702320098877
Validation loss: 2.9376815954844155

Epoch: 6| Step: 1
Training loss: 2.732560157775879
Validation loss: 2.9343861738840737

Epoch: 6| Step: 2
Training loss: 3.0539402961730957
Validation loss: 2.9311075607935586

Epoch: 6| Step: 3
Training loss: 2.9140172004699707
Validation loss: 2.92790695031484

Epoch: 6| Step: 4
Training loss: 3.238525867462158
Validation loss: 2.9249154329299927

Epoch: 6| Step: 5
Training loss: 3.9063985347747803
Validation loss: 2.921934167544047

Epoch: 6| Step: 6
Training loss: 3.3147215843200684
Validation loss: 2.918375094731649

Epoch: 6| Step: 7
Training loss: 2.737607955932617
Validation loss: 2.9153535763422647

Epoch: 6| Step: 8
Training loss: 3.1873040199279785
Validation loss: 2.91187846660614

Epoch: 6| Step: 9
Training loss: 3.099271297454834
Validation loss: 2.908866842587789

Epoch: 6| Step: 10
Training loss: 3.2413628101348877
Validation loss: 2.90590763092041

Epoch: 6| Step: 11
Training loss: 3.328219175338745
Validation loss: 2.902869939804077

Epoch: 6| Step: 12
Training loss: 2.848752737045288
Validation loss: 2.899690012137095

Epoch: 6| Step: 13
Training loss: 3.4855189323425293
Validation loss: 2.896657109260559

Epoch: 36| Step: 0
Training loss: 3.2459583282470703
Validation loss: 2.8933053016662598

Epoch: 6| Step: 1
Training loss: 3.0833334922790527
Validation loss: 2.8899588584899902

Epoch: 6| Step: 2
Training loss: 3.517301082611084
Validation loss: 2.8864095211029053

Epoch: 6| Step: 3
Training loss: 3.3611598014831543
Validation loss: 2.8827921946843467

Epoch: 6| Step: 4
Training loss: 2.4910643100738525
Validation loss: 2.8793253898620605

Epoch: 6| Step: 5
Training loss: 3.292670726776123
Validation loss: 2.8761618534723916

Epoch: 6| Step: 6
Training loss: 3.232038974761963
Validation loss: 2.872536023457845

Epoch: 6| Step: 7
Training loss: 3.9550037384033203
Validation loss: 2.8692687352498374

Epoch: 6| Step: 8
Training loss: 3.375746965408325
Validation loss: 2.865922490755717

Epoch: 6| Step: 9
Training loss: 2.8936381340026855
Validation loss: 2.862784206867218

Epoch: 6| Step: 10
Training loss: 2.870100498199463
Validation loss: 2.859412511189779

Epoch: 6| Step: 11
Training loss: 3.0459187030792236
Validation loss: 2.85607373714447

Epoch: 6| Step: 12
Training loss: 2.445559501647949
Validation loss: 2.853153944015503

Epoch: 6| Step: 13
Training loss: 2.3499302864074707
Validation loss: 2.8501893281936646

Epoch: 37| Step: 0
Training loss: 2.4640841484069824
Validation loss: 2.847378889719645

Epoch: 6| Step: 1
Training loss: 2.168458938598633
Validation loss: 2.8448370695114136

Epoch: 6| Step: 2
Training loss: 3.119600772857666
Validation loss: 2.842490077018738

Epoch: 6| Step: 3
Training loss: 3.0649290084838867
Validation loss: 2.839990973472595

Epoch: 6| Step: 4
Training loss: 2.941979169845581
Validation loss: 2.8371577660242715

Epoch: 6| Step: 5
Training loss: 3.4655017852783203
Validation loss: 2.834423820177714

Epoch: 6| Step: 6
Training loss: 2.27193021774292
Validation loss: 2.8311285972595215

Epoch: 6| Step: 7
Training loss: 2.603569746017456
Validation loss: 2.828009764353434

Epoch: 6| Step: 8
Training loss: 3.0896620750427246
Validation loss: 2.825123389561971

Epoch: 6| Step: 9
Training loss: 3.5859696865081787
Validation loss: 2.8222099939982095

Epoch: 6| Step: 10
Training loss: 3.1208462715148926
Validation loss: 2.8194665908813477

Epoch: 6| Step: 11
Training loss: 3.728910446166992
Validation loss: 2.816881616910299

Epoch: 6| Step: 12
Training loss: 3.2333950996398926
Validation loss: 2.814011057217916

Epoch: 6| Step: 13
Training loss: 3.7151622772216797
Validation loss: 2.811135689417521

Epoch: 38| Step: 0
Training loss: 3.015345335006714
Validation loss: 2.8083239793777466

Epoch: 6| Step: 1
Training loss: 3.203864574432373
Validation loss: 2.805283705393473

Epoch: 6| Step: 2
Training loss: 2.620293378829956
Validation loss: 2.802235802014669

Epoch: 6| Step: 3
Training loss: 3.348750114440918
Validation loss: 2.7996859550476074

Epoch: 6| Step: 4
Training loss: 3.5219178199768066
Validation loss: 2.7967047691345215

Epoch: 6| Step: 5
Training loss: 3.208334445953369
Validation loss: 2.7933579285939536

Epoch: 6| Step: 6
Training loss: 2.714573621749878
Validation loss: 2.7900906801223755

Epoch: 6| Step: 7
Training loss: 3.1225638389587402
Validation loss: 2.7868009010950723

Epoch: 6| Step: 8
Training loss: 2.8563385009765625
Validation loss: 2.783766825993856

Epoch: 6| Step: 9
Training loss: 2.590214967727661
Validation loss: 2.7804665168126426

Epoch: 6| Step: 10
Training loss: 2.906604766845703
Validation loss: 2.7774356404940286

Epoch: 6| Step: 11
Training loss: 3.2116518020629883
Validation loss: 2.7743863264719644

Epoch: 6| Step: 12
Training loss: 2.5318331718444824
Validation loss: 2.771588126818339

Epoch: 6| Step: 13
Training loss: 3.217240810394287
Validation loss: 2.7688349882761636

Epoch: 39| Step: 0
Training loss: 3.2829253673553467
Validation loss: 2.766443133354187

Epoch: 6| Step: 1
Training loss: 2.808323383331299
Validation loss: 2.763850371042887

Epoch: 6| Step: 2
Training loss: 3.3877639770507812
Validation loss: 2.7611718575159707

Epoch: 6| Step: 3
Training loss: 3.213627815246582
Validation loss: 2.758609493573507

Epoch: 6| Step: 4
Training loss: 2.5974974632263184
Validation loss: 2.7555952866872153

Epoch: 6| Step: 5
Training loss: 2.9173641204833984
Validation loss: 2.7529231309890747

Epoch: 6| Step: 6
Training loss: 3.0686707496643066
Validation loss: 2.7497783104578652

Epoch: 6| Step: 7
Training loss: 3.4070935249328613
Validation loss: 2.747087995211283

Epoch: 6| Step: 8
Training loss: 3.674776554107666
Validation loss: 2.744130293528239

Epoch: 6| Step: 9
Training loss: 2.217353343963623
Validation loss: 2.7415027618408203

Epoch: 6| Step: 10
Training loss: 2.7752108573913574
Validation loss: 2.7387468020121255

Epoch: 6| Step: 11
Training loss: 2.478917121887207
Validation loss: 2.7361146608988443

Epoch: 6| Step: 12
Training loss: 2.357511043548584
Validation loss: 2.7336217164993286

Epoch: 6| Step: 13
Training loss: 3.293497085571289
Validation loss: 2.731316606203715

Epoch: 40| Step: 0
Training loss: 2.558657646179199
Validation loss: 2.7286972204844155

Epoch: 6| Step: 1
Training loss: 3.458601951599121
Validation loss: 2.726276238759359

Epoch: 6| Step: 2
Training loss: 2.667037010192871
Validation loss: 2.723839362462362

Epoch: 6| Step: 3
Training loss: 2.746805429458618
Validation loss: 2.721288720766703

Epoch: 6| Step: 4
Training loss: 2.4577767848968506
Validation loss: 2.7187501192092896

Epoch: 6| Step: 5
Training loss: 3.2138054370880127
Validation loss: 2.716131567955017

Epoch: 6| Step: 6
Training loss: 2.9966330528259277
Validation loss: 2.7135923306147256

Epoch: 6| Step: 7
Training loss: 2.8180899620056152
Validation loss: 2.7113030354181924

Epoch: 6| Step: 8
Training loss: 3.4631354808807373
Validation loss: 2.7085474332173667

Epoch: 6| Step: 9
Training loss: 3.421431541442871
Validation loss: 2.7063412268956504

Epoch: 6| Step: 10
Training loss: 2.2910585403442383
Validation loss: 2.7036152283350625

Epoch: 6| Step: 11
Training loss: 3.116814136505127
Validation loss: 2.7004560629526773

Epoch: 6| Step: 12
Training loss: 3.38364839553833
Validation loss: 2.6982715129852295

Epoch: 6| Step: 13
Training loss: 2.294832468032837
Validation loss: 2.6953535079956055

Epoch: 41| Step: 0
Training loss: 3.223902463912964
Validation loss: 2.692560354868571

Epoch: 6| Step: 1
Training loss: 3.016080617904663
Validation loss: 2.689655303955078

Epoch: 6| Step: 2
Training loss: 3.1601672172546387
Validation loss: 2.687148332595825

Epoch: 6| Step: 3
Training loss: 2.40085506439209
Validation loss: 2.684098800023397

Epoch: 6| Step: 4
Training loss: 3.70658802986145
Validation loss: 2.6808262268702188

Epoch: 6| Step: 5
Training loss: 2.2954041957855225
Validation loss: 2.6783230702082315

Epoch: 6| Step: 6
Training loss: 3.0403690338134766
Validation loss: 2.6755583683649697

Epoch: 6| Step: 7
Training loss: 2.401543378829956
Validation loss: 2.672671318054199

Epoch: 6| Step: 8
Training loss: 2.9180002212524414
Validation loss: 2.6696521043777466

Epoch: 6| Step: 9
Training loss: 3.381420135498047
Validation loss: 2.666852831840515

Epoch: 6| Step: 10
Training loss: 2.7032923698425293
Validation loss: 2.6640388568242392

Epoch: 6| Step: 11
Training loss: 2.095986843109131
Validation loss: 2.6614923079808555

Epoch: 6| Step: 12
Training loss: 2.3001656532287598
Validation loss: 2.658767580986023

Epoch: 6| Step: 13
Training loss: 3.7078566551208496
Validation loss: 2.6561327377955117

Epoch: 42| Step: 0
Training loss: 2.5494425296783447
Validation loss: 2.6537532806396484

Epoch: 6| Step: 1
Training loss: 3.209362030029297
Validation loss: 2.6510213017463684

Epoch: 6| Step: 2
Training loss: 2.7969119548797607
Validation loss: 2.648648977279663

Epoch: 6| Step: 3
Training loss: 2.907196044921875
Validation loss: 2.645603895187378

Epoch: 6| Step: 4
Training loss: 3.1992270946502686
Validation loss: 2.6427714824676514

Epoch: 6| Step: 5
Training loss: 2.711474895477295
Validation loss: 2.639916698137919

Epoch: 6| Step: 6
Training loss: 2.891465663909912
Validation loss: 2.6370848019917807

Epoch: 6| Step: 7
Training loss: 3.073073625564575
Validation loss: 2.634593407313029

Epoch: 6| Step: 8
Training loss: 2.31593656539917
Validation loss: 2.6312269369761148

Epoch: 6| Step: 9
Training loss: 2.89703369140625
Validation loss: 2.6278058687845864

Epoch: 6| Step: 10
Training loss: 2.430180549621582
Validation loss: 2.626342753569285

Epoch: 6| Step: 11
Training loss: 2.901664972305298
Validation loss: 2.622068921724955

Epoch: 6| Step: 12
Training loss: 3.0058624744415283
Validation loss: 2.620599389076233

Epoch: 6| Step: 13
Training loss: 2.8927016258239746
Validation loss: 2.6166532039642334

Epoch: 43| Step: 0
Training loss: 3.495469570159912
Validation loss: 2.6151952544848123

Epoch: 6| Step: 1
Training loss: 3.0248911380767822
Validation loss: 2.612157464027405

Epoch: 6| Step: 2
Training loss: 2.407572031021118
Validation loss: 2.609947224458059

Epoch: 6| Step: 3
Training loss: 2.7352285385131836
Validation loss: 2.6074474453926086

Epoch: 6| Step: 4
Training loss: 2.0797743797302246
Validation loss: 2.604665915171305

Epoch: 6| Step: 5
Training loss: 3.6474387645721436
Validation loss: 2.6022650003433228

Epoch: 6| Step: 6
Training loss: 2.614943504333496
Validation loss: 2.5993384520212808

Epoch: 6| Step: 7
Training loss: 2.53102445602417
Validation loss: 2.596653918425242

Epoch: 6| Step: 8
Training loss: 2.4362897872924805
Validation loss: 2.5939694245656333

Epoch: 6| Step: 9
Training loss: 2.9456653594970703
Validation loss: 2.591208537419637

Epoch: 6| Step: 10
Training loss: 2.571183204650879
Validation loss: 2.58845845858256

Epoch: 6| Step: 11
Training loss: 3.2177844047546387
Validation loss: 2.5855202277501426

Epoch: 6| Step: 12
Training loss: 2.7906339168548584
Validation loss: 2.5826427141825357

Epoch: 6| Step: 13
Training loss: 2.737828254699707
Validation loss: 2.580578168233236

Epoch: 44| Step: 0
Training loss: 3.257584571838379
Validation loss: 2.5779778560002646

Epoch: 6| Step: 1
Training loss: 3.111182689666748
Validation loss: 2.578200022379557

Epoch: 6| Step: 2
Training loss: 2.308096408843994
Validation loss: 2.5723490715026855

Epoch: 6| Step: 3
Training loss: 2.2561466693878174
Validation loss: 2.5761537154515586

Epoch: 6| Step: 4
Training loss: 2.407457113265991
Validation loss: 2.5667791763941445

Epoch: 6| Step: 5
Training loss: 2.8851990699768066
Validation loss: 2.567529042561849

Epoch: 6| Step: 6
Training loss: 2.9907007217407227
Validation loss: 2.5666436354319253

Epoch: 6| Step: 7
Training loss: 2.926654100418091
Validation loss: 2.5663315852483115

Epoch: 6| Step: 8
Training loss: 3.0022120475769043
Validation loss: 2.5687275330225625

Epoch: 6| Step: 9
Training loss: 2.541550397872925
Validation loss: 2.5688755909601846

Epoch: 6| Step: 10
Training loss: 2.750741958618164
Validation loss: 2.5641098419825235

Epoch: 6| Step: 11
Training loss: 2.7055726051330566
Validation loss: 2.558236320813497

Epoch: 6| Step: 12
Training loss: 3.3315558433532715
Validation loss: 2.55402139822642

Epoch: 6| Step: 13
Training loss: 2.253582239151001
Validation loss: 2.55002228418986

Epoch: 45| Step: 0
Training loss: 3.149613380432129
Validation loss: 2.5463008880615234

Epoch: 6| Step: 1
Training loss: 3.0415151119232178
Validation loss: 2.5439008871714273

Epoch: 6| Step: 2
Training loss: 2.6972196102142334
Validation loss: 2.5407010316848755

Epoch: 6| Step: 3
Training loss: 2.936311960220337
Validation loss: 2.5374563137690225

Epoch: 6| Step: 4
Training loss: 2.5812883377075195
Validation loss: 2.5346343715985618

Epoch: 6| Step: 5
Training loss: 3.0078799724578857
Validation loss: 2.5308661460876465

Epoch: 6| Step: 6
Training loss: 2.348681688308716
Validation loss: 2.5296711126963296

Epoch: 6| Step: 7
Training loss: 1.6599501371383667
Validation loss: 2.5281715393066406

Epoch: 6| Step: 8
Training loss: 2.896775722503662
Validation loss: 2.5256044467290244

Epoch: 6| Step: 9
Training loss: 3.029264450073242
Validation loss: 2.5226099491119385

Epoch: 6| Step: 10
Training loss: 1.9571021795272827
Validation loss: 2.519402861595154

Epoch: 6| Step: 11
Training loss: 2.509655237197876
Validation loss: 2.5172536770502725

Epoch: 6| Step: 12
Training loss: 3.206615686416626
Validation loss: 2.514455954233805

Epoch: 6| Step: 13
Training loss: 3.1725106239318848
Validation loss: 2.51214329401652

Epoch: 46| Step: 0
Training loss: 3.0282187461853027
Validation loss: 2.5102140506108603

Epoch: 6| Step: 1
Training loss: 2.4248743057250977
Validation loss: 2.507663369178772

Epoch: 6| Step: 2
Training loss: 2.878969669342041
Validation loss: 2.5050305128097534

Epoch: 6| Step: 3
Training loss: 2.7764999866485596
Validation loss: 2.5067256689071655

Epoch: 6| Step: 4
Training loss: 2.558351516723633
Validation loss: 2.50184432665507

Epoch: 6| Step: 5
Training loss: 2.7081665992736816
Validation loss: 2.4949833949406943

Epoch: 6| Step: 6
Training loss: 2.787914991378784
Validation loss: 2.4951322277386985

Epoch: 6| Step: 7
Training loss: 2.4885501861572266
Validation loss: 2.4936074018478394

Epoch: 6| Step: 8
Training loss: 2.832648515701294
Validation loss: 2.4938586950302124

Epoch: 6| Step: 9
Training loss: 2.895240306854248
Validation loss: 2.4915364185969033

Epoch: 6| Step: 10
Training loss: 2.9953718185424805
Validation loss: 2.4882442156473794

Epoch: 6| Step: 11
Training loss: 1.9970146417617798
Validation loss: 2.4871090253194175

Epoch: 6| Step: 12
Training loss: 2.6138691902160645
Validation loss: 2.483272115389506

Epoch: 6| Step: 13
Training loss: 2.6791749000549316
Validation loss: 2.4802263180414834

Epoch: 47| Step: 0
Training loss: 2.4932894706726074
Validation loss: 2.4770583311716714

Epoch: 6| Step: 1
Training loss: 2.740487813949585
Validation loss: 2.4749929904937744

Epoch: 6| Step: 2
Training loss: 2.4695920944213867
Validation loss: 2.472135623296102

Epoch: 6| Step: 3
Training loss: 2.585177421569824
Validation loss: 2.467135469118754

Epoch: 6| Step: 4
Training loss: 2.5567455291748047
Validation loss: 2.4651571114857993

Epoch: 6| Step: 5
Training loss: 2.7625203132629395
Validation loss: 2.4614789485931396

Epoch: 6| Step: 6
Training loss: 2.990403175354004
Validation loss: 2.457942565282186

Epoch: 6| Step: 7
Training loss: 2.2953877449035645
Validation loss: 2.4560203552246094

Epoch: 6| Step: 8
Training loss: 3.2839717864990234
Validation loss: 2.452375849088033

Epoch: 6| Step: 9
Training loss: 2.344196081161499
Validation loss: 2.457639495531718

Epoch: 6| Step: 10
Training loss: 2.9897522926330566
Validation loss: 2.4563995997111

Epoch: 6| Step: 11
Training loss: 2.326052665710449
Validation loss: 2.4448396364847818

Epoch: 6| Step: 12
Training loss: 2.2488718032836914
Validation loss: 2.4425265192985535

Epoch: 6| Step: 13
Training loss: 3.0853543281555176
Validation loss: 2.443256417910258

Epoch: 48| Step: 0
Training loss: 3.7126364707946777
Validation loss: 2.443336804707845

Epoch: 6| Step: 1
Training loss: 2.1222195625305176
Validation loss: 2.4432632525761924

Epoch: 6| Step: 2
Training loss: 2.799966335296631
Validation loss: 2.444239298502604

Epoch: 6| Step: 3
Training loss: 2.124446392059326
Validation loss: 2.443491737047831

Epoch: 6| Step: 4
Training loss: 2.9103147983551025
Validation loss: 2.4431018034617105

Epoch: 6| Step: 5
Training loss: 3.4287054538726807
Validation loss: 2.442501743634542

Epoch: 6| Step: 6
Training loss: 2.1765716075897217
Validation loss: 2.4392491579055786

Epoch: 6| Step: 7
Training loss: 1.878645896911621
Validation loss: 2.4324286381403604

Epoch: 6| Step: 8
Training loss: 2.862049102783203
Validation loss: 2.428462545077006

Epoch: 6| Step: 9
Training loss: 1.9120372533798218
Validation loss: 2.4248076677322388

Epoch: 6| Step: 10
Training loss: 3.1286821365356445
Validation loss: 2.4228487412134805

Epoch: 6| Step: 11
Training loss: 3.0645365715026855
Validation loss: 2.4179738759994507

Epoch: 6| Step: 12
Training loss: 2.4688608646392822
Validation loss: 2.4153301318486533

Epoch: 6| Step: 13
Training loss: 2.0717649459838867
Validation loss: 2.4084169268608093

Epoch: 49| Step: 0
Training loss: 2.5916411876678467
Validation loss: 2.4046620527903237

Epoch: 6| Step: 1
Training loss: 2.6622886657714844
Validation loss: 2.4080460468928018

Epoch: 6| Step: 2
Training loss: 2.360849618911743
Validation loss: 2.4111313025156655

Epoch: 6| Step: 3
Training loss: 2.7460312843322754
Validation loss: 2.4144442876180015

Epoch: 6| Step: 4
Training loss: 2.530848264694214
Validation loss: 2.40324076016744

Epoch: 6| Step: 5
Training loss: 3.0632948875427246
Validation loss: 2.3919785420099893

Epoch: 6| Step: 6
Training loss: 2.774444818496704
Validation loss: 2.393964926401774

Epoch: 6| Step: 7
Training loss: 2.821873188018799
Validation loss: 2.3962883949279785

Epoch: 6| Step: 8
Training loss: 2.6924805641174316
Validation loss: 2.3999860684076944

Epoch: 6| Step: 9
Training loss: 2.7468855381011963
Validation loss: 2.406642436981201

Epoch: 6| Step: 10
Training loss: 1.948617696762085
Validation loss: 2.4094631671905518

Epoch: 6| Step: 11
Training loss: 2.7292821407318115
Validation loss: 2.4269872903823853

Epoch: 6| Step: 12
Training loss: 2.3930563926696777
Validation loss: 2.4267634948094687

Epoch: 6| Step: 13
Training loss: 2.3123226165771484
Validation loss: 2.4314083655675254

Epoch: 50| Step: 0
Training loss: 2.6131410598754883
Validation loss: 2.4184738794962564

Epoch: 6| Step: 1
Training loss: 2.6563291549682617
Validation loss: 2.4084678093592324

Epoch: 6| Step: 2
Training loss: 2.185811996459961
Validation loss: 2.406111240386963

Epoch: 6| Step: 3
Training loss: 2.5811219215393066
Validation loss: 2.399903337160746

Epoch: 6| Step: 4
Training loss: 2.695986270904541
Validation loss: 2.3966955145200095

Epoch: 6| Step: 5
Training loss: 2.1937341690063477
Validation loss: 2.4035985469818115

Epoch: 6| Step: 6
Training loss: 3.1239871978759766
Validation loss: 2.398080309232076

Epoch: 6| Step: 7
Training loss: 2.65140962600708
Validation loss: 2.4281643629074097

Epoch: 6| Step: 8
Training loss: 2.2526471614837646
Validation loss: 2.4305848677953086

Epoch: 6| Step: 9
Training loss: 2.471099853515625
Validation loss: 2.415503760178884

Epoch: 6| Step: 10
Training loss: 2.3897171020507812
Validation loss: 2.3815863927205405

Epoch: 6| Step: 11
Training loss: 3.318936824798584
Validation loss: 2.384387175242106

Epoch: 6| Step: 12
Training loss: 2.486325263977051
Validation loss: 2.3783854246139526

Epoch: 6| Step: 13
Training loss: 2.616971254348755
Validation loss: 2.376480837663015

Epoch: 51| Step: 0
Training loss: 2.848966121673584
Validation loss: 2.3753042221069336

Epoch: 6| Step: 1
Training loss: 2.7701432704925537
Validation loss: 2.3759529987970986

Epoch: 6| Step: 2
Training loss: 2.8354902267456055
Validation loss: 2.3683046102523804

Epoch: 6| Step: 3
Training loss: 2.031813144683838
Validation loss: 2.3645335833231607

Epoch: 6| Step: 4
Training loss: 2.7401392459869385
Validation loss: 2.3620956937472024

Epoch: 6| Step: 5
Training loss: 3.0839216709136963
Validation loss: 2.3623319268226624

Epoch: 6| Step: 6
Training loss: 2.294048547744751
Validation loss: 2.360908846060435

Epoch: 6| Step: 7
Training loss: 2.504218101501465
Validation loss: 2.3731391628583274

Epoch: 6| Step: 8
Training loss: 2.3199868202209473
Validation loss: 2.380014697710673

Epoch: 6| Step: 9
Training loss: 2.983058452606201
Validation loss: 2.373063325881958

Epoch: 6| Step: 10
Training loss: 2.332420825958252
Validation loss: 2.349817673365275

Epoch: 6| Step: 11
Training loss: 2.1102404594421387
Validation loss: 2.333220422267914

Epoch: 6| Step: 12
Training loss: 2.291057825088501
Validation loss: 2.3206308285395303

Epoch: 6| Step: 13
Training loss: 2.5221972465515137
Validation loss: 2.315543989340464

Epoch: 52| Step: 0
Training loss: 2.0876426696777344
Validation loss: 2.317591667175293

Epoch: 6| Step: 1
Training loss: 2.3289599418640137
Validation loss: 2.322316606839498

Epoch: 6| Step: 2
Training loss: 2.8071045875549316
Validation loss: 2.326530377070109

Epoch: 6| Step: 3
Training loss: 2.2373366355895996
Validation loss: 2.326424777507782

Epoch: 6| Step: 4
Training loss: 2.117020606994629
Validation loss: 2.328791856765747

Epoch: 6| Step: 5
Training loss: 2.4431536197662354
Validation loss: 2.321185350418091

Epoch: 6| Step: 6
Training loss: 2.389403820037842
Validation loss: 2.316945791244507

Epoch: 6| Step: 7
Training loss: 2.6842072010040283
Validation loss: 2.312101721763611

Epoch: 6| Step: 8
Training loss: 1.925286889076233
Validation loss: 2.308924674987793

Epoch: 6| Step: 9
Training loss: 2.6253790855407715
Validation loss: 2.3058258493741355

Epoch: 6| Step: 10
Training loss: 2.3760201930999756
Validation loss: 2.2989923556645713

Epoch: 6| Step: 11
Training loss: 3.1060361862182617
Validation loss: 2.2914726932843528

Epoch: 6| Step: 12
Training loss: 2.349299192428589
Validation loss: 2.288346588611603

Epoch: 6| Step: 13
Training loss: 3.244410514831543
Validation loss: 2.2909895976384482

Epoch: 53| Step: 0
Training loss: 2.782203435897827
Validation loss: 2.2979390621185303

Epoch: 6| Step: 1
Training loss: 2.423048973083496
Validation loss: 2.3045341968536377

Epoch: 6| Step: 2
Training loss: 2.4194459915161133
Validation loss: 2.301428437232971

Epoch: 6| Step: 3
Training loss: 2.8749938011169434
Validation loss: 2.295949419339498

Epoch: 6| Step: 4
Training loss: 2.033046007156372
Validation loss: 2.297402501106262

Epoch: 6| Step: 5
Training loss: 2.0158801078796387
Validation loss: 2.298253377278646

Epoch: 6| Step: 6
Training loss: 2.198350429534912
Validation loss: 2.28561802705129

Epoch: 6| Step: 7
Training loss: 2.5500450134277344
Validation loss: 2.2721974651018777

Epoch: 6| Step: 8
Training loss: 2.5079827308654785
Validation loss: 2.2738202810287476

Epoch: 6| Step: 9
Training loss: 2.103394031524658
Validation loss: 2.278219143549601

Epoch: 6| Step: 10
Training loss: 2.378957748413086
Validation loss: 2.2850904067357383

Epoch: 6| Step: 11
Training loss: 2.514676570892334
Validation loss: 2.2842044830322266

Epoch: 6| Step: 12
Training loss: 3.2515811920166016
Validation loss: 2.277932047843933

Epoch: 6| Step: 13
Training loss: 2.469080924987793
Validation loss: 2.2700424591700235

Epoch: 54| Step: 0
Training loss: 2.396796703338623
Validation loss: 2.2628188927968345

Epoch: 6| Step: 1
Training loss: 2.626955032348633
Validation loss: 2.25874662399292

Epoch: 6| Step: 2
Training loss: 3.011471748352051
Validation loss: 2.252131481965383

Epoch: 6| Step: 3
Training loss: 2.10745906829834
Validation loss: 2.2492371797561646

Epoch: 6| Step: 4
Training loss: 2.154360771179199
Validation loss: 2.2491738001505532

Epoch: 6| Step: 5
Training loss: 1.9542003870010376
Validation loss: 2.2426766753196716

Epoch: 6| Step: 6
Training loss: 2.793914318084717
Validation loss: 2.2456397215525308

Epoch: 6| Step: 7
Training loss: 2.1814966201782227
Validation loss: 2.2427454789479575

Epoch: 6| Step: 8
Training loss: 2.489657402038574
Validation loss: 2.244048317273458

Epoch: 6| Step: 9
Training loss: 2.357490062713623
Validation loss: 2.247589627901713

Epoch: 6| Step: 10
Training loss: 2.8936493396759033
Validation loss: 2.2547007401784263

Epoch: 6| Step: 11
Training loss: 1.9060230255126953
Validation loss: 2.2486931880315146

Epoch: 6| Step: 12
Training loss: 2.356843948364258
Validation loss: 2.2388808727264404

Epoch: 6| Step: 13
Training loss: 2.5448198318481445
Validation loss: 2.2324875593185425

Epoch: 55| Step: 0
Training loss: 1.649786353111267
Validation loss: 2.2281176249186196

Epoch: 6| Step: 1
Training loss: 2.466729164123535
Validation loss: 2.229959468046824

Epoch: 6| Step: 2
Training loss: 1.9181780815124512
Validation loss: 2.228258788585663

Epoch: 6| Step: 3
Training loss: 2.20035719871521
Validation loss: 2.2275940577189126

Epoch: 6| Step: 4
Training loss: 2.8834116458892822
Validation loss: 2.224756936232249

Epoch: 6| Step: 5
Training loss: 2.6361308097839355
Validation loss: 2.2236066261927285

Epoch: 6| Step: 6
Training loss: 2.229701280593872
Validation loss: 2.2213629881540933

Epoch: 6| Step: 7
Training loss: 2.5517776012420654
Validation loss: 2.2211050192515054

Epoch: 6| Step: 8
Training loss: 2.4960784912109375
Validation loss: 2.2167879144350686

Epoch: 6| Step: 9
Training loss: 2.473940372467041
Validation loss: 2.2169744968414307

Epoch: 6| Step: 10
Training loss: 2.113539457321167
Validation loss: 2.213001847267151

Epoch: 6| Step: 11
Training loss: 2.0192160606384277
Validation loss: 2.2100401520729065

Epoch: 6| Step: 12
Training loss: 3.3663103580474854
Validation loss: 2.206234931945801

Epoch: 6| Step: 13
Training loss: 2.4072508811950684
Validation loss: 2.2061038812001548

Epoch: 56| Step: 0
Training loss: 1.4760472774505615
Validation loss: 2.2002095580101013

Epoch: 6| Step: 1
Training loss: 1.778913140296936
Validation loss: 2.205263157685598

Epoch: 6| Step: 2
Training loss: 2.143587589263916
Validation loss: 2.199671963850657

Epoch: 6| Step: 3
Training loss: 2.771209716796875
Validation loss: 2.1972007552782693

Epoch: 6| Step: 4
Training loss: 2.734520435333252
Validation loss: 2.1945854822794595

Epoch: 6| Step: 5
Training loss: 2.758765697479248
Validation loss: 2.1937885681788125

Epoch: 6| Step: 6
Training loss: 2.0997796058654785
Validation loss: 2.1933947006861367

Epoch: 6| Step: 7
Training loss: 1.8652558326721191
Validation loss: 2.1911586920420327

Epoch: 6| Step: 8
Training loss: 2.295929193496704
Validation loss: 2.1932938496271768

Epoch: 6| Step: 9
Training loss: 2.9395768642425537
Validation loss: 2.1882073879241943

Epoch: 6| Step: 10
Training loss: 2.1470935344696045
Validation loss: 2.1907185316085815

Epoch: 6| Step: 11
Training loss: 2.1904876232147217
Validation loss: 2.1901267170906067

Epoch: 6| Step: 12
Training loss: 2.8847498893737793
Validation loss: 2.1856176058451333

Epoch: 6| Step: 13
Training loss: 2.872979164123535
Validation loss: 2.1842556397120156

Epoch: 57| Step: 0
Training loss: 2.771556854248047
Validation loss: 2.1862755616505942

Epoch: 6| Step: 1
Training loss: 2.0490989685058594
Validation loss: 2.188301702340444

Epoch: 6| Step: 2
Training loss: 2.3653483390808105
Validation loss: 2.1877238551775613

Epoch: 6| Step: 3
Training loss: 2.3758034706115723
Validation loss: 2.186858654022217

Epoch: 6| Step: 4
Training loss: 2.2062244415283203
Validation loss: 2.19072425365448

Epoch: 6| Step: 5
Training loss: 2.5002810955047607
Validation loss: 2.187355359395345

Epoch: 6| Step: 6
Training loss: 2.4564857482910156
Validation loss: 2.185751954714457

Epoch: 6| Step: 7
Training loss: 2.2331666946411133
Validation loss: 2.1838670571645102

Epoch: 6| Step: 8
Training loss: 2.144524097442627
Validation loss: 2.1783469319343567

Epoch: 6| Step: 9
Training loss: 1.9968730211257935
Validation loss: 2.174301028251648

Epoch: 6| Step: 10
Training loss: 2.0628907680511475
Validation loss: 2.1772145430246987

Epoch: 6| Step: 11
Training loss: 2.4331915378570557
Validation loss: 2.1757229566574097

Epoch: 6| Step: 12
Training loss: 2.544921398162842
Validation loss: 2.1702471176783242

Epoch: 6| Step: 13
Training loss: 2.655963659286499
Validation loss: 2.1681851148605347

Epoch: 58| Step: 0
Training loss: 2.380028247833252
Validation loss: 2.1621695359547934

Epoch: 6| Step: 1
Training loss: 2.423025131225586
Validation loss: 2.167197306950887

Epoch: 6| Step: 2
Training loss: 2.427664279937744
Validation loss: 2.1678212881088257

Epoch: 6| Step: 3
Training loss: 2.342493772506714
Validation loss: 2.161600569883982

Epoch: 6| Step: 4
Training loss: 2.2589292526245117
Validation loss: 2.164901932080587

Epoch: 6| Step: 5
Training loss: 2.909403085708618
Validation loss: 2.160473088423411

Epoch: 6| Step: 6
Training loss: 2.8428187370300293
Validation loss: 2.1567988197008767

Epoch: 6| Step: 7
Training loss: 2.3558177947998047
Validation loss: 2.1634276310602822

Epoch: 6| Step: 8
Training loss: 2.718595027923584
Validation loss: 2.1634361147880554

Epoch: 6| Step: 9
Training loss: 1.9058960676193237
Validation loss: 2.1605968674023948

Epoch: 6| Step: 10
Training loss: 2.1584935188293457
Validation loss: 2.158540149529775

Epoch: 6| Step: 11
Training loss: 1.594254732131958
Validation loss: 2.1569904883702598

Epoch: 6| Step: 12
Training loss: 1.6928009986877441
Validation loss: 2.15506774187088

Epoch: 6| Step: 13
Training loss: 2.4300174713134766
Validation loss: 2.1494038303693137

Epoch: 59| Step: 0
Training loss: 2.5540966987609863
Validation loss: 2.1531779368718467

Epoch: 6| Step: 1
Training loss: 2.443297863006592
Validation loss: 2.145439942677816

Epoch: 6| Step: 2
Training loss: 2.1208226680755615
Validation loss: 2.140476663907369

Epoch: 6| Step: 3
Training loss: 1.886029839515686
Validation loss: 2.146345535914103

Epoch: 6| Step: 4
Training loss: 2.601482391357422
Validation loss: 2.144176443417867

Epoch: 6| Step: 5
Training loss: 1.904807209968567
Validation loss: 2.135513663291931

Epoch: 6| Step: 6
Training loss: 2.0578420162200928
Validation loss: 2.139379560947418

Epoch: 6| Step: 7
Training loss: 2.4716122150421143
Validation loss: 2.1366788744926453

Epoch: 6| Step: 8
Training loss: 2.4445018768310547
Validation loss: 2.137068827946981

Epoch: 6| Step: 9
Training loss: 2.3909521102905273
Validation loss: 2.1344275871912637

Epoch: 6| Step: 10
Training loss: 2.074753999710083
Validation loss: 2.1332208116849265

Epoch: 6| Step: 11
Training loss: 2.219407081604004
Validation loss: 2.137242396672567

Epoch: 6| Step: 12
Training loss: 2.5513598918914795
Validation loss: 2.164278229077657

Epoch: 6| Step: 13
Training loss: 2.802534341812134
Validation loss: 2.147505303223928

Epoch: 60| Step: 0
Training loss: 1.7045214176177979
Validation loss: 2.133900006612142

Epoch: 6| Step: 1
Training loss: 2.26281476020813
Validation loss: 2.132061163584391

Epoch: 6| Step: 2
Training loss: 1.8922456502914429
Validation loss: 2.1277227799097695

Epoch: 6| Step: 3
Training loss: 2.6394801139831543
Validation loss: 2.1326589783032737

Epoch: 6| Step: 4
Training loss: 2.032667398452759
Validation loss: 2.1320852041244507

Epoch: 6| Step: 5
Training loss: 2.5683116912841797
Validation loss: 2.1394441525141397

Epoch: 6| Step: 6
Training loss: 1.6810017824172974
Validation loss: 2.136151055494944

Epoch: 6| Step: 7
Training loss: 2.3646116256713867
Validation loss: 2.137868881225586

Epoch: 6| Step: 8
Training loss: 3.062755823135376
Validation loss: 2.1412270863850913

Epoch: 6| Step: 9
Training loss: 2.470700263977051
Validation loss: 2.1384136080741882

Epoch: 6| Step: 10
Training loss: 2.2913780212402344
Validation loss: 2.1383458375930786

Epoch: 6| Step: 11
Training loss: 2.2411413192749023
Validation loss: 2.1398771603902182

Epoch: 6| Step: 12
Training loss: 2.8555755615234375
Validation loss: 2.1388273437817893

Epoch: 6| Step: 13
Training loss: 2.0944652557373047
Validation loss: 2.137107034524282

Epoch: 61| Step: 0
Training loss: 1.8113048076629639
Validation loss: 2.1339375575383506

Epoch: 6| Step: 1
Training loss: 2.4507761001586914
Validation loss: 2.1320914030075073

Epoch: 6| Step: 2
Training loss: 2.322394847869873
Validation loss: 2.1296186447143555

Epoch: 6| Step: 3
Training loss: 2.1975879669189453
Validation loss: 2.123914579550425

Epoch: 6| Step: 4
Training loss: 2.3743882179260254
Validation loss: 2.106892466545105

Epoch: 6| Step: 5
Training loss: 2.4362587928771973
Validation loss: 2.1097504099210105

Epoch: 6| Step: 6
Training loss: 2.130945920944214
Validation loss: 2.1054256161053977

Epoch: 6| Step: 7
Training loss: 1.9019076824188232
Validation loss: 2.1068450808525085

Epoch: 6| Step: 8
Training loss: 2.3422932624816895
Validation loss: 2.1026559869448342

Epoch: 6| Step: 9
Training loss: 2.5803370475769043
Validation loss: 2.109220584233602

Epoch: 6| Step: 10
Training loss: 2.359010934829712
Validation loss: 2.118491768836975

Epoch: 6| Step: 11
Training loss: 2.035588502883911
Validation loss: 2.1157294710477195

Epoch: 6| Step: 12
Training loss: 2.485513925552368
Validation loss: 2.114242732524872

Epoch: 6| Step: 13
Training loss: 2.729471206665039
Validation loss: 2.11780446767807

Epoch: 62| Step: 0
Training loss: 2.541374921798706
Validation loss: 2.1168732047080994

Epoch: 6| Step: 1
Training loss: 1.7560880184173584
Validation loss: 2.1203362743059793

Epoch: 6| Step: 2
Training loss: 2.7507479190826416
Validation loss: 2.117326001326243

Epoch: 6| Step: 3
Training loss: 1.7335102558135986
Validation loss: 2.1152337392171225

Epoch: 6| Step: 4
Training loss: 2.3671176433563232
Validation loss: 2.1087896823883057

Epoch: 6| Step: 5
Training loss: 2.339799642562866
Validation loss: 2.107488830884298

Epoch: 6| Step: 6
Training loss: 2.6086559295654297
Validation loss: 2.099672178427378

Epoch: 6| Step: 7
Training loss: 1.9888017177581787
Validation loss: 2.1032302379608154

Epoch: 6| Step: 8
Training loss: 2.1593148708343506
Validation loss: 2.1127278010050454

Epoch: 6| Step: 9
Training loss: 2.3837332725524902
Validation loss: 2.109789172808329

Epoch: 6| Step: 10
Training loss: 2.1903090476989746
Validation loss: 2.110254645347595

Epoch: 6| Step: 11
Training loss: 2.122117519378662
Validation loss: 2.097381035486857

Epoch: 6| Step: 12
Training loss: 2.975259304046631
Validation loss: 2.0918415983517966

Epoch: 6| Step: 13
Training loss: 2.2912778854370117
Validation loss: 2.102913558483124

Epoch: 63| Step: 0
Training loss: 2.780679941177368
Validation loss: 2.1062525510787964

Epoch: 6| Step: 1
Training loss: 1.7624256610870361
Validation loss: 2.112193524837494

Epoch: 6| Step: 2
Training loss: 2.445453643798828
Validation loss: 2.1206932266553244

Epoch: 6| Step: 3
Training loss: 2.119511604309082
Validation loss: 2.131126622358958

Epoch: 6| Step: 4
Training loss: 1.7026116847991943
Validation loss: 2.137762268384298

Epoch: 6| Step: 5
Training loss: 2.725876569747925
Validation loss: 2.1442030866940818

Epoch: 6| Step: 6
Training loss: 2.3858578205108643
Validation loss: 2.1574106216430664

Epoch: 6| Step: 7
Training loss: 2.8438196182250977
Validation loss: 2.1671570539474487

Epoch: 6| Step: 8
Training loss: 2.5716028213500977
Validation loss: 2.1695436437924704

Epoch: 6| Step: 9
Training loss: 2.193297863006592
Validation loss: 2.1605935096740723

Epoch: 6| Step: 10
Training loss: 1.964726448059082
Validation loss: 2.1453243494033813

Epoch: 6| Step: 11
Training loss: 2.279210090637207
Validation loss: 2.1404436429341636

Epoch: 6| Step: 12
Training loss: 2.142634868621826
Validation loss: 2.1240731477737427

Epoch: 6| Step: 13
Training loss: 2.433475971221924
Validation loss: 2.115983168284098

Epoch: 64| Step: 0
Training loss: 2.3377113342285156
Validation loss: 2.106047054131826

Epoch: 6| Step: 1
Training loss: 2.900606870651245
Validation loss: 2.0989407102266946

Epoch: 6| Step: 2
Training loss: 2.4280080795288086
Validation loss: 2.0980603098869324

Epoch: 6| Step: 3
Training loss: 2.088057518005371
Validation loss: 2.0941979686419168

Epoch: 6| Step: 4
Training loss: 2.402390718460083
Validation loss: 2.084718942642212

Epoch: 6| Step: 5
Training loss: 2.8248257637023926
Validation loss: 2.0833078622817993

Epoch: 6| Step: 6
Training loss: 2.4942729473114014
Validation loss: 2.082595149676005

Epoch: 6| Step: 7
Training loss: 1.4687610864639282
Validation loss: 2.0776309172312417

Epoch: 6| Step: 8
Training loss: 2.133223295211792
Validation loss: 2.075131118297577

Epoch: 6| Step: 9
Training loss: 2.494922637939453
Validation loss: 2.0743163228034973

Epoch: 6| Step: 10
Training loss: 1.6704750061035156
Validation loss: 2.0770436922709146

Epoch: 6| Step: 11
Training loss: 2.316317081451416
Validation loss: 2.0854642391204834

Epoch: 6| Step: 12
Training loss: 2.147459030151367
Validation loss: 2.080572466055552

Epoch: 6| Step: 13
Training loss: 2.1423282623291016
Validation loss: 2.086406966050466

Epoch: 65| Step: 0
Training loss: 2.6182379722595215
Validation loss: 2.089702288309733

Epoch: 6| Step: 1
Training loss: 2.3016672134399414
Validation loss: 2.088398059209188

Epoch: 6| Step: 2
Training loss: 1.7673523426055908
Validation loss: 2.0897144277890525

Epoch: 6| Step: 3
Training loss: 2.1047539710998535
Validation loss: 2.0860647161801658

Epoch: 6| Step: 4
Training loss: 2.3780016899108887
Validation loss: 2.080530524253845

Epoch: 6| Step: 5
Training loss: 1.6875736713409424
Validation loss: 2.0744020144144693

Epoch: 6| Step: 6
Training loss: 2.1770834922790527
Validation loss: 2.0703447262446084

Epoch: 6| Step: 7
Training loss: 2.525712728500366
Validation loss: 2.0668301979700723

Epoch: 6| Step: 8
Training loss: 2.328158378601074
Validation loss: 2.0627067486445108

Epoch: 6| Step: 9
Training loss: 2.5966787338256836
Validation loss: 2.058849652608236

Epoch: 6| Step: 10
Training loss: 2.1921544075012207
Validation loss: 2.0686435103416443

Epoch: 6| Step: 11
Training loss: 2.2052409648895264
Validation loss: 2.0635993679364524

Epoch: 6| Step: 12
Training loss: 2.482473373413086
Validation loss: 2.065397004286448

Epoch: 6| Step: 13
Training loss: 2.1754660606384277
Validation loss: 2.061219573020935

Epoch: 66| Step: 0
Training loss: 2.2977821826934814
Validation loss: 2.0618362426757812

Epoch: 6| Step: 1
Training loss: 2.0051770210266113
Validation loss: 2.0631821751594543

Epoch: 6| Step: 2
Training loss: 2.7641313076019287
Validation loss: 2.0614242355028787

Epoch: 6| Step: 3
Training loss: 2.2047171592712402
Validation loss: 2.054298222064972

Epoch: 6| Step: 4
Training loss: 2.2259433269500732
Validation loss: 2.055328845977783

Epoch: 6| Step: 5
Training loss: 1.8805116415023804
Validation loss: 2.0554850697517395

Epoch: 6| Step: 6
Training loss: 2.4543521404266357
Validation loss: 2.061596393585205

Epoch: 6| Step: 7
Training loss: 2.4504189491271973
Validation loss: 2.053734322388967

Epoch: 6| Step: 8
Training loss: 2.900242805480957
Validation loss: 2.043663501739502

Epoch: 6| Step: 9
Training loss: 1.8026692867279053
Validation loss: 2.04753843943278

Epoch: 6| Step: 10
Training loss: 1.9704686403274536
Validation loss: 2.0549439589182534

Epoch: 6| Step: 11
Training loss: 1.9688185453414917
Validation loss: 2.052453358968099

Epoch: 6| Step: 12
Training loss: 2.03879714012146
Validation loss: 2.0467097759246826

Epoch: 6| Step: 13
Training loss: 2.4020638465881348
Validation loss: 2.0482431451479592

Epoch: 67| Step: 0
Training loss: 2.2352800369262695
Validation loss: 2.0504248340924582

Epoch: 6| Step: 1
Training loss: 2.073601007461548
Validation loss: 2.0416236519813538

Epoch: 6| Step: 2
Training loss: 2.32900333404541
Validation loss: 2.045542279879252

Epoch: 6| Step: 3
Training loss: 2.1745612621307373
Validation loss: 2.0517717599868774

Epoch: 6| Step: 4
Training loss: 2.167738199234009
Validation loss: 2.054004748662313

Epoch: 6| Step: 5
Training loss: 2.6440415382385254
Validation loss: 2.0538655519485474

Epoch: 6| Step: 6
Training loss: 2.5571200847625732
Validation loss: 2.0571516156196594

Epoch: 6| Step: 7
Training loss: 2.2935891151428223
Validation loss: 2.053049882253011

Epoch: 6| Step: 8
Training loss: 2.478393077850342
Validation loss: 2.0482255816459656

Epoch: 6| Step: 9
Training loss: 2.210003137588501
Validation loss: 2.055607855319977

Epoch: 6| Step: 10
Training loss: 1.6376445293426514
Validation loss: 2.0522843400637307

Epoch: 6| Step: 11
Training loss: 2.3921988010406494
Validation loss: 2.053131341934204

Epoch: 6| Step: 12
Training loss: 1.726886510848999
Validation loss: 2.0443082451820374

Epoch: 6| Step: 13
Training loss: 2.2505321502685547
Validation loss: 2.042736570040385

Epoch: 68| Step: 0
Training loss: 2.271430730819702
Validation loss: 2.044628143310547

Epoch: 6| Step: 1
Training loss: 1.7111201286315918
Validation loss: 2.0508275230725608

Epoch: 6| Step: 2
Training loss: 2.140018939971924
Validation loss: 2.059453825155894

Epoch: 6| Step: 3
Training loss: 1.9953501224517822
Validation loss: 2.054892917474111

Epoch: 6| Step: 4
Training loss: 2.369485378265381
Validation loss: 2.0564851562182107

Epoch: 6| Step: 5
Training loss: 2.6393299102783203
Validation loss: 2.0636431773503623

Epoch: 6| Step: 6
Training loss: 2.6967339515686035
Validation loss: 2.0490124026934304

Epoch: 6| Step: 7
Training loss: 1.5235551595687866
Validation loss: 2.05265865723292

Epoch: 6| Step: 8
Training loss: 2.661018133163452
Validation loss: 2.055417239665985

Epoch: 6| Step: 9
Training loss: 1.8252041339874268
Validation loss: 2.0335803627967834

Epoch: 6| Step: 10
Training loss: 1.7590514421463013
Validation loss: 2.0350730816523233

Epoch: 6| Step: 11
Training loss: 2.7632365226745605
Validation loss: 2.035969932874044

Epoch: 6| Step: 12
Training loss: 2.7460103034973145
Validation loss: 2.0375487009684243

Epoch: 6| Step: 13
Training loss: 2.0320985317230225
Validation loss: 2.044618566830953

Epoch: 69| Step: 0
Training loss: 2.5698294639587402
Validation loss: 2.0467758973439536

Epoch: 6| Step: 1
Training loss: 1.743208885192871
Validation loss: 2.0482999682426453

Epoch: 6| Step: 2
Training loss: 2.843939781188965
Validation loss: 2.0546217958132424

Epoch: 6| Step: 3
Training loss: 3.049467086791992
Validation loss: 2.051966428756714

Epoch: 6| Step: 4
Training loss: 2.0667355060577393
Validation loss: 2.0509433348973594

Epoch: 6| Step: 5
Training loss: 1.9484519958496094
Validation loss: 2.052999476591746

Epoch: 6| Step: 6
Training loss: 2.065695285797119
Validation loss: 2.055261711279551

Epoch: 6| Step: 7
Training loss: 2.3382771015167236
Validation loss: 2.052288989226023

Epoch: 6| Step: 8
Training loss: 2.294593334197998
Validation loss: 2.0506911873817444

Epoch: 6| Step: 9
Training loss: 1.545225739479065
Validation loss: 2.0467939972877502

Epoch: 6| Step: 10
Training loss: 2.3734354972839355
Validation loss: 2.048935373624166

Epoch: 6| Step: 11
Training loss: 2.250997543334961
Validation loss: 2.0447059671084085

Epoch: 6| Step: 12
Training loss: 2.2089648246765137
Validation loss: 2.039406975110372

Epoch: 6| Step: 13
Training loss: 1.865919589996338
Validation loss: 2.0387731393178306

Epoch: 70| Step: 0
Training loss: 2.3155031204223633
Validation loss: 2.0326338609059653

Epoch: 6| Step: 1
Training loss: 2.7127585411071777
Validation loss: 2.0291197101275125

Epoch: 6| Step: 2
Training loss: 1.8836069107055664
Validation loss: 2.044164021809896

Epoch: 6| Step: 3
Training loss: 2.3736071586608887
Validation loss: 2.041993280251821

Epoch: 6| Step: 4
Training loss: 1.842958688735962
Validation loss: 2.046271502971649

Epoch: 6| Step: 5
Training loss: 1.7337281703948975
Validation loss: 2.052764058113098

Epoch: 6| Step: 6
Training loss: 1.4399505853652954
Validation loss: 2.0581016540527344

Epoch: 6| Step: 7
Training loss: 2.9543333053588867
Validation loss: 2.055764675140381

Epoch: 6| Step: 8
Training loss: 2.2563138008117676
Validation loss: 2.0464763243993125

Epoch: 6| Step: 9
Training loss: 2.4388980865478516
Validation loss: 2.03811506430308

Epoch: 6| Step: 10
Training loss: 2.0484843254089355
Validation loss: 2.039297342300415

Epoch: 6| Step: 11
Training loss: 1.8742437362670898
Validation loss: 2.0315073331197104

Epoch: 6| Step: 12
Training loss: 2.5606515407562256
Validation loss: 2.0294694105784097

Epoch: 6| Step: 13
Training loss: 2.4931154251098633
Validation loss: 2.0312106609344482

Epoch: 71| Step: 0
Training loss: 2.197300910949707
Validation loss: 2.0332143306732178

Epoch: 6| Step: 1
Training loss: 2.1597797870635986
Validation loss: 2.038267215092977

Epoch: 6| Step: 2
Training loss: 1.9600133895874023
Validation loss: 2.0424735148747764

Epoch: 6| Step: 3
Training loss: 2.3398139476776123
Validation loss: 2.0358856121699014

Epoch: 6| Step: 4
Training loss: 1.6314754486083984
Validation loss: 2.037521700064341

Epoch: 6| Step: 5
Training loss: 1.6849758625030518
Validation loss: 2.040291170279185

Epoch: 6| Step: 6
Training loss: 2.453037738800049
Validation loss: 2.0407623052597046

Epoch: 6| Step: 7
Training loss: 2.236868143081665
Validation loss: 2.034933547178904

Epoch: 6| Step: 8
Training loss: 2.5656633377075195
Validation loss: 2.0316492319107056

Epoch: 6| Step: 9
Training loss: 2.655991554260254
Validation loss: 2.0323142806688943

Epoch: 6| Step: 10
Training loss: 2.0283501148223877
Validation loss: 2.0408851305643716

Epoch: 6| Step: 11
Training loss: 2.141082763671875
Validation loss: 2.048038899898529

Epoch: 6| Step: 12
Training loss: 2.3331704139709473
Validation loss: 2.0558464725812278

Epoch: 6| Step: 13
Training loss: 2.596057415008545
Validation loss: 2.0580264925956726

Epoch: 72| Step: 0
Training loss: 2.041674852371216
Validation loss: 2.0487040281295776

Epoch: 6| Step: 1
Training loss: 2.2514772415161133
Validation loss: 2.0483607848485312

Epoch: 6| Step: 2
Training loss: 1.7770500183105469
Validation loss: 2.0493721961975098

Epoch: 6| Step: 3
Training loss: 1.8737130165100098
Validation loss: 2.0451894402503967

Epoch: 6| Step: 4
Training loss: 2.033092498779297
Validation loss: 2.027550975481669

Epoch: 6| Step: 5
Training loss: 2.405102252960205
Validation loss: 2.0248007774353027

Epoch: 6| Step: 6
Training loss: 2.8429644107818604
Validation loss: 2.027000387509664

Epoch: 6| Step: 7
Training loss: 2.3539252281188965
Validation loss: 2.0332043170928955

Epoch: 6| Step: 8
Training loss: 1.7667806148529053
Validation loss: 2.042113780975342

Epoch: 6| Step: 9
Training loss: 2.827467918395996
Validation loss: 2.0470828215281167

Epoch: 6| Step: 10
Training loss: 2.6072988510131836
Validation loss: 2.051258405049642

Epoch: 6| Step: 11
Training loss: 2.10858154296875
Validation loss: 2.052759369214376

Epoch: 6| Step: 12
Training loss: 2.162715435028076
Validation loss: 2.052116592725118

Epoch: 6| Step: 13
Training loss: 1.9623661041259766
Validation loss: 2.056109666824341

Epoch: 73| Step: 0
Training loss: 2.318624258041382
Validation loss: 2.051915764808655

Epoch: 6| Step: 1
Training loss: 2.5640339851379395
Validation loss: 2.0469106237093606

Epoch: 6| Step: 2
Training loss: 1.8226711750030518
Validation loss: 2.0487011869748435

Epoch: 6| Step: 3
Training loss: 3.023594856262207
Validation loss: 2.05094180504481

Epoch: 6| Step: 4
Training loss: 2.5211377143859863
Validation loss: 2.0499441027641296

Epoch: 6| Step: 5
Training loss: 1.2123963832855225
Validation loss: 2.047417461872101

Epoch: 6| Step: 6
Training loss: 2.285907506942749
Validation loss: 2.040947596232096

Epoch: 6| Step: 7
Training loss: 2.2763490676879883
Validation loss: 2.036348799864451

Epoch: 6| Step: 8
Training loss: 2.5182080268859863
Validation loss: 2.0360213915506997

Epoch: 6| Step: 9
Training loss: 2.5024731159210205
Validation loss: 2.0357876420021057

Epoch: 6| Step: 10
Training loss: 2.375613212585449
Validation loss: 2.0332878033320108

Epoch: 6| Step: 11
Training loss: 1.9478552341461182
Validation loss: 2.031397004922231

Epoch: 6| Step: 12
Training loss: 1.833832859992981
Validation loss: 2.027699291706085

Epoch: 6| Step: 13
Training loss: 1.8601675033569336
Validation loss: 2.024153014024099

Epoch: 74| Step: 0
Training loss: 2.259265184402466
Validation loss: 2.028292695681254

Epoch: 6| Step: 1
Training loss: 2.355762481689453
Validation loss: 2.0374388098716736

Epoch: 6| Step: 2
Training loss: 2.192800521850586
Validation loss: 2.0395818948745728

Epoch: 6| Step: 3
Training loss: 1.8055087327957153
Validation loss: 2.063832779725393

Epoch: 6| Step: 4
Training loss: 2.831610679626465
Validation loss: 2.055143435796102

Epoch: 6| Step: 5
Training loss: 2.4028239250183105
Validation loss: 2.0573853254318237

Epoch: 6| Step: 6
Training loss: 2.998842477798462
Validation loss: 2.0602803826332092

Epoch: 6| Step: 7
Training loss: 2.2408578395843506
Validation loss: 2.0629626512527466

Epoch: 6| Step: 8
Training loss: 1.844024896621704
Validation loss: 2.0450589259465537

Epoch: 6| Step: 9
Training loss: 1.595834732055664
Validation loss: 2.030753254890442

Epoch: 6| Step: 10
Training loss: 2.100249767303467
Validation loss: 2.020177483558655

Epoch: 6| Step: 11
Training loss: 1.8840969800949097
Validation loss: 2.020397881666819

Epoch: 6| Step: 12
Training loss: 2.0826575756073
Validation loss: 2.0235217014948526

Epoch: 6| Step: 13
Training loss: 2.2273759841918945
Validation loss: 2.025105039278666

Epoch: 75| Step: 0
Training loss: 2.2754950523376465
Validation loss: 2.0290172696113586

Epoch: 6| Step: 1
Training loss: 2.0118956565856934
Validation loss: 2.0363205075263977

Epoch: 6| Step: 2
Training loss: 2.560899257659912
Validation loss: 2.0324755708376565

Epoch: 6| Step: 3
Training loss: 2.505209445953369
Validation loss: 2.0321325063705444

Epoch: 6| Step: 4
Training loss: 2.483525514602661
Validation loss: 2.03262726465861

Epoch: 6| Step: 5
Training loss: 2.564321994781494
Validation loss: 2.0317895809809365

Epoch: 6| Step: 6
Training loss: 1.6393051147460938
Validation loss: 2.027475972970327

Epoch: 6| Step: 7
Training loss: 1.9259495735168457
Validation loss: 2.027766525745392

Epoch: 6| Step: 8
Training loss: 2.5581612586975098
Validation loss: 2.0235312978426614

Epoch: 6| Step: 9
Training loss: 2.0967888832092285
Validation loss: 2.0180176496505737

Epoch: 6| Step: 10
Training loss: 2.213529586791992
Validation loss: 2.0186374386151633

Epoch: 6| Step: 11
Training loss: 1.8834853172302246
Validation loss: 2.024017651875814

Epoch: 6| Step: 12
Training loss: 2.0287318229675293
Validation loss: 2.0184899965922036

Epoch: 6| Step: 13
Training loss: 2.0450639724731445
Validation loss: 2.0219041307767234

Epoch: 76| Step: 0
Training loss: 1.9755780696868896
Validation loss: 2.02895180384318

Epoch: 6| Step: 1
Training loss: 1.8143112659454346
Validation loss: 2.0366634925206504

Epoch: 6| Step: 2
Training loss: 1.7279469966888428
Validation loss: 2.037033041318258

Epoch: 6| Step: 3
Training loss: 1.9402575492858887
Validation loss: 2.060881276925405

Epoch: 6| Step: 4
Training loss: 2.230928421020508
Validation loss: 2.0776495337486267

Epoch: 6| Step: 5
Training loss: 2.029625415802002
Validation loss: 2.0787591139475503

Epoch: 6| Step: 6
Training loss: 2.379089832305908
Validation loss: 2.0620555679003396

Epoch: 6| Step: 7
Training loss: 2.820918083190918
Validation loss: 2.0435445308685303

Epoch: 6| Step: 8
Training loss: 2.6669440269470215
Validation loss: 2.026582737763723

Epoch: 6| Step: 9
Training loss: 2.139378786087036
Validation loss: 2.0230512420336404

Epoch: 6| Step: 10
Training loss: 2.444751739501953
Validation loss: 2.02598245938619

Epoch: 6| Step: 11
Training loss: 2.207913398742676
Validation loss: 2.0340118606885276

Epoch: 6| Step: 12
Training loss: 2.8711373805999756
Validation loss: 2.0355905493100486

Epoch: 6| Step: 13
Training loss: 2.038811445236206
Validation loss: 2.0395249923070273

Epoch: 77| Step: 0
Training loss: 2.402937173843384
Validation loss: 2.0409180323282876

Epoch: 6| Step: 1
Training loss: 1.681191325187683
Validation loss: 2.039929727713267

Epoch: 6| Step: 2
Training loss: 2.597874641418457
Validation loss: 2.044225017229716

Epoch: 6| Step: 3
Training loss: 1.8155632019042969
Validation loss: 2.048022290070852

Epoch: 6| Step: 4
Training loss: 2.0652828216552734
Validation loss: 2.0471690893173218

Epoch: 6| Step: 5
Training loss: 2.357797384262085
Validation loss: 2.0464994510014853

Epoch: 6| Step: 6
Training loss: 1.8737587928771973
Validation loss: 2.046150783697764

Epoch: 6| Step: 7
Training loss: 2.2249903678894043
Validation loss: 2.047379453976949

Epoch: 6| Step: 8
Training loss: 2.6442222595214844
Validation loss: 2.050620178381602

Epoch: 6| Step: 9
Training loss: 2.604435920715332
Validation loss: 2.0401254494984946

Epoch: 6| Step: 10
Training loss: 2.1981801986694336
Validation loss: 2.0429325501124063

Epoch: 6| Step: 11
Training loss: 2.1694343090057373
Validation loss: 2.03876785437266

Epoch: 6| Step: 12
Training loss: 2.161496877670288
Validation loss: 2.035594622294108

Epoch: 6| Step: 13
Training loss: 2.4226953983306885
Validation loss: 2.0355456670125327

Epoch: 78| Step: 0
Training loss: 2.1050503253936768
Validation loss: 2.030389110247294

Epoch: 6| Step: 1
Training loss: 2.007648229598999
Validation loss: 2.031381368637085

Epoch: 6| Step: 2
Training loss: 2.582643508911133
Validation loss: 2.0225400924682617

Epoch: 6| Step: 3
Training loss: 2.423719882965088
Validation loss: 2.0306371053059897

Epoch: 6| Step: 4
Training loss: 2.3169877529144287
Validation loss: 2.0258618195851645

Epoch: 6| Step: 5
Training loss: 1.9545432329177856
Validation loss: 2.024877905845642

Epoch: 6| Step: 6
Training loss: 2.444340229034424
Validation loss: 2.016609311103821

Epoch: 6| Step: 7
Training loss: 2.5160744190216064
Validation loss: 2.018810987472534

Epoch: 6| Step: 8
Training loss: 1.9159519672393799
Validation loss: 2.0241145888964334

Epoch: 6| Step: 9
Training loss: 1.9278547763824463
Validation loss: 2.0394587914148965

Epoch: 6| Step: 10
Training loss: 2.368114709854126
Validation loss: 2.043029308319092

Epoch: 6| Step: 11
Training loss: 2.262669324874878
Validation loss: 2.0601651867230735

Epoch: 6| Step: 12
Training loss: 2.483220338821411
Validation loss: 2.077876011530558

Epoch: 6| Step: 13
Training loss: 1.712094783782959
Validation loss: 2.0744192004203796

Epoch: 79| Step: 0
Training loss: 2.287322521209717
Validation loss: 2.0681052207946777

Epoch: 6| Step: 1
Training loss: 2.5006892681121826
Validation loss: 2.066162109375

Epoch: 6| Step: 2
Training loss: 2.0449442863464355
Validation loss: 2.061896880467733

Epoch: 6| Step: 3
Training loss: 2.059345245361328
Validation loss: 2.03961851199468

Epoch: 6| Step: 4
Training loss: 2.6448631286621094
Validation loss: 2.031695524851481

Epoch: 6| Step: 5
Training loss: 2.0891060829162598
Validation loss: 2.0239067475001016

Epoch: 6| Step: 6
Training loss: 2.0124988555908203
Validation loss: 2.009978771209717

Epoch: 6| Step: 7
Training loss: 2.3013076782226562
Validation loss: 2.0104894240697226

Epoch: 6| Step: 8
Training loss: 1.818669319152832
Validation loss: 2.0158896446228027

Epoch: 6| Step: 9
Training loss: 2.9459939002990723
Validation loss: 2.020610511302948

Epoch: 6| Step: 10
Training loss: 1.8846864700317383
Validation loss: 2.0210976203282676

Epoch: 6| Step: 11
Training loss: 2.5163967609405518
Validation loss: 2.0187827348709106

Epoch: 6| Step: 12
Training loss: 1.8879456520080566
Validation loss: 2.0167877078056335

Epoch: 6| Step: 13
Training loss: 2.160921096801758
Validation loss: 2.022906482219696

Epoch: 80| Step: 0
Training loss: 2.4899182319641113
Validation loss: 2.017197549343109

Epoch: 6| Step: 1
Training loss: 2.2956461906433105
Validation loss: 2.0189164082209268

Epoch: 6| Step: 2
Training loss: 1.9490747451782227
Validation loss: 2.0159552892049155

Epoch: 6| Step: 3
Training loss: 2.050750255584717
Validation loss: 2.014763514200846

Epoch: 6| Step: 4
Training loss: 2.260136127471924
Validation loss: 2.0170928637186685

Epoch: 6| Step: 5
Training loss: 1.8750942945480347
Validation loss: 2.0123167832692466

Epoch: 6| Step: 6
Training loss: 2.2627859115600586
Validation loss: 2.015837848186493

Epoch: 6| Step: 7
Training loss: 2.536716938018799
Validation loss: 2.0121292074521384

Epoch: 6| Step: 8
Training loss: 2.2513198852539062
Validation loss: 2.01111368338267

Epoch: 6| Step: 9
Training loss: 2.595327377319336
Validation loss: 2.006322721640269

Epoch: 6| Step: 10
Training loss: 2.688553810119629
Validation loss: 2.0080296794573465

Epoch: 6| Step: 11
Training loss: 1.5845974683761597
Validation loss: 2.0098818937937417

Epoch: 6| Step: 12
Training loss: 1.7950063943862915
Validation loss: 2.0313644806543985

Epoch: 6| Step: 13
Training loss: 1.879341959953308
Validation loss: 2.0315831899642944

Epoch: 81| Step: 0
Training loss: 2.216233730316162
Validation loss: 2.0417190392812095

Epoch: 6| Step: 1
Training loss: 2.344010353088379
Validation loss: 2.050519426663717

Epoch: 6| Step: 2
Training loss: 2.2508604526519775
Validation loss: 2.0472674568494162

Epoch: 6| Step: 3
Training loss: 1.9838601350784302
Validation loss: 2.0518401861190796

Epoch: 6| Step: 4
Training loss: 1.9133960008621216
Validation loss: 2.055000444253286

Epoch: 6| Step: 5
Training loss: 1.772549033164978
Validation loss: 2.0492458740870156

Epoch: 6| Step: 6
Training loss: 2.052058219909668
Validation loss: 2.0568005045255027

Epoch: 6| Step: 7
Training loss: 1.7388628721237183
Validation loss: 2.0459209382534027

Epoch: 6| Step: 8
Training loss: 2.550912618637085
Validation loss: 2.038133184115092

Epoch: 6| Step: 9
Training loss: 2.498955011367798
Validation loss: 2.023815472920736

Epoch: 6| Step: 10
Training loss: 2.2849221229553223
Validation loss: 2.0237600008646646

Epoch: 6| Step: 11
Training loss: 2.4076991081237793
Validation loss: 2.0133758187294006

Epoch: 6| Step: 12
Training loss: 2.374959945678711
Validation loss: 2.0103875398635864

Epoch: 6| Step: 13
Training loss: 2.1427745819091797
Validation loss: 2.0159552097320557

Epoch: 82| Step: 0
Training loss: 2.3473687171936035
Validation loss: 2.016874591509501

Epoch: 6| Step: 1
Training loss: 2.8994274139404297
Validation loss: 2.027476112047831

Epoch: 6| Step: 2
Training loss: 2.0258378982543945
Validation loss: 2.027541677157084

Epoch: 6| Step: 3
Training loss: 2.5860376358032227
Validation loss: 2.0302624305089316

Epoch: 6| Step: 4
Training loss: 2.4620792865753174
Validation loss: 2.025336523850759

Epoch: 6| Step: 5
Training loss: 2.233999729156494
Validation loss: 2.029482662677765

Epoch: 6| Step: 6
Training loss: 1.8758939504623413
Validation loss: 2.0343276262283325

Epoch: 6| Step: 7
Training loss: 2.3467302322387695
Validation loss: 2.029093166192373

Epoch: 6| Step: 8
Training loss: 1.9588786363601685
Validation loss: 2.0231276949246726

Epoch: 6| Step: 9
Training loss: 2.0925168991088867
Validation loss: 2.025432070096334

Epoch: 6| Step: 10
Training loss: 1.9044692516326904
Validation loss: 2.0233790477116904

Epoch: 6| Step: 11
Training loss: 1.8951058387756348
Validation loss: 2.0218103726704917

Epoch: 6| Step: 12
Training loss: 2.419339179992676
Validation loss: 2.0159443020820618

Epoch: 6| Step: 13
Training loss: 1.6718010902404785
Validation loss: 2.013052543004354

Epoch: 83| Step: 0
Training loss: 1.822394847869873
Validation loss: 2.0170235435167947

Epoch: 6| Step: 1
Training loss: 2.8653509616851807
Validation loss: 2.023816486199697

Epoch: 6| Step: 2
Training loss: 2.200554370880127
Validation loss: 2.0228628317515054

Epoch: 6| Step: 3
Training loss: 1.6437768936157227
Validation loss: 2.0302874644597373

Epoch: 6| Step: 4
Training loss: 1.9192242622375488
Validation loss: 2.036056955655416

Epoch: 6| Step: 5
Training loss: 2.1310489177703857
Validation loss: 2.034563958644867

Epoch: 6| Step: 6
Training loss: 2.2794923782348633
Validation loss: 2.0484361251195273

Epoch: 6| Step: 7
Training loss: 1.8282394409179688
Validation loss: 2.0451842546463013

Epoch: 6| Step: 8
Training loss: 2.362969160079956
Validation loss: 2.040192663669586

Epoch: 6| Step: 9
Training loss: 1.8230029344558716
Validation loss: 2.0332058866818747

Epoch: 6| Step: 10
Training loss: 2.3759994506835938
Validation loss: 2.026984473069509

Epoch: 6| Step: 11
Training loss: 2.5862700939178467
Validation loss: 2.022635539372762

Epoch: 6| Step: 12
Training loss: 2.045154571533203
Validation loss: 2.0110557874043784

Epoch: 6| Step: 13
Training loss: 2.5146570205688477
Validation loss: 2.0070846478144326

Epoch: 84| Step: 0
Training loss: 1.7134560346603394
Validation loss: 2.0132296482721963

Epoch: 6| Step: 1
Training loss: 2.2798142433166504
Validation loss: 2.0097047289212546

Epoch: 6| Step: 2
Training loss: 1.6313965320587158
Validation loss: 2.0162283182144165

Epoch: 6| Step: 3
Training loss: 2.112046003341675
Validation loss: 2.0083214243253074

Epoch: 6| Step: 4
Training loss: 2.1737990379333496
Validation loss: 2.0137121876080832

Epoch: 6| Step: 5
Training loss: 2.070326328277588
Validation loss: 2.019037206967672

Epoch: 6| Step: 6
Training loss: 2.2286148071289062
Validation loss: 2.0175954500834146

Epoch: 6| Step: 7
Training loss: 2.1968135833740234
Validation loss: 2.0073700745900473

Epoch: 6| Step: 8
Training loss: 2.5514748096466064
Validation loss: 2.0106028119723

Epoch: 6| Step: 9
Training loss: 2.288647174835205
Validation loss: 2.0127251744270325

Epoch: 6| Step: 10
Training loss: 2.3614165782928467
Validation loss: 2.011715054512024

Epoch: 6| Step: 11
Training loss: 2.0239059925079346
Validation loss: 2.0160622795422873

Epoch: 6| Step: 12
Training loss: 2.4364209175109863
Validation loss: 2.011758287747701

Epoch: 6| Step: 13
Training loss: 2.2942404747009277
Validation loss: 2.01401150226593

Epoch: 85| Step: 0
Training loss: 2.586388349533081
Validation loss: 2.030051271120707

Epoch: 6| Step: 1
Training loss: 1.855437994003296
Validation loss: 2.029435694217682

Epoch: 6| Step: 2
Training loss: 2.194277286529541
Validation loss: 2.0452946623166404

Epoch: 6| Step: 3
Training loss: 2.09086012840271
Validation loss: 2.0598241885503135

Epoch: 6| Step: 4
Training loss: 2.669671058654785
Validation loss: 2.0633785923322043

Epoch: 6| Step: 5
Training loss: 2.0786707401275635
Validation loss: 2.0620087385177612

Epoch: 6| Step: 6
Training loss: 2.2354724407196045
Validation loss: 2.060665249824524

Epoch: 6| Step: 7
Training loss: 2.6143226623535156
Validation loss: 2.056111454963684

Epoch: 6| Step: 8
Training loss: 2.288454532623291
Validation loss: 2.0593708157539368

Epoch: 6| Step: 9
Training loss: 1.0815625190734863
Validation loss: 2.041186769803365

Epoch: 6| Step: 10
Training loss: 2.516279697418213
Validation loss: 2.0290974974632263

Epoch: 6| Step: 11
Training loss: 2.2611091136932373
Validation loss: 2.0209996898969016

Epoch: 6| Step: 12
Training loss: 2.154345989227295
Validation loss: 2.013688385486603

Epoch: 6| Step: 13
Training loss: 2.073575496673584
Validation loss: 2.009407619635264

Epoch: 86| Step: 0
Training loss: 2.6414127349853516
Validation loss: 2.0134419202804565

Epoch: 6| Step: 1
Training loss: 2.259350299835205
Validation loss: 2.008784234523773

Epoch: 6| Step: 2
Training loss: 2.1070024967193604
Validation loss: 2.01470215121905

Epoch: 6| Step: 3
Training loss: 1.836495041847229
Validation loss: 2.0127031008402505

Epoch: 6| Step: 4
Training loss: 2.0463597774505615
Validation loss: 2.005542576313019

Epoch: 6| Step: 5
Training loss: 2.15147066116333
Validation loss: 2.0162185430526733

Epoch: 6| Step: 6
Training loss: 2.533426284790039
Validation loss: 2.011884888013204

Epoch: 6| Step: 7
Training loss: 1.874022126197815
Validation loss: 2.0142510135968528

Epoch: 6| Step: 8
Training loss: 2.2336606979370117
Validation loss: 2.01186333100001

Epoch: 6| Step: 9
Training loss: 1.5159603357315063
Validation loss: 2.011000374952952

Epoch: 6| Step: 10
Training loss: 2.1745855808258057
Validation loss: 2.0075339674949646

Epoch: 6| Step: 11
Training loss: 2.2126126289367676
Validation loss: 2.0106553037961326

Epoch: 6| Step: 12
Training loss: 2.0903687477111816
Validation loss: 2.0171894431114197

Epoch: 6| Step: 13
Training loss: 2.7314257621765137
Validation loss: 2.017330308755239

Epoch: 87| Step: 0
Training loss: 1.8865479230880737
Validation loss: 2.0227465430895486

Epoch: 6| Step: 1
Training loss: 2.423440456390381
Validation loss: 2.02338445186615

Epoch: 6| Step: 2
Training loss: 2.175063371658325
Validation loss: 2.015445331732432

Epoch: 6| Step: 3
Training loss: 2.469985008239746
Validation loss: 2.026869277159373

Epoch: 6| Step: 4
Training loss: 2.001194715499878
Validation loss: 2.015845318635305

Epoch: 6| Step: 5
Training loss: 2.0496809482574463
Validation loss: 2.0218883951505027

Epoch: 6| Step: 6
Training loss: 2.1670339107513428
Validation loss: 2.025358001391093

Epoch: 6| Step: 7
Training loss: 2.4809606075286865
Validation loss: 2.0235830545425415

Epoch: 6| Step: 8
Training loss: 1.537278413772583
Validation loss: 2.0294781724611917

Epoch: 6| Step: 9
Training loss: 2.5663514137268066
Validation loss: 2.033688406149546

Epoch: 6| Step: 10
Training loss: 2.1990115642547607
Validation loss: 2.0275251269340515

Epoch: 6| Step: 11
Training loss: 1.969614863395691
Validation loss: 2.0315616130828857

Epoch: 6| Step: 12
Training loss: 1.5009907484054565
Validation loss: 2.0279134114583335

Epoch: 6| Step: 13
Training loss: 2.7429661750793457
Validation loss: 2.0415928959846497

Epoch: 88| Step: 0
Training loss: 2.2707839012145996
Validation loss: 2.043661812941233

Epoch: 6| Step: 1
Training loss: 2.0190415382385254
Validation loss: 2.0469276905059814

Epoch: 6| Step: 2
Training loss: 2.0668678283691406
Validation loss: 2.046605169773102

Epoch: 6| Step: 3
Training loss: 1.9168486595153809
Validation loss: 2.040297210216522

Epoch: 6| Step: 4
Training loss: 1.3807933330535889
Validation loss: 2.0423065423965454

Epoch: 6| Step: 5
Training loss: 1.6606101989746094
Validation loss: 2.0354367097218833

Epoch: 6| Step: 6
Training loss: 2.5088133811950684
Validation loss: 2.029811898867289

Epoch: 6| Step: 7
Training loss: 2.884256362915039
Validation loss: 2.029676099618276

Epoch: 6| Step: 8
Training loss: 2.4413576126098633
Validation loss: 2.027052025000254

Epoch: 6| Step: 9
Training loss: 2.231447219848633
Validation loss: 2.0257782141367593

Epoch: 6| Step: 10
Training loss: 2.2037835121154785
Validation loss: 2.013771375020345

Epoch: 6| Step: 11
Training loss: 1.7604939937591553
Validation loss: 2.0162924329439798

Epoch: 6| Step: 12
Training loss: 2.2130603790283203
Validation loss: 2.014337420463562

Epoch: 6| Step: 13
Training loss: 2.6285555362701416
Validation loss: 2.0198108752568564

Epoch: 89| Step: 0
Training loss: 2.560997724533081
Validation loss: 2.019477208455404

Epoch: 6| Step: 1
Training loss: 1.9947823286056519
Validation loss: 2.016283710797628

Epoch: 6| Step: 2
Training loss: 1.7189085483551025
Validation loss: 2.0189133683840432

Epoch: 6| Step: 3
Training loss: 2.283405303955078
Validation loss: 2.014044721921285

Epoch: 6| Step: 4
Training loss: 2.0196142196655273
Validation loss: 2.016024351119995

Epoch: 6| Step: 5
Training loss: 2.6553382873535156
Validation loss: 2.0230668783187866

Epoch: 6| Step: 6
Training loss: 2.0933096408843994
Validation loss: 2.0182289679845176

Epoch: 6| Step: 7
Training loss: 1.8163480758666992
Validation loss: 2.0161913633346558

Epoch: 6| Step: 8
Training loss: 2.0792391300201416
Validation loss: 2.010152220726013

Epoch: 6| Step: 9
Training loss: 2.5370097160339355
Validation loss: 2.0155879060427346

Epoch: 6| Step: 10
Training loss: 2.251067638397217
Validation loss: 2.0139207243919373

Epoch: 6| Step: 11
Training loss: 2.3015031814575195
Validation loss: 2.012200653553009

Epoch: 6| Step: 12
Training loss: 2.083993434906006
Validation loss: 2.0118656555811563

Epoch: 6| Step: 13
Training loss: 1.7999396324157715
Validation loss: 2.0188190937042236

Epoch: 90| Step: 0
Training loss: 2.041954755783081
Validation loss: 2.0275630354881287

Epoch: 6| Step: 1
Training loss: 1.8581777811050415
Validation loss: 2.033362030982971

Epoch: 6| Step: 2
Training loss: 2.3375720977783203
Validation loss: 2.0397687554359436

Epoch: 6| Step: 3
Training loss: 1.939792275428772
Validation loss: 2.043782194455465

Epoch: 6| Step: 4
Training loss: 2.2249860763549805
Validation loss: 2.0423620541890464

Epoch: 6| Step: 5
Training loss: 2.6880078315734863
Validation loss: 2.050438125928243

Epoch: 6| Step: 6
Training loss: 2.247288703918457
Validation loss: 2.049304982026418

Epoch: 6| Step: 7
Training loss: 1.84979248046875
Validation loss: 2.0503333806991577

Epoch: 6| Step: 8
Training loss: 1.8024930953979492
Validation loss: 2.0478773514429727

Epoch: 6| Step: 9
Training loss: 1.9987547397613525
Validation loss: 2.0537323156992593

Epoch: 6| Step: 10
Training loss: 2.4286751747131348
Validation loss: 2.045154849688212

Epoch: 6| Step: 11
Training loss: 2.6527655124664307
Validation loss: 2.0443493326505027

Epoch: 6| Step: 12
Training loss: 1.9479957818984985
Validation loss: 2.0336767435073853

Epoch: 6| Step: 13
Training loss: 2.181914806365967
Validation loss: 2.0484222571055093

Epoch: 91| Step: 0
Training loss: 2.310222864151001
Validation loss: 2.04698779185613

Epoch: 6| Step: 1
Training loss: 1.1047909259796143
Validation loss: 2.0277671615282693

Epoch: 6| Step: 2
Training loss: 2.543313980102539
Validation loss: 2.025840957959493

Epoch: 6| Step: 3
Training loss: 2.4787395000457764
Validation loss: 2.0219943722089133

Epoch: 6| Step: 4
Training loss: 2.133908748626709
Validation loss: 2.020512898763021

Epoch: 6| Step: 5
Training loss: 2.2225522994995117
Validation loss: 2.027846018473307

Epoch: 6| Step: 6
Training loss: 2.3468222618103027
Validation loss: 2.018589417139689

Epoch: 6| Step: 7
Training loss: 2.0427000522613525
Validation loss: 2.0231142044067383

Epoch: 6| Step: 8
Training loss: 2.7519888877868652
Validation loss: 2.0278000036875405

Epoch: 6| Step: 9
Training loss: 2.3758740425109863
Validation loss: 2.026331663131714

Epoch: 6| Step: 10
Training loss: 2.6142921447753906
Validation loss: 2.024401843547821

Epoch: 6| Step: 11
Training loss: 1.973247766494751
Validation loss: 2.030836006005605

Epoch: 6| Step: 12
Training loss: 1.718152642250061
Validation loss: 2.035019636154175

Epoch: 6| Step: 13
Training loss: 1.4187126159667969
Validation loss: 2.0146891673405967

Epoch: 92| Step: 0
Training loss: 1.4782418012619019
Validation loss: 2.015601019064585

Epoch: 6| Step: 1
Training loss: 3.0486438274383545
Validation loss: 2.0138141910235086

Epoch: 6| Step: 2
Training loss: 1.8977810144424438
Validation loss: 2.019472599029541

Epoch: 6| Step: 3
Training loss: 1.7950859069824219
Validation loss: 2.0131243666013083

Epoch: 6| Step: 4
Training loss: 2.0488016605377197
Validation loss: 2.021367331345876

Epoch: 6| Step: 5
Training loss: 2.2541794776916504
Validation loss: 2.0219687620798745

Epoch: 6| Step: 6
Training loss: 2.1500463485717773
Validation loss: 2.0240147908528647

Epoch: 6| Step: 7
Training loss: 2.158595561981201
Validation loss: 2.0294258991877236

Epoch: 6| Step: 8
Training loss: 2.6611857414245605
Validation loss: 2.0347298781077066

Epoch: 6| Step: 9
Training loss: 2.1842360496520996
Validation loss: 2.02280730009079

Epoch: 6| Step: 10
Training loss: 2.2106823921203613
Validation loss: 2.020381589730581

Epoch: 6| Step: 11
Training loss: 2.255854845046997
Validation loss: 2.0206908186276755

Epoch: 6| Step: 12
Training loss: 2.509401798248291
Validation loss: 2.0212088028589883

Epoch: 6| Step: 13
Training loss: 1.665784239768982
Validation loss: 2.0137816270192466

Epoch: 93| Step: 0
Training loss: 1.8592643737792969
Validation loss: 2.021218498547872

Epoch: 6| Step: 1
Training loss: 1.6929826736450195
Validation loss: 2.0222884019215903

Epoch: 6| Step: 2
Training loss: 2.326256275177002
Validation loss: 2.023696859677633

Epoch: 6| Step: 3
Training loss: 2.4051971435546875
Validation loss: 2.0206275383631387

Epoch: 6| Step: 4
Training loss: 2.1313257217407227
Validation loss: 2.01727827390035

Epoch: 6| Step: 5
Training loss: 1.9317293167114258
Validation loss: 2.028759757677714

Epoch: 6| Step: 6
Training loss: 2.003535270690918
Validation loss: 2.0353503028551736

Epoch: 6| Step: 7
Training loss: 2.799551010131836
Validation loss: 2.040867249170939

Epoch: 6| Step: 8
Training loss: 2.7448196411132812
Validation loss: 2.043875296910604

Epoch: 6| Step: 9
Training loss: 1.8055682182312012
Validation loss: 2.0281240145365396

Epoch: 6| Step: 10
Training loss: 1.9085509777069092
Validation loss: 2.0341604749361673

Epoch: 6| Step: 11
Training loss: 2.310093879699707
Validation loss: 2.02695232629776

Epoch: 6| Step: 12
Training loss: 2.0632262229919434
Validation loss: 2.005574623743693

Epoch: 6| Step: 13
Training loss: 2.2869462966918945
Validation loss: 2.0129955808321633

Epoch: 94| Step: 0
Training loss: 2.1494767665863037
Validation loss: 2.0153542955716452

Epoch: 6| Step: 1
Training loss: 1.8366949558258057
Validation loss: 2.0163946747779846

Epoch: 6| Step: 2
Training loss: 2.2375528812408447
Validation loss: 2.021670718987783

Epoch: 6| Step: 3
Training loss: 2.081602096557617
Validation loss: 2.0208911299705505

Epoch: 6| Step: 4
Training loss: 1.865870475769043
Validation loss: 2.0182353258132935

Epoch: 6| Step: 5
Training loss: 1.6491117477416992
Validation loss: 2.0113141338030496

Epoch: 6| Step: 6
Training loss: 2.155280590057373
Validation loss: 2.01805716753006

Epoch: 6| Step: 7
Training loss: 2.673286199569702
Validation loss: 2.0230957865715027

Epoch: 6| Step: 8
Training loss: 2.415548801422119
Validation loss: 2.0188353657722473

Epoch: 6| Step: 9
Training loss: 1.8761425018310547
Validation loss: 2.0183940529823303

Epoch: 6| Step: 10
Training loss: 2.275120258331299
Validation loss: 2.0185116132100425

Epoch: 6| Step: 11
Training loss: 2.1723380088806152
Validation loss: 2.0210829774538674

Epoch: 6| Step: 12
Training loss: 2.353912830352783
Validation loss: 2.0205060044924417

Epoch: 6| Step: 13
Training loss: 2.2274489402770996
Validation loss: 2.0231221516927085

Epoch: 95| Step: 0
Training loss: 2.385852813720703
Validation loss: 2.030816435813904

Epoch: 6| Step: 1
Training loss: 2.5069375038146973
Validation loss: 2.034828841686249

Epoch: 6| Step: 2
Training loss: 2.1066932678222656
Validation loss: 2.024000604947408

Epoch: 6| Step: 3
Training loss: 1.3927350044250488
Validation loss: 2.0226534605026245

Epoch: 6| Step: 4
Training loss: 3.0270609855651855
Validation loss: 2.0183162887891135

Epoch: 6| Step: 5
Training loss: 2.2160863876342773
Validation loss: 2.0075634717941284

Epoch: 6| Step: 6
Training loss: 2.04469633102417
Validation loss: 2.0070608655611673

Epoch: 6| Step: 7
Training loss: 2.1359095573425293
Validation loss: 2.005884846051534

Epoch: 6| Step: 8
Training loss: 2.3663015365600586
Validation loss: 2.0060505469640098

Epoch: 6| Step: 9
Training loss: 1.8171802759170532
Validation loss: 2.0106118520100913

Epoch: 6| Step: 10
Training loss: 1.9915761947631836
Validation loss: 2.0172528823216758

Epoch: 6| Step: 11
Training loss: 2.4517018795013428
Validation loss: 2.0102108319600425

Epoch: 6| Step: 12
Training loss: 2.5560436248779297
Validation loss: 2.008918027083079

Epoch: 6| Step: 13
Training loss: 1.2215484380722046
Validation loss: 2.009510358174642

Epoch: 96| Step: 0
Training loss: 2.298177719116211
Validation loss: 1.9992282390594482

Epoch: 6| Step: 1
Training loss: 1.6166250705718994
Validation loss: 2.001261512438456

Epoch: 6| Step: 2
Training loss: 1.830768346786499
Validation loss: 2.010575771331787

Epoch: 6| Step: 3
Training loss: 1.9424786567687988
Validation loss: 2.0058650573094687

Epoch: 6| Step: 4
Training loss: 2.6633498668670654
Validation loss: 2.0295636852582297

Epoch: 6| Step: 5
Training loss: 1.8071389198303223
Validation loss: 2.028802533944448

Epoch: 6| Step: 6
Training loss: 2.238551616668701
Validation loss: 2.041118880112966

Epoch: 6| Step: 7
Training loss: 2.2221083641052246
Validation loss: 2.0447442531585693

Epoch: 6| Step: 8
Training loss: 2.48077392578125
Validation loss: 2.029873510201772

Epoch: 6| Step: 9
Training loss: 2.3863930702209473
Validation loss: 2.0276482105255127

Epoch: 6| Step: 10
Training loss: 2.697777509689331
Validation loss: 2.0264810919761658

Epoch: 6| Step: 11
Training loss: 2.676964044570923
Validation loss: 2.0243895848592124

Epoch: 6| Step: 12
Training loss: 1.3302769660949707
Validation loss: 2.013846218585968

Epoch: 6| Step: 13
Training loss: 2.0693140029907227
Validation loss: 2.0167009234428406

Epoch: 97| Step: 0
Training loss: 2.391378879547119
Validation loss: 2.0075253446896872

Epoch: 6| Step: 1
Training loss: 2.3074913024902344
Validation loss: 2.015057543913523

Epoch: 6| Step: 2
Training loss: 2.0404064655303955
Validation loss: 2.019925514856974

Epoch: 6| Step: 3
Training loss: 1.8521418571472168
Validation loss: 2.0169976949691772

Epoch: 6| Step: 4
Training loss: 2.093273162841797
Validation loss: 2.013762374718984

Epoch: 6| Step: 5
Training loss: 2.3392248153686523
Validation loss: 2.0181440313657126

Epoch: 6| Step: 6
Training loss: 2.19917631149292
Validation loss: 2.0168221990267434

Epoch: 6| Step: 7
Training loss: 1.7714035511016846
Validation loss: 2.021362026532491

Epoch: 6| Step: 8
Training loss: 3.033296585083008
Validation loss: 2.0102487007776895

Epoch: 6| Step: 9
Training loss: 1.9807353019714355
Validation loss: 2.014106790224711

Epoch: 6| Step: 10
Training loss: 1.9126253128051758
Validation loss: 2.02069628238678

Epoch: 6| Step: 11
Training loss: 2.550386905670166
Validation loss: 2.017399251461029

Epoch: 6| Step: 12
Training loss: 2.0520544052124023
Validation loss: 2.0147747794787088

Epoch: 6| Step: 13
Training loss: 1.7310444116592407
Validation loss: 2.0170528888702393

Epoch: 98| Step: 0
Training loss: 2.03460431098938
Validation loss: 2.020524044831594

Epoch: 6| Step: 1
Training loss: 1.6413285732269287
Validation loss: 2.018235504627228

Epoch: 6| Step: 2
Training loss: 2.035501003265381
Validation loss: 2.02414999405543

Epoch: 6| Step: 3
Training loss: 2.422168016433716
Validation loss: 2.0308637221654258

Epoch: 6| Step: 4
Training loss: 1.8736909627914429
Validation loss: 2.0302390257517495

Epoch: 6| Step: 5
Training loss: 2.9736671447753906
Validation loss: 2.0314332644144693

Epoch: 6| Step: 6
Training loss: 2.2088394165039062
Validation loss: 2.0305583477020264

Epoch: 6| Step: 7
Training loss: 2.243892192840576
Validation loss: 2.0184478759765625

Epoch: 6| Step: 8
Training loss: 2.61729097366333
Validation loss: 2.0349366664886475

Epoch: 6| Step: 9
Training loss: 1.7342190742492676
Validation loss: 2.025614857673645

Epoch: 6| Step: 10
Training loss: 2.109834671020508
Validation loss: 2.0202124317487082

Epoch: 6| Step: 11
Training loss: 2.141957998275757
Validation loss: 2.018390635649363

Epoch: 6| Step: 12
Training loss: 2.386573553085327
Validation loss: 2.016248881816864

Epoch: 6| Step: 13
Training loss: 1.5644296407699585
Validation loss: 2.0203498204549155

Epoch: 99| Step: 0
Training loss: 1.8775471448898315
Validation loss: 2.019200921058655

Epoch: 6| Step: 1
Training loss: 2.080348491668701
Validation loss: 2.0140520334243774

Epoch: 6| Step: 2
Training loss: 1.8787999153137207
Validation loss: 2.017523169517517

Epoch: 6| Step: 3
Training loss: 2.2806649208068848
Validation loss: 2.01449853181839

Epoch: 6| Step: 4
Training loss: 2.2270824909210205
Validation loss: 2.0231277346611023

Epoch: 6| Step: 5
Training loss: 1.9822056293487549
Validation loss: 2.021535317103068

Epoch: 6| Step: 6
Training loss: 2.6732001304626465
Validation loss: 2.0342519680658975

Epoch: 6| Step: 7
Training loss: 1.5539436340332031
Validation loss: 2.0300400058428445

Epoch: 6| Step: 8
Training loss: 1.5813837051391602
Validation loss: 2.035374701023102

Epoch: 6| Step: 9
Training loss: 2.9695544242858887
Validation loss: 2.047881523768107

Epoch: 6| Step: 10
Training loss: 2.3401594161987305
Validation loss: 2.0516910950342813

Epoch: 6| Step: 11
Training loss: 2.081240177154541
Validation loss: 2.0472810665766397

Epoch: 6| Step: 12
Training loss: 2.5558948516845703
Validation loss: 2.051367382208506

Epoch: 6| Step: 13
Training loss: 2.394695997238159
Validation loss: 2.0458250443140664

Epoch: 100| Step: 0
Training loss: 2.110832691192627
Validation loss: 2.03446763753891

Epoch: 6| Step: 1
Training loss: 1.792174220085144
Validation loss: 2.023977597554525

Epoch: 6| Step: 2
Training loss: 2.303854465484619
Validation loss: 2.0343597531318665

Epoch: 6| Step: 3
Training loss: 2.434469223022461
Validation loss: 2.0343344608942666

Epoch: 6| Step: 4
Training loss: 2.561677932739258
Validation loss: 2.0344825387001038

Epoch: 6| Step: 5
Training loss: 1.930983066558838
Validation loss: 2.0296056469281516

Epoch: 6| Step: 6
Training loss: 1.7810277938842773
Validation loss: 2.0303715467453003

Epoch: 6| Step: 7
Training loss: 1.9321283102035522
Validation loss: 2.0145403941472373

Epoch: 6| Step: 8
Training loss: 1.7909430265426636
Validation loss: 2.010974903901418

Epoch: 6| Step: 9
Training loss: 2.622612714767456
Validation loss: 2.0098525285720825

Epoch: 6| Step: 10
Training loss: 2.300801992416382
Validation loss: 2.016619304815928

Epoch: 6| Step: 11
Training loss: 2.5371384620666504
Validation loss: 2.020186205705007

Epoch: 6| Step: 12
Training loss: 2.1000208854675293
Validation loss: 2.0178093711535134

Epoch: 6| Step: 13
Training loss: 1.6072983741760254
Validation loss: 2.0142854849497476

Epoch: 101| Step: 0
Training loss: 2.513653516769409
Validation loss: 2.018079082171122

Epoch: 6| Step: 1
Training loss: 2.201451301574707
Validation loss: 2.013983647028605

Epoch: 6| Step: 2
Training loss: 1.8558580875396729
Validation loss: 2.018632789452871

Epoch: 6| Step: 3
Training loss: 1.7570176124572754
Validation loss: 2.0129631757736206

Epoch: 6| Step: 4
Training loss: 2.233891010284424
Validation loss: 2.0073836048444114

Epoch: 6| Step: 5
Training loss: 2.580087661743164
Validation loss: 2.0061758359273276

Epoch: 6| Step: 6
Training loss: 1.9539813995361328
Validation loss: 2.002848525842031

Epoch: 6| Step: 7
Training loss: 1.916090488433838
Validation loss: 2.005678037802378

Epoch: 6| Step: 8
Training loss: 2.6150588989257812
Validation loss: 2.0163299441337585

Epoch: 6| Step: 9
Training loss: 2.280655860900879
Validation loss: 2.0220821698506675

Epoch: 6| Step: 10
Training loss: 2.226585865020752
Validation loss: 2.0272762179374695

Epoch: 6| Step: 11
Training loss: 2.2457549571990967
Validation loss: 2.022324323654175

Epoch: 6| Step: 12
Training loss: 1.7731096744537354
Validation loss: 2.0273277958234153

Epoch: 6| Step: 13
Training loss: 1.8977280855178833
Validation loss: 2.0272690852483115

Epoch: 102| Step: 0
Training loss: 2.222019672393799
Validation loss: 2.023879865805308

Epoch: 6| Step: 1
Training loss: 1.9942396879196167
Validation loss: 2.0330355564753213

Epoch: 6| Step: 2
Training loss: 2.3879594802856445
Validation loss: 2.026175876458486

Epoch: 6| Step: 3
Training loss: 2.3726301193237305
Validation loss: 2.036192536354065

Epoch: 6| Step: 4
Training loss: 2.411365032196045
Validation loss: 2.026149113972982

Epoch: 6| Step: 5
Training loss: 1.6711039543151855
Validation loss: 2.0296306014060974

Epoch: 6| Step: 6
Training loss: 2.769160032272339
Validation loss: 2.0300004879633584

Epoch: 6| Step: 7
Training loss: 2.3363025188446045
Validation loss: 2.021870414415995

Epoch: 6| Step: 8
Training loss: 1.9742462635040283
Validation loss: 2.0181867480278015

Epoch: 6| Step: 9
Training loss: 1.8874826431274414
Validation loss: 2.0139830509821572

Epoch: 6| Step: 10
Training loss: 1.8648121356964111
Validation loss: 2.0165960987408957

Epoch: 6| Step: 11
Training loss: 2.0339815616607666
Validation loss: 2.012661894162496

Epoch: 6| Step: 12
Training loss: 1.6278787851333618
Validation loss: 2.018790304660797

Epoch: 6| Step: 13
Training loss: 2.366788864135742
Validation loss: 2.0149208704630532

Epoch: 103| Step: 0
Training loss: 2.2863075733184814
Validation loss: 2.0177915493647256

Epoch: 6| Step: 1
Training loss: 1.4850540161132812
Validation loss: 2.01181560754776

Epoch: 6| Step: 2
Training loss: 2.750391721725464
Validation loss: 2.01873246828715

Epoch: 6| Step: 3
Training loss: 2.582227945327759
Validation loss: 2.0207906564076743

Epoch: 6| Step: 4
Training loss: 1.760316014289856
Validation loss: 2.0261063973108926

Epoch: 6| Step: 5
Training loss: 2.2832889556884766
Validation loss: 2.049015541871389

Epoch: 6| Step: 6
Training loss: 2.102336883544922
Validation loss: 2.0284148255983987

Epoch: 6| Step: 7
Training loss: 2.264346122741699
Validation loss: 2.0395716428756714

Epoch: 6| Step: 8
Training loss: 2.424806594848633
Validation loss: 2.043110410372416

Epoch: 6| Step: 9
Training loss: 1.5797820091247559
Validation loss: 2.0371144811312356

Epoch: 6| Step: 10
Training loss: 2.516009569168091
Validation loss: 2.0335415999094644

Epoch: 6| Step: 11
Training loss: 1.728492021560669
Validation loss: 2.0395628213882446

Epoch: 6| Step: 12
Training loss: 1.8466793298721313
Validation loss: 2.0280121564865112

Epoch: 6| Step: 13
Training loss: 2.357130527496338
Validation loss: 2.0169399976730347

Epoch: 104| Step: 0
Training loss: 1.9827759265899658
Validation loss: 2.019082327683767

Epoch: 6| Step: 1
Training loss: 2.3557937145233154
Validation loss: 1.9990146160125732

Epoch: 6| Step: 2
Training loss: 2.0340824127197266
Validation loss: 2.0016926725705466

Epoch: 6| Step: 3
Training loss: 2.193601131439209
Validation loss: 2.0018843611081443

Epoch: 6| Step: 4
Training loss: 2.7039475440979004
Validation loss: 2.004962066809336

Epoch: 6| Step: 5
Training loss: 1.865332841873169
Validation loss: 2.0089717904726663

Epoch: 6| Step: 6
Training loss: 1.682665467262268
Validation loss: 2.008141875267029

Epoch: 6| Step: 7
Training loss: 2.6143369674682617
Validation loss: 1.9989671111106873

Epoch: 6| Step: 8
Training loss: 2.084111452102661
Validation loss: 2.0096712708473206

Epoch: 6| Step: 9
Training loss: 2.416271924972534
Validation loss: 2.0095420678456626

Epoch: 6| Step: 10
Training loss: 1.8011411428451538
Validation loss: 2.0102769335110984

Epoch: 6| Step: 11
Training loss: 1.874885082244873
Validation loss: 2.016284684340159

Epoch: 6| Step: 12
Training loss: 2.3326573371887207
Validation loss: 2.0127945144971213

Epoch: 6| Step: 13
Training loss: 2.0603208541870117
Validation loss: 2.0173255801200867

Epoch: 105| Step: 0
Training loss: 1.8163129091262817
Validation loss: 2.025880972544352

Epoch: 6| Step: 1
Training loss: 1.6637957096099854
Validation loss: 2.0280538400014243

Epoch: 6| Step: 2
Training loss: 2.4422054290771484
Validation loss: 2.02491158246994

Epoch: 6| Step: 3
Training loss: 1.7832224369049072
Validation loss: 2.021305421988169

Epoch: 6| Step: 4
Training loss: 1.5053249597549438
Validation loss: 2.026309907436371

Epoch: 6| Step: 5
Training loss: 2.570918560028076
Validation loss: 2.0296932657559714

Epoch: 6| Step: 6
Training loss: 2.1677374839782715
Validation loss: 2.027446428934733

Epoch: 6| Step: 7
Training loss: 2.5343363285064697
Validation loss: 2.0164640148480735

Epoch: 6| Step: 8
Training loss: 1.8482285737991333
Validation loss: 2.021051585674286

Epoch: 6| Step: 9
Training loss: 2.1907057762145996
Validation loss: 2.021617670853933

Epoch: 6| Step: 10
Training loss: 2.1633825302124023
Validation loss: 2.0232505202293396

Epoch: 6| Step: 11
Training loss: 2.199693441390991
Validation loss: 2.0218064983685813

Epoch: 6| Step: 12
Training loss: 2.7861204147338867
Validation loss: 2.023608128229777

Epoch: 6| Step: 13
Training loss: 1.9701021909713745
Validation loss: 2.0221869945526123

Epoch: 106| Step: 0
Training loss: 2.8002982139587402
Validation loss: 2.0110342502593994

Epoch: 6| Step: 1
Training loss: 2.346487522125244
Validation loss: 2.0078816016515098

Epoch: 6| Step: 2
Training loss: 1.8838024139404297
Validation loss: 2.0128496487935386

Epoch: 6| Step: 3
Training loss: 1.8338340520858765
Validation loss: 2.0090104738871255

Epoch: 6| Step: 4
Training loss: 2.134347438812256
Validation loss: 2.010280648867289

Epoch: 6| Step: 5
Training loss: 2.4770920276641846
Validation loss: 2.017451902230581

Epoch: 6| Step: 6
Training loss: 1.8211009502410889
Validation loss: 2.011774162451426

Epoch: 6| Step: 7
Training loss: 2.3113160133361816
Validation loss: 2.0169262886047363

Epoch: 6| Step: 8
Training loss: 1.834261178970337
Validation loss: 2.020740330219269

Epoch: 6| Step: 9
Training loss: 2.1513142585754395
Validation loss: 2.0206866463025412

Epoch: 6| Step: 10
Training loss: 2.248826265335083
Validation loss: 2.019392410914103

Epoch: 6| Step: 11
Training loss: 2.1477715969085693
Validation loss: 2.0248778462409973

Epoch: 6| Step: 12
Training loss: 1.898582935333252
Validation loss: 2.0243332783381143

Epoch: 6| Step: 13
Training loss: 1.879362940788269
Validation loss: 2.031370242436727

Epoch: 107| Step: 0
Training loss: 2.294323444366455
Validation loss: 2.0338467955589294

Epoch: 6| Step: 1
Training loss: 2.4798834323883057
Validation loss: 2.03134548664093

Epoch: 6| Step: 2
Training loss: 1.692680835723877
Validation loss: 2.041452646255493

Epoch: 6| Step: 3
Training loss: 2.0452356338500977
Validation loss: 2.0402005513509116

Epoch: 6| Step: 4
Training loss: 2.1167678833007812
Validation loss: 2.043357729911804

Epoch: 6| Step: 5
Training loss: 2.182523250579834
Validation loss: 2.0424628059069314

Epoch: 6| Step: 6
Training loss: 1.9933847188949585
Validation loss: 2.0254293282826743

Epoch: 6| Step: 7
Training loss: 2.2276859283447266
Validation loss: 2.0250878731409707

Epoch: 6| Step: 8
Training loss: 2.172882556915283
Validation loss: 2.0206449826558432

Epoch: 6| Step: 9
Training loss: 2.631317615509033
Validation loss: 2.017027258872986

Epoch: 6| Step: 10
Training loss: 2.202091932296753
Validation loss: 2.0180656711260476

Epoch: 6| Step: 11
Training loss: 2.1118030548095703
Validation loss: 2.0221208930015564

Epoch: 6| Step: 12
Training loss: 1.4140228033065796
Validation loss: 2.020326316356659

Epoch: 6| Step: 13
Training loss: 2.4982988834381104
Validation loss: 2.0212263663609824

Epoch: 108| Step: 0
Training loss: 2.46842360496521
Validation loss: 2.011355777581533

Epoch: 6| Step: 1
Training loss: 2.215388059616089
Validation loss: 2.0206403732299805

Epoch: 6| Step: 2
Training loss: 1.5097122192382812
Validation loss: 2.0215298334757485

Epoch: 6| Step: 3
Training loss: 1.6746482849121094
Validation loss: 2.028028130531311

Epoch: 6| Step: 4
Training loss: 1.8907454013824463
Validation loss: 2.0350194176038108

Epoch: 6| Step: 5
Training loss: 2.1409435272216797
Validation loss: 2.0308379928270974

Epoch: 6| Step: 6
Training loss: 1.459255576133728
Validation loss: 2.0283263325691223

Epoch: 6| Step: 7
Training loss: 1.9868757724761963
Validation loss: 2.0331032474835715

Epoch: 6| Step: 8
Training loss: 2.2617440223693848
Validation loss: 2.0328771273295083

Epoch: 6| Step: 9
Training loss: 2.334023952484131
Validation loss: 2.0337875286738076

Epoch: 6| Step: 10
Training loss: 2.4704647064208984
Validation loss: 2.026167333126068

Epoch: 6| Step: 11
Training loss: 2.696220636367798
Validation loss: 2.0182754000027976

Epoch: 6| Step: 12
Training loss: 2.675562620162964
Validation loss: 2.0211537877718606

Epoch: 6| Step: 13
Training loss: 2.0200257301330566
Validation loss: 2.0131744742393494

Epoch: 109| Step: 0
Training loss: 2.259720802307129
Validation loss: 2.023214022318522

Epoch: 6| Step: 1
Training loss: 1.9965797662734985
Validation loss: 2.0187658270200095

Epoch: 6| Step: 2
Training loss: 1.98289155960083
Validation loss: 2.0173690716425576

Epoch: 6| Step: 3
Training loss: 2.2436251640319824
Validation loss: 2.012170692284902

Epoch: 6| Step: 4
Training loss: 1.9689691066741943
Validation loss: 2.0183049043019614

Epoch: 6| Step: 5
Training loss: 2.4893383979797363
Validation loss: 2.020427127679189

Epoch: 6| Step: 6
Training loss: 2.0034267902374268
Validation loss: 2.02492618560791

Epoch: 6| Step: 7
Training loss: 1.9469711780548096
Validation loss: 2.014943460623423

Epoch: 6| Step: 8
Training loss: 1.9416106939315796
Validation loss: 2.0231298406918845

Epoch: 6| Step: 9
Training loss: 2.8108162879943848
Validation loss: 2.0179081360499063

Epoch: 6| Step: 10
Training loss: 1.9684911966323853
Validation loss: 2.0165698528289795

Epoch: 6| Step: 11
Training loss: 2.1393730640411377
Validation loss: 2.0169663031895957

Epoch: 6| Step: 12
Training loss: 2.0476927757263184
Validation loss: 2.018682897090912

Epoch: 6| Step: 13
Training loss: 1.8131208419799805
Validation loss: 2.0260620514551797

Epoch: 110| Step: 0
Training loss: 2.7728116512298584
Validation loss: 2.0249303778012595

Epoch: 6| Step: 1
Training loss: 2.6382627487182617
Validation loss: 2.035933534304301

Epoch: 6| Step: 2
Training loss: 2.094132900238037
Validation loss: 2.033921758333842

Epoch: 6| Step: 3
Training loss: 1.9742616415023804
Validation loss: 2.036467711130778

Epoch: 6| Step: 4
Training loss: 1.7168081998825073
Validation loss: 2.0516630013783774

Epoch: 6| Step: 5
Training loss: 2.3661060333251953
Validation loss: 2.0584994554519653

Epoch: 6| Step: 6
Training loss: 1.709986686706543
Validation loss: 2.0597890814145408

Epoch: 6| Step: 7
Training loss: 2.3907601833343506
Validation loss: 2.0475473006566367

Epoch: 6| Step: 8
Training loss: 1.9005870819091797
Validation loss: 2.0377726356188455

Epoch: 6| Step: 9
Training loss: 1.826720952987671
Validation loss: 2.0246698458989463

Epoch: 6| Step: 10
Training loss: 1.67738938331604
Validation loss: 2.0128901600837708

Epoch: 6| Step: 11
Training loss: 2.3227698802948
Validation loss: 2.012714664141337

Epoch: 6| Step: 12
Training loss: 2.411929130554199
Validation loss: 2.0122971534729004

Epoch: 6| Step: 13
Training loss: 2.1002414226531982
Validation loss: 2.019790848096212

Epoch: 111| Step: 0
Training loss: 2.5431530475616455
Validation loss: 2.0226489702860513

Epoch: 6| Step: 1
Training loss: 2.131251335144043
Validation loss: 2.023608386516571

Epoch: 6| Step: 2
Training loss: 1.7398273944854736
Validation loss: 2.0212810039520264

Epoch: 6| Step: 3
Training loss: 2.336129903793335
Validation loss: 2.0232563813527427

Epoch: 6| Step: 4
Training loss: 2.1056060791015625
Validation loss: 2.0303998390833535

Epoch: 6| Step: 5
Training loss: 2.2160611152648926
Validation loss: 2.023199141025543

Epoch: 6| Step: 6
Training loss: 2.3236751556396484
Validation loss: 2.030712346235911

Epoch: 6| Step: 7
Training loss: 2.387108325958252
Validation loss: 2.0248613754908242

Epoch: 6| Step: 8
Training loss: 2.210174322128296
Validation loss: 2.0232362151145935

Epoch: 6| Step: 9
Training loss: 1.73073410987854
Validation loss: 2.0248229106267295

Epoch: 6| Step: 10
Training loss: 2.3021507263183594
Validation loss: 2.015902837117513

Epoch: 6| Step: 11
Training loss: 1.97085702419281
Validation loss: 2.0068958004315696

Epoch: 6| Step: 12
Training loss: 2.4330739974975586
Validation loss: 2.0082415342330933

Epoch: 6| Step: 13
Training loss: 1.8498597145080566
Validation loss: 2.0071665247281394

Epoch: 112| Step: 0
Training loss: 2.2856545448303223
Validation loss: 2.009621242682139

Epoch: 6| Step: 1
Training loss: 1.7195312976837158
Validation loss: 2.0081526835759482

Epoch: 6| Step: 2
Training loss: 2.5459909439086914
Validation loss: 2.0247835318247476

Epoch: 6| Step: 3
Training loss: 2.0732226371765137
Validation loss: 2.030141313870748

Epoch: 6| Step: 4
Training loss: 2.0176711082458496
Validation loss: 2.030266503492991

Epoch: 6| Step: 5
Training loss: 1.9947775602340698
Validation loss: 2.039954682191213

Epoch: 6| Step: 6
Training loss: 2.1562538146972656
Validation loss: 2.042273461818695

Epoch: 6| Step: 7
Training loss: 2.6050221920013428
Validation loss: 2.042080501715342

Epoch: 6| Step: 8
Training loss: 1.884857177734375
Validation loss: 2.043532133102417

Epoch: 6| Step: 9
Training loss: 1.7582852840423584
Validation loss: 2.0470697482426963

Epoch: 6| Step: 10
Training loss: 2.7833311557769775
Validation loss: 2.042587081591288

Epoch: 6| Step: 11
Training loss: 1.9756357669830322
Validation loss: 2.040721813837687

Epoch: 6| Step: 12
Training loss: 2.1485447883605957
Validation loss: 2.0384198427200317

Epoch: 6| Step: 13
Training loss: 1.816753625869751
Validation loss: 2.0313086112340293

Epoch: 113| Step: 0
Training loss: 2.647843599319458
Validation loss: 2.013459026813507

Epoch: 6| Step: 1
Training loss: 2.0010437965393066
Validation loss: 2.019356290499369

Epoch: 6| Step: 2
Training loss: 1.5407202243804932
Validation loss: 2.0201224287350974

Epoch: 6| Step: 3
Training loss: 2.1182665824890137
Validation loss: 2.019708514213562

Epoch: 6| Step: 4
Training loss: 1.5671504735946655
Validation loss: 2.017508943875631

Epoch: 6| Step: 5
Training loss: 2.470644474029541
Validation loss: 2.032877484957377

Epoch: 6| Step: 6
Training loss: 1.9000787734985352
Validation loss: 2.0289807518323264

Epoch: 6| Step: 7
Training loss: 2.448716163635254
Validation loss: 2.0288646817207336

Epoch: 6| Step: 8
Training loss: 2.311704158782959
Validation loss: 2.0268747806549072

Epoch: 6| Step: 9
Training loss: 1.813982367515564
Validation loss: 2.0322441856066384

Epoch: 6| Step: 10
Training loss: 2.4321818351745605
Validation loss: 2.020935912926992

Epoch: 6| Step: 11
Training loss: 2.260568618774414
Validation loss: 2.01808754603068

Epoch: 6| Step: 12
Training loss: 2.5209219455718994
Validation loss: 2.017891983191172

Epoch: 6| Step: 13
Training loss: 1.9493261575698853
Validation loss: 2.0208833416303

Epoch: 114| Step: 0
Training loss: 2.811260223388672
Validation loss: 2.019605020682017

Epoch: 6| Step: 1
Training loss: 1.9613014459609985
Validation loss: 2.0208346843719482

Epoch: 6| Step: 2
Training loss: 1.833556056022644
Validation loss: 2.0288227796554565

Epoch: 6| Step: 3
Training loss: 2.0241923332214355
Validation loss: 2.0218345522880554

Epoch: 6| Step: 4
Training loss: 1.9696696996688843
Validation loss: 2.036868433157603

Epoch: 6| Step: 5
Training loss: 2.020556926727295
Validation loss: 2.0287673473358154

Epoch: 6| Step: 6
Training loss: 2.1069345474243164
Validation loss: 2.034121513366699

Epoch: 6| Step: 7
Training loss: 2.0883610248565674
Validation loss: 2.041050453980764

Epoch: 6| Step: 8
Training loss: 2.0498127937316895
Validation loss: 2.0360898971557617

Epoch: 6| Step: 9
Training loss: 2.1760025024414062
Validation loss: 2.0425142447153726

Epoch: 6| Step: 10
Training loss: 2.7401485443115234
Validation loss: 2.0460341374079385

Epoch: 6| Step: 11
Training loss: 2.5762155055999756
Validation loss: 2.041137437025706

Epoch: 6| Step: 12
Training loss: 1.6425964832305908
Validation loss: 2.054122567176819

Epoch: 6| Step: 13
Training loss: 1.741032361984253
Validation loss: 2.027614255746206

Epoch: 115| Step: 0
Training loss: 2.2265267372131348
Validation loss: 2.037510931491852

Epoch: 6| Step: 1
Training loss: 1.852752923965454
Validation loss: 2.035370131333669

Epoch: 6| Step: 2
Training loss: 2.2614235877990723
Validation loss: 2.0281856854756675

Epoch: 6| Step: 3
Training loss: 1.9112679958343506
Validation loss: 2.034152328968048

Epoch: 6| Step: 4
Training loss: 1.5628771781921387
Validation loss: 2.0304007728894553

Epoch: 6| Step: 5
Training loss: 2.138812780380249
Validation loss: 2.0288315614064536

Epoch: 6| Step: 6
Training loss: 2.7170839309692383
Validation loss: 2.0293036699295044

Epoch: 6| Step: 7
Training loss: 1.559915542602539
Validation loss: 2.0205031832059226

Epoch: 6| Step: 8
Training loss: 2.3387670516967773
Validation loss: 2.013859828313192

Epoch: 6| Step: 9
Training loss: 1.8355882167816162
Validation loss: 2.0194643139839172

Epoch: 6| Step: 10
Training loss: 2.1559224128723145
Validation loss: 2.014012038707733

Epoch: 6| Step: 11
Training loss: 2.022331714630127
Validation loss: 2.010265509287516

Epoch: 6| Step: 12
Training loss: 2.9213619232177734
Validation loss: 2.0138646761576333

Epoch: 6| Step: 13
Training loss: 2.0763487815856934
Validation loss: 2.0113830963770547

Epoch: 116| Step: 0
Training loss: 1.8052988052368164
Validation loss: 2.0134257276852927

Epoch: 6| Step: 1
Training loss: 2.2635912895202637
Validation loss: 2.0132822593053183

Epoch: 6| Step: 2
Training loss: 2.360177755355835
Validation loss: 2.0164781411488852

Epoch: 6| Step: 3
Training loss: 2.477386474609375
Validation loss: 2.022312819957733

Epoch: 6| Step: 4
Training loss: 2.0177597999572754
Validation loss: 2.0160589019457498

Epoch: 6| Step: 5
Training loss: 1.3203251361846924
Validation loss: 2.0142921209335327

Epoch: 6| Step: 6
Training loss: 2.339658737182617
Validation loss: 2.015471080938975

Epoch: 6| Step: 7
Training loss: 2.1311748027801514
Validation loss: 2.016928493976593

Epoch: 6| Step: 8
Training loss: 2.2754077911376953
Validation loss: 2.0306947032610574

Epoch: 6| Step: 9
Training loss: 2.1508278846740723
Validation loss: 2.0269302129745483

Epoch: 6| Step: 10
Training loss: 2.311100482940674
Validation loss: 2.031684656937917

Epoch: 6| Step: 11
Training loss: 2.092217445373535
Validation loss: 2.029452462991079

Epoch: 6| Step: 12
Training loss: 1.865921139717102
Validation loss: 2.042443593343099

Epoch: 6| Step: 13
Training loss: 1.9564502239227295
Validation loss: 2.039692978064219

Epoch: 117| Step: 0
Training loss: 2.312612533569336
Validation loss: 2.0510791738828025

Epoch: 6| Step: 1
Training loss: 2.0749895572662354
Validation loss: 2.0470527013142905

Epoch: 6| Step: 2
Training loss: 2.3970046043395996
Validation loss: 2.045176923274994

Epoch: 6| Step: 3
Training loss: 2.1026268005371094
Validation loss: 2.047426144282023

Epoch: 6| Step: 4
Training loss: 2.1853508949279785
Validation loss: 2.0375653306643167

Epoch: 6| Step: 5
Training loss: 2.0147018432617188
Validation loss: 2.0378153920173645

Epoch: 6| Step: 6
Training loss: 1.8631466627120972
Validation loss: 2.0380157033602395

Epoch: 6| Step: 7
Training loss: 2.3335182666778564
Validation loss: 2.032293140888214

Epoch: 6| Step: 8
Training loss: 1.4955788850784302
Validation loss: 2.014401137828827

Epoch: 6| Step: 9
Training loss: 2.070671796798706
Validation loss: 2.0202348629633584

Epoch: 6| Step: 10
Training loss: 2.199273109436035
Validation loss: 2.017040967941284

Epoch: 6| Step: 11
Training loss: 2.2031617164611816
Validation loss: 2.021512587865194

Epoch: 6| Step: 12
Training loss: 2.208544969558716
Validation loss: 2.0239386558532715

Epoch: 6| Step: 13
Training loss: 2.399608850479126
Validation loss: 2.0374280214309692

Epoch: 118| Step: 0
Training loss: 1.9788627624511719
Validation loss: 2.0305959383646646

Epoch: 6| Step: 1
Training loss: 1.8987826108932495
Validation loss: 2.035049259662628

Epoch: 6| Step: 2
Training loss: 2.4326868057250977
Validation loss: 2.042095979054769

Epoch: 6| Step: 3
Training loss: 2.3321876525878906
Validation loss: 2.0470380385716758

Epoch: 6| Step: 4
Training loss: 2.2111730575561523
Validation loss: 2.0485929052035012

Epoch: 6| Step: 5
Training loss: 2.0891590118408203
Validation loss: 2.0503933429718018

Epoch: 6| Step: 6
Training loss: 2.278027057647705
Validation loss: 2.0271790822347007

Epoch: 6| Step: 7
Training loss: 1.5215394496917725
Validation loss: 2.038856824239095

Epoch: 6| Step: 8
Training loss: 2.3334431648254395
Validation loss: 2.0288949608802795

Epoch: 6| Step: 9
Training loss: 2.6128406524658203
Validation loss: 2.0329866806666055

Epoch: 6| Step: 10
Training loss: 2.6865265369415283
Validation loss: 2.026382307211558

Epoch: 6| Step: 11
Training loss: 1.957517147064209
Validation loss: 2.026127894719442

Epoch: 6| Step: 12
Training loss: 1.566652536392212
Validation loss: 2.025776505470276

Epoch: 6| Step: 13
Training loss: 1.6427788734436035
Validation loss: 2.017669141292572

Epoch: 119| Step: 0
Training loss: 1.9024181365966797
Validation loss: 2.025683104991913

Epoch: 6| Step: 1
Training loss: 2.4764139652252197
Validation loss: 2.0166494051615396

Epoch: 6| Step: 2
Training loss: 1.755489468574524
Validation loss: 2.0224035580952964

Epoch: 6| Step: 3
Training loss: 1.5106761455535889
Validation loss: 2.0252082347869873

Epoch: 6| Step: 4
Training loss: 2.3630459308624268
Validation loss: 2.0246575673421225

Epoch: 6| Step: 5
Training loss: 2.199885845184326
Validation loss: 2.015997608502706

Epoch: 6| Step: 6
Training loss: 2.219606399536133
Validation loss: 2.0148577094078064

Epoch: 6| Step: 7
Training loss: 2.6856696605682373
Validation loss: 2.019392251968384

Epoch: 6| Step: 8
Training loss: 1.4873318672180176
Validation loss: 2.0212594270706177

Epoch: 6| Step: 9
Training loss: 2.3593363761901855
Validation loss: 2.024197260538737

Epoch: 6| Step: 10
Training loss: 2.8255667686462402
Validation loss: 2.022833446661631

Epoch: 6| Step: 11
Training loss: 1.645157814025879
Validation loss: 2.033058226108551

Epoch: 6| Step: 12
Training loss: 1.894068956375122
Validation loss: 2.036369740962982

Epoch: 6| Step: 13
Training loss: 2.2433366775512695
Validation loss: 2.0354706247647605

Epoch: 120| Step: 0
Training loss: 1.8973153829574585
Validation loss: 2.0447715322176614

Epoch: 6| Step: 1
Training loss: 1.5518616437911987
Validation loss: 2.0515220165252686

Epoch: 6| Step: 2
Training loss: 2.208120346069336
Validation loss: 2.0590312282244363

Epoch: 6| Step: 3
Training loss: 2.436861991882324
Validation loss: 2.0546621282895408

Epoch: 6| Step: 4
Training loss: 1.4838818311691284
Validation loss: 2.04962557554245

Epoch: 6| Step: 5
Training loss: 1.8610575199127197
Validation loss: 2.0507513682047525

Epoch: 6| Step: 6
Training loss: 2.1587295532226562
Validation loss: 2.044141948223114

Epoch: 6| Step: 7
Training loss: 2.1282544136047363
Validation loss: 2.0419790148735046

Epoch: 6| Step: 8
Training loss: 1.891116738319397
Validation loss: 2.0432520707448325

Epoch: 6| Step: 9
Training loss: 3.0136117935180664
Validation loss: 2.049832542737325

Epoch: 6| Step: 10
Training loss: 1.6626310348510742
Validation loss: 2.0501741965611777

Epoch: 6| Step: 11
Training loss: 2.8372159004211426
Validation loss: 2.0412095189094543

Epoch: 6| Step: 12
Training loss: 1.8671624660491943
Validation loss: 2.0384580890337625

Epoch: 6| Step: 13
Training loss: 2.377875804901123
Validation loss: 2.0360314647356668

Epoch: 121| Step: 0
Training loss: 2.1233677864074707
Validation loss: 2.0351322293281555

Epoch: 6| Step: 1
Training loss: 2.108358383178711
Validation loss: 2.03780597448349

Epoch: 6| Step: 2
Training loss: 1.7950018644332886
Validation loss: 2.0321425795555115

Epoch: 6| Step: 3
Training loss: 2.2849371433258057
Validation loss: 2.0221350391705832

Epoch: 6| Step: 4
Training loss: 2.0313358306884766
Validation loss: 2.0284979343414307

Epoch: 6| Step: 5
Training loss: 2.4843921661376953
Validation loss: 2.030307730038961

Epoch: 6| Step: 6
Training loss: 2.252692699432373
Validation loss: 2.0309130549430847

Epoch: 6| Step: 7
Training loss: 2.633481025695801
Validation loss: 2.035344739754995

Epoch: 6| Step: 8
Training loss: 2.0649938583374023
Validation loss: 2.0332485834757485

Epoch: 6| Step: 9
Training loss: 2.1542439460754395
Validation loss: 2.026757021745046

Epoch: 6| Step: 10
Training loss: 1.6943104267120361
Validation loss: 2.020422418912252

Epoch: 6| Step: 11
Training loss: 1.7033236026763916
Validation loss: 2.025506774584452

Epoch: 6| Step: 12
Training loss: 2.56610107421875
Validation loss: 2.0252685149510703

Epoch: 6| Step: 13
Training loss: 1.8688914775848389
Validation loss: 2.0203221638997397

Epoch: 122| Step: 0
Training loss: 1.967160701751709
Validation loss: 2.032313128312429

Epoch: 6| Step: 1
Training loss: 1.8463459014892578
Validation loss: 2.0322428941726685

Epoch: 6| Step: 2
Training loss: 1.8088209629058838
Validation loss: 2.046106199423472

Epoch: 6| Step: 3
Training loss: 2.4248766899108887
Validation loss: 2.0440619587898254

Epoch: 6| Step: 4
Training loss: 2.318892478942871
Validation loss: 2.0432735085487366

Epoch: 6| Step: 5
Training loss: 1.971339464187622
Validation loss: 2.049268960952759

Epoch: 6| Step: 6
Training loss: 1.641167163848877
Validation loss: 2.0509522954622903

Epoch: 6| Step: 7
Training loss: 2.5444741249084473
Validation loss: 2.043726623058319

Epoch: 6| Step: 8
Training loss: 2.0372142791748047
Validation loss: 2.055639147758484

Epoch: 6| Step: 9
Training loss: 2.581082344055176
Validation loss: 2.050651411215464

Epoch: 6| Step: 10
Training loss: 2.6276156902313232
Validation loss: 2.0353062550226846

Epoch: 6| Step: 11
Training loss: 2.278693914413452
Validation loss: 2.050213893254598

Epoch: 6| Step: 12
Training loss: 2.1797049045562744
Validation loss: 2.042733589808146

Epoch: 6| Step: 13
Training loss: 1.549562692642212
Validation loss: 2.04725315173467

Epoch: 123| Step: 0
Training loss: 1.9483567476272583
Validation loss: 2.0307000080744424

Epoch: 6| Step: 1
Training loss: 2.20648193359375
Validation loss: 2.0296160777409873

Epoch: 6| Step: 2
Training loss: 2.0282487869262695
Validation loss: 2.031926691532135

Epoch: 6| Step: 3
Training loss: 2.0191352367401123
Validation loss: 2.027649720509847

Epoch: 6| Step: 4
Training loss: 2.22855806350708
Validation loss: 2.016814927260081

Epoch: 6| Step: 5
Training loss: 2.7014071941375732
Validation loss: 2.0253661274909973

Epoch: 6| Step: 6
Training loss: 2.3625550270080566
Validation loss: 2.0260994831720986

Epoch: 6| Step: 7
Training loss: 1.7098859548568726
Validation loss: 2.024558881918589

Epoch: 6| Step: 8
Training loss: 1.9592913389205933
Validation loss: 2.0338173707326255

Epoch: 6| Step: 9
Training loss: 1.8710675239562988
Validation loss: 2.0268125335375466

Epoch: 6| Step: 10
Training loss: 2.6940994262695312
Validation loss: 2.016882677872976

Epoch: 6| Step: 11
Training loss: 1.298691749572754
Validation loss: 2.026279012362162

Epoch: 6| Step: 12
Training loss: 2.5041139125823975
Validation loss: 2.0292659600575766

Epoch: 6| Step: 13
Training loss: 2.0063858032226562
Validation loss: 2.028826137383779

Epoch: 124| Step: 0
Training loss: 2.218722343444824
Validation loss: 2.023914337158203

Epoch: 6| Step: 1
Training loss: 1.7014520168304443
Validation loss: 2.027820865313212

Epoch: 6| Step: 2
Training loss: 1.9797308444976807
Validation loss: 2.0336374243100486

Epoch: 6| Step: 3
Training loss: 1.7835869789123535
Validation loss: 2.0309616525967917

Epoch: 6| Step: 4
Training loss: 1.9071693420410156
Validation loss: 2.045894463857015

Epoch: 6| Step: 5
Training loss: 2.026806116104126
Validation loss: 2.043949822584788

Epoch: 6| Step: 6
Training loss: 2.315194606781006
Validation loss: 2.0407119194666543

Epoch: 6| Step: 7
Training loss: 2.410653591156006
Validation loss: 2.0553123553593955

Epoch: 6| Step: 8
Training loss: 1.5656518936157227
Validation loss: 2.0514971017837524

Epoch: 6| Step: 9
Training loss: 2.3548336029052734
Validation loss: 2.039529581864675

Epoch: 6| Step: 10
Training loss: 2.533811569213867
Validation loss: 2.0353806813557944

Epoch: 6| Step: 11
Training loss: 2.510439395904541
Validation loss: 2.036931097507477

Epoch: 6| Step: 12
Training loss: 2.0647971630096436
Validation loss: 2.0245174566904702

Epoch: 6| Step: 13
Training loss: 2.0593390464782715
Validation loss: 2.025177856286367

Epoch: 125| Step: 0
Training loss: 2.2680461406707764
Validation loss: 2.014562427997589

Epoch: 6| Step: 1
Training loss: 2.0715818405151367
Validation loss: 2.020260512828827

Epoch: 6| Step: 2
Training loss: 2.0686793327331543
Validation loss: 2.0113167762756348

Epoch: 6| Step: 3
Training loss: 2.0935182571411133
Validation loss: 2.013916770617167

Epoch: 6| Step: 4
Training loss: 1.986373782157898
Validation loss: 2.0159063140551248

Epoch: 6| Step: 5
Training loss: 2.2325973510742188
Validation loss: 2.0247398614883423

Epoch: 6| Step: 6
Training loss: 1.403908133506775
Validation loss: 2.025845686594645

Epoch: 6| Step: 7
Training loss: 2.163727283477783
Validation loss: 2.0268131494522095

Epoch: 6| Step: 8
Training loss: 2.3638486862182617
Validation loss: 2.0335590640703836

Epoch: 6| Step: 9
Training loss: 2.353445053100586
Validation loss: 2.0381635228792825

Epoch: 6| Step: 10
Training loss: 1.641916036605835
Validation loss: 2.0305431286493936

Epoch: 6| Step: 11
Training loss: 1.8716342449188232
Validation loss: 2.02812530597051

Epoch: 6| Step: 12
Training loss: 2.434866189956665
Validation loss: 2.039144138495127

Epoch: 6| Step: 13
Training loss: 2.591918468475342
Validation loss: 2.038838962713877

Epoch: 126| Step: 0
Training loss: 2.3145699501037598
Validation loss: 2.034258544445038

Epoch: 6| Step: 1
Training loss: 2.1181392669677734
Validation loss: 2.044619301954905

Epoch: 6| Step: 2
Training loss: 1.6877977848052979
Validation loss: 2.0384508768717446

Epoch: 6| Step: 3
Training loss: 2.5824146270751953
Validation loss: 2.0565735499064126

Epoch: 6| Step: 4
Training loss: 2.050088882446289
Validation loss: 2.049590011437734

Epoch: 6| Step: 5
Training loss: 1.8620654344558716
Validation loss: 2.0445527036984763

Epoch: 6| Step: 6
Training loss: 2.1288418769836426
Validation loss: 2.0438425540924072

Epoch: 6| Step: 7
Training loss: 2.155834674835205
Validation loss: 2.036384165287018

Epoch: 6| Step: 8
Training loss: 2.3568601608276367
Validation loss: 2.0412627458572388

Epoch: 6| Step: 9
Training loss: 1.9538086652755737
Validation loss: 2.035463015238444

Epoch: 6| Step: 10
Training loss: 1.6592594385147095
Validation loss: 2.04644376039505

Epoch: 6| Step: 11
Training loss: 2.299116849899292
Validation loss: 2.0499310294787088

Epoch: 6| Step: 12
Training loss: 2.46463680267334
Validation loss: 2.045918345451355

Epoch: 6| Step: 13
Training loss: 1.7835018634796143
Validation loss: 2.0524189869562783

Epoch: 127| Step: 0
Training loss: 2.9251255989074707
Validation loss: 2.047965407371521

Epoch: 6| Step: 1
Training loss: 2.8526177406311035
Validation loss: 2.054859717686971

Epoch: 6| Step: 2
Training loss: 1.7326058149337769
Validation loss: 2.0447612206141152

Epoch: 6| Step: 3
Training loss: 2.4319965839385986
Validation loss: 2.0310417811075845

Epoch: 6| Step: 4
Training loss: 2.2193593978881836
Validation loss: 2.040411909421285

Epoch: 6| Step: 5
Training loss: 1.569093942642212
Validation loss: 2.041639506816864

Epoch: 6| Step: 6
Training loss: 2.1108481884002686
Validation loss: 2.031461477279663

Epoch: 6| Step: 7
Training loss: 1.8477524518966675
Validation loss: 2.031178295612335

Epoch: 6| Step: 8
Training loss: 1.7062139511108398
Validation loss: 2.0301539301872253

Epoch: 6| Step: 9
Training loss: 2.4127233028411865
Validation loss: 2.035378416379293

Epoch: 6| Step: 10
Training loss: 1.92708420753479
Validation loss: 2.0379791061083474

Epoch: 6| Step: 11
Training loss: 2.0826926231384277
Validation loss: 2.0383116205533347

Epoch: 6| Step: 12
Training loss: 1.7408238649368286
Validation loss: 2.036086618900299

Epoch: 6| Step: 13
Training loss: 1.827291488647461
Validation loss: 2.0359378655751548

Epoch: 128| Step: 0
Training loss: 2.838571548461914
Validation loss: 2.0331930915514627

Epoch: 6| Step: 1
Training loss: 1.6790235042572021
Validation loss: 2.033056398232778

Epoch: 6| Step: 2
Training loss: 2.137266159057617
Validation loss: 2.0330741802851358

Epoch: 6| Step: 3
Training loss: 1.7477693557739258
Validation loss: 2.0406024058659873

Epoch: 6| Step: 4
Training loss: 1.9434349536895752
Validation loss: 2.0441916982332864

Epoch: 6| Step: 5
Training loss: 2.4767041206359863
Validation loss: 2.0456449588139853

Epoch: 6| Step: 6
Training loss: 2.2756412029266357
Validation loss: 2.0473342339197793

Epoch: 6| Step: 7
Training loss: 1.8449327945709229
Validation loss: 2.0473023653030396

Epoch: 6| Step: 8
Training loss: 2.0767040252685547
Validation loss: 2.0398807326952615

Epoch: 6| Step: 9
Training loss: 2.6769354343414307
Validation loss: 2.050079584121704

Epoch: 6| Step: 10
Training loss: 1.5918314456939697
Validation loss: 2.0457425912221274

Epoch: 6| Step: 11
Training loss: 2.522432804107666
Validation loss: 2.048972964286804

Epoch: 6| Step: 12
Training loss: 1.883620262145996
Validation loss: 2.0336067279179892

Epoch: 6| Step: 13
Training loss: 1.6379613876342773
Validation loss: 2.0338931679725647

Epoch: 129| Step: 0
Training loss: 2.213512897491455
Validation loss: 2.030399978160858

Epoch: 6| Step: 1
Training loss: 2.3013060092926025
Validation loss: 2.0367912650108337

Epoch: 6| Step: 2
Training loss: 1.907320499420166
Validation loss: 2.036273936430613

Epoch: 6| Step: 3
Training loss: 2.3494372367858887
Validation loss: 2.031254291534424

Epoch: 6| Step: 4
Training loss: 1.6582934856414795
Validation loss: 2.035448908805847

Epoch: 6| Step: 5
Training loss: 2.072347402572632
Validation loss: 2.0418129563331604

Epoch: 6| Step: 6
Training loss: 1.8691757917404175
Validation loss: 2.0299020806948342

Epoch: 6| Step: 7
Training loss: 2.659257411956787
Validation loss: 2.0397584438323975

Epoch: 6| Step: 8
Training loss: 2.0632097721099854
Validation loss: 2.0344616373380027

Epoch: 6| Step: 9
Training loss: 2.8075497150421143
Validation loss: 2.034789184729258

Epoch: 6| Step: 10
Training loss: 2.0054259300231934
Validation loss: 2.0419654846191406

Epoch: 6| Step: 11
Training loss: 1.6007487773895264
Validation loss: 2.0488219062487283

Epoch: 6| Step: 12
Training loss: 1.7230536937713623
Validation loss: 2.0439813335736594

Epoch: 6| Step: 13
Training loss: 2.326606512069702
Validation loss: 2.046315014362335

Epoch: 130| Step: 0
Training loss: 1.9218332767486572
Validation loss: 2.050219496091207

Epoch: 6| Step: 1
Training loss: 1.5976290702819824
Validation loss: 2.046978930632273

Epoch: 6| Step: 2
Training loss: 1.656123161315918
Validation loss: 2.057485342025757

Epoch: 6| Step: 3
Training loss: 1.7076425552368164
Validation loss: 2.0543945829073587

Epoch: 6| Step: 4
Training loss: 1.9891119003295898
Validation loss: 2.0538858771324158

Epoch: 6| Step: 5
Training loss: 3.2082176208496094
Validation loss: 2.0528053840001426

Epoch: 6| Step: 6
Training loss: 2.8408586978912354
Validation loss: 2.052945613861084

Epoch: 6| Step: 7
Training loss: 1.7182281017303467
Validation loss: 2.0485738714536033

Epoch: 6| Step: 8
Training loss: 2.081151247024536
Validation loss: 2.052046994368235

Epoch: 6| Step: 9
Training loss: 2.0370635986328125
Validation loss: 2.0532798767089844

Epoch: 6| Step: 10
Training loss: 2.5060195922851562
Validation loss: 2.0382885336875916

Epoch: 6| Step: 11
Training loss: 2.2713356018066406
Validation loss: 2.0450804829597473

Epoch: 6| Step: 12
Training loss: 1.5954103469848633
Validation loss: 2.0406499902407327

Epoch: 6| Step: 13
Training loss: 2.05108642578125
Validation loss: 2.037805994351705

Epoch: 131| Step: 0
Training loss: 2.523458480834961
Validation loss: 2.033788045247396

Epoch: 6| Step: 1
Training loss: 2.2325916290283203
Validation loss: 2.0330716371536255

Epoch: 6| Step: 2
Training loss: 2.2083077430725098
Validation loss: 2.030988176663717

Epoch: 6| Step: 3
Training loss: 2.215723991394043
Validation loss: 2.034611145655314

Epoch: 6| Step: 4
Training loss: 2.7534937858581543
Validation loss: 2.035385270913442

Epoch: 6| Step: 5
Training loss: 1.7530179023742676
Validation loss: 2.0290122429529824

Epoch: 6| Step: 6
Training loss: 1.4801924228668213
Validation loss: 2.0397223830223083

Epoch: 6| Step: 7
Training loss: 1.3013733625411987
Validation loss: 2.0492860277493796

Epoch: 6| Step: 8
Training loss: 1.9729506969451904
Validation loss: 2.0406516393025718

Epoch: 6| Step: 9
Training loss: 2.1643733978271484
Validation loss: 2.045313080151876

Epoch: 6| Step: 10
Training loss: 2.264815330505371
Validation loss: 2.0473705331484475

Epoch: 6| Step: 11
Training loss: 2.1115503311157227
Validation loss: 2.045147975285848

Epoch: 6| Step: 12
Training loss: 2.133866548538208
Validation loss: 2.0550844073295593

Epoch: 6| Step: 13
Training loss: 2.5946593284606934
Validation loss: 2.0453144709269204

Epoch: 132| Step: 0
Training loss: 2.5346508026123047
Validation loss: 2.0409148732821145

Epoch: 6| Step: 1
Training loss: 2.3056375980377197
Validation loss: 2.0402427911758423

Epoch: 6| Step: 2
Training loss: 2.3487823009490967
Validation loss: 2.03875325123469

Epoch: 6| Step: 3
Training loss: 2.072509527206421
Validation loss: 2.0415102442105613

Epoch: 6| Step: 4
Training loss: 1.5915415287017822
Validation loss: 2.0367738008499146

Epoch: 6| Step: 5
Training loss: 1.3141076564788818
Validation loss: 2.0337913831075034

Epoch: 6| Step: 6
Training loss: 1.7936973571777344
Validation loss: 2.044907788435618

Epoch: 6| Step: 7
Training loss: 1.6960670948028564
Validation loss: 2.0462308128674827

Epoch: 6| Step: 8
Training loss: 1.9542653560638428
Validation loss: 2.0381187001864114

Epoch: 6| Step: 9
Training loss: 2.335695505142212
Validation loss: 2.0368029475212097

Epoch: 6| Step: 10
Training loss: 2.482419729232788
Validation loss: 2.038827915986379

Epoch: 6| Step: 11
Training loss: 2.298100471496582
Validation loss: 2.03927210966746

Epoch: 6| Step: 12
Training loss: 2.5805065631866455
Validation loss: 2.0437626441319785

Epoch: 6| Step: 13
Training loss: 1.9524341821670532
Validation loss: 2.03612486521403

Epoch: 133| Step: 0
Training loss: 2.149716377258301
Validation loss: 2.052650233109792

Epoch: 6| Step: 1
Training loss: 2.012279510498047
Validation loss: 2.0507708191871643

Epoch: 6| Step: 2
Training loss: 1.8682572841644287
Validation loss: 2.0565621852874756

Epoch: 6| Step: 3
Training loss: 1.7144418954849243
Validation loss: 2.065384566783905

Epoch: 6| Step: 4
Training loss: 2.2925214767456055
Validation loss: 2.065237601598104

Epoch: 6| Step: 5
Training loss: 2.2305617332458496
Validation loss: 2.052100042502085

Epoch: 6| Step: 6
Training loss: 1.7300537824630737
Validation loss: 2.0538674791653952

Epoch: 6| Step: 7
Training loss: 2.5643534660339355
Validation loss: 2.0469664931297302

Epoch: 6| Step: 8
Training loss: 2.158710479736328
Validation loss: 2.0586023330688477

Epoch: 6| Step: 9
Training loss: 2.4416282176971436
Validation loss: 2.0535250703493753

Epoch: 6| Step: 10
Training loss: 2.4130561351776123
Validation loss: 2.0474586884180703

Epoch: 6| Step: 11
Training loss: 2.110562324523926
Validation loss: 2.0407830675443015

Epoch: 6| Step: 12
Training loss: 1.982691764831543
Validation loss: 2.033103624979655

Epoch: 6| Step: 13
Training loss: 1.7472819089889526
Validation loss: 2.031499663988749

Epoch: 134| Step: 0
Training loss: 1.7737305164337158
Validation loss: 2.034339110056559

Epoch: 6| Step: 1
Training loss: 1.8719494342803955
Validation loss: 2.029062032699585

Epoch: 6| Step: 2
Training loss: 2.169609546661377
Validation loss: 2.0308454632759094

Epoch: 6| Step: 3
Training loss: 1.8971989154815674
Validation loss: 2.032309651374817

Epoch: 6| Step: 4
Training loss: 1.5849084854125977
Validation loss: 2.036700169245402

Epoch: 6| Step: 5
Training loss: 1.714397668838501
Validation loss: 2.0339321891466775

Epoch: 6| Step: 6
Training loss: 2.335848093032837
Validation loss: 2.0307931502660117

Epoch: 6| Step: 7
Training loss: 2.2231345176696777
Validation loss: 2.0453570087750754

Epoch: 6| Step: 8
Training loss: 2.0593156814575195
Validation loss: 2.033563733100891

Epoch: 6| Step: 9
Training loss: 2.099651575088501
Validation loss: 2.0405884782473245

Epoch: 6| Step: 10
Training loss: 1.7167729139328003
Validation loss: 2.043997645378113

Epoch: 6| Step: 11
Training loss: 3.1687216758728027
Validation loss: 2.0459725658098855

Epoch: 6| Step: 12
Training loss: 1.7870306968688965
Validation loss: 2.0407174825668335

Epoch: 6| Step: 13
Training loss: 2.8564348220825195
Validation loss: 2.052202502886454

Epoch: 135| Step: 0
Training loss: 1.7855254411697388
Validation loss: 2.041159669558207

Epoch: 6| Step: 1
Training loss: 1.9146188497543335
Validation loss: 2.0573524634043374

Epoch: 6| Step: 2
Training loss: 2.3263676166534424
Validation loss: 2.0586862564086914

Epoch: 6| Step: 3
Training loss: 1.992527723312378
Validation loss: 2.057349602381388

Epoch: 6| Step: 4
Training loss: 2.0252344608306885
Validation loss: 2.0496040185292563

Epoch: 6| Step: 5
Training loss: 2.049112558364868
Validation loss: 2.051120479901632

Epoch: 6| Step: 6
Training loss: 1.6537138223648071
Validation loss: 2.0495388507843018

Epoch: 6| Step: 7
Training loss: 2.7066714763641357
Validation loss: 2.039360245068868

Epoch: 6| Step: 8
Training loss: 2.084749698638916
Validation loss: 2.043601095676422

Epoch: 6| Step: 9
Training loss: 1.8995800018310547
Validation loss: 2.0475770433743796

Epoch: 6| Step: 10
Training loss: 2.0312094688415527
Validation loss: 2.0419522325197854

Epoch: 6| Step: 11
Training loss: 2.295599937438965
Validation loss: 2.0419899622599282

Epoch: 6| Step: 12
Training loss: 2.658773422241211
Validation loss: 2.047908624013265

Epoch: 6| Step: 13
Training loss: 1.8453969955444336
Validation loss: 2.0400251944859824

Epoch: 136| Step: 0
Training loss: 2.0621466636657715
Validation loss: 2.0503332217534385

Epoch: 6| Step: 1
Training loss: 2.1285438537597656
Validation loss: 2.054318646589915

Epoch: 6| Step: 2
Training loss: 2.1155261993408203
Validation loss: 2.0503488580385842

Epoch: 6| Step: 3
Training loss: 2.034907102584839
Validation loss: 2.039864997069041

Epoch: 6| Step: 4
Training loss: 2.5947208404541016
Validation loss: 2.052394171555837

Epoch: 6| Step: 5
Training loss: 2.1000170707702637
Validation loss: 2.0428748726844788

Epoch: 6| Step: 6
Training loss: 2.109102725982666
Validation loss: 2.046134571234385

Epoch: 6| Step: 7
Training loss: 1.493548035621643
Validation loss: 2.042053699493408

Epoch: 6| Step: 8
Training loss: 1.8868902921676636
Validation loss: 2.0355005860328674

Epoch: 6| Step: 9
Training loss: 2.8484745025634766
Validation loss: 2.0485365986824036

Epoch: 6| Step: 10
Training loss: 1.354245662689209
Validation loss: 2.0451658566792807

Epoch: 6| Step: 11
Training loss: 2.058598041534424
Validation loss: 2.0574683944384256

Epoch: 6| Step: 12
Training loss: 2.3510689735412598
Validation loss: 2.0575608611106873

Epoch: 6| Step: 13
Training loss: 2.2731480598449707
Validation loss: 2.0543744961420694

Epoch: 137| Step: 0
Training loss: 2.7823047637939453
Validation loss: 2.0618637005488076

Epoch: 6| Step: 1
Training loss: 1.742365837097168
Validation loss: 2.0608847538630166

Epoch: 6| Step: 2
Training loss: 2.2672674655914307
Validation loss: 2.0652259588241577

Epoch: 6| Step: 3
Training loss: 2.018820285797119
Validation loss: 2.059200088183085

Epoch: 6| Step: 4
Training loss: 2.298987865447998
Validation loss: 2.058456540107727

Epoch: 6| Step: 5
Training loss: 1.9345368146896362
Validation loss: 2.0567951003710427

Epoch: 6| Step: 6
Training loss: 1.8437904119491577
Validation loss: 2.0693780382474265

Epoch: 6| Step: 7
Training loss: 2.0689663887023926
Validation loss: 2.061137557029724

Epoch: 6| Step: 8
Training loss: 2.0505080223083496
Validation loss: 2.0644654830296836

Epoch: 6| Step: 9
Training loss: 2.2376139163970947
Validation loss: 2.065293033917745

Epoch: 6| Step: 10
Training loss: 1.816414713859558
Validation loss: 2.0465539495150247

Epoch: 6| Step: 11
Training loss: 1.943424105644226
Validation loss: 2.052424192428589

Epoch: 6| Step: 12
Training loss: 1.8031895160675049
Validation loss: 2.0417919158935547

Epoch: 6| Step: 13
Training loss: 2.356802463531494
Validation loss: 2.0447722474733987

Epoch: 138| Step: 0
Training loss: 2.84033465385437
Validation loss: 2.0370675524075827

Epoch: 6| Step: 1
Training loss: 2.10990309715271
Validation loss: 2.036533693472544

Epoch: 6| Step: 2
Training loss: 1.9868090152740479
Validation loss: 2.0294282039006553

Epoch: 6| Step: 3
Training loss: 1.8830080032348633
Validation loss: 2.0364882349967957

Epoch: 6| Step: 4
Training loss: 1.8934868574142456
Validation loss: 2.032084027926127

Epoch: 6| Step: 5
Training loss: 2.1141185760498047
Validation loss: 2.030586024125417

Epoch: 6| Step: 6
Training loss: 1.705039620399475
Validation loss: 2.0309035579363504

Epoch: 6| Step: 7
Training loss: 2.966367721557617
Validation loss: 2.024221360683441

Epoch: 6| Step: 8
Training loss: 1.9915179014205933
Validation loss: 2.042101184527079

Epoch: 6| Step: 9
Training loss: 1.6799685955047607
Validation loss: 2.039415975411733

Epoch: 6| Step: 10
Training loss: 2.796325206756592
Validation loss: 2.040273904800415

Epoch: 6| Step: 11
Training loss: 1.97515869140625
Validation loss: 2.053943395614624

Epoch: 6| Step: 12
Training loss: 1.930082082748413
Validation loss: 2.063028633594513

Epoch: 6| Step: 13
Training loss: 1.3835346698760986
Validation loss: 2.044546286265055

Epoch: 139| Step: 0
Training loss: 1.6635591983795166
Validation loss: 2.0591699282328286

Epoch: 6| Step: 1
Training loss: 2.0657315254211426
Validation loss: 2.0687766472498574

Epoch: 6| Step: 2
Training loss: 1.6820545196533203
Validation loss: 2.066578765710195

Epoch: 6| Step: 3
Training loss: 2.285107135772705
Validation loss: 2.0643012126286826

Epoch: 6| Step: 4
Training loss: 2.476818084716797
Validation loss: 2.0550783276557922

Epoch: 6| Step: 5
Training loss: 1.8517142534255981
Validation loss: 2.045340597629547

Epoch: 6| Step: 6
Training loss: 1.9260380268096924
Validation loss: 2.04352597395579

Epoch: 6| Step: 7
Training loss: 2.7646710872650146
Validation loss: 2.0442673762639365

Epoch: 6| Step: 8
Training loss: 2.5279510021209717
Validation loss: 2.0363396803538003

Epoch: 6| Step: 9
Training loss: 2.020143508911133
Validation loss: 2.0354576905568442

Epoch: 6| Step: 10
Training loss: 1.940126657485962
Validation loss: 2.0333024859428406

Epoch: 6| Step: 11
Training loss: 2.3205089569091797
Validation loss: 2.039342999458313

Epoch: 6| Step: 12
Training loss: 2.302677869796753
Validation loss: 2.038902143637339

Epoch: 6| Step: 13
Training loss: 1.9492937326431274
Validation loss: 2.0388026436169944

Epoch: 140| Step: 0
Training loss: 2.4953696727752686
Validation loss: 2.035505930582682

Epoch: 6| Step: 1
Training loss: 2.0264267921447754
Validation loss: 2.034431517124176

Epoch: 6| Step: 2
Training loss: 1.9035593271255493
Validation loss: 2.042882482210795

Epoch: 6| Step: 3
Training loss: 2.087632179260254
Validation loss: 2.035593350728353

Epoch: 6| Step: 4
Training loss: 1.50247323513031
Validation loss: 2.032186726729075

Epoch: 6| Step: 5
Training loss: 2.01192569732666
Validation loss: 2.0263800024986267

Epoch: 6| Step: 6
Training loss: 2.0232677459716797
Validation loss: 2.0296761194864907

Epoch: 6| Step: 7
Training loss: 2.2689309120178223
Validation loss: 2.023093859354655

Epoch: 6| Step: 8
Training loss: 2.2256784439086914
Validation loss: 2.025026500225067

Epoch: 6| Step: 9
Training loss: 2.2000434398651123
Validation loss: 2.038180112838745

Epoch: 6| Step: 10
Training loss: 2.0754153728485107
Validation loss: 2.0429474512736

Epoch: 6| Step: 11
Training loss: 2.2811992168426514
Validation loss: 2.044085661570231

Epoch: 6| Step: 12
Training loss: 2.1238646507263184
Validation loss: 2.040411035219828

Epoch: 6| Step: 13
Training loss: 2.0147831439971924
Validation loss: 2.0535474022229514

Epoch: 141| Step: 0
Training loss: 2.259772300720215
Validation loss: 2.0550549626350403

Epoch: 6| Step: 1
Training loss: 2.2824158668518066
Validation loss: 2.060640573501587

Epoch: 6| Step: 2
Training loss: 2.3258979320526123
Validation loss: 2.0614093939463296

Epoch: 6| Step: 3
Training loss: 2.0330312252044678
Validation loss: 2.0562958121299744

Epoch: 6| Step: 4
Training loss: 2.1178669929504395
Validation loss: 2.057531476020813

Epoch: 6| Step: 5
Training loss: 1.7126092910766602
Validation loss: 2.0663484732309976

Epoch: 6| Step: 6
Training loss: 1.8464775085449219
Validation loss: 2.0589949091275535

Epoch: 6| Step: 7
Training loss: 2.371462345123291
Validation loss: 2.070729116598765

Epoch: 6| Step: 8
Training loss: 2.4555892944335938
Validation loss: 2.047684291998545

Epoch: 6| Step: 9
Training loss: 2.0480713844299316
Validation loss: 2.0507054924964905

Epoch: 6| Step: 10
Training loss: 2.119983673095703
Validation loss: 2.0508626302083335

Epoch: 6| Step: 11
Training loss: 1.8812437057495117
Validation loss: 2.039513270060221

Epoch: 6| Step: 12
Training loss: 2.1439874172210693
Validation loss: 2.0265222787857056

Epoch: 6| Step: 13
Training loss: 1.9963716268539429
Validation loss: 2.024454653263092

Epoch: 142| Step: 0
Training loss: 2.4644041061401367
Validation loss: 2.0317198038101196

Epoch: 6| Step: 1
Training loss: 1.497815728187561
Validation loss: 2.02759196360906

Epoch: 6| Step: 2
Training loss: 1.687498688697815
Validation loss: 2.026510993639628

Epoch: 6| Step: 3
Training loss: 1.7053120136260986
Validation loss: 2.0287664930025735

Epoch: 6| Step: 4
Training loss: 2.9246139526367188
Validation loss: 2.034222424030304

Epoch: 6| Step: 5
Training loss: 1.9672983884811401
Validation loss: 2.036068081855774

Epoch: 6| Step: 6
Training loss: 1.919739007949829
Validation loss: 2.0339940587679544

Epoch: 6| Step: 7
Training loss: 2.2865710258483887
Validation loss: 2.037958006064097

Epoch: 6| Step: 8
Training loss: 1.6847562789916992
Validation loss: 2.041422963142395

Epoch: 6| Step: 9
Training loss: 2.549752950668335
Validation loss: 2.0362321734428406

Epoch: 6| Step: 10
Training loss: 2.503885269165039
Validation loss: 2.0390051007270813

Epoch: 6| Step: 11
Training loss: 1.999891996383667
Validation loss: 2.0398376981417337

Epoch: 6| Step: 12
Training loss: 1.7770670652389526
Validation loss: 2.0345271627108255

Epoch: 6| Step: 13
Training loss: 2.413219928741455
Validation loss: 2.041971445083618

Epoch: 143| Step: 0
Training loss: 2.3001534938812256
Validation loss: 2.0396289626757302

Epoch: 6| Step: 1
Training loss: 2.361220121383667
Validation loss: 2.0499629179636636

Epoch: 6| Step: 2
Training loss: 2.099679470062256
Validation loss: 2.05061274766922

Epoch: 6| Step: 3
Training loss: 2.5352163314819336
Validation loss: 2.044792572657267

Epoch: 6| Step: 4
Training loss: 1.9445372819900513
Validation loss: 2.046268045902252

Epoch: 6| Step: 5
Training loss: 1.7437598705291748
Validation loss: 2.0549909273783364

Epoch: 6| Step: 6
Training loss: 2.0419564247131348
Validation loss: 2.0534067948659263

Epoch: 6| Step: 7
Training loss: 2.041254758834839
Validation loss: 2.042985280354818

Epoch: 6| Step: 8
Training loss: 2.5793888568878174
Validation loss: 2.0463284055391946

Epoch: 6| Step: 9
Training loss: 1.681262731552124
Validation loss: 2.0501699248949685

Epoch: 6| Step: 10
Training loss: 1.8229695558547974
Validation loss: 2.0486124952634177

Epoch: 6| Step: 11
Training loss: 1.8644495010375977
Validation loss: 2.049214939276377

Epoch: 6| Step: 12
Training loss: 2.0064096450805664
Validation loss: 2.048474391301473

Epoch: 6| Step: 13
Training loss: 2.411231756210327
Validation loss: 2.05004092057546

Epoch: 144| Step: 0
Training loss: 2.5055124759674072
Validation loss: 2.04998125632604

Epoch: 6| Step: 1
Training loss: 2.1523327827453613
Validation loss: 2.0483258167902627

Epoch: 6| Step: 2
Training loss: 2.157811164855957
Validation loss: 2.0474586884180703

Epoch: 6| Step: 3
Training loss: 1.8493701219558716
Validation loss: 2.0494079987208047

Epoch: 6| Step: 4
Training loss: 1.6026140451431274
Validation loss: 2.0470826824506125

Epoch: 6| Step: 5
Training loss: 2.054128646850586
Validation loss: 2.0515178442001343

Epoch: 6| Step: 6
Training loss: 2.4114584922790527
Validation loss: 2.0520439942677817

Epoch: 6| Step: 7
Training loss: 2.0415496826171875
Validation loss: 2.0490669210751853

Epoch: 6| Step: 8
Training loss: 2.2434215545654297
Validation loss: 2.04586398601532

Epoch: 6| Step: 9
Training loss: 1.5732612609863281
Validation loss: 2.0546228488286338

Epoch: 6| Step: 10
Training loss: 2.3505959510803223
Validation loss: 2.055385688940684

Epoch: 6| Step: 11
Training loss: 2.5674033164978027
Validation loss: 2.055650770664215

Epoch: 6| Step: 12
Training loss: 2.425489902496338
Validation loss: 2.0464163025220237

Epoch: 6| Step: 13
Training loss: 1.295232892036438
Validation loss: 2.0528501868247986

Epoch: 145| Step: 0
Training loss: 2.693906307220459
Validation loss: 2.052822987238566

Epoch: 6| Step: 1
Training loss: 1.6956220865249634
Validation loss: 2.0564761559168496

Epoch: 6| Step: 2
Training loss: 1.537494421005249
Validation loss: 2.0454203287760415

Epoch: 6| Step: 3
Training loss: 1.907975196838379
Validation loss: 2.058007021745046

Epoch: 6| Step: 4
Training loss: 2.7249298095703125
Validation loss: 2.0545948147773743

Epoch: 6| Step: 5
Training loss: 2.4097046852111816
Validation loss: 2.0485400358835855

Epoch: 6| Step: 6
Training loss: 2.081777811050415
Validation loss: 2.057347536087036

Epoch: 6| Step: 7
Training loss: 2.1603434085845947
Validation loss: 2.051675101121267

Epoch: 6| Step: 8
Training loss: 1.892957091331482
Validation loss: 2.0548925002415976

Epoch: 6| Step: 9
Training loss: 2.480095386505127
Validation loss: 2.054464260737101

Epoch: 6| Step: 10
Training loss: 2.2759366035461426
Validation loss: 2.0522698362668357

Epoch: 6| Step: 11
Training loss: 2.199570655822754
Validation loss: 2.0539768735567727

Epoch: 6| Step: 12
Training loss: 1.326925277709961
Validation loss: 2.056603789329529

Epoch: 6| Step: 13
Training loss: 1.936577320098877
Validation loss: 2.058524171511332

Epoch: 146| Step: 0
Training loss: 2.7018446922302246
Validation loss: 2.0528347293535867

Epoch: 6| Step: 1
Training loss: 2.1145358085632324
Validation loss: 2.0583799282709756

Epoch: 6| Step: 2
Training loss: 1.2069464921951294
Validation loss: 2.0538299481074014

Epoch: 6| Step: 3
Training loss: 2.320772171020508
Validation loss: 2.054158329963684

Epoch: 6| Step: 4
Training loss: 1.860634684562683
Validation loss: 2.0599934458732605

Epoch: 6| Step: 5
Training loss: 1.9352545738220215
Validation loss: 2.053556203842163

Epoch: 6| Step: 6
Training loss: 2.4848952293395996
Validation loss: 2.0591594179471335

Epoch: 6| Step: 7
Training loss: 2.3291280269622803
Validation loss: 2.0576565066973367

Epoch: 6| Step: 8
Training loss: 1.998068928718567
Validation loss: 2.0650335947672525

Epoch: 6| Step: 9
Training loss: 2.328420400619507
Validation loss: 2.0598683953285217

Epoch: 6| Step: 10
Training loss: 1.3962408304214478
Validation loss: 2.060689687728882

Epoch: 6| Step: 11
Training loss: 2.3931288719177246
Validation loss: 2.0638655026753745

Epoch: 6| Step: 12
Training loss: 2.0153207778930664
Validation loss: 2.0635407169659934

Epoch: 6| Step: 13
Training loss: 1.8328626155853271
Validation loss: 2.0543500979741416

Epoch: 147| Step: 0
Training loss: 1.8829851150512695
Validation loss: 2.055118143558502

Epoch: 6| Step: 1
Training loss: 1.6602002382278442
Validation loss: 2.0538894136746726

Epoch: 6| Step: 2
Training loss: 2.4413228034973145
Validation loss: 2.0522210597991943

Epoch: 6| Step: 3
Training loss: 2.455650568008423
Validation loss: 2.05745796362559

Epoch: 6| Step: 4
Training loss: 3.1016454696655273
Validation loss: 2.0502861936887107

Epoch: 6| Step: 5
Training loss: 2.588054656982422
Validation loss: 2.0425750414530435

Epoch: 6| Step: 6
Training loss: 1.6724069118499756
Validation loss: 2.0390905340512595

Epoch: 6| Step: 7
Training loss: 2.216022253036499
Validation loss: 2.0488439202308655

Epoch: 6| Step: 8
Training loss: 1.7495312690734863
Validation loss: 2.0507960319519043

Epoch: 6| Step: 9
Training loss: 1.6768484115600586
Validation loss: 2.048810382684072

Epoch: 6| Step: 10
Training loss: 1.9256339073181152
Validation loss: 2.0424489180246987

Epoch: 6| Step: 11
Training loss: 2.316981792449951
Validation loss: 2.044935663541158

Epoch: 6| Step: 12
Training loss: 1.692051887512207
Validation loss: 2.0526915987332663

Epoch: 6| Step: 13
Training loss: 1.6396106481552124
Validation loss: 2.0470871726671853

Epoch: 148| Step: 0
Training loss: 2.813143491744995
Validation loss: 2.0534764329592385

Epoch: 6| Step: 1
Training loss: 1.818748950958252
Validation loss: 2.0474528670310974

Epoch: 6| Step: 2
Training loss: 2.4050345420837402
Validation loss: 2.040884514649709

Epoch: 6| Step: 3
Training loss: 1.680368423461914
Validation loss: 2.046115219593048

Epoch: 6| Step: 4
Training loss: 2.0137410163879395
Validation loss: 2.0423566897710166

Epoch: 6| Step: 5
Training loss: 2.533154010772705
Validation loss: 2.0421874721844993

Epoch: 6| Step: 6
Training loss: 2.032539129257202
Validation loss: 2.05389271179835

Epoch: 6| Step: 7
Training loss: 1.853851556777954
Validation loss: 2.0468022425969443

Epoch: 6| Step: 8
Training loss: 1.6880369186401367
Validation loss: 2.0529706676801047

Epoch: 6| Step: 9
Training loss: 1.509965181350708
Validation loss: 2.057380497455597

Epoch: 6| Step: 10
Training loss: 1.8892390727996826
Validation loss: 2.0669939120610556

Epoch: 6| Step: 11
Training loss: 1.9617834091186523
Validation loss: 2.0599796374638877

Epoch: 6| Step: 12
Training loss: 2.3985595703125
Validation loss: 2.0626949866612754

Epoch: 6| Step: 13
Training loss: 2.1188483238220215
Validation loss: 2.0558127562204995

Epoch: 149| Step: 0
Training loss: 2.4433393478393555
Validation loss: 2.053739766279856

Epoch: 6| Step: 1
Training loss: 2.341228485107422
Validation loss: 2.062054435412089

Epoch: 6| Step: 2
Training loss: 1.7931782007217407
Validation loss: 2.0522987047831216

Epoch: 6| Step: 3
Training loss: 1.549975872039795
Validation loss: 2.05318146944046

Epoch: 6| Step: 4
Training loss: 2.002948522567749
Validation loss: 2.048074225584666

Epoch: 6| Step: 5
Training loss: 2.2590560913085938
Validation loss: 2.072304606437683

Epoch: 6| Step: 6
Training loss: 2.1112923622131348
Validation loss: 2.071287433306376

Epoch: 6| Step: 7
Training loss: 1.9580508470535278
Validation loss: 2.072788874308268

Epoch: 6| Step: 8
Training loss: 2.349550247192383
Validation loss: 2.0681878527005515

Epoch: 6| Step: 9
Training loss: 2.0915822982788086
Validation loss: 2.068396290143331

Epoch: 6| Step: 10
Training loss: 1.7089976072311401
Validation loss: 2.0670566956202188

Epoch: 6| Step: 11
Training loss: 1.8323090076446533
Validation loss: 2.059058666229248

Epoch: 6| Step: 12
Training loss: 2.759521245956421
Validation loss: 2.064417084058126

Epoch: 6| Step: 13
Training loss: 1.7505922317504883
Validation loss: 2.0717495481173196

Epoch: 150| Step: 0
Training loss: 1.802401065826416
Validation loss: 2.0657609701156616

Epoch: 6| Step: 1
Training loss: 2.3786702156066895
Validation loss: 2.0645503203074136

Epoch: 6| Step: 2
Training loss: 1.8877785205841064
Validation loss: 2.066926201184591

Epoch: 6| Step: 3
Training loss: 1.9202698469161987
Validation loss: 2.0609347422917685

Epoch: 6| Step: 4
Training loss: 2.2211999893188477
Validation loss: 2.054591198762258

Epoch: 6| Step: 5
Training loss: 2.0944225788116455
Validation loss: 2.062466581662496

Epoch: 6| Step: 6
Training loss: 2.2096009254455566
Validation loss: 2.061376214027405

Epoch: 6| Step: 7
Training loss: 2.2803053855895996
Validation loss: 2.0645254453023276

Epoch: 6| Step: 8
Training loss: 1.7726194858551025
Validation loss: 2.0654294888178506

Epoch: 6| Step: 9
Training loss: 1.5850448608398438
Validation loss: 2.067952275276184

Epoch: 6| Step: 10
Training loss: 2.168104648590088
Validation loss: 2.0571747422218323

Epoch: 6| Step: 11
Training loss: 1.9826405048370361
Validation loss: 2.0606213212013245

Epoch: 6| Step: 12
Training loss: 2.0419538021087646
Validation loss: 2.06335457166036

Epoch: 6| Step: 13
Training loss: 2.458198070526123
Validation loss: 2.0756265123685202

Epoch: 151| Step: 0
Training loss: 2.3686647415161133
Validation loss: 2.062564790248871

Epoch: 6| Step: 1
Training loss: 2.194385528564453
Validation loss: 2.0651556253433228

Epoch: 6| Step: 2
Training loss: 1.9547924995422363
Validation loss: 2.067992905775706

Epoch: 6| Step: 3
Training loss: 1.651795744895935
Validation loss: 2.0647324323654175

Epoch: 6| Step: 4
Training loss: 2.050830841064453
Validation loss: 2.0740641554196677

Epoch: 6| Step: 5
Training loss: 1.99576735496521
Validation loss: 2.072792132695516

Epoch: 6| Step: 6
Training loss: 1.8952157497406006
Validation loss: 2.0587074756622314

Epoch: 6| Step: 7
Training loss: 2.2828733921051025
Validation loss: 2.0666380524635315

Epoch: 6| Step: 8
Training loss: 1.9741272926330566
Validation loss: 2.053056538105011

Epoch: 6| Step: 9
Training loss: 2.0725135803222656
Validation loss: 2.0490392446517944

Epoch: 6| Step: 10
Training loss: 2.246722459793091
Validation loss: 2.0570956468582153

Epoch: 6| Step: 11
Training loss: 1.9781770706176758
Validation loss: 2.047640065352122

Epoch: 6| Step: 12
Training loss: 2.4247825145721436
Validation loss: 2.0530129075050354

Epoch: 6| Step: 13
Training loss: 1.930715799331665
Validation loss: 2.0570462544759116

Epoch: 152| Step: 0
Training loss: 1.8683233261108398
Validation loss: 2.0436802903811135

Epoch: 6| Step: 1
Training loss: 1.9752320051193237
Validation loss: 2.0560012459754944

Epoch: 6| Step: 2
Training loss: 2.023564338684082
Validation loss: 2.0637144446372986

Epoch: 6| Step: 3
Training loss: 2.680711507797241
Validation loss: 2.0616634289423623

Epoch: 6| Step: 4
Training loss: 2.298463821411133
Validation loss: 2.0635420282681785

Epoch: 6| Step: 5
Training loss: 1.864027738571167
Validation loss: 2.05815581480662

Epoch: 6| Step: 6
Training loss: 1.534060001373291
Validation loss: 2.062968611717224

Epoch: 6| Step: 7
Training loss: 1.3955984115600586
Validation loss: 2.0830702582995095

Epoch: 6| Step: 8
Training loss: 2.5437583923339844
Validation loss: 2.0771085818608603

Epoch: 6| Step: 9
Training loss: 2.341299533843994
Validation loss: 2.0712905526161194

Epoch: 6| Step: 10
Training loss: 1.6622288227081299
Validation loss: 2.071524977684021

Epoch: 6| Step: 11
Training loss: 1.9635589122772217
Validation loss: 2.0802762707074485

Epoch: 6| Step: 12
Training loss: 2.692103624343872
Validation loss: 2.0560145179430642

Epoch: 6| Step: 13
Training loss: 2.294605255126953
Validation loss: 2.064018666744232

Epoch: 153| Step: 0
Training loss: 1.5842599868774414
Validation loss: 2.056057413419088

Epoch: 6| Step: 1
Training loss: 2.415086269378662
Validation loss: 2.0490416487058005

Epoch: 6| Step: 2
Training loss: 2.2676899433135986
Validation loss: 2.0586425264676413

Epoch: 6| Step: 3
Training loss: 2.1129231452941895
Validation loss: 2.04537824789683

Epoch: 6| Step: 4
Training loss: 2.374018907546997
Validation loss: 2.0466841061909995

Epoch: 6| Step: 5
Training loss: 2.0928964614868164
Validation loss: 2.0519301891326904

Epoch: 6| Step: 6
Training loss: 1.7315609455108643
Validation loss: 2.043397227923075

Epoch: 6| Step: 7
Training loss: 2.042651414871216
Validation loss: 2.0443389415740967

Epoch: 6| Step: 8
Training loss: 1.975061058998108
Validation loss: 2.0558738907178244

Epoch: 6| Step: 9
Training loss: 2.203855514526367
Validation loss: 2.0493802428245544

Epoch: 6| Step: 10
Training loss: 2.2861874103546143
Validation loss: 2.055652439594269

Epoch: 6| Step: 11
Training loss: 1.9447355270385742
Validation loss: 2.0546183983484902

Epoch: 6| Step: 12
Training loss: 2.028017997741699
Validation loss: 2.053508718808492

Epoch: 6| Step: 13
Training loss: 2.0206542015075684
Validation loss: 2.048871914545695

Epoch: 154| Step: 0
Training loss: 1.6611039638519287
Validation loss: 2.0570305983225503

Epoch: 6| Step: 1
Training loss: 2.7496447563171387
Validation loss: 2.065747916698456

Epoch: 6| Step: 2
Training loss: 1.9658141136169434
Validation loss: 2.060549279054006

Epoch: 6| Step: 3
Training loss: 2.432845115661621
Validation loss: 2.067946473757426

Epoch: 6| Step: 4
Training loss: 2.0654826164245605
Validation loss: 2.0541476806004844

Epoch: 6| Step: 5
Training loss: 2.307668685913086
Validation loss: 2.0667854150136313

Epoch: 6| Step: 6
Training loss: 2.6078004837036133
Validation loss: 2.072908361752828

Epoch: 6| Step: 7
Training loss: 1.941821575164795
Validation loss: 2.0782126784324646

Epoch: 6| Step: 8
Training loss: 1.8666777610778809
Validation loss: 2.0735964179039

Epoch: 6| Step: 9
Training loss: 2.655442237854004
Validation loss: 2.0557721853256226

Epoch: 6| Step: 10
Training loss: 1.2452197074890137
Validation loss: 2.052623152732849

Epoch: 6| Step: 11
Training loss: 1.8073806762695312
Validation loss: 2.0520899891853333

Epoch: 6| Step: 12
Training loss: 2.3722128868103027
Validation loss: 2.0505091547966003

Epoch: 6| Step: 13
Training loss: 1.5904395580291748
Validation loss: 2.054209033648173

Epoch: 155| Step: 0
Training loss: 2.618762969970703
Validation loss: 2.0515437722206116

Epoch: 6| Step: 1
Training loss: 3.0583431720733643
Validation loss: 2.0586492816607156

Epoch: 6| Step: 2
Training loss: 1.766600251197815
Validation loss: 2.062742908795675

Epoch: 6| Step: 3
Training loss: 1.4596900939941406
Validation loss: 2.0531837542851767

Epoch: 6| Step: 4
Training loss: 1.7948578596115112
Validation loss: 2.0535150369008384

Epoch: 6| Step: 5
Training loss: 1.5841981172561646
Validation loss: 2.044071833292643

Epoch: 6| Step: 6
Training loss: 2.323211669921875
Validation loss: 2.063197056452433

Epoch: 6| Step: 7
Training loss: 2.27756929397583
Validation loss: 2.0629319151242576

Epoch: 6| Step: 8
Training loss: 1.9092546701431274
Validation loss: 2.0639341870943704

Epoch: 6| Step: 9
Training loss: 1.8399995565414429
Validation loss: 2.0563724040985107

Epoch: 6| Step: 10
Training loss: 2.2133121490478516
Validation loss: 2.0598671237627664

Epoch: 6| Step: 11
Training loss: 1.647665023803711
Validation loss: 2.05881275733312

Epoch: 6| Step: 12
Training loss: 2.2267959117889404
Validation loss: 2.05089271068573

Epoch: 6| Step: 13
Training loss: 2.012084722518921
Validation loss: 2.0594088435173035

Epoch: 156| Step: 0
Training loss: 2.2372379302978516
Validation loss: 2.0562140941619873

Epoch: 6| Step: 1
Training loss: 1.6464428901672363
Validation loss: 2.054248253504435

Epoch: 6| Step: 2
Training loss: 2.0481138229370117
Validation loss: 2.0621069272359214

Epoch: 6| Step: 3
Training loss: 1.626794457435608
Validation loss: 2.062399427096049

Epoch: 6| Step: 4
Training loss: 2.375974655151367
Validation loss: 2.0497681697209678

Epoch: 6| Step: 5
Training loss: 2.0162758827209473
Validation loss: 2.0658165216445923

Epoch: 6| Step: 6
Training loss: 2.230830192565918
Validation loss: 2.0627216895421348

Epoch: 6| Step: 7
Training loss: 2.094754695892334
Validation loss: 2.0623021324475608

Epoch: 6| Step: 8
Training loss: 1.7402019500732422
Validation loss: 2.0735451181729636

Epoch: 6| Step: 9
Training loss: 2.417072296142578
Validation loss: 2.064470330874125

Epoch: 6| Step: 10
Training loss: 2.439455509185791
Validation loss: 2.0653642813364663

Epoch: 6| Step: 11
Training loss: 2.170325756072998
Validation loss: 2.059022684892019

Epoch: 6| Step: 12
Training loss: 2.0573952198028564
Validation loss: 2.0530128876368203

Epoch: 6| Step: 13
Training loss: 1.7528972625732422
Validation loss: 2.060130556424459

Epoch: 157| Step: 0
Training loss: 1.6246600151062012
Validation loss: 2.061194578806559

Epoch: 6| Step: 1
Training loss: 2.321237325668335
Validation loss: 2.0499356985092163

Epoch: 6| Step: 2
Training loss: 2.099776029586792
Validation loss: 2.053961237271627

Epoch: 6| Step: 3
Training loss: 2.9250078201293945
Validation loss: 2.0529073079427085

Epoch: 6| Step: 4
Training loss: 2.493229866027832
Validation loss: 2.0519094467163086

Epoch: 6| Step: 5
Training loss: 2.2786312103271484
Validation loss: 2.0473729372024536

Epoch: 6| Step: 6
Training loss: 1.9866946935653687
Validation loss: 2.0477431813875833

Epoch: 6| Step: 7
Training loss: 1.8291677236557007
Validation loss: 2.049737513065338

Epoch: 6| Step: 8
Training loss: 1.651336431503296
Validation loss: 2.050930976867676

Epoch: 6| Step: 9
Training loss: 1.8639447689056396
Validation loss: 2.0597224036852517

Epoch: 6| Step: 10
Training loss: 1.268812656402588
Validation loss: 2.0577076276143393

Epoch: 6| Step: 11
Training loss: 2.1130857467651367
Validation loss: 2.0561305483182273

Epoch: 6| Step: 12
Training loss: 2.59330677986145
Validation loss: 2.0735827883084617

Epoch: 6| Step: 13
Training loss: 2.4956705570220947
Validation loss: 2.0534575382868447

Epoch: 158| Step: 0
Training loss: 2.2341160774230957
Validation loss: 2.055377244949341

Epoch: 6| Step: 1
Training loss: 2.0790958404541016
Validation loss: 2.0705047051111856

Epoch: 6| Step: 2
Training loss: 2.282719612121582
Validation loss: 2.0649851163228354

Epoch: 6| Step: 3
Training loss: 2.2313456535339355
Validation loss: 2.075559457143148

Epoch: 6| Step: 4
Training loss: 2.0957541465759277
Validation loss: 2.066507418950399

Epoch: 6| Step: 5
Training loss: 2.364931583404541
Validation loss: 2.069773534933726

Epoch: 6| Step: 6
Training loss: 2.035407543182373
Validation loss: 2.058601677417755

Epoch: 6| Step: 7
Training loss: 1.8859256505966187
Validation loss: 2.063729385534922

Epoch: 6| Step: 8
Training loss: 2.1466312408447266
Validation loss: 2.0569991072018943

Epoch: 6| Step: 9
Training loss: 1.826462984085083
Validation loss: 2.0592044591903687

Epoch: 6| Step: 10
Training loss: 2.0087389945983887
Validation loss: 2.050517479578654

Epoch: 6| Step: 11
Training loss: 2.0666308403015137
Validation loss: 2.0588181813557944

Epoch: 6| Step: 12
Training loss: 2.1515703201293945
Validation loss: 2.043905258178711

Epoch: 6| Step: 13
Training loss: 1.8272597789764404
Validation loss: 2.0419699350992837

Epoch: 159| Step: 0
Training loss: 2.186065673828125
Validation loss: 2.0476483702659607

Epoch: 6| Step: 1
Training loss: 2.6299920082092285
Validation loss: 2.041888972123464

Epoch: 6| Step: 2
Training loss: 2.0289127826690674
Validation loss: 2.044199446837107

Epoch: 6| Step: 3
Training loss: 2.62009334564209
Validation loss: 2.047725478808085

Epoch: 6| Step: 4
Training loss: 1.9282649755477905
Validation loss: 2.046706040700277

Epoch: 6| Step: 5
Training loss: 1.7616772651672363
Validation loss: 2.0409082969029746

Epoch: 6| Step: 6
Training loss: 2.1922736167907715
Validation loss: 2.055026431878408

Epoch: 6| Step: 7
Training loss: 1.9737792015075684
Validation loss: 2.0447157422701516

Epoch: 6| Step: 8
Training loss: 1.6985607147216797
Validation loss: 2.0566575129826865

Epoch: 6| Step: 9
Training loss: 2.338937997817993
Validation loss: 2.0519272089004517

Epoch: 6| Step: 10
Training loss: 1.6190706491470337
Validation loss: 2.0550203124682107

Epoch: 6| Step: 11
Training loss: 2.2786808013916016
Validation loss: 2.060633579889933

Epoch: 6| Step: 12
Training loss: 2.056617259979248
Validation loss: 2.0475222865740457

Epoch: 6| Step: 13
Training loss: 1.54680597782135
Validation loss: 2.0566577514012656

Epoch: 160| Step: 0
Training loss: 1.9094245433807373
Validation loss: 2.0500719944636026

Epoch: 6| Step: 1
Training loss: 1.9311233758926392
Validation loss: 2.0594959259033203

Epoch: 6| Step: 2
Training loss: 1.9629943370819092
Validation loss: 2.0664948225021362

Epoch: 6| Step: 3
Training loss: 2.3303475379943848
Validation loss: 2.0555061300595603

Epoch: 6| Step: 4
Training loss: 1.4706088304519653
Validation loss: 2.073259870211283

Epoch: 6| Step: 5
Training loss: 2.3963284492492676
Validation loss: 2.069730977217356

Epoch: 6| Step: 6
Training loss: 2.233225107192993
Validation loss: 2.071689168612162

Epoch: 6| Step: 7
Training loss: 1.7444634437561035
Validation loss: 2.066369672616323

Epoch: 6| Step: 8
Training loss: 2.656466245651245
Validation loss: 2.0640074412027993

Epoch: 6| Step: 9
Training loss: 2.4941415786743164
Validation loss: 2.061570107936859

Epoch: 6| Step: 10
Training loss: 1.679306983947754
Validation loss: 2.0587177077929177

Epoch: 6| Step: 11
Training loss: 1.9368810653686523
Validation loss: 2.0663885474205017

Epoch: 6| Step: 12
Training loss: 2.2609496116638184
Validation loss: 2.072904407978058

Epoch: 6| Step: 13
Training loss: 1.8708527088165283
Validation loss: 2.0670432249704995

Epoch: 161| Step: 0
Training loss: 2.14452862739563
Validation loss: 2.0694111386934915

Epoch: 6| Step: 1
Training loss: 2.2984519004821777
Validation loss: 2.0681072076161704

Epoch: 6| Step: 2
Training loss: 1.6671737432479858
Validation loss: 2.070816973845164

Epoch: 6| Step: 3
Training loss: 2.068678379058838
Validation loss: 2.072951098283132

Epoch: 6| Step: 4
Training loss: 1.9536246061325073
Validation loss: 2.0735121170679727

Epoch: 6| Step: 5
Training loss: 2.091911792755127
Validation loss: 2.07187290986379

Epoch: 6| Step: 6
Training loss: 1.8680890798568726
Validation loss: 2.060796936353048

Epoch: 6| Step: 7
Training loss: 2.0244827270507812
Validation loss: 2.0736277103424072

Epoch: 6| Step: 8
Training loss: 2.342355251312256
Validation loss: 2.0644128719965615

Epoch: 6| Step: 9
Training loss: 2.047403573989868
Validation loss: 2.078029692173004

Epoch: 6| Step: 10
Training loss: 2.00590443611145
Validation loss: 2.074267864227295

Epoch: 6| Step: 11
Training loss: 2.098986864089966
Validation loss: 2.0593976179758706

Epoch: 6| Step: 12
Training loss: 2.085967540740967
Validation loss: 2.062066455682119

Epoch: 6| Step: 13
Training loss: 2.09029221534729
Validation loss: 2.0582188963890076

Epoch: 162| Step: 0
Training loss: 2.271307945251465
Validation loss: 2.0612321496009827

Epoch: 6| Step: 1
Training loss: 1.4371263980865479
Validation loss: 2.061331788698832

Epoch: 6| Step: 2
Training loss: 2.438426971435547
Validation loss: 2.0563809672991433

Epoch: 6| Step: 3
Training loss: 2.2259671688079834
Validation loss: 2.062463919321696

Epoch: 6| Step: 4
Training loss: 2.238949775695801
Validation loss: 2.0504618287086487

Epoch: 6| Step: 5
Training loss: 1.8213422298431396
Validation loss: 2.0604703227678933

Epoch: 6| Step: 6
Training loss: 2.1304924488067627
Validation loss: 2.068806529045105

Epoch: 6| Step: 7
Training loss: 1.8125109672546387
Validation loss: 2.0642626682917276

Epoch: 6| Step: 8
Training loss: 2.1604669094085693
Validation loss: 2.067429463068644

Epoch: 6| Step: 9
Training loss: 2.1421613693237305
Validation loss: 2.0686921079953513

Epoch: 6| Step: 10
Training loss: 1.7338379621505737
Validation loss: 2.072674016157786

Epoch: 6| Step: 11
Training loss: 2.094078540802002
Validation loss: 2.075562516848246

Epoch: 6| Step: 12
Training loss: 1.8136606216430664
Validation loss: 2.072598318258921

Epoch: 6| Step: 13
Training loss: 2.307671546936035
Validation loss: 2.0641836921374

Epoch: 163| Step: 0
Training loss: 2.212521553039551
Validation loss: 2.0677720506985984

Epoch: 6| Step: 1
Training loss: 2.0878987312316895
Validation loss: 2.06462691227595

Epoch: 6| Step: 2
Training loss: 1.9244993925094604
Validation loss: 2.0600069165229797

Epoch: 6| Step: 3
Training loss: 1.8392213582992554
Validation loss: 2.06500514348348

Epoch: 6| Step: 4
Training loss: 1.9585480690002441
Validation loss: 2.072728236516317

Epoch: 6| Step: 5
Training loss: 2.065105438232422
Validation loss: 2.0624120434125266

Epoch: 6| Step: 6
Training loss: 2.0070393085479736
Validation loss: 2.0641613006591797

Epoch: 6| Step: 7
Training loss: 1.8311452865600586
Validation loss: 2.061165193716685

Epoch: 6| Step: 8
Training loss: 2.2567636966705322
Validation loss: 2.0653496583302817

Epoch: 6| Step: 9
Training loss: 2.1355652809143066
Validation loss: 2.067926903565725

Epoch: 6| Step: 10
Training loss: 1.682002067565918
Validation loss: 2.0713603695233664

Epoch: 6| Step: 11
Training loss: 2.0063977241516113
Validation loss: 2.0724366505940757

Epoch: 6| Step: 12
Training loss: 2.042564630508423
Validation loss: 2.0681989391644797

Epoch: 6| Step: 13
Training loss: 2.443471908569336
Validation loss: 2.0613527496655784

Epoch: 164| Step: 0
Training loss: 2.3212881088256836
Validation loss: 2.075124422709147

Epoch: 6| Step: 1
Training loss: 1.885878324508667
Validation loss: 2.0654146671295166

Epoch: 6| Step: 2
Training loss: 2.0502891540527344
Validation loss: 2.0657305916150412

Epoch: 6| Step: 3
Training loss: 1.720855951309204
Validation loss: 2.073929727077484

Epoch: 6| Step: 4
Training loss: 2.5395450592041016
Validation loss: 2.06513249874115

Epoch: 6| Step: 5
Training loss: 2.285893678665161
Validation loss: 2.06539652744929

Epoch: 6| Step: 6
Training loss: 1.5218656063079834
Validation loss: 2.071228265762329

Epoch: 6| Step: 7
Training loss: 2.0047857761383057
Validation loss: 2.076330999533335

Epoch: 6| Step: 8
Training loss: 2.0794026851654053
Validation loss: 2.080287973086039

Epoch: 6| Step: 9
Training loss: 2.149986743927002
Validation loss: 2.0746079484621682

Epoch: 6| Step: 10
Training loss: 2.3409366607666016
Validation loss: 2.072303613026937

Epoch: 6| Step: 11
Training loss: 2.079030752182007
Validation loss: 2.070731242497762

Epoch: 6| Step: 12
Training loss: 1.8382257223129272
Validation loss: 2.0612330436706543

Epoch: 6| Step: 13
Training loss: 1.702793002128601
Validation loss: 2.0577402909596763

Epoch: 165| Step: 0
Training loss: 1.7535269260406494
Validation loss: 2.0776331226030984

Epoch: 6| Step: 1
Training loss: 2.237274169921875
Validation loss: 2.07138321797053

Epoch: 6| Step: 2
Training loss: 2.478187084197998
Validation loss: 2.066901922225952

Epoch: 6| Step: 3
Training loss: 1.6953151226043701
Validation loss: 2.0719971458117166

Epoch: 6| Step: 4
Training loss: 2.6681575775146484
Validation loss: 2.0699325601259866

Epoch: 6| Step: 5
Training loss: 2.1168360710144043
Validation loss: 2.0635955333709717

Epoch: 6| Step: 6
Training loss: 2.440199375152588
Validation loss: 2.0670678416887918

Epoch: 6| Step: 7
Training loss: 1.9950618743896484
Validation loss: 2.071011404196421

Epoch: 6| Step: 8
Training loss: 1.5118848085403442
Validation loss: 2.0667264461517334

Epoch: 6| Step: 9
Training loss: 1.6580730676651
Validation loss: 2.0734692811965942

Epoch: 6| Step: 10
Training loss: 1.9778305292129517
Validation loss: 2.067939341068268

Epoch: 6| Step: 11
Training loss: 2.231503486633301
Validation loss: 2.0683769583702087

Epoch: 6| Step: 12
Training loss: 1.8819831609725952
Validation loss: 2.0655750830968223

Epoch: 6| Step: 13
Training loss: 2.141829013824463
Validation loss: 2.057744324207306

Epoch: 166| Step: 0
Training loss: 2.055556297302246
Validation loss: 2.063147564729055

Epoch: 6| Step: 1
Training loss: 2.4743218421936035
Validation loss: 2.0514962673187256

Epoch: 6| Step: 2
Training loss: 2.277648687362671
Validation loss: 2.0547595421473184

Epoch: 6| Step: 3
Training loss: 1.5823291540145874
Validation loss: 2.0609283248583474

Epoch: 6| Step: 4
Training loss: 1.820647120475769
Validation loss: 2.062164525190989

Epoch: 6| Step: 5
Training loss: 2.1494882106781006
Validation loss: 2.0619766314824424

Epoch: 6| Step: 6
Training loss: 2.4502921104431152
Validation loss: 2.0668777028719583

Epoch: 6| Step: 7
Training loss: 1.8910133838653564
Validation loss: 2.0682181119918823

Epoch: 6| Step: 8
Training loss: 2.4150264263153076
Validation loss: 2.0628280639648438

Epoch: 6| Step: 9
Training loss: 2.257502555847168
Validation loss: 2.0750436981519065

Epoch: 6| Step: 10
Training loss: 1.566988229751587
Validation loss: 2.0820595224698386

Epoch: 6| Step: 11
Training loss: 1.9914684295654297
Validation loss: 2.0755730668703714

Epoch: 6| Step: 12
Training loss: 1.9202386140823364
Validation loss: 2.0795143047968545

Epoch: 6| Step: 13
Training loss: 1.5120131969451904
Validation loss: 2.0746308167775473

Epoch: 167| Step: 0
Training loss: 1.935173749923706
Validation loss: 2.0784344474474588

Epoch: 6| Step: 1
Training loss: 1.9532818794250488
Validation loss: 2.0693979263305664

Epoch: 6| Step: 2
Training loss: 2.124434471130371
Validation loss: 2.073793033758799

Epoch: 6| Step: 3
Training loss: 1.985999584197998
Validation loss: 2.0760068694750466

Epoch: 6| Step: 4
Training loss: 2.7286415100097656
Validation loss: 2.076354126135508

Epoch: 6| Step: 5
Training loss: 1.9174892902374268
Validation loss: 2.078321913878123

Epoch: 6| Step: 6
Training loss: 1.8981391191482544
Validation loss: 2.085987667242686

Epoch: 6| Step: 7
Training loss: 2.431009292602539
Validation loss: 2.0883440574010215

Epoch: 6| Step: 8
Training loss: 1.9234025478363037
Validation loss: 2.090387145678202

Epoch: 6| Step: 9
Training loss: 1.9556416273117065
Validation loss: 2.0973963141441345

Epoch: 6| Step: 10
Training loss: 1.885880708694458
Validation loss: 2.091698408126831

Epoch: 6| Step: 11
Training loss: 1.984991431236267
Validation loss: 2.07915206750234

Epoch: 6| Step: 12
Training loss: 2.3570053577423096
Validation loss: 2.078853666782379

Epoch: 6| Step: 13
Training loss: 1.4533520936965942
Validation loss: 2.077155351638794

Epoch: 168| Step: 0
Training loss: 1.5277217626571655
Validation loss: 2.0744874278704324

Epoch: 6| Step: 1
Training loss: 1.5777264833450317
Validation loss: 2.0729548136393228

Epoch: 6| Step: 2
Training loss: 1.8121755123138428
Validation loss: 2.078465382258097

Epoch: 6| Step: 3
Training loss: 2.0420360565185547
Validation loss: 2.079618811607361

Epoch: 6| Step: 4
Training loss: 1.3380050659179688
Validation loss: 2.0734474261601767

Epoch: 6| Step: 5
Training loss: 1.398073434829712
Validation loss: 2.084274093310038

Epoch: 6| Step: 6
Training loss: 2.7235569953918457
Validation loss: 2.078981598218282

Epoch: 6| Step: 7
Training loss: 2.1631202697753906
Validation loss: 2.0774269302686057

Epoch: 6| Step: 8
Training loss: 2.1571552753448486
Validation loss: 2.069589376449585

Epoch: 6| Step: 9
Training loss: 2.4199092388153076
Validation loss: 2.0682852466901145

Epoch: 6| Step: 10
Training loss: 2.410555362701416
Validation loss: 2.0690447290738425

Epoch: 6| Step: 11
Training loss: 2.5226430892944336
Validation loss: 2.062808314959208

Epoch: 6| Step: 12
Training loss: 2.642284393310547
Validation loss: 2.0647065242131553

Epoch: 6| Step: 13
Training loss: 1.6618825197219849
Validation loss: 2.076794385910034

Epoch: 169| Step: 0
Training loss: 1.8934581279754639
Validation loss: 2.0780067841211953

Epoch: 6| Step: 1
Training loss: 1.9710830450057983
Validation loss: 2.0858752727508545

Epoch: 6| Step: 2
Training loss: 2.322542905807495
Validation loss: 2.0792498191197715

Epoch: 6| Step: 3
Training loss: 2.2032980918884277
Validation loss: 2.073143800099691

Epoch: 6| Step: 4
Training loss: 1.7012693881988525
Validation loss: 2.0764748255411782

Epoch: 6| Step: 5
Training loss: 1.9734801054000854
Validation loss: 2.0631269216537476

Epoch: 6| Step: 6
Training loss: 1.744130253791809
Validation loss: 2.069532871246338

Epoch: 6| Step: 7
Training loss: 1.6393232345581055
Validation loss: 2.0535791317621865

Epoch: 6| Step: 8
Training loss: 2.3477659225463867
Validation loss: 2.069236159324646

Epoch: 6| Step: 9
Training loss: 1.8209068775177002
Validation loss: 2.0549644430478415

Epoch: 6| Step: 10
Training loss: 1.89103364944458
Validation loss: 2.063642760117849

Epoch: 6| Step: 11
Training loss: 1.8285000324249268
Validation loss: 2.079990267753601

Epoch: 6| Step: 12
Training loss: 2.3829970359802246
Validation loss: 2.07527494430542

Epoch: 6| Step: 13
Training loss: 2.798151969909668
Validation loss: 2.0737616419792175

Epoch: 170| Step: 0
Training loss: 1.9199795722961426
Validation loss: 2.0773449738820395

Epoch: 6| Step: 1
Training loss: 1.7583963871002197
Validation loss: 2.0842239459355674

Epoch: 6| Step: 2
Training loss: 1.8776328563690186
Validation loss: 2.07376758257548

Epoch: 6| Step: 3
Training loss: 2.371195077896118
Validation loss: 2.064553995927175

Epoch: 6| Step: 4
Training loss: 1.9682437181472778
Validation loss: 2.0719125469525657

Epoch: 6| Step: 5
Training loss: 2.335946798324585
Validation loss: 2.070682406425476

Epoch: 6| Step: 6
Training loss: 1.9522671699523926
Validation loss: 2.0623532931009927

Epoch: 6| Step: 7
Training loss: 2.2313780784606934
Validation loss: 2.0688206950823465

Epoch: 6| Step: 8
Training loss: 1.750205397605896
Validation loss: 2.077608048915863

Epoch: 6| Step: 9
Training loss: 2.0727343559265137
Validation loss: 2.074646612008413

Epoch: 6| Step: 10
Training loss: 1.8190698623657227
Validation loss: 2.0600614746411643

Epoch: 6| Step: 11
Training loss: 2.1730358600616455
Validation loss: 2.0629109144210815

Epoch: 6| Step: 12
Training loss: 2.6445555686950684
Validation loss: 2.0750834941864014

Epoch: 6| Step: 13
Training loss: 1.3943030834197998
Validation loss: 2.0684173901875815

Epoch: 171| Step: 0
Training loss: 1.9147794246673584
Validation loss: 2.074017067750295

Epoch: 6| Step: 1
Training loss: 1.6749708652496338
Validation loss: 2.07193390528361

Epoch: 6| Step: 2
Training loss: 2.1514487266540527
Validation loss: 2.069952388604482

Epoch: 6| Step: 3
Training loss: 2.833245277404785
Validation loss: 2.085410793622335

Epoch: 6| Step: 4
Training loss: 2.2520241737365723
Validation loss: 2.076347907384237

Epoch: 6| Step: 5
Training loss: 1.5924899578094482
Validation loss: 2.091261347134908

Epoch: 6| Step: 6
Training loss: 2.202587604522705
Validation loss: 2.087432066599528

Epoch: 6| Step: 7
Training loss: 2.229013681411743
Validation loss: 2.0709726214408875

Epoch: 6| Step: 8
Training loss: 1.951970100402832
Validation loss: 2.079528292020162

Epoch: 6| Step: 9
Training loss: 2.285778045654297
Validation loss: 2.079721828301748

Epoch: 6| Step: 10
Training loss: 1.5062530040740967
Validation loss: 2.0823694864908853

Epoch: 6| Step: 11
Training loss: 1.1695845127105713
Validation loss: 2.0790050625801086

Epoch: 6| Step: 12
Training loss: 2.593015193939209
Validation loss: 2.083632449309031

Epoch: 6| Step: 13
Training loss: 2.136977195739746
Validation loss: 2.0834712187449136

Epoch: 172| Step: 0
Training loss: 2.5793561935424805
Validation loss: 2.075727621714274

Epoch: 6| Step: 1
Training loss: 1.7495486736297607
Validation loss: 2.072596251964569

Epoch: 6| Step: 2
Training loss: 1.6266586780548096
Validation loss: 2.085426171620687

Epoch: 6| Step: 3
Training loss: 2.4346699714660645
Validation loss: 2.0805300871531167

Epoch: 6| Step: 4
Training loss: 1.7580139636993408
Validation loss: 2.075399955113729

Epoch: 6| Step: 5
Training loss: 1.7492173910140991
Validation loss: 2.083919028441111

Epoch: 6| Step: 6
Training loss: 1.9820517301559448
Validation loss: 2.077515482902527

Epoch: 6| Step: 7
Training loss: 1.8540196418762207
Validation loss: 2.0720277627309165

Epoch: 6| Step: 8
Training loss: 1.7935428619384766
Validation loss: 2.088342229525248

Epoch: 6| Step: 9
Training loss: 2.5218119621276855
Validation loss: 2.0815443197886148

Epoch: 6| Step: 10
Training loss: 2.4032375812530518
Validation loss: 2.078424632549286

Epoch: 6| Step: 11
Training loss: 1.6402596235275269
Validation loss: 2.089366892973582

Epoch: 6| Step: 12
Training loss: 1.9325380325317383
Validation loss: 2.0886215766270957

Epoch: 6| Step: 13
Training loss: 2.187319755554199
Validation loss: 2.0930124521255493

Epoch: 173| Step: 0
Training loss: 1.7157154083251953
Validation loss: 2.102636933326721

Epoch: 6| Step: 1
Training loss: 2.1759583950042725
Validation loss: 2.082580029964447

Epoch: 6| Step: 2
Training loss: 2.1118850708007812
Validation loss: 2.081354320049286

Epoch: 6| Step: 3
Training loss: 2.3259341716766357
Validation loss: 2.0805468757947287

Epoch: 6| Step: 4
Training loss: 2.3399834632873535
Validation loss: 2.081514060497284

Epoch: 6| Step: 5
Training loss: 1.8447184562683105
Validation loss: 2.0729531049728394

Epoch: 6| Step: 6
Training loss: 1.8279688358306885
Validation loss: 2.0808263222376504

Epoch: 6| Step: 7
Training loss: 2.107149839401245
Validation loss: 2.07077294588089

Epoch: 6| Step: 8
Training loss: 1.679105520248413
Validation loss: 2.068423807621002

Epoch: 6| Step: 9
Training loss: 1.6870274543762207
Validation loss: 2.080662488937378

Epoch: 6| Step: 10
Training loss: 2.049259662628174
Validation loss: 2.0709712306658425

Epoch: 6| Step: 11
Training loss: 2.035611152648926
Validation loss: 2.082918186982473

Epoch: 6| Step: 12
Training loss: 2.561028003692627
Validation loss: 2.0805283784866333

Epoch: 6| Step: 13
Training loss: 2.007967948913574
Validation loss: 2.0823928912480674

Epoch: 174| Step: 0
Training loss: 2.1062655448913574
Validation loss: 2.083015580972036

Epoch: 6| Step: 1
Training loss: 2.517383098602295
Validation loss: 2.0891929467519126

Epoch: 6| Step: 2
Training loss: 2.7050061225891113
Validation loss: 2.089366098244985

Epoch: 6| Step: 3
Training loss: 1.6183974742889404
Validation loss: 2.079132616519928

Epoch: 6| Step: 4
Training loss: 2.384286403656006
Validation loss: 2.0890947182973227

Epoch: 6| Step: 5
Training loss: 2.302267074584961
Validation loss: 2.080021540323893

Epoch: 6| Step: 6
Training loss: 2.0353193283081055
Validation loss: 2.0730807781219482

Epoch: 6| Step: 7
Training loss: 1.5991365909576416
Validation loss: 2.067763030529022

Epoch: 6| Step: 8
Training loss: 1.9781124591827393
Validation loss: 2.068606952826182

Epoch: 6| Step: 9
Training loss: 2.761380910873413
Validation loss: 2.069328546524048

Epoch: 6| Step: 10
Training loss: 2.1856331825256348
Validation loss: 2.0585776964823403

Epoch: 6| Step: 11
Training loss: 1.715164303779602
Validation loss: 2.056233823299408

Epoch: 6| Step: 12
Training loss: 1.4847030639648438
Validation loss: 2.0563880999883017

Epoch: 6| Step: 13
Training loss: 1.6691160202026367
Validation loss: 2.052228351434072

Epoch: 175| Step: 0
Training loss: 2.3130903244018555
Validation loss: 2.0588391025861106

Epoch: 6| Step: 1
Training loss: 2.1198999881744385
Validation loss: 2.0567818880081177

Epoch: 6| Step: 2
Training loss: 1.7067749500274658
Validation loss: 2.0563698013623557

Epoch: 6| Step: 3
Training loss: 2.327575206756592
Validation loss: 2.0517959793408713

Epoch: 6| Step: 4
Training loss: 1.6975173950195312
Validation loss: 2.06140544017156

Epoch: 6| Step: 5
Training loss: 2.599255084991455
Validation loss: 2.060969352722168

Epoch: 6| Step: 6
Training loss: 1.638674020767212
Validation loss: 2.0637701749801636

Epoch: 6| Step: 7
Training loss: 2.066410779953003
Validation loss: 2.0652472575505576

Epoch: 6| Step: 8
Training loss: 2.4761972427368164
Validation loss: 2.068836828072866

Epoch: 6| Step: 9
Training loss: 1.9221686124801636
Validation loss: 2.063954552014669

Epoch: 6| Step: 10
Training loss: 2.2859880924224854
Validation loss: 2.0587146480878196

Epoch: 6| Step: 11
Training loss: 1.6001533269882202
Validation loss: 2.0454267660776773

Epoch: 6| Step: 12
Training loss: 2.0755343437194824
Validation loss: 2.0377578735351562

Epoch: 6| Step: 13
Training loss: 1.8522377014160156
Validation loss: 2.053094426790873

Epoch: 176| Step: 0
Training loss: 2.100679397583008
Validation loss: 2.037048578262329

Epoch: 6| Step: 1
Training loss: 2.3387839794158936
Validation loss: 2.0522505044937134

Epoch: 6| Step: 2
Training loss: 2.5372066497802734
Validation loss: 2.0570807258288064

Epoch: 6| Step: 3
Training loss: 2.126176118850708
Validation loss: 2.049541453520457

Epoch: 6| Step: 4
Training loss: 2.145486831665039
Validation loss: 2.0413365165392556

Epoch: 6| Step: 5
Training loss: 1.9276174306869507
Validation loss: 2.0451083381970725

Epoch: 6| Step: 6
Training loss: 2.4494879245758057
Validation loss: 2.05182554324468

Epoch: 6| Step: 7
Training loss: 1.5248006582260132
Validation loss: 2.0516875783602395

Epoch: 6| Step: 8
Training loss: 1.8983076810836792
Validation loss: 2.0530526439348855

Epoch: 6| Step: 9
Training loss: 2.1213085651397705
Validation loss: 2.0669630765914917

Epoch: 6| Step: 10
Training loss: 1.5523370504379272
Validation loss: 2.066271702448527

Epoch: 6| Step: 11
Training loss: 1.8978736400604248
Validation loss: 2.066032807032267

Epoch: 6| Step: 12
Training loss: 1.7033421993255615
Validation loss: 2.065877854824066

Epoch: 6| Step: 13
Training loss: 2.2789902687072754
Validation loss: 2.0760337313016257

Epoch: 177| Step: 0
Training loss: 1.9662142992019653
Validation loss: 2.0692496299743652

Epoch: 6| Step: 1
Training loss: 1.6658351421356201
Validation loss: 2.0688218474388123

Epoch: 6| Step: 2
Training loss: 2.3098301887512207
Validation loss: 2.063080052534739

Epoch: 6| Step: 3
Training loss: 2.365447998046875
Validation loss: 2.067797223726908

Epoch: 6| Step: 4
Training loss: 2.045884609222412
Validation loss: 2.0665722091992698

Epoch: 6| Step: 5
Training loss: 2.085242986679077
Validation loss: 2.0668636361757913

Epoch: 6| Step: 6
Training loss: 2.339757204055786
Validation loss: 2.0570940574010215

Epoch: 6| Step: 7
Training loss: 1.4356083869934082
Validation loss: 2.0514564911524453

Epoch: 6| Step: 8
Training loss: 2.0675642490386963
Validation loss: 2.059436837832133

Epoch: 6| Step: 9
Training loss: 1.9587247371673584
Validation loss: 2.0645357569058738

Epoch: 6| Step: 10
Training loss: 2.042426347732544
Validation loss: 2.0665761629740396

Epoch: 6| Step: 11
Training loss: 2.2304563522338867
Validation loss: 2.074066917101542

Epoch: 6| Step: 12
Training loss: 1.4807640314102173
Validation loss: 2.077196478843689

Epoch: 6| Step: 13
Training loss: 2.2572638988494873
Validation loss: 2.075199604034424

Epoch: 178| Step: 0
Training loss: 1.8884869813919067
Validation loss: 2.067535698413849

Epoch: 6| Step: 1
Training loss: 1.783197045326233
Validation loss: 2.0766045848528543

Epoch: 6| Step: 2
Training loss: 1.6693732738494873
Validation loss: 2.0732649167378745

Epoch: 6| Step: 3
Training loss: 2.138556957244873
Validation loss: 2.07496170202891

Epoch: 6| Step: 4
Training loss: 1.799060583114624
Validation loss: 2.0847418506940207

Epoch: 6| Step: 5
Training loss: 2.2158899307250977
Validation loss: 2.0813600619633994

Epoch: 6| Step: 6
Training loss: 2.5112533569335938
Validation loss: 2.068316876888275

Epoch: 6| Step: 7
Training loss: 2.4350662231445312
Validation loss: 2.077179471651713

Epoch: 6| Step: 8
Training loss: 1.6318893432617188
Validation loss: 2.0778135657310486

Epoch: 6| Step: 9
Training loss: 2.346674680709839
Validation loss: 2.06687593460083

Epoch: 6| Step: 10
Training loss: 2.1074538230895996
Validation loss: 2.0821728507677713

Epoch: 6| Step: 11
Training loss: 2.0336380004882812
Validation loss: 2.082243581612905

Epoch: 6| Step: 12
Training loss: 1.4917665719985962
Validation loss: 2.0718900163968406

Epoch: 6| Step: 13
Training loss: 2.302328109741211
Validation loss: 2.0780314207077026

Epoch: 179| Step: 0
Training loss: 2.327455759048462
Validation loss: 2.0767458279927573

Epoch: 6| Step: 1
Training loss: 2.0732643604278564
Validation loss: 2.0848437547683716

Epoch: 6| Step: 2
Training loss: 2.386643171310425
Validation loss: 2.0837309757868447

Epoch: 6| Step: 3
Training loss: 1.4081125259399414
Validation loss: 2.0846129258473716

Epoch: 6| Step: 4
Training loss: 2.741818904876709
Validation loss: 2.083585818608602

Epoch: 6| Step: 5
Training loss: 1.8237683773040771
Validation loss: 2.0823222994804382

Epoch: 6| Step: 6
Training loss: 1.5914326906204224
Validation loss: 2.077383557955424

Epoch: 6| Step: 7
Training loss: 2.14552640914917
Validation loss: 2.081290682156881

Epoch: 6| Step: 8
Training loss: 2.164376974105835
Validation loss: 2.078155299027761

Epoch: 6| Step: 9
Training loss: 1.7083555459976196
Validation loss: 2.0817519426345825

Epoch: 6| Step: 10
Training loss: 2.0475995540618896
Validation loss: 2.077623208363851

Epoch: 6| Step: 11
Training loss: 1.8333215713500977
Validation loss: 2.0596255461374917

Epoch: 6| Step: 12
Training loss: 2.0560665130615234
Validation loss: 2.069466710090637

Epoch: 6| Step: 13
Training loss: 1.8887004852294922
Validation loss: 2.0646347602208457

Epoch: 180| Step: 0
Training loss: 1.8355134725570679
Validation loss: 2.0526826779047647

Epoch: 6| Step: 1
Training loss: 2.0784473419189453
Validation loss: 2.0698421001434326

Epoch: 6| Step: 2
Training loss: 1.8213402032852173
Validation loss: 2.070967356363932

Epoch: 6| Step: 3
Training loss: 2.073946475982666
Validation loss: 2.058344860871633

Epoch: 6| Step: 4
Training loss: 1.668637752532959
Validation loss: 2.067166010538737

Epoch: 6| Step: 5
Training loss: 1.765834093093872
Validation loss: 2.076852858066559

Epoch: 6| Step: 6
Training loss: 2.3876819610595703
Validation loss: 2.060010393460592

Epoch: 6| Step: 7
Training loss: 2.5727546215057373
Validation loss: 2.065067489941915

Epoch: 6| Step: 8
Training loss: 2.508816719055176
Validation loss: 2.0648406545321145

Epoch: 6| Step: 9
Training loss: 2.4611222743988037
Validation loss: 2.079973121484121

Epoch: 6| Step: 10
Training loss: 1.224529504776001
Validation loss: 2.0778804620107016

Epoch: 6| Step: 11
Training loss: 1.5259075164794922
Validation loss: 2.086381117502848

Epoch: 6| Step: 12
Training loss: 2.274050712585449
Validation loss: 2.074048618475596

Epoch: 6| Step: 13
Training loss: 1.9947009086608887
Validation loss: 2.071982125441233

Epoch: 181| Step: 0
Training loss: 1.3694870471954346
Validation loss: 2.085707207520803

Epoch: 6| Step: 1
Training loss: 1.4575546979904175
Validation loss: 2.0800411303838096

Epoch: 6| Step: 2
Training loss: 2.1510887145996094
Validation loss: 2.070967197418213

Epoch: 6| Step: 3
Training loss: 2.28407621383667
Validation loss: 2.0909660259882608

Epoch: 6| Step: 4
Training loss: 1.8549907207489014
Validation loss: 2.083934764067332

Epoch: 6| Step: 5
Training loss: 2.683600425720215
Validation loss: 2.0751293102900186

Epoch: 6| Step: 6
Training loss: 1.5494754314422607
Validation loss: 2.076859474182129

Epoch: 6| Step: 7
Training loss: 2.169628381729126
Validation loss: 2.093944331010183

Epoch: 6| Step: 8
Training loss: 2.0723156929016113
Validation loss: 2.089419662952423

Epoch: 6| Step: 9
Training loss: 1.9831775426864624
Validation loss: 2.0871691703796387

Epoch: 6| Step: 10
Training loss: 2.375588893890381
Validation loss: 2.0913728872934976

Epoch: 6| Step: 11
Training loss: 1.6546952724456787
Validation loss: 2.0861856937408447

Epoch: 6| Step: 12
Training loss: 1.747021198272705
Validation loss: 2.0877374410629272

Epoch: 6| Step: 13
Training loss: 2.7840142250061035
Validation loss: 2.093638022740682

Epoch: 182| Step: 0
Training loss: 1.588000774383545
Validation loss: 2.0933008988698325

Epoch: 6| Step: 1
Training loss: 1.9707458019256592
Validation loss: 2.101715624332428

Epoch: 6| Step: 2
Training loss: 2.7233784198760986
Validation loss: 2.0989421208699546

Epoch: 6| Step: 3
Training loss: 2.1141233444213867
Validation loss: 2.08221564690272

Epoch: 6| Step: 4
Training loss: 1.994176983833313
Validation loss: 2.076286236445109

Epoch: 6| Step: 5
Training loss: 1.9403963088989258
Validation loss: 2.0740338563919067

Epoch: 6| Step: 6
Training loss: 1.4763364791870117
Validation loss: 2.0868295431137085

Epoch: 6| Step: 7
Training loss: 2.204072952270508
Validation loss: 2.1007736126581826

Epoch: 6| Step: 8
Training loss: 1.6459612846374512
Validation loss: 2.0807252724965415

Epoch: 6| Step: 9
Training loss: 1.7746587991714478
Validation loss: 2.07985512415568

Epoch: 6| Step: 10
Training loss: 2.1029298305511475
Validation loss: 2.0960559447606406

Epoch: 6| Step: 11
Training loss: 1.5738592147827148
Validation loss: 2.089717129866282

Epoch: 6| Step: 12
Training loss: 2.1323418617248535
Validation loss: 2.0918397307395935

Epoch: 6| Step: 13
Training loss: 2.780829668045044
Validation loss: 2.085222621758779

Epoch: 183| Step: 0
Training loss: 2.7042574882507324
Validation loss: 2.0947306752204895

Epoch: 6| Step: 1
Training loss: 2.1666386127471924
Validation loss: 2.091655910015106

Epoch: 6| Step: 2
Training loss: 2.288483142852783
Validation loss: 2.077957491079966

Epoch: 6| Step: 3
Training loss: 2.098721981048584
Validation loss: 2.087448795636495

Epoch: 6| Step: 4
Training loss: 1.7393152713775635
Validation loss: 2.0974053740501404

Epoch: 6| Step: 5
Training loss: 2.097370147705078
Validation loss: 2.093956013520559

Epoch: 6| Step: 6
Training loss: 1.6024951934814453
Validation loss: 2.091065545876821

Epoch: 6| Step: 7
Training loss: 2.2495782375335693
Validation loss: 2.097867429256439

Epoch: 6| Step: 8
Training loss: 1.959080457687378
Validation loss: 2.0964070757230124

Epoch: 6| Step: 9
Training loss: 1.6134259700775146
Validation loss: 2.073366343975067

Epoch: 6| Step: 10
Training loss: 1.7852280139923096
Validation loss: 2.0976009567578635

Epoch: 6| Step: 11
Training loss: 2.0462212562561035
Validation loss: 2.0731000701586404

Epoch: 6| Step: 12
Training loss: 1.7403556108474731
Validation loss: 2.079549551010132

Epoch: 6| Step: 13
Training loss: 2.0087597370147705
Validation loss: 2.0610686937967935

Epoch: 184| Step: 0
Training loss: 2.877493143081665
Validation loss: 2.0743317008018494

Epoch: 6| Step: 1
Training loss: 2.125215530395508
Validation loss: 2.080593446890513

Epoch: 6| Step: 2
Training loss: 1.673728585243225
Validation loss: 2.0764429569244385

Epoch: 6| Step: 3
Training loss: 2.1236486434936523
Validation loss: 2.0826472441355386

Epoch: 6| Step: 4
Training loss: 1.7802501916885376
Validation loss: 2.079759180545807

Epoch: 6| Step: 5
Training loss: 1.9546412229537964
Validation loss: 2.0932132403055825

Epoch: 6| Step: 6
Training loss: 1.629152536392212
Validation loss: 2.0818839271863303

Epoch: 6| Step: 7
Training loss: 2.150141477584839
Validation loss: 2.0949170192082724

Epoch: 6| Step: 8
Training loss: 1.5972139835357666
Validation loss: 2.1005188822746277

Epoch: 6| Step: 9
Training loss: 1.740851640701294
Validation loss: 2.0909524957338967

Epoch: 6| Step: 10
Training loss: 2.0392374992370605
Validation loss: 2.0866742928822837

Epoch: 6| Step: 11
Training loss: 2.7306113243103027
Validation loss: 2.0970603624979653

Epoch: 6| Step: 12
Training loss: 1.8472864627838135
Validation loss: 2.0934446454048157

Epoch: 6| Step: 13
Training loss: 1.5912246704101562
Validation loss: 2.090263843536377

Epoch: 185| Step: 0
Training loss: 2.0320358276367188
Validation loss: 2.0806200901667276

Epoch: 6| Step: 1
Training loss: 1.6361474990844727
Validation loss: 2.099080502986908

Epoch: 6| Step: 2
Training loss: 2.052614212036133
Validation loss: 2.0849605997403464

Epoch: 6| Step: 3
Training loss: 2.396984100341797
Validation loss: 2.081763664881388

Epoch: 6| Step: 4
Training loss: 1.9004478454589844
Validation loss: 2.0846805373827615

Epoch: 6| Step: 5
Training loss: 1.6625968217849731
Validation loss: 2.0778188506762185

Epoch: 6| Step: 6
Training loss: 2.261152505874634
Validation loss: 2.0965524514516196

Epoch: 6| Step: 7
Training loss: 1.920312523841858
Validation loss: 2.094327449798584

Epoch: 6| Step: 8
Training loss: 1.9458909034729004
Validation loss: 2.092315753300985

Epoch: 6| Step: 9
Training loss: 1.8943259716033936
Validation loss: 2.0821577111879983

Epoch: 6| Step: 10
Training loss: 2.161343812942505
Validation loss: 2.0958613753318787

Epoch: 6| Step: 11
Training loss: 2.542339563369751
Validation loss: 2.0889031291007996

Epoch: 6| Step: 12
Training loss: 2.3213796615600586
Validation loss: 2.0850170056025186

Epoch: 6| Step: 13
Training loss: 1.2327989339828491
Validation loss: 2.0799075961112976

Epoch: 186| Step: 0
Training loss: 2.3013558387756348
Validation loss: 2.0791809956232705

Epoch: 6| Step: 1
Training loss: 1.7438759803771973
Validation loss: 2.08199808994929

Epoch: 6| Step: 2
Training loss: 2.2563462257385254
Validation loss: 2.0772854884465537

Epoch: 6| Step: 3
Training loss: 2.5966436862945557
Validation loss: 2.0800511638323465

Epoch: 6| Step: 4
Training loss: 2.297283411026001
Validation loss: 2.0925928354263306

Epoch: 6| Step: 5
Training loss: 2.1258068084716797
Validation loss: 2.074845234553019

Epoch: 6| Step: 6
Training loss: 1.9553394317626953
Validation loss: 2.0856125354766846

Epoch: 6| Step: 7
Training loss: 1.5082076787948608
Validation loss: 2.0817237893740335

Epoch: 6| Step: 8
Training loss: 1.7618060111999512
Validation loss: 2.0741739670435586

Epoch: 6| Step: 9
Training loss: 1.9533402919769287
Validation loss: 2.0755293369293213

Epoch: 6| Step: 10
Training loss: 1.7555015087127686
Validation loss: 2.0822463830312095

Epoch: 6| Step: 11
Training loss: 2.3029348850250244
Validation loss: 2.087440768877665

Epoch: 6| Step: 12
Training loss: 1.9701120853424072
Validation loss: 2.088689863681793

Epoch: 6| Step: 13
Training loss: 1.502549409866333
Validation loss: 2.092583735783895

Epoch: 187| Step: 0
Training loss: 2.254106283187866
Validation loss: 2.0906291802724204

Epoch: 6| Step: 1
Training loss: 2.059276819229126
Validation loss: 2.084815184275309

Epoch: 6| Step: 2
Training loss: 1.4704294204711914
Validation loss: 2.0785003105799356

Epoch: 6| Step: 3
Training loss: 1.7835328578948975
Validation loss: 2.0853513876597085

Epoch: 6| Step: 4
Training loss: 1.1213582754135132
Validation loss: 2.0782758394877114

Epoch: 6| Step: 5
Training loss: 2.496598243713379
Validation loss: 2.088125546773275

Epoch: 6| Step: 6
Training loss: 2.2380309104919434
Validation loss: 2.1086831092834473

Epoch: 6| Step: 7
Training loss: 1.9796268939971924
Validation loss: 2.0876024961471558

Epoch: 6| Step: 8
Training loss: 1.9354684352874756
Validation loss: 2.0724364519119263

Epoch: 6| Step: 9
Training loss: 1.7204768657684326
Validation loss: 2.0864821473757424

Epoch: 6| Step: 10
Training loss: 2.8556814193725586
Validation loss: 2.085579435030619

Epoch: 6| Step: 11
Training loss: 2.3441967964172363
Validation loss: 2.0910030206044516

Epoch: 6| Step: 12
Training loss: 2.4921600818634033
Validation loss: 2.0934892892837524

Epoch: 6| Step: 13
Training loss: 1.4116172790527344
Validation loss: 2.0968299905459085

Epoch: 188| Step: 0
Training loss: 1.9981573820114136
Validation loss: 2.0828505158424377

Epoch: 6| Step: 1
Training loss: 2.0994372367858887
Validation loss: 2.0716509024302163

Epoch: 6| Step: 2
Training loss: 1.663515567779541
Validation loss: 2.084973414738973

Epoch: 6| Step: 3
Training loss: 1.2397812604904175
Validation loss: 2.0785322388013205

Epoch: 6| Step: 4
Training loss: 2.14854097366333
Validation loss: 2.0599059661229453

Epoch: 6| Step: 5
Training loss: 1.6146509647369385
Validation loss: 2.065453271071116

Epoch: 6| Step: 6
Training loss: 2.11159610748291
Validation loss: 2.0700398484865823

Epoch: 6| Step: 7
Training loss: 2.0972626209259033
Validation loss: 2.0710613131523132

Epoch: 6| Step: 8
Training loss: 1.9406219720840454
Validation loss: 2.059297561645508

Epoch: 6| Step: 9
Training loss: 1.878657579421997
Validation loss: 2.0741580526034036

Epoch: 6| Step: 10
Training loss: 2.8050434589385986
Validation loss: 2.0891822576522827

Epoch: 6| Step: 11
Training loss: 2.0561628341674805
Validation loss: 2.084608316421509

Epoch: 6| Step: 12
Training loss: 2.334404945373535
Validation loss: 2.089577833811442

Epoch: 6| Step: 13
Training loss: 1.959696888923645
Validation loss: 2.0876028339068093

Epoch: 189| Step: 0
Training loss: 1.6923282146453857
Validation loss: 2.085701604684194

Epoch: 6| Step: 1
Training loss: 2.7611000537872314
Validation loss: 2.072165846824646

Epoch: 6| Step: 2
Training loss: 2.347017765045166
Validation loss: 2.084243814150492

Epoch: 6| Step: 3
Training loss: 2.411203145980835
Validation loss: 2.0818655093510947

Epoch: 6| Step: 4
Training loss: 1.7765626907348633
Validation loss: 2.078432480494181

Epoch: 6| Step: 5
Training loss: 2.2588560581207275
Validation loss: 2.0791101257006326

Epoch: 6| Step: 6
Training loss: 1.2602274417877197
Validation loss: 2.087381819883982

Epoch: 6| Step: 7
Training loss: 1.9844075441360474
Validation loss: 2.0824788411458335

Epoch: 6| Step: 8
Training loss: 1.6655710935592651
Validation loss: 2.10269025961558

Epoch: 6| Step: 9
Training loss: 1.9830752611160278
Validation loss: 2.099155286947886

Epoch: 6| Step: 10
Training loss: 1.3310779333114624
Validation loss: 2.086786468823751

Epoch: 6| Step: 11
Training loss: 2.550060749053955
Validation loss: 2.0885326862335205

Epoch: 6| Step: 12
Training loss: 2.4685263633728027
Validation loss: 2.0766000548998513

Epoch: 6| Step: 13
Training loss: 1.847090244293213
Validation loss: 2.084795037905375

Epoch: 190| Step: 0
Training loss: 1.4311046600341797
Validation loss: 2.077509343624115

Epoch: 6| Step: 1
Training loss: 2.212033271789551
Validation loss: 2.104006210962931

Epoch: 6| Step: 2
Training loss: 2.1824350357055664
Validation loss: 2.09650049606959

Epoch: 6| Step: 3
Training loss: 2.083860397338867
Validation loss: 2.093494435151418

Epoch: 6| Step: 4
Training loss: 2.3306267261505127
Validation loss: 2.097006142139435

Epoch: 6| Step: 5
Training loss: 2.0074801445007324
Validation loss: 2.088798701763153

Epoch: 6| Step: 6
Training loss: 2.1790971755981445
Validation loss: 2.0988194942474365

Epoch: 6| Step: 7
Training loss: 1.7011998891830444
Validation loss: 2.0828488071759543

Epoch: 6| Step: 8
Training loss: 1.2748029232025146
Validation loss: 2.0971243182818093

Epoch: 6| Step: 9
Training loss: 2.3393924236297607
Validation loss: 2.1046881079673767

Epoch: 6| Step: 10
Training loss: 2.052455425262451
Validation loss: 2.099407354990641

Epoch: 6| Step: 11
Training loss: 2.1631293296813965
Validation loss: 2.108494301637014

Epoch: 6| Step: 12
Training loss: 2.2498512268066406
Validation loss: 2.097497582435608

Epoch: 6| Step: 13
Training loss: 1.7656753063201904
Validation loss: 2.0989574988683066

Epoch: 191| Step: 0
Training loss: 2.106062412261963
Validation loss: 2.0889554023742676

Epoch: 6| Step: 1
Training loss: 1.0085673332214355
Validation loss: 2.0894227226575217

Epoch: 6| Step: 2
Training loss: 1.9903290271759033
Validation loss: 2.074930270512899

Epoch: 6| Step: 3
Training loss: 2.0700807571411133
Validation loss: 2.0987837314605713

Epoch: 6| Step: 4
Training loss: 2.3534069061279297
Validation loss: 2.08289368947347

Epoch: 6| Step: 5
Training loss: 2.911745071411133
Validation loss: 2.08769154548645

Epoch: 6| Step: 6
Training loss: 2.2889482975006104
Validation loss: 2.090632597605387

Epoch: 6| Step: 7
Training loss: 2.1053578853607178
Validation loss: 2.094298561414083

Epoch: 6| Step: 8
Training loss: 2.0799248218536377
Validation loss: 2.101920485496521

Epoch: 6| Step: 9
Training loss: 1.4970749616622925
Validation loss: 2.092021107673645

Epoch: 6| Step: 10
Training loss: 1.4659686088562012
Validation loss: 2.1031080881754556

Epoch: 6| Step: 11
Training loss: 1.8989759683609009
Validation loss: 2.091622988382975

Epoch: 6| Step: 12
Training loss: 2.4099082946777344
Validation loss: 2.107400039831797

Epoch: 6| Step: 13
Training loss: 1.558842420578003
Validation loss: 2.1047595938046775

Epoch: 192| Step: 0
Training loss: 2.6189932823181152
Validation loss: 2.105712433656057

Epoch: 6| Step: 1
Training loss: 1.606259822845459
Validation loss: 2.095828652381897

Epoch: 6| Step: 2
Training loss: 2.5763115882873535
Validation loss: 2.0993455251057944

Epoch: 6| Step: 3
Training loss: 1.739379644393921
Validation loss: 2.0874401529630027

Epoch: 6| Step: 4
Training loss: 1.7124143838882446
Validation loss: 2.0919819672902427

Epoch: 6| Step: 5
Training loss: 2.360549211502075
Validation loss: 2.0962034662564597

Epoch: 6| Step: 6
Training loss: 1.7310330867767334
Validation loss: 2.098932941754659

Epoch: 6| Step: 7
Training loss: 2.1012420654296875
Validation loss: 2.095133900642395

Epoch: 6| Step: 8
Training loss: 1.828641414642334
Validation loss: 2.093480626742045

Epoch: 6| Step: 9
Training loss: 2.081470251083374
Validation loss: 2.0902708371480307

Epoch: 6| Step: 10
Training loss: 1.4579639434814453
Validation loss: 2.068675975004832

Epoch: 6| Step: 11
Training loss: 2.3187718391418457
Validation loss: 2.0767099062601724

Epoch: 6| Step: 12
Training loss: 2.2982993125915527
Validation loss: 2.070331255594889

Epoch: 6| Step: 13
Training loss: 1.4353348016738892
Validation loss: 2.0902602672576904

Epoch: 193| Step: 0
Training loss: 1.6304492950439453
Validation loss: 2.074862241744995

Epoch: 6| Step: 1
Training loss: 2.1048738956451416
Validation loss: 2.085403621196747

Epoch: 6| Step: 2
Training loss: 1.7130590677261353
Validation loss: 2.0927768548329673

Epoch: 6| Step: 3
Training loss: 1.771228313446045
Validation loss: 2.101351181666056

Epoch: 6| Step: 4
Training loss: 2.522920608520508
Validation loss: 2.1029828985532126

Epoch: 6| Step: 5
Training loss: 2.3628878593444824
Validation loss: 2.098711351553599

Epoch: 6| Step: 6
Training loss: 1.3201926946640015
Validation loss: 2.1092213789621987

Epoch: 6| Step: 7
Training loss: 2.0019288063049316
Validation loss: 2.108864446481069

Epoch: 6| Step: 8
Training loss: 2.1296067237854004
Validation loss: 2.11604243516922

Epoch: 6| Step: 9
Training loss: 2.1033568382263184
Validation loss: 2.1031780441602073

Epoch: 6| Step: 10
Training loss: 1.7903809547424316
Validation loss: 2.1111073891321817

Epoch: 6| Step: 11
Training loss: 2.417966842651367
Validation loss: 2.103842536608378

Epoch: 6| Step: 12
Training loss: 1.9714250564575195
Validation loss: 2.0927428603172302

Epoch: 6| Step: 13
Training loss: 2.1518542766571045
Validation loss: 2.0868203242619834

Epoch: 194| Step: 0
Training loss: 1.6453992128372192
Validation loss: 2.0986710588137307

Epoch: 6| Step: 1
Training loss: 2.186253070831299
Validation loss: 2.0899463097254434

Epoch: 6| Step: 2
Training loss: 1.978391170501709
Validation loss: 2.085659941037496

Epoch: 6| Step: 3
Training loss: 1.3801701068878174
Validation loss: 2.099031905333201

Epoch: 6| Step: 4
Training loss: 1.4036650657653809
Validation loss: 2.085496266682943

Epoch: 6| Step: 5
Training loss: 2.4137840270996094
Validation loss: 2.0942031145095825

Epoch: 6| Step: 6
Training loss: 2.179173469543457
Validation loss: 2.09939173857371

Epoch: 6| Step: 7
Training loss: 2.520972728729248
Validation loss: 2.0947081645329795

Epoch: 6| Step: 8
Training loss: 1.9276888370513916
Validation loss: 2.103950798511505

Epoch: 6| Step: 9
Training loss: 1.520648717880249
Validation loss: 2.103259722391764

Epoch: 6| Step: 10
Training loss: 1.5368480682373047
Validation loss: 2.0862468679745994

Epoch: 6| Step: 11
Training loss: 2.364020824432373
Validation loss: 2.086544851462046

Epoch: 6| Step: 12
Training loss: 2.4733428955078125
Validation loss: 2.1004088521003723

Epoch: 6| Step: 13
Training loss: 2.218122959136963
Validation loss: 2.0914854804674783

Epoch: 195| Step: 0
Training loss: 1.8703992366790771
Validation loss: 2.1060863931973777

Epoch: 6| Step: 1
Training loss: 1.9788254499435425
Validation loss: 2.101097881793976

Epoch: 6| Step: 2
Training loss: 2.587249517440796
Validation loss: 2.0898940563201904

Epoch: 6| Step: 3
Training loss: 1.5065810680389404
Validation loss: 2.0963064432144165

Epoch: 6| Step: 4
Training loss: 1.709342360496521
Validation loss: 2.0947519342104592

Epoch: 6| Step: 5
Training loss: 2.3990225791931152
Validation loss: 2.1040942470232644

Epoch: 6| Step: 6
Training loss: 1.4350554943084717
Validation loss: 2.1010995308558145

Epoch: 6| Step: 7
Training loss: 2.255079984664917
Validation loss: 2.1030318339665732

Epoch: 6| Step: 8
Training loss: 1.5644264221191406
Validation loss: 2.090138574441274

Epoch: 6| Step: 9
Training loss: 2.232393503189087
Validation loss: 2.073461393515269

Epoch: 6| Step: 10
Training loss: 2.9394209384918213
Validation loss: 2.094887892405192

Epoch: 6| Step: 11
Training loss: 1.9795446395874023
Validation loss: 2.0915596087773642

Epoch: 6| Step: 12
Training loss: 1.796190857887268
Validation loss: 2.09694242477417

Epoch: 6| Step: 13
Training loss: 1.7553709745407104
Validation loss: 2.1014790137608848

Epoch: 196| Step: 0
Training loss: 1.4895687103271484
Validation loss: 2.0892296036084494

Epoch: 6| Step: 1
Training loss: 2.806180000305176
Validation loss: 2.0876589814821878

Epoch: 6| Step: 2
Training loss: 2.0490260124206543
Validation loss: 2.0968804359436035

Epoch: 6| Step: 3
Training loss: 2.3750669956207275
Validation loss: 2.0941341718037925

Epoch: 6| Step: 4
Training loss: 2.0986547470092773
Validation loss: 2.107065717379252

Epoch: 6| Step: 5
Training loss: 1.6640212535858154
Validation loss: 2.0874419609705606

Epoch: 6| Step: 6
Training loss: 2.1071395874023438
Validation loss: 2.1056302984555564

Epoch: 6| Step: 7
Training loss: 2.060258388519287
Validation loss: 2.1074506839116416

Epoch: 6| Step: 8
Training loss: 1.6147700548171997
Validation loss: 2.1005210876464844

Epoch: 6| Step: 9
Training loss: 2.2376339435577393
Validation loss: 2.097211241722107

Epoch: 6| Step: 10
Training loss: 2.3671295642852783
Validation loss: 2.1012455026308694

Epoch: 6| Step: 11
Training loss: 1.5010628700256348
Validation loss: 2.09671421845754

Epoch: 6| Step: 12
Training loss: 1.6298294067382812
Validation loss: 2.10225502649943

Epoch: 6| Step: 13
Training loss: 2.0189003944396973
Validation loss: 2.0987708369890847

Epoch: 197| Step: 0
Training loss: 2.4932937622070312
Validation loss: 2.1052565773328147

Epoch: 6| Step: 1
Training loss: 1.9090445041656494
Validation loss: 2.0928160548210144

Epoch: 6| Step: 2
Training loss: 1.4162548780441284
Validation loss: 2.102171798547109

Epoch: 6| Step: 3
Training loss: 2.343783378601074
Validation loss: 2.0984782775243125

Epoch: 6| Step: 4
Training loss: 1.162142276763916
Validation loss: 2.0957597891489663

Epoch: 6| Step: 5
Training loss: 2.1775572299957275
Validation loss: 2.087172249952952

Epoch: 6| Step: 6
Training loss: 2.0377426147460938
Validation loss: 2.08824888865153

Epoch: 6| Step: 7
Training loss: 1.9868838787078857
Validation loss: 2.0879347125689187

Epoch: 6| Step: 8
Training loss: 1.5418481826782227
Validation loss: 2.105043431123098

Epoch: 6| Step: 9
Training loss: 2.2221579551696777
Validation loss: 2.0865978797276816

Epoch: 6| Step: 10
Training loss: 1.8439615964889526
Validation loss: 2.088350534439087

Epoch: 6| Step: 11
Training loss: 2.0839385986328125
Validation loss: 2.0726917386054993

Epoch: 6| Step: 12
Training loss: 2.412170886993408
Validation loss: 2.0834319988886514

Epoch: 6| Step: 13
Training loss: 2.1284737586975098
Validation loss: 2.089417278766632

Epoch: 198| Step: 0
Training loss: 1.421128749847412
Validation loss: 2.083881676197052

Epoch: 6| Step: 1
Training loss: 2.1545305252075195
Validation loss: 2.0920817852020264

Epoch: 6| Step: 2
Training loss: 1.7740755081176758
Validation loss: 2.0884875655174255

Epoch: 6| Step: 3
Training loss: 2.3172378540039062
Validation loss: 2.0891104539235434

Epoch: 6| Step: 4
Training loss: 2.002121925354004
Validation loss: 2.074837783972422

Epoch: 6| Step: 5
Training loss: 1.859197735786438
Validation loss: 2.0737505157788596

Epoch: 6| Step: 6
Training loss: 1.9710767269134521
Validation loss: 2.0777327020963035

Epoch: 6| Step: 7
Training loss: 1.8976954221725464
Validation loss: 2.072165846824646

Epoch: 6| Step: 8
Training loss: 1.932462215423584
Validation loss: 2.0852560798327127

Epoch: 6| Step: 9
Training loss: 2.5570478439331055
Validation loss: 2.0674425760904946

Epoch: 6| Step: 10
Training loss: 2.1544535160064697
Validation loss: 2.0776468912760415

Epoch: 6| Step: 11
Training loss: 2.140383720397949
Validation loss: 2.073215206464132

Epoch: 6| Step: 12
Training loss: 1.8250013589859009
Validation loss: 2.074078937371572

Epoch: 6| Step: 13
Training loss: 2.0145392417907715
Validation loss: 2.0844852924346924

Epoch: 199| Step: 0
Training loss: 1.9719055891036987
Validation loss: 2.083646853764852

Epoch: 6| Step: 1
Training loss: 2.369109630584717
Validation loss: 2.0902902285257974

Epoch: 6| Step: 2
Training loss: 1.8037322759628296
Validation loss: 2.087967813014984

Epoch: 6| Step: 3
Training loss: 2.7625389099121094
Validation loss: 2.0845015247662864

Epoch: 6| Step: 4
Training loss: 1.8719978332519531
Validation loss: 2.090490142504374

Epoch: 6| Step: 5
Training loss: 2.3707730770111084
Validation loss: 2.089318116505941

Epoch: 6| Step: 6
Training loss: 1.7219792604446411
Validation loss: 2.0932326912879944

Epoch: 6| Step: 7
Training loss: 2.7700517177581787
Validation loss: 2.0985927184422812

Epoch: 6| Step: 8
Training loss: 1.5456982851028442
Validation loss: 2.0867530504862466

Epoch: 6| Step: 9
Training loss: 1.1319091320037842
Validation loss: 2.0974168181419373

Epoch: 6| Step: 10
Training loss: 1.4490476846694946
Validation loss: 2.0942907333374023

Epoch: 6| Step: 11
Training loss: 2.1618971824645996
Validation loss: 2.09945140282313

Epoch: 6| Step: 12
Training loss: 1.783889889717102
Validation loss: 2.1146501302719116

Epoch: 6| Step: 13
Training loss: 2.063015937805176
Validation loss: 2.097385346889496

Epoch: 200| Step: 0
Training loss: 1.582257866859436
Validation loss: 2.118035852909088

Epoch: 6| Step: 1
Training loss: 2.127328395843506
Validation loss: 2.102513015270233

Epoch: 6| Step: 2
Training loss: 1.906674861907959
Validation loss: 2.1040990749994912

Epoch: 6| Step: 3
Training loss: 1.548527479171753
Validation loss: 2.097972790400187

Epoch: 6| Step: 4
Training loss: 1.8727796077728271
Validation loss: 2.1004698276519775

Epoch: 6| Step: 5
Training loss: 2.3336431980133057
Validation loss: 2.098919173081716

Epoch: 6| Step: 6
Training loss: 2.0632543563842773
Validation loss: 2.111186901728312

Epoch: 6| Step: 7
Training loss: 1.6407322883605957
Validation loss: 2.1225544214248657

Epoch: 6| Step: 8
Training loss: 1.8553831577301025
Validation loss: 2.1068047086397805

Epoch: 6| Step: 9
Training loss: 1.9034730195999146
Validation loss: 2.1002187728881836

Epoch: 6| Step: 10
Training loss: 1.9198276996612549
Validation loss: 2.1012243032455444

Epoch: 6| Step: 11
Training loss: 2.7199478149414062
Validation loss: 2.0993281602859497

Epoch: 6| Step: 12
Training loss: 1.8820909261703491
Validation loss: 2.1078136960665383

Epoch: 6| Step: 13
Training loss: 2.413912296295166
Validation loss: 2.0824541052182517

Epoch: 201| Step: 0
Training loss: 2.627688407897949
Validation loss: 2.0944559375445047

Epoch: 6| Step: 1
Training loss: 1.550950050354004
Validation loss: 2.08671764532725

Epoch: 6| Step: 2
Training loss: 2.137134552001953
Validation loss: 2.0981404781341553

Epoch: 6| Step: 3
Training loss: 1.738508939743042
Validation loss: 2.1023945609728494

Epoch: 6| Step: 4
Training loss: 2.3067855834960938
Validation loss: 2.100831627845764

Epoch: 6| Step: 5
Training loss: 1.2688536643981934
Validation loss: 2.1145761410395303

Epoch: 6| Step: 6
Training loss: 2.007936477661133
Validation loss: 2.109103739261627

Epoch: 6| Step: 7
Training loss: 1.869845986366272
Validation loss: 2.107034961382548

Epoch: 6| Step: 8
Training loss: 1.805248498916626
Validation loss: 2.110526223977407

Epoch: 6| Step: 9
Training loss: 2.304068088531494
Validation loss: 2.1166940530141196

Epoch: 6| Step: 10
Training loss: 1.8118840456008911
Validation loss: 2.1177944342295327

Epoch: 6| Step: 11
Training loss: 2.139549732208252
Validation loss: 2.128334899743398

Epoch: 6| Step: 12
Training loss: 2.2378244400024414
Validation loss: 2.1159644722938538

Epoch: 6| Step: 13
Training loss: 1.9417076110839844
Validation loss: 2.112753947575887

Epoch: 202| Step: 0
Training loss: 2.223620891571045
Validation loss: 2.107139607270559

Epoch: 6| Step: 1
Training loss: 1.9335312843322754
Validation loss: 2.1076423724492392

Epoch: 6| Step: 2
Training loss: 1.2491000890731812
Validation loss: 2.096638321876526

Epoch: 6| Step: 3
Training loss: 2.274092674255371
Validation loss: 2.110385258992513

Epoch: 6| Step: 4
Training loss: 2.0101678371429443
Validation loss: 2.1093210577964783

Epoch: 6| Step: 5
Training loss: 1.978097677230835
Validation loss: 2.107109089692434

Epoch: 6| Step: 6
Training loss: 2.484907627105713
Validation loss: 2.0972039302190146

Epoch: 6| Step: 7
Training loss: 1.94899582862854
Validation loss: 2.09482749303182

Epoch: 6| Step: 8
Training loss: 1.5053069591522217
Validation loss: 2.1167414585749307

Epoch: 6| Step: 9
Training loss: 1.5909243822097778
Validation loss: 2.124040186405182

Epoch: 6| Step: 10
Training loss: 2.326416492462158
Validation loss: 2.1129979888598123

Epoch: 6| Step: 11
Training loss: 2.4510068893432617
Validation loss: 2.121164302031199

Epoch: 6| Step: 12
Training loss: 1.7633063793182373
Validation loss: 2.1171589692433677

Epoch: 6| Step: 13
Training loss: 2.0853142738342285
Validation loss: 2.0956042210261026

Epoch: 203| Step: 0
Training loss: 1.8925042152404785
Validation loss: 2.1132184863090515

Epoch: 6| Step: 1
Training loss: 1.29551100730896
Validation loss: 2.1106397112210593

Epoch: 6| Step: 2
Training loss: 1.8970471620559692
Validation loss: 2.112015644709269

Epoch: 6| Step: 3
Training loss: 2.082855701446533
Validation loss: 2.129483918348948

Epoch: 6| Step: 4
Training loss: 1.7400025129318237
Validation loss: 2.115639626979828

Epoch: 6| Step: 5
Training loss: 1.5355684757232666
Validation loss: 2.1028594175974527

Epoch: 6| Step: 6
Training loss: 1.8636587858200073
Validation loss: 2.10579119126002

Epoch: 6| Step: 7
Training loss: 2.013106107711792
Validation loss: 2.1079072753588357

Epoch: 6| Step: 8
Training loss: 2.138761520385742
Validation loss: 2.123088558514913

Epoch: 6| Step: 9
Training loss: 1.746262788772583
Validation loss: 2.107917586962382

Epoch: 6| Step: 10
Training loss: 1.438265323638916
Validation loss: 2.115790685017904

Epoch: 6| Step: 11
Training loss: 2.8344223499298096
Validation loss: 2.118069887161255

Epoch: 6| Step: 12
Training loss: 2.1945502758026123
Validation loss: 2.110069513320923

Epoch: 6| Step: 13
Training loss: 2.7488584518432617
Validation loss: 2.095609446366628

Epoch: 204| Step: 0
Training loss: 2.3641104698181152
Validation loss: 2.1031837463378906

Epoch: 6| Step: 1
Training loss: 1.852102279663086
Validation loss: 2.092089573542277

Epoch: 6| Step: 2
Training loss: 1.7129815816879272
Validation loss: 2.093507766723633

Epoch: 6| Step: 3
Training loss: 1.5408662557601929
Validation loss: 2.114570955435435

Epoch: 6| Step: 4
Training loss: 1.9922714233398438
Validation loss: 2.1009741028149924

Epoch: 6| Step: 5
Training loss: 2.0835726261138916
Validation loss: 2.098566472530365

Epoch: 6| Step: 6
Training loss: 2.7847580909729004
Validation loss: 2.101818064848582

Epoch: 6| Step: 7
Training loss: 1.9723577499389648
Validation loss: 2.0990980863571167

Epoch: 6| Step: 8
Training loss: 2.4008774757385254
Validation loss: 2.091076076030731

Epoch: 6| Step: 9
Training loss: 1.7578186988830566
Validation loss: 2.100809951623281

Epoch: 6| Step: 10
Training loss: 1.4125944375991821
Validation loss: 2.1042654712994895

Epoch: 6| Step: 11
Training loss: 2.1046323776245117
Validation loss: 2.094503919283549

Epoch: 6| Step: 12
Training loss: 2.049929618835449
Validation loss: 2.092089752356211

Epoch: 6| Step: 13
Training loss: 1.7896064519882202
Validation loss: 2.0978418389956155

Epoch: 205| Step: 0
Training loss: 2.165865182876587
Validation loss: 2.0949532787005105

Epoch: 6| Step: 1
Training loss: 2.1844844818115234
Validation loss: 2.10311492284139

Epoch: 6| Step: 2
Training loss: 1.7078416347503662
Validation loss: 2.0868060986200967

Epoch: 6| Step: 3
Training loss: 2.349592447280884
Validation loss: 2.0950034062067666

Epoch: 6| Step: 4
Training loss: 1.739659070968628
Validation loss: 2.10666424036026

Epoch: 6| Step: 5
Training loss: 1.825305700302124
Validation loss: 2.1074532866477966

Epoch: 6| Step: 6
Training loss: 2.222433567047119
Validation loss: 2.094158132870992

Epoch: 6| Step: 7
Training loss: 2.285768508911133
Validation loss: 2.103268643220266

Epoch: 6| Step: 8
Training loss: 1.6334710121154785
Validation loss: 2.110758105913798

Epoch: 6| Step: 9
Training loss: 2.1045382022857666
Validation loss: 2.112440506617228

Epoch: 6| Step: 10
Training loss: 1.6183797121047974
Validation loss: 2.116418719291687

Epoch: 6| Step: 11
Training loss: 2.290132522583008
Validation loss: 2.0998708407084146

Epoch: 6| Step: 12
Training loss: 1.130140781402588
Validation loss: 2.1049163937568665

Epoch: 6| Step: 13
Training loss: 2.071920156478882
Validation loss: 2.1092117031415305

Epoch: 206| Step: 0
Training loss: 2.5087966918945312
Validation loss: 2.110337773958842

Epoch: 6| Step: 1
Training loss: 2.5675647258758545
Validation loss: 2.1002104679743447

Epoch: 6| Step: 2
Training loss: 2.3437962532043457
Validation loss: 2.1203625996907554

Epoch: 6| Step: 3
Training loss: 2.274519920349121
Validation loss: 2.0757238070170083

Epoch: 6| Step: 4
Training loss: 2.3626668453216553
Validation loss: 2.1140644749005637

Epoch: 6| Step: 5
Training loss: 1.8781371116638184
Validation loss: 2.093190391858419

Epoch: 6| Step: 6
Training loss: 1.2115007638931274
Validation loss: 2.1058239142100015

Epoch: 6| Step: 7
Training loss: 2.0098013877868652
Validation loss: 2.088067332903544

Epoch: 6| Step: 8
Training loss: 1.9612098932266235
Validation loss: 2.107645054658254

Epoch: 6| Step: 9
Training loss: 1.3023138046264648
Validation loss: 2.1049851377805076

Epoch: 6| Step: 10
Training loss: 2.0905215740203857
Validation loss: 2.1012277205785117

Epoch: 6| Step: 11
Training loss: 1.6684274673461914
Validation loss: 2.094527085622152

Epoch: 6| Step: 12
Training loss: 2.1717872619628906
Validation loss: 2.085321048895518

Epoch: 6| Step: 13
Training loss: 1.5339860916137695
Validation loss: 2.0879576206207275

Epoch: 207| Step: 0
Training loss: 2.046278476715088
Validation loss: 2.100256641705831

Epoch: 6| Step: 1
Training loss: 1.667403221130371
Validation loss: 2.09579598903656

Epoch: 6| Step: 2
Training loss: 2.626661777496338
Validation loss: 2.0907683769861856

Epoch: 6| Step: 3
Training loss: 1.620344877243042
Validation loss: 2.097475588321686

Epoch: 6| Step: 4
Training loss: 2.23671817779541
Validation loss: 2.1001422007878623

Epoch: 6| Step: 5
Training loss: 2.5309019088745117
Validation loss: 2.1080536246299744

Epoch: 6| Step: 6
Training loss: 1.8471544981002808
Validation loss: 2.0945187409718833

Epoch: 6| Step: 7
Training loss: 1.7176380157470703
Validation loss: 2.1031311750411987

Epoch: 6| Step: 8
Training loss: 1.970484733581543
Validation loss: 2.1024580200513205

Epoch: 6| Step: 9
Training loss: 1.8385887145996094
Validation loss: 2.111818552017212

Epoch: 6| Step: 10
Training loss: 1.5849366188049316
Validation loss: 2.110830624898275

Epoch: 6| Step: 11
Training loss: 2.0064945220947266
Validation loss: 2.101250688234965

Epoch: 6| Step: 12
Training loss: 2.024782419204712
Validation loss: 2.103540003299713

Epoch: 6| Step: 13
Training loss: 1.8009779453277588
Validation loss: 2.113764305909475

Epoch: 208| Step: 0
Training loss: 2.0083680152893066
Validation loss: 2.1086382468541465

Epoch: 6| Step: 1
Training loss: 2.123840808868408
Validation loss: 2.100987752278646

Epoch: 6| Step: 2
Training loss: 1.8283748626708984
Validation loss: 2.1155066092809043

Epoch: 6| Step: 3
Training loss: 2.1224212646484375
Validation loss: 2.101219097773234

Epoch: 6| Step: 4
Training loss: 1.9304289817810059
Validation loss: 2.10905522108078

Epoch: 6| Step: 5
Training loss: 1.7004194259643555
Validation loss: 2.1075268983840942

Epoch: 6| Step: 6
Training loss: 2.069248676300049
Validation loss: 2.116919974486033

Epoch: 6| Step: 7
Training loss: 2.3412230014801025
Validation loss: 2.102349797884623

Epoch: 6| Step: 8
Training loss: 1.8227314949035645
Validation loss: 2.117029368877411

Epoch: 6| Step: 9
Training loss: 1.5004277229309082
Validation loss: 2.1109370787938437

Epoch: 6| Step: 10
Training loss: 1.5626113414764404
Validation loss: 2.1121814250946045

Epoch: 6| Step: 11
Training loss: 2.517857551574707
Validation loss: 2.1087090969085693

Epoch: 6| Step: 12
Training loss: 1.7875239849090576
Validation loss: 2.112336575984955

Epoch: 6| Step: 13
Training loss: 2.453129291534424
Validation loss: 2.097082773844401

Epoch: 209| Step: 0
Training loss: 1.9334840774536133
Validation loss: 2.1068699757258096

Epoch: 6| Step: 1
Training loss: 2.0453319549560547
Validation loss: 2.1003678242365518

Epoch: 6| Step: 2
Training loss: 2.2444992065429688
Validation loss: 2.1197778383890786

Epoch: 6| Step: 3
Training loss: 2.1498022079467773
Validation loss: 2.1060397028923035

Epoch: 6| Step: 4
Training loss: 1.9252126216888428
Validation loss: 2.1062169671058655

Epoch: 6| Step: 5
Training loss: 1.9735474586486816
Validation loss: 2.1228861808776855

Epoch: 6| Step: 6
Training loss: 1.9127799272537231
Validation loss: 2.117430488268534

Epoch: 6| Step: 7
Training loss: 1.4590786695480347
Validation loss: 2.1060404976209006

Epoch: 6| Step: 8
Training loss: 1.9322949647903442
Validation loss: 2.1204067269961038

Epoch: 6| Step: 9
Training loss: 1.655267596244812
Validation loss: 2.109669884045919

Epoch: 6| Step: 10
Training loss: 2.610328197479248
Validation loss: 2.1099942723910012

Epoch: 6| Step: 11
Training loss: 2.447409152984619
Validation loss: 2.1049456795056662

Epoch: 6| Step: 12
Training loss: 2.0831313133239746
Validation loss: 2.11046310265859

Epoch: 6| Step: 13
Training loss: 1.6604140996932983
Validation loss: 2.107686181863149

Epoch: 210| Step: 0
Training loss: 2.1960532665252686
Validation loss: 2.117229461669922

Epoch: 6| Step: 1
Training loss: 1.7862145900726318
Validation loss: 2.1128419240315757

Epoch: 6| Step: 2
Training loss: 1.6482852697372437
Validation loss: 2.116875171661377

Epoch: 6| Step: 3
Training loss: 2.4848999977111816
Validation loss: 2.11590576171875

Epoch: 6| Step: 4
Training loss: 1.5061678886413574
Validation loss: 2.1174912254015603

Epoch: 6| Step: 5
Training loss: 1.5918482542037964
Validation loss: 2.119844893614451

Epoch: 6| Step: 6
Training loss: 2.0680136680603027
Validation loss: 2.105321248372396

Epoch: 6| Step: 7
Training loss: 2.6977713108062744
Validation loss: 2.11540158589681

Epoch: 6| Step: 8
Training loss: 1.7060928344726562
Validation loss: 2.11404025554657

Epoch: 6| Step: 9
Training loss: 2.013021945953369
Validation loss: 2.112769285837809

Epoch: 6| Step: 10
Training loss: 1.9639232158660889
Validation loss: 2.1161303718884787

Epoch: 6| Step: 11
Training loss: 1.3781168460845947
Validation loss: 2.1072553793589273

Epoch: 6| Step: 12
Training loss: 2.29056453704834
Validation loss: 2.107471307118734

Epoch: 6| Step: 13
Training loss: 2.3527708053588867
Validation loss: 2.0946505864461265

Epoch: 211| Step: 0
Training loss: 1.5049519538879395
Validation loss: 2.099430561065674

Epoch: 6| Step: 1
Training loss: 2.3405041694641113
Validation loss: 2.1178988416989646

Epoch: 6| Step: 2
Training loss: 1.3863129615783691
Validation loss: 2.107168714205424

Epoch: 6| Step: 3
Training loss: 1.6688340902328491
Validation loss: 2.1021841764450073

Epoch: 6| Step: 4
Training loss: 1.3992655277252197
Validation loss: 2.116461972395579

Epoch: 6| Step: 5
Training loss: 2.775083065032959
Validation loss: 2.112373153368632

Epoch: 6| Step: 6
Training loss: 1.50309157371521
Validation loss: 2.1192269325256348

Epoch: 6| Step: 7
Training loss: 2.1614065170288086
Validation loss: 2.1167476177215576

Epoch: 6| Step: 8
Training loss: 2.1903836727142334
Validation loss: 2.1127571066220603

Epoch: 6| Step: 9
Training loss: 1.7510991096496582
Validation loss: 2.106196403503418

Epoch: 6| Step: 10
Training loss: 2.766139507293701
Validation loss: 2.1034903526306152

Epoch: 6| Step: 11
Training loss: 2.7600173950195312
Validation loss: 2.116428236166636

Epoch: 6| Step: 12
Training loss: 2.206475019454956
Validation loss: 2.111579259236654

Epoch: 6| Step: 13
Training loss: 1.6988396644592285
Validation loss: 2.1263837019602456

Epoch: 212| Step: 0
Training loss: 2.10439133644104
Validation loss: 2.1339061657587686

Epoch: 6| Step: 1
Training loss: 1.5529483556747437
Validation loss: 2.1227076252301535

Epoch: 6| Step: 2
Training loss: 1.898239254951477
Validation loss: 2.124639630317688

Epoch: 6| Step: 3
Training loss: 1.6080055236816406
Validation loss: 2.115478197733561

Epoch: 6| Step: 4
Training loss: 2.2761740684509277
Validation loss: 2.1235040426254272

Epoch: 6| Step: 5
Training loss: 1.9925957918167114
Validation loss: 2.1265369653701782

Epoch: 6| Step: 6
Training loss: 2.325528621673584
Validation loss: 2.1248748103777566

Epoch: 6| Step: 7
Training loss: 2.0685014724731445
Validation loss: 2.1256809631983438

Epoch: 6| Step: 8
Training loss: 2.5040555000305176
Validation loss: 2.133083442846934

Epoch: 6| Step: 9
Training loss: 1.7698569297790527
Validation loss: 2.115145981311798

Epoch: 6| Step: 10
Training loss: 1.9243767261505127
Validation loss: 2.129159986972809

Epoch: 6| Step: 11
Training loss: 1.8856865167617798
Validation loss: 2.1146422624588013

Epoch: 6| Step: 12
Training loss: 1.8175020217895508
Validation loss: 2.1037179629007974

Epoch: 6| Step: 13
Training loss: 1.4320604801177979
Validation loss: 2.106387138366699

Epoch: 213| Step: 0
Training loss: 2.4762778282165527
Validation loss: 2.1108723084131875

Epoch: 6| Step: 1
Training loss: 1.9809491634368896
Validation loss: 2.108967403570811

Epoch: 6| Step: 2
Training loss: 1.907179832458496
Validation loss: 2.11760679880778

Epoch: 6| Step: 3
Training loss: 1.8011014461517334
Validation loss: 2.1117113629976907

Epoch: 6| Step: 4
Training loss: 1.718464732170105
Validation loss: 2.0958220760027566

Epoch: 6| Step: 5
Training loss: 1.544281005859375
Validation loss: 2.11611278851827

Epoch: 6| Step: 6
Training loss: 1.6577560901641846
Validation loss: 2.1215752164522805

Epoch: 6| Step: 7
Training loss: 2.0966084003448486
Validation loss: 2.1353970766067505

Epoch: 6| Step: 8
Training loss: 1.929076075553894
Validation loss: 2.1252530018488565

Epoch: 6| Step: 9
Training loss: 2.103750228881836
Validation loss: 2.1276410023371377

Epoch: 6| Step: 10
Training loss: 1.8505735397338867
Validation loss: 2.129216273625692

Epoch: 6| Step: 11
Training loss: 2.4713587760925293
Validation loss: 2.1265569925308228

Epoch: 6| Step: 12
Training loss: 1.886349081993103
Validation loss: 2.1382187008857727

Epoch: 6| Step: 13
Training loss: 1.7784006595611572
Validation loss: 2.1220726370811462

Epoch: 214| Step: 0
Training loss: 2.1048638820648193
Validation loss: 2.1140363017717996

Epoch: 6| Step: 1
Training loss: 1.2658675909042358
Validation loss: 2.1177786389986673

Epoch: 6| Step: 2
Training loss: 2.328643798828125
Validation loss: 2.1213685274124146

Epoch: 6| Step: 3
Training loss: 1.852195143699646
Validation loss: 2.117393891016642

Epoch: 6| Step: 4
Training loss: 1.450307846069336
Validation loss: 2.1087157328923545

Epoch: 6| Step: 5
Training loss: 1.352980136871338
Validation loss: 2.1285956303278604

Epoch: 6| Step: 6
Training loss: 2.171299457550049
Validation loss: 2.1141786773999534

Epoch: 6| Step: 7
Training loss: 2.893547773361206
Validation loss: 2.1066165367762246

Epoch: 6| Step: 8
Training loss: 1.9220788478851318
Validation loss: 2.1144882440567017

Epoch: 6| Step: 9
Training loss: 2.2060208320617676
Validation loss: 2.1227279901504517

Epoch: 6| Step: 10
Training loss: 1.947948932647705
Validation loss: 2.1200578808784485

Epoch: 6| Step: 11
Training loss: 1.787925124168396
Validation loss: 2.1329625646273294

Epoch: 6| Step: 12
Training loss: 2.1736667156219482
Validation loss: 2.12361212571462

Epoch: 6| Step: 13
Training loss: 1.6278069019317627
Validation loss: 2.1240145564079285

Epoch: 215| Step: 0
Training loss: 1.690176010131836
Validation loss: 2.135326345761617

Epoch: 6| Step: 1
Training loss: 1.7877426147460938
Validation loss: 2.1403592626253762

Epoch: 6| Step: 2
Training loss: 1.525390625
Validation loss: 2.1384721994400024

Epoch: 6| Step: 3
Training loss: 2.056985855102539
Validation loss: 2.127911408742269

Epoch: 6| Step: 4
Training loss: 2.0777840614318848
Validation loss: 2.128987431526184

Epoch: 6| Step: 5
Training loss: 1.7098772525787354
Validation loss: 2.13360188404719

Epoch: 6| Step: 6
Training loss: 2.0746428966522217
Validation loss: 2.138311227162679

Epoch: 6| Step: 7
Training loss: 2.0654096603393555
Validation loss: 2.1369667053222656

Epoch: 6| Step: 8
Training loss: 1.4335246086120605
Validation loss: 2.116525133450826

Epoch: 6| Step: 9
Training loss: 2.85237717628479
Validation loss: 2.1321192582448325

Epoch: 6| Step: 10
Training loss: 2.2966084480285645
Validation loss: 2.1279085675875344

Epoch: 6| Step: 11
Training loss: 1.1732869148254395
Validation loss: 2.1332303285598755

Epoch: 6| Step: 12
Training loss: 1.9958033561706543
Validation loss: 2.124795933564504

Epoch: 6| Step: 13
Training loss: 2.6403284072875977
Validation loss: 2.126171290874481

Epoch: 216| Step: 0
Training loss: 1.861855149269104
Validation loss: 2.1311725775400796

Epoch: 6| Step: 1
Training loss: 1.7502708435058594
Validation loss: 2.1309991081555686

Epoch: 6| Step: 2
Training loss: 2.0399303436279297
Validation loss: 2.1283397873242698

Epoch: 6| Step: 3
Training loss: 1.5564740896224976
Validation loss: 2.1333083311716714

Epoch: 6| Step: 4
Training loss: 1.900395154953003
Validation loss: 2.1160320043563843

Epoch: 6| Step: 5
Training loss: 1.886868953704834
Validation loss: 2.1140140493710837

Epoch: 6| Step: 6
Training loss: 2.1843888759613037
Validation loss: 2.1205496390660605

Epoch: 6| Step: 7
Training loss: 2.0885863304138184
Validation loss: 2.112935245037079

Epoch: 6| Step: 8
Training loss: 1.6564191579818726
Validation loss: 2.122248411178589

Epoch: 6| Step: 9
Training loss: 1.765706181526184
Validation loss: 2.1028507550557456

Epoch: 6| Step: 10
Training loss: 2.4868013858795166
Validation loss: 2.1154082218805947

Epoch: 6| Step: 11
Training loss: 2.3655271530151367
Validation loss: 2.0961923797925315

Epoch: 6| Step: 12
Training loss: 1.8988265991210938
Validation loss: 2.1062646905581155

Epoch: 6| Step: 13
Training loss: 1.75368332862854
Validation loss: 2.112180153528849

Epoch: 217| Step: 0
Training loss: 2.2943944931030273
Validation loss: 2.0951029658317566

Epoch: 6| Step: 1
Training loss: 1.8989899158477783
Validation loss: 2.13366691271464

Epoch: 6| Step: 2
Training loss: 2.098559856414795
Validation loss: 2.1305447618166604

Epoch: 6| Step: 3
Training loss: 1.9168615341186523
Validation loss: 2.1255183617273965

Epoch: 6| Step: 4
Training loss: 1.4113858938217163
Validation loss: 2.123088777065277

Epoch: 6| Step: 5
Training loss: 2.142925500869751
Validation loss: 2.119674583276113

Epoch: 6| Step: 6
Training loss: 1.5998311042785645
Validation loss: 2.1320574283599854

Epoch: 6| Step: 7
Training loss: 2.068901300430298
Validation loss: 2.1397762099901834

Epoch: 6| Step: 8
Training loss: 1.4251675605773926
Validation loss: 2.1305308739344277

Epoch: 6| Step: 9
Training loss: 1.8522851467132568
Validation loss: 2.1477617025375366

Epoch: 6| Step: 10
Training loss: 2.6638407707214355
Validation loss: 2.137245853741964

Epoch: 6| Step: 11
Training loss: 2.679387092590332
Validation loss: 2.1324645280838013

Epoch: 6| Step: 12
Training loss: 1.4262101650238037
Validation loss: 2.1340476075808206

Epoch: 6| Step: 13
Training loss: 1.5791500806808472
Validation loss: 2.1344971656799316

Epoch: 218| Step: 0
Training loss: 2.3378288745880127
Validation loss: 2.144838889439901

Epoch: 6| Step: 1
Training loss: 1.7030539512634277
Validation loss: 2.1436153451601663

Epoch: 6| Step: 2
Training loss: 2.6199469566345215
Validation loss: 2.145498832066854

Epoch: 6| Step: 3
Training loss: 1.6171047687530518
Validation loss: 2.1279104948043823

Epoch: 6| Step: 4
Training loss: 2.1373724937438965
Validation loss: 2.1424502531687417

Epoch: 6| Step: 5
Training loss: 1.8193999528884888
Validation loss: 2.1414282520612082

Epoch: 6| Step: 6
Training loss: 1.8293887376785278
Validation loss: 2.126250425974528

Epoch: 6| Step: 7
Training loss: 2.196350574493408
Validation loss: 2.142196754614512

Epoch: 6| Step: 8
Training loss: 1.5169845819473267
Validation loss: 2.14272807041804

Epoch: 6| Step: 9
Training loss: 2.0309088230133057
Validation loss: 2.1430481672286987

Epoch: 6| Step: 10
Training loss: 1.3396546840667725
Validation loss: 2.1267844438552856

Epoch: 6| Step: 11
Training loss: 2.0883841514587402
Validation loss: 2.1405850052833557

Epoch: 6| Step: 12
Training loss: 1.972585916519165
Validation loss: 2.147893746693929

Epoch: 6| Step: 13
Training loss: 1.876820683479309
Validation loss: 2.1370078921318054

Epoch: 219| Step: 0
Training loss: 2.3104140758514404
Validation loss: 2.1258426904678345

Epoch: 6| Step: 1
Training loss: 1.8792877197265625
Validation loss: 2.1485076347986856

Epoch: 6| Step: 2
Training loss: 2.088261365890503
Validation loss: 2.1252652208010354

Epoch: 6| Step: 3
Training loss: 2.142094612121582
Validation loss: 2.1350452502568564

Epoch: 6| Step: 4
Training loss: 1.8933240175247192
Validation loss: 2.137176215648651

Epoch: 6| Step: 5
Training loss: 1.4804906845092773
Validation loss: 2.1368377208709717

Epoch: 6| Step: 6
Training loss: 1.4427605867385864
Validation loss: 2.138846457004547

Epoch: 6| Step: 7
Training loss: 1.6654603481292725
Validation loss: 2.1471938490867615

Epoch: 6| Step: 8
Training loss: 2.114933490753174
Validation loss: 2.139689107735952

Epoch: 6| Step: 9
Training loss: 1.873849630355835
Validation loss: 2.1356321771939597

Epoch: 6| Step: 10
Training loss: 2.04233980178833
Validation loss: 2.1338181098302207

Epoch: 6| Step: 11
Training loss: 2.1520586013793945
Validation loss: 2.140718619028727

Epoch: 6| Step: 12
Training loss: 2.618116855621338
Validation loss: 2.126457850138346

Epoch: 6| Step: 13
Training loss: 1.5708012580871582
Validation loss: 2.135163446267446

Epoch: 220| Step: 0
Training loss: 2.056962013244629
Validation loss: 2.1248584191004434

Epoch: 6| Step: 1
Training loss: 1.7311224937438965
Validation loss: 2.1150052348772683

Epoch: 6| Step: 2
Training loss: 1.7570302486419678
Validation loss: 2.122080663839976

Epoch: 6| Step: 3
Training loss: 1.845754861831665
Validation loss: 2.122452517350515

Epoch: 6| Step: 4
Training loss: 1.9092433452606201
Validation loss: 2.1342190305391946

Epoch: 6| Step: 5
Training loss: 1.4306272268295288
Validation loss: 2.124247113863627

Epoch: 6| Step: 6
Training loss: 2.526564836502075
Validation loss: 2.1011143128077188

Epoch: 6| Step: 7
Training loss: 2.29461669921875
Validation loss: 2.1261876821517944

Epoch: 6| Step: 8
Training loss: 2.169752597808838
Validation loss: 2.133616586526235

Epoch: 6| Step: 9
Training loss: 1.6719334125518799
Validation loss: 2.1363921761512756

Epoch: 6| Step: 10
Training loss: 2.2713112831115723
Validation loss: 2.138874053955078

Epoch: 6| Step: 11
Training loss: 1.5183091163635254
Validation loss: 2.1402664383252463

Epoch: 6| Step: 12
Training loss: 1.711508870124817
Validation loss: 2.1239240169525146

Epoch: 6| Step: 13
Training loss: 2.1358540058135986
Validation loss: 2.1318318446477256

Epoch: 221| Step: 0
Training loss: 1.9779802560806274
Validation loss: 2.129940946896871

Epoch: 6| Step: 1
Training loss: 2.520677089691162
Validation loss: 2.136106808980306

Epoch: 6| Step: 2
Training loss: 1.4983634948730469
Validation loss: 2.1386004288991294

Epoch: 6| Step: 3
Training loss: 1.4033164978027344
Validation loss: 2.1133784651756287

Epoch: 6| Step: 4
Training loss: 2.0628509521484375
Validation loss: 2.1306979258855185

Epoch: 6| Step: 5
Training loss: 2.2696759700775146
Validation loss: 2.118929068247477

Epoch: 6| Step: 6
Training loss: 1.3838715553283691
Validation loss: 2.1192691127459207

Epoch: 6| Step: 7
Training loss: 1.9277976751327515
Validation loss: 2.1202187140782676

Epoch: 6| Step: 8
Training loss: 2.598323345184326
Validation loss: 2.112759828567505

Epoch: 6| Step: 9
Training loss: 2.501157760620117
Validation loss: 2.1280038356781006

Epoch: 6| Step: 10
Training loss: 1.4200050830841064
Validation loss: 2.122625748316447

Epoch: 6| Step: 11
Training loss: 1.4835214614868164
Validation loss: 2.1337735255559287

Epoch: 6| Step: 12
Training loss: 2.2588765621185303
Validation loss: 2.1342480977376304

Epoch: 6| Step: 13
Training loss: 1.892836570739746
Validation loss: 2.135419408480326

Epoch: 222| Step: 0
Training loss: 1.7360687255859375
Validation loss: 2.118770956993103

Epoch: 6| Step: 1
Training loss: 1.541897177696228
Validation loss: 2.1429390708605447

Epoch: 6| Step: 2
Training loss: 2.436950206756592
Validation loss: 2.1402637163798013

Epoch: 6| Step: 3
Training loss: 1.6161596775054932
Validation loss: 2.137272755304972

Epoch: 6| Step: 4
Training loss: 1.8787457942962646
Validation loss: 2.143980324268341

Epoch: 6| Step: 5
Training loss: 1.8970444202423096
Validation loss: 2.1379425724347434

Epoch: 6| Step: 6
Training loss: 2.357713222503662
Validation loss: 2.1335880359013877

Epoch: 6| Step: 7
Training loss: 1.8784958124160767
Validation loss: 2.1365169286727905

Epoch: 6| Step: 8
Training loss: 1.6198675632476807
Validation loss: 2.134663720925649

Epoch: 6| Step: 9
Training loss: 1.8883793354034424
Validation loss: 2.134015162785848

Epoch: 6| Step: 10
Training loss: 2.0020198822021484
Validation loss: 2.1434324582417807

Epoch: 6| Step: 11
Training loss: 2.126702070236206
Validation loss: 2.12512876590093

Epoch: 6| Step: 12
Training loss: 1.8052747249603271
Validation loss: 2.1278223792711892

Epoch: 6| Step: 13
Training loss: 2.169679641723633
Validation loss: 2.1363346576690674

Epoch: 223| Step: 0
Training loss: 1.5791577100753784
Validation loss: 2.1267473896344504

Epoch: 6| Step: 1
Training loss: 1.7429968118667603
Validation loss: 2.1338014801343284

Epoch: 6| Step: 2
Training loss: 1.7190005779266357
Validation loss: 2.148580332597097

Epoch: 6| Step: 3
Training loss: 2.2651665210723877
Validation loss: 2.1526255011558533

Epoch: 6| Step: 4
Training loss: 1.8006606101989746
Validation loss: 2.1403576135635376

Epoch: 6| Step: 5
Training loss: 1.8794279098510742
Validation loss: 2.1382060448328652

Epoch: 6| Step: 6
Training loss: 1.3971809148788452
Validation loss: 2.130533774693807

Epoch: 6| Step: 7
Training loss: 2.076077938079834
Validation loss: 2.1448691884676614

Epoch: 6| Step: 8
Training loss: 1.9335891008377075
Validation loss: 2.1271735429763794

Epoch: 6| Step: 9
Training loss: 1.9535514116287231
Validation loss: 2.1503363450368247

Epoch: 6| Step: 10
Training loss: 1.6885936260223389
Validation loss: 2.1431275606155396

Epoch: 6| Step: 11
Training loss: 1.5651001930236816
Validation loss: 2.1650041937828064

Epoch: 6| Step: 12
Training loss: 2.016874313354492
Validation loss: 2.147212247053782

Epoch: 6| Step: 13
Training loss: 3.08270263671875
Validation loss: 2.1414154370625815

Epoch: 224| Step: 0
Training loss: 1.742656946182251
Validation loss: 2.1396186550458274

Epoch: 6| Step: 1
Training loss: 2.0225584506988525
Validation loss: 2.129308025042216

Epoch: 6| Step: 2
Training loss: 1.5049591064453125
Validation loss: 2.1491058468818665

Epoch: 6| Step: 3
Training loss: 2.417311668395996
Validation loss: 2.1380547682444253

Epoch: 6| Step: 4
Training loss: 2.1921024322509766
Validation loss: 2.1488261818885803

Epoch: 6| Step: 5
Training loss: 1.93098783493042
Validation loss: 2.1574204762776694

Epoch: 6| Step: 6
Training loss: 2.1790175437927246
Validation loss: 2.1369464993476868

Epoch: 6| Step: 7
Training loss: 1.8141876459121704
Validation loss: 2.141233205795288

Epoch: 6| Step: 8
Training loss: 1.7046699523925781
Validation loss: 2.147401452064514

Epoch: 6| Step: 9
Training loss: 1.764613151550293
Validation loss: 2.1488196849823

Epoch: 6| Step: 10
Training loss: 1.9949254989624023
Validation loss: 2.14138917128245

Epoch: 6| Step: 11
Training loss: 1.8896468877792358
Validation loss: 2.153604745864868

Epoch: 6| Step: 12
Training loss: 1.2141509056091309
Validation loss: 2.164052426815033

Epoch: 6| Step: 13
Training loss: 2.4731101989746094
Validation loss: 2.141364653905233

Epoch: 225| Step: 0
Training loss: 1.6755363941192627
Validation loss: 2.1501776774724326

Epoch: 6| Step: 1
Training loss: 1.8619166612625122
Validation loss: 2.1462766329447427

Epoch: 6| Step: 2
Training loss: 1.4812935590744019
Validation loss: 2.138950745264689

Epoch: 6| Step: 3
Training loss: 1.4544346332550049
Validation loss: 2.151122589906057

Epoch: 6| Step: 4
Training loss: 1.8180772066116333
Validation loss: 2.137422581513723

Epoch: 6| Step: 5
Training loss: 2.672515869140625
Validation loss: 2.157650629679362

Epoch: 6| Step: 6
Training loss: 1.6366569995880127
Validation loss: 2.1542866627375283

Epoch: 6| Step: 7
Training loss: 1.9148478507995605
Validation loss: 2.1449994643529258

Epoch: 6| Step: 8
Training loss: 1.8665741682052612
Validation loss: 2.15086837609609

Epoch: 6| Step: 9
Training loss: 1.687534213066101
Validation loss: 2.157821615537008

Epoch: 6| Step: 10
Training loss: 1.8267567157745361
Validation loss: 2.140925168991089

Epoch: 6| Step: 11
Training loss: 2.5481534004211426
Validation loss: 2.1537978251775107

Epoch: 6| Step: 12
Training loss: 2.331739902496338
Validation loss: 2.1618232329686484

Epoch: 6| Step: 13
Training loss: 2.2020206451416016
Validation loss: 2.1670744021733603

Epoch: 226| Step: 0
Training loss: 2.3069896697998047
Validation loss: 2.1432130932807922

Epoch: 6| Step: 1
Training loss: 2.0336427688598633
Validation loss: 2.1445910930633545

Epoch: 6| Step: 2
Training loss: 2.2784337997436523
Validation loss: 2.140933036804199

Epoch: 6| Step: 3
Training loss: 1.9644663333892822
Validation loss: 2.1365694801012673

Epoch: 6| Step: 4
Training loss: 2.3344056606292725
Validation loss: 2.1403889656066895

Epoch: 6| Step: 5
Training loss: 1.9769867658615112
Validation loss: 2.1240514119466147

Epoch: 6| Step: 6
Training loss: 1.811462163925171
Validation loss: 2.121161858240763

Epoch: 6| Step: 7
Training loss: 1.752932071685791
Validation loss: 2.1146952708562217

Epoch: 6| Step: 8
Training loss: 1.597341537475586
Validation loss: 2.1345022718111673

Epoch: 6| Step: 9
Training loss: 2.266702651977539
Validation loss: 2.155862828095754

Epoch: 6| Step: 10
Training loss: 1.6690924167633057
Validation loss: 2.147621830304464

Epoch: 6| Step: 11
Training loss: 1.5419161319732666
Validation loss: 2.1494547923405967

Epoch: 6| Step: 12
Training loss: 1.4362740516662598
Validation loss: 2.136926531791687

Epoch: 6| Step: 13
Training loss: 2.500418186187744
Validation loss: 2.1362354159355164

Epoch: 227| Step: 0
Training loss: 2.332507848739624
Validation loss: 2.1298661828041077

Epoch: 6| Step: 1
Training loss: 1.8439265489578247
Validation loss: 2.14340603351593

Epoch: 6| Step: 2
Training loss: 2.503044843673706
Validation loss: 2.1354241569836936

Epoch: 6| Step: 3
Training loss: 1.6551499366760254
Validation loss: 2.1360283295313516

Epoch: 6| Step: 4
Training loss: 2.27518630027771
Validation loss: 2.1364030639330545

Epoch: 6| Step: 5
Training loss: 2.293437957763672
Validation loss: 2.1414939959843955

Epoch: 6| Step: 6
Training loss: 1.420318841934204
Validation loss: 2.131890833377838

Epoch: 6| Step: 7
Training loss: 1.3504443168640137
Validation loss: 2.1370604435602822

Epoch: 6| Step: 8
Training loss: 1.6732686758041382
Validation loss: 2.1343062122662864

Epoch: 6| Step: 9
Training loss: 1.9202537536621094
Validation loss: 2.14885147412618

Epoch: 6| Step: 10
Training loss: 2.2627108097076416
Validation loss: 2.124067743619283

Epoch: 6| Step: 11
Training loss: 2.1224327087402344
Validation loss: 2.1237112879753113

Epoch: 6| Step: 12
Training loss: 2.128509044647217
Validation loss: 2.1414934595425925

Epoch: 6| Step: 13
Training loss: 2.0131912231445312
Validation loss: 2.1282277504603067

Epoch: 228| Step: 0
Training loss: 1.8363282680511475
Validation loss: 2.131087044874827

Epoch: 6| Step: 1
Training loss: 1.7841551303863525
Validation loss: 2.1452924807866416

Epoch: 6| Step: 2
Training loss: 1.9963407516479492
Validation loss: 2.121200978755951

Epoch: 6| Step: 3
Training loss: 2.4101853370666504
Validation loss: 2.1320073207219443

Epoch: 6| Step: 4
Training loss: 1.6346096992492676
Validation loss: 2.117279330889384

Epoch: 6| Step: 5
Training loss: 1.7459903955459595
Validation loss: 2.1380436221758523

Epoch: 6| Step: 6
Training loss: 1.534156084060669
Validation loss: 2.14313006401062

Epoch: 6| Step: 7
Training loss: 1.6424800157546997
Validation loss: 2.132640461126963

Epoch: 6| Step: 8
Training loss: 1.4825106859207153
Validation loss: 2.1506680051485696

Epoch: 6| Step: 9
Training loss: 2.378762722015381
Validation loss: 2.149547000726064

Epoch: 6| Step: 10
Training loss: 2.1626193523406982
Validation loss: 2.144769231478373

Epoch: 6| Step: 11
Training loss: 1.980978012084961
Validation loss: 2.151067018508911

Epoch: 6| Step: 12
Training loss: 2.131955862045288
Validation loss: 2.147066652774811

Epoch: 6| Step: 13
Training loss: 2.2190136909484863
Validation loss: 2.152287761370341

Epoch: 229| Step: 0
Training loss: 1.2875611782073975
Validation loss: 2.1443122824033103

Epoch: 6| Step: 1
Training loss: 1.4214509725570679
Validation loss: 2.130527456601461

Epoch: 6| Step: 2
Training loss: 1.7095439434051514
Validation loss: 2.1275013089179993

Epoch: 6| Step: 3
Training loss: 2.854418992996216
Validation loss: 2.134127378463745

Epoch: 6| Step: 4
Training loss: 1.6027817726135254
Validation loss: 2.1285753647486367

Epoch: 6| Step: 5
Training loss: 1.498248815536499
Validation loss: 2.147708555062612

Epoch: 6| Step: 6
Training loss: 1.5663520097732544
Validation loss: 2.166175206502279

Epoch: 6| Step: 7
Training loss: 2.738539695739746
Validation loss: 2.1406116485595703

Epoch: 6| Step: 8
Training loss: 2.0701985359191895
Validation loss: 2.1617369055747986

Epoch: 6| Step: 9
Training loss: 2.5614233016967773
Validation loss: 2.14144374926885

Epoch: 6| Step: 10
Training loss: 1.6221345663070679
Validation loss: 2.1499062379201255

Epoch: 6| Step: 11
Training loss: 2.0994701385498047
Validation loss: 2.1608488957087197

Epoch: 6| Step: 12
Training loss: 1.7675997018814087
Validation loss: 2.1412005027135215

Epoch: 6| Step: 13
Training loss: 1.7696483135223389
Validation loss: 2.1447161038716636

Epoch: 230| Step: 0
Training loss: 1.9533168077468872
Validation loss: 2.153496503829956

Epoch: 6| Step: 1
Training loss: 1.3710659742355347
Validation loss: 2.1313048005104065

Epoch: 6| Step: 2
Training loss: 2.1872572898864746
Validation loss: 2.137579838434855

Epoch: 6| Step: 3
Training loss: 2.1047840118408203
Validation loss: 2.1419084469477334

Epoch: 6| Step: 4
Training loss: 1.6332464218139648
Validation loss: 2.1394304434458413

Epoch: 6| Step: 5
Training loss: 1.6655020713806152
Validation loss: 2.1559128562609353

Epoch: 6| Step: 6
Training loss: 2.045212984085083
Validation loss: 2.144700745741526

Epoch: 6| Step: 7
Training loss: 2.4620742797851562
Validation loss: 2.1460154255231223

Epoch: 6| Step: 8
Training loss: 1.4976894855499268
Validation loss: 2.166399876276652

Epoch: 6| Step: 9
Training loss: 2.060048818588257
Validation loss: 2.153409699598948

Epoch: 6| Step: 10
Training loss: 2.0138421058654785
Validation loss: 2.1520779530207315

Epoch: 6| Step: 11
Training loss: 2.185110569000244
Validation loss: 2.1412673791249595

Epoch: 6| Step: 12
Training loss: 1.6902776956558228
Validation loss: 2.1736185948053994

Epoch: 6| Step: 13
Training loss: 2.1659276485443115
Validation loss: 2.1508901516596475

Epoch: 231| Step: 0
Training loss: 1.4845895767211914
Validation loss: 2.1517514189084372

Epoch: 6| Step: 1
Training loss: 1.811699390411377
Validation loss: 2.1654653747876487

Epoch: 6| Step: 2
Training loss: 1.7037886381149292
Validation loss: 2.1596127351125083

Epoch: 6| Step: 3
Training loss: 2.4679272174835205
Validation loss: 2.1635821064313254

Epoch: 6| Step: 4
Training loss: 2.251671314239502
Validation loss: 2.165609339872996

Epoch: 6| Step: 5
Training loss: 2.0508780479431152
Validation loss: 2.1622940500577292

Epoch: 6| Step: 6
Training loss: 2.319972276687622
Validation loss: 2.1635513504346213

Epoch: 6| Step: 7
Training loss: 2.038437843322754
Validation loss: 2.166927675406138

Epoch: 6| Step: 8
Training loss: 1.959388017654419
Validation loss: 2.173078457514445

Epoch: 6| Step: 9
Training loss: 1.3091727495193481
Validation loss: 2.1477463245391846

Epoch: 6| Step: 10
Training loss: 2.0631208419799805
Validation loss: 2.1681870222091675

Epoch: 6| Step: 11
Training loss: 1.173761248588562
Validation loss: 2.154074192047119

Epoch: 6| Step: 12
Training loss: 2.3305904865264893
Validation loss: 2.143544773260752

Epoch: 6| Step: 13
Training loss: 1.6503866910934448
Validation loss: 2.1460148692131042

Epoch: 232| Step: 0
Training loss: 1.5558998584747314
Validation loss: 2.1550409396489463

Epoch: 6| Step: 1
Training loss: 1.8911073207855225
Validation loss: 2.1491480271021524

Epoch: 6| Step: 2
Training loss: 1.535670280456543
Validation loss: 2.1537466843922934

Epoch: 6| Step: 3
Training loss: 2.66544771194458
Validation loss: 2.1698999404907227

Epoch: 6| Step: 4
Training loss: 2.324162483215332
Validation loss: 2.1573566595713296

Epoch: 6| Step: 5
Training loss: 1.6251810789108276
Validation loss: 2.1491540670394897

Epoch: 6| Step: 6
Training loss: 2.264085054397583
Validation loss: 2.136716882387797

Epoch: 6| Step: 7
Training loss: 2.283949851989746
Validation loss: 2.151306390762329

Epoch: 6| Step: 8
Training loss: 1.5679352283477783
Validation loss: 2.151990592479706

Epoch: 6| Step: 9
Training loss: 1.8831082582473755
Validation loss: 2.131461799144745

Epoch: 6| Step: 10
Training loss: 1.9006513357162476
Validation loss: 2.165632506211599

Epoch: 6| Step: 11
Training loss: 1.6093394756317139
Validation loss: 2.159003575642904

Epoch: 6| Step: 12
Training loss: 1.7836488485336304
Validation loss: 2.16433318456014

Epoch: 6| Step: 13
Training loss: 1.7480164766311646
Validation loss: 2.1605073610941568

Epoch: 233| Step: 0
Training loss: 1.3891661167144775
Validation loss: 2.141057312488556

Epoch: 6| Step: 1
Training loss: 2.0708694458007812
Validation loss: 2.15405003229777

Epoch: 6| Step: 2
Training loss: 1.6316609382629395
Validation loss: 2.1617319186528525

Epoch: 6| Step: 3
Training loss: 1.7701715230941772
Validation loss: 2.156183381875356

Epoch: 6| Step: 4
Training loss: 1.9608968496322632
Validation loss: 2.153453608353933

Epoch: 6| Step: 5
Training loss: 1.8424233198165894
Validation loss: 2.1653921405474343

Epoch: 6| Step: 6
Training loss: 2.2629005908966064
Validation loss: 2.16652778784434

Epoch: 6| Step: 7
Training loss: 1.6396901607513428
Validation loss: 2.152186612288157

Epoch: 6| Step: 8
Training loss: 1.897472620010376
Validation loss: 2.1431206663449607

Epoch: 6| Step: 9
Training loss: 2.7561419010162354
Validation loss: 2.1395763158798218

Epoch: 6| Step: 10
Training loss: 1.1341900825500488
Validation loss: 2.1584418416023254

Epoch: 6| Step: 11
Training loss: 2.0910792350769043
Validation loss: 2.1346309582392373

Epoch: 6| Step: 12
Training loss: 2.5359785556793213
Validation loss: 2.154127836227417

Epoch: 6| Step: 13
Training loss: 1.5722460746765137
Validation loss: 2.143834630648295

Epoch: 234| Step: 0
Training loss: 1.8461347818374634
Validation loss: 2.1419397393862405

Epoch: 6| Step: 1
Training loss: 2.0007379055023193
Validation loss: 2.133979241053263

Epoch: 6| Step: 2
Training loss: 1.7936943769454956
Validation loss: 2.1432154178619385

Epoch: 6| Step: 3
Training loss: 2.332401752471924
Validation loss: 2.1597628196080527

Epoch: 6| Step: 4
Training loss: 2.058520793914795
Validation loss: 2.1472947796185813

Epoch: 6| Step: 5
Training loss: 1.6830425262451172
Validation loss: 2.162576675415039

Epoch: 6| Step: 6
Training loss: 2.191455125808716
Validation loss: 2.1716414292653403

Epoch: 6| Step: 7
Training loss: 2.5928988456726074
Validation loss: 2.173121988773346

Epoch: 6| Step: 8
Training loss: 1.8392168283462524
Validation loss: 2.1748634378115335

Epoch: 6| Step: 9
Training loss: 1.4361274242401123
Validation loss: 2.170801103115082

Epoch: 6| Step: 10
Training loss: 1.4997807741165161
Validation loss: 2.171284278233846

Epoch: 6| Step: 11
Training loss: 2.0087103843688965
Validation loss: 2.167605459690094

Epoch: 6| Step: 12
Training loss: 1.8318253755569458
Validation loss: 2.163796285788218

Epoch: 6| Step: 13
Training loss: 1.6531803607940674
Validation loss: 2.17352831363678

Epoch: 235| Step: 0
Training loss: 1.847847819328308
Validation loss: 2.173473040262858

Epoch: 6| Step: 1
Training loss: 2.1421308517456055
Validation loss: 2.1800014774004617

Epoch: 6| Step: 2
Training loss: 2.2876102924346924
Validation loss: 2.1768160661061606

Epoch: 6| Step: 3
Training loss: 1.8575952053070068
Validation loss: 2.159977753957113

Epoch: 6| Step: 4
Training loss: 2.1158580780029297
Validation loss: 2.152922511100769

Epoch: 6| Step: 5
Training loss: 1.7126203775405884
Validation loss: 2.1636578838030496

Epoch: 6| Step: 6
Training loss: 1.7732295989990234
Validation loss: 2.172016183535258

Epoch: 6| Step: 7
Training loss: 1.6941618919372559
Validation loss: 2.153736412525177

Epoch: 6| Step: 8
Training loss: 1.6611793041229248
Validation loss: 2.160820464293162

Epoch: 6| Step: 9
Training loss: 2.396019220352173
Validation loss: 2.152906358242035

Epoch: 6| Step: 10
Training loss: 1.815297245979309
Validation loss: 2.1589815417925515

Epoch: 6| Step: 11
Training loss: 1.9390881061553955
Validation loss: 2.1705853740374246

Epoch: 6| Step: 12
Training loss: 1.3489038944244385
Validation loss: 2.1578152974446616

Epoch: 6| Step: 13
Training loss: 1.8848215341567993
Validation loss: 2.177342414855957

Epoch: 236| Step: 0
Training loss: 1.925668478012085
Validation loss: 2.1745503346125283

Epoch: 6| Step: 1
Training loss: 1.4605650901794434
Validation loss: 2.1598487297693887

Epoch: 6| Step: 2
Training loss: 2.2239949703216553
Validation loss: 2.156571904818217

Epoch: 6| Step: 3
Training loss: 1.8561376333236694
Validation loss: 2.158106724421183

Epoch: 6| Step: 4
Training loss: 1.719274878501892
Validation loss: 2.160655955473582

Epoch: 6| Step: 5
Training loss: 2.0513954162597656
Validation loss: 2.1786231795946756

Epoch: 6| Step: 6
Training loss: 1.9094500541687012
Validation loss: 2.167813499768575

Epoch: 6| Step: 7
Training loss: 1.6747732162475586
Validation loss: 2.1606208880742392

Epoch: 6| Step: 8
Training loss: 1.6748135089874268
Validation loss: 2.1786680221557617

Epoch: 6| Step: 9
Training loss: 2.194166660308838
Validation loss: 2.177007555961609

Epoch: 6| Step: 10
Training loss: 2.228322982788086
Validation loss: 2.1715879440307617

Epoch: 6| Step: 11
Training loss: 2.0705528259277344
Validation loss: 2.1526376605033875

Epoch: 6| Step: 12
Training loss: 1.543660283088684
Validation loss: 2.1704044938087463

Epoch: 6| Step: 13
Training loss: 2.3331141471862793
Validation loss: 2.1702586809794107

Epoch: 237| Step: 0
Training loss: 2.2352538108825684
Validation loss: 2.163113534450531

Epoch: 6| Step: 1
Training loss: 1.7972936630249023
Validation loss: 2.1499144037564597

Epoch: 6| Step: 2
Training loss: 1.513431191444397
Validation loss: 2.1437845826148987

Epoch: 6| Step: 3
Training loss: 1.3706021308898926
Validation loss: 2.160529534022013

Epoch: 6| Step: 4
Training loss: 2.207526922225952
Validation loss: 2.1415780186653137

Epoch: 6| Step: 5
Training loss: 1.9278367757797241
Validation loss: 2.1411532759666443

Epoch: 6| Step: 6
Training loss: 1.8410398960113525
Validation loss: 2.156599462032318

Epoch: 6| Step: 7
Training loss: 2.1453468799591064
Validation loss: 2.148886958758036

Epoch: 6| Step: 8
Training loss: 2.339665651321411
Validation loss: 2.165973981221517

Epoch: 6| Step: 9
Training loss: 1.2753515243530273
Validation loss: 2.1624900301297507

Epoch: 6| Step: 10
Training loss: 2.1880364418029785
Validation loss: 2.1584041317303977

Epoch: 6| Step: 11
Training loss: 1.8850340843200684
Validation loss: 2.1569309631983438

Epoch: 6| Step: 12
Training loss: 1.7235580682754517
Validation loss: 2.159640928109487

Epoch: 6| Step: 13
Training loss: 1.777040719985962
Validation loss: 2.166948914527893

Epoch: 238| Step: 0
Training loss: 1.6000925302505493
Validation loss: 2.1591634353001914

Epoch: 6| Step: 1
Training loss: 1.6973026990890503
Validation loss: 2.1629525621732077

Epoch: 6| Step: 2
Training loss: 2.1229214668273926
Validation loss: 2.151349147160848

Epoch: 6| Step: 3
Training loss: 2.1102547645568848
Validation loss: 2.1693390806516013

Epoch: 6| Step: 4
Training loss: 2.0695064067840576
Validation loss: 2.1503853599230447

Epoch: 6| Step: 5
Training loss: 2.3613390922546387
Validation loss: 2.150909105936686

Epoch: 6| Step: 6
Training loss: 2.6667380332946777
Validation loss: 2.1595205068588257

Epoch: 6| Step: 7
Training loss: 1.534440040588379
Validation loss: 2.161549150943756

Epoch: 6| Step: 8
Training loss: 1.9784290790557861
Validation loss: 2.1444541215896606

Epoch: 6| Step: 9
Training loss: 2.049830913543701
Validation loss: 2.1615554094314575

Epoch: 6| Step: 10
Training loss: 1.312835931777954
Validation loss: 2.164182702700297

Epoch: 6| Step: 11
Training loss: 1.852799892425537
Validation loss: 2.160434345404307

Epoch: 6| Step: 12
Training loss: 1.9817363023757935
Validation loss: 2.167313357194265

Epoch: 6| Step: 13
Training loss: 1.6977112293243408
Validation loss: 2.164645195007324

Epoch: 239| Step: 0
Training loss: 2.436065673828125
Validation loss: 2.1543408830960593

Epoch: 6| Step: 1
Training loss: 1.655783772468567
Validation loss: 2.1550334095954895

Epoch: 6| Step: 2
Training loss: 1.902146816253662
Validation loss: 2.1507241129875183

Epoch: 6| Step: 3
Training loss: 2.457211494445801
Validation loss: 2.156430701414744

Epoch: 6| Step: 4
Training loss: 2.0106747150421143
Validation loss: 2.1614757776260376

Epoch: 6| Step: 5
Training loss: 1.3372459411621094
Validation loss: 2.1576254765192666

Epoch: 6| Step: 6
Training loss: 1.6139910221099854
Validation loss: 2.172719875971476

Epoch: 6| Step: 7
Training loss: 2.1468024253845215
Validation loss: 2.1604295571645102

Epoch: 6| Step: 8
Training loss: 1.6787943840026855
Validation loss: 2.1717549363772073

Epoch: 6| Step: 9
Training loss: 1.6258580684661865
Validation loss: 2.151829481124878

Epoch: 6| Step: 10
Training loss: 1.8672126531600952
Validation loss: 2.1715368032455444

Epoch: 6| Step: 11
Training loss: 1.25828218460083
Validation loss: 2.1570724844932556

Epoch: 6| Step: 12
Training loss: 2.1031808853149414
Validation loss: 2.1551403999328613

Epoch: 6| Step: 13
Training loss: 2.616105794906616
Validation loss: 2.16618545850118

Epoch: 240| Step: 0
Training loss: 2.3643109798431396
Validation loss: 2.1848745942115784

Epoch: 6| Step: 1
Training loss: 2.154895782470703
Validation loss: 2.166296362876892

Epoch: 6| Step: 2
Training loss: 1.7923595905303955
Validation loss: 2.169048488140106

Epoch: 6| Step: 3
Training loss: 2.530208110809326
Validation loss: 2.1580885648727417

Epoch: 6| Step: 4
Training loss: 1.6755127906799316
Validation loss: 2.156088431676229

Epoch: 6| Step: 5
Training loss: 1.7260380983352661
Validation loss: 2.1621948877970376

Epoch: 6| Step: 6
Training loss: 1.919071912765503
Validation loss: 2.1683965722719827

Epoch: 6| Step: 7
Training loss: 1.7743771076202393
Validation loss: 2.171539564927419

Epoch: 6| Step: 8
Training loss: 1.7901654243469238
Validation loss: 2.1685317754745483

Epoch: 6| Step: 9
Training loss: 2.076690196990967
Validation loss: 2.1652535796165466

Epoch: 6| Step: 10
Training loss: 1.1611580848693848
Validation loss: 2.161323924859365

Epoch: 6| Step: 11
Training loss: 2.0051321983337402
Validation loss: 2.163801650206248

Epoch: 6| Step: 12
Training loss: 1.5949392318725586
Validation loss: 2.151053170363108

Epoch: 6| Step: 13
Training loss: 1.8265464305877686
Validation loss: 2.1535322268803916

Epoch: 241| Step: 0
Training loss: 1.7732139825820923
Validation loss: 2.16812264919281

Epoch: 6| Step: 1
Training loss: 1.805397629737854
Validation loss: 2.15550434589386

Epoch: 6| Step: 2
Training loss: 1.6718353033065796
Validation loss: 2.163723031679789

Epoch: 6| Step: 3
Training loss: 1.8462364673614502
Validation loss: 2.1524455746014914

Epoch: 6| Step: 4
Training loss: 2.1694064140319824
Validation loss: 2.1703476707140603

Epoch: 6| Step: 5
Training loss: 1.5385549068450928
Validation loss: 2.1638715267181396

Epoch: 6| Step: 6
Training loss: 2.387578010559082
Validation loss: 2.1630507906277976

Epoch: 6| Step: 7
Training loss: 2.1328318119049072
Validation loss: 2.1572275161743164

Epoch: 6| Step: 8
Training loss: 1.627338171005249
Validation loss: 2.1688122749328613

Epoch: 6| Step: 9
Training loss: 1.8991774320602417
Validation loss: 2.158604164918264

Epoch: 6| Step: 10
Training loss: 1.8344943523406982
Validation loss: 2.1659191449483237

Epoch: 6| Step: 11
Training loss: 2.0028433799743652
Validation loss: 2.1615558664004006

Epoch: 6| Step: 12
Training loss: 1.3818339109420776
Validation loss: 2.180347204208374

Epoch: 6| Step: 13
Training loss: 2.0480222702026367
Validation loss: 2.1665154695510864

Epoch: 242| Step: 0
Training loss: 2.3504858016967773
Validation loss: 2.176965574423472

Epoch: 6| Step: 1
Training loss: 1.6846591234207153
Validation loss: 2.1642948985099792

Epoch: 6| Step: 2
Training loss: 2.140726089477539
Validation loss: 2.1637787024180093

Epoch: 6| Step: 3
Training loss: 2.5641286373138428
Validation loss: 2.144809603691101

Epoch: 6| Step: 4
Training loss: 2.013216972351074
Validation loss: 2.1976024508476257

Epoch: 6| Step: 5
Training loss: 1.4784733057022095
Validation loss: 2.183120012283325

Epoch: 6| Step: 6
Training loss: 1.458511471748352
Validation loss: 2.174793084462484

Epoch: 6| Step: 7
Training loss: 2.3200905323028564
Validation loss: 2.1633427143096924

Epoch: 6| Step: 8
Training loss: 1.411360502243042
Validation loss: 2.1793585419654846

Epoch: 6| Step: 9
Training loss: 1.9219770431518555
Validation loss: 2.186426520347595

Epoch: 6| Step: 10
Training loss: 1.4684451818466187
Validation loss: 2.1762704451878867

Epoch: 6| Step: 11
Training loss: 2.19337797164917
Validation loss: 2.1620808839797974

Epoch: 6| Step: 12
Training loss: 1.2205578088760376
Validation loss: 2.1709904273351035

Epoch: 6| Step: 13
Training loss: 1.9446327686309814
Validation loss: 2.1749528845151267

Epoch: 243| Step: 0
Training loss: 2.266427993774414
Validation loss: 2.1693727175394693

Epoch: 6| Step: 1
Training loss: 1.4761698246002197
Validation loss: 2.1496448119481406

Epoch: 6| Step: 2
Training loss: 1.993627667427063
Validation loss: 2.157364070415497

Epoch: 6| Step: 3
Training loss: 2.6300740242004395
Validation loss: 2.177578310171763

Epoch: 6| Step: 4
Training loss: 1.9825654029846191
Validation loss: 2.164756695429484

Epoch: 6| Step: 5
Training loss: 1.476741075515747
Validation loss: 2.154205600420634

Epoch: 6| Step: 6
Training loss: 1.5921672582626343
Validation loss: 2.1765329837799072

Epoch: 6| Step: 7
Training loss: 2.0393500328063965
Validation loss: 2.175332029660543

Epoch: 6| Step: 8
Training loss: 2.220712423324585
Validation loss: 2.182039817174276

Epoch: 6| Step: 9
Training loss: 1.2700053453445435
Validation loss: 2.173830231030782

Epoch: 6| Step: 10
Training loss: 1.4743082523345947
Validation loss: 2.159226894378662

Epoch: 6| Step: 11
Training loss: 1.3174149990081787
Validation loss: 2.180192510286967

Epoch: 6| Step: 12
Training loss: 1.6720340251922607
Validation loss: 2.1896026134490967

Epoch: 6| Step: 13
Training loss: 2.808851718902588
Validation loss: 2.184032956759135

Epoch: 244| Step: 0
Training loss: 1.8202794790267944
Validation loss: 2.17718638976415

Epoch: 6| Step: 1
Training loss: 1.669478416442871
Validation loss: 2.177968601385752

Epoch: 6| Step: 2
Training loss: 1.9929170608520508
Validation loss: 2.1769563357035318

Epoch: 6| Step: 3
Training loss: 1.3607418537139893
Validation loss: 2.1814016699790955

Epoch: 6| Step: 4
Training loss: 1.9658037424087524
Validation loss: 2.174456079800924

Epoch: 6| Step: 5
Training loss: 2.0363829135894775
Validation loss: 2.170971949895223

Epoch: 6| Step: 6
Training loss: 1.7108383178710938
Validation loss: 2.1848746140797934

Epoch: 6| Step: 7
Training loss: 1.660981297492981
Validation loss: 2.1733764012654624

Epoch: 6| Step: 8
Training loss: 2.151893377304077
Validation loss: 2.156274219353994

Epoch: 6| Step: 9
Training loss: 1.6507930755615234
Validation loss: 2.158026913801829

Epoch: 6| Step: 10
Training loss: 1.7474673986434937
Validation loss: 2.176754117012024

Epoch: 6| Step: 11
Training loss: 1.7134336233139038
Validation loss: 2.165620446205139

Epoch: 6| Step: 12
Training loss: 2.53184175491333
Validation loss: 2.1518244544665017

Epoch: 6| Step: 13
Training loss: 2.1198368072509766
Validation loss: 2.1453720132509866

Epoch: 245| Step: 0
Training loss: 1.9608756303787231
Validation loss: 2.1504632035891214

Epoch: 6| Step: 1
Training loss: 2.1895692348480225
Validation loss: 2.157250761985779

Epoch: 6| Step: 2
Training loss: 2.1870157718658447
Validation loss: 2.1595815817515054

Epoch: 6| Step: 3
Training loss: 2.784900188446045
Validation loss: 2.170553962389628

Epoch: 6| Step: 4
Training loss: 1.3565378189086914
Validation loss: 2.1714895168940225

Epoch: 6| Step: 5
Training loss: 1.0923538208007812
Validation loss: 2.183439294497172

Epoch: 6| Step: 6
Training loss: 1.8777058124542236
Validation loss: 2.169826944669088

Epoch: 6| Step: 7
Training loss: 2.122166156768799
Validation loss: 2.1841323574384055

Epoch: 6| Step: 8
Training loss: 1.6986654996871948
Validation loss: 2.178711235523224

Epoch: 6| Step: 9
Training loss: 1.8887579441070557
Validation loss: 2.18068661292394

Epoch: 6| Step: 10
Training loss: 1.9425913095474243
Validation loss: 2.1838361024856567

Epoch: 6| Step: 11
Training loss: 1.9851343631744385
Validation loss: 2.177623450756073

Epoch: 6| Step: 12
Training loss: 1.391588568687439
Validation loss: 2.1654081543286643

Epoch: 6| Step: 13
Training loss: 1.7045475244522095
Validation loss: 2.1579059958457947

Epoch: 246| Step: 0
Training loss: 1.501800537109375
Validation loss: 2.1854217052459717

Epoch: 6| Step: 1
Training loss: 2.168459415435791
Validation loss: 2.160595635573069

Epoch: 6| Step: 2
Training loss: 1.550316572189331
Validation loss: 2.1746668020884194

Epoch: 6| Step: 3
Training loss: 2.0351526737213135
Validation loss: 2.171572506427765

Epoch: 6| Step: 4
Training loss: 1.3829805850982666
Validation loss: 2.17602147658666

Epoch: 6| Step: 5
Training loss: 2.2087972164154053
Validation loss: 2.1877382596333823

Epoch: 6| Step: 6
Training loss: 1.6579291820526123
Validation loss: 2.1738792260487876

Epoch: 6| Step: 7
Training loss: 2.397395133972168
Validation loss: 2.187139650185903

Epoch: 6| Step: 8
Training loss: 1.5763487815856934
Validation loss: 2.1833873788515725

Epoch: 6| Step: 9
Training loss: 2.071763753890991
Validation loss: 2.173918922742208

Epoch: 6| Step: 10
Training loss: 1.6696209907531738
Validation loss: 2.1919950445493064

Epoch: 6| Step: 11
Training loss: 1.985370397567749
Validation loss: 2.1830522815386453

Epoch: 6| Step: 12
Training loss: 2.16251802444458
Validation loss: 2.170384486516317

Epoch: 6| Step: 13
Training loss: 1.8829158544540405
Validation loss: 2.1860187649726868

Epoch: 247| Step: 0
Training loss: 1.3508806228637695
Validation loss: 2.1741455594698587

Epoch: 6| Step: 1
Training loss: 1.4594011306762695
Validation loss: 2.165511508782705

Epoch: 6| Step: 2
Training loss: 1.7158358097076416
Validation loss: 2.1728652119636536

Epoch: 6| Step: 3
Training loss: 2.261951446533203
Validation loss: 2.1637855569521585

Epoch: 6| Step: 4
Training loss: 1.6289153099060059
Validation loss: 2.1788546244303384

Epoch: 6| Step: 5
Training loss: 1.973250150680542
Validation loss: 2.1799845894177756

Epoch: 6| Step: 6
Training loss: 1.5403876304626465
Validation loss: 2.1789121627807617

Epoch: 6| Step: 7
Training loss: 2.044497013092041
Validation loss: 2.1988165378570557

Epoch: 6| Step: 8
Training loss: 1.9249205589294434
Validation loss: 2.1957196791966758

Epoch: 6| Step: 9
Training loss: 2.563138484954834
Validation loss: 2.185547629992167

Epoch: 6| Step: 10
Training loss: 2.0609779357910156
Validation loss: 2.19518651564916

Epoch: 6| Step: 11
Training loss: 2.5815443992614746
Validation loss: 2.1896063486735025

Epoch: 6| Step: 12
Training loss: 1.7873286008834839
Validation loss: 2.1757070620854697

Epoch: 6| Step: 13
Training loss: 1.497370958328247
Validation loss: 2.193576773007711

Epoch: 248| Step: 0
Training loss: 1.1095173358917236
Validation loss: 2.1798293391863504

Epoch: 6| Step: 1
Training loss: 2.2335879802703857
Validation loss: 2.1838228702545166

Epoch: 6| Step: 2
Training loss: 1.8580577373504639
Validation loss: 2.165801783402761

Epoch: 6| Step: 3
Training loss: 1.899318814277649
Validation loss: 2.1598224441210427

Epoch: 6| Step: 4
Training loss: 2.44216251373291
Validation loss: 2.143791158994039

Epoch: 6| Step: 5
Training loss: 1.7848219871520996
Validation loss: 2.163461764653524

Epoch: 6| Step: 6
Training loss: 2.0912065505981445
Validation loss: 2.1558993260065713

Epoch: 6| Step: 7
Training loss: 2.079517364501953
Validation loss: 2.1700129906336465

Epoch: 6| Step: 8
Training loss: 1.1027190685272217
Validation loss: 2.1590994596481323

Epoch: 6| Step: 9
Training loss: 1.9415924549102783
Validation loss: 2.172067701816559

Epoch: 6| Step: 10
Training loss: 2.2632198333740234
Validation loss: 2.20372341076533

Epoch: 6| Step: 11
Training loss: 1.5927448272705078
Validation loss: 2.1875847379366555

Epoch: 6| Step: 12
Training loss: 1.3856927156448364
Validation loss: 2.2072431643803916

Epoch: 6| Step: 13
Training loss: 2.2143232822418213
Validation loss: 2.1784698168436685

Epoch: 249| Step: 0
Training loss: 1.7176618576049805
Validation loss: 2.173179507255554

Epoch: 6| Step: 1
Training loss: 2.1266884803771973
Validation loss: 2.1888699531555176

Epoch: 6| Step: 2
Training loss: 1.22519850730896
Validation loss: 2.186988572279612

Epoch: 6| Step: 3
Training loss: 2.2945563793182373
Validation loss: 2.191761553287506

Epoch: 6| Step: 4
Training loss: 1.5458273887634277
Validation loss: 2.192370295524597

Epoch: 6| Step: 5
Training loss: 1.4739933013916016
Validation loss: 2.1806019147237143

Epoch: 6| Step: 6
Training loss: 1.2652877569198608
Validation loss: 2.1805529395739236

Epoch: 6| Step: 7
Training loss: 1.553678274154663
Validation loss: 2.1886998812357583

Epoch: 6| Step: 8
Training loss: 2.489283800125122
Validation loss: 2.188576579093933

Epoch: 6| Step: 9
Training loss: 2.0234336853027344
Validation loss: 2.2066245079040527

Epoch: 6| Step: 10
Training loss: 2.1801552772521973
Validation loss: 2.1840473810831704

Epoch: 6| Step: 11
Training loss: 1.662656307220459
Validation loss: 2.190459052721659

Epoch: 6| Step: 12
Training loss: 2.369936943054199
Validation loss: 2.178940792878469

Epoch: 6| Step: 13
Training loss: 2.243016242980957
Validation loss: 2.1977978150049844

Epoch: 250| Step: 0
Training loss: 2.5421345233917236
Validation loss: 2.1725412209828696

Epoch: 6| Step: 1
Training loss: 2.211941719055176
Validation loss: 2.1880091031392417

Epoch: 6| Step: 2
Training loss: 1.8899813890457153
Validation loss: 2.1823810736338296

Epoch: 6| Step: 3
Training loss: 1.9732120037078857
Validation loss: 2.1821356813112893

Epoch: 6| Step: 4
Training loss: 1.6840691566467285
Validation loss: 2.197578946749369

Epoch: 6| Step: 5
Training loss: 1.8209657669067383
Validation loss: 2.19006884098053

Epoch: 6| Step: 6
Training loss: 1.600664734840393
Validation loss: 2.199458916982015

Epoch: 6| Step: 7
Training loss: 1.6304233074188232
Validation loss: 2.1876072684923806

Epoch: 6| Step: 8
Training loss: 1.4681181907653809
Validation loss: 2.204187750816345

Epoch: 6| Step: 9
Training loss: 1.6762877702713013
Validation loss: 2.203941603501638

Epoch: 6| Step: 10
Training loss: 2.2838339805603027
Validation loss: 2.1974172790845237

Epoch: 6| Step: 11
Training loss: 1.596069097518921
Validation loss: 2.190870722134908

Epoch: 6| Step: 12
Training loss: 1.8083661794662476
Validation loss: 2.1679336627324424

Epoch: 6| Step: 13
Training loss: 1.7884398698806763
Validation loss: 2.1888977686564126

Testing loss: 1.7577432745652233

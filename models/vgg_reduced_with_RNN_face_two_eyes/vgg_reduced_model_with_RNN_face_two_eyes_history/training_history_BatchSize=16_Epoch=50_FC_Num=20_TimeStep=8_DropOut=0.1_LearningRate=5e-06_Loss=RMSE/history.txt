Epoch: 1| Step: 0
Training loss: 5.879964475978535
Validation loss: 5.932234212604403

Epoch: 6| Step: 1
Training loss: 6.249837644375627
Validation loss: 5.931039233092359

Epoch: 6| Step: 2
Training loss: 6.356311406952026
Validation loss: 5.929748668866126

Epoch: 6| Step: 3
Training loss: 4.777408028620104
Validation loss: 5.928479298977174

Epoch: 6| Step: 4
Training loss: 5.670235052602124
Validation loss: 5.927208262793483

Epoch: 6| Step: 5
Training loss: 6.4210249066679985
Validation loss: 5.925945858899783

Epoch: 6| Step: 6
Training loss: 5.234300299723502
Validation loss: 5.924672535406023

Epoch: 6| Step: 7
Training loss: 6.857846031011384
Validation loss: 5.923347846925579

Epoch: 6| Step: 8
Training loss: 6.234814009351135
Validation loss: 5.922036443034558

Epoch: 6| Step: 9
Training loss: 6.329130417814372
Validation loss: 5.920671781966311

Epoch: 6| Step: 10
Training loss: 6.471494525536043
Validation loss: 5.91922906899197

Epoch: 6| Step: 11
Training loss: 5.673212571493127
Validation loss: 5.917815065644054

Epoch: 6| Step: 12
Training loss: 6.115136749160848
Validation loss: 5.916377217192881

Epoch: 6| Step: 13
Training loss: 5.980120468712357
Validation loss: 5.9148418227151325

Epoch: 2| Step: 0
Training loss: 6.583456529196967
Validation loss: 5.913230793840535

Epoch: 6| Step: 1
Training loss: 5.909161531253094
Validation loss: 5.911631613276945

Epoch: 6| Step: 2
Training loss: 5.982335472666134
Validation loss: 5.909869018755632

Epoch: 6| Step: 3
Training loss: 5.348357417281336
Validation loss: 5.908134469298067

Epoch: 6| Step: 4
Training loss: 6.734292014334259
Validation loss: 5.9062388048284555

Epoch: 6| Step: 5
Training loss: 5.246487850517239
Validation loss: 5.904378631527149

Epoch: 6| Step: 6
Training loss: 6.507121073432293
Validation loss: 5.902346335207905

Epoch: 6| Step: 7
Training loss: 5.652816151648303
Validation loss: 5.900241250962517

Epoch: 6| Step: 8
Training loss: 5.595860940728312
Validation loss: 5.897958738589521

Epoch: 6| Step: 9
Training loss: 5.5529650285699175
Validation loss: 5.8957587013867885

Epoch: 6| Step: 10
Training loss: 7.339214336537442
Validation loss: 5.893222981336053

Epoch: 6| Step: 11
Training loss: 5.784461866455051
Validation loss: 5.8907133411154575

Epoch: 6| Step: 12
Training loss: 5.763465704441169
Validation loss: 5.887984111344627

Epoch: 6| Step: 13
Training loss: 5.866743413827761
Validation loss: 5.885013298682973

Epoch: 3| Step: 0
Training loss: 4.503718005809699
Validation loss: 5.8820122338714365

Epoch: 6| Step: 1
Training loss: 6.426290585816936
Validation loss: 5.878819921457449

Epoch: 6| Step: 2
Training loss: 5.752230004477747
Validation loss: 5.875474572459613

Epoch: 6| Step: 3
Training loss: 6.200894432769955
Validation loss: 5.872079461506814

Epoch: 6| Step: 4
Training loss: 6.587060798611138
Validation loss: 5.868408915798496

Epoch: 6| Step: 5
Training loss: 6.434598694850327
Validation loss: 5.864496757812367

Epoch: 6| Step: 6
Training loss: 5.6965605280173435
Validation loss: 5.860340686481395

Epoch: 6| Step: 7
Training loss: 5.708260677274366
Validation loss: 5.856037786622419

Epoch: 6| Step: 8
Training loss: 5.952528715933563
Validation loss: 5.851559566400299

Epoch: 6| Step: 9
Training loss: 5.088056319278935
Validation loss: 5.8467524358155725

Epoch: 6| Step: 10
Training loss: 6.954464078394236
Validation loss: 5.84173092156016

Epoch: 6| Step: 11
Training loss: 5.580741717356639
Validation loss: 5.8365708858111605

Epoch: 6| Step: 12
Training loss: 5.97229761278143
Validation loss: 5.8309490326531686

Epoch: 6| Step: 13
Training loss: 6.313309721427571
Validation loss: 5.825316296941447

Epoch: 4| Step: 0
Training loss: 6.7713505787005275
Validation loss: 5.819292454440525

Epoch: 6| Step: 1
Training loss: 5.465646091810897
Validation loss: 5.812737900982426

Epoch: 6| Step: 2
Training loss: 6.218545728232694
Validation loss: 5.806248245813866

Epoch: 6| Step: 3
Training loss: 6.024621354863924
Validation loss: 5.799470140655396

Epoch: 6| Step: 4
Training loss: 6.268894474574413
Validation loss: 5.7923891819788995

Epoch: 6| Step: 5
Training loss: 5.797595203007835
Validation loss: 5.785043107874744

Epoch: 6| Step: 6
Training loss: 5.417838669186903
Validation loss: 5.777211801407628

Epoch: 6| Step: 7
Training loss: 6.0608282242138936
Validation loss: 5.769402290957143

Epoch: 6| Step: 8
Training loss: 6.248056338399366
Validation loss: 5.761516068336587

Epoch: 6| Step: 9
Training loss: 6.150138493661012
Validation loss: 5.753287495105974

Epoch: 6| Step: 10
Training loss: 6.066487367656311
Validation loss: 5.745094086153551

Epoch: 6| Step: 11
Training loss: 5.257117941157609
Validation loss: 5.736390884674344

Epoch: 6| Step: 12
Training loss: 4.599205462626253
Validation loss: 5.727515140970221

Epoch: 6| Step: 13
Training loss: 5.800918032899109
Validation loss: 5.719052610746441

Epoch: 5| Step: 0
Training loss: 6.217174541328541
Validation loss: 5.710260757309006

Epoch: 6| Step: 1
Training loss: 6.038066273545734
Validation loss: 5.701530449622881

Epoch: 6| Step: 2
Training loss: 5.5839384329935005
Validation loss: 5.6927006457650045

Epoch: 6| Step: 3
Training loss: 6.255855106539287
Validation loss: 5.683598224511412

Epoch: 6| Step: 4
Training loss: 6.1757241137260515
Validation loss: 5.674779339280062

Epoch: 6| Step: 5
Training loss: 5.177387906587483
Validation loss: 5.665288149762955

Epoch: 6| Step: 6
Training loss: 5.796265603497571
Validation loss: 5.65630993890354

Epoch: 6| Step: 7
Training loss: 5.123090062484757
Validation loss: 5.647334735552408

Epoch: 6| Step: 8
Training loss: 6.338443434577597
Validation loss: 5.638545025309402

Epoch: 6| Step: 9
Training loss: 4.725785423273182
Validation loss: 5.629672843412177

Epoch: 6| Step: 10
Training loss: 6.263869498655875
Validation loss: 5.6208358044497535

Epoch: 6| Step: 11
Training loss: 5.43410833405357
Validation loss: 5.612292311237514

Epoch: 6| Step: 12
Training loss: 5.962317070863946
Validation loss: 5.603969622507656

Epoch: 6| Step: 13
Training loss: 5.428574526219452
Validation loss: 5.595243655919923

Epoch: 6| Step: 0
Training loss: 6.605990217256477
Validation loss: 5.587210443035925

Epoch: 6| Step: 1
Training loss: 5.574421141812321
Validation loss: 5.578540291451299

Epoch: 6| Step: 2
Training loss: 5.741361016201592
Validation loss: 5.57057600927351

Epoch: 6| Step: 3
Training loss: 5.758278236548814
Validation loss: 5.562669787155207

Epoch: 6| Step: 4
Training loss: 6.232095019437591
Validation loss: 5.554536097556082

Epoch: 6| Step: 5
Training loss: 5.716443539964815
Validation loss: 5.547102859903209

Epoch: 6| Step: 6
Training loss: 5.98321856687062
Validation loss: 5.539762451826286

Epoch: 6| Step: 7
Training loss: 5.873138457466857
Validation loss: 5.532337979718

Epoch: 6| Step: 8
Training loss: 5.5388589883497
Validation loss: 5.525422757000141

Epoch: 6| Step: 9
Training loss: 6.786539150956173
Validation loss: 5.518472514623347

Epoch: 6| Step: 10
Training loss: 4.654789433126707
Validation loss: 5.51113567697184

Epoch: 6| Step: 11
Training loss: 4.404705236750384
Validation loss: 5.5046691726753405

Epoch: 6| Step: 12
Training loss: 4.200110506692963
Validation loss: 5.498336482621919

Epoch: 6| Step: 13
Training loss: 5.532480393972706
Validation loss: 5.49212559628498

Epoch: 7| Step: 0
Training loss: 6.044995708353619
Validation loss: 5.4858622766819

Epoch: 6| Step: 1
Training loss: 5.627757773950107
Validation loss: 5.4799164620410865

Epoch: 6| Step: 2
Training loss: 5.069013761344046
Validation loss: 5.47336788846506

Epoch: 6| Step: 3
Training loss: 5.74392711212752
Validation loss: 5.467226757262213

Epoch: 6| Step: 4
Training loss: 5.572395523755375
Validation loss: 5.460820463760993

Epoch: 6| Step: 5
Training loss: 4.470379298970072
Validation loss: 5.454548822026225

Epoch: 6| Step: 6
Training loss: 5.4052881697236
Validation loss: 5.448410896751949

Epoch: 6| Step: 7
Training loss: 5.774690773351594
Validation loss: 5.4420826626833625

Epoch: 6| Step: 8
Training loss: 5.397040842022957
Validation loss: 5.435609656997214

Epoch: 6| Step: 9
Training loss: 6.162070374282285
Validation loss: 5.428984481709711

Epoch: 6| Step: 10
Training loss: 5.159571831851661
Validation loss: 5.422127929416548

Epoch: 6| Step: 11
Training loss: 5.994927805512803
Validation loss: 5.415378750006516

Epoch: 6| Step: 12
Training loss: 5.709224919025821
Validation loss: 5.408521259649887

Epoch: 6| Step: 13
Training loss: 5.571162989508031
Validation loss: 5.401063640062985

Epoch: 8| Step: 0
Training loss: 4.051203118099803
Validation loss: 5.393682053463267

Epoch: 6| Step: 1
Training loss: 5.163528576371858
Validation loss: 5.386642355926974

Epoch: 6| Step: 2
Training loss: 5.124250031468972
Validation loss: 5.379445802679082

Epoch: 6| Step: 3
Training loss: 5.212800980987363
Validation loss: 5.372465645106455

Epoch: 6| Step: 4
Training loss: 6.259208304460941
Validation loss: 5.365264859163261

Epoch: 6| Step: 5
Training loss: 6.489829102133765
Validation loss: 5.358172697958993

Epoch: 6| Step: 6
Training loss: 5.398126220561653
Validation loss: 5.351462882569635

Epoch: 6| Step: 7
Training loss: 4.353530740005726
Validation loss: 5.3451666190498175

Epoch: 6| Step: 8
Training loss: 6.141216065204145
Validation loss: 5.338775133439821

Epoch: 6| Step: 9
Training loss: 5.119984102820513
Validation loss: 5.332751222473198

Epoch: 6| Step: 10
Training loss: 5.619547596163492
Validation loss: 5.326933547644126

Epoch: 6| Step: 11
Training loss: 6.005295006511702
Validation loss: 5.3210243798203845

Epoch: 6| Step: 12
Training loss: 4.750713094088481
Validation loss: 5.314766134988516

Epoch: 6| Step: 13
Training loss: 6.248056948940769
Validation loss: 5.308969558763631

Epoch: 9| Step: 0
Training loss: 4.259344710692239
Validation loss: 5.302957229193361

Epoch: 6| Step: 1
Training loss: 5.912611991353968
Validation loss: 5.297407185424167

Epoch: 6| Step: 2
Training loss: 5.697786858548608
Validation loss: 5.29140119812861

Epoch: 6| Step: 3
Training loss: 4.954203491245188
Validation loss: 5.28591039163703

Epoch: 6| Step: 4
Training loss: 5.678569111647909
Validation loss: 5.279612631604228

Epoch: 6| Step: 5
Training loss: 5.39688922855907
Validation loss: 5.273980308194094

Epoch: 6| Step: 6
Training loss: 4.70291694865301
Validation loss: 5.268350260070879

Epoch: 6| Step: 7
Training loss: 5.87770115867118
Validation loss: 5.262763569348443

Epoch: 6| Step: 8
Training loss: 5.110516248630333
Validation loss: 5.257261975786984

Epoch: 6| Step: 9
Training loss: 6.9209893135745615
Validation loss: 5.252044491571926

Epoch: 6| Step: 10
Training loss: 5.280547687850853
Validation loss: 5.24626047679457

Epoch: 6| Step: 11
Training loss: 4.604783490089472
Validation loss: 5.241373998641584

Epoch: 6| Step: 12
Training loss: 5.628974866481938
Validation loss: 5.235917808186806

Epoch: 6| Step: 13
Training loss: 4.870135888430357
Validation loss: 5.2303251930993415

Epoch: 10| Step: 0
Training loss: 6.5048428614268206
Validation loss: 5.225692456126406

Epoch: 6| Step: 1
Training loss: 5.205453345997206
Validation loss: 5.2199842322197885

Epoch: 6| Step: 2
Training loss: 4.4954408226221005
Validation loss: 5.21478490903426

Epoch: 6| Step: 3
Training loss: 5.1194224827617845
Validation loss: 5.209794513459933

Epoch: 6| Step: 4
Training loss: 5.357937900947998
Validation loss: 5.204407527206998

Epoch: 6| Step: 5
Training loss: 6.053855474586236
Validation loss: 5.199287549754734

Epoch: 6| Step: 6
Training loss: 4.876877643213432
Validation loss: 5.194002756027217

Epoch: 6| Step: 7
Training loss: 5.437737119919605
Validation loss: 5.189047065672855

Epoch: 6| Step: 8
Training loss: 5.133698315876917
Validation loss: 5.184117972709365

Epoch: 6| Step: 9
Training loss: 5.430594081801624
Validation loss: 5.1790610922837566

Epoch: 6| Step: 10
Training loss: 4.8862140495517705
Validation loss: 5.17390938168583

Epoch: 6| Step: 11
Training loss: 5.136976065917247
Validation loss: 5.16895927303833

Epoch: 6| Step: 12
Training loss: 5.285930658573388
Validation loss: 5.163586262237236

Epoch: 6| Step: 13
Training loss: 5.212146350014993
Validation loss: 5.158851753194502

Epoch: 11| Step: 0
Training loss: 5.943068292555662
Validation loss: 5.153883049741959

Epoch: 6| Step: 1
Training loss: 5.152966742922189
Validation loss: 5.148589823220073

Epoch: 6| Step: 2
Training loss: 4.64194046025714
Validation loss: 5.143560806076672

Epoch: 6| Step: 3
Training loss: 5.2221076808479445
Validation loss: 5.1385321255583785

Epoch: 6| Step: 4
Training loss: 5.649232336235564
Validation loss: 5.133797793410875

Epoch: 6| Step: 5
Training loss: 5.4110870772464885
Validation loss: 5.128776864373421

Epoch: 6| Step: 6
Training loss: 6.190101770109794
Validation loss: 5.123637196537709

Epoch: 6| Step: 7
Training loss: 5.288223267636224
Validation loss: 5.118567671301866

Epoch: 6| Step: 8
Training loss: 4.686982800243179
Validation loss: 5.113189673131914

Epoch: 6| Step: 9
Training loss: 4.132494274211593
Validation loss: 5.108084979510222

Epoch: 6| Step: 10
Training loss: 4.012555919982465
Validation loss: 5.103143810105955

Epoch: 6| Step: 11
Training loss: 4.495906663543509
Validation loss: 5.097907836474072

Epoch: 6| Step: 12
Training loss: 5.461929689572898
Validation loss: 5.093453178260709

Epoch: 6| Step: 13
Training loss: 6.47193159652417
Validation loss: 5.088648513510351

Epoch: 12| Step: 0
Training loss: 5.23586204244484
Validation loss: 5.083719765196724

Epoch: 6| Step: 1
Training loss: 4.21084192624031
Validation loss: 5.0789351066564095

Epoch: 6| Step: 2
Training loss: 4.64965988884232
Validation loss: 5.07395358392468

Epoch: 6| Step: 3
Training loss: 4.883328097777874
Validation loss: 5.069101871893911

Epoch: 6| Step: 4
Training loss: 4.824726179980787
Validation loss: 5.0643817559688795

Epoch: 6| Step: 5
Training loss: 5.557526840577581
Validation loss: 5.059879174027543

Epoch: 6| Step: 6
Training loss: 5.070916608608827
Validation loss: 5.0549270125042955

Epoch: 6| Step: 7
Training loss: 4.893258357777262
Validation loss: 5.050128026164068

Epoch: 6| Step: 8
Training loss: 5.371752267870156
Validation loss: 5.045100826368677

Epoch: 6| Step: 9
Training loss: 4.8243272776886705
Validation loss: 5.039965581675481

Epoch: 6| Step: 10
Training loss: 6.237329074757593
Validation loss: 5.035166540892477

Epoch: 6| Step: 11
Training loss: 5.848392050401145
Validation loss: 5.030097023727796

Epoch: 6| Step: 12
Training loss: 5.5045313808403575
Validation loss: 5.024992627443691

Epoch: 6| Step: 13
Training loss: 5.011117210769794
Validation loss: 5.01923583431721

Epoch: 13| Step: 0
Training loss: 4.634340700008458
Validation loss: 5.013641567359051

Epoch: 6| Step: 1
Training loss: 4.745144621209799
Validation loss: 5.007548705645145

Epoch: 6| Step: 2
Training loss: 6.020957267920039
Validation loss: 5.00191273661172

Epoch: 6| Step: 3
Training loss: 5.086854351042568
Validation loss: 4.995892411053164

Epoch: 6| Step: 4
Training loss: 5.5453287897760175
Validation loss: 4.990087410690619

Epoch: 6| Step: 5
Training loss: 4.658590867641235
Validation loss: 4.984089236304051

Epoch: 6| Step: 6
Training loss: 5.4682897755567055
Validation loss: 4.977219215391496

Epoch: 6| Step: 7
Training loss: 5.479762906702431
Validation loss: 4.971447786062804

Epoch: 6| Step: 8
Training loss: 4.745326905641618
Validation loss: 4.96449174090807

Epoch: 6| Step: 9
Training loss: 4.471814365362421
Validation loss: 4.957776569982047

Epoch: 6| Step: 10
Training loss: 5.412059859424789
Validation loss: 4.952414123673837

Epoch: 6| Step: 11
Training loss: 4.997340067011782
Validation loss: 4.946062381692918

Epoch: 6| Step: 12
Training loss: 5.478569936228119
Validation loss: 4.940683135547779

Epoch: 6| Step: 13
Training loss: 4.33420595528378
Validation loss: 4.934900814916131

Epoch: 14| Step: 0
Training loss: 4.8027405306360516
Validation loss: 4.930092224176697

Epoch: 6| Step: 1
Training loss: 4.632703397086939
Validation loss: 4.924912192760864

Epoch: 6| Step: 2
Training loss: 4.434151689150088
Validation loss: 4.920310281361556

Epoch: 6| Step: 3
Training loss: 5.602586639292716
Validation loss: 4.915127281540726

Epoch: 6| Step: 4
Training loss: 5.936807049921535
Validation loss: 4.909029972830639

Epoch: 6| Step: 5
Training loss: 5.520235463524532
Validation loss: 4.903496392704972

Epoch: 6| Step: 6
Training loss: 5.7620666605765445
Validation loss: 4.898700550891085

Epoch: 6| Step: 7
Training loss: 4.5119305449476546
Validation loss: 4.892759302252944

Epoch: 6| Step: 8
Training loss: 4.869247955596036
Validation loss: 4.8876756056046275

Epoch: 6| Step: 9
Training loss: 4.889378007831548
Validation loss: 4.882489816811615

Epoch: 6| Step: 10
Training loss: 5.477634036688886
Validation loss: 4.877267228800821

Epoch: 6| Step: 11
Training loss: 4.4762131053657885
Validation loss: 4.871406110768503

Epoch: 6| Step: 12
Training loss: 4.964159878648275
Validation loss: 4.867164150486144

Epoch: 6| Step: 13
Training loss: 4.017520204978702
Validation loss: 4.862565051430627

Epoch: 15| Step: 0
Training loss: 4.951375080581013
Validation loss: 4.858489353473039

Epoch: 6| Step: 1
Training loss: 5.576044800708369
Validation loss: 4.852884518932367

Epoch: 6| Step: 2
Training loss: 4.581445941143592
Validation loss: 4.848608554089198

Epoch: 6| Step: 3
Training loss: 4.979382923915223
Validation loss: 4.844994104235473

Epoch: 6| Step: 4
Training loss: 5.553461167481115
Validation loss: 4.838848628506707

Epoch: 6| Step: 5
Training loss: 4.9419028051522105
Validation loss: 4.833460126507234

Epoch: 6| Step: 6
Training loss: 5.295917795839827
Validation loss: 4.829175552917326

Epoch: 6| Step: 7
Training loss: 5.336546029596389
Validation loss: 4.824320260033999

Epoch: 6| Step: 8
Training loss: 4.181881679836874
Validation loss: 4.819643177157712

Epoch: 6| Step: 9
Training loss: 4.365524057082809
Validation loss: 4.815160239020235

Epoch: 6| Step: 10
Training loss: 4.103494243756767
Validation loss: 4.80992595977794

Epoch: 6| Step: 11
Training loss: 4.963802921855529
Validation loss: 4.805688504613806

Epoch: 6| Step: 12
Training loss: 4.79943183079388
Validation loss: 4.800565710110107

Epoch: 6| Step: 13
Training loss: 5.420124397961718
Validation loss: 4.796073921238433

Epoch: 16| Step: 0
Training loss: 3.9672144524949697
Validation loss: 4.791491640736683

Epoch: 6| Step: 1
Training loss: 4.844277630032773
Validation loss: 4.78731406625188

Epoch: 6| Step: 2
Training loss: 5.143075654519419
Validation loss: 4.782408829421733

Epoch: 6| Step: 3
Training loss: 5.283613951217296
Validation loss: 4.777939658170234

Epoch: 6| Step: 4
Training loss: 4.508724232836121
Validation loss: 4.773637316861274

Epoch: 6| Step: 5
Training loss: 6.016752697161388
Validation loss: 4.7690993021521635

Epoch: 6| Step: 6
Training loss: 5.230581429074749
Validation loss: 4.764117676876335

Epoch: 6| Step: 7
Training loss: 4.645114563677884
Validation loss: 4.759781653902022

Epoch: 6| Step: 8
Training loss: 4.547794000098728
Validation loss: 4.7552769611778185

Epoch: 6| Step: 9
Training loss: 5.4819342678712175
Validation loss: 4.750907761015827

Epoch: 6| Step: 10
Training loss: 4.046682699340365
Validation loss: 4.7461249623887936

Epoch: 6| Step: 11
Training loss: 4.2629748485448715
Validation loss: 4.741757318382149

Epoch: 6| Step: 12
Training loss: 5.169730395844899
Validation loss: 4.7373960820780825

Epoch: 6| Step: 13
Training loss: 4.857459570873032
Validation loss: 4.733403709497356

Epoch: 17| Step: 0
Training loss: 5.388252775701369
Validation loss: 4.7285683655348185

Epoch: 6| Step: 1
Training loss: 5.4086823917515945
Validation loss: 4.72421863964483

Epoch: 6| Step: 2
Training loss: 4.458270396326115
Validation loss: 4.720478238215383

Epoch: 6| Step: 3
Training loss: 4.537611291114307
Validation loss: 4.715732207363621

Epoch: 6| Step: 4
Training loss: 5.336104249373382
Validation loss: 4.711326249901385

Epoch: 6| Step: 5
Training loss: 4.6554384484403615
Validation loss: 4.707092774386124

Epoch: 6| Step: 6
Training loss: 3.445828284583185
Validation loss: 4.702427403621254

Epoch: 6| Step: 7
Training loss: 5.07426836725164
Validation loss: 4.698307413005113

Epoch: 6| Step: 8
Training loss: 5.086241260370118
Validation loss: 4.694252440358945

Epoch: 6| Step: 9
Training loss: 5.010123304422027
Validation loss: 4.689926773887789

Epoch: 6| Step: 10
Training loss: 4.97852328731266
Validation loss: 4.685546501851885

Epoch: 6| Step: 11
Training loss: 4.937939008679875
Validation loss: 4.681534765025697

Epoch: 6| Step: 12
Training loss: 4.676933076593002
Validation loss: 4.677121791667323

Epoch: 6| Step: 13
Training loss: 4.229613156857852
Validation loss: 4.673084965652962

Epoch: 18| Step: 0
Training loss: 4.310125166985076
Validation loss: 4.668676431766509

Epoch: 6| Step: 1
Training loss: 4.787113359341467
Validation loss: 4.664810163336233

Epoch: 6| Step: 2
Training loss: 4.192359723432518
Validation loss: 4.660611629486515

Epoch: 6| Step: 3
Training loss: 4.678198983646465
Validation loss: 4.656693200645862

Epoch: 6| Step: 4
Training loss: 4.498712143459174
Validation loss: 4.6525819057147135

Epoch: 6| Step: 5
Training loss: 4.444281326055648
Validation loss: 4.648977997288311

Epoch: 6| Step: 6
Training loss: 5.535735472185929
Validation loss: 4.6446355920807685

Epoch: 6| Step: 7
Training loss: 5.071351213843829
Validation loss: 4.640562320375839

Epoch: 6| Step: 8
Training loss: 4.931490174066462
Validation loss: 4.636832660324578

Epoch: 6| Step: 9
Training loss: 5.774111407725705
Validation loss: 4.632238359990239

Epoch: 6| Step: 10
Training loss: 4.7318456962624325
Validation loss: 4.62791244633491

Epoch: 6| Step: 11
Training loss: 4.3262448410184176
Validation loss: 4.624177876853771

Epoch: 6| Step: 12
Training loss: 4.725028402253219
Validation loss: 4.619636174638325

Epoch: 6| Step: 13
Training loss: 4.485029538332696
Validation loss: 4.615655204796368

Epoch: 19| Step: 0
Training loss: 5.3821250354726
Validation loss: 4.611715311019729

Epoch: 6| Step: 1
Training loss: 5.363693410396648
Validation loss: 4.607475277218075

Epoch: 6| Step: 2
Training loss: 4.783950940355831
Validation loss: 4.603624073122003

Epoch: 6| Step: 3
Training loss: 4.431455551997129
Validation loss: 4.599022294230103

Epoch: 6| Step: 4
Training loss: 4.627523816179829
Validation loss: 4.595018657138653

Epoch: 6| Step: 5
Training loss: 5.7215935980093215
Validation loss: 4.591177897161932

Epoch: 6| Step: 6
Training loss: 4.851137889027623
Validation loss: 4.586956337738086

Epoch: 6| Step: 7
Training loss: 4.4554921066150905
Validation loss: 4.582720328287534

Epoch: 6| Step: 8
Training loss: 4.0070435974323555
Validation loss: 4.578382968686763

Epoch: 6| Step: 9
Training loss: 4.519122923767627
Validation loss: 4.575049433762485

Epoch: 6| Step: 10
Training loss: 4.382369455938173
Validation loss: 4.571156724300321

Epoch: 6| Step: 11
Training loss: 3.97386236645709
Validation loss: 4.567230872410393

Epoch: 6| Step: 12
Training loss: 4.162946540777
Validation loss: 4.562556488523514

Epoch: 6| Step: 13
Training loss: 4.957105896059322
Validation loss: 4.55899780888556

Epoch: 20| Step: 0
Training loss: 4.680904376687416
Validation loss: 4.555090980695929

Epoch: 6| Step: 1
Training loss: 4.284730707562393
Validation loss: 4.551453582476695

Epoch: 6| Step: 2
Training loss: 4.862518961656172
Validation loss: 4.547533911714363

Epoch: 6| Step: 3
Training loss: 5.144581301365294
Validation loss: 4.543463267042448

Epoch: 6| Step: 4
Training loss: 4.501619683286141
Validation loss: 4.5395244947626185

Epoch: 6| Step: 5
Training loss: 4.413065515864732
Validation loss: 4.535533450154446

Epoch: 6| Step: 6
Training loss: 4.902631263356787
Validation loss: 4.531607953545074

Epoch: 6| Step: 7
Training loss: 4.537963524044309
Validation loss: 4.527862966681296

Epoch: 6| Step: 8
Training loss: 3.920908288242607
Validation loss: 4.523873579915029

Epoch: 6| Step: 9
Training loss: 4.786083495939299
Validation loss: 4.5199022989769775

Epoch: 6| Step: 10
Training loss: 4.839343028918087
Validation loss: 4.516034627988339

Epoch: 6| Step: 11
Training loss: 4.270528692529849
Validation loss: 4.512128979196674

Epoch: 6| Step: 12
Training loss: 5.608680108257441
Validation loss: 4.508182362809793

Epoch: 6| Step: 13
Training loss: 4.204883987332329
Validation loss: 4.504191036434866

Epoch: 21| Step: 0
Training loss: 5.210465119553624
Validation loss: 4.500335080422336

Epoch: 6| Step: 1
Training loss: 3.9655941408127533
Validation loss: 4.496220361256455

Epoch: 6| Step: 2
Training loss: 4.807362576254801
Validation loss: 4.492247544039387

Epoch: 6| Step: 3
Training loss: 4.910682376906529
Validation loss: 4.488411038577547

Epoch: 6| Step: 4
Training loss: 4.183740550937474
Validation loss: 4.484489909083261

Epoch: 6| Step: 5
Training loss: 4.937180907163378
Validation loss: 4.480546262736313

Epoch: 6| Step: 6
Training loss: 5.062559151009567
Validation loss: 4.476453494641615

Epoch: 6| Step: 7
Training loss: 4.56100941176929
Validation loss: 4.472483020520662

Epoch: 6| Step: 8
Training loss: 4.715766216006506
Validation loss: 4.468494327940665

Epoch: 6| Step: 9
Training loss: 3.8749593917195555
Validation loss: 4.4645018581563525

Epoch: 6| Step: 10
Training loss: 5.174272886048048
Validation loss: 4.460380951502192

Epoch: 6| Step: 11
Training loss: 3.776843948198735
Validation loss: 4.456531279301874

Epoch: 6| Step: 12
Training loss: 4.25518011911004
Validation loss: 4.452382200875725

Epoch: 6| Step: 13
Training loss: 4.6845459848948785
Validation loss: 4.448529894304412

Epoch: 22| Step: 0
Training loss: 4.194409036943042
Validation loss: 4.444066414385724

Epoch: 6| Step: 1
Training loss: 4.623384966975651
Validation loss: 4.439896809349666

Epoch: 6| Step: 2
Training loss: 4.24003622399457
Validation loss: 4.435550010519909

Epoch: 6| Step: 3
Training loss: 5.287996215622741
Validation loss: 4.431548627467588

Epoch: 6| Step: 4
Training loss: 4.682646413001864
Validation loss: 4.427034277550668

Epoch: 6| Step: 5
Training loss: 4.96491290028555
Validation loss: 4.422444481006107

Epoch: 6| Step: 6
Training loss: 4.024721048296585
Validation loss: 4.4180922936463665

Epoch: 6| Step: 7
Training loss: 4.807946366859926
Validation loss: 4.413908937196766

Epoch: 6| Step: 8
Training loss: 4.555010357205354
Validation loss: 4.409407492125229

Epoch: 6| Step: 9
Training loss: 4.782950105279662
Validation loss: 4.404819788484964

Epoch: 6| Step: 10
Training loss: 4.119949022736953
Validation loss: 4.401191459077922

Epoch: 6| Step: 11
Training loss: 5.032337142348256
Validation loss: 4.397668874106577

Epoch: 6| Step: 12
Training loss: 3.8314301838906473
Validation loss: 4.394552607731665

Epoch: 6| Step: 13
Training loss: 4.2500863907953255
Validation loss: 4.390020073140219

Epoch: 23| Step: 0
Training loss: 4.8564033867019445
Validation loss: 4.385681192691254

Epoch: 6| Step: 1
Training loss: 4.940431521329221
Validation loss: 4.380195221520333

Epoch: 6| Step: 2
Training loss: 5.574371015023558
Validation loss: 4.376070037050033

Epoch: 6| Step: 3
Training loss: 4.300579901338071
Validation loss: 4.371457254966902

Epoch: 6| Step: 4
Training loss: 4.060585392433959
Validation loss: 4.367614106534813

Epoch: 6| Step: 5
Training loss: 4.3965346995675985
Validation loss: 4.363566326412563

Epoch: 6| Step: 6
Training loss: 3.6380735961029735
Validation loss: 4.358673786751093

Epoch: 6| Step: 7
Training loss: 4.797864780761102
Validation loss: 4.354925913832039

Epoch: 6| Step: 8
Training loss: 3.921218634769686
Validation loss: 4.350453608163339

Epoch: 6| Step: 9
Training loss: 4.755062417183248
Validation loss: 4.346048730258984

Epoch: 6| Step: 10
Training loss: 4.566371581481955
Validation loss: 4.342022248031757

Epoch: 6| Step: 11
Training loss: 4.225794131794818
Validation loss: 4.3375605847381085

Epoch: 6| Step: 12
Training loss: 4.0143461454620395
Validation loss: 4.333342124245359

Epoch: 6| Step: 13
Training loss: 4.43037853199561
Validation loss: 4.329158710011038

Epoch: 24| Step: 0
Training loss: 4.248945890861836
Validation loss: 4.324997342206252

Epoch: 6| Step: 1
Training loss: 4.689930196864858
Validation loss: 4.321096927372904

Epoch: 6| Step: 2
Training loss: 4.263229871443432
Validation loss: 4.316597397198155

Epoch: 6| Step: 3
Training loss: 4.337090746126569
Validation loss: 4.312584180517086

Epoch: 6| Step: 4
Training loss: 4.6295078063760595
Validation loss: 4.308186497697856

Epoch: 6| Step: 5
Training loss: 4.8917336471798185
Validation loss: 4.303384074693279

Epoch: 6| Step: 6
Training loss: 5.176621578304311
Validation loss: 4.2984303196919065

Epoch: 6| Step: 7
Training loss: 4.417182508370677
Validation loss: 4.2942298099122915

Epoch: 6| Step: 8
Training loss: 4.704354261786174
Validation loss: 4.288971631883138

Epoch: 6| Step: 9
Training loss: 4.6417569921575135
Validation loss: 4.284861357147461

Epoch: 6| Step: 10
Training loss: 4.260156948440022
Validation loss: 4.279838979193671

Epoch: 6| Step: 11
Training loss: 3.4896974155889455
Validation loss: 4.275166944400691

Epoch: 6| Step: 12
Training loss: 3.9196016934508453
Validation loss: 4.270863708139968

Epoch: 6| Step: 13
Training loss: 4.062906098242336
Validation loss: 4.266567065645999

Epoch: 25| Step: 0
Training loss: 3.9235673269944034
Validation loss: 4.262873040186926

Epoch: 6| Step: 1
Training loss: 4.550337374409426
Validation loss: 4.258755510059968

Epoch: 6| Step: 2
Training loss: 4.448746734043433
Validation loss: 4.254668682092731

Epoch: 6| Step: 3
Training loss: 4.217577057718445
Validation loss: 4.250193778463537

Epoch: 6| Step: 4
Training loss: 4.503485707138894
Validation loss: 4.2462834582256646

Epoch: 6| Step: 5
Training loss: 3.4306208355072703
Validation loss: 4.242021888792743

Epoch: 6| Step: 6
Training loss: 3.807760701161775
Validation loss: 4.238061517741528

Epoch: 6| Step: 7
Training loss: 4.386163474289731
Validation loss: 4.234323311240389

Epoch: 6| Step: 8
Training loss: 4.7309543854895715
Validation loss: 4.230119018563685

Epoch: 6| Step: 9
Training loss: 4.393428681129565
Validation loss: 4.225877180965911

Epoch: 6| Step: 10
Training loss: 4.362233745680431
Validation loss: 4.221897204058594

Epoch: 6| Step: 11
Training loss: 4.826423814435265
Validation loss: 4.217834166030499

Epoch: 6| Step: 12
Training loss: 4.330943597962281
Validation loss: 4.213722133976236

Epoch: 6| Step: 13
Training loss: 4.967952256371032
Validation loss: 4.209433754801423

Epoch: 26| Step: 0
Training loss: 4.463921559331684
Validation loss: 4.205146879941377

Epoch: 6| Step: 1
Training loss: 3.4694200847969507
Validation loss: 4.200878483618495

Epoch: 6| Step: 2
Training loss: 4.602163900507963
Validation loss: 4.196827326712366

Epoch: 6| Step: 3
Training loss: 4.542809625464498
Validation loss: 4.192693573667085

Epoch: 6| Step: 4
Training loss: 3.6487422709629835
Validation loss: 4.188578092044642

Epoch: 6| Step: 5
Training loss: 4.547518445397497
Validation loss: 4.184142688458896

Epoch: 6| Step: 6
Training loss: 4.053594835236599
Validation loss: 4.180077593416323

Epoch: 6| Step: 7
Training loss: 4.317040306230095
Validation loss: 4.175690448036623

Epoch: 6| Step: 8
Training loss: 3.8941744504154605
Validation loss: 4.171570575098661

Epoch: 6| Step: 9
Training loss: 4.538346830010118
Validation loss: 4.167720731287584

Epoch: 6| Step: 10
Training loss: 4.823152716224465
Validation loss: 4.163215975811346

Epoch: 6| Step: 11
Training loss: 4.610227137532766
Validation loss: 4.15900415935524

Epoch: 6| Step: 12
Training loss: 4.1408709758895945
Validation loss: 4.154615649744614

Epoch: 6| Step: 13
Training loss: 4.4342833129780805
Validation loss: 4.15180860045298

Epoch: 27| Step: 0
Training loss: 2.8479117656926975
Validation loss: 4.146076208057987

Epoch: 6| Step: 1
Training loss: 3.949468074667137
Validation loss: 4.142864020189988

Epoch: 6| Step: 2
Training loss: 3.5906013295428445
Validation loss: 4.1379801046176805

Epoch: 6| Step: 3
Training loss: 4.720939077936496
Validation loss: 4.133925924326783

Epoch: 6| Step: 4
Training loss: 5.092213404760168
Validation loss: 4.129804568219642

Epoch: 6| Step: 5
Training loss: 4.669480951123505
Validation loss: 4.125388830335229

Epoch: 6| Step: 6
Training loss: 4.848740105339825
Validation loss: 4.121160974903418

Epoch: 6| Step: 7
Training loss: 4.184186165090619
Validation loss: 4.116738571795136

Epoch: 6| Step: 8
Training loss: 4.555191457097652
Validation loss: 4.11200992249802

Epoch: 6| Step: 9
Training loss: 4.771213544107168
Validation loss: 4.107622166815706

Epoch: 6| Step: 10
Training loss: 4.342022449366846
Validation loss: 4.103180000225147

Epoch: 6| Step: 11
Training loss: 3.865015518137584
Validation loss: 4.098566769320878

Epoch: 6| Step: 12
Training loss: 3.1056729615443066
Validation loss: 4.094274725057068

Epoch: 6| Step: 13
Training loss: 4.2916894313368985
Validation loss: 4.090064683136888

Epoch: 28| Step: 0
Training loss: 3.737357381185555
Validation loss: 4.085886928811884

Epoch: 6| Step: 1
Training loss: 4.932545167664934
Validation loss: 4.081479540567564

Epoch: 6| Step: 2
Training loss: 4.209321060344827
Validation loss: 4.0772534994572345

Epoch: 6| Step: 3
Training loss: 4.386297625252966
Validation loss: 4.072947034535608

Epoch: 6| Step: 4
Training loss: 3.5619238672018505
Validation loss: 4.068481365346049

Epoch: 6| Step: 5
Training loss: 3.785437794893905
Validation loss: 4.065079109983018

Epoch: 6| Step: 6
Training loss: 4.651861593403193
Validation loss: 4.060088649997112

Epoch: 6| Step: 7
Training loss: 4.485409926240956
Validation loss: 4.0557344294137865

Epoch: 6| Step: 8
Training loss: 3.517072727088104
Validation loss: 4.051503287854995

Epoch: 6| Step: 9
Training loss: 4.214811167621978
Validation loss: 4.04739794795105

Epoch: 6| Step: 10
Training loss: 3.846509981539881
Validation loss: 4.043187173296649

Epoch: 6| Step: 11
Training loss: 3.966356892604562
Validation loss: 4.039012581512483

Epoch: 6| Step: 12
Training loss: 4.682270438966874
Validation loss: 4.0348230343142975

Epoch: 6| Step: 13
Training loss: 4.403018713508257
Validation loss: 4.030507514650367

Epoch: 29| Step: 0
Training loss: 3.4992356146857215
Validation loss: 4.026277476811798

Epoch: 6| Step: 1
Training loss: 4.604953934462905
Validation loss: 4.022235441420337

Epoch: 6| Step: 2
Training loss: 4.182573523550689
Validation loss: 4.0179806342144815

Epoch: 6| Step: 3
Training loss: 4.697690339427077
Validation loss: 4.013679834808105

Epoch: 6| Step: 4
Training loss: 4.04109633573942
Validation loss: 4.009483162701205

Epoch: 6| Step: 5
Training loss: 4.471451376330584
Validation loss: 4.004989393159089

Epoch: 6| Step: 6
Training loss: 2.960243969219396
Validation loss: 4.000642367281923

Epoch: 6| Step: 7
Training loss: 3.7044008527831576
Validation loss: 3.9963901244554814

Epoch: 6| Step: 8
Training loss: 4.731883787893454
Validation loss: 3.992141215733562

Epoch: 6| Step: 9
Training loss: 4.0732977933524674
Validation loss: 3.98787711820614

Epoch: 6| Step: 10
Training loss: 3.591640483527693
Validation loss: 3.9835723068399327

Epoch: 6| Step: 11
Training loss: 4.689039257364401
Validation loss: 3.9793468787376622

Epoch: 6| Step: 12
Training loss: 4.489039742942883
Validation loss: 3.9751614423872135

Epoch: 6| Step: 13
Training loss: 3.640761999082055
Validation loss: 3.9708829739894504

Epoch: 30| Step: 0
Training loss: 4.8465680200401575
Validation loss: 3.966638899687277

Epoch: 6| Step: 1
Training loss: 3.8501529861373815
Validation loss: 3.9621248791710437

Epoch: 6| Step: 2
Training loss: 4.014824813498224
Validation loss: 3.9578258724516346

Epoch: 6| Step: 3
Training loss: 4.515624577611356
Validation loss: 3.9534190072355395

Epoch: 6| Step: 4
Training loss: 3.8843151232154725
Validation loss: 3.9490218750190107

Epoch: 6| Step: 5
Training loss: 3.667082430647233
Validation loss: 3.9446619687224596

Epoch: 6| Step: 6
Training loss: 4.496166291633516
Validation loss: 3.940395098238573

Epoch: 6| Step: 7
Training loss: 4.323743013506999
Validation loss: 3.9360913173800323

Epoch: 6| Step: 8
Training loss: 4.724255312410194
Validation loss: 3.931631266062766

Epoch: 6| Step: 9
Training loss: 3.965609772444784
Validation loss: 3.9270892290767594

Epoch: 6| Step: 10
Training loss: 3.2246736997199466
Validation loss: 3.9227435402831072

Epoch: 6| Step: 11
Training loss: 3.0439030164289553
Validation loss: 3.918320016020278

Epoch: 6| Step: 12
Training loss: 4.166103070607096
Validation loss: 3.914146823444277

Epoch: 6| Step: 13
Training loss: 3.875251761840947
Validation loss: 3.9098094439597415

Epoch: 31| Step: 0
Training loss: 4.078322833294142
Validation loss: 3.9057207283037045

Epoch: 6| Step: 1
Training loss: 4.175550349161627
Validation loss: 3.9013622578322003

Epoch: 6| Step: 2
Training loss: 4.0895172834123255
Validation loss: 3.8972147296142574

Epoch: 6| Step: 3
Training loss: 3.158527421313832
Validation loss: 3.8930384696603357

Epoch: 6| Step: 4
Training loss: 4.153340403967687
Validation loss: 3.888941307320104

Epoch: 6| Step: 5
Training loss: 3.9484922984177215
Validation loss: 3.8849327392843533

Epoch: 6| Step: 6
Training loss: 4.2154917249827975
Validation loss: 3.8806046797648945

Epoch: 6| Step: 7
Training loss: 3.7876406804009437
Validation loss: 3.876341269742129

Epoch: 6| Step: 8
Training loss: 3.7166621654352094
Validation loss: 3.872168172759798

Epoch: 6| Step: 9
Training loss: 4.148753583400244
Validation loss: 3.868015578265216

Epoch: 6| Step: 10
Training loss: 3.70373555876137
Validation loss: 3.8637091306814004

Epoch: 6| Step: 11
Training loss: 5.0832823870664665
Validation loss: 3.8595531386474704

Epoch: 6| Step: 12
Training loss: 4.193681623694518
Validation loss: 3.8552276422566267

Epoch: 6| Step: 13
Training loss: 3.396987921584663
Validation loss: 3.850887876574387

Epoch: 32| Step: 0
Training loss: 3.944688804352454
Validation loss: 3.8464422127631535

Epoch: 6| Step: 1
Training loss: 3.3547311311824104
Validation loss: 3.842076255053509

Epoch: 6| Step: 2
Training loss: 3.5974502753028244
Validation loss: 3.8379442568745237

Epoch: 6| Step: 3
Training loss: 4.33229387483069
Validation loss: 3.833727643561728

Epoch: 6| Step: 4
Training loss: 4.225595303642555
Validation loss: 3.8298009616205224

Epoch: 6| Step: 5
Training loss: 3.927445061953209
Validation loss: 3.825261567092

Epoch: 6| Step: 6
Training loss: 4.285999197799173
Validation loss: 3.8209486771662364

Epoch: 6| Step: 7
Training loss: 3.8443057271030745
Validation loss: 3.8166543599448097

Epoch: 6| Step: 8
Training loss: 3.319808095877988
Validation loss: 3.8123762392061877

Epoch: 6| Step: 9
Training loss: 3.942063124694625
Validation loss: 3.808058397537195

Epoch: 6| Step: 10
Training loss: 3.7158798553869326
Validation loss: 3.803873025870901

Epoch: 6| Step: 11
Training loss: 4.444157294427954
Validation loss: 3.7996112817076

Epoch: 6| Step: 12
Training loss: 3.999401763049103
Validation loss: 3.7953536095139286

Epoch: 6| Step: 13
Training loss: 4.214192054576009
Validation loss: 3.790847766742803

Epoch: 33| Step: 0
Training loss: 4.520354121016907
Validation loss: 3.786384228484498

Epoch: 6| Step: 1
Training loss: 4.110807567994881
Validation loss: 3.7819480356781616

Epoch: 6| Step: 2
Training loss: 4.084124481994131
Validation loss: 3.7775470267363866

Epoch: 6| Step: 3
Training loss: 4.113470900928218
Validation loss: 3.772967344154701

Epoch: 6| Step: 4
Training loss: 3.820448172876866
Validation loss: 3.7685418024540565

Epoch: 6| Step: 5
Training loss: 4.062132129152421
Validation loss: 3.763902916751135

Epoch: 6| Step: 6
Training loss: 3.3796470579707276
Validation loss: 3.7594325643874447

Epoch: 6| Step: 7
Training loss: 4.303582743758164
Validation loss: 3.7549909228042897

Epoch: 6| Step: 8
Training loss: 3.3531332536722016
Validation loss: 3.750460024273923

Epoch: 6| Step: 9
Training loss: 2.892578949242371
Validation loss: 3.746081424145248

Epoch: 6| Step: 10
Training loss: 3.594688226712523
Validation loss: 3.7418426800783666

Epoch: 6| Step: 11
Training loss: 3.8097784615735244
Validation loss: 3.737602265233639

Epoch: 6| Step: 12
Training loss: 3.8128476844216594
Validation loss: 3.7332022513356335

Epoch: 6| Step: 13
Training loss: 4.332036093183722
Validation loss: 3.7290699837936483

Epoch: 34| Step: 0
Training loss: 3.6500307316988705
Validation loss: 3.724437734339377

Epoch: 6| Step: 1
Training loss: 3.496964091648121
Validation loss: 3.720056042676329

Epoch: 6| Step: 2
Training loss: 3.5803234559456127
Validation loss: 3.7160914559006972

Epoch: 6| Step: 3
Training loss: 3.695993630862012
Validation loss: 3.71171451397874

Epoch: 6| Step: 4
Training loss: 4.217449524672946
Validation loss: 3.707554353198532

Epoch: 6| Step: 5
Training loss: 3.919890733869825
Validation loss: 3.7034556996352705

Epoch: 6| Step: 6
Training loss: 3.933292138697537
Validation loss: 3.6991461773916727

Epoch: 6| Step: 7
Training loss: 3.816333906723816
Validation loss: 3.6946670581478305

Epoch: 6| Step: 8
Training loss: 3.4855909748957785
Validation loss: 3.690643457791316

Epoch: 6| Step: 9
Training loss: 4.024436219341154
Validation loss: 3.6865818367684007

Epoch: 6| Step: 10
Training loss: 3.062664572517278
Validation loss: 3.6820304090883518

Epoch: 6| Step: 11
Training loss: 4.178411385215865
Validation loss: 3.6779377766777626

Epoch: 6| Step: 12
Training loss: 4.065455374085619
Validation loss: 3.6736981782809233

Epoch: 6| Step: 13
Training loss: 4.32222767437022
Validation loss: 3.6695200192715003

Epoch: 35| Step: 0
Training loss: 4.911330393004328
Validation loss: 3.664764033480425

Epoch: 6| Step: 1
Training loss: 3.527757656276987
Validation loss: 3.6599961133495045

Epoch: 6| Step: 2
Training loss: 3.4533172139659336
Validation loss: 3.655468283876956

Epoch: 6| Step: 3
Training loss: 3.784911093631439
Validation loss: 3.651192212594866

Epoch: 6| Step: 4
Training loss: 3.820871760772411
Validation loss: 3.6466715839034363

Epoch: 6| Step: 5
Training loss: 2.6700920714887437
Validation loss: 3.6423010508520353

Epoch: 6| Step: 6
Training loss: 3.8496464108679507
Validation loss: 3.6379091231558065

Epoch: 6| Step: 7
Training loss: 3.257039003461328
Validation loss: 3.633797723215962

Epoch: 6| Step: 8
Training loss: 4.163248923804881
Validation loss: 3.629358904652713

Epoch: 6| Step: 9
Training loss: 4.507782458332522
Validation loss: 3.6251242605375014

Epoch: 6| Step: 10
Training loss: 3.7000723702853597
Validation loss: 3.6206213277039443

Epoch: 6| Step: 11
Training loss: 4.013247487448016
Validation loss: 3.616026041686828

Epoch: 6| Step: 12
Training loss: 3.3469132051682617
Validation loss: 3.611715199405994

Epoch: 6| Step: 13
Training loss: 3.286934714796742
Validation loss: 3.607474306913452

Epoch: 36| Step: 0
Training loss: 3.1173227228267453
Validation loss: 3.603378533130726

Epoch: 6| Step: 1
Training loss: 3.369321779550061
Validation loss: 3.5992723135705518

Epoch: 6| Step: 2
Training loss: 3.7223343444815558
Validation loss: 3.5967075727633158

Epoch: 6| Step: 3
Training loss: 3.615680949339556
Validation loss: 3.590841692170171

Epoch: 6| Step: 4
Training loss: 3.9273038577923485
Validation loss: 3.5866957716724057

Epoch: 6| Step: 5
Training loss: 3.6997023591617095
Validation loss: 3.5828281016841217

Epoch: 6| Step: 6
Training loss: 2.988866172839712
Validation loss: 3.5787956797511074

Epoch: 6| Step: 7
Training loss: 4.142279298068049
Validation loss: 3.574989244138103

Epoch: 6| Step: 8
Training loss: 3.7268148212861614
Validation loss: 3.570819153789082

Epoch: 6| Step: 9
Training loss: 4.152005660036672
Validation loss: 3.566678858822338

Epoch: 6| Step: 10
Training loss: 3.519299295214443
Validation loss: 3.562280458523076

Epoch: 6| Step: 11
Training loss: 4.205291983574298
Validation loss: 3.557736228914968

Epoch: 6| Step: 12
Training loss: 3.737328546476298
Validation loss: 3.553437490374076

Epoch: 6| Step: 13
Training loss: 3.838853275620302
Validation loss: 3.5490734664265227

Epoch: 37| Step: 0
Training loss: 2.999160967163651
Validation loss: 3.5449375008623125

Epoch: 6| Step: 1
Training loss: 3.8795172910190665
Validation loss: 3.540866462215569

Epoch: 6| Step: 2
Training loss: 3.5989501481945334
Validation loss: 3.5365933077434004

Epoch: 6| Step: 3
Training loss: 3.8315842549566925
Validation loss: 3.532416890762448

Epoch: 6| Step: 4
Training loss: 3.8961725478295923
Validation loss: 3.528081276788534

Epoch: 6| Step: 5
Training loss: 3.3076248281484677
Validation loss: 3.5238703358249825

Epoch: 6| Step: 6
Training loss: 3.452744683082955
Validation loss: 3.5197228856895992

Epoch: 6| Step: 7
Training loss: 3.347226626092978
Validation loss: 3.5157504815207514

Epoch: 6| Step: 8
Training loss: 3.3104544027461142
Validation loss: 3.5116582348297913

Epoch: 6| Step: 9
Training loss: 3.6467932318482896
Validation loss: 3.5076896572338976

Epoch: 6| Step: 10
Training loss: 3.9391351362504774
Validation loss: 3.503458403421569

Epoch: 6| Step: 11
Training loss: 3.745927506657308
Validation loss: 3.4994785964964596

Epoch: 6| Step: 12
Training loss: 3.9918571081637015
Validation loss: 3.4951902333752143

Epoch: 6| Step: 13
Training loss: 4.051162628201529
Validation loss: 3.49102561334316

Epoch: 38| Step: 0
Training loss: 3.502667636603531
Validation loss: 3.4867186369466054

Epoch: 6| Step: 1
Training loss: 3.478706847240872
Validation loss: 3.4824741806749095

Epoch: 6| Step: 2
Training loss: 4.475357612514077
Validation loss: 3.4784315478562666

Epoch: 6| Step: 3
Training loss: 3.3539791331179485
Validation loss: 3.474006221656594

Epoch: 6| Step: 4
Training loss: 2.459271354381704
Validation loss: 3.469895571768915

Epoch: 6| Step: 5
Training loss: 3.637482428754073
Validation loss: 3.465837040231393

Epoch: 6| Step: 6
Training loss: 3.411488835244525
Validation loss: 3.4620878605513234

Epoch: 6| Step: 7
Training loss: 3.343988000118135
Validation loss: 3.4578655263561395

Epoch: 6| Step: 8
Training loss: 3.4578355791000015
Validation loss: 3.454009804348559

Epoch: 6| Step: 9
Training loss: 3.2569783924523303
Validation loss: 3.450181837403809

Epoch: 6| Step: 10
Training loss: 3.9408961590761424
Validation loss: 3.4462142767592256

Epoch: 6| Step: 11
Training loss: 3.5404720909722087
Validation loss: 3.442241958369769

Epoch: 6| Step: 12
Training loss: 4.081185898562212
Validation loss: 3.438177539059018

Epoch: 6| Step: 13
Training loss: 4.02349083133647
Validation loss: 3.4341949758185923

Epoch: 39| Step: 0
Training loss: 4.001833018878515
Validation loss: 3.4296291986490663

Epoch: 6| Step: 1
Training loss: 3.765641841613337
Validation loss: 3.4252983631572813

Epoch: 6| Step: 2
Training loss: 2.800761500758474
Validation loss: 3.420827487533227

Epoch: 6| Step: 3
Training loss: 3.6510922822513847
Validation loss: 3.416466315528218

Epoch: 6| Step: 4
Training loss: 3.6283046523505376
Validation loss: 3.4120923722237837

Epoch: 6| Step: 5
Training loss: 4.117115693307643
Validation loss: 3.408012564067336

Epoch: 6| Step: 6
Training loss: 3.4679612646260596
Validation loss: 3.4036255271362315

Epoch: 6| Step: 7
Training loss: 2.9625404065430723
Validation loss: 3.3993303200418548

Epoch: 6| Step: 8
Training loss: 3.7456608145211128
Validation loss: 3.3950469543850565

Epoch: 6| Step: 9
Training loss: 3.753620561571383
Validation loss: 3.391113035175927

Epoch: 6| Step: 10
Training loss: 3.6304721964423066
Validation loss: 3.386713352807719

Epoch: 6| Step: 11
Training loss: 3.0217990427816432
Validation loss: 3.382788395960963

Epoch: 6| Step: 12
Training loss: 3.4767558869141766
Validation loss: 3.378383376985581

Epoch: 6| Step: 13
Training loss: 3.3305426041541546
Validation loss: 3.3744571625999473

Epoch: 40| Step: 0
Training loss: 3.4801210171934427
Validation loss: 3.3707824657611694

Epoch: 6| Step: 1
Training loss: 3.3903691771020243
Validation loss: 3.366951564656836

Epoch: 6| Step: 2
Training loss: 3.3673143827602066
Validation loss: 3.362982464558127

Epoch: 6| Step: 3
Training loss: 3.67558860476682
Validation loss: 3.3592803941756806

Epoch: 6| Step: 4
Training loss: 3.039535054238932
Validation loss: 3.355590581254984

Epoch: 6| Step: 5
Training loss: 3.7605475546476743
Validation loss: 3.3519712493789386

Epoch: 6| Step: 6
Training loss: 3.1835933006613946
Validation loss: 3.3477961934950478

Epoch: 6| Step: 7
Training loss: 3.421175393129771
Validation loss: 3.344648222736765

Epoch: 6| Step: 8
Training loss: 3.5267280725662915
Validation loss: 3.3410709617513255

Epoch: 6| Step: 9
Training loss: 3.1269204914170037
Validation loss: 3.3365562395222463

Epoch: 6| Step: 10
Training loss: 3.521075689939401
Validation loss: 3.3328400405816554

Epoch: 6| Step: 11
Training loss: 3.3867512737679233
Validation loss: 3.3286682067669906

Epoch: 6| Step: 12
Training loss: 4.167432612908082
Validation loss: 3.32523708980119

Epoch: 6| Step: 13
Training loss: 3.6088340981504863
Validation loss: 3.3212276618693495

Epoch: 41| Step: 0
Training loss: 3.271491225505887
Validation loss: 3.3170747656045307

Epoch: 6| Step: 1
Training loss: 3.451152531602554
Validation loss: 3.3139736808110833

Epoch: 6| Step: 2
Training loss: 3.811883470057084
Validation loss: 3.310146647428801

Epoch: 6| Step: 3
Training loss: 2.9261938804906116
Validation loss: 3.3066755226488374

Epoch: 6| Step: 4
Training loss: 3.740352810012925
Validation loss: 3.303296601341134

Epoch: 6| Step: 5
Training loss: 4.029543966611783
Validation loss: 3.2992267145748295

Epoch: 6| Step: 6
Training loss: 3.5846326527536942
Validation loss: 3.2950188110540113

Epoch: 6| Step: 7
Training loss: 3.0418945296824336
Validation loss: 3.290724164694861

Epoch: 6| Step: 8
Training loss: 3.0560583760542865
Validation loss: 3.286745005506438

Epoch: 6| Step: 9
Training loss: 3.321058337647152
Validation loss: 3.283545959352383

Epoch: 6| Step: 10
Training loss: 3.243891624343113
Validation loss: 3.280377565688141

Epoch: 6| Step: 11
Training loss: 3.9258155024158725
Validation loss: 3.275684856801256

Epoch: 6| Step: 12
Training loss: 3.6242861867075535
Validation loss: 3.271974344333277

Epoch: 6| Step: 13
Training loss: 2.7744463737778817
Validation loss: 3.268395837837482

Epoch: 42| Step: 0
Training loss: 3.529105151172147
Validation loss: 3.2649663233517203

Epoch: 6| Step: 1
Training loss: 3.26418787531471
Validation loss: 3.2615688754005374

Epoch: 6| Step: 2
Training loss: 3.375581620574541
Validation loss: 3.2576958065951303

Epoch: 6| Step: 3
Training loss: 3.345998703103005
Validation loss: 3.2544992310341447

Epoch: 6| Step: 4
Training loss: 3.6395330142098503
Validation loss: 3.2505998180205893

Epoch: 6| Step: 5
Training loss: 3.6456108824667317
Validation loss: 3.2466868728215186

Epoch: 6| Step: 6
Training loss: 3.8634451589344128
Validation loss: 3.2431885793896975

Epoch: 6| Step: 7
Training loss: 2.987082646159184
Validation loss: 3.238851585213349

Epoch: 6| Step: 8
Training loss: 2.3924018390094197
Validation loss: 3.235161591984271

Epoch: 6| Step: 9
Training loss: 3.354552031625943
Validation loss: 3.2314820059365283

Epoch: 6| Step: 10
Training loss: 2.9231067040119405
Validation loss: 3.227715200907155

Epoch: 6| Step: 11
Training loss: 3.609038366167198
Validation loss: 3.2243704627425918

Epoch: 6| Step: 12
Training loss: 3.500399566730921
Validation loss: 3.220680793482755

Epoch: 6| Step: 13
Training loss: 3.6445167271364585
Validation loss: 3.2171064581845035

Epoch: 43| Step: 0
Training loss: 3.4694347908476875
Validation loss: 3.2129709158375626

Epoch: 6| Step: 1
Training loss: 3.279233040111645
Validation loss: 3.2094892962494623

Epoch: 6| Step: 2
Training loss: 2.952455313503468
Validation loss: 3.2062560122943546

Epoch: 6| Step: 3
Training loss: 3.83995228300647
Validation loss: 3.203136909276641

Epoch: 6| Step: 4
Training loss: 3.0494264532409345
Validation loss: 3.199542826042453

Epoch: 6| Step: 5
Training loss: 3.340107725541962
Validation loss: 3.196562344602702

Epoch: 6| Step: 6
Training loss: 2.9427441800165024
Validation loss: 3.1919233509427922

Epoch: 6| Step: 7
Training loss: 2.600080463191395
Validation loss: 3.1888477368381825

Epoch: 6| Step: 8
Training loss: 3.4989822815399854
Validation loss: 3.1850422003928114

Epoch: 6| Step: 9
Training loss: 3.4634298129651677
Validation loss: 3.1833314128357317

Epoch: 6| Step: 10
Training loss: 3.0991555201948953
Validation loss: 3.17941557950119

Epoch: 6| Step: 11
Training loss: 3.921833661230091
Validation loss: 3.174644088320635

Epoch: 6| Step: 12
Training loss: 3.7825859799291597
Validation loss: 3.171269589229432

Epoch: 6| Step: 13
Training loss: 3.120032673200064
Validation loss: 3.168249604632024

Epoch: 44| Step: 0
Training loss: 3.359096923674451
Validation loss: 3.1650437956557544

Epoch: 6| Step: 1
Training loss: 3.26007850201409
Validation loss: 3.1621438319665645

Epoch: 6| Step: 2
Training loss: 3.204375827611374
Validation loss: 3.158593745974266

Epoch: 6| Step: 3
Training loss: 2.6702592213841725
Validation loss: 3.1561330389629303

Epoch: 6| Step: 4
Training loss: 3.5414333191835805
Validation loss: 3.153318371875048

Epoch: 6| Step: 5
Training loss: 3.394604575678841
Validation loss: 3.1500030234993446

Epoch: 6| Step: 6
Training loss: 3.338942164036429
Validation loss: 3.1467145397159944

Epoch: 6| Step: 7
Training loss: 2.820899712041229
Validation loss: 3.1429507499126914

Epoch: 6| Step: 8
Training loss: 3.466929464894084
Validation loss: 3.139318836683641

Epoch: 6| Step: 9
Training loss: 3.904331315896743
Validation loss: 3.13574336600482

Epoch: 6| Step: 10
Training loss: 3.7381919603038147
Validation loss: 3.132203180001801

Epoch: 6| Step: 11
Training loss: 3.0437629652432574
Validation loss: 3.128604755329863

Epoch: 6| Step: 12
Training loss: 3.198978326541022
Validation loss: 3.1246999342224284

Epoch: 6| Step: 13
Training loss: 2.80667511368641
Validation loss: 3.1209794056571556

Epoch: 45| Step: 0
Training loss: 3.4122231283807403
Validation loss: 3.1166740383394202

Epoch: 6| Step: 1
Training loss: 3.7821760068351775
Validation loss: 3.1135700495525818

Epoch: 6| Step: 2
Training loss: 3.5658469289039223
Validation loss: 3.1099997169232827

Epoch: 6| Step: 3
Training loss: 2.8616469065702885
Validation loss: 3.106136533326533

Epoch: 6| Step: 4
Training loss: 3.62230555926709
Validation loss: 3.1047232382793557

Epoch: 6| Step: 5
Training loss: 3.239368656482701
Validation loss: 3.104180833071614

Epoch: 6| Step: 6
Training loss: 3.128964617657452
Validation loss: 3.095288468918643

Epoch: 6| Step: 7
Training loss: 2.998759172051075
Validation loss: 3.0936686444101698

Epoch: 6| Step: 8
Training loss: 3.4058689289241215
Validation loss: 3.090565439234687

Epoch: 6| Step: 9
Training loss: 3.338327878076899
Validation loss: 3.0894098503672747

Epoch: 6| Step: 10
Training loss: 3.1462617460326374
Validation loss: 3.090883140467778

Epoch: 6| Step: 11
Training loss: 2.998911660191057
Validation loss: 3.0921622418076726

Epoch: 6| Step: 12
Training loss: 2.695237310715288
Validation loss: 3.0815010339950915

Epoch: 6| Step: 13
Training loss: 3.035433684130957
Validation loss: 3.078047543243411

Epoch: 46| Step: 0
Training loss: 2.9759254875289165
Validation loss: 3.0757766130609614

Epoch: 6| Step: 1
Training loss: 3.2158554543232483
Validation loss: 3.0733089304415135

Epoch: 6| Step: 2
Training loss: 3.1313189896770823
Validation loss: 3.0710208583667087

Epoch: 6| Step: 3
Training loss: 3.057645102525479
Validation loss: 3.068608866940422

Epoch: 6| Step: 4
Training loss: 3.137854725515887
Validation loss: 3.0646610809429062

Epoch: 6| Step: 5
Training loss: 3.561533746686135
Validation loss: 3.059924879003877

Epoch: 6| Step: 6
Training loss: 3.5029703887114794
Validation loss: 3.0567160892616365

Epoch: 6| Step: 7
Training loss: 3.2350183114384063
Validation loss: 3.0530903340834397

Epoch: 6| Step: 8
Training loss: 3.0610896385749022
Validation loss: 3.0496889992875817

Epoch: 6| Step: 9
Training loss: 3.344372272522423
Validation loss: 3.0454385876507075

Epoch: 6| Step: 10
Training loss: 3.4766264148794725
Validation loss: 3.042060321434674

Epoch: 6| Step: 11
Training loss: 3.0560302905034513
Validation loss: 3.0387123775729266

Epoch: 6| Step: 12
Training loss: 3.009406284316232
Validation loss: 3.0364799607795168

Epoch: 6| Step: 13
Training loss: 2.903862413389029
Validation loss: 3.0329185953373012

Epoch: 47| Step: 0
Training loss: 3.223219499783814
Validation loss: 3.029294776218916

Epoch: 6| Step: 1
Training loss: 2.8204611107725577
Validation loss: 3.026938147496935

Epoch: 6| Step: 2
Training loss: 3.1053646430073294
Validation loss: 3.024917227293052

Epoch: 6| Step: 3
Training loss: 2.688852901147854
Validation loss: 3.0219629521475313

Epoch: 6| Step: 4
Training loss: 3.422080673934502
Validation loss: 3.021394313595283

Epoch: 6| Step: 5
Training loss: 2.9596756231572954
Validation loss: 3.017644650611752

Epoch: 6| Step: 6
Training loss: 3.241784495849361
Validation loss: 3.0145463875749847

Epoch: 6| Step: 7
Training loss: 3.264511575662846
Validation loss: 3.0106781226712527

Epoch: 6| Step: 8
Training loss: 3.8308295213670553
Validation loss: 3.007725094272492

Epoch: 6| Step: 9
Training loss: 3.1886492882484374
Validation loss: 3.0041421269346165

Epoch: 6| Step: 10
Training loss: 2.458772221412954
Validation loss: 3.001446242880144

Epoch: 6| Step: 11
Training loss: 3.5996575934440016
Validation loss: 2.9982380726305418

Epoch: 6| Step: 12
Training loss: 2.930535033657863
Validation loss: 2.995874535317514

Epoch: 6| Step: 13
Training loss: 3.1533637116187383
Validation loss: 2.9935797497595025

Epoch: 48| Step: 0
Training loss: 3.058452032905772
Validation loss: 2.9905247197467033

Epoch: 6| Step: 1
Training loss: 3.4612476218543624
Validation loss: 2.9878336652268014

Epoch: 6| Step: 2
Training loss: 2.847043320739257
Validation loss: 2.985330638369181

Epoch: 6| Step: 3
Training loss: 3.227963935253516
Validation loss: 2.9816838725151364

Epoch: 6| Step: 4
Training loss: 2.9440097507958023
Validation loss: 2.9789306084715546

Epoch: 6| Step: 5
Training loss: 3.1750451392622026
Validation loss: 2.9762363872619972

Epoch: 6| Step: 6
Training loss: 2.8370110447601045
Validation loss: 2.973684259313101

Epoch: 6| Step: 7
Training loss: 3.37022075643162
Validation loss: 2.970088817475743

Epoch: 6| Step: 8
Training loss: 3.2217686396006555
Validation loss: 2.967265591188892

Epoch: 6| Step: 9
Training loss: 2.7653415340693144
Validation loss: 2.964089439643695

Epoch: 6| Step: 10
Training loss: 2.806856724457246
Validation loss: 2.9612775756616907

Epoch: 6| Step: 11
Training loss: 3.0712223015399056
Validation loss: 2.958126575784689

Epoch: 6| Step: 12
Training loss: 3.2165312295444553
Validation loss: 2.9551689569330537

Epoch: 6| Step: 13
Training loss: 3.493845569484049
Validation loss: 2.953481151492468

Epoch: 49| Step: 0
Training loss: 3.0146187007563174
Validation loss: 2.9527999995119063

Epoch: 6| Step: 1
Training loss: 3.630593949374115
Validation loss: 2.946874108420568

Epoch: 6| Step: 2
Training loss: 2.735666634807336
Validation loss: 2.9441557888676204

Epoch: 6| Step: 3
Training loss: 3.505710983323667
Validation loss: 2.941279624317465

Epoch: 6| Step: 4
Training loss: 2.163083733068854
Validation loss: 2.939083687007777

Epoch: 6| Step: 5
Training loss: 2.924781536844939
Validation loss: 2.9375213459943756

Epoch: 6| Step: 6
Training loss: 2.303568112359348
Validation loss: 2.9361046662786707

Epoch: 6| Step: 7
Training loss: 2.9001001077671096
Validation loss: 2.9387465192719286

Epoch: 6| Step: 8
Training loss: 3.8099893526744504
Validation loss: 2.9345220526917797

Epoch: 6| Step: 9
Training loss: 2.5662648526378327
Validation loss: 2.9294206828369806

Epoch: 6| Step: 10
Training loss: 3.4392947973389734
Validation loss: 2.9251089847399543

Epoch: 6| Step: 11
Training loss: 3.3610293550943084
Validation loss: 2.922346515655229

Epoch: 6| Step: 12
Training loss: 2.0753107734423697
Validation loss: 2.919557764322379

Epoch: 6| Step: 13
Training loss: 3.885938648550418
Validation loss: 2.9182151952505873

Epoch: 50| Step: 0
Training loss: 3.004153713951458
Validation loss: 2.914732337035018

Epoch: 6| Step: 1
Training loss: 3.33799805086055
Validation loss: 2.91256348045488

Epoch: 6| Step: 2
Training loss: 2.5046991054101846
Validation loss: 2.9094864930104163

Epoch: 6| Step: 3
Training loss: 2.8051339221882317
Validation loss: 2.907394812647309

Epoch: 6| Step: 4
Training loss: 3.587182454267566
Validation loss: 2.9046001741997927

Epoch: 6| Step: 5
Training loss: 3.245396802061657
Validation loss: 2.9026903755824214

Epoch: 6| Step: 6
Training loss: 2.6339748997343198
Validation loss: 2.9006072471890216

Epoch: 6| Step: 7
Training loss: 2.7434972694457076
Validation loss: 2.8982892679729484

Epoch: 6| Step: 8
Training loss: 3.517441886199476
Validation loss: 2.895361928282873

Epoch: 6| Step: 9
Training loss: 2.9125745313546574
Validation loss: 2.893118365501235

Epoch: 6| Step: 10
Training loss: 3.4475585813019607
Validation loss: 2.8904652869289054

Epoch: 6| Step: 11
Training loss: 3.0249707181159597
Validation loss: 2.8885354540088937

Epoch: 6| Step: 12
Training loss: 2.745093997951012
Validation loss: 2.8842891367609904

Epoch: 6| Step: 13
Training loss: 2.84286854256277
Validation loss: 2.882016217564899

Testing loss: 2.4470394028013662

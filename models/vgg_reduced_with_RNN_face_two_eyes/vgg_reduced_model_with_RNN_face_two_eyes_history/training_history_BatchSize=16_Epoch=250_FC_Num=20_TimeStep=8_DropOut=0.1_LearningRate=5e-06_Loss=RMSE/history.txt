Epoch: 1| Step: 0
Training loss: 5.664890833827223
Validation loss: 5.884470941859116

Epoch: 6| Step: 1
Training loss: 5.07846565864582
Validation loss: 5.883036858686512

Epoch: 6| Step: 2
Training loss: 5.657986706251194
Validation loss: 5.881547647498689

Epoch: 6| Step: 3
Training loss: 6.554246635296755
Validation loss: 5.880049328024038

Epoch: 6| Step: 4
Training loss: 5.94228387153149
Validation loss: 5.8784604804436285

Epoch: 6| Step: 5
Training loss: 5.633721435691139
Validation loss: 5.876937817170853

Epoch: 6| Step: 6
Training loss: 6.892311356944378
Validation loss: 5.875300217120516

Epoch: 6| Step: 7
Training loss: 6.333341130034349
Validation loss: 5.873567798898108

Epoch: 6| Step: 8
Training loss: 6.471873832934619
Validation loss: 5.871778646601961

Epoch: 6| Step: 9
Training loss: 6.188177823017622
Validation loss: 5.869821660648628

Epoch: 6| Step: 10
Training loss: 5.81835839155329
Validation loss: 5.867865674359516

Epoch: 6| Step: 11
Training loss: 6.460088708699562
Validation loss: 5.865762170872187

Epoch: 6| Step: 12
Training loss: 6.219572846688402
Validation loss: 5.863505109071125

Epoch: 6| Step: 13
Training loss: 4.5225371681993
Validation loss: 5.8611722775241155

Epoch: 2| Step: 0
Training loss: 6.0665357862055425
Validation loss: 5.858749695366803

Epoch: 6| Step: 1
Training loss: 5.900160784066145
Validation loss: 5.856114381395731

Epoch: 6| Step: 2
Training loss: 6.398630365206066
Validation loss: 5.8533617418826465

Epoch: 6| Step: 3
Training loss: 5.747863704395455
Validation loss: 5.850461780765466

Epoch: 6| Step: 4
Training loss: 6.061501332250856
Validation loss: 5.8473910108273275

Epoch: 6| Step: 5
Training loss: 5.262701201953236
Validation loss: 5.8442448425213644

Epoch: 6| Step: 6
Training loss: 6.433691286923818
Validation loss: 5.840899229406815

Epoch: 6| Step: 7
Training loss: 5.500171138528381
Validation loss: 5.837340758586592

Epoch: 6| Step: 8
Training loss: 6.416416642243197
Validation loss: 5.833819296440064

Epoch: 6| Step: 9
Training loss: 7.16765581741982
Validation loss: 5.829868468365228

Epoch: 6| Step: 10
Training loss: 5.448236498613493
Validation loss: 5.82559697444327

Epoch: 6| Step: 11
Training loss: 5.990613907319705
Validation loss: 5.82130610030429

Epoch: 6| Step: 12
Training loss: 5.4633472894129955
Validation loss: 5.816997907036929

Epoch: 6| Step: 13
Training loss: 5.172268117848316
Validation loss: 5.812265699032722

Epoch: 3| Step: 0
Training loss: 5.5749432548064135
Validation loss: 5.8072801621213355

Epoch: 6| Step: 1
Training loss: 6.888420738528977
Validation loss: 5.802245580115741

Epoch: 6| Step: 2
Training loss: 5.844919995363261
Validation loss: 5.7966635117934455

Epoch: 6| Step: 3
Training loss: 6.516335135349578
Validation loss: 5.79098551561699

Epoch: 6| Step: 4
Training loss: 5.569432086037708
Validation loss: 5.785017281047361

Epoch: 6| Step: 5
Training loss: 5.7382732726289
Validation loss: 5.7785867691173225

Epoch: 6| Step: 6
Training loss: 5.331310941788354
Validation loss: 5.771641735464049

Epoch: 6| Step: 7
Training loss: 5.866270681946681
Validation loss: 5.764704712680242

Epoch: 6| Step: 8
Training loss: 5.744834570034245
Validation loss: 5.757446636306567

Epoch: 6| Step: 9
Training loss: 5.623605512911068
Validation loss: 5.749613071636307

Epoch: 6| Step: 10
Training loss: 6.181454566216533
Validation loss: 5.741701717112855

Epoch: 6| Step: 11
Training loss: 5.564843016515552
Validation loss: 5.733448199666593

Epoch: 6| Step: 12
Training loss: 6.087325401857182
Validation loss: 5.724886133208229

Epoch: 6| Step: 13
Training loss: 5.614933246001742
Validation loss: 5.715969472842322

Epoch: 4| Step: 0
Training loss: 5.821394155469124
Validation loss: 5.706976634205284

Epoch: 6| Step: 1
Training loss: 6.335137294374465
Validation loss: 5.697420543804565

Epoch: 6| Step: 2
Training loss: 6.12991945098677
Validation loss: 5.687524704652446

Epoch: 6| Step: 3
Training loss: 5.344722904627151
Validation loss: 5.678019352547475

Epoch: 6| Step: 4
Training loss: 6.1736019760462835
Validation loss: 5.667719472060791

Epoch: 6| Step: 5
Training loss: 5.367337109564209
Validation loss: 5.657507375548563

Epoch: 6| Step: 6
Training loss: 5.889374698937266
Validation loss: 5.64719966482167

Epoch: 6| Step: 7
Training loss: 5.210597998041337
Validation loss: 5.637162009381984

Epoch: 6| Step: 8
Training loss: 5.620027675628395
Validation loss: 5.626815757147232

Epoch: 6| Step: 9
Training loss: 6.010724338037063
Validation loss: 5.61642138155904

Epoch: 6| Step: 10
Training loss: 5.377205861806764
Validation loss: 5.605797805609283

Epoch: 6| Step: 11
Training loss: 5.42872560612
Validation loss: 5.595115594382839

Epoch: 6| Step: 12
Training loss: 5.758791629350157
Validation loss: 5.584828798043009

Epoch: 6| Step: 13
Training loss: 6.007191480825685
Validation loss: 5.573851315088776

Epoch: 5| Step: 0
Training loss: 6.173713197878421
Validation loss: 5.563449221425489

Epoch: 6| Step: 1
Training loss: 5.627520187389171
Validation loss: 5.5528141230240315

Epoch: 6| Step: 2
Training loss: 5.4967643583525465
Validation loss: 5.541950256816413

Epoch: 6| Step: 3
Training loss: 5.19142682463944
Validation loss: 5.530853228517682

Epoch: 6| Step: 4
Training loss: 5.379840888751381
Validation loss: 5.520562487982164

Epoch: 6| Step: 5
Training loss: 5.092401993135693
Validation loss: 5.509637480693306

Epoch: 6| Step: 6
Training loss: 5.614685435124606
Validation loss: 5.499262904857777

Epoch: 6| Step: 7
Training loss: 6.193807758327598
Validation loss: 5.488677218487557

Epoch: 6| Step: 8
Training loss: 5.458426360679508
Validation loss: 5.477894779234232

Epoch: 6| Step: 9
Training loss: 6.15817432052017
Validation loss: 5.466858687556196

Epoch: 6| Step: 10
Training loss: 5.442552871552426
Validation loss: 5.4561879064545025

Epoch: 6| Step: 11
Training loss: 5.77940809446463
Validation loss: 5.444944029007484

Epoch: 6| Step: 12
Training loss: 5.102915749403258
Validation loss: 5.434049512706618

Epoch: 6| Step: 13
Training loss: 5.699317141671242
Validation loss: 5.423237947181224

Epoch: 6| Step: 0
Training loss: 5.11545884148583
Validation loss: 5.412472945241875

Epoch: 6| Step: 1
Training loss: 5.438924317877176
Validation loss: 5.401743927500881

Epoch: 6| Step: 2
Training loss: 4.9046324961076335
Validation loss: 5.391136285243146

Epoch: 6| Step: 3
Training loss: 5.638943492873493
Validation loss: 5.3809348173212515

Epoch: 6| Step: 4
Training loss: 5.433268160906951
Validation loss: 5.36995942685921

Epoch: 6| Step: 5
Training loss: 5.077925927588371
Validation loss: 5.360069860079024

Epoch: 6| Step: 6
Training loss: 5.864166660537419
Validation loss: 5.349896976037529

Epoch: 6| Step: 7
Training loss: 5.713095615980357
Validation loss: 5.340772963264163

Epoch: 6| Step: 8
Training loss: 5.501403629486865
Validation loss: 5.331009726198782

Epoch: 6| Step: 9
Training loss: 5.864832744636337
Validation loss: 5.322111164616459

Epoch: 6| Step: 10
Training loss: 5.657961423147061
Validation loss: 5.313582396100327

Epoch: 6| Step: 11
Training loss: 5.242106907505026
Validation loss: 5.30483605575232

Epoch: 6| Step: 12
Training loss: 5.607808608986182
Validation loss: 5.29632424461047

Epoch: 6| Step: 13
Training loss: 5.379098438605398
Validation loss: 5.287933454490157

Epoch: 7| Step: 0
Training loss: 5.146426929826457
Validation loss: 5.2801007406661915

Epoch: 6| Step: 1
Training loss: 4.565971203680211
Validation loss: 5.272376376168297

Epoch: 6| Step: 2
Training loss: 5.438553280596658
Validation loss: 5.264544307240539

Epoch: 6| Step: 3
Training loss: 5.162384637832189
Validation loss: 5.256736369391519

Epoch: 6| Step: 4
Training loss: 4.760582228883247
Validation loss: 5.2495202042062195

Epoch: 6| Step: 5
Training loss: 6.2439686474400915
Validation loss: 5.242187863845926

Epoch: 6| Step: 6
Training loss: 5.635113588692841
Validation loss: 5.235659222563288

Epoch: 6| Step: 7
Training loss: 4.756584422401398
Validation loss: 5.229227235400226

Epoch: 6| Step: 8
Training loss: 4.794158464558799
Validation loss: 5.223290271074361

Epoch: 6| Step: 9
Training loss: 4.557309686534315
Validation loss: 5.217631206778877

Epoch: 6| Step: 10
Training loss: 5.79774028561418
Validation loss: 5.212189378638765

Epoch: 6| Step: 11
Training loss: 6.2177334822073
Validation loss: 5.206355988582635

Epoch: 6| Step: 12
Training loss: 5.61024846579012
Validation loss: 5.200462507806876

Epoch: 6| Step: 13
Training loss: 5.888681592031784
Validation loss: 5.194166778967566

Epoch: 8| Step: 0
Training loss: 5.374293081238394
Validation loss: 5.188349949097039

Epoch: 6| Step: 1
Training loss: 4.9699174001419175
Validation loss: 5.182709163033883

Epoch: 6| Step: 2
Training loss: 4.864664908902084
Validation loss: 5.176498881558497

Epoch: 6| Step: 3
Training loss: 5.024031679600254
Validation loss: 5.171254592363573

Epoch: 6| Step: 4
Training loss: 5.292006334031424
Validation loss: 5.165428146485351

Epoch: 6| Step: 5
Training loss: 5.479928586174305
Validation loss: 5.159864819305982

Epoch: 6| Step: 6
Training loss: 5.580663621606263
Validation loss: 5.154335822637284

Epoch: 6| Step: 7
Training loss: 5.589057189921821
Validation loss: 5.148664640240927

Epoch: 6| Step: 8
Training loss: 4.671353007980761
Validation loss: 5.143467481540339

Epoch: 6| Step: 9
Training loss: 4.810686835027513
Validation loss: 5.137591548206098

Epoch: 6| Step: 10
Training loss: 5.653650856337973
Validation loss: 5.132137662593757

Epoch: 6| Step: 11
Training loss: 4.566490831725752
Validation loss: 5.12689251576278

Epoch: 6| Step: 12
Training loss: 5.529058507203032
Validation loss: 5.1216028320790095

Epoch: 6| Step: 13
Training loss: 6.201661213468584
Validation loss: 5.116160111509485

Epoch: 9| Step: 0
Training loss: 5.431820062322861
Validation loss: 5.110955914384802

Epoch: 6| Step: 1
Training loss: 5.064262742577096
Validation loss: 5.105492877587005

Epoch: 6| Step: 2
Training loss: 5.573656530971759
Validation loss: 5.099744004325883

Epoch: 6| Step: 3
Training loss: 5.156385570246648
Validation loss: 5.094325370510949

Epoch: 6| Step: 4
Training loss: 4.793990569562553
Validation loss: 5.0886607577529785

Epoch: 6| Step: 5
Training loss: 5.732199939805166
Validation loss: 5.083282637213051

Epoch: 6| Step: 6
Training loss: 4.281901066413575
Validation loss: 5.077762782554458

Epoch: 6| Step: 7
Training loss: 5.016443108597166
Validation loss: 5.072579823720225

Epoch: 6| Step: 8
Training loss: 4.748382694572051
Validation loss: 5.066687810167735

Epoch: 6| Step: 9
Training loss: 5.242913868093493
Validation loss: 5.061440596386197

Epoch: 6| Step: 10
Training loss: 5.447948370623074
Validation loss: 5.056206704336677

Epoch: 6| Step: 11
Training loss: 5.459555258610922
Validation loss: 5.050572634167708

Epoch: 6| Step: 12
Training loss: 5.542315605238158
Validation loss: 5.045272304972975

Epoch: 6| Step: 13
Training loss: 5.1181602427063115
Validation loss: 5.039447874172877

Epoch: 10| Step: 0
Training loss: 5.606240079226694
Validation loss: 5.034006615422332

Epoch: 6| Step: 1
Training loss: 4.986681174462849
Validation loss: 5.028597051330203

Epoch: 6| Step: 2
Training loss: 5.159100478671009
Validation loss: 5.022928341641406

Epoch: 6| Step: 3
Training loss: 4.5312112740802055
Validation loss: 5.017328149437201

Epoch: 6| Step: 4
Training loss: 5.09771050318352
Validation loss: 5.012049492863855

Epoch: 6| Step: 5
Training loss: 4.727094131710698
Validation loss: 5.006797207545574

Epoch: 6| Step: 6
Training loss: 4.528931333621454
Validation loss: 5.001308524251689

Epoch: 6| Step: 7
Training loss: 5.079533308323816
Validation loss: 4.995981922543523

Epoch: 6| Step: 8
Training loss: 5.030166414515016
Validation loss: 4.99024706774825

Epoch: 6| Step: 9
Training loss: 5.185129566247541
Validation loss: 4.984859622367707

Epoch: 6| Step: 10
Training loss: 5.391619162175812
Validation loss: 4.979469811483441

Epoch: 6| Step: 11
Training loss: 5.929439494257822
Validation loss: 4.974134330897912

Epoch: 6| Step: 12
Training loss: 4.6955377458061225
Validation loss: 4.968936364859432

Epoch: 6| Step: 13
Training loss: 5.565280690753173
Validation loss: 4.963015432038259

Epoch: 11| Step: 0
Training loss: 5.13601394285407
Validation loss: 4.957236171261355

Epoch: 6| Step: 1
Training loss: 4.757091047838664
Validation loss: 4.951989590286217

Epoch: 6| Step: 2
Training loss: 5.2764759041929965
Validation loss: 4.946757366157778

Epoch: 6| Step: 3
Training loss: 5.503330956005474
Validation loss: 4.941065116837722

Epoch: 6| Step: 4
Training loss: 3.6711327634541817
Validation loss: 4.9359293503798165

Epoch: 6| Step: 5
Training loss: 4.553227447425428
Validation loss: 4.930569964225144

Epoch: 6| Step: 6
Training loss: 4.842184096683218
Validation loss: 4.9251243885423595

Epoch: 6| Step: 7
Training loss: 5.7375053987498426
Validation loss: 4.9202712256666015

Epoch: 6| Step: 8
Training loss: 4.782208116062289
Validation loss: 4.914901766851241

Epoch: 6| Step: 9
Training loss: 5.113768948464172
Validation loss: 4.910339594826761

Epoch: 6| Step: 10
Training loss: 5.0229769150000125
Validation loss: 4.90461350541425

Epoch: 6| Step: 11
Training loss: 5.654467560031729
Validation loss: 4.89905170684519

Epoch: 6| Step: 12
Training loss: 4.5930233335228285
Validation loss: 4.894059605262368

Epoch: 6| Step: 13
Training loss: 5.629179694412182
Validation loss: 4.888892568721735

Epoch: 12| Step: 0
Training loss: 5.280326627107496
Validation loss: 4.882874771738333

Epoch: 6| Step: 1
Training loss: 5.796847361372791
Validation loss: 4.877615235318322

Epoch: 6| Step: 2
Training loss: 4.871119788986638
Validation loss: 4.871907647239424

Epoch: 6| Step: 3
Training loss: 5.078361247509385
Validation loss: 4.866874770500909

Epoch: 6| Step: 4
Training loss: 4.699791217793055
Validation loss: 4.861529135517113

Epoch: 6| Step: 5
Training loss: 5.131166237671331
Validation loss: 4.855455459408768

Epoch: 6| Step: 6
Training loss: 4.959781061385662
Validation loss: 4.850361436308177

Epoch: 6| Step: 7
Training loss: 4.792686597109729
Validation loss: 4.844807039820312

Epoch: 6| Step: 8
Training loss: 4.976972003179702
Validation loss: 4.839002124022551

Epoch: 6| Step: 9
Training loss: 5.406705627645966
Validation loss: 4.834217215430852

Epoch: 6| Step: 10
Training loss: 4.265642885722576
Validation loss: 4.8287433446406585

Epoch: 6| Step: 11
Training loss: 4.766779925372431
Validation loss: 4.823607371273557

Epoch: 6| Step: 12
Training loss: 3.9278764134716404
Validation loss: 4.818417828528359

Epoch: 6| Step: 13
Training loss: 5.400234514018487
Validation loss: 4.813081020934807

Epoch: 13| Step: 0
Training loss: 4.94642585648083
Validation loss: 4.807543658770611

Epoch: 6| Step: 1
Training loss: 5.076664077714714
Validation loss: 4.802760751514005

Epoch: 6| Step: 2
Training loss: 5.016789286351615
Validation loss: 4.797213895508628

Epoch: 6| Step: 3
Training loss: 4.674217019837136
Validation loss: 4.792263270292062

Epoch: 6| Step: 4
Training loss: 4.92375568607464
Validation loss: 4.786945167442624

Epoch: 6| Step: 5
Training loss: 5.042064535452029
Validation loss: 4.7814808376329445

Epoch: 6| Step: 6
Training loss: 4.247897581787542
Validation loss: 4.776015164777747

Epoch: 6| Step: 7
Training loss: 4.2970039487043366
Validation loss: 4.77082679226134

Epoch: 6| Step: 8
Training loss: 4.881131351215849
Validation loss: 4.766172617315714

Epoch: 6| Step: 9
Training loss: 3.8223304957448634
Validation loss: 4.760877234996953

Epoch: 6| Step: 10
Training loss: 6.221109373999281
Validation loss: 4.756201259849538

Epoch: 6| Step: 11
Training loss: 4.651014008249345
Validation loss: 4.750920925879696

Epoch: 6| Step: 12
Training loss: 4.503196852333811
Validation loss: 4.745496053435531

Epoch: 6| Step: 13
Training loss: 5.806374552430737
Validation loss: 4.740297497052815

Epoch: 14| Step: 0
Training loss: 4.92665531592466
Validation loss: 4.735261236152592

Epoch: 6| Step: 1
Training loss: 5.503351404232406
Validation loss: 4.730191202418995

Epoch: 6| Step: 2
Training loss: 3.9290631444783806
Validation loss: 4.7247706524121895

Epoch: 6| Step: 3
Training loss: 5.067585968808968
Validation loss: 4.719586363518347

Epoch: 6| Step: 4
Training loss: 5.518639624108577
Validation loss: 4.713939075331446

Epoch: 6| Step: 5
Training loss: 4.959590314434866
Validation loss: 4.708877948332023

Epoch: 6| Step: 6
Training loss: 4.657892743852355
Validation loss: 4.703670094142751

Epoch: 6| Step: 7
Training loss: 4.8311260488778895
Validation loss: 4.6983829557286665

Epoch: 6| Step: 8
Training loss: 3.262826404832364
Validation loss: 4.693582682668736

Epoch: 6| Step: 9
Training loss: 4.898224115667299
Validation loss: 4.688541889960501

Epoch: 6| Step: 10
Training loss: 4.510991342538362
Validation loss: 4.683540638762232

Epoch: 6| Step: 11
Training loss: 4.766312946631244
Validation loss: 4.678533022136011

Epoch: 6| Step: 12
Training loss: 4.701882638677872
Validation loss: 4.673696547708707

Epoch: 6| Step: 13
Training loss: 5.558961235546668
Validation loss: 4.66828397700022

Epoch: 15| Step: 0
Training loss: 4.854088363779938
Validation loss: 4.663224290323204

Epoch: 6| Step: 1
Training loss: 3.644629637605514
Validation loss: 4.658673332149175

Epoch: 6| Step: 2
Training loss: 5.231907687209834
Validation loss: 4.6536160556870145

Epoch: 6| Step: 3
Training loss: 5.198894808770996
Validation loss: 4.6477526248359995

Epoch: 6| Step: 4
Training loss: 4.551435457938869
Validation loss: 4.64230720351134

Epoch: 6| Step: 5
Training loss: 4.017528750608126
Validation loss: 4.637860297698807

Epoch: 6| Step: 6
Training loss: 5.4718535417956256
Validation loss: 4.632811659437063

Epoch: 6| Step: 7
Training loss: 4.785658057234291
Validation loss: 4.6272071692418235

Epoch: 6| Step: 8
Training loss: 3.159666120056734
Validation loss: 4.622339737959215

Epoch: 6| Step: 9
Training loss: 5.460476093145941
Validation loss: 4.617343940817046

Epoch: 6| Step: 10
Training loss: 5.046628963744519
Validation loss: 4.612136754840774

Epoch: 6| Step: 11
Training loss: 5.38805188673562
Validation loss: 4.607668630846869

Epoch: 6| Step: 12
Training loss: 4.5229500358026495
Validation loss: 4.602894977071755

Epoch: 6| Step: 13
Training loss: 4.5954532709386635
Validation loss: 4.597201756754816

Epoch: 16| Step: 0
Training loss: 4.883759673758056
Validation loss: 4.591334306811993

Epoch: 6| Step: 1
Training loss: 2.436687627546519
Validation loss: 4.58558416873784

Epoch: 6| Step: 2
Training loss: 5.109318630829153
Validation loss: 4.581872215882431

Epoch: 6| Step: 3
Training loss: 5.153698839169157
Validation loss: 4.575816539167853

Epoch: 6| Step: 4
Training loss: 5.1479981022188825
Validation loss: 4.571043733097033

Epoch: 6| Step: 5
Training loss: 4.62110303682763
Validation loss: 4.565487601988948

Epoch: 6| Step: 6
Training loss: 4.245898567774217
Validation loss: 4.560596051383241

Epoch: 6| Step: 7
Training loss: 3.755994836960109
Validation loss: 4.555150928334785

Epoch: 6| Step: 8
Training loss: 4.997094645393414
Validation loss: 4.550209841238555

Epoch: 6| Step: 9
Training loss: 5.19340675220492
Validation loss: 4.545533287059895

Epoch: 6| Step: 10
Training loss: 5.390310084777925
Validation loss: 4.540791167587739

Epoch: 6| Step: 11
Training loss: 4.938279874994865
Validation loss: 4.534317171169279

Epoch: 6| Step: 12
Training loss: 4.203315815618093
Validation loss: 4.529241603499272

Epoch: 6| Step: 13
Training loss: 4.676999143028222
Validation loss: 4.5245085630284905

Epoch: 17| Step: 0
Training loss: 4.47409271760789
Validation loss: 4.519526045039858

Epoch: 6| Step: 1
Training loss: 4.5914186543376125
Validation loss: 4.514327552542897

Epoch: 6| Step: 2
Training loss: 3.792207965709158
Validation loss: 4.508991248213604

Epoch: 6| Step: 3
Training loss: 4.915139861033013
Validation loss: 4.503629562909105

Epoch: 6| Step: 4
Training loss: 4.002034146934833
Validation loss: 4.4982683947411495

Epoch: 6| Step: 5
Training loss: 4.652547913498833
Validation loss: 4.4934513691379205

Epoch: 6| Step: 6
Training loss: 4.048803628423241
Validation loss: 4.488443086748108

Epoch: 6| Step: 7
Training loss: 5.499478055289905
Validation loss: 4.483147855323086

Epoch: 6| Step: 8
Training loss: 4.374719883261879
Validation loss: 4.478250927428984

Epoch: 6| Step: 9
Training loss: 5.6632475167984175
Validation loss: 4.472516568819376

Epoch: 6| Step: 10
Training loss: 5.335205027851225
Validation loss: 4.467343871653319

Epoch: 6| Step: 11
Training loss: 4.269251584790744
Validation loss: 4.4621406887803206

Epoch: 6| Step: 12
Training loss: 4.064820316561683
Validation loss: 4.456501819259984

Epoch: 6| Step: 13
Training loss: 4.4346674085453195
Validation loss: 4.452134478425053

Epoch: 18| Step: 0
Training loss: 5.393408722761222
Validation loss: 4.446614303328215

Epoch: 6| Step: 1
Training loss: 5.077652472005626
Validation loss: 4.441057579022877

Epoch: 6| Step: 2
Training loss: 4.746773678148401
Validation loss: 4.435699939536387

Epoch: 6| Step: 3
Training loss: 3.5743092009559114
Validation loss: 4.430067347724313

Epoch: 6| Step: 4
Training loss: 4.506580627417645
Validation loss: 4.425567523030153

Epoch: 6| Step: 5
Training loss: 3.8968519759243647
Validation loss: 4.419660932789252

Epoch: 6| Step: 6
Training loss: 4.5175810804411
Validation loss: 4.415123303982311

Epoch: 6| Step: 7
Training loss: 5.248909246577673
Validation loss: 4.40988808000426

Epoch: 6| Step: 8
Training loss: 3.6267180482179104
Validation loss: 4.404539204519165

Epoch: 6| Step: 9
Training loss: 3.994387145700673
Validation loss: 4.400238174436746

Epoch: 6| Step: 10
Training loss: 4.468834296011653
Validation loss: 4.394888838171223

Epoch: 6| Step: 11
Training loss: 4.6904688270996076
Validation loss: 4.389934716314934

Epoch: 6| Step: 12
Training loss: 4.624010753513491
Validation loss: 4.385192696235093

Epoch: 6| Step: 13
Training loss: 4.769498457862292
Validation loss: 4.3794432828862595

Epoch: 19| Step: 0
Training loss: 4.882318725033476
Validation loss: 4.3740146072206585

Epoch: 6| Step: 1
Training loss: 4.651271539570062
Validation loss: 4.3692015596886105

Epoch: 6| Step: 2
Training loss: 4.340132050504951
Validation loss: 4.364088238272482

Epoch: 6| Step: 3
Training loss: 3.6165765695439136
Validation loss: 4.359242354076065

Epoch: 6| Step: 4
Training loss: 4.323533249357611
Validation loss: 4.3544019547306165

Epoch: 6| Step: 5
Training loss: 4.089365234363339
Validation loss: 4.34921341869406

Epoch: 6| Step: 6
Training loss: 3.735565761832352
Validation loss: 4.344201080539154

Epoch: 6| Step: 7
Training loss: 3.772860970692507
Validation loss: 4.339564239171814

Epoch: 6| Step: 8
Training loss: 5.27936588063022
Validation loss: 4.334510435497157

Epoch: 6| Step: 9
Training loss: 5.247871148958341
Validation loss: 4.329724692152

Epoch: 6| Step: 10
Training loss: 3.799976228338475
Validation loss: 4.324076038223705

Epoch: 6| Step: 11
Training loss: 5.406470498235518
Validation loss: 4.319653994897078

Epoch: 6| Step: 12
Training loss: 4.278080790523262
Validation loss: 4.314119891676436

Epoch: 6| Step: 13
Training loss: 4.640287136532895
Validation loss: 4.309488286075442

Epoch: 20| Step: 0
Training loss: 4.627157378287709
Validation loss: 4.3044608553226364

Epoch: 6| Step: 1
Training loss: 4.167360985444843
Validation loss: 4.299208461742926

Epoch: 6| Step: 2
Training loss: 4.799700473340921
Validation loss: 4.294584109798909

Epoch: 6| Step: 3
Training loss: 4.814096161335577
Validation loss: 4.289857958536974

Epoch: 6| Step: 4
Training loss: 4.1839030745386285
Validation loss: 4.284390970316735

Epoch: 6| Step: 5
Training loss: 4.686725196382464
Validation loss: 4.279584314653662

Epoch: 6| Step: 6
Training loss: 4.713120291686934
Validation loss: 4.274162049737585

Epoch: 6| Step: 7
Training loss: 4.372450603469999
Validation loss: 4.268992844651338

Epoch: 6| Step: 8
Training loss: 4.117031376754097
Validation loss: 4.26405954105754

Epoch: 6| Step: 9
Training loss: 4.3465652673591695
Validation loss: 4.25966569893208

Epoch: 6| Step: 10
Training loss: 4.511130660400458
Validation loss: 4.253508167071716

Epoch: 6| Step: 11
Training loss: 4.266071673471199
Validation loss: 4.249151294713115

Epoch: 6| Step: 12
Training loss: 4.579441128744396
Validation loss: 4.243429771330338

Epoch: 6| Step: 13
Training loss: 3.207985665115711
Validation loss: 4.238816150481717

Epoch: 21| Step: 0
Training loss: 3.8092791600802096
Validation loss: 4.234574729647697

Epoch: 6| Step: 1
Training loss: 5.044046560122497
Validation loss: 4.229419506513972

Epoch: 6| Step: 2
Training loss: 3.4908305354382985
Validation loss: 4.22421623537038

Epoch: 6| Step: 3
Training loss: 4.6990665401627325
Validation loss: 4.219646650559813

Epoch: 6| Step: 4
Training loss: 4.444357073772801
Validation loss: 4.2150980649063605

Epoch: 6| Step: 5
Training loss: 3.8103172432491315
Validation loss: 4.210044204618778

Epoch: 6| Step: 6
Training loss: 3.6505027006771775
Validation loss: 4.205634142232267

Epoch: 6| Step: 7
Training loss: 5.73112042029939
Validation loss: 4.200688881615608

Epoch: 6| Step: 8
Training loss: 4.783376981068665
Validation loss: 4.195882406897966

Epoch: 6| Step: 9
Training loss: 4.3287414772771555
Validation loss: 4.191068011712097

Epoch: 6| Step: 10
Training loss: 4.629365870838993
Validation loss: 4.186106340782071

Epoch: 6| Step: 11
Training loss: 3.8569390026092756
Validation loss: 4.180683604349718

Epoch: 6| Step: 12
Training loss: 4.168873596646861
Validation loss: 4.175891804577264

Epoch: 6| Step: 13
Training loss: 3.611385429591992
Validation loss: 4.17126302242866

Epoch: 22| Step: 0
Training loss: 3.8471545516824834
Validation loss: 4.166705601828178

Epoch: 6| Step: 1
Training loss: 4.777676113042584
Validation loss: 4.161740474502198

Epoch: 6| Step: 2
Training loss: 3.989631685739174
Validation loss: 4.157449668660668

Epoch: 6| Step: 3
Training loss: 4.216973729066927
Validation loss: 4.1527067870866325

Epoch: 6| Step: 4
Training loss: 4.741678879939453
Validation loss: 4.147276500974964

Epoch: 6| Step: 5
Training loss: 3.6924246538722936
Validation loss: 4.143294600236405

Epoch: 6| Step: 6
Training loss: 4.471655268039078
Validation loss: 4.1377876396883435

Epoch: 6| Step: 7
Training loss: 4.906647489931807
Validation loss: 4.1330506533178415

Epoch: 6| Step: 8
Training loss: 3.5642523220408147
Validation loss: 4.127894744111474

Epoch: 6| Step: 9
Training loss: 4.36314792139065
Validation loss: 4.123893743582362

Epoch: 6| Step: 10
Training loss: 4.4538926918453745
Validation loss: 4.118513729745483

Epoch: 6| Step: 11
Training loss: 4.2497863715840545
Validation loss: 4.11368229653736

Epoch: 6| Step: 12
Training loss: 4.464329380366804
Validation loss: 4.109254262332617

Epoch: 6| Step: 13
Training loss: 3.7330585537560865
Validation loss: 4.103844115206086

Epoch: 23| Step: 0
Training loss: 3.998443062090089
Validation loss: 4.09959281977412

Epoch: 6| Step: 1
Training loss: 4.6383807474432235
Validation loss: 4.094699157482424

Epoch: 6| Step: 2
Training loss: 4.650814288634119
Validation loss: 4.090046515382926

Epoch: 6| Step: 3
Training loss: 4.5739143459238125
Validation loss: 4.085387290733557

Epoch: 6| Step: 4
Training loss: 4.020280447519673
Validation loss: 4.080891865414448

Epoch: 6| Step: 5
Training loss: 3.569114834764406
Validation loss: 4.075858215732364

Epoch: 6| Step: 6
Training loss: 4.2695980363457835
Validation loss: 4.070728712712558

Epoch: 6| Step: 7
Training loss: 4.075604240693374
Validation loss: 4.066019011084499

Epoch: 6| Step: 8
Training loss: 3.587351268915119
Validation loss: 4.061949492103733

Epoch: 6| Step: 9
Training loss: 4.421659322809955
Validation loss: 4.057333708101728

Epoch: 6| Step: 10
Training loss: 5.068108736577332
Validation loss: 4.052420313665703

Epoch: 6| Step: 11
Training loss: 3.658787816578486
Validation loss: 4.047993234226669

Epoch: 6| Step: 12
Training loss: 4.091133970765079
Validation loss: 4.042827550907175

Epoch: 6| Step: 13
Training loss: 3.9089072992863443
Validation loss: 4.03836474611222

Epoch: 24| Step: 0
Training loss: 3.8981617923704848
Validation loss: 4.0338096279626745

Epoch: 6| Step: 1
Training loss: 4.358733536755096
Validation loss: 4.029427976658923

Epoch: 6| Step: 2
Training loss: 4.699252235502578
Validation loss: 4.024224579005264

Epoch: 6| Step: 3
Training loss: 4.0786922834908985
Validation loss: 4.020037827669632

Epoch: 6| Step: 4
Training loss: 3.0448642453654773
Validation loss: 4.016507003639409

Epoch: 6| Step: 5
Training loss: 3.740555377462062
Validation loss: 4.012829887638249

Epoch: 6| Step: 6
Training loss: 4.40501743689906
Validation loss: 4.009095520126185

Epoch: 6| Step: 7
Training loss: 4.652441938182483
Validation loss: 4.004093975845617

Epoch: 6| Step: 8
Training loss: 4.063631340366662
Validation loss: 3.9994519374811546

Epoch: 6| Step: 9
Training loss: 4.237829396996401
Validation loss: 3.994862853820562

Epoch: 6| Step: 10
Training loss: 3.002291598873355
Validation loss: 3.9904080062336207

Epoch: 6| Step: 11
Training loss: 4.7762412969314765
Validation loss: 3.9854577681394043

Epoch: 6| Step: 12
Training loss: 4.263720412071989
Validation loss: 3.981436110557543

Epoch: 6| Step: 13
Training loss: 4.263038829407393
Validation loss: 3.976097894193886

Epoch: 25| Step: 0
Training loss: 3.8830861047024445
Validation loss: 3.9711839316314292

Epoch: 6| Step: 1
Training loss: 4.358055215766762
Validation loss: 3.9667571062842697

Epoch: 6| Step: 2
Training loss: 4.060498727693747
Validation loss: 3.9615960511068753

Epoch: 6| Step: 3
Training loss: 3.176238117458511
Validation loss: 3.9566116738186823

Epoch: 6| Step: 4
Training loss: 4.176195514852182
Validation loss: 3.952026717216643

Epoch: 6| Step: 5
Training loss: 4.21754133076476
Validation loss: 3.947494860068565

Epoch: 6| Step: 6
Training loss: 5.097781592687945
Validation loss: 3.942999959597554

Epoch: 6| Step: 7
Training loss: 3.707666372834937
Validation loss: 3.938068691945493

Epoch: 6| Step: 8
Training loss: 4.278690213301992
Validation loss: 3.933238331945965

Epoch: 6| Step: 9
Training loss: 4.234836004295369
Validation loss: 3.92911215400839

Epoch: 6| Step: 10
Training loss: 3.983661423157167
Validation loss: 3.924106140566439

Epoch: 6| Step: 11
Training loss: 4.210498567539937
Validation loss: 3.9197207913904397

Epoch: 6| Step: 12
Training loss: 3.5515070174549477
Validation loss: 3.914372394448626

Epoch: 6| Step: 13
Training loss: 3.8020219493073864
Validation loss: 3.9098744578260165

Epoch: 26| Step: 0
Training loss: 4.057837525041961
Validation loss: 3.90552280014963

Epoch: 6| Step: 1
Training loss: 4.080014079948515
Validation loss: 3.901341215006632

Epoch: 6| Step: 2
Training loss: 3.454304903498388
Validation loss: 3.8965230457040962

Epoch: 6| Step: 3
Training loss: 4.5959029569681045
Validation loss: 3.8918735487280602

Epoch: 6| Step: 4
Training loss: 3.3852011900576855
Validation loss: 3.8872717893614848

Epoch: 6| Step: 5
Training loss: 3.5826869352008948
Validation loss: 3.882925235332083

Epoch: 6| Step: 6
Training loss: 3.881825004898127
Validation loss: 3.8785561271993907

Epoch: 6| Step: 7
Training loss: 4.498106876160276
Validation loss: 3.874299221865929

Epoch: 6| Step: 8
Training loss: 4.183317458488669
Validation loss: 3.8696164912557154

Epoch: 6| Step: 9
Training loss: 4.4776156320129665
Validation loss: 3.865081069576392

Epoch: 6| Step: 10
Training loss: 4.087918386715933
Validation loss: 3.8607699695582767

Epoch: 6| Step: 11
Training loss: 3.2622379826238057
Validation loss: 3.855954217927855

Epoch: 6| Step: 12
Training loss: 4.899539318616391
Validation loss: 3.8510496095935376

Epoch: 6| Step: 13
Training loss: 3.243460899214962
Validation loss: 3.8463806206532354

Epoch: 27| Step: 0
Training loss: 4.435284128473954
Validation loss: 3.841161605888786

Epoch: 6| Step: 1
Training loss: 4.3743189690363335
Validation loss: 3.836697919360906

Epoch: 6| Step: 2
Training loss: 3.8531292868139424
Validation loss: 3.8322284529790687

Epoch: 6| Step: 3
Training loss: 4.604775827193021
Validation loss: 3.8275825790908717

Epoch: 6| Step: 4
Training loss: 3.49071934366756
Validation loss: 3.822824724773917

Epoch: 6| Step: 5
Training loss: 4.392136934546399
Validation loss: 3.8173939921131703

Epoch: 6| Step: 6
Training loss: 3.9279746233942
Validation loss: 3.8129742853708377

Epoch: 6| Step: 7
Training loss: 3.8256886728687567
Validation loss: 3.8084620369852975

Epoch: 6| Step: 8
Training loss: 3.2276189889403852
Validation loss: 3.8040173284347882

Epoch: 6| Step: 9
Training loss: 3.889863701911135
Validation loss: 3.79951489932658

Epoch: 6| Step: 10
Training loss: 3.0613288975771002
Validation loss: 3.7952371421505533

Epoch: 6| Step: 11
Training loss: 3.8417349394870905
Validation loss: 3.7907800301520416

Epoch: 6| Step: 12
Training loss: 4.056921312358421
Validation loss: 3.7859959309081335

Epoch: 6| Step: 13
Training loss: 3.9595836605497525
Validation loss: 3.7812522068490138

Epoch: 28| Step: 0
Training loss: 3.837418093723643
Validation loss: 3.7771292056485612

Epoch: 6| Step: 1
Training loss: 3.180149803951652
Validation loss: 3.772683394052601

Epoch: 6| Step: 2
Training loss: 4.351771132229346
Validation loss: 3.7682319048819077

Epoch: 6| Step: 3
Training loss: 4.571229291727645
Validation loss: 3.763436701233202

Epoch: 6| Step: 4
Training loss: 4.3064444222597045
Validation loss: 3.7590616731407303

Epoch: 6| Step: 5
Training loss: 4.17208804611681
Validation loss: 3.753966437845352

Epoch: 6| Step: 6
Training loss: 3.938647269577202
Validation loss: 3.7491341332994517

Epoch: 6| Step: 7
Training loss: 3.372841886775672
Validation loss: 3.7442193511396535

Epoch: 6| Step: 8
Training loss: 4.406907918523344
Validation loss: 3.739686218943238

Epoch: 6| Step: 9
Training loss: 3.45496895694726
Validation loss: 3.7349300637053244

Epoch: 6| Step: 10
Training loss: 3.8875963303384693
Validation loss: 3.730046614218427

Epoch: 6| Step: 11
Training loss: 2.9801490782245077
Validation loss: 3.7253326583234556

Epoch: 6| Step: 12
Training loss: 4.088220487318375
Validation loss: 3.720715769882275

Epoch: 6| Step: 13
Training loss: 3.430551337592603
Validation loss: 3.716630347606158

Epoch: 29| Step: 0
Training loss: 4.533842621039411
Validation loss: 3.7120931331773352

Epoch: 6| Step: 1
Training loss: 4.296517652044057
Validation loss: 3.7079188011589954

Epoch: 6| Step: 2
Training loss: 3.6668352319522257
Validation loss: 3.703211701527659

Epoch: 6| Step: 3
Training loss: 3.956398916229047
Validation loss: 3.6984881465416444

Epoch: 6| Step: 4
Training loss: 2.711276953823957
Validation loss: 3.6940726632525753

Epoch: 6| Step: 5
Training loss: 3.706882678834213
Validation loss: 3.6895495823343727

Epoch: 6| Step: 6
Training loss: 3.5759790640437887
Validation loss: 3.6853744717186685

Epoch: 6| Step: 7
Training loss: 3.42172589260251
Validation loss: 3.6811308085343852

Epoch: 6| Step: 8
Training loss: 3.901160502777682
Validation loss: 3.676519910892916

Epoch: 6| Step: 9
Training loss: 3.637940821203542
Validation loss: 3.6723849544419016

Epoch: 6| Step: 10
Training loss: 4.64159858329512
Validation loss: 3.668255974489602

Epoch: 6| Step: 11
Training loss: 2.916115699680961
Validation loss: 3.6644652795273207

Epoch: 6| Step: 12
Training loss: 4.136764857509806
Validation loss: 3.6596800970411385

Epoch: 6| Step: 13
Training loss: 3.882197555876569
Validation loss: 3.654996477743766

Epoch: 30| Step: 0
Training loss: 4.365814375268577
Validation loss: 3.650613031335599

Epoch: 6| Step: 1
Training loss: 3.4802836530554657
Validation loss: 3.6460950194403545

Epoch: 6| Step: 2
Training loss: 3.305288706532716
Validation loss: 3.641769818577092

Epoch: 6| Step: 3
Training loss: 4.517852128390648
Validation loss: 3.6382102762248247

Epoch: 6| Step: 4
Training loss: 3.6726927800279903
Validation loss: 3.6332961987572174

Epoch: 6| Step: 5
Training loss: 4.459004051642963
Validation loss: 3.628472758429496

Epoch: 6| Step: 6
Training loss: 3.846734229827207
Validation loss: 3.624144244855259

Epoch: 6| Step: 7
Training loss: 4.090538104411518
Validation loss: 3.6195660125141176

Epoch: 6| Step: 8
Training loss: 3.753696018965229
Validation loss: 3.615360487942225

Epoch: 6| Step: 9
Training loss: 3.5156909173507804
Validation loss: 3.610579362664152

Epoch: 6| Step: 10
Training loss: 3.5077567658956235
Validation loss: 3.6054108223441026

Epoch: 6| Step: 11
Training loss: 2.780046695618938
Validation loss: 3.6009106490923353

Epoch: 6| Step: 12
Training loss: 3.6144671829133297
Validation loss: 3.5965928926818074

Epoch: 6| Step: 13
Training loss: 3.3276249326299894
Validation loss: 3.5922638847257424

Epoch: 31| Step: 0
Training loss: 3.4682886315147043
Validation loss: 3.5874931601216575

Epoch: 6| Step: 1
Training loss: 4.380932953990882
Validation loss: 3.583620318667804

Epoch: 6| Step: 2
Training loss: 2.819875329285176
Validation loss: 3.5795876679860275

Epoch: 6| Step: 3
Training loss: 4.255040713697077
Validation loss: 3.5749424269515258

Epoch: 6| Step: 4
Training loss: 3.969887878218351
Validation loss: 3.5708089159231204

Epoch: 6| Step: 5
Training loss: 3.840273641929369
Validation loss: 3.566241457955994

Epoch: 6| Step: 6
Training loss: 3.7557572992174295
Validation loss: 3.561207140736835

Epoch: 6| Step: 7
Training loss: 2.4879615853830614
Validation loss: 3.5570760785406033

Epoch: 6| Step: 8
Training loss: 3.828765940046117
Validation loss: 3.552926557284132

Epoch: 6| Step: 9
Training loss: 3.443303705502202
Validation loss: 3.5486261691383407

Epoch: 6| Step: 10
Training loss: 3.8302878984257385
Validation loss: 3.5442840254531345

Epoch: 6| Step: 11
Training loss: 3.638126154225895
Validation loss: 3.5401370316903478

Epoch: 6| Step: 12
Training loss: 3.426914771359027
Validation loss: 3.535926355604814

Epoch: 6| Step: 13
Training loss: 4.155058037344506
Validation loss: 3.531699813055944

Epoch: 32| Step: 0
Training loss: 3.8874382234920835
Validation loss: 3.527442567440235

Epoch: 6| Step: 1
Training loss: 4.043665021566527
Validation loss: 3.523599625132984

Epoch: 6| Step: 2
Training loss: 4.160944073364245
Validation loss: 3.519043702718779

Epoch: 6| Step: 3
Training loss: 2.992947554108743
Validation loss: 3.514685362529763

Epoch: 6| Step: 4
Training loss: 4.17982166110617
Validation loss: 3.5103403115198786

Epoch: 6| Step: 5
Training loss: 3.764928982940054
Validation loss: 3.5062071164659856

Epoch: 6| Step: 6
Training loss: 2.999841367978018
Validation loss: 3.501851545776495

Epoch: 6| Step: 7
Training loss: 3.6252678081637226
Validation loss: 3.4978580959945704

Epoch: 6| Step: 8
Training loss: 4.201230895099143
Validation loss: 3.4938830100702365

Epoch: 6| Step: 9
Training loss: 3.6582659518930924
Validation loss: 3.488911342919805

Epoch: 6| Step: 10
Training loss: 2.0471767756356325
Validation loss: 3.48478176325728

Epoch: 6| Step: 11
Training loss: 3.9222112868615557
Validation loss: 3.4806899354740577

Epoch: 6| Step: 12
Training loss: 2.885521585000688
Validation loss: 3.47723421044115

Epoch: 6| Step: 13
Training loss: 3.883293505834141
Validation loss: 3.472687831602034

Epoch: 33| Step: 0
Training loss: 3.4645495026374764
Validation loss: 3.468596927598922

Epoch: 6| Step: 1
Training loss: 3.3764371284676864
Validation loss: 3.4644771182687895

Epoch: 6| Step: 2
Training loss: 4.1234604245466855
Validation loss: 3.4609261793148067

Epoch: 6| Step: 3
Training loss: 3.7732497142876817
Validation loss: 3.4570673421638785

Epoch: 6| Step: 4
Training loss: 3.149044763958594
Validation loss: 3.4531754019670777

Epoch: 6| Step: 5
Training loss: 4.077861675077326
Validation loss: 3.4491641888851885

Epoch: 6| Step: 6
Training loss: 3.945597602438527
Validation loss: 3.4450549593833766

Epoch: 6| Step: 7
Training loss: 3.7701287779328263
Validation loss: 3.4407919870634656

Epoch: 6| Step: 8
Training loss: 3.3051272701626266
Validation loss: 3.436731472224525

Epoch: 6| Step: 9
Training loss: 3.4893635846688698
Validation loss: 3.4328119156700523

Epoch: 6| Step: 10
Training loss: 3.8507534825567387
Validation loss: 3.42903159900285

Epoch: 6| Step: 11
Training loss: 3.4462212642098433
Validation loss: 3.424526981073138

Epoch: 6| Step: 12
Training loss: 3.1865828447477136
Validation loss: 3.4204390719505504

Epoch: 6| Step: 13
Training loss: 2.965359970807883
Validation loss: 3.4164459265757308

Epoch: 34| Step: 0
Training loss: 3.961292860353584
Validation loss: 3.4127672009313925

Epoch: 6| Step: 1
Training loss: 4.085734433439807
Validation loss: 3.4085904401386706

Epoch: 6| Step: 2
Training loss: 4.163957605239566
Validation loss: 3.404405952146228

Epoch: 6| Step: 3
Training loss: 3.576886491612077
Validation loss: 3.4001809277733184

Epoch: 6| Step: 4
Training loss: 3.547011318191956
Validation loss: 3.395559502699052

Epoch: 6| Step: 5
Training loss: 3.740247761276633
Validation loss: 3.3913045574279956

Epoch: 6| Step: 6
Training loss: 3.632993369573396
Validation loss: 3.387169232706787

Epoch: 6| Step: 7
Training loss: 3.4696893193452234
Validation loss: 3.382773501176519

Epoch: 6| Step: 8
Training loss: 2.733359709666695
Validation loss: 3.378882706106257

Epoch: 6| Step: 9
Training loss: 3.098416594695928
Validation loss: 3.374907480549426

Epoch: 6| Step: 10
Training loss: 2.921381291584149
Validation loss: 3.3708331912448948

Epoch: 6| Step: 11
Training loss: 3.3783288715962283
Validation loss: 3.3669336493230886

Epoch: 6| Step: 12
Training loss: 2.997338704024059
Validation loss: 3.362898736542868

Epoch: 6| Step: 13
Training loss: 3.746402540795725
Validation loss: 3.3591561179601497

Epoch: 35| Step: 0
Training loss: 2.6466521150139313
Validation loss: 3.35538317653027

Epoch: 6| Step: 1
Training loss: 3.3760324947456612
Validation loss: 3.3522808784107547

Epoch: 6| Step: 2
Training loss: 3.0724071705314717
Validation loss: 3.3487110353153504

Epoch: 6| Step: 3
Training loss: 3.2310335894780557
Validation loss: 3.3454661020919914

Epoch: 6| Step: 4
Training loss: 3.9469094849542565
Validation loss: 3.342184395452041

Epoch: 6| Step: 5
Training loss: 3.7262145705715795
Validation loss: 3.3383265687363246

Epoch: 6| Step: 6
Training loss: 3.7525143141603756
Validation loss: 3.3347866982797383

Epoch: 6| Step: 7
Training loss: 3.515389396402254
Validation loss: 3.331625898479759

Epoch: 6| Step: 8
Training loss: 3.1317846272008993
Validation loss: 3.3277807162113904

Epoch: 6| Step: 9
Training loss: 3.6887089961845345
Validation loss: 3.3244655710251414

Epoch: 6| Step: 10
Training loss: 3.6670621456376034
Validation loss: 3.3207591515603347

Epoch: 6| Step: 11
Training loss: 3.525925394429803
Validation loss: 3.317070980124888

Epoch: 6| Step: 12
Training loss: 3.2036514966022955
Validation loss: 3.313554643887185

Epoch: 6| Step: 13
Training loss: 3.8683617860820645
Validation loss: 3.3097302115016625

Epoch: 36| Step: 0
Training loss: 3.394062040536349
Validation loss: 3.3058028386867306

Epoch: 6| Step: 1
Training loss: 3.5791704408021032
Validation loss: 3.302144207478536

Epoch: 6| Step: 2
Training loss: 3.1927778641979314
Validation loss: 3.298393538588817

Epoch: 6| Step: 3
Training loss: 3.4506150181229867
Validation loss: 3.294917545492889

Epoch: 6| Step: 4
Training loss: 4.066283826056997
Validation loss: 3.291081356369415

Epoch: 6| Step: 5
Training loss: 3.192343827432883
Validation loss: 3.286764034959414

Epoch: 6| Step: 6
Training loss: 3.5782036252107954
Validation loss: 3.2831557658310633

Epoch: 6| Step: 7
Training loss: 2.6879447746393956
Validation loss: 3.2793086651727763

Epoch: 6| Step: 8
Training loss: 3.3198132666962494
Validation loss: 3.2760840424837934

Epoch: 6| Step: 9
Training loss: 3.6454778588794197
Validation loss: 3.272171321883914

Epoch: 6| Step: 10
Training loss: 3.35401154785352
Validation loss: 3.268593420067603

Epoch: 6| Step: 11
Training loss: 3.6068941648400803
Validation loss: 3.26500797070187

Epoch: 6| Step: 12
Training loss: 3.4862414095483136
Validation loss: 3.2613597801781

Epoch: 6| Step: 13
Training loss: 3.1836308951053875
Validation loss: 3.257163223224441

Epoch: 37| Step: 0
Training loss: 2.924144822005825
Validation loss: 3.2536753144731767

Epoch: 6| Step: 1
Training loss: 3.5484802377899527
Validation loss: 3.2503037188516215

Epoch: 6| Step: 2
Training loss: 3.8034555684896083
Validation loss: 3.246752804025397

Epoch: 6| Step: 3
Training loss: 2.9328062826668915
Validation loss: 3.243395268638547

Epoch: 6| Step: 4
Training loss: 3.5716187072277656
Validation loss: 3.2399437919497767

Epoch: 6| Step: 5
Training loss: 3.269125198800084
Validation loss: 3.2366224334305502

Epoch: 6| Step: 6
Training loss: 2.7017929234812375
Validation loss: 3.232881456109549

Epoch: 6| Step: 7
Training loss: 2.7955573190657583
Validation loss: 3.229537245293164

Epoch: 6| Step: 8
Training loss: 3.612398276827294
Validation loss: 3.2262240548699777

Epoch: 6| Step: 9
Training loss: 3.7389294292588002
Validation loss: 3.2230368649525274

Epoch: 6| Step: 10
Training loss: 3.083398783908418
Validation loss: 3.2198020980963817

Epoch: 6| Step: 11
Training loss: 3.732123170854193
Validation loss: 3.2163144251487417

Epoch: 6| Step: 12
Training loss: 3.8351878503899024
Validation loss: 3.2131490276897323

Epoch: 6| Step: 13
Training loss: 3.3504061893912405
Validation loss: 3.2089820222542222

Epoch: 38| Step: 0
Training loss: 3.307371235688794
Validation loss: 3.205646545849449

Epoch: 6| Step: 1
Training loss: 2.80292023896459
Validation loss: 3.201887980458451

Epoch: 6| Step: 2
Training loss: 3.2727291824836167
Validation loss: 3.198082181310787

Epoch: 6| Step: 3
Training loss: 3.0765911482624557
Validation loss: 3.194708483375601

Epoch: 6| Step: 4
Training loss: 3.5779432225592154
Validation loss: 3.1910428572179286

Epoch: 6| Step: 5
Training loss: 2.904074398054574
Validation loss: 3.187651237938976

Epoch: 6| Step: 6
Training loss: 3.238851941005015
Validation loss: 3.184403355166169

Epoch: 6| Step: 7
Training loss: 2.864097156304575
Validation loss: 3.1809352017018426

Epoch: 6| Step: 8
Training loss: 3.558876738199191
Validation loss: 3.1778472899086476

Epoch: 6| Step: 9
Training loss: 4.3163439439790485
Validation loss: 3.1744715769016993

Epoch: 6| Step: 10
Training loss: 3.448327231188968
Validation loss: 3.171320147913216

Epoch: 6| Step: 11
Training loss: 3.1303503392527747
Validation loss: 3.167891043992946

Epoch: 6| Step: 12
Training loss: 3.0300732717687864
Validation loss: 3.1647600448991002

Epoch: 6| Step: 13
Training loss: 3.6796339804610425
Validation loss: 3.1607793658116696

Epoch: 39| Step: 0
Training loss: 3.035650775148899
Validation loss: 3.1574780810096974

Epoch: 6| Step: 1
Training loss: 2.760555642906074
Validation loss: 3.154131464759657

Epoch: 6| Step: 2
Training loss: 3.154846634802322
Validation loss: 3.150524738040488

Epoch: 6| Step: 3
Training loss: 2.980574979676347
Validation loss: 3.1471419402078427

Epoch: 6| Step: 4
Training loss: 3.121934226145376
Validation loss: 3.1443821210230216

Epoch: 6| Step: 5
Training loss: 3.0514413898457993
Validation loss: 3.140785580099399

Epoch: 6| Step: 6
Training loss: 3.362484934039203
Validation loss: 3.1377246553355187

Epoch: 6| Step: 7
Training loss: 3.5708274553414525
Validation loss: 3.1347239205348254

Epoch: 6| Step: 8
Training loss: 3.3328492925788673
Validation loss: 3.131660890301515

Epoch: 6| Step: 9
Training loss: 3.6886708615176516
Validation loss: 3.128395638967444

Epoch: 6| Step: 10
Training loss: 3.3057138278658007
Validation loss: 3.125272287586164

Epoch: 6| Step: 11
Training loss: 3.5744083208350967
Validation loss: 3.12181295002931

Epoch: 6| Step: 12
Training loss: 3.227237510733227
Validation loss: 3.118597376267193

Epoch: 6| Step: 13
Training loss: 3.568123110678636
Validation loss: 3.115258610390969

Epoch: 40| Step: 0
Training loss: 3.6126552720013083
Validation loss: 3.1116943910082537

Epoch: 6| Step: 1
Training loss: 2.99019993040253
Validation loss: 3.108644537052668

Epoch: 6| Step: 2
Training loss: 2.915374705865122
Validation loss: 3.1052680824432435

Epoch: 6| Step: 3
Training loss: 3.0445529011932218
Validation loss: 3.102482144501378

Epoch: 6| Step: 4
Training loss: 2.8746360880205
Validation loss: 3.099490622745693

Epoch: 6| Step: 5
Training loss: 2.72861418822515
Validation loss: 3.096799458577766

Epoch: 6| Step: 6
Training loss: 3.189218992573695
Validation loss: 3.0938152473325355

Epoch: 6| Step: 7
Training loss: 3.140587953567261
Validation loss: 3.0907496664722856

Epoch: 6| Step: 8
Training loss: 3.7352499475557632
Validation loss: 3.0877577937890566

Epoch: 6| Step: 9
Training loss: 3.5747792956110036
Validation loss: 3.084666599659795

Epoch: 6| Step: 10
Training loss: 2.8257923939142127
Validation loss: 3.0815543420999116

Epoch: 6| Step: 11
Training loss: 3.3789385667258762
Validation loss: 3.078770293936712

Epoch: 6| Step: 12
Training loss: 3.711956069094396
Validation loss: 3.0757553351417326

Epoch: 6| Step: 13
Training loss: 3.307105800582303
Validation loss: 3.0725736049384866

Epoch: 41| Step: 0
Training loss: 3.565344628453344
Validation loss: 3.0694485161372165

Epoch: 6| Step: 1
Training loss: 3.2684602977389074
Validation loss: 3.0663701988741305

Epoch: 6| Step: 2
Training loss: 2.7881933402036445
Validation loss: 3.0634195932302535

Epoch: 6| Step: 3
Training loss: 3.975211702863203
Validation loss: 3.059976744968585

Epoch: 6| Step: 4
Training loss: 2.695032519850278
Validation loss: 3.056877879399631

Epoch: 6| Step: 5
Training loss: 2.8358080285313068
Validation loss: 3.0540066193561146

Epoch: 6| Step: 6
Training loss: 3.399166756795396
Validation loss: 3.0509280903311744

Epoch: 6| Step: 7
Training loss: 3.189948842944894
Validation loss: 3.048020967344654

Epoch: 6| Step: 8
Training loss: 3.1113360944263913
Validation loss: 3.044920387292172

Epoch: 6| Step: 9
Training loss: 2.7173540762963437
Validation loss: 3.042159358493483

Epoch: 6| Step: 10
Training loss: 3.5479414760747345
Validation loss: 3.0395239681506445

Epoch: 6| Step: 11
Training loss: 3.3567821343672906
Validation loss: 3.036734872115533

Epoch: 6| Step: 12
Training loss: 3.25075404883026
Validation loss: 3.0338368864208567

Epoch: 6| Step: 13
Training loss: 2.686166831850761
Validation loss: 3.0310079078088714

Epoch: 42| Step: 0
Training loss: 3.392558728625468
Validation loss: 3.028695829617878

Epoch: 6| Step: 1
Training loss: 3.4142095678152837
Validation loss: 3.0254504643419113

Epoch: 6| Step: 2
Training loss: 3.049251627176715
Validation loss: 3.0227758191187326

Epoch: 6| Step: 3
Training loss: 3.2272404658112976
Validation loss: 3.019761906179259

Epoch: 6| Step: 4
Training loss: 2.578854174522978
Validation loss: 3.016771141547404

Epoch: 6| Step: 5
Training loss: 3.0942425576569192
Validation loss: 3.0145051817308204

Epoch: 6| Step: 6
Training loss: 3.520367894454312
Validation loss: 3.0115881667666824

Epoch: 6| Step: 7
Training loss: 3.0235641413540946
Validation loss: 3.008718577063566

Epoch: 6| Step: 8
Training loss: 3.4157353194029536
Validation loss: 3.0059283147205504

Epoch: 6| Step: 9
Training loss: 2.8531782197532487
Validation loss: 3.0035687386373433

Epoch: 6| Step: 10
Training loss: 3.249409842094317
Validation loss: 3.0008492724683817

Epoch: 6| Step: 11
Training loss: 2.9051334328135723
Validation loss: 2.998014720367402

Epoch: 6| Step: 12
Training loss: 3.4207901301813437
Validation loss: 2.9955551810630667

Epoch: 6| Step: 13
Training loss: 2.844057339000389
Validation loss: 2.9932105802869757

Epoch: 43| Step: 0
Training loss: 3.554115199036528
Validation loss: 2.9904445688101933

Epoch: 6| Step: 1
Training loss: 2.9651002634731816
Validation loss: 2.9874912921560477

Epoch: 6| Step: 2
Training loss: 3.4643747040185695
Validation loss: 2.9851436191089618

Epoch: 6| Step: 3
Training loss: 2.8201044486702815
Validation loss: 2.982283013789053

Epoch: 6| Step: 4
Training loss: 3.158211882041817
Validation loss: 2.9798460676985563

Epoch: 6| Step: 5
Training loss: 3.0921302948180944
Validation loss: 2.977331174051322

Epoch: 6| Step: 6
Training loss: 2.883235910640378
Validation loss: 2.974527512757891

Epoch: 6| Step: 7
Training loss: 3.161344740878272
Validation loss: 2.9717389235712766

Epoch: 6| Step: 8
Training loss: 2.5585987819010634
Validation loss: 2.9694655643549073

Epoch: 6| Step: 9
Training loss: 2.944915178045675
Validation loss: 2.966931358237669

Epoch: 6| Step: 10
Training loss: 3.355918347898779
Validation loss: 2.96461450413401

Epoch: 6| Step: 11
Training loss: 3.025923621041347
Validation loss: 2.962131309905968

Epoch: 6| Step: 12
Training loss: 3.326253732064901
Validation loss: 2.95982023043349

Epoch: 6| Step: 13
Training loss: 3.1736026061704843
Validation loss: 2.9574515725537847

Epoch: 44| Step: 0
Training loss: 3.388016079374707
Validation loss: 2.9549296817276858

Epoch: 6| Step: 1
Training loss: 2.8535604088289315
Validation loss: 2.952667443084213

Epoch: 6| Step: 2
Training loss: 3.1590115391881874
Validation loss: 2.9499477403397636

Epoch: 6| Step: 3
Training loss: 3.1966997413661162
Validation loss: 2.948310371950689

Epoch: 6| Step: 4
Training loss: 2.9296451819860345
Validation loss: 2.9452285957021695

Epoch: 6| Step: 5
Training loss: 2.8236665937813754
Validation loss: 2.9429436828663698

Epoch: 6| Step: 6
Training loss: 2.990198016802148
Validation loss: 2.9406367389202765

Epoch: 6| Step: 7
Training loss: 2.6955495536606695
Validation loss: 2.9379308939147326

Epoch: 6| Step: 8
Training loss: 3.701147267604544
Validation loss: 2.935673233598653

Epoch: 6| Step: 9
Training loss: 3.4042475702826596
Validation loss: 2.93292723160138

Epoch: 6| Step: 10
Training loss: 3.373929171756614
Validation loss: 2.9304109349857597

Epoch: 6| Step: 11
Training loss: 3.1951823686062655
Validation loss: 2.928095582578815

Epoch: 6| Step: 12
Training loss: 2.7240977692276456
Validation loss: 2.925504242669624

Epoch: 6| Step: 13
Training loss: 2.4842584930548073
Validation loss: 2.9233310899613523

Epoch: 45| Step: 0
Training loss: 2.818784601037241
Validation loss: 2.9209677771854716

Epoch: 6| Step: 1
Training loss: 3.0779025389551635
Validation loss: 2.918682060567354

Epoch: 6| Step: 2
Training loss: 2.972727309792513
Validation loss: 2.9167059259951365

Epoch: 6| Step: 3
Training loss: 2.7718937405508584
Validation loss: 2.914526349361145

Epoch: 6| Step: 4
Training loss: 2.8796966999670763
Validation loss: 2.911691841674636

Epoch: 6| Step: 5
Training loss: 3.688744932899627
Validation loss: 2.909106010023934

Epoch: 6| Step: 6
Training loss: 2.8832504642757093
Validation loss: 2.9070348739430574

Epoch: 6| Step: 7
Training loss: 2.7470621542160227
Validation loss: 2.904557121296117

Epoch: 6| Step: 8
Training loss: 2.897996894682069
Validation loss: 2.9022583159532047

Epoch: 6| Step: 9
Training loss: 3.312933551558349
Validation loss: 2.899894697645249

Epoch: 6| Step: 10
Training loss: 3.4577515966927344
Validation loss: 2.8978151405553088

Epoch: 6| Step: 11
Training loss: 2.6688625910502073
Validation loss: 2.8955334890083786

Epoch: 6| Step: 12
Training loss: 3.0440903678944413
Validation loss: 2.893275350115663

Epoch: 6| Step: 13
Training loss: 3.2899434049428646
Validation loss: 2.8911778239273516

Epoch: 46| Step: 0
Training loss: 2.9905180494423584
Validation loss: 2.8889705425321823

Epoch: 6| Step: 1
Training loss: 2.3353732138773244
Validation loss: 2.8872491179848687

Epoch: 6| Step: 2
Training loss: 3.2234264586236914
Validation loss: 2.885224089180879

Epoch: 6| Step: 3
Training loss: 3.086724212417655
Validation loss: 2.8834756503694634

Epoch: 6| Step: 4
Training loss: 2.7447490845670113
Validation loss: 2.881469744662313

Epoch: 6| Step: 5
Training loss: 2.9849817747649166
Validation loss: 2.8797029232302758

Epoch: 6| Step: 6
Training loss: 2.904183586286591
Validation loss: 2.8780788300573583

Epoch: 6| Step: 7
Training loss: 2.8981258913737644
Validation loss: 2.875813769842603

Epoch: 6| Step: 8
Training loss: 3.2214912667032505
Validation loss: 2.874269738263074

Epoch: 6| Step: 9
Training loss: 2.768273071821293
Validation loss: 2.872096129362673

Epoch: 6| Step: 10
Training loss: 3.2833927968896734
Validation loss: 2.870265614627287

Epoch: 6| Step: 11
Training loss: 2.986644740563472
Validation loss: 2.8678347543899982

Epoch: 6| Step: 12
Training loss: 3.667526722234043
Validation loss: 2.8661538182837747

Epoch: 6| Step: 13
Training loss: 2.9548496276496854
Validation loss: 2.8640067520168566

Epoch: 47| Step: 0
Training loss: 2.492212373137045
Validation loss: 2.8617037547186666

Epoch: 6| Step: 1
Training loss: 3.057106718657007
Validation loss: 2.85977070055937

Epoch: 6| Step: 2
Training loss: 2.8734979437506576
Validation loss: 2.8572529629654064

Epoch: 6| Step: 3
Training loss: 3.431910044516013
Validation loss: 2.8551281796801997

Epoch: 6| Step: 4
Training loss: 3.5475518341256276
Validation loss: 2.853366256936882

Epoch: 6| Step: 5
Training loss: 2.7911812564295957
Validation loss: 2.8510660514490254

Epoch: 6| Step: 6
Training loss: 2.8567826657092206
Validation loss: 2.848708667776105

Epoch: 6| Step: 7
Training loss: 2.6839146318119904
Validation loss: 2.847009670004877

Epoch: 6| Step: 8
Training loss: 2.4816170985402017
Validation loss: 2.8448064050580166

Epoch: 6| Step: 9
Training loss: 3.2616074959974535
Validation loss: 2.84321661897066

Epoch: 6| Step: 10
Training loss: 2.9679352294764096
Validation loss: 2.8414161942770964

Epoch: 6| Step: 11
Training loss: 3.474767465838598
Validation loss: 2.839115978318839

Epoch: 6| Step: 12
Training loss: 2.778438733196832
Validation loss: 2.8373417035731685

Epoch: 6| Step: 13
Training loss: 2.9264392800366132
Validation loss: 2.835561848290987

Epoch: 48| Step: 0
Training loss: 2.8364519645702537
Validation loss: 2.8335234120132493

Epoch: 6| Step: 1
Training loss: 2.855073288279184
Validation loss: 2.831500727630114

Epoch: 6| Step: 2
Training loss: 2.971968662779597
Validation loss: 2.829906471580585

Epoch: 6| Step: 3
Training loss: 3.0906211367005367
Validation loss: 2.827898385257814

Epoch: 6| Step: 4
Training loss: 1.4622774558004747
Validation loss: 2.8260170965161255

Epoch: 6| Step: 5
Training loss: 3.2194011733232193
Validation loss: 2.8248045592092725

Epoch: 6| Step: 6
Training loss: 3.1908273720521616
Validation loss: 2.8221426227521347

Epoch: 6| Step: 7
Training loss: 3.186208276247249
Validation loss: 2.8206272249039044

Epoch: 6| Step: 8
Training loss: 3.2414725009329017
Validation loss: 2.818811385225714

Epoch: 6| Step: 9
Training loss: 2.016279602211869
Validation loss: 2.8168116968576364

Epoch: 6| Step: 10
Training loss: 3.1747057079983745
Validation loss: 2.815134495875565

Epoch: 6| Step: 11
Training loss: 3.07086238838712
Validation loss: 2.8134035990137645

Epoch: 6| Step: 12
Training loss: 3.508483414750885
Validation loss: 2.811716450934686

Epoch: 6| Step: 13
Training loss: 3.028181745156928
Validation loss: 2.809942976326622

Epoch: 49| Step: 0
Training loss: 3.07763513065414
Validation loss: 2.8081044357546427

Epoch: 6| Step: 1
Training loss: 3.2706133693588137
Validation loss: 2.806265101289508

Epoch: 6| Step: 2
Training loss: 2.6500420225158416
Validation loss: 2.804109434098293

Epoch: 6| Step: 3
Training loss: 3.0672638878271234
Validation loss: 2.8025972587548993

Epoch: 6| Step: 4
Training loss: 2.9021584206474937
Validation loss: 2.8006624828856137

Epoch: 6| Step: 5
Training loss: 2.625190727934301
Validation loss: 2.7993188903822412

Epoch: 6| Step: 6
Training loss: 3.022481920110931
Validation loss: 2.7967721362529283

Epoch: 6| Step: 7
Training loss: 2.7849622661561457
Validation loss: 2.79484578261635

Epoch: 6| Step: 8
Training loss: 3.1686670777796597
Validation loss: 2.793134820442409

Epoch: 6| Step: 9
Training loss: 2.7141110536122746
Validation loss: 2.791384972171737

Epoch: 6| Step: 10
Training loss: 2.6279460179258485
Validation loss: 2.790397602312563

Epoch: 6| Step: 11
Training loss: 3.149862189532951
Validation loss: 2.7882733764708285

Epoch: 6| Step: 12
Training loss: 3.0383905682916077
Validation loss: 2.7869830674313048

Epoch: 6| Step: 13
Training loss: 2.950127265900535
Validation loss: 2.7847896011796545

Epoch: 50| Step: 0
Training loss: 2.869218569450537
Validation loss: 2.7837695449990294

Epoch: 6| Step: 1
Training loss: 3.066493641768214
Validation loss: 2.782864933994264

Epoch: 6| Step: 2
Training loss: 2.948658790228847
Validation loss: 2.781468129265893

Epoch: 6| Step: 3
Training loss: 2.523205064396683
Validation loss: 2.7818211315535666

Epoch: 6| Step: 4
Training loss: 3.216721275011001
Validation loss: 2.7839719332891626

Epoch: 6| Step: 5
Training loss: 3.310821683837408
Validation loss: 2.7758680841642605

Epoch: 6| Step: 6
Training loss: 2.610481621455986
Validation loss: 2.773230703461613

Epoch: 6| Step: 7
Training loss: 3.1529532863109755
Validation loss: 2.7715851951579182

Epoch: 6| Step: 8
Training loss: 2.6368046499317797
Validation loss: 2.7695349230111987

Epoch: 6| Step: 9
Training loss: 2.7204731709174492
Validation loss: 2.7715971952652363

Epoch: 6| Step: 10
Training loss: 2.820672263238245
Validation loss: 2.779341710958332

Epoch: 6| Step: 11
Training loss: 3.2066034549766615
Validation loss: 2.7800213389412445

Epoch: 6| Step: 12
Training loss: 3.1167403356919863
Validation loss: 2.7682698421175393

Epoch: 6| Step: 13
Training loss: 2.5062791171011627
Validation loss: 2.76267711767806

Epoch: 51| Step: 0
Training loss: 3.362185983282549
Validation loss: 2.7608815839308436

Epoch: 6| Step: 1
Training loss: 3.4244085539099736
Validation loss: 2.760545408497625

Epoch: 6| Step: 2
Training loss: 2.7903733365337997
Validation loss: 2.760610787145859

Epoch: 6| Step: 3
Training loss: 2.697791245146013
Validation loss: 2.761417442622436

Epoch: 6| Step: 4
Training loss: 2.8323100617066155
Validation loss: 2.765909485633962

Epoch: 6| Step: 5
Training loss: 2.399821723833431
Validation loss: 2.761183395906412

Epoch: 6| Step: 6
Training loss: 2.7857845929813796
Validation loss: 2.7582365147712107

Epoch: 6| Step: 7
Training loss: 2.650754022388164
Validation loss: 2.755371598500493

Epoch: 6| Step: 8
Training loss: 2.727067523518996
Validation loss: 2.7513222549982017

Epoch: 6| Step: 9
Training loss: 3.267775416189781
Validation loss: 2.7495220520370927

Epoch: 6| Step: 10
Training loss: 2.7167181556206486
Validation loss: 2.748006900369179

Epoch: 6| Step: 11
Training loss: 3.082306476096342
Validation loss: 2.745407691655667

Epoch: 6| Step: 12
Training loss: 3.0301744578279726
Validation loss: 2.7441850507042984

Epoch: 6| Step: 13
Training loss: 2.581255312861039
Validation loss: 2.741732857628862

Epoch: 52| Step: 0
Training loss: 2.371572128881555
Validation loss: 2.7405089251920365

Epoch: 6| Step: 1
Training loss: 2.3331648788497983
Validation loss: 2.739004332611531

Epoch: 6| Step: 2
Training loss: 3.1647442118619433
Validation loss: 2.738604487969055

Epoch: 6| Step: 3
Training loss: 2.7521855599310023
Validation loss: 2.7367395694778174

Epoch: 6| Step: 4
Training loss: 2.8184734910899563
Validation loss: 2.7356179745567726

Epoch: 6| Step: 5
Training loss: 3.170277009768161
Validation loss: 2.735379149118411

Epoch: 6| Step: 6
Training loss: 2.408984525435276
Validation loss: 2.732745195319369

Epoch: 6| Step: 7
Training loss: 3.1299751351438325
Validation loss: 2.7298088968202845

Epoch: 6| Step: 8
Training loss: 3.000705953823779
Validation loss: 2.728020818176895

Epoch: 6| Step: 9
Training loss: 2.5708765958817317
Validation loss: 2.7271422869141158

Epoch: 6| Step: 10
Training loss: 2.6975891228661313
Validation loss: 2.7269013566824225

Epoch: 6| Step: 11
Training loss: 2.8634840827074535
Validation loss: 2.725250628083656

Epoch: 6| Step: 12
Training loss: 3.2162993647986045
Validation loss: 2.7239821939292534

Epoch: 6| Step: 13
Training loss: 3.449443529004358
Validation loss: 2.7230336581263144

Epoch: 53| Step: 0
Training loss: 3.426958879957125
Validation loss: 2.7220280276879922

Epoch: 6| Step: 1
Training loss: 3.1174866955801335
Validation loss: 2.7202480909627553

Epoch: 6| Step: 2
Training loss: 2.8669744511168798
Validation loss: 2.719967668074697

Epoch: 6| Step: 3
Training loss: 3.224899787232979
Validation loss: 2.718934385397702

Epoch: 6| Step: 4
Training loss: 2.105349394010841
Validation loss: 2.716602288304407

Epoch: 6| Step: 5
Training loss: 3.257394740994285
Validation loss: 2.715793141733577

Epoch: 6| Step: 6
Training loss: 3.0068625795547526
Validation loss: 2.7135534060487254

Epoch: 6| Step: 7
Training loss: 2.5780199434087554
Validation loss: 2.7096509418229147

Epoch: 6| Step: 8
Training loss: 2.5137692827559754
Validation loss: 2.707320300179906

Epoch: 6| Step: 9
Training loss: 3.0898596453348253
Validation loss: 2.707063587577226

Epoch: 6| Step: 10
Training loss: 2.359854036925174
Validation loss: 2.7055708857560448

Epoch: 6| Step: 11
Training loss: 2.8517715168473363
Validation loss: 2.704858565801996

Epoch: 6| Step: 12
Training loss: 2.5846635613562614
Validation loss: 2.7050686649014786

Epoch: 6| Step: 13
Training loss: 2.6519251002843784
Validation loss: 2.7034105800531414

Epoch: 54| Step: 0
Training loss: 2.5013339298623083
Validation loss: 2.702669241231976

Epoch: 6| Step: 1
Training loss: 2.590837060879242
Validation loss: 2.702451427233766

Epoch: 6| Step: 2
Training loss: 3.163263968969066
Validation loss: 2.6998674245357446

Epoch: 6| Step: 3
Training loss: 3.7182565570076616
Validation loss: 2.698415118582804

Epoch: 6| Step: 4
Training loss: 2.5183353395115327
Validation loss: 2.6942809558921357

Epoch: 6| Step: 5
Training loss: 2.805745809753655
Validation loss: 2.6941777886081377

Epoch: 6| Step: 6
Training loss: 2.7720188004652955
Validation loss: 2.690816651362783

Epoch: 6| Step: 7
Training loss: 2.7395035958138565
Validation loss: 2.6884285705103586

Epoch: 6| Step: 8
Training loss: 2.691337861408881
Validation loss: 2.6902514722338133

Epoch: 6| Step: 9
Training loss: 2.866672563177151
Validation loss: 2.6903811838678346

Epoch: 6| Step: 10
Training loss: 2.350550132640423
Validation loss: 2.6893335489891745

Epoch: 6| Step: 11
Training loss: 2.8830278517760126
Validation loss: 2.689342340437571

Epoch: 6| Step: 12
Training loss: 2.564054250384257
Validation loss: 2.6821363286836357

Epoch: 6| Step: 13
Training loss: 3.2619044085447078
Validation loss: 2.6818455643032295

Epoch: 55| Step: 0
Training loss: 2.5416505740130484
Validation loss: 2.6841002851076814

Epoch: 6| Step: 1
Training loss: 2.2355047184316947
Validation loss: 2.689659049822258

Epoch: 6| Step: 2
Training loss: 2.519599759848596
Validation loss: 2.6992829936691143

Epoch: 6| Step: 3
Training loss: 3.1852516771944903
Validation loss: 2.695028288226267

Epoch: 6| Step: 4
Training loss: 3.003468733376968
Validation loss: 2.689704611772469

Epoch: 6| Step: 5
Training loss: 2.7658768878605695
Validation loss: 2.6873470824746435

Epoch: 6| Step: 6
Training loss: 2.460080533453956
Validation loss: 2.682575089872256

Epoch: 6| Step: 7
Training loss: 2.9980241308685325
Validation loss: 2.680656198699664

Epoch: 6| Step: 8
Training loss: 2.976522130016593
Validation loss: 2.679872882684592

Epoch: 6| Step: 9
Training loss: 2.9497781185177856
Validation loss: 2.677681975961492

Epoch: 6| Step: 10
Training loss: 2.903446280745656
Validation loss: 2.67637280981353

Epoch: 6| Step: 11
Training loss: 3.0488784854304867
Validation loss: 2.6761278062624836

Epoch: 6| Step: 12
Training loss: 2.2675931603116415
Validation loss: 2.674906551951103

Epoch: 6| Step: 13
Training loss: 3.3713885164075132
Validation loss: 2.675902441394226

Epoch: 56| Step: 0
Training loss: 2.8101766526704095
Validation loss: 2.674515473522668

Epoch: 6| Step: 1
Training loss: 2.996209611107241
Validation loss: 2.6693351973550303

Epoch: 6| Step: 2
Training loss: 3.06490013893329
Validation loss: 2.666559713920933

Epoch: 6| Step: 3
Training loss: 2.7604994179659914
Validation loss: 2.6654341104244845

Epoch: 6| Step: 4
Training loss: 3.086588884970627
Validation loss: 2.664746984288577

Epoch: 6| Step: 5
Training loss: 3.102355548160879
Validation loss: 2.6650423874862836

Epoch: 6| Step: 6
Training loss: 2.5638900336192636
Validation loss: 2.663465968192514

Epoch: 6| Step: 7
Training loss: 3.152492136813105
Validation loss: 2.6636749537574587

Epoch: 6| Step: 8
Training loss: 2.8026134079404663
Validation loss: 2.662369056550192

Epoch: 6| Step: 9
Training loss: 2.147047113129974
Validation loss: 2.66034568227905

Epoch: 6| Step: 10
Training loss: 2.9002523575523207
Validation loss: 2.6588373784585433

Epoch: 6| Step: 11
Training loss: 2.642139790361964
Validation loss: 2.65692260304702

Epoch: 6| Step: 12
Training loss: 2.459305382486069
Validation loss: 2.654735612199401

Epoch: 6| Step: 13
Training loss: 2.6269918786349185
Validation loss: 2.6544888660645056

Epoch: 57| Step: 0
Training loss: 2.9465903993233056
Validation loss: 2.6533621254833237

Epoch: 6| Step: 1
Training loss: 2.6278519351362375
Validation loss: 2.6525669219371304

Epoch: 6| Step: 2
Training loss: 3.127115982837411
Validation loss: 2.649647964030815

Epoch: 6| Step: 3
Training loss: 2.8394844006222244
Validation loss: 2.6488371690967853

Epoch: 6| Step: 4
Training loss: 2.548049000890191
Validation loss: 2.646260675076309

Epoch: 6| Step: 5
Training loss: 3.1328907336124705
Validation loss: 2.6437858909773944

Epoch: 6| Step: 6
Training loss: 2.6804682981512387
Validation loss: 2.645364694818336

Epoch: 6| Step: 7
Training loss: 3.1134757599800578
Validation loss: 2.641696103249891

Epoch: 6| Step: 8
Training loss: 2.8728229738872586
Validation loss: 2.6434422198843204

Epoch: 6| Step: 9
Training loss: 2.97686141307933
Validation loss: 2.6403552638021512

Epoch: 6| Step: 10
Training loss: 2.418236047517374
Validation loss: 2.6370460438986267

Epoch: 6| Step: 11
Training loss: 2.711944129345839
Validation loss: 2.638148770081027

Epoch: 6| Step: 12
Training loss: 2.413918697678923
Validation loss: 2.6413159190718645

Epoch: 6| Step: 13
Training loss: 2.4442192778293834
Validation loss: 2.6377761642001554

Epoch: 58| Step: 0
Training loss: 2.98319592576796
Validation loss: 2.638726042580464

Epoch: 6| Step: 1
Training loss: 3.2703659468802067
Validation loss: 2.6346170521616097

Epoch: 6| Step: 2
Training loss: 2.7392935847430278
Validation loss: 2.633336981839256

Epoch: 6| Step: 3
Training loss: 2.83789633785127
Validation loss: 2.6310733291863815

Epoch: 6| Step: 4
Training loss: 2.0633827401173916
Validation loss: 2.6307562430486326

Epoch: 6| Step: 5
Training loss: 2.9027720324040147
Validation loss: 2.631702887310196

Epoch: 6| Step: 6
Training loss: 2.7707182112180484
Validation loss: 2.6311480716102937

Epoch: 6| Step: 7
Training loss: 2.5308880959335567
Validation loss: 2.6331036234109497

Epoch: 6| Step: 8
Training loss: 3.163416365594611
Validation loss: 2.6328519823866334

Epoch: 6| Step: 9
Training loss: 2.5438645245689386
Validation loss: 2.6335038844139627

Epoch: 6| Step: 10
Training loss: 2.5667572013774023
Validation loss: 2.632804425356413

Epoch: 6| Step: 11
Training loss: 2.7377293529844895
Validation loss: 2.632099179494998

Epoch: 6| Step: 12
Training loss: 2.856570343150368
Validation loss: 2.63132332815171

Epoch: 6| Step: 13
Training loss: 2.6740998937398017
Validation loss: 2.628949191917461

Epoch: 59| Step: 0
Training loss: 3.0868416149854303
Validation loss: 2.626687128339097

Epoch: 6| Step: 1
Training loss: 3.2041657036497524
Validation loss: 2.625167599126312

Epoch: 6| Step: 2
Training loss: 2.6124900452638635
Validation loss: 2.6236869464704875

Epoch: 6| Step: 3
Training loss: 3.041608592136534
Validation loss: 2.621641720505374

Epoch: 6| Step: 4
Training loss: 2.94749131296057
Validation loss: 2.620533064080157

Epoch: 6| Step: 5
Training loss: 2.0909670653994543
Validation loss: 2.6180698406752794

Epoch: 6| Step: 6
Training loss: 2.359863331765337
Validation loss: 2.6166251797839504

Epoch: 6| Step: 7
Training loss: 2.922636860954214
Validation loss: 2.6153394864577426

Epoch: 6| Step: 8
Training loss: 2.935038082779043
Validation loss: 2.614826606486858

Epoch: 6| Step: 9
Training loss: 2.660381201196767
Validation loss: 2.613464942029949

Epoch: 6| Step: 10
Training loss: 2.3214474352395054
Validation loss: 2.6128724771140197

Epoch: 6| Step: 11
Training loss: 3.1994157853892404
Validation loss: 2.6148373960386073

Epoch: 6| Step: 12
Training loss: 2.6641493081187675
Validation loss: 2.6099282475787255

Epoch: 6| Step: 13
Training loss: 2.2127578843682794
Validation loss: 2.614635608854676

Epoch: 60| Step: 0
Training loss: 3.044886796221929
Validation loss: 2.6164827830907673

Epoch: 6| Step: 1
Training loss: 2.897274143740847
Validation loss: 2.6159304660369433

Epoch: 6| Step: 2
Training loss: 3.10755399354781
Validation loss: 2.608443636486549

Epoch: 6| Step: 3
Training loss: 2.572330691268465
Validation loss: 2.6061028314718837

Epoch: 6| Step: 4
Training loss: 3.0152937158645305
Validation loss: 2.6069507111266477

Epoch: 6| Step: 5
Training loss: 2.317405652344779
Validation loss: 2.608617508939897

Epoch: 6| Step: 6
Training loss: 2.9103710530519766
Validation loss: 2.6117130548398153

Epoch: 6| Step: 7
Training loss: 2.8558060515983406
Validation loss: 2.6149756199348784

Epoch: 6| Step: 8
Training loss: 2.466748935599101
Validation loss: 2.6122575017642653

Epoch: 6| Step: 9
Training loss: 3.105973266309656
Validation loss: 2.6120004751313086

Epoch: 6| Step: 10
Training loss: 2.442144029148659
Validation loss: 2.610187107190373

Epoch: 6| Step: 11
Training loss: 2.361389916607767
Validation loss: 2.6086636866261577

Epoch: 6| Step: 12
Training loss: 2.3708402444730923
Validation loss: 2.606711225307663

Epoch: 6| Step: 13
Training loss: 2.926120224021851
Validation loss: 2.604682410985975

Epoch: 61| Step: 0
Training loss: 3.015610927094349
Validation loss: 2.6032616810293723

Epoch: 6| Step: 1
Training loss: 3.063912513559594
Validation loss: 2.6015018001650123

Epoch: 6| Step: 2
Training loss: 2.301465798889006
Validation loss: 2.5986321014552414

Epoch: 6| Step: 3
Training loss: 2.187669366002058
Validation loss: 2.5993111676059204

Epoch: 6| Step: 4
Training loss: 2.907678232675161
Validation loss: 2.597555004623027

Epoch: 6| Step: 5
Training loss: 2.30727342263982
Validation loss: 2.595133125403804

Epoch: 6| Step: 6
Training loss: 2.4625473327735046
Validation loss: 2.593728812257564

Epoch: 6| Step: 7
Training loss: 2.6076398551206554
Validation loss: 2.5931455834816783

Epoch: 6| Step: 8
Training loss: 2.4925582753431152
Validation loss: 2.5928752297285147

Epoch: 6| Step: 9
Training loss: 3.1024125709479673
Validation loss: 2.5920674025460912

Epoch: 6| Step: 10
Training loss: 3.012061983927212
Validation loss: 2.591200191879697

Epoch: 6| Step: 11
Training loss: 2.6394850397713947
Validation loss: 2.5889660943361292

Epoch: 6| Step: 12
Training loss: 2.778930983112936
Validation loss: 2.5877914927947083

Epoch: 6| Step: 13
Training loss: 3.1450805942415694
Validation loss: 2.5872169855849485

Epoch: 62| Step: 0
Training loss: 2.6286944775923766
Validation loss: 2.58764370873946

Epoch: 6| Step: 1
Training loss: 2.966904879946506
Validation loss: 2.5902093981142347

Epoch: 6| Step: 2
Training loss: 2.3425173760648024
Validation loss: 2.5907258170972054

Epoch: 6| Step: 3
Training loss: 2.651432200946696
Validation loss: 2.5906810452266247

Epoch: 6| Step: 4
Training loss: 2.864529658883981
Validation loss: 2.596975901908445

Epoch: 6| Step: 5
Training loss: 3.0488772342497437
Validation loss: 2.5888962429262348

Epoch: 6| Step: 6
Training loss: 2.7811176397138855
Validation loss: 2.584406558009029

Epoch: 6| Step: 7
Training loss: 2.432817117812834
Validation loss: 2.5855777025342097

Epoch: 6| Step: 8
Training loss: 2.4951294183897876
Validation loss: 2.5855305823088304

Epoch: 6| Step: 9
Training loss: 2.7219907365931424
Validation loss: 2.5874337192468064

Epoch: 6| Step: 10
Training loss: 2.595070927703521
Validation loss: 2.5893785030983674

Epoch: 6| Step: 11
Training loss: 2.796083988450846
Validation loss: 2.590685585335084

Epoch: 6| Step: 12
Training loss: 3.1021918515877798
Validation loss: 2.5922250516170853

Epoch: 6| Step: 13
Training loss: 2.7265980015932825
Validation loss: 2.592390263195928

Epoch: 63| Step: 0
Training loss: 2.6258036881781734
Validation loss: 2.5901447349473865

Epoch: 6| Step: 1
Training loss: 2.9322482298394323
Validation loss: 2.586361945316745

Epoch: 6| Step: 2
Training loss: 2.6830460657237882
Validation loss: 2.585468522462954

Epoch: 6| Step: 3
Training loss: 2.1506955019883565
Validation loss: 2.5833968544400454

Epoch: 6| Step: 4
Training loss: 3.071292167625616
Validation loss: 2.5846323059729523

Epoch: 6| Step: 5
Training loss: 2.8882086474637148
Validation loss: 2.581104429633837

Epoch: 6| Step: 6
Training loss: 2.954098495607912
Validation loss: 2.5796663242998235

Epoch: 6| Step: 7
Training loss: 2.2912052267957987
Validation loss: 2.5785017874919065

Epoch: 6| Step: 8
Training loss: 2.9834395135777023
Validation loss: 2.5761672013408847

Epoch: 6| Step: 9
Training loss: 2.959250903322237
Validation loss: 2.575187666699262

Epoch: 6| Step: 10
Training loss: 2.6419789832619878
Validation loss: 2.574607082746563

Epoch: 6| Step: 11
Training loss: 2.4105327194705706
Validation loss: 2.5727033385102867

Epoch: 6| Step: 12
Training loss: 2.3221852358418285
Validation loss: 2.571722893989207

Epoch: 6| Step: 13
Training loss: 2.943399551816039
Validation loss: 2.5710405364123075

Epoch: 64| Step: 0
Training loss: 2.759421595075257
Validation loss: 2.569864962650637

Epoch: 6| Step: 1
Training loss: 3.060385460604672
Validation loss: 2.569819997428487

Epoch: 6| Step: 2
Training loss: 2.8091184844806873
Validation loss: 2.5681725959888455

Epoch: 6| Step: 3
Training loss: 2.2801416918579274
Validation loss: 2.566155145090277

Epoch: 6| Step: 4
Training loss: 2.6465780656847575
Validation loss: 2.5666482967074375

Epoch: 6| Step: 5
Training loss: 1.9476219471077005
Validation loss: 2.567543176648875

Epoch: 6| Step: 6
Training loss: 2.594915289111769
Validation loss: 2.565992146783012

Epoch: 6| Step: 7
Training loss: 2.7307595901691046
Validation loss: 2.564174260213041

Epoch: 6| Step: 8
Training loss: 2.968628569679406
Validation loss: 2.5630258160264723

Epoch: 6| Step: 9
Training loss: 2.9481661705585855
Validation loss: 2.562935908388027

Epoch: 6| Step: 10
Training loss: 2.2141569711025197
Validation loss: 2.5627838845902318

Epoch: 6| Step: 11
Training loss: 2.9120019576935947
Validation loss: 2.5611508857560437

Epoch: 6| Step: 12
Training loss: 2.8467894020988527
Validation loss: 2.559493526435898

Epoch: 6| Step: 13
Training loss: 2.904139090525228
Validation loss: 2.5590610533617935

Epoch: 65| Step: 0
Training loss: 2.818228926782287
Validation loss: 2.5592001315328012

Epoch: 6| Step: 1
Training loss: 2.72787068198652
Validation loss: 2.560331109037237

Epoch: 6| Step: 2
Training loss: 2.9128318826738337
Validation loss: 2.5594537352583377

Epoch: 6| Step: 3
Training loss: 2.9402329737940063
Validation loss: 2.5593606280061887

Epoch: 6| Step: 4
Training loss: 2.337270299059717
Validation loss: 2.5567726856897672

Epoch: 6| Step: 5
Training loss: 2.4203076613139576
Validation loss: 2.5559718318769096

Epoch: 6| Step: 6
Training loss: 2.8137881402066376
Validation loss: 2.556814336934394

Epoch: 6| Step: 7
Training loss: 2.6550414982657857
Validation loss: 2.5543443019594436

Epoch: 6| Step: 8
Training loss: 2.6085872717471594
Validation loss: 2.554504932409007

Epoch: 6| Step: 9
Training loss: 2.979060048860922
Validation loss: 2.55617425490798

Epoch: 6| Step: 10
Training loss: 2.701467546125808
Validation loss: 2.5517268703923937

Epoch: 6| Step: 11
Training loss: 2.716914028381459
Validation loss: 2.5522628461542065

Epoch: 6| Step: 12
Training loss: 2.2657187606878657
Validation loss: 2.5521557596530333

Epoch: 6| Step: 13
Training loss: 2.6863230412468253
Validation loss: 2.552336471355745

Epoch: 66| Step: 0
Training loss: 2.685225211560121
Validation loss: 2.551477949521239

Epoch: 6| Step: 1
Training loss: 2.8456912279371247
Validation loss: 2.551238567645039

Epoch: 6| Step: 2
Training loss: 2.213140569512005
Validation loss: 2.5516898701777833

Epoch: 6| Step: 3
Training loss: 2.019811496437263
Validation loss: 2.552943778795019

Epoch: 6| Step: 4
Training loss: 3.027477315601948
Validation loss: 2.5520036386515748

Epoch: 6| Step: 5
Training loss: 3.059744860664974
Validation loss: 2.5512388168506033

Epoch: 6| Step: 6
Training loss: 2.725851146515786
Validation loss: 2.5491695092609272

Epoch: 6| Step: 7
Training loss: 2.4342998135553047
Validation loss: 2.547482579938822

Epoch: 6| Step: 8
Training loss: 2.9461487410133937
Validation loss: 2.545408439038042

Epoch: 6| Step: 9
Training loss: 2.560305702612975
Validation loss: 2.5437052686102817

Epoch: 6| Step: 10
Training loss: 2.799449907490023
Validation loss: 2.548321303527793

Epoch: 6| Step: 11
Training loss: 2.7015482101812647
Validation loss: 2.5458780689544094

Epoch: 6| Step: 12
Training loss: 2.6968812661744628
Validation loss: 2.5449554605090303

Epoch: 6| Step: 13
Training loss: 2.7477818993719487
Validation loss: 2.5474960100506454

Epoch: 67| Step: 0
Training loss: 2.7709886512152253
Validation loss: 2.544452880676958

Epoch: 6| Step: 1
Training loss: 2.427707662765352
Validation loss: 2.541269041966719

Epoch: 6| Step: 2
Training loss: 2.8620817786794843
Validation loss: 2.5454951567940016

Epoch: 6| Step: 3
Training loss: 2.937212828539035
Validation loss: 2.5455269551660473

Epoch: 6| Step: 4
Training loss: 2.652926980616625
Validation loss: 2.5434875271636774

Epoch: 6| Step: 5
Training loss: 2.442631138040674
Validation loss: 2.5424982076042806

Epoch: 6| Step: 6
Training loss: 2.530933690028791
Validation loss: 2.544182983152029

Epoch: 6| Step: 7
Training loss: 2.7389698768009385
Validation loss: 2.5436616999416413

Epoch: 6| Step: 8
Training loss: 2.1575441761290177
Validation loss: 2.5457581799824727

Epoch: 6| Step: 9
Training loss: 3.3492284085468382
Validation loss: 2.539702271285968

Epoch: 6| Step: 10
Training loss: 2.6433407514098453
Validation loss: 2.544796257207485

Epoch: 6| Step: 11
Training loss: 2.913633749666774
Validation loss: 2.5414317340356853

Epoch: 6| Step: 12
Training loss: 2.6787169898309315
Validation loss: 2.5402304755547624

Epoch: 6| Step: 13
Training loss: 2.139311004314509
Validation loss: 2.5383944371016325

Epoch: 68| Step: 0
Training loss: 2.657871245240148
Validation loss: 2.539247273299852

Epoch: 6| Step: 1
Training loss: 2.2997475651411836
Validation loss: 2.5397251301318273

Epoch: 6| Step: 2
Training loss: 2.470143082061384
Validation loss: 2.5363201962767197

Epoch: 6| Step: 3
Training loss: 2.361007327871814
Validation loss: 2.5376668684138526

Epoch: 6| Step: 4
Training loss: 3.3382885976362795
Validation loss: 2.5407242408754636

Epoch: 6| Step: 5
Training loss: 2.6482292701084864
Validation loss: 2.537042136766001

Epoch: 6| Step: 6
Training loss: 2.6117746432180526
Validation loss: 2.5347334514584015

Epoch: 6| Step: 7
Training loss: 2.601487442171052
Validation loss: 2.5321581275049576

Epoch: 6| Step: 8
Training loss: 2.551367232354742
Validation loss: 2.531784942991524

Epoch: 6| Step: 9
Training loss: 2.721771315341827
Validation loss: 2.527176172910265

Epoch: 6| Step: 10
Training loss: 2.6814403973182244
Validation loss: 2.529907977537773

Epoch: 6| Step: 11
Training loss: 2.3308928760737686
Validation loss: 2.532443985515687

Epoch: 6| Step: 12
Training loss: 2.973759650671341
Validation loss: 2.531632790840065

Epoch: 6| Step: 13
Training loss: 2.8800598069444927
Validation loss: 2.5313238831714484

Epoch: 69| Step: 0
Training loss: 2.6857995264677728
Validation loss: 2.5307487964882887

Epoch: 6| Step: 1
Training loss: 2.285301060449071
Validation loss: 2.536424739753569

Epoch: 6| Step: 2
Training loss: 3.1806342287804865
Validation loss: 2.5349727538736935

Epoch: 6| Step: 3
Training loss: 2.490642582964127
Validation loss: 2.5302907908040995

Epoch: 6| Step: 4
Training loss: 3.0610600414350757
Validation loss: 2.5311468911853945

Epoch: 6| Step: 5
Training loss: 2.798035109326268
Validation loss: 2.531800386849749

Epoch: 6| Step: 6
Training loss: 2.54658272593236
Validation loss: 2.5322732152440532

Epoch: 6| Step: 7
Training loss: 2.82409304686
Validation loss: 2.5367710043527345

Epoch: 6| Step: 8
Training loss: 2.3769936475849445
Validation loss: 2.537840830020008

Epoch: 6| Step: 9
Training loss: 2.7870171577793252
Validation loss: 2.5377832877842907

Epoch: 6| Step: 10
Training loss: 2.397234089796096
Validation loss: 2.5469049401268915

Epoch: 6| Step: 11
Training loss: 2.3934848608682118
Validation loss: 2.5525772758540617

Epoch: 6| Step: 12
Training loss: 2.6829456507367286
Validation loss: 2.558282638155214

Epoch: 6| Step: 13
Training loss: 2.851904109193482
Validation loss: 2.559638340606453

Epoch: 70| Step: 0
Training loss: 2.765794048423027
Validation loss: 2.5586790580214234

Epoch: 6| Step: 1
Training loss: 2.7053876458750095
Validation loss: 2.5600705298999746

Epoch: 6| Step: 2
Training loss: 2.857658663238753
Validation loss: 2.559363841869731

Epoch: 6| Step: 3
Training loss: 2.7618995186485042
Validation loss: 2.5667864916025462

Epoch: 6| Step: 4
Training loss: 2.643995853472761
Validation loss: 2.5681787386226422

Epoch: 6| Step: 5
Training loss: 2.437415879583118
Validation loss: 2.5660985629636195

Epoch: 6| Step: 6
Training loss: 2.713918492565602
Validation loss: 2.559707523429544

Epoch: 6| Step: 7
Training loss: 2.3374212649543598
Validation loss: 2.5544225028405916

Epoch: 6| Step: 8
Training loss: 2.9021651571166878
Validation loss: 2.549072674825325

Epoch: 6| Step: 9
Training loss: 2.1197433528658705
Validation loss: 2.544237311671776

Epoch: 6| Step: 10
Training loss: 3.236050526529817
Validation loss: 2.546076097375511

Epoch: 6| Step: 11
Training loss: 2.5724272680798084
Validation loss: 2.539435905973607

Epoch: 6| Step: 12
Training loss: 2.7629929426381303
Validation loss: 2.528817031884575

Epoch: 6| Step: 13
Training loss: 2.58327990651002
Validation loss: 2.527478803576578

Epoch: 71| Step: 0
Training loss: 2.5141669366077974
Validation loss: 2.524614309685514

Epoch: 6| Step: 1
Training loss: 3.155230064648893
Validation loss: 2.5265199241282885

Epoch: 6| Step: 2
Training loss: 2.70511390855168
Validation loss: 2.5266626492705737

Epoch: 6| Step: 3
Training loss: 2.7708884116604673
Validation loss: 2.5240127962738197

Epoch: 6| Step: 4
Training loss: 2.5823082479539834
Validation loss: 2.5206090548870392

Epoch: 6| Step: 5
Training loss: 2.623748390084582
Validation loss: 2.5245388843757546

Epoch: 6| Step: 6
Training loss: 2.437831367074432
Validation loss: 2.5214664883105984

Epoch: 6| Step: 7
Training loss: 2.6490472412389927
Validation loss: 2.5234042264872474

Epoch: 6| Step: 8
Training loss: 2.883564011124081
Validation loss: 2.523149676683788

Epoch: 6| Step: 9
Training loss: 2.131184050582797
Validation loss: 2.521549207275945

Epoch: 6| Step: 10
Training loss: 2.6928384676882353
Validation loss: 2.520756560068177

Epoch: 6| Step: 11
Training loss: 2.4916549639815084
Validation loss: 2.5207611000073658

Epoch: 6| Step: 12
Training loss: 2.6084355473422516
Validation loss: 2.5218814583482576

Epoch: 6| Step: 13
Training loss: 2.865247355862722
Validation loss: 2.520073584410686

Epoch: 72| Step: 0
Training loss: 2.7151512795131856
Validation loss: 2.519152535455583

Epoch: 6| Step: 1
Training loss: 2.382000694191525
Validation loss: 2.51981565462206

Epoch: 6| Step: 2
Training loss: 2.5501940111143955
Validation loss: 2.517020981417395

Epoch: 6| Step: 3
Training loss: 2.5921958035640156
Validation loss: 2.518661135864176

Epoch: 6| Step: 4
Training loss: 2.719425073757504
Validation loss: 2.518658374921493

Epoch: 6| Step: 5
Training loss: 2.9069368924802603
Validation loss: 2.51829457452849

Epoch: 6| Step: 6
Training loss: 2.9533747814648383
Validation loss: 2.5157083059224217

Epoch: 6| Step: 7
Training loss: 3.0792832107753725
Validation loss: 2.515338194236369

Epoch: 6| Step: 8
Training loss: 2.6863119471291523
Validation loss: 2.5171977747986296

Epoch: 6| Step: 9
Training loss: 2.6644713683734866
Validation loss: 2.51728795847806

Epoch: 6| Step: 10
Training loss: 2.866201621045336
Validation loss: 2.515316330195161

Epoch: 6| Step: 11
Training loss: 2.4143390188515816
Validation loss: 2.5132939060198063

Epoch: 6| Step: 12
Training loss: 2.6825579069771988
Validation loss: 2.51675722851872

Epoch: 6| Step: 13
Training loss: 1.5504883458333976
Validation loss: 2.51294595610127

Epoch: 73| Step: 0
Training loss: 2.277922934530578
Validation loss: 2.510893738133252

Epoch: 6| Step: 1
Training loss: 2.607982058124025
Validation loss: 2.510576588666683

Epoch: 6| Step: 2
Training loss: 2.0391401371115467
Validation loss: 2.513466851113927

Epoch: 6| Step: 3
Training loss: 3.10446592249624
Validation loss: 2.5112967682973055

Epoch: 6| Step: 4
Training loss: 2.9596643453454945
Validation loss: 2.509729177801583

Epoch: 6| Step: 5
Training loss: 2.6814583579604725
Validation loss: 2.5102103901769004

Epoch: 6| Step: 6
Training loss: 2.311841097601451
Validation loss: 2.511941380816067

Epoch: 6| Step: 7
Training loss: 2.2753943971267017
Validation loss: 2.5087678858169022

Epoch: 6| Step: 8
Training loss: 2.404740690023538
Validation loss: 2.5088989344625605

Epoch: 6| Step: 9
Training loss: 2.438005248304293
Validation loss: 2.5097682294819696

Epoch: 6| Step: 10
Training loss: 3.0962341815789283
Validation loss: 2.514403194579481

Epoch: 6| Step: 11
Training loss: 2.9133568285148157
Validation loss: 2.513496248541297

Epoch: 6| Step: 12
Training loss: 2.5603030952198003
Validation loss: 2.514360983014022

Epoch: 6| Step: 13
Training loss: 3.108241195608559
Validation loss: 2.5083018268199107

Epoch: 74| Step: 0
Training loss: 2.5384484120676474
Validation loss: 2.507848848295368

Epoch: 6| Step: 1
Training loss: 2.958576962463925
Validation loss: 2.5071055841010623

Epoch: 6| Step: 2
Training loss: 2.9421991948702115
Validation loss: 2.5131873409998535

Epoch: 6| Step: 3
Training loss: 3.147860112030411
Validation loss: 2.518749752628387

Epoch: 6| Step: 4
Training loss: 2.1480367390777975
Validation loss: 2.5279772130194718

Epoch: 6| Step: 5
Training loss: 2.2102563031049227
Validation loss: 2.5325655246302743

Epoch: 6| Step: 6
Training loss: 2.393739255079252
Validation loss: 2.539140709870684

Epoch: 6| Step: 7
Training loss: 2.3405972447296723
Validation loss: 2.5359848541395134

Epoch: 6| Step: 8
Training loss: 2.9283582596504125
Validation loss: 2.536486401621294

Epoch: 6| Step: 9
Training loss: 3.0225996093491276
Validation loss: 2.5336027239525785

Epoch: 6| Step: 10
Training loss: 2.7901923618520814
Validation loss: 2.5328545057205005

Epoch: 6| Step: 11
Training loss: 2.4397285250994796
Validation loss: 2.5345638231459593

Epoch: 6| Step: 12
Training loss: 2.7690722861341452
Validation loss: 2.528355265827979

Epoch: 6| Step: 13
Training loss: 2.3505320778584893
Validation loss: 2.52084360567212

Epoch: 75| Step: 0
Training loss: 2.893109382921823
Validation loss: 2.516031031094275

Epoch: 6| Step: 1
Training loss: 1.9391340932608885
Validation loss: 2.5123416173811095

Epoch: 6| Step: 2
Training loss: 3.243521027761611
Validation loss: 2.510077953571923

Epoch: 6| Step: 3
Training loss: 2.42160544587384
Validation loss: 2.5092275871534406

Epoch: 6| Step: 4
Training loss: 2.992156105773213
Validation loss: 2.5084596236701873

Epoch: 6| Step: 5
Training loss: 2.826494199856513
Validation loss: 2.5062776584637008

Epoch: 6| Step: 6
Training loss: 2.2866702550001694
Validation loss: 2.504877704419337

Epoch: 6| Step: 7
Training loss: 2.280643539290633
Validation loss: 2.50463050528361

Epoch: 6| Step: 8
Training loss: 2.9284706131598988
Validation loss: 2.5016231512833795

Epoch: 6| Step: 9
Training loss: 2.608579777134354
Validation loss: 2.5049330519805477

Epoch: 6| Step: 10
Training loss: 2.8086532565247286
Validation loss: 2.50190841630888

Epoch: 6| Step: 11
Training loss: 2.736448885685111
Validation loss: 2.5046810353926063

Epoch: 6| Step: 12
Training loss: 2.1308568015638105
Validation loss: 2.502836874874397

Epoch: 6| Step: 13
Training loss: 2.480103281085502
Validation loss: 2.5003355595773256

Epoch: 76| Step: 0
Training loss: 2.2901633678283244
Validation loss: 2.4991711673100334

Epoch: 6| Step: 1
Training loss: 2.6150783865505742
Validation loss: 2.5001273917483684

Epoch: 6| Step: 2
Training loss: 2.4366379215964593
Validation loss: 2.50160303378857

Epoch: 6| Step: 3
Training loss: 2.210695718545987
Validation loss: 2.498570780711033

Epoch: 6| Step: 4
Training loss: 2.6286542978610887
Validation loss: 2.497878286932764

Epoch: 6| Step: 5
Training loss: 3.1322926686958423
Validation loss: 2.5013677352452324

Epoch: 6| Step: 6
Training loss: 2.650008824171813
Validation loss: 2.4978882294585305

Epoch: 6| Step: 7
Training loss: 2.35427689645891
Validation loss: 2.499027046975176

Epoch: 6| Step: 8
Training loss: 2.561895159539453
Validation loss: 2.4993127037069307

Epoch: 6| Step: 9
Training loss: 2.4649650435067936
Validation loss: 2.494267694696053

Epoch: 6| Step: 10
Training loss: 3.1484278070570397
Validation loss: 2.4996877634250447

Epoch: 6| Step: 11
Training loss: 2.676225121430133
Validation loss: 2.495497431836354

Epoch: 6| Step: 12
Training loss: 2.5830748233864407
Validation loss: 2.4973438298628885

Epoch: 6| Step: 13
Training loss: 2.844910772496529
Validation loss: 2.506386514879338

Epoch: 77| Step: 0
Training loss: 2.6307160041785416
Validation loss: 2.505607467441588

Epoch: 6| Step: 1
Training loss: 2.780598339073894
Validation loss: 2.5055043818183584

Epoch: 6| Step: 2
Training loss: 3.1039069320942962
Validation loss: 2.501470657749929

Epoch: 6| Step: 3
Training loss: 2.7342977458394744
Validation loss: 2.503076456514419

Epoch: 6| Step: 4
Training loss: 2.7291744804755798
Validation loss: 2.5095194615467475

Epoch: 6| Step: 5
Training loss: 2.3979699489883566
Validation loss: 2.509639245004252

Epoch: 6| Step: 6
Training loss: 2.446606668348166
Validation loss: 2.508982970516329

Epoch: 6| Step: 7
Training loss: 2.5270970502723795
Validation loss: 2.503505966398833

Epoch: 6| Step: 8
Training loss: 2.402043716178957
Validation loss: 2.5087112131559106

Epoch: 6| Step: 9
Training loss: 2.5421505034977927
Validation loss: 2.505812008480178

Epoch: 6| Step: 10
Training loss: 2.7247786055754797
Validation loss: 2.5050797353454106

Epoch: 6| Step: 11
Training loss: 2.5207961108609593
Validation loss: 2.5054735821831082

Epoch: 6| Step: 12
Training loss: 2.6343053552872377
Validation loss: 2.504320289947827

Epoch: 6| Step: 13
Training loss: 2.6896118915045357
Validation loss: 2.505850241117414

Epoch: 78| Step: 0
Training loss: 2.5386994557159848
Validation loss: 2.506244093712802

Epoch: 6| Step: 1
Training loss: 2.5656428718641386
Validation loss: 2.5059215352611135

Epoch: 6| Step: 2
Training loss: 2.076529212839864
Validation loss: 2.5043133083913642

Epoch: 6| Step: 3
Training loss: 2.638296240350838
Validation loss: 2.5048943928927407

Epoch: 6| Step: 4
Training loss: 2.6653407098433664
Validation loss: 2.5039696648195293

Epoch: 6| Step: 5
Training loss: 2.5070677985513683
Validation loss: 2.503306284740148

Epoch: 6| Step: 6
Training loss: 2.6144075575225556
Validation loss: 2.501613652478513

Epoch: 6| Step: 7
Training loss: 2.5928077365021833
Validation loss: 2.5043629246214265

Epoch: 6| Step: 8
Training loss: 2.768170236203692
Validation loss: 2.500975593309514

Epoch: 6| Step: 9
Training loss: 2.832159210471232
Validation loss: 2.4994022290507427

Epoch: 6| Step: 10
Training loss: 2.9409331692311182
Validation loss: 2.5016893242308287

Epoch: 6| Step: 11
Training loss: 2.780790140946166
Validation loss: 2.5008643404563653

Epoch: 6| Step: 12
Training loss: 2.629412258021141
Validation loss: 2.4997534312408343

Epoch: 6| Step: 13
Training loss: 2.579533879808299
Validation loss: 2.4994461240741828

Epoch: 79| Step: 0
Training loss: 2.5481982391887867
Validation loss: 2.4993382054811644

Epoch: 6| Step: 1
Training loss: 2.776953647005507
Validation loss: 2.4971714707920967

Epoch: 6| Step: 2
Training loss: 2.3802222008722005
Validation loss: 2.497711119309122

Epoch: 6| Step: 3
Training loss: 2.6262550986529627
Validation loss: 2.4993731666552494

Epoch: 6| Step: 4
Training loss: 2.175473093780453
Validation loss: 2.495299020251876

Epoch: 6| Step: 5
Training loss: 2.9738356547709413
Validation loss: 2.498211188898563

Epoch: 6| Step: 6
Training loss: 2.2516034029250624
Validation loss: 2.4996094080658757

Epoch: 6| Step: 7
Training loss: 2.5504776451209525
Validation loss: 2.4949498189493347

Epoch: 6| Step: 8
Training loss: 2.486869663137887
Validation loss: 2.497654370770679

Epoch: 6| Step: 9
Training loss: 3.094040057524058
Validation loss: 2.4961819262543634

Epoch: 6| Step: 10
Training loss: 2.804172394185974
Validation loss: 2.497684487270818

Epoch: 6| Step: 11
Training loss: 2.5914494986687258
Validation loss: 2.492087526618182

Epoch: 6| Step: 12
Training loss: 2.5248565935258767
Validation loss: 2.48928761554814

Epoch: 6| Step: 13
Training loss: 2.686946811963088
Validation loss: 2.4941891372013427

Epoch: 80| Step: 0
Training loss: 2.209979709169613
Validation loss: 2.4938391750010376

Epoch: 6| Step: 1
Training loss: 2.8624441216357925
Validation loss: 2.4908934196081716

Epoch: 6| Step: 2
Training loss: 2.5395392806561596
Validation loss: 2.4969168486448092

Epoch: 6| Step: 3
Training loss: 2.671305813311157
Validation loss: 2.492868974044452

Epoch: 6| Step: 4
Training loss: 3.055161851507021
Validation loss: 2.4909032623933283

Epoch: 6| Step: 5
Training loss: 2.856856774585167
Validation loss: 2.487129108376034

Epoch: 6| Step: 6
Training loss: 2.08867908003538
Validation loss: 2.4834498316259177

Epoch: 6| Step: 7
Training loss: 2.495711367455134
Validation loss: 2.4873094003266027

Epoch: 6| Step: 8
Training loss: 2.722578575744828
Validation loss: 2.4901041074102657

Epoch: 6| Step: 9
Training loss: 2.5628438579314032
Validation loss: 2.490388322447331

Epoch: 6| Step: 10
Training loss: 2.566349208900442
Validation loss: 2.4930244563107253

Epoch: 6| Step: 11
Training loss: 3.272138169279002
Validation loss: 2.489935691723107

Epoch: 6| Step: 12
Training loss: 2.299871656734604
Validation loss: 2.4884733707790847

Epoch: 6| Step: 13
Training loss: 2.195461498919374
Validation loss: 2.490134498737523

Epoch: 81| Step: 0
Training loss: 2.519611493392725
Validation loss: 2.486422560687138

Epoch: 6| Step: 1
Training loss: 2.349242222132386
Validation loss: 2.4877598494202267

Epoch: 6| Step: 2
Training loss: 2.5552275200636774
Validation loss: 2.486975630246722

Epoch: 6| Step: 3
Training loss: 2.1375824404874706
Validation loss: 2.4854602162930943

Epoch: 6| Step: 4
Training loss: 2.137312839635086
Validation loss: 2.485639174531815

Epoch: 6| Step: 5
Training loss: 2.978237211629206
Validation loss: 2.486627329493054

Epoch: 6| Step: 6
Training loss: 2.1411936206599314
Validation loss: 2.483245880925418

Epoch: 6| Step: 7
Training loss: 2.6991081601576274
Validation loss: 2.487220781716075

Epoch: 6| Step: 8
Training loss: 2.5528672759434174
Validation loss: 2.483316527913555

Epoch: 6| Step: 9
Training loss: 3.121583520610567
Validation loss: 2.4848494966294146

Epoch: 6| Step: 10
Training loss: 2.8031220809257897
Validation loss: 2.4819584735866886

Epoch: 6| Step: 11
Training loss: 2.394149476027345
Validation loss: 2.486901827636572

Epoch: 6| Step: 12
Training loss: 2.3604535801675866
Validation loss: 2.4841646469280327

Epoch: 6| Step: 13
Training loss: 3.301511424184535
Validation loss: 2.4829962241310133

Epoch: 82| Step: 0
Training loss: 2.8110221370828223
Validation loss: 2.496333071479405

Epoch: 6| Step: 1
Training loss: 2.9304074636186743
Validation loss: 2.498375436323306

Epoch: 6| Step: 2
Training loss: 2.351294416690509
Validation loss: 2.500628948569269

Epoch: 6| Step: 3
Training loss: 2.6568118230522746
Validation loss: 2.5074833448927536

Epoch: 6| Step: 4
Training loss: 3.1050175944887917
Validation loss: 2.5080542363706373

Epoch: 6| Step: 5
Training loss: 2.1732796716598624
Validation loss: 2.5028915213836433

Epoch: 6| Step: 6
Training loss: 2.9259716017851445
Validation loss: 2.4993338174138584

Epoch: 6| Step: 7
Training loss: 2.434162104139935
Validation loss: 2.4940419407825534

Epoch: 6| Step: 8
Training loss: 2.2100342971418923
Validation loss: 2.4933510737714473

Epoch: 6| Step: 9
Training loss: 2.195471055356282
Validation loss: 2.489821200532507

Epoch: 6| Step: 10
Training loss: 2.6379011201699165
Validation loss: 2.49401259286613

Epoch: 6| Step: 11
Training loss: 2.5961170655821335
Validation loss: 2.493396557433016

Epoch: 6| Step: 12
Training loss: 2.5281372711298777
Validation loss: 2.493441673728228

Epoch: 6| Step: 13
Training loss: 2.740395071547685
Validation loss: 2.4924115565559437

Epoch: 83| Step: 0
Training loss: 2.455918974568085
Validation loss: 2.500546872406573

Epoch: 6| Step: 1
Training loss: 2.5652903739355124
Validation loss: 2.4981466973795428

Epoch: 6| Step: 2
Training loss: 2.958847395241126
Validation loss: 2.4995606036285976

Epoch: 6| Step: 3
Training loss: 2.647110418053741
Validation loss: 2.4955998799248955

Epoch: 6| Step: 4
Training loss: 2.2491955908655963
Validation loss: 2.496728711861859

Epoch: 6| Step: 5
Training loss: 2.5005141683175944
Validation loss: 2.4942895839323853

Epoch: 6| Step: 6
Training loss: 2.7861426534121243
Validation loss: 2.495869020982778

Epoch: 6| Step: 7
Training loss: 2.5048630146634774
Validation loss: 2.4959174199870917

Epoch: 6| Step: 8
Training loss: 2.5441382746815204
Validation loss: 2.496228202020419

Epoch: 6| Step: 9
Training loss: 2.6271565978936158
Validation loss: 2.4926739161201397

Epoch: 6| Step: 10
Training loss: 2.753543651328322
Validation loss: 2.491683940976417

Epoch: 6| Step: 11
Training loss: 2.8475658261106456
Validation loss: 2.490399842592899

Epoch: 6| Step: 12
Training loss: 2.427014711471376
Validation loss: 2.4903283753449337

Epoch: 6| Step: 13
Training loss: 2.7300298553622877
Validation loss: 2.4883034154682235

Epoch: 84| Step: 0
Training loss: 2.276771434236938
Validation loss: 2.487825312979758

Epoch: 6| Step: 1
Training loss: 2.5702451563769935
Validation loss: 2.4874791280231823

Epoch: 6| Step: 2
Training loss: 2.942150249843024
Validation loss: 2.488632121517313

Epoch: 6| Step: 3
Training loss: 2.805557243632028
Validation loss: 2.4864894098439496

Epoch: 6| Step: 4
Training loss: 2.4577554141991818
Validation loss: 2.4864853826446414

Epoch: 6| Step: 5
Training loss: 2.8614906030554272
Validation loss: 2.486682931456499

Epoch: 6| Step: 6
Training loss: 2.7688702008266617
Validation loss: 2.4861359982751456

Epoch: 6| Step: 7
Training loss: 2.5385907013584204
Validation loss: 2.4847700576650733

Epoch: 6| Step: 8
Training loss: 2.9094868344489107
Validation loss: 2.4871822786857516

Epoch: 6| Step: 9
Training loss: 2.4444133824002883
Validation loss: 2.4841168747366904

Epoch: 6| Step: 10
Training loss: 2.221168382097228
Validation loss: 2.4855358362739612

Epoch: 6| Step: 11
Training loss: 2.664896814807158
Validation loss: 2.4878780531379676

Epoch: 6| Step: 12
Training loss: 2.3388423145859063
Validation loss: 2.4835712484240586

Epoch: 6| Step: 13
Training loss: 2.467139769195712
Validation loss: 2.482306765116755

Epoch: 85| Step: 0
Training loss: 2.103775844222567
Validation loss: 2.483242136498953

Epoch: 6| Step: 1
Training loss: 2.5787423059493872
Validation loss: 2.485348812518944

Epoch: 6| Step: 2
Training loss: 2.688310545296369
Validation loss: 2.4866571081385165

Epoch: 6| Step: 3
Training loss: 3.108586043515831
Validation loss: 2.4931685331478595

Epoch: 6| Step: 4
Training loss: 2.4961726455278965
Validation loss: 2.4952343977280584

Epoch: 6| Step: 5
Training loss: 3.2832440811135246
Validation loss: 2.5041945713496307

Epoch: 6| Step: 6
Training loss: 2.9525966272523103
Validation loss: 2.495309673730519

Epoch: 6| Step: 7
Training loss: 2.620717688244396
Validation loss: 2.492225734373556

Epoch: 6| Step: 8
Training loss: 2.5283451130675783
Validation loss: 2.492231936641579

Epoch: 6| Step: 9
Training loss: 2.137774497805322
Validation loss: 2.487538800344305

Epoch: 6| Step: 10
Training loss: 2.63166562238805
Validation loss: 2.4835674484890595

Epoch: 6| Step: 11
Training loss: 1.5903171893496872
Validation loss: 2.479938023777232

Epoch: 6| Step: 12
Training loss: 2.525358148234715
Validation loss: 2.4826910201662993

Epoch: 6| Step: 13
Training loss: 2.9091669026657647
Validation loss: 2.485489313471368

Epoch: 86| Step: 0
Training loss: 2.7245701722090065
Validation loss: 2.48310198847704

Epoch: 6| Step: 1
Training loss: 2.4889971842460596
Validation loss: 2.4848641367894766

Epoch: 6| Step: 2
Training loss: 1.9823421969632806
Validation loss: 2.4844533719990665

Epoch: 6| Step: 3
Training loss: 2.5868224505413973
Validation loss: 2.488092787635589

Epoch: 6| Step: 4
Training loss: 2.7970933882200493
Validation loss: 2.4811100328557645

Epoch: 6| Step: 5
Training loss: 2.1472484280510202
Validation loss: 2.482711538987265

Epoch: 6| Step: 6
Training loss: 2.5924425621622462
Validation loss: 2.478881279897456

Epoch: 6| Step: 7
Training loss: 3.104926986941606
Validation loss: 2.483689235512869

Epoch: 6| Step: 8
Training loss: 2.6252988009463776
Validation loss: 2.4850301938234214

Epoch: 6| Step: 9
Training loss: 2.6398370232994535
Validation loss: 2.48461038046234

Epoch: 6| Step: 10
Training loss: 2.4974823672538013
Validation loss: 2.4809263753395316

Epoch: 6| Step: 11
Training loss: 2.8051334972199347
Validation loss: 2.4822101557757184

Epoch: 6| Step: 12
Training loss: 2.7622674939108016
Validation loss: 2.48295952801925

Epoch: 6| Step: 13
Training loss: 2.468199173524448
Validation loss: 2.4791684083905765

Epoch: 87| Step: 0
Training loss: 2.7470139851306135
Validation loss: 2.4823581338217875

Epoch: 6| Step: 1
Training loss: 2.335112154797319
Validation loss: 2.4757664440992704

Epoch: 6| Step: 2
Training loss: 2.8599187615334114
Validation loss: 2.4772015217486527

Epoch: 6| Step: 3
Training loss: 2.766381284250433
Validation loss: 2.4729438923786162

Epoch: 6| Step: 4
Training loss: 2.6958798903189516
Validation loss: 2.4725022833016155

Epoch: 6| Step: 5
Training loss: 2.2221921627342844
Validation loss: 2.4715864891669104

Epoch: 6| Step: 6
Training loss: 2.8719369908596137
Validation loss: 2.4757068491644723

Epoch: 6| Step: 7
Training loss: 2.9415798667947013
Validation loss: 2.4722665617272956

Epoch: 6| Step: 8
Training loss: 2.9422162119902313
Validation loss: 2.475558601945768

Epoch: 6| Step: 9
Training loss: 2.4232968986252423
Validation loss: 2.473833009992208

Epoch: 6| Step: 10
Training loss: 2.7903616307833823
Validation loss: 2.475820933768561

Epoch: 6| Step: 11
Training loss: 2.315052685560345
Validation loss: 2.4728631791262585

Epoch: 6| Step: 12
Training loss: 2.1302839668358557
Validation loss: 2.474099058197789

Epoch: 6| Step: 13
Training loss: 2.1592076238087023
Validation loss: 2.4738298938270447

Epoch: 88| Step: 0
Training loss: 2.956661145772442
Validation loss: 2.475343879738712

Epoch: 6| Step: 1
Training loss: 2.4427875974980875
Validation loss: 2.474411986401305

Epoch: 6| Step: 2
Training loss: 2.535821718591957
Validation loss: 2.474636656809747

Epoch: 6| Step: 3
Training loss: 2.596588971008464
Validation loss: 2.4740611540190947

Epoch: 6| Step: 4
Training loss: 2.776903334839454
Validation loss: 2.4742146784166024

Epoch: 6| Step: 5
Training loss: 2.6841893763638986
Validation loss: 2.474615059404235

Epoch: 6| Step: 6
Training loss: 2.2476139132238617
Validation loss: 2.4736593982720145

Epoch: 6| Step: 7
Training loss: 2.636382537881664
Validation loss: 2.4731086208677433

Epoch: 6| Step: 8
Training loss: 3.1357681905356767
Validation loss: 2.4743742797264674

Epoch: 6| Step: 9
Training loss: 2.4314470696686157
Validation loss: 2.474090610113337

Epoch: 6| Step: 10
Training loss: 2.0424456245720464
Validation loss: 2.476519932265732

Epoch: 6| Step: 11
Training loss: 2.7458504668083763
Validation loss: 2.475729608724453

Epoch: 6| Step: 12
Training loss: 2.679106268392291
Validation loss: 2.483760733809217

Epoch: 6| Step: 13
Training loss: 2.164641044760339
Validation loss: 2.4714763733073246

Epoch: 89| Step: 0
Training loss: 2.4598479265798963
Validation loss: 2.476324749915501

Epoch: 6| Step: 1
Training loss: 2.117965273852505
Validation loss: 2.470177764724885

Epoch: 6| Step: 2
Training loss: 3.1437802383455047
Validation loss: 2.4720627332050134

Epoch: 6| Step: 3
Training loss: 2.0210511263911806
Validation loss: 2.4713925732816238

Epoch: 6| Step: 4
Training loss: 2.408074418068117
Validation loss: 2.473500578174995

Epoch: 6| Step: 5
Training loss: 2.768510767674119
Validation loss: 2.476558087622786

Epoch: 6| Step: 6
Training loss: 2.8639929330621405
Validation loss: 2.4718682277376085

Epoch: 6| Step: 7
Training loss: 2.807378216732157
Validation loss: 2.474034315582089

Epoch: 6| Step: 8
Training loss: 2.5413974765329463
Validation loss: 2.468828787532306

Epoch: 6| Step: 9
Training loss: 2.2153062556185907
Validation loss: 2.4722696878993835

Epoch: 6| Step: 10
Training loss: 2.6233249951447815
Validation loss: 2.468774272300412

Epoch: 6| Step: 11
Training loss: 2.7721676780551903
Validation loss: 2.473710914419978

Epoch: 6| Step: 12
Training loss: 2.3054272515114675
Validation loss: 2.4709156854638783

Epoch: 6| Step: 13
Training loss: 2.772614520831904
Validation loss: 2.4690893881535567

Epoch: 90| Step: 0
Training loss: 2.4649377674967448
Validation loss: 2.471388939524751

Epoch: 6| Step: 1
Training loss: 2.4195116879267013
Validation loss: 2.471943861640785

Epoch: 6| Step: 2
Training loss: 2.635200843467647
Validation loss: 2.4756312820725968

Epoch: 6| Step: 3
Training loss: 2.7839859781679475
Validation loss: 2.4712760170631682

Epoch: 6| Step: 4
Training loss: 2.039161182807228
Validation loss: 2.473838615867079

Epoch: 6| Step: 5
Training loss: 3.213698191156533
Validation loss: 2.474930651813832

Epoch: 6| Step: 6
Training loss: 2.3785218678387956
Validation loss: 2.4737944431562955

Epoch: 6| Step: 7
Training loss: 2.3678519446380246
Validation loss: 2.474116990205403

Epoch: 6| Step: 8
Training loss: 3.28102881276444
Validation loss: 2.4698583633853852

Epoch: 6| Step: 9
Training loss: 2.5314850874628663
Validation loss: 2.470262458514793

Epoch: 6| Step: 10
Training loss: 2.9626888037929526
Validation loss: 2.4719404376692493

Epoch: 6| Step: 11
Training loss: 2.258528335241528
Validation loss: 2.4745704830478554

Epoch: 6| Step: 12
Training loss: 2.1437326413889273
Validation loss: 2.476954079205539

Epoch: 6| Step: 13
Training loss: 2.378519562360738
Validation loss: 2.4787438991187485

Epoch: 91| Step: 0
Training loss: 2.373140912488625
Validation loss: 2.477513576923306

Epoch: 6| Step: 1
Training loss: 2.9522270977053546
Validation loss: 2.474234849969573

Epoch: 6| Step: 2
Training loss: 2.4246569990640636
Validation loss: 2.471076287798751

Epoch: 6| Step: 3
Training loss: 2.91151640168198
Validation loss: 2.4723164916073275

Epoch: 6| Step: 4
Training loss: 2.5481077615895393
Validation loss: 2.469338149820262

Epoch: 6| Step: 5
Training loss: 2.91404744571346
Validation loss: 2.46584838448336

Epoch: 6| Step: 6
Training loss: 2.293082603214458
Validation loss: 2.4658129318680797

Epoch: 6| Step: 7
Training loss: 2.5848183414225256
Validation loss: 2.4694913563839345

Epoch: 6| Step: 8
Training loss: 2.463499062124716
Validation loss: 2.4650255510013146

Epoch: 6| Step: 9
Training loss: 2.581758193266704
Validation loss: 2.466309447670074

Epoch: 6| Step: 10
Training loss: 2.833591468122272
Validation loss: 2.469485161365417

Epoch: 6| Step: 11
Training loss: 2.742006214376874
Validation loss: 2.4692506403252596

Epoch: 6| Step: 12
Training loss: 2.1108958095496835
Validation loss: 2.478742616648538

Epoch: 6| Step: 13
Training loss: 2.194594948137164
Validation loss: 2.4782234342314573

Epoch: 92| Step: 0
Training loss: 2.4016329574425335
Validation loss: 2.476082419345069

Epoch: 6| Step: 1
Training loss: 2.2486835443199022
Validation loss: 2.473398379484197

Epoch: 6| Step: 2
Training loss: 3.1155276360789794
Validation loss: 2.4702694558775407

Epoch: 6| Step: 3
Training loss: 2.2279234892687954
Validation loss: 2.4683700140323186

Epoch: 6| Step: 4
Training loss: 2.085805824495655
Validation loss: 2.4653912161456386

Epoch: 6| Step: 5
Training loss: 2.685555752826235
Validation loss: 2.472903431634963

Epoch: 6| Step: 6
Training loss: 2.6889037968175877
Validation loss: 2.464382573837753

Epoch: 6| Step: 7
Training loss: 2.379331654062887
Validation loss: 2.465000637278755

Epoch: 6| Step: 8
Training loss: 2.60350939784983
Validation loss: 2.4665171831389388

Epoch: 6| Step: 9
Training loss: 2.879235506315883
Validation loss: 2.460313893582698

Epoch: 6| Step: 10
Training loss: 2.4147120716004444
Validation loss: 2.4722955409499376

Epoch: 6| Step: 11
Training loss: 2.936388332962036
Validation loss: 2.4777504912566983

Epoch: 6| Step: 12
Training loss: 2.764837724086795
Validation loss: 2.485265400310451

Epoch: 6| Step: 13
Training loss: 2.447608917587152
Validation loss: 2.485023014159037

Epoch: 93| Step: 0
Training loss: 2.3428849722043457
Validation loss: 2.4873528298016305

Epoch: 6| Step: 1
Training loss: 2.8471759657992
Validation loss: 2.4869970244453894

Epoch: 6| Step: 2
Training loss: 2.3728008126993774
Validation loss: 2.486097646270698

Epoch: 6| Step: 3
Training loss: 2.370062262767485
Validation loss: 2.485689307386429

Epoch: 6| Step: 4
Training loss: 2.824917908547602
Validation loss: 2.485575387964927

Epoch: 6| Step: 5
Training loss: 2.28135837663289
Validation loss: 2.481556939550893

Epoch: 6| Step: 6
Training loss: 2.580156762587333
Validation loss: 2.4837651333910213

Epoch: 6| Step: 7
Training loss: 2.7166606723586653
Validation loss: 2.4819060078994784

Epoch: 6| Step: 8
Training loss: 2.4738525582171147
Validation loss: 2.480267886542024

Epoch: 6| Step: 9
Training loss: 2.9084956199451693
Validation loss: 2.4814683316867137

Epoch: 6| Step: 10
Training loss: 2.902559295495475
Validation loss: 2.4815388771581746

Epoch: 6| Step: 11
Training loss: 2.8035222345119855
Validation loss: 2.4791920016834808

Epoch: 6| Step: 12
Training loss: 2.545595186856441
Validation loss: 2.479436368189996

Epoch: 6| Step: 13
Training loss: 2.2953463287665183
Validation loss: 2.4797670027533916

Epoch: 94| Step: 0
Training loss: 1.7188118663403968
Validation loss: 2.4775019166632974

Epoch: 6| Step: 1
Training loss: 2.901938080193452
Validation loss: 2.4780703345767927

Epoch: 6| Step: 2
Training loss: 2.908563820815328
Validation loss: 2.4787032204434896

Epoch: 6| Step: 3
Training loss: 2.577669415534834
Validation loss: 2.4789511856765563

Epoch: 6| Step: 4
Training loss: 2.481086521864865
Validation loss: 2.476083478519751

Epoch: 6| Step: 5
Training loss: 2.2622915013806772
Validation loss: 2.473652820127234

Epoch: 6| Step: 6
Training loss: 2.3617965700563794
Validation loss: 2.466665796546095

Epoch: 6| Step: 7
Training loss: 2.7133047008956335
Validation loss: 2.465327147465578

Epoch: 6| Step: 8
Training loss: 2.7815868730873277
Validation loss: 2.4673812548541054

Epoch: 6| Step: 9
Training loss: 2.4657742879368123
Validation loss: 2.4681137810358167

Epoch: 6| Step: 10
Training loss: 2.955553136392749
Validation loss: 2.4659985045454036

Epoch: 6| Step: 11
Training loss: 2.4500258969865074
Validation loss: 2.467409389601028

Epoch: 6| Step: 12
Training loss: 2.7362546727126205
Validation loss: 2.4708616987361744

Epoch: 6| Step: 13
Training loss: 2.5252592992306413
Validation loss: 2.4717598124277456

Epoch: 95| Step: 0
Training loss: 2.540112275473129
Validation loss: 2.4725229911390967

Epoch: 6| Step: 1
Training loss: 2.7443129950312275
Validation loss: 2.476268377799131

Epoch: 6| Step: 2
Training loss: 2.187847327859783
Validation loss: 2.47399462756923

Epoch: 6| Step: 3
Training loss: 2.6816945025427974
Validation loss: 2.4740864422739746

Epoch: 6| Step: 4
Training loss: 2.478828043745468
Validation loss: 2.476597237167865

Epoch: 6| Step: 5
Training loss: 2.6700359953550623
Validation loss: 2.4742778504819953

Epoch: 6| Step: 6
Training loss: 2.802806085074564
Validation loss: 2.4756815132586905

Epoch: 6| Step: 7
Training loss: 2.5470081130738405
Validation loss: 2.477464441467046

Epoch: 6| Step: 8
Training loss: 2.5116923614112423
Validation loss: 2.480006457725444

Epoch: 6| Step: 9
Training loss: 2.551756878169079
Validation loss: 2.4758039851221634

Epoch: 6| Step: 10
Training loss: 2.651034631825964
Validation loss: 2.474853102181074

Epoch: 6| Step: 11
Training loss: 2.546477491565804
Validation loss: 2.4724746163315263

Epoch: 6| Step: 12
Training loss: 2.51244366331186
Validation loss: 2.4707931401638037

Epoch: 6| Step: 13
Training loss: 2.591932280279838
Validation loss: 2.46609873795514

Epoch: 96| Step: 0
Training loss: 2.5267300221729174
Validation loss: 2.4602856776947992

Epoch: 6| Step: 1
Training loss: 2.8355759739647213
Validation loss: 2.461925134831888

Epoch: 6| Step: 2
Training loss: 3.3462820003263336
Validation loss: 2.458096789170603

Epoch: 6| Step: 3
Training loss: 2.758594866644094
Validation loss: 2.460499719504103

Epoch: 6| Step: 4
Training loss: 2.551722307684492
Validation loss: 2.4561418905886745

Epoch: 6| Step: 5
Training loss: 2.007831736669156
Validation loss: 2.460537154315812

Epoch: 6| Step: 6
Training loss: 2.0194625400092194
Validation loss: 2.4681347671898988

Epoch: 6| Step: 7
Training loss: 2.3185592860839117
Validation loss: 2.463640728518691

Epoch: 6| Step: 8
Training loss: 2.0736743047234567
Validation loss: 2.4648387765809034

Epoch: 6| Step: 9
Training loss: 2.779936062194182
Validation loss: 2.4689521706770057

Epoch: 6| Step: 10
Training loss: 2.6089005638581955
Validation loss: 2.4615970757044003

Epoch: 6| Step: 11
Training loss: 2.9564718022117087
Validation loss: 2.463545903423341

Epoch: 6| Step: 12
Training loss: 2.6297894336258105
Validation loss: 2.4663679968810044

Epoch: 6| Step: 13
Training loss: 2.318854904886412
Validation loss: 2.466707020077244

Epoch: 97| Step: 0
Training loss: 2.2488542394561333
Validation loss: 2.46880932829185

Epoch: 6| Step: 1
Training loss: 2.5541235625056173
Validation loss: 2.4591156205149503

Epoch: 6| Step: 2
Training loss: 2.6161715685027778
Validation loss: 2.4716951693046596

Epoch: 6| Step: 3
Training loss: 2.5549973232904737
Validation loss: 2.47213263891076

Epoch: 6| Step: 4
Training loss: 2.8577212361341595
Validation loss: 2.470626512226723

Epoch: 6| Step: 5
Training loss: 2.2893683844326786
Validation loss: 2.4689834822329706

Epoch: 6| Step: 6
Training loss: 2.6765051092351686
Validation loss: 2.4710968387410386

Epoch: 6| Step: 7
Training loss: 2.4581832732182978
Validation loss: 2.4690464179834906

Epoch: 6| Step: 8
Training loss: 2.453251537776952
Validation loss: 2.4677287134048482

Epoch: 6| Step: 9
Training loss: 2.3928577244153972
Validation loss: 2.46949301374989

Epoch: 6| Step: 10
Training loss: 2.602736749351686
Validation loss: 2.468357256138705

Epoch: 6| Step: 11
Training loss: 2.4353524553390775
Validation loss: 2.4670974576503126

Epoch: 6| Step: 12
Training loss: 2.609163835398688
Validation loss: 2.470954731429692

Epoch: 6| Step: 13
Training loss: 2.9845934083917576
Validation loss: 2.473163747508843

Epoch: 98| Step: 0
Training loss: 2.444295946099641
Validation loss: 2.472073165338556

Epoch: 6| Step: 1
Training loss: 2.564347508297295
Validation loss: 2.4682365720570325

Epoch: 6| Step: 2
Training loss: 2.3888806729828462
Validation loss: 2.4702599330249795

Epoch: 6| Step: 3
Training loss: 1.9919919983921435
Validation loss: 2.4719597275878264

Epoch: 6| Step: 4
Training loss: 2.4363145146212246
Validation loss: 2.47221051497908

Epoch: 6| Step: 5
Training loss: 2.8254077988383073
Validation loss: 2.468246054418849

Epoch: 6| Step: 6
Training loss: 2.7063298273096335
Validation loss: 2.4722314261742566

Epoch: 6| Step: 7
Training loss: 2.1497221833758524
Validation loss: 2.4693219290961594

Epoch: 6| Step: 8
Training loss: 2.4999341956061243
Validation loss: 2.469290340217526

Epoch: 6| Step: 9
Training loss: 2.8105417851977887
Validation loss: 2.469770727520663

Epoch: 6| Step: 10
Training loss: 2.1197607864517556
Validation loss: 2.4665426937300277

Epoch: 6| Step: 11
Training loss: 2.589949247533118
Validation loss: 2.467513462829966

Epoch: 6| Step: 12
Training loss: 3.3686681620719523
Validation loss: 2.4649834207901904

Epoch: 6| Step: 13
Training loss: 2.6279745959269007
Validation loss: 2.466254683404888

Epoch: 99| Step: 0
Training loss: 2.5357543051802622
Validation loss: 2.464984871622518

Epoch: 6| Step: 1
Training loss: 2.4102323201693068
Validation loss: 2.4644151124488065

Epoch: 6| Step: 2
Training loss: 2.465021915918695
Validation loss: 2.4546855975879636

Epoch: 6| Step: 3
Training loss: 2.151090336603851
Validation loss: 2.4595782366047176

Epoch: 6| Step: 4
Training loss: 2.636287670924478
Validation loss: 2.4547214375397375

Epoch: 6| Step: 5
Training loss: 2.839875988469006
Validation loss: 2.46080883840256

Epoch: 6| Step: 6
Training loss: 2.952942534768914
Validation loss: 2.4584859111010577

Epoch: 6| Step: 7
Training loss: 2.7940611936930733
Validation loss: 2.4644892901688813

Epoch: 6| Step: 8
Training loss: 2.736248311982869
Validation loss: 2.469401309858744

Epoch: 6| Step: 9
Training loss: 2.1926281356341106
Validation loss: 2.4721122492810372

Epoch: 6| Step: 10
Training loss: 2.4994614974839653
Validation loss: 2.4770785815870067

Epoch: 6| Step: 11
Training loss: 2.4931185427199805
Validation loss: 2.4765311960193386

Epoch: 6| Step: 12
Training loss: 2.636850040134113
Validation loss: 2.4739383308538967

Epoch: 6| Step: 13
Training loss: 2.4753925436047677
Validation loss: 2.4761218492243096

Epoch: 100| Step: 0
Training loss: 2.136635508476245
Validation loss: 2.474068092454548

Epoch: 6| Step: 1
Training loss: 3.463509665128633
Validation loss: 2.4722934032817796

Epoch: 6| Step: 2
Training loss: 1.8272221610009758
Validation loss: 2.473493975510492

Epoch: 6| Step: 3
Training loss: 1.8942707315241616
Validation loss: 2.4694738332913655

Epoch: 6| Step: 4
Training loss: 2.9845997990330604
Validation loss: 2.4670172457247492

Epoch: 6| Step: 5
Training loss: 2.5593324017269223
Validation loss: 2.4685069258249333

Epoch: 6| Step: 6
Training loss: 2.1981554187950914
Validation loss: 2.4682944960010453

Epoch: 6| Step: 7
Training loss: 2.665017054862869
Validation loss: 2.464774790271252

Epoch: 6| Step: 8
Training loss: 2.6808383794643573
Validation loss: 2.4662665901844156

Epoch: 6| Step: 9
Training loss: 2.420477482373216
Validation loss: 2.4638201587738244

Epoch: 6| Step: 10
Training loss: 2.4974932500769023
Validation loss: 2.465800410527151

Epoch: 6| Step: 11
Training loss: 3.0474511799958353
Validation loss: 2.4644130808137144

Epoch: 6| Step: 12
Training loss: 2.3417165454516358
Validation loss: 2.4561262217646487

Epoch: 6| Step: 13
Training loss: 2.838246816084477
Validation loss: 2.4666175807330366

Epoch: 101| Step: 0
Training loss: 2.7034874821876307
Validation loss: 2.4722300599601414

Epoch: 6| Step: 1
Training loss: 2.2539286752838037
Validation loss: 2.4718491460876413

Epoch: 6| Step: 2
Training loss: 2.384407685182061
Validation loss: 2.470894264597801

Epoch: 6| Step: 3
Training loss: 3.313022932125811
Validation loss: 2.4758875716397353

Epoch: 6| Step: 4
Training loss: 2.6474459882347205
Validation loss: 2.476627770174499

Epoch: 6| Step: 5
Training loss: 2.694530074953535
Validation loss: 2.472016005065075

Epoch: 6| Step: 6
Training loss: 2.789430225397445
Validation loss: 2.4755725988145336

Epoch: 6| Step: 7
Training loss: 2.7746706517985427
Validation loss: 2.47280293572444

Epoch: 6| Step: 8
Training loss: 2.342232581704233
Validation loss: 2.4726287534931553

Epoch: 6| Step: 9
Training loss: 2.741469069049508
Validation loss: 2.4665115767343297

Epoch: 6| Step: 10
Training loss: 2.5764593118763575
Validation loss: 2.467508623622781

Epoch: 6| Step: 11
Training loss: 2.266064936576939
Validation loss: 2.4745457537430493

Epoch: 6| Step: 12
Training loss: 2.393388136162772
Validation loss: 2.462276445688124

Epoch: 6| Step: 13
Training loss: 1.7799641167313447
Validation loss: 2.4649372355148245

Epoch: 102| Step: 0
Training loss: 2.5895609291275736
Validation loss: 2.4624702806825574

Epoch: 6| Step: 1
Training loss: 3.040090352773638
Validation loss: 2.4612394359588583

Epoch: 6| Step: 2
Training loss: 2.71413213610876
Validation loss: 2.467074980856667

Epoch: 6| Step: 3
Training loss: 2.8963304454314773
Validation loss: 2.466031956418429

Epoch: 6| Step: 4
Training loss: 2.463397053405035
Validation loss: 2.4609128759050383

Epoch: 6| Step: 5
Training loss: 2.3550954241361666
Validation loss: 2.465343660420495

Epoch: 6| Step: 6
Training loss: 1.8896963818207169
Validation loss: 2.4623373742792314

Epoch: 6| Step: 7
Training loss: 2.2132739335251244
Validation loss: 2.467499798688072

Epoch: 6| Step: 8
Training loss: 2.5304176940195617
Validation loss: 2.465674557044793

Epoch: 6| Step: 9
Training loss: 2.153762598958418
Validation loss: 2.4658712833789256

Epoch: 6| Step: 10
Training loss: 3.2381509310610497
Validation loss: 2.4654199457740433

Epoch: 6| Step: 11
Training loss: 2.3497558953951656
Validation loss: 2.465486244258699

Epoch: 6| Step: 12
Training loss: 2.575269747928781
Validation loss: 2.4652670422769494

Epoch: 6| Step: 13
Training loss: 2.5763876870133395
Validation loss: 2.470341648200883

Epoch: 103| Step: 0
Training loss: 2.083765620840707
Validation loss: 2.4696755424711068

Epoch: 6| Step: 1
Training loss: 2.723824861853595
Validation loss: 2.472246341968656

Epoch: 6| Step: 2
Training loss: 2.904694171505921
Validation loss: 2.469937340794904

Epoch: 6| Step: 3
Training loss: 2.4166382316582684
Validation loss: 2.4741967872678328

Epoch: 6| Step: 4
Training loss: 2.698558500279576
Validation loss: 2.472423669082319

Epoch: 6| Step: 5
Training loss: 2.8174449259292595
Validation loss: 2.477410830019937

Epoch: 6| Step: 6
Training loss: 2.2129958852461646
Validation loss: 2.471297643649565

Epoch: 6| Step: 7
Training loss: 2.252365034931024
Validation loss: 2.4771525966584607

Epoch: 6| Step: 8
Training loss: 2.6303079617708045
Validation loss: 2.476579202782176

Epoch: 6| Step: 9
Training loss: 2.992546358825005
Validation loss: 2.4747371103688587

Epoch: 6| Step: 10
Training loss: 2.4756755985673973
Validation loss: 2.4713817041522357

Epoch: 6| Step: 11
Training loss: 3.1637854937578647
Validation loss: 2.4668751691855713

Epoch: 6| Step: 12
Training loss: 1.665620244862137
Validation loss: 2.466094379364947

Epoch: 6| Step: 13
Training loss: 2.6877042116159626
Validation loss: 2.46662118929962

Epoch: 104| Step: 0
Training loss: 2.3548104297770482
Validation loss: 2.4696547866232117

Epoch: 6| Step: 1
Training loss: 2.3772082854006995
Validation loss: 2.4641477284258975

Epoch: 6| Step: 2
Training loss: 2.5631839490733217
Validation loss: 2.4656641703605864

Epoch: 6| Step: 3
Training loss: 2.494004499026036
Validation loss: 2.4644126696492656

Epoch: 6| Step: 4
Training loss: 2.5363696093906887
Validation loss: 2.459716001247028

Epoch: 6| Step: 5
Training loss: 2.3031652563502867
Validation loss: 2.4599869925974356

Epoch: 6| Step: 6
Training loss: 2.462905629631881
Validation loss: 2.4580440364487797

Epoch: 6| Step: 7
Training loss: 2.4309230986733863
Validation loss: 2.46291598761475

Epoch: 6| Step: 8
Training loss: 2.655682491937965
Validation loss: 2.45758803972142

Epoch: 6| Step: 9
Training loss: 2.951316158420776
Validation loss: 2.4549081157289065

Epoch: 6| Step: 10
Training loss: 2.723580639962836
Validation loss: 2.4603131021855593

Epoch: 6| Step: 11
Training loss: 2.9430858988745636
Validation loss: 2.460586215989256

Epoch: 6| Step: 12
Training loss: 2.921240100117445
Validation loss: 2.4645190138390034

Epoch: 6| Step: 13
Training loss: 1.916329920944137
Validation loss: 2.465053011418991

Epoch: 105| Step: 0
Training loss: 2.6161645512813414
Validation loss: 2.470667870713565

Epoch: 6| Step: 1
Training loss: 2.658013689521113
Validation loss: 2.4679283513128465

Epoch: 6| Step: 2
Training loss: 2.624913895420957
Validation loss: 2.46701176931344

Epoch: 6| Step: 3
Training loss: 2.4074205796242762
Validation loss: 2.471817878870141

Epoch: 6| Step: 4
Training loss: 2.5060203542164357
Validation loss: 2.468364677457713

Epoch: 6| Step: 5
Training loss: 2.864671315084723
Validation loss: 2.474016053714336

Epoch: 6| Step: 6
Training loss: 2.057575589317587
Validation loss: 2.4720386057136388

Epoch: 6| Step: 7
Training loss: 2.2910842039701196
Validation loss: 2.4694556825189244

Epoch: 6| Step: 8
Training loss: 3.088770085465919
Validation loss: 2.4673368860123146

Epoch: 6| Step: 9
Training loss: 2.1366627352832603
Validation loss: 2.462628480944971

Epoch: 6| Step: 10
Training loss: 2.0330750702649936
Validation loss: 2.460151845670238

Epoch: 6| Step: 11
Training loss: 2.6603957193058294
Validation loss: 2.4636537930952196

Epoch: 6| Step: 12
Training loss: 2.6261474281408983
Validation loss: 2.4564735899699404

Epoch: 6| Step: 13
Training loss: 2.920682286280899
Validation loss: 2.461787744005408

Epoch: 106| Step: 0
Training loss: 2.0382865510917676
Validation loss: 2.4584497542750894

Epoch: 6| Step: 1
Training loss: 2.47661872101998
Validation loss: 2.452361658311853

Epoch: 6| Step: 2
Training loss: 2.6392853176987545
Validation loss: 2.4568486390939275

Epoch: 6| Step: 3
Training loss: 2.652268061730725
Validation loss: 2.465532854402699

Epoch: 6| Step: 4
Training loss: 2.6545016649750792
Validation loss: 2.45601959506801

Epoch: 6| Step: 5
Training loss: 2.22303165423207
Validation loss: 2.453060238382237

Epoch: 6| Step: 6
Training loss: 1.5512151999800576
Validation loss: 2.45746219415777

Epoch: 6| Step: 7
Training loss: 2.7277449964775125
Validation loss: 2.462337237108888

Epoch: 6| Step: 8
Training loss: 3.0116270771249822
Validation loss: 2.460321775238269

Epoch: 6| Step: 9
Training loss: 2.3373145697987163
Validation loss: 2.4686388723093193

Epoch: 6| Step: 10
Training loss: 2.5979967396093855
Validation loss: 2.4615167006843577

Epoch: 6| Step: 11
Training loss: 2.6469355912912915
Validation loss: 2.462154737302087

Epoch: 6| Step: 12
Training loss: 3.0482162262321935
Validation loss: 2.461388166226341

Epoch: 6| Step: 13
Training loss: 2.721546182140034
Validation loss: 2.4697951989461693

Epoch: 107| Step: 0
Training loss: 2.6808944075100665
Validation loss: 2.4694853705481137

Epoch: 6| Step: 1
Training loss: 2.393876600777186
Validation loss: 2.4668694105681794

Epoch: 6| Step: 2
Training loss: 2.7570434016594723
Validation loss: 2.459822047705519

Epoch: 6| Step: 3
Training loss: 2.5813032500383306
Validation loss: 2.4688177461624177

Epoch: 6| Step: 4
Training loss: 2.105780017612111
Validation loss: 2.4558898668240303

Epoch: 6| Step: 5
Training loss: 2.2398511031595274
Validation loss: 2.4592648670139727

Epoch: 6| Step: 6
Training loss: 2.9257869542146495
Validation loss: 2.4559934493432096

Epoch: 6| Step: 7
Training loss: 2.9920746704820442
Validation loss: 2.457201945203263

Epoch: 6| Step: 8
Training loss: 2.8946608953232458
Validation loss: 2.4567192619452753

Epoch: 6| Step: 9
Training loss: 2.075106730646161
Validation loss: 2.4581135527648708

Epoch: 6| Step: 10
Training loss: 2.452717546774967
Validation loss: 2.4564311917606267

Epoch: 6| Step: 11
Training loss: 3.043352173924143
Validation loss: 2.445899543537557

Epoch: 6| Step: 12
Training loss: 2.234120494512927
Validation loss: 2.453416778641502

Epoch: 6| Step: 13
Training loss: 2.189298272080189
Validation loss: 2.4643217038750884

Epoch: 108| Step: 0
Training loss: 2.4917628007117747
Validation loss: 2.461819074049902

Epoch: 6| Step: 1
Training loss: 2.3804920341872817
Validation loss: 2.4602801862994914

Epoch: 6| Step: 2
Training loss: 2.603486503787341
Validation loss: 2.4652309446109992

Epoch: 6| Step: 3
Training loss: 2.7156900308107375
Validation loss: 2.4601556090887806

Epoch: 6| Step: 4
Training loss: 2.584874237070765
Validation loss: 2.4576607258168397

Epoch: 6| Step: 5
Training loss: 2.3807135671102997
Validation loss: 2.463912836438932

Epoch: 6| Step: 6
Training loss: 2.134609597132841
Validation loss: 2.458462612071849

Epoch: 6| Step: 7
Training loss: 2.1159407544846944
Validation loss: 2.4599701286973077

Epoch: 6| Step: 8
Training loss: 2.8940491886242636
Validation loss: 2.462643713073593

Epoch: 6| Step: 9
Training loss: 3.259464128835525
Validation loss: 2.4630782247272394

Epoch: 6| Step: 10
Training loss: 2.0893862214558774
Validation loss: 2.455747672395905

Epoch: 6| Step: 11
Training loss: 2.888647279254231
Validation loss: 2.4557754711803095

Epoch: 6| Step: 12
Training loss: 2.5684775437703915
Validation loss: 2.4563660320024905

Epoch: 6| Step: 13
Training loss: 2.307852430779455
Validation loss: 2.456493179280762

Epoch: 109| Step: 0
Training loss: 2.6852698720777903
Validation loss: 2.455495171238408

Epoch: 6| Step: 1
Training loss: 3.0615453983961154
Validation loss: 2.4622498096325653

Epoch: 6| Step: 2
Training loss: 2.6197659217575615
Validation loss: 2.4599586437336303

Epoch: 6| Step: 3
Training loss: 2.4349367406880087
Validation loss: 2.4669787334097113

Epoch: 6| Step: 4
Training loss: 2.2365093574217303
Validation loss: 2.469393634189981

Epoch: 6| Step: 5
Training loss: 3.0205026028992417
Validation loss: 2.467224921598415

Epoch: 6| Step: 6
Training loss: 2.137769590632217
Validation loss: 2.467611686217034

Epoch: 6| Step: 7
Training loss: 2.2234704750387717
Validation loss: 2.4650970828558165

Epoch: 6| Step: 8
Training loss: 2.353071267374463
Validation loss: 2.468942924397565

Epoch: 6| Step: 9
Training loss: 2.856338973715072
Validation loss: 2.4659369009927103

Epoch: 6| Step: 10
Training loss: 2.873361203382293
Validation loss: 2.467893443820846

Epoch: 6| Step: 11
Training loss: 2.1405532713518522
Validation loss: 2.464755879411238

Epoch: 6| Step: 12
Training loss: 2.515920491367777
Validation loss: 2.4624234189229646

Epoch: 6| Step: 13
Training loss: 2.381219249937303
Validation loss: 2.463988496825792

Epoch: 110| Step: 0
Training loss: 3.014440748686852
Validation loss: 2.4607327330639523

Epoch: 6| Step: 1
Training loss: 2.95567381326656
Validation loss: 2.455939296390556

Epoch: 6| Step: 2
Training loss: 2.254555435172556
Validation loss: 2.459950970908047

Epoch: 6| Step: 3
Training loss: 2.5672058067667414
Validation loss: 2.46263917894346

Epoch: 6| Step: 4
Training loss: 2.4359672813723807
Validation loss: 2.459646768511776

Epoch: 6| Step: 5
Training loss: 2.519377001395125
Validation loss: 2.4614215677501874

Epoch: 6| Step: 6
Training loss: 3.161625882296084
Validation loss: 2.4644399756568878

Epoch: 6| Step: 7
Training loss: 2.966413200637515
Validation loss: 2.4656185780205067

Epoch: 6| Step: 8
Training loss: 2.070534748168111
Validation loss: 2.4593396767160214

Epoch: 6| Step: 9
Training loss: 2.690430307820936
Validation loss: 2.4599188497773516

Epoch: 6| Step: 10
Training loss: 2.5648744328569464
Validation loss: 2.4568224051484164

Epoch: 6| Step: 11
Training loss: 2.416441896004863
Validation loss: 2.4679513758182345

Epoch: 6| Step: 12
Training loss: 1.9604065924887968
Validation loss: 2.4705355339858373

Epoch: 6| Step: 13
Training loss: 1.81532068115114
Validation loss: 2.4714358081470174

Epoch: 111| Step: 0
Training loss: 3.053464210431687
Validation loss: 2.4653398162724924

Epoch: 6| Step: 1
Training loss: 2.135757369487649
Validation loss: 2.4683310559674596

Epoch: 6| Step: 2
Training loss: 2.5431986293226245
Validation loss: 2.4641005841079573

Epoch: 6| Step: 3
Training loss: 2.869714937840149
Validation loss: 2.465429350323007

Epoch: 6| Step: 4
Training loss: 2.58333454337143
Validation loss: 2.4601498751226103

Epoch: 6| Step: 5
Training loss: 2.0622689088770048
Validation loss: 2.4564413586623344

Epoch: 6| Step: 6
Training loss: 2.508802557196579
Validation loss: 2.4509581777931

Epoch: 6| Step: 7
Training loss: 2.4644278826881805
Validation loss: 2.452431517982237

Epoch: 6| Step: 8
Training loss: 2.755590305399803
Validation loss: 2.453496528238997

Epoch: 6| Step: 9
Training loss: 2.946889116368711
Validation loss: 2.4552843993009037

Epoch: 6| Step: 10
Training loss: 2.031174174140613
Validation loss: 2.4563658864102313

Epoch: 6| Step: 11
Training loss: 2.844980162432383
Validation loss: 2.458222860882152

Epoch: 6| Step: 12
Training loss: 2.4677065805150113
Validation loss: 2.4652901883477063

Epoch: 6| Step: 13
Training loss: 2.3412400093415355
Validation loss: 2.469518598190027

Epoch: 112| Step: 0
Training loss: 1.9639097993496772
Validation loss: 2.467558045894607

Epoch: 6| Step: 1
Training loss: 2.124363523420291
Validation loss: 2.466001509753897

Epoch: 6| Step: 2
Training loss: 2.785993495684368
Validation loss: 2.4691496738806546

Epoch: 6| Step: 3
Training loss: 2.0957447627358317
Validation loss: 2.464937912582703

Epoch: 6| Step: 4
Training loss: 2.773863166058065
Validation loss: 2.4642105538626167

Epoch: 6| Step: 5
Training loss: 2.8889564441052933
Validation loss: 2.458905708361571

Epoch: 6| Step: 6
Training loss: 2.5130392492275084
Validation loss: 2.4458683507471544

Epoch: 6| Step: 7
Training loss: 1.9031525132268523
Validation loss: 2.450644208590713

Epoch: 6| Step: 8
Training loss: 2.5659009183395205
Validation loss: 2.449293736755373

Epoch: 6| Step: 9
Training loss: 2.7399144329159126
Validation loss: 2.4460067169843738

Epoch: 6| Step: 10
Training loss: 2.8286203193017267
Validation loss: 2.442873533691066

Epoch: 6| Step: 11
Training loss: 2.9129021100962063
Validation loss: 2.4423477676726577

Epoch: 6| Step: 12
Training loss: 3.1052014121859632
Validation loss: 2.446824196604354

Epoch: 6| Step: 13
Training loss: 2.1960455597075574
Validation loss: 2.4465756794731295

Epoch: 113| Step: 0
Training loss: 2.1259415728344835
Validation loss: 2.451671291098868

Epoch: 6| Step: 1
Training loss: 2.734573705810203
Validation loss: 2.4490060262661726

Epoch: 6| Step: 2
Training loss: 2.9916800524342193
Validation loss: 2.4549505159631297

Epoch: 6| Step: 3
Training loss: 2.048755277169526
Validation loss: 2.455200888215533

Epoch: 6| Step: 4
Training loss: 1.394766736917678
Validation loss: 2.4528361512876637

Epoch: 6| Step: 5
Training loss: 3.4504689492217513
Validation loss: 2.4580842285240836

Epoch: 6| Step: 6
Training loss: 2.172670726323244
Validation loss: 2.4602377406304794

Epoch: 6| Step: 7
Training loss: 2.424159688964677
Validation loss: 2.4675745358686827

Epoch: 6| Step: 8
Training loss: 2.5088836188815216
Validation loss: 2.469056339813879

Epoch: 6| Step: 9
Training loss: 3.315041484774567
Validation loss: 2.470772763603984

Epoch: 6| Step: 10
Training loss: 2.4124106276961004
Validation loss: 2.4666198844167773

Epoch: 6| Step: 11
Training loss: 2.3772241317235383
Validation loss: 2.467784523797503

Epoch: 6| Step: 12
Training loss: 2.1527486929073847
Validation loss: 2.464975038186688

Epoch: 6| Step: 13
Training loss: 2.7536007109705944
Validation loss: 2.46446162998459

Epoch: 114| Step: 0
Training loss: 2.2582046537818083
Validation loss: 2.4677539216308846

Epoch: 6| Step: 1
Training loss: 2.134366988562186
Validation loss: 2.463384124578942

Epoch: 6| Step: 2
Training loss: 3.079371940436294
Validation loss: 2.467071252153432

Epoch: 6| Step: 3
Training loss: 2.219122519584597
Validation loss: 2.462108934723092

Epoch: 6| Step: 4
Training loss: 3.0049905593393715
Validation loss: 2.4570974918203583

Epoch: 6| Step: 5
Training loss: 2.2261343561087577
Validation loss: 2.455813592866593

Epoch: 6| Step: 6
Training loss: 2.594206069008205
Validation loss: 2.4547007575952895

Epoch: 6| Step: 7
Training loss: 2.448802079040781
Validation loss: 2.4507588113218253

Epoch: 6| Step: 8
Training loss: 2.003913388602051
Validation loss: 2.4509541165308497

Epoch: 6| Step: 9
Training loss: 2.609413740590035
Validation loss: 2.4485987801859506

Epoch: 6| Step: 10
Training loss: 2.833011347108705
Validation loss: 2.456259278206292

Epoch: 6| Step: 11
Training loss: 2.6804734570498874
Validation loss: 2.449384100695036

Epoch: 6| Step: 12
Training loss: 2.939110334819314
Validation loss: 2.4507315474482265

Epoch: 6| Step: 13
Training loss: 2.236782777371498
Validation loss: 2.4476305909429734

Epoch: 115| Step: 0
Training loss: 2.0578785763315377
Validation loss: 2.446219409146435

Epoch: 6| Step: 1
Training loss: 2.3538416972964424
Validation loss: 2.4474926899277047

Epoch: 6| Step: 2
Training loss: 2.749050843557883
Validation loss: 2.447423614736108

Epoch: 6| Step: 3
Training loss: 2.6810763571827705
Validation loss: 2.4418572744196934

Epoch: 6| Step: 4
Training loss: 2.445106960835611
Validation loss: 2.448684366387425

Epoch: 6| Step: 5
Training loss: 2.8843460759358615
Validation loss: 2.4480354821338883

Epoch: 6| Step: 6
Training loss: 2.717065398865818
Validation loss: 2.4473155534892146

Epoch: 6| Step: 7
Training loss: 2.604857137061127
Validation loss: 2.4511439105258654

Epoch: 6| Step: 8
Training loss: 2.4539090986942735
Validation loss: 2.444088256658598

Epoch: 6| Step: 9
Training loss: 2.7027411215861017
Validation loss: 2.4440886387258276

Epoch: 6| Step: 10
Training loss: 2.2469746064880547
Validation loss: 2.448246374990186

Epoch: 6| Step: 11
Training loss: 2.5806235488440366
Validation loss: 2.4451215870587064

Epoch: 6| Step: 12
Training loss: 2.425817615279454
Validation loss: 2.4506143409544543

Epoch: 6| Step: 13
Training loss: 2.650317579709162
Validation loss: 2.446418195350449

Epoch: 116| Step: 0
Training loss: 1.9979739655945479
Validation loss: 2.4475760419057986

Epoch: 6| Step: 1
Training loss: 2.479383143956604
Validation loss: 2.4498872549364026

Epoch: 6| Step: 2
Training loss: 2.683786710106949
Validation loss: 2.449870183645449

Epoch: 6| Step: 3
Training loss: 2.52362126014958
Validation loss: 2.4566888211759714

Epoch: 6| Step: 4
Training loss: 3.140954147475408
Validation loss: 2.4577188424261935

Epoch: 6| Step: 5
Training loss: 1.9899582301601164
Validation loss: 2.4542393512249356

Epoch: 6| Step: 6
Training loss: 2.909293601170652
Validation loss: 2.452007338762321

Epoch: 6| Step: 7
Training loss: 2.8027815014004007
Validation loss: 2.451492608757581

Epoch: 6| Step: 8
Training loss: 2.0543026872006105
Validation loss: 2.457362522097122

Epoch: 6| Step: 9
Training loss: 2.6281565806048377
Validation loss: 2.451064424831979

Epoch: 6| Step: 10
Training loss: 2.7220403119591046
Validation loss: 2.4559630481164367

Epoch: 6| Step: 11
Training loss: 2.680195662914166
Validation loss: 2.4478592940296444

Epoch: 6| Step: 12
Training loss: 2.59527046955026
Validation loss: 2.4543078055435115

Epoch: 6| Step: 13
Training loss: 2.0136857277424216
Validation loss: 2.4634998041081517

Epoch: 117| Step: 0
Training loss: 2.8350933069015367
Validation loss: 2.4578453700379046

Epoch: 6| Step: 1
Training loss: 2.8233258747271037
Validation loss: 2.454307724591093

Epoch: 6| Step: 2
Training loss: 2.61494919444925
Validation loss: 2.455207556259255

Epoch: 6| Step: 3
Training loss: 2.691273369021021
Validation loss: 2.45758930089322

Epoch: 6| Step: 4
Training loss: 2.2351306624482947
Validation loss: 2.4604369931157946

Epoch: 6| Step: 5
Training loss: 2.6508716661147442
Validation loss: 2.4626234627124726

Epoch: 6| Step: 6
Training loss: 2.232236892899252
Validation loss: 2.4650679061697667

Epoch: 6| Step: 7
Training loss: 2.733806703639632
Validation loss: 2.460022319185893

Epoch: 6| Step: 8
Training loss: 2.8001612957409305
Validation loss: 2.4570807132078416

Epoch: 6| Step: 9
Training loss: 2.2930147078683314
Validation loss: 2.4594505379243756

Epoch: 6| Step: 10
Training loss: 2.2479959676034404
Validation loss: 2.4501916473065286

Epoch: 6| Step: 11
Training loss: 2.412344015346166
Validation loss: 2.446586707529725

Epoch: 6| Step: 12
Training loss: 2.98956502627484
Validation loss: 2.4507293099000838

Epoch: 6| Step: 13
Training loss: 1.6452156147049597
Validation loss: 2.4481539320435233

Epoch: 118| Step: 0
Training loss: 2.8651989269599145
Validation loss: 2.45071307140072

Epoch: 6| Step: 1
Training loss: 2.669346734185105
Validation loss: 2.447519429759339

Epoch: 6| Step: 2
Training loss: 2.5992764346455366
Validation loss: 2.4490285634288043

Epoch: 6| Step: 3
Training loss: 2.4925429709507783
Validation loss: 2.4570791849333458

Epoch: 6| Step: 4
Training loss: 3.1615100502338267
Validation loss: 2.4601932106540683

Epoch: 6| Step: 5
Training loss: 2.4266433535318863
Validation loss: 2.4608138845570613

Epoch: 6| Step: 6
Training loss: 2.7601190274598393
Validation loss: 2.4639397770604443

Epoch: 6| Step: 7
Training loss: 2.455458484886722
Validation loss: 2.4708550809789807

Epoch: 6| Step: 8
Training loss: 2.1014687896596453
Validation loss: 2.4664283169039267

Epoch: 6| Step: 9
Training loss: 2.115050190720101
Validation loss: 2.4670514408063067

Epoch: 6| Step: 10
Training loss: 2.356852515615899
Validation loss: 2.463812094773143

Epoch: 6| Step: 11
Training loss: 2.231434307446878
Validation loss: 2.462485659020196

Epoch: 6| Step: 12
Training loss: 2.7465499564099356
Validation loss: 2.464309424891111

Epoch: 6| Step: 13
Training loss: 2.5397766048082935
Validation loss: 2.4619243036017036

Epoch: 119| Step: 0
Training loss: 2.6861963880753095
Validation loss: 2.4666384749047294

Epoch: 6| Step: 1
Training loss: 2.1867968927931076
Validation loss: 2.449781305582247

Epoch: 6| Step: 2
Training loss: 2.5457070135649102
Validation loss: 2.458310940742879

Epoch: 6| Step: 3
Training loss: 2.6218809034533415
Validation loss: 2.449977029316451

Epoch: 6| Step: 4
Training loss: 2.1751633593718
Validation loss: 2.453871433221977

Epoch: 6| Step: 5
Training loss: 2.9785165855531237
Validation loss: 2.453385276512339

Epoch: 6| Step: 6
Training loss: 2.509760400195194
Validation loss: 2.4583405693939095

Epoch: 6| Step: 7
Training loss: 2.757828752939891
Validation loss: 2.4618016254756556

Epoch: 6| Step: 8
Training loss: 2.527251299386158
Validation loss: 2.4656305281621846

Epoch: 6| Step: 9
Training loss: 2.0142491102516127
Validation loss: 2.4570355842362077

Epoch: 6| Step: 10
Training loss: 2.9856392145008144
Validation loss: 2.456046598002748

Epoch: 6| Step: 11
Training loss: 2.540976216907135
Validation loss: 2.457272014940915

Epoch: 6| Step: 12
Training loss: 1.9808091693068166
Validation loss: 2.4539975276966692

Epoch: 6| Step: 13
Training loss: 2.6999040939817136
Validation loss: 2.4504415490475724

Epoch: 120| Step: 0
Training loss: 2.7800595596849194
Validation loss: 2.4408897240053773

Epoch: 6| Step: 1
Training loss: 2.2353364169839685
Validation loss: 2.446416652293733

Epoch: 6| Step: 2
Training loss: 2.539673998630066
Validation loss: 2.4504270843075115

Epoch: 6| Step: 3
Training loss: 2.6279011997364305
Validation loss: 2.446580828071276

Epoch: 6| Step: 4
Training loss: 2.183704898851059
Validation loss: 2.452103582367197

Epoch: 6| Step: 5
Training loss: 2.5191835621898067
Validation loss: 2.454834797772273

Epoch: 6| Step: 6
Training loss: 2.3764962953955573
Validation loss: 2.454082472241582

Epoch: 6| Step: 7
Training loss: 2.318696046597386
Validation loss: 2.4530482836995806

Epoch: 6| Step: 8
Training loss: 2.7759922350024584
Validation loss: 2.459843904225906

Epoch: 6| Step: 9
Training loss: 2.2547157460305165
Validation loss: 2.4612365056603243

Epoch: 6| Step: 10
Training loss: 2.8038961415123236
Validation loss: 2.4483851424159204

Epoch: 6| Step: 11
Training loss: 2.5087839306949267
Validation loss: 2.4521402702970323

Epoch: 6| Step: 12
Training loss: 2.564692977763699
Validation loss: 2.4567466211140134

Epoch: 6| Step: 13
Training loss: 2.8691665513209146
Validation loss: 2.456784113112894

Epoch: 121| Step: 0
Training loss: 2.669839005206251
Validation loss: 2.452435665913531

Epoch: 6| Step: 1
Training loss: 2.769645742355324
Validation loss: 2.457008325321251

Epoch: 6| Step: 2
Training loss: 1.9395649273658389
Validation loss: 2.456285113729967

Epoch: 6| Step: 3
Training loss: 2.4193955065272803
Validation loss: 2.452953259719079

Epoch: 6| Step: 4
Training loss: 2.1278310878085764
Validation loss: 2.4584478955056954

Epoch: 6| Step: 5
Training loss: 3.03252564950082
Validation loss: 2.459918494399157

Epoch: 6| Step: 6
Training loss: 2.6571915696772535
Validation loss: 2.4586749459107957

Epoch: 6| Step: 7
Training loss: 2.5820526722352635
Validation loss: 2.4506657740301114

Epoch: 6| Step: 8
Training loss: 2.5310073547859298
Validation loss: 2.4533309850042273

Epoch: 6| Step: 9
Training loss: 2.164165177646497
Validation loss: 2.4508444517995227

Epoch: 6| Step: 10
Training loss: 2.850945058367502
Validation loss: 2.4536178968395643

Epoch: 6| Step: 11
Training loss: 2.3244556979402753
Validation loss: 2.4499694063261694

Epoch: 6| Step: 12
Training loss: 2.4151377829347713
Validation loss: 2.455430099922676

Epoch: 6| Step: 13
Training loss: 2.7518266333376253
Validation loss: 2.458551184382033

Epoch: 122| Step: 0
Training loss: 2.7562263401252967
Validation loss: 2.4579874592805946

Epoch: 6| Step: 1
Training loss: 2.6527662881687815
Validation loss: 2.457849314826792

Epoch: 6| Step: 2
Training loss: 2.5849967236378197
Validation loss: 2.460535603963906

Epoch: 6| Step: 3
Training loss: 1.9168496528295416
Validation loss: 2.454152954874987

Epoch: 6| Step: 4
Training loss: 2.662551048398349
Validation loss: 2.453910993287883

Epoch: 6| Step: 5
Training loss: 3.1834256929377283
Validation loss: 2.4591223587375888

Epoch: 6| Step: 6
Training loss: 2.2037075706374396
Validation loss: 2.457129480028756

Epoch: 6| Step: 7
Training loss: 2.5185242052367256
Validation loss: 2.4585443233745767

Epoch: 6| Step: 8
Training loss: 2.970463469078583
Validation loss: 2.452889562756977

Epoch: 6| Step: 9
Training loss: 2.3181372313538025
Validation loss: 2.4452084325191437

Epoch: 6| Step: 10
Training loss: 2.125291243397642
Validation loss: 2.45212476225939

Epoch: 6| Step: 11
Training loss: 2.5409329611301756
Validation loss: 2.4503346182229477

Epoch: 6| Step: 12
Training loss: 2.1953507280924245
Validation loss: 2.443424735248362

Epoch: 6| Step: 13
Training loss: 2.3890519295976915
Validation loss: 2.449186187965738

Epoch: 123| Step: 0
Training loss: 2.470183909747133
Validation loss: 2.445973576083537

Epoch: 6| Step: 1
Training loss: 2.483077952248652
Validation loss: 2.4517097035081212

Epoch: 6| Step: 2
Training loss: 2.1980553051461653
Validation loss: 2.4438672654682816

Epoch: 6| Step: 3
Training loss: 2.4375232793234307
Validation loss: 2.4562691788899342

Epoch: 6| Step: 4
Training loss: 2.597941768701376
Validation loss: 2.451251519316352

Epoch: 6| Step: 5
Training loss: 2.539260058419994
Validation loss: 2.4492429724980522

Epoch: 6| Step: 6
Training loss: 1.722615074997748
Validation loss: 2.440010147698985

Epoch: 6| Step: 7
Training loss: 2.167254466017129
Validation loss: 2.442690840430074

Epoch: 6| Step: 8
Training loss: 2.6382390365408934
Validation loss: 2.448400414467373

Epoch: 6| Step: 9
Training loss: 2.699903210918428
Validation loss: 2.459685621797632

Epoch: 6| Step: 10
Training loss: 2.3090993259852692
Validation loss: 2.462679493683106

Epoch: 6| Step: 11
Training loss: 2.339988640325638
Validation loss: 2.4590854194773

Epoch: 6| Step: 12
Training loss: 3.458962593893167
Validation loss: 2.4572914361785183

Epoch: 6| Step: 13
Training loss: 2.884418319426444
Validation loss: 2.4583170831003116

Epoch: 124| Step: 0
Training loss: 1.9853382325932856
Validation loss: 2.4641431809373566

Epoch: 6| Step: 1
Training loss: 2.762621698244898
Validation loss: 2.458024948519099

Epoch: 6| Step: 2
Training loss: 2.410162482106762
Validation loss: 2.4609454280987952

Epoch: 6| Step: 3
Training loss: 2.2503743390111635
Validation loss: 2.4690442292207293

Epoch: 6| Step: 4
Training loss: 2.827604804487163
Validation loss: 2.4755576709585703

Epoch: 6| Step: 5
Training loss: 2.6156819582335347
Validation loss: 2.4659402366155367

Epoch: 6| Step: 6
Training loss: 2.5551143371320726
Validation loss: 2.478553653442336

Epoch: 6| Step: 7
Training loss: 2.6981558577111926
Validation loss: 2.474276798564278

Epoch: 6| Step: 8
Training loss: 2.6984950639850283
Validation loss: 2.4847864574016247

Epoch: 6| Step: 9
Training loss: 2.4281765572709144
Validation loss: 2.474535637138036

Epoch: 6| Step: 10
Training loss: 2.5029263535446487
Validation loss: 2.4733756145383285

Epoch: 6| Step: 11
Training loss: 2.3426713623096793
Validation loss: 2.470461513682483

Epoch: 6| Step: 12
Training loss: 2.233911426178255
Validation loss: 2.4631553545945324

Epoch: 6| Step: 13
Training loss: 3.0608819852334808
Validation loss: 2.468510531629597

Epoch: 125| Step: 0
Training loss: 2.471174956491017
Validation loss: 2.4575333398140105

Epoch: 6| Step: 1
Training loss: 2.6504498153969993
Validation loss: 2.4528085945970854

Epoch: 6| Step: 2
Training loss: 2.6221932437430575
Validation loss: 2.453173732071051

Epoch: 6| Step: 3
Training loss: 2.072797327743804
Validation loss: 2.4566472030845157

Epoch: 6| Step: 4
Training loss: 1.9600552705834766
Validation loss: 2.45048067801179

Epoch: 6| Step: 5
Training loss: 2.8939536235180485
Validation loss: 2.447133906321587

Epoch: 6| Step: 6
Training loss: 2.458962268442131
Validation loss: 2.445342204895622

Epoch: 6| Step: 7
Training loss: 2.6438583351826725
Validation loss: 2.442709596765079

Epoch: 6| Step: 8
Training loss: 2.410187113609112
Validation loss: 2.447391686385601

Epoch: 6| Step: 9
Training loss: 2.7291983294409623
Validation loss: 2.4525374905659505

Epoch: 6| Step: 10
Training loss: 2.7573278071436054
Validation loss: 2.455250024151592

Epoch: 6| Step: 11
Training loss: 2.4162005490187717
Validation loss: 2.454114936926764

Epoch: 6| Step: 12
Training loss: 2.5945006973227436
Validation loss: 2.457965667084428

Epoch: 6| Step: 13
Training loss: 2.685557439609902
Validation loss: 2.455994727513207

Epoch: 126| Step: 0
Training loss: 2.6413803374613973
Validation loss: 2.455718805339121

Epoch: 6| Step: 1
Training loss: 1.8186254583236658
Validation loss: 2.456621986027704

Epoch: 6| Step: 2
Training loss: 2.8421200283403167
Validation loss: 2.456453410066499

Epoch: 6| Step: 3
Training loss: 2.710448646992036
Validation loss: 2.449208577452833

Epoch: 6| Step: 4
Training loss: 2.0853439801099625
Validation loss: 2.4534015702090817

Epoch: 6| Step: 5
Training loss: 2.4687123356131027
Validation loss: 2.4494944878099774

Epoch: 6| Step: 6
Training loss: 2.123014532297342
Validation loss: 2.4540739390585387

Epoch: 6| Step: 7
Training loss: 2.5168643528514396
Validation loss: 2.459876494796394

Epoch: 6| Step: 8
Training loss: 2.6593897495241774
Validation loss: 2.45658203288049

Epoch: 6| Step: 9
Training loss: 2.3522646036455943
Validation loss: 2.459152947127636

Epoch: 6| Step: 10
Training loss: 2.4029567029578964
Validation loss: 2.458834263040188

Epoch: 6| Step: 11
Training loss: 2.831577112025243
Validation loss: 2.4550004662062026

Epoch: 6| Step: 12
Training loss: 2.7176873772250496
Validation loss: 2.457640216121966

Epoch: 6| Step: 13
Training loss: 3.064209285879223
Validation loss: 2.457380907724524

Epoch: 127| Step: 0
Training loss: 2.619911620855278
Validation loss: 2.454557029191629

Epoch: 6| Step: 1
Training loss: 2.316784678227516
Validation loss: 2.4546439212497764

Epoch: 6| Step: 2
Training loss: 2.546390791658416
Validation loss: 2.459208071519652

Epoch: 6| Step: 3
Training loss: 2.7162743181736677
Validation loss: 2.449894537569694

Epoch: 6| Step: 4
Training loss: 2.007078994177157
Validation loss: 2.4532762711651013

Epoch: 6| Step: 5
Training loss: 2.8660577112471444
Validation loss: 2.456429040286772

Epoch: 6| Step: 6
Training loss: 2.775919574583908
Validation loss: 2.4614330378161027

Epoch: 6| Step: 7
Training loss: 2.2382922213886323
Validation loss: 2.462511429219582

Epoch: 6| Step: 8
Training loss: 2.662814119103688
Validation loss: 2.465675177504761

Epoch: 6| Step: 9
Training loss: 2.379676932037793
Validation loss: 2.4635006751318995

Epoch: 6| Step: 10
Training loss: 2.4028013212291475
Validation loss: 2.471075620453081

Epoch: 6| Step: 11
Training loss: 2.4119700032483458
Validation loss: 2.468872839853287

Epoch: 6| Step: 12
Training loss: 2.4701566913649464
Validation loss: 2.467790376891638

Epoch: 6| Step: 13
Training loss: 2.9134427554475857
Validation loss: 2.4689883507378694

Epoch: 128| Step: 0
Training loss: 2.6708987945612193
Validation loss: 2.4742257117625273

Epoch: 6| Step: 1
Training loss: 2.4918972312714387
Validation loss: 2.468149007405335

Epoch: 6| Step: 2
Training loss: 2.457458652990615
Validation loss: 2.4736484507515573

Epoch: 6| Step: 3
Training loss: 2.29862058572907
Validation loss: 2.4698827937216348

Epoch: 6| Step: 4
Training loss: 2.224330064020089
Validation loss: 2.4692632246326007

Epoch: 6| Step: 5
Training loss: 2.405970470491421
Validation loss: 2.466363147370648

Epoch: 6| Step: 6
Training loss: 3.00290078746256
Validation loss: 2.463362952773952

Epoch: 6| Step: 7
Training loss: 2.2937954349851237
Validation loss: 2.4592442252755182

Epoch: 6| Step: 8
Training loss: 2.6614146568063886
Validation loss: 2.452438420395279

Epoch: 6| Step: 9
Training loss: 3.065100208280995
Validation loss: 2.457745430579523

Epoch: 6| Step: 10
Training loss: 2.5169917593492594
Validation loss: 2.4594832548804626

Epoch: 6| Step: 11
Training loss: 2.1483979932447177
Validation loss: 2.4562790471786893

Epoch: 6| Step: 12
Training loss: 2.2149280350406277
Validation loss: 2.4510285395119276

Epoch: 6| Step: 13
Training loss: 2.8627666099358966
Validation loss: 2.4526485620286507

Epoch: 129| Step: 0
Training loss: 2.778999360942733
Validation loss: 2.455999370985955

Epoch: 6| Step: 1
Training loss: 2.8151403960591246
Validation loss: 2.457948126489455

Epoch: 6| Step: 2
Training loss: 2.3828319111017757
Validation loss: 2.4576175154008415

Epoch: 6| Step: 3
Training loss: 3.302675180113375
Validation loss: 2.4585314579238218

Epoch: 6| Step: 4
Training loss: 2.199407484720372
Validation loss: 2.459611468840552

Epoch: 6| Step: 5
Training loss: 2.769015286999595
Validation loss: 2.4562621982650126

Epoch: 6| Step: 6
Training loss: 2.9869054482012216
Validation loss: 2.453430658885718

Epoch: 6| Step: 7
Training loss: 1.6186808449838037
Validation loss: 2.4616865521787648

Epoch: 6| Step: 8
Training loss: 2.876227904445854
Validation loss: 2.4587649246717014

Epoch: 6| Step: 9
Training loss: 2.3534097608127698
Validation loss: 2.456513205186004

Epoch: 6| Step: 10
Training loss: 1.972966598421354
Validation loss: 2.451764427996021

Epoch: 6| Step: 11
Training loss: 1.8894268684700095
Validation loss: 2.453507800515654

Epoch: 6| Step: 12
Training loss: 2.2558680414470973
Validation loss: 2.4459893668040573

Epoch: 6| Step: 13
Training loss: 2.5466901091883067
Validation loss: 2.4467787566568213

Epoch: 130| Step: 0
Training loss: 2.2465680757655826
Validation loss: 2.445231492200819

Epoch: 6| Step: 1
Training loss: 2.9329262696658263
Validation loss: 2.449732351929427

Epoch: 6| Step: 2
Training loss: 2.13952296492841
Validation loss: 2.4419948346497073

Epoch: 6| Step: 3
Training loss: 2.96477652210918
Validation loss: 2.447773670862733

Epoch: 6| Step: 4
Training loss: 2.022521293914697
Validation loss: 2.4464376702668167

Epoch: 6| Step: 5
Training loss: 2.595791025158837
Validation loss: 2.4448726043067923

Epoch: 6| Step: 6
Training loss: 2.6588108060453925
Validation loss: 2.448208963244555

Epoch: 6| Step: 7
Training loss: 2.11194312977806
Validation loss: 2.4450756441726123

Epoch: 6| Step: 8
Training loss: 2.4338805885331105
Validation loss: 2.4451994458471713

Epoch: 6| Step: 9
Training loss: 2.407646467560457
Validation loss: 2.4439268888827135

Epoch: 6| Step: 10
Training loss: 1.8727012530200176
Validation loss: 2.451797644589091

Epoch: 6| Step: 11
Training loss: 3.199381506276655
Validation loss: 2.4556994202080755

Epoch: 6| Step: 12
Training loss: 2.9472570501345605
Validation loss: 2.4499203428955827

Epoch: 6| Step: 13
Training loss: 2.323781922907555
Validation loss: 2.4502428784294987

Epoch: 131| Step: 0
Training loss: 2.384804415250656
Validation loss: 2.4525400019010464

Epoch: 6| Step: 1
Training loss: 2.800924512957966
Validation loss: 2.448051632887244

Epoch: 6| Step: 2
Training loss: 2.26134618348993
Validation loss: 2.4529933043682846

Epoch: 6| Step: 3
Training loss: 2.4186949494477155
Validation loss: 2.4526213515651656

Epoch: 6| Step: 4
Training loss: 2.1484707916454697
Validation loss: 2.448929765070333

Epoch: 6| Step: 5
Training loss: 2.74283750823634
Validation loss: 2.4501482970580986

Epoch: 6| Step: 6
Training loss: 2.751510205440738
Validation loss: 2.4462985649609603

Epoch: 6| Step: 7
Training loss: 2.3864775940078844
Validation loss: 2.4548538821802053

Epoch: 6| Step: 8
Training loss: 2.7281793647933736
Validation loss: 2.447013856237227

Epoch: 6| Step: 9
Training loss: 2.79408602475203
Validation loss: 2.449625195489812

Epoch: 6| Step: 10
Training loss: 2.172418101076261
Validation loss: 2.450166736819799

Epoch: 6| Step: 11
Training loss: 2.542723284943996
Validation loss: 2.440909861646369

Epoch: 6| Step: 12
Training loss: 2.612675024859093
Validation loss: 2.4478406907864927

Epoch: 6| Step: 13
Training loss: 2.3010293150041137
Validation loss: 2.4490508570055014

Epoch: 132| Step: 0
Training loss: 2.1317224197115645
Validation loss: 2.445862599532989

Epoch: 6| Step: 1
Training loss: 2.887847886039504
Validation loss: 2.452191590345068

Epoch: 6| Step: 2
Training loss: 2.3212338040988065
Validation loss: 2.452096565576036

Epoch: 6| Step: 3
Training loss: 2.5917483039910016
Validation loss: 2.4487041153762212

Epoch: 6| Step: 4
Training loss: 2.629137774357585
Validation loss: 2.4467319354135597

Epoch: 6| Step: 5
Training loss: 2.0797082133304166
Validation loss: 2.448676852966954

Epoch: 6| Step: 6
Training loss: 2.7824646943929428
Validation loss: 2.4472121719729385

Epoch: 6| Step: 7
Training loss: 2.9345020660741503
Validation loss: 2.4465665435270347

Epoch: 6| Step: 8
Training loss: 2.2633551441016273
Validation loss: 2.445832624765669

Epoch: 6| Step: 9
Training loss: 2.9391474872248513
Validation loss: 2.446560818316761

Epoch: 6| Step: 10
Training loss: 2.2887799085311906
Validation loss: 2.4488866358789156

Epoch: 6| Step: 11
Training loss: 2.159794202820064
Validation loss: 2.452527518109916

Epoch: 6| Step: 12
Training loss: 2.681567008389572
Validation loss: 2.451041420027211

Epoch: 6| Step: 13
Training loss: 2.342445519138457
Validation loss: 2.450354937709026

Epoch: 133| Step: 0
Training loss: 2.5350409483000673
Validation loss: 2.4542423141642553

Epoch: 6| Step: 1
Training loss: 2.5631043256527204
Validation loss: 2.460338475220131

Epoch: 6| Step: 2
Training loss: 1.5669204743051355
Validation loss: 2.4569615292019518

Epoch: 6| Step: 3
Training loss: 2.709458797087347
Validation loss: 2.4607777215668043

Epoch: 6| Step: 4
Training loss: 2.0101950198785032
Validation loss: 2.465453300734702

Epoch: 6| Step: 5
Training loss: 3.1081387156358975
Validation loss: 2.468675411079421

Epoch: 6| Step: 6
Training loss: 2.335273367585684
Validation loss: 2.471357602169649

Epoch: 6| Step: 7
Training loss: 2.634668437502854
Validation loss: 2.4728905525103464

Epoch: 6| Step: 8
Training loss: 2.8701652011444136
Validation loss: 2.4716547845806507

Epoch: 6| Step: 9
Training loss: 1.956926902684554
Validation loss: 2.472370406387649

Epoch: 6| Step: 10
Training loss: 2.6049839009027296
Validation loss: 2.464068936355883

Epoch: 6| Step: 11
Training loss: 2.181782580395878
Validation loss: 2.4569548335723383

Epoch: 6| Step: 12
Training loss: 2.72602213666832
Validation loss: 2.459955090006862

Epoch: 6| Step: 13
Training loss: 3.090424108195842
Validation loss: 2.449310284799783

Epoch: 134| Step: 0
Training loss: 2.221733344132279
Validation loss: 2.4516918102075493

Epoch: 6| Step: 1
Training loss: 2.24888911904186
Validation loss: 2.4535548973412378

Epoch: 6| Step: 2
Training loss: 1.9902917556052981
Validation loss: 2.447985974135557

Epoch: 6| Step: 3
Training loss: 2.592241698946461
Validation loss: 2.4489345355211496

Epoch: 6| Step: 4
Training loss: 2.546345380701731
Validation loss: 2.449456438114186

Epoch: 6| Step: 5
Training loss: 2.626567917273005
Validation loss: 2.4495581351697897

Epoch: 6| Step: 6
Training loss: 3.1424753991425787
Validation loss: 2.4496198748541613

Epoch: 6| Step: 7
Training loss: 2.6806459186544447
Validation loss: 2.447491796971395

Epoch: 6| Step: 8
Training loss: 2.4929776747737167
Validation loss: 2.449798888431492

Epoch: 6| Step: 9
Training loss: 2.390336362081529
Validation loss: 2.4532044432263285

Epoch: 6| Step: 10
Training loss: 2.5980849292373085
Validation loss: 2.4533007775440967

Epoch: 6| Step: 11
Training loss: 2.4813906420765766
Validation loss: 2.451921430983262

Epoch: 6| Step: 12
Training loss: 2.137838066984254
Validation loss: 2.4511946678368397

Epoch: 6| Step: 13
Training loss: 2.7344665512017357
Validation loss: 2.44604557574324

Epoch: 135| Step: 0
Training loss: 2.596942453621542
Validation loss: 2.451832141072159

Epoch: 6| Step: 1
Training loss: 2.7933334005836836
Validation loss: 2.4494948041447704

Epoch: 6| Step: 2
Training loss: 2.0155798145522708
Validation loss: 2.4477634354918325

Epoch: 6| Step: 3
Training loss: 2.6510544172678596
Validation loss: 2.455344481827761

Epoch: 6| Step: 4
Training loss: 2.3939072758971394
Validation loss: 2.4494484647178214

Epoch: 6| Step: 5
Training loss: 2.5832491727676477
Validation loss: 2.4570349211632534

Epoch: 6| Step: 6
Training loss: 2.9802340393842215
Validation loss: 2.451440009741891

Epoch: 6| Step: 7
Training loss: 2.331896702960139
Validation loss: 2.4479861039939115

Epoch: 6| Step: 8
Training loss: 2.378721233119808
Validation loss: 2.446319811355527

Epoch: 6| Step: 9
Training loss: 2.472991004689937
Validation loss: 2.4432320158830754

Epoch: 6| Step: 10
Training loss: 2.250906126033272
Validation loss: 2.447946927039147

Epoch: 6| Step: 11
Training loss: 2.588435876374436
Validation loss: 2.4435457422304494

Epoch: 6| Step: 12
Training loss: 2.3547178877785275
Validation loss: 2.4533241336933362

Epoch: 6| Step: 13
Training loss: 2.719010088526931
Validation loss: 2.4455066802872714

Epoch: 136| Step: 0
Training loss: 2.2933994908918502
Validation loss: 2.4528414568465755

Epoch: 6| Step: 1
Training loss: 2.4517380991300994
Validation loss: 2.4434364117645444

Epoch: 6| Step: 2
Training loss: 3.199148696672734
Validation loss: 2.4483158327216845

Epoch: 6| Step: 3
Training loss: 2.4555389772932434
Validation loss: 2.4510328438326208

Epoch: 6| Step: 4
Training loss: 2.448875293651643
Validation loss: 2.4471483093378925

Epoch: 6| Step: 5
Training loss: 2.9107909464123014
Validation loss: 2.445362289620573

Epoch: 6| Step: 6
Training loss: 2.4314641313960657
Validation loss: 2.4516028361003834

Epoch: 6| Step: 7
Training loss: 2.4412759730866194
Validation loss: 2.454264123230492

Epoch: 6| Step: 8
Training loss: 2.2708305802168183
Validation loss: 2.4428043441502925

Epoch: 6| Step: 9
Training loss: 2.4193151912722035
Validation loss: 2.441080772575232

Epoch: 6| Step: 10
Training loss: 2.4585152467135023
Validation loss: 2.44411885436417

Epoch: 6| Step: 11
Training loss: 2.467018196041945
Validation loss: 2.4489217656200464

Epoch: 6| Step: 12
Training loss: 2.4551319246914534
Validation loss: 2.4472921724698606

Epoch: 6| Step: 13
Training loss: 2.2699097204531213
Validation loss: 2.451028045041458

Epoch: 137| Step: 0
Training loss: 1.7461773129123752
Validation loss: 2.4487054216923783

Epoch: 6| Step: 1
Training loss: 2.245733879118974
Validation loss: 2.440080434275987

Epoch: 6| Step: 2
Training loss: 2.5414509499315945
Validation loss: 2.4383263369173265

Epoch: 6| Step: 3
Training loss: 2.3487758410515824
Validation loss: 2.437115157957615

Epoch: 6| Step: 4
Training loss: 2.688698878243421
Validation loss: 2.444822317114674

Epoch: 6| Step: 5
Training loss: 2.3593531727570647
Validation loss: 2.447448220307599

Epoch: 6| Step: 6
Training loss: 2.8954875565796545
Validation loss: 2.4488525116801956

Epoch: 6| Step: 7
Training loss: 2.3292419007444973
Validation loss: 2.4521597807923285

Epoch: 6| Step: 8
Training loss: 2.307071603537718
Validation loss: 2.448753430356162

Epoch: 6| Step: 9
Training loss: 2.580682768855327
Validation loss: 2.458114215546866

Epoch: 6| Step: 10
Training loss: 2.9164698579781545
Validation loss: 2.4509581534741844

Epoch: 6| Step: 11
Training loss: 2.8222593736414328
Validation loss: 2.448285311813762

Epoch: 6| Step: 12
Training loss: 2.861558757769954
Validation loss: 2.4450327720344003

Epoch: 6| Step: 13
Training loss: 2.218258440610196
Validation loss: 2.441960793140086

Epoch: 138| Step: 0
Training loss: 2.120023735021484
Validation loss: 2.443854550387795

Epoch: 6| Step: 1
Training loss: 2.8762635481281986
Validation loss: 2.4474922678029447

Epoch: 6| Step: 2
Training loss: 3.398700011188432
Validation loss: 2.4503799435811304

Epoch: 6| Step: 3
Training loss: 1.7249612416875044
Validation loss: 2.4598008856271205

Epoch: 6| Step: 4
Training loss: 2.2651991871539487
Validation loss: 2.4494784520157853

Epoch: 6| Step: 5
Training loss: 2.155620095946147
Validation loss: 2.4597355243484245

Epoch: 6| Step: 6
Training loss: 2.764358489133657
Validation loss: 2.4621785097635485

Epoch: 6| Step: 7
Training loss: 2.750290161777209
Validation loss: 2.464257010759369

Epoch: 6| Step: 8
Training loss: 2.5644925442038904
Validation loss: 2.4571058366105234

Epoch: 6| Step: 9
Training loss: 2.729395927137577
Validation loss: 2.45593646493652

Epoch: 6| Step: 10
Training loss: 2.6203898864856057
Validation loss: 2.449210921847434

Epoch: 6| Step: 11
Training loss: 2.1263772204069475
Validation loss: 2.4547935043682476

Epoch: 6| Step: 12
Training loss: 2.2672076781076247
Validation loss: 2.4583318731874773

Epoch: 6| Step: 13
Training loss: 2.1655013055967007
Validation loss: 2.4468041482397487

Epoch: 139| Step: 0
Training loss: 2.7243230416300213
Validation loss: 2.4482730578989624

Epoch: 6| Step: 1
Training loss: 2.5340203089935174
Validation loss: 2.450050419658422

Epoch: 6| Step: 2
Training loss: 2.037818499987573
Validation loss: 2.448529062707415

Epoch: 6| Step: 3
Training loss: 2.407876790034392
Validation loss: 2.444917153279087

Epoch: 6| Step: 4
Training loss: 2.6481430205073457
Validation loss: 2.443912564437309

Epoch: 6| Step: 5
Training loss: 2.349973934110188
Validation loss: 2.443744770314376

Epoch: 6| Step: 6
Training loss: 3.107881887206961
Validation loss: 2.447321788389983

Epoch: 6| Step: 7
Training loss: 2.375646804271936
Validation loss: 2.4426168710683878

Epoch: 6| Step: 8
Training loss: 2.5685283184692205
Validation loss: 2.445213990263581

Epoch: 6| Step: 9
Training loss: 2.650788650479879
Validation loss: 2.4533353905651896

Epoch: 6| Step: 10
Training loss: 2.0423970635201383
Validation loss: 2.447914502298467

Epoch: 6| Step: 11
Training loss: 2.6473714207593018
Validation loss: 2.4372245600344993

Epoch: 6| Step: 12
Training loss: 2.2183246473471443
Validation loss: 2.441096513555296

Epoch: 6| Step: 13
Training loss: 2.5479282939288965
Validation loss: 2.434133536132823

Epoch: 140| Step: 0
Training loss: 2.363891724483803
Validation loss: 2.443515186134349

Epoch: 6| Step: 1
Training loss: 2.7061414701270796
Validation loss: 2.4394404520683466

Epoch: 6| Step: 2
Training loss: 1.9938109720407862
Validation loss: 2.4483056320776937

Epoch: 6| Step: 3
Training loss: 2.089621501762911
Validation loss: 2.4492320861940047

Epoch: 6| Step: 4
Training loss: 2.0172560841951652
Validation loss: 2.4506274830835513

Epoch: 6| Step: 5
Training loss: 2.3948717481960524
Validation loss: 2.4459175279499754

Epoch: 6| Step: 6
Training loss: 2.9388794602176955
Validation loss: 2.4432586722353253

Epoch: 6| Step: 7
Training loss: 2.5658316934383545
Validation loss: 2.4533396827401357

Epoch: 6| Step: 8
Training loss: 2.2281845876636117
Validation loss: 2.4628766447956667

Epoch: 6| Step: 9
Training loss: 2.5056947697179117
Validation loss: 2.459320715984153

Epoch: 6| Step: 10
Training loss: 3.2300884940954018
Validation loss: 2.4523673537763964

Epoch: 6| Step: 11
Training loss: 2.904481082864506
Validation loss: 2.4505312220440714

Epoch: 6| Step: 12
Training loss: 2.556648458377037
Validation loss: 2.4517749059794323

Epoch: 6| Step: 13
Training loss: 2.2237323107131113
Validation loss: 2.440415082004739

Epoch: 141| Step: 0
Training loss: 2.31574804722999
Validation loss: 2.4432629658487612

Epoch: 6| Step: 1
Training loss: 2.400762965203769
Validation loss: 2.440004903809253

Epoch: 6| Step: 2
Training loss: 2.495196687211959
Validation loss: 2.4440202883868394

Epoch: 6| Step: 3
Training loss: 2.5271390333795964
Validation loss: 2.43610553304637

Epoch: 6| Step: 4
Training loss: 2.479081181780737
Validation loss: 2.4407883983820207

Epoch: 6| Step: 5
Training loss: 2.5871977256354417
Validation loss: 2.4419877888121393

Epoch: 6| Step: 6
Training loss: 2.1609302534421833
Validation loss: 2.4392349393999417

Epoch: 6| Step: 7
Training loss: 3.182050047513837
Validation loss: 2.444970591471605

Epoch: 6| Step: 8
Training loss: 2.7456023860549785
Validation loss: 2.4402529502517067

Epoch: 6| Step: 9
Training loss: 2.226107045488247
Validation loss: 2.440261743452205

Epoch: 6| Step: 10
Training loss: 2.5600746586463092
Validation loss: 2.448355555588239

Epoch: 6| Step: 11
Training loss: 2.393729195367428
Validation loss: 2.44425083306388

Epoch: 6| Step: 12
Training loss: 2.0426681034757044
Validation loss: 2.44980414379736

Epoch: 6| Step: 13
Training loss: 2.647871378929775
Validation loss: 2.440805329691485

Epoch: 142| Step: 0
Training loss: 2.572455999444809
Validation loss: 2.4455781818720874

Epoch: 6| Step: 1
Training loss: 1.737881252257542
Validation loss: 2.4396117265473145

Epoch: 6| Step: 2
Training loss: 1.9315472494454309
Validation loss: 2.4438693141835017

Epoch: 6| Step: 3
Training loss: 2.56207327662432
Validation loss: 2.444171936084969

Epoch: 6| Step: 4
Training loss: 2.501559534017507
Validation loss: 2.4457335998199494

Epoch: 6| Step: 5
Training loss: 2.339770180591641
Validation loss: 2.4430969081046587

Epoch: 6| Step: 6
Training loss: 1.9865267043873995
Validation loss: 2.4444515918737975

Epoch: 6| Step: 7
Training loss: 2.6749884774503587
Validation loss: 2.435361152018381

Epoch: 6| Step: 8
Training loss: 3.0678722983270954
Validation loss: 2.4419133750647646

Epoch: 6| Step: 9
Training loss: 2.876409102395002
Validation loss: 2.439277978600382

Epoch: 6| Step: 10
Training loss: 2.8084144588114017
Validation loss: 2.445352507275144

Epoch: 6| Step: 11
Training loss: 2.652220777939054
Validation loss: 2.4393821687663886

Epoch: 6| Step: 12
Training loss: 2.5236408163545585
Validation loss: 2.4353870787072758

Epoch: 6| Step: 13
Training loss: 2.3012297991312
Validation loss: 2.440668085021616

Epoch: 143| Step: 0
Training loss: 2.538208051843055
Validation loss: 2.437057862570723

Epoch: 6| Step: 1
Training loss: 2.638237771355722
Validation loss: 2.4422032553241255

Epoch: 6| Step: 2
Training loss: 2.759948249276439
Validation loss: 2.4365905630157365

Epoch: 6| Step: 3
Training loss: 2.30195284742622
Validation loss: 2.437710549153835

Epoch: 6| Step: 4
Training loss: 2.136948595813072
Validation loss: 2.444141395946314

Epoch: 6| Step: 5
Training loss: 2.1331312898486687
Validation loss: 2.4494789305766664

Epoch: 6| Step: 6
Training loss: 2.6626915410592287
Validation loss: 2.4522937573095693

Epoch: 6| Step: 7
Training loss: 2.2150622601509906
Validation loss: 2.4539070907473124

Epoch: 6| Step: 8
Training loss: 2.9366544764681044
Validation loss: 2.447270918279973

Epoch: 6| Step: 9
Training loss: 2.612306330314904
Validation loss: 2.4529353187914142

Epoch: 6| Step: 10
Training loss: 2.4447518092838343
Validation loss: 2.4518400418896804

Epoch: 6| Step: 11
Training loss: 2.3897967929722475
Validation loss: 2.451992251234065

Epoch: 6| Step: 12
Training loss: 2.0564111942080814
Validation loss: 2.451546608699321

Epoch: 6| Step: 13
Training loss: 2.7357997996885355
Validation loss: 2.4470864098666656

Epoch: 144| Step: 0
Training loss: 1.9774082470235448
Validation loss: 2.4417928648961356

Epoch: 6| Step: 1
Training loss: 1.9871980905648057
Validation loss: 2.4599575776161386

Epoch: 6| Step: 2
Training loss: 2.9677976988252324
Validation loss: 2.4545890262243972

Epoch: 6| Step: 3
Training loss: 2.557528441449049
Validation loss: 2.450554312714783

Epoch: 6| Step: 4
Training loss: 2.7984134129904543
Validation loss: 2.4469970977967614

Epoch: 6| Step: 5
Training loss: 2.3527160045926925
Validation loss: 2.4554617538295838

Epoch: 6| Step: 6
Training loss: 2.2646989705713936
Validation loss: 2.441980987037419

Epoch: 6| Step: 7
Training loss: 1.5725265002178275
Validation loss: 2.4430192100226624

Epoch: 6| Step: 8
Training loss: 2.418974979790064
Validation loss: 2.448049887961739

Epoch: 6| Step: 9
Training loss: 2.5944548419015234
Validation loss: 2.441000812689263

Epoch: 6| Step: 10
Training loss: 2.207595078007374
Validation loss: 2.435261717588744

Epoch: 6| Step: 11
Training loss: 3.060966574904186
Validation loss: 2.4394660422006305

Epoch: 6| Step: 12
Training loss: 2.917908449584018
Validation loss: 2.4486559191252515

Epoch: 6| Step: 13
Training loss: 2.734900375030201
Validation loss: 2.440732638191204

Epoch: 145| Step: 0
Training loss: 2.0959963917737583
Validation loss: 2.441734629608476

Epoch: 6| Step: 1
Training loss: 3.2610540934886654
Validation loss: 2.4453569597096436

Epoch: 6| Step: 2
Training loss: 2.146859439088268
Validation loss: 2.4374350677709984

Epoch: 6| Step: 3
Training loss: 2.5590050286694184
Validation loss: 2.449548986014288

Epoch: 6| Step: 4
Training loss: 2.1215999554227016
Validation loss: 2.442961565134617

Epoch: 6| Step: 5
Training loss: 2.383820367503631
Validation loss: 2.4463601270189796

Epoch: 6| Step: 6
Training loss: 2.317050065989249
Validation loss: 2.452653260440725

Epoch: 6| Step: 7
Training loss: 2.5776190983870424
Validation loss: 2.4462792676169927

Epoch: 6| Step: 8
Training loss: 2.350160301097977
Validation loss: 2.4527889272576338

Epoch: 6| Step: 9
Training loss: 2.1231315476589177
Validation loss: 2.4416267478552665

Epoch: 6| Step: 10
Training loss: 2.522610649956107
Validation loss: 2.441343846858728

Epoch: 6| Step: 11
Training loss: 2.682585281054568
Validation loss: 2.44207679593623

Epoch: 6| Step: 12
Training loss: 2.660393837333198
Validation loss: 2.4560518642684857

Epoch: 6| Step: 13
Training loss: 2.736350953185915
Validation loss: 2.436445350389491

Epoch: 146| Step: 0
Training loss: 2.1993088373464125
Validation loss: 2.4476275388265725

Epoch: 6| Step: 1
Training loss: 2.6193903065122504
Validation loss: 2.455114266739784

Epoch: 6| Step: 2
Training loss: 2.307983937543117
Validation loss: 2.4564858191533983

Epoch: 6| Step: 3
Training loss: 2.359124783011973
Validation loss: 2.451540165725092

Epoch: 6| Step: 4
Training loss: 2.60426900026799
Validation loss: 2.4491019497429214

Epoch: 6| Step: 5
Training loss: 2.4254989582626507
Validation loss: 2.455303771524709

Epoch: 6| Step: 6
Training loss: 2.8484853366971903
Validation loss: 2.456565016201044

Epoch: 6| Step: 7
Training loss: 2.276962117432438
Validation loss: 2.46548255344547

Epoch: 6| Step: 8
Training loss: 2.282159075475758
Validation loss: 2.468987771346054

Epoch: 6| Step: 9
Training loss: 2.803668929010873
Validation loss: 2.4674568491482995

Epoch: 6| Step: 10
Training loss: 2.530853428799061
Validation loss: 2.4729173310516503

Epoch: 6| Step: 11
Training loss: 2.1868754994102098
Validation loss: 2.464596501689008

Epoch: 6| Step: 12
Training loss: 2.863724698937758
Validation loss: 2.4542289566226425

Epoch: 6| Step: 13
Training loss: 2.5953380824601764
Validation loss: 2.4524642475618794

Epoch: 147| Step: 0
Training loss: 1.972285894998828
Validation loss: 2.4454846143834557

Epoch: 6| Step: 1
Training loss: 1.662168974653938
Validation loss: 2.448892785648277

Epoch: 6| Step: 2
Training loss: 2.443149084956639
Validation loss: 2.45169254766047

Epoch: 6| Step: 3
Training loss: 2.82121038541755
Validation loss: 2.4507113202630584

Epoch: 6| Step: 4
Training loss: 2.5155681343048686
Validation loss: 2.450488072391528

Epoch: 6| Step: 5
Training loss: 2.754133066346569
Validation loss: 2.451424189293288

Epoch: 6| Step: 6
Training loss: 2.967603763350566
Validation loss: 2.450885552329167

Epoch: 6| Step: 7
Training loss: 2.6686404989679544
Validation loss: 2.445598776475498

Epoch: 6| Step: 8
Training loss: 2.509683166811763
Validation loss: 2.4516585597646126

Epoch: 6| Step: 9
Training loss: 2.615665915840281
Validation loss: 2.4473720242244044

Epoch: 6| Step: 10
Training loss: 2.401787124284927
Validation loss: 2.4542028242037186

Epoch: 6| Step: 11
Training loss: 2.4079674870776895
Validation loss: 2.443958951922734

Epoch: 6| Step: 12
Training loss: 2.107057329524943
Validation loss: 2.4531280776731457

Epoch: 6| Step: 13
Training loss: 2.958472521735707
Validation loss: 2.439323753752364

Epoch: 148| Step: 0
Training loss: 2.737942096423948
Validation loss: 2.4422729093260918

Epoch: 6| Step: 1
Training loss: 2.161148147080424
Validation loss: 2.4504223491946813

Epoch: 6| Step: 2
Training loss: 2.2335534352643442
Validation loss: 2.4531875084042363

Epoch: 6| Step: 3
Training loss: 2.728524712538553
Validation loss: 2.4440597802882604

Epoch: 6| Step: 4
Training loss: 3.028789504829114
Validation loss: 2.44615839585741

Epoch: 6| Step: 5
Training loss: 2.4475555369979767
Validation loss: 2.450217335980718

Epoch: 6| Step: 6
Training loss: 2.346945656679181
Validation loss: 2.4624639227688414

Epoch: 6| Step: 7
Training loss: 2.2886625077146587
Validation loss: 2.4670660174633556

Epoch: 6| Step: 8
Training loss: 2.0748529129291375
Validation loss: 2.4675624743652653

Epoch: 6| Step: 9
Training loss: 2.38793295085331
Validation loss: 2.4638647844764257

Epoch: 6| Step: 10
Training loss: 2.5060316756515464
Validation loss: 2.451958356625543

Epoch: 6| Step: 11
Training loss: 2.3029446493728307
Validation loss: 2.4629814096932248

Epoch: 6| Step: 12
Training loss: 2.8686998416945406
Validation loss: 2.4554682836108364

Epoch: 6| Step: 13
Training loss: 2.6483716028168036
Validation loss: 2.445547399319032

Epoch: 149| Step: 0
Training loss: 2.5222455212230694
Validation loss: 2.4518547251875424

Epoch: 6| Step: 1
Training loss: 2.6768853575732994
Validation loss: 2.4417317409979016

Epoch: 6| Step: 2
Training loss: 2.1663312896955227
Validation loss: 2.431007100501728

Epoch: 6| Step: 3
Training loss: 2.1825387552322684
Validation loss: 2.447183740072488

Epoch: 6| Step: 4
Training loss: 2.068305774026613
Validation loss: 2.440782236333031

Epoch: 6| Step: 5
Training loss: 2.842114995080894
Validation loss: 2.4416935540586078

Epoch: 6| Step: 6
Training loss: 2.312054771658941
Validation loss: 2.442129922042864

Epoch: 6| Step: 7
Training loss: 2.0320233560105945
Validation loss: 2.4389630678849477

Epoch: 6| Step: 8
Training loss: 2.8006232521683345
Validation loss: 2.4371301419302243

Epoch: 6| Step: 9
Training loss: 2.678809909195136
Validation loss: 2.4498868007848342

Epoch: 6| Step: 10
Training loss: 2.860615445588425
Validation loss: 2.446259661489571

Epoch: 6| Step: 11
Training loss: 2.4711530554689376
Validation loss: 2.443826941200084

Epoch: 6| Step: 12
Training loss: 2.53883487561416
Validation loss: 2.454646568029915

Epoch: 6| Step: 13
Training loss: 2.48005108057761
Validation loss: 2.4544899255345456

Epoch: 150| Step: 0
Training loss: 2.5481806491728745
Validation loss: 2.4565415452834114

Epoch: 6| Step: 1
Training loss: 2.45063402574585
Validation loss: 2.4548431988149106

Epoch: 6| Step: 2
Training loss: 2.3162895289021317
Validation loss: 2.455935024938644

Epoch: 6| Step: 3
Training loss: 2.3202892777536372
Validation loss: 2.4576240152013904

Epoch: 6| Step: 4
Training loss: 2.711812782250833
Validation loss: 2.460125485427044

Epoch: 6| Step: 5
Training loss: 2.8973699284767567
Validation loss: 2.4529704148490823

Epoch: 6| Step: 6
Training loss: 2.8216064528116496
Validation loss: 2.4569819879571595

Epoch: 6| Step: 7
Training loss: 2.6057767442935464
Validation loss: 2.465148487790105

Epoch: 6| Step: 8
Training loss: 2.136116235785034
Validation loss: 2.4594585354427205

Epoch: 6| Step: 9
Training loss: 2.8234961127323164
Validation loss: 2.460416789193491

Epoch: 6| Step: 10
Training loss: 2.4514284523955325
Validation loss: 2.4537779228680288

Epoch: 6| Step: 11
Training loss: 2.4576166907980506
Validation loss: 2.4557128830198267

Epoch: 6| Step: 12
Training loss: 2.153721307971881
Validation loss: 2.4505312869058105

Epoch: 6| Step: 13
Training loss: 1.7784082599408604
Validation loss: 2.4614760198185075

Epoch: 151| Step: 0
Training loss: 2.8265611740286327
Validation loss: 2.4496372642058897

Epoch: 6| Step: 1
Training loss: 2.5726241178385485
Validation loss: 2.44768078797094

Epoch: 6| Step: 2
Training loss: 2.5697341778503104
Validation loss: 2.4574222546707887

Epoch: 6| Step: 3
Training loss: 2.150002537215754
Validation loss: 2.45219590072648

Epoch: 6| Step: 4
Training loss: 2.593661754899456
Validation loss: 2.4572327111257253

Epoch: 6| Step: 5
Training loss: 2.124712531897123
Validation loss: 2.4504563866686735

Epoch: 6| Step: 6
Training loss: 2.6869635822879148
Validation loss: 2.456415953461607

Epoch: 6| Step: 7
Training loss: 2.7584275378441365
Validation loss: 2.4521254671723196

Epoch: 6| Step: 8
Training loss: 2.549290827703392
Validation loss: 2.455330102614757

Epoch: 6| Step: 9
Training loss: 2.0520067023154813
Validation loss: 2.4510589289891587

Epoch: 6| Step: 10
Training loss: 2.4089166306653484
Validation loss: 2.457438448836563

Epoch: 6| Step: 11
Training loss: 2.540501958253553
Validation loss: 2.4605578578799343

Epoch: 6| Step: 12
Training loss: 2.4150995786051674
Validation loss: 2.4469233722866623

Epoch: 6| Step: 13
Training loss: 2.315383968601775
Validation loss: 2.4526264064642147

Epoch: 152| Step: 0
Training loss: 2.1738614665008233
Validation loss: 2.434812776233536

Epoch: 6| Step: 1
Training loss: 2.632485389197976
Validation loss: 2.452295102222985

Epoch: 6| Step: 2
Training loss: 2.3105122038818817
Validation loss: 2.4479641253693356

Epoch: 6| Step: 3
Training loss: 2.5412173473326276
Validation loss: 2.4585471841503312

Epoch: 6| Step: 4
Training loss: 3.0862619736455787
Validation loss: 2.4480943304209792

Epoch: 6| Step: 5
Training loss: 2.4570215464583827
Validation loss: 2.453546589060445

Epoch: 6| Step: 6
Training loss: 2.0621969983026234
Validation loss: 2.4591880594803373

Epoch: 6| Step: 7
Training loss: 2.5070622828278575
Validation loss: 2.4673417819236745

Epoch: 6| Step: 8
Training loss: 2.694670050528149
Validation loss: 2.463029099760638

Epoch: 6| Step: 9
Training loss: 2.5124962346442556
Validation loss: 2.450870522747969

Epoch: 6| Step: 10
Training loss: 2.567872532874787
Validation loss: 2.447247959001148

Epoch: 6| Step: 11
Training loss: 1.9489205233564204
Validation loss: 2.446352037950423

Epoch: 6| Step: 12
Training loss: 2.673521282013599
Validation loss: 2.451268037933583

Epoch: 6| Step: 13
Training loss: 2.2363375068034648
Validation loss: 2.4459898054337277

Epoch: 153| Step: 0
Training loss: 2.391237666591299
Validation loss: 2.4460149858940485

Epoch: 6| Step: 1
Training loss: 2.56561081170307
Validation loss: 2.448446554635027

Epoch: 6| Step: 2
Training loss: 2.671997067524106
Validation loss: 2.448541753513535

Epoch: 6| Step: 3
Training loss: 1.8783004958027774
Validation loss: 2.449909556918767

Epoch: 6| Step: 4
Training loss: 2.543455672048615
Validation loss: 2.4470884396485526

Epoch: 6| Step: 5
Training loss: 2.1288996425141153
Validation loss: 2.4456962552451658

Epoch: 6| Step: 6
Training loss: 2.439479904088905
Validation loss: 2.4517870451493886

Epoch: 6| Step: 7
Training loss: 2.0663629826593235
Validation loss: 2.4478168927143575

Epoch: 6| Step: 8
Training loss: 3.0104534494513406
Validation loss: 2.4491962875966693

Epoch: 6| Step: 9
Training loss: 2.2545310068576496
Validation loss: 2.4558776023127256

Epoch: 6| Step: 10
Training loss: 2.5995773668847937
Validation loss: 2.451520731343023

Epoch: 6| Step: 11
Training loss: 2.584942490843701
Validation loss: 2.452300465665535

Epoch: 6| Step: 12
Training loss: 2.5433742123772882
Validation loss: 2.446642529197907

Epoch: 6| Step: 13
Training loss: 2.7054332954447564
Validation loss: 2.4471406450601743

Epoch: 154| Step: 0
Training loss: 2.6383446773623347
Validation loss: 2.4525702916401793

Epoch: 6| Step: 1
Training loss: 2.0991251258429813
Validation loss: 2.458723220511284

Epoch: 6| Step: 2
Training loss: 1.946082028078407
Validation loss: 2.4610843675173117

Epoch: 6| Step: 3
Training loss: 2.850145463410185
Validation loss: 2.4627190010036175

Epoch: 6| Step: 4
Training loss: 2.6197520885675805
Validation loss: 2.4560660774669625

Epoch: 6| Step: 5
Training loss: 2.076609352794543
Validation loss: 2.460460249223844

Epoch: 6| Step: 6
Training loss: 2.00677736665593
Validation loss: 2.4508998198074887

Epoch: 6| Step: 7
Training loss: 2.4380221663446062
Validation loss: 2.457263007703679

Epoch: 6| Step: 8
Training loss: 2.7119458876306
Validation loss: 2.454826364326759

Epoch: 6| Step: 9
Training loss: 2.5196671323581104
Validation loss: 2.453197210913453

Epoch: 6| Step: 10
Training loss: 2.508614385158239
Validation loss: 2.4562940517548832

Epoch: 6| Step: 11
Training loss: 2.781827288089701
Validation loss: 2.444391241602437

Epoch: 6| Step: 12
Training loss: 2.4950774366941193
Validation loss: 2.442096207845175

Epoch: 6| Step: 13
Training loss: 2.6503326926931283
Validation loss: 2.4468064299776615

Epoch: 155| Step: 0
Training loss: 2.354952678164477
Validation loss: 2.445084160004436

Epoch: 6| Step: 1
Training loss: 2.1665674578132506
Validation loss: 2.4486761876305523

Epoch: 6| Step: 2
Training loss: 2.1918818591782556
Validation loss: 2.4494516119007685

Epoch: 6| Step: 3
Training loss: 2.4108134012181788
Validation loss: 2.4541189605747906

Epoch: 6| Step: 4
Training loss: 2.72252804688298
Validation loss: 2.455459949438394

Epoch: 6| Step: 5
Training loss: 2.0597991599165963
Validation loss: 2.463760484543776

Epoch: 6| Step: 6
Training loss: 2.464037393019927
Validation loss: 2.455550968396988

Epoch: 6| Step: 7
Training loss: 3.0295328211929076
Validation loss: 2.4646620887519157

Epoch: 6| Step: 8
Training loss: 2.6159942184057714
Validation loss: 2.4541126134087015

Epoch: 6| Step: 9
Training loss: 2.6684988006366095
Validation loss: 2.451445034658095

Epoch: 6| Step: 10
Training loss: 2.0937628674467597
Validation loss: 2.448103087340717

Epoch: 6| Step: 11
Training loss: 2.756257653604205
Validation loss: 2.445891209252816

Epoch: 6| Step: 12
Training loss: 2.6310400727314756
Validation loss: 2.4606316431905575

Epoch: 6| Step: 13
Training loss: 2.1539710344015806
Validation loss: 2.4558883458985146

Epoch: 156| Step: 0
Training loss: 1.7768541940389424
Validation loss: 2.466800209459964

Epoch: 6| Step: 1
Training loss: 3.1743638369999934
Validation loss: 2.4590037180751074

Epoch: 6| Step: 2
Training loss: 1.9226730674556862
Validation loss: 2.4592399434059224

Epoch: 6| Step: 3
Training loss: 2.480876546664267
Validation loss: 2.4649118695163783

Epoch: 6| Step: 4
Training loss: 2.889015203143476
Validation loss: 2.4607534511306457

Epoch: 6| Step: 5
Training loss: 2.562478228220677
Validation loss: 2.464444425854436

Epoch: 6| Step: 6
Training loss: 2.100025394831107
Validation loss: 2.464208651062336

Epoch: 6| Step: 7
Training loss: 2.4146275522584197
Validation loss: 2.4600145819472177

Epoch: 6| Step: 8
Training loss: 2.797905502997704
Validation loss: 2.469499940883906

Epoch: 6| Step: 9
Training loss: 2.3178655906790318
Validation loss: 2.4586952287927795

Epoch: 6| Step: 10
Training loss: 2.7111898079465284
Validation loss: 2.454221670660212

Epoch: 6| Step: 11
Training loss: 2.7392869699595703
Validation loss: 2.4581406943496487

Epoch: 6| Step: 12
Training loss: 2.3935775973633295
Validation loss: 2.4559402995334882

Epoch: 6| Step: 13
Training loss: 2.0851585276688804
Validation loss: 2.4606644736319567

Epoch: 157| Step: 0
Training loss: 2.160018281859278
Validation loss: 2.4534604598448815

Epoch: 6| Step: 1
Training loss: 2.0114998411310805
Validation loss: 2.4531401939488497

Epoch: 6| Step: 2
Training loss: 2.8918823806671807
Validation loss: 2.4532739063594646

Epoch: 6| Step: 3
Training loss: 2.6048469773815373
Validation loss: 2.45171278295791

Epoch: 6| Step: 4
Training loss: 2.680646096535879
Validation loss: 2.4593188659517846

Epoch: 6| Step: 5
Training loss: 2.602269531950706
Validation loss: 2.4630755466754928

Epoch: 6| Step: 6
Training loss: 2.285383268637957
Validation loss: 2.447979002355039

Epoch: 6| Step: 7
Training loss: 2.999166849794742
Validation loss: 2.4561545905946907

Epoch: 6| Step: 8
Training loss: 2.067311656449941
Validation loss: 2.448408675292912

Epoch: 6| Step: 9
Training loss: 2.452274636773997
Validation loss: 2.4670082015860033

Epoch: 6| Step: 10
Training loss: 2.550840508559848
Validation loss: 2.452106337221979

Epoch: 6| Step: 11
Training loss: 2.63081116267535
Validation loss: 2.462591804045331

Epoch: 6| Step: 12
Training loss: 2.3149168427199487
Validation loss: 2.46701466859153

Epoch: 6| Step: 13
Training loss: 2.1688679248069365
Validation loss: 2.4830342959737983

Epoch: 158| Step: 0
Training loss: 2.4795972840135656
Validation loss: 2.471228679101276

Epoch: 6| Step: 1
Training loss: 2.227952275797487
Validation loss: 2.482615865735506

Epoch: 6| Step: 2
Training loss: 2.851664000494502
Validation loss: 2.477683374243966

Epoch: 6| Step: 3
Training loss: 2.0970493340826333
Validation loss: 2.4759858560163286

Epoch: 6| Step: 4
Training loss: 2.7563653450356904
Validation loss: 2.4732267295179406

Epoch: 6| Step: 5
Training loss: 2.992210925934327
Validation loss: 2.4715058762855238

Epoch: 6| Step: 6
Training loss: 2.1031694472719
Validation loss: 2.4716118911655083

Epoch: 6| Step: 7
Training loss: 2.4716123574022832
Validation loss: 2.461041830767857

Epoch: 6| Step: 8
Training loss: 2.0661574792657933
Validation loss: 2.4628859541846446

Epoch: 6| Step: 9
Training loss: 2.8574597557618096
Validation loss: 2.455748214459028

Epoch: 6| Step: 10
Training loss: 2.522694386760739
Validation loss: 2.458977749517401

Epoch: 6| Step: 11
Training loss: 2.576011947746291
Validation loss: 2.462103471603147

Epoch: 6| Step: 12
Training loss: 2.408537532856853
Validation loss: 2.441710316351399

Epoch: 6| Step: 13
Training loss: 2.2639031543035206
Validation loss: 2.440669778241311

Epoch: 159| Step: 0
Training loss: 1.9171423598169661
Validation loss: 2.456716301993296

Epoch: 6| Step: 1
Training loss: 2.2531745555999274
Validation loss: 2.450671692331797

Epoch: 6| Step: 2
Training loss: 1.794535963810404
Validation loss: 2.4615718528560513

Epoch: 6| Step: 3
Training loss: 3.219142112180748
Validation loss: 2.4697120095906864

Epoch: 6| Step: 4
Training loss: 2.5353860853780654
Validation loss: 2.4668183958796654

Epoch: 6| Step: 5
Training loss: 2.5045311872475273
Validation loss: 2.4804590420270163

Epoch: 6| Step: 6
Training loss: 2.602532741402971
Validation loss: 2.4813595351248954

Epoch: 6| Step: 7
Training loss: 2.870635326922558
Validation loss: 2.494871641296096

Epoch: 6| Step: 8
Training loss: 2.3267897290618724
Validation loss: 2.4777731518590573

Epoch: 6| Step: 9
Training loss: 2.017505921422501
Validation loss: 2.4551013023882753

Epoch: 6| Step: 10
Training loss: 2.126260944318105
Validation loss: 2.464771332158074

Epoch: 6| Step: 11
Training loss: 3.2103752145106554
Validation loss: 2.449568801030449

Epoch: 6| Step: 12
Training loss: 2.425185961327808
Validation loss: 2.4618554315744032

Epoch: 6| Step: 13
Training loss: 2.1279966878650103
Validation loss: 2.4518401229235747

Epoch: 160| Step: 0
Training loss: 2.3990956748466434
Validation loss: 2.449902460855491

Epoch: 6| Step: 1
Training loss: 2.2264722303864213
Validation loss: 2.4513240447817224

Epoch: 6| Step: 2
Training loss: 2.647974474556281
Validation loss: 2.453647242069076

Epoch: 6| Step: 3
Training loss: 2.362508347915352
Validation loss: 2.447926108700554

Epoch: 6| Step: 4
Training loss: 2.6014438178223984
Validation loss: 2.4585069471143046

Epoch: 6| Step: 5
Training loss: 2.184759439629799
Validation loss: 2.4549108269702287

Epoch: 6| Step: 6
Training loss: 2.0195988951814696
Validation loss: 2.4551647153812133

Epoch: 6| Step: 7
Training loss: 2.8183006655895673
Validation loss: 2.460498266027106

Epoch: 6| Step: 8
Training loss: 2.5429621870715042
Validation loss: 2.452640801528306

Epoch: 6| Step: 9
Training loss: 2.092056956360124
Validation loss: 2.452636815959086

Epoch: 6| Step: 10
Training loss: 2.1440603715524365
Validation loss: 2.456269890702253

Epoch: 6| Step: 11
Training loss: 2.8208499301288796
Validation loss: 2.4638321902137754

Epoch: 6| Step: 12
Training loss: 2.9051314631801732
Validation loss: 2.4615926526444007

Epoch: 6| Step: 13
Training loss: 2.51149823539318
Validation loss: 2.4696676423976247

Epoch: 161| Step: 0
Training loss: 2.7212209763247355
Validation loss: 2.461795798495933

Epoch: 6| Step: 1
Training loss: 2.422326814899499
Validation loss: 2.4633749864469556

Epoch: 6| Step: 2
Training loss: 2.652546353811494
Validation loss: 2.4585608414802285

Epoch: 6| Step: 3
Training loss: 2.343900853706405
Validation loss: 2.457448918797074

Epoch: 6| Step: 4
Training loss: 2.553429530085431
Validation loss: 2.4627510936800334

Epoch: 6| Step: 5
Training loss: 2.032115575792604
Validation loss: 2.454714687236515

Epoch: 6| Step: 6
Training loss: 2.749857898855449
Validation loss: 2.454792096074482

Epoch: 6| Step: 7
Training loss: 2.61354699943265
Validation loss: 2.4637687422531256

Epoch: 6| Step: 8
Training loss: 2.417737811126767
Validation loss: 2.4496370208855884

Epoch: 6| Step: 9
Training loss: 2.2093224979189414
Validation loss: 2.4530188098123573

Epoch: 6| Step: 10
Training loss: 1.8326273194510363
Validation loss: 2.446719836125932

Epoch: 6| Step: 11
Training loss: 2.8093934598492343
Validation loss: 2.4521086707436086

Epoch: 6| Step: 12
Training loss: 2.0469733061734154
Validation loss: 2.4535102946618252

Epoch: 6| Step: 13
Training loss: 2.8189352376426524
Validation loss: 2.461368131552081

Epoch: 162| Step: 0
Training loss: 2.3097150374185014
Validation loss: 2.4484038875879897

Epoch: 6| Step: 1
Training loss: 2.6159029868741426
Validation loss: 2.4518583230683073

Epoch: 6| Step: 2
Training loss: 1.8636946309574745
Validation loss: 2.459262919992423

Epoch: 6| Step: 3
Training loss: 2.587374653751743
Validation loss: 2.448942194184658

Epoch: 6| Step: 4
Training loss: 2.211792581687523
Validation loss: 2.464386250174108

Epoch: 6| Step: 5
Training loss: 2.463318656736571
Validation loss: 2.458728036606239

Epoch: 6| Step: 6
Training loss: 1.7900020347882006
Validation loss: 2.4640585993459947

Epoch: 6| Step: 7
Training loss: 2.5338325042164542
Validation loss: 2.469762674904547

Epoch: 6| Step: 8
Training loss: 2.3307422033758716
Validation loss: 2.4605383009296142

Epoch: 6| Step: 9
Training loss: 2.0551722429770267
Validation loss: 2.4658698975264075

Epoch: 6| Step: 10
Training loss: 2.9867202571193556
Validation loss: 2.4626849877960453

Epoch: 6| Step: 11
Training loss: 2.7372787316487024
Validation loss: 2.4701878670046775

Epoch: 6| Step: 12
Training loss: 2.623922035861729
Validation loss: 2.4496423252626784

Epoch: 6| Step: 13
Training loss: 2.8179284160827267
Validation loss: 2.4793844100687674

Epoch: 163| Step: 0
Training loss: 2.823794426705746
Validation loss: 2.4806652401772493

Epoch: 6| Step: 1
Training loss: 2.476855335913162
Validation loss: 2.492551021710545

Epoch: 6| Step: 2
Training loss: 2.0466789559400236
Validation loss: 2.5026618376619063

Epoch: 6| Step: 3
Training loss: 2.1054579923846815
Validation loss: 2.5150286360539327

Epoch: 6| Step: 4
Training loss: 2.6592166266255512
Validation loss: 2.4984394527552523

Epoch: 6| Step: 5
Training loss: 2.527291959205388
Validation loss: 2.475286000357181

Epoch: 6| Step: 6
Training loss: 2.7065069838651694
Validation loss: 2.471476614477511

Epoch: 6| Step: 7
Training loss: 2.600095409696799
Validation loss: 2.4718609133904565

Epoch: 6| Step: 8
Training loss: 2.3280897713882465
Validation loss: 2.4686421801365412

Epoch: 6| Step: 9
Training loss: 2.3543023151938036
Validation loss: 2.4703603313205944

Epoch: 6| Step: 10
Training loss: 2.1740161029601817
Validation loss: 2.45771691843191

Epoch: 6| Step: 11
Training loss: 2.6308561125397367
Validation loss: 2.450779273215054

Epoch: 6| Step: 12
Training loss: 2.511937252050645
Validation loss: 2.461055240135659

Epoch: 6| Step: 13
Training loss: 2.272251094871993
Validation loss: 2.4522898846029637

Epoch: 164| Step: 0
Training loss: 2.7840585993376465
Validation loss: 2.4594620252607577

Epoch: 6| Step: 1
Training loss: 2.3920981661450704
Validation loss: 2.4579011623447333

Epoch: 6| Step: 2
Training loss: 2.6513843627148588
Validation loss: 2.4601938244205557

Epoch: 6| Step: 3
Training loss: 2.2910100053879288
Validation loss: 2.462103786317919

Epoch: 6| Step: 4
Training loss: 3.0126174397350556
Validation loss: 2.4591904993904965

Epoch: 6| Step: 5
Training loss: 2.023269233924229
Validation loss: 2.463571388328792

Epoch: 6| Step: 6
Training loss: 2.4970720311271952
Validation loss: 2.4624592269387806

Epoch: 6| Step: 7
Training loss: 2.576077844900709
Validation loss: 2.4695573766159624

Epoch: 6| Step: 8
Training loss: 2.228388736733228
Validation loss: 2.4638578173075802

Epoch: 6| Step: 9
Training loss: 2.085823541735859
Validation loss: 2.4667521251446236

Epoch: 6| Step: 10
Training loss: 2.099858951373073
Validation loss: 2.4622165000367415

Epoch: 6| Step: 11
Training loss: 3.1016545894605634
Validation loss: 2.467323003473984

Epoch: 6| Step: 12
Training loss: 2.344933986426015
Validation loss: 2.4715825502260262

Epoch: 6| Step: 13
Training loss: 2.391377149951326
Validation loss: 2.4687073780356212

Epoch: 165| Step: 0
Training loss: 2.1489162951925085
Validation loss: 2.477474506014761

Epoch: 6| Step: 1
Training loss: 2.7065594855198043
Validation loss: 2.4733276741223573

Epoch: 6| Step: 2
Training loss: 2.248435748260643
Validation loss: 2.468865251064117

Epoch: 6| Step: 3
Training loss: 2.3832772223885543
Validation loss: 2.481628435231946

Epoch: 6| Step: 4
Training loss: 2.5892723684483743
Validation loss: 2.475448726987298

Epoch: 6| Step: 5
Training loss: 2.305853080191624
Validation loss: 2.4739085195779893

Epoch: 6| Step: 6
Training loss: 2.4109879454442305
Validation loss: 2.467362299551801

Epoch: 6| Step: 7
Training loss: 2.446556579216266
Validation loss: 2.48170008897105

Epoch: 6| Step: 8
Training loss: 1.9139795402142852
Validation loss: 2.4634201686542556

Epoch: 6| Step: 9
Training loss: 3.221599909733292
Validation loss: 2.4674767941250417

Epoch: 6| Step: 10
Training loss: 3.195096108867065
Validation loss: 2.465596853272041

Epoch: 6| Step: 11
Training loss: 2.4147501833266265
Validation loss: 2.4628265880757296

Epoch: 6| Step: 12
Training loss: 2.1549136678657432
Validation loss: 2.4561623399928534

Epoch: 6| Step: 13
Training loss: 2.1369821780099336
Validation loss: 2.4634122888466305

Epoch: 166| Step: 0
Training loss: 2.2245012345793973
Validation loss: 2.4627117078886176

Epoch: 6| Step: 1
Training loss: 2.40412214250067
Validation loss: 2.460637957392108

Epoch: 6| Step: 2
Training loss: 2.615442314741333
Validation loss: 2.4599010161900456

Epoch: 6| Step: 3
Training loss: 2.5861486250791033
Validation loss: 2.449268265527353

Epoch: 6| Step: 4
Training loss: 2.2950861186240124
Validation loss: 2.455322658081974

Epoch: 6| Step: 5
Training loss: 2.9989291505143445
Validation loss: 2.467417699516424

Epoch: 6| Step: 6
Training loss: 1.9110333821781011
Validation loss: 2.4670142981284084

Epoch: 6| Step: 7
Training loss: 2.225810034242077
Validation loss: 2.4574398071042283

Epoch: 6| Step: 8
Training loss: 2.7658152541760908
Validation loss: 2.4596962841305032

Epoch: 6| Step: 9
Training loss: 2.4694869474632593
Validation loss: 2.469064748795443

Epoch: 6| Step: 10
Training loss: 2.3956029587708763
Validation loss: 2.468040251614358

Epoch: 6| Step: 11
Training loss: 2.4266196750799973
Validation loss: 2.4661069717218758

Epoch: 6| Step: 12
Training loss: 2.646713370800717
Validation loss: 2.466910630696818

Epoch: 6| Step: 13
Training loss: 2.2353891058719237
Validation loss: 2.4745220840341187

Epoch: 167| Step: 0
Training loss: 2.7167386035717014
Validation loss: 2.4667062629494314

Epoch: 6| Step: 1
Training loss: 1.9839478280760279
Validation loss: 2.4662628199789847

Epoch: 6| Step: 2
Training loss: 2.2172020898920954
Validation loss: 2.4651134925750235

Epoch: 6| Step: 3
Training loss: 2.4718095194442147
Validation loss: 2.4654363211094994

Epoch: 6| Step: 4
Training loss: 2.7315373110288954
Validation loss: 2.4678401799618275

Epoch: 6| Step: 5
Training loss: 2.2020581415107436
Validation loss: 2.461210786692099

Epoch: 6| Step: 6
Training loss: 2.3682264811361664
Validation loss: 2.4659781125379467

Epoch: 6| Step: 7
Training loss: 2.1600116591668703
Validation loss: 2.4510147024071927

Epoch: 6| Step: 8
Training loss: 1.700848589200585
Validation loss: 2.4620097574381012

Epoch: 6| Step: 9
Training loss: 2.2246280230643993
Validation loss: 2.4658155424871353

Epoch: 6| Step: 10
Training loss: 2.789020099905302
Validation loss: 2.459745475658649

Epoch: 6| Step: 11
Training loss: 3.005380256446735
Validation loss: 2.4601331254040475

Epoch: 6| Step: 12
Training loss: 1.6033367470739959
Validation loss: 2.459702519958308

Epoch: 6| Step: 13
Training loss: 3.514354968080886
Validation loss: 2.452023285092991

Epoch: 168| Step: 0
Training loss: 2.242094563010218
Validation loss: 2.4646480782894495

Epoch: 6| Step: 1
Training loss: 2.0424982699850314
Validation loss: 2.4605486042803713

Epoch: 6| Step: 2
Training loss: 2.905522245123368
Validation loss: 2.4626919421475635

Epoch: 6| Step: 3
Training loss: 1.7685918946251808
Validation loss: 2.463029422423714

Epoch: 6| Step: 4
Training loss: 2.8969999383117226
Validation loss: 2.465223167304095

Epoch: 6| Step: 5
Training loss: 2.2708156713329655
Validation loss: 2.4711251884693346

Epoch: 6| Step: 6
Training loss: 2.4449211026745266
Validation loss: 2.4498156277057634

Epoch: 6| Step: 7
Training loss: 2.35223865610007
Validation loss: 2.452382276964998

Epoch: 6| Step: 8
Training loss: 2.513710383288466
Validation loss: 2.4677330369036232

Epoch: 6| Step: 9
Training loss: 2.4763317301480505
Validation loss: 2.4599912570136375

Epoch: 6| Step: 10
Training loss: 2.731300412897784
Validation loss: 2.461022043542

Epoch: 6| Step: 11
Training loss: 2.5209292769235887
Validation loss: 2.472251822857566

Epoch: 6| Step: 12
Training loss: 2.8728993246335626
Validation loss: 2.458903308569382

Epoch: 6| Step: 13
Training loss: 2.224556002089654
Validation loss: 2.476943940351826

Epoch: 169| Step: 0
Training loss: 2.322429680062552
Validation loss: 2.4764003118923235

Epoch: 6| Step: 1
Training loss: 3.0326852448468045
Validation loss: 2.4633731152630562

Epoch: 6| Step: 2
Training loss: 2.695963816798968
Validation loss: 2.4711126458049297

Epoch: 6| Step: 3
Training loss: 2.602937627050199
Validation loss: 2.4766241120096324

Epoch: 6| Step: 4
Training loss: 2.5175671395949233
Validation loss: 2.4729866823495197

Epoch: 6| Step: 5
Training loss: 2.5963467387734354
Validation loss: 2.4755391875799253

Epoch: 6| Step: 6
Training loss: 2.1086052514434264
Validation loss: 2.4719079177407206

Epoch: 6| Step: 7
Training loss: 1.8399233142794835
Validation loss: 2.4738896303508673

Epoch: 6| Step: 8
Training loss: 1.9642952671066445
Validation loss: 2.480812733745893

Epoch: 6| Step: 9
Training loss: 2.1661135872375072
Validation loss: 2.464397730627771

Epoch: 6| Step: 10
Training loss: 2.498644461299353
Validation loss: 2.464499585063474

Epoch: 6| Step: 11
Training loss: 1.613242160425154
Validation loss: 2.4597634719193833

Epoch: 6| Step: 12
Training loss: 3.0529802239255766
Validation loss: 2.4589865403977593

Epoch: 6| Step: 13
Training loss: 2.6362467025367224
Validation loss: 2.4708084506175187

Epoch: 170| Step: 0
Training loss: 2.4238306824749802
Validation loss: 2.464801092730871

Epoch: 6| Step: 1
Training loss: 2.722358063359848
Validation loss: 2.473925754265681

Epoch: 6| Step: 2
Training loss: 2.3959728587337583
Validation loss: 2.474373348293667

Epoch: 6| Step: 3
Training loss: 2.8565179277629014
Validation loss: 2.4911123126884704

Epoch: 6| Step: 4
Training loss: 2.775707766341042
Validation loss: 2.475311428603281

Epoch: 6| Step: 5
Training loss: 2.7380153292887917
Validation loss: 2.4836287587304247

Epoch: 6| Step: 6
Training loss: 2.815015557959269
Validation loss: 2.482817235437594

Epoch: 6| Step: 7
Training loss: 1.9005618670037652
Validation loss: 2.4718023496063783

Epoch: 6| Step: 8
Training loss: 2.1583350231895557
Validation loss: 2.4772357366555937

Epoch: 6| Step: 9
Training loss: 1.8930371242617001
Validation loss: 2.4700481526704334

Epoch: 6| Step: 10
Training loss: 2.5814021696076113
Validation loss: 2.4611193554407427

Epoch: 6| Step: 11
Training loss: 2.7228140436363155
Validation loss: 2.4638920239821993

Epoch: 6| Step: 12
Training loss: 1.983160593487057
Validation loss: 2.4594779232580426

Epoch: 6| Step: 13
Training loss: 2.0296160655265822
Validation loss: 2.4698228396203876

Epoch: 171| Step: 0
Training loss: 2.779070911349701
Validation loss: 2.4823988486481854

Epoch: 6| Step: 1
Training loss: 2.4539315422476817
Validation loss: 2.4764045961825256

Epoch: 6| Step: 2
Training loss: 2.373068676758133
Validation loss: 2.4779462668899894

Epoch: 6| Step: 3
Training loss: 1.9448462237684616
Validation loss: 2.476325463986171

Epoch: 6| Step: 4
Training loss: 1.9785105035834196
Validation loss: 2.4717591533047063

Epoch: 6| Step: 5
Training loss: 2.568283810972536
Validation loss: 2.4633122042247475

Epoch: 6| Step: 6
Training loss: 2.580224863943207
Validation loss: 2.459529300229526

Epoch: 6| Step: 7
Training loss: 2.6249104439127082
Validation loss: 2.462771496272704

Epoch: 6| Step: 8
Training loss: 1.9503245303726233
Validation loss: 2.448682435294516

Epoch: 6| Step: 9
Training loss: 2.6596179030909486
Validation loss: 2.4596387150615966

Epoch: 6| Step: 10
Training loss: 3.0139591972560957
Validation loss: 2.451164693429262

Epoch: 6| Step: 11
Training loss: 1.9315996464624514
Validation loss: 2.4643028217877587

Epoch: 6| Step: 12
Training loss: 2.4313820575064318
Validation loss: 2.4707785613715347

Epoch: 6| Step: 13
Training loss: 2.3101810478502407
Validation loss: 2.468745412701558

Epoch: 172| Step: 0
Training loss: 2.131800708488406
Validation loss: 2.4739762850996847

Epoch: 6| Step: 1
Training loss: 2.26304964165007
Validation loss: 2.467132134809883

Epoch: 6| Step: 2
Training loss: 3.0451666321910955
Validation loss: 2.4765190176851988

Epoch: 6| Step: 3
Training loss: 2.684922601447375
Validation loss: 2.4796455839669083

Epoch: 6| Step: 4
Training loss: 2.295479071320433
Validation loss: 2.464315205615418

Epoch: 6| Step: 5
Training loss: 2.629342256980955
Validation loss: 2.4604888022558873

Epoch: 6| Step: 6
Training loss: 2.7143008780235123
Validation loss: 2.4600572414509814

Epoch: 6| Step: 7
Training loss: 1.9935814861007395
Validation loss: 2.4536099450640667

Epoch: 6| Step: 8
Training loss: 1.7543835914328063
Validation loss: 2.4578686183335785

Epoch: 6| Step: 9
Training loss: 2.8204279741227847
Validation loss: 2.459419403903258

Epoch: 6| Step: 10
Training loss: 2.4762142670627334
Validation loss: 2.4659638597862314

Epoch: 6| Step: 11
Training loss: 2.2928616500355212
Validation loss: 2.468459792322423

Epoch: 6| Step: 12
Training loss: 2.2937587436436613
Validation loss: 2.462109919213137

Epoch: 6| Step: 13
Training loss: 2.507173545938359
Validation loss: 2.465034054333292

Epoch: 173| Step: 0
Training loss: 2.3205786144614873
Validation loss: 2.4637930474987377

Epoch: 6| Step: 1
Training loss: 2.1300838468283456
Validation loss: 2.456418606420674

Epoch: 6| Step: 2
Training loss: 2.527851319087488
Validation loss: 2.454226948937376

Epoch: 6| Step: 3
Training loss: 2.3587666921019923
Validation loss: 2.4651949592493403

Epoch: 6| Step: 4
Training loss: 3.1050983711913847
Validation loss: 2.462133918053625

Epoch: 6| Step: 5
Training loss: 2.1411327121944703
Validation loss: 2.474641104727847

Epoch: 6| Step: 6
Training loss: 1.8590691579397163
Validation loss: 2.4596669868367016

Epoch: 6| Step: 7
Training loss: 2.1062929596184574
Validation loss: 2.4639972537023342

Epoch: 6| Step: 8
Training loss: 2.7786234765671347
Validation loss: 2.4663618504069595

Epoch: 6| Step: 9
Training loss: 2.709214072194564
Validation loss: 2.4633394498409307

Epoch: 6| Step: 10
Training loss: 2.6824214768093486
Validation loss: 2.467547546361506

Epoch: 6| Step: 11
Training loss: 2.1717825670950104
Validation loss: 2.4606871946893274

Epoch: 6| Step: 12
Training loss: 2.1922562254657327
Validation loss: 2.4668519010671277

Epoch: 6| Step: 13
Training loss: 2.5420040052669015
Validation loss: 2.458476843659119

Epoch: 174| Step: 0
Training loss: 2.3845302706217066
Validation loss: 2.462490911515403

Epoch: 6| Step: 1
Training loss: 2.3969663401163217
Validation loss: 2.473039851432005

Epoch: 6| Step: 2
Training loss: 2.1883160839965012
Validation loss: 2.4668893844562683

Epoch: 6| Step: 3
Training loss: 2.9909652723657705
Validation loss: 2.461143250921696

Epoch: 6| Step: 4
Training loss: 2.8481200459286313
Validation loss: 2.460742187822963

Epoch: 6| Step: 5
Training loss: 2.1749690130372246
Validation loss: 2.465857537621244

Epoch: 6| Step: 6
Training loss: 1.759009331058738
Validation loss: 2.4521622520026254

Epoch: 6| Step: 7
Training loss: 2.5691167490718145
Validation loss: 2.46287174807521

Epoch: 6| Step: 8
Training loss: 2.026359186731193
Validation loss: 2.4650574766296973

Epoch: 6| Step: 9
Training loss: 2.058491018435528
Validation loss: 2.4672552162687595

Epoch: 6| Step: 10
Training loss: 2.070784604927534
Validation loss: 2.4661526597655956

Epoch: 6| Step: 11
Training loss: 2.7823057421165536
Validation loss: 2.4653120286260646

Epoch: 6| Step: 12
Training loss: 2.645595229441192
Validation loss: 2.4646412906834825

Epoch: 6| Step: 13
Training loss: 2.6897248218846026
Validation loss: 2.4556029291699186

Epoch: 175| Step: 0
Training loss: 3.0106001185362476
Validation loss: 2.4612336964452313

Epoch: 6| Step: 1
Training loss: 2.1003439394540755
Validation loss: 2.460246074769386

Epoch: 6| Step: 2
Training loss: 2.1942392352551723
Validation loss: 2.469004444900617

Epoch: 6| Step: 3
Training loss: 2.5434447046729285
Validation loss: 2.4754307163389666

Epoch: 6| Step: 4
Training loss: 2.155726272447119
Validation loss: 2.4581708342514936

Epoch: 6| Step: 5
Training loss: 2.2829545850877633
Validation loss: 2.4638139978796834

Epoch: 6| Step: 6
Training loss: 2.3101588590089257
Validation loss: 2.4608504184069955

Epoch: 6| Step: 7
Training loss: 2.4916067373496076
Validation loss: 2.4549345400875993

Epoch: 6| Step: 8
Training loss: 2.4053550145715015
Validation loss: 2.468055313490545

Epoch: 6| Step: 9
Training loss: 2.5709037680434084
Validation loss: 2.455643610240943

Epoch: 6| Step: 10
Training loss: 2.0209427112460836
Validation loss: 2.4791651626924285

Epoch: 6| Step: 11
Training loss: 2.5450568701305056
Validation loss: 2.479872792657777

Epoch: 6| Step: 12
Training loss: 2.7826608229890937
Validation loss: 2.508818950304283

Epoch: 6| Step: 13
Training loss: 2.7849206597414384
Validation loss: 2.5107376215312227

Epoch: 176| Step: 0
Training loss: 2.462928281612648
Validation loss: 2.4987671037285457

Epoch: 6| Step: 1
Training loss: 2.6093991215670256
Validation loss: 2.481461325869884

Epoch: 6| Step: 2
Training loss: 2.436691149976947
Validation loss: 2.4772463394914612

Epoch: 6| Step: 3
Training loss: 2.3466377270424084
Validation loss: 2.45922761481145

Epoch: 6| Step: 4
Training loss: 2.480083189307369
Validation loss: 2.4712475163098855

Epoch: 6| Step: 5
Training loss: 2.5837136470634614
Validation loss: 2.4669277773587375

Epoch: 6| Step: 6
Training loss: 2.997350794599903
Validation loss: 2.4744900717575624

Epoch: 6| Step: 7
Training loss: 1.8286452897941226
Validation loss: 2.4757823015995024

Epoch: 6| Step: 8
Training loss: 2.651816224348838
Validation loss: 2.473166703843105

Epoch: 6| Step: 9
Training loss: 2.7433776011860287
Validation loss: 2.4731613695852417

Epoch: 6| Step: 10
Training loss: 2.1441125235138125
Validation loss: 2.4829756756555255

Epoch: 6| Step: 11
Training loss: 2.461348435742357
Validation loss: 2.4717937972165585

Epoch: 6| Step: 12
Training loss: 2.6193840260806405
Validation loss: 2.4810420938119293

Epoch: 6| Step: 13
Training loss: 1.5492236869760279
Validation loss: 2.4720044313954834

Epoch: 177| Step: 0
Training loss: 2.350124388304357
Validation loss: 2.4710847301079335

Epoch: 6| Step: 1
Training loss: 2.166978104534004
Validation loss: 2.4682487268605904

Epoch: 6| Step: 2
Training loss: 3.4224242723222194
Validation loss: 2.4642325004613417

Epoch: 6| Step: 3
Training loss: 2.7451322996849234
Validation loss: 2.4592754423032663

Epoch: 6| Step: 4
Training loss: 1.5271318293525944
Validation loss: 2.4706730816890454

Epoch: 6| Step: 5
Training loss: 2.0970700259979895
Validation loss: 2.470137676927109

Epoch: 6| Step: 6
Training loss: 2.5511046317073993
Validation loss: 2.4720256015250195

Epoch: 6| Step: 7
Training loss: 2.338379670941747
Validation loss: 2.4939065742257447

Epoch: 6| Step: 8
Training loss: 2.5994844585760495
Validation loss: 2.5053746782196797

Epoch: 6| Step: 9
Training loss: 2.8493165250695087
Validation loss: 2.4875658445094184

Epoch: 6| Step: 10
Training loss: 1.8893126038984744
Validation loss: 2.4795329573991576

Epoch: 6| Step: 11
Training loss: 2.6365680114608914
Validation loss: 2.4632487751338155

Epoch: 6| Step: 12
Training loss: 2.2778611814017684
Validation loss: 2.4622117875962966

Epoch: 6| Step: 13
Training loss: 2.2274663863979987
Validation loss: 2.4673947666586935

Epoch: 178| Step: 0
Training loss: 2.808074351366996
Validation loss: 2.4651415484283534

Epoch: 6| Step: 1
Training loss: 2.856815881411619
Validation loss: 2.4674275876351017

Epoch: 6| Step: 2
Training loss: 2.739993443028756
Validation loss: 2.475354659232731

Epoch: 6| Step: 3
Training loss: 2.342382922427403
Validation loss: 2.4696592435137847

Epoch: 6| Step: 4
Training loss: 1.7532158323206264
Validation loss: 2.4705406165676274

Epoch: 6| Step: 5
Training loss: 2.4139770690220743
Validation loss: 2.4704941652413464

Epoch: 6| Step: 6
Training loss: 2.398412250795944
Validation loss: 2.46074122700795

Epoch: 6| Step: 7
Training loss: 2.3787587187422323
Validation loss: 2.469687030516576

Epoch: 6| Step: 8
Training loss: 2.743118171674704
Validation loss: 2.467385603118722

Epoch: 6| Step: 9
Training loss: 2.4728710689909694
Validation loss: 2.478900066968739

Epoch: 6| Step: 10
Training loss: 2.372670687775055
Validation loss: 2.471426555081203

Epoch: 6| Step: 11
Training loss: 2.038286200181441
Validation loss: 2.46807918206921

Epoch: 6| Step: 12
Training loss: 2.2233185275879763
Validation loss: 2.47531438237251

Epoch: 6| Step: 13
Training loss: 2.4057750480875666
Validation loss: 2.4649628833609745

Epoch: 179| Step: 0
Training loss: 2.0453174285013316
Validation loss: 2.458320849327674

Epoch: 6| Step: 1
Training loss: 2.594160116473134
Validation loss: 2.479620296318366

Epoch: 6| Step: 2
Training loss: 2.7462791933740065
Validation loss: 2.4669325210595656

Epoch: 6| Step: 3
Training loss: 1.7334427261067447
Validation loss: 2.469329749815725

Epoch: 6| Step: 4
Training loss: 2.218299282661944
Validation loss: 2.460045320770525

Epoch: 6| Step: 5
Training loss: 2.826167572587646
Validation loss: 2.4627659700829967

Epoch: 6| Step: 6
Training loss: 2.376145136976085
Validation loss: 2.4589993226695617

Epoch: 6| Step: 7
Training loss: 2.6077334785717023
Validation loss: 2.46738594131676

Epoch: 6| Step: 8
Training loss: 2.1768325101444255
Validation loss: 2.4659897225478513

Epoch: 6| Step: 9
Training loss: 1.728826083304002
Validation loss: 2.4587103398351338

Epoch: 6| Step: 10
Training loss: 2.9558974073929267
Validation loss: 2.4700398998667197

Epoch: 6| Step: 11
Training loss: 3.063557033679706
Validation loss: 2.4667832793774527

Epoch: 6| Step: 12
Training loss: 2.044741384853502
Validation loss: 2.4644512946218335

Epoch: 6| Step: 13
Training loss: 2.476309104495208
Validation loss: 2.4615050292190026

Epoch: 180| Step: 0
Training loss: 2.8864706343176816
Validation loss: 2.473160164555517

Epoch: 6| Step: 1
Training loss: 2.2652875977888107
Validation loss: 2.461558091161019

Epoch: 6| Step: 2
Training loss: 2.881536268465051
Validation loss: 2.4749173256523314

Epoch: 6| Step: 3
Training loss: 2.410918821508474
Validation loss: 2.466381224234533

Epoch: 6| Step: 4
Training loss: 2.2226287893734216
Validation loss: 2.4759908632151975

Epoch: 6| Step: 5
Training loss: 2.544146427690872
Validation loss: 2.482154341557102

Epoch: 6| Step: 6
Training loss: 2.3350517098952053
Validation loss: 2.491441541301979

Epoch: 6| Step: 7
Training loss: 2.151791482679123
Validation loss: 2.489926786686892

Epoch: 6| Step: 8
Training loss: 2.2766879725913776
Validation loss: 2.47786008774628

Epoch: 6| Step: 9
Training loss: 1.8122787998314391
Validation loss: 2.4790036818413004

Epoch: 6| Step: 10
Training loss: 2.427348001044817
Validation loss: 2.483266507173407

Epoch: 6| Step: 11
Training loss: 2.1473310361034885
Validation loss: 2.4768521433348467

Epoch: 6| Step: 12
Training loss: 2.6191025744695953
Validation loss: 2.484086121852221

Epoch: 6| Step: 13
Training loss: 2.5571708158591857
Validation loss: 2.473731604108218

Epoch: 181| Step: 0
Training loss: 1.9001102064194346
Validation loss: 2.4680955236611113

Epoch: 6| Step: 1
Training loss: 2.4707250300103567
Validation loss: 2.4728519308031034

Epoch: 6| Step: 2
Training loss: 2.379652385490167
Validation loss: 2.479763317170977

Epoch: 6| Step: 3
Training loss: 2.536310670746484
Validation loss: 2.4864375032197783

Epoch: 6| Step: 4
Training loss: 2.157239382884041
Validation loss: 2.489657466109237

Epoch: 6| Step: 5
Training loss: 2.5422707345345614
Validation loss: 2.498760424705592

Epoch: 6| Step: 6
Training loss: 3.074918674541085
Validation loss: 2.507773901146198

Epoch: 6| Step: 7
Training loss: 2.6345600248728824
Validation loss: 2.5060315646573326

Epoch: 6| Step: 8
Training loss: 2.866979274412724
Validation loss: 2.501649233100559

Epoch: 6| Step: 9
Training loss: 2.5711614719815716
Validation loss: 2.501347568038171

Epoch: 6| Step: 10
Training loss: 2.4498286362488426
Validation loss: 2.4899777907283664

Epoch: 6| Step: 11
Training loss: 2.3659069261148735
Validation loss: 2.482725975684785

Epoch: 6| Step: 12
Training loss: 2.261855785059522
Validation loss: 2.4797772181977553

Epoch: 6| Step: 13
Training loss: 2.391283131682474
Validation loss: 2.4869120377406744

Epoch: 182| Step: 0
Training loss: 2.1540912381314827
Validation loss: 2.469641383723315

Epoch: 6| Step: 1
Training loss: 2.2023214403357425
Validation loss: 2.4715226855905432

Epoch: 6| Step: 2
Training loss: 2.148601844744053
Validation loss: 2.47744061519152

Epoch: 6| Step: 3
Training loss: 1.9230273753899145
Validation loss: 2.4799762387060302

Epoch: 6| Step: 4
Training loss: 2.4927636320678515
Validation loss: 2.4820615285295924

Epoch: 6| Step: 5
Training loss: 2.487189659810773
Validation loss: 2.486046818364219

Epoch: 6| Step: 6
Training loss: 1.8317876788085963
Validation loss: 2.513608341357674

Epoch: 6| Step: 7
Training loss: 3.0209925817848706
Validation loss: 2.503650876420996

Epoch: 6| Step: 8
Training loss: 2.5124880738236266
Validation loss: 2.48983422349253

Epoch: 6| Step: 9
Training loss: 2.4882708059717533
Validation loss: 2.4942476054971525

Epoch: 6| Step: 10
Training loss: 2.5042290204283177
Validation loss: 2.479224858816507

Epoch: 6| Step: 11
Training loss: 2.5569856438560117
Validation loss: 2.47595885387994

Epoch: 6| Step: 12
Training loss: 2.778160522371147
Validation loss: 2.4774863909499327

Epoch: 6| Step: 13
Training loss: 2.6019756916913326
Validation loss: 2.4730308052167125

Epoch: 183| Step: 0
Training loss: 2.0499392570242208
Validation loss: 2.466716097538344

Epoch: 6| Step: 1
Training loss: 2.40131555899665
Validation loss: 2.4649820344385014

Epoch: 6| Step: 2
Training loss: 2.442311453282067
Validation loss: 2.47585062571979

Epoch: 6| Step: 3
Training loss: 2.1896403059971328
Validation loss: 2.464977875378757

Epoch: 6| Step: 4
Training loss: 2.447504103504205
Validation loss: 2.4700092049863467

Epoch: 6| Step: 5
Training loss: 2.5993446771348574
Validation loss: 2.4703915043904474

Epoch: 6| Step: 6
Training loss: 2.780655872450536
Validation loss: 2.474643898722854

Epoch: 6| Step: 7
Training loss: 2.7457746643220133
Validation loss: 2.462995066635368

Epoch: 6| Step: 8
Training loss: 2.345880582705136
Validation loss: 2.450359048614088

Epoch: 6| Step: 9
Training loss: 2.320616217364668
Validation loss: 2.4799888647249917

Epoch: 6| Step: 10
Training loss: 1.6956166640574877
Validation loss: 2.47029639956049

Epoch: 6| Step: 11
Training loss: 2.156514469458163
Validation loss: 2.467894973449151

Epoch: 6| Step: 12
Training loss: 2.6601631511761163
Validation loss: 2.4706955498741334

Epoch: 6| Step: 13
Training loss: 2.6789995650589757
Validation loss: 2.4821891285385367

Epoch: 184| Step: 0
Training loss: 2.181541829958659
Validation loss: 2.474268471535726

Epoch: 6| Step: 1
Training loss: 2.413227418328161
Validation loss: 2.4769601753317807

Epoch: 6| Step: 2
Training loss: 2.2469663301669227
Validation loss: 2.475545070493962

Epoch: 6| Step: 3
Training loss: 2.4594764853336533
Validation loss: 2.4646388884146595

Epoch: 6| Step: 4
Training loss: 2.235571480895077
Validation loss: 2.4586024918273015

Epoch: 6| Step: 5
Training loss: 2.7232859689180384
Validation loss: 2.4632327724194925

Epoch: 6| Step: 6
Training loss: 2.5843340463403197
Validation loss: 2.467477357767581

Epoch: 6| Step: 7
Training loss: 2.435132760065263
Validation loss: 2.4684477190468677

Epoch: 6| Step: 8
Training loss: 2.10148240401212
Validation loss: 2.474530707285367

Epoch: 6| Step: 9
Training loss: 2.1172487594919764
Validation loss: 2.473084881167677

Epoch: 6| Step: 10
Training loss: 2.4568969495787996
Validation loss: 2.491370343180531

Epoch: 6| Step: 11
Training loss: 2.6267501809387057
Validation loss: 2.503712948674011

Epoch: 6| Step: 12
Training loss: 2.245453161029298
Validation loss: 2.480294641506798

Epoch: 6| Step: 13
Training loss: 2.8748535450660113
Validation loss: 2.4777862702163804

Epoch: 185| Step: 0
Training loss: 3.264629449540952
Validation loss: 2.469056058172853

Epoch: 6| Step: 1
Training loss: 2.5973201218738367
Validation loss: 2.4663076753827977

Epoch: 6| Step: 2
Training loss: 2.0829943063175587
Validation loss: 2.465064706385118

Epoch: 6| Step: 3
Training loss: 2.266005490679372
Validation loss: 2.458202331630131

Epoch: 6| Step: 4
Training loss: 1.9630608480100948
Validation loss: 2.462969583833358

Epoch: 6| Step: 5
Training loss: 2.325559700487654
Validation loss: 2.466596058101273

Epoch: 6| Step: 6
Training loss: 2.196898515339395
Validation loss: 2.4632191087293176

Epoch: 6| Step: 7
Training loss: 2.6417036392881714
Validation loss: 2.4707433483684373

Epoch: 6| Step: 8
Training loss: 2.030846775820759
Validation loss: 2.4549014711516923

Epoch: 6| Step: 9
Training loss: 2.3212555789418268
Validation loss: 2.4664139620322345

Epoch: 6| Step: 10
Training loss: 2.3746568532302224
Validation loss: 2.467404590451093

Epoch: 6| Step: 11
Training loss: 2.402009869455966
Validation loss: 2.4731398315987554

Epoch: 6| Step: 12
Training loss: 2.8000991395020356
Validation loss: 2.4760254960634738

Epoch: 6| Step: 13
Training loss: 2.090438045245135
Validation loss: 2.4851423234178602

Epoch: 186| Step: 0
Training loss: 2.3687398623448344
Validation loss: 2.4810961392964486

Epoch: 6| Step: 1
Training loss: 1.8710567335317774
Validation loss: 2.4730219357159506

Epoch: 6| Step: 2
Training loss: 2.4323203978927963
Validation loss: 2.4804095003355715

Epoch: 6| Step: 3
Training loss: 2.685790826988023
Validation loss: 2.474402222547449

Epoch: 6| Step: 4
Training loss: 2.470364199826648
Validation loss: 2.4655104278130415

Epoch: 6| Step: 5
Training loss: 2.095835998816593
Validation loss: 2.4716977254811483

Epoch: 6| Step: 6
Training loss: 2.2778059161652755
Validation loss: 2.46492459688029

Epoch: 6| Step: 7
Training loss: 1.9640237906077518
Validation loss: 2.4463471568830637

Epoch: 6| Step: 8
Training loss: 2.5193949817801
Validation loss: 2.453320230219237

Epoch: 6| Step: 9
Training loss: 2.1701419420008907
Validation loss: 2.4575707227970125

Epoch: 6| Step: 10
Training loss: 2.7294437083407184
Validation loss: 2.4624752024065217

Epoch: 6| Step: 11
Training loss: 3.1127702588019615
Validation loss: 2.4735677845857778

Epoch: 6| Step: 12
Training loss: 2.3748515233258303
Validation loss: 2.481940013860434

Epoch: 6| Step: 13
Training loss: 2.4295368699853905
Validation loss: 2.4745568016708024

Epoch: 187| Step: 0
Training loss: 2.552872599308626
Validation loss: 2.457284288663462

Epoch: 6| Step: 1
Training loss: 2.2141387732484494
Validation loss: 2.476244949214996

Epoch: 6| Step: 2
Training loss: 2.3225310024003116
Validation loss: 2.453857353052815

Epoch: 6| Step: 3
Training loss: 3.401040108852215
Validation loss: 2.4627530621463216

Epoch: 6| Step: 4
Training loss: 2.7853351551608125
Validation loss: 2.4637209455382374

Epoch: 6| Step: 5
Training loss: 2.207929850251772
Validation loss: 2.4698875317470166

Epoch: 6| Step: 6
Training loss: 2.1632843271042805
Validation loss: 2.4680791257187056

Epoch: 6| Step: 7
Training loss: 2.608637539935033
Validation loss: 2.4677925828684932

Epoch: 6| Step: 8
Training loss: 1.9567231261227191
Validation loss: 2.466024963145303

Epoch: 6| Step: 9
Training loss: 1.9224494137700057
Validation loss: 2.479629606946462

Epoch: 6| Step: 10
Training loss: 2.5790397177304394
Validation loss: 2.475355943458226

Epoch: 6| Step: 11
Training loss: 2.840144628146437
Validation loss: 2.468769314847307

Epoch: 6| Step: 12
Training loss: 1.7842304658893466
Validation loss: 2.4638324885798375

Epoch: 6| Step: 13
Training loss: 2.0415596706873522
Validation loss: 2.45525083336497

Epoch: 188| Step: 0
Training loss: 2.5245291884713086
Validation loss: 2.4648498760778006

Epoch: 6| Step: 1
Training loss: 2.010735903151605
Validation loss: 2.470406841460779

Epoch: 6| Step: 2
Training loss: 2.1672264134132018
Validation loss: 2.4587883339631156

Epoch: 6| Step: 3
Training loss: 2.4676349874288257
Validation loss: 2.469138955810382

Epoch: 6| Step: 4
Training loss: 2.8235967645021622
Validation loss: 2.4751494314377824

Epoch: 6| Step: 5
Training loss: 1.6694347362620396
Validation loss: 2.475978425442934

Epoch: 6| Step: 6
Training loss: 2.2706913568285922
Validation loss: 2.478543440971072

Epoch: 6| Step: 7
Training loss: 2.642581192544875
Validation loss: 2.473568121938558

Epoch: 6| Step: 8
Training loss: 1.8150069558717044
Validation loss: 2.4753056494793775

Epoch: 6| Step: 9
Training loss: 2.716754224649495
Validation loss: 2.4711094860082805

Epoch: 6| Step: 10
Training loss: 3.1486437708678947
Validation loss: 2.46018993184629

Epoch: 6| Step: 11
Training loss: 2.069711965156838
Validation loss: 2.46903082300648

Epoch: 6| Step: 12
Training loss: 2.416197194066584
Validation loss: 2.4645246892698376

Epoch: 6| Step: 13
Training loss: 2.2422313087782513
Validation loss: 2.4668498875470797

Epoch: 189| Step: 0
Training loss: 2.184109266978211
Validation loss: 2.467936611177599

Epoch: 6| Step: 1
Training loss: 2.576094689127532
Validation loss: 2.470189733026361

Epoch: 6| Step: 2
Training loss: 2.6739679359904596
Validation loss: 2.469483310902326

Epoch: 6| Step: 3
Training loss: 2.187422832762791
Validation loss: 2.4621013654340227

Epoch: 6| Step: 4
Training loss: 2.1074209699372477
Validation loss: 2.4705737495977194

Epoch: 6| Step: 5
Training loss: 2.709631173629897
Validation loss: 2.4687141705580875

Epoch: 6| Step: 6
Training loss: 2.2813549278897476
Validation loss: 2.468416545576308

Epoch: 6| Step: 7
Training loss: 1.886725257878107
Validation loss: 2.465136237103432

Epoch: 6| Step: 8
Training loss: 2.297427571579847
Validation loss: 2.465811852166798

Epoch: 6| Step: 9
Training loss: 2.2903891615048555
Validation loss: 2.465602606803437

Epoch: 6| Step: 10
Training loss: 3.043611784191784
Validation loss: 2.4685477322800553

Epoch: 6| Step: 11
Training loss: 2.4568177632312325
Validation loss: 2.4572306331241704

Epoch: 6| Step: 12
Training loss: 2.4362740001014984
Validation loss: 2.4703622213399017

Epoch: 6| Step: 13
Training loss: 2.180846640993631
Validation loss: 2.4752780058167443

Epoch: 190| Step: 0
Training loss: 2.012312184923066
Validation loss: 2.4668522071220313

Epoch: 6| Step: 1
Training loss: 2.8142446404987482
Validation loss: 2.4617346063052987

Epoch: 6| Step: 2
Training loss: 2.267716383025345
Validation loss: 2.4714255180282274

Epoch: 6| Step: 3
Training loss: 2.6479583576881063
Validation loss: 2.458326692626087

Epoch: 6| Step: 4
Training loss: 2.3428925026408414
Validation loss: 2.4721375655087194

Epoch: 6| Step: 5
Training loss: 1.9511327730065913
Validation loss: 2.47171608485976

Epoch: 6| Step: 6
Training loss: 2.8458207524376657
Validation loss: 2.478269243632363

Epoch: 6| Step: 7
Training loss: 2.2882895352626123
Validation loss: 2.472112064431437

Epoch: 6| Step: 8
Training loss: 2.0978070707462586
Validation loss: 2.467750781687034

Epoch: 6| Step: 9
Training loss: 2.1885296169592636
Validation loss: 2.4643810178428636

Epoch: 6| Step: 10
Training loss: 1.8728470203433292
Validation loss: 2.4805160718193537

Epoch: 6| Step: 11
Training loss: 3.5765364006426763
Validation loss: 2.474423685310328

Epoch: 6| Step: 12
Training loss: 1.9971867086725006
Validation loss: 2.4761502858496702

Epoch: 6| Step: 13
Training loss: 2.2789627523286695
Validation loss: 2.483208420404638

Epoch: 191| Step: 0
Training loss: 2.2668492659713855
Validation loss: 2.473898368242566

Epoch: 6| Step: 1
Training loss: 2.134629366505725
Validation loss: 2.46698346895891

Epoch: 6| Step: 2
Training loss: 2.784074442147725
Validation loss: 2.4606066365324257

Epoch: 6| Step: 3
Training loss: 2.6184629150934575
Validation loss: 2.4622080192513

Epoch: 6| Step: 4
Training loss: 2.249423482997227
Validation loss: 2.4785408597903547

Epoch: 6| Step: 5
Training loss: 3.3146817202125205
Validation loss: 2.4852701090097726

Epoch: 6| Step: 6
Training loss: 2.5527508128774503
Validation loss: 2.493082633147424

Epoch: 6| Step: 7
Training loss: 2.5083391343701305
Validation loss: 2.4925722724030015

Epoch: 6| Step: 8
Training loss: 2.487479998637542
Validation loss: 2.501266428931463

Epoch: 6| Step: 9
Training loss: 2.501223646155693
Validation loss: 2.4918402705958926

Epoch: 6| Step: 10
Training loss: 2.2326098318732166
Validation loss: 2.480617248415399

Epoch: 6| Step: 11
Training loss: 1.5443933791051514
Validation loss: 2.478049793360953

Epoch: 6| Step: 12
Training loss: 1.930678325594602
Validation loss: 2.4778349743168353

Epoch: 6| Step: 13
Training loss: 1.8545832719576445
Validation loss: 2.4797242656851424

Epoch: 192| Step: 0
Training loss: 2.4871306101972563
Validation loss: 2.4803830750254856

Epoch: 6| Step: 1
Training loss: 2.106075616969754
Validation loss: 2.4933611937188602

Epoch: 6| Step: 2
Training loss: 2.0451366235288355
Validation loss: 2.4952375508619467

Epoch: 6| Step: 3
Training loss: 1.7895794916907375
Validation loss: 2.495658569925383

Epoch: 6| Step: 4
Training loss: 2.620987686762873
Validation loss: 2.493466343119843

Epoch: 6| Step: 5
Training loss: 2.06808639900749
Validation loss: 2.4936455554171357

Epoch: 6| Step: 6
Training loss: 2.2916790586194313
Validation loss: 2.486130372179412

Epoch: 6| Step: 7
Training loss: 1.9252652914373463
Validation loss: 2.4880799951214625

Epoch: 6| Step: 8
Training loss: 1.9703705793468
Validation loss: 2.4910166669778295

Epoch: 6| Step: 9
Training loss: 2.9899450918405974
Validation loss: 2.4842413860354817

Epoch: 6| Step: 10
Training loss: 3.082142333529674
Validation loss: 2.4784864299187976

Epoch: 6| Step: 11
Training loss: 2.7027981951958338
Validation loss: 2.482367818360387

Epoch: 6| Step: 12
Training loss: 2.972554549734531
Validation loss: 2.471336964961737

Epoch: 6| Step: 13
Training loss: 2.0322794726338493
Validation loss: 2.458503698383327

Epoch: 193| Step: 0
Training loss: 2.852727615409029
Validation loss: 2.473887686808261

Epoch: 6| Step: 1
Training loss: 2.2064413446980127
Validation loss: 2.4951272763958414

Epoch: 6| Step: 2
Training loss: 2.231261317975059
Validation loss: 2.5541688974027448

Epoch: 6| Step: 3
Training loss: 2.4550773481005135
Validation loss: 2.5679965731161194

Epoch: 6| Step: 4
Training loss: 2.678109018284797
Validation loss: 2.574145704360858

Epoch: 6| Step: 5
Training loss: 2.158143469905172
Validation loss: 2.5124986386018673

Epoch: 6| Step: 6
Training loss: 2.434280421093278
Validation loss: 2.4793467309351658

Epoch: 6| Step: 7
Training loss: 2.9797226358151456
Validation loss: 2.487144118559086

Epoch: 6| Step: 8
Training loss: 2.680047903486826
Validation loss: 2.4935366367983414

Epoch: 6| Step: 9
Training loss: 2.6382202394416043
Validation loss: 2.484033237198053

Epoch: 6| Step: 10
Training loss: 2.321562664644567
Validation loss: 2.490814037799173

Epoch: 6| Step: 11
Training loss: 2.386710159111482
Validation loss: 2.4881699166375597

Epoch: 6| Step: 12
Training loss: 1.4883958667264086
Validation loss: 2.4913885894879932

Epoch: 6| Step: 13
Training loss: 2.7457592951778684
Validation loss: 2.492740159257318

Epoch: 194| Step: 0
Training loss: 2.7695050796542096
Validation loss: 2.4922564745120663

Epoch: 6| Step: 1
Training loss: 1.935116932807973
Validation loss: 2.4832585703286716

Epoch: 6| Step: 2
Training loss: 2.786667423187348
Validation loss: 2.479446945588996

Epoch: 6| Step: 3
Training loss: 2.179687171853974
Validation loss: 2.4840643746541766

Epoch: 6| Step: 4
Training loss: 2.2411407260244087
Validation loss: 2.4838593225684913

Epoch: 6| Step: 5
Training loss: 2.174177856280053
Validation loss: 2.4777799997143934

Epoch: 6| Step: 6
Training loss: 2.4018915510637213
Validation loss: 2.487044349868062

Epoch: 6| Step: 7
Training loss: 2.6892028559174666
Validation loss: 2.484103461886949

Epoch: 6| Step: 8
Training loss: 2.0033660219966847
Validation loss: 2.4817389012164095

Epoch: 6| Step: 9
Training loss: 2.104015861117841
Validation loss: 2.4759618951456783

Epoch: 6| Step: 10
Training loss: 2.8331194030181313
Validation loss: 2.466158540901763

Epoch: 6| Step: 11
Training loss: 3.1079042876638425
Validation loss: 2.4861017300400956

Epoch: 6| Step: 12
Training loss: 1.9195787932392823
Validation loss: 2.4807603559522935

Epoch: 6| Step: 13
Training loss: 2.137252043680804
Validation loss: 2.4855534379732473

Epoch: 195| Step: 0
Training loss: 2.890159940682704
Validation loss: 2.4947463305554303

Epoch: 6| Step: 1
Training loss: 1.9446863599182642
Validation loss: 2.497020996943485

Epoch: 6| Step: 2
Training loss: 2.006793167883664
Validation loss: 2.4974845470043316

Epoch: 6| Step: 3
Training loss: 2.0497364107331233
Validation loss: 2.4995655318072107

Epoch: 6| Step: 4
Training loss: 2.0204416137883774
Validation loss: 2.4994915922099437

Epoch: 6| Step: 5
Training loss: 2.717550078687747
Validation loss: 2.509327400234288

Epoch: 6| Step: 6
Training loss: 2.360047503494368
Validation loss: 2.4875336406739708

Epoch: 6| Step: 7
Training loss: 2.217567021024802
Validation loss: 2.478022244452881

Epoch: 6| Step: 8
Training loss: 2.3819446421611716
Validation loss: 2.486526996722965

Epoch: 6| Step: 9
Training loss: 2.6914255614895892
Validation loss: 2.4738778325418784

Epoch: 6| Step: 10
Training loss: 2.7350414226958804
Validation loss: 2.484156001133264

Epoch: 6| Step: 11
Training loss: 2.6566756861049785
Validation loss: 2.4832849090343756

Epoch: 6| Step: 12
Training loss: 2.5748852343122306
Validation loss: 2.488217123933005

Epoch: 6| Step: 13
Training loss: 2.3393951648329216
Validation loss: 2.4888354871428864

Epoch: 196| Step: 0
Training loss: 2.256874391169586
Validation loss: 2.488221843016309

Epoch: 6| Step: 1
Training loss: 2.2689196604614854
Validation loss: 2.487613207031766

Epoch: 6| Step: 2
Training loss: 1.7651401630542123
Validation loss: 2.4880484447633404

Epoch: 6| Step: 3
Training loss: 2.007608959998707
Validation loss: 2.480776822240228

Epoch: 6| Step: 4
Training loss: 2.9380158316417457
Validation loss: 2.4776221734906123

Epoch: 6| Step: 5
Training loss: 2.8473069299849003
Validation loss: 2.496171173025243

Epoch: 6| Step: 6
Training loss: 1.8281716886899975
Validation loss: 2.500959895549005

Epoch: 6| Step: 7
Training loss: 2.271591958067359
Validation loss: 2.5303295330001814

Epoch: 6| Step: 8
Training loss: 2.4706469623680944
Validation loss: 2.527560146683948

Epoch: 6| Step: 9
Training loss: 2.9679419773116313
Validation loss: 2.5136913347219676

Epoch: 6| Step: 10
Training loss: 2.059000919617887
Validation loss: 2.5035262035158876

Epoch: 6| Step: 11
Training loss: 2.6038426210335914
Validation loss: 2.4787996298239126

Epoch: 6| Step: 12
Training loss: 1.8941836323323051
Validation loss: 2.482547335652001

Epoch: 6| Step: 13
Training loss: 2.8575844082918525
Validation loss: 2.4944709831880187

Epoch: 197| Step: 0
Training loss: 1.9514195434889683
Validation loss: 2.4986439842034605

Epoch: 6| Step: 1
Training loss: 2.5040823983111533
Validation loss: 2.4960294505291265

Epoch: 6| Step: 2
Training loss: 2.152326026469677
Validation loss: 2.498016047045168

Epoch: 6| Step: 3
Training loss: 2.548666669036176
Validation loss: 2.4926535111894905

Epoch: 6| Step: 4
Training loss: 3.3300028056914734
Validation loss: 2.489950381771488

Epoch: 6| Step: 5
Training loss: 2.4405957639068974
Validation loss: 2.493618728573462

Epoch: 6| Step: 6
Training loss: 1.9615922419912686
Validation loss: 2.5012153373475776

Epoch: 6| Step: 7
Training loss: 2.3690231809886892
Validation loss: 2.4877919784508986

Epoch: 6| Step: 8
Training loss: 2.551020892863325
Validation loss: 2.488536077093652

Epoch: 6| Step: 9
Training loss: 2.630249043266065
Validation loss: 2.493157464095998

Epoch: 6| Step: 10
Training loss: 2.7229046701421415
Validation loss: 2.4917636937500975

Epoch: 6| Step: 11
Training loss: 1.8976952000307765
Validation loss: 2.4898103958926505

Epoch: 6| Step: 12
Training loss: 2.2324853122138313
Validation loss: 2.4933782142621883

Epoch: 6| Step: 13
Training loss: 2.6675529100056186
Validation loss: 2.489804889817377

Epoch: 198| Step: 0
Training loss: 2.6557022427524974
Validation loss: 2.4951981204767906

Epoch: 6| Step: 1
Training loss: 3.0375459706787677
Validation loss: 2.490966832767476

Epoch: 6| Step: 2
Training loss: 2.507930669644577
Validation loss: 2.481409370115021

Epoch: 6| Step: 3
Training loss: 2.5660834958651413
Validation loss: 2.488483414753928

Epoch: 6| Step: 4
Training loss: 1.5106038558547108
Validation loss: 2.4897451999879907

Epoch: 6| Step: 5
Training loss: 1.9873423579814196
Validation loss: 2.493060263118945

Epoch: 6| Step: 6
Training loss: 1.9880403923636238
Validation loss: 2.4853696131722125

Epoch: 6| Step: 7
Training loss: 2.679202911780419
Validation loss: 2.490250172284452

Epoch: 6| Step: 8
Training loss: 2.0739414870304262
Validation loss: 2.491924044755748

Epoch: 6| Step: 9
Training loss: 1.9639011799333126
Validation loss: 2.4849945991145286

Epoch: 6| Step: 10
Training loss: 2.3131767519892645
Validation loss: 2.5025807173494043

Epoch: 6| Step: 11
Training loss: 2.5761270815618635
Validation loss: 2.518250668791237

Epoch: 6| Step: 12
Training loss: 2.566568819468108
Validation loss: 2.5084800425954623

Epoch: 6| Step: 13
Training loss: 2.5545314542842754
Validation loss: 2.5011574450006155

Epoch: 199| Step: 0
Training loss: 2.050826822410312
Validation loss: 2.508800814927627

Epoch: 6| Step: 1
Training loss: 2.2577185165749625
Validation loss: 2.488527326727506

Epoch: 6| Step: 2
Training loss: 2.5737117100109637
Validation loss: 2.483097203653852

Epoch: 6| Step: 3
Training loss: 2.832304000874401
Validation loss: 2.488697210863614

Epoch: 6| Step: 4
Training loss: 2.1942142440944132
Validation loss: 2.484033165212729

Epoch: 6| Step: 5
Training loss: 2.198550622519917
Validation loss: 2.502062868346396

Epoch: 6| Step: 6
Training loss: 2.1493898812389216
Validation loss: 2.4876478058625944

Epoch: 6| Step: 7
Training loss: 2.2386620215973063
Validation loss: 2.490526688317424

Epoch: 6| Step: 8
Training loss: 2.750235287397673
Validation loss: 2.483629750689529

Epoch: 6| Step: 9
Training loss: 2.2426547849516547
Validation loss: 2.4884656980492084

Epoch: 6| Step: 10
Training loss: 2.150211004504349
Validation loss: 2.4886368158629564

Epoch: 6| Step: 11
Training loss: 2.4211108571150115
Validation loss: 2.478346926872703

Epoch: 6| Step: 12
Training loss: 2.182670712081401
Validation loss: 2.4846192965564153

Epoch: 6| Step: 13
Training loss: 2.7524063679277058
Validation loss: 2.4793735759741597

Epoch: 200| Step: 0
Training loss: 2.990284924917681
Validation loss: 2.490916614699355

Epoch: 6| Step: 1
Training loss: 2.4095295936871173
Validation loss: 2.4833914611294396

Epoch: 6| Step: 2
Training loss: 2.5302656166959894
Validation loss: 2.498013820036792

Epoch: 6| Step: 3
Training loss: 2.3463049631890662
Validation loss: 2.4897868074613294

Epoch: 6| Step: 4
Training loss: 2.283726432357142
Validation loss: 2.4938636970574266

Epoch: 6| Step: 5
Training loss: 2.1176031063698355
Validation loss: 2.4956424088177

Epoch: 6| Step: 6
Training loss: 2.326470113320046
Validation loss: 2.494055706461503

Epoch: 6| Step: 7
Training loss: 2.700833622846987
Validation loss: 2.500399605285413

Epoch: 6| Step: 8
Training loss: 1.8125028939059602
Validation loss: 2.4899730191179716

Epoch: 6| Step: 9
Training loss: 2.460108832456259
Validation loss: 2.503772035712232

Epoch: 6| Step: 10
Training loss: 2.80468962517873
Validation loss: 2.4989763945901715

Epoch: 6| Step: 11
Training loss: 2.3044142738929203
Validation loss: 2.5078014718166783

Epoch: 6| Step: 12
Training loss: 1.9599717638356322
Validation loss: 2.498244233297457

Epoch: 6| Step: 13
Training loss: 1.4678284209023267
Validation loss: 2.5070964706082615

Epoch: 201| Step: 0
Training loss: 1.7547837676818132
Validation loss: 2.493096404123209

Epoch: 6| Step: 1
Training loss: 1.6873931850959925
Validation loss: 2.502672690023383

Epoch: 6| Step: 2
Training loss: 1.9954387388869232
Validation loss: 2.4925217359507643

Epoch: 6| Step: 3
Training loss: 2.092729462105052
Validation loss: 2.4931888462044625

Epoch: 6| Step: 4
Training loss: 2.6060420696642375
Validation loss: 2.4962691124579344

Epoch: 6| Step: 5
Training loss: 2.2696089870776492
Validation loss: 2.485583781027777

Epoch: 6| Step: 6
Training loss: 2.2328625875611827
Validation loss: 2.5128341579116684

Epoch: 6| Step: 7
Training loss: 2.1837854727717345
Validation loss: 2.5064557327976984

Epoch: 6| Step: 8
Training loss: 2.6921080185132813
Validation loss: 2.520081365888964

Epoch: 6| Step: 9
Training loss: 2.876194581347964
Validation loss: 2.5347601802079835

Epoch: 6| Step: 10
Training loss: 3.0677017876771653
Validation loss: 2.5045166223865043

Epoch: 6| Step: 11
Training loss: 2.233623244793673
Validation loss: 2.4934241436504454

Epoch: 6| Step: 12
Training loss: 2.5904613006004253
Validation loss: 2.5069232603982106

Epoch: 6| Step: 13
Training loss: 2.474607255388299
Validation loss: 2.5194061090505575

Epoch: 202| Step: 0
Training loss: 3.3992711239608906
Validation loss: 2.487774384603571

Epoch: 6| Step: 1
Training loss: 2.334531635304562
Validation loss: 2.499044156134602

Epoch: 6| Step: 2
Training loss: 2.375348918782652
Validation loss: 2.4890986865985827

Epoch: 6| Step: 3
Training loss: 2.4259678865588863
Validation loss: 2.4962706406169293

Epoch: 6| Step: 4
Training loss: 1.783065807324085
Validation loss: 2.490515535725972

Epoch: 6| Step: 5
Training loss: 2.2490929788695038
Validation loss: 2.4943996007221223

Epoch: 6| Step: 6
Training loss: 2.1001614009049576
Validation loss: 2.4966275037979817

Epoch: 6| Step: 7
Training loss: 1.9928506981471519
Validation loss: 2.4962696536810185

Epoch: 6| Step: 8
Training loss: 1.793901985986734
Validation loss: 2.489248585774305

Epoch: 6| Step: 9
Training loss: 2.00652643123493
Validation loss: 2.5011928970730004

Epoch: 6| Step: 10
Training loss: 2.891481674682448
Validation loss: 2.4955444527510493

Epoch: 6| Step: 11
Training loss: 2.4318818122103654
Validation loss: 2.5033079355918106

Epoch: 6| Step: 12
Training loss: 2.5528781094468878
Validation loss: 2.498334202510724

Epoch: 6| Step: 13
Training loss: 2.1900953972301083
Validation loss: 2.5017077017647713

Epoch: 203| Step: 0
Training loss: 2.476838394345029
Validation loss: 2.5093745262110714

Epoch: 6| Step: 1
Training loss: 2.7686053236110135
Validation loss: 2.5123766822980755

Epoch: 6| Step: 2
Training loss: 1.7723509026623279
Validation loss: 2.515773176485084

Epoch: 6| Step: 3
Training loss: 2.057986320042348
Validation loss: 2.5097156247552643

Epoch: 6| Step: 4
Training loss: 1.7463059264438492
Validation loss: 2.4988927296439876

Epoch: 6| Step: 5
Training loss: 3.0257538983098424
Validation loss: 2.511938794406338

Epoch: 6| Step: 6
Training loss: 1.9652694332034013
Validation loss: 2.499668568258363

Epoch: 6| Step: 7
Training loss: 2.2144919356550057
Validation loss: 2.4959466977449676

Epoch: 6| Step: 8
Training loss: 2.06565061429254
Validation loss: 2.5015755218795963

Epoch: 6| Step: 9
Training loss: 2.866092982293892
Validation loss: 2.491540870765598

Epoch: 6| Step: 10
Training loss: 2.1743414616335137
Validation loss: 2.499754901633328

Epoch: 6| Step: 11
Training loss: 3.0868456313100734
Validation loss: 2.50712587140956

Epoch: 6| Step: 12
Training loss: 2.3314607008555366
Validation loss: 2.500038242047597

Epoch: 6| Step: 13
Training loss: 1.9996629073259273
Validation loss: 2.5089151210453466

Epoch: 204| Step: 0
Training loss: 1.9166392517893485
Validation loss: 2.481690097601726

Epoch: 6| Step: 1
Training loss: 1.8094770280866623
Validation loss: 2.5090423452169874

Epoch: 6| Step: 2
Training loss: 1.951282272805104
Validation loss: 2.499174744772207

Epoch: 6| Step: 3
Training loss: 3.4578115843346335
Validation loss: 2.501514993026283

Epoch: 6| Step: 4
Training loss: 2.6295371127631344
Validation loss: 2.4900144312847643

Epoch: 6| Step: 5
Training loss: 1.825865302642436
Validation loss: 2.504623032769665

Epoch: 6| Step: 6
Training loss: 2.7993068245764197
Validation loss: 2.491066548045961

Epoch: 6| Step: 7
Training loss: 2.4890113609793403
Validation loss: 2.544455449653362

Epoch: 6| Step: 8
Training loss: 2.2661280632564367
Validation loss: 2.526589408295572

Epoch: 6| Step: 9
Training loss: 2.3623184134315993
Validation loss: 2.551361827975636

Epoch: 6| Step: 10
Training loss: 2.4532137326271513
Validation loss: 2.5400062098389657

Epoch: 6| Step: 11
Training loss: 2.6760479202484517
Validation loss: 2.5285342054677886

Epoch: 6| Step: 12
Training loss: 2.5440439040396203
Validation loss: 2.5058214437974202

Epoch: 6| Step: 13
Training loss: 1.5544947979221313
Validation loss: 2.4812834195337463

Epoch: 205| Step: 0
Training loss: 2.2593741642487433
Validation loss: 2.486094537486757

Epoch: 6| Step: 1
Training loss: 2.6802102516030066
Validation loss: 2.493206537315472

Epoch: 6| Step: 2
Training loss: 2.4868524063054225
Validation loss: 2.49804116438073

Epoch: 6| Step: 3
Training loss: 1.8949062959356557
Validation loss: 2.4972068123724473

Epoch: 6| Step: 4
Training loss: 2.478469643793831
Validation loss: 2.5026866225764786

Epoch: 6| Step: 5
Training loss: 2.154428569679911
Validation loss: 2.5010619610068052

Epoch: 6| Step: 6
Training loss: 1.9150743573645461
Validation loss: 2.500446724715498

Epoch: 6| Step: 7
Training loss: 1.835044741176062
Validation loss: 2.5009796448410166

Epoch: 6| Step: 8
Training loss: 1.9778846269252837
Validation loss: 2.491554722030813

Epoch: 6| Step: 9
Training loss: 3.0295460424505247
Validation loss: 2.502106319663882

Epoch: 6| Step: 10
Training loss: 3.037651931039893
Validation loss: 2.499099720343054

Epoch: 6| Step: 11
Training loss: 1.7048868288309034
Validation loss: 2.4872067385636942

Epoch: 6| Step: 12
Training loss: 1.9224953619376441
Validation loss: 2.4947390992128775

Epoch: 6| Step: 13
Training loss: 3.0829006269477395
Validation loss: 2.488532883532931

Epoch: 206| Step: 0
Training loss: 2.6925916123809888
Validation loss: 2.5015145164778665

Epoch: 6| Step: 1
Training loss: 2.1179092134320423
Validation loss: 2.499281112307822

Epoch: 6| Step: 2
Training loss: 2.814905960111282
Validation loss: 2.5360682433610364

Epoch: 6| Step: 3
Training loss: 2.500582245735578
Validation loss: 2.540946160006416

Epoch: 6| Step: 4
Training loss: 2.530233014047757
Validation loss: 2.552364961823827

Epoch: 6| Step: 5
Training loss: 2.577123088936061
Validation loss: 2.5566752066919305

Epoch: 6| Step: 6
Training loss: 1.8519956830470812
Validation loss: 2.5126260529713558

Epoch: 6| Step: 7
Training loss: 2.2232357402735845
Validation loss: 2.4968692329095052

Epoch: 6| Step: 8
Training loss: 2.524001303598397
Validation loss: 2.49040158177922

Epoch: 6| Step: 9
Training loss: 1.9369387890582896
Validation loss: 2.4765986491071086

Epoch: 6| Step: 10
Training loss: 2.2644932518115395
Validation loss: 2.486833483571622

Epoch: 6| Step: 11
Training loss: 2.262537885317087
Validation loss: 2.470486814657114

Epoch: 6| Step: 12
Training loss: 2.1183889435862797
Validation loss: 2.4650631024612752

Epoch: 6| Step: 13
Training loss: 2.45710101733594
Validation loss: 2.4846950140962467

Epoch: 207| Step: 0
Training loss: 2.1626952772227486
Validation loss: 2.474212108780211

Epoch: 6| Step: 1
Training loss: 2.236844065756869
Validation loss: 2.4670228268196497

Epoch: 6| Step: 2
Training loss: 2.3499106248133343
Validation loss: 2.4736591332189

Epoch: 6| Step: 3
Training loss: 2.1230252009516355
Validation loss: 2.4762953043501326

Epoch: 6| Step: 4
Training loss: 3.0431345354590094
Validation loss: 2.4617886559881232

Epoch: 6| Step: 5
Training loss: 2.3497164250797997
Validation loss: 2.4778441793965134

Epoch: 6| Step: 6
Training loss: 2.5968703837990614
Validation loss: 2.4740156200535686

Epoch: 6| Step: 7
Training loss: 2.134156861894044
Validation loss: 2.486283598823531

Epoch: 6| Step: 8
Training loss: 2.1979213431310094
Validation loss: 2.4722239039507037

Epoch: 6| Step: 9
Training loss: 2.2153007668232756
Validation loss: 2.4782485116562727

Epoch: 6| Step: 10
Training loss: 2.042398581070487
Validation loss: 2.484026550552406

Epoch: 6| Step: 11
Training loss: 2.066575849239902
Validation loss: 2.471892196138917

Epoch: 6| Step: 12
Training loss: 3.0097892309769443
Validation loss: 2.4664778897072717

Epoch: 6| Step: 13
Training loss: 2.3566484675947947
Validation loss: 2.48873670432989

Epoch: 208| Step: 0
Training loss: 2.552698136578091
Validation loss: 2.4831065812585176

Epoch: 6| Step: 1
Training loss: 2.591384912448425
Validation loss: 2.4834647920449

Epoch: 6| Step: 2
Training loss: 2.222528169022199
Validation loss: 2.474180397641437

Epoch: 6| Step: 3
Training loss: 2.6731119248590183
Validation loss: 2.494951714231284

Epoch: 6| Step: 4
Training loss: 2.400348709522666
Validation loss: 2.4766544200633795

Epoch: 6| Step: 5
Training loss: 2.297756103919244
Validation loss: 2.4874033035321

Epoch: 6| Step: 6
Training loss: 2.287010340578415
Validation loss: 2.471110860882128

Epoch: 6| Step: 7
Training loss: 2.22663402777105
Validation loss: 2.4718494434860117

Epoch: 6| Step: 8
Training loss: 1.8599444246705759
Validation loss: 2.481763975141295

Epoch: 6| Step: 9
Training loss: 2.284275922179162
Validation loss: 2.501047415662516

Epoch: 6| Step: 10
Training loss: 2.2712803205314827
Validation loss: 2.494249023376133

Epoch: 6| Step: 11
Training loss: 2.6317566698695902
Validation loss: 2.4933327940142993

Epoch: 6| Step: 12
Training loss: 2.099672509951933
Validation loss: 2.4785030716382006

Epoch: 6| Step: 13
Training loss: 2.707136231956039
Validation loss: 2.461847030249629

Epoch: 209| Step: 0
Training loss: 1.8100255979129405
Validation loss: 2.4663455053837606

Epoch: 6| Step: 1
Training loss: 2.988134442880787
Validation loss: 2.468382055491606

Epoch: 6| Step: 2
Training loss: 2.2936102052752285
Validation loss: 2.4619793498022564

Epoch: 6| Step: 3
Training loss: 2.993782275828267
Validation loss: 2.477572615169104

Epoch: 6| Step: 4
Training loss: 2.7701246650819193
Validation loss: 2.489966763347347

Epoch: 6| Step: 5
Training loss: 2.4221575172290186
Validation loss: 2.485491631638503

Epoch: 6| Step: 6
Training loss: 2.580974229763042
Validation loss: 2.486581554217586

Epoch: 6| Step: 7
Training loss: 2.5031358125717427
Validation loss: 2.488338994783918

Epoch: 6| Step: 8
Training loss: 2.254070837930172
Validation loss: 2.4756785679528526

Epoch: 6| Step: 9
Training loss: 1.8510084007192082
Validation loss: 2.4772508468860033

Epoch: 6| Step: 10
Training loss: 2.1586148106710445
Validation loss: 2.4584176135298357

Epoch: 6| Step: 11
Training loss: 1.5667911351587862
Validation loss: 2.476472165037624

Epoch: 6| Step: 12
Training loss: 2.3229154122068385
Validation loss: 2.5175834914160866

Epoch: 6| Step: 13
Training loss: 2.269015071295982
Validation loss: 2.5100455398911743

Epoch: 210| Step: 0
Training loss: 2.2039488205826285
Validation loss: 2.5026842171326806

Epoch: 6| Step: 1
Training loss: 2.074104377768667
Validation loss: 2.506640024508598

Epoch: 6| Step: 2
Training loss: 1.83637047389961
Validation loss: 2.4832636748748698

Epoch: 6| Step: 3
Training loss: 2.2918136780719234
Validation loss: 2.4909768666883245

Epoch: 6| Step: 4
Training loss: 2.4129987922472567
Validation loss: 2.479902420072586

Epoch: 6| Step: 5
Training loss: 2.3149487701086473
Validation loss: 2.48572949606841

Epoch: 6| Step: 6
Training loss: 2.170250813628502
Validation loss: 2.4990889558182423

Epoch: 6| Step: 7
Training loss: 2.5888320074349602
Validation loss: 2.4818239369803616

Epoch: 6| Step: 8
Training loss: 2.4841784513446292
Validation loss: 2.500414861589325

Epoch: 6| Step: 9
Training loss: 2.7692485283013193
Validation loss: 2.490188386779198

Epoch: 6| Step: 10
Training loss: 3.2634667378304596
Validation loss: 2.4946655660999277

Epoch: 6| Step: 11
Training loss: 2.158334249940577
Validation loss: 2.491545447991292

Epoch: 6| Step: 12
Training loss: 2.804821383115228
Validation loss: 2.4946758638926356

Epoch: 6| Step: 13
Training loss: 1.6959576763459236
Validation loss: 2.4753447947534304

Epoch: 211| Step: 0
Training loss: 2.2401352904539835
Validation loss: 2.48482496557256

Epoch: 6| Step: 1
Training loss: 2.955640901880528
Validation loss: 2.4742751925821675

Epoch: 6| Step: 2
Training loss: 2.27131622033166
Validation loss: 2.49208907328637

Epoch: 6| Step: 3
Training loss: 2.0918568265072492
Validation loss: 2.5285278958056705

Epoch: 6| Step: 4
Training loss: 2.677462351843314
Validation loss: 2.522328608486986

Epoch: 6| Step: 5
Training loss: 2.626688822670584
Validation loss: 2.526779528473406

Epoch: 6| Step: 6
Training loss: 2.1120954137403842
Validation loss: 2.515887284278999

Epoch: 6| Step: 7
Training loss: 2.4724576126029265
Validation loss: 2.4911139397184927

Epoch: 6| Step: 8
Training loss: 2.6808479843553545
Validation loss: 2.49489007701313

Epoch: 6| Step: 9
Training loss: 2.3819039035185536
Validation loss: 2.4800120656683906

Epoch: 6| Step: 10
Training loss: 2.088399398663925
Validation loss: 2.499206242118144

Epoch: 6| Step: 11
Training loss: 2.115042750871172
Validation loss: 2.5015369697521326

Epoch: 6| Step: 12
Training loss: 2.4138478797889222
Validation loss: 2.4971889745639264

Epoch: 6| Step: 13
Training loss: 2.055736897462648
Validation loss: 2.4960967428540677

Epoch: 212| Step: 0
Training loss: 2.4085658435275175
Validation loss: 2.485781049810511

Epoch: 6| Step: 1
Training loss: 1.9669313053378892
Validation loss: 2.480106789920012

Epoch: 6| Step: 2
Training loss: 1.832342580926821
Validation loss: 2.481917127131482

Epoch: 6| Step: 3
Training loss: 1.6942493955293916
Validation loss: 2.4854511193581232

Epoch: 6| Step: 4
Training loss: 1.8017391545019714
Validation loss: 2.4703397903366575

Epoch: 6| Step: 5
Training loss: 2.4788103461987316
Validation loss: 2.475116423877316

Epoch: 6| Step: 6
Training loss: 2.5714458529330635
Validation loss: 2.4716448490657243

Epoch: 6| Step: 7
Training loss: 2.136391567586268
Validation loss: 2.464303789276536

Epoch: 6| Step: 8
Training loss: 2.7468892623305434
Validation loss: 2.4771579223217026

Epoch: 6| Step: 9
Training loss: 2.4733421977245036
Validation loss: 2.5161323351699667

Epoch: 6| Step: 10
Training loss: 2.1387693060712007
Validation loss: 2.5143031404818186

Epoch: 6| Step: 11
Training loss: 2.717455851907786
Validation loss: 2.517917370486525

Epoch: 6| Step: 12
Training loss: 2.6294936817569847
Validation loss: 2.491538015975977

Epoch: 6| Step: 13
Training loss: 2.6863344015758432
Validation loss: 2.49936437473708

Epoch: 213| Step: 0
Training loss: 2.6558069196172958
Validation loss: 2.4838731846741564

Epoch: 6| Step: 1
Training loss: 3.1072240702972596
Validation loss: 2.449963745835062

Epoch: 6| Step: 2
Training loss: 2.349236437341183
Validation loss: 2.469459431758562

Epoch: 6| Step: 3
Training loss: 2.47259280347856
Validation loss: 2.462444332545699

Epoch: 6| Step: 4
Training loss: 1.6116437013417273
Validation loss: 2.4664038926279352

Epoch: 6| Step: 5
Training loss: 2.7670473223721395
Validation loss: 2.465709971360628

Epoch: 6| Step: 6
Training loss: 2.1482152095584763
Validation loss: 2.4535155582736703

Epoch: 6| Step: 7
Training loss: 1.7146235087783757
Validation loss: 2.4718836119092256

Epoch: 6| Step: 8
Training loss: 1.8560950577416024
Validation loss: 2.4395184107306807

Epoch: 6| Step: 9
Training loss: 2.776002197748463
Validation loss: 2.45443090679704

Epoch: 6| Step: 10
Training loss: 2.0512235618621695
Validation loss: 2.4597621633985804

Epoch: 6| Step: 11
Training loss: 1.993570124724386
Validation loss: 2.4792645592689797

Epoch: 6| Step: 12
Training loss: 2.1369974627553385
Validation loss: 2.470627292278477

Epoch: 6| Step: 13
Training loss: 2.683364347511642
Validation loss: 2.4709479772293275

Epoch: 214| Step: 0
Training loss: 2.682563328488731
Validation loss: 2.4629697935694304

Epoch: 6| Step: 1
Training loss: 1.937575738718586
Validation loss: 2.4580627120036174

Epoch: 6| Step: 2
Training loss: 2.0985045467070234
Validation loss: 2.4567056914257455

Epoch: 6| Step: 3
Training loss: 2.632036949507084
Validation loss: 2.460458117423091

Epoch: 6| Step: 4
Training loss: 2.9188742502954947
Validation loss: 2.465556022153003

Epoch: 6| Step: 5
Training loss: 1.817631921191372
Validation loss: 2.477060975812581

Epoch: 6| Step: 6
Training loss: 2.3229195177092112
Validation loss: 2.4741309470474926

Epoch: 6| Step: 7
Training loss: 2.0367727467849472
Validation loss: 2.4745478653784025

Epoch: 6| Step: 8
Training loss: 1.7203014654070705
Validation loss: 2.48388524696282

Epoch: 6| Step: 9
Training loss: 2.608066436307528
Validation loss: 2.474267034176949

Epoch: 6| Step: 10
Training loss: 2.0845050822649203
Validation loss: 2.4798731611999947

Epoch: 6| Step: 11
Training loss: 2.4890657682331514
Validation loss: 2.4792867412393096

Epoch: 6| Step: 12
Training loss: 2.4323098115972575
Validation loss: 2.4723616148790155

Epoch: 6| Step: 13
Training loss: 2.4578564930329514
Validation loss: 2.4792708099887943

Epoch: 215| Step: 0
Training loss: 1.613660716948835
Validation loss: 2.4805612782118875

Epoch: 6| Step: 1
Training loss: 2.6114534791633237
Validation loss: 2.4895496818364378

Epoch: 6| Step: 2
Training loss: 2.427184554405581
Validation loss: 2.4787649635970053

Epoch: 6| Step: 3
Training loss: 2.277259471766823
Validation loss: 2.474568130562937

Epoch: 6| Step: 4
Training loss: 2.1691655884880623
Validation loss: 2.478071232549557

Epoch: 6| Step: 5
Training loss: 3.019761800908892
Validation loss: 2.4774624766700923

Epoch: 6| Step: 6
Training loss: 1.939650541862719
Validation loss: 2.4770184888338354

Epoch: 6| Step: 7
Training loss: 2.591310755914045
Validation loss: 2.4902085087249355

Epoch: 6| Step: 8
Training loss: 2.038578019738223
Validation loss: 2.4896887247503834

Epoch: 6| Step: 9
Training loss: 2.684923045442593
Validation loss: 2.4929429267606626

Epoch: 6| Step: 10
Training loss: 2.1944606662370645
Validation loss: 2.4757399451551003

Epoch: 6| Step: 11
Training loss: 2.1255467777278114
Validation loss: 2.4829484694684107

Epoch: 6| Step: 12
Training loss: 2.7165971321927174
Validation loss: 2.4838171278584387

Epoch: 6| Step: 13
Training loss: 1.5900404150042997
Validation loss: 2.4997301591839314

Epoch: 216| Step: 0
Training loss: 2.5879561275613567
Validation loss: 2.510198153618857

Epoch: 6| Step: 1
Training loss: 1.9148338787644634
Validation loss: 2.520721769359996

Epoch: 6| Step: 2
Training loss: 2.342846912600947
Validation loss: 2.5109107822594323

Epoch: 6| Step: 3
Training loss: 2.2539138661591425
Validation loss: 2.5148063022479543

Epoch: 6| Step: 4
Training loss: 2.263336920456813
Validation loss: 2.4920727614694247

Epoch: 6| Step: 5
Training loss: 2.4053303335866105
Validation loss: 2.481455481001852

Epoch: 6| Step: 6
Training loss: 2.805677743859133
Validation loss: 2.4974129483641225

Epoch: 6| Step: 7
Training loss: 2.7229107993621384
Validation loss: 2.483259178394287

Epoch: 6| Step: 8
Training loss: 1.9316783936051054
Validation loss: 2.4936955273127586

Epoch: 6| Step: 9
Training loss: 1.8085445784605512
Validation loss: 2.500636417127989

Epoch: 6| Step: 10
Training loss: 2.504084492973274
Validation loss: 2.5051914989093516

Epoch: 6| Step: 11
Training loss: 2.7242759582621465
Validation loss: 2.495834265460911

Epoch: 6| Step: 12
Training loss: 2.319120364159276
Validation loss: 2.489455029574522

Epoch: 6| Step: 13
Training loss: 2.1046197485267193
Validation loss: 2.5004419333536347

Epoch: 217| Step: 0
Training loss: 2.1405857945771087
Validation loss: 2.4924069171488528

Epoch: 6| Step: 1
Training loss: 2.605445598630978
Validation loss: 2.494903726523199

Epoch: 6| Step: 2
Training loss: 2.304243038445513
Validation loss: 2.4899094074115826

Epoch: 6| Step: 3
Training loss: 2.42890304216754
Validation loss: 2.513120395618551

Epoch: 6| Step: 4
Training loss: 2.556442544489582
Validation loss: 2.5204473835813954

Epoch: 6| Step: 5
Training loss: 2.7518349507762045
Validation loss: 2.500298911344879

Epoch: 6| Step: 6
Training loss: 2.557405385106548
Validation loss: 2.494548496402165

Epoch: 6| Step: 7
Training loss: 2.590203491818417
Validation loss: 2.492319021870178

Epoch: 6| Step: 8
Training loss: 2.222565714462126
Validation loss: 2.495179448156567

Epoch: 6| Step: 9
Training loss: 1.9568909006560158
Validation loss: 2.482757297587986

Epoch: 6| Step: 10
Training loss: 1.9031540791712482
Validation loss: 2.4759890737847132

Epoch: 6| Step: 11
Training loss: 2.053728002893612
Validation loss: 2.5051571582370697

Epoch: 6| Step: 12
Training loss: 2.0107917501632073
Validation loss: 2.4786492670661584

Epoch: 6| Step: 13
Training loss: 2.327993504439793
Validation loss: 2.4996636641279557

Epoch: 218| Step: 0
Training loss: 1.8767477790531764
Validation loss: 2.5050392860484236

Epoch: 6| Step: 1
Training loss: 2.4391591220952336
Validation loss: 2.4962180538995455

Epoch: 6| Step: 2
Training loss: 2.2918453031331874
Validation loss: 2.4981480573732657

Epoch: 6| Step: 3
Training loss: 2.2481247186647333
Validation loss: 2.4959847073723314

Epoch: 6| Step: 4
Training loss: 2.3654129880690977
Validation loss: 2.509792144674861

Epoch: 6| Step: 5
Training loss: 1.8883202747231254
Validation loss: 2.496262681445245

Epoch: 6| Step: 6
Training loss: 2.817416154211291
Validation loss: 2.4958158765213128

Epoch: 6| Step: 7
Training loss: 2.9588167753690118
Validation loss: 2.5088368162938828

Epoch: 6| Step: 8
Training loss: 1.8132596890253643
Validation loss: 2.501358021010218

Epoch: 6| Step: 9
Training loss: 2.151250378056193
Validation loss: 2.482207122165514

Epoch: 6| Step: 10
Training loss: 2.2305215538709136
Validation loss: 2.5219157761212334

Epoch: 6| Step: 11
Training loss: 2.6453765915453684
Validation loss: 2.499406775981392

Epoch: 6| Step: 12
Training loss: 1.9914166324145788
Validation loss: 2.506919503785043

Epoch: 6| Step: 13
Training loss: 2.6537910299413032
Validation loss: 2.504456926196911

Epoch: 219| Step: 0
Training loss: 2.176755402855889
Validation loss: 2.508951873069892

Epoch: 6| Step: 1
Training loss: 2.5305886994234
Validation loss: 2.522941186241326

Epoch: 6| Step: 2
Training loss: 2.1044421125858057
Validation loss: 2.5169766824607387

Epoch: 6| Step: 3
Training loss: 2.0505809286463137
Validation loss: 2.5181980755743045

Epoch: 6| Step: 4
Training loss: 2.272593443138079
Validation loss: 2.521320316996565

Epoch: 6| Step: 5
Training loss: 2.2851973570688364
Validation loss: 2.5251313971647464

Epoch: 6| Step: 6
Training loss: 2.301009006593693
Validation loss: 2.484217128895893

Epoch: 6| Step: 7
Training loss: 2.889249237115899
Validation loss: 2.4967574549042593

Epoch: 6| Step: 8
Training loss: 2.0867902177221267
Validation loss: 2.4956857649324315

Epoch: 6| Step: 9
Training loss: 2.107137666192484
Validation loss: 2.4968199372536217

Epoch: 6| Step: 10
Training loss: 2.3720693073922323
Validation loss: 2.495622537676823

Epoch: 6| Step: 11
Training loss: 2.4451875989904317
Validation loss: 2.503978202527918

Epoch: 6| Step: 12
Training loss: 2.9614833841909136
Validation loss: 2.502164364745878

Epoch: 6| Step: 13
Training loss: 1.7543104763688664
Validation loss: 2.506434964452148

Epoch: 220| Step: 0
Training loss: 1.9014152125017965
Validation loss: 2.5112784134665826

Epoch: 6| Step: 1
Training loss: 1.7880017996742699
Validation loss: 2.502467257234362

Epoch: 6| Step: 2
Training loss: 2.849072181523702
Validation loss: 2.5001014688880785

Epoch: 6| Step: 3
Training loss: 2.504005180225559
Validation loss: 2.4912562847807846

Epoch: 6| Step: 4
Training loss: 2.0442112486754946
Validation loss: 2.4948880861215232

Epoch: 6| Step: 5
Training loss: 2.5834746373406157
Validation loss: 2.5084021361368127

Epoch: 6| Step: 6
Training loss: 2.5663855332651764
Validation loss: 2.517588494799605

Epoch: 6| Step: 7
Training loss: 2.534552409668894
Validation loss: 2.5281749461065246

Epoch: 6| Step: 8
Training loss: 2.6470266539669605
Validation loss: 2.5566767142876348

Epoch: 6| Step: 9
Training loss: 2.682001743882172
Validation loss: 2.547997521786006

Epoch: 6| Step: 10
Training loss: 2.17577883927436
Validation loss: 2.539652484915119

Epoch: 6| Step: 11
Training loss: 2.1100563361796305
Validation loss: 2.526334581728735

Epoch: 6| Step: 12
Training loss: 2.748537541705418
Validation loss: 2.5113812146133276

Epoch: 6| Step: 13
Training loss: 1.4949007780900472
Validation loss: 2.5134681158664685

Epoch: 221| Step: 0
Training loss: 2.381484665409032
Validation loss: 2.499251953107956

Epoch: 6| Step: 1
Training loss: 1.7547824090034634
Validation loss: 2.5035738196166246

Epoch: 6| Step: 2
Training loss: 3.020806954805806
Validation loss: 2.5074124081463576

Epoch: 6| Step: 3
Training loss: 2.274262059178396
Validation loss: 2.5053853681563245

Epoch: 6| Step: 4
Training loss: 2.536520004828229
Validation loss: 2.513862719203493

Epoch: 6| Step: 5
Training loss: 2.5518760960315197
Validation loss: 2.5128716511827647

Epoch: 6| Step: 6
Training loss: 2.0952278728875653
Validation loss: 2.5142436055351207

Epoch: 6| Step: 7
Training loss: 2.8040084654425854
Validation loss: 2.500498864945381

Epoch: 6| Step: 8
Training loss: 2.4012758440079542
Validation loss: 2.5098683139558653

Epoch: 6| Step: 9
Training loss: 2.4775144269813807
Validation loss: 2.5246935888268056

Epoch: 6| Step: 10
Training loss: 2.2609967543634792
Validation loss: 2.5279660998980753

Epoch: 6| Step: 11
Training loss: 1.897371345953285
Validation loss: 2.553757882400923

Epoch: 6| Step: 12
Training loss: 1.846753276809134
Validation loss: 2.5643358787275394

Epoch: 6| Step: 13
Training loss: 2.623707316770666
Validation loss: 2.531294661390856

Epoch: 222| Step: 0
Training loss: 2.0090333069641257
Validation loss: 2.5393266119897646

Epoch: 6| Step: 1
Training loss: 2.1595648012783526
Validation loss: 2.531268837941091

Epoch: 6| Step: 2
Training loss: 2.3384417630770664
Validation loss: 2.5227679455805965

Epoch: 6| Step: 3
Training loss: 2.3546202793664337
Validation loss: 2.5138745111390337

Epoch: 6| Step: 4
Training loss: 2.415204022084312
Validation loss: 2.5129925082229523

Epoch: 6| Step: 5
Training loss: 2.4038035849690096
Validation loss: 2.5245188629217328

Epoch: 6| Step: 6
Training loss: 2.6257100734353727
Validation loss: 2.493017706115884

Epoch: 6| Step: 7
Training loss: 2.0741639210841316
Validation loss: 2.490058523713397

Epoch: 6| Step: 8
Training loss: 2.4624099443883045
Validation loss: 2.4903662632941153

Epoch: 6| Step: 9
Training loss: 2.5686610519713478
Validation loss: 2.4760900903272214

Epoch: 6| Step: 10
Training loss: 2.884081057431882
Validation loss: 2.4859690160889505

Epoch: 6| Step: 11
Training loss: 2.376387542163381
Validation loss: 2.4920120895025137

Epoch: 6| Step: 12
Training loss: 2.4799861889116204
Validation loss: 2.5025040007415407

Epoch: 6| Step: 13
Training loss: 1.5273509305897384
Validation loss: 2.481633719265488

Epoch: 223| Step: 0
Training loss: 1.91512620913391
Validation loss: 2.4828515970181857

Epoch: 6| Step: 1
Training loss: 1.3766558387105763
Validation loss: 2.4848313782124962

Epoch: 6| Step: 2
Training loss: 2.5257777649713526
Validation loss: 2.4983225280887726

Epoch: 6| Step: 3
Training loss: 2.5833300211075088
Validation loss: 2.491724415708464

Epoch: 6| Step: 4
Training loss: 2.713488694819203
Validation loss: 2.4875364840849525

Epoch: 6| Step: 5
Training loss: 2.151744059740077
Validation loss: 2.493023324638249

Epoch: 6| Step: 6
Training loss: 2.5894167448818113
Validation loss: 2.480629727003547

Epoch: 6| Step: 7
Training loss: 2.2113946772245408
Validation loss: 2.481826610812341

Epoch: 6| Step: 8
Training loss: 1.7781812769560479
Validation loss: 2.476016091640074

Epoch: 6| Step: 9
Training loss: 2.8938732145701955
Validation loss: 2.4723791496485945

Epoch: 6| Step: 10
Training loss: 2.5631258130578045
Validation loss: 2.4664630518033412

Epoch: 6| Step: 11
Training loss: 2.212687308793396
Validation loss: 2.4773497749126228

Epoch: 6| Step: 12
Training loss: 1.8277818406641881
Validation loss: 2.4834245827144

Epoch: 6| Step: 13
Training loss: 2.539637855484856
Validation loss: 2.4791922260751345

Epoch: 224| Step: 0
Training loss: 1.935838910194845
Validation loss: 2.4904267679330543

Epoch: 6| Step: 1
Training loss: 2.4017221034170952
Validation loss: 2.5017701875470566

Epoch: 6| Step: 2
Training loss: 2.1720166057282073
Validation loss: 2.4943101188564634

Epoch: 6| Step: 3
Training loss: 1.7648292030927148
Validation loss: 2.4946833741663124

Epoch: 6| Step: 4
Training loss: 2.6867059821035943
Validation loss: 2.49214196238275

Epoch: 6| Step: 5
Training loss: 2.5305295319189742
Validation loss: 2.498944377555428

Epoch: 6| Step: 6
Training loss: 2.260179170241111
Validation loss: 2.519525911389592

Epoch: 6| Step: 7
Training loss: 2.818856664022395
Validation loss: 2.5041510689916637

Epoch: 6| Step: 8
Training loss: 2.463091195490879
Validation loss: 2.5021851208956885

Epoch: 6| Step: 9
Training loss: 2.3667358370419938
Validation loss: 2.4884722210687267

Epoch: 6| Step: 10
Training loss: 2.310123769243465
Validation loss: 2.4982066716038163

Epoch: 6| Step: 11
Training loss: 2.5302036147677596
Validation loss: 2.483038960894633

Epoch: 6| Step: 12
Training loss: 2.4749240208532832
Validation loss: 2.47785578993605

Epoch: 6| Step: 13
Training loss: 2.3292143660445426
Validation loss: 2.4803112951250688

Epoch: 225| Step: 0
Training loss: 2.7567226428522433
Validation loss: 2.461005897176994

Epoch: 6| Step: 1
Training loss: 2.3880879022851165
Validation loss: 2.490446185908809

Epoch: 6| Step: 2
Training loss: 2.244605272760752
Validation loss: 2.4688828991879195

Epoch: 6| Step: 3
Training loss: 1.6945798494959596
Validation loss: 2.4875742787783803

Epoch: 6| Step: 4
Training loss: 1.8694151036058961
Validation loss: 2.520700732334983

Epoch: 6| Step: 5
Training loss: 2.211978626782596
Validation loss: 2.5304717606136657

Epoch: 6| Step: 6
Training loss: 2.1540548235292842
Validation loss: 2.5243350579241493

Epoch: 6| Step: 7
Training loss: 2.1118547345690883
Validation loss: 2.5375959180921748

Epoch: 6| Step: 8
Training loss: 2.6005628343416136
Validation loss: 2.4999920606487094

Epoch: 6| Step: 9
Training loss: 2.5583744799590638
Validation loss: 2.493201453126153

Epoch: 6| Step: 10
Training loss: 2.5680396515586628
Validation loss: 2.4608553595180993

Epoch: 6| Step: 11
Training loss: 2.5201839104406343
Validation loss: 2.472839493311935

Epoch: 6| Step: 12
Training loss: 1.9858443459693553
Validation loss: 2.490334470644564

Epoch: 6| Step: 13
Training loss: 2.8146586293488713
Validation loss: 2.493827527338131

Epoch: 226| Step: 0
Training loss: 2.0506468519949212
Validation loss: 2.508904968807491

Epoch: 6| Step: 1
Training loss: 2.267512725491549
Validation loss: 2.5002505653700346

Epoch: 6| Step: 2
Training loss: 2.632795113073153
Validation loss: 2.500136896200006

Epoch: 6| Step: 3
Training loss: 2.1534980129695995
Validation loss: 2.4911305927882057

Epoch: 6| Step: 4
Training loss: 2.299726105013607
Validation loss: 2.4871603268987745

Epoch: 6| Step: 5
Training loss: 1.7770099703756457
Validation loss: 2.474312322622015

Epoch: 6| Step: 6
Training loss: 2.620428009307894
Validation loss: 2.480311543446573

Epoch: 6| Step: 7
Training loss: 2.5502033601341583
Validation loss: 2.486165854729221

Epoch: 6| Step: 8
Training loss: 1.6414683762803257
Validation loss: 2.4788781700784064

Epoch: 6| Step: 9
Training loss: 2.322372190245753
Validation loss: 2.4701896365080325

Epoch: 6| Step: 10
Training loss: 2.7932063075053803
Validation loss: 2.4972994205346732

Epoch: 6| Step: 11
Training loss: 2.4922303581738037
Validation loss: 2.484955677811551

Epoch: 6| Step: 12
Training loss: 2.917407532103464
Validation loss: 2.4663933720704097

Epoch: 6| Step: 13
Training loss: 1.8339739099433519
Validation loss: 2.485759629160681

Epoch: 227| Step: 0
Training loss: 2.2843294653157233
Validation loss: 2.489517647316944

Epoch: 6| Step: 1
Training loss: 2.604588029468993
Validation loss: 2.4886556650085985

Epoch: 6| Step: 2
Training loss: 2.154282819714179
Validation loss: 2.506024334166541

Epoch: 6| Step: 3
Training loss: 2.255447151865545
Validation loss: 2.5049866137397543

Epoch: 6| Step: 4
Training loss: 2.1033975183736175
Validation loss: 2.499354008819149

Epoch: 6| Step: 5
Training loss: 3.4101081486441482
Validation loss: 2.488784707167635

Epoch: 6| Step: 6
Training loss: 1.5696942834785395
Validation loss: 2.469102198591388

Epoch: 6| Step: 7
Training loss: 2.0232992824640843
Validation loss: 2.491579880579834

Epoch: 6| Step: 8
Training loss: 1.7439875861303549
Validation loss: 2.488197984005912

Epoch: 6| Step: 9
Training loss: 1.7922190842177612
Validation loss: 2.516839849647975

Epoch: 6| Step: 10
Training loss: 2.4825847581298865
Validation loss: 2.511149316671028

Epoch: 6| Step: 11
Training loss: 2.4334280775080535
Validation loss: 2.5142366673383862

Epoch: 6| Step: 12
Training loss: 2.432985673742296
Validation loss: 2.505770476804272

Epoch: 6| Step: 13
Training loss: 2.910502450307271
Validation loss: 2.5012919107866063

Epoch: 228| Step: 0
Training loss: 2.76813113355097
Validation loss: 2.4940505443408005

Epoch: 6| Step: 1
Training loss: 1.7964913663585012
Validation loss: 2.4959780049837397

Epoch: 6| Step: 2
Training loss: 1.9461424868563295
Validation loss: 2.5108991663122704

Epoch: 6| Step: 3
Training loss: 2.0639870802378866
Validation loss: 2.512704816855365

Epoch: 6| Step: 4
Training loss: 2.4951712705121976
Validation loss: 2.5179713425251595

Epoch: 6| Step: 5
Training loss: 2.545582823803185
Validation loss: 2.5717929028200395

Epoch: 6| Step: 6
Training loss: 2.1959328640332028
Validation loss: 2.602076406192238

Epoch: 6| Step: 7
Training loss: 2.7353037973337617
Validation loss: 2.5829665374823305

Epoch: 6| Step: 8
Training loss: 2.4340294804539138
Validation loss: 2.5691410011529805

Epoch: 6| Step: 9
Training loss: 2.420851952836288
Validation loss: 2.5507366961277715

Epoch: 6| Step: 10
Training loss: 1.5155851221245857
Validation loss: 2.5116025305517993

Epoch: 6| Step: 11
Training loss: 2.6446485394791845
Validation loss: 2.501117138807566

Epoch: 6| Step: 12
Training loss: 2.3925396600862125
Validation loss: 2.4761864570220005

Epoch: 6| Step: 13
Training loss: 2.397643911678603
Validation loss: 2.485397400398969

Epoch: 229| Step: 0
Training loss: 2.640405769953717
Validation loss: 2.504563577100527

Epoch: 6| Step: 1
Training loss: 2.4734942084515104
Validation loss: 2.5047838693452094

Epoch: 6| Step: 2
Training loss: 1.9484114265073946
Validation loss: 2.499317425688498

Epoch: 6| Step: 3
Training loss: 2.159036577262082
Validation loss: 2.48736395663541

Epoch: 6| Step: 4
Training loss: 2.2675767581485435
Validation loss: 2.4865826888217866

Epoch: 6| Step: 5
Training loss: 2.37283286554285
Validation loss: 2.484134142574356

Epoch: 6| Step: 6
Training loss: 1.9935538599044893
Validation loss: 2.4820647304167145

Epoch: 6| Step: 7
Training loss: 2.3934903394935656
Validation loss: 2.526487823622066

Epoch: 6| Step: 8
Training loss: 2.805028783069157
Validation loss: 2.514281804785641

Epoch: 6| Step: 9
Training loss: 2.124644866888179
Validation loss: 2.492546047778868

Epoch: 6| Step: 10
Training loss: 1.8121467772314208
Validation loss: 2.503498379421808

Epoch: 6| Step: 11
Training loss: 2.741169014920345
Validation loss: 2.4977930818411687

Epoch: 6| Step: 12
Training loss: 2.369809351145954
Validation loss: 2.4705585341934437

Epoch: 6| Step: 13
Training loss: 2.0693380113693007
Validation loss: 2.4949404858592383

Epoch: 230| Step: 0
Training loss: 2.9227365457761714
Validation loss: 2.4896216183409434

Epoch: 6| Step: 1
Training loss: 2.339318931451912
Validation loss: 2.493354826922091

Epoch: 6| Step: 2
Training loss: 1.881230619717255
Validation loss: 2.5058740906296935

Epoch: 6| Step: 3
Training loss: 2.4545711458191013
Validation loss: 2.48422664623244

Epoch: 6| Step: 4
Training loss: 2.1558465994933114
Validation loss: 2.485382195804174

Epoch: 6| Step: 5
Training loss: 1.8174592934186016
Validation loss: 2.4864265720068572

Epoch: 6| Step: 6
Training loss: 2.4100515877675073
Validation loss: 2.4807922793079498

Epoch: 6| Step: 7
Training loss: 2.4735516558563018
Validation loss: 2.4867365427486248

Epoch: 6| Step: 8
Training loss: 2.362787671210351
Validation loss: 2.4948660428634826

Epoch: 6| Step: 9
Training loss: 2.632512649983935
Validation loss: 2.488526895595532

Epoch: 6| Step: 10
Training loss: 2.231909398540509
Validation loss: 2.488705457681235

Epoch: 6| Step: 11
Training loss: 2.3423180847207252
Validation loss: 2.4746544243481314

Epoch: 6| Step: 12
Training loss: 2.215784481964326
Validation loss: 2.493000643263253

Epoch: 6| Step: 13
Training loss: 1.6639349803509298
Validation loss: 2.459442047601189

Epoch: 231| Step: 0
Training loss: 2.517820549325054
Validation loss: 2.5128845388713446

Epoch: 6| Step: 1
Training loss: 2.1186567888039622
Validation loss: 2.561072580011363

Epoch: 6| Step: 2
Training loss: 2.6892634084547598
Validation loss: 2.621217924645501

Epoch: 6| Step: 3
Training loss: 2.472703689192439
Validation loss: 2.561389403303216

Epoch: 6| Step: 4
Training loss: 2.6447739370041528
Validation loss: 2.527877649027411

Epoch: 6| Step: 5
Training loss: 2.2000635398012025
Validation loss: 2.4687721476788718

Epoch: 6| Step: 6
Training loss: 2.2214064478265367
Validation loss: 2.4854455476688564

Epoch: 6| Step: 7
Training loss: 2.6718553687372744
Validation loss: 2.5020703167435157

Epoch: 6| Step: 8
Training loss: 2.863096723035913
Validation loss: 2.5169020702673595

Epoch: 6| Step: 9
Training loss: 2.303087306134736
Validation loss: 2.515092939577042

Epoch: 6| Step: 10
Training loss: 2.132129378138116
Validation loss: 2.5063852069181363

Epoch: 6| Step: 11
Training loss: 1.799270492651146
Validation loss: 2.4960383815524603

Epoch: 6| Step: 12
Training loss: 2.0591031625923937
Validation loss: 2.48311348641747

Epoch: 6| Step: 13
Training loss: 2.3209934455617964
Validation loss: 2.4855854356568234

Epoch: 232| Step: 0
Training loss: 2.2627439925843493
Validation loss: 2.4824926574844306

Epoch: 6| Step: 1
Training loss: 2.343780313931565
Validation loss: 2.495855360830813

Epoch: 6| Step: 2
Training loss: 2.529893676574276
Validation loss: 2.5066515016688875

Epoch: 6| Step: 3
Training loss: 2.30377986298702
Validation loss: 2.5110317811513148

Epoch: 6| Step: 4
Training loss: 2.2989908450866516
Validation loss: 2.502791658034406

Epoch: 6| Step: 5
Training loss: 2.8691127040847024
Validation loss: 2.525083558113215

Epoch: 6| Step: 6
Training loss: 2.206849973542628
Validation loss: 2.5483657748858297

Epoch: 6| Step: 7
Training loss: 1.5079076865396928
Validation loss: 2.5385830000960112

Epoch: 6| Step: 8
Training loss: 2.0362420581079657
Validation loss: 2.5181721809406445

Epoch: 6| Step: 9
Training loss: 2.393995116111726
Validation loss: 2.5171065617624975

Epoch: 6| Step: 10
Training loss: 2.557875670793072
Validation loss: 2.493508621554822

Epoch: 6| Step: 11
Training loss: 2.1740588728049373
Validation loss: 2.477452283759663

Epoch: 6| Step: 12
Training loss: 2.2900648819173473
Validation loss: 2.494720128785106

Epoch: 6| Step: 13
Training loss: 1.9096263938519673
Validation loss: 2.4764899434753787

Epoch: 233| Step: 0
Training loss: 2.567775877605186
Validation loss: 2.507723132291031

Epoch: 6| Step: 1
Training loss: 2.094480358673691
Validation loss: 2.4809620604488813

Epoch: 6| Step: 2
Training loss: 2.3565083450632596
Validation loss: 2.514911716345033

Epoch: 6| Step: 3
Training loss: 2.1703222199037655
Validation loss: 2.4800602133424503

Epoch: 6| Step: 4
Training loss: 2.1297010145647177
Validation loss: 2.4972274824300866

Epoch: 6| Step: 5
Training loss: 2.4535272353537882
Validation loss: 2.487485254266378

Epoch: 6| Step: 6
Training loss: 1.6234332381074126
Validation loss: 2.4974434655466546

Epoch: 6| Step: 7
Training loss: 2.140940844770276
Validation loss: 2.4802819209188165

Epoch: 6| Step: 8
Training loss: 2.260854394488644
Validation loss: 2.489240955342098

Epoch: 6| Step: 9
Training loss: 2.5924225133279997
Validation loss: 2.4709258410122974

Epoch: 6| Step: 10
Training loss: 1.4546515537168043
Validation loss: 2.479668804147097

Epoch: 6| Step: 11
Training loss: 2.5270653502056897
Validation loss: 2.4831846892483775

Epoch: 6| Step: 12
Training loss: 2.4334367973940934
Validation loss: 2.4958389621813373

Epoch: 6| Step: 13
Training loss: 3.0540706860569107
Validation loss: 2.492386709321364

Epoch: 234| Step: 0
Training loss: 2.4545215106207268
Validation loss: 2.500546061960557

Epoch: 6| Step: 1
Training loss: 2.197771643122301
Validation loss: 2.4994698121222445

Epoch: 6| Step: 2
Training loss: 2.3153044732328345
Validation loss: 2.5122031243642686

Epoch: 6| Step: 3
Training loss: 1.9201237212492779
Validation loss: 2.4959029163050057

Epoch: 6| Step: 4
Training loss: 2.5700032242335418
Validation loss: 2.4748502441978704

Epoch: 6| Step: 5
Training loss: 3.0513030911225525
Validation loss: 2.482043757980989

Epoch: 6| Step: 6
Training loss: 1.9791835248798024
Validation loss: 2.4738756801785535

Epoch: 6| Step: 7
Training loss: 1.754135422719065
Validation loss: 2.4761616315227104

Epoch: 6| Step: 8
Training loss: 2.0893377244039395
Validation loss: 2.4634463323382945

Epoch: 6| Step: 9
Training loss: 2.6893185517461693
Validation loss: 2.479646865970105

Epoch: 6| Step: 10
Training loss: 1.9802253167234976
Validation loss: 2.4727490705491513

Epoch: 6| Step: 11
Training loss: 2.2225658217339026
Validation loss: 2.4599658723211015

Epoch: 6| Step: 12
Training loss: 2.5340136288018202
Validation loss: 2.466276660149298

Epoch: 6| Step: 13
Training loss: 2.3972882925363272
Validation loss: 2.4784754315727784

Epoch: 235| Step: 0
Training loss: 2.1389324987460205
Validation loss: 2.4701019641355533

Epoch: 6| Step: 1
Training loss: 2.3331508792205273
Validation loss: 2.4853101845565044

Epoch: 6| Step: 2
Training loss: 1.9968265151464202
Validation loss: 2.461397303662629

Epoch: 6| Step: 3
Training loss: 2.4868800171798866
Validation loss: 2.4671048827589313

Epoch: 6| Step: 4
Training loss: 1.6337768043473195
Validation loss: 2.477247077357321

Epoch: 6| Step: 5
Training loss: 2.258163266505724
Validation loss: 2.450468953982392

Epoch: 6| Step: 6
Training loss: 2.617091117336741
Validation loss: 2.469812912824791

Epoch: 6| Step: 7
Training loss: 2.2350804209054846
Validation loss: 2.4559699244247173

Epoch: 6| Step: 8
Training loss: 2.206622438631023
Validation loss: 2.464282617310341

Epoch: 6| Step: 9
Training loss: 3.0757371964714104
Validation loss: 2.4980410848455503

Epoch: 6| Step: 10
Training loss: 2.2968199846594755
Validation loss: 2.506398215156453

Epoch: 6| Step: 11
Training loss: 2.642560080546301
Validation loss: 2.492611297958992

Epoch: 6| Step: 12
Training loss: 2.130612758557667
Validation loss: 2.499645605399489

Epoch: 6| Step: 13
Training loss: 1.7991190290954269
Validation loss: 2.48055185894645

Epoch: 236| Step: 0
Training loss: 2.3648969691538646
Validation loss: 2.4691184368523644

Epoch: 6| Step: 1
Training loss: 1.8101463821399002
Validation loss: 2.486349093260627

Epoch: 6| Step: 2
Training loss: 1.6491641210401773
Validation loss: 2.4735840417246004

Epoch: 6| Step: 3
Training loss: 2.6519949547444908
Validation loss: 2.4772397147294383

Epoch: 6| Step: 4
Training loss: 1.8869669180407678
Validation loss: 2.4742465095661137

Epoch: 6| Step: 5
Training loss: 1.9092753432718343
Validation loss: 2.4778904287915067

Epoch: 6| Step: 6
Training loss: 2.3684656702267146
Validation loss: 2.4890172838924154

Epoch: 6| Step: 7
Training loss: 2.3527565392810796
Validation loss: 2.5055888647650515

Epoch: 6| Step: 8
Training loss: 1.6013829572570881
Validation loss: 2.4876720694832075

Epoch: 6| Step: 9
Training loss: 2.691305261054329
Validation loss: 2.4822062497017345

Epoch: 6| Step: 10
Training loss: 2.8216889211276897
Validation loss: 2.4888017190936487

Epoch: 6| Step: 11
Training loss: 2.93429065457187
Validation loss: 2.4769043390675103

Epoch: 6| Step: 12
Training loss: 2.4520561659698084
Validation loss: 2.5170330506166634

Epoch: 6| Step: 13
Training loss: 2.2690144408410804
Validation loss: 2.541635424520954

Epoch: 237| Step: 0
Training loss: 1.9138705410360073
Validation loss: 2.5358897256572033

Epoch: 6| Step: 1
Training loss: 2.463371502172347
Validation loss: 2.5070769913969193

Epoch: 6| Step: 2
Training loss: 1.675932052691653
Validation loss: 2.512459146980417

Epoch: 6| Step: 3
Training loss: 2.2020425504614956
Validation loss: 2.4994290812591293

Epoch: 6| Step: 4
Training loss: 2.5053177543319527
Validation loss: 2.4799336654754063

Epoch: 6| Step: 5
Training loss: 2.0701982574611817
Validation loss: 2.475464939669179

Epoch: 6| Step: 6
Training loss: 2.656486321202793
Validation loss: 2.4928073889412286

Epoch: 6| Step: 7
Training loss: 2.515344639668893
Validation loss: 2.492453199232266

Epoch: 6| Step: 8
Training loss: 2.4989980597211825
Validation loss: 2.488746364047353

Epoch: 6| Step: 9
Training loss: 2.2212810854649914
Validation loss: 2.468955148148654

Epoch: 6| Step: 10
Training loss: 2.300088308546395
Validation loss: 2.49116587651063

Epoch: 6| Step: 11
Training loss: 1.9248294606155667
Validation loss: 2.4848828866227297

Epoch: 6| Step: 12
Training loss: 2.382125004881019
Validation loss: 2.4918090229981167

Epoch: 6| Step: 13
Training loss: 2.310961752990603
Validation loss: 2.482701351621922

Epoch: 238| Step: 0
Training loss: 2.327360501313918
Validation loss: 2.4807085537072426

Epoch: 6| Step: 1
Training loss: 2.138919791566729
Validation loss: 2.486041391864381

Epoch: 6| Step: 2
Training loss: 2.2636474397408515
Validation loss: 2.497036417110829

Epoch: 6| Step: 3
Training loss: 2.046966783635158
Validation loss: 2.489580359250983

Epoch: 6| Step: 4
Training loss: 2.3454679169519
Validation loss: 2.4981615936446997

Epoch: 6| Step: 5
Training loss: 2.2766192741357645
Validation loss: 2.4843102372723243

Epoch: 6| Step: 6
Training loss: 1.8249781332926869
Validation loss: 2.4879684371387447

Epoch: 6| Step: 7
Training loss: 1.9581232431685733
Validation loss: 2.4878631032854046

Epoch: 6| Step: 8
Training loss: 2.6865518361139884
Validation loss: 2.508046021487434

Epoch: 6| Step: 9
Training loss: 1.9410583675635444
Validation loss: 2.50074651221893

Epoch: 6| Step: 10
Training loss: 2.061644289927857
Validation loss: 2.5082333885972803

Epoch: 6| Step: 11
Training loss: 3.2568827887372236
Validation loss: 2.51415549375559

Epoch: 6| Step: 12
Training loss: 2.0362492004355492
Validation loss: 2.5476717024600375

Epoch: 6| Step: 13
Training loss: 2.4678218400538614
Validation loss: 2.5346198393699932

Epoch: 239| Step: 0
Training loss: 2.0205415600280983
Validation loss: 2.498227492447228

Epoch: 6| Step: 1
Training loss: 2.217947586458448
Validation loss: 2.4747425937697836

Epoch: 6| Step: 2
Training loss: 2.3806278407209356
Validation loss: 2.483105701109415

Epoch: 6| Step: 3
Training loss: 2.28392572120498
Validation loss: 2.4885685553734085

Epoch: 6| Step: 4
Training loss: 2.953152772480188
Validation loss: 2.483502984705822

Epoch: 6| Step: 5
Training loss: 1.5174645340874147
Validation loss: 2.4891767741805753

Epoch: 6| Step: 6
Training loss: 2.2198118435965872
Validation loss: 2.4868029040974915

Epoch: 6| Step: 7
Training loss: 2.3951598395604954
Validation loss: 2.4878306876715146

Epoch: 6| Step: 8
Training loss: 1.766622413524855
Validation loss: 2.497555698108455

Epoch: 6| Step: 9
Training loss: 2.5958673498415963
Validation loss: 2.488297826211712

Epoch: 6| Step: 10
Training loss: 1.8013360416317639
Validation loss: 2.479545537598267

Epoch: 6| Step: 11
Training loss: 2.0253998067384695
Validation loss: 2.515156593705851

Epoch: 6| Step: 12
Training loss: 2.34443735851806
Validation loss: 2.523312859857398

Epoch: 6| Step: 13
Training loss: 2.6736037701482576
Validation loss: 2.5236094822654116

Epoch: 240| Step: 0
Training loss: 2.500522558911827
Validation loss: 2.5512670547984393

Epoch: 6| Step: 1
Training loss: 1.9285336029036897
Validation loss: 2.5448462083936247

Epoch: 6| Step: 2
Training loss: 1.5124213584443045
Validation loss: 2.525537787975993

Epoch: 6| Step: 3
Training loss: 1.9097883810278138
Validation loss: 2.5252789528985895

Epoch: 6| Step: 4
Training loss: 3.307025920774409
Validation loss: 2.5125567836259792

Epoch: 6| Step: 5
Training loss: 2.858115997800783
Validation loss: 2.4801854409853346

Epoch: 6| Step: 6
Training loss: 2.0487422434186935
Validation loss: 2.4989071126132676

Epoch: 6| Step: 7
Training loss: 1.764871622231074
Validation loss: 2.4792962775022582

Epoch: 6| Step: 8
Training loss: 1.7081653310771963
Validation loss: 2.48711566384033

Epoch: 6| Step: 9
Training loss: 2.2044318901921405
Validation loss: 2.4822284374009365

Epoch: 6| Step: 10
Training loss: 2.083730837728703
Validation loss: 2.4789090436760786

Epoch: 6| Step: 11
Training loss: 1.7430784402115898
Validation loss: 2.493625397465328

Epoch: 6| Step: 12
Training loss: 2.1570449690501676
Validation loss: 2.4751140799384816

Epoch: 6| Step: 13
Training loss: 3.144890313350819
Validation loss: 2.490045980669838

Epoch: 241| Step: 0
Training loss: 1.6242988981331867
Validation loss: 2.4863417735677853

Epoch: 6| Step: 1
Training loss: 2.420633009885758
Validation loss: 2.481819181715173

Epoch: 6| Step: 2
Training loss: 2.4290782495790713
Validation loss: 2.4529510565964285

Epoch: 6| Step: 3
Training loss: 1.9664939184882866
Validation loss: 2.516151704796492

Epoch: 6| Step: 4
Training loss: 2.4157191644354894
Validation loss: 2.5460772756984067

Epoch: 6| Step: 5
Training loss: 3.1045239816809325
Validation loss: 2.606999898189869

Epoch: 6| Step: 6
Training loss: 2.398452609632781
Validation loss: 2.6130582051870705

Epoch: 6| Step: 7
Training loss: 2.560783741904563
Validation loss: 2.569076581058783

Epoch: 6| Step: 8
Training loss: 1.7933375160116511
Validation loss: 2.5099064214772944

Epoch: 6| Step: 9
Training loss: 1.8682352741426282
Validation loss: 2.4733033019125137

Epoch: 6| Step: 10
Training loss: 2.032472682642617
Validation loss: 2.4752946931866067

Epoch: 6| Step: 11
Training loss: 2.5973123193663628
Validation loss: 2.509643282549282

Epoch: 6| Step: 12
Training loss: 2.262917103967242
Validation loss: 2.5378529489531205

Epoch: 6| Step: 13
Training loss: 2.6472469568403487
Validation loss: 2.5361463810544653

Epoch: 242| Step: 0
Training loss: 2.3546343538580787
Validation loss: 2.53297898225665

Epoch: 6| Step: 1
Training loss: 3.0006848189420685
Validation loss: 2.5379169560016157

Epoch: 6| Step: 2
Training loss: 2.4663399630348892
Validation loss: 2.527496742047714

Epoch: 6| Step: 3
Training loss: 2.010925610676841
Validation loss: 2.5329437711327776

Epoch: 6| Step: 4
Training loss: 2.159635567298061
Validation loss: 2.527008757679352

Epoch: 6| Step: 5
Training loss: 2.5275215664178368
Validation loss: 2.5178379253284047

Epoch: 6| Step: 6
Training loss: 2.1284642871945945
Validation loss: 2.520841063863287

Epoch: 6| Step: 7
Training loss: 1.856943672628281
Validation loss: 2.5056656217212163

Epoch: 6| Step: 8
Training loss: 2.180356596382834
Validation loss: 2.4670695609470834

Epoch: 6| Step: 9
Training loss: 1.9173711090336893
Validation loss: 2.4899845571434835

Epoch: 6| Step: 10
Training loss: 2.4354218647436094
Validation loss: 2.4965923768447227

Epoch: 6| Step: 11
Training loss: 2.187371495423326
Validation loss: 2.5625385498620727

Epoch: 6| Step: 12
Training loss: 2.901263481348774
Validation loss: 2.6374275968958076

Epoch: 6| Step: 13
Training loss: 2.419475917657683
Validation loss: 2.650919138856678

Epoch: 243| Step: 0
Training loss: 2.2382767762144424
Validation loss: 2.6162838566742046

Epoch: 6| Step: 1
Training loss: 2.437838506424193
Validation loss: 2.557205763317336

Epoch: 6| Step: 2
Training loss: 1.5784910174148366
Validation loss: 2.4919062568356884

Epoch: 6| Step: 3
Training loss: 2.6516218368733675
Validation loss: 2.476129544156008

Epoch: 6| Step: 4
Training loss: 2.0387537930825768
Validation loss: 2.496730701285751

Epoch: 6| Step: 5
Training loss: 2.4610775862286447
Validation loss: 2.4950385771210892

Epoch: 6| Step: 6
Training loss: 2.388679859962124
Validation loss: 2.503068915860772

Epoch: 6| Step: 7
Training loss: 2.9255670894375
Validation loss: 2.505410427472938

Epoch: 6| Step: 8
Training loss: 2.2983914513970376
Validation loss: 2.4795331176568727

Epoch: 6| Step: 9
Training loss: 2.0341967995409624
Validation loss: 2.4750145247062116

Epoch: 6| Step: 10
Training loss: 2.405961056490042
Validation loss: 2.485275441261221

Epoch: 6| Step: 11
Training loss: 2.2541051079411725
Validation loss: 2.479922449222252

Epoch: 6| Step: 12
Training loss: 2.0108791339884178
Validation loss: 2.497355214499002

Epoch: 6| Step: 13
Training loss: 2.3633871764096708
Validation loss: 2.49391473211707

Epoch: 244| Step: 0
Training loss: 2.2461005434679873
Validation loss: 2.4643662076247095

Epoch: 6| Step: 1
Training loss: 2.4571643787214597
Validation loss: 2.4926510402685818

Epoch: 6| Step: 2
Training loss: 1.5606246375044164
Validation loss: 2.4885210832905162

Epoch: 6| Step: 3
Training loss: 2.449321543895498
Validation loss: 2.4904361897388845

Epoch: 6| Step: 4
Training loss: 2.1876286060130683
Validation loss: 2.498187186700125

Epoch: 6| Step: 5
Training loss: 1.7562302313401346
Validation loss: 2.4919533136208285

Epoch: 6| Step: 6
Training loss: 2.0274298324390583
Validation loss: 2.5113273857551928

Epoch: 6| Step: 7
Training loss: 2.3941350363281666
Validation loss: 2.4952714229125674

Epoch: 6| Step: 8
Training loss: 1.8767261824422166
Validation loss: 2.478638269442689

Epoch: 6| Step: 9
Training loss: 2.4551414414816644
Validation loss: 2.5068660229516087

Epoch: 6| Step: 10
Training loss: 2.5669034014761376
Validation loss: 2.504424725362477

Epoch: 6| Step: 11
Training loss: 2.450463181134029
Validation loss: 2.4726563142813642

Epoch: 6| Step: 12
Training loss: 2.165524536268614
Validation loss: 2.482508255931063

Epoch: 6| Step: 13
Training loss: 2.689128382406048
Validation loss: 2.4749569507027447

Epoch: 245| Step: 0
Training loss: 2.3140307334764665
Validation loss: 2.4677280693075483

Epoch: 6| Step: 1
Training loss: 2.4861854338473246
Validation loss: 2.468325840047641

Epoch: 6| Step: 2
Training loss: 2.1759315863401167
Validation loss: 2.4629226670385442

Epoch: 6| Step: 3
Training loss: 1.6475526143125447
Validation loss: 2.4621023902751626

Epoch: 6| Step: 4
Training loss: 1.8911836917848561
Validation loss: 2.4958945579428007

Epoch: 6| Step: 5
Training loss: 1.911021966697164
Validation loss: 2.4836599412553215

Epoch: 6| Step: 6
Training loss: 2.1589168697465113
Validation loss: 2.4776601835703453

Epoch: 6| Step: 7
Training loss: 2.836263338396392
Validation loss: 2.482694757421282

Epoch: 6| Step: 8
Training loss: 2.6257363149559994
Validation loss: 2.474896051812072

Epoch: 6| Step: 9
Training loss: 2.375109619823211
Validation loss: 2.46649739956237

Epoch: 6| Step: 10
Training loss: 2.185568583553659
Validation loss: 2.482228349354903

Epoch: 6| Step: 11
Training loss: 2.0330971168930296
Validation loss: 2.4800819876408386

Epoch: 6| Step: 12
Training loss: 2.1815018298088757
Validation loss: 2.471072026431734

Epoch: 6| Step: 13
Training loss: 2.4003401674161777
Validation loss: 2.4687366646193225

Epoch: 246| Step: 0
Training loss: 1.977366649505637
Validation loss: 2.4876963807867165

Epoch: 6| Step: 1
Training loss: 2.125062604991739
Validation loss: 2.477876717635789

Epoch: 6| Step: 2
Training loss: 1.4492987844571692
Validation loss: 2.501975216992588

Epoch: 6| Step: 3
Training loss: 2.4729293021629415
Validation loss: 2.5011493583943403

Epoch: 6| Step: 4
Training loss: 2.352240277830051
Validation loss: 2.4914413180135595

Epoch: 6| Step: 5
Training loss: 2.1922108742085675
Validation loss: 2.507150913329605

Epoch: 6| Step: 6
Training loss: 2.61047559358578
Validation loss: 2.486215865084824

Epoch: 6| Step: 7
Training loss: 2.0466688212446535
Validation loss: 2.490440705172566

Epoch: 6| Step: 8
Training loss: 2.010918852653684
Validation loss: 2.481859481098242

Epoch: 6| Step: 9
Training loss: 2.5562563870450465
Validation loss: 2.477285173393887

Epoch: 6| Step: 10
Training loss: 2.249129020806474
Validation loss: 2.467485917066319

Epoch: 6| Step: 11
Training loss: 2.742904699713686
Validation loss: 2.4792842249419613

Epoch: 6| Step: 12
Training loss: 1.7618264717274208
Validation loss: 2.495737733957725

Epoch: 6| Step: 13
Training loss: 2.6053655280808696
Validation loss: 2.4694697059332875

Epoch: 247| Step: 0
Training loss: 1.965354352551814
Validation loss: 2.496427860892703

Epoch: 6| Step: 1
Training loss: 2.296433386874961
Validation loss: 2.511223110816437

Epoch: 6| Step: 2
Training loss: 2.1210341752539525
Validation loss: 2.461821785750615

Epoch: 6| Step: 3
Training loss: 2.0049607029058034
Validation loss: 2.4892657781059206

Epoch: 6| Step: 4
Training loss: 2.062732567824124
Validation loss: 2.5453236697712747

Epoch: 6| Step: 5
Training loss: 2.7674909428531858
Validation loss: 2.618937196922165

Epoch: 6| Step: 6
Training loss: 2.0242355128087044
Validation loss: 2.63973725273339

Epoch: 6| Step: 7
Training loss: 2.3403372777461797
Validation loss: 2.5644354760327013

Epoch: 6| Step: 8
Training loss: 2.3420574943593433
Validation loss: 2.571780804762797

Epoch: 6| Step: 9
Training loss: 2.438709154749943
Validation loss: 2.5200751139027413

Epoch: 6| Step: 10
Training loss: 2.8878066061153973
Validation loss: 2.482886542273914

Epoch: 6| Step: 11
Training loss: 2.556169925545432
Validation loss: 2.4810321238169246

Epoch: 6| Step: 12
Training loss: 2.031056321520643
Validation loss: 2.4852913979753235

Epoch: 6| Step: 13
Training loss: 1.746121195123025
Validation loss: 2.4917915213017534

Epoch: 248| Step: 0
Training loss: 1.902644452840617
Validation loss: 2.5191904236681464

Epoch: 6| Step: 1
Training loss: 3.1517035343536093
Validation loss: 2.50552644249843

Epoch: 6| Step: 2
Training loss: 2.6263449720071628
Validation loss: 2.532912818825028

Epoch: 6| Step: 3
Training loss: 2.509489075382499
Validation loss: 2.5168468280330507

Epoch: 6| Step: 4
Training loss: 2.235137595905862
Validation loss: 2.5096890884386154

Epoch: 6| Step: 5
Training loss: 2.0209968605602255
Validation loss: 2.5166886886576316

Epoch: 6| Step: 6
Training loss: 1.5656718294674306
Validation loss: 2.5057419640142853

Epoch: 6| Step: 7
Training loss: 1.8457111338285237
Validation loss: 2.511357892080998

Epoch: 6| Step: 8
Training loss: 2.077622316985191
Validation loss: 2.5021326664220225

Epoch: 6| Step: 9
Training loss: 2.048419398875974
Validation loss: 2.4759014061559816

Epoch: 6| Step: 10
Training loss: 2.480793032136768
Validation loss: 2.4825745462350852

Epoch: 6| Step: 11
Training loss: 2.137538606172342
Validation loss: 2.4721398238643713

Epoch: 6| Step: 12
Training loss: 2.6262926824297743
Validation loss: 2.484073420679873

Epoch: 6| Step: 13
Training loss: 2.7096628495965853
Validation loss: 2.494018997815461

Epoch: 249| Step: 0
Training loss: 1.9215680629359986
Validation loss: 2.5201059400653434

Epoch: 6| Step: 1
Training loss: 2.3763153047934127
Validation loss: 2.540854470188115

Epoch: 6| Step: 2
Training loss: 2.973253869276385
Validation loss: 2.555367413335436

Epoch: 6| Step: 3
Training loss: 1.8108331316148927
Validation loss: 2.5522895781551274

Epoch: 6| Step: 4
Training loss: 2.3446135900847933
Validation loss: 2.541732229627479

Epoch: 6| Step: 5
Training loss: 2.1038037229445368
Validation loss: 2.5129858986332034

Epoch: 6| Step: 6
Training loss: 3.1596582725304208
Validation loss: 2.4898540610323443

Epoch: 6| Step: 7
Training loss: 1.9009081301536004
Validation loss: 2.5001536322118016

Epoch: 6| Step: 8
Training loss: 2.1990947247998616
Validation loss: 2.476631861536699

Epoch: 6| Step: 9
Training loss: 2.303765788280349
Validation loss: 2.469611182684666

Epoch: 6| Step: 10
Training loss: 1.7660756169089995
Validation loss: 2.4634045783861125

Epoch: 6| Step: 11
Training loss: 1.961895043986807
Validation loss: 2.472426770948009

Epoch: 6| Step: 12
Training loss: 2.0232364746154503
Validation loss: 2.488773730382183

Epoch: 6| Step: 13
Training loss: 2.3432766245586287
Validation loss: 2.4889317195784244

Epoch: 250| Step: 0
Training loss: 2.2516012851569784
Validation loss: 2.4968718031002197

Epoch: 6| Step: 1
Training loss: 2.247012910602932
Validation loss: 2.4702592172009643

Epoch: 6| Step: 2
Training loss: 1.6641238205764501
Validation loss: 2.4669962581177822

Epoch: 6| Step: 3
Training loss: 2.1241731718055283
Validation loss: 2.47099821517285

Epoch: 6| Step: 4
Training loss: 2.608629679882274
Validation loss: 2.4844731565394267

Epoch: 6| Step: 5
Training loss: 2.0450130467826435
Validation loss: 2.4635399837904837

Epoch: 6| Step: 6
Training loss: 2.268870377302649
Validation loss: 2.4783805648041675

Epoch: 6| Step: 7
Training loss: 2.1295581624421818
Validation loss: 2.4836303426649344

Epoch: 6| Step: 8
Training loss: 2.088832489338977
Validation loss: 2.486214586468896

Epoch: 6| Step: 9
Training loss: 2.060525469194205
Validation loss: 2.4557223490151476

Epoch: 6| Step: 10
Training loss: 2.9161333595622736
Validation loss: 2.502111973361702

Epoch: 6| Step: 11
Training loss: 2.2129937305318537
Validation loss: 2.4962374188662144

Epoch: 6| Step: 12
Training loss: 2.3683871512189643
Validation loss: 2.496983345236925

Epoch: 6| Step: 13
Training loss: 2.1335354093371657
Validation loss: 2.5015460717294995

Testing loss: 2.024831120961948

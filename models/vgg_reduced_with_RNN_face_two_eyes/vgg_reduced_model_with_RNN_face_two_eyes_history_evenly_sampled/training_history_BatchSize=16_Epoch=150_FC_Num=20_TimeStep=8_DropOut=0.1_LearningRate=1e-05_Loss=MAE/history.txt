Epoch: 1| Step: 0
Training loss: 4.949955463409424
Validation loss: 5.087752501169841

Epoch: 6| Step: 1
Training loss: 5.021383285522461
Validation loss: 5.078610768882177

Epoch: 6| Step: 2
Training loss: 4.523065567016602
Validation loss: 5.069794029317876

Epoch: 6| Step: 3
Training loss: 5.084537982940674
Validation loss: 5.0618249472751415

Epoch: 6| Step: 4
Training loss: 4.24127197265625
Validation loss: 5.053538296812324

Epoch: 6| Step: 5
Training loss: 5.193145751953125
Validation loss: 5.045460859934489

Epoch: 6| Step: 6
Training loss: 4.403186798095703
Validation loss: 5.036667275172408

Epoch: 6| Step: 7
Training loss: 5.127527236938477
Validation loss: 5.027266984344811

Epoch: 6| Step: 8
Training loss: 3.998248338699341
Validation loss: 5.017639196047219

Epoch: 6| Step: 9
Training loss: 4.571783065795898
Validation loss: 5.006928728472802

Epoch: 6| Step: 10
Training loss: 6.52956485748291
Validation loss: 4.996032402079592

Epoch: 6| Step: 11
Training loss: 4.720292091369629
Validation loss: 4.984228211064493

Epoch: 6| Step: 12
Training loss: 5.59542179107666
Validation loss: 4.971108651930286

Epoch: 6| Step: 13
Training loss: 2.6625514030456543
Validation loss: 4.957496960957845

Epoch: 2| Step: 0
Training loss: 4.707352638244629
Validation loss: 4.94263401339131

Epoch: 6| Step: 1
Training loss: 4.8986287117004395
Validation loss: 4.92683470120994

Epoch: 6| Step: 2
Training loss: 5.338608264923096
Validation loss: 4.909538269042969

Epoch: 6| Step: 3
Training loss: 4.698901176452637
Validation loss: 4.89154520855155

Epoch: 6| Step: 4
Training loss: 4.8395490646362305
Validation loss: 4.871959465806202

Epoch: 6| Step: 5
Training loss: 5.177326679229736
Validation loss: 4.850965628059962

Epoch: 6| Step: 6
Training loss: 5.387664794921875
Validation loss: 4.827972809473674

Epoch: 6| Step: 7
Training loss: 4.886185169219971
Validation loss: 4.803114255269368

Epoch: 6| Step: 8
Training loss: 4.6806864738464355
Validation loss: 4.7780245324616795

Epoch: 6| Step: 9
Training loss: 4.173163414001465
Validation loss: 4.750633803747034

Epoch: 6| Step: 10
Training loss: 4.525059223175049
Validation loss: 4.722822599513556

Epoch: 6| Step: 11
Training loss: 3.0775809288024902
Validation loss: 4.693311768193399

Epoch: 6| Step: 12
Training loss: 3.166512966156006
Validation loss: 4.6639398503047165

Epoch: 6| Step: 13
Training loss: 4.962898254394531
Validation loss: 4.632421447384742

Epoch: 3| Step: 0
Training loss: 4.498939037322998
Validation loss: 4.600874326562368

Epoch: 6| Step: 1
Training loss: 6.119555950164795
Validation loss: 4.5686418984525945

Epoch: 6| Step: 2
Training loss: 3.483590602874756
Validation loss: 4.535101093271727

Epoch: 6| Step: 3
Training loss: 4.086027145385742
Validation loss: 4.5042034733679985

Epoch: 6| Step: 4
Training loss: 3.279552936553955
Validation loss: 4.4724686273964505

Epoch: 6| Step: 5
Training loss: 4.224011421203613
Validation loss: 4.44087742733699

Epoch: 6| Step: 6
Training loss: 3.9228782653808594
Validation loss: 4.411086102967621

Epoch: 6| Step: 7
Training loss: 3.815476417541504
Validation loss: 4.380528675612583

Epoch: 6| Step: 8
Training loss: 4.83033561706543
Validation loss: 4.353570445891349

Epoch: 6| Step: 9
Training loss: 4.629256248474121
Validation loss: 4.3248553224789195

Epoch: 6| Step: 10
Training loss: 3.6921939849853516
Validation loss: 4.298075578546011

Epoch: 6| Step: 11
Training loss: 3.72768235206604
Validation loss: 4.271393109393376

Epoch: 6| Step: 12
Training loss: 4.278014183044434
Validation loss: 4.245260397593181

Epoch: 6| Step: 13
Training loss: 4.273837089538574
Validation loss: 4.21906824522121

Epoch: 4| Step: 0
Training loss: 4.148159027099609
Validation loss: 4.191958409483715

Epoch: 6| Step: 1
Training loss: 3.5734267234802246
Validation loss: 4.165565008758216

Epoch: 6| Step: 2
Training loss: 3.0631234645843506
Validation loss: 4.134065084559943

Epoch: 6| Step: 3
Training loss: 4.405747413635254
Validation loss: 4.100415450270458

Epoch: 6| Step: 4
Training loss: 4.1021728515625
Validation loss: 4.074743229855773

Epoch: 6| Step: 5
Training loss: 3.8317627906799316
Validation loss: 4.054190163971276

Epoch: 6| Step: 6
Training loss: 4.90766716003418
Validation loss: 4.034714498827534

Epoch: 6| Step: 7
Training loss: 4.884154796600342
Validation loss: 4.0140561672949024

Epoch: 6| Step: 8
Training loss: 3.777763843536377
Validation loss: 3.995206884158555

Epoch: 6| Step: 9
Training loss: 4.416220664978027
Validation loss: 3.9730751283707155

Epoch: 6| Step: 10
Training loss: 2.5903432369232178
Validation loss: 3.9514904586217736

Epoch: 6| Step: 11
Training loss: 3.3242344856262207
Validation loss: 3.9261066990513958

Epoch: 6| Step: 12
Training loss: 2.786226272583008
Validation loss: 3.8967116263604935

Epoch: 6| Step: 13
Training loss: 4.992710113525391
Validation loss: 3.865050897803358

Epoch: 5| Step: 0
Training loss: 4.384417533874512
Validation loss: 3.827891242119574

Epoch: 6| Step: 1
Training loss: 4.910860061645508
Validation loss: 3.7998321082002375

Epoch: 6| Step: 2
Training loss: 3.272576332092285
Validation loss: 3.7788123879381406

Epoch: 6| Step: 3
Training loss: 3.338776111602783
Validation loss: 3.760204945841143

Epoch: 6| Step: 4
Training loss: 4.449943542480469
Validation loss: 3.7430682848858576

Epoch: 6| Step: 5
Training loss: 2.4366257190704346
Validation loss: 3.727816209998182

Epoch: 6| Step: 6
Training loss: 4.029564380645752
Validation loss: 3.710638928157027

Epoch: 6| Step: 7
Training loss: 2.7786853313446045
Validation loss: 3.694284298086679

Epoch: 6| Step: 8
Training loss: 3.8590898513793945
Validation loss: 3.6790069636478218

Epoch: 6| Step: 9
Training loss: 3.8141121864318848
Validation loss: 3.663945372386645

Epoch: 6| Step: 10
Training loss: 3.766079902648926
Validation loss: 3.648623466491699

Epoch: 6| Step: 11
Training loss: 3.253126859664917
Validation loss: 3.6341042851889007

Epoch: 6| Step: 12
Training loss: 2.634742498397827
Validation loss: 3.621326490115094

Epoch: 6| Step: 13
Training loss: 3.602403402328491
Validation loss: 3.6076932132885022

Epoch: 6| Step: 0
Training loss: 4.165832042694092
Validation loss: 3.5937137552486953

Epoch: 6| Step: 1
Training loss: 3.1432509422302246
Validation loss: 3.583049694697062

Epoch: 6| Step: 2
Training loss: 3.508636713027954
Validation loss: 3.5712108970970236

Epoch: 6| Step: 3
Training loss: 4.06104850769043
Validation loss: 3.556804439072968

Epoch: 6| Step: 4
Training loss: 3.72837495803833
Validation loss: 3.5391849548585954

Epoch: 6| Step: 5
Training loss: 3.942586898803711
Validation loss: 3.5234351593961

Epoch: 6| Step: 6
Training loss: 2.659388303756714
Validation loss: 3.5065144262006207

Epoch: 6| Step: 7
Training loss: 2.8012681007385254
Validation loss: 3.4924185763123217

Epoch: 6| Step: 8
Training loss: 3.4458696842193604
Validation loss: 3.477310542137392

Epoch: 6| Step: 9
Training loss: 3.210376739501953
Validation loss: 3.4643981072210495

Epoch: 6| Step: 10
Training loss: 3.566211462020874
Validation loss: 3.4492637111294653

Epoch: 6| Step: 11
Training loss: 3.1061019897460938
Validation loss: 3.4401562854807866

Epoch: 6| Step: 12
Training loss: 3.8870999813079834
Validation loss: 3.428648641032557

Epoch: 6| Step: 13
Training loss: 2.5688657760620117
Validation loss: 3.421579107161491

Epoch: 7| Step: 0
Training loss: 3.248908281326294
Validation loss: 3.4117142949053036

Epoch: 6| Step: 1
Training loss: 3.234097480773926
Validation loss: 3.3994328591131393

Epoch: 6| Step: 2
Training loss: 2.746690034866333
Validation loss: 3.384628680444533

Epoch: 6| Step: 3
Training loss: 4.068449974060059
Validation loss: 3.3723605832745953

Epoch: 6| Step: 4
Training loss: 3.811570167541504
Validation loss: 3.3632837854405886

Epoch: 6| Step: 5
Training loss: 3.0780129432678223
Validation loss: 3.352398685229722

Epoch: 6| Step: 6
Training loss: 2.6523818969726562
Validation loss: 3.346599348129765

Epoch: 6| Step: 7
Training loss: 2.7138333320617676
Validation loss: 3.3292352255954536

Epoch: 6| Step: 8
Training loss: 3.213151454925537
Validation loss: 3.318218646510955

Epoch: 6| Step: 9
Training loss: 3.103804349899292
Validation loss: 3.3104983914283013

Epoch: 6| Step: 10
Training loss: 4.266231536865234
Validation loss: 3.298144625079247

Epoch: 6| Step: 11
Training loss: 3.0241808891296387
Validation loss: 3.288279620550012

Epoch: 6| Step: 12
Training loss: 3.3975882530212402
Validation loss: 3.2742500407721407

Epoch: 6| Step: 13
Training loss: 3.8527657985687256
Validation loss: 3.261492980423794

Epoch: 8| Step: 0
Training loss: 2.521439790725708
Validation loss: 3.243361347465105

Epoch: 6| Step: 1
Training loss: 3.720083475112915
Validation loss: 3.2324126484573528

Epoch: 6| Step: 2
Training loss: 3.0820424556732178
Validation loss: 3.223583908491237

Epoch: 6| Step: 3
Training loss: 3.82820463180542
Validation loss: 3.212349130261329

Epoch: 6| Step: 4
Training loss: 2.8038082122802734
Validation loss: 3.2018155692726054

Epoch: 6| Step: 5
Training loss: 3.123603582382202
Validation loss: 3.1892571423643377

Epoch: 6| Step: 6
Training loss: 4.2890706062316895
Validation loss: 3.1872314945344002

Epoch: 6| Step: 7
Training loss: 4.264596939086914
Validation loss: 3.177395671926519

Epoch: 6| Step: 8
Training loss: 2.7611136436462402
Validation loss: 3.1689547005520073

Epoch: 6| Step: 9
Training loss: 3.6875877380371094
Validation loss: 3.160010814666748

Epoch: 6| Step: 10
Training loss: 2.537757635116577
Validation loss: 3.1528136499466433

Epoch: 6| Step: 11
Training loss: 2.4559857845306396
Validation loss: 3.148288611442812

Epoch: 6| Step: 12
Training loss: 2.9464406967163086
Validation loss: 3.1468714155176634

Epoch: 6| Step: 13
Training loss: 2.1568472385406494
Validation loss: 3.1383550218356553

Epoch: 9| Step: 0
Training loss: 2.05960750579834
Validation loss: 3.1379987808965866

Epoch: 6| Step: 1
Training loss: 2.962637424468994
Validation loss: 3.1452483771949686

Epoch: 6| Step: 2
Training loss: 3.7455689907073975
Validation loss: 3.139483287770261

Epoch: 6| Step: 3
Training loss: 4.06210470199585
Validation loss: 3.1336976584567817

Epoch: 6| Step: 4
Training loss: 2.4718217849731445
Validation loss: 3.130265615319693

Epoch: 6| Step: 5
Training loss: 3.44897723197937
Validation loss: 3.1229748572072675

Epoch: 6| Step: 6
Training loss: 2.830925464630127
Validation loss: 3.112296106994793

Epoch: 6| Step: 7
Training loss: 2.7134838104248047
Validation loss: 3.0994959954292542

Epoch: 6| Step: 8
Training loss: 2.9065840244293213
Validation loss: 3.0990960367264284

Epoch: 6| Step: 9
Training loss: 3.614238977432251
Validation loss: 3.093250802768174

Epoch: 6| Step: 10
Training loss: 3.683797836303711
Validation loss: 3.0859617802404586

Epoch: 6| Step: 11
Training loss: 2.572449207305908
Validation loss: 3.081352262086766

Epoch: 6| Step: 12
Training loss: 3.136674404144287
Validation loss: 3.0710663077651814

Epoch: 6| Step: 13
Training loss: 3.934948444366455
Validation loss: 3.0600530127043366

Epoch: 10| Step: 0
Training loss: 2.866239547729492
Validation loss: 3.054952795787524

Epoch: 6| Step: 1
Training loss: 3.1749472618103027
Validation loss: 3.05740951466304

Epoch: 6| Step: 2
Training loss: 1.808190107345581
Validation loss: 3.0542693266304592

Epoch: 6| Step: 3
Training loss: 3.8040874004364014
Validation loss: 3.0474944217230684

Epoch: 6| Step: 4
Training loss: 3.5295376777648926
Validation loss: 3.039387341468565

Epoch: 6| Step: 5
Training loss: 4.254421710968018
Validation loss: 3.0318675323199202

Epoch: 6| Step: 6
Training loss: 2.3914365768432617
Validation loss: 3.021079312088669

Epoch: 6| Step: 7
Training loss: 3.1969263553619385
Validation loss: 3.017883516127063

Epoch: 6| Step: 8
Training loss: 2.7545266151428223
Validation loss: 3.02171685618739

Epoch: 6| Step: 9
Training loss: 2.623917579650879
Validation loss: 3.0056673275527133

Epoch: 6| Step: 10
Training loss: 3.371290922164917
Validation loss: 2.9942538302431823

Epoch: 6| Step: 11
Training loss: 3.4808709621429443
Validation loss: 2.988502379386656

Epoch: 6| Step: 12
Training loss: 2.913419485092163
Validation loss: 2.9846497812578754

Epoch: 6| Step: 13
Training loss: 2.726578950881958
Validation loss: 2.9726062615712485

Epoch: 11| Step: 0
Training loss: 4.054049491882324
Validation loss: 2.964390239407939

Epoch: 6| Step: 1
Training loss: 2.7438223361968994
Validation loss: 2.96402064190116

Epoch: 6| Step: 2
Training loss: 3.026279926300049
Validation loss: 2.9538351438378774

Epoch: 6| Step: 3
Training loss: 2.401148796081543
Validation loss: 2.9539352898956626

Epoch: 6| Step: 4
Training loss: 2.9489662647247314
Validation loss: 2.9562087648658344

Epoch: 6| Step: 5
Training loss: 2.523077964782715
Validation loss: 2.9438046383601364

Epoch: 6| Step: 6
Training loss: 2.675351142883301
Validation loss: 2.968231911300331

Epoch: 6| Step: 7
Training loss: 3.4262492656707764
Validation loss: 3.0098822475761495

Epoch: 6| Step: 8
Training loss: 3.058556079864502
Validation loss: 2.9503283269943728

Epoch: 6| Step: 9
Training loss: 3.017685651779175
Validation loss: 2.938913365846039

Epoch: 6| Step: 10
Training loss: 2.757227659225464
Validation loss: 2.9416961234102965

Epoch: 6| Step: 11
Training loss: 4.16300106048584
Validation loss: 2.9696291057012414

Epoch: 6| Step: 12
Training loss: 2.7881174087524414
Validation loss: 2.969690868931432

Epoch: 6| Step: 13
Training loss: 2.8084144592285156
Validation loss: 2.9565170759795816

Epoch: 12| Step: 0
Training loss: 3.5026848316192627
Validation loss: 2.9464114532675794

Epoch: 6| Step: 1
Training loss: 3.544724225997925
Validation loss: 2.9294383423302763

Epoch: 6| Step: 2
Training loss: 2.562199592590332
Validation loss: 2.9261335531870523

Epoch: 6| Step: 3
Training loss: 3.2570323944091797
Validation loss: 2.924266243493685

Epoch: 6| Step: 4
Training loss: 3.07366943359375
Validation loss: 2.9221929709116616

Epoch: 6| Step: 5
Training loss: 3.2239513397216797
Validation loss: 2.90982977549235

Epoch: 6| Step: 6
Training loss: 3.10811710357666
Validation loss: 2.898733369765743

Epoch: 6| Step: 7
Training loss: 2.494126796722412
Validation loss: 2.8959917919610136

Epoch: 6| Step: 8
Training loss: 3.0245094299316406
Validation loss: 2.8846521326290664

Epoch: 6| Step: 9
Training loss: 2.7714719772338867
Validation loss: 2.882167705925562

Epoch: 6| Step: 10
Training loss: 2.879154682159424
Validation loss: 2.8706083349002305

Epoch: 6| Step: 11
Training loss: 2.44854736328125
Validation loss: 2.8719670618734052

Epoch: 6| Step: 12
Training loss: 3.681525230407715
Validation loss: 2.8699576341977684

Epoch: 6| Step: 13
Training loss: 1.8933569192886353
Validation loss: 2.868160363166563

Epoch: 13| Step: 0
Training loss: 3.476463794708252
Validation loss: 2.8672186328518774

Epoch: 6| Step: 1
Training loss: 2.4761152267456055
Validation loss: 2.865830265065675

Epoch: 6| Step: 2
Training loss: 3.028592109680176
Validation loss: 2.8545669483882126

Epoch: 6| Step: 3
Training loss: 3.3944859504699707
Validation loss: 2.8467173678900606

Epoch: 6| Step: 4
Training loss: 2.68405818939209
Validation loss: 2.8567294125915854

Epoch: 6| Step: 5
Training loss: 3.358189105987549
Validation loss: 2.8396002323396745

Epoch: 6| Step: 6
Training loss: 3.385366678237915
Validation loss: 2.8313733147036646

Epoch: 6| Step: 7
Training loss: 2.2561187744140625
Validation loss: 2.8267597562523297

Epoch: 6| Step: 8
Training loss: 3.289065361022949
Validation loss: 2.8253647332550376

Epoch: 6| Step: 9
Training loss: 3.5165047645568848
Validation loss: 2.828612253230105

Epoch: 6| Step: 10
Training loss: 2.4666342735290527
Validation loss: 2.8222109489543463

Epoch: 6| Step: 11
Training loss: 2.5431418418884277
Validation loss: 2.8277967258166243

Epoch: 6| Step: 12
Training loss: 2.8019943237304688
Validation loss: 2.8341094793811923

Epoch: 6| Step: 13
Training loss: 2.4675958156585693
Validation loss: 2.8319107024900374

Epoch: 14| Step: 0
Training loss: 3.1257801055908203
Validation loss: 2.826292666055823

Epoch: 6| Step: 1
Training loss: 2.063948154449463
Validation loss: 2.8162310764353764

Epoch: 6| Step: 2
Training loss: 2.9679291248321533
Validation loss: 2.8105937614235827

Epoch: 6| Step: 3
Training loss: 2.902212381362915
Validation loss: 2.798788119387883

Epoch: 6| Step: 4
Training loss: 2.9199414253234863
Validation loss: 2.799633031250328

Epoch: 6| Step: 5
Training loss: 3.717440605163574
Validation loss: 2.792870178017565

Epoch: 6| Step: 6
Training loss: 2.9009695053100586
Validation loss: 2.7928298519503687

Epoch: 6| Step: 7
Training loss: 3.0805673599243164
Validation loss: 2.792450071662985

Epoch: 6| Step: 8
Training loss: 2.7847766876220703
Validation loss: 2.7930404524649344

Epoch: 6| Step: 9
Training loss: 2.602900266647339
Validation loss: 2.7836550692076325

Epoch: 6| Step: 10
Training loss: 3.011634111404419
Validation loss: 2.7920357565726004

Epoch: 6| Step: 11
Training loss: 2.37162446975708
Validation loss: 2.8097537384238294

Epoch: 6| Step: 12
Training loss: 3.3298444747924805
Validation loss: 2.7825616687856694

Epoch: 6| Step: 13
Training loss: 3.3993356227874756
Validation loss: 2.7757380905971734

Epoch: 15| Step: 0
Training loss: 2.1433722972869873
Validation loss: 2.7749915661350375

Epoch: 6| Step: 1
Training loss: 2.8344602584838867
Validation loss: 2.7799915729030484

Epoch: 6| Step: 2
Training loss: 4.188936233520508
Validation loss: 2.7905137154363815

Epoch: 6| Step: 3
Training loss: 3.1873726844787598
Validation loss: 2.797061361292357

Epoch: 6| Step: 4
Training loss: 3.6072092056274414
Validation loss: 2.7675152158224456

Epoch: 6| Step: 5
Training loss: 2.3183577060699463
Validation loss: 2.7675924608784337

Epoch: 6| Step: 6
Training loss: 2.967168092727661
Validation loss: 2.774422845532817

Epoch: 6| Step: 7
Training loss: 2.6269233226776123
Validation loss: 2.7838195728999313

Epoch: 6| Step: 8
Training loss: 3.127340793609619
Validation loss: 2.7968941965410785

Epoch: 6| Step: 9
Training loss: 3.832846164703369
Validation loss: 2.8035532120735414

Epoch: 6| Step: 10
Training loss: 2.449406147003174
Validation loss: 2.7910987254112

Epoch: 6| Step: 11
Training loss: 2.718290090560913
Validation loss: 2.778742895331434

Epoch: 6| Step: 12
Training loss: 2.580991744995117
Validation loss: 2.765779649057696

Epoch: 6| Step: 13
Training loss: 1.7864843606948853
Validation loss: 2.757803650312526

Epoch: 16| Step: 0
Training loss: 3.6761534214019775
Validation loss: 2.7537549259842082

Epoch: 6| Step: 1
Training loss: 2.725472927093506
Validation loss: 2.7559073791708997

Epoch: 6| Step: 2
Training loss: 2.6316001415252686
Validation loss: 2.7625338518491356

Epoch: 6| Step: 3
Training loss: 2.673635959625244
Validation loss: 2.7512041702065417

Epoch: 6| Step: 4
Training loss: 2.371170997619629
Validation loss: 2.7457291746652253

Epoch: 6| Step: 5
Training loss: 3.2572712898254395
Validation loss: 2.7447638998749437

Epoch: 6| Step: 6
Training loss: 2.47271728515625
Validation loss: 2.7406934845832085

Epoch: 6| Step: 7
Training loss: 3.56772780418396
Validation loss: 2.736366548845845

Epoch: 6| Step: 8
Training loss: 3.4394359588623047
Validation loss: 2.7354338451098372

Epoch: 6| Step: 9
Training loss: 2.651057481765747
Validation loss: 2.7411645099680912

Epoch: 6| Step: 10
Training loss: 3.008024215698242
Validation loss: 2.7424658190819526

Epoch: 6| Step: 11
Training loss: 2.6924123764038086
Validation loss: 2.7484337052991314

Epoch: 6| Step: 12
Training loss: 2.56907320022583
Validation loss: 2.7485667736299577

Epoch: 6| Step: 13
Training loss: 2.6079037189483643
Validation loss: 2.740556893810149

Epoch: 17| Step: 0
Training loss: 3.482797384262085
Validation loss: 2.7329350902188208

Epoch: 6| Step: 1
Training loss: 3.5913021564483643
Validation loss: 2.7199184202378794

Epoch: 6| Step: 2
Training loss: 2.998579263687134
Validation loss: 2.720882741353845

Epoch: 6| Step: 3
Training loss: 2.8964173793792725
Validation loss: 2.7154158289714525

Epoch: 6| Step: 4
Training loss: 2.4509384632110596
Validation loss: 2.7145151938161542

Epoch: 6| Step: 5
Training loss: 1.928741216659546
Validation loss: 2.716391376269761

Epoch: 6| Step: 6
Training loss: 2.7650821208953857
Validation loss: 2.710582761354344

Epoch: 6| Step: 7
Training loss: 2.284355878829956
Validation loss: 2.7054613687658824

Epoch: 6| Step: 8
Training loss: 2.414518356323242
Validation loss: 2.7054089628240114

Epoch: 6| Step: 9
Training loss: 2.9526376724243164
Validation loss: 2.701306983988772

Epoch: 6| Step: 10
Training loss: 3.550795555114746
Validation loss: 2.7024910270526843

Epoch: 6| Step: 11
Training loss: 3.4397170543670654
Validation loss: 2.7013136827817528

Epoch: 6| Step: 12
Training loss: 2.6894826889038086
Validation loss: 2.7051634711603962

Epoch: 6| Step: 13
Training loss: 2.731645107269287
Validation loss: 2.7058468916082896

Epoch: 18| Step: 0
Training loss: 3.039055824279785
Validation loss: 2.713602706950198

Epoch: 6| Step: 1
Training loss: 3.0119147300720215
Validation loss: 2.707327001838274

Epoch: 6| Step: 2
Training loss: 3.5718069076538086
Validation loss: 2.7167632015802528

Epoch: 6| Step: 3
Training loss: 2.3573014736175537
Validation loss: 2.702123913713681

Epoch: 6| Step: 4
Training loss: 3.247112274169922
Validation loss: 2.6985159074106524

Epoch: 6| Step: 5
Training loss: 2.864691972732544
Validation loss: 2.6906747843629573

Epoch: 6| Step: 6
Training loss: 2.7770328521728516
Validation loss: 2.6881511493395736

Epoch: 6| Step: 7
Training loss: 2.1598756313323975
Validation loss: 2.685371842435611

Epoch: 6| Step: 8
Training loss: 2.5425877571105957
Validation loss: 2.6846276175591255

Epoch: 6| Step: 9
Training loss: 2.174388885498047
Validation loss: 2.6824371430181686

Epoch: 6| Step: 10
Training loss: 3.285470724105835
Validation loss: 2.6810092182569605

Epoch: 6| Step: 11
Training loss: 2.7944037914276123
Validation loss: 2.676243902534567

Epoch: 6| Step: 12
Training loss: 3.2174322605133057
Validation loss: 2.673356830432851

Epoch: 6| Step: 13
Training loss: 3.0950870513916016
Validation loss: 2.6719653401323544

Epoch: 19| Step: 0
Training loss: 3.1404101848602295
Validation loss: 2.6738062109998477

Epoch: 6| Step: 1
Training loss: 3.269651412963867
Validation loss: 2.6773764882036435

Epoch: 6| Step: 2
Training loss: 2.046780586242676
Validation loss: 2.6916077162629817

Epoch: 6| Step: 3
Training loss: 2.7832233905792236
Validation loss: 2.7097042017085577

Epoch: 6| Step: 4
Training loss: 3.0183424949645996
Validation loss: 2.693845191309529

Epoch: 6| Step: 5
Training loss: 3.0302352905273438
Validation loss: 2.6846752935840237

Epoch: 6| Step: 6
Training loss: 2.509112596511841
Validation loss: 2.6744925283616587

Epoch: 6| Step: 7
Training loss: 2.8937628269195557
Validation loss: 2.6715597619292555

Epoch: 6| Step: 8
Training loss: 3.582986831665039
Validation loss: 2.6616217346601587

Epoch: 6| Step: 9
Training loss: 2.631911277770996
Validation loss: 2.657104322987218

Epoch: 6| Step: 10
Training loss: 2.1726880073547363
Validation loss: 2.664515492736652

Epoch: 6| Step: 11
Training loss: 2.8867170810699463
Validation loss: 2.6720365862692557

Epoch: 6| Step: 12
Training loss: 3.068904399871826
Validation loss: 2.672081983217629

Epoch: 6| Step: 13
Training loss: 2.863722324371338
Validation loss: 2.6589291095733643

Epoch: 20| Step: 0
Training loss: 3.1731624603271484
Validation loss: 2.644970565713862

Epoch: 6| Step: 1
Training loss: 3.388761043548584
Validation loss: 2.646873349784523

Epoch: 6| Step: 2
Training loss: 2.781771659851074
Validation loss: 2.6468103201158586

Epoch: 6| Step: 3
Training loss: 3.4991042613983154
Validation loss: 2.6459451413923696

Epoch: 6| Step: 4
Training loss: 2.4483494758605957
Validation loss: 2.6480455578014417

Epoch: 6| Step: 5
Training loss: 1.706665277481079
Validation loss: 2.6490482361085954

Epoch: 6| Step: 6
Training loss: 2.2270913124084473
Validation loss: 2.6560604828660206

Epoch: 6| Step: 7
Training loss: 3.171247959136963
Validation loss: 2.663514885851132

Epoch: 6| Step: 8
Training loss: 2.7677690982818604
Validation loss: 2.6584101184721916

Epoch: 6| Step: 9
Training loss: 2.709268569946289
Validation loss: 2.6464030435008388

Epoch: 6| Step: 10
Training loss: 3.0077872276306152
Validation loss: 2.6423507967302875

Epoch: 6| Step: 11
Training loss: 2.4016458988189697
Validation loss: 2.632998763874013

Epoch: 6| Step: 12
Training loss: 3.045652389526367
Validation loss: 2.6302815637280865

Epoch: 6| Step: 13
Training loss: 3.4954957962036133
Validation loss: 2.6270569268093316

Epoch: 21| Step: 0
Training loss: 2.466623306274414
Validation loss: 2.629071827857725

Epoch: 6| Step: 1
Training loss: 3.069888114929199
Validation loss: 2.6316289530005506

Epoch: 6| Step: 2
Training loss: 2.670236349105835
Validation loss: 2.634101490820608

Epoch: 6| Step: 3
Training loss: 2.9466660022735596
Validation loss: 2.6249980183057886

Epoch: 6| Step: 4
Training loss: 3.3324294090270996
Validation loss: 2.622800560407741

Epoch: 6| Step: 5
Training loss: 2.5586068630218506
Validation loss: 2.6241872169638194

Epoch: 6| Step: 6
Training loss: 2.7082998752593994
Validation loss: 2.6238034258606615

Epoch: 6| Step: 7
Training loss: 2.4613637924194336
Validation loss: 2.629089347777828

Epoch: 6| Step: 8
Training loss: 2.4355080127716064
Validation loss: 2.63577857581518

Epoch: 6| Step: 9
Training loss: 2.9708640575408936
Validation loss: 2.6472156073457453

Epoch: 6| Step: 10
Training loss: 3.3988451957702637
Validation loss: 2.6444821408999863

Epoch: 6| Step: 11
Training loss: 2.82721209526062
Validation loss: 2.631346374429682

Epoch: 6| Step: 12
Training loss: 2.9709744453430176
Validation loss: 2.6115463933637066

Epoch: 6| Step: 13
Training loss: 2.4508538246154785
Validation loss: 2.604651202437698

Epoch: 22| Step: 0
Training loss: 2.9634971618652344
Validation loss: 2.600894199904575

Epoch: 6| Step: 1
Training loss: 2.1538894176483154
Validation loss: 2.6040950949474047

Epoch: 6| Step: 2
Training loss: 2.640101909637451
Validation loss: 2.6040002146074848

Epoch: 6| Step: 3
Training loss: 2.907905340194702
Validation loss: 2.60575008392334

Epoch: 6| Step: 4
Training loss: 2.9042630195617676
Validation loss: 2.6039441067685365

Epoch: 6| Step: 5
Training loss: 2.7547760009765625
Validation loss: 2.6009742777834655

Epoch: 6| Step: 6
Training loss: 2.485707998275757
Validation loss: 2.5921949904452086

Epoch: 6| Step: 7
Training loss: 3.128107786178589
Validation loss: 2.59412915988635

Epoch: 6| Step: 8
Training loss: 3.050600528717041
Validation loss: 2.589816298536075

Epoch: 6| Step: 9
Training loss: 2.7179036140441895
Validation loss: 2.5868740235605547

Epoch: 6| Step: 10
Training loss: 2.2746572494506836
Validation loss: 2.585072978850334

Epoch: 6| Step: 11
Training loss: 3.5224666595458984
Validation loss: 2.5833801197749313

Epoch: 6| Step: 12
Training loss: 3.1135854721069336
Validation loss: 2.581632826917915

Epoch: 6| Step: 13
Training loss: 2.2621493339538574
Validation loss: 2.5808469813357116

Epoch: 23| Step: 0
Training loss: 2.7360854148864746
Validation loss: 2.5777274177920435

Epoch: 6| Step: 1
Training loss: 2.7370247840881348
Validation loss: 2.5837974548339844

Epoch: 6| Step: 2
Training loss: 3.1367831230163574
Validation loss: 2.5817191472617527

Epoch: 6| Step: 3
Training loss: 2.281928777694702
Validation loss: 2.5866598352309196

Epoch: 6| Step: 4
Training loss: 3.2883148193359375
Validation loss: 2.5847076498052126

Epoch: 6| Step: 5
Training loss: 2.502880096435547
Validation loss: 2.576415874624765

Epoch: 6| Step: 6
Training loss: 2.9082632064819336
Validation loss: 2.5766581937830937

Epoch: 6| Step: 7
Training loss: 2.4653241634368896
Validation loss: 2.576940469844367

Epoch: 6| Step: 8
Training loss: 3.199636936187744
Validation loss: 2.5740553050912838

Epoch: 6| Step: 9
Training loss: 2.837559223175049
Validation loss: 2.5762670886132026

Epoch: 6| Step: 10
Training loss: 3.2158381938934326
Validation loss: 2.5747289337137693

Epoch: 6| Step: 11
Training loss: 2.240156888961792
Validation loss: 2.572100152251541

Epoch: 6| Step: 12
Training loss: 2.641298532485962
Validation loss: 2.575273698376071

Epoch: 6| Step: 13
Training loss: 2.796583414077759
Validation loss: 2.5707254358517226

Epoch: 24| Step: 0
Training loss: 3.1204018592834473
Validation loss: 2.5791856909310944

Epoch: 6| Step: 1
Training loss: 2.929743766784668
Validation loss: 2.5829126757960164

Epoch: 6| Step: 2
Training loss: 3.601861000061035
Validation loss: 2.5994827362798874

Epoch: 6| Step: 3
Training loss: 3.107102870941162
Validation loss: 2.604537617775702

Epoch: 6| Step: 4
Training loss: 3.1352014541625977
Validation loss: 2.5914380242747646

Epoch: 6| Step: 5
Training loss: 2.716071844100952
Validation loss: 2.589436267011909

Epoch: 6| Step: 6
Training loss: 2.2181732654571533
Validation loss: 2.583649209750596

Epoch: 6| Step: 7
Training loss: 2.9593660831451416
Validation loss: 2.5789074974675334

Epoch: 6| Step: 8
Training loss: 2.9805891513824463
Validation loss: 2.565706117178804

Epoch: 6| Step: 9
Training loss: 2.6187055110931396
Validation loss: 2.560746154477519

Epoch: 6| Step: 10
Training loss: 2.6407597064971924
Validation loss: 2.562027579994612

Epoch: 6| Step: 11
Training loss: 2.1998345851898193
Validation loss: 2.5621982261698735

Epoch: 6| Step: 12
Training loss: 2.3543314933776855
Validation loss: 2.558113064817203

Epoch: 6| Step: 13
Training loss: 2.0290606021881104
Validation loss: 2.558177201978622

Epoch: 25| Step: 0
Training loss: 2.4479243755340576
Validation loss: 2.564806774098386

Epoch: 6| Step: 1
Training loss: 2.3069024085998535
Validation loss: 2.5631072751937376

Epoch: 6| Step: 2
Training loss: 2.38314151763916
Validation loss: 2.5682147574681107

Epoch: 6| Step: 3
Training loss: 2.816953182220459
Validation loss: 2.5602103458937777

Epoch: 6| Step: 4
Training loss: 2.1133179664611816
Validation loss: 2.552872657775879

Epoch: 6| Step: 5
Training loss: 2.4490814208984375
Validation loss: 2.549697350430232

Epoch: 6| Step: 6
Training loss: 3.319638729095459
Validation loss: 2.5487864030304777

Epoch: 6| Step: 7
Training loss: 3.0114994049072266
Validation loss: 2.5492969071993263

Epoch: 6| Step: 8
Training loss: 3.357865571975708
Validation loss: 2.5555460837579544

Epoch: 6| Step: 9
Training loss: 3.015772819519043
Validation loss: 2.559304011765347

Epoch: 6| Step: 10
Training loss: 1.8240272998809814
Validation loss: 2.5460184081908195

Epoch: 6| Step: 11
Training loss: 3.3674802780151367
Validation loss: 2.54920256522394

Epoch: 6| Step: 12
Training loss: 3.522210121154785
Validation loss: 2.544578798355595

Epoch: 6| Step: 13
Training loss: 2.9377236366271973
Validation loss: 2.5469219992237706

Epoch: 26| Step: 0
Training loss: 2.5521974563598633
Validation loss: 2.547208851383578

Epoch: 6| Step: 1
Training loss: 2.709394931793213
Validation loss: 2.548364605954898

Epoch: 6| Step: 2
Training loss: 3.3891916275024414
Validation loss: 2.5473089141230427

Epoch: 6| Step: 3
Training loss: 2.4520275592803955
Validation loss: 2.544108990700014

Epoch: 6| Step: 4
Training loss: 2.9792351722717285
Validation loss: 2.538729957354966

Epoch: 6| Step: 5
Training loss: 2.9548444747924805
Validation loss: 2.539608822073988

Epoch: 6| Step: 6
Training loss: 3.3489437103271484
Validation loss: 2.53384724227331

Epoch: 6| Step: 7
Training loss: 1.6853256225585938
Validation loss: 2.5331652625914542

Epoch: 6| Step: 8
Training loss: 2.6329188346862793
Validation loss: 2.533618985965688

Epoch: 6| Step: 9
Training loss: 2.8455419540405273
Validation loss: 2.5344816254031275

Epoch: 6| Step: 10
Training loss: 2.3595924377441406
Validation loss: 2.542860569492463

Epoch: 6| Step: 11
Training loss: 2.930441379547119
Validation loss: 2.5307021115415838

Epoch: 6| Step: 12
Training loss: 3.4163904190063477
Validation loss: 2.5334574791692916

Epoch: 6| Step: 13
Training loss: 2.0664217472076416
Validation loss: 2.533513817735898

Epoch: 27| Step: 0
Training loss: 2.9001588821411133
Validation loss: 2.529844748076572

Epoch: 6| Step: 1
Training loss: 2.990872383117676
Validation loss: 2.5304062110121532

Epoch: 6| Step: 2
Training loss: 2.810889959335327
Validation loss: 2.5298182451596825

Epoch: 6| Step: 3
Training loss: 3.148695945739746
Validation loss: 2.5319471538707776

Epoch: 6| Step: 4
Training loss: 3.1349196434020996
Validation loss: 2.530386491488385

Epoch: 6| Step: 5
Training loss: 2.6615114212036133
Validation loss: 2.5327703952789307

Epoch: 6| Step: 6
Training loss: 3.033257484436035
Validation loss: 2.5337439813921527

Epoch: 6| Step: 7
Training loss: 1.993077039718628
Validation loss: 2.531441806465067

Epoch: 6| Step: 8
Training loss: 2.683408737182617
Validation loss: 2.528644082366779

Epoch: 6| Step: 9
Training loss: 2.721079111099243
Validation loss: 2.530664708024712

Epoch: 6| Step: 10
Training loss: 2.2299304008483887
Validation loss: 2.52770745882424

Epoch: 6| Step: 11
Training loss: 3.18890643119812
Validation loss: 2.5245659582076536

Epoch: 6| Step: 12
Training loss: 2.565171241760254
Validation loss: 2.524700298104235

Epoch: 6| Step: 13
Training loss: 2.2718005180358887
Validation loss: 2.521866657400644

Epoch: 28| Step: 0
Training loss: 2.6845908164978027
Validation loss: 2.5257627169291177

Epoch: 6| Step: 1
Training loss: 2.8736867904663086
Validation loss: 2.5220134822271203

Epoch: 6| Step: 2
Training loss: 2.698354482650757
Validation loss: 2.520571708679199

Epoch: 6| Step: 3
Training loss: 2.520411968231201
Validation loss: 2.526798777682807

Epoch: 6| Step: 4
Training loss: 2.807924509048462
Validation loss: 2.5259373700746925

Epoch: 6| Step: 5
Training loss: 3.3688297271728516
Validation loss: 2.5246386835652013

Epoch: 6| Step: 6
Training loss: 2.829986095428467
Validation loss: 2.522397659158194

Epoch: 6| Step: 7
Training loss: 2.622830867767334
Validation loss: 2.521948117081837

Epoch: 6| Step: 8
Training loss: 2.8440604209899902
Validation loss: 2.52166247880587

Epoch: 6| Step: 9
Training loss: 1.9848438501358032
Validation loss: 2.520443295919767

Epoch: 6| Step: 10
Training loss: 2.5385243892669678
Validation loss: 2.5222761400284304

Epoch: 6| Step: 11
Training loss: 2.899918556213379
Validation loss: 2.521596339441115

Epoch: 6| Step: 12
Training loss: 2.8710849285125732
Validation loss: 2.518387735530894

Epoch: 6| Step: 13
Training loss: 3.1057116985321045
Validation loss: 2.519415297815877

Epoch: 29| Step: 0
Training loss: 2.380105972290039
Validation loss: 2.5166628053111415

Epoch: 6| Step: 1
Training loss: 3.1366934776306152
Validation loss: 2.5177462100982666

Epoch: 6| Step: 2
Training loss: 2.2698159217834473
Validation loss: 2.513763596934657

Epoch: 6| Step: 3
Training loss: 2.8974719047546387
Validation loss: 2.5191570046127483

Epoch: 6| Step: 4
Training loss: 3.2155332565307617
Validation loss: 2.515976375149142

Epoch: 6| Step: 5
Training loss: 3.081998348236084
Validation loss: 2.5134130165141118

Epoch: 6| Step: 6
Training loss: 2.1242856979370117
Validation loss: 2.5121199571958153

Epoch: 6| Step: 7
Training loss: 2.445465087890625
Validation loss: 2.509317692889962

Epoch: 6| Step: 8
Training loss: 2.688459873199463
Validation loss: 2.511362960261683

Epoch: 6| Step: 9
Training loss: 2.4780776500701904
Validation loss: 2.51063968289283

Epoch: 6| Step: 10
Training loss: 2.9087884426116943
Validation loss: 2.5153503905060473

Epoch: 6| Step: 11
Training loss: 3.03640079498291
Validation loss: 2.518134404254216

Epoch: 6| Step: 12
Training loss: 2.881075620651245
Validation loss: 2.531815751906364

Epoch: 6| Step: 13
Training loss: 3.012035369873047
Validation loss: 2.5347909927368164

Epoch: 30| Step: 0
Training loss: 2.2806365489959717
Validation loss: 2.516006895290908

Epoch: 6| Step: 1
Training loss: 2.703752040863037
Validation loss: 2.53428283814461

Epoch: 6| Step: 2
Training loss: 3.0764293670654297
Validation loss: 2.5652301183310886

Epoch: 6| Step: 3
Training loss: 3.540384292602539
Validation loss: 2.5945420521561817

Epoch: 6| Step: 4
Training loss: 2.4763641357421875
Validation loss: 2.541343968401673

Epoch: 6| Step: 5
Training loss: 3.2487244606018066
Validation loss: 2.5301356315612793

Epoch: 6| Step: 6
Training loss: 2.4629645347595215
Validation loss: 2.5216787092147337

Epoch: 6| Step: 7
Training loss: 2.1088249683380127
Validation loss: 2.524103254400274

Epoch: 6| Step: 8
Training loss: 2.5302748680114746
Validation loss: 2.537276027023151

Epoch: 6| Step: 9
Training loss: 2.283107280731201
Validation loss: 2.547090268904163

Epoch: 6| Step: 10
Training loss: 2.751270294189453
Validation loss: 2.559850254366475

Epoch: 6| Step: 11
Training loss: 3.347411870956421
Validation loss: 2.59693266243063

Epoch: 6| Step: 12
Training loss: 2.922696828842163
Validation loss: 2.555493095869659

Epoch: 6| Step: 13
Training loss: 3.1093034744262695
Validation loss: 2.539523078549293

Epoch: 31| Step: 0
Training loss: 2.8179802894592285
Validation loss: 2.531684639633343

Epoch: 6| Step: 1
Training loss: 2.67734956741333
Validation loss: 2.5244375941573933

Epoch: 6| Step: 2
Training loss: 2.800095558166504
Validation loss: 2.5204348282147477

Epoch: 6| Step: 3
Training loss: 3.232412338256836
Validation loss: 2.516999336981004

Epoch: 6| Step: 4
Training loss: 2.0802412033081055
Validation loss: 2.517393640292588

Epoch: 6| Step: 5
Training loss: 3.5429556369781494
Validation loss: 2.521694434586392

Epoch: 6| Step: 6
Training loss: 3.1996796131134033
Validation loss: 2.5286859825093257

Epoch: 6| Step: 7
Training loss: 3.130106210708618
Validation loss: 2.5176955935775593

Epoch: 6| Step: 8
Training loss: 2.7613043785095215
Validation loss: 2.511550869992984

Epoch: 6| Step: 9
Training loss: 2.3044545650482178
Validation loss: 2.5098244759344284

Epoch: 6| Step: 10
Training loss: 2.211758613586426
Validation loss: 2.5063064380358626

Epoch: 6| Step: 11
Training loss: 2.901028633117676
Validation loss: 2.5123280094515894

Epoch: 6| Step: 12
Training loss: 2.457643508911133
Validation loss: 2.5122728604142384

Epoch: 6| Step: 13
Training loss: 2.0077168941497803
Validation loss: 2.526913394210159

Epoch: 32| Step: 0
Training loss: 2.9566383361816406
Validation loss: 2.534837007522583

Epoch: 6| Step: 1
Training loss: 2.6498985290527344
Validation loss: 2.536455108273414

Epoch: 6| Step: 2
Training loss: 2.672900676727295
Validation loss: 2.5357893590004212

Epoch: 6| Step: 3
Training loss: 3.327296257019043
Validation loss: 2.5205533478849675

Epoch: 6| Step: 4
Training loss: 2.293921709060669
Validation loss: 2.506104456481113

Epoch: 6| Step: 5
Training loss: 3.041985034942627
Validation loss: 2.5007236824240735

Epoch: 6| Step: 6
Training loss: 3.3541016578674316
Validation loss: 2.496528425524312

Epoch: 6| Step: 7
Training loss: 2.6867499351501465
Validation loss: 2.5011885294350247

Epoch: 6| Step: 8
Training loss: 2.542318344116211
Validation loss: 2.5030854081594818

Epoch: 6| Step: 9
Training loss: 2.0447473526000977
Validation loss: 2.4982772770748345

Epoch: 6| Step: 10
Training loss: 2.9293365478515625
Validation loss: 2.500777772677842

Epoch: 6| Step: 11
Training loss: 2.540949821472168
Validation loss: 2.4969094363592004

Epoch: 6| Step: 12
Training loss: 2.474236249923706
Validation loss: 2.4986138625811507

Epoch: 6| Step: 13
Training loss: 3.0248193740844727
Validation loss: 2.501091323873048

Epoch: 33| Step: 0
Training loss: 2.405378580093384
Validation loss: 2.503331104914347

Epoch: 6| Step: 1
Training loss: 2.5685408115386963
Validation loss: 2.5026404575635026

Epoch: 6| Step: 2
Training loss: 2.6581764221191406
Validation loss: 2.506075425814557

Epoch: 6| Step: 3
Training loss: 3.0130858421325684
Validation loss: 2.501762766991892

Epoch: 6| Step: 4
Training loss: 2.4865503311157227
Validation loss: 2.5024727031748784

Epoch: 6| Step: 5
Training loss: 2.6769301891326904
Validation loss: 2.500176998876756

Epoch: 6| Step: 6
Training loss: 2.7602758407592773
Validation loss: 2.500400394521734

Epoch: 6| Step: 7
Training loss: 3.07177472114563
Validation loss: 2.498542349825623

Epoch: 6| Step: 8
Training loss: 2.4336483478546143
Validation loss: 2.50044854482015

Epoch: 6| Step: 9
Training loss: 2.5915727615356445
Validation loss: 2.49816103904478

Epoch: 6| Step: 10
Training loss: 3.689028024673462
Validation loss: 2.499100605646769

Epoch: 6| Step: 11
Training loss: 2.965003728866577
Validation loss: 2.5026703931952037

Epoch: 6| Step: 12
Training loss: 2.8204846382141113
Validation loss: 2.494397004445394

Epoch: 6| Step: 13
Training loss: 1.4456647634506226
Validation loss: 2.5000265106078117

Epoch: 34| Step: 0
Training loss: 2.470315456390381
Validation loss: 2.5116235363867974

Epoch: 6| Step: 1
Training loss: 2.356785774230957
Validation loss: 2.5289987159031693

Epoch: 6| Step: 2
Training loss: 3.0821726322174072
Validation loss: 2.5344856759553314

Epoch: 6| Step: 3
Training loss: 2.4973108768463135
Validation loss: 2.5197481929614978

Epoch: 6| Step: 4
Training loss: 2.2949278354644775
Validation loss: 2.507739156805059

Epoch: 6| Step: 5
Training loss: 2.4073307514190674
Validation loss: 2.4980203003011723

Epoch: 6| Step: 6
Training loss: 3.506049156188965
Validation loss: 2.495523509158883

Epoch: 6| Step: 7
Training loss: 2.4196953773498535
Validation loss: 2.496188614958076

Epoch: 6| Step: 8
Training loss: 3.0679197311401367
Validation loss: 2.4979164369644655

Epoch: 6| Step: 9
Training loss: 2.9179279804229736
Validation loss: 2.496472963722803

Epoch: 6| Step: 10
Training loss: 2.934567451477051
Validation loss: 2.493853909994966

Epoch: 6| Step: 11
Training loss: 2.953540325164795
Validation loss: 2.489148944936773

Epoch: 6| Step: 12
Training loss: 2.8590118885040283
Validation loss: 2.491460648916101

Epoch: 6| Step: 13
Training loss: 2.1837575435638428
Validation loss: 2.4957343147646998

Epoch: 35| Step: 0
Training loss: 2.0309815406799316
Validation loss: 2.491921679947966

Epoch: 6| Step: 1
Training loss: 3.4037554264068604
Validation loss: 2.4891293125767864

Epoch: 6| Step: 2
Training loss: 2.1934189796447754
Validation loss: 2.4892869226394163

Epoch: 6| Step: 3
Training loss: 2.3460631370544434
Validation loss: 2.4867147296987553

Epoch: 6| Step: 4
Training loss: 3.635371685028076
Validation loss: 2.488379996309998

Epoch: 6| Step: 5
Training loss: 3.260776996612549
Validation loss: 2.4861070417588755

Epoch: 6| Step: 6
Training loss: 3.05635404586792
Validation loss: 2.4833809201435377

Epoch: 6| Step: 7
Training loss: 2.1329338550567627
Validation loss: 2.4826365517031763

Epoch: 6| Step: 8
Training loss: 2.899045467376709
Validation loss: 2.4821402449761667

Epoch: 6| Step: 9
Training loss: 2.902618885040283
Validation loss: 2.480384785641906

Epoch: 6| Step: 10
Training loss: 3.1622653007507324
Validation loss: 2.4844377579227572

Epoch: 6| Step: 11
Training loss: 2.495321750640869
Validation loss: 2.4851438768448366

Epoch: 6| Step: 12
Training loss: 2.167804718017578
Validation loss: 2.48689861964154

Epoch: 6| Step: 13
Training loss: 1.8723458051681519
Validation loss: 2.496093362890264

Epoch: 36| Step: 0
Training loss: 2.912773609161377
Validation loss: 2.511628996941351

Epoch: 6| Step: 1
Training loss: 2.3371241092681885
Validation loss: 2.5212681831852084

Epoch: 6| Step: 2
Training loss: 3.223196506500244
Validation loss: 2.510914517987159

Epoch: 6| Step: 3
Training loss: 3.426039457321167
Validation loss: 2.5047849121914116

Epoch: 6| Step: 4
Training loss: 2.554398775100708
Validation loss: 2.4904969840921383

Epoch: 6| Step: 5
Training loss: 2.845761299133301
Validation loss: 2.4774863925031436

Epoch: 6| Step: 6
Training loss: 2.1574244499206543
Validation loss: 2.475571391403034

Epoch: 6| Step: 7
Training loss: 2.4182939529418945
Validation loss: 2.4806403216495307

Epoch: 6| Step: 8
Training loss: 1.6505169868469238
Validation loss: 2.4810259111465944

Epoch: 6| Step: 9
Training loss: 2.7157773971557617
Validation loss: 2.4799988859443256

Epoch: 6| Step: 10
Training loss: 2.470733165740967
Validation loss: 2.4796097586231847

Epoch: 6| Step: 11
Training loss: 3.203965187072754
Validation loss: 2.4783085597458707

Epoch: 6| Step: 12
Training loss: 3.3242669105529785
Validation loss: 2.47635668323886

Epoch: 6| Step: 13
Training loss: 2.821054220199585
Validation loss: 2.4741727716179303

Epoch: 37| Step: 0
Training loss: 2.0941805839538574
Validation loss: 2.47369969788418

Epoch: 6| Step: 1
Training loss: 2.375338554382324
Validation loss: 2.4763565935114378

Epoch: 6| Step: 2
Training loss: 2.7964043617248535
Validation loss: 2.4750061573520785

Epoch: 6| Step: 3
Training loss: 2.7281367778778076
Validation loss: 2.4844040345120173

Epoch: 6| Step: 4
Training loss: 3.0049476623535156
Validation loss: 2.4953893179534585

Epoch: 6| Step: 5
Training loss: 2.596736431121826
Validation loss: 2.4901004055494904

Epoch: 6| Step: 6
Training loss: 2.361172676086426
Validation loss: 2.494348669564852

Epoch: 6| Step: 7
Training loss: 2.4966325759887695
Validation loss: 2.4882670756309264

Epoch: 6| Step: 8
Training loss: 2.7942676544189453
Validation loss: 2.4858353304606613

Epoch: 6| Step: 9
Training loss: 3.1193275451660156
Validation loss: 2.4794718578297603

Epoch: 6| Step: 10
Training loss: 2.9283487796783447
Validation loss: 2.474184654092276

Epoch: 6| Step: 11
Training loss: 2.37032151222229
Validation loss: 2.4722015062967935

Epoch: 6| Step: 12
Training loss: 3.2439236640930176
Validation loss: 2.480192158811836

Epoch: 6| Step: 13
Training loss: 2.9929659366607666
Validation loss: 2.483042468306839

Epoch: 38| Step: 0
Training loss: 3.7496166229248047
Validation loss: 2.5048066954458914

Epoch: 6| Step: 1
Training loss: 3.1774630546569824
Validation loss: 2.5269910391940864

Epoch: 6| Step: 2
Training loss: 1.9835689067840576
Validation loss: 2.4880058509047314

Epoch: 6| Step: 3
Training loss: 3.363130569458008
Validation loss: 2.473118282133533

Epoch: 6| Step: 4
Training loss: 3.6100010871887207
Validation loss: 2.4667694799361692

Epoch: 6| Step: 5
Training loss: 2.5966498851776123
Validation loss: 2.468818490223218

Epoch: 6| Step: 6
Training loss: 2.530879497528076
Validation loss: 2.484075802628712

Epoch: 6| Step: 7
Training loss: 1.5368854999542236
Validation loss: 2.4905164318699993

Epoch: 6| Step: 8
Training loss: 2.4628562927246094
Validation loss: 2.493037303288778

Epoch: 6| Step: 9
Training loss: 2.6727688312530518
Validation loss: 2.490356142802905

Epoch: 6| Step: 10
Training loss: 2.7596826553344727
Validation loss: 2.4948839295294976

Epoch: 6| Step: 11
Training loss: 2.4678566455841064
Validation loss: 2.4911779844632713

Epoch: 6| Step: 12
Training loss: 2.490281105041504
Validation loss: 2.493197082191385

Epoch: 6| Step: 13
Training loss: 2.355102300643921
Validation loss: 2.4906086511509393

Epoch: 39| Step: 0
Training loss: 2.948361396789551
Validation loss: 2.496701548176427

Epoch: 6| Step: 1
Training loss: 2.4940083026885986
Validation loss: 2.4970527464343655

Epoch: 6| Step: 2
Training loss: 2.770850658416748
Validation loss: 2.4922710054664203

Epoch: 6| Step: 3
Training loss: 2.1392822265625
Validation loss: 2.4882771456113426

Epoch: 6| Step: 4
Training loss: 2.7059805393218994
Validation loss: 2.4912875621549544

Epoch: 6| Step: 5
Training loss: 2.9198789596557617
Validation loss: 2.483204509622307

Epoch: 6| Step: 6
Training loss: 2.6018567085266113
Validation loss: 2.475110420616724

Epoch: 6| Step: 7
Training loss: 1.7984697818756104
Validation loss: 2.4661501505041636

Epoch: 6| Step: 8
Training loss: 2.7666866779327393
Validation loss: 2.466421029900992

Epoch: 6| Step: 9
Training loss: 3.489595413208008
Validation loss: 2.467736287783551

Epoch: 6| Step: 10
Training loss: 2.3260598182678223
Validation loss: 2.466460630457888

Epoch: 6| Step: 11
Training loss: 2.974241256713867
Validation loss: 2.4661310770178355

Epoch: 6| Step: 12
Training loss: 3.2204926013946533
Validation loss: 2.463226108140843

Epoch: 6| Step: 13
Training loss: 2.54544997215271
Validation loss: 2.4623011722359607

Epoch: 40| Step: 0
Training loss: 2.707350730895996
Validation loss: 2.4645859041521625

Epoch: 6| Step: 1
Training loss: 2.3396456241607666
Validation loss: 2.4625986224861554

Epoch: 6| Step: 2
Training loss: 2.4214887619018555
Validation loss: 2.4646295065520913

Epoch: 6| Step: 3
Training loss: 2.0679335594177246
Validation loss: 2.4689088713738228

Epoch: 6| Step: 4
Training loss: 2.4225656986236572
Validation loss: 2.4642915879526446

Epoch: 6| Step: 5
Training loss: 2.9169843196868896
Validation loss: 2.4624397241941063

Epoch: 6| Step: 6
Training loss: 2.300276517868042
Validation loss: 2.4659863236129924

Epoch: 6| Step: 7
Training loss: 2.932999610900879
Validation loss: 2.4644331829522246

Epoch: 6| Step: 8
Training loss: 3.4051742553710938
Validation loss: 2.4630036687338226

Epoch: 6| Step: 9
Training loss: 3.033592462539673
Validation loss: 2.4637517621440272

Epoch: 6| Step: 10
Training loss: 2.546757459640503
Validation loss: 2.4561842590249996

Epoch: 6| Step: 11
Training loss: 2.3900654315948486
Validation loss: 2.4669183018387004

Epoch: 6| Step: 12
Training loss: 3.025698661804199
Validation loss: 2.4819877762948312

Epoch: 6| Step: 13
Training loss: 3.5801243782043457
Validation loss: 2.5560708045959473

Epoch: 41| Step: 0
Training loss: 2.3515665531158447
Validation loss: 2.567863097754858

Epoch: 6| Step: 1
Training loss: 3.062248706817627
Validation loss: 2.5618435977607645

Epoch: 6| Step: 2
Training loss: 3.087052345275879
Validation loss: 2.5438496066677954

Epoch: 6| Step: 3
Training loss: 2.860206365585327
Validation loss: 2.5322805322626585

Epoch: 6| Step: 4
Training loss: 3.1464526653289795
Validation loss: 2.514654301827954

Epoch: 6| Step: 5
Training loss: 2.673783302307129
Validation loss: 2.5048624315569477

Epoch: 6| Step: 6
Training loss: 3.4099068641662598
Validation loss: 2.4960219296075965

Epoch: 6| Step: 7
Training loss: 2.8002803325653076
Validation loss: 2.4916789454798542

Epoch: 6| Step: 8
Training loss: 2.2931532859802246
Validation loss: 2.4732722184991323

Epoch: 6| Step: 9
Training loss: 2.4131832122802734
Validation loss: 2.465398644888273

Epoch: 6| Step: 10
Training loss: 2.1978871822357178
Validation loss: 2.4662501427435104

Epoch: 6| Step: 11
Training loss: 2.6412293910980225
Validation loss: 2.4791362695796515

Epoch: 6| Step: 12
Training loss: 2.2760841846466064
Validation loss: 2.491529526249055

Epoch: 6| Step: 13
Training loss: 2.988696575164795
Validation loss: 2.490431216455275

Epoch: 42| Step: 0
Training loss: 2.622337818145752
Validation loss: 2.499226698311426

Epoch: 6| Step: 1
Training loss: 3.180272102355957
Validation loss: 2.4812967405524304

Epoch: 6| Step: 2
Training loss: 3.1069207191467285
Validation loss: 2.474673007124214

Epoch: 6| Step: 3
Training loss: 1.9656062126159668
Validation loss: 2.468992802404588

Epoch: 6| Step: 4
Training loss: 3.1321260929107666
Validation loss: 2.4735687701932845

Epoch: 6| Step: 5
Training loss: 2.3358616828918457
Validation loss: 2.465025491611932

Epoch: 6| Step: 6
Training loss: 2.772249221801758
Validation loss: 2.460647077970607

Epoch: 6| Step: 7
Training loss: 3.0750980377197266
Validation loss: 2.4610113328503025

Epoch: 6| Step: 8
Training loss: 3.0900681018829346
Validation loss: 2.4558759197112052

Epoch: 6| Step: 9
Training loss: 2.4067602157592773
Validation loss: 2.4520679289294827

Epoch: 6| Step: 10
Training loss: 1.5720133781433105
Validation loss: 2.4517220886804725

Epoch: 6| Step: 11
Training loss: 2.9518165588378906
Validation loss: 2.4510763588772027

Epoch: 6| Step: 12
Training loss: 2.8828279972076416
Validation loss: 2.456395064630816

Epoch: 6| Step: 13
Training loss: 2.2758982181549072
Validation loss: 2.4577073538175194

Epoch: 43| Step: 0
Training loss: 2.9558348655700684
Validation loss: 2.4768171669334493

Epoch: 6| Step: 1
Training loss: 2.954641342163086
Validation loss: 2.466764619273524

Epoch: 6| Step: 2
Training loss: 2.20597767829895
Validation loss: 2.4639376876174763

Epoch: 6| Step: 3
Training loss: 2.549616813659668
Validation loss: 2.462080088994836

Epoch: 6| Step: 4
Training loss: 2.8133015632629395
Validation loss: 2.4521143487704697

Epoch: 6| Step: 5
Training loss: 2.618715524673462
Validation loss: 2.446871271697424

Epoch: 6| Step: 6
Training loss: 2.6887283325195312
Validation loss: 2.4414788599937194

Epoch: 6| Step: 7
Training loss: 2.826979398727417
Validation loss: 2.439574485184044

Epoch: 6| Step: 8
Training loss: 2.489881992340088
Validation loss: 2.4377600659606276

Epoch: 6| Step: 9
Training loss: 2.799628496170044
Validation loss: 2.436850522154121

Epoch: 6| Step: 10
Training loss: 2.992386817932129
Validation loss: 2.433695629078855

Epoch: 6| Step: 11
Training loss: 2.3976285457611084
Validation loss: 2.4348368490895917

Epoch: 6| Step: 12
Training loss: 3.089630603790283
Validation loss: 2.4330558315400155

Epoch: 6| Step: 13
Training loss: 1.6201282739639282
Validation loss: 2.4319439831600396

Epoch: 44| Step: 0
Training loss: 2.333466053009033
Validation loss: 2.4356611133903585

Epoch: 6| Step: 1
Training loss: 1.9900015592575073
Validation loss: 2.438494333656885

Epoch: 6| Step: 2
Training loss: 2.5283150672912598
Validation loss: 2.4437010493329776

Epoch: 6| Step: 3
Training loss: 2.1962740421295166
Validation loss: 2.452156525786205

Epoch: 6| Step: 4
Training loss: 3.506700038909912
Validation loss: 2.466419795508026

Epoch: 6| Step: 5
Training loss: 2.9206881523132324
Validation loss: 2.4895606528046312

Epoch: 6| Step: 6
Training loss: 3.041734218597412
Validation loss: 2.507506570508403

Epoch: 6| Step: 7
Training loss: 2.650998592376709
Validation loss: 2.509495125021986

Epoch: 6| Step: 8
Training loss: 2.998910665512085
Validation loss: 2.501450023343486

Epoch: 6| Step: 9
Training loss: 2.4192845821380615
Validation loss: 2.4698293234712336

Epoch: 6| Step: 10
Training loss: 2.7644917964935303
Validation loss: 2.451881559946204

Epoch: 6| Step: 11
Training loss: 3.1792593002319336
Validation loss: 2.4341767282896143

Epoch: 6| Step: 12
Training loss: 2.281582832336426
Validation loss: 2.4293503325472594

Epoch: 6| Step: 13
Training loss: 2.4924914836883545
Validation loss: 2.428308427974742

Epoch: 45| Step: 0
Training loss: 2.7181239128112793
Validation loss: 2.428622576498216

Epoch: 6| Step: 1
Training loss: 2.540004014968872
Validation loss: 2.439620453824279

Epoch: 6| Step: 2
Training loss: 2.6594667434692383
Validation loss: 2.4426157448881414

Epoch: 6| Step: 3
Training loss: 2.5002541542053223
Validation loss: 2.4394911412269837

Epoch: 6| Step: 4
Training loss: 2.4822897911071777
Validation loss: 2.4350679946202103

Epoch: 6| Step: 5
Training loss: 2.1760504245758057
Validation loss: 2.4327769946026545

Epoch: 6| Step: 6
Training loss: 2.5737438201904297
Validation loss: 2.431419362304031

Epoch: 6| Step: 7
Training loss: 3.1486525535583496
Validation loss: 2.4283174494261384

Epoch: 6| Step: 8
Training loss: 2.8546695709228516
Validation loss: 2.4332144901316655

Epoch: 6| Step: 9
Training loss: 2.676356792449951
Validation loss: 2.4255046972664456

Epoch: 6| Step: 10
Training loss: 1.7836079597473145
Validation loss: 2.4255083965998825

Epoch: 6| Step: 11
Training loss: 2.819335699081421
Validation loss: 2.4301134335097445

Epoch: 6| Step: 12
Training loss: 3.759786367416382
Validation loss: 2.4418570251875025

Epoch: 6| Step: 13
Training loss: 2.743791341781616
Validation loss: 2.4656875056605183

Epoch: 46| Step: 0
Training loss: 3.1649324893951416
Validation loss: 2.4898709456125894

Epoch: 6| Step: 1
Training loss: 2.382476329803467
Validation loss: 2.4885597664822816

Epoch: 6| Step: 2
Training loss: 2.8624277114868164
Validation loss: 2.496010375279252

Epoch: 6| Step: 3
Training loss: 2.77311635017395
Validation loss: 2.4715702892631612

Epoch: 6| Step: 4
Training loss: 2.248335361480713
Validation loss: 2.4637724302148305

Epoch: 6| Step: 5
Training loss: 2.7114787101745605
Validation loss: 2.460971483620264

Epoch: 6| Step: 6
Training loss: 2.551527261734009
Validation loss: 2.4541616029636835

Epoch: 6| Step: 7
Training loss: 2.6864447593688965
Validation loss: 2.4498101908673524

Epoch: 6| Step: 8
Training loss: 2.3692281246185303
Validation loss: 2.438665231068929

Epoch: 6| Step: 9
Training loss: 2.7562341690063477
Validation loss: 2.4316957971101165

Epoch: 6| Step: 10
Training loss: 2.332817554473877
Validation loss: 2.425290555082342

Epoch: 6| Step: 11
Training loss: 2.662655830383301
Validation loss: 2.4255383258224814

Epoch: 6| Step: 12
Training loss: 3.19944429397583
Validation loss: 2.42026093698317

Epoch: 6| Step: 13
Training loss: 2.419489622116089
Validation loss: 2.425244008341143

Epoch: 47| Step: 0
Training loss: 2.3593974113464355
Validation loss: 2.420839491710868

Epoch: 6| Step: 1
Training loss: 2.485482692718506
Validation loss: 2.422118866315452

Epoch: 6| Step: 2
Training loss: 2.783874750137329
Validation loss: 2.433815280596415

Epoch: 6| Step: 3
Training loss: 2.9435977935791016
Validation loss: 2.4432076433653473

Epoch: 6| Step: 4
Training loss: 3.896303176879883
Validation loss: 2.4378402925306752

Epoch: 6| Step: 5
Training loss: 2.932509422302246
Validation loss: 2.4375648216534684

Epoch: 6| Step: 6
Training loss: 1.5334281921386719
Validation loss: 2.434777211117488

Epoch: 6| Step: 7
Training loss: 2.1306865215301514
Validation loss: 2.4260474456253873

Epoch: 6| Step: 8
Training loss: 2.9544870853424072
Validation loss: 2.422723880378149

Epoch: 6| Step: 9
Training loss: 2.3805348873138428
Validation loss: 2.4212008599312074

Epoch: 6| Step: 10
Training loss: 2.828807830810547
Validation loss: 2.4178675092676634

Epoch: 6| Step: 11
Training loss: 2.1569478511810303
Validation loss: 2.4132884599829234

Epoch: 6| Step: 12
Training loss: 3.21441650390625
Validation loss: 2.415046194548248

Epoch: 6| Step: 13
Training loss: 2.4829347133636475
Validation loss: 2.4243449575157574

Epoch: 48| Step: 0
Training loss: 3.0486178398132324
Validation loss: 2.4403632071710404

Epoch: 6| Step: 1
Training loss: 2.783731698989868
Validation loss: 2.472550792078818

Epoch: 6| Step: 2
Training loss: 2.7617344856262207
Validation loss: 2.497414964501576

Epoch: 6| Step: 3
Training loss: 1.9982337951660156
Validation loss: 2.5055505075762348

Epoch: 6| Step: 4
Training loss: 2.3679444789886475
Validation loss: 2.5076192578961773

Epoch: 6| Step: 5
Training loss: 2.6839466094970703
Validation loss: 2.470125716219666

Epoch: 6| Step: 6
Training loss: 2.6784520149230957
Validation loss: 2.4542987039012294

Epoch: 6| Step: 7
Training loss: 2.7207956314086914
Validation loss: 2.4413941624344035

Epoch: 6| Step: 8
Training loss: 3.2203938961029053
Validation loss: 2.4305075112209527

Epoch: 6| Step: 9
Training loss: 2.8081932067871094
Validation loss: 2.4202345314846245

Epoch: 6| Step: 10
Training loss: 1.9905226230621338
Validation loss: 2.4199377952083463

Epoch: 6| Step: 11
Training loss: 2.074206829071045
Validation loss: 2.4252070765341482

Epoch: 6| Step: 12
Training loss: 2.8178725242614746
Validation loss: 2.4313214568681616

Epoch: 6| Step: 13
Training loss: 3.5850605964660645
Validation loss: 2.438364662149901

Epoch: 49| Step: 0
Training loss: 2.5164718627929688
Validation loss: 2.431745744520618

Epoch: 6| Step: 1
Training loss: 2.584355592727661
Validation loss: 2.4189373549594673

Epoch: 6| Step: 2
Training loss: 1.947962760925293
Validation loss: 2.4172204412439817

Epoch: 6| Step: 3
Training loss: 2.6836071014404297
Validation loss: 2.4177950736015075

Epoch: 6| Step: 4
Training loss: 2.847324848175049
Validation loss: 2.414354475595618

Epoch: 6| Step: 5
Training loss: 3.5434632301330566
Validation loss: 2.416560434526013

Epoch: 6| Step: 6
Training loss: 3.0108871459960938
Validation loss: 2.417512587321702

Epoch: 6| Step: 7
Training loss: 2.873760223388672
Validation loss: 2.416617331966277

Epoch: 6| Step: 8
Training loss: 1.8524909019470215
Validation loss: 2.4170466161543325

Epoch: 6| Step: 9
Training loss: 2.591521739959717
Validation loss: 2.411309293521348

Epoch: 6| Step: 10
Training loss: 1.6727471351623535
Validation loss: 2.408681769524851

Epoch: 6| Step: 11
Training loss: 3.91161847114563
Validation loss: 2.416295233593192

Epoch: 6| Step: 12
Training loss: 2.7673697471618652
Validation loss: 2.4247570281387656

Epoch: 6| Step: 13
Training loss: 1.8658430576324463
Validation loss: 2.4291445388588855

Epoch: 50| Step: 0
Training loss: 2.8405938148498535
Validation loss: 2.42023358550123

Epoch: 6| Step: 1
Training loss: 2.130002021789551
Validation loss: 2.423860019253146

Epoch: 6| Step: 2
Training loss: 2.248490571975708
Validation loss: 2.4236781238227763

Epoch: 6| Step: 3
Training loss: 2.925441026687622
Validation loss: 2.4399782970387447

Epoch: 6| Step: 4
Training loss: 1.825942873954773
Validation loss: 2.439675126024472

Epoch: 6| Step: 5
Training loss: 3.48718523979187
Validation loss: 2.4389252329385407

Epoch: 6| Step: 6
Training loss: 3.1339879035949707
Validation loss: 2.4232150764875513

Epoch: 6| Step: 7
Training loss: 3.0176048278808594
Validation loss: 2.410948322665307

Epoch: 6| Step: 8
Training loss: 2.5936317443847656
Validation loss: 2.4040224372699694

Epoch: 6| Step: 9
Training loss: 2.283412456512451
Validation loss: 2.4010348678917013

Epoch: 6| Step: 10
Training loss: 3.3632593154907227
Validation loss: 2.4065887005098405

Epoch: 6| Step: 11
Training loss: 2.547908306121826
Validation loss: 2.4019032421932427

Epoch: 6| Step: 12
Training loss: 2.0858094692230225
Validation loss: 2.41148421841283

Epoch: 6| Step: 13
Training loss: 2.455057382583618
Validation loss: 2.407714069530528

Epoch: 51| Step: 0
Training loss: 2.5550520420074463
Validation loss: 2.4196482396894887

Epoch: 6| Step: 1
Training loss: 2.354437828063965
Validation loss: 2.424630113827285

Epoch: 6| Step: 2
Training loss: 2.893676996231079
Validation loss: 2.4319193517008135

Epoch: 6| Step: 3
Training loss: 2.9210305213928223
Validation loss: 2.4329915533783617

Epoch: 6| Step: 4
Training loss: 2.967350721359253
Validation loss: 2.424617067460091

Epoch: 6| Step: 5
Training loss: 3.5690441131591797
Validation loss: 2.4140312261478876

Epoch: 6| Step: 6
Training loss: 2.109858989715576
Validation loss: 2.406074426507437

Epoch: 6| Step: 7
Training loss: 2.240835428237915
Validation loss: 2.4062843040753434

Epoch: 6| Step: 8
Training loss: 2.961226463317871
Validation loss: 2.4053650004889375

Epoch: 6| Step: 9
Training loss: 2.0218100547790527
Validation loss: 2.4027396735324653

Epoch: 6| Step: 10
Training loss: 2.608619213104248
Validation loss: 2.40557554716705

Epoch: 6| Step: 11
Training loss: 2.301971197128296
Validation loss: 2.415004171350951

Epoch: 6| Step: 12
Training loss: 3.1341607570648193
Validation loss: 2.409968952978811

Epoch: 6| Step: 13
Training loss: 2.094390869140625
Validation loss: 2.413292407989502

Epoch: 52| Step: 0
Training loss: 3.099867820739746
Validation loss: 2.416721454230688

Epoch: 6| Step: 1
Training loss: 2.8068041801452637
Validation loss: 2.4162079262477096

Epoch: 6| Step: 2
Training loss: 2.025017738342285
Validation loss: 2.4044308354777675

Epoch: 6| Step: 3
Training loss: 3.1210150718688965
Validation loss: 2.40504575544788

Epoch: 6| Step: 4
Training loss: 2.7003960609436035
Validation loss: 2.399024914669734

Epoch: 6| Step: 5
Training loss: 2.1103925704956055
Validation loss: 2.3964932503238803

Epoch: 6| Step: 6
Training loss: 3.409229278564453
Validation loss: 2.3961684985827376

Epoch: 6| Step: 7
Training loss: 2.7066335678100586
Validation loss: 2.398578582271453

Epoch: 6| Step: 8
Training loss: 2.0484085083007812
Validation loss: 2.4045842129697084

Epoch: 6| Step: 9
Training loss: 2.1301817893981934
Validation loss: 2.4224319996372348

Epoch: 6| Step: 10
Training loss: 1.9798921346664429
Validation loss: 2.431500473330098

Epoch: 6| Step: 11
Training loss: 2.7542366981506348
Validation loss: 2.437699956278647

Epoch: 6| Step: 12
Training loss: 3.0605692863464355
Validation loss: 2.417311554313988

Epoch: 6| Step: 13
Training loss: 3.3213887214660645
Validation loss: 2.413432651950467

Epoch: 53| Step: 0
Training loss: 2.7639708518981934
Validation loss: 2.4103495356857136

Epoch: 6| Step: 1
Training loss: 2.9077670574188232
Validation loss: 2.405488949950023

Epoch: 6| Step: 2
Training loss: 2.3277196884155273
Validation loss: 2.402582276252008

Epoch: 6| Step: 3
Training loss: 2.406306743621826
Validation loss: 2.40195700686465

Epoch: 6| Step: 4
Training loss: 2.600139617919922
Validation loss: 2.415756097403906

Epoch: 6| Step: 5
Training loss: 3.182081699371338
Validation loss: 2.407809721526279

Epoch: 6| Step: 6
Training loss: 2.6195006370544434
Validation loss: 2.413980837791197

Epoch: 6| Step: 7
Training loss: 2.1406869888305664
Validation loss: 2.407388646115539

Epoch: 6| Step: 8
Training loss: 2.556065320968628
Validation loss: 2.41450148500422

Epoch: 6| Step: 9
Training loss: 2.5226950645446777
Validation loss: 2.414286139190838

Epoch: 6| Step: 10
Training loss: 3.0396950244903564
Validation loss: 2.425721573573287

Epoch: 6| Step: 11
Training loss: 2.021867275238037
Validation loss: 2.4245574858880814

Epoch: 6| Step: 12
Training loss: 3.1210122108459473
Validation loss: 2.416769835256761

Epoch: 6| Step: 13
Training loss: 2.3972060680389404
Validation loss: 2.420836340996527

Epoch: 54| Step: 0
Training loss: 2.9124484062194824
Validation loss: 2.4229881404548563

Epoch: 6| Step: 1
Training loss: 1.7864141464233398
Validation loss: 2.412394280074745

Epoch: 6| Step: 2
Training loss: 2.630812168121338
Validation loss: 2.4187499092471216

Epoch: 6| Step: 3
Training loss: 2.119800567626953
Validation loss: 2.403888799810922

Epoch: 6| Step: 4
Training loss: 2.930375337600708
Validation loss: 2.45355631971872

Epoch: 6| Step: 5
Training loss: 1.668315052986145
Validation loss: 2.430231578888432

Epoch: 6| Step: 6
Training loss: 2.9177026748657227
Validation loss: 2.390446565484488

Epoch: 6| Step: 7
Training loss: 3.3853378295898438
Validation loss: 2.380771936908845

Epoch: 6| Step: 8
Training loss: 3.0058724880218506
Validation loss: 2.3807162289978354

Epoch: 6| Step: 9
Training loss: 2.549243450164795
Validation loss: 2.386406616498065

Epoch: 6| Step: 10
Training loss: 2.8230652809143066
Validation loss: 2.3888440183413926

Epoch: 6| Step: 11
Training loss: 3.0150580406188965
Validation loss: 2.3957786047330467

Epoch: 6| Step: 12
Training loss: 2.331784248352051
Validation loss: 2.395610163288732

Epoch: 6| Step: 13
Training loss: 3.1115400791168213
Validation loss: 2.4089646570144163

Epoch: 55| Step: 0
Training loss: 2.033749580383301
Validation loss: 2.4073294747260308

Epoch: 6| Step: 1
Training loss: 2.8235459327697754
Validation loss: 2.408474514561315

Epoch: 6| Step: 2
Training loss: 2.1882340908050537
Validation loss: 2.4069822988202496

Epoch: 6| Step: 3
Training loss: 2.2906198501586914
Validation loss: 2.4040733588639127

Epoch: 6| Step: 4
Training loss: 3.482357978820801
Validation loss: 2.410700567307011

Epoch: 6| Step: 5
Training loss: 2.8992080688476562
Validation loss: 2.412428520059073

Epoch: 6| Step: 6
Training loss: 1.7758219242095947
Validation loss: 2.4158961465281825

Epoch: 6| Step: 7
Training loss: 2.7796244621276855
Validation loss: 2.423726522794334

Epoch: 6| Step: 8
Training loss: 3.2435052394866943
Validation loss: 2.409374398569907

Epoch: 6| Step: 9
Training loss: 2.553776264190674
Validation loss: 2.384368376065326

Epoch: 6| Step: 10
Training loss: 3.175346851348877
Validation loss: 2.3757802055728052

Epoch: 6| Step: 11
Training loss: 2.81880784034729
Validation loss: 2.374314523512317

Epoch: 6| Step: 12
Training loss: 2.233565330505371
Validation loss: 2.3727058902863534

Epoch: 6| Step: 13
Training loss: 2.426607847213745
Validation loss: 2.378154682856734

Epoch: 56| Step: 0
Training loss: 2.7717502117156982
Validation loss: 2.376955270767212

Epoch: 6| Step: 1
Training loss: 2.8971505165100098
Validation loss: 2.37501234264784

Epoch: 6| Step: 2
Training loss: 2.4820847511291504
Validation loss: 2.374900230797388

Epoch: 6| Step: 3
Training loss: 2.2597620487213135
Validation loss: 2.3727859271469938

Epoch: 6| Step: 4
Training loss: 2.4593088626861572
Validation loss: 2.3737630151933238

Epoch: 6| Step: 5
Training loss: 2.3190228939056396
Validation loss: 2.3742579029452417

Epoch: 6| Step: 6
Training loss: 3.0430679321289062
Validation loss: 2.3723200367343042

Epoch: 6| Step: 7
Training loss: 2.709766387939453
Validation loss: 2.3767701784769693

Epoch: 6| Step: 8
Training loss: 1.9584137201309204
Validation loss: 2.3762812409349667

Epoch: 6| Step: 9
Training loss: 2.4656124114990234
Validation loss: 2.3740601334520566

Epoch: 6| Step: 10
Training loss: 3.3537611961364746
Validation loss: 2.3765818611268075

Epoch: 6| Step: 11
Training loss: 2.9340767860412598
Validation loss: 2.3720962052704184

Epoch: 6| Step: 12
Training loss: 2.318624973297119
Validation loss: 2.3698364919231785

Epoch: 6| Step: 13
Training loss: 3.0835764408111572
Validation loss: 2.3682531823394117

Epoch: 57| Step: 0
Training loss: 2.9718832969665527
Validation loss: 2.3668520117318756

Epoch: 6| Step: 1
Training loss: 2.70792818069458
Validation loss: 2.365819961793961

Epoch: 6| Step: 2
Training loss: 2.4064536094665527
Validation loss: 2.3759711429636967

Epoch: 6| Step: 3
Training loss: 3.390073537826538
Validation loss: 2.3757715250856135

Epoch: 6| Step: 4
Training loss: 2.912940502166748
Validation loss: 2.382968043768278

Epoch: 6| Step: 5
Training loss: 2.4560956954956055
Validation loss: 2.377800467193768

Epoch: 6| Step: 6
Training loss: 2.2309508323669434
Validation loss: 2.383923630560598

Epoch: 6| Step: 7
Training loss: 2.0390307903289795
Validation loss: 2.387925842756866

Epoch: 6| Step: 8
Training loss: 2.220088481903076
Validation loss: 2.386365226520005

Epoch: 6| Step: 9
Training loss: 3.0493698120117188
Validation loss: 2.395770078064293

Epoch: 6| Step: 10
Training loss: 2.3751535415649414
Validation loss: 2.4030791508254183

Epoch: 6| Step: 11
Training loss: 3.300377607345581
Validation loss: 2.3927884281322522

Epoch: 6| Step: 12
Training loss: 2.008101463317871
Validation loss: 2.381811426531884

Epoch: 6| Step: 13
Training loss: 2.7525341510772705
Validation loss: 2.3833218543760237

Epoch: 58| Step: 0
Training loss: 3.0315165519714355
Validation loss: 2.3814440388833322

Epoch: 6| Step: 1
Training loss: 2.6022403240203857
Validation loss: 2.3959251770409207

Epoch: 6| Step: 2
Training loss: 2.2494194507598877
Validation loss: 2.400764088476858

Epoch: 6| Step: 3
Training loss: 2.60979962348938
Validation loss: 2.4009003741766817

Epoch: 6| Step: 4
Training loss: 1.7205355167388916
Validation loss: 2.4001167076890186

Epoch: 6| Step: 5
Training loss: 3.1191349029541016
Validation loss: 2.4008709769095145

Epoch: 6| Step: 6
Training loss: 3.0318188667297363
Validation loss: 2.397262352769093

Epoch: 6| Step: 7
Training loss: 3.1166634559631348
Validation loss: 2.3860058066665486

Epoch: 6| Step: 8
Training loss: 2.3046135902404785
Validation loss: 2.3657701066745225

Epoch: 6| Step: 9
Training loss: 3.2297744750976562
Validation loss: 2.367098739070277

Epoch: 6| Step: 10
Training loss: 2.4819936752319336
Validation loss: 2.365444770423315

Epoch: 6| Step: 11
Training loss: 2.731628894805908
Validation loss: 2.3621535326844905

Epoch: 6| Step: 12
Training loss: 2.2826426029205322
Validation loss: 2.3597978263772945

Epoch: 6| Step: 13
Training loss: 1.8564082384109497
Validation loss: 2.361467653705228

Epoch: 59| Step: 0
Training loss: 1.933701753616333
Validation loss: 2.361390200994348

Epoch: 6| Step: 1
Training loss: 3.6790921688079834
Validation loss: 2.3600881330428587

Epoch: 6| Step: 2
Training loss: 2.705049753189087
Validation loss: 2.365928247410764

Epoch: 6| Step: 3
Training loss: 2.9243662357330322
Validation loss: 2.3647375670812463

Epoch: 6| Step: 4
Training loss: 1.706739902496338
Validation loss: 2.358851368709277

Epoch: 6| Step: 5
Training loss: 2.5293054580688477
Validation loss: 2.3670701647317536

Epoch: 6| Step: 6
Training loss: 2.3669357299804688
Validation loss: 2.379114008718921

Epoch: 6| Step: 7
Training loss: 2.380073070526123
Validation loss: 2.389558804932461

Epoch: 6| Step: 8
Training loss: 2.844550609588623
Validation loss: 2.431094002980058

Epoch: 6| Step: 9
Training loss: 2.5357413291931152
Validation loss: 2.4691172210119103

Epoch: 6| Step: 10
Training loss: 2.854078769683838
Validation loss: 2.450617136493806

Epoch: 6| Step: 11
Training loss: 3.064354419708252
Validation loss: 2.4702432668337257

Epoch: 6| Step: 12
Training loss: 2.8121442794799805
Validation loss: 2.426419347845098

Epoch: 6| Step: 13
Training loss: 2.314537286758423
Validation loss: 2.3901854766312467

Epoch: 60| Step: 0
Training loss: 2.9248697757720947
Validation loss: 2.3767266478589786

Epoch: 6| Step: 1
Training loss: 2.885469913482666
Validation loss: 2.3744485250083347

Epoch: 6| Step: 2
Training loss: 3.284674644470215
Validation loss: 2.3740624445740894

Epoch: 6| Step: 3
Training loss: 2.027963638305664
Validation loss: 2.370466470718384

Epoch: 6| Step: 4
Training loss: 1.9538090229034424
Validation loss: 2.368590247246527

Epoch: 6| Step: 5
Training loss: 2.2902865409851074
Validation loss: 2.3683392873374363

Epoch: 6| Step: 6
Training loss: 2.5664775371551514
Validation loss: 2.364362642329226

Epoch: 6| Step: 7
Training loss: 2.2970457077026367
Validation loss: 2.3655598343059583

Epoch: 6| Step: 8
Training loss: 2.7060461044311523
Validation loss: 2.3673488068324264

Epoch: 6| Step: 9
Training loss: 2.5807671546936035
Validation loss: 2.357995302446427

Epoch: 6| Step: 10
Training loss: 3.320305824279785
Validation loss: 2.358293702525477

Epoch: 6| Step: 11
Training loss: 2.731823205947876
Validation loss: 2.3591753564855105

Epoch: 6| Step: 12
Training loss: 2.562436580657959
Validation loss: 2.3543179471005677

Epoch: 6| Step: 13
Training loss: 2.259221076965332
Validation loss: 2.3537745809042327

Epoch: 61| Step: 0
Training loss: 2.2868266105651855
Validation loss: 2.3554258372194026

Epoch: 6| Step: 1
Training loss: 3.8275339603424072
Validation loss: 2.3519099297062045

Epoch: 6| Step: 2
Training loss: 2.02463960647583
Validation loss: 2.354335092729138

Epoch: 6| Step: 3
Training loss: 2.5069162845611572
Validation loss: 2.3591865724132908

Epoch: 6| Step: 4
Training loss: 2.160632371902466
Validation loss: 2.3581346850241385

Epoch: 6| Step: 5
Training loss: 2.1273345947265625
Validation loss: 2.3646629138659407

Epoch: 6| Step: 6
Training loss: 2.828590154647827
Validation loss: 2.3736031773269817

Epoch: 6| Step: 7
Training loss: 2.429854154586792
Validation loss: 2.3775193973254134

Epoch: 6| Step: 8
Training loss: 2.660050392150879
Validation loss: 2.3716368598322712

Epoch: 6| Step: 9
Training loss: 2.2844066619873047
Validation loss: 2.3781690084806053

Epoch: 6| Step: 10
Training loss: 2.7247135639190674
Validation loss: 2.375959483526086

Epoch: 6| Step: 11
Training loss: 2.7363877296447754
Validation loss: 2.3747843055314917

Epoch: 6| Step: 12
Training loss: 3.1735787391662598
Validation loss: 2.381459289981473

Epoch: 6| Step: 13
Training loss: 2.7030932903289795
Validation loss: 2.3728622031468216

Epoch: 62| Step: 0
Training loss: 2.9393739700317383
Validation loss: 2.3696093097809823

Epoch: 6| Step: 1
Training loss: 2.4486470222473145
Validation loss: 2.364573827353857

Epoch: 6| Step: 2
Training loss: 2.6229491233825684
Validation loss: 2.359220976470619

Epoch: 6| Step: 3
Training loss: 3.065157651901245
Validation loss: 2.3569083316351778

Epoch: 6| Step: 4
Training loss: 2.3137831687927246
Validation loss: 2.353416491580266

Epoch: 6| Step: 5
Training loss: 2.8010849952697754
Validation loss: 2.3612961999831663

Epoch: 6| Step: 6
Training loss: 1.8247897624969482
Validation loss: 2.366201405884117

Epoch: 6| Step: 7
Training loss: 3.299473285675049
Validation loss: 2.3869218262293006

Epoch: 6| Step: 8
Training loss: 2.7028017044067383
Validation loss: 2.398198186710317

Epoch: 6| Step: 9
Training loss: 2.5357613563537598
Validation loss: 2.4070857468471734

Epoch: 6| Step: 10
Training loss: 2.316742420196533
Validation loss: 2.400728578208595

Epoch: 6| Step: 11
Training loss: 2.9082207679748535
Validation loss: 2.377345908072687

Epoch: 6| Step: 12
Training loss: 1.9413347244262695
Validation loss: 2.364364745796368

Epoch: 6| Step: 13
Training loss: 2.990973472595215
Validation loss: 2.354919469484719

Epoch: 63| Step: 0
Training loss: 2.0396580696105957
Validation loss: 2.3589221892818326

Epoch: 6| Step: 1
Training loss: 3.0657799243927
Validation loss: 2.3712846720090477

Epoch: 6| Step: 2
Training loss: 2.2743000984191895
Validation loss: 2.3830616371605986

Epoch: 6| Step: 3
Training loss: 2.5440728664398193
Validation loss: 2.381878346525213

Epoch: 6| Step: 4
Training loss: 3.0551323890686035
Validation loss: 2.387685965466243

Epoch: 6| Step: 5
Training loss: 2.564112901687622
Validation loss: 2.397872009584981

Epoch: 6| Step: 6
Training loss: 2.920527935028076
Validation loss: 2.406347495253368

Epoch: 6| Step: 7
Training loss: 1.8868517875671387
Validation loss: 2.4143100143760763

Epoch: 6| Step: 8
Training loss: 2.983171224594116
Validation loss: 2.4086606963988273

Epoch: 6| Step: 9
Training loss: 2.1292591094970703
Validation loss: 2.3850591669800463

Epoch: 6| Step: 10
Training loss: 2.487577438354492
Validation loss: 2.3831112756524035

Epoch: 6| Step: 11
Training loss: 2.7487988471984863
Validation loss: 2.371662942312097

Epoch: 6| Step: 12
Training loss: 3.0735011100769043
Validation loss: 2.367918061953719

Epoch: 6| Step: 13
Training loss: 2.931643486022949
Validation loss: 2.3607616168196484

Epoch: 64| Step: 0
Training loss: 1.7671254873275757
Validation loss: 2.3433532381570465

Epoch: 6| Step: 1
Training loss: 2.6527647972106934
Validation loss: 2.345388156111522

Epoch: 6| Step: 2
Training loss: 2.2316274642944336
Validation loss: 2.350073810546629

Epoch: 6| Step: 3
Training loss: 3.0944597721099854
Validation loss: 2.3626609617663967

Epoch: 6| Step: 4
Training loss: 2.235044479370117
Validation loss: 2.3639015151608374

Epoch: 6| Step: 5
Training loss: 2.509317398071289
Validation loss: 2.360102812449137

Epoch: 6| Step: 6
Training loss: 2.40779709815979
Validation loss: 2.34481059094911

Epoch: 6| Step: 7
Training loss: 2.9784393310546875
Validation loss: 2.340256414105815

Epoch: 6| Step: 8
Training loss: 2.776033401489258
Validation loss: 2.342678731487643

Epoch: 6| Step: 9
Training loss: 3.516441822052002
Validation loss: 2.354383989046979

Epoch: 6| Step: 10
Training loss: 2.4438135623931885
Validation loss: 2.372234990519862

Epoch: 6| Step: 11
Training loss: 3.070734977722168
Validation loss: 2.3951698785187094

Epoch: 6| Step: 12
Training loss: 2.5000948905944824
Validation loss: 2.419328820320868

Epoch: 6| Step: 13
Training loss: 2.390767812728882
Validation loss: 2.413742688394362

Epoch: 65| Step: 0
Training loss: 2.9164767265319824
Validation loss: 2.437257907723868

Epoch: 6| Step: 1
Training loss: 2.8759098052978516
Validation loss: 2.4171629592936528

Epoch: 6| Step: 2
Training loss: 3.2105302810668945
Validation loss: 2.3893291411861295

Epoch: 6| Step: 3
Training loss: 1.449089765548706
Validation loss: 2.371676614207606

Epoch: 6| Step: 4
Training loss: 2.7365541458129883
Validation loss: 2.358675282488587

Epoch: 6| Step: 5
Training loss: 2.9064507484436035
Validation loss: 2.3545201978375836

Epoch: 6| Step: 6
Training loss: 2.752504348754883
Validation loss: 2.3641223305015155

Epoch: 6| Step: 7
Training loss: 2.0467729568481445
Validation loss: 2.3674434923356578

Epoch: 6| Step: 8
Training loss: 3.187708616256714
Validation loss: 2.3685456501540316

Epoch: 6| Step: 9
Training loss: 2.265533447265625
Validation loss: 2.366604071791454

Epoch: 6| Step: 10
Training loss: 2.4729485511779785
Validation loss: 2.36149719197263

Epoch: 6| Step: 11
Training loss: 2.7546515464782715
Validation loss: 2.3514327362019527

Epoch: 6| Step: 12
Training loss: 2.5914511680603027
Validation loss: 2.3534774882819063

Epoch: 6| Step: 13
Training loss: 2.0708820819854736
Validation loss: 2.34804396219151

Epoch: 66| Step: 0
Training loss: 3.1993229389190674
Validation loss: 2.357609102802892

Epoch: 6| Step: 1
Training loss: 2.3438801765441895
Validation loss: 2.36125099530784

Epoch: 6| Step: 2
Training loss: 2.532747745513916
Validation loss: 2.355466873415055

Epoch: 6| Step: 3
Training loss: 2.7380752563476562
Validation loss: 2.351705528074695

Epoch: 6| Step: 4
Training loss: 3.4371156692504883
Validation loss: 2.3345582126289286

Epoch: 6| Step: 5
Training loss: 2.6138172149658203
Validation loss: 2.3322829328557497

Epoch: 6| Step: 6
Training loss: 2.755913734436035
Validation loss: 2.3239903552557832

Epoch: 6| Step: 7
Training loss: 2.6608052253723145
Validation loss: 2.323042892640637

Epoch: 6| Step: 8
Training loss: 2.3876094818115234
Validation loss: 2.3210630852689027

Epoch: 6| Step: 9
Training loss: 2.281285047531128
Validation loss: 2.3280960846972722

Epoch: 6| Step: 10
Training loss: 2.313127279281616
Validation loss: 2.3284314524742866

Epoch: 6| Step: 11
Training loss: 2.5832056999206543
Validation loss: 2.3290155933749292

Epoch: 6| Step: 12
Training loss: 2.0109636783599854
Validation loss: 2.3378673702157955

Epoch: 6| Step: 13
Training loss: 2.525766372680664
Validation loss: 2.342153431266867

Epoch: 67| Step: 0
Training loss: 1.9045498371124268
Validation loss: 2.344200429096017

Epoch: 6| Step: 1
Training loss: 2.2270925045013428
Validation loss: 2.3511530994087138

Epoch: 6| Step: 2
Training loss: 2.4410295486450195
Validation loss: 2.3522140800312

Epoch: 6| Step: 3
Training loss: 2.9161324501037598
Validation loss: 2.3538967153077484

Epoch: 6| Step: 4
Training loss: 3.0852293968200684
Validation loss: 2.3652425786500335

Epoch: 6| Step: 5
Training loss: 3.13288950920105
Validation loss: 2.3607875403537544

Epoch: 6| Step: 6
Training loss: 2.4219539165496826
Validation loss: 2.3629774816574587

Epoch: 6| Step: 7
Training loss: 2.370889663696289
Validation loss: 2.357658947667768

Epoch: 6| Step: 8
Training loss: 2.2524266242980957
Validation loss: 2.350753661124937

Epoch: 6| Step: 9
Training loss: 2.3431763648986816
Validation loss: 2.3421272898233063

Epoch: 6| Step: 10
Training loss: 2.8405404090881348
Validation loss: 2.33219813787809

Epoch: 6| Step: 11
Training loss: 3.0262889862060547
Validation loss: 2.337493496556436

Epoch: 6| Step: 12
Training loss: 2.0962398052215576
Validation loss: 2.3312492703878753

Epoch: 6| Step: 13
Training loss: 3.819157600402832
Validation loss: 2.3306472980847923

Epoch: 68| Step: 0
Training loss: 2.8852458000183105
Validation loss: 2.3313583045877437

Epoch: 6| Step: 1
Training loss: 2.094761848449707
Validation loss: 2.3313428740347586

Epoch: 6| Step: 2
Training loss: 2.538796901702881
Validation loss: 2.328368298469051

Epoch: 6| Step: 3
Training loss: 2.132516622543335
Validation loss: 2.32837410383327

Epoch: 6| Step: 4
Training loss: 3.0950865745544434
Validation loss: 2.3354143942556074

Epoch: 6| Step: 5
Training loss: 2.312004566192627
Validation loss: 2.336209835544709

Epoch: 6| Step: 6
Training loss: 3.140998363494873
Validation loss: 2.3334579698501097

Epoch: 6| Step: 7
Training loss: 2.8120474815368652
Validation loss: 2.341747717190814

Epoch: 6| Step: 8
Training loss: 2.753004550933838
Validation loss: 2.346346380890057

Epoch: 6| Step: 9
Training loss: 2.169234275817871
Validation loss: 2.3538924263369654

Epoch: 6| Step: 10
Training loss: 3.0928566455841064
Validation loss: 2.3572428098288913

Epoch: 6| Step: 11
Training loss: 2.411926746368408
Validation loss: 2.3553260808349936

Epoch: 6| Step: 12
Training loss: 2.0893325805664062
Validation loss: 2.346793064507105

Epoch: 6| Step: 13
Training loss: 2.694448947906494
Validation loss: 2.3392994250020673

Epoch: 69| Step: 0
Training loss: 2.998046398162842
Validation loss: 2.3292736417503765

Epoch: 6| Step: 1
Training loss: 2.4304990768432617
Validation loss: 2.327308039511404

Epoch: 6| Step: 2
Training loss: 2.156655788421631
Validation loss: 2.319157164583924

Epoch: 6| Step: 3
Training loss: 2.879044532775879
Validation loss: 2.3221752233402704

Epoch: 6| Step: 4
Training loss: 2.1987807750701904
Validation loss: 2.3185944659735567

Epoch: 6| Step: 5
Training loss: 2.40725040435791
Validation loss: 2.3236726791627946

Epoch: 6| Step: 6
Training loss: 3.066924571990967
Validation loss: 2.32854264525957

Epoch: 6| Step: 7
Training loss: 2.639136552810669
Validation loss: 2.3268601266286706

Epoch: 6| Step: 8
Training loss: 2.266162395477295
Validation loss: 2.330109506525019

Epoch: 6| Step: 9
Training loss: 2.4458706378936768
Validation loss: 2.334087633317517

Epoch: 6| Step: 10
Training loss: 2.9108214378356934
Validation loss: 2.3377939911298853

Epoch: 6| Step: 11
Training loss: 2.2344589233398438
Validation loss: 2.3403289766721826

Epoch: 6| Step: 12
Training loss: 2.4407477378845215
Validation loss: 2.361975444260464

Epoch: 6| Step: 13
Training loss: 3.433610439300537
Validation loss: 2.3886422752052225

Epoch: 70| Step: 0
Training loss: 2.8559062480926514
Validation loss: 2.3994947377071587

Epoch: 6| Step: 1
Training loss: 2.3085668087005615
Validation loss: 2.389735306462934

Epoch: 6| Step: 2
Training loss: 2.4244751930236816
Validation loss: 2.389299979773901

Epoch: 6| Step: 3
Training loss: 2.284960985183716
Validation loss: 2.3855863937767605

Epoch: 6| Step: 4
Training loss: 2.4570655822753906
Validation loss: 2.3623984372743996

Epoch: 6| Step: 5
Training loss: 2.184131145477295
Validation loss: 2.3381308150547806

Epoch: 6| Step: 6
Training loss: 2.8109569549560547
Validation loss: 2.3243205393514326

Epoch: 6| Step: 7
Training loss: 2.7757925987243652
Validation loss: 2.323546092997315

Epoch: 6| Step: 8
Training loss: 2.507875919342041
Validation loss: 2.3282497262441986

Epoch: 6| Step: 9
Training loss: 2.3431835174560547
Validation loss: 2.3296395655601256

Epoch: 6| Step: 10
Training loss: 2.4286744594573975
Validation loss: 2.337097701206002

Epoch: 6| Step: 11
Training loss: 2.837191581726074
Validation loss: 2.337061825618949

Epoch: 6| Step: 12
Training loss: 2.5562398433685303
Validation loss: 2.3498402128937426

Epoch: 6| Step: 13
Training loss: 3.9965639114379883
Validation loss: 2.360403658241354

Epoch: 71| Step: 0
Training loss: 3.077709197998047
Validation loss: 2.346170345942179

Epoch: 6| Step: 1
Training loss: 2.60860538482666
Validation loss: 2.334114936090285

Epoch: 6| Step: 2
Training loss: 2.8549787998199463
Validation loss: 2.328583457136667

Epoch: 6| Step: 3
Training loss: 2.854483127593994
Validation loss: 2.3320738756528465

Epoch: 6| Step: 4
Training loss: 2.111025810241699
Validation loss: 2.34602556818275

Epoch: 6| Step: 5
Training loss: 2.0182125568389893
Validation loss: 2.3473756954234135

Epoch: 6| Step: 6
Training loss: 2.5970380306243896
Validation loss: 2.375893062160861

Epoch: 6| Step: 7
Training loss: 2.8102993965148926
Validation loss: 2.352430805083244

Epoch: 6| Step: 8
Training loss: 2.2224788665771484
Validation loss: 2.342520716369793

Epoch: 6| Step: 9
Training loss: 2.728792190551758
Validation loss: 2.332444288397348

Epoch: 6| Step: 10
Training loss: 2.7131474018096924
Validation loss: 2.3210152938801754

Epoch: 6| Step: 11
Training loss: 2.7190699577331543
Validation loss: 2.3149379555897047

Epoch: 6| Step: 12
Training loss: 2.0877652168273926
Validation loss: 2.3199346373158116

Epoch: 6| Step: 13
Training loss: 2.89308500289917
Validation loss: 2.315285821114817

Epoch: 72| Step: 0
Training loss: 2.8940107822418213
Validation loss: 2.3259335051300707

Epoch: 6| Step: 1
Training loss: 2.59552001953125
Validation loss: 2.3337641249420824

Epoch: 6| Step: 2
Training loss: 2.934086799621582
Validation loss: 2.3530948700443393

Epoch: 6| Step: 3
Training loss: 2.7531931400299072
Validation loss: 2.3575678204977386

Epoch: 6| Step: 4
Training loss: 2.007002115249634
Validation loss: 2.3627705420217207

Epoch: 6| Step: 5
Training loss: 2.631269931793213
Validation loss: 2.3951052978474605

Epoch: 6| Step: 6
Training loss: 2.4334986209869385
Validation loss: 2.419321096071633

Epoch: 6| Step: 7
Training loss: 2.162001609802246
Validation loss: 2.4204155245134906

Epoch: 6| Step: 8
Training loss: 2.2722628116607666
Validation loss: 2.3900675568529355

Epoch: 6| Step: 9
Training loss: 2.6509485244750977
Validation loss: 2.369608973944059

Epoch: 6| Step: 10
Training loss: 2.4571800231933594
Validation loss: 2.3451686033638577

Epoch: 6| Step: 11
Training loss: 2.7926745414733887
Validation loss: 2.3395230770111084

Epoch: 6| Step: 12
Training loss: 3.0315327644348145
Validation loss: 2.329056204006236

Epoch: 6| Step: 13
Training loss: 2.9741621017456055
Validation loss: 2.3252386636631464

Epoch: 73| Step: 0
Training loss: 2.645632743835449
Validation loss: 2.3222087749870877

Epoch: 6| Step: 1
Training loss: 1.9964451789855957
Validation loss: 2.3216904081324095

Epoch: 6| Step: 2
Training loss: 2.5298640727996826
Validation loss: 2.321131690855949

Epoch: 6| Step: 3
Training loss: 2.3984928131103516
Validation loss: 2.3354447400698097

Epoch: 6| Step: 4
Training loss: 2.651798725128174
Validation loss: 2.3733433138939644

Epoch: 6| Step: 5
Training loss: 1.7436213493347168
Validation loss: 2.4384240899034726

Epoch: 6| Step: 6
Training loss: 3.1704812049865723
Validation loss: 2.4880469153004308

Epoch: 6| Step: 7
Training loss: 2.8850300312042236
Validation loss: 2.465123766212053

Epoch: 6| Step: 8
Training loss: 3.493889331817627
Validation loss: 2.4551989980923232

Epoch: 6| Step: 9
Training loss: 2.043168544769287
Validation loss: 2.3993007470202703

Epoch: 6| Step: 10
Training loss: 2.7453572750091553
Validation loss: 2.3410034282233125

Epoch: 6| Step: 11
Training loss: 2.9585070610046387
Validation loss: 2.3023181602519047

Epoch: 6| Step: 12
Training loss: 2.401775598526001
Validation loss: 2.31052706831245

Epoch: 6| Step: 13
Training loss: 3.1776208877563477
Validation loss: 2.357553623055899

Epoch: 74| Step: 0
Training loss: 2.543506145477295
Validation loss: 2.40183433153296

Epoch: 6| Step: 1
Training loss: 3.089789390563965
Validation loss: 2.450779971256051

Epoch: 6| Step: 2
Training loss: 2.4524476528167725
Validation loss: 2.448241026170792

Epoch: 6| Step: 3
Training loss: 3.115870952606201
Validation loss: 2.436790512454125

Epoch: 6| Step: 4
Training loss: 3.149934768676758
Validation loss: 2.394395443700975

Epoch: 6| Step: 5
Training loss: 2.2927284240722656
Validation loss: 2.358461485114149

Epoch: 6| Step: 6
Training loss: 2.7602968215942383
Validation loss: 2.3261422341869724

Epoch: 6| Step: 7
Training loss: 2.1208903789520264
Validation loss: 2.3086994386488393

Epoch: 6| Step: 8
Training loss: 3.1197190284729004
Validation loss: 2.304986612771147

Epoch: 6| Step: 9
Training loss: 3.1258068084716797
Validation loss: 2.3044460281249015

Epoch: 6| Step: 10
Training loss: 2.2497801780700684
Validation loss: 2.312889920767917

Epoch: 6| Step: 11
Training loss: 2.317344903945923
Validation loss: 2.3153249781618834

Epoch: 6| Step: 12
Training loss: 2.254477024078369
Validation loss: 2.3237389838823708

Epoch: 6| Step: 13
Training loss: 1.7970812320709229
Validation loss: 2.3337020297204294

Epoch: 75| Step: 0
Training loss: 2.963606834411621
Validation loss: 2.3423104722012758

Epoch: 6| Step: 1
Training loss: 2.6527597904205322
Validation loss: 2.360948534422023

Epoch: 6| Step: 2
Training loss: 2.7875559329986572
Validation loss: 2.354212873725481

Epoch: 6| Step: 3
Training loss: 2.321812152862549
Validation loss: 2.377126883434993

Epoch: 6| Step: 4
Training loss: 1.8622772693634033
Validation loss: 2.3833527462456816

Epoch: 6| Step: 5
Training loss: 3.2473158836364746
Validation loss: 2.4007326351699008

Epoch: 6| Step: 6
Training loss: 2.5328683853149414
Validation loss: 2.371066911246187

Epoch: 6| Step: 7
Training loss: 2.5410542488098145
Validation loss: 2.352231902460898

Epoch: 6| Step: 8
Training loss: 2.7279088497161865
Validation loss: 2.319589771250243

Epoch: 6| Step: 9
Training loss: 2.3402037620544434
Validation loss: 2.303168906960436

Epoch: 6| Step: 10
Training loss: 2.929016590118408
Validation loss: 2.3018395644362255

Epoch: 6| Step: 11
Training loss: 2.1134564876556396
Validation loss: 2.3049524984052105

Epoch: 6| Step: 12
Training loss: 2.5265209674835205
Validation loss: 2.3045741409383793

Epoch: 6| Step: 13
Training loss: 2.641108512878418
Validation loss: 2.3057934648247174

Epoch: 76| Step: 0
Training loss: 2.1093430519104004
Validation loss: 2.3083409519605738

Epoch: 6| Step: 1
Training loss: 2.1960930824279785
Validation loss: 2.3073114502814507

Epoch: 6| Step: 2
Training loss: 2.8745737075805664
Validation loss: 2.3121418594032206

Epoch: 6| Step: 3
Training loss: 2.6848368644714355
Validation loss: 2.316687963342154

Epoch: 6| Step: 4
Training loss: 2.067122459411621
Validation loss: 2.3097128457920526

Epoch: 6| Step: 5
Training loss: 2.2894070148468018
Validation loss: 2.3116883308656755

Epoch: 6| Step: 6
Training loss: 2.834162473678589
Validation loss: 2.3146084585497455

Epoch: 6| Step: 7
Training loss: 3.226308822631836
Validation loss: 2.3119130237128145

Epoch: 6| Step: 8
Training loss: 2.0765624046325684
Validation loss: 2.31254146945092

Epoch: 6| Step: 9
Training loss: 2.7723474502563477
Validation loss: 2.3102440962227444

Epoch: 6| Step: 10
Training loss: 2.2747154235839844
Validation loss: 2.315439998462636

Epoch: 6| Step: 11
Training loss: 1.618790864944458
Validation loss: 2.3199486681210097

Epoch: 6| Step: 12
Training loss: 3.9121317863464355
Validation loss: 2.306753143187492

Epoch: 6| Step: 13
Training loss: 3.460601568222046
Validation loss: 2.3064988274728098

Epoch: 77| Step: 0
Training loss: 1.6101778745651245
Validation loss: 2.30181618659727

Epoch: 6| Step: 1
Training loss: 2.470768451690674
Validation loss: 2.299988715879379

Epoch: 6| Step: 2
Training loss: 2.515645980834961
Validation loss: 2.3098318384539698

Epoch: 6| Step: 3
Training loss: 2.6861143112182617
Validation loss: 2.299728960119268

Epoch: 6| Step: 4
Training loss: 2.7533085346221924
Validation loss: 2.2952343058842484

Epoch: 6| Step: 5
Training loss: 1.9325571060180664
Validation loss: 2.297701007576399

Epoch: 6| Step: 6
Training loss: 2.813626766204834
Validation loss: 2.2921438217163086

Epoch: 6| Step: 7
Training loss: 3.027146339416504
Validation loss: 2.2908756015121297

Epoch: 6| Step: 8
Training loss: 2.286186456680298
Validation loss: 2.2892961271347536

Epoch: 6| Step: 9
Training loss: 2.2350523471832275
Validation loss: 2.292105241488385

Epoch: 6| Step: 10
Training loss: 2.6345953941345215
Validation loss: 2.29265582946039

Epoch: 6| Step: 11
Training loss: 3.110049247741699
Validation loss: 2.2953671563056206

Epoch: 6| Step: 12
Training loss: 2.651120662689209
Validation loss: 2.3002922329851376

Epoch: 6| Step: 13
Training loss: 3.4226815700531006
Validation loss: 2.299440740257181

Epoch: 78| Step: 0
Training loss: 2.356903314590454
Validation loss: 2.294732601411881

Epoch: 6| Step: 1
Training loss: 3.194091558456421
Validation loss: 2.2998167186655025

Epoch: 6| Step: 2
Training loss: 2.314011335372925
Validation loss: 2.3051694618758334

Epoch: 6| Step: 3
Training loss: 2.2405314445495605
Validation loss: 2.3180185441047914

Epoch: 6| Step: 4
Training loss: 2.566364049911499
Validation loss: 2.314113765634516

Epoch: 6| Step: 5
Training loss: 3.104180335998535
Validation loss: 2.3343776682371735

Epoch: 6| Step: 6
Training loss: 3.26967191696167
Validation loss: 2.336510669800543

Epoch: 6| Step: 7
Training loss: 2.113999366760254
Validation loss: 2.3179947073741625

Epoch: 6| Step: 8
Training loss: 2.247063636779785
Validation loss: 2.3041074993789836

Epoch: 6| Step: 9
Training loss: 2.7419281005859375
Validation loss: 2.292675361838392

Epoch: 6| Step: 10
Training loss: 2.119525671005249
Validation loss: 2.2938440640767417

Epoch: 6| Step: 11
Training loss: 1.8740147352218628
Validation loss: 2.2939795063387964

Epoch: 6| Step: 12
Training loss: 2.6589598655700684
Validation loss: 2.2894623330844346

Epoch: 6| Step: 13
Training loss: 3.299241065979004
Validation loss: 2.2835155789570143

Epoch: 79| Step: 0
Training loss: 2.8930585384368896
Validation loss: 2.2841963896187405

Epoch: 6| Step: 1
Training loss: 2.8127360343933105
Validation loss: 2.284752584272815

Epoch: 6| Step: 2
Training loss: 3.160541534423828
Validation loss: 2.278118113035797

Epoch: 6| Step: 3
Training loss: 2.3547420501708984
Validation loss: 2.281689473377761

Epoch: 6| Step: 4
Training loss: 1.4885566234588623
Validation loss: 2.2893286751162623

Epoch: 6| Step: 5
Training loss: 3.228090763092041
Validation loss: 2.289038453050839

Epoch: 6| Step: 6
Training loss: 2.8328235149383545
Validation loss: 2.295565464163339

Epoch: 6| Step: 7
Training loss: 2.5097360610961914
Validation loss: 2.301027620992353

Epoch: 6| Step: 8
Training loss: 2.9470577239990234
Validation loss: 2.306345570471979

Epoch: 6| Step: 9
Training loss: 2.105377435684204
Validation loss: 2.2954606779160036

Epoch: 6| Step: 10
Training loss: 1.8899071216583252
Validation loss: 2.2896348404627975

Epoch: 6| Step: 11
Training loss: 2.532682418823242
Validation loss: 2.291740458498719

Epoch: 6| Step: 12
Training loss: 2.683342456817627
Validation loss: 2.298786135130031

Epoch: 6| Step: 13
Training loss: 2.190972089767456
Validation loss: 2.310076339270479

Epoch: 80| Step: 0
Training loss: 2.9741272926330566
Validation loss: 2.326791191613802

Epoch: 6| Step: 1
Training loss: 2.5356526374816895
Validation loss: 2.321863620511947

Epoch: 6| Step: 2
Training loss: 3.1252851486206055
Validation loss: 2.310583524806525

Epoch: 6| Step: 3
Training loss: 2.4709434509277344
Validation loss: 2.3148302955012166

Epoch: 6| Step: 4
Training loss: 2.357893943786621
Validation loss: 2.3195967571709746

Epoch: 6| Step: 5
Training loss: 1.931836485862732
Validation loss: 2.3448404342897478

Epoch: 6| Step: 6
Training loss: 2.5500218868255615
Validation loss: 2.3587259964276384

Epoch: 6| Step: 7
Training loss: 2.3105626106262207
Validation loss: 2.383221049462595

Epoch: 6| Step: 8
Training loss: 2.2742929458618164
Validation loss: 2.3687495582847187

Epoch: 6| Step: 9
Training loss: 2.8334264755249023
Validation loss: 2.3350668530310354

Epoch: 6| Step: 10
Training loss: 2.3530001640319824
Validation loss: 2.3102473853736796

Epoch: 6| Step: 11
Training loss: 2.8809266090393066
Validation loss: 2.3108873854401293

Epoch: 6| Step: 12
Training loss: 2.5883097648620605
Validation loss: 2.291616088600569

Epoch: 6| Step: 13
Training loss: 2.793640375137329
Validation loss: 2.281435187144946

Epoch: 81| Step: 0
Training loss: 2.9438374042510986
Validation loss: 2.276254048911474

Epoch: 6| Step: 1
Training loss: 2.3580784797668457
Validation loss: 2.2733253663586033

Epoch: 6| Step: 2
Training loss: 2.660568952560425
Validation loss: 2.2737947535771195

Epoch: 6| Step: 3
Training loss: 2.8360629081726074
Validation loss: 2.271712064743042

Epoch: 6| Step: 4
Training loss: 2.5425586700439453
Validation loss: 2.2712828895097137

Epoch: 6| Step: 5
Training loss: 2.841322660446167
Validation loss: 2.2817278626144573

Epoch: 6| Step: 6
Training loss: 2.655134916305542
Validation loss: 2.2827696364413024

Epoch: 6| Step: 7
Training loss: 2.098747730255127
Validation loss: 2.281193951124786

Epoch: 6| Step: 8
Training loss: 2.446894884109497
Validation loss: 2.274559623451643

Epoch: 6| Step: 9
Training loss: 2.6199522018432617
Validation loss: 2.2765415817178707

Epoch: 6| Step: 10
Training loss: 2.834224224090576
Validation loss: 2.2732496812779415

Epoch: 6| Step: 11
Training loss: 2.476388454437256
Validation loss: 2.274542309904611

Epoch: 6| Step: 12
Training loss: 2.1350576877593994
Validation loss: 2.2726712944687053

Epoch: 6| Step: 13
Training loss: 2.2387259006500244
Validation loss: 2.283290232381513

Epoch: 82| Step: 0
Training loss: 2.8850045204162598
Validation loss: 2.2832872611220165

Epoch: 6| Step: 1
Training loss: 2.303701400756836
Validation loss: 2.2809227961365894

Epoch: 6| Step: 2
Training loss: 2.056647300720215
Validation loss: 2.2929091376643025

Epoch: 6| Step: 3
Training loss: 1.9748008251190186
Validation loss: 2.2905514137719267

Epoch: 6| Step: 4
Training loss: 2.2636466026306152
Validation loss: 2.287272140543948

Epoch: 6| Step: 5
Training loss: 2.130199670791626
Validation loss: 2.2961539030075073

Epoch: 6| Step: 6
Training loss: 2.932713508605957
Validation loss: 2.2946104567538024

Epoch: 6| Step: 7
Training loss: 2.4237060546875
Validation loss: 2.3029140656994236

Epoch: 6| Step: 8
Training loss: 3.1414284706115723
Validation loss: 2.307628518791609

Epoch: 6| Step: 9
Training loss: 3.2430503368377686
Validation loss: 2.2992074284502255

Epoch: 6| Step: 10
Training loss: 3.414855480194092
Validation loss: 2.2952357184502388

Epoch: 6| Step: 11
Training loss: 2.2301368713378906
Validation loss: 2.2930277009164133

Epoch: 6| Step: 12
Training loss: 1.918165683746338
Validation loss: 2.287653571815901

Epoch: 6| Step: 13
Training loss: 2.740471124649048
Validation loss: 2.2820839087168374

Epoch: 83| Step: 0
Training loss: 2.1463615894317627
Validation loss: 2.287022852128552

Epoch: 6| Step: 1
Training loss: 2.5510146617889404
Validation loss: 2.28323172753857

Epoch: 6| Step: 2
Training loss: 2.254179000854492
Validation loss: 2.2784741206835677

Epoch: 6| Step: 3
Training loss: 3.2727010250091553
Validation loss: 2.281231459750924

Epoch: 6| Step: 4
Training loss: 2.2858710289001465
Validation loss: 2.289994357734598

Epoch: 6| Step: 5
Training loss: 2.200814962387085
Validation loss: 2.2870544566903064

Epoch: 6| Step: 6
Training loss: 2.6376760005950928
Validation loss: 2.285991063681982

Epoch: 6| Step: 7
Training loss: 2.788398504257202
Validation loss: 2.2728653851375786

Epoch: 6| Step: 8
Training loss: 2.0592594146728516
Validation loss: 2.2899091448835147

Epoch: 6| Step: 9
Training loss: 2.3071541786193848
Validation loss: 2.2839926391519527

Epoch: 6| Step: 10
Training loss: 2.713690757751465
Validation loss: 2.2886204309360956

Epoch: 6| Step: 11
Training loss: 3.5385472774505615
Validation loss: 2.283508969891456

Epoch: 6| Step: 12
Training loss: 2.163206100463867
Validation loss: 2.2855431623356317

Epoch: 6| Step: 13
Training loss: 2.5956637859344482
Validation loss: 2.2691819565270537

Epoch: 84| Step: 0
Training loss: 1.9678891897201538
Validation loss: 2.2780583263725362

Epoch: 6| Step: 1
Training loss: 2.8823952674865723
Validation loss: 2.269647959739931

Epoch: 6| Step: 2
Training loss: 2.6014599800109863
Validation loss: 2.275700597352879

Epoch: 6| Step: 3
Training loss: 1.45689058303833
Validation loss: 2.285676693403593

Epoch: 6| Step: 4
Training loss: 2.909287452697754
Validation loss: 2.2952406073129303

Epoch: 6| Step: 5
Training loss: 1.6859221458435059
Validation loss: 2.288393450039689

Epoch: 6| Step: 6
Training loss: 2.1441049575805664
Validation loss: 2.2911775214697725

Epoch: 6| Step: 7
Training loss: 2.5700039863586426
Validation loss: 2.2754016512183735

Epoch: 6| Step: 8
Training loss: 3.016446590423584
Validation loss: 2.276946081910082

Epoch: 6| Step: 9
Training loss: 2.9295411109924316
Validation loss: 2.277775300446377

Epoch: 6| Step: 10
Training loss: 2.677705764770508
Validation loss: 2.2753692980735534

Epoch: 6| Step: 11
Training loss: 3.267477035522461
Validation loss: 2.267711898332001

Epoch: 6| Step: 12
Training loss: 2.933511734008789
Validation loss: 2.2705514636091007

Epoch: 6| Step: 13
Training loss: 2.2784929275512695
Validation loss: 2.2695731783425934

Epoch: 85| Step: 0
Training loss: 2.2213780879974365
Validation loss: 2.268186779432399

Epoch: 6| Step: 1
Training loss: 2.1237809658050537
Validation loss: 2.271232710089735

Epoch: 6| Step: 2
Training loss: 2.149660110473633
Validation loss: 2.289643100512925

Epoch: 6| Step: 3
Training loss: 2.803528308868408
Validation loss: 2.2935261931470645

Epoch: 6| Step: 4
Training loss: 2.484647274017334
Validation loss: 2.3024402638917327

Epoch: 6| Step: 5
Training loss: 2.6522719860076904
Validation loss: 2.309637290175243

Epoch: 6| Step: 6
Training loss: 2.671912908554077
Validation loss: 2.3159852425257363

Epoch: 6| Step: 7
Training loss: 4.181573867797852
Validation loss: 2.3221899591466433

Epoch: 6| Step: 8
Training loss: 2.5060386657714844
Validation loss: 2.3080075556232083

Epoch: 6| Step: 9
Training loss: 2.1279687881469727
Validation loss: 2.3130539386503157

Epoch: 6| Step: 10
Training loss: 2.6358304023742676
Validation loss: 2.2824077247291483

Epoch: 6| Step: 11
Training loss: 2.158745288848877
Validation loss: 2.2732698686661257

Epoch: 6| Step: 12
Training loss: 2.068324089050293
Validation loss: 2.268438523815524

Epoch: 6| Step: 13
Training loss: 2.6695823669433594
Validation loss: 2.265986486147809

Epoch: 86| Step: 0
Training loss: 1.9147292375564575
Validation loss: 2.2568133159350325

Epoch: 6| Step: 1
Training loss: 2.121532678604126
Validation loss: 2.255012955716861

Epoch: 6| Step: 2
Training loss: 1.8190171718597412
Validation loss: 2.250969409942627

Epoch: 6| Step: 3
Training loss: 2.3062140941619873
Validation loss: 2.248013170816565

Epoch: 6| Step: 4
Training loss: 2.9827661514282227
Validation loss: 2.246066213935934

Epoch: 6| Step: 5
Training loss: 2.126891613006592
Validation loss: 2.2477543328398015

Epoch: 6| Step: 6
Training loss: 2.5985965728759766
Validation loss: 2.2455163668560725

Epoch: 6| Step: 7
Training loss: 2.687779664993286
Validation loss: 2.246036442377234

Epoch: 6| Step: 8
Training loss: 2.392444133758545
Validation loss: 2.2449677310964113

Epoch: 6| Step: 9
Training loss: 3.292177200317383
Validation loss: 2.2454627816395094

Epoch: 6| Step: 10
Training loss: 2.517390727996826
Validation loss: 2.250111231239893

Epoch: 6| Step: 11
Training loss: 3.2756638526916504
Validation loss: 2.2708453850079606

Epoch: 6| Step: 12
Training loss: 3.008042335510254
Validation loss: 2.2555700681542836

Epoch: 6| Step: 13
Training loss: 2.6435649394989014
Validation loss: 2.256340225537618

Epoch: 87| Step: 0
Training loss: 2.2289860248565674
Validation loss: 2.2479200542614026

Epoch: 6| Step: 1
Training loss: 2.5129246711730957
Validation loss: 2.2411217176786034

Epoch: 6| Step: 2
Training loss: 2.374267339706421
Validation loss: 2.243905908317976

Epoch: 6| Step: 3
Training loss: 2.556001663208008
Validation loss: 2.24490798160594

Epoch: 6| Step: 4
Training loss: 2.554677963256836
Validation loss: 2.252827784066559

Epoch: 6| Step: 5
Training loss: 2.667180061340332
Validation loss: 2.2456778287887573

Epoch: 6| Step: 6
Training loss: 2.746054172515869
Validation loss: 2.251255376364595

Epoch: 6| Step: 7
Training loss: 1.8825873136520386
Validation loss: 2.250983812475717

Epoch: 6| Step: 8
Training loss: 2.180006742477417
Validation loss: 2.2523219200872604

Epoch: 6| Step: 9
Training loss: 2.9846198558807373
Validation loss: 2.246125785253381

Epoch: 6| Step: 10
Training loss: 2.9084525108337402
Validation loss: 2.251277910765781

Epoch: 6| Step: 11
Training loss: 2.852813243865967
Validation loss: 2.2565917917477187

Epoch: 6| Step: 12
Training loss: 2.91764235496521
Validation loss: 2.2784894461272867

Epoch: 6| Step: 13
Training loss: 1.6174492835998535
Validation loss: 2.2868330324849775

Epoch: 88| Step: 0
Training loss: 2.326509952545166
Validation loss: 2.308160343477803

Epoch: 6| Step: 1
Training loss: 2.218852996826172
Validation loss: 2.3185951325201217

Epoch: 6| Step: 2
Training loss: 2.912749767303467
Validation loss: 2.3202330502130653

Epoch: 6| Step: 3
Training loss: 2.815237522125244
Validation loss: 2.319143372197305

Epoch: 6| Step: 4
Training loss: 2.328864097595215
Validation loss: 2.332683740123626

Epoch: 6| Step: 5
Training loss: 2.2840957641601562
Validation loss: 2.3035185875431186

Epoch: 6| Step: 6
Training loss: 1.8747708797454834
Validation loss: 2.2841636570551063

Epoch: 6| Step: 7
Training loss: 2.6829404830932617
Validation loss: 2.2791356079040037

Epoch: 6| Step: 8
Training loss: 2.561326026916504
Validation loss: 2.2860010990532498

Epoch: 6| Step: 9
Training loss: 2.3289647102355957
Validation loss: 2.294440072069886

Epoch: 6| Step: 10
Training loss: 2.660780429840088
Validation loss: 2.2968747333813737

Epoch: 6| Step: 11
Training loss: 3.4414350986480713
Validation loss: 2.3078146314108245

Epoch: 6| Step: 12
Training loss: 2.2169625759124756
Validation loss: 2.291855283962783

Epoch: 6| Step: 13
Training loss: 2.9357261657714844
Validation loss: 2.2821212327608498

Epoch: 89| Step: 0
Training loss: 1.881605625152588
Validation loss: 2.278646215315788

Epoch: 6| Step: 1
Training loss: 2.2471683025360107
Validation loss: 2.266124802251016

Epoch: 6| Step: 2
Training loss: 2.1037826538085938
Validation loss: 2.2572841798105547

Epoch: 6| Step: 3
Training loss: 2.6693594455718994
Validation loss: 2.2468556024694957

Epoch: 6| Step: 4
Training loss: 2.135776996612549
Validation loss: 2.2388576256331576

Epoch: 6| Step: 5
Training loss: 2.893428325653076
Validation loss: 2.2402016091090378

Epoch: 6| Step: 6
Training loss: 3.3997581005096436
Validation loss: 2.238479975731142

Epoch: 6| Step: 7
Training loss: 2.894102096557617
Validation loss: 2.2474702058299894

Epoch: 6| Step: 8
Training loss: 2.5992283821105957
Validation loss: 2.2348118174460625

Epoch: 6| Step: 9
Training loss: 2.359539747238159
Validation loss: 2.2418818345633884

Epoch: 6| Step: 10
Training loss: 2.88550066947937
Validation loss: 2.2383166705408404

Epoch: 6| Step: 11
Training loss: 2.0839061737060547
Validation loss: 2.240825083947951

Epoch: 6| Step: 12
Training loss: 3.193286895751953
Validation loss: 2.2378652198340303

Epoch: 6| Step: 13
Training loss: 2.1047370433807373
Validation loss: 2.246615461123887

Epoch: 90| Step: 0
Training loss: 2.511422872543335
Validation loss: 2.257040887750605

Epoch: 6| Step: 1
Training loss: 2.8492016792297363
Validation loss: 2.300063487022154

Epoch: 6| Step: 2
Training loss: 2.5140645503997803
Validation loss: 2.3316253180144937

Epoch: 6| Step: 3
Training loss: 2.550961494445801
Validation loss: 2.345056046721756

Epoch: 6| Step: 4
Training loss: 3.221749782562256
Validation loss: 2.338338610946491

Epoch: 6| Step: 5
Training loss: 2.4362125396728516
Validation loss: 2.3228166103363037

Epoch: 6| Step: 6
Training loss: 2.492781162261963
Validation loss: 2.3139025319007134

Epoch: 6| Step: 7
Training loss: 3.153073310852051
Validation loss: 2.293728974557692

Epoch: 6| Step: 8
Training loss: 1.8320152759552002
Validation loss: 2.270129910079382

Epoch: 6| Step: 9
Training loss: 2.388359308242798
Validation loss: 2.2597413063049316

Epoch: 6| Step: 10
Training loss: 2.581298351287842
Validation loss: 2.2650103671576387

Epoch: 6| Step: 11
Training loss: 2.8603031635284424
Validation loss: 2.2757000025882514

Epoch: 6| Step: 12
Training loss: 1.5261600017547607
Validation loss: 2.2810517485423754

Epoch: 6| Step: 13
Training loss: 2.5230605602264404
Validation loss: 2.2736233126732612

Epoch: 91| Step: 0
Training loss: 2.886320114135742
Validation loss: 2.2656433454123874

Epoch: 6| Step: 1
Training loss: 1.7161259651184082
Validation loss: 2.2576201385067356

Epoch: 6| Step: 2
Training loss: 2.349621295928955
Validation loss: 2.259565681539556

Epoch: 6| Step: 3
Training loss: 1.970097541809082
Validation loss: 2.2622775364947576

Epoch: 6| Step: 4
Training loss: 2.7545928955078125
Validation loss: 2.2675894050187964

Epoch: 6| Step: 5
Training loss: 3.2837915420532227
Validation loss: 2.2850556219777753

Epoch: 6| Step: 6
Training loss: 2.0306406021118164
Validation loss: 2.287241117928618

Epoch: 6| Step: 7
Training loss: 3.051520824432373
Validation loss: 2.2917092846285914

Epoch: 6| Step: 8
Training loss: 2.981093406677246
Validation loss: 2.282299344257642

Epoch: 6| Step: 9
Training loss: 2.7090845108032227
Validation loss: 2.275905562985328

Epoch: 6| Step: 10
Training loss: 2.844027042388916
Validation loss: 2.269494210520098

Epoch: 6| Step: 11
Training loss: 2.0436670780181885
Validation loss: 2.2548785542929046

Epoch: 6| Step: 12
Training loss: 2.2744662761688232
Validation loss: 2.2524824732093403

Epoch: 6| Step: 13
Training loss: 2.2445433139801025
Validation loss: 2.2471116101869972

Epoch: 92| Step: 0
Training loss: 2.511775016784668
Validation loss: 2.239073919993575

Epoch: 6| Step: 1
Training loss: 2.690542459487915
Validation loss: 2.2308190971292476

Epoch: 6| Step: 2
Training loss: 2.835491418838501
Validation loss: 2.230767398752192

Epoch: 6| Step: 3
Training loss: 2.4393746852874756
Validation loss: 2.2261892313598306

Epoch: 6| Step: 4
Training loss: 2.3405075073242188
Validation loss: 2.2309604716557327

Epoch: 6| Step: 5
Training loss: 2.4100165367126465
Validation loss: 2.2291489032007035

Epoch: 6| Step: 6
Training loss: 2.612031936645508
Validation loss: 2.230425816710277

Epoch: 6| Step: 7
Training loss: 2.67526912689209
Validation loss: 2.222150589830132

Epoch: 6| Step: 8
Training loss: 2.1339187622070312
Validation loss: 2.2259629131645284

Epoch: 6| Step: 9
Training loss: 2.488330125808716
Validation loss: 2.2375754515329995

Epoch: 6| Step: 10
Training loss: 2.5305991172790527
Validation loss: 2.241540167921333

Epoch: 6| Step: 11
Training loss: 2.381303071975708
Validation loss: 2.2516457970424364

Epoch: 6| Step: 12
Training loss: 2.644644260406494
Validation loss: 2.2618935723458566

Epoch: 6| Step: 13
Training loss: 2.5259029865264893
Validation loss: 2.281240637584399

Epoch: 93| Step: 0
Training loss: 2.180339813232422
Validation loss: 2.2979336861641175

Epoch: 6| Step: 1
Training loss: 2.934720039367676
Validation loss: 2.2903674571744856

Epoch: 6| Step: 2
Training loss: 2.786836624145508
Validation loss: 2.273874762237713

Epoch: 6| Step: 3
Training loss: 2.285813570022583
Validation loss: 2.2671348664068405

Epoch: 6| Step: 4
Training loss: 1.3904762268066406
Validation loss: 2.2680561862966067

Epoch: 6| Step: 5
Training loss: 2.7436468601226807
Validation loss: 2.2371064591151413

Epoch: 6| Step: 6
Training loss: 2.502169370651245
Validation loss: 2.2221639617796867

Epoch: 6| Step: 7
Training loss: 3.29013729095459
Validation loss: 2.22554257095501

Epoch: 6| Step: 8
Training loss: 2.410606861114502
Validation loss: 2.2175982126625637

Epoch: 6| Step: 9
Training loss: 2.395151138305664
Validation loss: 2.2202754892328733

Epoch: 6| Step: 10
Training loss: 2.7960400581359863
Validation loss: 2.224893671210094

Epoch: 6| Step: 11
Training loss: 2.6314213275909424
Validation loss: 2.226338396790207

Epoch: 6| Step: 12
Training loss: 2.805816650390625
Validation loss: 2.223795872862621

Epoch: 6| Step: 13
Training loss: 2.0722239017486572
Validation loss: 2.217430076291484

Epoch: 94| Step: 0
Training loss: 1.750307321548462
Validation loss: 2.218394325625512

Epoch: 6| Step: 1
Training loss: 2.6495532989501953
Validation loss: 2.2120188128563667

Epoch: 6| Step: 2
Training loss: 2.6978631019592285
Validation loss: 2.2185436602561706

Epoch: 6| Step: 3
Training loss: 2.557262897491455
Validation loss: 2.2218217170366676

Epoch: 6| Step: 4
Training loss: 2.87619686126709
Validation loss: 2.240066418083765

Epoch: 6| Step: 5
Training loss: 2.3405606746673584
Validation loss: 2.2498152909740323

Epoch: 6| Step: 6
Training loss: 1.8827952146530151
Validation loss: 2.266108535951184

Epoch: 6| Step: 7
Training loss: 2.6887693405151367
Validation loss: 2.293239167941514

Epoch: 6| Step: 8
Training loss: 2.2296624183654785
Validation loss: 2.315293904273741

Epoch: 6| Step: 9
Training loss: 3.163877010345459
Validation loss: 2.343114055613036

Epoch: 6| Step: 10
Training loss: 2.24269962310791
Validation loss: 2.3446544357525405

Epoch: 6| Step: 11
Training loss: 3.6231958866119385
Validation loss: 2.3343972980335193

Epoch: 6| Step: 12
Training loss: 2.201786994934082
Validation loss: 2.2948820001335553

Epoch: 6| Step: 13
Training loss: 2.521998882293701
Validation loss: 2.2551780259737404

Epoch: 95| Step: 0
Training loss: 3.0766847133636475
Validation loss: 2.216307804148684

Epoch: 6| Step: 1
Training loss: 2.509944438934326
Validation loss: 2.218549284883725

Epoch: 6| Step: 2
Training loss: 2.721315622329712
Validation loss: 2.2218477956710325

Epoch: 6| Step: 3
Training loss: 2.0635130405426025
Validation loss: 2.2313886650146975

Epoch: 6| Step: 4
Training loss: 2.8219175338745117
Validation loss: 2.254373238932702

Epoch: 6| Step: 5
Training loss: 2.3903002738952637
Validation loss: 2.2609035840598484

Epoch: 6| Step: 6
Training loss: 2.3301308155059814
Validation loss: 2.2873344600841565

Epoch: 6| Step: 7
Training loss: 2.6124229431152344
Validation loss: 2.292007297597906

Epoch: 6| Step: 8
Training loss: 2.423604965209961
Validation loss: 2.283812310106011

Epoch: 6| Step: 9
Training loss: 2.482131004333496
Validation loss: 2.265166836400186

Epoch: 6| Step: 10
Training loss: 2.2719388008117676
Validation loss: 2.2452943478861163

Epoch: 6| Step: 11
Training loss: 2.651818037033081
Validation loss: 2.2532297693273073

Epoch: 6| Step: 12
Training loss: 2.4608852863311768
Validation loss: 2.2632911128382527

Epoch: 6| Step: 13
Training loss: 2.8380727767944336
Validation loss: 2.285222481655818

Epoch: 96| Step: 0
Training loss: 2.2581844329833984
Validation loss: 2.307968931813394

Epoch: 6| Step: 1
Training loss: 3.5252366065979004
Validation loss: 2.337755474992978

Epoch: 6| Step: 2
Training loss: 2.638294219970703
Validation loss: 2.3586904054046958

Epoch: 6| Step: 3
Training loss: 2.168457508087158
Validation loss: 2.371713739569469

Epoch: 6| Step: 4
Training loss: 2.039858818054199
Validation loss: 2.379653884518531

Epoch: 6| Step: 5
Training loss: 3.052705764770508
Validation loss: 2.3458241570380425

Epoch: 6| Step: 6
Training loss: 3.096909999847412
Validation loss: 2.3419836131475305

Epoch: 6| Step: 7
Training loss: 2.8681304454803467
Validation loss: 2.316787868417719

Epoch: 6| Step: 8
Training loss: 3.0106277465820312
Validation loss: 2.2960947354634604

Epoch: 6| Step: 9
Training loss: 2.116828441619873
Validation loss: 2.2557149984503306

Epoch: 6| Step: 10
Training loss: 2.695838451385498
Validation loss: 2.239215117628856

Epoch: 6| Step: 11
Training loss: 1.7894132137298584
Validation loss: 2.231810208289854

Epoch: 6| Step: 12
Training loss: 1.7635371685028076
Validation loss: 2.221196151548816

Epoch: 6| Step: 13
Training loss: 2.4435105323791504
Validation loss: 2.2167460328789166

Epoch: 97| Step: 0
Training loss: 2.782374382019043
Validation loss: 2.2242598943812872

Epoch: 6| Step: 1
Training loss: 2.7164978981018066
Validation loss: 2.2219645182291665

Epoch: 6| Step: 2
Training loss: 2.2185089588165283
Validation loss: 2.2211585762680217

Epoch: 6| Step: 3
Training loss: 2.809851884841919
Validation loss: 2.2246878582944154

Epoch: 6| Step: 4
Training loss: 2.7378454208374023
Validation loss: 2.2193400424013854

Epoch: 6| Step: 5
Training loss: 2.2825100421905518
Validation loss: 2.21720362735051

Epoch: 6| Step: 6
Training loss: 3.114168167114258
Validation loss: 2.2160864312161683

Epoch: 6| Step: 7
Training loss: 2.4147534370422363
Validation loss: 2.2161610459768646

Epoch: 6| Step: 8
Training loss: 2.4670321941375732
Validation loss: 2.217972027358188

Epoch: 6| Step: 9
Training loss: 2.4796433448791504
Validation loss: 2.2203225628022225

Epoch: 6| Step: 10
Training loss: 2.144437074661255
Validation loss: 2.2298564321251324

Epoch: 6| Step: 11
Training loss: 2.217099189758301
Validation loss: 2.227349001874206

Epoch: 6| Step: 12
Training loss: 2.417612314224243
Validation loss: 2.2359693101657334

Epoch: 6| Step: 13
Training loss: 2.018800973892212
Validation loss: 2.238362322571457

Epoch: 98| Step: 0
Training loss: 3.245457172393799
Validation loss: 2.239695349047261

Epoch: 6| Step: 1
Training loss: 1.7582569122314453
Validation loss: 2.2594854344603834

Epoch: 6| Step: 2
Training loss: 2.415168523788452
Validation loss: 2.2684786345369075

Epoch: 6| Step: 3
Training loss: 2.680044651031494
Validation loss: 2.2844099844655683

Epoch: 6| Step: 4
Training loss: 2.724802017211914
Validation loss: 2.2999223022050757

Epoch: 6| Step: 5
Training loss: 2.462944269180298
Validation loss: 2.319492837434174

Epoch: 6| Step: 6
Training loss: 3.160583019256592
Validation loss: 2.3060275252147386

Epoch: 6| Step: 7
Training loss: 2.8291473388671875
Validation loss: 2.3075414703738306

Epoch: 6| Step: 8
Training loss: 2.6904473304748535
Validation loss: 2.2704166032934703

Epoch: 6| Step: 9
Training loss: 2.806779623031616
Validation loss: 2.2395191243899766

Epoch: 6| Step: 10
Training loss: 1.3657565116882324
Validation loss: 2.2299581573855494

Epoch: 6| Step: 11
Training loss: 2.281860113143921
Validation loss: 2.223697411116733

Epoch: 6| Step: 12
Training loss: 1.8687195777893066
Validation loss: 2.2198275648137575

Epoch: 6| Step: 13
Training loss: 2.834742546081543
Validation loss: 2.215497338643638

Epoch: 99| Step: 0
Training loss: 2.6048548221588135
Validation loss: 2.210650843958701

Epoch: 6| Step: 1
Training loss: 1.8517659902572632
Validation loss: 2.2151692349423646

Epoch: 6| Step: 2
Training loss: 2.838649034500122
Validation loss: 2.221044191750147

Epoch: 6| Step: 3
Training loss: 2.6551690101623535
Validation loss: 2.2146840403156896

Epoch: 6| Step: 4
Training loss: 2.840373992919922
Validation loss: 2.223544630953061

Epoch: 6| Step: 5
Training loss: 2.5074291229248047
Validation loss: 2.21655265490214

Epoch: 6| Step: 6
Training loss: 3.1225318908691406
Validation loss: 2.211962474289761

Epoch: 6| Step: 7
Training loss: 1.637010097503662
Validation loss: 2.206390534677813

Epoch: 6| Step: 8
Training loss: 2.1037070751190186
Validation loss: 2.207226248197658

Epoch: 6| Step: 9
Training loss: 3.0904629230499268
Validation loss: 2.2095985053687968

Epoch: 6| Step: 10
Training loss: 2.7534008026123047
Validation loss: 2.2142251665874193

Epoch: 6| Step: 11
Training loss: 2.618466377258301
Validation loss: 2.2267735670971613

Epoch: 6| Step: 12
Training loss: 2.1852049827575684
Validation loss: 2.230638578373899

Epoch: 6| Step: 13
Training loss: 1.7129731178283691
Validation loss: 2.243896738175423

Epoch: 100| Step: 0
Training loss: 2.6846060752868652
Validation loss: 2.2500460711858605

Epoch: 6| Step: 1
Training loss: 2.499666452407837
Validation loss: 2.246335783312398

Epoch: 6| Step: 2
Training loss: 2.4083988666534424
Validation loss: 2.247104310220288

Epoch: 6| Step: 3
Training loss: 3.0370662212371826
Validation loss: 2.2342429340526624

Epoch: 6| Step: 4
Training loss: 2.2580790519714355
Validation loss: 2.2255719451494116

Epoch: 6| Step: 5
Training loss: 2.558584213256836
Validation loss: 2.211139750737016

Epoch: 6| Step: 6
Training loss: 2.432737112045288
Validation loss: 2.20183898556617

Epoch: 6| Step: 7
Training loss: 2.6962130069732666
Validation loss: 2.1952756886841147

Epoch: 6| Step: 8
Training loss: 2.4496042728424072
Validation loss: 2.2027677951320523

Epoch: 6| Step: 9
Training loss: 2.359543800354004
Validation loss: 2.1934892490345943

Epoch: 6| Step: 10
Training loss: 1.6614001989364624
Validation loss: 2.1981968418244393

Epoch: 6| Step: 11
Training loss: 3.1185717582702637
Validation loss: 2.198436076923083

Epoch: 6| Step: 12
Training loss: 2.6342034339904785
Validation loss: 2.2015583489530828

Epoch: 6| Step: 13
Training loss: 1.6510038375854492
Validation loss: 2.200570885853101

Epoch: 101| Step: 0
Training loss: 2.3060555458068848
Validation loss: 2.2092102009763

Epoch: 6| Step: 1
Training loss: 2.683635711669922
Validation loss: 2.2198281775238695

Epoch: 6| Step: 2
Training loss: 2.659836530685425
Validation loss: 2.2109811741818666

Epoch: 6| Step: 3
Training loss: 2.4593114852905273
Validation loss: 2.229229724535378

Epoch: 6| Step: 4
Training loss: 2.9839882850646973
Validation loss: 2.227619283942766

Epoch: 6| Step: 5
Training loss: 2.497426986694336
Validation loss: 2.249108350405129

Epoch: 6| Step: 6
Training loss: 1.8727848529815674
Validation loss: 2.2415332640371015

Epoch: 6| Step: 7
Training loss: 2.3170735836029053
Validation loss: 2.254149224168511

Epoch: 6| Step: 8
Training loss: 2.6246542930603027
Validation loss: 2.2596582597301853

Epoch: 6| Step: 9
Training loss: 2.6475305557250977
Validation loss: 2.2512002119454007

Epoch: 6| Step: 10
Training loss: 2.07169246673584
Validation loss: 2.242980703230827

Epoch: 6| Step: 11
Training loss: 2.916431188583374
Validation loss: 2.2249568944336264

Epoch: 6| Step: 12
Training loss: 1.86183500289917
Validation loss: 2.213859227395827

Epoch: 6| Step: 13
Training loss: 3.0662031173706055
Validation loss: 2.215358787967313

Epoch: 102| Step: 0
Training loss: 2.315642833709717
Validation loss: 2.202963690603933

Epoch: 6| Step: 1
Training loss: 2.7950198650360107
Validation loss: 2.20926002020477

Epoch: 6| Step: 2
Training loss: 1.9729255437850952
Validation loss: 2.212985720685733

Epoch: 6| Step: 3
Training loss: 2.5682575702667236
Validation loss: 2.2068067930077993

Epoch: 6| Step: 4
Training loss: 2.6979808807373047
Validation loss: 2.209266360088061

Epoch: 6| Step: 5
Training loss: 2.787783622741699
Validation loss: 2.2171307276653986

Epoch: 6| Step: 6
Training loss: 2.997561454772949
Validation loss: 2.213228915327339

Epoch: 6| Step: 7
Training loss: 2.27397084236145
Validation loss: 2.2132122132085983

Epoch: 6| Step: 8
Training loss: 2.0066304206848145
Validation loss: 2.2184852271951656

Epoch: 6| Step: 9
Training loss: 2.6810152530670166
Validation loss: 2.2154817094085035

Epoch: 6| Step: 10
Training loss: 2.1774630546569824
Validation loss: 2.213528222935174

Epoch: 6| Step: 11
Training loss: 2.3579912185668945
Validation loss: 2.229404157207858

Epoch: 6| Step: 12
Training loss: 2.42378306388855
Validation loss: 2.2382708159826135

Epoch: 6| Step: 13
Training loss: 2.478671073913574
Validation loss: 2.2465027686088317

Epoch: 103| Step: 0
Training loss: 2.0610547065734863
Validation loss: 2.2752527677884666

Epoch: 6| Step: 1
Training loss: 2.4364418983459473
Validation loss: 2.3117849762721727

Epoch: 6| Step: 2
Training loss: 2.724567413330078
Validation loss: 2.3462613987666305

Epoch: 6| Step: 3
Training loss: 2.256228446960449
Validation loss: 2.355680165752288

Epoch: 6| Step: 4
Training loss: 2.7586259841918945
Validation loss: 2.344180776226905

Epoch: 6| Step: 5
Training loss: 2.4670116901397705
Validation loss: 2.3453040866441626

Epoch: 6| Step: 6
Training loss: 2.6658618450164795
Validation loss: 2.2874485113287486

Epoch: 6| Step: 7
Training loss: 3.245823860168457
Validation loss: 2.2516834274415047

Epoch: 6| Step: 8
Training loss: 1.8850703239440918
Validation loss: 2.22705166826966

Epoch: 6| Step: 9
Training loss: 2.1984481811523438
Validation loss: 2.2051437824003157

Epoch: 6| Step: 10
Training loss: 2.117924928665161
Validation loss: 2.1962695775493497

Epoch: 6| Step: 11
Training loss: 2.891555070877075
Validation loss: 2.186310137471845

Epoch: 6| Step: 12
Training loss: 2.6786270141601562
Validation loss: 2.1859919768507763

Epoch: 6| Step: 13
Training loss: 2.3409817218780518
Validation loss: 2.186120202464442

Epoch: 104| Step: 0
Training loss: 2.2205491065979004
Validation loss: 2.1859840859649

Epoch: 6| Step: 1
Training loss: 2.5043771266937256
Validation loss: 2.1878959645507154

Epoch: 6| Step: 2
Training loss: 2.367926597595215
Validation loss: 2.1856428705235964

Epoch: 6| Step: 3
Training loss: 3.028994083404541
Validation loss: 2.184231055680142

Epoch: 6| Step: 4
Training loss: 2.905247449874878
Validation loss: 2.180051367769959

Epoch: 6| Step: 5
Training loss: 2.617108106613159
Validation loss: 2.178125673724759

Epoch: 6| Step: 6
Training loss: 2.583451271057129
Validation loss: 2.1803446123676915

Epoch: 6| Step: 7
Training loss: 1.78670334815979
Validation loss: 2.182381063379267

Epoch: 6| Step: 8
Training loss: 2.2739691734313965
Validation loss: 2.1794438900486117

Epoch: 6| Step: 9
Training loss: 2.628788471221924
Validation loss: 2.1704944538813766

Epoch: 6| Step: 10
Training loss: 2.458134889602661
Validation loss: 2.173809592441846

Epoch: 6| Step: 11
Training loss: 2.1191821098327637
Validation loss: 2.177878172166886

Epoch: 6| Step: 12
Training loss: 2.8314709663391113
Validation loss: 2.190241139422181

Epoch: 6| Step: 13
Training loss: 2.5130608081817627
Validation loss: 2.2134049105387863

Epoch: 105| Step: 0
Training loss: 2.318697929382324
Validation loss: 2.2360044781879713

Epoch: 6| Step: 1
Training loss: 2.85701847076416
Validation loss: 2.245762622484597

Epoch: 6| Step: 2
Training loss: 2.282581329345703
Validation loss: 2.236658006586054

Epoch: 6| Step: 3
Training loss: 2.115116834640503
Validation loss: 2.2527079300213884

Epoch: 6| Step: 4
Training loss: 2.9947152137756348
Validation loss: 2.2280380238768873

Epoch: 6| Step: 5
Training loss: 1.987941026687622
Validation loss: 2.231552705969862

Epoch: 6| Step: 6
Training loss: 2.4164717197418213
Validation loss: 2.23730392097145

Epoch: 6| Step: 7
Training loss: 2.010268211364746
Validation loss: 2.224258235705796

Epoch: 6| Step: 8
Training loss: 2.901279926300049
Validation loss: 2.2116004702865437

Epoch: 6| Step: 9
Training loss: 2.3730292320251465
Validation loss: 2.203859638142329

Epoch: 6| Step: 10
Training loss: 2.304546594619751
Validation loss: 2.210195661872946

Epoch: 6| Step: 11
Training loss: 2.991809368133545
Validation loss: 2.199178793097055

Epoch: 6| Step: 12
Training loss: 2.228093385696411
Validation loss: 2.1950335118078415

Epoch: 6| Step: 13
Training loss: 3.095137357711792
Validation loss: 2.1901660324424825

Epoch: 106| Step: 0
Training loss: 2.0127370357513428
Validation loss: 2.1843480910024335

Epoch: 6| Step: 1
Training loss: 2.8594725131988525
Validation loss: 2.185083830228416

Epoch: 6| Step: 2
Training loss: 2.6208252906799316
Validation loss: 2.184983827734506

Epoch: 6| Step: 3
Training loss: 2.799111843109131
Validation loss: 2.200116490805021

Epoch: 6| Step: 4
Training loss: 2.5372025966644287
Validation loss: 2.1885807924373175

Epoch: 6| Step: 5
Training loss: 2.6205191612243652
Validation loss: 2.195164684326418

Epoch: 6| Step: 6
Training loss: 2.226440191268921
Validation loss: 2.20738677055605

Epoch: 6| Step: 7
Training loss: 2.122081756591797
Validation loss: 2.1959742628118044

Epoch: 6| Step: 8
Training loss: 1.245729684829712
Validation loss: 2.2030434300822597

Epoch: 6| Step: 9
Training loss: 3.464472770690918
Validation loss: 2.185993855999362

Epoch: 6| Step: 10
Training loss: 1.8324413299560547
Validation loss: 2.1801126515993507

Epoch: 6| Step: 11
Training loss: 2.465862274169922
Validation loss: 2.1827946503957114

Epoch: 6| Step: 12
Training loss: 3.066793203353882
Validation loss: 2.186905494300268

Epoch: 6| Step: 13
Training loss: 2.540919780731201
Validation loss: 2.1930420065438874

Epoch: 107| Step: 0
Training loss: 2.415971040725708
Validation loss: 2.2034751292197936

Epoch: 6| Step: 1
Training loss: 2.3368148803710938
Validation loss: 2.2100276716293825

Epoch: 6| Step: 2
Training loss: 2.9811863899230957
Validation loss: 2.225868537861814

Epoch: 6| Step: 3
Training loss: 2.771710157394409
Validation loss: 2.2333211552712227

Epoch: 6| Step: 4
Training loss: 2.5245141983032227
Validation loss: 2.232217332368256

Epoch: 6| Step: 5
Training loss: 2.6286873817443848
Validation loss: 2.2237754316740137

Epoch: 6| Step: 6
Training loss: 3.0539798736572266
Validation loss: 2.213267021281745

Epoch: 6| Step: 7
Training loss: 2.5566768646240234
Validation loss: 2.2099480539239864

Epoch: 6| Step: 8
Training loss: 2.1404545307159424
Validation loss: 2.1815714220846854

Epoch: 6| Step: 9
Training loss: 1.7734261751174927
Validation loss: 2.168388374390141

Epoch: 6| Step: 10
Training loss: 1.6860324144363403
Validation loss: 2.1698079724465646

Epoch: 6| Step: 11
Training loss: 2.328444480895996
Validation loss: 2.160713167600734

Epoch: 6| Step: 12
Training loss: 2.8149325847625732
Validation loss: 2.1566636177801315

Epoch: 6| Step: 13
Training loss: 1.931805968284607
Validation loss: 2.160261583584611

Epoch: 108| Step: 0
Training loss: 1.9239411354064941
Validation loss: 2.1585427663659535

Epoch: 6| Step: 1
Training loss: 2.6279544830322266
Validation loss: 2.1538320690072994

Epoch: 6| Step: 2
Training loss: 2.338054656982422
Validation loss: 2.169243251123736

Epoch: 6| Step: 3
Training loss: 2.492205858230591
Validation loss: 2.1586400334553053

Epoch: 6| Step: 4
Training loss: 2.9751501083374023
Validation loss: 2.161318584155011

Epoch: 6| Step: 5
Training loss: 2.0793774127960205
Validation loss: 2.1569557587305703

Epoch: 6| Step: 6
Training loss: 2.4850292205810547
Validation loss: 2.1546091841113184

Epoch: 6| Step: 7
Training loss: 2.4049901962280273
Validation loss: 2.1641204100783153

Epoch: 6| Step: 8
Training loss: 2.389477252960205
Validation loss: 2.152362160785224

Epoch: 6| Step: 9
Training loss: 2.587139129638672
Validation loss: 2.1509181402062856

Epoch: 6| Step: 10
Training loss: 2.1325418949127197
Validation loss: 2.150194037345148

Epoch: 6| Step: 11
Training loss: 2.448272466659546
Validation loss: 2.1616921745320803

Epoch: 6| Step: 12
Training loss: 2.8511853218078613
Validation loss: 2.1872748790248746

Epoch: 6| Step: 13
Training loss: 2.3639721870422363
Validation loss: 2.2285549358655046

Epoch: 109| Step: 0
Training loss: 2.6498286724090576
Validation loss: 2.275321134956934

Epoch: 6| Step: 1
Training loss: 2.9807677268981934
Validation loss: 2.309940858553815

Epoch: 6| Step: 2
Training loss: 2.8135478496551514
Validation loss: 2.2875374773497223

Epoch: 6| Step: 3
Training loss: 2.4353370666503906
Validation loss: 2.286348242913523

Epoch: 6| Step: 4
Training loss: 1.978444218635559
Validation loss: 2.2789042713821575

Epoch: 6| Step: 5
Training loss: 1.539832592010498
Validation loss: 2.245781816462035

Epoch: 6| Step: 6
Training loss: 2.555699348449707
Validation loss: 2.2092567182356313

Epoch: 6| Step: 7
Training loss: 2.552077054977417
Validation loss: 2.194307683616556

Epoch: 6| Step: 8
Training loss: 2.666944980621338
Validation loss: 2.1865385424706245

Epoch: 6| Step: 9
Training loss: 3.2196736335754395
Validation loss: 2.1736054676835255

Epoch: 6| Step: 10
Training loss: 2.2864937782287598
Validation loss: 2.170054442139082

Epoch: 6| Step: 11
Training loss: 2.5849967002868652
Validation loss: 2.182375946352559

Epoch: 6| Step: 12
Training loss: 1.6005923748016357
Validation loss: 2.18922088479483

Epoch: 6| Step: 13
Training loss: 2.731649398803711
Validation loss: 2.1985064347585044

Epoch: 110| Step: 0
Training loss: 2.7159581184387207
Validation loss: 2.193170291121288

Epoch: 6| Step: 1
Training loss: 2.199498176574707
Validation loss: 2.174410596970589

Epoch: 6| Step: 2
Training loss: 2.124790906906128
Validation loss: 2.161411141836515

Epoch: 6| Step: 3
Training loss: 2.123014450073242
Validation loss: 2.1419519045019664

Epoch: 6| Step: 4
Training loss: 2.626950263977051
Validation loss: 2.138220217920119

Epoch: 6| Step: 5
Training loss: 2.5214345455169678
Validation loss: 2.134507135678363

Epoch: 6| Step: 6
Training loss: 2.8641157150268555
Validation loss: 2.136144830334571

Epoch: 6| Step: 7
Training loss: 2.458214521408081
Validation loss: 2.146858788305713

Epoch: 6| Step: 8
Training loss: 2.442265033721924
Validation loss: 2.151986368240849

Epoch: 6| Step: 9
Training loss: 2.844893455505371
Validation loss: 2.160998039348151

Epoch: 6| Step: 10
Training loss: 2.1109957695007324
Validation loss: 2.1786204999493015

Epoch: 6| Step: 11
Training loss: 2.1294636726379395
Validation loss: 2.2184088947952434

Epoch: 6| Step: 12
Training loss: 2.5869431495666504
Validation loss: 2.2205861306959584

Epoch: 6| Step: 13
Training loss: 2.881830930709839
Validation loss: 2.21548701870826

Epoch: 111| Step: 0
Training loss: 2.2097291946411133
Validation loss: 2.208304871794998

Epoch: 6| Step: 1
Training loss: 2.1805694103240967
Validation loss: 2.192232740822659

Epoch: 6| Step: 2
Training loss: 2.779435157775879
Validation loss: 2.187035729808192

Epoch: 6| Step: 3
Training loss: 3.1908962726593018
Validation loss: 2.178257585853659

Epoch: 6| Step: 4
Training loss: 2.253042221069336
Validation loss: 2.159196102490989

Epoch: 6| Step: 5
Training loss: 2.4577713012695312
Validation loss: 2.154365675423735

Epoch: 6| Step: 6
Training loss: 2.8443379402160645
Validation loss: 2.1603155495018087

Epoch: 6| Step: 7
Training loss: 2.0409605503082275
Validation loss: 2.1649354632182787

Epoch: 6| Step: 8
Training loss: 1.5839223861694336
Validation loss: 2.1643553344152306

Epoch: 6| Step: 9
Training loss: 1.9219194650650024
Validation loss: 2.1636949636602916

Epoch: 6| Step: 10
Training loss: 2.4275131225585938
Validation loss: 2.151739133301602

Epoch: 6| Step: 11
Training loss: 2.7139194011688232
Validation loss: 2.147985981356713

Epoch: 6| Step: 12
Training loss: 3.073662757873535
Validation loss: 2.150024913972424

Epoch: 6| Step: 13
Training loss: 2.347400665283203
Validation loss: 2.1575954293691986

Epoch: 112| Step: 0
Training loss: 2.107884407043457
Validation loss: 2.1565172544089695

Epoch: 6| Step: 1
Training loss: 3.186771869659424
Validation loss: 2.158793721147763

Epoch: 6| Step: 2
Training loss: 1.8584551811218262
Validation loss: 2.1703318780468357

Epoch: 6| Step: 3
Training loss: 1.7559990882873535
Validation loss: 2.195965588733714

Epoch: 6| Step: 4
Training loss: 2.621847629547119
Validation loss: 2.2227505201934488

Epoch: 6| Step: 5
Training loss: 1.948195457458496
Validation loss: 2.2443593804554274

Epoch: 6| Step: 6
Training loss: 2.6890976428985596
Validation loss: 2.2682381573543755

Epoch: 6| Step: 7
Training loss: 2.324188709259033
Validation loss: 2.2605257905939573

Epoch: 6| Step: 8
Training loss: 3.386898994445801
Validation loss: 2.249605627470119

Epoch: 6| Step: 9
Training loss: 3.3310842514038086
Validation loss: 2.215061433853642

Epoch: 6| Step: 10
Training loss: 2.4333529472351074
Validation loss: 2.1908546801536315

Epoch: 6| Step: 11
Training loss: 2.357297420501709
Validation loss: 2.1589113461074008

Epoch: 6| Step: 12
Training loss: 1.9802266359329224
Validation loss: 2.1466721821856756

Epoch: 6| Step: 13
Training loss: 2.320033073425293
Validation loss: 2.1337688789572766

Epoch: 113| Step: 0
Training loss: 2.1238465309143066
Validation loss: 2.1378141808253464

Epoch: 6| Step: 1
Training loss: 2.8197269439697266
Validation loss: 2.1315418443372174

Epoch: 6| Step: 2
Training loss: 2.2110233306884766
Validation loss: 2.1255555588711976

Epoch: 6| Step: 3
Training loss: 2.388007164001465
Validation loss: 2.125785894291375

Epoch: 6| Step: 4
Training loss: 2.3229408264160156
Validation loss: 2.121566157187185

Epoch: 6| Step: 5
Training loss: 1.8240007162094116
Validation loss: 2.1280433926531064

Epoch: 6| Step: 6
Training loss: 2.768446445465088
Validation loss: 2.1336002375489924

Epoch: 6| Step: 7
Training loss: 3.1978108882904053
Validation loss: 2.134073990647511

Epoch: 6| Step: 8
Training loss: 3.146587371826172
Validation loss: 2.1432521009957917

Epoch: 6| Step: 9
Training loss: 2.8529090881347656
Validation loss: 2.152353799471291

Epoch: 6| Step: 10
Training loss: 1.6063838005065918
Validation loss: 2.1587083160236316

Epoch: 6| Step: 11
Training loss: 2.7115726470947266
Validation loss: 2.1653450637735348

Epoch: 6| Step: 12
Training loss: 2.342015504837036
Validation loss: 2.178073957402219

Epoch: 6| Step: 13
Training loss: 1.3938747644424438
Validation loss: 2.186322860820319

Epoch: 114| Step: 0
Training loss: 2.7134017944335938
Validation loss: 2.171734530438659

Epoch: 6| Step: 1
Training loss: 2.2219362258911133
Validation loss: 2.1516407343649093

Epoch: 6| Step: 2
Training loss: 2.6874566078186035
Validation loss: 2.1474826028270106

Epoch: 6| Step: 3
Training loss: 2.472433090209961
Validation loss: 2.1353603434819046

Epoch: 6| Step: 4
Training loss: 1.6738286018371582
Validation loss: 2.132971368810182

Epoch: 6| Step: 5
Training loss: 2.4676883220672607
Validation loss: 2.128329162956566

Epoch: 6| Step: 6
Training loss: 2.7449193000793457
Validation loss: 2.1237423471225205

Epoch: 6| Step: 7
Training loss: 2.0233047008514404
Validation loss: 2.133600709258869

Epoch: 6| Step: 8
Training loss: 1.8109101057052612
Validation loss: 2.136136762557491

Epoch: 6| Step: 9
Training loss: 2.6861367225646973
Validation loss: 2.152006677401963

Epoch: 6| Step: 10
Training loss: 2.967681884765625
Validation loss: 2.1390179664857927

Epoch: 6| Step: 11
Training loss: 2.4477145671844482
Validation loss: 2.143981756702546

Epoch: 6| Step: 12
Training loss: 2.524721622467041
Validation loss: 2.1567362457193355

Epoch: 6| Step: 13
Training loss: 2.708197832107544
Validation loss: 2.166992523336923

Epoch: 115| Step: 0
Training loss: 2.181142807006836
Validation loss: 2.177550467111731

Epoch: 6| Step: 1
Training loss: 2.8419036865234375
Validation loss: 2.1871672202182073

Epoch: 6| Step: 2
Training loss: 2.6604623794555664
Validation loss: 2.190659514037512

Epoch: 6| Step: 3
Training loss: 1.8081674575805664
Validation loss: 2.1855567732164936

Epoch: 6| Step: 4
Training loss: 1.9418303966522217
Validation loss: 2.1925664845333306

Epoch: 6| Step: 5
Training loss: 2.103180408477783
Validation loss: 2.1794215222840667

Epoch: 6| Step: 6
Training loss: 3.164534091949463
Validation loss: 2.1723660679273706

Epoch: 6| Step: 7
Training loss: 2.75484299659729
Validation loss: 2.1535963601963495

Epoch: 6| Step: 8
Training loss: 1.9802629947662354
Validation loss: 2.153645807696927

Epoch: 6| Step: 9
Training loss: 2.50372314453125
Validation loss: 2.152733354158299

Epoch: 6| Step: 10
Training loss: 2.210239887237549
Validation loss: 2.1593440322465796

Epoch: 6| Step: 11
Training loss: 2.4278483390808105
Validation loss: 2.1593080207865727

Epoch: 6| Step: 12
Training loss: 2.72157621383667
Validation loss: 2.152861702826715

Epoch: 6| Step: 13
Training loss: 2.5860416889190674
Validation loss: 2.1519949256732898

Epoch: 116| Step: 0
Training loss: 2.743421792984009
Validation loss: 2.151000461270732

Epoch: 6| Step: 1
Training loss: 1.8441784381866455
Validation loss: 2.139226218705536

Epoch: 6| Step: 2
Training loss: 2.5581767559051514
Validation loss: 2.1456398015381186

Epoch: 6| Step: 3
Training loss: 2.902355670928955
Validation loss: 2.129379298097344

Epoch: 6| Step: 4
Training loss: 2.1636390686035156
Validation loss: 2.1336348056793213

Epoch: 6| Step: 5
Training loss: 2.1676478385925293
Validation loss: 2.127879265815981

Epoch: 6| Step: 6
Training loss: 2.261049270629883
Validation loss: 2.1347966206971036

Epoch: 6| Step: 7
Training loss: 2.335503101348877
Validation loss: 2.13007519578421

Epoch: 6| Step: 8
Training loss: 2.9339001178741455
Validation loss: 2.1281665935311267

Epoch: 6| Step: 9
Training loss: 2.3520543575286865
Validation loss: 2.127716411826431

Epoch: 6| Step: 10
Training loss: 2.5602798461914062
Validation loss: 2.1254778754326606

Epoch: 6| Step: 11
Training loss: 2.3169891834259033
Validation loss: 2.1338923156902356

Epoch: 6| Step: 12
Training loss: 2.5877857208251953
Validation loss: 2.1393552775024087

Epoch: 6| Step: 13
Training loss: 1.6096245050430298
Validation loss: 2.138426019299415

Epoch: 117| Step: 0
Training loss: 2.61801815032959
Validation loss: 2.135759340819492

Epoch: 6| Step: 1
Training loss: 2.2165443897247314
Validation loss: 2.137341709547145

Epoch: 6| Step: 2
Training loss: 1.8585776090621948
Validation loss: 2.138555254987491

Epoch: 6| Step: 3
Training loss: 2.420821189880371
Validation loss: 2.137837647109903

Epoch: 6| Step: 4
Training loss: 2.780132293701172
Validation loss: 2.128354136661817

Epoch: 6| Step: 5
Training loss: 3.017869472503662
Validation loss: 2.1308799866707093

Epoch: 6| Step: 6
Training loss: 1.874875783920288
Validation loss: 2.137385199146886

Epoch: 6| Step: 7
Training loss: 3.261533260345459
Validation loss: 2.1378025931696736

Epoch: 6| Step: 8
Training loss: 2.0826644897460938
Validation loss: 2.1311105541003648

Epoch: 6| Step: 9
Training loss: 2.756143093109131
Validation loss: 2.1363406809427405

Epoch: 6| Step: 10
Training loss: 1.7740240097045898
Validation loss: 2.138323986402122

Epoch: 6| Step: 11
Training loss: 2.1260294914245605
Validation loss: 2.1345428830833844

Epoch: 6| Step: 12
Training loss: 2.085477352142334
Validation loss: 2.1489048837333597

Epoch: 6| Step: 13
Training loss: 2.9171459674835205
Validation loss: 2.1504781361549132

Epoch: 118| Step: 0
Training loss: 2.5676515102386475
Validation loss: 2.164787677026564

Epoch: 6| Step: 1
Training loss: 2.0463757514953613
Validation loss: 2.158866379850654

Epoch: 6| Step: 2
Training loss: 2.328084707260132
Validation loss: 2.1432816008085847

Epoch: 6| Step: 3
Training loss: 2.059220790863037
Validation loss: 2.1324318814021286

Epoch: 6| Step: 4
Training loss: 2.298949956893921
Validation loss: 2.1210957560487973

Epoch: 6| Step: 5
Training loss: 1.3048741817474365
Validation loss: 2.112963362406659

Epoch: 6| Step: 6
Training loss: 2.5827438831329346
Validation loss: 2.1208446410394486

Epoch: 6| Step: 7
Training loss: 2.6641221046447754
Validation loss: 2.1229666445844915

Epoch: 6| Step: 8
Training loss: 2.126476287841797
Validation loss: 2.112114091073313

Epoch: 6| Step: 9
Training loss: 2.6602976322174072
Validation loss: 2.11784057207005

Epoch: 6| Step: 10
Training loss: 2.7342777252197266
Validation loss: 2.116644097912696

Epoch: 6| Step: 11
Training loss: 2.5322399139404297
Validation loss: 2.1254407744253836

Epoch: 6| Step: 12
Training loss: 2.8364579677581787
Validation loss: 2.1199528709534676

Epoch: 6| Step: 13
Training loss: 3.128201484680176
Validation loss: 2.1276062560337845

Epoch: 119| Step: 0
Training loss: 2.9344828128814697
Validation loss: 2.1410128685735885

Epoch: 6| Step: 1
Training loss: 1.6375372409820557
Validation loss: 2.162336672506025

Epoch: 6| Step: 2
Training loss: 1.8630722761154175
Validation loss: 2.180008936953801

Epoch: 6| Step: 3
Training loss: 2.442866802215576
Validation loss: 2.2069815897172496

Epoch: 6| Step: 4
Training loss: 2.474858045578003
Validation loss: 2.227348650655439

Epoch: 6| Step: 5
Training loss: 2.267124652862549
Validation loss: 2.220926910318354

Epoch: 6| Step: 6
Training loss: 2.274406909942627
Validation loss: 2.2154824144096783

Epoch: 6| Step: 7
Training loss: 2.632857322692871
Validation loss: 2.190599864529025

Epoch: 6| Step: 8
Training loss: 2.5355188846588135
Validation loss: 2.182375746388589

Epoch: 6| Step: 9
Training loss: 3.104309320449829
Validation loss: 2.1505970967713224

Epoch: 6| Step: 10
Training loss: 2.5142946243286133
Validation loss: 2.1428747305306057

Epoch: 6| Step: 11
Training loss: 2.220902442932129
Validation loss: 2.144881256165043

Epoch: 6| Step: 12
Training loss: 1.8944754600524902
Validation loss: 2.1431967930127214

Epoch: 6| Step: 13
Training loss: 3.242089033126831
Validation loss: 2.1405531321802447

Epoch: 120| Step: 0
Training loss: 2.9163167476654053
Validation loss: 2.154168523767943

Epoch: 6| Step: 1
Training loss: 1.8967244625091553
Validation loss: 2.154666649397983

Epoch: 6| Step: 2
Training loss: 2.8575236797332764
Validation loss: 2.1713485179408902

Epoch: 6| Step: 3
Training loss: 1.9805148839950562
Validation loss: 2.1716017441083024

Epoch: 6| Step: 4
Training loss: 2.6243624687194824
Validation loss: 2.1760480173172487

Epoch: 6| Step: 5
Training loss: 2.2478690147399902
Validation loss: 2.1762553748264106

Epoch: 6| Step: 6
Training loss: 2.8856239318847656
Validation loss: 2.1784221741460983

Epoch: 6| Step: 7
Training loss: 2.9726414680480957
Validation loss: 2.205585495118172

Epoch: 6| Step: 8
Training loss: 2.542600154876709
Validation loss: 2.2077184236177834

Epoch: 6| Step: 9
Training loss: 2.224472999572754
Validation loss: 2.2201307665917183

Epoch: 6| Step: 10
Training loss: 1.8251286745071411
Validation loss: 2.211790341202931

Epoch: 6| Step: 11
Training loss: 3.105684518814087
Validation loss: 2.19744219574877

Epoch: 6| Step: 12
Training loss: 1.5414928197860718
Validation loss: 2.1660949286594184

Epoch: 6| Step: 13
Training loss: 2.038358688354492
Validation loss: 2.1342997268963884

Epoch: 121| Step: 0
Training loss: 2.5270586013793945
Validation loss: 2.1328203473039853

Epoch: 6| Step: 1
Training loss: 2.365846633911133
Validation loss: 2.118259463258969

Epoch: 6| Step: 2
Training loss: 2.5296406745910645
Validation loss: 2.128917973528626

Epoch: 6| Step: 3
Training loss: 2.185109853744507
Validation loss: 2.129715552894018

Epoch: 6| Step: 4
Training loss: 1.603107213973999
Validation loss: 2.1336447372231433

Epoch: 6| Step: 5
Training loss: 2.0478665828704834
Validation loss: 2.122581981843518

Epoch: 6| Step: 6
Training loss: 2.586648941040039
Validation loss: 2.0960495497590754

Epoch: 6| Step: 7
Training loss: 2.629992723464966
Validation loss: 2.1071322259082588

Epoch: 6| Step: 8
Training loss: 2.5188300609588623
Validation loss: 2.100794110246884

Epoch: 6| Step: 9
Training loss: 2.366112232208252
Validation loss: 2.1001219749450684

Epoch: 6| Step: 10
Training loss: 2.1964335441589355
Validation loss: 2.1045240586803806

Epoch: 6| Step: 11
Training loss: 2.731081008911133
Validation loss: 2.1177782371479976

Epoch: 6| Step: 12
Training loss: 2.687708854675293
Validation loss: 2.1092916380974556

Epoch: 6| Step: 13
Training loss: 2.7824008464813232
Validation loss: 2.1128511672378867

Epoch: 122| Step: 0
Training loss: 2.0086498260498047
Validation loss: 2.1179429549042896

Epoch: 6| Step: 1
Training loss: 2.2872672080993652
Validation loss: 2.1098705081529516

Epoch: 6| Step: 2
Training loss: 2.00746750831604
Validation loss: 2.1118172650696128

Epoch: 6| Step: 3
Training loss: 2.58137845993042
Validation loss: 2.1234773256445445

Epoch: 6| Step: 4
Training loss: 1.9801175594329834
Validation loss: 2.118631174487452

Epoch: 6| Step: 5
Training loss: 2.2596654891967773
Validation loss: 2.14783900783908

Epoch: 6| Step: 6
Training loss: 2.0521256923675537
Validation loss: 2.165934185827932

Epoch: 6| Step: 7
Training loss: 2.581845760345459
Validation loss: 2.163677643704158

Epoch: 6| Step: 8
Training loss: 1.9534099102020264
Validation loss: 2.14338792011302

Epoch: 6| Step: 9
Training loss: 2.687643051147461
Validation loss: 2.1303041570930072

Epoch: 6| Step: 10
Training loss: 2.672630786895752
Validation loss: 2.1261151375309115

Epoch: 6| Step: 11
Training loss: 2.699962615966797
Validation loss: 2.116807226211794

Epoch: 6| Step: 12
Training loss: 3.1813735961914062
Validation loss: 2.1127133215627363

Epoch: 6| Step: 13
Training loss: 2.659290313720703
Validation loss: 2.11574137339028

Epoch: 123| Step: 0
Training loss: 2.3875808715820312
Validation loss: 2.1100156166220225

Epoch: 6| Step: 1
Training loss: 2.53700590133667
Validation loss: 2.1098006386910715

Epoch: 6| Step: 2
Training loss: 2.7336339950561523
Validation loss: 2.111313268702517

Epoch: 6| Step: 3
Training loss: 1.9791438579559326
Validation loss: 2.1066329197217057

Epoch: 6| Step: 4
Training loss: 2.775994300842285
Validation loss: 2.1128726749009985

Epoch: 6| Step: 5
Training loss: 2.289175271987915
Validation loss: 2.1119829736730105

Epoch: 6| Step: 6
Training loss: 2.039513111114502
Validation loss: 2.132461183814592

Epoch: 6| Step: 7
Training loss: 2.108816623687744
Validation loss: 2.1324699309564408

Epoch: 6| Step: 8
Training loss: 1.976605772972107
Validation loss: 2.134783665339152

Epoch: 6| Step: 9
Training loss: 2.453446388244629
Validation loss: 2.1404633932216193

Epoch: 6| Step: 10
Training loss: 2.473240852355957
Validation loss: 2.128725508207916

Epoch: 6| Step: 11
Training loss: 2.4600510597229004
Validation loss: 2.1107738851219096

Epoch: 6| Step: 12
Training loss: 2.613492965698242
Validation loss: 2.1132977701002553

Epoch: 6| Step: 13
Training loss: 2.426492691040039
Validation loss: 2.1030894863990044

Epoch: 124| Step: 0
Training loss: 2.2707479000091553
Validation loss: 2.1001779443474224

Epoch: 6| Step: 1
Training loss: 2.670773506164551
Validation loss: 2.095905015545507

Epoch: 6| Step: 2
Training loss: 2.46286678314209
Validation loss: 2.1020768022024505

Epoch: 6| Step: 3
Training loss: 2.123093366622925
Validation loss: 2.097808317471576

Epoch: 6| Step: 4
Training loss: 2.0015313625335693
Validation loss: 2.1028356193214335

Epoch: 6| Step: 5
Training loss: 2.2457275390625
Validation loss: 2.111626737861223

Epoch: 6| Step: 6
Training loss: 3.094796895980835
Validation loss: 2.1172240395699777

Epoch: 6| Step: 7
Training loss: 2.4910974502563477
Validation loss: 2.1225101947784424

Epoch: 6| Step: 8
Training loss: 1.9782874584197998
Validation loss: 2.1338212361899753

Epoch: 6| Step: 9
Training loss: 2.4338884353637695
Validation loss: 2.1444247896953295

Epoch: 6| Step: 10
Training loss: 3.104404926300049
Validation loss: 2.1609627405802407

Epoch: 6| Step: 11
Training loss: 1.706763744354248
Validation loss: 2.184975959921396

Epoch: 6| Step: 12
Training loss: 2.3757259845733643
Validation loss: 2.1966380816633984

Epoch: 6| Step: 13
Training loss: 2.054396629333496
Validation loss: 2.209925118313041

Epoch: 125| Step: 0
Training loss: 2.4825599193573
Validation loss: 2.1953722302631666

Epoch: 6| Step: 1
Training loss: 1.7263364791870117
Validation loss: 2.188101348056588

Epoch: 6| Step: 2
Training loss: 2.7424700260162354
Validation loss: 2.181464813088858

Epoch: 6| Step: 3
Training loss: 2.9898064136505127
Validation loss: 2.174691943712132

Epoch: 6| Step: 4
Training loss: 2.362328052520752
Validation loss: 2.1620710370361165

Epoch: 6| Step: 5
Training loss: 1.8545023202896118
Validation loss: 2.1357787347609

Epoch: 6| Step: 6
Training loss: 1.681356430053711
Validation loss: 2.1226332982381186

Epoch: 6| Step: 7
Training loss: 2.713829517364502
Validation loss: 2.121095108729537

Epoch: 6| Step: 8
Training loss: 2.2746129035949707
Validation loss: 2.1142125770609868

Epoch: 6| Step: 9
Training loss: 1.9072479009628296
Validation loss: 2.1242674832702964

Epoch: 6| Step: 10
Training loss: 2.5559492111206055
Validation loss: 2.1107828924732823

Epoch: 6| Step: 11
Training loss: 2.383596181869507
Validation loss: 2.1137273362887803

Epoch: 6| Step: 12
Training loss: 2.8228647708892822
Validation loss: 2.111608668040204

Epoch: 6| Step: 13
Training loss: 2.6579763889312744
Validation loss: 2.1157125452513337

Epoch: 126| Step: 0
Training loss: 2.1707448959350586
Validation loss: 2.10191055651634

Epoch: 6| Step: 1
Training loss: 2.5811285972595215
Validation loss: 2.117824646734422

Epoch: 6| Step: 2
Training loss: 2.285545825958252
Validation loss: 2.1110921264976583

Epoch: 6| Step: 3
Training loss: 2.5018842220306396
Validation loss: 2.1152424171406734

Epoch: 6| Step: 4
Training loss: 1.6836470365524292
Validation loss: 2.108437644538059

Epoch: 6| Step: 5
Training loss: 2.4223902225494385
Validation loss: 2.1204029206306703

Epoch: 6| Step: 6
Training loss: 2.0523223876953125
Validation loss: 2.1025113495447303

Epoch: 6| Step: 7
Training loss: 2.195115089416504
Validation loss: 2.1122092239318357

Epoch: 6| Step: 8
Training loss: 2.5768237113952637
Validation loss: 2.122130186327042

Epoch: 6| Step: 9
Training loss: 2.3902716636657715
Validation loss: 2.1008708451383855

Epoch: 6| Step: 10
Training loss: 2.387847661972046
Validation loss: 2.109311167911817

Epoch: 6| Step: 11
Training loss: 2.4100146293640137
Validation loss: 2.1132621918955157

Epoch: 6| Step: 12
Training loss: 3.190174102783203
Validation loss: 2.1267956495285034

Epoch: 6| Step: 13
Training loss: 2.0280096530914307
Validation loss: 2.16472969003903

Epoch: 127| Step: 0
Training loss: 3.2375571727752686
Validation loss: 2.1862143290940153

Epoch: 6| Step: 1
Training loss: 1.817909836769104
Validation loss: 2.199674662723336

Epoch: 6| Step: 2
Training loss: 2.6832780838012695
Validation loss: 2.2103019017045216

Epoch: 6| Step: 3
Training loss: 2.365297317504883
Validation loss: 2.1962309409213323

Epoch: 6| Step: 4
Training loss: 2.305352210998535
Validation loss: 2.173911835557671

Epoch: 6| Step: 5
Training loss: 2.553065299987793
Validation loss: 2.1438795546049714

Epoch: 6| Step: 6
Training loss: 2.1274960041046143
Validation loss: 2.126103731893724

Epoch: 6| Step: 7
Training loss: 3.203813076019287
Validation loss: 2.1271278268547467

Epoch: 6| Step: 8
Training loss: 2.4309329986572266
Validation loss: 2.1172751098550777

Epoch: 6| Step: 9
Training loss: 2.1651625633239746
Validation loss: 2.105261371981713

Epoch: 6| Step: 10
Training loss: 2.0709829330444336
Validation loss: 2.106337830584536

Epoch: 6| Step: 11
Training loss: 2.276279926300049
Validation loss: 2.1045835941068587

Epoch: 6| Step: 12
Training loss: 1.8000948429107666
Validation loss: 2.096617708923996

Epoch: 6| Step: 13
Training loss: 2.217831611633301
Validation loss: 2.098096170733052

Epoch: 128| Step: 0
Training loss: 3.072235584259033
Validation loss: 2.113873379204863

Epoch: 6| Step: 1
Training loss: 2.0481929779052734
Validation loss: 2.135991783552272

Epoch: 6| Step: 2
Training loss: 2.572110891342163
Validation loss: 2.1537083938557613

Epoch: 6| Step: 3
Training loss: 2.731081962585449
Validation loss: 2.1860342256484495

Epoch: 6| Step: 4
Training loss: 2.195115566253662
Validation loss: 2.1830202071897444

Epoch: 6| Step: 5
Training loss: 2.912930965423584
Validation loss: 2.1713502842892884

Epoch: 6| Step: 6
Training loss: 2.531959056854248
Validation loss: 2.150530613878722

Epoch: 6| Step: 7
Training loss: 1.8335859775543213
Validation loss: 2.1264274017785185

Epoch: 6| Step: 8
Training loss: 2.4283204078674316
Validation loss: 2.1071513147764307

Epoch: 6| Step: 9
Training loss: 1.880947470664978
Validation loss: 2.0849651444342827

Epoch: 6| Step: 10
Training loss: 2.099926710128784
Validation loss: 2.0725982983907065

Epoch: 6| Step: 11
Training loss: 2.3586504459381104
Validation loss: 2.0722085968140633

Epoch: 6| Step: 12
Training loss: 2.1612014770507812
Validation loss: 2.0763126368163736

Epoch: 6| Step: 13
Training loss: 2.1124753952026367
Validation loss: 2.073378703927481

Epoch: 129| Step: 0
Training loss: 2.0025439262390137
Validation loss: 2.07616489677019

Epoch: 6| Step: 1
Training loss: 2.206512928009033
Validation loss: 2.084694398346768

Epoch: 6| Step: 2
Training loss: 2.4341702461242676
Validation loss: 2.0816943696750108

Epoch: 6| Step: 3
Training loss: 2.4898858070373535
Validation loss: 2.075634152658524

Epoch: 6| Step: 4
Training loss: 2.6216776371002197
Validation loss: 2.0780641135349067

Epoch: 6| Step: 5
Training loss: 2.1510043144226074
Validation loss: 2.0841330764114216

Epoch: 6| Step: 6
Training loss: 2.3732128143310547
Validation loss: 2.0923604939573552

Epoch: 6| Step: 7
Training loss: 1.9860973358154297
Validation loss: 2.115455107022357

Epoch: 6| Step: 8
Training loss: 2.58231258392334
Validation loss: 2.145355801428518

Epoch: 6| Step: 9
Training loss: 2.515995979309082
Validation loss: 2.1481085054336058

Epoch: 6| Step: 10
Training loss: 2.9613966941833496
Validation loss: 2.147629569935542

Epoch: 6| Step: 11
Training loss: 2.1423964500427246
Validation loss: 2.1337791809471707

Epoch: 6| Step: 12
Training loss: 2.262275218963623
Validation loss: 2.113715748633108

Epoch: 6| Step: 13
Training loss: 2.454799175262451
Validation loss: 2.103973711690595

Epoch: 130| Step: 0
Training loss: 1.9559820890426636
Validation loss: 2.08789760322981

Epoch: 6| Step: 1
Training loss: 2.328185796737671
Validation loss: 2.084637604733949

Epoch: 6| Step: 2
Training loss: 2.4031074047088623
Validation loss: 2.0946428391241256

Epoch: 6| Step: 3
Training loss: 2.494872570037842
Validation loss: 2.112752168409286

Epoch: 6| Step: 4
Training loss: 2.1879634857177734
Validation loss: 2.112791899711855

Epoch: 6| Step: 5
Training loss: 2.0516157150268555
Validation loss: 2.1103116568698677

Epoch: 6| Step: 6
Training loss: 2.2068026065826416
Validation loss: 2.118092357471425

Epoch: 6| Step: 7
Training loss: 2.0175023078918457
Validation loss: 2.116977410931741

Epoch: 6| Step: 8
Training loss: 1.8137836456298828
Validation loss: 2.114909864241077

Epoch: 6| Step: 9
Training loss: 2.5762667655944824
Validation loss: 2.1121478490932013

Epoch: 6| Step: 10
Training loss: 2.458322763442993
Validation loss: 2.1210900122119534

Epoch: 6| Step: 11
Training loss: 2.62906551361084
Validation loss: 2.1214111825471282

Epoch: 6| Step: 12
Training loss: 2.6849098205566406
Validation loss: 2.1064753763137327

Epoch: 6| Step: 13
Training loss: 3.4108922481536865
Validation loss: 2.0904220765636814

Epoch: 131| Step: 0
Training loss: 1.8600411415100098
Validation loss: 2.0902544939389793

Epoch: 6| Step: 1
Training loss: 2.906571865081787
Validation loss: 2.0816189319856706

Epoch: 6| Step: 2
Training loss: 2.4171500205993652
Validation loss: 2.0809007742071666

Epoch: 6| Step: 3
Training loss: 2.3926186561584473
Validation loss: 2.085514630040815

Epoch: 6| Step: 4
Training loss: 1.9839661121368408
Validation loss: 2.081260901625438

Epoch: 6| Step: 5
Training loss: 2.3352012634277344
Validation loss: 2.1011498025668565

Epoch: 6| Step: 6
Training loss: 2.612673282623291
Validation loss: 2.099693739286033

Epoch: 6| Step: 7
Training loss: 2.552825927734375
Validation loss: 2.085033844876033

Epoch: 6| Step: 8
Training loss: 2.2900333404541016
Validation loss: 2.0943783252469954

Epoch: 6| Step: 9
Training loss: 1.3469610214233398
Validation loss: 2.098154139775102

Epoch: 6| Step: 10
Training loss: 2.3369498252868652
Validation loss: 2.087432530618483

Epoch: 6| Step: 11
Training loss: 2.0507218837738037
Validation loss: 2.088155277313725

Epoch: 6| Step: 12
Training loss: 3.2176754474639893
Validation loss: 2.0768667779942995

Epoch: 6| Step: 13
Training loss: 2.667433738708496
Validation loss: 2.0609394029904435

Epoch: 132| Step: 0
Training loss: 2.7443618774414062
Validation loss: 2.08524763712319

Epoch: 6| Step: 1
Training loss: 2.277411460876465
Validation loss: 2.085034780604865

Epoch: 6| Step: 2
Training loss: 2.0282788276672363
Validation loss: 2.091678543757367

Epoch: 6| Step: 3
Training loss: 2.38767671585083
Validation loss: 2.1050036991796186

Epoch: 6| Step: 4
Training loss: 2.371748208999634
Validation loss: 2.0994789626008723

Epoch: 6| Step: 5
Training loss: 2.8990259170532227
Validation loss: 2.0931656104262157

Epoch: 6| Step: 6
Training loss: 1.7411751747131348
Validation loss: 2.0850055602289017

Epoch: 6| Step: 7
Training loss: 2.1878297328948975
Validation loss: 2.0823769710397206

Epoch: 6| Step: 8
Training loss: 2.6039741039276123
Validation loss: 2.075352591852988

Epoch: 6| Step: 9
Training loss: 2.6209545135498047
Validation loss: 2.073736629178447

Epoch: 6| Step: 10
Training loss: 1.9132301807403564
Validation loss: 2.0816373901982463

Epoch: 6| Step: 11
Training loss: 2.694112539291382
Validation loss: 2.0943245349391812

Epoch: 6| Step: 12
Training loss: 1.8856353759765625
Validation loss: 2.111048133142533

Epoch: 6| Step: 13
Training loss: 2.2535955905914307
Validation loss: 2.1243627455926712

Epoch: 133| Step: 0
Training loss: 2.533261299133301
Validation loss: 2.134549375503294

Epoch: 6| Step: 1
Training loss: 2.205278158187866
Validation loss: 2.1397380905766643

Epoch: 6| Step: 2
Training loss: 2.140385150909424
Validation loss: 2.126245703748477

Epoch: 6| Step: 3
Training loss: 1.8640165328979492
Validation loss: 2.116251525058541

Epoch: 6| Step: 4
Training loss: 2.886486053466797
Validation loss: 2.1017301518429994

Epoch: 6| Step: 5
Training loss: 2.5302517414093018
Validation loss: 2.082902903197914

Epoch: 6| Step: 6
Training loss: 2.644822120666504
Validation loss: 2.0757506931981733

Epoch: 6| Step: 7
Training loss: 2.5023012161254883
Validation loss: 2.073305917042558

Epoch: 6| Step: 8
Training loss: 2.2770466804504395
Validation loss: 2.069946353153516

Epoch: 6| Step: 9
Training loss: 2.242753028869629
Validation loss: 2.080295939599314

Epoch: 6| Step: 10
Training loss: 2.246579647064209
Validation loss: 2.098043293081304

Epoch: 6| Step: 11
Training loss: 2.395406723022461
Validation loss: 2.1307403733653407

Epoch: 6| Step: 12
Training loss: 2.233612537384033
Validation loss: 2.1441708303266958

Epoch: 6| Step: 13
Training loss: 1.910351037979126
Validation loss: 2.1686051173876693

Epoch: 134| Step: 0
Training loss: 1.738175392150879
Validation loss: 2.1614546468180995

Epoch: 6| Step: 1
Training loss: 2.412334680557251
Validation loss: 2.1437049501685688

Epoch: 6| Step: 2
Training loss: 2.602576971054077
Validation loss: 2.1191725500168337

Epoch: 6| Step: 3
Training loss: 2.1656408309936523
Validation loss: 2.109269698460897

Epoch: 6| Step: 4
Training loss: 2.1384217739105225
Validation loss: 2.1112244526545205

Epoch: 6| Step: 5
Training loss: 1.8660874366760254
Validation loss: 2.110839546367686

Epoch: 6| Step: 6
Training loss: 2.744187831878662
Validation loss: 2.1111417547349007

Epoch: 6| Step: 7
Training loss: 2.6472949981689453
Validation loss: 2.106199270935469

Epoch: 6| Step: 8
Training loss: 2.889863967895508
Validation loss: 2.099335237215924

Epoch: 6| Step: 9
Training loss: 2.503387451171875
Validation loss: 2.089432054950345

Epoch: 6| Step: 10
Training loss: 2.438762664794922
Validation loss: 2.0951644130932388

Epoch: 6| Step: 11
Training loss: 2.8194234371185303
Validation loss: 2.0902692681999615

Epoch: 6| Step: 12
Training loss: 1.95371675491333
Validation loss: 2.087866860051309

Epoch: 6| Step: 13
Training loss: 1.7574763298034668
Validation loss: 2.07240201580909

Epoch: 135| Step: 0
Training loss: 1.9158477783203125
Validation loss: 2.075279256348969

Epoch: 6| Step: 1
Training loss: 2.4707255363464355
Validation loss: 2.0698230856208393

Epoch: 6| Step: 2
Training loss: 1.6152387857437134
Validation loss: 2.06837805368567

Epoch: 6| Step: 3
Training loss: 2.4508187770843506
Validation loss: 2.0702047450568086

Epoch: 6| Step: 4
Training loss: 2.7133331298828125
Validation loss: 2.0669307426739763

Epoch: 6| Step: 5
Training loss: 2.4881515502929688
Validation loss: 2.0635656925939743

Epoch: 6| Step: 6
Training loss: 3.003934621810913
Validation loss: 2.0694679624290875

Epoch: 6| Step: 7
Training loss: 2.308995008468628
Validation loss: 2.063838461393951

Epoch: 6| Step: 8
Training loss: 2.2391457557678223
Validation loss: 2.06990227648007

Epoch: 6| Step: 9
Training loss: 2.686929225921631
Validation loss: 2.0697045813324633

Epoch: 6| Step: 10
Training loss: 1.952481746673584
Validation loss: 2.081169113036125

Epoch: 6| Step: 11
Training loss: 1.9594910144805908
Validation loss: 2.0854869606674358

Epoch: 6| Step: 12
Training loss: 2.4519271850585938
Validation loss: 2.1097462074730986

Epoch: 6| Step: 13
Training loss: 2.6896047592163086
Validation loss: 2.119102229354202

Epoch: 136| Step: 0
Training loss: 1.90934157371521
Validation loss: 2.123960316822093

Epoch: 6| Step: 1
Training loss: 2.670708417892456
Validation loss: 2.115531066412567

Epoch: 6| Step: 2
Training loss: 2.575287342071533
Validation loss: 2.1071186283583283

Epoch: 6| Step: 3
Training loss: 2.431727886199951
Validation loss: 2.100208020979358

Epoch: 6| Step: 4
Training loss: 2.2408013343811035
Validation loss: 2.091703563608149

Epoch: 6| Step: 5
Training loss: 2.3079826831817627
Validation loss: 2.0876690700489986

Epoch: 6| Step: 6
Training loss: 2.0570921897888184
Validation loss: 2.0781648030845066

Epoch: 6| Step: 7
Training loss: 2.461031913757324
Validation loss: 2.0679638437045518

Epoch: 6| Step: 8
Training loss: 2.044510841369629
Validation loss: 2.0704988382195912

Epoch: 6| Step: 9
Training loss: 2.0353994369506836
Validation loss: 2.06941395677546

Epoch: 6| Step: 10
Training loss: 2.4069271087646484
Validation loss: 2.0714629798807125

Epoch: 6| Step: 11
Training loss: 2.3580515384674072
Validation loss: 2.0796229031778153

Epoch: 6| Step: 12
Training loss: 2.536442518234253
Validation loss: 2.0772771168780584

Epoch: 6| Step: 13
Training loss: 2.563931703567505
Validation loss: 2.081292954824304

Epoch: 137| Step: 0
Training loss: 2.360231637954712
Validation loss: 2.0848483936761015

Epoch: 6| Step: 1
Training loss: 2.685072422027588
Validation loss: 2.090686466104241

Epoch: 6| Step: 2
Training loss: 2.2731618881225586
Validation loss: 2.099668946317447

Epoch: 6| Step: 3
Training loss: 2.359988212585449
Validation loss: 2.0975369484193864

Epoch: 6| Step: 4
Training loss: 2.0036933422088623
Validation loss: 2.1006598190594743

Epoch: 6| Step: 5
Training loss: 2.581083059310913
Validation loss: 2.1065648319900676

Epoch: 6| Step: 6
Training loss: 2.619762659072876
Validation loss: 2.1162506700843893

Epoch: 6| Step: 7
Training loss: 2.576239585876465
Validation loss: 2.104624926403005

Epoch: 6| Step: 8
Training loss: 2.3218164443969727
Validation loss: 2.116263375487379

Epoch: 6| Step: 9
Training loss: 1.7396153211593628
Validation loss: 2.1016132805937078

Epoch: 6| Step: 10
Training loss: 1.6807665824890137
Validation loss: 2.094998880099225

Epoch: 6| Step: 11
Training loss: 2.9196650981903076
Validation loss: 2.0954902607907533

Epoch: 6| Step: 12
Training loss: 1.9571964740753174
Validation loss: 2.0837982059806905

Epoch: 6| Step: 13
Training loss: 2.258883476257324
Validation loss: 2.078014117415233

Epoch: 138| Step: 0
Training loss: 1.7532726526260376
Validation loss: 2.074874672838437

Epoch: 6| Step: 1
Training loss: 2.591157913208008
Validation loss: 2.08154587335484

Epoch: 6| Step: 2
Training loss: 2.014531135559082
Validation loss: 2.0898124940933718

Epoch: 6| Step: 3
Training loss: 1.9248220920562744
Validation loss: 2.107800378594347

Epoch: 6| Step: 4
Training loss: 2.1468987464904785
Validation loss: 2.1315172128779913

Epoch: 6| Step: 5
Training loss: 3.2190845012664795
Validation loss: 2.159367927940943

Epoch: 6| Step: 6
Training loss: 2.5184385776519775
Validation loss: 2.1533090119720786

Epoch: 6| Step: 7
Training loss: 2.8332533836364746
Validation loss: 2.140525176960935

Epoch: 6| Step: 8
Training loss: 2.8326468467712402
Validation loss: 2.1351388577492005

Epoch: 6| Step: 9
Training loss: 2.278899669647217
Validation loss: 2.1150123098845124

Epoch: 6| Step: 10
Training loss: 1.957570195198059
Validation loss: 2.1162937661652923

Epoch: 6| Step: 11
Training loss: 1.837900161743164
Validation loss: 2.1002682460251676

Epoch: 6| Step: 12
Training loss: 2.2391209602355957
Validation loss: 2.0834943940562587

Epoch: 6| Step: 13
Training loss: 2.2440919876098633
Validation loss: 2.076774048548873

Epoch: 139| Step: 0
Training loss: 2.5731921195983887
Validation loss: 2.0640077360214724

Epoch: 6| Step: 1
Training loss: 2.289923667907715
Validation loss: 2.0713800717425603

Epoch: 6| Step: 2
Training loss: 2.10176944732666
Validation loss: 2.059546296314527

Epoch: 6| Step: 3
Training loss: 2.4498350620269775
Validation loss: 2.0741051807198474

Epoch: 6| Step: 4
Training loss: 1.9880479574203491
Validation loss: 2.086763847258783

Epoch: 6| Step: 5
Training loss: 2.209179401397705
Validation loss: 2.1007726577020462

Epoch: 6| Step: 6
Training loss: 2.404508590698242
Validation loss: 2.102937508654851

Epoch: 6| Step: 7
Training loss: 1.9387794733047485
Validation loss: 2.0946234990191717

Epoch: 6| Step: 8
Training loss: 1.927128553390503
Validation loss: 2.0919149332149054

Epoch: 6| Step: 9
Training loss: 2.597262382507324
Validation loss: 2.0847719843669603

Epoch: 6| Step: 10
Training loss: 2.755878448486328
Validation loss: 2.071840299073086

Epoch: 6| Step: 11
Training loss: 1.8958189487457275
Validation loss: 2.0617578990997805

Epoch: 6| Step: 12
Training loss: 2.739739179611206
Validation loss: 2.0583170690844135

Epoch: 6| Step: 13
Training loss: 2.6184093952178955
Validation loss: 2.060518054551976

Epoch: 140| Step: 0
Training loss: 2.268676280975342
Validation loss: 2.055518324657153

Epoch: 6| Step: 1
Training loss: 2.092461585998535
Validation loss: 2.060579779327557

Epoch: 6| Step: 2
Training loss: 1.8858569860458374
Validation loss: 2.0698770771744432

Epoch: 6| Step: 3
Training loss: 3.0220320224761963
Validation loss: 2.06990026145853

Epoch: 6| Step: 4
Training loss: 2.519477367401123
Validation loss: 2.076381793586157

Epoch: 6| Step: 5
Training loss: 1.7917677164077759
Validation loss: 2.0746384025901876

Epoch: 6| Step: 6
Training loss: 2.403250217437744
Validation loss: 2.0782530628224856

Epoch: 6| Step: 7
Training loss: 1.9644412994384766
Validation loss: 2.0594451812005814

Epoch: 6| Step: 8
Training loss: 1.9547444581985474
Validation loss: 2.0573078432390766

Epoch: 6| Step: 9
Training loss: 2.933377265930176
Validation loss: 2.0588995487459245

Epoch: 6| Step: 10
Training loss: 1.9152418375015259
Validation loss: 2.056456040310603

Epoch: 6| Step: 11
Training loss: 2.2884631156921387
Validation loss: 2.0659871870471584

Epoch: 6| Step: 12
Training loss: 2.940427780151367
Validation loss: 2.073188307464764

Epoch: 6| Step: 13
Training loss: 2.327469825744629
Validation loss: 2.0551297639005925

Epoch: 141| Step: 0
Training loss: 2.2131686210632324
Validation loss: 2.0422042262169624

Epoch: 6| Step: 1
Training loss: 2.2426395416259766
Validation loss: 2.047131810137021

Epoch: 6| Step: 2
Training loss: 2.688462257385254
Validation loss: 2.048030775080445

Epoch: 6| Step: 3
Training loss: 2.470989942550659
Validation loss: 2.0559005596304454

Epoch: 6| Step: 4
Training loss: 2.2202906608581543
Validation loss: 2.054334544366406

Epoch: 6| Step: 5
Training loss: 2.60690975189209
Validation loss: 2.0776627191933255

Epoch: 6| Step: 6
Training loss: 2.116364002227783
Validation loss: 2.088211072388516

Epoch: 6| Step: 7
Training loss: 1.689279556274414
Validation loss: 2.094761524149167

Epoch: 6| Step: 8
Training loss: 2.760845184326172
Validation loss: 2.089942955201672

Epoch: 6| Step: 9
Training loss: 1.9296801090240479
Validation loss: 2.088358138197212

Epoch: 6| Step: 10
Training loss: 2.158127784729004
Validation loss: 2.0872424443562827

Epoch: 6| Step: 11
Training loss: 2.458822250366211
Validation loss: 2.0886615373755015

Epoch: 6| Step: 12
Training loss: 2.416567325592041
Validation loss: 2.077647502704333

Epoch: 6| Step: 13
Training loss: 2.3256735801696777
Validation loss: 2.075083604422949

Epoch: 142| Step: 0
Training loss: 2.274738073348999
Validation loss: 2.0690508452794885

Epoch: 6| Step: 1
Training loss: 2.3981339931488037
Validation loss: 2.06756914559231

Epoch: 6| Step: 2
Training loss: 2.652754068374634
Validation loss: 2.064861334780211

Epoch: 6| Step: 3
Training loss: 1.8818902969360352
Validation loss: 2.070036179275923

Epoch: 6| Step: 4
Training loss: 2.4116015434265137
Validation loss: 2.0822807255611626

Epoch: 6| Step: 5
Training loss: 2.0657029151916504
Validation loss: 2.0851070227161532

Epoch: 6| Step: 6
Training loss: 2.019838333129883
Validation loss: 2.0829923229832805

Epoch: 6| Step: 7
Training loss: 3.10998272895813
Validation loss: 2.087857110525972

Epoch: 6| Step: 8
Training loss: 2.5225181579589844
Validation loss: 2.094785116052115

Epoch: 6| Step: 9
Training loss: 1.8901551961898804
Validation loss: 2.0857216004402406

Epoch: 6| Step: 10
Training loss: 2.627262592315674
Validation loss: 2.0826473389902422

Epoch: 6| Step: 11
Training loss: 2.435063123703003
Validation loss: 2.08430742192012

Epoch: 6| Step: 12
Training loss: 1.6625819206237793
Validation loss: 2.069671291176991

Epoch: 6| Step: 13
Training loss: 1.974039912223816
Validation loss: 2.0631505494476645

Epoch: 143| Step: 0
Training loss: 2.0172197818756104
Validation loss: 2.0581300989274056

Epoch: 6| Step: 1
Training loss: 2.921783924102783
Validation loss: 2.048792762141074

Epoch: 6| Step: 2
Training loss: 2.4764504432678223
Validation loss: 2.0529437116397324

Epoch: 6| Step: 3
Training loss: 2.9866957664489746
Validation loss: 2.0529704683570453

Epoch: 6| Step: 4
Training loss: 1.7587106227874756
Validation loss: 2.0461035979691373

Epoch: 6| Step: 5
Training loss: 2.40173077583313
Validation loss: 2.048159130157963

Epoch: 6| Step: 6
Training loss: 2.424858331680298
Validation loss: 2.04351229821482

Epoch: 6| Step: 7
Training loss: 1.3962458372116089
Validation loss: 2.0526291247337096

Epoch: 6| Step: 8
Training loss: 1.9001156091690063
Validation loss: 2.0553062000582294

Epoch: 6| Step: 9
Training loss: 1.8845181465148926
Validation loss: 2.0513481773355955

Epoch: 6| Step: 10
Training loss: 2.4171247482299805
Validation loss: 2.0597728119101575

Epoch: 6| Step: 11
Training loss: 2.1683950424194336
Validation loss: 2.064172157677271

Epoch: 6| Step: 12
Training loss: 2.730185031890869
Validation loss: 2.0640443858279975

Epoch: 6| Step: 13
Training loss: 2.551557779312134
Validation loss: 2.063371807016352

Epoch: 144| Step: 0
Training loss: 2.693629741668701
Validation loss: 2.062707580545897

Epoch: 6| Step: 1
Training loss: 1.9004510641098022
Validation loss: 2.0662620990507063

Epoch: 6| Step: 2
Training loss: 2.2613093852996826
Validation loss: 2.0684785868531916

Epoch: 6| Step: 3
Training loss: 2.4497315883636475
Validation loss: 2.0548700799224195

Epoch: 6| Step: 4
Training loss: 2.047177314758301
Validation loss: 2.0470911097782913

Epoch: 6| Step: 5
Training loss: 1.9336822032928467
Validation loss: 2.047108962971677

Epoch: 6| Step: 6
Training loss: 2.0293526649475098
Validation loss: 2.0542227298982683

Epoch: 6| Step: 7
Training loss: 1.9007278680801392
Validation loss: 2.072758350321042

Epoch: 6| Step: 8
Training loss: 2.201700210571289
Validation loss: 2.0886143663878083

Epoch: 6| Step: 9
Training loss: 3.377694606781006
Validation loss: 2.105718622925461

Epoch: 6| Step: 10
Training loss: 2.5093131065368652
Validation loss: 2.0942650866764847

Epoch: 6| Step: 11
Training loss: 2.202462673187256
Validation loss: 2.084440036486554

Epoch: 6| Step: 12
Training loss: 2.207606554031372
Validation loss: 2.0631101400621477

Epoch: 6| Step: 13
Training loss: 2.7310776710510254
Validation loss: 2.0384659664605254

Epoch: 145| Step: 0
Training loss: 1.682664394378662
Validation loss: 2.043502971690188

Epoch: 6| Step: 1
Training loss: 1.6073453426361084
Validation loss: 2.0346274234915294

Epoch: 6| Step: 2
Training loss: 2.028428077697754
Validation loss: 2.0437395252207273

Epoch: 6| Step: 3
Training loss: 1.8566111326217651
Validation loss: 2.0436461817833687

Epoch: 6| Step: 4
Training loss: 2.3014473915100098
Validation loss: 2.0320368723202775

Epoch: 6| Step: 5
Training loss: 3.175044059753418
Validation loss: 2.030554068985806

Epoch: 6| Step: 6
Training loss: 2.9563307762145996
Validation loss: 2.0417901200632893

Epoch: 6| Step: 7
Training loss: 1.838797926902771
Validation loss: 2.0454398457722

Epoch: 6| Step: 8
Training loss: 2.6181998252868652
Validation loss: 2.034343522082093

Epoch: 6| Step: 9
Training loss: 2.667095184326172
Validation loss: 2.0473181227202057

Epoch: 6| Step: 10
Training loss: 2.568408489227295
Validation loss: 2.0583930041200373

Epoch: 6| Step: 11
Training loss: 1.9404933452606201
Validation loss: 2.074040315484488

Epoch: 6| Step: 12
Training loss: 2.559326171875
Validation loss: 2.087395916702927

Epoch: 6| Step: 13
Training loss: 1.7585893869400024
Validation loss: 2.096674128245282

Epoch: 146| Step: 0
Training loss: 2.5168747901916504
Validation loss: 2.0897952510464575

Epoch: 6| Step: 1
Training loss: 2.6553311347961426
Validation loss: 2.100162452267062

Epoch: 6| Step: 2
Training loss: 2.866809368133545
Validation loss: 2.0860588217294342

Epoch: 6| Step: 3
Training loss: 1.2586276531219482
Validation loss: 2.0719734238040064

Epoch: 6| Step: 4
Training loss: 2.3344550132751465
Validation loss: 2.0772704180850776

Epoch: 6| Step: 5
Training loss: 1.979495882987976
Validation loss: 2.0781286852334135

Epoch: 6| Step: 6
Training loss: 1.763691782951355
Validation loss: 2.070419757596908

Epoch: 6| Step: 7
Training loss: 2.0010628700256348
Validation loss: 2.0536084816020024

Epoch: 6| Step: 8
Training loss: 2.6661434173583984
Validation loss: 2.0573242684846282

Epoch: 6| Step: 9
Training loss: 2.461357355117798
Validation loss: 2.0480138101885395

Epoch: 6| Step: 10
Training loss: 3.2495620250701904
Validation loss: 2.0512960418578117

Epoch: 6| Step: 11
Training loss: 2.9536399841308594
Validation loss: 2.0605468942273046

Epoch: 6| Step: 12
Training loss: 1.4275028705596924
Validation loss: 2.0566038880296933

Epoch: 6| Step: 13
Training loss: 1.2284992933273315
Validation loss: 2.070348738342203

Epoch: 147| Step: 0
Training loss: 2.1478219032287598
Validation loss: 2.0611676080252535

Epoch: 6| Step: 1
Training loss: 1.8346093893051147
Validation loss: 2.0942994804792505

Epoch: 6| Step: 2
Training loss: 2.8657872676849365
Validation loss: 2.1346750169671993

Epoch: 6| Step: 3
Training loss: 2.927598714828491
Validation loss: 2.1524359756900417

Epoch: 6| Step: 4
Training loss: 2.3699469566345215
Validation loss: 2.131503107727215

Epoch: 6| Step: 5
Training loss: 2.2083892822265625
Validation loss: 2.1072425637193906

Epoch: 6| Step: 6
Training loss: 1.8931357860565186
Validation loss: 2.0888234928090084

Epoch: 6| Step: 7
Training loss: 1.810204267501831
Validation loss: 2.086715007341036

Epoch: 6| Step: 8
Training loss: 2.876361608505249
Validation loss: 2.0634463243587042

Epoch: 6| Step: 9
Training loss: 2.0817999839782715
Validation loss: 2.0635899728344334

Epoch: 6| Step: 10
Training loss: 2.378551483154297
Validation loss: 2.0495714884932323

Epoch: 6| Step: 11
Training loss: 2.1884713172912598
Validation loss: 2.0495227741938766

Epoch: 6| Step: 12
Training loss: 1.9589316844940186
Validation loss: 2.053034661918558

Epoch: 6| Step: 13
Training loss: 2.2585041522979736
Validation loss: 2.0365168638126825

Epoch: 148| Step: 0
Training loss: 2.0926425457000732
Validation loss: 2.041653589535785

Epoch: 6| Step: 1
Training loss: 2.2622663974761963
Validation loss: 2.044490880863641

Epoch: 6| Step: 2
Training loss: 2.301779270172119
Validation loss: 2.0547310280543503

Epoch: 6| Step: 3
Training loss: 2.3884711265563965
Validation loss: 2.0522049498814408

Epoch: 6| Step: 4
Training loss: 3.069736957550049
Validation loss: 2.055215222861177

Epoch: 6| Step: 5
Training loss: 2.1391186714172363
Validation loss: 2.0574820298020557

Epoch: 6| Step: 6
Training loss: 2.3960750102996826
Validation loss: 2.052945254951395

Epoch: 6| Step: 7
Training loss: 2.483102321624756
Validation loss: 2.036031578176765

Epoch: 6| Step: 8
Training loss: 2.110417366027832
Validation loss: 2.0304484213552167

Epoch: 6| Step: 9
Training loss: 1.9900758266448975
Validation loss: 2.018214579551451

Epoch: 6| Step: 10
Training loss: 1.399132251739502
Validation loss: 2.0209844932761243

Epoch: 6| Step: 11
Training loss: 2.7301180362701416
Validation loss: 2.0264189986772436

Epoch: 6| Step: 12
Training loss: 2.249880790710449
Validation loss: 2.010845189453453

Epoch: 6| Step: 13
Training loss: 1.8962925672531128
Validation loss: 2.0208601144052323

Epoch: 149| Step: 0
Training loss: 2.381911277770996
Validation loss: 2.026845621806319

Epoch: 6| Step: 1
Training loss: 2.316235065460205
Validation loss: 2.0360366259851763

Epoch: 6| Step: 2
Training loss: 1.6759531497955322
Validation loss: 2.03160947368991

Epoch: 6| Step: 3
Training loss: 2.7627272605895996
Validation loss: 2.0425284883027435

Epoch: 6| Step: 4
Training loss: 2.0155181884765625
Validation loss: 2.048409499147887

Epoch: 6| Step: 5
Training loss: 2.9285802841186523
Validation loss: 2.066319420773496

Epoch: 6| Step: 6
Training loss: 1.4225503206253052
Validation loss: 2.0715897262737317

Epoch: 6| Step: 7
Training loss: 2.771226406097412
Validation loss: 2.0613543641182686

Epoch: 6| Step: 8
Training loss: 2.9368605613708496
Validation loss: 2.071263115893128

Epoch: 6| Step: 9
Training loss: 2.019015312194824
Validation loss: 2.0455665178196405

Epoch: 6| Step: 10
Training loss: 2.381474256515503
Validation loss: 2.043006370144506

Epoch: 6| Step: 11
Training loss: 2.187854051589966
Validation loss: 2.048608861943727

Epoch: 6| Step: 12
Training loss: 1.7572331428527832
Validation loss: 2.045339890705642

Epoch: 6| Step: 13
Training loss: 1.77896249294281
Validation loss: 2.060051502720002

Epoch: 150| Step: 0
Training loss: 2.4171972274780273
Validation loss: 2.068023440658405

Epoch: 6| Step: 1
Training loss: 2.433234214782715
Validation loss: 2.064406858977451

Epoch: 6| Step: 2
Training loss: 1.807468056678772
Validation loss: 2.0576487997526764

Epoch: 6| Step: 3
Training loss: 2.9194955825805664
Validation loss: 2.058284760803305

Epoch: 6| Step: 4
Training loss: 1.8463022708892822
Validation loss: 2.0431577928604616

Epoch: 6| Step: 5
Training loss: 1.9876413345336914
Validation loss: 2.0454629672470914

Epoch: 6| Step: 6
Training loss: 2.467332124710083
Validation loss: 2.0451915366675264

Epoch: 6| Step: 7
Training loss: 1.904228925704956
Validation loss: 2.0412106821613927

Epoch: 6| Step: 8
Training loss: 2.3985495567321777
Validation loss: 2.0462097275641655

Epoch: 6| Step: 9
Training loss: 1.9765750169754028
Validation loss: 2.040392303979525

Epoch: 6| Step: 10
Training loss: 2.2181553840637207
Validation loss: 2.0367925346538587

Epoch: 6| Step: 11
Training loss: 2.4835500717163086
Validation loss: 2.033315816233235

Epoch: 6| Step: 12
Training loss: 2.3926408290863037
Validation loss: 2.040405665674517

Epoch: 6| Step: 13
Training loss: 2.3326573371887207
Validation loss: 2.0459016138507473

Testing loss: 2.2624763170878093

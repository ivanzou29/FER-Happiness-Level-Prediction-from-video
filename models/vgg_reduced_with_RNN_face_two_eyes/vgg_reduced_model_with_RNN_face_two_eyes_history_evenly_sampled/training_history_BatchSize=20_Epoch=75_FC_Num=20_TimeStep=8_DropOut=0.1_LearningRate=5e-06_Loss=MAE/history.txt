Epoch: 1| Step: 0
Training loss: 4.52930212020874
Validation loss: 5.162451451824557

Epoch: 5| Step: 1
Training loss: 4.700374603271484
Validation loss: 5.157337198975266

Epoch: 5| Step: 2
Training loss: 4.510417461395264
Validation loss: 5.152672393347627

Epoch: 5| Step: 3
Training loss: 5.114656925201416
Validation loss: 5.147781654070783

Epoch: 5| Step: 4
Training loss: 5.323835372924805
Validation loss: 5.143097559611003

Epoch: 5| Step: 5
Training loss: 4.885340690612793
Validation loss: 5.138590863955918

Epoch: 5| Step: 6
Training loss: 5.215152263641357
Validation loss: 5.133855394137803

Epoch: 5| Step: 7
Training loss: 4.266107082366943
Validation loss: 5.128822429205782

Epoch: 5| Step: 8
Training loss: 5.163491725921631
Validation loss: 5.123954321748467

Epoch: 5| Step: 9
Training loss: 5.184196472167969
Validation loss: 5.118511266605829

Epoch: 5| Step: 10
Training loss: 5.468455791473389
Validation loss: 5.113266093756563

Epoch: 2| Step: 0
Training loss: 5.092706203460693
Validation loss: 5.108081417699014

Epoch: 5| Step: 1
Training loss: 5.254273891448975
Validation loss: 5.10214941475981

Epoch: 5| Step: 2
Training loss: 4.967318534851074
Validation loss: 5.096731093622023

Epoch: 5| Step: 3
Training loss: 4.270455837249756
Validation loss: 5.090339717044626

Epoch: 5| Step: 4
Training loss: 4.835751533508301
Validation loss: 5.084329169283631

Epoch: 5| Step: 5
Training loss: 5.457732200622559
Validation loss: 5.0774656572649555

Epoch: 5| Step: 6
Training loss: 4.453790187835693
Validation loss: 5.070807851770873

Epoch: 5| Step: 7
Training loss: 4.267531871795654
Validation loss: 5.063463236695977

Epoch: 5| Step: 8
Training loss: 4.394833087921143
Validation loss: 5.05610849524057

Epoch: 5| Step: 9
Training loss: 5.514113426208496
Validation loss: 5.048423736326156

Epoch: 5| Step: 10
Training loss: 5.120862007141113
Validation loss: 5.040327379780431

Epoch: 3| Step: 0
Training loss: 3.818974018096924
Validation loss: 5.032449619744414

Epoch: 5| Step: 1
Training loss: 4.693573474884033
Validation loss: 5.023122577257054

Epoch: 5| Step: 2
Training loss: 4.600323677062988
Validation loss: 5.014335406723843

Epoch: 5| Step: 3
Training loss: 4.5917768478393555
Validation loss: 5.003834662898894

Epoch: 5| Step: 4
Training loss: 5.425471782684326
Validation loss: 4.993269125620524

Epoch: 5| Step: 5
Training loss: 5.757974147796631
Validation loss: 4.983314365468999

Epoch: 5| Step: 6
Training loss: 5.773373126983643
Validation loss: 4.97091289745864

Epoch: 5| Step: 7
Training loss: 5.095900535583496
Validation loss: 4.959627884690479

Epoch: 5| Step: 8
Training loss: 4.587110996246338
Validation loss: 4.947425565411968

Epoch: 5| Step: 9
Training loss: 4.678675174713135
Validation loss: 4.933801589473601

Epoch: 5| Step: 10
Training loss: 3.2719430923461914
Validation loss: 4.920617852159726

Epoch: 4| Step: 0
Training loss: 3.3288512229919434
Validation loss: 4.90511893200618

Epoch: 5| Step: 1
Training loss: 4.088700294494629
Validation loss: 4.89039227783039

Epoch: 5| Step: 2
Training loss: 4.409189224243164
Validation loss: 4.874184023949407

Epoch: 5| Step: 3
Training loss: 5.64499044418335
Validation loss: 4.8570991664804435

Epoch: 5| Step: 4
Training loss: 4.827689170837402
Validation loss: 4.839331032127462

Epoch: 5| Step: 5
Training loss: 5.4700703620910645
Validation loss: 4.821389141903128

Epoch: 5| Step: 6
Training loss: 4.901123523712158
Validation loss: 4.801604758026779

Epoch: 5| Step: 7
Training loss: 4.915258884429932
Validation loss: 4.781691843463529

Epoch: 5| Step: 8
Training loss: 4.1400275230407715
Validation loss: 4.759686121376612

Epoch: 5| Step: 9
Training loss: 4.346340179443359
Validation loss: 4.738115238887008

Epoch: 5| Step: 10
Training loss: 4.589298248291016
Validation loss: 4.717697553737189

Epoch: 5| Step: 0
Training loss: 4.2300825119018555
Validation loss: 4.693302995415144

Epoch: 5| Step: 1
Training loss: 3.915407657623291
Validation loss: 4.670681891902801

Epoch: 5| Step: 2
Training loss: 5.405940055847168
Validation loss: 4.644800411757602

Epoch: 5| Step: 3
Training loss: 4.245602607727051
Validation loss: 4.620106266390893

Epoch: 5| Step: 4
Training loss: 4.230496406555176
Validation loss: 4.592050229349444

Epoch: 5| Step: 5
Training loss: 4.268096923828125
Validation loss: 4.565566960201468

Epoch: 5| Step: 6
Training loss: 3.6490912437438965
Validation loss: 4.5389737800885275

Epoch: 5| Step: 7
Training loss: 5.101742267608643
Validation loss: 4.51034189039661

Epoch: 5| Step: 8
Training loss: 4.7666730880737305
Validation loss: 4.481019066226098

Epoch: 5| Step: 9
Training loss: 4.2102508544921875
Validation loss: 4.452478434449883

Epoch: 5| Step: 10
Training loss: 3.6643190383911133
Validation loss: 4.422169582818144

Epoch: 6| Step: 0
Training loss: 4.34222936630249
Validation loss: 4.3919202896856495

Epoch: 5| Step: 1
Training loss: 3.6918857097625732
Validation loss: 4.361317096217986

Epoch: 5| Step: 2
Training loss: 4.683791160583496
Validation loss: 4.329773303001158

Epoch: 5| Step: 3
Training loss: 4.821503639221191
Validation loss: 4.300621355733564

Epoch: 5| Step: 4
Training loss: 4.105578899383545
Validation loss: 4.272269074634839

Epoch: 5| Step: 5
Training loss: 2.5652713775634766
Validation loss: 4.24012586121918

Epoch: 5| Step: 6
Training loss: 5.080562114715576
Validation loss: 4.211250561539845

Epoch: 5| Step: 7
Training loss: 4.76076602935791
Validation loss: 4.180150032043457

Epoch: 5| Step: 8
Training loss: 3.3593947887420654
Validation loss: 4.152048121216477

Epoch: 5| Step: 9
Training loss: 3.783761501312256
Validation loss: 4.124606978508734

Epoch: 5| Step: 10
Training loss: 3.235685110092163
Validation loss: 4.096739143453618

Epoch: 7| Step: 0
Training loss: 3.8131134510040283
Validation loss: 4.06940572492538

Epoch: 5| Step: 1
Training loss: 3.6728568077087402
Validation loss: 4.044534585809195

Epoch: 5| Step: 2
Training loss: 4.488943576812744
Validation loss: 4.0189490420843965

Epoch: 5| Step: 3
Training loss: 4.065356731414795
Validation loss: 3.9946300983428955

Epoch: 5| Step: 4
Training loss: 4.583510398864746
Validation loss: 3.969482050147108

Epoch: 5| Step: 5
Training loss: 4.197491645812988
Validation loss: 3.9429781872739076

Epoch: 5| Step: 6
Training loss: 2.685694456100464
Validation loss: 3.91996431350708

Epoch: 5| Step: 7
Training loss: 3.3082804679870605
Validation loss: 3.8979216673040904

Epoch: 5| Step: 8
Training loss: 3.145162343978882
Validation loss: 3.8774226942370014

Epoch: 5| Step: 9
Training loss: 4.346346855163574
Validation loss: 3.8545084948180826

Epoch: 5| Step: 10
Training loss: 3.490938425064087
Validation loss: 3.833800026165542

Epoch: 8| Step: 0
Training loss: 4.345297336578369
Validation loss: 3.8150441005665767

Epoch: 5| Step: 1
Training loss: 3.6519737243652344
Validation loss: 3.7934822831102597

Epoch: 5| Step: 2
Training loss: 2.644392251968384
Validation loss: 3.774273790338988

Epoch: 5| Step: 3
Training loss: 3.503762722015381
Validation loss: 3.7537916039907806

Epoch: 5| Step: 4
Training loss: 3.700753688812256
Validation loss: 3.736873442126859

Epoch: 5| Step: 5
Training loss: 3.1668708324432373
Validation loss: 3.7176410510975826

Epoch: 5| Step: 6
Training loss: 4.366215705871582
Validation loss: 3.695058666249757

Epoch: 5| Step: 7
Training loss: 4.317536354064941
Validation loss: 3.682004256915021

Epoch: 5| Step: 8
Training loss: 3.6959457397460938
Validation loss: 3.6643958783918813

Epoch: 5| Step: 9
Training loss: 2.696286678314209
Validation loss: 3.6488484797939176

Epoch: 5| Step: 10
Training loss: 3.643716812133789
Validation loss: 3.6286031687131493

Epoch: 9| Step: 0
Training loss: 3.277210235595703
Validation loss: 3.6154313625827914

Epoch: 5| Step: 1
Training loss: 3.6971137523651123
Validation loss: 3.5978803019369803

Epoch: 5| Step: 2
Training loss: 3.5485687255859375
Validation loss: 3.5856314218172463

Epoch: 5| Step: 3
Training loss: 3.243361234664917
Validation loss: 3.573668925992904

Epoch: 5| Step: 4
Training loss: 3.7618699073791504
Validation loss: 3.55802599845394

Epoch: 5| Step: 5
Training loss: 3.4605369567871094
Validation loss: 3.541561116454422

Epoch: 5| Step: 6
Training loss: 3.2411856651306152
Validation loss: 3.527866745507845

Epoch: 5| Step: 7
Training loss: 3.278453826904297
Validation loss: 3.5138100680484565

Epoch: 5| Step: 8
Training loss: 3.7400054931640625
Validation loss: 3.4975166218255156

Epoch: 5| Step: 9
Training loss: 3.148402690887451
Validation loss: 3.485897192391016

Epoch: 5| Step: 10
Training loss: 3.686677932739258
Validation loss: 3.4714886706362487

Epoch: 10| Step: 0
Training loss: 2.946843385696411
Validation loss: 3.460205001215781

Epoch: 5| Step: 1
Training loss: 3.2673251628875732
Validation loss: 3.445498756183091

Epoch: 5| Step: 2
Training loss: 3.7651877403259277
Validation loss: 3.4325721981704875

Epoch: 5| Step: 3
Training loss: 2.6070640087127686
Validation loss: 3.419469154009255

Epoch: 5| Step: 4
Training loss: 2.6090335845947266
Validation loss: 3.4092674024643435

Epoch: 5| Step: 5
Training loss: 4.35911750793457
Validation loss: 3.3963733924332487

Epoch: 5| Step: 6
Training loss: 4.269602298736572
Validation loss: 3.3851596078565045

Epoch: 5| Step: 7
Training loss: 3.588937282562256
Validation loss: 3.374035117446735

Epoch: 5| Step: 8
Training loss: 3.069380283355713
Validation loss: 3.3642240288437053

Epoch: 5| Step: 9
Training loss: 3.6963858604431152
Validation loss: 3.353532073318317

Epoch: 5| Step: 10
Training loss: 2.581761598587036
Validation loss: 3.3455795600850093

Epoch: 11| Step: 0
Training loss: 3.396427631378174
Validation loss: 3.3376243063198623

Epoch: 5| Step: 1
Training loss: 2.8824708461761475
Validation loss: 3.3285482339961554

Epoch: 5| Step: 2
Training loss: 3.1915030479431152
Validation loss: 3.319610400866437

Epoch: 5| Step: 3
Training loss: 2.9986064434051514
Validation loss: 3.3124368036946943

Epoch: 5| Step: 4
Training loss: 3.7376315593719482
Validation loss: 3.308497964694936

Epoch: 5| Step: 5
Training loss: 2.3800854682922363
Validation loss: 3.3004524323248092

Epoch: 5| Step: 6
Training loss: 2.941230535507202
Validation loss: 3.2882587063697075

Epoch: 5| Step: 7
Training loss: 3.1167924404144287
Validation loss: 3.285121351160029

Epoch: 5| Step: 8
Training loss: 4.177035808563232
Validation loss: 3.2780074586150465

Epoch: 5| Step: 9
Training loss: 4.446258544921875
Validation loss: 3.2718446818731164

Epoch: 5| Step: 10
Training loss: 2.5828776359558105
Validation loss: 3.266242842520437

Epoch: 12| Step: 0
Training loss: 3.789421558380127
Validation loss: 3.262210835692703

Epoch: 5| Step: 1
Training loss: 2.983532428741455
Validation loss: 3.255512673367736

Epoch: 5| Step: 2
Training loss: 2.065957546234131
Validation loss: 3.252620461166546

Epoch: 5| Step: 3
Training loss: 3.0214877128601074
Validation loss: 3.246626313014697

Epoch: 5| Step: 4
Training loss: 4.420668125152588
Validation loss: 3.2383863951570246

Epoch: 5| Step: 5
Training loss: 2.1100752353668213
Validation loss: 3.2345167385634555

Epoch: 5| Step: 6
Training loss: 4.141592025756836
Validation loss: 3.229292667040261

Epoch: 5| Step: 7
Training loss: 2.890850782394409
Validation loss: 3.2233698188617663

Epoch: 5| Step: 8
Training loss: 3.530663251876831
Validation loss: 3.2169609198006253

Epoch: 5| Step: 9
Training loss: 3.0769670009613037
Validation loss: 3.214744214088686

Epoch: 5| Step: 10
Training loss: 3.412179946899414
Validation loss: 3.211138076679681

Epoch: 13| Step: 0
Training loss: 2.9496963024139404
Validation loss: 3.2032247563844085

Epoch: 5| Step: 1
Training loss: 2.486992359161377
Validation loss: 3.199607992684969

Epoch: 5| Step: 2
Training loss: 3.1031582355499268
Validation loss: 3.1960134865135275

Epoch: 5| Step: 3
Training loss: 3.788944959640503
Validation loss: 3.1901495482331965

Epoch: 5| Step: 4
Training loss: 2.8593335151672363
Validation loss: 3.186838529443228

Epoch: 5| Step: 5
Training loss: 3.368539333343506
Validation loss: 3.1811809744886173

Epoch: 5| Step: 6
Training loss: 3.269627094268799
Validation loss: 3.179113203479398

Epoch: 5| Step: 7
Training loss: 3.8227686882019043
Validation loss: 3.175346084820327

Epoch: 5| Step: 8
Training loss: 3.763179063796997
Validation loss: 3.1682288800516436

Epoch: 5| Step: 9
Training loss: 3.278820037841797
Validation loss: 3.164341644574237

Epoch: 5| Step: 10
Training loss: 2.179165840148926
Validation loss: 3.164179427649385

Epoch: 14| Step: 0
Training loss: 3.1826038360595703
Validation loss: 3.1612557980322067

Epoch: 5| Step: 1
Training loss: 3.7727551460266113
Validation loss: 3.1567081738543767

Epoch: 5| Step: 2
Training loss: 2.6480939388275146
Validation loss: 3.1525109506422475

Epoch: 5| Step: 3
Training loss: 3.652886152267456
Validation loss: 3.1484258713260775

Epoch: 5| Step: 4
Training loss: 2.886479616165161
Validation loss: 3.147064939621956

Epoch: 5| Step: 5
Training loss: 3.3967432975769043
Validation loss: 3.1434665546622327

Epoch: 5| Step: 6
Training loss: 2.4598007202148438
Validation loss: 3.138659518252137

Epoch: 5| Step: 7
Training loss: 3.0119471549987793
Validation loss: 3.1358386367879887

Epoch: 5| Step: 8
Training loss: 3.3241562843322754
Validation loss: 3.1333735809531262

Epoch: 5| Step: 9
Training loss: 3.255507707595825
Validation loss: 3.130113929830572

Epoch: 5| Step: 10
Training loss: 3.120879888534546
Validation loss: 3.1258273919423423

Epoch: 15| Step: 0
Training loss: 2.687678813934326
Validation loss: 3.1222685767758276

Epoch: 5| Step: 1
Training loss: 2.5459096431732178
Validation loss: 3.1196051848832

Epoch: 5| Step: 2
Training loss: 3.7343909740448
Validation loss: 3.118318816666962

Epoch: 5| Step: 3
Training loss: 3.930112361907959
Validation loss: 3.1108266922735397

Epoch: 5| Step: 4
Training loss: 2.1343562602996826
Validation loss: 3.111756673423193

Epoch: 5| Step: 5
Training loss: 2.851715564727783
Validation loss: 3.112549712581019

Epoch: 5| Step: 6
Training loss: 3.746239423751831
Validation loss: 3.1127295545352403

Epoch: 5| Step: 7
Training loss: 2.8707756996154785
Validation loss: 3.1104191862126833

Epoch: 5| Step: 8
Training loss: 3.222670078277588
Validation loss: 3.1016712624539613

Epoch: 5| Step: 9
Training loss: 3.3943257331848145
Validation loss: 3.092544932519236

Epoch: 5| Step: 10
Training loss: 3.3739209175109863
Validation loss: 3.0908326154114096

Epoch: 16| Step: 0
Training loss: 2.8003804683685303
Validation loss: 3.090823640105545

Epoch: 5| Step: 1
Training loss: 2.4425032138824463
Validation loss: 3.0863231330789547

Epoch: 5| Step: 2
Training loss: 3.557152509689331
Validation loss: 3.0834440338996147

Epoch: 5| Step: 3
Training loss: 3.642916202545166
Validation loss: 3.083218297650737

Epoch: 5| Step: 4
Training loss: 3.1989197731018066
Validation loss: 3.078467510079825

Epoch: 5| Step: 5
Training loss: 3.375232219696045
Validation loss: 3.0746294990662606

Epoch: 5| Step: 6
Training loss: 2.916839361190796
Validation loss: 3.0757021801446074

Epoch: 5| Step: 7
Training loss: 2.8928415775299072
Validation loss: 3.0712807870680288

Epoch: 5| Step: 8
Training loss: 4.310492992401123
Validation loss: 3.069367854825912

Epoch: 5| Step: 9
Training loss: 2.467525005340576
Validation loss: 3.063342473840201

Epoch: 5| Step: 10
Training loss: 2.5434937477111816
Validation loss: 3.062658030499694

Epoch: 17| Step: 0
Training loss: 2.888144016265869
Validation loss: 3.0603737318387596

Epoch: 5| Step: 1
Training loss: 2.674193859100342
Validation loss: 3.0577640969266175

Epoch: 5| Step: 2
Training loss: 3.1339690685272217
Validation loss: 3.052955832532657

Epoch: 5| Step: 3
Training loss: 3.808767318725586
Validation loss: 3.0476448715374036

Epoch: 5| Step: 4
Training loss: 2.7347171306610107
Validation loss: 3.0430767049071608

Epoch: 5| Step: 5
Training loss: 3.2393393516540527
Validation loss: 3.0383766005116124

Epoch: 5| Step: 6
Training loss: 3.0828120708465576
Validation loss: 3.0368850718262377

Epoch: 5| Step: 7
Training loss: 3.746471881866455
Validation loss: 3.0299789649184032

Epoch: 5| Step: 8
Training loss: 2.522157907485962
Validation loss: 3.0280217227115425

Epoch: 5| Step: 9
Training loss: 4.176292419433594
Validation loss: 3.0213180280500844

Epoch: 5| Step: 10
Training loss: 1.7370214462280273
Validation loss: 3.0141666884063394

Epoch: 18| Step: 0
Training loss: 3.2319252490997314
Validation loss: 3.0094542631538967

Epoch: 5| Step: 1
Training loss: 3.3144383430480957
Validation loss: 3.004118811699652

Epoch: 5| Step: 2
Training loss: 3.284785032272339
Validation loss: 3.004644173447804

Epoch: 5| Step: 3
Training loss: 3.255986452102661
Validation loss: 2.998093035913283

Epoch: 5| Step: 4
Training loss: 2.9987616539001465
Validation loss: 2.995110514343426

Epoch: 5| Step: 5
Training loss: 2.5512404441833496
Validation loss: 3.001032616502495

Epoch: 5| Step: 6
Training loss: 2.917546033859253
Validation loss: 3.009148402880597

Epoch: 5| Step: 7
Training loss: 2.804443359375
Validation loss: 3.0025428777099936

Epoch: 5| Step: 8
Training loss: 2.9659295082092285
Validation loss: 2.9883625584263958

Epoch: 5| Step: 9
Training loss: 3.3671581745147705
Validation loss: 2.98492169380188

Epoch: 5| Step: 10
Training loss: 2.908785820007324
Validation loss: 2.9779891916500625

Epoch: 19| Step: 0
Training loss: 3.3633339405059814
Validation loss: 2.9801320593844176

Epoch: 5| Step: 1
Training loss: 2.467033863067627
Validation loss: 2.976385401141259

Epoch: 5| Step: 2
Training loss: 3.0058815479278564
Validation loss: 2.97454422520053

Epoch: 5| Step: 3
Training loss: 2.7682151794433594
Validation loss: 2.9678686459859214

Epoch: 5| Step: 4
Training loss: 2.6253390312194824
Validation loss: 2.9686526278013825

Epoch: 5| Step: 5
Training loss: 3.6510977745056152
Validation loss: 2.9626389113805627

Epoch: 5| Step: 6
Training loss: 2.7830138206481934
Validation loss: 2.95504444132569

Epoch: 5| Step: 7
Training loss: 2.8976223468780518
Validation loss: 2.9539032033694688

Epoch: 5| Step: 8
Training loss: 3.6008925437927246
Validation loss: 2.9513621317443026

Epoch: 5| Step: 9
Training loss: 3.025193452835083
Validation loss: 2.9498053930139028

Epoch: 5| Step: 10
Training loss: 3.2568178176879883
Validation loss: 2.9484639988150647

Epoch: 20| Step: 0
Training loss: 3.061692714691162
Validation loss: 2.946033082982545

Epoch: 5| Step: 1
Training loss: 2.4603426456451416
Validation loss: 2.942903805804509

Epoch: 5| Step: 2
Training loss: 3.2840397357940674
Validation loss: 2.9413553514788227

Epoch: 5| Step: 3
Training loss: 2.916879177093506
Validation loss: 2.936542998078049

Epoch: 5| Step: 4
Training loss: 3.807081699371338
Validation loss: 2.937810395353584

Epoch: 5| Step: 5
Training loss: 3.3419601917266846
Validation loss: 2.931481299861785

Epoch: 5| Step: 6
Training loss: 2.5180325508117676
Validation loss: 2.929380968052854

Epoch: 5| Step: 7
Training loss: 3.7868213653564453
Validation loss: 2.9277212876145557

Epoch: 5| Step: 8
Training loss: 2.947436571121216
Validation loss: 2.9247532557415705

Epoch: 5| Step: 9
Training loss: 2.532841444015503
Validation loss: 2.9237079210178827

Epoch: 5| Step: 10
Training loss: 2.4120688438415527
Validation loss: 2.9240445013969176

Epoch: 21| Step: 0
Training loss: 3.0383734703063965
Validation loss: 2.9325742952285276

Epoch: 5| Step: 1
Training loss: 3.6167423725128174
Validation loss: 2.924339420051985

Epoch: 5| Step: 2
Training loss: 2.7513599395751953
Validation loss: 2.911902581491778

Epoch: 5| Step: 3
Training loss: 2.8162617683410645
Validation loss: 2.9102269372632428

Epoch: 5| Step: 4
Training loss: 2.964019298553467
Validation loss: 2.9135110685902257

Epoch: 5| Step: 5
Training loss: 4.129762649536133
Validation loss: 2.9118785089062107

Epoch: 5| Step: 6
Training loss: 3.3552443981170654
Validation loss: 2.9074095654231247

Epoch: 5| Step: 7
Training loss: 2.1302170753479004
Validation loss: 2.9048377057557464

Epoch: 5| Step: 8
Training loss: 3.15083646774292
Validation loss: 2.9033611794953704

Epoch: 5| Step: 9
Training loss: 2.3422653675079346
Validation loss: 2.9044655804993003

Epoch: 5| Step: 10
Training loss: 2.7108840942382812
Validation loss: 2.901016555806642

Epoch: 22| Step: 0
Training loss: 3.50141978263855
Validation loss: 2.897649820132922

Epoch: 5| Step: 1
Training loss: 3.2216594219207764
Validation loss: 2.8926285902659097

Epoch: 5| Step: 2
Training loss: 2.375607967376709
Validation loss: 2.8917051053816274

Epoch: 5| Step: 3
Training loss: 3.2455406188964844
Validation loss: 2.890448921470232

Epoch: 5| Step: 4
Training loss: 2.794778347015381
Validation loss: 2.8906248974543747

Epoch: 5| Step: 5
Training loss: 2.3481087684631348
Validation loss: 2.8971369215237197

Epoch: 5| Step: 6
Training loss: 2.735600233078003
Validation loss: 2.8895146462225143

Epoch: 5| Step: 7
Training loss: 3.2834746837615967
Validation loss: 2.884217749359787

Epoch: 5| Step: 8
Training loss: 3.4469985961914062
Validation loss: 2.882778057488062

Epoch: 5| Step: 9
Training loss: 3.2346909046173096
Validation loss: 2.881631458959272

Epoch: 5| Step: 10
Training loss: 2.5996477603912354
Validation loss: 2.874858912601266

Epoch: 23| Step: 0
Training loss: 3.495746612548828
Validation loss: 2.875975977989935

Epoch: 5| Step: 1
Training loss: 2.9158518314361572
Validation loss: 2.873729295628045

Epoch: 5| Step: 2
Training loss: 2.5056569576263428
Validation loss: 2.8687613420588995

Epoch: 5| Step: 3
Training loss: 2.312743663787842
Validation loss: 2.8669778326506257

Epoch: 5| Step: 4
Training loss: 3.5363452434539795
Validation loss: 2.8618063670332714

Epoch: 5| Step: 5
Training loss: 2.9619529247283936
Validation loss: 2.8640396877001693

Epoch: 5| Step: 6
Training loss: 3.0451278686523438
Validation loss: 2.8656890110302995

Epoch: 5| Step: 7
Training loss: 3.282578706741333
Validation loss: 2.8664222840339906

Epoch: 5| Step: 8
Training loss: 3.2910614013671875
Validation loss: 2.857785883770194

Epoch: 5| Step: 9
Training loss: 2.710254669189453
Validation loss: 2.853029469008087

Epoch: 5| Step: 10
Training loss: 2.593859910964966
Validation loss: 2.852165529804845

Epoch: 24| Step: 0
Training loss: 2.4854915142059326
Validation loss: 2.849551470048966

Epoch: 5| Step: 1
Training loss: 2.7273669242858887
Validation loss: 2.851088636664934

Epoch: 5| Step: 2
Training loss: 2.970961093902588
Validation loss: 2.8495377084260345

Epoch: 5| Step: 3
Training loss: 3.21295166015625
Validation loss: 2.846217099056449

Epoch: 5| Step: 4
Training loss: 2.906952381134033
Validation loss: 2.843437164060531

Epoch: 5| Step: 5
Training loss: 3.268364667892456
Validation loss: 2.8439659021234

Epoch: 5| Step: 6
Training loss: 2.896228313446045
Validation loss: 2.8424460810999714

Epoch: 5| Step: 7
Training loss: 2.952775478363037
Validation loss: 2.8416646475433023

Epoch: 5| Step: 8
Training loss: 2.957432746887207
Validation loss: 2.8407618409843853

Epoch: 5| Step: 9
Training loss: 3.8089568614959717
Validation loss: 2.836615882894044

Epoch: 5| Step: 10
Training loss: 2.2167797088623047
Validation loss: 2.837829702643938

Epoch: 25| Step: 0
Training loss: 2.5903189182281494
Validation loss: 2.841417953532229

Epoch: 5| Step: 1
Training loss: 4.041113376617432
Validation loss: 2.844462812587779

Epoch: 5| Step: 2
Training loss: 3.195281982421875
Validation loss: 2.836837222499232

Epoch: 5| Step: 3
Training loss: 2.438642978668213
Validation loss: 2.8298715263284664

Epoch: 5| Step: 4
Training loss: 2.517951488494873
Validation loss: 2.8266852132735716

Epoch: 5| Step: 5
Training loss: 3.562661647796631
Validation loss: 2.826430166921308

Epoch: 5| Step: 6
Training loss: 3.5590221881866455
Validation loss: 2.823079983393351

Epoch: 5| Step: 7
Training loss: 2.0464611053466797
Validation loss: 2.8223442390400875

Epoch: 5| Step: 8
Training loss: 3.0471620559692383
Validation loss: 2.8206445683715162

Epoch: 5| Step: 9
Training loss: 2.8943591117858887
Validation loss: 2.8184421677743234

Epoch: 5| Step: 10
Training loss: 2.4181153774261475
Validation loss: 2.8187967782379477

Epoch: 26| Step: 0
Training loss: 2.660062074661255
Validation loss: 2.8189814962366575

Epoch: 5| Step: 1
Training loss: 3.4993178844451904
Validation loss: 2.8215490259150022

Epoch: 5| Step: 2
Training loss: 2.5266363620758057
Validation loss: 2.8234781731841383

Epoch: 5| Step: 3
Training loss: 2.907409906387329
Validation loss: 2.812759366086734

Epoch: 5| Step: 4
Training loss: 3.2005934715270996
Validation loss: 2.812152539530108

Epoch: 5| Step: 5
Training loss: 3.0826680660247803
Validation loss: 2.8093598888766382

Epoch: 5| Step: 6
Training loss: 2.6622939109802246
Validation loss: 2.8069504384071595

Epoch: 5| Step: 7
Training loss: 3.329510450363159
Validation loss: 2.8084533881115656

Epoch: 5| Step: 8
Training loss: 2.732034683227539
Validation loss: 2.8090683542272097

Epoch: 5| Step: 9
Training loss: 2.3735461235046387
Validation loss: 2.806643268113495

Epoch: 5| Step: 10
Training loss: 3.406839609146118
Validation loss: 2.805735065091041

Epoch: 27| Step: 0
Training loss: 3.002983570098877
Validation loss: 2.8024464063746954

Epoch: 5| Step: 1
Training loss: 3.0241997241973877
Validation loss: 2.800401964495259

Epoch: 5| Step: 2
Training loss: 3.450829267501831
Validation loss: 2.7985067239371677

Epoch: 5| Step: 3
Training loss: 4.110705375671387
Validation loss: 2.7979001921992146

Epoch: 5| Step: 4
Training loss: 2.8724372386932373
Validation loss: 2.7986249564796366

Epoch: 5| Step: 5
Training loss: 2.967855215072632
Validation loss: 2.7955627697770313

Epoch: 5| Step: 6
Training loss: 2.7707154750823975
Validation loss: 2.7919486748274935

Epoch: 5| Step: 7
Training loss: 2.7492988109588623
Validation loss: 2.7928738235145487

Epoch: 5| Step: 8
Training loss: 2.444060802459717
Validation loss: 2.7924001370706866

Epoch: 5| Step: 9
Training loss: 2.529301166534424
Validation loss: 2.7933049407056583

Epoch: 5| Step: 10
Training loss: 2.1681501865386963
Validation loss: 2.7912181449192826

Epoch: 28| Step: 0
Training loss: 3.345073699951172
Validation loss: 2.7901552364390385

Epoch: 5| Step: 1
Training loss: 2.4291625022888184
Validation loss: 2.7867318994255474

Epoch: 5| Step: 2
Training loss: 3.5735740661621094
Validation loss: 2.7834655495100122

Epoch: 5| Step: 3
Training loss: 3.5640969276428223
Validation loss: 2.7774949689065256

Epoch: 5| Step: 4
Training loss: 2.3901591300964355
Validation loss: 2.7797050937529533

Epoch: 5| Step: 5
Training loss: 2.922711133956909
Validation loss: 2.7788035254324637

Epoch: 5| Step: 6
Training loss: 3.3221001625061035
Validation loss: 2.7784211533043974

Epoch: 5| Step: 7
Training loss: 2.61047101020813
Validation loss: 2.774554665370654

Epoch: 5| Step: 8
Training loss: 2.9523181915283203
Validation loss: 2.7726873966955368

Epoch: 5| Step: 9
Training loss: 2.37739896774292
Validation loss: 2.7757700207412883

Epoch: 5| Step: 10
Training loss: 2.5755038261413574
Validation loss: 2.7768583502820743

Epoch: 29| Step: 0
Training loss: 3.181471586227417
Validation loss: 2.7734398739312285

Epoch: 5| Step: 1
Training loss: 2.9880428314208984
Validation loss: 2.7708493073781333

Epoch: 5| Step: 2
Training loss: 2.526026964187622
Validation loss: 2.769240860016115

Epoch: 5| Step: 3
Training loss: 2.706026554107666
Validation loss: 2.7670608925563034

Epoch: 5| Step: 4
Training loss: 2.726109743118286
Validation loss: 2.7651191526843655

Epoch: 5| Step: 5
Training loss: 2.361358642578125
Validation loss: 2.7622107562198432

Epoch: 5| Step: 6
Training loss: 3.3589425086975098
Validation loss: 2.760544094988095

Epoch: 5| Step: 7
Training loss: 2.619149684906006
Validation loss: 2.7611774039524857

Epoch: 5| Step: 8
Training loss: 3.358553647994995
Validation loss: 2.756368024374849

Epoch: 5| Step: 9
Training loss: 2.945220470428467
Validation loss: 2.756847212391515

Epoch: 5| Step: 10
Training loss: 3.262185573577881
Validation loss: 2.7597350766581874

Epoch: 30| Step: 0
Training loss: 2.237354278564453
Validation loss: 2.761540653885052

Epoch: 5| Step: 1
Training loss: 2.5979316234588623
Validation loss: 2.765400209734517

Epoch: 5| Step: 2
Training loss: 2.7102272510528564
Validation loss: 2.7550644361844627

Epoch: 5| Step: 3
Training loss: 3.516464948654175
Validation loss: 2.757857176565355

Epoch: 5| Step: 4
Training loss: 2.5258259773254395
Validation loss: 2.7513224078762915

Epoch: 5| Step: 5
Training loss: 3.099318742752075
Validation loss: 2.7509507107478317

Epoch: 5| Step: 6
Training loss: 2.78562593460083
Validation loss: 2.748170991097727

Epoch: 5| Step: 7
Training loss: 3.1328694820404053
Validation loss: 2.744875692552136

Epoch: 5| Step: 8
Training loss: 3.1306405067443848
Validation loss: 2.7456061686238935

Epoch: 5| Step: 9
Training loss: 3.21384859085083
Validation loss: 2.7416327204755557

Epoch: 5| Step: 10
Training loss: 2.934750556945801
Validation loss: 2.740732423720821

Epoch: 31| Step: 0
Training loss: 2.979182720184326
Validation loss: 2.7434590631915676

Epoch: 5| Step: 1
Training loss: 2.5667672157287598
Validation loss: 2.740924043040122

Epoch: 5| Step: 2
Training loss: 2.4685111045837402
Validation loss: 2.739314768903999

Epoch: 5| Step: 3
Training loss: 3.1708810329437256
Validation loss: 2.7371364485832954

Epoch: 5| Step: 4
Training loss: 2.4133803844451904
Validation loss: 2.7339905154320503

Epoch: 5| Step: 5
Training loss: 2.7316975593566895
Validation loss: 2.73311004331035

Epoch: 5| Step: 6
Training loss: 3.23042368888855
Validation loss: 2.7327693662335797

Epoch: 5| Step: 7
Training loss: 3.163991689682007
Validation loss: 2.73885408524544

Epoch: 5| Step: 8
Training loss: 2.923733711242676
Validation loss: 2.7322823155310845

Epoch: 5| Step: 9
Training loss: 2.8111820220947266
Validation loss: 2.727684728560909

Epoch: 5| Step: 10
Training loss: 3.391575574874878
Validation loss: 2.7246711741211596

Epoch: 32| Step: 0
Training loss: 3.3515472412109375
Validation loss: 2.7256132325818463

Epoch: 5| Step: 1
Training loss: 2.681973457336426
Validation loss: 2.72555547888561

Epoch: 5| Step: 2
Training loss: 3.0084035396575928
Validation loss: 2.72546786133961

Epoch: 5| Step: 3
Training loss: 2.496325969696045
Validation loss: 2.7240283514863703

Epoch: 5| Step: 4
Training loss: 2.773881196975708
Validation loss: 2.7297440985197663

Epoch: 5| Step: 5
Training loss: 3.1902623176574707
Validation loss: 2.7225018291063208

Epoch: 5| Step: 6
Training loss: 2.127668619155884
Validation loss: 2.718111986755043

Epoch: 5| Step: 7
Training loss: 3.046441078186035
Validation loss: 2.715227673130651

Epoch: 5| Step: 8
Training loss: 3.39105486869812
Validation loss: 2.713895044019145

Epoch: 5| Step: 9
Training loss: 3.150393486022949
Validation loss: 2.7119535297475834

Epoch: 5| Step: 10
Training loss: 2.3728187084198
Validation loss: 2.711578817777736

Epoch: 33| Step: 0
Training loss: 2.866581439971924
Validation loss: 2.7172446789280063

Epoch: 5| Step: 1
Training loss: 2.543024778366089
Validation loss: 2.709240787772722

Epoch: 5| Step: 2
Training loss: 2.609375
Validation loss: 2.708037578931419

Epoch: 5| Step: 3
Training loss: 2.1849892139434814
Validation loss: 2.70586077115869

Epoch: 5| Step: 4
Training loss: 2.4882009029388428
Validation loss: 2.7059298151282856

Epoch: 5| Step: 5
Training loss: 2.6879334449768066
Validation loss: 2.7043888133059264

Epoch: 5| Step: 6
Training loss: 3.1562983989715576
Validation loss: 2.7024300252237627

Epoch: 5| Step: 7
Training loss: 3.774132251739502
Validation loss: 2.7006566524505615

Epoch: 5| Step: 8
Training loss: 3.410271406173706
Validation loss: 2.6996003453449537

Epoch: 5| Step: 9
Training loss: 2.952427387237549
Validation loss: 2.6983756403769217

Epoch: 5| Step: 10
Training loss: 2.8841960430145264
Validation loss: 2.6969854062603367

Epoch: 34| Step: 0
Training loss: 2.6762237548828125
Validation loss: 2.69803104605726

Epoch: 5| Step: 1
Training loss: 2.326098680496216
Validation loss: 2.6998050084678074

Epoch: 5| Step: 2
Training loss: 2.249410390853882
Validation loss: 2.709845430107527

Epoch: 5| Step: 3
Training loss: 3.726815700531006
Validation loss: 2.6996890985837547

Epoch: 5| Step: 4
Training loss: 2.744499921798706
Validation loss: 2.6907094191479426

Epoch: 5| Step: 5
Training loss: 2.6393821239471436
Validation loss: 2.70201608186127

Epoch: 5| Step: 6
Training loss: 3.6449203491210938
Validation loss: 2.710415112074985

Epoch: 5| Step: 7
Training loss: 2.0964951515197754
Validation loss: 2.7196692369317494

Epoch: 5| Step: 8
Training loss: 3.1084303855895996
Validation loss: 2.7183161602225354

Epoch: 5| Step: 9
Training loss: 2.6254119873046875
Validation loss: 2.7161897049155286

Epoch: 5| Step: 10
Training loss: 3.909838914871216
Validation loss: 2.7171428254855576

Epoch: 35| Step: 0
Training loss: 3.8183391094207764
Validation loss: 2.7080571164367018

Epoch: 5| Step: 1
Training loss: 2.1100337505340576
Validation loss: 2.692988867400795

Epoch: 5| Step: 2
Training loss: 3.441669464111328
Validation loss: 2.687061112414124

Epoch: 5| Step: 3
Training loss: 2.4433999061584473
Validation loss: 2.6867065480960313

Epoch: 5| Step: 4
Training loss: 2.70327091217041
Validation loss: 2.692988698200513

Epoch: 5| Step: 5
Training loss: 3.562758684158325
Validation loss: 2.6949338861691055

Epoch: 5| Step: 6
Training loss: 2.577371120452881
Validation loss: 2.700071745021369

Epoch: 5| Step: 7
Training loss: 2.296107053756714
Validation loss: 2.6941003594347226

Epoch: 5| Step: 8
Training loss: 3.0480809211730957
Validation loss: 2.6875657445640972

Epoch: 5| Step: 9
Training loss: 3.3045272827148438
Validation loss: 2.696555596525951

Epoch: 5| Step: 10
Training loss: 2.083387851715088
Validation loss: 2.684853889608896

Epoch: 36| Step: 0
Training loss: 2.7321574687957764
Validation loss: 2.6795764174512637

Epoch: 5| Step: 1
Training loss: 2.61167573928833
Validation loss: 2.6794092039908133

Epoch: 5| Step: 2
Training loss: 2.233664035797119
Validation loss: 2.682722701821276

Epoch: 5| Step: 3
Training loss: 2.6799426078796387
Validation loss: 2.680276568217944

Epoch: 5| Step: 4
Training loss: 2.765176773071289
Validation loss: 2.680772243007537

Epoch: 5| Step: 5
Training loss: 2.6845386028289795
Validation loss: 2.6747064846818165

Epoch: 5| Step: 6
Training loss: 2.7646727561950684
Validation loss: 2.677864113161641

Epoch: 5| Step: 7
Training loss: 3.1656711101531982
Validation loss: 2.672927325771701

Epoch: 5| Step: 8
Training loss: 2.384880781173706
Validation loss: 2.6692654112333893

Epoch: 5| Step: 9
Training loss: 3.764749050140381
Validation loss: 2.6662180269918134

Epoch: 5| Step: 10
Training loss: 3.636244535446167
Validation loss: 2.6685018385610273

Epoch: 37| Step: 0
Training loss: 3.162520170211792
Validation loss: 2.6669021832045687

Epoch: 5| Step: 1
Training loss: 2.625683307647705
Validation loss: 2.664381568149854

Epoch: 5| Step: 2
Training loss: 3.4067161083221436
Validation loss: 2.6627018092780985

Epoch: 5| Step: 3
Training loss: 2.6808743476867676
Validation loss: 2.6635163676354194

Epoch: 5| Step: 4
Training loss: 2.1413090229034424
Validation loss: 2.656763530546619

Epoch: 5| Step: 5
Training loss: 2.6796135902404785
Validation loss: 2.655050540483126

Epoch: 5| Step: 6
Training loss: 2.6643242835998535
Validation loss: 2.6563382764016428

Epoch: 5| Step: 7
Training loss: 3.329075574874878
Validation loss: 2.6569107963192846

Epoch: 5| Step: 8
Training loss: 2.92301607131958
Validation loss: 2.658610495187903

Epoch: 5| Step: 9
Training loss: 2.807180881500244
Validation loss: 2.655241030518727

Epoch: 5| Step: 10
Training loss: 2.8000926971435547
Validation loss: 2.6516879322708293

Epoch: 38| Step: 0
Training loss: 3.026984691619873
Validation loss: 2.655359629661806

Epoch: 5| Step: 1
Training loss: 3.0816006660461426
Validation loss: 2.654450173019081

Epoch: 5| Step: 2
Training loss: 2.549903154373169
Validation loss: 2.6505779758576424

Epoch: 5| Step: 3
Training loss: 3.221604824066162
Validation loss: 2.6476293968897995

Epoch: 5| Step: 4
Training loss: 2.258258104324341
Validation loss: 2.645946346303468

Epoch: 5| Step: 5
Training loss: 3.1705002784729004
Validation loss: 2.6424887846874934

Epoch: 5| Step: 6
Training loss: 2.859917163848877
Validation loss: 2.641724059658666

Epoch: 5| Step: 7
Training loss: 2.269083261489868
Validation loss: 2.658240984844905

Epoch: 5| Step: 8
Training loss: 2.73283052444458
Validation loss: 2.6562734188572055

Epoch: 5| Step: 9
Training loss: 2.865825653076172
Validation loss: 2.6543592022311304

Epoch: 5| Step: 10
Training loss: 3.186039924621582
Validation loss: 2.6385884695155646

Epoch: 39| Step: 0
Training loss: 2.861450672149658
Validation loss: 2.6348463873709402

Epoch: 5| Step: 1
Training loss: 2.383669137954712
Validation loss: 2.6394794115456204

Epoch: 5| Step: 2
Training loss: 2.578195095062256
Validation loss: 2.640326735793903

Epoch: 5| Step: 3
Training loss: 2.798647880554199
Validation loss: 2.6391738563455562

Epoch: 5| Step: 4
Training loss: 2.9970717430114746
Validation loss: 2.6456822502997612

Epoch: 5| Step: 5
Training loss: 3.5563647747039795
Validation loss: 2.6409968278741323

Epoch: 5| Step: 6
Training loss: 2.8654892444610596
Validation loss: 2.6387980548284387

Epoch: 5| Step: 7
Training loss: 2.691145658493042
Validation loss: 2.639072323358187

Epoch: 5| Step: 8
Training loss: 2.2535557746887207
Validation loss: 2.6339260326918734

Epoch: 5| Step: 9
Training loss: 3.17699933052063
Validation loss: 2.636923013194915

Epoch: 5| Step: 10
Training loss: 2.96358323097229
Validation loss: 2.6394918785300305

Epoch: 40| Step: 0
Training loss: 3.314307451248169
Validation loss: 2.632476196494154

Epoch: 5| Step: 1
Training loss: 2.961960554122925
Validation loss: 2.627175869480256

Epoch: 5| Step: 2
Training loss: 2.799290895462036
Validation loss: 2.627225006780317

Epoch: 5| Step: 3
Training loss: 2.318790912628174
Validation loss: 2.6278401549144457

Epoch: 5| Step: 4
Training loss: 3.4143848419189453
Validation loss: 2.622658896189864

Epoch: 5| Step: 5
Training loss: 2.2773776054382324
Validation loss: 2.6213619760287705

Epoch: 5| Step: 6
Training loss: 2.683075428009033
Validation loss: 2.623754652597571

Epoch: 5| Step: 7
Training loss: 3.032712936401367
Validation loss: 2.6301345107375935

Epoch: 5| Step: 8
Training loss: 2.3777260780334473
Validation loss: 2.6309201589194675

Epoch: 5| Step: 9
Training loss: 3.4053051471710205
Validation loss: 2.644498863527852

Epoch: 5| Step: 10
Training loss: 2.330122947692871
Validation loss: 2.6307993704272854

Epoch: 41| Step: 0
Training loss: 3.0159332752227783
Validation loss: 2.614130509796963

Epoch: 5| Step: 1
Training loss: 3.4821407794952393
Validation loss: 2.6140075140101935

Epoch: 5| Step: 2
Training loss: 3.0205237865448
Validation loss: 2.6166533577826714

Epoch: 5| Step: 3
Training loss: 2.1260175704956055
Validation loss: 2.621179216651506

Epoch: 5| Step: 4
Training loss: 2.5993800163269043
Validation loss: 2.6181805313274427

Epoch: 5| Step: 5
Training loss: 3.226064682006836
Validation loss: 2.6237008084533033

Epoch: 5| Step: 6
Training loss: 2.329989194869995
Validation loss: 2.6181039374361754

Epoch: 5| Step: 7
Training loss: 2.7909579277038574
Validation loss: 2.620090367973492

Epoch: 5| Step: 8
Training loss: 2.759321689605713
Validation loss: 2.618941089158417

Epoch: 5| Step: 9
Training loss: 2.44100022315979
Validation loss: 2.616908901481218

Epoch: 5| Step: 10
Training loss: 3.199289321899414
Validation loss: 2.625678495694232

Epoch: 42| Step: 0
Training loss: 3.1100542545318604
Validation loss: 2.6191986196784565

Epoch: 5| Step: 1
Training loss: 2.3073134422302246
Validation loss: 2.6167283493985414

Epoch: 5| Step: 2
Training loss: 2.375181198120117
Validation loss: 2.613847747925789

Epoch: 5| Step: 3
Training loss: 3.0186452865600586
Validation loss: 2.640819436760359

Epoch: 5| Step: 4
Training loss: 2.643580675125122
Validation loss: 2.646705755623438

Epoch: 5| Step: 5
Training loss: 2.942427158355713
Validation loss: 2.655957624476443

Epoch: 5| Step: 6
Training loss: 3.0688822269439697
Validation loss: 2.658085992259364

Epoch: 5| Step: 7
Training loss: 2.730329990386963
Validation loss: 2.6656179376827773

Epoch: 5| Step: 8
Training loss: 2.9912197589874268
Validation loss: 2.6569867236639864

Epoch: 5| Step: 9
Training loss: 2.791038751602173
Validation loss: 2.6578709489555767

Epoch: 5| Step: 10
Training loss: 3.186354160308838
Validation loss: 2.658522275186354

Epoch: 43| Step: 0
Training loss: 2.730320453643799
Validation loss: 2.655600752881778

Epoch: 5| Step: 1
Training loss: 3.5254836082458496
Validation loss: 2.6617828774195846

Epoch: 5| Step: 2
Training loss: 3.049791097640991
Validation loss: 2.6521957305169876

Epoch: 5| Step: 3
Training loss: 2.6265406608581543
Validation loss: 2.6488067514152935

Epoch: 5| Step: 4
Training loss: 2.6903796195983887
Validation loss: 2.649945807713334

Epoch: 5| Step: 5
Training loss: 2.2935597896575928
Validation loss: 2.640258232752482

Epoch: 5| Step: 6
Training loss: 2.640611171722412
Validation loss: 2.640312784461565

Epoch: 5| Step: 7
Training loss: 2.952235221862793
Validation loss: 2.641330767703313

Epoch: 5| Step: 8
Training loss: 2.58980655670166
Validation loss: 2.6401176401363906

Epoch: 5| Step: 9
Training loss: 3.353412628173828
Validation loss: 2.641243444975986

Epoch: 5| Step: 10
Training loss: 2.672452926635742
Validation loss: 2.6405102258087485

Epoch: 44| Step: 0
Training loss: 2.9248363971710205
Validation loss: 2.6386422341869724

Epoch: 5| Step: 1
Training loss: 3.2622580528259277
Validation loss: 2.64015281328591

Epoch: 5| Step: 2
Training loss: 2.554173469543457
Validation loss: 2.637311771351804

Epoch: 5| Step: 3
Training loss: 3.2741761207580566
Validation loss: 2.635027952091668

Epoch: 5| Step: 4
Training loss: 2.4695756435394287
Validation loss: 2.631757859260805

Epoch: 5| Step: 5
Training loss: 2.9091365337371826
Validation loss: 2.6316051278063046

Epoch: 5| Step: 6
Training loss: 2.319030284881592
Validation loss: 2.630812639831215

Epoch: 5| Step: 7
Training loss: 2.627814531326294
Validation loss: 2.626944459894652

Epoch: 5| Step: 8
Training loss: 2.4392199516296387
Validation loss: 2.6251730790702243

Epoch: 5| Step: 9
Training loss: 3.1795525550842285
Validation loss: 2.6261530794123167

Epoch: 5| Step: 10
Training loss: 3.1299610137939453
Validation loss: 2.6225513258287982

Epoch: 45| Step: 0
Training loss: 3.105818271636963
Validation loss: 2.6230065720055693

Epoch: 5| Step: 1
Training loss: 2.53346586227417
Validation loss: 2.623440570728753

Epoch: 5| Step: 2
Training loss: 3.0071167945861816
Validation loss: 2.623334479588334

Epoch: 5| Step: 3
Training loss: 2.8007638454437256
Validation loss: 2.618493072448238

Epoch: 5| Step: 4
Training loss: 2.0252699851989746
Validation loss: 2.622237795142717

Epoch: 5| Step: 5
Training loss: 3.4378859996795654
Validation loss: 2.6206451615979596

Epoch: 5| Step: 6
Training loss: 2.23907470703125
Validation loss: 2.615929147248627

Epoch: 5| Step: 7
Training loss: 2.9747493267059326
Validation loss: 2.6154281400865123

Epoch: 5| Step: 8
Training loss: 3.064093828201294
Validation loss: 2.6143165506342405

Epoch: 5| Step: 9
Training loss: 2.210669755935669
Validation loss: 2.611504198402487

Epoch: 5| Step: 10
Training loss: 3.604769468307495
Validation loss: 2.609887005180441

Epoch: 46| Step: 0
Training loss: 2.783752202987671
Validation loss: 2.6082662228615052

Epoch: 5| Step: 1
Training loss: 2.5391108989715576
Validation loss: 2.611090470385808

Epoch: 5| Step: 2
Training loss: 2.9475271701812744
Validation loss: 2.6057118241504957

Epoch: 5| Step: 3
Training loss: 2.844879627227783
Validation loss: 2.60779143405217

Epoch: 5| Step: 4
Training loss: 2.655872344970703
Validation loss: 2.6040524000762613

Epoch: 5| Step: 5
Training loss: 3.306002378463745
Validation loss: 2.606655764323409

Epoch: 5| Step: 6
Training loss: 2.5355803966522217
Validation loss: 2.601965699144589

Epoch: 5| Step: 7
Training loss: 3.2855639457702637
Validation loss: 2.6057727695793234

Epoch: 5| Step: 8
Training loss: 3.3417041301727295
Validation loss: 2.6054366378374

Epoch: 5| Step: 9
Training loss: 2.386995553970337
Validation loss: 2.6059498248561734

Epoch: 5| Step: 10
Training loss: 2.117215156555176
Validation loss: 2.6101588664516324

Epoch: 47| Step: 0
Training loss: 2.760324239730835
Validation loss: 2.605077789675805

Epoch: 5| Step: 1
Training loss: 2.8768417835235596
Validation loss: 2.6012689554563133

Epoch: 5| Step: 2
Training loss: 3.1084659099578857
Validation loss: 2.597426933626975

Epoch: 5| Step: 3
Training loss: 2.5629982948303223
Validation loss: 2.603213564042122

Epoch: 5| Step: 4
Training loss: 2.9581592082977295
Validation loss: 2.602173400181596

Epoch: 5| Step: 5
Training loss: 2.3338117599487305
Validation loss: 2.598354213981218

Epoch: 5| Step: 6
Training loss: 2.3118343353271484
Validation loss: 2.5973142052209504

Epoch: 5| Step: 7
Training loss: 2.6772921085357666
Validation loss: 2.595771215295279

Epoch: 5| Step: 8
Training loss: 3.6113791465759277
Validation loss: 2.5944876568291777

Epoch: 5| Step: 9
Training loss: 2.82851505279541
Validation loss: 2.5921080599549

Epoch: 5| Step: 10
Training loss: 2.7004833221435547
Validation loss: 2.592117735134658

Epoch: 48| Step: 0
Training loss: 2.3553285598754883
Validation loss: 2.5999206240459154

Epoch: 5| Step: 1
Training loss: 2.186713695526123
Validation loss: 2.618059978690199

Epoch: 5| Step: 2
Training loss: 3.309678316116333
Validation loss: 2.5989108495814826

Epoch: 5| Step: 3
Training loss: 2.440603733062744
Validation loss: 2.5879444947806736

Epoch: 5| Step: 4
Training loss: 3.0457072257995605
Validation loss: 2.5956052246914116

Epoch: 5| Step: 5
Training loss: 2.1314938068389893
Validation loss: 2.605156921571301

Epoch: 5| Step: 6
Training loss: 3.032726287841797
Validation loss: 2.5988078835189983

Epoch: 5| Step: 7
Training loss: 3.5384621620178223
Validation loss: 2.602241316149312

Epoch: 5| Step: 8
Training loss: 2.547673225402832
Validation loss: 2.596572117138934

Epoch: 5| Step: 9
Training loss: 3.164383888244629
Validation loss: 2.588231243113036

Epoch: 5| Step: 10
Training loss: 3.0782179832458496
Validation loss: 2.5941987909296507

Epoch: 49| Step: 0
Training loss: 2.593708038330078
Validation loss: 2.5900803689033753

Epoch: 5| Step: 1
Training loss: 2.6237268447875977
Validation loss: 2.5946494225532777

Epoch: 5| Step: 2
Training loss: 3.3357901573181152
Validation loss: 2.6075772675134803

Epoch: 5| Step: 3
Training loss: 3.248716354370117
Validation loss: 2.59512282699667

Epoch: 5| Step: 4
Training loss: 2.4861767292022705
Validation loss: 2.5861292526286137

Epoch: 5| Step: 5
Training loss: 3.4118399620056152
Validation loss: 2.5807114647280787

Epoch: 5| Step: 6
Training loss: 2.3521361351013184
Validation loss: 2.5836047690401793

Epoch: 5| Step: 7
Training loss: 3.3811233043670654
Validation loss: 2.583610434685984

Epoch: 5| Step: 8
Training loss: 2.478097438812256
Validation loss: 2.5836851596832275

Epoch: 5| Step: 9
Training loss: 2.1791012287139893
Validation loss: 2.584232673850111

Epoch: 5| Step: 10
Training loss: 2.567617654800415
Validation loss: 2.5866704397304083

Epoch: 50| Step: 0
Training loss: 3.213341474533081
Validation loss: 2.58429891063321

Epoch: 5| Step: 1
Training loss: 2.848397970199585
Validation loss: 2.583649196932393

Epoch: 5| Step: 2
Training loss: 2.636809825897217
Validation loss: 2.5836389423698507

Epoch: 5| Step: 3
Training loss: 2.9872066974639893
Validation loss: 2.584405591410975

Epoch: 5| Step: 4
Training loss: 2.8838400840759277
Validation loss: 2.5804849491324475

Epoch: 5| Step: 5
Training loss: 2.8758246898651123
Validation loss: 2.5802512117611465

Epoch: 5| Step: 6
Training loss: 2.664175033569336
Validation loss: 2.5794543835424606

Epoch: 5| Step: 7
Training loss: 2.2255661487579346
Validation loss: 2.5776416947764735

Epoch: 5| Step: 8
Training loss: 2.4359889030456543
Validation loss: 2.581562431909705

Epoch: 5| Step: 9
Training loss: 3.1823933124542236
Validation loss: 2.5842296641360045

Epoch: 5| Step: 10
Training loss: 2.7271103858947754
Validation loss: 2.5824679610549763

Epoch: 51| Step: 0
Training loss: 3.28092622756958
Validation loss: 2.581173584025393

Epoch: 5| Step: 1
Training loss: 2.889547348022461
Validation loss: 2.578885675758444

Epoch: 5| Step: 2
Training loss: 2.1266610622406006
Validation loss: 2.575703595274238

Epoch: 5| Step: 3
Training loss: 3.293057918548584
Validation loss: 2.5741918087005615

Epoch: 5| Step: 4
Training loss: 2.9928174018859863
Validation loss: 2.5707899191046275

Epoch: 5| Step: 5
Training loss: 2.686098337173462
Validation loss: 2.5723476871367423

Epoch: 5| Step: 6
Training loss: 2.893059253692627
Validation loss: 2.566960357850598

Epoch: 5| Step: 7
Training loss: 3.118480682373047
Validation loss: 2.568114665246779

Epoch: 5| Step: 8
Training loss: 3.052924871444702
Validation loss: 2.5614812117750927

Epoch: 5| Step: 9
Training loss: 2.1074843406677246
Validation loss: 2.5659199145532425

Epoch: 5| Step: 10
Training loss: 2.0491249561309814
Validation loss: 2.5685495996987946

Epoch: 52| Step: 0
Training loss: 2.965820789337158
Validation loss: 2.5734591766070296

Epoch: 5| Step: 1
Training loss: 2.8803582191467285
Validation loss: 2.5719577855961298

Epoch: 5| Step: 2
Training loss: 2.3560566902160645
Validation loss: 2.5794510379914315

Epoch: 5| Step: 3
Training loss: 2.662813663482666
Validation loss: 2.5891280225528184

Epoch: 5| Step: 4
Training loss: 2.7457809448242188
Validation loss: 2.5971071848305325

Epoch: 5| Step: 5
Training loss: 2.8533012866973877
Validation loss: 2.581211607943299

Epoch: 5| Step: 6
Training loss: 4.01815938949585
Validation loss: 2.5703153687138713

Epoch: 5| Step: 7
Training loss: 2.2365310192108154
Validation loss: 2.566022721670007

Epoch: 5| Step: 8
Training loss: 2.674450397491455
Validation loss: 2.5642061182247695

Epoch: 5| Step: 9
Training loss: 2.878239631652832
Validation loss: 2.5699780628245366

Epoch: 5| Step: 10
Training loss: 2.229032516479492
Validation loss: 2.5641768414487123

Epoch: 53| Step: 0
Training loss: 2.6935606002807617
Validation loss: 2.55880392495022

Epoch: 5| Step: 1
Training loss: 1.9350881576538086
Validation loss: 2.5570156497340046

Epoch: 5| Step: 2
Training loss: 2.7693614959716797
Validation loss: 2.569997469584147

Epoch: 5| Step: 3
Training loss: 3.138421058654785
Validation loss: 2.572335030442925

Epoch: 5| Step: 4
Training loss: 2.8397116661071777
Validation loss: 2.565102956628287

Epoch: 5| Step: 5
Training loss: 3.2298691272735596
Validation loss: 2.563041333229311

Epoch: 5| Step: 6
Training loss: 2.5327141284942627
Validation loss: 2.5648339897073726

Epoch: 5| Step: 7
Training loss: 2.729994297027588
Validation loss: 2.562607575488347

Epoch: 5| Step: 8
Training loss: 2.781982898712158
Validation loss: 2.5577782610411286

Epoch: 5| Step: 9
Training loss: 3.1419739723205566
Validation loss: 2.5609852626759517

Epoch: 5| Step: 10
Training loss: 2.6538422107696533
Validation loss: 2.5596917880478727

Epoch: 54| Step: 0
Training loss: 2.8572921752929688
Validation loss: 2.5641124171595417

Epoch: 5| Step: 1
Training loss: 2.34061861038208
Validation loss: 2.5683112759743967

Epoch: 5| Step: 2
Training loss: 2.695913791656494
Validation loss: 2.5689112781196513

Epoch: 5| Step: 3
Training loss: 2.2047760486602783
Validation loss: 2.5596251974823656

Epoch: 5| Step: 4
Training loss: 3.070321559906006
Validation loss: 2.5460345078540105

Epoch: 5| Step: 5
Training loss: 2.8161189556121826
Validation loss: 2.551326846563688

Epoch: 5| Step: 6
Training loss: 2.6464829444885254
Validation loss: 2.5623638296640046

Epoch: 5| Step: 7
Training loss: 3.2975800037384033
Validation loss: 2.5690521450452906

Epoch: 5| Step: 8
Training loss: 3.3968346118927
Validation loss: 2.56131774122997

Epoch: 5| Step: 9
Training loss: 2.5317180156707764
Validation loss: 2.5587189479540755

Epoch: 5| Step: 10
Training loss: 2.5395257472991943
Validation loss: 2.550240155189268

Epoch: 55| Step: 0
Training loss: 2.9540176391601562
Validation loss: 2.5500036977952525

Epoch: 5| Step: 1
Training loss: 2.641270875930786
Validation loss: 2.543093084007181

Epoch: 5| Step: 2
Training loss: 2.6580607891082764
Validation loss: 2.5411953080085015

Epoch: 5| Step: 3
Training loss: 2.5266623497009277
Validation loss: 2.54083187349381

Epoch: 5| Step: 4
Training loss: 2.5406031608581543
Validation loss: 2.5406838873381257

Epoch: 5| Step: 5
Training loss: 3.70367431640625
Validation loss: 2.539642490366454

Epoch: 5| Step: 6
Training loss: 2.964475631713867
Validation loss: 2.540464872954994

Epoch: 5| Step: 7
Training loss: 2.864525318145752
Validation loss: 2.5387352128182687

Epoch: 5| Step: 8
Training loss: 2.651648759841919
Validation loss: 2.5389629205067954

Epoch: 5| Step: 9
Training loss: 2.3364651203155518
Validation loss: 2.538095369133898

Epoch: 5| Step: 10
Training loss: 2.3729801177978516
Validation loss: 2.54064465338184

Epoch: 56| Step: 0
Training loss: 2.480497121810913
Validation loss: 2.544423759624522

Epoch: 5| Step: 1
Training loss: 2.195117473602295
Validation loss: 2.5382056954086467

Epoch: 5| Step: 2
Training loss: 3.148780584335327
Validation loss: 2.5418346851102767

Epoch: 5| Step: 3
Training loss: 2.8008484840393066
Validation loss: 2.536704670998358

Epoch: 5| Step: 4
Training loss: 2.9633147716522217
Validation loss: 2.5319863134814846

Epoch: 5| Step: 5
Training loss: 2.7778120040893555
Validation loss: 2.5311373151758665

Epoch: 5| Step: 6
Training loss: 2.8369393348693848
Validation loss: 2.533259020056776

Epoch: 5| Step: 7
Training loss: 2.3914597034454346
Validation loss: 2.5317887208795034

Epoch: 5| Step: 8
Training loss: 2.165963649749756
Validation loss: 2.537950154273741

Epoch: 5| Step: 9
Training loss: 2.868898391723633
Validation loss: 2.539650014651719

Epoch: 5| Step: 10
Training loss: 3.798246145248413
Validation loss: 2.542123340791272

Epoch: 57| Step: 0
Training loss: 3.004187822341919
Validation loss: 2.540004886606688

Epoch: 5| Step: 1
Training loss: 1.9068959951400757
Validation loss: 2.5345186546284664

Epoch: 5| Step: 2
Training loss: 2.416562080383301
Validation loss: 2.5299757424221245

Epoch: 5| Step: 3
Training loss: 3.163111448287964
Validation loss: 2.530178064941078

Epoch: 5| Step: 4
Training loss: 2.464634895324707
Validation loss: 2.5254526843306837

Epoch: 5| Step: 5
Training loss: 3.0911448001861572
Validation loss: 2.5275773514983473

Epoch: 5| Step: 6
Training loss: 2.741492509841919
Validation loss: 2.5220486605039207

Epoch: 5| Step: 7
Training loss: 3.14369535446167
Validation loss: 2.5240393505301526

Epoch: 5| Step: 8
Training loss: 2.903747081756592
Validation loss: 2.530816980587539

Epoch: 5| Step: 9
Training loss: 2.405374050140381
Validation loss: 2.534172752852081

Epoch: 5| Step: 10
Training loss: 3.004551887512207
Validation loss: 2.523740650505148

Epoch: 58| Step: 0
Training loss: 3.211491346359253
Validation loss: 2.5210766561569704

Epoch: 5| Step: 1
Training loss: 2.591007709503174
Validation loss: 2.5200889136201594

Epoch: 5| Step: 2
Training loss: 2.2698557376861572
Validation loss: 2.5206651559440036

Epoch: 5| Step: 3
Training loss: 2.862905979156494
Validation loss: 2.5172181539638068

Epoch: 5| Step: 4
Training loss: 3.132140874862671
Validation loss: 2.5215627967670398

Epoch: 5| Step: 5
Training loss: 2.4896187782287598
Validation loss: 2.520018328902542

Epoch: 5| Step: 6
Training loss: 2.904738187789917
Validation loss: 2.515996261309552

Epoch: 5| Step: 7
Training loss: 1.8254997730255127
Validation loss: 2.5260499472259195

Epoch: 5| Step: 8
Training loss: 3.1290488243103027
Validation loss: 2.540590055527226

Epoch: 5| Step: 9
Training loss: 2.861131191253662
Validation loss: 2.5481220317143265

Epoch: 5| Step: 10
Training loss: 2.963296890258789
Validation loss: 2.540506819243072

Epoch: 59| Step: 0
Training loss: 2.5050065517425537
Validation loss: 2.531810783570813

Epoch: 5| Step: 1
Training loss: 3.0987257957458496
Validation loss: 2.529644940489082

Epoch: 5| Step: 2
Training loss: 3.0745902061462402
Validation loss: 2.525110290896508

Epoch: 5| Step: 3
Training loss: 2.6933112144470215
Validation loss: 2.514572161500172

Epoch: 5| Step: 4
Training loss: 2.602059841156006
Validation loss: 2.5204945072051017

Epoch: 5| Step: 5
Training loss: 2.588954448699951
Validation loss: 2.540249765560191

Epoch: 5| Step: 6
Training loss: 3.2280025482177734
Validation loss: 2.537736262044599

Epoch: 5| Step: 7
Training loss: 2.681851625442505
Validation loss: 2.5431133880410144

Epoch: 5| Step: 8
Training loss: 3.0863232612609863
Validation loss: 2.5283972755555184

Epoch: 5| Step: 9
Training loss: 1.8813438415527344
Validation loss: 2.5114861214032738

Epoch: 5| Step: 10
Training loss: 2.749558925628662
Validation loss: 2.5124604317449752

Epoch: 60| Step: 0
Training loss: 3.0680794715881348
Validation loss: 2.513919779049453

Epoch: 5| Step: 1
Training loss: 2.6019930839538574
Validation loss: 2.522505437174151

Epoch: 5| Step: 2
Training loss: 2.4691836833953857
Validation loss: 2.5363179804176412

Epoch: 5| Step: 3
Training loss: 2.728616237640381
Validation loss: 2.5500471873949935

Epoch: 5| Step: 4
Training loss: 2.564297676086426
Validation loss: 2.5645042388669905

Epoch: 5| Step: 5
Training loss: 2.627084732055664
Validation loss: 2.5596764702950754

Epoch: 5| Step: 6
Training loss: 2.759812831878662
Validation loss: 2.559488509290962

Epoch: 5| Step: 7
Training loss: 2.626826524734497
Validation loss: 2.5316556448577554

Epoch: 5| Step: 8
Training loss: 2.815739631652832
Validation loss: 2.5202921487951793

Epoch: 5| Step: 9
Training loss: 3.2269439697265625
Validation loss: 2.5147290870707524

Epoch: 5| Step: 10
Training loss: 2.647247791290283
Validation loss: 2.507507872837846

Epoch: 61| Step: 0
Training loss: 2.8113856315612793
Validation loss: 2.5090242688373854

Epoch: 5| Step: 1
Training loss: 3.556981325149536
Validation loss: 2.5133473334773893

Epoch: 5| Step: 2
Training loss: 2.631099224090576
Validation loss: 2.5163510743007866

Epoch: 5| Step: 3
Training loss: 2.3775439262390137
Validation loss: 2.520248774559267

Epoch: 5| Step: 4
Training loss: 2.628786325454712
Validation loss: 2.5210334870123092

Epoch: 5| Step: 5
Training loss: 2.6201603412628174
Validation loss: 2.5129854653471257

Epoch: 5| Step: 6
Training loss: 2.306467056274414
Validation loss: 2.5096991446710404

Epoch: 5| Step: 7
Training loss: 3.4473280906677246
Validation loss: 2.513381042788106

Epoch: 5| Step: 8
Training loss: 2.756258487701416
Validation loss: 2.513879117145333

Epoch: 5| Step: 9
Training loss: 2.229491710662842
Validation loss: 2.509510450465705

Epoch: 5| Step: 10
Training loss: 2.726900100708008
Validation loss: 2.508845116502495

Epoch: 62| Step: 0
Training loss: 2.9040143489837646
Validation loss: 2.514164647748393

Epoch: 5| Step: 1
Training loss: 2.9302871227264404
Validation loss: 2.538548900235084

Epoch: 5| Step: 2
Training loss: 2.389861583709717
Validation loss: 2.5397079888210503

Epoch: 5| Step: 3
Training loss: 3.2405846118927
Validation loss: 2.5607302650328605

Epoch: 5| Step: 4
Training loss: 2.8837196826934814
Validation loss: 2.5613977498905633

Epoch: 5| Step: 5
Training loss: 3.5400702953338623
Validation loss: 2.548054938675255

Epoch: 5| Step: 6
Training loss: 1.944811463356018
Validation loss: 2.5296542465045886

Epoch: 5| Step: 7
Training loss: 2.438868284225464
Validation loss: 2.518795108282438

Epoch: 5| Step: 8
Training loss: 2.7119011878967285
Validation loss: 2.505950174024028

Epoch: 5| Step: 9
Training loss: 2.5269641876220703
Validation loss: 2.5023082148644233

Epoch: 5| Step: 10
Training loss: 2.3552160263061523
Validation loss: 2.507126618457097

Epoch: 63| Step: 0
Training loss: 2.55060076713562
Validation loss: 2.5085871399089856

Epoch: 5| Step: 1
Training loss: 2.4483604431152344
Validation loss: 2.5099270523235364

Epoch: 5| Step: 2
Training loss: 2.9531779289245605
Validation loss: 2.508329217151929

Epoch: 5| Step: 3
Training loss: 2.668198585510254
Validation loss: 2.519214271217264

Epoch: 5| Step: 4
Training loss: 2.8397412300109863
Validation loss: 2.5531928026547996

Epoch: 5| Step: 5
Training loss: 2.9961700439453125
Validation loss: 2.5962832384212042

Epoch: 5| Step: 6
Training loss: 2.3406906127929688
Validation loss: 2.5218808522788425

Epoch: 5| Step: 7
Training loss: 2.8249571323394775
Validation loss: 2.5068285170421807

Epoch: 5| Step: 8
Training loss: 2.6231560707092285
Validation loss: 2.506250089214694

Epoch: 5| Step: 9
Training loss: 2.705150842666626
Validation loss: 2.510425054898826

Epoch: 5| Step: 10
Training loss: 3.0934088230133057
Validation loss: 2.5152883491208478

Epoch: 64| Step: 0
Training loss: 2.2935867309570312
Validation loss: 2.517815956505396

Epoch: 5| Step: 1
Training loss: 3.1070618629455566
Validation loss: 2.5092431242747972

Epoch: 5| Step: 2
Training loss: 2.9270718097686768
Validation loss: 2.4965756400938957

Epoch: 5| Step: 3
Training loss: 3.508816957473755
Validation loss: 2.4971085940637896

Epoch: 5| Step: 4
Training loss: 2.8301644325256348
Validation loss: 2.4963294844473563

Epoch: 5| Step: 5
Training loss: 3.029895067214966
Validation loss: 2.5025254372627503

Epoch: 5| Step: 6
Training loss: 2.847670793533325
Validation loss: 2.502775230715352

Epoch: 5| Step: 7
Training loss: 2.244373083114624
Validation loss: 2.5033510167111634

Epoch: 5| Step: 8
Training loss: 1.9161243438720703
Validation loss: 2.5115081751218407

Epoch: 5| Step: 9
Training loss: 2.8183369636535645
Validation loss: 2.521670654255857

Epoch: 5| Step: 10
Training loss: 2.380722761154175
Validation loss: 2.521859884262085

Epoch: 65| Step: 0
Training loss: 2.784411907196045
Validation loss: 2.5302184781720563

Epoch: 5| Step: 1
Training loss: 2.0204341411590576
Validation loss: 2.528259825962846

Epoch: 5| Step: 2
Training loss: 2.9795570373535156
Validation loss: 2.5219527111258557

Epoch: 5| Step: 3
Training loss: 2.603294610977173
Validation loss: 2.5147641474200833

Epoch: 5| Step: 4
Training loss: 3.1889681816101074
Validation loss: 2.51504260493863

Epoch: 5| Step: 5
Training loss: 2.280421257019043
Validation loss: 2.505410120051394

Epoch: 5| Step: 6
Training loss: 2.517911195755005
Validation loss: 2.512376072586224

Epoch: 5| Step: 7
Training loss: 3.3255324363708496
Validation loss: 2.509317777490103

Epoch: 5| Step: 8
Training loss: 2.3422114849090576
Validation loss: 2.5034033508710962

Epoch: 5| Step: 9
Training loss: 2.9862256050109863
Validation loss: 2.498974964182864

Epoch: 5| Step: 10
Training loss: 2.9138035774230957
Validation loss: 2.498077151595905

Epoch: 66| Step: 0
Training loss: 3.0101664066314697
Validation loss: 2.4990497891620924

Epoch: 5| Step: 1
Training loss: 2.7048356533050537
Validation loss: 2.4995995798418598

Epoch: 5| Step: 2
Training loss: 1.8862669467926025
Validation loss: 2.496682354198989

Epoch: 5| Step: 3
Training loss: 2.3410091400146484
Validation loss: 2.498134431018624

Epoch: 5| Step: 4
Training loss: 3.3598148822784424
Validation loss: 2.4947160546497633

Epoch: 5| Step: 5
Training loss: 2.4844748973846436
Validation loss: 2.490268753420922

Epoch: 5| Step: 6
Training loss: 2.2455391883850098
Validation loss: 2.496345550783219

Epoch: 5| Step: 7
Training loss: 2.736114978790283
Validation loss: 2.501899416728686

Epoch: 5| Step: 8
Training loss: 3.311800718307495
Validation loss: 2.5074571101896224

Epoch: 5| Step: 9
Training loss: 3.513017177581787
Validation loss: 2.5063362865037817

Epoch: 5| Step: 10
Training loss: 2.226194143295288
Validation loss: 2.5209053818897535

Epoch: 67| Step: 0
Training loss: 2.4609222412109375
Validation loss: 2.5184522572384087

Epoch: 5| Step: 1
Training loss: 2.164134979248047
Validation loss: 2.5077548847403577

Epoch: 5| Step: 2
Training loss: 2.4763951301574707
Validation loss: 2.5035879406877743

Epoch: 5| Step: 3
Training loss: 3.165677070617676
Validation loss: 2.49422021578717

Epoch: 5| Step: 4
Training loss: 3.0136756896972656
Validation loss: 2.4955018669046383

Epoch: 5| Step: 5
Training loss: 2.9115941524505615
Validation loss: 2.489952120729672

Epoch: 5| Step: 6
Training loss: 2.9236373901367188
Validation loss: 2.4912533811343613

Epoch: 5| Step: 7
Training loss: 2.5862948894500732
Validation loss: 2.4869396635281142

Epoch: 5| Step: 8
Training loss: 2.8555750846862793
Validation loss: 2.492219373744021

Epoch: 5| Step: 9
Training loss: 2.7486367225646973
Validation loss: 2.489532152811686

Epoch: 5| Step: 10
Training loss: 2.4779913425445557
Validation loss: 2.499515897484236

Epoch: 68| Step: 0
Training loss: 2.5956597328186035
Validation loss: 2.5017154370584795

Epoch: 5| Step: 1
Training loss: 2.678424119949341
Validation loss: 2.5055535788177163

Epoch: 5| Step: 2
Training loss: 1.965608835220337
Validation loss: 2.505686039565712

Epoch: 5| Step: 3
Training loss: 2.6612887382507324
Validation loss: 2.503444979267736

Epoch: 5| Step: 4
Training loss: 2.477498769760132
Validation loss: 2.499676586479269

Epoch: 5| Step: 5
Training loss: 3.1206443309783936
Validation loss: 2.492805691175563

Epoch: 5| Step: 6
Training loss: 2.83427095413208
Validation loss: 2.4874228251877653

Epoch: 5| Step: 7
Training loss: 2.800729751586914
Validation loss: 2.487426562975812

Epoch: 5| Step: 8
Training loss: 2.8618240356445312
Validation loss: 2.4914510942274526

Epoch: 5| Step: 9
Training loss: 3.0580859184265137
Validation loss: 2.495412339446365

Epoch: 5| Step: 10
Training loss: 2.6181418895721436
Validation loss: 2.5032910352112143

Epoch: 69| Step: 0
Training loss: 2.606243848800659
Validation loss: 2.5273948510487876

Epoch: 5| Step: 1
Training loss: 2.520202159881592
Validation loss: 2.5359561750965733

Epoch: 5| Step: 2
Training loss: 1.9976742267608643
Validation loss: 2.5284623740821757

Epoch: 5| Step: 3
Training loss: 3.0474672317504883
Validation loss: 2.48948493055118

Epoch: 5| Step: 4
Training loss: 2.637709140777588
Validation loss: 2.4895908012185046

Epoch: 5| Step: 5
Training loss: 2.8041858673095703
Validation loss: 2.4895682796355216

Epoch: 5| Step: 6
Training loss: 2.9488396644592285
Validation loss: 2.487121025721232

Epoch: 5| Step: 7
Training loss: 2.618847608566284
Validation loss: 2.495530451497724

Epoch: 5| Step: 8
Training loss: 2.892094612121582
Validation loss: 2.4940177548316216

Epoch: 5| Step: 9
Training loss: 2.3508427143096924
Validation loss: 2.501918936288485

Epoch: 5| Step: 10
Training loss: 3.439072370529175
Validation loss: 2.497088888640045

Epoch: 70| Step: 0
Training loss: 3.0791661739349365
Validation loss: 2.489043879252608

Epoch: 5| Step: 1
Training loss: 3.1641478538513184
Validation loss: 2.4931209677009174

Epoch: 5| Step: 2
Training loss: 2.6021552085876465
Validation loss: 2.4856141767194195

Epoch: 5| Step: 3
Training loss: 2.1379778385162354
Validation loss: 2.4816636116273942

Epoch: 5| Step: 4
Training loss: 2.4549598693847656
Validation loss: 2.482294979915824

Epoch: 5| Step: 5
Training loss: 2.927964925765991
Validation loss: 2.481656966670867

Epoch: 5| Step: 6
Training loss: 2.2187509536743164
Validation loss: 2.47714493607962

Epoch: 5| Step: 7
Training loss: 3.0723423957824707
Validation loss: 2.480949427491875

Epoch: 5| Step: 8
Training loss: 2.5212817192077637
Validation loss: 2.480011181164813

Epoch: 5| Step: 9
Training loss: 2.595029354095459
Validation loss: 2.4829231231443343

Epoch: 5| Step: 10
Training loss: 2.9860680103302
Validation loss: 2.48229718208313

Epoch: 71| Step: 0
Training loss: 3.1581645011901855
Validation loss: 2.483874285092918

Epoch: 5| Step: 1
Training loss: 2.9008612632751465
Validation loss: 2.4920073452816216

Epoch: 5| Step: 2
Training loss: 2.7212679386138916
Validation loss: 2.5024978422349498

Epoch: 5| Step: 3
Training loss: 2.6069960594177246
Validation loss: 2.503593070532686

Epoch: 5| Step: 4
Training loss: 3.2092063426971436
Validation loss: 2.496098164589174

Epoch: 5| Step: 5
Training loss: 2.5977954864501953
Validation loss: 2.4933255590418333

Epoch: 5| Step: 6
Training loss: 2.1567792892456055
Validation loss: 2.486047852423883

Epoch: 5| Step: 7
Training loss: 2.6451992988586426
Validation loss: 2.4742913041063535

Epoch: 5| Step: 8
Training loss: 2.490976333618164
Validation loss: 2.473340129339567

Epoch: 5| Step: 9
Training loss: 3.0705108642578125
Validation loss: 2.473568213883267

Epoch: 5| Step: 10
Training loss: 2.016101121902466
Validation loss: 2.467000081974973

Epoch: 72| Step: 0
Training loss: 3.449845790863037
Validation loss: 2.4747037451754332

Epoch: 5| Step: 1
Training loss: 2.610924243927002
Validation loss: 2.4737015437054377

Epoch: 5| Step: 2
Training loss: 2.493324041366577
Validation loss: 2.4758845888158327

Epoch: 5| Step: 3
Training loss: 2.6556057929992676
Validation loss: 2.4728461721892

Epoch: 5| Step: 4
Training loss: 2.863333225250244
Validation loss: 2.4790434093885523

Epoch: 5| Step: 5
Training loss: 2.7266998291015625
Validation loss: 2.4944814328224427

Epoch: 5| Step: 6
Training loss: 2.2906949520111084
Validation loss: 2.49926761914325

Epoch: 5| Step: 7
Training loss: 2.915726900100708
Validation loss: 2.503760968485186

Epoch: 5| Step: 8
Training loss: 2.3758504390716553
Validation loss: 2.503554477486559

Epoch: 5| Step: 9
Training loss: 2.262205123901367
Validation loss: 2.4979980837914253

Epoch: 5| Step: 10
Training loss: 2.977825164794922
Validation loss: 2.4852438793387464

Epoch: 73| Step: 0
Training loss: 2.6270031929016113
Validation loss: 2.474708213601061

Epoch: 5| Step: 1
Training loss: 2.6097235679626465
Validation loss: 2.467422536624375

Epoch: 5| Step: 2
Training loss: 2.7708733081817627
Validation loss: 2.4660072762479066

Epoch: 5| Step: 3
Training loss: 2.8526577949523926
Validation loss: 2.4721966815251175

Epoch: 5| Step: 4
Training loss: 2.6970534324645996
Validation loss: 2.4778145820863786

Epoch: 5| Step: 5
Training loss: 2.7102036476135254
Validation loss: 2.4785928264740975

Epoch: 5| Step: 6
Training loss: 2.301636219024658
Validation loss: 2.4702358835486957

Epoch: 5| Step: 7
Training loss: 3.5665180683135986
Validation loss: 2.470468607000125

Epoch: 5| Step: 8
Training loss: 2.238914966583252
Validation loss: 2.463069062079153

Epoch: 5| Step: 9
Training loss: 3.121865749359131
Validation loss: 2.4639823308555027

Epoch: 5| Step: 10
Training loss: 1.9621070623397827
Validation loss: 2.4718265815447737

Epoch: 74| Step: 0
Training loss: 3.2203049659729004
Validation loss: 2.4771650350222023

Epoch: 5| Step: 1
Training loss: 2.978454113006592
Validation loss: 2.484780967876475

Epoch: 5| Step: 2
Training loss: 2.8059768676757812
Validation loss: 2.48728669587002

Epoch: 5| Step: 3
Training loss: 2.4901328086853027
Validation loss: 2.499798905464911

Epoch: 5| Step: 4
Training loss: 3.423130512237549
Validation loss: 2.4906715193102436

Epoch: 5| Step: 5
Training loss: 1.705834984779358
Validation loss: 2.4817882532714517

Epoch: 5| Step: 6
Training loss: 3.0097837448120117
Validation loss: 2.478402171083676

Epoch: 5| Step: 7
Training loss: 2.106895685195923
Validation loss: 2.467492098449379

Epoch: 5| Step: 8
Training loss: 3.114239454269409
Validation loss: 2.471554525436894

Epoch: 5| Step: 9
Training loss: 2.31247878074646
Validation loss: 2.465753116915303

Epoch: 5| Step: 10
Training loss: 2.441615343093872
Validation loss: 2.4583781278261574

Epoch: 75| Step: 0
Training loss: 2.79996919631958
Validation loss: 2.4637444814046225

Epoch: 5| Step: 1
Training loss: 3.5030887126922607
Validation loss: 2.4609997862128803

Epoch: 5| Step: 2
Training loss: 2.2866034507751465
Validation loss: 2.4666985670725503

Epoch: 5| Step: 3
Training loss: 2.1595458984375
Validation loss: 2.4596145383773313

Epoch: 5| Step: 4
Training loss: 2.674410820007324
Validation loss: 2.458090059218868

Epoch: 5| Step: 5
Training loss: 2.5818591117858887
Validation loss: 2.4616347435981996

Epoch: 5| Step: 6
Training loss: 2.1906542778015137
Validation loss: 2.4643048112110426

Epoch: 5| Step: 7
Training loss: 2.6453442573547363
Validation loss: 2.4639867505719586

Epoch: 5| Step: 8
Training loss: 2.9938910007476807
Validation loss: 2.462893465513824

Epoch: 5| Step: 9
Training loss: 2.969329357147217
Validation loss: 2.4690298649572555

Epoch: 5| Step: 10
Training loss: 2.7703888416290283
Validation loss: 2.4623650197059876

Testing loss: 2.600097232394748
